{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dataset import *\n",
    "from save_load import *\n",
    "from NN_library.PINN.PINN import *\n",
    "from NN_library.PINN.train_PINN import *\n",
    "from matplotlib.tri import Triangulation\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_Sobol(14, [0, 2], [0, 2])\n",
    "loaders = get_loaders_Sobol(data, 2**14+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1aa7f226f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = 2\n",
    "N = 512 # number of nodes in each direction including the border\n",
    "H1 = torch.tensor([1, 0], device=dev).view(1, 2) # macrogradient\n",
    "H2 = torch.tensor([0, 1], device=dev).view(1, 2) # macrogradient\n",
    "x = np.linspace(0, L, N, endpoint=True)\n",
    "y = np.linspace(0, L, N, endpoint=True)\n",
    "\n",
    "XY = np.meshgrid(x, y)\n",
    "grid_data = torch.tensor(np.vstack((XY[0].flatten(), XY[1].flatten())).T, dtype=torch.float, device=dev)\n",
    "def a_function(x,y):\n",
    "    a = 1 + 0.6*np.abs(y-1) - 0.4*np.abs(x-1)\n",
    "    return a\n",
    "def A(x):\n",
    "    a = (1 + 0.6*torch.abs(x[:,1]-1) - 0.4*torch.abs(x[:,0]-1)).view(-1,1,1)\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = a * I\n",
    "    return A\n",
    "Z = a_function(XY[0].flatten(),XY[1].flatten())\n",
    "plt.pcolormesh(XY[0], XY[1], Z.reshape(N, N))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:,0], data[:,1], s = 0.5, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 65\n"
     ]
    }
   ],
   "source": [
    "net_H1 = PINN(n_periodic=2, n_hidden=4, n_layers=1, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_H1.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "args = {'lr' : 0.0005, 'epochs' : 10000, 'dev' : dev, 'name' : f'NN_library/PINN/PINN_H1_{total_params}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_H1 = load_network(net_H1, args['name']+'', args)\n",
    "net_H1 = net_H1.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 mean train loss:  4.80518615e+00, mean val. rec. loss:  4.70129322e+00\n",
      "Epoch: 1 mean train loss:  4.70529932e+00, mean val. rec. loss:  4.60330284e+00\n",
      "Epoch: 2 mean train loss:  4.60726027e+00, mean val. rec. loss:  4.50718604e+00\n",
      "Epoch: 3 mean train loss:  4.51109568e+00, mean val. rec. loss:  4.41296314e+00\n",
      "Epoch: 4 mean train loss:  4.41682510e+00, mean val. rec. loss:  4.32064925e+00\n",
      "Epoch: 5 mean train loss:  4.32446474e+00, mean val. rec. loss:  4.23025857e+00\n",
      "Epoch: 6 mean train loss:  4.23402890e+00, mean val. rec. loss:  4.14180361e+00\n",
      "Epoch: 7 mean train loss:  4.14552903e+00, mean val. rec. loss:  4.05529655e+00\n",
      "Epoch: 8 mean train loss:  4.05897656e+00, mean val. rec. loss:  3.97074144e+00\n",
      "Epoch: 9 mean train loss:  3.97437817e+00, mean val. rec. loss:  3.88814411e+00\n",
      "Epoch: 10 mean train loss:  3.89173767e+00, mean val. rec. loss:  3.80750368e+00\n",
      "Epoch: 11 mean train loss:  3.81105482e+00, mean val. rec. loss:  3.72881521e+00\n",
      "Epoch: 12 mean train loss:  3.73232439e+00, mean val. rec. loss:  3.65206971e+00\n",
      "Epoch: 13 mean train loss:  3.65553778e+00, mean val. rec. loss:  3.57725469e+00\n",
      "Epoch: 14 mean train loss:  3.58068237e+00, mean val. rec. loss:  3.50435041e+00\n",
      "Epoch: 15 mean train loss:  3.50773885e+00, mean val. rec. loss:  3.43333538e+00\n",
      "Epoch: 16 mean train loss:  3.43668505e+00, mean val. rec. loss:  3.36418408e+00\n",
      "Epoch: 17 mean train loss:  3.36749546e+00, mean val. rec. loss:  3.29686484e+00\n",
      "Epoch: 18 mean train loss:  3.30013911e+00, mean val. rec. loss:  3.23134371e+00\n",
      "Epoch: 19 mean train loss:  3.23458142e+00, mean val. rec. loss:  3.16758294e+00\n",
      "Epoch: 20 mean train loss:  3.17078522e+00, mean val. rec. loss:  3.10554249e+00\n",
      "Epoch: 21 mean train loss:  3.10870974e+00, mean val. rec. loss:  3.04517879e+00\n",
      "Epoch: 22 mean train loss:  3.04831231e+00, mean val. rec. loss:  2.98644715e+00\n",
      "Epoch: 23 mean train loss:  2.98954790e+00, mean val. rec. loss:  2.92930024e+00\n",
      "Epoch: 24 mean train loss:  2.93236858e+00, mean val. rec. loss:  2.87368959e+00\n",
      "Epoch: 25 mean train loss:  2.87672692e+00, mean val. rec. loss:  2.81956643e+00\n",
      "Epoch: 26 mean train loss:  2.82257359e+00, mean val. rec. loss:  2.76687880e+00\n",
      "Epoch: 27 mean train loss:  2.76985637e+00, mean val. rec. loss:  2.71557530e+00\n",
      "Epoch: 28 mean train loss:  2.71852451e+00, mean val. rec. loss:  2.66560456e+00\n",
      "Epoch: 29 mean train loss:  2.66852651e+00, mean val. rec. loss:  2.61691578e+00\n",
      "Epoch: 30 mean train loss:  2.61981137e+00, mean val. rec. loss:  2.56945844e+00\n",
      "Epoch: 31 mean train loss:  2.57232808e+00, mean val. rec. loss:  2.52318175e+00\n",
      "Epoch: 32 mean train loss:  2.52602681e+00, mean val. rec. loss:  2.47803779e+00\n",
      "Epoch: 33 mean train loss:  2.48085966e+00, mean val. rec. loss:  2.43397985e+00\n",
      "Epoch: 34 mean train loss:  2.43677872e+00, mean val. rec. loss:  2.39096262e+00\n",
      "Epoch: 35 mean train loss:  2.39373989e+00, mean val. rec. loss:  2.34894256e+00\n",
      "Epoch: 36 mean train loss:  2.35169884e+00, mean val. rec. loss:  2.30787904e+00\n",
      "Epoch: 37 mean train loss:  2.31061552e+00, mean val. rec. loss:  2.26773474e+00\n",
      "Epoch: 38 mean train loss:  2.27045180e+00, mean val. rec. loss:  2.22847135e+00\n",
      "Epoch: 39 mean train loss:  2.23117048e+00, mean val. rec. loss:  2.19005724e+00\n",
      "Epoch: 40 mean train loss:  2.19273868e+00, mean val. rec. loss:  2.15245885e+00\n",
      "Epoch: 41 mean train loss:  2.15512375e+00, mean val. rec. loss:  2.11564805e+00\n",
      "Epoch: 42 mean train loss:  2.11829731e+00, mean val. rec. loss:  2.07959768e+00\n",
      "Epoch: 43 mean train loss:  2.08223171e+00, mean val. rec. loss:  2.04428206e+00\n",
      "Epoch: 44 mean train loss:  2.04690217e+00, mean val. rec. loss:  2.00967897e+00\n",
      "Epoch: 45 mean train loss:  2.01228509e+00, mean val. rec. loss:  1.97576708e+00\n",
      "Epoch: 46 mean train loss:  1.97836021e+00, mean val. rec. loss:  1.94252636e+00\n",
      "Epoch: 47 mean train loss:  1.94510727e+00, mean val. rec. loss:  1.90993867e+00\n",
      "Epoch: 48 mean train loss:  1.91250791e+00, mean val. rec. loss:  1.87798775e+00\n",
      "Epoch: 49 mean train loss:  1.88054581e+00, mean val. rec. loss:  1.84665719e+00\n",
      "Epoch: 50 mean train loss:  1.84920464e+00, mean val. rec. loss:  1.81593321e+00\n",
      "Epoch: 51 mean train loss:  1.81847069e+00, mean val. rec. loss:  1.78580158e+00\n",
      "Epoch: 52 mean train loss:  1.78832954e+00, mean val. rec. loss:  1.75625026e+00\n",
      "Epoch: 53 mean train loss:  1.75876904e+00, mean val. rec. loss:  1.72726647e+00\n",
      "Epoch: 54 mean train loss:  1.72977679e+00, mean val. rec. loss:  1.69883948e+00\n",
      "Epoch: 55 mean train loss:  1.70134159e+00, mean val. rec. loss:  1.67095795e+00\n",
      "Epoch: 56 mean train loss:  1.67345223e+00, mean val. rec. loss:  1.64361173e+00\n",
      "Epoch: 57 mean train loss:  1.64609859e+00, mean val. rec. loss:  1.61679066e+00\n",
      "Epoch: 58 mean train loss:  1.61927053e+00, mean val. rec. loss:  1.59048502e+00\n",
      "Epoch: 59 mean train loss:  1.59295817e+00, mean val. rec. loss:  1.56468536e+00\n",
      "Epoch: 60 mean train loss:  1.56715208e+00, mean val. rec. loss:  1.53938241e+00\n",
      "Epoch: 61 mean train loss:  1.54184297e+00, mean val. rec. loss:  1.51456744e+00\n",
      "Epoch: 62 mean train loss:  1.51702227e+00, mean val. rec. loss:  1.49023176e+00\n",
      "Epoch: 63 mean train loss:  1.49268103e+00, mean val. rec. loss:  1.46636578e+00\n",
      "Epoch: 64 mean train loss:  1.46880971e+00, mean val. rec. loss:  1.44296138e+00\n",
      "Epoch: 65 mean train loss:  1.44540022e+00, mean val. rec. loss:  1.42000984e+00\n",
      "Epoch: 66 mean train loss:  1.42244385e+00, mean val. rec. loss:  1.39750318e+00\n",
      "Epoch: 67 mean train loss:  1.39993238e+00, mean val. rec. loss:  1.37543240e+00\n",
      "Epoch: 68 mean train loss:  1.37785699e+00, mean val. rec. loss:  1.35378996e+00\n",
      "Epoch: 69 mean train loss:  1.35621017e+00, mean val. rec. loss:  1.33256758e+00\n",
      "Epoch: 70 mean train loss:  1.33498382e+00, mean val. rec. loss:  1.31175713e+00\n",
      "Epoch: 71 mean train loss:  1.31416924e+00, mean val. rec. loss:  1.29135063e+00\n",
      "Epoch: 72 mean train loss:  1.29375868e+00, mean val. rec. loss:  1.27134010e+00\n",
      "Epoch: 73 mean train loss:  1.27374439e+00, mean val. rec. loss:  1.25171813e+00\n",
      "Epoch: 74 mean train loss:  1.25411875e+00, mean val. rec. loss:  1.23247674e+00\n",
      "Epoch: 75 mean train loss:  1.23487401e+00, mean val. rec. loss:  1.21360860e+00\n",
      "Epoch: 76 mean train loss:  1.21600242e+00, mean val. rec. loss:  1.19510639e+00\n",
      "Epoch: 77 mean train loss:  1.19749707e+00, mean val. rec. loss:  1.17696225e+00\n",
      "Epoch: 78 mean train loss:  1.17934962e+00, mean val. rec. loss:  1.15916938e+00\n",
      "Epoch: 79 mean train loss:  1.16155376e+00, mean val. rec. loss:  1.14172080e+00\n",
      "Epoch: 80 mean train loss:  1.14410196e+00, mean val. rec. loss:  1.12460926e+00\n",
      "Epoch: 81 mean train loss:  1.12698757e+00, mean val. rec. loss:  1.10782800e+00\n",
      "Epoch: 82 mean train loss:  1.11020354e+00, mean val. rec. loss:  1.09137014e+00\n",
      "Epoch: 83 mean train loss:  1.09374285e+00, mean val. rec. loss:  1.07522935e+00\n",
      "Epoch: 84 mean train loss:  1.07759942e+00, mean val. rec. loss:  1.05939904e+00\n",
      "Epoch: 85 mean train loss:  1.06176657e+00, mean val. rec. loss:  1.04387303e+00\n",
      "Epoch: 86 mean train loss:  1.04623798e+00, mean val. rec. loss:  1.02864487e+00\n",
      "Epoch: 87 mean train loss:  1.03100735e+00, mean val. rec. loss:  1.01370831e+00\n",
      "Epoch: 88 mean train loss:  1.01606859e+00, mean val. rec. loss:  9.99057548e-01\n",
      "Epoch: 89 mean train loss:  1.00141538e+00, mean val. rec. loss:  9.84686998e-01\n",
      "Epoch: 90 mean train loss:  9.87042491e-01, mean val. rec. loss:  9.70590852e-01\n",
      "Epoch: 91 mean train loss:  9.72944189e-01, mean val. rec. loss:  9.56763523e-01\n",
      "Epoch: 92 mean train loss:  9.59114758e-01, mean val. rec. loss:  9.43199421e-01\n",
      "Epoch: 93 mean train loss:  9.45548477e-01, mean val. rec. loss:  9.29893249e-01\n",
      "Epoch: 94 mean train loss:  9.32240340e-01, mean val. rec. loss:  9.16839708e-01\n",
      "Epoch: 95 mean train loss:  9.19184924e-01, mean val. rec. loss:  9.04033792e-01\n",
      "Epoch: 96 mean train loss:  9.06377046e-01, mean val. rec. loss:  8.91470348e-01\n",
      "Epoch: 97 mean train loss:  8.93811759e-01, mean val. rec. loss:  8.79144440e-01\n",
      "Epoch: 98 mean train loss:  8.81483998e-01, mean val. rec. loss:  8.67051713e-01\n",
      "Epoch: 99 mean train loss:  8.69389533e-01, mean val. rec. loss:  8.55186871e-01\n",
      "Epoch: 100 mean train loss:  8.57522940e-01, mean val. rec. loss:  8.43545485e-01\n",
      "Epoch: 101 mean train loss:  8.45879988e-01, mean val. rec. loss:  8.32123129e-01\n",
      "Epoch: 102 mean train loss:  8.34456150e-01, mean val. rec. loss:  8.20915449e-01\n",
      "Epoch: 103 mean train loss:  8.23246836e-01, mean val. rec. loss:  8.09917872e-01\n",
      "Epoch: 104 mean train loss:  8.12247815e-01, mean val. rec. loss:  7.99126406e-01\n",
      "Epoch: 105 mean train loss:  8.01454856e-01, mean val. rec. loss:  7.88536915e-01\n",
      "Epoch: 106 mean train loss:  7.90864028e-01, mean val. rec. loss:  7.78145408e-01\n",
      "Epoch: 107 mean train loss:  7.80471157e-01, mean val. rec. loss:  7.67947747e-01\n",
      "Epoch: 108 mean train loss:  7.70272193e-01, mean val. rec. loss:  7.57939941e-01\n",
      "Epoch: 109 mean train loss:  7.60263202e-01, mean val. rec. loss:  7.48118216e-01\n",
      "Epoch: 110 mean train loss:  7.50440311e-01, mean val. rec. loss:  7.38479088e-01\n",
      "Epoch: 111 mean train loss:  7.40800004e-01, mean val. rec. loss:  7.29018566e-01\n",
      "Epoch: 112 mean train loss:  7.31338408e-01, mean val. rec. loss:  7.19733239e-01\n",
      "Epoch: 113 mean train loss:  7.22052006e-01, mean val. rec. loss:  7.10619479e-01\n",
      "Epoch: 114 mean train loss:  7.12937284e-01, mean val. rec. loss:  7.01673945e-01\n",
      "Epoch: 115 mean train loss:  7.03990725e-01, mean val. rec. loss:  6.92893302e-01\n",
      "Epoch: 116 mean train loss:  6.95209230e-01, mean val. rec. loss:  6.84274354e-01\n",
      "Epoch: 117 mean train loss:  6.86589404e-01, mean val. rec. loss:  6.75813618e-01\n",
      "Epoch: 118 mean train loss:  6.78127908e-01, mean val. rec. loss:  6.67507902e-01\n",
      "Epoch: 119 mean train loss:  6.69821406e-01, mean val. rec. loss:  6.59354302e-01\n",
      "Epoch: 120 mean train loss:  6.61667038e-01, mean val. rec. loss:  6.51349624e-01\n",
      "Epoch: 121 mean train loss:  6.53661645e-01, mean val. rec. loss:  6.43490967e-01\n",
      "Epoch: 122 mean train loss:  6.45802366e-01, mean val. rec. loss:  6.35775427e-01\n",
      "Epoch: 123 mean train loss:  6.38086224e-01, mean val. rec. loss:  6.28199955e-01\n",
      "Epoch: 124 mean train loss:  6.30510237e-01, mean val. rec. loss:  6.20762085e-01\n",
      "Epoch: 125 mean train loss:  6.23071784e-01, mean val. rec. loss:  6.13458767e-01\n",
      "Epoch: 126 mean train loss:  6.15767945e-01, mean val. rec. loss:  6.06287318e-01\n",
      "Epoch: 127 mean train loss:  6.08596098e-01, mean val. rec. loss:  5.99245486e-01\n",
      "Epoch: 128 mean train loss:  6.01553800e-01, mean val. rec. loss:  5.92330406e-01\n",
      "Epoch: 129 mean train loss:  5.94638369e-01, mean val. rec. loss:  5.85539574e-01\n",
      "Epoch: 130 mean train loss:  5.87847124e-01, mean val. rec. loss:  5.78870558e-01\n",
      "Epoch: 131 mean train loss:  5.81177920e-01, mean val. rec. loss:  5.72320999e-01\n",
      "Epoch: 132 mean train loss:  5.74628015e-01, mean val. rec. loss:  5.65888358e-01\n",
      "Epoch: 133 mean train loss:  5.68195085e-01, mean val. rec. loss:  5.59570385e-01\n",
      "Epoch: 134 mean train loss:  5.61876926e-01, mean val. rec. loss:  5.53365011e-01\n",
      "Epoch: 135 mean train loss:  5.55671332e-01, mean val. rec. loss:  5.47269769e-01\n",
      "Epoch: 136 mean train loss:  5.49575921e-01, mean val. rec. loss:  5.41282554e-01\n",
      "Epoch: 137 mean train loss:  5.43588606e-01, mean val. rec. loss:  5.35401298e-01\n",
      "Epoch: 138 mean train loss:  5.37707242e-01, mean val. rec. loss:  5.29623896e-01\n",
      "Epoch: 139 mean train loss:  5.31929804e-01, mean val. rec. loss:  5.23948316e-01\n",
      "Epoch: 140 mean train loss:  5.26254145e-01, mean val. rec. loss:  5.18372634e-01\n",
      "Epoch: 141 mean train loss:  5.20678479e-01, mean val. rec. loss:  5.12894929e-01\n",
      "Epoch: 142 mean train loss:  5.15200719e-01, mean val. rec. loss:  5.07513094e-01\n",
      "Epoch: 143 mean train loss:  5.09819018e-01, mean val. rec. loss:  5.02225352e-01\n",
      "Epoch: 144 mean train loss:  5.04531291e-01, mean val. rec. loss:  4.97030034e-01\n",
      "Epoch: 145 mean train loss:  4.99336017e-01, mean val. rec. loss:  4.91925107e-01\n",
      "Epoch: 146 mean train loss:  4.94231262e-01, mean val. rec. loss:  4.86908939e-01\n",
      "Epoch: 147 mean train loss:  4.89215176e-01, mean val. rec. loss:  4.81979751e-01\n",
      "Epoch: 148 mean train loss:  4.84286211e-01, mean val. rec. loss:  4.77136274e-01\n",
      "Epoch: 149 mean train loss:  4.79442847e-01, mean val. rec. loss:  4.72376330e-01\n",
      "Epoch: 150 mean train loss:  4.74683088e-01, mean val. rec. loss:  4.67698541e-01\n",
      "Epoch: 151 mean train loss:  4.70005534e-01, mean val. rec. loss:  4.63101309e-01\n",
      "Epoch: 152 mean train loss:  4.65408545e-01, mean val. rec. loss:  4.58583183e-01\n",
      "Epoch: 153 mean train loss:  4.60890662e-01, mean val. rec. loss:  4.54142603e-01\n",
      "Epoch: 154 mean train loss:  4.56450366e-01, mean val. rec. loss:  4.49778154e-01\n",
      "Epoch: 155 mean train loss:  4.52086166e-01, mean val. rec. loss:  4.45488239e-01\n",
      "Epoch: 156 mean train loss:  4.47796603e-01, mean val. rec. loss:  4.41271661e-01\n",
      "Epoch: 157 mean train loss:  4.43580335e-01, mean val. rec. loss:  4.37126931e-01\n",
      "Epoch: 158 mean train loss:  4.39435963e-01, mean val. rec. loss:  4.33052707e-01\n",
      "Epoch: 159 mean train loss:  4.35362086e-01, mean val. rec. loss:  4.29047683e-01\n",
      "Epoch: 160 mean train loss:  4.31357453e-01, mean val. rec. loss:  4.25110662e-01\n",
      "Epoch: 161 mean train loss:  4.27420842e-01, mean val. rec. loss:  4.21240336e-01\n",
      "Epoch: 162 mean train loss:  4.23550912e-01, mean val. rec. loss:  4.17435400e-01\n",
      "Epoch: 163 mean train loss:  4.19746382e-01, mean val. rec. loss:  4.13694656e-01\n",
      "Epoch: 164 mean train loss:  4.16006120e-01, mean val. rec. loss:  4.10016943e-01\n",
      "Epoch: 165 mean train loss:  4.12328816e-01, mean val. rec. loss:  4.06401173e-01\n",
      "Epoch: 166 mean train loss:  4.08713514e-01, mean val. rec. loss:  4.02846183e-01\n",
      "Epoch: 167 mean train loss:  4.05159025e-01, mean val. rec. loss:  3.99350778e-01\n",
      "Epoch: 168 mean train loss:  4.01664125e-01, mean val. rec. loss:  3.95914085e-01\n",
      "Epoch: 169 mean train loss:  3.98227922e-01, mean val. rec. loss:  3.92534871e-01\n",
      "Epoch: 170 mean train loss:  3.94849253e-01, mean val. rec. loss:  3.89212083e-01\n",
      "Epoch: 171 mean train loss:  3.91526985e-01, mean val. rec. loss:  3.85944743e-01\n",
      "Epoch: 172 mean train loss:  3.88260227e-01, mean val. rec. loss:  3.82732015e-01\n",
      "Epoch: 173 mean train loss:  3.85048022e-01, mean val. rec. loss:  3.79572774e-01\n",
      "Epoch: 174 mean train loss:  3.81889390e-01, mean val. rec. loss:  3.76466078e-01\n",
      "Epoch: 175 mean train loss:  3.78783256e-01, mean val. rec. loss:  3.73410945e-01\n",
      "Epoch: 176 mean train loss:  3.75728727e-01, mean val. rec. loss:  3.70406615e-01\n",
      "Epoch: 177 mean train loss:  3.72724969e-01, mean val. rec. loss:  3.67451998e-01\n",
      "Epoch: 178 mean train loss:  3.69771029e-01, mean val. rec. loss:  3.64546405e-01\n",
      "Epoch: 179 mean train loss:  3.66866071e-01, mean val. rec. loss:  3.61688893e-01\n",
      "Epoch: 180 mean train loss:  3.64009173e-01, mean val. rec. loss:  3.58878554e-01\n",
      "Epoch: 181 mean train loss:  3.61199530e-01, mean val. rec. loss:  3.56114699e-01\n",
      "Epoch: 182 mean train loss:  3.58436307e-01, mean val. rec. loss:  3.53396458e-01\n",
      "Epoch: 183 mean train loss:  3.55718760e-01, mean val. rec. loss:  3.50723031e-01\n",
      "Epoch: 184 mean train loss:  3.53045995e-01, mean val. rec. loss:  3.48093620e-01\n",
      "Epoch: 185 mean train loss:  3.50417238e-01, mean val. rec. loss:  3.45507537e-01\n",
      "Epoch: 186 mean train loss:  3.47831922e-01, mean val. rec. loss:  3.42964091e-01\n",
      "Epoch: 187 mean train loss:  3.45289212e-01, mean val. rec. loss:  3.40462520e-01\n",
      "Epoch: 188 mean train loss:  3.42788305e-01, mean val. rec. loss:  3.38001954e-01\n",
      "Epoch: 189 mean train loss:  3.40328515e-01, mean val. rec. loss:  3.35581885e-01\n",
      "Epoch: 190 mean train loss:  3.37909158e-01, mean val. rec. loss:  3.33201586e-01\n",
      "Epoch: 191 mean train loss:  3.35529606e-01, mean val. rec. loss:  3.30860260e-01\n",
      "Epoch: 192 mean train loss:  3.33189086e-01, mean val. rec. loss:  3.28557399e-01\n",
      "Epoch: 193 mean train loss:  3.30886941e-01, mean val. rec. loss:  3.26292204e-01\n",
      "Epoch: 194 mean train loss:  3.28622577e-01, mean val. rec. loss:  3.24064239e-01\n",
      "Epoch: 195 mean train loss:  3.26395397e-01, mean val. rec. loss:  3.21872779e-01\n",
      "Epoch: 196 mean train loss:  3.24204687e-01, mean val. rec. loss:  3.19717171e-01\n",
      "Epoch: 197 mean train loss:  3.22049909e-01, mean val. rec. loss:  3.17596980e-01\n",
      "Epoch: 198 mean train loss:  3.19930498e-01, mean val. rec. loss:  3.15511479e-01\n",
      "Epoch: 199 mean train loss:  3.17845828e-01, mean val. rec. loss:  3.13460088e-01\n",
      "Epoch: 200 mean train loss:  3.15795273e-01, mean val. rec. loss:  3.11442263e-01\n",
      "Epoch: 201 mean train loss:  3.13778267e-01, mean val. rec. loss:  3.09457568e-01\n",
      "Epoch: 202 mean train loss:  3.11794453e-01, mean val. rec. loss:  3.07505350e-01\n",
      "Epoch: 203 mean train loss:  3.09843086e-01, mean val. rec. loss:  3.05585101e-01\n",
      "Epoch: 204 mean train loss:  3.07923659e-01, mean val. rec. loss:  3.03696277e-01\n",
      "Epoch: 205 mean train loss:  3.06035726e-01, mean val. rec. loss:  3.01838352e-01\n",
      "Epoch: 206 mean train loss:  3.04178660e-01, mean val. rec. loss:  3.00010835e-01\n",
      "Epoch: 207 mean train loss:  3.02352045e-01, mean val. rec. loss:  2.98213274e-01\n",
      "Epoch: 208 mean train loss:  3.00555344e-01, mean val. rec. loss:  2.96445141e-01\n",
      "Epoch: 209 mean train loss:  2.98788140e-01, mean val. rec. loss:  2.94705966e-01\n",
      "Epoch: 210 mean train loss:  2.97049867e-01, mean val. rec. loss:  2.92995276e-01\n",
      "Epoch: 211 mean train loss:  2.95340078e-01, mean val. rec. loss:  2.91312636e-01\n",
      "Epoch: 212 mean train loss:  2.93658386e-01, mean val. rec. loss:  2.89657538e-01\n",
      "Epoch: 213 mean train loss:  2.92004165e-01, mean val. rec. loss:  2.88029602e-01\n",
      "Epoch: 214 mean train loss:  2.90377177e-01, mean val. rec. loss:  2.86428336e-01\n",
      "Epoch: 215 mean train loss:  2.88776885e-01, mean val. rec. loss:  2.84853324e-01\n",
      "Epoch: 216 mean train loss:  2.87202812e-01, mean val. rec. loss:  2.83304221e-01\n",
      "Epoch: 217 mean train loss:  2.85654662e-01, mean val. rec. loss:  2.81780483e-01\n",
      "Epoch: 218 mean train loss:  2.84131896e-01, mean val. rec. loss:  2.80281784e-01\n",
      "Epoch: 219 mean train loss:  2.82634159e-01, mean val. rec. loss:  2.78807687e-01\n",
      "Epoch: 220 mean train loss:  2.81161002e-01, mean val. rec. loss:  2.77357793e-01\n",
      "Epoch: 221 mean train loss:  2.79712099e-01, mean val. rec. loss:  2.75931740e-01\n",
      "Epoch: 222 mean train loss:  2.78287031e-01, mean val. rec. loss:  2.74529129e-01\n",
      "Epoch: 223 mean train loss:  2.76885413e-01, mean val. rec. loss:  2.73149579e-01\n",
      "Epoch: 224 mean train loss:  2.75506826e-01, mean val. rec. loss:  2.71792726e-01\n",
      "Epoch: 225 mean train loss:  2.74151002e-01, mean val. rec. loss:  2.70458172e-01\n",
      "Epoch: 226 mean train loss:  2.72817465e-01, mean val. rec. loss:  2.69145625e-01\n",
      "Epoch: 227 mean train loss:  2.71505917e-01, mean val. rec. loss:  2.67854634e-01\n",
      "Epoch: 228 mean train loss:  2.70215940e-01, mean val. rec. loss:  2.66585016e-01\n",
      "Epoch: 229 mean train loss:  2.68947386e-01, mean val. rec. loss:  2.65336262e-01\n",
      "Epoch: 230 mean train loss:  2.67699629e-01, mean val. rec. loss:  2.64108120e-01\n",
      "Epoch: 231 mean train loss:  2.66472550e-01, mean val. rec. loss:  2.62900227e-01\n",
      "Epoch: 232 mean train loss:  2.65265672e-01, mean val. rec. loss:  2.61712273e-01\n",
      "Epoch: 233 mean train loss:  2.64078786e-01, mean val. rec. loss:  2.60543950e-01\n",
      "Epoch: 234 mean train loss:  2.62911505e-01, mean val. rec. loss:  2.59394914e-01\n",
      "Epoch: 235 mean train loss:  2.61763532e-01, mean val. rec. loss:  2.58264893e-01\n",
      "Epoch: 236 mean train loss:  2.60634568e-01, mean val. rec. loss:  2.57153524e-01\n",
      "Epoch: 237 mean train loss:  2.59524285e-01, mean val. rec. loss:  2.56060552e-01\n",
      "Epoch: 238 mean train loss:  2.58432386e-01, mean val. rec. loss:  2.54985578e-01\n",
      "Epoch: 239 mean train loss:  2.57358483e-01, mean val. rec. loss:  2.53928422e-01\n",
      "Epoch: 240 mean train loss:  2.56302427e-01, mean val. rec. loss:  2.52888719e-01\n",
      "Epoch: 241 mean train loss:  2.55263832e-01, mean val. rec. loss:  2.51866253e-01\n",
      "Epoch: 242 mean train loss:  2.54242457e-01, mean val. rec. loss:  2.50860734e-01\n",
      "Epoch: 243 mean train loss:  2.53238007e-01, mean val. rec. loss:  2.49871833e-01\n",
      "Epoch: 244 mean train loss:  2.52250241e-01, mean val. rec. loss:  2.48899335e-01\n",
      "Epoch: 245 mean train loss:  2.51278833e-01, mean val. rec. loss:  2.47942894e-01\n",
      "Epoch: 246 mean train loss:  2.50323515e-01, mean val. rec. loss:  2.47002310e-01\n",
      "Epoch: 247 mean train loss:  2.49384047e-01, mean val. rec. loss:  2.46077276e-01\n",
      "Epoch: 248 mean train loss:  2.48460162e-01, mean val. rec. loss:  2.45167590e-01\n",
      "Epoch: 249 mean train loss:  2.47551622e-01, mean val. rec. loss:  2.44272983e-01\n",
      "Epoch: 250 mean train loss:  2.46658128e-01, mean val. rec. loss:  2.43393162e-01\n",
      "Epoch: 251 mean train loss:  2.45779442e-01, mean val. rec. loss:  2.42527947e-01\n",
      "Epoch: 252 mean train loss:  2.44915356e-01, mean val. rec. loss:  2.41677047e-01\n",
      "Epoch: 253 mean train loss:  2.44065601e-01, mean val. rec. loss:  2.40840245e-01\n",
      "Epoch: 254 mean train loss:  2.43229939e-01, mean val. rec. loss:  2.40017250e-01\n",
      "Epoch: 255 mean train loss:  2.42408103e-01, mean val. rec. loss:  2.39207881e-01\n",
      "Epoch: 256 mean train loss:  2.41599882e-01, mean val. rec. loss:  2.38411866e-01\n",
      "Epoch: 257 mean train loss:  2.40805054e-01, mean val. rec. loss:  2.37629077e-01\n",
      "Epoch: 258 mean train loss:  2.40023426e-01, mean val. rec. loss:  2.36859207e-01\n",
      "Epoch: 259 mean train loss:  2.39254714e-01, mean val. rec. loss:  2.36102074e-01\n",
      "Epoch: 260 mean train loss:  2.38498754e-01, mean val. rec. loss:  2.35357405e-01\n",
      "Epoch: 261 mean train loss:  2.37755278e-01, mean val. rec. loss:  2.34625055e-01\n",
      "Epoch: 262 mean train loss:  2.37024093e-01, mean val. rec. loss:  2.33904790e-01\n",
      "Epoch: 263 mean train loss:  2.36305019e-01, mean val. rec. loss:  2.33196354e-01\n",
      "Epoch: 264 mean train loss:  2.35597789e-01, mean val. rec. loss:  2.32499639e-01\n",
      "Epoch: 265 mean train loss:  2.34902253e-01, mean val. rec. loss:  2.31814336e-01\n",
      "Epoch: 266 mean train loss:  2.34218159e-01, mean val. rec. loss:  2.31140318e-01\n",
      "Epoch: 267 mean train loss:  2.33545327e-01, mean val. rec. loss:  2.30477332e-01\n",
      "Epoch: 268 mean train loss:  2.32883549e-01, mean val. rec. loss:  2.29825286e-01\n",
      "Epoch: 269 mean train loss:  2.32232691e-01, mean val. rec. loss:  2.29183891e-01\n",
      "Epoch: 270 mean train loss:  2.31592500e-01, mean val. rec. loss:  2.28552965e-01\n",
      "Epoch: 271 mean train loss:  2.30962811e-01, mean val. rec. loss:  2.27932416e-01\n",
      "Epoch: 272 mean train loss:  2.30343462e-01, mean val. rec. loss:  2.27321956e-01\n",
      "Epoch: 273 mean train loss:  2.29734213e-01, mean val. rec. loss:  2.26721474e-01\n",
      "Epoch: 274 mean train loss:  2.29134930e-01, mean val. rec. loss:  2.26130718e-01\n",
      "Epoch: 275 mean train loss:  2.28545435e-01, mean val. rec. loss:  2.25549614e-01\n",
      "Epoch: 276 mean train loss:  2.27965549e-01, mean val. rec. loss:  2.24977908e-01\n",
      "Epoch: 277 mean train loss:  2.27395093e-01, mean val. rec. loss:  2.24415492e-01\n",
      "Epoch: 278 mean train loss:  2.26833888e-01, mean val. rec. loss:  2.23862166e-01\n",
      "Epoch: 279 mean train loss:  2.26281786e-01, mean val. rec. loss:  2.23317730e-01\n",
      "Epoch: 280 mean train loss:  2.25738607e-01, mean val. rec. loss:  2.22782094e-01\n",
      "Epoch: 281 mean train loss:  2.25204203e-01, mean val. rec. loss:  2.22255058e-01\n",
      "Epoch: 282 mean train loss:  2.24678425e-01, mean val. rec. loss:  2.21736477e-01\n",
      "Epoch: 283 mean train loss:  2.24161079e-01, mean val. rec. loss:  2.21226169e-01\n",
      "Epoch: 284 mean train loss:  2.23652030e-01, mean val. rec. loss:  2.20724026e-01\n",
      "Epoch: 285 mean train loss:  2.23151116e-01, mean val. rec. loss:  2.20229867e-01\n",
      "Epoch: 286 mean train loss:  2.22658216e-01, mean val. rec. loss:  2.19743563e-01\n",
      "Epoch: 287 mean train loss:  2.22173153e-01, mean val. rec. loss:  2.19264934e-01\n",
      "Epoch: 288 mean train loss:  2.21695777e-01, mean val. rec. loss:  2.18793871e-01\n",
      "Epoch: 289 mean train loss:  2.21225998e-01, mean val. rec. loss:  2.18330211e-01\n",
      "Epoch: 290 mean train loss:  2.20763579e-01, mean val. rec. loss:  2.17873881e-01\n",
      "Epoch: 291 mean train loss:  2.20308519e-01, mean val. rec. loss:  2.17424663e-01\n",
      "Epoch: 292 mean train loss:  2.19860551e-01, mean val. rec. loss:  2.16982430e-01\n",
      "Epoch: 293 mean train loss:  2.19419599e-01, mean val. rec. loss:  2.16547074e-01\n",
      "Epoch: 294 mean train loss:  2.18985500e-01, mean val. rec. loss:  2.16118449e-01\n",
      "Epoch: 295 mean train loss:  2.18558165e-01, mean val. rec. loss:  2.15696482e-01\n",
      "Epoch: 296 mean train loss:  2.18137444e-01, mean val. rec. loss:  2.15280975e-01\n",
      "Epoch: 297 mean train loss:  2.17723233e-01, mean val. rec. loss:  2.14871836e-01\n",
      "Epoch: 298 mean train loss:  2.17315354e-01, mean val. rec. loss:  2.14468939e-01\n",
      "Epoch: 299 mean train loss:  2.16913746e-01, mean val. rec. loss:  2.14072156e-01\n",
      "Epoch: 300 mean train loss:  2.16518232e-01, mean val. rec. loss:  2.13681397e-01\n",
      "Epoch: 301 mean train loss:  2.16128751e-01, mean val. rec. loss:  2.13296480e-01\n",
      "Epoch: 302 mean train loss:  2.15745140e-01, mean val. rec. loss:  2.12917369e-01\n",
      "Epoch: 303 mean train loss:  2.15367294e-01, mean val. rec. loss:  2.12543955e-01\n",
      "Epoch: 304 mean train loss:  2.14995154e-01, mean val. rec. loss:  2.12176038e-01\n",
      "Epoch: 305 mean train loss:  2.14628540e-01, mean val. rec. loss:  2.11813547e-01\n",
      "Epoch: 306 mean train loss:  2.14267350e-01, mean val. rec. loss:  2.11456444e-01\n",
      "Epoch: 307 mean train loss:  2.13911508e-01, mean val. rec. loss:  2.11104512e-01\n",
      "Epoch: 308 mean train loss:  2.13560865e-01, mean val. rec. loss:  2.10757733e-01\n",
      "Epoch: 309 mean train loss:  2.13215391e-01, mean val. rec. loss:  2.10415980e-01\n",
      "Epoch: 310 mean train loss:  2.12874923e-01, mean val. rec. loss:  2.10079161e-01\n",
      "Epoch: 311 mean train loss:  2.12539386e-01, mean val. rec. loss:  2.09747133e-01\n",
      "Epoch: 312 mean train loss:  2.12208646e-01, mean val. rec. loss:  2.09419822e-01\n",
      "Epoch: 313 mean train loss:  2.11882659e-01, mean val. rec. loss:  2.09097192e-01\n",
      "Epoch: 314 mean train loss:  2.11561289e-01, mean val. rec. loss:  2.08779044e-01\n",
      "Epoch: 315 mean train loss:  2.11244449e-01, mean val. rec. loss:  2.08465395e-01\n",
      "Epoch: 316 mean train loss:  2.10932107e-01, mean val. rec. loss:  2.08156100e-01\n",
      "Epoch: 317 mean train loss:  2.10624101e-01, mean val. rec. loss:  2.07851052e-01\n",
      "Epoch: 318 mean train loss:  2.10320325e-01, mean val. rec. loss:  2.07550158e-01\n",
      "Epoch: 319 mean train loss:  2.10020766e-01, mean val. rec. loss:  2.07253419e-01\n",
      "Epoch: 320 mean train loss:  2.09725318e-01, mean val. rec. loss:  2.06960635e-01\n",
      "Epoch: 321 mean train loss:  2.09433833e-01, mean val. rec. loss:  2.06671807e-01\n",
      "Epoch: 322 mean train loss:  2.09146296e-01, mean val. rec. loss:  2.06386861e-01\n",
      "Epoch: 323 mean train loss:  2.08862662e-01, mean val. rec. loss:  2.06105671e-01\n",
      "Epoch: 324 mean train loss:  2.08582768e-01, mean val. rec. loss:  2.05828164e-01\n",
      "Epoch: 325 mean train loss:  2.08306567e-01, mean val. rec. loss:  2.05554268e-01\n",
      "Epoch: 326 mean train loss:  2.08033973e-01, mean val. rec. loss:  2.05283928e-01\n",
      "Epoch: 327 mean train loss:  2.07764923e-01, mean val. rec. loss:  2.05017072e-01\n",
      "Epoch: 328 mean train loss:  2.07499360e-01, mean val. rec. loss:  2.04753572e-01\n",
      "Epoch: 329 mean train loss:  2.07237178e-01, mean val. rec. loss:  2.04493429e-01\n",
      "Epoch: 330 mean train loss:  2.06978334e-01, mean val. rec. loss:  2.04236551e-01\n",
      "Epoch: 331 mean train loss:  2.06722752e-01, mean val. rec. loss:  2.03982812e-01\n",
      "Epoch: 332 mean train loss:  2.06470328e-01, mean val. rec. loss:  2.03732249e-01\n",
      "Epoch: 333 mean train loss:  2.06221063e-01, mean val. rec. loss:  2.03484716e-01\n",
      "Epoch: 334 mean train loss:  2.05974837e-01, mean val. rec. loss:  2.03240158e-01\n",
      "Epoch: 335 mean train loss:  2.05731590e-01, mean val. rec. loss:  2.02998539e-01\n",
      "Epoch: 336 mean train loss:  2.05491278e-01, mean val. rec. loss:  2.02759805e-01\n",
      "Epoch: 337 mean train loss:  2.05253856e-01, mean val. rec. loss:  2.02523865e-01\n",
      "Epoch: 338 mean train loss:  2.05019205e-01, mean val. rec. loss:  2.02290684e-01\n",
      "Epoch: 339 mean train loss:  2.04787325e-01, mean val. rec. loss:  2.02060205e-01\n",
      "Epoch: 340 mean train loss:  2.04558142e-01, mean val. rec. loss:  2.01832303e-01\n",
      "Epoch: 341 mean train loss:  2.04331550e-01, mean val. rec. loss:  2.01606995e-01\n",
      "Epoch: 342 mean train loss:  2.04107551e-01, mean val. rec. loss:  2.01384228e-01\n",
      "Epoch: 343 mean train loss:  2.03886085e-01, mean val. rec. loss:  2.01163946e-01\n",
      "Epoch: 344 mean train loss:  2.03667091e-01, mean val. rec. loss:  2.00946023e-01\n",
      "Epoch: 345 mean train loss:  2.03450481e-01, mean val. rec. loss:  2.00730513e-01\n",
      "Epoch: 346 mean train loss:  2.03236255e-01, mean val. rec. loss:  2.00517289e-01\n",
      "Epoch: 347 mean train loss:  2.03024352e-01, mean val. rec. loss:  2.00306315e-01\n",
      "Epoch: 348 mean train loss:  2.02814685e-01, mean val. rec. loss:  2.00097591e-01\n",
      "Epoch: 349 mean train loss:  2.02607251e-01, mean val. rec. loss:  1.99891026e-01\n",
      "Epoch: 350 mean train loss:  2.02401978e-01, mean val. rec. loss:  1.99686602e-01\n",
      "Epoch: 351 mean train loss:  2.02198836e-01, mean val. rec. loss:  1.99484191e-01\n",
      "Epoch: 352 mean train loss:  2.01997764e-01, mean val. rec. loss:  1.99283849e-01\n",
      "Epoch: 353 mean train loss:  2.01798703e-01, mean val. rec. loss:  1.99085485e-01\n",
      "Epoch: 354 mean train loss:  2.01601639e-01, mean val. rec. loss:  1.98889062e-01\n",
      "Epoch: 355 mean train loss:  2.01406541e-01, mean val. rec. loss:  1.98694563e-01\n",
      "Epoch: 356 mean train loss:  2.01213320e-01, mean val. rec. loss:  1.98501895e-01\n",
      "Epoch: 357 mean train loss:  2.01021931e-01, mean val. rec. loss:  1.98311061e-01\n",
      "Epoch: 358 mean train loss:  2.00832390e-01, mean val. rec. loss:  1.98121968e-01\n",
      "Epoch: 359 mean train loss:  2.00644622e-01, mean val. rec. loss:  1.97934653e-01\n",
      "Epoch: 360 mean train loss:  2.00458611e-01, mean val. rec. loss:  1.97749062e-01\n",
      "Epoch: 361 mean train loss:  2.00274299e-01, mean val. rec. loss:  1.97565122e-01\n",
      "Epoch: 362 mean train loss:  2.00091656e-01, mean val. rec. loss:  1.97382797e-01\n",
      "Epoch: 363 mean train loss:  1.99910651e-01, mean val. rec. loss:  1.97202086e-01\n",
      "Epoch: 364 mean train loss:  1.99731225e-01, mean val. rec. loss:  1.97022936e-01\n",
      "Epoch: 365 mean train loss:  1.99553378e-01, mean val. rec. loss:  1.96845328e-01\n",
      "Epoch: 366 mean train loss:  1.99377036e-01, mean val. rec. loss:  1.96669208e-01\n",
      "Epoch: 367 mean train loss:  1.99202214e-01, mean val. rec. loss:  1.96494539e-01\n",
      "Epoch: 368 mean train loss:  1.99028836e-01, mean val. rec. loss:  1.96321322e-01\n",
      "Epoch: 369 mean train loss:  1.98856919e-01, mean val. rec. loss:  1.96149484e-01\n",
      "Epoch: 370 mean train loss:  1.98686372e-01, mean val. rec. loss:  1.95979061e-01\n",
      "Epoch: 371 mean train loss:  1.98517226e-01, mean val. rec. loss:  1.95809926e-01\n",
      "Epoch: 372 mean train loss:  1.98349405e-01, mean val. rec. loss:  1.95642152e-01\n",
      "Epoch: 373 mean train loss:  1.98182895e-01, mean val. rec. loss:  1.95475611e-01\n",
      "Epoch: 374 mean train loss:  1.98017652e-01, mean val. rec. loss:  1.95310341e-01\n",
      "Epoch: 375 mean train loss:  1.97853690e-01, mean val. rec. loss:  1.95146304e-01\n",
      "Epoch: 376 mean train loss:  1.97690934e-01, mean val. rec. loss:  1.94983483e-01\n",
      "Epoch: 377 mean train loss:  1.97529386e-01, mean val. rec. loss:  1.94821824e-01\n",
      "Epoch: 378 mean train loss:  1.97369014e-01, mean val. rec. loss:  1.94661343e-01\n",
      "Epoch: 379 mean train loss:  1.97209819e-01, mean val. rec. loss:  1.94501988e-01\n",
      "Epoch: 380 mean train loss:  1.97051726e-01, mean val. rec. loss:  1.94343703e-01\n",
      "Epoch: 381 mean train loss:  1.96894766e-01, mean val. rec. loss:  1.94186543e-01\n",
      "Epoch: 382 mean train loss:  1.96738878e-01, mean val. rec. loss:  1.94030435e-01\n",
      "Epoch: 383 mean train loss:  1.96584033e-01, mean val. rec. loss:  1.93875343e-01\n",
      "Epoch: 384 mean train loss:  1.96430217e-01, mean val. rec. loss:  1.93721268e-01\n",
      "Epoch: 385 mean train loss:  1.96277428e-01, mean val. rec. loss:  1.93568172e-01\n",
      "Epoch: 386 mean train loss:  1.96125607e-01, mean val. rec. loss:  1.93416074e-01\n",
      "Epoch: 387 mean train loss:  1.95974784e-01, mean val. rec. loss:  1.93264919e-01\n",
      "Epoch: 388 mean train loss:  1.95824900e-01, mean val. rec. loss:  1.93114654e-01\n",
      "Epoch: 389 mean train loss:  1.95675940e-01, mean val. rec. loss:  1.92965332e-01\n",
      "Epoch: 390 mean train loss:  1.95527888e-01, mean val. rec. loss:  1.92816881e-01\n",
      "Epoch: 391 mean train loss:  1.95380731e-01, mean val. rec. loss:  1.92669337e-01\n",
      "Epoch: 392 mean train loss:  1.95234437e-01, mean val. rec. loss:  1.92522627e-01\n",
      "Epoch: 393 mean train loss:  1.95089007e-01, mean val. rec. loss:  1.92376753e-01\n",
      "Epoch: 394 mean train loss:  1.94944412e-01, mean val. rec. loss:  1.92231712e-01\n",
      "Epoch: 395 mean train loss:  1.94800636e-01, mean val. rec. loss:  1.92087471e-01\n",
      "Epoch: 396 mean train loss:  1.94657664e-01, mean val. rec. loss:  1.91944027e-01\n",
      "Epoch: 397 mean train loss:  1.94515482e-01, mean val. rec. loss:  1.91801309e-01\n",
      "Epoch: 398 mean train loss:  1.94374045e-01, mean val. rec. loss:  1.91659390e-01\n",
      "Epoch: 399 mean train loss:  1.94233383e-01, mean val. rec. loss:  1.91518196e-01\n",
      "Epoch: 400 mean train loss:  1.94093465e-01, mean val. rec. loss:  1.91377710e-01\n",
      "Epoch: 401 mean train loss:  1.93954263e-01, mean val. rec. loss:  1.91237950e-01\n",
      "Epoch: 402 mean train loss:  1.93815746e-01, mean val. rec. loss:  1.91098843e-01\n",
      "Epoch: 403 mean train loss:  1.93677914e-01, mean val. rec. loss:  1.90960461e-01\n",
      "Epoch: 404 mean train loss:  1.93540797e-01, mean val. rec. loss:  1.90822715e-01\n",
      "Epoch: 405 mean train loss:  1.93404306e-01, mean val. rec. loss:  1.90685640e-01\n",
      "Epoch: 406 mean train loss:  1.93268500e-01, mean val. rec. loss:  1.90549200e-01\n",
      "Epoch: 407 mean train loss:  1.93133320e-01, mean val. rec. loss:  1.90413376e-01\n",
      "Epoch: 408 mean train loss:  1.92998766e-01, mean val. rec. loss:  1.90278152e-01\n",
      "Epoch: 409 mean train loss:  1.92864792e-01, mean val. rec. loss:  1.90143526e-01\n",
      "Epoch: 410 mean train loss:  1.92731444e-01, mean val. rec. loss:  1.90009517e-01\n",
      "Epoch: 411 mean train loss:  1.92598693e-01, mean val. rec. loss:  1.89876089e-01\n",
      "Epoch: 412 mean train loss:  1.92466507e-01, mean val. rec. loss:  1.89743205e-01\n",
      "Epoch: 413 mean train loss:  1.92334872e-01, mean val. rec. loss:  1.89610848e-01\n",
      "Epoch: 414 mean train loss:  1.92203804e-01, mean val. rec. loss:  1.89479089e-01\n",
      "Epoch: 415 mean train loss:  1.92073302e-01, mean val. rec. loss:  1.89347856e-01\n",
      "Epoch: 416 mean train loss:  1.91943306e-01, mean val. rec. loss:  1.89217149e-01\n",
      "Epoch: 417 mean train loss:  1.91813846e-01, mean val. rec. loss:  1.89086914e-01\n",
      "Epoch: 418 mean train loss:  1.91684893e-01, mean val. rec. loss:  1.88957223e-01\n",
      "Epoch: 419 mean train loss:  1.91556417e-01, mean val. rec. loss:  1.88827968e-01\n",
      "Epoch: 420 mean train loss:  1.91428462e-01, mean val. rec. loss:  1.88699221e-01\n",
      "Epoch: 421 mean train loss:  1.91300969e-01, mean val. rec. loss:  1.88570946e-01\n",
      "Epoch: 422 mean train loss:  1.91173938e-01, mean val. rec. loss:  1.88443160e-01\n",
      "Epoch: 423 mean train loss:  1.91047398e-01, mean val. rec. loss:  1.88315810e-01\n",
      "Epoch: 424 mean train loss:  1.90921291e-01, mean val. rec. loss:  1.88188895e-01\n",
      "Epoch: 425 mean train loss:  1.90795645e-01, mean val. rec. loss:  1.88062416e-01\n",
      "Epoch: 426 mean train loss:  1.90670402e-01, mean val. rec. loss:  1.87936390e-01\n",
      "Epoch: 427 mean train loss:  1.90545620e-01, mean val. rec. loss:  1.87810764e-01\n",
      "Epoch: 428 mean train loss:  1.90421256e-01, mean val. rec. loss:  1.87685537e-01\n",
      "Epoch: 429 mean train loss:  1.90297278e-01, mean val. rec. loss:  1.87560745e-01\n",
      "Epoch: 430 mean train loss:  1.90173718e-01, mean val. rec. loss:  1.87436316e-01\n",
      "Epoch: 431 mean train loss:  1.90050575e-01, mean val. rec. loss:  1.87312304e-01\n",
      "Epoch: 432 mean train loss:  1.89927790e-01, mean val. rec. loss:  1.87188655e-01\n",
      "Epoch: 433 mean train loss:  1.89805392e-01, mean val. rec. loss:  1.87065388e-01\n",
      "Epoch: 434 mean train loss:  1.89683381e-01, mean val. rec. loss:  1.86942501e-01\n",
      "Epoch: 435 mean train loss:  1.89561698e-01, mean val. rec. loss:  1.86819959e-01\n",
      "Epoch: 436 mean train loss:  1.89440417e-01, mean val. rec. loss:  1.86697761e-01\n",
      "Epoch: 437 mean train loss:  1.89319494e-01, mean val. rec. loss:  1.86575945e-01\n",
      "Epoch: 438 mean train loss:  1.89198884e-01, mean val. rec. loss:  1.86454419e-01\n",
      "Epoch: 439 mean train loss:  1.89078631e-01, mean val. rec. loss:  1.86333274e-01\n",
      "Epoch: 440 mean train loss:  1.88958706e-01, mean val. rec. loss:  1.86212419e-01\n",
      "Epoch: 441 mean train loss:  1.88839124e-01, mean val. rec. loss:  1.86091910e-01\n",
      "Epoch: 442 mean train loss:  1.88719839e-01, mean val. rec. loss:  1.85971690e-01\n",
      "Epoch: 443 mean train loss:  1.88600867e-01, mean val. rec. loss:  1.85851815e-01\n",
      "Epoch: 444 mean train loss:  1.88482224e-01, mean val. rec. loss:  1.85732194e-01\n",
      "Epoch: 445 mean train loss:  1.88363863e-01, mean val. rec. loss:  1.85612918e-01\n",
      "Epoch: 446 mean train loss:  1.88245830e-01, mean val. rec. loss:  1.85493914e-01\n",
      "Epoch: 447 mean train loss:  1.88128050e-01, mean val. rec. loss:  1.85375236e-01\n",
      "Epoch: 448 mean train loss:  1.88010598e-01, mean val. rec. loss:  1.85256777e-01\n",
      "Epoch: 449 mean train loss:  1.87893384e-01, mean val. rec. loss:  1.85138625e-01\n",
      "Epoch: 450 mean train loss:  1.87776483e-01, mean val. rec. loss:  1.85020728e-01\n",
      "Epoch: 451 mean train loss:  1.87659836e-01, mean val. rec. loss:  1.84903085e-01\n",
      "Epoch: 452 mean train loss:  1.87543441e-01, mean val. rec. loss:  1.84785732e-01\n",
      "Epoch: 453 mean train loss:  1.87427315e-01, mean val. rec. loss:  1.84668615e-01\n",
      "Epoch: 454 mean train loss:  1.87311457e-01, mean val. rec. loss:  1.84551770e-01\n",
      "Epoch: 455 mean train loss:  1.87195822e-01, mean val. rec. loss:  1.84435179e-01\n",
      "Epoch: 456 mean train loss:  1.87080471e-01, mean val. rec. loss:  1.84318806e-01\n",
      "Epoch: 457 mean train loss:  1.86965328e-01, mean val. rec. loss:  1.84202650e-01\n",
      "Epoch: 458 mean train loss:  1.86850438e-01, mean val. rec. loss:  1.84086730e-01\n",
      "Epoch: 459 mean train loss:  1.86735787e-01, mean val. rec. loss:  1.83971065e-01\n",
      "Epoch: 460 mean train loss:  1.86621344e-01, mean val. rec. loss:  1.83855635e-01\n",
      "Epoch: 461 mean train loss:  1.86507140e-01, mean val. rec. loss:  1.83740387e-01\n",
      "Epoch: 462 mean train loss:  1.86393129e-01, mean val. rec. loss:  1.83625374e-01\n",
      "Epoch: 463 mean train loss:  1.86279356e-01, mean val. rec. loss:  1.83510580e-01\n",
      "Epoch: 464 mean train loss:  1.86165793e-01, mean val. rec. loss:  1.83395984e-01\n",
      "Epoch: 465 mean train loss:  1.86052437e-01, mean val. rec. loss:  1.83281607e-01\n",
      "Epoch: 466 mean train loss:  1.85939291e-01, mean val. rec. loss:  1.83167411e-01\n",
      "Epoch: 467 mean train loss:  1.85826353e-01, mean val. rec. loss:  1.83053397e-01\n",
      "Epoch: 468 mean train loss:  1.85713578e-01, mean val. rec. loss:  1.82939600e-01\n",
      "Epoch: 469 mean train loss:  1.85601013e-01, mean val. rec. loss:  1.82825966e-01\n",
      "Epoch: 470 mean train loss:  1.85488626e-01, mean val. rec. loss:  1.82712550e-01\n",
      "Epoch: 471 mean train loss:  1.85376447e-01, mean val. rec. loss:  1.82599298e-01\n",
      "Epoch: 472 mean train loss:  1.85264448e-01, mean val. rec. loss:  1.82486227e-01\n",
      "Epoch: 473 mean train loss:  1.85152612e-01, mean val. rec. loss:  1.82373337e-01\n",
      "Epoch: 474 mean train loss:  1.85040955e-01, mean val. rec. loss:  1.82260593e-01\n",
      "Epoch: 475 mean train loss:  1.84929462e-01, mean val. rec. loss:  1.82147993e-01\n",
      "Epoch: 476 mean train loss:  1.84818118e-01, mean val. rec. loss:  1.82035630e-01\n",
      "Epoch: 477 mean train loss:  1.84706982e-01, mean val. rec. loss:  1.81923393e-01\n",
      "Epoch: 478 mean train loss:  1.84595996e-01, mean val. rec. loss:  1.81811320e-01\n",
      "Epoch: 479 mean train loss:  1.84485158e-01, mean val. rec. loss:  1.81699392e-01\n",
      "Epoch: 480 mean train loss:  1.84374455e-01, mean val. rec. loss:  1.81587609e-01\n",
      "Epoch: 481 mean train loss:  1.84263930e-01, mean val. rec. loss:  1.81475953e-01\n",
      "Epoch: 482 mean train loss:  1.84153510e-01, mean val. rec. loss:  1.81364479e-01\n",
      "Epoch: 483 mean train loss:  1.84043283e-01, mean val. rec. loss:  1.81253095e-01\n",
      "Epoch: 484 mean train loss:  1.83933175e-01, mean val. rec. loss:  1.81141893e-01\n",
      "Epoch: 485 mean train loss:  1.83823202e-01, mean val. rec. loss:  1.81030818e-01\n",
      "Epoch: 486 mean train loss:  1.83713377e-01, mean val. rec. loss:  1.80919888e-01\n",
      "Epoch: 487 mean train loss:  1.83603672e-01, mean val. rec. loss:  1.80809066e-01\n",
      "Epoch: 488 mean train loss:  1.83494100e-01, mean val. rec. loss:  1.80698372e-01\n",
      "Epoch: 489 mean train loss:  1.83384648e-01, mean val. rec. loss:  1.80587787e-01\n",
      "Epoch: 490 mean train loss:  1.83275330e-01, mean val. rec. loss:  1.80477346e-01\n",
      "Epoch: 491 mean train loss:  1.83166131e-01, mean val. rec. loss:  1.80367015e-01\n",
      "Epoch: 492 mean train loss:  1.83057037e-01, mean val. rec. loss:  1.80256793e-01\n",
      "Epoch: 493 mean train loss:  1.82948076e-01, mean val. rec. loss:  1.80146679e-01\n",
      "Epoch: 494 mean train loss:  1.82839220e-01, mean val. rec. loss:  1.80036674e-01\n",
      "Epoch: 495 mean train loss:  1.82730468e-01, mean val. rec. loss:  1.79926778e-01\n",
      "Epoch: 496 mean train loss:  1.82621836e-01, mean val. rec. loss:  1.79816991e-01\n",
      "Epoch: 497 mean train loss:  1.82513292e-01, mean val. rec. loss:  1.79707295e-01\n",
      "Epoch: 498 mean train loss:  1.82404838e-01, mean val. rec. loss:  1.79597707e-01\n",
      "Epoch: 499 mean train loss:  1.82296503e-01, mean val. rec. loss:  1.79488210e-01\n",
      "Epoch: 500 mean train loss:  1.82188258e-01, mean val. rec. loss:  1.79378804e-01\n",
      "Epoch: 501 mean train loss:  1.82080117e-01, mean val. rec. loss:  1.79269471e-01\n",
      "Epoch: 502 mean train loss:  1.81972050e-01, mean val. rec. loss:  1.79160246e-01\n",
      "Epoch: 503 mean train loss:  1.81864073e-01, mean val. rec. loss:  1.79051112e-01\n",
      "Epoch: 504 mean train loss:  1.81756200e-01, mean val. rec. loss:  1.78942033e-01\n",
      "Epoch: 505 mean train loss:  1.81648402e-01, mean val. rec. loss:  1.78833044e-01\n",
      "Epoch: 506 mean train loss:  1.81540663e-01, mean val. rec. loss:  1.78724146e-01\n",
      "Epoch: 507 mean train loss:  1.81433043e-01, mean val. rec. loss:  1.78615321e-01\n",
      "Epoch: 508 mean train loss:  1.81325453e-01, mean val. rec. loss:  1.78506531e-01\n",
      "Epoch: 509 mean train loss:  1.81217952e-01, mean val. rec. loss:  1.78397869e-01\n",
      "Epoch: 510 mean train loss:  1.81110541e-01, mean val. rec. loss:  1.78289261e-01\n",
      "Epoch: 511 mean train loss:  1.81003190e-01, mean val. rec. loss:  1.78180672e-01\n",
      "Epoch: 512 mean train loss:  1.80895883e-01, mean val. rec. loss:  1.78072191e-01\n",
      "Epoch: 513 mean train loss:  1.80788665e-01, mean val. rec. loss:  1.77963747e-01\n",
      "Epoch: 514 mean train loss:  1.80681508e-01, mean val. rec. loss:  1.77855375e-01\n",
      "Epoch: 515 mean train loss:  1.80574394e-01, mean val. rec. loss:  1.77747057e-01\n",
      "Epoch: 516 mean train loss:  1.80467326e-01, mean val. rec. loss:  1.77638794e-01\n",
      "Epoch: 517 mean train loss:  1.80360362e-01, mean val. rec. loss:  1.77530586e-01\n",
      "Epoch: 518 mean train loss:  1.80253397e-01, mean val. rec. loss:  1.77422395e-01\n",
      "Epoch: 519 mean train loss:  1.80146508e-01, mean val. rec. loss:  1.77314277e-01\n",
      "Epoch: 520 mean train loss:  1.80039648e-01, mean val. rec. loss:  1.77206178e-01\n",
      "Epoch: 521 mean train loss:  1.79932847e-01, mean val. rec. loss:  1.77098132e-01\n",
      "Epoch: 522 mean train loss:  1.79826077e-01, mean val. rec. loss:  1.76990160e-01\n",
      "Epoch: 523 mean train loss:  1.79719366e-01, mean val. rec. loss:  1.76882169e-01\n",
      "Epoch: 524 mean train loss:  1.79612685e-01, mean val. rec. loss:  1.76774251e-01\n",
      "Epoch: 525 mean train loss:  1.79506048e-01, mean val. rec. loss:  1.76666369e-01\n",
      "Epoch: 526 mean train loss:  1.79399441e-01, mean val. rec. loss:  1.76558487e-01\n",
      "Epoch: 527 mean train loss:  1.79292865e-01, mean val. rec. loss:  1.76450641e-01\n",
      "Epoch: 528 mean train loss:  1.79186303e-01, mean val. rec. loss:  1.76342850e-01\n",
      "Epoch: 529 mean train loss:  1.79079800e-01, mean val. rec. loss:  1.76235058e-01\n",
      "Epoch: 530 mean train loss:  1.78973313e-01, mean val. rec. loss:  1.76127322e-01\n",
      "Epoch: 531 mean train loss:  1.78866855e-01, mean val. rec. loss:  1.76019567e-01\n",
      "Epoch: 532 mean train loss:  1.78760397e-01, mean val. rec. loss:  1.75911848e-01\n",
      "Epoch: 533 mean train loss:  1.78653969e-01, mean val. rec. loss:  1.75804111e-01\n",
      "Epoch: 534 mean train loss:  1.78547541e-01, mean val. rec. loss:  1.75696429e-01\n",
      "Epoch: 535 mean train loss:  1.78441143e-01, mean val. rec. loss:  1.75588728e-01\n",
      "Epoch: 536 mean train loss:  1.78334760e-01, mean val. rec. loss:  1.75481046e-01\n",
      "Epoch: 537 mean train loss:  1.78228377e-01, mean val. rec. loss:  1.75373363e-01\n",
      "Epoch: 538 mean train loss:  1.78122009e-01, mean val. rec. loss:  1.75265681e-01\n",
      "Epoch: 539 mean train loss:  1.78015655e-01, mean val. rec. loss:  1.75158016e-01\n",
      "Epoch: 540 mean train loss:  1.77909272e-01, mean val. rec. loss:  1.75050352e-01\n",
      "Epoch: 541 mean train loss:  1.77802933e-01, mean val. rec. loss:  1.74942724e-01\n",
      "Epoch: 542 mean train loss:  1.77696595e-01, mean val. rec. loss:  1.74835024e-01\n",
      "Epoch: 543 mean train loss:  1.77590227e-01, mean val. rec. loss:  1.74727359e-01\n",
      "Epoch: 544 mean train loss:  1.77483858e-01, mean val. rec. loss:  1.74619659e-01\n",
      "Epoch: 545 mean train loss:  1.77377505e-01, mean val. rec. loss:  1.74511976e-01\n",
      "Epoch: 546 mean train loss:  1.77271122e-01, mean val. rec. loss:  1.74404258e-01\n",
      "Epoch: 547 mean train loss:  1.77164753e-01, mean val. rec. loss:  1.74296521e-01\n",
      "Epoch: 548 mean train loss:  1.77058340e-01, mean val. rec. loss:  1.74188802e-01\n",
      "Epoch: 549 mean train loss:  1.76951927e-01, mean val. rec. loss:  1.74081029e-01\n",
      "Epoch: 550 mean train loss:  1.76845499e-01, mean val. rec. loss:  1.73973256e-01\n",
      "Epoch: 551 mean train loss:  1.76739057e-01, mean val. rec. loss:  1.73865465e-01\n",
      "Epoch: 552 mean train loss:  1.76632584e-01, mean val. rec. loss:  1.73757655e-01\n",
      "Epoch: 553 mean train loss:  1.76526096e-01, mean val. rec. loss:  1.73649773e-01\n",
      "Epoch: 554 mean train loss:  1.76419564e-01, mean val. rec. loss:  1.73541891e-01\n",
      "Epoch: 555 mean train loss:  1.76313047e-01, mean val. rec. loss:  1.73433973e-01\n",
      "Epoch: 556 mean train loss:  1.76206485e-01, mean val. rec. loss:  1.73326036e-01\n",
      "Epoch: 557 mean train loss:  1.76099863e-01, mean val. rec. loss:  1.73218046e-01\n",
      "Epoch: 558 mean train loss:  1.75993242e-01, mean val. rec. loss:  1.73110055e-01\n",
      "Epoch: 559 mean train loss:  1.75886575e-01, mean val. rec. loss:  1.73001991e-01\n",
      "Epoch: 560 mean train loss:  1.75779879e-01, mean val. rec. loss:  1.72893892e-01\n",
      "Epoch: 561 mean train loss:  1.75673124e-01, mean val. rec. loss:  1.72785737e-01\n",
      "Epoch: 562 mean train loss:  1.75566338e-01, mean val. rec. loss:  1.72677547e-01\n",
      "Epoch: 563 mean train loss:  1.75459508e-01, mean val. rec. loss:  1.72569320e-01\n",
      "Epoch: 564 mean train loss:  1.75352633e-01, mean val. rec. loss:  1.72461039e-01\n",
      "Epoch: 565 mean train loss:  1.75245744e-01, mean val. rec. loss:  1.72352722e-01\n",
      "Epoch: 566 mean train loss:  1.75138779e-01, mean val. rec. loss:  1.72244332e-01\n",
      "Epoch: 567 mean train loss:  1.75031756e-01, mean val. rec. loss:  1.72135905e-01\n",
      "Epoch: 568 mean train loss:  1.74924687e-01, mean val. rec. loss:  1.72027370e-01\n",
      "Epoch: 569 mean train loss:  1.74817559e-01, mean val. rec. loss:  1.71918799e-01\n",
      "Epoch: 570 mean train loss:  1.74710386e-01, mean val. rec. loss:  1.71810191e-01\n",
      "Epoch: 571 mean train loss:  1.74603124e-01, mean val. rec. loss:  1.71701511e-01\n",
      "Epoch: 572 mean train loss:  1.74495832e-01, mean val. rec. loss:  1.71592776e-01\n",
      "Epoch: 573 mean train loss:  1.74388451e-01, mean val. rec. loss:  1.71483932e-01\n",
      "Epoch: 574 mean train loss:  1.74281025e-01, mean val. rec. loss:  1.71375071e-01\n",
      "Epoch: 575 mean train loss:  1.74173539e-01, mean val. rec. loss:  1.71266100e-01\n",
      "Epoch: 576 mean train loss:  1.74065979e-01, mean val. rec. loss:  1.71157093e-01\n",
      "Epoch: 577 mean train loss:  1.73958359e-01, mean val. rec. loss:  1.71047977e-01\n",
      "Epoch: 578 mean train loss:  1.73850650e-01, mean val. rec. loss:  1.70938807e-01\n",
      "Epoch: 579 mean train loss:  1.73742881e-01, mean val. rec. loss:  1.70829528e-01\n",
      "Epoch: 580 mean train loss:  1.73635038e-01, mean val. rec. loss:  1.70720195e-01\n",
      "Epoch: 581 mean train loss:  1.73527106e-01, mean val. rec. loss:  1.70610770e-01\n",
      "Epoch: 582 mean train loss:  1.73419084e-01, mean val. rec. loss:  1.70501237e-01\n",
      "Epoch: 583 mean train loss:  1.73310972e-01, mean val. rec. loss:  1.70391632e-01\n",
      "Epoch: 584 mean train loss:  1.73202801e-01, mean val. rec. loss:  1.70281953e-01\n",
      "Epoch: 585 mean train loss:  1.73094526e-01, mean val. rec. loss:  1.70172184e-01\n",
      "Epoch: 586 mean train loss:  1.72986177e-01, mean val. rec. loss:  1.70062307e-01\n",
      "Epoch: 587 mean train loss:  1.72877723e-01, mean val. rec. loss:  1.69952320e-01\n",
      "Epoch: 588 mean train loss:  1.72769179e-01, mean val. rec. loss:  1.69842261e-01\n",
      "Epoch: 589 mean train loss:  1.72660546e-01, mean val. rec. loss:  1.69732075e-01\n",
      "Epoch: 590 mean train loss:  1.72551809e-01, mean val. rec. loss:  1.69621816e-01\n",
      "Epoch: 591 mean train loss:  1.72442983e-01, mean val. rec. loss:  1.69511466e-01\n",
      "Epoch: 592 mean train loss:  1.72334052e-01, mean val. rec. loss:  1.69400972e-01\n",
      "Epoch: 593 mean train loss:  1.72225032e-01, mean val. rec. loss:  1.69290386e-01\n",
      "Epoch: 594 mean train loss:  1.72115908e-01, mean val. rec. loss:  1.69179692e-01\n",
      "Epoch: 595 mean train loss:  1.72006664e-01, mean val. rec. loss:  1.69068889e-01\n",
      "Epoch: 596 mean train loss:  1.71897316e-01, mean val. rec. loss:  1.68957977e-01\n",
      "Epoch: 597 mean train loss:  1.71787864e-01, mean val. rec. loss:  1.68846938e-01\n",
      "Epoch: 598 mean train loss:  1.71678293e-01, mean val. rec. loss:  1.68735790e-01\n",
      "Epoch: 599 mean train loss:  1.71568618e-01, mean val. rec. loss:  1.68624497e-01\n",
      "Epoch: 600 mean train loss:  1.71458793e-01, mean val. rec. loss:  1.68513113e-01\n",
      "Epoch: 601 mean train loss:  1.71348879e-01, mean val. rec. loss:  1.68401602e-01\n",
      "Epoch: 602 mean train loss:  1.71238846e-01, mean val. rec. loss:  1.68289965e-01\n",
      "Epoch: 603 mean train loss:  1.71128694e-01, mean val. rec. loss:  1.68178200e-01\n",
      "Epoch: 604 mean train loss:  1.71018422e-01, mean val. rec. loss:  1.68066290e-01\n",
      "Epoch: 605 mean train loss:  1.70908002e-01, mean val. rec. loss:  1.67954271e-01\n",
      "Epoch: 606 mean train loss:  1.70797462e-01, mean val. rec. loss:  1.67842089e-01\n",
      "Epoch: 607 mean train loss:  1.70686803e-01, mean val. rec. loss:  1.67729798e-01\n",
      "Epoch: 608 mean train loss:  1.70575995e-01, mean val. rec. loss:  1.67617344e-01\n",
      "Epoch: 609 mean train loss:  1.70465069e-01, mean val. rec. loss:  1.67504781e-01\n",
      "Epoch: 610 mean train loss:  1.70353993e-01, mean val. rec. loss:  1.67392055e-01\n",
      "Epoch: 611 mean train loss:  1.70242797e-01, mean val. rec. loss:  1.67279220e-01\n",
      "Epoch: 612 mean train loss:  1.70131468e-01, mean val. rec. loss:  1.67166185e-01\n",
      "Epoch: 613 mean train loss:  1.70019975e-01, mean val. rec. loss:  1.67053023e-01\n",
      "Epoch: 614 mean train loss:  1.69908348e-01, mean val. rec. loss:  1.66939716e-01\n",
      "Epoch: 615 mean train loss:  1.69796557e-01, mean val. rec. loss:  1.66826282e-01\n",
      "Epoch: 616 mean train loss:  1.69684647e-01, mean val. rec. loss:  1.66712667e-01\n",
      "Epoch: 617 mean train loss:  1.69572588e-01, mean val. rec. loss:  1.66598906e-01\n",
      "Epoch: 618 mean train loss:  1.69460350e-01, mean val. rec. loss:  1.66484982e-01\n",
      "Epoch: 619 mean train loss:  1.69347963e-01, mean val. rec. loss:  1.66370895e-01\n",
      "Epoch: 620 mean train loss:  1.69235427e-01, mean val. rec. loss:  1.66256645e-01\n",
      "Epoch: 621 mean train loss:  1.69122742e-01, mean val. rec. loss:  1.66142231e-01\n",
      "Epoch: 622 mean train loss:  1.69009878e-01, mean val. rec. loss:  1.66027654e-01\n",
      "Epoch: 623 mean train loss:  1.68896866e-01, mean val. rec. loss:  1.65912896e-01\n",
      "Epoch: 624 mean train loss:  1.68783689e-01, mean val. rec. loss:  1.65797992e-01\n",
      "Epoch: 625 mean train loss:  1.68670349e-01, mean val. rec. loss:  1.65682907e-01\n",
      "Epoch: 626 mean train loss:  1.68556845e-01, mean val. rec. loss:  1.65567659e-01\n",
      "Epoch: 627 mean train loss:  1.68443147e-01, mean val. rec. loss:  1.65452193e-01\n",
      "Epoch: 628 mean train loss:  1.68329300e-01, mean val. rec. loss:  1.65336618e-01\n",
      "Epoch: 629 mean train loss:  1.68215274e-01, mean val. rec. loss:  1.65220789e-01\n",
      "Epoch: 630 mean train loss:  1.68101070e-01, mean val. rec. loss:  1.65104833e-01\n",
      "Epoch: 631 mean train loss:  1.67986687e-01, mean val. rec. loss:  1.64988696e-01\n",
      "Epoch: 632 mean train loss:  1.67872154e-01, mean val. rec. loss:  1.64872323e-01\n",
      "Epoch: 633 mean train loss:  1.67757414e-01, mean val. rec. loss:  1.64755786e-01\n",
      "Epoch: 634 mean train loss:  1.67642494e-01, mean val. rec. loss:  1.64639068e-01\n",
      "Epoch: 635 mean train loss:  1.67527381e-01, mean val. rec. loss:  1.64522151e-01\n",
      "Epoch: 636 mean train loss:  1.67412089e-01, mean val. rec. loss:  1.64405052e-01\n",
      "Epoch: 637 mean train loss:  1.67296604e-01, mean val. rec. loss:  1.64287735e-01\n",
      "Epoch: 638 mean train loss:  1.67180924e-01, mean val. rec. loss:  1.64170237e-01\n",
      "Epoch: 639 mean train loss:  1.67065066e-01, mean val. rec. loss:  1.64052521e-01\n",
      "Epoch: 640 mean train loss:  1.66948985e-01, mean val. rec. loss:  1.63934624e-01\n",
      "Epoch: 641 mean train loss:  1.66832724e-01, mean val. rec. loss:  1.63816509e-01\n",
      "Epoch: 642 mean train loss:  1.66716256e-01, mean val. rec. loss:  1.63698194e-01\n",
      "Epoch: 643 mean train loss:  1.66599578e-01, mean val. rec. loss:  1.63579662e-01\n",
      "Epoch: 644 mean train loss:  1.66482722e-01, mean val. rec. loss:  1.63460912e-01\n",
      "Epoch: 645 mean train loss:  1.66365627e-01, mean val. rec. loss:  1.63341944e-01\n",
      "Epoch: 646 mean train loss:  1.66248354e-01, mean val. rec. loss:  1.63222813e-01\n",
      "Epoch: 647 mean train loss:  1.66130872e-01, mean val. rec. loss:  1.63103428e-01\n",
      "Epoch: 648 mean train loss:  1.66013167e-01, mean val. rec. loss:  1.62983825e-01\n",
      "Epoch: 649 mean train loss:  1.65895253e-01, mean val. rec. loss:  1.62863968e-01\n",
      "Epoch: 650 mean train loss:  1.65777116e-01, mean val. rec. loss:  1.62743930e-01\n",
      "Epoch: 651 mean train loss:  1.65658740e-01, mean val. rec. loss:  1.62623656e-01\n",
      "Epoch: 652 mean train loss:  1.65540171e-01, mean val. rec. loss:  1.62503164e-01\n",
      "Epoch: 653 mean train loss:  1.65421393e-01, mean val. rec. loss:  1.62382455e-01\n",
      "Epoch: 654 mean train loss:  1.65302376e-01, mean val. rec. loss:  1.62261473e-01\n",
      "Epoch: 655 mean train loss:  1.65183122e-01, mean val. rec. loss:  1.62140292e-01\n",
      "Epoch: 656 mean train loss:  1.65063643e-01, mean val. rec. loss:  1.62018856e-01\n",
      "Epoch: 657 mean train loss:  1.64943927e-01, mean val. rec. loss:  1.61897167e-01\n",
      "Epoch: 658 mean train loss:  1.64824002e-01, mean val. rec. loss:  1.61775278e-01\n",
      "Epoch: 659 mean train loss:  1.64703838e-01, mean val. rec. loss:  1.61653135e-01\n",
      "Epoch: 660 mean train loss:  1.64583437e-01, mean val. rec. loss:  1.61530738e-01\n",
      "Epoch: 661 mean train loss:  1.64462782e-01, mean val. rec. loss:  1.61408106e-01\n",
      "Epoch: 662 mean train loss:  1.64341888e-01, mean val. rec. loss:  1.61285219e-01\n",
      "Epoch: 663 mean train loss:  1.64220756e-01, mean val. rec. loss:  1.61162060e-01\n",
      "Epoch: 664 mean train loss:  1.64099386e-01, mean val. rec. loss:  1.61038683e-01\n",
      "Epoch: 665 mean train loss:  1.63977778e-01, mean val. rec. loss:  1.60915053e-01\n",
      "Epoch: 666 mean train loss:  1.63855901e-01, mean val. rec. loss:  1.60791168e-01\n",
      "Epoch: 667 mean train loss:  1.63733771e-01, mean val. rec. loss:  1.60666993e-01\n",
      "Epoch: 668 mean train loss:  1.63611403e-01, mean val. rec. loss:  1.60542582e-01\n",
      "Epoch: 669 mean train loss:  1.63488796e-01, mean val. rec. loss:  1.60417899e-01\n",
      "Epoch: 670 mean train loss:  1.63365922e-01, mean val. rec. loss:  1.60292944e-01\n",
      "Epoch: 671 mean train loss:  1.63242764e-01, mean val. rec. loss:  1.60167753e-01\n",
      "Epoch: 672 mean train loss:  1.63119368e-01, mean val. rec. loss:  1.60042272e-01\n",
      "Epoch: 673 mean train loss:  1.62995718e-01, mean val. rec. loss:  1.59916536e-01\n",
      "Epoch: 674 mean train loss:  1.62871771e-01, mean val. rec. loss:  1.59790511e-01\n",
      "Epoch: 675 mean train loss:  1.62747585e-01, mean val. rec. loss:  1.59664195e-01\n",
      "Epoch: 676 mean train loss:  1.62623131e-01, mean val. rec. loss:  1.59537625e-01\n",
      "Epoch: 677 mean train loss:  1.62498379e-01, mean val. rec. loss:  1.59410765e-01\n",
      "Epoch: 678 mean train loss:  1.62373359e-01, mean val. rec. loss:  1.59283650e-01\n",
      "Epoch: 679 mean train loss:  1.62248086e-01, mean val. rec. loss:  1.59156210e-01\n",
      "Epoch: 680 mean train loss:  1.62122515e-01, mean val. rec. loss:  1.59028515e-01\n",
      "Epoch: 681 mean train loss:  1.61996690e-01, mean val. rec. loss:  1.58900548e-01\n",
      "Epoch: 682 mean train loss:  1.61870568e-01, mean val. rec. loss:  1.58772254e-01\n",
      "Epoch: 683 mean train loss:  1.61744162e-01, mean val. rec. loss:  1.58643689e-01\n",
      "Epoch: 684 mean train loss:  1.61617474e-01, mean val. rec. loss:  1.58514832e-01\n",
      "Epoch: 685 mean train loss:  1.61490502e-01, mean val. rec. loss:  1.58385686e-01\n",
      "Epoch: 686 mean train loss:  1.61363247e-01, mean val. rec. loss:  1.58256250e-01\n",
      "Epoch: 687 mean train loss:  1.61235695e-01, mean val. rec. loss:  1.58126486e-01\n",
      "Epoch: 688 mean train loss:  1.61107844e-01, mean val. rec. loss:  1.57996433e-01\n",
      "Epoch: 689 mean train loss:  1.60979696e-01, mean val. rec. loss:  1.57866089e-01\n",
      "Epoch: 690 mean train loss:  1.60851264e-01, mean val. rec. loss:  1.57735419e-01\n",
      "Epoch: 691 mean train loss:  1.60722520e-01, mean val. rec. loss:  1.57604458e-01\n",
      "Epoch: 692 mean train loss:  1.60593477e-01, mean val. rec. loss:  1.57473171e-01\n",
      "Epoch: 693 mean train loss:  1.60464137e-01, mean val. rec. loss:  1.57341593e-01\n",
      "Epoch: 694 mean train loss:  1.60334499e-01, mean val. rec. loss:  1.57209689e-01\n",
      "Epoch: 695 mean train loss:  1.60204548e-01, mean val. rec. loss:  1.57077476e-01\n",
      "Epoch: 696 mean train loss:  1.60074284e-01, mean val. rec. loss:  1.56944937e-01\n",
      "Epoch: 697 mean train loss:  1.59943692e-01, mean val. rec. loss:  1.56812090e-01\n",
      "Epoch: 698 mean train loss:  1.59812817e-01, mean val. rec. loss:  1.56678897e-01\n",
      "Epoch: 699 mean train loss:  1.59681615e-01, mean val. rec. loss:  1.56545415e-01\n",
      "Epoch: 700 mean train loss:  1.59550100e-01, mean val. rec. loss:  1.56411587e-01\n",
      "Epoch: 701 mean train loss:  1.59418256e-01, mean val. rec. loss:  1.56277433e-01\n",
      "Epoch: 702 mean train loss:  1.59286101e-01, mean val. rec. loss:  1.56142953e-01\n",
      "Epoch: 703 mean train loss:  1.59153617e-01, mean val. rec. loss:  1.56008145e-01\n",
      "Epoch: 704 mean train loss:  1.59020820e-01, mean val. rec. loss:  1.55872975e-01\n",
      "Epoch: 705 mean train loss:  1.58887666e-01, mean val. rec. loss:  1.55737515e-01\n",
      "Epoch: 706 mean train loss:  1.58754214e-01, mean val. rec. loss:  1.55601710e-01\n",
      "Epoch: 707 mean train loss:  1.58620434e-01, mean val. rec. loss:  1.55465542e-01\n",
      "Epoch: 708 mean train loss:  1.58486297e-01, mean val. rec. loss:  1.55329048e-01\n",
      "Epoch: 709 mean train loss:  1.58351847e-01, mean val. rec. loss:  1.55192190e-01\n",
      "Epoch: 710 mean train loss:  1.58217039e-01, mean val. rec. loss:  1.55055006e-01\n",
      "Epoch: 711 mean train loss:  1.58081919e-01, mean val. rec. loss:  1.54917460e-01\n",
      "Epoch: 712 mean train loss:  1.57946441e-01, mean val. rec. loss:  1.54779586e-01\n",
      "Epoch: 713 mean train loss:  1.57810620e-01, mean val. rec. loss:  1.54641350e-01\n",
      "Epoch: 714 mean train loss:  1.57674442e-01, mean val. rec. loss:  1.54502751e-01\n",
      "Epoch: 715 mean train loss:  1.57537951e-01, mean val. rec. loss:  1.54363825e-01\n",
      "Epoch: 716 mean train loss:  1.57401102e-01, mean val. rec. loss:  1.54224518e-01\n",
      "Epoch: 717 mean train loss:  1.57263896e-01, mean val. rec. loss:  1.54084849e-01\n",
      "Epoch: 718 mean train loss:  1.57126332e-01, mean val. rec. loss:  1.53944834e-01\n",
      "Epoch: 719 mean train loss:  1.56988426e-01, mean val. rec. loss:  1.53804439e-01\n",
      "Epoch: 720 mean train loss:  1.56850147e-01, mean val. rec. loss:  1.53663699e-01\n",
      "Epoch: 721 mean train loss:  1.56711525e-01, mean val. rec. loss:  1.53522578e-01\n",
      "Epoch: 722 mean train loss:  1.56572546e-01, mean val. rec. loss:  1.53381094e-01\n",
      "Epoch: 723 mean train loss:  1.56433195e-01, mean val. rec. loss:  1.53239247e-01\n",
      "Epoch: 724 mean train loss:  1.56293486e-01, mean val. rec. loss:  1.53097001e-01\n",
      "Epoch: 725 mean train loss:  1.56153404e-01, mean val. rec. loss:  1.52954410e-01\n",
      "Epoch: 726 mean train loss:  1.56012951e-01, mean val. rec. loss:  1.52811438e-01\n",
      "Epoch: 727 mean train loss:  1.55872154e-01, mean val. rec. loss:  1.52668085e-01\n",
      "Epoch: 728 mean train loss:  1.55730940e-01, mean val. rec. loss:  1.52524334e-01\n",
      "Epoch: 729 mean train loss:  1.55589384e-01, mean val. rec. loss:  1.52380237e-01\n",
      "Epoch: 730 mean train loss:  1.55447470e-01, mean val. rec. loss:  1.52235723e-01\n",
      "Epoch: 731 mean train loss:  1.55305169e-01, mean val. rec. loss:  1.52090810e-01\n",
      "Epoch: 732 mean train loss:  1.55162466e-01, mean val. rec. loss:  1.51945525e-01\n",
      "Epoch: 733 mean train loss:  1.55019390e-01, mean val. rec. loss:  1.51799868e-01\n",
      "Epoch: 734 mean train loss:  1.54875942e-01, mean val. rec. loss:  1.51653811e-01\n",
      "Epoch: 735 mean train loss:  1.54732121e-01, mean val. rec. loss:  1.51507347e-01\n",
      "Epoch: 736 mean train loss:  1.54587898e-01, mean val. rec. loss:  1.51360511e-01\n",
      "Epoch: 737 mean train loss:  1.54443273e-01, mean val. rec. loss:  1.51213266e-01\n",
      "Epoch: 738 mean train loss:  1.54298275e-01, mean val. rec. loss:  1.51065613e-01\n",
      "Epoch: 739 mean train loss:  1.54152890e-01, mean val. rec. loss:  1.50917570e-01\n",
      "Epoch: 740 mean train loss:  1.54007103e-01, mean val. rec. loss:  1.50769137e-01\n",
      "Epoch: 741 mean train loss:  1.53860944e-01, mean val. rec. loss:  1.50620296e-01\n",
      "Epoch: 742 mean train loss:  1.53714367e-01, mean val. rec. loss:  1.50471038e-01\n",
      "Epoch: 743 mean train loss:  1.53567388e-01, mean val. rec. loss:  1.50321371e-01\n",
      "Epoch: 744 mean train loss:  1.53420037e-01, mean val. rec. loss:  1.50171305e-01\n",
      "Epoch: 745 mean train loss:  1.53272253e-01, mean val. rec. loss:  1.50020822e-01\n",
      "Epoch: 746 mean train loss:  1.53124082e-01, mean val. rec. loss:  1.49869939e-01\n",
      "Epoch: 747 mean train loss:  1.52975509e-01, mean val. rec. loss:  1.49718621e-01\n",
      "Epoch: 748 mean train loss:  1.52826519e-01, mean val. rec. loss:  1.49566904e-01\n",
      "Epoch: 749 mean train loss:  1.52677157e-01, mean val. rec. loss:  1.49414770e-01\n",
      "Epoch: 750 mean train loss:  1.52527347e-01, mean val. rec. loss:  1.49262237e-01\n",
      "Epoch: 751 mean train loss:  1.52377150e-01, mean val. rec. loss:  1.49109250e-01\n",
      "Epoch: 752 mean train loss:  1.52226506e-01, mean val. rec. loss:  1.48955863e-01\n",
      "Epoch: 753 mean train loss:  1.52075490e-01, mean val. rec. loss:  1.48802051e-01\n",
      "Epoch: 754 mean train loss:  1.51924042e-01, mean val. rec. loss:  1.48647785e-01\n",
      "Epoch: 755 mean train loss:  1.51772177e-01, mean val. rec. loss:  1.48493137e-01\n",
      "Epoch: 756 mean train loss:  1.51619879e-01, mean val. rec. loss:  1.48338037e-01\n",
      "Epoch: 757 mean train loss:  1.51467180e-01, mean val. rec. loss:  1.48182509e-01\n",
      "Epoch: 758 mean train loss:  1.51314063e-01, mean val. rec. loss:  1.48026547e-01\n",
      "Epoch: 759 mean train loss:  1.51160484e-01, mean val. rec. loss:  1.47870167e-01\n",
      "Epoch: 760 mean train loss:  1.51006548e-01, mean val. rec. loss:  1.47713343e-01\n",
      "Epoch: 761 mean train loss:  1.50852121e-01, mean val. rec. loss:  1.47556083e-01\n",
      "Epoch: 762 mean train loss:  1.50697305e-01, mean val. rec. loss:  1.47398396e-01\n",
      "Epoch: 763 mean train loss:  1.50542073e-01, mean val. rec. loss:  1.47240275e-01\n",
      "Epoch: 764 mean train loss:  1.50386364e-01, mean val. rec. loss:  1.47081681e-01\n",
      "Epoch: 765 mean train loss:  1.50230253e-01, mean val. rec. loss:  1.46922671e-01\n",
      "Epoch: 766 mean train loss:  1.50073695e-01, mean val. rec. loss:  1.46763224e-01\n",
      "Epoch: 767 mean train loss:  1.49916720e-01, mean val. rec. loss:  1.46603325e-01\n",
      "Epoch: 768 mean train loss:  1.49759298e-01, mean val. rec. loss:  1.46442971e-01\n",
      "Epoch: 769 mean train loss:  1.49601414e-01, mean val. rec. loss:  1.46282182e-01\n",
      "Epoch: 770 mean train loss:  1.49443142e-01, mean val. rec. loss:  1.46120940e-01\n",
      "Epoch: 771 mean train loss:  1.49284394e-01, mean val. rec. loss:  1.45959253e-01\n",
      "Epoch: 772 mean train loss:  1.49125214e-01, mean val. rec. loss:  1.45797131e-01\n",
      "Epoch: 773 mean train loss:  1.48965602e-01, mean val. rec. loss:  1.45634528e-01\n",
      "Epoch: 774 mean train loss:  1.48805528e-01, mean val. rec. loss:  1.45471489e-01\n",
      "Epoch: 775 mean train loss:  1.48645023e-01, mean val. rec. loss:  1.45307997e-01\n",
      "Epoch: 776 mean train loss:  1.48484070e-01, mean val. rec. loss:  1.45144051e-01\n",
      "Epoch: 777 mean train loss:  1.48322670e-01, mean val. rec. loss:  1.44979652e-01\n",
      "Epoch: 778 mean train loss:  1.48160823e-01, mean val. rec. loss:  1.44814780e-01\n",
      "Epoch: 779 mean train loss:  1.47998530e-01, mean val. rec. loss:  1.44649456e-01\n",
      "Epoch: 780 mean train loss:  1.47835789e-01, mean val. rec. loss:  1.44483677e-01\n",
      "Epoch: 781 mean train loss:  1.47672587e-01, mean val. rec. loss:  1.44317427e-01\n",
      "Epoch: 782 mean train loss:  1.47508937e-01, mean val. rec. loss:  1.44150715e-01\n",
      "Epoch: 783 mean train loss:  1.47344826e-01, mean val. rec. loss:  1.43983548e-01\n",
      "Epoch: 784 mean train loss:  1.47180253e-01, mean val. rec. loss:  1.43815919e-01\n",
      "Epoch: 785 mean train loss:  1.47015233e-01, mean val. rec. loss:  1.43647837e-01\n",
      "Epoch: 786 mean train loss:  1.46849767e-01, mean val. rec. loss:  1.43479274e-01\n",
      "Epoch: 787 mean train loss:  1.46683838e-01, mean val. rec. loss:  1.43310238e-01\n",
      "Epoch: 788 mean train loss:  1.46517448e-01, mean val. rec. loss:  1.43140759e-01\n",
      "Epoch: 789 mean train loss:  1.46350595e-01, mean val. rec. loss:  1.42970789e-01\n",
      "Epoch: 790 mean train loss:  1.46183296e-01, mean val. rec. loss:  1.42800357e-01\n",
      "Epoch: 791 mean train loss:  1.46015520e-01, mean val. rec. loss:  1.42629453e-01\n",
      "Epoch: 792 mean train loss:  1.45847297e-01, mean val. rec. loss:  1.42458087e-01\n",
      "Epoch: 793 mean train loss:  1.45678583e-01, mean val. rec. loss:  1.42286248e-01\n",
      "Epoch: 794 mean train loss:  1.45509436e-01, mean val. rec. loss:  1.42113911e-01\n",
      "Epoch: 795 mean train loss:  1.45339798e-01, mean val. rec. loss:  1.41941139e-01\n",
      "Epoch: 796 mean train loss:  1.45169728e-01, mean val. rec. loss:  1.41767867e-01\n",
      "Epoch: 797 mean train loss:  1.44999181e-01, mean val. rec. loss:  1.41594142e-01\n",
      "Epoch: 798 mean train loss:  1.44828158e-01, mean val. rec. loss:  1.41419936e-01\n",
      "Epoch: 799 mean train loss:  1.44656657e-01, mean val. rec. loss:  1.41245258e-01\n",
      "Epoch: 800 mean train loss:  1.44484740e-01, mean val. rec. loss:  1.41070081e-01\n",
      "Epoch: 801 mean train loss:  1.44312316e-01, mean val. rec. loss:  1.40894442e-01\n",
      "Epoch: 802 mean train loss:  1.44139415e-01, mean val. rec. loss:  1.40718340e-01\n",
      "Epoch: 803 mean train loss:  1.43966068e-01, mean val. rec. loss:  1.40541748e-01\n",
      "Epoch: 804 mean train loss:  1.43792243e-01, mean val. rec. loss:  1.40364675e-01\n",
      "Epoch: 805 mean train loss:  1.43617942e-01, mean val. rec. loss:  1.40187131e-01\n",
      "Epoch: 806 mean train loss:  1.43443180e-01, mean val. rec. loss:  1.40009115e-01\n",
      "Epoch: 807 mean train loss:  1.43267940e-01, mean val. rec. loss:  1.39830618e-01\n",
      "Epoch: 808 mean train loss:  1.43092239e-01, mean val. rec. loss:  1.39651631e-01\n",
      "Epoch: 809 mean train loss:  1.42916060e-01, mean val. rec. loss:  1.39472181e-01\n",
      "Epoch: 810 mean train loss:  1.42739435e-01, mean val. rec. loss:  1.39292233e-01\n",
      "Epoch: 811 mean train loss:  1.42562304e-01, mean val. rec. loss:  1.39111831e-01\n",
      "Epoch: 812 mean train loss:  1.42384725e-01, mean val. rec. loss:  1.38930930e-01\n",
      "Epoch: 813 mean train loss:  1.42206670e-01, mean val. rec. loss:  1.38749548e-01\n",
      "Epoch: 814 mean train loss:  1.42028123e-01, mean val. rec. loss:  1.38567694e-01\n",
      "Epoch: 815 mean train loss:  1.41849130e-01, mean val. rec. loss:  1.38385351e-01\n",
      "Epoch: 816 mean train loss:  1.41669644e-01, mean val. rec. loss:  1.38202518e-01\n",
      "Epoch: 817 mean train loss:  1.41489697e-01, mean val. rec. loss:  1.38019240e-01\n",
      "Epoch: 818 mean train loss:  1.41309273e-01, mean val. rec. loss:  1.37835463e-01\n",
      "Epoch: 819 mean train loss:  1.41128387e-01, mean val. rec. loss:  1.37651205e-01\n",
      "Epoch: 820 mean train loss:  1.40947010e-01, mean val. rec. loss:  1.37466467e-01\n",
      "Epoch: 821 mean train loss:  1.40765171e-01, mean val. rec. loss:  1.37281248e-01\n",
      "Epoch: 822 mean train loss:  1.40582870e-01, mean val. rec. loss:  1.37095566e-01\n",
      "Epoch: 823 mean train loss:  1.40400092e-01, mean val. rec. loss:  1.36909394e-01\n",
      "Epoch: 824 mean train loss:  1.40216838e-01, mean val. rec. loss:  1.36722742e-01\n",
      "Epoch: 825 mean train loss:  1.40033121e-01, mean val. rec. loss:  1.36535626e-01\n",
      "Epoch: 826 mean train loss:  1.39848928e-01, mean val. rec. loss:  1.36348012e-01\n",
      "Epoch: 827 mean train loss:  1.39664259e-01, mean val. rec. loss:  1.36159944e-01\n",
      "Epoch: 828 mean train loss:  1.39479142e-01, mean val. rec. loss:  1.35971378e-01\n",
      "Epoch: 829 mean train loss:  1.39293549e-01, mean val. rec. loss:  1.35782339e-01\n",
      "Epoch: 830 mean train loss:  1.39107479e-01, mean val. rec. loss:  1.35592838e-01\n",
      "Epoch: 831 mean train loss:  1.38920932e-01, mean val. rec. loss:  1.35402847e-01\n",
      "Epoch: 832 mean train loss:  1.38733938e-01, mean val. rec. loss:  1.35212394e-01\n",
      "Epoch: 833 mean train loss:  1.38546468e-01, mean val. rec. loss:  1.35021450e-01\n",
      "Epoch: 834 mean train loss:  1.38358520e-01, mean val. rec. loss:  1.34830053e-01\n",
      "Epoch: 835 mean train loss:  1.38170126e-01, mean val. rec. loss:  1.34638184e-01\n",
      "Epoch: 836 mean train loss:  1.37981256e-01, mean val. rec. loss:  1.34445844e-01\n",
      "Epoch: 837 mean train loss:  1.37791938e-01, mean val. rec. loss:  1.34253013e-01\n",
      "Epoch: 838 mean train loss:  1.37602143e-01, mean val. rec. loss:  1.34059729e-01\n",
      "Epoch: 839 mean train loss:  1.37411887e-01, mean val. rec. loss:  1.33865991e-01\n",
      "Epoch: 840 mean train loss:  1.37221184e-01, mean val. rec. loss:  1.33671764e-01\n",
      "Epoch: 841 mean train loss:  1.37030004e-01, mean val. rec. loss:  1.33477083e-01\n",
      "Epoch: 842 mean train loss:  1.36838377e-01, mean val. rec. loss:  1.33281930e-01\n",
      "Epoch: 843 mean train loss:  1.36646274e-01, mean val. rec. loss:  1.33086324e-01\n",
      "Epoch: 844 mean train loss:  1.36453753e-01, mean val. rec. loss:  1.32890236e-01\n",
      "Epoch: 845 mean train loss:  1.36260726e-01, mean val. rec. loss:  1.32693705e-01\n",
      "Epoch: 846 mean train loss:  1.36067281e-01, mean val. rec. loss:  1.32496692e-01\n",
      "Epoch: 847 mean train loss:  1.35873360e-01, mean val. rec. loss:  1.32299244e-01\n",
      "Epoch: 848 mean train loss:  1.35678992e-01, mean val. rec. loss:  1.32101342e-01\n",
      "Epoch: 849 mean train loss:  1.35484177e-01, mean val. rec. loss:  1.31902978e-01\n",
      "Epoch: 850 mean train loss:  1.35288945e-01, mean val. rec. loss:  1.31704142e-01\n",
      "Epoch: 851 mean train loss:  1.35093222e-01, mean val. rec. loss:  1.31504871e-01\n",
      "Epoch: 852 mean train loss:  1.34897066e-01, mean val. rec. loss:  1.31305137e-01\n",
      "Epoch: 853 mean train loss:  1.34700463e-01, mean val. rec. loss:  1.31104958e-01\n",
      "Epoch: 854 mean train loss:  1.34503429e-01, mean val. rec. loss:  1.30904353e-01\n",
      "Epoch: 855 mean train loss:  1.34305947e-01, mean val. rec. loss:  1.30703294e-01\n",
      "Epoch: 856 mean train loss:  1.34108048e-01, mean val. rec. loss:  1.30501782e-01\n",
      "Epoch: 857 mean train loss:  1.33909673e-01, mean val. rec. loss:  1.30299826e-01\n",
      "Epoch: 858 mean train loss:  1.33710880e-01, mean val. rec. loss:  1.30097433e-01\n",
      "Epoch: 859 mean train loss:  1.33511656e-01, mean val. rec. loss:  1.29894606e-01\n",
      "Epoch: 860 mean train loss:  1.33311984e-01, mean val. rec. loss:  1.29691334e-01\n",
      "Epoch: 861 mean train loss:  1.33111896e-01, mean val. rec. loss:  1.29487644e-01\n",
      "Epoch: 862 mean train loss:  1.32911390e-01, mean val. rec. loss:  1.29283519e-01\n",
      "Epoch: 863 mean train loss:  1.32710452e-01, mean val. rec. loss:  1.29078941e-01\n",
      "Epoch: 864 mean train loss:  1.32509067e-01, mean val. rec. loss:  1.28873963e-01\n",
      "Epoch: 865 mean train loss:  1.32307295e-01, mean val. rec. loss:  1.28668541e-01\n",
      "Epoch: 866 mean train loss:  1.32105091e-01, mean val. rec. loss:  1.28462711e-01\n",
      "Epoch: 867 mean train loss:  1.31902455e-01, mean val. rec. loss:  1.28256454e-01\n",
      "Epoch: 868 mean train loss:  1.31699417e-01, mean val. rec. loss:  1.28049780e-01\n",
      "Epoch: 869 mean train loss:  1.31495961e-01, mean val. rec. loss:  1.27842707e-01\n",
      "Epoch: 870 mean train loss:  1.31292118e-01, mean val. rec. loss:  1.27635189e-01\n",
      "Epoch: 871 mean train loss:  1.31087843e-01, mean val. rec. loss:  1.27427291e-01\n",
      "Epoch: 872 mean train loss:  1.30883181e-01, mean val. rec. loss:  1.27218965e-01\n",
      "Epoch: 873 mean train loss:  1.30678102e-01, mean val. rec. loss:  1.27010259e-01\n",
      "Epoch: 874 mean train loss:  1.30472635e-01, mean val. rec. loss:  1.26801154e-01\n",
      "Epoch: 875 mean train loss:  1.30266781e-01, mean val. rec. loss:  1.26591622e-01\n",
      "Epoch: 876 mean train loss:  1.30060510e-01, mean val. rec. loss:  1.26381710e-01\n",
      "Epoch: 877 mean train loss:  1.29853882e-01, mean val. rec. loss:  1.26171407e-01\n",
      "Epoch: 878 mean train loss:  1.29646836e-01, mean val. rec. loss:  1.25960714e-01\n",
      "Epoch: 879 mean train loss:  1.29439403e-01, mean val. rec. loss:  1.25749631e-01\n",
      "Epoch: 880 mean train loss:  1.29231612e-01, mean val. rec. loss:  1.25538176e-01\n",
      "Epoch: 881 mean train loss:  1.29023434e-01, mean val. rec. loss:  1.25326331e-01\n",
      "Epoch: 882 mean train loss:  1.28814869e-01, mean val. rec. loss:  1.25114105e-01\n",
      "Epoch: 883 mean train loss:  1.28605946e-01, mean val. rec. loss:  1.24901516e-01\n",
      "Epoch: 884 mean train loss:  1.28396650e-01, mean val. rec. loss:  1.24688583e-01\n",
      "Epoch: 885 mean train loss:  1.28186998e-01, mean val. rec. loss:  1.24475241e-01\n",
      "Epoch: 886 mean train loss:  1.27976972e-01, mean val. rec. loss:  1.24261564e-01\n",
      "Epoch: 887 mean train loss:  1.27766604e-01, mean val. rec. loss:  1.24047505e-01\n",
      "Epoch: 888 mean train loss:  1.27555849e-01, mean val. rec. loss:  1.23833093e-01\n",
      "Epoch: 889 mean train loss:  1.27344766e-01, mean val. rec. loss:  1.23618336e-01\n",
      "Epoch: 890 mean train loss:  1.27133325e-01, mean val. rec. loss:  1.23403243e-01\n",
      "Epoch: 891 mean train loss:  1.26921542e-01, mean val. rec. loss:  1.23187797e-01\n",
      "Epoch: 892 mean train loss:  1.26709416e-01, mean val. rec. loss:  1.22972014e-01\n",
      "Epoch: 893 mean train loss:  1.26496963e-01, mean val. rec. loss:  1.22755888e-01\n",
      "Epoch: 894 mean train loss:  1.26284167e-01, mean val. rec. loss:  1.22539434e-01\n",
      "Epoch: 895 mean train loss:  1.26071043e-01, mean val. rec. loss:  1.22322636e-01\n",
      "Epoch: 896 mean train loss:  1.25857606e-01, mean val. rec. loss:  1.22105538e-01\n",
      "Epoch: 897 mean train loss:  1.25643841e-01, mean val. rec. loss:  1.21888114e-01\n",
      "Epoch: 898 mean train loss:  1.25429749e-01, mean val. rec. loss:  1.21670373e-01\n",
      "Epoch: 899 mean train loss:  1.25215359e-01, mean val. rec. loss:  1.21452295e-01\n",
      "Epoch: 900 mean train loss:  1.25000641e-01, mean val. rec. loss:  1.21233946e-01\n",
      "Epoch: 901 mean train loss:  1.24785640e-01, mean val. rec. loss:  1.21015270e-01\n",
      "Epoch: 902 mean train loss:  1.24570326e-01, mean val. rec. loss:  1.20796303e-01\n",
      "Epoch: 903 mean train loss:  1.24354706e-01, mean val. rec. loss:  1.20577038e-01\n",
      "Epoch: 904 mean train loss:  1.24138804e-01, mean val. rec. loss:  1.20357482e-01\n",
      "Epoch: 905 mean train loss:  1.23922611e-01, mean val. rec. loss:  1.20137644e-01\n",
      "Epoch: 906 mean train loss:  1.23706135e-01, mean val. rec. loss:  1.19917535e-01\n",
      "Epoch: 907 mean train loss:  1.23489391e-01, mean val. rec. loss:  1.19697144e-01\n",
      "Epoch: 908 mean train loss:  1.23272356e-01, mean val. rec. loss:  1.19476473e-01\n",
      "Epoch: 909 mean train loss:  1.23055054e-01, mean val. rec. loss:  1.19255547e-01\n",
      "Epoch: 910 mean train loss:  1.22837490e-01, mean val. rec. loss:  1.19034358e-01\n",
      "Epoch: 911 mean train loss:  1.22619674e-01, mean val. rec. loss:  1.18812906e-01\n",
      "Epoch: 912 mean train loss:  1.22401589e-01, mean val. rec. loss:  1.18591209e-01\n",
      "Epoch: 913 mean train loss:  1.22183266e-01, mean val. rec. loss:  1.18369249e-01\n",
      "Epoch: 914 mean train loss:  1.21964689e-01, mean val. rec. loss:  1.18147062e-01\n",
      "Epoch: 915 mean train loss:  1.21745867e-01, mean val. rec. loss:  1.17924639e-01\n",
      "Epoch: 916 mean train loss:  1.21526821e-01, mean val. rec. loss:  1.17701981e-01\n",
      "Epoch: 917 mean train loss:  1.21307537e-01, mean val. rec. loss:  1.17479104e-01\n",
      "Epoch: 918 mean train loss:  1.21088029e-01, mean val. rec. loss:  1.17255992e-01\n",
      "Epoch: 919 mean train loss:  1.20868283e-01, mean val. rec. loss:  1.17032680e-01\n",
      "Epoch: 920 mean train loss:  1.20648344e-01, mean val. rec. loss:  1.16809142e-01\n",
      "Epoch: 921 mean train loss:  1.20428188e-01, mean val. rec. loss:  1.16585386e-01\n",
      "Epoch: 922 mean train loss:  1.20207809e-01, mean val. rec. loss:  1.16361448e-01\n",
      "Epoch: 923 mean train loss:  1.19987244e-01, mean val. rec. loss:  1.16137329e-01\n",
      "Epoch: 924 mean train loss:  1.19766477e-01, mean val. rec. loss:  1.15913001e-01\n",
      "Epoch: 925 mean train loss:  1.19545525e-01, mean val. rec. loss:  1.15688492e-01\n",
      "Epoch: 926 mean train loss:  1.19324393e-01, mean val. rec. loss:  1.15463810e-01\n",
      "Epoch: 927 mean train loss:  1.19103083e-01, mean val. rec. loss:  1.15238947e-01\n",
      "Epoch: 928 mean train loss:  1.18881594e-01, mean val. rec. loss:  1.15013930e-01\n",
      "Epoch: 929 mean train loss:  1.18659941e-01, mean val. rec. loss:  1.14788749e-01\n",
      "Epoch: 930 mean train loss:  1.18438117e-01, mean val. rec. loss:  1.14563415e-01\n",
      "Epoch: 931 mean train loss:  1.18216144e-01, mean val. rec. loss:  1.14337917e-01\n",
      "Epoch: 932 mean train loss:  1.17994015e-01, mean val. rec. loss:  1.14112282e-01\n",
      "Epoch: 933 mean train loss:  1.17771736e-01, mean val. rec. loss:  1.13886521e-01\n",
      "Epoch: 934 mean train loss:  1.17549316e-01, mean val. rec. loss:  1.13660615e-01\n",
      "Epoch: 935 mean train loss:  1.17326770e-01, mean val. rec. loss:  1.13434564e-01\n",
      "Epoch: 936 mean train loss:  1.17104089e-01, mean val. rec. loss:  1.13208440e-01\n",
      "Epoch: 937 mean train loss:  1.16881296e-01, mean val. rec. loss:  1.12982170e-01\n",
      "Epoch: 938 mean train loss:  1.16658370e-01, mean val. rec. loss:  1.12755792e-01\n",
      "Epoch: 939 mean train loss:  1.16435339e-01, mean val. rec. loss:  1.12529324e-01\n",
      "Epoch: 940 mean train loss:  1.16212204e-01, mean val. rec. loss:  1.12302746e-01\n",
      "Epoch: 941 mean train loss:  1.15988979e-01, mean val. rec. loss:  1.12076087e-01\n",
      "Epoch: 942 mean train loss:  1.15765636e-01, mean val. rec. loss:  1.11849355e-01\n",
      "Epoch: 943 mean train loss:  1.15542225e-01, mean val. rec. loss:  1.11622523e-01\n",
      "Epoch: 944 mean train loss:  1.15318725e-01, mean val. rec. loss:  1.11395619e-01\n",
      "Epoch: 945 mean train loss:  1.15095158e-01, mean val. rec. loss:  1.11168661e-01\n",
      "Epoch: 946 mean train loss:  1.14871501e-01, mean val. rec. loss:  1.10941648e-01\n",
      "Epoch: 947 mean train loss:  1.14647800e-01, mean val. rec. loss:  1.10714580e-01\n",
      "Epoch: 948 mean train loss:  1.14424032e-01, mean val. rec. loss:  1.10487458e-01\n",
      "Epoch: 949 mean train loss:  1.14200204e-01, mean val. rec. loss:  1.10260309e-01\n",
      "Epoch: 950 mean train loss:  1.13976354e-01, mean val. rec. loss:  1.10033106e-01\n",
      "Epoch: 951 mean train loss:  1.13752444e-01, mean val. rec. loss:  1.09805893e-01\n",
      "Epoch: 952 mean train loss:  1.13528519e-01, mean val. rec. loss:  1.09578644e-01\n",
      "Epoch: 953 mean train loss:  1.13304550e-01, mean val. rec. loss:  1.09351377e-01\n",
      "Epoch: 954 mean train loss:  1.13080551e-01, mean val. rec. loss:  1.09124119e-01\n",
      "Epoch: 955 mean train loss:  1.12856551e-01, mean val. rec. loss:  1.08896843e-01\n",
      "Epoch: 956 mean train loss:  1.12632537e-01, mean val. rec. loss:  1.08669576e-01\n",
      "Epoch: 957 mean train loss:  1.12408523e-01, mean val. rec. loss:  1.08442309e-01\n",
      "Epoch: 958 mean train loss:  1.12184509e-01, mean val. rec. loss:  1.08215069e-01\n",
      "Epoch: 959 mean train loss:  1.11960510e-01, mean val. rec. loss:  1.07987847e-01\n",
      "Epoch: 960 mean train loss:  1.11736511e-01, mean val. rec. loss:  1.07760653e-01\n",
      "Epoch: 961 mean train loss:  1.11512541e-01, mean val. rec. loss:  1.07533504e-01\n",
      "Epoch: 962 mean train loss:  1.11288609e-01, mean val. rec. loss:  1.07306373e-01\n",
      "Epoch: 963 mean train loss:  1.11064692e-01, mean val. rec. loss:  1.07079332e-01\n",
      "Epoch: 964 mean train loss:  1.10840842e-01, mean val. rec. loss:  1.06852310e-01\n",
      "Epoch: 965 mean train loss:  1.10617021e-01, mean val. rec. loss:  1.06625370e-01\n",
      "Epoch: 966 mean train loss:  1.10393261e-01, mean val. rec. loss:  1.06398502e-01\n",
      "Epoch: 967 mean train loss:  1.10169559e-01, mean val. rec. loss:  1.06171670e-01\n",
      "Epoch: 968 mean train loss:  1.09945903e-01, mean val. rec. loss:  1.05944957e-01\n",
      "Epoch: 969 mean train loss:  1.09722351e-01, mean val. rec. loss:  1.05718316e-01\n",
      "Epoch: 970 mean train loss:  1.09498851e-01, mean val. rec. loss:  1.05491765e-01\n",
      "Epoch: 971 mean train loss:  1.09275440e-01, mean val. rec. loss:  1.05265315e-01\n",
      "Epoch: 972 mean train loss:  1.09052126e-01, mean val. rec. loss:  1.05038991e-01\n",
      "Epoch: 973 mean train loss:  1.08828901e-01, mean val. rec. loss:  1.04812749e-01\n",
      "Epoch: 974 mean train loss:  1.08605781e-01, mean val. rec. loss:  1.04586643e-01\n",
      "Epoch: 975 mean train loss:  1.08382773e-01, mean val. rec. loss:  1.04360655e-01\n",
      "Epoch: 976 mean train loss:  1.08159869e-01, mean val. rec. loss:  1.04134794e-01\n",
      "Epoch: 977 mean train loss:  1.07937091e-01, mean val. rec. loss:  1.03909088e-01\n",
      "Epoch: 978 mean train loss:  1.07714433e-01, mean val. rec. loss:  1.03683517e-01\n",
      "Epoch: 979 mean train loss:  1.07491923e-01, mean val. rec. loss:  1.03458092e-01\n",
      "Epoch: 980 mean train loss:  1.07269548e-01, mean val. rec. loss:  1.03232838e-01\n",
      "Epoch: 981 mean train loss:  1.07047314e-01, mean val. rec. loss:  1.03007730e-01\n",
      "Epoch: 982 mean train loss:  1.06825229e-01, mean val. rec. loss:  1.02782795e-01\n",
      "Epoch: 983 mean train loss:  1.06603301e-01, mean val. rec. loss:  1.02558050e-01\n",
      "Epoch: 984 mean train loss:  1.06381536e-01, mean val. rec. loss:  1.02333477e-01\n",
      "Epoch: 985 mean train loss:  1.06159950e-01, mean val. rec. loss:  1.02109077e-01\n",
      "Epoch: 986 mean train loss:  1.05938514e-01, mean val. rec. loss:  1.01884894e-01\n",
      "Epoch: 987 mean train loss:  1.05717285e-01, mean val. rec. loss:  1.01660893e-01\n",
      "Epoch: 988 mean train loss:  1.05496221e-01, mean val. rec. loss:  1.01437091e-01\n",
      "Epoch: 989 mean train loss:  1.05275358e-01, mean val. rec. loss:  1.01213526e-01\n",
      "Epoch: 990 mean train loss:  1.05054696e-01, mean val. rec. loss:  1.00990159e-01\n",
      "Epoch: 991 mean train loss:  1.04834220e-01, mean val. rec. loss:  1.00767011e-01\n",
      "Epoch: 992 mean train loss:  1.04613967e-01, mean val. rec. loss:  1.00544098e-01\n",
      "Epoch: 993 mean train loss:  1.04393923e-01, mean val. rec. loss:  1.00321431e-01\n",
      "Epoch: 994 mean train loss:  1.04174103e-01, mean val. rec. loss:  1.00098999e-01\n",
      "Epoch: 995 mean train loss:  1.03954506e-01, mean val. rec. loss:  9.98767939e-02\n",
      "Epoch: 996 mean train loss:  1.03735140e-01, mean val. rec. loss:  9.96548611e-02\n",
      "Epoch: 997 mean train loss:  1.03516012e-01, mean val. rec. loss:  9.94331732e-02\n",
      "Epoch: 998 mean train loss:  1.03297116e-01, mean val. rec. loss:  9.92117665e-02\n",
      "Epoch: 999 mean train loss:  1.03078487e-01, mean val. rec. loss:  9.89906229e-02\n",
      "Epoch: 1000 mean train loss:  1.02860089e-01, mean val. rec. loss:  9.87697424e-02\n",
      "Epoch: 1001 mean train loss:  1.02641975e-01, mean val. rec. loss:  9.85491522e-02\n",
      "Epoch: 1002 mean train loss:  1.02424098e-01, mean val. rec. loss:  9.83288523e-02\n",
      "Epoch: 1003 mean train loss:  1.02206520e-01, mean val. rec. loss:  9.81088427e-02\n",
      "Epoch: 1004 mean train loss:  1.01989195e-01, mean val. rec. loss:  9.78891234e-02\n",
      "Epoch: 1005 mean train loss:  1.01772138e-01, mean val. rec. loss:  9.76696944e-02\n",
      "Epoch: 1006 mean train loss:  1.01555386e-01, mean val. rec. loss:  9.74505829e-02\n",
      "Epoch: 1007 mean train loss:  1.01338911e-01, mean val. rec. loss:  9.72317799e-02\n",
      "Epoch: 1008 mean train loss:  1.01122725e-01, mean val. rec. loss:  9.70133125e-02\n",
      "Epoch: 1009 mean train loss:  1.00906853e-01, mean val. rec. loss:  9.67951354e-02\n",
      "Epoch: 1010 mean train loss:  1.00691263e-01, mean val. rec. loss:  9.65773302e-02\n",
      "Epoch: 1011 mean train loss:  1.00476009e-01, mean val. rec. loss:  9.63598063e-02\n",
      "Epoch: 1012 mean train loss:  1.00261045e-01, mean val. rec. loss:  9.61426725e-02\n",
      "Epoch: 1013 mean train loss:  1.00046424e-01, mean val. rec. loss:  9.59258289e-02\n",
      "Epoch: 1014 mean train loss:  9.98321004e-02, mean val. rec. loss:  9.57093664e-02\n",
      "Epoch: 1015 mean train loss:  9.96181198e-02, mean val. rec. loss:  9.54932305e-02\n",
      "Epoch: 1016 mean train loss:  9.94044447e-02, mean val. rec. loss:  9.52774937e-02\n",
      "Epoch: 1017 mean train loss:  9.91911419e-02, mean val. rec. loss:  9.50620563e-02\n",
      "Epoch: 1018 mean train loss:  9.89781520e-02, mean val. rec. loss:  9.48470271e-02\n",
      "Epoch: 1019 mean train loss:  9.87655196e-02, mean val. rec. loss:  9.46323517e-02\n",
      "Epoch: 1020 mean train loss:  9.85532299e-02, mean val. rec. loss:  9.44180483e-02\n",
      "Epoch: 1021 mean train loss:  9.83412829e-02, mean val. rec. loss:  9.42041259e-02\n",
      "Epoch: 1022 mean train loss:  9.81297082e-02, mean val. rec. loss:  9.39905935e-02\n",
      "Epoch: 1023 mean train loss:  9.79184911e-02, mean val. rec. loss:  9.37774332e-02\n",
      "Epoch: 1024 mean train loss:  9.77076390e-02, mean val. rec. loss:  9.35646901e-02\n",
      "Epoch: 1025 mean train loss:  9.74971593e-02, mean val. rec. loss:  9.33523280e-02\n",
      "Epoch: 1026 mean train loss:  9.72870447e-02, mean val. rec. loss:  9.31403560e-02\n",
      "Epoch: 1027 mean train loss:  9.70773173e-02, mean val. rec. loss:  9.29287832e-02\n",
      "Epoch: 1028 mean train loss:  9.68679624e-02, mean val. rec. loss:  9.27176459e-02\n",
      "Epoch: 1029 mean train loss:  9.66589799e-02, mean val. rec. loss:  9.25068986e-02\n",
      "Epoch: 1030 mean train loss:  9.64503997e-02, mean val. rec. loss:  9.22965777e-02\n",
      "Epoch: 1031 mean train loss:  9.62422068e-02, mean val. rec. loss:  9.20866468e-02\n",
      "Epoch: 1032 mean train loss:  9.60344087e-02, mean val. rec. loss:  9.18771605e-02\n",
      "Epoch: 1033 mean train loss:  9.58269905e-02, mean val. rec. loss:  9.16680915e-02\n",
      "Epoch: 1034 mean train loss:  9.56199894e-02, mean val. rec. loss:  9.14594489e-02\n",
      "Epoch: 1035 mean train loss:  9.54133831e-02, mean val. rec. loss:  9.12512236e-02\n",
      "Epoch: 1036 mean train loss:  9.52071864e-02, mean val. rec. loss:  9.10434700e-02\n",
      "Epoch: 1037 mean train loss:  9.50014069e-02, mean val. rec. loss:  9.08361065e-02\n",
      "Epoch: 1038 mean train loss:  9.47960222e-02, mean val. rec. loss:  9.06292057e-02\n",
      "Epoch: 1039 mean train loss:  9.45910695e-02, mean val. rec. loss:  9.04227584e-02\n",
      "Epoch: 1040 mean train loss:  9.43865265e-02, mean val. rec. loss:  9.02167376e-02\n",
      "Epoch: 1041 mean train loss:  9.41824006e-02, mean val. rec. loss:  9.00111794e-02\n",
      "Epoch: 1042 mean train loss:  9.39787068e-02, mean val. rec. loss:  8.98060657e-02\n",
      "Epoch: 1043 mean train loss:  9.37754301e-02, mean val. rec. loss:  8.96014237e-02\n",
      "Epoch: 1044 mean train loss:  9.35726003e-02, mean val. rec. loss:  8.93972172e-02\n",
      "Epoch: 1045 mean train loss:  9.33701802e-02, mean val. rec. loss:  8.91934825e-02\n",
      "Epoch: 1046 mean train loss:  9.31681921e-02, mean val. rec. loss:  8.89902376e-02\n",
      "Epoch: 1047 mean train loss:  9.29666584e-02, mean val. rec. loss:  8.87874281e-02\n",
      "Epoch: 1048 mean train loss:  9.27655567e-02, mean val. rec. loss:  8.85850541e-02\n",
      "Epoch: 1049 mean train loss:  9.25648722e-02, mean val. rec. loss:  8.83832063e-02\n",
      "Epoch: 1050 mean train loss:  9.23646643e-02, mean val. rec. loss:  8.81817939e-02\n",
      "Epoch: 1051 mean train loss:  9.21648736e-02, mean val. rec. loss:  8.79808623e-02\n",
      "Epoch: 1052 mean train loss:  9.19655448e-02, mean val. rec. loss:  8.77804115e-02\n",
      "Epoch: 1053 mean train loss:  9.17666479e-02, mean val. rec. loss:  8.75804506e-02\n",
      "Epoch: 1054 mean train loss:  9.15682129e-02, mean val. rec. loss:  8.73809343e-02\n",
      "Epoch: 1055 mean train loss:  9.13702099e-02, mean val. rec. loss:  8.71819168e-02\n",
      "Epoch: 1056 mean train loss:  9.11726687e-02, mean val. rec. loss:  8.69833711e-02\n",
      "Epoch: 1057 mean train loss:  9.09755894e-02, mean val. rec. loss:  8.67853335e-02\n",
      "Epoch: 1058 mean train loss:  9.07789570e-02, mean val. rec. loss:  8.65877766e-02\n",
      "Epoch: 1059 mean train loss:  9.05827938e-02, mean val. rec. loss:  8.63907006e-02\n",
      "Epoch: 1060 mean train loss:  9.03870776e-02, mean val. rec. loss:  8.61941053e-02\n",
      "Epoch: 1061 mean train loss:  9.01918307e-02, mean val. rec. loss:  8.59980090e-02\n",
      "Epoch: 1062 mean train loss:  8.99970306e-02, mean val. rec. loss:  8.58024207e-02\n",
      "Epoch: 1063 mean train loss:  8.98027148e-02, mean val. rec. loss:  8.56073042e-02\n",
      "Epoch: 1064 mean train loss:  8.96088309e-02, mean val. rec. loss:  8.54126866e-02\n",
      "Epoch: 1065 mean train loss:  8.94154313e-02, mean val. rec. loss:  8.52185498e-02\n",
      "Epoch: 1066 mean train loss:  8.92224786e-02, mean val. rec. loss:  8.50249482e-02\n",
      "Epoch: 1067 mean train loss:  8.90300174e-02, mean val. rec. loss:  8.48317912e-02\n",
      "Epoch: 1068 mean train loss:  8.88379958e-02, mean val. rec. loss:  8.46391603e-02\n",
      "Epoch: 1069 mean train loss:  8.86464509e-02, mean val. rec. loss:  8.44470284e-02\n",
      "Epoch: 1070 mean train loss:  8.84553752e-02, mean val. rec. loss:  8.42553863e-02\n",
      "Epoch: 1071 mean train loss:  8.82647614e-02, mean val. rec. loss:  8.40642432e-02\n",
      "Epoch: 1072 mean train loss:  8.80746169e-02, mean val. rec. loss:  8.38735991e-02\n",
      "Epoch: 1073 mean train loss:  8.78849490e-02, mean val. rec. loss:  8.36834630e-02\n",
      "Epoch: 1074 mean train loss:  8.76957356e-02, mean val. rec. loss:  8.34938077e-02\n",
      "Epoch: 1075 mean train loss:  8.75069989e-02, mean val. rec. loss:  8.33046785e-02\n",
      "Epoch: 1076 mean train loss:  8.73187314e-02, mean val. rec. loss:  8.31160392e-02\n",
      "Epoch: 1077 mean train loss:  8.71309407e-02, mean val. rec. loss:  8.29279080e-02\n",
      "Epoch: 1078 mean train loss:  8.69436043e-02, mean val. rec. loss:  8.27402757e-02\n",
      "Epoch: 1079 mean train loss:  8.67567521e-02, mean val. rec. loss:  8.25531423e-02\n",
      "Epoch: 1080 mean train loss:  8.65703543e-02, mean val. rec. loss:  8.23665261e-02\n",
      "Epoch: 1081 mean train loss:  8.63844555e-02, mean val. rec. loss:  8.21803906e-02\n",
      "Epoch: 1082 mean train loss:  8.61989962e-02, mean val. rec. loss:  8.19947632e-02\n",
      "Epoch: 1083 mean train loss:  8.60140137e-02, mean val. rec. loss:  8.18096438e-02\n",
      "Epoch: 1084 mean train loss:  8.58295078e-02, mean val. rec. loss:  8.16250233e-02\n",
      "Epoch: 1085 mean train loss:  8.56454713e-02, mean val. rec. loss:  8.14408927e-02\n",
      "Epoch: 1086 mean train loss:  8.54618816e-02, mean val. rec. loss:  8.12572611e-02\n",
      "Epoch: 1087 mean train loss:  8.52787762e-02, mean val. rec. loss:  8.10741556e-02\n",
      "Epoch: 1088 mean train loss:  8.50961400e-02, mean val. rec. loss:  8.08915400e-02\n",
      "Epoch: 1089 mean train loss:  8.49139656e-02, mean val. rec. loss:  8.07094143e-02\n",
      "Epoch: 1090 mean train loss:  8.47322680e-02, mean val. rec. loss:  8.05277966e-02\n",
      "Epoch: 1091 mean train loss:  8.45510321e-02, mean val. rec. loss:  8.03466688e-02\n",
      "Epoch: 1092 mean train loss:  8.43702507e-02, mean val. rec. loss:  8.01660490e-02\n",
      "Epoch: 1093 mean train loss:  8.41899459e-02, mean val. rec. loss:  7.99859191e-02\n",
      "Epoch: 1094 mean train loss:  8.40101030e-02, mean val. rec. loss:  7.98062972e-02\n",
      "Epoch: 1095 mean train loss:  8.38307294e-02, mean val. rec. loss:  7.96271652e-02\n",
      "Epoch: 1096 mean train loss:  8.36518027e-02, mean val. rec. loss:  7.94485231e-02\n",
      "Epoch: 1097 mean train loss:  8.34733378e-02, mean val. rec. loss:  7.92703889e-02\n",
      "Epoch: 1098 mean train loss:  8.32953347e-02, mean val. rec. loss:  7.90927175e-02\n",
      "Epoch: 1099 mean train loss:  8.31178009e-02, mean val. rec. loss:  7.89155722e-02\n",
      "Epoch: 1100 mean train loss:  8.29407215e-02, mean val. rec. loss:  7.87389077e-02\n",
      "Epoch: 1101 mean train loss:  8.27640965e-02, mean val. rec. loss:  7.85627240e-02\n",
      "Epoch: 1102 mean train loss:  8.25879407e-02, mean val. rec. loss:  7.83870212e-02\n",
      "Epoch: 1103 mean train loss:  8.24122169e-02, mean val. rec. loss:  7.82118354e-02\n",
      "Epoch: 1104 mean train loss:  8.22369550e-02, mean val. rec. loss:  7.80371123e-02\n",
      "Epoch: 1105 mean train loss:  8.20621475e-02, mean val. rec. loss:  7.78628790e-02\n",
      "Epoch: 1106 mean train loss:  8.18877943e-02, mean val. rec. loss:  7.76891629e-02\n",
      "Epoch: 1107 mean train loss:  8.17139029e-02, mean val. rec. loss:  7.75158822e-02\n",
      "Epoch: 1108 mean train loss:  8.15404362e-02, mean val. rec. loss:  7.73431005e-02\n",
      "Epoch: 1109 mean train loss:  8.13674312e-02, mean val. rec. loss:  7.71708086e-02\n",
      "Epoch: 1110 mean train loss:  8.11948732e-02, mean val. rec. loss:  7.69989794e-02\n",
      "Epoch: 1111 mean train loss:  8.10227472e-02, mean val. rec. loss:  7.68276401e-02\n",
      "Epoch: 1112 mean train loss:  8.08510756e-02, mean val. rec. loss:  7.66567815e-02\n",
      "Epoch: 1113 mean train loss:  8.06798434e-02, mean val. rec. loss:  7.64863947e-02\n",
      "Epoch: 1114 mean train loss:  8.05090657e-02, mean val. rec. loss:  7.63164888e-02\n",
      "Epoch: 1115 mean train loss:  8.03387199e-02, mean val. rec. loss:  7.61470454e-02\n",
      "Epoch: 1116 mean train loss:  8.01688062e-02, mean val. rec. loss:  7.59780784e-02\n",
      "Epoch: 1117 mean train loss:  7.99993320e-02, mean val. rec. loss:  7.58095740e-02\n",
      "Epoch: 1118 mean train loss:  7.98303047e-02, mean val. rec. loss:  7.56415414e-02\n",
      "Epoch: 1119 mean train loss:  7.96616945e-02, mean val. rec. loss:  7.54739804e-02\n",
      "Epoch: 1120 mean train loss:  7.94935164e-02, mean val. rec. loss:  7.53068640e-02\n",
      "Epoch: 1121 mean train loss:  7.93257702e-02, mean val. rec. loss:  7.51402148e-02\n",
      "Epoch: 1122 mean train loss:  7.91584412e-02, mean val. rec. loss:  7.49740419e-02\n",
      "Epoch: 1123 mean train loss:  7.89915517e-02, mean val. rec. loss:  7.48083271e-02\n",
      "Epoch: 1124 mean train loss:  7.88250794e-02, mean val. rec. loss:  7.46430387e-02\n",
      "Epoch: 1125 mean train loss:  7.86590315e-02, mean val. rec. loss:  7.44782492e-02\n",
      "Epoch: 1126 mean train loss:  7.84934232e-02, mean val. rec. loss:  7.43138952e-02\n",
      "Epoch: 1127 mean train loss:  7.83282171e-02, mean val. rec. loss:  7.41499993e-02\n",
      "Epoch: 1128 mean train loss:  7.81634356e-02, mean val. rec. loss:  7.39865570e-02\n",
      "Epoch: 1129 mean train loss:  7.79990712e-02, mean val. rec. loss:  7.38235455e-02\n",
      "Epoch: 1130 mean train loss:  7.78351240e-02, mean val. rec. loss:  7.36609968e-02\n",
      "Epoch: 1131 mean train loss:  7.76715790e-02, mean val. rec. loss:  7.34988926e-02\n",
      "Epoch: 1132 mean train loss:  7.75084585e-02, mean val. rec. loss:  7.33372057e-02\n",
      "Epoch: 1133 mean train loss:  7.73457254e-02, mean val. rec. loss:  7.31760086e-02\n",
      "Epoch: 1134 mean train loss:  7.71834318e-02, mean val. rec. loss:  7.30152244e-02\n",
      "Epoch: 1135 mean train loss:  7.70215404e-02, mean val. rec. loss:  7.28548891e-02\n",
      "Epoch: 1136 mean train loss:  7.68600512e-02, mean val. rec. loss:  7.26949712e-02\n",
      "Epoch: 1137 mean train loss:  7.66989345e-02, mean val. rec. loss:  7.25355114e-02\n",
      "Epoch: 1138 mean train loss:  7.65382498e-02, mean val. rec. loss:  7.23764826e-02\n",
      "Epoch: 1139 mean train loss:  7.63779673e-02, mean val. rec. loss:  7.22178619e-02\n",
      "Epoch: 1140 mean train loss:  7.62180573e-02, mean val. rec. loss:  7.20597085e-02\n",
      "Epoch: 1141 mean train loss:  7.60585793e-02, mean val. rec. loss:  7.19019542e-02\n",
      "Epoch: 1142 mean train loss:  7.58994663e-02, mean val. rec. loss:  7.17446308e-02\n",
      "Epoch: 1143 mean train loss:  7.57407555e-02, mean val. rec. loss:  7.15877247e-02\n",
      "Epoch: 1144 mean train loss:  7.55824247e-02, mean val. rec. loss:  7.14312677e-02\n",
      "Epoch: 1145 mean train loss:  7.54245035e-02, mean val. rec. loss:  7.12751917e-02\n",
      "Epoch: 1146 mean train loss:  7.52669398e-02, mean val. rec. loss:  7.11195829e-02\n",
      "Epoch: 1147 mean train loss:  7.51097858e-02, mean val. rec. loss:  7.09643551e-02\n",
      "Epoch: 1148 mean train loss:  7.49530043e-02, mean val. rec. loss:  7.08095401e-02\n",
      "Epoch: 1149 mean train loss:  7.47965952e-02, mean val. rec. loss:  7.06551560e-02\n",
      "Epoch: 1150 mean train loss:  7.46405883e-02, mean val. rec. loss:  7.05011801e-02\n",
      "Epoch: 1151 mean train loss:  7.44849613e-02, mean val. rec. loss:  7.03475988e-02\n",
      "Epoch: 1152 mean train loss:  7.43296919e-02, mean val. rec. loss:  7.01944394e-02\n",
      "Epoch: 1153 mean train loss:  7.41748023e-02, mean val. rec. loss:  7.00416837e-02\n",
      "Epoch: 1154 mean train loss:  7.40203001e-02, mean val. rec. loss:  6.98893453e-02\n",
      "Epoch: 1155 mean train loss:  7.38661554e-02, mean val. rec. loss:  6.97373879e-02\n",
      "Epoch: 1156 mean train loss:  7.37123906e-02, mean val. rec. loss:  6.95858478e-02\n",
      "Epoch: 1157 mean train loss:  7.35589982e-02, mean val. rec. loss:  6.94346933e-02\n",
      "Epoch: 1158 mean train loss:  7.34059560e-02, mean val. rec. loss:  6.92839424e-02\n",
      "Epoch: 1159 mean train loss:  7.32532862e-02, mean val. rec. loss:  6.91335907e-02\n",
      "Epoch: 1160 mean train loss:  7.31009739e-02, mean val. rec. loss:  6.89836200e-02\n",
      "Epoch: 1161 mean train loss:  7.29490191e-02, mean val. rec. loss:  6.88340395e-02\n",
      "Epoch: 1162 mean train loss:  7.27974368e-02, mean val. rec. loss:  6.86848580e-02\n",
      "Epoch: 1163 mean train loss:  7.26462120e-02, mean val. rec. loss:  6.85360712e-02\n",
      "Epoch: 1164 mean train loss:  7.24953299e-02, mean val. rec. loss:  6.83876518e-02\n",
      "Epoch: 1165 mean train loss:  7.23448053e-02, mean val. rec. loss:  6.82396316e-02\n",
      "Epoch: 1166 mean train loss:  7.21946532e-02, mean val. rec. loss:  6.80919924e-02\n",
      "Epoch: 1167 mean train loss:  7.20448437e-02, mean val. rec. loss:  6.79447387e-02\n",
      "Epoch: 1168 mean train loss:  7.18953843e-02, mean val. rec. loss:  6.77978661e-02\n",
      "Epoch: 1169 mean train loss:  7.17462750e-02, mean val. rec. loss:  6.76513472e-02\n",
      "Epoch: 1170 mean train loss:  7.15974859e-02, mean val. rec. loss:  6.75052321e-02\n",
      "Epoch: 1171 mean train loss:  7.14490768e-02, mean val. rec. loss:  6.73594979e-02\n",
      "Epoch: 1172 mean train loss:  7.13010103e-02, mean val. rec. loss:  6.72141131e-02\n",
      "Epoch: 1173 mean train loss:  7.11532716e-02, mean val. rec. loss:  6.70691001e-02\n",
      "Epoch: 1174 mean train loss:  7.10058755e-02, mean val. rec. loss:  6.69244682e-02\n",
      "Epoch: 1175 mean train loss:  7.08588220e-02, mean val. rec. loss:  6.67802083e-02\n",
      "Epoch: 1176 mean train loss:  7.07121037e-02, mean val. rec. loss:  6.66363066e-02\n",
      "Epoch: 1177 mean train loss:  7.05657207e-02, mean val. rec. loss:  6.64927679e-02\n",
      "Epoch: 1178 mean train loss:  7.04196802e-02, mean val. rec. loss:  6.63495920e-02\n",
      "Epoch: 1179 mean train loss:  7.02739676e-02, mean val. rec. loss:  6.62067971e-02\n",
      "Epoch: 1180 mean train loss:  7.01285901e-02, mean val. rec. loss:  6.60643424e-02\n",
      "Epoch: 1181 mean train loss:  6.99835403e-02, mean val. rec. loss:  6.59222688e-02\n",
      "Epoch: 1182 mean train loss:  6.98388332e-02, mean val. rec. loss:  6.57805353e-02\n",
      "Epoch: 1183 mean train loss:  6.96944390e-02, mean val. rec. loss:  6.56391375e-02\n",
      "Epoch: 1184 mean train loss:  6.95503576e-02, mean val. rec. loss:  6.54981162e-02\n",
      "Epoch: 1185 mean train loss:  6.94066263e-02, mean val. rec. loss:  6.53574623e-02\n",
      "Epoch: 1186 mean train loss:  6.92632078e-02, mean val. rec. loss:  6.52171395e-02\n",
      "Epoch: 1187 mean train loss:  6.91201171e-02, mean val. rec. loss:  6.50771841e-02\n",
      "Epoch: 1188 mean train loss:  6.89773615e-02, mean val. rec. loss:  6.49375507e-02\n",
      "Epoch: 1189 mean train loss:  6.88349040e-02, mean val. rec. loss:  6.47982939e-02\n",
      "Epoch: 1190 mean train loss:  6.86927816e-02, mean val. rec. loss:  6.46593727e-02\n",
      "Epoch: 1191 mean train loss:  6.85509646e-02, mean val. rec. loss:  6.45207917e-02\n",
      "Epoch: 1192 mean train loss:  6.84094754e-02, mean val. rec. loss:  6.43825644e-02\n",
      "Epoch: 1193 mean train loss:  6.82682915e-02, mean val. rec. loss:  6.42446638e-02\n",
      "Epoch: 1194 mean train loss:  6.81274205e-02, mean val. rec. loss:  6.41071170e-02\n",
      "Epoch: 1195 mean train loss:  6.79868773e-02, mean val. rec. loss:  6.39699058e-02\n",
      "Epoch: 1196 mean train loss:  6.78466320e-02, mean val. rec. loss:  6.38330212e-02\n",
      "Epoch: 1197 mean train loss:  6.77067145e-02, mean val. rec. loss:  6.36964905e-02\n",
      "Epoch: 1198 mean train loss:  6.75670800e-02, mean val. rec. loss:  6.35603044e-02\n",
      "Epoch: 1199 mean train loss:  6.74277807e-02, mean val. rec. loss:  6.34244449e-02\n",
      "Epoch: 1200 mean train loss:  6.72887942e-02, mean val. rec. loss:  6.32889257e-02\n",
      "Epoch: 1201 mean train loss:  6.71500983e-02, mean val. rec. loss:  6.31536967e-02\n",
      "Epoch: 1202 mean train loss:  6.70116854e-02, mean val. rec. loss:  6.30188397e-02\n",
      "Epoch: 1203 mean train loss:  6.68736077e-02, mean val. rec. loss:  6.28843001e-02\n",
      "Epoch: 1204 mean train loss:  6.67358205e-02, mean val. rec. loss:  6.27500963e-02\n",
      "Epoch: 1205 mean train loss:  6.65983387e-02, mean val. rec. loss:  6.26162008e-02\n",
      "Epoch: 1206 mean train loss:  6.64611623e-02, mean val. rec. loss:  6.24826638e-02\n",
      "Epoch: 1207 mean train loss:  6.63242913e-02, mean val. rec. loss:  6.23494215e-02\n",
      "Epoch: 1208 mean train loss:  6.61877033e-02, mean val. rec. loss:  6.22165149e-02\n",
      "Epoch: 1209 mean train loss:  6.60514208e-02, mean val. rec. loss:  6.20839395e-02\n",
      "Epoch: 1210 mean train loss:  6.59154436e-02, mean val. rec. loss:  6.19516815e-02\n",
      "Epoch: 1211 mean train loss:  6.57797644e-02, mean val. rec. loss:  6.18197410e-02\n",
      "Epoch: 1212 mean train loss:  6.56443757e-02, mean val. rec. loss:  6.16881090e-02\n",
      "Epoch: 1213 mean train loss:  6.55092701e-02, mean val. rec. loss:  6.15568127e-02\n",
      "Epoch: 1214 mean train loss:  6.53744698e-02, mean val. rec. loss:  6.14258565e-02\n",
      "Epoch: 1215 mean train loss:  6.52399750e-02, mean val. rec. loss:  6.12951635e-02\n",
      "Epoch: 1216 mean train loss:  6.51057408e-02, mean val. rec. loss:  6.11648287e-02\n",
      "Epoch: 1217 mean train loss:  6.49718270e-02, mean val. rec. loss:  6.10347934e-02\n",
      "Epoch: 1218 mean train loss:  6.48381962e-02, mean val. rec. loss:  6.09050846e-02\n",
      "Epoch: 1219 mean train loss:  6.47048559e-02, mean val. rec. loss:  6.07756661e-02\n",
      "Epoch: 1220 mean train loss:  6.45717987e-02, mean val. rec. loss:  6.06465833e-02\n",
      "Epoch: 1221 mean train loss:  6.44390468e-02, mean val. rec. loss:  6.05177953e-02\n",
      "Epoch: 1222 mean train loss:  6.43065632e-02, mean val. rec. loss:  6.03893248e-02\n",
      "Epoch: 1223 mean train loss:  6.41743625e-02, mean val. rec. loss:  6.02611764e-02\n",
      "Epoch: 1224 mean train loss:  6.40424747e-02, mean val. rec. loss:  6.01333137e-02\n",
      "Epoch: 1225 mean train loss:  6.39108551e-02, mean val. rec. loss:  6.00057867e-02\n",
      "Epoch: 1226 mean train loss:  6.37795260e-02, mean val. rec. loss:  5.98785455e-02\n",
      "Epoch: 1227 mean train loss:  6.36484799e-02, mean val. rec. loss:  5.97516127e-02\n",
      "Epoch: 1228 mean train loss:  6.35177020e-02, mean val. rec. loss:  5.96249974e-02\n",
      "Epoch: 1229 mean train loss:  6.33872220e-02, mean val. rec. loss:  5.94986815e-02\n",
      "Epoch: 1230 mean train loss:  6.32570326e-02, mean val. rec. loss:  5.93726649e-02\n",
      "Epoch: 1231 mean train loss:  6.31271113e-02, mean val. rec. loss:  5.92469523e-02\n",
      "Epoch: 1232 mean train loss:  6.29974730e-02, mean val. rec. loss:  5.91215435e-02\n",
      "Epoch: 1233 mean train loss:  6.28681178e-02, mean val. rec. loss:  5.89964296e-02\n",
      "Epoch: 1234 mean train loss:  6.27390382e-02, mean val. rec. loss:  5.88716332e-02\n",
      "Epoch: 1235 mean train loss:  6.26102491e-02, mean val. rec. loss:  5.87471226e-02\n",
      "Epoch: 1236 mean train loss:  6.24817245e-02, mean val. rec. loss:  5.86229295e-02\n",
      "Epoch: 1237 mean train loss:  6.23534866e-02, mean val. rec. loss:  5.84990176e-02\n",
      "Epoch: 1238 mean train loss:  6.22255243e-02, mean val. rec. loss:  5.83754096e-02\n",
      "Epoch: 1239 mean train loss:  6.20978339e-02, mean val. rec. loss:  5.82520874e-02\n",
      "Epoch: 1240 mean train loss:  6.19704228e-02, mean val. rec. loss:  5.81290781e-02\n",
      "Epoch: 1241 mean train loss:  6.18432873e-02, mean val. rec. loss:  5.80063773e-02\n",
      "Epoch: 1242 mean train loss:  6.17164424e-02, mean val. rec. loss:  5.78839442e-02\n",
      "Epoch: 1243 mean train loss:  6.15898507e-02, mean val. rec. loss:  5.77618103e-02\n",
      "Epoch: 1244 mean train loss:  6.14635383e-02, mean val. rec. loss:  5.76399940e-02\n",
      "Epoch: 1245 mean train loss:  6.13375090e-02, mean val. rec. loss:  5.75184590e-02\n",
      "Epoch: 1246 mean train loss:  6.12117552e-02, mean val. rec. loss:  5.73972142e-02\n",
      "Epoch: 1247 mean train loss:  6.10862585e-02, mean val. rec. loss:  5.72762506e-02\n",
      "Epoch: 1248 mean train loss:  6.09610485e-02, mean val. rec. loss:  5.71556091e-02\n",
      "Epoch: 1249 mean train loss:  6.08361142e-02, mean val. rec. loss:  5.70352398e-02\n",
      "Epoch: 1250 mean train loss:  6.07114331e-02, mean val. rec. loss:  5.69151653e-02\n",
      "Epoch: 1251 mean train loss:  6.05870387e-02, mean val. rec. loss:  5.67953720e-02\n",
      "Epoch: 1252 mean train loss:  6.04629126e-02, mean val. rec. loss:  5.66758871e-02\n",
      "Epoch: 1253 mean train loss:  6.03390620e-02, mean val. rec. loss:  5.65566790e-02\n",
      "Epoch: 1254 mean train loss:  6.02154721e-02, mean val. rec. loss:  5.64377747e-02\n",
      "Epoch: 1255 mean train loss:  6.00921690e-02, mean val. rec. loss:  5.63191381e-02\n",
      "Epoch: 1256 mean train loss:  5.99691229e-02, mean val. rec. loss:  5.62008190e-02\n",
      "Epoch: 1257 mean train loss:  5.98463562e-02, mean val. rec. loss:  5.60827629e-02\n",
      "Epoch: 1258 mean train loss:  5.97238464e-02, mean val. rec. loss:  5.59649927e-02\n",
      "Epoch: 1259 mean train loss:  5.96016047e-02, mean val. rec. loss:  5.58475082e-02\n",
      "Epoch: 1260 mean train loss:  5.94796387e-02, mean val. rec. loss:  5.57303412e-02\n",
      "Epoch: 1261 mean train loss:  5.93579408e-02, mean val. rec. loss:  5.56134327e-02\n",
      "Epoch: 1262 mean train loss:  5.92365111e-02, mean val. rec. loss:  5.54968146e-02\n",
      "Epoch: 1263 mean train loss:  5.91153421e-02, mean val. rec. loss:  5.53804776e-02\n",
      "Epoch: 1264 mean train loss:  5.89944562e-02, mean val. rec. loss:  5.52644446e-02\n",
      "Epoch: 1265 mean train loss:  5.88738272e-02, mean val. rec. loss:  5.51486656e-02\n",
      "Epoch: 1266 mean train loss:  5.87534552e-02, mean val. rec. loss:  5.50331814e-02\n",
      "Epoch: 1267 mean train loss:  5.86333551e-02, mean val. rec. loss:  5.49179921e-02\n",
      "Epoch: 1268 mean train loss:  5.85135268e-02, mean val. rec. loss:  5.48030749e-02\n",
      "Epoch: 1269 mean train loss:  5.83939593e-02, mean val. rec. loss:  5.46884435e-02\n",
      "Epoch: 1270 mean train loss:  5.82746562e-02, mean val. rec. loss:  5.45741023e-02\n",
      "Epoch: 1271 mean train loss:  5.81556213e-02, mean val. rec. loss:  5.44600243e-02\n",
      "Epoch: 1272 mean train loss:  5.80368470e-02, mean val. rec. loss:  5.43462502e-02\n",
      "Epoch: 1273 mean train loss:  5.79183521e-02, mean val. rec. loss:  5.42327482e-02\n",
      "Epoch: 1274 mean train loss:  5.78001216e-02, mean val. rec. loss:  5.41195229e-02\n",
      "Epoch: 1275 mean train loss:  5.76821370e-02, mean val. rec. loss:  5.40065788e-02\n",
      "Epoch: 1276 mean train loss:  5.75644354e-02, mean val. rec. loss:  5.38939114e-02\n",
      "Epoch: 1277 mean train loss:  5.74469833e-02, mean val. rec. loss:  5.37815253e-02\n",
      "Epoch: 1278 mean train loss:  5.73297994e-02, mean val. rec. loss:  5.36694340e-02\n",
      "Epoch: 1279 mean train loss:  5.72128687e-02, mean val. rec. loss:  5.35576057e-02\n",
      "Epoch: 1280 mean train loss:  5.70962211e-02, mean val. rec. loss:  5.34460496e-02\n",
      "Epoch: 1281 mean train loss:  5.69798342e-02, mean val. rec. loss:  5.33347793e-02\n",
      "Epoch: 1282 mean train loss:  5.68636968e-02, mean val. rec. loss:  5.32238038e-02\n",
      "Epoch: 1283 mean train loss:  5.67478313e-02, mean val. rec. loss:  5.31130869e-02\n",
      "Epoch: 1284 mean train loss:  5.66322340e-02, mean val. rec. loss:  5.30026512e-02\n",
      "Epoch: 1285 mean train loss:  5.65168862e-02, mean val. rec. loss:  5.28924967e-02\n",
      "Epoch: 1286 mean train loss:  5.64018102e-02, mean val. rec. loss:  5.27826098e-02\n",
      "Epoch: 1287 mean train loss:  5.62869838e-02, mean val. rec. loss:  5.26730133e-02\n",
      "Epoch: 1288 mean train loss:  5.61724331e-02, mean val. rec. loss:  5.25636616e-02\n",
      "Epoch: 1289 mean train loss:  5.60581318e-02, mean val. rec. loss:  5.24546184e-02\n",
      "Epoch: 1290 mean train loss:  5.59441024e-02, mean val. rec. loss:  5.23458474e-02\n",
      "Epoch: 1291 mean train loss:  5.58303263e-02, mean val. rec. loss:  5.22373576e-02\n",
      "Epoch: 1292 mean train loss:  5.57168221e-02, mean val. rec. loss:  5.21291218e-02\n",
      "Epoch: 1293 mean train loss:  5.56035786e-02, mean val. rec. loss:  5.20211763e-02\n",
      "Epoch: 1294 mean train loss:  5.54905883e-02, mean val. rec. loss:  5.19134848e-02\n",
      "Epoch: 1295 mean train loss:  5.53778513e-02, mean val. rec. loss:  5.18061018e-02\n",
      "Epoch: 1296 mean train loss:  5.52654010e-02, mean val. rec. loss:  5.16989682e-02\n",
      "Epoch: 1297 mean train loss:  5.51531854e-02, mean val. rec. loss:  5.15921158e-02\n",
      "Epoch: 1298 mean train loss:  5.50412492e-02, mean val. rec. loss:  5.14855402e-02\n",
      "Epoch: 1299 mean train loss:  5.49295699e-02, mean val. rec. loss:  5.13792412e-02\n",
      "Epoch: 1300 mean train loss:  5.48181513e-02, mean val. rec. loss:  5.12732144e-02\n",
      "Epoch: 1301 mean train loss:  5.47069934e-02, mean val. rec. loss:  5.11674597e-02\n",
      "Epoch: 1302 mean train loss:  5.45960776e-02, mean val. rec. loss:  5.10619636e-02\n",
      "Epoch: 1303 mean train loss:  5.44854300e-02, mean val. rec. loss:  5.09567533e-02\n",
      "Epoch: 1304 mean train loss:  5.43750580e-02, mean val. rec. loss:  5.08518151e-02\n",
      "Epoch: 1305 mean train loss:  5.42649392e-02, mean val. rec. loss:  5.07471309e-02\n",
      "Epoch: 1306 mean train loss:  5.41550699e-02, mean val. rec. loss:  5.06427461e-02\n",
      "Epoch: 1307 mean train loss:  5.40454614e-02, mean val. rec. loss:  5.05386153e-02\n",
      "Epoch: 1308 mean train loss:  5.39361210e-02, mean val. rec. loss:  5.04347657e-02\n",
      "Epoch: 1309 mean train loss:  5.38270265e-02, mean val. rec. loss:  5.03311838e-02\n",
      "Epoch: 1310 mean train loss:  5.37182149e-02, mean val. rec. loss:  5.02278694e-02\n",
      "Epoch: 1311 mean train loss:  5.36096418e-02, mean val. rec. loss:  5.01248318e-02\n",
      "Epoch: 1312 mean train loss:  5.35013293e-02, mean val. rec. loss:  5.00220527e-02\n",
      "Epoch: 1313 mean train loss:  5.33932776e-02, mean val. rec. loss:  4.99195593e-02\n",
      "Epoch: 1314 mean train loss:  5.32854940e-02, mean val. rec. loss:  4.98173336e-02\n",
      "Epoch: 1315 mean train loss:  5.31779674e-02, mean val. rec. loss:  4.97153755e-02\n",
      "Epoch: 1316 mean train loss:  5.30706978e-02, mean val. rec. loss:  4.96136805e-02\n",
      "Epoch: 1317 mean train loss:  5.29636814e-02, mean val. rec. loss:  4.95122712e-02\n",
      "Epoch: 1318 mean train loss:  5.28569295e-02, mean val. rec. loss:  4.94111160e-02\n",
      "Epoch: 1319 mean train loss:  5.27504345e-02, mean val. rec. loss:  4.93102374e-02\n",
      "Epoch: 1320 mean train loss:  5.26442003e-02, mean val. rec. loss:  4.92096401e-02\n",
      "Epoch: 1321 mean train loss:  5.25382230e-02, mean val. rec. loss:  4.91092877e-02\n",
      "Epoch: 1322 mean train loss:  5.24324990e-02, mean val. rec. loss:  4.90092256e-02\n",
      "Epoch: 1323 mean train loss:  5.23270432e-02, mean val. rec. loss:  4.89094175e-02\n",
      "Epoch: 1324 mean train loss:  5.22218369e-02, mean val. rec. loss:  4.88098816e-02\n",
      "Epoch: 1325 mean train loss:  5.21168987e-02, mean val. rec. loss:  4.87106133e-02\n",
      "Epoch: 1326 mean train loss:  5.20122063e-02, mean val. rec. loss:  4.86116171e-02\n",
      "Epoch: 1327 mean train loss:  5.19077784e-02, mean val. rec. loss:  4.85128931e-02\n",
      "Epoch: 1328 mean train loss:  5.18036075e-02, mean val. rec. loss:  4.84144277e-02\n",
      "Epoch: 1329 mean train loss:  5.16996898e-02, mean val. rec. loss:  4.83162389e-02\n",
      "Epoch: 1330 mean train loss:  5.15960403e-02, mean val. rec. loss:  4.82183041e-02\n",
      "Epoch: 1331 mean train loss:  5.14926440e-02, mean val. rec. loss:  4.81206461e-02\n",
      "Epoch: 1332 mean train loss:  5.13895010e-02, mean val. rec. loss:  4.80232556e-02\n",
      "Epoch: 1333 mean train loss:  5.12866262e-02, mean val. rec. loss:  4.79261237e-02\n",
      "Epoch: 1334 mean train loss:  5.11840008e-02, mean val. rec. loss:  4.78292731e-02\n",
      "Epoch: 1335 mean train loss:  5.10816288e-02, mean val. rec. loss:  4.77326673e-02\n",
      "Epoch: 1336 mean train loss:  5.09795211e-02, mean val. rec. loss:  4.76363564e-02\n",
      "Epoch: 1337 mean train loss:  5.08776817e-02, mean val. rec. loss:  4.75402859e-02\n",
      "Epoch: 1338 mean train loss:  5.07760731e-02, mean val. rec. loss:  4.74444921e-02\n",
      "Epoch: 1339 mean train loss:  5.06747402e-02, mean val. rec. loss:  4.73489659e-02\n",
      "Epoch: 1340 mean train loss:  5.05736605e-02, mean val. rec. loss:  4.72536938e-02\n",
      "Epoch: 1341 mean train loss:  5.04728340e-02, mean val. rec. loss:  4.71586937e-02\n",
      "Epoch: 1342 mean train loss:  5.03722645e-02, mean val. rec. loss:  4.70639704e-02\n",
      "Epoch: 1343 mean train loss:  5.02719595e-02, mean val. rec. loss:  4.69695011e-02\n",
      "Epoch: 1344 mean train loss:  5.01718966e-02, mean val. rec. loss:  4.68753084e-02\n",
      "Epoch: 1345 mean train loss:  5.00721092e-02, mean val. rec. loss:  4.67813698e-02\n",
      "Epoch: 1346 mean train loss:  4.99725751e-02, mean val. rec. loss:  4.66876943e-02\n",
      "Epoch: 1347 mean train loss:  4.98732980e-02, mean val. rec. loss:  4.65943000e-02\n",
      "Epoch: 1348 mean train loss:  4.97742668e-02, mean val. rec. loss:  4.65011415e-02\n",
      "Epoch: 1349 mean train loss:  4.96754962e-02, mean val. rec. loss:  4.64082734e-02\n",
      "Epoch: 1350 mean train loss:  4.95769863e-02, mean val. rec. loss:  4.63156502e-02\n",
      "Epoch: 1351 mean train loss:  4.94787222e-02, mean val. rec. loss:  4.62232991e-02\n",
      "Epoch: 1352 mean train loss:  4.93807189e-02, mean val. rec. loss:  4.61312021e-02\n",
      "Epoch: 1353 mean train loss:  4.92829762e-02, mean val. rec. loss:  4.60393817e-02\n",
      "Epoch: 1354 mean train loss:  4.91854943e-02, mean val. rec. loss:  4.59478244e-02\n",
      "Epoch: 1355 mean train loss:  4.90882618e-02, mean val. rec. loss:  4.58565167e-02\n",
      "Epoch: 1356 mean train loss:  4.89912790e-02, mean val. rec. loss:  4.57654719e-02\n",
      "Epoch: 1357 mean train loss:  4.88945531e-02, mean val. rec. loss:  4.56747085e-02\n",
      "Epoch: 1358 mean train loss:  4.87980916e-02, mean val. rec. loss:  4.55841899e-02\n",
      "Epoch: 1359 mean train loss:  4.87018834e-02, mean val. rec. loss:  4.54939435e-02\n",
      "Epoch: 1360 mean train loss:  4.86059284e-02, mean val. rec. loss:  4.54039602e-02\n",
      "Epoch: 1361 mean train loss:  4.85102305e-02, mean val. rec. loss:  4.53142309e-02\n",
      "Epoch: 1362 mean train loss:  4.84147857e-02, mean val. rec. loss:  4.52247647e-02\n",
      "Epoch: 1363 mean train loss:  4.83195980e-02, mean val. rec. loss:  4.51355706e-02\n",
      "Epoch: 1364 mean train loss:  4.82246784e-02, mean val. rec. loss:  4.50466351e-02\n",
      "Epoch: 1365 mean train loss:  4.81300047e-02, mean val. rec. loss:  4.49579627e-02\n",
      "Epoch: 1366 mean train loss:  4.80355767e-02, mean val. rec. loss:  4.48695352e-02\n",
      "Epoch: 1367 mean train loss:  4.79414169e-02, mean val. rec. loss:  4.47813889e-02\n",
      "Epoch: 1368 mean train loss:  4.78475178e-02, mean val. rec. loss:  4.46934921e-02\n",
      "Epoch: 1369 mean train loss:  4.77538571e-02, mean val. rec. loss:  4.46058675e-02\n",
      "Epoch: 1370 mean train loss:  4.76604645e-02, mean val. rec. loss:  4.45184878e-02\n",
      "Epoch: 1371 mean train loss:  4.75673252e-02, mean val. rec. loss:  4.44313802e-02\n",
      "Epoch: 1372 mean train loss:  4.74744317e-02, mean val. rec. loss:  4.43445312e-02\n",
      "Epoch: 1373 mean train loss:  4.73817989e-02, mean val. rec. loss:  4.42579362e-02\n",
      "Epoch: 1374 mean train loss:  4.72894268e-02, mean val. rec. loss:  4.41716089e-02\n",
      "Epoch: 1375 mean train loss:  4.71973042e-02, mean val. rec. loss:  4.40855264e-02\n",
      "Epoch: 1376 mean train loss:  4.71054237e-02, mean val. rec. loss:  4.39997116e-02\n",
      "Epoch: 1377 mean train loss:  4.70138077e-02, mean val. rec. loss:  4.39141690e-02\n",
      "Epoch: 1378 mean train loss:  4.69224561e-02, mean val. rec. loss:  4.38288667e-02\n",
      "Epoch: 1379 mean train loss:  4.68313428e-02, mean val. rec. loss:  4.37438185e-02\n",
      "Epoch: 1380 mean train loss:  4.67404865e-02, mean val. rec. loss:  4.36590424e-02\n",
      "Epoch: 1381 mean train loss:  4.66498910e-02, mean val. rec. loss:  4.35745294e-02\n",
      "Epoch: 1382 mean train loss:  4.65595449e-02, mean val. rec. loss:  4.34902613e-02\n",
      "Epoch: 1383 mean train loss:  4.64694447e-02, mean val. rec. loss:  4.34062563e-02\n",
      "Epoch: 1384 mean train loss:  4.63796164e-02, mean val. rec. loss:  4.33225235e-02\n",
      "Epoch: 1385 mean train loss:  4.62900413e-02, mean val. rec. loss:  4.32390220e-02\n",
      "Epoch: 1386 mean train loss:  4.62006971e-02, mean val. rec. loss:  4.31558062e-02\n",
      "Epoch: 1387 mean train loss:  4.61116360e-02, mean val. rec. loss:  4.30728218e-02\n",
      "Epoch: 1388 mean train loss:  4.60228095e-02, mean val. rec. loss:  4.29901050e-02\n",
      "Epoch: 1389 mean train loss:  4.59342363e-02, mean val. rec. loss:  4.29076468e-02\n",
      "Epoch: 1390 mean train loss:  4.58459275e-02, mean val. rec. loss:  4.28254426e-02\n",
      "Epoch: 1391 mean train loss:  4.57578533e-02, mean val. rec. loss:  4.27434969e-02\n",
      "Epoch: 1392 mean train loss:  4.56700436e-02, mean val. rec. loss:  4.26617961e-02\n",
      "Epoch: 1393 mean train loss:  4.55824834e-02, mean val. rec. loss:  4.25803676e-02\n",
      "Epoch: 1394 mean train loss:  4.54951764e-02, mean val. rec. loss:  4.24991794e-02\n",
      "Epoch: 1395 mean train loss:  4.54081227e-02, mean val. rec. loss:  4.24182633e-02\n",
      "Epoch: 1396 mean train loss:  4.53213372e-02, mean val. rec. loss:  4.23375968e-02\n",
      "Epoch: 1397 mean train loss:  4.52347751e-02, mean val. rec. loss:  4.22571752e-02\n",
      "Epoch: 1398 mean train loss:  4.51484812e-02, mean val. rec. loss:  4.21770166e-02\n",
      "Epoch: 1399 mean train loss:  4.50624331e-02, mean val. rec. loss:  4.20971212e-02\n",
      "Epoch: 1400 mean train loss:  4.49766457e-02, mean val. rec. loss:  4.20174525e-02\n",
      "Epoch: 1401 mean train loss:  4.48911079e-02, mean val. rec. loss:  4.19380651e-02\n",
      "Epoch: 1402 mean train loss:  4.48058158e-02, mean val. rec. loss:  4.18589135e-02\n",
      "Epoch: 1403 mean train loss:  4.47207770e-02, mean val. rec. loss:  4.17800296e-02\n",
      "Epoch: 1404 mean train loss:  4.46359915e-02, mean val. rec. loss:  4.17013905e-02\n",
      "Epoch: 1405 mean train loss:  4.45514555e-02, mean val. rec. loss:  4.16230101e-02\n",
      "Epoch: 1406 mean train loss:  4.44671765e-02, mean val. rec. loss:  4.15448927e-02\n",
      "Epoch: 1407 mean train loss:  4.43831507e-02, mean val. rec. loss:  4.14670203e-02\n",
      "Epoch: 1408 mean train loss:  4.42993708e-02, mean val. rec. loss:  4.13893973e-02\n",
      "Epoch: 1409 mean train loss:  4.42158478e-02, mean val. rec. loss:  4.13120193e-02\n",
      "Epoch: 1410 mean train loss:  4.41325595e-02, mean val. rec. loss:  4.12348953e-02\n",
      "Epoch: 1411 mean train loss:  4.40495207e-02, mean val. rec. loss:  4.11580207e-02\n",
      "Epoch: 1412 mean train loss:  4.39667426e-02, mean val. rec. loss:  4.10813956e-02\n",
      "Epoch: 1413 mean train loss:  4.38842029e-02, mean val. rec. loss:  4.10050382e-02\n",
      "Epoch: 1414 mean train loss:  4.38019313e-02, mean val. rec. loss:  4.09289257e-02\n",
      "Epoch: 1415 mean train loss:  4.37198981e-02, mean val. rec. loss:  4.08530490e-02\n",
      "Epoch: 1416 mean train loss:  4.36381181e-02, mean val. rec. loss:  4.07774355e-02\n",
      "Epoch: 1417 mean train loss:  4.35565840e-02, mean val. rec. loss:  4.07020759e-02\n",
      "Epoch: 1418 mean train loss:  4.34753031e-02, mean val. rec. loss:  4.06269659e-02\n",
      "Epoch: 1419 mean train loss:  4.33942680e-02, mean val. rec. loss:  4.05520917e-02\n",
      "Epoch: 1420 mean train loss:  4.33134862e-02, mean val. rec. loss:  4.04774760e-02\n",
      "Epoch: 1421 mean train loss:  4.32329464e-02, mean val. rec. loss:  4.04031143e-02\n",
      "Epoch: 1422 mean train loss:  4.31526637e-02, mean val. rec. loss:  4.03289886e-02\n",
      "Epoch: 1423 mean train loss:  4.30726267e-02, mean val. rec. loss:  4.02551349e-02\n",
      "Epoch: 1424 mean train loss:  4.29928393e-02, mean val. rec. loss:  4.01815081e-02\n",
      "Epoch: 1425 mean train loss:  4.29133014e-02, mean val. rec. loss:  4.01081262e-02\n",
      "Epoch: 1426 mean train loss:  4.28340019e-02, mean val. rec. loss:  4.00350074e-02\n",
      "Epoch: 1427 mean train loss:  4.27549519e-02, mean val. rec. loss:  3.99621245e-02\n",
      "Epoch: 1428 mean train loss:  4.26761552e-02, mean val. rec. loss:  3.98894819e-02\n",
      "Epoch: 1429 mean train loss:  4.25976043e-02, mean val. rec. loss:  3.98170979e-02\n",
      "Epoch: 1430 mean train loss:  4.25192954e-02, mean val. rec. loss:  3.97449725e-02\n",
      "Epoch: 1431 mean train loss:  4.24412361e-02, mean val. rec. loss:  3.96730784e-02\n",
      "Epoch: 1432 mean train loss:  4.23634264e-02, mean val. rec. loss:  3.96014246e-02\n",
      "Epoch: 1433 mean train loss:  4.22858624e-02, mean val. rec. loss:  3.95300113e-02\n",
      "Epoch: 1434 mean train loss:  4.22085331e-02, mean val. rec. loss:  3.94588656e-02\n",
      "Epoch: 1435 mean train loss:  4.21314570e-02, mean val. rec. loss:  3.93879467e-02\n",
      "Epoch: 1436 mean train loss:  4.20546342e-02, mean val. rec. loss:  3.93172819e-02\n",
      "Epoch: 1437 mean train loss:  4.19780535e-02, mean val. rec. loss:  3.92468665e-02\n",
      "Epoch: 1438 mean train loss:  4.19017148e-02, mean val. rec. loss:  3.91766778e-02\n",
      "Epoch: 1439 mean train loss:  4.18256257e-02, mean val. rec. loss:  3.91067432e-02\n",
      "Epoch: 1440 mean train loss:  4.17497750e-02, mean val. rec. loss:  3.90370490e-02\n",
      "Epoch: 1441 mean train loss:  4.16741738e-02, mean val. rec. loss:  3.89676043e-02\n",
      "Epoch: 1442 mean train loss:  4.15988109e-02, mean val. rec. loss:  3.88983909e-02\n",
      "Epoch: 1443 mean train loss:  4.15236976e-02, mean val. rec. loss:  3.88294270e-02\n",
      "Epoch: 1444 mean train loss:  4.14488264e-02, mean val. rec. loss:  3.87607080e-02\n",
      "Epoch: 1445 mean train loss:  4.13742084e-02, mean val. rec. loss:  3.86922113e-02\n",
      "Epoch: 1446 mean train loss:  4.12998102e-02, mean val. rec. loss:  3.86239686e-02\n",
      "Epoch: 1447 mean train loss:  4.12256652e-02, mean val. rec. loss:  3.85559708e-02\n",
      "Epoch: 1448 mean train loss:  4.11517697e-02, mean val. rec. loss:  3.84882089e-02\n",
      "Epoch: 1449 mean train loss:  4.10781127e-02, mean val. rec. loss:  3.84206919e-02\n",
      "Epoch: 1450 mean train loss:  4.10046977e-02, mean val. rec. loss:  3.83534063e-02\n",
      "Epoch: 1451 mean train loss:  4.09315248e-02, mean val. rec. loss:  3.82863611e-02\n",
      "Epoch: 1452 mean train loss:  4.08585865e-02, mean val. rec. loss:  3.82195562e-02\n",
      "Epoch: 1453 mean train loss:  4.07859015e-02, mean val. rec. loss:  3.81530009e-02\n",
      "Epoch: 1454 mean train loss:  4.07134511e-02, mean val. rec. loss:  3.80866723e-02\n",
      "Epoch: 1455 mean train loss:  4.06412353e-02, mean val. rec. loss:  3.80205796e-02\n",
      "Epoch: 1456 mean train loss:  4.05692766e-02, mean val. rec. loss:  3.79547228e-02\n",
      "Epoch: 1457 mean train loss:  4.04975413e-02, mean val. rec. loss:  3.78891155e-02\n",
      "Epoch: 1458 mean train loss:  4.04260629e-02, mean val. rec. loss:  3.78237326e-02\n",
      "Epoch: 1459 mean train loss:  4.03548155e-02, mean val. rec. loss:  3.77585902e-02\n",
      "Epoch: 1460 mean train loss:  4.02838065e-02, mean val. rec. loss:  3.76936995e-02\n",
      "Epoch: 1461 mean train loss:  4.02130432e-02, mean val. rec. loss:  3.76290130e-02\n",
      "Epoch: 1462 mean train loss:  4.01425109e-02, mean val. rec. loss:  3.75645895e-02\n",
      "Epoch: 1463 mean train loss:  4.00722244e-02, mean val. rec. loss:  3.75003837e-02\n",
      "Epoch: 1464 mean train loss:  4.00021762e-02, mean val. rec. loss:  3.74364184e-02\n",
      "Epoch: 1465 mean train loss:  3.99323590e-02, mean val. rec. loss:  3.73726934e-02\n",
      "Epoch: 1466 mean train loss:  3.98627801e-02, mean val. rec. loss:  3.73091930e-02\n",
      "Epoch: 1467 mean train loss:  3.97934396e-02, mean val. rec. loss:  3.72459239e-02\n",
      "Epoch: 1468 mean train loss:  3.97243374e-02, mean val. rec. loss:  3.71828839e-02\n",
      "Epoch: 1469 mean train loss:  3.96554736e-02, mean val. rec. loss:  3.71200911e-02\n",
      "Epoch: 1470 mean train loss:  3.95868407e-02, mean val. rec. loss:  3.70575205e-02\n",
      "Epoch: 1471 mean train loss:  3.95184499e-02, mean val. rec. loss:  3.69951836e-02\n",
      "Epoch: 1472 mean train loss:  3.94502974e-02, mean val. rec. loss:  3.69330734e-02\n",
      "Epoch: 1473 mean train loss:  3.93823685e-02, mean val. rec. loss:  3.68711946e-02\n",
      "Epoch: 1474 mean train loss:  3.93146853e-02, mean val. rec. loss:  3.68095539e-02\n",
      "Epoch: 1475 mean train loss:  3.92472293e-02, mean val. rec. loss:  3.67481354e-02\n",
      "Epoch: 1476 mean train loss:  3.91800117e-02, mean val. rec. loss:  3.66869551e-02\n",
      "Epoch: 1477 mean train loss:  3.91130324e-02, mean val. rec. loss:  3.66259948e-02\n",
      "Epoch: 1478 mean train loss:  3.90462766e-02, mean val. rec. loss:  3.65652613e-02\n",
      "Epoch: 1479 mean train loss:  3.89797629e-02, mean val. rec. loss:  3.65047568e-02\n",
      "Epoch: 1480 mean train loss:  3.89134726e-02, mean val. rec. loss:  3.64444882e-02\n",
      "Epoch: 1481 mean train loss:  3.88474282e-02, mean val. rec. loss:  3.63844396e-02\n",
      "Epoch: 1482 mean train loss:  3.87816109e-02, mean val. rec. loss:  3.63246224e-02\n",
      "Epoch: 1483 mean train loss:  3.87160246e-02, mean val. rec. loss:  3.62650251e-02\n",
      "Epoch: 1484 mean train loss:  3.86506654e-02, mean val. rec. loss:  3.62056478e-02\n",
      "Epoch: 1485 mean train loss:  3.85855334e-02, mean val. rec. loss:  3.61465019e-02\n",
      "Epoch: 1486 mean train loss:  3.85206361e-02, mean val. rec. loss:  3.60875759e-02\n",
      "Epoch: 1487 mean train loss:  3.84559734e-02, mean val. rec. loss:  3.60288881e-02\n",
      "Epoch: 1488 mean train loss:  3.83915379e-02, mean val. rec. loss:  3.59704157e-02\n",
      "Epoch: 1489 mean train loss:  3.83273370e-02, mean val. rec. loss:  3.59121724e-02\n",
      "Epoch: 1490 mean train loss:  3.82633670e-02, mean val. rec. loss:  3.58541446e-02\n",
      "Epoch: 1491 mean train loss:  3.81996205e-02, mean val. rec. loss:  3.57963390e-02\n",
      "Epoch: 1492 mean train loss:  3.81360975e-02, mean val. rec. loss:  3.57387556e-02\n",
      "Epoch: 1493 mean train loss:  3.80728091e-02, mean val. rec. loss:  3.56813991e-02\n",
      "Epoch: 1494 mean train loss:  3.80097441e-02, mean val. rec. loss:  3.56242512e-02\n",
      "Epoch: 1495 mean train loss:  3.79469064e-02, mean val. rec. loss:  3.55673324e-02\n",
      "Epoch: 1496 mean train loss:  3.78843070e-02, mean val. rec. loss:  3.55106336e-02\n",
      "Epoch: 1497 mean train loss:  3.78219236e-02, mean val. rec. loss:  3.54541479e-02\n",
      "Epoch: 1498 mean train loss:  3.77597562e-02, mean val. rec. loss:  3.53978868e-02\n",
      "Epoch: 1499 mean train loss:  3.76978272e-02, mean val. rec. loss:  3.53418457e-02\n",
      "Epoch: 1500 mean train loss:  3.76361217e-02, mean val. rec. loss:  3.52860155e-02\n",
      "Epoch: 1501 mean train loss:  3.75746396e-02, mean val. rec. loss:  3.52304030e-02\n",
      "Epoch: 1502 mean train loss:  3.75133810e-02, mean val. rec. loss:  3.51750105e-02\n",
      "Epoch: 1503 mean train loss:  3.74523458e-02, mean val. rec. loss:  3.51198357e-02\n",
      "Epoch: 1504 mean train loss:  3.73915378e-02, mean val. rec. loss:  3.50648674e-02\n",
      "Epoch: 1505 mean train loss:  3.73309422e-02, mean val. rec. loss:  3.50101212e-02\n",
      "Epoch: 1506 mean train loss:  3.72705811e-02, mean val. rec. loss:  3.49555883e-02\n",
      "Epoch: 1507 mean train loss:  3.72104324e-02, mean val. rec. loss:  3.49012708e-02\n",
      "Epoch: 1508 mean train loss:  3.71505145e-02, mean val. rec. loss:  3.48471688e-02\n",
      "Epoch: 1509 mean train loss:  3.70908127e-02, mean val. rec. loss:  3.47932732e-02\n",
      "Epoch: 1510 mean train loss:  3.70313343e-02, mean val. rec. loss:  3.47395952e-02\n",
      "Epoch: 1511 mean train loss:  3.69720682e-02, mean val. rec. loss:  3.46861214e-02\n",
      "Epoch: 1512 mean train loss:  3.69130256e-02, mean val. rec. loss:  3.46328654e-02\n",
      "Epoch: 1513 mean train loss:  3.68541990e-02, mean val. rec. loss:  3.45798157e-02\n",
      "Epoch: 1514 mean train loss:  3.67955996e-02, mean val. rec. loss:  3.45269769e-02\n",
      "Epoch: 1515 mean train loss:  3.67372162e-02, mean val. rec. loss:  3.44743581e-02\n",
      "Epoch: 1516 mean train loss:  3.66790451e-02, mean val. rec. loss:  3.44219321e-02\n",
      "Epoch: 1517 mean train loss:  3.66210900e-02, mean val. rec. loss:  3.43697306e-02\n",
      "Epoch: 1518 mean train loss:  3.65633658e-02, mean val. rec. loss:  3.43177242e-02\n",
      "Epoch: 1519 mean train loss:  3.65058465e-02, mean val. rec. loss:  3.42659332e-02\n",
      "Epoch: 1520 mean train loss:  3.64485432e-02, mean val. rec. loss:  3.42143418e-02\n",
      "Epoch: 1521 mean train loss:  3.63914484e-02, mean val. rec. loss:  3.41629591e-02\n",
      "Epoch: 1522 mean train loss:  3.63345734e-02, mean val. rec. loss:  3.41117895e-02\n",
      "Epoch: 1523 mean train loss:  3.62779256e-02, mean val. rec. loss:  3.40608127e-02\n",
      "Epoch: 1524 mean train loss:  3.62214826e-02, mean val. rec. loss:  3.40100446e-02\n",
      "Epoch: 1525 mean train loss:  3.61652481e-02, mean val. rec. loss:  3.39594715e-02\n",
      "Epoch: 1526 mean train loss:  3.61092223e-02, mean val. rec. loss:  3.39091207e-02\n",
      "Epoch: 1527 mean train loss:  3.60534199e-02, mean val. rec. loss:  3.38589626e-02\n",
      "Epoch: 1528 mean train loss:  3.59978223e-02, mean val. rec. loss:  3.38090042e-02\n",
      "Epoch: 1529 mean train loss:  3.59424333e-02, mean val. rec. loss:  3.37592407e-02\n",
      "Epoch: 1530 mean train loss:  3.58872641e-02, mean val. rec. loss:  3.37096950e-02\n",
      "Epoch: 1531 mean train loss:  3.58323034e-02, mean val. rec. loss:  3.36603308e-02\n",
      "Epoch: 1532 mean train loss:  3.57775438e-02, mean val. rec. loss:  3.36111887e-02\n",
      "Epoch: 1533 mean train loss:  3.57230114e-02, mean val. rec. loss:  3.35622327e-02\n",
      "Epoch: 1534 mean train loss:  3.56686727e-02, mean val. rec. loss:  3.35134672e-02\n",
      "Epoch: 1535 mean train loss:  3.56145388e-02, mean val. rec. loss:  3.34649171e-02\n",
      "Epoch: 1536 mean train loss:  3.55606172e-02, mean val. rec. loss:  3.34165485e-02\n",
      "Epoch: 1537 mean train loss:  3.55069079e-02, mean val. rec. loss:  3.33683817e-02\n",
      "Epoch: 1538 mean train loss:  3.54533960e-02, mean val. rec. loss:  3.33204099e-02\n",
      "Epoch: 1539 mean train loss:  3.54000890e-02, mean val. rec. loss:  3.32726423e-02\n",
      "Epoch: 1540 mean train loss:  3.53469979e-02, mean val. rec. loss:  3.32250584e-02\n",
      "Epoch: 1541 mean train loss:  3.52941042e-02, mean val. rec. loss:  3.31776695e-02\n",
      "Epoch: 1542 mean train loss:  3.52414154e-02, mean val. rec. loss:  3.31304802e-02\n",
      "Epoch: 1543 mean train loss:  3.51889314e-02, mean val. rec. loss:  3.30834769e-02\n",
      "Epoch: 1544 mean train loss:  3.51366449e-02, mean val. rec. loss:  3.30366663e-02\n",
      "Epoch: 1545 mean train loss:  3.50845631e-02, mean val. rec. loss:  3.29900508e-02\n",
      "Epoch: 1546 mean train loss:  3.50326899e-02, mean val. rec. loss:  3.29436258e-02\n",
      "Epoch: 1547 mean train loss:  3.49810141e-02, mean val. rec. loss:  3.28973822e-02\n",
      "Epoch: 1548 mean train loss:  3.49295246e-02, mean val. rec. loss:  3.28513473e-02\n",
      "Epoch: 1549 mean train loss:  3.48782659e-02, mean val. rec. loss:  3.28054802e-02\n",
      "Epoch: 1550 mean train loss:  3.48271860e-02, mean val. rec. loss:  3.27598127e-02\n",
      "Epoch: 1551 mean train loss:  3.47763110e-02, mean val. rec. loss:  3.27143335e-02\n",
      "Epoch: 1552 mean train loss:  3.47256259e-02, mean val. rec. loss:  3.26690311e-02\n",
      "Epoch: 1553 mean train loss:  3.46751345e-02, mean val. rec. loss:  3.26239283e-02\n",
      "Epoch: 1554 mean train loss:  3.46248590e-02, mean val. rec. loss:  3.25789934e-02\n",
      "Epoch: 1555 mean train loss:  3.45747624e-02, mean val. rec. loss:  3.25342602e-02\n",
      "Epoch: 1556 mean train loss:  3.45248706e-02, mean val. rec. loss:  3.24897018e-02\n",
      "Epoch: 1557 mean train loss:  3.44751724e-02, mean val. rec. loss:  3.24453315e-02\n",
      "Epoch: 1558 mean train loss:  3.44256680e-02, mean val. rec. loss:  3.24011405e-02\n",
      "Epoch: 1559 mean train loss:  3.43763572e-02, mean val. rec. loss:  3.23571354e-02\n",
      "Epoch: 1560 mean train loss:  3.43272475e-02, mean val. rec. loss:  3.23132981e-02\n",
      "Epoch: 1561 mean train loss:  3.42783128e-02, mean val. rec. loss:  3.22696627e-02\n",
      "Epoch: 1562 mean train loss:  3.42295830e-02, mean val. rec. loss:  3.22261951e-02\n",
      "Epoch: 1563 mean train loss:  3.41810432e-02, mean val. rec. loss:  3.21829089e-02\n",
      "Epoch: 1564 mean train loss:  3.41327007e-02, mean val. rec. loss:  3.21398042e-02\n",
      "Epoch: 1565 mean train loss:  3.40845370e-02, mean val. rec. loss:  3.20968719e-02\n",
      "Epoch: 1566 mean train loss:  3.40365707e-02, mean val. rec. loss:  3.20541164e-02\n",
      "Epoch: 1567 mean train loss:  3.39887832e-02, mean val. rec. loss:  3.20115560e-02\n",
      "Epoch: 1568 mean train loss:  3.39412042e-02, mean val. rec. loss:  3.19691498e-02\n",
      "Epoch: 1569 mean train loss:  3.38937891e-02, mean val. rec. loss:  3.19269296e-02\n",
      "Epoch: 1570 mean train loss:  3.38465751e-02, mean val. rec. loss:  3.18848840e-02\n",
      "Epoch: 1571 mean train loss:  3.37995548e-02, mean val. rec. loss:  3.18430108e-02\n",
      "Epoch: 1572 mean train loss:  3.37527096e-02, mean val. rec. loss:  3.18013031e-02\n",
      "Epoch: 1573 mean train loss:  3.37060505e-02, mean val. rec. loss:  3.17597860e-02\n",
      "Epoch: 1574 mean train loss:  3.36595777e-02, mean val. rec. loss:  3.17184231e-02\n",
      "Epoch: 1575 mean train loss:  3.36132836e-02, mean val. rec. loss:  3.16772461e-02\n",
      "Epoch: 1576 mean train loss:  3.35671907e-02, mean val. rec. loss:  3.16362392e-02\n",
      "Epoch: 1577 mean train loss:  3.35212691e-02, mean val. rec. loss:  3.15953957e-02\n",
      "Epoch: 1578 mean train loss:  3.34755262e-02, mean val. rec. loss:  3.15547222e-02\n",
      "Epoch: 1579 mean train loss:  3.34299622e-02, mean val. rec. loss:  3.15142211e-02\n",
      "Epoch: 1580 mean train loss:  3.33845880e-02, mean val. rec. loss:  3.14738765e-02\n",
      "Epoch: 1581 mean train loss:  3.33393778e-02, mean val. rec. loss:  3.14337110e-02\n",
      "Epoch: 1582 mean train loss:  3.32943723e-02, mean val. rec. loss:  3.13936998e-02\n",
      "Epoch: 1583 mean train loss:  3.32495308e-02, mean val. rec. loss:  3.13538586e-02\n",
      "Epoch: 1584 mean train loss:  3.32048680e-02, mean val. rec. loss:  3.13141967e-02\n",
      "Epoch: 1585 mean train loss:  3.31603840e-02, mean val. rec. loss:  3.12746844e-02\n",
      "Epoch: 1586 mean train loss:  3.31160713e-02, mean val. rec. loss:  3.12353422e-02\n",
      "Epoch: 1587 mean train loss:  3.30719486e-02, mean val. rec. loss:  3.11961543e-02\n",
      "Epoch: 1588 mean train loss:  3.30279897e-02, mean val. rec. loss:  3.11571319e-02\n",
      "Epoch: 1589 mean train loss:  3.29842059e-02, mean val. rec. loss:  3.11182841e-02\n",
      "Epoch: 1590 mean train loss:  3.29406083e-02, mean val. rec. loss:  3.10795905e-02\n",
      "Epoch: 1591 mean train loss:  3.28971745e-02, mean val. rec. loss:  3.10410467e-02\n",
      "Epoch: 1592 mean train loss:  3.28539121e-02, mean val. rec. loss:  3.10026752e-02\n",
      "Epoch: 1593 mean train loss:  3.28108285e-02, mean val. rec. loss:  3.09644556e-02\n",
      "Epoch: 1594 mean train loss:  3.27679236e-02, mean val. rec. loss:  3.09263994e-02\n",
      "Epoch: 1595 mean train loss:  3.27251752e-02, mean val. rec. loss:  3.08884996e-02\n",
      "Epoch: 1596 mean train loss:  3.26826092e-02, mean val. rec. loss:  3.08507495e-02\n",
      "Epoch: 1597 mean train loss:  3.26401997e-02, mean val. rec. loss:  3.08131627e-02\n",
      "Epoch: 1598 mean train loss:  3.25979689e-02, mean val. rec. loss:  3.07757347e-02\n",
      "Epoch: 1599 mean train loss:  3.25558983e-02, mean val. rec. loss:  3.07384518e-02\n",
      "Epoch: 1600 mean train loss:  3.25140065e-02, mean val. rec. loss:  3.07013141e-02\n",
      "Epoch: 1601 mean train loss:  3.24722674e-02, mean val. rec. loss:  3.06643464e-02\n",
      "Epoch: 1602 mean train loss:  3.24307033e-02, mean val. rec. loss:  3.06275285e-02\n",
      "Epoch: 1603 mean train loss:  3.23893068e-02, mean val. rec. loss:  3.05908557e-02\n",
      "Epoch: 1604 mean train loss:  3.23480742e-02, mean val. rec. loss:  3.05543462e-02\n",
      "Epoch: 1605 mean train loss:  3.23070017e-02, mean val. rec. loss:  3.05179773e-02\n",
      "Epoch: 1606 mean train loss:  3.22660968e-02, mean val. rec. loss:  3.04817513e-02\n",
      "Epoch: 1607 mean train loss:  3.22253484e-02, mean val. rec. loss:  3.04456931e-02\n",
      "Epoch: 1608 mean train loss:  3.21847712e-02, mean val. rec. loss:  3.04097732e-02\n",
      "Epoch: 1609 mean train loss:  3.21443505e-02, mean val. rec. loss:  3.03740031e-02\n",
      "Epoch: 1610 mean train loss:  3.21040974e-02, mean val. rec. loss:  3.03383758e-02\n",
      "Epoch: 1611 mean train loss:  3.20639970e-02, mean val. rec. loss:  3.03029005e-02\n",
      "Epoch: 1612 mean train loss:  3.20240605e-02, mean val. rec. loss:  3.02675590e-02\n",
      "Epoch: 1613 mean train loss:  3.19842766e-02, mean val. rec. loss:  3.02323739e-02\n",
      "Epoch: 1614 mean train loss:  3.19446567e-02, mean val. rec. loss:  3.01973295e-02\n",
      "Epoch: 1615 mean train loss:  3.19051857e-02, mean val. rec. loss:  3.01624303e-02\n",
      "Epoch: 1616 mean train loss:  3.18658860e-02, mean val. rec. loss:  3.01276693e-02\n",
      "Epoch: 1617 mean train loss:  3.18267353e-02, mean val. rec. loss:  3.00930604e-02\n",
      "Epoch: 1618 mean train loss:  3.17877485e-02, mean val. rec. loss:  3.00585875e-02\n",
      "Epoch: 1619 mean train loss:  3.17489106e-02, mean val. rec. loss:  3.00242529e-02\n",
      "Epoch: 1620 mean train loss:  3.17102218e-02, mean val. rec. loss:  2.99900658e-02\n",
      "Epoch: 1621 mean train loss:  3.16716893e-02, mean val. rec. loss:  2.99560102e-02\n",
      "Epoch: 1622 mean train loss:  3.16333170e-02, mean val. rec. loss:  2.99220953e-02\n",
      "Epoch: 1623 mean train loss:  3.15950900e-02, mean val. rec. loss:  2.98883209e-02\n",
      "Epoch: 1624 mean train loss:  3.15570193e-02, mean val. rec. loss:  2.98546713e-02\n",
      "Epoch: 1625 mean train loss:  3.15190903e-02, mean val. rec. loss:  2.98211804e-02\n",
      "Epoch: 1626 mean train loss:  3.14813250e-02, mean val. rec. loss:  2.97878120e-02\n",
      "Epoch: 1627 mean train loss:  3.14437051e-02, mean val. rec. loss:  2.97545752e-02\n",
      "Epoch: 1628 mean train loss:  3.14062192e-02, mean val. rec. loss:  2.97214880e-02\n",
      "Epoch: 1629 mean train loss:  3.13689046e-02, mean val. rec. loss:  2.96885188e-02\n",
      "Epoch: 1630 mean train loss:  3.13317204e-02, mean val. rec. loss:  2.96556879e-02\n",
      "Epoch: 1631 mean train loss:  3.12946963e-02, mean val. rec. loss:  2.96229840e-02\n",
      "Epoch: 1632 mean train loss:  3.12578063e-02, mean val. rec. loss:  2.95904207e-02\n",
      "Epoch: 1633 mean train loss:  3.12210727e-02, mean val. rec. loss:  2.95579845e-02\n",
      "Epoch: 1634 mean train loss:  3.11844770e-02, mean val. rec. loss:  2.95256707e-02\n",
      "Epoch: 1635 mean train loss:  3.11480227e-02, mean val. rec. loss:  2.94934975e-02\n",
      "Epoch: 1636 mean train loss:  3.11117175e-02, mean val. rec. loss:  2.94614558e-02\n",
      "Epoch: 1637 mean train loss:  3.10755500e-02, mean val. rec. loss:  2.94295389e-02\n",
      "Epoch: 1638 mean train loss:  3.10395334e-02, mean val. rec. loss:  2.93977445e-02\n",
      "Epoch: 1639 mean train loss:  3.10036527e-02, mean val. rec. loss:  2.93660839e-02\n",
      "Epoch: 1640 mean train loss:  3.09679117e-02, mean val. rec. loss:  2.93345457e-02\n",
      "Epoch: 1641 mean train loss:  3.09323140e-02, mean val. rec. loss:  2.93031300e-02\n",
      "Epoch: 1642 mean train loss:  3.08968505e-02, mean val. rec. loss:  2.92718459e-02\n",
      "Epoch: 1643 mean train loss:  3.08615322e-02, mean val. rec. loss:  2.92406842e-02\n",
      "Epoch: 1644 mean train loss:  3.08263479e-02, mean val. rec. loss:  2.92096450e-02\n",
      "Epoch: 1645 mean train loss:  3.07913090e-02, mean val. rec. loss:  2.91787351e-02\n",
      "Epoch: 1646 mean train loss:  3.07563966e-02, mean val. rec. loss:  2.91479430e-02\n",
      "Epoch: 1647 mean train loss:  3.07216314e-02, mean val. rec. loss:  2.91172690e-02\n",
      "Epoch: 1648 mean train loss:  3.06869928e-02, mean val. rec. loss:  2.90867219e-02\n",
      "Epoch: 1649 mean train loss:  3.06524938e-02, mean val. rec. loss:  2.90562837e-02\n",
      "Epoch: 1650 mean train loss:  3.06181215e-02, mean val. rec. loss:  2.90259748e-02\n",
      "Epoch: 1651 mean train loss:  3.05838907e-02, mean val. rec. loss:  2.89957861e-02\n",
      "Epoch: 1652 mean train loss:  3.05497903e-02, mean val. rec. loss:  2.89657198e-02\n",
      "Epoch: 1653 mean train loss:  3.05158295e-02, mean val. rec. loss:  2.89357624e-02\n",
      "Epoch: 1654 mean train loss:  3.04819954e-02, mean val. rec. loss:  2.89059162e-02\n",
      "Epoch: 1655 mean train loss:  3.04482823e-02, mean val. rec. loss:  2.88761992e-02\n",
      "Epoch: 1656 mean train loss:  3.04147144e-02, mean val. rec. loss:  2.88465910e-02\n",
      "Epoch: 1657 mean train loss:  3.03812695e-02, mean val. rec. loss:  2.88171031e-02\n",
      "Epoch: 1658 mean train loss:  3.03479568e-02, mean val. rec. loss:  2.87877331e-02\n",
      "Epoch: 1659 mean train loss:  3.03147725e-02, mean val. rec. loss:  2.87584720e-02\n",
      "Epoch: 1660 mean train loss:  3.02817187e-02, mean val. rec. loss:  2.87293152e-02\n",
      "Epoch: 1661 mean train loss:  3.02487802e-02, mean val. rec. loss:  2.87002808e-02\n",
      "Epoch: 1662 mean train loss:  3.02159759e-02, mean val. rec. loss:  2.86713576e-02\n",
      "Epoch: 1663 mean train loss:  3.01832982e-02, mean val. rec. loss:  2.86425478e-02\n",
      "Epoch: 1664 mean train loss:  3.01507489e-02, mean val. rec. loss:  2.86138378e-02\n",
      "Epoch: 1665 mean train loss:  3.01183133e-02, mean val. rec. loss:  2.85852503e-02\n",
      "Epoch: 1666 mean train loss:  3.00860117e-02, mean val. rec. loss:  2.85567648e-02\n",
      "Epoch: 1667 mean train loss:  3.00538275e-02, mean val. rec. loss:  2.85283881e-02\n",
      "Epoch: 1668 mean train loss:  3.00217662e-02, mean val. rec. loss:  2.85001294e-02\n",
      "Epoch: 1669 mean train loss:  2.99898315e-02, mean val. rec. loss:  2.84719751e-02\n",
      "Epoch: 1670 mean train loss:  2.99580215e-02, mean val. rec. loss:  2.84439159e-02\n",
      "Epoch: 1671 mean train loss:  2.99263233e-02, mean val. rec. loss:  2.84159702e-02\n",
      "Epoch: 1672 mean train loss:  2.98947480e-02, mean val. rec. loss:  2.83881334e-02\n",
      "Epoch: 1673 mean train loss:  2.98633012e-02, mean val. rec. loss:  2.83604054e-02\n",
      "Epoch: 1674 mean train loss:  2.98319680e-02, mean val. rec. loss:  2.83327704e-02\n",
      "Epoch: 1675 mean train loss:  2.98007502e-02, mean val. rec. loss:  2.83052442e-02\n",
      "Epoch: 1676 mean train loss:  2.97696479e-02, mean val. rec. loss:  2.82778201e-02\n",
      "Epoch: 1677 mean train loss:  2.97386667e-02, mean val. rec. loss:  2.82505049e-02\n",
      "Epoch: 1678 mean train loss:  2.97078009e-02, mean val. rec. loss:  2.82232827e-02\n",
      "Epoch: 1679 mean train loss:  2.96770505e-02, mean val. rec. loss:  2.81961647e-02\n",
      "Epoch: 1680 mean train loss:  2.96464156e-02, mean val. rec. loss:  2.81691557e-02\n",
      "Epoch: 1681 mean train loss:  2.96159018e-02, mean val. rec. loss:  2.81422396e-02\n",
      "Epoch: 1682 mean train loss:  2.95854922e-02, mean val. rec. loss:  2.81154301e-02\n",
      "Epoch: 1683 mean train loss:  2.95552037e-02, mean val. rec. loss:  2.80887069e-02\n",
      "Epoch: 1684 mean train loss:  2.95250194e-02, mean val. rec. loss:  2.80620879e-02\n",
      "Epoch: 1685 mean train loss:  2.94949562e-02, mean val. rec. loss:  2.80355710e-02\n",
      "Epoch: 1686 mean train loss:  2.94650010e-02, mean val. rec. loss:  2.80091493e-02\n",
      "Epoch: 1687 mean train loss:  2.94351613e-02, mean val. rec. loss:  2.79828275e-02\n",
      "Epoch: 1688 mean train loss:  2.94054295e-02, mean val. rec. loss:  2.79565986e-02\n",
      "Epoch: 1689 mean train loss:  2.93758095e-02, mean val. rec. loss:  2.79304650e-02\n",
      "Epoch: 1690 mean train loss:  2.93462901e-02, mean val. rec. loss:  2.79044243e-02\n",
      "Epoch: 1691 mean train loss:  2.93168861e-02, mean val. rec. loss:  2.78784789e-02\n",
      "Epoch: 1692 mean train loss:  2.92875938e-02, mean val. rec. loss:  2.78526379e-02\n",
      "Epoch: 1693 mean train loss:  2.92584115e-02, mean val. rec. loss:  2.78268785e-02\n",
      "Epoch: 1694 mean train loss:  2.92293259e-02, mean val. rec. loss:  2.78012234e-02\n",
      "Epoch: 1695 mean train loss:  2.92003614e-02, mean val. rec. loss:  2.77756545e-02\n",
      "Epoch: 1696 mean train loss:  2.91714919e-02, mean val. rec. loss:  2.77501717e-02\n",
      "Epoch: 1697 mean train loss:  2.91427303e-02, mean val. rec. loss:  2.77247888e-02\n",
      "Epoch: 1698 mean train loss:  2.91140731e-02, mean val. rec. loss:  2.76994966e-02\n",
      "Epoch: 1699 mean train loss:  2.90855238e-02, mean val. rec. loss:  2.76742860e-02\n",
      "Epoch: 1700 mean train loss:  2.90570733e-02, mean val. rec. loss:  2.76491753e-02\n",
      "Epoch: 1701 mean train loss:  2.90287307e-02, mean val. rec. loss:  2.76241461e-02\n",
      "Epoch: 1702 mean train loss:  2.90004850e-02, mean val. rec. loss:  2.75992100e-02\n",
      "Epoch: 1703 mean train loss:  2.89723473e-02, mean val. rec. loss:  2.75743577e-02\n",
      "Epoch: 1704 mean train loss:  2.89443046e-02, mean val. rec. loss:  2.75495962e-02\n",
      "Epoch: 1705 mean train loss:  2.89163624e-02, mean val. rec. loss:  2.75249141e-02\n",
      "Epoch: 1706 mean train loss:  2.88885246e-02, mean val. rec. loss:  2.75003295e-02\n",
      "Epoch: 1707 mean train loss:  2.88607891e-02, mean val. rec. loss:  2.74758220e-02\n",
      "Epoch: 1708 mean train loss:  2.88331449e-02, mean val. rec. loss:  2.74513984e-02\n",
      "Epoch: 1709 mean train loss:  2.88056012e-02, mean val. rec. loss:  2.74270723e-02\n",
      "Epoch: 1710 mean train loss:  2.87781600e-02, mean val. rec. loss:  2.74028211e-02\n",
      "Epoch: 1711 mean train loss:  2.87508174e-02, mean val. rec. loss:  2.73786561e-02\n",
      "Epoch: 1712 mean train loss:  2.87235699e-02, mean val. rec. loss:  2.73545727e-02\n",
      "Epoch: 1713 mean train loss:  2.86964173e-02, mean val. rec. loss:  2.73305732e-02\n",
      "Epoch: 1714 mean train loss:  2.86693652e-02, mean val. rec. loss:  2.73066554e-02\n",
      "Epoch: 1715 mean train loss:  2.86424007e-02, mean val. rec. loss:  2.72828237e-02\n",
      "Epoch: 1716 mean train loss:  2.86155386e-02, mean val. rec. loss:  2.72590669e-02\n",
      "Epoch: 1717 mean train loss:  2.85887659e-02, mean val. rec. loss:  2.72353963e-02\n",
      "Epoch: 1718 mean train loss:  2.85620919e-02, mean val. rec. loss:  2.72118050e-02\n",
      "Epoch: 1719 mean train loss:  2.85355091e-02, mean val. rec. loss:  2.71882909e-02\n",
      "Epoch: 1720 mean train loss:  2.85090232e-02, mean val. rec. loss:  2.71648539e-02\n",
      "Epoch: 1721 mean train loss:  2.84826210e-02, mean val. rec. loss:  2.71414985e-02\n",
      "Epoch: 1722 mean train loss:  2.84563157e-02, mean val. rec. loss:  2.71182270e-02\n",
      "Epoch: 1723 mean train loss:  2.84301073e-02, mean val. rec. loss:  2.70950236e-02\n",
      "Epoch: 1724 mean train loss:  2.84039807e-02, mean val. rec. loss:  2.70718995e-02\n",
      "Epoch: 1725 mean train loss:  2.83779492e-02, mean val. rec. loss:  2.70488617e-02\n",
      "Epoch: 1726 mean train loss:  2.83520070e-02, mean val. rec. loss:  2.70258873e-02\n",
      "Epoch: 1727 mean train loss:  2.83261505e-02, mean val. rec. loss:  2.70029991e-02\n",
      "Epoch: 1728 mean train loss:  2.83003871e-02, mean val. rec. loss:  2.69801835e-02\n",
      "Epoch: 1729 mean train loss:  2.82747112e-02, mean val. rec. loss:  2.69574337e-02\n",
      "Epoch: 1730 mean train loss:  2.82491210e-02, mean val. rec. loss:  2.69347768e-02\n",
      "Epoch: 1731 mean train loss:  2.82236220e-02, mean val. rec. loss:  2.69121812e-02\n",
      "Epoch: 1732 mean train loss:  2.81982068e-02, mean val. rec. loss:  2.68896604e-02\n",
      "Epoch: 1733 mean train loss:  2.81728792e-02, mean val. rec. loss:  2.68672167e-02\n",
      "Epoch: 1734 mean train loss:  2.81476353e-02, mean val. rec. loss:  2.68448434e-02\n",
      "Epoch: 1735 mean train loss:  2.81224808e-02, mean val. rec. loss:  2.68225449e-02\n",
      "Epoch: 1736 mean train loss:  2.80974083e-02, mean val. rec. loss:  2.68003167e-02\n",
      "Epoch: 1737 mean train loss:  2.80724251e-02, mean val. rec. loss:  2.67781565e-02\n",
      "Epoch: 1738 mean train loss:  2.80475239e-02, mean val. rec. loss:  2.67560689e-02\n",
      "Epoch: 1739 mean train loss:  2.80227028e-02, mean val. rec. loss:  2.67340516e-02\n",
      "Epoch: 1740 mean train loss:  2.79979673e-02, mean val. rec. loss:  2.67121069e-02\n",
      "Epoch: 1741 mean train loss:  2.79733119e-02, mean val. rec. loss:  2.66902325e-02\n",
      "Epoch: 1742 mean train loss:  2.79487459e-02, mean val. rec. loss:  2.66684261e-02\n",
      "Epoch: 1743 mean train loss:  2.79242544e-02, mean val. rec. loss:  2.66466901e-02\n",
      "Epoch: 1744 mean train loss:  2.78998485e-02, mean val. rec. loss:  2.66250175e-02\n",
      "Epoch: 1745 mean train loss:  2.78755208e-02, mean val. rec. loss:  2.66034175e-02\n",
      "Epoch: 1746 mean train loss:  2.78512770e-02, mean val. rec. loss:  2.65818810e-02\n",
      "Epoch: 1747 mean train loss:  2.78271076e-02, mean val. rec. loss:  2.65604103e-02\n",
      "Epoch: 1748 mean train loss:  2.78030183e-02, mean val. rec. loss:  2.65390122e-02\n",
      "Epoch: 1749 mean train loss:  2.77790128e-02, mean val. rec. loss:  2.65176775e-02\n",
      "Epoch: 1750 mean train loss:  2.77550818e-02, mean val. rec. loss:  2.64964064e-02\n",
      "Epoch: 1751 mean train loss:  2.77312253e-02, mean val. rec. loss:  2.64752056e-02\n",
      "Epoch: 1752 mean train loss:  2.77074544e-02, mean val. rec. loss:  2.64540637e-02\n",
      "Epoch: 1753 mean train loss:  2.76837581e-02, mean val. rec. loss:  2.64329922e-02\n",
      "Epoch: 1754 mean train loss:  2.76601380e-02, mean val. rec. loss:  2.64119796e-02\n",
      "Epoch: 1755 mean train loss:  2.76365944e-02, mean val. rec. loss:  2.63910396e-02\n",
      "Epoch: 1756 mean train loss:  2.76131270e-02, mean val. rec. loss:  2.63701540e-02\n",
      "Epoch: 1757 mean train loss:  2.75897361e-02, mean val. rec. loss:  2.63493364e-02\n",
      "Epoch: 1758 mean train loss:  2.75664177e-02, mean val. rec. loss:  2.63285801e-02\n",
      "Epoch: 1759 mean train loss:  2.75431776e-02, mean val. rec. loss:  2.63078828e-02\n",
      "Epoch: 1760 mean train loss:  2.75200045e-02, mean val. rec. loss:  2.62872512e-02\n",
      "Epoch: 1761 mean train loss:  2.74969077e-02, mean val. rec. loss:  2.62666786e-02\n",
      "Epoch: 1762 mean train loss:  2.74738892e-02, mean val. rec. loss:  2.62461650e-02\n",
      "Epoch: 1763 mean train loss:  2.74509395e-02, mean val. rec. loss:  2.62257171e-02\n",
      "Epoch: 1764 mean train loss:  2.74280663e-02, mean val. rec. loss:  2.62053396e-02\n",
      "Epoch: 1765 mean train loss:  2.74052712e-02, mean val. rec. loss:  2.61850074e-02\n",
      "Epoch: 1766 mean train loss:  2.73825357e-02, mean val. rec. loss:  2.61647364e-02\n",
      "Epoch: 1767 mean train loss:  2.73598803e-02, mean val. rec. loss:  2.61445289e-02\n",
      "Epoch: 1768 mean train loss:  2.73372919e-02, mean val. rec. loss:  2.61243850e-02\n",
      "Epoch: 1769 mean train loss:  2.73147780e-02, mean val. rec. loss:  2.61042909e-02\n",
      "Epoch: 1770 mean train loss:  2.72923275e-02, mean val. rec. loss:  2.60842603e-02\n",
      "Epoch: 1771 mean train loss:  2.72699477e-02, mean val. rec. loss:  2.60642797e-02\n",
      "Epoch: 1772 mean train loss:  2.72476386e-02, mean val. rec. loss:  2.60443648e-02\n",
      "Epoch: 1773 mean train loss:  2.72254041e-02, mean val. rec. loss:  2.60245043e-02\n",
      "Epoch: 1774 mean train loss:  2.72032347e-02, mean val. rec. loss:  2.60047051e-02\n",
      "Epoch: 1775 mean train loss:  2.71811361e-02, mean val. rec. loss:  2.59849557e-02\n",
      "Epoch: 1776 mean train loss:  2.71590989e-02, mean val. rec. loss:  2.59652676e-02\n",
      "Epoch: 1777 mean train loss:  2.71371344e-02, mean val. rec. loss:  2.59456272e-02\n",
      "Epoch: 1778 mean train loss:  2.71152350e-02, mean val. rec. loss:  2.59260434e-02\n",
      "Epoch: 1779 mean train loss:  2.70934008e-02, mean val. rec. loss:  2.59065231e-02\n",
      "Epoch: 1780 mean train loss:  2.70716393e-02, mean val. rec. loss:  2.58870550e-02\n",
      "Epoch: 1781 mean train loss:  2.70499410e-02, mean val. rec. loss:  2.58676345e-02\n",
      "Epoch: 1782 mean train loss:  2.70283042e-02, mean val. rec. loss:  2.58482707e-02\n",
      "Epoch: 1783 mean train loss:  2.70067382e-02, mean val. rec. loss:  2.58289705e-02\n",
      "Epoch: 1784 mean train loss:  2.69852318e-02, mean val. rec. loss:  2.58097133e-02\n",
      "Epoch: 1785 mean train loss:  2.69637942e-02, mean val. rec. loss:  2.57905060e-02\n",
      "Epoch: 1786 mean train loss:  2.69424144e-02, mean val. rec. loss:  2.57713531e-02\n",
      "Epoch: 1787 mean train loss:  2.69211035e-02, mean val. rec. loss:  2.57522683e-02\n",
      "Epoch: 1788 mean train loss:  2.68998615e-02, mean val. rec. loss:  2.57332220e-02\n",
      "Epoch: 1789 mean train loss:  2.68786754e-02, mean val. rec. loss:  2.57142392e-02\n",
      "Epoch: 1790 mean train loss:  2.68575600e-02, mean val. rec. loss:  2.56952950e-02\n",
      "Epoch: 1791 mean train loss:  2.68365005e-02, mean val. rec. loss:  2.56764052e-02\n",
      "Epoch: 1792 mean train loss:  2.68155061e-02, mean val. rec. loss:  2.56575744e-02\n",
      "Epoch: 1793 mean train loss:  2.67945751e-02, mean val. rec. loss:  2.56387776e-02\n",
      "Epoch: 1794 mean train loss:  2.67736962e-02, mean val. rec. loss:  2.56200443e-02\n",
      "Epoch: 1795 mean train loss:  2.67528844e-02, mean val. rec. loss:  2.56013541e-02\n",
      "Epoch: 1796 mean train loss:  2.67321359e-02, mean val. rec. loss:  2.55827229e-02\n",
      "Epoch: 1797 mean train loss:  2.67114469e-02, mean val. rec. loss:  2.55641280e-02\n",
      "Epoch: 1798 mean train loss:  2.66908176e-02, mean val. rec. loss:  2.55455942e-02\n",
      "Epoch: 1799 mean train loss:  2.66702478e-02, mean val. rec. loss:  2.55271014e-02\n",
      "Epoch: 1800 mean train loss:  2.66497358e-02, mean val. rec. loss:  2.55086629e-02\n",
      "Epoch: 1801 mean train loss:  2.66292834e-02, mean val. rec. loss:  2.54902698e-02\n",
      "Epoch: 1802 mean train loss:  2.66088924e-02, mean val. rec. loss:  2.54719198e-02\n",
      "Epoch: 1803 mean train loss:  2.65885535e-02, mean val. rec. loss:  2.54536333e-02\n",
      "Epoch: 1804 mean train loss:  2.65682836e-02, mean val. rec. loss:  2.54353808e-02\n",
      "Epoch: 1805 mean train loss:  2.65480658e-02, mean val. rec. loss:  2.54171782e-02\n",
      "Epoch: 1806 mean train loss:  2.65279094e-02, mean val. rec. loss:  2.53990232e-02\n",
      "Epoch: 1807 mean train loss:  2.65078052e-02, mean val. rec. loss:  2.53809091e-02\n",
      "Epoch: 1808 mean train loss:  2.64877606e-02, mean val. rec. loss:  2.53628471e-02\n",
      "Epoch: 1809 mean train loss:  2.64677681e-02, mean val. rec. loss:  2.53448260e-02\n",
      "Epoch: 1810 mean train loss:  2.64478315e-02, mean val. rec. loss:  2.53268524e-02\n",
      "Epoch: 1811 mean train loss:  2.64279545e-02, mean val. rec. loss:  2.53089288e-02\n",
      "Epoch: 1812 mean train loss:  2.64081371e-02, mean val. rec. loss:  2.52910505e-02\n",
      "Epoch: 1813 mean train loss:  2.63883736e-02, mean val. rec. loss:  2.52732199e-02\n",
      "Epoch: 1814 mean train loss:  2.63686661e-02, mean val. rec. loss:  2.52554278e-02\n",
      "Epoch: 1815 mean train loss:  2.63490107e-02, mean val. rec. loss:  2.52376788e-02\n",
      "Epoch: 1816 mean train loss:  2.63294074e-02, mean val. rec. loss:  2.52199774e-02\n",
      "Epoch: 1817 mean train loss:  2.63098618e-02, mean val. rec. loss:  2.52023191e-02\n",
      "Epoch: 1818 mean train loss:  2.62903684e-02, mean val. rec. loss:  2.51847062e-02\n",
      "Epoch: 1819 mean train loss:  2.62709271e-02, mean val. rec. loss:  2.51671318e-02\n",
      "Epoch: 1820 mean train loss:  2.62515399e-02, mean val. rec. loss:  2.51496051e-02\n",
      "Epoch: 1821 mean train loss:  2.62322122e-02, mean val. rec. loss:  2.51321237e-02\n",
      "Epoch: 1822 mean train loss:  2.62129329e-02, mean val. rec. loss:  2.51146786e-02\n",
      "Epoch: 1823 mean train loss:  2.61937021e-02, mean val. rec. loss:  2.50972789e-02\n",
      "Epoch: 1824 mean train loss:  2.61745290e-02, mean val. rec. loss:  2.50799154e-02\n",
      "Epoch: 1825 mean train loss:  2.61554062e-02, mean val. rec. loss:  2.50626064e-02\n",
      "Epoch: 1826 mean train loss:  2.61363392e-02, mean val. rec. loss:  2.50453337e-02\n",
      "Epoch: 1827 mean train loss:  2.61173206e-02, mean val. rec. loss:  2.50281040e-02\n",
      "Epoch: 1828 mean train loss:  2.60983505e-02, mean val. rec. loss:  2.50109107e-02\n",
      "Epoch: 1829 mean train loss:  2.60794344e-02, mean val. rec. loss:  2.49937604e-02\n",
      "Epoch: 1830 mean train loss:  2.60605685e-02, mean val. rec. loss:  2.49766578e-02\n",
      "Epoch: 1831 mean train loss:  2.60417585e-02, mean val. rec. loss:  2.49595869e-02\n",
      "Epoch: 1832 mean train loss:  2.60229970e-02, mean val. rec. loss:  2.49425568e-02\n",
      "Epoch: 1833 mean train loss:  2.60042764e-02, mean val. rec. loss:  2.49255699e-02\n",
      "Epoch: 1834 mean train loss:  2.59856135e-02, mean val. rec. loss:  2.49086282e-02\n",
      "Epoch: 1835 mean train loss:  2.59670028e-02, mean val. rec. loss:  2.48917161e-02\n",
      "Epoch: 1836 mean train loss:  2.59484330e-02, mean val. rec. loss:  2.48748516e-02\n",
      "Epoch: 1837 mean train loss:  2.59299098e-02, mean val. rec. loss:  2.48580189e-02\n",
      "Epoch: 1838 mean train loss:  2.59114387e-02, mean val. rec. loss:  2.48412338e-02\n",
      "Epoch: 1839 mean train loss:  2.58930272e-02, mean val. rec. loss:  2.48244758e-02\n",
      "Epoch: 1840 mean train loss:  2.58746511e-02, mean val. rec. loss:  2.48077633e-02\n",
      "Epoch: 1841 mean train loss:  2.58563253e-02, mean val. rec. loss:  2.47910916e-02\n",
      "Epoch: 1842 mean train loss:  2.58380498e-02, mean val. rec. loss:  2.47744584e-02\n",
      "Epoch: 1843 mean train loss:  2.58198227e-02, mean val. rec. loss:  2.47578615e-02\n",
      "Epoch: 1844 mean train loss:  2.58016440e-02, mean val. rec. loss:  2.47412941e-02\n",
      "Epoch: 1845 mean train loss:  2.57835006e-02, mean val. rec. loss:  2.47247766e-02\n",
      "Epoch: 1846 mean train loss:  2.57654169e-02, mean val. rec. loss:  2.47082909e-02\n",
      "Epoch: 1847 mean train loss:  2.57473723e-02, mean val. rec. loss:  2.46918369e-02\n",
      "Epoch: 1848 mean train loss:  2.57293724e-02, mean val. rec. loss:  2.46754214e-02\n",
      "Epoch: 1849 mean train loss:  2.57114246e-02, mean val. rec. loss:  2.46590513e-02\n",
      "Epoch: 1850 mean train loss:  2.56935177e-02, mean val. rec. loss:  2.46427153e-02\n",
      "Epoch: 1851 mean train loss:  2.56756631e-02, mean val. rec. loss:  2.46264132e-02\n",
      "Epoch: 1852 mean train loss:  2.56578493e-02, mean val. rec. loss:  2.46101407e-02\n",
      "Epoch: 1853 mean train loss:  2.56400766e-02, mean val. rec. loss:  2.45939180e-02\n",
      "Epoch: 1854 mean train loss:  2.56223597e-02, mean val. rec. loss:  2.45777225e-02\n",
      "Epoch: 1855 mean train loss:  2.56046764e-02, mean val. rec. loss:  2.45615566e-02\n",
      "Epoch: 1856 mean train loss:  2.55870340e-02, mean val. rec. loss:  2.45454360e-02\n",
      "Epoch: 1857 mean train loss:  2.55694456e-02, mean val. rec. loss:  2.45293448e-02\n",
      "Epoch: 1858 mean train loss:  2.55518981e-02, mean val. rec. loss:  2.45132968e-02\n",
      "Epoch: 1859 mean train loss:  2.55343991e-02, mean val. rec. loss:  2.44972828e-02\n",
      "Epoch: 1860 mean train loss:  2.55169448e-02, mean val. rec. loss:  2.44812982e-02\n",
      "Epoch: 1861 mean train loss:  2.54995240e-02, mean val. rec. loss:  2.44653409e-02\n",
      "Epoch: 1862 mean train loss:  2.54821517e-02, mean val. rec. loss:  2.44494290e-02\n",
      "Epoch: 1863 mean train loss:  2.54648184e-02, mean val. rec. loss:  2.44335533e-02\n",
      "Epoch: 1864 mean train loss:  2.54475354e-02, mean val. rec. loss:  2.44177048e-02\n",
      "Epoch: 1865 mean train loss:  2.54302859e-02, mean val. rec. loss:  2.44018927e-02\n",
      "Epoch: 1866 mean train loss:  2.54130830e-02, mean val. rec. loss:  2.43861032e-02\n",
      "Epoch: 1867 mean train loss:  2.53959211e-02, mean val. rec. loss:  2.43703636e-02\n",
      "Epoch: 1868 mean train loss:  2.53788038e-02, mean val. rec. loss:  2.43546444e-02\n",
      "Epoch: 1869 mean train loss:  2.53617238e-02, mean val. rec. loss:  2.43389683e-02\n",
      "Epoch: 1870 mean train loss:  2.53446866e-02, mean val. rec. loss:  2.43233217e-02\n",
      "Epoch: 1871 mean train loss:  2.53276979e-02, mean val. rec. loss:  2.43077091e-02\n",
      "Epoch: 1872 mean train loss:  2.53107408e-02, mean val. rec. loss:  2.42921237e-02\n",
      "Epoch: 1873 mean train loss:  2.52938246e-02, mean val. rec. loss:  2.42765746e-02\n",
      "Epoch: 1874 mean train loss:  2.52769569e-02, mean val. rec. loss:  2.42610573e-02\n",
      "Epoch: 1875 mean train loss:  2.52601227e-02, mean val. rec. loss:  2.42455763e-02\n",
      "Epoch: 1876 mean train loss:  2.52433332e-02, mean val. rec. loss:  2.42301179e-02\n",
      "Epoch: 1877 mean train loss:  2.52265809e-02, mean val. rec. loss:  2.42147003e-02\n",
      "Epoch: 1878 mean train loss:  2.52098659e-02, mean val. rec. loss:  2.41993100e-02\n",
      "Epoch: 1879 mean train loss:  2.51931993e-02, mean val. rec. loss:  2.41839492e-02\n",
      "Epoch: 1880 mean train loss:  2.51765643e-02, mean val. rec. loss:  2.41686155e-02\n",
      "Epoch: 1881 mean train loss:  2.51599666e-02, mean val. rec. loss:  2.41533250e-02\n",
      "Epoch: 1882 mean train loss:  2.51434173e-02, mean val. rec. loss:  2.41380571e-02\n",
      "Epoch: 1883 mean train loss:  2.51269016e-02, mean val. rec. loss:  2.41228210e-02\n",
      "Epoch: 1884 mean train loss:  2.51104249e-02, mean val. rec. loss:  2.41076167e-02\n",
      "Epoch: 1885 mean train loss:  2.50939892e-02, mean val. rec. loss:  2.40924418e-02\n",
      "Epoch: 1886 mean train loss:  2.50775871e-02, mean val. rec. loss:  2.40772964e-02\n",
      "Epoch: 1887 mean train loss:  2.50612258e-02, mean val. rec. loss:  2.40621896e-02\n",
      "Epoch: 1888 mean train loss:  2.50449056e-02, mean val. rec. loss:  2.40471077e-02\n",
      "Epoch: 1889 mean train loss:  2.50286208e-02, mean val. rec. loss:  2.40320530e-02\n",
      "Epoch: 1890 mean train loss:  2.50123694e-02, mean val. rec. loss:  2.40170210e-02\n",
      "Epoch: 1891 mean train loss:  2.49961572e-02, mean val. rec. loss:  2.40020298e-02\n",
      "Epoch: 1892 mean train loss:  2.49799859e-02, mean val. rec. loss:  2.39870659e-02\n",
      "Epoch: 1893 mean train loss:  2.49638519e-02, mean val. rec. loss:  2.39721337e-02\n",
      "Epoch: 1894 mean train loss:  2.49477533e-02, mean val. rec. loss:  2.39572242e-02\n",
      "Epoch: 1895 mean train loss:  2.49316919e-02, mean val. rec. loss:  2.39423509e-02\n",
      "Epoch: 1896 mean train loss:  2.49156640e-02, mean val. rec. loss:  2.39275004e-02\n",
      "Epoch: 1897 mean train loss:  2.48996771e-02, mean val. rec. loss:  2.39126770e-02\n",
      "Epoch: 1898 mean train loss:  2.48837218e-02, mean val. rec. loss:  2.38978854e-02\n",
      "Epoch: 1899 mean train loss:  2.48678038e-02, mean val. rec. loss:  2.38831279e-02\n",
      "Epoch: 1900 mean train loss:  2.48519268e-02, mean val. rec. loss:  2.38683930e-02\n",
      "Epoch: 1901 mean train loss:  2.48360851e-02, mean val. rec. loss:  2.38536898e-02\n",
      "Epoch: 1902 mean train loss:  2.48202733e-02, mean val. rec. loss:  2.38390048e-02\n",
      "Epoch: 1903 mean train loss:  2.48044987e-02, mean val. rec. loss:  2.38243561e-02\n",
      "Epoch: 1904 mean train loss:  2.47887594e-02, mean val. rec. loss:  2.38097346e-02\n",
      "Epoch: 1905 mean train loss:  2.47730556e-02, mean val. rec. loss:  2.37951426e-02\n",
      "Epoch: 1906 mean train loss:  2.47573890e-02, mean val. rec. loss:  2.37805733e-02\n",
      "Epoch: 1907 mean train loss:  2.47417522e-02, mean val. rec. loss:  2.37660357e-02\n",
      "Epoch: 1908 mean train loss:  2.47261563e-02, mean val. rec. loss:  2.37515253e-02\n",
      "Epoch: 1909 mean train loss:  2.47105921e-02, mean val. rec. loss:  2.37370399e-02\n",
      "Epoch: 1910 mean train loss:  2.46950633e-02, mean val. rec. loss:  2.37225817e-02\n",
      "Epoch: 1911 mean train loss:  2.46795662e-02, mean val. rec. loss:  2.37081485e-02\n",
      "Epoch: 1912 mean train loss:  2.46641081e-02, mean val. rec. loss:  2.36937470e-02\n",
      "Epoch: 1913 mean train loss:  2.46486799e-02, mean val. rec. loss:  2.36793727e-02\n",
      "Epoch: 1914 mean train loss:  2.46332907e-02, mean val. rec. loss:  2.36650211e-02\n",
      "Epoch: 1915 mean train loss:  2.46179314e-02, mean val. rec. loss:  2.36506921e-02\n",
      "Epoch: 1916 mean train loss:  2.46026000e-02, mean val. rec. loss:  2.36363950e-02\n",
      "Epoch: 1917 mean train loss:  2.45873058e-02, mean val. rec. loss:  2.36221227e-02\n",
      "Epoch: 1918 mean train loss:  2.45720452e-02, mean val. rec. loss:  2.36078777e-02\n",
      "Epoch: 1919 mean train loss:  2.45568199e-02, mean val. rec. loss:  2.35936599e-02\n",
      "Epoch: 1920 mean train loss:  2.45416263e-02, mean val. rec. loss:  2.35794648e-02\n",
      "Epoch: 1921 mean train loss:  2.45264662e-02, mean val. rec. loss:  2.35652901e-02\n",
      "Epoch: 1922 mean train loss:  2.45113340e-02, mean val. rec. loss:  2.35511517e-02\n",
      "Epoch: 1923 mean train loss:  2.44962391e-02, mean val. rec. loss:  2.35370359e-02\n",
      "Epoch: 1924 mean train loss:  2.44811796e-02, mean val. rec. loss:  2.35229429e-02\n",
      "Epoch: 1925 mean train loss:  2.44661461e-02, mean val. rec. loss:  2.35088770e-02\n",
      "Epoch: 1926 mean train loss:  2.44511462e-02, mean val. rec. loss:  2.34948361e-02\n",
      "Epoch: 1927 mean train loss:  2.44361797e-02, mean val. rec. loss:  2.34808202e-02\n",
      "Epoch: 1928 mean train loss:  2.44212468e-02, mean val. rec. loss:  2.34668292e-02\n",
      "Epoch: 1929 mean train loss:  2.44063437e-02, mean val. rec. loss:  2.34528677e-02\n",
      "Epoch: 1930 mean train loss:  2.43914704e-02, mean val. rec. loss:  2.34389152e-02\n",
      "Epoch: 1931 mean train loss:  2.43766232e-02, mean val. rec. loss:  2.34250081e-02\n",
      "Epoch: 1932 mean train loss:  2.43618169e-02, mean val. rec. loss:  2.34111101e-02\n",
      "Epoch: 1933 mean train loss:  2.43470348e-02, mean val. rec. loss:  2.33972484e-02\n",
      "Epoch: 1934 mean train loss:  2.43322900e-02, mean val. rec. loss:  2.33834048e-02\n",
      "Epoch: 1935 mean train loss:  2.43175750e-02, mean val. rec. loss:  2.33695816e-02\n",
      "Epoch: 1936 mean train loss:  2.43028879e-02, mean val. rec. loss:  2.33557834e-02\n",
      "Epoch: 1937 mean train loss:  2.42882306e-02, mean val. rec. loss:  2.33420124e-02\n",
      "Epoch: 1938 mean train loss:  2.42736049e-02, mean val. rec. loss:  2.33282731e-02\n",
      "Epoch: 1939 mean train loss:  2.42590147e-02, mean val. rec. loss:  2.33145406e-02\n",
      "Epoch: 1940 mean train loss:  2.42444486e-02, mean val. rec. loss:  2.33008422e-02\n",
      "Epoch: 1941 mean train loss:  2.42299105e-02, mean val. rec. loss:  2.32871642e-02\n",
      "Epoch: 1942 mean train loss:  2.42154077e-02, mean val. rec. loss:  2.32735156e-02\n",
      "Epoch: 1943 mean train loss:  2.42009367e-02, mean val. rec. loss:  2.32598875e-02\n",
      "Epoch: 1944 mean train loss:  2.41864879e-02, mean val. rec. loss:  2.32462798e-02\n",
      "Epoch: 1945 mean train loss:  2.41720746e-02, mean val. rec. loss:  2.32326970e-02\n",
      "Epoch: 1946 mean train loss:  2.41576873e-02, mean val. rec. loss:  2.32191415e-02\n",
      "Epoch: 1947 mean train loss:  2.41433335e-02, mean val. rec. loss:  2.32056063e-02\n",
      "Epoch: 1948 mean train loss:  2.41290095e-02, mean val. rec. loss:  2.31920916e-02\n",
      "Epoch: 1949 mean train loss:  2.41147079e-02, mean val. rec. loss:  2.31785995e-02\n",
      "Epoch: 1950 mean train loss:  2.41004398e-02, mean val. rec. loss:  2.31651370e-02\n",
      "Epoch: 1951 mean train loss:  2.40862034e-02, mean val. rec. loss:  2.31516857e-02\n",
      "Epoch: 1952 mean train loss:  2.40719874e-02, mean val. rec. loss:  2.31382640e-02\n",
      "Epoch: 1953 mean train loss:  2.40578068e-02, mean val. rec. loss:  2.31248672e-02\n",
      "Epoch: 1954 mean train loss:  2.40436542e-02, mean val. rec. loss:  2.31114863e-02\n",
      "Epoch: 1955 mean train loss:  2.40295276e-02, mean val. rec. loss:  2.30981348e-02\n",
      "Epoch: 1956 mean train loss:  2.40154308e-02, mean val. rec. loss:  2.30848038e-02\n",
      "Epoch: 1957 mean train loss:  2.40013638e-02, mean val. rec. loss:  2.30714909e-02\n",
      "Epoch: 1958 mean train loss:  2.39873229e-02, mean val. rec. loss:  2.30581939e-02\n",
      "Epoch: 1959 mean train loss:  2.39733044e-02, mean val. rec. loss:  2.30449332e-02\n",
      "Epoch: 1960 mean train loss:  2.39593230e-02, mean val. rec. loss:  2.30316838e-02\n",
      "Epoch: 1961 mean train loss:  2.39453640e-02, mean val. rec. loss:  2.30184639e-02\n",
      "Epoch: 1962 mean train loss:  2.39314349e-02, mean val. rec. loss:  2.30052621e-02\n",
      "Epoch: 1963 mean train loss:  2.39175336e-02, mean val. rec. loss:  2.29920831e-02\n",
      "Epoch: 1964 mean train loss:  2.39036603e-02, mean val. rec. loss:  2.29789199e-02\n",
      "Epoch: 1965 mean train loss:  2.38898112e-02, mean val. rec. loss:  2.29657839e-02\n",
      "Epoch: 1966 mean train loss:  2.38759956e-02, mean val. rec. loss:  2.29526660e-02\n",
      "Epoch: 1967 mean train loss:  2.38621986e-02, mean val. rec. loss:  2.29395709e-02\n",
      "Epoch: 1968 mean train loss:  2.38484333e-02, mean val. rec. loss:  2.29264961e-02\n",
      "Epoch: 1969 mean train loss:  2.38346941e-02, mean val. rec. loss:  2.29134531e-02\n",
      "Epoch: 1970 mean train loss:  2.38209846e-02, mean val. rec. loss:  2.29004169e-02\n",
      "Epoch: 1971 mean train loss:  2.38073012e-02, mean val. rec. loss:  2.28874034e-02\n",
      "Epoch: 1972 mean train loss:  2.37936365e-02, mean val. rec. loss:  2.28744171e-02\n",
      "Epoch: 1973 mean train loss:  2.37800108e-02, mean val. rec. loss:  2.28614490e-02\n",
      "Epoch: 1974 mean train loss:  2.37664020e-02, mean val. rec. loss:  2.28484989e-02\n",
      "Epoch: 1975 mean train loss:  2.37528210e-02, mean val. rec. loss:  2.28355693e-02\n",
      "Epoch: 1976 mean train loss:  2.37392698e-02, mean val. rec. loss:  2.28226692e-02\n",
      "Epoch: 1977 mean train loss:  2.37257447e-02, mean val. rec. loss:  2.28097827e-02\n",
      "Epoch: 1978 mean train loss:  2.37122420e-02, mean val. rec. loss:  2.27969166e-02\n",
      "Epoch: 1979 mean train loss:  2.36987691e-02, mean val. rec. loss:  2.27840709e-02\n",
      "Epoch: 1980 mean train loss:  2.36853166e-02, mean val. rec. loss:  2.27712479e-02\n",
      "Epoch: 1981 mean train loss:  2.36718958e-02, mean val. rec. loss:  2.27584453e-02\n",
      "Epoch: 1982 mean train loss:  2.36584973e-02, mean val. rec. loss:  2.27456586e-02\n",
      "Epoch: 1983 mean train loss:  2.36451249e-02, mean val. rec. loss:  2.27328923e-02\n",
      "Epoch: 1984 mean train loss:  2.36317749e-02, mean val. rec. loss:  2.27201464e-02\n",
      "Epoch: 1985 mean train loss:  2.36184528e-02, mean val. rec. loss:  2.27074300e-02\n",
      "Epoch: 1986 mean train loss:  2.36051605e-02, mean val. rec. loss:  2.26947272e-02\n",
      "Epoch: 1987 mean train loss:  2.35918924e-02, mean val. rec. loss:  2.26820425e-02\n",
      "Epoch: 1988 mean train loss:  2.35786466e-02, mean val. rec. loss:  2.26693737e-02\n",
      "Epoch: 1989 mean train loss:  2.35654232e-02, mean val. rec. loss:  2.26567276e-02\n",
      "Epoch: 1990 mean train loss:  2.35522277e-02, mean val. rec. loss:  2.26441019e-02\n",
      "Epoch: 1991 mean train loss:  2.35390564e-02, mean val. rec. loss:  2.26314989e-02\n",
      "Epoch: 1992 mean train loss:  2.35259094e-02, mean val. rec. loss:  2.26189140e-02\n",
      "Epoch: 1993 mean train loss:  2.35127884e-02, mean val. rec. loss:  2.26063496e-02\n",
      "Epoch: 1994 mean train loss:  2.34996934e-02, mean val. rec. loss:  2.25937942e-02\n",
      "Epoch: 1995 mean train loss:  2.34866171e-02, mean val. rec. loss:  2.25812683e-02\n",
      "Epoch: 1996 mean train loss:  2.34735706e-02, mean val. rec. loss:  2.25687605e-02\n",
      "Epoch: 1997 mean train loss:  2.34605483e-02, mean val. rec. loss:  2.25562686e-02\n",
      "Epoch: 1998 mean train loss:  2.34475447e-02, mean val. rec. loss:  2.25437949e-02\n",
      "Epoch: 1999 mean train loss:  2.34345670e-02, mean val. rec. loss:  2.25313393e-02\n",
      "Epoch: 2000 mean train loss:  2.34216155e-02, mean val. rec. loss:  2.25189109e-02\n",
      "Epoch: 2001 mean train loss:  2.34086882e-02, mean val. rec. loss:  2.25064961e-02\n",
      "Epoch: 2002 mean train loss:  2.33957851e-02, mean val. rec. loss:  2.24941018e-02\n",
      "Epoch: 2003 mean train loss:  2.33829043e-02, mean val. rec. loss:  2.24817210e-02\n",
      "Epoch: 2004 mean train loss:  2.33700515e-02, mean val. rec. loss:  2.24693652e-02\n",
      "Epoch: 2005 mean train loss:  2.33572210e-02, mean val. rec. loss:  2.24570185e-02\n",
      "Epoch: 2006 mean train loss:  2.33444091e-02, mean val. rec. loss:  2.24447012e-02\n",
      "Epoch: 2007 mean train loss:  2.33316214e-02, mean val. rec. loss:  2.24323998e-02\n",
      "Epoch: 2008 mean train loss:  2.33188598e-02, mean val. rec. loss:  2.24201143e-02\n",
      "Epoch: 2009 mean train loss:  2.33061225e-02, mean val. rec. loss:  2.24078493e-02\n",
      "Epoch: 2010 mean train loss:  2.32934018e-02, mean val. rec. loss:  2.23955978e-02\n",
      "Epoch: 2011 mean train loss:  2.32807073e-02, mean val. rec. loss:  2.23833690e-02\n",
      "Epoch: 2012 mean train loss:  2.32680369e-02, mean val. rec. loss:  2.23711583e-02\n",
      "Epoch: 2013 mean train loss:  2.32553852e-02, mean val. rec. loss:  2.23589635e-02\n",
      "Epoch: 2014 mean train loss:  2.32427670e-02, mean val. rec. loss:  2.23467914e-02\n",
      "Epoch: 2015 mean train loss:  2.32301693e-02, mean val. rec. loss:  2.23346307e-02\n",
      "Epoch: 2016 mean train loss:  2.32175865e-02, mean val. rec. loss:  2.23224880e-02\n",
      "Epoch: 2017 mean train loss:  2.32050297e-02, mean val. rec. loss:  2.23103658e-02\n",
      "Epoch: 2018 mean train loss:  2.31924935e-02, mean val. rec. loss:  2.22982618e-02\n",
      "Epoch: 2019 mean train loss:  2.31799814e-02, mean val. rec. loss:  2.22861781e-02\n",
      "Epoch: 2020 mean train loss:  2.31674973e-02, mean val. rec. loss:  2.22741126e-02\n",
      "Epoch: 2021 mean train loss:  2.31550299e-02, mean val. rec. loss:  2.22620607e-02\n",
      "Epoch: 2022 mean train loss:  2.31425868e-02, mean val. rec. loss:  2.22500224e-02\n",
      "Epoch: 2023 mean train loss:  2.31301622e-02, mean val. rec. loss:  2.22380113e-02\n",
      "Epoch: 2024 mean train loss:  2.31177619e-02, mean val. rec. loss:  2.22260093e-02\n",
      "Epoch: 2025 mean train loss:  2.31053858e-02, mean val. rec. loss:  2.22140300e-02\n",
      "Epoch: 2026 mean train loss:  2.30930264e-02, mean val. rec. loss:  2.22020643e-02\n",
      "Epoch: 2027 mean train loss:  2.30806931e-02, mean val. rec. loss:  2.21901212e-02\n",
      "Epoch: 2028 mean train loss:  2.30683840e-02, mean val. rec. loss:  2.21781872e-02\n",
      "Epoch: 2029 mean train loss:  2.30560899e-02, mean val. rec. loss:  2.21662805e-02\n",
      "Epoch: 2030 mean train loss:  2.30438218e-02, mean val. rec. loss:  2.21543828e-02\n",
      "Epoch: 2031 mean train loss:  2.30315741e-02, mean val. rec. loss:  2.21425078e-02\n",
      "Epoch: 2032 mean train loss:  2.30193488e-02, mean val. rec. loss:  2.21306487e-02\n",
      "Epoch: 2033 mean train loss:  2.30071496e-02, mean val. rec. loss:  2.21188077e-02\n",
      "Epoch: 2034 mean train loss:  2.29949653e-02, mean val. rec. loss:  2.21069780e-02\n",
      "Epoch: 2035 mean train loss:  2.29828034e-02, mean val. rec. loss:  2.20951756e-02\n",
      "Epoch: 2036 mean train loss:  2.29706656e-02, mean val. rec. loss:  2.20833777e-02\n",
      "Epoch: 2037 mean train loss:  2.29585483e-02, mean val. rec. loss:  2.20716002e-02\n",
      "Epoch: 2038 mean train loss:  2.29464441e-02, mean val. rec. loss:  2.20598409e-02\n",
      "Epoch: 2039 mean train loss:  2.29343659e-02, mean val. rec. loss:  2.20481020e-02\n",
      "Epoch: 2040 mean train loss:  2.29223120e-02, mean val. rec. loss:  2.20363812e-02\n",
      "Epoch: 2041 mean train loss:  2.29102841e-02, mean val. rec. loss:  2.20246695e-02\n",
      "Epoch: 2042 mean train loss:  2.28982655e-02, mean val. rec. loss:  2.20129782e-02\n",
      "Epoch: 2043 mean train loss:  2.28862767e-02, mean val. rec. loss:  2.20012982e-02\n",
      "Epoch: 2044 mean train loss:  2.28742954e-02, mean val. rec. loss:  2.19896387e-02\n",
      "Epoch: 2045 mean train loss:  2.28623438e-02, mean val. rec. loss:  2.19779973e-02\n",
      "Epoch: 2046 mean train loss:  2.28504165e-02, mean val. rec. loss:  2.19663672e-02\n",
      "Epoch: 2047 mean train loss:  2.28385004e-02, mean val. rec. loss:  2.19547576e-02\n",
      "Epoch: 2048 mean train loss:  2.28266084e-02, mean val. rec. loss:  2.19431706e-02\n",
      "Epoch: 2049 mean train loss:  2.28147444e-02, mean val. rec. loss:  2.19315836e-02\n",
      "Epoch: 2050 mean train loss:  2.28028953e-02, mean val. rec. loss:  2.19200284e-02\n",
      "Epoch: 2051 mean train loss:  2.27910666e-02, mean val. rec. loss:  2.19084822e-02\n",
      "Epoch: 2052 mean train loss:  2.27792510e-02, mean val. rec. loss:  2.18969520e-02\n",
      "Epoch: 2053 mean train loss:  2.27674634e-02, mean val. rec. loss:  2.18854398e-02\n",
      "Epoch: 2054 mean train loss:  2.27556962e-02, mean val. rec. loss:  2.18739413e-02\n",
      "Epoch: 2055 mean train loss:  2.27439458e-02, mean val. rec. loss:  2.18624564e-02\n",
      "Epoch: 2056 mean train loss:  2.27322158e-02, mean val. rec. loss:  2.18509942e-02\n",
      "Epoch: 2057 mean train loss:  2.27205120e-02, mean val. rec. loss:  2.18395433e-02\n",
      "Epoch: 2058 mean train loss:  2.27088193e-02, mean val. rec. loss:  2.18281105e-02\n",
      "Epoch: 2059 mean train loss:  2.26971526e-02, mean val. rec. loss:  2.18166936e-02\n",
      "Epoch: 2060 mean train loss:  2.26854991e-02, mean val. rec. loss:  2.18052926e-02\n",
      "Epoch: 2061 mean train loss:  2.26738697e-02, mean val. rec. loss:  2.17939053e-02\n",
      "Epoch: 2062 mean train loss:  2.26622552e-02, mean val. rec. loss:  2.17825360e-02\n",
      "Epoch: 2063 mean train loss:  2.26506686e-02, mean val. rec. loss:  2.17711758e-02\n",
      "Epoch: 2064 mean train loss:  2.26390951e-02, mean val. rec. loss:  2.17598406e-02\n",
      "Epoch: 2065 mean train loss:  2.26275440e-02, mean val. rec. loss:  2.17485144e-02\n",
      "Epoch: 2066 mean train loss:  2.26160133e-02, mean val. rec. loss:  2.17372042e-02\n",
      "Epoch: 2067 mean train loss:  2.26044938e-02, mean val. rec. loss:  2.17259120e-02\n",
      "Epoch: 2068 mean train loss:  2.25929966e-02, mean val. rec. loss:  2.17146358e-02\n",
      "Epoch: 2069 mean train loss:  2.25815255e-02, mean val. rec. loss:  2.17033708e-02\n",
      "Epoch: 2070 mean train loss:  2.25700675e-02, mean val. rec. loss:  2.16921286e-02\n",
      "Epoch: 2071 mean train loss:  2.25586355e-02, mean val. rec. loss:  2.16808931e-02\n",
      "Epoch: 2072 mean train loss:  2.25472146e-02, mean val. rec. loss:  2.16696758e-02\n",
      "Epoch: 2073 mean train loss:  2.25358199e-02, mean val. rec. loss:  2.16584812e-02\n",
      "Epoch: 2074 mean train loss:  2.25244363e-02, mean val. rec. loss:  2.16472911e-02\n",
      "Epoch: 2075 mean train loss:  2.25130751e-02, mean val. rec. loss:  2.16361192e-02\n",
      "Epoch: 2076 mean train loss:  2.25017362e-02, mean val. rec. loss:  2.16249654e-02\n",
      "Epoch: 2077 mean train loss:  2.24904123e-02, mean val. rec. loss:  2.16138252e-02\n",
      "Epoch: 2078 mean train loss:  2.24791069e-02, mean val. rec. loss:  2.16027009e-02\n",
      "Epoch: 2079 mean train loss:  2.24678276e-02, mean val. rec. loss:  2.15915902e-02\n",
      "Epoch: 2080 mean train loss:  2.24565595e-02, mean val. rec. loss:  2.15804954e-02\n",
      "Epoch: 2081 mean train loss:  2.24453063e-02, mean val. rec. loss:  2.15694187e-02\n",
      "Epoch: 2082 mean train loss:  2.24340773e-02, mean val. rec. loss:  2.15583511e-02\n",
      "Epoch: 2083 mean train loss:  2.24228613e-02, mean val. rec. loss:  2.15473016e-02\n",
      "Epoch: 2084 mean train loss:  2.24116714e-02, mean val. rec. loss:  2.15362590e-02\n",
      "Epoch: 2085 mean train loss:  2.24004945e-02, mean val. rec. loss:  2.15252412e-02\n",
      "Epoch: 2086 mean train loss:  2.23893381e-02, mean val. rec. loss:  2.15142349e-02\n",
      "Epoch: 2087 mean train loss:  2.23782041e-02, mean val. rec. loss:  2.15032489e-02\n",
      "Epoch: 2088 mean train loss:  2.23670794e-02, mean val. rec. loss:  2.14922675e-02\n",
      "Epoch: 2089 mean train loss:  2.23559807e-02, mean val. rec. loss:  2.14812997e-02\n",
      "Epoch: 2090 mean train loss:  2.23448932e-02, mean val. rec. loss:  2.14703590e-02\n",
      "Epoch: 2091 mean train loss:  2.23338300e-02, mean val. rec. loss:  2.14594230e-02\n",
      "Epoch: 2092 mean train loss:  2.23227816e-02, mean val. rec. loss:  2.14485051e-02\n",
      "Epoch: 2093 mean train loss:  2.23117481e-02, mean val. rec. loss:  2.14376007e-02\n",
      "Epoch: 2094 mean train loss:  2.23007351e-02, mean val. rec. loss:  2.14267055e-02\n",
      "Epoch: 2095 mean train loss:  2.22897389e-02, mean val. rec. loss:  2.14158375e-02\n",
      "Epoch: 2096 mean train loss:  2.22787669e-02, mean val. rec. loss:  2.14049740e-02\n",
      "Epoch: 2097 mean train loss:  2.22678041e-02, mean val. rec. loss:  2.13941286e-02\n",
      "Epoch: 2098 mean train loss:  2.22568638e-02, mean val. rec. loss:  2.13832991e-02\n",
      "Epoch: 2099 mean train loss:  2.22459364e-02, mean val. rec. loss:  2.13724787e-02\n",
      "Epoch: 2100 mean train loss:  2.22350333e-02, mean val. rec. loss:  2.13616765e-02\n",
      "Epoch: 2101 mean train loss:  2.22241451e-02, mean val. rec. loss:  2.13508810e-02\n",
      "Epoch: 2102 mean train loss:  2.22132736e-02, mean val. rec. loss:  2.13401060e-02\n",
      "Epoch: 2103 mean train loss:  2.22024189e-02, mean val. rec. loss:  2.13293468e-02\n",
      "Epoch: 2104 mean train loss:  2.21915847e-02, mean val. rec. loss:  2.13185967e-02\n",
      "Epoch: 2105 mean train loss:  2.21807635e-02, mean val. rec. loss:  2.13078647e-02\n",
      "Epoch: 2106 mean train loss:  2.21699591e-02, mean val. rec. loss:  2.12971464e-02\n",
      "Epoch: 2107 mean train loss:  2.21591770e-02, mean val. rec. loss:  2.12864371e-02\n",
      "Epoch: 2108 mean train loss:  2.21484061e-02, mean val. rec. loss:  2.12757551e-02\n",
      "Epoch: 2109 mean train loss:  2.21376612e-02, mean val. rec. loss:  2.12650730e-02\n",
      "Epoch: 2110 mean train loss:  2.21269276e-02, mean val. rec. loss:  2.12544068e-02\n",
      "Epoch: 2111 mean train loss:  2.21162125e-02, mean val. rec. loss:  2.12437633e-02\n",
      "Epoch: 2112 mean train loss:  2.21055142e-02, mean val. rec. loss:  2.12331266e-02\n",
      "Epoch: 2113 mean train loss:  2.20948308e-02, mean val. rec. loss:  2.12225081e-02\n",
      "Epoch: 2114 mean train loss:  2.20841679e-02, mean val. rec. loss:  2.12118986e-02\n",
      "Epoch: 2115 mean train loss:  2.20735199e-02, mean val. rec. loss:  2.12013050e-02\n",
      "Epoch: 2116 mean train loss:  2.20628831e-02, mean val. rec. loss:  2.11907295e-02\n",
      "Epoch: 2117 mean train loss:  2.20522723e-02, mean val. rec. loss:  2.11801586e-02\n",
      "Epoch: 2118 mean train loss:  2.20416728e-02, mean val. rec. loss:  2.11696080e-02\n",
      "Epoch: 2119 mean train loss:  2.20310955e-02, mean val. rec. loss:  2.11590689e-02\n",
      "Epoch: 2120 mean train loss:  2.20205313e-02, mean val. rec. loss:  2.11485410e-02\n",
      "Epoch: 2121 mean train loss:  2.20099801e-02, mean val. rec. loss:  2.11380359e-02\n",
      "Epoch: 2122 mean train loss:  2.19994532e-02, mean val. rec. loss:  2.11275375e-02\n",
      "Epoch: 2123 mean train loss:  2.19889392e-02, mean val. rec. loss:  2.11170482e-02\n",
      "Epoch: 2124 mean train loss:  2.19784365e-02, mean val. rec. loss:  2.11065816e-02\n",
      "Epoch: 2125 mean train loss:  2.19679579e-02, mean val. rec. loss:  2.10961173e-02\n",
      "Epoch: 2126 mean train loss:  2.19574924e-02, mean val. rec. loss:  2.10856734e-02\n",
      "Epoch: 2127 mean train loss:  2.19470437e-02, mean val. rec. loss:  2.10752453e-02\n",
      "Epoch: 2128 mean train loss:  2.19366154e-02, mean val. rec. loss:  2.10648263e-02\n",
      "Epoch: 2129 mean train loss:  2.19261946e-02, mean val. rec. loss:  2.10544255e-02\n",
      "Epoch: 2130 mean train loss:  2.19157998e-02, mean val. rec. loss:  2.10440337e-02\n",
      "Epoch: 2131 mean train loss:  2.19054126e-02, mean val. rec. loss:  2.10336579e-02\n",
      "Epoch: 2132 mean train loss:  2.18950476e-02, mean val. rec. loss:  2.10232910e-02\n",
      "Epoch: 2133 mean train loss:  2.18846938e-02, mean val. rec. loss:  2.10129446e-02\n",
      "Epoch: 2134 mean train loss:  2.18743624e-02, mean val. rec. loss:  2.10026073e-02\n",
      "Epoch: 2135 mean train loss:  2.18640421e-02, mean val. rec. loss:  2.09922881e-02\n",
      "Epoch: 2136 mean train loss:  2.18537368e-02, mean val. rec. loss:  2.09819712e-02\n",
      "Epoch: 2137 mean train loss:  2.18434519e-02, mean val. rec. loss:  2.09716702e-02\n",
      "Epoch: 2138 mean train loss:  2.18331782e-02, mean val. rec. loss:  2.09613850e-02\n",
      "Epoch: 2139 mean train loss:  2.18229250e-02, mean val. rec. loss:  2.09511089e-02\n",
      "Epoch: 2140 mean train loss:  2.18126848e-02, mean val. rec. loss:  2.09408555e-02\n",
      "Epoch: 2141 mean train loss:  2.18024613e-02, mean val. rec. loss:  2.09306134e-02\n",
      "Epoch: 2142 mean train loss:  2.17922565e-02, mean val. rec. loss:  2.09203804e-02\n",
      "Epoch: 2143 mean train loss:  2.17820611e-02, mean val. rec. loss:  2.09101610e-02\n",
      "Epoch: 2144 mean train loss:  2.17718879e-02, mean val. rec. loss:  2.08999552e-02\n",
      "Epoch: 2145 mean train loss:  2.17617241e-02, mean val. rec. loss:  2.08897585e-02\n",
      "Epoch: 2146 mean train loss:  2.17515807e-02, mean val. rec. loss:  2.08795754e-02\n",
      "Epoch: 2147 mean train loss:  2.17414467e-02, mean val. rec. loss:  2.08694082e-02\n",
      "Epoch: 2148 mean train loss:  2.17313331e-02, mean val. rec. loss:  2.08592545e-02\n",
      "Epoch: 2149 mean train loss:  2.17212382e-02, mean val. rec. loss:  2.08491122e-02\n",
      "Epoch: 2150 mean train loss:  2.17111563e-02, mean val. rec. loss:  2.08389836e-02\n",
      "Epoch: 2151 mean train loss:  2.17010874e-02, mean val. rec. loss:  2.08288730e-02\n",
      "Epoch: 2152 mean train loss:  2.16910390e-02, mean val. rec. loss:  2.08187693e-02\n",
      "Epoch: 2153 mean train loss:  2.16810018e-02, mean val. rec. loss:  2.08086769e-02\n",
      "Epoch: 2154 mean train loss:  2.16709814e-02, mean val. rec. loss:  2.07986027e-02\n",
      "Epoch: 2155 mean train loss:  2.16609740e-02, mean val. rec. loss:  2.07885352e-02\n",
      "Epoch: 2156 mean train loss:  2.16509815e-02, mean val. rec. loss:  2.07784836e-02\n",
      "Epoch: 2157 mean train loss:  2.16410076e-02, mean val. rec. loss:  2.07684457e-02\n",
      "Epoch: 2158 mean train loss:  2.16310467e-02, mean val. rec. loss:  2.07584191e-02\n",
      "Epoch: 2159 mean train loss:  2.16211008e-02, mean val. rec. loss:  2.07484015e-02\n",
      "Epoch: 2160 mean train loss:  2.16111716e-02, mean val. rec. loss:  2.07384021e-02\n",
      "Epoch: 2161 mean train loss:  2.16012572e-02, mean val. rec. loss:  2.07284118e-02\n",
      "Epoch: 2162 mean train loss:  2.15913560e-02, mean val. rec. loss:  2.07184418e-02\n",
      "Epoch: 2163 mean train loss:  2.15814771e-02, mean val. rec. loss:  2.07084719e-02\n",
      "Epoch: 2164 mean train loss:  2.15716056e-02, mean val. rec. loss:  2.06985224e-02\n",
      "Epoch: 2165 mean train loss:  2.15617527e-02, mean val. rec. loss:  2.06885820e-02\n",
      "Epoch: 2166 mean train loss:  2.15519129e-02, mean val. rec. loss:  2.06786529e-02\n",
      "Epoch: 2167 mean train loss:  2.15420843e-02, mean val. rec. loss:  2.06687419e-02\n",
      "Epoch: 2168 mean train loss:  2.15322761e-02, mean val. rec. loss:  2.06588355e-02\n",
      "Epoch: 2169 mean train loss:  2.15224791e-02, mean val. rec. loss:  2.06489427e-02\n",
      "Epoch: 2170 mean train loss:  2.15126989e-02, mean val. rec. loss:  2.06390680e-02\n",
      "Epoch: 2171 mean train loss:  2.15029336e-02, mean val. rec. loss:  2.06292024e-02\n",
      "Epoch: 2172 mean train loss:  2.14931813e-02, mean val. rec. loss:  2.06193527e-02\n",
      "Epoch: 2173 mean train loss:  2.14834439e-02, mean val. rec. loss:  2.06095166e-02\n",
      "Epoch: 2174 mean train loss:  2.14737233e-02, mean val. rec. loss:  2.05996850e-02\n",
      "Epoch: 2175 mean train loss:  2.14640175e-02, mean val. rec. loss:  2.05898716e-02\n",
      "Epoch: 2176 mean train loss:  2.14543304e-02, mean val. rec. loss:  2.05800672e-02\n",
      "Epoch: 2177 mean train loss:  2.14446470e-02, mean val. rec. loss:  2.05702810e-02\n",
      "Epoch: 2178 mean train loss:  2.14349823e-02, mean val. rec. loss:  2.05605016e-02\n",
      "Epoch: 2179 mean train loss:  2.14253361e-02, mean val. rec. loss:  2.05507358e-02\n",
      "Epoch: 2180 mean train loss:  2.14157030e-02, mean val. rec. loss:  2.05409790e-02\n",
      "Epoch: 2181 mean train loss:  2.14060829e-02, mean val. rec. loss:  2.05312359e-02\n",
      "Epoch: 2182 mean train loss:  2.13964759e-02, mean val. rec. loss:  2.05215064e-02\n",
      "Epoch: 2183 mean train loss:  2.13868819e-02, mean val. rec. loss:  2.05117905e-02\n",
      "Epoch: 2184 mean train loss:  2.13773065e-02, mean val. rec. loss:  2.05020882e-02\n",
      "Epoch: 2185 mean train loss:  2.13677497e-02, mean val. rec. loss:  2.04923882e-02\n",
      "Epoch: 2186 mean train loss:  2.13581967e-02, mean val. rec. loss:  2.04827153e-02\n",
      "Epoch: 2187 mean train loss:  2.13486641e-02, mean val. rec. loss:  2.04730402e-02\n",
      "Epoch: 2188 mean train loss:  2.13391446e-02, mean val. rec. loss:  2.04633856e-02\n",
      "Epoch: 2189 mean train loss:  2.13296381e-02, mean val. rec. loss:  2.04537377e-02\n",
      "Epoch: 2190 mean train loss:  2.13201484e-02, mean val. rec. loss:  2.04441057e-02\n",
      "Epoch: 2191 mean train loss:  2.13106755e-02, mean val. rec. loss:  2.04344850e-02\n",
      "Epoch: 2192 mean train loss:  2.13012099e-02, mean val. rec. loss:  2.04248735e-02\n",
      "Epoch: 2193 mean train loss:  2.12917630e-02, mean val. rec. loss:  2.04152732e-02\n",
      "Epoch: 2194 mean train loss:  2.12823236e-02, mean val. rec. loss:  2.04056866e-02\n",
      "Epoch: 2195 mean train loss:  2.12729046e-02, mean val. rec. loss:  2.03961203e-02\n",
      "Epoch: 2196 mean train loss:  2.12634987e-02, mean val. rec. loss:  2.03865587e-02\n",
      "Epoch: 2197 mean train loss:  2.12541040e-02, mean val. rec. loss:  2.03770060e-02\n",
      "Epoch: 2198 mean train loss:  2.12447260e-02, mean val. rec. loss:  2.03674670e-02\n",
      "Epoch: 2199 mean train loss:  2.12353610e-02, mean val. rec. loss:  2.03579416e-02\n",
      "Epoch: 2200 mean train loss:  2.12260128e-02, mean val. rec. loss:  2.03484253e-02\n",
      "Epoch: 2201 mean train loss:  2.12166739e-02, mean val. rec. loss:  2.03389271e-02\n",
      "Epoch: 2202 mean train loss:  2.12073518e-02, mean val. rec. loss:  2.03294357e-02\n",
      "Epoch: 2203 mean train loss:  2.11980446e-02, mean val. rec. loss:  2.03199511e-02\n",
      "Epoch: 2204 mean train loss:  2.11887448e-02, mean val. rec. loss:  2.03104870e-02\n",
      "Epoch: 2205 mean train loss:  2.11794636e-02, mean val. rec. loss:  2.03010296e-02\n",
      "Epoch: 2206 mean train loss:  2.11701955e-02, mean val. rec. loss:  2.02915858e-02\n",
      "Epoch: 2207 mean train loss:  2.11609386e-02, mean val. rec. loss:  2.02821534e-02\n",
      "Epoch: 2208 mean train loss:  2.11517003e-02, mean val. rec. loss:  2.02727301e-02\n",
      "Epoch: 2209 mean train loss:  2.11424694e-02, mean val. rec. loss:  2.02633226e-02\n",
      "Epoch: 2210 mean train loss:  2.11332590e-02, mean val. rec. loss:  2.02539219e-02\n",
      "Epoch: 2211 mean train loss:  2.11240579e-02, mean val. rec. loss:  2.02445372e-02\n",
      "Epoch: 2212 mean train loss:  2.11148680e-02, mean val. rec. loss:  2.02351637e-02\n",
      "Epoch: 2213 mean train loss:  2.11056986e-02, mean val. rec. loss:  2.02258039e-02\n",
      "Epoch: 2214 mean train loss:  2.10965384e-02, mean val. rec. loss:  2.02164508e-02\n",
      "Epoch: 2215 mean train loss:  2.10873895e-02, mean val. rec. loss:  2.02071137e-02\n",
      "Epoch: 2216 mean train loss:  2.10782610e-02, mean val. rec. loss:  2.01977833e-02\n",
      "Epoch: 2217 mean train loss:  2.10691400e-02, mean val. rec. loss:  2.01884665e-02\n",
      "Epoch: 2218 mean train loss:  2.10600302e-02, mean val. rec. loss:  2.01791611e-02\n",
      "Epoch: 2219 mean train loss:  2.10509427e-02, mean val. rec. loss:  2.01698648e-02\n",
      "Epoch: 2220 mean train loss:  2.10418589e-02, mean val. rec. loss:  2.01605866e-02\n",
      "Epoch: 2221 mean train loss:  2.10327957e-02, mean val. rec. loss:  2.01513129e-02\n",
      "Epoch: 2222 mean train loss:  2.10237398e-02, mean val. rec. loss:  2.01420597e-02\n",
      "Epoch: 2223 mean train loss:  2.10146970e-02, mean val. rec. loss:  2.01328110e-02\n",
      "Epoch: 2224 mean train loss:  2.10056766e-02, mean val. rec. loss:  2.01235691e-02\n",
      "Epoch: 2225 mean train loss:  2.09966599e-02, mean val. rec. loss:  2.01143521e-02\n",
      "Epoch: 2226 mean train loss:  2.09876618e-02, mean val. rec. loss:  2.01051374e-02\n",
      "Epoch: 2227 mean train loss:  2.09786748e-02, mean val. rec. loss:  2.00959295e-02\n",
      "Epoch: 2228 mean train loss:  2.09697009e-02, mean val. rec. loss:  2.00867443e-02\n",
      "Epoch: 2229 mean train loss:  2.09607419e-02, mean val. rec. loss:  2.00775614e-02\n",
      "Epoch: 2230 mean train loss:  2.09517922e-02, mean val. rec. loss:  2.00683943e-02\n",
      "Epoch: 2231 mean train loss:  2.09428574e-02, mean val. rec. loss:  2.00592386e-02\n",
      "Epoch: 2232 mean train loss:  2.09339376e-02, mean val. rec. loss:  2.00500897e-02\n",
      "Epoch: 2233 mean train loss:  2.09250288e-02, mean val. rec. loss:  2.00409566e-02\n",
      "Epoch: 2234 mean train loss:  2.09161294e-02, mean val. rec. loss:  2.00318281e-02\n",
      "Epoch: 2235 mean train loss:  2.09072486e-02, mean val. rec. loss:  2.00227223e-02\n",
      "Epoch: 2236 mean train loss:  2.08983846e-02, mean val. rec. loss:  2.00136210e-02\n",
      "Epoch: 2237 mean train loss:  2.08895243e-02, mean val. rec. loss:  2.00045265e-02\n",
      "Epoch: 2238 mean train loss:  2.08806789e-02, mean val. rec. loss:  1.99954547e-02\n",
      "Epoch: 2239 mean train loss:  2.08718540e-02, mean val. rec. loss:  1.99863874e-02\n",
      "Epoch: 2240 mean train loss:  2.08630328e-02, mean val. rec. loss:  1.99773314e-02\n",
      "Epoch: 2241 mean train loss:  2.08542265e-02, mean val. rec. loss:  1.99682846e-02\n",
      "Epoch: 2242 mean train loss:  2.08454370e-02, mean val. rec. loss:  1.99592445e-02\n",
      "Epoch: 2243 mean train loss:  2.08366511e-02, mean val. rec. loss:  1.99502226e-02\n",
      "Epoch: 2244 mean train loss:  2.08278821e-02, mean val. rec. loss:  1.99412120e-02\n",
      "Epoch: 2245 mean train loss:  2.08191316e-02, mean val. rec. loss:  1.99322196e-02\n",
      "Epoch: 2246 mean train loss:  2.08103924e-02, mean val. rec. loss:  1.99232294e-02\n",
      "Epoch: 2247 mean train loss:  2.08016662e-02, mean val. rec. loss:  1.99142483e-02\n",
      "Epoch: 2248 mean train loss:  2.07929474e-02, mean val. rec. loss:  1.99052808e-02\n",
      "Epoch: 2249 mean train loss:  2.07842435e-02, mean val. rec. loss:  1.98963224e-02\n",
      "Epoch: 2250 mean train loss:  2.07755508e-02, mean val. rec. loss:  1.98873776e-02\n",
      "Epoch: 2251 mean train loss:  2.07668711e-02, mean val. rec. loss:  1.98784464e-02\n",
      "Epoch: 2252 mean train loss:  2.07582101e-02, mean val. rec. loss:  1.98695243e-02\n",
      "Epoch: 2253 mean train loss:  2.07495528e-02, mean val. rec. loss:  1.98606135e-02\n",
      "Epoch: 2254 mean train loss:  2.07409141e-02, mean val. rec. loss:  1.98517118e-02\n",
      "Epoch: 2255 mean train loss:  2.07322884e-02, mean val. rec. loss:  1.98428169e-02\n",
      "Epoch: 2256 mean train loss:  2.07236665e-02, mean val. rec. loss:  1.98339378e-02\n",
      "Epoch: 2257 mean train loss:  2.07150631e-02, mean val. rec. loss:  1.98250724e-02\n",
      "Epoch: 2258 mean train loss:  2.07064747e-02, mean val. rec. loss:  1.98162183e-02\n",
      "Epoch: 2259 mean train loss:  2.06978975e-02, mean val. rec. loss:  1.98073733e-02\n",
      "Epoch: 2260 mean train loss:  2.06893351e-02, mean val. rec. loss:  1.97985374e-02\n",
      "Epoch: 2261 mean train loss:  2.06807802e-02, mean val. rec. loss:  1.97897105e-02\n",
      "Epoch: 2262 mean train loss:  2.06722346e-02, mean val. rec. loss:  1.97808972e-02\n",
      "Epoch: 2263 mean train loss:  2.06637095e-02, mean val. rec. loss:  1.97720930e-02\n",
      "Epoch: 2264 mean train loss:  2.06551918e-02, mean val. rec. loss:  1.97633002e-02\n",
      "Epoch: 2265 mean train loss:  2.06466872e-02, mean val. rec. loss:  1.97545187e-02\n",
      "Epoch: 2266 mean train loss:  2.06381956e-02, mean val. rec. loss:  1.97457417e-02\n",
      "Epoch: 2267 mean train loss:  2.06297134e-02, mean val. rec. loss:  1.97369874e-02\n",
      "Epoch: 2268 mean train loss:  2.06212478e-02, mean val. rec. loss:  1.97282331e-02\n",
      "Epoch: 2269 mean train loss:  2.06127842e-02, mean val. rec. loss:  1.97194992e-02\n",
      "Epoch: 2270 mean train loss:  2.06043447e-02, mean val. rec. loss:  1.97107698e-02\n",
      "Epoch: 2271 mean train loss:  2.05959127e-02, mean val. rec. loss:  1.97020496e-02\n",
      "Epoch: 2272 mean train loss:  2.05874919e-02, mean val. rec. loss:  1.96933452e-02\n",
      "Epoch: 2273 mean train loss:  2.05790897e-02, mean val. rec. loss:  1.96846498e-02\n",
      "Epoch: 2274 mean train loss:  2.05706912e-02, mean val. rec. loss:  1.96759681e-02\n",
      "Epoch: 2275 mean train loss:  2.05623058e-02, mean val. rec. loss:  1.96672864e-02\n",
      "Epoch: 2276 mean train loss:  2.05539352e-02, mean val. rec. loss:  1.96586273e-02\n",
      "Epoch: 2277 mean train loss:  2.05455814e-02, mean val. rec. loss:  1.96499773e-02\n",
      "Epoch: 2278 mean train loss:  2.05372351e-02, mean val. rec. loss:  1.96413251e-02\n",
      "Epoch: 2279 mean train loss:  2.05288944e-02, mean val. rec. loss:  1.96326978e-02\n",
      "Epoch: 2280 mean train loss:  2.05205704e-02, mean val. rec. loss:  1.96240751e-02\n",
      "Epoch: 2281 mean train loss:  2.05122594e-02, mean val. rec. loss:  1.96154659e-02\n",
      "Epoch: 2282 mean train loss:  2.05039578e-02, mean val. rec. loss:  1.96068636e-02\n",
      "Epoch: 2283 mean train loss:  2.04956710e-02, mean val. rec. loss:  1.95982794e-02\n",
      "Epoch: 2284 mean train loss:  2.04873992e-02, mean val. rec. loss:  1.95896997e-02\n",
      "Epoch: 2285 mean train loss:  2.04791348e-02, mean val. rec. loss:  1.95811336e-02\n",
      "Epoch: 2286 mean train loss:  2.04708815e-02, mean val. rec. loss:  1.95725721e-02\n",
      "Epoch: 2287 mean train loss:  2.04626432e-02, mean val. rec. loss:  1.95640265e-02\n",
      "Epoch: 2288 mean train loss:  2.04544123e-02, mean val. rec. loss:  1.95554922e-02\n",
      "Epoch: 2289 mean train loss:  2.04461963e-02, mean val. rec. loss:  1.95469646e-02\n",
      "Epoch: 2290 mean train loss:  2.04379953e-02, mean val. rec. loss:  1.95384485e-02\n",
      "Epoch: 2291 mean train loss:  2.04297998e-02, mean val. rec. loss:  1.95299459e-02\n",
      "Epoch: 2292 mean train loss:  2.04216229e-02, mean val. rec. loss:  1.95214524e-02\n",
      "Epoch: 2293 mean train loss:  2.04134516e-02, mean val. rec. loss:  1.95129658e-02\n",
      "Epoch: 2294 mean train loss:  2.04052915e-02, mean val. rec. loss:  1.95044972e-02\n",
      "Epoch: 2295 mean train loss:  2.03971425e-02, mean val. rec. loss:  1.94960287e-02\n",
      "Epoch: 2296 mean train loss:  2.03890085e-02, mean val. rec. loss:  1.94875738e-02\n",
      "Epoch: 2297 mean train loss:  2.03808837e-02, mean val. rec. loss:  1.94791392e-02\n",
      "Epoch: 2298 mean train loss:  2.03727776e-02, mean val. rec. loss:  1.94707115e-02\n",
      "Epoch: 2299 mean train loss:  2.03646790e-02, mean val. rec. loss:  1.94622906e-02\n",
      "Epoch: 2300 mean train loss:  2.03565933e-02, mean val. rec. loss:  1.94538765e-02\n",
      "Epoch: 2301 mean train loss:  2.03485170e-02, mean val. rec. loss:  1.94454760e-02\n",
      "Epoch: 2302 mean train loss:  2.03404444e-02, mean val. rec. loss:  1.94370869e-02\n",
      "Epoch: 2303 mean train loss:  2.03323942e-02, mean val. rec. loss:  1.94287045e-02\n",
      "Epoch: 2304 mean train loss:  2.03243495e-02, mean val. rec. loss:  1.94203335e-02\n",
      "Epoch: 2305 mean train loss:  2.03163160e-02, mean val. rec. loss:  1.94119761e-02\n",
      "Epoch: 2306 mean train loss:  2.03082993e-02, mean val. rec. loss:  1.94036323e-02\n",
      "Epoch: 2307 mean train loss:  2.03002937e-02, mean val. rec. loss:  1.93952953e-02\n",
      "Epoch: 2308 mean train loss:  2.02923031e-02, mean val. rec. loss:  1.93869674e-02\n",
      "Epoch: 2309 mean train loss:  2.02843143e-02, mean val. rec. loss:  1.93786485e-02\n",
      "Epoch: 2310 mean train loss:  2.02763385e-02, mean val. rec. loss:  1.93703455e-02\n",
      "Epoch: 2311 mean train loss:  2.02683758e-02, mean val. rec. loss:  1.93620380e-02\n",
      "Epoch: 2312 mean train loss:  2.02604242e-02, mean val. rec. loss:  1.93537600e-02\n",
      "Epoch: 2313 mean train loss:  2.02524838e-02, mean val. rec. loss:  1.93454865e-02\n",
      "Epoch: 2314 mean train loss:  2.02445602e-02, mean val. rec. loss:  1.93372175e-02\n",
      "Epoch: 2315 mean train loss:  2.02366440e-02, mean val. rec. loss:  1.93289667e-02\n",
      "Epoch: 2316 mean train loss:  2.02287409e-02, mean val. rec. loss:  1.93207204e-02\n",
      "Epoch: 2317 mean train loss:  2.02208452e-02, mean val. rec. loss:  1.93124832e-02\n",
      "Epoch: 2318 mean train loss:  2.02129626e-02, mean val. rec. loss:  1.93042551e-02\n",
      "Epoch: 2319 mean train loss:  2.02050911e-02, mean val. rec. loss:  1.92960406e-02\n",
      "Epoch: 2320 mean train loss:  2.01972289e-02, mean val. rec. loss:  1.92878351e-02\n",
      "Epoch: 2321 mean train loss:  2.01893779e-02, mean val. rec. loss:  1.92796410e-02\n",
      "Epoch: 2322 mean train loss:  2.01815418e-02, mean val. rec. loss:  1.92714560e-02\n",
      "Epoch: 2323 mean train loss:  2.01737150e-02, mean val. rec. loss:  1.92632823e-02\n",
      "Epoch: 2324 mean train loss:  2.01658994e-02, mean val. rec. loss:  1.92551199e-02\n",
      "Epoch: 2325 mean train loss:  2.01580968e-02, mean val. rec. loss:  1.92469643e-02\n",
      "Epoch: 2326 mean train loss:  2.01503017e-02, mean val. rec. loss:  1.92388201e-02\n",
      "Epoch: 2327 mean train loss:  2.01425196e-02, mean val. rec. loss:  1.92306804e-02\n",
      "Epoch: 2328 mean train loss:  2.01347450e-02, mean val. rec. loss:  1.92225634e-02\n",
      "Epoch: 2329 mean train loss:  2.01269871e-02, mean val. rec. loss:  1.92144510e-02\n",
      "Epoch: 2330 mean train loss:  2.01192404e-02, mean val. rec. loss:  1.92063430e-02\n",
      "Epoch: 2331 mean train loss:  2.01115048e-02, mean val. rec. loss:  1.91982510e-02\n",
      "Epoch: 2332 mean train loss:  2.01037730e-02, mean val. rec. loss:  1.91901680e-02\n",
      "Epoch: 2333 mean train loss:  2.00960598e-02, mean val. rec. loss:  1.91820963e-02\n",
      "Epoch: 2334 mean train loss:  2.00883559e-02, mean val. rec. loss:  1.91740338e-02\n",
      "Epoch: 2335 mean train loss:  2.00806595e-02, mean val. rec. loss:  1.91659825e-02\n",
      "Epoch: 2336 mean train loss:  2.00729817e-02, mean val. rec. loss:  1.91579381e-02\n",
      "Epoch: 2337 mean train loss:  2.00653076e-02, mean val. rec. loss:  1.91499073e-02\n",
      "Epoch: 2338 mean train loss:  2.00576428e-02, mean val. rec. loss:  1.91418833e-02\n",
      "Epoch: 2339 mean train loss:  2.00499967e-02, mean val. rec. loss:  1.91338683e-02\n",
      "Epoch: 2340 mean train loss:  2.00423543e-02, mean val. rec. loss:  1.91258647e-02\n",
      "Epoch: 2341 mean train loss:  2.00347304e-02, mean val. rec. loss:  1.91178747e-02\n",
      "Epoch: 2342 mean train loss:  2.00271141e-02, mean val. rec. loss:  1.91098938e-02\n",
      "Epoch: 2343 mean train loss:  2.00195070e-02, mean val. rec. loss:  1.91019174e-02\n",
      "Epoch: 2344 mean train loss:  2.00119074e-02, mean val. rec. loss:  1.90939546e-02\n",
      "Epoch: 2345 mean train loss:  2.00043246e-02, mean val. rec. loss:  1.90860009e-02\n",
      "Epoch: 2346 mean train loss:  1.99967473e-02, mean val. rec. loss:  1.90780585e-02\n",
      "Epoch: 2347 mean train loss:  1.99891831e-02, mean val. rec. loss:  1.90701275e-02\n",
      "Epoch: 2348 mean train loss:  1.99816357e-02, mean val. rec. loss:  1.90622055e-02\n",
      "Epoch: 2349 mean train loss:  1.99740938e-02, mean val. rec. loss:  1.90542949e-02\n",
      "Epoch: 2350 mean train loss:  1.99665594e-02, mean val. rec. loss:  1.90463866e-02\n",
      "Epoch: 2351 mean train loss:  1.99590361e-02, mean val. rec. loss:  1.90384918e-02\n",
      "Epoch: 2352 mean train loss:  1.99515259e-02, mean val. rec. loss:  1.90306107e-02\n",
      "Epoch: 2353 mean train loss:  1.99440306e-02, mean val. rec. loss:  1.90227386e-02\n",
      "Epoch: 2354 mean train loss:  1.99365409e-02, mean val. rec. loss:  1.90148756e-02\n",
      "Epoch: 2355 mean train loss:  1.99290660e-02, mean val. rec. loss:  1.90070172e-02\n",
      "Epoch: 2356 mean train loss:  1.99215987e-02, mean val. rec. loss:  1.89991768e-02\n",
      "Epoch: 2357 mean train loss:  1.99141424e-02, mean val. rec. loss:  1.89913411e-02\n",
      "Epoch: 2358 mean train loss:  1.99066937e-02, mean val. rec. loss:  1.89835144e-02\n",
      "Epoch: 2359 mean train loss:  1.98992561e-02, mean val. rec. loss:  1.89756990e-02\n",
      "Epoch: 2360 mean train loss:  1.98918315e-02, mean val. rec. loss:  1.89678961e-02\n",
      "Epoch: 2361 mean train loss:  1.98844200e-02, mean val. rec. loss:  1.89601045e-02\n",
      "Epoch: 2362 mean train loss:  1.98770141e-02, mean val. rec. loss:  1.89523164e-02\n",
      "Epoch: 2363 mean train loss:  1.98696249e-02, mean val. rec. loss:  1.89445396e-02\n",
      "Epoch: 2364 mean train loss:  1.98622376e-02, mean val. rec. loss:  1.89367764e-02\n",
      "Epoch: 2365 mean train loss:  1.98548689e-02, mean val. rec. loss:  1.89290200e-02\n",
      "Epoch: 2366 mean train loss:  1.98475058e-02, mean val. rec. loss:  1.89212692e-02\n",
      "Epoch: 2367 mean train loss:  1.98401502e-02, mean val. rec. loss:  1.89135321e-02\n",
      "Epoch: 2368 mean train loss:  1.98328113e-02, mean val. rec. loss:  1.89058097e-02\n",
      "Epoch: 2369 mean train loss:  1.98254835e-02, mean val. rec. loss:  1.88980987e-02\n",
      "Epoch: 2370 mean train loss:  1.98181651e-02, mean val. rec. loss:  1.88903877e-02\n",
      "Epoch: 2371 mean train loss:  1.98108542e-02, mean val. rec. loss:  1.88826925e-02\n",
      "Epoch: 2372 mean train loss:  1.98035544e-02, mean val. rec. loss:  1.88750030e-02\n",
      "Epoch: 2373 mean train loss:  1.97962639e-02, mean val. rec. loss:  1.88673271e-02\n",
      "Epoch: 2374 mean train loss:  1.97889902e-02, mean val. rec. loss:  1.88596580e-02\n",
      "Epoch: 2375 mean train loss:  1.97817202e-02, mean val. rec. loss:  1.88520003e-02\n",
      "Epoch: 2376 mean train loss:  1.97744633e-02, mean val. rec. loss:  1.88443539e-02\n",
      "Epoch: 2377 mean train loss:  1.97672193e-02, mean val. rec. loss:  1.88367143e-02\n",
      "Epoch: 2378 mean train loss:  1.97599773e-02, mean val. rec. loss:  1.88290872e-02\n",
      "Epoch: 2379 mean train loss:  1.97527538e-02, mean val. rec. loss:  1.88214646e-02\n",
      "Epoch: 2380 mean train loss:  1.97455360e-02, mean val. rec. loss:  1.88138579e-02\n",
      "Epoch: 2381 mean train loss:  1.97383293e-02, mean val. rec. loss:  1.88062545e-02\n",
      "Epoch: 2382 mean train loss:  1.97311338e-02, mean val. rec. loss:  1.87986626e-02\n",
      "Epoch: 2383 mean train loss:  1.97239532e-02, mean val. rec. loss:  1.87910808e-02\n",
      "Epoch: 2384 mean train loss:  1.97167726e-02, mean val. rec. loss:  1.87835104e-02\n",
      "Epoch: 2385 mean train loss:  1.97096106e-02, mean val. rec. loss:  1.87759479e-02\n",
      "Epoch: 2386 mean train loss:  1.97024542e-02, mean val. rec. loss:  1.87683979e-02\n",
      "Epoch: 2387 mean train loss:  1.96953090e-02, mean val. rec. loss:  1.87608558e-02\n",
      "Epoch: 2388 mean train loss:  1.96881749e-02, mean val. rec. loss:  1.87533239e-02\n",
      "Epoch: 2389 mean train loss:  1.96810539e-02, mean val. rec. loss:  1.87457988e-02\n",
      "Epoch: 2390 mean train loss:  1.96739348e-02, mean val. rec. loss:  1.87382840e-02\n",
      "Epoch: 2391 mean train loss:  1.96668324e-02, mean val. rec. loss:  1.87307759e-02\n",
      "Epoch: 2392 mean train loss:  1.96597374e-02, mean val. rec. loss:  1.87232894e-02\n",
      "Epoch: 2393 mean train loss:  1.96526536e-02, mean val. rec. loss:  1.87158029e-02\n",
      "Epoch: 2394 mean train loss:  1.96455829e-02, mean val. rec. loss:  1.87083232e-02\n",
      "Epoch: 2395 mean train loss:  1.96385178e-02, mean val. rec. loss:  1.87008594e-02\n",
      "Epoch: 2396 mean train loss:  1.96314601e-02, mean val. rec. loss:  1.86934023e-02\n",
      "Epoch: 2397 mean train loss:  1.96244154e-02, mean val. rec. loss:  1.86859544e-02\n",
      "Epoch: 2398 mean train loss:  1.96173838e-02, mean val. rec. loss:  1.86785223e-02\n",
      "Epoch: 2399 mean train loss:  1.96103633e-02, mean val. rec. loss:  1.86710902e-02\n",
      "Epoch: 2400 mean train loss:  1.96033466e-02, mean val. rec. loss:  1.86636729e-02\n",
      "Epoch: 2401 mean train loss:  1.95963447e-02, mean val. rec. loss:  1.86562669e-02\n",
      "Epoch: 2402 mean train loss:  1.95893522e-02, mean val. rec. loss:  1.86488665e-02\n",
      "Epoch: 2403 mean train loss:  1.95823690e-02, mean val. rec. loss:  1.86414764e-02\n",
      "Epoch: 2404 mean train loss:  1.95753951e-02, mean val. rec. loss:  1.86340953e-02\n",
      "Epoch: 2405 mean train loss:  1.95684342e-02, mean val. rec. loss:  1.86267268e-02\n",
      "Epoch: 2406 mean train loss:  1.95614789e-02, mean val. rec. loss:  1.86193650e-02\n",
      "Epoch: 2407 mean train loss:  1.95545348e-02, mean val. rec. loss:  1.86120146e-02\n",
      "Epoch: 2408 mean train loss:  1.95476038e-02, mean val. rec. loss:  1.86046698e-02\n",
      "Epoch: 2409 mean train loss:  1.95406764e-02, mean val. rec. loss:  1.85973364e-02\n",
      "Epoch: 2410 mean train loss:  1.95337640e-02, mean val. rec. loss:  1.85900154e-02\n",
      "Epoch: 2411 mean train loss:  1.95268571e-02, mean val. rec. loss:  1.85827013e-02\n",
      "Epoch: 2412 mean train loss:  1.95199688e-02, mean val. rec. loss:  1.85753916e-02\n",
      "Epoch: 2413 mean train loss:  1.95130787e-02, mean val. rec. loss:  1.85681036e-02\n",
      "Epoch: 2414 mean train loss:  1.95062054e-02, mean val. rec. loss:  1.85608132e-02\n",
      "Epoch: 2415 mean train loss:  1.94993432e-02, mean val. rec. loss:  1.85535354e-02\n",
      "Epoch: 2416 mean train loss:  1.94924848e-02, mean val. rec. loss:  1.85462700e-02\n",
      "Epoch: 2417 mean train loss:  1.94856412e-02, mean val. rec. loss:  1.85390080e-02\n",
      "Epoch: 2418 mean train loss:  1.94788033e-02, mean val. rec. loss:  1.85317608e-02\n",
      "Epoch: 2419 mean train loss:  1.94719802e-02, mean val. rec. loss:  1.85245226e-02\n",
      "Epoch: 2420 mean train loss:  1.94651664e-02, mean val. rec. loss:  1.85172890e-02\n",
      "Epoch: 2421 mean train loss:  1.94583564e-02, mean val. rec. loss:  1.85100678e-02\n",
      "Epoch: 2422 mean train loss:  1.94515650e-02, mean val. rec. loss:  1.85028557e-02\n",
      "Epoch: 2423 mean train loss:  1.94447736e-02, mean val. rec. loss:  1.84956538e-02\n",
      "Epoch: 2424 mean train loss:  1.94379952e-02, mean val. rec. loss:  1.84884633e-02\n",
      "Epoch: 2425 mean train loss:  1.94312317e-02, mean val. rec. loss:  1.84812761e-02\n",
      "Epoch: 2426 mean train loss:  1.94244720e-02, mean val. rec. loss:  1.84741026e-02\n",
      "Epoch: 2427 mean train loss:  1.94177234e-02, mean val. rec. loss:  1.84669404e-02\n",
      "Epoch: 2428 mean train loss:  1.94109879e-02, mean val. rec. loss:  1.84597782e-02\n",
      "Epoch: 2429 mean train loss:  1.94042523e-02, mean val. rec. loss:  1.84526364e-02\n",
      "Epoch: 2430 mean train loss:  1.93975373e-02, mean val. rec. loss:  1.84454992e-02\n",
      "Epoch: 2431 mean train loss:  1.93908278e-02, mean val. rec. loss:  1.84383710e-02\n",
      "Epoch: 2432 mean train loss:  1.93841295e-02, mean val. rec. loss:  1.84312496e-02\n",
      "Epoch: 2433 mean train loss:  1.93774368e-02, mean val. rec. loss:  1.84241452e-02\n",
      "Epoch: 2434 mean train loss:  1.93707571e-02, mean val. rec. loss:  1.84170466e-02\n",
      "Epoch: 2435 mean train loss:  1.93640886e-02, mean val. rec. loss:  1.84099513e-02\n",
      "Epoch: 2436 mean train loss:  1.93574294e-02, mean val. rec. loss:  1.84028707e-02\n",
      "Epoch: 2437 mean train loss:  1.93507702e-02, mean val. rec. loss:  1.83957992e-02\n",
      "Epoch: 2438 mean train loss:  1.93441296e-02, mean val. rec. loss:  1.83887357e-02\n",
      "Epoch: 2439 mean train loss:  1.93374965e-02, mean val. rec. loss:  1.83816835e-02\n",
      "Epoch: 2440 mean train loss:  1.93308746e-02, mean val. rec. loss:  1.83746302e-02\n",
      "Epoch: 2441 mean train loss:  1.93242564e-02, mean val. rec. loss:  1.83675972e-02\n",
      "Epoch: 2442 mean train loss:  1.93176493e-02, mean val. rec. loss:  1.83605745e-02\n",
      "Epoch: 2443 mean train loss:  1.93110571e-02, mean val. rec. loss:  1.83535563e-02\n",
      "Epoch: 2444 mean train loss:  1.93044724e-02, mean val. rec. loss:  1.83465472e-02\n",
      "Epoch: 2445 mean train loss:  1.92978933e-02, mean val. rec. loss:  1.83395472e-02\n",
      "Epoch: 2446 mean train loss:  1.92913291e-02, mean val. rec. loss:  1.83325574e-02\n",
      "Epoch: 2447 mean train loss:  1.92847705e-02, mean val. rec. loss:  1.83255743e-02\n",
      "Epoch: 2448 mean train loss:  1.92782193e-02, mean val. rec. loss:  1.83186004e-02\n",
      "Epoch: 2449 mean train loss:  1.92716793e-02, mean val. rec. loss:  1.83116400e-02\n",
      "Epoch: 2450 mean train loss:  1.92651523e-02, mean val. rec. loss:  1.83046842e-02\n",
      "Epoch: 2451 mean train loss:  1.92586328e-02, mean val. rec. loss:  1.82977397e-02\n",
      "Epoch: 2452 mean train loss:  1.92521225e-02, mean val. rec. loss:  1.82908043e-02\n",
      "Epoch: 2453 mean train loss:  1.92456198e-02, mean val. rec. loss:  1.82838757e-02\n",
      "Epoch: 2454 mean train loss:  1.92391282e-02, mean val. rec. loss:  1.82769551e-02\n",
      "Epoch: 2455 mean train loss:  1.92326422e-02, mean val. rec. loss:  1.82700526e-02\n",
      "Epoch: 2456 mean train loss:  1.92261729e-02, mean val. rec. loss:  1.82631535e-02\n",
      "Epoch: 2457 mean train loss:  1.92197074e-02, mean val. rec. loss:  1.82562600e-02\n",
      "Epoch: 2458 mean train loss:  1.92132512e-02, mean val. rec. loss:  1.82493802e-02\n",
      "Epoch: 2459 mean train loss:  1.92068080e-02, mean val. rec. loss:  1.82425105e-02\n",
      "Epoch: 2460 mean train loss:  1.92003704e-02, mean val. rec. loss:  1.82356523e-02\n",
      "Epoch: 2461 mean train loss:  1.91939440e-02, mean val. rec. loss:  1.82287940e-02\n",
      "Epoch: 2462 mean train loss:  1.91875232e-02, mean val. rec. loss:  1.82219493e-02\n",
      "Epoch: 2463 mean train loss:  1.91811172e-02, mean val. rec. loss:  1.82151182e-02\n",
      "Epoch: 2464 mean train loss:  1.91747206e-02, mean val. rec. loss:  1.82082962e-02\n",
      "Epoch: 2465 mean train loss:  1.91683314e-02, mean val. rec. loss:  1.82014787e-02\n",
      "Epoch: 2466 mean train loss:  1.91619516e-02, mean val. rec. loss:  1.81946669e-02\n",
      "Epoch: 2467 mean train loss:  1.91555791e-02, mean val. rec. loss:  1.81878665e-02\n",
      "Epoch: 2468 mean train loss:  1.91492142e-02, mean val. rec. loss:  1.81810762e-02\n",
      "Epoch: 2469 mean train loss:  1.91428660e-02, mean val. rec. loss:  1.81742916e-02\n",
      "Epoch: 2470 mean train loss:  1.91365178e-02, mean val. rec. loss:  1.81675229e-02\n",
      "Epoch: 2471 mean train loss:  1.91301863e-02, mean val. rec. loss:  1.81607531e-02\n",
      "Epoch: 2472 mean train loss:  1.91238549e-02, mean val. rec. loss:  1.81539991e-02\n",
      "Epoch: 2473 mean train loss:  1.91175346e-02, mean val. rec. loss:  1.81472565e-02\n",
      "Epoch: 2474 mean train loss:  1.91112311e-02, mean val. rec. loss:  1.81405230e-02\n",
      "Epoch: 2475 mean train loss:  1.91049331e-02, mean val. rec. loss:  1.81337905e-02\n",
      "Epoch: 2476 mean train loss:  1.90986408e-02, mean val. rec. loss:  1.81270695e-02\n",
      "Epoch: 2477 mean train loss:  1.90923578e-02, mean val. rec. loss:  1.81203586e-02\n",
      "Epoch: 2478 mean train loss:  1.90860859e-02, mean val. rec. loss:  1.81136602e-02\n",
      "Epoch: 2479 mean train loss:  1.90798252e-02, mean val. rec. loss:  1.81069697e-02\n",
      "Epoch: 2480 mean train loss:  1.90735757e-02, mean val. rec. loss:  1.81002815e-02\n",
      "Epoch: 2481 mean train loss:  1.90673262e-02, mean val. rec. loss:  1.80936081e-02\n",
      "Epoch: 2482 mean train loss:  1.90610916e-02, mean val. rec. loss:  1.80869471e-02\n",
      "Epoch: 2483 mean train loss:  1.90548681e-02, mean val. rec. loss:  1.80802861e-02\n",
      "Epoch: 2484 mean train loss:  1.90486522e-02, mean val. rec. loss:  1.80736388e-02\n",
      "Epoch: 2485 mean train loss:  1.90424418e-02, mean val. rec. loss:  1.80670005e-02\n",
      "Epoch: 2486 mean train loss:  1.90362388e-02, mean val. rec. loss:  1.80603667e-02\n",
      "Epoch: 2487 mean train loss:  1.90300451e-02, mean val. rec. loss:  1.80537488e-02\n",
      "Epoch: 2488 mean train loss:  1.90238683e-02, mean val. rec. loss:  1.80471355e-02\n",
      "Epoch: 2489 mean train loss:  1.90176914e-02, mean val. rec. loss:  1.80405289e-02\n",
      "Epoch: 2490 mean train loss:  1.90115257e-02, mean val. rec. loss:  1.80339314e-02\n",
      "Epoch: 2491 mean train loss:  1.90053711e-02, mean val. rec. loss:  1.80273487e-02\n",
      "Epoch: 2492 mean train loss:  1.89992259e-02, mean val. rec. loss:  1.80207762e-02\n",
      "Epoch: 2493 mean train loss:  1.89930900e-02, mean val. rec. loss:  1.80142070e-02\n",
      "Epoch: 2494 mean train loss:  1.89869652e-02, mean val. rec. loss:  1.80076459e-02\n",
      "Epoch: 2495 mean train loss:  1.89808386e-02, mean val. rec. loss:  1.80010949e-02\n",
      "Epoch: 2496 mean train loss:  1.89747344e-02, mean val. rec. loss:  1.79945564e-02\n",
      "Epoch: 2497 mean train loss:  1.89686301e-02, mean val. rec. loss:  1.79880190e-02\n",
      "Epoch: 2498 mean train loss:  1.89625296e-02, mean val. rec. loss:  1.79814929e-02\n",
      "Epoch: 2499 mean train loss:  1.89564458e-02, mean val. rec. loss:  1.79749760e-02\n",
      "Epoch: 2500 mean train loss:  1.89503695e-02, mean val. rec. loss:  1.79684783e-02\n",
      "Epoch: 2501 mean train loss:  1.89443062e-02, mean val. rec. loss:  1.79619761e-02\n",
      "Epoch: 2502 mean train loss:  1.89382503e-02, mean val. rec. loss:  1.79554818e-02\n",
      "Epoch: 2503 mean train loss:  1.89322001e-02, mean val. rec. loss:  1.79489988e-02\n",
      "Epoch: 2504 mean train loss:  1.89261573e-02, mean val. rec. loss:  1.79425329e-02\n",
      "Epoch: 2505 mean train loss:  1.89201294e-02, mean val. rec. loss:  1.79360670e-02\n",
      "Epoch: 2506 mean train loss:  1.89141070e-02, mean val. rec. loss:  1.79296135e-02\n",
      "Epoch: 2507 mean train loss:  1.89080940e-02, mean val. rec. loss:  1.79231623e-02\n",
      "Epoch: 2508 mean train loss:  1.89020829e-02, mean val. rec. loss:  1.79167259e-02\n",
      "Epoch: 2509 mean train loss:  1.88960903e-02, mean val. rec. loss:  1.79102985e-02\n",
      "Epoch: 2510 mean train loss:  1.88900997e-02, mean val. rec. loss:  1.79038825e-02\n",
      "Epoch: 2511 mean train loss:  1.88841202e-02, mean val. rec. loss:  1.78974641e-02\n",
      "Epoch: 2512 mean train loss:  1.88781500e-02, mean val. rec. loss:  1.78910663e-02\n",
      "Epoch: 2513 mean train loss:  1.88721873e-02, mean val. rec. loss:  1.78846661e-02\n",
      "Epoch: 2514 mean train loss:  1.88662320e-02, mean val. rec. loss:  1.78782841e-02\n",
      "Epoch: 2515 mean train loss:  1.88602879e-02, mean val. rec. loss:  1.78719043e-02\n",
      "Epoch: 2516 mean train loss:  1.88543512e-02, mean val. rec. loss:  1.78655450e-02\n",
      "Epoch: 2517 mean train loss:  1.88484257e-02, mean val. rec. loss:  1.78591822e-02\n",
      "Epoch: 2518 mean train loss:  1.88425058e-02, mean val. rec. loss:  1.78528286e-02\n",
      "Epoch: 2519 mean train loss:  1.88365933e-02, mean val. rec. loss:  1.78464862e-02\n",
      "Epoch: 2520 mean train loss:  1.88306902e-02, mean val. rec. loss:  1.78401564e-02\n",
      "Epoch: 2521 mean train loss:  1.88248001e-02, mean val. rec. loss:  1.78338299e-02\n",
      "Epoch: 2522 mean train loss:  1.88189137e-02, mean val. rec. loss:  1.78275159e-02\n",
      "Epoch: 2523 mean train loss:  1.88130366e-02, mean val. rec. loss:  1.78212020e-02\n",
      "Epoch: 2524 mean train loss:  1.88071652e-02, mean val. rec. loss:  1.78149095e-02\n",
      "Epoch: 2525 mean train loss:  1.88013104e-02, mean val. rec. loss:  1.78086205e-02\n",
      "Epoch: 2526 mean train loss:  1.87954594e-02, mean val. rec. loss:  1.78023394e-02\n",
      "Epoch: 2527 mean train loss:  1.87896159e-02, mean val. rec. loss:  1.77960606e-02\n",
      "Epoch: 2528 mean train loss:  1.87837779e-02, mean val. rec. loss:  1.77897999e-02\n",
      "Epoch: 2529 mean train loss:  1.87779586e-02, mean val. rec. loss:  1.77835381e-02\n",
      "Epoch: 2530 mean train loss:  1.87721392e-02, mean val. rec. loss:  1.77772944e-02\n",
      "Epoch: 2531 mean train loss:  1.87663348e-02, mean val. rec. loss:  1.77710484e-02\n",
      "Epoch: 2532 mean train loss:  1.87605303e-02, mean val. rec. loss:  1.77648263e-02\n",
      "Epoch: 2533 mean train loss:  1.87547426e-02, mean val. rec. loss:  1.77585996e-02\n",
      "Epoch: 2534 mean train loss:  1.87489587e-02, mean val. rec. loss:  1.77523843e-02\n",
      "Epoch: 2535 mean train loss:  1.87431840e-02, mean val. rec. loss:  1.77461781e-02\n",
      "Epoch: 2536 mean train loss:  1.87374149e-02, mean val. rec. loss:  1.77399809e-02\n",
      "Epoch: 2537 mean train loss:  1.87316589e-02, mean val. rec. loss:  1.77337916e-02\n",
      "Epoch: 2538 mean train loss:  1.87259103e-02, mean val. rec. loss:  1.77276126e-02\n",
      "Epoch: 2539 mean train loss:  1.87201692e-02, mean val. rec. loss:  1.77214347e-02\n",
      "Epoch: 2540 mean train loss:  1.87144337e-02, mean val. rec. loss:  1.77152749e-02\n",
      "Epoch: 2541 mean train loss:  1.87087111e-02, mean val. rec. loss:  1.77091254e-02\n",
      "Epoch: 2542 mean train loss:  1.87029961e-02, mean val. rec. loss:  1.77029804e-02\n",
      "Epoch: 2543 mean train loss:  1.86972866e-02, mean val. rec. loss:  1.76968433e-02\n",
      "Epoch: 2544 mean train loss:  1.86915883e-02, mean val. rec. loss:  1.76907153e-02\n",
      "Epoch: 2545 mean train loss:  1.86859012e-02, mean val. rec. loss:  1.76845918e-02\n",
      "Epoch: 2546 mean train loss:  1.86802159e-02, mean val. rec. loss:  1.76784797e-02\n",
      "Epoch: 2547 mean train loss:  1.86745399e-02, mean val. rec. loss:  1.76723766e-02\n",
      "Epoch: 2548 mean train loss:  1.86688733e-02, mean val. rec. loss:  1.76662837e-02\n",
      "Epoch: 2549 mean train loss:  1.86632197e-02, mean val. rec. loss:  1.76601977e-02\n",
      "Epoch: 2550 mean train loss:  1.86575661e-02, mean val. rec. loss:  1.76541196e-02\n",
      "Epoch: 2551 mean train loss:  1.86519236e-02, mean val. rec. loss:  1.76480449e-02\n",
      "Epoch: 2552 mean train loss:  1.86462905e-02, mean val. rec. loss:  1.76419849e-02\n",
      "Epoch: 2553 mean train loss:  1.86406686e-02, mean val. rec. loss:  1.76359329e-02\n",
      "Epoch: 2554 mean train loss:  1.86350485e-02, mean val. rec. loss:  1.76298774e-02\n",
      "Epoch: 2555 mean train loss:  1.86294377e-02, mean val. rec. loss:  1.76238515e-02\n",
      "Epoch: 2556 mean train loss:  1.86238381e-02, mean val. rec. loss:  1.76178187e-02\n",
      "Epoch: 2557 mean train loss:  1.86182515e-02, mean val. rec. loss:  1.76117996e-02\n",
      "Epoch: 2558 mean train loss:  1.86126612e-02, mean val. rec. loss:  1.76057827e-02\n",
      "Epoch: 2559 mean train loss:  1.86070840e-02, mean val. rec. loss:  1.75997840e-02\n",
      "Epoch: 2560 mean train loss:  1.86015216e-02, mean val. rec. loss:  1.75937898e-02\n",
      "Epoch: 2561 mean train loss:  1.85959574e-02, mean val. rec. loss:  1.75878024e-02\n",
      "Epoch: 2562 mean train loss:  1.85904080e-02, mean val. rec. loss:  1.75818229e-02\n",
      "Epoch: 2563 mean train loss:  1.85848662e-02, mean val. rec. loss:  1.75758525e-02\n",
      "Epoch: 2564 mean train loss:  1.85793317e-02, mean val. rec. loss:  1.75698969e-02\n",
      "Epoch: 2565 mean train loss:  1.85738047e-02, mean val. rec. loss:  1.75639378e-02\n",
      "Epoch: 2566 mean train loss:  1.85682815e-02, mean val. rec. loss:  1.75579867e-02\n",
      "Epoch: 2567 mean train loss:  1.85627694e-02, mean val. rec. loss:  1.75520481e-02\n",
      "Epoch: 2568 mean train loss:  1.85572666e-02, mean val. rec. loss:  1.75461242e-02\n",
      "Epoch: 2569 mean train loss:  1.85517713e-02, mean val. rec. loss:  1.75402071e-02\n",
      "Epoch: 2570 mean train loss:  1.85462853e-02, mean val. rec. loss:  1.75342877e-02\n",
      "Epoch: 2571 mean train loss:  1.85408030e-02, mean val. rec. loss:  1.75283820e-02\n",
      "Epoch: 2572 mean train loss:  1.85353319e-02, mean val. rec. loss:  1.75224898e-02\n",
      "Epoch: 2573 mean train loss:  1.85298719e-02, mean val. rec. loss:  1.75166000e-02\n",
      "Epoch: 2574 mean train loss:  1.85244176e-02, mean val. rec. loss:  1.75107226e-02\n",
      "Epoch: 2575 mean train loss:  1.85189707e-02, mean val. rec. loss:  1.75048474e-02\n",
      "Epoch: 2576 mean train loss:  1.85135294e-02, mean val. rec. loss:  1.74989870e-02\n",
      "Epoch: 2577 mean train loss:  1.85081011e-02, mean val. rec. loss:  1.74931346e-02\n",
      "Epoch: 2578 mean train loss:  1.85026765e-02, mean val. rec. loss:  1.74872912e-02\n",
      "Epoch: 2579 mean train loss:  1.84972631e-02, mean val. rec. loss:  1.74814478e-02\n",
      "Epoch: 2580 mean train loss:  1.84918516e-02, mean val. rec. loss:  1.74756192e-02\n",
      "Epoch: 2581 mean train loss:  1.84864531e-02, mean val. rec. loss:  1.74697985e-02\n",
      "Epoch: 2582 mean train loss:  1.84810677e-02, mean val. rec. loss:  1.74639823e-02\n",
      "Epoch: 2583 mean train loss:  1.84756822e-02, mean val. rec. loss:  1.74581798e-02\n",
      "Epoch: 2584 mean train loss:  1.84703098e-02, mean val. rec. loss:  1.74523783e-02\n",
      "Epoch: 2585 mean train loss:  1.84649392e-02, mean val. rec. loss:  1.74465883e-02\n",
      "Epoch: 2586 mean train loss:  1.84595780e-02, mean val. rec. loss:  1.74408061e-02\n",
      "Epoch: 2587 mean train loss:  1.84542279e-02, mean val. rec. loss:  1.74350353e-02\n",
      "Epoch: 2588 mean train loss:  1.84488816e-02, mean val. rec. loss:  1.74292724e-02\n",
      "Epoch: 2589 mean train loss:  1.84435538e-02, mean val. rec. loss:  1.74235186e-02\n",
      "Epoch: 2590 mean train loss:  1.84382224e-02, mean val. rec. loss:  1.74177626e-02\n",
      "Epoch: 2591 mean train loss:  1.84328984e-02, mean val. rec. loss:  1.74120235e-02\n",
      "Epoch: 2592 mean train loss:  1.84275874e-02, mean val. rec. loss:  1.74062924e-02\n",
      "Epoch: 2593 mean train loss:  1.84222839e-02, mean val. rec. loss:  1.74005703e-02\n",
      "Epoch: 2594 mean train loss:  1.84169878e-02, mean val. rec. loss:  1.73948517e-02\n",
      "Epoch: 2595 mean train loss:  1.84116973e-02, mean val. rec. loss:  1.73891523e-02\n",
      "Epoch: 2596 mean train loss:  1.84064218e-02, mean val. rec. loss:  1.73834462e-02\n",
      "Epoch: 2597 mean train loss:  1.84011424e-02, mean val. rec. loss:  1.73777581e-02\n",
      "Epoch: 2598 mean train loss:  1.83958818e-02, mean val. rec. loss:  1.73720656e-02\n",
      "Epoch: 2599 mean train loss:  1.83906192e-02, mean val. rec. loss:  1.73663946e-02\n",
      "Epoch: 2600 mean train loss:  1.83853697e-02, mean val. rec. loss:  1.73607224e-02\n",
      "Epoch: 2601 mean train loss:  1.83801239e-02, mean val. rec. loss:  1.73550650e-02\n",
      "Epoch: 2602 mean train loss:  1.83748930e-02, mean val. rec. loss:  1.73494110e-02\n",
      "Epoch: 2603 mean train loss:  1.83696640e-02, mean val. rec. loss:  1.73437661e-02\n",
      "Epoch: 2604 mean train loss:  1.83644443e-02, mean val. rec. loss:  1.73381257e-02\n",
      "Epoch: 2605 mean train loss:  1.83592320e-02, mean val. rec. loss:  1.73325045e-02\n",
      "Epoch: 2606 mean train loss:  1.83540327e-02, mean val. rec. loss:  1.73268789e-02\n",
      "Epoch: 2607 mean train loss:  1.83488298e-02, mean val. rec. loss:  1.73212725e-02\n",
      "Epoch: 2608 mean train loss:  1.83436492e-02, mean val. rec. loss:  1.73156593e-02\n",
      "Epoch: 2609 mean train loss:  1.83384611e-02, mean val. rec. loss:  1.73100688e-02\n",
      "Epoch: 2610 mean train loss:  1.83332879e-02, mean val. rec. loss:  1.73044806e-02\n",
      "Epoch: 2611 mean train loss:  1.83281222e-02, mean val. rec. loss:  1.72989025e-02\n",
      "Epoch: 2612 mean train loss:  1.83229658e-02, mean val. rec. loss:  1.72933245e-02\n",
      "Epoch: 2613 mean train loss:  1.83178131e-02, mean val. rec. loss:  1.72877612e-02\n",
      "Epoch: 2614 mean train loss:  1.83126698e-02, mean val. rec. loss:  1.72822036e-02\n",
      "Epoch: 2615 mean train loss:  1.83075339e-02, mean val. rec. loss:  1.72766619e-02\n",
      "Epoch: 2616 mean train loss:  1.83024091e-02, mean val. rec. loss:  1.72711190e-02\n",
      "Epoch: 2617 mean train loss:  1.82972844e-02, mean val. rec. loss:  1.72655806e-02\n",
      "Epoch: 2618 mean train loss:  1.82921708e-02, mean val. rec. loss:  1.72600582e-02\n",
      "Epoch: 2619 mean train loss:  1.82870684e-02, mean val. rec. loss:  1.72545448e-02\n",
      "Epoch: 2620 mean train loss:  1.82819734e-02, mean val. rec. loss:  1.72490337e-02\n",
      "Epoch: 2621 mean train loss:  1.82768785e-02, mean val. rec. loss:  1.72435316e-02\n",
      "Epoch: 2622 mean train loss:  1.82717966e-02, mean val. rec. loss:  1.72380386e-02\n",
      "Epoch: 2623 mean train loss:  1.82667221e-02, mean val. rec. loss:  1.72325570e-02\n",
      "Epoch: 2624 mean train loss:  1.82616551e-02, mean val. rec. loss:  1.72270776e-02\n",
      "Epoch: 2625 mean train loss:  1.82565992e-02, mean val. rec. loss:  1.72216028e-02\n",
      "Epoch: 2626 mean train loss:  1.82515396e-02, mean val. rec. loss:  1.72161404e-02\n",
      "Epoch: 2627 mean train loss:  1.82464931e-02, mean val. rec. loss:  1.72106916e-02\n",
      "Epoch: 2628 mean train loss:  1.82414615e-02, mean val. rec. loss:  1.72052406e-02\n",
      "Epoch: 2629 mean train loss:  1.82364261e-02, mean val. rec. loss:  1.71998032e-02\n",
      "Epoch: 2630 mean train loss:  1.82314056e-02, mean val. rec. loss:  1.71943714e-02\n",
      "Epoch: 2631 mean train loss:  1.82263926e-02, mean val. rec. loss:  1.71889510e-02\n",
      "Epoch: 2632 mean train loss:  1.82213815e-02, mean val. rec. loss:  1.71835284e-02\n",
      "Epoch: 2633 mean train loss:  1.82163796e-02, mean val. rec. loss:  1.71781284e-02\n",
      "Epoch: 2634 mean train loss:  1.82113890e-02, mean val. rec. loss:  1.71727238e-02\n",
      "Epoch: 2635 mean train loss:  1.82064020e-02, mean val. rec. loss:  1.71673329e-02\n",
      "Epoch: 2636 mean train loss:  1.82014262e-02, mean val. rec. loss:  1.71619488e-02\n",
      "Epoch: 2637 mean train loss:  1.81964505e-02, mean val. rec. loss:  1.71565692e-02\n",
      "Epoch: 2638 mean train loss:  1.81914877e-02, mean val. rec. loss:  1.71512032e-02\n",
      "Epoch: 2639 mean train loss:  1.81865306e-02, mean val. rec. loss:  1.71458452e-02\n",
      "Epoch: 2640 mean train loss:  1.81815790e-02, mean val. rec. loss:  1.71404894e-02\n",
      "Epoch: 2641 mean train loss:  1.81766368e-02, mean val. rec. loss:  1.71351461e-02\n",
      "Epoch: 2642 mean train loss:  1.81717038e-02, mean val. rec. loss:  1.71298119e-02\n",
      "Epoch: 2643 mean train loss:  1.81667783e-02, mean val. rec. loss:  1.71244799e-02\n",
      "Epoch: 2644 mean train loss:  1.81618584e-02, mean val. rec. loss:  1.71191570e-02\n",
      "Epoch: 2645 mean train loss:  1.81569460e-02, mean val. rec. loss:  1.71138432e-02\n",
      "Epoch: 2646 mean train loss:  1.81520409e-02, mean val. rec. loss:  1.71085362e-02\n",
      "Epoch: 2647 mean train loss:  1.81471397e-02, mean val. rec. loss:  1.71032315e-02\n",
      "Epoch: 2648 mean train loss:  1.81422458e-02, mean val. rec. loss:  1.70979403e-02\n",
      "Epoch: 2649 mean train loss:  1.81373669e-02, mean val. rec. loss:  1.70926515e-02\n",
      "Epoch: 2650 mean train loss:  1.81324805e-02, mean val. rec. loss:  1.70873808e-02\n",
      "Epoch: 2651 mean train loss:  1.81276202e-02, mean val. rec. loss:  1.70821123e-02\n",
      "Epoch: 2652 mean train loss:  1.81227561e-02, mean val. rec. loss:  1.70768461e-02\n",
      "Epoch: 2653 mean train loss:  1.81178958e-02, mean val. rec. loss:  1.70715935e-02\n",
      "Epoch: 2654 mean train loss:  1.81130523e-02, mean val. rec. loss:  1.70663455e-02\n",
      "Epoch: 2655 mean train loss:  1.81082124e-02, mean val. rec. loss:  1.70611111e-02\n",
      "Epoch: 2656 mean train loss:  1.81033819e-02, mean val. rec. loss:  1.70558812e-02\n",
      "Epoch: 2657 mean train loss:  1.80985551e-02, mean val. rec. loss:  1.70506535e-02\n",
      "Epoch: 2658 mean train loss:  1.80937320e-02, mean val. rec. loss:  1.70454418e-02\n",
      "Epoch: 2659 mean train loss:  1.80889239e-02, mean val. rec. loss:  1.70402391e-02\n",
      "Epoch: 2660 mean train loss:  1.80841231e-02, mean val. rec. loss:  1.70350330e-02\n",
      "Epoch: 2661 mean train loss:  1.80793187e-02, mean val. rec. loss:  1.70298450e-02\n",
      "Epoch: 2662 mean train loss:  1.80745328e-02, mean val. rec. loss:  1.70246582e-02\n",
      "Epoch: 2663 mean train loss:  1.80697470e-02, mean val. rec. loss:  1.70194850e-02\n",
      "Epoch: 2664 mean train loss:  1.80649724e-02, mean val. rec. loss:  1.70143164e-02\n",
      "Epoch: 2665 mean train loss:  1.80602052e-02, mean val. rec. loss:  1.70091534e-02\n",
      "Epoch: 2666 mean train loss:  1.80554417e-02, mean val. rec. loss:  1.70040006e-02\n",
      "Epoch: 2667 mean train loss:  1.80506894e-02, mean val. rec. loss:  1.69988535e-02\n",
      "Epoch: 2668 mean train loss:  1.80459408e-02, mean val. rec. loss:  1.69937086e-02\n",
      "Epoch: 2669 mean train loss:  1.80411959e-02, mean val. rec. loss:  1.69885728e-02\n",
      "Epoch: 2670 mean train loss:  1.80364585e-02, mean val. rec. loss:  1.69834473e-02\n",
      "Epoch: 2671 mean train loss:  1.80317360e-02, mean val. rec. loss:  1.69783342e-02\n",
      "Epoch: 2672 mean train loss:  1.80270172e-02, mean val. rec. loss:  1.69732279e-02\n",
      "Epoch: 2673 mean train loss:  1.80223021e-02, mean val. rec. loss:  1.69681204e-02\n",
      "Epoch: 2674 mean train loss:  1.80175945e-02, mean val. rec. loss:  1.69630300e-02\n",
      "Epoch: 2675 mean train loss:  1.80129018e-02, mean val. rec. loss:  1.69579441e-02\n",
      "Epoch: 2676 mean train loss:  1.80082072e-02, mean val. rec. loss:  1.69528560e-02\n",
      "Epoch: 2677 mean train loss:  1.80035182e-02, mean val. rec. loss:  1.69477814e-02\n",
      "Epoch: 2678 mean train loss:  1.79988423e-02, mean val. rec. loss:  1.69427160e-02\n",
      "Epoch: 2679 mean train loss:  1.79941682e-02, mean val. rec. loss:  1.69376618e-02\n",
      "Epoch: 2680 mean train loss:  1.79895034e-02, mean val. rec. loss:  1.69326066e-02\n",
      "Epoch: 2681 mean train loss:  1.79848498e-02, mean val. rec. loss:  1.69275649e-02\n",
      "Epoch: 2682 mean train loss:  1.79802036e-02, mean val. rec. loss:  1.69225300e-02\n",
      "Epoch: 2683 mean train loss:  1.79755556e-02, mean val. rec. loss:  1.69175009e-02\n",
      "Epoch: 2684 mean train loss:  1.79709206e-02, mean val. rec. loss:  1.69124796e-02\n",
      "Epoch: 2685 mean train loss:  1.79662912e-02, mean val. rec. loss:  1.69074674e-02\n",
      "Epoch: 2686 mean train loss:  1.79616673e-02, mean val. rec. loss:  1.69024575e-02\n",
      "Epoch: 2687 mean train loss:  1.79570472e-02, mean val. rec. loss:  1.68974612e-02\n",
      "Epoch: 2688 mean train loss:  1.79524421e-02, mean val. rec. loss:  1.68924683e-02\n",
      "Epoch: 2689 mean train loss:  1.79478406e-02, mean val. rec. loss:  1.68874834e-02\n",
      "Epoch: 2690 mean train loss:  1.79432447e-02, mean val. rec. loss:  1.68825097e-02\n",
      "Epoch: 2691 mean train loss:  1.79386563e-02, mean val. rec. loss:  1.68775441e-02\n",
      "Epoch: 2692 mean train loss:  1.79340753e-02, mean val. rec. loss:  1.68725806e-02\n",
      "Epoch: 2693 mean train loss:  1.79294980e-02, mean val. rec. loss:  1.68676184e-02\n",
      "Epoch: 2694 mean train loss:  1.79249263e-02, mean val. rec. loss:  1.68626788e-02\n",
      "Epoch: 2695 mean train loss:  1.79203695e-02, mean val. rec. loss:  1.68577392e-02\n",
      "Epoch: 2696 mean train loss:  1.79158146e-02, mean val. rec. loss:  1.68527996e-02\n",
      "Epoch: 2697 mean train loss:  1.79112672e-02, mean val. rec. loss:  1.68478736e-02\n",
      "Epoch: 2698 mean train loss:  1.79067271e-02, mean val. rec. loss:  1.68429623e-02\n",
      "Epoch: 2699 mean train loss:  1.79021908e-02, mean val. rec. loss:  1.68380476e-02\n",
      "Epoch: 2700 mean train loss:  1.78976620e-02, mean val. rec. loss:  1.68331455e-02\n",
      "Epoch: 2701 mean train loss:  1.78931443e-02, mean val. rec. loss:  1.68282467e-02\n",
      "Epoch: 2702 mean train loss:  1.78886341e-02, mean val. rec. loss:  1.68233593e-02\n",
      "Epoch: 2703 mean train loss:  1.78841239e-02, mean val. rec. loss:  1.68184809e-02\n",
      "Epoch: 2704 mean train loss:  1.78796248e-02, mean val. rec. loss:  1.68136025e-02\n",
      "Epoch: 2705 mean train loss:  1.78751295e-02, mean val. rec. loss:  1.68087321e-02\n",
      "Epoch: 2706 mean train loss:  1.78706416e-02, mean val. rec. loss:  1.68038707e-02\n",
      "Epoch: 2707 mean train loss:  1.78661575e-02, mean val. rec. loss:  1.67990196e-02\n",
      "Epoch: 2708 mean train loss:  1.78616882e-02, mean val. rec. loss:  1.67941730e-02\n",
      "Epoch: 2709 mean train loss:  1.78572189e-02, mean val. rec. loss:  1.67893354e-02\n",
      "Epoch: 2710 mean train loss:  1.78527590e-02, mean val. rec. loss:  1.67845047e-02\n",
      "Epoch: 2711 mean train loss:  1.78483046e-02, mean val. rec. loss:  1.67796830e-02\n",
      "Epoch: 2712 mean train loss:  1.78438596e-02, mean val. rec. loss:  1.67748602e-02\n",
      "Epoch: 2713 mean train loss:  1.78394164e-02, mean val. rec. loss:  1.67700487e-02\n",
      "Epoch: 2714 mean train loss:  1.78349807e-02, mean val. rec. loss:  1.67652532e-02\n",
      "Epoch: 2715 mean train loss:  1.78305561e-02, mean val. rec. loss:  1.67604587e-02\n",
      "Epoch: 2716 mean train loss:  1.78261353e-02, mean val. rec. loss:  1.67556688e-02\n",
      "Epoch: 2717 mean train loss:  1.78217219e-02, mean val. rec. loss:  1.67508868e-02\n",
      "Epoch: 2718 mean train loss:  1.78173141e-02, mean val. rec. loss:  1.67461128e-02\n",
      "Epoch: 2719 mean train loss:  1.78129100e-02, mean val. rec. loss:  1.67413501e-02\n",
      "Epoch: 2720 mean train loss:  1.78085189e-02, mean val. rec. loss:  1.67365908e-02\n",
      "Epoch: 2721 mean train loss:  1.78041316e-02, mean val. rec. loss:  1.67318315e-02\n",
      "Epoch: 2722 mean train loss:  1.77997462e-02, mean val. rec. loss:  1.67270926e-02\n",
      "Epoch: 2723 mean train loss:  1.77953756e-02, mean val. rec. loss:  1.67223514e-02\n",
      "Epoch: 2724 mean train loss:  1.77909994e-02, mean val. rec. loss:  1.67176193e-02\n",
      "Epoch: 2725 mean train loss:  1.77866419e-02, mean val. rec. loss:  1.67128963e-02\n",
      "Epoch: 2726 mean train loss:  1.77822900e-02, mean val. rec. loss:  1.67081756e-02\n",
      "Epoch: 2727 mean train loss:  1.77779380e-02, mean val. rec. loss:  1.67034696e-02\n",
      "Epoch: 2728 mean train loss:  1.77735917e-02, mean val. rec. loss:  1.66987636e-02\n",
      "Epoch: 2729 mean train loss:  1.77692565e-02, mean val. rec. loss:  1.66940655e-02\n",
      "Epoch: 2730 mean train loss:  1.77649251e-02, mean val. rec. loss:  1.66893822e-02\n",
      "Epoch: 2731 mean train loss:  1.77606048e-02, mean val. rec. loss:  1.66847045e-02\n",
      "Epoch: 2732 mean train loss:  1.77562882e-02, mean val. rec. loss:  1.66800223e-02\n",
      "Epoch: 2733 mean train loss:  1.77519791e-02, mean val. rec. loss:  1.66753504e-02\n",
      "Epoch: 2734 mean train loss:  1.77476737e-02, mean val. rec. loss:  1.66706942e-02\n",
      "Epoch: 2735 mean train loss:  1.77433758e-02, mean val. rec. loss:  1.66660415e-02\n",
      "Epoch: 2736 mean train loss:  1.77390853e-02, mean val. rec. loss:  1.66613888e-02\n",
      "Epoch: 2737 mean train loss:  1.77347985e-02, mean val. rec. loss:  1.66567509e-02\n",
      "Epoch: 2738 mean train loss:  1.77305192e-02, mean val. rec. loss:  1.66521197e-02\n",
      "Epoch: 2739 mean train loss:  1.77262511e-02, mean val. rec. loss:  1.66474954e-02\n",
      "Epoch: 2740 mean train loss:  1.77219829e-02, mean val. rec. loss:  1.66428710e-02\n",
      "Epoch: 2741 mean train loss:  1.77177222e-02, mean val. rec. loss:  1.66382546e-02\n",
      "Epoch: 2742 mean train loss:  1.77134671e-02, mean val. rec. loss:  1.66336506e-02\n",
      "Epoch: 2743 mean train loss:  1.77092213e-02, mean val. rec. loss:  1.66290546e-02\n",
      "Epoch: 2744 mean train loss:  1.77049811e-02, mean val. rec. loss:  1.66244598e-02\n",
      "Epoch: 2745 mean train loss:  1.77007465e-02, mean val. rec. loss:  1.66198762e-02\n",
      "Epoch: 2746 mean train loss:  1.76965193e-02, mean val. rec. loss:  1.66153006e-02\n",
      "Epoch: 2747 mean train loss:  1.76922996e-02, mean val. rec. loss:  1.66107330e-02\n",
      "Epoch: 2748 mean train loss:  1.76880836e-02, mean val. rec. loss:  1.66061642e-02\n",
      "Epoch: 2749 mean train loss:  1.76838751e-02, mean val. rec. loss:  1.66016079e-02\n",
      "Epoch: 2750 mean train loss:  1.76796702e-02, mean val. rec. loss:  1.65970629e-02\n",
      "Epoch: 2751 mean train loss:  1.76754747e-02, mean val. rec. loss:  1.65925179e-02\n",
      "Epoch: 2752 mean train loss:  1.76712829e-02, mean val. rec. loss:  1.65879797e-02\n",
      "Epoch: 2753 mean train loss:  1.76671042e-02, mean val. rec. loss:  1.65834518e-02\n",
      "Epoch: 2754 mean train loss:  1.76629236e-02, mean val. rec. loss:  1.65789238e-02\n",
      "Epoch: 2755 mean train loss:  1.76587485e-02, mean val. rec. loss:  1.65744060e-02\n",
      "Epoch: 2756 mean train loss:  1.76545847e-02, mean val. rec. loss:  1.65698996e-02\n",
      "Epoch: 2757 mean train loss:  1.76504264e-02, mean val. rec. loss:  1.65653955e-02\n",
      "Epoch: 2758 mean train loss:  1.76462700e-02, mean val. rec. loss:  1.65608981e-02\n",
      "Epoch: 2759 mean train loss:  1.76421248e-02, mean val. rec. loss:  1.65564121e-02\n",
      "Epoch: 2760 mean train loss:  1.76379870e-02, mean val. rec. loss:  1.65519329e-02\n",
      "Epoch: 2761 mean train loss:  1.76338566e-02, mean val. rec. loss:  1.65474559e-02\n",
      "Epoch: 2762 mean train loss:  1.76297263e-02, mean val. rec. loss:  1.65429858e-02\n",
      "Epoch: 2763 mean train loss:  1.76256034e-02, mean val. rec. loss:  1.65385202e-02\n",
      "Epoch: 2764 mean train loss:  1.76214880e-02, mean val. rec. loss:  1.65340659e-02\n",
      "Epoch: 2765 mean train loss:  1.76173763e-02, mean val. rec. loss:  1.65296174e-02\n",
      "Epoch: 2766 mean train loss:  1.76132738e-02, mean val. rec. loss:  1.65251778e-02\n",
      "Epoch: 2767 mean train loss:  1.76091752e-02, mean val. rec. loss:  1.65207451e-02\n",
      "Epoch: 2768 mean train loss:  1.76050839e-02, mean val. rec. loss:  1.65163147e-02\n",
      "Epoch: 2769 mean train loss:  1.76009946e-02, mean val. rec. loss:  1.65118888e-02\n",
      "Epoch: 2770 mean train loss:  1.75969145e-02, mean val. rec. loss:  1.65074708e-02\n",
      "Epoch: 2771 mean train loss:  1.75928419e-02, mean val. rec. loss:  1.65030653e-02\n",
      "Epoch: 2772 mean train loss:  1.75887730e-02, mean val. rec. loss:  1.64986677e-02\n",
      "Epoch: 2773 mean train loss:  1.75847134e-02, mean val. rec. loss:  1.64942758e-02\n",
      "Epoch: 2774 mean train loss:  1.75806594e-02, mean val. rec. loss:  1.64898828e-02\n",
      "Epoch: 2775 mean train loss:  1.75766129e-02, mean val. rec. loss:  1.64855023e-02\n",
      "Epoch: 2776 mean train loss:  1.75725682e-02, mean val. rec. loss:  1.64811296e-02\n",
      "Epoch: 2777 mean train loss:  1.75685254e-02, mean val. rec. loss:  1.64767593e-02\n",
      "Epoch: 2778 mean train loss:  1.75644994e-02, mean val. rec. loss:  1.64723912e-02\n",
      "Epoch: 2779 mean train loss:  1.75604696e-02, mean val. rec. loss:  1.64680413e-02\n",
      "Epoch: 2780 mean train loss:  1.75564566e-02, mean val. rec. loss:  1.64636914e-02\n",
      "Epoch: 2781 mean train loss:  1.75524398e-02, mean val. rec. loss:  1.64593471e-02\n",
      "Epoch: 2782 mean train loss:  1.75484305e-02, mean val. rec. loss:  1.64550153e-02\n",
      "Epoch: 2783 mean train loss:  1.75444324e-02, mean val. rec. loss:  1.64506892e-02\n",
      "Epoch: 2784 mean train loss:  1.75404380e-02, mean val. rec. loss:  1.64463687e-02\n",
      "Epoch: 2785 mean train loss:  1.75364492e-02, mean val. rec. loss:  1.64420528e-02\n",
      "Epoch: 2786 mean train loss:  1.75324641e-02, mean val. rec. loss:  1.64377414e-02\n",
      "Epoch: 2787 mean train loss:  1.75284864e-02, mean val. rec. loss:  1.64334414e-02\n",
      "Epoch: 2788 mean train loss:  1.75245163e-02, mean val. rec. loss:  1.64291482e-02\n",
      "Epoch: 2789 mean train loss:  1.75205498e-02, mean val. rec. loss:  1.64248561e-02\n",
      "Epoch: 2790 mean train loss:  1.75165871e-02, mean val. rec. loss:  1.64205685e-02\n",
      "Epoch: 2791 mean train loss:  1.75126336e-02, mean val. rec. loss:  1.64163036e-02\n",
      "Epoch: 2792 mean train loss:  1.75086895e-02, mean val. rec. loss:  1.64120319e-02\n",
      "Epoch: 2793 mean train loss:  1.75047398e-02, mean val. rec. loss:  1.64077704e-02\n",
      "Epoch: 2794 mean train loss:  1.75008106e-02, mean val. rec. loss:  1.64035090e-02\n",
      "Epoch: 2795 mean train loss:  1.74968739e-02, mean val. rec. loss:  1.63992633e-02\n",
      "Epoch: 2796 mean train loss:  1.74929540e-02, mean val. rec. loss:  1.63950223e-02\n",
      "Epoch: 2797 mean train loss:  1.74890341e-02, mean val. rec. loss:  1.63907880e-02\n",
      "Epoch: 2798 mean train loss:  1.74851198e-02, mean val. rec. loss:  1.63865481e-02\n",
      "Epoch: 2799 mean train loss:  1.74812110e-02, mean val. rec. loss:  1.63823263e-02\n",
      "Epoch: 2800 mean train loss:  1.74773097e-02, mean val. rec. loss:  1.63781101e-02\n",
      "Epoch: 2801 mean train loss:  1.74734140e-02, mean val. rec. loss:  1.63738963e-02\n",
      "Epoch: 2802 mean train loss:  1.74695258e-02, mean val. rec. loss:  1.63696892e-02\n",
      "Epoch: 2803 mean train loss:  1.74656412e-02, mean val. rec. loss:  1.63654981e-02\n",
      "Epoch: 2804 mean train loss:  1.74617642e-02, mean val. rec. loss:  1.63613091e-02\n",
      "Epoch: 2805 mean train loss:  1.74578945e-02, mean val. rec. loss:  1.63571157e-02\n",
      "Epoch: 2806 mean train loss:  1.74540249e-02, mean val. rec. loss:  1.63529336e-02\n",
      "Epoch: 2807 mean train loss:  1.74501627e-02, mean val. rec. loss:  1.63487674e-02\n",
      "Epoch: 2808 mean train loss:  1.74463117e-02, mean val. rec. loss:  1.63446011e-02\n",
      "Epoch: 2809 mean train loss:  1.74424551e-02, mean val. rec. loss:  1.63404417e-02\n",
      "Epoch: 2810 mean train loss:  1.74386171e-02, mean val. rec. loss:  1.63362846e-02\n",
      "Epoch: 2811 mean train loss:  1.74347736e-02, mean val. rec. loss:  1.63321410e-02\n",
      "Epoch: 2812 mean train loss:  1.74309430e-02, mean val. rec. loss:  1.63280020e-02\n",
      "Epoch: 2813 mean train loss:  1.74271181e-02, mean val. rec. loss:  1.63238630e-02\n",
      "Epoch: 2814 mean train loss:  1.74232932e-02, mean val. rec. loss:  1.63197342e-02\n",
      "Epoch: 2815 mean train loss:  1.74194775e-02, mean val. rec. loss:  1.63156167e-02\n",
      "Epoch: 2816 mean train loss:  1.74156675e-02, mean val. rec. loss:  1.63115049e-02\n",
      "Epoch: 2817 mean train loss:  1.74118612e-02, mean val. rec. loss:  1.63073920e-02\n",
      "Epoch: 2818 mean train loss:  1.74080623e-02, mean val. rec. loss:  1.63032904e-02\n",
      "Epoch: 2819 mean train loss:  1.74042709e-02, mean val. rec. loss:  1.62991933e-02\n",
      "Epoch: 2820 mean train loss:  1.74004869e-02, mean val. rec. loss:  1.62951076e-02\n",
      "Epoch: 2821 mean train loss:  1.73967067e-02, mean val. rec. loss:  1.62910219e-02\n",
      "Epoch: 2822 mean train loss:  1.73929264e-02, mean val. rec. loss:  1.62869407e-02\n",
      "Epoch: 2823 mean train loss:  1.73891536e-02, mean val. rec. loss:  1.62828743e-02\n",
      "Epoch: 2824 mean train loss:  1.73853957e-02, mean val. rec. loss:  1.62788101e-02\n",
      "Epoch: 2825 mean train loss:  1.73816304e-02, mean val. rec. loss:  1.62747493e-02\n",
      "Epoch: 2826 mean train loss:  1.73778799e-02, mean val. rec. loss:  1.62706942e-02\n",
      "Epoch: 2827 mean train loss:  1.73741258e-02, mean val. rec. loss:  1.62666527e-02\n",
      "Epoch: 2828 mean train loss:  1.73703865e-02, mean val. rec. loss:  1.62626124e-02\n",
      "Epoch: 2829 mean train loss:  1.73666509e-02, mean val. rec. loss:  1.62585777e-02\n",
      "Epoch: 2830 mean train loss:  1.73629154e-02, mean val. rec. loss:  1.62545430e-02\n",
      "Epoch: 2831 mean train loss:  1.73591854e-02, mean val. rec. loss:  1.62505242e-02\n",
      "Epoch: 2832 mean train loss:  1.73554647e-02, mean val. rec. loss:  1.62465099e-02\n",
      "Epoch: 2833 mean train loss:  1.73517497e-02, mean val. rec. loss:  1.62424990e-02\n",
      "Epoch: 2834 mean train loss:  1.73480365e-02, mean val. rec. loss:  1.62384949e-02\n",
      "Epoch: 2835 mean train loss:  1.73443344e-02, mean val. rec. loss:  1.62345022e-02\n",
      "Epoch: 2836 mean train loss:  1.73406361e-02, mean val. rec. loss:  1.62305118e-02\n",
      "Epoch: 2837 mean train loss:  1.73369453e-02, mean val. rec. loss:  1.62265247e-02\n",
      "Epoch: 2838 mean train loss:  1.73332544e-02, mean val. rec. loss:  1.62225444e-02\n",
      "Epoch: 2839 mean train loss:  1.73295710e-02, mean val. rec. loss:  1.62185755e-02\n",
      "Epoch: 2840 mean train loss:  1.73258987e-02, mean val. rec. loss:  1.62146077e-02\n",
      "Epoch: 2841 mean train loss:  1.73222209e-02, mean val. rec. loss:  1.62106513e-02\n",
      "Epoch: 2842 mean train loss:  1.73185599e-02, mean val. rec. loss:  1.62066914e-02\n",
      "Epoch: 2843 mean train loss:  1.73148932e-02, mean val. rec. loss:  1.62027475e-02\n",
      "Epoch: 2844 mean train loss:  1.73112414e-02, mean val. rec. loss:  1.61988103e-02\n",
      "Epoch: 2845 mean train loss:  1.73075934e-02, mean val. rec. loss:  1.61948697e-02\n",
      "Epoch: 2846 mean train loss:  1.73039472e-02, mean val. rec. loss:  1.61909394e-02\n",
      "Epoch: 2847 mean train loss:  1.73003048e-02, mean val. rec. loss:  1.61870192e-02\n",
      "Epoch: 2848 mean train loss:  1.72966717e-02, mean val. rec. loss:  1.61831025e-02\n",
      "Epoch: 2849 mean train loss:  1.72930423e-02, mean val. rec. loss:  1.61791902e-02\n",
      "Epoch: 2850 mean train loss:  1.72894222e-02, mean val. rec. loss:  1.61752814e-02\n",
      "Epoch: 2851 mean train loss:  1.72857983e-02, mean val. rec. loss:  1.61713851e-02\n",
      "Epoch: 2852 mean train loss:  1.72821913e-02, mean val. rec. loss:  1.61674955e-02\n",
      "Epoch: 2853 mean train loss:  1.72785861e-02, mean val. rec. loss:  1.61636026e-02\n",
      "Epoch: 2854 mean train loss:  1.72749809e-02, mean val. rec. loss:  1.61597199e-02\n",
      "Epoch: 2855 mean train loss:  1.72713813e-02, mean val. rec. loss:  1.61558485e-02\n",
      "Epoch: 2856 mean train loss:  1.72677947e-02, mean val. rec. loss:  1.61519782e-02\n",
      "Epoch: 2857 mean train loss:  1.72642025e-02, mean val. rec. loss:  1.61481147e-02\n",
      "Epoch: 2858 mean train loss:  1.72606252e-02, mean val. rec. loss:  1.61442513e-02\n",
      "Epoch: 2859 mean train loss:  1.72570461e-02, mean val. rec. loss:  1.61404037e-02\n",
      "Epoch: 2860 mean train loss:  1.72534782e-02, mean val. rec. loss:  1.61365618e-02\n",
      "Epoch: 2861 mean train loss:  1.72499139e-02, mean val. rec. loss:  1.61327221e-02\n",
      "Epoch: 2862 mean train loss:  1.72463516e-02, mean val. rec. loss:  1.61288780e-02\n",
      "Epoch: 2863 mean train loss:  1.72427948e-02, mean val. rec. loss:  1.61250553e-02\n",
      "Epoch: 2864 mean train loss:  1.72392473e-02, mean val. rec. loss:  1.61212372e-02\n",
      "Epoch: 2865 mean train loss:  1.72357035e-02, mean val. rec. loss:  1.61174203e-02\n",
      "Epoch: 2866 mean train loss:  1.72321635e-02, mean val. rec. loss:  1.61136101e-02\n",
      "Epoch: 2867 mean train loss:  1.72286291e-02, mean val. rec. loss:  1.61098056e-02\n",
      "Epoch: 2868 mean train loss:  1.72251002e-02, mean val. rec. loss:  1.61060057e-02\n",
      "Epoch: 2869 mean train loss:  1.72215751e-02, mean val. rec. loss:  1.61022159e-02\n",
      "Epoch: 2870 mean train loss:  1.72180593e-02, mean val. rec. loss:  1.60984284e-02\n",
      "Epoch: 2871 mean train loss:  1.72145491e-02, mean val. rec. loss:  1.60946489e-02\n",
      "Epoch: 2872 mean train loss:  1.72110388e-02, mean val. rec. loss:  1.60908739e-02\n",
      "Epoch: 2873 mean train loss:  1.72075342e-02, mean val. rec. loss:  1.60871057e-02\n",
      "Epoch: 2874 mean train loss:  1.72040370e-02, mean val. rec. loss:  1.60833431e-02\n",
      "Epoch: 2875 mean train loss:  1.72005435e-02, mean val. rec. loss:  1.60795863e-02\n",
      "Epoch: 2876 mean train loss:  1.71970575e-02, mean val. rec. loss:  1.60758373e-02\n",
      "Epoch: 2877 mean train loss:  1.71935752e-02, mean val. rec. loss:  1.60720884e-02\n",
      "Epoch: 2878 mean train loss:  1.71900966e-02, mean val. rec. loss:  1.60683486e-02\n",
      "Epoch: 2879 mean train loss:  1.71866255e-02, mean val. rec. loss:  1.60646178e-02\n",
      "Epoch: 2880 mean train loss:  1.71831600e-02, mean val. rec. loss:  1.60608961e-02\n",
      "Epoch: 2881 mean train loss:  1.71796944e-02, mean val. rec. loss:  1.60571630e-02\n",
      "Epoch: 2882 mean train loss:  1.71762382e-02, mean val. rec. loss:  1.60534526e-02\n",
      "Epoch: 2883 mean train loss:  1.71727857e-02, mean val. rec. loss:  1.60497355e-02\n",
      "Epoch: 2884 mean train loss:  1.71693369e-02, mean val. rec. loss:  1.60460387e-02\n",
      "Epoch: 2885 mean train loss:  1.71658956e-02, mean val. rec. loss:  1.60423363e-02\n",
      "Epoch: 2886 mean train loss:  1.71624599e-02, mean val. rec. loss:  1.60386406e-02\n",
      "Epoch: 2887 mean train loss:  1.71590241e-02, mean val. rec. loss:  1.60349530e-02\n",
      "Epoch: 2888 mean train loss:  1.71555958e-02, mean val. rec. loss:  1.60312698e-02\n",
      "Epoch: 2889 mean train loss:  1.71521769e-02, mean val. rec. loss:  1.60275980e-02\n",
      "Epoch: 2890 mean train loss:  1.71487616e-02, mean val. rec. loss:  1.60239228e-02\n",
      "Epoch: 2891 mean train loss:  1.71453426e-02, mean val. rec. loss:  1.60202634e-02\n",
      "Epoch: 2892 mean train loss:  1.71419441e-02, mean val. rec. loss:  1.60166052e-02\n",
      "Epoch: 2893 mean train loss:  1.71385363e-02, mean val. rec. loss:  1.60129561e-02\n",
      "Epoch: 2894 mean train loss:  1.71351434e-02, mean val. rec. loss:  1.60093092e-02\n",
      "Epoch: 2895 mean train loss:  1.71317486e-02, mean val. rec. loss:  1.60056646e-02\n",
      "Epoch: 2896 mean train loss:  1.71283594e-02, mean val. rec. loss:  1.60020325e-02\n",
      "Epoch: 2897 mean train loss:  1.71249814e-02, mean val. rec. loss:  1.59984026e-02\n",
      "Epoch: 2898 mean train loss:  1.71216053e-02, mean val. rec. loss:  1.59947739e-02\n",
      "Epoch: 2899 mean train loss:  1.71182291e-02, mean val. rec. loss:  1.59911520e-02\n",
      "Epoch: 2900 mean train loss:  1.71148623e-02, mean val. rec. loss:  1.59875437e-02\n",
      "Epoch: 2901 mean train loss:  1.71115010e-02, mean val. rec. loss:  1.59839353e-02\n",
      "Epoch: 2902 mean train loss:  1.71081416e-02, mean val. rec. loss:  1.59803316e-02\n",
      "Epoch: 2903 mean train loss:  1.71047878e-02, mean val. rec. loss:  1.59767414e-02\n",
      "Epoch: 2904 mean train loss:  1.71014415e-02, mean val. rec. loss:  1.59731524e-02\n",
      "Epoch: 2905 mean train loss:  1.70981007e-02, mean val. rec. loss:  1.59695667e-02\n",
      "Epoch: 2906 mean train loss:  1.70947599e-02, mean val. rec. loss:  1.59659834e-02\n",
      "Epoch: 2907 mean train loss:  1.70914229e-02, mean val. rec. loss:  1.59624136e-02\n",
      "Epoch: 2908 mean train loss:  1.70880970e-02, mean val. rec. loss:  1.59588450e-02\n",
      "Epoch: 2909 mean train loss:  1.70847749e-02, mean val. rec. loss:  1.59552798e-02\n",
      "Epoch: 2910 mean train loss:  1.70814564e-02, mean val. rec. loss:  1.59517259e-02\n",
      "Epoch: 2911 mean train loss:  1.70781399e-02, mean val. rec. loss:  1.59481788e-02\n",
      "Epoch: 2912 mean train loss:  1.70748326e-02, mean val. rec. loss:  1.59446306e-02\n",
      "Epoch: 2913 mean train loss:  1.70715291e-02, mean val. rec. loss:  1.59410858e-02\n",
      "Epoch: 2914 mean train loss:  1.70682312e-02, mean val. rec. loss:  1.59375523e-02\n",
      "Epoch: 2915 mean train loss:  1.70649314e-02, mean val. rec. loss:  1.59340154e-02\n",
      "Epoch: 2916 mean train loss:  1.70616390e-02, mean val. rec. loss:  1.59305001e-02\n",
      "Epoch: 2917 mean train loss:  1.70583578e-02, mean val. rec. loss:  1.59269780e-02\n",
      "Epoch: 2918 mean train loss:  1.70550767e-02, mean val. rec. loss:  1.59234649e-02\n",
      "Epoch: 2919 mean train loss:  1.70517992e-02, mean val. rec. loss:  1.59199609e-02\n",
      "Epoch: 2920 mean train loss:  1.70485292e-02, mean val. rec. loss:  1.59164581e-02\n",
      "Epoch: 2921 mean train loss:  1.70452629e-02, mean val. rec. loss:  1.59129609e-02\n",
      "Epoch: 2922 mean train loss:  1.70420004e-02, mean val. rec. loss:  1.59094648e-02\n",
      "Epoch: 2923 mean train loss:  1.70387415e-02, mean val. rec. loss:  1.59059790e-02\n",
      "Epoch: 2924 mean train loss:  1.70354920e-02, mean val. rec. loss:  1.59025068e-02\n",
      "Epoch: 2925 mean train loss:  1.70322481e-02, mean val. rec. loss:  1.58990300e-02\n",
      "Epoch: 2926 mean train loss:  1.70290023e-02, mean val. rec. loss:  1.58955577e-02\n",
      "Epoch: 2927 mean train loss:  1.70257602e-02, mean val. rec. loss:  1.58921014e-02\n",
      "Epoch: 2928 mean train loss:  1.70225330e-02, mean val. rec. loss:  1.58886371e-02\n",
      "Epoch: 2929 mean train loss:  1.70193002e-02, mean val. rec. loss:  1.58851887e-02\n",
      "Epoch: 2930 mean train loss:  1.70160805e-02, mean val. rec. loss:  1.58817368e-02\n",
      "Epoch: 2931 mean train loss:  1.70128552e-02, mean val. rec. loss:  1.58782986e-02\n",
      "Epoch: 2932 mean train loss:  1.70096467e-02, mean val. rec. loss:  1.58748649e-02\n",
      "Epoch: 2933 mean train loss:  1.70064344e-02, mean val. rec. loss:  1.58714290e-02\n",
      "Epoch: 2934 mean train loss:  1.70032258e-02, mean val. rec. loss:  1.58680044e-02\n",
      "Epoch: 2935 mean train loss:  1.70000247e-02, mean val. rec. loss:  1.58645866e-02\n",
      "Epoch: 2936 mean train loss:  1.69968311e-02, mean val. rec. loss:  1.58611699e-02\n",
      "Epoch: 2937 mean train loss:  1.69936411e-02, mean val. rec. loss:  1.58577578e-02\n",
      "Epoch: 2938 mean train loss:  1.69904531e-02, mean val. rec. loss:  1.58543513e-02\n",
      "Epoch: 2939 mean train loss:  1.69872668e-02, mean val. rec. loss:  1.58509516e-02\n",
      "Epoch: 2940 mean train loss:  1.69840900e-02, mean val. rec. loss:  1.58475611e-02\n",
      "Epoch: 2941 mean train loss:  1.69809186e-02, mean val. rec. loss:  1.58441727e-02\n",
      "Epoch: 2942 mean train loss:  1.69777455e-02, mean val. rec. loss:  1.58407821e-02\n",
      "Epoch: 2943 mean train loss:  1.69745779e-02, mean val. rec. loss:  1.58374108e-02\n",
      "Epoch: 2944 mean train loss:  1.69714196e-02, mean val. rec. loss:  1.58340361e-02\n",
      "Epoch: 2945 mean train loss:  1.69682688e-02, mean val. rec. loss:  1.58306659e-02\n",
      "Epoch: 2946 mean train loss:  1.69651124e-02, mean val. rec. loss:  1.58272992e-02\n",
      "Epoch: 2947 mean train loss:  1.69619671e-02, mean val. rec. loss:  1.58239437e-02\n",
      "Epoch: 2948 mean train loss:  1.69588237e-02, mean val. rec. loss:  1.58205974e-02\n",
      "Epoch: 2949 mean train loss:  1.69556878e-02, mean val. rec. loss:  1.58172453e-02\n",
      "Epoch: 2950 mean train loss:  1.69525556e-02, mean val. rec. loss:  1.58139024e-02\n",
      "Epoch: 2951 mean train loss:  1.69494253e-02, mean val. rec. loss:  1.58105730e-02\n",
      "Epoch: 2952 mean train loss:  1.69462987e-02, mean val. rec. loss:  1.58072436e-02\n",
      "Epoch: 2953 mean train loss:  1.69431851e-02, mean val. rec. loss:  1.58039166e-02\n",
      "Epoch: 2954 mean train loss:  1.69400734e-02, mean val. rec. loss:  1.58005963e-02\n",
      "Epoch: 2955 mean train loss:  1.69369579e-02, mean val. rec. loss:  1.57972783e-02\n",
      "Epoch: 2956 mean train loss:  1.69338518e-02, mean val. rec. loss:  1.57939716e-02\n",
      "Epoch: 2957 mean train loss:  1.69307512e-02, mean val. rec. loss:  1.57906649e-02\n",
      "Epoch: 2958 mean train loss:  1.69276525e-02, mean val. rec. loss:  1.57873628e-02\n",
      "Epoch: 2959 mean train loss:  1.69245576e-02, mean val. rec. loss:  1.57840742e-02\n",
      "Epoch: 2960 mean train loss:  1.69214701e-02, mean val. rec. loss:  1.57807834e-02\n",
      "Epoch: 2961 mean train loss:  1.69183863e-02, mean val. rec. loss:  1.57774949e-02\n",
      "Epoch: 2962 mean train loss:  1.69153099e-02, mean val. rec. loss:  1.57742211e-02\n",
      "Epoch: 2963 mean train loss:  1.69122355e-02, mean val. rec. loss:  1.57709451e-02\n",
      "Epoch: 2964 mean train loss:  1.69091629e-02, mean val. rec. loss:  1.57676747e-02\n",
      "Epoch: 2965 mean train loss:  1.69060940e-02, mean val. rec. loss:  1.57644168e-02\n",
      "Epoch: 2966 mean train loss:  1.69030325e-02, mean val. rec. loss:  1.57611577e-02\n",
      "Epoch: 2967 mean train loss:  1.68999748e-02, mean val. rec. loss:  1.57578975e-02\n",
      "Epoch: 2968 mean train loss:  1.68969227e-02, mean val. rec. loss:  1.57546487e-02\n",
      "Epoch: 2969 mean train loss:  1.68938724e-02, mean val. rec. loss:  1.57514112e-02\n",
      "Epoch: 2970 mean train loss:  1.68908277e-02, mean val. rec. loss:  1.57481725e-02\n",
      "Epoch: 2971 mean train loss:  1.68877886e-02, mean val. rec. loss:  1.57449339e-02\n",
      "Epoch: 2972 mean train loss:  1.68847533e-02, mean val. rec. loss:  1.57416998e-02\n",
      "Epoch: 2973 mean train loss:  1.68817198e-02, mean val. rec. loss:  1.57384816e-02\n",
      "Epoch: 2974 mean train loss:  1.68786900e-02, mean val. rec. loss:  1.57352679e-02\n",
      "Epoch: 2975 mean train loss:  1.68756695e-02, mean val. rec. loss:  1.57320531e-02\n",
      "Epoch: 2976 mean train loss:  1.68726527e-02, mean val. rec. loss:  1.57288416e-02\n",
      "Epoch: 2977 mean train loss:  1.68696397e-02, mean val. rec. loss:  1.57256427e-02\n",
      "Epoch: 2978 mean train loss:  1.68666286e-02, mean val. rec. loss:  1.57224460e-02\n",
      "Epoch: 2979 mean train loss:  1.68636211e-02, mean val. rec. loss:  1.57192505e-02\n",
      "Epoch: 2980 mean train loss:  1.68606193e-02, mean val. rec. loss:  1.57160606e-02\n",
      "Epoch: 2981 mean train loss:  1.68576249e-02, mean val. rec. loss:  1.57128787e-02\n",
      "Epoch: 2982 mean train loss:  1.68546267e-02, mean val. rec. loss:  1.57097012e-02\n",
      "Epoch: 2983 mean train loss:  1.68516361e-02, mean val. rec. loss:  1.57065238e-02\n",
      "Epoch: 2984 mean train loss:  1.68486528e-02, mean val. rec. loss:  1.57033578e-02\n",
      "Epoch: 2985 mean train loss:  1.68456771e-02, mean val. rec. loss:  1.57001963e-02\n",
      "Epoch: 2986 mean train loss:  1.68426975e-02, mean val. rec. loss:  1.56970427e-02\n",
      "Epoch: 2987 mean train loss:  1.68397255e-02, mean val. rec. loss:  1.56938845e-02\n",
      "Epoch: 2988 mean train loss:  1.68367572e-02, mean val. rec. loss:  1.56907321e-02\n",
      "Epoch: 2989 mean train loss:  1.68337944e-02, mean val. rec. loss:  1.56875944e-02\n",
      "Epoch: 2990 mean train loss:  1.68308335e-02, mean val. rec. loss:  1.56844612e-02\n",
      "Epoch: 2991 mean train loss:  1.68278782e-02, mean val. rec. loss:  1.56813189e-02\n",
      "Epoch: 2992 mean train loss:  1.68249229e-02, mean val. rec. loss:  1.56781903e-02\n",
      "Epoch: 2993 mean train loss:  1.68219788e-02, mean val. rec. loss:  1.56750673e-02\n",
      "Epoch: 2994 mean train loss:  1.68190384e-02, mean val. rec. loss:  1.56719466e-02\n",
      "Epoch: 2995 mean train loss:  1.68160980e-02, mean val. rec. loss:  1.56688305e-02\n",
      "Epoch: 2996 mean train loss:  1.68131613e-02, mean val. rec. loss:  1.56657200e-02\n",
      "Epoch: 2997 mean train loss:  1.68102284e-02, mean val. rec. loss:  1.56626163e-02\n",
      "Epoch: 2998 mean train loss:  1.68073010e-02, mean val. rec. loss:  1.56595182e-02\n",
      "Epoch: 2999 mean train loss:  1.68043811e-02, mean val. rec. loss:  1.56564225e-02\n",
      "Epoch: 3000 mean train loss:  1.68014612e-02, mean val. rec. loss:  1.56533313e-02\n",
      "Epoch: 3001 mean train loss:  1.67985469e-02, mean val. rec. loss:  1.56502514e-02\n",
      "Epoch: 3002 mean train loss:  1.67956381e-02, mean val. rec. loss:  1.56471715e-02\n",
      "Epoch: 3003 mean train loss:  1.67927350e-02, mean val. rec. loss:  1.56440916e-02\n",
      "Epoch: 3004 mean train loss:  1.67898318e-02, mean val. rec. loss:  1.56410186e-02\n",
      "Epoch: 3005 mean train loss:  1.67869324e-02, mean val. rec. loss:  1.56379568e-02\n",
      "Epoch: 3006 mean train loss:  1.67840385e-02, mean val. rec. loss:  1.56348985e-02\n",
      "Epoch: 3007 mean train loss:  1.67811484e-02, mean val. rec. loss:  1.56318424e-02\n",
      "Epoch: 3008 mean train loss:  1.67782620e-02, mean val. rec. loss:  1.56287852e-02\n",
      "Epoch: 3009 mean train loss:  1.67753794e-02, mean val. rec. loss:  1.56257394e-02\n",
      "Epoch: 3010 mean train loss:  1.67725041e-02, mean val. rec. loss:  1.56227026e-02\n",
      "Epoch: 3011 mean train loss:  1.67696289e-02, mean val. rec. loss:  1.56196624e-02\n",
      "Epoch: 3012 mean train loss:  1.67667611e-02, mean val. rec. loss:  1.56166290e-02\n",
      "Epoch: 3013 mean train loss:  1.67638915e-02, mean val. rec. loss:  1.56135967e-02\n",
      "Epoch: 3014 mean train loss:  1.67610275e-02, mean val. rec. loss:  1.56105781e-02\n",
      "Epoch: 3015 mean train loss:  1.67581727e-02, mean val. rec. loss:  1.56075594e-02\n",
      "Epoch: 3016 mean train loss:  1.67553217e-02, mean val. rec. loss:  1.56045385e-02\n",
      "Epoch: 3017 mean train loss:  1.67524688e-02, mean val. rec. loss:  1.56015335e-02\n",
      "Epoch: 3018 mean train loss:  1.67496234e-02, mean val. rec. loss:  1.55985307e-02\n",
      "Epoch: 3019 mean train loss:  1.67467798e-02, mean val. rec. loss:  1.55955291e-02\n",
      "Epoch: 3020 mean train loss:  1.67439419e-02, mean val. rec. loss:  1.55925297e-02\n",
      "Epoch: 3021 mean train loss:  1.67411095e-02, mean val. rec. loss:  1.55895451e-02\n",
      "Epoch: 3022 mean train loss:  1.67382771e-02, mean val. rec. loss:  1.55865639e-02\n",
      "Epoch: 3023 mean train loss:  1.67354540e-02, mean val. rec. loss:  1.55835792e-02\n",
      "Epoch: 3024 mean train loss:  1.67326291e-02, mean val. rec. loss:  1.55806003e-02\n",
      "Epoch: 3025 mean train loss:  1.67298097e-02, mean val. rec. loss:  1.55776395e-02\n",
      "Epoch: 3026 mean train loss:  1.67269959e-02, mean val. rec. loss:  1.55746719e-02\n",
      "Epoch: 3027 mean train loss:  1.67241896e-02, mean val. rec. loss:  1.55717099e-02\n",
      "Epoch: 3028 mean train loss:  1.67213796e-02, mean val. rec. loss:  1.55687480e-02\n",
      "Epoch: 3029 mean train loss:  1.67185788e-02, mean val. rec. loss:  1.55657996e-02\n",
      "Epoch: 3030 mean train loss:  1.67157781e-02, mean val. rec. loss:  1.55628592e-02\n",
      "Epoch: 3031 mean train loss:  1.67129829e-02, mean val. rec. loss:  1.55599075e-02\n",
      "Epoch: 3032 mean train loss:  1.67101915e-02, mean val. rec. loss:  1.55569682e-02\n",
      "Epoch: 3033 mean train loss:  1.67074057e-02, mean val. rec. loss:  1.55540380e-02\n",
      "Epoch: 3034 mean train loss:  1.67046198e-02, mean val. rec. loss:  1.55511101e-02\n",
      "Epoch: 3035 mean train loss:  1.67018415e-02, mean val. rec. loss:  1.55481844e-02\n",
      "Epoch: 3036 mean train loss:  1.66990668e-02, mean val. rec. loss:  1.55452644e-02\n",
      "Epoch: 3037 mean train loss:  1.66962921e-02, mean val. rec. loss:  1.55423467e-02\n",
      "Epoch: 3038 mean train loss:  1.66935231e-02, mean val. rec. loss:  1.55394358e-02\n",
      "Epoch: 3039 mean train loss:  1.66907577e-02, mean val. rec. loss:  1.55365271e-02\n",
      "Epoch: 3040 mean train loss:  1.66879979e-02, mean val. rec. loss:  1.55336196e-02\n",
      "Epoch: 3041 mean train loss:  1.66852400e-02, mean val. rec. loss:  1.55307291e-02\n",
      "Epoch: 3042 mean train loss:  1.66824896e-02, mean val. rec. loss:  1.55278341e-02\n",
      "Epoch: 3043 mean train loss:  1.66797373e-02, mean val. rec. loss:  1.55249424e-02\n",
      "Epoch: 3044 mean train loss:  1.66769924e-02, mean val. rec. loss:  1.55220644e-02\n",
      "Epoch: 3045 mean train loss:  1.66742512e-02, mean val. rec. loss:  1.55191830e-02\n",
      "Epoch: 3046 mean train loss:  1.66715175e-02, mean val. rec. loss:  1.55163083e-02\n",
      "Epoch: 3047 mean train loss:  1.66687801e-02, mean val. rec. loss:  1.55134348e-02\n",
      "Epoch: 3048 mean train loss:  1.66660464e-02, mean val. rec. loss:  1.55105693e-02\n",
      "Epoch: 3049 mean train loss:  1.66633220e-02, mean val. rec. loss:  1.55077128e-02\n",
      "Epoch: 3050 mean train loss:  1.66605995e-02, mean val. rec. loss:  1.55048472e-02\n",
      "Epoch: 3051 mean train loss:  1.66578751e-02, mean val. rec. loss:  1.55019998e-02\n",
      "Epoch: 3052 mean train loss:  1.66551657e-02, mean val. rec. loss:  1.54991490e-02\n",
      "Epoch: 3053 mean train loss:  1.66524506e-02, mean val. rec. loss:  1.54963061e-02\n",
      "Epoch: 3054 mean train loss:  1.66497448e-02, mean val. rec. loss:  1.54934655e-02\n",
      "Epoch: 3055 mean train loss:  1.66470335e-02, mean val. rec. loss:  1.54906328e-02\n",
      "Epoch: 3056 mean train loss:  1.66443370e-02, mean val. rec. loss:  1.54878047e-02\n",
      "Epoch: 3057 mean train loss:  1.66416443e-02, mean val. rec. loss:  1.54849766e-02\n",
      "Epoch: 3058 mean train loss:  1.66389478e-02, mean val. rec. loss:  1.54821496e-02\n",
      "Epoch: 3059 mean train loss:  1.66362532e-02, mean val. rec. loss:  1.54793362e-02\n",
      "Epoch: 3060 mean train loss:  1.66335717e-02, mean val. rec. loss:  1.54765284e-02\n",
      "Epoch: 3061 mean train loss:  1.66308920e-02, mean val. rec. loss:  1.54737139e-02\n",
      "Epoch: 3062 mean train loss:  1.66282142e-02, mean val. rec. loss:  1.54709107e-02\n",
      "Epoch: 3063 mean train loss:  1.66255363e-02, mean val. rec. loss:  1.54681109e-02\n",
      "Epoch: 3064 mean train loss:  1.66228660e-02, mean val. rec. loss:  1.54653157e-02\n",
      "Epoch: 3065 mean train loss:  1.66201993e-02, mean val. rec. loss:  1.54625261e-02\n",
      "Epoch: 3066 mean train loss:  1.66175345e-02, mean val. rec. loss:  1.54597343e-02\n",
      "Epoch: 3067 mean train loss:  1.66148697e-02, mean val. rec. loss:  1.54569526e-02\n",
      "Epoch: 3068 mean train loss:  1.66122142e-02, mean val. rec. loss:  1.54541755e-02\n",
      "Epoch: 3069 mean train loss:  1.66095625e-02, mean val. rec. loss:  1.54514007e-02\n",
      "Epoch: 3070 mean train loss:  1.66069144e-02, mean val. rec. loss:  1.54486315e-02\n",
      "Epoch: 3071 mean train loss:  1.66042627e-02, mean val. rec. loss:  1.54458680e-02\n",
      "Epoch: 3072 mean train loss:  1.66016202e-02, mean val. rec. loss:  1.54431090e-02\n",
      "Epoch: 3073 mean train loss:  1.65989871e-02, mean val. rec. loss:  1.54403523e-02\n",
      "Epoch: 3074 mean train loss:  1.65963446e-02, mean val. rec. loss:  1.54375979e-02\n",
      "Epoch: 3075 mean train loss:  1.65937152e-02, mean val. rec. loss:  1.54348503e-02\n",
      "Epoch: 3076 mean train loss:  1.65910858e-02, mean val. rec. loss:  1.54321117e-02\n",
      "Epoch: 3077 mean train loss:  1.65884657e-02, mean val. rec. loss:  1.54293664e-02\n",
      "Epoch: 3078 mean train loss:  1.65858382e-02, mean val. rec. loss:  1.54266335e-02\n",
      "Epoch: 3079 mean train loss:  1.65832199e-02, mean val. rec. loss:  1.54239051e-02\n",
      "Epoch: 3080 mean train loss:  1.65806091e-02, mean val. rec. loss:  1.54211745e-02\n",
      "Epoch: 3081 mean train loss:  1.65779965e-02, mean val. rec. loss:  1.54184530e-02\n",
      "Epoch: 3082 mean train loss:  1.65753876e-02, mean val. rec. loss:  1.54157360e-02\n",
      "Epoch: 3083 mean train loss:  1.65727842e-02, mean val. rec. loss:  1.54130246e-02\n",
      "Epoch: 3084 mean train loss:  1.65701865e-02, mean val. rec. loss:  1.54103121e-02\n",
      "Epoch: 3085 mean train loss:  1.65675887e-02, mean val. rec. loss:  1.54076053e-02\n",
      "Epoch: 3086 mean train loss:  1.65649965e-02, mean val. rec. loss:  1.54049042e-02\n",
      "Epoch: 3087 mean train loss:  1.65624081e-02, mean val. rec. loss:  1.54022076e-02\n",
      "Epoch: 3088 mean train loss:  1.65598196e-02, mean val. rec. loss:  1.53995122e-02\n",
      "Epoch: 3089 mean train loss:  1.65572368e-02, mean val. rec. loss:  1.53968258e-02\n",
      "Epoch: 3090 mean train loss:  1.65546558e-02, mean val. rec. loss:  1.53941405e-02\n",
      "Epoch: 3091 mean train loss:  1.65520748e-02, mean val. rec. loss:  1.53914609e-02\n",
      "Epoch: 3092 mean train loss:  1.65495068e-02, mean val. rec. loss:  1.53887836e-02\n",
      "Epoch: 3093 mean train loss:  1.65469407e-02, mean val. rec. loss:  1.53861097e-02\n",
      "Epoch: 3094 mean train loss:  1.65443709e-02, mean val. rec. loss:  1.53834426e-02\n",
      "Epoch: 3095 mean train loss:  1.65418067e-02, mean val. rec. loss:  1.53807789e-02\n",
      "Epoch: 3096 mean train loss:  1.65392536e-02, mean val. rec. loss:  1.53781186e-02\n",
      "Epoch: 3097 mean train loss:  1.65366912e-02, mean val. rec. loss:  1.53754650e-02\n",
      "Epoch: 3098 mean train loss:  1.65341438e-02, mean val. rec. loss:  1.53728104e-02\n",
      "Epoch: 3099 mean train loss:  1.65315926e-02, mean val. rec. loss:  1.53701682e-02\n",
      "Epoch: 3100 mean train loss:  1.65290507e-02, mean val. rec. loss:  1.53675261e-02\n",
      "Epoch: 3101 mean train loss:  1.65265032e-02, mean val. rec. loss:  1.53648873e-02\n",
      "Epoch: 3102 mean train loss:  1.65239650e-02, mean val. rec. loss:  1.53622531e-02\n",
      "Epoch: 3103 mean train loss:  1.65214343e-02, mean val. rec. loss:  1.53596245e-02\n",
      "Epoch: 3104 mean train loss:  1.65188999e-02, mean val. rec. loss:  1.53569937e-02\n",
      "Epoch: 3105 mean train loss:  1.65163673e-02, mean val. rec. loss:  1.53543731e-02\n",
      "Epoch: 3106 mean train loss:  1.65138459e-02, mean val. rec. loss:  1.53517525e-02\n",
      "Epoch: 3107 mean train loss:  1.65113245e-02, mean val. rec. loss:  1.53491341e-02\n",
      "Epoch: 3108 mean train loss:  1.65088068e-02, mean val. rec. loss:  1.53465248e-02\n",
      "Epoch: 3109 mean train loss:  1.65062928e-02, mean val. rec. loss:  1.53439178e-02\n",
      "Epoch: 3110 mean train loss:  1.65037807e-02, mean val. rec. loss:  1.53413108e-02\n",
      "Epoch: 3111 mean train loss:  1.65012705e-02, mean val. rec. loss:  1.53387163e-02\n",
      "Epoch: 3112 mean train loss:  1.64987659e-02, mean val. rec. loss:  1.53361229e-02\n",
      "Epoch: 3113 mean train loss:  1.64962649e-02, mean val. rec. loss:  1.53335283e-02\n",
      "Epoch: 3114 mean train loss:  1.64937603e-02, mean val. rec. loss:  1.53309327e-02\n",
      "Epoch: 3115 mean train loss:  1.64912631e-02, mean val. rec. loss:  1.53283551e-02\n",
      "Epoch: 3116 mean train loss:  1.64887733e-02, mean val. rec. loss:  1.53257799e-02\n",
      "Epoch: 3117 mean train loss:  1.64862910e-02, mean val. rec. loss:  1.53231989e-02\n",
      "Epoch: 3118 mean train loss:  1.64837994e-02, mean val. rec. loss:  1.53206293e-02\n",
      "Epoch: 3119 mean train loss:  1.64813134e-02, mean val. rec. loss:  1.53180666e-02\n",
      "Epoch: 3120 mean train loss:  1.64788404e-02, mean val. rec. loss:  1.53154981e-02\n",
      "Epoch: 3121 mean train loss:  1.64763618e-02, mean val. rec. loss:  1.53129421e-02\n",
      "Epoch: 3122 mean train loss:  1.64738944e-02, mean val. rec. loss:  1.53103839e-02\n",
      "Epoch: 3123 mean train loss:  1.64714159e-02, mean val. rec. loss:  1.53078336e-02\n",
      "Epoch: 3124 mean train loss:  1.64689578e-02, mean val. rec. loss:  1.53052912e-02\n",
      "Epoch: 3125 mean train loss:  1.64664904e-02, mean val. rec. loss:  1.53027488e-02\n",
      "Epoch: 3126 mean train loss:  1.64640323e-02, mean val. rec. loss:  1.53002087e-02\n",
      "Epoch: 3127 mean train loss:  1.64615761e-02, mean val. rec. loss:  1.52976799e-02\n",
      "Epoch: 3128 mean train loss:  1.64591217e-02, mean val. rec. loss:  1.52951398e-02\n",
      "Epoch: 3129 mean train loss:  1.64566655e-02, mean val. rec. loss:  1.52926190e-02\n",
      "Epoch: 3130 mean train loss:  1.64542241e-02, mean val. rec. loss:  1.52900914e-02\n",
      "Epoch: 3131 mean train loss:  1.64517809e-02, mean val. rec. loss:  1.52875739e-02\n",
      "Epoch: 3132 mean train loss:  1.64493433e-02, mean val. rec. loss:  1.52850542e-02\n",
      "Epoch: 3133 mean train loss:  1.64469039e-02, mean val. rec. loss:  1.52825459e-02\n",
      "Epoch: 3134 mean train loss:  1.64444718e-02, mean val. rec. loss:  1.52800341e-02\n",
      "Epoch: 3135 mean train loss:  1.64420379e-02, mean val. rec. loss:  1.52775337e-02\n",
      "Epoch: 3136 mean train loss:  1.64396115e-02, mean val. rec. loss:  1.52750299e-02\n",
      "Epoch: 3137 mean train loss:  1.64371832e-02, mean val. rec. loss:  1.52725351e-02\n",
      "Epoch: 3138 mean train loss:  1.64347624e-02, mean val. rec. loss:  1.52700358e-02\n",
      "Epoch: 3139 mean train loss:  1.64323397e-02, mean val. rec. loss:  1.52675536e-02\n",
      "Epoch: 3140 mean train loss:  1.64299300e-02, mean val. rec. loss:  1.52650645e-02\n",
      "Epoch: 3141 mean train loss:  1.64275185e-02, mean val. rec. loss:  1.52625856e-02\n",
      "Epoch: 3142 mean train loss:  1.64251069e-02, mean val. rec. loss:  1.52601045e-02\n",
      "Epoch: 3143 mean train loss:  1.64226935e-02, mean val. rec. loss:  1.52576347e-02\n",
      "Epoch: 3144 mean train loss:  1.64203006e-02, mean val. rec. loss:  1.52551603e-02\n",
      "Epoch: 3145 mean train loss:  1.64178909e-02, mean val. rec. loss:  1.52526985e-02\n",
      "Epoch: 3146 mean train loss:  1.64155036e-02, mean val. rec. loss:  1.52502298e-02\n",
      "Epoch: 3147 mean train loss:  1.64131014e-02, mean val. rec. loss:  1.52477736e-02\n",
      "Epoch: 3148 mean train loss:  1.64107140e-02, mean val. rec. loss:  1.52453174e-02\n",
      "Epoch: 3149 mean train loss:  1.64083230e-02, mean val. rec. loss:  1.52428703e-02\n",
      "Epoch: 3150 mean train loss:  1.64059450e-02, mean val. rec. loss:  1.52404243e-02\n",
      "Epoch: 3151 mean train loss:  1.64035595e-02, mean val. rec. loss:  1.52379715e-02\n",
      "Epoch: 3152 mean train loss:  1.64011759e-02, mean val. rec. loss:  1.52355335e-02\n",
      "Epoch: 3153 mean train loss:  1.63988053e-02, mean val. rec. loss:  1.52331011e-02\n",
      "Epoch: 3154 mean train loss:  1.63964366e-02, mean val. rec. loss:  1.52306608e-02\n",
      "Epoch: 3155 mean train loss:  1.63940623e-02, mean val. rec. loss:  1.52282329e-02\n",
      "Epoch: 3156 mean train loss:  1.63916936e-02, mean val. rec. loss:  1.52258108e-02\n",
      "Epoch: 3157 mean train loss:  1.63893324e-02, mean val. rec. loss:  1.52233886e-02\n",
      "Epoch: 3158 mean train loss:  1.63869711e-02, mean val. rec. loss:  1.52209721e-02\n",
      "Epoch: 3159 mean train loss:  1.63846136e-02, mean val. rec. loss:  1.52185533e-02\n",
      "Epoch: 3160 mean train loss:  1.63822560e-02, mean val. rec. loss:  1.52161447e-02\n",
      "Epoch: 3161 mean train loss:  1.63799060e-02, mean val. rec. loss:  1.52137384e-02\n",
      "Epoch: 3162 mean train loss:  1.63775596e-02, mean val. rec. loss:  1.52113344e-02\n",
      "Epoch: 3163 mean train loss:  1.63752170e-02, mean val. rec. loss:  1.52089349e-02\n",
      "Epoch: 3164 mean train loss:  1.63728781e-02, mean val. rec. loss:  1.52065400e-02\n",
      "Epoch: 3165 mean train loss:  1.63705336e-02, mean val. rec. loss:  1.52041461e-02\n",
      "Epoch: 3166 mean train loss:  1.63681965e-02, mean val. rec. loss:  1.52017614e-02\n",
      "Epoch: 3167 mean train loss:  1.63658650e-02, mean val. rec. loss:  1.51993778e-02\n",
      "Epoch: 3168 mean train loss:  1.63635354e-02, mean val. rec. loss:  1.51969953e-02\n",
      "Epoch: 3169 mean train loss:  1.63612058e-02, mean val. rec. loss:  1.51946173e-02\n",
      "Epoch: 3170 mean train loss:  1.63588837e-02, mean val. rec. loss:  1.51922439e-02\n",
      "Epoch: 3171 mean train loss:  1.63565615e-02, mean val. rec. loss:  1.51898773e-02\n",
      "Epoch: 3172 mean train loss:  1.63542413e-02, mean val. rec. loss:  1.51875073e-02\n",
      "Epoch: 3173 mean train loss:  1.63519266e-02, mean val. rec. loss:  1.51851418e-02\n",
      "Epoch: 3174 mean train loss:  1.63496137e-02, mean val. rec. loss:  1.51827854e-02\n",
      "Epoch: 3175 mean train loss:  1.63473027e-02, mean val. rec. loss:  1.51804313e-02\n",
      "Epoch: 3176 mean train loss:  1.63449973e-02, mean val. rec. loss:  1.51780737e-02\n",
      "Epoch: 3177 mean train loss:  1.63426919e-02, mean val. rec. loss:  1.51757264e-02\n",
      "Epoch: 3178 mean train loss:  1.63403866e-02, mean val. rec. loss:  1.51733847e-02\n",
      "Epoch: 3179 mean train loss:  1.63380923e-02, mean val. rec. loss:  1.51710442e-02\n",
      "Epoch: 3180 mean train loss:  1.63357981e-02, mean val. rec. loss:  1.51687082e-02\n",
      "Epoch: 3181 mean train loss:  1.63335020e-02, mean val. rec. loss:  1.51663632e-02\n",
      "Epoch: 3182 mean train loss:  1.63312115e-02, mean val. rec. loss:  1.51640408e-02\n",
      "Epoch: 3183 mean train loss:  1.63289248e-02, mean val. rec. loss:  1.51617048e-02\n",
      "Epoch: 3184 mean train loss:  1.63266436e-02, mean val. rec. loss:  1.51593858e-02\n",
      "Epoch: 3185 mean train loss:  1.63243605e-02, mean val. rec. loss:  1.51570566e-02\n",
      "Epoch: 3186 mean train loss:  1.63220812e-02, mean val. rec. loss:  1.51547444e-02\n",
      "Epoch: 3187 mean train loss:  1.63198056e-02, mean val. rec. loss:  1.51524289e-02\n",
      "Epoch: 3188 mean train loss:  1.63175356e-02, mean val. rec. loss:  1.51501224e-02\n",
      "Epoch: 3189 mean train loss:  1.63152656e-02, mean val. rec. loss:  1.51478136e-02\n",
      "Epoch: 3190 mean train loss:  1.63129956e-02, mean val. rec. loss:  1.51455116e-02\n",
      "Epoch: 3191 mean train loss:  1.63107293e-02, mean val. rec. loss:  1.51432187e-02\n",
      "Epoch: 3192 mean train loss:  1.63084723e-02, mean val. rec. loss:  1.51409179e-02\n",
      "Epoch: 3193 mean train loss:  1.63062154e-02, mean val. rec. loss:  1.51386250e-02\n",
      "Epoch: 3194 mean train loss:  1.63039547e-02, mean val. rec. loss:  1.51363389e-02\n",
      "Epoch: 3195 mean train loss:  1.63017051e-02, mean val. rec. loss:  1.51340505e-02\n",
      "Epoch: 3196 mean train loss:  1.62994519e-02, mean val. rec. loss:  1.51317678e-02\n",
      "Epoch: 3197 mean train loss:  1.62972079e-02, mean val. rec. loss:  1.51294862e-02\n",
      "Epoch: 3198 mean train loss:  1.62949603e-02, mean val. rec. loss:  1.51272183e-02\n",
      "Epoch: 3199 mean train loss:  1.62927201e-02, mean val. rec. loss:  1.51249413e-02\n",
      "Epoch: 3200 mean train loss:  1.62904799e-02, mean val. rec. loss:  1.51226722e-02\n",
      "Epoch: 3201 mean train loss:  1.62882452e-02, mean val. rec. loss:  1.51204031e-02\n",
      "Epoch: 3202 mean train loss:  1.62860069e-02, mean val. rec. loss:  1.51181465e-02\n",
      "Epoch: 3203 mean train loss:  1.62837797e-02, mean val. rec. loss:  1.51158842e-02\n",
      "Epoch: 3204 mean train loss:  1.62815506e-02, mean val. rec. loss:  1.51136355e-02\n",
      "Epoch: 3205 mean train loss:  1.62793253e-02, mean val. rec. loss:  1.51113744e-02\n",
      "Epoch: 3206 mean train loss:  1.62770981e-02, mean val. rec. loss:  1.51091325e-02\n",
      "Epoch: 3207 mean train loss:  1.62748821e-02, mean val. rec. loss:  1.51068838e-02\n",
      "Epoch: 3208 mean train loss:  1.62726624e-02, mean val. rec. loss:  1.51046442e-02\n",
      "Epoch: 3209 mean train loss:  1.62704501e-02, mean val. rec. loss:  1.51024058e-02\n",
      "Epoch: 3210 mean train loss:  1.62682397e-02, mean val. rec. loss:  1.51001707e-02\n",
      "Epoch: 3211 mean train loss:  1.62660293e-02, mean val. rec. loss:  1.50979345e-02\n",
      "Epoch: 3212 mean train loss:  1.62638207e-02, mean val. rec. loss:  1.50957074e-02\n",
      "Epoch: 3213 mean train loss:  1.62616178e-02, mean val. rec. loss:  1.50934825e-02\n",
      "Epoch: 3214 mean train loss:  1.62594185e-02, mean val. rec. loss:  1.50912554e-02\n",
      "Epoch: 3215 mean train loss:  1.62572174e-02, mean val. rec. loss:  1.50890441e-02\n",
      "Epoch: 3216 mean train loss:  1.62550219e-02, mean val. rec. loss:  1.50868249e-02\n",
      "Epoch: 3217 mean train loss:  1.62528338e-02, mean val. rec. loss:  1.50846148e-02\n",
      "Epoch: 3218 mean train loss:  1.62506420e-02, mean val. rec. loss:  1.50824081e-02\n",
      "Epoch: 3219 mean train loss:  1.62484483e-02, mean val. rec. loss:  1.50802002e-02\n",
      "Epoch: 3220 mean train loss:  1.62462659e-02, mean val. rec. loss:  1.50780037e-02\n",
      "Epoch: 3221 mean train loss:  1.62440890e-02, mean val. rec. loss:  1.50758015e-02\n",
      "Epoch: 3222 mean train loss:  1.62419027e-02, mean val. rec. loss:  1.50736073e-02\n",
      "Epoch: 3223 mean train loss:  1.62397314e-02, mean val. rec. loss:  1.50714142e-02\n",
      "Epoch: 3224 mean train loss:  1.62375527e-02, mean val. rec. loss:  1.50692301e-02\n",
      "Epoch: 3225 mean train loss:  1.62353851e-02, mean val. rec. loss:  1.50670404e-02\n",
      "Epoch: 3226 mean train loss:  1.62332100e-02, mean val. rec. loss:  1.50648643e-02\n",
      "Epoch: 3227 mean train loss:  1.62310499e-02, mean val. rec. loss:  1.50626826e-02\n",
      "Epoch: 3228 mean train loss:  1.62288805e-02, mean val. rec. loss:  1.50605087e-02\n",
      "Epoch: 3229 mean train loss:  1.62267222e-02, mean val. rec. loss:  1.50583360e-02\n",
      "Epoch: 3230 mean train loss:  1.62245658e-02, mean val. rec. loss:  1.50561633e-02\n",
      "Epoch: 3231 mean train loss:  1.62224056e-02, mean val. rec. loss:  1.50539963e-02\n",
      "Epoch: 3232 mean train loss:  1.62202511e-02, mean val. rec. loss:  1.50518349e-02\n",
      "Epoch: 3233 mean train loss:  1.62181002e-02, mean val. rec. loss:  1.50496770e-02\n",
      "Epoch: 3234 mean train loss:  1.62159550e-02, mean val. rec. loss:  1.50475179e-02\n",
      "Epoch: 3235 mean train loss:  1.62138079e-02, mean val. rec. loss:  1.50453633e-02\n",
      "Epoch: 3236 mean train loss:  1.62116664e-02, mean val. rec. loss:  1.50432156e-02\n",
      "Epoch: 3237 mean train loss:  1.62095267e-02, mean val. rec. loss:  1.50410656e-02\n",
      "Epoch: 3238 mean train loss:  1.62073870e-02, mean val. rec. loss:  1.50389223e-02\n",
      "Epoch: 3239 mean train loss:  1.62052511e-02, mean val. rec. loss:  1.50367859e-02\n",
      "Epoch: 3240 mean train loss:  1.62031208e-02, mean val. rec. loss:  1.50346472e-02\n",
      "Epoch: 3241 mean train loss:  1.62009904e-02, mean val. rec. loss:  1.50325131e-02\n",
      "Epoch: 3242 mean train loss:  1.61988582e-02, mean val. rec. loss:  1.50303756e-02\n",
      "Epoch: 3243 mean train loss:  1.61967279e-02, mean val. rec. loss:  1.50282539e-02\n",
      "Epoch: 3244 mean train loss:  1.61946087e-02, mean val. rec. loss:  1.50261277e-02\n",
      "Epoch: 3245 mean train loss:  1.61924932e-02, mean val. rec. loss:  1.50240083e-02\n",
      "Epoch: 3246 mean train loss:  1.61903722e-02, mean val. rec. loss:  1.50218855e-02\n",
      "Epoch: 3247 mean train loss:  1.61882530e-02, mean val. rec. loss:  1.50197740e-02\n",
      "Epoch: 3248 mean train loss:  1.61861432e-02, mean val. rec. loss:  1.50176659e-02\n",
      "Epoch: 3249 mean train loss:  1.61840333e-02, mean val. rec. loss:  1.50155590e-02\n",
      "Epoch: 3250 mean train loss:  1.61819253e-02, mean val. rec. loss:  1.50134464e-02\n",
      "Epoch: 3251 mean train loss:  1.61798210e-02, mean val. rec. loss:  1.50113440e-02\n",
      "Epoch: 3252 mean train loss:  1.61777168e-02, mean val. rec. loss:  1.50092484e-02\n",
      "Epoch: 3253 mean train loss:  1.61756181e-02, mean val. rec. loss:  1.50071517e-02\n",
      "Epoch: 3254 mean train loss:  1.61735194e-02, mean val. rec. loss:  1.50050527e-02\n",
      "Epoch: 3255 mean train loss:  1.61714226e-02, mean val. rec. loss:  1.50029651e-02\n",
      "Epoch: 3256 mean train loss:  1.61693332e-02, mean val. rec. loss:  1.50008774e-02\n",
      "Epoch: 3257 mean train loss:  1.61672401e-02, mean val. rec. loss:  1.49987898e-02\n",
      "Epoch: 3258 mean train loss:  1.61651488e-02, mean val. rec. loss:  1.49967112e-02\n",
      "Epoch: 3259 mean train loss:  1.61630688e-02, mean val. rec. loss:  1.49946315e-02\n",
      "Epoch: 3260 mean train loss:  1.61609868e-02, mean val. rec. loss:  1.49925586e-02\n",
      "Epoch: 3261 mean train loss:  1.61589012e-02, mean val. rec. loss:  1.49904834e-02\n",
      "Epoch: 3262 mean train loss:  1.61568230e-02, mean val. rec. loss:  1.49884139e-02\n",
      "Epoch: 3263 mean train loss:  1.61547485e-02, mean val. rec. loss:  1.49863466e-02\n",
      "Epoch: 3264 mean train loss:  1.61526740e-02, mean val. rec. loss:  1.49842817e-02\n",
      "Epoch: 3265 mean train loss:  1.61506033e-02, mean val. rec. loss:  1.49822235e-02\n",
      "Epoch: 3266 mean train loss:  1.61485362e-02, mean val. rec. loss:  1.49801687e-02\n",
      "Epoch: 3267 mean train loss:  1.61464711e-02, mean val. rec. loss:  1.49781117e-02\n",
      "Epoch: 3268 mean train loss:  1.61444059e-02, mean val. rec. loss:  1.49760637e-02\n",
      "Epoch: 3269 mean train loss:  1.61423426e-02, mean val. rec. loss:  1.49740215e-02\n",
      "Epoch: 3270 mean train loss:  1.61402867e-02, mean val. rec. loss:  1.49719746e-02\n",
      "Epoch: 3271 mean train loss:  1.61382346e-02, mean val. rec. loss:  1.49699301e-02\n",
      "Epoch: 3272 mean train loss:  1.61361769e-02, mean val. rec. loss:  1.49678878e-02\n",
      "Epoch: 3273 mean train loss:  1.61341229e-02, mean val. rec. loss:  1.49658512e-02\n",
      "Epoch: 3274 mean train loss:  1.61320745e-02, mean val. rec. loss:  1.49638236e-02\n",
      "Epoch: 3275 mean train loss:  1.61300298e-02, mean val. rec. loss:  1.49617938e-02\n",
      "Epoch: 3276 mean train loss:  1.61279832e-02, mean val. rec. loss:  1.49597674e-02\n",
      "Epoch: 3277 mean train loss:  1.61259404e-02, mean val. rec. loss:  1.49577432e-02\n",
      "Epoch: 3278 mean train loss:  1.61239013e-02, mean val. rec. loss:  1.49557168e-02\n",
      "Epoch: 3279 mean train loss:  1.61218641e-02, mean val. rec. loss:  1.49536995e-02\n",
      "Epoch: 3280 mean train loss:  1.61198287e-02, mean val. rec. loss:  1.49516867e-02\n",
      "Epoch: 3281 mean train loss:  1.61177971e-02, mean val. rec. loss:  1.49496761e-02\n",
      "Epoch: 3282 mean train loss:  1.61157673e-02, mean val. rec. loss:  1.49476599e-02\n",
      "Epoch: 3283 mean train loss:  1.61137393e-02, mean val. rec. loss:  1.49456584e-02\n",
      "Epoch: 3284 mean train loss:  1.61117114e-02, mean val. rec. loss:  1.49436581e-02\n",
      "Epoch: 3285 mean train loss:  1.61096909e-02, mean val. rec. loss:  1.49416521e-02\n",
      "Epoch: 3286 mean train loss:  1.61076705e-02, mean val. rec. loss:  1.49396518e-02\n",
      "Epoch: 3287 mean train loss:  1.61056463e-02, mean val. rec. loss:  1.49376571e-02\n",
      "Epoch: 3288 mean train loss:  1.61036314e-02, mean val. rec. loss:  1.49356670e-02\n",
      "Epoch: 3289 mean train loss:  1.61016202e-02, mean val. rec. loss:  1.49336836e-02\n",
      "Epoch: 3290 mean train loss:  1.60996090e-02, mean val. rec. loss:  1.49316924e-02\n",
      "Epoch: 3291 mean train loss:  1.60975942e-02, mean val. rec. loss:  1.49297079e-02\n",
      "Epoch: 3292 mean train loss:  1.60955886e-02, mean val. rec. loss:  1.49277269e-02\n",
      "Epoch: 3293 mean train loss:  1.60935830e-02, mean val. rec. loss:  1.49257515e-02\n",
      "Epoch: 3294 mean train loss:  1.60915830e-02, mean val. rec. loss:  1.49237716e-02\n",
      "Epoch: 3295 mean train loss:  1.60895793e-02, mean val. rec. loss:  1.49218041e-02\n",
      "Epoch: 3296 mean train loss:  1.60875849e-02, mean val. rec. loss:  1.49198344e-02\n",
      "Epoch: 3297 mean train loss:  1.60855849e-02, mean val. rec. loss:  1.49178703e-02\n",
      "Epoch: 3298 mean train loss:  1.60835942e-02, mean val. rec. loss:  1.49159040e-02\n",
      "Epoch: 3299 mean train loss:  1.60816054e-02, mean val. rec. loss:  1.49139502e-02\n",
      "Epoch: 3300 mean train loss:  1.60796203e-02, mean val. rec. loss:  1.49119895e-02\n",
      "Epoch: 3301 mean train loss:  1.60776278e-02, mean val. rec. loss:  1.49100323e-02\n",
      "Epoch: 3302 mean train loss:  1.60756408e-02, mean val. rec. loss:  1.49080864e-02\n",
      "Epoch: 3303 mean train loss:  1.60736631e-02, mean val. rec. loss:  1.49061314e-02\n",
      "Epoch: 3304 mean train loss:  1.60716836e-02, mean val. rec. loss:  1.49041844e-02\n",
      "Epoch: 3305 mean train loss:  1.60697023e-02, mean val. rec. loss:  1.49022441e-02\n",
      "Epoch: 3306 mean train loss:  1.60677265e-02, mean val. rec. loss:  1.49003005e-02\n",
      "Epoch: 3307 mean train loss:  1.60657544e-02, mean val. rec. loss:  1.48983603e-02\n",
      "Epoch: 3308 mean train loss:  1.60637824e-02, mean val. rec. loss:  1.48964234e-02\n",
      "Epoch: 3309 mean train loss:  1.60618140e-02, mean val. rec. loss:  1.48944968e-02\n",
      "Epoch: 3310 mean train loss:  1.60598476e-02, mean val. rec. loss:  1.48925645e-02\n",
      "Epoch: 3311 mean train loss:  1.60578848e-02, mean val. rec. loss:  1.48906356e-02\n",
      "Epoch: 3312 mean train loss:  1.60559221e-02, mean val. rec. loss:  1.48887158e-02\n",
      "Epoch: 3313 mean train loss:  1.60539612e-02, mean val. rec. loss:  1.48867926e-02\n",
      "Epoch: 3314 mean train loss:  1.60520077e-02, mean val. rec. loss:  1.48848750e-02\n",
      "Epoch: 3315 mean train loss:  1.60500487e-02, mean val. rec. loss:  1.48829529e-02\n",
      "Epoch: 3316 mean train loss:  1.60480915e-02, mean val. rec. loss:  1.48810410e-02\n",
      "Epoch: 3317 mean train loss:  1.60461418e-02, mean val. rec. loss:  1.48791314e-02\n",
      "Epoch: 3318 mean train loss:  1.60441958e-02, mean val. rec. loss:  1.48772218e-02\n",
      "Epoch: 3319 mean train loss:  1.60422461e-02, mean val. rec. loss:  1.48753190e-02\n",
      "Epoch: 3320 mean train loss:  1.60403001e-02, mean val. rec. loss:  1.48734162e-02\n",
      "Epoch: 3321 mean train loss:  1.60383560e-02, mean val. rec. loss:  1.48715156e-02\n",
      "Epoch: 3322 mean train loss:  1.60364156e-02, mean val. rec. loss:  1.48696128e-02\n",
      "Epoch: 3323 mean train loss:  1.60344752e-02, mean val. rec. loss:  1.48677236e-02\n",
      "Epoch: 3324 mean train loss:  1.60325404e-02, mean val. rec. loss:  1.48658299e-02\n",
      "Epoch: 3325 mean train loss:  1.60306074e-02, mean val. rec. loss:  1.48639395e-02\n",
      "Epoch: 3326 mean train loss:  1.60286745e-02, mean val. rec. loss:  1.48620526e-02\n",
      "Epoch: 3327 mean train loss:  1.60267415e-02, mean val. rec. loss:  1.48601691e-02\n",
      "Epoch: 3328 mean train loss:  1.60248160e-02, mean val. rec. loss:  1.48582901e-02\n",
      "Epoch: 3329 mean train loss:  1.60228942e-02, mean val. rec. loss:  1.48564145e-02\n",
      "Epoch: 3330 mean train loss:  1.60209725e-02, mean val. rec. loss:  1.48545355e-02\n",
      "Epoch: 3331 mean train loss:  1.60190488e-02, mean val. rec. loss:  1.48526633e-02\n",
      "Epoch: 3332 mean train loss:  1.60171289e-02, mean val. rec. loss:  1.48508024e-02\n",
      "Epoch: 3333 mean train loss:  1.60152127e-02, mean val. rec. loss:  1.48489302e-02\n",
      "Epoch: 3334 mean train loss:  1.60133002e-02, mean val. rec. loss:  1.48470614e-02\n",
      "Epoch: 3335 mean train loss:  1.60113859e-02, mean val. rec. loss:  1.48451983e-02\n",
      "Epoch: 3336 mean train loss:  1.60094716e-02, mean val. rec. loss:  1.48433454e-02\n",
      "Epoch: 3337 mean train loss:  1.60075684e-02, mean val. rec. loss:  1.48414789e-02\n",
      "Epoch: 3338 mean train loss:  1.60056559e-02, mean val. rec. loss:  1.48396248e-02\n",
      "Epoch: 3339 mean train loss:  1.60037565e-02, mean val. rec. loss:  1.48377753e-02\n",
      "Epoch: 3340 mean train loss:  1.60018478e-02, mean val. rec. loss:  1.48359292e-02\n",
      "Epoch: 3341 mean train loss:  1.59999539e-02, mean val. rec. loss:  1.48340831e-02\n",
      "Epoch: 3342 mean train loss:  1.59980508e-02, mean val. rec. loss:  1.48322370e-02\n",
      "Epoch: 3343 mean train loss:  1.59961569e-02, mean val. rec. loss:  1.48303931e-02\n",
      "Epoch: 3344 mean train loss:  1.59942631e-02, mean val. rec. loss:  1.48285561e-02\n",
      "Epoch: 3345 mean train loss:  1.59923692e-02, mean val. rec. loss:  1.48267168e-02\n",
      "Epoch: 3346 mean train loss:  1.59904810e-02, mean val. rec. loss:  1.48248843e-02\n",
      "Epoch: 3347 mean train loss:  1.59885927e-02, mean val. rec. loss:  1.48230563e-02\n",
      "Epoch: 3348 mean train loss:  1.59867044e-02, mean val. rec. loss:  1.48212238e-02\n",
      "Epoch: 3349 mean train loss:  1.59848199e-02, mean val. rec. loss:  1.48193992e-02\n",
      "Epoch: 3350 mean train loss:  1.59829391e-02, mean val. rec. loss:  1.48175746e-02\n",
      "Epoch: 3351 mean train loss:  1.59810620e-02, mean val. rec. loss:  1.48157535e-02\n",
      "Epoch: 3352 mean train loss:  1.59791868e-02, mean val. rec. loss:  1.48139323e-02\n",
      "Epoch: 3353 mean train loss:  1.59773115e-02, mean val. rec. loss:  1.48121202e-02\n",
      "Epoch: 3354 mean train loss:  1.59754363e-02, mean val. rec. loss:  1.48103059e-02\n",
      "Epoch: 3355 mean train loss:  1.59735630e-02, mean val. rec. loss:  1.48084938e-02\n",
      "Epoch: 3356 mean train loss:  1.59716952e-02, mean val. rec. loss:  1.48066896e-02\n",
      "Epoch: 3357 mean train loss:  1.59698255e-02, mean val. rec. loss:  1.48048843e-02\n",
      "Epoch: 3358 mean train loss:  1.59679596e-02, mean val. rec. loss:  1.48030802e-02\n",
      "Epoch: 3359 mean train loss:  1.59660937e-02, mean val. rec. loss:  1.48012771e-02\n",
      "Epoch: 3360 mean train loss:  1.59642278e-02, mean val. rec. loss:  1.47994741e-02\n",
      "Epoch: 3361 mean train loss:  1.59623693e-02, mean val. rec. loss:  1.47976892e-02\n",
      "Epoch: 3362 mean train loss:  1.59605146e-02, mean val. rec. loss:  1.47958907e-02\n",
      "Epoch: 3363 mean train loss:  1.59586598e-02, mean val. rec. loss:  1.47940968e-02\n",
      "Epoch: 3364 mean train loss:  1.59568014e-02, mean val. rec. loss:  1.47923119e-02\n",
      "Epoch: 3365 mean train loss:  1.59549504e-02, mean val. rec. loss:  1.47905248e-02\n",
      "Epoch: 3366 mean train loss:  1.59531031e-02, mean val. rec. loss:  1.47887444e-02\n",
      "Epoch: 3367 mean train loss:  1.59512558e-02, mean val. rec. loss:  1.47869573e-02\n",
      "Epoch: 3368 mean train loss:  1.59494048e-02, mean val. rec. loss:  1.47851792e-02\n",
      "Epoch: 3369 mean train loss:  1.59475649e-02, mean val. rec. loss:  1.47834034e-02\n",
      "Epoch: 3370 mean train loss:  1.59457176e-02, mean val. rec. loss:  1.47816299e-02\n",
      "Epoch: 3371 mean train loss:  1.59438834e-02, mean val. rec. loss:  1.47798540e-02\n",
      "Epoch: 3372 mean train loss:  1.59420417e-02, mean val. rec. loss:  1.47780918e-02\n",
      "Epoch: 3373 mean train loss:  1.59402093e-02, mean val. rec. loss:  1.47763251e-02\n",
      "Epoch: 3374 mean train loss:  1.59383694e-02, mean val. rec. loss:  1.47745629e-02\n",
      "Epoch: 3375 mean train loss:  1.59365407e-02, mean val. rec. loss:  1.47728018e-02\n",
      "Epoch: 3376 mean train loss:  1.59347084e-02, mean val. rec. loss:  1.47710408e-02\n",
      "Epoch: 3377 mean train loss:  1.59328760e-02, mean val. rec. loss:  1.47692854e-02\n",
      "Epoch: 3378 mean train loss:  1.59310547e-02, mean val. rec. loss:  1.47675334e-02\n",
      "Epoch: 3379 mean train loss:  1.59292335e-02, mean val. rec. loss:  1.47657791e-02\n",
      "Epoch: 3380 mean train loss:  1.59274048e-02, mean val. rec. loss:  1.47640237e-02\n",
      "Epoch: 3381 mean train loss:  1.59255799e-02, mean val. rec. loss:  1.47622797e-02\n",
      "Epoch: 3382 mean train loss:  1.59237661e-02, mean val. rec. loss:  1.47605356e-02\n",
      "Epoch: 3383 mean train loss:  1.59219486e-02, mean val. rec. loss:  1.47587916e-02\n",
      "Epoch: 3384 mean train loss:  1.59201349e-02, mean val. rec. loss:  1.47570532e-02\n",
      "Epoch: 3385 mean train loss:  1.59183192e-02, mean val. rec. loss:  1.47553182e-02\n",
      "Epoch: 3386 mean train loss:  1.59165110e-02, mean val. rec. loss:  1.47535821e-02\n",
      "Epoch: 3387 mean train loss:  1.59146991e-02, mean val. rec. loss:  1.47518494e-02\n",
      "Epoch: 3388 mean train loss:  1.59128909e-02, mean val. rec. loss:  1.47501189e-02\n",
      "Epoch: 3389 mean train loss:  1.59110846e-02, mean val. rec. loss:  1.47483919e-02\n",
      "Epoch: 3390 mean train loss:  1.59092820e-02, mean val. rec. loss:  1.47466671e-02\n",
      "Epoch: 3391 mean train loss:  1.59074794e-02, mean val. rec. loss:  1.47449412e-02\n",
      "Epoch: 3392 mean train loss:  1.59056768e-02, mean val. rec. loss:  1.47432175e-02\n",
      "Epoch: 3393 mean train loss:  1.59038817e-02, mean val. rec. loss:  1.47415007e-02\n",
      "Epoch: 3394 mean train loss:  1.59020884e-02, mean val. rec. loss:  1.47397850e-02\n",
      "Epoch: 3395 mean train loss:  1.59002932e-02, mean val. rec. loss:  1.47380693e-02\n",
      "Epoch: 3396 mean train loss:  1.58984962e-02, mean val. rec. loss:  1.47363626e-02\n",
      "Epoch: 3397 mean train loss:  1.58967048e-02, mean val. rec. loss:  1.47346526e-02\n",
      "Epoch: 3398 mean train loss:  1.58949208e-02, mean val. rec. loss:  1.47329482e-02\n",
      "Epoch: 3399 mean train loss:  1.58931331e-02, mean val. rec. loss:  1.47312382e-02\n",
      "Epoch: 3400 mean train loss:  1.58913454e-02, mean val. rec. loss:  1.47295372e-02\n",
      "Epoch: 3401 mean train loss:  1.58895614e-02, mean val. rec. loss:  1.47278397e-02\n",
      "Epoch: 3402 mean train loss:  1.58877793e-02, mean val. rec. loss:  1.47261398e-02\n",
      "Epoch: 3403 mean train loss:  1.58860009e-02, mean val. rec. loss:  1.47244434e-02\n",
      "Epoch: 3404 mean train loss:  1.58842225e-02, mean val. rec. loss:  1.47227515e-02\n",
      "Epoch: 3405 mean train loss:  1.58824460e-02, mean val. rec. loss:  1.47210619e-02\n",
      "Epoch: 3406 mean train loss:  1.58806713e-02, mean val. rec. loss:  1.47193745e-02\n",
      "Epoch: 3407 mean train loss:  1.58788985e-02, mean val. rec. loss:  1.47176872e-02\n",
      "Epoch: 3408 mean train loss:  1.58771294e-02, mean val. rec. loss:  1.47160021e-02\n",
      "Epoch: 3409 mean train loss:  1.58753566e-02, mean val. rec. loss:  1.47143136e-02\n",
      "Epoch: 3410 mean train loss:  1.58735875e-02, mean val. rec. loss:  1.47126353e-02\n",
      "Epoch: 3411 mean train loss:  1.58718240e-02, mean val. rec. loss:  1.47109582e-02\n",
      "Epoch: 3412 mean train loss:  1.58700643e-02, mean val. rec. loss:  1.47092833e-02\n",
      "Epoch: 3413 mean train loss:  1.58683008e-02, mean val. rec. loss:  1.47076107e-02\n",
      "Epoch: 3414 mean train loss:  1.58665373e-02, mean val. rec. loss:  1.47059392e-02\n",
      "Epoch: 3415 mean train loss:  1.58647794e-02, mean val. rec. loss:  1.47042711e-02\n",
      "Epoch: 3416 mean train loss:  1.58630271e-02, mean val. rec. loss:  1.47026053e-02\n",
      "Epoch: 3417 mean train loss:  1.58612729e-02, mean val. rec. loss:  1.47009383e-02\n",
      "Epoch: 3418 mean train loss:  1.58595187e-02, mean val. rec. loss:  1.46992759e-02\n",
      "Epoch: 3419 mean train loss:  1.58577664e-02, mean val. rec. loss:  1.46976158e-02\n",
      "Epoch: 3420 mean train loss:  1.58560159e-02, mean val. rec. loss:  1.46959591e-02\n",
      "Epoch: 3421 mean train loss:  1.58542692e-02, mean val. rec. loss:  1.46943035e-02\n",
      "Epoch: 3422 mean train loss:  1.58525224e-02, mean val. rec. loss:  1.46926524e-02\n",
      "Epoch: 3423 mean train loss:  1.58507776e-02, mean val. rec. loss:  1.46910013e-02\n",
      "Epoch: 3424 mean train loss:  1.58490346e-02, mean val. rec. loss:  1.46893514e-02\n",
      "Epoch: 3425 mean train loss:  1.58472915e-02, mean val. rec. loss:  1.46877026e-02\n",
      "Epoch: 3426 mean train loss:  1.58455504e-02, mean val. rec. loss:  1.46860583e-02\n",
      "Epoch: 3427 mean train loss:  1.58438148e-02, mean val. rec. loss:  1.46844186e-02\n",
      "Epoch: 3428 mean train loss:  1.58420830e-02, mean val. rec. loss:  1.46827811e-02\n",
      "Epoch: 3429 mean train loss:  1.58403474e-02, mean val. rec. loss:  1.46811403e-02\n",
      "Epoch: 3430 mean train loss:  1.58386119e-02, mean val. rec. loss:  1.46795085e-02\n",
      "Epoch: 3431 mean train loss:  1.58368800e-02, mean val. rec. loss:  1.46778767e-02\n",
      "Epoch: 3432 mean train loss:  1.58351575e-02, mean val. rec. loss:  1.46762449e-02\n",
      "Epoch: 3433 mean train loss:  1.58334294e-02, mean val. rec. loss:  1.46746176e-02\n",
      "Epoch: 3434 mean train loss:  1.58317013e-02, mean val. rec. loss:  1.46729858e-02\n",
      "Epoch: 3435 mean train loss:  1.58299769e-02, mean val. rec. loss:  1.46713586e-02\n",
      "Epoch: 3436 mean train loss:  1.58282581e-02, mean val. rec. loss:  1.46697358e-02\n",
      "Epoch: 3437 mean train loss:  1.58265337e-02, mean val. rec. loss:  1.46681199e-02\n",
      "Epoch: 3438 mean train loss:  1.58248186e-02, mean val. rec. loss:  1.46664961e-02\n",
      "Epoch: 3439 mean train loss:  1.58230998e-02, mean val. rec. loss:  1.46648802e-02\n",
      "Epoch: 3440 mean train loss:  1.58213866e-02, mean val. rec. loss:  1.46632654e-02\n",
      "Epoch: 3441 mean train loss:  1.58196697e-02, mean val. rec. loss:  1.46616574e-02\n",
      "Epoch: 3442 mean train loss:  1.58179639e-02, mean val. rec. loss:  1.46600472e-02\n",
      "Epoch: 3443 mean train loss:  1.58162470e-02, mean val. rec. loss:  1.46584358e-02\n",
      "Epoch: 3444 mean train loss:  1.58145393e-02, mean val. rec. loss:  1.46568357e-02\n",
      "Epoch: 3445 mean train loss:  1.58128373e-02, mean val. rec. loss:  1.46552255e-02\n",
      "Epoch: 3446 mean train loss:  1.58111334e-02, mean val. rec. loss:  1.46536220e-02\n",
      "Epoch: 3447 mean train loss:  1.58094258e-02, mean val. rec. loss:  1.46520265e-02\n",
      "Epoch: 3448 mean train loss:  1.58077219e-02, mean val. rec. loss:  1.46504310e-02\n",
      "Epoch: 3449 mean train loss:  1.58060235e-02, mean val. rec. loss:  1.46488378e-02\n",
      "Epoch: 3450 mean train loss:  1.58043289e-02, mean val. rec. loss:  1.46472423e-02\n",
      "Epoch: 3451 mean train loss:  1.58026288e-02, mean val. rec. loss:  1.46456513e-02\n",
      "Epoch: 3452 mean train loss:  1.58009323e-02, mean val. rec. loss:  1.46440604e-02\n",
      "Epoch: 3453 mean train loss:  1.57992414e-02, mean val. rec. loss:  1.46424728e-02\n",
      "Epoch: 3454 mean train loss:  1.57975506e-02, mean val. rec. loss:  1.46408829e-02\n",
      "Epoch: 3455 mean train loss:  1.57958597e-02, mean val. rec. loss:  1.46393011e-02\n",
      "Epoch: 3456 mean train loss:  1.57941688e-02, mean val. rec. loss:  1.46377214e-02\n",
      "Epoch: 3457 mean train loss:  1.57924817e-02, mean val. rec. loss:  1.46361429e-02\n",
      "Epoch: 3458 mean train loss:  1.57907983e-02, mean val. rec. loss:  1.46345667e-02\n",
      "Epoch: 3459 mean train loss:  1.57891130e-02, mean val. rec. loss:  1.46329939e-02\n",
      "Epoch: 3460 mean train loss:  1.57874296e-02, mean val. rec. loss:  1.46314188e-02\n",
      "Epoch: 3461 mean train loss:  1.57857480e-02, mean val. rec. loss:  1.46298448e-02\n",
      "Epoch: 3462 mean train loss:  1.57840720e-02, mean val. rec. loss:  1.46282788e-02\n",
      "Epoch: 3463 mean train loss:  1.57823961e-02, mean val. rec. loss:  1.46267116e-02\n",
      "Epoch: 3464 mean train loss:  1.57807182e-02, mean val. rec. loss:  1.46251479e-02\n",
      "Epoch: 3465 mean train loss:  1.57790423e-02, mean val. rec. loss:  1.46235932e-02\n",
      "Epoch: 3466 mean train loss:  1.57773719e-02, mean val. rec. loss:  1.46220295e-02\n",
      "Epoch: 3467 mean train loss:  1.57756996e-02, mean val. rec. loss:  1.46204736e-02\n",
      "Epoch: 3468 mean train loss:  1.57740330e-02, mean val. rec. loss:  1.46189133e-02\n",
      "Epoch: 3469 mean train loss:  1.57723626e-02, mean val. rec. loss:  1.46173643e-02\n",
      "Epoch: 3470 mean train loss:  1.57706978e-02, mean val. rec. loss:  1.46158096e-02\n",
      "Epoch: 3471 mean train loss:  1.57690330e-02, mean val. rec. loss:  1.46142594e-02\n",
      "Epoch: 3472 mean train loss:  1.57673719e-02, mean val. rec. loss:  1.46127014e-02\n",
      "Epoch: 3473 mean train loss:  1.57657071e-02, mean val. rec. loss:  1.46111626e-02\n",
      "Epoch: 3474 mean train loss:  1.57640498e-02, mean val. rec. loss:  1.46096169e-02\n",
      "Epoch: 3475 mean train loss:  1.57623887e-02, mean val. rec. loss:  1.46080747e-02\n",
      "Epoch: 3476 mean train loss:  1.57607351e-02, mean val. rec. loss:  1.46065325e-02\n",
      "Epoch: 3477 mean train loss:  1.57590814e-02, mean val. rec. loss:  1.46049960e-02\n",
      "Epoch: 3478 mean train loss:  1.57574222e-02, mean val. rec. loss:  1.46034561e-02\n",
      "Epoch: 3479 mean train loss:  1.57557723e-02, mean val. rec. loss:  1.46019229e-02\n",
      "Epoch: 3480 mean train loss:  1.57541243e-02, mean val. rec. loss:  1.46003920e-02\n",
      "Epoch: 3481 mean train loss:  1.57524763e-02, mean val. rec. loss:  1.45988521e-02\n",
      "Epoch: 3482 mean train loss:  1.57508245e-02, mean val. rec. loss:  1.45973246e-02\n",
      "Epoch: 3483 mean train loss:  1.57491783e-02, mean val. rec. loss:  1.45958006e-02\n",
      "Epoch: 3484 mean train loss:  1.57475396e-02, mean val. rec. loss:  1.45942731e-02\n",
      "Epoch: 3485 mean train loss:  1.57458934e-02, mean val. rec. loss:  1.45927502e-02\n",
      "Epoch: 3486 mean train loss:  1.57442491e-02, mean val. rec. loss:  1.45912295e-02\n",
      "Epoch: 3487 mean train loss:  1.57426122e-02, mean val. rec. loss:  1.45897111e-02\n",
      "Epoch: 3488 mean train loss:  1.57409735e-02, mean val. rec. loss:  1.45881939e-02\n",
      "Epoch: 3489 mean train loss:  1.57393385e-02, mean val. rec. loss:  1.45866766e-02\n",
      "Epoch: 3490 mean train loss:  1.57377016e-02, mean val. rec. loss:  1.45851639e-02\n",
      "Epoch: 3491 mean train loss:  1.57360685e-02, mean val. rec. loss:  1.45836523e-02\n",
      "Epoch: 3492 mean train loss:  1.57344391e-02, mean val. rec. loss:  1.45821475e-02\n",
      "Epoch: 3493 mean train loss:  1.57328097e-02, mean val. rec. loss:  1.45806382e-02\n",
      "Epoch: 3494 mean train loss:  1.57311802e-02, mean val. rec. loss:  1.45791345e-02\n",
      "Epoch: 3495 mean train loss:  1.57295508e-02, mean val. rec. loss:  1.45776343e-02\n",
      "Epoch: 3496 mean train loss:  1.57279251e-02, mean val. rec. loss:  1.45761283e-02\n",
      "Epoch: 3497 mean train loss:  1.57262994e-02, mean val. rec. loss:  1.45746304e-02\n",
      "Epoch: 3498 mean train loss:  1.57246793e-02, mean val. rec. loss:  1.45731301e-02\n",
      "Epoch: 3499 mean train loss:  1.57230592e-02, mean val. rec. loss:  1.45716321e-02\n",
      "Epoch: 3500 mean train loss:  1.57214373e-02, mean val. rec. loss:  1.45701387e-02\n",
      "Epoch: 3501 mean train loss:  1.57198153e-02, mean val. rec. loss:  1.45686418e-02\n",
      "Epoch: 3502 mean train loss:  1.57182026e-02, mean val. rec. loss:  1.45671529e-02\n",
      "Epoch: 3503 mean train loss:  1.57165900e-02, mean val. rec. loss:  1.45656674e-02\n",
      "Epoch: 3504 mean train loss:  1.57149736e-02, mean val. rec. loss:  1.45641808e-02\n",
      "Epoch: 3505 mean train loss:  1.57133572e-02, mean val. rec. loss:  1.45626941e-02\n",
      "Epoch: 3506 mean train loss:  1.57117520e-02, mean val. rec. loss:  1.45612052e-02\n",
      "Epoch: 3507 mean train loss:  1.57101394e-02, mean val. rec. loss:  1.45597265e-02\n",
      "Epoch: 3508 mean train loss:  1.57085341e-02, mean val. rec. loss:  1.45582387e-02\n",
      "Epoch: 3509 mean train loss:  1.57069196e-02, mean val. rec. loss:  1.45567657e-02\n",
      "Epoch: 3510 mean train loss:  1.57053219e-02, mean val. rec. loss:  1.45552881e-02\n",
      "Epoch: 3511 mean train loss:  1.57037148e-02, mean val. rec. loss:  1.45538106e-02\n",
      "Epoch: 3512 mean train loss:  1.57021170e-02, mean val. rec. loss:  1.45523375e-02\n",
      "Epoch: 3513 mean train loss:  1.57005081e-02, mean val. rec. loss:  1.45508713e-02\n",
      "Epoch: 3514 mean train loss:  1.56989159e-02, mean val. rec. loss:  1.45494005e-02\n",
      "Epoch: 3515 mean train loss:  1.56973126e-02, mean val. rec. loss:  1.45479354e-02\n",
      "Epoch: 3516 mean train loss:  1.56957185e-02, mean val. rec. loss:  1.45464692e-02\n",
      "Epoch: 3517 mean train loss:  1.56941264e-02, mean val. rec. loss:  1.45450030e-02\n",
      "Epoch: 3518 mean train loss:  1.56925305e-02, mean val. rec. loss:  1.45435424e-02\n",
      "Epoch: 3519 mean train loss:  1.56909364e-02, mean val. rec. loss:  1.45420852e-02\n",
      "Epoch: 3520 mean train loss:  1.56893480e-02, mean val. rec. loss:  1.45406269e-02\n",
      "Epoch: 3521 mean train loss:  1.56877595e-02, mean val. rec. loss:  1.45391743e-02\n",
      "Epoch: 3522 mean train loss:  1.56861692e-02, mean val. rec. loss:  1.45377138e-02\n",
      "Epoch: 3523 mean train loss:  1.56845789e-02, mean val. rec. loss:  1.45362634e-02\n",
      "Epoch: 3524 mean train loss:  1.56829998e-02, mean val. rec. loss:  1.45348164e-02\n",
      "Epoch: 3525 mean train loss:  1.56814188e-02, mean val. rec. loss:  1.45333661e-02\n",
      "Epoch: 3526 mean train loss:  1.56798359e-02, mean val. rec. loss:  1.45319146e-02\n",
      "Epoch: 3527 mean train loss:  1.56782531e-02, mean val. rec. loss:  1.45304722e-02\n",
      "Epoch: 3528 mean train loss:  1.56766758e-02, mean val. rec. loss:  1.45290252e-02\n",
      "Epoch: 3529 mean train loss:  1.56750985e-02, mean val. rec. loss:  1.45275805e-02\n",
      "Epoch: 3530 mean train loss:  1.56735175e-02, mean val. rec. loss:  1.45261393e-02\n",
      "Epoch: 3531 mean train loss:  1.56719458e-02, mean val. rec. loss:  1.45246934e-02\n",
      "Epoch: 3532 mean train loss:  1.56703685e-02, mean val. rec. loss:  1.45232601e-02\n",
      "Epoch: 3533 mean train loss:  1.56687987e-02, mean val. rec. loss:  1.45218222e-02\n",
      "Epoch: 3534 mean train loss:  1.56672252e-02, mean val. rec. loss:  1.45203945e-02\n",
      "Epoch: 3535 mean train loss:  1.56656591e-02, mean val. rec. loss:  1.45189578e-02\n",
      "Epoch: 3536 mean train loss:  1.56640837e-02, mean val. rec. loss:  1.45175267e-02\n",
      "Epoch: 3537 mean train loss:  1.56625157e-02, mean val. rec. loss:  1.45160990e-02\n",
      "Epoch: 3538 mean train loss:  1.56609515e-02, mean val. rec. loss:  1.45146691e-02\n",
      "Epoch: 3539 mean train loss:  1.56593872e-02, mean val. rec. loss:  1.45132471e-02\n",
      "Epoch: 3540 mean train loss:  1.56578248e-02, mean val. rec. loss:  1.45118251e-02\n",
      "Epoch: 3541 mean train loss:  1.56562625e-02, mean val. rec. loss:  1.45104042e-02\n",
      "Epoch: 3542 mean train loss:  1.56547038e-02, mean val. rec. loss:  1.45089856e-02\n",
      "Epoch: 3543 mean train loss:  1.56531451e-02, mean val. rec. loss:  1.45075727e-02\n",
      "Epoch: 3544 mean train loss:  1.56515884e-02, mean val. rec. loss:  1.45061575e-02\n",
      "Epoch: 3545 mean train loss:  1.56500316e-02, mean val. rec. loss:  1.45047423e-02\n",
      "Epoch: 3546 mean train loss:  1.56484804e-02, mean val. rec. loss:  1.45033259e-02\n",
      "Epoch: 3547 mean train loss:  1.56469236e-02, mean val. rec. loss:  1.45019141e-02\n",
      "Epoch: 3548 mean train loss:  1.56453742e-02, mean val. rec. loss:  1.45005012e-02\n",
      "Epoch: 3549 mean train loss:  1.56438193e-02, mean val. rec. loss:  1.44990951e-02\n",
      "Epoch: 3550 mean train loss:  1.56422755e-02, mean val. rec. loss:  1.44976855e-02\n",
      "Epoch: 3551 mean train loss:  1.56407225e-02, mean val. rec. loss:  1.44962828e-02\n",
      "Epoch: 3552 mean train loss:  1.56391806e-02, mean val. rec. loss:  1.44948744e-02\n",
      "Epoch: 3553 mean train loss:  1.56376294e-02, mean val. rec. loss:  1.44934751e-02\n",
      "Epoch: 3554 mean train loss:  1.56360875e-02, mean val. rec. loss:  1.44920769e-02\n",
      "Epoch: 3555 mean train loss:  1.56345419e-02, mean val. rec. loss:  1.44906787e-02\n",
      "Epoch: 3556 mean train loss:  1.56330037e-02, mean val. rec. loss:  1.44892805e-02\n",
      "Epoch: 3557 mean train loss:  1.56314655e-02, mean val. rec. loss:  1.44878823e-02\n",
      "Epoch: 3558 mean train loss:  1.56299236e-02, mean val. rec. loss:  1.44864886e-02\n",
      "Epoch: 3559 mean train loss:  1.56283855e-02, mean val. rec. loss:  1.44851006e-02\n",
      "Epoch: 3560 mean train loss:  1.56268529e-02, mean val. rec. loss:  1.44837104e-02\n",
      "Epoch: 3561 mean train loss:  1.56253166e-02, mean val. rec. loss:  1.44823258e-02\n",
      "Epoch: 3562 mean train loss:  1.56237840e-02, mean val. rec. loss:  1.44809310e-02\n",
      "Epoch: 3563 mean train loss:  1.56222477e-02, mean val. rec. loss:  1.44795544e-02\n",
      "Epoch: 3564 mean train loss:  1.56207198e-02, mean val. rec. loss:  1.44781686e-02\n",
      "Epoch: 3565 mean train loss:  1.56191900e-02, mean val. rec. loss:  1.44767875e-02\n",
      "Epoch: 3566 mean train loss:  1.56176648e-02, mean val. rec. loss:  1.44754051e-02\n",
      "Epoch: 3567 mean train loss:  1.56161350e-02, mean val. rec. loss:  1.44740274e-02\n",
      "Epoch: 3568 mean train loss:  1.56146108e-02, mean val. rec. loss:  1.44726519e-02\n",
      "Epoch: 3569 mean train loss:  1.56130885e-02, mean val. rec. loss:  1.44712707e-02\n",
      "Epoch: 3570 mean train loss:  1.56115643e-02, mean val. rec. loss:  1.44698986e-02\n",
      "Epoch: 3571 mean train loss:  1.56100410e-02, mean val. rec. loss:  1.44685276e-02\n",
      "Epoch: 3572 mean train loss:  1.56085215e-02, mean val. rec. loss:  1.44671566e-02\n",
      "Epoch: 3573 mean train loss:  1.56070038e-02, mean val. rec. loss:  1.44657924e-02\n",
      "Epoch: 3574 mean train loss:  1.56054842e-02, mean val. rec. loss:  1.44644180e-02\n",
      "Epoch: 3575 mean train loss:  1.56039675e-02, mean val. rec. loss:  1.44630550e-02\n",
      "Epoch: 3576 mean train loss:  1.56024517e-02, mean val. rec. loss:  1.44616897e-02\n",
      "Epoch: 3577 mean train loss:  1.56009349e-02, mean val. rec. loss:  1.44603233e-02\n",
      "Epoch: 3578 mean train loss:  1.55994191e-02, mean val. rec. loss:  1.44589591e-02\n",
      "Epoch: 3579 mean train loss:  1.55979107e-02, mean val. rec. loss:  1.44575994e-02\n",
      "Epoch: 3580 mean train loss:  1.55964023e-02, mean val. rec. loss:  1.44562421e-02\n",
      "Epoch: 3581 mean train loss:  1.55948902e-02, mean val. rec. loss:  1.44548858e-02\n",
      "Epoch: 3582 mean train loss:  1.55933800e-02, mean val. rec. loss:  1.44535364e-02\n",
      "Epoch: 3583 mean train loss:  1.55918763e-02, mean val. rec. loss:  1.44521858e-02\n",
      "Epoch: 3584 mean train loss:  1.55903744e-02, mean val. rec. loss:  1.44508307e-02\n",
      "Epoch: 3585 mean train loss:  1.55888698e-02, mean val. rec. loss:  1.44494790e-02\n",
      "Epoch: 3586 mean train loss:  1.55873623e-02, mean val. rec. loss:  1.44481273e-02\n",
      "Epoch: 3587 mean train loss:  1.55858623e-02, mean val. rec. loss:  1.44467824e-02\n",
      "Epoch: 3588 mean train loss:  1.55843623e-02, mean val. rec. loss:  1.44454443e-02\n",
      "Epoch: 3589 mean train loss:  1.55828661e-02, mean val. rec. loss:  1.44440949e-02\n",
      "Epoch: 3590 mean train loss:  1.55813661e-02, mean val. rec. loss:  1.44427455e-02\n",
      "Epoch: 3591 mean train loss:  1.55798698e-02, mean val. rec. loss:  1.44414097e-02\n",
      "Epoch: 3592 mean train loss:  1.55783763e-02, mean val. rec. loss:  1.44400750e-02\n",
      "Epoch: 3593 mean train loss:  1.55768810e-02, mean val. rec. loss:  1.44387312e-02\n",
      "Epoch: 3594 mean train loss:  1.55753875e-02, mean val. rec. loss:  1.44373897e-02\n",
      "Epoch: 3595 mean train loss:  1.55738959e-02, mean val. rec. loss:  1.44360596e-02\n",
      "Epoch: 3596 mean train loss:  1.55724062e-02, mean val. rec. loss:  1.44347339e-02\n",
      "Epoch: 3597 mean train loss:  1.55709155e-02, mean val. rec. loss:  1.44333993e-02\n",
      "Epoch: 3598 mean train loss:  1.55694266e-02, mean val. rec. loss:  1.44320668e-02\n",
      "Epoch: 3599 mean train loss:  1.55679453e-02, mean val. rec. loss:  1.44307333e-02\n",
      "Epoch: 3600 mean train loss:  1.55664565e-02, mean val. rec. loss:  1.44294054e-02\n",
      "Epoch: 3601 mean train loss:  1.55649667e-02, mean val. rec. loss:  1.44280843e-02\n",
      "Epoch: 3602 mean train loss:  1.55634872e-02, mean val. rec. loss:  1.44267598e-02\n",
      "Epoch: 3603 mean train loss:  1.55620095e-02, mean val. rec. loss:  1.44254387e-02\n",
      "Epoch: 3604 mean train loss:  1.55605254e-02, mean val. rec. loss:  1.44241097e-02\n",
      "Epoch: 3605 mean train loss:  1.55590412e-02, mean val. rec. loss:  1.44227932e-02\n",
      "Epoch: 3606 mean train loss:  1.55575673e-02, mean val. rec. loss:  1.44214744e-02\n",
      "Epoch: 3607 mean train loss:  1.55560934e-02, mean val. rec. loss:  1.44201544e-02\n",
      "Epoch: 3608 mean train loss:  1.55546148e-02, mean val. rec. loss:  1.44188390e-02\n",
      "Epoch: 3609 mean train loss:  1.55531362e-02, mean val. rec. loss:  1.44175247e-02\n",
      "Epoch: 3610 mean train loss:  1.55516641e-02, mean val. rec. loss:  1.44162127e-02\n",
      "Epoch: 3611 mean train loss:  1.55501902e-02, mean val. rec. loss:  1.44148973e-02\n",
      "Epoch: 3612 mean train loss:  1.55487191e-02, mean val. rec. loss:  1.44135887e-02\n",
      "Epoch: 3613 mean train loss:  1.55472489e-02, mean val. rec. loss:  1.44122778e-02\n",
      "Epoch: 3614 mean train loss:  1.55457787e-02, mean val. rec. loss:  1.44109737e-02\n",
      "Epoch: 3615 mean train loss:  1.55443122e-02, mean val. rec. loss:  1.44096674e-02\n",
      "Epoch: 3616 mean train loss:  1.55428457e-02, mean val. rec. loss:  1.44083644e-02\n",
      "Epoch: 3617 mean train loss:  1.55413802e-02, mean val. rec. loss:  1.44070592e-02\n",
      "Epoch: 3618 mean train loss:  1.55399128e-02, mean val. rec. loss:  1.44057586e-02\n",
      "Epoch: 3619 mean train loss:  1.55384510e-02, mean val. rec. loss:  1.44044624e-02\n",
      "Epoch: 3620 mean train loss:  1.55369910e-02, mean val. rec. loss:  1.44031652e-02\n",
      "Epoch: 3621 mean train loss:  1.55355311e-02, mean val. rec. loss:  1.44018713e-02\n",
      "Epoch: 3622 mean train loss:  1.55340730e-02, mean val. rec. loss:  1.44005729e-02\n",
      "Epoch: 3623 mean train loss:  1.55326111e-02, mean val. rec. loss:  1.43992711e-02\n",
      "Epoch: 3624 mean train loss:  1.55311512e-02, mean val. rec. loss:  1.43979852e-02\n",
      "Epoch: 3625 mean train loss:  1.55296987e-02, mean val. rec. loss:  1.43966947e-02\n",
      "Epoch: 3626 mean train loss:  1.55282443e-02, mean val. rec. loss:  1.43954042e-02\n",
      "Epoch: 3627 mean train loss:  1.55267881e-02, mean val. rec. loss:  1.43941104e-02\n",
      "Epoch: 3628 mean train loss:  1.55253328e-02, mean val. rec. loss:  1.43928210e-02\n",
      "Epoch: 3629 mean train loss:  1.55238812e-02, mean val. rec. loss:  1.43915374e-02\n",
      "Epoch: 3630 mean train loss:  1.55224343e-02, mean val. rec. loss:  1.43902582e-02\n",
      "Epoch: 3631 mean train loss:  1.55209855e-02, mean val. rec. loss:  1.43889723e-02\n",
      "Epoch: 3632 mean train loss:  1.55195320e-02, mean val. rec. loss:  1.43876887e-02\n",
      "Epoch: 3633 mean train loss:  1.55180888e-02, mean val. rec. loss:  1.43864061e-02\n",
      "Epoch: 3634 mean train loss:  1.55166391e-02, mean val. rec. loss:  1.43851304e-02\n",
      "Epoch: 3635 mean train loss:  1.55152015e-02, mean val. rec. loss:  1.43838479e-02\n",
      "Epoch: 3636 mean train loss:  1.55137509e-02, mean val. rec. loss:  1.43825790e-02\n",
      "Epoch: 3637 mean train loss:  1.55123123e-02, mean val. rec. loss:  1.43812930e-02\n",
      "Epoch: 3638 mean train loss:  1.55108663e-02, mean val. rec. loss:  1.43800298e-02\n",
      "Epoch: 3639 mean train loss:  1.55094306e-02, mean val. rec. loss:  1.43787507e-02\n",
      "Epoch: 3640 mean train loss:  1.55079865e-02, mean val. rec. loss:  1.43774897e-02\n",
      "Epoch: 3641 mean train loss:  1.55065535e-02, mean val. rec. loss:  1.43762117e-02\n",
      "Epoch: 3642 mean train loss:  1.55051112e-02, mean val. rec. loss:  1.43749518e-02\n",
      "Epoch: 3643 mean train loss:  1.55036783e-02, mean val. rec. loss:  1.43736795e-02\n",
      "Epoch: 3644 mean train loss:  1.55022407e-02, mean val. rec. loss:  1.43724174e-02\n",
      "Epoch: 3645 mean train loss:  1.55008096e-02, mean val. rec. loss:  1.43711507e-02\n",
      "Epoch: 3646 mean train loss:  1.54993776e-02, mean val. rec. loss:  1.43698875e-02\n",
      "Epoch: 3647 mean train loss:  1.54979409e-02, mean val. rec. loss:  1.43686265e-02\n",
      "Epoch: 3648 mean train loss:  1.54965107e-02, mean val. rec. loss:  1.43673678e-02\n",
      "Epoch: 3649 mean train loss:  1.54950852e-02, mean val. rec. loss:  1.43661091e-02\n",
      "Epoch: 3650 mean train loss:  1.54936569e-02, mean val. rec. loss:  1.43648470e-02\n",
      "Epoch: 3651 mean train loss:  1.54922258e-02, mean val. rec. loss:  1.43635905e-02\n",
      "Epoch: 3652 mean train loss:  1.54908003e-02, mean val. rec. loss:  1.43623397e-02\n",
      "Epoch: 3653 mean train loss:  1.54893767e-02, mean val. rec. loss:  1.43610890e-02\n",
      "Epoch: 3654 mean train loss:  1.54879558e-02, mean val. rec. loss:  1.43598405e-02\n",
      "Epoch: 3655 mean train loss:  1.54865294e-02, mean val. rec. loss:  1.43585817e-02\n",
      "Epoch: 3656 mean train loss:  1.54851048e-02, mean val. rec. loss:  1.43573378e-02\n",
      "Epoch: 3657 mean train loss:  1.54836867e-02, mean val. rec. loss:  1.43560938e-02\n",
      "Epoch: 3658 mean train loss:  1.54822668e-02, mean val. rec. loss:  1.43548419e-02\n",
      "Epoch: 3659 mean train loss:  1.54808478e-02, mean val. rec. loss:  1.43535979e-02\n",
      "Epoch: 3660 mean train loss:  1.54794326e-02, mean val. rec. loss:  1.43523505e-02\n",
      "Epoch: 3661 mean train loss:  1.54780136e-02, mean val. rec. loss:  1.43511088e-02\n",
      "Epoch: 3662 mean train loss:  1.54765965e-02, mean val. rec. loss:  1.43498660e-02\n",
      "Epoch: 3663 mean train loss:  1.54751840e-02, mean val. rec. loss:  1.43486288e-02\n",
      "Epoch: 3664 mean train loss:  1.54737734e-02, mean val. rec. loss:  1.43473905e-02\n",
      "Epoch: 3665 mean train loss:  1.54723590e-02, mean val. rec. loss:  1.43461488e-02\n",
      "Epoch: 3666 mean train loss:  1.54709466e-02, mean val. rec. loss:  1.43449151e-02\n",
      "Epoch: 3667 mean train loss:  1.54695369e-02, mean val. rec. loss:  1.43436813e-02\n",
      "Epoch: 3668 mean train loss:  1.54681291e-02, mean val. rec. loss:  1.43424475e-02\n",
      "Epoch: 3669 mean train loss:  1.54667203e-02, mean val. rec. loss:  1.43412115e-02\n",
      "Epoch: 3670 mean train loss:  1.54653106e-02, mean val. rec. loss:  1.43399857e-02\n",
      "Epoch: 3671 mean train loss:  1.54639075e-02, mean val. rec. loss:  1.43387530e-02\n",
      "Epoch: 3672 mean train loss:  1.54625043e-02, mean val. rec. loss:  1.43375204e-02\n",
      "Epoch: 3673 mean train loss:  1.54610984e-02, mean val. rec. loss:  1.43362923e-02\n",
      "Epoch: 3674 mean train loss:  1.54596915e-02, mean val. rec. loss:  1.43350642e-02\n",
      "Epoch: 3675 mean train loss:  1.54582930e-02, mean val. rec. loss:  1.43338406e-02\n",
      "Epoch: 3676 mean train loss:  1.54568926e-02, mean val. rec. loss:  1.43326205e-02\n",
      "Epoch: 3677 mean train loss:  1.54554904e-02, mean val. rec. loss:  1.43313981e-02\n",
      "Epoch: 3678 mean train loss:  1.54540919e-02, mean val. rec. loss:  1.43301824e-02\n",
      "Epoch: 3679 mean train loss:  1.54526952e-02, mean val. rec. loss:  1.43289589e-02\n",
      "Epoch: 3680 mean train loss:  1.54512986e-02, mean val. rec. loss:  1.43277376e-02\n",
      "Epoch: 3681 mean train loss:  1.54499029e-02, mean val. rec. loss:  1.43265254e-02\n",
      "Epoch: 3682 mean train loss:  1.54485072e-02, mean val. rec. loss:  1.43253109e-02\n",
      "Epoch: 3683 mean train loss:  1.54471161e-02, mean val. rec. loss:  1.43240952e-02\n",
      "Epoch: 3684 mean train loss:  1.54457176e-02, mean val. rec. loss:  1.43228796e-02\n",
      "Epoch: 3685 mean train loss:  1.54443321e-02, mean val. rec. loss:  1.43216583e-02\n",
      "Epoch: 3686 mean train loss:  1.54429355e-02, mean val. rec. loss:  1.43204506e-02\n",
      "Epoch: 3687 mean train loss:  1.54415510e-02, mean val. rec. loss:  1.43192373e-02\n",
      "Epoch: 3688 mean train loss:  1.54401571e-02, mean val. rec. loss:  1.43180319e-02\n",
      "Epoch: 3689 mean train loss:  1.54387735e-02, mean val. rec. loss:  1.43168185e-02\n",
      "Epoch: 3690 mean train loss:  1.54373815e-02, mean val. rec. loss:  1.43156176e-02\n",
      "Epoch: 3691 mean train loss:  1.54360007e-02, mean val. rec. loss:  1.43144111e-02\n",
      "Epoch: 3692 mean train loss:  1.54346115e-02, mean val. rec. loss:  1.43132057e-02\n",
      "Epoch: 3693 mean train loss:  1.54332335e-02, mean val. rec. loss:  1.43120025e-02\n",
      "Epoch: 3694 mean train loss:  1.54318443e-02, mean val. rec. loss:  1.43108028e-02\n",
      "Epoch: 3695 mean train loss:  1.54304681e-02, mean val. rec. loss:  1.43096030e-02\n",
      "Epoch: 3696 mean train loss:  1.54290836e-02, mean val. rec. loss:  1.43084044e-02\n",
      "Epoch: 3697 mean train loss:  1.54277046e-02, mean val. rec. loss:  1.43072058e-02\n",
      "Epoch: 3698 mean train loss:  1.54263285e-02, mean val. rec. loss:  1.43060129e-02\n",
      "Epoch: 3699 mean train loss:  1.54249486e-02, mean val. rec. loss:  1.43048120e-02\n",
      "Epoch: 3700 mean train loss:  1.54235687e-02, mean val. rec. loss:  1.43036202e-02\n",
      "Epoch: 3701 mean train loss:  1.54221944e-02, mean val. rec. loss:  1.43024238e-02\n",
      "Epoch: 3702 mean train loss:  1.54208238e-02, mean val. rec. loss:  1.43012297e-02\n",
      "Epoch: 3703 mean train loss:  1.54194468e-02, mean val. rec. loss:  1.43000413e-02\n",
      "Epoch: 3704 mean train loss:  1.54180715e-02, mean val. rec. loss:  1.42988507e-02\n",
      "Epoch: 3705 mean train loss:  1.54167028e-02, mean val. rec. loss:  1.42976657e-02\n",
      "Epoch: 3706 mean train loss:  1.54153341e-02, mean val. rec. loss:  1.42964795e-02\n",
      "Epoch: 3707 mean train loss:  1.54139598e-02, mean val. rec. loss:  1.42952900e-02\n",
      "Epoch: 3708 mean train loss:  1.54125902e-02, mean val. rec. loss:  1.42941129e-02\n",
      "Epoch: 3709 mean train loss:  1.54112261e-02, mean val. rec. loss:  1.42929268e-02\n",
      "Epoch: 3710 mean train loss:  1.54098565e-02, mean val. rec. loss:  1.42917406e-02\n",
      "Epoch: 3711 mean train loss:  1.54084896e-02, mean val. rec. loss:  1.42905613e-02\n",
      "Epoch: 3712 mean train loss:  1.54071256e-02, mean val. rec. loss:  1.42893808e-02\n",
      "Epoch: 3713 mean train loss:  1.54057615e-02, mean val. rec. loss:  1.42881981e-02\n",
      "Epoch: 3714 mean train loss:  1.54043975e-02, mean val. rec. loss:  1.42870221e-02\n",
      "Epoch: 3715 mean train loss:  1.54030343e-02, mean val. rec. loss:  1.42858417e-02\n",
      "Epoch: 3716 mean train loss:  1.54016721e-02, mean val. rec. loss:  1.42846612e-02\n",
      "Epoch: 3717 mean train loss:  1.54003118e-02, mean val. rec. loss:  1.42834875e-02\n",
      "Epoch: 3718 mean train loss:  1.53989524e-02, mean val. rec. loss:  1.42823173e-02\n",
      "Epoch: 3719 mean train loss:  1.53975940e-02, mean val. rec. loss:  1.42811425e-02\n",
      "Epoch: 3720 mean train loss:  1.53962364e-02, mean val. rec. loss:  1.42799677e-02\n",
      "Epoch: 3721 mean train loss:  1.53948798e-02, mean val. rec. loss:  1.42788020e-02\n",
      "Epoch: 3722 mean train loss:  1.53935241e-02, mean val. rec. loss:  1.42776351e-02\n",
      "Epoch: 3723 mean train loss:  1.53921685e-02, mean val. rec. loss:  1.42764592e-02\n",
      "Epoch: 3724 mean train loss:  1.53908100e-02, mean val. rec. loss:  1.42752980e-02\n",
      "Epoch: 3725 mean train loss:  1.53894599e-02, mean val. rec. loss:  1.42741277e-02\n",
      "Epoch: 3726 mean train loss:  1.53881117e-02, mean val. rec. loss:  1.42729597e-02\n",
      "Epoch: 3727 mean train loss:  1.53867551e-02, mean val. rec. loss:  1.42717962e-02\n",
      "Epoch: 3728 mean train loss:  1.53854022e-02, mean val. rec. loss:  1.42706351e-02\n",
      "Epoch: 3729 mean train loss:  1.53840558e-02, mean val. rec. loss:  1.42694773e-02\n",
      "Epoch: 3730 mean train loss:  1.53827104e-02, mean val. rec. loss:  1.42683172e-02\n",
      "Epoch: 3731 mean train loss:  1.53813603e-02, mean val. rec. loss:  1.42671583e-02\n",
      "Epoch: 3732 mean train loss:  1.53800102e-02, mean val. rec. loss:  1.42660039e-02\n",
      "Epoch: 3733 mean train loss:  1.53786657e-02, mean val. rec. loss:  1.42648427e-02\n",
      "Epoch: 3734 mean train loss:  1.53773240e-02, mean val. rec. loss:  1.42636883e-02\n",
      "Epoch: 3735 mean train loss:  1.53759776e-02, mean val. rec. loss:  1.42625294e-02\n",
      "Epoch: 3736 mean train loss:  1.53746322e-02, mean val. rec. loss:  1.42613761e-02\n",
      "Epoch: 3737 mean train loss:  1.53732933e-02, mean val. rec. loss:  1.42602229e-02\n",
      "Epoch: 3738 mean train loss:  1.53719534e-02, mean val. rec. loss:  1.42590685e-02\n",
      "Epoch: 3739 mean train loss:  1.53706071e-02, mean val. rec. loss:  1.42579186e-02\n",
      "Epoch: 3740 mean train loss:  1.53692747e-02, mean val. rec. loss:  1.42567643e-02\n",
      "Epoch: 3741 mean train loss:  1.53679302e-02, mean val. rec. loss:  1.42556167e-02\n",
      "Epoch: 3742 mean train loss:  1.53665950e-02, mean val. rec. loss:  1.42544623e-02\n",
      "Epoch: 3743 mean train loss:  1.53652551e-02, mean val. rec. loss:  1.42533238e-02\n",
      "Epoch: 3744 mean train loss:  1.53639246e-02, mean val. rec. loss:  1.42521739e-02\n",
      "Epoch: 3745 mean train loss:  1.53625838e-02, mean val. rec. loss:  1.42510320e-02\n",
      "Epoch: 3746 mean train loss:  1.53612524e-02, mean val. rec. loss:  1.42498901e-02\n",
      "Epoch: 3747 mean train loss:  1.53599162e-02, mean val. rec. loss:  1.42487470e-02\n",
      "Epoch: 3748 mean train loss:  1.53585876e-02, mean val. rec. loss:  1.42476017e-02\n",
      "Epoch: 3749 mean train loss:  1.53572524e-02, mean val. rec. loss:  1.42464666e-02\n",
      "Epoch: 3750 mean train loss:  1.53559256e-02, mean val. rec. loss:  1.42453315e-02\n",
      "Epoch: 3751 mean train loss:  1.53545913e-02, mean val. rec. loss:  1.42441930e-02\n",
      "Epoch: 3752 mean train loss:  1.53532673e-02, mean val. rec. loss:  1.42430511e-02\n",
      "Epoch: 3753 mean train loss:  1.53519340e-02, mean val. rec. loss:  1.42419171e-02\n",
      "Epoch: 3754 mean train loss:  1.53506090e-02, mean val. rec. loss:  1.42407888e-02\n",
      "Epoch: 3755 mean train loss:  1.53492859e-02, mean val. rec. loss:  1.42396412e-02\n",
      "Epoch: 3756 mean train loss:  1.53479554e-02, mean val. rec. loss:  1.42385118e-02\n",
      "Epoch: 3757 mean train loss:  1.53466305e-02, mean val. rec. loss:  1.42373801e-02\n",
      "Epoch: 3758 mean train loss:  1.53453102e-02, mean val. rec. loss:  1.42362461e-02\n",
      "Epoch: 3759 mean train loss:  1.53439880e-02, mean val. rec. loss:  1.42351166e-02\n",
      "Epoch: 3760 mean train loss:  1.53426621e-02, mean val. rec. loss:  1.42339872e-02\n",
      "Epoch: 3761 mean train loss:  1.53413400e-02, mean val. rec. loss:  1.42328612e-02\n",
      "Epoch: 3762 mean train loss:  1.53400243e-02, mean val. rec. loss:  1.42317374e-02\n",
      "Epoch: 3763 mean train loss:  1.53387050e-02, mean val. rec. loss:  1.42306045e-02\n",
      "Epoch: 3764 mean train loss:  1.53373828e-02, mean val. rec. loss:  1.42294796e-02\n",
      "Epoch: 3765 mean train loss:  1.53360625e-02, mean val. rec. loss:  1.42283593e-02\n",
      "Epoch: 3766 mean train loss:  1.53347478e-02, mean val. rec. loss:  1.42272378e-02\n",
      "Epoch: 3767 mean train loss:  1.53334350e-02, mean val. rec. loss:  1.42261219e-02\n",
      "Epoch: 3768 mean train loss:  1.53321165e-02, mean val. rec. loss:  1.42249970e-02\n",
      "Epoch: 3769 mean train loss:  1.53307990e-02, mean val. rec. loss:  1.42238744e-02\n",
      "Epoch: 3770 mean train loss:  1.53294871e-02, mean val. rec. loss:  1.42227574e-02\n",
      "Epoch: 3771 mean train loss:  1.53281743e-02, mean val. rec. loss:  1.42216393e-02\n",
      "Epoch: 3772 mean train loss:  1.53268614e-02, mean val. rec. loss:  1.42205246e-02\n",
      "Epoch: 3773 mean train loss:  1.53255523e-02, mean val. rec. loss:  1.42194054e-02\n",
      "Epoch: 3774 mean train loss:  1.53242432e-02, mean val. rec. loss:  1.42182873e-02\n",
      "Epoch: 3775 mean train loss:  1.53229322e-02, mean val. rec. loss:  1.42171703e-02\n",
      "Epoch: 3776 mean train loss:  1.53216222e-02, mean val. rec. loss:  1.42160568e-02\n",
      "Epoch: 3777 mean train loss:  1.53203158e-02, mean val. rec. loss:  1.42149477e-02\n",
      "Epoch: 3778 mean train loss:  1.53190086e-02, mean val. rec. loss:  1.42138342e-02\n",
      "Epoch: 3779 mean train loss:  1.53177032e-02, mean val. rec. loss:  1.42127240e-02\n",
      "Epoch: 3780 mean train loss:  1.53163941e-02, mean val. rec. loss:  1.42116184e-02\n",
      "Epoch: 3781 mean train loss:  1.53150915e-02, mean val. rec. loss:  1.42105082e-02\n",
      "Epoch: 3782 mean train loss:  1.53137889e-02, mean val. rec. loss:  1.42094003e-02\n",
      "Epoch: 3783 mean train loss:  1.53124844e-02, mean val. rec. loss:  1.42082958e-02\n",
      "Epoch: 3784 mean train loss:  1.53111818e-02, mean val. rec. loss:  1.42071959e-02\n",
      "Epoch: 3785 mean train loss:  1.53098801e-02, mean val. rec. loss:  1.42060891e-02\n",
      "Epoch: 3786 mean train loss:  1.53085803e-02, mean val. rec. loss:  1.42049823e-02\n",
      "Epoch: 3787 mean train loss:  1.53072833e-02, mean val. rec. loss:  1.42038858e-02\n",
      "Epoch: 3788 mean train loss:  1.53059863e-02, mean val. rec. loss:  1.42027847e-02\n",
      "Epoch: 3789 mean train loss:  1.53046865e-02, mean val. rec. loss:  1.42016779e-02\n",
      "Epoch: 3790 mean train loss:  1.53033867e-02, mean val. rec. loss:  1.42005848e-02\n",
      "Epoch: 3791 mean train loss:  1.53020924e-02, mean val. rec. loss:  1.41994780e-02\n",
      "Epoch: 3792 mean train loss:  1.53007982e-02, mean val. rec. loss:  1.41983826e-02\n",
      "Epoch: 3793 mean train loss:  1.52994984e-02, mean val. rec. loss:  1.41972826e-02\n",
      "Epoch: 3794 mean train loss:  1.52982033e-02, mean val. rec. loss:  1.41961884e-02\n",
      "Epoch: 3795 mean train loss:  1.52969137e-02, mean val. rec. loss:  1.41950975e-02\n",
      "Epoch: 3796 mean train loss:  1.52956232e-02, mean val. rec. loss:  1.41940009e-02\n",
      "Epoch: 3797 mean train loss:  1.52943271e-02, mean val. rec. loss:  1.41929066e-02\n",
      "Epoch: 3798 mean train loss:  1.52930347e-02, mean val. rec. loss:  1.41918191e-02\n",
      "Epoch: 3799 mean train loss:  1.52917461e-02, mean val. rec. loss:  1.41907226e-02\n",
      "Epoch: 3800 mean train loss:  1.52904612e-02, mean val. rec. loss:  1.41896328e-02\n",
      "Epoch: 3801 mean train loss:  1.52891688e-02, mean val. rec. loss:  1.41885420e-02\n",
      "Epoch: 3802 mean train loss:  1.52878783e-02, mean val. rec. loss:  1.41874545e-02\n",
      "Epoch: 3803 mean train loss:  1.52865953e-02, mean val. rec. loss:  1.41863670e-02\n",
      "Epoch: 3804 mean train loss:  1.52853085e-02, mean val. rec. loss:  1.41852829e-02\n",
      "Epoch: 3805 mean train loss:  1.52840227e-02, mean val. rec. loss:  1.41841943e-02\n",
      "Epoch: 3806 mean train loss:  1.52827387e-02, mean val. rec. loss:  1.41831079e-02\n",
      "Epoch: 3807 mean train loss:  1.52814538e-02, mean val. rec. loss:  1.41820261e-02\n",
      "Epoch: 3808 mean train loss:  1.52801698e-02, mean val. rec. loss:  1.41809488e-02\n",
      "Epoch: 3809 mean train loss:  1.52788886e-02, mean val. rec. loss:  1.41798625e-02\n",
      "Epoch: 3810 mean train loss:  1.52776084e-02, mean val. rec. loss:  1.41787784e-02\n",
      "Epoch: 3811 mean train loss:  1.52763262e-02, mean val. rec. loss:  1.41777000e-02\n",
      "Epoch: 3812 mean train loss:  1.52750451e-02, mean val. rec. loss:  1.41766216e-02\n",
      "Epoch: 3813 mean train loss:  1.52737667e-02, mean val. rec. loss:  1.41755432e-02\n",
      "Epoch: 3814 mean train loss:  1.52724901e-02, mean val. rec. loss:  1.41744602e-02\n",
      "Epoch: 3815 mean train loss:  1.52712108e-02, mean val. rec. loss:  1.41733875e-02\n",
      "Epoch: 3816 mean train loss:  1.52699305e-02, mean val. rec. loss:  1.41723147e-02\n",
      "Epoch: 3817 mean train loss:  1.52686568e-02, mean val. rec. loss:  1.41712329e-02\n",
      "Epoch: 3818 mean train loss:  1.52673793e-02, mean val. rec. loss:  1.41701625e-02\n",
      "Epoch: 3819 mean train loss:  1.52661056e-02, mean val. rec. loss:  1.41690874e-02\n",
      "Epoch: 3820 mean train loss:  1.52648281e-02, mean val. rec. loss:  1.41680204e-02\n",
      "Epoch: 3821 mean train loss:  1.52635581e-02, mean val. rec. loss:  1.41669397e-02\n",
      "Epoch: 3822 mean train loss:  1.52622807e-02, mean val. rec. loss:  1.41658738e-02\n",
      "Epoch: 3823 mean train loss:  1.52610134e-02, mean val. rec. loss:  1.41647976e-02\n",
      "Epoch: 3824 mean train loss:  1.52597416e-02, mean val. rec. loss:  1.41637294e-02\n",
      "Epoch: 3825 mean train loss:  1.52584716e-02, mean val. rec. loss:  1.41626578e-02\n",
      "Epoch: 3826 mean train loss:  1.52571988e-02, mean val. rec. loss:  1.41615930e-02\n",
      "Epoch: 3827 mean train loss:  1.52559297e-02, mean val. rec. loss:  1.41605282e-02\n",
      "Epoch: 3828 mean train loss:  1.52546662e-02, mean val. rec. loss:  1.41594623e-02\n",
      "Epoch: 3829 mean train loss:  1.52533962e-02, mean val. rec. loss:  1.41583941e-02\n",
      "Epoch: 3830 mean train loss:  1.52521261e-02, mean val. rec. loss:  1.41573292e-02\n",
      "Epoch: 3831 mean train loss:  1.52508599e-02, mean val. rec. loss:  1.41562678e-02\n",
      "Epoch: 3832 mean train loss:  1.52495992e-02, mean val. rec. loss:  1.41552042e-02\n",
      "Epoch: 3833 mean train loss:  1.52483310e-02, mean val. rec. loss:  1.41541428e-02\n",
      "Epoch: 3834 mean train loss:  1.52470647e-02, mean val. rec. loss:  1.41530882e-02\n",
      "Epoch: 3835 mean train loss:  1.52458040e-02, mean val. rec. loss:  1.41520256e-02\n",
      "Epoch: 3836 mean train loss:  1.52445442e-02, mean val. rec. loss:  1.41509733e-02\n",
      "Epoch: 3837 mean train loss:  1.52432798e-02, mean val. rec. loss:  1.41499074e-02\n",
      "Epoch: 3838 mean train loss:  1.52420163e-02, mean val. rec. loss:  1.41488573e-02\n",
      "Epoch: 3839 mean train loss:  1.52407575e-02, mean val. rec. loss:  1.41477970e-02\n",
      "Epoch: 3840 mean train loss:  1.52395005e-02, mean val. rec. loss:  1.41467413e-02\n",
      "Epoch: 3841 mean train loss:  1.52382389e-02, mean val. rec. loss:  1.41456810e-02\n",
      "Epoch: 3842 mean train loss:  1.52369809e-02, mean val. rec. loss:  1.41446332e-02\n",
      "Epoch: 3843 mean train loss:  1.52357240e-02, mean val. rec. loss:  1.41435707e-02\n",
      "Epoch: 3844 mean train loss:  1.52344670e-02, mean val. rec. loss:  1.41425229e-02\n",
      "Epoch: 3845 mean train loss:  1.52332119e-02, mean val. rec. loss:  1.41414672e-02\n",
      "Epoch: 3846 mean train loss:  1.52319549e-02, mean val. rec. loss:  1.41404137e-02\n",
      "Epoch: 3847 mean train loss:  1.52306988e-02, mean val. rec. loss:  1.41393682e-02\n",
      "Epoch: 3848 mean train loss:  1.52294465e-02, mean val. rec. loss:  1.41383147e-02\n",
      "Epoch: 3849 mean train loss:  1.52281951e-02, mean val. rec. loss:  1.41372658e-02\n",
      "Epoch: 3850 mean train loss:  1.52269409e-02, mean val. rec. loss:  1.41362157e-02\n",
      "Epoch: 3851 mean train loss:  1.52256868e-02, mean val. rec. loss:  1.41351679e-02\n",
      "Epoch: 3852 mean train loss:  1.52244382e-02, mean val. rec. loss:  1.41341270e-02\n",
      "Epoch: 3853 mean train loss:  1.52231896e-02, mean val. rec. loss:  1.41330814e-02\n",
      "Epoch: 3854 mean train loss:  1.52219363e-02, mean val. rec. loss:  1.41320382e-02\n",
      "Epoch: 3855 mean train loss:  1.52206886e-02, mean val. rec. loss:  1.41309949e-02\n",
      "Epoch: 3856 mean train loss:  1.52194382e-02, mean val. rec. loss:  1.41299539e-02\n",
      "Epoch: 3857 mean train loss:  1.52181933e-02, mean val. rec. loss:  1.41289107e-02\n",
      "Epoch: 3858 mean train loss:  1.52169428e-02, mean val. rec. loss:  1.41278719e-02\n",
      "Epoch: 3859 mean train loss:  1.52156980e-02, mean val. rec. loss:  1.41268309e-02\n",
      "Epoch: 3860 mean train loss:  1.52144494e-02, mean val. rec. loss:  1.41257934e-02\n",
      "Epoch: 3861 mean train loss:  1.52132064e-02, mean val. rec. loss:  1.41247535e-02\n",
      "Epoch: 3862 mean train loss:  1.52119624e-02, mean val. rec. loss:  1.41237148e-02\n",
      "Epoch: 3863 mean train loss:  1.52107166e-02, mean val. rec. loss:  1.41226749e-02\n",
      "Epoch: 3864 mean train loss:  1.52094717e-02, mean val. rec. loss:  1.41216362e-02\n",
      "Epoch: 3865 mean train loss:  1.52082306e-02, mean val. rec. loss:  1.41205986e-02\n",
      "Epoch: 3866 mean train loss:  1.52069904e-02, mean val. rec. loss:  1.41195644e-02\n",
      "Epoch: 3867 mean train loss:  1.52057474e-02, mean val. rec. loss:  1.41185257e-02\n",
      "Epoch: 3868 mean train loss:  1.52045062e-02, mean val. rec. loss:  1.41174949e-02\n",
      "Epoch: 3869 mean train loss:  1.52032669e-02, mean val. rec. loss:  1.41164596e-02\n",
      "Epoch: 3870 mean train loss:  1.52020295e-02, mean val. rec. loss:  1.41154311e-02\n",
      "Epoch: 3871 mean train loss:  1.52007893e-02, mean val. rec. loss:  1.41143958e-02\n",
      "Epoch: 3872 mean train loss:  1.51995500e-02, mean val. rec. loss:  1.41133729e-02\n",
      "Epoch: 3873 mean train loss:  1.51983163e-02, mean val. rec. loss:  1.41123387e-02\n",
      "Epoch: 3874 mean train loss:  1.51970817e-02, mean val. rec. loss:  1.41113113e-02\n",
      "Epoch: 3875 mean train loss:  1.51958414e-02, mean val. rec. loss:  1.41102794e-02\n",
      "Epoch: 3876 mean train loss:  1.51946049e-02, mean val. rec. loss:  1.41092520e-02\n",
      "Epoch: 3877 mean train loss:  1.51933740e-02, mean val. rec. loss:  1.41082281e-02\n",
      "Epoch: 3878 mean train loss:  1.51921422e-02, mean val. rec. loss:  1.41072063e-02\n",
      "Epoch: 3879 mean train loss:  1.51909066e-02, mean val. rec. loss:  1.41061790e-02\n",
      "Epoch: 3880 mean train loss:  1.51896720e-02, mean val. rec. loss:  1.41051618e-02\n",
      "Epoch: 3881 mean train loss:  1.51884411e-02, mean val. rec. loss:  1.41041367e-02\n",
      "Epoch: 3882 mean train loss:  1.51872120e-02, mean val. rec. loss:  1.41031127e-02\n",
      "Epoch: 3883 mean train loss:  1.51859783e-02, mean val. rec. loss:  1.41020830e-02\n",
      "Epoch: 3884 mean train loss:  1.51847456e-02, mean val. rec. loss:  1.41010659e-02\n",
      "Epoch: 3885 mean train loss:  1.51835193e-02, mean val. rec. loss:  1.41000419e-02\n",
      "Epoch: 3886 mean train loss:  1.51822912e-02, mean val. rec. loss:  1.40990190e-02\n",
      "Epoch: 3887 mean train loss:  1.51810612e-02, mean val. rec. loss:  1.40979917e-02\n",
      "Epoch: 3888 mean train loss:  1.51798322e-02, mean val. rec. loss:  1.40969756e-02\n",
      "Epoch: 3889 mean train loss:  1.51786069e-02, mean val. rec. loss:  1.40959618e-02\n",
      "Epoch: 3890 mean train loss:  1.51773825e-02, mean val. rec. loss:  1.40949401e-02\n",
      "Epoch: 3891 mean train loss:  1.51761562e-02, mean val. rec. loss:  1.40939161e-02\n",
      "Epoch: 3892 mean train loss:  1.51749290e-02, mean val. rec. loss:  1.40929046e-02\n",
      "Epoch: 3893 mean train loss:  1.51737084e-02, mean val. rec. loss:  1.40918920e-02\n",
      "Epoch: 3894 mean train loss:  1.51724803e-02, mean val. rec. loss:  1.40908748e-02\n",
      "Epoch: 3895 mean train loss:  1.51712605e-02, mean val. rec. loss:  1.40898565e-02\n",
      "Epoch: 3896 mean train loss:  1.51700352e-02, mean val. rec. loss:  1.40888495e-02\n",
      "Epoch: 3897 mean train loss:  1.51688164e-02, mean val. rec. loss:  1.40878392e-02\n",
      "Epoch: 3898 mean train loss:  1.51675911e-02, mean val. rec. loss:  1.40868254e-02\n",
      "Epoch: 3899 mean train loss:  1.51663723e-02, mean val. rec. loss:  1.40858127e-02\n",
      "Epoch: 3900 mean train loss:  1.51651497e-02, mean val. rec. loss:  1.40848069e-02\n",
      "Epoch: 3901 mean train loss:  1.51639328e-02, mean val. rec. loss:  1.40837999e-02\n",
      "Epoch: 3902 mean train loss:  1.51627093e-02, mean val. rec. loss:  1.40827896e-02\n",
      "Epoch: 3903 mean train loss:  1.51614933e-02, mean val. rec. loss:  1.40817781e-02\n",
      "Epoch: 3904 mean train loss:  1.51602773e-02, mean val. rec. loss:  1.40807711e-02\n",
      "Epoch: 3905 mean train loss:  1.51590594e-02, mean val. rec. loss:  1.40797664e-02\n",
      "Epoch: 3906 mean train loss:  1.51578406e-02, mean val. rec. loss:  1.40787594e-02\n",
      "Epoch: 3907 mean train loss:  1.51566274e-02, mean val. rec. loss:  1.40777524e-02\n",
      "Epoch: 3908 mean train loss:  1.51554133e-02, mean val. rec. loss:  1.40767455e-02\n",
      "Epoch: 3909 mean train loss:  1.51541935e-02, mean val. rec. loss:  1.40757385e-02\n",
      "Epoch: 3910 mean train loss:  1.51529813e-02, mean val. rec. loss:  1.40747338e-02\n",
      "Epoch: 3911 mean train loss:  1.51517708e-02, mean val. rec. loss:  1.40737336e-02\n",
      "Epoch: 3912 mean train loss:  1.51505567e-02, mean val. rec. loss:  1.40727244e-02\n",
      "Epoch: 3913 mean train loss:  1.51493435e-02, mean val. rec. loss:  1.40717231e-02\n",
      "Epoch: 3914 mean train loss:  1.51481330e-02, mean val. rec. loss:  1.40707229e-02\n",
      "Epoch: 3915 mean train loss:  1.51469245e-02, mean val. rec. loss:  1.40697171e-02\n",
      "Epoch: 3916 mean train loss:  1.51457113e-02, mean val. rec. loss:  1.40687192e-02\n",
      "Epoch: 3917 mean train loss:  1.51444999e-02, mean val. rec. loss:  1.40677213e-02\n",
      "Epoch: 3918 mean train loss:  1.51432923e-02, mean val. rec. loss:  1.40667245e-02\n",
      "Epoch: 3919 mean train loss:  1.51420865e-02, mean val. rec. loss:  1.40657323e-02\n",
      "Epoch: 3920 mean train loss:  1.51408752e-02, mean val. rec. loss:  1.40647287e-02\n",
      "Epoch: 3921 mean train loss:  1.51396666e-02, mean val. rec. loss:  1.40637365e-02\n",
      "Epoch: 3922 mean train loss:  1.51384618e-02, mean val. rec. loss:  1.40627420e-02\n",
      "Epoch: 3923 mean train loss:  1.51372569e-02, mean val. rec. loss:  1.40617430e-02\n",
      "Epoch: 3924 mean train loss:  1.51360474e-02, mean val. rec. loss:  1.40607473e-02\n",
      "Epoch: 3925 mean train loss:  1.51348379e-02, mean val. rec. loss:  1.40597540e-02\n",
      "Epoch: 3926 mean train loss:  1.51336368e-02, mean val. rec. loss:  1.40587629e-02\n",
      "Epoch: 3927 mean train loss:  1.51324376e-02, mean val. rec. loss:  1.40577740e-02\n",
      "Epoch: 3928 mean train loss:  1.51312299e-02, mean val. rec. loss:  1.40567750e-02\n",
      "Epoch: 3929 mean train loss:  1.51300223e-02, mean val. rec. loss:  1.40557850e-02\n",
      "Epoch: 3930 mean train loss:  1.51288249e-02, mean val. rec. loss:  1.40547939e-02\n",
      "Epoch: 3931 mean train loss:  1.51276257e-02, mean val. rec. loss:  1.40538006e-02\n",
      "Epoch: 3932 mean train loss:  1.51264208e-02, mean val. rec. loss:  1.40528140e-02\n",
      "Epoch: 3933 mean train loss:  1.51252141e-02, mean val. rec. loss:  1.40518297e-02\n",
      "Epoch: 3934 mean train loss:  1.51240205e-02, mean val. rec. loss:  1.40508386e-02\n",
      "Epoch: 3935 mean train loss:  1.51228203e-02, mean val. rec. loss:  1.40498498e-02\n",
      "Epoch: 3936 mean train loss:  1.51216238e-02, mean val. rec. loss:  1.40488587e-02\n",
      "Epoch: 3937 mean train loss:  1.51204227e-02, mean val. rec. loss:  1.40478733e-02\n",
      "Epoch: 3938 mean train loss:  1.51192272e-02, mean val. rec. loss:  1.40468845e-02\n",
      "Epoch: 3939 mean train loss:  1.51180261e-02, mean val. rec. loss:  1.40459013e-02\n",
      "Epoch: 3940 mean train loss:  1.51168324e-02, mean val. rec. loss:  1.40449147e-02\n",
      "Epoch: 3941 mean train loss:  1.51156332e-02, mean val. rec. loss:  1.40439350e-02\n",
      "Epoch: 3942 mean train loss:  1.51144395e-02, mean val. rec. loss:  1.40429507e-02\n",
      "Epoch: 3943 mean train loss:  1.51132421e-02, mean val. rec. loss:  1.40419664e-02\n",
      "Epoch: 3944 mean train loss:  1.51120485e-02, mean val. rec. loss:  1.40409889e-02\n",
      "Epoch: 3945 mean train loss:  1.51108529e-02, mean val. rec. loss:  1.40400024e-02\n",
      "Epoch: 3946 mean train loss:  1.51096574e-02, mean val. rec. loss:  1.40390215e-02\n",
      "Epoch: 3947 mean train loss:  1.51084675e-02, mean val. rec. loss:  1.40380406e-02\n",
      "Epoch: 3948 mean train loss:  1.51072757e-02, mean val. rec. loss:  1.40370586e-02\n",
      "Epoch: 3949 mean train loss:  1.51060811e-02, mean val. rec. loss:  1.40360765e-02\n",
      "Epoch: 3950 mean train loss:  1.51048893e-02, mean val. rec. loss:  1.40351013e-02\n",
      "Epoch: 3951 mean train loss:  1.51036984e-02, mean val. rec. loss:  1.40341238e-02\n",
      "Epoch: 3952 mean train loss:  1.51025103e-02, mean val. rec. loss:  1.40331509e-02\n",
      "Epoch: 3953 mean train loss:  1.51013185e-02, mean val. rec. loss:  1.40321677e-02\n",
      "Epoch: 3954 mean train loss:  1.51001286e-02, mean val. rec. loss:  1.40311936e-02\n",
      "Epoch: 3955 mean train loss:  1.50989405e-02, mean val. rec. loss:  1.40302161e-02\n",
      "Epoch: 3956 mean train loss:  1.50977543e-02, mean val. rec. loss:  1.40292432e-02\n",
      "Epoch: 3957 mean train loss:  1.50965662e-02, mean val. rec. loss:  1.40282680e-02\n",
      "Epoch: 3958 mean train loss:  1.50953791e-02, mean val. rec. loss:  1.40272950e-02\n",
      "Epoch: 3959 mean train loss:  1.50941919e-02, mean val. rec. loss:  1.40263198e-02\n",
      "Epoch: 3960 mean train loss:  1.50930057e-02, mean val. rec. loss:  1.40253480e-02\n",
      "Epoch: 3961 mean train loss:  1.50918204e-02, mean val. rec. loss:  1.40243762e-02\n",
      "Epoch: 3962 mean train loss:  1.50906351e-02, mean val. rec. loss:  1.40234043e-02\n",
      "Epoch: 3963 mean train loss:  1.50894498e-02, mean val. rec. loss:  1.40224348e-02\n",
      "Epoch: 3964 mean train loss:  1.50882664e-02, mean val. rec. loss:  1.40214630e-02\n",
      "Epoch: 3965 mean train loss:  1.50870802e-02, mean val. rec. loss:  1.40204912e-02\n",
      "Epoch: 3966 mean train loss:  1.50858949e-02, mean val. rec. loss:  1.40195250e-02\n",
      "Epoch: 3967 mean train loss:  1.50847143e-02, mean val. rec. loss:  1.40185498e-02\n",
      "Epoch: 3968 mean train loss:  1.50835327e-02, mean val. rec. loss:  1.40175791e-02\n",
      "Epoch: 3969 mean train loss:  1.50823474e-02, mean val. rec. loss:  1.40166107e-02\n",
      "Epoch: 3970 mean train loss:  1.50811640e-02, mean val. rec. loss:  1.40156434e-02\n",
      "Epoch: 3971 mean train loss:  1.50799890e-02, mean val. rec. loss:  1.40146750e-02\n",
      "Epoch: 3972 mean train loss:  1.50788093e-02, mean val. rec. loss:  1.40137054e-02\n",
      "Epoch: 3973 mean train loss:  1.50776259e-02, mean val. rec. loss:  1.40127393e-02\n",
      "Epoch: 3974 mean train loss:  1.50764452e-02, mean val. rec. loss:  1.40117811e-02\n",
      "Epoch: 3975 mean train loss:  1.50752702e-02, mean val. rec. loss:  1.40108115e-02\n",
      "Epoch: 3976 mean train loss:  1.50740924e-02, mean val. rec. loss:  1.40098477e-02\n",
      "Epoch: 3977 mean train loss:  1.50729117e-02, mean val. rec. loss:  1.40088872e-02\n",
      "Epoch: 3978 mean train loss:  1.50717330e-02, mean val. rec. loss:  1.40079210e-02\n",
      "Epoch: 3979 mean train loss:  1.50705607e-02, mean val. rec. loss:  1.40069583e-02\n",
      "Epoch: 3980 mean train loss:  1.50693838e-02, mean val. rec. loss:  1.40059967e-02\n",
      "Epoch: 3981 mean train loss:  1.50682088e-02, mean val. rec. loss:  1.40050294e-02\n",
      "Epoch: 3982 mean train loss:  1.50670281e-02, mean val. rec. loss:  1.40040768e-02\n",
      "Epoch: 3983 mean train loss:  1.50658587e-02, mean val. rec. loss:  1.40031118e-02\n",
      "Epoch: 3984 mean train loss:  1.50646836e-02, mean val. rec. loss:  1.40021548e-02\n",
      "Epoch: 3985 mean train loss:  1.50635105e-02, mean val. rec. loss:  1.40011897e-02\n",
      "Epoch: 3986 mean train loss:  1.50623317e-02, mean val. rec. loss:  1.40002338e-02\n",
      "Epoch: 3987 mean train loss:  1.50611650e-02, mean val. rec. loss:  1.39992756e-02\n",
      "Epoch: 3988 mean train loss:  1.50599928e-02, mean val. rec. loss:  1.39983208e-02\n",
      "Epoch: 3989 mean train loss:  1.50588215e-02, mean val. rec. loss:  1.39973558e-02\n",
      "Epoch: 3990 mean train loss:  1.50576446e-02, mean val. rec. loss:  1.39964123e-02\n",
      "Epoch: 3991 mean train loss:  1.50564788e-02, mean val. rec. loss:  1.39954507e-02\n",
      "Epoch: 3992 mean train loss:  1.50553103e-02, mean val. rec. loss:  1.39944959e-02\n",
      "Epoch: 3993 mean train loss:  1.50541362e-02, mean val. rec. loss:  1.39935399e-02\n",
      "Epoch: 3994 mean train loss:  1.50529658e-02, mean val. rec. loss:  1.39925863e-02\n",
      "Epoch: 3995 mean train loss:  1.50518001e-02, mean val. rec. loss:  1.39916383e-02\n",
      "Epoch: 3996 mean train loss:  1.50506334e-02, mean val. rec. loss:  1.39906812e-02\n",
      "Epoch: 3997 mean train loss:  1.50494602e-02, mean val. rec. loss:  1.39897264e-02\n",
      "Epoch: 3998 mean train loss:  1.50482926e-02, mean val. rec. loss:  1.39887818e-02\n",
      "Epoch: 3999 mean train loss:  1.50471288e-02, mean val. rec. loss:  1.39878258e-02\n",
      "Epoch: 4000 mean train loss:  1.50459630e-02, mean val. rec. loss:  1.39868733e-02\n",
      "Epoch: 4001 mean train loss:  1.50447917e-02, mean val. rec. loss:  1.39859264e-02\n",
      "Epoch: 4002 mean train loss:  1.50436278e-02, mean val. rec. loss:  1.39849716e-02\n",
      "Epoch: 4003 mean train loss:  1.50424649e-02, mean val. rec. loss:  1.39840168e-02\n",
      "Epoch: 4004 mean train loss:  1.50413001e-02, mean val. rec. loss:  1.39830677e-02\n",
      "Epoch: 4005 mean train loss:  1.50401353e-02, mean val. rec. loss:  1.39821197e-02\n",
      "Epoch: 4006 mean train loss:  1.50389714e-02, mean val. rec. loss:  1.39811660e-02\n",
      "Epoch: 4007 mean train loss:  1.50378085e-02, mean val. rec. loss:  1.39802146e-02\n",
      "Epoch: 4008 mean train loss:  1.50366465e-02, mean val. rec. loss:  1.39792734e-02\n",
      "Epoch: 4009 mean train loss:  1.50354836e-02, mean val. rec. loss:  1.39783288e-02\n",
      "Epoch: 4010 mean train loss:  1.50343206e-02, mean val. rec. loss:  1.39773774e-02\n",
      "Epoch: 4011 mean train loss:  1.50331595e-02, mean val. rec. loss:  1.39764271e-02\n",
      "Epoch: 4012 mean train loss:  1.50319985e-02, mean val. rec. loss:  1.39754848e-02\n",
      "Epoch: 4013 mean train loss:  1.50308374e-02, mean val. rec. loss:  1.39745436e-02\n",
      "Epoch: 4014 mean train loss:  1.50296763e-02, mean val. rec. loss:  1.39735967e-02\n",
      "Epoch: 4015 mean train loss:  1.50285162e-02, mean val. rec. loss:  1.39726476e-02\n",
      "Epoch: 4016 mean train loss:  1.50273570e-02, mean val. rec. loss:  1.39717075e-02\n",
      "Epoch: 4017 mean train loss:  1.50261959e-02, mean val. rec. loss:  1.39707651e-02\n",
      "Epoch: 4018 mean train loss:  1.50250367e-02, mean val. rec. loss:  1.39698205e-02\n",
      "Epoch: 4019 mean train loss:  1.50238793e-02, mean val. rec. loss:  1.39688748e-02\n",
      "Epoch: 4020 mean train loss:  1.50227238e-02, mean val. rec. loss:  1.39679336e-02\n",
      "Epoch: 4021 mean train loss:  1.50215655e-02, mean val. rec. loss:  1.39669924e-02\n",
      "Epoch: 4022 mean train loss:  1.50204063e-02, mean val. rec. loss:  1.39660478e-02\n",
      "Epoch: 4023 mean train loss:  1.50192518e-02, mean val. rec. loss:  1.39651100e-02\n",
      "Epoch: 4024 mean train loss:  1.50180972e-02, mean val. rec. loss:  1.39641688e-02\n",
      "Epoch: 4025 mean train loss:  1.50169389e-02, mean val. rec. loss:  1.39632299e-02\n",
      "Epoch: 4026 mean train loss:  1.50157825e-02, mean val. rec. loss:  1.39622909e-02\n",
      "Epoch: 4027 mean train loss:  1.50146298e-02, mean val. rec. loss:  1.39613475e-02\n",
      "Epoch: 4028 mean train loss:  1.50134762e-02, mean val. rec. loss:  1.39604131e-02\n",
      "Epoch: 4029 mean train loss:  1.50123207e-02, mean val. rec. loss:  1.39594809e-02\n",
      "Epoch: 4030 mean train loss:  1.50111671e-02, mean val. rec. loss:  1.39585375e-02\n",
      "Epoch: 4031 mean train loss:  1.50100134e-02, mean val. rec. loss:  1.39575963e-02\n",
      "Epoch: 4032 mean train loss:  1.50088617e-02, mean val. rec. loss:  1.39566619e-02\n",
      "Epoch: 4033 mean train loss:  1.50077099e-02, mean val. rec. loss:  1.39557309e-02\n",
      "Epoch: 4034 mean train loss:  1.50065572e-02, mean val. rec. loss:  1.39547908e-02\n",
      "Epoch: 4035 mean train loss:  1.50054064e-02, mean val. rec. loss:  1.39538519e-02\n",
      "Epoch: 4036 mean train loss:  1.50042556e-02, mean val. rec. loss:  1.39529209e-02\n",
      "Epoch: 4037 mean train loss:  1.50031047e-02, mean val. rec. loss:  1.39519944e-02\n",
      "Epoch: 4038 mean train loss:  1.50019539e-02, mean val. rec. loss:  1.39510544e-02\n",
      "Epoch: 4039 mean train loss:  1.50008049e-02, mean val. rec. loss:  1.39501166e-02\n",
      "Epoch: 4040 mean train loss:  1.49996550e-02, mean val. rec. loss:  1.39491924e-02\n",
      "Epoch: 4041 mean train loss:  1.49985070e-02, mean val. rec. loss:  1.39482659e-02\n",
      "Epoch: 4042 mean train loss:  1.49973580e-02, mean val. rec. loss:  1.39473247e-02\n",
      "Epoch: 4043 mean train loss:  1.49962081e-02, mean val. rec. loss:  1.39463937e-02\n",
      "Epoch: 4044 mean train loss:  1.49950638e-02, mean val. rec. loss:  1.39454571e-02\n",
      "Epoch: 4045 mean train loss:  1.49939167e-02, mean val. rec. loss:  1.39445227e-02\n",
      "Epoch: 4046 mean train loss:  1.49927658e-02, mean val. rec. loss:  1.39435894e-02\n",
      "Epoch: 4047 mean train loss:  1.49916197e-02, mean val. rec. loss:  1.39426595e-02\n",
      "Epoch: 4048 mean train loss:  1.49904772e-02, mean val. rec. loss:  1.39417319e-02\n",
      "Epoch: 4049 mean train loss:  1.49893320e-02, mean val. rec. loss:  1.39407975e-02\n",
      "Epoch: 4050 mean train loss:  1.49881830e-02, mean val. rec. loss:  1.39398666e-02\n",
      "Epoch: 4051 mean train loss:  1.49870387e-02, mean val. rec. loss:  1.39389424e-02\n",
      "Epoch: 4052 mean train loss:  1.49858981e-02, mean val. rec. loss:  1.39380114e-02\n",
      "Epoch: 4053 mean train loss:  1.49847528e-02, mean val. rec. loss:  1.39370804e-02\n",
      "Epoch: 4054 mean train loss:  1.49836039e-02, mean val. rec. loss:  1.39361550e-02\n",
      "Epoch: 4055 mean train loss:  1.49824623e-02, mean val. rec. loss:  1.39352309e-02\n",
      "Epoch: 4056 mean train loss:  1.49813227e-02, mean val. rec. loss:  1.39343055e-02\n",
      "Epoch: 4057 mean train loss:  1.49801774e-02, mean val. rec. loss:  1.39333723e-02\n",
      "Epoch: 4058 mean train loss:  1.49790322e-02, mean val. rec. loss:  1.39324469e-02\n",
      "Epoch: 4059 mean train loss:  1.49778925e-02, mean val. rec. loss:  1.39315216e-02\n",
      "Epoch: 4060 mean train loss:  1.49767538e-02, mean val. rec. loss:  1.39305929e-02\n",
      "Epoch: 4061 mean train loss:  1.49756104e-02, mean val. rec. loss:  1.39296676e-02\n",
      "Epoch: 4062 mean train loss:  1.49744680e-02, mean val. rec. loss:  1.39287479e-02\n",
      "Epoch: 4063 mean train loss:  1.49733283e-02, mean val. rec. loss:  1.39278226e-02\n",
      "Epoch: 4064 mean train loss:  1.49721914e-02, mean val. rec. loss:  1.39269018e-02\n",
      "Epoch: 4065 mean train loss:  1.49710499e-02, mean val. rec. loss:  1.39259719e-02\n",
      "Epoch: 4066 mean train loss:  1.49699074e-02, mean val. rec. loss:  1.39250512e-02\n",
      "Epoch: 4067 mean train loss:  1.49687715e-02, mean val. rec. loss:  1.39241292e-02\n",
      "Epoch: 4068 mean train loss:  1.49676356e-02, mean val. rec. loss:  1.39232028e-02\n",
      "Epoch: 4069 mean train loss:  1.49664941e-02, mean val. rec. loss:  1.39222797e-02\n",
      "Epoch: 4070 mean train loss:  1.49653544e-02, mean val. rec. loss:  1.39213589e-02\n",
      "Epoch: 4071 mean train loss:  1.49642175e-02, mean val. rec. loss:  1.39204381e-02\n",
      "Epoch: 4072 mean train loss:  1.49630844e-02, mean val. rec. loss:  1.39195207e-02\n",
      "Epoch: 4073 mean train loss:  1.49619466e-02, mean val. rec. loss:  1.39185966e-02\n",
      "Epoch: 4074 mean train loss:  1.49608079e-02, mean val. rec. loss:  1.39176758e-02\n",
      "Epoch: 4075 mean train loss:  1.49596728e-02, mean val. rec. loss:  1.39167595e-02\n",
      "Epoch: 4076 mean train loss:  1.49585406e-02, mean val. rec. loss:  1.39158342e-02\n",
      "Epoch: 4077 mean train loss:  1.49574047e-02, mean val. rec. loss:  1.39149179e-02\n",
      "Epoch: 4078 mean train loss:  1.49562660e-02, mean val. rec. loss:  1.39139994e-02\n",
      "Epoch: 4079 mean train loss:  1.49551319e-02, mean val. rec. loss:  1.39130843e-02\n",
      "Epoch: 4080 mean train loss:  1.49540025e-02, mean val. rec. loss:  1.39121714e-02\n",
      "Epoch: 4081 mean train loss:  1.49528684e-02, mean val. rec. loss:  1.39112507e-02\n",
      "Epoch: 4082 mean train loss:  1.49517306e-02, mean val. rec. loss:  1.39103344e-02\n",
      "Epoch: 4083 mean train loss:  1.49505984e-02, mean val. rec. loss:  1.39094227e-02\n",
      "Epoch: 4084 mean train loss:  1.49494690e-02, mean val. rec. loss:  1.39085053e-02\n",
      "Epoch: 4085 mean train loss:  1.49483358e-02, mean val. rec. loss:  1.39075879e-02\n",
      "Epoch: 4086 mean train loss:  1.49471980e-02, mean val. rec. loss:  1.39066751e-02\n",
      "Epoch: 4087 mean train loss:  1.49460723e-02, mean val. rec. loss:  1.39057577e-02\n",
      "Epoch: 4088 mean train loss:  1.49449392e-02, mean val. rec. loss:  1.39048505e-02\n",
      "Epoch: 4089 mean train loss:  1.49438107e-02, mean val. rec. loss:  1.39039252e-02\n",
      "Epoch: 4090 mean train loss:  1.49426748e-02, mean val. rec. loss:  1.39030157e-02\n",
      "Epoch: 4091 mean train loss:  1.49415500e-02, mean val. rec. loss:  1.39021017e-02\n",
      "Epoch: 4092 mean train loss:  1.49404197e-02, mean val. rec. loss:  1.39011843e-02\n",
      "Epoch: 4093 mean train loss:  1.49392902e-02, mean val. rec. loss:  1.39002704e-02\n",
      "Epoch: 4094 mean train loss:  1.49381543e-02, mean val. rec. loss:  1.38993620e-02\n",
      "Epoch: 4095 mean train loss:  1.49370305e-02, mean val. rec. loss:  1.38984469e-02\n",
      "Epoch: 4096 mean train loss:  1.49359001e-02, mean val. rec. loss:  1.38975397e-02\n",
      "Epoch: 4097 mean train loss:  1.49347735e-02, mean val. rec. loss:  1.38966224e-02\n",
      "Epoch: 4098 mean train loss:  1.49336404e-02, mean val. rec. loss:  1.38957197e-02\n",
      "Epoch: 4099 mean train loss:  1.49325184e-02, mean val. rec. loss:  1.38948080e-02\n",
      "Epoch: 4100 mean train loss:  1.49313899e-02, mean val. rec. loss:  1.38938951e-02\n",
      "Epoch: 4101 mean train loss:  1.49302633e-02, mean val. rec. loss:  1.38929846e-02\n",
      "Epoch: 4102 mean train loss:  1.49291329e-02, mean val. rec. loss:  1.38920762e-02\n",
      "Epoch: 4103 mean train loss:  1.49280082e-02, mean val. rec. loss:  1.38911702e-02\n",
      "Epoch: 4104 mean train loss:  1.49268853e-02, mean val. rec. loss:  1.38902653e-02\n",
      "Epoch: 4105 mean train loss:  1.49257596e-02, mean val. rec. loss:  1.38893570e-02\n",
      "Epoch: 4106 mean train loss:  1.49246320e-02, mean val. rec. loss:  1.38884453e-02\n",
      "Epoch: 4107 mean train loss:  1.49235054e-02, mean val. rec. loss:  1.38875324e-02\n",
      "Epoch: 4108 mean train loss:  1.49223825e-02, mean val. rec. loss:  1.38866252e-02\n",
      "Epoch: 4109 mean train loss:  1.49212587e-02, mean val. rec. loss:  1.38857090e-02\n",
      "Epoch: 4110 mean train loss:  1.49201330e-02, mean val. rec. loss:  1.38848007e-02\n",
      "Epoch: 4111 mean train loss:  1.49190091e-02, mean val. rec. loss:  1.38838923e-02\n",
      "Epoch: 4112 mean train loss:  1.49178862e-02, mean val. rec. loss:  1.38829863e-02\n",
      "Epoch: 4113 mean train loss:  1.49167643e-02, mean val. rec. loss:  1.38820802e-02\n",
      "Epoch: 4114 mean train loss:  1.49156395e-02, mean val. rec. loss:  1.38811708e-02\n",
      "Epoch: 4115 mean train loss:  1.49145147e-02, mean val. rec. loss:  1.38802647e-02\n",
      "Epoch: 4116 mean train loss:  1.49133937e-02, mean val. rec. loss:  1.38793598e-02\n",
      "Epoch: 4117 mean train loss:  1.49122736e-02, mean val. rec. loss:  1.38784504e-02\n",
      "Epoch: 4118 mean train loss:  1.49111498e-02, mean val. rec. loss:  1.38775489e-02\n",
      "Epoch: 4119 mean train loss:  1.49100287e-02, mean val. rec. loss:  1.38766372e-02\n",
      "Epoch: 4120 mean train loss:  1.49089086e-02, mean val. rec. loss:  1.38757379e-02\n",
      "Epoch: 4121 mean train loss:  1.49077876e-02, mean val. rec. loss:  1.38748296e-02\n",
      "Epoch: 4122 mean train loss:  1.49066675e-02, mean val. rec. loss:  1.38739281e-02\n",
      "Epoch: 4123 mean train loss:  1.49055464e-02, mean val. rec. loss:  1.38730198e-02\n",
      "Epoch: 4124 mean train loss:  1.49044272e-02, mean val. rec. loss:  1.38721205e-02\n",
      "Epoch: 4125 mean train loss:  1.49033081e-02, mean val. rec. loss:  1.38712122e-02\n",
      "Epoch: 4126 mean train loss:  1.49021889e-02, mean val. rec. loss:  1.38703141e-02\n",
      "Epoch: 4127 mean train loss:  1.49010697e-02, mean val. rec. loss:  1.38694069e-02\n",
      "Epoch: 4128 mean train loss:  1.48999515e-02, mean val. rec. loss:  1.38685065e-02\n",
      "Epoch: 4129 mean train loss:  1.48988332e-02, mean val. rec. loss:  1.38676005e-02\n",
      "Epoch: 4130 mean train loss:  1.48977150e-02, mean val. rec. loss:  1.38667024e-02\n",
      "Epoch: 4131 mean train loss:  1.48965967e-02, mean val. rec. loss:  1.38657986e-02\n",
      "Epoch: 4132 mean train loss:  1.48954794e-02, mean val. rec. loss:  1.38648982e-02\n",
      "Epoch: 4133 mean train loss:  1.48943621e-02, mean val. rec. loss:  1.38639933e-02\n",
      "Epoch: 4134 mean train loss:  1.48932467e-02, mean val. rec. loss:  1.38630963e-02\n",
      "Epoch: 4135 mean train loss:  1.48921312e-02, mean val. rec. loss:  1.38621971e-02\n",
      "Epoch: 4136 mean train loss:  1.48910139e-02, mean val. rec. loss:  1.38612922e-02\n",
      "Epoch: 4137 mean train loss:  1.48898956e-02, mean val. rec. loss:  1.38603952e-02\n",
      "Epoch: 4138 mean train loss:  1.48887811e-02, mean val. rec. loss:  1.38594937e-02\n",
      "Epoch: 4139 mean train loss:  1.48876675e-02, mean val. rec. loss:  1.38585956e-02\n",
      "Epoch: 4140 mean train loss:  1.48865502e-02, mean val. rec. loss:  1.38576929e-02\n",
      "Epoch: 4141 mean train loss:  1.48854338e-02, mean val. rec. loss:  1.38567948e-02\n",
      "Epoch: 4142 mean train loss:  1.48843202e-02, mean val. rec. loss:  1.38558990e-02\n",
      "Epoch: 4143 mean train loss:  1.48832057e-02, mean val. rec. loss:  1.38550009e-02\n",
      "Epoch: 4144 mean train loss:  1.48820921e-02, mean val. rec. loss:  1.38541005e-02\n",
      "Epoch: 4145 mean train loss:  1.48809757e-02, mean val. rec. loss:  1.38532058e-02\n",
      "Epoch: 4146 mean train loss:  1.48798640e-02, mean val. rec. loss:  1.38523066e-02\n",
      "Epoch: 4147 mean train loss:  1.48787514e-02, mean val. rec. loss:  1.38514084e-02\n",
      "Epoch: 4148 mean train loss:  1.48776368e-02, mean val. rec. loss:  1.38505103e-02\n",
      "Epoch: 4149 mean train loss:  1.48765223e-02, mean val. rec. loss:  1.38496134e-02\n",
      "Epoch: 4150 mean train loss:  1.48754115e-02, mean val. rec. loss:  1.38487198e-02\n",
      "Epoch: 4151 mean train loss:  1.48743007e-02, mean val. rec. loss:  1.38478205e-02\n",
      "Epoch: 4152 mean train loss:  1.48731853e-02, mean val. rec. loss:  1.38469292e-02\n",
      "Epoch: 4153 mean train loss:  1.48720773e-02, mean val. rec. loss:  1.38460311e-02\n",
      "Epoch: 4154 mean train loss:  1.48709618e-02, mean val. rec. loss:  1.38451376e-02\n",
      "Epoch: 4155 mean train loss:  1.48698538e-02, mean val. rec. loss:  1.38442383e-02\n",
      "Epoch: 4156 mean train loss:  1.48687402e-02, mean val. rec. loss:  1.38433459e-02\n",
      "Epoch: 4157 mean train loss:  1.48676332e-02, mean val. rec. loss:  1.38424478e-02\n",
      "Epoch: 4158 mean train loss:  1.48665186e-02, mean val. rec. loss:  1.38415565e-02\n",
      "Epoch: 4159 mean train loss:  1.48654106e-02, mean val. rec. loss:  1.38406606e-02\n",
      "Epoch: 4160 mean train loss:  1.48642989e-02, mean val. rec. loss:  1.38397693e-02\n",
      "Epoch: 4161 mean train loss:  1.48631928e-02, mean val. rec. loss:  1.38388701e-02\n",
      "Epoch: 4162 mean train loss:  1.48620792e-02, mean val. rec. loss:  1.38379776e-02\n",
      "Epoch: 4163 mean train loss:  1.48609730e-02, mean val. rec. loss:  1.38370818e-02\n",
      "Epoch: 4164 mean train loss:  1.48598613e-02, mean val. rec. loss:  1.38361916e-02\n",
      "Epoch: 4165 mean train loss:  1.48587552e-02, mean val. rec. loss:  1.38352946e-02\n",
      "Epoch: 4166 mean train loss:  1.48576444e-02, mean val. rec. loss:  1.38344056e-02\n",
      "Epoch: 4167 mean train loss:  1.48565392e-02, mean val. rec. loss:  1.38335120e-02\n",
      "Epoch: 4168 mean train loss:  1.48554284e-02, mean val. rec. loss:  1.38326219e-02\n",
      "Epoch: 4169 mean train loss:  1.48543231e-02, mean val. rec. loss:  1.38317283e-02\n",
      "Epoch: 4170 mean train loss:  1.48532133e-02, mean val. rec. loss:  1.38308404e-02\n",
      "Epoch: 4171 mean train loss:  1.48521090e-02, mean val. rec. loss:  1.38299491e-02\n",
      "Epoch: 4172 mean train loss:  1.48510001e-02, mean val. rec. loss:  1.38290532e-02\n",
      "Epoch: 4173 mean train loss:  1.48498911e-02, mean val. rec. loss:  1.38281642e-02\n",
      "Epoch: 4174 mean train loss:  1.48487869e-02, mean val. rec. loss:  1.38272729e-02\n",
      "Epoch: 4175 mean train loss:  1.48476816e-02, mean val. rec. loss:  1.38263816e-02\n",
      "Epoch: 4176 mean train loss:  1.48465746e-02, mean val. rec. loss:  1.38254925e-02\n",
      "Epoch: 4177 mean train loss:  1.48454684e-02, mean val. rec. loss:  1.38246035e-02\n",
      "Epoch: 4178 mean train loss:  1.48443642e-02, mean val. rec. loss:  1.38237156e-02\n",
      "Epoch: 4179 mean train loss:  1.48432608e-02, mean val. rec. loss:  1.38228266e-02\n",
      "Epoch: 4180 mean train loss:  1.48421537e-02, mean val. rec. loss:  1.38219375e-02\n",
      "Epoch: 4181 mean train loss:  1.48410476e-02, mean val. rec. loss:  1.38210485e-02\n",
      "Epoch: 4182 mean train loss:  1.48399433e-02, mean val. rec. loss:  1.38201583e-02\n",
      "Epoch: 4183 mean train loss:  1.48388409e-02, mean val. rec. loss:  1.38192670e-02\n",
      "Epoch: 4184 mean train loss:  1.48377357e-02, mean val. rec. loss:  1.38183780e-02\n",
      "Epoch: 4185 mean train loss:  1.48366295e-02, mean val. rec. loss:  1.38174912e-02\n",
      "Epoch: 4186 mean train loss:  1.48355271e-02, mean val. rec. loss:  1.38166044e-02\n",
      "Epoch: 4187 mean train loss:  1.48344247e-02, mean val. rec. loss:  1.38157177e-02\n",
      "Epoch: 4188 mean train loss:  1.48333195e-02, mean val. rec. loss:  1.38148264e-02\n",
      "Epoch: 4189 mean train loss:  1.48322152e-02, mean val. rec. loss:  1.38139396e-02\n",
      "Epoch: 4190 mean train loss:  1.48311128e-02, mean val. rec. loss:  1.38130528e-02\n",
      "Epoch: 4191 mean train loss:  1.48300113e-02, mean val. rec. loss:  1.38121672e-02\n",
      "Epoch: 4192 mean train loss:  1.48289089e-02, mean val. rec. loss:  1.38112770e-02\n",
      "Epoch: 4193 mean train loss:  1.48278074e-02, mean val. rec. loss:  1.38103925e-02\n",
      "Epoch: 4194 mean train loss:  1.48267041e-02, mean val. rec. loss:  1.38095035e-02\n",
      "Epoch: 4195 mean train loss:  1.48256026e-02, mean val. rec. loss:  1.38086178e-02\n",
      "Epoch: 4196 mean train loss:  1.48245011e-02, mean val. rec. loss:  1.38077322e-02\n",
      "Epoch: 4197 mean train loss:  1.48233996e-02, mean val. rec. loss:  1.38068466e-02\n",
      "Epoch: 4198 mean train loss:  1.48222991e-02, mean val. rec. loss:  1.38059621e-02\n",
      "Epoch: 4199 mean train loss:  1.48211976e-02, mean val. rec. loss:  1.38050764e-02\n",
      "Epoch: 4200 mean train loss:  1.48200961e-02, mean val. rec. loss:  1.38041942e-02\n",
      "Epoch: 4201 mean train loss:  1.48189955e-02, mean val. rec. loss:  1.38033074e-02\n",
      "Epoch: 4202 mean train loss:  1.48178950e-02, mean val. rec. loss:  1.38024207e-02\n",
      "Epoch: 4203 mean train loss:  1.48167963e-02, mean val. rec. loss:  1.38015373e-02\n",
      "Epoch: 4204 mean train loss:  1.48156957e-02, mean val. rec. loss:  1.38006539e-02\n",
      "Epoch: 4205 mean train loss:  1.48145970e-02, mean val. rec. loss:  1.37997694e-02\n",
      "Epoch: 4206 mean train loss:  1.48134965e-02, mean val. rec. loss:  1.37988838e-02\n",
      "Epoch: 4207 mean train loss:  1.48123978e-02, mean val. rec. loss:  1.37979981e-02\n",
      "Epoch: 4208 mean train loss:  1.48112982e-02, mean val. rec. loss:  1.37971159e-02\n",
      "Epoch: 4209 mean train loss:  1.48101985e-02, mean val. rec. loss:  1.37962325e-02\n",
      "Epoch: 4210 mean train loss:  1.48090998e-02, mean val. rec. loss:  1.37953480e-02\n",
      "Epoch: 4211 mean train loss:  1.48080012e-02, mean val. rec. loss:  1.37944635e-02\n",
      "Epoch: 4212 mean train loss:  1.48069015e-02, mean val. rec. loss:  1.37935813e-02\n",
      "Epoch: 4213 mean train loss:  1.48058047e-02, mean val. rec. loss:  1.37926979e-02\n",
      "Epoch: 4214 mean train loss:  1.48047088e-02, mean val. rec. loss:  1.37918123e-02\n",
      "Epoch: 4215 mean train loss:  1.48036092e-02, mean val. rec. loss:  1.37909289e-02\n",
      "Epoch: 4216 mean train loss:  1.48025105e-02, mean val. rec. loss:  1.37900467e-02\n",
      "Epoch: 4217 mean train loss:  1.48014146e-02, mean val. rec. loss:  1.37891667e-02\n",
      "Epoch: 4218 mean train loss:  1.48003196e-02, mean val. rec. loss:  1.37882822e-02\n",
      "Epoch: 4219 mean train loss:  1.47992209e-02, mean val. rec. loss:  1.37874000e-02\n",
      "Epoch: 4220 mean train loss:  1.47981213e-02, mean val. rec. loss:  1.37865223e-02\n",
      "Epoch: 4221 mean train loss:  1.47970263e-02, mean val. rec. loss:  1.37856378e-02\n",
      "Epoch: 4222 mean train loss:  1.47959314e-02, mean val. rec. loss:  1.37847556e-02\n",
      "Epoch: 4223 mean train loss:  1.47948345e-02, mean val. rec. loss:  1.37838745e-02\n",
      "Epoch: 4224 mean train loss:  1.47937358e-02, mean val. rec. loss:  1.37829888e-02\n",
      "Epoch: 4225 mean train loss:  1.47926418e-02, mean val. rec. loss:  1.37821134e-02\n",
      "Epoch: 4226 mean train loss:  1.47915468e-02, mean val. rec. loss:  1.37812312e-02\n",
      "Epoch: 4227 mean train loss:  1.47904500e-02, mean val. rec. loss:  1.37803478e-02\n",
      "Epoch: 4228 mean train loss:  1.47893532e-02, mean val. rec. loss:  1.37794724e-02\n",
      "Epoch: 4229 mean train loss:  1.47882601e-02, mean val. rec. loss:  1.37785913e-02\n",
      "Epoch: 4230 mean train loss:  1.47871651e-02, mean val. rec. loss:  1.37777090e-02\n",
      "Epoch: 4231 mean train loss:  1.47860692e-02, mean val. rec. loss:  1.37768279e-02\n",
      "Epoch: 4232 mean train loss:  1.47849724e-02, mean val. rec. loss:  1.37759468e-02\n",
      "Epoch: 4233 mean train loss:  1.47838802e-02, mean val. rec. loss:  1.37750703e-02\n",
      "Epoch: 4234 mean train loss:  1.47827871e-02, mean val. rec. loss:  1.37741903e-02\n",
      "Epoch: 4235 mean train loss:  1.47816903e-02, mean val. rec. loss:  1.37733115e-02\n",
      "Epoch: 4236 mean train loss:  1.47805972e-02, mean val. rec. loss:  1.37724281e-02\n",
      "Epoch: 4237 mean train loss:  1.47795022e-02, mean val. rec. loss:  1.37715470e-02\n",
      "Epoch: 4238 mean train loss:  1.47784082e-02, mean val. rec. loss:  1.37706693e-02\n",
      "Epoch: 4239 mean train loss:  1.47773160e-02, mean val. rec. loss:  1.37697916e-02\n",
      "Epoch: 4240 mean train loss:  1.47762219e-02, mean val. rec. loss:  1.37689116e-02\n",
      "Epoch: 4241 mean train loss:  1.47751279e-02, mean val. rec. loss:  1.37680294e-02\n",
      "Epoch: 4242 mean train loss:  1.47740348e-02, mean val. rec. loss:  1.37671517e-02\n",
      "Epoch: 4243 mean train loss:  1.47729417e-02, mean val. rec. loss:  1.37662751e-02\n",
      "Epoch: 4244 mean train loss:  1.47718486e-02, mean val. rec. loss:  1.37653940e-02\n",
      "Epoch: 4245 mean train loss:  1.47707564e-02, mean val. rec. loss:  1.37645141e-02\n",
      "Epoch: 4246 mean train loss:  1.47696642e-02, mean val. rec. loss:  1.37636387e-02\n",
      "Epoch: 4247 mean train loss:  1.47685711e-02, mean val. rec. loss:  1.37627621e-02\n",
      "Epoch: 4248 mean train loss:  1.47674799e-02, mean val. rec. loss:  1.37618833e-02\n",
      "Epoch: 4249 mean train loss:  1.47663868e-02, mean val. rec. loss:  1.37610010e-02\n",
      "Epoch: 4250 mean train loss:  1.47652955e-02, mean val. rec. loss:  1.37601256e-02\n",
      "Epoch: 4251 mean train loss:  1.47642024e-02, mean val. rec. loss:  1.37592524e-02\n",
      "Epoch: 4252 mean train loss:  1.47631121e-02, mean val. rec. loss:  1.37583725e-02\n",
      "Epoch: 4253 mean train loss:  1.47620209e-02, mean val. rec. loss:  1.37574959e-02\n",
      "Epoch: 4254 mean train loss:  1.47609306e-02, mean val. rec. loss:  1.37566193e-02\n",
      "Epoch: 4255 mean train loss:  1.47598393e-02, mean val. rec. loss:  1.37557439e-02\n",
      "Epoch: 4256 mean train loss:  1.47587481e-02, mean val. rec. loss:  1.37548617e-02\n",
      "Epoch: 4257 mean train loss:  1.47576568e-02, mean val. rec. loss:  1.37539817e-02\n",
      "Epoch: 4258 mean train loss:  1.47565656e-02, mean val. rec. loss:  1.37531063e-02\n",
      "Epoch: 4259 mean train loss:  1.47554743e-02, mean val. rec. loss:  1.37522286e-02\n",
      "Epoch: 4260 mean train loss:  1.47543868e-02, mean val. rec. loss:  1.37513475e-02\n",
      "Epoch: 4261 mean train loss:  1.47532956e-02, mean val. rec. loss:  1.37504686e-02\n",
      "Epoch: 4262 mean train loss:  1.47522034e-02, mean val. rec. loss:  1.37495955e-02\n",
      "Epoch: 4263 mean train loss:  1.47511150e-02, mean val. rec. loss:  1.37487201e-02\n",
      "Epoch: 4264 mean train loss:  1.47500274e-02, mean val. rec. loss:  1.37478435e-02\n",
      "Epoch: 4265 mean train loss:  1.47489362e-02, mean val. rec. loss:  1.37469624e-02\n",
      "Epoch: 4266 mean train loss:  1.47478431e-02, mean val. rec. loss:  1.37460915e-02\n",
      "Epoch: 4267 mean train loss:  1.47467556e-02, mean val. rec. loss:  1.37452172e-02\n",
      "Epoch: 4268 mean train loss:  1.47456681e-02, mean val. rec. loss:  1.37443372e-02\n",
      "Epoch: 4269 mean train loss:  1.47445768e-02, mean val. rec. loss:  1.37434629e-02\n",
      "Epoch: 4270 mean train loss:  1.47434874e-02, mean val. rec. loss:  1.37425920e-02\n",
      "Epoch: 4271 mean train loss:  1.47423990e-02, mean val. rec. loss:  1.37417155e-02\n",
      "Epoch: 4272 mean train loss:  1.47413115e-02, mean val. rec. loss:  1.37408389e-02\n",
      "Epoch: 4273 mean train loss:  1.47402202e-02, mean val. rec. loss:  1.37399578e-02\n",
      "Epoch: 4274 mean train loss:  1.47391318e-02, mean val. rec. loss:  1.37390858e-02\n",
      "Epoch: 4275 mean train loss:  1.47380433e-02, mean val. rec. loss:  1.37382104e-02\n",
      "Epoch: 4276 mean train loss:  1.47369577e-02, mean val. rec. loss:  1.37373304e-02\n",
      "Epoch: 4277 mean train loss:  1.47358673e-02, mean val. rec. loss:  1.37364561e-02\n",
      "Epoch: 4278 mean train loss:  1.47347780e-02, mean val. rec. loss:  1.37355795e-02\n",
      "Epoch: 4279 mean train loss:  1.47336904e-02, mean val. rec. loss:  1.37347041e-02\n",
      "Epoch: 4280 mean train loss:  1.47326039e-02, mean val. rec. loss:  1.37338287e-02\n",
      "Epoch: 4281 mean train loss:  1.47315145e-02, mean val. rec. loss:  1.37329510e-02\n",
      "Epoch: 4282 mean train loss:  1.47304260e-02, mean val. rec. loss:  1.37320778e-02\n",
      "Epoch: 4283 mean train loss:  1.47293394e-02, mean val. rec. loss:  1.37312058e-02\n",
      "Epoch: 4284 mean train loss:  1.47282538e-02, mean val. rec. loss:  1.37303315e-02\n",
      "Epoch: 4285 mean train loss:  1.47271653e-02, mean val. rec. loss:  1.37294583e-02\n",
      "Epoch: 4286 mean train loss:  1.47260759e-02, mean val. rec. loss:  1.37285829e-02\n",
      "Epoch: 4287 mean train loss:  1.47249884e-02, mean val. rec. loss:  1.37277097e-02\n",
      "Epoch: 4288 mean train loss:  1.47239028e-02, mean val. rec. loss:  1.37268354e-02\n",
      "Epoch: 4289 mean train loss:  1.47228143e-02, mean val. rec. loss:  1.37259589e-02\n",
      "Epoch: 4290 mean train loss:  1.47217277e-02, mean val. rec. loss:  1.37250834e-02\n",
      "Epoch: 4291 mean train loss:  1.47206421e-02, mean val. rec. loss:  1.37242080e-02\n",
      "Epoch: 4292 mean train loss:  1.47195546e-02, mean val. rec. loss:  1.37233349e-02\n",
      "Epoch: 4293 mean train loss:  1.47184689e-02, mean val. rec. loss:  1.37224594e-02\n",
      "Epoch: 4294 mean train loss:  1.47173814e-02, mean val. rec. loss:  1.37215840e-02\n",
      "Epoch: 4295 mean train loss:  1.47162957e-02, mean val. rec. loss:  1.37207063e-02\n",
      "Epoch: 4296 mean train loss:  1.47152101e-02, mean val. rec. loss:  1.37198354e-02\n",
      "Epoch: 4297 mean train loss:  1.47141235e-02, mean val. rec. loss:  1.37189588e-02\n",
      "Epoch: 4298 mean train loss:  1.47130388e-02, mean val. rec. loss:  1.37180823e-02\n",
      "Epoch: 4299 mean train loss:  1.47119512e-02, mean val. rec. loss:  1.37172080e-02\n",
      "Epoch: 4300 mean train loss:  1.47108665e-02, mean val. rec. loss:  1.37163360e-02\n",
      "Epoch: 4301 mean train loss:  1.47097799e-02, mean val. rec. loss:  1.37154617e-02\n",
      "Epoch: 4302 mean train loss:  1.47086943e-02, mean val. rec. loss:  1.37145874e-02\n",
      "Epoch: 4303 mean train loss:  1.47076086e-02, mean val. rec. loss:  1.37137119e-02\n",
      "Epoch: 4304 mean train loss:  1.47065229e-02, mean val. rec. loss:  1.37128388e-02\n",
      "Epoch: 4305 mean train loss:  1.47054382e-02, mean val. rec. loss:  1.37119679e-02\n",
      "Epoch: 4306 mean train loss:  1.47043526e-02, mean val. rec. loss:  1.37110924e-02\n",
      "Epoch: 4307 mean train loss:  1.47032678e-02, mean val. rec. loss:  1.37102204e-02\n",
      "Epoch: 4308 mean train loss:  1.47021831e-02, mean val. rec. loss:  1.37093484e-02\n",
      "Epoch: 4309 mean train loss:  1.47010975e-02, mean val. rec. loss:  1.37084741e-02\n",
      "Epoch: 4310 mean train loss:  1.47000127e-02, mean val. rec. loss:  1.37076009e-02\n",
      "Epoch: 4311 mean train loss:  1.46989271e-02, mean val. rec. loss:  1.37067221e-02\n",
      "Epoch: 4312 mean train loss:  1.46978414e-02, mean val. rec. loss:  1.37058557e-02\n",
      "Epoch: 4313 mean train loss:  1.46967576e-02, mean val. rec. loss:  1.37049837e-02\n",
      "Epoch: 4314 mean train loss:  1.46956757e-02, mean val. rec. loss:  1.37041094e-02\n",
      "Epoch: 4315 mean train loss:  1.46945900e-02, mean val. rec. loss:  1.37032408e-02\n",
      "Epoch: 4316 mean train loss:  1.46935025e-02, mean val. rec. loss:  1.37023699e-02\n",
      "Epoch: 4317 mean train loss:  1.46924224e-02, mean val. rec. loss:  1.37014967e-02\n",
      "Epoch: 4318 mean train loss:  1.46913396e-02, mean val. rec. loss:  1.37006258e-02\n",
      "Epoch: 4319 mean train loss:  1.46902511e-02, mean val. rec. loss:  1.36997481e-02\n",
      "Epoch: 4320 mean train loss:  1.46891655e-02, mean val. rec. loss:  1.36988807e-02\n",
      "Epoch: 4321 mean train loss:  1.46880854e-02, mean val. rec. loss:  1.36980075e-02\n",
      "Epoch: 4322 mean train loss:  1.46870016e-02, mean val. rec. loss:  1.36971309e-02\n",
      "Epoch: 4323 mean train loss:  1.46859141e-02, mean val. rec. loss:  1.36962600e-02\n",
      "Epoch: 4324 mean train loss:  1.46848322e-02, mean val. rec. loss:  1.36953846e-02\n",
      "Epoch: 4325 mean train loss:  1.46837502e-02, mean val. rec. loss:  1.36945114e-02\n",
      "Epoch: 4326 mean train loss:  1.46826646e-02, mean val. rec. loss:  1.36936349e-02\n",
      "Epoch: 4327 mean train loss:  1.46815798e-02, mean val. rec. loss:  1.36927606e-02\n",
      "Epoch: 4328 mean train loss:  1.46804970e-02, mean val. rec. loss:  1.36918908e-02\n",
      "Epoch: 4329 mean train loss:  1.46794150e-02, mean val. rec. loss:  1.36910177e-02\n",
      "Epoch: 4330 mean train loss:  1.46783303e-02, mean val. rec. loss:  1.36901468e-02\n",
      "Epoch: 4331 mean train loss:  1.46772437e-02, mean val. rec. loss:  1.36892838e-02\n",
      "Epoch: 4332 mean train loss:  1.46761637e-02, mean val. rec. loss:  1.36884095e-02\n",
      "Epoch: 4333 mean train loss:  1.46750845e-02, mean val. rec. loss:  1.36875375e-02\n",
      "Epoch: 4334 mean train loss:  1.46739998e-02, mean val. rec. loss:  1.36866621e-02\n",
      "Epoch: 4335 mean train loss:  1.46729141e-02, mean val. rec. loss:  1.36857912e-02\n",
      "Epoch: 4336 mean train loss:  1.46718303e-02, mean val. rec. loss:  1.36849180e-02\n",
      "Epoch: 4337 mean train loss:  1.46707465e-02, mean val. rec. loss:  1.36840460e-02\n",
      "Epoch: 4338 mean train loss:  1.46696655e-02, mean val. rec. loss:  1.36831717e-02\n",
      "Epoch: 4339 mean train loss:  1.46685827e-02, mean val. rec. loss:  1.36822996e-02\n",
      "Epoch: 4340 mean train loss:  1.46674989e-02, mean val. rec. loss:  1.36814276e-02\n",
      "Epoch: 4341 mean train loss:  1.46664151e-02, mean val. rec. loss:  1.36805567e-02\n",
      "Epoch: 4342 mean train loss:  1.46653332e-02, mean val. rec. loss:  1.36796824e-02\n",
      "Epoch: 4343 mean train loss:  1.46642512e-02, mean val. rec. loss:  1.36788149e-02\n",
      "Epoch: 4344 mean train loss:  1.46631684e-02, mean val. rec. loss:  1.36779440e-02\n",
      "Epoch: 4345 mean train loss:  1.46620836e-02, mean val. rec. loss:  1.36770731e-02\n",
      "Epoch: 4346 mean train loss:  1.46610036e-02, mean val. rec. loss:  1.36762023e-02\n",
      "Epoch: 4347 mean train loss:  1.46599207e-02, mean val. rec. loss:  1.36753291e-02\n",
      "Epoch: 4348 mean train loss:  1.46588378e-02, mean val. rec. loss:  1.36744605e-02\n",
      "Epoch: 4349 mean train loss:  1.46577550e-02, mean val. rec. loss:  1.36735884e-02\n",
      "Epoch: 4350 mean train loss:  1.46566740e-02, mean val. rec. loss:  1.36727164e-02\n",
      "Epoch: 4351 mean train loss:  1.46555920e-02, mean val. rec. loss:  1.36718478e-02\n",
      "Epoch: 4352 mean train loss:  1.46545101e-02, mean val. rec. loss:  1.36709678e-02\n",
      "Epoch: 4353 mean train loss:  1.46534226e-02, mean val. rec. loss:  1.36701037e-02\n",
      "Epoch: 4354 mean train loss:  1.46523462e-02, mean val. rec. loss:  1.36692272e-02\n",
      "Epoch: 4355 mean train loss:  1.46512624e-02, mean val. rec. loss:  1.36683551e-02\n",
      "Epoch: 4356 mean train loss:  1.46501805e-02, mean val. rec. loss:  1.36674797e-02\n",
      "Epoch: 4357 mean train loss:  1.46490930e-02, mean val. rec. loss:  1.36666133e-02\n",
      "Epoch: 4358 mean train loss:  1.46480176e-02, mean val. rec. loss:  1.36657379e-02\n",
      "Epoch: 4359 mean train loss:  1.46469347e-02, mean val. rec. loss:  1.36648716e-02\n",
      "Epoch: 4360 mean train loss:  1.46458528e-02, mean val. rec. loss:  1.36639916e-02\n",
      "Epoch: 4361 mean train loss:  1.46447653e-02, mean val. rec. loss:  1.36631252e-02\n",
      "Epoch: 4362 mean train loss:  1.46436889e-02, mean val. rec. loss:  1.36622509e-02\n",
      "Epoch: 4363 mean train loss:  1.46426051e-02, mean val. rec. loss:  1.36613789e-02\n",
      "Epoch: 4364 mean train loss:  1.46415250e-02, mean val. rec. loss:  1.36605035e-02\n",
      "Epoch: 4365 mean train loss:  1.46404375e-02, mean val. rec. loss:  1.36596360e-02\n",
      "Epoch: 4366 mean train loss:  1.46393612e-02, mean val. rec. loss:  1.36587594e-02\n",
      "Epoch: 4367 mean train loss:  1.46382774e-02, mean val. rec. loss:  1.36578953e-02\n",
      "Epoch: 4368 mean train loss:  1.46371964e-02, mean val. rec. loss:  1.36570154e-02\n",
      "Epoch: 4369 mean train loss:  1.46361098e-02, mean val. rec. loss:  1.36561467e-02\n",
      "Epoch: 4370 mean train loss:  1.46350335e-02, mean val. rec. loss:  1.36552736e-02\n",
      "Epoch: 4371 mean train loss:  1.46339506e-02, mean val. rec. loss:  1.36544027e-02\n",
      "Epoch: 4372 mean train loss:  1.46328705e-02, mean val. rec. loss:  1.36535261e-02\n",
      "Epoch: 4373 mean train loss:  1.46317830e-02, mean val. rec. loss:  1.36526586e-02\n",
      "Epoch: 4374 mean train loss:  1.46307067e-02, mean val. rec. loss:  1.36517832e-02\n",
      "Epoch: 4375 mean train loss:  1.46296247e-02, mean val. rec. loss:  1.36509168e-02\n",
      "Epoch: 4376 mean train loss:  1.46285437e-02, mean val. rec. loss:  1.36500380e-02\n",
      "Epoch: 4377 mean train loss:  1.46274571e-02, mean val. rec. loss:  1.36491705e-02\n",
      "Epoch: 4378 mean train loss:  1.46263817e-02, mean val. rec. loss:  1.36482951e-02\n",
      "Epoch: 4379 mean train loss:  1.46252998e-02, mean val. rec. loss:  1.36474208e-02\n",
      "Epoch: 4380 mean train loss:  1.46242179e-02, mean val. rec. loss:  1.36465454e-02\n",
      "Epoch: 4381 mean train loss:  1.46231313e-02, mean val. rec. loss:  1.36456779e-02\n",
      "Epoch: 4382 mean train loss:  1.46220540e-02, mean val. rec. loss:  1.36448047e-02\n",
      "Epoch: 4383 mean train loss:  1.46209721e-02, mean val. rec. loss:  1.36439395e-02\n",
      "Epoch: 4384 mean train loss:  1.46198920e-02, mean val. rec. loss:  1.36430573e-02\n",
      "Epoch: 4385 mean train loss:  1.46188045e-02, mean val. rec. loss:  1.36421886e-02\n",
      "Epoch: 4386 mean train loss:  1.46177290e-02, mean val. rec. loss:  1.36413121e-02\n",
      "Epoch: 4387 mean train loss:  1.46166462e-02, mean val. rec. loss:  1.36404400e-02\n",
      "Epoch: 4388 mean train loss:  1.46155661e-02, mean val. rec. loss:  1.36395669e-02\n",
      "Epoch: 4389 mean train loss:  1.46144805e-02, mean val. rec. loss:  1.36386926e-02\n",
      "Epoch: 4390 mean train loss:  1.46134004e-02, mean val. rec. loss:  1.36378228e-02\n",
      "Epoch: 4391 mean train loss:  1.46123222e-02, mean val. rec. loss:  1.36369519e-02\n",
      "Epoch: 4392 mean train loss:  1.46112402e-02, mean val. rec. loss:  1.36360776e-02\n",
      "Epoch: 4393 mean train loss:  1.46101546e-02, mean val. rec. loss:  1.36351999e-02\n",
      "Epoch: 4394 mean train loss:  1.46090745e-02, mean val. rec. loss:  1.36343290e-02\n",
      "Epoch: 4395 mean train loss:  1.46079963e-02, mean val. rec. loss:  1.36334513e-02\n",
      "Epoch: 4396 mean train loss:  1.46069144e-02, mean val. rec. loss:  1.36325759e-02\n",
      "Epoch: 4397 mean train loss:  1.46058287e-02, mean val. rec. loss:  1.36317039e-02\n",
      "Epoch: 4398 mean train loss:  1.46047486e-02, mean val. rec. loss:  1.36308330e-02\n",
      "Epoch: 4399 mean train loss:  1.46036714e-02, mean val. rec. loss:  1.36299610e-02\n",
      "Epoch: 4400 mean train loss:  1.46025885e-02, mean val. rec. loss:  1.36290855e-02\n",
      "Epoch: 4401 mean train loss:  1.46015038e-02, mean val. rec. loss:  1.36282078e-02\n",
      "Epoch: 4402 mean train loss:  1.46004228e-02, mean val. rec. loss:  1.36273347e-02\n",
      "Epoch: 4403 mean train loss:  1.45993446e-02, mean val. rec. loss:  1.36264570e-02\n",
      "Epoch: 4404 mean train loss:  1.45982626e-02, mean val. rec. loss:  1.36255793e-02\n",
      "Epoch: 4405 mean train loss:  1.45971779e-02, mean val. rec. loss:  1.36247084e-02\n",
      "Epoch: 4406 mean train loss:  1.45960960e-02, mean val. rec. loss:  1.36238352e-02\n",
      "Epoch: 4407 mean train loss:  1.45950178e-02, mean val. rec. loss:  1.36229655e-02\n",
      "Epoch: 4408 mean train loss:  1.45939349e-02, mean val. rec. loss:  1.36220889e-02\n",
      "Epoch: 4409 mean train loss:  1.45928511e-02, mean val. rec. loss:  1.36212112e-02\n",
      "Epoch: 4410 mean train loss:  1.45917710e-02, mean val. rec. loss:  1.36203392e-02\n",
      "Epoch: 4411 mean train loss:  1.45906928e-02, mean val. rec. loss:  1.36194637e-02\n",
      "Epoch: 4412 mean train loss:  1.45896109e-02, mean val. rec. loss:  1.36185838e-02\n",
      "Epoch: 4413 mean train loss:  1.45885243e-02, mean val. rec. loss:  1.36177117e-02\n",
      "Epoch: 4414 mean train loss:  1.45874442e-02, mean val. rec. loss:  1.36168408e-02\n",
      "Epoch: 4415 mean train loss:  1.45863651e-02, mean val. rec. loss:  1.36159688e-02\n",
      "Epoch: 4416 mean train loss:  1.45852832e-02, mean val. rec. loss:  1.36150900e-02\n",
      "Epoch: 4417 mean train loss:  1.45841966e-02, mean val. rec. loss:  1.36142123e-02\n",
      "Epoch: 4418 mean train loss:  1.45831156e-02, mean val. rec. loss:  1.36133369e-02\n",
      "Epoch: 4419 mean train loss:  1.45820392e-02, mean val. rec. loss:  1.36124569e-02\n",
      "Epoch: 4420 mean train loss:  1.45809564e-02, mean val. rec. loss:  1.36115803e-02\n",
      "Epoch: 4421 mean train loss:  1.45798707e-02, mean val. rec. loss:  1.36107060e-02\n",
      "Epoch: 4422 mean train loss:  1.45787897e-02, mean val. rec. loss:  1.36098306e-02\n",
      "Epoch: 4423 mean train loss:  1.45777106e-02, mean val. rec. loss:  1.36089597e-02\n",
      "Epoch: 4424 mean train loss:  1.45766286e-02, mean val. rec. loss:  1.36080843e-02\n",
      "Epoch: 4425 mean train loss:  1.45755439e-02, mean val. rec. loss:  1.36072032e-02\n",
      "Epoch: 4426 mean train loss:  1.45744620e-02, mean val. rec. loss:  1.36063323e-02\n",
      "Epoch: 4427 mean train loss:  1.45733847e-02, mean val. rec. loss:  1.36054557e-02\n",
      "Epoch: 4428 mean train loss:  1.45722972e-02, mean val. rec. loss:  1.36045837e-02\n",
      "Epoch: 4429 mean train loss:  1.45712199e-02, mean val. rec. loss:  1.36037049e-02\n",
      "Epoch: 4430 mean train loss:  1.45701343e-02, mean val. rec. loss:  1.36028340e-02\n",
      "Epoch: 4431 mean train loss:  1.45690560e-02, mean val. rec. loss:  1.36019563e-02\n",
      "Epoch: 4432 mean train loss:  1.45679695e-02, mean val. rec. loss:  1.36010820e-02\n",
      "Epoch: 4433 mean train loss:  1.45668931e-02, mean val. rec. loss:  1.36002043e-02\n",
      "Epoch: 4434 mean train loss:  1.45658065e-02, mean val. rec. loss:  1.35993300e-02\n",
      "Epoch: 4435 mean train loss:  1.45647283e-02, mean val. rec. loss:  1.35984534e-02\n",
      "Epoch: 4436 mean train loss:  1.45636408e-02, mean val. rec. loss:  1.35975769e-02\n",
      "Epoch: 4437 mean train loss:  1.45625635e-02, mean val. rec. loss:  1.35966992e-02\n",
      "Epoch: 4438 mean train loss:  1.45614769e-02, mean val. rec. loss:  1.35958271e-02\n",
      "Epoch: 4439 mean train loss:  1.45603987e-02, mean val. rec. loss:  1.35949460e-02\n",
      "Epoch: 4440 mean train loss:  1.45593103e-02, mean val. rec. loss:  1.35940717e-02\n",
      "Epoch: 4441 mean train loss:  1.45582330e-02, mean val. rec. loss:  1.35931929e-02\n",
      "Epoch: 4442 mean train loss:  1.45571455e-02, mean val. rec. loss:  1.35923197e-02\n",
      "Epoch: 4443 mean train loss:  1.45560682e-02, mean val. rec. loss:  1.35914443e-02\n",
      "Epoch: 4444 mean train loss:  1.45549797e-02, mean val. rec. loss:  1.35905666e-02\n",
      "Epoch: 4445 mean train loss:  1.45539015e-02, mean val. rec. loss:  1.35896878e-02\n",
      "Epoch: 4446 mean train loss:  1.45528159e-02, mean val. rec. loss:  1.35888146e-02\n",
      "Epoch: 4447 mean train loss:  1.45517367e-02, mean val. rec. loss:  1.35879335e-02\n",
      "Epoch: 4448 mean train loss:  1.45506502e-02, mean val. rec. loss:  1.35870581e-02\n",
      "Epoch: 4449 mean train loss:  1.45495719e-02, mean val. rec. loss:  1.35861770e-02\n",
      "Epoch: 4450 mean train loss:  1.45484844e-02, mean val. rec. loss:  1.35853027e-02\n",
      "Epoch: 4451 mean train loss:  1.45474062e-02, mean val. rec. loss:  1.35844227e-02\n",
      "Epoch: 4452 mean train loss:  1.45463178e-02, mean val. rec. loss:  1.35835462e-02\n",
      "Epoch: 4453 mean train loss:  1.45452405e-02, mean val. rec. loss:  1.35826639e-02\n",
      "Epoch: 4454 mean train loss:  1.45441511e-02, mean val. rec. loss:  1.35817896e-02\n",
      "Epoch: 4455 mean train loss:  1.45430729e-02, mean val. rec. loss:  1.35809074e-02\n",
      "Epoch: 4456 mean train loss:  1.45419854e-02, mean val. rec. loss:  1.35800331e-02\n",
      "Epoch: 4457 mean train loss:  1.45409062e-02, mean val. rec. loss:  1.35791543e-02\n",
      "Epoch: 4458 mean train loss:  1.45398178e-02, mean val. rec. loss:  1.35782766e-02\n",
      "Epoch: 4459 mean train loss:  1.45387396e-02, mean val. rec. loss:  1.35773966e-02\n",
      "Epoch: 4460 mean train loss:  1.45376511e-02, mean val. rec. loss:  1.35765212e-02\n",
      "Epoch: 4461 mean train loss:  1.45365720e-02, mean val. rec. loss:  1.35756389e-02\n",
      "Epoch: 4462 mean train loss:  1.45354845e-02, mean val. rec. loss:  1.35747635e-02\n",
      "Epoch: 4463 mean train loss:  1.45344063e-02, mean val. rec. loss:  1.35738824e-02\n",
      "Epoch: 4464 mean train loss:  1.45333169e-02, mean val. rec. loss:  1.35730058e-02\n",
      "Epoch: 4465 mean train loss:  1.45322387e-02, mean val. rec. loss:  1.35721236e-02\n",
      "Epoch: 4466 mean train loss:  1.45311502e-02, mean val. rec. loss:  1.35712459e-02\n",
      "Epoch: 4467 mean train loss:  1.45300711e-02, mean val. rec. loss:  1.35703625e-02\n",
      "Epoch: 4468 mean train loss:  1.45289817e-02, mean val. rec. loss:  1.35694871e-02\n",
      "Epoch: 4469 mean train loss:  1.45279035e-02, mean val. rec. loss:  1.35686060e-02\n",
      "Epoch: 4470 mean train loss:  1.45268141e-02, mean val. rec. loss:  1.35677295e-02\n",
      "Epoch: 4471 mean train loss:  1.45257359e-02, mean val. rec. loss:  1.35668484e-02\n",
      "Epoch: 4472 mean train loss:  1.45246465e-02, mean val. rec. loss:  1.35659695e-02\n",
      "Epoch: 4473 mean train loss:  1.45235674e-02, mean val. rec. loss:  1.35650850e-02\n",
      "Epoch: 4474 mean train loss:  1.45224780e-02, mean val. rec. loss:  1.35642062e-02\n",
      "Epoch: 4475 mean train loss:  1.45213979e-02, mean val. rec. loss:  1.35633240e-02\n",
      "Epoch: 4476 mean train loss:  1.45203076e-02, mean val. rec. loss:  1.35624463e-02\n",
      "Epoch: 4477 mean train loss:  1.45192285e-02, mean val. rec. loss:  1.35615652e-02\n",
      "Epoch: 4478 mean train loss:  1.45181391e-02, mean val. rec. loss:  1.35606863e-02\n",
      "Epoch: 4479 mean train loss:  1.45170590e-02, mean val. rec. loss:  1.35598018e-02\n",
      "Epoch: 4480 mean train loss:  1.45159678e-02, mean val. rec. loss:  1.35589241e-02\n",
      "Epoch: 4481 mean train loss:  1.45148886e-02, mean val. rec. loss:  1.35580419e-02\n",
      "Epoch: 4482 mean train loss:  1.45138011e-02, mean val. rec. loss:  1.35571619e-02\n",
      "Epoch: 4483 mean train loss:  1.45127192e-02, mean val. rec. loss:  1.35562740e-02\n",
      "Epoch: 4484 mean train loss:  1.45116279e-02, mean val. rec. loss:  1.35553929e-02\n",
      "Epoch: 4485 mean train loss:  1.45105488e-02, mean val. rec. loss:  1.35545118e-02\n",
      "Epoch: 4486 mean train loss:  1.45094604e-02, mean val. rec. loss:  1.35536319e-02\n",
      "Epoch: 4487 mean train loss:  1.45083784e-02, mean val. rec. loss:  1.35527428e-02\n",
      "Epoch: 4488 mean train loss:  1.45072853e-02, mean val. rec. loss:  1.35518651e-02\n",
      "Epoch: 4489 mean train loss:  1.45062080e-02, mean val. rec. loss:  1.35509761e-02\n",
      "Epoch: 4490 mean train loss:  1.45051187e-02, mean val. rec. loss:  1.35500950e-02\n",
      "Epoch: 4491 mean train loss:  1.45040358e-02, mean val. rec. loss:  1.35492116e-02\n",
      "Epoch: 4492 mean train loss:  1.45029464e-02, mean val. rec. loss:  1.35483282e-02\n",
      "Epoch: 4493 mean train loss:  1.45018589e-02, mean val. rec. loss:  1.35474483e-02\n",
      "Epoch: 4494 mean train loss:  1.45007770e-02, mean val. rec. loss:  1.35465683e-02\n",
      "Epoch: 4495 mean train loss:  1.44996922e-02, mean val. rec. loss:  1.35456872e-02\n",
      "Epoch: 4496 mean train loss:  1.44986019e-02, mean val. rec. loss:  1.35448061e-02\n",
      "Epoch: 4497 mean train loss:  1.44975144e-02, mean val. rec. loss:  1.35439262e-02\n",
      "Epoch: 4498 mean train loss:  1.44964325e-02, mean val. rec. loss:  1.35430417e-02\n",
      "Epoch: 4499 mean train loss:  1.44953468e-02, mean val. rec. loss:  1.35421571e-02\n",
      "Epoch: 4500 mean train loss:  1.44942574e-02, mean val. rec. loss:  1.35412726e-02\n",
      "Epoch: 4501 mean train loss:  1.44931699e-02, mean val. rec. loss:  1.35403927e-02\n",
      "Epoch: 4502 mean train loss:  1.44920871e-02, mean val. rec. loss:  1.35395127e-02\n",
      "Epoch: 4503 mean train loss:  1.44910005e-02, mean val. rec. loss:  1.35386248e-02\n",
      "Epoch: 4504 mean train loss:  1.44899102e-02, mean val. rec. loss:  1.35377414e-02\n",
      "Epoch: 4505 mean train loss:  1.44888217e-02, mean val. rec. loss:  1.35368592e-02\n",
      "Epoch: 4506 mean train loss:  1.44877398e-02, mean val. rec. loss:  1.35359679e-02\n",
      "Epoch: 4507 mean train loss:  1.44866541e-02, mean val. rec. loss:  1.35350811e-02\n",
      "Epoch: 4508 mean train loss:  1.44855629e-02, mean val. rec. loss:  1.35341955e-02\n",
      "Epoch: 4509 mean train loss:  1.44844735e-02, mean val. rec. loss:  1.35333144e-02\n",
      "Epoch: 4510 mean train loss:  1.44833906e-02, mean val. rec. loss:  1.35324276e-02\n",
      "Epoch: 4511 mean train loss:  1.44823031e-02, mean val. rec. loss:  1.35315420e-02\n",
      "Epoch: 4512 mean train loss:  1.44812119e-02, mean val. rec. loss:  1.35306586e-02\n",
      "Epoch: 4513 mean train loss:  1.44801234e-02, mean val. rec. loss:  1.35297798e-02\n",
      "Epoch: 4514 mean train loss:  1.44790396e-02, mean val. rec. loss:  1.35288930e-02\n",
      "Epoch: 4515 mean train loss:  1.44779521e-02, mean val. rec. loss:  1.35280074e-02\n",
      "Epoch: 4516 mean train loss:  1.44768599e-02, mean val. rec. loss:  1.35271172e-02\n",
      "Epoch: 4517 mean train loss:  1.44757715e-02, mean val. rec. loss:  1.35262361e-02\n",
      "Epoch: 4518 mean train loss:  1.44746877e-02, mean val. rec. loss:  1.35253539e-02\n",
      "Epoch: 4519 mean train loss:  1.44735992e-02, mean val. rec. loss:  1.35244648e-02\n",
      "Epoch: 4520 mean train loss:  1.44725080e-02, mean val. rec. loss:  1.35235803e-02\n",
      "Epoch: 4521 mean train loss:  1.44714177e-02, mean val. rec. loss:  1.35226958e-02\n",
      "Epoch: 4522 mean train loss:  1.44703329e-02, mean val. rec. loss:  1.35218079e-02\n",
      "Epoch: 4523 mean train loss:  1.44692445e-02, mean val. rec. loss:  1.35209166e-02\n",
      "Epoch: 4524 mean train loss:  1.44681514e-02, mean val. rec. loss:  1.35200242e-02\n",
      "Epoch: 4525 mean train loss:  1.44670620e-02, mean val. rec. loss:  1.35191363e-02\n",
      "Epoch: 4526 mean train loss:  1.44659754e-02, mean val. rec. loss:  1.35182507e-02\n",
      "Epoch: 4527 mean train loss:  1.44648870e-02, mean val. rec. loss:  1.35173616e-02\n",
      "Epoch: 4528 mean train loss:  1.44637939e-02, mean val. rec. loss:  1.35164748e-02\n",
      "Epoch: 4529 mean train loss:  1.44627045e-02, mean val. rec. loss:  1.35155881e-02\n",
      "Epoch: 4530 mean train loss:  1.44616170e-02, mean val. rec. loss:  1.35146968e-02\n",
      "Epoch: 4531 mean train loss:  1.44605285e-02, mean val. rec. loss:  1.35138089e-02\n",
      "Epoch: 4532 mean train loss:  1.44594354e-02, mean val. rec. loss:  1.35129164e-02\n",
      "Epoch: 4533 mean train loss:  1.44583432e-02, mean val. rec. loss:  1.35120308e-02\n",
      "Epoch: 4534 mean train loss:  1.44572557e-02, mean val. rec. loss:  1.35111474e-02\n",
      "Epoch: 4535 mean train loss:  1.44561682e-02, mean val. rec. loss:  1.35102550e-02\n",
      "Epoch: 4536 mean train loss:  1.44550751e-02, mean val. rec. loss:  1.35093614e-02\n",
      "Epoch: 4537 mean train loss:  1.44539829e-02, mean val. rec. loss:  1.35084780e-02\n",
      "Epoch: 4538 mean train loss:  1.44528945e-02, mean val. rec. loss:  1.35075845e-02\n",
      "Epoch: 4539 mean train loss:  1.44518060e-02, mean val. rec. loss:  1.35066920e-02\n",
      "Epoch: 4540 mean train loss:  1.44507120e-02, mean val. rec. loss:  1.35057951e-02\n",
      "Epoch: 4541 mean train loss:  1.44496189e-02, mean val. rec. loss:  1.35049060e-02\n",
      "Epoch: 4542 mean train loss:  1.44485295e-02, mean val. rec. loss:  1.35040192e-02\n",
      "Epoch: 4543 mean train loss:  1.44474410e-02, mean val. rec. loss:  1.35031257e-02\n",
      "Epoch: 4544 mean train loss:  1.44463470e-02, mean val. rec. loss:  1.35022276e-02\n",
      "Epoch: 4545 mean train loss:  1.44452530e-02, mean val. rec. loss:  1.35013408e-02\n",
      "Epoch: 4546 mean train loss:  1.44441645e-02, mean val. rec. loss:  1.35004472e-02\n",
      "Epoch: 4547 mean train loss:  1.44430751e-02, mean val. rec. loss:  1.34995536e-02\n",
      "Epoch: 4548 mean train loss:  1.44419802e-02, mean val. rec. loss:  1.34986555e-02\n",
      "Epoch: 4549 mean train loss:  1.44408861e-02, mean val. rec. loss:  1.34977665e-02\n",
      "Epoch: 4550 mean train loss:  1.44397958e-02, mean val. rec. loss:  1.34968843e-02\n",
      "Epoch: 4551 mean train loss:  1.44387046e-02, mean val. rec. loss:  1.34959850e-02\n",
      "Epoch: 4552 mean train loss:  1.44376115e-02, mean val. rec. loss:  1.34950949e-02\n",
      "Epoch: 4553 mean train loss:  1.44365202e-02, mean val. rec. loss:  1.34942013e-02\n",
      "Epoch: 4554 mean train loss:  1.44354271e-02, mean val. rec. loss:  1.34933168e-02\n",
      "Epoch: 4555 mean train loss:  1.44343340e-02, mean val. rec. loss:  1.34924164e-02\n",
      "Epoch: 4556 mean train loss:  1.44332409e-02, mean val. rec. loss:  1.34915285e-02\n",
      "Epoch: 4557 mean train loss:  1.44321478e-02, mean val. rec. loss:  1.34906327e-02\n",
      "Epoch: 4558 mean train loss:  1.44310547e-02, mean val. rec. loss:  1.34897493e-02\n",
      "Epoch: 4559 mean train loss:  1.44299616e-02, mean val. rec. loss:  1.34888489e-02\n",
      "Epoch: 4560 mean train loss:  1.44288685e-02, mean val. rec. loss:  1.34879587e-02\n",
      "Epoch: 4561 mean train loss:  1.44277744e-02, mean val. rec. loss:  1.34870640e-02\n",
      "Epoch: 4562 mean train loss:  1.44266813e-02, mean val. rec. loss:  1.34861773e-02\n",
      "Epoch: 4563 mean train loss:  1.44255864e-02, mean val. rec. loss:  1.34852746e-02\n",
      "Epoch: 4564 mean train loss:  1.44244933e-02, mean val. rec. loss:  1.34843810e-02\n",
      "Epoch: 4565 mean train loss:  1.44233974e-02, mean val. rec. loss:  1.34834829e-02\n",
      "Epoch: 4566 mean train loss:  1.44223043e-02, mean val. rec. loss:  1.34825962e-02\n",
      "Epoch: 4567 mean train loss:  1.44212093e-02, mean val. rec. loss:  1.34816935e-02\n",
      "Epoch: 4568 mean train loss:  1.44201143e-02, mean val. rec. loss:  1.34808022e-02\n",
      "Epoch: 4569 mean train loss:  1.44190194e-02, mean val. rec. loss:  1.34799041e-02\n",
      "Epoch: 4570 mean train loss:  1.44179244e-02, mean val. rec. loss:  1.34790162e-02\n",
      "Epoch: 4571 mean train loss:  1.44168294e-02, mean val. rec. loss:  1.34781147e-02\n",
      "Epoch: 4572 mean train loss:  1.44157344e-02, mean val. rec. loss:  1.34772211e-02\n",
      "Epoch: 4573 mean train loss:  1.44146376e-02, mean val. rec. loss:  1.34763207e-02\n",
      "Epoch: 4574 mean train loss:  1.44135427e-02, mean val. rec. loss:  1.34754317e-02\n",
      "Epoch: 4575 mean train loss:  1.44124468e-02, mean val. rec. loss:  1.34745279e-02\n",
      "Epoch: 4576 mean train loss:  1.44113518e-02, mean val. rec. loss:  1.34736332e-02\n",
      "Epoch: 4577 mean train loss:  1.44102559e-02, mean val. rec. loss:  1.34727340e-02\n",
      "Epoch: 4578 mean train loss:  1.44091600e-02, mean val. rec. loss:  1.34718438e-02\n",
      "Epoch: 4579 mean train loss:  1.44080622e-02, mean val. rec. loss:  1.34709366e-02\n",
      "Epoch: 4580 mean train loss:  1.44069663e-02, mean val. rec. loss:  1.34700430e-02\n",
      "Epoch: 4581 mean train loss:  1.44058704e-02, mean val. rec. loss:  1.34691438e-02\n",
      "Epoch: 4582 mean train loss:  1.44047717e-02, mean val. rec. loss:  1.34682536e-02\n",
      "Epoch: 4583 mean train loss:  1.44036768e-02, mean val. rec. loss:  1.34673487e-02\n",
      "Epoch: 4584 mean train loss:  1.44025790e-02, mean val. rec. loss:  1.34664529e-02\n",
      "Epoch: 4585 mean train loss:  1.44014812e-02, mean val. rec. loss:  1.34655525e-02\n",
      "Epoch: 4586 mean train loss:  1.44003844e-02, mean val. rec. loss:  1.34646635e-02\n",
      "Epoch: 4587 mean train loss:  1.43992867e-02, mean val. rec. loss:  1.34637517e-02\n",
      "Epoch: 4588 mean train loss:  1.43981889e-02, mean val. rec. loss:  1.34628559e-02\n",
      "Epoch: 4589 mean train loss:  1.43970902e-02, mean val. rec. loss:  1.34619533e-02\n",
      "Epoch: 4590 mean train loss:  1.43959934e-02, mean val. rec. loss:  1.34610574e-02\n",
      "Epoch: 4591 mean train loss:  1.43948947e-02, mean val. rec. loss:  1.34601491e-02\n",
      "Epoch: 4592 mean train loss:  1.43937978e-02, mean val. rec. loss:  1.34592533e-02\n",
      "Epoch: 4593 mean train loss:  1.43926982e-02, mean val. rec. loss:  1.34583495e-02\n",
      "Epoch: 4594 mean train loss:  1.43915995e-02, mean val. rec. loss:  1.34574548e-02\n",
      "Epoch: 4595 mean train loss:  1.43904999e-02, mean val. rec. loss:  1.34565487e-02\n",
      "Epoch: 4596 mean train loss:  1.43894012e-02, mean val. rec. loss:  1.34556495e-02\n",
      "Epoch: 4597 mean train loss:  1.43883025e-02, mean val. rec. loss:  1.34547423e-02\n",
      "Epoch: 4598 mean train loss:  1.43872020e-02, mean val. rec. loss:  1.34538499e-02\n",
      "Epoch: 4599 mean train loss:  1.43861033e-02, mean val. rec. loss:  1.34529427e-02\n",
      "Epoch: 4600 mean train loss:  1.43850046e-02, mean val. rec. loss:  1.34520468e-02\n",
      "Epoch: 4601 mean train loss:  1.43839040e-02, mean val. rec. loss:  1.34511294e-02\n",
      "Epoch: 4602 mean train loss:  1.43828025e-02, mean val. rec. loss:  1.34502370e-02\n",
      "Epoch: 4603 mean train loss:  1.43817029e-02, mean val. rec. loss:  1.34493276e-02\n",
      "Epoch: 4604 mean train loss:  1.43806042e-02, mean val. rec. loss:  1.34484249e-02\n",
      "Epoch: 4605 mean train loss:  1.43795027e-02, mean val. rec. loss:  1.34475155e-02\n",
      "Epoch: 4606 mean train loss:  1.43783994e-02, mean val. rec. loss:  1.34466151e-02\n",
      "Epoch: 4607 mean train loss:  1.43772970e-02, mean val. rec. loss:  1.34457124e-02\n",
      "Epoch: 4608 mean train loss:  1.43762011e-02, mean val. rec. loss:  1.34448121e-02\n",
      "Epoch: 4609 mean train loss:  1.43751005e-02, mean val. rec. loss:  1.34439094e-02\n",
      "Epoch: 4610 mean train loss:  1.43739953e-02, mean val. rec. loss:  1.34430034e-02\n",
      "Epoch: 4611 mean train loss:  1.43728929e-02, mean val. rec. loss:  1.34421030e-02\n",
      "Epoch: 4612 mean train loss:  1.43717951e-02, mean val. rec. loss:  1.34411958e-02\n",
      "Epoch: 4613 mean train loss:  1.43706936e-02, mean val. rec. loss:  1.34402886e-02\n",
      "Epoch: 4614 mean train loss:  1.43695884e-02, mean val. rec. loss:  1.34393871e-02\n",
      "Epoch: 4615 mean train loss:  1.43684842e-02, mean val. rec. loss:  1.34384845e-02\n",
      "Epoch: 4616 mean train loss:  1.43673855e-02, mean val. rec. loss:  1.34375830e-02\n",
      "Epoch: 4617 mean train loss:  1.43662840e-02, mean val. rec. loss:  1.34366746e-02\n",
      "Epoch: 4618 mean train loss:  1.43651769e-02, mean val. rec. loss:  1.34357641e-02\n",
      "Epoch: 4619 mean train loss:  1.43640736e-02, mean val. rec. loss:  1.34348603e-02\n",
      "Epoch: 4620 mean train loss:  1.43629739e-02, mean val. rec. loss:  1.34339497e-02\n",
      "Epoch: 4621 mean train loss:  1.43618715e-02, mean val. rec. loss:  1.34330414e-02\n",
      "Epoch: 4622 mean train loss:  1.43607644e-02, mean val. rec. loss:  1.34321342e-02\n",
      "Epoch: 4623 mean train loss:  1.43596592e-02, mean val. rec. loss:  1.34312293e-02\n",
      "Epoch: 4624 mean train loss:  1.43585596e-02, mean val. rec. loss:  1.34303266e-02\n",
      "Epoch: 4625 mean train loss:  1.43574563e-02, mean val. rec. loss:  1.34294195e-02\n",
      "Epoch: 4626 mean train loss:  1.43563483e-02, mean val. rec. loss:  1.34285077e-02\n",
      "Epoch: 4627 mean train loss:  1.43552431e-02, mean val. rec. loss:  1.34276040e-02\n",
      "Epoch: 4628 mean train loss:  1.43541425e-02, mean val. rec. loss:  1.34266934e-02\n",
      "Epoch: 4629 mean train loss:  1.43530382e-02, mean val. rec. loss:  1.34257817e-02\n",
      "Epoch: 4630 mean train loss:  1.43519302e-02, mean val. rec. loss:  1.34248756e-02\n",
      "Epoch: 4631 mean train loss:  1.43508241e-02, mean val. rec. loss:  1.34239696e-02\n",
      "Epoch: 4632 mean train loss:  1.43497226e-02, mean val. rec. loss:  1.34230613e-02\n",
      "Epoch: 4633 mean train loss:  1.43486183e-02, mean val. rec. loss:  1.34221495e-02\n",
      "Epoch: 4634 mean train loss:  1.43475075e-02, mean val. rec. loss:  1.34212356e-02\n",
      "Epoch: 4635 mean train loss:  1.43464014e-02, mean val. rec. loss:  1.34203295e-02\n",
      "Epoch: 4636 mean train loss:  1.43452990e-02, mean val. rec. loss:  1.34194155e-02\n",
      "Epoch: 4637 mean train loss:  1.43441947e-02, mean val. rec. loss:  1.34185015e-02\n",
      "Epoch: 4638 mean train loss:  1.43430829e-02, mean val. rec. loss:  1.34175921e-02\n",
      "Epoch: 4639 mean train loss:  1.43419768e-02, mean val. rec. loss:  1.34166849e-02\n",
      "Epoch: 4640 mean train loss:  1.43408725e-02, mean val. rec. loss:  1.34157755e-02\n",
      "Epoch: 4641 mean train loss:  1.43397664e-02, mean val. rec. loss:  1.34148615e-02\n",
      "Epoch: 4642 mean train loss:  1.43386547e-02, mean val. rec. loss:  1.34139498e-02\n",
      "Epoch: 4643 mean train loss:  1.43375467e-02, mean val. rec. loss:  1.34130392e-02\n",
      "Epoch: 4644 mean train loss:  1.43364424e-02, mean val. rec. loss:  1.34121241e-02\n",
      "Epoch: 4645 mean train loss:  1.43353362e-02, mean val. rec. loss:  1.34112078e-02\n",
      "Epoch: 4646 mean train loss:  1.43342226e-02, mean val. rec. loss:  1.34102961e-02\n",
      "Epoch: 4647 mean train loss:  1.43331137e-02, mean val. rec. loss:  1.34093855e-02\n",
      "Epoch: 4648 mean train loss:  1.43320094e-02, mean val. rec. loss:  1.34084726e-02\n",
      "Epoch: 4649 mean train loss:  1.43309005e-02, mean val. rec. loss:  1.34075575e-02\n",
      "Epoch: 4650 mean train loss:  1.43297888e-02, mean val. rec. loss:  1.34066424e-02\n",
      "Epoch: 4651 mean train loss:  1.43286780e-02, mean val. rec. loss:  1.34057307e-02\n",
      "Epoch: 4652 mean train loss:  1.43275737e-02, mean val. rec. loss:  1.34048156e-02\n",
      "Epoch: 4653 mean train loss:  1.43264648e-02, mean val. rec. loss:  1.34038959e-02\n",
      "Epoch: 4654 mean train loss:  1.43253512e-02, mean val. rec. loss:  1.34029842e-02\n",
      "Epoch: 4655 mean train loss:  1.43242394e-02, mean val. rec. loss:  1.34020702e-02\n",
      "Epoch: 4656 mean train loss:  1.43231324e-02, mean val. rec. loss:  1.34011596e-02\n",
      "Epoch: 4657 mean train loss:  1.43220244e-02, mean val. rec. loss:  1.34002422e-02\n",
      "Epoch: 4658 mean train loss:  1.43209108e-02, mean val. rec. loss:  1.33993215e-02\n",
      "Epoch: 4659 mean train loss:  1.43198000e-02, mean val. rec. loss:  1.33984097e-02\n",
      "Epoch: 4660 mean train loss:  1.43186920e-02, mean val. rec. loss:  1.33974867e-02\n",
      "Epoch: 4661 mean train loss:  1.43175812e-02, mean val. rec. loss:  1.33965682e-02\n",
      "Epoch: 4662 mean train loss:  1.43164657e-02, mean val. rec. loss:  1.33956485e-02\n",
      "Epoch: 4663 mean train loss:  1.43153512e-02, mean val. rec. loss:  1.33947345e-02\n",
      "Epoch: 4664 mean train loss:  1.43142441e-02, mean val. rec. loss:  1.33938126e-02\n",
      "Epoch: 4665 mean train loss:  1.43131278e-02, mean val. rec. loss:  1.33928997e-02\n",
      "Epoch: 4666 mean train loss:  1.43120216e-02, mean val. rec. loss:  1.33919801e-02\n",
      "Epoch: 4667 mean train loss:  1.43109043e-02, mean val. rec. loss:  1.33910661e-02\n",
      "Epoch: 4668 mean train loss:  1.43097963e-02, mean val. rec. loss:  1.33901453e-02\n",
      "Epoch: 4669 mean train loss:  1.43086780e-02, mean val. rec. loss:  1.33892325e-02\n",
      "Epoch: 4670 mean train loss:  1.43075710e-02, mean val. rec. loss:  1.33883105e-02\n",
      "Epoch: 4671 mean train loss:  1.43064518e-02, mean val. rec. loss:  1.33873943e-02\n",
      "Epoch: 4672 mean train loss:  1.43053438e-02, mean val. rec. loss:  1.33864758e-02\n",
      "Epoch: 4673 mean train loss:  1.43042265e-02, mean val. rec. loss:  1.33855572e-02\n",
      "Epoch: 4674 mean train loss:  1.43031175e-02, mean val. rec. loss:  1.33846365e-02\n",
      "Epoch: 4675 mean train loss:  1.43019984e-02, mean val. rec. loss:  1.33837145e-02\n",
      "Epoch: 4676 mean train loss:  1.43008885e-02, mean val. rec. loss:  1.33827926e-02\n",
      "Epoch: 4677 mean train loss:  1.42997693e-02, mean val. rec. loss:  1.33818718e-02\n",
      "Epoch: 4678 mean train loss:  1.42986604e-02, mean val. rec. loss:  1.33809510e-02\n",
      "Epoch: 4679 mean train loss:  1.42975403e-02, mean val. rec. loss:  1.33800302e-02\n",
      "Epoch: 4680 mean train loss:  1.42964304e-02, mean val. rec. loss:  1.33791072e-02\n",
      "Epoch: 4681 mean train loss:  1.42953103e-02, mean val. rec. loss:  1.33781887e-02\n",
      "Epoch: 4682 mean train loss:  1.42941986e-02, mean val. rec. loss:  1.33772656e-02\n",
      "Epoch: 4683 mean train loss:  1.42930775e-02, mean val. rec. loss:  1.33763426e-02\n",
      "Epoch: 4684 mean train loss:  1.42919667e-02, mean val. rec. loss:  1.33754195e-02\n",
      "Epoch: 4685 mean train loss:  1.42908466e-02, mean val. rec. loss:  1.33744987e-02\n",
      "Epoch: 4686 mean train loss:  1.42897349e-02, mean val. rec. loss:  1.33735734e-02\n",
      "Epoch: 4687 mean train loss:  1.42886129e-02, mean val. rec. loss:  1.33726515e-02\n",
      "Epoch: 4688 mean train loss:  1.42875003e-02, mean val. rec. loss:  1.33717250e-02\n",
      "Epoch: 4689 mean train loss:  1.42863792e-02, mean val. rec. loss:  1.33708042e-02\n",
      "Epoch: 4690 mean train loss:  1.42852666e-02, mean val. rec. loss:  1.33698766e-02\n",
      "Epoch: 4691 mean train loss:  1.42841446e-02, mean val. rec. loss:  1.33689524e-02\n",
      "Epoch: 4692 mean train loss:  1.42830310e-02, mean val. rec. loss:  1.33680260e-02\n",
      "Epoch: 4693 mean train loss:  1.42819081e-02, mean val. rec. loss:  1.33671029e-02\n",
      "Epoch: 4694 mean train loss:  1.42807945e-02, mean val. rec. loss:  1.33661753e-02\n",
      "Epoch: 4695 mean train loss:  1.42796716e-02, mean val. rec. loss:  1.33652523e-02\n",
      "Epoch: 4696 mean train loss:  1.42785590e-02, mean val. rec. loss:  1.33643247e-02\n",
      "Epoch: 4697 mean train loss:  1.42774351e-02, mean val. rec. loss:  1.33633993e-02\n",
      "Epoch: 4698 mean train loss:  1.42763206e-02, mean val. rec. loss:  1.33624718e-02\n",
      "Epoch: 4699 mean train loss:  1.42751968e-02, mean val. rec. loss:  1.33615487e-02\n",
      "Epoch: 4700 mean train loss:  1.42740823e-02, mean val. rec. loss:  1.33606154e-02\n",
      "Epoch: 4701 mean train loss:  1.42729575e-02, mean val. rec. loss:  1.33596901e-02\n",
      "Epoch: 4702 mean train loss:  1.42718430e-02, mean val. rec. loss:  1.33587591e-02\n",
      "Epoch: 4703 mean train loss:  1.42707164e-02, mean val. rec. loss:  1.33578315e-02\n",
      "Epoch: 4704 mean train loss:  1.42696009e-02, mean val. rec. loss:  1.33569028e-02\n",
      "Epoch: 4705 mean train loss:  1.42684761e-02, mean val. rec. loss:  1.33559752e-02\n",
      "Epoch: 4706 mean train loss:  1.42673598e-02, mean val. rec. loss:  1.33550465e-02\n",
      "Epoch: 4707 mean train loss:  1.42662341e-02, mean val. rec. loss:  1.33541189e-02\n",
      "Epoch: 4708 mean train loss:  1.42651177e-02, mean val. rec. loss:  1.33531868e-02\n",
      "Epoch: 4709 mean train loss:  1.42639911e-02, mean val. rec. loss:  1.33522626e-02\n",
      "Epoch: 4710 mean train loss:  1.42628747e-02, mean val. rec. loss:  1.33513293e-02\n",
      "Epoch: 4711 mean train loss:  1.42617481e-02, mean val. rec. loss:  1.33503983e-02\n",
      "Epoch: 4712 mean train loss:  1.42606298e-02, mean val. rec. loss:  1.33494639e-02\n",
      "Epoch: 4713 mean train loss:  1.42595032e-02, mean val. rec. loss:  1.33485386e-02\n",
      "Epoch: 4714 mean train loss:  1.42583849e-02, mean val. rec. loss:  1.33476031e-02\n",
      "Epoch: 4715 mean train loss:  1.42572583e-02, mean val. rec. loss:  1.33466732e-02\n",
      "Epoch: 4716 mean train loss:  1.42561391e-02, mean val. rec. loss:  1.33457377e-02\n",
      "Epoch: 4717 mean train loss:  1.42550106e-02, mean val. rec. loss:  1.33448078e-02\n",
      "Epoch: 4718 mean train loss:  1.42538915e-02, mean val. rec. loss:  1.33438711e-02\n",
      "Epoch: 4719 mean train loss:  1.42527630e-02, mean val. rec. loss:  1.33429469e-02\n",
      "Epoch: 4720 mean train loss:  1.42516419e-02, mean val. rec. loss:  1.33420125e-02\n",
      "Epoch: 4721 mean train loss:  1.42505190e-02, mean val. rec. loss:  1.33410827e-02\n",
      "Epoch: 4722 mean train loss:  1.42493934e-02, mean val. rec. loss:  1.33401506e-02\n",
      "Epoch: 4723 mean train loss:  1.42482658e-02, mean val. rec. loss:  1.33392162e-02\n",
      "Epoch: 4724 mean train loss:  1.42471429e-02, mean val. rec. loss:  1.33382897e-02\n",
      "Epoch: 4725 mean train loss:  1.42460191e-02, mean val. rec. loss:  1.33373553e-02\n",
      "Epoch: 4726 mean train loss:  1.42448915e-02, mean val. rec. loss:  1.33364186e-02\n",
      "Epoch: 4727 mean train loss:  1.42437630e-02, mean val. rec. loss:  1.33354956e-02\n",
      "Epoch: 4728 mean train loss:  1.42426401e-02, mean val. rec. loss:  1.33345533e-02\n",
      "Epoch: 4729 mean train loss:  1.42415191e-02, mean val. rec. loss:  1.33336177e-02\n",
      "Epoch: 4730 mean train loss:  1.42403878e-02, mean val. rec. loss:  1.33326777e-02\n",
      "Epoch: 4731 mean train loss:  1.42392574e-02, mean val. rec. loss:  1.33317410e-02\n",
      "Epoch: 4732 mean train loss:  1.42381345e-02, mean val. rec. loss:  1.33308089e-02\n",
      "Epoch: 4733 mean train loss:  1.42370098e-02, mean val. rec. loss:  1.33298699e-02\n",
      "Epoch: 4734 mean train loss:  1.42358794e-02, mean val. rec. loss:  1.33289310e-02\n",
      "Epoch: 4735 mean train loss:  1.42347472e-02, mean val. rec. loss:  1.33280011e-02\n",
      "Epoch: 4736 mean train loss:  1.42336234e-02, mean val. rec. loss:  1.33270611e-02\n",
      "Epoch: 4737 mean train loss:  1.42324996e-02, mean val. rec. loss:  1.33261267e-02\n",
      "Epoch: 4738 mean train loss:  1.42313683e-02, mean val. rec. loss:  1.33251821e-02\n",
      "Epoch: 4739 mean train loss:  1.42302342e-02, mean val. rec. loss:  1.33242431e-02\n",
      "Epoch: 4740 mean train loss:  1.42291085e-02, mean val. rec. loss:  1.33233087e-02\n",
      "Epoch: 4741 mean train loss:  1.42279847e-02, mean val. rec. loss:  1.33223698e-02\n",
      "Epoch: 4742 mean train loss:  1.42268515e-02, mean val. rec. loss:  1.33214286e-02\n",
      "Epoch: 4743 mean train loss:  1.42257165e-02, mean val. rec. loss:  1.33204965e-02\n",
      "Epoch: 4744 mean train loss:  1.42245918e-02, mean val. rec. loss:  1.33195507e-02\n",
      "Epoch: 4745 mean train loss:  1.42234651e-02, mean val. rec. loss:  1.33186129e-02\n",
      "Epoch: 4746 mean train loss:  1.42223311e-02, mean val. rec. loss:  1.33176661e-02\n",
      "Epoch: 4747 mean train loss:  1.42211970e-02, mean val. rec. loss:  1.33167226e-02\n",
      "Epoch: 4748 mean train loss:  1.42200704e-02, mean val. rec. loss:  1.33157859e-02\n",
      "Epoch: 4749 mean train loss:  1.42189419e-02, mean val. rec. loss:  1.33148447e-02\n",
      "Epoch: 4750 mean train loss:  1.42178078e-02, mean val. rec. loss:  1.33138967e-02\n",
      "Epoch: 4751 mean train loss:  1.42166719e-02, mean val. rec. loss:  1.33129669e-02\n",
      "Epoch: 4752 mean train loss:  1.42155443e-02, mean val. rec. loss:  1.33120189e-02\n",
      "Epoch: 4753 mean train loss:  1.42144168e-02, mean val. rec. loss:  1.33110754e-02\n",
      "Epoch: 4754 mean train loss:  1.42132799e-02, mean val. rec. loss:  1.33101297e-02\n",
      "Epoch: 4755 mean train loss:  1.42121440e-02, mean val. rec. loss:  1.33091862e-02\n",
      "Epoch: 4756 mean train loss:  1.42110145e-02, mean val. rec. loss:  1.33082473e-02\n",
      "Epoch: 4757 mean train loss:  1.42098860e-02, mean val. rec. loss:  1.33073027e-02\n",
      "Epoch: 4758 mean train loss:  1.42087482e-02, mean val. rec. loss:  1.33063558e-02\n",
      "Epoch: 4759 mean train loss:  1.42076114e-02, mean val. rec. loss:  1.33054191e-02\n",
      "Epoch: 4760 mean train loss:  1.42064820e-02, mean val. rec. loss:  1.33044711e-02\n",
      "Epoch: 4761 mean train loss:  1.42053498e-02, mean val. rec. loss:  1.33035231e-02\n",
      "Epoch: 4762 mean train loss:  1.42042138e-02, mean val. rec. loss:  1.33025728e-02\n",
      "Epoch: 4763 mean train loss:  1.42030742e-02, mean val. rec. loss:  1.33016271e-02\n",
      "Epoch: 4764 mean train loss:  1.42019438e-02, mean val. rec. loss:  1.33006870e-02\n",
      "Epoch: 4765 mean train loss:  1.42008116e-02, mean val. rec. loss:  1.32997424e-02\n",
      "Epoch: 4766 mean train loss:  1.41996719e-02, mean val. rec. loss:  1.32987922e-02\n",
      "Epoch: 4767 mean train loss:  1.41985341e-02, mean val. rec. loss:  1.32978532e-02\n",
      "Epoch: 4768 mean train loss:  1.41974010e-02, mean val. rec. loss:  1.32969030e-02\n",
      "Epoch: 4769 mean train loss:  1.41962678e-02, mean val. rec. loss:  1.32959527e-02\n",
      "Epoch: 4770 mean train loss:  1.41951291e-02, mean val. rec. loss:  1.32950024e-02\n",
      "Epoch: 4771 mean train loss:  1.41939885e-02, mean val. rec. loss:  1.32940533e-02\n",
      "Epoch: 4772 mean train loss:  1.41928545e-02, mean val. rec. loss:  1.32931087e-02\n",
      "Epoch: 4773 mean train loss:  1.41917222e-02, mean val. rec. loss:  1.32921584e-02\n",
      "Epoch: 4774 mean train loss:  1.41905807e-02, mean val. rec. loss:  1.32912127e-02\n",
      "Epoch: 4775 mean train loss:  1.41894411e-02, mean val. rec. loss:  1.32902681e-02\n",
      "Epoch: 4776 mean train loss:  1.41883051e-02, mean val. rec. loss:  1.32893201e-02\n",
      "Epoch: 4777 mean train loss:  1.41871673e-02, mean val. rec. loss:  1.32883709e-02\n",
      "Epoch: 4778 mean train loss:  1.41860277e-02, mean val. rec. loss:  1.32874195e-02\n",
      "Epoch: 4779 mean train loss:  1.41848880e-02, mean val. rec. loss:  1.32864715e-02\n",
      "Epoch: 4780 mean train loss:  1.41837502e-02, mean val. rec. loss:  1.32855280e-02\n",
      "Epoch: 4781 mean train loss:  1.41826133e-02, mean val. rec. loss:  1.32845744e-02\n",
      "Epoch: 4782 mean train loss:  1.41814700e-02, mean val. rec. loss:  1.32836252e-02\n",
      "Epoch: 4783 mean train loss:  1.41803340e-02, mean val. rec. loss:  1.32826716e-02\n",
      "Epoch: 4784 mean train loss:  1.41791916e-02, mean val. rec. loss:  1.32817213e-02\n",
      "Epoch: 4785 mean train loss:  1.41780538e-02, mean val. rec. loss:  1.32807642e-02\n",
      "Epoch: 4786 mean train loss:  1.41769094e-02, mean val. rec. loss:  1.32798139e-02\n",
      "Epoch: 4787 mean train loss:  1.41757726e-02, mean val. rec. loss:  1.32788614e-02\n",
      "Epoch: 4788 mean train loss:  1.41746292e-02, mean val. rec. loss:  1.32779111e-02\n",
      "Epoch: 4789 mean train loss:  1.41734914e-02, mean val. rec. loss:  1.32769563e-02\n",
      "Epoch: 4790 mean train loss:  1.41723461e-02, mean val. rec. loss:  1.32760060e-02\n",
      "Epoch: 4791 mean train loss:  1.41712074e-02, mean val. rec. loss:  1.32750490e-02\n",
      "Epoch: 4792 mean train loss:  1.41700622e-02, mean val. rec. loss:  1.32740976e-02\n",
      "Epoch: 4793 mean train loss:  1.41689234e-02, mean val. rec. loss:  1.32731416e-02\n",
      "Epoch: 4794 mean train loss:  1.41677782e-02, mean val. rec. loss:  1.32721879e-02\n",
      "Epoch: 4795 mean train loss:  1.41666385e-02, mean val. rec. loss:  1.32712320e-02\n",
      "Epoch: 4796 mean train loss:  1.41654924e-02, mean val. rec. loss:  1.32702772e-02\n",
      "Epoch: 4797 mean train loss:  1.41643518e-02, mean val. rec. loss:  1.32693201e-02\n",
      "Epoch: 4798 mean train loss:  1.41632047e-02, mean val. rec. loss:  1.32683664e-02\n",
      "Epoch: 4799 mean train loss:  1.41620650e-02, mean val. rec. loss:  1.32674105e-02\n",
      "Epoch: 4800 mean train loss:  1.41609179e-02, mean val. rec. loss:  1.32664557e-02\n",
      "Epoch: 4801 mean train loss:  1.41597764e-02, mean val. rec. loss:  1.32654952e-02\n",
      "Epoch: 4802 mean train loss:  1.41586283e-02, mean val. rec. loss:  1.32645415e-02\n",
      "Epoch: 4803 mean train loss:  1.41574859e-02, mean val. rec. loss:  1.32635822e-02\n",
      "Epoch: 4804 mean train loss:  1.41563388e-02, mean val. rec. loss:  1.32626274e-02\n",
      "Epoch: 4805 mean train loss:  1.41551954e-02, mean val. rec. loss:  1.32616635e-02\n",
      "Epoch: 4806 mean train loss:  1.41540473e-02, mean val. rec. loss:  1.32607064e-02\n",
      "Epoch: 4807 mean train loss:  1.41529039e-02, mean val. rec. loss:  1.32597459e-02\n",
      "Epoch: 4808 mean train loss:  1.41517540e-02, mean val. rec. loss:  1.32587866e-02\n",
      "Epoch: 4809 mean train loss:  1.41506116e-02, mean val. rec. loss:  1.32578261e-02\n",
      "Epoch: 4810 mean train loss:  1.41494617e-02, mean val. rec. loss:  1.32568690e-02\n",
      "Epoch: 4811 mean train loss:  1.41483164e-02, mean val. rec. loss:  1.32559074e-02\n",
      "Epoch: 4812 mean train loss:  1.41471675e-02, mean val. rec. loss:  1.32549481e-02\n",
      "Epoch: 4813 mean train loss:  1.41460232e-02, mean val. rec. loss:  1.32539842e-02\n",
      "Epoch: 4814 mean train loss:  1.41448723e-02, mean val. rec. loss:  1.32530260e-02\n",
      "Epoch: 4815 mean train loss:  1.41437261e-02, mean val. rec. loss:  1.32520610e-02\n",
      "Epoch: 4816 mean train loss:  1.41425753e-02, mean val. rec. loss:  1.32511016e-02\n",
      "Epoch: 4817 mean train loss:  1.41414301e-02, mean val. rec. loss:  1.32501389e-02\n",
      "Epoch: 4818 mean train loss:  1.41402774e-02, mean val. rec. loss:  1.32491762e-02\n",
      "Epoch: 4819 mean train loss:  1.41391312e-02, mean val. rec. loss:  1.32482111e-02\n",
      "Epoch: 4820 mean train loss:  1.41379794e-02, mean val. rec. loss:  1.32472507e-02\n",
      "Epoch: 4821 mean train loss:  1.41368342e-02, mean val. rec. loss:  1.32462879e-02\n",
      "Epoch: 4822 mean train loss:  1.41356796e-02, mean val. rec. loss:  1.32453240e-02\n",
      "Epoch: 4823 mean train loss:  1.41345325e-02, mean val. rec. loss:  1.32443568e-02\n",
      "Epoch: 4824 mean train loss:  1.41333798e-02, mean val. rec. loss:  1.32433940e-02\n",
      "Epoch: 4825 mean train loss:  1.41322308e-02, mean val. rec. loss:  1.32424290e-02\n",
      "Epoch: 4826 mean train loss:  1.41310763e-02, mean val. rec. loss:  1.32414674e-02\n",
      "Epoch: 4827 mean train loss:  1.41299273e-02, mean val. rec. loss:  1.32405001e-02\n",
      "Epoch: 4828 mean train loss:  1.41287774e-02, mean val. rec. loss:  1.32395283e-02\n",
      "Epoch: 4829 mean train loss:  1.41276238e-02, mean val. rec. loss:  1.32385621e-02\n",
      "Epoch: 4830 mean train loss:  1.41264711e-02, mean val. rec. loss:  1.32375971e-02\n",
      "Epoch: 4831 mean train loss:  1.41253221e-02, mean val. rec. loss:  1.32366321e-02\n",
      "Epoch: 4832 mean train loss:  1.41241685e-02, mean val. rec. loss:  1.32356682e-02\n",
      "Epoch: 4833 mean train loss:  1.41230111e-02, mean val. rec. loss:  1.32346998e-02\n",
      "Epoch: 4834 mean train loss:  1.41218603e-02, mean val. rec. loss:  1.32337337e-02\n",
      "Epoch: 4835 mean train loss:  1.41207104e-02, mean val. rec. loss:  1.32327698e-02\n",
      "Epoch: 4836 mean train loss:  1.41195549e-02, mean val. rec. loss:  1.32317968e-02\n",
      "Epoch: 4837 mean train loss:  1.41183985e-02, mean val. rec. loss:  1.32308329e-02\n",
      "Epoch: 4838 mean train loss:  1.41172458e-02, mean val. rec. loss:  1.32298668e-02\n",
      "Epoch: 4839 mean train loss:  1.41160931e-02, mean val. rec. loss:  1.32288961e-02\n",
      "Epoch: 4840 mean train loss:  1.41149385e-02, mean val. rec. loss:  1.32279277e-02\n",
      "Epoch: 4841 mean train loss:  1.41137793e-02, mean val. rec. loss:  1.32269570e-02\n",
      "Epoch: 4842 mean train loss:  1.41126257e-02, mean val. rec. loss:  1.32259897e-02\n",
      "Epoch: 4843 mean train loss:  1.41114730e-02, mean val. rec. loss:  1.32250225e-02\n",
      "Epoch: 4844 mean train loss:  1.41103166e-02, mean val. rec. loss:  1.32240438e-02\n",
      "Epoch: 4845 mean train loss:  1.41091564e-02, mean val. rec. loss:  1.32230743e-02\n",
      "Epoch: 4846 mean train loss:  1.41080009e-02, mean val. rec. loss:  1.32221059e-02\n",
      "Epoch: 4847 mean train loss:  1.41068473e-02, mean val. rec. loss:  1.32211329e-02\n",
      "Epoch: 4848 mean train loss:  1.41056900e-02, mean val. rec. loss:  1.32201634e-02\n",
      "Epoch: 4849 mean train loss:  1.41045289e-02, mean val. rec. loss:  1.32191927e-02\n",
      "Epoch: 4850 mean train loss:  1.41033725e-02, mean val. rec. loss:  1.32182209e-02\n",
      "Epoch: 4851 mean train loss:  1.41022179e-02, mean val. rec. loss:  1.32172524e-02\n",
      "Epoch: 4852 mean train loss:  1.41010596e-02, mean val. rec. loss:  1.32162761e-02\n",
      "Epoch: 4853 mean train loss:  1.40998976e-02, mean val. rec. loss:  1.32152997e-02\n",
      "Epoch: 4854 mean train loss:  1.40987393e-02, mean val. rec. loss:  1.32143313e-02\n",
      "Epoch: 4855 mean train loss:  1.40975848e-02, mean val. rec. loss:  1.32133538e-02\n",
      "Epoch: 4856 mean train loss:  1.40964246e-02, mean val. rec. loss:  1.32123809e-02\n",
      "Epoch: 4857 mean train loss:  1.40952608e-02, mean val. rec. loss:  1.32114057e-02\n",
      "Epoch: 4858 mean train loss:  1.40941025e-02, mean val. rec. loss:  1.32104327e-02\n",
      "Epoch: 4859 mean train loss:  1.40929461e-02, mean val. rec. loss:  1.32094586e-02\n",
      "Epoch: 4860 mean train loss:  1.40917859e-02, mean val. rec. loss:  1.32084743e-02\n",
      "Epoch: 4861 mean train loss:  1.40906202e-02, mean val. rec. loss:  1.32075025e-02\n",
      "Epoch: 4862 mean train loss:  1.40894610e-02, mean val. rec. loss:  1.32065296e-02\n",
      "Epoch: 4863 mean train loss:  1.40883036e-02, mean val. rec. loss:  1.32055521e-02\n",
      "Epoch: 4864 mean train loss:  1.40871416e-02, mean val. rec. loss:  1.32045757e-02\n",
      "Epoch: 4865 mean train loss:  1.40859759e-02, mean val. rec. loss:  1.32036028e-02\n",
      "Epoch: 4866 mean train loss:  1.40848167e-02, mean val. rec. loss:  1.32026241e-02\n",
      "Epoch: 4867 mean train loss:  1.40836575e-02, mean val. rec. loss:  1.32016467e-02\n",
      "Epoch: 4868 mean train loss:  1.40824917e-02, mean val. rec. loss:  1.32006624e-02\n",
      "Epoch: 4869 mean train loss:  1.40813269e-02, mean val. rec. loss:  1.31996883e-02\n",
      "Epoch: 4870 mean train loss:  1.40801659e-02, mean val. rec. loss:  1.31987074e-02\n",
      "Epoch: 4871 mean train loss:  1.40790057e-02, mean val. rec. loss:  1.31977265e-02\n",
      "Epoch: 4872 mean train loss:  1.40778400e-02, mean val. rec. loss:  1.31967445e-02\n",
      "Epoch: 4873 mean train loss:  1.40766733e-02, mean val. rec. loss:  1.31957670e-02\n",
      "Epoch: 4874 mean train loss:  1.40755122e-02, mean val. rec. loss:  1.31947838e-02\n",
      "Epoch: 4875 mean train loss:  1.40743502e-02, mean val. rec. loss:  1.31938052e-02\n",
      "Epoch: 4876 mean train loss:  1.40731836e-02, mean val. rec. loss:  1.31928186e-02\n",
      "Epoch: 4877 mean train loss:  1.40720160e-02, mean val. rec. loss:  1.31918400e-02\n",
      "Epoch: 4878 mean train loss:  1.40708530e-02, mean val. rec. loss:  1.31908603e-02\n",
      "Epoch: 4879 mean train loss:  1.40696910e-02, mean val. rec. loss:  1.31898748e-02\n",
      "Epoch: 4880 mean train loss:  1.40685216e-02, mean val. rec. loss:  1.31888917e-02\n",
      "Epoch: 4881 mean train loss:  1.40673531e-02, mean val. rec. loss:  1.31879108e-02\n",
      "Epoch: 4882 mean train loss:  1.40661892e-02, mean val. rec. loss:  1.31869276e-02\n",
      "Epoch: 4883 mean train loss:  1.40650262e-02, mean val. rec. loss:  1.31859479e-02\n",
      "Epoch: 4884 mean train loss:  1.40638559e-02, mean val. rec. loss:  1.31849591e-02\n",
      "Epoch: 4885 mean train loss:  1.40626864e-02, mean val. rec. loss:  1.31839782e-02\n",
      "Epoch: 4886 mean train loss:  1.40615216e-02, mean val. rec. loss:  1.31829916e-02\n",
      "Epoch: 4887 mean train loss:  1.40603559e-02, mean val. rec. loss:  1.31820016e-02\n",
      "Epoch: 4888 mean train loss:  1.40591864e-02, mean val. rec. loss:  1.31810208e-02\n",
      "Epoch: 4889 mean train loss:  1.40580179e-02, mean val. rec. loss:  1.31800421e-02\n",
      "Epoch: 4890 mean train loss:  1.40568494e-02, mean val. rec. loss:  1.31790556e-02\n",
      "Epoch: 4891 mean train loss:  1.40556808e-02, mean val. rec. loss:  1.31780679e-02\n",
      "Epoch: 4892 mean train loss:  1.40545114e-02, mean val. rec. loss:  1.31770859e-02\n",
      "Epoch: 4893 mean train loss:  1.40533410e-02, mean val. rec. loss:  1.31761016e-02\n",
      "Epoch: 4894 mean train loss:  1.40521734e-02, mean val. rec. loss:  1.31751116e-02\n",
      "Epoch: 4895 mean train loss:  1.40510040e-02, mean val. rec. loss:  1.31741250e-02\n",
      "Epoch: 4896 mean train loss:  1.40498326e-02, mean val. rec. loss:  1.31731408e-02\n",
      "Epoch: 4897 mean train loss:  1.40486613e-02, mean val. rec. loss:  1.31721553e-02\n",
      "Epoch: 4898 mean train loss:  1.40474919e-02, mean val. rec. loss:  1.31711654e-02\n",
      "Epoch: 4899 mean train loss:  1.40463205e-02, mean val. rec. loss:  1.31701720e-02\n",
      "Epoch: 4900 mean train loss:  1.40451474e-02, mean val. rec. loss:  1.31691854e-02\n",
      "Epoch: 4901 mean train loss:  1.40439779e-02, mean val. rec. loss:  1.31682034e-02\n",
      "Epoch: 4902 mean train loss:  1.40428066e-02, mean val. rec. loss:  1.31672112e-02\n",
      "Epoch: 4903 mean train loss:  1.40416334e-02, mean val. rec. loss:  1.31662190e-02\n",
      "Epoch: 4904 mean train loss:  1.40404593e-02, mean val. rec. loss:  1.31652324e-02\n",
      "Epoch: 4905 mean train loss:  1.40392880e-02, mean val. rec. loss:  1.31642481e-02\n",
      "Epoch: 4906 mean train loss:  1.40381176e-02, mean val. rec. loss:  1.31632491e-02\n",
      "Epoch: 4907 mean train loss:  1.40369426e-02, mean val. rec. loss:  1.31622568e-02\n",
      "Epoch: 4908 mean train loss:  1.40357675e-02, mean val. rec. loss:  1.31612692e-02\n",
      "Epoch: 4909 mean train loss:  1.40345962e-02, mean val. rec. loss:  1.31602803e-02\n",
      "Epoch: 4910 mean train loss:  1.40334239e-02, mean val. rec. loss:  1.31592858e-02\n",
      "Epoch: 4911 mean train loss:  1.40322470e-02, mean val. rec. loss:  1.31582902e-02\n",
      "Epoch: 4912 mean train loss:  1.40310711e-02, mean val. rec. loss:  1.31573014e-02\n",
      "Epoch: 4913 mean train loss:  1.40298979e-02, mean val. rec. loss:  1.31563103e-02\n",
      "Epoch: 4914 mean train loss:  1.40287247e-02, mean val. rec. loss:  1.31553146e-02\n",
      "Epoch: 4915 mean train loss:  1.40275478e-02, mean val. rec. loss:  1.31543167e-02\n",
      "Epoch: 4916 mean train loss:  1.40263709e-02, mean val. rec. loss:  1.31533290e-02\n",
      "Epoch: 4917 mean train loss:  1.40251959e-02, mean val. rec. loss:  1.31523391e-02\n",
      "Epoch: 4918 mean train loss:  1.40240208e-02, mean val. rec. loss:  1.31513412e-02\n",
      "Epoch: 4919 mean train loss:  1.40228448e-02, mean val. rec. loss:  1.31503410e-02\n",
      "Epoch: 4920 mean train loss:  1.40216670e-02, mean val. rec. loss:  1.31493511e-02\n",
      "Epoch: 4921 mean train loss:  1.40204901e-02, mean val. rec. loss:  1.31483577e-02\n",
      "Epoch: 4922 mean train loss:  1.40193151e-02, mean val. rec. loss:  1.31473575e-02\n",
      "Epoch: 4923 mean train loss:  1.40181354e-02, mean val. rec. loss:  1.31463585e-02\n",
      "Epoch: 4924 mean train loss:  1.40169566e-02, mean val. rec. loss:  1.31453651e-02\n",
      "Epoch: 4925 mean train loss:  1.40157797e-02, mean val. rec. loss:  1.31443729e-02\n",
      "Epoch: 4926 mean train loss:  1.40146019e-02, mean val. rec. loss:  1.31433727e-02\n",
      "Epoch: 4927 mean train loss:  1.40134222e-02, mean val. rec. loss:  1.31423703e-02\n",
      "Epoch: 4928 mean train loss:  1.40122406e-02, mean val. rec. loss:  1.31413803e-02\n",
      "Epoch: 4929 mean train loss:  1.40110628e-02, mean val. rec. loss:  1.31403802e-02\n",
      "Epoch: 4930 mean train loss:  1.40098831e-02, mean val. rec. loss:  1.31393800e-02\n",
      "Epoch: 4931 mean train loss:  1.40087043e-02, mean val. rec. loss:  1.31383776e-02\n",
      "Epoch: 4932 mean train loss:  1.40075237e-02, mean val. rec. loss:  1.31373808e-02\n",
      "Epoch: 4933 mean train loss:  1.40063430e-02, mean val. rec. loss:  1.31363840e-02\n",
      "Epoch: 4934 mean train loss:  1.40051615e-02, mean val. rec. loss:  1.31353771e-02\n",
      "Epoch: 4935 mean train loss:  1.40039818e-02, mean val. rec. loss:  1.31343746e-02\n",
      "Epoch: 4936 mean train loss:  1.40028002e-02, mean val. rec. loss:  1.31333790e-02\n",
      "Epoch: 4937 mean train loss:  1.40016187e-02, mean val. rec. loss:  1.31323788e-02\n",
      "Epoch: 4938 mean train loss:  1.40004371e-02, mean val. rec. loss:  1.31313696e-02\n",
      "Epoch: 4939 mean train loss:  1.39992555e-02, mean val. rec. loss:  1.31303672e-02\n",
      "Epoch: 4940 mean train loss:  1.39980740e-02, mean val. rec. loss:  1.31293704e-02\n",
      "Epoch: 4941 mean train loss:  1.39968906e-02, mean val. rec. loss:  1.31283702e-02\n",
      "Epoch: 4942 mean train loss:  1.39957081e-02, mean val. rec. loss:  1.31273644e-02\n",
      "Epoch: 4943 mean train loss:  1.39945274e-02, mean val. rec. loss:  1.31263665e-02\n",
      "Epoch: 4944 mean train loss:  1.39933422e-02, mean val. rec. loss:  1.31253641e-02\n",
      "Epoch: 4945 mean train loss:  1.39921559e-02, mean val. rec. loss:  1.31243593e-02\n",
      "Epoch: 4946 mean train loss:  1.39909744e-02, mean val. rec. loss:  1.31233569e-02\n",
      "Epoch: 4947 mean train loss:  1.39897919e-02, mean val. rec. loss:  1.31223556e-02\n",
      "Epoch: 4948 mean train loss:  1.39886057e-02, mean val. rec. loss:  1.31213520e-02\n",
      "Epoch: 4949 mean train loss:  1.39874195e-02, mean val. rec. loss:  1.31203519e-02\n",
      "Epoch: 4950 mean train loss:  1.39862370e-02, mean val. rec. loss:  1.31193426e-02\n",
      "Epoch: 4951 mean train loss:  1.39850526e-02, mean val. rec. loss:  1.31183402e-02\n",
      "Epoch: 4952 mean train loss:  1.39838655e-02, mean val. rec. loss:  1.31173332e-02\n",
      "Epoch: 4953 mean train loss:  1.39826774e-02, mean val. rec. loss:  1.31163263e-02\n",
      "Epoch: 4954 mean train loss:  1.39814930e-02, mean val. rec. loss:  1.31153238e-02\n",
      "Epoch: 4955 mean train loss:  1.39803115e-02, mean val. rec. loss:  1.31143191e-02\n",
      "Epoch: 4956 mean train loss:  1.39791215e-02, mean val. rec. loss:  1.31133087e-02\n",
      "Epoch: 4957 mean train loss:  1.39779288e-02, mean val. rec. loss:  1.31123086e-02\n",
      "Epoch: 4958 mean train loss:  1.39767463e-02, mean val. rec. loss:  1.31112982e-02\n",
      "Epoch: 4959 mean train loss:  1.39755629e-02, mean val. rec. loss:  1.31102867e-02\n",
      "Epoch: 4960 mean train loss:  1.39743711e-02, mean val. rec. loss:  1.31092797e-02\n",
      "Epoch: 4961 mean train loss:  1.39731793e-02, mean val. rec. loss:  1.31082739e-02\n",
      "Epoch: 4962 mean train loss:  1.39719959e-02, mean val. rec. loss:  1.31072681e-02\n",
      "Epoch: 4963 mean train loss:  1.39708106e-02, mean val. rec. loss:  1.31062588e-02\n",
      "Epoch: 4964 mean train loss:  1.39696169e-02, mean val. rec. loss:  1.31052462e-02\n",
      "Epoch: 4965 mean train loss:  1.39684242e-02, mean val. rec. loss:  1.31042426e-02\n",
      "Epoch: 4966 mean train loss:  1.39672389e-02, mean val. rec. loss:  1.31032300e-02\n",
      "Epoch: 4967 mean train loss:  1.39660545e-02, mean val. rec. loss:  1.31022173e-02\n",
      "Epoch: 4968 mean train loss:  1.39648609e-02, mean val. rec. loss:  1.31012058e-02\n",
      "Epoch: 4969 mean train loss:  1.39636663e-02, mean val. rec. loss:  1.31001943e-02\n",
      "Epoch: 4970 mean train loss:  1.39624801e-02, mean val. rec. loss:  1.30991851e-02\n",
      "Epoch: 4971 mean train loss:  1.39612929e-02, mean val. rec. loss:  1.30981713e-02\n",
      "Epoch: 4972 mean train loss:  1.39600974e-02, mean val. rec. loss:  1.30971564e-02\n",
      "Epoch: 4973 mean train loss:  1.39589028e-02, mean val. rec. loss:  1.30961505e-02\n",
      "Epoch: 4974 mean train loss:  1.39577147e-02, mean val. rec. loss:  1.30951390e-02\n",
      "Epoch: 4975 mean train loss:  1.39565276e-02, mean val. rec. loss:  1.30941207e-02\n",
      "Epoch: 4976 mean train loss:  1.39553311e-02, mean val. rec. loss:  1.30931081e-02\n",
      "Epoch: 4977 mean train loss:  1.39541347e-02, mean val. rec. loss:  1.30920954e-02\n",
      "Epoch: 4978 mean train loss:  1.39529466e-02, mean val. rec. loss:  1.30910851e-02\n",
      "Epoch: 4979 mean train loss:  1.39517566e-02, mean val. rec. loss:  1.30900747e-02\n",
      "Epoch: 4980 mean train loss:  1.39505602e-02, mean val. rec. loss:  1.30890496e-02\n",
      "Epoch: 4981 mean train loss:  1.39493628e-02, mean val. rec. loss:  1.30880403e-02\n",
      "Epoch: 4982 mean train loss:  1.39481701e-02, mean val. rec. loss:  1.30870232e-02\n",
      "Epoch: 4983 mean train loss:  1.39469792e-02, mean val. rec. loss:  1.30860083e-02\n",
      "Epoch: 4984 mean train loss:  1.39457827e-02, mean val. rec. loss:  1.30849877e-02\n",
      "Epoch: 4985 mean train loss:  1.39445854e-02, mean val. rec. loss:  1.30839694e-02\n",
      "Epoch: 4986 mean train loss:  1.39433945e-02, mean val. rec. loss:  1.30829579e-02\n",
      "Epoch: 4987 mean train loss:  1.39422017e-02, mean val. rec. loss:  1.30819407e-02\n",
      "Epoch: 4988 mean train loss:  1.39410044e-02, mean val. rec. loss:  1.30809190e-02\n",
      "Epoch: 4989 mean train loss:  1.39398051e-02, mean val. rec. loss:  1.30799075e-02\n",
      "Epoch: 4990 mean train loss:  1.39386114e-02, mean val. rec. loss:  1.30788869e-02\n",
      "Epoch: 4991 mean train loss:  1.39374178e-02, mean val. rec. loss:  1.30778686e-02\n",
      "Epoch: 4992 mean train loss:  1.39362204e-02, mean val. rec. loss:  1.30768491e-02\n",
      "Epoch: 4993 mean train loss:  1.39350211e-02, mean val. rec. loss:  1.30758297e-02\n",
      "Epoch: 4994 mean train loss:  1.39338265e-02, mean val. rec. loss:  1.30748193e-02\n",
      "Epoch: 4995 mean train loss:  1.39326301e-02, mean val. rec. loss:  1.30737976e-02\n",
      "Epoch: 4996 mean train loss:  1.39314327e-02, mean val. rec. loss:  1.30727838e-02\n",
      "Epoch: 4997 mean train loss:  1.39302353e-02, mean val. rec. loss:  1.30717587e-02\n",
      "Epoch: 4998 mean train loss:  1.39290370e-02, mean val. rec. loss:  1.30707461e-02\n",
      "Epoch: 4999 mean train loss:  1.39278396e-02, mean val. rec. loss:  1.30697198e-02\n",
      "Epoch: 5000 mean train loss:  1.39266394e-02, mean val. rec. loss:  1.30687060e-02\n",
      "Epoch: 5001 mean train loss:  1.39254420e-02, mean val. rec. loss:  1.30676809e-02\n",
      "Epoch: 5002 mean train loss:  1.39242428e-02, mean val. rec. loss:  1.30666649e-02\n",
      "Epoch: 5003 mean train loss:  1.39230454e-02, mean val. rec. loss:  1.30656386e-02\n",
      "Epoch: 5004 mean train loss:  1.39218471e-02, mean val. rec. loss:  1.30646237e-02\n",
      "Epoch: 5005 mean train loss:  1.39206460e-02, mean val. rec. loss:  1.30635941e-02\n",
      "Epoch: 5006 mean train loss:  1.39194458e-02, mean val. rec. loss:  1.30625803e-02\n",
      "Epoch: 5007 mean train loss:  1.39182456e-02, mean val. rec. loss:  1.30615518e-02\n",
      "Epoch: 5008 mean train loss:  1.39170473e-02, mean val. rec. loss:  1.30605357e-02\n",
      "Epoch: 5009 mean train loss:  1.39158443e-02, mean val. rec. loss:  1.30595050e-02\n",
      "Epoch: 5010 mean train loss:  1.39146432e-02, mean val. rec. loss:  1.30584878e-02\n",
      "Epoch: 5011 mean train loss:  1.39134421e-02, mean val. rec. loss:  1.30574593e-02\n",
      "Epoch: 5012 mean train loss:  1.39122428e-02, mean val. rec. loss:  1.30564421e-02\n",
      "Epoch: 5013 mean train loss:  1.39110389e-02, mean val. rec. loss:  1.30554113e-02\n",
      "Epoch: 5014 mean train loss:  1.39098359e-02, mean val. rec. loss:  1.30543975e-02\n",
      "Epoch: 5015 mean train loss:  1.39086339e-02, mean val. rec. loss:  1.30533633e-02\n",
      "Epoch: 5016 mean train loss:  1.39074337e-02, mean val. rec. loss:  1.30523428e-02\n",
      "Epoch: 5017 mean train loss:  1.39062298e-02, mean val. rec. loss:  1.30513086e-02\n",
      "Epoch: 5018 mean train loss:  1.39050250e-02, mean val. rec. loss:  1.30502880e-02\n",
      "Epoch: 5019 mean train loss:  1.39038229e-02, mean val. rec. loss:  1.30492583e-02\n",
      "Epoch: 5020 mean train loss:  1.39026209e-02, mean val. rec. loss:  1.30482389e-02\n",
      "Epoch: 5021 mean train loss:  1.39014161e-02, mean val. rec. loss:  1.30472036e-02\n",
      "Epoch: 5022 mean train loss:  1.39002094e-02, mean val. rec. loss:  1.30461864e-02\n",
      "Epoch: 5023 mean train loss:  1.38990064e-02, mean val. rec. loss:  1.30451488e-02\n",
      "Epoch: 5024 mean train loss:  1.38978034e-02, mean val. rec. loss:  1.30441248e-02\n",
      "Epoch: 5025 mean train loss:  1.38965958e-02, mean val. rec. loss:  1.30430940e-02\n",
      "Epoch: 5026 mean train loss:  1.38953937e-02, mean val. rec. loss:  1.30420701e-02\n",
      "Epoch: 5027 mean train loss:  1.38941833e-02, mean val. rec. loss:  1.30410359e-02\n",
      "Epoch: 5028 mean train loss:  1.38929822e-02, mean val. rec. loss:  1.30400096e-02\n",
      "Epoch: 5029 mean train loss:  1.38917718e-02, mean val. rec. loss:  1.30389766e-02\n",
      "Epoch: 5030 mean train loss:  1.38905697e-02, mean val. rec. loss:  1.30379458e-02\n",
      "Epoch: 5031 mean train loss:  1.38893602e-02, mean val. rec. loss:  1.30369139e-02\n",
      "Epoch: 5032 mean train loss:  1.38881573e-02, mean val. rec. loss:  1.30358888e-02\n",
      "Epoch: 5033 mean train loss:  1.38869468e-02, mean val. rec. loss:  1.30348500e-02\n",
      "Epoch: 5034 mean train loss:  1.38857429e-02, mean val. rec. loss:  1.30338227e-02\n",
      "Epoch: 5035 mean train loss:  1.38845316e-02, mean val. rec. loss:  1.30327885e-02\n",
      "Epoch: 5036 mean train loss:  1.38833267e-02, mean val. rec. loss:  1.30317577e-02\n",
      "Epoch: 5037 mean train loss:  1.38821172e-02, mean val. rec. loss:  1.30307224e-02\n",
      "Epoch: 5038 mean train loss:  1.38809105e-02, mean val. rec. loss:  1.30296950e-02\n",
      "Epoch: 5039 mean train loss:  1.38796992e-02, mean val. rec. loss:  1.30286608e-02\n",
      "Epoch: 5040 mean train loss:  1.38784943e-02, mean val. rec. loss:  1.30276311e-02\n",
      "Epoch: 5041 mean train loss:  1.38772821e-02, mean val. rec. loss:  1.30265947e-02\n",
      "Epoch: 5042 mean train loss:  1.38760763e-02, mean val. rec. loss:  1.30255650e-02\n",
      "Epoch: 5043 mean train loss:  1.38748631e-02, mean val. rec. loss:  1.30245297e-02\n",
      "Epoch: 5044 mean train loss:  1.38736554e-02, mean val. rec. loss:  1.30234978e-02\n",
      "Epoch: 5045 mean train loss:  1.38724497e-02, mean val. rec. loss:  1.30224693e-02\n",
      "Epoch: 5046 mean train loss:  1.38712393e-02, mean val. rec. loss:  1.30214260e-02\n",
      "Epoch: 5047 mean train loss:  1.38700242e-02, mean val. rec. loss:  1.30203941e-02\n",
      "Epoch: 5048 mean train loss:  1.38688138e-02, mean val. rec. loss:  1.30193622e-02\n",
      "Epoch: 5049 mean train loss:  1.38676071e-02, mean val. rec. loss:  1.30183257e-02\n",
      "Epoch: 5050 mean train loss:  1.38663948e-02, mean val. rec. loss:  1.30172915e-02\n",
      "Epoch: 5051 mean train loss:  1.38651797e-02, mean val. rec. loss:  1.30162562e-02\n",
      "Epoch: 5052 mean train loss:  1.38639693e-02, mean val. rec. loss:  1.30152209e-02\n",
      "Epoch: 5053 mean train loss:  1.38627607e-02, mean val. rec. loss:  1.30141867e-02\n",
      "Epoch: 5054 mean train loss:  1.38615484e-02, mean val. rec. loss:  1.30131457e-02\n",
      "Epoch: 5055 mean train loss:  1.38603315e-02, mean val. rec. loss:  1.30121093e-02\n",
      "Epoch: 5056 mean train loss:  1.38591201e-02, mean val. rec. loss:  1.30110706e-02\n",
      "Epoch: 5057 mean train loss:  1.38579116e-02, mean val. rec. loss:  1.30100364e-02\n",
      "Epoch: 5058 mean train loss:  1.38566974e-02, mean val. rec. loss:  1.30090022e-02\n",
      "Epoch: 5059 mean train loss:  1.38554795e-02, mean val. rec. loss:  1.30079623e-02\n",
      "Epoch: 5060 mean train loss:  1.38542672e-02, mean val. rec. loss:  1.30069213e-02\n",
      "Epoch: 5061 mean train loss:  1.38530568e-02, mean val. rec. loss:  1.30058860e-02\n",
      "Epoch: 5062 mean train loss:  1.38518390e-02, mean val. rec. loss:  1.30048428e-02\n",
      "Epoch: 5063 mean train loss:  1.38506229e-02, mean val. rec. loss:  1.30038040e-02\n",
      "Epoch: 5064 mean train loss:  1.38494107e-02, mean val. rec. loss:  1.30027630e-02\n",
      "Epoch: 5065 mean train loss:  1.38481984e-02, mean val. rec. loss:  1.30017220e-02\n",
      "Epoch: 5066 mean train loss:  1.38469814e-02, mean val. rec. loss:  1.30006811e-02\n",
      "Epoch: 5067 mean train loss:  1.38457645e-02, mean val. rec. loss:  1.29996423e-02\n",
      "Epoch: 5068 mean train loss:  1.38445513e-02, mean val. rec. loss:  1.29986002e-02\n",
      "Epoch: 5069 mean train loss:  1.38433381e-02, mean val. rec. loss:  1.29975626e-02\n",
      "Epoch: 5070 mean train loss:  1.38421192e-02, mean val. rec. loss:  1.29965148e-02\n",
      "Epoch: 5071 mean train loss:  1.38409004e-02, mean val. rec. loss:  1.29954761e-02\n",
      "Epoch: 5072 mean train loss:  1.38396863e-02, mean val. rec. loss:  1.29944328e-02\n",
      "Epoch: 5073 mean train loss:  1.38384721e-02, mean val. rec. loss:  1.29933885e-02\n",
      "Epoch: 5074 mean train loss:  1.38372524e-02, mean val. rec. loss:  1.29923441e-02\n",
      "Epoch: 5075 mean train loss:  1.38360327e-02, mean val. rec. loss:  1.29913053e-02\n",
      "Epoch: 5076 mean train loss:  1.38348185e-02, mean val. rec. loss:  1.29902621e-02\n",
      "Epoch: 5077 mean train loss:  1.38336025e-02, mean val. rec. loss:  1.29892211e-02\n",
      "Epoch: 5078 mean train loss:  1.38323809e-02, mean val. rec. loss:  1.29881733e-02\n",
      "Epoch: 5079 mean train loss:  1.38311631e-02, mean val. rec. loss:  1.29871300e-02\n",
      "Epoch: 5080 mean train loss:  1.38299489e-02, mean val. rec. loss:  1.29860811e-02\n",
      "Epoch: 5081 mean train loss:  1.38287310e-02, mean val. rec. loss:  1.29850333e-02\n",
      "Epoch: 5082 mean train loss:  1.38275085e-02, mean val. rec. loss:  1.29839889e-02\n",
      "Epoch: 5083 mean train loss:  1.38262897e-02, mean val. rec. loss:  1.29829479e-02\n",
      "Epoch: 5084 mean train loss:  1.38250746e-02, mean val. rec. loss:  1.29819001e-02\n",
      "Epoch: 5085 mean train loss:  1.38238530e-02, mean val. rec. loss:  1.29808569e-02\n",
      "Epoch: 5086 mean train loss:  1.38226324e-02, mean val. rec. loss:  1.29798204e-02\n",
      "Epoch: 5087 mean train loss:  1.38214145e-02, mean val. rec. loss:  1.29787704e-02\n",
      "Epoch: 5088 mean train loss:  1.38201947e-02, mean val. rec. loss:  1.29777226e-02\n",
      "Epoch: 5089 mean train loss:  1.38189732e-02, mean val. rec. loss:  1.29766770e-02\n",
      "Epoch: 5090 mean train loss:  1.38177506e-02, mean val. rec. loss:  1.29756315e-02\n",
      "Epoch: 5091 mean train loss:  1.38165328e-02, mean val. rec. loss:  1.29745883e-02\n",
      "Epoch: 5092 mean train loss:  1.38153130e-02, mean val. rec. loss:  1.29735359e-02\n",
      "Epoch: 5093 mean train loss:  1.38140905e-02, mean val. rec. loss:  1.29724904e-02\n",
      "Epoch: 5094 mean train loss:  1.38128670e-02, mean val. rec. loss:  1.29714460e-02\n",
      "Epoch: 5095 mean train loss:  1.38116492e-02, mean val. rec. loss:  1.29703925e-02\n",
      "Epoch: 5096 mean train loss:  1.38104238e-02, mean val. rec. loss:  1.29693425e-02\n",
      "Epoch: 5097 mean train loss:  1.38092060e-02, mean val. rec. loss:  1.29682970e-02\n",
      "Epoch: 5098 mean train loss:  1.38079797e-02, mean val. rec. loss:  1.29672503e-02\n",
      "Epoch: 5099 mean train loss:  1.38067600e-02, mean val. rec. loss:  1.29661957e-02\n",
      "Epoch: 5100 mean train loss:  1.38055337e-02, mean val. rec. loss:  1.29651468e-02\n",
      "Epoch: 5101 mean train loss:  1.38043140e-02, mean val. rec. loss:  1.29640956e-02\n",
      "Epoch: 5102 mean train loss:  1.38030868e-02, mean val. rec. loss:  1.29630512e-02\n",
      "Epoch: 5103 mean train loss:  1.38018671e-02, mean val. rec. loss:  1.29619966e-02\n",
      "Epoch: 5104 mean train loss:  1.38006408e-02, mean val. rec. loss:  1.29609454e-02\n",
      "Epoch: 5105 mean train loss:  1.37994202e-02, mean val. rec. loss:  1.29598953e-02\n",
      "Epoch: 5106 mean train loss:  1.37981920e-02, mean val. rec. loss:  1.29588521e-02\n",
      "Epoch: 5107 mean train loss:  1.37969714e-02, mean val. rec. loss:  1.29577952e-02\n",
      "Epoch: 5108 mean train loss:  1.37957442e-02, mean val. rec. loss:  1.29567417e-02\n",
      "Epoch: 5109 mean train loss:  1.37945226e-02, mean val. rec. loss:  1.29556917e-02\n",
      "Epoch: 5110 mean train loss:  1.37932954e-02, mean val. rec. loss:  1.29546371e-02\n",
      "Epoch: 5111 mean train loss:  1.37920692e-02, mean val. rec. loss:  1.29535836e-02\n",
      "Epoch: 5112 mean train loss:  1.37908448e-02, mean val. rec. loss:  1.29525279e-02\n",
      "Epoch: 5113 mean train loss:  1.37896213e-02, mean val. rec. loss:  1.29514767e-02\n",
      "Epoch: 5114 mean train loss:  1.37883941e-02, mean val. rec. loss:  1.29504255e-02\n",
      "Epoch: 5115 mean train loss:  1.37871669e-02, mean val. rec. loss:  1.29493686e-02\n",
      "Epoch: 5116 mean train loss:  1.37859425e-02, mean val. rec. loss:  1.29483118e-02\n",
      "Epoch: 5117 mean train loss:  1.37847163e-02, mean val. rec. loss:  1.29472640e-02\n",
      "Epoch: 5118 mean train loss:  1.37834891e-02, mean val. rec. loss:  1.29462071e-02\n",
      "Epoch: 5119 mean train loss:  1.37822601e-02, mean val. rec. loss:  1.29451502e-02\n",
      "Epoch: 5120 mean train loss:  1.37810347e-02, mean val. rec. loss:  1.29440900e-02\n",
      "Epoch: 5121 mean train loss:  1.37798094e-02, mean val. rec. loss:  1.29430365e-02\n",
      "Epoch: 5122 mean train loss:  1.37785804e-02, mean val. rec. loss:  1.29419819e-02\n",
      "Epoch: 5123 mean train loss:  1.37773504e-02, mean val. rec. loss:  1.29409228e-02\n",
      "Epoch: 5124 mean train loss:  1.37761241e-02, mean val. rec. loss:  1.29398704e-02\n",
      "Epoch: 5125 mean train loss:  1.37748942e-02, mean val. rec. loss:  1.29388113e-02\n",
      "Epoch: 5126 mean train loss:  1.37736633e-02, mean val. rec. loss:  1.29377544e-02\n",
      "Epoch: 5127 mean train loss:  1.37724379e-02, mean val. rec. loss:  1.29366987e-02\n",
      "Epoch: 5128 mean train loss:  1.37712098e-02, mean val. rec. loss:  1.29356430e-02\n",
      "Epoch: 5129 mean train loss:  1.37699808e-02, mean val. rec. loss:  1.29345838e-02\n",
      "Epoch: 5130 mean train loss:  1.37687480e-02, mean val. rec. loss:  1.29335338e-02\n",
      "Epoch: 5131 mean train loss:  1.37675218e-02, mean val. rec. loss:  1.29324701e-02\n",
      "Epoch: 5132 mean train loss:  1.37662936e-02, mean val. rec. loss:  1.29314144e-02\n",
      "Epoch: 5133 mean train loss:  1.37650618e-02, mean val. rec. loss:  1.29303564e-02\n",
      "Epoch: 5134 mean train loss:  1.37638300e-02, mean val. rec. loss:  1.29292972e-02\n",
      "Epoch: 5135 mean train loss:  1.37626028e-02, mean val. rec. loss:  1.29282438e-02\n",
      "Epoch: 5136 mean train loss:  1.37613756e-02, mean val. rec. loss:  1.29271812e-02\n",
      "Epoch: 5137 mean train loss:  1.37601410e-02, mean val. rec. loss:  1.29261176e-02\n",
      "Epoch: 5138 mean train loss:  1.37589091e-02, mean val. rec. loss:  1.29250686e-02\n",
      "Epoch: 5139 mean train loss:  1.37576829e-02, mean val. rec. loss:  1.29240038e-02\n",
      "Epoch: 5140 mean train loss:  1.37564520e-02, mean val. rec. loss:  1.29229424e-02\n",
      "Epoch: 5141 mean train loss:  1.37552173e-02, mean val. rec. loss:  1.29218810e-02\n",
      "Epoch: 5142 mean train loss:  1.37539846e-02, mean val. rec. loss:  1.29208196e-02\n",
      "Epoch: 5143 mean train loss:  1.37527574e-02, mean val. rec. loss:  1.29197616e-02\n",
      "Epoch: 5144 mean train loss:  1.37515265e-02, mean val. rec. loss:  1.29186980e-02\n",
      "Epoch: 5145 mean train loss:  1.37502909e-02, mean val. rec. loss:  1.29176365e-02\n",
      "Epoch: 5146 mean train loss:  1.37490572e-02, mean val. rec. loss:  1.29165808e-02\n",
      "Epoch: 5147 mean train loss:  1.37478291e-02, mean val. rec. loss:  1.29155137e-02\n",
      "Epoch: 5148 mean train loss:  1.37465963e-02, mean val. rec. loss:  1.29144489e-02\n",
      "Epoch: 5149 mean train loss:  1.37453598e-02, mean val. rec. loss:  1.29133875e-02\n",
      "Epoch: 5150 mean train loss:  1.37441261e-02, mean val. rec. loss:  1.29123227e-02\n",
      "Epoch: 5151 mean train loss:  1.37428971e-02, mean val. rec. loss:  1.29112579e-02\n",
      "Epoch: 5152 mean train loss:  1.37416625e-02, mean val. rec. loss:  1.29101977e-02\n",
      "Epoch: 5153 mean train loss:  1.37404297e-02, mean val. rec. loss:  1.29091306e-02\n",
      "Epoch: 5154 mean train loss:  1.37391904e-02, mean val. rec. loss:  1.29080737e-02\n",
      "Epoch: 5155 mean train loss:  1.37379613e-02, mean val. rec. loss:  1.29070055e-02\n",
      "Epoch: 5156 mean train loss:  1.37367258e-02, mean val. rec. loss:  1.29059430e-02\n",
      "Epoch: 5157 mean train loss:  1.37354949e-02, mean val. rec. loss:  1.29048759e-02\n",
      "Epoch: 5158 mean train loss:  1.37342584e-02, mean val. rec. loss:  1.29038190e-02\n",
      "Epoch: 5159 mean train loss:  1.37330265e-02, mean val. rec. loss:  1.29027497e-02\n",
      "Epoch: 5160 mean train loss:  1.37317882e-02, mean val. rec. loss:  1.29016883e-02\n",
      "Epoch: 5161 mean train loss:  1.37305563e-02, mean val. rec. loss:  1.29006212e-02\n",
      "Epoch: 5162 mean train loss:  1.37293180e-02, mean val. rec. loss:  1.28995598e-02\n",
      "Epoch: 5163 mean train loss:  1.37280852e-02, mean val. rec. loss:  1.28984939e-02\n",
      "Epoch: 5164 mean train loss:  1.37268469e-02, mean val. rec. loss:  1.28974302e-02\n",
      "Epoch: 5165 mean train loss:  1.37256132e-02, mean val. rec. loss:  1.28963666e-02\n",
      "Epoch: 5166 mean train loss:  1.37243785e-02, mean val. rec. loss:  1.28953006e-02\n",
      "Epoch: 5167 mean train loss:  1.37231411e-02, mean val. rec. loss:  1.28942301e-02\n",
      "Epoch: 5168 mean train loss:  1.37219037e-02, mean val. rec. loss:  1.28931676e-02\n",
      "Epoch: 5169 mean train loss:  1.37206681e-02, mean val. rec. loss:  1.28920983e-02\n",
      "Epoch: 5170 mean train loss:  1.37194335e-02, mean val. rec. loss:  1.28910301e-02\n",
      "Epoch: 5171 mean train loss:  1.37181951e-02, mean val. rec. loss:  1.28899641e-02\n",
      "Epoch: 5172 mean train loss:  1.37169568e-02, mean val. rec. loss:  1.28888982e-02\n",
      "Epoch: 5173 mean train loss:  1.37157221e-02, mean val. rec. loss:  1.28878300e-02\n",
      "Epoch: 5174 mean train loss:  1.37144866e-02, mean val. rec. loss:  1.28867640e-02\n",
      "Epoch: 5175 mean train loss:  1.37132464e-02, mean val. rec. loss:  1.28856913e-02\n",
      "Epoch: 5176 mean train loss:  1.37120071e-02, mean val. rec. loss:  1.28846220e-02\n",
      "Epoch: 5177 mean train loss:  1.37107687e-02, mean val. rec. loss:  1.28835503e-02\n",
      "Epoch: 5178 mean train loss:  1.37095313e-02, mean val. rec. loss:  1.28824810e-02\n",
      "Epoch: 5179 mean train loss:  1.37082929e-02, mean val. rec. loss:  1.28814117e-02\n",
      "Epoch: 5180 mean train loss:  1.37070536e-02, mean val. rec. loss:  1.28803412e-02\n",
      "Epoch: 5181 mean train loss:  1.37058144e-02, mean val. rec. loss:  1.28792662e-02\n",
      "Epoch: 5182 mean train loss:  1.37045769e-02, mean val. rec. loss:  1.28781991e-02\n",
      "Epoch: 5183 mean train loss:  1.37033386e-02, mean val. rec. loss:  1.28771275e-02\n",
      "Epoch: 5184 mean train loss:  1.37020984e-02, mean val. rec. loss:  1.28760593e-02\n",
      "Epoch: 5185 mean train loss:  1.37008591e-02, mean val. rec. loss:  1.28749945e-02\n",
      "Epoch: 5186 mean train loss:  1.36996170e-02, mean val. rec. loss:  1.28739218e-02\n",
      "Epoch: 5187 mean train loss:  1.36983758e-02, mean val. rec. loss:  1.28728547e-02\n",
      "Epoch: 5188 mean train loss:  1.36971403e-02, mean val. rec. loss:  1.28717842e-02\n",
      "Epoch: 5189 mean train loss:  1.36959028e-02, mean val. rec. loss:  1.28707069e-02\n",
      "Epoch: 5190 mean train loss:  1.36946598e-02, mean val. rec. loss:  1.28696331e-02\n",
      "Epoch: 5191 mean train loss:  1.36934168e-02, mean val. rec. loss:  1.28685637e-02\n",
      "Epoch: 5192 mean train loss:  1.36921803e-02, mean val. rec. loss:  1.28674966e-02\n",
      "Epoch: 5193 mean train loss:  1.36909420e-02, mean val. rec. loss:  1.28664262e-02\n",
      "Epoch: 5194 mean train loss:  1.36896980e-02, mean val. rec. loss:  1.28653466e-02\n",
      "Epoch: 5195 mean train loss:  1.36884532e-02, mean val. rec. loss:  1.28642818e-02\n",
      "Epoch: 5196 mean train loss:  1.36872157e-02, mean val. rec. loss:  1.28632091e-02\n",
      "Epoch: 5197 mean train loss:  1.36859792e-02, mean val. rec. loss:  1.28621352e-02\n",
      "Epoch: 5198 mean train loss:  1.36847353e-02, mean val. rec. loss:  1.28610636e-02\n",
      "Epoch: 5199 mean train loss:  1.36834895e-02, mean val. rec. loss:  1.28599931e-02\n",
      "Epoch: 5200 mean train loss:  1.36822511e-02, mean val. rec. loss:  1.28589204e-02\n",
      "Epoch: 5201 mean train loss:  1.36810118e-02, mean val. rec. loss:  1.28578499e-02\n",
      "Epoch: 5202 mean train loss:  1.36797670e-02, mean val. rec. loss:  1.28567681e-02\n",
      "Epoch: 5203 mean train loss:  1.36785230e-02, mean val. rec. loss:  1.28556931e-02\n",
      "Epoch: 5204 mean train loss:  1.36772828e-02, mean val. rec. loss:  1.28546181e-02\n",
      "Epoch: 5205 mean train loss:  1.36760426e-02, mean val. rec. loss:  1.28535419e-02\n",
      "Epoch: 5206 mean train loss:  1.36747977e-02, mean val. rec. loss:  1.28524624e-02\n",
      "Epoch: 5207 mean train loss:  1.36735528e-02, mean val. rec. loss:  1.28513862e-02\n",
      "Epoch: 5208 mean train loss:  1.36723126e-02, mean val. rec. loss:  1.28503135e-02\n",
      "Epoch: 5209 mean train loss:  1.36710724e-02, mean val. rec. loss:  1.28492419e-02\n",
      "Epoch: 5210 mean train loss:  1.36698266e-02, mean val. rec. loss:  1.28481578e-02\n",
      "Epoch: 5211 mean train loss:  1.36685808e-02, mean val. rec. loss:  1.28470885e-02\n",
      "Epoch: 5212 mean train loss:  1.36673415e-02, mean val. rec. loss:  1.28460101e-02\n",
      "Epoch: 5213 mean train loss:  1.36660985e-02, mean val. rec. loss:  1.28449294e-02\n",
      "Epoch: 5214 mean train loss:  1.36648508e-02, mean val. rec. loss:  1.28438566e-02\n",
      "Epoch: 5215 mean train loss:  1.36636069e-02, mean val. rec. loss:  1.28427816e-02\n",
      "Epoch: 5216 mean train loss:  1.36623694e-02, mean val. rec. loss:  1.28417009e-02\n",
      "Epoch: 5217 mean train loss:  1.36611190e-02, mean val. rec. loss:  1.28406271e-02\n",
      "Epoch: 5218 mean train loss:  1.36598788e-02, mean val. rec. loss:  1.28395498e-02\n",
      "Epoch: 5219 mean train loss:  1.36586311e-02, mean val. rec. loss:  1.28384748e-02\n",
      "Epoch: 5220 mean train loss:  1.36573927e-02, mean val. rec. loss:  1.28373964e-02\n",
      "Epoch: 5221 mean train loss:  1.36561414e-02, mean val. rec. loss:  1.28363180e-02\n",
      "Epoch: 5222 mean train loss:  1.36549021e-02, mean val. rec. loss:  1.28352407e-02\n",
      "Epoch: 5223 mean train loss:  1.36536525e-02, mean val. rec. loss:  1.28341611e-02\n",
      "Epoch: 5224 mean train loss:  1.36524123e-02, mean val. rec. loss:  1.28330861e-02\n",
      "Epoch: 5225 mean train loss:  1.36511619e-02, mean val. rec. loss:  1.28320043e-02\n",
      "Epoch: 5226 mean train loss:  1.36499216e-02, mean val. rec. loss:  1.28309270e-02\n",
      "Epoch: 5227 mean train loss:  1.36486721e-02, mean val. rec. loss:  1.28298486e-02\n",
      "Epoch: 5228 mean train loss:  1.36474319e-02, mean val. rec. loss:  1.28287668e-02\n",
      "Epoch: 5229 mean train loss:  1.36461805e-02, mean val. rec. loss:  1.28276839e-02\n",
      "Epoch: 5230 mean train loss:  1.36449384e-02, mean val. rec. loss:  1.28266077e-02\n",
      "Epoch: 5231 mean train loss:  1.36436880e-02, mean val. rec. loss:  1.28255293e-02\n",
      "Epoch: 5232 mean train loss:  1.36424496e-02, mean val. rec. loss:  1.28244475e-02\n",
      "Epoch: 5233 mean train loss:  1.36411973e-02, mean val. rec. loss:  1.28233657e-02\n",
      "Epoch: 5234 mean train loss:  1.36399533e-02, mean val. rec. loss:  1.28222839e-02\n",
      "Epoch: 5235 mean train loss:  1.36387038e-02, mean val. rec. loss:  1.28212043e-02\n",
      "Epoch: 5236 mean train loss:  1.36374636e-02, mean val. rec. loss:  1.28201225e-02\n",
      "Epoch: 5237 mean train loss:  1.36362103e-02, mean val. rec. loss:  1.28190362e-02\n",
      "Epoch: 5238 mean train loss:  1.36349645e-02, mean val. rec. loss:  1.28179657e-02\n",
      "Epoch: 5239 mean train loss:  1.36337215e-02, mean val. rec. loss:  1.28168850e-02\n",
      "Epoch: 5240 mean train loss:  1.36324748e-02, mean val. rec. loss:  1.28158020e-02\n",
      "Epoch: 5241 mean train loss:  1.36312234e-02, mean val. rec. loss:  1.28147248e-02\n",
      "Epoch: 5242 mean train loss:  1.36299766e-02, mean val. rec. loss:  1.28136418e-02\n",
      "Epoch: 5243 mean train loss:  1.36287355e-02, mean val. rec. loss:  1.28125589e-02\n",
      "Epoch: 5244 mean train loss:  1.36274822e-02, mean val. rec. loss:  1.28114782e-02\n",
      "Epoch: 5245 mean train loss:  1.36262346e-02, mean val. rec. loss:  1.28103964e-02\n",
      "Epoch: 5246 mean train loss:  1.36249878e-02, mean val. rec. loss:  1.28093180e-02\n",
      "Epoch: 5247 mean train loss:  1.36237458e-02, mean val. rec. loss:  1.28082316e-02\n",
      "Epoch: 5248 mean train loss:  1.36224916e-02, mean val. rec. loss:  1.28071521e-02\n",
      "Epoch: 5249 mean train loss:  1.36212448e-02, mean val. rec. loss:  1.28060725e-02\n",
      "Epoch: 5250 mean train loss:  1.36199972e-02, mean val. rec. loss:  1.28049884e-02\n",
      "Epoch: 5251 mean train loss:  1.36187542e-02, mean val. rec. loss:  1.28039010e-02\n",
      "Epoch: 5252 mean train loss:  1.36175009e-02, mean val. rec. loss:  1.28028180e-02\n",
      "Epoch: 5253 mean train loss:  1.36162495e-02, mean val. rec. loss:  1.28017317e-02\n",
      "Epoch: 5254 mean train loss:  1.36150056e-02, mean val. rec. loss:  1.28006499e-02\n",
      "Epoch: 5255 mean train loss:  1.36137598e-02, mean val. rec. loss:  1.27995646e-02\n",
      "Epoch: 5256 mean train loss:  1.36125074e-02, mean val. rec. loss:  1.27984749e-02\n",
      "Epoch: 5257 mean train loss:  1.36112551e-02, mean val. rec. loss:  1.27973953e-02\n",
      "Epoch: 5258 mean train loss:  1.36100093e-02, mean val. rec. loss:  1.27963101e-02\n",
      "Epoch: 5259 mean train loss:  1.36087635e-02, mean val. rec. loss:  1.27952249e-02\n",
      "Epoch: 5260 mean train loss:  1.36075121e-02, mean val. rec. loss:  1.27941374e-02\n",
      "Epoch: 5261 mean train loss:  1.36062589e-02, mean val. rec. loss:  1.27930522e-02\n",
      "Epoch: 5262 mean train loss:  1.36050112e-02, mean val. rec. loss:  1.27919727e-02\n",
      "Epoch: 5263 mean train loss:  1.36037617e-02, mean val. rec. loss:  1.27908818e-02\n",
      "Epoch: 5264 mean train loss:  1.36025121e-02, mean val. rec. loss:  1.27898022e-02\n",
      "Epoch: 5265 mean train loss:  1.36012626e-02, mean val. rec. loss:  1.27887113e-02\n",
      "Epoch: 5266 mean train loss:  1.36000131e-02, mean val. rec. loss:  1.27876318e-02\n",
      "Epoch: 5267 mean train loss:  1.35987635e-02, mean val. rec. loss:  1.27865386e-02\n",
      "Epoch: 5268 mean train loss:  1.35975140e-02, mean val. rec. loss:  1.27854602e-02\n",
      "Epoch: 5269 mean train loss:  1.35962626e-02, mean val. rec. loss:  1.27843705e-02\n",
      "Epoch: 5270 mean train loss:  1.35950131e-02, mean val. rec. loss:  1.27832887e-02\n",
      "Epoch: 5271 mean train loss:  1.35937645e-02, mean val. rec. loss:  1.27822001e-02\n",
      "Epoch: 5272 mean train loss:  1.35925159e-02, mean val. rec. loss:  1.27811205e-02\n",
      "Epoch: 5273 mean train loss:  1.35912636e-02, mean val. rec. loss:  1.27800262e-02\n",
      "Epoch: 5274 mean train loss:  1.35900112e-02, mean val. rec. loss:  1.27789501e-02\n",
      "Epoch: 5275 mean train loss:  1.35887617e-02, mean val. rec. loss:  1.27778558e-02\n",
      "Epoch: 5276 mean train loss:  1.35875140e-02, mean val. rec. loss:  1.27767728e-02\n",
      "Epoch: 5277 mean train loss:  1.35862608e-02, mean val. rec. loss:  1.27756752e-02\n",
      "Epoch: 5278 mean train loss:  1.35850094e-02, mean val. rec. loss:  1.27745945e-02\n",
      "Epoch: 5279 mean train loss:  1.35837599e-02, mean val. rec. loss:  1.27735002e-02\n",
      "Epoch: 5280 mean train loss:  1.35825085e-02, mean val. rec. loss:  1.27724161e-02\n",
      "Epoch: 5281 mean train loss:  1.35812580e-02, mean val. rec. loss:  1.27713218e-02\n",
      "Epoch: 5282 mean train loss:  1.35800066e-02, mean val. rec. loss:  1.27702366e-02\n",
      "Epoch: 5283 mean train loss:  1.35787562e-02, mean val. rec. loss:  1.27691503e-02\n",
      "Epoch: 5284 mean train loss:  1.35775010e-02, mean val. rec. loss:  1.27680662e-02\n",
      "Epoch: 5285 mean train loss:  1.35762515e-02, mean val. rec. loss:  1.27669753e-02\n",
      "Epoch: 5286 mean train loss:  1.35750029e-02, mean val. rec. loss:  1.27658855e-02\n",
      "Epoch: 5287 mean train loss:  1.35737487e-02, mean val. rec. loss:  1.27647992e-02\n",
      "Epoch: 5288 mean train loss:  1.35724945e-02, mean val. rec. loss:  1.27637117e-02\n",
      "Epoch: 5289 mean train loss:  1.35712469e-02, mean val. rec. loss:  1.27626208e-02\n",
      "Epoch: 5290 mean train loss:  1.35699946e-02, mean val. rec. loss:  1.27615345e-02\n",
      "Epoch: 5291 mean train loss:  1.35687394e-02, mean val. rec. loss:  1.27604436e-02\n",
      "Epoch: 5292 mean train loss:  1.35674871e-02, mean val. rec. loss:  1.27593561e-02\n",
      "Epoch: 5293 mean train loss:  1.35662385e-02, mean val. rec. loss:  1.27582675e-02\n",
      "Epoch: 5294 mean train loss:  1.35649871e-02, mean val. rec. loss:  1.27571732e-02\n",
      "Epoch: 5295 mean train loss:  1.35637320e-02, mean val. rec. loss:  1.27560846e-02\n",
      "Epoch: 5296 mean train loss:  1.35624778e-02, mean val. rec. loss:  1.27549948e-02\n",
      "Epoch: 5297 mean train loss:  1.35612302e-02, mean val. rec. loss:  1.27539028e-02\n",
      "Epoch: 5298 mean train loss:  1.35599797e-02, mean val. rec. loss:  1.27528142e-02\n",
      "Epoch: 5299 mean train loss:  1.35587246e-02, mean val. rec. loss:  1.27517222e-02\n",
      "Epoch: 5300 mean train loss:  1.35574695e-02, mean val. rec. loss:  1.27506313e-02\n",
      "Epoch: 5301 mean train loss:  1.35562199e-02, mean val. rec. loss:  1.27495393e-02\n",
      "Epoch: 5302 mean train loss:  1.35549704e-02, mean val. rec. loss:  1.27484461e-02\n",
      "Epoch: 5303 mean train loss:  1.35537144e-02, mean val. rec. loss:  1.27473598e-02\n",
      "Epoch: 5304 mean train loss:  1.35524620e-02, mean val. rec. loss:  1.27462734e-02\n",
      "Epoch: 5305 mean train loss:  1.35512088e-02, mean val. rec. loss:  1.27451803e-02\n",
      "Epoch: 5306 mean train loss:  1.35499555e-02, mean val. rec. loss:  1.27440860e-02\n",
      "Epoch: 5307 mean train loss:  1.35487023e-02, mean val. rec. loss:  1.27429962e-02\n",
      "Epoch: 5308 mean train loss:  1.35474472e-02, mean val. rec. loss:  1.27419099e-02\n",
      "Epoch: 5309 mean train loss:  1.35461958e-02, mean val. rec. loss:  1.27408167e-02\n",
      "Epoch: 5310 mean train loss:  1.35449444e-02, mean val. rec. loss:  1.27397213e-02\n",
      "Epoch: 5311 mean train loss:  1.35436893e-02, mean val. rec. loss:  1.27386293e-02\n",
      "Epoch: 5312 mean train loss:  1.35424360e-02, mean val. rec. loss:  1.27375441e-02\n",
      "Epoch: 5313 mean train loss:  1.35411837e-02, mean val. rec. loss:  1.27364486e-02\n",
      "Epoch: 5314 mean train loss:  1.35399314e-02, mean val. rec. loss:  1.27353475e-02\n",
      "Epoch: 5315 mean train loss:  1.35386762e-02, mean val. rec. loss:  1.27342623e-02\n",
      "Epoch: 5316 mean train loss:  1.35374248e-02, mean val. rec. loss:  1.27331692e-02\n",
      "Epoch: 5317 mean train loss:  1.35361697e-02, mean val. rec. loss:  1.27320760e-02\n",
      "Epoch: 5318 mean train loss:  1.35349174e-02, mean val. rec. loss:  1.27309795e-02\n",
      "Epoch: 5319 mean train loss:  1.35336614e-02, mean val. rec. loss:  1.27298943e-02\n",
      "Epoch: 5320 mean train loss:  1.35324109e-02, mean val. rec. loss:  1.27287977e-02\n",
      "Epoch: 5321 mean train loss:  1.35311521e-02, mean val. rec. loss:  1.27277136e-02\n",
      "Epoch: 5322 mean train loss:  1.35299035e-02, mean val. rec. loss:  1.27266125e-02\n",
      "Epoch: 5323 mean train loss:  1.35286502e-02, mean val. rec. loss:  1.27255182e-02\n",
      "Epoch: 5324 mean train loss:  1.35273932e-02, mean val. rec. loss:  1.27244262e-02\n",
      "Epoch: 5325 mean train loss:  1.35261372e-02, mean val. rec. loss:  1.27233331e-02\n",
      "Epoch: 5326 mean train loss:  1.35248886e-02, mean val. rec. loss:  1.27222433e-02\n",
      "Epoch: 5327 mean train loss:  1.35236353e-02, mean val. rec. loss:  1.27211468e-02\n",
      "Epoch: 5328 mean train loss:  1.35223774e-02, mean val. rec. loss:  1.27200536e-02\n",
      "Epoch: 5329 mean train loss:  1.35211232e-02, mean val. rec. loss:  1.27189650e-02\n",
      "Epoch: 5330 mean train loss:  1.35198718e-02, mean val. rec. loss:  1.27178673e-02\n",
      "Epoch: 5331 mean train loss:  1.35186186e-02, mean val. rec. loss:  1.27167741e-02\n",
      "Epoch: 5332 mean train loss:  1.35173625e-02, mean val. rec. loss:  1.27156787e-02\n",
      "Epoch: 5333 mean train loss:  1.35161065e-02, mean val. rec. loss:  1.27145833e-02\n",
      "Epoch: 5334 mean train loss:  1.35148551e-02, mean val. rec. loss:  1.27134947e-02\n",
      "Epoch: 5335 mean train loss:  1.35136028e-02, mean val. rec. loss:  1.27123970e-02\n",
      "Epoch: 5336 mean train loss:  1.35123458e-02, mean val. rec. loss:  1.27112993e-02\n",
      "Epoch: 5337 mean train loss:  1.35110897e-02, mean val. rec. loss:  1.27102096e-02\n",
      "Epoch: 5338 mean train loss:  1.35098384e-02, mean val. rec. loss:  1.27091130e-02\n",
      "Epoch: 5339 mean train loss:  1.35085870e-02, mean val. rec. loss:  1.27080187e-02\n",
      "Epoch: 5340 mean train loss:  1.35073318e-02, mean val. rec. loss:  1.27069222e-02\n",
      "Epoch: 5341 mean train loss:  1.35060767e-02, mean val. rec. loss:  1.27058335e-02\n",
      "Epoch: 5342 mean train loss:  1.35048235e-02, mean val. rec. loss:  1.27047381e-02\n",
      "Epoch: 5343 mean train loss:  1.35035702e-02, mean val. rec. loss:  1.27036450e-02\n",
      "Epoch: 5344 mean train loss:  1.35023151e-02, mean val. rec. loss:  1.27025473e-02\n",
      "Epoch: 5345 mean train loss:  1.35010591e-02, mean val. rec. loss:  1.27014541e-02\n",
      "Epoch: 5346 mean train loss:  1.34998067e-02, mean val. rec. loss:  1.27003598e-02\n",
      "Epoch: 5347 mean train loss:  1.34985535e-02, mean val. rec. loss:  1.26992655e-02\n",
      "Epoch: 5348 mean train loss:  1.34972984e-02, mean val. rec. loss:  1.26981679e-02\n",
      "Epoch: 5349 mean train loss:  1.34960423e-02, mean val. rec. loss:  1.26970702e-02\n",
      "Epoch: 5350 mean train loss:  1.34947891e-02, mean val. rec. loss:  1.26959725e-02\n",
      "Epoch: 5351 mean train loss:  1.34935349e-02, mean val. rec. loss:  1.26948782e-02\n",
      "Epoch: 5352 mean train loss:  1.34922798e-02, mean val. rec. loss:  1.26937816e-02\n",
      "Epoch: 5353 mean train loss:  1.34910246e-02, mean val. rec. loss:  1.26926885e-02\n",
      "Epoch: 5354 mean train loss:  1.34897686e-02, mean val. rec. loss:  1.26915908e-02\n",
      "Epoch: 5355 mean train loss:  1.34885107e-02, mean val. rec. loss:  1.26904942e-02\n",
      "Epoch: 5356 mean train loss:  1.34872565e-02, mean val. rec. loss:  1.26894033e-02\n",
      "Epoch: 5357 mean train loss:  1.34860060e-02, mean val. rec. loss:  1.26883045e-02\n",
      "Epoch: 5358 mean train loss:  1.34847509e-02, mean val. rec. loss:  1.26872068e-02\n",
      "Epoch: 5359 mean train loss:  1.34834921e-02, mean val. rec. loss:  1.26861125e-02\n",
      "Epoch: 5360 mean train loss:  1.34822388e-02, mean val. rec. loss:  1.26850183e-02\n",
      "Epoch: 5361 mean train loss:  1.34809884e-02, mean val. rec. loss:  1.26839228e-02\n",
      "Epoch: 5362 mean train loss:  1.34797305e-02, mean val. rec. loss:  1.26828206e-02\n",
      "Epoch: 5363 mean train loss:  1.34784735e-02, mean val. rec. loss:  1.26817252e-02\n",
      "Epoch: 5364 mean train loss:  1.34772212e-02, mean val. rec. loss:  1.26806309e-02\n",
      "Epoch: 5365 mean train loss:  1.34759688e-02, mean val. rec. loss:  1.26795355e-02\n",
      "Epoch: 5366 mean train loss:  1.34747119e-02, mean val. rec. loss:  1.26784389e-02\n",
      "Epoch: 5367 mean train loss:  1.34734558e-02, mean val. rec. loss:  1.26773435e-02\n",
      "Epoch: 5368 mean train loss:  1.34722035e-02, mean val. rec. loss:  1.26762458e-02\n",
      "Epoch: 5369 mean train loss:  1.34709502e-02, mean val. rec. loss:  1.26751481e-02\n",
      "Epoch: 5370 mean train loss:  1.34696914e-02, mean val. rec. loss:  1.26740459e-02\n",
      "Epoch: 5371 mean train loss:  1.34684372e-02, mean val. rec. loss:  1.26729550e-02\n",
      "Epoch: 5372 mean train loss:  1.34671886e-02, mean val. rec. loss:  1.26718562e-02\n",
      "Epoch: 5373 mean train loss:  1.34659279e-02, mean val. rec. loss:  1.26707596e-02\n",
      "Epoch: 5374 mean train loss:  1.34646765e-02, mean val. rec. loss:  1.26696631e-02\n",
      "Epoch: 5375 mean train loss:  1.34634195e-02, mean val. rec. loss:  1.26685620e-02\n",
      "Epoch: 5376 mean train loss:  1.34621691e-02, mean val. rec. loss:  1.26674598e-02\n",
      "Epoch: 5377 mean train loss:  1.34609102e-02, mean val. rec. loss:  1.26663666e-02\n",
      "Epoch: 5378 mean train loss:  1.34596598e-02, mean val. rec. loss:  1.26652689e-02\n",
      "Epoch: 5379 mean train loss:  1.34584009e-02, mean val. rec. loss:  1.26641735e-02\n",
      "Epoch: 5380 mean train loss:  1.34571505e-02, mean val. rec. loss:  1.26630747e-02\n",
      "Epoch: 5381 mean train loss:  1.34558926e-02, mean val. rec. loss:  1.26619804e-02\n",
      "Epoch: 5382 mean train loss:  1.34546421e-02, mean val. rec. loss:  1.26608816e-02\n",
      "Epoch: 5383 mean train loss:  1.34533851e-02, mean val. rec. loss:  1.26597827e-02\n",
      "Epoch: 5384 mean train loss:  1.34521337e-02, mean val. rec. loss:  1.26586760e-02\n",
      "Epoch: 5385 mean train loss:  1.34508721e-02, mean val. rec. loss:  1.26575840e-02\n",
      "Epoch: 5386 mean train loss:  1.34496226e-02, mean val. rec. loss:  1.26564829e-02\n",
      "Epoch: 5387 mean train loss:  1.34483721e-02, mean val. rec. loss:  1.26553829e-02\n",
      "Epoch: 5388 mean train loss:  1.34471142e-02, mean val. rec. loss:  1.26542830e-02\n",
      "Epoch: 5389 mean train loss:  1.34458554e-02, mean val. rec. loss:  1.26531853e-02\n",
      "Epoch: 5390 mean train loss:  1.34446021e-02, mean val. rec. loss:  1.26520910e-02\n",
      "Epoch: 5391 mean train loss:  1.34433507e-02, mean val. rec. loss:  1.26509944e-02\n",
      "Epoch: 5392 mean train loss:  1.34420937e-02, mean val. rec. loss:  1.26498933e-02\n",
      "Epoch: 5393 mean train loss:  1.34408377e-02, mean val. rec. loss:  1.26487990e-02\n",
      "Epoch: 5394 mean train loss:  1.34395854e-02, mean val. rec. loss:  1.26476968e-02\n",
      "Epoch: 5395 mean train loss:  1.34383340e-02, mean val. rec. loss:  1.26465969e-02\n",
      "Epoch: 5396 mean train loss:  1.34370770e-02, mean val. rec. loss:  1.26454946e-02\n",
      "Epoch: 5397 mean train loss:  1.34358219e-02, mean val. rec. loss:  1.26443970e-02\n",
      "Epoch: 5398 mean train loss:  1.34345714e-02, mean val. rec. loss:  1.26433015e-02\n",
      "Epoch: 5399 mean train loss:  1.34333154e-02, mean val. rec. loss:  1.26421982e-02\n",
      "Epoch: 5400 mean train loss:  1.34320602e-02, mean val. rec. loss:  1.26411084e-02\n",
      "Epoch: 5401 mean train loss:  1.34308079e-02, mean val. rec. loss:  1.26400051e-02\n",
      "Epoch: 5402 mean train loss:  1.34295556e-02, mean val. rec. loss:  1.26389119e-02\n",
      "Epoch: 5403 mean train loss:  1.34282995e-02, mean val. rec. loss:  1.26378051e-02\n",
      "Epoch: 5404 mean train loss:  1.34270454e-02, mean val. rec. loss:  1.26367120e-02\n",
      "Epoch: 5405 mean train loss:  1.34257921e-02, mean val. rec. loss:  1.26356075e-02\n",
      "Epoch: 5406 mean train loss:  1.34245389e-02, mean val. rec. loss:  1.26345121e-02\n",
      "Epoch: 5407 mean train loss:  1.34232847e-02, mean val. rec. loss:  1.26334087e-02\n",
      "Epoch: 5408 mean train loss:  1.34220314e-02, mean val. rec. loss:  1.26323156e-02\n",
      "Epoch: 5409 mean train loss:  1.34207763e-02, mean val. rec. loss:  1.26312122e-02\n",
      "Epoch: 5410 mean train loss:  1.34195230e-02, mean val. rec. loss:  1.26301122e-02\n",
      "Epoch: 5411 mean train loss:  1.34182679e-02, mean val. rec. loss:  1.26290089e-02\n",
      "Epoch: 5412 mean train loss:  1.34170156e-02, mean val. rec. loss:  1.26279112e-02\n",
      "Epoch: 5413 mean train loss:  1.34157614e-02, mean val. rec. loss:  1.26268135e-02\n",
      "Epoch: 5414 mean train loss:  1.34145091e-02, mean val. rec. loss:  1.26257181e-02\n",
      "Epoch: 5415 mean train loss:  1.34132549e-02, mean val. rec. loss:  1.26246238e-02\n",
      "Epoch: 5416 mean train loss:  1.34120007e-02, mean val. rec. loss:  1.26235216e-02\n",
      "Epoch: 5417 mean train loss:  1.34107475e-02, mean val. rec. loss:  1.26224250e-02\n",
      "Epoch: 5418 mean train loss:  1.34094961e-02, mean val. rec. loss:  1.26213251e-02\n",
      "Epoch: 5419 mean train loss:  1.34082456e-02, mean val. rec. loss:  1.26202228e-02\n",
      "Epoch: 5420 mean train loss:  1.34069896e-02, mean val. rec. loss:  1.26191206e-02\n",
      "Epoch: 5421 mean train loss:  1.34057345e-02, mean val. rec. loss:  1.26180207e-02\n",
      "Epoch: 5422 mean train loss:  1.34044831e-02, mean val. rec. loss:  1.26169230e-02\n",
      "Epoch: 5423 mean train loss:  1.34032335e-02, mean val. rec. loss:  1.26158230e-02\n",
      "Epoch: 5424 mean train loss:  1.34019766e-02, mean val. rec. loss:  1.26147185e-02\n",
      "Epoch: 5425 mean train loss:  1.34007214e-02, mean val. rec. loss:  1.26136208e-02\n",
      "Epoch: 5426 mean train loss:  1.33994700e-02, mean val. rec. loss:  1.26125265e-02\n",
      "Epoch: 5427 mean train loss:  1.33982187e-02, mean val. rec. loss:  1.26114323e-02\n",
      "Epoch: 5428 mean train loss:  1.33969654e-02, mean val. rec. loss:  1.26103278e-02\n",
      "Epoch: 5429 mean train loss:  1.33957112e-02, mean val. rec. loss:  1.26092278e-02\n",
      "Epoch: 5430 mean train loss:  1.33944589e-02, mean val. rec. loss:  1.26081290e-02\n",
      "Epoch: 5431 mean train loss:  1.33932066e-02, mean val. rec. loss:  1.26070324e-02\n",
      "Epoch: 5432 mean train loss:  1.33919542e-02, mean val. rec. loss:  1.26059279e-02\n",
      "Epoch: 5433 mean train loss:  1.33907010e-02, mean val. rec. loss:  1.26048268e-02\n",
      "Epoch: 5434 mean train loss:  1.33894496e-02, mean val. rec. loss:  1.26037280e-02\n",
      "Epoch: 5435 mean train loss:  1.33881963e-02, mean val. rec. loss:  1.26026281e-02\n",
      "Epoch: 5436 mean train loss:  1.33869450e-02, mean val. rec. loss:  1.26015236e-02\n",
      "Epoch: 5437 mean train loss:  1.33856898e-02, mean val. rec. loss:  1.26004247e-02\n",
      "Epoch: 5438 mean train loss:  1.33844375e-02, mean val. rec. loss:  1.25993259e-02\n",
      "Epoch: 5439 mean train loss:  1.33831889e-02, mean val. rec. loss:  1.25982328e-02\n",
      "Epoch: 5440 mean train loss:  1.33819375e-02, mean val. rec. loss:  1.25971328e-02\n",
      "Epoch: 5441 mean train loss:  1.33806824e-02, mean val. rec. loss:  1.25960363e-02\n",
      "Epoch: 5442 mean train loss:  1.33794301e-02, mean val. rec. loss:  1.25949420e-02\n",
      "Epoch: 5443 mean train loss:  1.33781806e-02, mean val. rec. loss:  1.25938397e-02\n",
      "Epoch: 5444 mean train loss:  1.33769301e-02, mean val. rec. loss:  1.25927375e-02\n",
      "Epoch: 5445 mean train loss:  1.33756768e-02, mean val. rec. loss:  1.25916342e-02\n",
      "Epoch: 5446 mean train loss:  1.33744236e-02, mean val. rec. loss:  1.25905308e-02\n",
      "Epoch: 5447 mean train loss:  1.33731750e-02, mean val. rec. loss:  1.25894308e-02\n",
      "Epoch: 5448 mean train loss:  1.33719245e-02, mean val. rec. loss:  1.25883309e-02\n",
      "Epoch: 5449 mean train loss:  1.33706703e-02, mean val. rec. loss:  1.25872355e-02\n",
      "Epoch: 5450 mean train loss:  1.33694199e-02, mean val. rec. loss:  1.25861355e-02\n",
      "Epoch: 5451 mean train loss:  1.33681703e-02, mean val. rec. loss:  1.25850389e-02\n",
      "Epoch: 5452 mean train loss:  1.33669180e-02, mean val. rec. loss:  1.25839379e-02\n",
      "Epoch: 5453 mean train loss:  1.33656657e-02, mean val. rec. loss:  1.25828390e-02\n",
      "Epoch: 5454 mean train loss:  1.33644162e-02, mean val. rec. loss:  1.25817425e-02\n",
      "Epoch: 5455 mean train loss:  1.33631657e-02, mean val. rec. loss:  1.25806425e-02\n",
      "Epoch: 5456 mean train loss:  1.33619143e-02, mean val. rec. loss:  1.25795414e-02\n",
      "Epoch: 5457 mean train loss:  1.33606639e-02, mean val. rec. loss:  1.25784358e-02\n",
      "Epoch: 5458 mean train loss:  1.33594125e-02, mean val. rec. loss:  1.25773381e-02\n",
      "Epoch: 5459 mean train loss:  1.33581629e-02, mean val. rec. loss:  1.25762416e-02\n",
      "Epoch: 5460 mean train loss:  1.33569125e-02, mean val. rec. loss:  1.25751359e-02\n",
      "Epoch: 5461 mean train loss:  1.33556611e-02, mean val. rec. loss:  1.25740382e-02\n",
      "Epoch: 5462 mean train loss:  1.33544134e-02, mean val. rec. loss:  1.25729371e-02\n",
      "Epoch: 5463 mean train loss:  1.33531630e-02, mean val. rec. loss:  1.25718361e-02\n",
      "Epoch: 5464 mean train loss:  1.33519097e-02, mean val. rec. loss:  1.25707406e-02\n",
      "Epoch: 5465 mean train loss:  1.33506602e-02, mean val. rec. loss:  1.25696407e-02\n",
      "Epoch: 5466 mean train loss:  1.33494125e-02, mean val. rec. loss:  1.25685430e-02\n",
      "Epoch: 5467 mean train loss:  1.33481611e-02, mean val. rec. loss:  1.25674385e-02\n",
      "Epoch: 5468 mean train loss:  1.33469088e-02, mean val. rec. loss:  1.25663408e-02\n",
      "Epoch: 5469 mean train loss:  1.33456621e-02, mean val. rec. loss:  1.25652420e-02\n",
      "Epoch: 5470 mean train loss:  1.33444144e-02, mean val. rec. loss:  1.25641386e-02\n",
      "Epoch: 5471 mean train loss:  1.33431621e-02, mean val. rec. loss:  1.25630398e-02\n",
      "Epoch: 5472 mean train loss:  1.33419097e-02, mean val. rec. loss:  1.25619432e-02\n",
      "Epoch: 5473 mean train loss:  1.33406639e-02, mean val. rec. loss:  1.25608410e-02\n",
      "Epoch: 5474 mean train loss:  1.33394125e-02, mean val. rec. loss:  1.25597479e-02\n",
      "Epoch: 5475 mean train loss:  1.33381686e-02, mean val. rec. loss:  1.25586456e-02\n",
      "Epoch: 5476 mean train loss:  1.33369135e-02, mean val. rec. loss:  1.25575445e-02\n",
      "Epoch: 5477 mean train loss:  1.33356667e-02, mean val. rec. loss:  1.25564457e-02\n",
      "Epoch: 5478 mean train loss:  1.33344181e-02, mean val. rec. loss:  1.25553435e-02\n",
      "Epoch: 5479 mean train loss:  1.33331695e-02, mean val. rec. loss:  1.25542435e-02\n",
      "Epoch: 5480 mean train loss:  1.33319191e-02, mean val. rec. loss:  1.25531425e-02\n",
      "Epoch: 5481 mean train loss:  1.33306714e-02, mean val. rec. loss:  1.25520436e-02\n",
      "Epoch: 5482 mean train loss:  1.33294228e-02, mean val. rec. loss:  1.25509459e-02\n",
      "Epoch: 5483 mean train loss:  1.33281761e-02, mean val. rec. loss:  1.25498483e-02\n",
      "Epoch: 5484 mean train loss:  1.33269266e-02, mean val. rec. loss:  1.25487483e-02\n",
      "Epoch: 5485 mean train loss:  1.33256770e-02, mean val. rec. loss:  1.25476461e-02\n",
      "Epoch: 5486 mean train loss:  1.33244312e-02, mean val. rec. loss:  1.25465484e-02\n",
      "Epoch: 5487 mean train loss:  1.33231826e-02, mean val. rec. loss:  1.25454507e-02\n",
      "Epoch: 5488 mean train loss:  1.33219340e-02, mean val. rec. loss:  1.25443485e-02\n",
      "Epoch: 5489 mean train loss:  1.33206854e-02, mean val. rec. loss:  1.25432530e-02\n",
      "Epoch: 5490 mean train loss:  1.33194405e-02, mean val. rec. loss:  1.25421508e-02\n",
      "Epoch: 5491 mean train loss:  1.33181957e-02, mean val. rec. loss:  1.25410486e-02\n",
      "Epoch: 5492 mean train loss:  1.33169452e-02, mean val. rec. loss:  1.25399464e-02\n",
      "Epoch: 5493 mean train loss:  1.33156966e-02, mean val. rec. loss:  1.25388487e-02\n",
      "Epoch: 5494 mean train loss:  1.33144517e-02, mean val. rec. loss:  1.25377601e-02\n",
      "Epoch: 5495 mean train loss:  1.33132041e-02, mean val. rec. loss:  1.25366556e-02\n",
      "Epoch: 5496 mean train loss:  1.33119564e-02, mean val. rec. loss:  1.25355579e-02\n",
      "Epoch: 5497 mean train loss:  1.33107069e-02, mean val. rec. loss:  1.25344523e-02\n",
      "Epoch: 5498 mean train loss:  1.33094629e-02, mean val. rec. loss:  1.25333614e-02\n",
      "Epoch: 5499 mean train loss:  1.33082171e-02, mean val. rec. loss:  1.25322569e-02\n",
      "Epoch: 5500 mean train loss:  1.33069695e-02, mean val. rec. loss:  1.25311603e-02\n",
      "Epoch: 5501 mean train loss:  1.33057209e-02, mean val. rec. loss:  1.25300604e-02\n",
      "Epoch: 5502 mean train loss:  1.33044797e-02, mean val. rec. loss:  1.25289649e-02\n",
      "Epoch: 5503 mean train loss:  1.33032302e-02, mean val. rec. loss:  1.25278582e-02\n",
      "Epoch: 5504 mean train loss:  1.33019872e-02, mean val. rec. loss:  1.25267605e-02\n",
      "Epoch: 5505 mean train loss:  1.33007358e-02, mean val. rec. loss:  1.25256662e-02\n",
      "Epoch: 5506 mean train loss:  1.32994965e-02, mean val. rec. loss:  1.25245674e-02\n",
      "Epoch: 5507 mean train loss:  1.32982488e-02, mean val. rec. loss:  1.25234742e-02\n",
      "Epoch: 5508 mean train loss:  1.32970030e-02, mean val. rec. loss:  1.25223709e-02\n",
      "Epoch: 5509 mean train loss:  1.32957572e-02, mean val. rec. loss:  1.25212720e-02\n",
      "Epoch: 5510 mean train loss:  1.32945133e-02, mean val. rec. loss:  1.25201744e-02\n",
      "Epoch: 5511 mean train loss:  1.32932684e-02, mean val. rec. loss:  1.25190744e-02\n",
      "Epoch: 5512 mean train loss:  1.32920235e-02, mean val. rec. loss:  1.25179756e-02\n",
      "Epoch: 5513 mean train loss:  1.32907768e-02, mean val. rec. loss:  1.25168790e-02\n",
      "Epoch: 5514 mean train loss:  1.32895347e-02, mean val. rec. loss:  1.25157791e-02\n",
      "Epoch: 5515 mean train loss:  1.32882898e-02, mean val. rec. loss:  1.25146882e-02\n",
      "Epoch: 5516 mean train loss:  1.32870459e-02, mean val. rec. loss:  1.25135905e-02\n",
      "Epoch: 5517 mean train loss:  1.32858010e-02, mean val. rec. loss:  1.25124837e-02\n",
      "Epoch: 5518 mean train loss:  1.32845552e-02, mean val. rec. loss:  1.25113872e-02\n",
      "Epoch: 5519 mean train loss:  1.32833131e-02, mean val. rec. loss:  1.25102895e-02\n",
      "Epoch: 5520 mean train loss:  1.32820701e-02, mean val. rec. loss:  1.25091941e-02\n",
      "Epoch: 5521 mean train loss:  1.32808252e-02, mean val. rec. loss:  1.25080907e-02\n",
      "Epoch: 5522 mean train loss:  1.32795804e-02, mean val. rec. loss:  1.25069930e-02\n",
      "Epoch: 5523 mean train loss:  1.32783392e-02, mean val. rec. loss:  1.25058987e-02\n",
      "Epoch: 5524 mean train loss:  1.32770971e-02, mean val. rec. loss:  1.25048044e-02\n",
      "Epoch: 5525 mean train loss:  1.32758523e-02, mean val. rec. loss:  1.25037045e-02\n",
      "Epoch: 5526 mean train loss:  1.32746064e-02, mean val. rec. loss:  1.25026113e-02\n",
      "Epoch: 5527 mean train loss:  1.32733653e-02, mean val. rec. loss:  1.25015136e-02\n",
      "Epoch: 5528 mean train loss:  1.32721241e-02, mean val. rec. loss:  1.25004137e-02\n",
      "Epoch: 5529 mean train loss:  1.32708802e-02, mean val. rec. loss:  1.24993114e-02\n",
      "Epoch: 5530 mean train loss:  1.32696344e-02, mean val. rec. loss:  1.24982081e-02\n",
      "Epoch: 5531 mean train loss:  1.32683923e-02, mean val. rec. loss:  1.24971138e-02\n",
      "Epoch: 5532 mean train loss:  1.32671540e-02, mean val. rec. loss:  1.24960184e-02\n",
      "Epoch: 5533 mean train loss:  1.32659100e-02, mean val. rec. loss:  1.24949230e-02\n",
      "Epoch: 5534 mean train loss:  1.32646679e-02, mean val. rec. loss:  1.24938241e-02\n",
      "Epoch: 5535 mean train loss:  1.32634259e-02, mean val. rec. loss:  1.24927298e-02\n",
      "Epoch: 5536 mean train loss:  1.32621838e-02, mean val. rec. loss:  1.24916322e-02\n",
      "Epoch: 5537 mean train loss:  1.32609417e-02, mean val. rec. loss:  1.24905345e-02\n",
      "Epoch: 5538 mean train loss:  1.32596996e-02, mean val. rec. loss:  1.24894345e-02\n",
      "Epoch: 5539 mean train loss:  1.32584594e-02, mean val. rec. loss:  1.24883402e-02\n",
      "Epoch: 5540 mean train loss:  1.32572192e-02, mean val. rec. loss:  1.24872414e-02\n",
      "Epoch: 5541 mean train loss:  1.32559762e-02, mean val. rec. loss:  1.24861392e-02\n",
      "Epoch: 5542 mean train loss:  1.32547332e-02, mean val. rec. loss:  1.24850460e-02\n",
      "Epoch: 5543 mean train loss:  1.32534948e-02, mean val. rec. loss:  1.24839506e-02\n",
      "Epoch: 5544 mean train loss:  1.32522583e-02, mean val. rec. loss:  1.24828529e-02\n",
      "Epoch: 5545 mean train loss:  1.32510125e-02, mean val. rec. loss:  1.24817598e-02\n",
      "Epoch: 5546 mean train loss:  1.32497723e-02, mean val. rec. loss:  1.24806632e-02\n",
      "Epoch: 5547 mean train loss:  1.32485321e-02, mean val. rec. loss:  1.24795689e-02\n",
      "Epoch: 5548 mean train loss:  1.32472965e-02, mean val. rec. loss:  1.24784712e-02\n",
      "Epoch: 5549 mean train loss:  1.32460526e-02, mean val. rec. loss:  1.24773679e-02\n",
      "Epoch: 5550 mean train loss:  1.32448133e-02, mean val. rec. loss:  1.24762736e-02\n",
      "Epoch: 5551 mean train loss:  1.32435749e-02, mean val. rec. loss:  1.24751714e-02\n",
      "Epoch: 5552 mean train loss:  1.32423365e-02, mean val. rec. loss:  1.24740714e-02\n",
      "Epoch: 5553 mean train loss:  1.32410963e-02, mean val. rec. loss:  1.24729828e-02\n",
      "Epoch: 5554 mean train loss:  1.32398589e-02, mean val. rec. loss:  1.24718919e-02\n",
      "Epoch: 5555 mean train loss:  1.32386196e-02, mean val. rec. loss:  1.24707942e-02\n",
      "Epoch: 5556 mean train loss:  1.32373794e-02, mean val. rec. loss:  1.24696931e-02\n",
      "Epoch: 5557 mean train loss:  1.32361410e-02, mean val. rec. loss:  1.24686034e-02\n",
      "Epoch: 5558 mean train loss:  1.32349036e-02, mean val. rec. loss:  1.24675102e-02\n",
      "Epoch: 5559 mean train loss:  1.32336643e-02, mean val. rec. loss:  1.24664103e-02\n",
      "Epoch: 5560 mean train loss:  1.32324260e-02, mean val. rec. loss:  1.24653103e-02\n",
      "Epoch: 5561 mean train loss:  1.32311895e-02, mean val. rec. loss:  1.24642171e-02\n",
      "Epoch: 5562 mean train loss:  1.32299539e-02, mean val. rec. loss:  1.24631183e-02\n",
      "Epoch: 5563 mean train loss:  1.32287118e-02, mean val. rec. loss:  1.24620218e-02\n",
      "Epoch: 5564 mean train loss:  1.32274744e-02, mean val. rec. loss:  1.24609354e-02\n",
      "Epoch: 5565 mean train loss:  1.32262407e-02, mean val. rec. loss:  1.24598321e-02\n",
      "Epoch: 5566 mean train loss:  1.32250033e-02, mean val. rec. loss:  1.24587423e-02\n",
      "Epoch: 5567 mean train loss:  1.32237668e-02, mean val. rec. loss:  1.24576423e-02\n",
      "Epoch: 5568 mean train loss:  1.32225247e-02, mean val. rec. loss:  1.24565481e-02\n",
      "Epoch: 5569 mean train loss:  1.32212929e-02, mean val. rec. loss:  1.24554606e-02\n",
      "Epoch: 5570 mean train loss:  1.32200526e-02, mean val. rec. loss:  1.24543629e-02\n",
      "Epoch: 5571 mean train loss:  1.32188171e-02, mean val. rec. loss:  1.24532720e-02\n",
      "Epoch: 5572 mean train loss:  1.32175843e-02, mean val. rec. loss:  1.24521698e-02\n",
      "Epoch: 5573 mean train loss:  1.32163469e-02, mean val. rec. loss:  1.24510812e-02\n",
      "Epoch: 5574 mean train loss:  1.32151095e-02, mean val. rec. loss:  1.24499812e-02\n",
      "Epoch: 5575 mean train loss:  1.32138758e-02, mean val. rec. loss:  1.24488948e-02\n",
      "Epoch: 5576 mean train loss:  1.32126402e-02, mean val. rec. loss:  1.24477926e-02\n",
      "Epoch: 5577 mean train loss:  1.32114056e-02, mean val. rec. loss:  1.24467017e-02\n",
      "Epoch: 5578 mean train loss:  1.32101700e-02, mean val. rec. loss:  1.24456052e-02\n",
      "Epoch: 5579 mean train loss:  1.32089326e-02, mean val. rec. loss:  1.24445109e-02\n",
      "Epoch: 5580 mean train loss:  1.32076998e-02, mean val. rec. loss:  1.24434166e-02\n",
      "Epoch: 5581 mean train loss:  1.32064670e-02, mean val. rec. loss:  1.24423201e-02\n",
      "Epoch: 5582 mean train loss:  1.32052324e-02, mean val. rec. loss:  1.24412303e-02\n",
      "Epoch: 5583 mean train loss:  1.32039968e-02, mean val. rec. loss:  1.24401360e-02\n",
      "Epoch: 5584 mean train loss:  1.32027631e-02, mean val. rec. loss:  1.24390395e-02\n",
      "Epoch: 5585 mean train loss:  1.32015304e-02, mean val. rec. loss:  1.24379542e-02\n",
      "Epoch: 5586 mean train loss:  1.32002967e-02, mean val. rec. loss:  1.24368498e-02\n",
      "Epoch: 5587 mean train loss:  1.31990592e-02, mean val. rec. loss:  1.24357589e-02\n",
      "Epoch: 5588 mean train loss:  1.31978339e-02, mean val. rec. loss:  1.24346612e-02\n",
      "Epoch: 5589 mean train loss:  1.31965918e-02, mean val. rec. loss:  1.24335760e-02\n",
      "Epoch: 5590 mean train loss:  1.31953656e-02, mean val. rec. loss:  1.24324771e-02\n",
      "Epoch: 5591 mean train loss:  1.31941272e-02, mean val. rec. loss:  1.24313840e-02\n",
      "Epoch: 5592 mean train loss:  1.31928973e-02, mean val. rec. loss:  1.24302931e-02\n",
      "Epoch: 5593 mean train loss:  1.31916626e-02, mean val. rec. loss:  1.24291999e-02\n",
      "Epoch: 5594 mean train loss:  1.31904364e-02, mean val. rec. loss:  1.24281045e-02\n",
      "Epoch: 5595 mean train loss:  1.31891980e-02, mean val. rec. loss:  1.24270080e-02\n",
      "Epoch: 5596 mean train loss:  1.31879680e-02, mean val. rec. loss:  1.24259250e-02\n",
      "Epoch: 5597 mean train loss:  1.31867390e-02, mean val. rec. loss:  1.24248296e-02\n",
      "Epoch: 5598 mean train loss:  1.31855062e-02, mean val. rec. loss:  1.24237364e-02\n",
      "Epoch: 5599 mean train loss:  1.31842697e-02, mean val. rec. loss:  1.24226478e-02\n",
      "Epoch: 5600 mean train loss:  1.31830435e-02, mean val. rec. loss:  1.24215501e-02\n",
      "Epoch: 5601 mean train loss:  1.31818135e-02, mean val. rec. loss:  1.24204570e-02\n",
      "Epoch: 5602 mean train loss:  1.31805789e-02, mean val. rec. loss:  1.24193604e-02\n",
      "Epoch: 5603 mean train loss:  1.31793480e-02, mean val. rec. loss:  1.24182729e-02\n",
      "Epoch: 5604 mean train loss:  1.31781198e-02, mean val. rec. loss:  1.24171787e-02\n",
      "Epoch: 5605 mean train loss:  1.31768880e-02, mean val. rec. loss:  1.24160866e-02\n",
      "Epoch: 5606 mean train loss:  1.31756552e-02, mean val. rec. loss:  1.24149935e-02\n",
      "Epoch: 5607 mean train loss:  1.31744262e-02, mean val. rec. loss:  1.24139060e-02\n",
      "Epoch: 5608 mean train loss:  1.31731971e-02, mean val. rec. loss:  1.24128094e-02\n",
      "Epoch: 5609 mean train loss:  1.31719681e-02, mean val. rec. loss:  1.24117186e-02\n",
      "Epoch: 5610 mean train loss:  1.31707390e-02, mean val. rec. loss:  1.24106243e-02\n",
      "Epoch: 5611 mean train loss:  1.31695091e-02, mean val. rec. loss:  1.24095379e-02\n",
      "Epoch: 5612 mean train loss:  1.31682828e-02, mean val. rec. loss:  1.24084493e-02\n",
      "Epoch: 5613 mean train loss:  1.31670575e-02, mean val. rec. loss:  1.24073641e-02\n",
      "Epoch: 5614 mean train loss:  1.31658257e-02, mean val. rec. loss:  1.24062641e-02\n",
      "Epoch: 5615 mean train loss:  1.31645938e-02, mean val. rec. loss:  1.24051789e-02\n",
      "Epoch: 5616 mean train loss:  1.31633694e-02, mean val. rec. loss:  1.24040846e-02\n",
      "Epoch: 5617 mean train loss:  1.31621432e-02, mean val. rec. loss:  1.24029915e-02\n",
      "Epoch: 5618 mean train loss:  1.31609141e-02, mean val. rec. loss:  1.24018983e-02\n",
      "Epoch: 5619 mean train loss:  1.31596832e-02, mean val. rec. loss:  1.24008097e-02\n",
      "Epoch: 5620 mean train loss:  1.31584626e-02, mean val. rec. loss:  1.23997166e-02\n",
      "Epoch: 5621 mean train loss:  1.31572289e-02, mean val. rec. loss:  1.23986279e-02\n",
      "Epoch: 5622 mean train loss:  1.31560035e-02, mean val. rec. loss:  1.23975348e-02\n",
      "Epoch: 5623 mean train loss:  1.31547736e-02, mean val. rec. loss:  1.23964507e-02\n",
      "Epoch: 5624 mean train loss:  1.31535529e-02, mean val. rec. loss:  1.23953553e-02\n",
      "Epoch: 5625 mean train loss:  1.31523183e-02, mean val. rec. loss:  1.23942678e-02\n",
      "Epoch: 5626 mean train loss:  1.31511004e-02, mean val. rec. loss:  1.23931712e-02\n",
      "Epoch: 5627 mean train loss:  1.31498658e-02, mean val. rec. loss:  1.23920894e-02\n",
      "Epoch: 5628 mean train loss:  1.31486451e-02, mean val. rec. loss:  1.23909997e-02\n",
      "Epoch: 5629 mean train loss:  1.31474179e-02, mean val. rec. loss:  1.23899133e-02\n",
      "Epoch: 5630 mean train loss:  1.31461917e-02, mean val. rec. loss:  1.23888168e-02\n",
      "Epoch: 5631 mean train loss:  1.31449645e-02, mean val. rec. loss:  1.23877259e-02\n",
      "Epoch: 5632 mean train loss:  1.31437410e-02, mean val. rec. loss:  1.23866339e-02\n",
      "Epoch: 5633 mean train loss:  1.31425157e-02, mean val. rec. loss:  1.23855441e-02\n",
      "Epoch: 5634 mean train loss:  1.31412885e-02, mean val. rec. loss:  1.23844532e-02\n",
      "Epoch: 5635 mean train loss:  1.31400613e-02, mean val. rec. loss:  1.23833646e-02\n",
      "Epoch: 5636 mean train loss:  1.31388360e-02, mean val. rec. loss:  1.23822794e-02\n",
      "Epoch: 5637 mean train loss:  1.31376107e-02, mean val. rec. loss:  1.23811919e-02\n",
      "Epoch: 5638 mean train loss:  1.31363872e-02, mean val. rec. loss:  1.23800999e-02\n",
      "Epoch: 5639 mean train loss:  1.31351656e-02, mean val. rec. loss:  1.23790124e-02\n",
      "Epoch: 5640 mean train loss:  1.31339440e-02, mean val. rec. loss:  1.23779215e-02\n",
      "Epoch: 5641 mean train loss:  1.31327187e-02, mean val. rec. loss:  1.23768295e-02\n",
      "Epoch: 5642 mean train loss:  1.31314925e-02, mean val. rec. loss:  1.23757443e-02\n",
      "Epoch: 5643 mean train loss:  1.31302718e-02, mean val. rec. loss:  1.23746534e-02\n",
      "Epoch: 5644 mean train loss:  1.31290502e-02, mean val. rec. loss:  1.23735625e-02\n",
      "Epoch: 5645 mean train loss:  1.31278305e-02, mean val. rec. loss:  1.23724739e-02\n",
      "Epoch: 5646 mean train loss:  1.31266051e-02, mean val. rec. loss:  1.23713910e-02\n",
      "Epoch: 5647 mean train loss:  1.31253798e-02, mean val. rec. loss:  1.23703035e-02\n",
      "Epoch: 5648 mean train loss:  1.31241601e-02, mean val. rec. loss:  1.23692171e-02\n",
      "Epoch: 5649 mean train loss:  1.31229413e-02, mean val. rec. loss:  1.23681296e-02\n",
      "Epoch: 5650 mean train loss:  1.31217160e-02, mean val. rec. loss:  1.23670387e-02\n",
      "Epoch: 5651 mean train loss:  1.31204934e-02, mean val. rec. loss:  1.23659490e-02\n",
      "Epoch: 5652 mean train loss:  1.31192737e-02, mean val. rec. loss:  1.23648649e-02\n",
      "Epoch: 5653 mean train loss:  1.31180521e-02, mean val. rec. loss:  1.23637774e-02\n",
      "Epoch: 5654 mean train loss:  1.31168296e-02, mean val. rec. loss:  1.23626877e-02\n",
      "Epoch: 5655 mean train loss:  1.31156089e-02, mean val. rec. loss:  1.23615968e-02\n",
      "Epoch: 5656 mean train loss:  1.31143892e-02, mean val. rec. loss:  1.23605127e-02\n",
      "Epoch: 5657 mean train loss:  1.31131685e-02, mean val. rec. loss:  1.23594230e-02\n",
      "Epoch: 5658 mean train loss:  1.31119441e-02, mean val. rec. loss:  1.23583423e-02\n",
      "Epoch: 5659 mean train loss:  1.31107281e-02, mean val. rec. loss:  1.23572571e-02\n",
      "Epoch: 5660 mean train loss:  1.31095121e-02, mean val. rec. loss:  1.23561673e-02\n",
      "Epoch: 5661 mean train loss:  1.31082877e-02, mean val. rec. loss:  1.23550821e-02\n",
      "Epoch: 5662 mean train loss:  1.31070670e-02, mean val. rec. loss:  1.23539912e-02\n",
      "Epoch: 5663 mean train loss:  1.31058492e-02, mean val. rec. loss:  1.23529060e-02\n",
      "Epoch: 5664 mean train loss:  1.31046313e-02, mean val. rec. loss:  1.23518219e-02\n",
      "Epoch: 5665 mean train loss:  1.31034116e-02, mean val. rec. loss:  1.23507378e-02\n",
      "Epoch: 5666 mean train loss:  1.31021918e-02, mean val. rec. loss:  1.23496424e-02\n",
      "Epoch: 5667 mean train loss:  1.31009730e-02, mean val. rec. loss:  1.23485606e-02\n",
      "Epoch: 5668 mean train loss:  1.30997561e-02, mean val. rec. loss:  1.23474765e-02\n",
      "Epoch: 5669 mean train loss:  1.30985401e-02, mean val. rec. loss:  1.23463856e-02\n",
      "Epoch: 5670 mean train loss:  1.30973203e-02, mean val. rec. loss:  1.23453038e-02\n",
      "Epoch: 5671 mean train loss:  1.30961025e-02, mean val. rec. loss:  1.23442129e-02\n",
      "Epoch: 5672 mean train loss:  1.30948864e-02, mean val. rec. loss:  1.23431323e-02\n",
      "Epoch: 5673 mean train loss:  1.30936723e-02, mean val. rec. loss:  1.23420448e-02\n",
      "Epoch: 5674 mean train loss:  1.30924516e-02, mean val. rec. loss:  1.23409630e-02\n",
      "Epoch: 5675 mean train loss:  1.30912328e-02, mean val. rec. loss:  1.23398766e-02\n",
      "Epoch: 5676 mean train loss:  1.30900196e-02, mean val. rec. loss:  1.23387891e-02\n",
      "Epoch: 5677 mean train loss:  1.30888027e-02, mean val. rec. loss:  1.23377039e-02\n",
      "Epoch: 5678 mean train loss:  1.30875829e-02, mean val. rec. loss:  1.23366130e-02\n",
      "Epoch: 5679 mean train loss:  1.30863669e-02, mean val. rec. loss:  1.23355312e-02\n",
      "Epoch: 5680 mean train loss:  1.30851556e-02, mean val. rec. loss:  1.23344426e-02\n",
      "Epoch: 5681 mean train loss:  1.30839368e-02, mean val. rec. loss:  1.23333630e-02\n",
      "Epoch: 5682 mean train loss:  1.30827245e-02, mean val. rec. loss:  1.23322790e-02\n",
      "Epoch: 5683 mean train loss:  1.30815047e-02, mean val. rec. loss:  1.23311949e-02\n",
      "Epoch: 5684 mean train loss:  1.30802943e-02, mean val. rec. loss:  1.23301085e-02\n",
      "Epoch: 5685 mean train loss:  1.30790783e-02, mean val. rec. loss:  1.23290245e-02\n",
      "Epoch: 5686 mean train loss:  1.30778614e-02, mean val. rec. loss:  1.23279426e-02\n",
      "Epoch: 5687 mean train loss:  1.30766463e-02, mean val. rec. loss:  1.23268608e-02\n",
      "Epoch: 5688 mean train loss:  1.30754359e-02, mean val. rec. loss:  1.23257767e-02\n",
      "Epoch: 5689 mean train loss:  1.30742217e-02, mean val. rec. loss:  1.23246904e-02\n",
      "Epoch: 5690 mean train loss:  1.30730076e-02, mean val. rec. loss:  1.23236018e-02\n",
      "Epoch: 5691 mean train loss:  1.30717906e-02, mean val. rec. loss:  1.23225222e-02\n",
      "Epoch: 5692 mean train loss:  1.30705793e-02, mean val. rec. loss:  1.23214359e-02\n",
      "Epoch: 5693 mean train loss:  1.30693688e-02, mean val. rec. loss:  1.23203563e-02\n",
      "Epoch: 5694 mean train loss:  1.30681575e-02, mean val. rec. loss:  1.23192757e-02\n",
      "Epoch: 5695 mean train loss:  1.30669433e-02, mean val. rec. loss:  1.23181950e-02\n",
      "Epoch: 5696 mean train loss:  1.30657292e-02, mean val. rec. loss:  1.23171154e-02\n",
      "Epoch: 5697 mean train loss:  1.30645178e-02, mean val. rec. loss:  1.23160325e-02\n",
      "Epoch: 5698 mean train loss:  1.30633093e-02, mean val. rec. loss:  1.23149461e-02\n",
      "Epoch: 5699 mean train loss:  1.30620933e-02, mean val. rec. loss:  1.23138666e-02\n",
      "Epoch: 5700 mean train loss:  1.30608847e-02, mean val. rec. loss:  1.23127768e-02\n",
      "Epoch: 5701 mean train loss:  1.30596696e-02, mean val. rec. loss:  1.23117018e-02\n",
      "Epoch: 5702 mean train loss:  1.30584592e-02, mean val. rec. loss:  1.23106087e-02\n",
      "Epoch: 5703 mean train loss:  1.30572441e-02, mean val. rec. loss:  1.23095257e-02\n",
      "Epoch: 5704 mean train loss:  1.30560337e-02, mean val. rec. loss:  1.23084450e-02\n",
      "Epoch: 5705 mean train loss:  1.30548279e-02, mean val. rec. loss:  1.23073621e-02\n",
      "Epoch: 5706 mean train loss:  1.30536166e-02, mean val. rec. loss:  1.23062860e-02\n",
      "Epoch: 5707 mean train loss:  1.30524034e-02, mean val. rec. loss:  1.23052019e-02\n",
      "Epoch: 5708 mean train loss:  1.30511957e-02, mean val. rec. loss:  1.23041223e-02\n",
      "Epoch: 5709 mean train loss:  1.30499872e-02, mean val. rec. loss:  1.23030428e-02\n",
      "Epoch: 5710 mean train loss:  1.30487758e-02, mean val. rec. loss:  1.23019700e-02\n",
      "Epoch: 5711 mean train loss:  1.30475673e-02, mean val. rec. loss:  1.23008814e-02\n",
      "Epoch: 5712 mean train loss:  1.30463596e-02, mean val. rec. loss:  1.22997951e-02\n",
      "Epoch: 5713 mean train loss:  1.30451511e-02, mean val. rec. loss:  1.22987110e-02\n",
      "Epoch: 5714 mean train loss:  1.30439406e-02, mean val. rec. loss:  1.22976292e-02\n",
      "Epoch: 5715 mean train loss:  1.30427321e-02, mean val. rec. loss:  1.22965564e-02\n",
      "Epoch: 5716 mean train loss:  1.30415263e-02, mean val. rec. loss:  1.22954724e-02\n",
      "Epoch: 5717 mean train loss:  1.30403187e-02, mean val. rec. loss:  1.22943951e-02\n",
      "Epoch: 5718 mean train loss:  1.30391110e-02, mean val. rec. loss:  1.22933076e-02\n",
      "Epoch: 5719 mean train loss:  1.30378969e-02, mean val. rec. loss:  1.22922326e-02\n",
      "Epoch: 5720 mean train loss:  1.30366949e-02, mean val. rec. loss:  1.22911598e-02\n",
      "Epoch: 5721 mean train loss:  1.30354891e-02, mean val. rec. loss:  1.22900758e-02\n",
      "Epoch: 5722 mean train loss:  1.30342805e-02, mean val. rec. loss:  1.22890007e-02\n",
      "Epoch: 5723 mean train loss:  1.30330766e-02, mean val. rec. loss:  1.22879178e-02\n",
      "Epoch: 5724 mean train loss:  1.30318718e-02, mean val. rec. loss:  1.22868337e-02\n",
      "Epoch: 5725 mean train loss:  1.30306632e-02, mean val. rec. loss:  1.22857496e-02\n",
      "Epoch: 5726 mean train loss:  1.30294584e-02, mean val. rec. loss:  1.22846746e-02\n",
      "Epoch: 5727 mean train loss:  1.30282517e-02, mean val. rec. loss:  1.22835905e-02\n",
      "Epoch: 5728 mean train loss:  1.30270440e-02, mean val. rec. loss:  1.22825121e-02\n",
      "Epoch: 5729 mean train loss:  1.30258364e-02, mean val. rec. loss:  1.22814417e-02\n",
      "Epoch: 5730 mean train loss:  1.30246353e-02, mean val. rec. loss:  1.22803598e-02\n",
      "Epoch: 5731 mean train loss:  1.30234305e-02, mean val. rec. loss:  1.22792837e-02\n",
      "Epoch: 5732 mean train loss:  1.30222219e-02, mean val. rec. loss:  1.22782064e-02\n",
      "Epoch: 5733 mean train loss:  1.30210171e-02, mean val. rec. loss:  1.22771246e-02\n",
      "Epoch: 5734 mean train loss:  1.30198150e-02, mean val. rec. loss:  1.22760428e-02\n",
      "Epoch: 5735 mean train loss:  1.30186111e-02, mean val. rec. loss:  1.22749678e-02\n",
      "Epoch: 5736 mean train loss:  1.30174063e-02, mean val. rec. loss:  1.22738928e-02\n",
      "Epoch: 5737 mean train loss:  1.30162033e-02, mean val. rec. loss:  1.22728110e-02\n",
      "Epoch: 5738 mean train loss:  1.30149994e-02, mean val. rec. loss:  1.22717303e-02\n",
      "Epoch: 5739 mean train loss:  1.30137974e-02, mean val. rec. loss:  1.22706564e-02\n",
      "Epoch: 5740 mean train loss:  1.30125925e-02, mean val. rec. loss:  1.22695769e-02\n",
      "Epoch: 5741 mean train loss:  1.30113877e-02, mean val. rec. loss:  1.22685030e-02\n",
      "Epoch: 5742 mean train loss:  1.30101894e-02, mean val. rec. loss:  1.22674223e-02\n",
      "Epoch: 5743 mean train loss:  1.30089855e-02, mean val. rec. loss:  1.22663394e-02\n",
      "Epoch: 5744 mean train loss:  1.30077806e-02, mean val. rec. loss:  1.22652644e-02\n",
      "Epoch: 5745 mean train loss:  1.30065814e-02, mean val. rec. loss:  1.22641882e-02\n",
      "Epoch: 5746 mean train loss:  1.30053812e-02, mean val. rec. loss:  1.22631132e-02\n",
      "Epoch: 5747 mean train loss:  1.30041773e-02, mean val. rec. loss:  1.22620337e-02\n",
      "Epoch: 5748 mean train loss:  1.30029734e-02, mean val. rec. loss:  1.22609552e-02\n",
      "Epoch: 5749 mean train loss:  1.30017732e-02, mean val. rec. loss:  1.22598825e-02\n",
      "Epoch: 5750 mean train loss:  1.30005739e-02, mean val. rec. loss:  1.22588052e-02\n",
      "Epoch: 5751 mean train loss:  1.29993710e-02, mean val. rec. loss:  1.22577268e-02\n",
      "Epoch: 5752 mean train loss:  1.29981699e-02, mean val. rec. loss:  1.22566473e-02\n",
      "Epoch: 5753 mean train loss:  1.29969706e-02, mean val. rec. loss:  1.22555734e-02\n",
      "Epoch: 5754 mean train loss:  1.29957695e-02, mean val. rec. loss:  1.22544984e-02\n",
      "Epoch: 5755 mean train loss:  1.29945721e-02, mean val. rec. loss:  1.22534245e-02\n",
      "Epoch: 5756 mean train loss:  1.29933756e-02, mean val. rec. loss:  1.22523404e-02\n",
      "Epoch: 5757 mean train loss:  1.29921736e-02, mean val. rec. loss:  1.22512677e-02\n",
      "Epoch: 5758 mean train loss:  1.29909697e-02, mean val. rec. loss:  1.22501949e-02\n",
      "Epoch: 5759 mean train loss:  1.29897732e-02, mean val. rec. loss:  1.22491222e-02\n",
      "Epoch: 5760 mean train loss:  1.29885796e-02, mean val. rec. loss:  1.22480483e-02\n",
      "Epoch: 5761 mean train loss:  1.29873785e-02, mean val. rec. loss:  1.22469688e-02\n",
      "Epoch: 5762 mean train loss:  1.29861811e-02, mean val. rec. loss:  1.22458892e-02\n",
      "Epoch: 5763 mean train loss:  1.29849828e-02, mean val. rec. loss:  1.22448176e-02\n",
      "Epoch: 5764 mean train loss:  1.29837844e-02, mean val. rec. loss:  1.22437449e-02\n",
      "Epoch: 5765 mean train loss:  1.29825852e-02, mean val. rec. loss:  1.22426631e-02\n",
      "Epoch: 5766 mean train loss:  1.29813859e-02, mean val. rec. loss:  1.22415881e-02\n",
      "Epoch: 5767 mean train loss:  1.29801904e-02, mean val. rec. loss:  1.22405164e-02\n",
      "Epoch: 5768 mean train loss:  1.29789958e-02, mean val. rec. loss:  1.22394403e-02\n",
      "Epoch: 5769 mean train loss:  1.29777938e-02, mean val. rec. loss:  1.22383687e-02\n",
      "Epoch: 5770 mean train loss:  1.29765982e-02, mean val. rec. loss:  1.22372971e-02\n",
      "Epoch: 5771 mean train loss:  1.29753990e-02, mean val. rec. loss:  1.22362209e-02\n",
      "Epoch: 5772 mean train loss:  1.29742090e-02, mean val. rec. loss:  1.22351437e-02\n",
      "Epoch: 5773 mean train loss:  1.29730070e-02, mean val. rec. loss:  1.22340675e-02\n",
      "Epoch: 5774 mean train loss:  1.29718105e-02, mean val. rec. loss:  1.22329970e-02\n",
      "Epoch: 5775 mean train loss:  1.29706169e-02, mean val. rec. loss:  1.22319243e-02\n",
      "Epoch: 5776 mean train loss:  1.29694195e-02, mean val. rec. loss:  1.22308448e-02\n",
      "Epoch: 5777 mean train loss:  1.29682221e-02, mean val. rec. loss:  1.22297675e-02\n",
      "Epoch: 5778 mean train loss:  1.29670256e-02, mean val. rec. loss:  1.22286936e-02\n",
      "Epoch: 5779 mean train loss:  1.29658311e-02, mean val. rec. loss:  1.22276197e-02\n",
      "Epoch: 5780 mean train loss:  1.29646374e-02, mean val. rec. loss:  1.22265459e-02\n",
      "Epoch: 5781 mean train loss:  1.29634419e-02, mean val. rec. loss:  1.22254810e-02\n",
      "Epoch: 5782 mean train loss:  1.29622519e-02, mean val. rec. loss:  1.22244060e-02\n",
      "Epoch: 5783 mean train loss:  1.29610620e-02, mean val. rec. loss:  1.22233378e-02\n",
      "Epoch: 5784 mean train loss:  1.29598646e-02, mean val. rec. loss:  1.22222549e-02\n",
      "Epoch: 5785 mean train loss:  1.29586663e-02, mean val. rec. loss:  1.22211855e-02\n",
      "Epoch: 5786 mean train loss:  1.29574773e-02, mean val. rec. loss:  1.22201105e-02\n",
      "Epoch: 5787 mean train loss:  1.29562864e-02, mean val. rec. loss:  1.22190378e-02\n",
      "Epoch: 5788 mean train loss:  1.29550881e-02, mean val. rec. loss:  1.22179651e-02\n",
      "Epoch: 5789 mean train loss:  1.29539000e-02, mean val. rec. loss:  1.22168912e-02\n",
      "Epoch: 5790 mean train loss:  1.29527035e-02, mean val. rec. loss:  1.22158173e-02\n",
      "Epoch: 5791 mean train loss:  1.29515136e-02, mean val. rec. loss:  1.22147457e-02\n",
      "Epoch: 5792 mean train loss:  1.29503181e-02, mean val. rec. loss:  1.22136764e-02\n",
      "Epoch: 5793 mean train loss:  1.29491319e-02, mean val. rec. loss:  1.22126059e-02\n",
      "Epoch: 5794 mean train loss:  1.29479363e-02, mean val. rec. loss:  1.22115331e-02\n",
      "Epoch: 5795 mean train loss:  1.29467427e-02, mean val. rec. loss:  1.22104695e-02\n",
      "Epoch: 5796 mean train loss:  1.29455527e-02, mean val. rec. loss:  1.22093933e-02\n",
      "Epoch: 5797 mean train loss:  1.29443637e-02, mean val. rec. loss:  1.22083206e-02\n",
      "Epoch: 5798 mean train loss:  1.29431701e-02, mean val. rec. loss:  1.22072433e-02\n",
      "Epoch: 5799 mean train loss:  1.29419773e-02, mean val. rec. loss:  1.22061694e-02\n",
      "Epoch: 5800 mean train loss:  1.29407920e-02, mean val. rec. loss:  1.22051058e-02\n",
      "Epoch: 5801 mean train loss:  1.29395993e-02, mean val. rec. loss:  1.22040239e-02\n",
      "Epoch: 5802 mean train loss:  1.29384066e-02, mean val. rec. loss:  1.22029625e-02\n",
      "Epoch: 5803 mean train loss:  1.29372185e-02, mean val. rec. loss:  1.22018864e-02\n",
      "Epoch: 5804 mean train loss:  1.29360286e-02, mean val. rec. loss:  1.22008216e-02\n",
      "Epoch: 5805 mean train loss:  1.29348386e-02, mean val. rec. loss:  1.21997398e-02\n",
      "Epoch: 5806 mean train loss:  1.29336505e-02, mean val. rec. loss:  1.21986704e-02\n",
      "Epoch: 5807 mean train loss:  1.29324587e-02, mean val. rec. loss:  1.21976068e-02\n",
      "Epoch: 5808 mean train loss:  1.29312707e-02, mean val. rec. loss:  1.21965363e-02\n",
      "Epoch: 5809 mean train loss:  1.29300863e-02, mean val. rec. loss:  1.21954670e-02\n",
      "Epoch: 5810 mean train loss:  1.29288964e-02, mean val. rec. loss:  1.21943919e-02\n",
      "Epoch: 5811 mean train loss:  1.29277046e-02, mean val. rec. loss:  1.21933215e-02\n",
      "Epoch: 5812 mean train loss:  1.29265165e-02, mean val. rec. loss:  1.21922510e-02\n",
      "Epoch: 5813 mean train loss:  1.29253284e-02, mean val. rec. loss:  1.21911839e-02\n",
      "Epoch: 5814 mean train loss:  1.29241413e-02, mean val. rec. loss:  1.21901123e-02\n",
      "Epoch: 5815 mean train loss:  1.29229541e-02, mean val. rec. loss:  1.21890441e-02\n",
      "Epoch: 5816 mean train loss:  1.29217660e-02, mean val. rec. loss:  1.21879782e-02\n",
      "Epoch: 5817 mean train loss:  1.29205798e-02, mean val. rec. loss:  1.21869054e-02\n",
      "Epoch: 5818 mean train loss:  1.29193955e-02, mean val. rec. loss:  1.21858338e-02\n",
      "Epoch: 5819 mean train loss:  1.29182055e-02, mean val. rec. loss:  1.21847611e-02\n",
      "Epoch: 5820 mean train loss:  1.29170193e-02, mean val. rec. loss:  1.21837008e-02\n",
      "Epoch: 5821 mean train loss:  1.29158387e-02, mean val. rec. loss:  1.21826281e-02\n",
      "Epoch: 5822 mean train loss:  1.29146460e-02, mean val. rec. loss:  1.21815599e-02\n",
      "Epoch: 5823 mean train loss:  1.29134616e-02, mean val. rec. loss:  1.21804939e-02\n",
      "Epoch: 5824 mean train loss:  1.29122763e-02, mean val. rec. loss:  1.21794303e-02\n",
      "Epoch: 5825 mean train loss:  1.29110938e-02, mean val. rec. loss:  1.21783575e-02\n",
      "Epoch: 5826 mean train loss:  1.29099067e-02, mean val. rec. loss:  1.21772904e-02\n",
      "Epoch: 5827 mean train loss:  1.29087214e-02, mean val. rec. loss:  1.21762234e-02\n",
      "Epoch: 5828 mean train loss:  1.29075370e-02, mean val. rec. loss:  1.21751574e-02\n",
      "Epoch: 5829 mean train loss:  1.29063546e-02, mean val. rec. loss:  1.21740847e-02\n",
      "Epoch: 5830 mean train loss:  1.29051693e-02, mean val. rec. loss:  1.21730176e-02\n",
      "Epoch: 5831 mean train loss:  1.29039858e-02, mean val. rec. loss:  1.21719449e-02\n",
      "Epoch: 5832 mean train loss:  1.29027996e-02, mean val. rec. loss:  1.21708778e-02\n",
      "Epoch: 5833 mean train loss:  1.29016181e-02, mean val. rec. loss:  1.21698107e-02\n",
      "Epoch: 5834 mean train loss:  1.29004365e-02, mean val. rec. loss:  1.21687414e-02\n",
      "Epoch: 5835 mean train loss:  1.28992484e-02, mean val. rec. loss:  1.21676732e-02\n",
      "Epoch: 5836 mean train loss:  1.28980622e-02, mean val. rec. loss:  1.21666084e-02\n",
      "Epoch: 5837 mean train loss:  1.28968834e-02, mean val. rec. loss:  1.21655459e-02\n",
      "Epoch: 5838 mean train loss:  1.28957019e-02, mean val. rec. loss:  1.21644742e-02\n",
      "Epoch: 5839 mean train loss:  1.28945157e-02, mean val. rec. loss:  1.21634049e-02\n",
      "Epoch: 5840 mean train loss:  1.28933369e-02, mean val. rec. loss:  1.21623412e-02\n",
      "Epoch: 5841 mean train loss:  1.28921507e-02, mean val. rec. loss:  1.21612753e-02\n",
      "Epoch: 5842 mean train loss:  1.28909719e-02, mean val. rec. loss:  1.21602048e-02\n",
      "Epoch: 5843 mean train loss:  1.28897885e-02, mean val. rec. loss:  1.21591298e-02\n",
      "Epoch: 5844 mean train loss:  1.28886032e-02, mean val. rec. loss:  1.21580741e-02\n",
      "Epoch: 5845 mean train loss:  1.28874254e-02, mean val. rec. loss:  1.21570070e-02\n",
      "Epoch: 5846 mean train loss:  1.28862475e-02, mean val. rec. loss:  1.21559422e-02\n",
      "Epoch: 5847 mean train loss:  1.28850641e-02, mean val. rec. loss:  1.21548729e-02\n",
      "Epoch: 5848 mean train loss:  1.28838816e-02, mean val. rec. loss:  1.21538013e-02\n",
      "Epoch: 5849 mean train loss:  1.28827029e-02, mean val. rec. loss:  1.21527387e-02\n",
      "Epoch: 5850 mean train loss:  1.28815250e-02, mean val. rec. loss:  1.21516739e-02\n",
      "Epoch: 5851 mean train loss:  1.28803453e-02, mean val. rec. loss:  1.21506034e-02\n",
      "Epoch: 5852 mean train loss:  1.28791628e-02, mean val. rec. loss:  1.21495341e-02\n",
      "Epoch: 5853 mean train loss:  1.28779813e-02, mean val. rec. loss:  1.21484750e-02\n",
      "Epoch: 5854 mean train loss:  1.28768034e-02, mean val. rec. loss:  1.21474090e-02\n",
      "Epoch: 5855 mean train loss:  1.28756247e-02, mean val. rec. loss:  1.21463408e-02\n",
      "Epoch: 5856 mean train loss:  1.28744422e-02, mean val. rec. loss:  1.21452749e-02\n",
      "Epoch: 5857 mean train loss:  1.28732644e-02, mean val. rec. loss:  1.21442101e-02\n",
      "Epoch: 5858 mean train loss:  1.28720791e-02, mean val. rec. loss:  1.21431487e-02\n",
      "Epoch: 5859 mean train loss:  1.28709068e-02, mean val. rec. loss:  1.21420816e-02\n",
      "Epoch: 5860 mean train loss:  1.28697290e-02, mean val. rec. loss:  1.21410134e-02\n",
      "Epoch: 5861 mean train loss:  1.28685456e-02, mean val. rec. loss:  1.21399429e-02\n",
      "Epoch: 5862 mean train loss:  1.28673677e-02, mean val. rec. loss:  1.21388827e-02\n",
      "Epoch: 5863 mean train loss:  1.28661946e-02, mean val. rec. loss:  1.21378167e-02\n",
      "Epoch: 5864 mean train loss:  1.28650121e-02, mean val. rec. loss:  1.21367542e-02\n",
      "Epoch: 5865 mean train loss:  1.28638389e-02, mean val. rec. loss:  1.21356882e-02\n",
      "Epoch: 5866 mean train loss:  1.28626573e-02, mean val. rec. loss:  1.21346268e-02\n",
      "Epoch: 5867 mean train loss:  1.28614841e-02, mean val. rec. loss:  1.21335586e-02\n",
      "Epoch: 5868 mean train loss:  1.28603026e-02, mean val. rec. loss:  1.21325018e-02\n",
      "Epoch: 5869 mean train loss:  1.28591294e-02, mean val. rec. loss:  1.21314347e-02\n",
      "Epoch: 5870 mean train loss:  1.28579544e-02, mean val. rec. loss:  1.21303654e-02\n",
      "Epoch: 5871 mean train loss:  1.28567737e-02, mean val. rec. loss:  1.21293040e-02\n",
      "Epoch: 5872 mean train loss:  1.28555959e-02, mean val. rec. loss:  1.21282335e-02\n",
      "Epoch: 5873 mean train loss:  1.28544236e-02, mean val. rec. loss:  1.21271675e-02\n",
      "Epoch: 5874 mean train loss:  1.28532495e-02, mean val. rec. loss:  1.21261095e-02\n",
      "Epoch: 5875 mean train loss:  1.28520736e-02, mean val. rec. loss:  1.21250493e-02\n",
      "Epoch: 5876 mean train loss:  1.28508976e-02, mean val. rec. loss:  1.21239879e-02\n",
      "Epoch: 5877 mean train loss:  1.28497216e-02, mean val. rec. loss:  1.21229242e-02\n",
      "Epoch: 5878 mean train loss:  1.28485484e-02, mean val. rec. loss:  1.21218583e-02\n",
      "Epoch: 5879 mean train loss:  1.28473715e-02, mean val. rec. loss:  1.21207901e-02\n",
      "Epoch: 5880 mean train loss:  1.28461956e-02, mean val. rec. loss:  1.21197253e-02\n",
      "Epoch: 5881 mean train loss:  1.28450214e-02, mean val. rec. loss:  1.21186627e-02\n",
      "Epoch: 5882 mean train loss:  1.28438445e-02, mean val. rec. loss:  1.21176013e-02\n",
      "Epoch: 5883 mean train loss:  1.28426723e-02, mean val. rec. loss:  1.21165376e-02\n",
      "Epoch: 5884 mean train loss:  1.28415000e-02, mean val. rec. loss:  1.21154762e-02\n",
      "Epoch: 5885 mean train loss:  1.28403250e-02, mean val. rec. loss:  1.21144126e-02\n",
      "Epoch: 5886 mean train loss:  1.28391490e-02, mean val. rec. loss:  1.21133489e-02\n",
      "Epoch: 5887 mean train loss:  1.28379786e-02, mean val. rec. loss:  1.21122875e-02\n",
      "Epoch: 5888 mean train loss:  1.28368027e-02, mean val. rec. loss:  1.21112306e-02\n",
      "Epoch: 5889 mean train loss:  1.28356360e-02, mean val. rec. loss:  1.21101670e-02\n",
      "Epoch: 5890 mean train loss:  1.28344563e-02, mean val. rec. loss:  1.21091010e-02\n",
      "Epoch: 5891 mean train loss:  1.28332887e-02, mean val. rec. loss:  1.21080351e-02\n",
      "Epoch: 5892 mean train loss:  1.28321109e-02, mean val. rec. loss:  1.21069805e-02\n",
      "Epoch: 5893 mean train loss:  1.28309414e-02, mean val. rec. loss:  1.21059157e-02\n",
      "Epoch: 5894 mean train loss:  1.28297710e-02, mean val. rec. loss:  1.21048554e-02\n",
      "Epoch: 5895 mean train loss:  1.28285988e-02, mean val. rec. loss:  1.21037895e-02\n",
      "Epoch: 5896 mean train loss:  1.28274237e-02, mean val. rec. loss:  1.21027281e-02\n",
      "Epoch: 5897 mean train loss:  1.28262524e-02, mean val. rec. loss:  1.21016701e-02\n",
      "Epoch: 5898 mean train loss:  1.28250848e-02, mean val. rec. loss:  1.21006087e-02\n",
      "Epoch: 5899 mean train loss:  1.28239135e-02, mean val. rec. loss:  1.20995461e-02\n",
      "Epoch: 5900 mean train loss:  1.28227422e-02, mean val. rec. loss:  1.20984847e-02\n",
      "Epoch: 5901 mean train loss:  1.28215709e-02, mean val. rec. loss:  1.20974233e-02\n",
      "Epoch: 5902 mean train loss:  1.28203996e-02, mean val. rec. loss:  1.20963619e-02\n",
      "Epoch: 5903 mean train loss:  1.28192301e-02, mean val. rec. loss:  1.20952983e-02\n",
      "Epoch: 5904 mean train loss:  1.28180588e-02, mean val. rec. loss:  1.20942346e-02\n",
      "Epoch: 5905 mean train loss:  1.28168875e-02, mean val. rec. loss:  1.20931732e-02\n",
      "Epoch: 5906 mean train loss:  1.28157189e-02, mean val. rec. loss:  1.20921175e-02\n",
      "Epoch: 5907 mean train loss:  1.28145532e-02, mean val. rec. loss:  1.20910606e-02\n",
      "Epoch: 5908 mean train loss:  1.28133791e-02, mean val. rec. loss:  1.20899935e-02\n",
      "Epoch: 5909 mean train loss:  1.28122078e-02, mean val. rec. loss:  1.20889355e-02\n",
      "Epoch: 5910 mean train loss:  1.28110411e-02, mean val. rec. loss:  1.20878696e-02\n",
      "Epoch: 5911 mean train loss:  1.28098745e-02, mean val. rec. loss:  1.20868048e-02\n",
      "Epoch: 5912 mean train loss:  1.28087031e-02, mean val. rec. loss:  1.20857479e-02\n",
      "Epoch: 5913 mean train loss:  1.28075355e-02, mean val. rec. loss:  1.20846876e-02\n",
      "Epoch: 5914 mean train loss:  1.28063661e-02, mean val. rec. loss:  1.20836228e-02\n",
      "Epoch: 5915 mean train loss:  1.28051966e-02, mean val. rec. loss:  1.20825614e-02\n",
      "Epoch: 5916 mean train loss:  1.28040290e-02, mean val. rec. loss:  1.20815023e-02\n",
      "Epoch: 5917 mean train loss:  1.28028624e-02, mean val. rec. loss:  1.20804454e-02\n",
      "Epoch: 5918 mean train loss:  1.28016920e-02, mean val. rec. loss:  1.20793931e-02\n",
      "Epoch: 5919 mean train loss:  1.28005263e-02, mean val. rec. loss:  1.20783294e-02\n",
      "Epoch: 5920 mean train loss:  1.27993624e-02, mean val. rec. loss:  1.20772692e-02\n",
      "Epoch: 5921 mean train loss:  1.27981929e-02, mean val. rec. loss:  1.20762044e-02\n",
      "Epoch: 5922 mean train loss:  1.27970216e-02, mean val. rec. loss:  1.20751464e-02\n",
      "Epoch: 5923 mean train loss:  1.27958596e-02, mean val. rec. loss:  1.20740861e-02\n",
      "Epoch: 5924 mean train loss:  1.27946939e-02, mean val. rec. loss:  1.20730247e-02\n",
      "Epoch: 5925 mean train loss:  1.27935263e-02, mean val. rec. loss:  1.20719690e-02\n",
      "Epoch: 5926 mean train loss:  1.27923596e-02, mean val. rec. loss:  1.20709064e-02\n",
      "Epoch: 5927 mean train loss:  1.27911939e-02, mean val. rec. loss:  1.20698462e-02\n",
      "Epoch: 5928 mean train loss:  1.27900272e-02, mean val. rec. loss:  1.20687893e-02\n",
      "Epoch: 5929 mean train loss:  1.27888624e-02, mean val. rec. loss:  1.20677370e-02\n",
      "Epoch: 5930 mean train loss:  1.27876939e-02, mean val. rec. loss:  1.20666767e-02\n",
      "Epoch: 5931 mean train loss:  1.27865282e-02, mean val. rec. loss:  1.20656176e-02\n",
      "Epoch: 5932 mean train loss:  1.27853671e-02, mean val. rec. loss:  1.20645618e-02\n",
      "Epoch: 5933 mean train loss:  1.27842023e-02, mean val. rec. loss:  1.20634959e-02\n",
      "Epoch: 5934 mean train loss:  1.27830328e-02, mean val. rec. loss:  1.20624379e-02\n",
      "Epoch: 5935 mean train loss:  1.27818708e-02, mean val. rec. loss:  1.20613776e-02\n",
      "Epoch: 5936 mean train loss:  1.27807088e-02, mean val. rec. loss:  1.20603162e-02\n",
      "Epoch: 5937 mean train loss:  1.27795394e-02, mean val. rec. loss:  1.20592605e-02\n",
      "Epoch: 5938 mean train loss:  1.27783792e-02, mean val. rec. loss:  1.20582002e-02\n",
      "Epoch: 5939 mean train loss:  1.27772116e-02, mean val. rec. loss:  1.20571479e-02\n",
      "Epoch: 5940 mean train loss:  1.27760534e-02, mean val. rec. loss:  1.20560888e-02\n",
      "Epoch: 5941 mean train loss:  1.27748858e-02, mean val. rec. loss:  1.20550296e-02\n",
      "Epoch: 5942 mean train loss:  1.27737238e-02, mean val. rec. loss:  1.20539762e-02\n",
      "Epoch: 5943 mean train loss:  1.27725599e-02, mean val. rec. loss:  1.20529148e-02\n",
      "Epoch: 5944 mean train loss:  1.27713960e-02, mean val. rec. loss:  1.20518590e-02\n",
      "Epoch: 5945 mean train loss:  1.27702321e-02, mean val. rec. loss:  1.20508010e-02\n",
      "Epoch: 5946 mean train loss:  1.27690757e-02, mean val. rec. loss:  1.20497419e-02\n",
      "Epoch: 5947 mean train loss:  1.27679072e-02, mean val. rec. loss:  1.20486828e-02\n",
      "Epoch: 5948 mean train loss:  1.27667461e-02, mean val. rec. loss:  1.20476202e-02\n",
      "Epoch: 5949 mean train loss:  1.27655851e-02, mean val. rec. loss:  1.20465690e-02\n",
      "Epoch: 5950 mean train loss:  1.27644212e-02, mean val. rec. loss:  1.20455099e-02\n",
      "Epoch: 5951 mean train loss:  1.27632601e-02, mean val. rec. loss:  1.20444587e-02\n",
      "Epoch: 5952 mean train loss:  1.27621000e-02, mean val. rec. loss:  1.20433950e-02\n",
      "Epoch: 5953 mean train loss:  1.27609380e-02, mean val. rec. loss:  1.20423393e-02\n",
      "Epoch: 5954 mean train loss:  1.27597750e-02, mean val. rec. loss:  1.20412779e-02\n",
      "Epoch: 5955 mean train loss:  1.27586149e-02, mean val. rec. loss:  1.20402165e-02\n",
      "Epoch: 5956 mean train loss:  1.27574519e-02, mean val. rec. loss:  1.20391664e-02\n",
      "Epoch: 5957 mean train loss:  1.27562937e-02, mean val. rec. loss:  1.20381096e-02\n",
      "Epoch: 5958 mean train loss:  1.27551363e-02, mean val. rec. loss:  1.20370550e-02\n",
      "Epoch: 5959 mean train loss:  1.27539715e-02, mean val. rec. loss:  1.20359936e-02\n",
      "Epoch: 5960 mean train loss:  1.27528076e-02, mean val. rec. loss:  1.20349390e-02\n",
      "Epoch: 5961 mean train loss:  1.27516540e-02, mean val. rec. loss:  1.20338855e-02\n",
      "Epoch: 5962 mean train loss:  1.27504892e-02, mean val. rec. loss:  1.20328298e-02\n",
      "Epoch: 5963 mean train loss:  1.27493337e-02, mean val. rec. loss:  1.20317684e-02\n",
      "Epoch: 5964 mean train loss:  1.27481689e-02, mean val. rec. loss:  1.20307115e-02\n",
      "Epoch: 5965 mean train loss:  1.27470144e-02, mean val. rec. loss:  1.20296569e-02\n",
      "Epoch: 5966 mean train loss:  1.27458505e-02, mean val. rec. loss:  1.20285989e-02\n",
      "Epoch: 5967 mean train loss:  1.27446969e-02, mean val. rec. loss:  1.20275443e-02\n",
      "Epoch: 5968 mean train loss:  1.27435339e-02, mean val. rec. loss:  1.20264840e-02\n",
      "Epoch: 5969 mean train loss:  1.27423710e-02, mean val. rec. loss:  1.20254294e-02\n",
      "Epoch: 5970 mean train loss:  1.27412174e-02, mean val. rec. loss:  1.20243782e-02\n",
      "Epoch: 5971 mean train loss:  1.27400619e-02, mean val. rec. loss:  1.20233191e-02\n",
      "Epoch: 5972 mean train loss:  1.27389027e-02, mean val. rec. loss:  1.20222634e-02\n",
      "Epoch: 5973 mean train loss:  1.27377406e-02, mean val. rec. loss:  1.20212088e-02\n",
      "Epoch: 5974 mean train loss:  1.27365824e-02, mean val. rec. loss:  1.20201564e-02\n",
      "Epoch: 5975 mean train loss:  1.27354259e-02, mean val. rec. loss:  1.20190973e-02\n",
      "Epoch: 5976 mean train loss:  1.27342649e-02, mean val. rec. loss:  1.20180359e-02\n",
      "Epoch: 5977 mean train loss:  1.27331075e-02, mean val. rec. loss:  1.20169824e-02\n",
      "Epoch: 5978 mean train loss:  1.27319511e-02, mean val. rec. loss:  1.20159267e-02\n",
      "Epoch: 5979 mean train loss:  1.27307947e-02, mean val. rec. loss:  1.20148755e-02\n",
      "Epoch: 5980 mean train loss:  1.27296392e-02, mean val. rec. loss:  1.20138186e-02\n",
      "Epoch: 5981 mean train loss:  1.27284837e-02, mean val. rec. loss:  1.20127652e-02\n",
      "Epoch: 5982 mean train loss:  1.27273254e-02, mean val. rec. loss:  1.20117140e-02\n",
      "Epoch: 5983 mean train loss:  1.27261671e-02, mean val. rec. loss:  1.20106583e-02\n",
      "Epoch: 5984 mean train loss:  1.27250126e-02, mean val. rec. loss:  1.20096037e-02\n",
      "Epoch: 5985 mean train loss:  1.27238589e-02, mean val. rec. loss:  1.20085468e-02\n",
      "Epoch: 5986 mean train loss:  1.27226979e-02, mean val. rec. loss:  1.20074877e-02\n",
      "Epoch: 5987 mean train loss:  1.27215442e-02, mean val. rec. loss:  1.20064331e-02\n",
      "Epoch: 5988 mean train loss:  1.27203897e-02, mean val. rec. loss:  1.20053830e-02\n",
      "Epoch: 5989 mean train loss:  1.27192323e-02, mean val. rec. loss:  1.20043250e-02\n",
      "Epoch: 5990 mean train loss:  1.27180778e-02, mean val. rec. loss:  1.20032704e-02\n",
      "Epoch: 5991 mean train loss:  1.27169223e-02, mean val. rec. loss:  1.20022101e-02\n",
      "Epoch: 5992 mean train loss:  1.27157677e-02, mean val. rec. loss:  1.20011657e-02\n",
      "Epoch: 5993 mean train loss:  1.27146150e-02, mean val. rec. loss:  1.20001077e-02\n",
      "Epoch: 5994 mean train loss:  1.27134577e-02, mean val. rec. loss:  1.19990531e-02\n",
      "Epoch: 5995 mean train loss:  1.27123031e-02, mean val. rec. loss:  1.19980053e-02\n",
      "Epoch: 5996 mean train loss:  1.27111486e-02, mean val. rec. loss:  1.19969417e-02\n",
      "Epoch: 5997 mean train loss:  1.27099940e-02, mean val. rec. loss:  1.19958905e-02\n",
      "Epoch: 5998 mean train loss:  1.27088395e-02, mean val. rec. loss:  1.19948370e-02\n",
      "Epoch: 5999 mean train loss:  1.27076877e-02, mean val. rec. loss:  1.19937813e-02\n",
      "Epoch: 6000 mean train loss:  1.27065294e-02, mean val. rec. loss:  1.19927324e-02\n",
      "Epoch: 6001 mean train loss:  1.27053795e-02, mean val. rec. loss:  1.19916755e-02\n",
      "Epoch: 6002 mean train loss:  1.27042222e-02, mean val. rec. loss:  1.19906232e-02\n",
      "Epoch: 6003 mean train loss:  1.27030732e-02, mean val. rec. loss:  1.19895663e-02\n",
      "Epoch: 6004 mean train loss:  1.27019168e-02, mean val. rec. loss:  1.19885140e-02\n",
      "Epoch: 6005 mean train loss:  1.27007650e-02, mean val. rec. loss:  1.19874628e-02\n",
      "Epoch: 6006 mean train loss:  1.26996160e-02, mean val. rec. loss:  1.19864070e-02\n",
      "Epoch: 6007 mean train loss:  1.26984596e-02, mean val. rec. loss:  1.19853502e-02\n",
      "Epoch: 6008 mean train loss:  1.26973032e-02, mean val. rec. loss:  1.19843001e-02\n",
      "Epoch: 6009 mean train loss:  1.26961523e-02, mean val. rec. loss:  1.19832455e-02\n",
      "Epoch: 6010 mean train loss:  1.26950034e-02, mean val. rec. loss:  1.19821954e-02\n",
      "Epoch: 6011 mean train loss:  1.26938488e-02, mean val. rec. loss:  1.19811363e-02\n",
      "Epoch: 6012 mean train loss:  1.26926961e-02, mean val. rec. loss:  1.19800840e-02\n",
      "Epoch: 6013 mean train loss:  1.26915453e-02, mean val. rec. loss:  1.19790373e-02\n",
      "Epoch: 6014 mean train loss:  1.26903945e-02, mean val. rec. loss:  1.19779839e-02\n",
      "Epoch: 6015 mean train loss:  1.26892455e-02, mean val. rec. loss:  1.19769270e-02\n",
      "Epoch: 6016 mean train loss:  1.26880919e-02, mean val. rec. loss:  1.19758656e-02\n",
      "Epoch: 6017 mean train loss:  1.26869401e-02, mean val. rec. loss:  1.19748167e-02\n",
      "Epoch: 6018 mean train loss:  1.26857893e-02, mean val. rec. loss:  1.19737655e-02\n",
      "Epoch: 6019 mean train loss:  1.26846375e-02, mean val. rec. loss:  1.19727199e-02\n",
      "Epoch: 6020 mean train loss:  1.26834876e-02, mean val. rec. loss:  1.19716608e-02\n",
      "Epoch: 6021 mean train loss:  1.26823386e-02, mean val. rec. loss:  1.19706062e-02\n",
      "Epoch: 6022 mean train loss:  1.26811878e-02, mean val. rec. loss:  1.19695561e-02\n",
      "Epoch: 6023 mean train loss:  1.26800332e-02, mean val. rec. loss:  1.19685015e-02\n",
      "Epoch: 6024 mean train loss:  1.26788880e-02, mean val. rec. loss:  1.19674447e-02\n",
      "Epoch: 6025 mean train loss:  1.26777371e-02, mean val. rec. loss:  1.19663923e-02\n",
      "Epoch: 6026 mean train loss:  1.26765863e-02, mean val. rec. loss:  1.19653423e-02\n",
      "Epoch: 6027 mean train loss:  1.26754373e-02, mean val. rec. loss:  1.19642922e-02\n",
      "Epoch: 6028 mean train loss:  1.26742893e-02, mean val. rec. loss:  1.19632376e-02\n",
      "Epoch: 6029 mean train loss:  1.26731394e-02, mean val. rec. loss:  1.19621830e-02\n",
      "Epoch: 6030 mean train loss:  1.26719904e-02, mean val. rec. loss:  1.19611364e-02\n",
      "Epoch: 6031 mean train loss:  1.26708387e-02, mean val. rec. loss:  1.19600874e-02\n",
      "Epoch: 6032 mean train loss:  1.26696925e-02, mean val. rec. loss:  1.19590396e-02\n",
      "Epoch: 6033 mean train loss:  1.26685463e-02, mean val. rec. loss:  1.19579873e-02\n",
      "Epoch: 6034 mean train loss:  1.26673927e-02, mean val. rec. loss:  1.19569282e-02\n",
      "Epoch: 6035 mean train loss:  1.26662409e-02, mean val. rec. loss:  1.19558804e-02\n",
      "Epoch: 6036 mean train loss:  1.26650966e-02, mean val. rec. loss:  1.19548281e-02\n",
      "Epoch: 6037 mean train loss:  1.26639532e-02, mean val. rec. loss:  1.19537678e-02\n",
      "Epoch: 6038 mean train loss:  1.26627968e-02, mean val. rec. loss:  1.19527211e-02\n",
      "Epoch: 6039 mean train loss:  1.26616571e-02, mean val. rec. loss:  1.19516654e-02\n",
      "Epoch: 6040 mean train loss:  1.26605016e-02, mean val. rec. loss:  1.19506210e-02\n",
      "Epoch: 6041 mean train loss:  1.26593620e-02, mean val. rec. loss:  1.19495653e-02\n",
      "Epoch: 6042 mean train loss:  1.26582065e-02, mean val. rec. loss:  1.19485175e-02\n",
      "Epoch: 6043 mean train loss:  1.26570650e-02, mean val. rec. loss:  1.19474651e-02\n",
      "Epoch: 6044 mean train loss:  1.26559179e-02, mean val. rec. loss:  1.19464128e-02\n",
      "Epoch: 6045 mean train loss:  1.26547670e-02, mean val. rec. loss:  1.19453639e-02\n",
      "Epoch: 6046 mean train loss:  1.26536199e-02, mean val. rec. loss:  1.19443059e-02\n",
      "Epoch: 6047 mean train loss:  1.26524765e-02, mean val. rec. loss:  1.19432581e-02\n",
      "Epoch: 6048 mean train loss:  1.26513294e-02, mean val. rec. loss:  1.19422024e-02\n",
      "Epoch: 6049 mean train loss:  1.26501805e-02, mean val. rec. loss:  1.19411534e-02\n",
      "Epoch: 6050 mean train loss:  1.26490333e-02, mean val. rec. loss:  1.19401068e-02\n",
      "Epoch: 6051 mean train loss:  1.26478881e-02, mean val. rec. loss:  1.19390567e-02\n",
      "Epoch: 6052 mean train loss:  1.26467410e-02, mean val. rec. loss:  1.19380021e-02\n",
      "Epoch: 6053 mean train loss:  1.26455939e-02, mean val. rec. loss:  1.19369543e-02\n",
      "Epoch: 6054 mean train loss:  1.26444496e-02, mean val. rec. loss:  1.19358997e-02\n",
      "Epoch: 6055 mean train loss:  1.26433062e-02, mean val. rec. loss:  1.19348462e-02\n",
      "Epoch: 6056 mean train loss:  1.26421581e-02, mean val. rec. loss:  1.19337928e-02\n",
      "Epoch: 6057 mean train loss:  1.26410110e-02, mean val. rec. loss:  1.19327484e-02\n",
      "Epoch: 6058 mean train loss:  1.26398714e-02, mean val. rec. loss:  1.19317017e-02\n",
      "Epoch: 6059 mean train loss:  1.26387280e-02, mean val. rec. loss:  1.19306562e-02\n",
      "Epoch: 6060 mean train loss:  1.26375809e-02, mean val. rec. loss:  1.19295993e-02\n",
      "Epoch: 6061 mean train loss:  1.26364338e-02, mean val. rec. loss:  1.19285470e-02\n",
      "Epoch: 6062 mean train loss:  1.26352913e-02, mean val. rec. loss:  1.19275003e-02\n",
      "Epoch: 6063 mean train loss:  1.26341507e-02, mean val. rec. loss:  1.19264446e-02\n",
      "Epoch: 6064 mean train loss:  1.26330055e-02, mean val. rec. loss:  1.19253946e-02\n",
      "Epoch: 6065 mean train loss:  1.26318602e-02, mean val. rec. loss:  1.19243422e-02\n",
      "Epoch: 6066 mean train loss:  1.26307150e-02, mean val. rec. loss:  1.19232978e-02\n",
      "Epoch: 6067 mean train loss:  1.26295735e-02, mean val. rec. loss:  1.19222444e-02\n",
      "Epoch: 6068 mean train loss:  1.26284301e-02, mean val. rec. loss:  1.19211977e-02\n",
      "Epoch: 6069 mean train loss:  1.26272867e-02, mean val. rec. loss:  1.19201465e-02\n",
      "Epoch: 6070 mean train loss:  1.26261433e-02, mean val. rec. loss:  1.19190987e-02\n",
      "Epoch: 6071 mean train loss:  1.26250027e-02, mean val. rec. loss:  1.19180464e-02\n",
      "Epoch: 6072 mean train loss:  1.26238575e-02, mean val. rec. loss:  1.19169963e-02\n",
      "Epoch: 6073 mean train loss:  1.26227122e-02, mean val. rec. loss:  1.19159417e-02\n",
      "Epoch: 6074 mean train loss:  1.26215698e-02, mean val. rec. loss:  1.19148939e-02\n",
      "Epoch: 6075 mean train loss:  1.26204282e-02, mean val. rec. loss:  1.19138507e-02\n",
      "Epoch: 6076 mean train loss:  1.26192848e-02, mean val. rec. loss:  1.19128006e-02\n",
      "Epoch: 6077 mean train loss:  1.26181452e-02, mean val. rec. loss:  1.19117471e-02\n",
      "Epoch: 6078 mean train loss:  1.26169990e-02, mean val. rec. loss:  1.19106982e-02\n",
      "Epoch: 6079 mean train loss:  1.26158603e-02, mean val. rec. loss:  1.19096459e-02\n",
      "Epoch: 6080 mean train loss:  1.26147150e-02, mean val. rec. loss:  1.19086026e-02\n",
      "Epoch: 6081 mean train loss:  1.26135772e-02, mean val. rec. loss:  1.19075469e-02\n",
      "Epoch: 6082 mean train loss:  1.26124320e-02, mean val. rec. loss:  1.19065002e-02\n",
      "Epoch: 6083 mean train loss:  1.26112923e-02, mean val. rec. loss:  1.19054547e-02\n",
      "Epoch: 6084 mean train loss:  1.26101545e-02, mean val. rec. loss:  1.19044024e-02\n",
      "Epoch: 6085 mean train loss:  1.26090093e-02, mean val. rec. loss:  1.19033523e-02\n",
      "Epoch: 6086 mean train loss:  1.26078659e-02, mean val. rec. loss:  1.19023068e-02\n",
      "Epoch: 6087 mean train loss:  1.26067281e-02, mean val. rec. loss:  1.19012601e-02\n",
      "Epoch: 6088 mean train loss:  1.26055903e-02, mean val. rec. loss:  1.19002112e-02\n",
      "Epoch: 6089 mean train loss:  1.26044488e-02, mean val. rec. loss:  1.18991566e-02\n",
      "Epoch: 6090 mean train loss:  1.26033054e-02, mean val. rec. loss:  1.18981133e-02\n",
      "Epoch: 6091 mean train loss:  1.26021704e-02, mean val. rec. loss:  1.18970633e-02\n",
      "Epoch: 6092 mean train loss:  1.26010261e-02, mean val. rec. loss:  1.18960155e-02\n",
      "Epoch: 6093 mean train loss:  1.25998892e-02, mean val. rec. loss:  1.18949620e-02\n",
      "Epoch: 6094 mean train loss:  1.25987430e-02, mean val. rec. loss:  1.18939131e-02\n",
      "Epoch: 6095 mean train loss:  1.25976071e-02, mean val. rec. loss:  1.18928653e-02\n",
      "Epoch: 6096 mean train loss:  1.25964637e-02, mean val. rec. loss:  1.18918175e-02\n",
      "Epoch: 6097 mean train loss:  1.25953306e-02, mean val. rec. loss:  1.18907697e-02\n",
      "Epoch: 6098 mean train loss:  1.25941872e-02, mean val. rec. loss:  1.18897197e-02\n",
      "Epoch: 6099 mean train loss:  1.25930447e-02, mean val. rec. loss:  1.18886764e-02\n",
      "Epoch: 6100 mean train loss:  1.25919106e-02, mean val. rec. loss:  1.18876207e-02\n",
      "Epoch: 6101 mean train loss:  1.25907710e-02, mean val. rec. loss:  1.18865763e-02\n",
      "Epoch: 6102 mean train loss:  1.25896332e-02, mean val. rec. loss:  1.18855217e-02\n",
      "Epoch: 6103 mean train loss:  1.25884898e-02, mean val. rec. loss:  1.18844750e-02\n",
      "Epoch: 6104 mean train loss:  1.25873567e-02, mean val. rec. loss:  1.18834352e-02\n",
      "Epoch: 6105 mean train loss:  1.25862161e-02, mean val. rec. loss:  1.18823794e-02\n",
      "Epoch: 6106 mean train loss:  1.25850773e-02, mean val. rec. loss:  1.18813384e-02\n",
      "Epoch: 6107 mean train loss:  1.25839405e-02, mean val. rec. loss:  1.18802850e-02\n",
      "Epoch: 6108 mean train loss:  1.25828045e-02, mean val. rec. loss:  1.18792406e-02\n",
      "Epoch: 6109 mean train loss:  1.25816649e-02, mean val. rec. loss:  1.18781871e-02\n",
      "Epoch: 6110 mean train loss:  1.25805280e-02, mean val. rec. loss:  1.18771393e-02\n",
      "Epoch: 6111 mean train loss:  1.25793893e-02, mean val. rec. loss:  1.18760915e-02\n",
      "Epoch: 6112 mean train loss:  1.25782515e-02, mean val. rec. loss:  1.18750426e-02\n",
      "Epoch: 6113 mean train loss:  1.25771155e-02, mean val. rec. loss:  1.18740016e-02\n",
      "Epoch: 6114 mean train loss:  1.25759815e-02, mean val. rec. loss:  1.18729459e-02\n",
      "Epoch: 6115 mean train loss:  1.25748418e-02, mean val. rec. loss:  1.18719015e-02\n",
      "Epoch: 6116 mean train loss:  1.25737040e-02, mean val. rec. loss:  1.18708548e-02\n",
      "Epoch: 6117 mean train loss:  1.25725690e-02, mean val. rec. loss:  1.18698014e-02\n",
      "Epoch: 6118 mean train loss:  1.25714321e-02, mean val. rec. loss:  1.18687581e-02\n",
      "Epoch: 6119 mean train loss:  1.25702943e-02, mean val. rec. loss:  1.18677148e-02\n",
      "Epoch: 6120 mean train loss:  1.25691640e-02, mean val. rec. loss:  1.18666636e-02\n",
      "Epoch: 6121 mean train loss:  1.25680197e-02, mean val. rec. loss:  1.18656181e-02\n",
      "Epoch: 6122 mean train loss:  1.25668921e-02, mean val. rec. loss:  1.18645646e-02\n",
      "Epoch: 6123 mean train loss:  1.25657478e-02, mean val. rec. loss:  1.18635214e-02\n",
      "Epoch: 6124 mean train loss:  1.25646202e-02, mean val. rec. loss:  1.18624702e-02\n",
      "Epoch: 6125 mean train loss:  1.25634769e-02, mean val. rec. loss:  1.18614269e-02\n",
      "Epoch: 6126 mean train loss:  1.25623493e-02, mean val. rec. loss:  1.18603825e-02\n",
      "Epoch: 6127 mean train loss:  1.25612096e-02, mean val. rec. loss:  1.18593268e-02\n",
      "Epoch: 6128 mean train loss:  1.25600709e-02, mean val. rec. loss:  1.18582858e-02\n",
      "Epoch: 6129 mean train loss:  1.25589387e-02, mean val. rec. loss:  1.18572358e-02\n",
      "Epoch: 6130 mean train loss:  1.25578102e-02, mean val. rec. loss:  1.18561834e-02\n",
      "Epoch: 6131 mean train loss:  1.25566706e-02, mean val. rec. loss:  1.18551334e-02\n",
      "Epoch: 6132 mean train loss:  1.25555328e-02, mean val. rec. loss:  1.18540878e-02\n",
      "Epoch: 6133 mean train loss:  1.25543987e-02, mean val. rec. loss:  1.18530412e-02\n",
      "Epoch: 6134 mean train loss:  1.25532655e-02, mean val. rec. loss:  1.18519991e-02\n",
      "Epoch: 6135 mean train loss:  1.25521324e-02, mean val. rec. loss:  1.18509513e-02\n",
      "Epoch: 6136 mean train loss:  1.25509983e-02, mean val. rec. loss:  1.18499035e-02\n",
      "Epoch: 6137 mean train loss:  1.25498642e-02, mean val. rec. loss:  1.18488511e-02\n",
      "Epoch: 6138 mean train loss:  1.25487330e-02, mean val. rec. loss:  1.18478079e-02\n",
      "Epoch: 6139 mean train loss:  1.25476008e-02, mean val. rec. loss:  1.18467589e-02\n",
      "Epoch: 6140 mean train loss:  1.25464648e-02, mean val. rec. loss:  1.18457112e-02\n",
      "Epoch: 6141 mean train loss:  1.25453289e-02, mean val. rec. loss:  1.18446758e-02\n",
      "Epoch: 6142 mean train loss:  1.25441995e-02, mean val. rec. loss:  1.18436258e-02\n",
      "Epoch: 6143 mean train loss:  1.25430700e-02, mean val. rec. loss:  1.18425848e-02\n",
      "Epoch: 6144 mean train loss:  1.25419322e-02, mean val. rec. loss:  1.18415324e-02\n",
      "Epoch: 6145 mean train loss:  1.25408010e-02, mean val. rec. loss:  1.18404847e-02\n",
      "Epoch: 6146 mean train loss:  1.25396669e-02, mean val. rec. loss:  1.18394391e-02\n",
      "Epoch: 6147 mean train loss:  1.25385403e-02, mean val. rec. loss:  1.18383913e-02\n",
      "Epoch: 6148 mean train loss:  1.25373997e-02, mean val. rec. loss:  1.18373481e-02\n",
      "Epoch: 6149 mean train loss:  1.25362740e-02, mean val. rec. loss:  1.18362980e-02\n",
      "Epoch: 6150 mean train loss:  1.25351399e-02, mean val. rec. loss:  1.18352525e-02\n",
      "Epoch: 6151 mean train loss:  1.25340096e-02, mean val. rec. loss:  1.18342024e-02\n",
      "Epoch: 6152 mean train loss:  1.25328718e-02, mean val. rec. loss:  1.18331626e-02\n",
      "Epoch: 6153 mean train loss:  1.25317470e-02, mean val. rec. loss:  1.18321136e-02\n",
      "Epoch: 6154 mean train loss:  1.25306129e-02, mean val. rec. loss:  1.18310726e-02\n",
      "Epoch: 6155 mean train loss:  1.25294844e-02, mean val. rec. loss:  1.18300294e-02\n",
      "Epoch: 6156 mean train loss:  1.25283541e-02, mean val. rec. loss:  1.18289816e-02\n",
      "Epoch: 6157 mean train loss:  1.25272219e-02, mean val. rec. loss:  1.18279361e-02\n",
      "Epoch: 6158 mean train loss:  1.25260897e-02, mean val. rec. loss:  1.18268928e-02\n",
      "Epoch: 6159 mean train loss:  1.25249593e-02, mean val. rec. loss:  1.18258450e-02\n",
      "Epoch: 6160 mean train loss:  1.25238308e-02, mean val. rec. loss:  1.18247972e-02\n",
      "Epoch: 6161 mean train loss:  1.25226977e-02, mean val. rec. loss:  1.18237472e-02\n",
      "Epoch: 6162 mean train loss:  1.25215627e-02, mean val. rec. loss:  1.18227028e-02\n",
      "Epoch: 6163 mean train loss:  1.25204342e-02, mean val. rec. loss:  1.18216595e-02\n",
      "Epoch: 6164 mean train loss:  1.25193066e-02, mean val. rec. loss:  1.18206163e-02\n",
      "Epoch: 6165 mean train loss:  1.25181754e-02, mean val. rec. loss:  1.18195696e-02\n",
      "Epoch: 6166 mean train loss:  1.25170431e-02, mean val. rec. loss:  1.18185263e-02\n",
      "Epoch: 6167 mean train loss:  1.25159174e-02, mean val. rec. loss:  1.18174729e-02\n",
      "Epoch: 6168 mean train loss:  1.25147834e-02, mean val. rec. loss:  1.18164262e-02\n",
      "Epoch: 6169 mean train loss:  1.25136577e-02, mean val. rec. loss:  1.18153841e-02\n",
      "Epoch: 6170 mean train loss:  1.25125273e-02, mean val. rec. loss:  1.18143465e-02\n",
      "Epoch: 6171 mean train loss:  1.25114044e-02, mean val. rec. loss:  1.18132953e-02\n",
      "Epoch: 6172 mean train loss:  1.25102704e-02, mean val. rec. loss:  1.18122532e-02\n",
      "Epoch: 6173 mean train loss:  1.25091409e-02, mean val. rec. loss:  1.18112122e-02\n",
      "Epoch: 6174 mean train loss:  1.25080162e-02, mean val. rec. loss:  1.18101621e-02\n",
      "Epoch: 6175 mean train loss:  1.25068877e-02, mean val. rec. loss:  1.18091200e-02\n",
      "Epoch: 6176 mean train loss:  1.25057555e-02, mean val. rec. loss:  1.18080733e-02\n",
      "Epoch: 6177 mean train loss:  1.25046279e-02, mean val. rec. loss:  1.18070278e-02\n",
      "Epoch: 6178 mean train loss:  1.25035050e-02, mean val. rec. loss:  1.18059823e-02\n",
      "Epoch: 6179 mean train loss:  1.25023719e-02, mean val. rec. loss:  1.18049413e-02\n",
      "Epoch: 6180 mean train loss:  1.25012499e-02, mean val. rec. loss:  1.18038958e-02\n",
      "Epoch: 6181 mean train loss:  1.25001177e-02, mean val. rec. loss:  1.18028570e-02\n",
      "Epoch: 6182 mean train loss:  1.24989948e-02, mean val. rec. loss:  1.18018092e-02\n",
      "Epoch: 6183 mean train loss:  1.24978616e-02, mean val. rec. loss:  1.18007637e-02\n",
      "Epoch: 6184 mean train loss:  1.24967397e-02, mean val. rec. loss:  1.17997171e-02\n",
      "Epoch: 6185 mean train loss:  1.24956056e-02, mean val. rec. loss:  1.17986659e-02\n",
      "Epoch: 6186 mean train loss:  1.24944808e-02, mean val. rec. loss:  1.17976237e-02\n",
      "Epoch: 6187 mean train loss:  1.24933570e-02, mean val. rec. loss:  1.17965839e-02\n",
      "Epoch: 6188 mean train loss:  1.24922313e-02, mean val. rec. loss:  1.17955372e-02\n",
      "Epoch: 6189 mean train loss:  1.24910991e-02, mean val. rec. loss:  1.17945019e-02\n",
      "Epoch: 6190 mean train loss:  1.24899734e-02, mean val. rec. loss:  1.17934541e-02\n",
      "Epoch: 6191 mean train loss:  1.24888533e-02, mean val. rec. loss:  1.17924063e-02\n",
      "Epoch: 6192 mean train loss:  1.24877211e-02, mean val. rec. loss:  1.17913642e-02\n",
      "Epoch: 6193 mean train loss:  1.24865982e-02, mean val. rec. loss:  1.17903198e-02\n",
      "Epoch: 6194 mean train loss:  1.24854688e-02, mean val. rec. loss:  1.17892765e-02\n",
      "Epoch: 6195 mean train loss:  1.24843468e-02, mean val. rec. loss:  1.17882310e-02\n",
      "Epoch: 6196 mean train loss:  1.24832183e-02, mean val. rec. loss:  1.17871900e-02\n",
      "Epoch: 6197 mean train loss:  1.24820954e-02, mean val. rec. loss:  1.17861468e-02\n",
      "Epoch: 6198 mean train loss:  1.24809706e-02, mean val. rec. loss:  1.17851012e-02\n",
      "Epoch: 6199 mean train loss:  1.24798459e-02, mean val. rec. loss:  1.17840534e-02\n",
      "Epoch: 6200 mean train loss:  1.24787174e-02, mean val. rec. loss:  1.17830159e-02\n",
      "Epoch: 6201 mean train loss:  1.24775973e-02, mean val. rec. loss:  1.17819737e-02\n",
      "Epoch: 6202 mean train loss:  1.24764716e-02, mean val. rec. loss:  1.17809248e-02\n",
      "Epoch: 6203 mean train loss:  1.24753450e-02, mean val. rec. loss:  1.17798849e-02\n",
      "Epoch: 6204 mean train loss:  1.24742239e-02, mean val. rec. loss:  1.17788394e-02\n",
      "Epoch: 6205 mean train loss:  1.24731020e-02, mean val. rec. loss:  1.17777894e-02\n",
      "Epoch: 6206 mean train loss:  1.24719744e-02, mean val. rec. loss:  1.17767518e-02\n",
      "Epoch: 6207 mean train loss:  1.24708487e-02, mean val. rec. loss:  1.17757096e-02\n",
      "Epoch: 6208 mean train loss:  1.24697267e-02, mean val. rec. loss:  1.17746664e-02\n",
      "Epoch: 6209 mean train loss:  1.24686057e-02, mean val. rec. loss:  1.17736197e-02\n",
      "Epoch: 6210 mean train loss:  1.24674754e-02, mean val. rec. loss:  1.17725776e-02\n",
      "Epoch: 6211 mean train loss:  1.24663552e-02, mean val. rec. loss:  1.17715343e-02\n",
      "Epoch: 6212 mean train loss:  1.24652286e-02, mean val. rec. loss:  1.17704922e-02\n",
      "Epoch: 6213 mean train loss:  1.24641132e-02, mean val. rec. loss:  1.17694467e-02\n",
      "Epoch: 6214 mean train loss:  1.24629828e-02, mean val. rec. loss:  1.17684057e-02\n",
      "Epoch: 6215 mean train loss:  1.24618627e-02, mean val. rec. loss:  1.17673613e-02\n",
      "Epoch: 6216 mean train loss:  1.24607398e-02, mean val. rec. loss:  1.17663203e-02\n",
      "Epoch: 6217 mean train loss:  1.24596160e-02, mean val. rec. loss:  1.17652816e-02\n",
      "Epoch: 6218 mean train loss:  1.24584987e-02, mean val. rec. loss:  1.17642361e-02\n",
      "Epoch: 6219 mean train loss:  1.24573720e-02, mean val. rec. loss:  1.17631905e-02\n",
      "Epoch: 6220 mean train loss:  1.24562501e-02, mean val. rec. loss:  1.17621473e-02\n",
      "Epoch: 6221 mean train loss:  1.24551281e-02, mean val. rec. loss:  1.17611086e-02\n",
      "Epoch: 6222 mean train loss:  1.24540099e-02, mean val. rec. loss:  1.17600653e-02\n",
      "Epoch: 6223 mean train loss:  1.24528879e-02, mean val. rec. loss:  1.17590198e-02\n",
      "Epoch: 6224 mean train loss:  1.24517622e-02, mean val. rec. loss:  1.17579788e-02\n",
      "Epoch: 6225 mean train loss:  1.24506412e-02, mean val. rec. loss:  1.17569333e-02\n",
      "Epoch: 6226 mean train loss:  1.24495220e-02, mean val. rec. loss:  1.17559013e-02\n",
      "Epoch: 6227 mean train loss:  1.24484028e-02, mean val. rec. loss:  1.17548479e-02\n",
      "Epoch: 6228 mean train loss:  1.24472799e-02, mean val. rec. loss:  1.17538126e-02\n",
      "Epoch: 6229 mean train loss:  1.24461589e-02, mean val. rec. loss:  1.17527614e-02\n",
      "Epoch: 6230 mean train loss:  1.24450388e-02, mean val. rec. loss:  1.17517238e-02\n",
      "Epoch: 6231 mean train loss:  1.24439177e-02, mean val. rec. loss:  1.17506760e-02\n",
      "Epoch: 6232 mean train loss:  1.24427976e-02, mean val. rec. loss:  1.17496395e-02\n",
      "Epoch: 6233 mean train loss:  1.24416775e-02, mean val. rec. loss:  1.17485929e-02\n",
      "Epoch: 6234 mean train loss:  1.24405555e-02, mean val. rec. loss:  1.17475553e-02\n",
      "Epoch: 6235 mean train loss:  1.24394391e-02, mean val. rec. loss:  1.17465143e-02\n",
      "Epoch: 6236 mean train loss:  1.24383200e-02, mean val. rec. loss:  1.17454688e-02\n",
      "Epoch: 6237 mean train loss:  1.24371961e-02, mean val. rec. loss:  1.17444289e-02\n",
      "Epoch: 6238 mean train loss:  1.24360760e-02, mean val. rec. loss:  1.17433868e-02\n",
      "Epoch: 6239 mean train loss:  1.24349606e-02, mean val. rec. loss:  1.17423447e-02\n",
      "Epoch: 6240 mean train loss:  1.24338423e-02, mean val. rec. loss:  1.17413037e-02\n",
      "Epoch: 6241 mean train loss:  1.24327185e-02, mean val. rec. loss:  1.17402604e-02\n",
      "Epoch: 6242 mean train loss:  1.24316040e-02, mean val. rec. loss:  1.17392171e-02\n",
      "Epoch: 6243 mean train loss:  1.24304811e-02, mean val. rec. loss:  1.17381796e-02\n",
      "Epoch: 6244 mean train loss:  1.24293675e-02, mean val. rec. loss:  1.17371386e-02\n",
      "Epoch: 6245 mean train loss:  1.24282446e-02, mean val. rec. loss:  1.17360953e-02\n",
      "Epoch: 6246 mean train loss:  1.24271301e-02, mean val. rec. loss:  1.17350475e-02\n",
      "Epoch: 6247 mean train loss:  1.24260081e-02, mean val. rec. loss:  1.17340122e-02\n",
      "Epoch: 6248 mean train loss:  1.24248926e-02, mean val. rec. loss:  1.17329701e-02\n",
      "Epoch: 6249 mean train loss:  1.24237735e-02, mean val. rec. loss:  1.17319257e-02\n",
      "Epoch: 6250 mean train loss:  1.24226543e-02, mean val. rec. loss:  1.17308858e-02\n",
      "Epoch: 6251 mean train loss:  1.24215370e-02, mean val. rec. loss:  1.17298448e-02\n",
      "Epoch: 6252 mean train loss:  1.24204234e-02, mean val. rec. loss:  1.17288072e-02\n",
      "Epoch: 6253 mean train loss:  1.24193061e-02, mean val. rec. loss:  1.17277651e-02\n",
      "Epoch: 6254 mean train loss:  1.24181850e-02, mean val. rec. loss:  1.17267219e-02\n",
      "Epoch: 6255 mean train loss:  1.24170677e-02, mean val. rec. loss:  1.17256843e-02\n",
      "Epoch: 6256 mean train loss:  1.24159541e-02, mean val. rec. loss:  1.17246399e-02\n",
      "Epoch: 6257 mean train loss:  1.24148396e-02, mean val. rec. loss:  1.17236012e-02\n",
      "Epoch: 6258 mean train loss:  1.24137214e-02, mean val. rec. loss:  1.17225590e-02\n",
      "Epoch: 6259 mean train loss:  1.24126022e-02, mean val. rec. loss:  1.17215169e-02\n",
      "Epoch: 6260 mean train loss:  1.24114849e-02, mean val. rec. loss:  1.17204782e-02\n",
      "Epoch: 6261 mean train loss:  1.24103703e-02, mean val. rec. loss:  1.17194338e-02\n",
      "Epoch: 6262 mean train loss:  1.24092521e-02, mean val. rec. loss:  1.17183951e-02\n",
      "Epoch: 6263 mean train loss:  1.24081385e-02, mean val. rec. loss:  1.17173518e-02\n",
      "Epoch: 6264 mean train loss:  1.24070240e-02, mean val. rec. loss:  1.17163120e-02\n",
      "Epoch: 6265 mean train loss:  1.24059057e-02, mean val. rec. loss:  1.17152687e-02\n",
      "Epoch: 6266 mean train loss:  1.24047894e-02, mean val. rec. loss:  1.17142300e-02\n",
      "Epoch: 6267 mean train loss:  1.24036767e-02, mean val. rec. loss:  1.17131878e-02\n",
      "Epoch: 6268 mean train loss:  1.24025631e-02, mean val. rec. loss:  1.17121503e-02\n",
      "Epoch: 6269 mean train loss:  1.24014477e-02, mean val. rec. loss:  1.17111070e-02\n",
      "Epoch: 6270 mean train loss:  1.24003359e-02, mean val. rec. loss:  1.17100751e-02\n",
      "Epoch: 6271 mean train loss:  1.23992205e-02, mean val. rec. loss:  1.17090330e-02\n",
      "Epoch: 6272 mean train loss:  1.23981032e-02, mean val. rec. loss:  1.17079931e-02\n",
      "Epoch: 6273 mean train loss:  1.23969914e-02, mean val. rec. loss:  1.17069487e-02\n",
      "Epoch: 6274 mean train loss:  1.23958788e-02, mean val. rec. loss:  1.17059077e-02\n",
      "Epoch: 6275 mean train loss:  1.23947643e-02, mean val. rec. loss:  1.17048701e-02\n",
      "Epoch: 6276 mean train loss:  1.23936469e-02, mean val. rec. loss:  1.17038291e-02\n",
      "Epoch: 6277 mean train loss:  1.23925362e-02, mean val. rec. loss:  1.17027915e-02\n",
      "Epoch: 6278 mean train loss:  1.23914198e-02, mean val. rec. loss:  1.17017540e-02\n",
      "Epoch: 6279 mean train loss:  1.23903136e-02, mean val. rec. loss:  1.17007062e-02\n",
      "Epoch: 6280 mean train loss:  1.23891926e-02, mean val. rec. loss:  1.16996708e-02\n",
      "Epoch: 6281 mean train loss:  1.23880827e-02, mean val. rec. loss:  1.16986310e-02\n",
      "Epoch: 6282 mean train loss:  1.23869673e-02, mean val. rec. loss:  1.16975934e-02\n",
      "Epoch: 6283 mean train loss:  1.23858611e-02, mean val. rec. loss:  1.16965524e-02\n",
      "Epoch: 6284 mean train loss:  1.23847438e-02, mean val. rec. loss:  1.16955126e-02\n",
      "Epoch: 6285 mean train loss:  1.23836358e-02, mean val. rec. loss:  1.16944727e-02\n",
      "Epoch: 6286 mean train loss:  1.23825222e-02, mean val. rec. loss:  1.16934328e-02\n",
      "Epoch: 6287 mean train loss:  1.23814068e-02, mean val. rec. loss:  1.16924043e-02\n",
      "Epoch: 6288 mean train loss:  1.23802969e-02, mean val. rec. loss:  1.16913599e-02\n",
      "Epoch: 6289 mean train loss:  1.23791880e-02, mean val. rec. loss:  1.16903235e-02\n",
      "Epoch: 6290 mean train loss:  1.23780734e-02, mean val. rec. loss:  1.16892779e-02\n",
      "Epoch: 6291 mean train loss:  1.23769589e-02, mean val. rec. loss:  1.16882347e-02\n",
      "Epoch: 6292 mean train loss:  1.23758481e-02, mean val. rec. loss:  1.16871982e-02\n",
      "Epoch: 6293 mean train loss:  1.23747411e-02, mean val. rec. loss:  1.16861572e-02\n",
      "Epoch: 6294 mean train loss:  1.23736284e-02, mean val. rec. loss:  1.16851197e-02\n",
      "Epoch: 6295 mean train loss:  1.23725148e-02, mean val. rec. loss:  1.16840809e-02\n",
      "Epoch: 6296 mean train loss:  1.23714022e-02, mean val. rec. loss:  1.16830467e-02\n",
      "Epoch: 6297 mean train loss:  1.23702923e-02, mean val. rec. loss:  1.16820080e-02\n",
      "Epoch: 6298 mean train loss:  1.23691852e-02, mean val. rec. loss:  1.16809670e-02\n",
      "Epoch: 6299 mean train loss:  1.23680707e-02, mean val. rec. loss:  1.16799272e-02\n",
      "Epoch: 6300 mean train loss:  1.23669646e-02, mean val. rec. loss:  1.16788862e-02\n",
      "Epoch: 6301 mean train loss:  1.23658510e-02, mean val. rec. loss:  1.16778497e-02\n",
      "Epoch: 6302 mean train loss:  1.23647439e-02, mean val. rec. loss:  1.16768065e-02\n",
      "Epoch: 6303 mean train loss:  1.23636322e-02, mean val. rec. loss:  1.16757723e-02\n",
      "Epoch: 6304 mean train loss:  1.23625260e-02, mean val. rec. loss:  1.16747358e-02\n",
      "Epoch: 6305 mean train loss:  1.23614162e-02, mean val. rec. loss:  1.16737016e-02\n",
      "Epoch: 6306 mean train loss:  1.23603100e-02, mean val. rec. loss:  1.16726618e-02\n",
      "Epoch: 6307 mean train loss:  1.23592011e-02, mean val. rec. loss:  1.16716219e-02\n",
      "Epoch: 6308 mean train loss:  1.23580884e-02, mean val. rec. loss:  1.16705832e-02\n",
      "Epoch: 6309 mean train loss:  1.23569786e-02, mean val. rec. loss:  1.16695468e-02\n",
      "Epoch: 6310 mean train loss:  1.23558752e-02, mean val. rec. loss:  1.16685058e-02\n",
      "Epoch: 6311 mean train loss:  1.23547663e-02, mean val. rec. loss:  1.16674693e-02\n",
      "Epoch: 6312 mean train loss:  1.23536536e-02, mean val. rec. loss:  1.16664306e-02\n",
      "Epoch: 6313 mean train loss:  1.23525465e-02, mean val. rec. loss:  1.16653896e-02\n",
      "Epoch: 6314 mean train loss:  1.23514404e-02, mean val. rec. loss:  1.16643509e-02\n",
      "Epoch: 6315 mean train loss:  1.23503287e-02, mean val. rec. loss:  1.16633201e-02\n",
      "Epoch: 6316 mean train loss:  1.23492263e-02, mean val. rec. loss:  1.16622802e-02\n",
      "Epoch: 6317 mean train loss:  1.23481145e-02, mean val. rec. loss:  1.16612426e-02\n",
      "Epoch: 6318 mean train loss:  1.23470121e-02, mean val. rec. loss:  1.16602039e-02\n",
      "Epoch: 6319 mean train loss:  1.23458995e-02, mean val. rec. loss:  1.16591686e-02\n",
      "Epoch: 6320 mean train loss:  1.23447970e-02, mean val. rec. loss:  1.16581310e-02\n",
      "Epoch: 6321 mean train loss:  1.23436862e-02, mean val. rec. loss:  1.16570946e-02\n",
      "Epoch: 6322 mean train loss:  1.23425866e-02, mean val. rec. loss:  1.16560558e-02\n",
      "Epoch: 6323 mean train loss:  1.23414767e-02, mean val. rec. loss:  1.16550171e-02\n",
      "Epoch: 6324 mean train loss:  1.23403743e-02, mean val. rec. loss:  1.16539795e-02\n",
      "Epoch: 6325 mean train loss:  1.23392635e-02, mean val. rec. loss:  1.16529476e-02\n",
      "Epoch: 6326 mean train loss:  1.23381574e-02, mean val. rec. loss:  1.16519089e-02\n",
      "Epoch: 6327 mean train loss:  1.23370568e-02, mean val. rec. loss:  1.16508770e-02\n",
      "Epoch: 6328 mean train loss:  1.23359507e-02, mean val. rec. loss:  1.16498416e-02\n",
      "Epoch: 6329 mean train loss:  1.23348427e-02, mean val. rec. loss:  1.16487950e-02\n",
      "Epoch: 6330 mean train loss:  1.23337366e-02, mean val. rec. loss:  1.16477608e-02\n",
      "Epoch: 6331 mean train loss:  1.23326323e-02, mean val. rec. loss:  1.16467221e-02\n",
      "Epoch: 6332 mean train loss:  1.23315299e-02, mean val. rec. loss:  1.16456868e-02\n",
      "Epoch: 6333 mean train loss:  1.23304237e-02, mean val. rec. loss:  1.16446458e-02\n",
      "Epoch: 6334 mean train loss:  1.23293166e-02, mean val. rec. loss:  1.16436127e-02\n",
      "Epoch: 6335 mean train loss:  1.23282114e-02, mean val. rec. loss:  1.16425785e-02\n",
      "Epoch: 6336 mean train loss:  1.23271081e-02, mean val. rec. loss:  1.16415443e-02\n",
      "Epoch: 6337 mean train loss:  1.23260038e-02, mean val. rec. loss:  1.16405011e-02\n",
      "Epoch: 6338 mean train loss:  1.23248977e-02, mean val. rec. loss:  1.16394680e-02\n",
      "Epoch: 6339 mean train loss:  1.23237971e-02, mean val. rec. loss:  1.16384270e-02\n",
      "Epoch: 6340 mean train loss:  1.23226947e-02, mean val. rec. loss:  1.16373917e-02\n",
      "Epoch: 6341 mean train loss:  1.23215904e-02, mean val. rec. loss:  1.16363575e-02\n",
      "Epoch: 6342 mean train loss:  1.23204852e-02, mean val. rec. loss:  1.16353256e-02\n",
      "Epoch: 6343 mean train loss:  1.23193837e-02, mean val. rec. loss:  1.16342892e-02\n",
      "Epoch: 6344 mean train loss:  1.23182822e-02, mean val. rec. loss:  1.16332595e-02\n",
      "Epoch: 6345 mean train loss:  1.23171798e-02, mean val. rec. loss:  1.16322230e-02\n",
      "Epoch: 6346 mean train loss:  1.23160793e-02, mean val. rec. loss:  1.16311889e-02\n",
      "Epoch: 6347 mean train loss:  1.23149750e-02, mean val. rec. loss:  1.16301479e-02\n",
      "Epoch: 6348 mean train loss:  1.23138707e-02, mean val. rec. loss:  1.16291092e-02\n",
      "Epoch: 6349 mean train loss:  1.23127711e-02, mean val. rec. loss:  1.16280750e-02\n",
      "Epoch: 6350 mean train loss:  1.23116715e-02, mean val. rec. loss:  1.16270374e-02\n",
      "Epoch: 6351 mean train loss:  1.23105672e-02, mean val. rec. loss:  1.16260043e-02\n",
      "Epoch: 6352 mean train loss:  1.23094629e-02, mean val. rec. loss:  1.16249735e-02\n",
      "Epoch: 6353 mean train loss:  1.23083642e-02, mean val. rec. loss:  1.16239337e-02\n",
      "Epoch: 6354 mean train loss:  1.23072674e-02, mean val. rec. loss:  1.16228961e-02\n",
      "Epoch: 6355 mean train loss:  1.23061594e-02, mean val. rec. loss:  1.16218630e-02\n",
      "Epoch: 6356 mean train loss:  1.23050625e-02, mean val. rec. loss:  1.16208243e-02\n",
      "Epoch: 6357 mean train loss:  1.23039583e-02, mean val. rec. loss:  1.16197947e-02\n",
      "Epoch: 6358 mean train loss:  1.23028642e-02, mean val. rec. loss:  1.16187582e-02\n",
      "Epoch: 6359 mean train loss:  1.23017572e-02, mean val. rec. loss:  1.16177229e-02\n",
      "Epoch: 6360 mean train loss:  1.23006603e-02, mean val. rec. loss:  1.16166898e-02\n",
      "Epoch: 6361 mean train loss:  1.22995579e-02, mean val. rec. loss:  1.16156557e-02\n",
      "Epoch: 6362 mean train loss:  1.22984620e-02, mean val. rec. loss:  1.16146169e-02\n",
      "Epoch: 6363 mean train loss:  1.22973587e-02, mean val. rec. loss:  1.16135861e-02\n",
      "Epoch: 6364 mean train loss:  1.22962637e-02, mean val. rec. loss:  1.16125531e-02\n",
      "Epoch: 6365 mean train loss:  1.22951585e-02, mean val. rec. loss:  1.16115166e-02\n",
      "Epoch: 6366 mean train loss:  1.22940626e-02, mean val. rec. loss:  1.16104802e-02\n",
      "Epoch: 6367 mean train loss:  1.22929667e-02, mean val. rec. loss:  1.16094483e-02\n",
      "Epoch: 6368 mean train loss:  1.22918652e-02, mean val. rec. loss:  1.16084163e-02\n",
      "Epoch: 6369 mean train loss:  1.22907646e-02, mean val. rec. loss:  1.16073799e-02\n",
      "Epoch: 6370 mean train loss:  1.22896687e-02, mean val. rec. loss:  1.16063457e-02\n",
      "Epoch: 6371 mean train loss:  1.22885728e-02, mean val. rec. loss:  1.16053138e-02\n",
      "Epoch: 6372 mean train loss:  1.22874723e-02, mean val. rec. loss:  1.16042705e-02\n",
      "Epoch: 6373 mean train loss:  1.22863699e-02, mean val. rec. loss:  1.16032454e-02\n",
      "Epoch: 6374 mean train loss:  1.22852768e-02, mean val. rec. loss:  1.16022101e-02\n",
      "Epoch: 6375 mean train loss:  1.22841827e-02, mean val. rec. loss:  1.16011725e-02\n",
      "Epoch: 6376 mean train loss:  1.22830812e-02, mean val. rec. loss:  1.16001406e-02\n",
      "Epoch: 6377 mean train loss:  1.22819835e-02, mean val. rec. loss:  1.15991087e-02\n",
      "Epoch: 6378 mean train loss:  1.22808876e-02, mean val. rec. loss:  1.15980722e-02\n",
      "Epoch: 6379 mean train loss:  1.22797908e-02, mean val. rec. loss:  1.15970392e-02\n",
      "Epoch: 6380 mean train loss:  1.22786949e-02, mean val. rec. loss:  1.15960038e-02\n",
      "Epoch: 6381 mean train loss:  1.22775971e-02, mean val. rec. loss:  1.15949719e-02\n",
      "Epoch: 6382 mean train loss:  1.22765021e-02, mean val. rec. loss:  1.15939400e-02\n",
      "Epoch: 6383 mean train loss:  1.22754053e-02, mean val. rec. loss:  1.15929058e-02\n",
      "Epoch: 6384 mean train loss:  1.22743085e-02, mean val. rec. loss:  1.15918739e-02\n",
      "Epoch: 6385 mean train loss:  1.22732135e-02, mean val. rec. loss:  1.15908397e-02\n",
      "Epoch: 6386 mean train loss:  1.22721176e-02, mean val. rec. loss:  1.15898067e-02\n",
      "Epoch: 6387 mean train loss:  1.22710236e-02, mean val. rec. loss:  1.15887713e-02\n",
      "Epoch: 6388 mean train loss:  1.22699286e-02, mean val. rec. loss:  1.15877349e-02\n",
      "Epoch: 6389 mean train loss:  1.22688336e-02, mean val. rec. loss:  1.15867030e-02\n",
      "Epoch: 6390 mean train loss:  1.22677387e-02, mean val. rec. loss:  1.15856733e-02\n",
      "Epoch: 6391 mean train loss:  1.22666418e-02, mean val. rec. loss:  1.15846425e-02\n",
      "Epoch: 6392 mean train loss:  1.22655487e-02, mean val. rec. loss:  1.15836083e-02\n",
      "Epoch: 6393 mean train loss:  1.22644556e-02, mean val. rec. loss:  1.15825810e-02\n",
      "Epoch: 6394 mean train loss:  1.22633607e-02, mean val. rec. loss:  1.15815456e-02\n",
      "Epoch: 6395 mean train loss:  1.22622657e-02, mean val. rec. loss:  1.15805114e-02\n",
      "Epoch: 6396 mean train loss:  1.22611726e-02, mean val. rec. loss:  1.15794807e-02\n",
      "Epoch: 6397 mean train loss:  1.22600813e-02, mean val. rec. loss:  1.15784453e-02\n",
      "Epoch: 6398 mean train loss:  1.22589864e-02, mean val. rec. loss:  1.15774066e-02\n",
      "Epoch: 6399 mean train loss:  1.22578905e-02, mean val. rec. loss:  1.15763815e-02\n",
      "Epoch: 6400 mean train loss:  1.22568020e-02, mean val. rec. loss:  1.15753519e-02\n",
      "Epoch: 6401 mean train loss:  1.22557043e-02, mean val. rec. loss:  1.15743211e-02\n",
      "Epoch: 6402 mean train loss:  1.22546158e-02, mean val. rec. loss:  1.15732835e-02\n",
      "Epoch: 6403 mean train loss:  1.22535190e-02, mean val. rec. loss:  1.15722584e-02\n",
      "Epoch: 6404 mean train loss:  1.22524315e-02, mean val. rec. loss:  1.15712264e-02\n",
      "Epoch: 6405 mean train loss:  1.22513346e-02, mean val. rec. loss:  1.15701923e-02\n",
      "Epoch: 6406 mean train loss:  1.22502471e-02, mean val. rec. loss:  1.15691592e-02\n",
      "Epoch: 6407 mean train loss:  1.22491531e-02, mean val. rec. loss:  1.15681273e-02\n",
      "Epoch: 6408 mean train loss:  1.22480646e-02, mean val. rec. loss:  1.15670920e-02\n",
      "Epoch: 6409 mean train loss:  1.22469678e-02, mean val. rec. loss:  1.15660691e-02\n",
      "Epoch: 6410 mean train loss:  1.22458831e-02, mean val. rec. loss:  1.15650372e-02\n",
      "Epoch: 6411 mean train loss:  1.22447909e-02, mean val. rec. loss:  1.15640053e-02\n",
      "Epoch: 6412 mean train loss:  1.22436959e-02, mean val. rec. loss:  1.15629677e-02\n",
      "Epoch: 6413 mean train loss:  1.22426065e-02, mean val. rec. loss:  1.15619437e-02\n",
      "Epoch: 6414 mean train loss:  1.22415199e-02, mean val. rec. loss:  1.15609141e-02\n",
      "Epoch: 6415 mean train loss:  1.22404296e-02, mean val. rec. loss:  1.15598810e-02\n",
      "Epoch: 6416 mean train loss:  1.22393375e-02, mean val. rec. loss:  1.15588570e-02\n",
      "Epoch: 6417 mean train loss:  1.22382490e-02, mean val. rec. loss:  1.15578217e-02\n",
      "Epoch: 6418 mean train loss:  1.22371615e-02, mean val. rec. loss:  1.15567909e-02\n",
      "Epoch: 6419 mean train loss:  1.22360712e-02, mean val. rec. loss:  1.15557590e-02\n",
      "Epoch: 6420 mean train loss:  1.22349781e-02, mean val. rec. loss:  1.15547271e-02\n",
      "Epoch: 6421 mean train loss:  1.22338924e-02, mean val. rec. loss:  1.15536974e-02\n",
      "Epoch: 6422 mean train loss:  1.22328058e-02, mean val. rec. loss:  1.15526678e-02\n",
      "Epoch: 6423 mean train loss:  1.22317146e-02, mean val. rec. loss:  1.15516313e-02\n",
      "Epoch: 6424 mean train loss:  1.22306233e-02, mean val. rec. loss:  1.15506040e-02\n",
      "Epoch: 6425 mean train loss:  1.22295358e-02, mean val. rec. loss:  1.15495754e-02\n",
      "Epoch: 6426 mean train loss:  1.22284455e-02, mean val. rec. loss:  1.15485424e-02\n",
      "Epoch: 6427 mean train loss:  1.22273561e-02, mean val. rec. loss:  1.15475173e-02\n",
      "Epoch: 6428 mean train loss:  1.22262695e-02, mean val. rec. loss:  1.15464876e-02\n",
      "Epoch: 6429 mean train loss:  1.22251848e-02, mean val. rec. loss:  1.15454580e-02\n",
      "Epoch: 6430 mean train loss:  1.22240945e-02, mean val. rec. loss:  1.15444249e-02\n",
      "Epoch: 6431 mean train loss:  1.22230051e-02, mean val. rec. loss:  1.15433941e-02\n",
      "Epoch: 6432 mean train loss:  1.22219213e-02, mean val. rec. loss:  1.15423667e-02\n",
      "Epoch: 6433 mean train loss:  1.22208357e-02, mean val. rec. loss:  1.15413371e-02\n",
      "Epoch: 6434 mean train loss:  1.22197472e-02, mean val. rec. loss:  1.15403097e-02\n",
      "Epoch: 6435 mean train loss:  1.22186606e-02, mean val. rec. loss:  1.15392778e-02\n",
      "Epoch: 6436 mean train loss:  1.22175759e-02, mean val. rec. loss:  1.15382459e-02\n",
      "Epoch: 6437 mean train loss:  1.22164912e-02, mean val. rec. loss:  1.15372242e-02\n",
      "Epoch: 6438 mean train loss:  1.22154083e-02, mean val. rec. loss:  1.15361934e-02\n",
      "Epoch: 6439 mean train loss:  1.22143199e-02, mean val. rec. loss:  1.15351615e-02\n",
      "Epoch: 6440 mean train loss:  1.22132314e-02, mean val. rec. loss:  1.15341363e-02\n",
      "Epoch: 6441 mean train loss:  1.22121504e-02, mean val. rec. loss:  1.15331090e-02\n",
      "Epoch: 6442 mean train loss:  1.22110666e-02, mean val. rec. loss:  1.15320838e-02\n",
      "Epoch: 6443 mean train loss:  1.22099791e-02, mean val. rec. loss:  1.15310451e-02\n",
      "Epoch: 6444 mean train loss:  1.22088916e-02, mean val. rec. loss:  1.15300200e-02\n",
      "Epoch: 6445 mean train loss:  1.22078115e-02, mean val. rec. loss:  1.15289926e-02\n",
      "Epoch: 6446 mean train loss:  1.22067305e-02, mean val. rec. loss:  1.15279539e-02\n",
      "Epoch: 6447 mean train loss:  1.22056421e-02, mean val. rec. loss:  1.15269288e-02\n",
      "Epoch: 6448 mean train loss:  1.22045545e-02, mean val. rec. loss:  1.15259037e-02\n",
      "Epoch: 6449 mean train loss:  1.22034754e-02, mean val. rec. loss:  1.15248786e-02\n",
      "Epoch: 6450 mean train loss:  1.22023944e-02, mean val. rec. loss:  1.15238500e-02\n",
      "Epoch: 6451 mean train loss:  1.22013078e-02, mean val. rec. loss:  1.15228170e-02\n",
      "Epoch: 6452 mean train loss:  1.22002240e-02, mean val. rec. loss:  1.15217941e-02\n",
      "Epoch: 6453 mean train loss:  1.21991430e-02, mean val. rec. loss:  1.15207702e-02\n",
      "Epoch: 6454 mean train loss:  1.21980620e-02, mean val. rec. loss:  1.15197382e-02\n",
      "Epoch: 6455 mean train loss:  1.21969801e-02, mean val. rec. loss:  1.15187075e-02\n",
      "Epoch: 6456 mean train loss:  1.21958972e-02, mean val. rec. loss:  1.15176846e-02\n",
      "Epoch: 6457 mean train loss:  1.21948153e-02, mean val. rec. loss:  1.15166572e-02\n",
      "Epoch: 6458 mean train loss:  1.21937333e-02, mean val. rec. loss:  1.15156287e-02\n",
      "Epoch: 6459 mean train loss:  1.21926514e-02, mean val. rec. loss:  1.15145968e-02\n",
      "Epoch: 6460 mean train loss:  1.21915713e-02, mean val. rec. loss:  1.15135728e-02\n",
      "Epoch: 6461 mean train loss:  1.21904903e-02, mean val. rec. loss:  1.15125477e-02\n",
      "Epoch: 6462 mean train loss:  1.21894093e-02, mean val. rec. loss:  1.15115214e-02\n",
      "Epoch: 6463 mean train loss:  1.21883293e-02, mean val. rec. loss:  1.15104952e-02\n",
      "Epoch: 6464 mean train loss:  1.21872473e-02, mean val. rec. loss:  1.15094689e-02\n",
      "Epoch: 6465 mean train loss:  1.21861654e-02, mean val. rec. loss:  1.15084416e-02\n",
      "Epoch: 6466 mean train loss:  1.21850853e-02, mean val. rec. loss:  1.15074153e-02\n",
      "Epoch: 6467 mean train loss:  1.21840053e-02, mean val. rec. loss:  1.15063868e-02\n",
      "Epoch: 6468 mean train loss:  1.21829252e-02, mean val. rec. loss:  1.15053628e-02\n",
      "Epoch: 6469 mean train loss:  1.21818470e-02, mean val. rec. loss:  1.15043377e-02\n",
      "Epoch: 6470 mean train loss:  1.21807669e-02, mean val. rec. loss:  1.15033058e-02\n",
      "Epoch: 6471 mean train loss:  1.21796878e-02, mean val. rec. loss:  1.15022886e-02\n",
      "Epoch: 6472 mean train loss:  1.21786096e-02, mean val. rec. loss:  1.15012578e-02\n",
      "Epoch: 6473 mean train loss:  1.21775295e-02, mean val. rec. loss:  1.15002338e-02\n",
      "Epoch: 6474 mean train loss:  1.21764522e-02, mean val. rec. loss:  1.14992099e-02\n",
      "Epoch: 6475 mean train loss:  1.21753749e-02, mean val. rec. loss:  1.14981779e-02\n",
      "Epoch: 6476 mean train loss:  1.21742949e-02, mean val. rec. loss:  1.14971415e-02\n",
      "Epoch: 6477 mean train loss:  1.21732148e-02, mean val. rec. loss:  1.14961186e-02\n",
      "Epoch: 6478 mean train loss:  1.21721375e-02, mean val. rec. loss:  1.14950981e-02\n",
      "Epoch: 6479 mean train loss:  1.21710593e-02, mean val. rec. loss:  1.14940729e-02\n",
      "Epoch: 6480 mean train loss:  1.21699820e-02, mean val. rec. loss:  1.14930456e-02\n",
      "Epoch: 6481 mean train loss:  1.21689048e-02, mean val. rec. loss:  1.14920239e-02\n",
      "Epoch: 6482 mean train loss:  1.21678284e-02, mean val. rec. loss:  1.14910044e-02\n",
      "Epoch: 6483 mean train loss:  1.21667521e-02, mean val. rec. loss:  1.14899725e-02\n",
      "Epoch: 6484 mean train loss:  1.21656757e-02, mean val. rec. loss:  1.14889428e-02\n",
      "Epoch: 6485 mean train loss:  1.21645994e-02, mean val. rec. loss:  1.14879211e-02\n",
      "Epoch: 6486 mean train loss:  1.21635230e-02, mean val. rec. loss:  1.14868949e-02\n",
      "Epoch: 6487 mean train loss:  1.21624495e-02, mean val. rec. loss:  1.14858743e-02\n",
      "Epoch: 6488 mean train loss:  1.21613713e-02, mean val. rec. loss:  1.14848446e-02\n",
      "Epoch: 6489 mean train loss:  1.21602940e-02, mean val. rec. loss:  1.14838207e-02\n",
      "Epoch: 6490 mean train loss:  1.21592214e-02, mean val. rec. loss:  1.14827967e-02\n",
      "Epoch: 6491 mean train loss:  1.21581487e-02, mean val. rec. loss:  1.14817727e-02\n",
      "Epoch: 6492 mean train loss:  1.21570743e-02, mean val. rec. loss:  1.14807510e-02\n",
      "Epoch: 6493 mean train loss:  1.21559998e-02, mean val. rec. loss:  1.14797259e-02\n",
      "Epoch: 6494 mean train loss:  1.21549253e-02, mean val. rec. loss:  1.14787030e-02\n",
      "Epoch: 6495 mean train loss:  1.21538499e-02, mean val. rec. loss:  1.14776825e-02\n",
      "Epoch: 6496 mean train loss:  1.21527773e-02, mean val. rec. loss:  1.14766607e-02\n",
      "Epoch: 6497 mean train loss:  1.21517065e-02, mean val. rec. loss:  1.14756402e-02\n",
      "Epoch: 6498 mean train loss:  1.21506302e-02, mean val. rec. loss:  1.14746094e-02\n",
      "Epoch: 6499 mean train loss:  1.21495547e-02, mean val. rec. loss:  1.14735911e-02\n",
      "Epoch: 6500 mean train loss:  1.21484840e-02, mean val. rec. loss:  1.14725671e-02\n",
      "Epoch: 6501 mean train loss:  1.21474132e-02, mean val. rec. loss:  1.14715431e-02\n",
      "Epoch: 6502 mean train loss:  1.21463397e-02, mean val. rec. loss:  1.14705157e-02\n",
      "Epoch: 6503 mean train loss:  1.21452643e-02, mean val. rec. loss:  1.14694929e-02\n",
      "Epoch: 6504 mean train loss:  1.21441944e-02, mean val. rec. loss:  1.14684678e-02\n",
      "Epoch: 6505 mean train loss:  1.21431246e-02, mean val. rec. loss:  1.14674472e-02\n",
      "Epoch: 6506 mean train loss:  1.21420501e-02, mean val. rec. loss:  1.14664232e-02\n",
      "Epoch: 6507 mean train loss:  1.21409784e-02, mean val. rec. loss:  1.14653992e-02\n",
      "Epoch: 6508 mean train loss:  1.21399105e-02, mean val. rec. loss:  1.14643764e-02\n",
      "Epoch: 6509 mean train loss:  1.21388369e-02, mean val. rec. loss:  1.14633581e-02\n",
      "Epoch: 6510 mean train loss:  1.21377643e-02, mean val. rec. loss:  1.14623307e-02\n",
      "Epoch: 6511 mean train loss:  1.21366954e-02, mean val. rec. loss:  1.14613056e-02\n",
      "Epoch: 6512 mean train loss:  1.21356274e-02, mean val. rec. loss:  1.14602861e-02\n",
      "Epoch: 6513 mean train loss:  1.21345548e-02, mean val. rec. loss:  1.14592655e-02\n",
      "Epoch: 6514 mean train loss:  1.21334831e-02, mean val. rec. loss:  1.14582438e-02\n",
      "Epoch: 6515 mean train loss:  1.21324151e-02, mean val. rec. loss:  1.14572164e-02\n",
      "Epoch: 6516 mean train loss:  1.21313481e-02, mean val. rec. loss:  1.14562004e-02\n",
      "Epoch: 6517 mean train loss:  1.21302764e-02, mean val. rec. loss:  1.14551798e-02\n",
      "Epoch: 6518 mean train loss:  1.21292047e-02, mean val. rec. loss:  1.14541524e-02\n",
      "Epoch: 6519 mean train loss:  1.21281377e-02, mean val. rec. loss:  1.14531387e-02\n",
      "Epoch: 6520 mean train loss:  1.21270716e-02, mean val. rec. loss:  1.14521113e-02\n",
      "Epoch: 6521 mean train loss:  1.21259999e-02, mean val. rec. loss:  1.14510918e-02\n",
      "Epoch: 6522 mean train loss:  1.21249347e-02, mean val. rec. loss:  1.14500610e-02\n",
      "Epoch: 6523 mean train loss:  1.21238658e-02, mean val. rec. loss:  1.14490461e-02\n",
      "Epoch: 6524 mean train loss:  1.21227988e-02, mean val. rec. loss:  1.14480244e-02\n",
      "Epoch: 6525 mean train loss:  1.21217262e-02, mean val. rec. loss:  1.14470038e-02\n",
      "Epoch: 6526 mean train loss:  1.21206647e-02, mean val. rec. loss:  1.14459878e-02\n",
      "Epoch: 6527 mean train loss:  1.21195967e-02, mean val. rec. loss:  1.14449695e-02\n",
      "Epoch: 6528 mean train loss:  1.21185316e-02, mean val. rec. loss:  1.14439455e-02\n",
      "Epoch: 6529 mean train loss:  1.21174580e-02, mean val. rec. loss:  1.14429283e-02\n",
      "Epoch: 6530 mean train loss:  1.21163984e-02, mean val. rec. loss:  1.14419021e-02\n",
      "Epoch: 6531 mean train loss:  1.21153295e-02, mean val. rec. loss:  1.14408826e-02\n",
      "Epoch: 6532 mean train loss:  1.21142644e-02, mean val. rec. loss:  1.14398643e-02\n",
      "Epoch: 6533 mean train loss:  1.21131973e-02, mean val. rec. loss:  1.14388415e-02\n",
      "Epoch: 6534 mean train loss:  1.21121331e-02, mean val. rec. loss:  1.14378243e-02\n",
      "Epoch: 6535 mean train loss:  1.21110679e-02, mean val. rec. loss:  1.14368071e-02\n",
      "Epoch: 6536 mean train loss:  1.21100009e-02, mean val. rec. loss:  1.14357843e-02\n",
      "Epoch: 6537 mean train loss:  1.21089366e-02, mean val. rec. loss:  1.14347682e-02\n",
      "Epoch: 6538 mean train loss:  1.21078733e-02, mean val. rec. loss:  1.14337409e-02\n",
      "Epoch: 6539 mean train loss:  1.21068091e-02, mean val. rec. loss:  1.14327203e-02\n",
      "Epoch: 6540 mean train loss:  1.21057439e-02, mean val. rec. loss:  1.14317054e-02\n",
      "Epoch: 6541 mean train loss:  1.21046825e-02, mean val. rec. loss:  1.14306859e-02\n",
      "Epoch: 6542 mean train loss:  1.21036164e-02, mean val. rec. loss:  1.14296710e-02\n",
      "Epoch: 6543 mean train loss:  1.21025540e-02, mean val. rec. loss:  1.14286470e-02\n",
      "Epoch: 6544 mean train loss:  1.21014907e-02, mean val. rec. loss:  1.14276310e-02\n",
      "Epoch: 6545 mean train loss:  1.21004264e-02, mean val. rec. loss:  1.14266081e-02\n",
      "Epoch: 6546 mean train loss:  1.20993650e-02, mean val. rec. loss:  1.14255921e-02\n",
      "Epoch: 6547 mean train loss:  1.20983017e-02, mean val. rec. loss:  1.14245670e-02\n",
      "Epoch: 6548 mean train loss:  1.20972393e-02, mean val. rec. loss:  1.14235555e-02\n",
      "Epoch: 6549 mean train loss:  1.20961769e-02, mean val. rec. loss:  1.14225304e-02\n",
      "Epoch: 6550 mean train loss:  1.20951155e-02, mean val. rec. loss:  1.14215189e-02\n",
      "Epoch: 6551 mean train loss:  1.20940531e-02, mean val. rec. loss:  1.14204960e-02\n",
      "Epoch: 6552 mean train loss:  1.20929926e-02, mean val. rec. loss:  1.14194845e-02\n",
      "Epoch: 6553 mean train loss:  1.20919311e-02, mean val. rec. loss:  1.14184594e-02\n",
      "Epoch: 6554 mean train loss:  1.20908697e-02, mean val. rec. loss:  1.14174445e-02\n",
      "Epoch: 6555 mean train loss:  1.20898092e-02, mean val. rec. loss:  1.14164250e-02\n",
      "Epoch: 6556 mean train loss:  1.20887505e-02, mean val. rec. loss:  1.14154022e-02\n",
      "Epoch: 6557 mean train loss:  1.20876872e-02, mean val. rec. loss:  1.14143850e-02\n",
      "Epoch: 6558 mean train loss:  1.20866295e-02, mean val. rec. loss:  1.14133701e-02\n",
      "Epoch: 6559 mean train loss:  1.20855689e-02, mean val. rec. loss:  1.14123552e-02\n",
      "Epoch: 6560 mean train loss:  1.20845084e-02, mean val. rec. loss:  1.14113369e-02\n",
      "Epoch: 6561 mean train loss:  1.20834488e-02, mean val. rec. loss:  1.14103174e-02\n",
      "Epoch: 6562 mean train loss:  1.20823911e-02, mean val. rec. loss:  1.14093025e-02\n",
      "Epoch: 6563 mean train loss:  1.20813325e-02, mean val. rec. loss:  1.14082842e-02\n",
      "Epoch: 6564 mean train loss:  1.20802738e-02, mean val. rec. loss:  1.14072693e-02\n",
      "Epoch: 6565 mean train loss:  1.20792142e-02, mean val. rec. loss:  1.14062510e-02\n",
      "Epoch: 6566 mean train loss:  1.20781565e-02, mean val. rec. loss:  1.14052384e-02\n",
      "Epoch: 6567 mean train loss:  1.20770988e-02, mean val. rec. loss:  1.14042212e-02\n",
      "Epoch: 6568 mean train loss:  1.20760420e-02, mean val. rec. loss:  1.14032097e-02\n",
      "Epoch: 6569 mean train loss:  1.20749833e-02, mean val. rec. loss:  1.14021868e-02\n",
      "Epoch: 6570 mean train loss:  1.20739293e-02, mean val. rec. loss:  1.14011719e-02\n",
      "Epoch: 6571 mean train loss:  1.20728697e-02, mean val. rec. loss:  1.14001547e-02\n",
      "Epoch: 6572 mean train loss:  1.20718101e-02, mean val. rec. loss:  1.13991364e-02\n",
      "Epoch: 6573 mean train loss:  1.20707561e-02, mean val. rec. loss:  1.13981170e-02\n",
      "Epoch: 6574 mean train loss:  1.20697021e-02, mean val. rec. loss:  1.13971055e-02\n",
      "Epoch: 6575 mean train loss:  1.20686435e-02, mean val. rec. loss:  1.13960883e-02\n",
      "Epoch: 6576 mean train loss:  1.20675848e-02, mean val. rec. loss:  1.13950700e-02\n",
      "Epoch: 6577 mean train loss:  1.20665327e-02, mean val. rec. loss:  1.13940539e-02\n",
      "Epoch: 6578 mean train loss:  1.20654778e-02, mean val. rec. loss:  1.13930424e-02\n",
      "Epoch: 6579 mean train loss:  1.20644228e-02, mean val. rec. loss:  1.13920343e-02\n",
      "Epoch: 6580 mean train loss:  1.20633670e-02, mean val. rec. loss:  1.13910104e-02\n",
      "Epoch: 6581 mean train loss:  1.20623111e-02, mean val. rec. loss:  1.13899943e-02\n",
      "Epoch: 6582 mean train loss:  1.20612580e-02, mean val. rec. loss:  1.13889817e-02\n",
      "Epoch: 6583 mean train loss:  1.20602059e-02, mean val. rec. loss:  1.13879668e-02\n",
      "Epoch: 6584 mean train loss:  1.20591510e-02, mean val. rec. loss:  1.13869484e-02\n",
      "Epoch: 6585 mean train loss:  1.20580951e-02, mean val. rec. loss:  1.13859369e-02\n",
      "Epoch: 6586 mean train loss:  1.20570430e-02, mean val. rec. loss:  1.13849254e-02\n",
      "Epoch: 6587 mean train loss:  1.20559918e-02, mean val. rec. loss:  1.13839139e-02\n",
      "Epoch: 6588 mean train loss:  1.20549368e-02, mean val. rec. loss:  1.13828888e-02\n",
      "Epoch: 6589 mean train loss:  1.20538828e-02, mean val. rec. loss:  1.13818750e-02\n",
      "Epoch: 6590 mean train loss:  1.20528307e-02, mean val. rec. loss:  1.13808635e-02\n",
      "Epoch: 6591 mean train loss:  1.20517804e-02, mean val. rec. loss:  1.13798498e-02\n",
      "Epoch: 6592 mean train loss:  1.20507283e-02, mean val. rec. loss:  1.13788348e-02\n",
      "Epoch: 6593 mean train loss:  1.20496743e-02, mean val. rec. loss:  1.13778245e-02\n",
      "Epoch: 6594 mean train loss:  1.20486240e-02, mean val. rec. loss:  1.13768107e-02\n",
      "Epoch: 6595 mean train loss:  1.20475728e-02, mean val. rec. loss:  1.13757981e-02\n",
      "Epoch: 6596 mean train loss:  1.20465216e-02, mean val. rec. loss:  1.13747831e-02\n",
      "Epoch: 6597 mean train loss:  1.20454732e-02, mean val. rec. loss:  1.13737694e-02\n",
      "Epoch: 6598 mean train loss:  1.20444211e-02, mean val. rec. loss:  1.13727545e-02\n",
      "Epoch: 6599 mean train loss:  1.20433680e-02, mean val. rec. loss:  1.13717418e-02\n",
      "Epoch: 6600 mean train loss:  1.20423205e-02, mean val. rec. loss:  1.13707371e-02\n",
      "Epoch: 6601 mean train loss:  1.20412739e-02, mean val. rec. loss:  1.13697211e-02\n",
      "Epoch: 6602 mean train loss:  1.20402218e-02, mean val. rec. loss:  1.13687084e-02\n",
      "Epoch: 6603 mean train loss:  1.20391697e-02, mean val. rec. loss:  1.13677015e-02\n",
      "Epoch: 6604 mean train loss:  1.20381241e-02, mean val. rec. loss:  1.13666877e-02\n",
      "Epoch: 6605 mean train loss:  1.20370766e-02, mean val. rec. loss:  1.13656739e-02\n",
      "Epoch: 6606 mean train loss:  1.20360263e-02, mean val. rec. loss:  1.13646635e-02\n",
      "Epoch: 6607 mean train loss:  1.20349751e-02, mean val. rec. loss:  1.13636543e-02\n",
      "Epoch: 6608 mean train loss:  1.20339295e-02, mean val. rec. loss:  1.13626417e-02\n",
      "Epoch: 6609 mean train loss:  1.20328811e-02, mean val. rec. loss:  1.13616245e-02\n",
      "Epoch: 6610 mean train loss:  1.20318336e-02, mean val. rec. loss:  1.13606107e-02\n",
      "Epoch: 6611 mean train loss:  1.20307861e-02, mean val. rec. loss:  1.13596037e-02\n",
      "Epoch: 6612 mean train loss:  1.20297395e-02, mean val. rec. loss:  1.13585877e-02\n",
      "Epoch: 6613 mean train loss:  1.20286930e-02, mean val. rec. loss:  1.13575807e-02\n",
      "Epoch: 6614 mean train loss:  1.20276464e-02, mean val. rec. loss:  1.13565624e-02\n",
      "Epoch: 6615 mean train loss:  1.20265980e-02, mean val. rec. loss:  1.13555532e-02\n",
      "Epoch: 6616 mean train loss:  1.20255533e-02, mean val. rec. loss:  1.13545371e-02\n",
      "Epoch: 6617 mean train loss:  1.20245068e-02, mean val. rec. loss:  1.13535256e-02\n",
      "Epoch: 6618 mean train loss:  1.20234621e-02, mean val. rec. loss:  1.13525164e-02\n",
      "Epoch: 6619 mean train loss:  1.20224155e-02, mean val. rec. loss:  1.13515094e-02\n",
      "Epoch: 6620 mean train loss:  1.20213718e-02, mean val. rec. loss:  1.13504979e-02\n",
      "Epoch: 6621 mean train loss:  1.20203252e-02, mean val. rec. loss:  1.13494898e-02\n",
      "Epoch: 6622 mean train loss:  1.20192815e-02, mean val. rec. loss:  1.13484738e-02\n",
      "Epoch: 6623 mean train loss:  1.20182377e-02, mean val. rec. loss:  1.13474679e-02\n",
      "Epoch: 6624 mean train loss:  1.20171921e-02, mean val. rec. loss:  1.13464587e-02\n",
      "Epoch: 6625 mean train loss:  1.20161483e-02, mean val. rec. loss:  1.13454494e-02\n",
      "Epoch: 6626 mean train loss:  1.20151055e-02, mean val. rec. loss:  1.13444311e-02\n",
      "Epoch: 6627 mean train loss:  1.20140618e-02, mean val. rec. loss:  1.13434332e-02\n",
      "Epoch: 6628 mean train loss:  1.20130189e-02, mean val. rec. loss:  1.13424195e-02\n",
      "Epoch: 6629 mean train loss:  1.20119761e-02, mean val. rec. loss:  1.13414125e-02\n",
      "Epoch: 6630 mean train loss:  1.20109342e-02, mean val. rec. loss:  1.13403964e-02\n",
      "Epoch: 6631 mean train loss:  1.20098905e-02, mean val. rec. loss:  1.13393963e-02\n",
      "Epoch: 6632 mean train loss:  1.20088495e-02, mean val. rec. loss:  1.13383825e-02\n",
      "Epoch: 6633 mean train loss:  1.20078067e-02, mean val. rec. loss:  1.13373801e-02\n",
      "Epoch: 6634 mean train loss:  1.20067657e-02, mean val. rec. loss:  1.13363652e-02\n",
      "Epoch: 6635 mean train loss:  1.20057247e-02, mean val. rec. loss:  1.13353639e-02\n",
      "Epoch: 6636 mean train loss:  1.20046838e-02, mean val. rec. loss:  1.13343478e-02\n",
      "Epoch: 6637 mean train loss:  1.20036437e-02, mean val. rec. loss:  1.13333386e-02\n",
      "Epoch: 6638 mean train loss:  1.20026009e-02, mean val. rec. loss:  1.13323248e-02\n",
      "Epoch: 6639 mean train loss:  1.20015590e-02, mean val. rec. loss:  1.13313224e-02\n",
      "Epoch: 6640 mean train loss:  1.20005199e-02, mean val. rec. loss:  1.13303097e-02\n",
      "Epoch: 6641 mean train loss:  1.19994827e-02, mean val. rec. loss:  1.13293096e-02\n",
      "Epoch: 6642 mean train loss:  1.19984408e-02, mean val. rec. loss:  1.13282924e-02\n",
      "Epoch: 6643 mean train loss:  1.19974007e-02, mean val. rec. loss:  1.13272877e-02\n",
      "Epoch: 6644 mean train loss:  1.19963635e-02, mean val. rec. loss:  1.13262762e-02\n",
      "Epoch: 6645 mean train loss:  1.19953263e-02, mean val. rec. loss:  1.13252692e-02\n",
      "Epoch: 6646 mean train loss:  1.19942834e-02, mean val. rec. loss:  1.13242600e-02\n",
      "Epoch: 6647 mean train loss:  1.19932453e-02, mean val. rec. loss:  1.13232541e-02\n",
      "Epoch: 6648 mean train loss:  1.19922117e-02, mean val. rec. loss:  1.13222528e-02\n",
      "Epoch: 6649 mean train loss:  1.19911726e-02, mean val. rec. loss:  1.13212481e-02\n",
      "Epoch: 6650 mean train loss:  1.19901307e-02, mean val. rec. loss:  1.13202366e-02\n",
      "Epoch: 6651 mean train loss:  1.19890954e-02, mean val. rec. loss:  1.13192410e-02\n",
      "Epoch: 6652 mean train loss:  1.19880619e-02, mean val. rec. loss:  1.13182227e-02\n",
      "Epoch: 6653 mean train loss:  1.19870228e-02, mean val. rec. loss:  1.13172202e-02\n",
      "Epoch: 6654 mean train loss:  1.19859827e-02, mean val. rec. loss:  1.13162087e-02\n",
      "Epoch: 6655 mean train loss:  1.19849501e-02, mean val. rec. loss:  1.13152074e-02\n",
      "Epoch: 6656 mean train loss:  1.19839157e-02, mean val. rec. loss:  1.13141948e-02\n",
      "Epoch: 6657 mean train loss:  1.19828775e-02, mean val. rec. loss:  1.13131946e-02\n",
      "Epoch: 6658 mean train loss:  1.19818403e-02, mean val. rec. loss:  1.13121842e-02\n",
      "Epoch: 6659 mean train loss:  1.19808068e-02, mean val. rec. loss:  1.13111886e-02\n",
      "Epoch: 6660 mean train loss:  1.19797742e-02, mean val. rec. loss:  1.13101760e-02\n",
      "Epoch: 6661 mean train loss:  1.19787360e-02, mean val. rec. loss:  1.13091713e-02\n",
      "Epoch: 6662 mean train loss:  1.19776997e-02, mean val. rec. loss:  1.13081632e-02\n",
      "Epoch: 6663 mean train loss:  1.19766690e-02, mean val. rec. loss:  1.13071630e-02\n",
      "Epoch: 6664 mean train loss:  1.19756364e-02, mean val. rec. loss:  1.13061537e-02\n",
      "Epoch: 6665 mean train loss:  1.19746019e-02, mean val. rec. loss:  1.13051502e-02\n",
      "Epoch: 6666 mean train loss:  1.19735647e-02, mean val. rec. loss:  1.13041466e-02\n",
      "Epoch: 6667 mean train loss:  1.19725368e-02, mean val. rec. loss:  1.13031408e-02\n",
      "Epoch: 6668 mean train loss:  1.19715033e-02, mean val. rec. loss:  1.13021315e-02\n",
      "Epoch: 6669 mean train loss:  1.19704660e-02, mean val. rec. loss:  1.13011223e-02\n",
      "Epoch: 6670 mean train loss:  1.19694344e-02, mean val. rec. loss:  1.13001210e-02\n",
      "Epoch: 6671 mean train loss:  1.19684055e-02, mean val. rec. loss:  1.12991152e-02\n",
      "Epoch: 6672 mean train loss:  1.19673738e-02, mean val. rec. loss:  1.12981082e-02\n",
      "Epoch: 6673 mean train loss:  1.19663357e-02, mean val. rec. loss:  1.12971148e-02\n",
      "Epoch: 6674 mean train loss:  1.19653096e-02, mean val. rec. loss:  1.12961079e-02\n",
      "Epoch: 6675 mean train loss:  1.19642761e-02, mean val. rec. loss:  1.12951077e-02\n",
      "Epoch: 6676 mean train loss:  1.19632482e-02, mean val. rec. loss:  1.12941007e-02\n",
      "Epoch: 6677 mean train loss:  1.19622109e-02, mean val. rec. loss:  1.12930994e-02\n",
      "Epoch: 6678 mean train loss:  1.19611867e-02, mean val. rec. loss:  1.12920981e-02\n",
      "Epoch: 6679 mean train loss:  1.19601551e-02, mean val. rec. loss:  1.12910957e-02\n",
      "Epoch: 6680 mean train loss:  1.19591281e-02, mean val. rec. loss:  1.12900898e-02\n",
      "Epoch: 6681 mean train loss:  1.19580936e-02, mean val. rec. loss:  1.12890908e-02\n",
      "Epoch: 6682 mean train loss:  1.19570685e-02, mean val. rec. loss:  1.12880895e-02\n",
      "Epoch: 6683 mean train loss:  1.19560350e-02, mean val. rec. loss:  1.12870859e-02\n",
      "Epoch: 6684 mean train loss:  1.19550108e-02, mean val. rec. loss:  1.12860778e-02\n",
      "Epoch: 6685 mean train loss:  1.19539782e-02, mean val. rec. loss:  1.12850788e-02\n",
      "Epoch: 6686 mean train loss:  1.19529540e-02, mean val. rec. loss:  1.12840764e-02\n",
      "Epoch: 6687 mean train loss:  1.19519223e-02, mean val. rec. loss:  1.12830739e-02\n",
      "Epoch: 6688 mean train loss:  1.19508981e-02, mean val. rec. loss:  1.12820704e-02\n",
      "Epoch: 6689 mean train loss:  1.19498665e-02, mean val. rec. loss:  1.12810713e-02\n",
      "Epoch: 6690 mean train loss:  1.19488423e-02, mean val. rec. loss:  1.12800689e-02\n",
      "Epoch: 6691 mean train loss:  1.19478115e-02, mean val. rec. loss:  1.12790642e-02\n",
      "Epoch: 6692 mean train loss:  1.19467901e-02, mean val. rec. loss:  1.12780561e-02\n",
      "Epoch: 6693 mean train loss:  1.19457585e-02, mean val. rec. loss:  1.12770593e-02\n",
      "Epoch: 6694 mean train loss:  1.19447361e-02, mean val. rec. loss:  1.12760625e-02\n",
      "Epoch: 6695 mean train loss:  1.19437063e-02, mean val. rec. loss:  1.12750590e-02\n",
      "Epoch: 6696 mean train loss:  1.19426859e-02, mean val. rec. loss:  1.12740565e-02\n",
      "Epoch: 6697 mean train loss:  1.19416561e-02, mean val. rec. loss:  1.12730575e-02\n",
      "Epoch: 6698 mean train loss:  1.19406356e-02, mean val. rec. loss:  1.12720551e-02\n",
      "Epoch: 6699 mean train loss:  1.19396077e-02, mean val. rec. loss:  1.12710560e-02\n",
      "Epoch: 6700 mean train loss:  1.19385853e-02, mean val. rec. loss:  1.12700468e-02\n",
      "Epoch: 6701 mean train loss:  1.19375546e-02, mean val. rec. loss:  1.12690512e-02\n",
      "Epoch: 6702 mean train loss:  1.19365369e-02, mean val. rec. loss:  1.12680487e-02\n",
      "Epoch: 6703 mean train loss:  1.19355099e-02, mean val. rec. loss:  1.12670497e-02\n",
      "Epoch: 6704 mean train loss:  1.19344904e-02, mean val. rec. loss:  1.12660473e-02\n",
      "Epoch: 6705 mean train loss:  1.19334606e-02, mean val. rec. loss:  1.12650505e-02\n",
      "Epoch: 6706 mean train loss:  1.19324438e-02, mean val. rec. loss:  1.12640469e-02\n",
      "Epoch: 6707 mean train loss:  1.19314177e-02, mean val. rec. loss:  1.12630502e-02\n",
      "Epoch: 6708 mean train loss:  1.19303982e-02, mean val. rec. loss:  1.12620455e-02\n",
      "Epoch: 6709 mean train loss:  1.19293693e-02, mean val. rec. loss:  1.12610498e-02\n",
      "Epoch: 6710 mean train loss:  1.19283535e-02, mean val. rec. loss:  1.12600519e-02\n",
      "Epoch: 6711 mean train loss:  1.19273321e-02, mean val. rec. loss:  1.12590495e-02\n",
      "Epoch: 6712 mean train loss:  1.19263060e-02, mean val. rec. loss:  1.12580516e-02\n",
      "Epoch: 6713 mean train loss:  1.19252874e-02, mean val. rec. loss:  1.12570548e-02\n",
      "Epoch: 6714 mean train loss:  1.19242697e-02, mean val. rec. loss:  1.12560569e-02\n",
      "Epoch: 6715 mean train loss:  1.19232465e-02, mean val. rec. loss:  1.12550534e-02\n",
      "Epoch: 6716 mean train loss:  1.19222241e-02, mean val. rec. loss:  1.12540577e-02\n",
      "Epoch: 6717 mean train loss:  1.19212055e-02, mean val. rec. loss:  1.12530598e-02\n",
      "Epoch: 6718 mean train loss:  1.19201878e-02, mean val. rec. loss:  1.12520619e-02\n",
      "Epoch: 6719 mean train loss:  1.19191655e-02, mean val. rec. loss:  1.12510652e-02\n",
      "Epoch: 6720 mean train loss:  1.19181450e-02, mean val. rec. loss:  1.12500661e-02\n",
      "Epoch: 6721 mean train loss:  1.19171273e-02, mean val. rec. loss:  1.12490694e-02\n",
      "Epoch: 6722 mean train loss:  1.19161115e-02, mean val. rec. loss:  1.12480692e-02\n",
      "Epoch: 6723 mean train loss:  1.19150901e-02, mean val. rec. loss:  1.12470724e-02\n",
      "Epoch: 6724 mean train loss:  1.19140705e-02, mean val. rec. loss:  1.12460791e-02\n",
      "Epoch: 6725 mean train loss:  1.19130538e-02, mean val. rec. loss:  1.12450812e-02\n",
      "Epoch: 6726 mean train loss:  1.19120389e-02, mean val. rec. loss:  1.12440833e-02\n",
      "Epoch: 6727 mean train loss:  1.19110193e-02, mean val. rec. loss:  1.12430842e-02\n",
      "Epoch: 6728 mean train loss:  1.19100007e-02, mean val. rec. loss:  1.12420875e-02\n",
      "Epoch: 6729 mean train loss:  1.19089858e-02, mean val. rec. loss:  1.12410964e-02\n",
      "Epoch: 6730 mean train loss:  1.19079709e-02, mean val. rec. loss:  1.12400962e-02\n",
      "Epoch: 6731 mean train loss:  1.19069523e-02, mean val. rec. loss:  1.12391006e-02\n",
      "Epoch: 6732 mean train loss:  1.19059337e-02, mean val. rec. loss:  1.12381118e-02\n",
      "Epoch: 6733 mean train loss:  1.19049225e-02, mean val. rec. loss:  1.12371116e-02\n",
      "Epoch: 6734 mean train loss:  1.19039057e-02, mean val. rec. loss:  1.12361137e-02\n",
      "Epoch: 6735 mean train loss:  1.19028871e-02, mean val. rec. loss:  1.12351169e-02\n",
      "Epoch: 6736 mean train loss:  1.19018722e-02, mean val. rec. loss:  1.12341213e-02\n",
      "Epoch: 6737 mean train loss:  1.19008611e-02, mean val. rec. loss:  1.12331268e-02\n",
      "Epoch: 6738 mean train loss:  1.18998480e-02, mean val. rec. loss:  1.12321289e-02\n",
      "Epoch: 6739 mean train loss:  1.18988285e-02, mean val. rec. loss:  1.12311310e-02\n",
      "Epoch: 6740 mean train loss:  1.18978145e-02, mean val. rec. loss:  1.12301422e-02\n",
      "Epoch: 6741 mean train loss:  1.18968052e-02, mean val. rec. loss:  1.12291431e-02\n",
      "Epoch: 6742 mean train loss:  1.18957922e-02, mean val. rec. loss:  1.12281452e-02\n",
      "Epoch: 6743 mean train loss:  1.18947745e-02, mean val. rec. loss:  1.12271541e-02\n",
      "Epoch: 6744 mean train loss:  1.18937605e-02, mean val. rec. loss:  1.12261562e-02\n",
      "Epoch: 6745 mean train loss:  1.18927531e-02, mean val. rec. loss:  1.12251651e-02\n",
      "Epoch: 6746 mean train loss:  1.18917419e-02, mean val. rec. loss:  1.12241672e-02\n",
      "Epoch: 6747 mean train loss:  1.18907252e-02, mean val. rec. loss:  1.12231716e-02\n",
      "Epoch: 6748 mean train loss:  1.18897131e-02, mean val. rec. loss:  1.12221828e-02\n",
      "Epoch: 6749 mean train loss:  1.18887047e-02, mean val. rec. loss:  1.12211860e-02\n",
      "Epoch: 6750 mean train loss:  1.18876944e-02, mean val. rec. loss:  1.12201904e-02\n",
      "Epoch: 6751 mean train loss:  1.18866823e-02, mean val. rec. loss:  1.12191880e-02\n",
      "Epoch: 6752 mean train loss:  1.18856693e-02, mean val. rec. loss:  1.12181969e-02\n",
      "Epoch: 6753 mean train loss:  1.18846600e-02, mean val. rec. loss:  1.12172080e-02\n",
      "Epoch: 6754 mean train loss:  1.18836526e-02, mean val. rec. loss:  1.12162113e-02\n",
      "Epoch: 6755 mean train loss:  1.18826405e-02, mean val. rec. loss:  1.12152122e-02\n",
      "Epoch: 6756 mean train loss:  1.18816284e-02, mean val. rec. loss:  1.12142257e-02\n",
      "Epoch: 6757 mean train loss:  1.18806209e-02, mean val. rec. loss:  1.12132380e-02\n",
      "Epoch: 6758 mean train loss:  1.18796116e-02, mean val. rec. loss:  1.12122435e-02\n",
      "Epoch: 6759 mean train loss:  1.18786032e-02, mean val. rec. loss:  1.12112490e-02\n",
      "Epoch: 6760 mean train loss:  1.18775949e-02, mean val. rec. loss:  1.12102579e-02\n",
      "Epoch: 6761 mean train loss:  1.18765865e-02, mean val. rec. loss:  1.12092702e-02\n",
      "Epoch: 6762 mean train loss:  1.18755781e-02, mean val. rec. loss:  1.12082791e-02\n",
      "Epoch: 6763 mean train loss:  1.18745697e-02, mean val. rec. loss:  1.12072835e-02\n",
      "Epoch: 6764 mean train loss:  1.18735632e-02, mean val. rec. loss:  1.12062958e-02\n",
      "Epoch: 6765 mean train loss:  1.18725558e-02, mean val. rec. loss:  1.12053069e-02\n",
      "Epoch: 6766 mean train loss:  1.18715483e-02, mean val. rec. loss:  1.12043158e-02\n",
      "Epoch: 6767 mean train loss:  1.18705418e-02, mean val. rec. loss:  1.12033248e-02\n",
      "Epoch: 6768 mean train loss:  1.18695372e-02, mean val. rec. loss:  1.12023348e-02\n",
      "Epoch: 6769 mean train loss:  1.18685297e-02, mean val. rec. loss:  1.12013346e-02\n",
      "Epoch: 6770 mean train loss:  1.18675241e-02, mean val. rec. loss:  1.12003458e-02\n",
      "Epoch: 6771 mean train loss:  1.18665185e-02, mean val. rec. loss:  1.11993468e-02\n",
      "Epoch: 6772 mean train loss:  1.18655130e-02, mean val. rec. loss:  1.11983523e-02\n",
      "Epoch: 6773 mean train loss:  1.18645083e-02, mean val. rec. loss:  1.11973612e-02\n",
      "Epoch: 6774 mean train loss:  1.18635018e-02, mean val. rec. loss:  1.11963746e-02\n",
      "Epoch: 6775 mean train loss:  1.18624990e-02, mean val. rec. loss:  1.11953858e-02\n",
      "Epoch: 6776 mean train loss:  1.18614971e-02, mean val. rec. loss:  1.11943970e-02\n",
      "Epoch: 6777 mean train loss:  1.18604906e-02, mean val. rec. loss:  1.11934013e-02\n",
      "Epoch: 6778 mean train loss:  1.18594850e-02, mean val. rec. loss:  1.11924125e-02\n",
      "Epoch: 6779 mean train loss:  1.18584832e-02, mean val. rec. loss:  1.11914271e-02\n",
      "Epoch: 6780 mean train loss:  1.18574813e-02, mean val. rec. loss:  1.11904348e-02\n",
      "Epoch: 6781 mean train loss:  1.18564785e-02, mean val. rec. loss:  1.11894460e-02\n",
      "Epoch: 6782 mean train loss:  1.18554739e-02, mean val. rec. loss:  1.11884629e-02\n",
      "Epoch: 6783 mean train loss:  1.18544748e-02, mean val. rec. loss:  1.11874740e-02\n",
      "Epoch: 6784 mean train loss:  1.18534758e-02, mean val. rec. loss:  1.11864863e-02\n",
      "Epoch: 6785 mean train loss:  1.18524720e-02, mean val. rec. loss:  1.11854930e-02\n",
      "Epoch: 6786 mean train loss:  1.18514683e-02, mean val. rec. loss:  1.11845087e-02\n",
      "Epoch: 6787 mean train loss:  1.18504683e-02, mean val. rec. loss:  1.11835210e-02\n",
      "Epoch: 6788 mean train loss:  1.18494702e-02, mean val. rec. loss:  1.11825310e-02\n",
      "Epoch: 6789 mean train loss:  1.18484683e-02, mean val. rec. loss:  1.11815422e-02\n",
      "Epoch: 6790 mean train loss:  1.18474665e-02, mean val. rec. loss:  1.11805466e-02\n",
      "Epoch: 6791 mean train loss:  1.18464646e-02, mean val. rec. loss:  1.11795577e-02\n",
      "Epoch: 6792 mean train loss:  1.18454637e-02, mean val. rec. loss:  1.11785712e-02\n",
      "Epoch: 6793 mean train loss:  1.18444637e-02, mean val. rec. loss:  1.11775812e-02\n",
      "Epoch: 6794 mean train loss:  1.18434656e-02, mean val. rec. loss:  1.11765958e-02\n",
      "Epoch: 6795 mean train loss:  1.18424665e-02, mean val. rec. loss:  1.11756081e-02\n",
      "Epoch: 6796 mean train loss:  1.18414674e-02, mean val. rec. loss:  1.11746204e-02\n",
      "Epoch: 6797 mean train loss:  1.18404693e-02, mean val. rec. loss:  1.11736327e-02\n",
      "Epoch: 6798 mean train loss:  1.18394712e-02, mean val. rec. loss:  1.11726450e-02\n",
      "Epoch: 6799 mean train loss:  1.18384740e-02, mean val. rec. loss:  1.11716607e-02\n",
      "Epoch: 6800 mean train loss:  1.18374758e-02, mean val. rec. loss:  1.11706719e-02\n",
      "Epoch: 6801 mean train loss:  1.18364777e-02, mean val. rec. loss:  1.11696865e-02\n",
      "Epoch: 6802 mean train loss:  1.18354823e-02, mean val. rec. loss:  1.11687010e-02\n",
      "Epoch: 6803 mean train loss:  1.18344842e-02, mean val. rec. loss:  1.11677134e-02\n",
      "Epoch: 6804 mean train loss:  1.18334870e-02, mean val. rec. loss:  1.11667279e-02\n",
      "Epoch: 6805 mean train loss:  1.18324907e-02, mean val. rec. loss:  1.11657425e-02\n",
      "Epoch: 6806 mean train loss:  1.18314945e-02, mean val. rec. loss:  1.11647548e-02\n",
      "Epoch: 6807 mean train loss:  1.18305001e-02, mean val. rec. loss:  1.11637705e-02\n",
      "Epoch: 6808 mean train loss:  1.18295019e-02, mean val. rec. loss:  1.11627828e-02\n",
      "Epoch: 6809 mean train loss:  1.18285047e-02, mean val. rec. loss:  1.11617974e-02\n",
      "Epoch: 6810 mean train loss:  1.18275122e-02, mean val. rec. loss:  1.11608108e-02\n",
      "Epoch: 6811 mean train loss:  1.18265196e-02, mean val. rec. loss:  1.11598243e-02\n",
      "Epoch: 6812 mean train loss:  1.18255224e-02, mean val. rec. loss:  1.11588377e-02\n",
      "Epoch: 6813 mean train loss:  1.18245271e-02, mean val. rec. loss:  1.11578580e-02\n",
      "Epoch: 6814 mean train loss:  1.18235345e-02, mean val. rec. loss:  1.11568703e-02\n",
      "Epoch: 6815 mean train loss:  1.18225429e-02, mean val. rec. loss:  1.11558860e-02\n",
      "Epoch: 6816 mean train loss:  1.18215504e-02, mean val. rec. loss:  1.11548983e-02\n",
      "Epoch: 6817 mean train loss:  1.18205550e-02, mean val. rec. loss:  1.11539117e-02\n",
      "Epoch: 6818 mean train loss:  1.18195643e-02, mean val. rec. loss:  1.11529320e-02\n",
      "Epoch: 6819 mean train loss:  1.18185737e-02, mean val. rec. loss:  1.11519488e-02\n",
      "Epoch: 6820 mean train loss:  1.18175802e-02, mean val. rec. loss:  1.11509634e-02\n",
      "Epoch: 6821 mean train loss:  1.18165858e-02, mean val. rec. loss:  1.11499836e-02\n",
      "Epoch: 6822 mean train loss:  1.18155960e-02, mean val. rec. loss:  1.11489959e-02\n",
      "Epoch: 6823 mean train loss:  1.18146063e-02, mean val. rec. loss:  1.11480105e-02\n",
      "Epoch: 6824 mean train loss:  1.18136137e-02, mean val. rec. loss:  1.11470239e-02\n",
      "Epoch: 6825 mean train loss:  1.18126202e-02, mean val. rec. loss:  1.11460419e-02\n",
      "Epoch: 6826 mean train loss:  1.18116323e-02, mean val. rec. loss:  1.11450644e-02\n",
      "Epoch: 6827 mean train loss:  1.18106444e-02, mean val. rec. loss:  1.11440824e-02\n",
      "Epoch: 6828 mean train loss:  1.18096528e-02, mean val. rec. loss:  1.11430981e-02\n",
      "Epoch: 6829 mean train loss:  1.18086603e-02, mean val. rec. loss:  1.11421172e-02\n",
      "Epoch: 6830 mean train loss:  1.18076752e-02, mean val. rec. loss:  1.11411307e-02\n",
      "Epoch: 6831 mean train loss:  1.18066817e-02, mean val. rec. loss:  1.11401475e-02\n",
      "Epoch: 6832 mean train loss:  1.18056957e-02, mean val. rec. loss:  1.11391632e-02\n",
      "Epoch: 6833 mean train loss:  1.18047041e-02, mean val. rec. loss:  1.11381835e-02\n",
      "Epoch: 6834 mean train loss:  1.18037199e-02, mean val. rec. loss:  1.11371969e-02\n",
      "Epoch: 6835 mean train loss:  1.18027273e-02, mean val. rec. loss:  1.11362206e-02\n",
      "Epoch: 6836 mean train loss:  1.18017441e-02, mean val. rec. loss:  1.11352329e-02\n",
      "Epoch: 6837 mean train loss:  1.18007516e-02, mean val. rec. loss:  1.11342565e-02\n",
      "Epoch: 6838 mean train loss:  1.17997693e-02, mean val. rec. loss:  1.11332722e-02\n",
      "Epoch: 6839 mean train loss:  1.17987786e-02, mean val. rec. loss:  1.11322947e-02\n",
      "Epoch: 6840 mean train loss:  1.17977953e-02, mean val. rec. loss:  1.11313059e-02\n",
      "Epoch: 6841 mean train loss:  1.17968046e-02, mean val. rec. loss:  1.11303284e-02\n",
      "Epoch: 6842 mean train loss:  1.17958233e-02, mean val. rec. loss:  1.11293453e-02\n",
      "Epoch: 6843 mean train loss:  1.17948326e-02, mean val. rec. loss:  1.11283689e-02\n",
      "Epoch: 6844 mean train loss:  1.17938503e-02, mean val. rec. loss:  1.11273835e-02\n",
      "Epoch: 6845 mean train loss:  1.17928615e-02, mean val. rec. loss:  1.11264071e-02\n",
      "Epoch: 6846 mean train loss:  1.17918810e-02, mean val. rec. loss:  1.11254217e-02\n",
      "Epoch: 6847 mean train loss:  1.17908950e-02, mean val. rec. loss:  1.11244465e-02\n",
      "Epoch: 6848 mean train loss:  1.17899127e-02, mean val. rec. loss:  1.11234622e-02\n",
      "Epoch: 6849 mean train loss:  1.17889257e-02, mean val. rec. loss:  1.11224824e-02\n",
      "Epoch: 6850 mean train loss:  1.17879425e-02, mean val. rec. loss:  1.11215027e-02\n",
      "Epoch: 6851 mean train loss:  1.17869620e-02, mean val. rec. loss:  1.11205229e-02\n",
      "Epoch: 6852 mean train loss:  1.17859751e-02, mean val. rec. loss:  1.11195386e-02\n",
      "Epoch: 6853 mean train loss:  1.17849918e-02, mean val. rec. loss:  1.11185634e-02\n",
      "Epoch: 6854 mean train loss:  1.17840095e-02, mean val. rec. loss:  1.11175836e-02\n",
      "Epoch: 6855 mean train loss:  1.17830291e-02, mean val. rec. loss:  1.11165948e-02\n",
      "Epoch: 6856 mean train loss:  1.17820421e-02, mean val. rec. loss:  1.11156128e-02\n",
      "Epoch: 6857 mean train loss:  1.17810561e-02, mean val. rec. loss:  1.11146353e-02\n",
      "Epoch: 6858 mean train loss:  1.17800775e-02, mean val. rec. loss:  1.11136578e-02\n",
      "Epoch: 6859 mean train loss:  1.17790980e-02, mean val. rec. loss:  1.11126803e-02\n",
      "Epoch: 6860 mean train loss:  1.17781129e-02, mean val. rec. loss:  1.11116949e-02\n",
      "Epoch: 6861 mean train loss:  1.17771334e-02, mean val. rec. loss:  1.11107242e-02\n",
      "Epoch: 6862 mean train loss:  1.17761557e-02, mean val. rec. loss:  1.11097433e-02\n",
      "Epoch: 6863 mean train loss:  1.17751781e-02, mean val. rec. loss:  1.11087636e-02\n",
      "Epoch: 6864 mean train loss:  1.17741958e-02, mean val. rec. loss:  1.11077861e-02\n",
      "Epoch: 6865 mean train loss:  1.17732144e-02, mean val. rec. loss:  1.11068120e-02\n",
      "Epoch: 6866 mean train loss:  1.17722368e-02, mean val. rec. loss:  1.11058357e-02\n",
      "Epoch: 6867 mean train loss:  1.17712591e-02, mean val. rec. loss:  1.11048582e-02\n",
      "Epoch: 6868 mean train loss:  1.17702787e-02, mean val. rec. loss:  1.11038807e-02\n",
      "Epoch: 6869 mean train loss:  1.17692964e-02, mean val. rec. loss:  1.11029055e-02\n",
      "Epoch: 6870 mean train loss:  1.17683215e-02, mean val. rec. loss:  1.11019268e-02\n",
      "Epoch: 6871 mean train loss:  1.17673420e-02, mean val. rec. loss:  1.11009425e-02\n",
      "Epoch: 6872 mean train loss:  1.17663606e-02, mean val. rec. loss:  1.10999685e-02\n",
      "Epoch: 6873 mean train loss:  1.17653802e-02, mean val. rec. loss:  1.10989955e-02\n",
      "Epoch: 6874 mean train loss:  1.17644053e-02, mean val. rec. loss:  1.10980203e-02\n",
      "Epoch: 6875 mean train loss:  1.17634314e-02, mean val. rec. loss:  1.10970485e-02\n",
      "Epoch: 6876 mean train loss:  1.17624519e-02, mean val. rec. loss:  1.10960732e-02\n",
      "Epoch: 6877 mean train loss:  1.17614761e-02, mean val. rec. loss:  1.10950912e-02\n",
      "Epoch: 6878 mean train loss:  1.17605012e-02, mean val. rec. loss:  1.10941126e-02\n",
      "Epoch: 6879 mean train loss:  1.17595255e-02, mean val. rec. loss:  1.10931340e-02\n",
      "Epoch: 6880 mean train loss:  1.17585497e-02, mean val. rec. loss:  1.10921644e-02\n",
      "Epoch: 6881 mean train loss:  1.17575739e-02, mean val. rec. loss:  1.10911915e-02\n",
      "Epoch: 6882 mean train loss:  1.17565990e-02, mean val. rec. loss:  1.10902129e-02\n",
      "Epoch: 6883 mean train loss:  1.17556223e-02, mean val. rec. loss:  1.10892308e-02\n",
      "Epoch: 6884 mean train loss:  1.17546456e-02, mean val. rec. loss:  1.10882522e-02\n",
      "Epoch: 6885 mean train loss:  1.17536717e-02, mean val. rec. loss:  1.10872815e-02\n",
      "Epoch: 6886 mean train loss:  1.17526996e-02, mean val. rec. loss:  1.10863018e-02\n",
      "Epoch: 6887 mean train loss:  1.17517247e-02, mean val. rec. loss:  1.10853288e-02\n",
      "Epoch: 6888 mean train loss:  1.17507499e-02, mean val. rec. loss:  1.10843593e-02\n",
      "Epoch: 6889 mean train loss:  1.17497769e-02, mean val. rec. loss:  1.10833874e-02\n",
      "Epoch: 6890 mean train loss:  1.17488039e-02, mean val. rec. loss:  1.10824134e-02\n",
      "Epoch: 6891 mean train loss:  1.17478318e-02, mean val. rec. loss:  1.10814336e-02\n",
      "Epoch: 6892 mean train loss:  1.17468560e-02, mean val. rec. loss:  1.10804618e-02\n",
      "Epoch: 6893 mean train loss:  1.17458868e-02, mean val. rec. loss:  1.10794866e-02\n",
      "Epoch: 6894 mean train loss:  1.17449156e-02, mean val. rec. loss:  1.10785159e-02\n",
      "Epoch: 6895 mean train loss:  1.17439445e-02, mean val. rec. loss:  1.10775407e-02\n",
      "Epoch: 6896 mean train loss:  1.17429706e-02, mean val. rec. loss:  1.10765688e-02\n",
      "Epoch: 6897 mean train loss:  1.17419976e-02, mean val. rec. loss:  1.10755936e-02\n",
      "Epoch: 6898 mean train loss:  1.17410292e-02, mean val. rec. loss:  1.10746150e-02\n",
      "Epoch: 6899 mean train loss:  1.17400600e-02, mean val. rec. loss:  1.10736455e-02\n",
      "Epoch: 6900 mean train loss:  1.17390888e-02, mean val. rec. loss:  1.10726725e-02\n",
      "Epoch: 6901 mean train loss:  1.17381224e-02, mean val. rec. loss:  1.10717030e-02\n",
      "Epoch: 6902 mean train loss:  1.17371503e-02, mean val. rec. loss:  1.10707300e-02\n",
      "Epoch: 6903 mean train loss:  1.17361782e-02, mean val. rec. loss:  1.10697548e-02\n",
      "Epoch: 6904 mean train loss:  1.17352080e-02, mean val. rec. loss:  1.10687796e-02\n",
      "Epoch: 6905 mean train loss:  1.17342388e-02, mean val. rec. loss:  1.10678055e-02\n",
      "Epoch: 6906 mean train loss:  1.17332695e-02, mean val. rec. loss:  1.10668303e-02\n",
      "Epoch: 6907 mean train loss:  1.17323021e-02, mean val. rec. loss:  1.10658630e-02\n",
      "Epoch: 6908 mean train loss:  1.17313338e-02, mean val. rec. loss:  1.10648878e-02\n",
      "Epoch: 6909 mean train loss:  1.17303654e-02, mean val. rec. loss:  1.10639171e-02\n",
      "Epoch: 6910 mean train loss:  1.17293980e-02, mean val. rec. loss:  1.10629441e-02\n",
      "Epoch: 6911 mean train loss:  1.17284306e-02, mean val. rec. loss:  1.10619689e-02\n",
      "Epoch: 6912 mean train loss:  1.17274632e-02, mean val. rec. loss:  1.10609982e-02\n",
      "Epoch: 6913 mean train loss:  1.17264967e-02, mean val. rec. loss:  1.10600298e-02\n",
      "Epoch: 6914 mean train loss:  1.17255293e-02, mean val. rec. loss:  1.10590546e-02\n",
      "Epoch: 6915 mean train loss:  1.17245628e-02, mean val. rec. loss:  1.10580907e-02\n",
      "Epoch: 6916 mean train loss:  1.17235964e-02, mean val. rec. loss:  1.10571155e-02\n",
      "Epoch: 6917 mean train loss:  1.17226308e-02, mean val. rec. loss:  1.10561414e-02\n",
      "Epoch: 6918 mean train loss:  1.17216643e-02, mean val. rec. loss:  1.10551684e-02\n",
      "Epoch: 6919 mean train loss:  1.17206997e-02, mean val. rec. loss:  1.10541978e-02\n",
      "Epoch: 6920 mean train loss:  1.17197351e-02, mean val. rec. loss:  1.10532327e-02\n",
      "Epoch: 6921 mean train loss:  1.17187705e-02, mean val. rec. loss:  1.10522689e-02\n",
      "Epoch: 6922 mean train loss:  1.17178049e-02, mean val. rec. loss:  1.10512982e-02\n",
      "Epoch: 6923 mean train loss:  1.17168431e-02, mean val. rec. loss:  1.10503298e-02\n",
      "Epoch: 6924 mean train loss:  1.17158776e-02, mean val. rec. loss:  1.10493591e-02\n",
      "Epoch: 6925 mean train loss:  1.17149139e-02, mean val. rec. loss:  1.10483861e-02\n",
      "Epoch: 6926 mean train loss:  1.17139502e-02, mean val. rec. loss:  1.10474200e-02\n",
      "Epoch: 6927 mean train loss:  1.17129875e-02, mean val. rec. loss:  1.10464516e-02\n",
      "Epoch: 6928 mean train loss:  1.17120238e-02, mean val. rec. loss:  1.10454854e-02\n",
      "Epoch: 6929 mean train loss:  1.17110610e-02, mean val. rec. loss:  1.10445215e-02\n",
      "Epoch: 6930 mean train loss:  1.17100992e-02, mean val. rec. loss:  1.10435520e-02\n",
      "Epoch: 6931 mean train loss:  1.17091392e-02, mean val. rec. loss:  1.10425756e-02\n",
      "Epoch: 6932 mean train loss:  1.17081746e-02, mean val. rec. loss:  1.10416049e-02\n",
      "Epoch: 6933 mean train loss:  1.17072119e-02, mean val. rec. loss:  1.10406388e-02\n",
      "Epoch: 6934 mean train loss:  1.17062528e-02, mean val. rec. loss:  1.10396772e-02\n",
      "Epoch: 6935 mean train loss:  1.17052929e-02, mean val. rec. loss:  1.10387088e-02\n",
      "Epoch: 6936 mean train loss:  1.17043301e-02, mean val. rec. loss:  1.10377313e-02\n",
      "Epoch: 6937 mean train loss:  1.17033683e-02, mean val. rec. loss:  1.10367719e-02\n",
      "Epoch: 6938 mean train loss:  1.17024093e-02, mean val. rec. loss:  1.10358035e-02\n",
      "Epoch: 6939 mean train loss:  1.17014521e-02, mean val. rec. loss:  1.10348351e-02\n",
      "Epoch: 6940 mean train loss:  1.17004894e-02, mean val. rec. loss:  1.10338667e-02\n",
      "Epoch: 6941 mean train loss:  1.16995285e-02, mean val. rec. loss:  1.10329005e-02\n",
      "Epoch: 6942 mean train loss:  1.16985732e-02, mean val. rec. loss:  1.10319344e-02\n",
      "Epoch: 6943 mean train loss:  1.16976095e-02, mean val. rec. loss:  1.10309637e-02\n",
      "Epoch: 6944 mean train loss:  1.16966560e-02, mean val. rec. loss:  1.10299885e-02\n",
      "Epoch: 6945 mean train loss:  1.16956924e-02, mean val. rec. loss:  1.10290291e-02\n",
      "Epoch: 6946 mean train loss:  1.16947389e-02, mean val. rec. loss:  1.10280607e-02\n",
      "Epoch: 6947 mean train loss:  1.16937762e-02, mean val. rec. loss:  1.10270934e-02\n",
      "Epoch: 6948 mean train loss:  1.16928237e-02, mean val. rec. loss:  1.10261262e-02\n",
      "Epoch: 6949 mean train loss:  1.16918618e-02, mean val. rec. loss:  1.10251623e-02\n",
      "Epoch: 6950 mean train loss:  1.16909084e-02, mean val. rec. loss:  1.10241984e-02\n",
      "Epoch: 6951 mean train loss:  1.16899475e-02, mean val. rec. loss:  1.10232277e-02\n",
      "Epoch: 6952 mean train loss:  1.16889959e-02, mean val. rec. loss:  1.10222570e-02\n",
      "Epoch: 6953 mean train loss:  1.16880350e-02, mean val. rec. loss:  1.10212954e-02\n",
      "Epoch: 6954 mean train loss:  1.16870825e-02, mean val. rec. loss:  1.10203338e-02\n",
      "Epoch: 6955 mean train loss:  1.16861226e-02, mean val. rec. loss:  1.10193665e-02\n",
      "Epoch: 6956 mean train loss:  1.16851728e-02, mean val. rec. loss:  1.10183993e-02\n",
      "Epoch: 6957 mean train loss:  1.16842138e-02, mean val. rec. loss:  1.10174399e-02\n",
      "Epoch: 6958 mean train loss:  1.16832613e-02, mean val. rec. loss:  1.10164738e-02\n",
      "Epoch: 6959 mean train loss:  1.16823013e-02, mean val. rec. loss:  1.10155076e-02\n",
      "Epoch: 6960 mean train loss:  1.16813516e-02, mean val. rec. loss:  1.10145403e-02\n",
      "Epoch: 6961 mean train loss:  1.16803945e-02, mean val. rec. loss:  1.10135821e-02\n",
      "Epoch: 6962 mean train loss:  1.16794438e-02, mean val. rec. loss:  1.10126205e-02\n",
      "Epoch: 6963 mean train loss:  1.16784857e-02, mean val. rec. loss:  1.10116521e-02\n",
      "Epoch: 6964 mean train loss:  1.16775351e-02, mean val. rec. loss:  1.10106871e-02\n",
      "Epoch: 6965 mean train loss:  1.16765798e-02, mean val. rec. loss:  1.10097243e-02\n",
      "Epoch: 6966 mean train loss:  1.16756319e-02, mean val. rec. loss:  1.10087536e-02\n",
      "Epoch: 6967 mean train loss:  1.16746738e-02, mean val. rec. loss:  1.10077920e-02\n",
      "Epoch: 6968 mean train loss:  1.16737241e-02, mean val. rec. loss:  1.10068304e-02\n",
      "Epoch: 6969 mean train loss:  1.16727688e-02, mean val. rec. loss:  1.10058699e-02\n",
      "Epoch: 6970 mean train loss:  1.16718228e-02, mean val. rec. loss:  1.10049095e-02\n",
      "Epoch: 6971 mean train loss:  1.16708647e-02, mean val. rec. loss:  1.10039433e-02\n",
      "Epoch: 6972 mean train loss:  1.16699159e-02, mean val. rec. loss:  1.10029772e-02\n",
      "Epoch: 6973 mean train loss:  1.16689634e-02, mean val. rec. loss:  1.10020178e-02\n",
      "Epoch: 6974 mean train loss:  1.16680174e-02, mean val. rec. loss:  1.10010471e-02\n",
      "Epoch: 6975 mean train loss:  1.16670612e-02, mean val. rec. loss:  1.10000901e-02\n",
      "Epoch: 6976 mean train loss:  1.16661124e-02, mean val. rec. loss:  1.09991285e-02\n",
      "Epoch: 6977 mean train loss:  1.16651599e-02, mean val. rec. loss:  1.09981702e-02\n",
      "Epoch: 6978 mean train loss:  1.16642139e-02, mean val. rec. loss:  1.09972120e-02\n",
      "Epoch: 6979 mean train loss:  1.16632660e-02, mean val. rec. loss:  1.09962379e-02\n",
      "Epoch: 6980 mean train loss:  1.16623107e-02, mean val. rec. loss:  1.09952797e-02\n",
      "Epoch: 6981 mean train loss:  1.16613629e-02, mean val. rec. loss:  1.09943193e-02\n",
      "Epoch: 6982 mean train loss:  1.16604169e-02, mean val. rec. loss:  1.09933520e-02\n",
      "Epoch: 6983 mean train loss:  1.16594690e-02, mean val. rec. loss:  1.09923926e-02\n",
      "Epoch: 6984 mean train loss:  1.16585165e-02, mean val. rec. loss:  1.09914322e-02\n",
      "Epoch: 6985 mean train loss:  1.16575687e-02, mean val. rec. loss:  1.09904762e-02\n",
      "Epoch: 6986 mean train loss:  1.16566236e-02, mean val. rec. loss:  1.09895157e-02\n",
      "Epoch: 6987 mean train loss:  1.16556767e-02, mean val. rec. loss:  1.09885462e-02\n",
      "Epoch: 6988 mean train loss:  1.16547260e-02, mean val. rec. loss:  1.09875823e-02\n",
      "Epoch: 6989 mean train loss:  1.16537782e-02, mean val. rec. loss:  1.09866275e-02\n",
      "Epoch: 6990 mean train loss:  1.16528359e-02, mean val. rec. loss:  1.09856682e-02\n",
      "Epoch: 6991 mean train loss:  1.16518880e-02, mean val. rec. loss:  1.09847133e-02\n",
      "Epoch: 6992 mean train loss:  1.16509383e-02, mean val. rec. loss:  1.09837495e-02\n",
      "Epoch: 6993 mean train loss:  1.16499923e-02, mean val. rec. loss:  1.09827924e-02\n",
      "Epoch: 6994 mean train loss:  1.16490501e-02, mean val. rec. loss:  1.09818308e-02\n",
      "Epoch: 6995 mean train loss:  1.16481050e-02, mean val. rec. loss:  1.09808658e-02\n",
      "Epoch: 6996 mean train loss:  1.16471553e-02, mean val. rec. loss:  1.09799098e-02\n",
      "Epoch: 6997 mean train loss:  1.16462102e-02, mean val. rec. loss:  1.09789539e-02\n",
      "Epoch: 6998 mean train loss:  1.16452698e-02, mean val. rec. loss:  1.09779957e-02\n",
      "Epoch: 6999 mean train loss:  1.16443257e-02, mean val. rec. loss:  1.09770397e-02\n",
      "Epoch: 7000 mean train loss:  1.16433778e-02, mean val. rec. loss:  1.09760736e-02\n",
      "Epoch: 7001 mean train loss:  1.16424328e-02, mean val. rec. loss:  1.09751210e-02\n",
      "Epoch: 7002 mean train loss:  1.16414905e-02, mean val. rec. loss:  1.09741515e-02\n",
      "Epoch: 7003 mean train loss:  1.16405473e-02, mean val. rec. loss:  1.09731967e-02\n",
      "Epoch: 7004 mean train loss:  1.16396041e-02, mean val. rec. loss:  1.09722328e-02\n",
      "Epoch: 7005 mean train loss:  1.16386618e-02, mean val. rec. loss:  1.09712780e-02\n",
      "Epoch: 7006 mean train loss:  1.16377186e-02, mean val. rec. loss:  1.09703141e-02\n",
      "Epoch: 7007 mean train loss:  1.16367764e-02, mean val. rec. loss:  1.09693616e-02\n",
      "Epoch: 7008 mean train loss:  1.16358322e-02, mean val. rec. loss:  1.09683954e-02\n",
      "Epoch: 7009 mean train loss:  1.16348918e-02, mean val. rec. loss:  1.09674395e-02\n",
      "Epoch: 7010 mean train loss:  1.16339496e-02, mean val. rec. loss:  1.09664767e-02\n",
      "Epoch: 7011 mean train loss:  1.16330082e-02, mean val. rec. loss:  1.09655231e-02\n",
      "Epoch: 7012 mean train loss:  1.16320660e-02, mean val. rec. loss:  1.09645569e-02\n",
      "Epoch: 7013 mean train loss:  1.16311256e-02, mean val. rec. loss:  1.09636078e-02\n",
      "Epoch: 7014 mean train loss:  1.16301842e-02, mean val. rec. loss:  1.09626473e-02\n",
      "Epoch: 7015 mean train loss:  1.16292447e-02, mean val. rec. loss:  1.09616959e-02\n",
      "Epoch: 7016 mean train loss:  1.16283034e-02, mean val. rec. loss:  1.09607275e-02\n",
      "Epoch: 7017 mean train loss:  1.16273639e-02, mean val. rec. loss:  1.09597783e-02\n",
      "Epoch: 7018 mean train loss:  1.16264235e-02, mean val. rec. loss:  1.09588156e-02\n",
      "Epoch: 7019 mean train loss:  1.16254859e-02, mean val. rec. loss:  1.09578676e-02\n",
      "Epoch: 7020 mean train loss:  1.16245446e-02, mean val. rec. loss:  1.09569071e-02\n",
      "Epoch: 7021 mean train loss:  1.16236051e-02, mean val. rec. loss:  1.09559568e-02\n",
      "Epoch: 7022 mean train loss:  1.16226666e-02, mean val. rec. loss:  1.09549941e-02\n",
      "Epoch: 7023 mean train loss:  1.16217289e-02, mean val. rec. loss:  1.09540415e-02\n",
      "Epoch: 7024 mean train loss:  1.16207904e-02, mean val. rec. loss:  1.09530765e-02\n",
      "Epoch: 7025 mean train loss:  1.16198491e-02, mean val. rec. loss:  1.09521331e-02\n",
      "Epoch: 7026 mean train loss:  1.16189133e-02, mean val. rec. loss:  1.09511692e-02\n",
      "Epoch: 7027 mean train loss:  1.16179785e-02, mean val. rec. loss:  1.09502212e-02\n",
      "Epoch: 7028 mean train loss:  1.16170390e-02, mean val. rec. loss:  1.09492562e-02\n",
      "Epoch: 7029 mean train loss:  1.16160995e-02, mean val. rec. loss:  1.09483059e-02\n",
      "Epoch: 7030 mean train loss:  1.16151647e-02, mean val. rec. loss:  1.09473431e-02\n",
      "Epoch: 7031 mean train loss:  1.16142308e-02, mean val. rec. loss:  1.09463940e-02\n",
      "Epoch: 7032 mean train loss:  1.16132914e-02, mean val. rec. loss:  1.09454324e-02\n",
      "Epoch: 7033 mean train loss:  1.16123537e-02, mean val. rec. loss:  1.09444912e-02\n",
      "Epoch: 7034 mean train loss:  1.16114189e-02, mean val. rec. loss:  1.09435284e-02\n",
      "Epoch: 7035 mean train loss:  1.16104860e-02, mean val. rec. loss:  1.09425748e-02\n",
      "Epoch: 7036 mean train loss:  1.16095474e-02, mean val. rec. loss:  1.09416109e-02\n",
      "Epoch: 7037 mean train loss:  1.16086107e-02, mean val. rec. loss:  1.09406686e-02\n",
      "Epoch: 7038 mean train loss:  1.16076787e-02, mean val. rec. loss:  1.09397103e-02\n",
      "Epoch: 7039 mean train loss:  1.16067448e-02, mean val. rec. loss:  1.09387623e-02\n",
      "Epoch: 7040 mean train loss:  1.16058100e-02, mean val. rec. loss:  1.09378030e-02\n",
      "Epoch: 7041 mean train loss:  1.16048715e-02, mean val. rec. loss:  1.09368573e-02\n",
      "Epoch: 7042 mean train loss:  1.16039404e-02, mean val. rec. loss:  1.09359002e-02\n",
      "Epoch: 7043 mean train loss:  1.16030093e-02, mean val. rec. loss:  1.09349544e-02\n",
      "Epoch: 7044 mean train loss:  1.16020735e-02, mean val. rec. loss:  1.09339917e-02\n",
      "Epoch: 7045 mean train loss:  1.16011396e-02, mean val. rec. loss:  1.09330380e-02\n",
      "Epoch: 7046 mean train loss:  1.16002067e-02, mean val. rec. loss:  1.09320878e-02\n",
      "Epoch: 7047 mean train loss:  1.15992737e-02, mean val. rec. loss:  1.09311318e-02\n",
      "Epoch: 7048 mean train loss:  1.15983426e-02, mean val. rec. loss:  1.09301827e-02\n",
      "Epoch: 7049 mean train loss:  1.15974115e-02, mean val. rec. loss:  1.09292256e-02\n",
      "Epoch: 7050 mean train loss:  1.15964776e-02, mean val. rec. loss:  1.09282731e-02\n",
      "Epoch: 7051 mean train loss:  1.15955428e-02, mean val. rec. loss:  1.09273228e-02\n",
      "Epoch: 7052 mean train loss:  1.15946136e-02, mean val. rec. loss:  1.09263634e-02\n",
      "Epoch: 7053 mean train loss:  1.15936844e-02, mean val. rec. loss:  1.09254132e-02\n",
      "Epoch: 7054 mean train loss:  1.15927523e-02, mean val. rec. loss:  1.09244652e-02\n",
      "Epoch: 7055 mean train loss:  1.15918222e-02, mean val. rec. loss:  1.09235058e-02\n",
      "Epoch: 7056 mean train loss:  1.15908901e-02, mean val. rec. loss:  1.09225533e-02\n",
      "Epoch: 7057 mean train loss:  1.15899572e-02, mean val. rec. loss:  1.09216007e-02\n",
      "Epoch: 7058 mean train loss:  1.15890289e-02, mean val. rec. loss:  1.09206573e-02\n",
      "Epoch: 7059 mean train loss:  1.15881006e-02, mean val. rec. loss:  1.09197047e-02\n",
      "Epoch: 7060 mean train loss:  1.15871695e-02, mean val. rec. loss:  1.09187499e-02\n",
      "Epoch: 7061 mean train loss:  1.15862375e-02, mean val. rec. loss:  1.09178019e-02\n",
      "Epoch: 7062 mean train loss:  1.15853101e-02, mean val. rec. loss:  1.09168426e-02\n",
      "Epoch: 7063 mean train loss:  1.15843836e-02, mean val. rec. loss:  1.09158980e-02\n",
      "Epoch: 7064 mean train loss:  1.15834544e-02, mean val. rec. loss:  1.09149420e-02\n",
      "Epoch: 7065 mean train loss:  1.15825215e-02, mean val. rec. loss:  1.09139918e-02\n",
      "Epoch: 7066 mean train loss:  1.15815932e-02, mean val. rec. loss:  1.09130438e-02\n",
      "Epoch: 7067 mean train loss:  1.15806686e-02, mean val. rec. loss:  1.09120867e-02\n",
      "Epoch: 7068 mean train loss:  1.15797412e-02, mean val. rec. loss:  1.09111387e-02\n",
      "Epoch: 7069 mean train loss:  1.15788101e-02, mean val. rec. loss:  1.09101861e-02\n",
      "Epoch: 7070 mean train loss:  1.15778827e-02, mean val. rec. loss:  1.09092347e-02\n",
      "Epoch: 7071 mean train loss:  1.15769600e-02, mean val. rec. loss:  1.09082879e-02\n",
      "Epoch: 7072 mean train loss:  1.15760317e-02, mean val. rec. loss:  1.09073330e-02\n",
      "Epoch: 7073 mean train loss:  1.15751006e-02, mean val. rec. loss:  1.09063850e-02\n",
      "Epoch: 7074 mean train loss:  1.15741751e-02, mean val. rec. loss:  1.09054427e-02\n",
      "Epoch: 7075 mean train loss:  1.15732533e-02, mean val. rec. loss:  1.09044868e-02\n",
      "Epoch: 7076 mean train loss:  1.15723269e-02, mean val. rec. loss:  1.09035399e-02\n",
      "Epoch: 7077 mean train loss:  1.15713967e-02, mean val. rec. loss:  1.09025896e-02\n",
      "Epoch: 7078 mean train loss:  1.15704712e-02, mean val. rec. loss:  1.09016382e-02\n",
      "Epoch: 7079 mean train loss:  1.15695504e-02, mean val. rec. loss:  1.09006925e-02\n",
      "Epoch: 7080 mean train loss:  1.15686249e-02, mean val. rec. loss:  1.08997411e-02\n",
      "Epoch: 7081 mean train loss:  1.15676956e-02, mean val. rec. loss:  1.08987897e-02\n",
      "Epoch: 7082 mean train loss:  1.15667692e-02, mean val. rec. loss:  1.08978462e-02\n",
      "Epoch: 7083 mean train loss:  1.15658455e-02, mean val. rec. loss:  1.08968937e-02\n",
      "Epoch: 7084 mean train loss:  1.15649238e-02, mean val. rec. loss:  1.08959411e-02\n",
      "Epoch: 7085 mean train loss:  1.15639982e-02, mean val. rec. loss:  1.08949897e-02\n",
      "Epoch: 7086 mean train loss:  1.15630727e-02, mean val. rec. loss:  1.08940406e-02\n",
      "Epoch: 7087 mean train loss:  1.15621510e-02, mean val. rec. loss:  1.08930971e-02\n",
      "Epoch: 7088 mean train loss:  1.15612292e-02, mean val. rec. loss:  1.08921502e-02\n",
      "Epoch: 7089 mean train loss:  1.15603037e-02, mean val. rec. loss:  1.08912000e-02\n",
      "Epoch: 7090 mean train loss:  1.15593800e-02, mean val. rec. loss:  1.08902576e-02\n",
      "Epoch: 7091 mean train loss:  1.15584592e-02, mean val. rec. loss:  1.08893074e-02\n",
      "Epoch: 7092 mean train loss:  1.15575383e-02, mean val. rec. loss:  1.08883594e-02\n",
      "Epoch: 7093 mean train loss:  1.15566137e-02, mean val. rec. loss:  1.08874091e-02\n",
      "Epoch: 7094 mean train loss:  1.15556901e-02, mean val. rec. loss:  1.08864633e-02\n",
      "Epoch: 7095 mean train loss:  1.15547711e-02, mean val. rec. loss:  1.08855199e-02\n",
      "Epoch: 7096 mean train loss:  1.15538502e-02, mean val. rec. loss:  1.08845719e-02\n",
      "Epoch: 7097 mean train loss:  1.15529285e-02, mean val. rec. loss:  1.08836250e-02\n",
      "Epoch: 7098 mean train loss:  1.15520039e-02, mean val. rec. loss:  1.08826872e-02\n",
      "Epoch: 7099 mean train loss:  1.15510877e-02, mean val. rec. loss:  1.08817324e-02\n",
      "Epoch: 7100 mean train loss:  1.15501705e-02, mean val. rec. loss:  1.08807878e-02\n",
      "Epoch: 7101 mean train loss:  1.15492460e-02, mean val. rec. loss:  1.08798387e-02\n",
      "Epoch: 7102 mean train loss:  1.15483223e-02, mean val. rec. loss:  1.08788940e-02\n",
      "Epoch: 7103 mean train loss:  1.15474080e-02, mean val. rec. loss:  1.08779540e-02\n",
      "Epoch: 7104 mean train loss:  1.15464899e-02, mean val. rec. loss:  1.08770014e-02\n",
      "Epoch: 7105 mean train loss:  1.15455663e-02, mean val. rec. loss:  1.08760534e-02\n",
      "Epoch: 7106 mean train loss:  1.15446482e-02, mean val. rec. loss:  1.08751088e-02\n",
      "Epoch: 7107 mean train loss:  1.15437274e-02, mean val. rec. loss:  1.08741642e-02\n",
      "Epoch: 7108 mean train loss:  1.15428140e-02, mean val. rec. loss:  1.08732140e-02\n",
      "Epoch: 7109 mean train loss:  1.15418885e-02, mean val. rec. loss:  1.08722705e-02\n",
      "Epoch: 7110 mean train loss:  1.15409723e-02, mean val. rec. loss:  1.08713270e-02\n",
      "Epoch: 7111 mean train loss:  1.15400551e-02, mean val. rec. loss:  1.08703813e-02\n",
      "Epoch: 7112 mean train loss:  1.15391361e-02, mean val. rec. loss:  1.08694378e-02\n",
      "Epoch: 7113 mean train loss:  1.15382190e-02, mean val. rec. loss:  1.08684921e-02\n",
      "Epoch: 7114 mean train loss:  1.15373000e-02, mean val. rec. loss:  1.08675520e-02\n",
      "Epoch: 7115 mean train loss:  1.15363857e-02, mean val. rec. loss:  1.08665995e-02\n",
      "Epoch: 7116 mean train loss:  1.15354620e-02, mean val. rec. loss:  1.08656617e-02\n",
      "Epoch: 7117 mean train loss:  1.15345524e-02, mean val. rec. loss:  1.08647103e-02\n",
      "Epoch: 7118 mean train loss:  1.15336287e-02, mean val. rec. loss:  1.08637691e-02\n",
      "Epoch: 7119 mean train loss:  1.15327200e-02, mean val. rec. loss:  1.08628177e-02\n",
      "Epoch: 7120 mean train loss:  1.15317954e-02, mean val. rec. loss:  1.08618787e-02\n",
      "Epoch: 7121 mean train loss:  1.15308876e-02, mean val. rec. loss:  1.08609262e-02\n",
      "Epoch: 7122 mean train loss:  1.15299630e-02, mean val. rec. loss:  1.08599873e-02\n",
      "Epoch: 7123 mean train loss:  1.15290561e-02, mean val. rec. loss:  1.08590370e-02\n",
      "Epoch: 7124 mean train loss:  1.15281325e-02, mean val. rec. loss:  1.08581037e-02\n",
      "Epoch: 7125 mean train loss:  1.15272228e-02, mean val. rec. loss:  1.08571580e-02\n",
      "Epoch: 7126 mean train loss:  1.15263066e-02, mean val. rec. loss:  1.08562157e-02\n",
      "Epoch: 7127 mean train loss:  1.15253913e-02, mean val. rec. loss:  1.08552699e-02\n",
      "Epoch: 7128 mean train loss:  1.15244760e-02, mean val. rec. loss:  1.08543287e-02\n",
      "Epoch: 7129 mean train loss:  1.15235645e-02, mean val. rec. loss:  1.08533784e-02\n",
      "Epoch: 7130 mean train loss:  1.15226436e-02, mean val. rec. loss:  1.08524418e-02\n",
      "Epoch: 7131 mean train loss:  1.15217368e-02, mean val. rec. loss:  1.08514949e-02\n",
      "Epoch: 7132 mean train loss:  1.15208215e-02, mean val. rec. loss:  1.08505514e-02\n",
      "Epoch: 7133 mean train loss:  1.15199090e-02, mean val. rec. loss:  1.08496046e-02\n",
      "Epoch: 7134 mean train loss:  1.15189900e-02, mean val. rec. loss:  1.08486634e-02\n",
      "Epoch: 7135 mean train loss:  1.15180822e-02, mean val. rec. loss:  1.08477222e-02\n",
      "Epoch: 7136 mean train loss:  1.15171697e-02, mean val. rec. loss:  1.08467787e-02\n",
      "Epoch: 7137 mean train loss:  1.15162517e-02, mean val. rec. loss:  1.08458409e-02\n",
      "Epoch: 7138 mean train loss:  1.15153392e-02, mean val. rec. loss:  1.08448963e-02\n",
      "Epoch: 7139 mean train loss:  1.15144286e-02, mean val. rec. loss:  1.08439528e-02\n",
      "Epoch: 7140 mean train loss:  1.15135198e-02, mean val. rec. loss:  1.08430128e-02\n",
      "Epoch: 7141 mean train loss:  1.15126074e-02, mean val. rec. loss:  1.08420670e-02\n",
      "Epoch: 7142 mean train loss:  1.15116958e-02, mean val. rec. loss:  1.08411202e-02\n",
      "Epoch: 7143 mean train loss:  1.15107843e-02, mean val. rec. loss:  1.08401756e-02\n",
      "Epoch: 7144 mean train loss:  1.15098737e-02, mean val. rec. loss:  1.08392400e-02\n",
      "Epoch: 7145 mean train loss:  1.15089631e-02, mean val. rec. loss:  1.08383000e-02\n",
      "Epoch: 7146 mean train loss:  1.15080506e-02, mean val. rec. loss:  1.08373554e-02\n",
      "Epoch: 7147 mean train loss:  1.15071409e-02, mean val. rec. loss:  1.08364164e-02\n",
      "Epoch: 7148 mean train loss:  1.15062294e-02, mean val. rec. loss:  1.08354775e-02\n",
      "Epoch: 7149 mean train loss:  1.15053197e-02, mean val. rec. loss:  1.08345306e-02\n",
      "Epoch: 7150 mean train loss:  1.15044091e-02, mean val. rec. loss:  1.08335871e-02\n",
      "Epoch: 7151 mean train loss:  1.15034994e-02, mean val. rec. loss:  1.08326539e-02\n",
      "Epoch: 7152 mean train loss:  1.15025897e-02, mean val. rec. loss:  1.08317172e-02\n",
      "Epoch: 7153 mean train loss:  1.15016800e-02, mean val. rec. loss:  1.08307749e-02\n",
      "Epoch: 7154 mean train loss:  1.15007713e-02, mean val. rec. loss:  1.08298303e-02\n",
      "Epoch: 7155 mean train loss:  1.14998579e-02, mean val. rec. loss:  1.08288891e-02\n",
      "Epoch: 7156 mean train loss:  1.14989501e-02, mean val. rec. loss:  1.08279513e-02\n",
      "Epoch: 7157 mean train loss:  1.14980413e-02, mean val. rec. loss:  1.08270067e-02\n",
      "Epoch: 7158 mean train loss:  1.14971363e-02, mean val. rec. loss:  1.08260700e-02\n",
      "Epoch: 7159 mean train loss:  1.14962257e-02, mean val. rec. loss:  1.08251288e-02\n",
      "Epoch: 7160 mean train loss:  1.14953151e-02, mean val. rec. loss:  1.08241876e-02\n",
      "Epoch: 7161 mean train loss:  1.14944100e-02, mean val. rec. loss:  1.08232464e-02\n",
      "Epoch: 7162 mean train loss:  1.14935050e-02, mean val. rec. loss:  1.08223075e-02\n",
      "Epoch: 7163 mean train loss:  1.14925944e-02, mean val. rec. loss:  1.08213663e-02\n",
      "Epoch: 7164 mean train loss:  1.14916829e-02, mean val. rec. loss:  1.08204296e-02\n",
      "Epoch: 7165 mean train loss:  1.14907797e-02, mean val. rec. loss:  1.08194930e-02\n",
      "Epoch: 7166 mean train loss:  1.14898738e-02, mean val. rec. loss:  1.08185506e-02\n",
      "Epoch: 7167 mean train loss:  1.14889659e-02, mean val. rec. loss:  1.08176106e-02\n",
      "Epoch: 7168 mean train loss:  1.14880600e-02, mean val. rec. loss:  1.08166694e-02\n",
      "Epoch: 7169 mean train loss:  1.14871531e-02, mean val. rec. loss:  1.08157270e-02\n",
      "Epoch: 7170 mean train loss:  1.14862462e-02, mean val. rec. loss:  1.08147915e-02\n",
      "Epoch: 7171 mean train loss:  1.14853412e-02, mean val. rec. loss:  1.08138526e-02\n",
      "Epoch: 7172 mean train loss:  1.14844362e-02, mean val. rec. loss:  1.08129091e-02\n",
      "Epoch: 7173 mean train loss:  1.14835302e-02, mean val. rec. loss:  1.08119713e-02\n",
      "Epoch: 7174 mean train loss:  1.14826242e-02, mean val. rec. loss:  1.08110346e-02\n",
      "Epoch: 7175 mean train loss:  1.14817201e-02, mean val. rec. loss:  1.08100934e-02\n",
      "Epoch: 7176 mean train loss:  1.14808151e-02, mean val. rec. loss:  1.08091556e-02\n",
      "Epoch: 7177 mean train loss:  1.14799101e-02, mean val. rec. loss:  1.08082235e-02\n",
      "Epoch: 7178 mean train loss:  1.14790051e-02, mean val. rec. loss:  1.08072823e-02\n",
      "Epoch: 7179 mean train loss:  1.14781010e-02, mean val. rec. loss:  1.08063411e-02\n",
      "Epoch: 7180 mean train loss:  1.14771969e-02, mean val. rec. loss:  1.08054044e-02\n",
      "Epoch: 7181 mean train loss:  1.14762919e-02, mean val. rec. loss:  1.08044678e-02\n",
      "Epoch: 7182 mean train loss:  1.14753878e-02, mean val. rec. loss:  1.08035311e-02\n",
      "Epoch: 7183 mean train loss:  1.14744902e-02, mean val. rec. loss:  1.08025922e-02\n",
      "Epoch: 7184 mean train loss:  1.14735796e-02, mean val. rec. loss:  1.08016544e-02\n",
      "Epoch: 7185 mean train loss:  1.14726783e-02, mean val. rec. loss:  1.08007154e-02\n",
      "Epoch: 7186 mean train loss:  1.14717733e-02, mean val. rec. loss:  1.07997799e-02\n",
      "Epoch: 7187 mean train loss:  1.14708766e-02, mean val. rec. loss:  1.07988398e-02\n",
      "Epoch: 7188 mean train loss:  1.14699679e-02, mean val. rec. loss:  1.07979054e-02\n",
      "Epoch: 7189 mean train loss:  1.14690675e-02, mean val. rec. loss:  1.07969676e-02\n",
      "Epoch: 7190 mean train loss:  1.14681634e-02, mean val. rec. loss:  1.07960321e-02\n",
      "Epoch: 7191 mean train loss:  1.14672649e-02, mean val. rec. loss:  1.07950920e-02\n",
      "Epoch: 7192 mean train loss:  1.14663589e-02, mean val. rec. loss:  1.07941554e-02\n",
      "Epoch: 7193 mean train loss:  1.14654614e-02, mean val. rec. loss:  1.07932187e-02\n",
      "Epoch: 7194 mean train loss:  1.14645563e-02, mean val. rec. loss:  1.07922843e-02\n",
      "Epoch: 7195 mean train loss:  1.14636578e-02, mean val. rec. loss:  1.07913454e-02\n",
      "Epoch: 7196 mean train loss:  1.14627547e-02, mean val. rec. loss:  1.07904065e-02\n",
      "Epoch: 7197 mean train loss:  1.14618515e-02, mean val. rec. loss:  1.07894755e-02\n",
      "Epoch: 7198 mean train loss:  1.14609549e-02, mean val. rec. loss:  1.07885377e-02\n",
      "Epoch: 7199 mean train loss:  1.14600508e-02, mean val. rec. loss:  1.07876055e-02\n",
      "Epoch: 7200 mean train loss:  1.14591532e-02, mean val. rec. loss:  1.07866711e-02\n",
      "Epoch: 7201 mean train loss:  1.14582500e-02, mean val. rec. loss:  1.07857322e-02\n",
      "Epoch: 7202 mean train loss:  1.14573506e-02, mean val. rec. loss:  1.07847910e-02\n",
      "Epoch: 7203 mean train loss:  1.14564493e-02, mean val. rec. loss:  1.07838566e-02\n",
      "Epoch: 7204 mean train loss:  1.14555554e-02, mean val. rec. loss:  1.07829177e-02\n",
      "Epoch: 7205 mean train loss:  1.14546495e-02, mean val. rec. loss:  1.07819844e-02\n",
      "Epoch: 7206 mean train loss:  1.14537528e-02, mean val. rec. loss:  1.07810557e-02\n",
      "Epoch: 7207 mean train loss:  1.14528515e-02, mean val. rec. loss:  1.07801156e-02\n",
      "Epoch: 7208 mean train loss:  1.14519577e-02, mean val. rec. loss:  1.07791767e-02\n",
      "Epoch: 7209 mean train loss:  1.14510536e-02, mean val. rec. loss:  1.07782366e-02\n",
      "Epoch: 7210 mean train loss:  1.14501551e-02, mean val. rec. loss:  1.07773056e-02\n",
      "Epoch: 7211 mean train loss:  1.14492575e-02, mean val. rec. loss:  1.07763746e-02\n",
      "Epoch: 7212 mean train loss:  1.14483590e-02, mean val. rec. loss:  1.07754448e-02\n",
      "Epoch: 7213 mean train loss:  1.14474624e-02, mean val. rec. loss:  1.07745149e-02\n",
      "Epoch: 7214 mean train loss:  1.14465648e-02, mean val. rec. loss:  1.07735760e-02\n",
      "Epoch: 7215 mean train loss:  1.14456635e-02, mean val. rec. loss:  1.07726348e-02\n",
      "Epoch: 7216 mean train loss:  1.14447696e-02, mean val. rec. loss:  1.07717004e-02\n",
      "Epoch: 7217 mean train loss:  1.14438683e-02, mean val. rec. loss:  1.07707682e-02\n",
      "Epoch: 7218 mean train loss:  1.14429745e-02, mean val. rec. loss:  1.07698304e-02\n",
      "Epoch: 7219 mean train loss:  1.14420732e-02, mean val. rec. loss:  1.07689017e-02\n",
      "Epoch: 7220 mean train loss:  1.14411812e-02, mean val. rec. loss:  1.07679651e-02\n",
      "Epoch: 7221 mean train loss:  1.14402799e-02, mean val. rec. loss:  1.07670352e-02\n",
      "Epoch: 7222 mean train loss:  1.14393879e-02, mean val. rec. loss:  1.07660963e-02\n",
      "Epoch: 7223 mean train loss:  1.14384847e-02, mean val. rec. loss:  1.07651641e-02\n",
      "Epoch: 7224 mean train loss:  1.14375955e-02, mean val. rec. loss:  1.07642286e-02\n",
      "Epoch: 7225 mean train loss:  1.14366989e-02, mean val. rec. loss:  1.07632919e-02\n",
      "Epoch: 7226 mean train loss:  1.14357995e-02, mean val. rec. loss:  1.07623587e-02\n",
      "Epoch: 7227 mean train loss:  1.14349037e-02, mean val. rec. loss:  1.07614265e-02\n",
      "Epoch: 7228 mean train loss:  1.14340071e-02, mean val. rec. loss:  1.07604944e-02\n",
      "Epoch: 7229 mean train loss:  1.14331179e-02, mean val. rec. loss:  1.07595612e-02\n",
      "Epoch: 7230 mean train loss:  1.14322166e-02, mean val. rec. loss:  1.07586234e-02\n",
      "Epoch: 7231 mean train loss:  1.14313237e-02, mean val. rec. loss:  1.07576935e-02\n",
      "Epoch: 7232 mean train loss:  1.14304270e-02, mean val. rec. loss:  1.07567670e-02\n",
      "Epoch: 7233 mean train loss:  1.14295351e-02, mean val. rec. loss:  1.07558315e-02\n",
      "Epoch: 7234 mean train loss:  1.14286440e-02, mean val. rec. loss:  1.07548971e-02\n",
      "Epoch: 7235 mean train loss:  1.14277474e-02, mean val. rec. loss:  1.07539695e-02\n",
      "Epoch: 7236 mean train loss:  1.14268535e-02, mean val. rec. loss:  1.07530306e-02\n",
      "Epoch: 7237 mean train loss:  1.14259596e-02, mean val. rec. loss:  1.07520928e-02\n",
      "Epoch: 7238 mean train loss:  1.14250658e-02, mean val. rec. loss:  1.07511652e-02\n",
      "Epoch: 7239 mean train loss:  1.14241719e-02, mean val. rec. loss:  1.07502376e-02\n",
      "Epoch: 7240 mean train loss:  1.14232772e-02, mean val. rec. loss:  1.07493009e-02\n",
      "Epoch: 7241 mean train loss:  1.14223842e-02, mean val. rec. loss:  1.07483654e-02\n",
      "Epoch: 7242 mean train loss:  1.14214913e-02, mean val. rec. loss:  1.07474367e-02\n",
      "Epoch: 7243 mean train loss:  1.14205975e-02, mean val. rec. loss:  1.07465057e-02\n",
      "Epoch: 7244 mean train loss:  1.14197027e-02, mean val. rec. loss:  1.07455690e-02\n",
      "Epoch: 7245 mean train loss:  1.14188107e-02, mean val. rec. loss:  1.07446403e-02\n",
      "Epoch: 7246 mean train loss:  1.14179168e-02, mean val. rec. loss:  1.07437116e-02\n",
      "Epoch: 7247 mean train loss:  1.14170249e-02, mean val. rec. loss:  1.07427772e-02\n",
      "Epoch: 7248 mean train loss:  1.14161329e-02, mean val. rec. loss:  1.07418518e-02\n",
      "Epoch: 7249 mean train loss:  1.14152409e-02, mean val. rec. loss:  1.07409174e-02\n",
      "Epoch: 7250 mean train loss:  1.14143461e-02, mean val. rec. loss:  1.07399830e-02\n",
      "Epoch: 7251 mean train loss:  1.14134569e-02, mean val. rec. loss:  1.07390532e-02\n",
      "Epoch: 7252 mean train loss:  1.14125668e-02, mean val. rec. loss:  1.07381256e-02\n",
      "Epoch: 7253 mean train loss:  1.14116729e-02, mean val. rec. loss:  1.07371912e-02\n",
      "Epoch: 7254 mean train loss:  1.14107819e-02, mean val. rec. loss:  1.07362523e-02\n",
      "Epoch: 7255 mean train loss:  1.14098908e-02, mean val. rec. loss:  1.07353247e-02\n",
      "Epoch: 7256 mean train loss:  1.14090025e-02, mean val. rec. loss:  1.07343937e-02\n",
      "Epoch: 7257 mean train loss:  1.14081087e-02, mean val. rec. loss:  1.07334650e-02\n",
      "Epoch: 7258 mean train loss:  1.14072158e-02, mean val. rec. loss:  1.07325396e-02\n",
      "Epoch: 7259 mean train loss:  1.14063275e-02, mean val. rec. loss:  1.07316030e-02\n",
      "Epoch: 7260 mean train loss:  1.14054364e-02, mean val. rec. loss:  1.07306686e-02\n",
      "Epoch: 7261 mean train loss:  1.14045473e-02, mean val. rec. loss:  1.07297410e-02\n",
      "Epoch: 7262 mean train loss:  1.14036571e-02, mean val. rec. loss:  1.07288134e-02\n",
      "Epoch: 7263 mean train loss:  1.14027661e-02, mean val. rec. loss:  1.07278835e-02\n",
      "Epoch: 7264 mean train loss:  1.14018815e-02, mean val. rec. loss:  1.07269514e-02\n",
      "Epoch: 7265 mean train loss:  1.14009886e-02, mean val. rec. loss:  1.07260204e-02\n",
      "Epoch: 7266 mean train loss:  1.14000938e-02, mean val. rec. loss:  1.07250871e-02\n",
      "Epoch: 7267 mean train loss:  1.13992093e-02, mean val. rec. loss:  1.07241618e-02\n",
      "Epoch: 7268 mean train loss:  1.13983247e-02, mean val. rec. loss:  1.07232319e-02\n",
      "Epoch: 7269 mean train loss:  1.13974290e-02, mean val. rec. loss:  1.07223010e-02\n",
      "Epoch: 7270 mean train loss:  1.13965417e-02, mean val. rec. loss:  1.07213722e-02\n",
      "Epoch: 7271 mean train loss:  1.13956534e-02, mean val. rec. loss:  1.07204424e-02\n",
      "Epoch: 7272 mean train loss:  1.13947652e-02, mean val. rec. loss:  1.07195125e-02\n",
      "Epoch: 7273 mean train loss:  1.13938806e-02, mean val. rec. loss:  1.07185826e-02\n",
      "Epoch: 7274 mean train loss:  1.13929896e-02, mean val. rec. loss:  1.07176505e-02\n",
      "Epoch: 7275 mean train loss:  1.13921004e-02, mean val. rec. loss:  1.07167229e-02\n",
      "Epoch: 7276 mean train loss:  1.13912158e-02, mean val. rec. loss:  1.07157942e-02\n",
      "Epoch: 7277 mean train loss:  1.13903266e-02, mean val. rec. loss:  1.07148609e-02\n",
      "Epoch: 7278 mean train loss:  1.13894365e-02, mean val. rec. loss:  1.07139322e-02\n",
      "Epoch: 7279 mean train loss:  1.13885520e-02, mean val. rec. loss:  1.07130012e-02\n",
      "Epoch: 7280 mean train loss:  1.13876637e-02, mean val. rec. loss:  1.07120725e-02\n",
      "Epoch: 7281 mean train loss:  1.13867754e-02, mean val. rec. loss:  1.07111460e-02\n",
      "Epoch: 7282 mean train loss:  1.13858890e-02, mean val. rec. loss:  1.07102241e-02\n",
      "Epoch: 7283 mean train loss:  1.13850045e-02, mean val. rec. loss:  1.07092908e-02\n",
      "Epoch: 7284 mean train loss:  1.13841144e-02, mean val. rec. loss:  1.07083576e-02\n",
      "Epoch: 7285 mean train loss:  1.13832233e-02, mean val. rec. loss:  1.07074323e-02\n",
      "Epoch: 7286 mean train loss:  1.13823425e-02, mean val. rec. loss:  1.07065035e-02\n",
      "Epoch: 7287 mean train loss:  1.13814570e-02, mean val. rec. loss:  1.07055725e-02\n",
      "Epoch: 7288 mean train loss:  1.13805669e-02, mean val. rec. loss:  1.07046506e-02\n",
      "Epoch: 7289 mean train loss:  1.13796796e-02, mean val. rec. loss:  1.07037207e-02\n",
      "Epoch: 7290 mean train loss:  1.13787978e-02, mean val. rec. loss:  1.07027932e-02\n",
      "Epoch: 7291 mean train loss:  1.13779133e-02, mean val. rec. loss:  1.07018644e-02\n",
      "Epoch: 7292 mean train loss:  1.13770231e-02, mean val. rec. loss:  1.07009380e-02\n",
      "Epoch: 7293 mean train loss:  1.13761414e-02, mean val. rec. loss:  1.07000081e-02\n",
      "Epoch: 7294 mean train loss:  1.13752541e-02, mean val. rec. loss:  1.06990782e-02\n",
      "Epoch: 7295 mean train loss:  1.13743667e-02, mean val. rec. loss:  1.06981507e-02\n",
      "Epoch: 7296 mean train loss:  1.13734841e-02, mean val. rec. loss:  1.06972287e-02\n",
      "Epoch: 7297 mean train loss:  1.13726004e-02, mean val. rec. loss:  1.06962977e-02\n",
      "Epoch: 7298 mean train loss:  1.13717140e-02, mean val. rec. loss:  1.06953679e-02\n",
      "Epoch: 7299 mean train loss:  1.13708276e-02, mean val. rec. loss:  1.06944403e-02\n",
      "Epoch: 7300 mean train loss:  1.13699422e-02, mean val. rec. loss:  1.06935127e-02\n",
      "Epoch: 7301 mean train loss:  1.13690623e-02, mean val. rec. loss:  1.06925896e-02\n",
      "Epoch: 7302 mean train loss:  1.13681796e-02, mean val. rec. loss:  1.06916620e-02\n",
      "Epoch: 7303 mean train loss:  1.13672913e-02, mean val. rec. loss:  1.06907276e-02\n",
      "Epoch: 7304 mean train loss:  1.13664068e-02, mean val. rec. loss:  1.06898023e-02\n",
      "Epoch: 7305 mean train loss:  1.13655260e-02, mean val. rec. loss:  1.06888747e-02\n",
      "Epoch: 7306 mean train loss:  1.13646433e-02, mean val. rec. loss:  1.06879449e-02\n",
      "Epoch: 7307 mean train loss:  1.13637550e-02, mean val. rec. loss:  1.06870241e-02\n",
      "Epoch: 7308 mean train loss:  1.13628714e-02, mean val. rec. loss:  1.06860942e-02\n",
      "Epoch: 7309 mean train loss:  1.13619915e-02, mean val. rec. loss:  1.06851644e-02\n",
      "Epoch: 7310 mean train loss:  1.13611051e-02, mean val. rec. loss:  1.06842322e-02\n",
      "Epoch: 7311 mean train loss:  1.13602206e-02, mean val. rec. loss:  1.06833046e-02\n",
      "Epoch: 7312 mean train loss:  1.13593398e-02, mean val. rec. loss:  1.06823838e-02\n",
      "Epoch: 7313 mean train loss:  1.13584590e-02, mean val. rec. loss:  1.06814562e-02\n",
      "Epoch: 7314 mean train loss:  1.13575744e-02, mean val. rec. loss:  1.06805264e-02\n",
      "Epoch: 7315 mean train loss:  1.13566899e-02, mean val. rec. loss:  1.06796056e-02\n",
      "Epoch: 7316 mean train loss:  1.13558091e-02, mean val. rec. loss:  1.06786757e-02\n",
      "Epoch: 7317 mean train loss:  1.13549292e-02, mean val. rec. loss:  1.06777549e-02\n",
      "Epoch: 7318 mean train loss:  1.13540474e-02, mean val. rec. loss:  1.06768228e-02\n",
      "Epoch: 7319 mean train loss:  1.13531666e-02, mean val. rec. loss:  1.06758952e-02\n",
      "Epoch: 7320 mean train loss:  1.13522830e-02, mean val. rec. loss:  1.06749710e-02\n",
      "Epoch: 7321 mean train loss:  1.13514013e-02, mean val. rec. loss:  1.06740423e-02\n",
      "Epoch: 7322 mean train loss:  1.13505204e-02, mean val. rec. loss:  1.06731159e-02\n",
      "Epoch: 7323 mean train loss:  1.13496396e-02, mean val. rec. loss:  1.06721985e-02\n",
      "Epoch: 7324 mean train loss:  1.13487570e-02, mean val. rec. loss:  1.06712675e-02\n",
      "Epoch: 7325 mean train loss:  1.13478761e-02, mean val. rec. loss:  1.06703455e-02\n",
      "Epoch: 7326 mean train loss:  1.13469953e-02, mean val. rec. loss:  1.06694112e-02\n",
      "Epoch: 7327 mean train loss:  1.13461126e-02, mean val. rec. loss:  1.06684858e-02\n",
      "Epoch: 7328 mean train loss:  1.13452337e-02, mean val. rec. loss:  1.06675628e-02\n",
      "Epoch: 7329 mean train loss:  1.13443557e-02, mean val. rec. loss:  1.06666386e-02\n",
      "Epoch: 7330 mean train loss:  1.13434721e-02, mean val. rec. loss:  1.06657099e-02\n",
      "Epoch: 7331 mean train loss:  1.13425903e-02, mean val. rec. loss:  1.06647879e-02\n",
      "Epoch: 7332 mean train loss:  1.13417123e-02, mean val. rec. loss:  1.06638592e-02\n",
      "Epoch: 7333 mean train loss:  1.13408333e-02, mean val. rec. loss:  1.06629316e-02\n",
      "Epoch: 7334 mean train loss:  1.13399562e-02, mean val. rec. loss:  1.06620063e-02\n",
      "Epoch: 7335 mean train loss:  1.13390736e-02, mean val. rec. loss:  1.06610821e-02\n",
      "Epoch: 7336 mean train loss:  1.13381927e-02, mean val. rec. loss:  1.06601624e-02\n",
      "Epoch: 7337 mean train loss:  1.13373138e-02, mean val. rec. loss:  1.06592360e-02\n",
      "Epoch: 7338 mean train loss:  1.13364367e-02, mean val. rec. loss:  1.06583084e-02\n",
      "Epoch: 7339 mean train loss:  1.13355559e-02, mean val. rec. loss:  1.06573819e-02\n",
      "Epoch: 7340 mean train loss:  1.13346741e-02, mean val. rec. loss:  1.06564611e-02\n",
      "Epoch: 7341 mean train loss:  1.13337980e-02, mean val. rec. loss:  1.06555381e-02\n",
      "Epoch: 7342 mean train loss:  1.13329209e-02, mean val. rec. loss:  1.06546184e-02\n",
      "Epoch: 7343 mean train loss:  1.13320382e-02, mean val. rec. loss:  1.06536942e-02\n",
      "Epoch: 7344 mean train loss:  1.13311593e-02, mean val. rec. loss:  1.06527666e-02\n",
      "Epoch: 7345 mean train loss:  1.13302794e-02, mean val. rec. loss:  1.06518413e-02\n",
      "Epoch: 7346 mean train loss:  1.13293995e-02, mean val. rec. loss:  1.06509160e-02\n",
      "Epoch: 7347 mean train loss:  1.13285215e-02, mean val. rec. loss:  1.06499861e-02\n",
      "Epoch: 7348 mean train loss:  1.13276462e-02, mean val. rec. loss:  1.06490631e-02\n",
      "Epoch: 7349 mean train loss:  1.13267682e-02, mean val. rec. loss:  1.06481400e-02\n",
      "Epoch: 7350 mean train loss:  1.13258893e-02, mean val. rec. loss:  1.06472192e-02\n",
      "Epoch: 7351 mean train loss:  1.13250122e-02, mean val. rec. loss:  1.06462939e-02\n",
      "Epoch: 7352 mean train loss:  1.13241369e-02, mean val. rec. loss:  1.06453652e-02\n",
      "Epoch: 7353 mean train loss:  1.13232561e-02, mean val. rec. loss:  1.06444387e-02\n",
      "Epoch: 7354 mean train loss:  1.13223762e-02, mean val. rec. loss:  1.06435134e-02\n",
      "Epoch: 7355 mean train loss:  1.13215010e-02, mean val. rec. loss:  1.06425915e-02\n",
      "Epoch: 7356 mean train loss:  1.13206258e-02, mean val. rec. loss:  1.06416695e-02\n",
      "Epoch: 7357 mean train loss:  1.13197477e-02, mean val. rec. loss:  1.06407499e-02\n",
      "Epoch: 7358 mean train loss:  1.13188669e-02, mean val. rec. loss:  1.06398280e-02\n",
      "Epoch: 7359 mean train loss:  1.13179926e-02, mean val. rec. loss:  1.06389026e-02\n",
      "Epoch: 7360 mean train loss:  1.13171174e-02, mean val. rec. loss:  1.06379739e-02\n",
      "Epoch: 7361 mean train loss:  1.13162375e-02, mean val. rec. loss:  1.06370520e-02\n",
      "Epoch: 7362 mean train loss:  1.13153632e-02, mean val. rec. loss:  1.06361312e-02\n",
      "Epoch: 7363 mean train loss:  1.13144843e-02, mean val. rec. loss:  1.06352115e-02\n",
      "Epoch: 7364 mean train loss:  1.13136118e-02, mean val. rec. loss:  1.06342851e-02\n",
      "Epoch: 7365 mean train loss:  1.13127310e-02, mean val. rec. loss:  1.06333632e-02\n",
      "Epoch: 7366 mean train loss:  1.13118576e-02, mean val. rec. loss:  1.06324390e-02\n",
      "Epoch: 7367 mean train loss:  1.13109787e-02, mean val. rec. loss:  1.06315171e-02\n",
      "Epoch: 7368 mean train loss:  1.13101063e-02, mean val. rec. loss:  1.06305906e-02\n",
      "Epoch: 7369 mean train loss:  1.13092292e-02, mean val. rec. loss:  1.06296653e-02\n",
      "Epoch: 7370 mean train loss:  1.13083511e-02, mean val. rec. loss:  1.06287445e-02\n",
      "Epoch: 7371 mean train loss:  1.13074778e-02, mean val. rec. loss:  1.06278226e-02\n",
      "Epoch: 7372 mean train loss:  1.13066035e-02, mean val. rec. loss:  1.06268984e-02\n",
      "Epoch: 7373 mean train loss:  1.13057255e-02, mean val. rec. loss:  1.06259719e-02\n",
      "Epoch: 7374 mean train loss:  1.13048484e-02, mean val. rec. loss:  1.06250500e-02\n",
      "Epoch: 7375 mean train loss:  1.13039731e-02, mean val. rec. loss:  1.06241224e-02\n",
      "Epoch: 7376 mean train loss:  1.13030998e-02, mean val. rec. loss:  1.06232005e-02\n",
      "Epoch: 7377 mean train loss:  1.13022227e-02, mean val. rec. loss:  1.06222785e-02\n",
      "Epoch: 7378 mean train loss:  1.13013474e-02, mean val. rec. loss:  1.06213600e-02\n",
      "Epoch: 7379 mean train loss:  1.13004713e-02, mean val. rec. loss:  1.06204313e-02\n",
      "Epoch: 7380 mean train loss:  1.12995951e-02, mean val. rec. loss:  1.06195139e-02\n",
      "Epoch: 7381 mean train loss:  1.12987236e-02, mean val. rec. loss:  1.06185931e-02\n",
      "Epoch: 7382 mean train loss:  1.12978503e-02, mean val. rec. loss:  1.06176701e-02\n",
      "Epoch: 7383 mean train loss:  1.12969760e-02, mean val. rec. loss:  1.06167481e-02\n",
      "Epoch: 7384 mean train loss:  1.12961026e-02, mean val. rec. loss:  1.06158285e-02\n",
      "Epoch: 7385 mean train loss:  1.12952255e-02, mean val. rec. loss:  1.06149009e-02\n",
      "Epoch: 7386 mean train loss:  1.12943484e-02, mean val. rec. loss:  1.06139733e-02\n",
      "Epoch: 7387 mean train loss:  1.12934778e-02, mean val. rec. loss:  1.06130548e-02\n",
      "Epoch: 7388 mean train loss:  1.12926054e-02, mean val. rec. loss:  1.06121340e-02\n",
      "Epoch: 7389 mean train loss:  1.12917292e-02, mean val. rec. loss:  1.06112087e-02\n",
      "Epoch: 7390 mean train loss:  1.12908531e-02, mean val. rec. loss:  1.06102970e-02\n",
      "Epoch: 7391 mean train loss:  1.12899844e-02, mean val. rec. loss:  1.06093705e-02\n",
      "Epoch: 7392 mean train loss:  1.12891138e-02, mean val. rec. loss:  1.06084497e-02\n",
      "Epoch: 7393 mean train loss:  1.12882376e-02, mean val. rec. loss:  1.06075244e-02\n",
      "Epoch: 7394 mean train loss:  1.12873624e-02, mean val. rec. loss:  1.06066070e-02\n",
      "Epoch: 7395 mean train loss:  1.12864909e-02, mean val. rec. loss:  1.06056885e-02\n",
      "Epoch: 7396 mean train loss:  1.12856194e-02, mean val. rec. loss:  1.06047586e-02\n",
      "Epoch: 7397 mean train loss:  1.12847460e-02, mean val. rec. loss:  1.06038401e-02\n",
      "Epoch: 7398 mean train loss:  1.12838708e-02, mean val. rec. loss:  1.06029216e-02\n",
      "Epoch: 7399 mean train loss:  1.12830030e-02, mean val. rec. loss:  1.06020030e-02\n",
      "Epoch: 7400 mean train loss:  1.12821287e-02, mean val. rec. loss:  1.06010800e-02\n",
      "Epoch: 7401 mean train loss:  1.12812525e-02, mean val. rec. loss:  1.06001524e-02\n",
      "Epoch: 7402 mean train loss:  1.12803810e-02, mean val. rec. loss:  1.05992361e-02\n",
      "Epoch: 7403 mean train loss:  1.12795123e-02, mean val. rec. loss:  1.05983131e-02\n",
      "Epoch: 7404 mean train loss:  1.12786399e-02, mean val. rec. loss:  1.05973889e-02\n",
      "Epoch: 7405 mean train loss:  1.12777647e-02, mean val. rec. loss:  1.05964738e-02\n",
      "Epoch: 7406 mean train loss:  1.12768932e-02, mean val. rec. loss:  1.05955553e-02\n",
      "Epoch: 7407 mean train loss:  1.12760244e-02, mean val. rec. loss:  1.05946322e-02\n",
      "Epoch: 7408 mean train loss:  1.12751539e-02, mean val. rec. loss:  1.05937137e-02\n",
      "Epoch: 7409 mean train loss:  1.12742796e-02, mean val. rec. loss:  1.05927906e-02\n",
      "Epoch: 7410 mean train loss:  1.12734071e-02, mean val. rec. loss:  1.05918676e-02\n",
      "Epoch: 7411 mean train loss:  1.12725375e-02, mean val. rec. loss:  1.05909490e-02\n",
      "Epoch: 7412 mean train loss:  1.12716679e-02, mean val. rec. loss:  1.05900249e-02\n",
      "Epoch: 7413 mean train loss:  1.12707945e-02, mean val. rec. loss:  1.05891052e-02\n",
      "Epoch: 7414 mean train loss:  1.12699211e-02, mean val. rec. loss:  1.05881844e-02\n",
      "Epoch: 7415 mean train loss:  1.12690496e-02, mean val. rec. loss:  1.05872613e-02\n",
      "Epoch: 7416 mean train loss:  1.12681790e-02, mean val. rec. loss:  1.05863428e-02\n",
      "Epoch: 7417 mean train loss:  1.12673085e-02, mean val. rec. loss:  1.05854175e-02\n",
      "Epoch: 7418 mean train loss:  1.12664379e-02, mean val. rec. loss:  1.05845013e-02\n",
      "Epoch: 7419 mean train loss:  1.12655673e-02, mean val. rec. loss:  1.05835782e-02\n",
      "Epoch: 7420 mean train loss:  1.12646958e-02, mean val. rec. loss:  1.05826642e-02\n",
      "Epoch: 7421 mean train loss:  1.12638234e-02, mean val. rec. loss:  1.05817412e-02\n",
      "Epoch: 7422 mean train loss:  1.12629565e-02, mean val. rec. loss:  1.05808170e-02\n",
      "Epoch: 7423 mean train loss:  1.12620832e-02, mean val. rec. loss:  1.05799030e-02\n",
      "Epoch: 7424 mean train loss:  1.12612163e-02, mean val. rec. loss:  1.05789822e-02\n",
      "Epoch: 7425 mean train loss:  1.12603411e-02, mean val. rec. loss:  1.05780671e-02\n",
      "Epoch: 7426 mean train loss:  1.12594761e-02, mean val. rec. loss:  1.05771417e-02\n",
      "Epoch: 7427 mean train loss:  1.12586018e-02, mean val. rec. loss:  1.05762255e-02\n",
      "Epoch: 7428 mean train loss:  1.12577349e-02, mean val. rec. loss:  1.05753024e-02\n",
      "Epoch: 7429 mean train loss:  1.12568634e-02, mean val. rec. loss:  1.05743839e-02\n",
      "Epoch: 7430 mean train loss:  1.12559957e-02, mean val. rec. loss:  1.05734609e-02\n",
      "Epoch: 7431 mean train loss:  1.12551195e-02, mean val. rec. loss:  1.05725423e-02\n",
      "Epoch: 7432 mean train loss:  1.12542564e-02, mean val. rec. loss:  1.05716238e-02\n",
      "Epoch: 7433 mean train loss:  1.12533858e-02, mean val. rec. loss:  1.05707064e-02\n",
      "Epoch: 7434 mean train loss:  1.12525171e-02, mean val. rec. loss:  1.05697845e-02\n",
      "Epoch: 7435 mean train loss:  1.12516419e-02, mean val. rec. loss:  1.05688682e-02\n",
      "Epoch: 7436 mean train loss:  1.12507787e-02, mean val. rec. loss:  1.05679475e-02\n",
      "Epoch: 7437 mean train loss:  1.12499063e-02, mean val. rec. loss:  1.05670255e-02\n",
      "Epoch: 7438 mean train loss:  1.12490413e-02, mean val. rec. loss:  1.05661059e-02\n",
      "Epoch: 7439 mean train loss:  1.12481661e-02, mean val. rec. loss:  1.05651896e-02\n",
      "Epoch: 7440 mean train loss:  1.12473020e-02, mean val. rec. loss:  1.05642688e-02\n",
      "Epoch: 7441 mean train loss:  1.12464296e-02, mean val. rec. loss:  1.05633503e-02\n",
      "Epoch: 7442 mean train loss:  1.12455646e-02, mean val. rec. loss:  1.05624318e-02\n",
      "Epoch: 7443 mean train loss:  1.12446922e-02, mean val. rec. loss:  1.05615133e-02\n",
      "Epoch: 7444 mean train loss:  1.12438262e-02, mean val. rec. loss:  1.05605925e-02\n",
      "Epoch: 7445 mean train loss:  1.12429557e-02, mean val. rec. loss:  1.05596740e-02\n",
      "Epoch: 7446 mean train loss:  1.12420935e-02, mean val. rec. loss:  1.05587554e-02\n",
      "Epoch: 7447 mean train loss:  1.12412192e-02, mean val. rec. loss:  1.05578347e-02\n",
      "Epoch: 7448 mean train loss:  1.12403514e-02, mean val. rec. loss:  1.05569161e-02\n",
      "Epoch: 7449 mean train loss:  1.12394827e-02, mean val. rec. loss:  1.05559999e-02\n",
      "Epoch: 7450 mean train loss:  1.12386205e-02, mean val. rec. loss:  1.05550791e-02\n",
      "Epoch: 7451 mean train loss:  1.12377462e-02, mean val. rec. loss:  1.05541640e-02\n",
      "Epoch: 7452 mean train loss:  1.12368831e-02, mean val. rec. loss:  1.05532398e-02\n",
      "Epoch: 7453 mean train loss:  1.12360106e-02, mean val. rec. loss:  1.05523190e-02\n",
      "Epoch: 7454 mean train loss:  1.12351466e-02, mean val. rec. loss:  1.05513982e-02\n",
      "Epoch: 7455 mean train loss:  1.12342751e-02, mean val. rec. loss:  1.05504865e-02\n",
      "Epoch: 7456 mean train loss:  1.12334129e-02, mean val. rec. loss:  1.05495646e-02\n",
      "Epoch: 7457 mean train loss:  1.12325395e-02, mean val. rec. loss:  1.05486472e-02\n",
      "Epoch: 7458 mean train loss:  1.12316792e-02, mean val. rec. loss:  1.05477241e-02\n",
      "Epoch: 7459 mean train loss:  1.12308058e-02, mean val. rec. loss:  1.05468011e-02\n",
      "Epoch: 7460 mean train loss:  1.12299390e-02, mean val. rec. loss:  1.05458859e-02\n",
      "Epoch: 7461 mean train loss:  1.12290730e-02, mean val. rec. loss:  1.05449697e-02\n",
      "Epoch: 7462 mean train loss:  1.12282062e-02, mean val. rec. loss:  1.05440523e-02\n",
      "Epoch: 7463 mean train loss:  1.12273393e-02, mean val. rec. loss:  1.05431361e-02\n",
      "Epoch: 7464 mean train loss:  1.12264725e-02, mean val. rec. loss:  1.05422153e-02\n",
      "Epoch: 7465 mean train loss:  1.12256028e-02, mean val. rec. loss:  1.05412945e-02\n",
      "Epoch: 7466 mean train loss:  1.12247351e-02, mean val. rec. loss:  1.05403816e-02\n",
      "Epoch: 7467 mean train loss:  1.12238710e-02, mean val. rec. loss:  1.05394620e-02\n",
      "Epoch: 7468 mean train loss:  1.12230051e-02, mean val. rec. loss:  1.05385434e-02\n",
      "Epoch: 7469 mean train loss:  1.12221354e-02, mean val. rec. loss:  1.05376261e-02\n",
      "Epoch: 7470 mean train loss:  1.12212686e-02, mean val. rec. loss:  1.05367064e-02\n",
      "Epoch: 7471 mean train loss:  1.12204036e-02, mean val. rec. loss:  1.05357924e-02\n",
      "Epoch: 7472 mean train loss:  1.12195414e-02, mean val. rec. loss:  1.05348739e-02\n",
      "Epoch: 7473 mean train loss:  1.12186755e-02, mean val. rec. loss:  1.05339554e-02\n",
      "Epoch: 7474 mean train loss:  1.12178059e-02, mean val. rec. loss:  1.05330391e-02\n",
      "Epoch: 7475 mean train loss:  1.12169399e-02, mean val. rec. loss:  1.05321183e-02\n",
      "Epoch: 7476 mean train loss:  1.12160777e-02, mean val. rec. loss:  1.05312009e-02\n",
      "Epoch: 7477 mean train loss:  1.12152128e-02, mean val. rec. loss:  1.05302824e-02\n",
      "Epoch: 7478 mean train loss:  1.12143422e-02, mean val. rec. loss:  1.05293616e-02\n",
      "Epoch: 7479 mean train loss:  1.12134763e-02, mean val. rec. loss:  1.05284488e-02\n",
      "Epoch: 7480 mean train loss:  1.12126178e-02, mean val. rec. loss:  1.05275269e-02\n",
      "Epoch: 7481 mean train loss:  1.12117444e-02, mean val. rec. loss:  1.05266106e-02\n",
      "Epoch: 7482 mean train loss:  1.12108813e-02, mean val. rec. loss:  1.05256955e-02\n",
      "Epoch: 7483 mean train loss:  1.12100135e-02, mean val. rec. loss:  1.05247781e-02\n",
      "Epoch: 7484 mean train loss:  1.12091541e-02, mean val. rec. loss:  1.05238573e-02\n",
      "Epoch: 7485 mean train loss:  1.12082826e-02, mean val. rec. loss:  1.05229445e-02\n",
      "Epoch: 7486 mean train loss:  1.12074223e-02, mean val. rec. loss:  1.05220248e-02\n",
      "Epoch: 7487 mean train loss:  1.12065545e-02, mean val. rec. loss:  1.05211074e-02\n",
      "Epoch: 7488 mean train loss:  1.12056867e-02, mean val. rec. loss:  1.05201968e-02\n",
      "Epoch: 7489 mean train loss:  1.12048255e-02, mean val. rec. loss:  1.05192783e-02\n",
      "Epoch: 7490 mean train loss:  1.12039623e-02, mean val. rec. loss:  1.05183621e-02\n",
      "Epoch: 7491 mean train loss:  1.12030945e-02, mean val. rec. loss:  1.05174458e-02\n",
      "Epoch: 7492 mean train loss:  1.12022286e-02, mean val. rec. loss:  1.05165296e-02\n",
      "Epoch: 7493 mean train loss:  1.12013664e-02, mean val. rec. loss:  1.05156122e-02\n",
      "Epoch: 7494 mean train loss:  1.12005005e-02, mean val. rec. loss:  1.05146982e-02\n",
      "Epoch: 7495 mean train loss:  1.11996365e-02, mean val. rec. loss:  1.05137797e-02\n",
      "Epoch: 7496 mean train loss:  1.11987724e-02, mean val. rec. loss:  1.05128634e-02\n",
      "Epoch: 7497 mean train loss:  1.11979065e-02, mean val. rec. loss:  1.05119426e-02\n",
      "Epoch: 7498 mean train loss:  1.11970406e-02, mean val. rec. loss:  1.05110286e-02\n",
      "Epoch: 7499 mean train loss:  1.11961765e-02, mean val. rec. loss:  1.05101101e-02\n",
      "Epoch: 7500 mean train loss:  1.11953125e-02, mean val. rec. loss:  1.05091893e-02\n",
      "Epoch: 7501 mean train loss:  1.11944465e-02, mean val. rec. loss:  1.05082765e-02\n",
      "Epoch: 7502 mean train loss:  1.11935843e-02, mean val. rec. loss:  1.05073625e-02\n",
      "Epoch: 7503 mean train loss:  1.11927221e-02, mean val. rec. loss:  1.05064451e-02\n",
      "Epoch: 7504 mean train loss:  1.11918562e-02, mean val. rec. loss:  1.05055243e-02\n",
      "Epoch: 7505 mean train loss:  1.11909903e-02, mean val. rec. loss:  1.05046081e-02\n",
      "Epoch: 7506 mean train loss:  1.11901281e-02, mean val. rec. loss:  1.05036952e-02\n",
      "Epoch: 7507 mean train loss:  1.11892678e-02, mean val. rec. loss:  1.05027756e-02\n",
      "Epoch: 7508 mean train loss:  1.11883991e-02, mean val. rec. loss:  1.05018650e-02\n",
      "Epoch: 7509 mean train loss:  1.11875397e-02, mean val. rec. loss:  1.05009465e-02\n",
      "Epoch: 7510 mean train loss:  1.11866719e-02, mean val. rec. loss:  1.05000279e-02\n",
      "Epoch: 7511 mean train loss:  1.11858125e-02, mean val. rec. loss:  1.04991083e-02\n",
      "Epoch: 7512 mean train loss:  1.11849456e-02, mean val. rec. loss:  1.04981943e-02\n",
      "Epoch: 7513 mean train loss:  1.11840853e-02, mean val. rec. loss:  1.04972803e-02\n",
      "Epoch: 7514 mean train loss:  1.11832185e-02, mean val. rec. loss:  1.04963663e-02\n",
      "Epoch: 7515 mean train loss:  1.11823581e-02, mean val. rec. loss:  1.04954489e-02\n",
      "Epoch: 7516 mean train loss:  1.11814959e-02, mean val. rec. loss:  1.04945338e-02\n",
      "Epoch: 7517 mean train loss:  1.11806309e-02, mean val. rec. loss:  1.04936153e-02\n",
      "Epoch: 7518 mean train loss:  1.11797688e-02, mean val. rec. loss:  1.04927013e-02\n",
      "Epoch: 7519 mean train loss:  1.11789084e-02, mean val. rec. loss:  1.04917919e-02\n",
      "Epoch: 7520 mean train loss:  1.11780444e-02, mean val. rec. loss:  1.04908711e-02\n",
      "Epoch: 7521 mean train loss:  1.11771784e-02, mean val. rec. loss:  1.04899571e-02\n",
      "Epoch: 7522 mean train loss:  1.11763200e-02, mean val. rec. loss:  1.04890397e-02\n",
      "Epoch: 7523 mean train loss:  1.11754578e-02, mean val. rec. loss:  1.04881200e-02\n",
      "Epoch: 7524 mean train loss:  1.11745937e-02, mean val. rec. loss:  1.04872072e-02\n",
      "Epoch: 7525 mean train loss:  1.11737297e-02, mean val. rec. loss:  1.04862921e-02\n",
      "Epoch: 7526 mean train loss:  1.11728693e-02, mean val. rec. loss:  1.04853747e-02\n",
      "Epoch: 7527 mean train loss:  1.11720053e-02, mean val. rec. loss:  1.04844686e-02\n",
      "Epoch: 7528 mean train loss:  1.11711431e-02, mean val. rec. loss:  1.04835433e-02\n",
      "Epoch: 7529 mean train loss:  1.11702809e-02, mean val. rec. loss:  1.04826293e-02\n",
      "Epoch: 7530 mean train loss:  1.11694187e-02, mean val. rec. loss:  1.04817097e-02\n",
      "Epoch: 7531 mean train loss:  1.11685565e-02, mean val. rec. loss:  1.04807889e-02\n",
      "Epoch: 7532 mean train loss:  1.11676952e-02, mean val. rec. loss:  1.04798783e-02\n",
      "Epoch: 7533 mean train loss:  1.11668330e-02, mean val. rec. loss:  1.04789700e-02\n",
      "Epoch: 7534 mean train loss:  1.11659699e-02, mean val. rec. loss:  1.04780571e-02\n",
      "Epoch: 7535 mean train loss:  1.11651114e-02, mean val. rec. loss:  1.04771409e-02\n",
      "Epoch: 7536 mean train loss:  1.11642474e-02, mean val. rec. loss:  1.04762133e-02\n",
      "Epoch: 7537 mean train loss:  1.11633833e-02, mean val. rec. loss:  1.04753004e-02\n",
      "Epoch: 7538 mean train loss:  1.11625230e-02, mean val. rec. loss:  1.04743876e-02\n",
      "Epoch: 7539 mean train loss:  1.11616627e-02, mean val. rec. loss:  1.04734713e-02\n",
      "Epoch: 7540 mean train loss:  1.11608005e-02, mean val. rec. loss:  1.04725573e-02\n",
      "Epoch: 7541 mean train loss:  1.11599364e-02, mean val. rec. loss:  1.04716456e-02\n",
      "Epoch: 7542 mean train loss:  1.11590752e-02, mean val. rec. loss:  1.04707260e-02\n",
      "Epoch: 7543 mean train loss:  1.11582176e-02, mean val. rec. loss:  1.04698086e-02\n",
      "Epoch: 7544 mean train loss:  1.11573545e-02, mean val. rec. loss:  1.04689048e-02\n",
      "Epoch: 7545 mean train loss:  1.11564951e-02, mean val. rec. loss:  1.04679840e-02\n",
      "Epoch: 7546 mean train loss:  1.11556282e-02, mean val. rec. loss:  1.04670757e-02\n",
      "Epoch: 7547 mean train loss:  1.11547726e-02, mean val. rec. loss:  1.04661572e-02\n",
      "Epoch: 7548 mean train loss:  1.11539085e-02, mean val. rec. loss:  1.04652455e-02\n",
      "Epoch: 7549 mean train loss:  1.11530528e-02, mean val. rec. loss:  1.04643281e-02\n",
      "Epoch: 7550 mean train loss:  1.11521878e-02, mean val. rec. loss:  1.04634175e-02\n",
      "Epoch: 7551 mean train loss:  1.11513303e-02, mean val. rec. loss:  1.04625024e-02\n",
      "Epoch: 7552 mean train loss:  1.11504653e-02, mean val. rec. loss:  1.04615907e-02\n",
      "Epoch: 7553 mean train loss:  1.11496087e-02, mean val. rec. loss:  1.04606721e-02\n",
      "Epoch: 7554 mean train loss:  1.11487447e-02, mean val. rec. loss:  1.04597593e-02\n",
      "Epoch: 7555 mean train loss:  1.11478862e-02, mean val. rec. loss:  1.04588408e-02\n",
      "Epoch: 7556 mean train loss:  1.11470193e-02, mean val. rec. loss:  1.04579336e-02\n",
      "Epoch: 7557 mean train loss:  1.11461646e-02, mean val. rec. loss:  1.04570139e-02\n",
      "Epoch: 7558 mean train loss:  1.11453024e-02, mean val. rec. loss:  1.04561033e-02\n",
      "Epoch: 7559 mean train loss:  1.11444449e-02, mean val. rec. loss:  1.04551860e-02\n",
      "Epoch: 7560 mean train loss:  1.11435771e-02, mean val. rec. loss:  1.04542742e-02\n",
      "Epoch: 7561 mean train loss:  1.11427233e-02, mean val. rec. loss:  1.04533580e-02\n",
      "Epoch: 7562 mean train loss:  1.11418592e-02, mean val. rec. loss:  1.04524451e-02\n",
      "Epoch: 7563 mean train loss:  1.11410017e-02, mean val. rec. loss:  1.04515289e-02\n",
      "Epoch: 7564 mean train loss:  1.11401385e-02, mean val. rec. loss:  1.04506194e-02\n",
      "Epoch: 7565 mean train loss:  1.11392829e-02, mean val. rec. loss:  1.04497009e-02\n",
      "Epoch: 7566 mean train loss:  1.11384188e-02, mean val. rec. loss:  1.04487892e-02\n",
      "Epoch: 7567 mean train loss:  1.11375622e-02, mean val. rec. loss:  1.04478695e-02\n",
      "Epoch: 7568 mean train loss:  1.11366972e-02, mean val. rec. loss:  1.04469556e-02\n",
      "Epoch: 7569 mean train loss:  1.11358406e-02, mean val. rec. loss:  1.04460529e-02\n",
      "Epoch: 7570 mean train loss:  1.11349868e-02, mean val. rec. loss:  1.04451367e-02\n",
      "Epoch: 7571 mean train loss:  1.11341218e-02, mean val. rec. loss:  1.04442193e-02\n",
      "Epoch: 7572 mean train loss:  1.11332577e-02, mean val. rec. loss:  1.04433132e-02\n",
      "Epoch: 7573 mean train loss:  1.11324021e-02, mean val. rec. loss:  1.04423958e-02\n",
      "Epoch: 7574 mean train loss:  1.11315473e-02, mean val. rec. loss:  1.04414841e-02\n",
      "Epoch: 7575 mean train loss:  1.11306861e-02, mean val. rec. loss:  1.04405701e-02\n",
      "Epoch: 7576 mean train loss:  1.11298248e-02, mean val. rec. loss:  1.04396584e-02\n",
      "Epoch: 7577 mean train loss:  1.11289673e-02, mean val. rec. loss:  1.04387490e-02\n",
      "Epoch: 7578 mean train loss:  1.11281088e-02, mean val. rec. loss:  1.04378327e-02\n",
      "Epoch: 7579 mean train loss:  1.11272475e-02, mean val. rec. loss:  1.04369153e-02\n",
      "Epoch: 7580 mean train loss:  1.11263872e-02, mean val. rec. loss:  1.04360070e-02\n",
      "Epoch: 7581 mean train loss:  1.11255297e-02, mean val. rec. loss:  1.04350998e-02\n",
      "Epoch: 7582 mean train loss:  1.11246731e-02, mean val. rec. loss:  1.04341858e-02\n",
      "Epoch: 7583 mean train loss:  1.11238099e-02, mean val. rec. loss:  1.04332639e-02\n",
      "Epoch: 7584 mean train loss:  1.11229468e-02, mean val. rec. loss:  1.04323465e-02\n",
      "Epoch: 7585 mean train loss:  1.11220930e-02, mean val. rec. loss:  1.04314405e-02\n",
      "Epoch: 7586 mean train loss:  1.11212364e-02, mean val. rec. loss:  1.04305265e-02\n",
      "Epoch: 7587 mean train loss:  1.11203742e-02, mean val. rec. loss:  1.04296182e-02\n",
      "Epoch: 7588 mean train loss:  1.11195139e-02, mean val. rec. loss:  1.04287087e-02\n",
      "Epoch: 7589 mean train loss:  1.11186572e-02, mean val. rec. loss:  1.04277857e-02\n",
      "Epoch: 7590 mean train loss:  1.11178006e-02, mean val. rec. loss:  1.04268728e-02\n",
      "Epoch: 7591 mean train loss:  1.11169403e-02, mean val. rec. loss:  1.04259566e-02\n",
      "Epoch: 7592 mean train loss:  1.11160781e-02, mean val. rec. loss:  1.04250437e-02\n",
      "Epoch: 7593 mean train loss:  1.11152234e-02, mean val. rec. loss:  1.04241411e-02\n",
      "Epoch: 7594 mean train loss:  1.11143649e-02, mean val. rec. loss:  1.04232226e-02\n",
      "Epoch: 7595 mean train loss:  1.11135027e-02, mean val. rec. loss:  1.04223154e-02\n",
      "Epoch: 7596 mean train loss:  1.11126480e-02, mean val. rec. loss:  1.04213991e-02\n",
      "Epoch: 7597 mean train loss:  1.11117923e-02, mean val. rec. loss:  1.04204919e-02\n",
      "Epoch: 7598 mean train loss:  1.11109310e-02, mean val. rec. loss:  1.04195712e-02\n",
      "Epoch: 7599 mean train loss:  1.11100707e-02, mean val. rec. loss:  1.04186708e-02\n",
      "Epoch: 7600 mean train loss:  1.11092150e-02, mean val. rec. loss:  1.04177500e-02\n",
      "Epoch: 7601 mean train loss:  1.11083565e-02, mean val. rec. loss:  1.04168428e-02\n",
      "Epoch: 7602 mean train loss:  1.11074990e-02, mean val. rec. loss:  1.04159220e-02\n",
      "Epoch: 7603 mean train loss:  1.11066415e-02, mean val. rec. loss:  1.04150148e-02\n",
      "Epoch: 7604 mean train loss:  1.11057821e-02, mean val. rec. loss:  1.04140975e-02\n",
      "Epoch: 7605 mean train loss:  1.11049255e-02, mean val. rec. loss:  1.04131948e-02\n",
      "Epoch: 7606 mean train loss:  1.11040670e-02, mean val. rec. loss:  1.04122774e-02\n",
      "Epoch: 7607 mean train loss:  1.11032094e-02, mean val. rec. loss:  1.04113691e-02\n",
      "Epoch: 7608 mean train loss:  1.11023500e-02, mean val. rec. loss:  1.04104460e-02\n",
      "Epoch: 7609 mean train loss:  1.11014953e-02, mean val. rec. loss:  1.04095434e-02\n",
      "Epoch: 7610 mean train loss:  1.11006387e-02, mean val. rec. loss:  1.04086215e-02\n",
      "Epoch: 7611 mean train loss:  1.10997793e-02, mean val. rec. loss:  1.04077177e-02\n",
      "Epoch: 7612 mean train loss:  1.10989190e-02, mean val. rec. loss:  1.04068049e-02\n",
      "Epoch: 7613 mean train loss:  1.10980651e-02, mean val. rec. loss:  1.04058965e-02\n",
      "Epoch: 7614 mean train loss:  1.10972067e-02, mean val. rec. loss:  1.04049701e-02\n",
      "Epoch: 7615 mean train loss:  1.10963501e-02, mean val. rec. loss:  1.04040652e-02\n",
      "Epoch: 7616 mean train loss:  1.10954916e-02, mean val. rec. loss:  1.04031546e-02\n",
      "Epoch: 7617 mean train loss:  1.10946331e-02, mean val. rec. loss:  1.04022440e-02\n",
      "Epoch: 7618 mean train loss:  1.10937784e-02, mean val. rec. loss:  1.04013323e-02\n",
      "Epoch: 7619 mean train loss:  1.10929236e-02, mean val. rec. loss:  1.04004206e-02\n",
      "Epoch: 7620 mean train loss:  1.10920642e-02, mean val. rec. loss:  1.03995088e-02\n",
      "Epoch: 7621 mean train loss:  1.10912076e-02, mean val. rec. loss:  1.03985994e-02\n",
      "Epoch: 7622 mean train loss:  1.10903501e-02, mean val. rec. loss:  1.03976854e-02\n",
      "Epoch: 7623 mean train loss:  1.10894935e-02, mean val. rec. loss:  1.03967828e-02\n",
      "Epoch: 7624 mean train loss:  1.10886369e-02, mean val. rec. loss:  1.03958620e-02\n",
      "Epoch: 7625 mean train loss:  1.10877793e-02, mean val. rec. loss:  1.03949548e-02\n",
      "Epoch: 7626 mean train loss:  1.10869237e-02, mean val. rec. loss:  1.03940431e-02\n",
      "Epoch: 7627 mean train loss:  1.10860652e-02, mean val. rec. loss:  1.03931314e-02\n",
      "Epoch: 7628 mean train loss:  1.10852095e-02, mean val. rec. loss:  1.03922196e-02\n",
      "Epoch: 7629 mean train loss:  1.10843510e-02, mean val. rec. loss:  1.03913113e-02\n",
      "Epoch: 7630 mean train loss:  1.10834963e-02, mean val. rec. loss:  1.03904007e-02\n",
      "Epoch: 7631 mean train loss:  1.10826415e-02, mean val. rec. loss:  1.03894913e-02\n",
      "Epoch: 7632 mean train loss:  1.10817831e-02, mean val. rec. loss:  1.03885773e-02\n",
      "Epoch: 7633 mean train loss:  1.10809255e-02, mean val. rec. loss:  1.03876679e-02\n",
      "Epoch: 7634 mean train loss:  1.10800708e-02, mean val. rec. loss:  1.03867539e-02\n",
      "Epoch: 7635 mean train loss:  1.10792142e-02, mean val. rec. loss:  1.03858388e-02\n",
      "Epoch: 7636 mean train loss:  1.10783594e-02, mean val. rec. loss:  1.03849304e-02\n",
      "Epoch: 7637 mean train loss:  1.10775019e-02, mean val. rec. loss:  1.03840233e-02\n",
      "Epoch: 7638 mean train loss:  1.10766453e-02, mean val. rec. loss:  1.03831115e-02\n",
      "Epoch: 7639 mean train loss:  1.10757887e-02, mean val. rec. loss:  1.03822066e-02\n",
      "Epoch: 7640 mean train loss:  1.10749330e-02, mean val. rec. loss:  1.03812881e-02\n",
      "Epoch: 7641 mean train loss:  1.10740783e-02, mean val. rec. loss:  1.03803764e-02\n",
      "Epoch: 7642 mean train loss:  1.10732217e-02, mean val. rec. loss:  1.03794715e-02\n",
      "Epoch: 7643 mean train loss:  1.10723669e-02, mean val. rec. loss:  1.03785643e-02\n",
      "Epoch: 7644 mean train loss:  1.10715103e-02, mean val. rec. loss:  1.03776526e-02\n",
      "Epoch: 7645 mean train loss:  1.10706546e-02, mean val. rec. loss:  1.03767341e-02\n",
      "Epoch: 7646 mean train loss:  1.10697990e-02, mean val. rec. loss:  1.03758280e-02\n",
      "Epoch: 7647 mean train loss:  1.10689405e-02, mean val. rec. loss:  1.03749242e-02\n",
      "Epoch: 7648 mean train loss:  1.10680876e-02, mean val. rec. loss:  1.03740136e-02\n",
      "Epoch: 7649 mean train loss:  1.10672347e-02, mean val. rec. loss:  1.03731008e-02\n",
      "Epoch: 7650 mean train loss:  1.10663781e-02, mean val. rec. loss:  1.03721891e-02\n",
      "Epoch: 7651 mean train loss:  1.10655196e-02, mean val. rec. loss:  1.03712819e-02\n",
      "Epoch: 7652 mean train loss:  1.10646668e-02, mean val. rec. loss:  1.03703702e-02\n",
      "Epoch: 7653 mean train loss:  1.10638139e-02, mean val. rec. loss:  1.03694585e-02\n",
      "Epoch: 7654 mean train loss:  1.10629582e-02, mean val. rec. loss:  1.03685501e-02\n",
      "Epoch: 7655 mean train loss:  1.10621035e-02, mean val. rec. loss:  1.03676464e-02\n",
      "Epoch: 7656 mean train loss:  1.10612478e-02, mean val. rec. loss:  1.03667290e-02\n",
      "Epoch: 7657 mean train loss:  1.10603930e-02, mean val. rec. loss:  1.03658139e-02\n",
      "Epoch: 7658 mean train loss:  1.10595374e-02, mean val. rec. loss:  1.03649090e-02\n",
      "Epoch: 7659 mean train loss:  1.10586826e-02, mean val. rec. loss:  1.03640052e-02\n",
      "Epoch: 7660 mean train loss:  1.10578269e-02, mean val. rec. loss:  1.03630946e-02\n",
      "Epoch: 7661 mean train loss:  1.10569722e-02, mean val. rec. loss:  1.03621817e-02\n",
      "Epoch: 7662 mean train loss:  1.10561137e-02, mean val. rec. loss:  1.03612712e-02\n",
      "Epoch: 7663 mean train loss:  1.10552590e-02, mean val. rec. loss:  1.03603617e-02\n",
      "Epoch: 7664 mean train loss:  1.10544079e-02, mean val. rec. loss:  1.03594591e-02\n",
      "Epoch: 7665 mean train loss:  1.10535532e-02, mean val. rec. loss:  1.03585473e-02\n",
      "Epoch: 7666 mean train loss:  1.10526957e-02, mean val. rec. loss:  1.03576390e-02\n",
      "Epoch: 7667 mean train loss:  1.10518437e-02, mean val. rec. loss:  1.03567330e-02\n",
      "Epoch: 7668 mean train loss:  1.10509862e-02, mean val. rec. loss:  1.03558190e-02\n",
      "Epoch: 7669 mean train loss:  1.10501370e-02, mean val. rec. loss:  1.03549107e-02\n",
      "Epoch: 7670 mean train loss:  1.10492767e-02, mean val. rec. loss:  1.03540012e-02\n",
      "Epoch: 7671 mean train loss:  1.10484247e-02, mean val. rec. loss:  1.03530872e-02\n",
      "Epoch: 7672 mean train loss:  1.10475681e-02, mean val. rec. loss:  1.03521880e-02\n",
      "Epoch: 7673 mean train loss:  1.10467208e-02, mean val. rec. loss:  1.03512740e-02\n",
      "Epoch: 7674 mean train loss:  1.10458624e-02, mean val. rec. loss:  1.03503634e-02\n",
      "Epoch: 7675 mean train loss:  1.10450030e-02, mean val. rec. loss:  1.03494619e-02\n",
      "Epoch: 7676 mean train loss:  1.10441529e-02, mean val. rec. loss:  1.03485525e-02\n",
      "Epoch: 7677 mean train loss:  1.10433037e-02, mean val. rec. loss:  1.03476408e-02\n",
      "Epoch: 7678 mean train loss:  1.10424452e-02, mean val. rec. loss:  1.03467290e-02\n",
      "Epoch: 7679 mean train loss:  1.10415877e-02, mean val. rec. loss:  1.03458196e-02\n",
      "Epoch: 7680 mean train loss:  1.10407376e-02, mean val. rec. loss:  1.03449124e-02\n",
      "Epoch: 7681 mean train loss:  1.10398866e-02, mean val. rec. loss:  1.03440052e-02\n",
      "Epoch: 7682 mean train loss:  1.10390309e-02, mean val. rec. loss:  1.03430958e-02\n",
      "Epoch: 7683 mean train loss:  1.10381752e-02, mean val. rec. loss:  1.03421920e-02\n",
      "Epoch: 7684 mean train loss:  1.10373224e-02, mean val. rec. loss:  1.03412803e-02\n",
      "Epoch: 7685 mean train loss:  1.10364713e-02, mean val. rec. loss:  1.03403742e-02\n",
      "Epoch: 7686 mean train loss:  1.10356184e-02, mean val. rec. loss:  1.03394637e-02\n",
      "Epoch: 7687 mean train loss:  1.10347628e-02, mean val. rec. loss:  1.03385599e-02\n",
      "Epoch: 7688 mean train loss:  1.10339108e-02, mean val. rec. loss:  1.03376527e-02\n",
      "Epoch: 7689 mean train loss:  1.10330598e-02, mean val. rec. loss:  1.03367478e-02\n",
      "Epoch: 7690 mean train loss:  1.10322051e-02, mean val. rec. loss:  1.03358338e-02\n",
      "Epoch: 7691 mean train loss:  1.10313494e-02, mean val. rec. loss:  1.03349289e-02\n",
      "Epoch: 7692 mean train loss:  1.10304984e-02, mean val. rec. loss:  1.03340183e-02\n",
      "Epoch: 7693 mean train loss:  1.10296436e-02, mean val. rec. loss:  1.03331145e-02\n",
      "Epoch: 7694 mean train loss:  1.10287917e-02, mean val. rec. loss:  1.03322096e-02\n",
      "Epoch: 7695 mean train loss:  1.10279388e-02, mean val. rec. loss:  1.03312968e-02\n",
      "Epoch: 7696 mean train loss:  1.10270850e-02, mean val. rec. loss:  1.03303850e-02\n",
      "Epoch: 7697 mean train loss:  1.10262311e-02, mean val. rec. loss:  1.03294813e-02\n",
      "Epoch: 7698 mean train loss:  1.10253792e-02, mean val. rec. loss:  1.03285763e-02\n",
      "Epoch: 7699 mean train loss:  1.10245263e-02, mean val. rec. loss:  1.03276692e-02\n",
      "Epoch: 7700 mean train loss:  1.10236762e-02, mean val. rec. loss:  1.03267608e-02\n",
      "Epoch: 7701 mean train loss:  1.10228215e-02, mean val. rec. loss:  1.03258491e-02\n",
      "Epoch: 7702 mean train loss:  1.10219667e-02, mean val. rec. loss:  1.03249408e-02\n",
      "Epoch: 7703 mean train loss:  1.10211157e-02, mean val. rec. loss:  1.03240359e-02\n",
      "Epoch: 7704 mean train loss:  1.10202656e-02, mean val. rec. loss:  1.03231287e-02\n",
      "Epoch: 7705 mean train loss:  1.10194090e-02, mean val. rec. loss:  1.03222193e-02\n",
      "Epoch: 7706 mean train loss:  1.10185515e-02, mean val. rec. loss:  1.03213166e-02\n",
      "Epoch: 7707 mean train loss:  1.10177023e-02, mean val. rec. loss:  1.03204094e-02\n",
      "Epoch: 7708 mean train loss:  1.10168513e-02, mean val. rec. loss:  1.03194977e-02\n",
      "Epoch: 7709 mean train loss:  1.10159965e-02, mean val. rec. loss:  1.03185928e-02\n",
      "Epoch: 7710 mean train loss:  1.10151418e-02, mean val. rec. loss:  1.03176902e-02\n",
      "Epoch: 7711 mean train loss:  1.10142926e-02, mean val. rec. loss:  1.03167830e-02\n",
      "Epoch: 7712 mean train loss:  1.10134416e-02, mean val. rec. loss:  1.03158849e-02\n",
      "Epoch: 7713 mean train loss:  1.10125887e-02, mean val. rec. loss:  1.03149754e-02\n",
      "Epoch: 7714 mean train loss:  1.10117359e-02, mean val. rec. loss:  1.03140682e-02\n",
      "Epoch: 7715 mean train loss:  1.10108886e-02, mean val. rec. loss:  1.03131633e-02\n",
      "Epoch: 7716 mean train loss:  1.10100357e-02, mean val. rec. loss:  1.03122493e-02\n",
      "Epoch: 7717 mean train loss:  1.10091809e-02, mean val. rec. loss:  1.03113467e-02\n",
      "Epoch: 7718 mean train loss:  1.10083280e-02, mean val. rec. loss:  1.03104441e-02\n",
      "Epoch: 7719 mean train loss:  1.10074807e-02, mean val. rec. loss:  1.03095369e-02\n",
      "Epoch: 7720 mean train loss:  1.10066269e-02, mean val. rec. loss:  1.03086342e-02\n",
      "Epoch: 7721 mean train loss:  1.10057778e-02, mean val. rec. loss:  1.03077180e-02\n",
      "Epoch: 7722 mean train loss:  1.10049202e-02, mean val. rec. loss:  1.03068176e-02\n",
      "Epoch: 7723 mean train loss:  1.10040748e-02, mean val. rec. loss:  1.03059116e-02\n",
      "Epoch: 7724 mean train loss:  1.10032210e-02, mean val. rec. loss:  1.03050010e-02\n",
      "Epoch: 7725 mean train loss:  1.10023709e-02, mean val. rec. loss:  1.03040938e-02\n",
      "Epoch: 7726 mean train loss:  1.10015143e-02, mean val. rec. loss:  1.03031934e-02\n",
      "Epoch: 7727 mean train loss:  1.10006689e-02, mean val. rec. loss:  1.03022874e-02\n",
      "Epoch: 7728 mean train loss:  1.09998160e-02, mean val. rec. loss:  1.03013870e-02\n",
      "Epoch: 7729 mean train loss:  1.09989649e-02, mean val. rec. loss:  1.03004764e-02\n",
      "Epoch: 7730 mean train loss:  1.09981093e-02, mean val. rec. loss:  1.02995749e-02\n",
      "Epoch: 7731 mean train loss:  1.09972638e-02, mean val. rec. loss:  1.02986688e-02\n",
      "Epoch: 7732 mean train loss:  1.09964110e-02, mean val. rec. loss:  1.02977617e-02\n",
      "Epoch: 7733 mean train loss:  1.09955646e-02, mean val. rec. loss:  1.02968579e-02\n",
      "Epoch: 7734 mean train loss:  1.09947098e-02, mean val. rec. loss:  1.02959518e-02\n",
      "Epoch: 7735 mean train loss:  1.09938625e-02, mean val. rec. loss:  1.02950481e-02\n",
      "Epoch: 7736 mean train loss:  1.09930069e-02, mean val. rec. loss:  1.02941443e-02\n",
      "Epoch: 7737 mean train loss:  1.09921605e-02, mean val. rec. loss:  1.02932371e-02\n",
      "Epoch: 7738 mean train loss:  1.09913058e-02, mean val. rec. loss:  1.02923367e-02\n",
      "Epoch: 7739 mean train loss:  1.09904594e-02, mean val. rec. loss:  1.02914273e-02\n",
      "Epoch: 7740 mean train loss:  1.09896037e-02, mean val. rec. loss:  1.02905201e-02\n",
      "Epoch: 7741 mean train loss:  1.09887536e-02, mean val. rec. loss:  1.02896163e-02\n",
      "Epoch: 7742 mean train loss:  1.09879017e-02, mean val. rec. loss:  1.02887125e-02\n",
      "Epoch: 7743 mean train loss:  1.09870581e-02, mean val. rec. loss:  1.02878076e-02\n",
      "Epoch: 7744 mean train loss:  1.09862006e-02, mean val. rec. loss:  1.02869050e-02\n",
      "Epoch: 7745 mean train loss:  1.09853523e-02, mean val. rec. loss:  1.02860001e-02\n",
      "Epoch: 7746 mean train loss:  1.09845032e-02, mean val. rec. loss:  1.02850906e-02\n",
      "Epoch: 7747 mean train loss:  1.09836522e-02, mean val. rec. loss:  1.02841857e-02\n",
      "Epoch: 7748 mean train loss:  1.09828021e-02, mean val. rec. loss:  1.02832808e-02\n",
      "Epoch: 7749 mean train loss:  1.09819501e-02, mean val. rec. loss:  1.02823827e-02\n",
      "Epoch: 7750 mean train loss:  1.09811065e-02, mean val. rec. loss:  1.02814778e-02\n",
      "Epoch: 7751 mean train loss:  1.09802499e-02, mean val. rec. loss:  1.02805751e-02\n",
      "Epoch: 7752 mean train loss:  1.09794026e-02, mean val. rec. loss:  1.02796725e-02\n",
      "Epoch: 7753 mean train loss:  1.09785516e-02, mean val. rec. loss:  1.02787653e-02\n",
      "Epoch: 7754 mean train loss:  1.09777025e-02, mean val. rec. loss:  1.02778638e-02\n",
      "Epoch: 7755 mean train loss:  1.09768533e-02, mean val. rec. loss:  1.02769577e-02\n",
      "Epoch: 7756 mean train loss:  1.09760023e-02, mean val. rec. loss:  1.02760528e-02\n",
      "Epoch: 7757 mean train loss:  1.09751531e-02, mean val. rec. loss:  1.02751502e-02\n",
      "Epoch: 7758 mean train loss:  1.09743040e-02, mean val. rec. loss:  1.02742498e-02\n",
      "Epoch: 7759 mean train loss:  1.09734539e-02, mean val. rec. loss:  1.02733437e-02\n",
      "Epoch: 7760 mean train loss:  1.09726056e-02, mean val. rec. loss:  1.02724468e-02\n",
      "Epoch: 7761 mean train loss:  1.09717565e-02, mean val. rec. loss:  1.02715441e-02\n",
      "Epoch: 7762 mean train loss:  1.09709110e-02, mean val. rec. loss:  1.02706392e-02\n",
      "Epoch: 7763 mean train loss:  1.09700600e-02, mean val. rec. loss:  1.02697332e-02\n",
      "Epoch: 7764 mean train loss:  1.09692071e-02, mean val. rec. loss:  1.02688271e-02\n",
      "Epoch: 7765 mean train loss:  1.09683617e-02, mean val. rec. loss:  1.02679313e-02\n",
      "Epoch: 7766 mean train loss:  1.09675135e-02, mean val. rec. loss:  1.02670252e-02\n",
      "Epoch: 7767 mean train loss:  1.09666597e-02, mean val. rec. loss:  1.02661158e-02\n",
      "Epoch: 7768 mean train loss:  1.09658077e-02, mean val. rec. loss:  1.02652165e-02\n",
      "Epoch: 7769 mean train loss:  1.09649604e-02, mean val. rec. loss:  1.02643162e-02\n",
      "Epoch: 7770 mean train loss:  1.09641141e-02, mean val. rec. loss:  1.02634124e-02\n",
      "Epoch: 7771 mean train loss:  1.09632630e-02, mean val. rec. loss:  1.02625120e-02\n",
      "Epoch: 7772 mean train loss:  1.09624120e-02, mean val. rec. loss:  1.02616150e-02\n",
      "Epoch: 7773 mean train loss:  1.09615656e-02, mean val. rec. loss:  1.02607101e-02\n",
      "Epoch: 7774 mean train loss:  1.09607174e-02, mean val. rec. loss:  1.02598109e-02\n",
      "Epoch: 7775 mean train loss:  1.09598683e-02, mean val. rec. loss:  1.02589014e-02\n",
      "Epoch: 7776 mean train loss:  1.09590210e-02, mean val. rec. loss:  1.02580022e-02\n",
      "Epoch: 7777 mean train loss:  1.09581718e-02, mean val. rec. loss:  1.02571041e-02\n",
      "Epoch: 7778 mean train loss:  1.09573236e-02, mean val. rec. loss:  1.02562026e-02\n",
      "Epoch: 7779 mean train loss:  1.09564753e-02, mean val. rec. loss:  1.02552931e-02\n",
      "Epoch: 7780 mean train loss:  1.09556243e-02, mean val. rec. loss:  1.02543927e-02\n",
      "Epoch: 7781 mean train loss:  1.09547789e-02, mean val. rec. loss:  1.02534946e-02\n",
      "Epoch: 7782 mean train loss:  1.09539335e-02, mean val. rec. loss:  1.02525897e-02\n",
      "Epoch: 7783 mean train loss:  1.09530834e-02, mean val. rec. loss:  1.02516882e-02\n",
      "Epoch: 7784 mean train loss:  1.09522333e-02, mean val. rec. loss:  1.02507833e-02\n",
      "Epoch: 7785 mean train loss:  1.09513878e-02, mean val. rec. loss:  1.02498840e-02\n",
      "Epoch: 7786 mean train loss:  1.09505415e-02, mean val. rec. loss:  1.02489837e-02\n",
      "Epoch: 7787 mean train loss:  1.09496933e-02, mean val. rec. loss:  1.02480821e-02\n",
      "Epoch: 7788 mean train loss:  1.09488460e-02, mean val. rec. loss:  1.02471761e-02\n",
      "Epoch: 7789 mean train loss:  1.09479987e-02, mean val. rec. loss:  1.02462746e-02\n",
      "Epoch: 7790 mean train loss:  1.09471504e-02, mean val. rec. loss:  1.02453765e-02\n",
      "Epoch: 7791 mean train loss:  1.09463059e-02, mean val. rec. loss:  1.02444806e-02\n",
      "Epoch: 7792 mean train loss:  1.09454605e-02, mean val. rec. loss:  1.02435746e-02\n",
      "Epoch: 7793 mean train loss:  1.09446113e-02, mean val. rec. loss:  1.02426742e-02\n",
      "Epoch: 7794 mean train loss:  1.09437584e-02, mean val. rec. loss:  1.02417750e-02\n",
      "Epoch: 7795 mean train loss:  1.09429112e-02, mean val. rec. loss:  1.02408757e-02\n",
      "Epoch: 7796 mean train loss:  1.09420685e-02, mean val. rec. loss:  1.02399810e-02\n",
      "Epoch: 7797 mean train loss:  1.09412221e-02, mean val. rec. loss:  1.02390727e-02\n",
      "Epoch: 7798 mean train loss:  1.09403693e-02, mean val. rec. loss:  1.02381735e-02\n",
      "Epoch: 7799 mean train loss:  1.09395238e-02, mean val. rec. loss:  1.02372776e-02\n",
      "Epoch: 7800 mean train loss:  1.09386803e-02, mean val. rec. loss:  1.02363704e-02\n",
      "Epoch: 7801 mean train loss:  1.09378311e-02, mean val. rec. loss:  1.02354735e-02\n",
      "Epoch: 7802 mean train loss:  1.09369829e-02, mean val. rec. loss:  1.02345719e-02\n",
      "Epoch: 7803 mean train loss:  1.09361384e-02, mean val. rec. loss:  1.02336738e-02\n",
      "Epoch: 7804 mean train loss:  1.09352929e-02, mean val. rec. loss:  1.02327757e-02\n",
      "Epoch: 7805 mean train loss:  1.09344447e-02, mean val. rec. loss:  1.02318697e-02\n",
      "Epoch: 7806 mean train loss:  1.09335965e-02, mean val. rec. loss:  1.02309727e-02\n",
      "Epoch: 7807 mean train loss:  1.09327520e-02, mean val. rec. loss:  1.02300746e-02\n",
      "Epoch: 7808 mean train loss:  1.09319093e-02, mean val. rec. loss:  1.02291663e-02\n",
      "Epoch: 7809 mean train loss:  1.09310592e-02, mean val. rec. loss:  1.02282693e-02\n",
      "Epoch: 7810 mean train loss:  1.09302101e-02, mean val. rec. loss:  1.02273746e-02\n",
      "Epoch: 7811 mean train loss:  1.09293656e-02, mean val. rec. loss:  1.02264788e-02\n",
      "Epoch: 7812 mean train loss:  1.09285229e-02, mean val. rec. loss:  1.02255784e-02\n",
      "Epoch: 7813 mean train loss:  1.09276747e-02, mean val. rec. loss:  1.02246825e-02\n",
      "Epoch: 7814 mean train loss:  1.09268256e-02, mean val. rec. loss:  1.02237833e-02\n",
      "Epoch: 7815 mean train loss:  1.09259801e-02, mean val. rec. loss:  1.02228784e-02\n",
      "Epoch: 7816 mean train loss:  1.09251347e-02, mean val. rec. loss:  1.02219803e-02\n",
      "Epoch: 7817 mean train loss:  1.09242883e-02, mean val. rec. loss:  1.02210844e-02\n",
      "Epoch: 7818 mean train loss:  1.09234438e-02, mean val. rec. loss:  1.02201886e-02\n",
      "Epoch: 7819 mean train loss:  1.09225965e-02, mean val. rec. loss:  1.02192893e-02\n",
      "Epoch: 7820 mean train loss:  1.09217511e-02, mean val. rec. loss:  1.02183856e-02\n",
      "Epoch: 7821 mean train loss:  1.09209056e-02, mean val. rec. loss:  1.02174943e-02\n",
      "Epoch: 7822 mean train loss:  1.09200602e-02, mean val. rec. loss:  1.02166007e-02\n",
      "Epoch: 7823 mean train loss:  1.09192176e-02, mean val. rec. loss:  1.02157003e-02\n",
      "Epoch: 7824 mean train loss:  1.09183749e-02, mean val. rec. loss:  1.02147977e-02\n",
      "Epoch: 7825 mean train loss:  1.09175276e-02, mean val. rec. loss:  1.02138996e-02\n",
      "Epoch: 7826 mean train loss:  1.09166794e-02, mean val. rec. loss:  1.02130060e-02\n",
      "Epoch: 7827 mean train loss:  1.09158368e-02, mean val. rec. loss:  1.02121056e-02\n",
      "Epoch: 7828 mean train loss:  1.09149932e-02, mean val. rec. loss:  1.02112030e-02\n",
      "Epoch: 7829 mean train loss:  1.09141440e-02, mean val. rec. loss:  1.02103128e-02\n",
      "Epoch: 7830 mean train loss:  1.09133042e-02, mean val. rec. loss:  1.02094124e-02\n",
      "Epoch: 7831 mean train loss:  1.09124550e-02, mean val. rec. loss:  1.02085120e-02\n",
      "Epoch: 7832 mean train loss:  1.09116142e-02, mean val. rec. loss:  1.02076117e-02\n",
      "Epoch: 7833 mean train loss:  1.09107632e-02, mean val. rec. loss:  1.02067158e-02\n",
      "Epoch: 7834 mean train loss:  1.09099262e-02, mean val. rec. loss:  1.02058234e-02\n",
      "Epoch: 7835 mean train loss:  1.09090789e-02, mean val. rec. loss:  1.02049264e-02\n",
      "Epoch: 7836 mean train loss:  1.09082334e-02, mean val. rec. loss:  1.02040260e-02\n",
      "Epoch: 7837 mean train loss:  1.09073899e-02, mean val. rec. loss:  1.02031358e-02\n",
      "Epoch: 7838 mean train loss:  1.09065444e-02, mean val. rec. loss:  1.02022355e-02\n",
      "Epoch: 7839 mean train loss:  1.09057009e-02, mean val. rec. loss:  1.02013419e-02\n",
      "Epoch: 7840 mean train loss:  1.09048573e-02, mean val. rec. loss:  1.02004381e-02\n",
      "Epoch: 7841 mean train loss:  1.09040119e-02, mean val. rec. loss:  1.01995411e-02\n",
      "Epoch: 7842 mean train loss:  1.09031683e-02, mean val. rec. loss:  1.01986498e-02\n",
      "Epoch: 7843 mean train loss:  1.09023275e-02, mean val. rec. loss:  1.01977540e-02\n",
      "Epoch: 7844 mean train loss:  1.09014811e-02, mean val. rec. loss:  1.01968514e-02\n",
      "Epoch: 7845 mean train loss:  1.09006357e-02, mean val. rec. loss:  1.01959612e-02\n",
      "Epoch: 7846 mean train loss:  1.08997940e-02, mean val. rec. loss:  1.01950619e-02\n",
      "Epoch: 7847 mean train loss:  1.08989514e-02, mean val. rec. loss:  1.01941650e-02\n",
      "Epoch: 7848 mean train loss:  1.08981068e-02, mean val. rec. loss:  1.01932669e-02\n",
      "Epoch: 7849 mean train loss:  1.08972596e-02, mean val. rec. loss:  1.01923699e-02\n",
      "Epoch: 7850 mean train loss:  1.08964216e-02, mean val. rec. loss:  1.01914752e-02\n",
      "Epoch: 7851 mean train loss:  1.08955761e-02, mean val. rec. loss:  1.01905793e-02\n",
      "Epoch: 7852 mean train loss:  1.08947298e-02, mean val. rec. loss:  1.01896880e-02\n",
      "Epoch: 7853 mean train loss:  1.08938890e-02, mean val. rec. loss:  1.01887922e-02\n",
      "Epoch: 7854 mean train loss:  1.08930482e-02, mean val. rec. loss:  1.01878963e-02\n",
      "Epoch: 7855 mean train loss:  1.08922028e-02, mean val. rec. loss:  1.01870039e-02\n",
      "Epoch: 7856 mean train loss:  1.08913573e-02, mean val. rec. loss:  1.01861081e-02\n",
      "Epoch: 7857 mean train loss:  1.08905166e-02, mean val. rec. loss:  1.01852145e-02\n",
      "Epoch: 7858 mean train loss:  1.08896739e-02, mean val. rec. loss:  1.01843232e-02\n",
      "Epoch: 7859 mean train loss:  1.08888313e-02, mean val. rec. loss:  1.01834262e-02\n",
      "Epoch: 7860 mean train loss:  1.08879877e-02, mean val. rec. loss:  1.01825292e-02\n",
      "Epoch: 7861 mean train loss:  1.08871469e-02, mean val. rec. loss:  1.01816345e-02\n",
      "Epoch: 7862 mean train loss:  1.08863043e-02, mean val. rec. loss:  1.01807443e-02\n",
      "Epoch: 7863 mean train loss:  1.08854607e-02, mean val. rec. loss:  1.01798496e-02\n",
      "Epoch: 7864 mean train loss:  1.08846190e-02, mean val. rec. loss:  1.01789549e-02\n",
      "Epoch: 7865 mean train loss:  1.08837754e-02, mean val. rec. loss:  1.01780546e-02\n",
      "Epoch: 7866 mean train loss:  1.08829309e-02, mean val. rec. loss:  1.01771632e-02\n",
      "Epoch: 7867 mean train loss:  1.08820920e-02, mean val. rec. loss:  1.01762651e-02\n",
      "Epoch: 7868 mean train loss:  1.08812512e-02, mean val. rec. loss:  1.01753761e-02\n",
      "Epoch: 7869 mean train loss:  1.08804067e-02, mean val. rec. loss:  1.01744780e-02\n",
      "Epoch: 7870 mean train loss:  1.08795622e-02, mean val. rec. loss:  1.01735878e-02\n",
      "Epoch: 7871 mean train loss:  1.08787233e-02, mean val. rec. loss:  1.01726931e-02\n",
      "Epoch: 7872 mean train loss:  1.08778816e-02, mean val. rec. loss:  1.01717905e-02\n",
      "Epoch: 7873 mean train loss:  1.08770399e-02, mean val. rec. loss:  1.01708992e-02\n",
      "Epoch: 7874 mean train loss:  1.08761982e-02, mean val. rec. loss:  1.01700113e-02\n",
      "Epoch: 7875 mean train loss:  1.08753565e-02, mean val. rec. loss:  1.01691211e-02\n",
      "Epoch: 7876 mean train loss:  1.08745176e-02, mean val. rec. loss:  1.01682207e-02\n",
      "Epoch: 7877 mean train loss:  1.08736740e-02, mean val. rec. loss:  1.01673271e-02\n",
      "Epoch: 7878 mean train loss:  1.08728304e-02, mean val. rec. loss:  1.01664392e-02\n",
      "Epoch: 7879 mean train loss:  1.08719887e-02, mean val. rec. loss:  1.01655491e-02\n",
      "Epoch: 7880 mean train loss:  1.08711526e-02, mean val. rec. loss:  1.01646600e-02\n",
      "Epoch: 7881 mean train loss:  1.08703099e-02, mean val. rec. loss:  1.01637596e-02\n",
      "Epoch: 7882 mean train loss:  1.08694664e-02, mean val. rec. loss:  1.01628638e-02\n",
      "Epoch: 7883 mean train loss:  1.08686246e-02, mean val. rec. loss:  1.01619793e-02\n",
      "Epoch: 7884 mean train loss:  1.08677876e-02, mean val. rec. loss:  1.01610835e-02\n",
      "Epoch: 7885 mean train loss:  1.08669477e-02, mean val. rec. loss:  1.01601910e-02\n",
      "Epoch: 7886 mean train loss:  1.08661023e-02, mean val. rec. loss:  1.01593031e-02\n",
      "Epoch: 7887 mean train loss:  1.08652634e-02, mean val. rec. loss:  1.01584084e-02\n",
      "Epoch: 7888 mean train loss:  1.08644254e-02, mean val. rec. loss:  1.01575171e-02\n",
      "Epoch: 7889 mean train loss:  1.08635818e-02, mean val. rec. loss:  1.01566156e-02\n",
      "Epoch: 7890 mean train loss:  1.08627401e-02, mean val. rec. loss:  1.01557311e-02\n",
      "Epoch: 7891 mean train loss:  1.08619021e-02, mean val. rec. loss:  1.01548421e-02\n",
      "Epoch: 7892 mean train loss:  1.08610632e-02, mean val. rec. loss:  1.01539439e-02\n",
      "Epoch: 7893 mean train loss:  1.08602224e-02, mean val. rec. loss:  1.01530470e-02\n",
      "Epoch: 7894 mean train loss:  1.08593770e-02, mean val. rec. loss:  1.01521591e-02\n",
      "Epoch: 7895 mean train loss:  1.08585409e-02, mean val. rec. loss:  1.01512746e-02\n",
      "Epoch: 7896 mean train loss:  1.08577038e-02, mean val. rec. loss:  1.01503855e-02\n",
      "Epoch: 7897 mean train loss:  1.08568621e-02, mean val. rec. loss:  1.01494897e-02\n",
      "Epoch: 7898 mean train loss:  1.08560204e-02, mean val. rec. loss:  1.01486006e-02\n",
      "Epoch: 7899 mean train loss:  1.08551824e-02, mean val. rec. loss:  1.01477127e-02\n",
      "Epoch: 7900 mean train loss:  1.08543454e-02, mean val. rec. loss:  1.01468214e-02\n",
      "Epoch: 7901 mean train loss:  1.08535037e-02, mean val. rec. loss:  1.01459358e-02\n",
      "Epoch: 7902 mean train loss:  1.08526620e-02, mean val. rec. loss:  1.01450445e-02\n",
      "Epoch: 7903 mean train loss:  1.08518221e-02, mean val. rec. loss:  1.01441532e-02\n",
      "Epoch: 7904 mean train loss:  1.08509832e-02, mean val. rec. loss:  1.01432573e-02\n",
      "Epoch: 7905 mean train loss:  1.08501443e-02, mean val. rec. loss:  1.01423706e-02\n",
      "Epoch: 7906 mean train loss:  1.08493044e-02, mean val. rec. loss:  1.01414861e-02\n",
      "Epoch: 7907 mean train loss:  1.08484664e-02, mean val. rec. loss:  1.01406027e-02\n",
      "Epoch: 7908 mean train loss:  1.08476275e-02, mean val. rec. loss:  1.01397035e-02\n",
      "Epoch: 7909 mean train loss:  1.08467886e-02, mean val. rec. loss:  1.01388076e-02\n",
      "Epoch: 7910 mean train loss:  1.08459525e-02, mean val. rec. loss:  1.01379197e-02\n",
      "Epoch: 7911 mean train loss:  1.08451154e-02, mean val. rec. loss:  1.01370398e-02\n",
      "Epoch: 7912 mean train loss:  1.08442746e-02, mean val. rec. loss:  1.01361450e-02\n",
      "Epoch: 7913 mean train loss:  1.08434329e-02, mean val. rec. loss:  1.01352537e-02\n",
      "Epoch: 7914 mean train loss:  1.08425977e-02, mean val. rec. loss:  1.01343624e-02\n",
      "Epoch: 7915 mean train loss:  1.08417607e-02, mean val. rec. loss:  1.01334779e-02\n",
      "Epoch: 7916 mean train loss:  1.08409208e-02, mean val. rec. loss:  1.01325878e-02\n",
      "Epoch: 7917 mean train loss:  1.08400810e-02, mean val. rec. loss:  1.01316976e-02\n",
      "Epoch: 7918 mean train loss:  1.08392477e-02, mean val. rec. loss:  1.01308097e-02\n",
      "Epoch: 7919 mean train loss:  1.08384050e-02, mean val. rec. loss:  1.01299218e-02\n",
      "Epoch: 7920 mean train loss:  1.08375689e-02, mean val. rec. loss:  1.01290282e-02\n",
      "Epoch: 7921 mean train loss:  1.08367291e-02, mean val. rec. loss:  1.01281505e-02\n",
      "Epoch: 7922 mean train loss:  1.08358985e-02, mean val. rec. loss:  1.01272615e-02\n",
      "Epoch: 7923 mean train loss:  1.08350531e-02, mean val. rec. loss:  1.01263747e-02\n",
      "Epoch: 7924 mean train loss:  1.08342170e-02, mean val. rec. loss:  1.01254902e-02\n",
      "Epoch: 7925 mean train loss:  1.08333827e-02, mean val. rec. loss:  1.01245989e-02\n",
      "Epoch: 7926 mean train loss:  1.08325447e-02, mean val. rec. loss:  1.01237076e-02\n",
      "Epoch: 7927 mean train loss:  1.08317039e-02, mean val. rec. loss:  1.01228254e-02\n",
      "Epoch: 7928 mean train loss:  1.08308669e-02, mean val. rec. loss:  1.01219329e-02\n",
      "Epoch: 7929 mean train loss:  1.08300345e-02, mean val. rec. loss:  1.01210496e-02\n",
      "Epoch: 7930 mean train loss:  1.08291984e-02, mean val. rec. loss:  1.01201594e-02\n",
      "Epoch: 7931 mean train loss:  1.08283557e-02, mean val. rec. loss:  1.01192726e-02\n",
      "Epoch: 7932 mean train loss:  1.08275187e-02, mean val. rec. loss:  1.01183915e-02\n",
      "Epoch: 7933 mean train loss:  1.08266844e-02, mean val. rec. loss:  1.01174968e-02\n",
      "Epoch: 7934 mean train loss:  1.08258501e-02, mean val. rec. loss:  1.01166112e-02\n",
      "Epoch: 7935 mean train loss:  1.08250103e-02, mean val. rec. loss:  1.01157221e-02\n",
      "Epoch: 7936 mean train loss:  1.08241732e-02, mean val. rec. loss:  1.01148331e-02\n",
      "Epoch: 7937 mean train loss:  1.08233390e-02, mean val. rec. loss:  1.01139486e-02\n",
      "Epoch: 7938 mean train loss:  1.08225038e-02, mean val. rec. loss:  1.01130630e-02\n",
      "Epoch: 7939 mean train loss:  1.08216658e-02, mean val. rec. loss:  1.01121751e-02\n",
      "Epoch: 7940 mean train loss:  1.08208278e-02, mean val. rec. loss:  1.01112962e-02\n",
      "Epoch: 7941 mean train loss:  1.08199935e-02, mean val. rec. loss:  1.01104026e-02\n",
      "Epoch: 7942 mean train loss:  1.08191556e-02, mean val. rec. loss:  1.01095261e-02\n",
      "Epoch: 7943 mean train loss:  1.08183213e-02, mean val. rec. loss:  1.01086325e-02\n",
      "Epoch: 7944 mean train loss:  1.08174889e-02, mean val. rec. loss:  1.01077559e-02\n",
      "Epoch: 7945 mean train loss:  1.08166509e-02, mean val. rec. loss:  1.01068646e-02\n",
      "Epoch: 7946 mean train loss:  1.08158129e-02, mean val. rec. loss:  1.01059881e-02\n",
      "Epoch: 7947 mean train loss:  1.08149796e-02, mean val. rec. loss:  1.01050922e-02\n",
      "Epoch: 7948 mean train loss:  1.08141444e-02, mean val. rec. loss:  1.01042145e-02\n",
      "Epoch: 7949 mean train loss:  1.08133073e-02, mean val. rec. loss:  1.01033198e-02\n",
      "Epoch: 7950 mean train loss:  1.08124740e-02, mean val. rec. loss:  1.01024421e-02\n",
      "Epoch: 7951 mean train loss:  1.08116379e-02, mean val. rec. loss:  1.01015508e-02\n",
      "Epoch: 7952 mean train loss:  1.08108036e-02, mean val. rec. loss:  1.01006743e-02\n",
      "Epoch: 7953 mean train loss:  1.08099684e-02, mean val. rec. loss:  1.00997784e-02\n",
      "Epoch: 7954 mean train loss:  1.08091332e-02, mean val. rec. loss:  1.00989030e-02\n",
      "Epoch: 7955 mean train loss:  1.08082990e-02, mean val. rec. loss:  1.00980151e-02\n",
      "Epoch: 7956 mean train loss:  1.08074647e-02, mean val. rec. loss:  1.00971385e-02\n",
      "Epoch: 7957 mean train loss:  1.08066323e-02, mean val. rec. loss:  1.00962449e-02\n",
      "Epoch: 7958 mean train loss:  1.08057943e-02, mean val. rec. loss:  1.00953673e-02\n",
      "Epoch: 7959 mean train loss:  1.08049582e-02, mean val. rec. loss:  1.00944759e-02\n",
      "Epoch: 7960 mean train loss:  1.08041258e-02, mean val. rec. loss:  1.00935982e-02\n",
      "Epoch: 7961 mean train loss:  1.08032944e-02, mean val. rec. loss:  1.00927024e-02\n",
      "Epoch: 7962 mean train loss:  1.08024573e-02, mean val. rec. loss:  1.00918292e-02\n",
      "Epoch: 7963 mean train loss:  1.08016221e-02, mean val. rec. loss:  1.00909425e-02\n",
      "Epoch: 7964 mean train loss:  1.08007906e-02, mean val. rec. loss:  1.00900602e-02\n",
      "Epoch: 7965 mean train loss:  1.07999545e-02, mean val. rec. loss:  1.00891780e-02\n",
      "Epoch: 7966 mean train loss:  1.07991230e-02, mean val. rec. loss:  1.00882958e-02\n",
      "Epoch: 7967 mean train loss:  1.07982916e-02, mean val. rec. loss:  1.00874158e-02\n",
      "Epoch: 7968 mean train loss:  1.07974536e-02, mean val. rec. loss:  1.00865313e-02\n",
      "Epoch: 7969 mean train loss:  1.07966193e-02, mean val. rec. loss:  1.00856491e-02\n",
      "Epoch: 7970 mean train loss:  1.07957888e-02, mean val. rec. loss:  1.00847680e-02\n",
      "Epoch: 7971 mean train loss:  1.07949573e-02, mean val. rec. loss:  1.00838823e-02\n",
      "Epoch: 7972 mean train loss:  1.07941221e-02, mean val. rec. loss:  1.00830024e-02\n",
      "Epoch: 7973 mean train loss:  1.07932869e-02, mean val. rec. loss:  1.00821201e-02\n",
      "Epoch: 7974 mean train loss:  1.07924536e-02, mean val. rec. loss:  1.00812402e-02\n",
      "Epoch: 7975 mean train loss:  1.07916249e-02, mean val. rec. loss:  1.00803602e-02\n",
      "Epoch: 7976 mean train loss:  1.07907925e-02, mean val. rec. loss:  1.00794746e-02\n",
      "Epoch: 7977 mean train loss:  1.07899555e-02, mean val. rec. loss:  1.00785935e-02\n",
      "Epoch: 7978 mean train loss:  1.07891221e-02, mean val. rec. loss:  1.00777158e-02\n",
      "Epoch: 7979 mean train loss:  1.07882935e-02, mean val. rec. loss:  1.00768279e-02\n",
      "Epoch: 7980 mean train loss:  1.07874629e-02, mean val. rec. loss:  1.00759445e-02\n",
      "Epoch: 7981 mean train loss:  1.07866240e-02, mean val. rec. loss:  1.00750691e-02\n",
      "Epoch: 7982 mean train loss:  1.07857981e-02, mean val. rec. loss:  1.00741857e-02\n",
      "Epoch: 7983 mean train loss:  1.07849629e-02, mean val. rec. loss:  1.00733069e-02\n",
      "Epoch: 7984 mean train loss:  1.07841333e-02, mean val. rec. loss:  1.00724190e-02\n",
      "Epoch: 7985 mean train loss:  1.07832954e-02, mean val. rec. loss:  1.00715447e-02\n",
      "Epoch: 7986 mean train loss:  1.07824695e-02, mean val. rec. loss:  1.00706658e-02\n",
      "Epoch: 7987 mean train loss:  1.07816361e-02, mean val. rec. loss:  1.00697825e-02\n",
      "Epoch: 7988 mean train loss:  1.07808056e-02, mean val. rec. loss:  1.00689014e-02\n",
      "Epoch: 7989 mean train loss:  1.07799686e-02, mean val. rec. loss:  1.00680225e-02\n",
      "Epoch: 7990 mean train loss:  1.07791427e-02, mean val. rec. loss:  1.00671448e-02\n",
      "Epoch: 7991 mean train loss:  1.07783103e-02, mean val. rec. loss:  1.00662694e-02\n",
      "Epoch: 7992 mean train loss:  1.07774807e-02, mean val. rec. loss:  1.00653872e-02\n",
      "Epoch: 7993 mean train loss:  1.07766427e-02, mean val. rec. loss:  1.00645095e-02\n",
      "Epoch: 7994 mean train loss:  1.07758177e-02, mean val. rec. loss:  1.00636238e-02\n",
      "Epoch: 7995 mean train loss:  1.07749863e-02, mean val. rec. loss:  1.00627507e-02\n",
      "Epoch: 7996 mean train loss:  1.07741567e-02, mean val. rec. loss:  1.00618673e-02\n",
      "Epoch: 7997 mean train loss:  1.07733196e-02, mean val. rec. loss:  1.00609896e-02\n",
      "Epoch: 7998 mean train loss:  1.07724928e-02, mean val. rec. loss:  1.00601074e-02\n",
      "Epoch: 7999 mean train loss:  1.07716585e-02, mean val. rec. loss:  1.00592274e-02\n",
      "Epoch: 8000 mean train loss:  1.07708317e-02, mean val. rec. loss:  1.00583440e-02\n",
      "Epoch: 8001 mean train loss:  1.07699975e-02, mean val. rec. loss:  1.00574675e-02\n",
      "Epoch: 8002 mean train loss:  1.07691716e-02, mean val. rec. loss:  1.00565898e-02\n",
      "Epoch: 8003 mean train loss:  1.07683382e-02, mean val. rec. loss:  1.00557144e-02\n",
      "Epoch: 8004 mean train loss:  1.07675114e-02, mean val. rec. loss:  1.00548333e-02\n",
      "Epoch: 8005 mean train loss:  1.07666772e-02, mean val. rec. loss:  1.00539578e-02\n",
      "Epoch: 8006 mean train loss:  1.07658504e-02, mean val. rec. loss:  1.00530779e-02\n",
      "Epoch: 8007 mean train loss:  1.07650180e-02, mean val. rec. loss:  1.00522070e-02\n",
      "Epoch: 8008 mean train loss:  1.07641930e-02, mean val. rec. loss:  1.00513259e-02\n",
      "Epoch: 8009 mean train loss:  1.07633625e-02, mean val. rec. loss:  1.00504504e-02\n",
      "Epoch: 8010 mean train loss:  1.07625347e-02, mean val. rec. loss:  1.00495659e-02\n",
      "Epoch: 8011 mean train loss:  1.07616995e-02, mean val. rec. loss:  1.00486894e-02\n",
      "Epoch: 8012 mean train loss:  1.07608755e-02, mean val. rec. loss:  1.00478139e-02\n",
      "Epoch: 8013 mean train loss:  1.07600459e-02, mean val. rec. loss:  1.00469442e-02\n",
      "Epoch: 8014 mean train loss:  1.07592154e-02, mean val. rec. loss:  1.00460642e-02\n",
      "Epoch: 8015 mean train loss:  1.07583867e-02, mean val. rec. loss:  1.00451877e-02\n",
      "Epoch: 8016 mean train loss:  1.07575599e-02, mean val. rec. loss:  1.00443043e-02\n",
      "Epoch: 8017 mean train loss:  1.07567256e-02, mean val. rec. loss:  1.00434311e-02\n",
      "Epoch: 8018 mean train loss:  1.07559035e-02, mean val. rec. loss:  1.00425602e-02\n",
      "Epoch: 8019 mean train loss:  1.07550720e-02, mean val. rec. loss:  1.00416893e-02\n",
      "Epoch: 8020 mean train loss:  1.07542461e-02, mean val. rec. loss:  1.00408048e-02\n",
      "Epoch: 8021 mean train loss:  1.07534109e-02, mean val. rec. loss:  1.00399362e-02\n",
      "Epoch: 8022 mean train loss:  1.07525897e-02, mean val. rec. loss:  1.00390562e-02\n",
      "Epoch: 8023 mean train loss:  1.07517592e-02, mean val. rec. loss:  1.00381865e-02\n",
      "Epoch: 8024 mean train loss:  1.07509342e-02, mean val. rec. loss:  1.00373099e-02\n",
      "Epoch: 8025 mean train loss:  1.07500999e-02, mean val. rec. loss:  1.00364300e-02\n",
      "Epoch: 8026 mean train loss:  1.07492750e-02, mean val. rec. loss:  1.00355591e-02\n",
      "Epoch: 8027 mean train loss:  1.07484519e-02, mean val. rec. loss:  1.00346791e-02\n",
      "Epoch: 8028 mean train loss:  1.07476204e-02, mean val. rec. loss:  1.00338025e-02\n",
      "Epoch: 8029 mean train loss:  1.07467890e-02, mean val. rec. loss:  1.00329350e-02\n",
      "Epoch: 8030 mean train loss:  1.07459650e-02, mean val. rec. loss:  1.00320585e-02\n",
      "Epoch: 8031 mean train loss:  1.07451419e-02, mean val. rec. loss:  1.00311830e-02\n",
      "Epoch: 8032 mean train loss:  1.07443104e-02, mean val. rec. loss:  1.00303065e-02\n",
      "Epoch: 8033 mean train loss:  1.07434799e-02, mean val. rec. loss:  1.00294322e-02\n",
      "Epoch: 8034 mean train loss:  1.07426558e-02, mean val. rec. loss:  1.00285602e-02\n",
      "Epoch: 8035 mean train loss:  1.07418309e-02, mean val. rec. loss:  1.00276847e-02\n",
      "Epoch: 8036 mean train loss:  1.07410022e-02, mean val. rec. loss:  1.00268116e-02\n",
      "Epoch: 8037 mean train loss:  1.07401735e-02, mean val. rec. loss:  1.00259407e-02\n",
      "Epoch: 8038 mean train loss:  1.07393495e-02, mean val. rec. loss:  1.00250630e-02\n",
      "Epoch: 8039 mean train loss:  1.07385246e-02, mean val. rec. loss:  1.00241887e-02\n",
      "Epoch: 8040 mean train loss:  1.07376996e-02, mean val. rec. loss:  1.00233189e-02\n",
      "Epoch: 8041 mean train loss:  1.07368728e-02, mean val. rec. loss:  1.00224458e-02\n",
      "Epoch: 8042 mean train loss:  1.07360441e-02, mean val. rec. loss:  1.00215771e-02\n",
      "Epoch: 8043 mean train loss:  1.07352201e-02, mean val. rec. loss:  1.00207028e-02\n",
      "Epoch: 8044 mean train loss:  1.07343961e-02, mean val. rec. loss:  1.00198308e-02\n",
      "Epoch: 8045 mean train loss:  1.07335684e-02, mean val. rec. loss:  1.00189542e-02\n",
      "Epoch: 8046 mean train loss:  1.07327406e-02, mean val. rec. loss:  1.00180811e-02\n",
      "Epoch: 8047 mean train loss:  1.07319166e-02, mean val. rec. loss:  1.00172090e-02\n",
      "Epoch: 8048 mean train loss:  1.07310935e-02, mean val. rec. loss:  1.00163393e-02\n",
      "Epoch: 8049 mean train loss:  1.07302658e-02, mean val. rec. loss:  1.00154650e-02\n",
      "Epoch: 8050 mean train loss:  1.07294389e-02, mean val. rec. loss:  1.00145964e-02\n",
      "Epoch: 8051 mean train loss:  1.07286159e-02, mean val. rec. loss:  1.00137187e-02\n",
      "Epoch: 8052 mean train loss:  1.07277909e-02, mean val. rec. loss:  1.00128455e-02\n",
      "Epoch: 8053 mean train loss:  1.07269660e-02, mean val. rec. loss:  1.00119791e-02\n",
      "Epoch: 8054 mean train loss:  1.07261401e-02, mean val. rec. loss:  1.00111083e-02\n",
      "Epoch: 8055 mean train loss:  1.07253151e-02, mean val. rec. loss:  1.00102351e-02\n",
      "Epoch: 8056 mean train loss:  1.07244902e-02, mean val. rec. loss:  1.00093597e-02\n",
      "Epoch: 8057 mean train loss:  1.07236662e-02, mean val. rec. loss:  1.00084888e-02\n",
      "Epoch: 8058 mean train loss:  1.07228412e-02, mean val. rec. loss:  1.00076224e-02\n",
      "Epoch: 8059 mean train loss:  1.07220172e-02, mean val. rec. loss:  1.00067538e-02\n",
      "Epoch: 8060 mean train loss:  1.07211932e-02, mean val. rec. loss:  1.00058840e-02\n",
      "Epoch: 8061 mean train loss:  1.07203691e-02, mean val. rec. loss:  1.00050131e-02\n",
      "Epoch: 8062 mean train loss:  1.07195442e-02, mean val. rec. loss:  1.00041388e-02\n",
      "Epoch: 8063 mean train loss:  1.07187239e-02, mean val. rec. loss:  1.00032679e-02\n",
      "Epoch: 8064 mean train loss:  1.07178971e-02, mean val. rec. loss:  1.00023993e-02\n",
      "Epoch: 8065 mean train loss:  1.07170703e-02, mean val. rec. loss:  1.00015307e-02\n",
      "Epoch: 8066 mean train loss:  1.07162490e-02, mean val. rec. loss:  1.00006575e-02\n",
      "Epoch: 8067 mean train loss:  1.07154278e-02, mean val. rec. loss:  9.99979457e-03\n",
      "Epoch: 8068 mean train loss:  1.07146001e-02, mean val. rec. loss:  9.99891687e-03\n",
      "Epoch: 8069 mean train loss:  1.07137733e-02, mean val. rec. loss:  9.99805505e-03\n",
      "Epoch: 8070 mean train loss:  1.07129548e-02, mean val. rec. loss:  9.99718756e-03\n",
      "Epoch: 8071 mean train loss:  1.07121327e-02, mean val. rec. loss:  9.99631326e-03\n",
      "Epoch: 8072 mean train loss:  1.07113087e-02, mean val. rec. loss:  9.99544690e-03\n",
      "Epoch: 8073 mean train loss:  1.07104837e-02, mean val. rec. loss:  9.99457714e-03\n",
      "Epoch: 8074 mean train loss:  1.07096625e-02, mean val. rec. loss:  9.99370738e-03\n",
      "Epoch: 8075 mean train loss:  1.07088422e-02, mean val. rec. loss:  9.99284216e-03\n",
      "Epoch: 8076 mean train loss:  1.07080163e-02, mean val. rec. loss:  9.99197126e-03\n",
      "Epoch: 8077 mean train loss:  1.07071932e-02, mean val. rec. loss:  9.99110264e-03\n",
      "Epoch: 8078 mean train loss:  1.07063720e-02, mean val. rec. loss:  9.99023628e-03\n",
      "Epoch: 8079 mean train loss:  1.07055517e-02, mean val. rec. loss:  9.98936425e-03\n",
      "Epoch: 8080 mean train loss:  1.07047286e-02, mean val. rec. loss:  9.98849903e-03\n",
      "Epoch: 8081 mean train loss:  1.07039046e-02, mean val. rec. loss:  9.98763267e-03\n",
      "Epoch: 8082 mean train loss:  1.07030815e-02, mean val. rec. loss:  9.98676291e-03\n",
      "Epoch: 8083 mean train loss:  1.07022649e-02, mean val. rec. loss:  9.98589995e-03\n",
      "Epoch: 8084 mean train loss:  1.07014428e-02, mean val. rec. loss:  9.98502566e-03\n",
      "Epoch: 8085 mean train loss:  1.07006178e-02, mean val. rec. loss:  9.98416043e-03\n",
      "Epoch: 8086 mean train loss:  1.06997957e-02, mean val. rec. loss:  9.98329861e-03\n",
      "Epoch: 8087 mean train loss:  1.06989772e-02, mean val. rec. loss:  9.98242545e-03\n",
      "Epoch: 8088 mean train loss:  1.06981579e-02, mean val. rec. loss:  9.98155909e-03\n",
      "Epoch: 8089 mean train loss:  1.06973320e-02, mean val. rec. loss:  9.98069047e-03\n",
      "Epoch: 8090 mean train loss:  1.06965098e-02, mean val. rec. loss:  9.97982637e-03\n",
      "Epoch: 8091 mean train loss:  1.06956961e-02, mean val. rec. loss:  9.97896228e-03\n",
      "Epoch: 8092 mean train loss:  1.06948692e-02, mean val. rec. loss:  9.97808912e-03\n",
      "Epoch: 8093 mean train loss:  1.06940499e-02, mean val. rec. loss:  9.97722276e-03\n",
      "Epoch: 8094 mean train loss:  1.06932268e-02, mean val. rec. loss:  9.97635867e-03\n",
      "Epoch: 8095 mean train loss:  1.06924130e-02, mean val. rec. loss:  9.97548778e-03\n",
      "Epoch: 8096 mean train loss:  1.06915843e-02, mean val. rec. loss:  9.97462823e-03\n",
      "Epoch: 8097 mean train loss:  1.06907724e-02, mean val. rec. loss:  9.97376073e-03\n",
      "Epoch: 8098 mean train loss:  1.06899437e-02, mean val. rec. loss:  9.97290345e-03\n",
      "Epoch: 8099 mean train loss:  1.06891318e-02, mean val. rec. loss:  9.97203142e-03\n",
      "Epoch: 8100 mean train loss:  1.06883069e-02, mean val. rec. loss:  9.97116506e-03\n",
      "Epoch: 8101 mean train loss:  1.06874884e-02, mean val. rec. loss:  9.97029870e-03\n",
      "Epoch: 8102 mean train loss:  1.06866663e-02, mean val. rec. loss:  9.96943688e-03\n",
      "Epoch: 8103 mean train loss:  1.06858544e-02, mean val. rec. loss:  9.96857733e-03\n",
      "Epoch: 8104 mean train loss:  1.06850276e-02, mean val. rec. loss:  9.96770870e-03\n",
      "Epoch: 8105 mean train loss:  1.06842101e-02, mean val. rec. loss:  9.96684234e-03\n",
      "Epoch: 8106 mean train loss:  1.06833888e-02, mean val. rec. loss:  9.96597939e-03\n",
      "Epoch: 8107 mean train loss:  1.06825760e-02, mean val. rec. loss:  9.96511870e-03\n",
      "Epoch: 8108 mean train loss:  1.06817520e-02, mean val. rec. loss:  9.96425914e-03\n",
      "Epoch: 8109 mean train loss:  1.06809345e-02, mean val. rec. loss:  9.96338712e-03\n",
      "Epoch: 8110 mean train loss:  1.06801132e-02, mean val. rec. loss:  9.96252870e-03\n",
      "Epoch: 8111 mean train loss:  1.06793004e-02, mean val. rec. loss:  9.96166461e-03\n",
      "Epoch: 8112 mean train loss:  1.06784764e-02, mean val. rec. loss:  9.96080732e-03\n",
      "Epoch: 8113 mean train loss:  1.06776617e-02, mean val. rec. loss:  9.95993869e-03\n",
      "Epoch: 8114 mean train loss:  1.06768386e-02, mean val. rec. loss:  9.95907460e-03\n",
      "Epoch: 8115 mean train loss:  1.06760239e-02, mean val. rec. loss:  9.95820938e-03\n",
      "Epoch: 8116 mean train loss:  1.06752026e-02, mean val. rec. loss:  9.95734982e-03\n",
      "Epoch: 8117 mean train loss:  1.06743889e-02, mean val. rec. loss:  9.95648460e-03\n",
      "Epoch: 8118 mean train loss:  1.06735658e-02, mean val. rec. loss:  9.95562505e-03\n",
      "Epoch: 8119 mean train loss:  1.06727520e-02, mean val. rec. loss:  9.95475869e-03\n",
      "Epoch: 8120 mean train loss:  1.06719308e-02, mean val. rec. loss:  9.95389913e-03\n",
      "Epoch: 8121 mean train loss:  1.06711170e-02, mean val. rec. loss:  9.95303731e-03\n",
      "Epoch: 8122 mean train loss:  1.06702948e-02, mean val. rec. loss:  9.95217436e-03\n",
      "Epoch: 8123 mean train loss:  1.06694848e-02, mean val. rec. loss:  9.95131253e-03\n",
      "Epoch: 8124 mean train loss:  1.06686617e-02, mean val. rec. loss:  9.95044731e-03\n",
      "Epoch: 8125 mean train loss:  1.06678470e-02, mean val. rec. loss:  9.94958095e-03\n",
      "Epoch: 8126 mean train loss:  1.06670276e-02, mean val. rec. loss:  9.94872366e-03\n",
      "Epoch: 8127 mean train loss:  1.06662166e-02, mean val. rec. loss:  9.94786184e-03\n",
      "Epoch: 8128 mean train loss:  1.06653963e-02, mean val. rec. loss:  9.94700002e-03\n",
      "Epoch: 8129 mean train loss:  1.06645742e-02, mean val. rec. loss:  9.94613593e-03\n",
      "Epoch: 8130 mean train loss:  1.06637651e-02, mean val. rec. loss:  9.94527524e-03\n",
      "Epoch: 8131 mean train loss:  1.06629457e-02, mean val. rec. loss:  9.94441796e-03\n",
      "Epoch: 8132 mean train loss:  1.06621319e-02, mean val. rec. loss:  9.94355727e-03\n",
      "Epoch: 8133 mean train loss:  1.06613088e-02, mean val. rec. loss:  9.94269998e-03\n",
      "Epoch: 8134 mean train loss:  1.06604988e-02, mean val. rec. loss:  9.94183362e-03\n",
      "Epoch: 8135 mean train loss:  1.06596813e-02, mean val. rec. loss:  9.94097634e-03\n",
      "Epoch: 8136 mean train loss:  1.06588675e-02, mean val. rec. loss:  9.94011338e-03\n",
      "Epoch: 8137 mean train loss:  1.06580444e-02, mean val. rec. loss:  9.93925383e-03\n",
      "Epoch: 8138 mean train loss:  1.06572362e-02, mean val. rec. loss:  9.93839994e-03\n",
      "Epoch: 8139 mean train loss:  1.06564187e-02, mean val. rec. loss:  9.93753812e-03\n",
      "Epoch: 8140 mean train loss:  1.06556050e-02, mean val. rec. loss:  9.93667403e-03\n",
      "Epoch: 8141 mean train loss:  1.06547828e-02, mean val. rec. loss:  9.93582582e-03\n",
      "Epoch: 8142 mean train loss:  1.06539737e-02, mean val. rec. loss:  9.93496286e-03\n",
      "Epoch: 8143 mean train loss:  1.06531562e-02, mean val. rec. loss:  9.93410444e-03\n",
      "Epoch: 8144 mean train loss:  1.06523415e-02, mean val. rec. loss:  9.93324262e-03\n",
      "Epoch: 8145 mean train loss:  1.06515268e-02, mean val. rec. loss:  9.93238306e-03\n",
      "Epoch: 8146 mean train loss:  1.06507111e-02, mean val. rec. loss:  9.93152804e-03\n",
      "Epoch: 8147 mean train loss:  1.06498973e-02, mean val. rec. loss:  9.93067076e-03\n",
      "Epoch: 8148 mean train loss:  1.06490826e-02, mean val. rec. loss:  9.92980894e-03\n",
      "Epoch: 8149 mean train loss:  1.06482679e-02, mean val. rec. loss:  9.92895732e-03\n",
      "Epoch: 8150 mean train loss:  1.06474532e-02, mean val. rec. loss:  9.92809323e-03\n",
      "Epoch: 8151 mean train loss:  1.06466394e-02, mean val. rec. loss:  9.92723594e-03\n",
      "Epoch: 8152 mean train loss:  1.06458247e-02, mean val. rec. loss:  9.92637752e-03\n",
      "Epoch: 8153 mean train loss:  1.06450110e-02, mean val. rec. loss:  9.92551797e-03\n",
      "Epoch: 8154 mean train loss:  1.06441972e-02, mean val. rec. loss:  9.92466522e-03\n",
      "Epoch: 8155 mean train loss:  1.06433853e-02, mean val. rec. loss:  9.92379999e-03\n",
      "Epoch: 8156 mean train loss:  1.06425696e-02, mean val. rec. loss:  9.92294157e-03\n",
      "Epoch: 8157 mean train loss:  1.06417540e-02, mean val. rec. loss:  9.92209109e-03\n",
      "Epoch: 8158 mean train loss:  1.06409430e-02, mean val. rec. loss:  9.92123154e-03\n",
      "Epoch: 8159 mean train loss:  1.06401292e-02, mean val. rec. loss:  9.92037425e-03\n",
      "Epoch: 8160 mean train loss:  1.06393164e-02, mean val. rec. loss:  9.91951356e-03\n",
      "Epoch: 8161 mean train loss:  1.06385035e-02, mean val. rec. loss:  9.91866535e-03\n",
      "Epoch: 8162 mean train loss:  1.06376907e-02, mean val. rec. loss:  9.91780693e-03\n",
      "Epoch: 8163 mean train loss:  1.06368778e-02, mean val. rec. loss:  9.91695871e-03\n",
      "Epoch: 8164 mean train loss:  1.06360650e-02, mean val. rec. loss:  9.91609576e-03\n",
      "Epoch: 8165 mean train loss:  1.06352549e-02, mean val. rec. loss:  9.91524641e-03\n",
      "Epoch: 8166 mean train loss:  1.06344393e-02, mean val. rec. loss:  9.91437892e-03\n",
      "Epoch: 8167 mean train loss:  1.06336246e-02, mean val. rec. loss:  9.91353297e-03\n",
      "Epoch: 8168 mean train loss:  1.06328136e-02, mean val. rec. loss:  9.91267001e-03\n",
      "Epoch: 8169 mean train loss:  1.06320063e-02, mean val. rec. loss:  9.91181840e-03\n",
      "Epoch: 8170 mean train loss:  1.06311926e-02, mean val. rec. loss:  9.91095771e-03\n",
      "Epoch: 8171 mean train loss:  1.06303779e-02, mean val. rec. loss:  9.91010156e-03\n",
      "Epoch: 8172 mean train loss:  1.06295697e-02, mean val. rec. loss:  9.90924767e-03\n",
      "Epoch: 8173 mean train loss:  1.06287596e-02, mean val. rec. loss:  9.90839832e-03\n",
      "Epoch: 8174 mean train loss:  1.06279458e-02, mean val. rec. loss:  9.90754217e-03\n",
      "Epoch: 8175 mean train loss:  1.06271311e-02, mean val. rec. loss:  9.90669169e-03\n",
      "Epoch: 8176 mean train loss:  1.06263220e-02, mean val. rec. loss:  9.90583440e-03\n",
      "Epoch: 8177 mean train loss:  1.06255138e-02, mean val. rec. loss:  9.90497938e-03\n",
      "Epoch: 8178 mean train loss:  1.06246982e-02, mean val. rec. loss:  9.90413117e-03\n",
      "Epoch: 8179 mean train loss:  1.06238918e-02, mean val. rec. loss:  9.90327615e-03\n",
      "Epoch: 8180 mean train loss:  1.06230762e-02, mean val. rec. loss:  9.90242113e-03\n",
      "Epoch: 8181 mean train loss:  1.06222689e-02, mean val. rec. loss:  9.90156385e-03\n",
      "Epoch: 8182 mean train loss:  1.06214542e-02, mean val. rec. loss:  9.90071563e-03\n",
      "Epoch: 8183 mean train loss:  1.06206488e-02, mean val. rec. loss:  9.89986061e-03\n",
      "Epoch: 8184 mean train loss:  1.06198332e-02, mean val. rec. loss:  9.89900900e-03\n",
      "Epoch: 8185 mean train loss:  1.06190278e-02, mean val. rec. loss:  9.89815284e-03\n",
      "Epoch: 8186 mean train loss:  1.06182122e-02, mean val. rec. loss:  9.89730123e-03\n",
      "Epoch: 8187 mean train loss:  1.06174077e-02, mean val. rec. loss:  9.89644734e-03\n",
      "Epoch: 8188 mean train loss:  1.06165949e-02, mean val. rec. loss:  9.89559119e-03\n",
      "Epoch: 8189 mean train loss:  1.06157839e-02, mean val. rec. loss:  9.89474071e-03\n",
      "Epoch: 8190 mean train loss:  1.06149747e-02, mean val. rec. loss:  9.89388682e-03\n",
      "Epoch: 8191 mean train loss:  1.06141684e-02, mean val. rec. loss:  9.89302954e-03\n",
      "Epoch: 8192 mean train loss:  1.06133565e-02, mean val. rec. loss:  9.89217225e-03\n",
      "Epoch: 8193 mean train loss:  1.06125446e-02, mean val. rec. loss:  9.89132631e-03\n",
      "Epoch: 8194 mean train loss:  1.06117373e-02, mean val. rec. loss:  9.89047356e-03\n",
      "Epoch: 8195 mean train loss:  1.06109291e-02, mean val. rec. loss:  9.88962647e-03\n",
      "Epoch: 8196 mean train loss:  1.06101144e-02, mean val. rec. loss:  9.88877259e-03\n",
      "Epoch: 8197 mean train loss:  1.06093062e-02, mean val. rec. loss:  9.88792664e-03\n",
      "Epoch: 8198 mean train loss:  1.06085008e-02, mean val. rec. loss:  9.88708297e-03\n",
      "Epoch: 8199 mean train loss:  1.06076936e-02, mean val. rec. loss:  9.88622568e-03\n",
      "Epoch: 8200 mean train loss:  1.06068798e-02, mean val. rec. loss:  9.88537633e-03\n",
      "Epoch: 8201 mean train loss:  1.06060744e-02, mean val. rec. loss:  9.88452245e-03\n",
      "Epoch: 8202 mean train loss:  1.06052644e-02, mean val. rec. loss:  9.88366856e-03\n",
      "Epoch: 8203 mean train loss:  1.06044562e-02, mean val. rec. loss:  9.88281921e-03\n",
      "Epoch: 8204 mean train loss:  1.06036498e-02, mean val. rec. loss:  9.88196419e-03\n",
      "Epoch: 8205 mean train loss:  1.06028398e-02, mean val. rec. loss:  9.88111258e-03\n",
      "Epoch: 8206 mean train loss:  1.06020307e-02, mean val. rec. loss:  9.88026663e-03\n",
      "Epoch: 8207 mean train loss:  1.06012253e-02, mean val. rec. loss:  9.87941728e-03\n",
      "Epoch: 8208 mean train loss:  1.06004189e-02, mean val. rec. loss:  9.87856907e-03\n",
      "Epoch: 8209 mean train loss:  1.05996089e-02, mean val. rec. loss:  9.87771405e-03\n",
      "Epoch: 8210 mean train loss:  1.05988007e-02, mean val. rec. loss:  9.87686584e-03\n",
      "Epoch: 8211 mean train loss:  1.05979944e-02, mean val. rec. loss:  9.87602102e-03\n",
      "Epoch: 8212 mean train loss:  1.05971899e-02, mean val. rec. loss:  9.87516714e-03\n",
      "Epoch: 8213 mean train loss:  1.05963798e-02, mean val. rec. loss:  9.87432119e-03\n",
      "Epoch: 8214 mean train loss:  1.05955707e-02, mean val. rec. loss:  9.87347525e-03\n",
      "Epoch: 8215 mean train loss:  1.05947653e-02, mean val. rec. loss:  9.87262817e-03\n",
      "Epoch: 8216 mean train loss:  1.05939627e-02, mean val. rec. loss:  9.87178449e-03\n",
      "Epoch: 8217 mean train loss:  1.05931517e-02, mean val. rec. loss:  9.87092607e-03\n",
      "Epoch: 8218 mean train loss:  1.05923408e-02, mean val. rec. loss:  9.87008012e-03\n",
      "Epoch: 8219 mean train loss:  1.05915400e-02, mean val. rec. loss:  9.86923191e-03\n",
      "Epoch: 8220 mean train loss:  1.05907374e-02, mean val. rec. loss:  9.86838142e-03\n",
      "Epoch: 8221 mean train loss:  1.05899274e-02, mean val. rec. loss:  9.86753094e-03\n",
      "Epoch: 8222 mean train loss:  1.05891154e-02, mean val. rec. loss:  9.86668499e-03\n",
      "Epoch: 8223 mean train loss:  1.05883128e-02, mean val. rec. loss:  9.86583451e-03\n",
      "Epoch: 8224 mean train loss:  1.05875102e-02, mean val. rec. loss:  9.86499083e-03\n",
      "Epoch: 8225 mean train loss:  1.05867020e-02, mean val. rec. loss:  9.86413808e-03\n",
      "Epoch: 8226 mean train loss:  1.05858939e-02, mean val. rec. loss:  9.86328987e-03\n",
      "Epoch: 8227 mean train loss:  1.05850912e-02, mean val. rec. loss:  9.86243939e-03\n",
      "Epoch: 8228 mean train loss:  1.05842877e-02, mean val. rec. loss:  9.86159004e-03\n",
      "Epoch: 8229 mean train loss:  1.05834805e-02, mean val. rec. loss:  9.86074296e-03\n",
      "Epoch: 8230 mean train loss:  1.05826732e-02, mean val. rec. loss:  9.85989701e-03\n",
      "Epoch: 8231 mean train loss:  1.05818697e-02, mean val. rec. loss:  9.85905220e-03\n",
      "Epoch: 8232 mean train loss:  1.05810671e-02, mean val. rec. loss:  9.85820965e-03\n",
      "Epoch: 8233 mean train loss:  1.05802589e-02, mean val. rec. loss:  9.85736030e-03\n",
      "Epoch: 8234 mean train loss:  1.05794563e-02, mean val. rec. loss:  9.85651209e-03\n",
      "Epoch: 8235 mean train loss:  1.05786499e-02, mean val. rec. loss:  9.85567181e-03\n",
      "Epoch: 8236 mean train loss:  1.05778455e-02, mean val. rec. loss:  9.85483154e-03\n",
      "Epoch: 8237 mean train loss:  1.05770410e-02, mean val. rec. loss:  9.85397992e-03\n",
      "Epoch: 8238 mean train loss:  1.05762347e-02, mean val. rec. loss:  9.85313171e-03\n",
      "Epoch: 8239 mean train loss:  1.05754339e-02, mean val. rec. loss:  9.85229030e-03\n",
      "Epoch: 8240 mean train loss:  1.05746313e-02, mean val. rec. loss:  9.85144548e-03\n",
      "Epoch: 8241 mean train loss:  1.05738278e-02, mean val. rec. loss:  9.85059160e-03\n",
      "Epoch: 8242 mean train loss:  1.05730196e-02, mean val. rec. loss:  9.84974565e-03\n",
      "Epoch: 8243 mean train loss:  1.05722161e-02, mean val. rec. loss:  9.84889971e-03\n",
      "Epoch: 8244 mean train loss:  1.05714172e-02, mean val. rec. loss:  9.84805376e-03\n",
      "Epoch: 8245 mean train loss:  1.05706109e-02, mean val. rec. loss:  9.84721235e-03\n",
      "Epoch: 8246 mean train loss:  1.05698055e-02, mean val. rec. loss:  9.84636980e-03\n",
      "Epoch: 8247 mean train loss:  1.05690038e-02, mean val. rec. loss:  9.84552499e-03\n",
      "Epoch: 8248 mean train loss:  1.05682030e-02, mean val. rec. loss:  9.84467564e-03\n",
      "Epoch: 8249 mean train loss:  1.05673986e-02, mean val. rec. loss:  9.84383763e-03\n",
      "Epoch: 8250 mean train loss:  1.05665932e-02, mean val. rec. loss:  9.84300189e-03\n",
      "Epoch: 8251 mean train loss:  1.05657924e-02, mean val. rec. loss:  9.84215028e-03\n",
      "Epoch: 8252 mean train loss:  1.05649898e-02, mean val. rec. loss:  9.84130206e-03\n",
      "Epoch: 8253 mean train loss:  1.05641872e-02, mean val. rec. loss:  9.84046405e-03\n",
      "Epoch: 8254 mean train loss:  1.05633856e-02, mean val. rec. loss:  9.83962378e-03\n",
      "Epoch: 8255 mean train loss:  1.05625820e-02, mean val. rec. loss:  9.83877783e-03\n",
      "Epoch: 8256 mean train loss:  1.05617803e-02, mean val. rec. loss:  9.83792508e-03\n",
      "Epoch: 8257 mean train loss:  1.05609777e-02, mean val. rec. loss:  9.83708821e-03\n",
      "Epoch: 8258 mean train loss:  1.05601761e-02, mean val. rec. loss:  9.83624906e-03\n",
      "Epoch: 8259 mean train loss:  1.05593744e-02, mean val. rec. loss:  9.83540312e-03\n",
      "Epoch: 8260 mean train loss:  1.05585727e-02, mean val. rec. loss:  9.83455717e-03\n",
      "Epoch: 8261 mean train loss:  1.05577710e-02, mean val. rec. loss:  9.83371803e-03\n",
      "Epoch: 8262 mean train loss:  1.05569684e-02, mean val. rec. loss:  9.83287888e-03\n",
      "Epoch: 8263 mean train loss:  1.05561677e-02, mean val. rec. loss:  9.83203407e-03\n",
      "Epoch: 8264 mean train loss:  1.05553670e-02, mean val. rec. loss:  9.83119039e-03\n",
      "Epoch: 8265 mean train loss:  1.05545653e-02, mean val. rec. loss:  9.83035238e-03\n",
      "Epoch: 8266 mean train loss:  1.05537636e-02, mean val. rec. loss:  9.82951551e-03\n",
      "Epoch: 8267 mean train loss:  1.05529629e-02, mean val. rec. loss:  9.82866162e-03\n",
      "Epoch: 8268 mean train loss:  1.05521593e-02, mean val. rec. loss:  9.82781568e-03\n",
      "Epoch: 8269 mean train loss:  1.05513614e-02, mean val. rec. loss:  9.82698221e-03\n",
      "Epoch: 8270 mean train loss:  1.05505625e-02, mean val. rec. loss:  9.82614420e-03\n",
      "Epoch: 8271 mean train loss:  1.05497599e-02, mean val. rec. loss:  9.82529258e-03\n",
      "Epoch: 8272 mean train loss:  1.05489573e-02, mean val. rec. loss:  9.82445230e-03\n",
      "Epoch: 8273 mean train loss:  1.05481566e-02, mean val. rec. loss:  9.82361089e-03\n",
      "Epoch: 8274 mean train loss:  1.05473614e-02, mean val. rec. loss:  9.82277969e-03\n",
      "Epoch: 8275 mean train loss:  1.05465607e-02, mean val. rec. loss:  9.82193828e-03\n",
      "Epoch: 8276 mean train loss:  1.05457571e-02, mean val. rec. loss:  9.82109460e-03\n",
      "Epoch: 8277 mean train loss:  1.05449592e-02, mean val. rec. loss:  9.82025092e-03\n",
      "Epoch: 8278 mean train loss:  1.05441584e-02, mean val. rec. loss:  9.81940951e-03\n",
      "Epoch: 8279 mean train loss:  1.05433605e-02, mean val. rec. loss:  9.81857490e-03\n",
      "Epoch: 8280 mean train loss:  1.05425607e-02, mean val. rec. loss:  9.81772896e-03\n",
      "Epoch: 8281 mean train loss:  1.05417618e-02, mean val. rec. loss:  9.81688755e-03\n",
      "Epoch: 8282 mean train loss:  1.05409620e-02, mean val. rec. loss:  9.81604614e-03\n",
      "Epoch: 8283 mean train loss:  1.05401640e-02, mean val. rec. loss:  9.81520472e-03\n",
      "Epoch: 8284 mean train loss:  1.05393596e-02, mean val. rec. loss:  9.81437012e-03\n",
      "Epoch: 8285 mean train loss:  1.05385681e-02, mean val. rec. loss:  9.81353438e-03\n",
      "Epoch: 8286 mean train loss:  1.05377655e-02, mean val. rec. loss:  9.81269864e-03\n",
      "Epoch: 8287 mean train loss:  1.05369695e-02, mean val. rec. loss:  9.81185156e-03\n",
      "Epoch: 8288 mean train loss:  1.05361650e-02, mean val. rec. loss:  9.81102489e-03\n",
      "Epoch: 8289 mean train loss:  1.05353726e-02, mean val. rec. loss:  9.81018007e-03\n",
      "Epoch: 8290 mean train loss:  1.05345691e-02, mean val. rec. loss:  9.80933640e-03\n",
      "Epoch: 8291 mean train loss:  1.05337749e-02, mean val. rec. loss:  9.80849725e-03\n",
      "Epoch: 8292 mean train loss:  1.05329732e-02, mean val. rec. loss:  9.80766378e-03\n",
      "Epoch: 8293 mean train loss:  1.05321771e-02, mean val. rec. loss:  9.80682691e-03\n",
      "Epoch: 8294 mean train loss:  1.05313773e-02, mean val. rec. loss:  9.80598436e-03\n",
      "Epoch: 8295 mean train loss:  1.05305831e-02, mean val. rec. loss:  9.80514749e-03\n",
      "Epoch: 8296 mean train loss:  1.05297861e-02, mean val. rec. loss:  9.80430834e-03\n",
      "Epoch: 8297 mean train loss:  1.05289834e-02, mean val. rec. loss:  9.80346580e-03\n",
      "Epoch: 8298 mean train loss:  1.05281864e-02, mean val. rec. loss:  9.80263459e-03\n",
      "Epoch: 8299 mean train loss:  1.05273931e-02, mean val. rec. loss:  9.80179432e-03\n",
      "Epoch: 8300 mean train loss:  1.05265961e-02, mean val. rec. loss:  9.80095291e-03\n",
      "Epoch: 8301 mean train loss:  1.05257954e-02, mean val. rec. loss:  9.80012737e-03\n",
      "Epoch: 8302 mean train loss:  1.05249974e-02, mean val. rec. loss:  9.79928936e-03\n",
      "Epoch: 8303 mean train loss:  1.05242051e-02, mean val. rec. loss:  9.79844795e-03\n",
      "Epoch: 8304 mean train loss:  1.05234080e-02, mean val. rec. loss:  9.79760994e-03\n",
      "Epoch: 8305 mean train loss:  1.05226064e-02, mean val. rec. loss:  9.79677194e-03\n",
      "Epoch: 8306 mean train loss:  1.05218112e-02, mean val. rec. loss:  9.79593960e-03\n",
      "Epoch: 8307 mean train loss:  1.05210179e-02, mean val. rec. loss:  9.79510612e-03\n",
      "Epoch: 8308 mean train loss:  1.05202218e-02, mean val. rec. loss:  9.79426698e-03\n",
      "Epoch: 8309 mean train loss:  1.05194230e-02, mean val. rec. loss:  9.79343237e-03\n",
      "Epoch: 8310 mean train loss:  1.05186297e-02, mean val. rec. loss:  9.79259437e-03\n",
      "Epoch: 8311 mean train loss:  1.05178354e-02, mean val. rec. loss:  9.79175862e-03\n",
      "Epoch: 8312 mean train loss:  1.05170375e-02, mean val. rec. loss:  9.79091948e-03\n",
      "Epoch: 8313 mean train loss:  1.05162386e-02, mean val. rec. loss:  9.79008488e-03\n",
      "Epoch: 8314 mean train loss:  1.05154472e-02, mean val. rec. loss:  9.78924913e-03\n",
      "Epoch: 8315 mean train loss:  1.05146520e-02, mean val. rec. loss:  9.78841339e-03\n",
      "Epoch: 8316 mean train loss:  1.05138550e-02, mean val. rec. loss:  9.78757425e-03\n",
      "Epoch: 8317 mean train loss:  1.05130580e-02, mean val. rec. loss:  9.78674872e-03\n",
      "Epoch: 8318 mean train loss:  1.05122638e-02, mean val. rec. loss:  9.78590844e-03\n",
      "Epoch: 8319 mean train loss:  1.05114714e-02, mean val. rec. loss:  9.78507270e-03\n",
      "Epoch: 8320 mean train loss:  1.05106735e-02, mean val. rec. loss:  9.78423356e-03\n",
      "Epoch: 8321 mean train loss:  1.05098755e-02, mean val. rec. loss:  9.78339215e-03\n",
      "Epoch: 8322 mean train loss:  1.05090804e-02, mean val. rec. loss:  9.78256321e-03\n",
      "Epoch: 8323 mean train loss:  1.05082899e-02, mean val. rec. loss:  9.78173200e-03\n",
      "Epoch: 8324 mean train loss:  1.05074966e-02, mean val. rec. loss:  9.78089513e-03\n",
      "Epoch: 8325 mean train loss:  1.05067005e-02, mean val. rec. loss:  9.78005825e-03\n",
      "Epoch: 8326 mean train loss:  1.05059035e-02, mean val. rec. loss:  9.77922592e-03\n",
      "Epoch: 8327 mean train loss:  1.05051120e-02, mean val. rec. loss:  9.77839358e-03\n",
      "Epoch: 8328 mean train loss:  1.05043197e-02, mean val. rec. loss:  9.77756351e-03\n",
      "Epoch: 8329 mean train loss:  1.05035236e-02, mean val. rec. loss:  9.77672663e-03\n",
      "Epoch: 8330 mean train loss:  1.05027247e-02, mean val. rec. loss:  9.77589543e-03\n",
      "Epoch: 8331 mean train loss:  1.05019323e-02, mean val. rec. loss:  9.77505969e-03\n",
      "Epoch: 8332 mean train loss:  1.05011418e-02, mean val. rec. loss:  9.77422508e-03\n",
      "Epoch: 8333 mean train loss:  1.05003485e-02, mean val. rec. loss:  9.77338820e-03\n",
      "Epoch: 8334 mean train loss:  1.04995515e-02, mean val. rec. loss:  9.77255360e-03\n",
      "Epoch: 8335 mean train loss:  1.04987573e-02, mean val. rec. loss:  9.77172466e-03\n",
      "Epoch: 8336 mean train loss:  1.04979668e-02, mean val. rec. loss:  9.77089232e-03\n",
      "Epoch: 8337 mean train loss:  1.04971707e-02, mean val. rec. loss:  9.77006338e-03\n",
      "Epoch: 8338 mean train loss:  1.04963765e-02, mean val. rec. loss:  9.76923558e-03\n",
      "Epoch: 8339 mean train loss:  1.04955851e-02, mean val. rec. loss:  9.76840551e-03\n",
      "Epoch: 8340 mean train loss:  1.04947955e-02, mean val. rec. loss:  9.76756977e-03\n",
      "Epoch: 8341 mean train loss:  1.04939966e-02, mean val. rec. loss:  9.76673857e-03\n",
      "Epoch: 8342 mean train loss:  1.04932089e-02, mean val. rec. loss:  9.76590396e-03\n",
      "Epoch: 8343 mean train loss:  1.04924119e-02, mean val. rec. loss:  9.76507616e-03\n",
      "Epoch: 8344 mean train loss:  1.04916242e-02, mean val. rec. loss:  9.76423928e-03\n",
      "Epoch: 8345 mean train loss:  1.04908290e-02, mean val. rec. loss:  9.76340581e-03\n",
      "Epoch: 8346 mean train loss:  1.04900413e-02, mean val. rec. loss:  9.76257347e-03\n",
      "Epoch: 8347 mean train loss:  1.04892462e-02, mean val. rec. loss:  9.76174340e-03\n",
      "Epoch: 8348 mean train loss:  1.04884575e-02, mean val. rec. loss:  9.76091333e-03\n",
      "Epoch: 8349 mean train loss:  1.04876614e-02, mean val. rec. loss:  9.76008212e-03\n",
      "Epoch: 8350 mean train loss:  1.04868737e-02, mean val. rec. loss:  9.75924752e-03\n",
      "Epoch: 8351 mean train loss:  1.04860786e-02, mean val. rec. loss:  9.75842085e-03\n",
      "Epoch: 8352 mean train loss:  1.04852909e-02, mean val. rec. loss:  9.75758964e-03\n",
      "Epoch: 8353 mean train loss:  1.04844976e-02, mean val. rec. loss:  9.75675730e-03\n",
      "Epoch: 8354 mean train loss:  1.04837043e-02, mean val. rec. loss:  9.75592723e-03\n",
      "Epoch: 8355 mean train loss:  1.04829147e-02, mean val. rec. loss:  9.75509603e-03\n",
      "Epoch: 8356 mean train loss:  1.04821261e-02, mean val. rec. loss:  9.75426936e-03\n",
      "Epoch: 8357 mean train loss:  1.04813346e-02, mean val. rec. loss:  9.75343589e-03\n",
      "Epoch: 8358 mean train loss:  1.04805414e-02, mean val. rec. loss:  9.75260695e-03\n",
      "Epoch: 8359 mean train loss:  1.04797536e-02, mean val. rec. loss:  9.75177801e-03\n",
      "Epoch: 8360 mean train loss:  1.04789650e-02, mean val. rec. loss:  9.75094907e-03\n",
      "Epoch: 8361 mean train loss:  1.04781717e-02, mean val. rec. loss:  9.75011560e-03\n",
      "Epoch: 8362 mean train loss:  1.04773784e-02, mean val. rec. loss:  9.74928893e-03\n",
      "Epoch: 8363 mean train loss:  1.04765907e-02, mean val. rec. loss:  9.74845886e-03\n",
      "Epoch: 8364 mean train loss:  1.04758030e-02, mean val. rec. loss:  9.74763219e-03\n",
      "Epoch: 8365 mean train loss:  1.04750106e-02, mean val. rec. loss:  9.74680552e-03\n",
      "Epoch: 8366 mean train loss:  1.04742183e-02, mean val. rec. loss:  9.74597205e-03\n",
      "Epoch: 8367 mean train loss:  1.04734306e-02, mean val. rec. loss:  9.74514311e-03\n",
      "Epoch: 8368 mean train loss:  1.04726429e-02, mean val. rec. loss:  9.74431304e-03\n",
      "Epoch: 8369 mean train loss:  1.04718505e-02, mean val. rec. loss:  9.74348637e-03\n",
      "Epoch: 8370 mean train loss:  1.04710619e-02, mean val. rec. loss:  9.74265290e-03\n",
      "Epoch: 8371 mean train loss:  1.04702723e-02, mean val. rec. loss:  9.74182623e-03\n",
      "Epoch: 8372 mean train loss:  1.04694827e-02, mean val. rec. loss:  9.74099276e-03\n",
      "Epoch: 8373 mean train loss:  1.04686932e-02, mean val. rec. loss:  9.74016496e-03\n",
      "Epoch: 8374 mean train loss:  1.04679045e-02, mean val. rec. loss:  9.73933602e-03\n",
      "Epoch: 8375 mean train loss:  1.04671150e-02, mean val. rec. loss:  9.73851162e-03\n",
      "Epoch: 8376 mean train loss:  1.04663263e-02, mean val. rec. loss:  9.73767928e-03\n",
      "Epoch: 8377 mean train loss:  1.04655377e-02, mean val. rec. loss:  9.73685488e-03\n",
      "Epoch: 8378 mean train loss:  1.04647490e-02, mean val. rec. loss:  9.73602367e-03\n",
      "Epoch: 8379 mean train loss:  1.04639604e-02, mean val. rec. loss:  9.73520268e-03\n",
      "Epoch: 8380 mean train loss:  1.04631708e-02, mean val. rec. loss:  9.73437261e-03\n",
      "Epoch: 8381 mean train loss:  1.04623831e-02, mean val. rec. loss:  9.73354480e-03\n",
      "Epoch: 8382 mean train loss:  1.04615936e-02, mean val. rec. loss:  9.73271587e-03\n",
      "Epoch: 8383 mean train loss:  1.04608049e-02, mean val. rec. loss:  9.73189373e-03\n",
      "Epoch: 8384 mean train loss:  1.04600144e-02, mean val. rec. loss:  9.73107160e-03\n",
      "Epoch: 8385 mean train loss:  1.04592314e-02, mean val. rec. loss:  9.73024266e-03\n",
      "Epoch: 8386 mean train loss:  1.04584418e-02, mean val. rec. loss:  9.72941599e-03\n",
      "Epoch: 8387 mean train loss:  1.04576504e-02, mean val. rec. loss:  9.72858479e-03\n",
      "Epoch: 8388 mean train loss:  1.04568645e-02, mean val. rec. loss:  9.72775812e-03\n",
      "Epoch: 8389 mean train loss:  1.04560787e-02, mean val. rec. loss:  9.72693258e-03\n",
      "Epoch: 8390 mean train loss:  1.04552891e-02, mean val. rec. loss:  9.72610365e-03\n",
      "Epoch: 8391 mean train loss:  1.04544995e-02, mean val. rec. loss:  9.72527244e-03\n",
      "Epoch: 8392 mean train loss:  1.04537118e-02, mean val. rec. loss:  9.72444804e-03\n",
      "Epoch: 8393 mean train loss:  1.04529297e-02, mean val. rec. loss:  9.72362251e-03\n",
      "Epoch: 8394 mean train loss:  1.04521420e-02, mean val. rec. loss:  9.72279470e-03\n",
      "Epoch: 8395 mean train loss:  1.04513515e-02, mean val. rec. loss:  9.72197370e-03\n",
      "Epoch: 8396 mean train loss:  1.04505647e-02, mean val. rec. loss:  9.72114817e-03\n",
      "Epoch: 8397 mean train loss:  1.04497808e-02, mean val. rec. loss:  9.72032150e-03\n",
      "Epoch: 8398 mean train loss:  1.04489912e-02, mean val. rec. loss:  9.71949483e-03\n",
      "Epoch: 8399 mean train loss:  1.04482016e-02, mean val. rec. loss:  9.71868064e-03\n",
      "Epoch: 8400 mean train loss:  1.04474195e-02, mean val. rec. loss:  9.71785057e-03\n",
      "Epoch: 8401 mean train loss:  1.04466374e-02, mean val. rec. loss:  9.71702730e-03\n",
      "Epoch: 8402 mean train loss:  1.04458478e-02, mean val. rec. loss:  9.71620176e-03\n",
      "Epoch: 8403 mean train loss:  1.04450582e-02, mean val. rec. loss:  9.71538303e-03\n",
      "Epoch: 8404 mean train loss:  1.04442761e-02, mean val. rec. loss:  9.71455977e-03\n",
      "Epoch: 8405 mean train loss:  1.04434940e-02, mean val. rec. loss:  9.71372403e-03\n",
      "Epoch: 8406 mean train loss:  1.04427016e-02, mean val. rec. loss:  9.71289962e-03\n",
      "Epoch: 8407 mean train loss:  1.04419205e-02, mean val. rec. loss:  9.71207409e-03\n",
      "Epoch: 8408 mean train loss:  1.04411309e-02, mean val. rec. loss:  9.71125536e-03\n",
      "Epoch: 8409 mean train loss:  1.04403469e-02, mean val. rec. loss:  9.71042982e-03\n",
      "Epoch: 8410 mean train loss:  1.04395592e-02, mean val. rec. loss:  9.70960429e-03\n",
      "Epoch: 8411 mean train loss:  1.04387808e-02, mean val. rec. loss:  9.70878216e-03\n",
      "Epoch: 8412 mean train loss:  1.04379894e-02, mean val. rec. loss:  9.70796229e-03\n",
      "Epoch: 8413 mean train loss:  1.04372063e-02, mean val. rec. loss:  9.70714129e-03\n",
      "Epoch: 8414 mean train loss:  1.04364195e-02, mean val. rec. loss:  9.70631576e-03\n",
      "Epoch: 8415 mean train loss:  1.04356411e-02, mean val. rec. loss:  9.70548342e-03\n",
      "Epoch: 8416 mean train loss:  1.04348497e-02, mean val. rec. loss:  9.70466695e-03\n",
      "Epoch: 8417 mean train loss:  1.04340676e-02, mean val. rec. loss:  9.70384936e-03\n",
      "Epoch: 8418 mean train loss:  1.04332808e-02, mean val. rec. loss:  9.70302836e-03\n",
      "Epoch: 8419 mean train loss:  1.04325015e-02, mean val. rec. loss:  9.70220622e-03\n",
      "Epoch: 8420 mean train loss:  1.04317166e-02, mean val. rec. loss:  9.70137162e-03\n",
      "Epoch: 8421 mean train loss:  1.04309289e-02, mean val. rec. loss:  9.70055062e-03\n",
      "Epoch: 8422 mean train loss:  1.04301440e-02, mean val. rec. loss:  9.69972849e-03\n",
      "Epoch: 8423 mean train loss:  1.04293628e-02, mean val. rec. loss:  9.69890635e-03\n",
      "Epoch: 8424 mean train loss:  1.04285816e-02, mean val. rec. loss:  9.69807855e-03\n",
      "Epoch: 8425 mean train loss:  1.04277939e-02, mean val. rec. loss:  9.69725302e-03\n",
      "Epoch: 8426 mean train loss:  1.04270062e-02, mean val. rec. loss:  9.69643655e-03\n",
      "Epoch: 8427 mean train loss:  1.04262278e-02, mean val. rec. loss:  9.69561782e-03\n",
      "Epoch: 8428 mean train loss:  1.04254475e-02, mean val. rec. loss:  9.69479796e-03\n",
      "Epoch: 8429 mean train loss:  1.04246598e-02, mean val. rec. loss:  9.69397355e-03\n",
      "Epoch: 8430 mean train loss:  1.04238721e-02, mean val. rec. loss:  9.69315596e-03\n",
      "Epoch: 8431 mean train loss:  1.04230928e-02, mean val. rec. loss:  9.69233156e-03\n",
      "Epoch: 8432 mean train loss:  1.04223144e-02, mean val. rec. loss:  9.69150829e-03\n",
      "Epoch: 8433 mean train loss:  1.04215276e-02, mean val. rec. loss:  9.69068616e-03\n",
      "Epoch: 8434 mean train loss:  1.04207399e-02, mean val. rec. loss:  9.68987083e-03\n",
      "Epoch: 8435 mean train loss:  1.04199615e-02, mean val. rec. loss:  9.68904869e-03\n",
      "Epoch: 8436 mean train loss:  1.04191822e-02, mean val. rec. loss:  9.68822883e-03\n",
      "Epoch: 8437 mean train loss:  1.04183954e-02, mean val. rec. loss:  9.68740329e-03\n",
      "Epoch: 8438 mean train loss:  1.04176077e-02, mean val. rec. loss:  9.68658456e-03\n",
      "Epoch: 8439 mean train loss:  1.04168284e-02, mean val. rec. loss:  9.68576130e-03\n",
      "Epoch: 8440 mean train loss:  1.04160490e-02, mean val. rec. loss:  9.68493123e-03\n",
      "Epoch: 8441 mean train loss:  1.04152641e-02, mean val. rec. loss:  9.68411023e-03\n",
      "Epoch: 8442 mean train loss:  1.04144792e-02, mean val. rec. loss:  9.68329490e-03\n",
      "Epoch: 8443 mean train loss:  1.04137008e-02, mean val. rec. loss:  9.68247730e-03\n",
      "Epoch: 8444 mean train loss:  1.04129206e-02, mean val. rec. loss:  9.68165744e-03\n",
      "Epoch: 8445 mean train loss:  1.04121366e-02, mean val. rec. loss:  9.68083077e-03\n",
      "Epoch: 8446 mean train loss:  1.04113526e-02, mean val. rec. loss:  9.68001430e-03\n",
      "Epoch: 8447 mean train loss:  1.04105742e-02, mean val. rec. loss:  9.67919784e-03\n",
      "Epoch: 8448 mean train loss:  1.04097884e-02, mean val. rec. loss:  9.67838024e-03\n",
      "Epoch: 8449 mean train loss:  1.04090118e-02, mean val. rec. loss:  9.67756038e-03\n",
      "Epoch: 8450 mean train loss:  1.04082260e-02, mean val. rec. loss:  9.67674165e-03\n",
      "Epoch: 8451 mean train loss:  1.04074467e-02, mean val. rec. loss:  9.67592178e-03\n",
      "Epoch: 8452 mean train loss:  1.04066683e-02, mean val. rec. loss:  9.67509965e-03\n",
      "Epoch: 8453 mean train loss:  1.04058871e-02, mean val. rec. loss:  9.67427978e-03\n",
      "Epoch: 8454 mean train loss:  1.04051059e-02, mean val. rec. loss:  9.67345992e-03\n",
      "Epoch: 8455 mean train loss:  1.04043247e-02, mean val. rec. loss:  9.67264232e-03\n",
      "Epoch: 8456 mean train loss:  1.04035426e-02, mean val. rec. loss:  9.67182472e-03\n",
      "Epoch: 8457 mean train loss:  1.04027642e-02, mean val. rec. loss:  9.67100146e-03\n",
      "Epoch: 8458 mean train loss:  1.04019858e-02, mean val. rec. loss:  9.67018273e-03\n",
      "Epoch: 8459 mean train loss:  1.04011999e-02, mean val. rec. loss:  9.66936173e-03\n",
      "Epoch: 8460 mean train loss:  1.04004197e-02, mean val. rec. loss:  9.66854526e-03\n",
      "Epoch: 8461 mean train loss:  1.03996441e-02, mean val. rec. loss:  9.66772540e-03\n",
      "Epoch: 8462 mean train loss:  1.03988629e-02, mean val. rec. loss:  9.66691574e-03\n",
      "Epoch: 8463 mean train loss:  1.03980873e-02, mean val. rec. loss:  9.66609134e-03\n",
      "Epoch: 8464 mean train loss:  1.03973042e-02, mean val. rec. loss:  9.66527941e-03\n",
      "Epoch: 8465 mean train loss:  1.03965221e-02, mean val. rec. loss:  9.66446068e-03\n",
      "Epoch: 8466 mean train loss:  1.03957437e-02, mean val. rec. loss:  9.66364308e-03\n",
      "Epoch: 8467 mean train loss:  1.03949616e-02, mean val. rec. loss:  9.66283116e-03\n",
      "Epoch: 8468 mean train loss:  1.03941869e-02, mean val. rec. loss:  9.66200902e-03\n",
      "Epoch: 8469 mean train loss:  1.03934048e-02, mean val. rec. loss:  9.66119029e-03\n",
      "Epoch: 8470 mean train loss:  1.03926199e-02, mean val. rec. loss:  9.66037610e-03\n",
      "Epoch: 8471 mean train loss:  1.03918480e-02, mean val. rec. loss:  9.65955737e-03\n",
      "Epoch: 8472 mean train loss:  1.03910659e-02, mean val. rec. loss:  9.65874430e-03\n",
      "Epoch: 8473 mean train loss:  1.03902894e-02, mean val. rec. loss:  9.65792217e-03\n",
      "Epoch: 8474 mean train loss:  1.03895035e-02, mean val. rec. loss:  9.65711705e-03\n",
      "Epoch: 8475 mean train loss:  1.03887316e-02, mean val. rec. loss:  9.65629265e-03\n",
      "Epoch: 8476 mean train loss:  1.03879486e-02, mean val. rec. loss:  9.65547505e-03\n",
      "Epoch: 8477 mean train loss:  1.03871739e-02, mean val. rec. loss:  9.65465859e-03\n",
      "Epoch: 8478 mean train loss:  1.03863918e-02, mean val. rec. loss:  9.65384553e-03\n",
      "Epoch: 8479 mean train loss:  1.03856153e-02, mean val. rec. loss:  9.65302566e-03\n",
      "Epoch: 8480 mean train loss:  1.03848378e-02, mean val. rec. loss:  9.65220239e-03\n",
      "Epoch: 8481 mean train loss:  1.03840566e-02, mean val. rec. loss:  9.65138933e-03\n",
      "Epoch: 8482 mean train loss:  1.03832819e-02, mean val. rec. loss:  9.65057400e-03\n",
      "Epoch: 8483 mean train loss:  1.03824980e-02, mean val. rec. loss:  9.64976208e-03\n",
      "Epoch: 8484 mean train loss:  1.03817251e-02, mean val. rec. loss:  9.64894221e-03\n",
      "Epoch: 8485 mean train loss:  1.03809440e-02, mean val. rec. loss:  9.64812915e-03\n",
      "Epoch: 8486 mean train loss:  1.03801702e-02, mean val. rec. loss:  9.64731495e-03\n",
      "Epoch: 8487 mean train loss:  1.03793881e-02, mean val. rec. loss:  9.64650189e-03\n",
      "Epoch: 8488 mean train loss:  1.03786125e-02, mean val. rec. loss:  9.64568430e-03\n",
      "Epoch: 8489 mean train loss:  1.03778369e-02, mean val. rec. loss:  9.64486783e-03\n",
      "Epoch: 8490 mean train loss:  1.03770548e-02, mean val. rec. loss:  9.64405817e-03\n",
      "Epoch: 8491 mean train loss:  1.03762820e-02, mean val. rec. loss:  9.64324058e-03\n",
      "Epoch: 8492 mean train loss:  1.03754998e-02, mean val. rec. loss:  9.64242751e-03\n",
      "Epoch: 8493 mean train loss:  1.03747298e-02, mean val. rec. loss:  9.64160765e-03\n",
      "Epoch: 8494 mean train loss:  1.03739468e-02, mean val. rec. loss:  9.64079232e-03\n",
      "Epoch: 8495 mean train loss:  1.03731721e-02, mean val. rec. loss:  9.63997699e-03\n",
      "Epoch: 8496 mean train loss:  1.03723928e-02, mean val. rec. loss:  9.63916620e-03\n",
      "Epoch: 8497 mean train loss:  1.03716218e-02, mean val. rec. loss:  9.63834520e-03\n",
      "Epoch: 8498 mean train loss:  1.03708388e-02, mean val. rec. loss:  9.63752987e-03\n",
      "Epoch: 8499 mean train loss:  1.03700632e-02, mean val. rec. loss:  9.63671114e-03\n",
      "Epoch: 8500 mean train loss:  1.03692848e-02, mean val. rec. loss:  9.63590375e-03\n",
      "Epoch: 8501 mean train loss:  1.03685157e-02, mean val. rec. loss:  9.63508955e-03\n",
      "Epoch: 8502 mean train loss:  1.03677326e-02, mean val. rec. loss:  9.63427196e-03\n",
      "Epoch: 8503 mean train loss:  1.03669580e-02, mean val. rec. loss:  9.63345776e-03\n",
      "Epoch: 8504 mean train loss:  1.03661805e-02, mean val. rec. loss:  9.63264583e-03\n",
      "Epoch: 8505 mean train loss:  1.03654105e-02, mean val. rec. loss:  9.63183391e-03\n",
      "Epoch: 8506 mean train loss:  1.03646284e-02, mean val. rec. loss:  9.63101971e-03\n",
      "Epoch: 8507 mean train loss:  1.03638546e-02, mean val. rec. loss:  9.63020325e-03\n",
      "Epoch: 8508 mean train loss:  1.03630772e-02, mean val. rec. loss:  9.62939359e-03\n",
      "Epoch: 8509 mean train loss:  1.03623081e-02, mean val. rec. loss:  9.62858279e-03\n",
      "Epoch: 8510 mean train loss:  1.03615260e-02, mean val. rec. loss:  9.62777200e-03\n",
      "Epoch: 8511 mean train loss:  1.03607522e-02, mean val. rec. loss:  9.62695327e-03\n",
      "Epoch: 8512 mean train loss:  1.03599720e-02, mean val. rec. loss:  9.62613907e-03\n",
      "Epoch: 8513 mean train loss:  1.03592001e-02, mean val. rec. loss:  9.62532148e-03\n",
      "Epoch: 8514 mean train loss:  1.03584217e-02, mean val. rec. loss:  9.62451409e-03\n",
      "Epoch: 8515 mean train loss:  1.03576507e-02, mean val. rec. loss:  9.62369762e-03\n",
      "Epoch: 8516 mean train loss:  1.03568714e-02, mean val. rec. loss:  9.62288456e-03\n",
      "Epoch: 8517 mean train loss:  1.03561014e-02, mean val. rec. loss:  9.62206810e-03\n",
      "Epoch: 8518 mean train loss:  1.03553221e-02, mean val. rec. loss:  9.62125844e-03\n",
      "Epoch: 8519 mean train loss:  1.03545511e-02, mean val. rec. loss:  9.62044651e-03\n",
      "Epoch: 8520 mean train loss:  1.03537737e-02, mean val. rec. loss:  9.61963118e-03\n",
      "Epoch: 8521 mean train loss:  1.03530027e-02, mean val. rec. loss:  9.61881812e-03\n",
      "Epoch: 8522 mean train loss:  1.03522234e-02, mean val. rec. loss:  9.61800733e-03\n",
      "Epoch: 8523 mean train loss:  1.03514534e-02, mean val. rec. loss:  9.61719427e-03\n",
      "Epoch: 8524 mean train loss:  1.03506750e-02, mean val. rec. loss:  9.61638688e-03\n",
      "Epoch: 8525 mean train loss:  1.03499040e-02, mean val. rec. loss:  9.61556815e-03\n",
      "Epoch: 8526 mean train loss:  1.03491266e-02, mean val. rec. loss:  9.61475622e-03\n",
      "Epoch: 8527 mean train loss:  1.03483547e-02, mean val. rec. loss:  9.61395563e-03\n",
      "Epoch: 8528 mean train loss:  1.03475819e-02, mean val. rec. loss:  9.61314824e-03\n",
      "Epoch: 8529 mean train loss:  1.03468072e-02, mean val. rec. loss:  9.61233178e-03\n",
      "Epoch: 8530 mean train loss:  1.03460325e-02, mean val. rec. loss:  9.61151985e-03\n",
      "Epoch: 8531 mean train loss:  1.03452653e-02, mean val. rec. loss:  9.61070792e-03\n",
      "Epoch: 8532 mean train loss:  1.03444860e-02, mean val. rec. loss:  9.60989599e-03\n",
      "Epoch: 8533 mean train loss:  1.03437141e-02, mean val. rec. loss:  9.60907953e-03\n",
      "Epoch: 8534 mean train loss:  1.03429394e-02, mean val. rec. loss:  9.60826760e-03\n",
      "Epoch: 8535 mean train loss:  1.03421722e-02, mean val. rec. loss:  9.60745681e-03\n",
      "Epoch: 8536 mean train loss:  1.03413929e-02, mean val. rec. loss:  9.60664829e-03\n",
      "Epoch: 8537 mean train loss:  1.03406219e-02, mean val. rec. loss:  9.60583976e-03\n",
      "Epoch: 8538 mean train loss:  1.03398482e-02, mean val. rec. loss:  9.60502556e-03\n",
      "Epoch: 8539 mean train loss:  1.03390754e-02, mean val. rec. loss:  9.60421477e-03\n",
      "Epoch: 8540 mean train loss:  1.03383026e-02, mean val. rec. loss:  9.60340738e-03\n",
      "Epoch: 8541 mean train loss:  1.03375298e-02, mean val. rec. loss:  9.60260339e-03\n",
      "Epoch: 8542 mean train loss:  1.03367570e-02, mean val. rec. loss:  9.60179260e-03\n",
      "Epoch: 8543 mean train loss:  1.03359842e-02, mean val. rec. loss:  9.60098521e-03\n",
      "Epoch: 8544 mean train loss:  1.03352132e-02, mean val. rec. loss:  9.60016534e-03\n",
      "Epoch: 8545 mean train loss:  1.03344348e-02, mean val. rec. loss:  9.59936022e-03\n",
      "Epoch: 8546 mean train loss:  1.03336685e-02, mean val. rec. loss:  9.59854942e-03\n",
      "Epoch: 8547 mean train loss:  1.03328939e-02, mean val. rec. loss:  9.59773977e-03\n",
      "Epoch: 8548 mean train loss:  1.03321238e-02, mean val. rec. loss:  9.59692897e-03\n",
      "Epoch: 8549 mean train loss:  1.03313473e-02, mean val. rec. loss:  9.59611591e-03\n",
      "Epoch: 8550 mean train loss:  1.03305764e-02, mean val. rec. loss:  9.59531986e-03\n",
      "Epoch: 8551 mean train loss:  1.03298082e-02, mean val. rec. loss:  9.59450907e-03\n",
      "Epoch: 8552 mean train loss:  1.03290363e-02, mean val. rec. loss:  9.59369827e-03\n",
      "Epoch: 8553 mean train loss:  1.03282598e-02, mean val. rec. loss:  9.59289088e-03\n",
      "Epoch: 8554 mean train loss:  1.03274888e-02, mean val. rec. loss:  9.59208009e-03\n",
      "Epoch: 8555 mean train loss:  1.03267216e-02, mean val. rec. loss:  9.59127723e-03\n",
      "Epoch: 8556 mean train loss:  1.03259479e-02, mean val. rec. loss:  9.59046871e-03\n",
      "Epoch: 8557 mean train loss:  1.03251788e-02, mean val. rec. loss:  9.58964998e-03\n",
      "Epoch: 8558 mean train loss:  1.03244032e-02, mean val. rec. loss:  9.58884712e-03\n",
      "Epoch: 8559 mean train loss:  1.03236360e-02, mean val. rec. loss:  9.58803973e-03\n",
      "Epoch: 8560 mean train loss:  1.03228604e-02, mean val. rec. loss:  9.58723007e-03\n",
      "Epoch: 8561 mean train loss:  1.03220959e-02, mean val. rec. loss:  9.58641814e-03\n",
      "Epoch: 8562 mean train loss:  1.03213194e-02, mean val. rec. loss:  9.58561302e-03\n",
      "Epoch: 8563 mean train loss:  1.03205494e-02, mean val. rec. loss:  9.58480336e-03\n",
      "Epoch: 8564 mean train loss:  1.03197775e-02, mean val. rec. loss:  9.58399937e-03\n",
      "Epoch: 8565 mean train loss:  1.03190131e-02, mean val. rec. loss:  9.58318971e-03\n",
      "Epoch: 8566 mean train loss:  1.03182356e-02, mean val. rec. loss:  9.58238119e-03\n",
      "Epoch: 8567 mean train loss:  1.03174656e-02, mean val. rec. loss:  9.58158060e-03\n",
      "Epoch: 8568 mean train loss:  1.03166984e-02, mean val. rec. loss:  9.58076867e-03\n",
      "Epoch: 8569 mean train loss:  1.03159284e-02, mean val. rec. loss:  9.57995221e-03\n",
      "Epoch: 8570 mean train loss:  1.03151546e-02, mean val. rec. loss:  9.57914935e-03\n",
      "Epoch: 8571 mean train loss:  1.03143865e-02, mean val. rec. loss:  9.57833516e-03\n",
      "Epoch: 8572 mean train loss:  1.03136174e-02, mean val. rec. loss:  9.57752550e-03\n",
      "Epoch: 8573 mean train loss:  1.03128446e-02, mean val. rec. loss:  9.57671924e-03\n",
      "Epoch: 8574 mean train loss:  1.03120783e-02, mean val. rec. loss:  9.57591185e-03\n",
      "Epoch: 8575 mean train loss:  1.03113073e-02, mean val. rec. loss:  9.57510332e-03\n",
      "Epoch: 8576 mean train loss:  1.03105317e-02, mean val. rec. loss:  9.57430954e-03\n",
      "Epoch: 8577 mean train loss:  1.03097682e-02, mean val. rec. loss:  9.57349761e-03\n",
      "Epoch: 8578 mean train loss:  1.03089973e-02, mean val. rec. loss:  9.57268569e-03\n",
      "Epoch: 8579 mean train loss:  1.03082263e-02, mean val. rec. loss:  9.57187943e-03\n",
      "Epoch: 8580 mean train loss:  1.03074554e-02, mean val. rec. loss:  9.57107431e-03\n",
      "Epoch: 8581 mean train loss:  1.03066900e-02, mean val. rec. loss:  9.57027258e-03\n",
      "Epoch: 8582 mean train loss:  1.03059210e-02, mean val. rec. loss:  9.56945952e-03\n",
      "Epoch: 8583 mean train loss:  1.03051481e-02, mean val. rec. loss:  9.56864873e-03\n",
      "Epoch: 8584 mean train loss:  1.03043791e-02, mean val. rec. loss:  9.56785608e-03\n",
      "Epoch: 8585 mean train loss:  1.03036137e-02, mean val. rec. loss:  9.56704982e-03\n",
      "Epoch: 8586 mean train loss:  1.03028446e-02, mean val. rec. loss:  9.56624243e-03\n",
      "Epoch: 8587 mean train loss:  1.03020718e-02, mean val. rec. loss:  9.56543050e-03\n",
      "Epoch: 8588 mean train loss:  1.03013027e-02, mean val. rec. loss:  9.56461744e-03\n",
      "Epoch: 8589 mean train loss:  1.03005392e-02, mean val. rec. loss:  9.56382479e-03\n",
      "Epoch: 8590 mean train loss:  1.02997692e-02, mean val. rec. loss:  9.56302194e-03\n",
      "Epoch: 8591 mean train loss:  1.02989973e-02, mean val. rec. loss:  9.56220547e-03\n",
      "Epoch: 8592 mean train loss:  1.02982264e-02, mean val. rec. loss:  9.56139808e-03\n",
      "Epoch: 8593 mean train loss:  1.02974601e-02, mean val. rec. loss:  9.56059069e-03\n",
      "Epoch: 8594 mean train loss:  1.02966938e-02, mean val. rec. loss:  9.55979010e-03\n",
      "Epoch: 8595 mean train loss:  1.02959229e-02, mean val. rec. loss:  9.55898271e-03\n",
      "Epoch: 8596 mean train loss:  1.02951528e-02, mean val. rec. loss:  9.55816965e-03\n",
      "Epoch: 8597 mean train loss:  1.02943875e-02, mean val. rec. loss:  9.55736566e-03\n",
      "Epoch: 8598 mean train loss:  1.02936212e-02, mean val. rec. loss:  9.55656281e-03\n",
      "Epoch: 8599 mean train loss:  1.02928521e-02, mean val. rec. loss:  9.55576222e-03\n",
      "Epoch: 8600 mean train loss:  1.02920812e-02, mean val. rec. loss:  9.55496163e-03\n",
      "Epoch: 8601 mean train loss:  1.02913158e-02, mean val. rec. loss:  9.55414517e-03\n",
      "Epoch: 8602 mean train loss:  1.02905504e-02, mean val. rec. loss:  9.55334005e-03\n",
      "Epoch: 8603 mean train loss:  1.02897804e-02, mean val. rec. loss:  9.55253946e-03\n",
      "Epoch: 8604 mean train loss:  1.02890113e-02, mean val. rec. loss:  9.55173433e-03\n",
      "Epoch: 8605 mean train loss:  1.02882460e-02, mean val. rec. loss:  9.55093148e-03\n",
      "Epoch: 8606 mean train loss:  1.02874806e-02, mean val. rec. loss:  9.55012182e-03\n",
      "Epoch: 8607 mean train loss:  1.02867115e-02, mean val. rec. loss:  9.54931670e-03\n",
      "Epoch: 8608 mean train loss:  1.02859415e-02, mean val. rec. loss:  9.54852291e-03\n",
      "Epoch: 8609 mean train loss:  1.02851762e-02, mean val. rec. loss:  9.54771439e-03\n",
      "Epoch: 8610 mean train loss:  1.02844136e-02, mean val. rec. loss:  9.54690586e-03\n",
      "Epoch: 8611 mean train loss:  1.02836473e-02, mean val. rec. loss:  9.54609620e-03\n",
      "Epoch: 8612 mean train loss:  1.02828782e-02, mean val. rec. loss:  9.54529562e-03\n",
      "Epoch: 8613 mean train loss:  1.02821091e-02, mean val. rec. loss:  9.54449956e-03\n",
      "Epoch: 8614 mean train loss:  1.02813438e-02, mean val. rec. loss:  9.54369217e-03\n",
      "Epoch: 8615 mean train loss:  1.02805803e-02, mean val. rec. loss:  9.54288251e-03\n",
      "Epoch: 8616 mean train loss:  1.02798093e-02, mean val. rec. loss:  9.54207285e-03\n",
      "Epoch: 8617 mean train loss:  1.02790430e-02, mean val. rec. loss:  9.54127453e-03\n",
      "Epoch: 8618 mean train loss:  1.02782786e-02, mean val. rec. loss:  9.54047168e-03\n",
      "Epoch: 8619 mean train loss:  1.02775114e-02, mean val. rec. loss:  9.53966882e-03\n",
      "Epoch: 8620 mean train loss:  1.02767460e-02, mean val. rec. loss:  9.53886824e-03\n",
      "Epoch: 8621 mean train loss:  1.02759797e-02, mean val. rec. loss:  9.53806084e-03\n",
      "Epoch: 8622 mean train loss:  1.02752134e-02, mean val. rec. loss:  9.53726253e-03\n",
      "Epoch: 8623 mean train loss:  1.02744490e-02, mean val. rec. loss:  9.53645967e-03\n",
      "Epoch: 8624 mean train loss:  1.02736864e-02, mean val. rec. loss:  9.53566248e-03\n",
      "Epoch: 8625 mean train loss:  1.02729174e-02, mean val. rec. loss:  9.53485736e-03\n",
      "Epoch: 8626 mean train loss:  1.02721501e-02, mean val. rec. loss:  9.53405451e-03\n",
      "Epoch: 8627 mean train loss:  1.02713857e-02, mean val. rec. loss:  9.53325052e-03\n",
      "Epoch: 8628 mean train loss:  1.02706194e-02, mean val. rec. loss:  9.53244653e-03\n",
      "Epoch: 8629 mean train loss:  1.02698503e-02, mean val. rec. loss:  9.53164934e-03\n",
      "Epoch: 8630 mean train loss:  1.02690850e-02, mean val. rec. loss:  9.53084195e-03\n",
      "Epoch: 8631 mean train loss:  1.02683233e-02, mean val. rec. loss:  9.53003683e-03\n",
      "Epoch: 8632 mean train loss:  1.02675589e-02, mean val. rec. loss:  9.52923511e-03\n",
      "Epoch: 8633 mean train loss:  1.02667889e-02, mean val. rec. loss:  9.52842885e-03\n",
      "Epoch: 8634 mean train loss:  1.02660235e-02, mean val. rec. loss:  9.52763506e-03\n",
      "Epoch: 8635 mean train loss:  1.02652637e-02, mean val. rec. loss:  9.52682087e-03\n",
      "Epoch: 8636 mean train loss:  1.02644937e-02, mean val. rec. loss:  9.52603162e-03\n",
      "Epoch: 8637 mean train loss:  1.02637330e-02, mean val. rec. loss:  9.52522423e-03\n",
      "Epoch: 8638 mean train loss:  1.02629639e-02, mean val. rec. loss:  9.52442931e-03\n",
      "Epoch: 8639 mean train loss:  1.02622032e-02, mean val. rec. loss:  9.52361285e-03\n",
      "Epoch: 8640 mean train loss:  1.02614351e-02, mean val. rec. loss:  9.52282247e-03\n",
      "Epoch: 8641 mean train loss:  1.02606735e-02, mean val. rec. loss:  9.52201961e-03\n",
      "Epoch: 8642 mean train loss:  1.02599062e-02, mean val. rec. loss:  9.52121676e-03\n",
      "Epoch: 8643 mean train loss:  1.02591446e-02, mean val. rec. loss:  9.52041050e-03\n",
      "Epoch: 8644 mean train loss:  1.02583736e-02, mean val. rec. loss:  9.51960538e-03\n",
      "Epoch: 8645 mean train loss:  1.02576167e-02, mean val. rec. loss:  9.51881046e-03\n",
      "Epoch: 8646 mean train loss:  1.02568476e-02, mean val. rec. loss:  9.51800760e-03\n",
      "Epoch: 8647 mean train loss:  1.02560869e-02, mean val. rec. loss:  9.51721382e-03\n",
      "Epoch: 8648 mean train loss:  1.02553197e-02, mean val. rec. loss:  9.51640529e-03\n",
      "Epoch: 8649 mean train loss:  1.02545589e-02, mean val. rec. loss:  9.51561264e-03\n",
      "Epoch: 8650 mean train loss:  1.02537908e-02, mean val. rec. loss:  9.51480865e-03\n",
      "Epoch: 8651 mean train loss:  1.02530310e-02, mean val. rec. loss:  9.51401487e-03\n",
      "Epoch: 8652 mean train loss:  1.02522629e-02, mean val. rec. loss:  9.51320181e-03\n",
      "Epoch: 8653 mean train loss:  1.02515012e-02, mean val. rec. loss:  9.51240859e-03\n",
      "Epoch: 8654 mean train loss:  1.02507396e-02, mean val. rec. loss:  9.51159837e-03\n",
      "Epoch: 8655 mean train loss:  1.02499742e-02, mean val. rec. loss:  9.51080345e-03\n",
      "Epoch: 8656 mean train loss:  1.02492079e-02, mean val. rec. loss:  9.50999379e-03\n",
      "Epoch: 8657 mean train loss:  1.02484472e-02, mean val. rec. loss:  9.50919547e-03\n",
      "Epoch: 8658 mean train loss:  1.02476884e-02, mean val. rec. loss:  9.50839148e-03\n",
      "Epoch: 8659 mean train loss:  1.02469212e-02, mean val. rec. loss:  9.50758863e-03\n",
      "Epoch: 8660 mean train loss:  1.02461530e-02, mean val. rec. loss:  9.50679031e-03\n",
      "Epoch: 8661 mean train loss:  1.02453923e-02, mean val. rec. loss:  9.50599085e-03\n",
      "Epoch: 8662 mean train loss:  1.02446316e-02, mean val. rec. loss:  9.50519367e-03\n",
      "Epoch: 8663 mean train loss:  1.02438653e-02, mean val. rec. loss:  9.50439535e-03\n",
      "Epoch: 8664 mean train loss:  1.02431028e-02, mean val. rec. loss:  9.50359476e-03\n",
      "Epoch: 8665 mean train loss:  1.02423420e-02, mean val. rec. loss:  9.50279984e-03\n",
      "Epoch: 8666 mean train loss:  1.02415767e-02, mean val. rec. loss:  9.50199585e-03\n",
      "Epoch: 8667 mean train loss:  1.02408123e-02, mean val. rec. loss:  9.50119753e-03\n",
      "Epoch: 8668 mean train loss:  1.02400516e-02, mean val. rec. loss:  9.50039581e-03\n",
      "Epoch: 8669 mean train loss:  1.02392918e-02, mean val. rec. loss:  9.49959749e-03\n",
      "Epoch: 8670 mean train loss:  1.02385264e-02, mean val. rec. loss:  9.49879804e-03\n",
      "Epoch: 8671 mean train loss:  1.02377620e-02, mean val. rec. loss:  9.49799859e-03\n",
      "Epoch: 8672 mean train loss:  1.02370013e-02, mean val. rec. loss:  9.49719857e-03\n",
      "Epoch: 8673 mean train loss:  1.02362415e-02, mean val. rec. loss:  9.49640081e-03\n",
      "Epoch: 8674 mean train loss:  1.02354780e-02, mean val. rec. loss:  9.49560136e-03\n",
      "Epoch: 8675 mean train loss:  1.02347117e-02, mean val. rec. loss:  9.49479964e-03\n",
      "Epoch: 8676 mean train loss:  1.02339501e-02, mean val. rec. loss:  9.49399792e-03\n",
      "Epoch: 8677 mean train loss:  1.02331894e-02, mean val. rec. loss:  9.49319563e-03\n",
      "Epoch: 8678 mean train loss:  1.02324259e-02, mean val. rec. loss:  9.49239844e-03\n",
      "Epoch: 8679 mean train loss:  1.02316642e-02, mean val. rec. loss:  9.49160069e-03\n",
      "Epoch: 8680 mean train loss:  1.02309026e-02, mean val. rec. loss:  9.49080124e-03\n",
      "Epoch: 8681 mean train loss:  1.02301400e-02, mean val. rec. loss:  9.48999951e-03\n",
      "Epoch: 8682 mean train loss:  1.02293793e-02, mean val. rec. loss:  9.48920346e-03\n",
      "Epoch: 8683 mean train loss:  1.02286177e-02, mean val. rec. loss:  9.48840401e-03\n",
      "Epoch: 8684 mean train loss:  1.02278561e-02, mean val. rec. loss:  9.48760796e-03\n",
      "Epoch: 8685 mean train loss:  1.02270935e-02, mean val. rec. loss:  9.48680624e-03\n",
      "Epoch: 8686 mean train loss:  1.02263328e-02, mean val. rec. loss:  9.48600848e-03\n",
      "Epoch: 8687 mean train loss:  1.02255712e-02, mean val. rec. loss:  9.48521187e-03\n",
      "Epoch: 8688 mean train loss:  1.02248105e-02, mean val. rec. loss:  9.48441355e-03\n",
      "Epoch: 8689 mean train loss:  1.02240488e-02, mean val. rec. loss:  9.48361523e-03\n",
      "Epoch: 8690 mean train loss:  1.02232881e-02, mean val. rec. loss:  9.48281577e-03\n",
      "Epoch: 8691 mean train loss:  1.02225265e-02, mean val. rec. loss:  9.48201859e-03\n",
      "Epoch: 8692 mean train loss:  1.02217658e-02, mean val. rec. loss:  9.48122027e-03\n",
      "Epoch: 8693 mean train loss:  1.02210051e-02, mean val. rec. loss:  9.48042478e-03\n",
      "Epoch: 8694 mean train loss:  1.02202434e-02, mean val. rec. loss:  9.47962023e-03\n",
      "Epoch: 8695 mean train loss:  1.02194855e-02, mean val. rec. loss:  9.47882644e-03\n",
      "Epoch: 8696 mean train loss:  1.02187267e-02, mean val. rec. loss:  9.47802699e-03\n",
      "Epoch: 8697 mean train loss:  1.02179641e-02, mean val. rec. loss:  9.47722413e-03\n",
      "Epoch: 8698 mean train loss:  1.02172006e-02, mean val. rec. loss:  9.47643092e-03\n",
      "Epoch: 8699 mean train loss:  1.02164446e-02, mean val. rec. loss:  9.47563317e-03\n",
      "Epoch: 8700 mean train loss:  1.02156885e-02, mean val. rec. loss:  9.47483371e-03\n",
      "Epoch: 8701 mean train loss:  1.02149241e-02, mean val. rec. loss:  9.47404106e-03\n",
      "Epoch: 8702 mean train loss:  1.02141615e-02, mean val. rec. loss:  9.47323594e-03\n",
      "Epoch: 8703 mean train loss:  1.02134036e-02, mean val. rec. loss:  9.47244159e-03\n",
      "Epoch: 8704 mean train loss:  1.02126438e-02, mean val. rec. loss:  9.47164270e-03\n",
      "Epoch: 8705 mean train loss:  1.02118822e-02, mean val. rec. loss:  9.47085232e-03\n",
      "Epoch: 8706 mean train loss:  1.02111196e-02, mean val. rec. loss:  9.47004493e-03\n",
      "Epoch: 8707 mean train loss:  1.02103626e-02, mean val. rec. loss:  9.46925228e-03\n",
      "Epoch: 8708 mean train loss:  1.02096038e-02, mean val. rec. loss:  9.46844999e-03\n",
      "Epoch: 8709 mean train loss:  1.02088422e-02, mean val. rec. loss:  9.46765791e-03\n",
      "Epoch: 8710 mean train loss:  1.02080777e-02, mean val. rec. loss:  9.46686186e-03\n",
      "Epoch: 8711 mean train loss:  1.02073236e-02, mean val. rec. loss:  9.46606807e-03\n",
      "Epoch: 8712 mean train loss:  1.02065666e-02, mean val. rec. loss:  9.46527712e-03\n",
      "Epoch: 8713 mean train loss:  1.02058031e-02, mean val. rec. loss:  9.46447484e-03\n",
      "Epoch: 8714 mean train loss:  1.02050396e-02, mean val. rec. loss:  9.46368275e-03\n",
      "Epoch: 8715 mean train loss:  1.02042835e-02, mean val. rec. loss:  9.46288500e-03\n",
      "Epoch: 8716 mean train loss:  1.02035265e-02, mean val. rec. loss:  9.46209178e-03\n",
      "Epoch: 8717 mean train loss:  1.02027649e-02, mean val. rec. loss:  9.46129403e-03\n",
      "Epoch: 8718 mean train loss:  1.02020033e-02, mean val. rec. loss:  9.46049458e-03\n",
      "Epoch: 8719 mean train loss:  1.02012472e-02, mean val. rec. loss:  9.45969853e-03\n",
      "Epoch: 8720 mean train loss:  1.02004893e-02, mean val. rec. loss:  9.45890021e-03\n",
      "Epoch: 8721 mean train loss:  1.01997277e-02, mean val. rec. loss:  9.45810812e-03\n",
      "Epoch: 8722 mean train loss:  1.01989660e-02, mean val. rec. loss:  9.45731037e-03\n",
      "Epoch: 8723 mean train loss:  1.01982100e-02, mean val. rec. loss:  9.45651375e-03\n",
      "Epoch: 8724 mean train loss:  1.01974539e-02, mean val. rec. loss:  9.45572337e-03\n",
      "Epoch: 8725 mean train loss:  1.01966923e-02, mean val. rec. loss:  9.45492392e-03\n",
      "Epoch: 8726 mean train loss:  1.01959316e-02, mean val. rec. loss:  9.45412900e-03\n",
      "Epoch: 8727 mean train loss:  1.01951728e-02, mean val. rec. loss:  9.45332898e-03\n",
      "Epoch: 8728 mean train loss:  1.01944130e-02, mean val. rec. loss:  9.45253179e-03\n",
      "Epoch: 8729 mean train loss:  1.01936551e-02, mean val. rec. loss:  9.45173688e-03\n",
      "Epoch: 8730 mean train loss:  1.01928953e-02, mean val. rec. loss:  9.45094196e-03\n",
      "Epoch: 8731 mean train loss:  1.01921374e-02, mean val. rec. loss:  9.45014818e-03\n",
      "Epoch: 8732 mean train loss:  1.01913795e-02, mean val. rec. loss:  9.44935439e-03\n",
      "Epoch: 8733 mean train loss:  1.01906225e-02, mean val. rec. loss:  9.44855721e-03\n",
      "Epoch: 8734 mean train loss:  1.01898674e-02, mean val. rec. loss:  9.44775889e-03\n",
      "Epoch: 8735 mean train loss:  1.01891067e-02, mean val. rec. loss:  9.44696850e-03\n",
      "Epoch: 8736 mean train loss:  1.01883460e-02, mean val. rec. loss:  9.44617529e-03\n",
      "Epoch: 8737 mean train loss:  1.01875918e-02, mean val. rec. loss:  9.44537697e-03\n",
      "Epoch: 8738 mean train loss:  1.01868339e-02, mean val. rec. loss:  9.44458092e-03\n",
      "Epoch: 8739 mean train loss:  1.01860713e-02, mean val. rec. loss:  9.44377976e-03\n",
      "Epoch: 8740 mean train loss:  1.01853143e-02, mean val. rec. loss:  9.44298825e-03\n",
      "Epoch: 8741 mean train loss:  1.01845601e-02, mean val. rec. loss:  9.44219560e-03\n",
      "Epoch: 8742 mean train loss:  1.01838022e-02, mean val. rec. loss:  9.44138877e-03\n",
      "Epoch: 8743 mean train loss:  1.01830397e-02, mean val. rec. loss:  9.44059556e-03\n",
      "Epoch: 8744 mean train loss:  1.01822827e-02, mean val. rec. loss:  9.43980801e-03\n",
      "Epoch: 8745 mean train loss:  1.01815248e-02, mean val. rec. loss:  9.43901649e-03\n",
      "Epoch: 8746 mean train loss:  1.01807678e-02, mean val. rec. loss:  9.43822101e-03\n",
      "Epoch: 8747 mean train loss:  1.01800145e-02, mean val. rec. loss:  9.43742269e-03\n",
      "Epoch: 8748 mean train loss:  1.01792566e-02, mean val. rec. loss:  9.43663117e-03\n",
      "Epoch: 8749 mean train loss:  1.01784950e-02, mean val. rec. loss:  9.43583966e-03\n",
      "Epoch: 8750 mean train loss:  1.01777389e-02, mean val. rec. loss:  9.43504871e-03\n",
      "Epoch: 8751 mean train loss:  1.01769847e-02, mean val. rec. loss:  9.43425209e-03\n",
      "Epoch: 8752 mean train loss:  1.01762268e-02, mean val. rec. loss:  9.43344583e-03\n",
      "Epoch: 8753 mean train loss:  1.01754652e-02, mean val. rec. loss:  9.43265772e-03\n",
      "Epoch: 8754 mean train loss:  1.01747091e-02, mean val. rec. loss:  9.43187301e-03\n",
      "Epoch: 8755 mean train loss:  1.01739559e-02, mean val. rec. loss:  9.43107696e-03\n",
      "Epoch: 8756 mean train loss:  1.01731980e-02, mean val. rec. loss:  9.43028034e-03\n",
      "Epoch: 8757 mean train loss:  1.01724373e-02, mean val. rec. loss:  9.42948145e-03\n",
      "Epoch: 8758 mean train loss:  1.01716803e-02, mean val. rec. loss:  9.42869220e-03\n",
      "Epoch: 8759 mean train loss:  1.01709270e-02, mean val. rec. loss:  9.42790409e-03\n",
      "Epoch: 8760 mean train loss:  1.01701729e-02, mean val. rec. loss:  9.42710010e-03\n",
      "Epoch: 8761 mean train loss:  1.01694140e-02, mean val. rec. loss:  9.42630235e-03\n",
      "Epoch: 8762 mean train loss:  1.01686552e-02, mean val. rec. loss:  9.42551310e-03\n",
      "Epoch: 8763 mean train loss:  1.01679001e-02, mean val. rec. loss:  9.42472329e-03\n",
      "Epoch: 8764 mean train loss:  1.01671459e-02, mean val. rec. loss:  9.42393404e-03\n",
      "Epoch: 8765 mean train loss:  1.01663880e-02, mean val. rec. loss:  9.42313005e-03\n",
      "Epoch: 8766 mean train loss:  1.01656282e-02, mean val. rec. loss:  9.42233853e-03\n",
      "Epoch: 8767 mean train loss:  1.01648749e-02, mean val. rec. loss:  9.42154588e-03\n",
      "Epoch: 8768 mean train loss:  1.01641207e-02, mean val. rec. loss:  9.42074643e-03\n",
      "Epoch: 8769 mean train loss:  1.01633610e-02, mean val. rec. loss:  9.41995605e-03\n",
      "Epoch: 8770 mean train loss:  1.01626030e-02, mean val. rec. loss:  9.41916793e-03\n",
      "Epoch: 8771 mean train loss:  1.01618489e-02, mean val. rec. loss:  9.41837302e-03\n",
      "Epoch: 8772 mean train loss:  1.01610947e-02, mean val. rec. loss:  9.41757867e-03\n",
      "Epoch: 8773 mean train loss:  1.01603368e-02, mean val. rec. loss:  9.41678148e-03\n",
      "Epoch: 8774 mean train loss:  1.01595789e-02, mean val. rec. loss:  9.41599393e-03\n",
      "Epoch: 8775 mean train loss:  1.01588247e-02, mean val. rec. loss:  9.41520355e-03\n",
      "Epoch: 8776 mean train loss:  1.01580705e-02, mean val. rec. loss:  9.41440410e-03\n",
      "Epoch: 8777 mean train loss:  1.01573126e-02, mean val. rec. loss:  9.41361032e-03\n",
      "Epoch: 8778 mean train loss:  1.01565547e-02, mean val. rec. loss:  9.41282334e-03\n",
      "Epoch: 8779 mean train loss:  1.01558014e-02, mean val. rec. loss:  9.41203295e-03\n",
      "Epoch: 8780 mean train loss:  1.01550472e-02, mean val. rec. loss:  9.41123917e-03\n",
      "Epoch: 8781 mean train loss:  1.01542865e-02, mean val. rec. loss:  9.41043065e-03\n",
      "Epoch: 8782 mean train loss:  1.01535267e-02, mean val. rec. loss:  9.40964083e-03\n",
      "Epoch: 8783 mean train loss:  1.01527763e-02, mean val. rec. loss:  9.40885952e-03\n",
      "Epoch: 8784 mean train loss:  1.01520230e-02, mean val. rec. loss:  9.40806290e-03\n",
      "Epoch: 8785 mean train loss:  1.01512651e-02, mean val. rec. loss:  9.40726061e-03\n",
      "Epoch: 8786 mean train loss:  1.01505081e-02, mean val. rec. loss:  9.40647363e-03\n",
      "Epoch: 8787 mean train loss:  1.01497539e-02, mean val. rec. loss:  9.40568949e-03\n",
      "Epoch: 8788 mean train loss:  1.01490007e-02, mean val. rec. loss:  9.40489854e-03\n",
      "Epoch: 8789 mean train loss:  1.01482418e-02, mean val. rec. loss:  9.40409852e-03\n",
      "Epoch: 8790 mean train loss:  1.01474876e-02, mean val. rec. loss:  9.40330247e-03\n",
      "Epoch: 8791 mean train loss:  1.01467353e-02, mean val. rec. loss:  9.40250585e-03\n",
      "Epoch: 8792 mean train loss:  1.01459802e-02, mean val. rec. loss:  9.40172057e-03\n",
      "Epoch: 8793 mean train loss:  1.01452214e-02, mean val. rec. loss:  9.40093359e-03\n",
      "Epoch: 8794 mean train loss:  1.01444690e-02, mean val. rec. loss:  9.40013641e-03\n",
      "Epoch: 8795 mean train loss:  1.01437111e-02, mean val. rec. loss:  9.39934602e-03\n",
      "Epoch: 8796 mean train loss:  1.01429625e-02, mean val. rec. loss:  9.39855508e-03\n",
      "Epoch: 8797 mean train loss:  1.01422028e-02, mean val. rec. loss:  9.39776980e-03\n",
      "Epoch: 8798 mean train loss:  1.01414467e-02, mean val. rec. loss:  9.39697545e-03\n",
      "Epoch: 8799 mean train loss:  1.01406972e-02, mean val. rec. loss:  9.39617543e-03\n",
      "Epoch: 8800 mean train loss:  1.01399402e-02, mean val. rec. loss:  9.39539071e-03\n",
      "Epoch: 8801 mean train loss:  1.01391860e-02, mean val. rec. loss:  9.39460430e-03\n",
      "Epoch: 8802 mean train loss:  1.01384309e-02, mean val. rec. loss:  9.39380825e-03\n",
      "Epoch: 8803 mean train loss:  1.01376739e-02, mean val. rec. loss:  9.39301333e-03\n",
      "Epoch: 8804 mean train loss:  1.01369272e-02, mean val. rec. loss:  9.39222011e-03\n",
      "Epoch: 8805 mean train loss:  1.01361627e-02, mean val. rec. loss:  9.39143484e-03\n",
      "Epoch: 8806 mean train loss:  1.01354169e-02, mean val. rec. loss:  9.39063595e-03\n",
      "Epoch: 8807 mean train loss:  1.01346553e-02, mean val. rec. loss:  9.38984784e-03\n",
      "Epoch: 8808 mean train loss:  1.01339086e-02, mean val. rec. loss:  9.38905292e-03\n",
      "Epoch: 8809 mean train loss:  1.01331451e-02, mean val. rec. loss:  9.38826934e-03\n",
      "Epoch: 8810 mean train loss:  1.01324011e-02, mean val. rec. loss:  9.38747329e-03\n",
      "Epoch: 8811 mean train loss:  1.01316376e-02, mean val. rec. loss:  9.38668518e-03\n",
      "Epoch: 8812 mean train loss:  1.01308899e-02, mean val. rec. loss:  9.38588912e-03\n",
      "Epoch: 8813 mean train loss:  1.01301339e-02, mean val. rec. loss:  9.38510384e-03\n",
      "Epoch: 8814 mean train loss:  1.01293816e-02, mean val. rec. loss:  9.38430723e-03\n",
      "Epoch: 8815 mean train loss:  1.01286237e-02, mean val. rec. loss:  9.38351798e-03\n",
      "Epoch: 8816 mean train loss:  1.01278751e-02, mean val. rec. loss:  9.38273043e-03\n",
      "Epoch: 8817 mean train loss:  1.01271162e-02, mean val. rec. loss:  9.38194629e-03\n",
      "Epoch: 8818 mean train loss:  1.01263667e-02, mean val. rec. loss:  9.38115080e-03\n",
      "Epoch: 8819 mean train loss:  1.01256088e-02, mean val. rec. loss:  9.38036269e-03\n",
      "Epoch: 8820 mean train loss:  1.01248602e-02, mean val. rec. loss:  9.37957174e-03\n",
      "Epoch: 8821 mean train loss:  1.01241013e-02, mean val. rec. loss:  9.37878816e-03\n",
      "Epoch: 8822 mean train loss:  1.01233518e-02, mean val. rec. loss:  9.37799211e-03\n",
      "Epoch: 8823 mean train loss:  1.01225967e-02, mean val. rec. loss:  9.37719719e-03\n",
      "Epoch: 8824 mean train loss:  1.01218434e-02, mean val. rec. loss:  9.37640284e-03\n",
      "Epoch: 8825 mean train loss:  1.01210874e-02, mean val. rec. loss:  9.37561303e-03\n",
      "Epoch: 8826 mean train loss:  1.01203406e-02, mean val. rec. loss:  9.37481698e-03\n",
      "Epoch: 8827 mean train loss:  1.01195818e-02, mean val. rec. loss:  9.37403113e-03\n",
      "Epoch: 8828 mean train loss:  1.01188285e-02, mean val. rec. loss:  9.37323565e-03\n",
      "Epoch: 8829 mean train loss:  1.01180743e-02, mean val. rec. loss:  9.37244867e-03\n",
      "Epoch: 8830 mean train loss:  1.01173276e-02, mean val. rec. loss:  9.37165772e-03\n",
      "Epoch: 8831 mean train loss:  1.01165678e-02, mean val. rec. loss:  9.37086734e-03\n",
      "Epoch: 8832 mean train loss:  1.01158174e-02, mean val. rec. loss:  9.37007242e-03\n",
      "Epoch: 8833 mean train loss:  1.01150623e-02, mean val. rec. loss:  9.36928601e-03\n",
      "Epoch: 8834 mean train loss:  1.01143146e-02, mean val. rec. loss:  9.36849732e-03\n",
      "Epoch: 8835 mean train loss:  1.01135567e-02, mean val. rec. loss:  9.36770921e-03\n",
      "Epoch: 8836 mean train loss:  1.01128044e-02, mean val. rec. loss:  9.36691656e-03\n",
      "Epoch: 8837 mean train loss:  1.01120502e-02, mean val. rec. loss:  9.36612618e-03\n",
      "Epoch: 8838 mean train loss:  1.01113016e-02, mean val. rec. loss:  9.36533807e-03\n",
      "Epoch: 8839 mean train loss:  1.01105474e-02, mean val. rec. loss:  9.36454995e-03\n",
      "Epoch: 8840 mean train loss:  1.01097913e-02, mean val. rec. loss:  9.36376411e-03\n",
      "Epoch: 8841 mean train loss:  1.01090409e-02, mean val. rec. loss:  9.36296919e-03\n",
      "Epoch: 8842 mean train loss:  1.01082904e-02, mean val. rec. loss:  9.36217881e-03\n",
      "Epoch: 8843 mean train loss:  1.01075362e-02, mean val. rec. loss:  9.36139296e-03\n",
      "Epoch: 8844 mean train loss:  1.01067820e-02, mean val. rec. loss:  9.36060485e-03\n",
      "Epoch: 8845 mean train loss:  1.01060316e-02, mean val. rec. loss:  9.35981050e-03\n",
      "Epoch: 8846 mean train loss:  1.01052802e-02, mean val. rec. loss:  9.35902238e-03\n",
      "Epoch: 8847 mean train loss:  1.01045251e-02, mean val. rec. loss:  9.35823710e-03\n",
      "Epoch: 8848 mean train loss:  1.01037746e-02, mean val. rec. loss:  9.35744899e-03\n",
      "Epoch: 8849 mean train loss:  1.01030213e-02, mean val. rec. loss:  9.35664840e-03\n",
      "Epoch: 8850 mean train loss:  1.01022690e-02, mean val. rec. loss:  9.35586199e-03\n",
      "Epoch: 8851 mean train loss:  1.01015148e-02, mean val. rec. loss:  9.35507331e-03\n",
      "Epoch: 8852 mean train loss:  1.01007644e-02, mean val. rec. loss:  9.35429200e-03\n",
      "Epoch: 8853 mean train loss:  1.01000148e-02, mean val. rec. loss:  9.35349481e-03\n",
      "Epoch: 8854 mean train loss:  1.00992606e-02, mean val. rec. loss:  9.35271350e-03\n",
      "Epoch: 8855 mean train loss:  1.00985083e-02, mean val. rec. loss:  9.35192085e-03\n",
      "Epoch: 8856 mean train loss:  1.00977569e-02, mean val. rec. loss:  9.35113161e-03\n",
      "Epoch: 8857 mean train loss:  1.00970037e-02, mean val. rec. loss:  9.35034236e-03\n",
      "Epoch: 8858 mean train loss:  1.00962532e-02, mean val. rec. loss:  9.34955538e-03\n",
      "Epoch: 8859 mean train loss:  1.00955009e-02, mean val. rec. loss:  9.34876556e-03\n",
      "Epoch: 8860 mean train loss:  1.00947495e-02, mean val. rec. loss:  9.34797915e-03\n",
      "Epoch: 8861 mean train loss:  1.00939944e-02, mean val. rec. loss:  9.34718197e-03\n",
      "Epoch: 8862 mean train loss:  1.00932448e-02, mean val. rec. loss:  9.34640066e-03\n",
      "Epoch: 8863 mean train loss:  1.00924953e-02, mean val. rec. loss:  9.34561368e-03\n",
      "Epoch: 8864 mean train loss:  1.00917439e-02, mean val. rec. loss:  9.34482556e-03\n",
      "Epoch: 8865 mean train loss:  1.00909888e-02, mean val. rec. loss:  9.34403632e-03\n",
      "Epoch: 8866 mean train loss:  1.00902365e-02, mean val. rec. loss:  9.34323800e-03\n",
      "Epoch: 8867 mean train loss:  1.00894869e-02, mean val. rec. loss:  9.34244648e-03\n",
      "Epoch: 8868 mean train loss:  1.00887374e-02, mean val. rec. loss:  9.34166347e-03\n",
      "Epoch: 8869 mean train loss:  1.00879869e-02, mean val. rec. loss:  9.34088329e-03\n",
      "Epoch: 8870 mean train loss:  1.00872346e-02, mean val. rec. loss:  9.34008668e-03\n",
      "Epoch: 8871 mean train loss:  1.00864841e-02, mean val. rec. loss:  9.33929062e-03\n",
      "Epoch: 8872 mean train loss:  1.00857281e-02, mean val. rec. loss:  9.33850421e-03\n",
      "Epoch: 8873 mean train loss:  1.00849776e-02, mean val. rec. loss:  9.33772120e-03\n",
      "Epoch: 8874 mean train loss:  1.00842309e-02, mean val. rec. loss:  9.33693252e-03\n",
      "Epoch: 8875 mean train loss:  1.00834786e-02, mean val. rec. loss:  9.33614044e-03\n",
      "Epoch: 8876 mean train loss:  1.00827235e-02, mean val. rec. loss:  9.33536310e-03\n",
      "Epoch: 8877 mean train loss:  1.00819721e-02, mean val. rec. loss:  9.33457271e-03\n",
      "Epoch: 8878 mean train loss:  1.00812235e-02, mean val. rec. loss:  9.33377950e-03\n",
      "Epoch: 8879 mean train loss:  1.00804749e-02, mean val. rec. loss:  9.33299138e-03\n",
      "Epoch: 8880 mean train loss:  1.00797216e-02, mean val. rec. loss:  9.33220270e-03\n",
      "Epoch: 8881 mean train loss:  1.00789665e-02, mean val. rec. loss:  9.33142196e-03\n",
      "Epoch: 8882 mean train loss:  1.00782197e-02, mean val. rec. loss:  9.33063328e-03\n",
      "Epoch: 8883 mean train loss:  1.00774730e-02, mean val. rec. loss:  9.32984233e-03\n",
      "Epoch: 8884 mean train loss:  1.00767188e-02, mean val. rec. loss:  9.32905422e-03\n",
      "Epoch: 8885 mean train loss:  1.00759628e-02, mean val. rec. loss:  9.32826610e-03\n",
      "Epoch: 8886 mean train loss:  1.00752170e-02, mean val. rec. loss:  9.32748026e-03\n",
      "Epoch: 8887 mean train loss:  1.00744674e-02, mean val. rec. loss:  9.32668534e-03\n",
      "Epoch: 8888 mean train loss:  1.00737151e-02, mean val. rec. loss:  9.32589836e-03\n",
      "Epoch: 8889 mean train loss:  1.00729619e-02, mean val. rec. loss:  9.32511705e-03\n",
      "Epoch: 8890 mean train loss:  1.00722142e-02, mean val. rec. loss:  9.32432327e-03\n",
      "Epoch: 8891 mean train loss:  1.00714656e-02, mean val. rec. loss:  9.32353402e-03\n",
      "Epoch: 8892 mean train loss:  1.00707114e-02, mean val. rec. loss:  9.32275157e-03\n",
      "Epoch: 8893 mean train loss:  1.00699609e-02, mean val. rec. loss:  9.32196800e-03\n",
      "Epoch: 8894 mean train loss:  1.00692142e-02, mean val. rec. loss:  9.32118215e-03\n",
      "Epoch: 8895 mean train loss:  1.00684619e-02, mean val. rec. loss:  9.32039177e-03\n",
      "Epoch: 8896 mean train loss:  1.00677142e-02, mean val. rec. loss:  9.31960195e-03\n",
      "Epoch: 8897 mean train loss:  1.00669572e-02, mean val. rec. loss:  9.31882461e-03\n",
      "Epoch: 8898 mean train loss:  1.00662133e-02, mean val. rec. loss:  9.31803196e-03\n",
      "Epoch: 8899 mean train loss:  1.00654600e-02, mean val. rec. loss:  9.31724385e-03\n",
      "Epoch: 8900 mean train loss:  1.00647123e-02, mean val. rec. loss:  9.31645007e-03\n",
      "Epoch: 8901 mean train loss:  1.00639563e-02, mean val. rec. loss:  9.31566876e-03\n",
      "Epoch: 8902 mean train loss:  1.00632133e-02, mean val. rec. loss:  9.31488518e-03\n",
      "Epoch: 8903 mean train loss:  1.00624600e-02, mean val. rec. loss:  9.31409310e-03\n",
      "Epoch: 8904 mean train loss:  1.00617114e-02, mean val. rec. loss:  9.31329988e-03\n",
      "Epoch: 8905 mean train loss:  1.00609554e-02, mean val. rec. loss:  9.31252537e-03\n",
      "Epoch: 8906 mean train loss:  1.00602124e-02, mean val. rec. loss:  9.31173669e-03\n",
      "Epoch: 8907 mean train loss:  1.00594619e-02, mean val. rec. loss:  9.31094121e-03\n",
      "Epoch: 8908 mean train loss:  1.00587077e-02, mean val. rec. loss:  9.31014856e-03\n",
      "Epoch: 8909 mean train loss:  1.00579591e-02, mean val. rec. loss:  9.30936271e-03\n",
      "Epoch: 8910 mean train loss:  1.00572124e-02, mean val. rec. loss:  9.30858707e-03\n",
      "Epoch: 8911 mean train loss:  1.00564628e-02, mean val. rec. loss:  9.30779159e-03\n",
      "Epoch: 8912 mean train loss:  1.00557087e-02, mean val. rec. loss:  9.30700064e-03\n",
      "Epoch: 8913 mean train loss:  1.00549591e-02, mean val. rec. loss:  9.30622387e-03\n",
      "Epoch: 8914 mean train loss:  1.00542133e-02, mean val. rec. loss:  9.30543462e-03\n",
      "Epoch: 8915 mean train loss:  1.00534647e-02, mean val. rec. loss:  9.30464254e-03\n",
      "Epoch: 8916 mean train loss:  1.00527096e-02, mean val. rec. loss:  9.30385612e-03\n",
      "Epoch: 8917 mean train loss:  1.00519629e-02, mean val. rec. loss:  9.30307028e-03\n",
      "Epoch: 8918 mean train loss:  1.00512161e-02, mean val. rec. loss:  9.30229010e-03\n",
      "Epoch: 8919 mean train loss:  1.00504638e-02, mean val. rec. loss:  9.30150199e-03\n",
      "Epoch: 8920 mean train loss:  1.00497115e-02, mean val. rec. loss:  9.30070934e-03\n",
      "Epoch: 8921 mean train loss:  1.00489619e-02, mean val. rec. loss:  9.29991896e-03\n",
      "Epoch: 8922 mean train loss:  1.00482171e-02, mean val. rec. loss:  9.29913821e-03\n",
      "Epoch: 8923 mean train loss:  1.00474675e-02, mean val. rec. loss:  9.29835067e-03\n",
      "Epoch: 8924 mean train loss:  1.00467143e-02, mean val. rec. loss:  9.29756595e-03\n",
      "Epoch: 8925 mean train loss:  1.00459666e-02, mean val. rec. loss:  9.29677897e-03\n",
      "Epoch: 8926 mean train loss:  1.00452199e-02, mean val. rec. loss:  9.29599766e-03\n",
      "Epoch: 8927 mean train loss:  1.00444675e-02, mean val. rec. loss:  9.29521182e-03\n",
      "Epoch: 8928 mean train loss:  1.00437171e-02, mean val. rec. loss:  9.29442484e-03\n",
      "Epoch: 8929 mean train loss:  1.00429694e-02, mean val. rec. loss:  9.29364013e-03\n",
      "Epoch: 8930 mean train loss:  1.00422217e-02, mean val. rec. loss:  9.29285428e-03\n",
      "Epoch: 8931 mean train loss:  1.00414694e-02, mean val. rec. loss:  9.29207411e-03\n",
      "Epoch: 8932 mean train loss:  1.00407208e-02, mean val. rec. loss:  9.29128769e-03\n",
      "Epoch: 8933 mean train loss:  1.00399731e-02, mean val. rec. loss:  9.29050241e-03\n",
      "Epoch: 8934 mean train loss:  1.00392236e-02, mean val. rec. loss:  9.28971657e-03\n",
      "Epoch: 8935 mean train loss:  1.00384750e-02, mean val. rec. loss:  9.28893242e-03\n",
      "Epoch: 8936 mean train loss:  1.00377255e-02, mean val. rec. loss:  9.28814658e-03\n",
      "Epoch: 8937 mean train loss:  1.00369778e-02, mean val. rec. loss:  9.28735506e-03\n",
      "Epoch: 8938 mean train loss:  1.00362292e-02, mean val. rec. loss:  9.28656355e-03\n",
      "Epoch: 8939 mean train loss:  1.00354815e-02, mean val. rec. loss:  9.28577883e-03\n",
      "Epoch: 8940 mean train loss:  1.00347329e-02, mean val. rec. loss:  9.28499356e-03\n",
      "Epoch: 8941 mean train loss:  1.00339843e-02, mean val. rec. loss:  9.28420658e-03\n",
      "Epoch: 8942 mean train loss:  1.00332348e-02, mean val. rec. loss:  9.28342413e-03\n",
      "Epoch: 8943 mean train loss:  1.00324862e-02, mean val. rec. loss:  9.28263942e-03\n",
      "Epoch: 8944 mean train loss:  1.00317376e-02, mean val. rec. loss:  9.28185074e-03\n",
      "Epoch: 8945 mean train loss:  1.00309871e-02, mean val. rec. loss:  9.28106433e-03\n",
      "Epoch: 8946 mean train loss:  1.00302367e-02, mean val. rec. loss:  9.28027508e-03\n",
      "Epoch: 8947 mean train loss:  1.00294899e-02, mean val. rec. loss:  9.27949150e-03\n",
      "Epoch: 8948 mean train loss:  1.00287423e-02, mean val. rec. loss:  9.27870395e-03\n",
      "Epoch: 8949 mean train loss:  1.00279927e-02, mean val. rec. loss:  9.27792094e-03\n",
      "Epoch: 8950 mean train loss:  1.00272451e-02, mean val. rec. loss:  9.27713056e-03\n",
      "Epoch: 8951 mean train loss:  1.00264937e-02, mean val. rec. loss:  9.27634585e-03\n",
      "Epoch: 8952 mean train loss:  1.00257479e-02, mean val. rec. loss:  9.27556227e-03\n",
      "Epoch: 8953 mean train loss:  1.00250021e-02, mean val. rec. loss:  9.27477699e-03\n",
      "Epoch: 8954 mean train loss:  1.00242516e-02, mean val. rec. loss:  9.27398775e-03\n",
      "Epoch: 8955 mean train loss:  1.00235011e-02, mean val. rec. loss:  9.27320020e-03\n",
      "Epoch: 8956 mean train loss:  1.00227544e-02, mean val. rec. loss:  9.27241662e-03\n",
      "Epoch: 8957 mean train loss:  1.00220086e-02, mean val. rec. loss:  9.27163134e-03\n",
      "Epoch: 8958 mean train loss:  1.00212572e-02, mean val. rec. loss:  9.27084266e-03\n",
      "Epoch: 8959 mean train loss:  1.00205077e-02, mean val. rec. loss:  9.27005852e-03\n",
      "Epoch: 8960 mean train loss:  1.00197619e-02, mean val. rec. loss:  9.26927551e-03\n",
      "Epoch: 8961 mean train loss:  1.00190170e-02, mean val. rec. loss:  9.26849079e-03\n",
      "Epoch: 8962 mean train loss:  1.00182647e-02, mean val. rec. loss:  9.26770608e-03\n",
      "Epoch: 8963 mean train loss:  1.00175151e-02, mean val. rec. loss:  9.26691910e-03\n",
      "Epoch: 8964 mean train loss:  1.00167693e-02, mean val. rec. loss:  9.26613212e-03\n",
      "Epoch: 8965 mean train loss:  1.00160226e-02, mean val. rec. loss:  9.26535195e-03\n",
      "Epoch: 8966 mean train loss:  1.00152721e-02, mean val. rec. loss:  9.26456724e-03\n",
      "Epoch: 8967 mean train loss:  1.00145207e-02, mean val. rec. loss:  9.26378763e-03\n",
      "Epoch: 8968 mean train loss:  1.00137768e-02, mean val. rec. loss:  9.26300178e-03\n",
      "Epoch: 8969 mean train loss:  1.00130319e-02, mean val. rec. loss:  9.26222614e-03\n",
      "Epoch: 8970 mean train loss:  1.00122814e-02, mean val. rec. loss:  9.26143859e-03\n",
      "Epoch: 8971 mean train loss:  1.00115310e-02, mean val. rec. loss:  9.26064935e-03\n",
      "Epoch: 8972 mean train loss:  1.00107870e-02, mean val. rec. loss:  9.25986350e-03\n",
      "Epoch: 8973 mean train loss:  1.00100356e-02, mean val. rec. loss:  9.25907312e-03\n",
      "Epoch: 8974 mean train loss:  1.00092889e-02, mean val. rec. loss:  9.25828614e-03\n",
      "Epoch: 8975 mean train loss:  1.00085366e-02, mean val. rec. loss:  9.25750426e-03\n",
      "Epoch: 8976 mean train loss:  1.00077954e-02, mean val. rec. loss:  9.25671898e-03\n",
      "Epoch: 8977 mean train loss:  1.00070459e-02, mean val. rec. loss:  9.25592974e-03\n",
      "Epoch: 8978 mean train loss:  1.00063001e-02, mean val. rec. loss:  9.25514956e-03\n",
      "Epoch: 8979 mean train loss:  1.00055506e-02, mean val. rec. loss:  9.25436598e-03\n",
      "Epoch: 8980 mean train loss:  1.00048020e-02, mean val. rec. loss:  9.25357673e-03\n",
      "Epoch: 8981 mean train loss:  1.00040543e-02, mean val. rec. loss:  9.25279202e-03\n",
      "Epoch: 8982 mean train loss:  1.00033057e-02, mean val. rec. loss:  9.25201298e-03\n",
      "Epoch: 8983 mean train loss:  1.00025599e-02, mean val. rec. loss:  9.25123054e-03\n",
      "Epoch: 8984 mean train loss:  1.00018076e-02, mean val. rec. loss:  9.25044412e-03\n",
      "Epoch: 8985 mean train loss:  1.00010664e-02, mean val. rec. loss:  9.24965771e-03\n",
      "Epoch: 8986 mean train loss:  1.00003178e-02, mean val. rec. loss:  9.24888377e-03\n",
      "Epoch: 8987 mean train loss:  9.99957199e-03, mean val. rec. loss:  9.24809396e-03\n",
      "Epoch: 8988 mean train loss:  9.99881781e-03, mean val. rec. loss:  9.24731378e-03\n",
      "Epoch: 8989 mean train loss:  9.99807759e-03, mean val. rec. loss:  9.24652567e-03\n",
      "Epoch: 8990 mean train loss:  9.99732712e-03, mean val. rec. loss:  9.24573755e-03\n",
      "Epoch: 8991 mean train loss:  9.99658132e-03, mean val. rec. loss:  9.24495624e-03\n",
      "Epoch: 8992 mean train loss:  9.99582899e-03, mean val. rec. loss:  9.24417720e-03\n",
      "Epoch: 8993 mean train loss:  9.99508784e-03, mean val. rec. loss:  9.24338795e-03\n",
      "Epoch: 8994 mean train loss:  9.99434110e-03, mean val. rec. loss:  9.24260324e-03\n",
      "Epoch: 8995 mean train loss:  9.99358878e-03, mean val. rec. loss:  9.24181173e-03\n",
      "Epoch: 8996 mean train loss:  9.99284204e-03, mean val. rec. loss:  9.24103439e-03\n",
      "Epoch: 8997 mean train loss:  9.99209902e-03, mean val. rec. loss:  9.24024967e-03\n",
      "Epoch: 8998 mean train loss:  9.99135135e-03, mean val. rec. loss:  9.23945759e-03\n",
      "Epoch: 8999 mean train loss:  9.99059996e-03, mean val. rec. loss:  9.23867685e-03\n",
      "Epoch: 9000 mean train loss:  9.98985322e-03, mean val. rec. loss:  9.23790291e-03\n",
      "Epoch: 9001 mean train loss:  9.98910835e-03, mean val. rec. loss:  9.23712047e-03\n",
      "Epoch: 9002 mean train loss:  9.98835975e-03, mean val. rec. loss:  9.23633405e-03\n",
      "Epoch: 9003 mean train loss:  9.98761301e-03, mean val. rec. loss:  9.23554537e-03\n",
      "Epoch: 9004 mean train loss:  9.98686627e-03, mean val. rec. loss:  9.23476293e-03\n",
      "Epoch: 9005 mean train loss:  9.98611860e-03, mean val. rec. loss:  9.23398105e-03\n",
      "Epoch: 9006 mean train loss:  9.98537093e-03, mean val. rec. loss:  9.23319577e-03\n",
      "Epoch: 9007 mean train loss:  9.98462792e-03, mean val. rec. loss:  9.23240199e-03\n",
      "Epoch: 9008 mean train loss:  9.98388118e-03, mean val. rec. loss:  9.23162522e-03\n",
      "Epoch: 9009 mean train loss:  9.98312885e-03, mean val. rec. loss:  9.23084391e-03\n",
      "Epoch: 9010 mean train loss:  9.98238304e-03, mean val. rec. loss:  9.23006260e-03\n",
      "Epoch: 9011 mean train loss:  9.98164003e-03, mean val. rec. loss:  9.22926995e-03\n",
      "Epoch: 9012 mean train loss:  9.98089143e-03, mean val. rec. loss:  9.22848977e-03\n",
      "Epoch: 9013 mean train loss:  9.98014469e-03, mean val. rec. loss:  9.22770959e-03\n",
      "Epoch: 9014 mean train loss:  9.97939982e-03, mean val. rec. loss:  9.22692261e-03\n",
      "Epoch: 9015 mean train loss:  9.97865494e-03, mean val. rec. loss:  9.22612430e-03\n",
      "Epoch: 9016 mean train loss:  9.97790541e-03, mean val. rec. loss:  9.22534639e-03\n",
      "Epoch: 9017 mean train loss:  9.97715681e-03, mean val. rec. loss:  9.22456735e-03\n",
      "Epoch: 9018 mean train loss:  9.97641193e-03, mean val. rec. loss:  9.22378547e-03\n",
      "Epoch: 9019 mean train loss:  9.97566799e-03, mean val. rec. loss:  9.22299622e-03\n",
      "Epoch: 9020 mean train loss:  9.97491659e-03, mean val. rec. loss:  9.22221434e-03\n",
      "Epoch: 9021 mean train loss:  9.97417358e-03, mean val. rec. loss:  9.22143644e-03\n",
      "Epoch: 9022 mean train loss:  9.97342311e-03, mean val. rec. loss:  9.22065626e-03\n",
      "Epoch: 9023 mean train loss:  9.97268103e-03, mean val. rec. loss:  9.21986701e-03\n",
      "Epoch: 9024 mean train loss:  9.97193057e-03, mean val. rec. loss:  9.21908514e-03\n",
      "Epoch: 9025 mean train loss:  9.97118849e-03, mean val. rec. loss:  9.21829589e-03\n",
      "Epoch: 9026 mean train loss:  9.97043895e-03, mean val. rec. loss:  9.21751855e-03\n",
      "Epoch: 9027 mean train loss:  9.96969035e-03, mean val. rec. loss:  9.21673270e-03\n",
      "Epoch: 9028 mean train loss:  9.96894641e-03, mean val. rec. loss:  9.21594515e-03\n",
      "Epoch: 9029 mean train loss:  9.96820153e-03, mean val. rec. loss:  9.21516441e-03\n",
      "Epoch: 9030 mean train loss:  9.96745107e-03, mean val. rec. loss:  9.21438820e-03\n",
      "Epoch: 9031 mean train loss:  9.96670992e-03, mean val. rec. loss:  9.21360122e-03\n",
      "Epoch: 9032 mean train loss:  9.96595852e-03, mean val. rec. loss:  9.21281935e-03\n",
      "Epoch: 9033 mean train loss:  9.96521644e-03, mean val. rec. loss:  9.21203520e-03\n",
      "Epoch: 9034 mean train loss:  9.96446505e-03, mean val. rec. loss:  9.21125673e-03\n",
      "Epoch: 9035 mean train loss:  9.96372296e-03, mean val. rec. loss:  9.21047315e-03\n",
      "Epoch: 9036 mean train loss:  9.96297250e-03, mean val. rec. loss:  9.20969071e-03\n",
      "Epoch: 9037 mean train loss:  9.96223135e-03, mean val. rec. loss:  9.20890770e-03\n",
      "Epoch: 9038 mean train loss:  9.96147903e-03, mean val. rec. loss:  9.20812809e-03\n",
      "Epoch: 9039 mean train loss:  9.96073694e-03, mean val. rec. loss:  9.20734281e-03\n",
      "Epoch: 9040 mean train loss:  9.95998555e-03, mean val. rec. loss:  9.20655469e-03\n",
      "Epoch: 9041 mean train loss:  9.95924067e-03, mean val. rec. loss:  9.20577055e-03\n",
      "Epoch: 9042 mean train loss:  9.95849207e-03, mean val. rec. loss:  9.20499491e-03\n",
      "Epoch: 9043 mean train loss:  9.95775278e-03, mean val. rec. loss:  9.20420963e-03\n",
      "Epoch: 9044 mean train loss:  9.95700046e-03, mean val. rec. loss:  9.20342152e-03\n",
      "Epoch: 9045 mean train loss:  9.95625558e-03, mean val. rec. loss:  9.20264077e-03\n",
      "Epoch: 9046 mean train loss:  9.95550698e-03, mean val. rec. loss:  9.20185890e-03\n",
      "Epoch: 9047 mean train loss:  9.95476862e-03, mean val. rec. loss:  9.20107532e-03\n",
      "Epoch: 9048 mean train loss:  9.95401537e-03, mean val. rec. loss:  9.20029344e-03\n",
      "Epoch: 9049 mean train loss:  9.95327049e-03, mean val. rec. loss:  9.19950703e-03\n",
      "Epoch: 9050 mean train loss:  9.95252561e-03, mean val. rec. loss:  9.19872742e-03\n",
      "Epoch: 9051 mean train loss:  9.95178074e-03, mean val. rec. loss:  9.19794384e-03\n",
      "Epoch: 9052 mean train loss:  9.95103027e-03, mean val. rec. loss:  9.19716026e-03\n",
      "Epoch: 9053 mean train loss:  9.95028912e-03, mean val. rec. loss:  9.19636988e-03\n",
      "Epoch: 9054 mean train loss:  9.94953866e-03, mean val. rec. loss:  9.19559368e-03\n",
      "Epoch: 9055 mean train loss:  9.94879937e-03, mean val. rec. loss:  9.19481067e-03\n",
      "Epoch: 9056 mean train loss:  9.94804704e-03, mean val. rec. loss:  9.19402539e-03\n",
      "Epoch: 9057 mean train loss:  9.94730589e-03, mean val. rec. loss:  9.19323274e-03\n",
      "Epoch: 9058 mean train loss:  9.94655450e-03, mean val. rec. loss:  9.19246220e-03\n",
      "Epoch: 9059 mean train loss:  9.94581149e-03, mean val. rec. loss:  9.19167465e-03\n",
      "Epoch: 9060 mean train loss:  9.94506475e-03, mean val. rec. loss:  9.19089278e-03\n",
      "Epoch: 9061 mean train loss:  9.94431894e-03, mean val. rec. loss:  9.19010353e-03\n",
      "Epoch: 9062 mean train loss:  9.94357406e-03, mean val. rec. loss:  9.18932392e-03\n",
      "Epoch: 9063 mean train loss:  9.94282732e-03, mean val. rec. loss:  9.18853977e-03\n",
      "Epoch: 9064 mean train loss:  9.94208245e-03, mean val. rec. loss:  9.18775847e-03\n",
      "Epoch: 9065 mean train loss:  9.94133571e-03, mean val. rec. loss:  9.18697602e-03\n",
      "Epoch: 9066 mean train loss:  9.94059270e-03, mean val. rec. loss:  9.18619131e-03\n",
      "Epoch: 9067 mean train loss:  9.93984037e-03, mean val. rec. loss:  9.18541907e-03\n",
      "Epoch: 9068 mean train loss:  9.93910108e-03, mean val. rec. loss:  9.18463436e-03\n",
      "Epoch: 9069 mean train loss:  9.93835062e-03, mean val. rec. loss:  9.18384625e-03\n",
      "Epoch: 9070 mean train loss:  9.93760760e-03, mean val. rec. loss:  9.18306947e-03\n",
      "Epoch: 9071 mean train loss:  9.93685900e-03, mean val. rec. loss:  9.18228930e-03\n",
      "Epoch: 9072 mean train loss:  9.93612065e-03, mean val. rec. loss:  9.18150005e-03\n",
      "Epoch: 9073 mean train loss:  9.93536739e-03, mean val. rec. loss:  9.18071987e-03\n",
      "Epoch: 9074 mean train loss:  9.93462438e-03, mean val. rec. loss:  9.17993516e-03\n",
      "Epoch: 9075 mean train loss:  9.93387671e-03, mean val. rec. loss:  9.17915782e-03\n",
      "Epoch: 9076 mean train loss:  9.93313742e-03, mean val. rec. loss:  9.17837481e-03\n",
      "Epoch: 9077 mean train loss:  9.93238602e-03, mean val. rec. loss:  9.17759293e-03\n",
      "Epoch: 9078 mean train loss:  9.93164115e-03, mean val. rec. loss:  9.17682126e-03\n",
      "Epoch: 9079 mean train loss:  9.93089348e-03, mean val. rec. loss:  9.17603655e-03\n",
      "Epoch: 9080 mean train loss:  9.93015512e-03, mean val. rec. loss:  9.17525240e-03\n",
      "Epoch: 9081 mean train loss:  9.92940466e-03, mean val. rec. loss:  9.17446599e-03\n",
      "Epoch: 9082 mean train loss:  9.92865512e-03, mean val. rec. loss:  9.17368582e-03\n",
      "Epoch: 9083 mean train loss:  9.92791677e-03, mean val. rec. loss:  9.17290564e-03\n",
      "Epoch: 9084 mean train loss:  9.92717003e-03, mean val. rec. loss:  9.17211979e-03\n",
      "Epoch: 9085 mean train loss:  9.92642143e-03, mean val. rec. loss:  9.17133508e-03\n",
      "Epoch: 9086 mean train loss:  9.92567748e-03, mean val. rec. loss:  9.17055264e-03\n",
      "Epoch: 9087 mean train loss:  9.92493167e-03, mean val. rec. loss:  9.16976623e-03\n",
      "Epoch: 9088 mean train loss:  9.92418587e-03, mean val. rec. loss:  9.16898492e-03\n",
      "Epoch: 9089 mean train loss:  9.92344099e-03, mean val. rec. loss:  9.16820077e-03\n",
      "Epoch: 9090 mean train loss:  9.92269518e-03, mean val. rec. loss:  9.16741719e-03\n",
      "Epoch: 9091 mean train loss:  9.92194844e-03, mean val. rec. loss:  9.16663021e-03\n",
      "Epoch: 9092 mean train loss:  9.92119984e-03, mean val. rec. loss:  9.16586478e-03\n",
      "Epoch: 9093 mean train loss:  9.92045869e-03, mean val. rec. loss:  9.16507496e-03\n",
      "Epoch: 9094 mean train loss:  9.91971475e-03, mean val. rec. loss:  9.16428742e-03\n",
      "Epoch: 9095 mean train loss:  9.91896801e-03, mean val. rec. loss:  9.16350271e-03\n",
      "Epoch: 9096 mean train loss:  9.91822220e-03, mean val. rec. loss:  9.16272933e-03\n",
      "Epoch: 9097 mean train loss:  9.91747639e-03, mean val. rec. loss:  9.16194859e-03\n",
      "Epoch: 9098 mean train loss:  9.91673152e-03, mean val. rec. loss:  9.16115764e-03\n",
      "Epoch: 9099 mean train loss:  9.91598385e-03, mean val. rec. loss:  9.16037406e-03\n",
      "Epoch: 9100 mean train loss:  9.91523711e-03, mean val. rec. loss:  9.15960580e-03\n",
      "Epoch: 9101 mean train loss:  9.91449410e-03, mean val. rec. loss:  9.15881711e-03\n",
      "Epoch: 9102 mean train loss:  9.91375015e-03, mean val. rec. loss:  9.15802446e-03\n",
      "Epoch: 9103 mean train loss:  9.91300434e-03, mean val. rec. loss:  9.15724315e-03\n",
      "Epoch: 9104 mean train loss:  9.91225947e-03, mean val. rec. loss:  9.15647205e-03\n",
      "Epoch: 9105 mean train loss:  9.91151459e-03, mean val. rec. loss:  9.15568961e-03\n",
      "Epoch: 9106 mean train loss:  9.91076972e-03, mean val. rec. loss:  9.15490149e-03\n",
      "Epoch: 9107 mean train loss:  9.91002484e-03, mean val. rec. loss:  9.15411905e-03\n",
      "Epoch: 9108 mean train loss:  9.90927903e-03, mean val. rec. loss:  9.15335021e-03\n",
      "Epoch: 9109 mean train loss:  9.90853416e-03, mean val. rec. loss:  9.15256890e-03\n",
      "Epoch: 9110 mean train loss:  9.90778835e-03, mean val. rec. loss:  9.15177682e-03\n",
      "Epoch: 9111 mean train loss:  9.90704627e-03, mean val. rec. loss:  9.15099551e-03\n",
      "Epoch: 9112 mean train loss:  9.90630232e-03, mean val. rec. loss:  9.15021930e-03\n",
      "Epoch: 9113 mean train loss:  9.90555558e-03, mean val. rec. loss:  9.14944140e-03\n",
      "Epoch: 9114 mean train loss:  9.90480884e-03, mean val. rec. loss:  9.14865555e-03\n",
      "Epoch: 9115 mean train loss:  9.90406397e-03, mean val. rec. loss:  9.14787311e-03\n",
      "Epoch: 9116 mean train loss:  9.90331909e-03, mean val. rec. loss:  9.14709690e-03\n",
      "Epoch: 9117 mean train loss:  9.90257421e-03, mean val. rec. loss:  9.14631729e-03\n",
      "Epoch: 9118 mean train loss:  9.90182934e-03, mean val. rec. loss:  9.14553144e-03\n",
      "Epoch: 9119 mean train loss:  9.90108260e-03, mean val. rec. loss:  9.14474220e-03\n",
      "Epoch: 9120 mean train loss:  9.90033400e-03, mean val. rec. loss:  9.14396769e-03\n",
      "Epoch: 9121 mean train loss:  9.89959099e-03, mean val. rec. loss:  9.14318808e-03\n",
      "Epoch: 9122 mean train loss:  9.89884983e-03, mean val. rec. loss:  9.14240394e-03\n",
      "Epoch: 9123 mean train loss:  9.89810123e-03, mean val. rec. loss:  9.14161696e-03\n",
      "Epoch: 9124 mean train loss:  9.89735449e-03, mean val. rec. loss:  9.14083281e-03\n",
      "Epoch: 9125 mean train loss:  9.89661148e-03, mean val. rec. loss:  9.14005774e-03\n",
      "Epoch: 9126 mean train loss:  9.89586847e-03, mean val. rec. loss:  9.13927756e-03\n",
      "Epoch: 9127 mean train loss:  9.89512173e-03, mean val. rec. loss:  9.13848832e-03\n",
      "Epoch: 9128 mean train loss:  9.89437406e-03, mean val. rec. loss:  9.13770531e-03\n",
      "Epoch: 9129 mean train loss:  9.89363011e-03, mean val. rec. loss:  9.13692683e-03\n",
      "Epoch: 9130 mean train loss:  9.89288896e-03, mean val. rec. loss:  9.13615232e-03\n",
      "Epoch: 9131 mean train loss:  9.89214129e-03, mean val. rec. loss:  9.13536308e-03\n",
      "Epoch: 9132 mean train loss:  9.89139362e-03, mean val. rec. loss:  9.13457383e-03\n",
      "Epoch: 9133 mean train loss:  9.89065061e-03, mean val. rec. loss:  9.13380159e-03\n",
      "Epoch: 9134 mean train loss:  9.88990759e-03, mean val. rec. loss:  9.13302708e-03\n",
      "Epoch: 9135 mean train loss:  9.88916179e-03, mean val. rec. loss:  9.13223897e-03\n",
      "Epoch: 9136 mean train loss:  9.88841598e-03, mean val. rec. loss:  9.13144859e-03\n",
      "Epoch: 9137 mean train loss:  9.88767204e-03, mean val. rec. loss:  9.13067805e-03\n",
      "Epoch: 9138 mean train loss:  9.88692623e-03, mean val. rec. loss:  9.12990865e-03\n",
      "Epoch: 9139 mean train loss:  9.88617949e-03, mean val. rec. loss:  9.12911430e-03\n",
      "Epoch: 9140 mean train loss:  9.88543461e-03, mean val. rec. loss:  9.12832675e-03\n",
      "Epoch: 9141 mean train loss:  9.88469160e-03, mean val. rec. loss:  9.12754884e-03\n",
      "Epoch: 9142 mean train loss:  9.88394859e-03, mean val. rec. loss:  9.12678114e-03\n",
      "Epoch: 9143 mean train loss:  9.88320185e-03, mean val. rec. loss:  9.12598849e-03\n",
      "Epoch: 9144 mean train loss:  9.88245511e-03, mean val. rec. loss:  9.12520265e-03\n",
      "Epoch: 9145 mean train loss:  9.88171023e-03, mean val. rec. loss:  9.12442077e-03\n",
      "Epoch: 9146 mean train loss:  9.88096442e-03, mean val. rec. loss:  9.12364570e-03\n",
      "Epoch: 9147 mean train loss:  9.88022234e-03, mean val. rec. loss:  9.12286552e-03\n",
      "Epoch: 9148 mean train loss:  9.87947560e-03, mean val. rec. loss:  9.12207854e-03\n",
      "Epoch: 9149 mean train loss:  9.87873631e-03, mean val. rec. loss:  9.12129950e-03\n",
      "Epoch: 9150 mean train loss:  9.87798492e-03, mean val. rec. loss:  9.12052499e-03\n",
      "Epoch: 9151 mean train loss:  9.87724284e-03, mean val. rec. loss:  9.11974595e-03\n",
      "Epoch: 9152 mean train loss:  9.87649610e-03, mean val. rec. loss:  9.11895897e-03\n",
      "Epoch: 9153 mean train loss:  9.87575681e-03, mean val. rec. loss:  9.11816689e-03\n",
      "Epoch: 9154 mean train loss:  9.87500635e-03, mean val. rec. loss:  9.11739635e-03\n",
      "Epoch: 9155 mean train loss:  9.87426333e-03, mean val. rec. loss:  9.11662014e-03\n",
      "Epoch: 9156 mean train loss:  9.87351287e-03, mean val. rec. loss:  9.11583033e-03\n",
      "Epoch: 9157 mean train loss:  9.87277544e-03, mean val. rec. loss:  9.11504278e-03\n",
      "Epoch: 9158 mean train loss:  9.87202870e-03, mean val. rec. loss:  9.11426998e-03\n",
      "Epoch: 9159 mean train loss:  9.87128569e-03, mean val. rec. loss:  9.11349547e-03\n",
      "Epoch: 9160 mean train loss:  9.87053709e-03, mean val. rec. loss:  9.11270622e-03\n",
      "Epoch: 9161 mean train loss:  9.86979221e-03, mean val. rec. loss:  9.11191811e-03\n",
      "Epoch: 9162 mean train loss:  9.86905106e-03, mean val. rec. loss:  9.11114871e-03\n",
      "Epoch: 9163 mean train loss:  9.86830712e-03, mean val. rec. loss:  9.11037137e-03\n",
      "Epoch: 9164 mean train loss:  9.86755758e-03, mean val. rec. loss:  9.10957985e-03\n",
      "Epoch: 9165 mean train loss:  9.86681271e-03, mean val. rec. loss:  9.10879344e-03\n",
      "Epoch: 9166 mean train loss:  9.86607249e-03, mean val. rec. loss:  9.10801156e-03\n",
      "Epoch: 9167 mean train loss:  9.86532761e-03, mean val. rec. loss:  9.10724329e-03\n",
      "Epoch: 9168 mean train loss:  9.86457808e-03, mean val. rec. loss:  9.10645858e-03\n",
      "Epoch: 9169 mean train loss:  9.86383413e-03, mean val. rec. loss:  9.10566706e-03\n",
      "Epoch: 9170 mean train loss:  9.86309391e-03, mean val. rec. loss:  9.10489142e-03\n",
      "Epoch: 9171 mean train loss:  9.86234811e-03, mean val. rec. loss:  9.10411862e-03\n",
      "Epoch: 9172 mean train loss:  9.86159951e-03, mean val. rec. loss:  9.10333391e-03\n",
      "Epoch: 9173 mean train loss:  9.86085463e-03, mean val. rec. loss:  9.10254693e-03\n",
      "Epoch: 9174 mean train loss:  9.86011255e-03, mean val. rec. loss:  9.10176278e-03\n",
      "Epoch: 9175 mean train loss:  9.85936953e-03, mean val. rec. loss:  9.10099168e-03\n",
      "Epoch: 9176 mean train loss:  9.85862279e-03, mean val. rec. loss:  9.10021320e-03\n",
      "Epoch: 9177 mean train loss:  9.85787606e-03, mean val. rec. loss:  9.09942566e-03\n",
      "Epoch: 9178 mean train loss:  9.85713304e-03, mean val. rec. loss:  9.09864038e-03\n",
      "Epoch: 9179 mean train loss:  9.85639096e-03, mean val. rec. loss:  9.09786984e-03\n",
      "Epoch: 9180 mean train loss:  9.85564515e-03, mean val. rec. loss:  9.09709250e-03\n",
      "Epoch: 9181 mean train loss:  9.85489655e-03, mean val. rec. loss:  9.09630325e-03\n",
      "Epoch: 9182 mean train loss:  9.85415261e-03, mean val. rec. loss:  9.09551741e-03\n",
      "Epoch: 9183 mean train loss:  9.85340773e-03, mean val. rec. loss:  9.09474290e-03\n",
      "Epoch: 9184 mean train loss:  9.85266378e-03, mean val. rec. loss:  9.09396953e-03\n",
      "Epoch: 9185 mean train loss:  9.85191891e-03, mean val. rec. loss:  9.09317858e-03\n",
      "Epoch: 9186 mean train loss:  9.85117403e-03, mean val. rec. loss:  9.09239103e-03\n",
      "Epoch: 9187 mean train loss:  9.85042916e-03, mean val. rec. loss:  9.09162050e-03\n",
      "Epoch: 9188 mean train loss:  9.84968614e-03, mean val. rec. loss:  9.09084316e-03\n",
      "Epoch: 9189 mean train loss:  9.84893940e-03, mean val. rec. loss:  9.09005334e-03\n",
      "Epoch: 9190 mean train loss:  9.84819453e-03, mean val. rec. loss:  9.08926353e-03\n",
      "Epoch: 9191 mean train loss:  9.84745058e-03, mean val. rec. loss:  9.08849639e-03\n",
      "Epoch: 9192 mean train loss:  9.84670664e-03, mean val. rec. loss:  9.08772359e-03\n",
      "Epoch: 9193 mean train loss:  9.84596083e-03, mean val. rec. loss:  9.08693207e-03\n",
      "Epoch: 9194 mean train loss:  9.84521595e-03, mean val. rec. loss:  9.08614452e-03\n",
      "Epoch: 9195 mean train loss:  9.84447201e-03, mean val. rec. loss:  9.08537342e-03\n",
      "Epoch: 9196 mean train loss:  9.84372713e-03, mean val. rec. loss:  9.08460061e-03\n",
      "Epoch: 9197 mean train loss:  9.84298226e-03, mean val. rec. loss:  9.08380910e-03\n",
      "Epoch: 9198 mean train loss:  9.84223738e-03, mean val. rec. loss:  9.08302098e-03\n",
      "Epoch: 9199 mean train loss:  9.84149437e-03, mean val. rec. loss:  9.08224648e-03\n",
      "Epoch: 9200 mean train loss:  9.84074856e-03, mean val. rec. loss:  9.08147084e-03\n",
      "Epoch: 9201 mean train loss:  9.84000648e-03, mean val. rec. loss:  9.08068216e-03\n",
      "Epoch: 9202 mean train loss:  9.83926439e-03, mean val. rec. loss:  9.07989518e-03\n",
      "Epoch: 9203 mean train loss:  9.83851672e-03, mean val. rec. loss:  9.07911500e-03\n",
      "Epoch: 9204 mean train loss:  9.83776998e-03, mean val. rec. loss:  9.07833596e-03\n",
      "Epoch: 9205 mean train loss:  9.83702511e-03, mean val. rec. loss:  9.07756542e-03\n",
      "Epoch: 9206 mean train loss:  9.83628582e-03, mean val. rec. loss:  9.07678185e-03\n",
      "Epoch: 9207 mean train loss:  9.83554094e-03, mean val. rec. loss:  9.07599827e-03\n",
      "Epoch: 9208 mean train loss:  9.83479141e-03, mean val. rec. loss:  9.07521923e-03\n",
      "Epoch: 9209 mean train loss:  9.83404467e-03, mean val. rec. loss:  9.07444585e-03\n",
      "Epoch: 9210 mean train loss:  9.83330631e-03, mean val. rec. loss:  9.07365887e-03\n",
      "Epoch: 9211 mean train loss:  9.83255958e-03, mean val. rec. loss:  9.07288040e-03\n",
      "Epoch: 9212 mean train loss:  9.83181749e-03, mean val. rec. loss:  9.07209512e-03\n",
      "Epoch: 9213 mean train loss:  9.83106796e-03, mean val. rec. loss:  9.07130927e-03\n",
      "Epoch: 9214 mean train loss:  9.83032402e-03, mean val. rec. loss:  9.07053363e-03\n",
      "Epoch: 9215 mean train loss:  9.82958380e-03, mean val. rec. loss:  9.06975970e-03\n",
      "Epoch: 9216 mean train loss:  9.82883892e-03, mean val. rec. loss:  9.06896988e-03\n",
      "Epoch: 9217 mean train loss:  9.82809032e-03, mean val. rec. loss:  9.06818630e-03\n",
      "Epoch: 9218 mean train loss:  9.82734637e-03, mean val. rec. loss:  9.06742427e-03\n",
      "Epoch: 9219 mean train loss:  9.82660522e-03, mean val. rec. loss:  9.06663842e-03\n",
      "Epoch: 9220 mean train loss:  9.82586128e-03, mean val. rec. loss:  9.06584804e-03\n",
      "Epoch: 9221 mean train loss:  9.82511268e-03, mean val. rec. loss:  9.06506560e-03\n",
      "Epoch: 9222 mean train loss:  9.82436780e-03, mean val. rec. loss:  9.06429676e-03\n",
      "Epoch: 9223 mean train loss:  9.82362758e-03, mean val. rec. loss:  9.06351886e-03\n",
      "Epoch: 9224 mean train loss:  9.82288270e-03, mean val. rec. loss:  9.06272847e-03\n",
      "Epoch: 9225 mean train loss:  9.82213503e-03, mean val. rec. loss:  9.06194716e-03\n",
      "Epoch: 9226 mean train loss:  9.82139016e-03, mean val. rec. loss:  9.06118003e-03\n",
      "Epoch: 9227 mean train loss:  9.82064621e-03, mean val. rec. loss:  9.06039362e-03\n",
      "Epoch: 9228 mean train loss:  9.81990413e-03, mean val. rec. loss:  9.05960323e-03\n",
      "Epoch: 9229 mean train loss:  9.81915739e-03, mean val. rec. loss:  9.05882192e-03\n",
      "Epoch: 9230 mean train loss:  9.81841065e-03, mean val. rec. loss:  9.05804628e-03\n",
      "Epoch: 9231 mean train loss:  9.81766764e-03, mean val. rec. loss:  9.05726951e-03\n",
      "Epoch: 9232 mean train loss:  9.81692462e-03, mean val. rec. loss:  9.05648253e-03\n",
      "Epoch: 9233 mean train loss:  9.81617882e-03, mean val. rec. loss:  9.05570576e-03\n",
      "Epoch: 9234 mean train loss:  9.81543487e-03, mean val. rec. loss:  9.05493068e-03\n",
      "Epoch: 9235 mean train loss:  9.81469000e-03, mean val. rec. loss:  9.05415107e-03\n",
      "Epoch: 9236 mean train loss:  9.81394698e-03, mean val. rec. loss:  9.05336750e-03\n",
      "Epoch: 9237 mean train loss:  9.81320024e-03, mean val. rec. loss:  9.05258165e-03\n",
      "Epoch: 9238 mean train loss:  9.81245350e-03, mean val. rec. loss:  9.05180715e-03\n",
      "Epoch: 9239 mean train loss:  9.81170956e-03, mean val. rec. loss:  9.05102130e-03\n",
      "Epoch: 9240 mean train loss:  9.81096189e-03, mean val. rec. loss:  9.05023715e-03\n",
      "Epoch: 9241 mean train loss:  9.81021981e-03, mean val. rec. loss:  9.04945981e-03\n",
      "Epoch: 9242 mean train loss:  9.80947586e-03, mean val. rec. loss:  9.04868417e-03\n",
      "Epoch: 9243 mean train loss:  9.80873005e-03, mean val. rec. loss:  9.04790286e-03\n",
      "Epoch: 9244 mean train loss:  9.80798797e-03, mean val. rec. loss:  9.04712269e-03\n",
      "Epoch: 9245 mean train loss:  9.80724123e-03, mean val. rec. loss:  9.04634081e-03\n",
      "Epoch: 9246 mean train loss:  9.80649356e-03, mean val. rec. loss:  9.04556460e-03\n",
      "Epoch: 9247 mean train loss:  9.80575521e-03, mean val. rec. loss:  9.04478443e-03\n",
      "Epoch: 9248 mean train loss:  9.80501219e-03, mean val. rec. loss:  9.04400198e-03\n",
      "Epoch: 9249 mean train loss:  9.80426545e-03, mean val. rec. loss:  9.04321897e-03\n",
      "Epoch: 9250 mean train loss:  9.80351778e-03, mean val. rec. loss:  9.04243936e-03\n",
      "Epoch: 9251 mean train loss:  9.80277570e-03, mean val. rec. loss:  9.04166146e-03\n",
      "Epoch: 9252 mean train loss:  9.80203269e-03, mean val. rec. loss:  9.04088071e-03\n",
      "Epoch: 9253 mean train loss:  9.80128595e-03, mean val. rec. loss:  9.04009657e-03\n",
      "Epoch: 9254 mean train loss:  9.80054014e-03, mean val. rec. loss:  9.03931866e-03\n",
      "Epoch: 9255 mean train loss:  9.79979899e-03, mean val. rec. loss:  9.03854132e-03\n",
      "Epoch: 9256 mean train loss:  9.79904946e-03, mean val. rec. loss:  9.03776058e-03\n",
      "Epoch: 9257 mean train loss:  9.79830831e-03, mean val. rec. loss:  9.03697757e-03\n",
      "Epoch: 9258 mean train loss:  9.79756529e-03, mean val. rec. loss:  9.03620249e-03\n",
      "Epoch: 9259 mean train loss:  9.79681855e-03, mean val. rec. loss:  9.03541778e-03\n",
      "Epoch: 9260 mean train loss:  9.79607181e-03, mean val. rec. loss:  9.03463817e-03\n",
      "Epoch: 9261 mean train loss:  9.79532880e-03, mean val. rec. loss:  9.03385970e-03\n",
      "Epoch: 9262 mean train loss:  9.79458672e-03, mean val. rec. loss:  9.03307499e-03\n",
      "Epoch: 9263 mean train loss:  9.79383812e-03, mean val. rec. loss:  9.03229708e-03\n",
      "Epoch: 9264 mean train loss:  9.79309510e-03, mean val. rec. loss:  9.03151237e-03\n",
      "Epoch: 9265 mean train loss:  9.79235302e-03, mean val. rec. loss:  9.03072992e-03\n",
      "Epoch: 9266 mean train loss:  9.79160628e-03, mean val. rec. loss:  9.02995201e-03\n",
      "Epoch: 9267 mean train loss:  9.79085954e-03, mean val. rec. loss:  9.02917637e-03\n",
      "Epoch: 9268 mean train loss:  9.79011653e-03, mean val. rec. loss:  9.02839166e-03\n",
      "Epoch: 9269 mean train loss:  9.78937445e-03, mean val. rec. loss:  9.02761375e-03\n",
      "Epoch: 9270 mean train loss:  9.78862864e-03, mean val. rec. loss:  9.02682791e-03\n",
      "Epoch: 9271 mean train loss:  9.78788004e-03, mean val. rec. loss:  9.02605170e-03\n",
      "Epoch: 9272 mean train loss:  9.78713609e-03, mean val. rec. loss:  9.02526982e-03\n",
      "Epoch: 9273 mean train loss:  9.78639308e-03, mean val. rec. loss:  9.02448851e-03\n",
      "Epoch: 9274 mean train loss:  9.78565100e-03, mean val. rec. loss:  9.02370777e-03\n",
      "Epoch: 9275 mean train loss:  9.78490426e-03, mean val. rec. loss:  9.02293327e-03\n",
      "Epoch: 9276 mean train loss:  9.78415752e-03, mean val. rec. loss:  9.02215309e-03\n",
      "Epoch: 9277 mean train loss:  9.78341451e-03, mean val. rec. loss:  9.02137121e-03\n",
      "Epoch: 9278 mean train loss:  9.78267149e-03, mean val. rec. loss:  9.02059557e-03\n",
      "Epoch: 9279 mean train loss:  9.78192475e-03, mean val. rec. loss:  9.01981086e-03\n",
      "Epoch: 9280 mean train loss:  9.78117802e-03, mean val. rec. loss:  9.01903409e-03\n",
      "Epoch: 9281 mean train loss:  9.78043500e-03, mean val. rec. loss:  9.01825051e-03\n",
      "Epoch: 9282 mean train loss:  9.77969292e-03, mean val. rec. loss:  9.01746523e-03\n",
      "Epoch: 9283 mean train loss:  9.77894711e-03, mean val. rec. loss:  9.01668902e-03\n",
      "Epoch: 9284 mean train loss:  9.77819944e-03, mean val. rec. loss:  9.01591338e-03\n",
      "Epoch: 9285 mean train loss:  9.77745643e-03, mean val. rec. loss:  9.01512640e-03\n",
      "Epoch: 9286 mean train loss:  9.77671528e-03, mean val. rec. loss:  9.01434509e-03\n",
      "Epoch: 9287 mean train loss:  9.77596668e-03, mean val. rec. loss:  9.01356152e-03\n",
      "Epoch: 9288 mean train loss:  9.77521714e-03, mean val. rec. loss:  9.01278474e-03\n",
      "Epoch: 9289 mean train loss:  9.77447692e-03, mean val. rec. loss:  9.01199776e-03\n",
      "Epoch: 9290 mean train loss:  9.77373484e-03, mean val. rec. loss:  9.01122156e-03\n",
      "Epoch: 9291 mean train loss:  9.77298717e-03, mean val. rec. loss:  9.01044705e-03\n",
      "Epoch: 9292 mean train loss:  9.77224043e-03, mean val. rec. loss:  9.00966744e-03\n",
      "Epoch: 9293 mean train loss:  9.77149835e-03, mean val. rec. loss:  9.00887819e-03\n",
      "Epoch: 9294 mean train loss:  9.77075533e-03, mean val. rec. loss:  9.00810822e-03\n",
      "Epoch: 9295 mean train loss:  9.77000860e-03, mean val. rec. loss:  9.00733258e-03\n",
      "Epoch: 9296 mean train loss:  9.76926186e-03, mean val. rec. loss:  9.00654447e-03\n",
      "Epoch: 9297 mean train loss:  9.76851884e-03, mean val. rec. loss:  9.00576316e-03\n",
      "Epoch: 9298 mean train loss:  9.76777862e-03, mean val. rec. loss:  9.00498242e-03\n",
      "Epoch: 9299 mean train loss:  9.76702723e-03, mean val. rec. loss:  9.00420451e-03\n",
      "Epoch: 9300 mean train loss:  9.76628422e-03, mean val. rec. loss:  9.00341583e-03\n",
      "Epoch: 9301 mean train loss:  9.76553748e-03, mean val. rec. loss:  9.00264132e-03\n",
      "Epoch: 9302 mean train loss:  9.76479912e-03, mean val. rec. loss:  9.00186682e-03\n",
      "Epoch: 9303 mean train loss:  9.76404772e-03, mean val. rec. loss:  9.00108721e-03\n",
      "Epoch: 9304 mean train loss:  9.76330471e-03, mean val. rec. loss:  9.00030193e-03\n",
      "Epoch: 9305 mean train loss:  9.76255797e-03, mean val. rec. loss:  8.99952969e-03\n",
      "Epoch: 9306 mean train loss:  9.76181961e-03, mean val. rec. loss:  8.99875065e-03\n",
      "Epoch: 9307 mean train loss:  9.76106822e-03, mean val. rec. loss:  8.99796934e-03\n",
      "Epoch: 9308 mean train loss:  9.76032800e-03, mean val. rec. loss:  8.99717726e-03\n",
      "Epoch: 9309 mean train loss:  9.75957847e-03, mean val. rec. loss:  8.99640048e-03\n",
      "Epoch: 9310 mean train loss:  9.75883732e-03, mean val. rec. loss:  8.99562541e-03\n",
      "Epoch: 9311 mean train loss:  9.75808871e-03, mean val. rec. loss:  8.99484296e-03\n",
      "Epoch: 9312 mean train loss:  9.75734849e-03, mean val. rec. loss:  8.99405315e-03\n",
      "Epoch: 9313 mean train loss:  9.75660176e-03, mean val. rec. loss:  8.99327808e-03\n",
      "Epoch: 9314 mean train loss:  9.75585409e-03, mean val. rec. loss:  8.99250924e-03\n",
      "Epoch: 9315 mean train loss:  9.75511107e-03, mean val. rec. loss:  8.99172226e-03\n",
      "Epoch: 9316 mean train loss:  9.75436899e-03, mean val. rec. loss:  8.99093698e-03\n",
      "Epoch: 9317 mean train loss:  9.75362132e-03, mean val. rec. loss:  8.99016645e-03\n",
      "Epoch: 9318 mean train loss:  9.75287365e-03, mean val. rec. loss:  8.98939364e-03\n",
      "Epoch: 9319 mean train loss:  9.75213343e-03, mean val. rec. loss:  8.98860326e-03\n",
      "Epoch: 9320 mean train loss:  9.75138669e-03, mean val. rec. loss:  8.98781855e-03\n",
      "Epoch: 9321 mean train loss:  9.75063995e-03, mean val. rec. loss:  8.98704574e-03\n",
      "Epoch: 9322 mean train loss:  9.74989601e-03, mean val. rec. loss:  8.98627577e-03\n",
      "Epoch: 9323 mean train loss:  9.74915299e-03, mean val. rec. loss:  8.98548255e-03\n",
      "Epoch: 9324 mean train loss:  9.74840905e-03, mean val. rec. loss:  8.98469728e-03\n",
      "Epoch: 9325 mean train loss:  9.74765859e-03, mean val. rec. loss:  8.98393071e-03\n",
      "Epoch: 9326 mean train loss:  9.74691743e-03, mean val. rec. loss:  8.98314656e-03\n",
      "Epoch: 9327 mean train loss:  9.74617535e-03, mean val. rec. loss:  8.98235902e-03\n",
      "Epoch: 9328 mean train loss:  9.74542675e-03, mean val. rec. loss:  8.98157430e-03\n",
      "Epoch: 9329 mean train loss:  9.74467908e-03, mean val. rec. loss:  8.98079980e-03\n",
      "Epoch: 9330 mean train loss:  9.74393607e-03, mean val. rec. loss:  8.98002643e-03\n",
      "Epoch: 9331 mean train loss:  9.74319305e-03, mean val. rec. loss:  8.97924058e-03\n",
      "Epoch: 9332 mean train loss:  9.74244818e-03, mean val. rec. loss:  8.97845587e-03\n",
      "Epoch: 9333 mean train loss:  9.74169958e-03, mean val. rec. loss:  8.97768873e-03\n",
      "Epoch: 9334 mean train loss:  9.74095284e-03, mean val. rec. loss:  8.97690346e-03\n",
      "Epoch: 9335 mean train loss:  9.74021355e-03, mean val. rec. loss:  8.97611534e-03\n",
      "Epoch: 9336 mean train loss:  9.73946867e-03, mean val. rec. loss:  8.97533800e-03\n",
      "Epoch: 9337 mean train loss:  9.73871821e-03, mean val. rec. loss:  8.97455839e-03\n",
      "Epoch: 9338 mean train loss:  9.73797333e-03, mean val. rec. loss:  8.97377822e-03\n",
      "Epoch: 9339 mean train loss:  9.73723218e-03, mean val. rec. loss:  8.97299180e-03\n",
      "Epoch: 9340 mean train loss:  9.73648917e-03, mean val. rec. loss:  8.97222013e-03\n",
      "Epoch: 9341 mean train loss:  9.73574243e-03, mean val. rec. loss:  8.97144166e-03\n",
      "Epoch: 9342 mean train loss:  9.73499476e-03, mean val. rec. loss:  8.97065411e-03\n",
      "Epoch: 9343 mean train loss:  9.73424895e-03, mean val. rec. loss:  8.96987337e-03\n",
      "Epoch: 9344 mean train loss:  9.73350408e-03, mean val. rec. loss:  8.96909489e-03\n",
      "Epoch: 9345 mean train loss:  9.73275920e-03, mean val. rec. loss:  8.96831585e-03\n",
      "Epoch: 9346 mean train loss:  9.73201339e-03, mean val. rec. loss:  8.96752944e-03\n",
      "Epoch: 9347 mean train loss:  9.73126759e-03, mean val. rec. loss:  8.96674756e-03\n",
      "Epoch: 9348 mean train loss:  9.73052550e-03, mean val. rec. loss:  8.96597816e-03\n",
      "Epoch: 9349 mean train loss:  9.72978249e-03, mean val. rec. loss:  8.96520138e-03\n",
      "Epoch: 9350 mean train loss:  9.72903575e-03, mean val. rec. loss:  8.96441837e-03\n",
      "Epoch: 9351 mean train loss:  9.72828715e-03, mean val. rec. loss:  8.96363536e-03\n",
      "Epoch: 9352 mean train loss:  9.72754507e-03, mean val. rec. loss:  8.96286539e-03\n",
      "Epoch: 9353 mean train loss:  9.72680205e-03, mean val. rec. loss:  8.96208692e-03\n",
      "Epoch: 9354 mean train loss:  9.72605624e-03, mean val. rec. loss:  8.96130334e-03\n",
      "Epoch: 9355 mean train loss:  9.72531137e-03, mean val. rec. loss:  8.96051806e-03\n",
      "Epoch: 9356 mean train loss:  9.72456556e-03, mean val. rec. loss:  8.95974185e-03\n",
      "Epoch: 9357 mean train loss:  9.72382069e-03, mean val. rec. loss:  8.95896054e-03\n",
      "Epoch: 9358 mean train loss:  9.72307581e-03, mean val. rec. loss:  8.95817923e-03\n",
      "Epoch: 9359 mean train loss:  9.72232721e-03, mean val. rec. loss:  8.95739679e-03\n",
      "Epoch: 9360 mean train loss:  9.72158140e-03, mean val. rec. loss:  8.95661718e-03\n",
      "Epoch: 9361 mean train loss:  9.72083559e-03, mean val. rec. loss:  8.95583871e-03\n",
      "Epoch: 9362 mean train loss:  9.72008886e-03, mean val. rec. loss:  8.95505569e-03\n",
      "Epoch: 9363 mean train loss:  9.71934677e-03, mean val. rec. loss:  8.95428062e-03\n",
      "Epoch: 9364 mean train loss:  9.71860283e-03, mean val. rec. loss:  8.95350101e-03\n",
      "Epoch: 9365 mean train loss:  9.71785609e-03, mean val. rec. loss:  8.95272311e-03\n",
      "Epoch: 9366 mean train loss:  9.71710749e-03, mean val. rec. loss:  8.95194236e-03\n",
      "Epoch: 9367 mean train loss:  9.71636261e-03, mean val. rec. loss:  8.95115765e-03\n",
      "Epoch: 9368 mean train loss:  9.71561774e-03, mean val. rec. loss:  8.95038258e-03\n",
      "Epoch: 9369 mean train loss:  9.71487193e-03, mean val. rec. loss:  8.94959957e-03\n",
      "Epoch: 9370 mean train loss:  9.71412612e-03, mean val. rec. loss:  8.94881485e-03\n",
      "Epoch: 9371 mean train loss:  9.71338125e-03, mean val. rec. loss:  8.94803468e-03\n",
      "Epoch: 9372 mean train loss:  9.71263823e-03, mean val. rec. loss:  8.94725224e-03\n",
      "Epoch: 9373 mean train loss:  9.71189149e-03, mean val. rec. loss:  8.94647886e-03\n",
      "Epoch: 9374 mean train loss:  9.71114289e-03, mean val. rec. loss:  8.94570322e-03\n",
      "Epoch: 9375 mean train loss:  9.71039895e-03, mean val. rec. loss:  8.94491568e-03\n",
      "Epoch: 9376 mean train loss:  9.70965314e-03, mean val. rec. loss:  8.94414060e-03\n",
      "Epoch: 9377 mean train loss:  9.70890826e-03, mean val. rec. loss:  8.94335816e-03\n",
      "Epoch: 9378 mean train loss:  9.70816339e-03, mean val. rec. loss:  8.94257968e-03\n",
      "Epoch: 9379 mean train loss:  9.70741851e-03, mean val. rec. loss:  8.94179157e-03\n",
      "Epoch: 9380 mean train loss:  9.70667177e-03, mean val. rec. loss:  8.94100516e-03\n",
      "Epoch: 9381 mean train loss:  9.70592690e-03, mean val. rec. loss:  8.94023405e-03\n",
      "Epoch: 9382 mean train loss:  9.70518016e-03, mean val. rec. loss:  8.93945728e-03\n",
      "Epoch: 9383 mean train loss:  9.70443528e-03, mean val. rec. loss:  8.93866463e-03\n",
      "Epoch: 9384 mean train loss:  9.70368761e-03, mean val. rec. loss:  8.93788956e-03\n",
      "Epoch: 9385 mean train loss:  9.70294646e-03, mean val. rec. loss:  8.93712696e-03\n",
      "Epoch: 9386 mean train loss:  9.70219879e-03, mean val. rec. loss:  8.93634111e-03\n",
      "Epoch: 9387 mean train loss:  9.70145205e-03, mean val. rec. loss:  8.93554563e-03\n",
      "Epoch: 9388 mean train loss:  9.70070718e-03, mean val. rec. loss:  8.93477282e-03\n",
      "Epoch: 9389 mean train loss:  9.69996230e-03, mean val. rec. loss:  8.93400796e-03\n",
      "Epoch: 9390 mean train loss:  9.69921836e-03, mean val. rec. loss:  8.93321757e-03\n",
      "Epoch: 9391 mean train loss:  9.69846789e-03, mean val. rec. loss:  8.93242606e-03\n",
      "Epoch: 9392 mean train loss:  9.69772767e-03, mean val. rec. loss:  8.93165665e-03\n",
      "Epoch: 9393 mean train loss:  9.69697814e-03, mean val. rec. loss:  8.93088442e-03\n",
      "Epoch: 9394 mean train loss:  9.69623047e-03, mean val. rec. loss:  8.93009630e-03\n",
      "Epoch: 9395 mean train loss:  9.69548746e-03, mean val. rec. loss:  8.92930876e-03\n",
      "Epoch: 9396 mean train loss:  9.69474258e-03, mean val. rec. loss:  8.92853822e-03\n",
      "Epoch: 9397 mean train loss:  9.69399398e-03, mean val. rec. loss:  8.92776315e-03\n",
      "Epoch: 9398 mean train loss:  9.69325283e-03, mean val. rec. loss:  8.92697390e-03\n",
      "Epoch: 9399 mean train loss:  9.69250330e-03, mean val. rec. loss:  8.92618975e-03\n",
      "Epoch: 9400 mean train loss:  9.69176121e-03, mean val. rec. loss:  8.92541411e-03\n",
      "Epoch: 9401 mean train loss:  9.69101075e-03, mean val. rec. loss:  8.92463847e-03\n",
      "Epoch: 9402 mean train loss:  9.69026960e-03, mean val. rec. loss:  8.92385490e-03\n",
      "Epoch: 9403 mean train loss:  9.68952193e-03, mean val. rec. loss:  8.92307926e-03\n",
      "Epoch: 9404 mean train loss:  9.68877798e-03, mean val. rec. loss:  8.92229681e-03\n",
      "Epoch: 9405 mean train loss:  9.68802566e-03, mean val. rec. loss:  8.92151664e-03\n",
      "Epoch: 9406 mean train loss:  9.68728637e-03, mean val. rec. loss:  8.92074043e-03\n",
      "Epoch: 9407 mean train loss:  9.68653591e-03, mean val. rec. loss:  8.91996195e-03\n",
      "Epoch: 9408 mean train loss:  9.68579289e-03, mean val. rec. loss:  8.91918235e-03\n",
      "Epoch: 9409 mean train loss:  9.68504615e-03, mean val. rec. loss:  8.91839820e-03\n",
      "Epoch: 9410 mean train loss:  9.68430035e-03, mean val. rec. loss:  8.91761235e-03\n",
      "Epoch: 9411 mean train loss:  9.68355454e-03, mean val. rec. loss:  8.91683615e-03\n",
      "Epoch: 9412 mean train loss:  9.68280687e-03, mean val. rec. loss:  8.91605881e-03\n",
      "Epoch: 9413 mean train loss:  9.68206479e-03, mean val. rec. loss:  8.91527410e-03\n",
      "Epoch: 9414 mean train loss:  9.68131805e-03, mean val. rec. loss:  8.91449619e-03\n",
      "Epoch: 9415 mean train loss:  9.68056759e-03, mean val. rec. loss:  8.91372282e-03\n",
      "Epoch: 9416 mean train loss:  9.67982457e-03, mean val. rec. loss:  8.91293697e-03\n",
      "Epoch: 9417 mean train loss:  9.67908342e-03, mean val. rec. loss:  8.91215566e-03\n",
      "Epoch: 9418 mean train loss:  9.67833110e-03, mean val. rec. loss:  8.91137322e-03\n",
      "Epoch: 9419 mean train loss:  9.67758436e-03, mean val. rec. loss:  8.91059304e-03\n",
      "Epoch: 9420 mean train loss:  9.67684227e-03, mean val. rec. loss:  8.90981513e-03\n",
      "Epoch: 9421 mean train loss:  9.67609647e-03, mean val. rec. loss:  8.90902872e-03\n",
      "Epoch: 9422 mean train loss:  9.67534600e-03, mean val. rec. loss:  8.90824457e-03\n",
      "Epoch: 9423 mean train loss:  9.67460020e-03, mean val. rec. loss:  8.90747007e-03\n",
      "Epoch: 9424 mean train loss:  9.67385625e-03, mean val. rec. loss:  8.90669443e-03\n",
      "Epoch: 9425 mean train loss:  9.67310858e-03, mean val. rec. loss:  8.90590405e-03\n",
      "Epoch: 9426 mean train loss:  9.67235998e-03, mean val. rec. loss:  8.90512614e-03\n",
      "Epoch: 9427 mean train loss:  9.67161604e-03, mean val. rec. loss:  8.90435390e-03\n",
      "Epoch: 9428 mean train loss:  9.67087116e-03, mean val. rec. loss:  8.90357939e-03\n",
      "Epoch: 9429 mean train loss:  9.67012256e-03, mean val. rec. loss:  8.90278901e-03\n",
      "Epoch: 9430 mean train loss:  9.66937582e-03, mean val. rec. loss:  8.90201110e-03\n",
      "Epoch: 9431 mean train loss:  9.66863094e-03, mean val. rec. loss:  8.90124113e-03\n",
      "Epoch: 9432 mean train loss:  9.66788793e-03, mean val. rec. loss:  8.90046096e-03\n",
      "Epoch: 9433 mean train loss:  9.66713933e-03, mean val. rec. loss:  8.89966774e-03\n",
      "Epoch: 9434 mean train loss:  9.66639259e-03, mean val. rec. loss:  8.89888643e-03\n",
      "Epoch: 9435 mean train loss:  9.66564399e-03, mean val. rec. loss:  8.89811930e-03\n",
      "Epoch: 9436 mean train loss:  9.66490005e-03, mean val. rec. loss:  8.89733799e-03\n",
      "Epoch: 9437 mean train loss:  9.66415424e-03, mean val. rec. loss:  8.89655044e-03\n",
      "Epoch: 9438 mean train loss:  9.66340750e-03, mean val. rec. loss:  8.89576970e-03\n",
      "Epoch: 9439 mean train loss:  9.66266076e-03, mean val. rec. loss:  8.89499803e-03\n",
      "Epoch: 9440 mean train loss:  9.66191495e-03, mean val. rec. loss:  8.89421331e-03\n",
      "Epoch: 9441 mean train loss:  9.66116729e-03, mean val. rec. loss:  8.89342747e-03\n",
      "Epoch: 9442 mean train loss:  9.66042241e-03, mean val. rec. loss:  8.89265126e-03\n",
      "Epoch: 9443 mean train loss:  9.65967567e-03, mean val. rec. loss:  8.89187449e-03\n",
      "Epoch: 9444 mean train loss:  9.65892986e-03, mean val. rec. loss:  8.89109204e-03\n",
      "Epoch: 9445 mean train loss:  9.65818499e-03, mean val. rec. loss:  8.89030733e-03\n",
      "Epoch: 9446 mean train loss:  9.65744104e-03, mean val. rec. loss:  8.88952716e-03\n",
      "Epoch: 9447 mean train loss:  9.65669244e-03, mean val. rec. loss:  8.88875775e-03\n",
      "Epoch: 9448 mean train loss:  9.65594384e-03, mean val. rec. loss:  8.88797247e-03\n",
      "Epoch: 9449 mean train loss:  9.65519896e-03, mean val. rec. loss:  8.88719060e-03\n",
      "Epoch: 9450 mean train loss:  9.65445502e-03, mean val. rec. loss:  8.88641893e-03\n",
      "Epoch: 9451 mean train loss:  9.65370549e-03, mean val. rec. loss:  8.88563705e-03\n",
      "Epoch: 9452 mean train loss:  9.65295875e-03, mean val. rec. loss:  8.88485914e-03\n",
      "Epoch: 9453 mean train loss:  9.65221294e-03, mean val. rec. loss:  8.88407046e-03\n",
      "Epoch: 9454 mean train loss:  9.65146713e-03, mean val. rec. loss:  8.88328802e-03\n",
      "Epoch: 9455 mean train loss:  9.65071853e-03, mean val. rec. loss:  8.88251465e-03\n",
      "Epoch: 9456 mean train loss:  9.64997180e-03, mean val. rec. loss:  8.88173334e-03\n",
      "Epoch: 9457 mean train loss:  9.64922971e-03, mean val. rec. loss:  8.88094579e-03\n",
      "Epoch: 9458 mean train loss:  9.64848297e-03, mean val. rec. loss:  8.88016391e-03\n",
      "Epoch: 9459 mean train loss:  9.64772972e-03, mean val. rec. loss:  8.87939508e-03\n",
      "Epoch: 9460 mean train loss:  9.64698950e-03, mean val. rec. loss:  8.87861263e-03\n",
      "Epoch: 9461 mean train loss:  9.64624090e-03, mean val. rec. loss:  8.87783132e-03\n",
      "Epoch: 9462 mean train loss:  9.64549881e-03, mean val. rec. loss:  8.87706192e-03\n",
      "Epoch: 9463 mean train loss:  9.64475021e-03, mean val. rec. loss:  8.87627891e-03\n",
      "Epoch: 9464 mean train loss:  9.64400068e-03, mean val. rec. loss:  8.87550440e-03\n",
      "Epoch: 9465 mean train loss:  9.64325674e-03, mean val. rec. loss:  8.87471062e-03\n",
      "Epoch: 9466 mean train loss:  9.64250814e-03, mean val. rec. loss:  8.87393838e-03\n",
      "Epoch: 9467 mean train loss:  9.64175860e-03, mean val. rec. loss:  8.87316274e-03\n",
      "Epoch: 9468 mean train loss:  9.64101466e-03, mean val. rec. loss:  8.87237803e-03\n",
      "Epoch: 9469 mean train loss:  9.64026978e-03, mean val. rec. loss:  8.87159332e-03\n",
      "Epoch: 9470 mean train loss:  9.63952118e-03, mean val. rec. loss:  8.87081768e-03\n",
      "Epoch: 9471 mean train loss:  9.63877258e-03, mean val. rec. loss:  8.87004090e-03\n",
      "Epoch: 9472 mean train loss:  9.63802584e-03, mean val. rec. loss:  8.86925279e-03\n",
      "Epoch: 9473 mean train loss:  9.63728097e-03, mean val. rec. loss:  8.86846524e-03\n",
      "Epoch: 9474 mean train loss:  9.63653330e-03, mean val. rec. loss:  8.86768790e-03\n",
      "Epoch: 9475 mean train loss:  9.63578749e-03, mean val. rec. loss:  8.86691226e-03\n",
      "Epoch: 9476 mean train loss:  9.63503703e-03, mean val. rec. loss:  8.86612868e-03\n",
      "Epoch: 9477 mean train loss:  9.63429495e-03, mean val. rec. loss:  8.86535191e-03\n",
      "Epoch: 9478 mean train loss:  9.63354634e-03, mean val. rec. loss:  8.86457287e-03\n",
      "Epoch: 9479 mean train loss:  9.63280147e-03, mean val. rec. loss:  8.86379383e-03\n",
      "Epoch: 9480 mean train loss:  9.63205194e-03, mean val. rec. loss:  8.86301819e-03\n",
      "Epoch: 9481 mean train loss:  9.63130147e-03, mean val. rec. loss:  8.86224255e-03\n",
      "Epoch: 9482 mean train loss:  9.63056032e-03, mean val. rec. loss:  8.86146010e-03\n",
      "Epoch: 9483 mean train loss:  9.62981172e-03, mean val. rec. loss:  8.86067539e-03\n",
      "Epoch: 9484 mean train loss:  9.62906312e-03, mean val. rec. loss:  8.85989748e-03\n",
      "Epoch: 9485 mean train loss:  9.62831918e-03, mean val. rec. loss:  8.85910937e-03\n",
      "Epoch: 9486 mean train loss:  9.62756964e-03, mean val. rec. loss:  8.85832919e-03\n",
      "Epoch: 9487 mean train loss:  9.62682104e-03, mean val. rec. loss:  8.85755639e-03\n",
      "Epoch: 9488 mean train loss:  9.62607431e-03, mean val. rec. loss:  8.85677281e-03\n",
      "Epoch: 9489 mean train loss:  9.62533408e-03, mean val. rec. loss:  8.85598753e-03\n",
      "Epoch: 9490 mean train loss:  9.62457897e-03, mean val. rec. loss:  8.85520962e-03\n",
      "Epoch: 9491 mean train loss:  9.62383502e-03, mean val. rec. loss:  8.85442775e-03\n",
      "Epoch: 9492 mean train loss:  9.62308549e-03, mean val. rec. loss:  8.85365494e-03\n",
      "Epoch: 9493 mean train loss:  9.62234434e-03, mean val. rec. loss:  8.85286683e-03\n",
      "Epoch: 9494 mean train loss:  9.62159015e-03, mean val. rec. loss:  8.85208892e-03\n",
      "Epoch: 9495 mean train loss:  9.62084714e-03, mean val. rec. loss:  8.85131838e-03\n",
      "Epoch: 9496 mean train loss:  9.62009667e-03, mean val. rec. loss:  8.85052913e-03\n",
      "Epoch: 9497 mean train loss:  9.61935366e-03, mean val. rec. loss:  8.84975576e-03\n",
      "Epoch: 9498 mean train loss:  9.61860227e-03, mean val. rec. loss:  8.84897105e-03\n",
      "Epoch: 9499 mean train loss:  9.61785925e-03, mean val. rec. loss:  8.84820618e-03\n",
      "Epoch: 9500 mean train loss:  9.61710786e-03, mean val. rec. loss:  8.84741013e-03\n",
      "Epoch: 9501 mean train loss:  9.61636298e-03, mean val. rec. loss:  8.84663619e-03\n",
      "Epoch: 9502 mean train loss:  9.61561811e-03, mean val. rec. loss:  8.84585318e-03\n",
      "Epoch: 9503 mean train loss:  9.61486857e-03, mean val. rec. loss:  8.84508094e-03\n",
      "Epoch: 9504 mean train loss:  9.61411904e-03, mean val. rec. loss:  8.84428546e-03\n",
      "Epoch: 9505 mean train loss:  9.61337323e-03, mean val. rec. loss:  8.84351492e-03\n",
      "Epoch: 9506 mean train loss:  9.61262557e-03, mean val. rec. loss:  8.84273134e-03\n",
      "Epoch: 9507 mean train loss:  9.61187883e-03, mean val. rec. loss:  8.84195797e-03\n",
      "Epoch: 9508 mean train loss:  9.61113023e-03, mean val. rec. loss:  8.84116872e-03\n",
      "Epoch: 9509 mean train loss:  9.61038163e-03, mean val. rec. loss:  8.84039762e-03\n",
      "Epoch: 9510 mean train loss:  9.60963675e-03, mean val. rec. loss:  8.83961177e-03\n",
      "Epoch: 9511 mean train loss:  9.60889094e-03, mean val. rec. loss:  8.83883613e-03\n",
      "Epoch: 9512 mean train loss:  9.60814234e-03, mean val. rec. loss:  8.83804575e-03\n",
      "Epoch: 9513 mean train loss:  9.60739188e-03, mean val. rec. loss:  8.83727522e-03\n",
      "Epoch: 9514 mean train loss:  9.60664700e-03, mean val. rec. loss:  8.83648994e-03\n",
      "Epoch: 9515 mean train loss:  9.60590120e-03, mean val. rec. loss:  8.83571543e-03\n",
      "Epoch: 9516 mean train loss:  9.60515166e-03, mean val. rec. loss:  8.83492675e-03\n",
      "Epoch: 9517 mean train loss:  9.60440027e-03, mean val. rec. loss:  8.83415168e-03\n",
      "Epoch: 9518 mean train loss:  9.60365632e-03, mean val. rec. loss:  8.83337377e-03\n",
      "Epoch: 9519 mean train loss:  9.60291331e-03, mean val. rec. loss:  8.83258679e-03\n",
      "Epoch: 9520 mean train loss:  9.60216378e-03, mean val. rec. loss:  8.83181115e-03\n",
      "Epoch: 9521 mean train loss:  9.60141332e-03, mean val. rec. loss:  8.83102871e-03\n",
      "Epoch: 9522 mean train loss:  9.60066844e-03, mean val. rec. loss:  8.83025080e-03\n",
      "Epoch: 9523 mean train loss:  9.59992077e-03, mean val. rec. loss:  8.82946949e-03\n",
      "Epoch: 9524 mean train loss:  9.59917124e-03, mean val. rec. loss:  8.82868591e-03\n",
      "Epoch: 9525 mean train loss:  9.59841891e-03, mean val. rec. loss:  8.82790800e-03\n",
      "Epoch: 9526 mean train loss:  9.59767311e-03, mean val. rec. loss:  8.82712556e-03\n",
      "Epoch: 9527 mean train loss:  9.59692730e-03, mean val. rec. loss:  8.82634595e-03\n",
      "Epoch: 9528 mean train loss:  9.59617777e-03, mean val. rec. loss:  8.82557314e-03\n",
      "Epoch: 9529 mean train loss:  9.59543382e-03, mean val. rec. loss:  8.82479750e-03\n",
      "Epoch: 9530 mean train loss:  9.59468708e-03, mean val. rec. loss:  8.82400996e-03\n",
      "Epoch: 9531 mean train loss:  9.59393476e-03, mean val. rec. loss:  8.82323318e-03\n",
      "Epoch: 9532 mean train loss:  9.59318988e-03, mean val. rec. loss:  8.82245641e-03\n",
      "Epoch: 9533 mean train loss:  9.59243942e-03, mean val. rec. loss:  8.82167680e-03\n",
      "Epoch: 9534 mean train loss:  9.59169548e-03, mean val. rec. loss:  8.82088699e-03\n",
      "Epoch: 9535 mean train loss:  9.59094408e-03, mean val. rec. loss:  8.82011361e-03\n",
      "Epoch: 9536 mean train loss:  9.59019920e-03, mean val. rec. loss:  8.81932720e-03\n",
      "Epoch: 9537 mean train loss:  9.58944781e-03, mean val. rec. loss:  8.81854929e-03\n",
      "Epoch: 9538 mean train loss:  9.58870387e-03, mean val. rec. loss:  8.81776231e-03\n",
      "Epoch: 9539 mean train loss:  9.58795154e-03, mean val. rec. loss:  8.81699121e-03\n",
      "Epoch: 9540 mean train loss:  9.58720853e-03, mean val. rec. loss:  8.81620763e-03\n",
      "Epoch: 9541 mean train loss:  9.58645620e-03, mean val. rec. loss:  8.81543199e-03\n",
      "Epoch: 9542 mean train loss:  9.58571226e-03, mean val. rec. loss:  8.81465408e-03\n",
      "Epoch: 9543 mean train loss:  9.58496086e-03, mean val. rec. loss:  8.81387901e-03\n",
      "Epoch: 9544 mean train loss:  9.58421599e-03, mean val. rec. loss:  8.81309260e-03\n",
      "Epoch: 9545 mean train loss:  9.58346366e-03, mean val. rec. loss:  8.81231412e-03\n",
      "Epoch: 9546 mean train loss:  9.58272065e-03, mean val. rec. loss:  8.81152998e-03\n",
      "Epoch: 9547 mean train loss:  9.58197019e-03, mean val. rec. loss:  8.81075094e-03\n",
      "Epoch: 9548 mean train loss:  9.58122438e-03, mean val. rec. loss:  8.80996112e-03\n",
      "Epoch: 9549 mean train loss:  9.58046926e-03, mean val. rec. loss:  8.80919172e-03\n",
      "Epoch: 9550 mean train loss:  9.57972531e-03, mean val. rec. loss:  8.80840984e-03\n",
      "Epoch: 9551 mean train loss:  9.57898137e-03, mean val. rec. loss:  8.80762910e-03\n",
      "Epoch: 9552 mean train loss:  9.57822904e-03, mean val. rec. loss:  8.80684155e-03\n",
      "Epoch: 9553 mean train loss:  9.57747672e-03, mean val. rec. loss:  8.80606421e-03\n",
      "Epoch: 9554 mean train loss:  9.57673277e-03, mean val. rec. loss:  8.80529084e-03\n",
      "Epoch: 9555 mean train loss:  9.57598883e-03, mean val. rec. loss:  8.80450669e-03\n",
      "Epoch: 9556 mean train loss:  9.57523650e-03, mean val. rec. loss:  8.80372482e-03\n",
      "Epoch: 9557 mean train loss:  9.57448604e-03, mean val. rec. loss:  8.80295598e-03\n",
      "Epoch: 9558 mean train loss:  9.57374023e-03, mean val. rec. loss:  8.80217410e-03\n",
      "Epoch: 9559 mean train loss:  9.57299350e-03, mean val. rec. loss:  8.80138316e-03\n",
      "Epoch: 9560 mean train loss:  9.57224210e-03, mean val. rec. loss:  8.80060808e-03\n",
      "Epoch: 9561 mean train loss:  9.57149536e-03, mean val. rec. loss:  8.79983414e-03\n",
      "Epoch: 9562 mean train loss:  9.57074863e-03, mean val. rec. loss:  8.79905397e-03\n",
      "Epoch: 9563 mean train loss:  9.56999816e-03, mean val. rec. loss:  8.79826472e-03\n",
      "Epoch: 9564 mean train loss:  9.56924863e-03, mean val. rec. loss:  8.79748625e-03\n",
      "Epoch: 9565 mean train loss:  9.56850189e-03, mean val. rec. loss:  8.79671571e-03\n",
      "Epoch: 9566 mean train loss:  9.56775608e-03, mean val. rec. loss:  8.79593667e-03\n",
      "Epoch: 9567 mean train loss:  9.56700469e-03, mean val. rec. loss:  8.79514969e-03\n",
      "Epoch: 9568 mean train loss:  9.56625702e-03, mean val. rec. loss:  8.79437178e-03\n",
      "Epoch: 9569 mean train loss:  9.56551028e-03, mean val. rec. loss:  8.79359841e-03\n",
      "Epoch: 9570 mean train loss:  9.56475982e-03, mean val. rec. loss:  8.79281313e-03\n",
      "Epoch: 9571 mean train loss:  9.56401122e-03, mean val. rec. loss:  8.79202898e-03\n",
      "Epoch: 9572 mean train loss:  9.56326262e-03, mean val. rec. loss:  8.79125221e-03\n",
      "Epoch: 9573 mean train loss:  9.56251402e-03, mean val. rec. loss:  8.79047033e-03\n",
      "Epoch: 9574 mean train loss:  9.56176542e-03, mean val. rec. loss:  8.78968619e-03\n",
      "Epoch: 9575 mean train loss:  9.56101682e-03, mean val. rec. loss:  8.78890374e-03\n",
      "Epoch: 9576 mean train loss:  9.56026822e-03, mean val. rec. loss:  8.78812810e-03\n",
      "Epoch: 9577 mean train loss:  9.55951775e-03, mean val. rec. loss:  8.78734849e-03\n",
      "Epoch: 9578 mean train loss:  9.55877288e-03, mean val. rec. loss:  8.78656208e-03\n",
      "Epoch: 9579 mean train loss:  9.55801869e-03, mean val. rec. loss:  8.78578644e-03\n",
      "Epoch: 9580 mean train loss:  9.55727381e-03, mean val. rec. loss:  8.78500740e-03\n",
      "Epoch: 9581 mean train loss:  9.55652521e-03, mean val. rec. loss:  8.78422609e-03\n",
      "Epoch: 9582 mean train loss:  9.55577848e-03, mean val. rec. loss:  8.78344535e-03\n",
      "Epoch: 9583 mean train loss:  9.55502708e-03, mean val. rec. loss:  8.78266574e-03\n",
      "Epoch: 9584 mean train loss:  9.55427569e-03, mean val. rec. loss:  8.78189123e-03\n",
      "Epoch: 9585 mean train loss:  9.55352988e-03, mean val. rec. loss:  8.78110312e-03\n",
      "Epoch: 9586 mean train loss:  9.55278407e-03, mean val. rec. loss:  8.78031727e-03\n",
      "Epoch: 9587 mean train loss:  9.55203175e-03, mean val. rec. loss:  8.77955014e-03\n",
      "Epoch: 9588 mean train loss:  9.55128315e-03, mean val. rec. loss:  8.77877790e-03\n",
      "Epoch: 9589 mean train loss:  9.55053455e-03, mean val. rec. loss:  8.77799262e-03\n",
      "Epoch: 9590 mean train loss:  9.54978595e-03, mean val. rec. loss:  8.77720791e-03\n",
      "Epoch: 9591 mean train loss:  9.54903735e-03, mean val. rec. loss:  8.77643397e-03\n",
      "Epoch: 9592 mean train loss:  9.54828502e-03, mean val. rec. loss:  8.77565549e-03\n",
      "Epoch: 9593 mean train loss:  9.54753828e-03, mean val. rec. loss:  8.77486681e-03\n",
      "Epoch: 9594 mean train loss:  9.54679341e-03, mean val. rec. loss:  8.77408494e-03\n",
      "Epoch: 9595 mean train loss:  9.54603922e-03, mean val. rec. loss:  8.77331383e-03\n",
      "Epoch: 9596 mean train loss:  9.54529434e-03, mean val. rec. loss:  8.77252855e-03\n",
      "Epoch: 9597 mean train loss:  9.54454202e-03, mean val. rec. loss:  8.77174895e-03\n",
      "Epoch: 9598 mean train loss:  9.54379342e-03, mean val. rec. loss:  8.77097331e-03\n",
      "Epoch: 9599 mean train loss:  9.54304482e-03, mean val. rec. loss:  8.77019653e-03\n",
      "Epoch: 9600 mean train loss:  9.54229808e-03, mean val. rec. loss:  8.76941069e-03\n",
      "Epoch: 9601 mean train loss:  9.54154203e-03, mean val. rec. loss:  8.76863278e-03\n",
      "Epoch: 9602 mean train loss:  9.54079901e-03, mean val. rec. loss:  8.76784977e-03\n",
      "Epoch: 9603 mean train loss:  9.54004762e-03, mean val. rec. loss:  8.76707129e-03\n",
      "Epoch: 9604 mean train loss:  9.53930088e-03, mean val. rec. loss:  8.76627978e-03\n",
      "Epoch: 9605 mean train loss:  9.53854576e-03, mean val. rec. loss:  8.76550073e-03\n",
      "Epoch: 9606 mean train loss:  9.53780275e-03, mean val. rec. loss:  8.76471999e-03\n",
      "Epoch: 9607 mean train loss:  9.53705229e-03, mean val. rec. loss:  8.76394719e-03\n",
      "Epoch: 9608 mean train loss:  9.53630462e-03, mean val. rec. loss:  8.76315794e-03\n",
      "Epoch: 9609 mean train loss:  9.53554857e-03, mean val. rec. loss:  8.76238570e-03\n",
      "Epoch: 9610 mean train loss:  9.53480276e-03, mean val. rec. loss:  8.76160893e-03\n",
      "Epoch: 9611 mean train loss:  9.53405789e-03, mean val. rec. loss:  8.76083102e-03\n",
      "Epoch: 9612 mean train loss:  9.53330556e-03, mean val. rec. loss:  8.76003950e-03\n",
      "Epoch: 9613 mean train loss:  9.53255137e-03, mean val. rec. loss:  8.75926273e-03\n",
      "Epoch: 9614 mean train loss:  9.53180650e-03, mean val. rec. loss:  8.75848822e-03\n",
      "Epoch: 9615 mean train loss:  9.53106162e-03, mean val. rec. loss:  8.75770918e-03\n",
      "Epoch: 9616 mean train loss:  9.53030836e-03, mean val. rec. loss:  8.75691767e-03\n",
      "Epoch: 9617 mean train loss:  9.52955418e-03, mean val. rec. loss:  8.75614713e-03\n",
      "Epoch: 9618 mean train loss:  9.52880837e-03, mean val. rec. loss:  8.75536639e-03\n",
      "Epoch: 9619 mean train loss:  9.52806349e-03, mean val. rec. loss:  8.75458621e-03\n",
      "Epoch: 9620 mean train loss:  9.52731489e-03, mean val. rec. loss:  8.75380830e-03\n",
      "Epoch: 9621 mean train loss:  9.52656629e-03, mean val. rec. loss:  8.75302813e-03\n",
      "Epoch: 9622 mean train loss:  9.52581583e-03, mean val. rec. loss:  8.75224852e-03\n",
      "Epoch: 9623 mean train loss:  9.52506630e-03, mean val. rec. loss:  8.75146891e-03\n",
      "Epoch: 9624 mean train loss:  9.52431677e-03, mean val. rec. loss:  8.75069213e-03\n",
      "Epoch: 9625 mean train loss:  9.52356537e-03, mean val. rec. loss:  8.74991082e-03\n",
      "Epoch: 9626 mean train loss:  9.52281119e-03, mean val. rec. loss:  8.74912498e-03\n",
      "Epoch: 9627 mean train loss:  9.52205979e-03, mean val. rec. loss:  8.74834310e-03\n",
      "Epoch: 9628 mean train loss:  9.52131119e-03, mean val. rec. loss:  8.74756689e-03\n",
      "Epoch: 9629 mean train loss:  9.52056166e-03, mean val. rec. loss:  8.74678445e-03\n",
      "Epoch: 9630 mean train loss:  9.51981213e-03, mean val. rec. loss:  8.74600144e-03\n",
      "Epoch: 9631 mean train loss:  9.51906167e-03, mean val. rec. loss:  8.74521729e-03\n",
      "Epoch: 9632 mean train loss:  9.51831306e-03, mean val. rec. loss:  8.74444279e-03\n",
      "Epoch: 9633 mean train loss:  9.51756260e-03, mean val. rec. loss:  8.74366261e-03\n",
      "Epoch: 9634 mean train loss:  9.51681307e-03, mean val. rec. loss:  8.74288017e-03\n",
      "Epoch: 9635 mean train loss:  9.51606354e-03, mean val. rec. loss:  8.74210226e-03\n",
      "Epoch: 9636 mean train loss:  9.51531308e-03, mean val. rec. loss:  8.74132719e-03\n",
      "Epoch: 9637 mean train loss:  9.51456634e-03, mean val. rec. loss:  8.74054871e-03\n",
      "Epoch: 9638 mean train loss:  9.51381401e-03, mean val. rec. loss:  8.73975720e-03\n",
      "Epoch: 9639 mean train loss:  9.51306169e-03, mean val. rec. loss:  8.73898326e-03\n",
      "Epoch: 9640 mean train loss:  9.51231402e-03, mean val. rec. loss:  8.73820478e-03\n",
      "Epoch: 9641 mean train loss:  9.51156635e-03, mean val. rec. loss:  8.73741667e-03\n",
      "Epoch: 9642 mean train loss:  9.51081402e-03, mean val. rec. loss:  8.73663763e-03\n",
      "Epoch: 9643 mean train loss:  9.51006263e-03, mean val. rec. loss:  8.73586539e-03\n",
      "Epoch: 9644 mean train loss:  9.50931496e-03, mean val. rec. loss:  8.73508181e-03\n",
      "Epoch: 9645 mean train loss:  9.50856822e-03, mean val. rec. loss:  8.73430164e-03\n",
      "Epoch: 9646 mean train loss:  9.50781683e-03, mean val. rec. loss:  8.73353110e-03\n",
      "Epoch: 9647 mean train loss:  9.50706916e-03, mean val. rec. loss:  8.73274185e-03\n",
      "Epoch: 9648 mean train loss:  9.50631218e-03, mean val. rec. loss:  8.73196508e-03\n",
      "Epoch: 9649 mean train loss:  9.50556916e-03, mean val. rec. loss:  8.73119397e-03\n",
      "Epoch: 9650 mean train loss:  9.50481591e-03, mean val. rec. loss:  8.73040416e-03\n",
      "Epoch: 9651 mean train loss:  9.50406731e-03, mean val. rec. loss:  8.72962398e-03\n",
      "Epoch: 9652 mean train loss:  9.50331871e-03, mean val. rec. loss:  8.72884381e-03\n",
      "Epoch: 9653 mean train loss:  9.50257104e-03, mean val. rec. loss:  8.72806250e-03\n",
      "Epoch: 9654 mean train loss:  9.50181313e-03, mean val. rec. loss:  8.72728119e-03\n",
      "Epoch: 9655 mean train loss:  9.50106546e-03, mean val. rec. loss:  8.72650215e-03\n",
      "Epoch: 9656 mean train loss:  9.50031313e-03, mean val. rec. loss:  8.72572197e-03\n",
      "Epoch: 9657 mean train loss:  9.49957012e-03, mean val. rec. loss:  8.72494123e-03\n",
      "Epoch: 9658 mean train loss:  9.49881221e-03, mean val. rec. loss:  8.72416162e-03\n",
      "Epoch: 9659 mean train loss:  9.49806547e-03, mean val. rec. loss:  8.72337691e-03\n",
      "Epoch: 9660 mean train loss:  9.49731314e-03, mean val. rec. loss:  8.72260240e-03\n",
      "Epoch: 9661 mean train loss:  9.49656827e-03, mean val. rec. loss:  8.72182733e-03\n",
      "Epoch: 9662 mean train loss:  9.49581408e-03, mean val. rec. loss:  8.72104545e-03\n",
      "Epoch: 9663 mean train loss:  9.49505896e-03, mean val. rec. loss:  8.72025734e-03\n",
      "Epoch: 9664 mean train loss:  9.49431315e-03, mean val. rec. loss:  8.71948623e-03\n",
      "Epoch: 9665 mean train loss:  9.49356735e-03, mean val. rec. loss:  8.71871399e-03\n",
      "Epoch: 9666 mean train loss:  9.49281223e-03, mean val. rec. loss:  8.71792361e-03\n",
      "Epoch: 9667 mean train loss:  9.49205804e-03, mean val. rec. loss:  8.71714911e-03\n",
      "Epoch: 9668 mean train loss:  9.49131130e-03, mean val. rec. loss:  8.71637006e-03\n",
      "Epoch: 9669 mean train loss:  9.49056456e-03, mean val. rec. loss:  8.71559102e-03\n",
      "Epoch: 9670 mean train loss:  9.48981038e-03, mean val. rec. loss:  8.71480177e-03\n",
      "Epoch: 9671 mean train loss:  9.48905805e-03, mean val. rec. loss:  8.71402557e-03\n",
      "Epoch: 9672 mean train loss:  9.48831131e-03, mean val. rec. loss:  8.71325163e-03\n",
      "Epoch: 9673 mean train loss:  9.48756085e-03, mean val. rec. loss:  8.71246578e-03\n",
      "Epoch: 9674 mean train loss:  9.48681039e-03, mean val. rec. loss:  8.71168674e-03\n",
      "Epoch: 9675 mean train loss:  9.48605992e-03, mean val. rec. loss:  8.71090997e-03\n",
      "Epoch: 9676 mean train loss:  9.48530946e-03, mean val. rec. loss:  8.71012922e-03\n",
      "Epoch: 9677 mean train loss:  9.48455900e-03, mean val. rec. loss:  8.70934338e-03\n",
      "Epoch: 9678 mean train loss:  9.48380761e-03, mean val. rec. loss:  8.70856377e-03\n",
      "Epoch: 9679 mean train loss:  9.48305621e-03, mean val. rec. loss:  8.70778473e-03\n",
      "Epoch: 9680 mean train loss:  9.48230482e-03, mean val. rec. loss:  8.70699718e-03\n",
      "Epoch: 9681 mean train loss:  9.48155156e-03, mean val. rec. loss:  8.70621417e-03\n",
      "Epoch: 9682 mean train loss:  9.48080110e-03, mean val. rec. loss:  8.70544307e-03\n",
      "Epoch: 9683 mean train loss:  9.48005250e-03, mean val. rec. loss:  8.70466913e-03\n",
      "Epoch: 9684 mean train loss:  9.47929831e-03, mean val. rec. loss:  8.70388838e-03\n",
      "Epoch: 9685 mean train loss:  9.47854785e-03, mean val. rec. loss:  8.70310424e-03\n",
      "Epoch: 9686 mean train loss:  9.47779832e-03, mean val. rec. loss:  8.70232350e-03\n",
      "Epoch: 9687 mean train loss:  9.47704692e-03, mean val. rec. loss:  8.70155012e-03\n",
      "Epoch: 9688 mean train loss:  9.47629460e-03, mean val. rec. loss:  8.70076144e-03\n",
      "Epoch: 9689 mean train loss:  9.47554414e-03, mean val. rec. loss:  8.69998410e-03\n",
      "Epoch: 9690 mean train loss:  9.47479553e-03, mean val. rec. loss:  8.69920846e-03\n",
      "Epoch: 9691 mean train loss:  9.47404693e-03, mean val. rec. loss:  8.69842715e-03\n",
      "Epoch: 9692 mean train loss:  9.47329461e-03, mean val. rec. loss:  8.69764017e-03\n",
      "Epoch: 9693 mean train loss:  9.47254415e-03, mean val. rec. loss:  8.69686567e-03\n",
      "Epoch: 9694 mean train loss:  9.47179089e-03, mean val. rec. loss:  8.69609400e-03\n",
      "Epoch: 9695 mean train loss:  9.47104415e-03, mean val. rec. loss:  8.69530588e-03\n",
      "Epoch: 9696 mean train loss:  9.47029276e-03, mean val. rec. loss:  8.69452174e-03\n",
      "Epoch: 9697 mean train loss:  9.46953578e-03, mean val. rec. loss:  8.69375063e-03\n",
      "Epoch: 9698 mean train loss:  9.46878997e-03, mean val. rec. loss:  8.69296479e-03\n",
      "Epoch: 9699 mean train loss:  9.46803765e-03, mean val. rec. loss:  8.69218461e-03\n",
      "Epoch: 9700 mean train loss:  9.46728532e-03, mean val. rec. loss:  8.69140954e-03\n",
      "Epoch: 9701 mean train loss:  9.46653486e-03, mean val. rec. loss:  8.69062936e-03\n",
      "Epoch: 9702 mean train loss:  9.46578439e-03, mean val. rec. loss:  8.68984692e-03\n",
      "Epoch: 9703 mean train loss:  9.46503300e-03, mean val. rec. loss:  8.68906504e-03\n",
      "Epoch: 9704 mean train loss:  9.46428161e-03, mean val. rec. loss:  8.68829621e-03\n",
      "Epoch: 9705 mean train loss:  9.46353021e-03, mean val. rec. loss:  8.68751206e-03\n",
      "Epoch: 9706 mean train loss:  9.46277882e-03, mean val. rec. loss:  8.68672338e-03\n",
      "Epoch: 9707 mean train loss:  9.46202556e-03, mean val. rec. loss:  8.68595001e-03\n",
      "Epoch: 9708 mean train loss:  9.46127603e-03, mean val. rec. loss:  8.68517607e-03\n",
      "Epoch: 9709 mean train loss:  9.46052743e-03, mean val. rec. loss:  8.68438852e-03\n",
      "Epoch: 9710 mean train loss:  9.45977417e-03, mean val. rec. loss:  8.68359927e-03\n",
      "Epoch: 9711 mean train loss:  9.45901906e-03, mean val. rec. loss:  8.68282363e-03\n",
      "Epoch: 9712 mean train loss:  9.45827139e-03, mean val. rec. loss:  8.68205593e-03\n",
      "Epoch: 9713 mean train loss:  9.45752186e-03, mean val. rec. loss:  8.68126555e-03\n",
      "Epoch: 9714 mean train loss:  9.45676767e-03, mean val. rec. loss:  8.68047687e-03\n",
      "Epoch: 9715 mean train loss:  9.45601162e-03, mean val. rec. loss:  8.67971314e-03\n",
      "Epoch: 9716 mean train loss:  9.45526488e-03, mean val. rec. loss:  8.67893239e-03\n",
      "Epoch: 9717 mean train loss:  9.45451814e-03, mean val. rec. loss:  8.67814428e-03\n",
      "Epoch: 9718 mean train loss:  9.45376209e-03, mean val. rec. loss:  8.67736127e-03\n",
      "Epoch: 9719 mean train loss:  9.45300697e-03, mean val. rec. loss:  8.67658563e-03\n",
      "Epoch: 9720 mean train loss:  9.45225930e-03, mean val. rec. loss:  8.67580999e-03\n",
      "Epoch: 9721 mean train loss:  9.45151164e-03, mean val. rec. loss:  8.67502755e-03\n",
      "Epoch: 9722 mean train loss:  9.45075559e-03, mean val. rec. loss:  8.67424510e-03\n",
      "Epoch: 9723 mean train loss:  9.44999954e-03, mean val. rec. loss:  8.67347853e-03\n",
      "Epoch: 9724 mean train loss:  9.44925187e-03, mean val. rec. loss:  8.67268248e-03\n",
      "Epoch: 9725 mean train loss:  9.44850140e-03, mean val. rec. loss:  8.67191818e-03\n",
      "Epoch: 9726 mean train loss:  9.44774908e-03, mean val. rec. loss:  8.67112723e-03\n",
      "Epoch: 9727 mean train loss:  9.44699769e-03, mean val. rec. loss:  8.67035103e-03\n",
      "Epoch: 9728 mean train loss:  9.44624536e-03, mean val. rec. loss:  8.66956291e-03\n",
      "Epoch: 9729 mean train loss:  9.44549397e-03, mean val. rec. loss:  8.66879464e-03\n",
      "Epoch: 9730 mean train loss:  9.44474350e-03, mean val. rec. loss:  8.66800823e-03\n",
      "Epoch: 9731 mean train loss:  9.44399025e-03, mean val. rec. loss:  8.66723089e-03\n",
      "Epoch: 9732 mean train loss:  9.44323606e-03, mean val. rec. loss:  8.66644788e-03\n",
      "Epoch: 9733 mean train loss:  9.44248653e-03, mean val. rec. loss:  8.66567791e-03\n",
      "Epoch: 9734 mean train loss:  9.44173514e-03, mean val. rec. loss:  8.66488866e-03\n",
      "Epoch: 9735 mean train loss:  9.44098188e-03, mean val. rec. loss:  8.66411415e-03\n",
      "Epoch: 9736 mean train loss:  9.44023142e-03, mean val. rec. loss:  8.66332547e-03\n",
      "Epoch: 9737 mean train loss:  9.43947909e-03, mean val. rec. loss:  8.66255834e-03\n",
      "Epoch: 9738 mean train loss:  9.43872770e-03, mean val. rec. loss:  8.66176966e-03\n",
      "Epoch: 9739 mean train loss:  9.43797258e-03, mean val. rec. loss:  8.66098892e-03\n",
      "Epoch: 9740 mean train loss:  9.43722305e-03, mean val. rec. loss:  8.66021101e-03\n",
      "Epoch: 9741 mean train loss:  9.43647352e-03, mean val. rec. loss:  8.65942403e-03\n",
      "Epoch: 9742 mean train loss:  9.43571933e-03, mean val. rec. loss:  8.65864952e-03\n",
      "Epoch: 9743 mean train loss:  9.43496514e-03, mean val. rec. loss:  8.65787161e-03\n",
      "Epoch: 9744 mean train loss:  9.43421468e-03, mean val. rec. loss:  8.65708804e-03\n",
      "Epoch: 9745 mean train loss:  9.43346515e-03, mean val. rec. loss:  8.65631466e-03\n",
      "Epoch: 9746 mean train loss:  9.43271003e-03, mean val. rec. loss:  8.65553279e-03\n",
      "Epoch: 9747 mean train loss:  9.43195863e-03, mean val. rec. loss:  8.65475148e-03\n",
      "Epoch: 9748 mean train loss:  9.43120631e-03, mean val. rec. loss:  8.65397470e-03\n",
      "Epoch: 9749 mean train loss:  9.43045492e-03, mean val. rec. loss:  8.65320076e-03\n",
      "Epoch: 9750 mean train loss:  9.42970166e-03, mean val. rec. loss:  8.65241945e-03\n",
      "Epoch: 9751 mean train loss:  9.42894934e-03, mean val. rec. loss:  8.65163474e-03\n",
      "Epoch: 9752 mean train loss:  9.42819794e-03, mean val. rec. loss:  8.65085683e-03\n",
      "Epoch: 9753 mean train loss:  9.42744469e-03, mean val. rec. loss:  8.65008233e-03\n",
      "Epoch: 9754 mean train loss:  9.42669329e-03, mean val. rec. loss:  8.64929535e-03\n",
      "Epoch: 9755 mean train loss:  9.42593910e-03, mean val. rec. loss:  8.64850553e-03\n",
      "Epoch: 9756 mean train loss:  9.42518864e-03, mean val. rec. loss:  8.64773726e-03\n",
      "Epoch: 9757 mean train loss:  9.42443539e-03, mean val. rec. loss:  8.64696616e-03\n",
      "Epoch: 9758 mean train loss:  9.42368399e-03, mean val. rec. loss:  8.64618032e-03\n",
      "Epoch: 9759 mean train loss:  9.42293260e-03, mean val. rec. loss:  8.64539560e-03\n",
      "Epoch: 9760 mean train loss:  9.42217841e-03, mean val. rec. loss:  8.64461770e-03\n",
      "Epoch: 9761 mean train loss:  9.42142515e-03, mean val. rec. loss:  8.64384035e-03\n",
      "Epoch: 9762 mean train loss:  9.42067283e-03, mean val. rec. loss:  8.64305734e-03\n",
      "Epoch: 9763 mean train loss:  9.41992609e-03, mean val. rec. loss:  8.64227547e-03\n",
      "Epoch: 9764 mean train loss:  9.41917004e-03, mean val. rec. loss:  8.64149075e-03\n",
      "Epoch: 9765 mean train loss:  9.41841213e-03, mean val. rec. loss:  8.64071058e-03\n",
      "Epoch: 9766 mean train loss:  9.41766353e-03, mean val. rec. loss:  8.63993494e-03\n",
      "Epoch: 9767 mean train loss:  9.41691679e-03, mean val. rec. loss:  8.63915420e-03\n",
      "Epoch: 9768 mean train loss:  9.41615888e-03, mean val. rec. loss:  8.63837629e-03\n",
      "Epoch: 9769 mean train loss:  9.41540469e-03, mean val. rec. loss:  8.63759611e-03\n",
      "Epoch: 9770 mean train loss:  9.41465423e-03, mean val. rec. loss:  8.63681991e-03\n",
      "Epoch: 9771 mean train loss:  9.41390377e-03, mean val. rec. loss:  8.63604653e-03\n",
      "Epoch: 9772 mean train loss:  9.41315144e-03, mean val. rec. loss:  8.63526579e-03\n",
      "Epoch: 9773 mean train loss:  9.41239819e-03, mean val. rec. loss:  8.63448335e-03\n",
      "Epoch: 9774 mean train loss:  9.41164307e-03, mean val. rec. loss:  8.63370544e-03\n",
      "Epoch: 9775 mean train loss:  9.41088888e-03, mean val. rec. loss:  8.63292753e-03\n",
      "Epoch: 9776 mean train loss:  9.41013842e-03, mean val. rec. loss:  8.63214509e-03\n",
      "Epoch: 9777 mean train loss:  9.40938795e-03, mean val. rec. loss:  8.63136945e-03\n",
      "Epoch: 9778 mean train loss:  9.40863191e-03, mean val. rec. loss:  8.63058644e-03\n",
      "Epoch: 9779 mean train loss:  9.40787772e-03, mean val. rec. loss:  8.62980513e-03\n",
      "Epoch: 9780 mean train loss:  9.40712726e-03, mean val. rec. loss:  8.62903232e-03\n",
      "Epoch: 9781 mean train loss:  9.40637679e-03, mean val. rec. loss:  8.62825101e-03\n",
      "Epoch: 9782 mean train loss:  9.40562167e-03, mean val. rec. loss:  8.62746630e-03\n",
      "Epoch: 9783 mean train loss:  9.40486842e-03, mean val. rec. loss:  8.62668272e-03\n",
      "Epoch: 9784 mean train loss:  9.40411516e-03, mean val. rec. loss:  8.62591275e-03\n",
      "Epoch: 9785 mean train loss:  9.40336284e-03, mean val. rec. loss:  8.62512974e-03\n",
      "Epoch: 9786 mean train loss:  9.40260679e-03, mean val. rec. loss:  8.62434106e-03\n",
      "Epoch: 9787 mean train loss:  9.40186098e-03, mean val. rec. loss:  8.62356996e-03\n",
      "Epoch: 9788 mean train loss:  9.40110121e-03, mean val. rec. loss:  8.62278751e-03\n",
      "Epoch: 9789 mean train loss:  9.40035074e-03, mean val. rec. loss:  8.62200393e-03\n",
      "Epoch: 9790 mean train loss:  9.39959563e-03, mean val. rec. loss:  8.62122603e-03\n",
      "Epoch: 9791 mean train loss:  9.39884703e-03, mean val. rec. loss:  8.62044812e-03\n",
      "Epoch: 9792 mean train loss:  9.39809377e-03, mean val. rec. loss:  8.61966851e-03\n",
      "Epoch: 9793 mean train loss:  9.39733586e-03, mean val. rec. loss:  8.61889003e-03\n",
      "Epoch: 9794 mean train loss:  9.39658726e-03, mean val. rec. loss:  8.61810872e-03\n",
      "Epoch: 9795 mean train loss:  9.39583121e-03, mean val. rec. loss:  8.61733762e-03\n",
      "Epoch: 9796 mean train loss:  9.39508168e-03, mean val. rec. loss:  8.61654837e-03\n",
      "Epoch: 9797 mean train loss:  9.39432376e-03, mean val. rec. loss:  8.61577273e-03\n",
      "Epoch: 9798 mean train loss:  9.39357610e-03, mean val. rec. loss:  8.61498915e-03\n",
      "Epoch: 9799 mean train loss:  9.39281818e-03, mean val. rec. loss:  8.61421351e-03\n",
      "Epoch: 9800 mean train loss:  9.39206865e-03, mean val. rec. loss:  8.61342540e-03\n",
      "Epoch: 9801 mean train loss:  9.39131167e-03, mean val. rec. loss:  8.61264976e-03\n",
      "Epoch: 9802 mean train loss:  9.39056307e-03, mean val. rec. loss:  8.61187129e-03\n",
      "Epoch: 9803 mean train loss:  9.38980702e-03, mean val. rec. loss:  8.61108941e-03\n",
      "Epoch: 9804 mean train loss:  9.38905097e-03, mean val. rec. loss:  8.61032171e-03\n",
      "Epoch: 9805 mean train loss:  9.38830237e-03, mean val. rec. loss:  8.60954380e-03\n",
      "Epoch: 9806 mean train loss:  9.38754818e-03, mean val. rec. loss:  8.60876022e-03\n",
      "Epoch: 9807 mean train loss:  9.38679307e-03, mean val. rec. loss:  8.60798345e-03\n",
      "Epoch: 9808 mean train loss:  9.38603795e-03, mean val. rec. loss:  8.60720327e-03\n",
      "Epoch: 9809 mean train loss:  9.38528749e-03, mean val. rec. loss:  8.60642083e-03\n",
      "Epoch: 9810 mean train loss:  9.38453516e-03, mean val. rec. loss:  8.60564179e-03\n",
      "Epoch: 9811 mean train loss:  9.38377911e-03, mean val. rec. loss:  8.60486728e-03\n",
      "Epoch: 9812 mean train loss:  9.38302306e-03, mean val. rec. loss:  8.60409277e-03\n",
      "Epoch: 9813 mean train loss:  9.38227260e-03, mean val. rec. loss:  8.60329899e-03\n",
      "Epoch: 9814 mean train loss:  9.38152120e-03, mean val. rec. loss:  8.60252392e-03\n",
      "Epoch: 9815 mean train loss:  9.38076795e-03, mean val. rec. loss:  8.60175281e-03\n",
      "Epoch: 9816 mean train loss:  9.38001376e-03, mean val. rec. loss:  8.60096243e-03\n",
      "Epoch: 9817 mean train loss:  9.37926330e-03, mean val. rec. loss:  8.60018226e-03\n",
      "Epoch: 9818 mean train loss:  9.37850725e-03, mean val. rec. loss:  8.59940718e-03\n",
      "Epoch: 9819 mean train loss:  9.37775120e-03, mean val. rec. loss:  8.59862587e-03\n",
      "Epoch: 9820 mean train loss:  9.37699981e-03, mean val. rec. loss:  8.59784116e-03\n",
      "Epoch: 9821 mean train loss:  9.37624841e-03, mean val. rec. loss:  8.59706665e-03\n",
      "Epoch: 9822 mean train loss:  9.37549050e-03, mean val. rec. loss:  8.59628818e-03\n",
      "Epoch: 9823 mean train loss:  9.37474004e-03, mean val. rec. loss:  8.59549780e-03\n",
      "Epoch: 9824 mean train loss:  9.37398213e-03, mean val. rec. loss:  8.59472840e-03\n",
      "Epoch: 9825 mean train loss:  9.37322608e-03, mean val. rec. loss:  8.59395842e-03\n",
      "Epoch: 9826 mean train loss:  9.37247375e-03, mean val. rec. loss:  8.59317258e-03\n",
      "Epoch: 9827 mean train loss:  9.37172143e-03, mean val. rec. loss:  8.59239240e-03\n",
      "Epoch: 9828 mean train loss:  9.37096910e-03, mean val. rec. loss:  8.59162300e-03\n",
      "Epoch: 9829 mean train loss:  9.37020933e-03, mean val. rec. loss:  8.59084112e-03\n",
      "Epoch: 9830 mean train loss:  9.36945980e-03, mean val. rec. loss:  8.59005358e-03\n",
      "Epoch: 9831 mean train loss:  9.36871027e-03, mean val. rec. loss:  8.58927737e-03\n",
      "Epoch: 9832 mean train loss:  9.36795142e-03, mean val. rec. loss:  8.58849719e-03\n",
      "Epoch: 9833 mean train loss:  9.36719258e-03, mean val. rec. loss:  8.58771362e-03\n",
      "Epoch: 9834 mean train loss:  9.36644398e-03, mean val. rec. loss:  8.58693117e-03\n",
      "Epoch: 9835 mean train loss:  9.36569445e-03, mean val. rec. loss:  8.58615610e-03\n",
      "Epoch: 9836 mean train loss:  9.36493374e-03, mean val. rec. loss:  8.58537649e-03\n",
      "Epoch: 9837 mean train loss:  9.36418235e-03, mean val. rec. loss:  8.58458894e-03\n",
      "Epoch: 9838 mean train loss:  9.36342630e-03, mean val. rec. loss:  8.58382124e-03\n",
      "Epoch: 9839 mean train loss:  9.36267863e-03, mean val. rec. loss:  8.58304277e-03\n",
      "Epoch: 9840 mean train loss:  9.36191699e-03, mean val. rec. loss:  8.58225579e-03\n",
      "Epoch: 9841 mean train loss:  9.36116467e-03, mean val. rec. loss:  8.58147448e-03\n",
      "Epoch: 9842 mean train loss:  9.36040862e-03, mean val. rec. loss:  8.58070677e-03\n",
      "Epoch: 9843 mean train loss:  9.35966188e-03, mean val. rec. loss:  8.57992433e-03\n",
      "Epoch: 9844 mean train loss:  9.35890304e-03, mean val. rec. loss:  8.57913622e-03\n",
      "Epoch: 9845 mean train loss:  9.35814420e-03, mean val. rec. loss:  8.57837418e-03\n",
      "Epoch: 9846 mean train loss:  9.35739466e-03, mean val. rec. loss:  8.57759854e-03\n",
      "Epoch: 9847 mean train loss:  9.35664513e-03, mean val. rec. loss:  8.57681270e-03\n",
      "Epoch: 9848 mean train loss:  9.35588722e-03, mean val. rec. loss:  8.57602572e-03\n",
      "Epoch: 9849 mean train loss:  9.35512745e-03, mean val. rec. loss:  8.57525235e-03\n",
      "Epoch: 9850 mean train loss:  9.35437792e-03, mean val. rec. loss:  8.57447671e-03\n",
      "Epoch: 9851 mean train loss:  9.35362745e-03, mean val. rec. loss:  8.57369029e-03\n",
      "Epoch: 9852 mean train loss:  9.35286861e-03, mean val. rec. loss:  8.57291522e-03\n",
      "Epoch: 9853 mean train loss:  9.35211070e-03, mean val. rec. loss:  8.57213845e-03\n",
      "Epoch: 9854 mean train loss:  9.35135837e-03, mean val. rec. loss:  8.57135600e-03\n",
      "Epoch: 9855 mean train loss:  9.35060326e-03, mean val. rec. loss:  8.57057696e-03\n",
      "Epoch: 9856 mean train loss:  9.34985000e-03, mean val. rec. loss:  8.56980019e-03\n",
      "Epoch: 9857 mean train loss:  9.34909395e-03, mean val. rec. loss:  8.56902171e-03\n",
      "Epoch: 9858 mean train loss:  9.34834069e-03, mean val. rec. loss:  8.56823870e-03\n",
      "Epoch: 9859 mean train loss:  9.34758558e-03, mean val. rec. loss:  8.56745626e-03\n",
      "Epoch: 9860 mean train loss:  9.34683046e-03, mean val. rec. loss:  8.56668345e-03\n",
      "Epoch: 9861 mean train loss:  9.34607906e-03, mean val. rec. loss:  8.56590611e-03\n",
      "Epoch: 9862 mean train loss:  9.34532581e-03, mean val. rec. loss:  8.56512707e-03\n",
      "Epoch: 9863 mean train loss:  9.34456976e-03, mean val. rec. loss:  8.56434689e-03\n",
      "Epoch: 9864 mean train loss:  9.34381278e-03, mean val. rec. loss:  8.56356672e-03\n",
      "Epoch: 9865 mean train loss:  9.34305859e-03, mean val. rec. loss:  8.56279221e-03\n",
      "Epoch: 9866 mean train loss:  9.34230533e-03, mean val. rec. loss:  8.56201090e-03\n",
      "Epoch: 9867 mean train loss:  9.34155394e-03, mean val. rec. loss:  8.56122165e-03\n",
      "Epoch: 9868 mean train loss:  9.34079789e-03, mean val. rec. loss:  8.56045112e-03\n",
      "Epoch: 9869 mean train loss:  9.34003998e-03, mean val. rec. loss:  8.55967378e-03\n",
      "Epoch: 9870 mean train loss:  9.33928766e-03, mean val. rec. loss:  8.55889133e-03\n",
      "Epoch: 9871 mean train loss:  9.33853347e-03, mean val. rec. loss:  8.55811002e-03\n",
      "Epoch: 9872 mean train loss:  9.33777742e-03, mean val. rec. loss:  8.55733892e-03\n",
      "Epoch: 9873 mean train loss:  9.33702323e-03, mean val. rec. loss:  8.55655307e-03\n",
      "Epoch: 9874 mean train loss:  9.33626811e-03, mean val. rec. loss:  8.55577460e-03\n",
      "Epoch: 9875 mean train loss:  9.33551300e-03, mean val. rec. loss:  8.55500519e-03\n",
      "Epoch: 9876 mean train loss:  9.33475881e-03, mean val. rec. loss:  8.55422275e-03\n",
      "Epoch: 9877 mean train loss:  9.33400369e-03, mean val. rec. loss:  8.55343577e-03\n",
      "Epoch: 9878 mean train loss:  9.33324857e-03, mean val. rec. loss:  8.55266580e-03\n",
      "Epoch: 9879 mean train loss:  9.33249438e-03, mean val. rec. loss:  8.55188903e-03\n",
      "Epoch: 9880 mean train loss:  9.33173833e-03, mean val. rec. loss:  8.55110375e-03\n",
      "Epoch: 9881 mean train loss:  9.33098415e-03, mean val. rec. loss:  8.55032187e-03\n",
      "Epoch: 9882 mean train loss:  9.33022903e-03, mean val. rec. loss:  8.54954737e-03\n",
      "Epoch: 9883 mean train loss:  9.32947391e-03, mean val. rec. loss:  8.54877173e-03\n",
      "Epoch: 9884 mean train loss:  9.32871879e-03, mean val. rec. loss:  8.54798701e-03\n",
      "Epoch: 9885 mean train loss:  9.32796367e-03, mean val. rec. loss:  8.54721081e-03\n",
      "Epoch: 9886 mean train loss:  9.32720669e-03, mean val. rec. loss:  8.54644027e-03\n",
      "Epoch: 9887 mean train loss:  9.32645344e-03, mean val. rec. loss:  8.54565216e-03\n",
      "Epoch: 9888 mean train loss:  9.32570111e-03, mean val. rec. loss:  8.54487368e-03\n",
      "Epoch: 9889 mean train loss:  9.32494320e-03, mean val. rec. loss:  8.54408954e-03\n",
      "Epoch: 9890 mean train loss:  9.32418343e-03, mean val. rec. loss:  8.54331333e-03\n",
      "Epoch: 9891 mean train loss:  9.32343110e-03, mean val. rec. loss:  8.54253372e-03\n",
      "Epoch: 9892 mean train loss:  9.32267785e-03, mean val. rec. loss:  8.54175695e-03\n",
      "Epoch: 9893 mean train loss:  9.32192273e-03, mean val. rec. loss:  8.54097790e-03\n",
      "Epoch: 9894 mean train loss:  9.32116854e-03, mean val. rec. loss:  8.54020226e-03\n",
      "Epoch: 9895 mean train loss:  9.32041249e-03, mean val. rec. loss:  8.53943116e-03\n",
      "Epoch: 9896 mean train loss:  9.31965737e-03, mean val. rec. loss:  8.53864872e-03\n",
      "Epoch: 9897 mean train loss:  9.31890132e-03, mean val. rec. loss:  8.53786967e-03\n",
      "Epoch: 9898 mean train loss:  9.31814621e-03, mean val. rec. loss:  8.53709177e-03\n",
      "Epoch: 9899 mean train loss:  9.31738923e-03, mean val. rec. loss:  8.53630876e-03\n",
      "Epoch: 9900 mean train loss:  9.31663038e-03, mean val. rec. loss:  8.53552461e-03\n",
      "Epoch: 9901 mean train loss:  9.31587806e-03, mean val. rec. loss:  8.53474670e-03\n",
      "Epoch: 9902 mean train loss:  9.31512480e-03, mean val. rec. loss:  8.53397390e-03\n",
      "Epoch: 9903 mean train loss:  9.31436968e-03, mean val. rec. loss:  8.53319316e-03\n",
      "Epoch: 9904 mean train loss:  9.31361363e-03, mean val. rec. loss:  8.53241014e-03\n",
      "Epoch: 9905 mean train loss:  9.31285759e-03, mean val. rec. loss:  8.53163734e-03\n",
      "Epoch: 9906 mean train loss:  9.31210340e-03, mean val. rec. loss:  8.53086283e-03\n",
      "Epoch: 9907 mean train loss:  9.31134735e-03, mean val. rec. loss:  8.53007926e-03\n",
      "Epoch: 9908 mean train loss:  9.31059130e-03, mean val. rec. loss:  8.52929568e-03\n",
      "Epoch: 9909 mean train loss:  9.30983618e-03, mean val. rec. loss:  8.52852344e-03\n",
      "Epoch: 9910 mean train loss:  9.30908013e-03, mean val. rec. loss:  8.52774383e-03\n",
      "Epoch: 9911 mean train loss:  9.30832688e-03, mean val. rec. loss:  8.52695742e-03\n",
      "Epoch: 9912 mean train loss:  9.30756990e-03, mean val. rec. loss:  8.52618121e-03\n",
      "Epoch: 9913 mean train loss:  9.30681198e-03, mean val. rec. loss:  8.52541067e-03\n",
      "Epoch: 9914 mean train loss:  9.30605687e-03, mean val. rec. loss:  8.52463560e-03\n",
      "Epoch: 9915 mean train loss:  9.30530268e-03, mean val. rec. loss:  8.52384805e-03\n",
      "Epoch: 9916 mean train loss:  9.30454849e-03, mean val. rec. loss:  8.52307468e-03\n",
      "Epoch: 9917 mean train loss:  9.30379058e-03, mean val. rec. loss:  8.52230301e-03\n",
      "Epoch: 9918 mean train loss:  9.30303453e-03, mean val. rec. loss:  8.52151773e-03\n",
      "Epoch: 9919 mean train loss:  9.30227941e-03, mean val. rec. loss:  8.52073642e-03\n",
      "Epoch: 9920 mean train loss:  9.30152336e-03, mean val. rec. loss:  8.51996702e-03\n",
      "Epoch: 9921 mean train loss:  9.30076731e-03, mean val. rec. loss:  8.51918514e-03\n",
      "Epoch: 9922 mean train loss:  9.30001220e-03, mean val. rec. loss:  8.51840553e-03\n",
      "Epoch: 9923 mean train loss:  9.29925615e-03, mean val. rec. loss:  8.51762876e-03\n",
      "Epoch: 9924 mean train loss:  9.29850010e-03, mean val. rec. loss:  8.51685482e-03\n",
      "Epoch: 9925 mean train loss:  9.29774498e-03, mean val. rec. loss:  8.51608031e-03\n",
      "Epoch: 9926 mean train loss:  9.29698800e-03, mean val. rec. loss:  8.51528993e-03\n",
      "Epoch: 9927 mean train loss:  9.29623288e-03, mean val. rec. loss:  8.51451146e-03\n",
      "Epoch: 9928 mean train loss:  9.29547683e-03, mean val. rec. loss:  8.51374035e-03\n",
      "Epoch: 9929 mean train loss:  9.29472078e-03, mean val. rec. loss:  8.51296301e-03\n",
      "Epoch: 9930 mean train loss:  9.29396194e-03, mean val. rec. loss:  8.51217490e-03\n",
      "Epoch: 9931 mean train loss:  9.29320403e-03, mean val. rec. loss:  8.51140663e-03\n",
      "Epoch: 9932 mean train loss:  9.29245077e-03, mean val. rec. loss:  8.51063156e-03\n",
      "Epoch: 9933 mean train loss:  9.29169472e-03, mean val. rec. loss:  8.50984288e-03\n",
      "Epoch: 9934 mean train loss:  9.29093867e-03, mean val. rec. loss:  8.50906327e-03\n",
      "Epoch: 9935 mean train loss:  9.29018262e-03, mean val. rec. loss:  8.50828593e-03\n",
      "Epoch: 9936 mean train loss:  9.28942564e-03, mean val. rec. loss:  8.50750178e-03\n",
      "Epoch: 9937 mean train loss:  9.28866959e-03, mean val. rec. loss:  8.50671934e-03\n",
      "Epoch: 9938 mean train loss:  9.28791168e-03, mean val. rec. loss:  8.50594597e-03\n",
      "Epoch: 9939 mean train loss:  9.28716215e-03, mean val. rec. loss:  8.50517486e-03\n",
      "Epoch: 9940 mean train loss:  9.28640517e-03, mean val. rec. loss:  8.50440149e-03\n",
      "Epoch: 9941 mean train loss:  9.28564540e-03, mean val. rec. loss:  8.50361224e-03\n",
      "Epoch: 9942 mean train loss:  9.28489121e-03, mean val. rec. loss:  8.50284567e-03\n",
      "Epoch: 9943 mean train loss:  9.28413423e-03, mean val. rec. loss:  8.50206493e-03\n",
      "Epoch: 9944 mean train loss:  9.28338097e-03, mean val. rec. loss:  8.50126945e-03\n",
      "Epoch: 9945 mean train loss:  9.28262213e-03, mean val. rec. loss:  8.50050571e-03\n",
      "Epoch: 9946 mean train loss:  9.28186329e-03, mean val. rec. loss:  8.49973518e-03\n",
      "Epoch: 9947 mean train loss:  9.28111096e-03, mean val. rec. loss:  8.49894423e-03\n",
      "Epoch: 9948 mean train loss:  9.28035026e-03, mean val. rec. loss:  8.49816689e-03\n",
      "Epoch: 9949 mean train loss:  9.27959607e-03, mean val. rec. loss:  8.49739918e-03\n",
      "Epoch: 9950 mean train loss:  9.27884281e-03, mean val. rec. loss:  8.49662468e-03\n",
      "Epoch: 9951 mean train loss:  9.27808304e-03, mean val. rec. loss:  8.49583883e-03\n",
      "Epoch: 9952 mean train loss:  9.27732699e-03, mean val. rec. loss:  8.49505525e-03\n",
      "Epoch: 9953 mean train loss:  9.27656908e-03, mean val. rec. loss:  8.49428642e-03\n",
      "Epoch: 9954 mean train loss:  9.27581396e-03, mean val. rec. loss:  8.49350397e-03\n",
      "Epoch: 9955 mean train loss:  9.27505605e-03, mean val. rec. loss:  8.49272040e-03\n",
      "Epoch: 9956 mean train loss:  9.27430093e-03, mean val. rec. loss:  8.49195326e-03\n",
      "Epoch: 9957 mean train loss:  9.27354861e-03, mean val. rec. loss:  8.49117705e-03\n",
      "Epoch: 9958 mean train loss:  9.27278418e-03, mean val. rec. loss:  8.49038781e-03\n",
      "Epoch: 9959 mean train loss:  9.27202999e-03, mean val. rec. loss:  8.48962181e-03\n",
      "Epoch: 9960 mean train loss:  9.27127208e-03, mean val. rec. loss:  8.48884333e-03\n",
      "Epoch: 9961 mean train loss:  9.27052068e-03, mean val. rec. loss:  8.48805465e-03\n",
      "Epoch: 9962 mean train loss:  9.26975905e-03, mean val. rec. loss:  8.48728355e-03\n",
      "Epoch: 9963 mean train loss:  9.26900672e-03, mean val. rec. loss:  8.48651131e-03\n",
      "Epoch: 9964 mean train loss:  9.26824509e-03, mean val. rec. loss:  8.48572263e-03\n",
      "Epoch: 9965 mean train loss:  9.26749276e-03, mean val. rec. loss:  8.48494925e-03\n",
      "Epoch: 9966 mean train loss:  9.26673113e-03, mean val. rec. loss:  8.48417588e-03\n",
      "Epoch: 9967 mean train loss:  9.26597973e-03, mean val. rec. loss:  8.48339344e-03\n",
      "Epoch: 9968 mean train loss:  9.26521810e-03, mean val. rec. loss:  8.48261440e-03\n",
      "Epoch: 9969 mean train loss:  9.26446484e-03, mean val. rec. loss:  8.48183989e-03\n",
      "Epoch: 9970 mean train loss:  9.26370507e-03, mean val. rec. loss:  8.48106425e-03\n",
      "Epoch: 9971 mean train loss:  9.26295181e-03, mean val. rec. loss:  8.48028237e-03\n",
      "Epoch: 9972 mean train loss:  9.26219017e-03, mean val. rec. loss:  8.47951070e-03\n",
      "Epoch: 9973 mean train loss:  9.26144064e-03, mean val. rec. loss:  8.47873676e-03\n",
      "Epoch: 9974 mean train loss:  9.26067621e-03, mean val. rec. loss:  8.47794978e-03\n",
      "Epoch: 9975 mean train loss:  9.25992016e-03, mean val. rec. loss:  8.47717698e-03\n",
      "Epoch: 9976 mean train loss:  9.25916505e-03, mean val. rec. loss:  8.47639567e-03\n",
      "Epoch: 9977 mean train loss:  9.25841086e-03, mean val. rec. loss:  8.47560869e-03\n",
      "Epoch: 9978 mean train loss:  9.25765109e-03, mean val. rec. loss:  8.47483759e-03\n",
      "Epoch: 9979 mean train loss:  9.25689131e-03, mean val. rec. loss:  8.47406421e-03\n",
      "Epoch: 9980 mean train loss:  9.25613619e-03, mean val. rec. loss:  8.47327723e-03\n",
      "Epoch: 9981 mean train loss:  9.25537735e-03, mean val. rec. loss:  8.47250159e-03\n",
      "Epoch: 9982 mean train loss:  9.25462223e-03, mean val. rec. loss:  8.47173503e-03\n",
      "Epoch: 9983 mean train loss:  9.25386711e-03, mean val. rec. loss:  8.47095258e-03\n",
      "Epoch: 9984 mean train loss:  9.25310734e-03, mean val. rec. loss:  8.47015993e-03\n",
      "Epoch: 9985 mean train loss:  9.25234850e-03, mean val. rec. loss:  8.46939393e-03\n",
      "Epoch: 9986 mean train loss:  9.25159524e-03, mean val. rec. loss:  8.46862509e-03\n",
      "Epoch: 9987 mean train loss:  9.25083454e-03, mean val. rec. loss:  8.46783585e-03\n",
      "Epoch: 9988 mean train loss:  9.25008314e-03, mean val. rec. loss:  8.46706304e-03\n",
      "Epoch: 9989 mean train loss:  9.24931964e-03, mean val. rec. loss:  8.46629080e-03\n",
      "Epoch: 9990 mean train loss:  9.24856453e-03, mean val. rec. loss:  8.46551063e-03\n",
      "Epoch: 9991 mean train loss:  9.24780475e-03, mean val. rec. loss:  8.46472818e-03\n",
      "Epoch: 9992 mean train loss:  9.24705150e-03, mean val. rec. loss:  8.46395595e-03\n",
      "Epoch: 9993 mean train loss:  9.24629265e-03, mean val. rec. loss:  8.46318144e-03\n",
      "Epoch: 9994 mean train loss:  9.24553288e-03, mean val. rec. loss:  8.46240126e-03\n",
      "Epoch: 9995 mean train loss:  9.24477962e-03, mean val. rec. loss:  8.46162449e-03\n",
      "Epoch: 9996 mean train loss:  9.24401799e-03, mean val. rec. loss:  8.46084772e-03\n",
      "Epoch: 9997 mean train loss:  9.24326287e-03, mean val. rec. loss:  8.46007321e-03\n",
      "Epoch: 9998 mean train loss:  9.24250775e-03, mean val. rec. loss:  8.45929474e-03\n",
      "Epoch: 9999 mean train loss:  9.24174798e-03, mean val. rec. loss:  8.45851626e-03\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = train(net_H1, loaders, args, A, H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NN_library/training_data/PINN_H1_{total_params}', np.vstack([losses_train, losses_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 65\n"
     ]
    }
   ],
   "source": [
    "args = {'lr' : 0.0005, 'epochs' : 10000, 'dev' : dev, 'name' : f'NN_library/PINN/PINN_H2_{total_params}'}\n",
    "net_H2 = PINN(n_periodic=2, n_hidden=4, n_layers=1, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_H2.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_H2 = load_network(net_H2, args['name']+'_4999', args)\n",
    "net_H2 = net_H2.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 mean train loss:  1.65125289e-01, mean val. rec. loss:  1.53807487e-01\n",
      "Epoch: 1 mean train loss:  1.62121770e-01, mean val. rec. loss:  1.50869218e-01\n",
      "Epoch: 2 mean train loss:  1.59194123e-01, mean val. rec. loss:  1.48007079e-01\n",
      "Epoch: 3 mean train loss:  1.56343110e-01, mean val. rec. loss:  1.45221706e-01\n",
      "Epoch: 4 mean train loss:  1.53569295e-01, mean val. rec. loss:  1.42513561e-01\n",
      "Epoch: 5 mean train loss:  1.50873156e-01, mean val. rec. loss:  1.39882998e-01\n",
      "Epoch: 6 mean train loss:  1.48255020e-01, mean val. rec. loss:  1.37330208e-01\n",
      "Epoch: 7 mean train loss:  1.45715007e-01, mean val. rec. loss:  1.34855200e-01\n",
      "Epoch: 8 mean train loss:  1.43253102e-01, mean val. rec. loss:  1.32457810e-01\n",
      "Epoch: 9 mean train loss:  1.40869141e-01, mean val. rec. loss:  1.30137712e-01\n",
      "Epoch: 10 mean train loss:  1.38562706e-01, mean val. rec. loss:  1.27894407e-01\n",
      "Epoch: 11 mean train loss:  1.36333262e-01, mean val. rec. loss:  1.25727142e-01\n",
      "Epoch: 12 mean train loss:  1.34180033e-01, mean val. rec. loss:  1.23635037e-01\n",
      "Epoch: 13 mean train loss:  1.32102052e-01, mean val. rec. loss:  1.21617003e-01\n",
      "Epoch: 14 mean train loss:  1.30098201e-01, mean val. rec. loss:  1.19671725e-01\n",
      "Epoch: 15 mean train loss:  1.28167124e-01, mean val. rec. loss:  1.17797752e-01\n",
      "Epoch: 16 mean train loss:  1.26307332e-01, mean val. rec. loss:  1.15993441e-01\n",
      "Epoch: 17 mean train loss:  1.24517134e-01, mean val. rec. loss:  1.14257023e-01\n",
      "Epoch: 18 mean train loss:  1.22794734e-01, mean val. rec. loss:  1.12586639e-01\n",
      "Epoch: 19 mean train loss:  1.21138189e-01, mean val. rec. loss:  1.10980321e-01\n",
      "Epoch: 20 mean train loss:  1.19545547e-01, mean val. rec. loss:  1.09436044e-01\n",
      "Epoch: 21 mean train loss:  1.18014760e-01, mean val. rec. loss:  1.07951814e-01\n",
      "Epoch: 22 mean train loss:  1.16543771e-01, mean val. rec. loss:  1.06525616e-01\n",
      "Epoch: 23 mean train loss:  1.15130569e-01, mean val. rec. loss:  1.05155482e-01\n",
      "Epoch: 24 mean train loss:  1.13773136e-01, mean val. rec. loss:  1.03839489e-01\n",
      "Epoch: 25 mean train loss:  1.12469544e-01, mean val. rec. loss:  1.02575767e-01\n",
      "Epoch: 26 mean train loss:  1.11217891e-01, mean val. rec. loss:  1.01362557e-01\n",
      "Epoch: 27 mean train loss:  1.10016391e-01, mean val. rec. loss:  1.00198108e-01\n",
      "Epoch: 28 mean train loss:  1.08863307e-01, mean val. rec. loss:  9.90808058e-02\n",
      "Epoch: 29 mean train loss:  1.07756995e-01, mean val. rec. loss:  9.80090528e-02\n",
      "Epoch: 30 mean train loss:  1.06695852e-01, mean val. rec. loss:  9.69813343e-02\n",
      "Epoch: 31 mean train loss:  1.05678358e-01, mean val. rec. loss:  9.59961899e-02\n",
      "Epoch: 32 mean train loss:  1.04703092e-01, mean val. rec. loss:  9.50521861e-02\n",
      "Epoch: 33 mean train loss:  1.03768615e-01, mean val. rec. loss:  9.41479804e-02\n",
      "Epoch: 34 mean train loss:  1.02873586e-01, mean val. rec. loss:  9.32821848e-02\n",
      "Epoch: 35 mean train loss:  1.02016696e-01, mean val. rec. loss:  9.24535472e-02\n",
      "Epoch: 36 mean train loss:  1.01196647e-01, mean val. rec. loss:  9.16607434e-02\n",
      "Epoch: 37 mean train loss:  1.00412217e-01, mean val. rec. loss:  9.09025213e-02\n",
      "Epoch: 38 mean train loss:  9.96621495e-02, mean val. rec. loss:  9.01776109e-02\n",
      "Epoch: 39 mean train loss:  9.89452434e-02, mean val. rec. loss:  8.94848147e-02\n",
      "Epoch: 40 mean train loss:  9.82602999e-02, mean val. rec. loss:  8.88228989e-02\n",
      "Epoch: 41 mean train loss:  9.76061794e-02, mean val. rec. loss:  8.81906661e-02\n",
      "Epoch: 42 mean train loss:  9.69816678e-02, mean val. rec. loss:  8.75869370e-02\n",
      "Epoch: 43 mean train loss:  9.63856551e-02, mean val. rec. loss:  8.70105322e-02\n",
      "Epoch: 44 mean train loss:  9.58169868e-02, mean val. rec. loss:  8.64603176e-02\n",
      "Epoch: 45 mean train loss:  9.52745679e-02, mean val. rec. loss:  8.59351595e-02\n",
      "Epoch: 46 mean train loss:  9.47572812e-02, mean val. rec. loss:  8.54339418e-02\n",
      "Epoch: 47 mean train loss:  9.42640540e-02, mean val. rec. loss:  8.49556123e-02\n",
      "Epoch: 48 mean train loss:  9.37938434e-02, mean val. rec. loss:  8.44991097e-02\n",
      "Epoch: 49 mean train loss:  9.33456067e-02, mean val. rec. loss:  8.40634268e-02\n",
      "Epoch: 50 mean train loss:  9.29183606e-02, mean val. rec. loss:  8.36475930e-02\n",
      "Epoch: 51 mean train loss:  9.25111144e-02, mean val. rec. loss:  8.32506468e-02\n",
      "Epoch: 52 mean train loss:  9.21229446e-02, mean val. rec. loss:  8.28717081e-02\n",
      "Epoch: 53 mean train loss:  9.17529273e-02, mean val. rec. loss:  8.25098788e-02\n",
      "Epoch: 54 mean train loss:  9.14002210e-02, mean val. rec. loss:  8.21643426e-02\n",
      "Epoch: 55 mean train loss:  9.10639466e-02, mean val. rec. loss:  8.18343191e-02\n",
      "Epoch: 56 mean train loss:  9.07433221e-02, mean val. rec. loss:  8.15190101e-02\n",
      "Epoch: 57 mean train loss:  9.04375653e-02, mean val. rec. loss:  8.12177443e-02\n",
      "Epoch: 58 mean train loss:  9.01459239e-02, mean val. rec. loss:  8.09297959e-02\n",
      "Epoch: 59 mean train loss:  8.98677127e-02, mean val. rec. loss:  8.06545117e-02\n",
      "Epoch: 60 mean train loss:  8.96022313e-02, mean val. rec. loss:  8.03912749e-02\n",
      "Epoch: 61 mean train loss:  8.93488021e-02, mean val. rec. loss:  8.01394867e-02\n",
      "Epoch: 62 mean train loss:  8.91068291e-02, mean val. rec. loss:  7.98985575e-02\n",
      "Epoch: 63 mean train loss:  8.88757312e-02, mean val. rec. loss:  7.96679339e-02\n",
      "Epoch: 64 mean train loss:  8.86548903e-02, mean val. rec. loss:  7.94471078e-02\n",
      "Epoch: 65 mean train loss:  8.84437850e-02, mean val. rec. loss:  7.92355713e-02\n",
      "Epoch: 66 mean train loss:  8.82418714e-02, mean val. rec. loss:  7.90328617e-02\n",
      "Epoch: 67 mean train loss:  8.80486356e-02, mean val. rec. loss:  7.88384981e-02\n",
      "Epoch: 68 mean train loss:  8.78636009e-02, mean val. rec. loss:  7.86520088e-02\n",
      "Epoch: 69 mean train loss:  8.76863055e-02, mean val. rec. loss:  7.84730038e-02\n",
      "Epoch: 70 mean train loss:  8.75162949e-02, mean val. rec. loss:  7.83010657e-02\n",
      "Epoch: 71 mean train loss:  8.73531447e-02, mean val. rec. loss:  7.81357955e-02\n",
      "Epoch: 72 mean train loss:  8.71964451e-02, mean val. rec. loss:  7.79768301e-02\n",
      "Epoch: 73 mean train loss:  8.70458162e-02, mean val. rec. loss:  7.78237886e-02\n",
      "Epoch: 74 mean train loss:  8.69008782e-02, mean val. rec. loss:  7.76763444e-02\n",
      "Epoch: 75 mean train loss:  8.67612735e-02, mean val. rec. loss:  7.75341710e-02\n",
      "Epoch: 76 mean train loss:  8.66266893e-02, mean val. rec. loss:  7.73969507e-02\n",
      "Epoch: 77 mean train loss:  8.64967903e-02, mean val. rec. loss:  7.72643843e-02\n",
      "Epoch: 78 mean train loss:  8.63713010e-02, mean val. rec. loss:  7.71361996e-02\n",
      "Epoch: 79 mean train loss:  8.62499309e-02, mean val. rec. loss:  7.70121244e-02\n",
      "Epoch: 80 mean train loss:  8.61324118e-02, mean val. rec. loss:  7.68918685e-02\n",
      "Epoch: 81 mean train loss:  8.60185053e-02, mean val. rec. loss:  7.67752413e-02\n",
      "Epoch: 82 mean train loss:  8.59079582e-02, mean val. rec. loss:  7.66619978e-02\n",
      "Epoch: 83 mean train loss:  8.58005545e-02, mean val. rec. loss:  7.65519114e-02\n",
      "Epoch: 84 mean train loss:  8.56961005e-02, mean val. rec. loss:  7.64448005e-02\n",
      "Epoch: 85 mean train loss:  8.55944026e-02, mean val. rec. loss:  7.63404384e-02\n",
      "Epoch: 86 mean train loss:  8.54952670e-02, mean val. rec. loss:  7.62386798e-02\n",
      "Epoch: 87 mean train loss:  8.53985225e-02, mean val. rec. loss:  7.61393435e-02\n",
      "Epoch: 88 mean train loss:  8.53040200e-02, mean val. rec. loss:  7.60422479e-02\n",
      "Epoch: 89 mean train loss:  8.52116032e-02, mean val. rec. loss:  7.59472569e-02\n",
      "Epoch: 90 mean train loss:  8.51211380e-02, mean val. rec. loss:  7.58542345e-02\n",
      "Epoch: 91 mean train loss:  8.50324903e-02, mean val. rec. loss:  7.57630628e-02\n",
      "Epoch: 92 mean train loss:  8.49455335e-02, mean val. rec. loss:  7.56735875e-02\n",
      "Epoch: 93 mean train loss:  8.48601557e-02, mean val. rec. loss:  7.55857225e-02\n",
      "Epoch: 94 mean train loss:  8.47762603e-02, mean val. rec. loss:  7.54993498e-02\n",
      "Epoch: 95 mean train loss:  8.46937504e-02, mean val. rec. loss:  7.54143741e-02\n",
      "Epoch: 96 mean train loss:  8.46125142e-02, mean val. rec. loss:  7.53307093e-02\n",
      "Epoch: 97 mean train loss:  8.45324847e-02, mean val. rec. loss:  7.52482737e-02\n",
      "Epoch: 98 mean train loss:  8.44535725e-02, mean val. rec. loss:  7.51669812e-02\n",
      "Epoch: 99 mean train loss:  8.43757032e-02, mean val. rec. loss:  7.50867773e-02\n",
      "Epoch: 100 mean train loss:  8.42988170e-02, mean val. rec. loss:  7.50075895e-02\n",
      "Epoch: 101 mean train loss:  8.42228546e-02, mean val. rec. loss:  7.49293814e-02\n",
      "Epoch: 102 mean train loss:  8.41477636e-02, mean val. rec. loss:  7.48520532e-02\n",
      "Epoch: 103 mean train loss:  8.40734771e-02, mean val. rec. loss:  7.47756051e-02\n",
      "Epoch: 104 mean train loss:  8.39999578e-02, mean val. rec. loss:  7.46999734e-02\n",
      "Epoch: 105 mean train loss:  8.39271536e-02, mean val. rec. loss:  7.46251219e-02\n",
      "Epoch: 106 mean train loss:  8.38550123e-02, mean val. rec. loss:  7.45510051e-02\n",
      "Epoch: 107 mean train loss:  8.37835266e-02, mean val. rec. loss:  7.44776051e-02\n",
      "Epoch: 108 mean train loss:  8.37126293e-02, mean val. rec. loss:  7.44048764e-02\n",
      "Epoch: 109 mean train loss:  8.36423204e-02, mean val. rec. loss:  7.43328054e-02\n",
      "Epoch: 110 mean train loss:  8.35725404e-02, mean val. rec. loss:  7.42613467e-02\n",
      "Epoch: 111 mean train loss:  8.35032744e-02, mean val. rec. loss:  7.41905004e-02\n",
      "Epoch: 112 mean train loss:  8.34344925e-02, mean val. rec. loss:  7.41202483e-02\n",
      "Epoch: 113 mean train loss:  8.33661873e-02, mean val. rec. loss:  7.40505132e-02\n",
      "Epoch: 114 mean train loss:  8.32983366e-02, mean val. rec. loss:  7.39813497e-02\n",
      "Epoch: 115 mean train loss:  8.32308880e-02, mean val. rec. loss:  7.39126897e-02\n",
      "Epoch: 116 mean train loss:  8.31638641e-02, mean val. rec. loss:  7.38445377e-02\n",
      "Epoch: 117 mean train loss:  8.30972349e-02, mean val. rec. loss:  7.37768620e-02\n",
      "Epoch: 118 mean train loss:  8.30309558e-02, mean val. rec. loss:  7.37096671e-02\n",
      "Epoch: 119 mean train loss:  8.29650566e-02, mean val. rec. loss:  7.36429167e-02\n",
      "Epoch: 120 mean train loss:  8.28995075e-02, mean val. rec. loss:  7.35766108e-02\n",
      "Epoch: 121 mean train loss:  8.28342787e-02, mean val. rec. loss:  7.35106995e-02\n",
      "Epoch: 122 mean train loss:  8.27693776e-02, mean val. rec. loss:  7.34452237e-02\n",
      "Epoch: 123 mean train loss:  8.27048043e-02, mean val. rec. loss:  7.33801244e-02\n",
      "Epoch: 124 mean train loss:  8.26405066e-02, mean val. rec. loss:  7.33154152e-02\n",
      "Epoch: 125 mean train loss:  8.25765143e-02, mean val. rec. loss:  7.32510733e-02\n",
      "Epoch: 126 mean train loss:  8.25128050e-02, mean val. rec. loss:  7.31870762e-02\n",
      "Epoch: 127 mean train loss:  8.24493639e-02, mean val. rec. loss:  7.31234465e-02\n",
      "Epoch: 128 mean train loss:  8.23861984e-02, mean val. rec. loss:  7.30601253e-02\n",
      "Epoch: 129 mean train loss:  8.23232862e-02, mean val. rec. loss:  7.29971306e-02\n",
      "Epoch: 130 mean train loss:  8.22606197e-02, mean val. rec. loss:  7.29344399e-02\n",
      "Epoch: 131 mean train loss:  8.21982066e-02, mean val. rec. loss:  7.28720439e-02\n",
      "Epoch: 132 mean train loss:  8.21360243e-02, mean val. rec. loss:  7.28099428e-02\n",
      "Epoch: 133 mean train loss:  8.20740729e-02, mean val. rec. loss:  7.27481094e-02\n",
      "Epoch: 134 mean train loss:  8.20123599e-02, mean val. rec. loss:  7.26865662e-02\n",
      "Epoch: 135 mean train loss:  8.19508481e-02, mean val. rec. loss:  7.26252725e-02\n",
      "Epoch: 136 mean train loss:  8.18895596e-02, mean val. rec. loss:  7.25642237e-02\n",
      "Epoch: 137 mean train loss:  8.18284649e-02, mean val. rec. loss:  7.25034244e-02\n",
      "Epoch: 138 mean train loss:  8.17675862e-02, mean val. rec. loss:  7.24428610e-02\n",
      "Epoch: 139 mean train loss:  8.17069086e-02, mean val. rec. loss:  7.23825244e-02\n",
      "Epoch: 140 mean train loss:  8.16464023e-02, mean val. rec. loss:  7.23224145e-02\n",
      "Epoch: 141 mean train loss:  8.15860971e-02, mean val. rec. loss:  7.22625088e-02\n",
      "Epoch: 142 mean train loss:  8.15259632e-02, mean val. rec. loss:  7.22028118e-02\n",
      "Epoch: 143 mean train loss:  8.14660081e-02, mean val. rec. loss:  7.21433188e-02\n",
      "Epoch: 144 mean train loss:  8.14062318e-02, mean val. rec. loss:  7.20840164e-02\n",
      "Epoch: 145 mean train loss:  8.13466194e-02, mean val. rec. loss:  7.20249044e-02\n",
      "Epoch: 146 mean train loss:  8.12871634e-02, mean val. rec. loss:  7.19659785e-02\n",
      "Epoch: 147 mean train loss:  8.12278638e-02, mean val. rec. loss:  7.19072340e-02\n",
      "Epoch: 148 mean train loss:  8.11687131e-02, mean val. rec. loss:  7.18486573e-02\n",
      "Epoch: 149 mean train loss:  8.11097190e-02, mean val. rec. loss:  7.17902529e-02\n",
      "Epoch: 150 mean train loss:  8.10508588e-02, mean val. rec. loss:  7.17320119e-02\n",
      "Epoch: 151 mean train loss:  8.09921626e-02, mean val. rec. loss:  7.16739251e-02\n",
      "Epoch: 152 mean train loss:  8.09335706e-02, mean val. rec. loss:  7.16159834e-02\n",
      "Epoch: 153 mean train loss:  8.08751276e-02, mean val. rec. loss:  7.15581914e-02\n",
      "Epoch: 154 mean train loss:  8.08168113e-02, mean val. rec. loss:  7.15005355e-02\n",
      "Epoch: 155 mean train loss:  8.07585992e-02, mean val. rec. loss:  7.14430248e-02\n",
      "Epoch: 156 mean train loss:  8.07005287e-02, mean val. rec. loss:  7.13856410e-02\n",
      "Epoch: 157 mean train loss:  8.06425624e-02, mean val. rec. loss:  7.13283797e-02\n",
      "Epoch: 158 mean train loss:  8.05847004e-02, mean val. rec. loss:  7.12712409e-02\n",
      "Epoch: 159 mean train loss:  8.05269427e-02, mean val. rec. loss:  7.12142155e-02\n",
      "Epoch: 160 mean train loss:  8.04692893e-02, mean val. rec. loss:  7.11572853e-02\n",
      "Epoch: 161 mean train loss:  8.04117476e-02, mean val. rec. loss:  7.11004913e-02\n",
      "Epoch: 162 mean train loss:  8.03542879e-02, mean val. rec. loss:  7.10437879e-02\n",
      "Epoch: 163 mean train loss:  8.02969175e-02, mean val. rec. loss:  7.09871798e-02\n",
      "Epoch: 164 mean train loss:  8.02396440e-02, mean val. rec. loss:  7.09306624e-02\n",
      "Epoch: 165 mean train loss:  8.01824375e-02, mean val. rec. loss:  7.08742221e-02\n",
      "Epoch: 166 mean train loss:  8.01253204e-02, mean val. rec. loss:  7.08178816e-02\n",
      "Epoch: 167 mean train loss:  8.00682777e-02, mean val. rec. loss:  7.07616092e-02\n",
      "Epoch: 168 mean train loss:  8.00113022e-02, mean val. rec. loss:  7.07054093e-02\n",
      "Epoch: 169 mean train loss:  7.99544011e-02, mean val. rec. loss:  7.06492774e-02\n",
      "Epoch: 170 mean train loss:  7.98975521e-02, mean val. rec. loss:  7.05932318e-02\n",
      "Epoch: 171 mean train loss:  7.98407776e-02, mean val. rec. loss:  7.05372179e-02\n",
      "Epoch: 172 mean train loss:  7.97840479e-02, mean val. rec. loss:  7.04812629e-02\n",
      "Epoch: 173 mean train loss:  7.97273926e-02, mean val. rec. loss:  7.04253806e-02\n",
      "Epoch: 174 mean train loss:  7.96707671e-02, mean val. rec. loss:  7.03695436e-02\n",
      "Epoch: 175 mean train loss:  7.96142012e-02, mean val. rec. loss:  7.03137383e-02\n",
      "Epoch: 176 mean train loss:  7.95576725e-02, mean val. rec. loss:  7.02579829e-02\n",
      "Epoch: 177 mean train loss:  7.95011886e-02, mean val. rec. loss:  7.02022639e-02\n",
      "Epoch: 178 mean train loss:  7.94447419e-02, mean val. rec. loss:  7.01465811e-02\n",
      "Epoch: 179 mean train loss:  7.93883175e-02, mean val. rec. loss:  7.00909164e-02\n",
      "Epoch: 180 mean train loss:  7.93319378e-02, mean val. rec. loss:  7.00353062e-02\n",
      "Epoch: 181 mean train loss:  7.92755730e-02, mean val. rec. loss:  6.99797096e-02\n",
      "Epoch: 182 mean train loss:  7.92192380e-02, mean val. rec. loss:  6.99241357e-02\n",
      "Epoch: 183 mean train loss:  7.91629254e-02, mean val. rec. loss:  6.98685708e-02\n",
      "Epoch: 184 mean train loss:  7.91066202e-02, mean val. rec. loss:  6.98130241e-02\n",
      "Epoch: 185 mean train loss:  7.90503448e-02, mean val. rec. loss:  6.97575001e-02\n",
      "Epoch: 186 mean train loss:  7.89940694e-02, mean val. rec. loss:  6.97019761e-02\n",
      "Epoch: 187 mean train loss:  7.89378089e-02, mean val. rec. loss:  6.96464657e-02\n",
      "Epoch: 188 mean train loss:  7.88815559e-02, mean val. rec. loss:  6.95909552e-02\n",
      "Epoch: 189 mean train loss:  7.88252954e-02, mean val. rec. loss:  6.95354448e-02\n",
      "Epoch: 190 mean train loss:  7.87690349e-02, mean val. rec. loss:  6.94799298e-02\n",
      "Epoch: 191 mean train loss:  7.87127818e-02, mean val. rec. loss:  6.94244013e-02\n",
      "Epoch: 192 mean train loss:  7.86565213e-02, mean val. rec. loss:  6.93688818e-02\n",
      "Epoch: 193 mean train loss:  7.86002385e-02, mean val. rec. loss:  6.93133487e-02\n",
      "Epoch: 194 mean train loss:  7.85439482e-02, mean val. rec. loss:  6.92577793e-02\n",
      "Epoch: 195 mean train loss:  7.84876504e-02, mean val. rec. loss:  6.92022235e-02\n",
      "Epoch: 196 mean train loss:  7.84313378e-02, mean val. rec. loss:  6.91466269e-02\n",
      "Epoch: 197 mean train loss:  7.83749954e-02, mean val. rec. loss:  6.90910258e-02\n",
      "Epoch: 198 mean train loss:  7.83186380e-02, mean val. rec. loss:  6.90353702e-02\n",
      "Epoch: 199 mean train loss:  7.82622434e-02, mean val. rec. loss:  6.89797146e-02\n",
      "Epoch: 200 mean train loss:  7.82058191e-02, mean val. rec. loss:  6.89240182e-02\n",
      "Epoch: 201 mean train loss:  7.81493723e-02, mean val. rec. loss:  6.88682856e-02\n",
      "Epoch: 202 mean train loss:  7.80928884e-02, mean val. rec. loss:  6.88125211e-02\n",
      "Epoch: 203 mean train loss:  7.80363746e-02, mean val. rec. loss:  6.87567204e-02\n",
      "Epoch: 204 mean train loss:  7.79798087e-02, mean val. rec. loss:  6.87008652e-02\n",
      "Epoch: 205 mean train loss:  7.79232130e-02, mean val. rec. loss:  6.86449738e-02\n",
      "Epoch: 206 mean train loss:  7.78665726e-02, mean val. rec. loss:  6.85890279e-02\n",
      "Epoch: 207 mean train loss:  7.78098727e-02, mean val. rec. loss:  6.85330412e-02\n",
      "Epoch: 208 mean train loss:  7.77531429e-02, mean val. rec. loss:  6.84769911e-02\n",
      "Epoch: 209 mean train loss:  7.76963535e-02, mean val. rec. loss:  6.84209000e-02\n",
      "Epoch: 210 mean train loss:  7.76395195e-02, mean val. rec. loss:  6.83647455e-02\n",
      "Epoch: 211 mean train loss:  7.75826035e-02, mean val. rec. loss:  6.83085366e-02\n",
      "Epoch: 212 mean train loss:  7.75256577e-02, mean val. rec. loss:  6.82522596e-02\n",
      "Epoch: 213 mean train loss:  7.74686300e-02, mean val. rec. loss:  6.81959100e-02\n",
      "Epoch: 214 mean train loss:  7.74115576e-02, mean val. rec. loss:  6.81395015e-02\n",
      "Epoch: 215 mean train loss:  7.73543958e-02, mean val. rec. loss:  6.80830294e-02\n",
      "Epoch: 216 mean train loss:  7.72971893e-02, mean val. rec. loss:  6.80264712e-02\n",
      "Epoch: 217 mean train loss:  7.72398934e-02, mean val. rec. loss:  6.79698495e-02\n",
      "Epoch: 218 mean train loss:  7.71825454e-02, mean val. rec. loss:  6.79131461e-02\n",
      "Epoch: 219 mean train loss:  7.71251080e-02, mean val. rec. loss:  6.78563611e-02\n",
      "Epoch: 220 mean train loss:  7.70676035e-02, mean val. rec. loss:  6.77995081e-02\n",
      "Epoch: 221 mean train loss:  7.70100097e-02, mean val. rec. loss:  6.77425643e-02\n",
      "Epoch: 222 mean train loss:  7.69523488e-02, mean val. rec. loss:  6.76855298e-02\n",
      "Epoch: 223 mean train loss:  7.68945986e-02, mean val. rec. loss:  6.76284182e-02\n",
      "Epoch: 224 mean train loss:  7.68367664e-02, mean val. rec. loss:  6.75712068e-02\n",
      "Epoch: 225 mean train loss:  7.67788448e-02, mean val. rec. loss:  6.75139047e-02\n",
      "Epoch: 226 mean train loss:  7.67208488e-02, mean val. rec. loss:  6.74565164e-02\n",
      "Epoch: 227 mean train loss:  7.66627410e-02, mean val. rec. loss:  6.73990329e-02\n",
      "Epoch: 228 mean train loss:  7.66045438e-02, mean val. rec. loss:  6.73414405e-02\n",
      "Epoch: 229 mean train loss:  7.65462647e-02, mean val. rec. loss:  6.72837528e-02\n",
      "Epoch: 230 mean train loss:  7.64878887e-02, mean val. rec. loss:  6.72259699e-02\n",
      "Epoch: 231 mean train loss:  7.64294085e-02, mean val. rec. loss:  6.71680600e-02\n",
      "Epoch: 232 mean train loss:  7.63708240e-02, mean val. rec. loss:  6.71100594e-02\n",
      "Epoch: 233 mean train loss:  7.63121501e-02, mean val. rec. loss:  6.70519499e-02\n",
      "Epoch: 234 mean train loss:  7.62533645e-02, mean val. rec. loss:  6.69937225e-02\n",
      "Epoch: 235 mean train loss:  7.61944745e-02, mean val. rec. loss:  6.69353907e-02\n",
      "Epoch: 236 mean train loss:  7.61354803e-02, mean val. rec. loss:  6.68769410e-02\n",
      "Epoch: 237 mean train loss:  7.60763744e-02, mean val. rec. loss:  6.68183825e-02\n",
      "Epoch: 238 mean train loss:  7.60171642e-02, mean val. rec. loss:  6.67596969e-02\n",
      "Epoch: 239 mean train loss:  7.59578423e-02, mean val. rec. loss:  6.67008934e-02\n",
      "Epoch: 240 mean train loss:  7.58984012e-02, mean val. rec. loss:  6.66419765e-02\n",
      "Epoch: 241 mean train loss:  7.58388408e-02, mean val. rec. loss:  6.65829326e-02\n",
      "Epoch: 242 mean train loss:  7.57791763e-02, mean val. rec. loss:  6.65237572e-02\n",
      "Epoch: 243 mean train loss:  7.57193851e-02, mean val. rec. loss:  6.64644548e-02\n",
      "Epoch: 244 mean train loss:  7.56594747e-02, mean val. rec. loss:  6.64050344e-02\n",
      "Epoch: 245 mean train loss:  7.55994600e-02, mean val. rec. loss:  6.63454825e-02\n",
      "Epoch: 246 mean train loss:  7.55393112e-02, mean val. rec. loss:  6.62857854e-02\n",
      "Epoch: 247 mean train loss:  7.54790358e-02, mean val. rec. loss:  6.62259704e-02\n",
      "Epoch: 248 mean train loss:  7.54186413e-02, mean val. rec. loss:  6.61660012e-02\n",
      "Epoch: 249 mean train loss:  7.53581126e-02, mean val. rec. loss:  6.61059231e-02\n",
      "Epoch: 250 mean train loss:  7.52974648e-02, mean val. rec. loss:  6.60456772e-02\n",
      "Epoch: 251 mean train loss:  7.52366680e-02, mean val. rec. loss:  6.59853179e-02\n",
      "Epoch: 252 mean train loss:  7.51757669e-02, mean val. rec. loss:  6.59247908e-02\n",
      "Epoch: 253 mean train loss:  7.51147094e-02, mean val. rec. loss:  6.58641638e-02\n",
      "Epoch: 254 mean train loss:  7.50535328e-02, mean val. rec. loss:  6.58033555e-02\n",
      "Epoch: 255 mean train loss:  7.49922295e-02, mean val. rec. loss:  6.57424156e-02\n",
      "Epoch: 256 mean train loss:  7.49307772e-02, mean val. rec. loss:  6.56813214e-02\n",
      "Epoch: 257 mean train loss:  7.48691908e-02, mean val. rec. loss:  6.56200958e-02\n",
      "Epoch: 258 mean train loss:  7.48074704e-02, mean val. rec. loss:  6.55587159e-02\n",
      "Epoch: 259 mean train loss:  7.47456084e-02, mean val. rec. loss:  6.54971682e-02\n",
      "Epoch: 260 mean train loss:  7.46836123e-02, mean val. rec. loss:  6.54354753e-02\n",
      "Epoch: 261 mean train loss:  7.46214599e-02, mean val. rec. loss:  6.53736418e-02\n",
      "Epoch: 262 mean train loss:  7.45591882e-02, mean val. rec. loss:  6.53116541e-02\n",
      "Epoch: 263 mean train loss:  7.44967601e-02, mean val. rec. loss:  6.52495032e-02\n",
      "Epoch: 264 mean train loss:  7.44341831e-02, mean val. rec. loss:  6.51871979e-02\n",
      "Epoch: 265 mean train loss:  7.43714720e-02, mean val. rec. loss:  6.51247294e-02\n",
      "Epoch: 266 mean train loss:  7.43086044e-02, mean val. rec. loss:  6.50621067e-02\n",
      "Epoch: 267 mean train loss:  7.42455953e-02, mean val. rec. loss:  6.49993388e-02\n",
      "Epoch: 268 mean train loss:  7.41824373e-02, mean val. rec. loss:  6.49363895e-02\n",
      "Epoch: 269 mean train loss:  7.41191303e-02, mean val. rec. loss:  6.48732905e-02\n",
      "Epoch: 270 mean train loss:  7.40556743e-02, mean val. rec. loss:  6.48100283e-02\n",
      "Epoch: 271 mean train loss:  7.39920693e-02, mean val. rec. loss:  6.47465891e-02\n",
      "Epoch: 272 mean train loss:  7.39283004e-02, mean val. rec. loss:  6.46830138e-02\n",
      "Epoch: 273 mean train loss:  7.38643975e-02, mean val. rec. loss:  6.46192435e-02\n",
      "Epoch: 274 mean train loss:  7.38003381e-02, mean val. rec. loss:  6.45553190e-02\n",
      "Epoch: 275 mean train loss:  7.37361149e-02, mean val. rec. loss:  6.44912357e-02\n",
      "Epoch: 276 mean train loss:  7.36717427e-02, mean val. rec. loss:  6.44269710e-02\n",
      "Epoch: 277 mean train loss:  7.36072141e-02, mean val. rec. loss:  6.43625384e-02\n",
      "Epoch: 278 mean train loss:  7.35425365e-02, mean val. rec. loss:  6.42979426e-02\n",
      "Epoch: 279 mean train loss:  7.34776950e-02, mean val. rec. loss:  6.42331834e-02\n",
      "Epoch: 280 mean train loss:  7.34126897e-02, mean val. rec. loss:  6.41682338e-02\n",
      "Epoch: 281 mean train loss:  7.33475428e-02, mean val. rec. loss:  6.41031163e-02\n",
      "Epoch: 282 mean train loss:  7.32822246e-02, mean val. rec. loss:  6.40378356e-02\n",
      "Epoch: 283 mean train loss:  7.32167500e-02, mean val. rec. loss:  6.39723824e-02\n",
      "Epoch: 284 mean train loss:  7.31511338e-02, mean val. rec. loss:  6.39067388e-02\n",
      "Epoch: 285 mean train loss:  7.30853315e-02, mean val. rec. loss:  6.38409455e-02\n",
      "Epoch: 286 mean train loss:  7.30193950e-02, mean val. rec. loss:  6.37749662e-02\n",
      "Epoch: 287 mean train loss:  7.29532798e-02, mean val. rec. loss:  6.37088100e-02\n",
      "Epoch: 288 mean train loss:  7.28870007e-02, mean val. rec. loss:  6.36424860e-02\n",
      "Epoch: 289 mean train loss:  7.28205727e-02, mean val. rec. loss:  6.35759714e-02\n",
      "Epoch: 290 mean train loss:  7.27539658e-02, mean val. rec. loss:  6.35093027e-02\n",
      "Epoch: 291 mean train loss:  7.26872100e-02, mean val. rec. loss:  6.34424253e-02\n",
      "Epoch: 292 mean train loss:  7.26202978e-02, mean val. rec. loss:  6.33753982e-02\n",
      "Epoch: 293 mean train loss:  7.25532143e-02, mean val. rec. loss:  6.33081624e-02\n",
      "Epoch: 294 mean train loss:  7.24859668e-02, mean val. rec. loss:  6.32407816e-02\n",
      "Epoch: 295 mean train loss:  7.24185630e-02, mean val. rec. loss:  6.31731920e-02\n",
      "Epoch: 296 mean train loss:  7.23509878e-02, mean val. rec. loss:  6.31054528e-02\n",
      "Epoch: 297 mean train loss:  7.22832562e-02, mean val. rec. loss:  6.30374958e-02\n",
      "Epoch: 298 mean train loss:  7.22153608e-02, mean val. rec. loss:  6.29693892e-02\n",
      "Epoch: 299 mean train loss:  7.21472940e-02, mean val. rec. loss:  6.29010921e-02\n",
      "Epoch: 300 mean train loss:  7.20790708e-02, mean val. rec. loss:  6.28326271e-02\n",
      "Epoch: 301 mean train loss:  7.20106762e-02, mean val. rec. loss:  6.27639535e-02\n",
      "Epoch: 302 mean train loss:  7.19421253e-02, mean val. rec. loss:  6.26951256e-02\n",
      "Epoch: 303 mean train loss:  7.18734030e-02, mean val. rec. loss:  6.26261073e-02\n",
      "Epoch: 304 mean train loss:  7.18045168e-02, mean val. rec. loss:  6.25569256e-02\n",
      "Epoch: 305 mean train loss:  7.17354743e-02, mean val. rec. loss:  6.24875308e-02\n",
      "Epoch: 306 mean train loss:  7.16662678e-02, mean val. rec. loss:  6.24179636e-02\n",
      "Epoch: 307 mean train loss:  7.15968900e-02, mean val. rec. loss:  6.23482376e-02\n",
      "Epoch: 308 mean train loss:  7.15273484e-02, mean val. rec. loss:  6.22783121e-02\n",
      "Epoch: 309 mean train loss:  7.14576578e-02, mean val. rec. loss:  6.22082233e-02\n",
      "Epoch: 310 mean train loss:  7.13877884e-02, mean val. rec. loss:  6.21379349e-02\n",
      "Epoch: 311 mean train loss:  7.13177551e-02, mean val. rec. loss:  6.20674696e-02\n",
      "Epoch: 312 mean train loss:  7.12475729e-02, mean val. rec. loss:  6.19968228e-02\n",
      "Epoch: 313 mean train loss:  7.11772119e-02, mean val. rec. loss:  6.19260037e-02\n",
      "Epoch: 314 mean train loss:  7.11067094e-02, mean val. rec. loss:  6.18549986e-02\n",
      "Epoch: 315 mean train loss:  7.10360206e-02, mean val. rec. loss:  6.17838167e-02\n",
      "Epoch: 316 mean train loss:  7.09651903e-02, mean val. rec. loss:  6.17124578e-02\n",
      "Epoch: 317 mean train loss:  7.08941887e-02, mean val. rec. loss:  6.16409175e-02\n",
      "Epoch: 318 mean train loss:  7.08230382e-02, mean val. rec. loss:  6.15692048e-02\n",
      "Epoch: 319 mean train loss:  7.07517163e-02, mean val. rec. loss:  6.14973061e-02\n",
      "Epoch: 320 mean train loss:  7.06802454e-02, mean val. rec. loss:  6.14252261e-02\n",
      "Epoch: 321 mean train loss:  7.06086032e-02, mean val. rec. loss:  6.13529872e-02\n",
      "Epoch: 322 mean train loss:  7.05368195e-02, mean val. rec. loss:  6.12805579e-02\n",
      "Epoch: 323 mean train loss:  7.04648644e-02, mean val. rec. loss:  6.12079289e-02\n",
      "Epoch: 324 mean train loss:  7.03927604e-02, mean val. rec. loss:  6.11351503e-02\n",
      "Epoch: 325 mean train loss:  7.03205000e-02, mean val. rec. loss:  6.10622129e-02\n",
      "Epoch: 326 mean train loss:  7.02480831e-02, mean val. rec. loss:  6.09890805e-02\n",
      "Epoch: 327 mean train loss:  7.01755098e-02, mean val. rec. loss:  6.09157803e-02\n",
      "Epoch: 328 mean train loss:  7.01027876e-02, mean val. rec. loss:  6.08423077e-02\n",
      "Epoch: 329 mean train loss:  7.00299089e-02, mean val. rec. loss:  6.07686627e-02\n",
      "Epoch: 330 mean train loss:  6.99568812e-02, mean val. rec. loss:  6.06948453e-02\n",
      "Epoch: 331 mean train loss:  6.98837046e-02, mean val. rec. loss:  6.06208556e-02\n",
      "Epoch: 332 mean train loss:  6.98103790e-02, mean val. rec. loss:  6.05466845e-02\n",
      "Epoch: 333 mean train loss:  6.97368895e-02, mean val. rec. loss:  6.04723546e-02\n",
      "Epoch: 334 mean train loss:  6.96632808e-02, mean val. rec. loss:  6.03978795e-02\n",
      "Epoch: 335 mean train loss:  6.95895083e-02, mean val. rec. loss:  6.03232185e-02\n",
      "Epoch: 336 mean train loss:  6.95155867e-02, mean val. rec. loss:  6.02483942e-02\n",
      "Epoch: 337 mean train loss:  6.94415312e-02, mean val. rec. loss:  6.01734157e-02\n",
      "Epoch: 338 mean train loss:  6.93673340e-02, mean val. rec. loss:  6.00982512e-02\n",
      "Epoch: 339 mean train loss:  6.92929954e-02, mean val. rec. loss:  6.00229325e-02\n",
      "Epoch: 340 mean train loss:  6.92185152e-02, mean val. rec. loss:  5.99474550e-02\n",
      "Epoch: 341 mean train loss:  6.91438935e-02, mean val. rec. loss:  5.98718233e-02\n",
      "Epoch: 342 mean train loss:  6.90691377e-02, mean val. rec. loss:  5.97960283e-02\n",
      "Epoch: 343 mean train loss:  6.89942479e-02, mean val. rec. loss:  5.97200700e-02\n",
      "Epoch: 344 mean train loss:  6.89192165e-02, mean val. rec. loss:  5.96439756e-02\n",
      "Epoch: 345 mean train loss:  6.88440659e-02, mean val. rec. loss:  5.95677225e-02\n",
      "Epoch: 346 mean train loss:  6.87687739e-02, mean val. rec. loss:  5.94913061e-02\n",
      "Epoch: 347 mean train loss:  6.86933551e-02, mean val. rec. loss:  5.94147581e-02\n",
      "Epoch: 348 mean train loss:  6.86178098e-02, mean val. rec. loss:  5.93380378e-02\n",
      "Epoch: 349 mean train loss:  6.85421378e-02, mean val. rec. loss:  5.92611905e-02\n",
      "Epoch: 350 mean train loss:  6.84663541e-02, mean val. rec. loss:  5.91841753e-02\n",
      "Epoch: 351 mean train loss:  6.83904363e-02, mean val. rec. loss:  5.91070286e-02\n",
      "Epoch: 352 mean train loss:  6.83143994e-02, mean val. rec. loss:  5.90297413e-02\n",
      "Epoch: 353 mean train loss:  6.82382581e-02, mean val. rec. loss:  5.89522998e-02\n",
      "Epoch: 354 mean train loss:  6.81619977e-02, mean val. rec. loss:  5.88747585e-02\n",
      "Epoch: 355 mean train loss:  6.80856032e-02, mean val. rec. loss:  5.87970403e-02\n",
      "Epoch: 356 mean train loss:  6.80091193e-02, mean val. rec. loss:  5.87192041e-02\n",
      "Epoch: 357 mean train loss:  6.79325162e-02, mean val. rec. loss:  5.86412500e-02\n",
      "Epoch: 358 mean train loss:  6.78558089e-02, mean val. rec. loss:  5.85631326e-02\n",
      "Epoch: 359 mean train loss:  6.77789973e-02, mean val. rec. loss:  5.84848882e-02\n",
      "Epoch: 360 mean train loss:  6.77020813e-02, mean val. rec. loss:  5.84065531e-02\n",
      "Epoch: 361 mean train loss:  6.76250760e-02, mean val. rec. loss:  5.83280684e-02\n",
      "Epoch: 362 mean train loss:  6.75479516e-02, mean val. rec. loss:  5.82494656e-02\n",
      "Epoch: 363 mean train loss:  6.74707526e-02, mean val. rec. loss:  5.81707586e-02\n",
      "Epoch: 364 mean train loss:  6.73934568e-02, mean val. rec. loss:  5.80919155e-02\n",
      "Epoch: 365 mean train loss:  6.73160567e-02, mean val. rec. loss:  5.80129816e-02\n",
      "Epoch: 366 mean train loss:  6.72385896e-02, mean val. rec. loss:  5.79339072e-02\n",
      "Epoch: 367 mean train loss:  6.71610182e-02, mean val. rec. loss:  5.78547329e-02\n",
      "Epoch: 368 mean train loss:  6.70833872e-02, mean val. rec. loss:  5.77754634e-02\n",
      "Epoch: 369 mean train loss:  6.70056593e-02, mean val. rec. loss:  5.76960987e-02\n",
      "Epoch: 370 mean train loss:  6.69278570e-02, mean val. rec. loss:  5.76166069e-02\n",
      "Epoch: 371 mean train loss:  6.68499877e-02, mean val. rec. loss:  5.75370244e-02\n",
      "Epoch: 372 mean train loss:  6.67720438e-02, mean val. rec. loss:  5.74573558e-02\n",
      "Epoch: 373 mean train loss:  6.66940329e-02, mean val. rec. loss:  5.73775828e-02\n",
      "Epoch: 374 mean train loss:  6.66159475e-02, mean val. rec. loss:  5.72977236e-02\n",
      "Epoch: 375 mean train loss:  6.65378175e-02, mean val. rec. loss:  5.72177692e-02\n",
      "Epoch: 376 mean train loss:  6.64596129e-02, mean val. rec. loss:  5.71377513e-02\n",
      "Epoch: 377 mean train loss:  6.63813711e-02, mean val. rec. loss:  5.70576472e-02\n",
      "Epoch: 378 mean train loss:  6.63030549e-02, mean val. rec. loss:  5.69774614e-02\n",
      "Epoch: 379 mean train loss:  6.62247013e-02, mean val. rec. loss:  5.68971849e-02\n",
      "Epoch: 380 mean train loss:  6.61463031e-02, mean val. rec. loss:  5.68168631e-02\n",
      "Epoch: 381 mean train loss:  6.60678527e-02, mean val. rec. loss:  5.67364596e-02\n",
      "Epoch: 382 mean train loss:  6.59893651e-02, mean val. rec. loss:  5.66559836e-02\n",
      "Epoch: 383 mean train loss:  6.59108328e-02, mean val. rec. loss:  5.65754531e-02\n",
      "Epoch: 384 mean train loss:  6.58322782e-02, mean val. rec. loss:  5.64948500e-02\n",
      "Epoch: 385 mean train loss:  6.57536938e-02, mean val. rec. loss:  5.64142107e-02\n",
      "Epoch: 386 mean train loss:  6.56750795e-02, mean val. rec. loss:  5.63335124e-02\n",
      "Epoch: 387 mean train loss:  6.55964355e-02, mean val. rec. loss:  5.62527596e-02\n",
      "Epoch: 388 mean train loss:  6.55177766e-02, mean val. rec. loss:  5.61719434e-02\n",
      "Epoch: 389 mean train loss:  6.54391176e-02, mean val. rec. loss:  5.60911181e-02\n",
      "Epoch: 390 mean train loss:  6.53604289e-02, mean val. rec. loss:  5.60102429e-02\n",
      "Epoch: 391 mean train loss:  6.52817253e-02, mean val. rec. loss:  5.59293223e-02\n",
      "Epoch: 392 mean train loss:  6.52030217e-02, mean val. rec. loss:  5.58483609e-02\n",
      "Epoch: 393 mean train loss:  6.51243181e-02, mean val. rec. loss:  5.57673995e-02\n",
      "Epoch: 394 mean train loss:  6.50456144e-02, mean val. rec. loss:  5.56863973e-02\n",
      "Epoch: 395 mean train loss:  6.49669108e-02, mean val. rec. loss:  5.56053860e-02\n",
      "Epoch: 396 mean train loss:  6.48882072e-02, mean val. rec. loss:  5.55243430e-02\n",
      "Epoch: 397 mean train loss:  6.48095259e-02, mean val. rec. loss:  5.54432727e-02\n",
      "Epoch: 398 mean train loss:  6.47308595e-02, mean val. rec. loss:  5.53622070e-02\n",
      "Epoch: 399 mean train loss:  6.46522006e-02, mean val. rec. loss:  5.52811322e-02\n",
      "Epoch: 400 mean train loss:  6.45735789e-02, mean val. rec. loss:  5.52000483e-02\n",
      "Epoch: 401 mean train loss:  6.44949721e-02, mean val. rec. loss:  5.51189826e-02\n",
      "Epoch: 402 mean train loss:  6.44163877e-02, mean val. rec. loss:  5.50378897e-02\n",
      "Epoch: 403 mean train loss:  6.43378480e-02, mean val. rec. loss:  5.49568240e-02\n",
      "Epoch: 404 mean train loss:  6.42593529e-02, mean val. rec. loss:  5.48757583e-02\n",
      "Epoch: 405 mean train loss:  6.41808727e-02, mean val. rec. loss:  5.47947016e-02\n",
      "Epoch: 406 mean train loss:  6.41024447e-02, mean val. rec. loss:  5.47136540e-02\n",
      "Epoch: 407 mean train loss:  6.40240614e-02, mean val. rec. loss:  5.46326337e-02\n",
      "Epoch: 408 mean train loss:  6.39457302e-02, mean val. rec. loss:  5.45516496e-02\n",
      "Epoch: 409 mean train loss:  6.38674437e-02, mean val. rec. loss:  5.44706837e-02\n",
      "Epoch: 410 mean train loss:  6.37892168e-02, mean val. rec. loss:  5.43897404e-02\n",
      "Epoch: 411 mean train loss:  6.37110495e-02, mean val. rec. loss:  5.43088335e-02\n",
      "Epoch: 412 mean train loss:  6.36329418e-02, mean val. rec. loss:  5.42279673e-02\n",
      "Epoch: 413 mean train loss:  6.35549011e-02, mean val. rec. loss:  5.41471375e-02\n",
      "Epoch: 414 mean train loss:  6.34769349e-02, mean val. rec. loss:  5.40663439e-02\n",
      "Epoch: 415 mean train loss:  6.33990283e-02, mean val. rec. loss:  5.39856138e-02\n",
      "Epoch: 416 mean train loss:  6.33211962e-02, mean val. rec. loss:  5.39049291e-02\n",
      "Epoch: 417 mean train loss:  6.32434460e-02, mean val. rec. loss:  5.38242807e-02\n",
      "Epoch: 418 mean train loss:  6.31657852e-02, mean val. rec. loss:  5.37437321e-02\n",
      "Epoch: 419 mean train loss:  6.30881840e-02, mean val. rec. loss:  5.36631925e-02\n",
      "Epoch: 420 mean train loss:  6.30106871e-02, mean val. rec. loss:  5.35827392e-02\n",
      "Epoch: 421 mean train loss:  6.29332796e-02, mean val. rec. loss:  5.35023221e-02\n",
      "Epoch: 422 mean train loss:  6.28559540e-02, mean val. rec. loss:  5.34220184e-02\n",
      "Epoch: 423 mean train loss:  6.27787252e-02, mean val. rec. loss:  5.33417555e-02\n",
      "Epoch: 424 mean train loss:  6.27015933e-02, mean val. rec. loss:  5.32615743e-02\n",
      "Epoch: 425 mean train loss:  6.26245731e-02, mean val. rec. loss:  5.31814611e-02\n",
      "Epoch: 426 mean train loss:  6.25476274e-02, mean val. rec. loss:  5.31014342e-02\n",
      "Epoch: 427 mean train loss:  6.24708009e-02, mean val. rec. loss:  5.30214979e-02\n",
      "Epoch: 428 mean train loss:  6.23940972e-02, mean val. rec. loss:  5.29416387e-02\n",
      "Epoch: 429 mean train loss:  6.23174830e-02, mean val. rec. loss:  5.28618612e-02\n",
      "Epoch: 430 mean train loss:  6.22409767e-02, mean val. rec. loss:  5.27821789e-02\n",
      "Epoch: 431 mean train loss:  6.21645897e-02, mean val. rec. loss:  5.27025783e-02\n",
      "Epoch: 432 mean train loss:  6.20883218e-02, mean val. rec. loss:  5.26230865e-02\n",
      "Epoch: 433 mean train loss:  6.20121582e-02, mean val. rec. loss:  5.25436764e-02\n",
      "Epoch: 434 mean train loss:  6.19361213e-02, mean val. rec. loss:  5.24643706e-02\n",
      "Epoch: 435 mean train loss:  6.18602109e-02, mean val. rec. loss:  5.23851692e-02\n",
      "Epoch: 436 mean train loss:  6.17844049e-02, mean val. rec. loss:  5.23060675e-02\n",
      "Epoch: 437 mean train loss:  6.17087329e-02, mean val. rec. loss:  5.22270611e-02\n",
      "Epoch: 438 mean train loss:  6.16331801e-02, mean val. rec. loss:  5.21481635e-02\n",
      "Epoch: 439 mean train loss:  6.15577539e-02, mean val. rec. loss:  5.20693748e-02\n",
      "Epoch: 440 mean train loss:  6.14824581e-02, mean val. rec. loss:  5.19906859e-02\n",
      "Epoch: 441 mean train loss:  6.14072964e-02, mean val. rec. loss:  5.19121286e-02\n",
      "Epoch: 442 mean train loss:  6.13322538e-02, mean val. rec. loss:  5.18336619e-02\n",
      "Epoch: 443 mean train loss:  6.12573454e-02, mean val. rec. loss:  5.17553314e-02\n",
      "Epoch: 444 mean train loss:  6.11825598e-02, mean val. rec. loss:  5.16771051e-02\n",
      "Epoch: 445 mean train loss:  6.11079158e-02, mean val. rec. loss:  5.15989741e-02\n",
      "Epoch: 446 mean train loss:  6.10334058e-02, mean val. rec. loss:  5.15210019e-02\n",
      "Epoch: 447 mean train loss:  6.09590225e-02, mean val. rec. loss:  5.14431204e-02\n",
      "Epoch: 448 mean train loss:  6.08847769e-02, mean val. rec. loss:  5.13653795e-02\n",
      "Epoch: 449 mean train loss:  6.08106580e-02, mean val. rec. loss:  5.12877293e-02\n",
      "Epoch: 450 mean train loss:  6.07366844e-02, mean val. rec. loss:  5.12102016e-02\n",
      "Epoch: 451 mean train loss:  6.06628411e-02, mean val. rec. loss:  5.11328372e-02\n",
      "Epoch: 452 mean train loss:  6.05891244e-02, mean val. rec. loss:  5.10555453e-02\n",
      "Epoch: 453 mean train loss:  6.05155530e-02, mean val. rec. loss:  5.09783850e-02\n",
      "Epoch: 454 mean train loss:  6.04421156e-02, mean val. rec. loss:  5.09013699e-02\n",
      "Epoch: 455 mean train loss:  6.03688049e-02, mean val. rec. loss:  5.08244681e-02\n",
      "Epoch: 456 mean train loss:  6.02956394e-02, mean val. rec. loss:  5.07476934e-02\n",
      "Epoch: 457 mean train loss:  6.02226080e-02, mean val. rec. loss:  5.06710502e-02\n",
      "Epoch: 458 mean train loss:  6.01497033e-02, mean val. rec. loss:  5.05945158e-02\n",
      "Epoch: 459 mean train loss:  6.00769326e-02, mean val. rec. loss:  5.05181085e-02\n",
      "Epoch: 460 mean train loss:  6.00043072e-02, mean val. rec. loss:  5.04418372e-02\n",
      "Epoch: 461 mean train loss:  5.99318009e-02, mean val. rec. loss:  5.03656793e-02\n",
      "Epoch: 462 mean train loss:  5.98594250e-02, mean val. rec. loss:  5.02896666e-02\n",
      "Epoch: 463 mean train loss:  5.97871869e-02, mean val. rec. loss:  5.02137628e-02\n",
      "Epoch: 464 mean train loss:  5.97150792e-02, mean val. rec. loss:  5.01379678e-02\n",
      "Epoch: 465 mean train loss:  5.96431018e-02, mean val. rec. loss:  5.00623179e-02\n",
      "Epoch: 466 mean train loss:  5.95712510e-02, mean val. rec. loss:  4.99867724e-02\n",
      "Epoch: 467 mean train loss:  5.94995306e-02, mean val. rec. loss:  4.99113766e-02\n",
      "Epoch: 468 mean train loss:  5.94279294e-02, mean val. rec. loss:  4.98360941e-02\n",
      "Epoch: 469 mean train loss:  5.93564623e-02, mean val. rec. loss:  4.97609523e-02\n",
      "Epoch: 470 mean train loss:  5.92851143e-02, mean val. rec. loss:  4.96859012e-02\n",
      "Epoch: 471 mean train loss:  5.92139004e-02, mean val. rec. loss:  4.96110043e-02\n",
      "Epoch: 472 mean train loss:  5.91427983e-02, mean val. rec. loss:  4.95361981e-02\n",
      "Epoch: 473 mean train loss:  5.90718153e-02, mean val. rec. loss:  4.94615280e-02\n",
      "Epoch: 474 mean train loss:  5.90009589e-02, mean val. rec. loss:  4.93869713e-02\n",
      "Epoch: 475 mean train loss:  5.89302255e-02, mean val. rec. loss:  4.93125235e-02\n",
      "Epoch: 476 mean train loss:  5.88596038e-02, mean val. rec. loss:  4.92382254e-02\n",
      "Epoch: 477 mean train loss:  5.87890938e-02, mean val. rec. loss:  4.91640225e-02\n",
      "Epoch: 478 mean train loss:  5.87187067e-02, mean val. rec. loss:  4.90899375e-02\n",
      "Epoch: 479 mean train loss:  5.86484351e-02, mean val. rec. loss:  4.90159932e-02\n",
      "Epoch: 480 mean train loss:  5.85782640e-02, mean val. rec. loss:  4.89421395e-02\n",
      "Epoch: 481 mean train loss:  5.85082159e-02, mean val. rec. loss:  4.88684220e-02\n",
      "Epoch: 482 mean train loss:  5.84382720e-02, mean val. rec. loss:  4.87948088e-02\n",
      "Epoch: 483 mean train loss:  5.83684361e-02, mean val. rec. loss:  4.87213089e-02\n",
      "Epoch: 484 mean train loss:  5.82987045e-02, mean val. rec. loss:  4.86479134e-02\n",
      "Epoch: 485 mean train loss:  5.82290772e-02, mean val. rec. loss:  4.85746586e-02\n",
      "Epoch: 486 mean train loss:  5.81595542e-02, mean val. rec. loss:  4.85014944e-02\n",
      "Epoch: 487 mean train loss:  5.80901317e-02, mean val. rec. loss:  4.84284391e-02\n",
      "Epoch: 488 mean train loss:  5.80208136e-02, mean val. rec. loss:  4.83555017e-02\n",
      "Epoch: 489 mean train loss:  5.79515997e-02, mean val. rec. loss:  4.82826913e-02\n",
      "Epoch: 490 mean train loss:  5.78824752e-02, mean val. rec. loss:  4.82099490e-02\n",
      "Epoch: 491 mean train loss:  5.78134587e-02, mean val. rec. loss:  4.81373291e-02\n",
      "Epoch: 492 mean train loss:  5.77445241e-02, mean val. rec. loss:  4.80648136e-02\n",
      "Epoch: 493 mean train loss:  5.76756901e-02, mean val. rec. loss:  4.79924251e-02\n",
      "Epoch: 494 mean train loss:  5.76069566e-02, mean val. rec. loss:  4.79201091e-02\n",
      "Epoch: 495 mean train loss:  5.75383126e-02, mean val. rec. loss:  4.78479338e-02\n",
      "Epoch: 496 mean train loss:  5.74697504e-02, mean val. rec. loss:  4.77758401e-02\n",
      "Epoch: 497 mean train loss:  5.74012777e-02, mean val. rec. loss:  4.77038825e-02\n",
      "Epoch: 498 mean train loss:  5.73329018e-02, mean val. rec. loss:  4.76320020e-02\n",
      "Epoch: 499 mean train loss:  5.72646078e-02, mean val. rec. loss:  4.75601940e-02\n",
      "Epoch: 500 mean train loss:  5.71964069e-02, mean val. rec. loss:  4.74885086e-02\n",
      "Epoch: 501 mean train loss:  5.71282880e-02, mean val. rec. loss:  4.74169728e-02\n",
      "Epoch: 502 mean train loss:  5.70602473e-02, mean val. rec. loss:  4.73454914e-02\n",
      "Epoch: 503 mean train loss:  5.69922922e-02, mean val. rec. loss:  4.72740872e-02\n",
      "Epoch: 504 mean train loss:  5.69244303e-02, mean val. rec. loss:  4.72028100e-02\n",
      "Epoch: 505 mean train loss:  5.68566391e-02, mean val. rec. loss:  4.71316779e-02\n",
      "Epoch: 506 mean train loss:  5.67889187e-02, mean val. rec. loss:  4.70605866e-02\n",
      "Epoch: 507 mean train loss:  5.67212951e-02, mean val. rec. loss:  4.69896042e-02\n",
      "Epoch: 508 mean train loss:  5.66537385e-02, mean val. rec. loss:  4.69187352e-02\n",
      "Epoch: 509 mean train loss:  5.65862714e-02, mean val. rec. loss:  4.68479433e-02\n",
      "Epoch: 510 mean train loss:  5.65188750e-02, mean val. rec. loss:  4.67772603e-02\n",
      "Epoch: 511 mean train loss:  5.64515531e-02, mean val. rec. loss:  4.67066952e-02\n",
      "Epoch: 512 mean train loss:  5.63843056e-02, mean val. rec. loss:  4.66362072e-02\n",
      "Epoch: 513 mean train loss:  5.63171402e-02, mean val. rec. loss:  4.65658009e-02\n",
      "Epoch: 514 mean train loss:  5.62500529e-02, mean val. rec. loss:  4.64955079e-02\n",
      "Epoch: 515 mean train loss:  5.61830401e-02, mean val. rec. loss:  4.64252966e-02\n",
      "Epoch: 516 mean train loss:  5.61160981e-02, mean val. rec. loss:  4.63551987e-02\n",
      "Epoch: 517 mean train loss:  5.60492305e-02, mean val. rec. loss:  4.62852097e-02\n",
      "Epoch: 518 mean train loss:  5.59824300e-02, mean val. rec. loss:  4.62153068e-02\n",
      "Epoch: 519 mean train loss:  5.59157152e-02, mean val. rec. loss:  4.61455083e-02\n",
      "Epoch: 520 mean train loss:  5.58490637e-02, mean val. rec. loss:  4.60757914e-02\n",
      "Epoch: 521 mean train loss:  5.57824904e-02, mean val. rec. loss:  4.60061834e-02\n",
      "Epoch: 522 mean train loss:  5.57159878e-02, mean val. rec. loss:  4.59366661e-02\n",
      "Epoch: 523 mean train loss:  5.56495560e-02, mean val. rec. loss:  4.58672486e-02\n",
      "Epoch: 524 mean train loss:  5.55831987e-02, mean val. rec. loss:  4.57979218e-02\n",
      "Epoch: 525 mean train loss:  5.55169122e-02, mean val. rec. loss:  4.57287039e-02\n",
      "Epoch: 526 mean train loss:  5.54507002e-02, mean val. rec. loss:  4.56595721e-02\n",
      "Epoch: 527 mean train loss:  5.53845701e-02, mean val. rec. loss:  4.55905674e-02\n",
      "Epoch: 528 mean train loss:  5.53184995e-02, mean val. rec. loss:  4.55216443e-02\n",
      "Epoch: 529 mean train loss:  5.52525109e-02, mean val. rec. loss:  4.54528210e-02\n",
      "Epoch: 530 mean train loss:  5.51865857e-02, mean val. rec. loss:  4.53840929e-02\n",
      "Epoch: 531 mean train loss:  5.51207423e-02, mean val. rec. loss:  4.53154692e-02\n",
      "Epoch: 532 mean train loss:  5.50549661e-02, mean val. rec. loss:  4.52469498e-02\n",
      "Epoch: 533 mean train loss:  5.49892717e-02, mean val. rec. loss:  4.51785257e-02\n",
      "Epoch: 534 mean train loss:  5.49236556e-02, mean val. rec. loss:  4.51101922e-02\n",
      "Epoch: 535 mean train loss:  5.48581064e-02, mean val. rec. loss:  4.50419903e-02\n",
      "Epoch: 536 mean train loss:  5.47926393e-02, mean val. rec. loss:  4.49738792e-02\n",
      "Epoch: 537 mean train loss:  5.47272429e-02, mean val. rec. loss:  4.49058723e-02\n",
      "Epoch: 538 mean train loss:  5.46619210e-02, mean val. rec. loss:  4.48379653e-02\n",
      "Epoch: 539 mean train loss:  5.45966772e-02, mean val. rec. loss:  4.47701671e-02\n",
      "Epoch: 540 mean train loss:  5.45315080e-02, mean val. rec. loss:  4.47024687e-02\n",
      "Epoch: 541 mean train loss:  5.44664133e-02, mean val. rec. loss:  4.46348882e-02\n",
      "Epoch: 542 mean train loss:  5.44014154e-02, mean val. rec. loss:  4.45674030e-02\n",
      "Epoch: 543 mean train loss:  5.43364883e-02, mean val. rec. loss:  4.45000448e-02\n",
      "Epoch: 544 mean train loss:  5.42716393e-02, mean val. rec. loss:  4.44327909e-02\n",
      "Epoch: 545 mean train loss:  5.42068612e-02, mean val. rec. loss:  4.43656413e-02\n",
      "Epoch: 546 mean train loss:  5.41421724e-02, mean val. rec. loss:  4.42985961e-02\n",
      "Epoch: 547 mean train loss:  5.40775730e-02, mean val. rec. loss:  4.42316688e-02\n",
      "Epoch: 548 mean train loss:  5.40130407e-02, mean val. rec. loss:  4.41648413e-02\n",
      "Epoch: 549 mean train loss:  5.39485977e-02, mean val. rec. loss:  4.40981453e-02\n",
      "Epoch: 550 mean train loss:  5.38842478e-02, mean val. rec. loss:  4.40315673e-02\n",
      "Epoch: 551 mean train loss:  5.38199650e-02, mean val. rec. loss:  4.39650664e-02\n",
      "Epoch: 552 mean train loss:  5.37557828e-02, mean val. rec. loss:  4.38987015e-02\n",
      "Epoch: 553 mean train loss:  5.36916750e-02, mean val. rec. loss:  4.38324637e-02\n",
      "Epoch: 554 mean train loss:  5.36276715e-02, mean val. rec. loss:  4.37663302e-02\n",
      "Epoch: 555 mean train loss:  5.35637351e-02, mean val. rec. loss:  4.37003282e-02\n",
      "Epoch: 556 mean train loss:  5.34999029e-02, mean val. rec. loss:  4.36344124e-02\n",
      "Epoch: 557 mean train loss:  5.34361527e-02, mean val. rec. loss:  4.35686780e-02\n",
      "Epoch: 558 mean train loss:  5.33724881e-02, mean val. rec. loss:  4.35030163e-02\n",
      "Epoch: 559 mean train loss:  5.33089204e-02, mean val. rec. loss:  4.34374815e-02\n",
      "Epoch: 560 mean train loss:  5.32454532e-02, mean val. rec. loss:  4.33720737e-02\n",
      "Epoch: 561 mean train loss:  5.31820717e-02, mean val. rec. loss:  4.33068020e-02\n",
      "Epoch: 562 mean train loss:  5.31187870e-02, mean val. rec. loss:  4.32416392e-02\n",
      "Epoch: 563 mean train loss:  5.30555880e-02, mean val. rec. loss:  4.31766079e-02\n",
      "Epoch: 564 mean train loss:  5.29924970e-02, mean val. rec. loss:  4.31117218e-02\n",
      "Epoch: 565 mean train loss:  5.29294916e-02, mean val. rec. loss:  4.30469309e-02\n",
      "Epoch: 566 mean train loss:  5.28665980e-02, mean val. rec. loss:  4.29822761e-02\n",
      "Epoch: 567 mean train loss:  5.28037975e-02, mean val. rec. loss:  4.29177755e-02\n",
      "Epoch: 568 mean train loss:  5.27410826e-02, mean val. rec. loss:  4.28533701e-02\n",
      "Epoch: 569 mean train loss:  5.26784832e-02, mean val. rec. loss:  4.27891145e-02\n",
      "Epoch: 570 mean train loss:  5.26159844e-02, mean val. rec. loss:  4.27249995e-02\n",
      "Epoch: 571 mean train loss:  5.25535935e-02, mean val. rec. loss:  4.26609888e-02\n",
      "Epoch: 572 mean train loss:  5.24912921e-02, mean val. rec. loss:  4.25971504e-02\n",
      "Epoch: 573 mean train loss:  5.24290987e-02, mean val. rec. loss:  4.25334255e-02\n",
      "Epoch: 574 mean train loss:  5.23670170e-02, mean val. rec. loss:  4.24698457e-02\n",
      "Epoch: 575 mean train loss:  5.23050358e-02, mean val. rec. loss:  4.24063883e-02\n",
      "Epoch: 576 mean train loss:  5.22431738e-02, mean val. rec. loss:  4.23430852e-02\n",
      "Epoch: 577 mean train loss:  5.21814050e-02, mean val. rec. loss:  4.22799227e-02\n",
      "Epoch: 578 mean train loss:  5.21197553e-02, mean val. rec. loss:  4.22168963e-02\n",
      "Epoch: 579 mean train loss:  5.20582062e-02, mean val. rec. loss:  4.21539833e-02\n",
      "Epoch: 580 mean train loss:  5.19967725e-02, mean val. rec. loss:  4.20912426e-02\n",
      "Epoch: 581 mean train loss:  5.19354506e-02, mean val. rec. loss:  4.20286517e-02\n",
      "Epoch: 582 mean train loss:  5.18742367e-02, mean val. rec. loss:  4.19661786e-02\n",
      "Epoch: 583 mean train loss:  5.18131382e-02, mean val. rec. loss:  4.19038779e-02\n",
      "Epoch: 584 mean train loss:  5.17521514e-02, mean val. rec. loss:  4.18417043e-02\n",
      "Epoch: 585 mean train loss:  5.16912876e-02, mean val. rec. loss:  4.17796576e-02\n",
      "Epoch: 586 mean train loss:  5.16305355e-02, mean val. rec. loss:  4.17177652e-02\n",
      "Epoch: 587 mean train loss:  5.15699063e-02, mean val. rec. loss:  4.16560768e-02\n",
      "Epoch: 588 mean train loss:  5.15093888e-02, mean val. rec. loss:  4.15944702e-02\n",
      "Epoch: 589 mean train loss:  5.14489868e-02, mean val. rec. loss:  4.15330540e-02\n",
      "Epoch: 590 mean train loss:  5.13887189e-02, mean val. rec. loss:  4.14717694e-02\n",
      "Epoch: 591 mean train loss:  5.13285590e-02, mean val. rec. loss:  4.14106435e-02\n",
      "Epoch: 592 mean train loss:  5.12685257e-02, mean val. rec. loss:  4.13496718e-02\n",
      "Epoch: 593 mean train loss:  5.12086153e-02, mean val. rec. loss:  4.12888317e-02\n",
      "Epoch: 594 mean train loss:  5.11488241e-02, mean val. rec. loss:  4.12281866e-02\n",
      "Epoch: 595 mean train loss:  5.10891520e-02, mean val. rec. loss:  4.11676913e-02\n",
      "Epoch: 596 mean train loss:  5.10296103e-02, mean val. rec. loss:  4.11073138e-02\n",
      "Epoch: 597 mean train loss:  5.09701953e-02, mean val. rec. loss:  4.10470951e-02\n",
      "Epoch: 598 mean train loss:  5.09109106e-02, mean val. rec. loss:  4.09870760e-02\n",
      "Epoch: 599 mean train loss:  5.08517525e-02, mean val. rec. loss:  4.09271839e-02\n",
      "Epoch: 600 mean train loss:  5.07927062e-02, mean val. rec. loss:  4.08674687e-02\n",
      "Epoch: 601 mean train loss:  5.07338163e-02, mean val. rec. loss:  4.08079032e-02\n",
      "Epoch: 602 mean train loss:  5.06750307e-02, mean val. rec. loss:  4.07485100e-02\n",
      "Epoch: 603 mean train loss:  5.06163940e-02, mean val. rec. loss:  4.06892847e-02\n",
      "Epoch: 604 mean train loss:  5.05578691e-02, mean val. rec. loss:  4.06301864e-02\n",
      "Epoch: 605 mean train loss:  5.04994857e-02, mean val. rec. loss:  4.05712695e-02\n",
      "Epoch: 606 mean train loss:  5.04412364e-02, mean val. rec. loss:  4.05125113e-02\n",
      "Epoch: 607 mean train loss:  5.03831211e-02, mean val. rec. loss:  4.04539482e-02\n",
      "Epoch: 608 mean train loss:  5.03251362e-02, mean val. rec. loss:  4.03955076e-02\n",
      "Epoch: 609 mean train loss:  5.02672929e-02, mean val. rec. loss:  4.03372439e-02\n",
      "Epoch: 610 mean train loss:  5.02095799e-02, mean val. rec. loss:  4.02791526e-02\n",
      "Epoch: 611 mean train loss:  5.01520047e-02, mean val. rec. loss:  4.02212245e-02\n",
      "Epoch: 612 mean train loss:  5.00945561e-02, mean val. rec. loss:  4.01634461e-02\n",
      "Epoch: 613 mean train loss:  5.00372565e-02, mean val. rec. loss:  4.01058719e-02\n",
      "Epoch: 614 mean train loss:  4.99800835e-02, mean val. rec. loss:  4.00484518e-02\n",
      "Epoch: 615 mean train loss:  4.99230595e-02, mean val. rec. loss:  3.99911905e-02\n",
      "Epoch: 616 mean train loss:  4.98661770e-02, mean val. rec. loss:  3.99341152e-02\n",
      "Epoch: 617 mean train loss:  4.98094175e-02, mean val. rec. loss:  3.98772078e-02\n",
      "Epoch: 618 mean train loss:  4.97528069e-02, mean val. rec. loss:  3.98204545e-02\n",
      "Epoch: 619 mean train loss:  4.96963453e-02, mean val. rec. loss:  3.97639190e-02\n",
      "Epoch: 620 mean train loss:  4.96400140e-02, mean val. rec. loss:  3.97074968e-02\n",
      "Epoch: 621 mean train loss:  4.95838354e-02, mean val. rec. loss:  3.96512652e-02\n",
      "Epoch: 622 mean train loss:  4.95277798e-02, mean val. rec. loss:  3.95952150e-02\n",
      "Epoch: 623 mean train loss:  4.94718805e-02, mean val. rec. loss:  3.95393326e-02\n",
      "Epoch: 624 mean train loss:  4.94161303e-02, mean val. rec. loss:  3.94836090e-02\n",
      "Epoch: 625 mean train loss:  4.93605178e-02, mean val. rec. loss:  3.94280895e-02\n",
      "Epoch: 626 mean train loss:  4.93050469e-02, mean val. rec. loss:  3.93727560e-02\n",
      "Epoch: 627 mean train loss:  4.92497100e-02, mean val. rec. loss:  3.93175631e-02\n",
      "Epoch: 628 mean train loss:  4.91945371e-02, mean val. rec. loss:  3.92625471e-02\n",
      "Epoch: 629 mean train loss:  4.91395056e-02, mean val. rec. loss:  3.92077352e-02\n",
      "Epoch: 630 mean train loss:  4.90846157e-02, mean val. rec. loss:  3.91531002e-02\n",
      "Epoch: 631 mean train loss:  4.90298673e-02, mean val. rec. loss:  3.90986013e-02\n",
      "Epoch: 632 mean train loss:  4.89752753e-02, mean val. rec. loss:  3.90442838e-02\n",
      "Epoch: 633 mean train loss:  4.89208248e-02, mean val. rec. loss:  3.89901795e-02\n",
      "Epoch: 634 mean train loss:  4.88665234e-02, mean val. rec. loss:  3.89362567e-02\n",
      "Epoch: 635 mean train loss:  4.88123597e-02, mean val. rec. loss:  3.88824653e-02\n",
      "Epoch: 636 mean train loss:  4.87583562e-02, mean val. rec. loss:  3.88288781e-02\n",
      "Epoch: 637 mean train loss:  4.87044979e-02, mean val. rec. loss:  3.87754860e-02\n",
      "Epoch: 638 mean train loss:  4.86507849e-02, mean val. rec. loss:  3.87222208e-02\n",
      "Epoch: 639 mean train loss:  4.85972245e-02, mean val. rec. loss:  3.86691553e-02\n",
      "Epoch: 640 mean train loss:  4.85438058e-02, mean val. rec. loss:  3.86162938e-02\n",
      "Epoch: 641 mean train loss:  4.84905397e-02, mean val. rec. loss:  3.85635911e-02\n",
      "Epoch: 642 mean train loss:  4.84374263e-02, mean val. rec. loss:  3.85110789e-02\n",
      "Epoch: 643 mean train loss:  4.83844618e-02, mean val. rec. loss:  3.84587346e-02\n",
      "Epoch: 644 mean train loss:  4.83316427e-02, mean val. rec. loss:  3.84065626e-02\n",
      "Epoch: 645 mean train loss:  4.82789725e-02, mean val. rec. loss:  3.83545811e-02\n",
      "Epoch: 646 mean train loss:  4.82264587e-02, mean val. rec. loss:  3.83027720e-02\n",
      "Epoch: 647 mean train loss:  4.81740827e-02, mean val. rec. loss:  3.82511534e-02\n",
      "Epoch: 648 mean train loss:  4.81218632e-02, mean val. rec. loss:  3.81996935e-02\n",
      "Epoch: 649 mean train loss:  4.80698001e-02, mean val. rec. loss:  3.81484151e-02\n",
      "Epoch: 650 mean train loss:  4.80178785e-02, mean val. rec. loss:  3.80973498e-02\n",
      "Epoch: 651 mean train loss:  4.79661170e-02, mean val. rec. loss:  3.80464343e-02\n",
      "Epoch: 652 mean train loss:  4.79144971e-02, mean val. rec. loss:  3.79956934e-02\n",
      "Epoch: 653 mean train loss:  4.78630299e-02, mean val. rec. loss:  3.79451611e-02\n",
      "Epoch: 654 mean train loss:  4.78117042e-02, mean val. rec. loss:  3.78947853e-02\n",
      "Epoch: 655 mean train loss:  4.77605349e-02, mean val. rec. loss:  3.78446182e-02\n",
      "Epoch: 656 mean train loss:  4.77095221e-02, mean val. rec. loss:  3.77945985e-02\n",
      "Epoch: 657 mean train loss:  4.76586545e-02, mean val. rec. loss:  3.77447376e-02\n",
      "Epoch: 658 mean train loss:  4.76079321e-02, mean val. rec. loss:  3.76950853e-02\n",
      "Epoch: 659 mean train loss:  4.75573699e-02, mean val. rec. loss:  3.76456030e-02\n",
      "Epoch: 660 mean train loss:  4.75069455e-02, mean val. rec. loss:  3.75963182e-02\n",
      "Epoch: 661 mean train loss:  4.74566738e-02, mean val. rec. loss:  3.75471761e-02\n",
      "Epoch: 662 mean train loss:  4.74065586e-02, mean val. rec. loss:  3.74982315e-02\n",
      "Epoch: 663 mean train loss:  4.73565923e-02, mean val. rec. loss:  3.74494568e-02\n",
      "Epoch: 664 mean train loss:  4.73067638e-02, mean val. rec. loss:  3.74008614e-02\n",
      "Epoch: 665 mean train loss:  4.72570917e-02, mean val. rec. loss:  3.73524542e-02\n",
      "Epoch: 666 mean train loss:  4.72075537e-02, mean val. rec. loss:  3.73041831e-02\n",
      "Epoch: 667 mean train loss:  4.71581721e-02, mean val. rec. loss:  3.72561093e-02\n",
      "Epoch: 668 mean train loss:  4.71089433e-02, mean val. rec. loss:  3.72082237e-02\n",
      "Epoch: 669 mean train loss:  4.70598634e-02, mean val. rec. loss:  3.71604811e-02\n",
      "Epoch: 670 mean train loss:  4.70109176e-02, mean val. rec. loss:  3.71129470e-02\n",
      "Epoch: 671 mean train loss:  4.69621244e-02, mean val. rec. loss:  3.70655763e-02\n",
      "Epoch: 672 mean train loss:  4.69134803e-02, mean val. rec. loss:  3.70183756e-02\n",
      "Epoch: 673 mean train loss:  4.68649851e-02, mean val. rec. loss:  3.69713632e-02\n",
      "Epoch: 674 mean train loss:  4.68166241e-02, mean val. rec. loss:  3.69244665e-02\n",
      "Epoch: 675 mean train loss:  4.67684194e-02, mean val. rec. loss:  3.68777852e-02\n",
      "Epoch: 676 mean train loss:  4.67203525e-02, mean val. rec. loss:  3.68312967e-02\n",
      "Epoch: 677 mean train loss:  4.66724272e-02, mean val. rec. loss:  3.67849262e-02\n",
      "Epoch: 678 mean train loss:  4.66246546e-02, mean val. rec. loss:  3.67387506e-02\n",
      "Epoch: 679 mean train loss:  4.65770123e-02, mean val. rec. loss:  3.66927588e-02\n",
      "Epoch: 680 mean train loss:  4.65295152e-02, mean val. rec. loss:  3.66469008e-02\n",
      "Epoch: 681 mean train loss:  4.64821635e-02, mean val. rec. loss:  3.66012310e-02\n",
      "Epoch: 682 mean train loss:  4.64349569e-02, mean val. rec. loss:  3.65557223e-02\n",
      "Epoch: 683 mean train loss:  4.63878882e-02, mean val. rec. loss:  3.65103723e-02\n",
      "Epoch: 684 mean train loss:  4.63409461e-02, mean val. rec. loss:  3.64652083e-02\n",
      "Epoch: 685 mean train loss:  4.62941530e-02, mean val. rec. loss:  3.64201712e-02\n",
      "Epoch: 686 mean train loss:  4.62474977e-02, mean val. rec. loss:  3.63753202e-02\n",
      "Epoch: 687 mean train loss:  4.62009839e-02, mean val. rec. loss:  3.63306324e-02\n",
      "Epoch: 688 mean train loss:  4.61546004e-02, mean val. rec. loss:  3.62861171e-02\n",
      "Epoch: 689 mean train loss:  4.61083436e-02, mean val. rec. loss:  3.62417378e-02\n",
      "Epoch: 690 mean train loss:  4.60622358e-02, mean val. rec. loss:  3.61975127e-02\n",
      "Epoch: 691 mean train loss:  4.60162620e-02, mean val. rec. loss:  3.61534781e-02\n",
      "Epoch: 692 mean train loss:  4.59704149e-02, mean val. rec. loss:  3.61095660e-02\n",
      "Epoch: 693 mean train loss:  4.59247018e-02, mean val. rec. loss:  3.60658217e-02\n",
      "Epoch: 694 mean train loss:  4.58791154e-02, mean val. rec. loss:  3.60222407e-02\n",
      "Epoch: 695 mean train loss:  4.58336780e-02, mean val. rec. loss:  3.59787913e-02\n",
      "Epoch: 696 mean train loss:  4.57883522e-02, mean val. rec. loss:  3.59355233e-02\n",
      "Epoch: 697 mean train loss:  4.57431606e-02, mean val. rec. loss:  3.58923822e-02\n",
      "Epoch: 698 mean train loss:  4.56980919e-02, mean val. rec. loss:  3.58494340e-02\n",
      "Epoch: 699 mean train loss:  4.56531460e-02, mean val. rec. loss:  3.58065878e-02\n",
      "Epoch: 700 mean train loss:  4.56083306e-02, mean val. rec. loss:  3.57639118e-02\n",
      "Epoch: 701 mean train loss:  4.55636417e-02, mean val. rec. loss:  3.57213922e-02\n",
      "Epoch: 702 mean train loss:  4.55190720e-02, mean val. rec. loss:  3.56789701e-02\n",
      "Epoch: 703 mean train loss:  4.54746327e-02, mean val. rec. loss:  3.56367431e-02\n",
      "Epoch: 704 mean train loss:  4.54303014e-02, mean val. rec. loss:  3.55946408e-02\n",
      "Epoch: 705 mean train loss:  4.53860930e-02, mean val. rec. loss:  3.55526678e-02\n",
      "Epoch: 706 mean train loss:  4.53420075e-02, mean val. rec. loss:  3.55108604e-02\n",
      "Epoch: 707 mean train loss:  4.52980375e-02, mean val. rec. loss:  3.54691595e-02\n",
      "Epoch: 708 mean train loss:  4.52541792e-02, mean val. rec. loss:  3.54276333e-02\n",
      "Epoch: 709 mean train loss:  4.52104363e-02, mean val. rec. loss:  3.53862205e-02\n",
      "Epoch: 710 mean train loss:  4.51668089e-02, mean val. rec. loss:  3.53449392e-02\n",
      "Epoch: 711 mean train loss:  4.51232821e-02, mean val. rec. loss:  3.53038166e-02\n",
      "Epoch: 712 mean train loss:  4.50798781e-02, mean val. rec. loss:  3.52627848e-02\n",
      "Epoch: 713 mean train loss:  4.50365748e-02, mean val. rec. loss:  3.52218982e-02\n",
      "Epoch: 714 mean train loss:  4.49933906e-02, mean val. rec. loss:  3.51811521e-02\n",
      "Epoch: 715 mean train loss:  4.49502995e-02, mean val. rec. loss:  3.51405195e-02\n",
      "Epoch: 716 mean train loss:  4.49073238e-02, mean val. rec. loss:  3.51000184e-02\n",
      "Epoch: 717 mean train loss:  4.48644376e-02, mean val. rec. loss:  3.50596669e-02\n",
      "Epoch: 718 mean train loss:  4.48216594e-02, mean val. rec. loss:  3.50193677e-02\n",
      "Epoch: 719 mean train loss:  4.47789817e-02, mean val. rec. loss:  3.49792385e-02\n",
      "Epoch: 720 mean train loss:  4.47363934e-02, mean val. rec. loss:  3.49392522e-02\n",
      "Epoch: 721 mean train loss:  4.46939094e-02, mean val. rec. loss:  3.48993453e-02\n",
      "Epoch: 722 mean train loss:  4.46515147e-02, mean val. rec. loss:  3.48595382e-02\n",
      "Epoch: 723 mean train loss:  4.46092169e-02, mean val. rec. loss:  3.48198695e-02\n",
      "Epoch: 724 mean train loss:  4.45670160e-02, mean val. rec. loss:  3.47803005e-02\n",
      "Epoch: 725 mean train loss:  4.45248895e-02, mean val. rec. loss:  3.47408676e-02\n",
      "Epoch: 726 mean train loss:  4.44828599e-02, mean val. rec. loss:  3.47014778e-02\n",
      "Epoch: 727 mean train loss:  4.44409159e-02, mean val. rec. loss:  3.46622490e-02\n",
      "Epoch: 728 mean train loss:  4.43990427e-02, mean val. rec. loss:  3.46230973e-02\n",
      "Epoch: 729 mean train loss:  4.43572700e-02, mean val. rec. loss:  3.45840636e-02\n",
      "Epoch: 730 mean train loss:  4.43155532e-02, mean val. rec. loss:  3.45451001e-02\n",
      "Epoch: 731 mean train loss:  4.42739370e-02, mean val. rec. loss:  3.45062523e-02\n",
      "Epoch: 732 mean train loss:  4.42323803e-02, mean val. rec. loss:  3.44675021e-02\n",
      "Epoch: 733 mean train loss:  4.41909019e-02, mean val. rec. loss:  3.44288539e-02\n",
      "Epoch: 734 mean train loss:  4.41494905e-02, mean val. rec. loss:  3.43902919e-02\n",
      "Epoch: 735 mean train loss:  4.41081573e-02, mean val. rec. loss:  3.43518093e-02\n",
      "Epoch: 736 mean train loss:  4.40668763e-02, mean val. rec. loss:  3.43134015e-02\n",
      "Epoch: 737 mean train loss:  4.40256697e-02, mean val. rec. loss:  3.42751094e-02\n",
      "Epoch: 738 mean train loss:  4.39845265e-02, mean val. rec. loss:  3.42368739e-02\n",
      "Epoch: 739 mean train loss:  4.39434429e-02, mean val. rec. loss:  3.41987247e-02\n",
      "Epoch: 740 mean train loss:  4.39024114e-02, mean val. rec. loss:  3.41606367e-02\n",
      "Epoch: 741 mean train loss:  4.38614320e-02, mean val. rec. loss:  3.41226439e-02\n",
      "Epoch: 742 mean train loss:  4.38205085e-02, mean val. rec. loss:  3.40847238e-02\n",
      "Epoch: 743 mean train loss:  4.37796446e-02, mean val. rec. loss:  3.40468603e-02\n",
      "Epoch: 744 mean train loss:  4.37388105e-02, mean val. rec. loss:  3.40090626e-02\n",
      "Epoch: 745 mean train loss:  4.36980397e-02, mean val. rec. loss:  3.39713374e-02\n",
      "Epoch: 746 mean train loss:  4.36572987e-02, mean val. rec. loss:  3.39336645e-02\n",
      "Epoch: 747 mean train loss:  4.36165949e-02, mean val. rec. loss:  3.38960641e-02\n",
      "Epoch: 748 mean train loss:  4.35759358e-02, mean val. rec. loss:  3.38585090e-02\n",
      "Epoch: 749 mean train loss:  4.35353103e-02, mean val. rec. loss:  3.38210198e-02\n",
      "Epoch: 750 mean train loss:  4.34947183e-02, mean val. rec. loss:  3.37835759e-02\n",
      "Epoch: 751 mean train loss:  4.34541560e-02, mean val. rec. loss:  3.37461864e-02\n",
      "Epoch: 752 mean train loss:  4.34136236e-02, mean val. rec. loss:  3.37088241e-02\n",
      "Epoch: 753 mean train loss:  4.33730986e-02, mean val. rec. loss:  3.36715299e-02\n",
      "Epoch: 754 mean train loss:  4.33326146e-02, mean val. rec. loss:  3.36342516e-02\n",
      "Epoch: 755 mean train loss:  4.32921268e-02, mean val. rec. loss:  3.35970140e-02\n",
      "Epoch: 756 mean train loss:  4.32516726e-02, mean val. rec. loss:  3.35598287e-02\n",
      "Epoch: 757 mean train loss:  4.32112221e-02, mean val. rec. loss:  3.35226615e-02\n",
      "Epoch: 758 mean train loss:  4.31707753e-02, mean val. rec. loss:  3.34855215e-02\n",
      "Epoch: 759 mean train loss:  4.31303360e-02, mean val. rec. loss:  3.34484109e-02\n",
      "Epoch: 760 mean train loss:  4.30899004e-02, mean val. rec. loss:  3.34113322e-02\n",
      "Epoch: 761 mean train loss:  4.30494536e-02, mean val. rec. loss:  3.33742421e-02\n",
      "Epoch: 762 mean train loss:  4.30090106e-02, mean val. rec. loss:  3.33371882e-02\n",
      "Epoch: 763 mean train loss:  4.29685638e-02, mean val. rec. loss:  3.33001367e-02\n",
      "Epoch: 764 mean train loss:  4.29280947e-02, mean val. rec. loss:  3.32631282e-02\n",
      "Epoch: 765 mean train loss:  4.28876181e-02, mean val. rec. loss:  3.32261039e-02\n",
      "Epoch: 766 mean train loss:  4.28471266e-02, mean val. rec. loss:  3.31890818e-02\n",
      "Epoch: 767 mean train loss:  4.28066053e-02, mean val. rec. loss:  3.31520394e-02\n",
      "Epoch: 768 mean train loss:  4.27660617e-02, mean val. rec. loss:  3.31150128e-02\n",
      "Epoch: 769 mean train loss:  4.27254995e-02, mean val. rec. loss:  3.30779657e-02\n",
      "Epoch: 770 mean train loss:  4.26848926e-02, mean val. rec. loss:  3.30409391e-02\n",
      "Epoch: 771 mean train loss:  4.26442596e-02, mean val. rec. loss:  3.30038762e-02\n",
      "Epoch: 772 mean train loss:  4.26035819e-02, mean val. rec. loss:  3.29667952e-02\n",
      "Epoch: 773 mean train loss:  4.25628558e-02, mean val. rec. loss:  3.29296870e-02\n",
      "Epoch: 774 mean train loss:  4.25220850e-02, mean val. rec. loss:  3.28925810e-02\n",
      "Epoch: 775 mean train loss:  4.24812583e-02, mean val. rec. loss:  3.28554342e-02\n",
      "Epoch: 776 mean train loss:  4.24403721e-02, mean val. rec. loss:  3.28182556e-02\n",
      "Epoch: 777 mean train loss:  4.23994374e-02, mean val. rec. loss:  3.27810385e-02\n",
      "Epoch: 778 mean train loss:  4.23584394e-02, mean val. rec. loss:  3.27437647e-02\n",
      "Epoch: 779 mean train loss:  4.23173744e-02, mean val. rec. loss:  3.27064818e-02\n",
      "Epoch: 780 mean train loss:  4.22762386e-02, mean val. rec. loss:  3.26691332e-02\n",
      "Epoch: 781 mean train loss:  4.22350246e-02, mean val. rec. loss:  3.26317550e-02\n",
      "Epoch: 782 mean train loss:  4.21937212e-02, mean val. rec. loss:  3.25942884e-02\n",
      "Epoch: 783 mean train loss:  4.21523433e-02, mean val. rec. loss:  3.25567765e-02\n",
      "Epoch: 784 mean train loss:  4.21108761e-02, mean val. rec. loss:  3.25192010e-02\n",
      "Epoch: 785 mean train loss:  4.20693194e-02, mean val. rec. loss:  3.24815575e-02\n",
      "Epoch: 786 mean train loss:  4.20276511e-02, mean val. rec. loss:  3.24438528e-02\n",
      "Epoch: 787 mean train loss:  4.19858933e-02, mean val. rec. loss:  3.24060619e-02\n",
      "Epoch: 788 mean train loss:  4.19440313e-02, mean val. rec. loss:  3.23682007e-02\n",
      "Epoch: 789 mean train loss:  4.19020500e-02, mean val. rec. loss:  3.23302556e-02\n",
      "Epoch: 790 mean train loss:  4.18599571e-02, mean val. rec. loss:  3.22922197e-02\n",
      "Epoch: 791 mean train loss:  4.18177561e-02, mean val. rec. loss:  3.22541045e-02\n",
      "Epoch: 792 mean train loss:  4.17754174e-02, mean val. rec. loss:  3.22158623e-02\n",
      "Epoch: 793 mean train loss:  4.17329520e-02, mean val. rec. loss:  3.21775611e-02\n",
      "Epoch: 794 mean train loss:  4.16903562e-02, mean val. rec. loss:  3.21391216e-02\n",
      "Epoch: 795 mean train loss:  4.16476189e-02, mean val. rec. loss:  3.21006095e-02\n",
      "Epoch: 796 mean train loss:  4.16047364e-02, mean val. rec. loss:  3.20619635e-02\n",
      "Epoch: 797 mean train loss:  4.15617086e-02, mean val. rec. loss:  3.20232269e-02\n",
      "Epoch: 798 mean train loss:  4.15185282e-02, mean val. rec. loss:  3.19843451e-02\n",
      "Epoch: 799 mean train loss:  4.14751913e-02, mean val. rec. loss:  3.19453590e-02\n",
      "Epoch: 800 mean train loss:  4.14316905e-02, mean val. rec. loss:  3.19062209e-02\n",
      "Epoch: 801 mean train loss:  4.13880110e-02, mean val. rec. loss:  3.18669604e-02\n",
      "Epoch: 802 mean train loss:  4.13441713e-02, mean val. rec. loss:  3.18275524e-02\n",
      "Epoch: 803 mean train loss:  4.13001640e-02, mean val. rec. loss:  3.17880016e-02\n",
      "Epoch: 804 mean train loss:  4.12559593e-02, mean val. rec. loss:  3.17483192e-02\n",
      "Epoch: 805 mean train loss:  4.12115833e-02, mean val. rec. loss:  3.17084871e-02\n",
      "Epoch: 806 mean train loss:  4.11669950e-02, mean val. rec. loss:  3.16684918e-02\n",
      "Epoch: 807 mean train loss:  4.11222242e-02, mean val. rec. loss:  3.16283604e-02\n",
      "Epoch: 808 mean train loss:  4.10772561e-02, mean val. rec. loss:  3.15880407e-02\n",
      "Epoch: 809 mean train loss:  4.10320793e-02, mean val. rec. loss:  3.15475668e-02\n",
      "Epoch: 810 mean train loss:  4.09866940e-02, mean val. rec. loss:  3.15069183e-02\n",
      "Epoch: 811 mean train loss:  4.09410890e-02, mean val. rec. loss:  3.14660906e-02\n",
      "Epoch: 812 mean train loss:  4.08952530e-02, mean val. rec. loss:  3.14251019e-02\n",
      "Epoch: 813 mean train loss:  4.08492122e-02, mean val. rec. loss:  3.13839136e-02\n",
      "Epoch: 814 mean train loss:  4.08029330e-02, mean val. rec. loss:  3.13425529e-02\n",
      "Epoch: 815 mean train loss:  4.07564230e-02, mean val. rec. loss:  3.13009995e-02\n",
      "Epoch: 816 mean train loss:  4.07096671e-02, mean val. rec. loss:  3.12592487e-02\n",
      "Epoch: 817 mean train loss:  4.06626878e-02, mean val. rec. loss:  3.12173188e-02\n",
      "Epoch: 818 mean train loss:  4.06154403e-02, mean val. rec. loss:  3.11751757e-02\n",
      "Epoch: 819 mean train loss:  4.05679544e-02, mean val. rec. loss:  3.11328081e-02\n",
      "Epoch: 820 mean train loss:  4.05202078e-02, mean val. rec. loss:  3.10902499e-02\n",
      "Epoch: 821 mean train loss:  4.04722117e-02, mean val. rec. loss:  3.10474854e-02\n",
      "Epoch: 822 mean train loss:  4.04239326e-02, mean val. rec. loss:  3.10045122e-02\n",
      "Epoch: 823 mean train loss:  4.03754114e-02, mean val. rec. loss:  3.09612941e-02\n",
      "Epoch: 824 mean train loss:  4.03265996e-02, mean val. rec. loss:  3.09178855e-02\n",
      "Epoch: 825 mean train loss:  4.02775123e-02, mean val. rec. loss:  3.08742501e-02\n",
      "Epoch: 826 mean train loss:  4.02281642e-02, mean val. rec. loss:  3.08303992e-02\n",
      "Epoch: 827 mean train loss:  4.01785182e-02, mean val. rec. loss:  3.07863124e-02\n",
      "Epoch: 828 mean train loss:  4.01286003e-02, mean val. rec. loss:  3.07420170e-02\n",
      "Epoch: 829 mean train loss:  4.00783920e-02, mean val. rec. loss:  3.06974631e-02\n",
      "Epoch: 830 mean train loss:  4.00278968e-02, mean val. rec. loss:  3.06526756e-02\n",
      "Epoch: 831 mean train loss:  3.99771037e-02, mean val. rec. loss:  3.06076726e-02\n",
      "Epoch: 832 mean train loss:  3.99260275e-02, mean val. rec. loss:  3.05624337e-02\n",
      "Epoch: 833 mean train loss:  3.98746385e-02, mean val. rec. loss:  3.05169431e-02\n",
      "Epoch: 834 mean train loss:  3.98229516e-02, mean val. rec. loss:  3.04712166e-02\n",
      "Epoch: 835 mean train loss:  3.97709741e-02, mean val. rec. loss:  3.04252634e-02\n",
      "Epoch: 836 mean train loss:  3.97186950e-02, mean val. rec. loss:  3.03790833e-02\n",
      "Epoch: 837 mean train loss:  3.96661104e-02, mean val. rec. loss:  3.03326356e-02\n",
      "Epoch: 838 mean train loss:  3.96132317e-02, mean val. rec. loss:  3.02859498e-02\n",
      "Epoch: 839 mean train loss:  3.95600326e-02, mean val. rec. loss:  3.02390417e-02\n",
      "Epoch: 840 mean train loss:  3.95065431e-02, mean val. rec. loss:  3.01918978e-02\n",
      "Epoch: 841 mean train loss:  3.94527406e-02, mean val. rec. loss:  3.01445044e-02\n",
      "Epoch: 842 mean train loss:  3.93986328e-02, mean val. rec. loss:  3.00968683e-02\n",
      "Epoch: 843 mean train loss:  3.93442383e-02, mean val. rec. loss:  3.00490009e-02\n",
      "Epoch: 844 mean train loss:  3.92895159e-02, mean val. rec. loss:  3.00009180e-02\n",
      "Epoch: 845 mean train loss:  3.92345105e-02, mean val. rec. loss:  2.99525992e-02\n",
      "Epoch: 846 mean train loss:  3.91792072e-02, mean val. rec. loss:  2.99040424e-02\n",
      "Epoch: 847 mean train loss:  3.91235947e-02, mean val. rec. loss:  2.98552791e-02\n",
      "Epoch: 848 mean train loss:  3.90677067e-02, mean val. rec. loss:  2.98062754e-02\n",
      "Epoch: 849 mean train loss:  3.90115207e-02, mean val. rec. loss:  2.97570563e-02\n",
      "Epoch: 850 mean train loss:  3.89550367e-02, mean val. rec. loss:  2.97076240e-02\n",
      "Epoch: 851 mean train loss:  3.88982771e-02, mean val. rec. loss:  2.96579830e-02\n",
      "Epoch: 852 mean train loss:  3.88412308e-02, mean val. rec. loss:  2.96081244e-02\n",
      "Epoch: 853 mean train loss:  3.87839237e-02, mean val. rec. loss:  2.95580706e-02\n",
      "Epoch: 854 mean train loss:  3.87263299e-02, mean val. rec. loss:  2.95078151e-02\n",
      "Epoch: 855 mean train loss:  3.86684828e-02, mean val. rec. loss:  2.94573690e-02\n",
      "Epoch: 856 mean train loss:  3.86103601e-02, mean val. rec. loss:  2.94067573e-02\n",
      "Epoch: 857 mean train loss:  3.85519991e-02, mean val. rec. loss:  2.93559302e-02\n",
      "Epoch: 858 mean train loss:  3.84933997e-02, mean val. rec. loss:  2.93049648e-02\n",
      "Epoch: 859 mean train loss:  3.84345396e-02, mean val. rec. loss:  2.92538202e-02\n",
      "Epoch: 860 mean train loss:  3.83754709e-02, mean val. rec. loss:  2.92025100e-02\n",
      "Epoch: 861 mean train loss:  3.83161750e-02, mean val. rec. loss:  2.91510638e-02\n",
      "Epoch: 862 mean train loss:  3.82566631e-02, mean val. rec. loss:  2.90994905e-02\n",
      "Epoch: 863 mean train loss:  3.81969464e-02, mean val. rec. loss:  2.90477539e-02\n",
      "Epoch: 864 mean train loss:  3.81370286e-02, mean val. rec. loss:  2.89958927e-02\n",
      "Epoch: 865 mean train loss:  3.80769357e-02, mean val. rec. loss:  2.89439180e-02\n",
      "Epoch: 866 mean train loss:  3.80166789e-02, mean val. rec. loss:  2.88918707e-02\n",
      "Epoch: 867 mean train loss:  3.79562397e-02, mean val. rec. loss:  2.88397078e-02\n",
      "Epoch: 868 mean train loss:  3.78956626e-02, mean val. rec. loss:  2.87874428e-02\n",
      "Epoch: 869 mean train loss:  3.78349477e-02, mean val. rec. loss:  2.87351574e-02\n",
      "Epoch: 870 mean train loss:  3.77740951e-02, mean val. rec. loss:  2.86827677e-02\n",
      "Epoch: 871 mean train loss:  3.77131419e-02, mean val. rec. loss:  2.86303417e-02\n",
      "Epoch: 872 mean train loss:  3.76520881e-02, mean val. rec. loss:  2.85778635e-02\n",
      "Epoch: 873 mean train loss:  3.75909337e-02, mean val. rec. loss:  2.85253672e-02\n",
      "Epoch: 874 mean train loss:  3.75297235e-02, mean val. rec. loss:  2.84728414e-02\n",
      "Epoch: 875 mean train loss:  3.74684426e-02, mean val. rec. loss:  2.84203247e-02\n",
      "Epoch: 876 mean train loss:  3.74071355e-02, mean val. rec. loss:  2.83678239e-02\n",
      "Epoch: 877 mean train loss:  3.73457838e-02, mean val. rec. loss:  2.83153321e-02\n",
      "Epoch: 878 mean train loss:  3.72844358e-02, mean val. rec. loss:  2.82628834e-02\n",
      "Epoch: 879 mean train loss:  3.72230766e-02, mean val. rec. loss:  2.82104755e-02\n",
      "Epoch: 880 mean train loss:  3.71617547e-02, mean val. rec. loss:  2.81581402e-02\n",
      "Epoch: 881 mean train loss:  3.71004551e-02, mean val. rec. loss:  2.81058843e-02\n",
      "Epoch: 882 mean train loss:  3.70392039e-02, mean val. rec. loss:  2.80537055e-02\n",
      "Epoch: 883 mean train loss:  3.69780384e-02, mean val. rec. loss:  2.80016243e-02\n",
      "Epoch: 884 mean train loss:  3.69169549e-02, mean val. rec. loss:  2.79496722e-02\n",
      "Epoch: 885 mean train loss:  3.68559569e-02, mean val. rec. loss:  2.78978427e-02\n",
      "Epoch: 886 mean train loss:  3.67950968e-02, mean val. rec. loss:  2.78461470e-02\n",
      "Epoch: 887 mean train loss:  3.67343857e-02, mean val. rec. loss:  2.77946259e-02\n",
      "Epoch: 888 mean train loss:  3.66738086e-02, mean val. rec. loss:  2.77432568e-02\n",
      "Epoch: 889 mean train loss:  3.66134215e-02, mean val. rec. loss:  2.76920691e-02\n",
      "Epoch: 890 mean train loss:  3.65532132e-02, mean val. rec. loss:  2.76410787e-02\n",
      "Epoch: 891 mean train loss:  3.64932209e-02, mean val. rec. loss:  2.75903037e-02\n",
      "Epoch: 892 mean train loss:  3.64334445e-02, mean val. rec. loss:  2.75397647e-02\n",
      "Epoch: 893 mean train loss:  3.63739178e-02, mean val. rec. loss:  2.74894297e-02\n",
      "Epoch: 894 mean train loss:  3.63146368e-02, mean val. rec. loss:  2.74393669e-02\n",
      "Epoch: 895 mean train loss:  3.62556500e-02, mean val. rec. loss:  2.73895490e-02\n",
      "Epoch: 896 mean train loss:  3.61969389e-02, mean val. rec. loss:  2.73399988e-02\n",
      "Epoch: 897 mean train loss:  3.61385331e-02, mean val. rec. loss:  2.72907230e-02\n",
      "Epoch: 898 mean train loss:  3.60804552e-02, mean val. rec. loss:  2.72417511e-02\n",
      "Epoch: 899 mean train loss:  3.60227310e-02, mean val. rec. loss:  2.71930627e-02\n",
      "Epoch: 900 mean train loss:  3.59653494e-02, mean val. rec. loss:  2.71446963e-02\n",
      "Epoch: 901 mean train loss:  3.59083292e-02, mean val. rec. loss:  2.70966202e-02\n",
      "Epoch: 902 mean train loss:  3.58516888e-02, mean val. rec. loss:  2.70488866e-02\n",
      "Epoch: 903 mean train loss:  3.57954469e-02, mean val. rec. loss:  2.70015068e-02\n",
      "Epoch: 904 mean train loss:  3.57396221e-02, mean val. rec. loss:  2.69544377e-02\n",
      "Epoch: 905 mean train loss:  3.56842108e-02, mean val. rec. loss:  2.69077541e-02\n",
      "Epoch: 906 mean train loss:  3.56292389e-02, mean val. rec. loss:  2.68613881e-02\n",
      "Epoch: 907 mean train loss:  3.55747066e-02, mean val. rec. loss:  2.68154076e-02\n",
      "Epoch: 908 mean train loss:  3.55206397e-02, mean val. rec. loss:  2.67697923e-02\n",
      "Epoch: 909 mean train loss:  3.54670273e-02, mean val. rec. loss:  2.67245421e-02\n",
      "Epoch: 910 mean train loss:  3.54138841e-02, mean val. rec. loss:  2.66796797e-02\n",
      "Epoch: 911 mean train loss:  3.53612288e-02, mean val. rec. loss:  2.66351802e-02\n",
      "Epoch: 912 mean train loss:  3.53090614e-02, mean val. rec. loss:  2.65910572e-02\n",
      "Epoch: 913 mean train loss:  3.52573893e-02, mean val. rec. loss:  2.65473356e-02\n",
      "Epoch: 914 mean train loss:  3.52062237e-02, mean val. rec. loss:  2.65039723e-02\n",
      "Epoch: 915 mean train loss:  3.51555796e-02, mean val. rec. loss:  2.64610037e-02\n",
      "Epoch: 916 mean train loss:  3.51054420e-02, mean val. rec. loss:  2.64184206e-02\n",
      "Epoch: 917 mean train loss:  3.50558146e-02, mean val. rec. loss:  2.63762049e-02\n",
      "Epoch: 918 mean train loss:  3.50067087e-02, mean val. rec. loss:  2.63343838e-02\n",
      "Epoch: 919 mean train loss:  3.49581353e-02, mean val. rec. loss:  2.62929256e-02\n",
      "Epoch: 920 mean train loss:  3.49100908e-02, mean val. rec. loss:  2.62518575e-02\n",
      "Epoch: 921 mean train loss:  3.48625602e-02, mean val. rec. loss:  2.62111795e-02\n",
      "Epoch: 922 mean train loss:  3.48155622e-02, mean val. rec. loss:  2.61708417e-02\n",
      "Epoch: 923 mean train loss:  3.47691080e-02, mean val. rec. loss:  2.61308985e-02\n",
      "Epoch: 924 mean train loss:  3.47231641e-02, mean val. rec. loss:  2.60913114e-02\n",
      "Epoch: 925 mean train loss:  3.46777564e-02, mean val. rec. loss:  2.60520667e-02\n",
      "Epoch: 926 mean train loss:  3.46328627e-02, mean val. rec. loss:  2.60131918e-02\n",
      "Epoch: 927 mean train loss:  3.45884979e-02, mean val. rec. loss:  2.59746456e-02\n",
      "Epoch: 928 mean train loss:  3.45446433e-02, mean val. rec. loss:  2.59364578e-02\n",
      "Epoch: 929 mean train loss:  3.45013176e-02, mean val. rec. loss:  2.58985875e-02\n",
      "Epoch: 930 mean train loss:  3.44584909e-02, mean val. rec. loss:  2.58610665e-02\n",
      "Epoch: 931 mean train loss:  3.44161708e-02, mean val. rec. loss:  2.58238449e-02\n",
      "Epoch: 932 mean train loss:  3.43743460e-02, mean val. rec. loss:  2.57869589e-02\n",
      "Epoch: 933 mean train loss:  3.43330240e-02, mean val. rec. loss:  2.57503655e-02\n",
      "Epoch: 934 mean train loss:  3.42921862e-02, mean val. rec. loss:  2.57140941e-02\n",
      "Epoch: 935 mean train loss:  3.42518437e-02, mean val. rec. loss:  2.56781062e-02\n",
      "Epoch: 936 mean train loss:  3.42119518e-02, mean val. rec. loss:  2.56423928e-02\n",
      "Epoch: 937 mean train loss:  3.41725479e-02, mean val. rec. loss:  2.56069855e-02\n",
      "Epoch: 938 mean train loss:  3.41336095e-02, mean val. rec. loss:  2.55718345e-02\n",
      "Epoch: 939 mean train loss:  3.40951292e-02, mean val. rec. loss:  2.55369669e-02\n",
      "Epoch: 940 mean train loss:  3.40570958e-02, mean val. rec. loss:  2.55023262e-02\n",
      "Epoch: 941 mean train loss:  3.40194981e-02, mean val. rec. loss:  2.54679713e-02\n",
      "Epoch: 942 mean train loss:  3.39823549e-02, mean val. rec. loss:  2.54338522e-02\n",
      "Epoch: 943 mean train loss:  3.39456251e-02, mean val. rec. loss:  2.53999780e-02\n",
      "Epoch: 944 mean train loss:  3.39093123e-02, mean val. rec. loss:  2.53663420e-02\n",
      "Epoch: 945 mean train loss:  3.38734279e-02, mean val. rec. loss:  2.53329578e-02\n",
      "Epoch: 946 mean train loss:  3.38379383e-02, mean val. rec. loss:  2.52997640e-02\n",
      "Epoch: 947 mean train loss:  3.38028435e-02, mean val. rec. loss:  2.52668152e-02\n",
      "Epoch: 948 mean train loss:  3.37681583e-02, mean val. rec. loss:  2.52340546e-02\n",
      "Epoch: 949 mean train loss:  3.37338418e-02, mean val. rec. loss:  2.52015367e-02\n",
      "Epoch: 950 mean train loss:  3.36999202e-02, mean val. rec. loss:  2.51692002e-02\n",
      "Epoch: 951 mean train loss:  3.36663672e-02, mean val. rec. loss:  2.51370746e-02\n",
      "Epoch: 952 mean train loss:  3.36331905e-02, mean val. rec. loss:  2.51051396e-02\n",
      "Epoch: 953 mean train loss:  3.36003563e-02, mean val. rec. loss:  2.50734041e-02\n",
      "Epoch: 954 mean train loss:  3.35678760e-02, mean val. rec. loss:  2.50418728e-02\n",
      "Epoch: 955 mean train loss:  3.35357383e-02, mean val. rec. loss:  2.50105251e-02\n",
      "Epoch: 956 mean train loss:  3.35039693e-02, mean val. rec. loss:  2.49793612e-02\n",
      "Epoch: 957 mean train loss:  3.34725095e-02, mean val. rec. loss:  2.49483855e-02\n",
      "Epoch: 958 mean train loss:  3.34413923e-02, mean val. rec. loss:  2.49175889e-02\n",
      "Epoch: 959 mean train loss:  3.34106028e-02, mean val. rec. loss:  2.48869761e-02\n",
      "Epoch: 960 mean train loss:  3.33801150e-02, mean val. rec. loss:  2.48565583e-02\n",
      "Epoch: 961 mean train loss:  3.33499550e-02, mean val. rec. loss:  2.48262902e-02\n",
      "Epoch: 962 mean train loss:  3.33201078e-02, mean val. rec. loss:  2.47962035e-02\n",
      "Epoch: 963 mean train loss:  3.32905586e-02, mean val. rec. loss:  2.47663051e-02\n",
      "Epoch: 964 mean train loss:  3.32613296e-02, mean val. rec. loss:  2.47365836e-02\n",
      "Epoch: 965 mean train loss:  3.32323763e-02, mean val. rec. loss:  2.47070299e-02\n",
      "Epoch: 966 mean train loss:  3.32037283e-02, mean val. rec. loss:  2.46776871e-02\n",
      "Epoch: 967 mean train loss:  3.31753635e-02, mean val. rec. loss:  2.46484759e-02\n",
      "Epoch: 968 mean train loss:  3.31472891e-02, mean val. rec. loss:  2.46194733e-02\n",
      "Epoch: 969 mean train loss:  3.31194829e-02, mean val. rec. loss:  2.45906363e-02\n",
      "Epoch: 970 mean train loss:  3.30919671e-02, mean val. rec. loss:  2.45619693e-02\n",
      "Epoch: 971 mean train loss:  3.30647196e-02, mean val. rec. loss:  2.45334816e-02\n",
      "Epoch: 972 mean train loss:  3.30377364e-02, mean val. rec. loss:  2.45051662e-02\n",
      "Epoch: 973 mean train loss:  3.30110214e-02, mean val. rec. loss:  2.44770390e-02\n",
      "Epoch: 974 mean train loss:  3.29845709e-02, mean val. rec. loss:  2.44490933e-02\n",
      "Epoch: 975 mean train loss:  3.29583736e-02, mean val. rec. loss:  2.44213200e-02\n",
      "Epoch: 976 mean train loss:  3.29324519e-02, mean val. rec. loss:  2.43937416e-02\n",
      "Epoch: 977 mean train loss:  3.29067574e-02, mean val. rec. loss:  2.43663312e-02\n",
      "Epoch: 978 mean train loss:  3.28813385e-02, mean val. rec. loss:  2.43391066e-02\n",
      "Epoch: 979 mean train loss:  3.28561393e-02, mean val. rec. loss:  2.43120726e-02\n",
      "Epoch: 980 mean train loss:  3.28312009e-02, mean val. rec. loss:  2.42852201e-02\n",
      "Epoch: 981 mean train loss:  3.28065045e-02, mean val. rec. loss:  2.42585558e-02\n",
      "Epoch: 982 mean train loss:  3.27820465e-02, mean val. rec. loss:  2.42320706e-02\n",
      "Epoch: 983 mean train loss:  3.27578119e-02, mean val. rec. loss:  2.42057759e-02\n",
      "Epoch: 984 mean train loss:  3.27338195e-02, mean val. rec. loss:  2.41796650e-02\n",
      "Epoch: 985 mean train loss:  3.27100356e-02, mean val. rec. loss:  2.41537491e-02\n",
      "Epoch: 986 mean train loss:  3.26864900e-02, mean val. rec. loss:  2.41279965e-02\n",
      "Epoch: 987 mean train loss:  3.26631680e-02, mean val. rec. loss:  2.41024593e-02\n",
      "Epoch: 988 mean train loss:  3.26400656e-02, mean val. rec. loss:  2.40770877e-02\n",
      "Epoch: 989 mean train loss:  3.26171756e-02, mean val. rec. loss:  2.40519225e-02\n",
      "Epoch: 990 mean train loss:  3.25945016e-02, mean val. rec. loss:  2.40269501e-02\n",
      "Epoch: 991 mean train loss:  3.25720435e-02, mean val. rec. loss:  2.40021432e-02\n",
      "Epoch: 992 mean train loss:  3.25497866e-02, mean val. rec. loss:  2.39775359e-02\n",
      "Epoch: 993 mean train loss:  3.25277346e-02, mean val. rec. loss:  2.39531260e-02\n",
      "Epoch: 994 mean train loss:  3.25058799e-02, mean val. rec. loss:  2.39289065e-02\n",
      "Epoch: 995 mean train loss:  3.24842226e-02, mean val. rec. loss:  2.39048639e-02\n",
      "Epoch: 996 mean train loss:  3.24627702e-02, mean val. rec. loss:  2.38810051e-02\n",
      "Epoch: 997 mean train loss:  3.24414965e-02, mean val. rec. loss:  2.38573367e-02\n",
      "Epoch: 998 mean train loss:  3.24204165e-02, mean val. rec. loss:  2.38338520e-02\n",
      "Epoch: 999 mean train loss:  3.23995377e-02, mean val. rec. loss:  2.38105602e-02\n",
      "Epoch: 1000 mean train loss:  3.23788301e-02, mean val. rec. loss:  2.37874361e-02\n",
      "Epoch: 1001 mean train loss:  3.23583088e-02, mean val. rec. loss:  2.37645162e-02\n",
      "Epoch: 1002 mean train loss:  3.23379513e-02, mean val. rec. loss:  2.37417822e-02\n",
      "Epoch: 1003 mean train loss:  3.23177875e-02, mean val. rec. loss:  2.37192206e-02\n",
      "Epoch: 1004 mean train loss:  3.22977801e-02, mean val. rec. loss:  2.36968404e-02\n",
      "Epoch: 1005 mean train loss:  3.22779552e-02, mean val. rec. loss:  2.36746372e-02\n",
      "Epoch: 1006 mean train loss:  3.22582793e-02, mean val. rec. loss:  2.36526290e-02\n",
      "Epoch: 1007 mean train loss:  3.22387785e-02, mean val. rec. loss:  2.36307795e-02\n",
      "Epoch: 1008 mean train loss:  3.22194452e-02, mean val. rec. loss:  2.36091274e-02\n",
      "Epoch: 1009 mean train loss:  3.22002572e-02, mean val. rec. loss:  2.35876385e-02\n",
      "Epoch: 1010 mean train loss:  3.21812293e-02, mean val. rec. loss:  2.35663243e-02\n",
      "Epoch: 1011 mean train loss:  3.21623616e-02, mean val. rec. loss:  2.35451847e-02\n",
      "Epoch: 1012 mean train loss:  3.21436243e-02, mean val. rec. loss:  2.35242152e-02\n",
      "Epoch: 1013 mean train loss:  3.21250545e-02, mean val. rec. loss:  2.35034181e-02\n",
      "Epoch: 1014 mean train loss:  3.21066225e-02, mean val. rec. loss:  2.34827797e-02\n",
      "Epoch: 1015 mean train loss:  3.20883172e-02, mean val. rec. loss:  2.34623296e-02\n",
      "Epoch: 1016 mean train loss:  3.20701683e-02, mean val. rec. loss:  2.34420132e-02\n",
      "Epoch: 1017 mean train loss:  3.20521572e-02, mean val. rec. loss:  2.34218715e-02\n",
      "Epoch: 1018 mean train loss:  3.20342802e-02, mean val. rec. loss:  2.34019022e-02\n",
      "Epoch: 1019 mean train loss:  3.20165260e-02, mean val. rec. loss:  2.33820871e-02\n",
      "Epoch: 1020 mean train loss:  3.19989023e-02, mean val. rec. loss:  2.33624716e-02\n",
      "Epoch: 1021 mean train loss:  3.19814014e-02, mean val. rec. loss:  2.33429831e-02\n",
      "Epoch: 1022 mean train loss:  3.19640458e-02, mean val. rec. loss:  2.33236487e-02\n",
      "Epoch: 1023 mean train loss:  3.19467907e-02, mean val. rec. loss:  2.33044755e-02\n",
      "Epoch: 1024 mean train loss:  3.19296623e-02, mean val. rec. loss:  2.32854292e-02\n",
      "Epoch: 1025 mean train loss:  3.19126568e-02, mean val. rec. loss:  2.32665462e-02\n",
      "Epoch: 1026 mean train loss:  3.18957704e-02, mean val. rec. loss:  2.32478220e-02\n",
      "Epoch: 1027 mean train loss:  3.18789809e-02, mean val. rec. loss:  2.32292248e-02\n",
      "Epoch: 1028 mean train loss:  3.18623180e-02, mean val. rec. loss:  2.32108022e-02\n",
      "Epoch: 1029 mean train loss:  3.18457594e-02, mean val. rec. loss:  2.31925338e-02\n",
      "Epoch: 1030 mean train loss:  3.18293051e-02, mean val. rec. loss:  2.31743970e-02\n",
      "Epoch: 1031 mean train loss:  3.18129476e-02, mean val. rec. loss:  2.31564053e-02\n",
      "Epoch: 1032 mean train loss:  3.17967019e-02, mean val. rec. loss:  2.31385293e-02\n",
      "Epoch: 1033 mean train loss:  3.17805567e-02, mean val. rec. loss:  2.31208121e-02\n",
      "Epoch: 1034 mean train loss:  3.17645158e-02, mean val. rec. loss:  2.31032196e-02\n",
      "Epoch: 1035 mean train loss:  3.17485680e-02, mean val. rec. loss:  2.30857631e-02\n",
      "Epoch: 1036 mean train loss:  3.17327096e-02, mean val. rec. loss:  2.30684519e-02\n",
      "Epoch: 1037 mean train loss:  3.17169592e-02, mean val. rec. loss:  2.30512789e-02\n",
      "Epoch: 1038 mean train loss:  3.17012907e-02, mean val. rec. loss:  2.30342352e-02\n",
      "Epoch: 1039 mean train loss:  3.16857116e-02, mean val. rec. loss:  2.30172982e-02\n",
      "Epoch: 1040 mean train loss:  3.16702294e-02, mean val. rec. loss:  2.30004926e-02\n",
      "Epoch: 1041 mean train loss:  3.16548179e-02, mean val. rec. loss:  2.29838323e-02\n",
      "Epoch: 1042 mean train loss:  3.16395069e-02, mean val. rec. loss:  2.29672830e-02\n",
      "Epoch: 1043 mean train loss:  3.16242779e-02, mean val. rec. loss:  2.29508630e-02\n",
      "Epoch: 1044 mean train loss:  3.16091197e-02, mean val. rec. loss:  2.29345678e-02\n",
      "Epoch: 1045 mean train loss:  3.15940583e-02, mean val. rec. loss:  2.29183927e-02\n",
      "Epoch: 1046 mean train loss:  3.15790677e-02, mean val. rec. loss:  2.29023356e-02\n",
      "Epoch: 1047 mean train loss:  3.15641515e-02, mean val. rec. loss:  2.28863874e-02\n",
      "Epoch: 1048 mean train loss:  3.15493062e-02, mean val. rec. loss:  2.28705571e-02\n",
      "Epoch: 1049 mean train loss:  3.15345502e-02, mean val. rec. loss:  2.28548311e-02\n",
      "Epoch: 1050 mean train loss:  3.15198612e-02, mean val. rec. loss:  2.28392276e-02\n",
      "Epoch: 1051 mean train loss:  3.15052467e-02, mean val. rec. loss:  2.28237261e-02\n",
      "Epoch: 1052 mean train loss:  3.14907030e-02, mean val. rec. loss:  2.28083471e-02\n",
      "Epoch: 1053 mean train loss:  3.14762264e-02, mean val. rec. loss:  2.27930702e-02\n",
      "Epoch: 1054 mean train loss:  3.14618204e-02, mean val. rec. loss:  2.27779089e-02\n",
      "Epoch: 1055 mean train loss:  3.14474741e-02, mean val. rec. loss:  2.27628565e-02\n",
      "Epoch: 1056 mean train loss:  3.14332098e-02, mean val. rec. loss:  2.27479084e-02\n",
      "Epoch: 1057 mean train loss:  3.14189901e-02, mean val. rec. loss:  2.27330556e-02\n",
      "Epoch: 1058 mean train loss:  3.14048449e-02, mean val. rec. loss:  2.27183026e-02\n",
      "Epoch: 1059 mean train loss:  3.13907593e-02, mean val. rec. loss:  2.27036357e-02\n",
      "Epoch: 1060 mean train loss:  3.13767407e-02, mean val. rec. loss:  2.26890732e-02\n",
      "Epoch: 1061 mean train loss:  3.13627817e-02, mean val. rec. loss:  2.26746240e-02\n",
      "Epoch: 1062 mean train loss:  3.13488749e-02, mean val. rec. loss:  2.26602611e-02\n",
      "Epoch: 1063 mean train loss:  3.13350313e-02, mean val. rec. loss:  2.26459979e-02\n",
      "Epoch: 1064 mean train loss:  3.13212400e-02, mean val. rec. loss:  2.26318368e-02\n",
      "Epoch: 1065 mean train loss:  3.13075119e-02, mean val. rec. loss:  2.26177619e-02\n",
      "Epoch: 1066 mean train loss:  3.12938397e-02, mean val. rec. loss:  2.26037596e-02\n",
      "Epoch: 1067 mean train loss:  3.12802234e-02, mean val. rec. loss:  2.25898616e-02\n",
      "Epoch: 1068 mean train loss:  3.12666554e-02, mean val. rec. loss:  2.25760407e-02\n",
      "Epoch: 1069 mean train loss:  3.12531397e-02, mean val. rec. loss:  2.25623422e-02\n",
      "Epoch: 1070 mean train loss:  3.12396779e-02, mean val. rec. loss:  2.25487118e-02\n",
      "Epoch: 1071 mean train loss:  3.12262682e-02, mean val. rec. loss:  2.25351812e-02\n",
      "Epoch: 1072 mean train loss:  3.12129089e-02, mean val. rec. loss:  2.25217164e-02\n",
      "Epoch: 1073 mean train loss:  3.11995980e-02, mean val. rec. loss:  2.25083309e-02\n",
      "Epoch: 1074 mean train loss:  3.11863373e-02, mean val. rec. loss:  2.24950294e-02\n",
      "Epoch: 1075 mean train loss:  3.11731269e-02, mean val. rec. loss:  2.24818117e-02\n",
      "Epoch: 1076 mean train loss:  3.11599575e-02, mean val. rec. loss:  2.24686553e-02\n",
      "Epoch: 1077 mean train loss:  3.11468495e-02, mean val. rec. loss:  2.24555942e-02\n",
      "Epoch: 1078 mean train loss:  3.11337658e-02, mean val. rec. loss:  2.24426079e-02\n",
      "Epoch: 1079 mean train loss:  3.11207342e-02, mean val. rec. loss:  2.24297032e-02\n",
      "Epoch: 1080 mean train loss:  3.11077622e-02, mean val. rec. loss:  2.24168825e-02\n",
      "Epoch: 1081 mean train loss:  3.10948106e-02, mean val. rec. loss:  2.24041230e-02\n",
      "Epoch: 1082 mean train loss:  3.10819112e-02, mean val. rec. loss:  2.23914429e-02\n",
      "Epoch: 1083 mean train loss:  3.10690547e-02, mean val. rec. loss:  2.23788263e-02\n",
      "Epoch: 1084 mean train loss:  3.10562503e-02, mean val. rec. loss:  2.23662981e-02\n",
      "Epoch: 1085 mean train loss:  3.10434775e-02, mean val. rec. loss:  2.23538266e-02\n",
      "Epoch: 1086 mean train loss:  3.10307494e-02, mean val. rec. loss:  2.23414186e-02\n",
      "Epoch: 1087 mean train loss:  3.10180567e-02, mean val. rec. loss:  2.23290832e-02\n",
      "Epoch: 1088 mean train loss:  3.10053976e-02, mean val. rec. loss:  2.23168045e-02\n",
      "Epoch: 1089 mean train loss:  3.09927868e-02, mean val. rec. loss:  2.23046030e-02\n",
      "Epoch: 1090 mean train loss:  3.09802133e-02, mean val. rec. loss:  2.22924785e-02\n",
      "Epoch: 1091 mean train loss:  3.09676677e-02, mean val. rec. loss:  2.22804039e-02\n",
      "Epoch: 1092 mean train loss:  3.09551705e-02, mean val. rec. loss:  2.22684064e-02\n",
      "Epoch: 1093 mean train loss:  3.09426995e-02, mean val. rec. loss:  2.22564566e-02\n",
      "Epoch: 1094 mean train loss:  3.09302693e-02, mean val. rec. loss:  2.22445566e-02\n",
      "Epoch: 1095 mean train loss:  3.09178765e-02, mean val. rec. loss:  2.22327270e-02\n",
      "Epoch: 1096 mean train loss:  3.09055134e-02, mean val. rec. loss:  2.22209540e-02\n",
      "Epoch: 1097 mean train loss:  3.08931894e-02, mean val. rec. loss:  2.22092333e-02\n",
      "Epoch: 1098 mean train loss:  3.08809027e-02, mean val. rec. loss:  2.21975941e-02\n",
      "Epoch: 1099 mean train loss:  3.08686346e-02, mean val. rec. loss:  2.21859981e-02\n",
      "Epoch: 1100 mean train loss:  3.08564149e-02, mean val. rec. loss:  2.21744610e-02\n",
      "Epoch: 1101 mean train loss:  3.08442212e-02, mean val. rec. loss:  2.21629806e-02\n",
      "Epoch: 1102 mean train loss:  3.08320555e-02, mean val. rec. loss:  2.21515388e-02\n",
      "Epoch: 1103 mean train loss:  3.08199345e-02, mean val. rec. loss:  2.21401673e-02\n",
      "Epoch: 1104 mean train loss:  3.08078303e-02, mean val. rec. loss:  2.21288388e-02\n",
      "Epoch: 1105 mean train loss:  3.07957577e-02, mean val. rec. loss:  2.21175580e-02\n",
      "Epoch: 1106 mean train loss:  3.07837205e-02, mean val. rec. loss:  2.21063226e-02\n",
      "Epoch: 1107 mean train loss:  3.07717169e-02, mean val. rec. loss:  2.20951484e-02\n",
      "Epoch: 1108 mean train loss:  3.07597299e-02, mean val. rec. loss:  2.20840150e-02\n",
      "Epoch: 1109 mean train loss:  3.07477765e-02, mean val. rec. loss:  2.20729315e-02\n",
      "Epoch: 1110 mean train loss:  3.07358566e-02, mean val. rec. loss:  2.20618866e-02\n",
      "Epoch: 1111 mean train loss:  3.07239461e-02, mean val. rec. loss:  2.20509074e-02\n",
      "Epoch: 1112 mean train loss:  3.07120802e-02, mean val. rec. loss:  2.20399668e-02\n",
      "Epoch: 1113 mean train loss:  3.07002404e-02, mean val. rec. loss:  2.20290580e-02\n",
      "Epoch: 1114 mean train loss:  3.06884267e-02, mean val. rec. loss:  2.20182058e-02\n",
      "Epoch: 1115 mean train loss:  3.06766371e-02, mean val. rec. loss:  2.20073922e-02\n",
      "Epoch: 1116 mean train loss:  3.06648774e-02, mean val. rec. loss:  2.19966149e-02\n",
      "Epoch: 1117 mean train loss:  3.06531270e-02, mean val. rec. loss:  2.19858920e-02\n",
      "Epoch: 1118 mean train loss:  3.06414101e-02, mean val. rec. loss:  2.19751986e-02\n",
      "Epoch: 1119 mean train loss:  3.06297248e-02, mean val. rec. loss:  2.19645415e-02\n",
      "Epoch: 1120 mean train loss:  3.06180582e-02, mean val. rec. loss:  2.19539275e-02\n",
      "Epoch: 1121 mean train loss:  3.06064158e-02, mean val. rec. loss:  2.19433543e-02\n",
      "Epoch: 1122 mean train loss:  3.05947920e-02, mean val. rec. loss:  2.19328310e-02\n",
      "Epoch: 1123 mean train loss:  3.05831905e-02, mean val. rec. loss:  2.19223258e-02\n",
      "Epoch: 1124 mean train loss:  3.05716152e-02, mean val. rec. loss:  2.19118683e-02\n",
      "Epoch: 1125 mean train loss:  3.05600659e-02, mean val. rec. loss:  2.19014334e-02\n",
      "Epoch: 1126 mean train loss:  3.05485277e-02, mean val. rec. loss:  2.18910349e-02\n",
      "Epoch: 1127 mean train loss:  3.05370157e-02, mean val. rec. loss:  2.18806794e-02\n",
      "Epoch: 1128 mean train loss:  3.05255297e-02, mean val. rec. loss:  2.18703580e-02\n",
      "Epoch: 1129 mean train loss:  3.05140511e-02, mean val. rec. loss:  2.18600614e-02\n",
      "Epoch: 1130 mean train loss:  3.05026080e-02, mean val. rec. loss:  2.18498126e-02\n",
      "Epoch: 1131 mean train loss:  3.04911685e-02, mean val. rec. loss:  2.18395796e-02\n",
      "Epoch: 1132 mean train loss:  3.04797682e-02, mean val. rec. loss:  2.18293692e-02\n",
      "Epoch: 1133 mean train loss:  3.04683679e-02, mean val. rec. loss:  2.18191975e-02\n",
      "Epoch: 1134 mean train loss:  3.04569899e-02, mean val. rec. loss:  2.18090552e-02\n",
      "Epoch: 1135 mean train loss:  3.04456380e-02, mean val. rec. loss:  2.17989401e-02\n",
      "Epoch: 1136 mean train loss:  3.04342954e-02, mean val. rec. loss:  2.17888568e-02\n",
      "Epoch: 1137 mean train loss:  3.04229788e-02, mean val. rec. loss:  2.17788098e-02\n",
      "Epoch: 1138 mean train loss:  3.04116679e-02, mean val. rec. loss:  2.17687673e-02\n",
      "Epoch: 1139 mean train loss:  3.04003719e-02, mean val. rec. loss:  2.17587701e-02\n",
      "Epoch: 1140 mean train loss:  3.03891149e-02, mean val. rec. loss:  2.17487821e-02\n",
      "Epoch: 1141 mean train loss:  3.03778431e-02, mean val. rec. loss:  2.17388235e-02\n",
      "Epoch: 1142 mean train loss:  3.03666141e-02, mean val. rec. loss:  2.17288966e-02\n",
      "Epoch: 1143 mean train loss:  3.03553851e-02, mean val. rec. loss:  2.17189811e-02\n",
      "Epoch: 1144 mean train loss:  3.03441691e-02, mean val. rec. loss:  2.17090951e-02\n",
      "Epoch: 1145 mean train loss:  3.03329773e-02, mean val. rec. loss:  2.16992409e-02\n",
      "Epoch: 1146 mean train loss:  3.03217949e-02, mean val. rec. loss:  2.16894138e-02\n",
      "Epoch: 1147 mean train loss:  3.03106217e-02, mean val. rec. loss:  2.16795913e-02\n",
      "Epoch: 1148 mean train loss:  3.02994635e-02, mean val. rec. loss:  2.16697892e-02\n",
      "Epoch: 1149 mean train loss:  3.02883201e-02, mean val. rec. loss:  2.16600234e-02\n",
      "Epoch: 1150 mean train loss:  3.02771917e-02, mean val. rec. loss:  2.16502576e-02\n",
      "Epoch: 1151 mean train loss:  3.02660800e-02, mean val. rec. loss:  2.16405190e-02\n",
      "Epoch: 1152 mean train loss:  3.02549646e-02, mean val. rec. loss:  2.16307963e-02\n",
      "Epoch: 1153 mean train loss:  3.02438771e-02, mean val. rec. loss:  2.16210918e-02\n",
      "Epoch: 1154 mean train loss:  3.02327859e-02, mean val. rec. loss:  2.16113985e-02\n",
      "Epoch: 1155 mean train loss:  3.02217207e-02, mean val. rec. loss:  2.16017416e-02\n",
      "Epoch: 1156 mean train loss:  3.02106649e-02, mean val. rec. loss:  2.15920937e-02\n",
      "Epoch: 1157 mean train loss:  3.01996016e-02, mean val. rec. loss:  2.15824662e-02\n",
      "Epoch: 1158 mean train loss:  3.01885607e-02, mean val. rec. loss:  2.15728547e-02\n",
      "Epoch: 1159 mean train loss:  3.01775310e-02, mean val. rec. loss:  2.15632453e-02\n",
      "Epoch: 1160 mean train loss:  3.01665068e-02, mean val. rec. loss:  2.15536519e-02\n",
      "Epoch: 1161 mean train loss:  3.01554938e-02, mean val. rec. loss:  2.15440675e-02\n",
      "Epoch: 1162 mean train loss:  3.01444845e-02, mean val. rec. loss:  2.15344990e-02\n",
      "Epoch: 1163 mean train loss:  3.01334790e-02, mean val. rec. loss:  2.15249487e-02\n",
      "Epoch: 1164 mean train loss:  3.01224995e-02, mean val. rec. loss:  2.15154097e-02\n",
      "Epoch: 1165 mean train loss:  3.01114996e-02, mean val. rec. loss:  2.15058843e-02\n",
      "Epoch: 1166 mean train loss:  3.01005331e-02, mean val. rec. loss:  2.14963725e-02\n",
      "Epoch: 1167 mean train loss:  3.00895648e-02, mean val. rec. loss:  2.14868697e-02\n",
      "Epoch: 1168 mean train loss:  3.00786021e-02, mean val. rec. loss:  2.14773716e-02\n",
      "Epoch: 1169 mean train loss:  3.00676468e-02, mean val. rec. loss:  2.14678802e-02\n",
      "Epoch: 1170 mean train loss:  3.00566897e-02, mean val. rec. loss:  2.14584160e-02\n",
      "Epoch: 1171 mean train loss:  3.00457363e-02, mean val. rec. loss:  2.14489496e-02\n",
      "Epoch: 1172 mean train loss:  3.00348090e-02, mean val. rec. loss:  2.14394854e-02\n",
      "Epoch: 1173 mean train loss:  3.00238593e-02, mean val. rec. loss:  2.14300371e-02\n",
      "Epoch: 1174 mean train loss:  3.00129357e-02, mean val. rec. loss:  2.14205956e-02\n",
      "Epoch: 1175 mean train loss:  3.00020121e-02, mean val. rec. loss:  2.14111609e-02\n",
      "Epoch: 1176 mean train loss:  2.99910810e-02, mean val. rec. loss:  2.14017444e-02\n",
      "Epoch: 1177 mean train loss:  2.99801574e-02, mean val. rec. loss:  2.13923142e-02\n",
      "Epoch: 1178 mean train loss:  2.99692338e-02, mean val. rec. loss:  2.13829000e-02\n",
      "Epoch: 1179 mean train loss:  2.99583325e-02, mean val. rec. loss:  2.13734902e-02\n",
      "Epoch: 1180 mean train loss:  2.99474070e-02, mean val. rec. loss:  2.13640850e-02\n",
      "Epoch: 1181 mean train loss:  2.99364965e-02, mean val. rec. loss:  2.13546798e-02\n",
      "Epoch: 1182 mean train loss:  2.99255840e-02, mean val. rec. loss:  2.13452860e-02\n",
      "Epoch: 1183 mean train loss:  2.99146846e-02, mean val. rec. loss:  2.13358966e-02\n",
      "Epoch: 1184 mean train loss:  2.99037722e-02, mean val. rec. loss:  2.13265028e-02\n",
      "Epoch: 1185 mean train loss:  2.98928765e-02, mean val. rec. loss:  2.13171225e-02\n",
      "Epoch: 1186 mean train loss:  2.98819604e-02, mean val. rec. loss:  2.13077423e-02\n",
      "Epoch: 1187 mean train loss:  2.98710591e-02, mean val. rec. loss:  2.12983461e-02\n",
      "Epoch: 1188 mean train loss:  2.98601578e-02, mean val. rec. loss:  2.12889682e-02\n",
      "Epoch: 1189 mean train loss:  2.98492398e-02, mean val. rec. loss:  2.12795970e-02\n",
      "Epoch: 1190 mean train loss:  2.98383404e-02, mean val. rec. loss:  2.12702031e-02\n",
      "Epoch: 1191 mean train loss:  2.98274354e-02, mean val. rec. loss:  2.12608297e-02\n",
      "Epoch: 1192 mean train loss:  2.98165137e-02, mean val. rec. loss:  2.12514562e-02\n",
      "Epoch: 1193 mean train loss:  2.98056180e-02, mean val. rec. loss:  2.12420850e-02\n",
      "Epoch: 1194 mean train loss:  2.97946981e-02, mean val. rec. loss:  2.12327161e-02\n",
      "Epoch: 1195 mean train loss:  2.97837931e-02, mean val. rec. loss:  2.12233359e-02\n",
      "Epoch: 1196 mean train loss:  2.97728565e-02, mean val. rec. loss:  2.12139669e-02\n",
      "Epoch: 1197 mean train loss:  2.97619440e-02, mean val. rec. loss:  2.12045822e-02\n",
      "Epoch: 1198 mean train loss:  2.97510186e-02, mean val. rec. loss:  2.11951860e-02\n",
      "Epoch: 1199 mean train loss:  2.97400838e-02, mean val. rec. loss:  2.11857944e-02\n",
      "Epoch: 1200 mean train loss:  2.97291602e-02, mean val. rec. loss:  2.11764074e-02\n",
      "Epoch: 1201 mean train loss:  2.97182254e-02, mean val. rec. loss:  2.11670112e-02\n",
      "Epoch: 1202 mean train loss:  2.97072832e-02, mean val. rec. loss:  2.11576219e-02\n",
      "Epoch: 1203 mean train loss:  2.96963335e-02, mean val. rec. loss:  2.11482349e-02\n",
      "Epoch: 1204 mean train loss:  2.96853838e-02, mean val. rec. loss:  2.11388297e-02\n",
      "Epoch: 1205 mean train loss:  2.96744285e-02, mean val. rec. loss:  2.11294290e-02\n",
      "Epoch: 1206 mean train loss:  2.96634584e-02, mean val. rec. loss:  2.11200056e-02\n",
      "Epoch: 1207 mean train loss:  2.96524938e-02, mean val. rec. loss:  2.11105755e-02\n",
      "Epoch: 1208 mean train loss:  2.96415218e-02, mean val. rec. loss:  2.11011521e-02\n",
      "Epoch: 1209 mean train loss:  2.96305386e-02, mean val. rec. loss:  2.10917333e-02\n",
      "Epoch: 1210 mean train loss:  2.96195442e-02, mean val. rec. loss:  2.10822941e-02\n",
      "Epoch: 1211 mean train loss:  2.96085498e-02, mean val. rec. loss:  2.10728504e-02\n",
      "Epoch: 1212 mean train loss:  2.95975461e-02, mean val. rec. loss:  2.10633930e-02\n",
      "Epoch: 1213 mean train loss:  2.95865387e-02, mean val. rec. loss:  2.10539379e-02\n",
      "Epoch: 1214 mean train loss:  2.95755239e-02, mean val. rec. loss:  2.10444828e-02\n",
      "Epoch: 1215 mean train loss:  2.95644829e-02, mean val. rec. loss:  2.10350118e-02\n",
      "Epoch: 1216 mean train loss:  2.95534513e-02, mean val. rec. loss:  2.10255182e-02\n",
      "Epoch: 1217 mean train loss:  2.95424048e-02, mean val. rec. loss:  2.10160200e-02\n",
      "Epoch: 1218 mean train loss:  2.95313434e-02, mean val. rec. loss:  2.10065173e-02\n",
      "Epoch: 1219 mean train loss:  2.95202764e-02, mean val. rec. loss:  2.09970100e-02\n",
      "Epoch: 1220 mean train loss:  2.95092131e-02, mean val. rec. loss:  2.09874959e-02\n",
      "Epoch: 1221 mean train loss:  2.94981275e-02, mean val. rec. loss:  2.09779660e-02\n",
      "Epoch: 1222 mean train loss:  2.94870195e-02, mean val. rec. loss:  2.09684225e-02\n",
      "Epoch: 1223 mean train loss:  2.94759097e-02, mean val. rec. loss:  2.09588744e-02\n",
      "Epoch: 1224 mean train loss:  2.94648036e-02, mean val. rec. loss:  2.09493059e-02\n",
      "Epoch: 1225 mean train loss:  2.94536733e-02, mean val. rec. loss:  2.09397328e-02\n",
      "Epoch: 1226 mean train loss:  2.94425262e-02, mean val. rec. loss:  2.09301462e-02\n",
      "Epoch: 1227 mean train loss:  2.94313717e-02, mean val. rec. loss:  2.09205528e-02\n",
      "Epoch: 1228 mean train loss:  2.94202153e-02, mean val. rec. loss:  2.09109548e-02\n",
      "Epoch: 1229 mean train loss:  2.94090254e-02, mean val. rec. loss:  2.09013296e-02\n",
      "Epoch: 1230 mean train loss:  2.93978467e-02, mean val. rec. loss:  2.08916908e-02\n",
      "Epoch: 1231 mean train loss:  2.93866363e-02, mean val. rec. loss:  2.08820475e-02\n",
      "Epoch: 1232 mean train loss:  2.93754222e-02, mean val. rec. loss:  2.08723792e-02\n",
      "Epoch: 1233 mean train loss:  2.93641820e-02, mean val. rec. loss:  2.08627109e-02\n",
      "Epoch: 1234 mean train loss:  2.93529381e-02, mean val. rec. loss:  2.08530177e-02\n",
      "Epoch: 1235 mean train loss:  2.93416905e-02, mean val. rec. loss:  2.08433154e-02\n",
      "Epoch: 1236 mean train loss:  2.93304131e-02, mean val. rec. loss:  2.08336040e-02\n",
      "Epoch: 1237 mean train loss:  2.93191282e-02, mean val. rec. loss:  2.08238745e-02\n",
      "Epoch: 1238 mean train loss:  2.93078135e-02, mean val. rec. loss:  2.08141223e-02\n",
      "Epoch: 1239 mean train loss:  2.92964951e-02, mean val. rec. loss:  2.08043701e-02\n",
      "Epoch: 1240 mean train loss:  2.92851655e-02, mean val. rec. loss:  2.07945793e-02\n",
      "Epoch: 1241 mean train loss:  2.92738080e-02, mean val. rec. loss:  2.07847840e-02\n",
      "Epoch: 1242 mean train loss:  2.92624431e-02, mean val. rec. loss:  2.07749660e-02\n",
      "Epoch: 1243 mean train loss:  2.92510726e-02, mean val. rec. loss:  2.07651299e-02\n",
      "Epoch: 1244 mean train loss:  2.92396685e-02, mean val. rec. loss:  2.07552870e-02\n",
      "Epoch: 1245 mean train loss:  2.92282533e-02, mean val. rec. loss:  2.07454441e-02\n",
      "Epoch: 1246 mean train loss:  2.92168287e-02, mean val. rec. loss:  2.07355740e-02\n",
      "Epoch: 1247 mean train loss:  2.92053688e-02, mean val. rec. loss:  2.07256744e-02\n",
      "Epoch: 1248 mean train loss:  2.91939070e-02, mean val. rec. loss:  2.07157657e-02\n",
      "Epoch: 1249 mean train loss:  2.91824248e-02, mean val. rec. loss:  2.07058298e-02\n",
      "Epoch: 1250 mean train loss:  2.91709053e-02, mean val. rec. loss:  2.06958848e-02\n",
      "Epoch: 1251 mean train loss:  2.91593932e-02, mean val. rec. loss:  2.06859126e-02\n",
      "Epoch: 1252 mean train loss:  2.91478513e-02, mean val. rec. loss:  2.06759132e-02\n",
      "Epoch: 1253 mean train loss:  2.91362871e-02, mean val. rec. loss:  2.06659002e-02\n",
      "Epoch: 1254 mean train loss:  2.91247099e-02, mean val. rec. loss:  2.06558690e-02\n",
      "Epoch: 1255 mean train loss:  2.91131178e-02, mean val. rec. loss:  2.06458401e-02\n",
      "Epoch: 1256 mean train loss:  2.91014940e-02, mean val. rec. loss:  2.06357795e-02\n",
      "Epoch: 1257 mean train loss:  2.90898553e-02, mean val. rec. loss:  2.06256984e-02\n",
      "Epoch: 1258 mean train loss:  2.90782091e-02, mean val. rec. loss:  2.06155879e-02\n",
      "Epoch: 1259 mean train loss:  2.90665239e-02, mean val. rec. loss:  2.06054570e-02\n",
      "Epoch: 1260 mean train loss:  2.90548275e-02, mean val. rec. loss:  2.05953056e-02\n",
      "Epoch: 1261 mean train loss:  2.90431106e-02, mean val. rec. loss:  2.05851248e-02\n",
      "Epoch: 1262 mean train loss:  2.90313527e-02, mean val. rec. loss:  2.05749348e-02\n",
      "Epoch: 1263 mean train loss:  2.90196041e-02, mean val. rec. loss:  2.05647268e-02\n",
      "Epoch: 1264 mean train loss:  2.90078202e-02, mean val. rec. loss:  2.05544892e-02\n",
      "Epoch: 1265 mean train loss:  2.89960195e-02, mean val. rec. loss:  2.05442268e-02\n",
      "Epoch: 1266 mean train loss:  2.89841834e-02, mean val. rec. loss:  2.05339484e-02\n",
      "Epoch: 1267 mean train loss:  2.89723287e-02, mean val. rec. loss:  2.05236428e-02\n",
      "Epoch: 1268 mean train loss:  2.89604647e-02, mean val. rec. loss:  2.05133282e-02\n",
      "Epoch: 1269 mean train loss:  2.89485634e-02, mean val. rec. loss:  2.05029659e-02\n",
      "Epoch: 1270 mean train loss:  2.89366417e-02, mean val. rec. loss:  2.04925945e-02\n",
      "Epoch: 1271 mean train loss:  2.89247013e-02, mean val. rec. loss:  2.04822005e-02\n",
      "Epoch: 1272 mean train loss:  2.89127386e-02, mean val. rec. loss:  2.04717883e-02\n",
      "Epoch: 1273 mean train loss:  2.89007536e-02, mean val. rec. loss:  2.04613399e-02\n",
      "Epoch: 1274 mean train loss:  2.88887499e-02, mean val. rec. loss:  2.04508597e-02\n",
      "Epoch: 1275 mean train loss:  2.88767015e-02, mean val. rec. loss:  2.04403726e-02\n",
      "Epoch: 1276 mean train loss:  2.88646569e-02, mean val. rec. loss:  2.04298607e-02\n",
      "Epoch: 1277 mean train loss:  2.88525601e-02, mean val. rec. loss:  2.04193170e-02\n",
      "Epoch: 1278 mean train loss:  2.88404484e-02, mean val. rec. loss:  2.04087551e-02\n",
      "Epoch: 1279 mean train loss:  2.88283218e-02, mean val. rec. loss:  2.03981570e-02\n",
      "Epoch: 1280 mean train loss:  2.88161580e-02, mean val. rec. loss:  2.03875339e-02\n",
      "Epoch: 1281 mean train loss:  2.88039755e-02, mean val. rec. loss:  2.03768926e-02\n",
      "Epoch: 1282 mean train loss:  2.87917651e-02, mean val. rec. loss:  2.03662287e-02\n",
      "Epoch: 1283 mean train loss:  2.87795287e-02, mean val. rec. loss:  2.03555262e-02\n",
      "Epoch: 1284 mean train loss:  2.87672606e-02, mean val. rec. loss:  2.03448147e-02\n",
      "Epoch: 1285 mean train loss:  2.87549776e-02, mean val. rec. loss:  2.03340601e-02\n",
      "Epoch: 1286 mean train loss:  2.87426759e-02, mean val. rec. loss:  2.03232873e-02\n",
      "Epoch: 1287 mean train loss:  2.87303296e-02, mean val. rec. loss:  2.03124828e-02\n",
      "Epoch: 1288 mean train loss:  2.87179535e-02, mean val. rec. loss:  2.03016556e-02\n",
      "Epoch: 1289 mean train loss:  2.87055681e-02, mean val. rec. loss:  2.02907921e-02\n",
      "Epoch: 1290 mean train loss:  2.86931417e-02, mean val. rec. loss:  2.02799195e-02\n",
      "Epoch: 1291 mean train loss:  2.86806911e-02, mean val. rec. loss:  2.02690084e-02\n",
      "Epoch: 1292 mean train loss:  2.86682218e-02, mean val. rec. loss:  2.02580610e-02\n",
      "Epoch: 1293 mean train loss:  2.86557191e-02, mean val. rec. loss:  2.02471022e-02\n",
      "Epoch: 1294 mean train loss:  2.86431847e-02, mean val. rec. loss:  2.02361049e-02\n",
      "Epoch: 1295 mean train loss:  2.86306186e-02, mean val. rec. loss:  2.02250713e-02\n",
      "Epoch: 1296 mean train loss:  2.86180395e-02, mean val. rec. loss:  2.02140128e-02\n",
      "Epoch: 1297 mean train loss:  2.86054250e-02, mean val. rec. loss:  2.02029293e-02\n",
      "Epoch: 1298 mean train loss:  2.85927808e-02, mean val. rec. loss:  2.01918254e-02\n",
      "Epoch: 1299 mean train loss:  2.85801011e-02, mean val. rec. loss:  2.01806920e-02\n",
      "Epoch: 1300 mean train loss:  2.85674084e-02, mean val. rec. loss:  2.01695337e-02\n",
      "Epoch: 1301 mean train loss:  2.85546692e-02, mean val. rec. loss:  2.01583323e-02\n",
      "Epoch: 1302 mean train loss:  2.85419057e-02, mean val. rec. loss:  2.01471036e-02\n",
      "Epoch: 1303 mean train loss:  2.85291236e-02, mean val. rec. loss:  2.01358410e-02\n",
      "Epoch: 1304 mean train loss:  2.85163081e-02, mean val. rec. loss:  2.01245556e-02\n",
      "Epoch: 1305 mean train loss:  2.85034534e-02, mean val. rec. loss:  2.01132272e-02\n",
      "Epoch: 1306 mean train loss:  2.84905651e-02, mean val. rec. loss:  2.01018761e-02\n",
      "Epoch: 1307 mean train loss:  2.84776564e-02, mean val. rec. loss:  2.00904955e-02\n",
      "Epoch: 1308 mean train loss:  2.84647161e-02, mean val. rec. loss:  2.00790922e-02\n",
      "Epoch: 1309 mean train loss:  2.84517496e-02, mean val. rec. loss:  2.00676527e-02\n",
      "Epoch: 1310 mean train loss:  2.84387478e-02, mean val. rec. loss:  2.00561904e-02\n",
      "Epoch: 1311 mean train loss:  2.84257237e-02, mean val. rec. loss:  2.00446919e-02\n",
      "Epoch: 1312 mean train loss:  2.84126697e-02, mean val. rec. loss:  2.00331526e-02\n",
      "Epoch: 1313 mean train loss:  2.83995766e-02, mean val. rec. loss:  2.00215996e-02\n",
      "Epoch: 1314 mean train loss:  2.83864482e-02, mean val. rec. loss:  2.00100172e-02\n",
      "Epoch: 1315 mean train loss:  2.83732918e-02, mean val. rec. loss:  1.99983849e-02\n",
      "Epoch: 1316 mean train loss:  2.83601094e-02, mean val. rec. loss:  1.99867253e-02\n",
      "Epoch: 1317 mean train loss:  2.83468841e-02, mean val. rec. loss:  1.99750227e-02\n",
      "Epoch: 1318 mean train loss:  2.83336421e-02, mean val. rec. loss:  1.99632974e-02\n",
      "Epoch: 1319 mean train loss:  2.83203646e-02, mean val. rec. loss:  1.99515403e-02\n",
      "Epoch: 1320 mean train loss:  2.83070556e-02, mean val. rec. loss:  1.99397583e-02\n",
      "Epoch: 1321 mean train loss:  2.82937092e-02, mean val. rec. loss:  1.99279513e-02\n",
      "Epoch: 1322 mean train loss:  2.82803275e-02, mean val. rec. loss:  1.99161126e-02\n",
      "Epoch: 1323 mean train loss:  2.82669105e-02, mean val. rec. loss:  1.99042330e-02\n",
      "Epoch: 1324 mean train loss:  2.82534785e-02, mean val. rec. loss:  1.98923240e-02\n",
      "Epoch: 1325 mean train loss:  2.82399999e-02, mean val. rec. loss:  1.98803628e-02\n",
      "Epoch: 1326 mean train loss:  2.82264953e-02, mean val. rec. loss:  1.98683744e-02\n",
      "Epoch: 1327 mean train loss:  2.82129535e-02, mean val. rec. loss:  1.98563611e-02\n",
      "Epoch: 1328 mean train loss:  2.81993874e-02, mean val. rec. loss:  1.98443092e-02\n",
      "Epoch: 1329 mean train loss:  2.81857841e-02, mean val. rec. loss:  1.98322437e-02\n",
      "Epoch: 1330 mean train loss:  2.81721417e-02, mean val. rec. loss:  1.98201192e-02\n",
      "Epoch: 1331 mean train loss:  2.81584695e-02, mean val. rec. loss:  1.98079811e-02\n",
      "Epoch: 1332 mean train loss:  2.81447787e-02, mean val. rec. loss:  1.97958022e-02\n",
      "Epoch: 1333 mean train loss:  2.81310283e-02, mean val. rec. loss:  1.97836074e-02\n",
      "Epoch: 1334 mean train loss:  2.81172592e-02, mean val. rec. loss:  1.97713537e-02\n",
      "Epoch: 1335 mean train loss:  2.81034641e-02, mean val. rec. loss:  1.97590863e-02\n",
      "Epoch: 1336 mean train loss:  2.80896299e-02, mean val. rec. loss:  1.97467759e-02\n",
      "Epoch: 1337 mean train loss:  2.80757547e-02, mean val. rec. loss:  1.97344427e-02\n",
      "Epoch: 1338 mean train loss:  2.80618405e-02, mean val. rec. loss:  1.97220484e-02\n",
      "Epoch: 1339 mean train loss:  2.80479001e-02, mean val. rec. loss:  1.97096336e-02\n",
      "Epoch: 1340 mean train loss:  2.80339411e-02, mean val. rec. loss:  1.96971689e-02\n",
      "Epoch: 1341 mean train loss:  2.80199207e-02, mean val. rec. loss:  1.96846974e-02\n",
      "Epoch: 1342 mean train loss:  2.80058816e-02, mean val. rec. loss:  1.96721738e-02\n",
      "Epoch: 1343 mean train loss:  2.79918109e-02, mean val. rec. loss:  1.96596411e-02\n",
      "Epoch: 1344 mean train loss:  2.79776992e-02, mean val. rec. loss:  1.96470585e-02\n",
      "Epoch: 1345 mean train loss:  2.79635540e-02, mean val. rec. loss:  1.96344532e-02\n",
      "Epoch: 1346 mean train loss:  2.79493865e-02, mean val. rec. loss:  1.96218003e-02\n",
      "Epoch: 1347 mean train loss:  2.79351631e-02, mean val. rec. loss:  1.96091292e-02\n",
      "Epoch: 1348 mean train loss:  2.79209211e-02, mean val. rec. loss:  1.95963947e-02\n",
      "Epoch: 1349 mean train loss:  2.79066436e-02, mean val. rec. loss:  1.95836443e-02\n",
      "Epoch: 1350 mean train loss:  2.78923327e-02, mean val. rec. loss:  1.95708643e-02\n",
      "Epoch: 1351 mean train loss:  2.78779715e-02, mean val. rec. loss:  1.95580368e-02\n",
      "Epoch: 1352 mean train loss:  2.78635861e-02, mean val. rec. loss:  1.95451798e-02\n",
      "Epoch: 1353 mean train loss:  2.78491727e-02, mean val. rec. loss:  1.95322797e-02\n",
      "Epoch: 1354 mean train loss:  2.78347128e-02, mean val. rec. loss:  1.95193614e-02\n",
      "Epoch: 1355 mean train loss:  2.78202250e-02, mean val. rec. loss:  1.95063910e-02\n",
      "Epoch: 1356 mean train loss:  2.78057073e-02, mean val. rec. loss:  1.94934092e-02\n",
      "Epoch: 1357 mean train loss:  2.77911394e-02, mean val. rec. loss:  1.94803843e-02\n",
      "Epoch: 1358 mean train loss:  2.77765417e-02, mean val. rec. loss:  1.94673300e-02\n",
      "Epoch: 1359 mean train loss:  2.77619254e-02, mean val. rec. loss:  1.94542303e-02\n",
      "Epoch: 1360 mean train loss:  2.77472494e-02, mean val. rec. loss:  1.94411057e-02\n",
      "Epoch: 1361 mean train loss:  2.77325568e-02, mean val. rec. loss:  1.94279357e-02\n",
      "Epoch: 1362 mean train loss:  2.77178306e-02, mean val. rec. loss:  1.94147362e-02\n",
      "Epoch: 1363 mean train loss:  2.77030634e-02, mean val. rec. loss:  1.94015095e-02\n",
      "Epoch: 1364 mean train loss:  2.76882515e-02, mean val. rec. loss:  1.93882283e-02\n",
      "Epoch: 1365 mean train loss:  2.76734099e-02, mean val. rec. loss:  1.93749291e-02\n",
      "Epoch: 1366 mean train loss:  2.76585496e-02, mean val. rec. loss:  1.93615980e-02\n",
      "Epoch: 1367 mean train loss:  2.76436334e-02, mean val. rec. loss:  1.93482262e-02\n",
      "Epoch: 1368 mean train loss:  2.76286838e-02, mean val. rec. loss:  1.93348226e-02\n",
      "Epoch: 1369 mean train loss:  2.76137043e-02, mean val. rec. loss:  1.93213850e-02\n",
      "Epoch: 1370 mean train loss:  2.75986932e-02, mean val. rec. loss:  1.93079156e-02\n",
      "Epoch: 1371 mean train loss:  2.75836486e-02, mean val. rec. loss:  1.92944031e-02\n",
      "Epoch: 1372 mean train loss:  2.75685760e-02, mean val. rec. loss:  1.92808725e-02\n",
      "Epoch: 1373 mean train loss:  2.75534438e-02, mean val. rec. loss:  1.92673147e-02\n",
      "Epoch: 1374 mean train loss:  2.75382968e-02, mean val. rec. loss:  1.92536979e-02\n",
      "Epoch: 1375 mean train loss:  2.75231125e-02, mean val. rec. loss:  1.92400630e-02\n",
      "Epoch: 1376 mean train loss:  2.75078946e-02, mean val. rec. loss:  1.92263827e-02\n",
      "Epoch: 1377 mean train loss:  2.74926396e-02, mean val. rec. loss:  1.92126820e-02\n",
      "Epoch: 1378 mean train loss:  2.74773361e-02, mean val. rec. loss:  1.91989268e-02\n",
      "Epoch: 1379 mean train loss:  2.74620028e-02, mean val. rec. loss:  1.91851535e-02\n",
      "Epoch: 1380 mean train loss:  2.74466584e-02, mean val. rec. loss:  1.91713372e-02\n",
      "Epoch: 1381 mean train loss:  2.74312469e-02, mean val. rec. loss:  1.91575117e-02\n",
      "Epoch: 1382 mean train loss:  2.74158205e-02, mean val. rec. loss:  1.91436319e-02\n",
      "Epoch: 1383 mean train loss:  2.74003643e-02, mean val. rec. loss:  1.91297202e-02\n",
      "Epoch: 1384 mean train loss:  2.73848616e-02, mean val. rec. loss:  1.91157746e-02\n",
      "Epoch: 1385 mean train loss:  2.73693346e-02, mean val. rec. loss:  1.91018040e-02\n",
      "Epoch: 1386 mean train loss:  2.73537574e-02, mean val. rec. loss:  1.90877948e-02\n",
      "Epoch: 1387 mean train loss:  2.73381523e-02, mean val. rec. loss:  1.90737698e-02\n",
      "Epoch: 1388 mean train loss:  2.73225229e-02, mean val. rec. loss:  1.90597017e-02\n",
      "Epoch: 1389 mean train loss:  2.73068470e-02, mean val. rec. loss:  1.90456086e-02\n",
      "Epoch: 1390 mean train loss:  2.72911431e-02, mean val. rec. loss:  1.90314725e-02\n",
      "Epoch: 1391 mean train loss:  2.72754169e-02, mean val. rec. loss:  1.90172944e-02\n",
      "Epoch: 1392 mean train loss:  2.72596330e-02, mean val. rec. loss:  1.90030834e-02\n",
      "Epoch: 1393 mean train loss:  2.72438323e-02, mean val. rec. loss:  1.89888509e-02\n",
      "Epoch: 1394 mean train loss:  2.72279925e-02, mean val. rec. loss:  1.89745888e-02\n",
      "Epoch: 1395 mean train loss:  2.72121248e-02, mean val. rec. loss:  1.89603132e-02\n",
      "Epoch: 1396 mean train loss:  2.71962086e-02, mean val. rec. loss:  1.89459775e-02\n",
      "Epoch: 1397 mean train loss:  2.71802664e-02, mean val. rec. loss:  1.89316236e-02\n",
      "Epoch: 1398 mean train loss:  2.71643019e-02, mean val. rec. loss:  1.89172244e-02\n",
      "Epoch: 1399 mean train loss:  2.71482982e-02, mean val. rec. loss:  1.89028115e-02\n",
      "Epoch: 1400 mean train loss:  2.71322610e-02, mean val. rec. loss:  1.88883669e-02\n",
      "Epoch: 1401 mean train loss:  2.71161847e-02, mean val. rec. loss:  1.88738928e-02\n",
      "Epoch: 1402 mean train loss:  2.71000842e-02, mean val. rec. loss:  1.88593757e-02\n",
      "Epoch: 1403 mean train loss:  2.70839409e-02, mean val. rec. loss:  1.88448506e-02\n",
      "Epoch: 1404 mean train loss:  2.70677771e-02, mean val. rec. loss:  1.88302642e-02\n",
      "Epoch: 1405 mean train loss:  2.70515891e-02, mean val. rec. loss:  1.88156631e-02\n",
      "Epoch: 1406 mean train loss:  2.70353489e-02, mean val. rec. loss:  1.88010235e-02\n",
      "Epoch: 1407 mean train loss:  2.70190808e-02, mean val. rec. loss:  1.87863635e-02\n",
      "Epoch: 1408 mean train loss:  2.70027978e-02, mean val. rec. loss:  1.87716943e-02\n",
      "Epoch: 1409 mean train loss:  2.69864627e-02, mean val. rec. loss:  1.87569708e-02\n",
      "Epoch: 1410 mean train loss:  2.69701071e-02, mean val. rec. loss:  1.87422336e-02\n",
      "Epoch: 1411 mean train loss:  2.69537142e-02, mean val. rec. loss:  1.87274500e-02\n",
      "Epoch: 1412 mean train loss:  2.69373008e-02, mean val. rec. loss:  1.87126572e-02\n",
      "Epoch: 1413 mean train loss:  2.69208428e-02, mean val. rec. loss:  1.86978214e-02\n",
      "Epoch: 1414 mean train loss:  2.69043661e-02, mean val. rec. loss:  1.86829731e-02\n",
      "Epoch: 1415 mean train loss:  2.68878522e-02, mean val. rec. loss:  1.86680772e-02\n",
      "Epoch: 1416 mean train loss:  2.68713160e-02, mean val. rec. loss:  1.86531756e-02\n",
      "Epoch: 1417 mean train loss:  2.68547388e-02, mean val. rec. loss:  1.86382253e-02\n",
      "Epoch: 1418 mean train loss:  2.68381467e-02, mean val. rec. loss:  1.86232568e-02\n",
      "Epoch: 1419 mean train loss:  2.68215136e-02, mean val. rec. loss:  1.86082566e-02\n",
      "Epoch: 1420 mean train loss:  2.68048451e-02, mean val. rec. loss:  1.85932382e-02\n",
      "Epoch: 1421 mean train loss:  2.67881636e-02, mean val. rec. loss:  1.85782050e-02\n",
      "Epoch: 1422 mean train loss:  2.67714523e-02, mean val. rec. loss:  1.85631447e-02\n",
      "Epoch: 1423 mean train loss:  2.67547038e-02, mean val. rec. loss:  1.85480469e-02\n",
      "Epoch: 1424 mean train loss:  2.67379310e-02, mean val. rec. loss:  1.85329265e-02\n",
      "Epoch: 1425 mean train loss:  2.67211285e-02, mean val. rec. loss:  1.85177766e-02\n",
      "Epoch: 1426 mean train loss:  2.67042980e-02, mean val. rec. loss:  1.85025869e-02\n",
      "Epoch: 1427 mean train loss:  2.66874340e-02, mean val. rec. loss:  1.84873985e-02\n",
      "Epoch: 1428 mean train loss:  2.66705644e-02, mean val. rec. loss:  1.84721771e-02\n",
      "Epoch: 1429 mean train loss:  2.66536539e-02, mean val. rec. loss:  1.84569376e-02\n",
      "Epoch: 1430 mean train loss:  2.66367191e-02, mean val. rec. loss:  1.84416754e-02\n",
      "Epoch: 1431 mean train loss:  2.66197564e-02, mean val. rec. loss:  1.84263905e-02\n",
      "Epoch: 1432 mean train loss:  2.66027658e-02, mean val. rec. loss:  1.84110841e-02\n",
      "Epoch: 1433 mean train loss:  2.65857491e-02, mean val. rec. loss:  1.83957516e-02\n",
      "Epoch: 1434 mean train loss:  2.65687026e-02, mean val. rec. loss:  1.83803976e-02\n",
      "Epoch: 1435 mean train loss:  2.65516505e-02, mean val. rec. loss:  1.83650163e-02\n",
      "Epoch: 1436 mean train loss:  2.65345593e-02, mean val. rec. loss:  1.83496203e-02\n",
      "Epoch: 1437 mean train loss:  2.65174365e-02, mean val. rec. loss:  1.83341971e-02\n",
      "Epoch: 1438 mean train loss:  2.65002987e-02, mean val. rec. loss:  1.83187603e-02\n",
      "Epoch: 1439 mean train loss:  2.64831387e-02, mean val. rec. loss:  1.83032962e-02\n",
      "Epoch: 1440 mean train loss:  2.64659618e-02, mean val. rec. loss:  1.82878265e-02\n",
      "Epoch: 1441 mean train loss:  2.64487496e-02, mean val. rec. loss:  1.82723273e-02\n",
      "Epoch: 1442 mean train loss:  2.64315299e-02, mean val. rec. loss:  1.82568213e-02\n",
      "Epoch: 1443 mean train loss:  2.64142693e-02, mean val. rec. loss:  1.82412927e-02\n",
      "Epoch: 1444 mean train loss:  2.63969937e-02, mean val. rec. loss:  1.82257322e-02\n",
      "Epoch: 1445 mean train loss:  2.63797089e-02, mean val. rec. loss:  1.82101605e-02\n",
      "Epoch: 1446 mean train loss:  2.63623793e-02, mean val. rec. loss:  1.81945649e-02\n",
      "Epoch: 1447 mean train loss:  2.63450461e-02, mean val. rec. loss:  1.81789568e-02\n",
      "Epoch: 1448 mean train loss:  2.63276942e-02, mean val. rec. loss:  1.81633295e-02\n",
      "Epoch: 1449 mean train loss:  2.63103106e-02, mean val. rec. loss:  1.81476920e-02\n",
      "Epoch: 1450 mean train loss:  2.62929252e-02, mean val. rec. loss:  1.81320351e-02\n",
      "Epoch: 1451 mean train loss:  2.62755081e-02, mean val. rec. loss:  1.81163693e-02\n",
      "Epoch: 1452 mean train loss:  2.62580743e-02, mean val. rec. loss:  1.81006943e-02\n",
      "Epoch: 1453 mean train loss:  2.62406219e-02, mean val. rec. loss:  1.80849944e-02\n",
      "Epoch: 1454 mean train loss:  2.62231396e-02, mean val. rec. loss:  1.80692934e-02\n",
      "Epoch: 1455 mean train loss:  2.62056611e-02, mean val. rec. loss:  1.80535538e-02\n",
      "Epoch: 1456 mean train loss:  2.61881509e-02, mean val. rec. loss:  1.80378153e-02\n",
      "Epoch: 1457 mean train loss:  2.61706407e-02, mean val. rec. loss:  1.80220644e-02\n",
      "Epoch: 1458 mean train loss:  2.61530989e-02, mean val. rec. loss:  1.80062919e-02\n",
      "Epoch: 1459 mean train loss:  2.61355478e-02, mean val. rec. loss:  1.79905035e-02\n",
      "Epoch: 1460 mean train loss:  2.61179705e-02, mean val. rec. loss:  1.79747220e-02\n",
      "Epoch: 1461 mean train loss:  2.61003915e-02, mean val. rec. loss:  1.79589098e-02\n",
      "Epoch: 1462 mean train loss:  2.60827938e-02, mean val. rec. loss:  1.79431101e-02\n",
      "Epoch: 1463 mean train loss:  2.60651923e-02, mean val. rec. loss:  1.79272968e-02\n",
      "Epoch: 1464 mean train loss:  2.60475742e-02, mean val. rec. loss:  1.79114733e-02\n",
      "Epoch: 1465 mean train loss:  2.60299299e-02, mean val. rec. loss:  1.78956384e-02\n",
      "Epoch: 1466 mean train loss:  2.60122912e-02, mean val. rec. loss:  1.78797877e-02\n",
      "Epoch: 1467 mean train loss:  2.59946246e-02, mean val. rec. loss:  1.78639279e-02\n",
      "Epoch: 1468 mean train loss:  2.59769562e-02, mean val. rec. loss:  1.78480647e-02\n",
      "Epoch: 1469 mean train loss:  2.59592709e-02, mean val. rec. loss:  1.78322061e-02\n",
      "Epoch: 1470 mean train loss:  2.59415783e-02, mean val. rec. loss:  1.78163236e-02\n",
      "Epoch: 1471 mean train loss:  2.59238800e-02, mean val. rec. loss:  1.78004388e-02\n",
      "Epoch: 1472 mean train loss:  2.59061650e-02, mean val. rec. loss:  1.77845428e-02\n",
      "Epoch: 1473 mean train loss:  2.58884518e-02, mean val. rec. loss:  1.77686478e-02\n",
      "Epoch: 1474 mean train loss:  2.58707201e-02, mean val. rec. loss:  1.77527438e-02\n",
      "Epoch: 1475 mean train loss:  2.58529920e-02, mean val. rec. loss:  1.77368534e-02\n",
      "Epoch: 1476 mean train loss:  2.58352416e-02, mean val. rec. loss:  1.77209448e-02\n",
      "Epoch: 1477 mean train loss:  2.58175024e-02, mean val. rec. loss:  1.77050374e-02\n",
      "Epoch: 1478 mean train loss:  2.57997408e-02, mean val. rec. loss:  1.76891254e-02\n",
      "Epoch: 1479 mean train loss:  2.57819885e-02, mean val. rec. loss:  1.76732089e-02\n",
      "Epoch: 1480 mean train loss:  2.57642214e-02, mean val. rec. loss:  1.76573038e-02\n",
      "Epoch: 1481 mean train loss:  2.57464598e-02, mean val. rec. loss:  1.76413975e-02\n",
      "Epoch: 1482 mean train loss:  2.57286796e-02, mean val. rec. loss:  1.76254799e-02\n",
      "Epoch: 1483 mean train loss:  2.57109050e-02, mean val. rec. loss:  1.76095645e-02\n",
      "Epoch: 1484 mean train loss:  2.56931230e-02, mean val. rec. loss:  1.75936423e-02\n",
      "Epoch: 1485 mean train loss:  2.56753465e-02, mean val. rec. loss:  1.75777122e-02\n",
      "Epoch: 1486 mean train loss:  2.56575663e-02, mean val. rec. loss:  1.75618093e-02\n",
      "Epoch: 1487 mean train loss:  2.56397880e-02, mean val. rec. loss:  1.75459053e-02\n",
      "Epoch: 1488 mean train loss:  2.56220022e-02, mean val. rec. loss:  1.75300036e-02\n",
      "Epoch: 1489 mean train loss:  2.56042331e-02, mean val. rec. loss:  1.75141098e-02\n",
      "Epoch: 1490 mean train loss:  2.55864567e-02, mean val. rec. loss:  1.74982069e-02\n",
      "Epoch: 1491 mean train loss:  2.55686765e-02, mean val. rec. loss:  1.74823233e-02\n",
      "Epoch: 1492 mean train loss:  2.55509019e-02, mean val. rec. loss:  1.74664453e-02\n",
      "Epoch: 1493 mean train loss:  2.55331291e-02, mean val. rec. loss:  1.74505685e-02\n",
      "Epoch: 1494 mean train loss:  2.55153527e-02, mean val. rec. loss:  1.74346860e-02\n",
      "Epoch: 1495 mean train loss:  2.54975967e-02, mean val. rec. loss:  1.74188126e-02\n",
      "Epoch: 1496 mean train loss:  2.54798351e-02, mean val. rec. loss:  1.74029460e-02\n",
      "Epoch: 1497 mean train loss:  2.54620791e-02, mean val. rec. loss:  1.73870885e-02\n",
      "Epoch: 1498 mean train loss:  2.54443269e-02, mean val. rec. loss:  1.73712332e-02\n",
      "Epoch: 1499 mean train loss:  2.54265820e-02, mean val. rec. loss:  1.73554063e-02\n",
      "Epoch: 1500 mean train loss:  2.54088540e-02, mean val. rec. loss:  1.73395908e-02\n",
      "Epoch: 1501 mean train loss:  2.53911278e-02, mean val. rec. loss:  1.73237854e-02\n",
      "Epoch: 1502 mean train loss:  2.53734146e-02, mean val. rec. loss:  1.73079868e-02\n",
      "Epoch: 1503 mean train loss:  2.53557089e-02, mean val. rec. loss:  1.72922019e-02\n",
      "Epoch: 1504 mean train loss:  2.53380069e-02, mean val. rec. loss:  1.72764260e-02\n",
      "Epoch: 1505 mean train loss:  2.53203161e-02, mean val. rec. loss:  1.72606614e-02\n",
      "Epoch: 1506 mean train loss:  2.53026365e-02, mean val. rec. loss:  1.72449071e-02\n",
      "Epoch: 1507 mean train loss:  2.52849792e-02, mean val. rec. loss:  1.72291618e-02\n",
      "Epoch: 1508 mean train loss:  2.52673256e-02, mean val. rec. loss:  1.72134347e-02\n",
      "Epoch: 1509 mean train loss:  2.52496814e-02, mean val. rec. loss:  1.71977246e-02\n",
      "Epoch: 1510 mean train loss:  2.52320520e-02, mean val. rec. loss:  1.71820202e-02\n",
      "Epoch: 1511 mean train loss:  2.52144450e-02, mean val. rec. loss:  1.71663497e-02\n",
      "Epoch: 1512 mean train loss:  2.51968492e-02, mean val. rec. loss:  1.71507020e-02\n",
      "Epoch: 1513 mean train loss:  2.51792664e-02, mean val. rec. loss:  1.71350656e-02\n",
      "Epoch: 1514 mean train loss:  2.51617003e-02, mean val. rec. loss:  1.71194371e-02\n",
      "Epoch: 1515 mean train loss:  2.51441492e-02, mean val. rec. loss:  1.71038279e-02\n",
      "Epoch: 1516 mean train loss:  2.51266111e-02, mean val. rec. loss:  1.70882494e-02\n",
      "Epoch: 1517 mean train loss:  2.51091102e-02, mean val. rec. loss:  1.70726799e-02\n",
      "Epoch: 1518 mean train loss:  2.50916056e-02, mean val. rec. loss:  1.70571285e-02\n",
      "Epoch: 1519 mean train loss:  2.50741401e-02, mean val. rec. loss:  1.70415998e-02\n",
      "Epoch: 1520 mean train loss:  2.50566802e-02, mean val. rec. loss:  1.70260870e-02\n",
      "Epoch: 1521 mean train loss:  2.50392445e-02, mean val. rec. loss:  1.70106003e-02\n",
      "Epoch: 1522 mean train loss:  2.50218405e-02, mean val. rec. loss:  1.69951522e-02\n",
      "Epoch: 1523 mean train loss:  2.50044458e-02, mean val. rec. loss:  1.69797153e-02\n",
      "Epoch: 1524 mean train loss:  2.49870753e-02, mean val. rec. loss:  1.69643057e-02\n",
      "Epoch: 1525 mean train loss:  2.49697364e-02, mean val. rec. loss:  1.69489188e-02\n",
      "Epoch: 1526 mean train loss:  2.49524143e-02, mean val. rec. loss:  1.69335614e-02\n",
      "Epoch: 1527 mean train loss:  2.49351146e-02, mean val. rec. loss:  1.69182209e-02\n",
      "Epoch: 1528 mean train loss:  2.49178409e-02, mean val. rec. loss:  1.69029134e-02\n",
      "Epoch: 1529 mean train loss:  2.49006007e-02, mean val. rec. loss:  1.68876262e-02\n",
      "Epoch: 1530 mean train loss:  2.48833829e-02, mean val. rec. loss:  1.68723697e-02\n",
      "Epoch: 1531 mean train loss:  2.48661893e-02, mean val. rec. loss:  1.68571393e-02\n",
      "Epoch: 1532 mean train loss:  2.48490143e-02, mean val. rec. loss:  1.68419224e-02\n",
      "Epoch: 1533 mean train loss:  2.48318822e-02, mean val. rec. loss:  1.68267396e-02\n",
      "Epoch: 1534 mean train loss:  2.48147761e-02, mean val. rec. loss:  1.68116022e-02\n",
      "Epoch: 1535 mean train loss:  2.47976961e-02, mean val. rec. loss:  1.67964931e-02\n",
      "Epoch: 1536 mean train loss:  2.47806440e-02, mean val. rec. loss:  1.67814169e-02\n",
      "Epoch: 1537 mean train loss:  2.47636273e-02, mean val. rec. loss:  1.67663611e-02\n",
      "Epoch: 1538 mean train loss:  2.47466460e-02, mean val. rec. loss:  1.67513359e-02\n",
      "Epoch: 1539 mean train loss:  2.47296963e-02, mean val. rec. loss:  1.67363538e-02\n",
      "Epoch: 1540 mean train loss:  2.47127690e-02, mean val. rec. loss:  1.67213977e-02\n",
      "Epoch: 1541 mean train loss:  2.46958734e-02, mean val. rec. loss:  1.67064735e-02\n",
      "Epoch: 1542 mean train loss:  2.46790280e-02, mean val. rec. loss:  1.66915753e-02\n",
      "Epoch: 1543 mean train loss:  2.46621882e-02, mean val. rec. loss:  1.66767100e-02\n",
      "Epoch: 1544 mean train loss:  2.46454099e-02, mean val. rec. loss:  1.66618708e-02\n",
      "Epoch: 1545 mean train loss:  2.46286613e-02, mean val. rec. loss:  1.66470758e-02\n",
      "Epoch: 1546 mean train loss:  2.46119463e-02, mean val. rec. loss:  1.66323250e-02\n",
      "Epoch: 1547 mean train loss:  2.45952611e-02, mean val. rec. loss:  1.66176094e-02\n",
      "Epoch: 1548 mean train loss:  2.45786094e-02, mean val. rec. loss:  1.66029301e-02\n",
      "Epoch: 1549 mean train loss:  2.45619968e-02, mean val. rec. loss:  1.65882814e-02\n",
      "Epoch: 1550 mean train loss:  2.45454400e-02, mean val. rec. loss:  1.65736633e-02\n",
      "Epoch: 1551 mean train loss:  2.45288963e-02, mean val. rec. loss:  1.65590860e-02\n",
      "Epoch: 1552 mean train loss:  2.45124011e-02, mean val. rec. loss:  1.65445484e-02\n",
      "Epoch: 1553 mean train loss:  2.44959505e-02, mean val. rec. loss:  1.65300471e-02\n",
      "Epoch: 1554 mean train loss:  2.44795408e-02, mean val. rec. loss:  1.65155889e-02\n",
      "Epoch: 1555 mean train loss:  2.44631666e-02, mean val. rec. loss:  1.65011647e-02\n",
      "Epoch: 1556 mean train loss:  2.44468352e-02, mean val. rec. loss:  1.64867769e-02\n",
      "Epoch: 1557 mean train loss:  2.44305336e-02, mean val. rec. loss:  1.64724275e-02\n",
      "Epoch: 1558 mean train loss:  2.44142860e-02, mean val. rec. loss:  1.64581258e-02\n",
      "Epoch: 1559 mean train loss:  2.43980812e-02, mean val. rec. loss:  1.64438649e-02\n",
      "Epoch: 1560 mean train loss:  2.43819155e-02, mean val. rec. loss:  1.64296460e-02\n",
      "Epoch: 1561 mean train loss:  2.43657964e-02, mean val. rec. loss:  1.64154747e-02\n",
      "Epoch: 1562 mean train loss:  2.43497108e-02, mean val. rec. loss:  1.64013272e-02\n",
      "Epoch: 1563 mean train loss:  2.43336717e-02, mean val. rec. loss:  1.63872239e-02\n",
      "Epoch: 1564 mean train loss:  2.43176811e-02, mean val. rec. loss:  1.63731637e-02\n",
      "Epoch: 1565 mean train loss:  2.43017259e-02, mean val. rec. loss:  1.63591353e-02\n",
      "Epoch: 1566 mean train loss:  2.42858209e-02, mean val. rec. loss:  1.63451636e-02\n",
      "Epoch: 1567 mean train loss:  2.42699662e-02, mean val. rec. loss:  1.63312338e-02\n",
      "Epoch: 1568 mean train loss:  2.42541562e-02, mean val. rec. loss:  1.63173449e-02\n",
      "Epoch: 1569 mean train loss:  2.42383946e-02, mean val. rec. loss:  1.63034934e-02\n",
      "Epoch: 1570 mean train loss:  2.42226740e-02, mean val. rec. loss:  1.62896940e-02\n",
      "Epoch: 1571 mean train loss:  2.42069962e-02, mean val. rec. loss:  1.62759445e-02\n",
      "Epoch: 1572 mean train loss:  2.41913669e-02, mean val. rec. loss:  1.62622336e-02\n",
      "Epoch: 1573 mean train loss:  2.41757822e-02, mean val. rec. loss:  1.62485726e-02\n",
      "Epoch: 1574 mean train loss:  2.41602497e-02, mean val. rec. loss:  1.62349569e-02\n",
      "Epoch: 1575 mean train loss:  2.41447711e-02, mean val. rec. loss:  1.62213696e-02\n",
      "Epoch: 1576 mean train loss:  2.41293280e-02, mean val. rec. loss:  1.62078175e-02\n",
      "Epoch: 1577 mean train loss:  2.41139407e-02, mean val. rec. loss:  1.61943266e-02\n",
      "Epoch: 1578 mean train loss:  2.40986056e-02, mean val. rec. loss:  1.61808629e-02\n",
      "Epoch: 1579 mean train loss:  2.40833114e-02, mean val. rec. loss:  1.61674627e-02\n",
      "Epoch: 1580 mean train loss:  2.40680712e-02, mean val. rec. loss:  1.61541180e-02\n",
      "Epoch: 1581 mean train loss:  2.40528869e-02, mean val. rec. loss:  1.61408210e-02\n",
      "Epoch: 1582 mean train loss:  2.40377436e-02, mean val. rec. loss:  1.61275626e-02\n",
      "Epoch: 1583 mean train loss:  2.40226487e-02, mean val. rec. loss:  1.61143551e-02\n",
      "Epoch: 1584 mean train loss:  2.40076022e-02, mean val. rec. loss:  1.61011908e-02\n",
      "Epoch: 1585 mean train loss:  2.39926190e-02, mean val. rec. loss:  1.60880627e-02\n",
      "Epoch: 1586 mean train loss:  2.39776805e-02, mean val. rec. loss:  1.60749891e-02\n",
      "Epoch: 1587 mean train loss:  2.39627848e-02, mean val. rec. loss:  1.60619507e-02\n",
      "Epoch: 1588 mean train loss:  2.39479506e-02, mean val. rec. loss:  1.60489621e-02\n",
      "Epoch: 1589 mean train loss:  2.39331611e-02, mean val. rec. loss:  1.60360223e-02\n",
      "Epoch: 1590 mean train loss:  2.39184181e-02, mean val. rec. loss:  1.60231233e-02\n",
      "Epoch: 1591 mean train loss:  2.39037366e-02, mean val. rec. loss:  1.60102822e-02\n",
      "Epoch: 1592 mean train loss:  2.38890998e-02, mean val. rec. loss:  1.59974807e-02\n",
      "Epoch: 1593 mean train loss:  2.38745077e-02, mean val. rec. loss:  1.59847382e-02\n",
      "Epoch: 1594 mean train loss:  2.38599770e-02, mean val. rec. loss:  1.59720286e-02\n",
      "Epoch: 1595 mean train loss:  2.38454910e-02, mean val. rec. loss:  1.59593643e-02\n",
      "Epoch: 1596 mean train loss:  2.38310740e-02, mean val. rec. loss:  1.59467420e-02\n",
      "Epoch: 1597 mean train loss:  2.38166960e-02, mean val. rec. loss:  1.59341833e-02\n",
      "Epoch: 1598 mean train loss:  2.38023627e-02, mean val. rec. loss:  1.59216596e-02\n",
      "Epoch: 1599 mean train loss:  2.37880927e-02, mean val. rec. loss:  1.59091938e-02\n",
      "Epoch: 1600 mean train loss:  2.37738731e-02, mean val. rec. loss:  1.58967711e-02\n",
      "Epoch: 1601 mean train loss:  2.37596888e-02, mean val. rec. loss:  1.58843915e-02\n",
      "Epoch: 1602 mean train loss:  2.37455771e-02, mean val. rec. loss:  1.58720459e-02\n",
      "Epoch: 1603 mean train loss:  2.37315026e-02, mean val. rec. loss:  1.58597558e-02\n",
      "Epoch: 1604 mean train loss:  2.37174990e-02, mean val. rec. loss:  1.58475157e-02\n",
      "Epoch: 1605 mean train loss:  2.37035363e-02, mean val. rec. loss:  1.58353288e-02\n",
      "Epoch: 1606 mean train loss:  2.36896182e-02, mean val. rec. loss:  1.58231783e-02\n",
      "Epoch: 1607 mean train loss:  2.36757617e-02, mean val. rec. loss:  1.58110844e-02\n",
      "Epoch: 1608 mean train loss:  2.36619535e-02, mean val. rec. loss:  1.57990166e-02\n",
      "Epoch: 1609 mean train loss:  2.36481845e-02, mean val. rec. loss:  1.57870067e-02\n",
      "Epoch: 1610 mean train loss:  2.36344844e-02, mean val. rec. loss:  1.57750330e-02\n",
      "Epoch: 1611 mean train loss:  2.36208252e-02, mean val. rec. loss:  1.57631252e-02\n",
      "Epoch: 1612 mean train loss:  2.36072331e-02, mean val. rec. loss:  1.57512479e-02\n",
      "Epoch: 1613 mean train loss:  2.35936782e-02, mean val. rec. loss:  1.57394160e-02\n",
      "Epoch: 1614 mean train loss:  2.35801717e-02, mean val. rec. loss:  1.57276271e-02\n",
      "Epoch: 1615 mean train loss:  2.35667286e-02, mean val. rec. loss:  1.57158894e-02\n",
      "Epoch: 1616 mean train loss:  2.35533301e-02, mean val. rec. loss:  1.57041947e-02\n",
      "Epoch: 1617 mean train loss:  2.35399726e-02, mean val. rec. loss:  1.56925521e-02\n",
      "Epoch: 1618 mean train loss:  2.35266729e-02, mean val. rec. loss:  1.56809436e-02\n",
      "Epoch: 1619 mean train loss:  2.35134308e-02, mean val. rec. loss:  1.56693680e-02\n",
      "Epoch: 1620 mean train loss:  2.35002260e-02, mean val. rec. loss:  1.56578479e-02\n",
      "Epoch: 1621 mean train loss:  2.34870808e-02, mean val. rec. loss:  1.56463687e-02\n",
      "Epoch: 1622 mean train loss:  2.34739896e-02, mean val. rec. loss:  1.56349382e-02\n",
      "Epoch: 1623 mean train loss:  2.34609375e-02, mean val. rec. loss:  1.56235428e-02\n",
      "Epoch: 1624 mean train loss:  2.34479432e-02, mean val. rec. loss:  1.56121929e-02\n",
      "Epoch: 1625 mean train loss:  2.34349823e-02, mean val. rec. loss:  1.56008803e-02\n",
      "Epoch: 1626 mean train loss:  2.34220811e-02, mean val. rec. loss:  1.55896177e-02\n",
      "Epoch: 1627 mean train loss:  2.34092338e-02, mean val. rec. loss:  1.55783913e-02\n",
      "Epoch: 1628 mean train loss:  2.33964238e-02, mean val. rec. loss:  1.55672092e-02\n",
      "Epoch: 1629 mean train loss:  2.33836678e-02, mean val. rec. loss:  1.55560724e-02\n",
      "Epoch: 1630 mean train loss:  2.33709658e-02, mean val. rec. loss:  1.55449753e-02\n",
      "Epoch: 1631 mean train loss:  2.33582936e-02, mean val. rec. loss:  1.55339133e-02\n",
      "Epoch: 1632 mean train loss:  2.33456847e-02, mean val. rec. loss:  1.55228945e-02\n",
      "Epoch: 1633 mean train loss:  2.33331242e-02, mean val. rec. loss:  1.55119165e-02\n",
      "Epoch: 1634 mean train loss:  2.33205973e-02, mean val. rec. loss:  1.55009747e-02\n",
      "Epoch: 1635 mean train loss:  2.33081299e-02, mean val. rec. loss:  1.54900704e-02\n",
      "Epoch: 1636 mean train loss:  2.32956942e-02, mean val. rec. loss:  1.54792046e-02\n",
      "Epoch: 1637 mean train loss:  2.32833143e-02, mean val. rec. loss:  1.54683808e-02\n",
      "Epoch: 1638 mean train loss:  2.32709829e-02, mean val. rec. loss:  1.54576012e-02\n",
      "Epoch: 1639 mean train loss:  2.32586887e-02, mean val. rec. loss:  1.54468523e-02\n",
      "Epoch: 1640 mean train loss:  2.32464430e-02, mean val. rec. loss:  1.54361453e-02\n",
      "Epoch: 1641 mean train loss:  2.32342345e-02, mean val. rec. loss:  1.54254780e-02\n",
      "Epoch: 1642 mean train loss:  2.32220893e-02, mean val. rec. loss:  1.54148356e-02\n",
      "Epoch: 1643 mean train loss:  2.32099738e-02, mean val. rec. loss:  1.54042318e-02\n",
      "Epoch: 1644 mean train loss:  2.31978938e-02, mean val. rec. loss:  1.53936710e-02\n",
      "Epoch: 1645 mean train loss:  2.31858659e-02, mean val. rec. loss:  1.53831251e-02\n",
      "Epoch: 1646 mean train loss:  2.31738846e-02, mean val. rec. loss:  1.53726199e-02\n",
      "Epoch: 1647 mean train loss:  2.31619424e-02, mean val. rec. loss:  1.53621544e-02\n",
      "Epoch: 1648 mean train loss:  2.31500318e-02, mean val. rec. loss:  1.53517332e-02\n",
      "Epoch: 1649 mean train loss:  2.31381789e-02, mean val. rec. loss:  1.53413403e-02\n",
      "Epoch: 1650 mean train loss:  2.31263540e-02, mean val. rec. loss:  1.53309837e-02\n",
      "Epoch: 1651 mean train loss:  2.31145887e-02, mean val. rec. loss:  1.53206634e-02\n",
      "Epoch: 1652 mean train loss:  2.31028420e-02, mean val. rec. loss:  1.53103748e-02\n",
      "Epoch: 1653 mean train loss:  2.30911437e-02, mean val. rec. loss:  1.53001134e-02\n",
      "Epoch: 1654 mean train loss:  2.30794901e-02, mean val. rec. loss:  1.52898884e-02\n",
      "Epoch: 1655 mean train loss:  2.30678626e-02, mean val. rec. loss:  1.52796814e-02\n",
      "Epoch: 1656 mean train loss:  2.30562798e-02, mean val. rec. loss:  1.52695097e-02\n",
      "Epoch: 1657 mean train loss:  2.30447454e-02, mean val. rec. loss:  1.52593674e-02\n",
      "Epoch: 1658 mean train loss:  2.30332408e-02, mean val. rec. loss:  1.52492546e-02\n",
      "Epoch: 1659 mean train loss:  2.30217771e-02, mean val. rec. loss:  1.52391701e-02\n",
      "Epoch: 1660 mean train loss:  2.30103507e-02, mean val. rec. loss:  1.52291208e-02\n",
      "Epoch: 1661 mean train loss:  2.29989504e-02, mean val. rec. loss:  1.52191078e-02\n",
      "Epoch: 1662 mean train loss:  2.29875910e-02, mean val. rec. loss:  1.52091186e-02\n",
      "Epoch: 1663 mean train loss:  2.29762652e-02, mean val. rec. loss:  1.51991498e-02\n",
      "Epoch: 1664 mean train loss:  2.29649803e-02, mean val. rec. loss:  1.51892196e-02\n",
      "Epoch: 1665 mean train loss:  2.29537308e-02, mean val. rec. loss:  1.51793098e-02\n",
      "Epoch: 1666 mean train loss:  2.29425130e-02, mean val. rec. loss:  1.51694283e-02\n",
      "Epoch: 1667 mean train loss:  2.29313343e-02, mean val. rec. loss:  1.51595604e-02\n",
      "Epoch: 1668 mean train loss:  2.29201872e-02, mean val. rec. loss:  1.51497300e-02\n",
      "Epoch: 1669 mean train loss:  2.29090625e-02, mean val. rec. loss:  1.51399120e-02\n",
      "Epoch: 1670 mean train loss:  2.28979806e-02, mean val. rec. loss:  1.51301179e-02\n",
      "Epoch: 1671 mean train loss:  2.28869210e-02, mean val. rec. loss:  1.51203589e-02\n",
      "Epoch: 1672 mean train loss:  2.28759062e-02, mean val. rec. loss:  1.51106282e-02\n",
      "Epoch: 1673 mean train loss:  2.28649137e-02, mean val. rec. loss:  1.51009112e-02\n",
      "Epoch: 1674 mean train loss:  2.28539491e-02, mean val. rec. loss:  1.50912157e-02\n",
      "Epoch: 1675 mean train loss:  2.28430329e-02, mean val. rec. loss:  1.50815417e-02\n",
      "Epoch: 1676 mean train loss:  2.28321317e-02, mean val. rec. loss:  1.50718950e-02\n",
      "Epoch: 1677 mean train loss:  2.28212602e-02, mean val. rec. loss:  1.50622789e-02\n",
      "Epoch: 1678 mean train loss:  2.28104148e-02, mean val. rec. loss:  1.50526673e-02\n",
      "Epoch: 1679 mean train loss:  2.27996085e-02, mean val. rec. loss:  1.50430772e-02\n",
      "Epoch: 1680 mean train loss:  2.27888190e-02, mean val. rec. loss:  1.50335201e-02\n",
      "Epoch: 1681 mean train loss:  2.27780704e-02, mean val. rec. loss:  1.50239731e-02\n",
      "Epoch: 1682 mean train loss:  2.27673330e-02, mean val. rec. loss:  1.50144443e-02\n",
      "Epoch: 1683 mean train loss:  2.27566366e-02, mean val. rec. loss:  1.50049348e-02\n",
      "Epoch: 1684 mean train loss:  2.27459551e-02, mean val. rec. loss:  1.49954343e-02\n",
      "Epoch: 1685 mean train loss:  2.27353089e-02, mean val. rec. loss:  1.49859611e-02\n",
      "Epoch: 1686 mean train loss:  2.27246758e-02, mean val. rec. loss:  1.49765060e-02\n",
      "Epoch: 1687 mean train loss:  2.27140725e-02, mean val. rec. loss:  1.49670532e-02\n",
      "Epoch: 1688 mean train loss:  2.27034915e-02, mean val. rec. loss:  1.49576332e-02\n",
      "Epoch: 1689 mean train loss:  2.26929422e-02, mean val. rec. loss:  1.49482156e-02\n",
      "Epoch: 1690 mean train loss:  2.26824134e-02, mean val. rec. loss:  1.49388206e-02\n",
      "Epoch: 1691 mean train loss:  2.26719051e-02, mean val. rec. loss:  1.49294346e-02\n",
      "Epoch: 1692 mean train loss:  2.26614135e-02, mean val. rec. loss:  1.49200703e-02\n",
      "Epoch: 1693 mean train loss:  2.26509573e-02, mean val. rec. loss:  1.49107240e-02\n",
      "Epoch: 1694 mean train loss:  2.26405197e-02, mean val. rec. loss:  1.49013869e-02\n",
      "Epoch: 1695 mean train loss:  2.26300896e-02, mean val. rec. loss:  1.48920724e-02\n",
      "Epoch: 1696 mean train loss:  2.26196986e-02, mean val. rec. loss:  1.48827488e-02\n",
      "Epoch: 1697 mean train loss:  2.26093150e-02, mean val. rec. loss:  1.48734457e-02\n",
      "Epoch: 1698 mean train loss:  2.25989482e-02, mean val. rec. loss:  1.48641607e-02\n",
      "Epoch: 1699 mean train loss:  2.25886130e-02, mean val. rec. loss:  1.48548768e-02\n",
      "Epoch: 1700 mean train loss:  2.25782890e-02, mean val. rec. loss:  1.48456088e-02\n",
      "Epoch: 1701 mean train loss:  2.25679855e-02, mean val. rec. loss:  1.48363442e-02\n",
      "Epoch: 1702 mean train loss:  2.25576988e-02, mean val. rec. loss:  1.48270932e-02\n",
      "Epoch: 1703 mean train loss:  2.25474269e-02, mean val. rec. loss:  1.48178513e-02\n",
      "Epoch: 1704 mean train loss:  2.25371700e-02, mean val. rec. loss:  1.48086174e-02\n",
      "Epoch: 1705 mean train loss:  2.25269410e-02, mean val. rec. loss:  1.47994015e-02\n",
      "Epoch: 1706 mean train loss:  2.25167250e-02, mean val. rec. loss:  1.47901891e-02\n",
      "Epoch: 1707 mean train loss:  2.25065128e-02, mean val. rec. loss:  1.47809812e-02\n",
      "Epoch: 1708 mean train loss:  2.24963210e-02, mean val. rec. loss:  1.47717858e-02\n",
      "Epoch: 1709 mean train loss:  2.24861516e-02, mean val. rec. loss:  1.47625927e-02\n",
      "Epoch: 1710 mean train loss:  2.24759914e-02, mean val. rec. loss:  1.47534086e-02\n",
      "Epoch: 1711 mean train loss:  2.24658425e-02, mean val. rec. loss:  1.47442120e-02\n",
      "Epoch: 1712 mean train loss:  2.24557122e-02, mean val. rec. loss:  1.47350393e-02\n",
      "Epoch: 1713 mean train loss:  2.24455912e-02, mean val. rec. loss:  1.47258632e-02\n",
      "Epoch: 1714 mean train loss:  2.24354851e-02, mean val. rec. loss:  1.47167040e-02\n",
      "Epoch: 1715 mean train loss:  2.24253864e-02, mean val. rec. loss:  1.47075381e-02\n",
      "Epoch: 1716 mean train loss:  2.24153064e-02, mean val. rec. loss:  1.46983824e-02\n",
      "Epoch: 1717 mean train loss:  2.24052282e-02, mean val. rec. loss:  1.46892164e-02\n",
      "Epoch: 1718 mean train loss:  2.23951724e-02, mean val. rec. loss:  1.46800584e-02\n",
      "Epoch: 1719 mean train loss:  2.23851184e-02, mean val. rec. loss:  1.46709061e-02\n",
      "Epoch: 1720 mean train loss:  2.23750756e-02, mean val. rec. loss:  1.46617572e-02\n",
      "Epoch: 1721 mean train loss:  2.23650421e-02, mean val. rec. loss:  1.46526071e-02\n",
      "Epoch: 1722 mean train loss:  2.23550198e-02, mean val. rec. loss:  1.46434616e-02\n",
      "Epoch: 1723 mean train loss:  2.23450031e-02, mean val. rec. loss:  1.46343093e-02\n",
      "Epoch: 1724 mean train loss:  2.23350013e-02, mean val. rec. loss:  1.46251592e-02\n",
      "Epoch: 1725 mean train loss:  2.23249976e-02, mean val. rec. loss:  1.46160273e-02\n",
      "Epoch: 1726 mean train loss:  2.23150088e-02, mean val. rec. loss:  1.46068795e-02\n",
      "Epoch: 1727 mean train loss:  2.23050312e-02, mean val. rec. loss:  1.45977317e-02\n",
      "Epoch: 1728 mean train loss:  2.22950480e-02, mean val. rec. loss:  1.45885703e-02\n",
      "Epoch: 1729 mean train loss:  2.22850741e-02, mean val. rec. loss:  1.45794123e-02\n",
      "Epoch: 1730 mean train loss:  2.22751132e-02, mean val. rec. loss:  1.45702668e-02\n",
      "Epoch: 1731 mean train loss:  2.22651505e-02, mean val. rec. loss:  1.45611100e-02\n",
      "Epoch: 1732 mean train loss:  2.22551952e-02, mean val. rec. loss:  1.45519474e-02\n",
      "Epoch: 1733 mean train loss:  2.22452493e-02, mean val. rec. loss:  1.45427804e-02\n",
      "Epoch: 1734 mean train loss:  2.22352996e-02, mean val. rec. loss:  1.45336156e-02\n",
      "Epoch: 1735 mean train loss:  2.22253518e-02, mean val. rec. loss:  1.45244485e-02\n",
      "Epoch: 1736 mean train loss:  2.22154151e-02, mean val. rec. loss:  1.45152758e-02\n",
      "Epoch: 1737 mean train loss:  2.22054859e-02, mean val. rec. loss:  1.45060951e-02\n",
      "Epoch: 1738 mean train loss:  2.21955437e-02, mean val. rec. loss:  1.44969087e-02\n",
      "Epoch: 1739 mean train loss:  2.21856126e-02, mean val. rec. loss:  1.44877133e-02\n",
      "Epoch: 1740 mean train loss:  2.21756834e-02, mean val. rec. loss:  1.44785066e-02\n",
      "Epoch: 1741 mean train loss:  2.21657561e-02, mean val. rec. loss:  1.44693009e-02\n",
      "Epoch: 1742 mean train loss:  2.21558213e-02, mean val. rec. loss:  1.44600908e-02\n",
      "Epoch: 1743 mean train loss:  2.21458996e-02, mean val. rec. loss:  1.44508761e-02\n",
      "Epoch: 1744 mean train loss:  2.21359759e-02, mean val. rec. loss:  1.44416523e-02\n",
      "Epoch: 1745 mean train loss:  2.21260486e-02, mean val. rec. loss:  1.44324127e-02\n",
      "Epoch: 1746 mean train loss:  2.21161194e-02, mean val. rec. loss:  1.44231617e-02\n",
      "Epoch: 1747 mean train loss:  2.21061865e-02, mean val. rec. loss:  1.44139085e-02\n",
      "Epoch: 1748 mean train loss:  2.20962647e-02, mean val. rec. loss:  1.44046643e-02\n",
      "Epoch: 1749 mean train loss:  2.20863281e-02, mean val. rec. loss:  1.43953952e-02\n",
      "Epoch: 1750 mean train loss:  2.20763952e-02, mean val. rec. loss:  1.43861056e-02\n",
      "Epoch: 1751 mean train loss:  2.20664585e-02, mean val. rec. loss:  1.43768116e-02\n",
      "Epoch: 1752 mean train loss:  2.20565256e-02, mean val. rec. loss:  1.43675050e-02\n",
      "Epoch: 1753 mean train loss:  2.20465815e-02, mean val. rec. loss:  1.43582053e-02\n",
      "Epoch: 1754 mean train loss:  2.20366337e-02, mean val. rec. loss:  1.43488840e-02\n",
      "Epoch: 1755 mean train loss:  2.20266821e-02, mean val. rec. loss:  1.43395468e-02\n",
      "Epoch: 1756 mean train loss:  2.20167343e-02, mean val. rec. loss:  1.43301858e-02\n",
      "Epoch: 1757 mean train loss:  2.20067734e-02, mean val. rec. loss:  1.43208283e-02\n",
      "Epoch: 1758 mean train loss:  2.19968089e-02, mean val. rec. loss:  1.43114514e-02\n",
      "Epoch: 1759 mean train loss:  2.19868424e-02, mean val. rec. loss:  1.43020689e-02\n",
      "Epoch: 1760 mean train loss:  2.19768685e-02, mean val. rec. loss:  1.42926750e-02\n",
      "Epoch: 1761 mean train loss:  2.19668835e-02, mean val. rec. loss:  1.42832585e-02\n",
      "Epoch: 1762 mean train loss:  2.19568965e-02, mean val. rec. loss:  1.42738227e-02\n",
      "Epoch: 1763 mean train loss:  2.19469022e-02, mean val. rec. loss:  1.42643812e-02\n",
      "Epoch: 1764 mean train loss:  2.19369059e-02, mean val. rec. loss:  1.42549272e-02\n",
      "Epoch: 1765 mean train loss:  2.19268911e-02, mean val. rec. loss:  1.42454528e-02\n",
      "Epoch: 1766 mean train loss:  2.19168762e-02, mean val. rec. loss:  1.42359649e-02\n",
      "Epoch: 1767 mean train loss:  2.19068502e-02, mean val. rec. loss:  1.42264576e-02\n",
      "Epoch: 1768 mean train loss:  2.18968204e-02, mean val. rec. loss:  1.42169322e-02\n",
      "Epoch: 1769 mean train loss:  2.18867720e-02, mean val. rec. loss:  1.42073909e-02\n",
      "Epoch: 1770 mean train loss:  2.18767199e-02, mean val. rec. loss:  1.41978406e-02\n",
      "Epoch: 1771 mean train loss:  2.18666622e-02, mean val. rec. loss:  1.41882664e-02\n",
      "Epoch: 1772 mean train loss:  2.18565841e-02, mean val. rec. loss:  1.41786798e-02\n",
      "Epoch: 1773 mean train loss:  2.18465022e-02, mean val. rec. loss:  1.41690625e-02\n",
      "Epoch: 1774 mean train loss:  2.18364072e-02, mean val. rec. loss:  1.41594328e-02\n",
      "Epoch: 1775 mean train loss:  2.18263067e-02, mean val. rec. loss:  1.41497747e-02\n",
      "Epoch: 1776 mean train loss:  2.18161876e-02, mean val. rec. loss:  1.41401121e-02\n",
      "Epoch: 1777 mean train loss:  2.18060572e-02, mean val. rec. loss:  1.41304302e-02\n",
      "Epoch: 1778 mean train loss:  2.17959232e-02, mean val. rec. loss:  1.41207302e-02\n",
      "Epoch: 1779 mean train loss:  2.17857631e-02, mean val. rec. loss:  1.41110097e-02\n",
      "Epoch: 1780 mean train loss:  2.17755974e-02, mean val. rec. loss:  1.41012552e-02\n",
      "Epoch: 1781 mean train loss:  2.17654187e-02, mean val. rec. loss:  1.40914815e-02\n",
      "Epoch: 1782 mean train loss:  2.17552306e-02, mean val. rec. loss:  1.40816975e-02\n",
      "Epoch: 1783 mean train loss:  2.17450221e-02, mean val. rec. loss:  1.40718875e-02\n",
      "Epoch: 1784 mean train loss:  2.17348024e-02, mean val. rec. loss:  1.40620639e-02\n",
      "Epoch: 1785 mean train loss:  2.17245585e-02, mean val. rec. loss:  1.40521994e-02\n",
      "Epoch: 1786 mean train loss:  2.17143034e-02, mean val. rec. loss:  1.40423270e-02\n",
      "Epoch: 1787 mean train loss:  2.17040446e-02, mean val. rec. loss:  1.40324195e-02\n",
      "Epoch: 1788 mean train loss:  2.16937541e-02, mean val. rec. loss:  1.40224960e-02\n",
      "Epoch: 1789 mean train loss:  2.16834599e-02, mean val. rec. loss:  1.40125397e-02\n",
      "Epoch: 1790 mean train loss:  2.16731490e-02, mean val. rec. loss:  1.40025653e-02\n",
      "Epoch: 1791 mean train loss:  2.16628175e-02, mean val. rec. loss:  1.39925749e-02\n",
      "Epoch: 1792 mean train loss:  2.16524638e-02, mean val. rec. loss:  1.39825528e-02\n",
      "Epoch: 1793 mean train loss:  2.16420951e-02, mean val. rec. loss:  1.39725058e-02\n",
      "Epoch: 1794 mean train loss:  2.16317190e-02, mean val. rec. loss:  1.39624338e-02\n",
      "Epoch: 1795 mean train loss:  2.16213075e-02, mean val. rec. loss:  1.39523437e-02\n",
      "Epoch: 1796 mean train loss:  2.16108885e-02, mean val. rec. loss:  1.39422196e-02\n",
      "Epoch: 1797 mean train loss:  2.16004584e-02, mean val. rec. loss:  1.39320625e-02\n",
      "Epoch: 1798 mean train loss:  2.15899984e-02, mean val. rec. loss:  1.39218942e-02\n",
      "Epoch: 1799 mean train loss:  2.15795143e-02, mean val. rec. loss:  1.39116861e-02\n",
      "Epoch: 1800 mean train loss:  2.15690116e-02, mean val. rec. loss:  1.39014599e-02\n",
      "Epoch: 1801 mean train loss:  2.15585014e-02, mean val. rec. loss:  1.38912008e-02\n",
      "Epoch: 1802 mean train loss:  2.15479539e-02, mean val. rec. loss:  1.38809213e-02\n",
      "Epoch: 1803 mean train loss:  2.15373990e-02, mean val. rec. loss:  1.38705976e-02\n",
      "Epoch: 1804 mean train loss:  2.15268218e-02, mean val. rec. loss:  1.38602512e-02\n",
      "Epoch: 1805 mean train loss:  2.15162185e-02, mean val. rec. loss:  1.38498753e-02\n",
      "Epoch: 1806 mean train loss:  2.15055947e-02, mean val. rec. loss:  1.38394801e-02\n",
      "Epoch: 1807 mean train loss:  2.14949429e-02, mean val. rec. loss:  1.38290510e-02\n",
      "Epoch: 1808 mean train loss:  2.14842689e-02, mean val. rec. loss:  1.38185923e-02\n",
      "Epoch: 1809 mean train loss:  2.14735892e-02, mean val. rec. loss:  1.38080996e-02\n",
      "Epoch: 1810 mean train loss:  2.14628704e-02, mean val. rec. loss:  1.37975740e-02\n",
      "Epoch: 1811 mean train loss:  2.14521256e-02, mean val. rec. loss:  1.37870315e-02\n",
      "Epoch: 1812 mean train loss:  2.14413621e-02, mean val. rec. loss:  1.37764526e-02\n",
      "Epoch: 1813 mean train loss:  2.14305763e-02, mean val. rec. loss:  1.37658295e-02\n",
      "Epoch: 1814 mean train loss:  2.14197682e-02, mean val. rec. loss:  1.37551826e-02\n",
      "Epoch: 1815 mean train loss:  2.14089358e-02, mean val. rec. loss:  1.37444994e-02\n",
      "Epoch: 1816 mean train loss:  2.13980811e-02, mean val. rec. loss:  1.37337924e-02\n",
      "Epoch: 1817 mean train loss:  2.13871854e-02, mean val. rec. loss:  1.37230604e-02\n",
      "Epoch: 1818 mean train loss:  2.13762842e-02, mean val. rec. loss:  1.37122899e-02\n",
      "Epoch: 1819 mean train loss:  2.13653419e-02, mean val. rec. loss:  1.37014831e-02\n",
      "Epoch: 1820 mean train loss:  2.13543699e-02, mean val. rec. loss:  1.36906400e-02\n",
      "Epoch: 1821 mean train loss:  2.13433830e-02, mean val. rec. loss:  1.36797607e-02\n",
      "Epoch: 1822 mean train loss:  2.13323644e-02, mean val. rec. loss:  1.36688541e-02\n",
      "Epoch: 1823 mean train loss:  2.13213253e-02, mean val. rec. loss:  1.36579112e-02\n",
      "Epoch: 1824 mean train loss:  2.13102565e-02, mean val. rec. loss:  1.36469377e-02\n",
      "Epoch: 1825 mean train loss:  2.12991504e-02, mean val. rec. loss:  1.36359268e-02\n",
      "Epoch: 1826 mean train loss:  2.12880294e-02, mean val. rec. loss:  1.36248830e-02\n",
      "Epoch: 1827 mean train loss:  2.12768711e-02, mean val. rec. loss:  1.36138086e-02\n",
      "Epoch: 1828 mean train loss:  2.12656793e-02, mean val. rec. loss:  1.36027036e-02\n",
      "Epoch: 1829 mean train loss:  2.12544652e-02, mean val. rec. loss:  1.35915498e-02\n",
      "Epoch: 1830 mean train loss:  2.12432232e-02, mean val. rec. loss:  1.35803654e-02\n",
      "Epoch: 1831 mean train loss:  2.12319551e-02, mean val. rec. loss:  1.35691503e-02\n",
      "Epoch: 1832 mean train loss:  2.12206590e-02, mean val. rec. loss:  1.35578922e-02\n",
      "Epoch: 1833 mean train loss:  2.12093183e-02, mean val. rec. loss:  1.35466023e-02\n",
      "Epoch: 1834 mean train loss:  2.11979608e-02, mean val. rec. loss:  1.35352728e-02\n",
      "Epoch: 1835 mean train loss:  2.11865660e-02, mean val. rec. loss:  1.35239092e-02\n",
      "Epoch: 1836 mean train loss:  2.11751508e-02, mean val. rec. loss:  1.35125082e-02\n",
      "Epoch: 1837 mean train loss:  2.11636909e-02, mean val. rec. loss:  1.35010777e-02\n",
      "Epoch: 1838 mean train loss:  2.11522068e-02, mean val. rec. loss:  1.34896030e-02\n",
      "Epoch: 1839 mean train loss:  2.11406910e-02, mean val. rec. loss:  1.34780863e-02\n",
      "Epoch: 1840 mean train loss:  2.11291435e-02, mean val. rec. loss:  1.34665334e-02\n",
      "Epoch: 1841 mean train loss:  2.11175682e-02, mean val. rec. loss:  1.34549430e-02\n",
      "Epoch: 1842 mean train loss:  2.11059500e-02, mean val. rec. loss:  1.34433152e-02\n",
      "Epoch: 1843 mean train loss:  2.10943057e-02, mean val. rec. loss:  1.34316466e-02\n",
      "Epoch: 1844 mean train loss:  2.10826297e-02, mean val. rec. loss:  1.34199553e-02\n",
      "Epoch: 1845 mean train loss:  2.10709240e-02, mean val. rec. loss:  1.34082198e-02\n",
      "Epoch: 1846 mean train loss:  2.10591810e-02, mean val. rec. loss:  1.33964514e-02\n",
      "Epoch: 1847 mean train loss:  2.10474045e-02, mean val. rec. loss:  1.33846297e-02\n",
      "Epoch: 1848 mean train loss:  2.10355983e-02, mean val. rec. loss:  1.33727830e-02\n",
      "Epoch: 1849 mean train loss:  2.10237585e-02, mean val. rec. loss:  1.33608967e-02\n",
      "Epoch: 1850 mean train loss:  2.10118833e-02, mean val. rec. loss:  1.33489808e-02\n",
      "Epoch: 1851 mean train loss:  2.09999727e-02, mean val. rec. loss:  1.33370140e-02\n",
      "Epoch: 1852 mean train loss:  2.09880249e-02, mean val. rec. loss:  1.33250029e-02\n",
      "Epoch: 1853 mean train loss:  2.09760584e-02, mean val. rec. loss:  1.33129657e-02\n",
      "Epoch: 1854 mean train loss:  2.09640455e-02, mean val. rec. loss:  1.33008980e-02\n",
      "Epoch: 1855 mean train loss:  2.09520027e-02, mean val. rec. loss:  1.32887882e-02\n",
      "Epoch: 1856 mean train loss:  2.09399226e-02, mean val. rec. loss:  1.32766343e-02\n",
      "Epoch: 1857 mean train loss:  2.09278147e-02, mean val. rec. loss:  1.32644417e-02\n",
      "Epoch: 1858 mean train loss:  2.09156713e-02, mean val. rec. loss:  1.32522265e-02\n",
      "Epoch: 1859 mean train loss:  2.09034926e-02, mean val. rec. loss:  1.32399637e-02\n",
      "Epoch: 1860 mean train loss:  2.08912748e-02, mean val. rec. loss:  1.32276714e-02\n",
      "Epoch: 1861 mean train loss:  2.08790290e-02, mean val. rec. loss:  1.32153349e-02\n",
      "Epoch: 1862 mean train loss:  2.08667497e-02, mean val. rec. loss:  1.32029553e-02\n",
      "Epoch: 1863 mean train loss:  2.08544257e-02, mean val. rec. loss:  1.31905360e-02\n",
      "Epoch: 1864 mean train loss:  2.08420776e-02, mean val. rec. loss:  1.31780906e-02\n",
      "Epoch: 1865 mean train loss:  2.08296884e-02, mean val. rec. loss:  1.31656134e-02\n",
      "Epoch: 1866 mean train loss:  2.08172695e-02, mean val. rec. loss:  1.31531011e-02\n",
      "Epoch: 1867 mean train loss:  2.08048207e-02, mean val. rec. loss:  1.31405503e-02\n",
      "Epoch: 1868 mean train loss:  2.07923366e-02, mean val. rec. loss:  1.31279609e-02\n",
      "Epoch: 1869 mean train loss:  2.07798096e-02, mean val. rec. loss:  1.31153329e-02\n",
      "Epoch: 1870 mean train loss:  2.07672510e-02, mean val. rec. loss:  1.31026777e-02\n",
      "Epoch: 1871 mean train loss:  2.07546608e-02, mean val. rec. loss:  1.30899897e-02\n",
      "Epoch: 1872 mean train loss:  2.07420444e-02, mean val. rec. loss:  1.30772596e-02\n",
      "Epoch: 1873 mean train loss:  2.07293871e-02, mean val. rec. loss:  1.30644979e-02\n",
      "Epoch: 1874 mean train loss:  2.07167019e-02, mean val. rec. loss:  1.30517100e-02\n",
      "Epoch: 1875 mean train loss:  2.07039775e-02, mean val. rec. loss:  1.30388949e-02\n",
      "Epoch: 1876 mean train loss:  2.06912196e-02, mean val. rec. loss:  1.30260424e-02\n",
      "Epoch: 1877 mean train loss:  2.06784301e-02, mean val. rec. loss:  1.30131548e-02\n",
      "Epoch: 1878 mean train loss:  2.06656089e-02, mean val. rec. loss:  1.30002456e-02\n",
      "Epoch: 1879 mean train loss:  2.06527654e-02, mean val. rec. loss:  1.29872967e-02\n",
      "Epoch: 1880 mean train loss:  2.06398791e-02, mean val. rec. loss:  1.29743206e-02\n",
      "Epoch: 1881 mean train loss:  2.06269648e-02, mean val. rec. loss:  1.29613185e-02\n",
      "Epoch: 1882 mean train loss:  2.06140207e-02, mean val. rec. loss:  1.29482891e-02\n",
      "Epoch: 1883 mean train loss:  2.06010468e-02, mean val. rec. loss:  1.29352257e-02\n",
      "Epoch: 1884 mean train loss:  2.05880319e-02, mean val. rec. loss:  1.29221350e-02\n",
      "Epoch: 1885 mean train loss:  2.05749966e-02, mean val. rec. loss:  1.29090195e-02\n",
      "Epoch: 1886 mean train loss:  2.05619315e-02, mean val. rec. loss:  1.28958857e-02\n",
      "Epoch: 1887 mean train loss:  2.05488366e-02, mean val. rec. loss:  1.28827271e-02\n",
      "Epoch: 1888 mean train loss:  2.05357118e-02, mean val. rec. loss:  1.28695367e-02\n",
      "Epoch: 1889 mean train loss:  2.05225536e-02, mean val. rec. loss:  1.28563281e-02\n",
      "Epoch: 1890 mean train loss:  2.05093842e-02, mean val. rec. loss:  1.28430957e-02\n",
      "Epoch: 1891 mean train loss:  2.04961701e-02, mean val. rec. loss:  1.28298418e-02\n",
      "Epoch: 1892 mean train loss:  2.04829429e-02, mean val. rec. loss:  1.28165720e-02\n",
      "Epoch: 1893 mean train loss:  2.04696823e-02, mean val. rec. loss:  1.28032818e-02\n",
      "Epoch: 1894 mean train loss:  2.04563993e-02, mean val. rec. loss:  1.27899723e-02\n",
      "Epoch: 1895 mean train loss:  2.04430902e-02, mean val. rec. loss:  1.27766526e-02\n",
      "Epoch: 1896 mean train loss:  2.04297532e-02, mean val. rec. loss:  1.27633159e-02\n",
      "Epoch: 1897 mean train loss:  2.04164032e-02, mean val. rec. loss:  1.27499645e-02\n",
      "Epoch: 1898 mean train loss:  2.04030252e-02, mean val. rec. loss:  1.27365836e-02\n",
      "Epoch: 1899 mean train loss:  2.03896267e-02, mean val. rec. loss:  1.27231970e-02\n",
      "Epoch: 1900 mean train loss:  2.03762003e-02, mean val. rec. loss:  1.27098161e-02\n",
      "Epoch: 1901 mean train loss:  2.03627721e-02, mean val. rec. loss:  1.26964238e-02\n",
      "Epoch: 1902 mean train loss:  2.03493066e-02, mean val. rec. loss:  1.26830157e-02\n",
      "Epoch: 1903 mean train loss:  2.03358280e-02, mean val. rec. loss:  1.26695928e-02\n",
      "Epoch: 1904 mean train loss:  2.03223365e-02, mean val. rec. loss:  1.26561688e-02\n",
      "Epoch: 1905 mean train loss:  2.03088170e-02, mean val. rec. loss:  1.26427413e-02\n",
      "Epoch: 1906 mean train loss:  2.02952919e-02, mean val. rec. loss:  1.26293128e-02\n",
      "Epoch: 1907 mean train loss:  2.02817537e-02, mean val. rec. loss:  1.26158729e-02\n",
      "Epoch: 1908 mean train loss:  2.02682007e-02, mean val. rec. loss:  1.26024466e-02\n",
      "Epoch: 1909 mean train loss:  2.02546328e-02, mean val. rec. loss:  1.25890226e-02\n",
      "Epoch: 1910 mean train loss:  2.02410500e-02, mean val. rec. loss:  1.25756009e-02\n",
      "Epoch: 1911 mean train loss:  2.02274579e-02, mean val. rec. loss:  1.25621836e-02\n",
      "Epoch: 1912 mean train loss:  2.02138564e-02, mean val. rec. loss:  1.25487687e-02\n",
      "Epoch: 1913 mean train loss:  2.02002494e-02, mean val. rec. loss:  1.25353526e-02\n",
      "Epoch: 1914 mean train loss:  2.01866312e-02, mean val. rec. loss:  1.25219445e-02\n",
      "Epoch: 1915 mean train loss:  2.01730093e-02, mean val. rec. loss:  1.25085624e-02\n",
      "Epoch: 1916 mean train loss:  2.01593780e-02, mean val. rec. loss:  1.24951804e-02\n",
      "Epoch: 1917 mean train loss:  2.01457431e-02, mean val. rec. loss:  1.24818210e-02\n",
      "Epoch: 1918 mean train loss:  2.01321193e-02, mean val. rec. loss:  1.24684798e-02\n",
      "Epoch: 1919 mean train loss:  2.01184769e-02, mean val. rec. loss:  1.24551533e-02\n",
      "Epoch: 1920 mean train loss:  2.01048456e-02, mean val. rec. loss:  1.24418517e-02\n",
      "Epoch: 1921 mean train loss:  2.00912163e-02, mean val. rec. loss:  1.24285695e-02\n",
      "Epoch: 1922 mean train loss:  2.00775757e-02, mean val. rec. loss:  1.24153031e-02\n",
      "Epoch: 1923 mean train loss:  2.00639557e-02, mean val. rec. loss:  1.24020616e-02\n",
      "Epoch: 1924 mean train loss:  2.00503300e-02, mean val. rec. loss:  1.23888451e-02\n",
      "Epoch: 1925 mean train loss:  2.00367249e-02, mean val. rec. loss:  1.23756672e-02\n",
      "Epoch: 1926 mean train loss:  2.00231178e-02, mean val. rec. loss:  1.23625199e-02\n",
      "Epoch: 1927 mean train loss:  2.00095127e-02, mean val. rec. loss:  1.23494088e-02\n",
      "Epoch: 1928 mean train loss:  1.99959261e-02, mean val. rec. loss:  1.23363193e-02\n",
      "Epoch: 1929 mean train loss:  1.99823657e-02, mean val. rec. loss:  1.23232820e-02\n",
      "Epoch: 1930 mean train loss:  1.99688015e-02, mean val. rec. loss:  1.23102753e-02\n",
      "Epoch: 1931 mean train loss:  1.99552596e-02, mean val. rec. loss:  1.22973117e-02\n",
      "Epoch: 1932 mean train loss:  1.99417252e-02, mean val. rec. loss:  1.22843832e-02\n",
      "Epoch: 1933 mean train loss:  1.99282169e-02, mean val. rec. loss:  1.22715001e-02\n",
      "Epoch: 1934 mean train loss:  1.99147328e-02, mean val. rec. loss:  1.22586657e-02\n",
      "Epoch: 1935 mean train loss:  1.99012654e-02, mean val. rec. loss:  1.22458790e-02\n",
      "Epoch: 1936 mean train loss:  1.98878260e-02, mean val. rec. loss:  1.22331513e-02\n",
      "Epoch: 1937 mean train loss:  1.98744033e-02, mean val. rec. loss:  1.22204723e-02\n",
      "Epoch: 1938 mean train loss:  1.98610104e-02, mean val. rec. loss:  1.22078545e-02\n",
      "Epoch: 1939 mean train loss:  1.98476399e-02, mean val. rec. loss:  1.21952810e-02\n",
      "Epoch: 1940 mean train loss:  1.98343085e-02, mean val. rec. loss:  1.21827766e-02\n",
      "Epoch: 1941 mean train loss:  1.98209957e-02, mean val. rec. loss:  1.21703142e-02\n",
      "Epoch: 1942 mean train loss:  1.98077145e-02, mean val. rec. loss:  1.21579142e-02\n",
      "Epoch: 1943 mean train loss:  1.97944799e-02, mean val. rec. loss:  1.21455901e-02\n",
      "Epoch: 1944 mean train loss:  1.97812751e-02, mean val. rec. loss:  1.21333375e-02\n",
      "Epoch: 1945 mean train loss:  1.97681020e-02, mean val. rec. loss:  1.21211393e-02\n",
      "Epoch: 1946 mean train loss:  1.97549661e-02, mean val. rec. loss:  1.21090171e-02\n",
      "Epoch: 1947 mean train loss:  1.97418786e-02, mean val. rec. loss:  1.20969652e-02\n",
      "Epoch: 1948 mean train loss:  1.97288266e-02, mean val. rec. loss:  1.20849938e-02\n",
      "Epoch: 1949 mean train loss:  1.97158229e-02, mean val. rec. loss:  1.20730939e-02\n",
      "Epoch: 1950 mean train loss:  1.97028583e-02, mean val. rec. loss:  1.20612642e-02\n",
      "Epoch: 1951 mean train loss:  1.96899384e-02, mean val. rec. loss:  1.20495106e-02\n",
      "Epoch: 1952 mean train loss:  1.96770670e-02, mean val. rec. loss:  1.20378453e-02\n",
      "Epoch: 1953 mean train loss:  1.96642439e-02, mean val. rec. loss:  1.20262663e-02\n",
      "Epoch: 1954 mean train loss:  1.96514581e-02, mean val. rec. loss:  1.20147542e-02\n",
      "Epoch: 1955 mean train loss:  1.96387375e-02, mean val. rec. loss:  1.20033282e-02\n",
      "Epoch: 1956 mean train loss:  1.96260653e-02, mean val. rec. loss:  1.19919930e-02\n",
      "Epoch: 1957 mean train loss:  1.96134508e-02, mean val. rec. loss:  1.19807428e-02\n",
      "Epoch: 1958 mean train loss:  1.96008848e-02, mean val. rec. loss:  1.19695845e-02\n",
      "Epoch: 1959 mean train loss:  1.95883820e-02, mean val. rec. loss:  1.19585157e-02\n",
      "Epoch: 1960 mean train loss:  1.95759295e-02, mean val. rec. loss:  1.19475434e-02\n",
      "Epoch: 1961 mean train loss:  1.95635404e-02, mean val. rec. loss:  1.19366640e-02\n",
      "Epoch: 1962 mean train loss:  1.95512201e-02, mean val. rec. loss:  1.19258754e-02\n",
      "Epoch: 1963 mean train loss:  1.95389483e-02, mean val. rec. loss:  1.19151820e-02\n",
      "Epoch: 1964 mean train loss:  1.95267435e-02, mean val. rec. loss:  1.19045838e-02\n",
      "Epoch: 1965 mean train loss:  1.95145908e-02, mean val. rec. loss:  1.18940923e-02\n",
      "Epoch: 1966 mean train loss:  1.95025164e-02, mean val. rec. loss:  1.18836937e-02\n",
      "Epoch: 1967 mean train loss:  1.94905034e-02, mean val. rec. loss:  1.18733790e-02\n",
      "Epoch: 1968 mean train loss:  1.94785537e-02, mean val. rec. loss:  1.18631665e-02\n",
      "Epoch: 1969 mean train loss:  1.94666692e-02, mean val. rec. loss:  1.18530423e-02\n",
      "Epoch: 1970 mean train loss:  1.94548574e-02, mean val. rec. loss:  1.18430497e-02\n",
      "Epoch: 1971 mean train loss:  1.94431125e-02, mean val. rec. loss:  1.18331569e-02\n",
      "Epoch: 1972 mean train loss:  1.94314478e-02, mean val. rec. loss:  1.18233571e-02\n",
      "Epoch: 1973 mean train loss:  1.94198445e-02, mean val. rec. loss:  1.18136661e-02\n",
      "Epoch: 1974 mean train loss:  1.94083156e-02, mean val. rec. loss:  1.18040942e-02\n",
      "Epoch: 1975 mean train loss:  1.93968520e-02, mean val. rec. loss:  1.17946130e-02\n",
      "Epoch: 1976 mean train loss:  1.93854628e-02, mean val. rec. loss:  1.17852260e-02\n",
      "Epoch: 1977 mean train loss:  1.93741556e-02, mean val. rec. loss:  1.17759444e-02\n",
      "Epoch: 1978 mean train loss:  1.93629117e-02, mean val. rec. loss:  1.17667682e-02\n",
      "Epoch: 1979 mean train loss:  1.93517423e-02, mean val. rec. loss:  1.17576976e-02\n",
      "Epoch: 1980 mean train loss:  1.93406511e-02, mean val. rec. loss:  1.17487210e-02\n",
      "Epoch: 1981 mean train loss:  1.93296399e-02, mean val. rec. loss:  1.17398578e-02\n",
      "Epoch: 1982 mean train loss:  1.93186996e-02, mean val. rec. loss:  1.17311035e-02\n",
      "Epoch: 1983 mean train loss:  1.93078393e-02, mean val. rec. loss:  1.17224581e-02\n",
      "Epoch: 1984 mean train loss:  1.92970460e-02, mean val. rec. loss:  1.17139283e-02\n",
      "Epoch: 1985 mean train loss:  1.92863384e-02, mean val. rec. loss:  1.17054847e-02\n",
      "Epoch: 1986 mean train loss:  1.92757053e-02, mean val. rec. loss:  1.16971421e-02\n",
      "Epoch: 1987 mean train loss:  1.92651579e-02, mean val. rec. loss:  1.16889173e-02\n",
      "Epoch: 1988 mean train loss:  1.92546700e-02, mean val. rec. loss:  1.16807890e-02\n",
      "Epoch: 1989 mean train loss:  1.92442715e-02, mean val. rec. loss:  1.16727582e-02\n",
      "Epoch: 1990 mean train loss:  1.92339513e-02, mean val. rec. loss:  1.16648317e-02\n",
      "Epoch: 1991 mean train loss:  1.92236981e-02, mean val. rec. loss:  1.16569936e-02\n",
      "Epoch: 1992 mean train loss:  1.92135342e-02, mean val. rec. loss:  1.16492645e-02\n",
      "Epoch: 1993 mean train loss:  1.92034337e-02, mean val. rec. loss:  1.16416373e-02\n",
      "Epoch: 1994 mean train loss:  1.91934151e-02, mean val. rec. loss:  1.16341089e-02\n",
      "Epoch: 1995 mean train loss:  1.91834859e-02, mean val. rec. loss:  1.16266802e-02\n",
      "Epoch: 1996 mean train loss:  1.91736219e-02, mean val. rec. loss:  1.16193501e-02\n",
      "Epoch: 1997 mean train loss:  1.91638417e-02, mean val. rec. loss:  1.16121199e-02\n",
      "Epoch: 1998 mean train loss:  1.91541341e-02, mean val. rec. loss:  1.16049849e-02\n",
      "Epoch: 1999 mean train loss:  1.91444972e-02, mean val. rec. loss:  1.15979452e-02\n",
      "Epoch: 2000 mean train loss:  1.91349386e-02, mean val. rec. loss:  1.15909962e-02\n",
      "Epoch: 2001 mean train loss:  1.91254526e-02, mean val. rec. loss:  1.15841266e-02\n",
      "Epoch: 2002 mean train loss:  1.91160504e-02, mean val. rec. loss:  1.15773522e-02\n",
      "Epoch: 2003 mean train loss:  1.91067227e-02, mean val. rec. loss:  1.15706651e-02\n",
      "Epoch: 2004 mean train loss:  1.90974658e-02, mean val. rec. loss:  1.15640688e-02\n",
      "Epoch: 2005 mean train loss:  1.90882814e-02, mean val. rec. loss:  1.15575734e-02\n",
      "Epoch: 2006 mean train loss:  1.90791660e-02, mean val. rec. loss:  1.15511596e-02\n",
      "Epoch: 2007 mean train loss:  1.90701344e-02, mean val. rec. loss:  1.15448388e-02\n",
      "Epoch: 2008 mean train loss:  1.90611624e-02, mean val. rec. loss:  1.15385985e-02\n",
      "Epoch: 2009 mean train loss:  1.90522760e-02, mean val. rec. loss:  1.15324172e-02\n",
      "Epoch: 2010 mean train loss:  1.90434548e-02, mean val. rec. loss:  1.15263255e-02\n",
      "Epoch: 2011 mean train loss:  1.90347006e-02, mean val. rec. loss:  1.15203200e-02\n",
      "Epoch: 2012 mean train loss:  1.90260172e-02, mean val. rec. loss:  1.15143734e-02\n",
      "Epoch: 2013 mean train loss:  1.90174120e-02, mean val. rec. loss:  1.15085085e-02\n",
      "Epoch: 2014 mean train loss:  1.90088571e-02, mean val. rec. loss:  1.15027297e-02\n",
      "Epoch: 2015 mean train loss:  1.90003805e-02, mean val. rec. loss:  1.14970190e-02\n",
      "Epoch: 2016 mean train loss:  1.89919764e-02, mean val. rec. loss:  1.14913854e-02\n",
      "Epoch: 2017 mean train loss:  1.89836319e-02, mean val. rec. loss:  1.14858176e-02\n",
      "Epoch: 2018 mean train loss:  1.89753601e-02, mean val. rec. loss:  1.14803246e-02\n",
      "Epoch: 2019 mean train loss:  1.89671515e-02, mean val. rec. loss:  1.14749019e-02\n",
      "Epoch: 2020 mean train loss:  1.89590026e-02, mean val. rec. loss:  1.14695382e-02\n",
      "Epoch: 2021 mean train loss:  1.89509132e-02, mean val. rec. loss:  1.14642153e-02\n",
      "Epoch: 2022 mean train loss:  1.89428872e-02, mean val. rec. loss:  1.14589730e-02\n",
      "Epoch: 2023 mean train loss:  1.89349394e-02, mean val. rec. loss:  1.14537964e-02\n",
      "Epoch: 2024 mean train loss:  1.89270325e-02, mean val. rec. loss:  1.14486753e-02\n",
      "Epoch: 2025 mean train loss:  1.89192057e-02, mean val. rec. loss:  1.14436076e-02\n",
      "Epoch: 2026 mean train loss:  1.89114199e-02, mean val. rec. loss:  1.14386034e-02\n",
      "Epoch: 2027 mean train loss:  1.89037104e-02, mean val. rec. loss:  1.14336694e-02\n",
      "Epoch: 2028 mean train loss:  1.88960419e-02, mean val. rec. loss:  1.14287808e-02\n",
      "Epoch: 2029 mean train loss:  1.88884479e-02, mean val. rec. loss:  1.14239365e-02\n",
      "Epoch: 2030 mean train loss:  1.88809060e-02, mean val. rec. loss:  1.14191454e-02\n",
      "Epoch: 2031 mean train loss:  1.88734163e-02, mean val. rec. loss:  1.14144190e-02\n",
      "Epoch: 2032 mean train loss:  1.88659843e-02, mean val. rec. loss:  1.14097391e-02\n",
      "Epoch: 2033 mean train loss:  1.88585989e-02, mean val. rec. loss:  1.14050943e-02\n",
      "Epoch: 2034 mean train loss:  1.88512711e-02, mean val. rec. loss:  1.14004859e-02\n",
      "Epoch: 2035 mean train loss:  1.88439956e-02, mean val. rec. loss:  1.13959273e-02\n",
      "Epoch: 2036 mean train loss:  1.88367758e-02, mean val. rec. loss:  1.13914254e-02\n",
      "Epoch: 2037 mean train loss:  1.88296101e-02, mean val. rec. loss:  1.13869519e-02\n",
      "Epoch: 2038 mean train loss:  1.88224854e-02, mean val. rec. loss:  1.13825259e-02\n",
      "Epoch: 2039 mean train loss:  1.88154202e-02, mean val. rec. loss:  1.13781386e-02\n",
      "Epoch: 2040 mean train loss:  1.88084110e-02, mean val. rec. loss:  1.13738034e-02\n",
      "Epoch: 2041 mean train loss:  1.88014389e-02, mean val. rec. loss:  1.13695079e-02\n",
      "Epoch: 2042 mean train loss:  1.87945190e-02, mean val. rec. loss:  1.13652373e-02\n",
      "Epoch: 2043 mean train loss:  1.87876419e-02, mean val. rec. loss:  1.13610031e-02\n",
      "Epoch: 2044 mean train loss:  1.87808152e-02, mean val. rec. loss:  1.13567994e-02\n",
      "Epoch: 2045 mean train loss:  1.87740293e-02, mean val. rec. loss:  1.13526298e-02\n",
      "Epoch: 2046 mean train loss:  1.87672957e-02, mean val. rec. loss:  1.13484874e-02\n",
      "Epoch: 2047 mean train loss:  1.87606011e-02, mean val. rec. loss:  1.13443801e-02\n",
      "Epoch: 2048 mean train loss:  1.87539531e-02, mean val. rec. loss:  1.13403069e-02\n",
      "Epoch: 2049 mean train loss:  1.87473441e-02, mean val. rec. loss:  1.13362688e-02\n",
      "Epoch: 2050 mean train loss:  1.87407799e-02, mean val. rec. loss:  1.13322624e-02\n",
      "Epoch: 2051 mean train loss:  1.87342585e-02, mean val. rec. loss:  1.13282765e-02\n",
      "Epoch: 2052 mean train loss:  1.87277818e-02, mean val. rec. loss:  1.13243235e-02\n",
      "Epoch: 2053 mean train loss:  1.87213387e-02, mean val. rec. loss:  1.13203897e-02\n",
      "Epoch: 2054 mean train loss:  1.87149402e-02, mean val. rec. loss:  1.13164865e-02\n",
      "Epoch: 2055 mean train loss:  1.87085826e-02, mean val. rec. loss:  1.13126049e-02\n",
      "Epoch: 2056 mean train loss:  1.87022512e-02, mean val. rec. loss:  1.13087494e-02\n",
      "Epoch: 2057 mean train loss:  1.86959756e-02, mean val. rec. loss:  1.13049075e-02\n",
      "Epoch: 2058 mean train loss:  1.86897317e-02, mean val. rec. loss:  1.13010905e-02\n",
      "Epoch: 2059 mean train loss:  1.86835194e-02, mean val. rec. loss:  1.12973019e-02\n",
      "Epoch: 2060 mean train loss:  1.86773593e-02, mean val. rec. loss:  1.12935292e-02\n",
      "Epoch: 2061 mean train loss:  1.86712197e-02, mean val. rec. loss:  1.12897848e-02\n",
      "Epoch: 2062 mean train loss:  1.86651210e-02, mean val. rec. loss:  1.12860608e-02\n",
      "Epoch: 2063 mean train loss:  1.86590633e-02, mean val. rec. loss:  1.12823754e-02\n",
      "Epoch: 2064 mean train loss:  1.86530242e-02, mean val. rec. loss:  1.12786832e-02\n",
      "Epoch: 2065 mean train loss:  1.86470391e-02, mean val. rec. loss:  1.12750102e-02\n",
      "Epoch: 2066 mean train loss:  1.86410745e-02, mean val. rec. loss:  1.12713656e-02\n",
      "Epoch: 2067 mean train loss:  1.86351490e-02, mean val. rec. loss:  1.12677403e-02\n",
      "Epoch: 2068 mean train loss:  1.86292533e-02, mean val. rec. loss:  1.12641365e-02\n",
      "Epoch: 2069 mean train loss:  1.86233986e-02, mean val. rec. loss:  1.12605361e-02\n",
      "Epoch: 2070 mean train loss:  1.86175625e-02, mean val. rec. loss:  1.12569596e-02\n",
      "Epoch: 2071 mean train loss:  1.86117674e-02, mean val. rec. loss:  1.12534068e-02\n",
      "Epoch: 2072 mean train loss:  1.86060020e-02, mean val. rec. loss:  1.12498620e-02\n",
      "Epoch: 2073 mean train loss:  1.86002665e-02, mean val. rec. loss:  1.12463263e-02\n",
      "Epoch: 2074 mean train loss:  1.85945570e-02, mean val. rec. loss:  1.12428041e-02\n",
      "Epoch: 2075 mean train loss:  1.85888885e-02, mean val. rec. loss:  1.12393070e-02\n",
      "Epoch: 2076 mean train loss:  1.85832461e-02, mean val. rec. loss:  1.12358257e-02\n",
      "Epoch: 2077 mean train loss:  1.85776260e-02, mean val. rec. loss:  1.12323602e-02\n",
      "Epoch: 2078 mean train loss:  1.85720431e-02, mean val. rec. loss:  1.12289084e-02\n",
      "Epoch: 2079 mean train loss:  1.85664845e-02, mean val. rec. loss:  1.12254724e-02\n",
      "Epoch: 2080 mean train loss:  1.85609556e-02, mean val. rec. loss:  1.12220558e-02\n",
      "Epoch: 2081 mean train loss:  1.85554584e-02, mean val. rec. loss:  1.12186266e-02\n",
      "Epoch: 2082 mean train loss:  1.85499836e-02, mean val. rec. loss:  1.12152122e-02\n",
      "Epoch: 2083 mean train loss:  1.85445386e-02, mean val. rec. loss:  1.12118262e-02\n",
      "Epoch: 2084 mean train loss:  1.85391196e-02, mean val. rec. loss:  1.12084492e-02\n",
      "Epoch: 2085 mean train loss:  1.85337267e-02, mean val. rec. loss:  1.12050926e-02\n",
      "Epoch: 2086 mean train loss:  1.85283673e-02, mean val. rec. loss:  1.12017531e-02\n",
      "Epoch: 2087 mean train loss:  1.85230265e-02, mean val. rec. loss:  1.11984282e-02\n",
      "Epoch: 2088 mean train loss:  1.85177081e-02, mean val. rec. loss:  1.11951159e-02\n",
      "Epoch: 2089 mean train loss:  1.85124269e-02, mean val. rec. loss:  1.11918001e-02\n",
      "Epoch: 2090 mean train loss:  1.85071700e-02, mean val. rec. loss:  1.11884969e-02\n",
      "Epoch: 2091 mean train loss:  1.85019279e-02, mean val. rec. loss:  1.11852027e-02\n",
      "Epoch: 2092 mean train loss:  1.84967212e-02, mean val. rec. loss:  1.11819232e-02\n",
      "Epoch: 2093 mean train loss:  1.84915313e-02, mean val. rec. loss:  1.11786619e-02\n",
      "Epoch: 2094 mean train loss:  1.84863730e-02, mean val. rec. loss:  1.11754165e-02\n",
      "Epoch: 2095 mean train loss:  1.84812334e-02, mean val. rec. loss:  1.11721812e-02\n",
      "Epoch: 2096 mean train loss:  1.84761217e-02, mean val. rec. loss:  1.11689664e-02\n",
      "Epoch: 2097 mean train loss:  1.84710305e-02, mean val. rec. loss:  1.11657459e-02\n",
      "Epoch: 2098 mean train loss:  1.84659690e-02, mean val. rec. loss:  1.11625424e-02\n",
      "Epoch: 2099 mean train loss:  1.84609225e-02, mean val. rec. loss:  1.11593525e-02\n",
      "Epoch: 2100 mean train loss:  1.84559076e-02, mean val. rec. loss:  1.11561683e-02\n",
      "Epoch: 2101 mean train loss:  1.84509076e-02, mean val. rec. loss:  1.11529932e-02\n",
      "Epoch: 2102 mean train loss:  1.84459393e-02, mean val. rec. loss:  1.11498407e-02\n",
      "Epoch: 2103 mean train loss:  1.84409821e-02, mean val. rec. loss:  1.11466951e-02\n",
      "Epoch: 2104 mean train loss:  1.84360622e-02, mean val. rec. loss:  1.11435676e-02\n",
      "Epoch: 2105 mean train loss:  1.84311535e-02, mean val. rec. loss:  1.11404435e-02\n",
      "Epoch: 2106 mean train loss:  1.84262634e-02, mean val. rec. loss:  1.11373375e-02\n",
      "Epoch: 2107 mean train loss:  1.84214087e-02, mean val. rec. loss:  1.11342429e-02\n",
      "Epoch: 2108 mean train loss:  1.84165726e-02, mean val. rec. loss:  1.11311585e-02\n",
      "Epoch: 2109 mean train loss:  1.84117458e-02, mean val. rec. loss:  1.11280650e-02\n",
      "Epoch: 2110 mean train loss:  1.84069488e-02, mean val. rec. loss:  1.11249931e-02\n",
      "Epoch: 2111 mean train loss:  1.84021741e-02, mean val. rec. loss:  1.11219268e-02\n",
      "Epoch: 2112 mean train loss:  1.83974162e-02, mean val. rec. loss:  1.11188798e-02\n",
      "Epoch: 2113 mean train loss:  1.83926825e-02, mean val. rec. loss:  1.11158430e-02\n",
      "Epoch: 2114 mean train loss:  1.83879712e-02, mean val. rec. loss:  1.11128278e-02\n",
      "Epoch: 2115 mean train loss:  1.83832747e-02, mean val. rec. loss:  1.11098148e-02\n",
      "Epoch: 2116 mean train loss:  1.83786062e-02, mean val. rec. loss:  1.11068188e-02\n",
      "Epoch: 2117 mean train loss:  1.83739507e-02, mean val. rec. loss:  1.11038285e-02\n",
      "Epoch: 2118 mean train loss:  1.83693139e-02, mean val. rec. loss:  1.11008507e-02\n",
      "Epoch: 2119 mean train loss:  1.83647031e-02, mean val. rec. loss:  1.10978819e-02\n",
      "Epoch: 2120 mean train loss:  1.83601184e-02, mean val. rec. loss:  1.10949245e-02\n",
      "Epoch: 2121 mean train loss:  1.83555411e-02, mean val. rec. loss:  1.10919592e-02\n",
      "Epoch: 2122 mean train loss:  1.83509862e-02, mean val. rec. loss:  1.10890131e-02\n",
      "Epoch: 2123 mean train loss:  1.83464536e-02, mean val. rec. loss:  1.10860840e-02\n",
      "Epoch: 2124 mean train loss:  1.83419378e-02, mean val. rec. loss:  1.10831743e-02\n",
      "Epoch: 2125 mean train loss:  1.83374406e-02, mean val. rec. loss:  1.10802838e-02\n",
      "Epoch: 2126 mean train loss:  1.83329621e-02, mean val. rec. loss:  1.10773955e-02\n",
      "Epoch: 2127 mean train loss:  1.83285058e-02, mean val. rec. loss:  1.10745073e-02\n",
      "Epoch: 2128 mean train loss:  1.83240682e-02, mean val. rec. loss:  1.10716224e-02\n",
      "Epoch: 2129 mean train loss:  1.83196418e-02, mean val. rec. loss:  1.10687592e-02\n",
      "Epoch: 2130 mean train loss:  1.83152340e-02, mean val. rec. loss:  1.10658959e-02\n",
      "Epoch: 2131 mean train loss:  1.83108523e-02, mean val. rec. loss:  1.10630484e-02\n",
      "Epoch: 2132 mean train loss:  1.83064910e-02, mean val. rec. loss:  1.10602203e-02\n",
      "Epoch: 2133 mean train loss:  1.83021391e-02, mean val. rec. loss:  1.10573990e-02\n",
      "Epoch: 2134 mean train loss:  1.82978095e-02, mean val. rec. loss:  1.10545924e-02\n",
      "Epoch: 2135 mean train loss:  1.82934967e-02, mean val. rec. loss:  1.10517971e-02\n",
      "Epoch: 2136 mean train loss:  1.82892006e-02, mean val. rec. loss:  1.10490098e-02\n",
      "Epoch: 2137 mean train loss:  1.82849231e-02, mean val. rec. loss:  1.10462304e-02\n",
      "Epoch: 2138 mean train loss:  1.82806662e-02, mean val. rec. loss:  1.10434533e-02\n",
      "Epoch: 2139 mean train loss:  1.82764222e-02, mean val. rec. loss:  1.10406932e-02\n",
      "Epoch: 2140 mean train loss:  1.82721932e-02, mean val. rec. loss:  1.10379331e-02\n",
      "Epoch: 2141 mean train loss:  1.82679828e-02, mean val. rec. loss:  1.10351855e-02\n",
      "Epoch: 2142 mean train loss:  1.82637947e-02, mean val. rec. loss:  1.10324515e-02\n",
      "Epoch: 2143 mean train loss:  1.82596290e-02, mean val. rec. loss:  1.10297470e-02\n",
      "Epoch: 2144 mean train loss:  1.82554670e-02, mean val. rec. loss:  1.10270481e-02\n",
      "Epoch: 2145 mean train loss:  1.82513218e-02, mean val. rec. loss:  1.10243549e-02\n",
      "Epoch: 2146 mean train loss:  1.82472007e-02, mean val. rec. loss:  1.10216674e-02\n",
      "Epoch: 2147 mean train loss:  1.82430965e-02, mean val. rec. loss:  1.10189957e-02\n",
      "Epoch: 2148 mean train loss:  1.82390015e-02, mean val. rec. loss:  1.10163184e-02\n",
      "Epoch: 2149 mean train loss:  1.82349215e-02, mean val. rec. loss:  1.10136547e-02\n",
      "Epoch: 2150 mean train loss:  1.82308675e-02, mean val. rec. loss:  1.10110001e-02\n",
      "Epoch: 2151 mean train loss:  1.82268247e-02, mean val. rec. loss:  1.10083579e-02\n",
      "Epoch: 2152 mean train loss:  1.82227986e-02, mean val. rec. loss:  1.10057350e-02\n",
      "Epoch: 2153 mean train loss:  1.82187856e-02, mean val. rec. loss:  1.10031235e-02\n",
      "Epoch: 2154 mean train loss:  1.82147930e-02, mean val. rec. loss:  1.10005176e-02\n",
      "Epoch: 2155 mean train loss:  1.82108117e-02, mean val. rec. loss:  1.09979196e-02\n",
      "Epoch: 2156 mean train loss:  1.82068489e-02, mean val. rec. loss:  1.09953330e-02\n",
      "Epoch: 2157 mean train loss:  1.82029011e-02, mean val. rec. loss:  1.09927521e-02\n",
      "Epoch: 2158 mean train loss:  1.81989681e-02, mean val. rec. loss:  1.09901836e-02\n",
      "Epoch: 2159 mean train loss:  1.81950501e-02, mean val. rec. loss:  1.09876231e-02\n",
      "Epoch: 2160 mean train loss:  1.81911469e-02, mean val. rec. loss:  1.09850717e-02\n",
      "Epoch: 2161 mean train loss:  1.81872587e-02, mean val. rec. loss:  1.09825293e-02\n",
      "Epoch: 2162 mean train loss:  1.81833853e-02, mean val. rec. loss:  1.09800073e-02\n",
      "Epoch: 2163 mean train loss:  1.81795306e-02, mean val. rec. loss:  1.09774956e-02\n",
      "Epoch: 2164 mean train loss:  1.81756852e-02, mean val. rec. loss:  1.09749906e-02\n",
      "Epoch: 2165 mean train loss:  1.81718565e-02, mean val. rec. loss:  1.09724902e-02\n",
      "Epoch: 2166 mean train loss:  1.81680427e-02, mean val. rec. loss:  1.09699909e-02\n",
      "Epoch: 2167 mean train loss:  1.81642383e-02, mean val. rec. loss:  1.09674950e-02\n",
      "Epoch: 2168 mean train loss:  1.81604580e-02, mean val. rec. loss:  1.09650082e-02\n",
      "Epoch: 2169 mean train loss:  1.81566964e-02, mean val. rec. loss:  1.09625373e-02\n",
      "Epoch: 2170 mean train loss:  1.81529329e-02, mean val. rec. loss:  1.09600777e-02\n",
      "Epoch: 2171 mean train loss:  1.81491843e-02, mean val. rec. loss:  1.09576351e-02\n",
      "Epoch: 2172 mean train loss:  1.81454637e-02, mean val. rec. loss:  1.09552073e-02\n",
      "Epoch: 2173 mean train loss:  1.81417486e-02, mean val. rec. loss:  1.09527874e-02\n",
      "Epoch: 2174 mean train loss:  1.81380522e-02, mean val. rec. loss:  1.09503720e-02\n",
      "Epoch: 2175 mean train loss:  1.81343650e-02, mean val. rec. loss:  1.09479680e-02\n",
      "Epoch: 2176 mean train loss:  1.81306947e-02, mean val. rec. loss:  1.09455628e-02\n",
      "Epoch: 2177 mean train loss:  1.81270336e-02, mean val. rec. loss:  1.09431633e-02\n",
      "Epoch: 2178 mean train loss:  1.81233893e-02, mean val. rec. loss:  1.09407683e-02\n",
      "Epoch: 2179 mean train loss:  1.81197636e-02, mean val. rec. loss:  1.09384063e-02\n",
      "Epoch: 2180 mean train loss:  1.81161454e-02, mean val. rec. loss:  1.09360521e-02\n",
      "Epoch: 2181 mean train loss:  1.81125402e-02, mean val. rec. loss:  1.09337048e-02\n",
      "Epoch: 2182 mean train loss:  1.81089499e-02, mean val. rec. loss:  1.09313654e-02\n",
      "Epoch: 2183 mean train loss:  1.81053707e-02, mean val. rec. loss:  1.09290158e-02\n",
      "Epoch: 2184 mean train loss:  1.81018028e-02, mean val. rec. loss:  1.09266571e-02\n",
      "Epoch: 2185 mean train loss:  1.80982423e-02, mean val. rec. loss:  1.09243212e-02\n",
      "Epoch: 2186 mean train loss:  1.80947041e-02, mean val. rec. loss:  1.09220078e-02\n",
      "Epoch: 2187 mean train loss:  1.80911790e-02, mean val. rec. loss:  1.09197002e-02\n",
      "Epoch: 2188 mean train loss:  1.80876650e-02, mean val. rec. loss:  1.09174005e-02\n",
      "Epoch: 2189 mean train loss:  1.80841622e-02, mean val. rec. loss:  1.09151133e-02\n",
      "Epoch: 2190 mean train loss:  1.80806762e-02, mean val. rec. loss:  1.09128464e-02\n",
      "Epoch: 2191 mean train loss:  1.80771995e-02, mean val. rec. loss:  1.09105774e-02\n",
      "Epoch: 2192 mean train loss:  1.80737377e-02, mean val. rec. loss:  1.09082981e-02\n",
      "Epoch: 2193 mean train loss:  1.80702871e-02, mean val. rec. loss:  1.09060346e-02\n",
      "Epoch: 2194 mean train loss:  1.80668476e-02, mean val. rec. loss:  1.09037724e-02\n",
      "Epoch: 2195 mean train loss:  1.80634230e-02, mean val. rec. loss:  1.09015180e-02\n",
      "Epoch: 2196 mean train loss:  1.80600059e-02, mean val. rec. loss:  1.08992750e-02\n",
      "Epoch: 2197 mean train loss:  1.80566037e-02, mean val. rec. loss:  1.08970581e-02\n",
      "Epoch: 2198 mean train loss:  1.80532201e-02, mean val. rec. loss:  1.08948525e-02\n",
      "Epoch: 2199 mean train loss:  1.80498477e-02, mean val. rec. loss:  1.08926481e-02\n",
      "Epoch: 2200 mean train loss:  1.80464771e-02, mean val. rec. loss:  1.08904436e-02\n",
      "Epoch: 2201 mean train loss:  1.80431177e-02, mean val. rec. loss:  1.08882528e-02\n",
      "Epoch: 2202 mean train loss:  1.80397807e-02, mean val. rec. loss:  1.08860630e-02\n",
      "Epoch: 2203 mean train loss:  1.80364511e-02, mean val. rec. loss:  1.08838609e-02\n",
      "Epoch: 2204 mean train loss:  1.80331252e-02, mean val. rec. loss:  1.08816780e-02\n",
      "Epoch: 2205 mean train loss:  1.80298142e-02, mean val. rec. loss:  1.08795075e-02\n",
      "Epoch: 2206 mean train loss:  1.80265256e-02, mean val. rec. loss:  1.08773439e-02\n",
      "Epoch: 2207 mean train loss:  1.80232425e-02, mean val. rec. loss:  1.08751996e-02\n",
      "Epoch: 2208 mean train loss:  1.80199595e-02, mean val. rec. loss:  1.08730688e-02\n",
      "Epoch: 2209 mean train loss:  1.80167044e-02, mean val. rec. loss:  1.08709335e-02\n",
      "Epoch: 2210 mean train loss:  1.80134456e-02, mean val. rec. loss:  1.08687960e-02\n",
      "Epoch: 2211 mean train loss:  1.80102147e-02, mean val. rec. loss:  1.08666596e-02\n",
      "Epoch: 2212 mean train loss:  1.80069744e-02, mean val. rec. loss:  1.08645345e-02\n",
      "Epoch: 2213 mean train loss:  1.80037659e-02, mean val. rec. loss:  1.08624185e-02\n",
      "Epoch: 2214 mean train loss:  1.80005480e-02, mean val. rec. loss:  1.08603252e-02\n",
      "Epoch: 2215 mean train loss:  1.79973562e-02, mean val. rec. loss:  1.08582341e-02\n",
      "Epoch: 2216 mean train loss:  1.79941756e-02, mean val. rec. loss:  1.08561555e-02\n",
      "Epoch: 2217 mean train loss:  1.79909950e-02, mean val. rec. loss:  1.08540724e-02\n",
      "Epoch: 2218 mean train loss:  1.79878255e-02, mean val. rec. loss:  1.08519814e-02\n",
      "Epoch: 2219 mean train loss:  1.79846784e-02, mean val. rec. loss:  1.08498983e-02\n",
      "Epoch: 2220 mean train loss:  1.79815369e-02, mean val. rec. loss:  1.08478208e-02\n",
      "Epoch: 2221 mean train loss:  1.79783991e-02, mean val. rec. loss:  1.08457695e-02\n",
      "Epoch: 2222 mean train loss:  1.79752781e-02, mean val. rec. loss:  1.08437238e-02\n",
      "Epoch: 2223 mean train loss:  1.79721645e-02, mean val. rec. loss:  1.08416894e-02\n",
      "Epoch: 2224 mean train loss:  1.79690621e-02, mean val. rec. loss:  1.08396585e-02\n",
      "Epoch: 2225 mean train loss:  1.79659709e-02, mean val. rec. loss:  1.08376218e-02\n",
      "Epoch: 2226 mean train loss:  1.79628889e-02, mean val. rec. loss:  1.08355863e-02\n",
      "Epoch: 2227 mean train loss:  1.79598163e-02, mean val. rec. loss:  1.08335679e-02\n",
      "Epoch: 2228 mean train loss:  1.79567567e-02, mean val. rec. loss:  1.08315539e-02\n",
      "Epoch: 2229 mean train loss:  1.79537028e-02, mean val. rec. loss:  1.08295354e-02\n",
      "Epoch: 2230 mean train loss:  1.79506562e-02, mean val. rec. loss:  1.08275317e-02\n",
      "Epoch: 2231 mean train loss:  1.79476264e-02, mean val. rec. loss:  1.08255393e-02\n",
      "Epoch: 2232 mean train loss:  1.79446115e-02, mean val. rec. loss:  1.08235515e-02\n",
      "Epoch: 2233 mean train loss:  1.79415911e-02, mean val. rec. loss:  1.08215704e-02\n",
      "Epoch: 2234 mean train loss:  1.79385855e-02, mean val. rec. loss:  1.08195859e-02\n",
      "Epoch: 2235 mean train loss:  1.79355948e-02, mean val. rec. loss:  1.08176174e-02\n",
      "Epoch: 2236 mean train loss:  1.79326079e-02, mean val. rec. loss:  1.08156306e-02\n",
      "Epoch: 2237 mean train loss:  1.79296321e-02, mean val. rec. loss:  1.08136643e-02\n",
      "Epoch: 2238 mean train loss:  1.79266675e-02, mean val. rec. loss:  1.08117184e-02\n",
      "Epoch: 2239 mean train loss:  1.79237066e-02, mean val. rec. loss:  1.08097634e-02\n",
      "Epoch: 2240 mean train loss:  1.79207643e-02, mean val. rec. loss:  1.08078164e-02\n",
      "Epoch: 2241 mean train loss:  1.79178332e-02, mean val. rec. loss:  1.08058830e-02\n",
      "Epoch: 2242 mean train loss:  1.79148984e-02, mean val. rec. loss:  1.08039484e-02\n",
      "Epoch: 2243 mean train loss:  1.79119748e-02, mean val. rec. loss:  1.08020172e-02\n",
      "Epoch: 2244 mean train loss:  1.79090623e-02, mean val. rec. loss:  1.08000963e-02\n",
      "Epoch: 2245 mean train loss:  1.79061610e-02, mean val. rec. loss:  1.07981651e-02\n",
      "Epoch: 2246 mean train loss:  1.79032672e-02, mean val. rec. loss:  1.07962442e-02\n",
      "Epoch: 2247 mean train loss:  1.79003845e-02, mean val. rec. loss:  1.07943277e-02\n",
      "Epoch: 2248 mean train loss:  1.78975093e-02, mean val. rec. loss:  1.07924272e-02\n",
      "Epoch: 2249 mean train loss:  1.78946471e-02, mean val. rec. loss:  1.07905267e-02\n",
      "Epoch: 2250 mean train loss:  1.78917961e-02, mean val. rec. loss:  1.07886420e-02\n",
      "Epoch: 2251 mean train loss:  1.78889432e-02, mean val. rec. loss:  1.07867573e-02\n",
      "Epoch: 2252 mean train loss:  1.78860959e-02, mean val. rec. loss:  1.07848806e-02\n",
      "Epoch: 2253 mean train loss:  1.78832747e-02, mean val. rec. loss:  1.07829970e-02\n",
      "Epoch: 2254 mean train loss:  1.78804405e-02, mean val. rec. loss:  1.07811146e-02\n",
      "Epoch: 2255 mean train loss:  1.78776341e-02, mean val. rec. loss:  1.07792322e-02\n",
      "Epoch: 2256 mean train loss:  1.78748204e-02, mean val. rec. loss:  1.07773612e-02\n",
      "Epoch: 2257 mean train loss:  1.78720308e-02, mean val. rec. loss:  1.07755060e-02\n",
      "Epoch: 2258 mean train loss:  1.78692375e-02, mean val. rec. loss:  1.07736554e-02\n",
      "Epoch: 2259 mean train loss:  1.78664517e-02, mean val. rec. loss:  1.07718104e-02\n",
      "Epoch: 2260 mean train loss:  1.78636826e-02, mean val. rec. loss:  1.07699563e-02\n",
      "Epoch: 2261 mean train loss:  1.78609210e-02, mean val. rec. loss:  1.07681215e-02\n",
      "Epoch: 2262 mean train loss:  1.78581631e-02, mean val. rec. loss:  1.07662890e-02\n",
      "Epoch: 2263 mean train loss:  1.78554126e-02, mean val. rec. loss:  1.07644452e-02\n",
      "Epoch: 2264 mean train loss:  1.78526715e-02, mean val. rec. loss:  1.07626036e-02\n",
      "Epoch: 2265 mean train loss:  1.78499396e-02, mean val. rec. loss:  1.07607824e-02\n",
      "Epoch: 2266 mean train loss:  1.78472115e-02, mean val. rec. loss:  1.07589726e-02\n",
      "Epoch: 2267 mean train loss:  1.78444853e-02, mean val. rec. loss:  1.07571458e-02\n",
      "Epoch: 2268 mean train loss:  1.78417814e-02, mean val. rec. loss:  1.07553167e-02\n",
      "Epoch: 2269 mean train loss:  1.78390849e-02, mean val. rec. loss:  1.07535205e-02\n",
      "Epoch: 2270 mean train loss:  1.78363773e-02, mean val. rec. loss:  1.07517197e-02\n",
      "Epoch: 2271 mean train loss:  1.78337013e-02, mean val. rec. loss:  1.07499167e-02\n",
      "Epoch: 2272 mean train loss:  1.78310123e-02, mean val. rec. loss:  1.07481182e-02\n",
      "Epoch: 2273 mean train loss:  1.78283513e-02, mean val. rec. loss:  1.07463356e-02\n",
      "Epoch: 2274 mean train loss:  1.78256809e-02, mean val. rec. loss:  1.07445405e-02\n",
      "Epoch: 2275 mean train loss:  1.78230179e-02, mean val. rec. loss:  1.07427454e-02\n",
      "Epoch: 2276 mean train loss:  1.78203736e-02, mean val. rec. loss:  1.07409560e-02\n",
      "Epoch: 2277 mean train loss:  1.78177349e-02, mean val. rec. loss:  1.07391983e-02\n",
      "Epoch: 2278 mean train loss:  1.78150999e-02, mean val. rec. loss:  1.07374248e-02\n",
      "Epoch: 2279 mean train loss:  1.78124742e-02, mean val. rec. loss:  1.07356535e-02\n",
      "Epoch: 2280 mean train loss:  1.78098523e-02, mean val. rec. loss:  1.07339095e-02\n",
      "Epoch: 2281 mean train loss:  1.78072377e-02, mean val. rec. loss:  1.07321427e-02\n",
      "Epoch: 2282 mean train loss:  1.78046251e-02, mean val. rec. loss:  1.07303579e-02\n",
      "Epoch: 2283 mean train loss:  1.78020292e-02, mean val. rec. loss:  1.07285957e-02\n",
      "Epoch: 2284 mean train loss:  1.77994426e-02, mean val. rec. loss:  1.07268516e-02\n",
      "Epoch: 2285 mean train loss:  1.77968467e-02, mean val. rec. loss:  1.07251007e-02\n",
      "Epoch: 2286 mean train loss:  1.77942769e-02, mean val. rec. loss:  1.07233635e-02\n",
      "Epoch: 2287 mean train loss:  1.77916959e-02, mean val. rec. loss:  1.07216296e-02\n",
      "Epoch: 2288 mean train loss:  1.77891410e-02, mean val. rec. loss:  1.07199003e-02\n",
      "Epoch: 2289 mean train loss:  1.77865786e-02, mean val. rec. loss:  1.07181574e-02\n",
      "Epoch: 2290 mean train loss:  1.77840237e-02, mean val. rec. loss:  1.07164122e-02\n",
      "Epoch: 2291 mean train loss:  1.77814837e-02, mean val. rec. loss:  1.07146818e-02\n",
      "Epoch: 2292 mean train loss:  1.77789511e-02, mean val. rec. loss:  1.07129581e-02\n",
      "Epoch: 2293 mean train loss:  1.77764204e-02, mean val. rec. loss:  1.07112322e-02\n",
      "Epoch: 2294 mean train loss:  1.77738934e-02, mean val. rec. loss:  1.07095256e-02\n",
      "Epoch: 2295 mean train loss:  1.77713775e-02, mean val. rec. loss:  1.07078235e-02\n",
      "Epoch: 2296 mean train loss:  1.77688580e-02, mean val. rec. loss:  1.07061044e-02\n",
      "Epoch: 2297 mean train loss:  1.77663515e-02, mean val. rec. loss:  1.07043841e-02\n",
      "Epoch: 2298 mean train loss:  1.77638599e-02, mean val. rec. loss:  1.07026798e-02\n",
      "Epoch: 2299 mean train loss:  1.77613664e-02, mean val. rec. loss:  1.07009765e-02\n",
      "Epoch: 2300 mean train loss:  1.77588804e-02, mean val. rec. loss:  1.06992676e-02\n",
      "Epoch: 2301 mean train loss:  1.77563999e-02, mean val. rec. loss:  1.06975769e-02\n",
      "Epoch: 2302 mean train loss:  1.77539270e-02, mean val. rec. loss:  1.06958918e-02\n",
      "Epoch: 2303 mean train loss:  1.77514558e-02, mean val. rec. loss:  1.06942067e-02\n",
      "Epoch: 2304 mean train loss:  1.77489996e-02, mean val. rec. loss:  1.06925193e-02\n",
      "Epoch: 2305 mean train loss:  1.77465490e-02, mean val. rec. loss:  1.06908399e-02\n",
      "Epoch: 2306 mean train loss:  1.77440983e-02, mean val. rec. loss:  1.06891457e-02\n",
      "Epoch: 2307 mean train loss:  1.77416588e-02, mean val. rec. loss:  1.06874595e-02\n",
      "Epoch: 2308 mean train loss:  1.77392212e-02, mean val. rec. loss:  1.06857767e-02\n",
      "Epoch: 2309 mean train loss:  1.77367911e-02, mean val. rec. loss:  1.06841098e-02\n",
      "Epoch: 2310 mean train loss:  1.77343591e-02, mean val. rec. loss:  1.06824405e-02\n",
      "Epoch: 2311 mean train loss:  1.77319363e-02, mean val. rec. loss:  1.06807645e-02\n",
      "Epoch: 2312 mean train loss:  1.77295285e-02, mean val. rec. loss:  1.06791066e-02\n",
      "Epoch: 2313 mean train loss:  1.77271226e-02, mean val. rec. loss:  1.06774374e-02\n",
      "Epoch: 2314 mean train loss:  1.77247222e-02, mean val. rec. loss:  1.06757705e-02\n",
      "Epoch: 2315 mean train loss:  1.77223274e-02, mean val. rec. loss:  1.06741126e-02\n",
      "Epoch: 2316 mean train loss:  1.77199420e-02, mean val. rec. loss:  1.06724661e-02\n",
      "Epoch: 2317 mean train loss:  1.77175528e-02, mean val. rec. loss:  1.06708059e-02\n",
      "Epoch: 2318 mean train loss:  1.77151710e-02, mean val. rec. loss:  1.06691628e-02\n",
      "Epoch: 2319 mean train loss:  1.77127949e-02, mean val. rec. loss:  1.06675129e-02\n",
      "Epoch: 2320 mean train loss:  1.77104299e-02, mean val. rec. loss:  1.06658652e-02\n",
      "Epoch: 2321 mean train loss:  1.77080687e-02, mean val. rec. loss:  1.06642017e-02\n",
      "Epoch: 2322 mean train loss:  1.77057111e-02, mean val. rec. loss:  1.06625676e-02\n",
      "Epoch: 2323 mean train loss:  1.77033666e-02, mean val. rec. loss:  1.06609347e-02\n",
      "Epoch: 2324 mean train loss:  1.77010258e-02, mean val. rec. loss:  1.06593052e-02\n",
      "Epoch: 2325 mean train loss:  1.76986832e-02, mean val. rec. loss:  1.06576779e-02\n",
      "Epoch: 2326 mean train loss:  1.76963480e-02, mean val. rec. loss:  1.06560438e-02\n",
      "Epoch: 2327 mean train loss:  1.76940166e-02, mean val. rec. loss:  1.06544052e-02\n",
      "Epoch: 2328 mean train loss:  1.76916926e-02, mean val. rec. loss:  1.06527723e-02\n",
      "Epoch: 2329 mean train loss:  1.76893704e-02, mean val. rec. loss:  1.06511405e-02\n",
      "Epoch: 2330 mean train loss:  1.76870613e-02, mean val. rec. loss:  1.06495167e-02\n",
      "Epoch: 2331 mean train loss:  1.76847578e-02, mean val. rec. loss:  1.06479041e-02\n",
      "Epoch: 2332 mean train loss:  1.76824561e-02, mean val. rec. loss:  1.06462871e-02\n",
      "Epoch: 2333 mean train loss:  1.76801581e-02, mean val. rec. loss:  1.06446723e-02\n",
      "Epoch: 2334 mean train loss:  1.76778639e-02, mean val. rec. loss:  1.06430666e-02\n",
      "Epoch: 2335 mean train loss:  1.76755753e-02, mean val. rec. loss:  1.06414496e-02\n",
      "Epoch: 2336 mean train loss:  1.76732848e-02, mean val. rec. loss:  1.06398268e-02\n",
      "Epoch: 2337 mean train loss:  1.76710148e-02, mean val. rec. loss:  1.06382189e-02\n",
      "Epoch: 2338 mean train loss:  1.76687411e-02, mean val. rec. loss:  1.06366041e-02\n",
      "Epoch: 2339 mean train loss:  1.76664748e-02, mean val. rec. loss:  1.06349904e-02\n",
      "Epoch: 2340 mean train loss:  1.76642103e-02, mean val. rec. loss:  1.06334017e-02\n",
      "Epoch: 2341 mean train loss:  1.76619571e-02, mean val. rec. loss:  1.06318096e-02\n",
      "Epoch: 2342 mean train loss:  1.76596983e-02, mean val. rec. loss:  1.06302141e-02\n",
      "Epoch: 2343 mean train loss:  1.76574469e-02, mean val. rec. loss:  1.06286220e-02\n",
      "Epoch: 2344 mean train loss:  1.76552029e-02, mean val. rec. loss:  1.06270186e-02\n",
      "Epoch: 2345 mean train loss:  1.76529627e-02, mean val. rec. loss:  1.06254083e-02\n",
      "Epoch: 2346 mean train loss:  1.76507318e-02, mean val. rec. loss:  1.06238151e-02\n",
      "Epoch: 2347 mean train loss:  1.76485084e-02, mean val. rec. loss:  1.06222252e-02\n",
      "Epoch: 2348 mean train loss:  1.76462849e-02, mean val. rec. loss:  1.06206434e-02\n",
      "Epoch: 2349 mean train loss:  1.76440614e-02, mean val. rec. loss:  1.06190739e-02\n",
      "Epoch: 2350 mean train loss:  1.76418473e-02, mean val. rec. loss:  1.06174954e-02\n",
      "Epoch: 2351 mean train loss:  1.76396369e-02, mean val. rec. loss:  1.06159056e-02\n",
      "Epoch: 2352 mean train loss:  1.76374320e-02, mean val. rec. loss:  1.06143203e-02\n",
      "Epoch: 2353 mean train loss:  1.76352235e-02, mean val. rec. loss:  1.06127395e-02\n",
      "Epoch: 2354 mean train loss:  1.76330335e-02, mean val. rec. loss:  1.06111463e-02\n",
      "Epoch: 2355 mean train loss:  1.76308436e-02, mean val. rec. loss:  1.06095689e-02\n",
      "Epoch: 2356 mean train loss:  1.76286574e-02, mean val. rec. loss:  1.06080029e-02\n",
      "Epoch: 2357 mean train loss:  1.76264731e-02, mean val. rec. loss:  1.06064358e-02\n",
      "Epoch: 2358 mean train loss:  1.76242924e-02, mean val. rec. loss:  1.06048777e-02\n",
      "Epoch: 2359 mean train loss:  1.76221099e-02, mean val. rec. loss:  1.06033026e-02\n",
      "Epoch: 2360 mean train loss:  1.76199386e-02, mean val. rec. loss:  1.06017173e-02\n",
      "Epoch: 2361 mean train loss:  1.76177748e-02, mean val. rec. loss:  1.06001479e-02\n",
      "Epoch: 2362 mean train loss:  1.76156109e-02, mean val. rec. loss:  1.05985648e-02\n",
      "Epoch: 2363 mean train loss:  1.76134563e-02, mean val. rec. loss:  1.05970045e-02\n",
      "Epoch: 2364 mean train loss:  1.76113036e-02, mean val. rec. loss:  1.05954521e-02\n",
      "Epoch: 2365 mean train loss:  1.76091584e-02, mean val. rec. loss:  1.05938996e-02\n",
      "Epoch: 2366 mean train loss:  1.76070076e-02, mean val. rec. loss:  1.05923540e-02\n",
      "Epoch: 2367 mean train loss:  1.76048679e-02, mean val. rec. loss:  1.05907982e-02\n",
      "Epoch: 2368 mean train loss:  1.76027283e-02, mean val. rec. loss:  1.05892356e-02\n",
      "Epoch: 2369 mean train loss:  1.76005961e-02, mean val. rec. loss:  1.05876628e-02\n",
      "Epoch: 2370 mean train loss:  1.75984713e-02, mean val. rec. loss:  1.05861070e-02\n",
      "Epoch: 2371 mean train loss:  1.75963447e-02, mean val. rec. loss:  1.05845466e-02\n",
      "Epoch: 2372 mean train loss:  1.75942181e-02, mean val. rec. loss:  1.05829942e-02\n",
      "Epoch: 2373 mean train loss:  1.75921101e-02, mean val. rec. loss:  1.05814622e-02\n",
      "Epoch: 2374 mean train loss:  1.75899890e-02, mean val. rec. loss:  1.05799154e-02\n",
      "Epoch: 2375 mean train loss:  1.75878903e-02, mean val. rec. loss:  1.05783834e-02\n",
      "Epoch: 2376 mean train loss:  1.75857786e-02, mean val. rec. loss:  1.05768231e-02\n",
      "Epoch: 2377 mean train loss:  1.75836725e-02, mean val. rec. loss:  1.05752707e-02\n",
      "Epoch: 2378 mean train loss:  1.75815831e-02, mean val. rec. loss:  1.05737205e-02\n",
      "Epoch: 2379 mean train loss:  1.75794863e-02, mean val. rec. loss:  1.05721749e-02\n",
      "Epoch: 2380 mean train loss:  1.75773987e-02, mean val. rec. loss:  1.05706248e-02\n",
      "Epoch: 2381 mean train loss:  1.75753094e-02, mean val. rec. loss:  1.05691064e-02\n",
      "Epoch: 2382 mean train loss:  1.75732274e-02, mean val. rec. loss:  1.05675608e-02\n",
      "Epoch: 2383 mean train loss:  1.75711530e-02, mean val. rec. loss:  1.05660242e-02\n",
      "Epoch: 2384 mean train loss:  1.75690766e-02, mean val. rec. loss:  1.05644911e-02\n",
      "Epoch: 2385 mean train loss:  1.75670077e-02, mean val. rec. loss:  1.05629466e-02\n",
      "Epoch: 2386 mean train loss:  1.75649407e-02, mean val. rec. loss:  1.05614033e-02\n",
      "Epoch: 2387 mean train loss:  1.75628811e-02, mean val. rec. loss:  1.05598804e-02\n",
      "Epoch: 2388 mean train loss:  1.75608141e-02, mean val. rec. loss:  1.05583518e-02\n",
      "Epoch: 2389 mean train loss:  1.75587545e-02, mean val. rec. loss:  1.05568220e-02\n",
      "Epoch: 2390 mean train loss:  1.75567024e-02, mean val. rec. loss:  1.05552809e-02\n",
      "Epoch: 2391 mean train loss:  1.75546540e-02, mean val. rec. loss:  1.05537648e-02\n",
      "Epoch: 2392 mean train loss:  1.75526130e-02, mean val. rec. loss:  1.05522328e-02\n",
      "Epoch: 2393 mean train loss:  1.75505720e-02, mean val. rec. loss:  1.05506917e-02\n",
      "Epoch: 2394 mean train loss:  1.75485311e-02, mean val. rec. loss:  1.05491733e-02\n",
      "Epoch: 2395 mean train loss:  1.75464957e-02, mean val. rec. loss:  1.05476527e-02\n",
      "Epoch: 2396 mean train loss:  1.75444640e-02, mean val. rec. loss:  1.05461252e-02\n",
      "Epoch: 2397 mean train loss:  1.75424343e-02, mean val. rec. loss:  1.05446057e-02\n",
      "Epoch: 2398 mean train loss:  1.75404063e-02, mean val. rec. loss:  1.05430884e-02\n",
      "Epoch: 2399 mean train loss:  1.75383877e-02, mean val. rec. loss:  1.05415700e-02\n",
      "Epoch: 2400 mean train loss:  1.75363728e-02, mean val. rec. loss:  1.05400437e-02\n",
      "Epoch: 2401 mean train loss:  1.75343523e-02, mean val. rec. loss:  1.05385276e-02\n",
      "Epoch: 2402 mean train loss:  1.75323468e-02, mean val. rec. loss:  1.05370114e-02\n",
      "Epoch: 2403 mean train loss:  1.75303300e-02, mean val. rec. loss:  1.05354772e-02\n",
      "Epoch: 2404 mean train loss:  1.75283319e-02, mean val. rec. loss:  1.05339701e-02\n",
      "Epoch: 2405 mean train loss:  1.75263300e-02, mean val. rec. loss:  1.05324676e-02\n",
      "Epoch: 2406 mean train loss:  1.75243319e-02, mean val. rec. loss:  1.05309424e-02\n",
      "Epoch: 2407 mean train loss:  1.75223356e-02, mean val. rec. loss:  1.05294331e-02\n",
      "Epoch: 2408 mean train loss:  1.75203487e-02, mean val. rec. loss:  1.05279249e-02\n",
      "Epoch: 2409 mean train loss:  1.75183561e-02, mean val. rec. loss:  1.05264122e-02\n",
      "Epoch: 2410 mean train loss:  1.75163636e-02, mean val. rec. loss:  1.05248870e-02\n",
      "Epoch: 2411 mean train loss:  1.75143822e-02, mean val. rec. loss:  1.05233799e-02\n",
      "Epoch: 2412 mean train loss:  1.75124008e-02, mean val. rec. loss:  1.05218774e-02\n",
      "Epoch: 2413 mean train loss:  1.75104307e-02, mean val. rec. loss:  1.05203658e-02\n",
      "Epoch: 2414 mean train loss:  1.75084642e-02, mean val. rec. loss:  1.05188542e-02\n",
      "Epoch: 2415 mean train loss:  1.75064884e-02, mean val. rec. loss:  1.05173642e-02\n",
      "Epoch: 2416 mean train loss:  1.75045238e-02, mean val. rec. loss:  1.05158605e-02\n",
      "Epoch: 2417 mean train loss:  1.75025610e-02, mean val. rec. loss:  1.05143546e-02\n",
      "Epoch: 2418 mean train loss:  1.75006001e-02, mean val. rec. loss:  1.05128430e-02\n",
      "Epoch: 2419 mean train loss:  1.74986393e-02, mean val. rec. loss:  1.05113280e-02\n",
      "Epoch: 2420 mean train loss:  1.74966784e-02, mean val. rec. loss:  1.05098289e-02\n",
      "Epoch: 2421 mean train loss:  1.74947249e-02, mean val. rec. loss:  1.05083207e-02\n",
      "Epoch: 2422 mean train loss:  1.74927771e-02, mean val. rec. loss:  1.05068125e-02\n",
      "Epoch: 2423 mean train loss:  1.74908292e-02, mean val. rec. loss:  1.05053179e-02\n",
      "Epoch: 2424 mean train loss:  1.74888926e-02, mean val. rec. loss:  1.05038302e-02\n",
      "Epoch: 2425 mean train loss:  1.74869559e-02, mean val. rec. loss:  1.05023401e-02\n",
      "Epoch: 2426 mean train loss:  1.74850080e-02, mean val. rec. loss:  1.05008319e-02\n",
      "Epoch: 2427 mean train loss:  1.74830844e-02, mean val. rec. loss:  1.04993362e-02\n",
      "Epoch: 2428 mean train loss:  1.74811458e-02, mean val. rec. loss:  1.04978382e-02\n",
      "Epoch: 2429 mean train loss:  1.74792129e-02, mean val. rec. loss:  1.04963300e-02\n",
      "Epoch: 2430 mean train loss:  1.74772930e-02, mean val. rec. loss:  1.04948389e-02\n",
      "Epoch: 2431 mean train loss:  1.74753693e-02, mean val. rec. loss:  1.04933477e-02\n",
      "Epoch: 2432 mean train loss:  1.74734475e-02, mean val. rec. loss:  1.04918610e-02\n",
      "Epoch: 2433 mean train loss:  1.74715220e-02, mean val. rec. loss:  1.04903778e-02\n",
      "Epoch: 2434 mean train loss:  1.74696152e-02, mean val. rec. loss:  1.04888730e-02\n",
      "Epoch: 2435 mean train loss:  1.74677046e-02, mean val. rec. loss:  1.04873830e-02\n",
      "Epoch: 2436 mean train loss:  1.74657865e-02, mean val. rec. loss:  1.04858793e-02\n",
      "Epoch: 2437 mean train loss:  1.74638871e-02, mean val. rec. loss:  1.04843938e-02\n",
      "Epoch: 2438 mean train loss:  1.74619802e-02, mean val. rec. loss:  1.04828958e-02\n",
      "Epoch: 2439 mean train loss:  1.74600714e-02, mean val. rec. loss:  1.04814137e-02\n",
      "Epoch: 2440 mean train loss:  1.74581794e-02, mean val. rec. loss:  1.04799350e-02\n",
      "Epoch: 2441 mean train loss:  1.74562782e-02, mean val. rec. loss:  1.04784427e-02\n",
      "Epoch: 2442 mean train loss:  1.74543880e-02, mean val. rec. loss:  1.04769708e-02\n",
      "Epoch: 2443 mean train loss:  1.74524886e-02, mean val. rec. loss:  1.04754762e-02\n",
      "Epoch: 2444 mean train loss:  1.74506041e-02, mean val. rec. loss:  1.04739657e-02\n",
      "Epoch: 2445 mean train loss:  1.74487232e-02, mean val. rec. loss:  1.04724678e-02\n",
      "Epoch: 2446 mean train loss:  1.74468294e-02, mean val. rec. loss:  1.04709868e-02\n",
      "Epoch: 2447 mean train loss:  1.74449579e-02, mean val. rec. loss:  1.04695115e-02\n",
      "Epoch: 2448 mean train loss:  1.74430734e-02, mean val. rec. loss:  1.04680271e-02\n",
      "Epoch: 2449 mean train loss:  1.74411944e-02, mean val. rec. loss:  1.04665586e-02\n",
      "Epoch: 2450 mean train loss:  1.74393173e-02, mean val. rec. loss:  1.04650742e-02\n",
      "Epoch: 2451 mean train loss:  1.74374477e-02, mean val. rec. loss:  1.04635819e-02\n",
      "Epoch: 2452 mean train loss:  1.74355818e-02, mean val. rec. loss:  1.04620998e-02\n",
      "Epoch: 2453 mean train loss:  1.74337196e-02, mean val. rec. loss:  1.04606120e-02\n",
      "Epoch: 2454 mean train loss:  1.74318499e-02, mean val. rec. loss:  1.04591379e-02\n",
      "Epoch: 2455 mean train loss:  1.74299877e-02, mean val. rec. loss:  1.04576592e-02\n",
      "Epoch: 2456 mean train loss:  1.74281330e-02, mean val. rec. loss:  1.04561884e-02\n",
      "Epoch: 2457 mean train loss:  1.74262783e-02, mean val. rec. loss:  1.04547188e-02\n",
      "Epoch: 2458 mean train loss:  1.74244198e-02, mean val. rec. loss:  1.04532412e-02\n",
      "Epoch: 2459 mean train loss:  1.74225651e-02, mean val. rec. loss:  1.04517511e-02\n",
      "Epoch: 2460 mean train loss:  1.74207140e-02, mean val. rec. loss:  1.04502634e-02\n",
      "Epoch: 2461 mean train loss:  1.74188686e-02, mean val. rec. loss:  1.04487779e-02\n",
      "Epoch: 2462 mean train loss:  1.74170250e-02, mean val. rec. loss:  1.04472969e-02\n",
      "Epoch: 2463 mean train loss:  1.74151759e-02, mean val. rec. loss:  1.04458397e-02\n",
      "Epoch: 2464 mean train loss:  1.74133398e-02, mean val. rec. loss:  1.04443746e-02\n",
      "Epoch: 2465 mean train loss:  1.74114999e-02, mean val. rec. loss:  1.04429061e-02\n",
      "Epoch: 2466 mean train loss:  1.74096675e-02, mean val. rec. loss:  1.04414297e-02\n",
      "Epoch: 2467 mean train loss:  1.74078258e-02, mean val. rec. loss:  1.04399510e-02\n",
      "Epoch: 2468 mean train loss:  1.74059897e-02, mean val. rec. loss:  1.04384530e-02\n",
      "Epoch: 2469 mean train loss:  1.74041629e-02, mean val. rec. loss:  1.04369720e-02\n",
      "Epoch: 2470 mean train loss:  1.74023324e-02, mean val. rec. loss:  1.04355103e-02\n",
      "Epoch: 2471 mean train loss:  1.74005130e-02, mean val. rec. loss:  1.04340305e-02\n",
      "Epoch: 2472 mean train loss:  1.73986955e-02, mean val. rec. loss:  1.04325779e-02\n",
      "Epoch: 2473 mean train loss:  1.73968631e-02, mean val. rec. loss:  1.04311173e-02\n",
      "Epoch: 2474 mean train loss:  1.73950531e-02, mean val. rec. loss:  1.04296454e-02\n",
      "Epoch: 2475 mean train loss:  1.73932318e-02, mean val. rec. loss:  1.04281758e-02\n",
      "Epoch: 2476 mean train loss:  1.73914125e-02, mean val. rec. loss:  1.04267016e-02\n",
      "Epoch: 2477 mean train loss:  1.73895987e-02, mean val. rec. loss:  1.04252116e-02\n",
      "Epoch: 2478 mean train loss:  1.73877868e-02, mean val. rec. loss:  1.04237272e-02\n",
      "Epoch: 2479 mean train loss:  1.73859786e-02, mean val. rec. loss:  1.04222734e-02\n",
      "Epoch: 2480 mean train loss:  1.73841779e-02, mean val. rec. loss:  1.04208163e-02\n",
      "Epoch: 2481 mean train loss:  1.73823771e-02, mean val. rec. loss:  1.04193625e-02\n",
      "Epoch: 2482 mean train loss:  1.73805671e-02, mean val. rec. loss:  1.04178963e-02\n",
      "Epoch: 2483 mean train loss:  1.73787756e-02, mean val. rec. loss:  1.04164300e-02\n",
      "Epoch: 2484 mean train loss:  1.73769730e-02, mean val. rec. loss:  1.04149536e-02\n",
      "Epoch: 2485 mean train loss:  1.73751704e-02, mean val. rec. loss:  1.04134772e-02\n",
      "Epoch: 2486 mean train loss:  1.73733753e-02, mean val. rec. loss:  1.04120064e-02\n",
      "Epoch: 2487 mean train loss:  1.73715857e-02, mean val. rec. loss:  1.04105447e-02\n",
      "Epoch: 2488 mean train loss:  1.73697980e-02, mean val. rec. loss:  1.04090955e-02\n",
      "Epoch: 2489 mean train loss:  1.73680140e-02, mean val. rec. loss:  1.04076474e-02\n",
      "Epoch: 2490 mean train loss:  1.73662189e-02, mean val. rec. loss:  1.04061789e-02\n",
      "Epoch: 2491 mean train loss:  1.73644424e-02, mean val. rec. loss:  1.04047093e-02\n",
      "Epoch: 2492 mean train loss:  1.73626509e-02, mean val. rec. loss:  1.04032385e-02\n",
      "Epoch: 2493 mean train loss:  1.73608725e-02, mean val. rec. loss:  1.04017632e-02\n",
      "Epoch: 2494 mean train loss:  1.73590997e-02, mean val. rec. loss:  1.04003094e-02\n",
      "Epoch: 2495 mean train loss:  1.73573213e-02, mean val. rec. loss:  1.03988511e-02\n",
      "Epoch: 2496 mean train loss:  1.73555467e-02, mean val. rec. loss:  1.03974065e-02\n",
      "Epoch: 2497 mean train loss:  1.73537683e-02, mean val. rec. loss:  1.03959380e-02\n",
      "Epoch: 2498 mean train loss:  1.73519955e-02, mean val. rec. loss:  1.03944729e-02\n",
      "Epoch: 2499 mean train loss:  1.73502338e-02, mean val. rec. loss:  1.03930010e-02\n",
      "Epoch: 2500 mean train loss:  1.73484573e-02, mean val. rec. loss:  1.03915393e-02\n",
      "Epoch: 2501 mean train loss:  1.73466920e-02, mean val. rec. loss:  1.03900798e-02\n",
      "Epoch: 2502 mean train loss:  1.73449359e-02, mean val. rec. loss:  1.03886385e-02\n",
      "Epoch: 2503 mean train loss:  1.73431724e-02, mean val. rec. loss:  1.03871882e-02\n",
      "Epoch: 2504 mean train loss:  1.73414108e-02, mean val. rec. loss:  1.03857401e-02\n",
      "Epoch: 2505 mean train loss:  1.73396454e-02, mean val. rec. loss:  1.03842739e-02\n",
      "Epoch: 2506 mean train loss:  1.73378875e-02, mean val. rec. loss:  1.03828144e-02\n",
      "Epoch: 2507 mean train loss:  1.73361408e-02, mean val. rec. loss:  1.03813448e-02\n",
      "Epoch: 2508 mean train loss:  1.73343792e-02, mean val. rec. loss:  1.03798797e-02\n",
      "Epoch: 2509 mean train loss:  1.73326306e-02, mean val. rec. loss:  1.03784328e-02\n",
      "Epoch: 2510 mean train loss:  1.73308857e-02, mean val. rec. loss:  1.03769926e-02\n",
      "Epoch: 2511 mean train loss:  1.73291371e-02, mean val. rec. loss:  1.03755547e-02\n",
      "Epoch: 2512 mean train loss:  1.73273904e-02, mean val. rec. loss:  1.03740976e-02\n",
      "Epoch: 2513 mean train loss:  1.73256418e-02, mean val. rec. loss:  1.03726393e-02\n",
      "Epoch: 2514 mean train loss:  1.73238932e-02, mean val. rec. loss:  1.03711708e-02\n",
      "Epoch: 2515 mean train loss:  1.73221613e-02, mean val. rec. loss:  1.03697034e-02\n",
      "Epoch: 2516 mean train loss:  1.73204127e-02, mean val. rec. loss:  1.03682530e-02\n",
      "Epoch: 2517 mean train loss:  1.73186772e-02, mean val. rec. loss:  1.03668106e-02\n",
      "Epoch: 2518 mean train loss:  1.73169472e-02, mean val. rec. loss:  1.03653727e-02\n",
      "Epoch: 2519 mean train loss:  1.73152135e-02, mean val. rec. loss:  1.03639303e-02\n",
      "Epoch: 2520 mean train loss:  1.73134817e-02, mean val. rec. loss:  1.03624936e-02\n",
      "Epoch: 2521 mean train loss:  1.73117424e-02, mean val. rec. loss:  1.03610285e-02\n",
      "Epoch: 2522 mean train loss:  1.73100087e-02, mean val. rec. loss:  1.03595634e-02\n",
      "Epoch: 2523 mean train loss:  1.73082899e-02, mean val. rec. loss:  1.03581006e-02\n",
      "Epoch: 2524 mean train loss:  1.73065543e-02, mean val. rec. loss:  1.03566559e-02\n",
      "Epoch: 2525 mean train loss:  1.73048299e-02, mean val. rec. loss:  1.03552066e-02\n",
      "Epoch: 2526 mean train loss:  1.73031130e-02, mean val. rec. loss:  1.03537778e-02\n",
      "Epoch: 2527 mean train loss:  1.73013923e-02, mean val. rec. loss:  1.03523354e-02\n",
      "Epoch: 2528 mean train loss:  1.72996717e-02, mean val. rec. loss:  1.03509021e-02\n",
      "Epoch: 2529 mean train loss:  1.72979473e-02, mean val. rec. loss:  1.03494392e-02\n",
      "Epoch: 2530 mean train loss:  1.72962266e-02, mean val. rec. loss:  1.03479798e-02\n",
      "Epoch: 2531 mean train loss:  1.72945208e-02, mean val. rec. loss:  1.03465317e-02\n",
      "Epoch: 2532 mean train loss:  1.72928002e-02, mean val. rec. loss:  1.03450882e-02\n",
      "Epoch: 2533 mean train loss:  1.72910832e-02, mean val. rec. loss:  1.03436514e-02\n",
      "Epoch: 2534 mean train loss:  1.72893737e-02, mean val. rec. loss:  1.03422181e-02\n",
      "Epoch: 2535 mean train loss:  1.72876642e-02, mean val. rec. loss:  1.03407825e-02\n",
      "Epoch: 2536 mean train loss:  1.72859622e-02, mean val. rec. loss:  1.03393446e-02\n",
      "Epoch: 2537 mean train loss:  1.72842602e-02, mean val. rec. loss:  1.03379044e-02\n",
      "Epoch: 2538 mean train loss:  1.72825432e-02, mean val. rec. loss:  1.03364552e-02\n",
      "Epoch: 2539 mean train loss:  1.72808524e-02, mean val. rec. loss:  1.03350117e-02\n",
      "Epoch: 2540 mean train loss:  1.72791447e-02, mean val. rec. loss:  1.03335590e-02\n",
      "Epoch: 2541 mean train loss:  1.72774390e-02, mean val. rec. loss:  1.03321280e-02\n",
      "Epoch: 2542 mean train loss:  1.72757388e-02, mean val. rec. loss:  1.03306855e-02\n",
      "Epoch: 2543 mean train loss:  1.72740423e-02, mean val. rec. loss:  1.03292545e-02\n",
      "Epoch: 2544 mean train loss:  1.72723515e-02, mean val. rec. loss:  1.03278052e-02\n",
      "Epoch: 2545 mean train loss:  1.72706624e-02, mean val. rec. loss:  1.03263685e-02\n",
      "Epoch: 2546 mean train loss:  1.72689604e-02, mean val. rec. loss:  1.03249215e-02\n",
      "Epoch: 2547 mean train loss:  1.72672770e-02, mean val. rec. loss:  1.03234780e-02\n",
      "Epoch: 2548 mean train loss:  1.72655805e-02, mean val. rec. loss:  1.03220356e-02\n",
      "Epoch: 2549 mean train loss:  1.72638897e-02, mean val. rec. loss:  1.03205999e-02\n",
      "Epoch: 2550 mean train loss:  1.72622007e-02, mean val. rec. loss:  1.03191689e-02\n",
      "Epoch: 2551 mean train loss:  1.72605172e-02, mean val. rec. loss:  1.03177423e-02\n",
      "Epoch: 2552 mean train loss:  1.72588394e-02, mean val. rec. loss:  1.03163022e-02\n",
      "Epoch: 2553 mean train loss:  1.72571467e-02, mean val. rec. loss:  1.03148507e-02\n",
      "Epoch: 2554 mean train loss:  1.72554744e-02, mean val. rec. loss:  1.03134332e-02\n",
      "Epoch: 2555 mean train loss:  1.72537873e-02, mean val. rec. loss:  1.03119942e-02\n",
      "Epoch: 2556 mean train loss:  1.72521113e-02, mean val. rec. loss:  1.03105620e-02\n",
      "Epoch: 2557 mean train loss:  1.72504316e-02, mean val. rec. loss:  1.03091502e-02\n",
      "Epoch: 2558 mean train loss:  1.72487594e-02, mean val. rec. loss:  1.03077112e-02\n",
      "Epoch: 2559 mean train loss:  1.72470834e-02, mean val. rec. loss:  1.03062858e-02\n",
      "Epoch: 2560 mean train loss:  1.72454074e-02, mean val. rec. loss:  1.03048694e-02\n",
      "Epoch: 2561 mean train loss:  1.72437296e-02, mean val. rec. loss:  1.03034225e-02\n",
      "Epoch: 2562 mean train loss:  1.72420629e-02, mean val. rec. loss:  1.03019925e-02\n",
      "Epoch: 2563 mean train loss:  1.72403944e-02, mean val. rec. loss:  1.03005694e-02\n",
      "Epoch: 2564 mean train loss:  1.72387259e-02, mean val. rec. loss:  1.02991406e-02\n",
      "Epoch: 2565 mean train loss:  1.72370574e-02, mean val. rec. loss:  1.02977084e-02\n",
      "Epoch: 2566 mean train loss:  1.72353926e-02, mean val. rec. loss:  1.02962966e-02\n",
      "Epoch: 2567 mean train loss:  1.72337315e-02, mean val. rec. loss:  1.02948553e-02\n",
      "Epoch: 2568 mean train loss:  1.72320667e-02, mean val. rec. loss:  1.02934299e-02\n",
      "Epoch: 2569 mean train loss:  1.72304000e-02, mean val. rec. loss:  1.02920124e-02\n",
      "Epoch: 2570 mean train loss:  1.72287483e-02, mean val. rec. loss:  1.02905757e-02\n",
      "Epoch: 2571 mean train loss:  1.72270872e-02, mean val. rec. loss:  1.02891480e-02\n",
      "Epoch: 2572 mean train loss:  1.72254261e-02, mean val. rec. loss:  1.02877316e-02\n",
      "Epoch: 2573 mean train loss:  1.72237744e-02, mean val. rec. loss:  1.02863006e-02\n",
      "Epoch: 2574 mean train loss:  1.72221151e-02, mean val. rec. loss:  1.02848683e-02\n",
      "Epoch: 2575 mean train loss:  1.72204634e-02, mean val. rec. loss:  1.02834622e-02\n",
      "Epoch: 2576 mean train loss:  1.72188079e-02, mean val. rec. loss:  1.02820379e-02\n",
      "Epoch: 2577 mean train loss:  1.72171561e-02, mean val. rec. loss:  1.02806114e-02\n",
      "Epoch: 2578 mean train loss:  1.72155081e-02, mean val. rec. loss:  1.02791962e-02\n",
      "Epoch: 2579 mean train loss:  1.72138619e-02, mean val. rec. loss:  1.02777583e-02\n",
      "Epoch: 2580 mean train loss:  1.72122195e-02, mean val. rec. loss:  1.02763499e-02\n",
      "Epoch: 2581 mean train loss:  1.72105658e-02, mean val. rec. loss:  1.02749483e-02\n",
      "Epoch: 2582 mean train loss:  1.72089271e-02, mean val. rec. loss:  1.02735263e-02\n",
      "Epoch: 2583 mean train loss:  1.72072884e-02, mean val. rec. loss:  1.02721202e-02\n",
      "Epoch: 2584 mean train loss:  1.72056422e-02, mean val. rec. loss:  1.02707118e-02\n",
      "Epoch: 2585 mean train loss:  1.72039979e-02, mean val. rec. loss:  1.02692852e-02\n",
      "Epoch: 2586 mean train loss:  1.72023517e-02, mean val. rec. loss:  1.02678678e-02\n",
      "Epoch: 2587 mean train loss:  1.72007242e-02, mean val. rec. loss:  1.02664526e-02\n",
      "Epoch: 2588 mean train loss:  1.71990761e-02, mean val. rec. loss:  1.02650170e-02\n",
      "Epoch: 2589 mean train loss:  1.71974448e-02, mean val. rec. loss:  1.02636301e-02\n",
      "Epoch: 2590 mean train loss:  1.71958098e-02, mean val. rec. loss:  1.02622228e-02\n",
      "Epoch: 2591 mean train loss:  1.71941786e-02, mean val. rec. loss:  1.02608099e-02\n",
      "Epoch: 2592 mean train loss:  1.71925398e-02, mean val. rec. loss:  1.02594083e-02\n",
      "Epoch: 2593 mean train loss:  1.71909123e-02, mean val. rec. loss:  1.02580010e-02\n",
      "Epoch: 2594 mean train loss:  1.71892847e-02, mean val. rec. loss:  1.02565643e-02\n",
      "Epoch: 2595 mean train loss:  1.71876553e-02, mean val. rec. loss:  1.02551627e-02\n",
      "Epoch: 2596 mean train loss:  1.71860240e-02, mean val. rec. loss:  1.02537543e-02\n",
      "Epoch: 2597 mean train loss:  1.71843946e-02, mean val. rec. loss:  1.02523278e-02\n",
      "Epoch: 2598 mean train loss:  1.71827708e-02, mean val. rec. loss:  1.02509341e-02\n",
      "Epoch: 2599 mean train loss:  1.71811525e-02, mean val. rec. loss:  1.02495472e-02\n",
      "Epoch: 2600 mean train loss:  1.71795306e-02, mean val. rec. loss:  1.02481275e-02\n",
      "Epoch: 2601 mean train loss:  1.71779086e-02, mean val. rec. loss:  1.02467407e-02\n",
      "Epoch: 2602 mean train loss:  1.71762829e-02, mean val. rec. loss:  1.02453379e-02\n",
      "Epoch: 2603 mean train loss:  1.71746647e-02, mean val. rec. loss:  1.02439091e-02\n",
      "Epoch: 2604 mean train loss:  1.71730464e-02, mean val. rec. loss:  1.02425200e-02\n",
      "Epoch: 2605 mean train loss:  1.71714338e-02, mean val. rec. loss:  1.02411195e-02\n",
      "Epoch: 2606 mean train loss:  1.71698230e-02, mean val. rec. loss:  1.02397157e-02\n",
      "Epoch: 2607 mean train loss:  1.71682047e-02, mean val. rec. loss:  1.02383402e-02\n",
      "Epoch: 2608 mean train loss:  1.71665883e-02, mean val. rec. loss:  1.02369522e-02\n",
      "Epoch: 2609 mean train loss:  1.71649757e-02, mean val. rec. loss:  1.02355449e-02\n",
      "Epoch: 2610 mean train loss:  1.71633630e-02, mean val. rec. loss:  1.02341490e-02\n",
      "Epoch: 2611 mean train loss:  1.71617578e-02, mean val. rec. loss:  1.02327349e-02\n",
      "Epoch: 2612 mean train loss:  1.71601563e-02, mean val. rec. loss:  1.02313378e-02\n",
      "Epoch: 2613 mean train loss:  1.71585400e-02, mean val. rec. loss:  1.02299612e-02\n",
      "Epoch: 2614 mean train loss:  1.71569366e-02, mean val. rec. loss:  1.02285709e-02\n",
      "Epoch: 2615 mean train loss:  1.71553389e-02, mean val. rec. loss:  1.02271898e-02\n",
      "Epoch: 2616 mean train loss:  1.71537336e-02, mean val. rec. loss:  1.02257927e-02\n",
      "Epoch: 2617 mean train loss:  1.71521266e-02, mean val. rec. loss:  1.02243968e-02\n",
      "Epoch: 2618 mean train loss:  1.71505214e-02, mean val. rec. loss:  1.02229850e-02\n",
      "Epoch: 2619 mean train loss:  1.71489311e-02, mean val. rec. loss:  1.02216015e-02\n",
      "Epoch: 2620 mean train loss:  1.71473258e-02, mean val. rec. loss:  1.02202283e-02\n",
      "Epoch: 2621 mean train loss:  1.71457262e-02, mean val. rec. loss:  1.02188426e-02\n",
      "Epoch: 2622 mean train loss:  1.71441303e-02, mean val. rec. loss:  1.02174738e-02\n",
      "Epoch: 2623 mean train loss:  1.71425363e-02, mean val. rec. loss:  1.02160995e-02\n",
      "Epoch: 2624 mean train loss:  1.71409423e-02, mean val. rec. loss:  1.02147024e-02\n",
      "Epoch: 2625 mean train loss:  1.71393501e-02, mean val. rec. loss:  1.02133122e-02\n",
      "Epoch: 2626 mean train loss:  1.71377542e-02, mean val. rec. loss:  1.02119276e-02\n",
      "Epoch: 2627 mean train loss:  1.71361676e-02, mean val. rec. loss:  1.02105430e-02\n",
      "Epoch: 2628 mean train loss:  1.71345792e-02, mean val. rec. loss:  1.02091845e-02\n",
      "Epoch: 2629 mean train loss:  1.71329833e-02, mean val. rec. loss:  1.02078294e-02\n",
      "Epoch: 2630 mean train loss:  1.71314060e-02, mean val. rec. loss:  1.02064391e-02\n",
      "Epoch: 2631 mean train loss:  1.71298101e-02, mean val. rec. loss:  1.02050704e-02\n",
      "Epoch: 2632 mean train loss:  1.71282309e-02, mean val. rec. loss:  1.02036734e-02\n",
      "Epoch: 2633 mean train loss:  1.71266518e-02, mean val. rec. loss:  1.02022854e-02\n",
      "Epoch: 2634 mean train loss:  1.71250708e-02, mean val. rec. loss:  1.02009325e-02\n",
      "Epoch: 2635 mean train loss:  1.71234824e-02, mean val. rec. loss:  1.01995661e-02\n",
      "Epoch: 2636 mean train loss:  1.71219032e-02, mean val. rec. loss:  1.01981917e-02\n",
      "Epoch: 2637 mean train loss:  1.71203241e-02, mean val. rec. loss:  1.01968377e-02\n",
      "Epoch: 2638 mean train loss:  1.71187505e-02, mean val. rec. loss:  1.01954566e-02\n",
      "Epoch: 2639 mean train loss:  1.71171733e-02, mean val. rec. loss:  1.01940776e-02\n",
      "Epoch: 2640 mean train loss:  1.71155978e-02, mean val. rec. loss:  1.01927305e-02\n",
      "Epoch: 2641 mean train loss:  1.71140206e-02, mean val. rec. loss:  1.01913561e-02\n",
      "Epoch: 2642 mean train loss:  1.71124470e-02, mean val. rec. loss:  1.01899965e-02\n",
      "Epoch: 2643 mean train loss:  1.71108716e-02, mean val. rec. loss:  1.01886357e-02\n",
      "Epoch: 2644 mean train loss:  1.71093074e-02, mean val. rec. loss:  1.01872647e-02\n",
      "Epoch: 2645 mean train loss:  1.71077375e-02, mean val. rec. loss:  1.01859130e-02\n",
      "Epoch: 2646 mean train loss:  1.71061696e-02, mean val. rec. loss:  1.01845500e-02\n",
      "Epoch: 2647 mean train loss:  1.71045960e-02, mean val. rec. loss:  1.01831915e-02\n",
      "Epoch: 2648 mean train loss:  1.71030318e-02, mean val. rec. loss:  1.01818466e-02\n",
      "Epoch: 2649 mean train loss:  1.71014620e-02, mean val. rec. loss:  1.01804779e-02\n",
      "Epoch: 2650 mean train loss:  1.70999033e-02, mean val. rec. loss:  1.01791205e-02\n",
      "Epoch: 2651 mean train loss:  1.70983428e-02, mean val. rec. loss:  1.01777756e-02\n",
      "Epoch: 2652 mean train loss:  1.70967711e-02, mean val. rec. loss:  1.01764114e-02\n",
      "Epoch: 2653 mean train loss:  1.70952180e-02, mean val. rec. loss:  1.01750609e-02\n",
      "Epoch: 2654 mean train loss:  1.70936557e-02, mean val. rec. loss:  1.01737171e-02\n",
      "Epoch: 2655 mean train loss:  1.70921063e-02, mean val. rec. loss:  1.01723745e-02\n",
      "Epoch: 2656 mean train loss:  1.70905477e-02, mean val. rec. loss:  1.01710296e-02\n",
      "Epoch: 2657 mean train loss:  1.70889909e-02, mean val. rec. loss:  1.01696926e-02\n",
      "Epoch: 2658 mean train loss:  1.70874304e-02, mean val. rec. loss:  1.01683454e-02\n",
      "Epoch: 2659 mean train loss:  1.70858810e-02, mean val. rec. loss:  1.01669937e-02\n",
      "Epoch: 2660 mean train loss:  1.70843279e-02, mean val. rec. loss:  1.01656534e-02\n",
      "Epoch: 2661 mean train loss:  1.70827823e-02, mean val. rec. loss:  1.01643108e-02\n",
      "Epoch: 2662 mean train loss:  1.70812330e-02, mean val. rec. loss:  1.01629851e-02\n",
      "Epoch: 2663 mean train loss:  1.70796799e-02, mean val. rec. loss:  1.01616448e-02\n",
      "Epoch: 2664 mean train loss:  1.70781268e-02, mean val. rec. loss:  1.01602919e-02\n",
      "Epoch: 2665 mean train loss:  1.70765887e-02, mean val. rec. loss:  1.01589436e-02\n",
      "Epoch: 2666 mean train loss:  1.70750412e-02, mean val. rec. loss:  1.01576362e-02\n",
      "Epoch: 2667 mean train loss:  1.70734974e-02, mean val. rec. loss:  1.01563072e-02\n",
      "Epoch: 2668 mean train loss:  1.70719537e-02, mean val. rec. loss:  1.01549838e-02\n",
      "Epoch: 2669 mean train loss:  1.70704174e-02, mean val. rec. loss:  1.01536502e-02\n",
      "Epoch: 2670 mean train loss:  1.70688680e-02, mean val. rec. loss:  1.01523088e-02\n",
      "Epoch: 2671 mean train loss:  1.70673373e-02, mean val. rec. loss:  1.01509899e-02\n",
      "Epoch: 2672 mean train loss:  1.70657991e-02, mean val. rec. loss:  1.01496575e-02\n",
      "Epoch: 2673 mean train loss:  1.70642610e-02, mean val. rec. loss:  1.01483466e-02\n",
      "Epoch: 2674 mean train loss:  1.70627303e-02, mean val. rec. loss:  1.01470267e-02\n",
      "Epoch: 2675 mean train loss:  1.70611921e-02, mean val. rec. loss:  1.01456965e-02\n",
      "Epoch: 2676 mean train loss:  1.70596576e-02, mean val. rec. loss:  1.01443845e-02\n",
      "Epoch: 2677 mean train loss:  1.70581195e-02, mean val. rec. loss:  1.01430532e-02\n",
      "Epoch: 2678 mean train loss:  1.70565906e-02, mean val. rec. loss:  1.01417253e-02\n",
      "Epoch: 2679 mean train loss:  1.70550618e-02, mean val. rec. loss:  1.01404235e-02\n",
      "Epoch: 2680 mean train loss:  1.70535385e-02, mean val. rec. loss:  1.01391206e-02\n",
      "Epoch: 2681 mean train loss:  1.70520115e-02, mean val. rec. loss:  1.01378188e-02\n",
      "Epoch: 2682 mean train loss:  1.70504919e-02, mean val. rec. loss:  1.01365136e-02\n",
      "Epoch: 2683 mean train loss:  1.70489668e-02, mean val. rec. loss:  1.01351902e-02\n",
      "Epoch: 2684 mean train loss:  1.70474435e-02, mean val. rec. loss:  1.01338794e-02\n",
      "Epoch: 2685 mean train loss:  1.70459184e-02, mean val. rec. loss:  1.01325844e-02\n",
      "Epoch: 2686 mean train loss:  1.70443989e-02, mean val. rec. loss:  1.01312746e-02\n",
      "Epoch: 2687 mean train loss:  1.70428756e-02, mean val. rec. loss:  1.01299773e-02\n",
      "Epoch: 2688 mean train loss:  1.70413653e-02, mean val. rec. loss:  1.01286812e-02\n",
      "Epoch: 2689 mean train loss:  1.70398495e-02, mean val. rec. loss:  1.01273919e-02\n",
      "Epoch: 2690 mean train loss:  1.70383300e-02, mean val. rec. loss:  1.01260901e-02\n",
      "Epoch: 2691 mean train loss:  1.70368086e-02, mean val. rec. loss:  1.01247724e-02\n",
      "Epoch: 2692 mean train loss:  1.70353058e-02, mean val. rec. loss:  1.01234853e-02\n",
      "Epoch: 2693 mean train loss:  1.70337881e-02, mean val. rec. loss:  1.01222051e-02\n",
      "Epoch: 2694 mean train loss:  1.70322797e-02, mean val. rec. loss:  1.01209067e-02\n",
      "Epoch: 2695 mean train loss:  1.70307751e-02, mean val. rec. loss:  1.01196321e-02\n",
      "Epoch: 2696 mean train loss:  1.70292611e-02, mean val. rec. loss:  1.01183530e-02\n",
      "Epoch: 2697 mean train loss:  1.70277546e-02, mean val. rec. loss:  1.01170580e-02\n",
      "Epoch: 2698 mean train loss:  1.70262425e-02, mean val. rec. loss:  1.01157788e-02\n",
      "Epoch: 2699 mean train loss:  1.70247416e-02, mean val. rec. loss:  1.01144838e-02\n",
      "Epoch: 2700 mean train loss:  1.70232369e-02, mean val. rec. loss:  1.01131979e-02\n",
      "Epoch: 2701 mean train loss:  1.70217416e-02, mean val. rec. loss:  1.01119301e-02\n",
      "Epoch: 2702 mean train loss:  1.70202407e-02, mean val. rec. loss:  1.01106578e-02\n",
      "Epoch: 2703 mean train loss:  1.70187490e-02, mean val. rec. loss:  1.01093923e-02\n",
      "Epoch: 2704 mean train loss:  1.70172463e-02, mean val. rec. loss:  1.01081211e-02\n",
      "Epoch: 2705 mean train loss:  1.70157491e-02, mean val. rec. loss:  1.01068272e-02\n",
      "Epoch: 2706 mean train loss:  1.70142593e-02, mean val. rec. loss:  1.01055730e-02\n",
      "Epoch: 2707 mean train loss:  1.70127565e-02, mean val. rec. loss:  1.01042905e-02\n",
      "Epoch: 2708 mean train loss:  1.70112742e-02, mean val. rec. loss:  1.01030137e-02\n",
      "Epoch: 2709 mean train loss:  1.70097770e-02, mean val. rec. loss:  1.01017674e-02\n",
      "Epoch: 2710 mean train loss:  1.70082891e-02, mean val. rec. loss:  1.01005042e-02\n",
      "Epoch: 2711 mean train loss:  1.70068050e-02, mean val. rec. loss:  1.00992613e-02\n",
      "Epoch: 2712 mean train loss:  1.70053077e-02, mean val. rec. loss:  1.00979913e-02\n",
      "Epoch: 2713 mean train loss:  1.70038329e-02, mean val. rec. loss:  1.00967224e-02\n",
      "Epoch: 2714 mean train loss:  1.70023431e-02, mean val. rec. loss:  1.00954636e-02\n",
      "Epoch: 2715 mean train loss:  1.70008571e-02, mean val. rec. loss:  1.00942197e-02\n",
      "Epoch: 2716 mean train loss:  1.69993823e-02, mean val. rec. loss:  1.00929723e-02\n",
      "Epoch: 2717 mean train loss:  1.69978925e-02, mean val. rec. loss:  1.00917295e-02\n",
      "Epoch: 2718 mean train loss:  1.69964232e-02, mean val. rec. loss:  1.00904934e-02\n",
      "Epoch: 2719 mean train loss:  1.69949409e-02, mean val. rec. loss:  1.00892392e-02\n",
      "Epoch: 2720 mean train loss:  1.69934642e-02, mean val. rec. loss:  1.00880055e-02\n",
      "Epoch: 2721 mean train loss:  1.69919949e-02, mean val. rec. loss:  1.00867490e-02\n",
      "Epoch: 2722 mean train loss:  1.69905164e-02, mean val. rec. loss:  1.00854948e-02\n",
      "Epoch: 2723 mean train loss:  1.69890434e-02, mean val. rec. loss:  1.00842622e-02\n",
      "Epoch: 2724 mean train loss:  1.69875685e-02, mean val. rec. loss:  1.00830364e-02\n",
      "Epoch: 2725 mean train loss:  1.69861011e-02, mean val. rec. loss:  1.00817913e-02\n",
      "Epoch: 2726 mean train loss:  1.69846318e-02, mean val. rec. loss:  1.00805768e-02\n",
      "Epoch: 2727 mean train loss:  1.69831700e-02, mean val. rec. loss:  1.00793362e-02\n",
      "Epoch: 2728 mean train loss:  1.69817026e-02, mean val. rec. loss:  1.00781104e-02\n",
      "Epoch: 2729 mean train loss:  1.69802464e-02, mean val. rec. loss:  1.00768948e-02\n",
      "Epoch: 2730 mean train loss:  1.69787827e-02, mean val. rec. loss:  1.00756542e-02\n",
      "Epoch: 2731 mean train loss:  1.69773246e-02, mean val. rec. loss:  1.00744272e-02\n",
      "Epoch: 2732 mean train loss:  1.69758572e-02, mean val. rec. loss:  1.00732071e-02\n",
      "Epoch: 2733 mean train loss:  1.69744010e-02, mean val. rec. loss:  1.00719960e-02\n",
      "Epoch: 2734 mean train loss:  1.69729392e-02, mean val. rec. loss:  1.00707826e-02\n",
      "Epoch: 2735 mean train loss:  1.69714829e-02, mean val. rec. loss:  1.00695784e-02\n",
      "Epoch: 2736 mean train loss:  1.69700341e-02, mean val. rec. loss:  1.00683673e-02\n",
      "Epoch: 2737 mean train loss:  1.69685854e-02, mean val. rec. loss:  1.00671573e-02\n",
      "Epoch: 2738 mean train loss:  1.69671291e-02, mean val. rec. loss:  1.00659496e-02\n",
      "Epoch: 2739 mean train loss:  1.69656841e-02, mean val. rec. loss:  1.00647453e-02\n",
      "Epoch: 2740 mean train loss:  1.69642334e-02, mean val. rec. loss:  1.00635433e-02\n",
      "Epoch: 2741 mean train loss:  1.69627884e-02, mean val. rec. loss:  1.00623413e-02\n",
      "Epoch: 2742 mean train loss:  1.69613377e-02, mean val. rec. loss:  1.00611495e-02\n",
      "Epoch: 2743 mean train loss:  1.69599020e-02, mean val. rec. loss:  1.00599566e-02\n",
      "Epoch: 2744 mean train loss:  1.69584550e-02, mean val. rec. loss:  1.00587591e-02\n",
      "Epoch: 2745 mean train loss:  1.69570137e-02, mean val. rec. loss:  1.00575627e-02\n",
      "Epoch: 2746 mean train loss:  1.69555761e-02, mean val. rec. loss:  1.00563675e-02\n",
      "Epoch: 2747 mean train loss:  1.69541422e-02, mean val. rec. loss:  1.00551859e-02\n",
      "Epoch: 2748 mean train loss:  1.69526971e-02, mean val. rec. loss:  1.00540043e-02\n",
      "Epoch: 2749 mean train loss:  1.69512670e-02, mean val. rec. loss:  1.00528114e-02\n",
      "Epoch: 2750 mean train loss:  1.69498387e-02, mean val. rec. loss:  1.00516320e-02\n",
      "Epoch: 2751 mean train loss:  1.69484067e-02, mean val. rec. loss:  1.00504504e-02\n",
      "Epoch: 2752 mean train loss:  1.69469690e-02, mean val. rec. loss:  1.00492745e-02\n",
      "Epoch: 2753 mean train loss:  1.69455426e-02, mean val. rec. loss:  1.00480918e-02\n",
      "Epoch: 2754 mean train loss:  1.69441087e-02, mean val. rec. loss:  1.00469158e-02\n",
      "Epoch: 2755 mean train loss:  1.69426897e-02, mean val. rec. loss:  1.00457490e-02\n",
      "Epoch: 2756 mean train loss:  1.69412633e-02, mean val. rec. loss:  1.00445764e-02\n",
      "Epoch: 2757 mean train loss:  1.69398369e-02, mean val. rec. loss:  1.00434289e-02\n",
      "Epoch: 2758 mean train loss:  1.69384253e-02, mean val. rec. loss:  1.00422631e-02\n",
      "Epoch: 2759 mean train loss:  1.69369989e-02, mean val. rec. loss:  1.00411110e-02\n",
      "Epoch: 2760 mean train loss:  1.69355799e-02, mean val. rec. loss:  1.00399396e-02\n",
      "Epoch: 2761 mean train loss:  1.69341628e-02, mean val. rec. loss:  1.00387796e-02\n",
      "Epoch: 2762 mean train loss:  1.69327512e-02, mean val. rec. loss:  1.00376297e-02\n",
      "Epoch: 2763 mean train loss:  1.69313304e-02, mean val. rec. loss:  1.00364776e-02\n",
      "Epoch: 2764 mean train loss:  1.69299226e-02, mean val. rec. loss:  1.00353277e-02\n",
      "Epoch: 2765 mean train loss:  1.69285147e-02, mean val. rec. loss:  1.00341892e-02\n",
      "Epoch: 2766 mean train loss:  1.69271069e-02, mean val. rec. loss:  1.00330439e-02\n",
      "Epoch: 2767 mean train loss:  1.69256954e-02, mean val. rec. loss:  1.00319031e-02\n",
      "Epoch: 2768 mean train loss:  1.69242857e-02, mean val. rec. loss:  1.00307419e-02\n",
      "Epoch: 2769 mean train loss:  1.69228797e-02, mean val. rec. loss:  1.00295875e-02\n",
      "Epoch: 2770 mean train loss:  1.69214850e-02, mean val. rec. loss:  1.00284581e-02\n",
      "Epoch: 2771 mean train loss:  1.69200809e-02, mean val. rec. loss:  1.00273377e-02\n",
      "Epoch: 2772 mean train loss:  1.69186786e-02, mean val. rec. loss:  1.00262185e-02\n",
      "Epoch: 2773 mean train loss:  1.69172783e-02, mean val. rec. loss:  1.00250822e-02\n",
      "Epoch: 2774 mean train loss:  1.69158798e-02, mean val. rec. loss:  1.00239381e-02\n",
      "Epoch: 2775 mean train loss:  1.69144924e-02, mean val. rec. loss:  1.00228052e-02\n",
      "Epoch: 2776 mean train loss:  1.69130995e-02, mean val. rec. loss:  1.00216883e-02\n",
      "Epoch: 2777 mean train loss:  1.69116992e-02, mean val. rec. loss:  1.00205815e-02\n",
      "Epoch: 2778 mean train loss:  1.69103100e-02, mean val. rec. loss:  1.00194702e-02\n",
      "Epoch: 2779 mean train loss:  1.69089152e-02, mean val. rec. loss:  1.00183419e-02\n",
      "Epoch: 2780 mean train loss:  1.69075316e-02, mean val. rec. loss:  1.00172193e-02\n",
      "Epoch: 2781 mean train loss:  1.69061498e-02, mean val. rec. loss:  1.00161046e-02\n",
      "Epoch: 2782 mean train loss:  1.69047569e-02, mean val. rec. loss:  1.00150103e-02\n",
      "Epoch: 2783 mean train loss:  1.69033826e-02, mean val. rec. loss:  1.00139080e-02\n",
      "Epoch: 2784 mean train loss:  1.69019953e-02, mean val. rec. loss:  1.00128183e-02\n",
      "Epoch: 2785 mean train loss:  1.69006210e-02, mean val. rec. loss:  1.00117161e-02\n",
      "Epoch: 2786 mean train loss:  1.68992411e-02, mean val. rec. loss:  1.00106104e-02\n",
      "Epoch: 2787 mean train loss:  1.68978594e-02, mean val. rec. loss:  1.00095003e-02\n",
      "Epoch: 2788 mean train loss:  1.68964906e-02, mean val. rec. loss:  1.00083958e-02\n",
      "Epoch: 2789 mean train loss:  1.68951164e-02, mean val. rec. loss:  1.00073072e-02\n",
      "Epoch: 2790 mean train loss:  1.68937514e-02, mean val. rec. loss:  1.00062367e-02\n",
      "Epoch: 2791 mean train loss:  1.68923808e-02, mean val. rec. loss:  1.00051435e-02\n",
      "Epoch: 2792 mean train loss:  1.68910121e-02, mean val. rec. loss:  1.00040549e-02\n",
      "Epoch: 2793 mean train loss:  1.68896415e-02, mean val. rec. loss:  1.00029606e-02\n",
      "Epoch: 2794 mean train loss:  1.68882765e-02, mean val. rec. loss:  1.00018686e-02\n",
      "Epoch: 2795 mean train loss:  1.68869078e-02, mean val. rec. loss:  1.00007981e-02\n",
      "Epoch: 2796 mean train loss:  1.68855466e-02, mean val. rec. loss:  9.99972313e-03\n",
      "Epoch: 2797 mean train loss:  1.68841909e-02, mean val. rec. loss:  9.99866059e-03\n",
      "Epoch: 2798 mean train loss:  1.68828389e-02, mean val. rec. loss:  9.99759579e-03\n",
      "Epoch: 2799 mean train loss:  1.68814702e-02, mean val. rec. loss:  9.99653098e-03\n",
      "Epoch: 2800 mean train loss:  1.68801220e-02, mean val. rec. loss:  9.99547072e-03\n",
      "Epoch: 2801 mean train loss:  1.68787645e-02, mean val. rec. loss:  9.99438777e-03\n",
      "Epoch: 2802 mean train loss:  1.68774162e-02, mean val. rec. loss:  9.99333317e-03\n",
      "Epoch: 2803 mean train loss:  1.68760606e-02, mean val. rec. loss:  9.99227744e-03\n",
      "Epoch: 2804 mean train loss:  1.68747198e-02, mean val. rec. loss:  9.99123191e-03\n",
      "Epoch: 2805 mean train loss:  1.68733678e-02, mean val. rec. loss:  9.99017278e-03\n",
      "Epoch: 2806 mean train loss:  1.68720196e-02, mean val. rec. loss:  9.98909550e-03\n",
      "Epoch: 2807 mean train loss:  1.68706751e-02, mean val. rec. loss:  9.98806018e-03\n",
      "Epoch: 2808 mean train loss:  1.68693343e-02, mean val. rec. loss:  9.98700785e-03\n",
      "Epoch: 2809 mean train loss:  1.68679935e-02, mean val. rec. loss:  9.98596346e-03\n",
      "Epoch: 2810 mean train loss:  1.68666602e-02, mean val. rec. loss:  9.98493494e-03\n",
      "Epoch: 2811 mean train loss:  1.68653176e-02, mean val. rec. loss:  9.98388374e-03\n",
      "Epoch: 2812 mean train loss:  1.68639805e-02, mean val. rec. loss:  9.98286203e-03\n",
      "Epoch: 2813 mean train loss:  1.68626509e-02, mean val. rec. loss:  9.98181197e-03\n",
      "Epoch: 2814 mean train loss:  1.68613232e-02, mean val. rec. loss:  9.98076871e-03\n",
      "Epoch: 2815 mean train loss:  1.68599861e-02, mean val. rec. loss:  9.97974133e-03\n",
      "Epoch: 2816 mean train loss:  1.68586584e-02, mean val. rec. loss:  9.97871394e-03\n",
      "Epoch: 2817 mean train loss:  1.68573306e-02, mean val. rec. loss:  9.97770244e-03\n",
      "Epoch: 2818 mean train loss:  1.68560010e-02, mean val. rec. loss:  9.97668753e-03\n",
      "Epoch: 2819 mean train loss:  1.68546826e-02, mean val. rec. loss:  9.97564994e-03\n",
      "Epoch: 2820 mean train loss:  1.68533623e-02, mean val. rec. loss:  9.97464410e-03\n",
      "Epoch: 2821 mean train loss:  1.68520457e-02, mean val. rec. loss:  9.97362806e-03\n",
      "Epoch: 2822 mean train loss:  1.68507255e-02, mean val. rec. loss:  9.97263016e-03\n",
      "Epoch: 2823 mean train loss:  1.68494089e-02, mean val. rec. loss:  9.97162999e-03\n",
      "Epoch: 2824 mean train loss:  1.68480905e-02, mean val. rec. loss:  9.97060488e-03\n",
      "Epoch: 2825 mean train loss:  1.68467739e-02, mean val. rec. loss:  9.96958657e-03\n",
      "Epoch: 2826 mean train loss:  1.68454610e-02, mean val. rec. loss:  9.96855805e-03\n",
      "Epoch: 2827 mean train loss:  1.68441575e-02, mean val. rec. loss:  9.96757262e-03\n",
      "Epoch: 2828 mean train loss:  1.68428484e-02, mean val. rec. loss:  9.96658947e-03\n",
      "Epoch: 2829 mean train loss:  1.68415355e-02, mean val. rec. loss:  9.96558816e-03\n",
      "Epoch: 2830 mean train loss:  1.68402246e-02, mean val. rec. loss:  9.96459253e-03\n",
      "Epoch: 2831 mean train loss:  1.68389303e-02, mean val. rec. loss:  9.96359917e-03\n",
      "Epoch: 2832 mean train loss:  1.68376231e-02, mean val. rec. loss:  9.96260354e-03\n",
      "Epoch: 2833 mean train loss:  1.68363233e-02, mean val. rec. loss:  9.96161698e-03\n",
      "Epoch: 2834 mean train loss:  1.68350235e-02, mean val. rec. loss:  9.96063155e-03\n",
      "Epoch: 2835 mean train loss:  1.68337311e-02, mean val. rec. loss:  9.95965974e-03\n",
      "Epoch: 2836 mean train loss:  1.68324276e-02, mean val. rec. loss:  9.95868452e-03\n",
      "Epoch: 2837 mean train loss:  1.68311371e-02, mean val. rec. loss:  9.95769115e-03\n",
      "Epoch: 2838 mean train loss:  1.68298503e-02, mean val. rec. loss:  9.95672274e-03\n",
      "Epoch: 2839 mean train loss:  1.68285598e-02, mean val. rec. loss:  9.95574978e-03\n",
      "Epoch: 2840 mean train loss:  1.68272637e-02, mean val. rec. loss:  9.95477456e-03\n",
      "Epoch: 2841 mean train loss:  1.68259788e-02, mean val. rec. loss:  9.95380842e-03\n",
      "Epoch: 2842 mean train loss:  1.68246958e-02, mean val. rec. loss:  9.95285814e-03\n",
      "Epoch: 2843 mean train loss:  1.68234146e-02, mean val. rec. loss:  9.95190787e-03\n",
      "Epoch: 2844 mean train loss:  1.68221315e-02, mean val. rec. loss:  9.95094626e-03\n",
      "Epoch: 2845 mean train loss:  1.68208447e-02, mean val. rec. loss:  9.94996650e-03\n",
      "Epoch: 2846 mean train loss:  1.68195617e-02, mean val. rec. loss:  9.94900036e-03\n",
      "Epoch: 2847 mean train loss:  1.68182935e-02, mean val. rec. loss:  9.94804781e-03\n",
      "Epoch: 2848 mean train loss:  1.68170142e-02, mean val. rec. loss:  9.94712249e-03\n",
      "Epoch: 2849 mean train loss:  1.68157349e-02, mean val. rec. loss:  9.94616768e-03\n",
      "Epoch: 2850 mean train loss:  1.68144630e-02, mean val. rec. loss:  9.94522421e-03\n",
      "Epoch: 2851 mean train loss:  1.68131986e-02, mean val. rec. loss:  9.94427507e-03\n",
      "Epoch: 2852 mean train loss:  1.68119342e-02, mean val. rec. loss:  9.94331686e-03\n",
      "Epoch: 2853 mean train loss:  1.68106567e-02, mean val. rec. loss:  9.94238133e-03\n",
      "Epoch: 2854 mean train loss:  1.68093941e-02, mean val. rec. loss:  9.94142539e-03\n",
      "Epoch: 2855 mean train loss:  1.68081390e-02, mean val. rec. loss:  9.94050574e-03\n",
      "Epoch: 2856 mean train loss:  1.68068746e-02, mean val. rec. loss:  9.93958495e-03\n",
      "Epoch: 2857 mean train loss:  1.68056120e-02, mean val. rec. loss:  9.93865849e-03\n",
      "Epoch: 2858 mean train loss:  1.68043495e-02, mean val. rec. loss:  9.93772182e-03\n",
      "Epoch: 2859 mean train loss:  1.68030906e-02, mean val. rec. loss:  9.93676475e-03\n",
      "Epoch: 2860 mean train loss:  1.68018411e-02, mean val. rec. loss:  9.93585870e-03\n",
      "Epoch: 2861 mean train loss:  1.68005897e-02, mean val. rec. loss:  9.93493791e-03\n",
      "Epoch: 2862 mean train loss:  1.67993309e-02, mean val. rec. loss:  9.93405681e-03\n",
      "Epoch: 2863 mean train loss:  1.67980757e-02, mean val. rec. loss:  9.93312695e-03\n",
      "Epoch: 2864 mean train loss:  1.67968355e-02, mean val. rec. loss:  9.93220389e-03\n",
      "Epoch: 2865 mean train loss:  1.67955841e-02, mean val. rec. loss:  9.93128084e-03\n",
      "Epoch: 2866 mean train loss:  1.67943327e-02, mean val. rec. loss:  9.93034190e-03\n",
      "Epoch: 2867 mean train loss:  1.67930888e-02, mean val. rec. loss:  9.92948575e-03\n",
      "Epoch: 2868 mean train loss:  1.67918504e-02, mean val. rec. loss:  9.92855816e-03\n",
      "Epoch: 2869 mean train loss:  1.67906158e-02, mean val. rec. loss:  9.92768840e-03\n",
      "Epoch: 2870 mean train loss:  1.67893663e-02, mean val. rec. loss:  9.92677328e-03\n",
      "Epoch: 2871 mean train loss:  1.67881354e-02, mean val. rec. loss:  9.92584796e-03\n",
      "Epoch: 2872 mean train loss:  1.67868989e-02, mean val. rec. loss:  9.92495892e-03\n",
      "Epoch: 2873 mean train loss:  1.67856680e-02, mean val. rec. loss:  9.92405967e-03\n",
      "Epoch: 2874 mean train loss:  1.67844296e-02, mean val. rec. loss:  9.92320012e-03\n",
      "Epoch: 2875 mean train loss:  1.67831931e-02, mean val. rec. loss:  9.92228387e-03\n",
      "Epoch: 2876 mean train loss:  1.67819734e-02, mean val. rec. loss:  9.92141297e-03\n",
      "Epoch: 2877 mean train loss:  1.67807406e-02, mean val. rec. loss:  9.92051713e-03\n",
      "Epoch: 2878 mean train loss:  1.67795097e-02, mean val. rec. loss:  9.91960428e-03\n",
      "Epoch: 2879 mean train loss:  1.67782844e-02, mean val. rec. loss:  9.91875153e-03\n",
      "Epoch: 2880 mean train loss:  1.67770646e-02, mean val. rec. loss:  9.91783981e-03\n",
      "Epoch: 2881 mean train loss:  1.67758486e-02, mean val. rec. loss:  9.91700180e-03\n",
      "Epoch: 2882 mean train loss:  1.67746196e-02, mean val. rec. loss:  9.91611277e-03\n",
      "Epoch: 2883 mean train loss:  1.67734036e-02, mean val. rec. loss:  9.91524187e-03\n",
      "Epoch: 2884 mean train loss:  1.67721932e-02, mean val. rec. loss:  9.91436417e-03\n",
      "Epoch: 2885 mean train loss:  1.67709790e-02, mean val. rec. loss:  9.91350122e-03\n",
      "Epoch: 2886 mean train loss:  1.67697593e-02, mean val. rec. loss:  9.91264847e-03\n",
      "Epoch: 2887 mean train loss:  1.67685414e-02, mean val. rec. loss:  9.91176170e-03\n",
      "Epoch: 2888 mean train loss:  1.67673421e-02, mean val. rec. loss:  9.91092142e-03\n",
      "Epoch: 2889 mean train loss:  1.67661280e-02, mean val. rec. loss:  9.91004372e-03\n",
      "Epoch: 2890 mean train loss:  1.67649139e-02, mean val. rec. loss:  9.90918984e-03\n",
      "Epoch: 2891 mean train loss:  1.67637109e-02, mean val. rec. loss:  9.90833255e-03\n",
      "Epoch: 2892 mean train loss:  1.67625116e-02, mean val. rec. loss:  9.90747640e-03\n",
      "Epoch: 2893 mean train loss:  1.67613124e-02, mean val. rec. loss:  9.90664293e-03\n",
      "Epoch: 2894 mean train loss:  1.67601020e-02, mean val. rec. loss:  9.90577657e-03\n",
      "Epoch: 2895 mean train loss:  1.67589101e-02, mean val. rec. loss:  9.90493516e-03\n",
      "Epoch: 2896 mean train loss:  1.67577072e-02, mean val. rec. loss:  9.90408354e-03\n",
      "Epoch: 2897 mean train loss:  1.67565098e-02, mean val. rec. loss:  9.90322399e-03\n",
      "Epoch: 2898 mean train loss:  1.67553161e-02, mean val. rec. loss:  9.90240526e-03\n",
      "Epoch: 2899 mean train loss:  1.67541262e-02, mean val. rec. loss:  9.90155931e-03\n",
      "Epoch: 2900 mean train loss:  1.67529232e-02, mean val. rec. loss:  9.90072697e-03\n",
      "Epoch: 2901 mean train loss:  1.67517444e-02, mean val. rec. loss:  9.89986628e-03\n",
      "Epoch: 2902 mean train loss:  1.67505489e-02, mean val. rec. loss:  9.89905776e-03\n",
      "Epoch: 2903 mean train loss:  1.67493608e-02, mean val. rec. loss:  9.89821181e-03\n",
      "Epoch: 2904 mean train loss:  1.67481746e-02, mean val. rec. loss:  9.89739988e-03\n",
      "Epoch: 2905 mean train loss:  1.67469959e-02, mean val. rec. loss:  9.89656981e-03\n",
      "Epoch: 2906 mean train loss:  1.67458041e-02, mean val. rec. loss:  9.89573634e-03\n",
      "Epoch: 2907 mean train loss:  1.67446309e-02, mean val. rec. loss:  9.89491648e-03\n",
      "Epoch: 2908 mean train loss:  1.67434447e-02, mean val. rec. loss:  9.89406939e-03\n",
      "Epoch: 2909 mean train loss:  1.67422622e-02, mean val. rec. loss:  9.89327221e-03\n",
      "Epoch: 2910 mean train loss:  1.67410853e-02, mean val. rec. loss:  9.89245008e-03\n",
      "Epoch: 2911 mean train loss:  1.67399139e-02, mean val. rec. loss:  9.89165289e-03\n",
      "Epoch: 2912 mean train loss:  1.67387426e-02, mean val. rec. loss:  9.89082509e-03\n",
      "Epoch: 2913 mean train loss:  1.67375620e-02, mean val. rec. loss:  9.89001770e-03\n",
      "Epoch: 2914 mean train loss:  1.67363981e-02, mean val. rec. loss:  9.88918989e-03\n",
      "Epoch: 2915 mean train loss:  1.67352249e-02, mean val. rec. loss:  9.88839384e-03\n",
      "Epoch: 2916 mean train loss:  1.67340574e-02, mean val. rec. loss:  9.88759779e-03\n",
      "Epoch: 2917 mean train loss:  1.67328879e-02, mean val. rec. loss:  9.88676772e-03\n",
      "Epoch: 2918 mean train loss:  1.67317259e-02, mean val. rec. loss:  9.88598528e-03\n",
      "Epoch: 2919 mean train loss:  1.67305583e-02, mean val. rec. loss:  9.88515294e-03\n",
      "Epoch: 2920 mean train loss:  1.67293963e-02, mean val. rec. loss:  9.88439771e-03\n",
      "Epoch: 2921 mean train loss:  1.67282287e-02, mean val. rec. loss:  9.88355630e-03\n",
      "Epoch: 2922 mean train loss:  1.67270723e-02, mean val. rec. loss:  9.88278519e-03\n",
      "Epoch: 2923 mean train loss:  1.67259140e-02, mean val. rec. loss:  9.88195966e-03\n",
      "Epoch: 2924 mean train loss:  1.67247557e-02, mean val. rec. loss:  9.88119309e-03\n",
      "Epoch: 2925 mean train loss:  1.67235937e-02, mean val. rec. loss:  9.88040044e-03\n",
      "Epoch: 2926 mean train loss:  1.67224485e-02, mean val. rec. loss:  9.87961006e-03\n",
      "Epoch: 2927 mean train loss:  1.67212865e-02, mean val. rec. loss:  9.87881514e-03\n",
      "Epoch: 2928 mean train loss:  1.67201393e-02, mean val. rec. loss:  9.87802136e-03\n",
      "Epoch: 2929 mean train loss:  1.67189941e-02, mean val. rec. loss:  9.87725819e-03\n",
      "Epoch: 2930 mean train loss:  1.67178451e-02, mean val. rec. loss:  9.87648369e-03\n",
      "Epoch: 2931 mean train loss:  1.67166943e-02, mean val. rec. loss:  9.87571939e-03\n",
      "Epoch: 2932 mean train loss:  1.67155397e-02, mean val. rec. loss:  9.87491313e-03\n",
      "Epoch: 2933 mean train loss:  1.67144038e-02, mean val. rec. loss:  9.87414769e-03\n",
      "Epoch: 2934 mean train loss:  1.67132492e-02, mean val. rec. loss:  9.87334030e-03\n",
      "Epoch: 2935 mean train loss:  1.67121133e-02, mean val. rec. loss:  9.87260208e-03\n",
      "Epoch: 2936 mean train loss:  1.67109755e-02, mean val. rec. loss:  9.87180150e-03\n",
      "Epoch: 2937 mean train loss:  1.67098358e-02, mean val. rec. loss:  9.87109049e-03\n",
      "Epoch: 2938 mean train loss:  1.67086943e-02, mean val. rec. loss:  9.87026949e-03\n",
      "Epoch: 2939 mean train loss:  1.67075472e-02, mean val. rec. loss:  9.86952561e-03\n",
      "Epoch: 2940 mean train loss:  1.67064224e-02, mean val. rec. loss:  9.86872615e-03\n",
      "Epoch: 2941 mean train loss:  1.67052791e-02, mean val. rec. loss:  9.86799587e-03\n",
      "Epoch: 2942 mean train loss:  1.67041487e-02, mean val. rec. loss:  9.86721116e-03\n",
      "Epoch: 2943 mean train loss:  1.67030184e-02, mean val. rec. loss:  9.86647407e-03\n",
      "Epoch: 2944 mean train loss:  1.67018899e-02, mean val. rec. loss:  9.86571318e-03\n",
      "Epoch: 2945 mean train loss:  1.67007614e-02, mean val. rec. loss:  9.86494207e-03\n",
      "Epoch: 2946 mean train loss:  1.66996273e-02, mean val. rec. loss:  9.86419478e-03\n",
      "Epoch: 2947 mean train loss:  1.66984970e-02, mean val. rec. loss:  9.86339986e-03\n",
      "Epoch: 2948 mean train loss:  1.66973685e-02, mean val. rec. loss:  9.86268319e-03\n",
      "Epoch: 2949 mean train loss:  1.66962512e-02, mean val. rec. loss:  9.86190415e-03\n",
      "Epoch: 2950 mean train loss:  1.66951208e-02, mean val. rec. loss:  9.86121923e-03\n",
      "Epoch: 2951 mean train loss:  1.66939979e-02, mean val. rec. loss:  9.86041637e-03\n",
      "Epoch: 2952 mean train loss:  1.66928769e-02, mean val. rec. loss:  9.85970537e-03\n",
      "Epoch: 2953 mean train loss:  1.66917596e-02, mean val. rec. loss:  9.85888550e-03\n",
      "Epoch: 2954 mean train loss:  1.66906348e-02, mean val. rec. loss:  9.85820285e-03\n",
      "Epoch: 2955 mean train loss:  1.66895175e-02, mean val. rec. loss:  9.85742494e-03\n",
      "Epoch: 2956 mean train loss:  1.66883946e-02, mean val. rec. loss:  9.85672641e-03\n",
      "Epoch: 2957 mean train loss:  1.66872829e-02, mean val. rec. loss:  9.85594283e-03\n",
      "Epoch: 2958 mean train loss:  1.66861730e-02, mean val. rec. loss:  9.85524090e-03\n",
      "Epoch: 2959 mean train loss:  1.66850538e-02, mean val. rec. loss:  9.85447207e-03\n",
      "Epoch: 2960 mean train loss:  1.66839384e-02, mean val. rec. loss:  9.85377127e-03\n",
      "Epoch: 2961 mean train loss:  1.66828304e-02, mean val. rec. loss:  9.85300130e-03\n",
      "Epoch: 2962 mean train loss:  1.66817186e-02, mean val. rec. loss:  9.85230390e-03\n",
      "Epoch: 2963 mean train loss:  1.66806106e-02, mean val. rec. loss:  9.85153053e-03\n",
      "Epoch: 2964 mean train loss:  1.66794989e-02, mean val. rec. loss:  9.85084561e-03\n",
      "Epoch: 2965 mean train loss:  1.66783965e-02, mean val. rec. loss:  9.85005409e-03\n",
      "Epoch: 2966 mean train loss:  1.66772903e-02, mean val. rec. loss:  9.84940546e-03\n",
      "Epoch: 2967 mean train loss:  1.66761842e-02, mean val. rec. loss:  9.84859580e-03\n",
      "Epoch: 2968 mean train loss:  1.66750743e-02, mean val. rec. loss:  9.84795397e-03\n",
      "Epoch: 2969 mean train loss:  1.66739794e-02, mean val. rec. loss:  9.84711029e-03\n",
      "Epoch: 2970 mean train loss:  1.66728732e-02, mean val. rec. loss:  9.84653650e-03\n",
      "Epoch: 2971 mean train loss:  1.66717783e-02, mean val. rec. loss:  9.84566560e-03\n",
      "Epoch: 2972 mean train loss:  1.66706721e-02, mean val. rec. loss:  9.84513604e-03\n",
      "Epoch: 2973 mean train loss:  1.66695753e-02, mean val. rec. loss:  9.84419143e-03\n",
      "Epoch: 2974 mean train loss:  1.66684747e-02, mean val. rec. loss:  9.84372650e-03\n",
      "Epoch: 2975 mean train loss:  1.66673835e-02, mean val. rec. loss:  9.84270819e-03\n",
      "Epoch: 2976 mean train loss:  1.66662811e-02, mean val. rec. loss:  9.84236800e-03\n",
      "Epoch: 2977 mean train loss:  1.66651861e-02, mean val. rec. loss:  9.84120000e-03\n",
      "Epoch: 2978 mean train loss:  1.66640949e-02, mean val. rec. loss:  9.84104692e-03\n",
      "Epoch: 2979 mean train loss:  1.66630036e-02, mean val. rec. loss:  9.83963058e-03\n",
      "Epoch: 2980 mean train loss:  1.66619012e-02, mean val. rec. loss:  9.83978480e-03\n",
      "Epoch: 2981 mean train loss:  1.66608137e-02, mean val. rec. loss:  9.83798631e-03\n",
      "Epoch: 2982 mean train loss:  1.66597280e-02, mean val. rec. loss:  9.83866897e-03\n",
      "Epoch: 2983 mean train loss:  1.66586424e-02, mean val. rec. loss:  9.83621504e-03\n",
      "Epoch: 2984 mean train loss:  1.66575623e-02, mean val. rec. loss:  9.83774024e-03\n",
      "Epoch: 2985 mean train loss:  1.66564766e-02, mean val. rec. loss:  9.83419089e-03\n",
      "Epoch: 2986 mean train loss:  1.66554059e-02, mean val. rec. loss:  9.83717212e-03\n",
      "Epoch: 2987 mean train loss:  1.66543482e-02, mean val. rec. loss:  9.83178573e-03\n",
      "Epoch: 2988 mean train loss:  1.66533016e-02, mean val. rec. loss:  9.83730706e-03\n",
      "Epoch: 2989 mean train loss:  1.66522960e-02, mean val. rec. loss:  9.82874667e-03\n",
      "Epoch: 2990 mean train loss:  1.66513351e-02, mean val. rec. loss:  9.83867577e-03\n",
      "Epoch: 2991 mean train loss:  1.66504636e-02, mean val. rec. loss:  9.82472786e-03\n",
      "Epoch: 2992 mean train loss:  1.66497616e-02, mean val. rec. loss:  9.84242810e-03\n",
      "Epoch: 2993 mean train loss:  1.66493016e-02, mean val. rec. loss:  9.81967940e-03\n",
      "Epoch: 2994 mean train loss:  1.66492420e-02, mean val. rec. loss:  9.85026955e-03\n",
      "Epoch: 2995 mean train loss:  1.66497392e-02, mean val. rec. loss:  9.81446197e-03\n",
      "Epoch: 2996 mean train loss:  1.66508826e-02, mean val. rec. loss:  9.86155375e-03\n",
      "Epoch: 2997 mean train loss:  1.66523072e-02, mean val. rec. loss:  9.81105210e-03\n",
      "Epoch: 2998 mean train loss:  1.66529888e-02, mean val. rec. loss:  9.86321389e-03\n",
      "Epoch: 2999 mean train loss:  1.66513053e-02, mean val. rec. loss:  9.81127436e-03\n",
      "Epoch: 3000 mean train loss:  1.66468026e-02, mean val. rec. loss:  9.84014994e-03\n",
      "Epoch: 3001 mean train loss:  1.66414134e-02, mean val. rec. loss:  9.82336822e-03\n",
      "Epoch: 3002 mean train loss:  1.66382123e-02, mean val. rec. loss:  9.81636932e-03\n",
      "Epoch: 3003 mean train loss:  1.66382048e-02, mean val. rec. loss:  9.84283973e-03\n",
      "Epoch: 3004 mean train loss:  1.66396480e-02, mean val. rec. loss:  9.80903589e-03\n",
      "Epoch: 3005 mean train loss:  1.66397989e-02, mean val. rec. loss:  9.84138598e-03\n",
      "Epoch: 3006 mean train loss:  1.66374842e-02, mean val. rec. loss:  9.81341644e-03\n",
      "Epoch: 3007 mean train loss:  1.66340484e-02, mean val. rec. loss:  9.82071812e-03\n",
      "Epoch: 3008 mean train loss:  1.66318659e-02, mean val. rec. loss:  9.82837926e-03\n",
      "Epoch: 3009 mean train loss:  1.66316704e-02, mean val. rec. loss:  9.80861405e-03\n",
      "Epoch: 3010 mean train loss:  1.66320205e-02, mean val. rec. loss:  9.83420677e-03\n",
      "Epoch: 3011 mean train loss:  1.66312011e-02, mean val. rec. loss:  9.80923887e-03\n",
      "Epoch: 3012 mean train loss:  1.66290335e-02, mean val. rec. loss:  9.82076461e-03\n",
      "Epoch: 3013 mean train loss:  1.66268157e-02, mean val. rec. loss:  9.81953765e-03\n",
      "Epoch: 3014 mean train loss:  1.66257356e-02, mean val. rec. loss:  9.80819335e-03\n",
      "Epoch: 3015 mean train loss:  1.66254954e-02, mean val. rec. loss:  9.82651387e-03\n",
      "Epoch: 3016 mean train loss:  1.66249721e-02, mean val. rec. loss:  9.80646630e-03\n",
      "Epoch: 3017 mean train loss:  1.66235550e-02, mean val. rec. loss:  9.81857037e-03\n",
      "Epoch: 3018 mean train loss:  1.66217375e-02, mean val. rec. loss:  9.81341304e-03\n",
      "Epoch: 3019 mean train loss:  1.66203930e-02, mean val. rec. loss:  9.80751410e-03\n",
      "Epoch: 3020 mean train loss:  1.66197040e-02, mean val. rec. loss:  9.81980413e-03\n",
      "Epoch: 3021 mean train loss:  1.66191043e-02, mean val. rec. loss:  9.80429700e-03\n",
      "Epoch: 3022 mean train loss:  1.66180299e-02, mean val. rec. loss:  9.81549162e-03\n",
      "Epoch: 3023 mean train loss:  1.66165680e-02, mean val. rec. loss:  9.80863333e-03\n",
      "Epoch: 3024 mean train loss:  1.66152198e-02, mean val. rec. loss:  9.80644476e-03\n",
      "Epoch: 3025 mean train loss:  1.66142664e-02, mean val. rec. loss:  9.81392106e-03\n",
      "Epoch: 3026 mean train loss:  1.66135178e-02, mean val. rec. loss:  9.80228873e-03\n",
      "Epoch: 3027 mean train loss:  1.66125792e-02, mean val. rec. loss:  9.81194908e-03\n",
      "Epoch: 3028 mean train loss:  1.66113632e-02, mean val. rec. loss:  9.80446483e-03\n",
      "Epoch: 3029 mean train loss:  1.66100839e-02, mean val. rec. loss:  9.80490935e-03\n",
      "Epoch: 3030 mean train loss:  1.66090075e-02, mean val. rec. loss:  9.80853694e-03\n",
      "Epoch: 3031 mean train loss:  1.66081174e-02, mean val. rec. loss:  9.80030200e-03\n",
      "Epoch: 3032 mean train loss:  1.66071994e-02, mean val. rec. loss:  9.80810830e-03\n",
      "Epoch: 3033 mean train loss:  1.66061286e-02, mean val. rec. loss:  9.80071704e-03\n",
      "Epoch: 3034 mean train loss:  1.66049591e-02, mean val. rec. loss:  9.80296912e-03\n",
      "Epoch: 3035 mean train loss:  1.66038381e-02, mean val. rec. loss:  9.80361322e-03\n",
      "Epoch: 3036 mean train loss:  1.66028362e-02, mean val. rec. loss:  9.79837198e-03\n",
      "Epoch: 3037 mean train loss:  1.66018940e-02, mean val. rec. loss:  9.80409289e-03\n",
      "Epoch: 3038 mean train loss:  1.66008884e-02, mean val. rec. loss:  9.79740129e-03\n",
      "Epoch: 3039 mean train loss:  1.65998102e-02, mean val. rec. loss:  9.80065240e-03\n",
      "Epoch: 3040 mean train loss:  1.65986985e-02, mean val. rec. loss:  9.79908185e-03\n",
      "Epoch: 3041 mean train loss:  1.65976407e-02, mean val. rec. loss:  9.79644081e-03\n",
      "Epoch: 3042 mean train loss:  1.65966575e-02, mean val. rec. loss:  9.79996635e-03\n",
      "Epoch: 3043 mean train loss:  1.65956668e-02, mean val. rec. loss:  9.79445522e-03\n",
      "Epoch: 3044 mean train loss:  1.65946389e-02, mean val. rec. loss:  9.79791612e-03\n",
      "Epoch: 3045 mean train loss:  1.65935644e-02, mean val. rec. loss:  9.79494056e-03\n",
      "Epoch: 3046 mean train loss:  1.65925029e-02, mean val. rec. loss:  9.79438265e-03\n",
      "Epoch: 3047 mean train loss:  1.65914732e-02, mean val. rec. loss:  9.79576837e-03\n",
      "Epoch: 3048 mean train loss:  1.65904713e-02, mean val. rec. loss:  9.79180285e-03\n",
      "Epoch: 3049 mean train loss:  1.65894583e-02, mean val. rec. loss:  9.79475005e-03\n",
      "Epoch: 3050 mean train loss:  1.65884303e-02, mean val. rec. loss:  9.79119050e-03\n",
      "Epoch: 3051 mean train loss:  1.65873801e-02, mean val. rec. loss:  9.79203418e-03\n",
      "Epoch: 3052 mean train loss:  1.65863335e-02, mean val. rec. loss:  9.79156131e-03\n",
      "Epoch: 3053 mean train loss:  1.65853037e-02, mean val. rec. loss:  9.78932284e-03\n",
      "Epoch: 3054 mean train loss:  1.65842925e-02, mean val. rec. loss:  9.79117916e-03\n",
      "Epoch: 3055 mean train loss:  1.65832721e-02, mean val. rec. loss:  9.78786228e-03\n",
      "Epoch: 3056 mean train loss:  1.65822423e-02, mean val. rec. loss:  9.78932965e-03\n",
      "Epoch: 3057 mean train loss:  1.65812050e-02, mean val. rec. loss:  9.78756972e-03\n",
      "Epoch: 3058 mean train loss:  1.65801697e-02, mean val. rec. loss:  9.78687912e-03\n",
      "Epoch: 3059 mean train loss:  1.65791324e-02, mean val. rec. loss:  9.78736787e-03\n",
      "Epoch: 3060 mean train loss:  1.65781175e-02, mean val. rec. loss:  9.78492301e-03\n",
      "Epoch: 3061 mean train loss:  1.65770896e-02, mean val. rec. loss:  9.78622028e-03\n",
      "Epoch: 3062 mean train loss:  1.65760691e-02, mean val. rec. loss:  9.78393872e-03\n",
      "Epoch: 3063 mean train loss:  1.65750337e-02, mean val. rec. loss:  9.78427438e-03\n",
      "Epoch: 3064 mean train loss:  1.65740002e-02, mean val. rec. loss:  9.78352142e-03\n",
      "Epoch: 3065 mean train loss:  1.65729704e-02, mean val. rec. loss:  9.78221961e-03\n",
      "Epoch: 3066 mean train loss:  1.65719481e-02, mean val. rec. loss:  9.78276392e-03\n",
      "Epoch: 3067 mean train loss:  1.65709257e-02, mean val. rec. loss:  9.78070122e-03\n",
      "Epoch: 3068 mean train loss:  1.65698997e-02, mean val. rec. loss:  9.78135666e-03\n",
      "Epoch: 3069 mean train loss:  1.65688662e-02, mean val. rec. loss:  9.77983486e-03\n",
      "Epoch: 3070 mean train loss:  1.65678382e-02, mean val. rec. loss:  9.77951054e-03\n",
      "Epoch: 3071 mean train loss:  1.65668103e-02, mean val. rec. loss:  9.77912613e-03\n",
      "Epoch: 3072 mean train loss:  1.65657880e-02, mean val. rec. loss:  9.77774948e-03\n",
      "Epoch: 3073 mean train loss:  1.65647544e-02, mean val. rec. loss:  9.77807946e-03\n",
      "Epoch: 3074 mean train loss:  1.65637265e-02, mean val. rec. loss:  9.77642273e-03\n",
      "Epoch: 3075 mean train loss:  1.65627060e-02, mean val. rec. loss:  9.77662344e-03\n",
      "Epoch: 3076 mean train loss:  1.65616688e-02, mean val. rec. loss:  9.77550194e-03\n",
      "Epoch: 3077 mean train loss:  1.65606446e-02, mean val. rec. loss:  9.77495536e-03\n",
      "Epoch: 3078 mean train loss:  1.65596185e-02, mean val. rec. loss:  9.77458455e-03\n",
      "Epoch: 3079 mean train loss:  1.65585906e-02, mean val. rec. loss:  9.77337460e-03\n",
      "Epoch: 3080 mean train loss:  1.65575627e-02, mean val. rec. loss:  9.77345738e-03\n",
      "Epoch: 3081 mean train loss:  1.65565385e-02, mean val. rec. loss:  9.77212609e-03\n",
      "Epoch: 3082 mean train loss:  1.65555031e-02, mean val. rec. loss:  9.77205918e-03\n",
      "Epoch: 3083 mean train loss:  1.65544752e-02, mean val. rec. loss:  9.77107603e-03\n",
      "Epoch: 3084 mean train loss:  1.65534435e-02, mean val. rec. loss:  9.77048296e-03\n",
      "Epoch: 3085 mean train loss:  1.65524156e-02, mean val. rec. loss:  9.77003730e-03\n",
      "Epoch: 3086 mean train loss:  1.65513914e-02, mean val. rec. loss:  9.76902239e-03\n",
      "Epoch: 3087 mean train loss:  1.65503560e-02, mean val. rec. loss:  9.76888178e-03\n",
      "Epoch: 3088 mean train loss:  1.65493281e-02, mean val. rec. loss:  9.76773420e-03\n",
      "Epoch: 3089 mean train loss:  1.65483001e-02, mean val. rec. loss:  9.76751194e-03\n",
      "Epoch: 3090 mean train loss:  1.65472647e-02, mean val. rec. loss:  9.76657867e-03\n",
      "Epoch: 3091 mean train loss:  1.65462331e-02, mean val. rec. loss:  9.76605024e-03\n",
      "Epoch: 3092 mean train loss:  1.65452070e-02, mean val. rec. loss:  9.76547305e-03\n",
      "Epoch: 3093 mean train loss:  1.65441717e-02, mean val. rec. loss:  9.76462256e-03\n",
      "Epoch: 3094 mean train loss:  1.65431400e-02, mean val. rec. loss:  9.76427784e-03\n",
      "Epoch: 3095 mean train loss:  1.65421083e-02, mean val. rec. loss:  9.76327086e-03\n",
      "Epoch: 3096 mean train loss:  1.65410748e-02, mean val. rec. loss:  9.76296242e-03\n",
      "Epoch: 3097 mean train loss:  1.65400488e-02, mean val. rec. loss:  9.76207792e-03\n",
      "Epoch: 3098 mean train loss:  1.65390115e-02, mean val. rec. loss:  9.76158691e-03\n",
      "Epoch: 3099 mean train loss:  1.65379761e-02, mean val. rec. loss:  9.76089065e-03\n",
      "Epoch: 3100 mean train loss:  1.65369408e-02, mean val. rec. loss:  9.76017738e-03\n",
      "Epoch: 3101 mean train loss:  1.65359128e-02, mean val. rec. loss:  9.75970451e-03\n",
      "Epoch: 3102 mean train loss:  1.65348793e-02, mean val. rec. loss:  9.75887557e-03\n",
      "Epoch: 3103 mean train loss:  1.65338439e-02, mean val. rec. loss:  9.75844239e-03\n",
      "Epoch: 3104 mean train loss:  1.65328086e-02, mean val. rec. loss:  9.75757603e-03\n",
      "Epoch: 3105 mean train loss:  1.65317732e-02, mean val. rec. loss:  9.75709750e-03\n",
      "Epoch: 3106 mean train loss:  1.65307304e-02, mean val. rec. loss:  9.75635701e-03\n",
      "Epoch: 3107 mean train loss:  1.65296894e-02, mean val. rec. loss:  9.75573219e-03\n",
      "Epoch: 3108 mean train loss:  1.65286577e-02, mean val. rec. loss:  9.75512551e-03\n",
      "Epoch: 3109 mean train loss:  1.65276205e-02, mean val. rec. loss:  9.75437822e-03\n",
      "Epoch: 3110 mean train loss:  1.65265795e-02, mean val. rec. loss:  9.75387814e-03\n",
      "Epoch: 3111 mean train loss:  1.65255367e-02, mean val. rec. loss:  9.75306394e-03\n",
      "Epoch: 3112 mean train loss:  1.65245013e-02, mean val. rec. loss:  9.75256953e-03\n",
      "Epoch: 3113 mean train loss:  1.65234660e-02, mean val. rec. loss:  9.75176894e-03\n",
      "Epoch: 3114 mean train loss:  1.65224269e-02, mean val. rec. loss:  9.75123937e-03\n",
      "Epoch: 3115 mean train loss:  1.65213859e-02, mean val. rec. loss:  9.75053744e-03\n",
      "Epoch: 3116 mean train loss:  1.65203412e-02, mean val. rec. loss:  9.74991262e-03\n",
      "Epoch: 3117 mean train loss:  1.65192909e-02, mean val. rec. loss:  9.74924357e-03\n",
      "Epoch: 3118 mean train loss:  1.65182555e-02, mean val. rec. loss:  9.74854731e-03\n",
      "Epoch: 3119 mean train loss:  1.65172109e-02, mean val. rec. loss:  9.74795764e-03\n",
      "Epoch: 3120 mean train loss:  1.65161680e-02, mean val. rec. loss:  9.74723530e-03\n",
      "Epoch: 3121 mean train loss:  1.65151196e-02, mean val. rec. loss:  9.74668305e-03\n",
      "Epoch: 3122 mean train loss:  1.65140749e-02, mean val. rec. loss:  9.74593123e-03\n",
      "Epoch: 3123 mean train loss:  1.65130302e-02, mean val. rec. loss:  9.74537104e-03\n",
      "Epoch: 3124 mean train loss:  1.65119837e-02, mean val. rec. loss:  9.74462829e-03\n",
      "Epoch: 3125 mean train loss:  1.65109371e-02, mean val. rec. loss:  9.74402955e-03\n",
      "Epoch: 3126 mean train loss:  1.65098869e-02, mean val. rec. loss:  9.74334463e-03\n",
      "Epoch: 3127 mean train loss:  1.65088440e-02, mean val. rec. loss:  9.74272094e-03\n",
      "Epoch: 3128 mean train loss:  1.65077938e-02, mean val. rec. loss:  9.74205530e-03\n",
      "Epoch: 3129 mean train loss:  1.65067435e-02, mean val. rec. loss:  9.74137491e-03\n",
      "Epoch: 3130 mean train loss:  1.65056895e-02, mean val. rec. loss:  9.74075689e-03\n",
      "Epoch: 3131 mean train loss:  1.65046467e-02, mean val. rec. loss:  9.74006857e-03\n",
      "Epoch: 3132 mean train loss:  1.65035889e-02, mean val. rec. loss:  9.73945509e-03\n",
      "Epoch: 3133 mean train loss:  1.65025368e-02, mean val. rec. loss:  9.73874862e-03\n",
      "Epoch: 3134 mean train loss:  1.65014828e-02, mean val. rec. loss:  9.73811359e-03\n",
      "Epoch: 3135 mean train loss:  1.65004288e-02, mean val. rec. loss:  9.73742527e-03\n",
      "Epoch: 3136 mean train loss:  1.64993767e-02, mean val. rec. loss:  9.73677890e-03\n",
      "Epoch: 3137 mean train loss:  1.64983189e-02, mean val. rec. loss:  9.73612120e-03\n",
      "Epoch: 3138 mean train loss:  1.64972612e-02, mean val. rec. loss:  9.73545215e-03\n",
      "Epoch: 3139 mean train loss:  1.64961998e-02, mean val. rec. loss:  9.73480465e-03\n",
      "Epoch: 3140 mean train loss:  1.64951495e-02, mean val. rec. loss:  9.73411633e-03\n",
      "Epoch: 3141 mean train loss:  1.64940862e-02, mean val. rec. loss:  9.73347676e-03\n",
      "Epoch: 3142 mean train loss:  1.64930191e-02, mean val. rec. loss:  9.73278957e-03\n",
      "Epoch: 3143 mean train loss:  1.64919596e-02, mean val. rec. loss:  9.73214888e-03\n",
      "Epoch: 3144 mean train loss:  1.64908981e-02, mean val. rec. loss:  9.73143447e-03\n",
      "Epoch: 3145 mean train loss:  1.64898348e-02, mean val. rec. loss:  9.73080965e-03\n",
      "Epoch: 3146 mean train loss:  1.64887771e-02, mean val. rec. loss:  9.73011339e-03\n",
      "Epoch: 3147 mean train loss:  1.64877193e-02, mean val. rec. loss:  9.72949197e-03\n",
      "Epoch: 3148 mean train loss:  1.64866430e-02, mean val. rec. loss:  9.72877076e-03\n",
      "Epoch: 3149 mean train loss:  1.64855853e-02, mean val. rec. loss:  9.72814254e-03\n",
      "Epoch: 3150 mean train loss:  1.64845127e-02, mean val. rec. loss:  9.72743494e-03\n",
      "Epoch: 3151 mean train loss:  1.64834400e-02, mean val. rec. loss:  9.72681238e-03\n",
      "Epoch: 3152 mean train loss:  1.64823711e-02, mean val. rec. loss:  9.72610592e-03\n",
      "Epoch: 3153 mean train loss:  1.64813004e-02, mean val. rec. loss:  9.72544367e-03\n",
      "Epoch: 3154 mean train loss:  1.64802296e-02, mean val. rec. loss:  9.72473834e-03\n",
      "Epoch: 3155 mean train loss:  1.64791607e-02, mean val. rec. loss:  9.72409197e-03\n",
      "Epoch: 3156 mean train loss:  1.64780918e-02, mean val. rec. loss:  9.72340932e-03\n",
      "Epoch: 3157 mean train loss:  1.64770155e-02, mean val. rec. loss:  9.72275728e-03\n",
      "Epoch: 3158 mean train loss:  1.64759391e-02, mean val. rec. loss:  9.72206442e-03\n",
      "Epoch: 3159 mean train loss:  1.64748591e-02, mean val. rec. loss:  9.72138404e-03\n",
      "Epoch: 3160 mean train loss:  1.64737902e-02, mean val. rec. loss:  9.72069912e-03\n",
      "Epoch: 3161 mean train loss:  1.64727064e-02, mean val. rec. loss:  9.72001760e-03\n",
      "Epoch: 3162 mean train loss:  1.64716263e-02, mean val. rec. loss:  9.71934968e-03\n",
      "Epoch: 3163 mean train loss:  1.64705462e-02, mean val. rec. loss:  9.71865682e-03\n",
      "Epoch: 3164 mean train loss:  1.64694699e-02, mean val. rec. loss:  9.71799572e-03\n",
      "Epoch: 3165 mean train loss:  1.64683824e-02, mean val. rec. loss:  9.71729038e-03\n",
      "Epoch: 3166 mean train loss:  1.64672911e-02, mean val. rec. loss:  9.71662927e-03\n",
      "Epoch: 3167 mean train loss:  1.64662166e-02, mean val. rec. loss:  9.71592054e-03\n",
      "Epoch: 3168 mean train loss:  1.64651198e-02, mean val. rec. loss:  9.71526850e-03\n",
      "Epoch: 3169 mean train loss:  1.64640360e-02, mean val. rec. loss:  9.71454956e-03\n",
      "Epoch: 3170 mean train loss:  1.64629559e-02, mean val. rec. loss:  9.71388845e-03\n",
      "Epoch: 3171 mean train loss:  1.64618610e-02, mean val. rec. loss:  9.71315363e-03\n",
      "Epoch: 3172 mean train loss:  1.64607697e-02, mean val. rec. loss:  9.71250954e-03\n",
      "Epoch: 3173 mean train loss:  1.64596766e-02, mean val. rec. loss:  9.71178152e-03\n",
      "Epoch: 3174 mean train loss:  1.64585872e-02, mean val. rec. loss:  9.71114876e-03\n",
      "Epoch: 3175 mean train loss:  1.64574867e-02, mean val. rec. loss:  9.71039467e-03\n",
      "Epoch: 3176 mean train loss:  1.64563936e-02, mean val. rec. loss:  9.70976078e-03\n",
      "Epoch: 3177 mean train loss:  1.64552986e-02, mean val. rec. loss:  9.70899194e-03\n",
      "Epoch: 3178 mean train loss:  1.64541962e-02, mean val. rec. loss:  9.70838300e-03\n",
      "Epoch: 3179 mean train loss:  1.64530938e-02, mean val. rec. loss:  9.70759035e-03\n",
      "Epoch: 3180 mean train loss:  1.64519951e-02, mean val. rec. loss:  9.70700975e-03\n",
      "Epoch: 3181 mean train loss:  1.64508927e-02, mean val. rec. loss:  9.70615587e-03\n",
      "Epoch: 3182 mean train loss:  1.64497902e-02, mean val. rec. loss:  9.70563083e-03\n",
      "Epoch: 3183 mean train loss:  1.64486804e-02, mean val. rec. loss:  9.70472819e-03\n",
      "Epoch: 3184 mean train loss:  1.64475817e-02, mean val. rec. loss:  9.70431429e-03\n",
      "Epoch: 3185 mean train loss:  1.64464737e-02, mean val. rec. loss:  9.70329257e-03\n",
      "Epoch: 3186 mean train loss:  1.64453620e-02, mean val. rec. loss:  9.70298527e-03\n",
      "Epoch: 3187 mean train loss:  1.64442484e-02, mean val. rec. loss:  9.70176397e-03\n",
      "Epoch: 3188 mean train loss:  1.64431460e-02, mean val. rec. loss:  9.70168460e-03\n",
      "Epoch: 3189 mean train loss:  1.64420342e-02, mean val. rec. loss:  9.70022857e-03\n",
      "Epoch: 3190 mean train loss:  1.64409169e-02, mean val. rec. loss:  9.70050186e-03\n",
      "Epoch: 3191 mean train loss:  1.64398052e-02, mean val. rec. loss:  9.69855028e-03\n",
      "Epoch: 3192 mean train loss:  1.64386860e-02, mean val. rec. loss:  9.69937355e-03\n",
      "Epoch: 3193 mean train loss:  1.64375706e-02, mean val. rec. loss:  9.69667696e-03\n",
      "Epoch: 3194 mean train loss:  1.64364644e-02, mean val. rec. loss:  9.69852987e-03\n",
      "Epoch: 3195 mean train loss:  1.64353564e-02, mean val. rec. loss:  9.69460518e-03\n",
      "Epoch: 3196 mean train loss:  1.64342521e-02, mean val. rec. loss:  9.69813298e-03\n",
      "Epoch: 3197 mean train loss:  1.64331590e-02, mean val. rec. loss:  9.69202879e-03\n",
      "Epoch: 3198 mean train loss:  1.64321069e-02, mean val. rec. loss:  9.69845390e-03\n",
      "Epoch: 3199 mean train loss:  1.64310790e-02, mean val. rec. loss:  9.68876066e-03\n",
      "Epoch: 3200 mean train loss:  1.64301292e-02, mean val. rec. loss:  9.70033970e-03\n",
      "Epoch: 3201 mean train loss:  1.64293192e-02, mean val. rec. loss:  9.68458650e-03\n",
      "Epoch: 3202 mean train loss:  1.64287214e-02, mean val. rec. loss:  9.70506385e-03\n",
      "Epoch: 3203 mean train loss:  1.64284868e-02, mean val. rec. loss:  9.67951535e-03\n",
      "Epoch: 3204 mean train loss:  1.64287959e-02, mean val. rec. loss:  9.71454389e-03\n",
      "Epoch: 3205 mean train loss:  1.64298462e-02, mean val. rec. loss:  9.67499986e-03\n",
      "Epoch: 3206 mean train loss:  1.64316358e-02, mean val. rec. loss:  9.72723989e-03\n",
      "Epoch: 3207 mean train loss:  1.64335203e-02, mean val. rec. loss:  9.67252325e-03\n",
      "Epoch: 3208 mean train loss:  1.64339728e-02, mean val. rec. loss:  9.72587912e-03\n",
      "Epoch: 3209 mean train loss:  1.64310864e-02, mean val. rec. loss:  9.67273304e-03\n",
      "Epoch: 3210 mean train loss:  1.64250045e-02, mean val. rec. loss:  9.69757393e-03\n",
      "Epoch: 3211 mean train loss:  1.64188630e-02, mean val. rec. loss:  9.68671610e-03\n",
      "Epoch: 3212 mean train loss:  1.64161684e-02, mean val. rec. loss:  9.67466420e-03\n",
      "Epoch: 3213 mean train loss:  1.64171833e-02, mean val. rec. loss:  9.70672059e-03\n",
      "Epoch: 3214 mean train loss:  1.64190231e-02, mean val. rec. loss:  9.66933111e-03\n",
      "Epoch: 3215 mean train loss:  1.64185241e-02, mean val. rec. loss:  9.70034310e-03\n",
      "Epoch: 3216 mean train loss:  1.64150641e-02, mean val. rec. loss:  9.67522552e-03\n",
      "Epoch: 3217 mean train loss:  1.64111610e-02, mean val. rec. loss:  9.67769872e-03\n",
      "Epoch: 3218 mean train loss:  1.64094589e-02, mean val. rec. loss:  9.69164437e-03\n",
      "Epoch: 3219 mean train loss:  1.64099059e-02, mean val. rec. loss:  9.66788869e-03\n",
      "Epoch: 3220 mean train loss:  1.64102150e-02, mean val. rec. loss:  9.69403025e-03\n",
      "Epoch: 3221 mean train loss:  1.64086526e-02, mean val. rec. loss:  9.67035622e-03\n",
      "Epoch: 3222 mean train loss:  1.64058314e-02, mean val. rec. loss:  9.67748213e-03\n",
      "Epoch: 3223 mean train loss:  1.64036750e-02, mean val. rec. loss:  9.68205092e-03\n",
      "Epoch: 3224 mean train loss:  1.64030642e-02, mean val. rec. loss:  9.66650637e-03\n",
      "Epoch: 3225 mean train loss:  1.64029617e-02, mean val. rec. loss:  9.68652786e-03\n",
      "Epoch: 3226 mean train loss:  1.64019878e-02, mean val. rec. loss:  9.66692708e-03\n",
      "Epoch: 3227 mean train loss:  1.63999916e-02, mean val. rec. loss:  9.67524707e-03\n",
      "Epoch: 3228 mean train loss:  1.63980325e-02, mean val. rec. loss:  9.67527088e-03\n",
      "Epoch: 3229 mean train loss:  1.63969506e-02, mean val. rec. loss:  9.66505262e-03\n",
      "Epoch: 3230 mean train loss:  1.63964273e-02, mean val. rec. loss:  9.67981132e-03\n",
      "Epoch: 3231 mean train loss:  1.63955558e-02, mean val. rec. loss:  9.66409781e-03\n",
      "Epoch: 3232 mean train loss:  1.63940214e-02, mean val. rec. loss:  9.67227264e-03\n",
      "Epoch: 3233 mean train loss:  1.63923230e-02, mean val. rec. loss:  9.66982666e-03\n",
      "Epoch: 3234 mean train loss:  1.63910661e-02, mean val. rec. loss:  9.66329495e-03\n",
      "Epoch: 3235 mean train loss:  1.63902299e-02, mean val. rec. loss:  9.67368331e-03\n",
      "Epoch: 3236 mean train loss:  1.63893286e-02, mean val. rec. loss:  9.66120163e-03\n",
      "Epoch: 3237 mean train loss:  1.63880400e-02, mean val. rec. loss:  9.66872216e-03\n",
      "Epoch: 3238 mean train loss:  1.63865391e-02, mean val. rec. loss:  9.66488139e-03\n",
      "Epoch: 3239 mean train loss:  1.63852281e-02, mean val. rec. loss:  9.66113019e-03\n",
      "Epoch: 3240 mean train loss:  1.63841965e-02, mean val. rec. loss:  9.66802817e-03\n",
      "Epoch: 3241 mean train loss:  1.63832244e-02, mean val. rec. loss:  9.65828164e-03\n",
      "Epoch: 3242 mean train loss:  1.63820419e-02, mean val. rec. loss:  9.66492674e-03\n",
      "Epoch: 3243 mean train loss:  1.63806993e-02, mean val. rec. loss:  9.66032167e-03\n",
      "Epoch: 3244 mean train loss:  1.63793827e-02, mean val. rec. loss:  9.65870915e-03\n",
      "Epoch: 3245 mean train loss:  1.63782337e-02, mean val. rec. loss:  9.66273023e-03\n",
      "Epoch: 3246 mean train loss:  1.63771723e-02, mean val. rec. loss:  9.65530949e-03\n",
      "Epoch: 3247 mean train loss:  1.63760401e-02, mean val. rec. loss:  9.66088072e-03\n",
      "Epoch: 3248 mean train loss:  1.63747849e-02, mean val. rec. loss:  9.65599668e-03\n",
      "Epoch: 3249 mean train loss:  1.63734982e-02, mean val. rec. loss:  9.65599441e-03\n",
      "Epoch: 3250 mean train loss:  1.63722822e-02, mean val. rec. loss:  9.65770785e-03\n",
      "Epoch: 3251 mean train loss:  1.63711499e-02, mean val. rec. loss:  9.65236795e-03\n",
      "Epoch: 3252 mean train loss:  1.63700103e-02, mean val. rec. loss:  9.65671789e-03\n",
      "Epoch: 3253 mean train loss:  1.63688036e-02, mean val. rec. loss:  9.65192457e-03\n",
      "Epoch: 3254 mean train loss:  1.63675559e-02, mean val. rec. loss:  9.65297803e-03\n",
      "Epoch: 3255 mean train loss:  1.63663213e-02, mean val. rec. loss:  9.65290206e-03\n",
      "Epoch: 3256 mean train loss:  1.63651295e-02, mean val. rec. loss:  9.64934817e-03\n",
      "Epoch: 3257 mean train loss:  1.63639600e-02, mean val. rec. loss:  9.65236115e-03\n",
      "Epoch: 3258 mean train loss:  1.63627738e-02, mean val. rec. loss:  9.64797266e-03\n",
      "Epoch: 3259 mean train loss:  1.63615485e-02, mean val. rec. loss:  9.64957157e-03\n",
      "Epoch: 3260 mean train loss:  1.63602971e-02, mean val. rec. loss:  9.64818812e-03\n",
      "Epoch: 3261 mean train loss:  1.63590811e-02, mean val. rec. loss:  9.64619232e-03\n",
      "Epoch: 3262 mean train loss:  1.63578911e-02, mean val. rec. loss:  9.64784565e-03\n",
      "Epoch: 3263 mean train loss:  1.63566938e-02, mean val. rec. loss:  9.64417724e-03\n",
      "Epoch: 3264 mean train loss:  1.63554666e-02, mean val. rec. loss:  9.64587480e-03\n",
      "Epoch: 3265 mean train loss:  1.63542357e-02, mean val. rec. loss:  9.64370210e-03\n",
      "Epoch: 3266 mean train loss:  1.63530010e-02, mean val. rec. loss:  9.64293780e-03\n",
      "Epoch: 3267 mean train loss:  1.63517720e-02, mean val. rec. loss:  9.64330748e-03\n",
      "Epoch: 3268 mean train loss:  1.63505616e-02, mean val. rec. loss:  9.64053831e-03\n",
      "Epoch: 3269 mean train loss:  1.63493325e-02, mean val. rec. loss:  9.64189455e-03\n",
      "Epoch: 3270 mean train loss:  1.63481053e-02, mean val. rec. loss:  9.63940207e-03\n",
      "Epoch: 3271 mean train loss:  1.63468633e-02, mean val. rec. loss:  9.63948371e-03\n",
      "Epoch: 3272 mean train loss:  1.63456174e-02, mean val. rec. loss:  9.63879312e-03\n",
      "Epoch: 3273 mean train loss:  1.63443754e-02, mean val. rec. loss:  9.63699463e-03\n",
      "Epoch: 3274 mean train loss:  1.63431500e-02, mean val. rec. loss:  9.63768069e-03\n",
      "Epoch: 3275 mean train loss:  1.63419042e-02, mean val. rec. loss:  9.63532996e-03\n",
      "Epoch: 3276 mean train loss:  1.63406584e-02, mean val. rec. loss:  9.63578241e-03\n",
      "Epoch: 3277 mean train loss:  1.63394052e-02, mean val. rec. loss:  9.63435474e-03\n",
      "Epoch: 3278 mean train loss:  1.63381519e-02, mean val. rec. loss:  9.63343281e-03\n",
      "Epoch: 3279 mean train loss:  1.63368949e-02, mean val. rec. loss:  9.63329447e-03\n",
      "Epoch: 3280 mean train loss:  1.63356529e-02, mean val. rec. loss:  9.63140866e-03\n",
      "Epoch: 3281 mean train loss:  1.63343940e-02, mean val. rec. loss:  9.63178061e-03\n",
      "Epoch: 3282 mean train loss:  1.63331352e-02, mean val. rec. loss:  9.63002635e-03\n",
      "Epoch: 3283 mean train loss:  1.63318670e-02, mean val. rec. loss:  9.62971904e-03\n",
      "Epoch: 3284 mean train loss:  1.63305989e-02, mean val. rec. loss:  9.62884248e-03\n",
      "Epoch: 3285 mean train loss:  1.63293326e-02, mean val. rec. loss:  9.62759510e-03\n",
      "Epoch: 3286 mean train loss:  1.63280700e-02, mean val. rec. loss:  9.62754747e-03\n",
      "Epoch: 3287 mean train loss:  1.63267926e-02, mean val. rec. loss:  9.62588733e-03\n",
      "Epoch: 3288 mean train loss:  1.63255226e-02, mean val. rec. loss:  9.62580909e-03\n",
      "Epoch: 3289 mean train loss:  1.63242432e-02, mean val. rec. loss:  9.62449708e-03\n",
      "Epoch: 3290 mean train loss:  1.63229583e-02, mean val. rec. loss:  9.62377020e-03\n",
      "Epoch: 3291 mean train loss:  1.63216753e-02, mean val. rec. loss:  9.62317486e-03\n",
      "Epoch: 3292 mean train loss:  1.63203941e-02, mean val. rec. loss:  9.62186852e-03\n",
      "Epoch: 3293 mean train loss:  1.63191054e-02, mean val. rec. loss:  9.62165306e-03\n",
      "Epoch: 3294 mean train loss:  1.63178094e-02, mean val. rec. loss:  9.62023106e-03\n",
      "Epoch: 3295 mean train loss:  1.63165189e-02, mean val. rec. loss:  9.61981375e-03\n",
      "Epoch: 3296 mean train loss:  1.63152172e-02, mean val. rec. loss:  9.61875689e-03\n",
      "Epoch: 3297 mean train loss:  1.63139248e-02, mean val. rec. loss:  9.61787919e-03\n",
      "Epoch: 3298 mean train loss:  1.63126269e-02, mean val. rec. loss:  9.61729633e-03\n",
      "Epoch: 3299 mean train loss:  1.63113252e-02, mean val. rec. loss:  9.61607276e-03\n",
      "Epoch: 3300 mean train loss:  1.63100180e-02, mean val. rec. loss:  9.61564866e-03\n",
      "Epoch: 3301 mean train loss:  1.63087032e-02, mean val. rec. loss:  9.61438541e-03\n",
      "Epoch: 3302 mean train loss:  1.63073848e-02, mean val. rec. loss:  9.61378780e-03\n",
      "Epoch: 3303 mean train loss:  1.63060813e-02, mean val. rec. loss:  9.61284547e-03\n",
      "Epoch: 3304 mean train loss:  1.63047554e-02, mean val. rec. loss:  9.61194963e-03\n",
      "Epoch: 3305 mean train loss:  1.63034332e-02, mean val. rec. loss:  9.61127264e-03\n",
      "Epoch: 3306 mean train loss:  1.63021092e-02, mean val. rec. loss:  9.61011599e-03\n",
      "Epoch: 3307 mean train loss:  1.63007815e-02, mean val. rec. loss:  9.60954900e-03\n",
      "Epoch: 3308 mean train loss:  1.62994631e-02, mean val. rec. loss:  9.60844904e-03\n",
      "Epoch: 3309 mean train loss:  1.62981223e-02, mean val. rec. loss:  9.60778453e-03\n",
      "Epoch: 3310 mean train loss:  1.62967890e-02, mean val. rec. loss:  9.60683653e-03\n",
      "Epoch: 3311 mean train loss:  1.62954593e-02, mean val. rec. loss:  9.60592821e-03\n",
      "Epoch: 3312 mean train loss:  1.62941111e-02, mean val. rec. loss:  9.60518092e-03\n",
      "Epoch: 3313 mean train loss:  1.62927703e-02, mean val. rec. loss:  9.60412179e-03\n",
      "Epoch: 3314 mean train loss:  1.62914258e-02, mean val. rec. loss:  9.60345387e-03\n",
      "Epoch: 3315 mean train loss:  1.62900795e-02, mean val. rec. loss:  9.60237773e-03\n",
      "Epoch: 3316 mean train loss:  1.62887219e-02, mean val. rec. loss:  9.60164291e-03\n",
      "Epoch: 3317 mean train loss:  1.62873681e-02, mean val. rec. loss:  9.60066769e-03\n",
      "Epoch: 3318 mean train loss:  1.62860050e-02, mean val. rec. loss:  9.59978999e-03\n",
      "Epoch: 3319 mean train loss:  1.62846475e-02, mean val. rec. loss:  9.59897013e-03\n",
      "Epoch: 3320 mean train loss:  1.62832843e-02, mean val. rec. loss:  9.59799151e-03\n",
      "Epoch: 3321 mean train loss:  1.62819138e-02, mean val. rec. loss:  9.59722381e-03\n",
      "Epoch: 3322 mean train loss:  1.62805469e-02, mean val. rec. loss:  9.59620890e-03\n",
      "Epoch: 3323 mean train loss:  1.62791764e-02, mean val. rec. loss:  9.59543099e-03\n",
      "Epoch: 3324 mean train loss:  1.62778039e-02, mean val. rec. loss:  9.59444897e-03\n",
      "Epoch: 3325 mean train loss:  1.62764203e-02, mean val. rec. loss:  9.59360302e-03\n",
      "Epoch: 3326 mean train loss:  1.62750367e-02, mean val. rec. loss:  9.59268677e-03\n",
      "Epoch: 3327 mean train loss:  1.62736531e-02, mean val. rec. loss:  9.59175690e-03\n",
      "Epoch: 3328 mean train loss:  1.62722695e-02, mean val. rec. loss:  9.59090869e-03\n",
      "Epoch: 3329 mean train loss:  1.62708728e-02, mean val. rec. loss:  9.58991986e-03\n",
      "Epoch: 3330 mean train loss:  1.62694781e-02, mean val. rec. loss:  9.58911247e-03\n",
      "Epoch: 3331 mean train loss:  1.62680740e-02, mean val. rec. loss:  9.58811684e-03\n",
      "Epoch: 3332 mean train loss:  1.62666792e-02, mean val. rec. loss:  9.58725955e-03\n",
      "Epoch: 3333 mean train loss:  1.62652732e-02, mean val. rec. loss:  9.58630021e-03\n",
      "Epoch: 3334 mean train loss:  1.62638673e-02, mean val. rec. loss:  9.58540777e-03\n",
      "Epoch: 3335 mean train loss:  1.62624502e-02, mean val. rec. loss:  9.58447904e-03\n",
      "Epoch: 3336 mean train loss:  1.62610423e-02, mean val. rec. loss:  9.58352197e-03\n",
      "Epoch: 3337 mean train loss:  1.62596252e-02, mean val. rec. loss:  9.58264427e-03\n",
      "Epoch: 3338 mean train loss:  1.62582081e-02, mean val. rec. loss:  9.58167132e-03\n",
      "Epoch: 3339 mean train loss:  1.62567798e-02, mean val. rec. loss:  9.58079589e-03\n",
      "Epoch: 3340 mean train loss:  1.62553534e-02, mean val. rec. loss:  9.57980706e-03\n",
      "Epoch: 3341 mean train loss:  1.62539288e-02, mean val. rec. loss:  9.57892596e-03\n",
      "Epoch: 3342 mean train loss:  1.62524819e-02, mean val. rec. loss:  9.57795074e-03\n",
      "Epoch: 3343 mean train loss:  1.62510554e-02, mean val. rec. loss:  9.57704356e-03\n",
      "Epoch: 3344 mean train loss:  1.62496104e-02, mean val. rec. loss:  9.57609102e-03\n",
      "Epoch: 3345 mean train loss:  1.62481616e-02, mean val. rec. loss:  9.57514642e-03\n",
      "Epoch: 3346 mean train loss:  1.62467146e-02, mean val. rec. loss:  9.57419501e-03\n",
      "Epoch: 3347 mean train loss:  1.62452677e-02, mean val. rec. loss:  9.57323680e-03\n",
      "Epoch: 3348 mean train loss:  1.62438208e-02, mean val. rec. loss:  9.57231828e-03\n",
      "Epoch: 3349 mean train loss:  1.62423515e-02, mean val. rec. loss:  9.57135326e-03\n",
      "Epoch: 3350 mean train loss:  1.62408934e-02, mean val. rec. loss:  9.57041660e-03\n",
      "Epoch: 3351 mean train loss:  1.62394372e-02, mean val. rec. loss:  9.56944138e-03\n",
      "Epoch: 3352 mean train loss:  1.62379698e-02, mean val. rec. loss:  9.56848657e-03\n",
      "Epoch: 3353 mean train loss:  1.62364949e-02, mean val. rec. loss:  9.56751929e-03\n",
      "Epoch: 3354 mean train loss:  1.62350145e-02, mean val. rec. loss:  9.56656561e-03\n",
      "Epoch: 3355 mean train loss:  1.62335378e-02, mean val. rec. loss:  9.56558246e-03\n",
      "Epoch: 3356 mean train loss:  1.62320555e-02, mean val. rec. loss:  9.56460043e-03\n",
      "Epoch: 3357 mean train loss:  1.62305732e-02, mean val. rec. loss:  9.56363202e-03\n",
      "Epoch: 3358 mean train loss:  1.62290834e-02, mean val. rec. loss:  9.56268061e-03\n",
      "Epoch: 3359 mean train loss:  1.62275937e-02, mean val. rec. loss:  9.56170426e-03\n",
      "Epoch: 3360 mean train loss:  1.62260928e-02, mean val. rec. loss:  9.56071997e-03\n",
      "Epoch: 3361 mean train loss:  1.62245900e-02, mean val. rec. loss:  9.55973341e-03\n",
      "Epoch: 3362 mean train loss:  1.62230797e-02, mean val. rec. loss:  9.55873437e-03\n",
      "Epoch: 3363 mean train loss:  1.62215769e-02, mean val. rec. loss:  9.55775575e-03\n",
      "Epoch: 3364 mean train loss:  1.62200667e-02, mean val. rec. loss:  9.55678053e-03\n",
      "Epoch: 3365 mean train loss:  1.62185472e-02, mean val. rec. loss:  9.55578943e-03\n",
      "Epoch: 3366 mean train loss:  1.62170313e-02, mean val. rec. loss:  9.55478700e-03\n",
      "Epoch: 3367 mean train loss:  1.62155099e-02, mean val. rec. loss:  9.55378230e-03\n",
      "Epoch: 3368 mean train loss:  1.62139829e-02, mean val. rec. loss:  9.55278666e-03\n",
      "Epoch: 3369 mean train loss:  1.62124466e-02, mean val. rec. loss:  9.55178763e-03\n",
      "Epoch: 3370 mean train loss:  1.62109122e-02, mean val. rec. loss:  9.55078973e-03\n",
      "Epoch: 3371 mean train loss:  1.62093815e-02, mean val. rec. loss:  9.54977256e-03\n",
      "Epoch: 3372 mean train loss:  1.62078433e-02, mean val. rec. loss:  9.54877579e-03\n",
      "Epoch: 3373 mean train loss:  1.62062884e-02, mean val. rec. loss:  9.54774727e-03\n",
      "Epoch: 3374 mean train loss:  1.62047446e-02, mean val. rec. loss:  9.54673690e-03\n",
      "Epoch: 3375 mean train loss:  1.62031953e-02, mean val. rec. loss:  9.54572539e-03\n",
      "Epoch: 3376 mean train loss:  1.62016385e-02, mean val. rec. loss:  9.54472409e-03\n",
      "Epoch: 3377 mean train loss:  1.62000742e-02, mean val. rec. loss:  9.54368423e-03\n",
      "Epoch: 3378 mean train loss:  1.61985025e-02, mean val. rec. loss:  9.54263984e-03\n",
      "Epoch: 3379 mean train loss:  1.61969439e-02, mean val. rec. loss:  9.54161019e-03\n",
      "Epoch: 3380 mean train loss:  1.61953647e-02, mean val. rec. loss:  9.54058054e-03\n",
      "Epoch: 3381 mean train loss:  1.61937875e-02, mean val. rec. loss:  9.53955769e-03\n",
      "Epoch: 3382 mean train loss:  1.61922083e-02, mean val. rec. loss:  9.53853031e-03\n",
      "Epoch: 3383 mean train loss:  1.61906255e-02, mean val. rec. loss:  9.53749159e-03\n",
      "Epoch: 3384 mean train loss:  1.61890258e-02, mean val. rec. loss:  9.53642111e-03\n",
      "Epoch: 3385 mean train loss:  1.61874430e-02, mean val. rec. loss:  9.53538579e-03\n",
      "Epoch: 3386 mean train loss:  1.61858415e-02, mean val. rec. loss:  9.53433233e-03\n",
      "Epoch: 3387 mean train loss:  1.61842400e-02, mean val. rec. loss:  9.53331062e-03\n",
      "Epoch: 3388 mean train loss:  1.61826367e-02, mean val. rec. loss:  9.53222994e-03\n",
      "Epoch: 3389 mean train loss:  1.61810240e-02, mean val. rec. loss:  9.53119915e-03\n",
      "Epoch: 3390 mean train loss:  1.61794095e-02, mean val. rec. loss:  9.53009693e-03\n",
      "Epoch: 3391 mean train loss:  1.61777838e-02, mean val. rec. loss:  9.52907295e-03\n",
      "Epoch: 3392 mean train loss:  1.61761637e-02, mean val. rec. loss:  9.52796392e-03\n",
      "Epoch: 3393 mean train loss:  1.61745361e-02, mean val. rec. loss:  9.52693880e-03\n",
      "Epoch: 3394 mean train loss:  1.61729123e-02, mean val. rec. loss:  9.52583885e-03\n",
      "Epoch: 3395 mean train loss:  1.61712773e-02, mean val. rec. loss:  9.52481146e-03\n",
      "Epoch: 3396 mean train loss:  1.61696349e-02, mean val. rec. loss:  9.52367862e-03\n",
      "Epoch: 3397 mean train loss:  1.61679868e-02, mean val. rec. loss:  9.52264217e-03\n",
      "Epoch: 3398 mean train loss:  1.61663388e-02, mean val. rec. loss:  9.52149798e-03\n",
      "Epoch: 3399 mean train loss:  1.61646907e-02, mean val. rec. loss:  9.52049101e-03\n",
      "Epoch: 3400 mean train loss:  1.61630353e-02, mean val. rec. loss:  9.51932755e-03\n",
      "Epoch: 3401 mean train loss:  1.61613723e-02, mean val. rec. loss:  9.51832965e-03\n",
      "Epoch: 3402 mean train loss:  1.61597001e-02, mean val. rec. loss:  9.51712764e-03\n",
      "Epoch: 3403 mean train loss:  1.61580316e-02, mean val. rec. loss:  9.51612860e-03\n",
      "Epoch: 3404 mean train loss:  1.61563612e-02, mean val. rec. loss:  9.51491978e-03\n",
      "Epoch: 3405 mean train loss:  1.61546852e-02, mean val. rec. loss:  9.51394116e-03\n",
      "Epoch: 3406 mean train loss:  1.61529943e-02, mean val. rec. loss:  9.51271647e-03\n",
      "Epoch: 3407 mean train loss:  1.61513109e-02, mean val. rec. loss:  9.51174635e-03\n",
      "Epoch: 3408 mean train loss:  1.61496163e-02, mean val. rec. loss:  9.51046212e-03\n",
      "Epoch: 3409 mean train loss:  1.61479217e-02, mean val. rec. loss:  9.50951355e-03\n",
      "Epoch: 3410 mean train loss:  1.61462104e-02, mean val. rec. loss:  9.50819133e-03\n",
      "Epoch: 3411 mean train loss:  1.61445139e-02, mean val. rec. loss:  9.50730683e-03\n",
      "Epoch: 3412 mean train loss:  1.61427970e-02, mean val. rec. loss:  9.50592395e-03\n",
      "Epoch: 3413 mean train loss:  1.61410782e-02, mean val. rec. loss:  9.50508991e-03\n",
      "Epoch: 3414 mean train loss:  1.61393612e-02, mean val. rec. loss:  9.50360270e-03\n",
      "Epoch: 3415 mean train loss:  1.61376424e-02, mean val. rec. loss:  9.50287469e-03\n",
      "Epoch: 3416 mean train loss:  1.61359013e-02, mean val. rec. loss:  9.50125763e-03\n",
      "Epoch: 3417 mean train loss:  1.61341788e-02, mean val. rec. loss:  9.50068895e-03\n",
      "Epoch: 3418 mean train loss:  1.61324376e-02, mean val. rec. loss:  9.49885927e-03\n",
      "Epoch: 3419 mean train loss:  1.61306909e-02, mean val. rec. loss:  9.49853609e-03\n",
      "Epoch: 3420 mean train loss:  1.61289441e-02, mean val. rec. loss:  9.49638947e-03\n",
      "Epoch: 3421 mean train loss:  1.61271881e-02, mean val. rec. loss:  9.49644731e-03\n",
      "Epoch: 3422 mean train loss:  1.61254339e-02, mean val. rec. loss:  9.49382385e-03\n",
      "Epoch: 3423 mean train loss:  1.61236797e-02, mean val. rec. loss:  9.49448156e-03\n",
      "Epoch: 3424 mean train loss:  1.61219218e-02, mean val. rec. loss:  9.49111308e-03\n",
      "Epoch: 3425 mean train loss:  1.61201509e-02, mean val. rec. loss:  9.49270632e-03\n",
      "Epoch: 3426 mean train loss:  1.61184023e-02, mean val. rec. loss:  9.48816247e-03\n",
      "Epoch: 3427 mean train loss:  1.61166406e-02, mean val. rec. loss:  9.49126163e-03\n",
      "Epoch: 3428 mean train loss:  1.61149088e-02, mean val. rec. loss:  9.48487848e-03\n",
      "Epoch: 3429 mean train loss:  1.61131993e-02, mean val. rec. loss:  9.49045594e-03\n",
      "Epoch: 3430 mean train loss:  1.61115196e-02, mean val. rec. loss:  9.48113976e-03\n",
      "Epoch: 3431 mean train loss:  1.61099256e-02, mean val. rec. loss:  9.49078196e-03\n",
      "Epoch: 3432 mean train loss:  1.61084433e-02, mean val. rec. loss:  9.47687544e-03\n",
      "Epoch: 3433 mean train loss:  1.61071621e-02, mean val. rec. loss:  9.49317635e-03\n",
      "Epoch: 3434 mean train loss:  1.61061584e-02, mean val. rec. loss:  9.47240360e-03\n",
      "Epoch: 3435 mean train loss:  1.61055755e-02, mean val. rec. loss:  9.49873907e-03\n",
      "Epoch: 3436 mean train loss:  1.61054861e-02, mean val. rec. loss:  9.46861045e-03\n",
      "Epoch: 3437 mean train loss:  1.61058846e-02, mean val. rec. loss:  9.50622105e-03\n",
      "Epoch: 3438 mean train loss:  1.61063204e-02, mean val. rec. loss:  9.46603065e-03\n",
      "Epoch: 3439 mean train loss:  1.61059014e-02, mean val. rec. loss:  9.50555314e-03\n",
      "Epoch: 3440 mean train loss:  1.61033744e-02, mean val. rec. loss:  9.46370940e-03\n",
      "Epoch: 3441 mean train loss:  1.60984061e-02, mean val. rec. loss:  9.48656924e-03\n",
      "Epoch: 3442 mean train loss:  1.60923986e-02, mean val. rec. loss:  9.46759667e-03\n",
      "Epoch: 3443 mean train loss:  1.60878139e-02, mean val. rec. loss:  9.46620755e-03\n",
      "Epoch: 3444 mean train loss:  1.60859722e-02, mean val. rec. loss:  9.48033123e-03\n",
      "Epoch: 3445 mean train loss:  1.60861287e-02, mean val. rec. loss:  9.45823003e-03\n",
      "Epoch: 3446 mean train loss:  1.60862199e-02, mean val. rec. loss:  9.48370424e-03\n",
      "Epoch: 3447 mean train loss:  1.60845551e-02, mean val. rec. loss:  9.45670993e-03\n",
      "Epoch: 3448 mean train loss:  1.60810449e-02, mean val. rec. loss:  9.46964690e-03\n",
      "Epoch: 3449 mean train loss:  1.60771361e-02, mean val. rec. loss:  9.46228513e-03\n",
      "Epoch: 3450 mean train loss:  1.60744918e-02, mean val. rec. loss:  9.45599212e-03\n",
      "Epoch: 3451 mean train loss:  1.60733726e-02, mean val. rec. loss:  9.47032275e-03\n",
      "Epoch: 3452 mean train loss:  1.60727078e-02, mean val. rec. loss:  9.45089206e-03\n",
      "Epoch: 3453 mean train loss:  1.60712181e-02, mean val. rec. loss:  9.46646950e-03\n",
      "Epoch: 3454 mean train loss:  1.60685607e-02, mean val. rec. loss:  9.45164446e-03\n",
      "Epoch: 3455 mean train loss:  1.60655570e-02, mean val. rec. loss:  9.45422992e-03\n",
      "Epoch: 3456 mean train loss:  1.60631716e-02, mean val. rec. loss:  9.45727408e-03\n",
      "Epoch: 3457 mean train loss:  1.60616557e-02, mean val. rec. loss:  9.44636523e-03\n",
      "Epoch: 3458 mean train loss:  1.60604137e-02, mean val. rec. loss:  9.45881856e-03\n",
      "Epoch: 3459 mean train loss:  1.60587116e-02, mean val. rec. loss:  9.44441762e-03\n",
      "Epoch: 3460 mean train loss:  1.60563690e-02, mean val. rec. loss:  9.45132864e-03\n",
      "Epoch: 3461 mean train loss:  1.60538513e-02, mean val. rec. loss:  9.44694923e-03\n",
      "Epoch: 3462 mean train loss:  1.60516800e-02, mean val. rec. loss:  9.44279490e-03\n",
      "Epoch: 3463 mean train loss:  1.60499779e-02, mean val. rec. loss:  9.44963675e-03\n",
      "Epoch: 3464 mean train loss:  1.60483914e-02, mean val. rec. loss:  9.43859295e-03\n",
      "Epoch: 3465 mean train loss:  1.60465292e-02, mean val. rec. loss:  9.44620194e-03\n",
      "Epoch: 3466 mean train loss:  1.60443430e-02, mean val. rec. loss:  9.43831570e-03\n",
      "Epoch: 3467 mean train loss:  1.60420580e-02, mean val. rec. loss:  9.43881068e-03\n",
      "Epoch: 3468 mean train loss:  1.60399873e-02, mean val. rec. loss:  9.44007336e-03\n",
      "Epoch: 3469 mean train loss:  1.60381474e-02, mean val. rec. loss:  9.43330239e-03\n",
      "Epoch: 3470 mean train loss:  1.60363560e-02, mean val. rec. loss:  9.43922628e-03\n",
      "Epoch: 3471 mean train loss:  1.60344212e-02, mean val. rec. loss:  9.43104294e-03\n",
      "Epoch: 3472 mean train loss:  1.60323113e-02, mean val. rec. loss:  9.43413586e-03\n",
      "Epoch: 3473 mean train loss:  1.60301419e-02, mean val. rec. loss:  9.43123458e-03\n",
      "Epoch: 3474 mean train loss:  1.60280823e-02, mean val. rec. loss:  9.42839624e-03\n",
      "Epoch: 3475 mean train loss:  1.60261438e-02, mean val. rec. loss:  9.43116711e-03\n",
      "Epoch: 3476 mean train loss:  1.60242313e-02, mean val. rec. loss:  9.42465525e-03\n",
      "Epoch: 3477 mean train loss:  1.60222481e-02, mean val. rec. loss:  9.42819212e-03\n",
      "Epoch: 3478 mean train loss:  1.60201643e-02, mean val. rec. loss:  9.42318051e-03\n",
      "Epoch: 3479 mean train loss:  1.60180451e-02, mean val. rec. loss:  9.42327690e-03\n",
      "Epoch: 3480 mean train loss:  1.60159669e-02, mean val. rec. loss:  9.42276434e-03\n",
      "Epoch: 3481 mean train loss:  1.60139557e-02, mean val. rec. loss:  9.41886970e-03\n",
      "Epoch: 3482 mean train loss:  1.60119595e-02, mean val. rec. loss:  9.42117451e-03\n",
      "Epoch: 3483 mean train loss:  1.60099408e-02, mean val. rec. loss:  9.41606141e-03\n",
      "Epoch: 3484 mean train loss:  1.60078589e-02, mean val. rec. loss:  9.41757923e-03\n",
      "Epoch: 3485 mean train loss:  1.60057435e-02, mean val. rec. loss:  9.41463543e-03\n",
      "Epoch: 3486 mean train loss:  1.60036429e-02, mean val. rec. loss:  9.41319925e-03\n",
      "Epoch: 3487 mean train loss:  1.60015647e-02, mean val. rec. loss:  9.41329337e-03\n",
      "Epoch: 3488 mean train loss:  1.59995014e-02, mean val. rec. loss:  9.40948151e-03\n",
      "Epoch: 3489 mean train loss:  1.59974381e-02, mean val. rec. loss:  9.41080826e-03\n",
      "Epoch: 3490 mean train loss:  1.59953431e-02, mean val. rec. loss:  9.40698846e-03\n",
      "Epoch: 3491 mean train loss:  1.59932184e-02, mean val. rec. loss:  9.40716819e-03\n",
      "Epoch: 3492 mean train loss:  1.59910806e-02, mean val. rec. loss:  9.40523476e-03\n",
      "Epoch: 3493 mean train loss:  1.59889614e-02, mean val. rec. loss:  9.40327582e-03\n",
      "Epoch: 3494 mean train loss:  1.59868348e-02, mean val. rec. loss:  9.40326845e-03\n",
      "Epoch: 3495 mean train loss:  1.59847287e-02, mean val. rec. loss:  9.39997822e-03\n",
      "Epoch: 3496 mean train loss:  1.59825927e-02, mean val. rec. loss:  9.40048227e-03\n",
      "Epoch: 3497 mean train loss:  1.59804493e-02, mean val. rec. loss:  9.39746816e-03\n",
      "Epoch: 3498 mean train loss:  1.59782873e-02, mean val. rec. loss:  9.39694710e-03\n",
      "Epoch: 3499 mean train loss:  1.59761253e-02, mean val. rec. loss:  9.39533515e-03\n",
      "Epoch: 3500 mean train loss:  1.59739559e-02, mean val. rec. loss:  9.39336203e-03\n",
      "Epoch: 3501 mean train loss:  1.59717864e-02, mean val. rec. loss:  9.39300142e-03\n",
      "Epoch: 3502 mean train loss:  1.59696226e-02, mean val. rec. loss:  9.39020674e-03\n",
      "Epoch: 3503 mean train loss:  1.59674419e-02, mean val. rec. loss:  9.39009277e-03\n",
      "Epoch: 3504 mean train loss:  1.59652539e-02, mean val. rec. loss:  9.38758669e-03\n",
      "Epoch: 3505 mean train loss:  1.59630472e-02, mean val. rec. loss:  9.38673564e-03\n",
      "Epoch: 3506 mean train loss:  1.59608386e-02, mean val. rec. loss:  9.38513673e-03\n",
      "Epoch: 3507 mean train loss:  1.59586263e-02, mean val. rec. loss:  9.38329288e-03\n",
      "Epoch: 3508 mean train loss:  1.59564103e-02, mean val. rec. loss:  9.38255750e-03\n",
      "Epoch: 3509 mean train loss:  1.59541906e-02, mean val. rec. loss:  9.38015574e-03\n",
      "Epoch: 3510 mean train loss:  1.59519522e-02, mean val. rec. loss:  9.37960349e-03\n",
      "Epoch: 3511 mean train loss:  1.59497213e-02, mean val. rec. loss:  9.37729755e-03\n",
      "Epoch: 3512 mean train loss:  1.59474718e-02, mean val. rec. loss:  9.37632290e-03\n",
      "Epoch: 3513 mean train loss:  1.59452111e-02, mean val. rec. loss:  9.37463157e-03\n",
      "Epoch: 3514 mean train loss:  1.59429467e-02, mean val. rec. loss:  9.37300148e-03\n",
      "Epoch: 3515 mean train loss:  1.59406785e-02, mean val. rec. loss:  9.37188451e-03\n",
      "Epoch: 3516 mean train loss:  1.59384122e-02, mean val. rec. loss:  9.36979459e-03\n",
      "Epoch: 3517 mean train loss:  1.59361236e-02, mean val. rec. loss:  9.36889648e-03\n",
      "Epoch: 3518 mean train loss:  1.59338368e-02, mean val. rec. loss:  9.36677198e-03\n",
      "Epoch: 3519 mean train loss:  1.59315389e-02, mean val. rec. loss:  9.36570774e-03\n",
      "Epoch: 3520 mean train loss:  1.59292279e-02, mean val. rec. loss:  9.36391379e-03\n",
      "Epoch: 3521 mean train loss:  1.59269207e-02, mean val. rec. loss:  9.36242261e-03\n",
      "Epoch: 3522 mean train loss:  1.59246004e-02, mean val. rec. loss:  9.36099040e-03\n",
      "Epoch: 3523 mean train loss:  1.59222671e-02, mean val. rec. loss:  9.35914655e-03\n",
      "Epoch: 3524 mean train loss:  1.59199412e-02, mean val. rec. loss:  9.35798763e-03\n",
      "Epoch: 3525 mean train loss:  1.59175985e-02, mean val. rec. loss:  9.35601961e-03\n",
      "Epoch: 3526 mean train loss:  1.59152485e-02, mean val. rec. loss:  9.35483177e-03\n",
      "Epoch: 3527 mean train loss:  1.59128798e-02, mean val. rec. loss:  9.35292442e-03\n",
      "Epoch: 3528 mean train loss:  1.59105222e-02, mean val. rec. loss:  9.35152850e-03\n",
      "Epoch: 3529 mean train loss:  1.59081498e-02, mean val. rec. loss:  9.34988026e-03\n",
      "Epoch: 3530 mean train loss:  1.59057662e-02, mean val. rec. loss:  9.34824790e-03\n",
      "Epoch: 3531 mean train loss:  1.59033733e-02, mean val. rec. loss:  9.34681286e-03\n",
      "Epoch: 3532 mean train loss:  1.59009785e-02, mean val. rec. loss:  9.34497355e-03\n",
      "Epoch: 3533 mean train loss:  1.58985725e-02, mean val. rec. loss:  9.34362241e-03\n",
      "Epoch: 3534 mean train loss:  1.58961610e-02, mean val. rec. loss:  9.34173434e-03\n",
      "Epoch: 3535 mean train loss:  1.58937383e-02, mean val. rec. loss:  9.34035996e-03\n",
      "Epoch: 3536 mean train loss:  1.58913081e-02, mean val. rec. loss:  9.33856998e-03\n",
      "Epoch: 3537 mean train loss:  1.58888743e-02, mean val. rec. loss:  9.33703231e-03\n",
      "Epoch: 3538 mean train loss:  1.58864180e-02, mean val. rec. loss:  9.33534892e-03\n",
      "Epoch: 3539 mean train loss:  1.58839692e-02, mean val. rec. loss:  9.33364059e-03\n",
      "Epoch: 3540 mean train loss:  1.58815093e-02, mean val. rec. loss:  9.33212162e-03\n",
      "Epoch: 3541 mean train loss:  1.58790382e-02, mean val. rec. loss:  9.33032257e-03\n",
      "Epoch: 3542 mean train loss:  1.58765521e-02, mean val. rec. loss:  9.32883536e-03\n",
      "Epoch: 3543 mean train loss:  1.58740680e-02, mean val. rec. loss:  9.32697280e-03\n",
      "Epoch: 3544 mean train loss:  1.58715782e-02, mean val. rec. loss:  9.32547028e-03\n",
      "Epoch: 3545 mean train loss:  1.58690680e-02, mean val. rec. loss:  9.32366896e-03\n",
      "Epoch: 3546 mean train loss:  1.58665559e-02, mean val. rec. loss:  9.32208253e-03\n",
      "Epoch: 3547 mean train loss:  1.58640419e-02, mean val. rec. loss:  9.32033677e-03\n",
      "Epoch: 3548 mean train loss:  1.58615131e-02, mean val. rec. loss:  9.31864658e-03\n",
      "Epoch: 3549 mean train loss:  1.58589768e-02, mean val. rec. loss:  9.31697737e-03\n",
      "Epoch: 3550 mean train loss:  1.58564293e-02, mean val. rec. loss:  9.31520553e-03\n",
      "Epoch: 3551 mean train loss:  1.58538651e-02, mean val. rec. loss:  9.31357770e-03\n",
      "Epoch: 3552 mean train loss:  1.58513046e-02, mean val. rec. loss:  9.31174010e-03\n",
      "Epoch: 3553 mean train loss:  1.58487329e-02, mean val. rec. loss:  9.31012248e-03\n",
      "Epoch: 3554 mean train loss:  1.58461444e-02, mean val. rec. loss:  9.30827296e-03\n",
      "Epoch: 3555 mean train loss:  1.58435634e-02, mean val. rec. loss:  9.30664911e-03\n",
      "Epoch: 3556 mean train loss:  1.58409638e-02, mean val. rec. loss:  9.30481887e-03\n",
      "Epoch: 3557 mean train loss:  1.58383474e-02, mean val. rec. loss:  9.30311394e-03\n",
      "Epoch: 3558 mean train loss:  1.58357329e-02, mean val. rec. loss:  9.30130808e-03\n",
      "Epoch: 3559 mean train loss:  1.58331054e-02, mean val. rec. loss:  9.29957026e-03\n",
      "Epoch: 3560 mean train loss:  1.58304741e-02, mean val. rec. loss:  9.29781259e-03\n",
      "Epoch: 3561 mean train loss:  1.58278186e-02, mean val. rec. loss:  9.29601467e-03\n",
      "Epoch: 3562 mean train loss:  1.58251687e-02, mean val. rec. loss:  9.29427345e-03\n",
      "Epoch: 3563 mean train loss:  1.58225021e-02, mean val. rec. loss:  9.29239729e-03\n",
      "Epoch: 3564 mean train loss:  1.58198373e-02, mean val. rec. loss:  9.29065664e-03\n",
      "Epoch: 3565 mean train loss:  1.58171501e-02, mean val. rec. loss:  9.28878841e-03\n",
      "Epoch: 3566 mean train loss:  1.58144593e-02, mean val. rec. loss:  9.28707157e-03\n",
      "Epoch: 3567 mean train loss:  1.58117554e-02, mean val. rec. loss:  9.28518066e-03\n",
      "Epoch: 3568 mean train loss:  1.58090496e-02, mean val. rec. loss:  9.28341846e-03\n",
      "Epoch: 3569 mean train loss:  1.58063308e-02, mean val. rec. loss:  9.28147823e-03\n",
      "Epoch: 3570 mean train loss:  1.58035952e-02, mean val. rec. loss:  9.27973077e-03\n",
      "Epoch: 3571 mean train loss:  1.58008597e-02, mean val. rec. loss:  9.27781548e-03\n",
      "Epoch: 3572 mean train loss:  1.57981111e-02, mean val. rec. loss:  9.27605215e-03\n",
      "Epoch: 3573 mean train loss:  1.57953383e-02, mean val. rec. loss:  9.27410228e-03\n",
      "Epoch: 3574 mean train loss:  1.57925785e-02, mean val. rec. loss:  9.27228621e-03\n",
      "Epoch: 3575 mean train loss:  1.57897983e-02, mean val. rec. loss:  9.27035392e-03\n",
      "Epoch: 3576 mean train loss:  1.57870069e-02, mean val. rec. loss:  9.26854523e-03\n",
      "Epoch: 3577 mean train loss:  1.57842024e-02, mean val. rec. loss:  9.26660442e-03\n",
      "Epoch: 3578 mean train loss:  1.57813961e-02, mean val. rec. loss:  9.26474414e-03\n",
      "Epoch: 3579 mean train loss:  1.57785842e-02, mean val. rec. loss:  9.26280504e-03\n",
      "Epoch: 3580 mean train loss:  1.57757518e-02, mean val. rec. loss:  9.26092263e-03\n",
      "Epoch: 3581 mean train loss:  1.57729119e-02, mean val. rec. loss:  9.25897787e-03\n",
      "Epoch: 3582 mean train loss:  1.57700646e-02, mean val. rec. loss:  9.25707902e-03\n",
      "Epoch: 3583 mean train loss:  1.57672006e-02, mean val. rec. loss:  9.25512178e-03\n",
      "Epoch: 3584 mean train loss:  1.57643310e-02, mean val. rec. loss:  9.25318154e-03\n",
      "Epoch: 3585 mean train loss:  1.57614464e-02, mean val. rec. loss:  9.25121126e-03\n",
      "Epoch: 3586 mean train loss:  1.57585619e-02, mean val. rec. loss:  9.24925855e-03\n",
      "Epoch: 3587 mean train loss:  1.57556587e-02, mean val. rec. loss:  9.24731945e-03\n",
      "Epoch: 3588 mean train loss:  1.57527370e-02, mean val. rec. loss:  9.24535144e-03\n",
      "Epoch: 3589 mean train loss:  1.57498189e-02, mean val. rec. loss:  9.24336074e-03\n",
      "Epoch: 3590 mean train loss:  1.57468785e-02, mean val. rec. loss:  9.24134906e-03\n",
      "Epoch: 3591 mean train loss:  1.57439456e-02, mean val. rec. loss:  9.23936461e-03\n",
      "Epoch: 3592 mean train loss:  1.57409884e-02, mean val. rec. loss:  9.23738582e-03\n",
      "Epoch: 3593 mean train loss:  1.57380163e-02, mean val. rec. loss:  9.23537811e-03\n",
      "Epoch: 3594 mean train loss:  1.57350387e-02, mean val. rec. loss:  9.23334546e-03\n",
      "Epoch: 3595 mean train loss:  1.57320517e-02, mean val. rec. loss:  9.23129183e-03\n",
      "Epoch: 3596 mean train loss:  1.57290555e-02, mean val. rec. loss:  9.22927051e-03\n",
      "Epoch: 3597 mean train loss:  1.57260387e-02, mean val. rec. loss:  9.22724636e-03\n",
      "Epoch: 3598 mean train loss:  1.57230164e-02, mean val. rec. loss:  9.22519784e-03\n",
      "Epoch: 3599 mean train loss:  1.57199885e-02, mean val. rec. loss:  9.22313457e-03\n",
      "Epoch: 3600 mean train loss:  1.57169382e-02, mean val. rec. loss:  9.22103331e-03\n",
      "Epoch: 3601 mean train loss:  1.57138805e-02, mean val. rec. loss:  9.21896153e-03\n",
      "Epoch: 3602 mean train loss:  1.57108135e-02, mean val. rec. loss:  9.21688522e-03\n",
      "Epoch: 3603 mean train loss:  1.57077390e-02, mean val. rec. loss:  9.21481571e-03\n",
      "Epoch: 3604 mean train loss:  1.57046422e-02, mean val. rec. loss:  9.21268667e-03\n",
      "Epoch: 3605 mean train loss:  1.57015416e-02, mean val. rec. loss:  9.21059392e-03\n",
      "Epoch: 3606 mean train loss:  1.56984262e-02, mean val. rec. loss:  9.20843256e-03\n",
      "Epoch: 3607 mean train loss:  1.56953051e-02, mean val. rec. loss:  9.20636192e-03\n",
      "Epoch: 3608 mean train loss:  1.56921618e-02, mean val. rec. loss:  9.20418468e-03\n",
      "Epoch: 3609 mean train loss:  1.56890147e-02, mean val. rec. loss:  9.20210384e-03\n",
      "Epoch: 3610 mean train loss:  1.56858527e-02, mean val. rec. loss:  9.19985516e-03\n",
      "Epoch: 3611 mean train loss:  1.56826832e-02, mean val. rec. loss:  9.19779416e-03\n",
      "Epoch: 3612 mean train loss:  1.56794933e-02, mean val. rec. loss:  9.19548992e-03\n",
      "Epoch: 3613 mean train loss:  1.56762940e-02, mean val. rec. loss:  9.19347654e-03\n",
      "Epoch: 3614 mean train loss:  1.56730836e-02, mean val. rec. loss:  9.19108045e-03\n",
      "Epoch: 3615 mean train loss:  1.56698676e-02, mean val. rec. loss:  9.18914929e-03\n",
      "Epoch: 3616 mean train loss:  1.56666274e-02, mean val. rec. loss:  9.18659841e-03\n",
      "Epoch: 3617 mean train loss:  1.56633723e-02, mean val. rec. loss:  9.18480899e-03\n",
      "Epoch: 3618 mean train loss:  1.56601246e-02, mean val. rec. loss:  9.18199956e-03\n",
      "Epoch: 3619 mean train loss:  1.56568435e-02, mean val. rec. loss:  9.18053844e-03\n",
      "Epoch: 3620 mean train loss:  1.56535716e-02, mean val. rec. loss:  9.17727769e-03\n",
      "Epoch: 3621 mean train loss:  1.56502811e-02, mean val. rec. loss:  9.17638355e-03\n",
      "Epoch: 3622 mean train loss:  1.56469813e-02, mean val. rec. loss:  9.17235396e-03\n",
      "Epoch: 3623 mean train loss:  1.56436815e-02, mean val. rec. loss:  9.17251839e-03\n",
      "Epoch: 3624 mean train loss:  1.56403966e-02, mean val. rec. loss:  9.16713483e-03\n",
      "Epoch: 3625 mean train loss:  1.56371378e-02, mean val. rec. loss:  9.16932851e-03\n",
      "Epoch: 3626 mean train loss:  1.56339385e-02, mean val. rec. loss:  9.16151541e-03\n",
      "Epoch: 3627 mean train loss:  1.56308622e-02, mean val. rec. loss:  9.16764229e-03\n",
      "Epoch: 3628 mean train loss:  1.56280186e-02, mean val. rec. loss:  9.15568564e-03\n",
      "Epoch: 3629 mean train loss:  1.56256425e-02, mean val. rec. loss:  9.16963072e-03\n",
      "Epoch: 3630 mean train loss:  1.56240652e-02, mean val. rec. loss:  9.15117751e-03\n",
      "Epoch: 3631 mean train loss:  1.56237970e-02, mean val. rec. loss:  9.17927349e-03\n",
      "Epoch: 3632 mean train loss:  1.56252514e-02, mean val. rec. loss:  9.15095412e-03\n",
      "Epoch: 3633 mean train loss:  1.56280428e-02, mean val. rec. loss:  9.19204149e-03\n",
      "Epoch: 3634 mean train loss:  1.56293836e-02, mean val. rec. loss:  9.14761626e-03\n",
      "Epoch: 3635 mean train loss:  1.56245419e-02, mean val. rec. loss:  9.17195650e-03\n",
      "Epoch: 3636 mean train loss:  1.56118241e-02, mean val. rec. loss:  9.13700111e-03\n",
      "Epoch: 3637 mean train loss:  1.55983688e-02, mean val. rec. loss:  9.13603893e-03\n",
      "Epoch: 3638 mean train loss:  1.55930411e-02, mean val. rec. loss:  9.15312398e-03\n",
      "Epoch: 3639 mean train loss:  1.55953604e-02, mean val. rec. loss:  9.12967618e-03\n",
      "Epoch: 3640 mean train loss:  1.55967925e-02, mean val. rec. loss:  9.15271745e-03\n",
      "Epoch: 3641 mean train loss:  1.55908754e-02, mean val. rec. loss:  9.12393713e-03\n",
      "Epoch: 3642 mean train loss:  1.55809825e-02, mean val. rec. loss:  9.12441226e-03\n",
      "Epoch: 3643 mean train loss:  1.55753065e-02, mean val. rec. loss:  9.13552297e-03\n",
      "Epoch: 3644 mean train loss:  1.55753149e-02, mean val. rec. loss:  9.11633835e-03\n",
      "Epoch: 3645 mean train loss:  1.55746966e-02, mean val. rec. loss:  9.13258540e-03\n",
      "Epoch: 3646 mean train loss:  1.55690551e-02, mean val. rec. loss:  9.11331517e-03\n",
      "Epoch: 3647 mean train loss:  1.55617526e-02, mean val. rec. loss:  9.11091341e-03\n",
      "Epoch: 3648 mean train loss:  1.55580058e-02, mean val. rec. loss:  9.12149171e-03\n",
      "Epoch: 3649 mean train loss:  1.55570440e-02, mean val. rec. loss:  9.10337190e-03\n",
      "Epoch: 3650 mean train loss:  1.55542414e-02, mean val. rec. loss:  9.11321765e-03\n",
      "Epoch: 3651 mean train loss:  1.55484900e-02, mean val. rec. loss:  9.10289846e-03\n",
      "Epoch: 3652 mean train loss:  1.55432219e-02, mean val. rec. loss:  9.09758521e-03\n",
      "Epoch: 3653 mean train loss:  1.55405394e-02, mean val. rec. loss:  9.10754436e-03\n",
      "Epoch: 3654 mean train loss:  1.55383430e-02, mean val. rec. loss:  9.09186827e-03\n",
      "Epoch: 3655 mean train loss:  1.55341977e-02, mean val. rec. loss:  9.09669674e-03\n",
      "Epoch: 3656 mean train loss:  1.55290097e-02, mean val. rec. loss:  9.09301926e-03\n",
      "Epoch: 3657 mean train loss:  1.55251047e-02, mean val. rec. loss:  9.08529971e-03\n",
      "Epoch: 3658 mean train loss:  1.55224268e-02, mean val. rec. loss:  9.09292740e-03\n",
      "Epoch: 3659 mean train loss:  1.55191000e-02, mean val. rec. loss:  9.08057386e-03\n",
      "Epoch: 3660 mean train loss:  1.55145591e-02, mean val. rec. loss:  9.08132456e-03\n",
      "Epoch: 3661 mean train loss:  1.55101550e-02, mean val. rec. loss:  9.08128997e-03\n",
      "Epoch: 3662 mean train loss:  1.55067677e-02, mean val. rec. loss:  9.07250335e-03\n",
      "Epoch: 3663 mean train loss:  1.55035740e-02, mean val. rec. loss:  9.07744069e-03\n",
      "Epoch: 3664 mean train loss:  1.54996187e-02, mean val. rec. loss:  9.06876520e-03\n",
      "Epoch: 3665 mean train loss:  1.54952267e-02, mean val. rec. loss:  9.06683233e-03\n",
      "Epoch: 3666 mean train loss:  1.54913096e-02, mean val. rec. loss:  9.06829573e-03\n",
      "Epoch: 3667 mean train loss:  1.54878785e-02, mean val. rec. loss:  9.05982379e-03\n",
      "Epoch: 3668 mean train loss:  1.54842212e-02, mean val. rec. loss:  9.06235029e-03\n",
      "Epoch: 3669 mean train loss:  1.54800620e-02, mean val. rec. loss:  9.05657552e-03\n",
      "Epoch: 3670 mean train loss:  1.54759084e-02, mean val. rec. loss:  9.05289066e-03\n",
      "Epoch: 3671 mean train loss:  1.54721346e-02, mean val. rec. loss:  9.05431437e-03\n",
      "Epoch: 3672 mean train loss:  1.54684652e-02, mean val. rec. loss:  9.04673714e-03\n",
      "Epoch: 3673 mean train loss:  1.54645248e-02, mean val. rec. loss:  9.04738690e-03\n",
      "Epoch: 3674 mean train loss:  1.54603665e-02, mean val. rec. loss:  9.04357221e-03\n",
      "Epoch: 3675 mean train loss:  1.54563302e-02, mean val. rec. loss:  9.03914516e-03\n",
      "Epoch: 3676 mean train loss:  1.54525016e-02, mean val. rec. loss:  9.04003307e-03\n",
      "Epoch: 3677 mean train loss:  1.54486189e-02, mean val. rec. loss:  9.03359718e-03\n",
      "Epoch: 3678 mean train loss:  1.54445426e-02, mean val. rec. loss:  9.03286860e-03\n",
      "Epoch: 3679 mean train loss:  1.54403992e-02, mean val. rec. loss:  9.02998944e-03\n",
      "Epoch: 3680 mean train loss:  1.54363703e-02, mean val. rec. loss:  9.02524828e-03\n",
      "Epoch: 3681 mean train loss:  1.54324160e-02, mean val. rec. loss:  9.02529421e-03\n",
      "Epoch: 3682 mean train loss:  1.54283843e-02, mean val. rec. loss:  9.01977457e-03\n",
      "Epoch: 3683 mean train loss:  1.54242279e-02, mean val. rec. loss:  9.01814108e-03\n",
      "Epoch: 3684 mean train loss:  1.54200585e-02, mean val. rec. loss:  9.01571607e-03\n",
      "Epoch: 3685 mean train loss:  1.54159728e-02, mean val. rec. loss:  9.01112006e-03\n",
      "Epoch: 3686 mean train loss:  1.54118993e-02, mean val. rec. loss:  9.01050885e-03\n",
      "Epoch: 3687 mean train loss:  1.54077494e-02, mean val. rec. loss:  9.00571553e-03\n",
      "Epoch: 3688 mean train loss:  1.54035250e-02, mean val. rec. loss:  9.00352015e-03\n",
      "Epoch: 3689 mean train loss:  1.53993155e-02, mean val. rec. loss:  9.00112520e-03\n",
      "Epoch: 3690 mean train loss:  1.53951312e-02, mean val. rec. loss:  8.99671459e-03\n",
      "Epoch: 3691 mean train loss:  1.53909450e-02, mean val. rec. loss:  8.99549614e-03\n",
      "Epoch: 3692 mean train loss:  1.53867029e-02, mean val. rec. loss:  8.99114280e-03\n",
      "Epoch: 3693 mean train loss:  1.53824162e-02, mean val. rec. loss:  8.98863841e-03\n",
      "Epoch: 3694 mean train loss:  1.53781257e-02, mean val. rec. loss:  8.98613856e-03\n",
      "Epoch: 3695 mean train loss:  1.53738454e-02, mean val. rec. loss:  8.98199841e-03\n",
      "Epoch: 3696 mean train loss:  1.53695522e-02, mean val. rec. loss:  8.98031105e-03\n",
      "Epoch: 3697 mean train loss:  1.53652170e-02, mean val. rec. loss:  8.97621286e-03\n",
      "Epoch: 3698 mean train loss:  1.53608473e-02, mean val. rec. loss:  8.97352194e-03\n",
      "Epoch: 3699 mean train loss:  1.53564684e-02, mean val. rec. loss:  8.97079699e-03\n",
      "Epoch: 3700 mean train loss:  1.53520988e-02, mean val. rec. loss:  8.96685415e-03\n",
      "Epoch: 3701 mean train loss:  1.53477068e-02, mean val. rec. loss:  8.96478975e-03\n",
      "Epoch: 3702 mean train loss:  1.53432776e-02, mean val. rec. loss:  8.96081403e-03\n",
      "Epoch: 3703 mean train loss:  1.53388223e-02, mean val. rec. loss:  8.95801367e-03\n",
      "Epoch: 3704 mean train loss:  1.53343568e-02, mean val. rec. loss:  8.95506080e-03\n",
      "Epoch: 3705 mean train loss:  1.53298726e-02, mean val. rec. loss:  8.95129600e-03\n",
      "Epoch: 3706 mean train loss:  1.53253791e-02, mean val. rec. loss:  8.94890614e-03\n",
      "Epoch: 3707 mean train loss:  1.53208587e-02, mean val. rec. loss:  8.94499846e-03\n",
      "Epoch: 3708 mean train loss:  1.53163084e-02, mean val. rec. loss:  8.94214367e-03\n",
      "Epoch: 3709 mean train loss:  1.53117349e-02, mean val. rec. loss:  8.93895890e-03\n",
      "Epoch: 3710 mean train loss:  1.53071576e-02, mean val. rec. loss:  8.93535172e-03\n",
      "Epoch: 3711 mean train loss:  1.53025617e-02, mean val. rec. loss:  8.93267440e-03\n",
      "Epoch: 3712 mean train loss:  1.52979453e-02, mean val. rec. loss:  8.92883419e-03\n",
      "Epoch: 3713 mean train loss:  1.52932992e-02, mean val. rec. loss:  8.92592440e-03\n",
      "Epoch: 3714 mean train loss:  1.52886307e-02, mean val. rec. loss:  8.92253722e-03\n",
      "Epoch: 3715 mean train loss:  1.52839417e-02, mean val. rec. loss:  8.91901678e-03\n",
      "Epoch: 3716 mean train loss:  1.52792331e-02, mean val. rec. loss:  8.91607128e-03\n",
      "Epoch: 3717 mean train loss:  1.52745162e-02, mean val. rec. loss:  8.91227529e-03\n",
      "Epoch: 3718 mean train loss:  1.52697667e-02, mean val. rec. loss:  8.90928897e-03\n",
      "Epoch: 3719 mean train loss:  1.52649911e-02, mean val. rec. loss:  8.90571467e-03\n",
      "Epoch: 3720 mean train loss:  1.52602043e-02, mean val. rec. loss:  8.90227589e-03\n",
      "Epoch: 3721 mean train loss:  1.52553878e-02, mean val. rec. loss:  8.89907127e-03\n",
      "Epoch: 3722 mean train loss:  1.52505545e-02, mean val. rec. loss:  8.89530704e-03\n",
      "Epoch: 3723 mean train loss:  1.52456997e-02, mean val. rec. loss:  8.89217839e-03\n",
      "Epoch: 3724 mean train loss:  1.52408226e-02, mean val. rec. loss:  8.88846859e-03\n",
      "Epoch: 3725 mean train loss:  1.52359288e-02, mean val. rec. loss:  8.88509501e-03\n",
      "Epoch: 3726 mean train loss:  1.52310052e-02, mean val. rec. loss:  8.88164375e-03\n",
      "Epoch: 3727 mean train loss:  1.52260601e-02, mean val. rec. loss:  8.87794755e-03\n",
      "Epoch: 3728 mean train loss:  1.52211002e-02, mean val. rec. loss:  8.87468284e-03\n",
      "Epoch: 3729 mean train loss:  1.52161067e-02, mean val. rec. loss:  8.87089252e-03\n",
      "Epoch: 3730 mean train loss:  1.52110891e-02, mean val. rec. loss:  8.86750646e-03\n",
      "Epoch: 3731 mean train loss:  1.52060611e-02, mean val. rec. loss:  8.86384259e-03\n",
      "Epoch: 3732 mean train loss:  1.52010034e-02, mean val. rec. loss:  8.86020252e-03\n",
      "Epoch: 3733 mean train loss:  1.51959196e-02, mean val. rec. loss:  8.85670080e-03\n",
      "Epoch: 3734 mean train loss:  1.51908182e-02, mean val. rec. loss:  8.85288100e-03\n",
      "Epoch: 3735 mean train loss:  1.51856925e-02, mean val. rec. loss:  8.84940253e-03\n",
      "Epoch: 3736 mean train loss:  1.51805435e-02, mean val. rec. loss:  8.84558443e-03\n",
      "Epoch: 3737 mean train loss:  1.51753620e-02, mean val. rec. loss:  8.84196081e-03\n",
      "Epoch: 3738 mean train loss:  1.51701637e-02, mean val. rec. loss:  8.83823853e-03\n",
      "Epoch: 3739 mean train loss:  1.51649393e-02, mean val. rec. loss:  8.83442667e-03\n",
      "Epoch: 3740 mean train loss:  1.51596917e-02, mean val. rec. loss:  8.83078830e-03\n",
      "Epoch: 3741 mean train loss:  1.51544217e-02, mean val. rec. loss:  8.82688742e-03\n",
      "Epoch: 3742 mean train loss:  1.51491237e-02, mean val. rec. loss:  8.82321901e-03\n",
      "Epoch: 3743 mean train loss:  1.51438081e-02, mean val. rec. loss:  8.81933627e-03\n",
      "Epoch: 3744 mean train loss:  1.51384618e-02, mean val. rec. loss:  8.81553178e-03\n",
      "Epoch: 3745 mean train loss:  1.51330819e-02, mean val. rec. loss:  8.81170178e-03\n",
      "Epoch: 3746 mean train loss:  1.51276899e-02, mean val. rec. loss:  8.80775837e-03\n",
      "Epoch: 3747 mean train loss:  1.51222709e-02, mean val. rec. loss:  8.80396068e-03\n",
      "Epoch: 3748 mean train loss:  1.51168175e-02, mean val. rec. loss:  8.79996455e-03\n",
      "Epoch: 3749 mean train loss:  1.51113483e-02, mean val. rec. loss:  8.79610336e-03\n",
      "Epoch: 3750 mean train loss:  1.51058464e-02, mean val. rec. loss:  8.79210156e-03\n",
      "Epoch: 3751 mean train loss:  1.51003297e-02, mean val. rec. loss:  8.78814795e-03\n",
      "Epoch: 3752 mean train loss:  1.50947738e-02, mean val. rec. loss:  8.78417846e-03\n",
      "Epoch: 3753 mean train loss:  1.50892003e-02, mean val. rec. loss:  8.78013017e-03\n",
      "Epoch: 3754 mean train loss:  1.50835942e-02, mean val. rec. loss:  8.77616465e-03\n",
      "Epoch: 3755 mean train loss:  1.50779648e-02, mean val. rec. loss:  8.77203131e-03\n",
      "Epoch: 3756 mean train loss:  1.50723121e-02, mean val. rec. loss:  8.76801079e-03\n",
      "Epoch: 3757 mean train loss:  1.50666287e-02, mean val. rec. loss:  8.76386384e-03\n",
      "Epoch: 3758 mean train loss:  1.50609174e-02, mean val. rec. loss:  8.75977812e-03\n",
      "Epoch: 3759 mean train loss:  1.50551809e-02, mean val. rec. loss:  8.75564081e-03\n",
      "Epoch: 3760 mean train loss:  1.50494174e-02, mean val. rec. loss:  8.75148252e-03\n",
      "Epoch: 3761 mean train loss:  1.50436288e-02, mean val. rec. loss:  8.74734464e-03\n",
      "Epoch: 3762 mean train loss:  1.50378048e-02, mean val. rec. loss:  8.74309279e-03\n",
      "Epoch: 3763 mean train loss:  1.50319612e-02, mean val. rec. loss:  8.73891579e-03\n",
      "Epoch: 3764 mean train loss:  1.50260804e-02, mean val. rec. loss:  8.73463162e-03\n",
      "Epoch: 3765 mean train loss:  1.50201801e-02, mean val. rec. loss:  8.73039565e-03\n",
      "Epoch: 3766 mean train loss:  1.50142415e-02, mean val. rec. loss:  8.72606840e-03\n",
      "Epoch: 3767 mean train loss:  1.50082872e-02, mean val. rec. loss:  8.72174908e-03\n",
      "Epoch: 3768 mean train loss:  1.50022909e-02, mean val. rec. loss:  8.71742183e-03\n",
      "Epoch: 3769 mean train loss:  1.49962714e-02, mean val. rec. loss:  8.71305318e-03\n",
      "Epoch: 3770 mean train loss:  1.49902212e-02, mean val. rec. loss:  8.70869928e-03\n",
      "Epoch: 3771 mean train loss:  1.49841458e-02, mean val. rec. loss:  8.70425126e-03\n",
      "Epoch: 3772 mean train loss:  1.49780387e-02, mean val. rec. loss:  8.69985370e-03\n",
      "Epoch: 3773 mean train loss:  1.49719019e-02, mean val. rec. loss:  8.69537562e-03\n",
      "Epoch: 3774 mean train loss:  1.49657361e-02, mean val. rec. loss:  8.69093100e-03\n",
      "Epoch: 3775 mean train loss:  1.49595434e-02, mean val. rec. loss:  8.68640360e-03\n",
      "Epoch: 3776 mean train loss:  1.49533135e-02, mean val. rec. loss:  8.68188867e-03\n",
      "Epoch: 3777 mean train loss:  1.49470584e-02, mean val. rec. loss:  8.67733179e-03\n",
      "Epoch: 3778 mean train loss:  1.49407670e-02, mean val. rec. loss:  8.67276356e-03\n",
      "Epoch: 3779 mean train loss:  1.49344467e-02, mean val. rec. loss:  8.66819080e-03\n",
      "Epoch: 3780 mean train loss:  1.49280994e-02, mean val. rec. loss:  8.66354943e-03\n",
      "Epoch: 3781 mean train loss:  1.49217177e-02, mean val. rec. loss:  8.65891317e-03\n",
      "Epoch: 3782 mean train loss:  1.49153043e-02, mean val. rec. loss:  8.65421681e-03\n",
      "Epoch: 3783 mean train loss:  1.49088630e-02, mean val. rec. loss:  8.64955957e-03\n",
      "Epoch: 3784 mean train loss:  1.49023900e-02, mean val. rec. loss:  8.64480594e-03\n",
      "Epoch: 3785 mean train loss:  1.48958854e-02, mean val. rec. loss:  8.64010163e-03\n",
      "Epoch: 3786 mean train loss:  1.48893416e-02, mean val. rec. loss:  8.63529471e-03\n",
      "Epoch: 3787 mean train loss:  1.48827746e-02, mean val. rec. loss:  8.63051329e-03\n",
      "Epoch: 3788 mean train loss:  1.48761657e-02, mean val. rec. loss:  8.62566498e-03\n",
      "Epoch: 3789 mean train loss:  1.48695354e-02, mean val. rec. loss:  8.62083821e-03\n",
      "Epoch: 3790 mean train loss:  1.48628650e-02, mean val. rec. loss:  8.61595474e-03\n",
      "Epoch: 3791 mean train loss:  1.48561546e-02, mean val. rec. loss:  8.61104462e-03\n",
      "Epoch: 3792 mean train loss:  1.48494181e-02, mean val. rec. loss:  8.60610558e-03\n",
      "Epoch: 3793 mean train loss:  1.48426584e-02, mean val. rec. loss:  8.60115577e-03\n",
      "Epoch: 3794 mean train loss:  1.48358586e-02, mean val. rec. loss:  8.59620086e-03\n",
      "Epoch: 3795 mean train loss:  1.48290160e-02, mean val. rec. loss:  8.59118188e-03\n",
      "Epoch: 3796 mean train loss:  1.48221519e-02, mean val. rec. loss:  8.58614306e-03\n",
      "Epoch: 3797 mean train loss:  1.48152441e-02, mean val. rec. loss:  8.58105717e-03\n",
      "Epoch: 3798 mean train loss:  1.48083093e-02, mean val. rec. loss:  8.57598943e-03\n",
      "Epoch: 3799 mean train loss:  1.48013336e-02, mean val. rec. loss:  8.57085762e-03\n",
      "Epoch: 3800 mean train loss:  1.47943290e-02, mean val. rec. loss:  8.56573091e-03\n",
      "Epoch: 3801 mean train loss:  1.47872880e-02, mean val. rec. loss:  8.56052539e-03\n",
      "Epoch: 3802 mean train loss:  1.47802024e-02, mean val. rec. loss:  8.55533291e-03\n",
      "Epoch: 3803 mean train loss:  1.47730888e-02, mean val. rec. loss:  8.55007013e-03\n",
      "Epoch: 3804 mean train loss:  1.47659492e-02, mean val. rec. loss:  8.54488445e-03\n",
      "Epoch: 3805 mean train loss:  1.47587639e-02, mean val. rec. loss:  8.53954116e-03\n",
      "Epoch: 3806 mean train loss:  1.47515423e-02, mean val. rec. loss:  8.53426987e-03\n",
      "Epoch: 3807 mean train loss:  1.47442900e-02, mean val. rec. loss:  8.52884209e-03\n",
      "Epoch: 3808 mean train loss:  1.47369958e-02, mean val. rec. loss:  8.52357080e-03\n",
      "Epoch: 3809 mean train loss:  1.47296700e-02, mean val. rec. loss:  8.51806421e-03\n",
      "Epoch: 3810 mean train loss:  1.47223069e-02, mean val. rec. loss:  8.51277591e-03\n",
      "Epoch: 3811 mean train loss:  1.47148972e-02, mean val. rec. loss:  8.50711056e-03\n",
      "Epoch: 3812 mean train loss:  1.47074624e-02, mean val. rec. loss:  8.50184777e-03\n",
      "Epoch: 3813 mean train loss:  1.46999941e-02, mean val. rec. loss:  8.49603955e-03\n",
      "Epoch: 3814 mean train loss:  1.46924792e-02, mean val. rec. loss:  8.49089980e-03\n",
      "Epoch: 3815 mean train loss:  1.46849308e-02, mean val. rec. loss:  8.48479107e-03\n",
      "Epoch: 3816 mean train loss:  1.46773499e-02, mean val. rec. loss:  8.47985430e-03\n",
      "Epoch: 3817 mean train loss:  1.46697279e-02, mean val. rec. loss:  8.47331239e-03\n",
      "Epoch: 3818 mean train loss:  1.46620883e-02, mean val. rec. loss:  8.46892730e-03\n",
      "Epoch: 3819 mean train loss:  1.46544207e-02, mean val. rec. loss:  8.46161088e-03\n",
      "Epoch: 3820 mean train loss:  1.46467429e-02, mean val. rec. loss:  8.45829627e-03\n",
      "Epoch: 3821 mean train loss:  1.46390958e-02, mean val. rec. loss:  8.44959810e-03\n",
      "Epoch: 3822 mean train loss:  1.46315437e-02, mean val. rec. loss:  8.44874705e-03\n",
      "Epoch: 3823 mean train loss:  1.46241992e-02, mean val. rec. loss:  8.43777696e-03\n",
      "Epoch: 3824 mean train loss:  1.46173082e-02, mean val. rec. loss:  8.44254533e-03\n",
      "Epoch: 3825 mean train loss:  1.46113576e-02, mean val. rec. loss:  8.42830712e-03\n",
      "Epoch: 3826 mean train loss:  1.46071620e-02, mean val. rec. loss:  8.44591381e-03\n",
      "Epoch: 3827 mean train loss:  1.46058241e-02, mean val. rec. loss:  8.42722418e-03\n",
      "Epoch: 3828 mean train loss:  1.46078203e-02, mean val. rec. loss:  8.46077968e-03\n",
      "Epoch: 3829 mean train loss:  1.46098557e-02, mean val. rec. loss:  8.42238777e-03\n",
      "Epoch: 3830 mean train loss:  1.46028976e-02, mean val. rec. loss:  8.43396794e-03\n",
      "Epoch: 3831 mean train loss:  1.45804731e-02, mean val. rec. loss:  8.39094260e-03\n",
      "Epoch: 3832 mean train loss:  1.45549546e-02, mean val. rec. loss:  8.38469212e-03\n",
      "Epoch: 3833 mean train loss:  1.45449425e-02, mean val. rec. loss:  8.40401508e-03\n",
      "Epoch: 3834 mean train loss:  1.45482470e-02, mean val. rec. loss:  8.37829195e-03\n",
      "Epoch: 3835 mean train loss:  1.45459518e-02, mean val. rec. loss:  8.38623036e-03\n",
      "Epoch: 3836 mean train loss:  1.45290944e-02, mean val. rec. loss:  8.35882271e-03\n",
      "Epoch: 3837 mean train loss:  1.45114901e-02, mean val. rec. loss:  8.35271624e-03\n",
      "Epoch: 3838 mean train loss:  1.45064269e-02, mean val. rec. loss:  8.37051854e-03\n",
      "Epoch: 3839 mean train loss:  1.45055181e-02, mean val. rec. loss:  8.34192192e-03\n",
      "Epoch: 3840 mean train loss:  1.44952649e-02, mean val. rec. loss:  8.34104876e-03\n",
      "Epoch: 3841 mean train loss:  1.44796150e-02, mean val. rec. loss:  8.33401810e-03\n",
      "Epoch: 3842 mean train loss:  1.44709782e-02, mean val. rec. loss:  8.32182445e-03\n",
      "Epoch: 3843 mean train loss:  1.44678171e-02, mean val. rec. loss:  8.33137651e-03\n",
      "Epoch: 3844 mean train loss:  1.44595965e-02, mean val. rec. loss:  8.30750913e-03\n",
      "Epoch: 3845 mean train loss:  1.44463321e-02, mean val. rec. loss:  8.30190445e-03\n",
      "Epoch: 3846 mean train loss:  1.44367660e-02, mean val. rec. loss:  8.30764861e-03\n",
      "Epoch: 3847 mean train loss:  1.44316441e-02, mean val. rec. loss:  8.28986049e-03\n",
      "Epoch: 3848 mean train loss:  1.44238098e-02, mean val. rec. loss:  8.29086066e-03\n",
      "Epoch: 3849 mean train loss:  1.44121395e-02, mean val. rec. loss:  8.28080228e-03\n",
      "Epoch: 3850 mean train loss:  1.44024524e-02, mean val. rec. loss:  8.27052959e-03\n",
      "Epoch: 3851 mean train loss:  1.43959477e-02, mean val. rec. loss:  8.27522312e-03\n",
      "Epoch: 3852 mean train loss:  1.43880120e-02, mean val. rec. loss:  8.25815734e-03\n",
      "Epoch: 3853 mean train loss:  1.43773454e-02, mean val. rec. loss:  8.25294331e-03\n",
      "Epoch: 3854 mean train loss:  1.43676620e-02, mean val. rec. loss:  8.25158424e-03\n",
      "Epoch: 3855 mean train loss:  1.43602011e-02, mean val. rec. loss:  8.23729954e-03\n",
      "Epoch: 3856 mean train loss:  1.43520559e-02, mean val. rec. loss:  8.23692986e-03\n",
      "Epoch: 3857 mean train loss:  1.43420047e-02, mean val. rec. loss:  8.22702741e-03\n",
      "Epoch: 3858 mean train loss:  1.43323213e-02, mean val. rec. loss:  8.21817162e-03\n",
      "Epoch: 3859 mean train loss:  1.43241519e-02, mean val. rec. loss:  8.21821415e-03\n",
      "Epoch: 3860 mean train loss:  1.43157646e-02, mean val. rec. loss:  8.20439551e-03\n",
      "Epoch: 3861 mean train loss:  1.43060794e-02, mean val. rec. loss:  8.19973316e-03\n",
      "Epoch: 3862 mean train loss:  1.42963792e-02, mean val. rec. loss:  8.19499144e-03\n",
      "Epoch: 3863 mean train loss:  1.42876828e-02, mean val. rec. loss:  8.18356096e-03\n",
      "Epoch: 3864 mean train loss:  1.42790553e-02, mean val. rec. loss:  8.18134006e-03\n",
      "Epoch: 3865 mean train loss:  1.42695683e-02, mean val. rec. loss:  8.17085985e-03\n",
      "Epoch: 3866 mean train loss:  1.42598468e-02, mean val. rec. loss:  8.16358369e-03\n",
      "Epoch: 3867 mean train loss:  1.42507211e-02, mean val. rec. loss:  8.16068411e-03\n",
      "Epoch: 3868 mean train loss:  1.42418198e-02, mean val. rec. loss:  8.14924796e-03\n",
      "Epoch: 3869 mean train loss:  1.42324288e-02, mean val. rec. loss:  8.14476988e-03\n",
      "Epoch: 3870 mean train loss:  1.42226858e-02, mean val. rec. loss:  8.13728110e-03\n",
      "Epoch: 3871 mean train loss:  1.42132343e-02, mean val. rec. loss:  8.12827052e-03\n",
      "Epoch: 3872 mean train loss:  1.42040444e-02, mean val. rec. loss:  8.12467922e-03\n",
      "Epoch: 3873 mean train loss:  1.41946403e-02, mean val. rec. loss:  8.11417293e-03\n",
      "Epoch: 3874 mean train loss:  1.41849020e-02, mean val. rec. loss:  8.10779204e-03\n",
      "Epoch: 3875 mean train loss:  1.41752037e-02, mean val. rec. loss:  8.10168558e-03\n",
      "Epoch: 3876 mean train loss:  1.41657205e-02, mean val. rec. loss:  8.09209837e-03\n",
      "Epoch: 3877 mean train loss:  1.41562047e-02, mean val. rec. loss:  8.08749103e-03\n",
      "Epoch: 3878 mean train loss:  1.41464505e-02, mean val. rec. loss:  8.07832453e-03\n",
      "Epoch: 3879 mean train loss:  1.41365940e-02, mean val. rec. loss:  8.07089017e-03\n",
      "Epoch: 3880 mean train loss:  1.41268342e-02, mean val. rec. loss:  8.06512504e-03\n",
      "Epoch: 3881 mean train loss:  1.41171443e-02, mean val. rec. loss:  8.05555995e-03\n",
      "Epoch: 3882 mean train loss:  1.41073334e-02, mean val. rec. loss:  8.04989176e-03\n",
      "Epoch: 3883 mean train loss:  1.40973818e-02, mean val. rec. loss:  8.04145101e-03\n",
      "Epoch: 3884 mean train loss:  1.40874051e-02, mean val. rec. loss:  8.03340148e-03\n",
      "Epoch: 3885 mean train loss:  1.40774955e-02, mean val. rec. loss:  8.02742088e-03\n",
      "Epoch: 3886 mean train loss:  1.40675681e-02, mean val. rec. loss:  8.01817274e-03\n",
      "Epoch: 3887 mean train loss:  1.40575328e-02, mean val. rec. loss:  8.01182360e-03\n",
      "Epoch: 3888 mean train loss:  1.40474192e-02, mean val. rec. loss:  8.00378371e-03\n",
      "Epoch: 3889 mean train loss:  1.40372908e-02, mean val. rec. loss:  7.99552609e-03\n",
      "Epoch: 3890 mean train loss:  1.40271958e-02, mean val. rec. loss:  7.98914861e-03\n",
      "Epoch: 3891 mean train loss:  1.40170544e-02, mean val. rec. loss:  7.98007282e-03\n",
      "Epoch: 3892 mean train loss:  1.40068291e-02, mean val. rec. loss:  7.97315273e-03\n",
      "Epoch: 3893 mean train loss:  1.39965442e-02, mean val. rec. loss:  7.96507939e-03\n",
      "Epoch: 3894 mean train loss:  1.39862593e-02, mean val. rec. loss:  7.95677584e-03\n",
      "Epoch: 3895 mean train loss:  1.39759679e-02, mean val. rec. loss:  7.94998105e-03\n",
      "Epoch: 3896 mean train loss:  1.39656244e-02, mean val. rec. loss:  7.94108614e-03\n",
      "Epoch: 3897 mean train loss:  1.39552352e-02, mean val. rec. loss:  7.93390524e-03\n",
      "Epoch: 3898 mean train loss:  1.39447976e-02, mean val. rec. loss:  7.92574004e-03\n",
      "Epoch: 3899 mean train loss:  1.39343303e-02, mean val. rec. loss:  7.91752891e-03\n",
      "Epoch: 3900 mean train loss:  1.39238564e-02, mean val. rec. loss:  7.91030208e-03\n",
      "Epoch: 3901 mean train loss:  1.39133443e-02, mean val. rec. loss:  7.90146046e-03\n",
      "Epoch: 3902 mean train loss:  1.39027903e-02, mean val. rec. loss:  7.89407884e-03\n",
      "Epoch: 3903 mean train loss:  1.38921945e-02, mean val. rec. loss:  7.88572597e-03\n",
      "Epoch: 3904 mean train loss:  1.38815744e-02, mean val. rec. loss:  7.87764072e-03\n",
      "Epoch: 3905 mean train loss:  1.38709292e-02, mean val. rec. loss:  7.87002096e-03\n",
      "Epoch: 3906 mean train loss:  1.38602523e-02, mean val. rec. loss:  7.86129728e-03\n",
      "Epoch: 3907 mean train loss:  1.38495485e-02, mean val. rec. loss:  7.85376087e-03\n",
      "Epoch: 3908 mean train loss:  1.38388101e-02, mean val. rec. loss:  7.84521806e-03\n",
      "Epoch: 3909 mean train loss:  1.38280411e-02, mean val. rec. loss:  7.83722239e-03\n",
      "Epoch: 3910 mean train loss:  1.38172376e-02, mean val. rec. loss:  7.82914508e-03\n",
      "Epoch: 3911 mean train loss:  1.38064155e-02, mean val. rec. loss:  7.82054330e-03\n",
      "Epoch: 3912 mean train loss:  1.37955691e-02, mean val. rec. loss:  7.81279710e-03\n",
      "Epoch: 3913 mean train loss:  1.37846977e-02, mean val. rec. loss:  7.80415110e-03\n",
      "Epoch: 3914 mean train loss:  1.37737815e-02, mean val. rec. loss:  7.79621893e-03\n",
      "Epoch: 3915 mean train loss:  1.37628542e-02, mean val. rec. loss:  7.78779235e-03\n",
      "Epoch: 3916 mean train loss:  1.37519064e-02, mean val. rec. loss:  7.77943097e-03\n",
      "Epoch: 3917 mean train loss:  1.37409306e-02, mean val. rec. loss:  7.77139335e-03\n",
      "Epoch: 3918 mean train loss:  1.37299279e-02, mean val. rec. loss:  7.76276322e-03\n",
      "Epoch: 3919 mean train loss:  1.37189121e-02, mean val. rec. loss:  7.75476585e-03\n",
      "Epoch: 3920 mean train loss:  1.37078711e-02, mean val. rec. loss:  7.74609773e-03\n",
      "Epoch: 3921 mean train loss:  1.36968004e-02, mean val. rec. loss:  7.73793990e-03\n",
      "Epoch: 3922 mean train loss:  1.36857129e-02, mean val. rec. loss:  7.72955471e-03\n",
      "Epoch: 3923 mean train loss:  1.36746189e-02, mean val. rec. loss:  7.72111509e-03\n",
      "Epoch: 3924 mean train loss:  1.36634933e-02, mean val. rec. loss:  7.71292211e-03\n",
      "Epoch: 3925 mean train loss:  1.36523592e-02, mean val. rec. loss:  7.70429255e-03\n",
      "Epoch: 3926 mean train loss:  1.36412019e-02, mean val. rec. loss:  7.69618462e-03\n",
      "Epoch: 3927 mean train loss:  1.36300353e-02, mean val. rec. loss:  7.68756923e-03\n",
      "Epoch: 3928 mean train loss:  1.36188519e-02, mean val. rec. loss:  7.67931672e-03\n",
      "Epoch: 3929 mean train loss:  1.36076508e-02, mean val. rec. loss:  7.67079715e-03\n",
      "Epoch: 3930 mean train loss:  1.35964488e-02, mean val. rec. loss:  7.66240629e-03\n",
      "Epoch: 3931 mean train loss:  1.35852226e-02, mean val. rec. loss:  7.65407213e-03\n",
      "Epoch: 3932 mean train loss:  1.35740001e-02, mean val. rec. loss:  7.64553102e-03\n",
      "Epoch: 3933 mean train loss:  1.35627581e-02, mean val. rec. loss:  7.63727340e-03\n",
      "Epoch: 3934 mean train loss:  1.35515142e-02, mean val. rec. loss:  7.62868920e-03\n",
      "Epoch: 3935 mean train loss:  1.35402563e-02, mean val. rec. loss:  7.62047127e-03\n",
      "Epoch: 3936 mean train loss:  1.35289966e-02, mean val. rec. loss:  7.61191145e-03\n",
      "Epoch: 3937 mean train loss:  1.35177303e-02, mean val. rec. loss:  7.60361924e-03\n",
      "Epoch: 3938 mean train loss:  1.35064622e-02, mean val. rec. loss:  7.59512462e-03\n",
      "Epoch: 3939 mean train loss:  1.34951848e-02, mean val. rec. loss:  7.58679273e-03\n",
      "Epoch: 3940 mean train loss:  1.34839185e-02, mean val. rec. loss:  7.57842455e-03\n",
      "Epoch: 3941 mean train loss:  1.34726364e-02, mean val. rec. loss:  7.57002802e-03\n",
      "Epoch: 3942 mean train loss:  1.34613637e-02, mean val. rec. loss:  7.56171938e-03\n",
      "Epoch: 3943 mean train loss:  1.34500974e-02, mean val. rec. loss:  7.55330470e-03\n",
      "Epoch: 3944 mean train loss:  1.34388302e-02, mean val. rec. loss:  7.54510038e-03\n",
      "Epoch: 3945 mean train loss:  1.34275724e-02, mean val. rec. loss:  7.53666756e-03\n",
      "Epoch: 3946 mean train loss:  1.34163136e-02, mean val. rec. loss:  7.52846891e-03\n",
      "Epoch: 3947 mean train loss:  1.34050678e-02, mean val. rec. loss:  7.52004177e-03\n",
      "Epoch: 3948 mean train loss:  1.33938313e-02, mean val. rec. loss:  7.51193950e-03\n",
      "Epoch: 3949 mean train loss:  1.33825958e-02, mean val. rec. loss:  7.50355715e-03\n",
      "Epoch: 3950 mean train loss:  1.33713845e-02, mean val. rec. loss:  7.49547076e-03\n",
      "Epoch: 3951 mean train loss:  1.33601778e-02, mean val. rec. loss:  7.48709521e-03\n",
      "Epoch: 3952 mean train loss:  1.33489842e-02, mean val. rec. loss:  7.47912109e-03\n",
      "Epoch: 3953 mean train loss:  1.33378185e-02, mean val. rec. loss:  7.47081357e-03\n",
      "Epoch: 3954 mean train loss:  1.33266509e-02, mean val. rec. loss:  7.46289161e-03\n",
      "Epoch: 3955 mean train loss:  1.33155169e-02, mean val. rec. loss:  7.45452684e-03\n",
      "Epoch: 3956 mean train loss:  1.33043978e-02, mean val. rec. loss:  7.44678971e-03\n",
      "Epoch: 3957 mean train loss:  1.32932991e-02, mean val. rec. loss:  7.43843344e-03\n",
      "Epoch: 3958 mean train loss:  1.32822303e-02, mean val. rec. loss:  7.43084884e-03\n",
      "Epoch: 3959 mean train loss:  1.32711707e-02, mean val. rec. loss:  7.42234571e-03\n",
      "Epoch: 3960 mean train loss:  1.32601521e-02, mean val. rec. loss:  7.41511264e-03\n",
      "Epoch: 3961 mean train loss:  1.32491633e-02, mean val. rec. loss:  7.40641164e-03\n",
      "Epoch: 3962 mean train loss:  1.32382025e-02, mean val. rec. loss:  7.39968943e-03\n",
      "Epoch: 3963 mean train loss:  1.32272994e-02, mean val. rec. loss:  7.39044355e-03\n",
      "Epoch: 3964 mean train loss:  1.32164353e-02, mean val. rec. loss:  7.38474362e-03\n",
      "Epoch: 3965 mean train loss:  1.32056616e-02, mean val. rec. loss:  7.37459906e-03\n",
      "Epoch: 3966 mean train loss:  1.31950295e-02, mean val. rec. loss:  7.37095106e-03\n",
      "Epoch: 3967 mean train loss:  1.31846124e-02, mean val. rec. loss:  7.35903864e-03\n",
      "Epoch: 3968 mean train loss:  1.31746310e-02, mean val. rec. loss:  7.36009834e-03\n",
      "Epoch: 3969 mean train loss:  1.31654746e-02, mean val. rec. loss:  7.34583971e-03\n",
      "Epoch: 3970 mean train loss:  1.31579151e-02, mean val. rec. loss:  7.35836562e-03\n",
      "Epoch: 3971 mean train loss:  1.31532717e-02, mean val. rec. loss:  7.34189064e-03\n",
      "Epoch: 3972 mean train loss:  1.31532019e-02, mean val. rec. loss:  7.37614694e-03\n",
      "Epoch: 3973 mean train loss:  1.31574020e-02, mean val. rec. loss:  7.34657170e-03\n",
      "Epoch: 3974 mean train loss:  1.31586423e-02, mean val. rec. loss:  7.37133208e-03\n",
      "Epoch: 3975 mean train loss:  1.31420390e-02, mean val. rec. loss:  7.30690520e-03\n",
      "Epoch: 3976 mean train loss:  1.31067793e-02, mean val. rec. loss:  7.29753741e-03\n",
      "Epoch: 3977 mean train loss:  1.30794954e-02, mean val. rec. loss:  7.30574117e-03\n",
      "Epoch: 3978 mean train loss:  1.30777375e-02, mean val. rec. loss:  7.28878992e-03\n",
      "Epoch: 3979 mean train loss:  1.30834424e-02, mean val. rec. loss:  7.30729302e-03\n",
      "Epoch: 3980 mean train loss:  1.30707878e-02, mean val. rec. loss:  7.26379424e-03\n",
      "Epoch: 3981 mean train loss:  1.30445365e-02, mean val. rec. loss:  7.25869191e-03\n",
      "Epoch: 3982 mean train loss:  1.30309407e-02, mean val. rec. loss:  7.27673291e-03\n",
      "Epoch: 3983 mean train loss:  1.30321334e-02, mean val. rec. loss:  7.24925269e-03\n",
      "Epoch: 3984 mean train loss:  1.30259603e-02, mean val. rec. loss:  7.25285420e-03\n",
      "Epoch: 3985 mean train loss:  1.30064277e-02, mean val. rec. loss:  7.23619268e-03\n",
      "Epoch: 3986 mean train loss:  1.29917881e-02, mean val. rec. loss:  7.22625565e-03\n",
      "Epoch: 3987 mean train loss:  1.29889334e-02, mean val. rec. loss:  7.24028520e-03\n",
      "Epoch: 3988 mean train loss:  1.29831457e-02, mean val. rec. loss:  7.21156952e-03\n",
      "Epoch: 3989 mean train loss:  1.29677007e-02, mean val. rec. loss:  7.20866937e-03\n",
      "Epoch: 3990 mean train loss:  1.29544717e-02, mean val. rec. loss:  7.21391571e-03\n",
      "Epoch: 3991 mean train loss:  1.29494456e-02, mean val. rec. loss:  7.19494202e-03\n",
      "Epoch: 3992 mean train loss:  1.29430323e-02, mean val. rec. loss:  7.19866033e-03\n",
      "Epoch: 3993 mean train loss:  1.29300621e-02, mean val. rec. loss:  7.18603917e-03\n",
      "Epoch: 3994 mean train loss:  1.29182707e-02, mean val. rec. loss:  7.17743455e-03\n",
      "Epoch: 3995 mean train loss:  1.29120268e-02, mean val. rec. loss:  7.18421403e-03\n",
      "Epoch: 3996 mean train loss:  1.29051097e-02, mean val. rec. loss:  7.16494380e-03\n",
      "Epoch: 3997 mean train loss:  1.28938425e-02, mean val. rec. loss:  7.16282383e-03\n",
      "Epoch: 3998 mean train loss:  1.28831274e-02, mean val. rec. loss:  7.16316572e-03\n",
      "Epoch: 3999 mean train loss:  1.28761405e-02, mean val. rec. loss:  7.14909081e-03\n",
      "Epoch: 4000 mean train loss:  1.28689990e-02, mean val. rec. loss:  7.15127881e-03\n",
      "Epoch: 4001 mean train loss:  1.28590009e-02, mean val. rec. loss:  7.14120116e-03\n",
      "Epoch: 4002 mean train loss:  1.28490736e-02, mean val. rec. loss:  7.13434684e-03\n",
      "Epoch: 4003 mean train loss:  1.28416192e-02, mean val. rec. loss:  7.13670778e-03\n",
      "Epoch: 4004 mean train loss:  1.28344479e-02, mean val. rec. loss:  7.12321686e-03\n",
      "Epoch: 4005 mean train loss:  1.28254470e-02, mean val. rec. loss:  7.12195985e-03\n",
      "Epoch: 4006 mean train loss:  1.28161472e-02, mean val. rec. loss:  7.11881420e-03\n",
      "Epoch: 4007 mean train loss:  1.28084182e-02, mean val. rec. loss:  7.10905066e-03\n",
      "Epoch: 4008 mean train loss:  1.28012805e-02, mean val. rec. loss:  7.11038535e-03\n",
      "Epoch: 4009 mean train loss:  1.27930868e-02, mean val. rec. loss:  7.10138668e-03\n",
      "Epoch: 4010 mean train loss:  1.27843708e-02, mean val. rec. loss:  7.09691144e-03\n",
      "Epoch: 4011 mean train loss:  1.27765319e-02, mean val. rec. loss:  7.09597704e-03\n",
      "Epoch: 4012 mean train loss:  1.27693988e-02, mean val. rec. loss:  7.08669828e-03\n",
      "Epoch: 4013 mean train loss:  1.27618420e-02, mean val. rec. loss:  7.08656220e-03\n",
      "Epoch: 4014 mean train loss:  1.27537229e-02, mean val. rec. loss:  7.08084243e-03\n",
      "Epoch: 4015 mean train loss:  1.27459352e-02, mean val. rec. loss:  7.07495595e-03\n",
      "Epoch: 4016 mean train loss:  1.27387946e-02, mean val. rec. loss:  7.07505121e-03\n",
      "Epoch: 4017 mean train loss:  1.27316801e-02, mean val. rec. loss:  7.06749382e-03\n",
      "Epoch: 4018 mean train loss:  1.27241578e-02, mean val. rec. loss:  7.06539993e-03\n",
      "Epoch: 4019 mean train loss:  1.27166029e-02, mean val. rec. loss:  7.06143612e-03\n",
      "Epoch: 4020 mean train loss:  1.27094614e-02, mean val. rec. loss:  7.05587623e-03\n",
      "Epoch: 4021 mean train loss:  1.27025983e-02, mean val. rec. loss:  7.05572825e-03\n",
      "Epoch: 4022 mean train loss:  1.26956151e-02, mean val. rec. loss:  7.04920618e-03\n",
      "Epoch: 4023 mean train loss:  1.26884503e-02, mean val. rec. loss:  7.04649881e-03\n",
      "Epoch: 4024 mean train loss:  1.26814178e-02, mean val. rec. loss:  7.04399612e-03\n",
      "Epoch: 4025 mean train loss:  1.26746794e-02, mean val. rec. loss:  7.03893859e-03\n",
      "Epoch: 4026 mean train loss:  1.26680538e-02, mean val. rec. loss:  7.03785848e-03\n",
      "Epoch: 4027 mean train loss:  1.26613555e-02, mean val. rec. loss:  7.03243410e-03\n",
      "Epoch: 4028 mean train loss:  1.26546087e-02, mean val. rec. loss:  7.03007259e-03\n",
      "Epoch: 4029 mean train loss:  1.26479868e-02, mean val. rec. loss:  7.02775928e-03\n",
      "Epoch: 4030 mean train loss:  1.26415585e-02, mean val. rec. loss:  7.02300622e-03\n",
      "Epoch: 4031 mean train loss:  1.26352177e-02, mean val. rec. loss:  7.02183085e-03\n",
      "Epoch: 4032 mean train loss:  1.26288528e-02, mean val. rec. loss:  7.01750643e-03\n",
      "Epoch: 4033 mean train loss:  1.26224785e-02, mean val. rec. loss:  7.01521750e-03\n",
      "Epoch: 4034 mean train loss:  1.26162122e-02, mean val. rec. loss:  7.01262013e-03\n",
      "Epoch: 4035 mean train loss:  1.26100726e-02, mean val. rec. loss:  7.00882697e-03\n",
      "Epoch: 4036 mean train loss:  1.26040279e-02, mean val. rec. loss:  7.00773949e-03\n",
      "Epoch: 4037 mean train loss:  1.25980065e-02, mean val. rec. loss:  7.00379552e-03\n",
      "Epoch: 4038 mean train loss:  1.25919972e-02, mean val. rec. loss:  7.00184281e-03\n",
      "Epoch: 4039 mean train loss:  1.25860475e-02, mean val. rec. loss:  6.99934353e-03\n",
      "Epoch: 4040 mean train loss:  1.25801993e-02, mean val. rec. loss:  6.99640596e-03\n",
      "Epoch: 4041 mean train loss:  1.25744405e-02, mean val. rec. loss:  6.99495390e-03\n",
      "Epoch: 4042 mean train loss:  1.25687366e-02, mean val. rec. loss:  6.99147430e-03\n",
      "Epoch: 4043 mean train loss:  1.25630653e-02, mean val. rec. loss:  6.99010842e-03\n",
      "Epoch: 4044 mean train loss:  1.25574415e-02, mean val. rec. loss:  6.98751842e-03\n",
      "Epoch: 4045 mean train loss:  1.25518680e-02, mean val. rec. loss:  6.98524310e-03\n",
      "Epoch: 4046 mean train loss:  1.25463894e-02, mean val. rec. loss:  6.98363852e-03\n",
      "Epoch: 4047 mean train loss:  1.25409714e-02, mean val. rec. loss:  6.98091527e-03\n",
      "Epoch: 4048 mean train loss:  1.25356055e-02, mean val. rec. loss:  6.97969852e-03\n",
      "Epoch: 4049 mean train loss:  1.25302768e-02, mean val. rec. loss:  6.97696904e-03\n",
      "Epoch: 4050 mean train loss:  1.25249993e-02, mean val. rec. loss:  6.97545858e-03\n",
      "Epoch: 4051 mean train loss:  1.25197806e-02, mean val. rec. loss:  6.97358412e-03\n",
      "Epoch: 4052 mean train loss:  1.25146204e-02, mean val. rec. loss:  6.97150554e-03\n",
      "Epoch: 4053 mean train loss:  1.25095162e-02, mean val. rec. loss:  6.97018559e-03\n",
      "Epoch: 4054 mean train loss:  1.25044771e-02, mean val. rec. loss:  6.96791253e-03\n",
      "Epoch: 4055 mean train loss:  1.24994706e-02, mean val. rec. loss:  6.96682732e-03\n",
      "Epoch: 4056 mean train loss:  1.24945218e-02, mean val. rec. loss:  6.96464498e-03\n",
      "Epoch: 4057 mean train loss:  1.24896131e-02, mean val. rec. loss:  6.96327910e-03\n",
      "Epoch: 4058 mean train loss:  1.24847574e-02, mean val. rec. loss:  6.96166149e-03\n",
      "Epoch: 4059 mean train loss:  1.24799492e-02, mean val. rec. loss:  6.95998830e-03\n",
      "Epoch: 4060 mean train loss:  1.24751979e-02, mean val. rec. loss:  6.95876758e-03\n",
      "Epoch: 4061 mean train loss:  1.24704940e-02, mean val. rec. loss:  6.95692203e-03\n",
      "Epoch: 4062 mean train loss:  1.24658403e-02, mean val. rec. loss:  6.95594908e-03\n",
      "Epoch: 4063 mean train loss:  1.24612221e-02, mean val. rec. loss:  6.95412564e-03\n",
      "Epoch: 4064 mean train loss:  1.24566560e-02, mean val. rec. loss:  6.95308125e-03\n",
      "Epoch: 4065 mean train loss:  1.24521225e-02, mean val. rec. loss:  6.95154188e-03\n",
      "Epoch: 4066 mean train loss:  1.24476430e-02, mean val. rec. loss:  6.95032059e-03\n",
      "Epoch: 4067 mean train loss:  1.24431980e-02, mean val. rec. loss:  6.94908795e-03\n",
      "Epoch: 4068 mean train loss:  1.24388097e-02, mean val. rec. loss:  6.94773909e-03\n",
      "Epoch: 4069 mean train loss:  1.24344475e-02, mean val. rec. loss:  6.94676670e-03\n",
      "Epoch: 4070 mean train loss:  1.24301384e-02, mean val. rec. loss:  6.94529480e-03\n",
      "Epoch: 4071 mean train loss:  1.24258666e-02, mean val. rec. loss:  6.94446019e-03\n",
      "Epoch: 4072 mean train loss:  1.24216357e-02, mean val. rec. loss:  6.94304216e-03\n",
      "Epoch: 4073 mean train loss:  1.24174346e-02, mean val. rec. loss:  6.94221095e-03\n",
      "Epoch: 4074 mean train loss:  1.24132782e-02, mean val. rec. loss:  6.94087569e-03\n",
      "Epoch: 4075 mean train loss:  1.24091599e-02, mean val. rec. loss:  6.94005583e-03\n",
      "Epoch: 4076 mean train loss:  1.24050752e-02, mean val. rec. loss:  6.93887990e-03\n",
      "Epoch: 4077 mean train loss:  1.24010184e-02, mean val. rec. loss:  6.93795060e-03\n",
      "Epoch: 4078 mean train loss:  1.23970082e-02, mean val. rec. loss:  6.93693059e-03\n",
      "Epoch: 4079 mean train loss:  1.23930296e-02, mean val. rec. loss:  6.93597578e-03\n",
      "Epoch: 4080 mean train loss:  1.23890864e-02, mean val. rec. loss:  6.93506803e-03\n",
      "Epoch: 4081 mean train loss:  1.23851814e-02, mean val. rec. loss:  6.93406220e-03\n",
      "Epoch: 4082 mean train loss:  1.23813062e-02, mean val. rec. loss:  6.93329279e-03\n",
      "Epoch: 4083 mean train loss:  1.23774645e-02, mean val. rec. loss:  6.93227562e-03\n",
      "Epoch: 4084 mean train loss:  1.23736489e-02, mean val. rec. loss:  6.93155100e-03\n",
      "Epoch: 4085 mean train loss:  1.23698705e-02, mean val. rec. loss:  6.93054063e-03\n",
      "Epoch: 4086 mean train loss:  1.23661247e-02, mean val. rec. loss:  6.92992432e-03\n",
      "Epoch: 4087 mean train loss:  1.23624022e-02, mean val. rec. loss:  6.92883230e-03\n",
      "Epoch: 4088 mean train loss:  1.23587169e-02, mean val. rec. loss:  6.92832371e-03\n",
      "Epoch: 4089 mean train loss:  1.23550568e-02, mean val. rec. loss:  6.92724473e-03\n",
      "Epoch: 4090 mean train loss:  1.23514218e-02, mean val. rec. loss:  6.92683480e-03\n",
      "Epoch: 4091 mean train loss:  1.23478129e-02, mean val. rec. loss:  6.92561917e-03\n",
      "Epoch: 4092 mean train loss:  1.23442393e-02, mean val. rec. loss:  6.92544114e-03\n",
      "Epoch: 4093 mean train loss:  1.23406984e-02, mean val. rec. loss:  6.92409851e-03\n",
      "Epoch: 4094 mean train loss:  1.23371779e-02, mean val. rec. loss:  6.92411098e-03\n",
      "Epoch: 4095 mean train loss:  1.23336956e-02, mean val. rec. loss:  6.92249790e-03\n",
      "Epoch: 4096 mean train loss:  1.23302403e-02, mean val. rec. loss:  6.92305468e-03\n",
      "Epoch: 4097 mean train loss:  1.23268222e-02, mean val. rec. loss:  6.92087405e-03\n",
      "Epoch: 4098 mean train loss:  1.23234619e-02, mean val. rec. loss:  6.92222745e-03\n",
      "Epoch: 4099 mean train loss:  1.23201612e-02, mean val. rec. loss:  6.91924622e-03\n",
      "Epoch: 4100 mean train loss:  1.23169601e-02, mean val. rec. loss:  6.92221384e-03\n",
      "Epoch: 4101 mean train loss:  1.23139312e-02, mean val. rec. loss:  6.91770685e-03\n",
      "Epoch: 4102 mean train loss:  1.23111817e-02, mean val. rec. loss:  6.92402027e-03\n",
      "Epoch: 4103 mean train loss:  1.23089275e-02, mean val. rec. loss:  6.91739671e-03\n",
      "Epoch: 4104 mean train loss:  1.23075616e-02, mean val. rec. loss:  6.93086948e-03\n",
      "Epoch: 4105 mean train loss:  1.23077255e-02, mean val. rec. loss:  6.92176479e-03\n",
      "Epoch: 4106 mean train loss:  1.23103614e-02, mean val. rec. loss:  6.94935330e-03\n",
      "Epoch: 4107 mean train loss:  1.23162068e-02, mean val. rec. loss:  6.93552899e-03\n",
      "Epoch: 4108 mean train loss:  1.23243623e-02, mean val. rec. loss:  6.97297177e-03\n",
      "Epoch: 4109 mean train loss:  1.23293213e-02, mean val. rec. loss:  6.93635453e-03\n",
      "Epoch: 4110 mean train loss:  1.23221872e-02, mean val. rec. loss:  6.94682624e-03\n",
      "Epoch: 4111 mean train loss:  1.23011594e-02, mean val. rec. loss:  6.91184986e-03\n",
      "Epoch: 4112 mean train loss:  1.22809705e-02, mean val. rec. loss:  6.91126700e-03\n",
      "Epoch: 4113 mean train loss:  1.22768727e-02, mean val. rec. loss:  6.93554600e-03\n",
      "Epoch: 4114 mean train loss:  1.22854118e-02, mean val. rec. loss:  6.91947699e-03\n",
      "Epoch: 4115 mean train loss:  1.22899918e-02, mean val. rec. loss:  6.93541843e-03\n",
      "Epoch: 4116 mean train loss:  1.22806716e-02, mean val. rec. loss:  6.90826423e-03\n",
      "Epoch: 4117 mean train loss:  1.22661660e-02, mean val. rec. loss:  6.90843149e-03\n",
      "Epoch: 4118 mean train loss:  1.22611549e-02, mean val. rec. loss:  6.92475565e-03\n",
      "Epoch: 4119 mean train loss:  1.22655506e-02, mean val. rec. loss:  6.90998901e-03\n",
      "Epoch: 4120 mean train loss:  1.22668541e-02, mean val. rec. loss:  6.92233858e-03\n",
      "Epoch: 4121 mean train loss:  1.22589063e-02, mean val. rec. loss:  6.90517357e-03\n",
      "Epoch: 4122 mean train loss:  1.22496158e-02, mean val. rec. loss:  6.90391089e-03\n",
      "Epoch: 4123 mean train loss:  1.22475981e-02, mean val. rec. loss:  6.91887825e-03\n",
      "Epoch: 4124 mean train loss:  1.22496587e-02, mean val. rec. loss:  6.90428907e-03\n",
      "Epoch: 4125 mean train loss:  1.22474724e-02, mean val. rec. loss:  6.91168600e-03\n",
      "Epoch: 4126 mean train loss:  1.22403579e-02, mean val. rec. loss:  6.90371301e-03\n",
      "Epoch: 4127 mean train loss:  1.22350051e-02, mean val. rec. loss:  6.90138950e-03\n",
      "Epoch: 4128 mean train loss:  1.22343896e-02, mean val. rec. loss:  6.91388648e-03\n",
      "Epoch: 4129 mean train loss:  1.22341140e-02, mean val. rec. loss:  6.90021129e-03\n",
      "Epoch: 4130 mean train loss:  1.22301634e-02, mean val. rec. loss:  6.90481070e-03\n",
      "Epoch: 4131 mean train loss:  1.22246550e-02, mean val. rec. loss:  6.90338359e-03\n",
      "Epoch: 4132 mean train loss:  1.22216420e-02, mean val. rec. loss:  6.89862826e-03\n",
      "Epoch: 4133 mean train loss:  1.22208273e-02, mean val. rec. loss:  6.90792914e-03\n",
      "Epoch: 4134 mean train loss:  1.22188580e-02, mean val. rec. loss:  6.89747898e-03\n",
      "Epoch: 4135 mean train loss:  1.22147044e-02, mean val. rec. loss:  6.89983595e-03\n",
      "Epoch: 4136 mean train loss:  1.22107091e-02, mean val. rec. loss:  6.90115476e-03\n",
      "Epoch: 4137 mean train loss:  1.22085750e-02, mean val. rec. loss:  6.89571111e-03\n",
      "Epoch: 4138 mean train loss:  1.22070573e-02, mean val. rec. loss:  6.90296289e-03\n",
      "Epoch: 4139 mean train loss:  1.22043255e-02, mean val. rec. loss:  6.89541571e-03\n",
      "Epoch: 4140 mean train loss:  1.22006235e-02, mean val. rec. loss:  6.89621970e-03\n",
      "Epoch: 4141 mean train loss:  1.21975304e-02, mean val. rec. loss:  6.89887887e-03\n",
      "Epoch: 4142 mean train loss:  1.21955006e-02, mean val. rec. loss:  6.89339723e-03\n",
      "Epoch: 4143 mean train loss:  1.21934633e-02, mean val. rec. loss:  6.89844909e-03\n",
      "Epoch: 4144 mean train loss:  1.21905872e-02, mean val. rec. loss:  6.89324924e-03\n",
      "Epoch: 4145 mean train loss:  1.21873544e-02, mean val. rec. loss:  6.89343748e-03\n",
      "Epoch: 4146 mean train loss:  1.21846570e-02, mean val. rec. loss:  6.89592770e-03\n",
      "Epoch: 4147 mean train loss:  1.21825211e-02, mean val. rec. loss:  6.89100454e-03\n",
      "Epoch: 4148 mean train loss:  1.21802325e-02, mean val. rec. loss:  6.89496949e-03\n",
      "Epoch: 4149 mean train loss:  1.21774373e-02, mean val. rec. loss:  6.89115479e-03\n",
      "Epoch: 4150 mean train loss:  1.21745295e-02, mean val. rec. loss:  6.89073975e-03\n",
      "Epoch: 4151 mean train loss:  1.21719830e-02, mean val. rec. loss:  6.89270890e-03\n",
      "Epoch: 4152 mean train loss:  1.21697195e-02, mean val. rec. loss:  6.88865721e-03\n",
      "Epoch: 4153 mean train loss:  1.21673536e-02, mean val. rec. loss:  6.89155168e-03\n",
      "Epoch: 4154 mean train loss:  1.21646999e-02, mean val. rec. loss:  6.88845763e-03\n",
      "Epoch: 4155 mean train loss:  1.21619849e-02, mean val. rec. loss:  6.88820532e-03\n",
      "Epoch: 4156 mean train loss:  1.21594802e-02, mean val. rec. loss:  6.88948047e-03\n",
      "Epoch: 4157 mean train loss:  1.21571525e-02, mean val. rec. loss:  6.88612560e-03\n",
      "Epoch: 4158 mean train loss:  1.21547791e-02, mean val. rec. loss:  6.88847464e-03\n",
      "Epoch: 4159 mean train loss:  1.21522475e-02, mean val. rec. loss:  6.88583644e-03\n",
      "Epoch: 4160 mean train loss:  1.21496609e-02, mean val. rec. loss:  6.88574856e-03\n",
      "Epoch: 4161 mean train loss:  1.21471711e-02, mean val. rec. loss:  6.88627359e-03\n",
      "Epoch: 4162 mean train loss:  1.21448062e-02, mean val. rec. loss:  6.88373235e-03\n",
      "Epoch: 4163 mean train loss:  1.21424496e-02, mean val. rec. loss:  6.88555011e-03\n",
      "Epoch: 4164 mean train loss:  1.21400073e-02, mean val. rec. loss:  6.88296805e-03\n",
      "Epoch: 4165 mean train loss:  1.21375129e-02, mean val. rec. loss:  6.88318237e-03\n",
      "Epoch: 4166 mean train loss:  1.21350520e-02, mean val. rec. loss:  6.88304629e-03\n",
      "Epoch: 4167 mean train loss:  1.21326693e-02, mean val. rec. loss:  6.88117183e-03\n",
      "Epoch: 4168 mean train loss:  1.21303211e-02, mean val. rec. loss:  6.88241807e-03\n",
      "Epoch: 4169 mean train loss:  1.21279552e-02, mean val. rec. loss:  6.88013991e-03\n",
      "Epoch: 4170 mean train loss:  1.21255325e-02, mean val. rec. loss:  6.88065360e-03\n",
      "Epoch: 4171 mean train loss:  1.21231079e-02, mean val. rec. loss:  6.87975209e-03\n",
      "Epoch: 4172 mean train loss:  1.21207224e-02, mean val. rec. loss:  6.87861868e-03\n",
      "Epoch: 4173 mean train loss:  1.21183873e-02, mean val. rec. loss:  6.87929339e-03\n",
      "Epoch: 4174 mean train loss:  1.21160446e-02, mean val. rec. loss:  6.87726244e-03\n",
      "Epoch: 4175 mean train loss:  1.21136908e-02, mean val. rec. loss:  6.87789010e-03\n",
      "Epoch: 4176 mean train loss:  1.21113221e-02, mean val. rec. loss:  6.87661041e-03\n",
      "Epoch: 4177 mean train loss:  1.21089562e-02, mean val. rec. loss:  6.87613584e-03\n",
      "Epoch: 4178 mean train loss:  1.21066229e-02, mean val. rec. loss:  6.87606723e-03\n",
      "Epoch: 4179 mean train loss:  1.21043035e-02, mean val. rec. loss:  6.87459703e-03\n",
      "Epoch: 4180 mean train loss:  1.21019869e-02, mean val. rec. loss:  6.87509485e-03\n",
      "Epoch: 4181 mean train loss:  1.20996750e-02, mean val. rec. loss:  6.87352712e-03\n",
      "Epoch: 4182 mean train loss:  1.20973473e-02, mean val. rec. loss:  6.87354470e-03\n",
      "Epoch: 4183 mean train loss:  1.20950251e-02, mean val. rec. loss:  6.87279854e-03\n",
      "Epoch: 4184 mean train loss:  1.20927170e-02, mean val. rec. loss:  6.87189420e-03\n",
      "Epoch: 4185 mean train loss:  1.20904265e-02, mean val. rec. loss:  6.87195997e-03\n",
      "Epoch: 4186 mean train loss:  1.20881425e-02, mean val. rec. loss:  6.87055440e-03\n",
      "Epoch: 4187 mean train loss:  1.20858539e-02, mean val. rec. loss:  6.87074038e-03\n",
      "Epoch: 4188 mean train loss:  1.20835652e-02, mean val. rec. loss:  6.86955083e-03\n",
      "Epoch: 4189 mean train loss:  1.20812850e-02, mean val. rec. loss:  6.86924806e-03\n",
      "Epoch: 4190 mean train loss:  1.20790038e-02, mean val. rec. loss:  6.86868107e-03\n",
      "Epoch: 4191 mean train loss:  1.20767394e-02, mean val. rec. loss:  6.86775178e-03\n",
      "Epoch: 4192 mean train loss:  1.20744824e-02, mean val. rec. loss:  6.86767240e-03\n",
      "Epoch: 4193 mean train loss:  1.20722291e-02, mean val. rec. loss:  6.86647662e-03\n",
      "Epoch: 4194 mean train loss:  1.20699778e-02, mean val. rec. loss:  6.86640405e-03\n",
      "Epoch: 4195 mean train loss:  1.20677226e-02, mean val. rec. loss:  6.86540842e-03\n",
      "Epoch: 4196 mean train loss:  1.20654768e-02, mean val. rec. loss:  6.86501833e-03\n",
      "Epoch: 4197 mean train loss:  1.20632376e-02, mean val. rec. loss:  6.86439578e-03\n",
      "Epoch: 4198 mean train loss:  1.20610094e-02, mean val. rec. loss:  6.86359576e-03\n",
      "Epoch: 4199 mean train loss:  1.20587860e-02, mean val. rec. loss:  6.86336216e-03\n",
      "Epoch: 4200 mean train loss:  1.20565569e-02, mean val. rec. loss:  6.86227070e-03\n",
      "Epoch: 4201 mean train loss:  1.20543447e-02, mean val. rec. loss:  6.86208587e-03\n",
      "Epoch: 4202 mean train loss:  1.20521231e-02, mean val. rec. loss:  6.86114637e-03\n",
      "Epoch: 4203 mean train loss:  1.20499145e-02, mean val. rec. loss:  6.86073076e-03\n",
      "Epoch: 4204 mean train loss:  1.20477078e-02, mean val. rec. loss:  6.86000729e-03\n",
      "Epoch: 4205 mean train loss:  1.20455104e-02, mean val. rec. loss:  6.85937680e-03\n",
      "Epoch: 4206 mean train loss:  1.20433130e-02, mean val. rec. loss:  6.85891357e-03\n",
      "Epoch: 4207 mean train loss:  1.20411259e-02, mean val. rec. loss:  6.85803757e-03\n",
      "Epoch: 4208 mean train loss:  1.20389369e-02, mean val. rec. loss:  6.85771495e-03\n",
      "Epoch: 4209 mean train loss:  1.20367525e-02, mean val. rec. loss:  6.85681855e-03\n",
      "Epoch: 4210 mean train loss:  1.20345775e-02, mean val. rec. loss:  6.85644887e-03\n",
      "Epoch: 4211 mean train loss:  1.20324006e-02, mean val. rec. loss:  6.85560349e-03\n",
      "Epoch: 4212 mean train loss:  1.20302237e-02, mean val. rec. loss:  6.85512835e-03\n",
      "Epoch: 4213 mean train loss:  1.20280617e-02, mean val. rec. loss:  6.85443436e-03\n",
      "Epoch: 4214 mean train loss:  1.20258997e-02, mean val. rec. loss:  6.85377835e-03\n",
      "Epoch: 4215 mean train loss:  1.20237424e-02, mean val. rec. loss:  6.85325105e-03\n",
      "Epoch: 4216 mean train loss:  1.20215943e-02, mean val. rec. loss:  6.85248222e-03\n",
      "Epoch: 4217 mean train loss:  1.20194472e-02, mean val. rec. loss:  6.85203203e-03\n",
      "Epoch: 4218 mean train loss:  1.20172982e-02, mean val. rec. loss:  6.85118041e-03\n",
      "Epoch: 4219 mean train loss:  1.20151586e-02, mean val. rec. loss:  6.85075347e-03\n",
      "Epoch: 4220 mean train loss:  1.20130208e-02, mean val. rec. loss:  6.84994098e-03\n",
      "Epoch: 4221 mean train loss:  1.20108923e-02, mean val. rec. loss:  6.84946471e-03\n",
      "Epoch: 4222 mean train loss:  1.20087592e-02, mean val. rec. loss:  6.84869304e-03\n",
      "Epoch: 4223 mean train loss:  1.20066363e-02, mean val. rec. loss:  6.84813852e-03\n",
      "Epoch: 4224 mean train loss:  1.20045208e-02, mean val. rec. loss:  6.84748819e-03\n",
      "Epoch: 4225 mean train loss:  1.20024007e-02, mean val. rec. loss:  6.84682424e-03\n",
      "Epoch: 4226 mean train loss:  1.20002927e-02, mean val. rec. loss:  6.84622947e-03\n",
      "Epoch: 4227 mean train loss:  1.19981838e-02, mean val. rec. loss:  6.84551677e-03\n",
      "Epoch: 4228 mean train loss:  1.19960730e-02, mean val. rec. loss:  6.84496282e-03\n",
      "Epoch: 4229 mean train loss:  1.19939799e-02, mean val. rec. loss:  6.84421440e-03\n",
      "Epoch: 4230 mean train loss:  1.19918793e-02, mean val. rec. loss:  6.84370354e-03\n",
      "Epoch: 4231 mean train loss:  1.19897825e-02, mean val. rec. loss:  6.84290012e-03\n",
      "Epoch: 4232 mean train loss:  1.19876950e-02, mean val. rec. loss:  6.84242101e-03\n",
      "Epoch: 4233 mean train loss:  1.19856112e-02, mean val. rec. loss:  6.84166805e-03\n",
      "Epoch: 4234 mean train loss:  1.19835386e-02, mean val. rec. loss:  6.84113225e-03\n",
      "Epoch: 4235 mean train loss:  1.19814529e-02, mean val. rec. loss:  6.84036341e-03\n",
      "Epoch: 4236 mean train loss:  1.19793803e-02, mean val. rec. loss:  6.83987467e-03\n",
      "Epoch: 4237 mean train loss:  1.19773179e-02, mean val. rec. loss:  6.83910640e-03\n",
      "Epoch: 4238 mean train loss:  1.19752500e-02, mean val. rec. loss:  6.83853601e-03\n",
      "Epoch: 4239 mean train loss:  1.19731848e-02, mean val. rec. loss:  6.83781990e-03\n",
      "Epoch: 4240 mean train loss:  1.19711290e-02, mean val. rec. loss:  6.83727162e-03\n",
      "Epoch: 4241 mean train loss:  1.19690759e-02, mean val. rec. loss:  6.83651583e-03\n",
      "Epoch: 4242 mean train loss:  1.19670247e-02, mean val. rec. loss:  6.83598683e-03\n",
      "Epoch: 4243 mean train loss:  1.19649772e-02, mean val. rec. loss:  6.83523443e-03\n",
      "Epoch: 4244 mean train loss:  1.19629279e-02, mean val. rec. loss:  6.83466121e-03\n",
      "Epoch: 4245 mean train loss:  1.19608934e-02, mean val. rec. loss:  6.83392583e-03\n",
      "Epoch: 4246 mean train loss:  1.19588534e-02, mean val. rec. loss:  6.83339569e-03\n",
      "Epoch: 4247 mean train loss:  1.19568273e-02, mean val. rec. loss:  6.83257186e-03\n",
      "Epoch: 4248 mean train loss:  1.19547920e-02, mean val. rec. loss:  6.83211997e-03\n",
      "Epoch: 4249 mean train loss:  1.19527752e-02, mean val. rec. loss:  6.83129783e-03\n",
      "Epoch: 4250 mean train loss:  1.19507510e-02, mean val. rec. loss:  6.83083857e-03\n",
      "Epoch: 4251 mean train loss:  1.19487333e-02, mean val. rec. loss:  6.82993820e-03\n",
      "Epoch: 4252 mean train loss:  1.19467231e-02, mean val. rec. loss:  6.82966548e-03\n",
      "Epoch: 4253 mean train loss:  1.19447101e-02, mean val. rec. loss:  6.82855418e-03\n",
      "Epoch: 4254 mean train loss:  1.19427063e-02, mean val. rec. loss:  6.82843681e-03\n",
      "Epoch: 4255 mean train loss:  1.19407036e-02, mean val. rec. loss:  6.82719908e-03\n",
      "Epoch: 4256 mean train loss:  1.19387017e-02, mean val. rec. loss:  6.82731928e-03\n",
      "Epoch: 4257 mean train loss:  1.19367147e-02, mean val. rec. loss:  6.82565347e-03\n",
      "Epoch: 4258 mean train loss:  1.19347306e-02, mean val. rec. loss:  6.82635710e-03\n",
      "Epoch: 4259 mean train loss:  1.19327548e-02, mean val. rec. loss:  6.82402904e-03\n",
      "Epoch: 4260 mean train loss:  1.19307967e-02, mean val. rec. loss:  6.82559337e-03\n",
      "Epoch: 4261 mean train loss:  1.19288544e-02, mean val. rec. loss:  6.82219881e-03\n",
      "Epoch: 4262 mean train loss:  1.19269448e-02, mean val. rec. loss:  6.82538585e-03\n",
      "Epoch: 4263 mean train loss:  1.19250900e-02, mean val. rec. loss:  6.82003121e-03\n",
      "Epoch: 4264 mean train loss:  1.19233302e-02, mean val. rec. loss:  6.82634406e-03\n",
      "Epoch: 4265 mean train loss:  1.19217576e-02, mean val. rec. loss:  6.81761244e-03\n",
      "Epoch: 4266 mean train loss:  1.19205044e-02, mean val. rec. loss:  6.83002098e-03\n",
      "Epoch: 4267 mean train loss:  1.19198293e-02, mean val. rec. loss:  6.81592735e-03\n",
      "Epoch: 4268 mean train loss:  1.19201813e-02, mean val. rec. loss:  6.84040707e-03\n",
      "Epoch: 4269 mean train loss:  1.19222874e-02, mean val. rec. loss:  6.81881672e-03\n",
      "Epoch: 4270 mean train loss:  1.19270993e-02, mean val. rec. loss:  6.86370235e-03\n",
      "Epoch: 4271 mean train loss:  1.19351254e-02, mean val. rec. loss:  6.83179395e-03\n",
      "Epoch: 4272 mean train loss:  1.19448441e-02, mean val. rec. loss:  6.88594870e-03\n",
      "Epoch: 4273 mean train loss:  1.19498125e-02, mean val. rec. loss:  6.83303509e-03\n",
      "Epoch: 4274 mean train loss:  1.19415117e-02, mean val. rec. loss:  6.84948739e-03\n",
      "Epoch: 4275 mean train loss:  1.19202353e-02, mean val. rec. loss:  6.81301473e-03\n",
      "Epoch: 4276 mean train loss:  1.19018536e-02, mean val. rec. loss:  6.81202363e-03\n",
      "Epoch: 4277 mean train loss:  1.19001851e-02, mean val. rec. loss:  6.84048928e-03\n",
      "Epoch: 4278 mean train loss:  1.19102623e-02, mean val. rec. loss:  6.81756084e-03\n",
      "Epoch: 4279 mean train loss:  1.19155212e-02, mean val. rec. loss:  6.84022280e-03\n",
      "Epoch: 4280 mean train loss:  1.19068648e-02, mean val. rec. loss:  6.80813183e-03\n",
      "Epoch: 4281 mean train loss:  1.18933453e-02, mean val. rec. loss:  6.80791580e-03\n",
      "Epoch: 4282 mean train loss:  1.18893043e-02, mean val. rec. loss:  6.82781879e-03\n",
      "Epoch: 4283 mean train loss:  1.18946972e-02, mean val. rec. loss:  6.80932987e-03\n",
      "Epoch: 4284 mean train loss:  1.18973099e-02, mean val. rec. loss:  6.82482566e-03\n",
      "Epoch: 4285 mean train loss:  1.18907103e-02, mean val. rec. loss:  6.80502303e-03\n",
      "Epoch: 4286 mean train loss:  1.18821907e-02, mean val. rec. loss:  6.80442656e-03\n",
      "Epoch: 4287 mean train loss:  1.18806395e-02, mean val. rec. loss:  6.82014574e-03\n",
      "Epoch: 4288 mean train loss:  1.18836833e-02, mean val. rec. loss:  6.80291213e-03\n",
      "Epoch: 4289 mean train loss:  1.18830287e-02, mean val. rec. loss:  6.81431597e-03\n",
      "Epoch: 4290 mean train loss:  1.18771274e-02, mean val. rec. loss:  6.80383235e-03\n",
      "Epoch: 4291 mean train loss:  1.18722913e-02, mean val. rec. loss:  6.80068047e-03\n",
      "Epoch: 4292 mean train loss:  1.18722141e-02, mean val. rec. loss:  6.81507120e-03\n",
      "Epoch: 4293 mean train loss:  1.18731107e-02, mean val. rec. loss:  6.79976762e-03\n",
      "Epoch: 4294 mean train loss:  1.18705669e-02, mean val. rec. loss:  6.80581455e-03\n",
      "Epoch: 4295 mean train loss:  1.18659571e-02, mean val. rec. loss:  6.80247272e-03\n",
      "Epoch: 4296 mean train loss:  1.18634441e-02, mean val. rec. loss:  6.79779166e-03\n",
      "Epoch: 4297 mean train loss:  1.18634105e-02, mean val. rec. loss:  6.80858372e-03\n",
      "Epoch: 4298 mean train loss:  1.18626368e-02, mean val. rec. loss:  6.79608673e-03\n",
      "Epoch: 4299 mean train loss:  1.18595949e-02, mean val. rec. loss:  6.80006812e-03\n",
      "Epoch: 4300 mean train loss:  1.18563333e-02, mean val. rec. loss:  6.80074567e-03\n",
      "Epoch: 4301 mean train loss:  1.18548426e-02, mean val. rec. loss:  6.79438406e-03\n",
      "Epoch: 4302 mean train loss:  1.18542393e-02, mean val. rec. loss:  6.80299321e-03\n",
      "Epoch: 4303 mean train loss:  1.18525521e-02, mean val. rec. loss:  6.79425195e-03\n",
      "Epoch: 4304 mean train loss:  1.18497523e-02, mean val. rec. loss:  6.79562407e-03\n",
      "Epoch: 4305 mean train loss:  1.18474004e-02, mean val. rec. loss:  6.79799861e-03\n",
      "Epoch: 4306 mean train loss:  1.18461369e-02, mean val. rec. loss:  6.79201802e-03\n",
      "Epoch: 4307 mean train loss:  1.18449842e-02, mean val. rec. loss:  6.79821917e-03\n",
      "Epoch: 4308 mean train loss:  1.18430102e-02, mean val. rec. loss:  6.79196983e-03\n",
      "Epoch: 4309 mean train loss:  1.18406034e-02, mean val. rec. loss:  6.79237182e-03\n",
      "Epoch: 4310 mean train loss:  1.18386956e-02, mean val. rec. loss:  6.79522434e-03\n",
      "Epoch: 4311 mean train loss:  1.18373529e-02, mean val. rec. loss:  6.78942291e-03\n",
      "Epoch: 4312 mean train loss:  1.18358762e-02, mean val. rec. loss:  6.79395769e-03\n",
      "Epoch: 4313 mean train loss:  1.18338874e-02, mean val. rec. loss:  6.78984816e-03\n",
      "Epoch: 4314 mean train loss:  1.18317831e-02, mean val. rec. loss:  6.78926756e-03\n",
      "Epoch: 4315 mean train loss:  1.18300466e-02, mean val. rec. loss:  6.79163530e-03\n",
      "Epoch: 4316 mean train loss:  1.18285857e-02, mean val. rec. loss:  6.78688961e-03\n",
      "Epoch: 4317 mean train loss:  1.18269712e-02, mean val. rec. loss:  6.79024561e-03\n",
      "Epoch: 4318 mean train loss:  1.18250699e-02, mean val. rec. loss:  6.78702569e-03\n",
      "Epoch: 4319 mean train loss:  1.18231351e-02, mean val. rec. loss:  6.78630901e-03\n",
      "Epoch: 4320 mean train loss:  1.18214433e-02, mean val. rec. loss:  6.78837058e-03\n",
      "Epoch: 4321 mean train loss:  1.18198967e-02, mean val. rec. loss:  6.78430017e-03\n",
      "Epoch: 4322 mean train loss:  1.18182413e-02, mean val. rec. loss:  6.78682044e-03\n",
      "Epoch: 4323 mean train loss:  1.18164200e-02, mean val. rec. loss:  6.78442548e-03\n",
      "Epoch: 4324 mean train loss:  1.18145914e-02, mean val. rec. loss:  6.78370257e-03\n",
      "Epoch: 4325 mean train loss:  1.18129024e-02, mean val. rec. loss:  6.78503669e-03\n",
      "Epoch: 4326 mean train loss:  1.18112999e-02, mean val. rec. loss:  6.78178898e-03\n",
      "Epoch: 4327 mean train loss:  1.18096454e-02, mean val. rec. loss:  6.78372582e-03\n",
      "Epoch: 4328 mean train loss:  1.18078903e-02, mean val. rec. loss:  6.78152760e-03\n",
      "Epoch: 4329 mean train loss:  1.18061240e-02, mean val. rec. loss:  6.78101221e-03\n",
      "Epoch: 4330 mean train loss:  1.18044350e-02, mean val. rec. loss:  6.78179635e-03\n",
      "Epoch: 4331 mean train loss:  1.18028018e-02, mean val. rec. loss:  6.77918481e-03\n",
      "Epoch: 4332 mean train loss:  1.18011519e-02, mean val. rec. loss:  6.78066238e-03\n",
      "Epoch: 4333 mean train loss:  1.17994443e-02, mean val. rec. loss:  6.77861839e-03\n",
      "Epoch: 4334 mean train loss:  1.17977255e-02, mean val. rec. loss:  6.77836948e-03\n",
      "Epoch: 4335 mean train loss:  1.17960384e-02, mean val. rec. loss:  6.77856055e-03\n",
      "Epoch: 4336 mean train loss:  1.17943978e-02, mean val. rec. loss:  6.77649445e-03\n",
      "Epoch: 4337 mean train loss:  1.17927590e-02, mean val. rec. loss:  6.77764260e-03\n",
      "Epoch: 4338 mean train loss:  1.17910840e-02, mean val. rec. loss:  6.77570350e-03\n",
      "Epoch: 4339 mean train loss:  1.17893959e-02, mean val. rec. loss:  6.77574262e-03\n",
      "Epoch: 4340 mean train loss:  1.17877181e-02, mean val. rec. loss:  6.77547160e-03\n",
      "Epoch: 4341 mean train loss:  1.17860691e-02, mean val. rec. loss:  6.77399063e-03\n",
      "Epoch: 4342 mean train loss:  1.17844378e-02, mean val. rec. loss:  6.77475833e-03\n",
      "Epoch: 4343 mean train loss:  1.17827870e-02, mean val. rec. loss:  6.77294510e-03\n",
      "Epoch: 4344 mean train loss:  1.17811334e-02, mean val. rec. loss:  6.77317757e-03\n",
      "Epoch: 4345 mean train loss:  1.17794704e-02, mean val. rec. loss:  6.77244161e-03\n",
      "Epoch: 4346 mean train loss:  1.17778280e-02, mean val. rec. loss:  6.77144372e-03\n",
      "Epoch: 4347 mean train loss:  1.17761939e-02, mean val. rec. loss:  6.77175216e-03\n",
      "Epoch: 4348 mean train loss:  1.17745626e-02, mean val. rec. loss:  6.77020088e-03\n",
      "Epoch: 4349 mean train loss:  1.17729295e-02, mean val. rec. loss:  6.77050762e-03\n",
      "Epoch: 4350 mean train loss:  1.17712908e-02, mean val. rec. loss:  6.76943034e-03\n",
      "Epoch: 4351 mean train loss:  1.17696558e-02, mean val. rec. loss:  6.76893876e-03\n",
      "Epoch: 4352 mean train loss:  1.17680273e-02, mean val. rec. loss:  6.76879475e-03\n",
      "Epoch: 4353 mean train loss:  1.17664100e-02, mean val. rec. loss:  6.76754057e-03\n",
      "Epoch: 4354 mean train loss:  1.17647945e-02, mean val. rec. loss:  6.76777587e-03\n",
      "Epoch: 4355 mean train loss:  1.17631716e-02, mean val. rec. loss:  6.76661978e-03\n",
      "Epoch: 4356 mean train loss:  1.17615506e-02, mean val. rec. loss:  6.76639525e-03\n",
      "Epoch: 4357 mean train loss:  1.17599333e-02, mean val. rec. loss:  6.76584244e-03\n",
      "Epoch: 4358 mean train loss:  1.17583243e-02, mean val. rec. loss:  6.76505036e-03\n",
      "Epoch: 4359 mean train loss:  1.17567182e-02, mean val. rec. loss:  6.76496474e-03\n",
      "Epoch: 4360 mean train loss:  1.17551167e-02, mean val. rec. loss:  6.76385514e-03\n",
      "Epoch: 4361 mean train loss:  1.17535143e-02, mean val. rec. loss:  6.76385004e-03\n",
      "Epoch: 4362 mean train loss:  1.17519119e-02, mean val. rec. loss:  6.76296781e-03\n",
      "Epoch: 4363 mean train loss:  1.17503085e-02, mean val. rec. loss:  6.76249097e-03\n",
      "Epoch: 4364 mean train loss:  1.17487145e-02, mean val. rec. loss:  6.76214964e-03\n",
      "Epoch: 4365 mean train loss:  1.17471261e-02, mean val. rec. loss:  6.76127195e-03\n",
      "Epoch: 4366 mean train loss:  1.17455320e-02, mean val. rec. loss:  6.76112113e-03\n",
      "Epoch: 4367 mean train loss:  1.17439454e-02, mean val. rec. loss:  6.76020601e-03\n",
      "Epoch: 4368 mean train loss:  1.17423644e-02, mean val. rec. loss:  6.76004895e-03\n",
      "Epoch: 4369 mean train loss:  1.17407816e-02, mean val. rec. loss:  6.75930166e-03\n",
      "Epoch: 4370 mean train loss:  1.17391941e-02, mean val. rec. loss:  6.75876756e-03\n",
      "Epoch: 4371 mean train loss:  1.17376242e-02, mean val. rec. loss:  6.75846875e-03\n",
      "Epoch: 4372 mean train loss:  1.17360479e-02, mean val. rec. loss:  6.75761600e-03\n",
      "Epoch: 4373 mean train loss:  1.17344753e-02, mean val. rec. loss:  6.75736369e-03\n",
      "Epoch: 4374 mean train loss:  1.17329017e-02, mean val. rec. loss:  6.75659429e-03\n",
      "Epoch: 4375 mean train loss:  1.17313365e-02, mean val. rec. loss:  6.75628018e-03\n",
      "Epoch: 4376 mean train loss:  1.17297751e-02, mean val. rec. loss:  6.75561340e-03\n",
      "Epoch: 4377 mean train loss:  1.17282109e-02, mean val. rec. loss:  6.75512976e-03\n",
      "Epoch: 4378 mean train loss:  1.17266466e-02, mean val. rec. loss:  6.75472550e-03\n",
      "Epoch: 4379 mean train loss:  1.17250945e-02, mean val. rec. loss:  6.75400656e-03\n",
      "Epoch: 4380 mean train loss:  1.17235368e-02, mean val. rec. loss:  6.75372873e-03\n",
      "Epoch: 4381 mean train loss:  1.17219846e-02, mean val. rec. loss:  6.75297180e-03\n",
      "Epoch: 4382 mean train loss:  1.17204390e-02, mean val. rec. loss:  6.75265259e-03\n",
      "Epoch: 4383 mean train loss:  1.17188925e-02, mean val. rec. loss:  6.75200736e-03\n",
      "Epoch: 4384 mean train loss:  1.17173440e-02, mean val. rec. loss:  6.75154696e-03\n",
      "Epoch: 4385 mean train loss:  1.17158077e-02, mean val. rec. loss:  6.75106502e-03\n",
      "Epoch: 4386 mean train loss:  1.17142640e-02, mean val. rec. loss:  6.75045608e-03\n",
      "Epoch: 4387 mean train loss:  1.17127295e-02, mean val. rec. loss:  6.75008357e-03\n",
      "Epoch: 4388 mean train loss:  1.17111970e-02, mean val. rec. loss:  6.74941338e-03\n",
      "Epoch: 4389 mean train loss:  1.17096569e-02, mean val. rec. loss:  6.74906015e-03\n",
      "Epoch: 4390 mean train loss:  1.17081262e-02, mean val. rec. loss:  6.74837750e-03\n",
      "Epoch: 4391 mean train loss:  1.17066076e-02, mean val. rec. loss:  6.74802993e-03\n",
      "Epoch: 4392 mean train loss:  1.17050815e-02, mean val. rec. loss:  6.74743119e-03\n",
      "Epoch: 4393 mean train loss:  1.17035536e-02, mean val. rec. loss:  6.74693395e-03\n",
      "Epoch: 4394 mean train loss:  1.17020313e-02, mean val. rec. loss:  6.74647865e-03\n",
      "Epoch: 4395 mean train loss:  1.17005192e-02, mean val. rec. loss:  6.74588615e-03\n",
      "Epoch: 4396 mean train loss:  1.16990052e-02, mean val. rec. loss:  6.74550060e-03\n",
      "Epoch: 4397 mean train loss:  1.16974940e-02, mean val. rec. loss:  6.74486160e-03\n",
      "Epoch: 4398 mean train loss:  1.16959801e-02, mean val. rec. loss:  6.74447265e-03\n",
      "Epoch: 4399 mean train loss:  1.16944717e-02, mean val. rec. loss:  6.74387788e-03\n",
      "Epoch: 4400 mean train loss:  1.16929624e-02, mean val. rec. loss:  6.74345604e-03\n",
      "Epoch: 4401 mean train loss:  1.16914587e-02, mean val. rec. loss:  6.74288848e-03\n",
      "Epoch: 4402 mean train loss:  1.16899577e-02, mean val. rec. loss:  6.74243433e-03\n",
      "Epoch: 4403 mean train loss:  1.16884596e-02, mean val. rec. loss:  6.74193254e-03\n",
      "Epoch: 4404 mean train loss:  1.16869624e-02, mean val. rec. loss:  6.74142395e-03\n",
      "Epoch: 4405 mean train loss:  1.16854708e-02, mean val. rec. loss:  6.74094712e-03\n",
      "Epoch: 4406 mean train loss:  1.16839782e-02, mean val. rec. loss:  6.74042379e-03\n",
      "Epoch: 4407 mean train loss:  1.16824894e-02, mean val. rec. loss:  6.73998153e-03\n",
      "Epoch: 4408 mean train loss:  1.16810062e-02, mean val. rec. loss:  6.73943439e-03\n",
      "Epoch: 4409 mean train loss:  1.16795211e-02, mean val. rec. loss:  6.73901539e-03\n",
      "Epoch: 4410 mean train loss:  1.16780434e-02, mean val. rec. loss:  6.73845407e-03\n",
      "Epoch: 4411 mean train loss:  1.16765639e-02, mean val. rec. loss:  6.73803450e-03\n",
      "Epoch: 4412 mean train loss:  1.16750854e-02, mean val. rec. loss:  6.73749246e-03\n",
      "Epoch: 4413 mean train loss:  1.16736105e-02, mean val. rec. loss:  6.73702979e-03\n",
      "Epoch: 4414 mean train loss:  1.16721319e-02, mean val. rec. loss:  6.73652801e-03\n",
      "Epoch: 4415 mean train loss:  1.16706645e-02, mean val. rec. loss:  6.73606535e-03\n",
      "Epoch: 4416 mean train loss:  1.16692008e-02, mean val. rec. loss:  6.73555562e-03\n",
      "Epoch: 4417 mean train loss:  1.16677297e-02, mean val. rec. loss:  6.73506915e-03\n",
      "Epoch: 4418 mean train loss:  1.16662642e-02, mean val. rec. loss:  6.73460535e-03\n",
      "Epoch: 4419 mean train loss:  1.16648070e-02, mean val. rec. loss:  6.73411604e-03\n",
      "Epoch: 4420 mean train loss:  1.16633545e-02, mean val. rec. loss:  6.73364374e-03\n",
      "Epoch: 4421 mean train loss:  1.16618945e-02, mean val. rec. loss:  6.73313799e-03\n",
      "Epoch: 4422 mean train loss:  1.16604346e-02, mean val. rec. loss:  6.73270027e-03\n",
      "Epoch: 4423 mean train loss:  1.16589886e-02, mean val. rec. loss:  6.73217014e-03\n",
      "Epoch: 4424 mean train loss:  1.16575342e-02, mean val. rec. loss:  6.73174036e-03\n",
      "Epoch: 4425 mean train loss:  1.16560892e-02, mean val. rec. loss:  6.73120399e-03\n",
      "Epoch: 4426 mean train loss:  1.16546478e-02, mean val. rec. loss:  6.73079462e-03\n",
      "Epoch: 4427 mean train loss:  1.16532028e-02, mean val. rec. loss:  6.73029624e-03\n",
      "Epoch: 4428 mean train loss:  1.16517605e-02, mean val. rec. loss:  6.72983244e-03\n",
      "Epoch: 4429 mean train loss:  1.16503219e-02, mean val. rec. loss:  6.72935164e-03\n",
      "Epoch: 4430 mean train loss:  1.16488881e-02, mean val. rec. loss:  6.72890315e-03\n",
      "Epoch: 4431 mean train loss:  1.16474542e-02, mean val. rec. loss:  6.72839853e-03\n",
      "Epoch: 4432 mean train loss:  1.16460203e-02, mean val. rec. loss:  6.72794097e-03\n",
      "Epoch: 4433 mean train loss:  1.16445957e-02, mean val. rec. loss:  6.72746243e-03\n",
      "Epoch: 4434 mean train loss:  1.16431618e-02, mean val. rec. loss:  6.72700714e-03\n",
      "Epoch: 4435 mean train loss:  1.16417382e-02, mean val. rec. loss:  6.72652180e-03\n",
      "Epoch: 4436 mean train loss:  1.16403183e-02, mean val. rec. loss:  6.72609372e-03\n",
      "Epoch: 4437 mean train loss:  1.16388955e-02, mean val. rec. loss:  6.72558343e-03\n",
      "Epoch: 4438 mean train loss:  1.16374784e-02, mean val. rec. loss:  6.72515025e-03\n",
      "Epoch: 4439 mean train loss:  1.16360678e-02, mean val. rec. loss:  6.72468532e-03\n",
      "Epoch: 4440 mean train loss:  1.16346507e-02, mean val. rec. loss:  6.72420452e-03\n",
      "Epoch: 4441 mean train loss:  1.16332345e-02, mean val. rec. loss:  6.72372144e-03\n",
      "Epoch: 4442 mean train loss:  1.16318313e-02, mean val. rec. loss:  6.72334440e-03\n",
      "Epoch: 4443 mean train loss:  1.16304263e-02, mean val. rec. loss:  6.72277684e-03\n",
      "Epoch: 4444 mean train loss:  1.16290166e-02, mean val. rec. loss:  6.72240830e-03\n",
      "Epoch: 4445 mean train loss:  1.16276144e-02, mean val. rec. loss:  6.72188780e-03\n",
      "Epoch: 4446 mean train loss:  1.16262103e-02, mean val. rec. loss:  6.72149602e-03\n",
      "Epoch: 4447 mean train loss:  1.16248118e-02, mean val. rec. loss:  6.72091032e-03\n",
      "Epoch: 4448 mean train loss:  1.16234179e-02, mean val. rec. loss:  6.72065064e-03\n",
      "Epoch: 4449 mean train loss:  1.16220222e-02, mean val. rec. loss:  6.71995040e-03\n",
      "Epoch: 4450 mean train loss:  1.16206330e-02, mean val. rec. loss:  6.71976727e-03\n",
      "Epoch: 4451 mean train loss:  1.16192420e-02, mean val. rec. loss:  6.71901091e-03\n",
      "Epoch: 4452 mean train loss:  1.16178621e-02, mean val. rec. loss:  6.71897292e-03\n",
      "Epoch: 4453 mean train loss:  1.16164701e-02, mean val. rec. loss:  6.71793816e-03\n",
      "Epoch: 4454 mean train loss:  1.16150958e-02, mean val. rec. loss:  6.71826532e-03\n",
      "Epoch: 4455 mean train loss:  1.16137178e-02, mean val. rec. loss:  6.71685748e-03\n",
      "Epoch: 4456 mean train loss:  1.16123444e-02, mean val. rec. loss:  6.71766884e-03\n",
      "Epoch: 4457 mean train loss:  1.16109729e-02, mean val. rec. loss:  6.71556985e-03\n",
      "Epoch: 4458 mean train loss:  1.16096200e-02, mean val. rec. loss:  6.71741653e-03\n",
      "Epoch: 4459 mean train loss:  1.16082755e-02, mean val. rec. loss:  6.71401347e-03\n",
      "Epoch: 4460 mean train loss:  1.16069515e-02, mean val. rec. loss:  6.71766261e-03\n",
      "Epoch: 4461 mean train loss:  1.16056824e-02, mean val. rec. loss:  6.71203355e-03\n",
      "Epoch: 4462 mean train loss:  1.16045037e-02, mean val. rec. loss:  6.71924280e-03\n",
      "Epoch: 4463 mean train loss:  1.16034851e-02, mean val. rec. loss:  6.70939705e-03\n",
      "Epoch: 4464 mean train loss:  1.16027746e-02, mean val. rec. loss:  6.72395618e-03\n",
      "Epoch: 4465 mean train loss:  1.16026331e-02, mean val. rec. loss:  6.70701287e-03\n",
      "Epoch: 4466 mean train loss:  1.16035605e-02, mean val. rec. loss:  6.73629895e-03\n",
      "Epoch: 4467 mean train loss:  1.16063854e-02, mean val. rec. loss:  6.70908748e-03\n",
      "Epoch: 4468 mean train loss:  1.16123668e-02, mean val. rec. loss:  6.76496304e-03\n",
      "Epoch: 4469 mean train loss:  1.16226312e-02, mean val. rec. loss:  6.72463089e-03\n",
      "Epoch: 4470 mean train loss:  1.16363220e-02, mean val. rec. loss:  6.79808196e-03\n",
      "Epoch: 4471 mean train loss:  1.16464151e-02, mean val. rec. loss:  6.73302686e-03\n",
      "Epoch: 4472 mean train loss:  1.16407559e-02, mean val. rec. loss:  6.75698098e-03\n",
      "Epoch: 4473 mean train loss:  1.16160083e-02, mean val. rec. loss:  6.70829142e-03\n",
      "Epoch: 4474 mean train loss:  1.15917961e-02, mean val. rec. loss:  6.70662335e-03\n",
      "Epoch: 4475 mean train loss:  1.15893240e-02, mean val. rec. loss:  6.74222000e-03\n",
      "Epoch: 4476 mean train loss:  1.16035791e-02, mean val. rec. loss:  6.71479024e-03\n",
      "Epoch: 4477 mean train loss:  1.16112690e-02, mean val. rec. loss:  6.74148405e-03\n",
      "Epoch: 4478 mean train loss:  1.15999944e-02, mean val. rec. loss:  6.70230063e-03\n",
      "Epoch: 4479 mean train loss:  1.15839870e-02, mean val. rec. loss:  6.70239021e-03\n",
      "Epoch: 4480 mean train loss:  1.15825308e-02, mean val. rec. loss:  6.73314252e-03\n",
      "Epoch: 4481 mean train loss:  1.15916183e-02, mean val. rec. loss:  6.70504088e-03\n",
      "Epoch: 4482 mean train loss:  1.15931471e-02, mean val. rec. loss:  6.71984721e-03\n",
      "Epoch: 4483 mean train loss:  1.15830308e-02, mean val. rec. loss:  6.70425220e-03\n",
      "Epoch: 4484 mean train loss:  1.15754023e-02, mean val. rec. loss:  6.69978093e-03\n",
      "Epoch: 4485 mean train loss:  1.15783846e-02, mean val. rec. loss:  6.72339883e-03\n",
      "Epoch: 4486 mean train loss:  1.15825717e-02, mean val. rec. loss:  6.69941862e-03\n",
      "Epoch: 4487 mean train loss:  1.15785196e-02, mean val. rec. loss:  6.70738991e-03\n",
      "Epoch: 4488 mean train loss:  1.15712952e-02, mean val. rec. loss:  6.70615218e-03\n",
      "Epoch: 4489 mean train loss:  1.15701584e-02, mean val. rec. loss:  6.69788719e-03\n",
      "Epoch: 4490 mean train loss:  1.15731723e-02, mean val. rec. loss:  6.71674976e-03\n",
      "Epoch: 4491 mean train loss:  1.15722617e-02, mean val. rec. loss:  6.69721191e-03\n",
      "Epoch: 4492 mean train loss:  1.15671286e-02, mean val. rec. loss:  6.69873881e-03\n",
      "Epoch: 4493 mean train loss:  1.15642850e-02, mean val. rec. loss:  6.70876769e-03\n",
      "Epoch: 4494 mean train loss:  1.15654498e-02, mean val. rec. loss:  6.69549223e-03\n",
      "Epoch: 4495 mean train loss:  1.15656696e-02, mean val. rec. loss:  6.70545592e-03\n",
      "Epoch: 4496 mean train loss:  1.15624768e-02, mean val. rec. loss:  6.69770405e-03\n",
      "Epoch: 4497 mean train loss:  1.15593279e-02, mean val. rec. loss:  6.69543156e-03\n",
      "Epoch: 4498 mean train loss:  1.15590076e-02, mean val. rec. loss:  6.70561070e-03\n",
      "Epoch: 4499 mean train loss:  1.15592944e-02, mean val. rec. loss:  6.69354746e-03\n",
      "Epoch: 4500 mean train loss:  1.15574657e-02, mean val. rec. loss:  6.69957341e-03\n",
      "Epoch: 4501 mean train loss:  1.15546761e-02, mean val. rec. loss:  6.69837707e-03\n",
      "Epoch: 4502 mean train loss:  1.15534210e-02, mean val. rec. loss:  6.69259492e-03\n",
      "Epoch: 4503 mean train loss:  1.15533083e-02, mean val. rec. loss:  6.70238000e-03\n",
      "Epoch: 4504 mean train loss:  1.15522404e-02, mean val. rec. loss:  6.69325716e-03\n",
      "Epoch: 4505 mean train loss:  1.15500355e-02, mean val. rec. loss:  6.69442913e-03\n",
      "Epoch: 4506 mean train loss:  1.15483428e-02, mean val. rec. loss:  6.69849443e-03\n",
      "Epoch: 4507 mean train loss:  1.15477153e-02, mean val. rec. loss:  6.69170928e-03\n",
      "Epoch: 4508 mean train loss:  1.15469499e-02, mean val. rec. loss:  6.69746081e-03\n",
      "Epoch: 4509 mean train loss:  1.15453223e-02, mean val. rec. loss:  6.69290109e-03\n",
      "Epoch: 4510 mean train loss:  1.15435523e-02, mean val. rec. loss:  6.69219009e-03\n",
      "Epoch: 4511 mean train loss:  1.15424844e-02, mean val. rec. loss:  6.69646632e-03\n",
      "Epoch: 4512 mean train loss:  1.15417125e-02, mean val. rec. loss:  6.68994198e-03\n",
      "Epoch: 4513 mean train loss:  1.15404769e-02, mean val. rec. loss:  6.69408780e-03\n",
      "Epoch: 4514 mean train loss:  1.15388726e-02, mean val. rec. loss:  6.69217705e-03\n",
      "Epoch: 4515 mean train loss:  1.15375412e-02, mean val. rec. loss:  6.68936082e-03\n",
      "Epoch: 4516 mean train loss:  1.15365961e-02, mean val. rec. loss:  6.69409574e-03\n",
      "Epoch: 4517 mean train loss:  1.15355579e-02, mean val. rec. loss:  6.68902743e-03\n",
      "Epoch: 4518 mean train loss:  1.15341902e-02, mean val. rec. loss:  6.69067170e-03\n",
      "Epoch: 4519 mean train loss:  1.15327814e-02, mean val. rec. loss:  6.69103797e-03\n",
      "Epoch: 4520 mean train loss:  1.15316473e-02, mean val. rec. loss:  6.68808283e-03\n",
      "Epoch: 4521 mean train loss:  1.15306334e-02, mean val. rec. loss:  6.69137249e-03\n",
      "Epoch: 4522 mean train loss:  1.15294481e-02, mean val. rec. loss:  6.68780841e-03\n",
      "Epoch: 4523 mean train loss:  1.15281166e-02, mean val. rec. loss:  6.68857781e-03\n",
      "Epoch: 4524 mean train loss:  1.15268587e-02, mean val. rec. loss:  6.68936082e-03\n",
      "Epoch: 4525 mean train loss:  1.15257665e-02, mean val. rec. loss:  6.68619702e-03\n",
      "Epoch: 4526 mean train loss:  1.15246678e-02, mean val. rec. loss:  6.68898887e-03\n",
      "Epoch: 4527 mean train loss:  1.15234490e-02, mean val. rec. loss:  6.68646181e-03\n",
      "Epoch: 4528 mean train loss:  1.15221846e-02, mean val. rec. loss:  6.68621403e-03\n",
      "Epoch: 4529 mean train loss:  1.15210021e-02, mean val. rec. loss:  6.68751130e-03\n",
      "Epoch: 4530 mean train loss:  1.15198885e-02, mean val. rec. loss:  6.68476368e-03\n",
      "Epoch: 4531 mean train loss:  1.15187545e-02, mean val. rec. loss:  6.68649526e-03\n",
      "Epoch: 4532 mean train loss:  1.15175459e-02, mean val. rec. loss:  6.68492414e-03\n",
      "Epoch: 4533 mean train loss:  1.15163299e-02, mean val. rec. loss:  6.68442178e-03\n",
      "Epoch: 4534 mean train loss:  1.15151697e-02, mean val. rec. loss:  6.68536128e-03\n",
      "Epoch: 4535 mean train loss:  1.15140496e-02, mean val. rec. loss:  6.68315230e-03\n",
      "Epoch: 4536 mean train loss:  1.15129007e-02, mean val. rec. loss:  6.68451364e-03\n",
      "Epoch: 4537 mean train loss:  1.15117135e-02, mean val. rec. loss:  6.68325719e-03\n",
      "Epoch: 4538 mean train loss:  1.15105236e-02, mean val. rec. loss:  6.68263974e-03\n",
      "Epoch: 4539 mean train loss:  1.15093839e-02, mean val. rec. loss:  6.68357073e-03\n",
      "Epoch: 4540 mean train loss:  1.15082498e-02, mean val. rec. loss:  6.68160215e-03\n",
      "Epoch: 4541 mean train loss:  1.15071009e-02, mean val. rec. loss:  6.68249062e-03\n",
      "Epoch: 4542 mean train loss:  1.15059296e-02, mean val. rec. loss:  6.68159195e-03\n",
      "Epoch: 4543 mean train loss:  1.15047694e-02, mean val. rec. loss:  6.68096769e-03\n",
      "Epoch: 4544 mean train loss:  1.15036298e-02, mean val. rec. loss:  6.68150066e-03\n",
      "Epoch: 4545 mean train loss:  1.15025013e-02, mean val. rec. loss:  6.68001118e-03\n",
      "Epoch: 4546 mean train loss:  1.15013551e-02, mean val. rec. loss:  6.68068080e-03\n",
      "Epoch: 4547 mean train loss:  1.15002089e-02, mean val. rec. loss:  6.67980423e-03\n",
      "Epoch: 4548 mean train loss:  1.14990609e-02, mean val. rec. loss:  6.67929564e-03\n",
      "Epoch: 4549 mean train loss:  1.14979166e-02, mean val. rec. loss:  6.67970047e-03\n",
      "Epoch: 4550 mean train loss:  1.14967881e-02, mean val. rec. loss:  6.67834254e-03\n",
      "Epoch: 4551 mean train loss:  1.14956577e-02, mean val. rec. loss:  6.67883525e-03\n",
      "Epoch: 4552 mean train loss:  1.14945208e-02, mean val. rec. loss:  6.67805791e-03\n",
      "Epoch: 4553 mean train loss:  1.14933840e-02, mean val. rec. loss:  6.67762133e-03\n",
      "Epoch: 4554 mean train loss:  1.14922508e-02, mean val. rec. loss:  6.67777158e-03\n",
      "Epoch: 4555 mean train loss:  1.14911233e-02, mean val. rec. loss:  6.67673059e-03\n",
      "Epoch: 4556 mean train loss:  1.14900022e-02, mean val. rec. loss:  6.67709176e-03\n",
      "Epoch: 4557 mean train loss:  1.14888682e-02, mean val. rec. loss:  6.67626793e-03\n",
      "Epoch: 4558 mean train loss:  1.14877434e-02, mean val. rec. loss:  6.67604510e-03\n",
      "Epoch: 4559 mean train loss:  1.14866233e-02, mean val. rec. loss:  6.67599237e-03\n",
      "Epoch: 4560 mean train loss:  1.14854976e-02, mean val. rec. loss:  6.67507838e-03\n",
      "Epoch: 4561 mean train loss:  1.14843822e-02, mean val. rec. loss:  6.67535961e-03\n",
      "Epoch: 4562 mean train loss:  1.14832639e-02, mean val. rec. loss:  6.67459701e-03\n",
      "Epoch: 4563 mean train loss:  1.14821419e-02, mean val. rec. loss:  6.67439346e-03\n",
      "Epoch: 4564 mean train loss:  1.14810237e-02, mean val. rec. loss:  6.67419502e-03\n",
      "Epoch: 4565 mean train loss:  1.14799138e-02, mean val. rec. loss:  6.67354071e-03\n",
      "Epoch: 4566 mean train loss:  1.14788021e-02, mean val. rec. loss:  6.67362009e-03\n",
      "Epoch: 4567 mean train loss:  1.14776960e-02, mean val. rec. loss:  6.67285919e-03\n",
      "Epoch: 4568 mean train loss:  1.14765833e-02, mean val. rec. loss:  6.67281780e-03\n",
      "Epoch: 4569 mean train loss:  1.14754697e-02, mean val. rec. loss:  6.67243849e-03\n",
      "Epoch: 4570 mean train loss:  1.14743673e-02, mean val. rec. loss:  6.67192253e-03\n",
      "Epoch: 4571 mean train loss:  1.14732630e-02, mean val. rec. loss:  6.67192253e-03\n",
      "Epoch: 4572 mean train loss:  1.14721578e-02, mean val. rec. loss:  6.67125178e-03\n",
      "Epoch: 4573 mean train loss:  1.14710563e-02, mean val. rec. loss:  6.67116446e-03\n",
      "Epoch: 4574 mean train loss:  1.14699613e-02, mean val. rec. loss:  6.67071881e-03\n",
      "Epoch: 4575 mean train loss:  1.14688627e-02, mean val. rec. loss:  6.67038712e-03\n",
      "Epoch: 4576 mean train loss:  1.14677612e-02, mean val. rec. loss:  6.67016827e-03\n",
      "Epoch: 4577 mean train loss:  1.14666634e-02, mean val. rec. loss:  6.66963586e-03\n",
      "Epoch: 4578 mean train loss:  1.14655694e-02, mean val. rec. loss:  6.66955195e-03\n",
      "Epoch: 4579 mean train loss:  1.14644828e-02, mean val. rec. loss:  6.66900310e-03\n",
      "Epoch: 4580 mean train loss:  1.14633897e-02, mean val. rec. loss:  6.66879785e-03\n",
      "Epoch: 4581 mean train loss:  1.14623012e-02, mean val. rec. loss:  6.66846503e-03\n",
      "Epoch: 4582 mean train loss:  1.14612044e-02, mean val. rec. loss:  6.66801825e-03\n",
      "Epoch: 4583 mean train loss:  1.14601215e-02, mean val. rec. loss:  6.66790938e-03\n",
      "Epoch: 4584 mean train loss:  1.14590322e-02, mean val. rec. loss:  6.66735147e-03\n",
      "Epoch: 4585 mean train loss:  1.14579511e-02, mean val. rec. loss:  6.66718477e-03\n",
      "Epoch: 4586 mean train loss:  1.14568646e-02, mean val. rec. loss:  6.66681283e-03\n",
      "Epoch: 4587 mean train loss:  1.14557901e-02, mean val. rec. loss:  6.66645959e-03\n",
      "Epoch: 4588 mean train loss:  1.14547081e-02, mean val. rec. loss:  6.66619595e-03\n",
      "Epoch: 4589 mean train loss:  1.14536262e-02, mean val. rec. loss:  6.66578431e-03\n",
      "Epoch: 4590 mean train loss:  1.14525555e-02, mean val. rec. loss:  6.66562499e-03\n",
      "Epoch: 4591 mean train loss:  1.14514800e-02, mean val. rec. loss:  6.66511413e-03\n",
      "Epoch: 4592 mean train loss:  1.14503981e-02, mean val. rec. loss:  6.66493270e-03\n",
      "Epoch: 4593 mean train loss:  1.14493246e-02, mean val. rec. loss:  6.66458286e-03\n",
      "Epoch: 4594 mean train loss:  1.14482547e-02, mean val. rec. loss:  6.66420298e-03\n",
      "Epoch: 4595 mean train loss:  1.14471923e-02, mean val. rec. loss:  6.66398129e-03\n",
      "Epoch: 4596 mean train loss:  1.14461188e-02, mean val. rec. loss:  6.66358100e-03\n",
      "Epoch: 4597 mean train loss:  1.14450508e-02, mean val. rec. loss:  6.66331168e-03\n",
      "Epoch: 4598 mean train loss:  1.14439894e-02, mean val. rec. loss:  6.66292896e-03\n",
      "Epoch: 4599 mean train loss:  1.14429186e-02, mean val. rec. loss:  6.66265284e-03\n",
      "Epoch: 4600 mean train loss:  1.14418544e-02, mean val. rec. loss:  6.66229223e-03\n",
      "Epoch: 4601 mean train loss:  1.14407994e-02, mean val. rec. loss:  6.66198322e-03\n",
      "Epoch: 4602 mean train loss:  1.14397408e-02, mean val. rec. loss:  6.66173998e-03\n",
      "Epoch: 4603 mean train loss:  1.14386821e-02, mean val. rec. loss:  6.66129887e-03\n",
      "Epoch: 4604 mean train loss:  1.14376188e-02, mean val. rec. loss:  6.66112820e-03\n",
      "Epoch: 4605 mean train loss:  1.14365667e-02, mean val. rec. loss:  6.66075966e-03\n",
      "Epoch: 4606 mean train loss:  1.14355145e-02, mean val. rec. loss:  6.66043534e-03\n",
      "Epoch: 4607 mean train loss:  1.14344605e-02, mean val. rec. loss:  6.66015412e-03\n",
      "Epoch: 4608 mean train loss:  1.14334084e-02, mean val. rec. loss:  6.65983774e-03\n",
      "Epoch: 4609 mean train loss:  1.14323563e-02, mean val. rec. loss:  6.65951682e-03\n",
      "Epoch: 4610 mean train loss:  1.14313079e-02, mean val. rec. loss:  6.65917550e-03\n",
      "Epoch: 4611 mean train loss:  1.14302576e-02, mean val. rec. loss:  6.65894133e-03\n",
      "Epoch: 4612 mean train loss:  1.14292110e-02, mean val. rec. loss:  6.65851325e-03\n",
      "Epoch: 4613 mean train loss:  1.14281608e-02, mean val. rec. loss:  6.65825527e-03\n",
      "Epoch: 4614 mean train loss:  1.14271179e-02, mean val. rec. loss:  6.65795761e-03\n",
      "Epoch: 4615 mean train loss:  1.14260751e-02, mean val. rec. loss:  6.65758906e-03\n",
      "Epoch: 4616 mean train loss:  1.14250332e-02, mean val. rec. loss:  6.65732031e-03\n",
      "Epoch: 4617 mean train loss:  1.14239950e-02, mean val. rec. loss:  6.65700790e-03\n",
      "Epoch: 4618 mean train loss:  1.14229541e-02, mean val. rec. loss:  6.65671250e-03\n",
      "Epoch: 4619 mean train loss:  1.14219224e-02, mean val. rec. loss:  6.65637684e-03\n",
      "Epoch: 4620 mean train loss:  1.14208898e-02, mean val. rec. loss:  6.65613701e-03\n",
      "Epoch: 4621 mean train loss:  1.14198554e-02, mean val. rec. loss:  6.65579965e-03\n",
      "Epoch: 4622 mean train loss:  1.14188144e-02, mean val. rec. loss:  6.65548384e-03\n",
      "Epoch: 4623 mean train loss:  1.14177865e-02, mean val. rec. loss:  6.65522359e-03\n",
      "Epoch: 4624 mean train loss:  1.14167567e-02, mean val. rec. loss:  6.65488850e-03\n",
      "Epoch: 4625 mean train loss:  1.14157288e-02, mean val. rec. loss:  6.65455001e-03\n",
      "Epoch: 4626 mean train loss:  1.14146999e-02, mean val. rec. loss:  6.65425971e-03\n",
      "Epoch: 4627 mean train loss:  1.14136738e-02, mean val. rec. loss:  6.65397508e-03\n",
      "Epoch: 4628 mean train loss:  1.14126515e-02, mean val. rec. loss:  6.65363489e-03\n",
      "Epoch: 4629 mean train loss:  1.14116217e-02, mean val. rec. loss:  6.65335990e-03\n",
      "Epoch: 4630 mean train loss:  1.14105975e-02, mean val. rec. loss:  6.65304862e-03\n",
      "Epoch: 4631 mean train loss:  1.14095798e-02, mean val. rec. loss:  6.65275549e-03\n",
      "Epoch: 4632 mean train loss:  1.14085640e-02, mean val. rec. loss:  6.65246122e-03\n",
      "Epoch: 4633 mean train loss:  1.14075426e-02, mean val. rec. loss:  6.65216412e-03\n",
      "Epoch: 4634 mean train loss:  1.14065202e-02, mean val. rec. loss:  6.65183640e-03\n",
      "Epoch: 4635 mean train loss:  1.14055072e-02, mean val. rec. loss:  6.65154724e-03\n",
      "Epoch: 4636 mean train loss:  1.14044951e-02, mean val. rec. loss:  6.65128472e-03\n",
      "Epoch: 4637 mean train loss:  1.14034774e-02, mean val. rec. loss:  6.65088783e-03\n",
      "Epoch: 4638 mean train loss:  1.14024635e-02, mean val. rec. loss:  6.65066784e-03\n",
      "Epoch: 4639 mean train loss:  1.14014551e-02, mean val. rec. loss:  6.65037811e-03\n",
      "Epoch: 4640 mean train loss:  1.14004495e-02, mean val. rec. loss:  6.65003224e-03\n",
      "Epoch: 4641 mean train loss:  1.13994402e-02, mean val. rec. loss:  6.64971983e-03\n",
      "Epoch: 4642 mean train loss:  1.13984262e-02, mean val. rec. loss:  6.64949814e-03\n",
      "Epoch: 4643 mean train loss:  1.13974272e-02, mean val. rec. loss:  6.64915795e-03\n",
      "Epoch: 4644 mean train loss:  1.13964141e-02, mean val. rec. loss:  6.64884724e-03\n",
      "Epoch: 4645 mean train loss:  1.13954197e-02, mean val. rec. loss:  6.64865333e-03\n",
      "Epoch: 4646 mean train loss:  1.13944086e-02, mean val. rec. loss:  6.64828989e-03\n",
      "Epoch: 4647 mean train loss:  1.13934104e-02, mean val. rec. loss:  6.64797804e-03\n",
      "Epoch: 4648 mean train loss:  1.13924160e-02, mean val. rec. loss:  6.64772800e-03\n",
      "Epoch: 4649 mean train loss:  1.13914104e-02, mean val. rec. loss:  6.64741332e-03\n",
      "Epoch: 4650 mean train loss:  1.13904114e-02, mean val. rec. loss:  6.64703684e-03\n",
      "Epoch: 4651 mean train loss:  1.13894179e-02, mean val. rec. loss:  6.64686845e-03\n",
      "Epoch: 4652 mean train loss:  1.13884225e-02, mean val. rec. loss:  6.64648743e-03\n",
      "Epoch: 4653 mean train loss:  1.13874253e-02, mean val. rec. loss:  6.64618069e-03\n",
      "Epoch: 4654 mean train loss:  1.13864421e-02, mean val. rec. loss:  6.64597487e-03\n",
      "Epoch: 4655 mean train loss:  1.13854412e-02, mean val. rec. loss:  6.64562278e-03\n",
      "Epoch: 4656 mean train loss:  1.13844607e-02, mean val. rec. loss:  6.64534382e-03\n",
      "Epoch: 4657 mean train loss:  1.13834626e-02, mean val. rec. loss:  6.64510171e-03\n",
      "Epoch: 4658 mean train loss:  1.13824831e-02, mean val. rec. loss:  6.64479554e-03\n",
      "Epoch: 4659 mean train loss:  1.13814887e-02, mean val. rec. loss:  6.64442530e-03\n",
      "Epoch: 4660 mean train loss:  1.13805082e-02, mean val. rec. loss:  6.64423649e-03\n",
      "Epoch: 4661 mean train loss:  1.13795259e-02, mean val. rec. loss:  6.64388496e-03\n",
      "Epoch: 4662 mean train loss:  1.13785371e-02, mean val. rec. loss:  6.64356858e-03\n",
      "Epoch: 4663 mean train loss:  1.13775622e-02, mean val. rec. loss:  6.64334008e-03\n",
      "Epoch: 4664 mean train loss:  1.13765753e-02, mean val. rec. loss:  6.64305715e-03\n",
      "Epoch: 4665 mean train loss:  1.13756032e-02, mean val. rec. loss:  6.64269428e-03\n",
      "Epoch: 4666 mean train loss:  1.13746172e-02, mean val. rec. loss:  6.64253326e-03\n",
      "Epoch: 4667 mean train loss:  1.13736414e-02, mean val. rec. loss:  6.64214203e-03\n",
      "Epoch: 4668 mean train loss:  1.13726684e-02, mean val. rec. loss:  6.64192148e-03\n",
      "Epoch: 4669 mean train loss:  1.13716963e-02, mean val. rec. loss:  6.64158072e-03\n",
      "Epoch: 4670 mean train loss:  1.13707159e-02, mean val. rec. loss:  6.64139758e-03\n",
      "Epoch: 4671 mean train loss:  1.13697457e-02, mean val. rec. loss:  6.64094002e-03\n",
      "Epoch: 4672 mean train loss:  1.13687746e-02, mean val. rec. loss:  6.64085100e-03\n",
      "Epoch: 4673 mean train loss:  1.13678072e-02, mean val. rec. loss:  6.64038947e-03\n",
      "Epoch: 4674 mean train loss:  1.13668351e-02, mean val. rec. loss:  6.64025056e-03\n",
      "Epoch: 4675 mean train loss:  1.13658686e-02, mean val. rec. loss:  6.63978166e-03\n",
      "Epoch: 4676 mean train loss:  1.13649068e-02, mean val. rec. loss:  6.63980831e-03\n",
      "Epoch: 4677 mean train loss:  1.13639422e-02, mean val. rec. loss:  6.63910581e-03\n",
      "Epoch: 4678 mean train loss:  1.13629729e-02, mean val. rec. loss:  6.63931276e-03\n",
      "Epoch: 4679 mean train loss:  1.13620083e-02, mean val. rec. loss:  6.63851104e-03\n",
      "Epoch: 4680 mean train loss:  1.13610549e-02, mean val. rec. loss:  6.63879567e-03\n",
      "Epoch: 4681 mean train loss:  1.13600921e-02, mean val. rec. loss:  6.63780344e-03\n",
      "Epoch: 4682 mean train loss:  1.13591312e-02, mean val. rec. loss:  6.63845037e-03\n",
      "Epoch: 4683 mean train loss:  1.13581834e-02, mean val. rec. loss:  6.63703120e-03\n",
      "Epoch: 4684 mean train loss:  1.13572309e-02, mean val. rec. loss:  6.63817425e-03\n",
      "Epoch: 4685 mean train loss:  1.13562737e-02, mean val. rec. loss:  6.63617505e-03\n",
      "Epoch: 4686 mean train loss:  1.13553296e-02, mean val. rec. loss:  6.63807219e-03\n",
      "Epoch: 4687 mean train loss:  1.13543966e-02, mean val. rec. loss:  6.63506489e-03\n",
      "Epoch: 4688 mean train loss:  1.13534813e-02, mean val. rec. loss:  6.63836703e-03\n",
      "Epoch: 4689 mean train loss:  1.13525744e-02, mean val. rec. loss:  6.63366556e-03\n",
      "Epoch: 4690 mean train loss:  1.13517122e-02, mean val. rec. loss:  6.63933147e-03\n",
      "Epoch: 4691 mean train loss:  1.13509059e-02, mean val. rec. loss:  6.63182738e-03\n",
      "Epoch: 4692 mean train loss:  1.13502057e-02, mean val. rec. loss:  6.64169468e-03\n",
      "Epoch: 4693 mean train loss:  1.13496862e-02, mean val. rec. loss:  6.62949933e-03\n",
      "Epoch: 4694 mean train loss:  1.13494711e-02, mean val. rec. loss:  6.64699205e-03\n",
      "Epoch: 4695 mean train loss:  1.13497812e-02, mean val. rec. loss:  6.62733457e-03\n",
      "Epoch: 4696 mean train loss:  1.13509823e-02, mean val. rec. loss:  6.65866067e-03\n",
      "Epoch: 4697 mean train loss:  1.13535968e-02, mean val. rec. loss:  6.62820660e-03\n",
      "Epoch: 4698 mean train loss:  1.13583081e-02, mean val. rec. loss:  6.68118031e-03\n",
      "Epoch: 4699 mean train loss:  1.13655418e-02, mean val. rec. loss:  6.63713042e-03\n",
      "Epoch: 4700 mean train loss:  1.13744887e-02, mean val. rec. loss:  6.70515655e-03\n",
      "Epoch: 4701 mean train loss:  1.13813807e-02, mean val. rec. loss:  6.64392351e-03\n",
      "Epoch: 4702 mean train loss:  1.13798630e-02, mean val. rec. loss:  6.68360135e-03\n",
      "Epoch: 4703 mean train loss:  1.13663705e-02, mean val. rec. loss:  6.63000281e-03\n",
      "Epoch: 4704 mean train loss:  1.13479255e-02, mean val. rec. loss:  6.63440208e-03\n",
      "Epoch: 4705 mean train loss:  1.13377076e-02, mean val. rec. loss:  6.64489306e-03\n",
      "Epoch: 4706 mean train loss:  1.13409907e-02, mean val. rec. loss:  6.62919656e-03\n",
      "Epoch: 4707 mean train loss:  1.13502532e-02, mean val. rec. loss:  6.66889935e-03\n",
      "Epoch: 4708 mean train loss:  1.13536601e-02, mean val. rec. loss:  6.62593637e-03\n",
      "Epoch: 4709 mean train loss:  1.13465912e-02, mean val. rec. loss:  6.64134485e-03\n",
      "Epoch: 4710 mean train loss:  1.13361183e-02, mean val. rec. loss:  6.63226510e-03\n",
      "Epoch: 4711 mean train loss:  1.13323622e-02, mean val. rec. loss:  6.62347848e-03\n",
      "Epoch: 4712 mean train loss:  1.13364050e-02, mean val. rec. loss:  6.65331000e-03\n",
      "Epoch: 4713 mean train loss:  1.13405047e-02, mean val. rec. loss:  6.62435448e-03\n",
      "Epoch: 4714 mean train loss:  1.13382356e-02, mean val. rec. loss:  6.63790777e-03\n",
      "Epoch: 4715 mean train loss:  1.13316583e-02, mean val. rec. loss:  6.62739864e-03\n",
      "Epoch: 4716 mean train loss:  1.13277552e-02, mean val. rec. loss:  6.62350740e-03\n",
      "Epoch: 4717 mean train loss:  1.13291081e-02, mean val. rec. loss:  6.64285304e-03\n",
      "Epoch: 4718 mean train loss:  1.13315931e-02, mean val. rec. loss:  6.62210920e-03\n",
      "Epoch: 4719 mean train loss:  1.13305112e-02, mean val. rec. loss:  6.63620737e-03\n",
      "Epoch: 4720 mean train loss:  1.13263762e-02, mean val. rec. loss:  6.62575834e-03\n",
      "Epoch: 4721 mean train loss:  1.13233902e-02, mean val. rec. loss:  6.62249986e-03\n",
      "Epoch: 4722 mean train loss:  1.13236230e-02, mean val. rec. loss:  6.63755000e-03\n",
      "Epoch: 4723 mean train loss:  1.13248250e-02, mean val. rec. loss:  6.62054318e-03\n",
      "Epoch: 4724 mean train loss:  1.13240289e-02, mean val. rec. loss:  6.63145487e-03\n",
      "Epoch: 4725 mean train loss:  1.13212831e-02, mean val. rec. loss:  6.62435108e-03\n",
      "Epoch: 4726 mean train loss:  1.13190513e-02, mean val. rec. loss:  6.62176164e-03\n",
      "Epoch: 4727 mean train loss:  1.13187245e-02, mean val. rec. loss:  6.63189656e-03\n",
      "Epoch: 4728 mean train loss:  1.13191118e-02, mean val. rec. loss:  6.62008732e-03\n",
      "Epoch: 4729 mean train loss:  1.13184107e-02, mean val. rec. loss:  6.62938026e-03\n",
      "Epoch: 4730 mean train loss:  1.13164777e-02, mean val. rec. loss:  6.62247491e-03\n",
      "Epoch: 4731 mean train loss:  1.13147208e-02, mean val. rec. loss:  6.62164994e-03\n",
      "Epoch: 4732 mean train loss:  1.13140830e-02, mean val. rec. loss:  6.62913986e-03\n",
      "Epoch: 4733 mean train loss:  1.13139805e-02, mean val. rec. loss:  6.61912344e-03\n",
      "Epoch: 4734 mean train loss:  1.13133111e-02, mean val. rec. loss:  6.62727447e-03\n",
      "Epoch: 4735 mean train loss:  1.13118874e-02, mean val. rec. loss:  6.62180190e-03\n",
      "Epoch: 4736 mean train loss:  1.13104349e-02, mean val. rec. loss:  6.62097069e-03\n",
      "Epoch: 4737 mean train loss:  1.13096044e-02, mean val. rec. loss:  6.62605941e-03\n",
      "Epoch: 4738 mean train loss:  1.13091863e-02, mean val. rec. loss:  6.61909283e-03\n",
      "Epoch: 4739 mean train loss:  1.13085327e-02, mean val. rec. loss:  6.62527073e-03\n",
      "Epoch: 4740 mean train loss:  1.13074182e-02, mean val. rec. loss:  6.62013722e-03\n",
      "Epoch: 4741 mean train loss:  1.13061761e-02, mean val. rec. loss:  6.62082668e-03\n",
      "Epoch: 4742 mean train loss:  1.13052469e-02, mean val. rec. loss:  6.62341214e-03\n",
      "Epoch: 4743 mean train loss:  1.13046202e-02, mean val. rec. loss:  6.61802802e-03\n",
      "Epoch: 4744 mean train loss:  1.13039554e-02, mean val. rec. loss:  6.62365255e-03\n",
      "Epoch: 4745 mean train loss:  1.13030160e-02, mean val. rec. loss:  6.61879799e-03\n",
      "Epoch: 4746 mean train loss:  1.13019378e-02, mean val. rec. loss:  6.61996769e-03\n",
      "Epoch: 4747 mean train loss:  1.13009694e-02, mean val. rec. loss:  6.62137892e-03\n",
      "Epoch: 4748 mean train loss:  1.13002078e-02, mean val. rec. loss:  6.61773205e-03\n",
      "Epoch: 4749 mean train loss:  1.12994927e-02, mean val. rec. loss:  6.62180757e-03\n",
      "Epoch: 4750 mean train loss:  1.12986612e-02, mean val. rec. loss:  6.61773489e-03\n",
      "Epoch: 4751 mean train loss:  1.12977069e-02, mean val. rec. loss:  6.61958384e-03\n",
      "Epoch: 4752 mean train loss:  1.12967478e-02, mean val. rec. loss:  6.61915349e-03\n",
      "Epoch: 4753 mean train loss:  1.12958987e-02, mean val. rec. loss:  6.61709022e-03\n",
      "Epoch: 4754 mean train loss:  1.12951333e-02, mean val. rec. loss:  6.62009073e-03\n",
      "Epoch: 4755 mean train loss:  1.12943456e-02, mean val. rec. loss:  6.61638319e-03\n",
      "Epoch: 4756 mean train loss:  1.12934760e-02, mean val. rec. loss:  6.61861769e-03\n",
      "Epoch: 4757 mean train loss:  1.12925588e-02, mean val. rec. loss:  6.61740263e-03\n",
      "Epoch: 4758 mean train loss:  1.12916790e-02, mean val. rec. loss:  6.61648355e-03\n",
      "Epoch: 4759 mean train loss:  1.12908624e-02, mean val. rec. loss:  6.61821853e-03\n",
      "Epoch: 4760 mean train loss:  1.12900616e-02, mean val. rec. loss:  6.61550436e-03\n",
      "Epoch: 4761 mean train loss:  1.12892441e-02, mean val. rec. loss:  6.61756763e-03\n",
      "Epoch: 4762 mean train loss:  1.12883754e-02, mean val. rec. loss:  6.61566708e-03\n",
      "Epoch: 4763 mean train loss:  1.12875049e-02, mean val. rec. loss:  6.61597099e-03\n",
      "Epoch: 4764 mean train loss:  1.12866482e-02, mean val. rec. loss:  6.61641607e-03\n",
      "Epoch: 4765 mean train loss:  1.12858382e-02, mean val. rec. loss:  6.61464367e-03\n",
      "Epoch: 4766 mean train loss:  1.12850207e-02, mean val. rec. loss:  6.61640757e-03\n",
      "Epoch: 4767 mean train loss:  1.12841967e-02, mean val. rec. loss:  6.61438229e-03\n",
      "Epoch: 4768 mean train loss:  1.12833429e-02, mean val. rec. loss:  6.61522370e-03\n",
      "Epoch: 4769 mean train loss:  1.12824890e-02, mean val. rec. loss:  6.61473212e-03\n",
      "Epoch: 4770 mean train loss:  1.12816604e-02, mean val. rec. loss:  6.61396498e-03\n",
      "Epoch: 4771 mean train loss:  1.12808410e-02, mean val. rec. loss:  6.61483815e-03\n",
      "Epoch: 4772 mean train loss:  1.12800254e-02, mean val. rec. loss:  6.61320749e-03\n",
      "Epoch: 4773 mean train loss:  1.12791920e-02, mean val. rec. loss:  6.61432899e-03\n",
      "Epoch: 4774 mean train loss:  1.12783624e-02, mean val. rec. loss:  6.61311677e-03\n",
      "Epoch: 4775 mean train loss:  1.12775189e-02, mean val. rec. loss:  6.61323357e-03\n",
      "Epoch: 4776 mean train loss:  1.12766892e-02, mean val. rec. loss:  6.61335831e-03\n",
      "Epoch: 4777 mean train loss:  1.12758764e-02, mean val. rec. loss:  6.61229123e-03\n",
      "Epoch: 4778 mean train loss:  1.12750505e-02, mean val. rec. loss:  6.61314172e-03\n",
      "Epoch: 4779 mean train loss:  1.12742302e-02, mean val. rec. loss:  6.61186486e-03\n",
      "Epoch: 4780 mean train loss:  1.12734090e-02, mean val. rec. loss:  6.61240066e-03\n",
      "Epoch: 4781 mean train loss:  1.12725841e-02, mean val. rec. loss:  6.61176393e-03\n",
      "Epoch: 4782 mean train loss:  1.12717526e-02, mean val. rec. loss:  6.61151219e-03\n",
      "Epoch: 4783 mean train loss:  1.12709341e-02, mean val. rec. loss:  6.61171517e-03\n",
      "Epoch: 4784 mean train loss:  1.12701185e-02, mean val. rec. loss:  6.61081820e-03\n",
      "Epoch: 4785 mean train loss:  1.12693038e-02, mean val. rec. loss:  6.61139766e-03\n",
      "Epoch: 4786 mean train loss:  1.12684891e-02, mean val. rec. loss:  6.61043265e-03\n",
      "Epoch: 4787 mean train loss:  1.12676660e-02, mean val. rec. loss:  6.61073769e-03\n",
      "Epoch: 4788 mean train loss:  1.12668466e-02, mean val. rec. loss:  6.61028636e-03\n",
      "Epoch: 4789 mean train loss:  1.12660366e-02, mean val. rec. loss:  6.60995751e-03\n",
      "Epoch: 4790 mean train loss:  1.12652247e-02, mean val. rec. loss:  6.61012647e-03\n",
      "Epoch: 4791 mean train loss:  1.12644081e-02, mean val. rec. loss:  6.60934630e-03\n",
      "Epoch: 4792 mean train loss:  1.12635971e-02, mean val. rec. loss:  6.60972164e-03\n",
      "Epoch: 4793 mean train loss:  1.12627852e-02, mean val. rec. loss:  6.60899817e-03\n",
      "Epoch: 4794 mean train loss:  1.12619770e-02, mean val. rec. loss:  6.60910986e-03\n",
      "Epoch: 4795 mean train loss:  1.12611632e-02, mean val. rec. loss:  6.60872941e-03\n",
      "Epoch: 4796 mean train loss:  1.12603467e-02, mean val. rec. loss:  6.60848164e-03\n",
      "Epoch: 4797 mean train loss:  1.12595394e-02, mean val. rec. loss:  6.60847824e-03\n",
      "Epoch: 4798 mean train loss:  1.12587396e-02, mean val. rec. loss:  6.60791408e-03\n",
      "Epoch: 4799 mean train loss:  1.12579305e-02, mean val. rec. loss:  6.60811763e-03\n",
      "Epoch: 4800 mean train loss:  1.12571260e-02, mean val. rec. loss:  6.60747070e-03\n",
      "Epoch: 4801 mean train loss:  1.12563187e-02, mean val. rec. loss:  6.60755972e-03\n",
      "Epoch: 4802 mean train loss:  1.12555096e-02, mean val. rec. loss:  6.60720365e-03\n",
      "Epoch: 4803 mean train loss:  1.12547014e-02, mean val. rec. loss:  6.60696098e-03\n",
      "Epoch: 4804 mean train loss:  1.12538970e-02, mean val. rec. loss:  6.60686402e-03\n",
      "Epoch: 4805 mean train loss:  1.12531000e-02, mean val. rec. loss:  6.60647847e-03\n",
      "Epoch: 4806 mean train loss:  1.12522936e-02, mean val. rec. loss:  6.60645522e-03\n",
      "Epoch: 4807 mean train loss:  1.12514966e-02, mean val. rec. loss:  6.60600333e-03\n",
      "Epoch: 4808 mean train loss:  1.12506977e-02, mean val. rec. loss:  6.60604699e-03\n",
      "Epoch: 4809 mean train loss:  1.12498951e-02, mean val. rec. loss:  6.60560304e-03\n",
      "Epoch: 4810 mean train loss:  1.12490925e-02, mean val. rec. loss:  6.60551686e-03\n",
      "Epoch: 4811 mean train loss:  1.12482983e-02, mean val. rec. loss:  6.60529687e-03\n",
      "Epoch: 4812 mean train loss:  1.12475003e-02, mean val. rec. loss:  6.60500657e-03\n",
      "Epoch: 4813 mean train loss:  1.12467015e-02, mean val. rec. loss:  6.60489601e-03\n",
      "Epoch: 4814 mean train loss:  1.12459044e-02, mean val. rec. loss:  6.60455184e-03\n",
      "Epoch: 4815 mean train loss:  1.12451130e-02, mean val. rec. loss:  6.60448834e-03\n",
      "Epoch: 4816 mean train loss:  1.12443141e-02, mean val. rec. loss:  6.60406877e-03\n",
      "Epoch: 4817 mean train loss:  1.12435274e-02, mean val. rec. loss:  6.60406877e-03\n",
      "Epoch: 4818 mean train loss:  1.12427331e-02, mean val. rec. loss:  6.60369229e-03\n",
      "Epoch: 4819 mean train loss:  1.12419398e-02, mean val. rec. loss:  6.60357492e-03\n",
      "Epoch: 4820 mean train loss:  1.12411493e-02, mean val. rec. loss:  6.60337137e-03\n",
      "Epoch: 4821 mean train loss:  1.12403588e-02, mean val. rec. loss:  6.60306407e-03\n",
      "Epoch: 4822 mean train loss:  1.12395656e-02, mean val. rec. loss:  6.60292856e-03\n",
      "Epoch: 4823 mean train loss:  1.12387741e-02, mean val. rec. loss:  6.60264450e-03\n",
      "Epoch: 4824 mean train loss:  1.12379827e-02, mean val. rec. loss:  6.60250615e-03\n",
      "Epoch: 4825 mean train loss:  1.12371950e-02, mean val. rec. loss:  6.60216652e-03\n",
      "Epoch: 4826 mean train loss:  1.12364091e-02, mean val. rec. loss:  6.60212570e-03\n",
      "Epoch: 4827 mean train loss:  1.12356196e-02, mean val. rec. loss:  6.60177020e-03\n",
      "Epoch: 4828 mean train loss:  1.12348356e-02, mean val. rec. loss:  6.60164943e-03\n",
      "Epoch: 4829 mean train loss:  1.12340497e-02, mean val. rec. loss:  6.60141186e-03\n",
      "Epoch: 4830 mean train loss:  1.12332704e-02, mean val. rec. loss:  6.60119641e-03\n",
      "Epoch: 4831 mean train loss:  1.12324836e-02, mean val. rec. loss:  6.60099116e-03\n",
      "Epoch: 4832 mean train loss:  1.12316959e-02, mean val. rec. loss:  6.60074735e-03\n",
      "Epoch: 4833 mean train loss:  1.12309213e-02, mean val. rec. loss:  6.60058519e-03\n",
      "Epoch: 4834 mean train loss:  1.12301373e-02, mean val. rec. loss:  6.60029603e-03\n",
      "Epoch: 4835 mean train loss:  1.12293570e-02, mean val. rec. loss:  6.60019284e-03\n",
      "Epoch: 4836 mean train loss:  1.12285786e-02, mean val. rec. loss:  6.59987306e-03\n",
      "Epoch: 4837 mean train loss:  1.12277909e-02, mean val. rec. loss:  6.59975456e-03\n",
      "Epoch: 4838 mean train loss:  1.12270125e-02, mean val. rec. loss:  6.59948977e-03\n",
      "Epoch: 4839 mean train loss:  1.12262360e-02, mean val. rec. loss:  6.59929700e-03\n",
      "Epoch: 4840 mean train loss:  1.12254557e-02, mean val. rec. loss:  6.59903108e-03\n",
      "Epoch: 4841 mean train loss:  1.12246848e-02, mean val. rec. loss:  6.59889217e-03\n",
      "Epoch: 4842 mean train loss:  1.12239101e-02, mean val. rec. loss:  6.59862682e-03\n",
      "Epoch: 4843 mean train loss:  1.12231308e-02, mean val. rec. loss:  6.59843971e-03\n",
      "Epoch: 4844 mean train loss:  1.12223515e-02, mean val. rec. loss:  6.59826111e-03\n",
      "Epoch: 4845 mean train loss:  1.12215787e-02, mean val. rec. loss:  6.59805359e-03\n",
      "Epoch: 4846 mean train loss:  1.12208096e-02, mean val. rec. loss:  6.59786308e-03\n",
      "Epoch: 4847 mean train loss:  1.12200330e-02, mean val. rec. loss:  6.59762325e-03\n",
      "Epoch: 4848 mean train loss:  1.12192639e-02, mean val. rec. loss:  6.59744975e-03\n",
      "Epoch: 4849 mean train loss:  1.12184930e-02, mean val. rec. loss:  6.59719687e-03\n",
      "Epoch: 4850 mean train loss:  1.12177146e-02, mean val. rec. loss:  6.59698709e-03\n",
      "Epoch: 4851 mean train loss:  1.12169520e-02, mean val. rec. loss:  6.59679771e-03\n",
      "Epoch: 4852 mean train loss:  1.12161727e-02, mean val. rec. loss:  6.59659530e-03\n",
      "Epoch: 4853 mean train loss:  1.12154129e-02, mean val. rec. loss:  6.59632938e-03\n",
      "Epoch: 4854 mean train loss:  1.12146364e-02, mean val. rec. loss:  6.59623072e-03\n",
      "Epoch: 4855 mean train loss:  1.12138701e-02, mean val. rec. loss:  6.59592115e-03\n",
      "Epoch: 4856 mean train loss:  1.12131057e-02, mean val. rec. loss:  6.59580321e-03\n",
      "Epoch: 4857 mean train loss:  1.12123338e-02, mean val. rec. loss:  6.59553673e-03\n",
      "Epoch: 4858 mean train loss:  1.12115731e-02, mean val. rec. loss:  6.59536493e-03\n",
      "Epoch: 4859 mean train loss:  1.12108049e-02, mean val. rec. loss:  6.59508541e-03\n",
      "Epoch: 4860 mean train loss:  1.12100396e-02, mean val. rec. loss:  6.59499356e-03\n",
      "Epoch: 4861 mean train loss:  1.12092807e-02, mean val. rec. loss:  6.59465676e-03\n",
      "Epoch: 4862 mean train loss:  1.12085126e-02, mean val. rec. loss:  6.59458135e-03\n",
      "Epoch: 4863 mean train loss:  1.12077510e-02, mean val. rec. loss:  6.59426611e-03\n",
      "Epoch: 4864 mean train loss:  1.12069875e-02, mean val. rec. loss:  6.59418730e-03\n",
      "Epoch: 4865 mean train loss:  1.12062323e-02, mean val. rec. loss:  6.59385221e-03\n",
      "Epoch: 4866 mean train loss:  1.12054651e-02, mean val. rec. loss:  6.59380628e-03\n",
      "Epoch: 4867 mean train loss:  1.12047119e-02, mean val. rec. loss:  6.59343661e-03\n",
      "Epoch: 4868 mean train loss:  1.12039521e-02, mean val. rec. loss:  6.59342640e-03\n",
      "Epoch: 4869 mean train loss:  1.12031895e-02, mean val. rec. loss:  6.59297508e-03\n",
      "Epoch: 4870 mean train loss:  1.12024307e-02, mean val. rec. loss:  6.59303631e-03\n",
      "Epoch: 4871 mean train loss:  1.12016700e-02, mean val. rec. loss:  6.59255947e-03\n",
      "Epoch: 4872 mean train loss:  1.12009158e-02, mean val. rec. loss:  6.59265813e-03\n",
      "Epoch: 4873 mean train loss:  1.12001653e-02, mean val. rec. loss:  6.59209057e-03\n",
      "Epoch: 4874 mean train loss:  1.11994009e-02, mean val. rec. loss:  6.59235819e-03\n",
      "Epoch: 4875 mean train loss:  1.11986467e-02, mean val. rec. loss:  6.59154400e-03\n",
      "Epoch: 4876 mean train loss:  1.11978962e-02, mean val. rec. loss:  6.59207640e-03\n",
      "Epoch: 4877 mean train loss:  1.11971486e-02, mean val. rec. loss:  6.59103824e-03\n",
      "Epoch: 4878 mean train loss:  1.11963963e-02, mean val. rec. loss:  6.59185131e-03\n",
      "Epoch: 4879 mean train loss:  1.11956421e-02, mean val. rec. loss:  6.59041966e-03\n",
      "Epoch: 4880 mean train loss:  1.11949009e-02, mean val. rec. loss:  6.59183033e-03\n",
      "Epoch: 4881 mean train loss:  1.11941607e-02, mean val. rec. loss:  6.58960603e-03\n",
      "Epoch: 4882 mean train loss:  1.11934223e-02, mean val. rec. loss:  6.59199305e-03\n",
      "Epoch: 4883 mean train loss:  1.11926970e-02, mean val. rec. loss:  6.58859396e-03\n",
      "Epoch: 4884 mean train loss:  1.11919959e-02, mean val. rec. loss:  6.59254643e-03\n",
      "Epoch: 4885 mean train loss:  1.11913115e-02, mean val. rec. loss:  6.58725246e-03\n",
      "Epoch: 4886 mean train loss:  1.11906784e-02, mean val. rec. loss:  6.59396617e-03\n",
      "Epoch: 4887 mean train loss:  1.11901281e-02, mean val. rec. loss:  6.58542846e-03\n",
      "Epoch: 4888 mean train loss:  1.11897222e-02, mean val. rec. loss:  6.59708631e-03\n",
      "Epoch: 4889 mean train loss:  1.11895527e-02, mean val. rec. loss:  6.58324329e-03\n",
      "Epoch: 4890 mean train loss:  1.11897836e-02, mean val. rec. loss:  6.60373481e-03\n",
      "Epoch: 4891 mean train loss:  1.11907073e-02, mean val. rec. loss:  6.58171696e-03\n",
      "Epoch: 4892 mean train loss:  1.11927547e-02, mean val. rec. loss:  6.61792710e-03\n",
      "Epoch: 4893 mean train loss:  1.11965406e-02, mean val. rec. loss:  6.58434892e-03\n",
      "Epoch: 4894 mean train loss:  1.12026979e-02, mean val. rec. loss:  6.64334802e-03\n",
      "Epoch: 4895 mean train loss:  1.12112509e-02, mean val. rec. loss:  6.59500546e-03\n",
      "Epoch: 4896 mean train loss:  1.12202872e-02, mean val. rec. loss:  6.66304009e-03\n",
      "Epoch: 4897 mean train loss:  1.12246438e-02, mean val. rec. loss:  6.59698425e-03\n",
      "Epoch: 4898 mean train loss:  1.12182099e-02, mean val. rec. loss:  6.62936098e-03\n",
      "Epoch: 4899 mean train loss:  1.12012398e-02, mean val. rec. loss:  6.58367193e-03\n",
      "Epoch: 4900 mean train loss:  1.11845518e-02, mean val. rec. loss:  6.58522775e-03\n",
      "Epoch: 4901 mean train loss:  1.11796989e-02, mean val. rec. loss:  6.60736411e-03\n",
      "Epoch: 4902 mean train loss:  1.11867650e-02, mean val. rec. loss:  6.58459953e-03\n",
      "Epoch: 4903 mean train loss:  1.11954000e-02, mean val. rec. loss:  6.62368373e-03\n",
      "Epoch: 4904 mean train loss:  1.11953087e-02, mean val. rec. loss:  6.57987765e-03\n",
      "Epoch: 4905 mean train loss:  1.11860313e-02, mean val. rec. loss:  6.59175492e-03\n",
      "Epoch: 4906 mean train loss:  1.11770108e-02, mean val. rec. loss:  6.59144421e-03\n",
      "Epoch: 4907 mean train loss:  1.11762604e-02, mean val. rec. loss:  6.57882191e-03\n",
      "Epoch: 4908 mean train loss:  1.11814717e-02, mean val. rec. loss:  6.60995524e-03\n",
      "Epoch: 4909 mean train loss:  1.11842874e-02, mean val. rec. loss:  6.57921314e-03\n",
      "Epoch: 4910 mean train loss:  1.11804680e-02, mean val. rec. loss:  6.59105072e-03\n",
      "Epoch: 4911 mean train loss:  1.11741431e-02, mean val. rec. loss:  6.58534682e-03\n",
      "Epoch: 4912 mean train loss:  1.11717716e-02, mean val. rec. loss:  6.57879527e-03\n",
      "Epoch: 4913 mean train loss:  1.11740760e-02, mean val. rec. loss:  6.60008171e-03\n",
      "Epoch: 4914 mean train loss:  1.11762027e-02, mean val. rec. loss:  6.57793515e-03\n",
      "Epoch: 4915 mean train loss:  1.11744354e-02, mean val. rec. loss:  6.59069352e-03\n",
      "Epoch: 4916 mean train loss:  1.11703861e-02, mean val. rec. loss:  6.58285377e-03\n",
      "Epoch: 4917 mean train loss:  1.11681692e-02, mean val. rec. loss:  6.57819483e-03\n",
      "Epoch: 4918 mean train loss:  1.11689662e-02, mean val. rec. loss:  6.59424456e-03\n",
      "Epoch: 4919 mean train loss:  1.11701580e-02, mean val. rec. loss:  6.57648366e-03\n",
      "Epoch: 4920 mean train loss:  1.11691692e-02, mean val. rec. loss:  6.58734602e-03\n",
      "Epoch: 4921 mean train loss:  1.11665360e-02, mean val. rec. loss:  6.58116244e-03\n",
      "Epoch: 4922 mean train loss:  1.11647111e-02, mean val. rec. loss:  6.57784273e-03\n",
      "Epoch: 4923 mean train loss:  1.11647195e-02, mean val. rec. loss:  6.58908554e-03\n",
      "Epoch: 4924 mean train loss:  1.11652232e-02, mean val. rec. loss:  6.57668550e-03\n",
      "Epoch: 4925 mean train loss:  1.11645686e-02, mean val. rec. loss:  6.58595066e-03\n",
      "Epoch: 4926 mean train loss:  1.11628005e-02, mean val. rec. loss:  6.57974214e-03\n",
      "Epoch: 4927 mean train loss:  1.11612940e-02, mean val. rec. loss:  6.57862460e-03\n",
      "Epoch: 4928 mean train loss:  1.11608545e-02, mean val. rec. loss:  6.58615988e-03\n",
      "Epoch: 4929 mean train loss:  1.11609066e-02, mean val. rec. loss:  6.57624325e-03\n",
      "Epoch: 4930 mean train loss:  1.11603964e-02, mean val. rec. loss:  6.58459556e-03\n",
      "Epoch: 4931 mean train loss:  1.11591552e-02, mean val. rec. loss:  6.57872893e-03\n",
      "Epoch: 4932 mean train loss:  1.11578917e-02, mean val. rec. loss:  6.57827761e-03\n",
      "Epoch: 4933 mean train loss:  1.11572195e-02, mean val. rec. loss:  6.58344967e-03\n",
      "Epoch: 4934 mean train loss:  1.11569560e-02, mean val. rec. loss:  6.57618882e-03\n",
      "Epoch: 4935 mean train loss:  1.11564895e-02, mean val. rec. loss:  6.58283619e-03\n",
      "Epoch: 4936 mean train loss:  1.11555687e-02, mean val. rec. loss:  6.57759609e-03\n",
      "Epoch: 4937 mean train loss:  1.11545109e-02, mean val. rec. loss:  6.57843409e-03\n",
      "Epoch: 4938 mean train loss:  1.11537223e-02, mean val. rec. loss:  6.58084266e-03\n",
      "Epoch: 4939 mean train loss:  1.11532400e-02, mean val. rec. loss:  6.57589455e-03\n",
      "Epoch: 4940 mean train loss:  1.11527512e-02, mean val. rec. loss:  6.58151624e-03\n",
      "Epoch: 4941 mean train loss:  1.11520249e-02, mean val. rec. loss:  6.57642185e-03\n",
      "Epoch: 4942 mean train loss:  1.11511264e-02, mean val. rec. loss:  6.57834451e-03\n",
      "Epoch: 4943 mean train loss:  1.11502996e-02, mean val. rec. loss:  6.57915757e-03\n",
      "Epoch: 4944 mean train loss:  1.11496776e-02, mean val. rec. loss:  6.57577549e-03\n",
      "Epoch: 4945 mean train loss:  1.11491320e-02, mean val. rec. loss:  6.58012202e-03\n",
      "Epoch: 4946 mean train loss:  1.11485007e-02, mean val. rec. loss:  6.57576245e-03\n",
      "Epoch: 4947 mean train loss:  1.11477205e-02, mean val. rec. loss:  6.57790169e-03\n",
      "Epoch: 4948 mean train loss:  1.11469346e-02, mean val. rec. loss:  6.57722868e-03\n",
      "Epoch: 4949 mean train loss:  1.11462279e-02, mean val. rec. loss:  6.57558271e-03\n",
      "Epoch: 4950 mean train loss:  1.11456143e-02, mean val. rec. loss:  6.57830369e-03\n",
      "Epoch: 4951 mean train loss:  1.11449923e-02, mean val. rec. loss:  6.57483315e-03\n",
      "Epoch: 4952 mean train loss:  1.11443145e-02, mean val. rec. loss:  6.57746965e-03\n",
      "Epoch: 4953 mean train loss:  1.11435678e-02, mean val. rec. loss:  6.57568080e-03\n",
      "Epoch: 4954 mean train loss:  1.11428341e-02, mean val. rec. loss:  6.57544380e-03\n",
      "Epoch: 4955 mean train loss:  1.11421655e-02, mean val. rec. loss:  6.57691457e-03\n",
      "Epoch: 4956 mean train loss:  1.11415305e-02, mean val. rec. loss:  6.57425879e-03\n",
      "Epoch: 4957 mean train loss:  1.11408806e-02, mean val. rec. loss:  6.57664581e-03\n",
      "Epoch: 4958 mean train loss:  1.11401907e-02, mean val. rec. loss:  6.57453945e-03\n",
      "Epoch: 4959 mean train loss:  1.11394840e-02, mean val. rec. loss:  6.57514783e-03\n",
      "Epoch: 4960 mean train loss:  1.11387838e-02, mean val. rec. loss:  6.57532020e-03\n",
      "Epoch: 4961 mean train loss:  1.11381218e-02, mean val. rec. loss:  6.57395262e-03\n",
      "Epoch: 4962 mean train loss:  1.11374672e-02, mean val. rec. loss:  6.57554019e-03\n",
      "Epoch: 4963 mean train loss:  1.11368117e-02, mean val. rec. loss:  6.57351831e-03\n",
      "Epoch: 4964 mean train loss:  1.11361218e-02, mean val. rec. loss:  6.57478496e-03\n",
      "Epoch: 4965 mean train loss:  1.11354374e-02, mean val. rec. loss:  6.57387721e-03\n",
      "Epoch: 4966 mean train loss:  1.11347522e-02, mean val. rec. loss:  6.57355119e-03\n",
      "Epoch: 4967 mean train loss:  1.11340771e-02, mean val. rec. loss:  6.57429111e-03\n",
      "Epoch: 4968 mean train loss:  1.11334226e-02, mean val. rec. loss:  6.57279937e-03\n",
      "Epoch: 4969 mean train loss:  1.11327559e-02, mean val. rec. loss:  6.57405751e-03\n",
      "Epoch: 4970 mean train loss:  1.11320902e-02, mean val. rec. loss:  6.57276081e-03\n",
      "Epoch: 4971 mean train loss:  1.11314105e-02, mean val. rec. loss:  6.57322574e-03\n",
      "Epoch: 4972 mean train loss:  1.11307298e-02, mean val. rec. loss:  6.57300688e-03\n",
      "Epoch: 4973 mean train loss:  1.11300641e-02, mean val. rec. loss:  6.57237015e-03\n",
      "Epoch: 4974 mean train loss:  1.11294002e-02, mean val. rec. loss:  6.57306585e-03\n",
      "Epoch: 4975 mean train loss:  1.11287364e-02, mean val. rec. loss:  6.57182868e-03\n",
      "Epoch: 4976 mean train loss:  1.11280706e-02, mean val. rec. loss:  6.57264855e-03\n",
      "Epoch: 4977 mean train loss:  1.11273928e-02, mean val. rec. loss:  6.57179466e-03\n",
      "Epoch: 4978 mean train loss:  1.11267205e-02, mean val. rec. loss:  6.57186270e-03\n",
      "Epoch: 4979 mean train loss:  1.11260557e-02, mean val. rec. loss:  6.57194094e-03\n",
      "Epoch: 4980 mean train loss:  1.11253937e-02, mean val. rec. loss:  6.57122087e-03\n",
      "Epoch: 4981 mean train loss:  1.11247289e-02, mean val. rec. loss:  6.57177085e-03\n",
      "Epoch: 4982 mean train loss:  1.11240706e-02, mean val. rec. loss:  6.57092604e-03\n",
      "Epoch: 4983 mean train loss:  1.11234012e-02, mean val. rec. loss:  6.57132860e-03\n",
      "Epoch: 4984 mean train loss:  1.11227317e-02, mean val. rec. loss:  6.57078656e-03\n",
      "Epoch: 4985 mean train loss:  1.11220632e-02, mean val. rec. loss:  6.57074687e-03\n",
      "Epoch: 4986 mean train loss:  1.11214030e-02, mean val. rec. loss:  6.57075481e-03\n",
      "Epoch: 4987 mean train loss:  1.11207392e-02, mean val. rec. loss:  6.57016344e-03\n",
      "Epoch: 4988 mean train loss:  1.11200818e-02, mean val. rec. loss:  6.57057224e-03\n",
      "Epoch: 4989 mean train loss:  1.11194170e-02, mean val. rec. loss:  6.56983515e-03\n",
      "Epoch: 4990 mean train loss:  1.11187597e-02, mean val. rec. loss:  6.57013565e-03\n",
      "Epoch: 4991 mean train loss:  1.11180967e-02, mean val. rec. loss:  6.56974046e-03\n",
      "Epoch: 4992 mean train loss:  1.11174282e-02, mean val. rec. loss:  6.56960212e-03\n",
      "Epoch: 4993 mean train loss:  1.11167653e-02, mean val. rec. loss:  6.56956753e-03\n",
      "Epoch: 4994 mean train loss:  1.11161107e-02, mean val. rec. loss:  6.56916554e-03\n",
      "Epoch: 4995 mean train loss:  1.11154533e-02, mean val. rec. loss:  6.56934017e-03\n",
      "Epoch: 4996 mean train loss:  1.11147876e-02, mean val. rec. loss:  6.56877772e-03\n",
      "Epoch: 4997 mean train loss:  1.11141265e-02, mean val. rec. loss:  6.56901755e-03\n",
      "Epoch: 4998 mean train loss:  1.11134748e-02, mean val. rec. loss:  6.56858721e-03\n",
      "Epoch: 4999 mean train loss:  1.11128100e-02, mean val. rec. loss:  6.56852881e-03\n",
      "Epoch: 5000 mean train loss:  1.11121508e-02, mean val. rec. loss:  6.56840747e-03\n",
      "Epoch: 5001 mean train loss:  1.11114953e-02, mean val. rec. loss:  6.56811151e-03\n",
      "Epoch: 5002 mean train loss:  1.11108398e-02, mean val. rec. loss:  6.56815913e-03\n",
      "Epoch: 5003 mean train loss:  1.11101852e-02, mean val. rec. loss:  6.56777301e-03\n",
      "Epoch: 5004 mean train loss:  1.11095279e-02, mean val. rec. loss:  6.56785806e-03\n",
      "Epoch: 5005 mean train loss:  1.11088658e-02, mean val. rec. loss:  6.56747081e-03\n",
      "Epoch: 5006 mean train loss:  1.11082141e-02, mean val. rec. loss:  6.56752014e-03\n",
      "Epoch: 5007 mean train loss:  1.11075567e-02, mean val. rec. loss:  6.56721510e-03\n",
      "Epoch: 5008 mean train loss:  1.11069059e-02, mean val. rec. loss:  6.56706711e-03\n",
      "Epoch: 5009 mean train loss:  1.11062420e-02, mean val. rec. loss:  6.56700474e-03\n",
      "Epoch: 5010 mean train loss:  1.11055940e-02, mean val. rec. loss:  6.56670311e-03\n",
      "Epoch: 5011 mean train loss:  1.11049348e-02, mean val. rec. loss:  6.56670197e-03\n",
      "Epoch: 5012 mean train loss:  1.11042811e-02, mean val. rec. loss:  6.56637879e-03\n",
      "Epoch: 5013 mean train loss:  1.11036322e-02, mean val. rec. loss:  6.56639240e-03\n",
      "Epoch: 5014 mean train loss:  1.11029813e-02, mean val. rec. loss:  6.56606921e-03\n",
      "Epoch: 5015 mean train loss:  1.11023221e-02, mean val. rec. loss:  6.56602499e-03\n",
      "Epoch: 5016 mean train loss:  1.11016759e-02, mean val. rec. loss:  6.56583391e-03\n",
      "Epoch: 5017 mean train loss:  1.11010139e-02, mean val. rec. loss:  6.56567232e-03\n",
      "Epoch: 5018 mean train loss:  1.11003705e-02, mean val. rec. loss:  6.56558160e-03\n",
      "Epoch: 5019 mean train loss:  1.10997122e-02, mean val. rec. loss:  6.56534460e-03\n",
      "Epoch: 5020 mean train loss:  1.10990633e-02, mean val. rec. loss:  6.56527033e-03\n",
      "Epoch: 5021 mean train loss:  1.10984152e-02, mean val. rec. loss:  6.56502709e-03\n",
      "Epoch: 5022 mean train loss:  1.10977616e-02, mean val. rec. loss:  6.56497323e-03\n",
      "Epoch: 5023 mean train loss:  1.10971136e-02, mean val. rec. loss:  6.56465685e-03\n",
      "Epoch: 5024 mean train loss:  1.10964599e-02, mean val. rec. loss:  6.56465968e-03\n",
      "Epoch: 5025 mean train loss:  1.10958138e-02, mean val. rec. loss:  6.56439150e-03\n",
      "Epoch: 5026 mean train loss:  1.10951638e-02, mean val. rec. loss:  6.56427186e-03\n",
      "Epoch: 5027 mean train loss:  1.10945195e-02, mean val. rec. loss:  6.56412898e-03\n",
      "Epoch: 5028 mean train loss:  1.10938706e-02, mean val. rec. loss:  6.56397022e-03\n",
      "Epoch: 5029 mean train loss:  1.10932179e-02, mean val. rec. loss:  6.56381260e-03\n",
      "Epoch: 5030 mean train loss:  1.10925661e-02, mean val. rec. loss:  6.56364931e-03\n",
      "Epoch: 5031 mean train loss:  1.10919190e-02, mean val. rec. loss:  6.56356199e-03\n",
      "Epoch: 5032 mean train loss:  1.10912765e-02, mean val. rec. loss:  6.56330345e-03\n",
      "Epoch: 5033 mean train loss:  1.10906257e-02, mean val. rec. loss:  6.56324108e-03\n",
      "Epoch: 5034 mean train loss:  1.10899776e-02, mean val. rec. loss:  6.56300975e-03\n",
      "Epoch: 5035 mean train loss:  1.10893343e-02, mean val. rec. loss:  6.56290202e-03\n",
      "Epoch: 5036 mean train loss:  1.10886927e-02, mean val. rec. loss:  6.56272285e-03\n",
      "Epoch: 5037 mean train loss:  1.10880494e-02, mean val. rec. loss:  6.56260492e-03\n",
      "Epoch: 5038 mean train loss:  1.10874041e-02, mean val. rec. loss:  6.56241441e-03\n",
      "Epoch: 5039 mean train loss:  1.10867533e-02, mean val. rec. loss:  6.56230781e-03\n",
      "Epoch: 5040 mean train loss:  1.10861080e-02, mean val. rec. loss:  6.56209803e-03\n",
      "Epoch: 5041 mean train loss:  1.10854600e-02, mean val. rec. loss:  6.56199427e-03\n",
      "Epoch: 5042 mean train loss:  1.10848194e-02, mean val. rec. loss:  6.56182134e-03\n",
      "Epoch: 5043 mean train loss:  1.10841806e-02, mean val. rec. loss:  6.56168016e-03\n",
      "Epoch: 5044 mean train loss:  1.10835326e-02, mean val. rec. loss:  6.56154011e-03\n",
      "Epoch: 5045 mean train loss:  1.10828883e-02, mean val. rec. loss:  6.56135641e-03\n",
      "Epoch: 5046 mean train loss:  1.10822477e-02, mean val. rec. loss:  6.56121806e-03\n",
      "Epoch: 5047 mean train loss:  1.10816062e-02, mean val. rec. loss:  6.56104059e-03\n",
      "Epoch: 5048 mean train loss:  1.10809609e-02, mean val. rec. loss:  6.56087447e-03\n",
      "Epoch: 5049 mean train loss:  1.10803166e-02, mean val. rec. loss:  6.56077354e-03\n",
      "Epoch: 5050 mean train loss:  1.10796816e-02, mean val. rec. loss:  6.56059211e-03\n",
      "Epoch: 5051 mean train loss:  1.10790336e-02, mean val. rec. loss:  6.56043335e-03\n",
      "Epoch: 5052 mean train loss:  1.10783985e-02, mean val. rec. loss:  6.56035794e-03\n",
      "Epoch: 5053 mean train loss:  1.10777505e-02, mean val. rec. loss:  6.56014645e-03\n",
      "Epoch: 5054 mean train loss:  1.10771174e-02, mean val. rec. loss:  6.55997352e-03\n",
      "Epoch: 5055 mean train loss:  1.10764749e-02, mean val. rec. loss:  6.55993553e-03\n",
      "Epoch: 5056 mean train loss:  1.10758324e-02, mean val. rec. loss:  6.55964410e-03\n",
      "Epoch: 5057 mean train loss:  1.10751974e-02, mean val. rec. loss:  6.55957039e-03\n",
      "Epoch: 5058 mean train loss:  1.10745522e-02, mean val. rec. loss:  6.55941164e-03\n",
      "Epoch: 5059 mean train loss:  1.10739125e-02, mean val. rec. loss:  6.55922680e-03\n",
      "Epoch: 5060 mean train loss:  1.10732719e-02, mean val. rec. loss:  6.55906464e-03\n",
      "Epoch: 5061 mean train loss:  1.10726341e-02, mean val. rec. loss:  6.55897903e-03\n",
      "Epoch: 5062 mean train loss:  1.10720001e-02, mean val. rec. loss:  6.55874826e-03\n",
      "Epoch: 5063 mean train loss:  1.10713623e-02, mean val. rec. loss:  6.55864847e-03\n",
      "Epoch: 5064 mean train loss:  1.10707273e-02, mean val. rec. loss:  6.55850219e-03\n",
      "Epoch: 5065 mean train loss:  1.10700913e-02, mean val. rec. loss:  6.55836498e-03\n",
      "Epoch: 5066 mean train loss:  1.10694517e-02, mean val. rec. loss:  6.55816653e-03\n",
      "Epoch: 5067 mean train loss:  1.10688157e-02, mean val. rec. loss:  6.55812571e-03\n",
      "Epoch: 5068 mean train loss:  1.10681723e-02, mean val. rec. loss:  6.55781557e-03\n",
      "Epoch: 5069 mean train loss:  1.10675420e-02, mean val. rec. loss:  6.55785185e-03\n",
      "Epoch: 5070 mean train loss:  1.10668995e-02, mean val. rec. loss:  6.55754795e-03\n",
      "Epoch: 5071 mean train loss:  1.10662720e-02, mean val. rec. loss:  6.55751336e-03\n",
      "Epoch: 5072 mean train loss:  1.10656351e-02, mean val. rec. loss:  6.55724971e-03\n",
      "Epoch: 5073 mean train loss:  1.10649945e-02, mean val. rec. loss:  6.55724177e-03\n",
      "Epoch: 5074 mean train loss:  1.10643614e-02, mean val. rec. loss:  6.55686473e-03\n",
      "Epoch: 5075 mean train loss:  1.10637319e-02, mean val. rec. loss:  6.55706771e-03\n",
      "Epoch: 5076 mean train loss:  1.10630895e-02, mean val. rec. loss:  6.55646216e-03\n",
      "Epoch: 5077 mean train loss:  1.10624638e-02, mean val. rec. loss:  6.55689137e-03\n",
      "Epoch: 5078 mean train loss:  1.10618325e-02, mean val. rec. loss:  6.55610836e-03\n",
      "Epoch: 5079 mean train loss:  1.10611919e-02, mean val. rec. loss:  6.55668669e-03\n",
      "Epoch: 5080 mean train loss:  1.10605653e-02, mean val. rec. loss:  6.55562529e-03\n",
      "Epoch: 5081 mean train loss:  1.10599321e-02, mean val. rec. loss:  6.55668612e-03\n",
      "Epoch: 5082 mean train loss:  1.10593074e-02, mean val. rec. loss:  6.55494831e-03\n",
      "Epoch: 5083 mean train loss:  1.10586901e-02, mean val. rec. loss:  6.55690101e-03\n",
      "Epoch: 5084 mean train loss:  1.10580783e-02, mean val. rec. loss:  6.55415792e-03\n",
      "Epoch: 5085 mean train loss:  1.10574750e-02, mean val. rec. loss:  6.55739996e-03\n",
      "Epoch: 5086 mean train loss:  1.10569005e-02, mean val. rec. loss:  6.55302678e-03\n",
      "Epoch: 5087 mean train loss:  1.10563670e-02, mean val. rec. loss:  6.55875053e-03\n",
      "Epoch: 5088 mean train loss:  1.10559107e-02, mean val. rec. loss:  6.55136381e-03\n",
      "Epoch: 5089 mean train loss:  1.10555662e-02, mean val. rec. loss:  6.56168810e-03\n",
      "Epoch: 5090 mean train loss:  1.10554526e-02, mean val. rec. loss:  6.54934589e-03\n",
      "Epoch: 5091 mean train loss:  1.10557329e-02, mean val. rec. loss:  6.56810470e-03\n",
      "Epoch: 5092 mean train loss:  1.10566966e-02, mean val. rec. loss:  6.54800156e-03\n",
      "Epoch: 5093 mean train loss:  1.10588344e-02, mean val. rec. loss:  6.58249600e-03\n",
      "Epoch: 5094 mean train loss:  1.10628818e-02, mean val. rec. loss:  6.55124701e-03\n",
      "Epoch: 5095 mean train loss:  1.10697487e-02, mean val. rec. loss:  6.61096335e-03\n",
      "Epoch: 5096 mean train loss:  1.10799106e-02, mean val. rec. loss:  6.56473566e-03\n",
      "Epoch: 5097 mean train loss:  1.10918361e-02, mean val. rec. loss:  6.63926230e-03\n",
      "Epoch: 5098 mean train loss:  1.10997039e-02, mean val. rec. loss:  6.56970474e-03\n",
      "Epoch: 5099 mean train loss:  1.10948557e-02, mean val. rec. loss:  6.60534676e-03\n",
      "Epoch: 5100 mean train loss:  1.10752617e-02, mean val. rec. loss:  6.55046003e-03\n",
      "Epoch: 5101 mean train loss:  1.10538670e-02, mean val. rec. loss:  6.55162292e-03\n",
      "Epoch: 5102 mean train loss:  1.10471389e-02, mean val. rec. loss:  6.57712549e-03\n",
      "Epoch: 5103 mean train loss:  1.10563428e-02, mean val. rec. loss:  6.55355918e-03\n",
      "Epoch: 5104 mean train loss:  1.10674600e-02, mean val. rec. loss:  6.59462388e-03\n",
      "Epoch: 5105 mean train loss:  1.10665457e-02, mean val. rec. loss:  6.54652569e-03\n",
      "Epoch: 5106 mean train loss:  1.10542310e-02, mean val. rec. loss:  6.55580389e-03\n",
      "Epoch: 5107 mean train loss:  1.10443689e-02, mean val. rec. loss:  6.56214339e-03\n",
      "Epoch: 5108 mean train loss:  1.10461715e-02, mean val. rec. loss:  6.54628132e-03\n",
      "Epoch: 5109 mean train loss:  1.10535327e-02, mean val. rec. loss:  6.58005001e-03\n",
      "Epoch: 5110 mean train loss:  1.10548530e-02, mean val. rec. loss:  6.54557032e-03\n",
      "Epoch: 5111 mean train loss:  1.10479052e-02, mean val. rec. loss:  6.55299787e-03\n",
      "Epoch: 5112 mean train loss:  1.10413428e-02, mean val. rec. loss:  6.55777134e-03\n",
      "Epoch: 5113 mean train loss:  1.10419592e-02, mean val. rec. loss:  6.54524770e-03\n",
      "Epoch: 5114 mean train loss:  1.10462041e-02, mean val. rec. loss:  6.56924208e-03\n",
      "Epoch: 5115 mean train loss:  1.10465774e-02, mean val. rec. loss:  6.54516265e-03\n",
      "Epoch: 5116 mean train loss:  1.10419834e-02, mean val. rec. loss:  6.55180039e-03\n",
      "Epoch: 5117 mean train loss:  1.10380896e-02, mean val. rec. loss:  6.55554931e-03\n",
      "Epoch: 5118 mean train loss:  1.10386780e-02, mean val. rec. loss:  6.54423903e-03\n",
      "Epoch: 5119 mean train loss:  1.10409555e-02, mean val. rec. loss:  6.56277331e-03\n",
      "Epoch: 5120 mean train loss:  1.10404843e-02, mean val. rec. loss:  6.54402868e-03\n",
      "Epoch: 5121 mean train loss:  1.10372926e-02, mean val. rec. loss:  6.54874659e-03\n",
      "Epoch: 5122 mean train loss:  1.10350533e-02, mean val. rec. loss:  6.55413468e-03\n",
      "Epoch: 5123 mean train loss:  1.10355067e-02, mean val. rec. loss:  6.54309258e-03\n",
      "Epoch: 5124 mean train loss:  1.10365440e-02, mean val. rec. loss:  6.55746914e-03\n",
      "Epoch: 5125 mean train loss:  1.10356911e-02, mean val. rec. loss:  6.54516436e-03\n",
      "Epoch: 5126 mean train loss:  1.10334751e-02, mean val. rec. loss:  6.54763075e-03\n",
      "Epoch: 5127 mean train loss:  1.10321175e-02, mean val. rec. loss:  6.55281473e-03\n",
      "Epoch: 5128 mean train loss:  1.10323280e-02, mean val. rec. loss:  6.54406383e-03\n",
      "Epoch: 5129 mean train loss:  1.10326259e-02, mean val. rec. loss:  6.55435183e-03\n",
      "Epoch: 5130 mean train loss:  1.10317265e-02, mean val. rec. loss:  6.54520121e-03\n",
      "Epoch: 5131 mean train loss:  1.10301492e-02, mean val. rec. loss:  6.54715108e-03\n",
      "Epoch: 5132 mean train loss:  1.10292190e-02, mean val. rec. loss:  6.55116139e-03\n",
      "Epoch: 5133 mean train loss:  1.10291687e-02, mean val. rec. loss:  6.54351839e-03\n",
      "Epoch: 5134 mean train loss:  1.10290617e-02, mean val. rec. loss:  6.55203002e-03\n",
      "Epoch: 5135 mean train loss:  1.10282283e-02, mean val. rec. loss:  6.54514508e-03\n",
      "Epoch: 5136 mean train loss:  1.10270607e-02, mean val. rec. loss:  6.54619571e-03\n",
      "Epoch: 5137 mean train loss:  1.10263038e-02, mean val. rec. loss:  6.54989134e-03\n",
      "Epoch: 5138 mean train loss:  1.10260477e-02, mean val. rec. loss:  6.54364936e-03\n",
      "Epoch: 5139 mean train loss:  1.10257414e-02, mean val. rec. loss:  6.54993840e-03\n",
      "Epoch: 5140 mean train loss:  1.10250086e-02, mean val. rec. loss:  6.54503848e-03\n",
      "Epoch: 5141 mean train loss:  1.10240729e-02, mean val. rec. loss:  6.54588500e-03\n",
      "Epoch: 5142 mean train loss:  1.10233783e-02, mean val. rec. loss:  6.54822949e-03\n",
      "Epoch: 5143 mean train loss:  1.10229826e-02, mean val. rec. loss:  6.54379508e-03\n",
      "Epoch 05145: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 5144 mean train loss:  1.10225757e-02, mean val. rec. loss:  6.54878968e-03\n",
      "Epoch: 5145 mean train loss:  1.10219230e-02, mean val. rec. loss:  6.54829753e-03\n",
      "Epoch: 5146 mean train loss:  1.10217805e-02, mean val. rec. loss:  6.54704562e-03\n",
      "Epoch: 5147 mean train loss:  1.10215691e-02, mean val. rec. loss:  6.54567635e-03\n",
      "Epoch: 5148 mean train loss:  1.10214639e-02, mean val. rec. loss:  6.54466541e-03\n",
      "Epoch: 5149 mean train loss:  1.10214854e-02, mean val. rec. loss:  6.54423223e-03\n",
      "Epoch: 5150 mean train loss:  1.10215049e-02, mean val. rec. loss:  6.54438701e-03\n",
      "Epoch: 5151 mean train loss:  1.10214202e-02, mean val. rec. loss:  6.54502601e-03\n",
      "Epoch: 5152 mean train loss:  1.10212675e-02, mean val. rec. loss:  6.54597855e-03\n",
      "Epoch: 5153 mean train loss:  1.10211688e-02, mean val. rec. loss:  6.54686759e-03\n",
      "Epoch: 5154 mean train loss:  1.10211492e-02, mean val. rec. loss:  6.54734896e-03\n",
      "Epoch: 5155 mean train loss:  1.10211399e-02, mean val. rec. loss:  6.54724293e-03\n",
      "Epoch: 5156 mean train loss:  1.10210729e-02, mean val. rec. loss:  6.54662095e-03\n",
      "Epoch: 5157 mean train loss:  1.10209611e-02, mean val. rec. loss:  6.54579768e-03\n",
      "Epoch: 5158 mean train loss:  1.10208718e-02, mean val. rec. loss:  6.54510199e-03\n",
      "Epoch: 5159 mean train loss:  1.10208410e-02, mean val. rec. loss:  6.54471530e-03\n",
      "Epoch: 5160 mean train loss:  1.10208084e-02, mean val. rec. loss:  6.54470283e-03\n",
      "Epoch: 5161 mean train loss:  1.10207470e-02, mean val. rec. loss:  6.54501921e-03\n",
      "Epoch: 5162 mean train loss:  1.10206576e-02, mean val. rec. loss:  6.54555104e-03\n",
      "Epoch: 5163 mean train loss:  1.10205850e-02, mean val. rec. loss:  6.54608855e-03\n",
      "Epoch: 5164 mean train loss:  1.10205328e-02, mean val. rec. loss:  6.54644575e-03\n",
      "Epoch 05166: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch: 5165 mean train loss:  1.10204956e-02, mean val. rec. loss:  6.54647183e-03\n",
      "Epoch: 5166 mean train loss:  1.10204425e-02, mean val. rec. loss:  6.54644291e-03\n",
      "Epoch: 5167 mean train loss:  1.10204341e-02, mean val. rec. loss:  6.54639699e-03\n",
      "Epoch: 5168 mean train loss:  1.10204258e-02, mean val. rec. loss:  6.54631988e-03\n",
      "Epoch: 5169 mean train loss:  1.10204118e-02, mean val. rec. loss:  6.54622916e-03\n",
      "Epoch: 5170 mean train loss:  1.10203950e-02, mean val. rec. loss:  6.54611803e-03\n",
      "Epoch: 5171 mean train loss:  1.10203904e-02, mean val. rec. loss:  6.54600520e-03\n",
      "Epoch: 5172 mean train loss:  1.10203811e-02, mean val. rec. loss:  6.54588953e-03\n",
      "Epoch: 5173 mean train loss:  1.10203699e-02, mean val. rec. loss:  6.54577330e-03\n",
      "Epoch: 5174 mean train loss:  1.10203569e-02, mean val. rec. loss:  6.54566444e-03\n",
      "Epoch: 5175 mean train loss:  1.10203513e-02, mean val. rec. loss:  6.54556805e-03\n",
      "Epoch: 5176 mean train loss:  1.10203531e-02, mean val. rec. loss:  6.54548300e-03\n",
      "Epoch: 5177 mean train loss:  1.10203429e-02, mean val. rec. loss:  6.54541043e-03\n",
      "Epoch: 5178 mean train loss:  1.10203354e-02, mean val. rec. loss:  6.54535827e-03\n",
      "Epoch: 5179 mean train loss:  1.10203299e-02, mean val. rec. loss:  6.54532084e-03\n",
      "Epoch: 5180 mean train loss:  1.10203261e-02, mean val. rec. loss:  6.54529363e-03\n",
      "Epoch: 5181 mean train loss:  1.10203196e-02, mean val. rec. loss:  6.54528172e-03\n",
      "Epoch: 5182 mean train loss:  1.10203150e-02, mean val. rec. loss:  6.54527549e-03\n",
      "Epoch: 5183 mean train loss:  1.10203094e-02, mean val. rec. loss:  6.54527945e-03\n",
      "Epoch: 5184 mean train loss:  1.10203019e-02, mean val. rec. loss:  6.54528682e-03\n",
      "Epoch: 5185 mean train loss:  1.10202963e-02, mean val. rec. loss:  6.54530270e-03\n",
      "Epoch 05187: reducing learning rate of group 0 to 5.0000e-07.\n",
      "Epoch: 5186 mean train loss:  1.10202889e-02, mean val. rec. loss:  6.54534296e-03\n",
      "Epoch: 5187 mean train loss:  1.10202852e-02, mean val. rec. loss:  6.54534466e-03\n",
      "Epoch: 5188 mean train loss:  1.10202852e-02, mean val. rec. loss:  6.54535600e-03\n",
      "Epoch: 5189 mean train loss:  1.10202852e-02, mean val. rec. loss:  6.54536053e-03\n",
      "Epoch: 5190 mean train loss:  1.10202842e-02, mean val. rec. loss:  6.54537244e-03\n",
      "Epoch: 5191 mean train loss:  1.10202824e-02, mean val. rec. loss:  6.54539002e-03\n",
      "Epoch: 5192 mean train loss:  1.10202833e-02, mean val. rec. loss:  6.54540362e-03\n",
      "Epoch: 5193 mean train loss:  1.10202833e-02, mean val. rec. loss:  6.54541553e-03\n",
      "Epoch: 5194 mean train loss:  1.10202814e-02, mean val. rec. loss:  6.54543538e-03\n",
      "Epoch: 5195 mean train loss:  1.10202814e-02, mean val. rec. loss:  6.54545125e-03\n",
      "Epoch: 5196 mean train loss:  1.10202814e-02, mean val. rec. loss:  6.54546826e-03\n",
      "Epoch: 5197 mean train loss:  1.10202796e-02, mean val. rec. loss:  6.54548017e-03\n",
      "Epoch: 5198 mean train loss:  1.10202796e-02, mean val. rec. loss:  6.54549718e-03\n",
      "Epoch: 5199 mean train loss:  1.10202787e-02, mean val. rec. loss:  6.54551249e-03\n",
      "Epoch: 5200 mean train loss:  1.10202787e-02, mean val. rec. loss:  6.54552326e-03\n",
      "Epoch: 5201 mean train loss:  1.10202768e-02, mean val. rec. loss:  6.54553743e-03\n",
      "Epoch: 5202 mean train loss:  1.10202759e-02, mean val. rec. loss:  6.54555614e-03\n",
      "Epoch: 5203 mean train loss:  1.10202759e-02, mean val. rec. loss:  6.54557145e-03\n",
      "Epoch: 5204 mean train loss:  1.10202759e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5205 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54559924e-03\n",
      "Epoch: 5206 mean train loss:  1.10202749e-02, mean val. rec. loss:  6.54561681e-03\n",
      "Epoch 05208: reducing learning rate of group 0 to 5.0000e-08.\n",
      "Epoch: 5207 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54562362e-03\n",
      "Epoch: 5208 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54562192e-03\n",
      "Epoch: 5209 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54561965e-03\n",
      "Epoch: 5210 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54561795e-03\n",
      "Epoch: 5211 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54561625e-03\n",
      "Epoch: 5212 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54561511e-03\n",
      "Epoch: 5213 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54561341e-03\n",
      "Epoch: 5214 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54561228e-03\n",
      "Epoch: 5215 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54561114e-03\n",
      "Epoch: 5216 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54561001e-03\n",
      "Epoch: 5217 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54560887e-03\n",
      "Epoch: 5218 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54560717e-03\n",
      "Epoch: 5219 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54560604e-03\n",
      "Epoch: 5220 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54559980e-03\n",
      "Epoch: 5221 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54559867e-03\n",
      "Epoch: 5222 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54559697e-03\n",
      "Epoch: 5223 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54559640e-03\n",
      "Epoch: 5224 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54559470e-03\n",
      "Epoch: 5225 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54559357e-03\n",
      "Epoch: 5226 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54559130e-03\n",
      "Epoch: 5227 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54559016e-03\n",
      "Epoch 05229: reducing learning rate of group 0 to 5.0000e-09.\n",
      "Epoch: 5228 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5229 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5230 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5231 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5232 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5233 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5234 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5235 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5236 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5237 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5238 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5239 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5240 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5241 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5242 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5243 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5244 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5245 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5246 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5247 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5248 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch 05250: reducing learning rate of group 0 to 5.0000e-10.\n",
      "Epoch: 5249 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5250 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5251 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5252 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5253 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5254 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5255 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5256 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5257 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5258 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5259 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5260 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5261 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5262 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5263 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5264 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5265 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5266 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5267 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5268 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5269 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch 05271: reducing learning rate of group 0 to 5.0000e-11.\n",
      "Epoch: 5270 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5271 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5272 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5273 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5274 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5275 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5276 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5277 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5278 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5279 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5280 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5281 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5282 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5283 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5284 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5285 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5286 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5287 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5288 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5289 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5290 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5291 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5292 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5293 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5294 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5295 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5296 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5297 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5298 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5299 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5300 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5301 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5302 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5303 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5304 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5305 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5306 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5307 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5308 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5309 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5310 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5311 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5312 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5313 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5314 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5315 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5316 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5317 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5318 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5319 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5320 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5321 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5322 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5323 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5324 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5325 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5326 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5327 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5328 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5329 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5330 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5331 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5332 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5333 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5334 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5335 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5336 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5337 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5338 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5339 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5340 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5341 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5342 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5343 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5344 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5345 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5346 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5347 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5348 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5349 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5350 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5351 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5352 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5353 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5354 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5355 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5356 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5357 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5358 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5359 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5360 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5361 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5362 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5363 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5364 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5365 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5366 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5367 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5368 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5369 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5370 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5371 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5372 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5373 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5374 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5375 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5376 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5377 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5378 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5379 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5380 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5381 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5382 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5383 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5384 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5385 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5386 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5387 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5388 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5389 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5390 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5391 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5392 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5393 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5394 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5395 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5396 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5397 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5398 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5399 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5400 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5401 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5402 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5403 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5404 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5405 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5406 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5407 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5408 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5409 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5410 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5411 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5412 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5413 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5414 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5415 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5416 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5417 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5418 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5419 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5420 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5421 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5422 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5423 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5424 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5425 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5426 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5427 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5428 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5429 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5430 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5431 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5432 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5433 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5434 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5435 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5436 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5437 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5438 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5439 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5440 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5441 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5442 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5443 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5444 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5445 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5446 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5447 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5448 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5449 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5450 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5451 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5452 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5453 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5454 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5455 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5456 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5457 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5458 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5459 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5460 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5461 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5462 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5463 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5464 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5465 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5466 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5467 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5468 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5469 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5470 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5471 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5472 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5473 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5474 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5475 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5476 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5477 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5478 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5479 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5480 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5481 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5482 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5483 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5484 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5485 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5486 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5487 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5488 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5489 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5490 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5491 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5492 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5493 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5494 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5495 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5496 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5497 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5498 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5499 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5500 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5501 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5502 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5503 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5504 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5505 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5506 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5507 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5508 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5509 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5510 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5511 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5512 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5513 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5514 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5515 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5516 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5517 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5518 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5519 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5520 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5521 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5522 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5523 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5524 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5525 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5526 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5527 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5528 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5529 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5530 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5531 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5532 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5533 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5534 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5535 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5536 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5537 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5538 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5539 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5540 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5541 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5542 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5543 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5544 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5545 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5546 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5547 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5548 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5549 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5550 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5551 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5552 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5553 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5554 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5555 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5556 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5557 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5558 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5559 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5560 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5561 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5562 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5563 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5564 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5565 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5566 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5567 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5568 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5569 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5570 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5571 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5572 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5573 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5574 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5575 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5576 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5577 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5578 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5579 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5580 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5581 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5582 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5583 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5584 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5585 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5586 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5587 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5588 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5589 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5590 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5591 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5592 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5593 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5594 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5595 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5596 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5597 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5598 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5599 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5600 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5601 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5602 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5603 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5604 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5605 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5606 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5607 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5608 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5609 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5610 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5611 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5612 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5613 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5614 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5615 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5616 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5617 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5618 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5619 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5620 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5621 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5622 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5623 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5624 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5625 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5626 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5627 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5628 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5629 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5630 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5631 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5632 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5633 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5634 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5635 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5636 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5637 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5638 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5639 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5640 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5641 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5642 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5643 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5644 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5645 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5646 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5647 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5648 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5649 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5650 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5651 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5652 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5653 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5654 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5655 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5656 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5657 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5658 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5659 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5660 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5661 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5662 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5663 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5664 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5665 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5666 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5667 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5668 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5669 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5670 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5671 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5672 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5673 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5674 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5675 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5676 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5677 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5678 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5679 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5680 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5681 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5682 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5683 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5684 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5685 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5686 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5687 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5688 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5689 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5690 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5691 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5692 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5693 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5694 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5695 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5696 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5697 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5698 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5699 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5700 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5701 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5702 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5703 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5704 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5705 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5706 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5707 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5708 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5709 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5710 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5711 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5712 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5713 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5714 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5715 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5716 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5717 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5718 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5719 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5720 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5721 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5722 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5723 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5724 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5725 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5726 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5727 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5728 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5729 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5730 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5731 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5732 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5733 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5734 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5735 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5736 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5737 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5738 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5739 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5740 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5741 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5742 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5743 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5744 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5745 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5746 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5747 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5748 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5749 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5750 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5751 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5752 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5753 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5754 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5755 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5756 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5757 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5758 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5759 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5760 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5761 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5762 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5763 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5764 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5765 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5766 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5767 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5768 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5769 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5770 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5771 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5772 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5773 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5774 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5775 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5776 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5777 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5778 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5779 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5780 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5781 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5782 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5783 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5784 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5785 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5786 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5787 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5788 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5789 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5790 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5791 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5792 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5793 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5794 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5795 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5796 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5797 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5798 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5799 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5800 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5801 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5802 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5803 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5804 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5805 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5806 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5807 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5808 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5809 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5810 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5811 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5812 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5813 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5814 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5815 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5816 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5817 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5818 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5819 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5820 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5821 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5822 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5823 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5824 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5825 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5826 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5827 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5828 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5829 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5830 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5831 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5832 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5833 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5834 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5835 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5836 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5837 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5838 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5839 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5840 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5841 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5842 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5843 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5844 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5845 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5846 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5847 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5848 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5849 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5850 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5851 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5852 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5853 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5854 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5855 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5856 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5857 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5858 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5859 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5860 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5861 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5862 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5863 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5864 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5865 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5866 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5867 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5868 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5869 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5870 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5871 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5872 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5873 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5874 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5875 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5876 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5877 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5878 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5879 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5880 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5881 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5882 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5883 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5884 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5885 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5886 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5887 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5888 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5889 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5890 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5891 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5892 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5893 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5894 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5895 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5896 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5897 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5898 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5899 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5900 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5901 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5902 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5903 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5904 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5905 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5906 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5907 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5908 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5909 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5910 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5911 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5912 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5913 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5914 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5915 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5916 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5917 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5918 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5919 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5920 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5921 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5922 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5923 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5924 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5925 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5926 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5927 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5928 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5929 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5930 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5931 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5932 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5933 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5934 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5935 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5936 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5937 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5938 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5939 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5940 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5941 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5942 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5943 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5944 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5945 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5946 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5947 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5948 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5949 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5950 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5951 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5952 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5953 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5954 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5955 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5956 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5957 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5958 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5959 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5960 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5961 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5962 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5963 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5964 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5965 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5966 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5967 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5968 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5969 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5970 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5971 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5972 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5973 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5974 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5975 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5976 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5977 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5978 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5979 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5980 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5981 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5982 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5983 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5984 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5985 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5986 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5987 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5988 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5989 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5990 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5991 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5992 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5993 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5994 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5995 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5996 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5997 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5998 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 5999 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6000 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6001 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6002 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6003 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6004 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6005 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6006 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6007 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6008 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6009 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6010 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6011 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6012 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6013 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6014 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6015 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6016 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6017 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6018 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6019 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6020 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6021 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6022 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6023 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6024 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6025 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6026 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6027 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6028 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6029 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6030 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6031 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6032 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6033 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6034 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6035 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6036 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6037 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6038 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6039 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6040 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6041 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6042 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6043 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6044 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6045 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6046 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6047 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6048 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6049 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6050 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6051 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6052 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6053 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6054 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6055 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6056 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6057 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6058 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6059 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6060 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6061 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6062 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6063 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6064 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6065 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6066 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6067 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6068 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6069 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6070 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6071 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6072 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6073 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6074 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6075 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6076 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6077 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6078 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6079 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6080 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6081 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6082 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6083 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6084 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6085 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6086 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6087 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6088 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6089 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6090 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6091 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6092 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6093 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6094 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6095 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6096 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6097 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6098 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6099 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6100 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6101 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6102 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6103 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6104 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6105 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6106 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6107 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6108 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6109 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6110 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6111 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6112 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6113 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6114 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6115 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6116 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6117 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6118 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6119 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6120 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6121 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6122 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6123 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6124 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6125 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6126 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6127 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6128 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6129 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6130 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6131 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6132 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6133 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6134 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6135 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6136 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6137 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6138 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6139 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6140 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6141 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6142 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6143 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6144 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6145 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6146 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6147 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6148 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6149 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6150 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6151 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6152 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6153 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6154 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6155 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6156 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6157 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6158 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6159 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6160 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6161 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6162 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6163 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6164 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6165 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6166 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6167 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6168 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6169 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6170 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6171 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6172 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6173 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6174 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6175 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6176 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6177 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6178 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6179 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6180 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6181 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6182 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6183 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6184 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6185 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6186 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6187 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6188 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6189 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6190 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6191 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6192 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6193 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6194 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6195 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6196 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6197 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6198 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6199 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6200 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6201 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6202 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6203 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6204 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6205 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6206 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6207 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6208 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6209 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6210 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6211 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6212 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6213 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6214 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6215 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6216 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6217 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6218 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6219 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6220 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6221 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6222 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6223 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6224 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6225 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6226 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6227 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6228 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6229 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6230 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6231 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6232 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6233 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6234 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6235 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6236 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6237 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6238 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6239 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6240 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6241 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6242 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6243 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6244 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6245 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6246 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6247 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6248 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6249 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6250 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6251 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6252 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6253 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6254 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6255 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6256 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6257 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6258 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6259 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6260 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6261 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6262 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6263 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6264 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6265 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6266 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6267 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6268 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6269 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6270 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6271 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6272 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6273 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6274 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6275 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6276 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6277 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6278 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6279 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6280 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6281 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6282 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6283 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6284 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6285 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6286 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6287 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6288 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6289 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6290 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6291 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6292 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6293 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6294 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6295 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6296 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6297 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6298 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6299 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6300 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6301 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6302 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6303 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6304 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6305 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6306 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6307 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6308 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6309 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6310 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6311 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6312 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6313 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6314 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6315 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6316 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6317 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6318 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6319 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6320 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6321 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6322 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6323 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6324 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6325 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6326 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6327 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6328 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6329 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6330 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6331 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6332 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6333 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6334 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6335 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6336 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6337 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6338 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6339 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6340 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6341 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6342 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6343 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6344 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6345 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6346 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6347 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6348 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6349 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6350 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6351 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6352 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6353 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6354 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6355 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6356 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6357 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6358 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6359 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6360 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6361 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6362 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6363 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6364 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6365 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6366 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6367 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6368 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6369 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6370 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6371 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6372 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6373 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6374 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6375 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6376 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6377 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6378 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6379 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6380 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6381 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6382 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6383 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6384 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6385 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6386 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6387 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6388 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6389 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6390 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6391 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6392 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6393 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6394 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6395 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6396 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6397 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6398 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6399 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6400 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6401 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6402 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6403 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6404 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6405 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6406 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6407 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6408 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6409 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6410 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6411 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6412 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6413 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6414 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6415 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6416 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6417 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6418 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6419 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6420 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6421 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6422 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6423 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6424 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6425 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6426 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6427 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6428 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6429 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6430 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6431 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6432 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6433 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6434 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6435 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6436 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6437 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6438 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6439 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6440 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6441 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6442 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6443 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6444 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6445 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6446 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6447 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6448 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6449 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6450 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6451 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6452 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6453 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6454 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6455 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6456 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6457 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6458 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6459 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6460 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6461 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6462 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6463 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6464 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6465 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6466 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6467 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6468 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6469 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6470 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6471 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6472 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6473 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6474 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6475 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6476 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6477 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6478 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6479 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6480 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6481 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6482 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6483 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6484 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6485 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6486 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6487 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6488 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6489 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6490 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6491 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6492 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6493 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6494 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6495 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6496 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6497 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6498 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6499 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6500 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6501 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6502 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6503 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6504 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6505 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6506 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6507 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6508 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6509 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6510 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6511 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6512 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6513 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6514 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6515 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6516 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6517 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6518 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6519 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6520 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6521 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6522 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6523 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6524 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6525 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6526 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6527 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6528 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6529 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6530 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6531 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6532 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6533 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6534 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6535 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6536 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6537 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6538 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6539 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6540 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6541 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6542 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6543 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6544 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6545 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6546 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6547 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6548 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6549 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6550 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6551 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6552 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6553 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6554 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6555 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6556 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6557 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6558 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6559 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6560 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6561 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6562 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6563 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6564 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6565 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6566 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6567 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6568 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6569 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6570 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6571 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6572 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6573 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6574 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6575 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6576 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6577 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6578 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6579 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6580 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6581 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6582 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6583 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6584 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6585 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6586 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6587 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6588 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6589 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6590 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6591 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6592 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6593 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6594 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6595 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6596 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6597 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6598 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6599 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6600 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6601 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6602 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6603 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6604 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6605 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6606 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6607 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6608 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6609 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6610 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6611 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6612 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6613 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6614 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6615 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6616 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6617 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6618 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6619 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6620 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6621 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6622 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6623 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6624 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6625 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6626 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6627 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6628 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6629 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6630 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6631 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6632 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6633 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6634 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6635 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6636 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6637 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6638 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6639 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6640 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6641 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6642 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6643 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6644 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6645 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6646 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6647 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6648 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6649 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6650 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6651 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6652 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6653 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6654 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6655 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6656 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6657 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6658 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6659 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6660 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6661 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6662 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6663 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6664 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6665 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6666 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6667 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6668 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6669 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6670 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6671 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6672 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6673 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6674 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6675 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6676 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6677 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6678 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6679 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6680 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6681 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6682 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6683 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6684 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6685 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6686 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6687 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6688 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6689 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6690 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6691 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6692 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6693 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6694 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6695 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6696 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6697 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6698 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6699 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6700 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6701 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6702 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6703 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6704 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6705 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6706 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6707 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6708 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6709 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6710 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6711 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6712 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6713 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6714 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6715 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6716 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6717 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6718 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6719 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6720 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6721 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6722 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6723 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6724 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6725 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6726 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6727 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6728 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6729 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6730 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6731 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6732 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6733 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6734 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6735 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6736 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6737 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6738 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6739 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6740 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6741 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6742 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6743 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6744 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6745 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6746 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6747 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6748 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6749 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6750 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6751 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6752 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6753 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6754 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6755 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6756 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6757 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6758 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6759 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6760 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6761 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6762 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6763 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6764 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6765 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6766 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6767 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6768 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6769 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6770 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6771 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6772 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6773 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6774 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6775 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6776 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6777 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6778 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6779 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6780 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6781 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6782 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6783 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6784 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6785 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6786 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6787 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6788 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6789 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6790 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6791 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6792 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6793 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6794 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6795 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6796 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6797 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6798 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6799 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6800 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6801 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6802 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6803 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6804 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6805 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6806 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6807 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6808 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6809 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6810 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6811 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6812 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6813 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6814 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6815 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6816 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6817 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6818 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6819 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6820 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6821 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6822 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6823 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6824 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6825 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6826 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6827 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6828 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6829 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6830 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6831 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6832 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6833 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6834 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6835 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6836 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6837 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6838 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6839 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6840 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6841 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6842 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6843 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6844 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6845 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6846 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6847 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6848 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6849 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6850 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6851 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6852 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6853 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6854 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6855 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6856 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6857 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6858 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6859 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6860 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6861 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6862 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6863 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6864 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6865 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6866 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6867 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6868 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6869 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6870 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6871 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6872 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6873 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6874 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6875 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6876 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6877 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6878 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6879 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6880 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6881 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6882 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6883 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6884 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6885 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6886 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6887 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6888 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6889 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6890 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6891 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6892 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6893 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6894 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6895 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6896 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6897 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6898 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6899 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6900 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6901 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6902 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6903 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6904 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6905 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6906 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6907 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6908 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6909 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6910 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6911 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6912 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6913 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6914 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6915 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6916 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6917 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6918 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6919 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6920 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6921 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6922 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6923 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6924 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6925 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6926 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6927 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6928 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6929 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6930 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6931 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6932 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6933 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6934 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6935 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6936 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6937 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6938 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6939 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6940 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6941 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6942 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6943 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6944 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6945 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6946 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6947 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6948 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6949 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6950 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6951 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6952 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6953 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6954 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6955 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6956 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6957 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6958 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6959 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6960 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6961 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6962 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6963 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6964 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6965 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6966 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6967 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6968 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6969 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6970 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6971 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6972 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6973 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6974 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6975 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6976 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6977 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6978 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6979 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6980 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6981 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6982 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6983 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6984 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6985 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6986 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6987 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6988 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6989 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6990 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6991 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6992 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6993 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6994 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6995 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6996 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6997 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6998 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 6999 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7000 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7001 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7002 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7003 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7004 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7005 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7006 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7007 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7008 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7009 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7010 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7011 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7012 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7013 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7014 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7015 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7016 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7017 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7018 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7019 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7020 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7021 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7022 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7023 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7024 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7025 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7026 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7027 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7028 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7029 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7030 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7031 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7032 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7033 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7034 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7035 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7036 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7037 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7038 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7039 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7040 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7041 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7042 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7043 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7044 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7045 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7046 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7047 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7048 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7049 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7050 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7051 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7052 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7053 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7054 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7055 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7056 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7057 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7058 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7059 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7060 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7061 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7062 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7063 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7064 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7065 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7066 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7067 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7068 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7069 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7070 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7071 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7072 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7073 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7074 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7075 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7076 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7077 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7078 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7079 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7080 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7081 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7082 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7083 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7084 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7085 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7086 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7087 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7088 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7089 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7090 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7091 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7092 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7093 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7094 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7095 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7096 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7097 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7098 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7099 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7100 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7101 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7102 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7103 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7104 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7105 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7106 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7107 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7108 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7109 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7110 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7111 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7112 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7113 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7114 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7115 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7116 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7117 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7118 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7119 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7120 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7121 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7122 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7123 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7124 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7125 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7126 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7127 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7128 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7129 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7130 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7131 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7132 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7133 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7134 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7135 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7136 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7137 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7138 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7139 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7140 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7141 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7142 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7143 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7144 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7145 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7146 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7147 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7148 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7149 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7150 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7151 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7152 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7153 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7154 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7155 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7156 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7157 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7158 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7159 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7160 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7161 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7162 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7163 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7164 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7165 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7166 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7167 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7168 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7169 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7170 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7171 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7172 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7173 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7174 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7175 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7176 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7177 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7178 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7179 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7180 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7181 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7182 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7183 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7184 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7185 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7186 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7187 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7188 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7189 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7190 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7191 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7192 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7193 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7194 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7195 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7196 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7197 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7198 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7199 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7200 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7201 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7202 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7203 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7204 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7205 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7206 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7207 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7208 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7209 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7210 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7211 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7212 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7213 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7214 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7215 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7216 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7217 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7218 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7219 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7220 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7221 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7222 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7223 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7224 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7225 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7226 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7227 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7228 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7229 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7230 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7231 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7232 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7233 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7234 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7235 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7236 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7237 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7238 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7239 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7240 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7241 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7242 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7243 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7244 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7245 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7246 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7247 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7248 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7249 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7250 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7251 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7252 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7253 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7254 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7255 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7256 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7257 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7258 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7259 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7260 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7261 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7262 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7263 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7264 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7265 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7266 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7267 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7268 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7269 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7270 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7271 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7272 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7273 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7274 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7275 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7276 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7277 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7278 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7279 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7280 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7281 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7282 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7283 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7284 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7285 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7286 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7287 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7288 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7289 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7290 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7291 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7292 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7293 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7294 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7295 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7296 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7297 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7298 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7299 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7300 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7301 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7302 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7303 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7304 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7305 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7306 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7307 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7308 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7309 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7310 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7311 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7312 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7313 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7314 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7315 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7316 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7317 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7318 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7319 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7320 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7321 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7322 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7323 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7324 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7325 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7326 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7327 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7328 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7329 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7330 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7331 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7332 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7333 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7334 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7335 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7336 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7337 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7338 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7339 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7340 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7341 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7342 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7343 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7344 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7345 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7346 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7347 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7348 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7349 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7350 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7351 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7352 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7353 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7354 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7355 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7356 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7357 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7358 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7359 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7360 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7361 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7362 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7363 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7364 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7365 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7366 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7367 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7368 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7369 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7370 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7371 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7372 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7373 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7374 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7375 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7376 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7377 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7378 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7379 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7380 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7381 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7382 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7383 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7384 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7385 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7386 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7387 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7388 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7389 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7390 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7391 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7392 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7393 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7394 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7395 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7396 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7397 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7398 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7399 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7400 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7401 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7402 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7403 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7404 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7405 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7406 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7407 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7408 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7409 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7410 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7411 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7412 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7413 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7414 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7415 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7416 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7417 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7418 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7419 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7420 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7421 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7422 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7423 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7424 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7425 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7426 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7427 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7428 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7429 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7430 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7431 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7432 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7433 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7434 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7435 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7436 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7437 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7438 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7439 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7440 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7441 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7442 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7443 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7444 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7445 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7446 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7447 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7448 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7449 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7450 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7451 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7452 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7453 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7454 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7455 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7456 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7457 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7458 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7459 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7460 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7461 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7462 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7463 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7464 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7465 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7466 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7467 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7468 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7469 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7470 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7471 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7472 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7473 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7474 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7475 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7476 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7477 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7478 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7479 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7480 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7481 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7482 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7483 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7484 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7485 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7486 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7487 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7488 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7489 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7490 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7491 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7492 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7493 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7494 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7495 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7496 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7497 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7498 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7499 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7500 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7501 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7502 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7503 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7504 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7505 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7506 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7507 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7508 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7509 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7510 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7511 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7512 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7513 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7514 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7515 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7516 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7517 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7518 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7519 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7520 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7521 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7522 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7523 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7524 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7525 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7526 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7527 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7528 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7529 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7530 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7531 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7532 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7533 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7534 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7535 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7536 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7537 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7538 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7539 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7540 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7541 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7542 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7543 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7544 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7545 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7546 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7547 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7548 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7549 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7550 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7551 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7552 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7553 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7554 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7555 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7556 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7557 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7558 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7559 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7560 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7561 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7562 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7563 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7564 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7565 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7566 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7567 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7568 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7569 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7570 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7571 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7572 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7573 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7574 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7575 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7576 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7577 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7578 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7579 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7580 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7581 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7582 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7583 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7584 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7585 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7586 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7587 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7588 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7589 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7590 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7591 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7592 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7593 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7594 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7595 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7596 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7597 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7598 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7599 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7600 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7601 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7602 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7603 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7604 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7605 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7606 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7607 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7608 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7609 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7610 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7611 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7612 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7613 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7614 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7615 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7616 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7617 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7618 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7619 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7620 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7621 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7622 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7623 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7624 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7625 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7626 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7627 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7628 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7629 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7630 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7631 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7632 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7633 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7634 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7635 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7636 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7637 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7638 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7639 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7640 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7641 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7642 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7643 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7644 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7645 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7646 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7647 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7648 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7649 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7650 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7651 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7652 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7653 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7654 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7655 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7656 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7657 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7658 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7659 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7660 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7661 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7662 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7663 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7664 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7665 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7666 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7667 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7668 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7669 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7670 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7671 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7672 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7673 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7674 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7675 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7676 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7677 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7678 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7679 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7680 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7681 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7682 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7683 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7684 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7685 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7686 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7687 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7688 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7689 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7690 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7691 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7692 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7693 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7694 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7695 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7696 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7697 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7698 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7699 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7700 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7701 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7702 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7703 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7704 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7705 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7706 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7707 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7708 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7709 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7710 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7711 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7712 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7713 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7714 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7715 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7716 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7717 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7718 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7719 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7720 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7721 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7722 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7723 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7724 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7725 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7726 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7727 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7728 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7729 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7730 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7731 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7732 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7733 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7734 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7735 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7736 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7737 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7738 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7739 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7740 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7741 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7742 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7743 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7744 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7745 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7746 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7747 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7748 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7749 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7750 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7751 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7752 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7753 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7754 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7755 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7756 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7757 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7758 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7759 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7760 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7761 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7762 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7763 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7764 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7765 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7766 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7767 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7768 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7769 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7770 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7771 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7772 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7773 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7774 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7775 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7776 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7777 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7778 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7779 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7780 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7781 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7782 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7783 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7784 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7785 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7786 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7787 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7788 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7789 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7790 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7791 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7792 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7793 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7794 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7795 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7796 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7797 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7798 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7799 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7800 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7801 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7802 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7803 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7804 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7805 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7806 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7807 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7808 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7809 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7810 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7811 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7812 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7813 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7814 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7815 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7816 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7817 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7818 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7819 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7820 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7821 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7822 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7823 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7824 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7825 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7826 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7827 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7828 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7829 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7830 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7831 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7832 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7833 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7834 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7835 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7836 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7837 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7838 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7839 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7840 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7841 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7842 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7843 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7844 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7845 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7846 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7847 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7848 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7849 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7850 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7851 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7852 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7853 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7854 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7855 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7856 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7857 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7858 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7859 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7860 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7861 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7862 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7863 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7864 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7865 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7866 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7867 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7868 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7869 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7870 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7871 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7872 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7873 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7874 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7875 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7876 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7877 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7878 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7879 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7880 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7881 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7882 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7883 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7884 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7885 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7886 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7887 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7888 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7889 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7890 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7891 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7892 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7893 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7894 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7895 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7896 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7897 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7898 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7899 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7900 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7901 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7902 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7903 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7904 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7905 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7906 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7907 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7908 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7909 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7910 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7911 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7912 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7913 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7914 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7915 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7916 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7917 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7918 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7919 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7920 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7921 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7922 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7923 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7924 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7925 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7926 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7927 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7928 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7929 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7930 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7931 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7932 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7933 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7934 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7935 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7936 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7937 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7938 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7939 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7940 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7941 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7942 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7943 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7944 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7945 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7946 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7947 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7948 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7949 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7950 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7951 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7952 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7953 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7954 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7955 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7956 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7957 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7958 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7959 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7960 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7961 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7962 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7963 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7964 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7965 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7966 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7967 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7968 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7969 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7970 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7971 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7972 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7973 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7974 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7975 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7976 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7977 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7978 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7979 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7980 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7981 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7982 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7983 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7984 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7985 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7986 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7987 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7988 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7989 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7990 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7991 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7992 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7993 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7994 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7995 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7996 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7997 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7998 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 7999 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8000 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8001 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8002 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8003 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8004 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8005 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8006 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8007 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8008 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8009 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8010 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8011 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8012 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8013 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8014 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8015 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8016 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8017 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8018 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8019 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8020 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8021 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8022 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8023 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8024 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8025 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8026 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8027 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8028 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8029 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8030 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8031 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8032 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8033 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8034 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8035 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8036 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8037 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8038 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8039 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8040 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8041 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8042 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8043 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8044 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8045 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8046 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8047 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8048 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8049 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8050 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8051 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8052 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8053 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8054 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8055 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8056 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8057 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8058 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8059 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8060 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8061 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8062 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8063 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8064 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8065 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8066 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8067 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8068 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8069 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8070 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8071 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8072 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8073 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8074 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8075 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8076 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8077 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8078 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8079 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8080 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8081 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8082 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8083 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8084 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8085 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8086 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8087 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8088 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8089 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8090 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8091 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8092 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8093 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8094 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8095 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8096 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8097 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8098 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8099 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8100 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8101 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8102 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8103 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8104 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8105 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8106 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8107 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8108 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8109 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8110 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8111 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8112 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8113 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8114 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8115 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8116 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8117 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8118 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8119 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8120 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8121 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8122 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8123 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8124 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8125 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8126 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8127 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8128 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8129 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8130 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8131 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8132 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8133 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8134 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8135 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8136 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8137 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8138 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8139 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8140 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8141 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8142 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8143 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8144 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8145 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8146 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8147 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8148 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8149 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8150 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8151 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8152 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8153 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8154 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8155 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8156 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8157 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8158 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8159 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8160 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8161 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8162 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8163 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8164 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8165 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8166 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8167 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8168 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8169 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8170 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8171 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8172 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8173 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8174 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8175 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8176 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8177 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8178 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8179 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8180 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8181 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8182 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8183 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8184 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8185 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8186 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8187 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8188 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8189 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8190 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8191 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8192 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8193 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8194 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8195 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8196 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8197 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8198 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8199 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8200 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8201 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8202 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8203 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8204 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8205 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8206 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8207 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8208 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8209 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8210 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8211 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8212 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8213 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8214 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8215 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8216 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8217 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8218 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8219 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8220 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8221 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8222 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8223 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8224 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8225 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8226 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8227 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8228 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8229 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8230 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8231 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8232 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8233 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8234 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8235 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8236 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8237 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8238 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8239 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8240 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8241 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8242 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8243 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8244 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8245 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8246 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8247 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8248 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8249 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8250 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8251 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8252 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8253 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8254 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8255 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8256 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8257 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8258 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8259 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8260 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8261 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8262 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8263 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8264 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8265 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8266 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8267 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8268 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8269 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8270 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8271 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8272 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8273 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8274 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8275 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8276 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8277 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8278 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8279 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8280 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8281 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8282 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8283 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8284 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8285 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8286 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8287 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8288 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8289 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8290 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8291 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8292 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8293 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8294 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8295 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8296 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8297 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8298 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8299 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8300 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8301 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8302 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8303 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8304 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8305 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8306 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8307 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8308 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8309 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8310 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8311 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8312 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8313 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8314 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8315 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8316 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8317 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8318 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8319 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8320 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8321 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8322 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8323 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8324 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8325 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8326 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8327 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8328 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8329 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8330 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8331 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8332 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8333 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8334 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8335 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8336 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8337 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8338 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8339 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8340 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8341 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8342 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8343 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8344 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8345 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8346 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8347 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8348 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8349 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8350 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8351 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8352 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8353 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8354 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8355 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8356 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8357 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8358 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8359 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8360 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8361 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8362 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8363 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8364 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8365 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8366 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8367 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8368 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8369 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8370 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8371 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8372 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8373 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8374 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8375 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8376 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8377 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8378 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8379 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8380 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8381 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8382 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8383 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8384 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8385 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8386 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8387 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8388 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8389 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8390 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8391 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8392 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8393 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8394 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8395 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8396 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8397 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8398 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8399 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8400 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8401 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8402 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8403 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8404 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8405 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8406 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8407 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8408 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8409 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8410 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8411 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8412 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8413 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8414 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8415 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8416 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8417 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8418 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8419 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8420 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8421 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8422 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8423 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8424 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8425 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8426 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8427 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8428 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8429 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8430 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8431 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8432 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8433 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8434 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8435 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8436 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8437 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8438 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8439 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8440 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8441 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8442 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8443 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8444 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8445 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8446 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8447 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8448 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8449 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8450 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8451 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8452 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8453 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8454 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8455 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8456 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8457 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8458 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8459 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8460 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8461 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8462 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8463 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8464 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8465 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8466 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8467 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8468 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8469 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8470 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8471 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8472 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8473 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8474 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8475 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8476 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8477 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8478 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8479 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8480 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8481 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8482 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8483 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8484 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8485 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8486 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8487 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8488 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8489 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8490 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8491 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8492 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8493 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8494 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8495 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8496 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8497 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8498 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8499 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8500 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8501 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8502 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8503 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8504 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8505 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8506 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8507 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8508 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8509 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8510 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8511 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8512 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8513 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8514 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8515 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8516 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8517 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8518 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8519 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8520 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8521 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8522 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8523 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8524 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8525 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8526 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8527 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8528 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8529 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8530 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8531 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8532 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8533 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8534 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8535 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8536 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8537 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8538 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8539 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8540 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8541 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8542 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8543 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8544 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8545 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8546 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8547 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8548 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8549 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8550 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8551 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8552 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8553 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8554 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8555 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8556 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8557 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8558 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8559 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8560 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8561 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8562 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8563 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8564 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8565 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8566 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8567 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8568 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8569 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8570 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8571 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8572 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8573 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8574 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8575 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8576 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8577 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8578 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8579 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8580 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8581 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8582 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8583 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8584 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8585 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8586 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8587 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8588 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8589 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8590 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8591 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8592 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8593 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8594 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8595 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8596 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8597 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8598 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8599 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8600 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8601 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8602 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8603 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8604 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8605 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8606 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8607 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8608 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8609 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8610 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8611 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8612 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8613 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8614 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8615 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8616 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8617 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8618 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8619 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8620 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8621 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8622 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8623 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8624 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8625 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8626 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8627 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8628 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8629 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8630 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8631 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8632 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8633 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8634 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8635 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8636 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8637 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8638 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8639 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8640 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8641 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8642 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8643 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8644 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8645 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8646 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8647 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8648 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8649 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8650 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8651 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8652 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8653 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8654 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8655 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8656 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8657 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8658 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8659 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8660 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8661 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8662 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8663 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8664 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8665 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8666 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8667 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8668 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8669 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8670 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8671 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8672 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8673 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8674 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8675 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8676 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8677 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8678 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8679 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8680 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8681 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8682 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8683 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8684 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8685 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8686 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8687 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8688 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8689 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8690 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8691 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8692 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8693 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8694 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8695 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8696 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8697 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8698 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8699 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8700 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8701 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8702 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8703 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8704 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8705 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8706 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8707 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8708 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8709 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8710 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8711 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8712 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8713 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8714 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8715 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8716 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8717 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8718 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8719 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8720 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8721 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8722 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8723 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8724 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8725 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8726 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8727 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8728 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8729 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8730 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8731 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8732 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8733 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8734 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8735 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8736 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8737 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8738 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8739 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8740 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8741 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8742 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8743 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8744 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8745 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8746 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8747 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8748 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8749 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8750 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8751 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8752 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8753 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8754 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8755 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8756 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8757 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8758 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8759 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8760 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8761 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8762 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8763 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8764 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8765 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8766 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8767 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8768 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8769 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8770 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8771 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8772 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8773 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8774 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8775 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8776 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8777 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8778 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8779 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8780 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8781 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8782 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8783 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8784 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8785 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8786 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8787 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8788 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8789 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8790 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8791 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8792 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8793 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8794 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8795 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8796 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8797 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8798 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8799 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8800 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8801 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8802 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8803 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8804 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8805 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8806 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8807 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8808 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8809 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8810 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8811 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8812 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8813 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8814 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8815 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8816 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8817 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8818 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8819 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8820 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8821 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8822 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8823 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8824 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8825 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8826 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8827 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8828 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8829 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8830 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8831 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8832 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8833 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8834 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8835 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8836 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8837 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8838 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8839 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8840 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8841 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8842 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8843 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8844 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8845 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8846 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8847 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8848 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8849 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8850 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8851 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8852 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8853 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8854 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8855 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8856 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8857 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8858 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8859 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8860 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8861 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8862 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8863 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8864 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8865 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8866 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8867 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8868 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8869 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8870 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8871 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8872 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8873 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8874 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8875 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8876 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8877 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8878 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8879 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8880 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8881 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8882 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8883 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8884 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8885 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8886 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8887 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8888 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8889 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8890 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8891 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8892 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8893 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8894 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8895 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8896 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8897 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8898 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8899 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8900 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8901 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8902 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8903 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8904 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8905 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8906 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8907 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8908 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8909 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8910 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8911 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8912 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8913 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8914 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8915 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8916 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8917 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8918 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8919 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8920 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8921 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8922 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8923 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8924 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8925 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8926 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8927 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8928 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8929 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8930 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8931 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8932 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8933 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8934 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8935 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8936 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8937 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8938 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8939 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8940 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8941 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8942 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8943 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8944 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8945 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8946 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8947 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8948 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8949 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8950 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8951 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8952 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8953 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8954 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8955 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8956 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8957 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8958 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8959 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8960 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8961 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8962 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8963 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8964 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8965 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8966 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8967 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8968 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8969 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8970 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8971 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8972 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8973 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8974 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8975 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8976 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8977 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8978 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8979 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8980 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8981 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8982 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8983 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8984 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8985 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8986 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8987 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8988 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8989 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8990 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8991 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8992 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8993 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8994 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8995 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8996 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8997 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8998 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 8999 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9000 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9001 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9002 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9003 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9004 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9005 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9006 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9007 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9008 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9009 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9010 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9011 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9012 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9013 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9014 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9015 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9016 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9017 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9018 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9019 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9020 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9021 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9022 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9023 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9024 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9025 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9026 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9027 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9028 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9029 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9030 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9031 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9032 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9033 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9034 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9035 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9036 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9037 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9038 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9039 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9040 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9041 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9042 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9043 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9044 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9045 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9046 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9047 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9048 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9049 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9050 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9051 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9052 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9053 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9054 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9055 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9056 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9057 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9058 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9059 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9060 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9061 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9062 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9063 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9064 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9065 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9066 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9067 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9068 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9069 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9070 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9071 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9072 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9073 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9074 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9075 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9076 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9077 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9078 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9079 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9080 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9081 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9082 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9083 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9084 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9085 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9086 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9087 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9088 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9089 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9090 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9091 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9092 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9093 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9094 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9095 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9096 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9097 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9098 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9099 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9100 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9101 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9102 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9103 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9104 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9105 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9106 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9107 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9108 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9109 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9110 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9111 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9112 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9113 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9114 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9115 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9116 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9117 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9118 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9119 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9120 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9121 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9122 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9123 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9124 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9125 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9126 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9127 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9128 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9129 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9130 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9131 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9132 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9133 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9134 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9135 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9136 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9137 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9138 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9139 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9140 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9141 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9142 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9143 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9144 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9145 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9146 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9147 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9148 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9149 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9150 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9151 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9152 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9153 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9154 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9155 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9156 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9157 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9158 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9159 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9160 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9161 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9162 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9163 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9164 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9165 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9166 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9167 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9168 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9169 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9170 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9171 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9172 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9173 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9174 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9175 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9176 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9177 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9178 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9179 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9180 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9181 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9182 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9183 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9184 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9185 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9186 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9187 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9188 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9189 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9190 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9191 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9192 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9193 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9194 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9195 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9196 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9197 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9198 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9199 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9200 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9201 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9202 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9203 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9204 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9205 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9206 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9207 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9208 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9209 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9210 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9211 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9212 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9213 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9214 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9215 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9216 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9217 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9218 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9219 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9220 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9221 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9222 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9223 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9224 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9225 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9226 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9227 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9228 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9229 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9230 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9231 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9232 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9233 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9234 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9235 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9236 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9237 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9238 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9239 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9240 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9241 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9242 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9243 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9244 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9245 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9246 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9247 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9248 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9249 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9250 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9251 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9252 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9253 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9254 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9255 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9256 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9257 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9258 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9259 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9260 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9261 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9262 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9263 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9264 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9265 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9266 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9267 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9268 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9269 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9270 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9271 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9272 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9273 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9274 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9275 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9276 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9277 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9278 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9279 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9280 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9281 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9282 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9283 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9284 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9285 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9286 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9287 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9288 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9289 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9290 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9291 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9292 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9293 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9294 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9295 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9296 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9297 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9298 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9299 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9300 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9301 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9302 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9303 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9304 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9305 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9306 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9307 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9308 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9309 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9310 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9311 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9312 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9313 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9314 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9315 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9316 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9317 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9318 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9319 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9320 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9321 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9322 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9323 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9324 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9325 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9326 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9327 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9328 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9329 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9330 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9331 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9332 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9333 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9334 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9335 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9336 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9337 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9338 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9339 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9340 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9341 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9342 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9343 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9344 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9345 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9346 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9347 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9348 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9349 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9350 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9351 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9352 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9353 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9354 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9355 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9356 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9357 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9358 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9359 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9360 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9361 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9362 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9363 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9364 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9365 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9366 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9367 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9368 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9369 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9370 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9371 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9372 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9373 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9374 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9375 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9376 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9377 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9378 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9379 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9380 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9381 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9382 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9383 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9384 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9385 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9386 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9387 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9388 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9389 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9390 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9391 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9392 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9393 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9394 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9395 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9396 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9397 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9398 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9399 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9400 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9401 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9402 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9403 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9404 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9405 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9406 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9407 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9408 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9409 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9410 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9411 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9412 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9413 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9414 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9415 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9416 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9417 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9418 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9419 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9420 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9421 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9422 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9423 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9424 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9425 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9426 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9427 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9428 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9429 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9430 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9431 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9432 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9433 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9434 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9435 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9436 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9437 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9438 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9439 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9440 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9441 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9442 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9443 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9444 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9445 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9446 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9447 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9448 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9449 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9450 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9451 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9452 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9453 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9454 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9455 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9456 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9457 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9458 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9459 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9460 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9461 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9462 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9463 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9464 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9465 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9466 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9467 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9468 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9469 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9470 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9471 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9472 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9473 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9474 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9475 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9476 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9477 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9478 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9479 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9480 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9481 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9482 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9483 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9484 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9485 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9486 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9487 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9488 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9489 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9490 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9491 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9492 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9493 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9494 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9495 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9496 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9497 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9498 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9499 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9500 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9501 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9502 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9503 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9504 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9505 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9506 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9507 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9508 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9509 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9510 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9511 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9512 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9513 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9514 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9515 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9516 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9517 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9518 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9519 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9520 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9521 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9522 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9523 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9524 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9525 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9526 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9527 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9528 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9529 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9530 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9531 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9532 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9533 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9534 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9535 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9536 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9537 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9538 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9539 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9540 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9541 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9542 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9543 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9544 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9545 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9546 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9547 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9548 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9549 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9550 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9551 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9552 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9553 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9554 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9555 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9556 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9557 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9558 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9559 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9560 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9561 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9562 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9563 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9564 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9565 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9566 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9567 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9568 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9569 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9570 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9571 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9572 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9573 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9574 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9575 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9576 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9577 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9578 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9579 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9580 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9581 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9582 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9583 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9584 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9585 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9586 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9587 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9588 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9589 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9590 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9591 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9592 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9593 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9594 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9595 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9596 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9597 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9598 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9599 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9600 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9601 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9602 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9603 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9604 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9605 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9606 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9607 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9608 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9609 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9610 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9611 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9612 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9613 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9614 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9615 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9616 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9617 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9618 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9619 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9620 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9621 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9622 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9623 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9624 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9625 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9626 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9627 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9628 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9629 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9630 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9631 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9632 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9633 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9634 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9635 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9636 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9637 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9638 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9639 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9640 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9641 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9642 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9643 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9644 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9645 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9646 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9647 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9648 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9649 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9650 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9651 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9652 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9653 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9654 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9655 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9656 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9657 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9658 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9659 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9660 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9661 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9662 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9663 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9664 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9665 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9666 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9667 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9668 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9669 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9670 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9671 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9672 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9673 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9674 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9675 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9676 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9677 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9678 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9679 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9680 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9681 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9682 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9683 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9684 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9685 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9686 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9687 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9688 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9689 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9690 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9691 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9692 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9693 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9694 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9695 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9696 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9697 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9698 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9699 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9700 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9701 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9702 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9703 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9704 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9705 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9706 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9707 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9708 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9709 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9710 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9711 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9712 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9713 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9714 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9715 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9716 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9717 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9718 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9719 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9720 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9721 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9722 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9723 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9724 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9725 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9726 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9727 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9728 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9729 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9730 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9731 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9732 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9733 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9734 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9735 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9736 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9737 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9738 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9739 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9740 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9741 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9742 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9743 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9744 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9745 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9746 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9747 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9748 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9749 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9750 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9751 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9752 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9753 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9754 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9755 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9756 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9757 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9758 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9759 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9760 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9761 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9762 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9763 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9764 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9765 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9766 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9767 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9768 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9769 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9770 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9771 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9772 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9773 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9774 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9775 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9776 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9777 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9778 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9779 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9780 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9781 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9782 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9783 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9784 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9785 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9786 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9787 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9788 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9789 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9790 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9791 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9792 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9793 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9794 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9795 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9796 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9797 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9798 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9799 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9800 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9801 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9802 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9803 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9804 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9805 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9806 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9807 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9808 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9809 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9810 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9811 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9812 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9813 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9814 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9815 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9816 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9817 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9818 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9819 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9820 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9821 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9822 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9823 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9824 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9825 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9826 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9827 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9828 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9829 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9830 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9831 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9832 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9833 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9834 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9835 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9836 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9837 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9838 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9839 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9840 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9841 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9842 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9843 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9844 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9845 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9846 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9847 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9848 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9849 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9850 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9851 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9852 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9853 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9854 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9855 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9856 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9857 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9858 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9859 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9860 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9861 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9862 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9863 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9864 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9865 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9866 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9867 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9868 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9869 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9870 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9871 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9872 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9873 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9874 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9875 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9876 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9877 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9878 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9879 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9880 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9881 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9882 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9883 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9884 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9885 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9886 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9887 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9888 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9889 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9890 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9891 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9892 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9893 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9894 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9895 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9896 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9897 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9898 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9899 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9900 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9901 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9902 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9903 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9904 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9905 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9906 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9907 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9908 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9909 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9910 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9911 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9912 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9913 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9914 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9915 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9916 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9917 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9918 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9919 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9920 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9921 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9922 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9923 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9924 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9925 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9926 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9927 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9928 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9929 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9930 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9931 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9932 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9933 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9934 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9935 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9936 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9937 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9938 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9939 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9940 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9941 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9942 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9943 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9944 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9945 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9946 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9947 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9948 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9949 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9950 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9951 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9952 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9953 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9954 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9955 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9956 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9957 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9958 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9959 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9960 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9961 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9962 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9963 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9964 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9965 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9966 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9967 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9968 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9969 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9970 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9971 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9972 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9973 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9974 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9975 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9976 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9977 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9978 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9979 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9980 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9981 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9982 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9983 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9984 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9985 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9986 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9987 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9988 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9989 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9990 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9991 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9992 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9993 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9994 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9995 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9996 mean train loss:  1.10202740e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9997 mean train loss:  1.10202731e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9998 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n",
      "Epoch: 9999 mean train loss:  1.10202721e-02, mean val. rec. loss:  6.54558846e-03\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = train(net_H2, loaders, args, A, H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NN_library/training_data/PINN_H2_{total_params}', np.vstack([losses_train, losses_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGwCAYAAACOzu5xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKxElEQVR4nO3dd3wUdf7H8ddm0zshpAGBYGgxECBEpIiighXFXlGU844Tz8Kp51nOfvjz7CdY7+x32BALeAgIiIIQQQQJvYUSCAGSkELK7vz+GLIQQ0lgdye7eT8fj33sZOa7M5+dqHn7ne98x2YYhoGIiIiIDwiwugARERGRxlJwEREREZ+h4CIiIiI+Q8FFREREfIaCi4iIiPgMBRcRERHxGQouIiIi4jMCrS7A3ZxOJ9u3bycqKgqbzWZ1OSIiItIIhmGwb98+UlJSCAg4cr+K3wWX7du30759e6vLEBERkeOwZcsW2rVrd8TtfhdcoqKiAPOLR0dHW1yNiIiINEZpaSnt27d3/R0/Er8LLnWXh6KjoxVcREREfMyxhnlocK6IiIj4DAUXERER8RkKLiIiIuIz/G6Mi4iI+A+Hw0FNTY3VZYgbBAUFYbfbT3g/Ci4iItLsGIbBjh07KC4utroUcaPY2FiSkpJOaJ41BRcREWl26kJLQkIC4eHhmlDUxxmGQUVFBYWFhQAkJycf974UXEREpFlxOByu0NK6dWuryxE3CQsLA6CwsJCEhITjvmykwbkiItKs1I1pCQ8Pt7gScbe63+mJjFtScBERkWZJl4f8jzt+pwouIiIi4jMUXERERMRnKLiIiIg0Qx07duSFF16wuoxmR3cVNVJJZQ2llTVEhQYSGx5sdTkiItIMnXHGGfTq1cstgSM3N5eIiIgTL8rPqMelkZ76ehWnPT2bdxdstroUERHxUYZhUFtb26i2bdq00Z1Vh6Hg0kghgeapqq51WlyJiEjLYxgGFdW1lrwMw2hUjaNGjWLu3Lm8+OKL2Gw2bDYbb7/9NjabjenTp9O3b19CQkKYN28e69ev5+KLLyYxMZHIyEhycnKYOXNmvf399lKRzWbjzTff5JJLLiE8PJzOnTvzxRdfuPM0+wRdKmqk4Lrg4lBwERHxtsoaBxl/m27JsfMeO4fw4GP/uXzxxRdZs2YNmZmZPPbYYwCsWLECgHvvvZdnnnmGTp06ERsby9atWzn//PN54oknCA0N5Z133mH48OGsXr2a1NTUIx7j0Ucf5emnn+Yf//gH//znP7nuuuvYvHkzcXFx7vmyPkA9Lo0UbFePi4iIHFlMTAzBwcGEh4eTlJREUlKSa3bYxx57jKFDh3LSSSfRunVrsrKy+MMf/kCPHj3o3LkzTzzxBJ06dTpmD8qoUaO45pprSE9P5+9//zvl5eUsWrTIG1+v2VCPSyPV9bhUKbiIiHhdWJCdvMfOsezYJ6pv3771fi4vL+fRRx/lq6++Yvv27dTW1lJZWUl+fv5R99OzZ0/XckREBFFRUa7n/7QUCi6NFKQeFxERy9hstkZdrmmufnt30D333MP06dN55plnSE9PJywsjMsvv5zq6uqj7icoKKjezzabDaezZf1d8t1/CrxMY1xERORYgoODcTgcx2w3b948Ro0axSWXXAJAWVkZmzZt8nB1/kFjXBqpx7ZJvB/0JH32/s/qUkREpJnq2LEjCxcuZNOmTRQVFR2xNyQ9PZ3JkyezdOlSfvnlF6699toW13NyvBRcGqlVxWYG2VfQumqr1aWIiEgzdffdd2O328nIyKBNmzZHHLPy/PPP06pVKwYMGMDw4cM555xz6NOnj5er9U26VNRItkBztlyb8/gfxS0iIv6tS5cuLFiwoN66UaNGNWjXsWNHvv3223rrxo4dW+/n3146Otx8MsXFxcdVpy9Tj0sj1QWXAMfRB06JiIiI5yi4NJLNHmK+OxVcRERErKLg0ki2oAM9LrpUJCIiYhkFl0ayBYYCEKAeFxEREcsouDRSwIExLnZDPS4iIiJWUXBpJHuQOcbFrktFIiIillFwaaS64BKoHhcRERHLKLg0kqvHxai1uBIREZGWS8GlkeqCS5ChwbkiIuIZHTt25IUXXnD9bLPZmDJlyhHbb9q0CZvNxtKlS0/ouO7ajzdo5txGsgebdxUFUothGNhsNosrEhERf1dQUECrVq3cus9Ro0ZRXFxcLxC1b9+egoIC4uPj3XosT1BwaaTAAz0uwdRS7XASEmi3uCIREfF3SUlJXjmO3W732rFOlC4VNVLggR6XIGqprtUTPEVEpL7XXnuNtm3bNnjK80UXXcSNN97I+vXrufjii0lMTCQyMpKcnBxmzpx51H3+9lLRokWL6N27N6GhofTt25eff/65XnuHw8Ho0aNJS0sjLCyMrl278uKLL7q2P/LII7zzzjt8/vnn2Gw2bDYbc+bMOeylorlz53LKKacQEhJCcnIy9913H7W1B8d5nnHGGdx+++3ce++9xMXFkZSUxCOPPNL0E9dE6nFppCBXj0sNNY6GD7oSEREPMgyoqbDm2EHh0IjhAVdccQW33347s2fP5qyzzgJg7969TJ8+nS+//JKysjLOP/98nnjiCUJDQ3nnnXcYPnw4q1evJjU19Zj7Ly8v58ILL+TMM8/k/fffZ+PGjdxxxx312jidTtq1a8dHH31EfHw88+fP5/e//z3JyclceeWV3H333axcuZLS0lLeeustAOLi4ti+fXu9/Wzbto3zzz+fUaNG8e6777Jq1SpuueUWQkND64WTd955h3HjxrFw4UIWLFjAqFGjGDhwIEOHDj3m9zleCi6NFFA3ONemHhcREa+rqYC/p1hz7Pu3Q3DEMZvFxcVx7rnn8p///McVXD7++GPi4uI466yzsNvtZGVludo/8cQTfPbZZ3zxxRfcdtttx9z/Bx98gMPh4N///jfh4eGcfPLJbN26lT/+8Y+uNkFBQTz66KOun9PS0pg/fz4fffQRV155JZGRkYSFhVFVVXXUS0MTJ06kffv2vPzyy9hsNrp168b27dv5y1/+wt/+9jcCAswLNj179uThhx8GoHPnzrz88svMmjXLo8FFl4oay27OnBusS0UiInIE1113HZ9++ilVVVWAGTauvvpq7HY75eXl3HvvvWRkZBAbG0tkZCSrVq0iPz+/UfteuXIlWVlZhIeHu9b179+/QbtXX32Vvn370qZNGyIjI3njjTcafYxDj9W/f/96N6IMHDiQsrIytm7d6lrXs2fPep9LTk6msLCwScdqKvW4NFbgwcG5xQ6HxcWIiLQwQeFmz4dVx26k4cOH43Q6mTp1Kjk5OcybN4/nnnsOgHvuuYfp06fzzDPPkJ6eTlhYGJdffjnV1Y2bZsMwjj1M4aOPPuKuu+7i2WefpX///kRFRfGPf/yDhQsXNvo71B3rt3fP1h3/0PVBQUH12thstgZjfNxNwaWx7OYvJ5gaqtTjIiLiXTZboy7XWC0sLIxLL72UDz74gHXr1tGlSxeys7MBmDdvHqNGjeKSSy4BoKysjE2bNjV63xkZGbz33ntUVlYSFhYGwI8//livzbx58xgwYAC33nqra9369evrtQkODsZxjP8Bz8jI4NNPP60XYObPn09UVBRt27ZtdM2eoEtFjWU/MMZFl4pEROQorrvuOqZOncq///1vrr/+etf69PR0Jk+ezNKlS/nll1+49tprm9Q7ce211xIQEMDo0aPJy8tj2rRpPPPMM/XapKen89NPPzF9+nTWrFnDQw89RG5ubr02HTt2ZNmyZaxevZqioiJqaho+yubWW29ly5Yt/OlPf2LVqlV8/vnnPPzww4wbN841vsUqCi6NdaDHxW4zGt2tJyIiLc+ZZ55JXFwcq1ev5tprr3Wtf/7552nVqhUDBgxg+PDhnHPOOfTp06fR+42MjOTLL78kLy+P3r1788ADD/B///d/9dqMGTOGSy+9lKuuuop+/fqxe/fuer0vALfccgtdu3Z1jYP54YcfGhyrbdu2TJs2jUWLFpGVlcWYMWMYPXo0Dz74YBPPhvvZjMZcNPMhpaWlxMTEUFJSQnR0tPt2XF3uGtH+w1XLGdj92LeuiYhI0+3fv5+NGzeSlpZGaGio1eWIGx3td9vYv9/qcWmsA3cVAThq9ltYiIiISMul4NJYAYE4MQco1VZXWVyMiIhIy6Tg0lg2G7UHbsKqrVaPi4iIiBUUXJqg1mYO0K2t0eBcERERKyi4NIHjQHBxqMdFRMTj/OzeEcE9v1MFlyZwBNT1uGiMi4iIp9TNxlpRYdFDFcVj6n6nv51xtyk0c24T1PW4aIyLiIjn2O12YmNjXc+8CQ8PbzD9vPgWwzCoqKigsLCQ2NhY7Hb7ce9LwaUJnPZgqIEa3VUkIuJRdU8u9vQD+8S7YmNjj/pU6sZQcGkC48ClIs3jIiLiWTabjeTkZBISEg47Jb34nqCgoBPqaamj4NIERoA5CZ3mcRER8Q673e6WP3biPzQ4twmMA7PnOmsVXERERKyg4NIUgQeCi+4qEhERsYSCS1Oox0VERMRSCi5NYDsQXAz1uIiIiFhCwaUJbEFh5oJDwUVERMQKCi5NcSC42GsrLS5ERESkZVJwaQJ7SIT57tA8LiIiIlZQcGmCgOADPS4KLiIiIpZolsHlkksuoVWrVlx++eVWl1KPPTgcgECnxriIiIhYoVkGl9tvv513333X6jIaCAw1LxUFO/fjcOpx6yIiIt7WLIPLkCFDiIqKsrqMBoIOBJcwWzVlVbUWVyMiItLyNDm4fPfddwwfPpyUlBRsNhtTpkxp0GbixImkpaURGhpKdnY28+bNc0etlqsLLqFUsW+/HvolIiLibU1+yGJ5eTlZWVncdNNNXHbZZQ22f/jhh9x5551MnDiRgQMH8tprr3HeeeeRl5dHamoqANnZ2VRVNRwn8s0335CSktKkeqqqqurtq7S0tInfqAkCzcG5oVRTWlkLrTx3KBEREWmoycHlvPPO47zzzjvi9ueee47Ro0fzu9/9DoAXXniB6dOn88orrzB+/HgAFi9efJzlNjR+/HgeffRRt+3vqA7M4xJmq6ZUPS4iIiJe59YxLtXV1SxevJhhw4bVWz9s2DDmz5/vzkO5/PWvf6WkpMT12rJli0eOAxwMLlRTUqngIiIi4m1N7nE5mqKiIhwOB4mJifXWJyYmsmPHjkbv55xzzmHJkiWUl5fTrl07PvvsM3Jycg7bNiQkhJCQkBOqu9GCzNuhQ6miVMFFRETE69waXOrYbLZ6PxuG0WDd0UyfPt3dJblHUCgAobZqSvfrriIRERFvc+ulovj4eOx2e4PelcLCwga9MD7pQI9LGNXqcREREbGAW4NLcHAw2dnZzJgxo976GTNmMGDAAHceyhquMS5VGuMiIiJigSZfKiorK2PdunWunzdu3MjSpUuJi4sjNTWVcePGMXLkSPr27Uv//v15/fXXyc/PZ8yYMW4t3BJ1Y1xsNRSV6gnRIiIi3tbk4PLTTz8xZMgQ18/jxo0D4MYbb+Ttt9/mqquuYvfu3Tz22GMUFBSQmZnJtGnT6NChg/uqtsqBHheA4pISCwsRERFpmZocXM444wwM4+jP6bn11lu59dZbj7uoZisoHMNmx2Y4KNu3x+pqREREWpxm+ayiZstmwxkSDUDVvuJjBjgRERFxLwWXJgoINYNLmLOM3eXVFlcjIiLSsvhNcJkwYQIZGRlHnKjOXWwHgkuUrZKNReUePZaIiIjU5zfBZezYseTl5ZGbm+vZA4XEABBNOat27PPssURERKQevwkuXnNIj8uqAg8+iVpEREQaUHBpqgODc6Oo4McNuy0uRkREpGVRcGmqUPNSUUxAJet3lZO/u8LigkRERFoOBZemOhBcOkebD1n86KctVlYjIiLSoii4NFVkAgA9YqoA+O+ifCqq9aRoERERb1BwaaoDwSUhoJj2cWHsLq/m7fmbrK1JRESkhVBwaarIJAACygq56+wuALw6Zz0lFXpatIiIiKcpuDRVVKL5XraTi7NS6JoYRen+Wp6bsdraukRERFoABZemijwQXGr3Y6/Zx9+GZwDw3o+bWbFdT4wWERHxJAWXpgoKg7A4c3nvZgamx3NBz2ScBjz8+Qo9eFFERMSD/Ca4eOtZRQDEm2NbKFoDwIMXdCc82M5Pm/cyeck2zx9fRESkhfKb4OK1ZxUBxHc234vWApAcE8afzjTXjf96Ffv2a6CuiIiIJ/hNcPGquh6Xnb+6Vo0elEan+AiKyqp4+dt1FhUmIiLi3xRcjkf7U8z3/AVwYExLcGAAD11oDtT99w8b2bCrzKrqRERE/JaCy/FI6QOBYVCxG3aucK0e0i2BIV3bUOMweGLqSgsLFBER8U8KLscjMBg6nW4ur/is3qYHL8wgMMDGt6sKmb260ILiRERE/JeCy/HqeaX5vuwjcDpcq09qE8lNAzsC8PT/VuN06vZoERERd1FwOV5dz4fQWCjJh1VT62269Yx0IkMCWVlQyvQVO6ypT0RExA8puByvoDDI+Z25/MOLrkG6AK0igrl5UBoAz89cg0O9LiIiIm6h4HIi+v0B7CGw7SfYOLfeptGD0ogKDWTNzjKmLS+wqEARERH/ouByIiITIHuUuTzz0Xq9LjFhQYw+0OvyxrwNehSAiIiIGyi4nKjBd0NQBGxfAiu/rLdp5KkdCAkMYNnWEnI37bWoQBEREf+h4HKiIhOg/1hz+dvHwVHr2tQ6MoRL+7QD4M15G6yoTkRExK8ouLjDgNvMJ0YXrYFf/ltv0+hBHQGYsXInm4rKLShORETEf/hNcPHq06F/KzQGThtnLs/9P6itcm1KT4ji9C5tMAyYlLvF+7WJiIj4Eb8JLl59OvTh5PwOolKgZAv89Fa9Tdf2SwXgk8VbqXE4rahORETEL/hNcLFcUBicfq+5PO8ZqDr4kMUzuyUQHxlCUVkVs1bqMQAiIiLHS8HFnXpfD3GdoHwXLHzFtTrIHsDl2eYg3Q9z862qTkRExOcpuLiTPQiGPGAu//BPqNjj2nRVTnsA5q7ZRUFJpRXViYiI+DwFF3c7+VJIzISqElj0umt1WnwEp6TF4TTg86XbLSxQRETEdym4uFtAwME7jBa9DjUHe1cu6d0WgCk/b7OiMhEREZ+n4OIJ3S+G2FSo2A1L/+NafX5mMsH2AFbt2MeqHaUWFigiIuKbFFw8wR4I/W8zlxe8DE4HADHhQQzp1gaAKT/rcpGIiEhTKbh4Su/rITQW9myAdbNcq0f0Mi8Xfb50G06nHrwoIiLSFAounhIcAb2uM5cXv+1aPaRbAlGhgRSU7Gfhxj2H/6yIiIgcloKLJ2XfaL6v+R+UFgAQGmTn/MxkwOx1ERERkcZTcPGkNl0htT8YDlj6vmv1iAN3F01dXsD+GodV1YmIiPgcBRdPyx5lvi95DwxzTEu/tDiSY0LZt7+W2av0CAAREZHGUnDxtO7DISgCijfDtsUABATYuPjAIN3JmtNFRESk0RRcPC04ArqeZy4v/8S1um4yujmrC9lbXm1FZSIiIj7Hb4LLhAkTyMjIICcnx+pSGupxufm+YrJrTpeuSVFkJEdT4zCYurzAwuJERER8h98El7Fjx5KXl0dubq7VpTR00lnmnC5lO2HT967Vdb0un+lykYiISKP4TXBp1gKDIeMic3nFZ67VF/VKIcAGizfvJX93hUXFiYiI+A4FF2/pfrH5vvprcDoBSIwOZWB6PKBeFxERkcZQcPGWtNMgOArKdsD2n12r6x4BMGXpNgxDjwAQERE5GgUXbwkMgfSzzOXVU12rz81MIizIzsaicpZuKbamNhERER+h4OJN3S4w31dNc62KCAlk2MmJAEzR5SIREZGjUnDxps5DwWaHXSvNp0YfUHd30ZfLCqhxOK2qTkREpNlTcPGmsFbQcaC5fEivy6D0eOIjg9lTXs13a3ZZVJyIiEjzp+DibV3PN9/XfuNaFWgPYHhWCqC7i0RERI5GwcXb0oea75vnQ9U+1+pLe7cDYEbeTkr311hRmYiISLOn4OJtrU+CVmngrIGN37lWZ7aN5qQ2EVTVOvnf8h0WFigiItJ8Kbh4m80GnYeZy4dcLrLZbFzax+x1+XTJVisqExERafYUXKzQ+cDlorUz4JBJ5y7p3RabDRZu3MOWPXoEgIiIyG8puFih4yAIDIXSbVCY51qdEhvGoAOPAFCvi4iISEMKLlYICoO0weby2hn1Nl2effBykdOpRwCIiIgcSsHFKq5xLvWDy7CMJCJDAtmyp5JFm/ZYUJiIiEjzpeBilfSzzff8BbC/xLU6LNjOhT2TAfhksS4XiYiIHErBxSpxadC6MxgOWD+73qa6y0XTlhdQXlVrRXUiIiLNkt8ElwkTJpCRkUFOTo7VpTRe3eWidfUvF2V3aEXH1uFUVDv4+lfN6SIiIlLHb4LL2LFjycvLIzc31+pSGq/zgctFv7kt2mazuXpdPlm8xYrKREREmiW/CS4+qcNACAqHsp2wY1m9TZf0aYfNBj9u0JwuIiIidRRcrBQYAp3OMJcPmUUXoG1sGANPMud0+TBXvS4iIiKg4GK9uruL1s5ssOnafqkATMrdQnWt05tViYiINEsKLlarm/5/6yKoqD9vy9CMRBKjQygqq+J/KzRIV0RERMHFarGp0KY7GE5Y/229TUH2AK45xex1eX/BZiuqExERaVYUXJoD10MXv2mw6ZpTUrEH2Fi0aQ+rdpR6uTAREZHmRcGlOehyjvm+dgY4HfU2JUaHcs7JiQC8q14XERFp4RRcmoP2/SAkBir3wLbFDTaPPLUjAJ8t2cbe8movFyciItJ8KLg0B/YgSD/TXF4zvcHmUzvFcXJKNJU1Dt7/Ub0uIiLScim4NBed6y4XNQwuNpuN3w/uBMA7Czaxv8bRoI2IiEhLoODSXHQeCthgx3Io3d5g8/k9kmkbG0ZRWTWTl2zzfn0iIiLNgIJLcxERD22zzeXD3F0UZA/g5kFpALw5bwNOp9GgjYiIiL9TcGlO6u4uWtMwuABcndOe6NBANhSVM2PlTi8WJiIi0jwouDQnnYeZ7xvmQG1Vg80RIYFcf2oHAN74boMXCxMREWkeFFyak+QsiEyCmnLY9P1hm4wa0JEgu42fNu9l2dZi79YnIiJiMQWX5sRmO2QW3RmHbZIQHcqFPVMAeOuHTV4qTEREpHlQcGluuhz5tug6Nw3sCMBXy7ZTWLrfC0WJiIg0DwouzU2nMyAgCPZsgKJ1h23Ss10sfTu0osZhaEI6ERFpURRcmpuQKOg40Fw+aq+LeWv0f3O3UOtweqMyERERyym4NEd1dxcdZvr/OkMzEomLCGbXviq+X1fkpcJERESspeDSHNVN/795PlTtO2yT4MAALsoyB+lqJl0REWkp/Ca4TJgwgYyMDHJycqwu5cTFp0NcJ3DWwPrZR2x2Se+2AHyTt4OyqlpvVSciImIZvwkuY8eOJS8vj9zcXKtLcY+jPHSxTs92MXRsHc7+Gifz1uzyUmEiIiLW8Zvg4ne6HBjnsnYGOA8/+NZms3F290QAZq4s9FZlIiIillFwaa46DITgSCjbCduXHLHZ2RlmcJm9uhCHHrwoIiJ+TsGluQoMOTgZXd6UIzbr26EVMWFB7CmvZkn+Xu/UJiIiYhEFl+Ys42LzfcXnYBy+NyXQHsAZXdsAMHuVLheJiIh/U3BpztKHQlA4lOQf9XLRkK4JAMxZrQG6IiLi3xRcmrPg8IOXi1ZMOWKzwV3aYLNBXkEpO0r07CIREfFfCi7NXcYI8z1vyhEvF8VFBJPVLhaAuWt0uUhERPyXgktz13koBIZBcT5s//mIzeouF81epctFIiLivxRcmrvgiIOXi5Z/fMRmdQN0v19XRI0euigiIn5KwcUXZF1tvi//GBw1h23So20MrSOCKauq5adNui1aRET8k4KLL0g/G8LjoXwXrJt12CYBATZOP9DrMme1xrmIiIh/UnDxBfYg6HmlufzLf47Y7AzdFi0iIn5OwcVXZF1jvq/+Gir2HLbJ4M7xBNhg9c59bCuu9GJxIiIi3qHg4iuSe0JiJjiqYcXkwzaJDQ+mT2orQJeLRETEPym4+JK6XpdfJh2xSd3dRTPzdnqjIhEREa9ScPElPa8Emx225sKuNYdtcl6PZADmrtnFlj0V3qxORETE4xRcfElkAnQeZi4fYZDuSW0iOa1zPE4D3vtxsxeLExER8TwFF1/T65DLRU7HYZuMGtARgA9+3MzusiovFSYiIuJ5Ci6+psu5ENYK9hXAhtmHbTKkawKZbaMpr3bwz2/XeblAERERz1Fw8TWBIdDjCnN56eEvFwUE2Ljv3O6Aebno120l3qpORETEoxRcfFGva833lV9BZfFhmwzqHM8FPZNxOA3GfbSU/TWHv6wkIiLiSxRcfFFyL0jIAEfVEed0AXj84kziI4NZs7OMez5ZhmEY3qtRRETEAxRcfJHNdrDX5QiXiwDiIoL55zV9CAyw8eUv23l6+mqFFxER8WkKLr6qxyFzuuzZcMRm/U9qzRMjMgF4Zc56np+51lsVioiIuJ2Ci6+KSoQOA8zl1f87atOrT0nlwQvMwbovzVrL+K9X4nSq50VERHyPgosv63q++b562jGb/u60TjxwvhleXpu7gXEfLaW61unJ6kRERNxOwcWXdT3XfN88Hyr3HrP5LYM78Y/LexIYYGPK0u3c9PYiSvfXeLhIERER91Fw8WVxnaBNNzAcsG5Woz5yRd/2/GtUDhHBdn5Yt5srX11AQUmlhwsVERFxD78JLhMmTCAjI4OcnByrS/Guk84y3zd+1+iPnN6lDR/+oT9tokJYtWMfIyb8QN72Ug8VKCIi4j5+E1zGjh1LXl4eubm5VpfiXWmDzfdN85r0scy2MXx26wA6J0Sys7SKK19bwHdrdnmgQBEREffxm+DSYnXoD7YA85bokm1N+mi7VuF88scBnNopjrKqWm56O5ePcrd4qFAREZETp+Di60JjzJl0ocm9LgAxYUG8c/MpjOiVgsNpcO+ny3h+xhpNVCciIs2Sgos/SDvNfG/COJdDhQTaef6qXtw2JB2AF2et1UR1IiLSLCm4+IOOB4LLpu+Pexc2m427z+nKQxdmAOZEdRNmr3NHdSIiIm6j4OIP2vczp/8v3gzFJzZGZfSgNO47rxsA/5i+mkmL8t1RoYiIiFsouPiD0GhIzjKXN/9wwrsbc/pJ3H6mednowSm/Mn990QnvU0RExB0UXPxFx0Hm+3EM0D2cu4Z24aKsFGqdBn98fwmbisrdsl8REZEToeDiL1zjXE68xwXMMS9PX96TPqmxlFTWMPY/S6iqdbhl3yIiIsdLwcVfpJ5qzueyd2OT53M5ktAgOxOvy6ZVeBArtpcyftoqt+xXRETkeCm4+As3j3OpkxQTynNX9gLg7fmbmJG30237FhERaSoFF3/SYaD57qZxLnWGdEvgltPSAPjr5OUUV1S7df8iIiKNpeDiT9w8zuVQfx7WlfSESIrKqnjsyzy3719ERKQxFFz8SeqpgA32rIfS7W7ddWiQnacv70mADSb/vI1vV+mSkYiIeJ+Ciz8Ji4WUXubyullu332f1FaMHnTwklHp/hq3H0NERORoFFz8TdcLzPeVX3pk938e1pW0+Ah2llbpLiMREfE6BRd/0324+b5hNuwvdfvuQ4PsjL+0BwD/XZTPgvW73X4MERGRI1Fw8TdtukLrzuCohrXfeOQQp3ZqzbX9UgH46+RlVFZrYjoREfEOBRd/Y7Md7HVZ/onHDnPfed1Iig5l0+4KXpi5xmPHEREROZSCiz/qeZX5vvYbKCv0yCGiQ4N4YkQmAG/M28CyrcUeOY6IiMihFFz8UUI3aJsNhgOWf+yxw5ydkcjwrBScBtz7yTJqHE6PHUtERAQUXPxXr2vN958/AMPw2GEeHp5BbHgQq3bs47W56z12HBEREVBw8V+Zl4E9BApXQP4Cjx0mPjKEh4dnAPDSrHWsKyzz2LFEREQUXPxVWCvodY25/MNLHj3UiF5tOaNrG6odTu76cCn7a3SXkYiIeIaCiz/r/yfABmu+hkLPTRZns9n4+yU9iA0PYvm2Eh79coXHjiUiIi2bgos/i0+H7heay7Me8+ihUmLDeOnq3ths8N9FW/jvonyPHk9ERFomBRd/d+bfwGaH1VNh4zyPHmpwlzb8eWgXAB6c8iuzV3vmVmwREWm5FFz8XZsu0Pcmc3nqOKip9Ojhbj0jnUt6t8XhNLj1/SUs3VLs0eOJiEjLouDSEgx5ACIToWiNxy8ZBQTY+L/LenJa53gqaxzc/HYu63fpTiMREXEPBZeWIDwOLnrZXP5xoseeHF0nODCAV67PpkfbGPaUVzPyzYVsL/ZsT4+IiLQMCi4tRZdh0G+MuTz5D7DTs3f+RIYE8vZNOXRqE8H2kv2M/NdC9pRXe/SYIiLi/xRcWpJhT0La6VBTDh9cAcWevfOndWQI743uR3JMKOt3lTPqrUWUVdV69JgiIuLfFFxaEnsgXPE2xHeF0m3w7sWwb6dHD9k2Noz3Rp9Cq/Aglm0t4ffv/qQJ6kRE5LgpuLQ04XFwwxSI7QB7NsB7I6Bij0cPmZ4QxTs3n0JEsJ3563dzx6SfqdUDGUVE5DgouLRE0Slww+cQmQSFefD+ZVC1z6OH7Nkuljdu6EuwPYDpK3by4JRfMTz48EcREfFPCi4tVVya2fMSFgfbl8B/robqCo8eckB6PC9d05sAG0zK3cIHCzW7roiINI2CS0uW0B1GToaQaNj8PXx0A9R69s6fczOT+Mu53QB47Ms8ft1W4tHjiYiIf1FwaelSesO1H0FgGKybAZN/Bw7P3vnz+8GdOLt7ItUOJ2P/s4SKat1pJCIijaPgItChP1z9AdiDIe9zmP5Xjx7OZrPx7BVZpMSEsnl3BU//b7VHjyciIv5DwUVM6WfBZW+ay4tehyXvevRwMeFBPHVZTwDenr+JHzfs9ujxRETEPyi4yEEZF8MZ95vLX42DLbkePdzgLm245pT2ANz7yTJdMhIRkWNScJH6Bt8D3YeDswY+vRn2l3r0cPef352UmFDy91Twf1+v8uixRETE9/lNcJkwYQIZGRnk5ORYXYpvCwiAiydCbKr5SICv7/Xo4aJCg/i/y81LRu8s2Mz89UUePZ6IiPg2vwkuY8eOJS8vj9xcz17eaBFCo+HSN8AWAL/8F/K+8OjhTuvchmv7pQLmJaNyPc9IRESOwG+Ci7hZ6qkw6C5z+et7PT6z7v3nd6dtbBhb91by2Jd5mlVXREQOS8FFjmzwvdAqDfYVwOy/e/RQkSGB/OPynths8OFPW3h7/iaPHk9ERHyTgoscWVAoXPCMubzwVdix3KOHG5Aez1/POzCr7ld5fJS7xaPHExER36PgIkeXfjZkjADDCTMe9vjhbjmtEyNP7YBhwL2fLuOlWWtxOnXZSERETAoucmxnPwIBQbB+FmyY49FD2Ww2Hrv4ZH4/uBMAz81Yw6i3cykoqfTocUVExDcouMixxaVBzmhzecbfwOn06OFsNhv3n9+dpy/vSWhQAN+t2cXZz87lzXkbqHF49tgiItK8KbhI4wy+B4KjoOAXyJvilUNe2bc9X9w2iD6psZRXO3hi6krOfHYOH+VuUYAREWmhFFykcSLiof9Yc/m7Zzze61KnS2IUn4wZwN8v6UF8ZDBb9lRy76fLGPz0bP45ay279lV5pQ4REWkebIafTZhRWlpKTEwMJSUlREdHW12Of6ncC8/3gOp9cNX75qMBvHn4agfv/7iZV+euZ3d5NQBBdhund0lgeFYyZ3VPJDIk0Ks1iYiIezT277eCizTNrMdh3jOQ1BP+8B3YbF4vYX+Ng69/LeC9BZtZkl/sWh8SGMDgLm04o2sbzuiaQNvYMK/XJiIix0fBRcHFM8p3wws9oKYcrvkQup5raTmrd+zjq2Xb+WpZARuLyutt65wQyWmd29CvUxw5HeOIiwi2qEoRETkWBRcFF8/55iGY/xK0zYbfzbKk1+W3DMMgr6CUb1cWMnfNLpbk7+W30790SYzklDQzxJySFkdyjHpkRESaCwUXBRfPKSuEF3pCbSVc/6k5SV0zU1JRw7x1u1iwfjeLNu5hbWFZgzaJ0SFktYulV2osvdrF0qNdDFGhQRZUKyIiCi4KLp71v7/CjxOhfT+4eXqz6HU5mt1lVeRu2kvupj0s3LiblQX7cPymS8Zmg/Q2kfRqH0tW+1gyUqLplhRFeLAG/IqIeJqCi4KLZ5UWwItZ4KiCG76ATqdbXVGTVFTXsmJ7KUvzi1m6tZil+cVsK244O6/NBh1bR9A9OYruSdF0T46me0o0KTGh2Jp5WBMR8SUKLgounjf1bsh9AzqeBqO+srqaE7ZrXxW/bClm6ZZilm0rYWVB6RHniQkLstMxPoK0+HDS4iNIi4+kR9sYOrQOJzTI7uXKRUR8n4KLgovnlWyFF3uBswZu+ho6DLC6IrcrKqtiZUHpgdc+VhaUsq6wjNqjPPgxOSaUtPgITmoTSac25vtJCZEkR4cSEKBeGhGRw1FwUXDxji/vgMVvQ6chcMMUq6vxihqHk617K9lYVMaGXeVs2l3O6h37WLVjH/v21x7xc6FBAXSKN0NMp/gI0hMiyWwbQ4e4cAUaEWnxFFwUXLxj72b4Zx9w1sLomdA+x+qKLGMYBnsrathYVM7GonLW7ypjfWEZ63eVsXl3xRF7aaJCAjm5bTQ92sbQO7UV/dLiaB0Z4uXqRUSspeCi4OI9U8bC0vchfShc/4nV1TRLNQ4nW/ZUsH5XORt2mWFm9c4yVhaUUl3b8LlPXROj6H9SawZ3iWdgejwhgRo3IyL+TcFFwcV7dq+Hl3PAcPjkHUZWqnE4WbuzjF+3lbB8Wwm5m/awase+em2iQgMZ2j2R83skc1oXhRgR8U8KLgou3jXtHlj0OiRmms8wCtAf1+O1u6yKhRv3MH99ETPydrKz9OCdTa3Cg7jmlFSuP7UDKXoWk4j4EQUXBRfvqtgDL/WC/SUw/CXIvtHqivyC02mwJH8vU5cX8PXyHewo3Q+APcDG4M7x3HNONzJS9M+5iPg+BRcFF+9bMAGm3w8RbWDsIgiPs7oiv+JwGsxcuZO3ftjIjxv2AGaAGT0ojXFDu2j+GBHxaY39+x3gxZrE3+XcAm26Qfku+OZBq6vxO/YAG+ecnMSk3/fnf3eexrCMRBxOg9e/28Dwf37Pr9tKrC5RRMTjFFzEfQKD4aJ/AjZY+gGs/9bqivxWt6RoXr+hL2/e0Jf4yBDWFpYxYsIPvDRrLbWOhncpiYj4CwUXca/2p8ApvzeXv7wDqho+lVnc5+yMRL65azDnZSZR6zR4bsYaxry/mP01DqtLExHxCAUXcb+z/gYxqVCcD7Mes7oavxcXEczE6/rw3JVZhAQGMHNlITf8axHlVUeexVdExFcpuIj7hUTCRS+ay4teg03fW1tPC2Cz2bi0TzveG92PqNBAFm3aw9j/LKFGl41ExM8ouIhnnHQm9DlwS/Tnt0F1ubX1tBCnpMXxzs2nEBoUwJzVu7hj0s+HnZlXRMRXKbiI5wx7AqLbwd6NMOtxq6tpMfqktmLidX2wB9iYtnwHb/2w0eqSRETcRsFFPCc0+uAlo4WvwuYF1tbTgpzZLZFHhmcA8PzMNWzZU2FxRSIi7qHgIp6Vfjb0vh4w4POxUK0/oN5y/akd6N+pNftrnDww5Vf8bK5JEWmhFFzE84Y9CVEpsGc9zH7S6mpaDJvNxpOXZBIcGMB3a3bxxS/brS5JROSEKbiI54XFwvADl4wWTID8hZaW05J0ahPJbUPSAXj8qzyKK6otrkhE5MQouIh3dBkGWdcCBnx1Jzg1QZq3jDn9JNITIikqq+apr1dZXY6IyAlRcBHvOedJCI2Bwjz4ZZLV1bQYwYEBjL+0BwCTcrewfKueaSQivkvBRbwnPA5O+7O5PPtJqKm0tp4WJKdjHCN6pQDwxNQ8DdQVEZ+l4CLedcofzLldSrdB7r+srqZFuefcboQEBrBw4x6+ydtpdTkiIsdFwUW8KygUTr/XXF7wMtRWWVtPC9I2NoxbTusEwPhpKzWjroj4JAUX8b6sqyEqGfYVwLKPrK6mRRlzxknER4awaXcF7/242epyRESaTMFFvC8wBPqPNZd/eAGc+j9/b4kMCeTPw7oA8NKstbo9WkR8joKLWCN7lHmH0e51sH6W1dW0KFf2bU+3pChKKmt4adY6q8sREWkSBRexRkgU9LreXM5909paWhh7gI0HLugOwLsLNrF+V5nFFYmINJ6Ci1in783m+5rpsFfjLbzptM5tGNK1DbVOgz/952cqqzUhoIj4BgUXsU58OnQaAhjw07+trqbFefKSHrSOCCavoJRb3v1J4UVEfIKCi1gr53fm+8/vQc1+a2tpYVJiw3j9hmwigu18v66IK16bz8aicqvLEhE5KgUXsVaXc80J6Sp2w4rJVlfT4mR3iOOdm0+hVXgQv24rZcgzc+h431QqqmutLk1E5LAUXMRa9kDIGW0u//gKaCp6r+vbMY6pt59GTsdWrnWn/2MOb87bwL79NRZWJiLSkIKLWC97FASGwY5lkL/A6mpapJTYMD4eM4DhWebzjHbtq+KJqSs59e+zePjzX3XnkYg0GzbDz562VlpaSkxMDCUlJURHR1tdjjTWF7fDkneg+3C46n2rq2nRqmudTF6ylTe/38i6woaB5ds/n06nNpEWVCYi/qyxf78VXKR5KFwJE08FWwD8cQEkdLO6ohbPMAx+WLebt+dv5NtVhTgP+S/FuScncX7PZM7slkBkSKB1RYqI31BwUXDxPZOug1VfQZfz4NpJVlcjh9i1r4o//XcJP27YU299kN1Gv7TWnNG1DWd2S6B9XDi1DoOwYLtFlYqIr1JwUXDxPUVrYUI/MBxw5XuQcZHVFclvGIbBiu2lfP1rAV8v38GGI9w+Peb0k7h5YEcSokO9XKGI+CoFFwUX3zTjYfPBi6ExMHomtOlidUVyFBt2lfHtqkJmry5k0cY91Djq/+ekfVwY2amtyO7Qit6preiSGEVwoO4JEJGGFFwUXHyTowbeOg+25kJkElz/KSRlWl2VNEJZVS3vzN/EP6avpnVEMHsqqhvc3R5sD6BzYiRxEcHMW1vE7wd3YuyQdGLCgqwpWkSaDQUXBRffVb4b3rkQCvMgKByGvwg9rgCbzerKpAlK99ewNL+YxZv3siR/L0u3FLNv/+EntmvXKoyM5GhOSogkrXUEHeMjaNsqjMpqB+kJuoNJpCVQcFFw8W0Ve+DT0bD+W/Pn7hfBBc9BZBtr65LjZhgGW/dWsmJ7KfPW7uKDhfmN/uxlfdrRrlXYgVc47VqFkRwTij3ARllVLVGh6rER8XUKLgouvs/pgO/+Yb6ctRDWCk7/C/QdDYHBVlcnblJSUUNeQSkrC0rZWFTOpt3lbCwqZ+veyqN+zh5gw3HIPdqPXnQyIYEB9TrmbNT7QUTcpE9qK7f3hiq4KLj4j4JfYMpY2Lnc/DnuJDj9Xsi8DOz6P21/tb/Gwfz1RZRU1rC9eD9b91aydW8F2/ZWsrW4kupap9UlirRYj4/IZOSpHdy6TwUXBRf/4qiFn9+F2X+H8l3muuh20P9WyLoGwuOsrU+8yuk0KCqrYt2uMq59YyFZ7WJIjgmjxnH4MONX/5ETaQZGntqBId0S3LpPBRcFF/+0vxQWvQ4LX4PyQnOdPQS6Xwi9R0LaYAjQ5GciIr5GwUXBxb/V7Idlk2DRmwcvIYF5C/XJI+DkS6FdDgRozhAREV+g4KLg0jIYBhQshSXvwfJPoKrk4LbodmaIybwUUvrodmoRkWbMZ4PLli1bGDlyJIWFhQQGBvLQQw9xxRVXNPrzCi4tWG2Vefv0r5Nh9TSoPuTJxjHtodsF0O1CSO0Pdj0YUESkOfHZ4FJQUMDOnTvp1asXhYWF9OnTh9WrVxMREdGozyu4CAA1lbB2BqyYDGumQ03FwW1hcdD1PDPEnDQEgsKsq1NERAAfDi6/1bNnT6ZOnUr79u0b1V7BRRqoqYT1s80nT6+eBpV7D24LioD0s6D7cOg8DMJiLStTRKQla+zf7yaPXPzuu+8YPnw4KSkp2Gw2pkyZ0qDNxIkTSUtLIzQ0lOzsbObNm9fUwwDw008/4XQ6Gx1aRA4rKAy6nQ8jJsLd6+DGL+GUP5hjYGrKYeUXMPkW+MdJ8N4lkPsm7NloddUiInIYTb7QX15eTlZWFjfddBOXXXZZg+0ffvghd955JxMnTmTgwIG89tprnHfeeeTl5ZGamgpAdnY2VVVVDT77zTffkJKSAsDu3bu54YYbePPNN5taosiR2QPNW6bTBsN5/2cO7F35ldkbs2uVOUam7jEDsR3MS0mdzoCOgyGitZWVi4gIJ3ipyGaz8dlnnzFixAjXun79+tGnTx9eeeUV17ru3bszYsQIxo8f36j9VlVVMXToUG655RZGjhx5zLaHhqDS0lLat2+vS0XSdEXrzACzZjpsXWQ+ZuBQcSdB2+yDr6QeEBRqTa0iIn6msZeK3HprRXV1NYsXL+a+++6rt37YsGHMnz+/UfswDINRo0Zx5plnHjO0AIwfP55HH330uOoVqSc+HQbdab6qymDzD7Bhjjk+ZtdK2LPefC3/yGwfEAjxXaFsJ1QUmevOfQqyb1KgERHxELcGl6KiIhwOB4mJifXWJyYmsmPHjkbt44cffuDDDz+kZ8+ervEz7733Hj169Dhs+7/+9a+MGzfO9XNdj4vICQmJhC7nmC8wn1a9fQlsWwJbf4Jti82wUrii/uf+dx9Mvx9adYQ23aFNV2jTDRK6Qf6PULQGLnjW619HRMRfeGQyC9tvJvoyDKPBuiMZNGgQTmfjH54WEhJCSEhIk+oTabLwOEg/23yBOfFd6TbYsRwWvmr2zNQxnLBng/laPbXhvnLfhJxbzJl9U3pB686a4VdEpJHcGlzi4+Ox2+0NelcKCwsb9MKI+DSbDWLama+u5x1cbxjmpaNdqw+8VprvBb/UnxAv9w3zBRASA237mBPjdRhgBhpdahIROSy3Bpfg4GCys7OZMWMGl1xyiWv9jBkzuPjii915KJHmyWaDqCTz1en0+tt+nQyf3GQu97gSijdDwTLzMQUbZpuv3xrzAyRler5uEREf0eTgUlZWxrp161w/b9y4kaVLlxIXF0dqairjxo1j5MiR9O3bl/79+/P666+Tn5/PmDFj3Fq4iM/JvNR8HcpRA4UrzbuYNs+HTT9A2SE9lq8OhA6DoNc1kHm5emJEpMVr8u3Qc+bMYciQIQ3W33jjjbz99tuAOQHd008/TUFBAZmZmTz//PMMHjzYLQUfi2bOFZ9mGLDsI/js9w23RSZCvzHQ92bN8CsifsdvpvxvKgUX8SvFW8zbr3P/DaVbzXXBUdC2t/nE67Mf0VOvRcQvKLgouIg/cdTA8k/ghxfNAb+Hemi3nnYtIj7PY88qEhEL2IPMcS63LoBrP6q/7Z0LYe9ma+oSEfEyBRcRX2KzmZPi/W3PwXX5C+C10w4+Y0lExI/5TXCZMGECGRkZ5OTkWF2KiOcF2OGRErjjF2jbF/aXwPuXQ97nVlcmIuJRGuMi4utqq2DKrfDrJ+bzk676ALqea3VVIiJNojEuIi1FYAhc+ro5z4uzFj4aaT4YUkTEDym4iPiDADtc8ip0uxAc1fDxjRqwKyJ+ScFFxF/Yg+Dytw6OefnkJqittroqERG3UnAR8SeBwXDFWxAaC9sWwxNtwOmwuioREbdRcBHxN7GpcMGzB3+edrd1tYiIuJmCi4g/yrzs4PJP/7auDhERN1NwEfFHNhuM+eHgzyu/tK4WERE3UnAR8VdJmQeXf3jJfPK0iIiPU3AR8Wd/Xg2BobB1EeT/aHU1IiInTMFFxJ9FJUHW1ebygpetrUVExA38JrjoWUUiR3Dqreb7qqmwZ4O1tYiInCC/CS5jx44lLy+P3Nxcq0sRaV7adIX0oYABP75qdTUiIifEb4KLiBxF/7Hm+8/vQ2WxpaWIiJwIBReRlqDTGdCmG9SUw7IPra5GROS4KbiItAQ2G/QdbS7n/ku3RouIz1JwEWkpsq6CoAgoWg2bvre6GhGR46LgItJShMZAzyvM5Z/+ZW0tIiLHScFFpCWpu1y04jPYPN/aWkREjoOCi0hLktzz4PJb51lXh4jIcVJwEWlpTvn9weX9pdbVISJyHBRcRFqaYU8eXH7tNOvqEBE5DgouIi1NYDDEpJrLezdB5V5LyxERaQoFF5GW6NZDBuZ+fhvUVFpXi4hIEyi4iLREIVFw5Xvm8qqv4N/nwLpZ8OH1sGqatbWJiByFzTD8YwrNCRMmMGHCBBwOB2vWrKGkpITo6GiryxJp3jZ+Bx+PgordDbddPBFSekF8V7AHws4VUFYIJw3xdpUi0gKUlpYSExNzzL/ffhNc6jT2i4vIAcVbYO5T5gMYDycoHNpmw6Z5B9c9XGw+RuC3nA4wnGAP8kipIuK/FFwUXESaxlEDyz+GKX8Eewi06wsFv0B12eHbn3QWJGZAwsmQeDLEd4GJ/cwBvw8VKbyISJMouCi4iJw4p9N8ttHm+TB1XNM+e9rdkHoqxLSD6LYQesi/j1t/gmn3wHUfQ0T8kfdhGIfv2RERv6PgouAi4hn7S2BnHuxaZY57Kcwz3/cXH/1zwVFmiIlpB+tmHFwf0QYu+5cZYMJaQVgcBIXCI7GAAX+YV3/G30PlL4Ql78IFz0BQmJu+oIhYQcFFwUXEewwDSrbC9Pth5Rfmuk5DzMG8pduOHWp+KzAMan9zi3bvkeaDIkOizbuiQmPg81sPbh/+ktnDIyKeF5lg/o+GGym4KLiINB9VZVC6HUq2mAHnyzuAQ/7T0zrdnAivshgMh1VVikhjXfAs5PzOrbts7N/vQLceVUTkcEIioU0X8wWQfePh2xmGeSmqrBDWzYTpfzXXt+kOPS4zt1WVQdU+M+isn1X/86GxGhMj4g32EMsOreAiIs2HzQZhsearTRfof+uxPiEiLYxmzhURERGfoeAiIiIiPkPBRURERHyGgouIiIj4DAUXERER8RkKLiIiIuIzFFxERETEZ/hNcJkwYQIZGRnk5ORYXYqIiIh4iKb8FxEREcs19u+33/S4iIiIiP9TcBERERGfoeAiIiIiPkPBRURERHyGgouIiIj4jECrC3C3upukSktLLa5EREREGqvu7/axbnb2u+Cyb98+ANq3b29xJSIiItJU+/btIyYm5ojb/W4eF6fTyfbt24mKisJms7ltv6WlpbRv354tW7ZofhgP07n2Dp1n79B59g6dZ+/w5Hk2DIN9+/aRkpJCQMCRR7L4XY9LQEAA7dq189j+o6Oj9S+Fl+hce4fOs3foPHuHzrN3eOo8H62npY4G54qIiIjPUHARERERn6Hg0kghISE8/PDDhISEWF2K39O59g6dZ+/QefYOnWfvaA7n2e8G54qIiIj/Uo+LiIiI+AwFFxEREfEZCi4iIiLiMxRcRERExGcouDTSxIkTSUtLIzQ0lOzsbObNm2d1Sc3W+PHjycnJISoqioSEBEaMGMHq1avrtTEMg0ceeYSUlBTCwsI444wzWLFiRb02VVVV/OlPfyI+Pp6IiAguuugitm7dWq/N3r17GTlyJDExMcTExDBy5EiKi4s9/RWbpfHjx2Oz2bjzzjtd63Se3WPbtm1cf/31tG7dmvDwcHr16sXixYtd23WeT1xtbS0PPvggaWlphIWF0alTJx577DGcTqerjc7z8fnuu+8YPnw4KSkp2Gw2pkyZUm+7N89rfn4+w4cPJyIigvj4eG6//Xaqq6ub9oUMOaZJkyYZQUFBxhtvvGHk5eUZd9xxhxEREWFs3rzZ6tKapXPOOcd46623jF9//dVYunSpccEFFxipqalGWVmZq81TTz1lREVFGZ9++qmxfPly46qrrjKSk5ON0tJSV5sxY8YYbdu2NWbMmGEsWbLEGDJkiJGVlWXU1ta62px77rlGZmamMX/+fGP+/PlGZmamceGFF3r1+zYHixYtMjp27Gj07NnTuOOOO1zrdZ5P3J49e4wOHToYo0aNMhYuXGhs3LjRmDlzprFu3TpXG53nE/fEE08YrVu3Nr766itj48aNxscff2xERkYaL7zwgquNzvPxmTZtmvHAAw8Yn376qQEYn332Wb3t3jqvtbW1RmZmpjFkyBBjyZIlxowZM4yUlBTjtttua9L3UXBphFNOOcUYM2ZMvXXdunUz7rvvPosq8i2FhYUGYMydO9cwDMNwOp1GUlKS8dRTT7na7N+/34iJiTFeffVVwzAMo7i42AgKCjImTZrkarNt2zYjICDA+N///mcYhmHk5eUZgPHjjz+62ixYsMAAjFWrVnnjqzUL+/btMzp37mzMmDHDOP30013BRefZPf7yl78YgwYNOuJ2nWf3uOCCC4ybb7653rpLL73UuP766w3D0Hl2l98GF2+e12nTphkBAQHGtm3bXG3++9//GiEhIUZJSUmjv4MuFR1DdXU1ixcvZtiwYfXWDxs2jPnz51tUlW8pKSkBIC4uDoCNGzeyY8eOeuc0JCSE008/3XVOFy9eTE1NTb02KSkpZGZmutosWLCAmJgY+vXr52pz6qmnEhMT06J+N2PHjuWCCy7g7LPPrrde59k9vvjiC/r27csVV1xBQkICvXv35o033nBt13l2j0GDBjFr1izWrFkDwC+//ML333/P+eefD+g8e4o3z+uCBQvIzMwkJSXF1eacc86hqqqq3qXXY/G7hyy6W1FREQ6Hg8TExHrrExMT2bFjh0VV+Q7DMBg3bhyDBg0iMzMTwHXeDndON2/e7GoTHBxMq1atGrSp+/yOHTtISEhocMyEhIQW87uZNGkSS5YsITc3t8E2nWf32LBhA6+88grjxo3j/vvvZ9GiRdx+++2EhIRwww036Dy7yV/+8hdKSkro1q0bdrsdh8PBk08+yTXXXAPon2dP8eZ53bFjR4PjtGrViuDg4CadewWXRrLZbPV+NgyjwTpp6LbbbmPZsmV8//33DbYdzzn9bZvDtW8pv5stW7Zwxx138M033xAaGnrEdjrPJ8bpdNK3b1/+/ve/A9C7d29WrFjBK6+8wg033OBqp/N8Yj788EPef/99/vOf/3DyySezdOlS7rzzTlJSUrjxxhtd7XSePcNb59Ud516Xio4hPj4eu93eIA0WFhY2SI5S35/+9Ce++OILZs+eTbt27Vzrk5KSAI56TpOSkqiurmbv3r1HbbNz584Gx921a1eL+N0sXryYwsJCsrOzCQwMJDAwkLlz5/LSSy8RGBjoOgc6zycmOTmZjIyMeuu6d+9Ofn4+oH+e3eWee+7hvvvu4+qrr6ZHjx6MHDmSu+66i/HjxwM6z57izfOalJTU4Dh79+6lpqamSedeweUYgoODyc7OZsaMGfXWz5gxgwEDBlhUVfNmGAa33XYbkydP5ttvvyUtLa3e9rS0NJKSkuqd0+rqaubOnes6p9nZ2QQFBdVrU1BQwK+//upq079/f0pKSli0aJGrzcKFCykpKWkRv5uzzjqL5cuXs3TpUterb9++XHfddSxdupROnTrpPLvBwIEDG9zOv2bNGjp06ADon2d3qaioICCg/p8ku93uuh1a59kzvHle+/fvz6+//kpBQYGrzTfffENISAjZ2dmNL7rRw3hbsLrbof/1r38ZeXl5xp133mlEREQYmzZtsrq0ZumPf/yjERMTY8yZM8coKChwvSoqKlxtnnrqKSMmJsaYPHmysXz5cuOaa6457O137dq1M2bOnGksWbLEOPPMMw97+13Pnj2NBQsWGAsWLDB69Ojh17c1HsuhdxUZhs6zOyxatMgIDAw0nnzySWPt2rXGBx98YISHhxvvv/++q43O84m78cYbjbZt27puh548ebIRHx9v3Hvvva42Os/HZ9++fcbPP/9s/PzzzwZgPPfcc8bPP//smtLDW+e17nbos846y1iyZIkxc+ZMo127drod2lMmTJhgdOjQwQgODjb69OnjurVXGgIO+3rrrbdcbZxOp/Hwww8bSUlJRkhIiDF48GBj+fLl9fZTWVlp3HbbbUZcXJwRFhZmXHjhhUZ+fn69Nrt37zauu+46IyoqyoiKijKuu+46Y+/evV74ls3Tb4OLzrN7fPnll0ZmZqYREhJidOvWzXj99dfrbdd5PnGlpaXGHXfcYaSmphqhoaFGp06djAceeMCoqqpytdF5Pj6zZ88+7H+Tb7zxRsMwvHteN2/ebFxwwQVGWFiYERcXZ9x2223G/v37m/R9bIZhGI3vnxERERGxjsa4iIiIiM9QcBERERGfoeAiIiIiPkPBRURERHyGgouIiIj4DAUXERER8RkKLiIiIuIzFFxERETEZyi4iIjfmzNnDjabjeLiYqtLEZETpOAiIiIiPkPBRURERHyGgouIeJxhGDz99NN06tSJsLAwsrKy+OSTT4CDl3GmTp1KVlYWoaGh9OvXj+XLl9fbx6effsrJJ59MSEgIHTt25Nlnn623vaqqinvvvZf27dsTEhJC586d+de//lWvzeLFi+nbty/h4eEMGDCA1atXe/aLi4jbKbiIiMc9+OCDvPXWW7zyyiusWLGCu+66i+uvv565c+e62txzzz0888wz5ObmkpCQwEUXXURNTQ1gBo4rr7ySq6++muXLl/PII4/w0EMP8fbbb7s+f8MNNzBp0iReeuklVq5cyauvvkpkZGS9Oh544AGeffZZfvrpJwIDA7n55pu98v1FxI2a9nBsEZGmKSsrM0JDQ4358+fXWz969GjjmmuuMWbPnm0AxqRJk1zbdu/ebYSFhRkffvihYRiGce211xpDhw6t9/l77rnHyMjIMAzDMFavXm0AxowZMw5bQ90xZs6c6Vo3depUAzAqKyvd8j1FxDvU4yIiHpWXl8f+/fsZOnQokZGRrte7777L+vXrXe369+/vWo6Li6Nr166sXLkSgJUrVzJw4MB6+x04cCBr167F4XCwdOlS7HY7p59++lFr6dmzp2s5OTkZgMLCwhP+jiLiPYFWFyAi/s3pdAIwdepU2rZtW29bSEhIvfDyWzabDTDHyNQt1zEMw7UcFhbWqFqCgoIa7LuuPhHxDepxERGPysjIICQkhPz8fNLT0+u92rdv72r3448/upb37t3LmjVr6Natm2sf33//fb39zp8/ny5dumC32+nRowdOp7PemBkR8U/qcRERj4qKiuLuu+/mrrvuwul0MmjQIEpLS5k/fz6RkZF06NABgMcee4zWrVuTmJjIAw88QHx8PCNGjADgz3/+Mzk5OTz++ONcddVVLFiwgJdffpmJEycC0LFjR2688UZuvvlmXnrpJbKysti8eTOFhYVceeWVVn11EfEABRcR8bjHH3+chIQExo8fz4YNG4iNjaVPnz7cf//9rks1Tz31FHfccQdr164lKyuLL774guDgYAD69OnDRx99xN/+9jcef/xxkpOTeeyxxxg1apTrGK+88gr3338/t956K7t37yY1NZX777/fiq8rIh5kMw69UCwi4mVz5sxhyJAh7N27l9jYWKvLEZFmTmNcRERExGcouIiIiIjP0KUiERER8RnqcRERERGfoeAiIiIiPkPBRURERHyGgouIiIj4DAUXERER8RkKLiIiIuIzFFxERETEZyi4iIiIiM/4f187XMdVKUP7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1 = net_H1(grid_data).detach().cpu()\n",
    "U2 = net_H2(grid_data).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1aa17f26790>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n",
    "pos1 = axs[0].pcolormesh(XY[0], XY[1], U1.reshape(N, N), cmap='inferno')\n",
    "fig.colorbar(pos1, ax=axs[0])\n",
    "pos2 = axs[1].pcolormesh(XY[0], XY[1], U2.reshape(N, N), cmap='inferno')\n",
    "fig.colorbar(pos2, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1aa0b8a5950>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_1 = PDE_loss(grid_data, net_H1, A, H1).detach().cpu()\n",
    "error_2 = PDE_loss(grid_data, net_H2, A, H2).detach().cpu()\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n",
    "pos1 = axs[0].pcolormesh(XY[0], XY[1], error_1.reshape(N, N), cmap='hot')\n",
    "#axs[0].scatter(data[:,0], data[:,1], s=0.5, c='g')\n",
    "fig.colorbar(pos1, ax=axs[0])\n",
    "pos2 = axs[1].pcolormesh(XY[0], XY[1], error_2.reshape(N, N), cmap='hot')\n",
    "#axs[1].scatter(data[:,0], data[:,1], s=0.5, c='g')\n",
    "fig.colorbar(pos2, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data[:].to(dev).requires_grad_(True)\n",
    "T1 = net_H1(inputs)\n",
    "T2 = net_H2(inputs)\n",
    "dq1 = torch.autograd.grad(\n",
    "            outputs=T1, inputs=inputs,\n",
    "            grad_outputs=torch.ones_like(T1),\n",
    "            create_graph=True, retain_graph=True\n",
    "    )[0].detach()\n",
    "dq2 = torch.autograd.grad(\n",
    "            outputs=T2, inputs=inputs,\n",
    "            grad_outputs=torch.ones_like(T2),\n",
    "            create_graph=True, retain_graph=True\n",
    "    )[0].detach()\n",
    "triang = Triangulation(data[:,0], data[:,1])\n",
    "n_elem = len(triang.triangles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bound(dq1, dq2, tri, data):\n",
    "    # Compute the A_h\n",
    "    A_h = np.zeros((2,2))\n",
    "    n_elem = len(tri.triangles)\n",
    "    for i in range(n_elem):\n",
    "        elem = tri.triangles[i]\n",
    "        node_coords = data[elem]\n",
    "        vector = node_coords[1:]-node_coords[:-1]\n",
    "        vectors = torch.hstack((vector, torch.tensor([[0], [0]], device=dev)))\n",
    "        area = (1/2) * abs(torch.cross(vectors[0], vectors[1])[-1])\n",
    "        A_loc = A(node_coords)\n",
    "        Q1 = A_loc @ (dq1[elem] + H1).view(3,2,1)\n",
    "        Q2 = A_loc @ (dq2[elem] + H2).view(3,2,1)\n",
    "        Q1_mean = Q1.mean(dim=0)\n",
    "        Q2_mean = Q2.mean(dim=0)\n",
    "        A_h_elem_1 = (area * Q1_mean).squeeze().detach().cpu().numpy()\n",
    "        A_h_elem_2 = (area * Q2_mean).squeeze().detach().cpu().numpy()\n",
    "        A_h[:,0] += A_h_elem_1\n",
    "        A_h[:,1] += A_h_elem_2\n",
    "    return A_h / L**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.08792234e+00 3.45842555e-05]\n",
      " [1.00970743e-06 1.07226192e+00]]\n"
     ]
    }
   ],
   "source": [
    "A_bound = compute_bound(dq1, dq2, triang, inputs)\n",
    "print(A_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'bounds/linear/A_u_NN_{total_params}.npy', A_bound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
