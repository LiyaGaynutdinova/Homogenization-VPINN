{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dataset import *\n",
    "from save_load import *\n",
    "from NN_library.PINN.PINN import *\n",
    "from NN_library.PINN.train_PINN import *\n",
    "from matplotlib.tri import Triangulation\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_Sobol(14, [0, 2], [0, 2])\n",
    "loaders = get_loaders_Sobol(data, 2**14+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13e244169d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = 2\n",
    "N = 512 # number of nodes in each direction including the border\n",
    "H1 = torch.tensor([1, 0], device=dev).view(1, 2) # macrogradient\n",
    "H2 = torch.tensor([0, 1], device=dev).view(1, 2) # macrogradient\n",
    "x = np.linspace(0, L, N, endpoint=True)\n",
    "y = np.linspace(0, L, N, endpoint=True)\n",
    "\n",
    "XY = np.meshgrid(x, y)\n",
    "grid_data = torch.tensor(np.vstack((XY[0].flatten(), XY[1].flatten())).T, dtype=torch.float, device=dev)\n",
    "def a_function(x,y):\n",
    "    a = 1 + 0.6*np.abs(y-1) - 0.4*np.abs(x-1)\n",
    "    return a\n",
    "def A(x):\n",
    "    a = (1 + 0.6*torch.abs(x[:,1]-1) - 0.4*torch.abs(x[:,0]-1)).view(-1,1,1)\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = a * I\n",
    "    return A\n",
    "Z = a_function(XY[0].flatten(),XY[1].flatten())\n",
    "plt.pcolormesh(XY[0], XY[1], Z.reshape(N, N))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:,0], data[:,1], s = 0.5, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 391\n"
     ]
    }
   ],
   "source": [
    "net_H1 = PINN(n_periodic=5, n_hidden=10, n_layers=2, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_H1.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "args = {'lr' : 0.0001, 'epochs' : 10000, 'dev' : dev, 'name' : f'NN_library/PINN/PINN_H1_{total_params}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_H1 = load_network(net_H1, args['name']+'', args)\n",
    "net_H1 = net_H1.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 mean train loss:  1.12335984e+00, mean val. rec. loss:  1.10603389e+00\n",
      "Epoch: 1 mean train loss:  1.10586157e+00, mean val. rec. loss:  1.08877400e+00\n",
      "Epoch: 2 mean train loss:  1.08861691e+00, mean val. rec. loss:  1.07176943e+00\n",
      "Epoch: 3 mean train loss:  1.07162777e+00, mean val. rec. loss:  1.05502221e+00\n",
      "Epoch: 4 mean train loss:  1.05489607e+00, mean val. rec. loss:  1.03853336e+00\n",
      "Epoch: 5 mean train loss:  1.03842274e+00, mean val. rec. loss:  1.02230403e+00\n",
      "Epoch: 6 mean train loss:  1.02220875e+00, mean val. rec. loss:  1.00633503e+00\n",
      "Epoch: 7 mean train loss:  1.00625505e+00, mean val. rec. loss:  9.90627003e-01\n",
      "Epoch: 8 mean train loss:  9.90562240e-01, mean val. rec. loss:  9.75180101e-01\n",
      "Epoch: 9 mean train loss:  9.75130550e-01, mean val. rec. loss:  9.59994467e-01\n",
      "Epoch: 10 mean train loss:  9.59960282e-01, mean val. rec. loss:  9.45070029e-01\n",
      "Epoch: 11 mean train loss:  9.45050899e-01, mean val. rec. loss:  9.30405552e-01\n",
      "Epoch: 12 mean train loss:  9.30401568e-01, mean val. rec. loss:  9.16001110e-01\n",
      "Epoch: 13 mean train loss:  9.16012109e-01, mean val. rec. loss:  9.01854961e-01\n",
      "Epoch: 14 mean train loss:  9.01880914e-01, mean val. rec. loss:  8.87965943e-01\n",
      "Epoch: 15 mean train loss:  8.88006730e-01, mean val. rec. loss:  8.74332532e-01\n",
      "Epoch: 16 mean train loss:  8.74388069e-01, mean val. rec. loss:  8.60952697e-01\n",
      "Epoch: 17 mean train loss:  8.61023143e-01, mean val. rec. loss:  8.47824695e-01\n",
      "Epoch: 18 mean train loss:  8.47909687e-01, mean val. rec. loss:  8.34946132e-01\n",
      "Epoch: 19 mean train loss:  8.35045675e-01, mean val. rec. loss:  8.22314613e-01\n",
      "Epoch: 20 mean train loss:  8.22428664e-01, mean val. rec. loss:  8.09927089e-01\n",
      "Epoch: 21 mean train loss:  8.10055614e-01, mean val. rec. loss:  7.97781238e-01\n",
      "Epoch: 22 mean train loss:  7.97924143e-01, mean val. rec. loss:  7.85874012e-01\n",
      "Epoch: 23 mean train loss:  7.86031152e-01, mean val. rec. loss:  7.74202218e-01\n",
      "Epoch: 24 mean train loss:  7.74373601e-01, mean val. rec. loss:  7.62762371e-01\n",
      "Epoch: 25 mean train loss:  7.62947915e-01, mean val. rec. loss:  7.51551353e-01\n",
      "Epoch: 26 mean train loss:  7.51750876e-01, mean val. rec. loss:  7.40565895e-01\n",
      "Epoch: 27 mean train loss:  7.40779386e-01, mean val. rec. loss:  7.29801935e-01\n",
      "Epoch: 28 mean train loss:  7.30029273e-01, mean val. rec. loss:  7.19256497e-01\n",
      "Epoch: 29 mean train loss:  7.19497677e-01, mean val. rec. loss:  7.08925299e-01\n",
      "Epoch: 30 mean train loss:  7.09180188e-01, mean val. rec. loss:  6.98804858e-01\n",
      "Epoch: 31 mean train loss:  6.99073350e-01, mean val. rec. loss:  6.88891398e-01\n",
      "Epoch: 32 mean train loss:  6.89173528e-01, mean val. rec. loss:  6.79180785e-01\n",
      "Epoch: 33 mean train loss:  6.79476432e-01, mean val. rec. loss:  6.69669679e-01\n",
      "Epoch: 34 mean train loss:  6.69978724e-01, mean val. rec. loss:  6.60353798e-01\n",
      "Epoch: 35 mean train loss:  6.60676233e-01, mean val. rec. loss:  6.51229586e-01\n",
      "Epoch: 36 mean train loss:  6.51565146e-01, mean val. rec. loss:  6.42292980e-01\n",
      "Epoch: 37 mean train loss:  6.42641767e-01, mean val. rec. loss:  6.33540059e-01\n",
      "Epoch: 38 mean train loss:  6.33902045e-01, mean val. rec. loss:  6.24967196e-01\n",
      "Epoch: 39 mean train loss:  6.25342107e-01, mean val. rec. loss:  6.16570326e-01\n",
      "Epoch: 40 mean train loss:  6.16958197e-01, mean val. rec. loss:  6.08345712e-01\n",
      "Epoch: 41 mean train loss:  6.08746444e-01, mean val. rec. loss:  6.00289616e-01\n",
      "Epoch: 42 mean train loss:  6.00703211e-01, mean val. rec. loss:  5.92398372e-01\n",
      "Epoch: 43 mean train loss:  5.92824745e-01, mean val. rec. loss:  5.84668172e-01\n",
      "Epoch: 44 mean train loss:  5.85107172e-01, mean val. rec. loss:  5.77095386e-01\n",
      "Epoch: 45 mean train loss:  5.77546976e-01, mean val. rec. loss:  5.69676493e-01\n",
      "Epoch: 46 mean train loss:  5.70140642e-01, mean val. rec. loss:  5.62407903e-01\n",
      "Epoch: 47 mean train loss:  5.62884535e-01, mean val. rec. loss:  5.55286167e-01\n",
      "Epoch: 48 mean train loss:  5.55775198e-01, mean val. rec. loss:  5.48307657e-01\n",
      "Epoch: 49 mean train loss:  5.48809116e-01, mean val. rec. loss:  5.41469361e-01\n",
      "Epoch: 50 mean train loss:  5.41983011e-01, mean val. rec. loss:  5.34767614e-01\n",
      "Epoch: 51 mean train loss:  5.35293486e-01, mean val. rec. loss:  5.28199403e-01\n",
      "Epoch: 52 mean train loss:  5.28737443e-01, mean val. rec. loss:  5.21761319e-01\n",
      "Epoch: 53 mean train loss:  5.22311366e-01, mean val. rec. loss:  5.15450495e-01\n",
      "Epoch: 54 mean train loss:  5.16012574e-01, mean val. rec. loss:  5.09263700e-01\n",
      "Epoch: 55 mean train loss:  5.09837729e-01, mean val. rec. loss:  5.03198104e-01\n",
      "Epoch: 56 mean train loss:  5.03784090e-01, mean val. rec. loss:  4.97250733e-01\n",
      "Epoch: 57 mean train loss:  4.97848500e-01, mean val. rec. loss:  4.91418718e-01\n",
      "Epoch: 58 mean train loss:  4.92028275e-01, mean val. rec. loss:  4.85699267e-01\n",
      "Epoch: 59 mean train loss:  4.86320557e-01, mean val. rec. loss:  4.80089766e-01\n",
      "Epoch: 60 mean train loss:  4.80722604e-01, mean val. rec. loss:  4.74587276e-01\n",
      "Epoch: 61 mean train loss:  4.75231794e-01, mean val. rec. loss:  4.69189584e-01\n",
      "Epoch: 62 mean train loss:  4.69845683e-01, mean val. rec. loss:  4.63893822e-01\n",
      "Epoch: 63 mean train loss:  4.64561353e-01, mean val. rec. loss:  4.58697778e-01\n",
      "Epoch: 64 mean train loss:  4.59376716e-01, mean val. rec. loss:  4.53598839e-01\n",
      "Epoch: 65 mean train loss:  4.54289212e-01, mean val. rec. loss:  4.48594827e-01\n",
      "Epoch: 66 mean train loss:  4.49296515e-01, mean val. rec. loss:  4.43683493e-01\n",
      "Epoch: 67 mean train loss:  4.44396332e-01, mean val. rec. loss:  4.38862296e-01\n",
      "Epoch: 68 mean train loss:  4.39586428e-01, mean val. rec. loss:  4.34129459e-01\n",
      "Epoch: 69 mean train loss:  4.34864717e-01, mean val. rec. loss:  4.29482658e-01\n",
      "Epoch: 70 mean train loss:  4.30228995e-01, mean val. rec. loss:  4.24919863e-01\n",
      "Epoch: 71 mean train loss:  4.25677265e-01, mean val. rec. loss:  4.20439113e-01\n",
      "Epoch: 72 mean train loss:  4.21207502e-01, mean val. rec. loss:  4.16038413e-01\n",
      "Epoch: 73 mean train loss:  4.16817737e-01, mean val. rec. loss:  4.11715840e-01\n",
      "Epoch: 74 mean train loss:  4.12506007e-01, mean val. rec. loss:  4.07469542e-01\n",
      "Epoch: 75 mean train loss:  4.08270641e-01, mean val. rec. loss:  4.03297923e-01\n",
      "Epoch: 76 mean train loss:  4.04109762e-01, mean val. rec. loss:  3.99199024e-01\n",
      "Epoch: 77 mean train loss:  4.00021584e-01, mean val. rec. loss:  3.95171248e-01\n",
      "Epoch: 78 mean train loss:  3.96004466e-01, mean val. rec. loss:  3.91212853e-01\n",
      "Epoch: 79 mean train loss:  3.92056712e-01, mean val. rec. loss:  3.87322316e-01\n",
      "Epoch: 80 mean train loss:  3.88176741e-01, mean val. rec. loss:  3.83497894e-01\n",
      "Epoch: 81 mean train loss:  3.84362856e-01, mean val. rec. loss:  3.79738244e-01\n",
      "Epoch: 82 mean train loss:  3.80613685e-01, mean val. rec. loss:  3.76041880e-01\n",
      "Epoch: 83 mean train loss:  3.76927740e-01, mean val. rec. loss:  3.72407276e-01\n",
      "Epoch: 84 mean train loss:  3.73303500e-01, mean val. rec. loss:  3.68833054e-01\n",
      "Epoch: 85 mean train loss:  3.69739625e-01, mean val. rec. loss:  3.65317836e-01\n",
      "Epoch: 86 mean train loss:  3.66234654e-01, mean val. rec. loss:  3.61860205e-01\n",
      "Epoch: 87 mean train loss:  3.62787248e-01, mean val. rec. loss:  3.58459037e-01\n",
      "Epoch: 88 mean train loss:  3.59396244e-01, mean val. rec. loss:  3.55112990e-01\n",
      "Epoch: 89 mean train loss:  3.56060331e-01, mean val. rec. loss:  3.51820793e-01\n",
      "Epoch: 90 mean train loss:  3.52778257e-01, mean val. rec. loss:  3.48581357e-01\n",
      "Epoch: 91 mean train loss:  3.49548831e-01, mean val. rec. loss:  3.45393377e-01\n",
      "Epoch: 92 mean train loss:  3.46370832e-01, mean val. rec. loss:  3.42255800e-01\n",
      "Epoch: 93 mean train loss:  3.43243186e-01, mean val. rec. loss:  3.39167501e-01\n",
      "Epoch: 94 mean train loss:  3.40164792e-01, mean val. rec. loss:  3.36127500e-01\n",
      "Epoch: 95 mean train loss:  3.37134605e-01, mean val. rec. loss:  3.33134528e-01\n",
      "Epoch: 96 mean train loss:  3.34151436e-01, mean val. rec. loss:  3.30187785e-01\n",
      "Epoch: 97 mean train loss:  3.31214389e-01, mean val. rec. loss:  3.27286111e-01\n",
      "Epoch: 98 mean train loss:  3.28322482e-01, mean val. rec. loss:  3.24428744e-01\n",
      "Epoch: 99 mean train loss:  3.25474730e-01, mean val. rec. loss:  3.21614559e-01\n",
      "Epoch: 100 mean train loss:  3.22670152e-01, mean val. rec. loss:  3.18842757e-01\n",
      "Epoch: 101 mean train loss:  3.19907913e-01, mean val. rec. loss:  3.16112360e-01\n",
      "Epoch: 102 mean train loss:  3.17186999e-01, mean val. rec. loss:  3.13422567e-01\n",
      "Epoch: 103 mean train loss:  3.14506696e-01, mean val. rec. loss:  3.10772437e-01\n",
      "Epoch: 104 mean train loss:  3.11865961e-01, mean val. rec. loss:  3.08161206e-01\n",
      "Epoch: 105 mean train loss:  3.09264079e-01, mean val. rec. loss:  3.05588077e-01\n",
      "Epoch: 106 mean train loss:  3.06700275e-01, mean val. rec. loss:  3.03052179e-01\n",
      "Epoch: 107 mean train loss:  3.04173655e-01, mean val. rec. loss:  3.00552804e-01\n",
      "Epoch: 108 mean train loss:  3.01683563e-01, mean val. rec. loss:  2.98089244e-01\n",
      "Epoch: 109 mean train loss:  2.99229166e-01, mean val. rec. loss:  2.95660792e-01\n",
      "Epoch: 110 mean train loss:  2.96809868e-01, mean val. rec. loss:  2.93266596e-01\n",
      "Epoch: 111 mean train loss:  2.94424774e-01, mean val. rec. loss:  2.90906074e-01\n",
      "Epoch: 112 mean train loss:  2.92073349e-01, mean val. rec. loss:  2.88578573e-01\n",
      "Epoch: 113 mean train loss:  2.89754818e-01, mean val. rec. loss:  2.86283405e-01\n",
      "Epoch: 114 mean train loss:  2.87468614e-01, mean val. rec. loss:  2.84019915e-01\n",
      "Epoch: 115 mean train loss:  2.85214052e-01, mean val. rec. loss:  2.81787414e-01\n",
      "Epoch: 116 mean train loss:  2.82990448e-01, mean val. rec. loss:  2.79585504e-01\n",
      "Epoch: 117 mean train loss:  2.80797354e-01, mean val. rec. loss:  2.77413313e-01\n",
      "Epoch: 118 mean train loss:  2.78633965e-01, mean val. rec. loss:  2.75270442e-01\n",
      "Epoch: 119 mean train loss:  2.76499865e-01, mean val. rec. loss:  2.73156310e-01\n",
      "Epoch: 120 mean train loss:  2.74394428e-01, mean val. rec. loss:  2.71070301e-01\n",
      "Epoch: 121 mean train loss:  2.72317087e-01, mean val. rec. loss:  2.69011925e-01\n",
      "Epoch: 122 mean train loss:  2.70267337e-01, mean val. rec. loss:  2.66980619e-01\n",
      "Epoch: 123 mean train loss:  2.68244640e-01, mean val. rec. loss:  2.64975857e-01\n",
      "Epoch: 124 mean train loss:  2.66248432e-01, mean val. rec. loss:  2.62997241e-01\n",
      "Epoch: 125 mean train loss:  2.64278264e-01, mean val. rec. loss:  2.61044134e-01\n",
      "Epoch: 126 mean train loss:  2.62333660e-01, mean val. rec. loss:  2.59116210e-01\n",
      "Epoch: 127 mean train loss:  2.60414114e-01, mean val. rec. loss:  2.57212908e-01\n",
      "Epoch: 128 mean train loss:  2.58519179e-01, mean val. rec. loss:  2.55333790e-01\n",
      "Epoch: 129 mean train loss:  2.56648437e-01, mean val. rec. loss:  2.53478405e-01\n",
      "Epoch: 130 mean train loss:  2.54801353e-01, mean val. rec. loss:  2.51646280e-01\n",
      "Epoch: 131 mean train loss:  2.52977479e-01, mean val. rec. loss:  2.49837107e-01\n",
      "Epoch: 132 mean train loss:  2.51176487e-01, mean val. rec. loss:  2.48050340e-01\n",
      "Epoch: 133 mean train loss:  2.49397902e-01, mean val. rec. loss:  2.46285637e-01\n",
      "Epoch: 134 mean train loss:  2.47641349e-01, mean val. rec. loss:  2.44542651e-01\n",
      "Epoch: 135 mean train loss:  2.45906458e-01, mean val. rec. loss:  2.42820930e-01\n",
      "Epoch: 136 mean train loss:  2.44192811e-01, mean val. rec. loss:  2.41120111e-01\n",
      "Epoch: 137 mean train loss:  2.42500005e-01, mean val. rec. loss:  2.39439793e-01\n",
      "Epoch: 138 mean train loss:  2.40827684e-01, mean val. rec. loss:  2.37779651e-01\n",
      "Epoch: 139 mean train loss:  2.39175474e-01, mean val. rec. loss:  2.36139304e-01\n",
      "Epoch: 140 mean train loss:  2.37543048e-01, mean val. rec. loss:  2.34518480e-01\n",
      "Epoch: 141 mean train loss:  2.35930063e-01, mean val. rec. loss:  2.32916779e-01\n",
      "Epoch: 142 mean train loss:  2.34336207e-01, mean val. rec. loss:  2.31333820e-01\n",
      "Epoch: 143 mean train loss:  2.32761047e-01, mean val. rec. loss:  2.29769386e-01\n",
      "Epoch: 144 mean train loss:  2.31204360e-01, mean val. rec. loss:  2.28223150e-01\n",
      "Epoch: 145 mean train loss:  2.29665833e-01, mean val. rec. loss:  2.26694767e-01\n",
      "Epoch: 146 mean train loss:  2.28145124e-01, mean val. rec. loss:  2.25183929e-01\n",
      "Epoch: 147 mean train loss:  2.26641934e-01, mean val. rec. loss:  2.23690328e-01\n",
      "Epoch: 148 mean train loss:  2.25155935e-01, mean val. rec. loss:  2.22213763e-01\n",
      "Epoch: 149 mean train loss:  2.23686920e-01, mean val. rec. loss:  2.20753855e-01\n",
      "Epoch: 150 mean train loss:  2.22234576e-01, mean val. rec. loss:  2.19310384e-01\n",
      "Epoch: 151 mean train loss:  2.20798588e-01, mean val. rec. loss:  2.17883043e-01\n",
      "Epoch: 152 mean train loss:  2.19378705e-01, mean val. rec. loss:  2.16471596e-01\n",
      "Epoch: 153 mean train loss:  2.17974688e-01, mean val. rec. loss:  2.15075734e-01\n",
      "Epoch: 154 mean train loss:  2.16586224e-01, mean val. rec. loss:  2.13695295e-01\n",
      "Epoch: 155 mean train loss:  2.15213134e-01, mean val. rec. loss:  2.12329933e-01\n",
      "Epoch: 156 mean train loss:  2.13855091e-01, mean val. rec. loss:  2.10979466e-01\n",
      "Epoch: 157 mean train loss:  2.12511900e-01, mean val. rec. loss:  2.09643660e-01\n",
      "Epoch: 158 mean train loss:  2.11183354e-01, mean val. rec. loss:  2.08322242e-01\n",
      "Epoch: 159 mean train loss:  2.09869154e-01, mean val. rec. loss:  2.07015011e-01\n",
      "Epoch: 160 mean train loss:  2.08569092e-01, mean val. rec. loss:  2.05721716e-01\n",
      "Epoch: 161 mean train loss:  2.07282959e-01, mean val. rec. loss:  2.04442191e-01\n",
      "Epoch: 162 mean train loss:  2.06010561e-01, mean val. rec. loss:  2.03176219e-01\n",
      "Epoch: 163 mean train loss:  2.04751631e-01, mean val. rec. loss:  2.01923529e-01\n",
      "Epoch: 164 mean train loss:  2.03505989e-01, mean val. rec. loss:  2.00683920e-01\n",
      "Epoch: 165 mean train loss:  2.02273398e-01, mean val. rec. loss:  1.99457266e-01\n",
      "Epoch: 166 mean train loss:  2.01053738e-01, mean val. rec. loss:  1.98243312e-01\n",
      "Epoch: 167 mean train loss:  1.99846696e-01, mean val. rec. loss:  1.97041860e-01\n",
      "Epoch: 168 mean train loss:  1.98652152e-01, mean val. rec. loss:  1.95852781e-01\n",
      "Epoch: 169 mean train loss:  1.97469945e-01, mean val. rec. loss:  1.94675822e-01\n",
      "Epoch: 170 mean train loss:  1.96299833e-01, mean val. rec. loss:  1.93510838e-01\n",
      "Epoch: 171 mean train loss:  1.95141655e-01, mean val. rec. loss:  1.92357647e-01\n",
      "Epoch: 172 mean train loss:  1.93995246e-01, mean val. rec. loss:  1.91216032e-01\n",
      "Epoch: 173 mean train loss:  1.92860382e-01, mean val. rec. loss:  1.90085866e-01\n",
      "Epoch: 174 mean train loss:  1.91736945e-01, mean val. rec. loss:  1.88967003e-01\n",
      "Epoch: 175 mean train loss:  1.90624741e-01, mean val. rec. loss:  1.87859207e-01\n",
      "Epoch: 176 mean train loss:  1.89523620e-01, mean val. rec. loss:  1.86762389e-01\n",
      "Epoch: 177 mean train loss:  1.88433389e-01, mean val. rec. loss:  1.85676348e-01\n",
      "Epoch: 178 mean train loss:  1.87353945e-01, mean val. rec. loss:  1.84600939e-01\n",
      "Epoch: 179 mean train loss:  1.86285077e-01, mean val. rec. loss:  1.83536017e-01\n",
      "Epoch: 180 mean train loss:  1.85226668e-01, mean val. rec. loss:  1.82481364e-01\n",
      "Epoch: 181 mean train loss:  1.84178523e-01, mean val. rec. loss:  1.81436945e-01\n",
      "Epoch: 182 mean train loss:  1.83140538e-01, mean val. rec. loss:  1.80402540e-01\n",
      "Epoch: 183 mean train loss:  1.82112579e-01, mean val. rec. loss:  1.79378024e-01\n",
      "Epoch: 184 mean train loss:  1.81094422e-01, mean val. rec. loss:  1.78363233e-01\n",
      "Epoch: 185 mean train loss:  1.80086024e-01, mean val. rec. loss:  1.77358076e-01\n",
      "Epoch: 186 mean train loss:  1.79087189e-01, mean val. rec. loss:  1.76362390e-01\n",
      "Epoch: 187 mean train loss:  1.78097845e-01, mean val. rec. loss:  1.75376103e-01\n",
      "Epoch: 188 mean train loss:  1.77117796e-01, mean val. rec. loss:  1.74398996e-01\n",
      "Epoch: 189 mean train loss:  1.76146925e-01, mean val. rec. loss:  1.73430961e-01\n",
      "Epoch: 190 mean train loss:  1.75185126e-01, mean val. rec. loss:  1.72471943e-01\n",
      "Epoch: 191 mean train loss:  1.74232265e-01, mean val. rec. loss:  1.71521744e-01\n",
      "Epoch: 192 mean train loss:  1.73288209e-01, mean val. rec. loss:  1.70580289e-01\n",
      "Epoch: 193 mean train loss:  1.72352868e-01, mean val. rec. loss:  1.69647435e-01\n",
      "Epoch: 194 mean train loss:  1.71426108e-01, mean val. rec. loss:  1.68723053e-01\n",
      "Epoch: 195 mean train loss:  1.70507795e-01, mean val. rec. loss:  1.67807090e-01\n",
      "Epoch: 196 mean train loss:  1.69597839e-01, mean val. rec. loss:  1.66899347e-01\n",
      "Epoch: 197 mean train loss:  1.68696122e-01, mean val. rec. loss:  1.65999767e-01\n",
      "Epoch: 198 mean train loss:  1.67802509e-01, mean val. rec. loss:  1.65108262e-01\n",
      "Epoch: 199 mean train loss:  1.66916895e-01, mean val. rec. loss:  1.64224650e-01\n",
      "Epoch: 200 mean train loss:  1.66039223e-01, mean val. rec. loss:  1.63348893e-01\n",
      "Epoch: 201 mean train loss:  1.65169356e-01, mean val. rec. loss:  1.62480884e-01\n",
      "Epoch: 202 mean train loss:  1.64307177e-01, mean val. rec. loss:  1.61620513e-01\n",
      "Epoch: 203 mean train loss:  1.63452625e-01, mean val. rec. loss:  1.60767636e-01\n",
      "Epoch: 204 mean train loss:  1.62605552e-01, mean val. rec. loss:  1.59922197e-01\n",
      "Epoch: 205 mean train loss:  1.61765898e-01, mean val. rec. loss:  1.59084107e-01\n",
      "Epoch: 206 mean train loss:  1.60933499e-01, mean val. rec. loss:  1.58253274e-01\n",
      "Epoch: 207 mean train loss:  1.60108384e-01, mean val. rec. loss:  1.57429572e-01\n",
      "Epoch: 208 mean train loss:  1.59290361e-01, mean val. rec. loss:  1.56612927e-01\n",
      "Epoch: 209 mean train loss:  1.58479355e-01, mean val. rec. loss:  1.55803268e-01\n",
      "Epoch: 210 mean train loss:  1.57675306e-01, mean val. rec. loss:  1.55000430e-01\n",
      "Epoch: 211 mean train loss:  1.56878065e-01, mean val. rec. loss:  1.54204397e-01\n",
      "Epoch: 212 mean train loss:  1.56087587e-01, mean val. rec. loss:  1.53415095e-01\n",
      "Epoch: 213 mean train loss:  1.55303814e-01, mean val. rec. loss:  1.52632379e-01\n",
      "Epoch: 214 mean train loss:  1.54526610e-01, mean val. rec. loss:  1.51856249e-01\n",
      "Epoch: 215 mean train loss:  1.53755916e-01, mean val. rec. loss:  1.51086515e-01\n",
      "Epoch: 216 mean train loss:  1.52991643e-01, mean val. rec. loss:  1.50323203e-01\n",
      "Epoch: 217 mean train loss:  1.52233747e-01, mean val. rec. loss:  1.49566179e-01\n",
      "Epoch: 218 mean train loss:  1.51482092e-01, mean val. rec. loss:  1.48815323e-01\n",
      "Epoch: 219 mean train loss:  1.50736605e-01, mean val. rec. loss:  1.48070636e-01\n",
      "Epoch: 220 mean train loss:  1.49997241e-01, mean val. rec. loss:  1.47332000e-01\n",
      "Epoch: 221 mean train loss:  1.49263910e-01, mean val. rec. loss:  1.46599333e-01\n",
      "Epoch: 222 mean train loss:  1.48536524e-01, mean val. rec. loss:  1.45872608e-01\n",
      "Epoch: 223 mean train loss:  1.47815037e-01, mean val. rec. loss:  1.45151708e-01\n",
      "Epoch: 224 mean train loss:  1.47099345e-01, mean val. rec. loss:  1.44436558e-01\n",
      "Epoch: 225 mean train loss:  1.46389388e-01, mean val. rec. loss:  1.43727125e-01\n",
      "Epoch: 226 mean train loss:  1.45685123e-01, mean val. rec. loss:  1.43023297e-01\n",
      "Epoch: 227 mean train loss:  1.44986399e-01, mean val. rec. loss:  1.42325040e-01\n",
      "Epoch: 228 mean train loss:  1.44293262e-01, mean val. rec. loss:  1.41632262e-01\n",
      "Epoch: 229 mean train loss:  1.43605548e-01, mean val. rec. loss:  1.40944927e-01\n",
      "Epoch: 230 mean train loss:  1.42923271e-01, mean val. rec. loss:  1.40262926e-01\n",
      "Epoch: 231 mean train loss:  1.42246298e-01, mean val. rec. loss:  1.39586250e-01\n",
      "Epoch: 232 mean train loss:  1.41574598e-01, mean val. rec. loss:  1.38914773e-01\n",
      "Epoch: 233 mean train loss:  1.40908068e-01, mean val. rec. loss:  1.38248493e-01\n",
      "Epoch: 234 mean train loss:  1.40246722e-01, mean val. rec. loss:  1.37587303e-01\n",
      "Epoch: 235 mean train loss:  1.39590456e-01, mean val. rec. loss:  1.36931157e-01\n",
      "Epoch: 236 mean train loss:  1.38939181e-01, mean val. rec. loss:  1.36280001e-01\n",
      "Epoch: 237 mean train loss:  1.38292882e-01, mean val. rec. loss:  1.35633761e-01\n",
      "Epoch: 238 mean train loss:  1.37651454e-01, mean val. rec. loss:  1.34992393e-01\n",
      "Epoch: 239 mean train loss:  1.37014883e-01, mean val. rec. loss:  1.34355851e-01\n",
      "Epoch: 240 mean train loss:  1.36383109e-01, mean val. rec. loss:  1.33724018e-01\n",
      "Epoch: 241 mean train loss:  1.35756012e-01, mean val. rec. loss:  1.33096892e-01\n",
      "Epoch: 242 mean train loss:  1.35133609e-01, mean val. rec. loss:  1.32474430e-01\n",
      "Epoch: 243 mean train loss:  1.34515823e-01, mean val. rec. loss:  1.31856549e-01\n",
      "Epoch: 244 mean train loss:  1.33902597e-01, mean val. rec. loss:  1.31243185e-01\n",
      "Epoch: 245 mean train loss:  1.33293869e-01, mean val. rec. loss:  1.30634321e-01\n",
      "Epoch: 246 mean train loss:  1.32689595e-01, mean val. rec. loss:  1.30029848e-01\n",
      "Epoch: 247 mean train loss:  1.32089702e-01, mean val. rec. loss:  1.29429784e-01\n",
      "Epoch: 248 mean train loss:  1.31494188e-01, mean val. rec. loss:  1.28834047e-01\n",
      "Epoch: 249 mean train loss:  1.30902965e-01, mean val. rec. loss:  1.28242556e-01\n",
      "Epoch: 250 mean train loss:  1.30315973e-01, mean val. rec. loss:  1.27655301e-01\n",
      "Epoch: 251 mean train loss:  1.29733182e-01, mean val. rec. loss:  1.27072220e-01\n",
      "Epoch: 252 mean train loss:  1.29154547e-01, mean val. rec. loss:  1.26493293e-01\n",
      "Epoch: 253 mean train loss:  1.28580024e-01, mean val. rec. loss:  1.25918430e-01\n",
      "Epoch: 254 mean train loss:  1.28009508e-01, mean val. rec. loss:  1.25347577e-01\n",
      "Epoch: 255 mean train loss:  1.27443030e-01, mean val. rec. loss:  1.24780734e-01\n",
      "Epoch: 256 mean train loss:  1.26880514e-01, mean val. rec. loss:  1.24217828e-01\n",
      "Epoch: 257 mean train loss:  1.26321887e-01, mean val. rec. loss:  1.23658787e-01\n",
      "Epoch: 258 mean train loss:  1.25767133e-01, mean val. rec. loss:  1.23103628e-01\n",
      "Epoch: 259 mean train loss:  1.25216223e-01, mean val. rec. loss:  1.22552280e-01\n",
      "Epoch: 260 mean train loss:  1.24669081e-01, mean val. rec. loss:  1.22004669e-01\n",
      "Epoch: 261 mean train loss:  1.24125664e-01, mean val. rec. loss:  1.21460777e-01\n",
      "Epoch: 262 mean train loss:  1.23585942e-01, mean val. rec. loss:  1.20920587e-01\n",
      "Epoch: 263 mean train loss:  1.23049884e-01, mean val. rec. loss:  1.20384035e-01\n",
      "Epoch: 264 mean train loss:  1.22517432e-01, mean val. rec. loss:  1.19851057e-01\n",
      "Epoch: 265 mean train loss:  1.21988547e-01, mean val. rec. loss:  1.19321644e-01\n",
      "Epoch: 266 mean train loss:  1.21463186e-01, mean val. rec. loss:  1.18795733e-01\n",
      "Epoch: 267 mean train loss:  1.20941311e-01, mean val. rec. loss:  1.18273323e-01\n",
      "Epoch: 268 mean train loss:  1.20422885e-01, mean val. rec. loss:  1.17754343e-01\n",
      "Epoch: 269 mean train loss:  1.19907877e-01, mean val. rec. loss:  1.17238747e-01\n",
      "Epoch: 270 mean train loss:  1.19396237e-01, mean val. rec. loss:  1.16726516e-01\n",
      "Epoch: 271 mean train loss:  1.18887918e-01, mean val. rec. loss:  1.16217623e-01\n",
      "Epoch: 272 mean train loss:  1.18382907e-01, mean val. rec. loss:  1.15711970e-01\n",
      "Epoch: 273 mean train loss:  1.17881129e-01, mean val. rec. loss:  1.15209609e-01\n",
      "Epoch: 274 mean train loss:  1.17382591e-01, mean val. rec. loss:  1.14710432e-01\n",
      "Epoch: 275 mean train loss:  1.16887226e-01, mean val. rec. loss:  1.14214440e-01\n",
      "Epoch: 276 mean train loss:  1.16395026e-01, mean val. rec. loss:  1.13721578e-01\n",
      "Epoch: 277 mean train loss:  1.15905903e-01, mean val. rec. loss:  1.13231845e-01\n",
      "Epoch: 278 mean train loss:  1.15419894e-01, mean val. rec. loss:  1.12745160e-01\n",
      "Epoch: 279 mean train loss:  1.14936916e-01, mean val. rec. loss:  1.12261506e-01\n",
      "Epoch: 280 mean train loss:  1.14456948e-01, mean val. rec. loss:  1.11780881e-01\n",
      "Epoch: 281 mean train loss:  1.13979966e-01, mean val. rec. loss:  1.11303214e-01\n",
      "Epoch: 282 mean train loss:  1.13505912e-01, mean val. rec. loss:  1.10828486e-01\n",
      "Epoch: 283 mean train loss:  1.13034778e-01, mean val. rec. loss:  1.10356688e-01\n",
      "Epoch: 284 mean train loss:  1.12566526e-01, mean val. rec. loss:  1.09887748e-01\n",
      "Epoch: 285 mean train loss:  1.12101135e-01, mean val. rec. loss:  1.09421647e-01\n",
      "Epoch: 286 mean train loss:  1.11638545e-01, mean val. rec. loss:  1.08958350e-01\n",
      "Epoch: 287 mean train loss:  1.11178740e-01, mean val. rec. loss:  1.08497865e-01\n",
      "Epoch: 288 mean train loss:  1.10721691e-01, mean val. rec. loss:  1.08040119e-01\n",
      "Epoch: 289 mean train loss:  1.10267377e-01, mean val. rec. loss:  1.07585086e-01\n",
      "Epoch: 290 mean train loss:  1.09815758e-01, mean val. rec. loss:  1.07132747e-01\n",
      "Epoch: 291 mean train loss:  1.09366791e-01, mean val. rec. loss:  1.06683076e-01\n",
      "Epoch: 292 mean train loss:  1.08920469e-01, mean val. rec. loss:  1.06236053e-01\n",
      "Epoch: 293 mean train loss:  1.08476769e-01, mean val. rec. loss:  1.05791625e-01\n",
      "Epoch: 294 mean train loss:  1.08035631e-01, mean val. rec. loss:  1.05349746e-01\n",
      "Epoch: 295 mean train loss:  1.07597040e-01, mean val. rec. loss:  1.04910471e-01\n",
      "Epoch: 296 mean train loss:  1.07160982e-01, mean val. rec. loss:  1.04473681e-01\n",
      "Epoch: 297 mean train loss:  1.06727412e-01, mean val. rec. loss:  1.04039377e-01\n",
      "Epoch: 298 mean train loss:  1.06296307e-01, mean val. rec. loss:  1.03607577e-01\n",
      "Epoch: 299 mean train loss:  1.05867661e-01, mean val. rec. loss:  1.03178226e-01\n",
      "Epoch: 300 mean train loss:  1.05441435e-01, mean val. rec. loss:  1.02751270e-01\n",
      "Epoch: 301 mean train loss:  1.05017593e-01, mean val. rec. loss:  1.02326728e-01\n",
      "Epoch: 302 mean train loss:  1.04596128e-01, mean val. rec. loss:  1.01904562e-01\n",
      "Epoch: 303 mean train loss:  1.04177008e-01, mean val. rec. loss:  1.01484727e-01\n",
      "Epoch: 304 mean train loss:  1.03760213e-01, mean val. rec. loss:  1.01067215e-01\n",
      "Epoch: 305 mean train loss:  1.03345689e-01, mean val. rec. loss:  1.00652017e-01\n",
      "Epoch: 306 mean train loss:  1.02933460e-01, mean val. rec. loss:  1.00239068e-01\n",
      "Epoch: 307 mean train loss:  1.02523472e-01, mean val. rec. loss:  9.98284139e-02\n",
      "Epoch: 308 mean train loss:  1.02115720e-01, mean val. rec. loss:  9.94199555e-02\n",
      "Epoch: 309 mean train loss:  1.01710157e-01, mean val. rec. loss:  9.90137016e-02\n",
      "Epoch: 310 mean train loss:  1.01306777e-01, mean val. rec. loss:  9.86096431e-02\n",
      "Epoch: 311 mean train loss:  1.00905542e-01, mean val. rec. loss:  9.82077255e-02\n",
      "Epoch: 312 mean train loss:  1.00506437e-01, mean val. rec. loss:  9.78079670e-02\n",
      "Epoch: 313 mean train loss:  1.00109470e-01, mean val. rec. loss:  9.74103132e-02\n",
      "Epoch: 314 mean train loss:  9.97145664e-02, mean val. rec. loss:  9.70147549e-02\n",
      "Epoch: 315 mean train loss:  9.93217411e-02, mean val. rec. loss:  9.66212832e-02\n",
      "Epoch: 316 mean train loss:  9.89309715e-02, mean val. rec. loss:  9.62298526e-02\n",
      "Epoch: 317 mean train loss:  9.85422281e-02, mean val. rec. loss:  9.58404450e-02\n",
      "Epoch: 318 mean train loss:  9.81554809e-02, mean val. rec. loss:  9.54530877e-02\n",
      "Epoch: 319 mean train loss:  9.77707375e-02, mean val. rec. loss:  9.50676989e-02\n",
      "Epoch: 320 mean train loss:  9.73879456e-02, mean val. rec. loss:  9.46842969e-02\n",
      "Epoch: 321 mean train loss:  9.70071202e-02, mean val. rec. loss:  9.43028453e-02\n",
      "Epoch: 322 mean train loss:  9.66282017e-02, mean val. rec. loss:  9.39233261e-02\n",
      "Epoch: 323 mean train loss:  9.62512198e-02, mean val. rec. loss:  9.35457391e-02\n",
      "Epoch: 324 mean train loss:  9.58761150e-02, mean val. rec. loss:  9.31700390e-02\n",
      "Epoch: 325 mean train loss:  9.55028799e-02, mean val. rec. loss:  9.27962350e-02\n",
      "Epoch: 326 mean train loss:  9.51315070e-02, mean val. rec. loss:  9.24243179e-02\n",
      "Epoch: 327 mean train loss:  9.47620037e-02, mean val. rec. loss:  9.20542060e-02\n",
      "Epoch: 328 mean train loss:  9.43942881e-02, mean val. rec. loss:  9.16859449e-02\n",
      "Epoch: 329 mean train loss:  9.40283900e-02, mean val. rec. loss:  9.13194799e-02\n",
      "Epoch: 330 mean train loss:  9.36642573e-02, mean val. rec. loss:  9.09548203e-02\n",
      "Epoch: 331 mean train loss:  9.33019123e-02, mean val. rec. loss:  9.05919568e-02\n",
      "Epoch: 332 mean train loss:  9.29413177e-02, mean val. rec. loss:  9.02308443e-02\n",
      "Epoch: 333 mean train loss:  9.25824512e-02, mean val. rec. loss:  8.98714553e-02\n",
      "Epoch: 334 mean train loss:  9.22253129e-02, mean val. rec. loss:  8.95138354e-02\n",
      "Epoch: 335 mean train loss:  9.18698878e-02, mean val. rec. loss:  8.91579210e-02\n",
      "Epoch: 336 mean train loss:  9.15161461e-02, mean val. rec. loss:  8.88037211e-02\n",
      "Epoch: 337 mean train loss:  9.11640952e-02, mean val. rec. loss:  8.84511905e-02\n",
      "Epoch: 338 mean train loss:  9.08136831e-02, mean val. rec. loss:  8.81003290e-02\n",
      "Epoch: 339 mean train loss:  9.04649395e-02, mean val. rec. loss:  8.77511640e-02\n",
      "Epoch: 340 mean train loss:  9.01178346e-02, mean val. rec. loss:  8.74035957e-02\n",
      "Epoch: 341 mean train loss:  8.97723164e-02, mean val. rec. loss:  8.70576784e-02\n",
      "Epoch: 342 mean train loss:  8.94284070e-02, mean val. rec. loss:  8.67133668e-02\n",
      "Epoch: 343 mean train loss:  8.90860992e-02, mean val. rec. loss:  8.63706609e-02\n",
      "Epoch: 344 mean train loss:  8.87453555e-02, mean val. rec. loss:  8.60295245e-02\n",
      "Epoch: 345 mean train loss:  8.84061762e-02, mean val. rec. loss:  8.56900028e-02\n",
      "Epoch: 346 mean train loss:  8.80685536e-02, mean val. rec. loss:  8.53520052e-02\n",
      "Epoch: 347 mean train loss:  8.77324580e-02, mean val. rec. loss:  8.50155680e-02\n",
      "Epoch: 348 mean train loss:  8.73978745e-02, mean val. rec. loss:  8.46806820e-02\n",
      "Epoch: 349 mean train loss:  8.70648403e-02, mean val. rec. loss:  8.43473019e-02\n",
      "Epoch: 350 mean train loss:  8.67332736e-02, mean val. rec. loss:  8.40154641e-02\n",
      "Epoch: 351 mean train loss:  8.64032115e-02, mean val. rec. loss:  8.36851140e-02\n",
      "Epoch: 352 mean train loss:  8.60746243e-02, mean val. rec. loss:  8.33562608e-02\n",
      "Epoch: 353 mean train loss:  8.57474970e-02, mean val. rec. loss:  8.30288591e-02\n",
      "Epoch: 354 mean train loss:  8.54218073e-02, mean val. rec. loss:  8.27029270e-02\n",
      "Epoch: 355 mean train loss:  8.50975776e-02, mean val. rec. loss:  8.23784736e-02\n",
      "Epoch: 356 mean train loss:  8.47747855e-02, mean val. rec. loss:  8.20554536e-02\n",
      "Epoch: 357 mean train loss:  8.44533938e-02, mean val. rec. loss:  8.17338669e-02\n",
      "Epoch: 358 mean train loss:  8.41334247e-02, mean val. rec. loss:  8.14137136e-02\n",
      "Epoch: 359 mean train loss:  8.38148412e-02, mean val. rec. loss:  8.10949573e-02\n",
      "Epoch: 360 mean train loss:  8.34976580e-02, mean val. rec. loss:  8.07775981e-02\n",
      "Epoch: 361 mean train loss:  8.31818305e-02, mean val. rec. loss:  8.04616631e-02\n",
      "Epoch: 362 mean train loss:  8.28673959e-02, mean val. rec. loss:  8.01470889e-02\n",
      "Epoch: 363 mean train loss:  8.25543095e-02, mean val. rec. loss:  7.98338937e-02\n",
      "Epoch: 364 mean train loss:  8.22425714e-02, mean val. rec. loss:  7.95220592e-02\n",
      "Epoch: 365 mean train loss:  8.19321666e-02, mean val. rec. loss:  7.92115945e-02\n",
      "Epoch: 366 mean train loss:  8.16231025e-02, mean val. rec. loss:  7.89024544e-02\n",
      "Epoch: 367 mean train loss:  8.13153495e-02, mean val. rec. loss:  7.85946840e-02\n",
      "Epoch: 368 mean train loss:  8.10089372e-02, mean val. rec. loss:  7.82882019e-02\n",
      "Epoch: 369 mean train loss:  8.07037838e-02, mean val. rec. loss:  7.79830715e-02\n",
      "Epoch: 370 mean train loss:  8.03999637e-02, mean val. rec. loss:  7.76792474e-02\n",
      "Epoch: 371 mean train loss:  8.00974247e-02, mean val. rec. loss:  7.73767025e-02\n",
      "Epoch: 372 mean train loss:  7.97961521e-02, mean val. rec. loss:  7.70754820e-02\n",
      "Epoch: 373 mean train loss:  7.94961532e-02, mean val. rec. loss:  7.67755316e-02\n",
      "Epoch: 374 mean train loss:  7.91974206e-02, mean val. rec. loss:  7.64768784e-02\n",
      "Epoch: 375 mean train loss:  7.88999469e-02, mean val. rec. loss:  7.61794772e-02\n",
      "Epoch: 376 mean train loss:  7.86037021e-02, mean val. rec. loss:  7.58833596e-02\n",
      "Epoch: 377 mean train loss:  7.83087163e-02, mean val. rec. loss:  7.55884803e-02\n",
      "Epoch: 378 mean train loss:  7.80149594e-02, mean val. rec. loss:  7.52948575e-02\n",
      "Epoch: 379 mean train loss:  7.77224391e-02, mean val. rec. loss:  7.50025002e-02\n",
      "Epoch: 380 mean train loss:  7.74311180e-02, mean val. rec. loss:  7.47113585e-02\n",
      "Epoch: 381 mean train loss:  7.71410185e-02, mean val. rec. loss:  7.44214234e-02\n",
      "Epoch: 382 mean train loss:  7.68521108e-02, mean val. rec. loss:  7.41327583e-02\n",
      "Epoch: 383 mean train loss:  7.65644322e-02, mean val. rec. loss:  7.38452816e-02\n",
      "Epoch: 384 mean train loss:  7.62779156e-02, mean val. rec. loss:  7.35590251e-02\n",
      "Epoch: 385 mean train loss:  7.59926131e-02, mean val. rec. loss:  7.32739706e-02\n",
      "Epoch: 386 mean train loss:  7.57084726e-02, mean val. rec. loss:  7.29901090e-02\n",
      "Epoch: 387 mean train loss:  7.54255090e-02, mean val. rec. loss:  7.27074314e-02\n",
      "Epoch: 388 mean train loss:  7.51437075e-02, mean val. rec. loss:  7.24259602e-02\n",
      "Epoch: 389 mean train loss:  7.48630828e-02, mean val. rec. loss:  7.21456548e-02\n",
      "Epoch: 390 mean train loss:  7.45836053e-02, mean val. rec. loss:  7.18665469e-02\n",
      "Epoch: 391 mean train loss:  7.43052823e-02, mean val. rec. loss:  7.15886047e-02\n",
      "Epoch: 392 mean train loss:  7.40281213e-02, mean val. rec. loss:  7.13117965e-02\n",
      "Epoch: 393 mean train loss:  7.37520702e-02, mean val. rec. loss:  7.10361766e-02\n",
      "Epoch: 394 mean train loss:  7.34771810e-02, mean val. rec. loss:  7.07616772e-02\n",
      "Epoch: 395 mean train loss:  7.32034018e-02, mean val. rec. loss:  7.04883571e-02\n",
      "Epoch: 396 mean train loss:  7.29307622e-02, mean val. rec. loss:  7.02161664e-02\n",
      "Epoch: 397 mean train loss:  7.26592250e-02, mean val. rec. loss:  6.99451415e-02\n",
      "Epoch: 398 mean train loss:  7.23888424e-02, mean val. rec. loss:  6.96752233e-02\n",
      "Epoch: 399 mean train loss:  7.21195250e-02, mean val. rec. loss:  6.94064527e-02\n",
      "Epoch: 400 mean train loss:  7.18513546e-02, mean val. rec. loss:  6.91387843e-02\n",
      "Epoch: 401 mean train loss:  7.15842569e-02, mean val. rec. loss:  6.88722590e-02\n",
      "Epoch: 402 mean train loss:  7.13182765e-02, mean val. rec. loss:  6.86068450e-02\n",
      "Epoch: 403 mean train loss:  7.10533762e-02, mean val. rec. loss:  6.83425286e-02\n",
      "Epoch: 404 mean train loss:  7.07895783e-02, mean val. rec. loss:  6.80793554e-02\n",
      "Epoch: 405 mean train loss:  7.05268605e-02, mean val. rec. loss:  6.78172525e-02\n",
      "Epoch: 406 mean train loss:  7.02652302e-02, mean val. rec. loss:  6.75562746e-02\n",
      "Epoch: 407 mean train loss:  7.00046874e-02, mean val. rec. loss:  6.72963717e-02\n",
      "Epoch: 408 mean train loss:  6.97452023e-02, mean val. rec. loss:  6.70375892e-02\n",
      "Epoch: 409 mean train loss:  6.94868047e-02, mean val. rec. loss:  6.67798817e-02\n",
      "Epoch: 410 mean train loss:  6.92294649e-02, mean val. rec. loss:  6.65232764e-02\n",
      "Epoch: 411 mean train loss:  6.89731902e-02, mean val. rec. loss:  6.62677280e-02\n",
      "Epoch: 412 mean train loss:  6.87179808e-02, mean val. rec. loss:  6.60132681e-02\n",
      "Epoch: 413 mean train loss:  6.84638290e-02, mean val. rec. loss:  6.57598969e-02\n",
      "Epoch: 414 mean train loss:  6.82107275e-02, mean val. rec. loss:  6.55076098e-02\n",
      "Epoch: 415 mean train loss:  6.79586912e-02, mean val. rec. loss:  6.52563614e-02\n",
      "Epoch: 416 mean train loss:  6.77076903e-02, mean val. rec. loss:  6.50062062e-02\n",
      "Epoch: 417 mean train loss:  6.74577321e-02, mean val. rec. loss:  6.47570942e-02\n",
      "Epoch: 418 mean train loss:  6.72088243e-02, mean val. rec. loss:  6.45090618e-02\n",
      "Epoch: 419 mean train loss:  6.69609593e-02, mean val. rec. loss:  6.42620772e-02\n",
      "Epoch: 420 mean train loss:  6.67141297e-02, mean val. rec. loss:  6.40161539e-02\n",
      "Epoch: 421 mean train loss:  6.64683280e-02, mean val. rec. loss:  6.37712785e-02\n",
      "Epoch: 422 mean train loss:  6.62235617e-02, mean val. rec. loss:  6.35274644e-02\n",
      "Epoch: 423 mean train loss:  6.59798307e-02, mean val. rec. loss:  6.32846846e-02\n",
      "Epoch: 424 mean train loss:  6.57371203e-02, mean val. rec. loss:  6.30429661e-02\n",
      "Epoch: 425 mean train loss:  6.54954452e-02, mean val. rec. loss:  6.28022864e-02\n",
      "Epoch: 426 mean train loss:  6.52547906e-02, mean val. rec. loss:  6.25626499e-02\n",
      "Epoch: 427 mean train loss:  6.50151564e-02, mean val. rec. loss:  6.23240431e-02\n",
      "Epoch: 428 mean train loss:  6.47765354e-02, mean val. rec. loss:  6.20864886e-02\n",
      "Epoch: 429 mean train loss:  6.45389347e-02, mean val. rec. loss:  6.18499547e-02\n",
      "Epoch: 430 mean train loss:  6.43023472e-02, mean val. rec. loss:  6.16144595e-02\n",
      "Epoch: 431 mean train loss:  6.40667726e-02, mean val. rec. loss:  6.13800031e-02\n",
      "Epoch: 432 mean train loss:  6.38322111e-02, mean val. rec. loss:  6.11465763e-02\n",
      "Epoch: 433 mean train loss:  6.35986701e-02, mean val. rec. loss:  6.09141746e-02\n",
      "Epoch: 434 mean train loss:  6.33661123e-02, mean val. rec. loss:  6.06827798e-02\n",
      "Epoch: 435 mean train loss:  6.31345675e-02, mean val. rec. loss:  6.04524284e-02\n",
      "Epoch: 436 mean train loss:  6.29040358e-02, mean val. rec. loss:  6.02230793e-02\n",
      "Epoch: 437 mean train loss:  6.26744947e-02, mean val. rec. loss:  5.99947735e-02\n",
      "Epoch: 438 mean train loss:  6.24459592e-02, mean val. rec. loss:  5.97674657e-02\n",
      "Epoch: 439 mean train loss:  6.22184144e-02, mean val. rec. loss:  5.95411829e-02\n",
      "Epoch: 440 mean train loss:  6.19918678e-02, mean val. rec. loss:  5.93159207e-02\n",
      "Epoch: 441 mean train loss:  6.17663230e-02, mean val. rec. loss:  5.90916564e-02\n",
      "Epoch: 442 mean train loss:  6.15417652e-02, mean val. rec. loss:  5.88683991e-02\n",
      "Epoch: 443 mean train loss:  6.13181980e-02, mean val. rec. loss:  5.86461669e-02\n",
      "Epoch: 444 mean train loss:  6.10956253e-02, mean val. rec. loss:  5.84249417e-02\n",
      "Epoch: 445 mean train loss:  6.08740321e-02, mean val. rec. loss:  5.82047325e-02\n",
      "Epoch: 446 mean train loss:  6.06534519e-02, mean val. rec. loss:  5.79855031e-02\n",
      "Epoch: 447 mean train loss:  6.04338363e-02, mean val. rec. loss:  5.77672897e-02\n",
      "Epoch: 448 mean train loss:  6.02152002e-02, mean val. rec. loss:  5.75500652e-02\n",
      "Epoch: 449 mean train loss:  5.99975511e-02, mean val. rec. loss:  5.73338748e-02\n",
      "Epoch: 450 mean train loss:  5.97808965e-02, mean val. rec. loss:  5.71186596e-02\n",
      "Epoch: 451 mean train loss:  5.95652213e-02, mean val. rec. loss:  5.69044379e-02\n",
      "Epoch: 452 mean train loss:  5.93505107e-02, mean val. rec. loss:  5.66912276e-02\n",
      "Epoch: 453 mean train loss:  5.91367945e-02, mean val. rec. loss:  5.64790061e-02\n",
      "Epoch: 454 mean train loss:  5.89240505e-02, mean val. rec. loss:  5.62677780e-02\n",
      "Epoch: 455 mean train loss:  5.87122747e-02, mean val. rec. loss:  5.60575705e-02\n",
      "Epoch: 456 mean train loss:  5.85014971e-02, mean val. rec. loss:  5.58483110e-02\n",
      "Epoch: 457 mean train loss:  5.82916692e-02, mean val. rec. loss:  5.56400585e-02\n",
      "Epoch: 458 mean train loss:  5.80828171e-02, mean val. rec. loss:  5.54328038e-02\n",
      "Epoch: 459 mean train loss:  5.78749482e-02, mean val. rec. loss:  5.52265244e-02\n",
      "Epoch: 460 mean train loss:  5.76680402e-02, mean val. rec. loss:  5.50212520e-02\n",
      "Epoch: 461 mean train loss:  5.74621192e-02, mean val. rec. loss:  5.48169548e-02\n",
      "Epoch: 462 mean train loss:  5.72571479e-02, mean val. rec. loss:  5.46136418e-02\n",
      "Epoch: 463 mean train loss:  5.70531523e-02, mean val. rec. loss:  5.44113177e-02\n",
      "Epoch: 464 mean train loss:  5.68501289e-02, mean val. rec. loss:  5.42099870e-02\n",
      "Epoch: 465 mean train loss:  5.66480738e-02, mean val. rec. loss:  5.40096088e-02\n",
      "Epoch: 466 mean train loss:  5.64469684e-02, mean val. rec. loss:  5.38102330e-02\n",
      "Epoch: 467 mean train loss:  5.62468387e-02, mean val. rec. loss:  5.36118416e-02\n",
      "Epoch: 468 mean train loss:  5.60476663e-02, mean val. rec. loss:  5.34144162e-02\n",
      "Epoch: 469 mean train loss:  5.58494622e-02, mean val. rec. loss:  5.32179843e-02\n",
      "Epoch: 470 mean train loss:  5.56522152e-02, mean val. rec. loss:  5.30225139e-02\n",
      "Epoch: 471 mean train loss:  5.54559329e-02, mean val. rec. loss:  5.28280279e-02\n",
      "Epoch: 472 mean train loss:  5.52606078e-02, mean val. rec. loss:  5.26345170e-02\n",
      "Epoch: 473 mean train loss:  5.50662398e-02, mean val. rec. loss:  5.24419814e-02\n",
      "Epoch: 474 mean train loss:  5.48728326e-02, mean val. rec. loss:  5.22504301e-02\n",
      "Epoch: 475 mean train loss:  5.46803864e-02, mean val. rec. loss:  5.20598313e-02\n",
      "Epoch: 476 mean train loss:  5.44888937e-02, mean val. rec. loss:  5.18701987e-02\n",
      "Epoch: 477 mean train loss:  5.42983469e-02, mean val. rec. loss:  5.16815548e-02\n",
      "Epoch: 478 mean train loss:  5.41087647e-02, mean val. rec. loss:  5.14938726e-02\n",
      "Epoch: 479 mean train loss:  5.39201323e-02, mean val. rec. loss:  5.13071521e-02\n",
      "Epoch: 480 mean train loss:  5.37324495e-02, mean val. rec. loss:  5.11214112e-02\n",
      "Epoch: 481 mean train loss:  5.35457165e-02, mean val. rec. loss:  5.09366184e-02\n",
      "Epoch: 482 mean train loss:  5.33599295e-02, mean val. rec. loss:  5.07528099e-02\n",
      "Epoch: 483 mean train loss:  5.31751108e-02, mean val. rec. loss:  5.05699403e-02\n",
      "Epoch: 484 mean train loss:  5.29912158e-02, mean val. rec. loss:  5.03880640e-02\n",
      "Epoch: 485 mean train loss:  5.28082853e-02, mean val. rec. loss:  5.02071131e-02\n",
      "Epoch: 486 mean train loss:  5.26262860e-02, mean val. rec. loss:  5.00271556e-02\n",
      "Epoch: 487 mean train loss:  5.24452476e-02, mean val. rec. loss:  4.98481324e-02\n",
      "Epoch: 488 mean train loss:  5.22651402e-02, mean val. rec. loss:  4.96700754e-02\n",
      "Epoch: 489 mean train loss:  5.20859826e-02, mean val. rec. loss:  4.94929664e-02\n",
      "Epoch: 490 mean train loss:  5.19077561e-02, mean val. rec. loss:  4.93168326e-02\n",
      "Epoch: 491 mean train loss:  5.17304904e-02, mean val. rec. loss:  4.91416423e-02\n",
      "Epoch: 492 mean train loss:  5.15541559e-02, mean val. rec. loss:  4.89674091e-02\n",
      "Epoch: 493 mean train loss:  5.13787599e-02, mean val. rec. loss:  4.87941238e-02\n",
      "Epoch: 494 mean train loss:  5.12043024e-02, mean val. rec. loss:  4.86217957e-02\n",
      "Epoch: 495 mean train loss:  5.10307835e-02, mean val. rec. loss:  4.84504201e-02\n",
      "Epoch: 496 mean train loss:  5.08582069e-02, mean val. rec. loss:  4.82799789e-02\n",
      "Epoch: 497 mean train loss:  5.06865464e-02, mean val. rec. loss:  4.81104856e-02\n",
      "Epoch: 498 mean train loss:  5.05158171e-02, mean val. rec. loss:  4.79419495e-02\n",
      "Epoch: 499 mean train loss:  5.03460449e-02, mean val. rec. loss:  4.77743478e-02\n",
      "Epoch: 500 mean train loss:  5.01771889e-02, mean val. rec. loss:  4.76076986e-02\n",
      "Epoch: 501 mean train loss:  5.00092640e-02, mean val. rec. loss:  4.74419883e-02\n",
      "Epoch: 502 mean train loss:  4.98422740e-02, mean val. rec. loss:  4.72772215e-02\n",
      "Epoch: 503 mean train loss:  4.96762038e-02, mean val. rec. loss:  4.71133891e-02\n",
      "Epoch: 504 mean train loss:  4.95110573e-02, mean val. rec. loss:  4.69504866e-02\n",
      "Epoch: 505 mean train loss:  4.93468419e-02, mean val. rec. loss:  4.67885320e-02\n",
      "Epoch: 506 mean train loss:  4.91835352e-02, mean val. rec. loss:  4.66274983e-02\n",
      "Epoch: 507 mean train loss:  4.90211671e-02, mean val. rec. loss:  4.64674171e-02\n",
      "Epoch: 508 mean train loss:  4.88597115e-02, mean val. rec. loss:  4.63082612e-02\n",
      "Epoch: 509 mean train loss:  4.86991795e-02, mean val. rec. loss:  4.61500352e-02\n",
      "Epoch: 510 mean train loss:  4.85395674e-02, mean val. rec. loss:  4.59927481e-02\n",
      "Epoch: 511 mean train loss:  4.83808715e-02, mean val. rec. loss:  4.58363682e-02\n",
      "Epoch: 512 mean train loss:  4.82230807e-02, mean val. rec. loss:  4.56809226e-02\n",
      "Epoch: 513 mean train loss:  4.80662135e-02, mean val. rec. loss:  4.55264025e-02\n",
      "Epoch: 514 mean train loss:  4.79102513e-02, mean val. rec. loss:  4.53728076e-02\n",
      "Epoch: 515 mean train loss:  4.77552128e-02, mean val. rec. loss:  4.52201199e-02\n",
      "Epoch: 516 mean train loss:  4.76010681e-02, mean val. rec. loss:  4.50683666e-02\n",
      "Epoch: 517 mean train loss:  4.74478321e-02, mean val. rec. loss:  4.49175160e-02\n",
      "Epoch: 518 mean train loss:  4.72954975e-02, mean val. rec. loss:  4.47675907e-02\n",
      "Epoch: 519 mean train loss:  4.71440828e-02, mean val. rec. loss:  4.46185771e-02\n",
      "Epoch: 520 mean train loss:  4.69935508e-02, mean val. rec. loss:  4.44704707e-02\n",
      "Epoch: 521 mean train loss:  4.68439312e-02, mean val. rec. loss:  4.43232714e-02\n",
      "Epoch: 522 mean train loss:  4.66952092e-02, mean val. rec. loss:  4.41769975e-02\n",
      "Epoch: 523 mean train loss:  4.65473885e-02, mean val. rec. loss:  4.40316217e-02\n",
      "Epoch: 524 mean train loss:  4.64004505e-02, mean val. rec. loss:  4.38871531e-02\n",
      "Epoch: 525 mean train loss:  4.62544250e-02, mean val. rec. loss:  4.37435735e-02\n",
      "Epoch: 526 mean train loss:  4.61092747e-02, mean val. rec. loss:  4.36009102e-02\n",
      "Epoch: 527 mean train loss:  4.59650220e-02, mean val. rec. loss:  4.34591450e-02\n",
      "Epoch: 528 mean train loss:  4.58216631e-02, mean val. rec. loss:  4.33182552e-02\n",
      "Epoch: 529 mean train loss:  4.56791757e-02, mean val. rec. loss:  4.31782771e-02\n",
      "Epoch: 530 mean train loss:  4.55375710e-02, mean val. rec. loss:  4.30391790e-02\n",
      "Epoch: 531 mean train loss:  4.53968528e-02, mean val. rec. loss:  4.29009836e-02\n",
      "Epoch: 532 mean train loss:  4.52570097e-02, mean val. rec. loss:  4.27636635e-02\n",
      "Epoch: 533 mean train loss:  4.51180530e-02, mean val. rec. loss:  4.26272461e-02\n",
      "Epoch: 534 mean train loss:  4.49799716e-02, mean val. rec. loss:  4.24916815e-02\n",
      "Epoch: 535 mean train loss:  4.48427505e-02, mean val. rec. loss:  4.23570331e-02\n",
      "Epoch: 536 mean train loss:  4.47064121e-02, mean val. rec. loss:  4.22232330e-02\n",
      "Epoch: 537 mean train loss:  4.45709377e-02, mean val. rec. loss:  4.20903445e-02\n",
      "Epoch: 538 mean train loss:  4.44363423e-02, mean val. rec. loss:  4.19583043e-02\n",
      "Epoch: 539 mean train loss:  4.43026073e-02, mean val. rec. loss:  4.18271485e-02\n",
      "Epoch: 540 mean train loss:  4.41697325e-02, mean val. rec. loss:  4.16968456e-02\n",
      "Epoch: 541 mean train loss:  4.40377069e-02, mean val. rec. loss:  4.15674180e-02\n",
      "Epoch: 542 mean train loss:  4.39065417e-02, mean val. rec. loss:  4.14388341e-02\n",
      "Epoch: 543 mean train loss:  4.37762331e-02, mean val. rec. loss:  4.13111302e-02\n",
      "Epoch: 544 mean train loss:  4.36467773e-02, mean val. rec. loss:  4.11842791e-02\n",
      "Epoch: 545 mean train loss:  4.35181632e-02, mean val. rec. loss:  4.10583033e-02\n",
      "Epoch: 546 mean train loss:  4.33904132e-02, mean val. rec. loss:  4.09331713e-02\n",
      "Epoch: 547 mean train loss:  4.32635012e-02, mean val. rec. loss:  4.08088784e-02\n",
      "Epoch: 548 mean train loss:  4.31374272e-02, mean val. rec. loss:  4.06854382e-02\n",
      "Epoch: 549 mean train loss:  4.30121912e-02, mean val. rec. loss:  4.05628418e-02\n",
      "Epoch: 550 mean train loss:  4.28878006e-02, mean val. rec. loss:  4.04410980e-02\n",
      "Epoch: 551 mean train loss:  4.27642442e-02, mean val. rec. loss:  4.03201844e-02\n",
      "Epoch: 552 mean train loss:  4.26415184e-02, mean val. rec. loss:  4.02001189e-02\n",
      "Epoch: 553 mean train loss:  4.25196269e-02, mean val. rec. loss:  4.00808836e-02\n",
      "Epoch: 554 mean train loss:  4.23985547e-02, mean val. rec. loss:  3.99624783e-02\n",
      "Epoch: 555 mean train loss:  4.22783093e-02, mean val. rec. loss:  3.98449030e-02\n",
      "Epoch: 556 mean train loss:  4.21588833e-02, mean val. rec. loss:  3.97281443e-02\n",
      "Epoch: 557 mean train loss:  4.20402730e-02, mean val. rec. loss:  3.96122246e-02\n",
      "Epoch: 558 mean train loss:  4.19224894e-02, mean val. rec. loss:  3.94971169e-02\n",
      "Epoch: 559 mean train loss:  4.18055141e-02, mean val. rec. loss:  3.93828212e-02\n",
      "Epoch: 560 mean train loss:  4.16893432e-02, mean val. rec. loss:  3.92693509e-02\n",
      "Epoch: 561 mean train loss:  4.15739842e-02, mean val. rec. loss:  3.91566790e-02\n",
      "Epoch: 562 mean train loss:  4.14594222e-02, mean val. rec. loss:  3.90448281e-02\n",
      "Epoch: 563 mean train loss:  4.13456647e-02, mean val. rec. loss:  3.89337664e-02\n",
      "Epoch: 564 mean train loss:  4.12326968e-02, mean val. rec. loss:  3.88235122e-02\n",
      "Epoch: 565 mean train loss:  4.11205408e-02, mean val. rec. loss:  3.87140517e-02\n",
      "Epoch: 566 mean train loss:  4.10091558e-02, mean val. rec. loss:  3.86053850e-02\n",
      "Epoch: 567 mean train loss:  4.08985677e-02, mean val. rec. loss:  3.84975166e-02\n",
      "Epoch: 568 mean train loss:  4.07887730e-02, mean val. rec. loss:  3.83904374e-02\n",
      "Epoch: 569 mean train loss:  4.06797491e-02, mean val. rec. loss:  3.82841385e-02\n",
      "Epoch: 570 mean train loss:  4.05715037e-02, mean val. rec. loss:  3.81786242e-02\n",
      "Epoch: 571 mean train loss:  4.04640367e-02, mean val. rec. loss:  3.80738811e-02\n",
      "Epoch: 572 mean train loss:  4.03573369e-02, mean val. rec. loss:  3.79699113e-02\n",
      "Epoch: 573 mean train loss:  4.02514118e-02, mean val. rec. loss:  3.78667217e-02\n",
      "Epoch: 574 mean train loss:  4.01462576e-02, mean val. rec. loss:  3.77643009e-02\n",
      "Epoch: 575 mean train loss:  4.00418632e-02, mean val. rec. loss:  3.76626513e-02\n",
      "Epoch: 576 mean train loss:  3.99382323e-02, mean val. rec. loss:  3.75617614e-02\n",
      "Epoch: 577 mean train loss:  3.98353612e-02, mean val. rec. loss:  3.74616290e-02\n",
      "Epoch: 578 mean train loss:  3.97332387e-02, mean val. rec. loss:  3.73622427e-02\n",
      "Epoch: 579 mean train loss:  3.96318610e-02, mean val. rec. loss:  3.72636049e-02\n",
      "Epoch: 580 mean train loss:  3.95312208e-02, mean val. rec. loss:  3.71657200e-02\n",
      "Epoch: 581 mean train loss:  3.94313329e-02, mean val. rec. loss:  3.70685836e-02\n",
      "Epoch: 582 mean train loss:  3.93321899e-02, mean val. rec. loss:  3.69721797e-02\n",
      "Epoch: 583 mean train loss:  3.92337731e-02, mean val. rec. loss:  3.68765152e-02\n",
      "Epoch: 584 mean train loss:  3.91360826e-02, mean val. rec. loss:  3.67815855e-02\n",
      "Epoch: 585 mean train loss:  3.90391295e-02, mean val. rec. loss:  3.66873724e-02\n",
      "Epoch: 586 mean train loss:  3.89428915e-02, mean val. rec. loss:  3.65939101e-02\n",
      "Epoch: 587 mean train loss:  3.88473946e-02, mean val. rec. loss:  3.65011553e-02\n",
      "Epoch: 588 mean train loss:  3.87526092e-02, mean val. rec. loss:  3.64091172e-02\n",
      "Epoch: 589 mean train loss:  3.86585276e-02, mean val. rec. loss:  3.63177913e-02\n",
      "Epoch: 590 mean train loss:  3.85651536e-02, mean val. rec. loss:  3.62271798e-02\n",
      "Epoch: 591 mean train loss:  3.84724947e-02, mean val. rec. loss:  3.61372872e-02\n",
      "Epoch: 592 mean train loss:  3.83805509e-02, mean val. rec. loss:  3.60480818e-02\n",
      "Epoch: 593 mean train loss:  3.82892813e-02, mean val. rec. loss:  3.59595704e-02\n",
      "Epoch: 594 mean train loss:  3.81987118e-02, mean val. rec. loss:  3.58717802e-02\n",
      "Epoch: 595 mean train loss:  3.81088573e-02, mean val. rec. loss:  3.57846613e-02\n",
      "Epoch: 596 mean train loss:  3.80196733e-02, mean val. rec. loss:  3.56982523e-02\n",
      "Epoch: 597 mean train loss:  3.79311857e-02, mean val. rec. loss:  3.56125032e-02\n",
      "Epoch: 598 mean train loss:  3.78433686e-02, mean val. rec. loss:  3.55274595e-02\n",
      "Epoch: 599 mean train loss:  3.77562404e-02, mean val. rec. loss:  3.54430599e-02\n",
      "Epoch: 600 mean train loss:  3.76697714e-02, mean val. rec. loss:  3.53593588e-02\n",
      "Epoch: 601 mean train loss:  3.75839729e-02, mean val. rec. loss:  3.52763200e-02\n",
      "Epoch: 602 mean train loss:  3.74988484e-02, mean val. rec. loss:  3.51939411e-02\n",
      "Epoch: 603 mean train loss:  3.74143869e-02, mean val. rec. loss:  3.51122336e-02\n",
      "Epoch: 604 mean train loss:  3.73305883e-02, mean val. rec. loss:  3.50311724e-02\n",
      "Epoch: 605 mean train loss:  3.72474378e-02, mean val. rec. loss:  3.49507734e-02\n",
      "Epoch: 606 mean train loss:  3.71649390e-02, mean val. rec. loss:  3.48710186e-02\n",
      "Epoch: 607 mean train loss:  3.70830809e-02, mean val. rec. loss:  3.47919056e-02\n",
      "Epoch: 608 mean train loss:  3.70018596e-02, mean val. rec. loss:  3.47134389e-02\n",
      "Epoch: 609 mean train loss:  3.69212863e-02, mean val. rec. loss:  3.46356096e-02\n",
      "Epoch: 610 mean train loss:  3.68413574e-02, mean val. rec. loss:  3.45584085e-02\n",
      "Epoch: 611 mean train loss:  3.67620541e-02, mean val. rec. loss:  3.44818514e-02\n",
      "Epoch: 612 mean train loss:  3.66833803e-02, mean val. rec. loss:  3.44059204e-02\n",
      "Epoch: 613 mean train loss:  3.66053247e-02, mean val. rec. loss:  3.43305880e-02\n",
      "Epoch: 614 mean train loss:  3.65278874e-02, mean val. rec. loss:  3.42558749e-02\n",
      "Epoch: 615 mean train loss:  3.64510609e-02, mean val. rec. loss:  3.41818103e-02\n",
      "Epoch: 616 mean train loss:  3.63748637e-02, mean val. rec. loss:  3.41083332e-02\n",
      "Epoch: 617 mean train loss:  3.62992700e-02, mean val. rec. loss:  3.40354570e-02\n",
      "Epoch: 618 mean train loss:  3.62242833e-02, mean val. rec. loss:  3.39632023e-02\n",
      "Epoch: 619 mean train loss:  3.61498888e-02, mean val. rec. loss:  3.38915259e-02\n",
      "Epoch: 620 mean train loss:  3.60760939e-02, mean val. rec. loss:  3.38204505e-02\n",
      "Epoch: 621 mean train loss:  3.60028912e-02, mean val. rec. loss:  3.37499671e-02\n",
      "Epoch: 622 mean train loss:  3.59302732e-02, mean val. rec. loss:  3.36800665e-02\n",
      "Epoch: 623 mean train loss:  3.58582549e-02, mean val. rec. loss:  3.36107465e-02\n",
      "Epoch: 624 mean train loss:  3.57868101e-02, mean val. rec. loss:  3.35420116e-02\n",
      "Epoch: 625 mean train loss:  3.57159425e-02, mean val. rec. loss:  3.34738460e-02\n",
      "Epoch: 626 mean train loss:  3.56456299e-02, mean val. rec. loss:  3.34062452e-02\n",
      "Epoch: 627 mean train loss:  3.55759058e-02, mean val. rec. loss:  3.33392090e-02\n",
      "Epoch: 628 mean train loss:  3.55067440e-02, mean val. rec. loss:  3.32727489e-02\n",
      "Epoch: 629 mean train loss:  3.54381410e-02, mean val. rec. loss:  3.32068399e-02\n",
      "Epoch: 630 mean train loss:  3.53700965e-02, mean val. rec. loss:  3.31414752e-02\n",
      "Epoch: 631 mean train loss:  3.53026033e-02, mean val. rec. loss:  3.30766639e-02\n",
      "Epoch: 632 mean train loss:  3.52356613e-02, mean val. rec. loss:  3.30124128e-02\n",
      "Epoch: 633 mean train loss:  3.51692704e-02, mean val. rec. loss:  3.29486924e-02\n",
      "Epoch: 634 mean train loss:  3.51034159e-02, mean val. rec. loss:  3.28855027e-02\n",
      "Epoch: 635 mean train loss:  3.50380903e-02, mean val. rec. loss:  3.28228618e-02\n",
      "Epoch: 636 mean train loss:  3.49733196e-02, mean val. rec. loss:  3.27607471e-02\n",
      "Epoch: 637 mean train loss:  3.49090628e-02, mean val. rec. loss:  3.26991586e-02\n",
      "Epoch: 638 mean train loss:  3.48453461e-02, mean val. rec. loss:  3.26380917e-02\n",
      "Epoch: 639 mean train loss:  3.47821434e-02, mean val. rec. loss:  3.25775396e-02\n",
      "Epoch: 640 mean train loss:  3.47194620e-02, mean val. rec. loss:  3.25175091e-02\n",
      "Epoch: 641 mean train loss:  3.46572872e-02, mean val. rec. loss:  3.24579822e-02\n",
      "Epoch: 642 mean train loss:  3.45956301e-02, mean val. rec. loss:  3.23989655e-02\n",
      "Epoch: 643 mean train loss:  3.45344832e-02, mean val. rec. loss:  3.23404614e-02\n",
      "Epoch: 644 mean train loss:  3.44738391e-02, mean val. rec. loss:  3.22824494e-02\n",
      "Epoch: 645 mean train loss:  3.44136941e-02, mean val. rec. loss:  3.22249341e-02\n",
      "Epoch: 646 mean train loss:  3.43540444e-02, mean val. rec. loss:  3.21679042e-02\n",
      "Epoch: 647 mean train loss:  3.42948826e-02, mean val. rec. loss:  3.21113505e-02\n",
      "Epoch: 648 mean train loss:  3.42362013e-02, mean val. rec. loss:  3.20552889e-02\n",
      "Epoch: 649 mean train loss:  3.41780153e-02, mean val. rec. loss:  3.19997014e-02\n",
      "Epoch: 650 mean train loss:  3.41203022e-02, mean val. rec. loss:  3.19446060e-02\n",
      "Epoch: 651 mean train loss:  3.40630771e-02, mean val. rec. loss:  3.18899801e-02\n",
      "Epoch: 652 mean train loss:  3.40063176e-02, mean val. rec. loss:  3.18358123e-02\n",
      "Epoch: 653 mean train loss:  3.39500347e-02, mean val. rec. loss:  3.17821049e-02\n",
      "Epoch: 654 mean train loss:  3.38942062e-02, mean val. rec. loss:  3.17288760e-02\n",
      "Epoch: 655 mean train loss:  3.38388508e-02, mean val. rec. loss:  3.16760917e-02\n",
      "Epoch: 656 mean train loss:  3.37839385e-02, mean val. rec. loss:  3.16237587e-02\n",
      "Epoch: 657 mean train loss:  3.37294918e-02, mean val. rec. loss:  3.15718611e-02\n",
      "Epoch: 658 mean train loss:  3.36754808e-02, mean val. rec. loss:  3.15204330e-02\n",
      "Epoch: 659 mean train loss:  3.36219354e-02, mean val. rec. loss:  3.14694449e-02\n",
      "Epoch: 660 mean train loss:  3.35688294e-02, mean val. rec. loss:  3.14188967e-02\n",
      "Epoch: 661 mean train loss:  3.35161667e-02, mean val. rec. loss:  3.13687727e-02\n",
      "Epoch: 662 mean train loss:  3.34639322e-02, mean val. rec. loss:  3.13190773e-02\n",
      "Epoch: 663 mean train loss:  3.34121298e-02, mean val. rec. loss:  3.12698083e-02\n",
      "Epoch: 664 mean train loss:  3.33607557e-02, mean val. rec. loss:  3.12209611e-02\n",
      "Epoch: 665 mean train loss:  3.33098136e-02, mean val. rec. loss:  3.11725290e-02\n",
      "Epoch: 666 mean train loss:  3.32592812e-02, mean val. rec. loss:  3.11245323e-02\n",
      "Epoch: 667 mean train loss:  3.32091734e-02, mean val. rec. loss:  3.10769325e-02\n",
      "Epoch: 668 mean train loss:  3.31594753e-02, mean val. rec. loss:  3.10297545e-02\n",
      "Epoch: 669 mean train loss:  3.31101942e-02, mean val. rec. loss:  3.09829644e-02\n",
      "Epoch: 670 mean train loss:  3.30613229e-02, mean val. rec. loss:  3.09365825e-02\n",
      "Epoch: 671 mean train loss:  3.30128501e-02, mean val. rec. loss:  3.08906111e-02\n",
      "Epoch: 672 mean train loss:  3.29647870e-02, mean val. rec. loss:  3.08450297e-02\n",
      "Epoch: 673 mean train loss:  3.29171074e-02, mean val. rec. loss:  3.07998294e-02\n",
      "Epoch: 674 mean train loss:  3.28698376e-02, mean val. rec. loss:  3.07550101e-02\n",
      "Epoch: 675 mean train loss:  3.28229439e-02, mean val. rec. loss:  3.07105968e-02\n",
      "Epoch: 676 mean train loss:  3.27764450e-02, mean val. rec. loss:  3.06665600e-02\n",
      "Epoch: 677 mean train loss:  3.27303260e-02, mean val. rec. loss:  3.06228883e-02\n",
      "Epoch: 678 mean train loss:  3.26845869e-02, mean val. rec. loss:  3.05795862e-02\n",
      "Epoch: 679 mean train loss:  3.26392165e-02, mean val. rec. loss:  3.05366698e-02\n",
      "Epoch: 680 mean train loss:  3.25942371e-02, mean val. rec. loss:  3.04941139e-02\n",
      "Epoch: 681 mean train loss:  3.25496079e-02, mean val. rec. loss:  3.04519231e-02\n",
      "Epoch: 682 mean train loss:  3.25053585e-02, mean val. rec. loss:  3.04100817e-02\n",
      "Epoch: 683 mean train loss:  3.24614667e-02, mean val. rec. loss:  3.03686190e-02\n",
      "Epoch: 684 mean train loss:  3.24179398e-02, mean val. rec. loss:  3.03274964e-02\n",
      "Epoch: 685 mean train loss:  3.23747556e-02, mean val. rec. loss:  3.02867232e-02\n",
      "Epoch: 686 mean train loss:  3.23319401e-02, mean val. rec. loss:  3.02463037e-02\n",
      "Epoch: 687 mean train loss:  3.22894747e-02, mean val. rec. loss:  3.02062176e-02\n",
      "Epoch: 688 mean train loss:  3.22473520e-02, mean val. rec. loss:  3.01664831e-02\n",
      "Epoch: 689 mean train loss:  3.22055793e-02, mean val. rec. loss:  3.01270910e-02\n",
      "Epoch: 690 mean train loss:  3.21641568e-02, mean val. rec. loss:  3.00880233e-02\n",
      "Epoch: 691 mean train loss:  3.21230545e-02, mean val. rec. loss:  3.00492889e-02\n",
      "Epoch: 692 mean train loss:  3.20822874e-02, mean val. rec. loss:  3.00108834e-02\n",
      "Epoch: 693 mean train loss:  3.20418555e-02, mean val. rec. loss:  2.99728135e-02\n",
      "Epoch: 694 mean train loss:  3.20017663e-02, mean val. rec. loss:  2.99350498e-02\n",
      "Epoch: 695 mean train loss:  3.19619899e-02, mean val. rec. loss:  2.98976036e-02\n",
      "Epoch: 696 mean train loss:  3.19225338e-02, mean val. rec. loss:  2.98604840e-02\n",
      "Epoch: 697 mean train loss:  3.18833980e-02, mean val. rec. loss:  2.98236797e-02\n",
      "Epoch: 698 mean train loss:  3.18445900e-02, mean val. rec. loss:  2.97871793e-02\n",
      "Epoch: 699 mean train loss:  3.18060873e-02, mean val. rec. loss:  2.97509805e-02\n",
      "Epoch: 700 mean train loss:  3.17678901e-02, mean val. rec. loss:  2.97150901e-02\n",
      "Epoch: 701 mean train loss:  3.17300020e-02, mean val. rec. loss:  2.96794991e-02\n",
      "Epoch: 702 mean train loss:  3.16924192e-02, mean val. rec. loss:  2.96441984e-02\n",
      "Epoch: 703 mean train loss:  3.16551307e-02, mean val. rec. loss:  2.96091994e-02\n",
      "Epoch: 704 mean train loss:  3.16181476e-02, mean val. rec. loss:  2.95744770e-02\n",
      "Epoch: 705 mean train loss:  3.15814513e-02, mean val. rec. loss:  2.95400699e-02\n",
      "Epoch: 706 mean train loss:  3.15450604e-02, mean val. rec. loss:  2.95059259e-02\n",
      "Epoch: 707 mean train loss:  3.15089488e-02, mean val. rec. loss:  2.94720857e-02\n",
      "Epoch: 708 mean train loss:  3.14731351e-02, mean val. rec. loss:  2.94385041e-02\n",
      "Epoch: 709 mean train loss:  3.14375971e-02, mean val. rec. loss:  2.94052151e-02\n",
      "Epoch: 710 mean train loss:  3.14023495e-02, mean val. rec. loss:  2.93721869e-02\n",
      "Epoch: 711 mean train loss:  3.13673739e-02, mean val. rec. loss:  2.93394536e-02\n",
      "Epoch: 712 mean train loss:  3.13326813e-02, mean val. rec. loss:  2.93069674e-02\n",
      "Epoch: 713 mean train loss:  3.12982605e-02, mean val. rec. loss:  2.92747670e-02\n",
      "Epoch: 714 mean train loss:  3.12641080e-02, mean val. rec. loss:  2.92428161e-02\n",
      "Epoch: 715 mean train loss:  3.12302273e-02, mean val. rec. loss:  2.92111396e-02\n",
      "Epoch: 716 mean train loss:  3.11966147e-02, mean val. rec. loss:  2.91797035e-02\n",
      "Epoch: 717 mean train loss:  3.11632592e-02, mean val. rec. loss:  2.91485304e-02\n",
      "Epoch: 718 mean train loss:  3.11301643e-02, mean val. rec. loss:  2.91176205e-02\n",
      "Epoch: 719 mean train loss:  3.10973339e-02, mean val. rec. loss:  2.90869533e-02\n",
      "Epoch: 720 mean train loss:  3.10647530e-02, mean val. rec. loss:  2.90565264e-02\n",
      "Epoch: 721 mean train loss:  3.10324179e-02, mean val. rec. loss:  2.90263513e-02\n",
      "Epoch: 722 mean train loss:  3.10003398e-02, mean val. rec. loss:  2.89964324e-02\n",
      "Epoch: 723 mean train loss:  3.09685187e-02, mean val. rec. loss:  2.89667427e-02\n",
      "Epoch: 724 mean train loss:  3.09369360e-02, mean val. rec. loss:  2.89372865e-02\n",
      "Epoch: 725 mean train loss:  3.09055897e-02, mean val. rec. loss:  2.89080775e-02\n",
      "Epoch: 726 mean train loss:  3.08744930e-02, mean val. rec. loss:  2.88790908e-02\n",
      "Epoch: 727 mean train loss:  3.08436253e-02, mean val. rec. loss:  2.88503422e-02\n",
      "Epoch: 728 mean train loss:  3.08130035e-02, mean val. rec. loss:  2.88218159e-02\n",
      "Epoch: 729 mean train loss:  3.07826032e-02, mean val. rec. loss:  2.87935209e-02\n",
      "Epoch: 730 mean train loss:  3.07524469e-02, mean val. rec. loss:  2.87654459e-02\n",
      "Epoch: 731 mean train loss:  3.07225103e-02, mean val. rec. loss:  2.87376000e-02\n",
      "Epoch: 732 mean train loss:  3.06928065e-02, mean val. rec. loss:  2.87099627e-02\n",
      "Epoch: 733 mean train loss:  3.06633206e-02, mean val. rec. loss:  2.86825409e-02\n",
      "Epoch: 734 mean train loss:  3.06340488e-02, mean val. rec. loss:  2.86553436e-02\n",
      "Epoch: 735 mean train loss:  3.06050080e-02, mean val. rec. loss:  2.86283595e-02\n",
      "Epoch: 736 mean train loss:  3.05761924e-02, mean val. rec. loss:  2.86015818e-02\n",
      "Epoch: 737 mean train loss:  3.05475836e-02, mean val. rec. loss:  2.85750082e-02\n",
      "Epoch: 738 mean train loss:  3.05191796e-02, mean val. rec. loss:  2.85486432e-02\n",
      "Epoch: 739 mean train loss:  3.04909972e-02, mean val. rec. loss:  2.85224824e-02\n",
      "Epoch: 740 mean train loss:  3.04630159e-02, mean val. rec. loss:  2.84965234e-02\n",
      "Epoch: 741 mean train loss:  3.04352470e-02, mean val. rec. loss:  2.84707594e-02\n",
      "Epoch: 742 mean train loss:  3.04076791e-02, mean val. rec. loss:  2.84451860e-02\n",
      "Epoch: 743 mean train loss:  3.03803030e-02, mean val. rec. loss:  2.84198167e-02\n",
      "Epoch: 744 mean train loss:  3.03531355e-02, mean val. rec. loss:  2.83946424e-02\n",
      "Epoch: 745 mean train loss:  3.03261747e-02, mean val. rec. loss:  2.83696609e-02\n",
      "Epoch: 746 mean train loss:  3.02994076e-02, mean val. rec. loss:  2.83448608e-02\n",
      "Epoch: 747 mean train loss:  3.02728267e-02, mean val. rec. loss:  2.83202535e-02\n",
      "Epoch: 748 mean train loss:  3.02464432e-02, mean val. rec. loss:  2.82958209e-02\n",
      "Epoch: 749 mean train loss:  3.02202422e-02, mean val. rec. loss:  2.82715833e-02\n",
      "Epoch: 750 mean train loss:  3.01942292e-02, mean val. rec. loss:  2.82475067e-02\n",
      "Epoch: 751 mean train loss:  3.01684062e-02, mean val. rec. loss:  2.82236251e-02\n",
      "Epoch: 752 mean train loss:  3.01427639e-02, mean val. rec. loss:  2.81999205e-02\n",
      "Epoch: 753 mean train loss:  3.01173114e-02, mean val. rec. loss:  2.81763791e-02\n",
      "Epoch: 754 mean train loss:  3.00920303e-02, mean val. rec. loss:  2.81530237e-02\n",
      "Epoch: 755 mean train loss:  3.00669355e-02, mean val. rec. loss:  2.81298294e-02\n",
      "Epoch: 756 mean train loss:  3.00420082e-02, mean val. rec. loss:  2.81068119e-02\n",
      "Epoch: 757 mean train loss:  3.00172671e-02, mean val. rec. loss:  2.80839623e-02\n",
      "Epoch: 758 mean train loss:  2.99926899e-02, mean val. rec. loss:  2.80612669e-02\n",
      "Epoch: 759 mean train loss:  2.99682822e-02, mean val. rec. loss:  2.80387438e-02\n",
      "Epoch: 760 mean train loss:  2.99440458e-02, mean val. rec. loss:  2.80163773e-02\n",
      "Epoch: 761 mean train loss:  2.99199751e-02, mean val. rec. loss:  2.79941808e-02\n",
      "Epoch: 762 mean train loss:  2.98960776e-02, mean val. rec. loss:  2.79721341e-02\n",
      "Epoch: 763 mean train loss:  2.98723347e-02, mean val. rec. loss:  2.79502392e-02\n",
      "Epoch: 764 mean train loss:  2.98487575e-02, mean val. rec. loss:  2.79285236e-02\n",
      "Epoch: 765 mean train loss:  2.98253460e-02, mean val. rec. loss:  2.79069372e-02\n",
      "Epoch: 766 mean train loss:  2.98020910e-02, mean val. rec. loss:  2.78855028e-02\n",
      "Epoch: 767 mean train loss:  2.97789887e-02, mean val. rec. loss:  2.78642362e-02\n",
      "Epoch: 768 mean train loss:  2.97560484e-02, mean val. rec. loss:  2.78431011e-02\n",
      "Epoch: 769 mean train loss:  2.97332496e-02, mean val. rec. loss:  2.78221203e-02\n",
      "Epoch: 770 mean train loss:  2.97106128e-02, mean val. rec. loss:  2.78012914e-02\n",
      "Epoch: 771 mean train loss:  2.96881324e-02, mean val. rec. loss:  2.77805986e-02\n",
      "Epoch: 772 mean train loss:  2.96657898e-02, mean val. rec. loss:  2.77600464e-02\n",
      "Epoch: 773 mean train loss:  2.96436037e-02, mean val. rec. loss:  2.77396258e-02\n",
      "Epoch: 774 mean train loss:  2.96215442e-02, mean val. rec. loss:  2.77193548e-02\n",
      "Epoch: 775 mean train loss:  2.95996448e-02, mean val. rec. loss:  2.76992176e-02\n",
      "Epoch: 776 mean train loss:  2.95778889e-02, mean val. rec. loss:  2.76792166e-02\n",
      "Epoch: 777 mean train loss:  2.95562670e-02, mean val. rec. loss:  2.76593493e-02\n",
      "Epoch: 778 mean train loss:  2.95347940e-02, mean val. rec. loss:  2.76396204e-02\n",
      "Epoch: 779 mean train loss:  2.95134496e-02, mean val. rec. loss:  2.76200230e-02\n",
      "Epoch: 780 mean train loss:  2.94922523e-02, mean val. rec. loss:  2.76005503e-02\n",
      "Epoch: 781 mean train loss:  2.94711760e-02, mean val. rec. loss:  2.75812047e-02\n",
      "Epoch: 782 mean train loss:  2.94502469e-02, mean val. rec. loss:  2.75619929e-02\n",
      "Epoch: 783 mean train loss:  2.94294425e-02, mean val. rec. loss:  2.75429058e-02\n",
      "Epoch: 784 mean train loss:  2.94087740e-02, mean val. rec. loss:  2.75239253e-02\n",
      "Epoch: 785 mean train loss:  2.93882248e-02, mean val. rec. loss:  2.75050922e-02\n",
      "Epoch: 786 mean train loss:  2.93678170e-02, mean val. rec. loss:  2.74863634e-02\n",
      "Epoch: 787 mean train loss:  2.93475303e-02, mean val. rec. loss:  2.74677594e-02\n",
      "Epoch: 788 mean train loss:  2.93273702e-02, mean val. rec. loss:  2.74492801e-02\n",
      "Epoch: 789 mean train loss:  2.93073424e-02, mean val. rec. loss:  2.74309074e-02\n",
      "Epoch: 790 mean train loss:  2.92874262e-02, mean val. rec. loss:  2.74126663e-02\n",
      "Epoch: 791 mean train loss:  2.92676423e-02, mean val. rec. loss:  2.73945249e-02\n",
      "Epoch: 792 mean train loss:  2.92479608e-02, mean val. rec. loss:  2.73765038e-02\n",
      "Epoch: 793 mean train loss:  2.92284134e-02, mean val. rec. loss:  2.73586096e-02\n",
      "Epoch: 794 mean train loss:  2.92089815e-02, mean val. rec. loss:  2.73407994e-02\n",
      "Epoch: 795 mean train loss:  2.91896650e-02, mean val. rec. loss:  2.73231116e-02\n",
      "Epoch: 796 mean train loss:  2.91704621e-02, mean val. rec. loss:  2.73055282e-02\n",
      "Epoch: 797 mean train loss:  2.91513672e-02, mean val. rec. loss:  2.72880581e-02\n",
      "Epoch: 798 mean train loss:  2.91323952e-02, mean val. rec. loss:  2.72706902e-02\n",
      "Epoch: 799 mean train loss:  2.91135293e-02, mean val. rec. loss:  2.72534265e-02\n",
      "Epoch: 800 mean train loss:  2.90947789e-02, mean val. rec. loss:  2.72362672e-02\n",
      "Epoch: 801 mean train loss:  2.90761291e-02, mean val. rec. loss:  2.72192122e-02\n",
      "Epoch: 802 mean train loss:  2.90575984e-02, mean val. rec. loss:  2.72022660e-02\n",
      "Epoch: 803 mean train loss:  2.90391702e-02, mean val. rec. loss:  2.71854106e-02\n",
      "Epoch: 804 mean train loss:  2.90208462e-02, mean val. rec. loss:  2.71686549e-02\n",
      "Epoch: 805 mean train loss:  2.90026266e-02, mean val. rec. loss:  2.71519900e-02\n",
      "Epoch: 806 mean train loss:  2.89845093e-02, mean val. rec. loss:  2.71354476e-02\n",
      "Epoch: 807 mean train loss:  2.89664963e-02, mean val. rec. loss:  2.71189777e-02\n",
      "Epoch: 808 mean train loss:  2.89485895e-02, mean val. rec. loss:  2.71026008e-02\n",
      "Epoch: 809 mean train loss:  2.89307683e-02, mean val. rec. loss:  2.70863305e-02\n",
      "Epoch: 810 mean train loss:  2.89130589e-02, mean val. rec. loss:  2.70701487e-02\n",
      "Epoch: 811 mean train loss:  2.88954482e-02, mean val. rec. loss:  2.70540575e-02\n",
      "Epoch: 812 mean train loss:  2.88779287e-02, mean val. rec. loss:  2.70380594e-02\n",
      "Epoch: 813 mean train loss:  2.88605079e-02, mean val. rec. loss:  2.70221610e-02\n",
      "Epoch: 814 mean train loss:  2.88431858e-02, mean val. rec. loss:  2.70063262e-02\n",
      "Epoch: 815 mean train loss:  2.88259494e-02, mean val. rec. loss:  2.69905957e-02\n",
      "Epoch: 816 mean train loss:  2.88088154e-02, mean val. rec. loss:  2.69749604e-02\n",
      "Epoch: 817 mean train loss:  2.87917707e-02, mean val. rec. loss:  2.69593932e-02\n",
      "Epoch: 818 mean train loss:  2.87748136e-02, mean val. rec. loss:  2.69439144e-02\n",
      "Epoch: 819 mean train loss:  2.87579478e-02, mean val. rec. loss:  2.69285241e-02\n",
      "Epoch: 820 mean train loss:  2.87411750e-02, mean val. rec. loss:  2.69132154e-02\n",
      "Epoch: 821 mean train loss:  2.87244842e-02, mean val. rec. loss:  2.68979883e-02\n",
      "Epoch: 822 mean train loss:  2.87078865e-02, mean val. rec. loss:  2.68828429e-02\n",
      "Epoch: 823 mean train loss:  2.86913744e-02, mean val. rec. loss:  2.68677815e-02\n",
      "Epoch: 824 mean train loss:  2.86749462e-02, mean val. rec. loss:  2.68527971e-02\n",
      "Epoch: 825 mean train loss:  2.86586036e-02, mean val. rec. loss:  2.68379012e-02\n",
      "Epoch: 826 mean train loss:  2.86423486e-02, mean val. rec. loss:  2.68230620e-02\n",
      "Epoch: 827 mean train loss:  2.86261698e-02, mean val. rec. loss:  2.68083157e-02\n",
      "Epoch: 828 mean train loss:  2.86100768e-02, mean val. rec. loss:  2.67936330e-02\n",
      "Epoch: 829 mean train loss:  2.85940582e-02, mean val. rec. loss:  2.67790342e-02\n",
      "Epoch: 830 mean train loss:  2.85781309e-02, mean val. rec. loss:  2.67644989e-02\n",
      "Epoch: 831 mean train loss:  2.85622744e-02, mean val. rec. loss:  2.67500566e-02\n",
      "Epoch: 832 mean train loss:  2.85465053e-02, mean val. rec. loss:  2.67356664e-02\n",
      "Epoch: 833 mean train loss:  2.85307996e-02, mean val. rec. loss:  2.67213601e-02\n",
      "Epoch: 834 mean train loss:  2.85151814e-02, mean val. rec. loss:  2.67071242e-02\n",
      "Epoch: 835 mean train loss:  2.84996359e-02, mean val. rec. loss:  2.66929608e-02\n",
      "Epoch: 836 mean train loss:  2.84841666e-02, mean val. rec. loss:  2.66788542e-02\n",
      "Epoch: 837 mean train loss:  2.84687719e-02, mean val. rec. loss:  2.66648246e-02\n",
      "Epoch: 838 mean train loss:  2.84534517e-02, mean val. rec. loss:  2.66508585e-02\n",
      "Epoch: 839 mean train loss:  2.84382078e-02, mean val. rec. loss:  2.66369719e-02\n",
      "Epoch: 840 mean train loss:  2.84230309e-02, mean val. rec. loss:  2.66231442e-02\n",
      "Epoch: 841 mean train loss:  2.84079286e-02, mean val. rec. loss:  2.66093822e-02\n",
      "Epoch: 842 mean train loss:  2.83929025e-02, mean val. rec. loss:  2.65956929e-02\n",
      "Epoch: 843 mean train loss:  2.83779380e-02, mean val. rec. loss:  2.65820625e-02\n",
      "Epoch: 844 mean train loss:  2.83630479e-02, mean val. rec. loss:  2.65684910e-02\n",
      "Epoch: 845 mean train loss:  2.83482230e-02, mean val. rec. loss:  2.65549786e-02\n",
      "Epoch: 846 mean train loss:  2.83334596e-02, mean val. rec. loss:  2.65415432e-02\n",
      "Epoch: 847 mean train loss:  2.83187762e-02, mean val. rec. loss:  2.65281623e-02\n",
      "Epoch: 848 mean train loss:  2.83041524e-02, mean val. rec. loss:  2.65148335e-02\n",
      "Epoch: 849 mean train loss:  2.82895938e-02, mean val. rec. loss:  2.65015819e-02\n",
      "Epoch: 850 mean train loss:  2.82751115e-02, mean val. rec. loss:  2.64883869e-02\n",
      "Epoch: 851 mean train loss:  2.82606777e-02, mean val. rec. loss:  2.64752419e-02\n",
      "Epoch: 852 mean train loss:  2.82463184e-02, mean val. rec. loss:  2.64621558e-02\n",
      "Epoch: 853 mean train loss:  2.82320149e-02, mean val. rec. loss:  2.64491332e-02\n",
      "Epoch: 854 mean train loss:  2.82177840e-02, mean val. rec. loss:  2.64361650e-02\n",
      "Epoch: 855 mean train loss:  2.82036090e-02, mean val. rec. loss:  2.64232513e-02\n",
      "Epoch: 856 mean train loss:  2.81894936e-02, mean val. rec. loss:  2.64104056e-02\n",
      "Epoch: 857 mean train loss:  2.81754490e-02, mean val. rec. loss:  2.63976053e-02\n",
      "Epoch: 858 mean train loss:  2.81614490e-02, mean val. rec. loss:  2.63848481e-02\n",
      "Epoch: 859 mean train loss:  2.81475161e-02, mean val. rec. loss:  2.63721634e-02\n",
      "Epoch: 860 mean train loss:  2.81336428e-02, mean val. rec. loss:  2.63595264e-02\n",
      "Epoch: 861 mean train loss:  2.81198309e-02, mean val. rec. loss:  2.63469460e-02\n",
      "Epoch: 862 mean train loss:  2.81060749e-02, mean val. rec. loss:  2.63344156e-02\n",
      "Epoch: 863 mean train loss:  2.80923729e-02, mean val. rec. loss:  2.63219237e-02\n",
      "Epoch: 864 mean train loss:  2.80787268e-02, mean val. rec. loss:  2.63094998e-02\n",
      "Epoch: 865 mean train loss:  2.80651365e-02, mean val. rec. loss:  2.62971259e-02\n",
      "Epoch: 866 mean train loss:  2.80516059e-02, mean val. rec. loss:  2.62848041e-02\n",
      "Epoch: 867 mean train loss:  2.80381310e-02, mean val. rec. loss:  2.62725231e-02\n",
      "Epoch: 868 mean train loss:  2.80247084e-02, mean val. rec. loss:  2.62602943e-02\n",
      "Epoch: 869 mean train loss:  2.80113416e-02, mean val. rec. loss:  2.62481200e-02\n",
      "Epoch: 870 mean train loss:  2.79980269e-02, mean val. rec. loss:  2.62359864e-02\n",
      "Epoch: 871 mean train loss:  2.79847607e-02, mean val. rec. loss:  2.62239118e-02\n",
      "Epoch: 872 mean train loss:  2.79715503e-02, mean val. rec. loss:  2.62118645e-02\n",
      "Epoch: 873 mean train loss:  2.79583865e-02, mean val. rec. loss:  2.61998761e-02\n",
      "Epoch: 874 mean train loss:  2.79452785e-02, mean val. rec. loss:  2.61879330e-02\n",
      "Epoch: 875 mean train loss:  2.79322208e-02, mean val. rec. loss:  2.61760399e-02\n",
      "Epoch: 876 mean train loss:  2.79192153e-02, mean val. rec. loss:  2.61641875e-02\n",
      "Epoch: 877 mean train loss:  2.79062582e-02, mean val. rec. loss:  2.61523806e-02\n",
      "Epoch: 878 mean train loss:  2.78933513e-02, mean val. rec. loss:  2.61406190e-02\n",
      "Epoch: 879 mean train loss:  2.78804854e-02, mean val. rec. loss:  2.61288914e-02\n",
      "Epoch: 880 mean train loss:  2.78676754e-02, mean val. rec. loss:  2.61172182e-02\n",
      "Epoch: 881 mean train loss:  2.78549064e-02, mean val. rec. loss:  2.61055882e-02\n",
      "Epoch: 882 mean train loss:  2.78421914e-02, mean val. rec. loss:  2.60940080e-02\n",
      "Epoch: 883 mean train loss:  2.78295247e-02, mean val. rec. loss:  2.60824573e-02\n",
      "Epoch: 884 mean train loss:  2.78169028e-02, mean val. rec. loss:  2.60709520e-02\n",
      "Epoch: 885 mean train loss:  2.78043312e-02, mean val. rec. loss:  2.60594898e-02\n",
      "Epoch: 886 mean train loss:  2.77918023e-02, mean val. rec. loss:  2.60480683e-02\n",
      "Epoch: 887 mean train loss:  2.77793126e-02, mean val. rec. loss:  2.60366900e-02\n",
      "Epoch: 888 mean train loss:  2.77668751e-02, mean val. rec. loss:  2.60253366e-02\n",
      "Epoch: 889 mean train loss:  2.77544766e-02, mean val. rec. loss:  2.60140354e-02\n",
      "Epoch: 890 mean train loss:  2.77421266e-02, mean val. rec. loss:  2.60027728e-02\n",
      "Epoch: 891 mean train loss:  2.77298231e-02, mean val. rec. loss:  2.59915555e-02\n",
      "Epoch: 892 mean train loss:  2.77175587e-02, mean val. rec. loss:  2.59803722e-02\n",
      "Epoch: 893 mean train loss:  2.77053427e-02, mean val. rec. loss:  2.59692298e-02\n",
      "Epoch: 894 mean train loss:  2.76931640e-02, mean val. rec. loss:  2.59581168e-02\n",
      "Epoch: 895 mean train loss:  2.76810262e-02, mean val. rec. loss:  2.59470469e-02\n",
      "Epoch: 896 mean train loss:  2.76689294e-02, mean val. rec. loss:  2.59360178e-02\n",
      "Epoch: 897 mean train loss:  2.76568755e-02, mean val. rec. loss:  2.59250228e-02\n",
      "Epoch: 898 mean train loss:  2.76448625e-02, mean val. rec. loss:  2.59140641e-02\n",
      "Epoch: 899 mean train loss:  2.76328886e-02, mean val. rec. loss:  2.59031371e-02\n",
      "Epoch: 900 mean train loss:  2.76209557e-02, mean val. rec. loss:  2.58922554e-02\n",
      "Epoch: 901 mean train loss:  2.76090600e-02, mean val. rec. loss:  2.58813919e-02\n",
      "Epoch: 902 mean train loss:  2.75972034e-02, mean val. rec. loss:  2.58705738e-02\n",
      "Epoch: 903 mean train loss:  2.75853897e-02, mean val. rec. loss:  2.58598010e-02\n",
      "Epoch: 904 mean train loss:  2.75736169e-02, mean val. rec. loss:  2.58490486e-02\n",
      "Epoch: 905 mean train loss:  2.75618833e-02, mean val. rec. loss:  2.58383258e-02\n",
      "Epoch: 906 mean train loss:  2.75501813e-02, mean val. rec. loss:  2.58276505e-02\n",
      "Epoch: 907 mean train loss:  2.75385146e-02, mean val. rec. loss:  2.58170002e-02\n",
      "Epoch: 908 mean train loss:  2.75268871e-02, mean val. rec. loss:  2.58063953e-02\n",
      "Epoch: 909 mean train loss:  2.75153024e-02, mean val. rec. loss:  2.57958084e-02\n",
      "Epoch: 910 mean train loss:  2.75037513e-02, mean val. rec. loss:  2.57852602e-02\n",
      "Epoch: 911 mean train loss:  2.74922392e-02, mean val. rec. loss:  2.57747414e-02\n",
      "Epoch: 912 mean train loss:  2.74807588e-02, mean val. rec. loss:  2.57642567e-02\n",
      "Epoch: 913 mean train loss:  2.74693138e-02, mean val. rec. loss:  2.57538014e-02\n",
      "Epoch: 914 mean train loss:  2.74579023e-02, mean val. rec. loss:  2.57433824e-02\n",
      "Epoch: 915 mean train loss:  2.74465280e-02, mean val. rec. loss:  2.57329816e-02\n",
      "Epoch: 916 mean train loss:  2.74351854e-02, mean val. rec. loss:  2.57226193e-02\n",
      "Epoch: 917 mean train loss:  2.74238801e-02, mean val. rec. loss:  2.57122911e-02\n",
      "Epoch: 918 mean train loss:  2.74126082e-02, mean val. rec. loss:  2.57019968e-02\n",
      "Epoch: 919 mean train loss:  2.74013736e-02, mean val. rec. loss:  2.56917207e-02\n",
      "Epoch: 920 mean train loss:  2.73901707e-02, mean val. rec. loss:  2.56814855e-02\n",
      "Epoch: 921 mean train loss:  2.73790050e-02, mean val. rec. loss:  2.56712797e-02\n",
      "Epoch: 922 mean train loss:  2.73678728e-02, mean val. rec. loss:  2.56610966e-02\n",
      "Epoch: 923 mean train loss:  2.73567667e-02, mean val. rec. loss:  2.56509429e-02\n",
      "Epoch: 924 mean train loss:  2.73456904e-02, mean val. rec. loss:  2.56408211e-02\n",
      "Epoch: 925 mean train loss:  2.73346513e-02, mean val. rec. loss:  2.56307196e-02\n",
      "Epoch: 926 mean train loss:  2.73236383e-02, mean val. rec. loss:  2.56206567e-02\n",
      "Epoch: 927 mean train loss:  2.73126700e-02, mean val. rec. loss:  2.56106165e-02\n",
      "Epoch: 928 mean train loss:  2.73017241e-02, mean val. rec. loss:  2.56006148e-02\n",
      "Epoch: 929 mean train loss:  2.72908154e-02, mean val. rec. loss:  2.55906267e-02\n",
      "Epoch: 930 mean train loss:  2.72799309e-02, mean val. rec. loss:  2.55806704e-02\n",
      "Epoch: 931 mean train loss:  2.72690855e-02, mean val. rec. loss:  2.55707300e-02\n",
      "Epoch: 932 mean train loss:  2.72582606e-02, mean val. rec. loss:  2.55608258e-02\n",
      "Epoch: 933 mean train loss:  2.72474673e-02, mean val. rec. loss:  2.55509625e-02\n",
      "Epoch: 934 mean train loss:  2.72367113e-02, mean val. rec. loss:  2.55411060e-02\n",
      "Epoch: 935 mean train loss:  2.72259739e-02, mean val. rec. loss:  2.55312835e-02\n",
      "Epoch: 936 mean train loss:  2.72152738e-02, mean val. rec. loss:  2.55214746e-02\n",
      "Epoch: 937 mean train loss:  2.72045960e-02, mean val. rec. loss:  2.55117156e-02\n",
      "Epoch: 938 mean train loss:  2.71939517e-02, mean val. rec. loss:  2.55019634e-02\n",
      "Epoch: 939 mean train loss:  2.71833390e-02, mean val. rec. loss:  2.54922406e-02\n",
      "Epoch: 940 mean train loss:  2.71727506e-02, mean val. rec. loss:  2.54825519e-02\n",
      "Epoch: 941 mean train loss:  2.71621901e-02, mean val. rec. loss:  2.54728678e-02\n",
      "Epoch: 942 mean train loss:  2.71516576e-02, mean val. rec. loss:  2.54632199e-02\n",
      "Epoch: 943 mean train loss:  2.71411548e-02, mean val. rec. loss:  2.54536015e-02\n",
      "Epoch: 944 mean train loss:  2.71306782e-02, mean val. rec. loss:  2.54439967e-02\n",
      "Epoch: 945 mean train loss:  2.71202275e-02, mean val. rec. loss:  2.54344282e-02\n",
      "Epoch: 946 mean train loss:  2.71098067e-02, mean val. rec. loss:  2.54248711e-02\n",
      "Epoch: 947 mean train loss:  2.70994120e-02, mean val. rec. loss:  2.54153480e-02\n",
      "Epoch: 948 mean train loss:  2.70890470e-02, mean val. rec. loss:  2.54058430e-02\n",
      "Epoch: 949 mean train loss:  2.70787081e-02, mean val. rec. loss:  2.53963607e-02\n",
      "Epoch: 950 mean train loss:  2.70683916e-02, mean val. rec. loss:  2.53869078e-02\n",
      "Epoch: 951 mean train loss:  2.70581049e-02, mean val. rec. loss:  2.53774777e-02\n",
      "Epoch: 952 mean train loss:  2.70478442e-02, mean val. rec. loss:  2.53680611e-02\n",
      "Epoch: 953 mean train loss:  2.70376059e-02, mean val. rec. loss:  2.53586763e-02\n",
      "Epoch: 954 mean train loss:  2.70273992e-02, mean val. rec. loss:  2.53493074e-02\n",
      "Epoch: 955 mean train loss:  2.70172149e-02, mean val. rec. loss:  2.53399635e-02\n",
      "Epoch: 956 mean train loss:  2.70070548e-02, mean val. rec. loss:  2.53306399e-02\n",
      "Epoch: 957 mean train loss:  2.69969170e-02, mean val. rec. loss:  2.53213481e-02\n",
      "Epoch: 958 mean train loss:  2.69868053e-02, mean val. rec. loss:  2.53120586e-02\n",
      "Epoch: 959 mean train loss:  2.69767178e-02, mean val. rec. loss:  2.53028030e-02\n",
      "Epoch: 960 mean train loss:  2.69666601e-02, mean val. rec. loss:  2.52935725e-02\n",
      "Epoch: 961 mean train loss:  2.69566155e-02, mean val. rec. loss:  2.52843419e-02\n",
      "Epoch: 962 mean train loss:  2.69466006e-02, mean val. rec. loss:  2.52751590e-02\n",
      "Epoch: 963 mean train loss:  2.69366174e-02, mean val. rec. loss:  2.52659806e-02\n",
      "Epoch: 964 mean train loss:  2.69266454e-02, mean val. rec. loss:  2.52568248e-02\n",
      "Epoch: 965 mean train loss:  2.69167032e-02, mean val. rec. loss:  2.52476941e-02\n",
      "Epoch: 966 mean train loss:  2.69067833e-02, mean val. rec. loss:  2.52385905e-02\n",
      "Epoch: 967 mean train loss:  2.68968895e-02, mean val. rec. loss:  2.52295005e-02\n",
      "Epoch: 968 mean train loss:  2.68870180e-02, mean val. rec. loss:  2.52204287e-02\n",
      "Epoch: 969 mean train loss:  2.68771651e-02, mean val. rec. loss:  2.52113751e-02\n",
      "Epoch: 970 mean train loss:  2.68673328e-02, mean val. rec. loss:  2.52023509e-02\n",
      "Epoch: 971 mean train loss:  2.68575376e-02, mean val. rec. loss:  2.51933358e-02\n",
      "Epoch: 972 mean train loss:  2.68477462e-02, mean val. rec. loss:  2.51843456e-02\n",
      "Epoch: 973 mean train loss:  2.68379921e-02, mean val. rec. loss:  2.51753758e-02\n",
      "Epoch: 974 mean train loss:  2.68282547e-02, mean val. rec. loss:  2.51664220e-02\n",
      "Epoch: 975 mean train loss:  2.68185378e-02, mean val. rec. loss:  2.51574839e-02\n",
      "Epoch: 976 mean train loss:  2.68088395e-02, mean val. rec. loss:  2.51485709e-02\n",
      "Epoch: 977 mean train loss:  2.67991729e-02, mean val. rec. loss:  2.51396828e-02\n",
      "Epoch: 978 mean train loss:  2.67895230e-02, mean val. rec. loss:  2.51308128e-02\n",
      "Epoch: 979 mean train loss:  2.67798936e-02, mean val. rec. loss:  2.51219451e-02\n",
      "Epoch: 980 mean train loss:  2.67702828e-02, mean val. rec. loss:  2.51131092e-02\n",
      "Epoch: 981 mean train loss:  2.67606982e-02, mean val. rec. loss:  2.51042914e-02\n",
      "Epoch: 982 mean train loss:  2.67511339e-02, mean val. rec. loss:  2.50954985e-02\n",
      "Epoch: 983 mean train loss:  2.67415921e-02, mean val. rec. loss:  2.50867102e-02\n",
      "Epoch: 984 mean train loss:  2.67320726e-02, mean val. rec. loss:  2.50779400e-02\n",
      "Epoch: 985 mean train loss:  2.67225642e-02, mean val. rec. loss:  2.50691993e-02\n",
      "Epoch: 986 mean train loss:  2.67130875e-02, mean val. rec. loss:  2.50604768e-02\n",
      "Epoch: 987 mean train loss:  2.67036220e-02, mean val. rec. loss:  2.50517520e-02\n",
      "Epoch: 988 mean train loss:  2.66941807e-02, mean val. rec. loss:  2.50430748e-02\n",
      "Epoch: 989 mean train loss:  2.66847617e-02, mean val. rec. loss:  2.50343976e-02\n",
      "Epoch: 990 mean train loss:  2.66753633e-02, mean val. rec. loss:  2.50257385e-02\n",
      "Epoch: 991 mean train loss:  2.66659797e-02, mean val. rec. loss:  2.50171067e-02\n",
      "Epoch: 992 mean train loss:  2.66566203e-02, mean val. rec. loss:  2.50084749e-02\n",
      "Epoch: 993 mean train loss:  2.66472759e-02, mean val. rec. loss:  2.49998703e-02\n",
      "Epoch: 994 mean train loss:  2.66379593e-02, mean val. rec. loss:  2.49912747e-02\n",
      "Epoch: 995 mean train loss:  2.66286502e-02, mean val. rec. loss:  2.49826996e-02\n",
      "Epoch: 996 mean train loss:  2.66193653e-02, mean val. rec. loss:  2.49741449e-02\n",
      "Epoch: 997 mean train loss:  2.66100991e-02, mean val. rec. loss:  2.49656151e-02\n",
      "Epoch: 998 mean train loss:  2.66008552e-02, mean val. rec. loss:  2.49570876e-02\n",
      "Epoch: 999 mean train loss:  2.65916206e-02, mean val. rec. loss:  2.49485896e-02\n",
      "Epoch: 1000 mean train loss:  2.65824139e-02, mean val. rec. loss:  2.49400938e-02\n",
      "Epoch: 1001 mean train loss:  2.65732221e-02, mean val. rec. loss:  2.49316185e-02\n",
      "Epoch: 1002 mean train loss:  2.65640508e-02, mean val. rec. loss:  2.49231681e-02\n",
      "Epoch: 1003 mean train loss:  2.65548963e-02, mean val. rec. loss:  2.49147154e-02\n",
      "Epoch: 1004 mean train loss:  2.65457604e-02, mean val. rec. loss:  2.49063036e-02\n",
      "Epoch: 1005 mean train loss:  2.65366431e-02, mean val. rec. loss:  2.48978895e-02\n",
      "Epoch: 1006 mean train loss:  2.65275482e-02, mean val. rec. loss:  2.48894981e-02\n",
      "Epoch: 1007 mean train loss:  2.65184644e-02, mean val. rec. loss:  2.48811248e-02\n",
      "Epoch: 1008 mean train loss:  2.65094011e-02, mean val. rec. loss:  2.48727606e-02\n",
      "Epoch: 1009 mean train loss:  2.65003565e-02, mean val. rec. loss:  2.48644213e-02\n",
      "Epoch: 1010 mean train loss:  2.64913342e-02, mean val. rec. loss:  2.48560775e-02\n",
      "Epoch: 1011 mean train loss:  2.64823193e-02, mean val. rec. loss:  2.48477677e-02\n",
      "Epoch: 1012 mean train loss:  2.64733305e-02, mean val. rec. loss:  2.48394693e-02\n",
      "Epoch: 1013 mean train loss:  2.64643547e-02, mean val. rec. loss:  2.48311913e-02\n",
      "Epoch: 1014 mean train loss:  2.64554013e-02, mean val. rec. loss:  2.48229246e-02\n",
      "Epoch: 1015 mean train loss:  2.64464591e-02, mean val. rec. loss:  2.48146783e-02\n",
      "Epoch: 1016 mean train loss:  2.64375392e-02, mean val. rec. loss:  2.48064365e-02\n",
      "Epoch: 1017 mean train loss:  2.64286323e-02, mean val. rec. loss:  2.47982129e-02\n",
      "Epoch: 1018 mean train loss:  2.64197497e-02, mean val. rec. loss:  2.47900120e-02\n",
      "Epoch: 1019 mean train loss:  2.64108726e-02, mean val. rec. loss:  2.47818111e-02\n",
      "Epoch: 1020 mean train loss:  2.64020235e-02, mean val. rec. loss:  2.47736419e-02\n",
      "Epoch: 1021 mean train loss:  2.63931837e-02, mean val. rec. loss:  2.47654750e-02\n",
      "Epoch: 1022 mean train loss:  2.63843681e-02, mean val. rec. loss:  2.47573263e-02\n",
      "Epoch: 1023 mean train loss:  2.63755618e-02, mean val. rec. loss:  2.47491979e-02\n",
      "Epoch: 1024 mean train loss:  2.63667741e-02, mean val. rec. loss:  2.47410741e-02\n",
      "Epoch: 1025 mean train loss:  2.63580032e-02, mean val. rec. loss:  2.47329617e-02\n",
      "Epoch: 1026 mean train loss:  2.63492472e-02, mean val. rec. loss:  2.47248764e-02\n",
      "Epoch: 1027 mean train loss:  2.63405079e-02, mean val. rec. loss:  2.47168025e-02\n",
      "Epoch: 1028 mean train loss:  2.63317873e-02, mean val. rec. loss:  2.47087354e-02\n",
      "Epoch: 1029 mean train loss:  2.63230815e-02, mean val. rec. loss:  2.47006910e-02\n",
      "Epoch: 1030 mean train loss:  2.63143907e-02, mean val. rec. loss:  2.46926533e-02\n",
      "Epoch: 1031 mean train loss:  2.63057129e-02, mean val. rec. loss:  2.46846293e-02\n",
      "Epoch: 1032 mean train loss:  2.62970574e-02, mean val. rec. loss:  2.46766325e-02\n",
      "Epoch: 1033 mean train loss:  2.62884150e-02, mean val. rec. loss:  2.46686425e-02\n",
      "Epoch: 1034 mean train loss:  2.62797875e-02, mean val. rec. loss:  2.46606571e-02\n",
      "Epoch: 1035 mean train loss:  2.62711748e-02, mean val. rec. loss:  2.46526920e-02\n",
      "Epoch: 1036 mean train loss:  2.62625827e-02, mean val. rec. loss:  2.46447451e-02\n",
      "Epoch: 1037 mean train loss:  2.62539942e-02, mean val. rec. loss:  2.46367982e-02\n",
      "Epoch: 1038 mean train loss:  2.62454300e-02, mean val. rec. loss:  2.46288876e-02\n",
      "Epoch: 1039 mean train loss:  2.62368807e-02, mean val. rec. loss:  2.46209611e-02\n",
      "Epoch: 1040 mean train loss:  2.62283426e-02, mean val. rec. loss:  2.46130709e-02\n",
      "Epoch: 1041 mean train loss:  2.62198286e-02, mean val. rec. loss:  2.46051875e-02\n",
      "Epoch: 1042 mean train loss:  2.62113184e-02, mean val. rec. loss:  2.45973131e-02\n",
      "Epoch: 1043 mean train loss:  2.62028306e-02, mean val. rec. loss:  2.45894615e-02\n",
      "Epoch: 1044 mean train loss:  2.61943539e-02, mean val. rec. loss:  2.45816166e-02\n",
      "Epoch: 1045 mean train loss:  2.61858977e-02, mean val. rec. loss:  2.45737899e-02\n",
      "Epoch: 1046 mean train loss:  2.61774508e-02, mean val. rec. loss:  2.45659609e-02\n",
      "Epoch: 1047 mean train loss:  2.61690132e-02, mean val. rec. loss:  2.45581592e-02\n",
      "Epoch: 1048 mean train loss:  2.61605961e-02, mean val. rec. loss:  2.45503620e-02\n",
      "Epoch: 1049 mean train loss:  2.61521976e-02, mean val. rec. loss:  2.45425965e-02\n",
      "Epoch: 1050 mean train loss:  2.61438159e-02, mean val. rec. loss:  2.45348265e-02\n",
      "Epoch: 1051 mean train loss:  2.61354453e-02, mean val. rec. loss:  2.45270701e-02\n",
      "Epoch: 1052 mean train loss:  2.61270822e-02, mean val. rec. loss:  2.45193227e-02\n",
      "Epoch: 1053 mean train loss:  2.61187378e-02, mean val. rec. loss:  2.45116026e-02\n",
      "Epoch: 1054 mean train loss:  2.61104101e-02, mean val. rec. loss:  2.45038939e-02\n",
      "Epoch: 1055 mean train loss:  2.61020935e-02, mean val. rec. loss:  2.44961805e-02\n",
      "Epoch: 1056 mean train loss:  2.60937900e-02, mean val. rec. loss:  2.44884945e-02\n",
      "Epoch: 1057 mean train loss:  2.60855033e-02, mean val. rec. loss:  2.44808174e-02\n",
      "Epoch: 1058 mean train loss:  2.60772333e-02, mean val. rec. loss:  2.44731518e-02\n",
      "Epoch: 1059 mean train loss:  2.60689707e-02, mean val. rec. loss:  2.44654997e-02\n",
      "Epoch: 1060 mean train loss:  2.60607249e-02, mean val. rec. loss:  2.44578499e-02\n",
      "Epoch: 1061 mean train loss:  2.60524903e-02, mean val. rec. loss:  2.44502182e-02\n",
      "Epoch: 1062 mean train loss:  2.60442725e-02, mean val. rec. loss:  2.44426070e-02\n",
      "Epoch: 1063 mean train loss:  2.60360714e-02, mean val. rec. loss:  2.44349957e-02\n",
      "Epoch: 1064 mean train loss:  2.60278703e-02, mean val. rec. loss:  2.44274117e-02\n",
      "Epoch: 1065 mean train loss:  2.60197028e-02, mean val. rec. loss:  2.44198186e-02\n",
      "Epoch: 1066 mean train loss:  2.60115333e-02, mean val. rec. loss:  2.44122549e-02\n",
      "Epoch: 1067 mean train loss:  2.60033788e-02, mean val. rec. loss:  2.44047004e-02\n",
      "Epoch: 1068 mean train loss:  2.59952447e-02, mean val. rec. loss:  2.43971594e-02\n",
      "Epoch: 1069 mean train loss:  2.59871219e-02, mean val. rec. loss:  2.43896230e-02\n",
      "Epoch: 1070 mean train loss:  2.59790120e-02, mean val. rec. loss:  2.43821025e-02\n",
      "Epoch: 1071 mean train loss:  2.59709134e-02, mean val. rec. loss:  2.43745865e-02\n",
      "Epoch: 1072 mean train loss:  2.59628240e-02, mean val. rec. loss:  2.43670932e-02\n",
      "Epoch: 1073 mean train loss:  2.59547607e-02, mean val. rec. loss:  2.43596135e-02\n",
      "Epoch: 1074 mean train loss:  2.59467049e-02, mean val. rec. loss:  2.43521338e-02\n",
      "Epoch: 1075 mean train loss:  2.59386528e-02, mean val. rec. loss:  2.43446767e-02\n",
      "Epoch: 1076 mean train loss:  2.59306267e-02, mean val. rec. loss:  2.43372197e-02\n",
      "Epoch: 1077 mean train loss:  2.59226007e-02, mean val. rec. loss:  2.43297854e-02\n",
      "Epoch: 1078 mean train loss:  2.59145989e-02, mean val. rec. loss:  2.43223601e-02\n",
      "Epoch: 1079 mean train loss:  2.59066119e-02, mean val. rec. loss:  2.43149348e-02\n",
      "Epoch: 1080 mean train loss:  2.58986268e-02, mean val. rec. loss:  2.43075413e-02\n",
      "Epoch: 1081 mean train loss:  2.58906567e-02, mean val. rec. loss:  2.43001387e-02\n",
      "Epoch: 1082 mean train loss:  2.58827014e-02, mean val. rec. loss:  2.42927633e-02\n",
      "Epoch: 1083 mean train loss:  2.58747647e-02, mean val. rec. loss:  2.42853924e-02\n",
      "Epoch: 1084 mean train loss:  2.58668318e-02, mean val. rec. loss:  2.42780284e-02\n",
      "Epoch: 1085 mean train loss:  2.58589119e-02, mean val. rec. loss:  2.42706916e-02\n",
      "Epoch: 1086 mean train loss:  2.58510106e-02, mean val. rec. loss:  2.42633457e-02\n",
      "Epoch: 1087 mean train loss:  2.58431187e-02, mean val. rec. loss:  2.42560134e-02\n",
      "Epoch: 1088 mean train loss:  2.58352379e-02, mean val. rec. loss:  2.42487060e-02\n",
      "Epoch: 1089 mean train loss:  2.58273683e-02, mean val. rec. loss:  2.42414078e-02\n",
      "Epoch: 1090 mean train loss:  2.58195210e-02, mean val. rec. loss:  2.42341140e-02\n",
      "Epoch: 1091 mean train loss:  2.58116700e-02, mean val. rec. loss:  2.42268226e-02\n",
      "Epoch: 1092 mean train loss:  2.58038358e-02, mean val. rec. loss:  2.42195492e-02\n",
      "Epoch: 1093 mean train loss:  2.57960164e-02, mean val. rec. loss:  2.42122918e-02\n",
      "Epoch: 1094 mean train loss:  2.57882120e-02, mean val. rec. loss:  2.42050411e-02\n",
      "Epoch: 1095 mean train loss:  2.57804150e-02, mean val. rec. loss:  2.41978109e-02\n",
      "Epoch: 1096 mean train loss:  2.57726348e-02, mean val. rec. loss:  2.41905784e-02\n",
      "Epoch: 1097 mean train loss:  2.57648694e-02, mean val. rec. loss:  2.41833618e-02\n",
      "Epoch: 1098 mean train loss:  2.57571078e-02, mean val. rec. loss:  2.41761497e-02\n",
      "Epoch: 1099 mean train loss:  2.57493611e-02, mean val. rec. loss:  2.41689625e-02\n",
      "Epoch: 1100 mean train loss:  2.57416274e-02, mean val. rec. loss:  2.41617822e-02\n",
      "Epoch: 1101 mean train loss:  2.57339031e-02, mean val. rec. loss:  2.41545973e-02\n",
      "Epoch: 1102 mean train loss:  2.57261899e-02, mean val. rec. loss:  2.41474374e-02\n",
      "Epoch: 1103 mean train loss:  2.57184934e-02, mean val. rec. loss:  2.41402843e-02\n",
      "Epoch: 1104 mean train loss:  2.57108082e-02, mean val. rec. loss:  2.41331357e-02\n",
      "Epoch: 1105 mean train loss:  2.57031285e-02, mean val. rec. loss:  2.41260166e-02\n",
      "Epoch: 1106 mean train loss:  2.56954656e-02, mean val. rec. loss:  2.41188793e-02\n",
      "Epoch: 1107 mean train loss:  2.56878120e-02, mean val. rec. loss:  2.41117829e-02\n",
      "Epoch: 1108 mean train loss:  2.56801696e-02, mean val. rec. loss:  2.41046751e-02\n",
      "Epoch: 1109 mean train loss:  2.56725439e-02, mean val. rec. loss:  2.40975855e-02\n",
      "Epoch: 1110 mean train loss:  2.56649219e-02, mean val. rec. loss:  2.40905027e-02\n",
      "Epoch: 1111 mean train loss:  2.56573112e-02, mean val. rec. loss:  2.40834380e-02\n",
      "Epoch: 1112 mean train loss:  2.56497246e-02, mean val. rec. loss:  2.40763733e-02\n",
      "Epoch: 1113 mean train loss:  2.56421362e-02, mean val. rec. loss:  2.40693291e-02\n",
      "Epoch: 1114 mean train loss:  2.56345682e-02, mean val. rec. loss:  2.40622803e-02\n",
      "Epoch: 1115 mean train loss:  2.56269966e-02, mean val. rec. loss:  2.40552587e-02\n",
      "Epoch: 1116 mean train loss:  2.56194510e-02, mean val. rec. loss:  2.40482439e-02\n",
      "Epoch: 1117 mean train loss:  2.56119165e-02, mean val. rec. loss:  2.40412246e-02\n",
      "Epoch: 1118 mean train loss:  2.56043821e-02, mean val. rec. loss:  2.40342257e-02\n",
      "Epoch: 1119 mean train loss:  2.55968700e-02, mean val. rec. loss:  2.40272427e-02\n",
      "Epoch: 1120 mean train loss:  2.55893654e-02, mean val. rec. loss:  2.40202596e-02\n",
      "Epoch: 1121 mean train loss:  2.55818682e-02, mean val. rec. loss:  2.40132925e-02\n",
      "Epoch: 1122 mean train loss:  2.55743860e-02, mean val. rec. loss:  2.40063389e-02\n",
      "Epoch: 1123 mean train loss:  2.55669149e-02, mean val. rec. loss:  2.39993854e-02\n",
      "Epoch: 1124 mean train loss:  2.55594512e-02, mean val. rec. loss:  2.39924409e-02\n",
      "Epoch: 1125 mean train loss:  2.55519950e-02, mean val. rec. loss:  2.39855259e-02\n",
      "Epoch: 1126 mean train loss:  2.55445574e-02, mean val. rec. loss:  2.39785973e-02\n",
      "Epoch: 1127 mean train loss:  2.55371273e-02, mean val. rec. loss:  2.39716869e-02\n",
      "Epoch: 1128 mean train loss:  2.55297083e-02, mean val. rec. loss:  2.39647855e-02\n",
      "Epoch: 1129 mean train loss:  2.55222986e-02, mean val. rec. loss:  2.39578955e-02\n",
      "Epoch: 1130 mean train loss:  2.55149039e-02, mean val. rec. loss:  2.39510100e-02\n",
      "Epoch: 1131 mean train loss:  2.55075147e-02, mean val. rec. loss:  2.39441403e-02\n",
      "Epoch: 1132 mean train loss:  2.55001348e-02, mean val. rec. loss:  2.39372798e-02\n",
      "Epoch: 1133 mean train loss:  2.54927699e-02, mean val. rec. loss:  2.39304306e-02\n",
      "Epoch: 1134 mean train loss:  2.54854179e-02, mean val. rec. loss:  2.39235859e-02\n",
      "Epoch: 1135 mean train loss:  2.54780735e-02, mean val. rec. loss:  2.39167548e-02\n",
      "Epoch: 1136 mean train loss:  2.54707383e-02, mean val. rec. loss:  2.39099328e-02\n",
      "Epoch: 1137 mean train loss:  2.54634162e-02, mean val. rec. loss:  2.39031131e-02\n",
      "Epoch: 1138 mean train loss:  2.54560996e-02, mean val. rec. loss:  2.38963069e-02\n",
      "Epoch: 1139 mean train loss:  2.54487998e-02, mean val. rec. loss:  2.38895212e-02\n",
      "Epoch: 1140 mean train loss:  2.54415094e-02, mean val. rec. loss:  2.38827332e-02\n",
      "Epoch: 1141 mean train loss:  2.54342263e-02, mean val. rec. loss:  2.38759498e-02\n",
      "Epoch: 1142 mean train loss:  2.54269545e-02, mean val. rec. loss:  2.38691890e-02\n",
      "Epoch: 1143 mean train loss:  2.54196901e-02, mean val. rec. loss:  2.38624373e-02\n",
      "Epoch: 1144 mean train loss:  2.54124387e-02, mean val. rec. loss:  2.38556811e-02\n",
      "Epoch: 1145 mean train loss:  2.54051985e-02, mean val. rec. loss:  2.38489453e-02\n",
      "Epoch: 1146 mean train loss:  2.53979732e-02, mean val. rec. loss:  2.38422049e-02\n",
      "Epoch: 1147 mean train loss:  2.53907479e-02, mean val. rec. loss:  2.38354895e-02\n",
      "Epoch: 1148 mean train loss:  2.53835319e-02, mean val. rec. loss:  2.38287809e-02\n",
      "Epoch: 1149 mean train loss:  2.53763346e-02, mean val. rec. loss:  2.38220791e-02\n",
      "Epoch: 1150 mean train loss:  2.53691428e-02, mean val. rec. loss:  2.38153864e-02\n",
      "Epoch: 1151 mean train loss:  2.53619640e-02, mean val. rec. loss:  2.38086959e-02\n",
      "Epoch: 1152 mean train loss:  2.53547909e-02, mean val. rec. loss:  2.38020236e-02\n",
      "Epoch: 1153 mean train loss:  2.53476307e-02, mean val. rec. loss:  2.37953558e-02\n",
      "Epoch: 1154 mean train loss:  2.53404799e-02, mean val. rec. loss:  2.37886994e-02\n",
      "Epoch: 1155 mean train loss:  2.53333403e-02, mean val. rec. loss:  2.37820565e-02\n",
      "Epoch: 1156 mean train loss:  2.53262137e-02, mean val. rec. loss:  2.37754160e-02\n",
      "Epoch: 1157 mean train loss:  2.53190871e-02, mean val. rec. loss:  2.37687935e-02\n",
      "Epoch: 1158 mean train loss:  2.53119772e-02, mean val. rec. loss:  2.37621666e-02\n",
      "Epoch: 1159 mean train loss:  2.53048749e-02, mean val. rec. loss:  2.37555487e-02\n",
      "Epoch: 1160 mean train loss:  2.52977874e-02, mean val. rec. loss:  2.37489535e-02\n",
      "Epoch: 1161 mean train loss:  2.52907017e-02, mean val. rec. loss:  2.37423583e-02\n",
      "Epoch: 1162 mean train loss:  2.52836328e-02, mean val. rec. loss:  2.37357631e-02\n",
      "Epoch: 1163 mean train loss:  2.52765677e-02, mean val. rec. loss:  2.37291860e-02\n",
      "Epoch: 1164 mean train loss:  2.52695119e-02, mean val. rec. loss:  2.37226271e-02\n",
      "Epoch: 1165 mean train loss:  2.52624709e-02, mean val. rec. loss:  2.37160636e-02\n",
      "Epoch: 1166 mean train loss:  2.52554411e-02, mean val. rec. loss:  2.37095138e-02\n",
      "Epoch: 1167 mean train loss:  2.52484151e-02, mean val. rec. loss:  2.37029730e-02\n",
      "Epoch: 1168 mean train loss:  2.52414021e-02, mean val. rec. loss:  2.36964413e-02\n",
      "Epoch: 1169 mean train loss:  2.52343965e-02, mean val. rec. loss:  2.36899187e-02\n",
      "Epoch: 1170 mean train loss:  2.52274003e-02, mean val. rec. loss:  2.36833983e-02\n",
      "Epoch: 1171 mean train loss:  2.52204115e-02, mean val. rec. loss:  2.36768961e-02\n",
      "Epoch: 1172 mean train loss:  2.52134357e-02, mean val. rec. loss:  2.36703961e-02\n",
      "Epoch: 1173 mean train loss:  2.52064730e-02, mean val. rec. loss:  2.36639052e-02\n",
      "Epoch: 1174 mean train loss:  2.51995121e-02, mean val. rec. loss:  2.36574280e-02\n",
      "Epoch: 1175 mean train loss:  2.51925624e-02, mean val. rec. loss:  2.36509507e-02\n",
      "Epoch: 1176 mean train loss:  2.51856239e-02, mean val. rec. loss:  2.36444779e-02\n",
      "Epoch: 1177 mean train loss:  2.51786965e-02, mean val. rec. loss:  2.36380233e-02\n",
      "Epoch: 1178 mean train loss:  2.51717729e-02, mean val. rec. loss:  2.36315869e-02\n",
      "Epoch: 1179 mean train loss:  2.51648605e-02, mean val. rec. loss:  2.36251346e-02\n",
      "Epoch: 1180 mean train loss:  2.51579536e-02, mean val. rec. loss:  2.36187095e-02\n",
      "Epoch: 1181 mean train loss:  2.51510654e-02, mean val. rec. loss:  2.36122866e-02\n",
      "Epoch: 1182 mean train loss:  2.51441827e-02, mean val. rec. loss:  2.36058615e-02\n",
      "Epoch: 1183 mean train loss:  2.51373038e-02, mean val. rec. loss:  2.35994591e-02\n",
      "Epoch: 1184 mean train loss:  2.51304397e-02, mean val. rec. loss:  2.35930589e-02\n",
      "Epoch: 1185 mean train loss:  2.51235813e-02, mean val. rec. loss:  2.35866701e-02\n",
      "Epoch: 1186 mean train loss:  2.51167340e-02, mean val. rec. loss:  2.35802835e-02\n",
      "Epoch: 1187 mean train loss:  2.51098923e-02, mean val. rec. loss:  2.35739083e-02\n",
      "Epoch: 1188 mean train loss:  2.51030618e-02, mean val. rec. loss:  2.35675535e-02\n",
      "Epoch: 1189 mean train loss:  2.50962425e-02, mean val. rec. loss:  2.35611896e-02\n",
      "Epoch: 1190 mean train loss:  2.50894306e-02, mean val. rec. loss:  2.35548416e-02\n",
      "Epoch: 1191 mean train loss:  2.50826224e-02, mean val. rec. loss:  2.35485095e-02\n",
      "Epoch: 1192 mean train loss:  2.50758291e-02, mean val. rec. loss:  2.35421683e-02\n",
      "Epoch: 1193 mean train loss:  2.50690470e-02, mean val. rec. loss:  2.35358385e-02\n",
      "Epoch: 1194 mean train loss:  2.50622631e-02, mean val. rec. loss:  2.35295245e-02\n",
      "Epoch: 1195 mean train loss:  2.50555015e-02, mean val. rec. loss:  2.35232105e-02\n",
      "Epoch: 1196 mean train loss:  2.50487343e-02, mean val. rec. loss:  2.35169146e-02\n",
      "Epoch: 1197 mean train loss:  2.50419876e-02, mean val. rec. loss:  2.35106165e-02\n",
      "Epoch: 1198 mean train loss:  2.50352446e-02, mean val. rec. loss:  2.35043275e-02\n",
      "Epoch: 1199 mean train loss:  2.50285053e-02, mean val. rec. loss:  2.34980566e-02\n",
      "Epoch: 1200 mean train loss:  2.50217846e-02, mean val. rec. loss:  2.34917925e-02\n",
      "Epoch: 1201 mean train loss:  2.50150733e-02, mean val. rec. loss:  2.34855307e-02\n",
      "Epoch: 1202 mean train loss:  2.50083601e-02, mean val. rec. loss:  2.34792666e-02\n",
      "Epoch: 1203 mean train loss:  2.50016618e-02, mean val. rec. loss:  2.34730366e-02\n",
      "Epoch: 1204 mean train loss:  2.49949728e-02, mean val. rec. loss:  2.34667838e-02\n",
      "Epoch: 1205 mean train loss:  2.49882913e-02, mean val. rec. loss:  2.34605628e-02\n",
      "Epoch: 1206 mean train loss:  2.49816135e-02, mean val. rec. loss:  2.34543282e-02\n",
      "Epoch: 1207 mean train loss:  2.49749506e-02, mean val. rec. loss:  2.34481231e-02\n",
      "Epoch: 1208 mean train loss:  2.49682988e-02, mean val. rec. loss:  2.34419157e-02\n",
      "Epoch: 1209 mean train loss:  2.49616471e-02, mean val. rec. loss:  2.34357129e-02\n",
      "Epoch: 1210 mean train loss:  2.49550046e-02, mean val. rec. loss:  2.34295304e-02\n",
      "Epoch: 1211 mean train loss:  2.49483771e-02, mean val. rec. loss:  2.34233366e-02\n",
      "Epoch: 1212 mean train loss:  2.49417570e-02, mean val. rec. loss:  2.34171565e-02\n",
      "Epoch: 1213 mean train loss:  2.49351369e-02, mean val. rec. loss:  2.34109899e-02\n",
      "Epoch: 1214 mean train loss:  2.49285373e-02, mean val. rec. loss:  2.34048256e-02\n",
      "Epoch: 1215 mean train loss:  2.49219303e-02, mean val. rec. loss:  2.33986726e-02\n",
      "Epoch: 1216 mean train loss:  2.49153456e-02, mean val. rec. loss:  2.33925220e-02\n",
      "Epoch: 1217 mean train loss:  2.49087534e-02, mean val. rec. loss:  2.33863894e-02\n",
      "Epoch: 1218 mean train loss:  2.49021892e-02, mean val. rec. loss:  2.33802546e-02\n",
      "Epoch: 1219 mean train loss:  2.48956231e-02, mean val. rec. loss:  2.33741357e-02\n",
      "Epoch: 1220 mean train loss:  2.48890645e-02, mean val. rec. loss:  2.33680076e-02\n",
      "Epoch: 1221 mean train loss:  2.48825114e-02, mean val. rec. loss:  2.33619000e-02\n",
      "Epoch: 1222 mean train loss:  2.48759714e-02, mean val. rec. loss:  2.33558038e-02\n",
      "Epoch: 1223 mean train loss:  2.48694426e-02, mean val. rec. loss:  2.33497075e-02\n",
      "Epoch: 1224 mean train loss:  2.48629156e-02, mean val. rec. loss:  2.33436203e-02\n",
      "Epoch: 1225 mean train loss:  2.48563998e-02, mean val. rec. loss:  2.33375377e-02\n",
      "Epoch: 1226 mean train loss:  2.48498877e-02, mean val. rec. loss:  2.33314664e-02\n",
      "Epoch: 1227 mean train loss:  2.48433887e-02, mean val. rec. loss:  2.33253973e-02\n",
      "Epoch: 1228 mean train loss:  2.48368915e-02, mean val. rec. loss:  2.33193351e-02\n",
      "Epoch: 1229 mean train loss:  2.48304036e-02, mean val. rec. loss:  2.33132819e-02\n",
      "Epoch: 1230 mean train loss:  2.48239232e-02, mean val. rec. loss:  2.33072469e-02\n",
      "Epoch: 1231 mean train loss:  2.48174577e-02, mean val. rec. loss:  2.33012051e-02\n",
      "Epoch: 1232 mean train loss:  2.48109977e-02, mean val. rec. loss:  2.32951746e-02\n",
      "Epoch: 1233 mean train loss:  2.48045415e-02, mean val. rec. loss:  2.32891486e-02\n",
      "Epoch: 1234 mean train loss:  2.47980946e-02, mean val. rec. loss:  2.32831386e-02\n",
      "Epoch: 1235 mean train loss:  2.47916589e-02, mean val. rec. loss:  2.32771376e-02\n",
      "Epoch: 1236 mean train loss:  2.47852269e-02, mean val. rec. loss:  2.32711298e-02\n",
      "Epoch: 1237 mean train loss:  2.47788060e-02, mean val. rec. loss:  2.32651310e-02\n",
      "Epoch: 1238 mean train loss:  2.47723889e-02, mean val. rec. loss:  2.32591482e-02\n",
      "Epoch: 1239 mean train loss:  2.47659830e-02, mean val. rec. loss:  2.32531698e-02\n",
      "Epoch: 1240 mean train loss:  2.47595864e-02, mean val. rec. loss:  2.32471915e-02\n",
      "Epoch: 1241 mean train loss:  2.47531935e-02, mean val. rec. loss:  2.32412268e-02\n",
      "Epoch: 1242 mean train loss:  2.47468061e-02, mean val. rec. loss:  2.32352711e-02\n",
      "Epoch: 1243 mean train loss:  2.47404337e-02, mean val. rec. loss:  2.32293246e-02\n",
      "Epoch: 1244 mean train loss:  2.47340669e-02, mean val. rec. loss:  2.32233780e-02\n",
      "Epoch: 1245 mean train loss:  2.47277075e-02, mean val. rec. loss:  2.32174405e-02\n",
      "Epoch: 1246 mean train loss:  2.47213575e-02, mean val. rec. loss:  2.32115121e-02\n",
      "Epoch: 1247 mean train loss:  2.47150111e-02, mean val. rec. loss:  2.32055882e-02\n",
      "Epoch: 1248 mean train loss:  2.47086722e-02, mean val. rec. loss:  2.31996688e-02\n",
      "Epoch: 1249 mean train loss:  2.47023408e-02, mean val. rec. loss:  2.31937585e-02\n",
      "Epoch: 1250 mean train loss:  2.46960168e-02, mean val. rec. loss:  2.31878528e-02\n",
      "Epoch: 1251 mean train loss:  2.46896965e-02, mean val. rec. loss:  2.31819538e-02\n",
      "Epoch: 1252 mean train loss:  2.46833893e-02, mean val. rec. loss:  2.31760617e-02\n",
      "Epoch: 1253 mean train loss:  2.46770895e-02, mean val. rec. loss:  2.31701809e-02\n",
      "Epoch: 1254 mean train loss:  2.46707990e-02, mean val. rec. loss:  2.31643024e-02\n",
      "Epoch: 1255 mean train loss:  2.46645085e-02, mean val. rec. loss:  2.31584284e-02\n",
      "Epoch: 1256 mean train loss:  2.46582236e-02, mean val. rec. loss:  2.31525680e-02\n",
      "Epoch: 1257 mean train loss:  2.46519592e-02, mean val. rec. loss:  2.31467053e-02\n",
      "Epoch: 1258 mean train loss:  2.46456873e-02, mean val. rec. loss:  2.31408608e-02\n",
      "Epoch: 1259 mean train loss:  2.46394360e-02, mean val. rec. loss:  2.31350117e-02\n",
      "Epoch: 1260 mean train loss:  2.46331790e-02, mean val. rec. loss:  2.31291808e-02\n",
      "Epoch: 1261 mean train loss:  2.46269444e-02, mean val. rec. loss:  2.31233522e-02\n",
      "Epoch: 1262 mean train loss:  2.46207061e-02, mean val. rec. loss:  2.31175213e-02\n",
      "Epoch: 1263 mean train loss:  2.46144752e-02, mean val. rec. loss:  2.31117108e-02\n",
      "Epoch: 1264 mean train loss:  2.46082629e-02, mean val. rec. loss:  2.31059003e-02\n",
      "Epoch: 1265 mean train loss:  2.46020506e-02, mean val. rec. loss:  2.31000921e-02\n",
      "Epoch: 1266 mean train loss:  2.45958384e-02, mean val. rec. loss:  2.30942929e-02\n",
      "Epoch: 1267 mean train loss:  2.45896373e-02, mean val. rec. loss:  2.30885051e-02\n",
      "Epoch: 1268 mean train loss:  2.45834492e-02, mean val. rec. loss:  2.30827150e-02\n",
      "Epoch: 1269 mean train loss:  2.45772612e-02, mean val. rec. loss:  2.30769408e-02\n",
      "Epoch: 1270 mean train loss:  2.45710843e-02, mean val. rec. loss:  2.30711666e-02\n",
      "Epoch: 1271 mean train loss:  2.45649148e-02, mean val. rec. loss:  2.30654015e-02\n",
      "Epoch: 1272 mean train loss:  2.45587510e-02, mean val. rec. loss:  2.30596409e-02\n",
      "Epoch: 1273 mean train loss:  2.45525946e-02, mean val. rec. loss:  2.30538893e-02\n",
      "Epoch: 1274 mean train loss:  2.45464456e-02, mean val. rec. loss:  2.30481469e-02\n",
      "Epoch: 1275 mean train loss:  2.45403078e-02, mean val. rec. loss:  2.30424089e-02\n",
      "Epoch: 1276 mean train loss:  2.45341719e-02, mean val. rec. loss:  2.30366688e-02\n",
      "Epoch: 1277 mean train loss:  2.45280379e-02, mean val. rec. loss:  2.30309535e-02\n",
      "Epoch: 1278 mean train loss:  2.45219206e-02, mean val. rec. loss:  2.30252269e-02\n",
      "Epoch: 1279 mean train loss:  2.45158089e-02, mean val. rec. loss:  2.30195049e-02\n",
      "Epoch: 1280 mean train loss:  2.45096934e-02, mean val. rec. loss:  2.30138010e-02\n",
      "Epoch: 1281 mean train loss:  2.45036003e-02, mean val. rec. loss:  2.30080948e-02\n",
      "Epoch: 1282 mean train loss:  2.44975017e-02, mean val. rec. loss:  2.30024023e-02\n",
      "Epoch: 1283 mean train loss:  2.44914198e-02, mean val. rec. loss:  2.29967074e-02\n",
      "Epoch: 1284 mean train loss:  2.44853323e-02, mean val. rec. loss:  2.29910285e-02\n",
      "Epoch: 1285 mean train loss:  2.44792634e-02, mean val. rec. loss:  2.29853427e-02\n",
      "Epoch: 1286 mean train loss:  2.44731889e-02, mean val. rec. loss:  2.29796751e-02\n",
      "Epoch: 1287 mean train loss:  2.44671312e-02, mean val. rec. loss:  2.29740097e-02\n",
      "Epoch: 1288 mean train loss:  2.44610809e-02, mean val. rec. loss:  2.29683557e-02\n",
      "Epoch: 1289 mean train loss:  2.44550307e-02, mean val. rec. loss:  2.29626949e-02\n",
      "Epoch: 1290 mean train loss:  2.44489860e-02, mean val. rec. loss:  2.29570477e-02\n",
      "Epoch: 1291 mean train loss:  2.44429562e-02, mean val. rec. loss:  2.29514096e-02\n",
      "Epoch: 1292 mean train loss:  2.44369339e-02, mean val. rec. loss:  2.29457760e-02\n",
      "Epoch: 1293 mean train loss:  2.44309153e-02, mean val. rec. loss:  2.29401469e-02\n",
      "Epoch: 1294 mean train loss:  2.44249023e-02, mean val. rec. loss:  2.29345201e-02\n",
      "Epoch: 1295 mean train loss:  2.44188930e-02, mean val. rec. loss:  2.29289047e-02\n",
      "Epoch: 1296 mean train loss:  2.44128930e-02, mean val. rec. loss:  2.29232938e-02\n",
      "Epoch: 1297 mean train loss:  2.44068968e-02, mean val. rec. loss:  2.29176897e-02\n",
      "Epoch: 1298 mean train loss:  2.44009117e-02, mean val. rec. loss:  2.29120856e-02\n",
      "Epoch: 1299 mean train loss:  2.43949285e-02, mean val. rec. loss:  2.29064905e-02\n",
      "Epoch: 1300 mean train loss:  2.43889509e-02, mean val. rec. loss:  2.29009113e-02\n",
      "Epoch: 1301 mean train loss:  2.43829881e-02, mean val. rec. loss:  2.28953231e-02\n",
      "Epoch: 1302 mean train loss:  2.43770310e-02, mean val. rec. loss:  2.28897462e-02\n",
      "Epoch: 1303 mean train loss:  2.43710738e-02, mean val. rec. loss:  2.28841716e-02\n",
      "Epoch: 1304 mean train loss:  2.43651204e-02, mean val. rec. loss:  2.28786060e-02\n",
      "Epoch: 1305 mean train loss:  2.43591800e-02, mean val. rec. loss:  2.28730473e-02\n",
      "Epoch: 1306 mean train loss:  2.43532452e-02, mean val. rec. loss:  2.28675044e-02\n",
      "Epoch: 1307 mean train loss:  2.43473179e-02, mean val. rec. loss:  2.28619479e-02\n",
      "Epoch: 1308 mean train loss:  2.43413924e-02, mean val. rec. loss:  2.28564005e-02\n",
      "Epoch: 1309 mean train loss:  2.43354762e-02, mean val. rec. loss:  2.28508712e-02\n",
      "Epoch: 1310 mean train loss:  2.43295656e-02, mean val. rec. loss:  2.28453420e-02\n",
      "Epoch: 1311 mean train loss:  2.43236587e-02, mean val. rec. loss:  2.28398082e-02\n",
      "Epoch: 1312 mean train loss:  2.43177593e-02, mean val. rec. loss:  2.28342902e-02\n",
      "Epoch: 1313 mean train loss:  2.43118729e-02, mean val. rec. loss:  2.28287814e-02\n",
      "Epoch: 1314 mean train loss:  2.43059903e-02, mean val. rec. loss:  2.28232657e-02\n",
      "Epoch: 1315 mean train loss:  2.43001076e-02, mean val. rec. loss:  2.28177614e-02\n",
      "Epoch: 1316 mean train loss:  2.42942305e-02, mean val. rec. loss:  2.28122661e-02\n",
      "Epoch: 1317 mean train loss:  2.42883646e-02, mean val. rec. loss:  2.28067777e-02\n",
      "Epoch: 1318 mean train loss:  2.42825099e-02, mean val. rec. loss:  2.28012892e-02\n",
      "Epoch: 1319 mean train loss:  2.42766552e-02, mean val. rec. loss:  2.27958099e-02\n",
      "Epoch: 1320 mean train loss:  2.42708079e-02, mean val. rec. loss:  2.27903328e-02\n",
      "Epoch: 1321 mean train loss:  2.42649681e-02, mean val. rec. loss:  2.27848625e-02\n",
      "Epoch: 1322 mean train loss:  2.42591301e-02, mean val. rec. loss:  2.27793990e-02\n",
      "Epoch: 1323 mean train loss:  2.42532996e-02, mean val. rec. loss:  2.27739377e-02\n",
      "Epoch: 1324 mean train loss:  2.42474747e-02, mean val. rec. loss:  2.27684856e-02\n",
      "Epoch: 1325 mean train loss:  2.42416553e-02, mean val. rec. loss:  2.27630334e-02\n",
      "Epoch: 1326 mean train loss:  2.42358397e-02, mean val. rec. loss:  2.27575858e-02\n",
      "Epoch: 1327 mean train loss:  2.42300408e-02, mean val. rec. loss:  2.27521586e-02\n",
      "Epoch: 1328 mean train loss:  2.42242420e-02, mean val. rec. loss:  2.27467268e-02\n",
      "Epoch: 1329 mean train loss:  2.42184506e-02, mean val. rec. loss:  2.27412973e-02\n",
      "Epoch: 1330 mean train loss:  2.42126573e-02, mean val. rec. loss:  2.27358837e-02\n",
      "Epoch: 1331 mean train loss:  2.42068789e-02, mean val. rec. loss:  2.27304611e-02\n",
      "Epoch: 1332 mean train loss:  2.42011024e-02, mean val. rec. loss:  2.27250497e-02\n",
      "Epoch: 1333 mean train loss:  2.41953371e-02, mean val. rec. loss:  2.27196497e-02\n",
      "Epoch: 1334 mean train loss:  2.41895736e-02, mean val. rec. loss:  2.27142543e-02\n",
      "Epoch: 1335 mean train loss:  2.41838157e-02, mean val. rec. loss:  2.27088520e-02\n",
      "Epoch: 1336 mean train loss:  2.41780634e-02, mean val. rec. loss:  2.27034565e-02\n",
      "Epoch: 1337 mean train loss:  2.41723148e-02, mean val. rec. loss:  2.26980815e-02\n",
      "Epoch: 1338 mean train loss:  2.41665774e-02, mean val. rec. loss:  2.26927019e-02\n",
      "Epoch: 1339 mean train loss:  2.41608456e-02, mean val. rec. loss:  2.26873201e-02\n",
      "Epoch: 1340 mean train loss:  2.41551175e-02, mean val. rec. loss:  2.26819541e-02\n",
      "Epoch: 1341 mean train loss:  2.41493894e-02, mean val. rec. loss:  2.26765972e-02\n",
      "Epoch: 1342 mean train loss:  2.41436799e-02, mean val. rec. loss:  2.26712403e-02\n",
      "Epoch: 1343 mean train loss:  2.41379667e-02, mean val. rec. loss:  2.26658879e-02\n",
      "Epoch: 1344 mean train loss:  2.41322609e-02, mean val. rec. loss:  2.26605310e-02\n",
      "Epoch: 1345 mean train loss:  2.41265589e-02, mean val. rec. loss:  2.26551877e-02\n",
      "Epoch: 1346 mean train loss:  2.41208643e-02, mean val. rec. loss:  2.26498489e-02\n",
      "Epoch: 1347 mean train loss:  2.41151791e-02, mean val. rec. loss:  2.26445147e-02\n",
      "Epoch: 1348 mean train loss:  2.41094957e-02, mean val. rec. loss:  2.26391895e-02\n",
      "Epoch: 1349 mean train loss:  2.41038178e-02, mean val. rec. loss:  2.26338644e-02\n",
      "Epoch: 1350 mean train loss:  2.40981493e-02, mean val. rec. loss:  2.26285506e-02\n",
      "Epoch: 1351 mean train loss:  2.40924864e-02, mean val. rec. loss:  2.26232345e-02\n",
      "Epoch: 1352 mean train loss:  2.40868253e-02, mean val. rec. loss:  2.26179298e-02\n",
      "Epoch: 1353 mean train loss:  2.40811773e-02, mean val. rec. loss:  2.26126341e-02\n",
      "Epoch: 1354 mean train loss:  2.40755312e-02, mean val. rec. loss:  2.26073339e-02\n",
      "Epoch: 1355 mean train loss:  2.40698850e-02, mean val. rec. loss:  2.26020359e-02\n",
      "Epoch: 1356 mean train loss:  2.40642444e-02, mean val. rec. loss:  2.25967561e-02\n",
      "Epoch: 1357 mean train loss:  2.40586150e-02, mean val. rec. loss:  2.25914673e-02\n",
      "Epoch: 1358 mean train loss:  2.40529931e-02, mean val. rec. loss:  2.25861875e-02\n",
      "Epoch: 1359 mean train loss:  2.40473674e-02, mean val. rec. loss:  2.25809213e-02\n",
      "Epoch: 1360 mean train loss:  2.40417529e-02, mean val. rec. loss:  2.25756460e-02\n",
      "Epoch: 1361 mean train loss:  2.40361458e-02, mean val. rec. loss:  2.25703753e-02\n",
      "Epoch: 1362 mean train loss:  2.40305406e-02, mean val. rec. loss:  2.25651227e-02\n",
      "Epoch: 1363 mean train loss:  2.40249429e-02, mean val. rec. loss:  2.25598747e-02\n",
      "Epoch: 1364 mean train loss:  2.40193507e-02, mean val. rec. loss:  2.25546198e-02\n",
      "Epoch: 1365 mean train loss:  2.40137623e-02, mean val. rec. loss:  2.25493695e-02\n",
      "Epoch: 1366 mean train loss:  2.40081757e-02, mean val. rec. loss:  2.25441306e-02\n",
      "Epoch: 1367 mean train loss:  2.40026003e-02, mean val. rec. loss:  2.25388961e-02\n",
      "Epoch: 1368 mean train loss:  2.39970324e-02, mean val. rec. loss:  2.25336708e-02\n",
      "Epoch: 1369 mean train loss:  2.39914626e-02, mean val. rec. loss:  2.25284341e-02\n",
      "Epoch: 1370 mean train loss:  2.39858965e-02, mean val. rec. loss:  2.25232155e-02\n",
      "Epoch: 1371 mean train loss:  2.39803472e-02, mean val. rec. loss:  2.25179992e-02\n",
      "Epoch: 1372 mean train loss:  2.39747997e-02, mean val. rec. loss:  2.25127829e-02\n",
      "Epoch: 1373 mean train loss:  2.39692485e-02, mean val. rec. loss:  2.25075757e-02\n",
      "Epoch: 1374 mean train loss:  2.39637029e-02, mean val. rec. loss:  2.25023730e-02\n",
      "Epoch: 1375 mean train loss:  2.39581722e-02, mean val. rec. loss:  2.24971703e-02\n",
      "Epoch: 1376 mean train loss:  2.39526415e-02, mean val. rec. loss:  2.24919790e-02\n",
      "Epoch: 1377 mean train loss:  2.39471182e-02, mean val. rec. loss:  2.24867876e-02\n",
      "Epoch: 1378 mean train loss:  2.39415987e-02, mean val. rec. loss:  2.24816076e-02\n",
      "Epoch: 1379 mean train loss:  2.39360866e-02, mean val. rec. loss:  2.24764276e-02\n",
      "Epoch: 1380 mean train loss:  2.39305783e-02, mean val. rec. loss:  2.24712476e-02\n",
      "Epoch: 1381 mean train loss:  2.39250718e-02, mean val. rec. loss:  2.24660789e-02\n",
      "Epoch: 1382 mean train loss:  2.39195746e-02, mean val. rec. loss:  2.24609080e-02\n",
      "Epoch: 1383 mean train loss:  2.39140867e-02, mean val. rec. loss:  2.24557598e-02\n",
      "Epoch: 1384 mean train loss:  2.39086044e-02, mean val. rec. loss:  2.24505979e-02\n",
      "Epoch: 1385 mean train loss:  2.39031184e-02, mean val. rec. loss:  2.24454451e-02\n",
      "Epoch: 1386 mean train loss:  2.38976380e-02, mean val. rec. loss:  2.24403037e-02\n",
      "Epoch: 1387 mean train loss:  2.38921687e-02, mean val. rec. loss:  2.24351577e-02\n",
      "Epoch: 1388 mean train loss:  2.38867050e-02, mean val. rec. loss:  2.24300140e-02\n",
      "Epoch: 1389 mean train loss:  2.38812414e-02, mean val. rec. loss:  2.24248725e-02\n",
      "Epoch: 1390 mean train loss:  2.38757852e-02, mean val. rec. loss:  2.24197469e-02\n",
      "Epoch: 1391 mean train loss:  2.38703308e-02, mean val. rec. loss:  2.24146259e-02\n",
      "Epoch: 1392 mean train loss:  2.38648839e-02, mean val. rec. loss:  2.24095003e-02\n",
      "Epoch: 1393 mean train loss:  2.38594444e-02, mean val. rec. loss:  2.24043793e-02\n",
      "Epoch: 1394 mean train loss:  2.38540087e-02, mean val. rec. loss:  2.23992696e-02\n",
      "Epoch: 1395 mean train loss:  2.38485748e-02, mean val. rec. loss:  2.23941644e-02\n",
      "Epoch: 1396 mean train loss:  2.38431484e-02, mean val. rec. loss:  2.23890570e-02\n",
      "Epoch: 1397 mean train loss:  2.38377257e-02, mean val. rec. loss:  2.23839586e-02\n",
      "Epoch: 1398 mean train loss:  2.38323105e-02, mean val. rec. loss:  2.23788648e-02\n",
      "Epoch: 1399 mean train loss:  2.38268971e-02, mean val. rec. loss:  2.23737596e-02\n",
      "Epoch: 1400 mean train loss:  2.38214837e-02, mean val. rec. loss:  2.23686772e-02\n",
      "Epoch: 1401 mean train loss:  2.38160852e-02, mean val. rec. loss:  2.23635947e-02\n",
      "Epoch: 1402 mean train loss:  2.38106923e-02, mean val. rec. loss:  2.23585031e-02\n",
      "Epoch: 1403 mean train loss:  2.38052919e-02, mean val. rec. loss:  2.23534297e-02\n",
      "Epoch: 1404 mean train loss:  2.37998990e-02, mean val. rec. loss:  2.23483608e-02\n",
      "Epoch: 1405 mean train loss:  2.37945192e-02, mean val. rec. loss:  2.23432942e-02\n",
      "Epoch: 1406 mean train loss:  2.37891412e-02, mean val. rec. loss:  2.23382322e-02\n",
      "Epoch: 1407 mean train loss:  2.37837687e-02, mean val. rec. loss:  2.23331656e-02\n",
      "Epoch: 1408 mean train loss:  2.37783963e-02, mean val. rec. loss:  2.23281126e-02\n",
      "Epoch: 1409 mean train loss:  2.37730332e-02, mean val. rec. loss:  2.23230596e-02\n",
      "Epoch: 1410 mean train loss:  2.37676738e-02, mean val. rec. loss:  2.23180111e-02\n",
      "Epoch: 1411 mean train loss:  2.37623200e-02, mean val. rec. loss:  2.23129717e-02\n",
      "Epoch: 1412 mean train loss:  2.37569662e-02, mean val. rec. loss:  2.23079346e-02\n",
      "Epoch: 1413 mean train loss:  2.37516217e-02, mean val. rec. loss:  2.23028906e-02\n",
      "Epoch: 1414 mean train loss:  2.37462773e-02, mean val. rec. loss:  2.22978581e-02\n",
      "Epoch: 1415 mean train loss:  2.37409439e-02, mean val. rec. loss:  2.22928368e-02\n",
      "Epoch: 1416 mean train loss:  2.37356181e-02, mean val. rec. loss:  2.22878133e-02\n",
      "Epoch: 1417 mean train loss:  2.37302866e-02, mean val. rec. loss:  2.22827898e-02\n",
      "Epoch: 1418 mean train loss:  2.37249608e-02, mean val. rec. loss:  2.22777821e-02\n",
      "Epoch: 1419 mean train loss:  2.37196461e-02, mean val. rec. loss:  2.22727677e-02\n",
      "Epoch: 1420 mean train loss:  2.37143370e-02, mean val. rec. loss:  2.22677578e-02\n",
      "Epoch: 1421 mean train loss:  2.37090223e-02, mean val. rec. loss:  2.22627569e-02\n",
      "Epoch: 1422 mean train loss:  2.37037188e-02, mean val. rec. loss:  2.22577561e-02\n",
      "Epoch: 1423 mean train loss:  2.36984208e-02, mean val. rec. loss:  2.22527507e-02\n",
      "Epoch: 1424 mean train loss:  2.36931266e-02, mean val. rec. loss:  2.22477658e-02\n",
      "Epoch: 1425 mean train loss:  2.36878361e-02, mean val. rec. loss:  2.22427831e-02\n",
      "Epoch: 1426 mean train loss:  2.36825512e-02, mean val. rec. loss:  2.22377936e-02\n",
      "Epoch: 1427 mean train loss:  2.36772682e-02, mean val. rec. loss:  2.22328086e-02\n",
      "Epoch: 1428 mean train loss:  2.36719926e-02, mean val. rec. loss:  2.22278418e-02\n",
      "Epoch: 1429 mean train loss:  2.36667245e-02, mean val. rec. loss:  2.22228659e-02\n",
      "Epoch: 1430 mean train loss:  2.36614582e-02, mean val. rec. loss:  2.22179059e-02\n",
      "Epoch: 1431 mean train loss:  2.36561919e-02, mean val. rec. loss:  2.22129300e-02\n",
      "Epoch: 1432 mean train loss:  2.36509331e-02, mean val. rec. loss:  2.22079677e-02\n",
      "Epoch: 1433 mean train loss:  2.36456817e-02, mean val. rec. loss:  2.22030145e-02\n",
      "Epoch: 1434 mean train loss:  2.36404378e-02, mean val. rec. loss:  2.21980545e-02\n",
      "Epoch: 1435 mean train loss:  2.36351864e-02, mean val. rec. loss:  2.21931058e-02\n",
      "Epoch: 1436 mean train loss:  2.36299425e-02, mean val. rec. loss:  2.21881640e-02\n",
      "Epoch: 1437 mean train loss:  2.36247097e-02, mean val. rec. loss:  2.21832221e-02\n",
      "Epoch: 1438 mean train loss:  2.36194788e-02, mean val. rec. loss:  2.21782825e-02\n",
      "Epoch: 1439 mean train loss:  2.36142517e-02, mean val. rec. loss:  2.21733429e-02\n",
      "Epoch: 1440 mean train loss:  2.36090301e-02, mean val. rec. loss:  2.21684169e-02\n",
      "Epoch: 1441 mean train loss:  2.36038122e-02, mean val. rec. loss:  2.21634909e-02\n",
      "Epoch: 1442 mean train loss:  2.35985981e-02, mean val. rec. loss:  2.21585649e-02\n",
      "Epoch: 1443 mean train loss:  2.35933877e-02, mean val. rec. loss:  2.21536457e-02\n",
      "Epoch: 1444 mean train loss:  2.35881829e-02, mean val. rec. loss:  2.21487265e-02\n",
      "Epoch: 1445 mean train loss:  2.35829762e-02, mean val. rec. loss:  2.21438141e-02\n",
      "Epoch: 1446 mean train loss:  2.35777807e-02, mean val. rec. loss:  2.21389018e-02\n",
      "Epoch: 1447 mean train loss:  2.35725870e-02, mean val. rec. loss:  2.21339939e-02\n",
      "Epoch: 1448 mean train loss:  2.35674027e-02, mean val. rec. loss:  2.21290997e-02\n",
      "Epoch: 1449 mean train loss:  2.35622221e-02, mean val. rec. loss:  2.21242009e-02\n",
      "Epoch: 1450 mean train loss:  2.35570396e-02, mean val. rec. loss:  2.21192976e-02\n",
      "Epoch: 1451 mean train loss:  2.35518571e-02, mean val. rec. loss:  2.21144124e-02\n",
      "Epoch: 1452 mean train loss:  2.35466914e-02, mean val. rec. loss:  2.21095227e-02\n",
      "Epoch: 1453 mean train loss:  2.35415275e-02, mean val. rec. loss:  2.21046330e-02\n",
      "Epoch: 1454 mean train loss:  2.35363562e-02, mean val. rec. loss:  2.20997501e-02\n",
      "Epoch: 1455 mean train loss:  2.35311943e-02, mean val. rec. loss:  2.20948785e-02\n",
      "Epoch: 1456 mean train loss:  2.35260472e-02, mean val. rec. loss:  2.20899956e-02\n",
      "Epoch: 1457 mean train loss:  2.35208889e-02, mean val. rec. loss:  2.20851286e-02\n",
      "Epoch: 1458 mean train loss:  2.35157511e-02, mean val. rec. loss:  2.20802525e-02\n",
      "Epoch: 1459 mean train loss:  2.35106003e-02, mean val. rec. loss:  2.20753945e-02\n",
      "Epoch: 1460 mean train loss:  2.35054681e-02, mean val. rec. loss:  2.20705298e-02\n",
      "Epoch: 1461 mean train loss:  2.35003266e-02, mean val. rec. loss:  2.20656786e-02\n",
      "Epoch: 1462 mean train loss:  2.34952018e-02, mean val. rec. loss:  2.20608207e-02\n",
      "Epoch: 1463 mean train loss:  2.34900678e-02, mean val. rec. loss:  2.20559718e-02\n",
      "Epoch: 1464 mean train loss:  2.34849523e-02, mean val. rec. loss:  2.20511274e-02\n",
      "Epoch: 1465 mean train loss:  2.34798369e-02, mean val. rec. loss:  2.20462740e-02\n",
      "Epoch: 1466 mean train loss:  2.34747215e-02, mean val. rec. loss:  2.20414387e-02\n",
      "Epoch: 1467 mean train loss:  2.34696079e-02, mean val. rec. loss:  2.20366012e-02\n",
      "Epoch: 1468 mean train loss:  2.34645055e-02, mean val. rec. loss:  2.20317727e-02\n",
      "Epoch: 1469 mean train loss:  2.34594068e-02, mean val. rec. loss:  2.20269420e-02\n",
      "Epoch: 1470 mean train loss:  2.34543044e-02, mean val. rec. loss:  2.20221067e-02\n",
      "Epoch: 1471 mean train loss:  2.34492076e-02, mean val. rec. loss:  2.20172850e-02\n",
      "Epoch: 1472 mean train loss:  2.34441182e-02, mean val. rec. loss:  2.20124679e-02\n",
      "Epoch: 1473 mean train loss:  2.34390344e-02, mean val. rec. loss:  2.20076508e-02\n",
      "Epoch: 1474 mean train loss:  2.34339544e-02, mean val. rec. loss:  2.20028314e-02\n",
      "Epoch: 1475 mean train loss:  2.34288762e-02, mean val. rec. loss:  2.19980210e-02\n",
      "Epoch: 1476 mean train loss:  2.34238017e-02, mean val. rec. loss:  2.19932130e-02\n",
      "Epoch: 1477 mean train loss:  2.34187310e-02, mean val. rec. loss:  2.19884095e-02\n",
      "Epoch: 1478 mean train loss:  2.34136621e-02, mean val. rec. loss:  2.19836059e-02\n",
      "Epoch: 1479 mean train loss:  2.34086025e-02, mean val. rec. loss:  2.19788092e-02\n",
      "Epoch: 1480 mean train loss:  2.34035411e-02, mean val. rec. loss:  2.19740125e-02\n",
      "Epoch: 1481 mean train loss:  2.33984871e-02, mean val. rec. loss:  2.19692158e-02\n",
      "Epoch: 1482 mean train loss:  2.33934313e-02, mean val. rec. loss:  2.19644258e-02\n",
      "Epoch: 1483 mean train loss:  2.33883903e-02, mean val. rec. loss:  2.19596473e-02\n",
      "Epoch: 1484 mean train loss:  2.33833512e-02, mean val. rec. loss:  2.19548619e-02\n",
      "Epoch: 1485 mean train loss:  2.33783066e-02, mean val. rec. loss:  2.19500810e-02\n",
      "Epoch: 1486 mean train loss:  2.33732693e-02, mean val. rec. loss:  2.19453115e-02\n",
      "Epoch: 1487 mean train loss:  2.33682414e-02, mean val. rec. loss:  2.19405330e-02\n",
      "Epoch: 1488 mean train loss:  2.33632135e-02, mean val. rec. loss:  2.19357612e-02\n",
      "Epoch: 1489 mean train loss:  2.33581856e-02, mean val. rec. loss:  2.19309917e-02\n",
      "Epoch: 1490 mean train loss:  2.33531633e-02, mean val. rec. loss:  2.19262312e-02\n",
      "Epoch: 1491 mean train loss:  2.33481502e-02, mean val. rec. loss:  2.19214754e-02\n",
      "Epoch: 1492 mean train loss:  2.33431391e-02, mean val. rec. loss:  2.19167240e-02\n",
      "Epoch: 1493 mean train loss:  2.33381335e-02, mean val. rec. loss:  2.19119636e-02\n",
      "Epoch: 1494 mean train loss:  2.33331279e-02, mean val. rec. loss:  2.19072099e-02\n",
      "Epoch: 1495 mean train loss:  2.33281261e-02, mean val. rec. loss:  2.19024676e-02\n",
      "Epoch: 1496 mean train loss:  2.33231317e-02, mean val. rec. loss:  2.18977276e-02\n",
      "Epoch: 1497 mean train loss:  2.33181392e-02, mean val. rec. loss:  2.18929785e-02\n",
      "Epoch: 1498 mean train loss:  2.33131485e-02, mean val. rec. loss:  2.18882385e-02\n",
      "Epoch: 1499 mean train loss:  2.33081616e-02, mean val. rec. loss:  2.18835075e-02\n",
      "Epoch: 1500 mean train loss:  2.33031783e-02, mean val. rec. loss:  2.18787766e-02\n",
      "Epoch: 1501 mean train loss:  2.32982044e-02, mean val. rec. loss:  2.18740502e-02\n",
      "Epoch: 1502 mean train loss:  2.32932287e-02, mean val. rec. loss:  2.18693124e-02\n",
      "Epoch: 1503 mean train loss:  2.32882492e-02, mean val. rec. loss:  2.18645928e-02\n",
      "Epoch: 1504 mean train loss:  2.32832864e-02, mean val. rec. loss:  2.18598709e-02\n",
      "Epoch: 1505 mean train loss:  2.32783237e-02, mean val. rec. loss:  2.18551423e-02\n",
      "Epoch: 1506 mean train loss:  2.32733609e-02, mean val. rec. loss:  2.18504294e-02\n",
      "Epoch: 1507 mean train loss:  2.32683982e-02, mean val. rec. loss:  2.18457189e-02\n",
      "Epoch: 1508 mean train loss:  2.32634485e-02, mean val. rec. loss:  2.18410106e-02\n",
      "Epoch: 1509 mean train loss:  2.32585007e-02, mean val. rec. loss:  2.18363069e-02\n",
      "Epoch: 1510 mean train loss:  2.32535491e-02, mean val. rec. loss:  2.18316032e-02\n",
      "Epoch: 1511 mean train loss:  2.32486068e-02, mean val. rec. loss:  2.18268994e-02\n",
      "Epoch: 1512 mean train loss:  2.32436646e-02, mean val. rec. loss:  2.18222002e-02\n",
      "Epoch: 1513 mean train loss:  2.32387298e-02, mean val. rec. loss:  2.18175010e-02\n",
      "Epoch: 1514 mean train loss:  2.32337987e-02, mean val. rec. loss:  2.18128064e-02\n",
      "Epoch: 1515 mean train loss:  2.32288676e-02, mean val. rec. loss:  2.18081162e-02\n",
      "Epoch: 1516 mean train loss:  2.32239421e-02, mean val. rec. loss:  2.18034307e-02\n",
      "Epoch: 1517 mean train loss:  2.32190204e-02, mean val. rec. loss:  2.17987428e-02\n",
      "Epoch: 1518 mean train loss:  2.32141005e-02, mean val. rec. loss:  2.17940640e-02\n",
      "Epoch: 1519 mean train loss:  2.32091843e-02, mean val. rec. loss:  2.17893852e-02\n",
      "Epoch: 1520 mean train loss:  2.32042699e-02, mean val. rec. loss:  2.17847019e-02\n",
      "Epoch: 1521 mean train loss:  2.31993668e-02, mean val. rec. loss:  2.17800254e-02\n",
      "Epoch: 1522 mean train loss:  2.31944581e-02, mean val. rec. loss:  2.17753489e-02\n",
      "Epoch: 1523 mean train loss:  2.31895512e-02, mean val. rec. loss:  2.17706769e-02\n",
      "Epoch: 1524 mean train loss:  2.31846555e-02, mean val. rec. loss:  2.17660230e-02\n",
      "Epoch: 1525 mean train loss:  2.31797654e-02, mean val. rec. loss:  2.17613556e-02\n",
      "Epoch: 1526 mean train loss:  2.31748678e-02, mean val. rec. loss:  2.17566904e-02\n",
      "Epoch: 1527 mean train loss:  2.31699777e-02, mean val. rec. loss:  2.17520411e-02\n",
      "Epoch: 1528 mean train loss:  2.31650950e-02, mean val. rec. loss:  2.17473805e-02\n",
      "Epoch: 1529 mean train loss:  2.31602161e-02, mean val. rec. loss:  2.17427221e-02\n",
      "Epoch: 1530 mean train loss:  2.31553334e-02, mean val. rec. loss:  2.17380682e-02\n",
      "Epoch: 1531 mean train loss:  2.31504582e-02, mean val. rec. loss:  2.17334257e-02\n",
      "Epoch: 1532 mean train loss:  2.31455867e-02, mean val. rec. loss:  2.17287878e-02\n",
      "Epoch: 1533 mean train loss:  2.31407190e-02, mean val. rec. loss:  2.17241385e-02\n",
      "Epoch: 1534 mean train loss:  2.31358549e-02, mean val. rec. loss:  2.17194937e-02\n",
      "Epoch: 1535 mean train loss:  2.31309946e-02, mean val. rec. loss:  2.17148625e-02\n",
      "Epoch: 1536 mean train loss:  2.31261343e-02, mean val. rec. loss:  2.17102314e-02\n",
      "Epoch: 1537 mean train loss:  2.31212777e-02, mean val. rec. loss:  2.17055957e-02\n",
      "Epoch: 1538 mean train loss:  2.31164248e-02, mean val. rec. loss:  2.17009600e-02\n",
      "Epoch: 1539 mean train loss:  2.31115757e-02, mean val. rec. loss:  2.16963379e-02\n",
      "Epoch: 1540 mean train loss:  2.31067303e-02, mean val. rec. loss:  2.16917181e-02\n",
      "Epoch: 1541 mean train loss:  2.31018886e-02, mean val. rec. loss:  2.16870892e-02\n",
      "Epoch: 1542 mean train loss:  2.30970469e-02, mean val. rec. loss:  2.16824739e-02\n",
      "Epoch: 1543 mean train loss:  2.30922126e-02, mean val. rec. loss:  2.16778564e-02\n",
      "Epoch: 1544 mean train loss:  2.30873858e-02, mean val. rec. loss:  2.16732365e-02\n",
      "Epoch: 1545 mean train loss:  2.30825535e-02, mean val. rec. loss:  2.16686258e-02\n",
      "Epoch: 1546 mean train loss:  2.30777229e-02, mean val. rec. loss:  2.16640196e-02\n",
      "Epoch: 1547 mean train loss:  2.30729054e-02, mean val. rec. loss:  2.16594156e-02\n",
      "Epoch: 1548 mean train loss:  2.30680880e-02, mean val. rec. loss:  2.16548117e-02\n",
      "Epoch: 1549 mean train loss:  2.30632667e-02, mean val. rec. loss:  2.16502032e-02\n",
      "Epoch: 1550 mean train loss:  2.30584511e-02, mean val. rec. loss:  2.16456083e-02\n",
      "Epoch: 1551 mean train loss:  2.30536429e-02, mean val. rec. loss:  2.16410135e-02\n",
      "Epoch: 1552 mean train loss:  2.30488366e-02, mean val. rec. loss:  2.16364140e-02\n",
      "Epoch: 1553 mean train loss:  2.30440303e-02, mean val. rec. loss:  2.16318237e-02\n",
      "Epoch: 1554 mean train loss:  2.30392315e-02, mean val. rec. loss:  2.16272311e-02\n",
      "Epoch: 1555 mean train loss:  2.30344326e-02, mean val. rec. loss:  2.16226408e-02\n",
      "Epoch: 1556 mean train loss:  2.30296374e-02, mean val. rec. loss:  2.16180618e-02\n",
      "Epoch: 1557 mean train loss:  2.30248460e-02, mean val. rec. loss:  2.16134714e-02\n",
      "Epoch: 1558 mean train loss:  2.30200546e-02, mean val. rec. loss:  2.16088947e-02\n",
      "Epoch: 1559 mean train loss:  2.30152706e-02, mean val. rec. loss:  2.16043134e-02\n",
      "Epoch: 1560 mean train loss:  2.30104867e-02, mean val. rec. loss:  2.15997367e-02\n",
      "Epoch: 1561 mean train loss:  2.30057046e-02, mean val. rec. loss:  2.15951600e-02\n",
      "Epoch: 1562 mean train loss:  2.30009281e-02, mean val. rec. loss:  2.15905832e-02\n",
      "Epoch: 1563 mean train loss:  2.29961515e-02, mean val. rec. loss:  2.15860156e-02\n",
      "Epoch: 1564 mean train loss:  2.29913825e-02, mean val. rec. loss:  2.15814525e-02\n",
      "Epoch: 1565 mean train loss:  2.29866190e-02, mean val. rec. loss:  2.15768848e-02\n",
      "Epoch: 1566 mean train loss:  2.29818518e-02, mean val. rec. loss:  2.15723194e-02\n",
      "Epoch: 1567 mean train loss:  2.29770846e-02, mean val. rec. loss:  2.15677676e-02\n",
      "Epoch: 1568 mean train loss:  2.29723285e-02, mean val. rec. loss:  2.15632045e-02\n",
      "Epoch: 1569 mean train loss:  2.29675762e-02, mean val. rec. loss:  2.15586459e-02\n",
      "Epoch: 1570 mean train loss:  2.29628165e-02, mean val. rec. loss:  2.15540873e-02\n",
      "Epoch: 1571 mean train loss:  2.29580642e-02, mean val. rec. loss:  2.15495333e-02\n",
      "Epoch: 1572 mean train loss:  2.29533174e-02, mean val. rec. loss:  2.15449929e-02\n",
      "Epoch: 1573 mean train loss:  2.29485763e-02, mean val. rec. loss:  2.15404433e-02\n",
      "Epoch: 1574 mean train loss:  2.29438296e-02, mean val. rec. loss:  2.15358893e-02\n",
      "Epoch: 1575 mean train loss:  2.29390922e-02, mean val. rec. loss:  2.15313534e-02\n",
      "Epoch: 1576 mean train loss:  2.29343547e-02, mean val. rec. loss:  2.15268175e-02\n",
      "Epoch: 1577 mean train loss:  2.29296229e-02, mean val. rec. loss:  2.15222748e-02\n",
      "Epoch: 1578 mean train loss:  2.29248930e-02, mean val. rec. loss:  2.15177321e-02\n",
      "Epoch: 1579 mean train loss:  2.29201649e-02, mean val. rec. loss:  2.15132052e-02\n",
      "Epoch: 1580 mean train loss:  2.29154423e-02, mean val. rec. loss:  2.15086761e-02\n",
      "Epoch: 1581 mean train loss:  2.29107198e-02, mean val. rec. loss:  2.15041425e-02\n",
      "Epoch: 1582 mean train loss:  2.29060010e-02, mean val. rec. loss:  2.14996066e-02\n",
      "Epoch: 1583 mean train loss:  2.29012822e-02, mean val. rec. loss:  2.14950888e-02\n",
      "Epoch: 1584 mean train loss:  2.28965709e-02, mean val. rec. loss:  2.14905688e-02\n",
      "Epoch: 1585 mean train loss:  2.28918558e-02, mean val. rec. loss:  2.14860397e-02\n",
      "Epoch: 1586 mean train loss:  2.28871482e-02, mean val. rec. loss:  2.14815219e-02\n",
      "Epoch: 1587 mean train loss:  2.28824480e-02, mean val. rec. loss:  2.14770087e-02\n",
      "Epoch: 1588 mean train loss:  2.28777497e-02, mean val. rec. loss:  2.14724864e-02\n",
      "Epoch: 1589 mean train loss:  2.28730477e-02, mean val. rec. loss:  2.14679777e-02\n",
      "Epoch: 1590 mean train loss:  2.28683494e-02, mean val. rec. loss:  2.14634690e-02\n",
      "Epoch: 1591 mean train loss:  2.28636604e-02, mean val. rec. loss:  2.14589649e-02\n",
      "Epoch: 1592 mean train loss:  2.28589770e-02, mean val. rec. loss:  2.14544607e-02\n",
      "Epoch: 1593 mean train loss:  2.28542843e-02, mean val. rec. loss:  2.14499520e-02\n",
      "Epoch: 1594 mean train loss:  2.28495953e-02, mean val. rec. loss:  2.14454524e-02\n",
      "Epoch: 1595 mean train loss:  2.28449156e-02, mean val. rec. loss:  2.14409528e-02\n",
      "Epoch: 1596 mean train loss:  2.28402396e-02, mean val. rec. loss:  2.14364486e-02\n",
      "Epoch: 1597 mean train loss:  2.28355600e-02, mean val. rec. loss:  2.14319490e-02\n",
      "Epoch: 1598 mean train loss:  2.28308859e-02, mean val. rec. loss:  2.14274584e-02\n",
      "Epoch: 1599 mean train loss:  2.28262155e-02, mean val. rec. loss:  2.14229656e-02\n",
      "Epoch: 1600 mean train loss:  2.28215488e-02, mean val. rec. loss:  2.14184728e-02\n",
      "Epoch: 1601 mean train loss:  2.28168822e-02, mean val. rec. loss:  2.14139868e-02\n",
      "Epoch: 1602 mean train loss:  2.28122174e-02, mean val. rec. loss:  2.14095008e-02\n",
      "Epoch: 1603 mean train loss:  2.28075601e-02, mean val. rec. loss:  2.14050148e-02\n",
      "Epoch: 1604 mean train loss:  2.28029027e-02, mean val. rec. loss:  2.14005378e-02\n",
      "Epoch: 1605 mean train loss:  2.27982491e-02, mean val. rec. loss:  2.13960586e-02\n",
      "Epoch: 1606 mean train loss:  2.27935955e-02, mean val. rec. loss:  2.13915749e-02\n",
      "Epoch: 1607 mean train loss:  2.27889437e-02, mean val. rec. loss:  2.13871025e-02\n",
      "Epoch: 1608 mean train loss:  2.27842957e-02, mean val. rec. loss:  2.13826255e-02\n",
      "Epoch: 1609 mean train loss:  2.27796495e-02, mean val. rec. loss:  2.13781509e-02\n",
      "Epoch: 1610 mean train loss:  2.27750071e-02, mean val. rec. loss:  2.13736898e-02\n",
      "Epoch: 1611 mean train loss:  2.27703702e-02, mean val. rec. loss:  2.13692151e-02\n",
      "Epoch: 1612 mean train loss:  2.27657297e-02, mean val. rec. loss:  2.13647427e-02\n",
      "Epoch: 1613 mean train loss:  2.27610928e-02, mean val. rec. loss:  2.13602862e-02\n",
      "Epoch: 1614 mean train loss:  2.27564615e-02, mean val. rec. loss:  2.13558183e-02\n",
      "Epoch: 1615 mean train loss:  2.27518359e-02, mean val. rec. loss:  2.13513550e-02\n",
      "Epoch: 1616 mean train loss:  2.27472046e-02, mean val. rec. loss:  2.13468917e-02\n",
      "Epoch: 1617 mean train loss:  2.27425752e-02, mean val. rec. loss:  2.13424261e-02\n",
      "Epoch: 1618 mean train loss:  2.27379551e-02, mean val. rec. loss:  2.13379786e-02\n",
      "Epoch: 1619 mean train loss:  2.27333387e-02, mean val. rec. loss:  2.13335198e-02\n",
      "Epoch: 1620 mean train loss:  2.27287186e-02, mean val. rec. loss:  2.13290678e-02\n",
      "Epoch: 1621 mean train loss:  2.27240985e-02, mean val. rec. loss:  2.13246249e-02\n",
      "Epoch: 1622 mean train loss:  2.27194896e-02, mean val. rec. loss:  2.13201729e-02\n",
      "Epoch: 1623 mean train loss:  2.27148844e-02, mean val. rec. loss:  2.13157209e-02\n",
      "Epoch: 1624 mean train loss:  2.27102699e-02, mean val. rec. loss:  2.13112803e-02\n",
      "Epoch: 1625 mean train loss:  2.27056722e-02, mean val. rec. loss:  2.13068306e-02\n",
      "Epoch: 1626 mean train loss:  2.27010632e-02, mean val. rec. loss:  2.13023922e-02\n",
      "Epoch: 1627 mean train loss:  2.26964692e-02, mean val. rec. loss:  2.12979447e-02\n",
      "Epoch: 1628 mean train loss:  2.26918640e-02, mean val. rec. loss:  2.12935086e-02\n",
      "Epoch: 1629 mean train loss:  2.26872793e-02, mean val. rec. loss:  2.12890679e-02\n",
      "Epoch: 1630 mean train loss:  2.26826778e-02, mean val. rec. loss:  2.12846409e-02\n",
      "Epoch: 1631 mean train loss:  2.26780950e-02, mean val. rec. loss:  2.12801980e-02\n",
      "Epoch: 1632 mean train loss:  2.26734991e-02, mean val. rec. loss:  2.12757709e-02\n",
      "Epoch: 1633 mean train loss:  2.26689200e-02, mean val. rec. loss:  2.12713348e-02\n",
      "Epoch: 1634 mean train loss:  2.26643297e-02, mean val. rec. loss:  2.12669100e-02\n",
      "Epoch: 1635 mean train loss:  2.26597524e-02, mean val. rec. loss:  2.12624807e-02\n",
      "Epoch: 1636 mean train loss:  2.26551677e-02, mean val. rec. loss:  2.12580605e-02\n",
      "Epoch: 1637 mean train loss:  2.26505979e-02, mean val. rec. loss:  2.12536380e-02\n",
      "Epoch: 1638 mean train loss:  2.26460206e-02, mean val. rec. loss:  2.12492087e-02\n",
      "Epoch: 1639 mean train loss:  2.26414471e-02, mean val. rec. loss:  2.12447907e-02\n",
      "Epoch: 1640 mean train loss:  2.26368810e-02, mean val. rec. loss:  2.12403750e-02\n",
      "Epoch: 1641 mean train loss:  2.26323186e-02, mean val. rec. loss:  2.12359502e-02\n",
      "Epoch: 1642 mean train loss:  2.26277525e-02, mean val. rec. loss:  2.12315390e-02\n",
      "Epoch: 1643 mean train loss:  2.26231864e-02, mean val. rec. loss:  2.12271301e-02\n",
      "Epoch: 1644 mean train loss:  2.26186315e-02, mean val. rec. loss:  2.12227190e-02\n",
      "Epoch: 1645 mean train loss:  2.26140747e-02, mean val. rec. loss:  2.12183123e-02\n",
      "Epoch: 1646 mean train loss:  2.26095180e-02, mean val. rec. loss:  2.12138966e-02\n",
      "Epoch: 1647 mean train loss:  2.26049612e-02, mean val. rec. loss:  2.12094923e-02\n",
      "Epoch: 1648 mean train loss:  2.26004156e-02, mean val. rec. loss:  2.12050902e-02\n",
      "Epoch: 1649 mean train loss:  2.25958718e-02, mean val. rec. loss:  2.12006813e-02\n",
      "Epoch: 1650 mean train loss:  2.25913206e-02, mean val. rec. loss:  2.11962814e-02\n",
      "Epoch: 1651 mean train loss:  2.25867769e-02, mean val. rec. loss:  2.11918816e-02\n",
      "Epoch: 1652 mean train loss:  2.25822350e-02, mean val. rec. loss:  2.11874818e-02\n",
      "Epoch: 1653 mean train loss:  2.25776968e-02, mean val. rec. loss:  2.11830842e-02\n",
      "Epoch: 1654 mean train loss:  2.25731605e-02, mean val. rec. loss:  2.11786889e-02\n",
      "Epoch: 1655 mean train loss:  2.25686280e-02, mean val. rec. loss:  2.11742982e-02\n",
      "Epoch: 1656 mean train loss:  2.25640991e-02, mean val. rec. loss:  2.11699051e-02\n",
      "Epoch: 1657 mean train loss:  2.25595666e-02, mean val. rec. loss:  2.11655144e-02\n",
      "Epoch: 1658 mean train loss:  2.25550414e-02, mean val. rec. loss:  2.11611236e-02\n",
      "Epoch: 1659 mean train loss:  2.25505126e-02, mean val. rec. loss:  2.11567329e-02\n",
      "Epoch: 1660 mean train loss:  2.25459893e-02, mean val. rec. loss:  2.11523489e-02\n",
      "Epoch: 1661 mean train loss:  2.25414679e-02, mean val. rec. loss:  2.11479650e-02\n",
      "Epoch: 1662 mean train loss:  2.25369484e-02, mean val. rec. loss:  2.11435787e-02\n",
      "Epoch: 1663 mean train loss:  2.25324307e-02, mean val. rec. loss:  2.11391948e-02\n",
      "Epoch: 1664 mean train loss:  2.25279168e-02, mean val. rec. loss:  2.11348199e-02\n",
      "Epoch: 1665 mean train loss:  2.25234028e-02, mean val. rec. loss:  2.11304428e-02\n",
      "Epoch: 1666 mean train loss:  2.25188963e-02, mean val. rec. loss:  2.11260701e-02\n",
      "Epoch: 1667 mean train loss:  2.25143880e-02, mean val. rec. loss:  2.11216885e-02\n",
      "Epoch: 1668 mean train loss:  2.25098870e-02, mean val. rec. loss:  2.11173113e-02\n",
      "Epoch: 1669 mean train loss:  2.25053787e-02, mean val. rec. loss:  2.11129342e-02\n",
      "Epoch: 1670 mean train loss:  2.25008740e-02, mean val. rec. loss:  2.11085615e-02\n",
      "Epoch: 1671 mean train loss:  2.24963768e-02, mean val. rec. loss:  2.11041935e-02\n",
      "Epoch: 1672 mean train loss:  2.24918815e-02, mean val. rec. loss:  2.10998254e-02\n",
      "Epoch: 1673 mean train loss:  2.24873843e-02, mean val. rec. loss:  2.10954528e-02\n",
      "Epoch: 1674 mean train loss:  2.24828871e-02, mean val. rec. loss:  2.10910938e-02\n",
      "Epoch: 1675 mean train loss:  2.24783974e-02, mean val. rec. loss:  2.10867257e-02\n",
      "Epoch: 1676 mean train loss:  2.24739095e-02, mean val. rec. loss:  2.10823622e-02\n",
      "Epoch: 1677 mean train loss:  2.24694198e-02, mean val. rec. loss:  2.10779941e-02\n",
      "Epoch: 1678 mean train loss:  2.24649319e-02, mean val. rec. loss:  2.10736351e-02\n",
      "Epoch: 1679 mean train loss:  2.24604496e-02, mean val. rec. loss:  2.10692806e-02\n",
      "Epoch: 1680 mean train loss:  2.24559729e-02, mean val. rec. loss:  2.10649216e-02\n",
      "Epoch: 1681 mean train loss:  2.24514887e-02, mean val. rec. loss:  2.10605581e-02\n",
      "Epoch: 1682 mean train loss:  2.24470065e-02, mean val. rec. loss:  2.10562081e-02\n",
      "Epoch: 1683 mean train loss:  2.24425391e-02, mean val. rec. loss:  2.10518536e-02\n",
      "Epoch: 1684 mean train loss:  2.24380624e-02, mean val. rec. loss:  2.10475060e-02\n",
      "Epoch: 1685 mean train loss:  2.24335987e-02, mean val. rec. loss:  2.10431470e-02\n",
      "Epoch: 1686 mean train loss:  2.24291238e-02, mean val. rec. loss:  2.10388061e-02\n",
      "Epoch: 1687 mean train loss:  2.24246620e-02, mean val. rec. loss:  2.10344494e-02\n",
      "Epoch: 1688 mean train loss:  2.24201928e-02, mean val. rec. loss:  2.10301085e-02\n",
      "Epoch: 1689 mean train loss:  2.24157365e-02, mean val. rec. loss:  2.10257563e-02\n",
      "Epoch: 1690 mean train loss:  2.24112654e-02, mean val. rec. loss:  2.10214177e-02\n",
      "Epoch: 1691 mean train loss:  2.24068148e-02, mean val. rec. loss:  2.10170678e-02\n",
      "Epoch: 1692 mean train loss:  2.24023511e-02, mean val. rec. loss:  2.10127315e-02\n",
      "Epoch: 1693 mean train loss:  2.23979024e-02, mean val. rec. loss:  2.10083838e-02\n",
      "Epoch: 1694 mean train loss:  2.23934443e-02, mean val. rec. loss:  2.10040475e-02\n",
      "Epoch: 1695 mean train loss:  2.23889974e-02, mean val. rec. loss:  2.09997043e-02\n",
      "Epoch: 1696 mean train loss:  2.23845393e-02, mean val. rec. loss:  2.09953771e-02\n",
      "Epoch: 1697 mean train loss:  2.23800998e-02, mean val. rec. loss:  2.09910362e-02\n",
      "Epoch: 1698 mean train loss:  2.23756473e-02, mean val. rec. loss:  2.09867090e-02\n",
      "Epoch: 1699 mean train loss:  2.23712097e-02, mean val. rec. loss:  2.09823772e-02\n",
      "Epoch: 1700 mean train loss:  2.23667665e-02, mean val. rec. loss:  2.09780363e-02\n",
      "Epoch: 1701 mean train loss:  2.23623233e-02, mean val. rec. loss:  2.09737113e-02\n",
      "Epoch: 1702 mean train loss:  2.23578913e-02, mean val. rec. loss:  2.09693841e-02\n",
      "Epoch: 1703 mean train loss:  2.23534593e-02, mean val. rec. loss:  2.09650500e-02\n",
      "Epoch: 1704 mean train loss:  2.23490217e-02, mean val. rec. loss:  2.09607228e-02\n",
      "Epoch: 1705 mean train loss:  2.23445860e-02, mean val. rec. loss:  2.09564046e-02\n",
      "Epoch: 1706 mean train loss:  2.23401595e-02, mean val. rec. loss:  2.09520773e-02\n",
      "Epoch: 1707 mean train loss:  2.23357350e-02, mean val. rec. loss:  2.09477614e-02\n",
      "Epoch: 1708 mean train loss:  2.23313048e-02, mean val. rec. loss:  2.09434319e-02\n",
      "Epoch: 1709 mean train loss:  2.23268784e-02, mean val. rec. loss:  2.09391137e-02\n",
      "Epoch: 1710 mean train loss:  2.23224613e-02, mean val. rec. loss:  2.09347955e-02\n",
      "Epoch: 1711 mean train loss:  2.23180404e-02, mean val. rec. loss:  2.09304728e-02\n",
      "Epoch: 1712 mean train loss:  2.23136196e-02, mean val. rec. loss:  2.09261546e-02\n",
      "Epoch: 1713 mean train loss:  2.23091988e-02, mean val. rec. loss:  2.09218455e-02\n",
      "Epoch: 1714 mean train loss:  2.23047854e-02, mean val. rec. loss:  2.09175296e-02\n",
      "Epoch: 1715 mean train loss:  2.23003757e-02, mean val. rec. loss:  2.09132227e-02\n",
      "Epoch: 1716 mean train loss:  2.22959586e-02, mean val. rec. loss:  2.09089000e-02\n",
      "Epoch: 1717 mean train loss:  2.22915433e-02, mean val. rec. loss:  2.09045864e-02\n",
      "Epoch: 1718 mean train loss:  2.22871355e-02, mean val. rec. loss:  2.09002773e-02\n",
      "Epoch: 1719 mean train loss:  2.22827296e-02, mean val. rec. loss:  2.08959704e-02\n",
      "Epoch: 1720 mean train loss:  2.22783274e-02, mean val. rec. loss:  2.08916613e-02\n",
      "Epoch: 1721 mean train loss:  2.22739233e-02, mean val. rec. loss:  2.08873499e-02\n",
      "Epoch: 1722 mean train loss:  2.22695192e-02, mean val. rec. loss:  2.08830431e-02\n",
      "Epoch: 1723 mean train loss:  2.22651207e-02, mean val. rec. loss:  2.08787408e-02\n",
      "Epoch: 1724 mean train loss:  2.22607241e-02, mean val. rec. loss:  2.08744385e-02\n",
      "Epoch: 1725 mean train loss:  2.22563275e-02, mean val. rec. loss:  2.08701339e-02\n",
      "Epoch: 1726 mean train loss:  2.22519327e-02, mean val. rec. loss:  2.08658339e-02\n",
      "Epoch: 1727 mean train loss:  2.22475398e-02, mean val. rec. loss:  2.08615316e-02\n",
      "Epoch: 1728 mean train loss:  2.22431487e-02, mean val. rec. loss:  2.08572315e-02\n",
      "Epoch: 1729 mean train loss:  2.22387595e-02, mean val. rec. loss:  2.08529360e-02\n",
      "Epoch: 1730 mean train loss:  2.22343722e-02, mean val. rec. loss:  2.08486360e-02\n",
      "Epoch: 1731 mean train loss:  2.22299849e-02, mean val. rec. loss:  2.08443427e-02\n",
      "Epoch: 1732 mean train loss:  2.22256013e-02, mean val. rec. loss:  2.08400450e-02\n",
      "Epoch: 1733 mean train loss:  2.22212196e-02, mean val. rec. loss:  2.08357540e-02\n",
      "Epoch: 1734 mean train loss:  2.22168378e-02, mean val. rec. loss:  2.08314608e-02\n",
      "Epoch: 1735 mean train loss:  2.22124580e-02, mean val. rec. loss:  2.08271675e-02\n",
      "Epoch: 1736 mean train loss:  2.22080818e-02, mean val. rec. loss:  2.08228766e-02\n",
      "Epoch: 1737 mean train loss:  2.22037038e-02, mean val. rec. loss:  2.08185969e-02\n",
      "Epoch: 1738 mean train loss:  2.21993333e-02, mean val. rec. loss:  2.08143037e-02\n",
      "Epoch: 1739 mean train loss:  2.21949664e-02, mean val. rec. loss:  2.08100150e-02\n",
      "Epoch: 1740 mean train loss:  2.21905921e-02, mean val. rec. loss:  2.08057218e-02\n",
      "Epoch: 1741 mean train loss:  2.21862197e-02, mean val. rec. loss:  2.08014353e-02\n",
      "Epoch: 1742 mean train loss:  2.21818566e-02, mean val. rec. loss:  2.07971557e-02\n",
      "Epoch: 1743 mean train loss:  2.21774935e-02, mean val. rec. loss:  2.07928715e-02\n",
      "Epoch: 1744 mean train loss:  2.21731285e-02, mean val. rec. loss:  2.07885851e-02\n",
      "Epoch: 1745 mean train loss:  2.21687635e-02, mean val. rec. loss:  2.07843123e-02\n",
      "Epoch: 1746 mean train loss:  2.21644041e-02, mean val. rec. loss:  2.07800304e-02\n",
      "Epoch: 1747 mean train loss:  2.21600503e-02, mean val. rec. loss:  2.07757485e-02\n",
      "Epoch: 1748 mean train loss:  2.21556891e-02, mean val. rec. loss:  2.07714643e-02\n",
      "Epoch: 1749 mean train loss:  2.21513297e-02, mean val. rec. loss:  2.07671847e-02\n",
      "Epoch: 1750 mean train loss:  2.21469759e-02, mean val. rec. loss:  2.07629164e-02\n",
      "Epoch: 1751 mean train loss:  2.21426296e-02, mean val. rec. loss:  2.07586368e-02\n",
      "Epoch: 1752 mean train loss:  2.21382720e-02, mean val. rec. loss:  2.07543594e-02\n",
      "Epoch: 1753 mean train loss:  2.21339201e-02, mean val. rec. loss:  2.07500934e-02\n",
      "Epoch: 1754 mean train loss:  2.21295737e-02, mean val. rec. loss:  2.07458183e-02\n",
      "Epoch: 1755 mean train loss:  2.21252311e-02, mean val. rec. loss:  2.07415432e-02\n",
      "Epoch: 1756 mean train loss:  2.21208829e-02, mean val. rec. loss:  2.07372681e-02\n",
      "Epoch: 1757 mean train loss:  2.21165365e-02, mean val. rec. loss:  2.07329976e-02\n",
      "Epoch: 1758 mean train loss:  2.21121958e-02, mean val. rec. loss:  2.07287338e-02\n",
      "Epoch: 1759 mean train loss:  2.21078587e-02, mean val. rec. loss:  2.07244610e-02\n",
      "Epoch: 1760 mean train loss:  2.21035124e-02, mean val. rec. loss:  2.07201995e-02\n",
      "Epoch: 1761 mean train loss:  2.20991846e-02, mean val. rec. loss:  2.07159244e-02\n",
      "Epoch: 1762 mean train loss:  2.20948402e-02, mean val. rec. loss:  2.07116652e-02\n",
      "Epoch: 1763 mean train loss:  2.20905143e-02, mean val. rec. loss:  2.07073924e-02\n",
      "Epoch: 1764 mean train loss:  2.20861717e-02, mean val. rec. loss:  2.07031354e-02\n",
      "Epoch: 1765 mean train loss:  2.20818514e-02, mean val. rec. loss:  2.06988649e-02\n",
      "Epoch: 1766 mean train loss:  2.20775106e-02, mean val. rec. loss:  2.06946079e-02\n",
      "Epoch: 1767 mean train loss:  2.20731903e-02, mean val. rec. loss:  2.06903396e-02\n",
      "Epoch: 1768 mean train loss:  2.20688570e-02, mean val. rec. loss:  2.06860827e-02\n",
      "Epoch: 1769 mean train loss:  2.20645367e-02, mean val. rec. loss:  2.06818235e-02\n",
      "Epoch: 1770 mean train loss:  2.20602071e-02, mean val. rec. loss:  2.06775643e-02\n",
      "Epoch: 1771 mean train loss:  2.20558887e-02, mean val. rec. loss:  2.06733005e-02\n",
      "Epoch: 1772 mean train loss:  2.20515610e-02, mean val. rec. loss:  2.06690504e-02\n",
      "Epoch: 1773 mean train loss:  2.20472482e-02, mean val. rec. loss:  2.06647866e-02\n",
      "Epoch: 1774 mean train loss:  2.20429241e-02, mean val. rec. loss:  2.06605387e-02\n",
      "Epoch: 1775 mean train loss:  2.20386150e-02, mean val. rec. loss:  2.06562773e-02\n",
      "Epoch: 1776 mean train loss:  2.20342948e-02, mean val. rec. loss:  2.06520294e-02\n",
      "Epoch: 1777 mean train loss:  2.20299875e-02, mean val. rec. loss:  2.06477724e-02\n",
      "Epoch: 1778 mean train loss:  2.20256672e-02, mean val. rec. loss:  2.06435246e-02\n",
      "Epoch: 1779 mean train loss:  2.20213637e-02, mean val. rec. loss:  2.06392676e-02\n",
      "Epoch: 1780 mean train loss:  2.20170471e-02, mean val. rec. loss:  2.06350265e-02\n",
      "Epoch: 1781 mean train loss:  2.20127455e-02, mean val. rec. loss:  2.06307719e-02\n",
      "Epoch: 1782 mean train loss:  2.20084327e-02, mean val. rec. loss:  2.06265262e-02\n",
      "Epoch: 1783 mean train loss:  2.20041347e-02, mean val. rec. loss:  2.06222761e-02\n",
      "Epoch: 1784 mean train loss:  2.19998237e-02, mean val. rec. loss:  2.06180350e-02\n",
      "Epoch: 1785 mean train loss:  2.19955277e-02, mean val. rec. loss:  2.06137803e-02\n",
      "Epoch: 1786 mean train loss:  2.19912223e-02, mean val. rec. loss:  2.06095370e-02\n",
      "Epoch: 1787 mean train loss:  2.19869225e-02, mean val. rec. loss:  2.06052982e-02\n",
      "Epoch: 1788 mean train loss:  2.19826264e-02, mean val. rec. loss:  2.06010526e-02\n",
      "Epoch: 1789 mean train loss:  2.19783359e-02, mean val. rec. loss:  2.05968138e-02\n",
      "Epoch: 1790 mean train loss:  2.19740380e-02, mean val. rec. loss:  2.05925614e-02\n",
      "Epoch: 1791 mean train loss:  2.19697401e-02, mean val. rec. loss:  2.05883248e-02\n",
      "Epoch: 1792 mean train loss:  2.19654514e-02, mean val. rec. loss:  2.05840860e-02\n",
      "Epoch: 1793 mean train loss:  2.19611647e-02, mean val. rec. loss:  2.05798382e-02\n",
      "Epoch: 1794 mean train loss:  2.19568723e-02, mean val. rec. loss:  2.05755994e-02\n",
      "Epoch: 1795 mean train loss:  2.19525800e-02, mean val. rec. loss:  2.05713651e-02\n",
      "Epoch: 1796 mean train loss:  2.19482951e-02, mean val. rec. loss:  2.05671285e-02\n",
      "Epoch: 1797 mean train loss:  2.19440120e-02, mean val. rec. loss:  2.05628988e-02\n",
      "Epoch: 1798 mean train loss:  2.19397253e-02, mean val. rec. loss:  2.05586487e-02\n",
      "Epoch: 1799 mean train loss:  2.19354366e-02, mean val. rec. loss:  2.05544144e-02\n",
      "Epoch: 1800 mean train loss:  2.19311592e-02, mean val. rec. loss:  2.05501824e-02\n",
      "Epoch: 1801 mean train loss:  2.19268817e-02, mean val. rec. loss:  2.05459413e-02\n",
      "Epoch: 1802 mean train loss:  2.19226006e-02, mean val. rec. loss:  2.05417048e-02\n",
      "Epoch: 1803 mean train loss:  2.19183157e-02, mean val. rec. loss:  2.05374773e-02\n",
      "Epoch: 1804 mean train loss:  2.19140419e-02, mean val. rec. loss:  2.05332453e-02\n",
      "Epoch: 1805 mean train loss:  2.19097682e-02, mean val. rec. loss:  2.05290179e-02\n",
      "Epoch: 1806 mean train loss:  2.19054870e-02, mean val. rec. loss:  2.05247768e-02\n",
      "Epoch: 1807 mean train loss:  2.19012114e-02, mean val. rec. loss:  2.05205516e-02\n",
      "Epoch: 1808 mean train loss:  2.18969433e-02, mean val. rec. loss:  2.05163219e-02\n",
      "Epoch: 1809 mean train loss:  2.18926752e-02, mean val. rec. loss:  2.05120831e-02\n",
      "Epoch: 1810 mean train loss:  2.18883996e-02, mean val. rec. loss:  2.05078556e-02\n",
      "Epoch: 1811 mean train loss:  2.18841277e-02, mean val. rec. loss:  2.05036304e-02\n",
      "Epoch: 1812 mean train loss:  2.18798633e-02, mean val. rec. loss:  2.04994029e-02\n",
      "Epoch: 1813 mean train loss:  2.18755989e-02, mean val. rec. loss:  2.04951800e-02\n",
      "Epoch: 1814 mean train loss:  2.18713307e-02, mean val. rec. loss:  2.04909435e-02\n",
      "Epoch: 1815 mean train loss:  2.18670626e-02, mean val. rec. loss:  2.04867205e-02\n",
      "Epoch: 1816 mean train loss:  2.18628019e-02, mean val. rec. loss:  2.04824976e-02\n",
      "Epoch: 1817 mean train loss:  2.18585412e-02, mean val. rec. loss:  2.04782701e-02\n",
      "Epoch: 1818 mean train loss:  2.18542842e-02, mean val. rec. loss:  2.04740472e-02\n",
      "Epoch: 1819 mean train loss:  2.18500254e-02, mean val. rec. loss:  2.04698243e-02\n",
      "Epoch: 1820 mean train loss:  2.18457685e-02, mean val. rec. loss:  2.04656059e-02\n",
      "Epoch: 1821 mean train loss:  2.18415152e-02, mean val. rec. loss:  2.04613830e-02\n",
      "Epoch: 1822 mean train loss:  2.18372601e-02, mean val. rec. loss:  2.04571623e-02\n",
      "Epoch: 1823 mean train loss:  2.18330069e-02, mean val. rec. loss:  2.04529416e-02\n",
      "Epoch: 1824 mean train loss:  2.18287555e-02, mean val. rec. loss:  2.04487210e-02\n",
      "Epoch: 1825 mean train loss:  2.18245041e-02, mean val. rec. loss:  2.04445049e-02\n",
      "Epoch: 1826 mean train loss:  2.18202546e-02, mean val. rec. loss:  2.04402865e-02\n",
      "Epoch: 1827 mean train loss:  2.18160069e-02, mean val. rec. loss:  2.04360681e-02\n",
      "Epoch: 1828 mean train loss:  2.18117593e-02, mean val. rec. loss:  2.04318497e-02\n",
      "Epoch: 1829 mean train loss:  2.18075135e-02, mean val. rec. loss:  2.04276336e-02\n",
      "Epoch: 1830 mean train loss:  2.18032677e-02, mean val. rec. loss:  2.04234174e-02\n",
      "Epoch: 1831 mean train loss:  2.17990237e-02, mean val. rec. loss:  2.04192036e-02\n",
      "Epoch: 1832 mean train loss:  2.17947798e-02, mean val. rec. loss:  2.04149852e-02\n",
      "Epoch: 1833 mean train loss:  2.17905377e-02, mean val. rec. loss:  2.04107713e-02\n",
      "Epoch: 1834 mean train loss:  2.17862957e-02, mean val. rec. loss:  2.04065575e-02\n",
      "Epoch: 1835 mean train loss:  2.17820536e-02, mean val. rec. loss:  2.04023413e-02\n",
      "Epoch: 1836 mean train loss:  2.17778153e-02, mean val. rec. loss:  2.03981275e-02\n",
      "Epoch: 1837 mean train loss:  2.17735788e-02, mean val. rec. loss:  2.03939159e-02\n",
      "Epoch: 1838 mean train loss:  2.17693404e-02, mean val. rec. loss:  2.03897020e-02\n",
      "Epoch: 1839 mean train loss:  2.17651058e-02, mean val. rec. loss:  2.03854927e-02\n",
      "Epoch: 1840 mean train loss:  2.17608693e-02, mean val. rec. loss:  2.03812789e-02\n",
      "Epoch: 1841 mean train loss:  2.17566366e-02, mean val. rec. loss:  2.03770695e-02\n",
      "Epoch: 1842 mean train loss:  2.17524038e-02, mean val. rec. loss:  2.03728602e-02\n",
      "Epoch: 1843 mean train loss:  2.17481710e-02, mean val. rec. loss:  2.03686554e-02\n",
      "Epoch: 1844 mean train loss:  2.17439420e-02, mean val. rec. loss:  2.03644416e-02\n",
      "Epoch: 1845 mean train loss:  2.17397130e-02, mean val. rec. loss:  2.03602368e-02\n",
      "Epoch: 1846 mean train loss:  2.17354839e-02, mean val. rec. loss:  2.03560275e-02\n",
      "Epoch: 1847 mean train loss:  2.17312568e-02, mean val. rec. loss:  2.03518181e-02\n",
      "Epoch: 1848 mean train loss:  2.17270315e-02, mean val. rec. loss:  2.03476156e-02\n",
      "Epoch: 1849 mean train loss:  2.17228061e-02, mean val. rec. loss:  2.03434086e-02\n",
      "Epoch: 1850 mean train loss:  2.17185808e-02, mean val. rec. loss:  2.03392038e-02\n",
      "Epoch: 1851 mean train loss:  2.17143593e-02, mean val. rec. loss:  2.03349967e-02\n",
      "Epoch: 1852 mean train loss:  2.17101358e-02, mean val. rec. loss:  2.03307942e-02\n",
      "Epoch: 1853 mean train loss:  2.17059161e-02, mean val. rec. loss:  2.03265894e-02\n",
      "Epoch: 1854 mean train loss:  2.17016945e-02, mean val. rec. loss:  2.03223846e-02\n",
      "Epoch: 1855 mean train loss:  2.16974804e-02, mean val. rec. loss:  2.03181889e-02\n",
      "Epoch: 1856 mean train loss:  2.16932681e-02, mean val. rec. loss:  2.03139841e-02\n",
      "Epoch: 1857 mean train loss:  2.16890502e-02, mean val. rec. loss:  2.03097794e-02\n",
      "Epoch: 1858 mean train loss:  2.16848324e-02, mean val. rec. loss:  2.03055836e-02\n",
      "Epoch: 1859 mean train loss:  2.16806201e-02, mean val. rec. loss:  2.03013789e-02\n",
      "Epoch: 1860 mean train loss:  2.16764115e-02, mean val. rec. loss:  2.02971809e-02\n",
      "Epoch: 1861 mean train loss:  2.16721993e-02, mean val. rec. loss:  2.02929738e-02\n",
      "Epoch: 1862 mean train loss:  2.16679833e-02, mean val. rec. loss:  2.02887690e-02\n",
      "Epoch: 1863 mean train loss:  2.16637784e-02, mean val. rec. loss:  2.02845801e-02\n",
      "Epoch: 1864 mean train loss:  2.16595736e-02, mean val. rec. loss:  2.02803776e-02\n",
      "Epoch: 1865 mean train loss:  2.16553613e-02, mean val. rec. loss:  2.02761751e-02\n",
      "Epoch: 1866 mean train loss:  2.16511509e-02, mean val. rec. loss:  2.02719885e-02\n",
      "Epoch: 1867 mean train loss:  2.16469480e-02, mean val. rec. loss:  2.02677837e-02\n",
      "Epoch: 1868 mean train loss:  2.16427450e-02, mean val. rec. loss:  2.02635857e-02\n",
      "Epoch: 1869 mean train loss:  2.16385383e-02, mean val. rec. loss:  2.02593809e-02\n",
      "Epoch: 1870 mean train loss:  2.16343298e-02, mean val. rec. loss:  2.02551852e-02\n",
      "Epoch: 1871 mean train loss:  2.16301324e-02, mean val. rec. loss:  2.02509940e-02\n",
      "Epoch: 1872 mean train loss:  2.16259350e-02, mean val. rec. loss:  2.02467983e-02\n",
      "Epoch: 1873 mean train loss:  2.16217302e-02, mean val. rec. loss:  2.02425958e-02\n",
      "Epoch: 1874 mean train loss:  2.16175272e-02, mean val. rec. loss:  2.02384114e-02\n",
      "Epoch: 1875 mean train loss:  2.16133317e-02, mean val. rec. loss:  2.02342157e-02\n",
      "Epoch: 1876 mean train loss:  2.16091380e-02, mean val. rec. loss:  2.02300154e-02\n",
      "Epoch: 1877 mean train loss:  2.16049369e-02, mean val. rec. loss:  2.02258197e-02\n",
      "Epoch: 1878 mean train loss:  2.16007396e-02, mean val. rec. loss:  2.02216195e-02\n",
      "Epoch: 1879 mean train loss:  2.15965459e-02, mean val. rec. loss:  2.02174328e-02\n",
      "Epoch: 1880 mean train loss:  2.15923523e-02, mean val. rec. loss:  2.02132394e-02\n",
      "Epoch: 1881 mean train loss:  2.15881567e-02, mean val. rec. loss:  2.02090460e-02\n",
      "Epoch: 1882 mean train loss:  2.15839612e-02, mean val. rec. loss:  2.02048593e-02\n",
      "Epoch: 1883 mean train loss:  2.15797713e-02, mean val. rec. loss:  2.02006613e-02\n",
      "Epoch: 1884 mean train loss:  2.15755832e-02, mean val. rec. loss:  2.01964702e-02\n",
      "Epoch: 1885 mean train loss:  2.15713896e-02, mean val. rec. loss:  2.01922744e-02\n",
      "Epoch: 1886 mean train loss:  2.15671978e-02, mean val. rec. loss:  2.01880787e-02\n",
      "Epoch: 1887 mean train loss:  2.15630116e-02, mean val. rec. loss:  2.01838989e-02\n",
      "Epoch: 1888 mean train loss:  2.15588254e-02, mean val. rec. loss:  2.01797032e-02\n",
      "Epoch: 1889 mean train loss:  2.15546392e-02, mean val. rec. loss:  2.01755120e-02\n",
      "Epoch: 1890 mean train loss:  2.15504492e-02, mean val. rec. loss:  2.01713299e-02\n",
      "Epoch: 1891 mean train loss:  2.15462668e-02, mean val. rec. loss:  2.01671364e-02\n",
      "Epoch: 1892 mean train loss:  2.15420843e-02, mean val. rec. loss:  2.01629475e-02\n",
      "Epoch: 1893 mean train loss:  2.15379018e-02, mean val. rec. loss:  2.01587541e-02\n",
      "Epoch: 1894 mean train loss:  2.15337156e-02, mean val. rec. loss:  2.01545629e-02\n",
      "Epoch: 1895 mean train loss:  2.15295387e-02, mean val. rec. loss:  2.01503831e-02\n",
      "Epoch: 1896 mean train loss:  2.15253600e-02, mean val. rec. loss:  2.01461942e-02\n",
      "Epoch: 1897 mean train loss:  2.15211775e-02, mean val. rec. loss:  2.01420075e-02\n",
      "Epoch: 1898 mean train loss:  2.15169969e-02, mean val. rec. loss:  2.01378277e-02\n",
      "Epoch: 1899 mean train loss:  2.15128218e-02, mean val. rec. loss:  2.01336388e-02\n",
      "Epoch: 1900 mean train loss:  2.15086468e-02, mean val. rec. loss:  2.01294521e-02\n",
      "Epoch: 1901 mean train loss:  2.15044680e-02, mean val. rec. loss:  2.01252632e-02\n",
      "Epoch: 1902 mean train loss:  2.15002911e-02, mean val. rec. loss:  2.01210743e-02\n",
      "Epoch: 1903 mean train loss:  2.14961180e-02, mean val. rec. loss:  2.01168990e-02\n",
      "Epoch: 1904 mean train loss:  2.14919485e-02, mean val. rec. loss:  2.01127101e-02\n",
      "Epoch: 1905 mean train loss:  2.14877716e-02, mean val. rec. loss:  2.01085235e-02\n",
      "Epoch: 1906 mean train loss:  2.14835985e-02, mean val. rec. loss:  2.01043482e-02\n",
      "Epoch: 1907 mean train loss:  2.14794290e-02, mean val. rec. loss:  2.01001615e-02\n",
      "Epoch: 1908 mean train loss:  2.14752614e-02, mean val. rec. loss:  2.00959749e-02\n",
      "Epoch: 1909 mean train loss:  2.14710883e-02, mean val. rec. loss:  2.00917882e-02\n",
      "Epoch: 1910 mean train loss:  2.14669151e-02, mean val. rec. loss:  2.00876016e-02\n",
      "Epoch: 1911 mean train loss:  2.14627512e-02, mean val. rec. loss:  2.00834263e-02\n",
      "Epoch: 1912 mean train loss:  2.14585855e-02, mean val. rec. loss:  2.00792419e-02\n",
      "Epoch: 1913 mean train loss:  2.14544142e-02, mean val. rec. loss:  2.00750553e-02\n",
      "Epoch: 1914 mean train loss:  2.14502448e-02, mean val. rec. loss:  2.00708845e-02\n",
      "Epoch: 1915 mean train loss:  2.14460846e-02, mean val. rec. loss:  2.00666979e-02\n",
      "Epoch: 1916 mean train loss:  2.14419208e-02, mean val. rec. loss:  2.00625135e-02\n",
      "Epoch: 1917 mean train loss:  2.14377532e-02, mean val. rec. loss:  2.00583291e-02\n",
      "Epoch: 1918 mean train loss:  2.14335893e-02, mean val. rec. loss:  2.00541493e-02\n",
      "Epoch: 1919 mean train loss:  2.14294292e-02, mean val. rec. loss:  2.00499717e-02\n",
      "Epoch: 1920 mean train loss:  2.14252691e-02, mean val. rec. loss:  2.00457896e-02\n",
      "Epoch: 1921 mean train loss:  2.14211052e-02, mean val. rec. loss:  2.00416030e-02\n",
      "Epoch: 1922 mean train loss:  2.14169432e-02, mean val. rec. loss:  2.00374345e-02\n",
      "Epoch: 1923 mean train loss:  2.14127849e-02, mean val. rec. loss:  2.00332501e-02\n",
      "Epoch: 1924 mean train loss:  2.14086304e-02, mean val. rec. loss:  2.00290725e-02\n",
      "Epoch: 1925 mean train loss:  2.14044665e-02, mean val. rec. loss:  2.00248882e-02\n",
      "Epoch: 1926 mean train loss:  2.14003083e-02, mean val. rec. loss:  2.00207060e-02\n",
      "Epoch: 1927 mean train loss:  2.13961556e-02, mean val. rec. loss:  2.00165330e-02\n",
      "Epoch: 1928 mean train loss:  2.13920010e-02, mean val. rec. loss:  2.00123532e-02\n",
      "Epoch: 1929 mean train loss:  2.13878428e-02, mean val. rec. loss:  2.00081733e-02\n",
      "Epoch: 1930 mean train loss:  2.13836864e-02, mean val. rec. loss:  2.00040026e-02\n",
      "Epoch: 1931 mean train loss:  2.13795337e-02, mean val. rec. loss:  1.99998205e-02\n",
      "Epoch: 1932 mean train loss:  2.13753847e-02, mean val. rec. loss:  1.99956452e-02\n",
      "Epoch: 1933 mean train loss:  2.13712302e-02, mean val. rec. loss:  1.99914676e-02\n",
      "Epoch: 1934 mean train loss:  2.13670793e-02, mean val. rec. loss:  1.99872855e-02\n",
      "Epoch: 1935 mean train loss:  2.13629304e-02, mean val. rec. loss:  1.99831034e-02\n",
      "Epoch: 1936 mean train loss:  2.13587777e-02, mean val. rec. loss:  1.99789304e-02\n",
      "Epoch: 1937 mean train loss:  2.13546306e-02, mean val. rec. loss:  1.99747619e-02\n",
      "Epoch: 1938 mean train loss:  2.13504872e-02, mean val. rec. loss:  1.99705797e-02\n",
      "Epoch: 1939 mean train loss:  2.13463383e-02, mean val. rec. loss:  1.99663931e-02\n",
      "Epoch: 1940 mean train loss:  2.13421930e-02, mean val. rec. loss:  1.99622201e-02\n",
      "Epoch: 1941 mean train loss:  2.13380459e-02, mean val. rec. loss:  1.99580516e-02\n",
      "Epoch: 1942 mean train loss:  2.13339026e-02, mean val. rec. loss:  1.99538695e-02\n",
      "Epoch: 1943 mean train loss:  2.13297573e-02, mean val. rec. loss:  1.99496874e-02\n",
      "Epoch: 1944 mean train loss:  2.13256158e-02, mean val. rec. loss:  1.99455143e-02\n",
      "Epoch: 1945 mean train loss:  2.13214706e-02, mean val. rec. loss:  1.99413458e-02\n",
      "Epoch: 1946 mean train loss:  2.13173291e-02, mean val. rec. loss:  1.99371637e-02\n",
      "Epoch: 1947 mean train loss:  2.13131876e-02, mean val. rec. loss:  1.99329816e-02\n",
      "Epoch: 1948 mean train loss:  2.13090460e-02, mean val. rec. loss:  1.99288131e-02\n",
      "Epoch: 1949 mean train loss:  2.13049045e-02, mean val. rec. loss:  1.99246378e-02\n",
      "Epoch: 1950 mean train loss:  2.13007630e-02, mean val. rec. loss:  1.99204580e-02\n",
      "Epoch: 1951 mean train loss:  2.12966252e-02, mean val. rec. loss:  1.99162759e-02\n",
      "Epoch: 1952 mean train loss:  2.12924856e-02, mean val. rec. loss:  1.99121074e-02\n",
      "Epoch: 1953 mean train loss:  2.12883459e-02, mean val. rec. loss:  1.99079343e-02\n",
      "Epoch: 1954 mean train loss:  2.12842119e-02, mean val. rec. loss:  1.99037568e-02\n",
      "Epoch: 1955 mean train loss:  2.12800722e-02, mean val. rec. loss:  1.98995747e-02\n",
      "Epoch: 1956 mean train loss:  2.12759363e-02, mean val. rec. loss:  1.98954062e-02\n",
      "Epoch: 1957 mean train loss:  2.12718022e-02, mean val. rec. loss:  1.98912377e-02\n",
      "Epoch: 1958 mean train loss:  2.12676663e-02, mean val. rec. loss:  1.98870556e-02\n",
      "Epoch: 1959 mean train loss:  2.12635304e-02, mean val. rec. loss:  1.98828780e-02\n",
      "Epoch: 1960 mean train loss:  2.12593963e-02, mean val. rec. loss:  1.98787072e-02\n",
      "Epoch: 1961 mean train loss:  2.12552623e-02, mean val. rec. loss:  1.98745365e-02\n",
      "Epoch: 1962 mean train loss:  2.12511301e-02, mean val. rec. loss:  1.98703566e-02\n",
      "Epoch: 1963 mean train loss:  2.12469979e-02, mean val. rec. loss:  1.98661791e-02\n",
      "Epoch: 1964 mean train loss:  2.12428675e-02, mean val. rec. loss:  1.98620128e-02\n",
      "Epoch: 1965 mean train loss:  2.12387372e-02, mean val. rec. loss:  1.98578398e-02\n",
      "Epoch: 1966 mean train loss:  2.12346068e-02, mean val. rec. loss:  1.98536622e-02\n",
      "Epoch: 1967 mean train loss:  2.12304765e-02, mean val. rec. loss:  1.98494801e-02\n",
      "Epoch: 1968 mean train loss:  2.12263462e-02, mean val. rec. loss:  1.98453116e-02\n",
      "Epoch: 1969 mean train loss:  2.12222177e-02, mean val. rec. loss:  1.98411431e-02\n",
      "Epoch: 1970 mean train loss:  2.12180873e-02, mean val. rec. loss:  1.98369656e-02\n",
      "Epoch: 1971 mean train loss:  2.12139626e-02, mean val. rec. loss:  1.98327880e-02\n",
      "Epoch: 1972 mean train loss:  2.12098360e-02, mean val. rec. loss:  1.98286195e-02\n",
      "Epoch: 1973 mean train loss:  2.12057094e-02, mean val. rec. loss:  1.98244533e-02\n",
      "Epoch: 1974 mean train loss:  2.12015827e-02, mean val. rec. loss:  1.98202734e-02\n",
      "Epoch: 1975 mean train loss:  2.11974561e-02, mean val. rec. loss:  1.98160913e-02\n",
      "Epoch: 1976 mean train loss:  2.11933332e-02, mean val. rec. loss:  1.98119296e-02\n",
      "Epoch: 1977 mean train loss:  2.11892104e-02, mean val. rec. loss:  1.98077543e-02\n",
      "Epoch: 1978 mean train loss:  2.11850819e-02, mean val. rec. loss:  1.98035767e-02\n",
      "Epoch: 1979 mean train loss:  2.11809534e-02, mean val. rec. loss:  1.97994173e-02\n",
      "Epoch: 1980 mean train loss:  2.11768342e-02, mean val. rec. loss:  1.97952398e-02\n",
      "Epoch: 1981 mean train loss:  2.11727151e-02, mean val. rec. loss:  1.97910667e-02\n",
      "Epoch: 1982 mean train loss:  2.11685885e-02, mean val. rec. loss:  1.97868891e-02\n",
      "Epoch: 1983 mean train loss:  2.11644618e-02, mean val. rec. loss:  1.97827161e-02\n",
      "Epoch: 1984 mean train loss:  2.11603445e-02, mean val. rec. loss:  1.97785522e-02\n",
      "Epoch: 1985 mean train loss:  2.11562272e-02, mean val. rec. loss:  1.97743791e-02\n",
      "Epoch: 1986 mean train loss:  2.11521006e-02, mean val. rec. loss:  1.97702015e-02\n",
      "Epoch: 1987 mean train loss:  2.11479777e-02, mean val. rec. loss:  1.97660399e-02\n",
      "Epoch: 1988 mean train loss:  2.11438623e-02, mean val. rec. loss:  1.97618668e-02\n",
      "Epoch: 1989 mean train loss:  2.11397469e-02, mean val. rec. loss:  1.97576915e-02\n",
      "Epoch: 1990 mean train loss:  2.11356240e-02, mean val. rec. loss:  1.97535185e-02\n",
      "Epoch: 1991 mean train loss:  2.11315011e-02, mean val. rec. loss:  1.97493432e-02\n",
      "Epoch: 1992 mean train loss:  2.11273875e-02, mean val. rec. loss:  1.97451792e-02\n",
      "Epoch: 1993 mean train loss:  2.11232739e-02, mean val. rec. loss:  1.97410039e-02\n",
      "Epoch: 1994 mean train loss:  2.11191547e-02, mean val. rec. loss:  1.97368286e-02\n",
      "Epoch: 1995 mean train loss:  2.11150337e-02, mean val. rec. loss:  1.97326715e-02\n",
      "Epoch: 1996 mean train loss:  2.11109220e-02, mean val. rec. loss:  1.97284962e-02\n",
      "Epoch: 1997 mean train loss:  2.11068121e-02, mean val. rec. loss:  1.97243209e-02\n",
      "Epoch: 1998 mean train loss:  2.11026930e-02, mean val. rec. loss:  1.97201456e-02\n",
      "Epoch: 1999 mean train loss:  2.10985757e-02, mean val. rec. loss:  1.97159703e-02\n",
      "Epoch: 2000 mean train loss:  2.10944658e-02, mean val. rec. loss:  1.97118063e-02\n",
      "Epoch: 2001 mean train loss:  2.10903560e-02, mean val. rec. loss:  1.97076333e-02\n",
      "Epoch: 2002 mean train loss:  2.10862387e-02, mean val. rec. loss:  1.97034602e-02\n",
      "Epoch: 2003 mean train loss:  2.10821251e-02, mean val. rec. loss:  1.96992985e-02\n",
      "Epoch: 2004 mean train loss:  2.10780171e-02, mean val. rec. loss:  1.96951232e-02\n",
      "Epoch: 2005 mean train loss:  2.10739072e-02, mean val. rec. loss:  1.96909502e-02\n",
      "Epoch: 2006 mean train loss:  2.10697918e-02, mean val. rec. loss:  1.96867726e-02\n",
      "Epoch: 2007 mean train loss:  2.10656782e-02, mean val. rec. loss:  1.96825996e-02\n",
      "Epoch: 2008 mean train loss:  2.10615721e-02, mean val. rec. loss:  1.96784356e-02\n",
      "Epoch: 2009 mean train loss:  2.10574641e-02, mean val. rec. loss:  1.96742603e-02\n",
      "Epoch: 2010 mean train loss:  2.10533524e-02, mean val. rec. loss:  1.96700850e-02\n",
      "Epoch: 2011 mean train loss:  2.10492388e-02, mean val. rec. loss:  1.96659211e-02\n",
      "Epoch: 2012 mean train loss:  2.10451327e-02, mean val. rec. loss:  1.96617435e-02\n",
      "Epoch: 2013 mean train loss:  2.10410284e-02, mean val. rec. loss:  1.96575727e-02\n",
      "Epoch: 2014 mean train loss:  2.10369167e-02, mean val. rec. loss:  1.96533974e-02\n",
      "Epoch: 2015 mean train loss:  2.10328068e-02, mean val. rec. loss:  1.96492221e-02\n",
      "Epoch: 2016 mean train loss:  2.10287044e-02, mean val. rec. loss:  1.96450559e-02\n",
      "Epoch: 2017 mean train loss:  2.10246002e-02, mean val. rec. loss:  1.96408829e-02\n",
      "Epoch: 2018 mean train loss:  2.10204922e-02, mean val. rec. loss:  1.96367076e-02\n",
      "Epoch: 2019 mean train loss:  2.10163842e-02, mean val. rec. loss:  1.96325459e-02\n",
      "Epoch: 2020 mean train loss:  2.10122818e-02, mean val. rec. loss:  1.96283683e-02\n",
      "Epoch: 2021 mean train loss:  2.10081794e-02, mean val. rec. loss:  1.96241907e-02\n",
      "Epoch: 2022 mean train loss:  2.10040714e-02, mean val. rec. loss:  1.96200177e-02\n",
      "Epoch: 2023 mean train loss:  2.09999652e-02, mean val. rec. loss:  1.96158424e-02\n",
      "Epoch: 2024 mean train loss:  2.09958666e-02, mean val. rec. loss:  1.96116762e-02\n",
      "Epoch: 2025 mean train loss:  2.09917660e-02, mean val. rec. loss:  1.96075031e-02\n",
      "Epoch: 2026 mean train loss:  2.09876580e-02, mean val. rec. loss:  1.96033256e-02\n",
      "Epoch: 2027 mean train loss:  2.09835538e-02, mean val. rec. loss:  1.95991616e-02\n",
      "Epoch: 2028 mean train loss:  2.09794569e-02, mean val. rec. loss:  1.95949840e-02\n",
      "Epoch: 2029 mean train loss:  2.09753601e-02, mean val. rec. loss:  1.95908110e-02\n",
      "Epoch: 2030 mean train loss:  2.09712559e-02, mean val. rec. loss:  1.95866357e-02\n",
      "Epoch: 2031 mean train loss:  2.09671516e-02, mean val. rec. loss:  1.95824558e-02\n",
      "Epoch: 2032 mean train loss:  2.09630548e-02, mean val. rec. loss:  1.95782919e-02\n",
      "Epoch: 2033 mean train loss:  2.09589579e-02, mean val. rec. loss:  1.95741189e-02\n",
      "Epoch: 2034 mean train loss:  2.09548537e-02, mean val. rec. loss:  1.95699413e-02\n",
      "Epoch: 2035 mean train loss:  2.09507531e-02, mean val. rec. loss:  1.95657773e-02\n",
      "Epoch: 2036 mean train loss:  2.09466563e-02, mean val. rec. loss:  1.95615997e-02\n",
      "Epoch: 2037 mean train loss:  2.09425632e-02, mean val. rec. loss:  1.95574244e-02\n",
      "Epoch: 2038 mean train loss:  2.09384627e-02, mean val. rec. loss:  1.95532446e-02\n",
      "Epoch: 2039 mean train loss:  2.09343603e-02, mean val. rec. loss:  1.95490716e-02\n",
      "Epoch: 2040 mean train loss:  2.09302653e-02, mean val. rec. loss:  1.95449031e-02\n",
      "Epoch: 2041 mean train loss:  2.09261722e-02, mean val. rec. loss:  1.95407300e-02\n",
      "Epoch: 2042 mean train loss:  2.09220717e-02, mean val. rec. loss:  1.95365479e-02\n",
      "Epoch: 2043 mean train loss:  2.09179711e-02, mean val. rec. loss:  1.95323840e-02\n",
      "Epoch: 2044 mean train loss:  2.09138780e-02, mean val. rec. loss:  1.95282064e-02\n",
      "Epoch: 2045 mean train loss:  2.09097849e-02, mean val. rec. loss:  1.95240288e-02\n",
      "Epoch: 2046 mean train loss:  2.09056863e-02, mean val. rec. loss:  1.95198513e-02\n",
      "Epoch: 2047 mean train loss:  2.09015913e-02, mean val. rec. loss:  1.95156737e-02\n",
      "Epoch: 2048 mean train loss:  2.08975001e-02, mean val. rec. loss:  1.95115052e-02\n",
      "Epoch: 2049 mean train loss:  2.08934088e-02, mean val. rec. loss:  1.95073322e-02\n",
      "Epoch: 2050 mean train loss:  2.08893120e-02, mean val. rec. loss:  1.95031523e-02\n",
      "Epoch: 2051 mean train loss:  2.08852152e-02, mean val. rec. loss:  1.94989861e-02\n",
      "Epoch: 2052 mean train loss:  2.08811258e-02, mean val. rec. loss:  1.94948085e-02\n",
      "Epoch: 2053 mean train loss:  2.08770365e-02, mean val. rec. loss:  1.94906310e-02\n",
      "Epoch: 2054 mean train loss:  2.08729396e-02, mean val. rec. loss:  1.94864489e-02\n",
      "Epoch: 2055 mean train loss:  2.08688428e-02, mean val. rec. loss:  1.94822713e-02\n",
      "Epoch: 2056 mean train loss:  2.08647535e-02, mean val. rec. loss:  1.94781028e-02\n",
      "Epoch: 2057 mean train loss:  2.08606678e-02, mean val. rec. loss:  1.94739252e-02\n",
      "Epoch: 2058 mean train loss:  2.08565710e-02, mean val. rec. loss:  1.94697431e-02\n",
      "Epoch: 2059 mean train loss:  2.08524779e-02, mean val. rec. loss:  1.94655791e-02\n",
      "Epoch: 2060 mean train loss:  2.08483885e-02, mean val. rec. loss:  1.94613970e-02\n",
      "Epoch: 2061 mean train loss:  2.08443047e-02, mean val. rec. loss:  1.94572195e-02\n",
      "Epoch: 2062 mean train loss:  2.08402098e-02, mean val. rec. loss:  1.94530419e-02\n",
      "Epoch: 2063 mean train loss:  2.08361167e-02, mean val. rec. loss:  1.94488598e-02\n",
      "Epoch: 2064 mean train loss:  2.08320329e-02, mean val. rec. loss:  1.94446958e-02\n",
      "Epoch: 2065 mean train loss:  2.08279417e-02, mean val. rec. loss:  1.94405092e-02\n",
      "Epoch: 2066 mean train loss:  2.08238523e-02, mean val. rec. loss:  1.94363452e-02\n",
      "Epoch: 2067 mean train loss:  2.08197611e-02, mean val. rec. loss:  1.94321586e-02\n",
      "Epoch: 2068 mean train loss:  2.08156736e-02, mean val. rec. loss:  1.94279901e-02\n",
      "Epoch: 2069 mean train loss:  2.08115861e-02, mean val. rec. loss:  1.94238034e-02\n",
      "Epoch: 2070 mean train loss:  2.08074948e-02, mean val. rec. loss:  1.94196350e-02\n",
      "Epoch: 2071 mean train loss:  2.08034054e-02, mean val. rec. loss:  1.94154483e-02\n",
      "Epoch: 2072 mean train loss:  2.07993198e-02, mean val. rec. loss:  1.94112843e-02\n",
      "Epoch: 2073 mean train loss:  2.07952323e-02, mean val. rec. loss:  1.94070932e-02\n",
      "Epoch: 2074 mean train loss:  2.07911448e-02, mean val. rec. loss:  1.94029247e-02\n",
      "Epoch: 2075 mean train loss:  2.07870554e-02, mean val. rec. loss:  1.93987335e-02\n",
      "Epoch: 2076 mean train loss:  2.07829698e-02, mean val. rec. loss:  1.93945673e-02\n",
      "Epoch: 2077 mean train loss:  2.07788804e-02, mean val. rec. loss:  1.93903784e-02\n",
      "Epoch: 2078 mean train loss:  2.07747985e-02, mean val. rec. loss:  1.93862099e-02\n",
      "Epoch: 2079 mean train loss:  2.07707091e-02, mean val. rec. loss:  1.93820232e-02\n",
      "Epoch: 2080 mean train loss:  2.07666235e-02, mean val. rec. loss:  1.93778525e-02\n",
      "Epoch: 2081 mean train loss:  2.07625378e-02, mean val. rec. loss:  1.93736658e-02\n",
      "Epoch: 2082 mean train loss:  2.07584522e-02, mean val. rec. loss:  1.93694950e-02\n",
      "Epoch: 2083 mean train loss:  2.07543665e-02, mean val. rec. loss:  1.93653039e-02\n",
      "Epoch: 2084 mean train loss:  2.07502809e-02, mean val. rec. loss:  1.93611331e-02\n",
      "Epoch: 2085 mean train loss:  2.07461971e-02, mean val. rec. loss:  1.93569442e-02\n",
      "Epoch: 2086 mean train loss:  2.07421133e-02, mean val. rec. loss:  1.93527712e-02\n",
      "Epoch: 2087 mean train loss:  2.07380277e-02, mean val. rec. loss:  1.93485822e-02\n",
      "Epoch: 2088 mean train loss:  2.07339439e-02, mean val. rec. loss:  1.93444092e-02\n",
      "Epoch: 2089 mean train loss:  2.07298601e-02, mean val. rec. loss:  1.93402180e-02\n",
      "Epoch: 2090 mean train loss:  2.07257763e-02, mean val. rec. loss:  1.93360473e-02\n",
      "Epoch: 2091 mean train loss:  2.07216925e-02, mean val. rec. loss:  1.93318584e-02\n",
      "Epoch: 2092 mean train loss:  2.07176106e-02, mean val. rec. loss:  1.93276853e-02\n",
      "Epoch: 2093 mean train loss:  2.07135287e-02, mean val. rec. loss:  1.93234941e-02\n",
      "Epoch: 2094 mean train loss:  2.07094468e-02, mean val. rec. loss:  1.93193234e-02\n",
      "Epoch: 2095 mean train loss:  2.07053648e-02, mean val. rec. loss:  1.93151277e-02\n",
      "Epoch: 2096 mean train loss:  2.07012829e-02, mean val. rec. loss:  1.93109614e-02\n",
      "Epoch: 2097 mean train loss:  2.06972029e-02, mean val. rec. loss:  1.93067635e-02\n",
      "Epoch: 2098 mean train loss:  2.06931172e-02, mean val. rec. loss:  1.93025950e-02\n",
      "Epoch: 2099 mean train loss:  2.06890372e-02, mean val. rec. loss:  1.92983992e-02\n",
      "Epoch: 2100 mean train loss:  2.06849571e-02, mean val. rec. loss:  1.92942285e-02\n",
      "Epoch: 2101 mean train loss:  2.06808770e-02, mean val. rec. loss:  1.92900350e-02\n",
      "Epoch: 2102 mean train loss:  2.06767970e-02, mean val. rec. loss:  1.92858620e-02\n",
      "Epoch: 2103 mean train loss:  2.06727169e-02, mean val. rec. loss:  1.92816686e-02\n",
      "Epoch: 2104 mean train loss:  2.06686369e-02, mean val. rec. loss:  1.92774955e-02\n",
      "Epoch: 2105 mean train loss:  2.06645568e-02, mean val. rec. loss:  1.92733021e-02\n",
      "Epoch: 2106 mean train loss:  2.06604786e-02, mean val. rec. loss:  1.92691290e-02\n",
      "Epoch: 2107 mean train loss:  2.06564004e-02, mean val. rec. loss:  1.92649288e-02\n",
      "Epoch: 2108 mean train loss:  2.06523203e-02, mean val. rec. loss:  1.92607580e-02\n",
      "Epoch: 2109 mean train loss:  2.06482403e-02, mean val. rec. loss:  1.92565646e-02\n",
      "Epoch: 2110 mean train loss:  2.06441584e-02, mean val. rec. loss:  1.92523825e-02\n",
      "Epoch: 2111 mean train loss:  2.06400820e-02, mean val. rec. loss:  1.92481958e-02\n",
      "Epoch: 2112 mean train loss:  2.06360057e-02, mean val. rec. loss:  1.92440069e-02\n",
      "Epoch: 2113 mean train loss:  2.06319238e-02, mean val. rec. loss:  1.92398248e-02\n",
      "Epoch: 2114 mean train loss:  2.06278419e-02, mean val. rec. loss:  1.92356427e-02\n",
      "Epoch: 2115 mean train loss:  2.06237655e-02, mean val. rec. loss:  1.92314538e-02\n",
      "Epoch: 2116 mean train loss:  2.06196892e-02, mean val. rec. loss:  1.92272762e-02\n",
      "Epoch: 2117 mean train loss:  2.06156073e-02, mean val. rec. loss:  1.92230805e-02\n",
      "Epoch: 2118 mean train loss:  2.06115253e-02, mean val. rec. loss:  1.92188939e-02\n",
      "Epoch: 2119 mean train loss:  2.06074509e-02, mean val. rec. loss:  1.92147072e-02\n",
      "Epoch: 2120 mean train loss:  2.06033764e-02, mean val. rec. loss:  1.92105115e-02\n",
      "Epoch: 2121 mean train loss:  2.05992945e-02, mean val. rec. loss:  1.92063272e-02\n",
      "Epoch: 2122 mean train loss:  2.05952125e-02, mean val. rec. loss:  1.92021450e-02\n",
      "Epoch: 2123 mean train loss:  2.05911381e-02, mean val. rec. loss:  1.91979561e-02\n",
      "Epoch: 2124 mean train loss:  2.05870673e-02, mean val. rec. loss:  1.91937740e-02\n",
      "Epoch: 2125 mean train loss:  2.05829854e-02, mean val. rec. loss:  1.91895738e-02\n",
      "Epoch: 2126 mean train loss:  2.05789072e-02, mean val. rec. loss:  1.91853917e-02\n",
      "Epoch: 2127 mean train loss:  2.05748309e-02, mean val. rec. loss:  1.91812050e-02\n",
      "Epoch: 2128 mean train loss:  2.05707583e-02, mean val. rec. loss:  1.91770071e-02\n",
      "Epoch: 2129 mean train loss:  2.05666801e-02, mean val. rec. loss:  1.91728204e-02\n",
      "Epoch: 2130 mean train loss:  2.05626019e-02, mean val. rec. loss:  1.91686338e-02\n",
      "Epoch: 2131 mean train loss:  2.05585274e-02, mean val. rec. loss:  1.91644449e-02\n",
      "Epoch: 2132 mean train loss:  2.05544548e-02, mean val. rec. loss:  1.91602582e-02\n",
      "Epoch: 2133 mean train loss:  2.05503785e-02, mean val. rec. loss:  1.91560580e-02\n",
      "Epoch: 2134 mean train loss:  2.05462984e-02, mean val. rec. loss:  1.91518713e-02\n",
      "Epoch: 2135 mean train loss:  2.05422258e-02, mean val. rec. loss:  1.91476847e-02\n",
      "Epoch: 2136 mean train loss:  2.05381550e-02, mean val. rec. loss:  1.91434844e-02\n",
      "Epoch: 2137 mean train loss:  2.05340787e-02, mean val. rec. loss:  1.91392933e-02\n",
      "Epoch: 2138 mean train loss:  2.05299986e-02, mean val. rec. loss:  1.91351043e-02\n",
      "Epoch: 2139 mean train loss:  2.05259279e-02, mean val. rec. loss:  1.91309132e-02\n",
      "Epoch: 2140 mean train loss:  2.05218571e-02, mean val. rec. loss:  1.91267265e-02\n",
      "Epoch: 2141 mean train loss:  2.05177808e-02, mean val. rec. loss:  1.91225240e-02\n",
      "Epoch: 2142 mean train loss:  2.05137007e-02, mean val. rec. loss:  1.91183328e-02\n",
      "Epoch: 2143 mean train loss:  2.05096319e-02, mean val. rec. loss:  1.91141439e-02\n",
      "Epoch: 2144 mean train loss:  2.05055630e-02, mean val. rec. loss:  1.91099459e-02\n",
      "Epoch: 2145 mean train loss:  2.05014811e-02, mean val. rec. loss:  1.91057570e-02\n",
      "Epoch: 2146 mean train loss:  2.04974140e-02, mean val. rec. loss:  1.91015568e-02\n",
      "Epoch: 2147 mean train loss:  2.04933321e-02, mean val. rec. loss:  1.90973679e-02\n",
      "Epoch: 2148 mean train loss:  2.04892651e-02, mean val. rec. loss:  1.90931676e-02\n",
      "Epoch: 2149 mean train loss:  2.04851832e-02, mean val. rec. loss:  1.90889787e-02\n",
      "Epoch: 2150 mean train loss:  2.04811161e-02, mean val. rec. loss:  1.90847785e-02\n",
      "Epoch: 2151 mean train loss:  2.04770342e-02, mean val. rec. loss:  1.90805896e-02\n",
      "Epoch: 2152 mean train loss:  2.04729691e-02, mean val. rec. loss:  1.90763870e-02\n",
      "Epoch: 2153 mean train loss:  2.04688890e-02, mean val. rec. loss:  1.90721981e-02\n",
      "Epoch: 2154 mean train loss:  2.04648220e-02, mean val. rec. loss:  1.90679956e-02\n",
      "Epoch: 2155 mean train loss:  2.04607400e-02, mean val. rec. loss:  1.90638044e-02\n",
      "Epoch: 2156 mean train loss:  2.04566730e-02, mean val. rec. loss:  1.90596042e-02\n",
      "Epoch: 2157 mean train loss:  2.04525948e-02, mean val. rec. loss:  1.90554107e-02\n",
      "Epoch: 2158 mean train loss:  2.04485278e-02, mean val. rec. loss:  1.90512060e-02\n",
      "Epoch: 2159 mean train loss:  2.04444496e-02, mean val. rec. loss:  1.90470125e-02\n",
      "Epoch: 2160 mean train loss:  2.04403826e-02, mean val. rec. loss:  1.90428100e-02\n",
      "Epoch: 2161 mean train loss:  2.04363007e-02, mean val. rec. loss:  1.90386143e-02\n",
      "Epoch: 2162 mean train loss:  2.04322374e-02, mean val. rec. loss:  1.90344118e-02\n",
      "Epoch: 2163 mean train loss:  2.04281573e-02, mean val. rec. loss:  1.90302160e-02\n",
      "Epoch: 2164 mean train loss:  2.04240921e-02, mean val. rec. loss:  1.90260090e-02\n",
      "Epoch: 2165 mean train loss:  2.04200102e-02, mean val. rec. loss:  1.90218178e-02\n",
      "Epoch: 2166 mean train loss:  2.04159469e-02, mean val. rec. loss:  1.90176130e-02\n",
      "Epoch: 2167 mean train loss:  2.04118669e-02, mean val. rec. loss:  1.90134173e-02\n",
      "Epoch: 2168 mean train loss:  2.04078017e-02, mean val. rec. loss:  1.90092103e-02\n",
      "Epoch: 2169 mean train loss:  2.04037216e-02, mean val. rec. loss:  1.90050157e-02\n",
      "Epoch: 2170 mean train loss:  2.03996602e-02, mean val. rec. loss:  1.90008064e-02\n",
      "Epoch: 2171 mean train loss:  2.03955820e-02, mean val. rec. loss:  1.89966118e-02\n",
      "Epoch: 2172 mean train loss:  2.03915150e-02, mean val. rec. loss:  1.89924047e-02\n",
      "Epoch: 2173 mean train loss:  2.03874368e-02, mean val. rec. loss:  1.89882045e-02\n",
      "Epoch: 2174 mean train loss:  2.03833716e-02, mean val. rec. loss:  1.89840020e-02\n",
      "Epoch: 2175 mean train loss:  2.03792953e-02, mean val. rec. loss:  1.89797949e-02\n",
      "Epoch: 2176 mean train loss:  2.03752189e-02, mean val. rec. loss:  1.89756037e-02\n",
      "Epoch: 2177 mean train loss:  2.03711501e-02, mean val. rec. loss:  1.89713967e-02\n",
      "Epoch: 2178 mean train loss:  2.03670830e-02, mean val. rec. loss:  1.89671896e-02\n",
      "Epoch: 2179 mean train loss:  2.03630086e-02, mean val. rec. loss:  1.89629837e-02\n",
      "Epoch: 2180 mean train loss:  2.03589341e-02, mean val. rec. loss:  1.89587789e-02\n",
      "Epoch: 2181 mean train loss:  2.03548633e-02, mean val. rec. loss:  1.89545798e-02\n",
      "Epoch: 2182 mean train loss:  2.03507963e-02, mean val. rec. loss:  1.89503728e-02\n",
      "Epoch: 2183 mean train loss:  2.03467218e-02, mean val. rec. loss:  1.89461657e-02\n",
      "Epoch: 2184 mean train loss:  2.03426474e-02, mean val. rec. loss:  1.89419666e-02\n",
      "Epoch: 2185 mean train loss:  2.03385785e-02, mean val. rec. loss:  1.89377584e-02\n",
      "Epoch: 2186 mean train loss:  2.03345096e-02, mean val. rec. loss:  1.89335502e-02\n",
      "Epoch: 2187 mean train loss:  2.03304351e-02, mean val. rec. loss:  1.89293409e-02\n",
      "Epoch: 2188 mean train loss:  2.03263644e-02, mean val. rec. loss:  1.89251304e-02\n",
      "Epoch: 2189 mean train loss:  2.03222936e-02, mean val. rec. loss:  1.89209302e-02\n",
      "Epoch: 2190 mean train loss:  2.03182266e-02, mean val. rec. loss:  1.89167209e-02\n",
      "Epoch: 2191 mean train loss:  2.03141521e-02, mean val. rec. loss:  1.89125104e-02\n",
      "Epoch: 2192 mean train loss:  2.03100777e-02, mean val. rec. loss:  1.89083113e-02\n",
      "Epoch: 2193 mean train loss:  2.03060106e-02, mean val. rec. loss:  1.89040974e-02\n",
      "Epoch: 2194 mean train loss:  2.03019436e-02, mean val. rec. loss:  1.88998881e-02\n",
      "Epoch: 2195 mean train loss:  2.02978710e-02, mean val. rec. loss:  1.88956743e-02\n",
      "Epoch: 2196 mean train loss:  2.02937965e-02, mean val. rec. loss:  1.88914638e-02\n",
      "Epoch: 2197 mean train loss:  2.02897295e-02, mean val. rec. loss:  1.88872624e-02\n",
      "Epoch: 2198 mean train loss:  2.02856643e-02, mean val. rec. loss:  1.88830486e-02\n",
      "Epoch: 2199 mean train loss:  2.02815899e-02, mean val. rec. loss:  1.88788336e-02\n",
      "Epoch: 2200 mean train loss:  2.02775154e-02, mean val. rec. loss:  1.88746333e-02\n",
      "Epoch: 2201 mean train loss:  2.02734484e-02, mean val. rec. loss:  1.88704229e-02\n",
      "Epoch: 2202 mean train loss:  2.02693814e-02, mean val. rec. loss:  1.88662124e-02\n",
      "Epoch: 2203 mean train loss:  2.02653087e-02, mean val. rec. loss:  1.88620020e-02\n",
      "Epoch: 2204 mean train loss:  2.02612399e-02, mean val. rec. loss:  1.88577858e-02\n",
      "Epoch: 2205 mean train loss:  2.02571654e-02, mean val. rec. loss:  1.88535742e-02\n",
      "Epoch: 2206 mean train loss:  2.02530984e-02, mean val. rec. loss:  1.88493604e-02\n",
      "Epoch: 2207 mean train loss:  2.02490239e-02, mean val. rec. loss:  1.88451511e-02\n",
      "Epoch: 2208 mean train loss:  2.02449531e-02, mean val. rec. loss:  1.88409372e-02\n",
      "Epoch: 2209 mean train loss:  2.02408824e-02, mean val. rec. loss:  1.88367234e-02\n",
      "Epoch: 2210 mean train loss:  2.02368135e-02, mean val. rec. loss:  1.88325095e-02\n",
      "Epoch: 2211 mean train loss:  2.02327428e-02, mean val. rec. loss:  1.88282956e-02\n",
      "Epoch: 2212 mean train loss:  2.02286739e-02, mean val. rec. loss:  1.88240772e-02\n",
      "Epoch: 2213 mean train loss:  2.02246013e-02, mean val. rec. loss:  1.88198611e-02\n",
      "Epoch: 2214 mean train loss:  2.02205324e-02, mean val. rec. loss:  1.88156461e-02\n",
      "Epoch: 2215 mean train loss:  2.02164616e-02, mean val. rec. loss:  1.88114311e-02\n",
      "Epoch: 2216 mean train loss:  2.02123909e-02, mean val. rec. loss:  1.88072139e-02\n",
      "Epoch: 2217 mean train loss:  2.02083201e-02, mean val. rec. loss:  1.88029944e-02\n",
      "Epoch: 2218 mean train loss:  2.02042494e-02, mean val. rec. loss:  1.87987805e-02\n",
      "Epoch: 2219 mean train loss:  2.02001824e-02, mean val. rec. loss:  1.87945621e-02\n",
      "Epoch: 2220 mean train loss:  2.01961097e-02, mean val. rec. loss:  1.87903448e-02\n",
      "Epoch: 2221 mean train loss:  2.01920409e-02, mean val. rec. loss:  1.87861265e-02\n",
      "Epoch: 2222 mean train loss:  2.01879738e-02, mean val. rec. loss:  1.87819081e-02\n",
      "Epoch: 2223 mean train loss:  2.01839012e-02, mean val. rec. loss:  1.87776874e-02\n",
      "Epoch: 2224 mean train loss:  2.01798323e-02, mean val. rec. loss:  1.87734701e-02\n",
      "Epoch: 2225 mean train loss:  2.01757616e-02, mean val. rec. loss:  1.87692517e-02\n",
      "Epoch: 2226 mean train loss:  2.01716946e-02, mean val. rec. loss:  1.87650197e-02\n",
      "Epoch: 2227 mean train loss:  2.01676201e-02, mean val. rec. loss:  1.87608070e-02\n",
      "Epoch: 2228 mean train loss:  2.01635549e-02, mean val. rec. loss:  1.87565875e-02\n",
      "Epoch: 2229 mean train loss:  2.01594860e-02, mean val. rec. loss:  1.87523612e-02\n",
      "Epoch: 2230 mean train loss:  2.01554134e-02, mean val. rec. loss:  1.87481416e-02\n",
      "Epoch: 2231 mean train loss:  2.01513390e-02, mean val. rec. loss:  1.87439255e-02\n",
      "Epoch: 2232 mean train loss:  2.01472738e-02, mean val. rec. loss:  1.87397037e-02\n",
      "Epoch: 2233 mean train loss:  2.01432068e-02, mean val. rec. loss:  1.87354865e-02\n",
      "Epoch: 2234 mean train loss:  2.01391323e-02, mean val. rec. loss:  1.87312567e-02\n",
      "Epoch: 2235 mean train loss:  2.01350597e-02, mean val. rec. loss:  1.87270372e-02\n",
      "Epoch: 2236 mean train loss:  2.01309945e-02, mean val. rec. loss:  1.87228154e-02\n",
      "Epoch: 2237 mean train loss:  2.01269275e-02, mean val. rec. loss:  1.87185834e-02\n",
      "Epoch: 2238 mean train loss:  2.01228549e-02, mean val. rec. loss:  1.87143661e-02\n",
      "Epoch: 2239 mean train loss:  2.01187823e-02, mean val. rec. loss:  1.87101444e-02\n",
      "Epoch: 2240 mean train loss:  2.01147153e-02, mean val. rec. loss:  1.87059214e-02\n",
      "Epoch: 2241 mean train loss:  2.01106482e-02, mean val. rec. loss:  1.87017008e-02\n",
      "Epoch: 2242 mean train loss:  2.01065756e-02, mean val. rec. loss:  1.86974642e-02\n",
      "Epoch: 2243 mean train loss:  2.01025030e-02, mean val. rec. loss:  1.86932436e-02\n",
      "Epoch: 2244 mean train loss:  2.00984378e-02, mean val. rec. loss:  1.86890184e-02\n",
      "Epoch: 2245 mean train loss:  2.00943690e-02, mean val. rec. loss:  1.86847864e-02\n",
      "Epoch: 2246 mean train loss:  2.00902982e-02, mean val. rec. loss:  1.86805612e-02\n",
      "Epoch: 2247 mean train loss:  2.00862237e-02, mean val. rec. loss:  1.86763360e-02\n",
      "Epoch: 2248 mean train loss:  2.00821586e-02, mean val. rec. loss:  1.86721085e-02\n",
      "Epoch: 2249 mean train loss:  2.00780916e-02, mean val. rec. loss:  1.86678788e-02\n",
      "Epoch: 2250 mean train loss:  2.00740152e-02, mean val. rec. loss:  1.86636581e-02\n",
      "Epoch: 2251 mean train loss:  2.00699538e-02, mean val. rec. loss:  1.86594227e-02\n",
      "Epoch: 2252 mean train loss:  2.00658737e-02, mean val. rec. loss:  1.86551964e-02\n",
      "Epoch: 2253 mean train loss:  2.00618123e-02, mean val. rec. loss:  1.86509644e-02\n",
      "Epoch: 2254 mean train loss:  2.00577322e-02, mean val. rec. loss:  1.86467392e-02\n",
      "Epoch: 2255 mean train loss:  2.00536708e-02, mean val. rec. loss:  1.86425026e-02\n",
      "Epoch: 2256 mean train loss:  2.00495945e-02, mean val. rec. loss:  1.86382786e-02\n",
      "Epoch: 2257 mean train loss:  2.00455293e-02, mean val. rec. loss:  1.86340432e-02\n",
      "Epoch: 2258 mean train loss:  2.00414492e-02, mean val. rec. loss:  1.86298157e-02\n",
      "Epoch: 2259 mean train loss:  2.00373878e-02, mean val. rec. loss:  1.86255814e-02\n",
      "Epoch: 2260 mean train loss:  2.00333115e-02, mean val. rec. loss:  1.86213551e-02\n",
      "Epoch: 2261 mean train loss:  2.00292482e-02, mean val. rec. loss:  1.86171197e-02\n",
      "Epoch: 2262 mean train loss:  2.00251737e-02, mean val. rec. loss:  1.86128911e-02\n",
      "Epoch: 2263 mean train loss:  2.00211104e-02, mean val. rec. loss:  1.86086534e-02\n",
      "Epoch: 2264 mean train loss:  2.00170359e-02, mean val. rec. loss:  1.86044260e-02\n",
      "Epoch: 2265 mean train loss:  2.00129726e-02, mean val. rec. loss:  1.86001883e-02\n",
      "Epoch: 2266 mean train loss:  2.00088944e-02, mean val. rec. loss:  1.85959586e-02\n",
      "Epoch: 2267 mean train loss:  2.00048311e-02, mean val. rec. loss:  1.85917186e-02\n",
      "Epoch: 2268 mean train loss:  2.00007566e-02, mean val. rec. loss:  1.85874923e-02\n",
      "Epoch: 2269 mean train loss:  1.99966933e-02, mean val. rec. loss:  1.85832490e-02\n",
      "Epoch: 2270 mean train loss:  1.99926170e-02, mean val. rec. loss:  1.85790204e-02\n",
      "Epoch: 2271 mean train loss:  1.99885537e-02, mean val. rec. loss:  1.85747804e-02\n",
      "Epoch: 2272 mean train loss:  1.99844774e-02, mean val. rec. loss:  1.85705416e-02\n",
      "Epoch: 2273 mean train loss:  1.99804048e-02, mean val. rec. loss:  1.85663040e-02\n",
      "Epoch: 2274 mean train loss:  1.99763377e-02, mean val. rec. loss:  1.85620742e-02\n",
      "Epoch: 2275 mean train loss:  1.99722707e-02, mean val. rec. loss:  1.85578377e-02\n",
      "Epoch: 2276 mean train loss:  1.99681981e-02, mean val. rec. loss:  1.85535943e-02\n",
      "Epoch: 2277 mean train loss:  1.99641236e-02, mean val. rec. loss:  1.85493669e-02\n",
      "Epoch: 2278 mean train loss:  1.99600547e-02, mean val. rec. loss:  1.85451281e-02\n",
      "Epoch: 2279 mean train loss:  1.99559896e-02, mean val. rec. loss:  1.85408870e-02\n",
      "Epoch: 2280 mean train loss:  1.99519151e-02, mean val. rec. loss:  1.85366437e-02\n",
      "Epoch: 2281 mean train loss:  1.99478425e-02, mean val. rec. loss:  1.85324049e-02\n",
      "Epoch: 2282 mean train loss:  1.99437755e-02, mean val. rec. loss:  1.85281706e-02\n",
      "Epoch: 2283 mean train loss:  1.99397085e-02, mean val. rec. loss:  1.85239295e-02\n",
      "Epoch: 2284 mean train loss:  1.99356358e-02, mean val. rec. loss:  1.85196862e-02\n",
      "Epoch: 2285 mean train loss:  1.99315614e-02, mean val. rec. loss:  1.85154542e-02\n",
      "Epoch: 2286 mean train loss:  1.99274925e-02, mean val. rec. loss:  1.85112086e-02\n",
      "Epoch: 2287 mean train loss:  1.99234273e-02, mean val. rec. loss:  1.85069652e-02\n",
      "Epoch: 2288 mean train loss:  1.99193528e-02, mean val. rec. loss:  1.85027185e-02\n",
      "Epoch: 2289 mean train loss:  1.99152784e-02, mean val. rec. loss:  1.84984740e-02\n",
      "Epoch: 2290 mean train loss:  1.99112114e-02, mean val. rec. loss:  1.84942397e-02\n",
      "Epoch: 2291 mean train loss:  1.99071443e-02, mean val. rec. loss:  1.84899964e-02\n",
      "Epoch: 2292 mean train loss:  1.99030736e-02, mean val. rec. loss:  1.84857531e-02\n",
      "Epoch: 2293 mean train loss:  1.98990028e-02, mean val. rec. loss:  1.84815097e-02\n",
      "Epoch: 2294 mean train loss:  1.98949321e-02, mean val. rec. loss:  1.84772664e-02\n",
      "Epoch: 2295 mean train loss:  1.98908613e-02, mean val. rec. loss:  1.84730253e-02\n",
      "Epoch: 2296 mean train loss:  1.98867906e-02, mean val. rec. loss:  1.84687797e-02\n",
      "Epoch: 2297 mean train loss:  1.98827198e-02, mean val. rec. loss:  1.84645364e-02\n",
      "Epoch: 2298 mean train loss:  1.98786491e-02, mean val. rec. loss:  1.84602908e-02\n",
      "Epoch: 2299 mean train loss:  1.98745783e-02, mean val. rec. loss:  1.84560429e-02\n",
      "Epoch: 2300 mean train loss:  1.98705076e-02, mean val. rec. loss:  1.84517973e-02\n",
      "Epoch: 2301 mean train loss:  1.98664368e-02, mean val. rec. loss:  1.84475517e-02\n",
      "Epoch: 2302 mean train loss:  1.98623661e-02, mean val. rec. loss:  1.84433061e-02\n",
      "Epoch: 2303 mean train loss:  1.98582972e-02, mean val. rec. loss:  1.84390593e-02\n",
      "Epoch: 2304 mean train loss:  1.98542246e-02, mean val. rec. loss:  1.84348126e-02\n",
      "Epoch: 2305 mean train loss:  1.98501538e-02, mean val. rec. loss:  1.84305647e-02\n",
      "Epoch: 2306 mean train loss:  1.98460868e-02, mean val. rec. loss:  1.84263145e-02\n",
      "Epoch: 2307 mean train loss:  1.98420142e-02, mean val. rec. loss:  1.84220655e-02\n",
      "Epoch: 2308 mean train loss:  1.98379453e-02, mean val. rec. loss:  1.84178165e-02\n",
      "Epoch: 2309 mean train loss:  1.98338746e-02, mean val. rec. loss:  1.84135664e-02\n",
      "Epoch: 2310 mean train loss:  1.98298020e-02, mean val. rec. loss:  1.84093162e-02\n",
      "Epoch: 2311 mean train loss:  1.98257331e-02, mean val. rec. loss:  1.84050616e-02\n",
      "Epoch: 2312 mean train loss:  1.98216586e-02, mean val. rec. loss:  1.84008103e-02\n",
      "Epoch: 2313 mean train loss:  1.98175823e-02, mean val. rec. loss:  1.83965669e-02\n",
      "Epoch: 2314 mean train loss:  1.98135171e-02, mean val. rec. loss:  1.83923179e-02\n",
      "Epoch: 2315 mean train loss:  1.98094482e-02, mean val. rec. loss:  1.83880723e-02\n",
      "Epoch: 2316 mean train loss:  1.98053719e-02, mean val. rec. loss:  1.83838097e-02\n",
      "Epoch: 2317 mean train loss:  1.98013011e-02, mean val. rec. loss:  1.83795618e-02\n",
      "Epoch: 2318 mean train loss:  1.97972304e-02, mean val. rec. loss:  1.83753128e-02\n",
      "Epoch: 2319 mean train loss:  1.97931634e-02, mean val. rec. loss:  1.83710502e-02\n",
      "Epoch: 2320 mean train loss:  1.97890908e-02, mean val. rec. loss:  1.83668012e-02\n",
      "Epoch: 2321 mean train loss:  1.97850144e-02, mean val. rec. loss:  1.83625533e-02\n",
      "Epoch: 2322 mean train loss:  1.97809474e-02, mean val. rec. loss:  1.83582998e-02\n",
      "Epoch: 2323 mean train loss:  1.97768804e-02, mean val. rec. loss:  1.83540508e-02\n",
      "Epoch: 2324 mean train loss:  1.97728078e-02, mean val. rec. loss:  1.83497870e-02\n",
      "Epoch: 2325 mean train loss:  1.97687352e-02, mean val. rec. loss:  1.83455369e-02\n",
      "Epoch: 2326 mean train loss:  1.97646644e-02, mean val. rec. loss:  1.83412822e-02\n",
      "Epoch: 2327 mean train loss:  1.97605992e-02, mean val. rec. loss:  1.83370207e-02\n",
      "Epoch: 2328 mean train loss:  1.97565266e-02, mean val. rec. loss:  1.83327683e-02\n",
      "Epoch: 2329 mean train loss:  1.97524522e-02, mean val. rec. loss:  1.83285147e-02\n",
      "Epoch: 2330 mean train loss:  1.97483851e-02, mean val. rec. loss:  1.83242544e-02\n",
      "Epoch: 2331 mean train loss:  1.97443144e-02, mean val. rec. loss:  1.83200042e-02\n",
      "Epoch: 2332 mean train loss:  1.97402418e-02, mean val. rec. loss:  1.83157541e-02\n",
      "Epoch: 2333 mean train loss:  1.97361692e-02, mean val. rec. loss:  1.83114903e-02\n",
      "Epoch: 2334 mean train loss:  1.97320984e-02, mean val. rec. loss:  1.83072266e-02\n",
      "Epoch: 2335 mean train loss:  1.97280277e-02, mean val. rec. loss:  1.83029753e-02\n",
      "Epoch: 2336 mean train loss:  1.97239569e-02, mean val. rec. loss:  1.82987240e-02\n",
      "Epoch: 2337 mean train loss:  1.97198843e-02, mean val. rec. loss:  1.82944580e-02\n",
      "Epoch: 2338 mean train loss:  1.97158136e-02, mean val. rec. loss:  1.82901943e-02\n",
      "Epoch: 2339 mean train loss:  1.97117410e-02, mean val. rec. loss:  1.82859396e-02\n",
      "Epoch: 2340 mean train loss:  1.97076721e-02, mean val. rec. loss:  1.82816849e-02\n",
      "Epoch: 2341 mean train loss:  1.97035995e-02, mean val. rec. loss:  1.82774200e-02\n",
      "Epoch: 2342 mean train loss:  1.96995287e-02, mean val. rec. loss:  1.82731551e-02\n",
      "Epoch: 2343 mean train loss:  1.96954580e-02, mean val. rec. loss:  1.82689005e-02\n",
      "Epoch: 2344 mean train loss:  1.96913872e-02, mean val. rec. loss:  1.82646446e-02\n",
      "Epoch: 2345 mean train loss:  1.96873165e-02, mean val. rec. loss:  1.82603775e-02\n",
      "Epoch: 2346 mean train loss:  1.96832457e-02, mean val. rec. loss:  1.82561069e-02\n",
      "Epoch: 2347 mean train loss:  1.96791750e-02, mean val. rec. loss:  1.82518522e-02\n",
      "Epoch: 2348 mean train loss:  1.96751042e-02, mean val. rec. loss:  1.82475840e-02\n",
      "Epoch: 2349 mean train loss:  1.96710279e-02, mean val. rec. loss:  1.82433349e-02\n",
      "Epoch: 2350 mean train loss:  1.96669609e-02, mean val. rec. loss:  1.82390678e-02\n",
      "Epoch: 2351 mean train loss:  1.96628938e-02, mean val. rec. loss:  1.82348040e-02\n",
      "Epoch: 2352 mean train loss:  1.96588175e-02, mean val. rec. loss:  1.82305380e-02\n",
      "Epoch: 2353 mean train loss:  1.96547449e-02, mean val. rec. loss:  1.82262720e-02\n",
      "Epoch: 2354 mean train loss:  1.96506760e-02, mean val. rec. loss:  1.82220139e-02\n",
      "Epoch: 2355 mean train loss:  1.96466090e-02, mean val. rec. loss:  1.82177513e-02\n",
      "Epoch: 2356 mean train loss:  1.96425345e-02, mean val. rec. loss:  1.82134830e-02\n",
      "Epoch: 2357 mean train loss:  1.96384582e-02, mean val. rec. loss:  1.82092272e-02\n",
      "Epoch: 2358 mean train loss:  1.96343930e-02, mean val. rec. loss:  1.82049578e-02\n",
      "Epoch: 2359 mean train loss:  1.96303241e-02, mean val. rec. loss:  1.82006918e-02\n",
      "Epoch: 2360 mean train loss:  1.96262515e-02, mean val. rec. loss:  1.81964189e-02\n",
      "Epoch: 2361 mean train loss:  1.96221770e-02, mean val. rec. loss:  1.81921506e-02\n",
      "Epoch: 2362 mean train loss:  1.96181100e-02, mean val. rec. loss:  1.81878937e-02\n",
      "Epoch: 2363 mean train loss:  1.96140430e-02, mean val. rec. loss:  1.81836231e-02\n",
      "Epoch: 2364 mean train loss:  1.96099685e-02, mean val. rec. loss:  1.81793549e-02\n",
      "Epoch: 2365 mean train loss:  1.96058959e-02, mean val. rec. loss:  1.81750968e-02\n",
      "Epoch: 2366 mean train loss:  1.96018270e-02, mean val. rec. loss:  1.81708296e-02\n",
      "Epoch: 2367 mean train loss:  1.95977563e-02, mean val. rec. loss:  1.81665636e-02\n",
      "Epoch: 2368 mean train loss:  1.95936855e-02, mean val. rec. loss:  1.81622953e-02\n",
      "Epoch: 2369 mean train loss:  1.95896148e-02, mean val. rec. loss:  1.81580248e-02\n",
      "Epoch: 2370 mean train loss:  1.95855440e-02, mean val. rec. loss:  1.81537565e-02\n",
      "Epoch: 2371 mean train loss:  1.95814733e-02, mean val. rec. loss:  1.81494893e-02\n",
      "Epoch: 2372 mean train loss:  1.95774025e-02, mean val. rec. loss:  1.81452210e-02\n",
      "Epoch: 2373 mean train loss:  1.95733318e-02, mean val. rec. loss:  1.81409493e-02\n",
      "Epoch: 2374 mean train loss:  1.95692610e-02, mean val. rec. loss:  1.81366810e-02\n",
      "Epoch: 2375 mean train loss:  1.95651903e-02, mean val. rec. loss:  1.81324116e-02\n",
      "Epoch: 2376 mean train loss:  1.95611214e-02, mean val. rec. loss:  1.81281399e-02\n",
      "Epoch: 2377 mean train loss:  1.95570488e-02, mean val. rec. loss:  1.81238682e-02\n",
      "Epoch: 2378 mean train loss:  1.95529780e-02, mean val. rec. loss:  1.81195988e-02\n",
      "Epoch: 2379 mean train loss:  1.95489073e-02, mean val. rec. loss:  1.81153249e-02\n",
      "Epoch: 2380 mean train loss:  1.95448365e-02, mean val. rec. loss:  1.81110532e-02\n",
      "Epoch: 2381 mean train loss:  1.95407677e-02, mean val. rec. loss:  1.81067815e-02\n",
      "Epoch: 2382 mean train loss:  1.95366969e-02, mean val. rec. loss:  1.81025075e-02\n",
      "Epoch: 2383 mean train loss:  1.95326243e-02, mean val. rec. loss:  1.80982268e-02\n",
      "Epoch: 2384 mean train loss:  1.95285517e-02, mean val. rec. loss:  1.80939551e-02\n",
      "Epoch: 2385 mean train loss:  1.95244828e-02, mean val. rec. loss:  1.80896868e-02\n",
      "Epoch: 2386 mean train loss:  1.95204176e-02, mean val. rec. loss:  1.80854049e-02\n",
      "Epoch: 2387 mean train loss:  1.95163432e-02, mean val. rec. loss:  1.80811343e-02\n",
      "Epoch: 2388 mean train loss:  1.95122706e-02, mean val. rec. loss:  1.80768626e-02\n",
      "Epoch: 2389 mean train loss:  1.95082017e-02, mean val. rec. loss:  1.80725910e-02\n",
      "Epoch: 2390 mean train loss:  1.95041328e-02, mean val. rec. loss:  1.80683227e-02\n",
      "Epoch: 2391 mean train loss:  1.95000602e-02, mean val. rec. loss:  1.80640362e-02\n",
      "Epoch: 2392 mean train loss:  1.94959857e-02, mean val. rec. loss:  1.80597657e-02\n",
      "Epoch: 2393 mean train loss:  1.94919205e-02, mean val. rec. loss:  1.80554929e-02\n",
      "Epoch: 2394 mean train loss:  1.94878535e-02, mean val. rec. loss:  1.80512087e-02\n",
      "Epoch: 2395 mean train loss:  1.94837809e-02, mean val. rec. loss:  1.80469359e-02\n",
      "Epoch: 2396 mean train loss:  1.94797083e-02, mean val. rec. loss:  1.80426630e-02\n",
      "Epoch: 2397 mean train loss:  1.94756413e-02, mean val. rec. loss:  1.80383891e-02\n",
      "Epoch: 2398 mean train loss:  1.94715742e-02, mean val. rec. loss:  1.80341163e-02\n",
      "Epoch: 2399 mean train loss:  1.94675016e-02, mean val. rec. loss:  1.80298276e-02\n",
      "Epoch: 2400 mean train loss:  1.94634290e-02, mean val. rec. loss:  1.80255547e-02\n",
      "Epoch: 2401 mean train loss:  1.94593657e-02, mean val. rec. loss:  1.80212740e-02\n",
      "Epoch: 2402 mean train loss:  1.94552875e-02, mean val. rec. loss:  1.80170023e-02\n",
      "Epoch: 2403 mean train loss:  1.94512242e-02, mean val. rec. loss:  1.80127181e-02\n",
      "Epoch: 2404 mean train loss:  1.94471479e-02, mean val. rec. loss:  1.80084464e-02\n",
      "Epoch: 2405 mean train loss:  1.94430827e-02, mean val. rec. loss:  1.80041600e-02\n",
      "Epoch: 2406 mean train loss:  1.94390083e-02, mean val. rec. loss:  1.79998872e-02\n",
      "Epoch: 2407 mean train loss:  1.94349450e-02, mean val. rec. loss:  1.79956042e-02\n",
      "Epoch: 2408 mean train loss:  1.94308668e-02, mean val. rec. loss:  1.79913302e-02\n",
      "Epoch: 2409 mean train loss:  1.94268072e-02, mean val. rec. loss:  1.79870460e-02\n",
      "Epoch: 2410 mean train loss:  1.94227290e-02, mean val. rec. loss:  1.79827698e-02\n",
      "Epoch: 2411 mean train loss:  1.94186657e-02, mean val. rec. loss:  1.79784845e-02\n",
      "Epoch: 2412 mean train loss:  1.94145912e-02, mean val. rec. loss:  1.79742117e-02\n",
      "Epoch: 2413 mean train loss:  1.94105298e-02, mean val. rec. loss:  1.79699253e-02\n",
      "Epoch: 2414 mean train loss:  1.94064534e-02, mean val. rec. loss:  1.79656479e-02\n",
      "Epoch: 2415 mean train loss:  1.94023901e-02, mean val. rec. loss:  1.79613615e-02\n",
      "Epoch: 2416 mean train loss:  1.93983157e-02, mean val. rec. loss:  1.79570796e-02\n",
      "Epoch: 2417 mean train loss:  1.93942505e-02, mean val. rec. loss:  1.79528056e-02\n",
      "Epoch: 2418 mean train loss:  1.93901853e-02, mean val. rec. loss:  1.79485226e-02\n",
      "Epoch: 2419 mean train loss:  1.93861109e-02, mean val. rec. loss:  1.79442373e-02\n",
      "Epoch: 2420 mean train loss:  1.93820383e-02, mean val. rec. loss:  1.79399656e-02\n",
      "Epoch: 2421 mean train loss:  1.93779731e-02, mean val. rec. loss:  1.79356792e-02\n",
      "Epoch: 2422 mean train loss:  1.93739098e-02, mean val. rec. loss:  1.79313961e-02\n",
      "Epoch: 2423 mean train loss:  1.93698372e-02, mean val. rec. loss:  1.79271108e-02\n",
      "Epoch: 2424 mean train loss:  1.93657646e-02, mean val. rec. loss:  1.79228255e-02\n",
      "Epoch: 2425 mean train loss:  1.93617013e-02, mean val. rec. loss:  1.79185493e-02\n",
      "Epoch: 2426 mean train loss:  1.93576343e-02, mean val. rec. loss:  1.79142651e-02\n",
      "Epoch: 2427 mean train loss:  1.93535635e-02, mean val. rec. loss:  1.79099787e-02\n",
      "Epoch: 2428 mean train loss:  1.93494928e-02, mean val. rec. loss:  1.79057059e-02\n",
      "Epoch: 2429 mean train loss:  1.93454295e-02, mean val. rec. loss:  1.79014149e-02\n",
      "Epoch: 2430 mean train loss:  1.93413662e-02, mean val. rec. loss:  1.78971308e-02\n",
      "Epoch: 2431 mean train loss:  1.93372954e-02, mean val. rec. loss:  1.78928421e-02\n",
      "Epoch: 2432 mean train loss:  1.93332247e-02, mean val. rec. loss:  1.78885602e-02\n",
      "Epoch: 2433 mean train loss:  1.93291558e-02, mean val. rec. loss:  1.78842737e-02\n",
      "Epoch: 2434 mean train loss:  1.93250888e-02, mean val. rec. loss:  1.78799918e-02\n",
      "Epoch: 2435 mean train loss:  1.93210217e-02, mean val. rec. loss:  1.78757054e-02\n",
      "Epoch: 2436 mean train loss:  1.93169528e-02, mean val. rec. loss:  1.78714224e-02\n",
      "Epoch: 2437 mean train loss:  1.93128858e-02, mean val. rec. loss:  1.78671393e-02\n",
      "Epoch: 2438 mean train loss:  1.93088188e-02, mean val. rec. loss:  1.78628518e-02\n",
      "Epoch: 2439 mean train loss:  1.93047518e-02, mean val. rec. loss:  1.78585665e-02\n",
      "Epoch: 2440 mean train loss:  1.93006847e-02, mean val. rec. loss:  1.78542801e-02\n",
      "Epoch: 2441 mean train loss:  1.92966177e-02, mean val. rec. loss:  1.78499936e-02\n",
      "Epoch: 2442 mean train loss:  1.92925526e-02, mean val. rec. loss:  1.78457072e-02\n",
      "Epoch: 2443 mean train loss:  1.92884874e-02, mean val. rec. loss:  1.78414208e-02\n",
      "Epoch: 2444 mean train loss:  1.92844204e-02, mean val. rec. loss:  1.78371343e-02\n",
      "Epoch: 2445 mean train loss:  1.92803533e-02, mean val. rec. loss:  1.78328479e-02\n",
      "Epoch: 2446 mean train loss:  1.92762863e-02, mean val. rec. loss:  1.78285592e-02\n",
      "Epoch: 2447 mean train loss:  1.92722230e-02, mean val. rec. loss:  1.78242773e-02\n",
      "Epoch: 2448 mean train loss:  1.92681597e-02, mean val. rec. loss:  1.78199909e-02\n",
      "Epoch: 2449 mean train loss:  1.92640927e-02, mean val. rec. loss:  1.78156954e-02\n",
      "Epoch: 2450 mean train loss:  1.92600257e-02, mean val. rec. loss:  1.78114112e-02\n",
      "Epoch: 2451 mean train loss:  1.92559568e-02, mean val. rec. loss:  1.78071270e-02\n",
      "Epoch: 2452 mean train loss:  1.92518954e-02, mean val. rec. loss:  1.78028406e-02\n",
      "Epoch: 2453 mean train loss:  1.92478321e-02, mean val. rec. loss:  1.77985553e-02\n",
      "Epoch: 2454 mean train loss:  1.92437632e-02, mean val. rec. loss:  1.77942575e-02\n",
      "Epoch: 2455 mean train loss:  1.92396943e-02, mean val. rec. loss:  1.77899722e-02\n",
      "Epoch: 2456 mean train loss:  1.92356328e-02, mean val. rec. loss:  1.77856835e-02\n",
      "Epoch: 2457 mean train loss:  1.92315714e-02, mean val. rec. loss:  1.77813926e-02\n",
      "Epoch: 2458 mean train loss:  1.92275044e-02, mean val. rec. loss:  1.77771016e-02\n",
      "Epoch: 2459 mean train loss:  1.92234355e-02, mean val. rec. loss:  1.77728163e-02\n",
      "Epoch: 2460 mean train loss:  1.92193741e-02, mean val. rec. loss:  1.77685299e-02\n",
      "Epoch: 2461 mean train loss:  1.92153163e-02, mean val. rec. loss:  1.77642446e-02\n",
      "Epoch: 2462 mean train loss:  1.92112475e-02, mean val. rec. loss:  1.77599468e-02\n",
      "Epoch: 2463 mean train loss:  1.92071804e-02, mean val. rec. loss:  1.77556615e-02\n",
      "Epoch: 2464 mean train loss:  1.92031246e-02, mean val. rec. loss:  1.77513671e-02\n",
      "Epoch: 2465 mean train loss:  1.91990538e-02, mean val. rec. loss:  1.77470830e-02\n",
      "Epoch: 2466 mean train loss:  1.91949961e-02, mean val. rec. loss:  1.77427897e-02\n",
      "Epoch: 2467 mean train loss:  1.91909272e-02, mean val. rec. loss:  1.77385056e-02\n",
      "Epoch: 2468 mean train loss:  1.91868714e-02, mean val. rec. loss:  1.77342101e-02\n",
      "Epoch: 2469 mean train loss:  1.91828006e-02, mean val. rec. loss:  1.77299259e-02\n",
      "Epoch: 2470 mean train loss:  1.91787466e-02, mean val. rec. loss:  1.77256304e-02\n",
      "Epoch: 2471 mean train loss:  1.91746759e-02, mean val. rec. loss:  1.77213451e-02\n",
      "Epoch: 2472 mean train loss:  1.91706200e-02, mean val. rec. loss:  1.77170462e-02\n",
      "Epoch: 2473 mean train loss:  1.91665493e-02, mean val. rec. loss:  1.77127654e-02\n",
      "Epoch: 2474 mean train loss:  1.91624972e-02, mean val. rec. loss:  1.77084711e-02\n",
      "Epoch: 2475 mean train loss:  1.91584283e-02, mean val. rec. loss:  1.77041824e-02\n",
      "Epoch: 2476 mean train loss:  1.91543743e-02, mean val. rec. loss:  1.76998891e-02\n",
      "Epoch: 2477 mean train loss:  1.91503054e-02, mean val. rec. loss:  1.76956004e-02\n",
      "Epoch: 2478 mean train loss:  1.91462533e-02, mean val. rec. loss:  1.76913106e-02\n",
      "Epoch: 2479 mean train loss:  1.91421881e-02, mean val. rec. loss:  1.76870162e-02\n",
      "Epoch: 2480 mean train loss:  1.91381230e-02, mean val. rec. loss:  1.76827230e-02\n",
      "Epoch: 2481 mean train loss:  1.91340652e-02, mean val. rec. loss:  1.76784411e-02\n",
      "Epoch: 2482 mean train loss:  1.91300094e-02, mean val. rec. loss:  1.76741479e-02\n",
      "Epoch: 2483 mean train loss:  1.91259442e-02, mean val. rec. loss:  1.76698524e-02\n",
      "Epoch: 2484 mean train loss:  1.91218809e-02, mean val. rec. loss:  1.76655727e-02\n",
      "Epoch: 2485 mean train loss:  1.91178251e-02, mean val. rec. loss:  1.76612772e-02\n",
      "Epoch: 2486 mean train loss:  1.91137692e-02, mean val. rec. loss:  1.76569840e-02\n",
      "Epoch: 2487 mean train loss:  1.91097078e-02, mean val. rec. loss:  1.76526896e-02\n",
      "Epoch: 2488 mean train loss:  1.91056464e-02, mean val. rec. loss:  1.76483953e-02\n",
      "Epoch: 2489 mean train loss:  1.91015905e-02, mean val. rec. loss:  1.76441111e-02\n",
      "Epoch: 2490 mean train loss:  1.90975365e-02, mean val. rec. loss:  1.76398167e-02\n",
      "Epoch: 2491 mean train loss:  1.90934751e-02, mean val. rec. loss:  1.76355224e-02\n",
      "Epoch: 2492 mean train loss:  1.90894136e-02, mean val. rec. loss:  1.76312314e-02\n",
      "Epoch: 2493 mean train loss:  1.90853541e-02, mean val. rec. loss:  1.76269404e-02\n",
      "Epoch: 2494 mean train loss:  1.90812945e-02, mean val. rec. loss:  1.76226483e-02\n",
      "Epoch: 2495 mean train loss:  1.90772368e-02, mean val. rec. loss:  1.76183540e-02\n",
      "Epoch: 2496 mean train loss:  1.90731809e-02, mean val. rec. loss:  1.76140630e-02\n",
      "Epoch: 2497 mean train loss:  1.90691232e-02, mean val. rec. loss:  1.76097709e-02\n",
      "Epoch: 2498 mean train loss:  1.90650655e-02, mean val. rec. loss:  1.76054811e-02\n",
      "Epoch: 2499 mean train loss:  1.90610078e-02, mean val. rec. loss:  1.76011855e-02\n",
      "Epoch: 2500 mean train loss:  1.90569501e-02, mean val. rec. loss:  1.75968946e-02\n",
      "Epoch: 2501 mean train loss:  1.90528942e-02, mean val. rec. loss:  1.75926013e-02\n",
      "Epoch: 2502 mean train loss:  1.90488384e-02, mean val. rec. loss:  1.75883081e-02\n",
      "Epoch: 2503 mean train loss:  1.90447825e-02, mean val. rec. loss:  1.75840160e-02\n",
      "Epoch: 2504 mean train loss:  1.90407267e-02, mean val. rec. loss:  1.75797216e-02\n",
      "Epoch: 2505 mean train loss:  1.90366745e-02, mean val. rec. loss:  1.75754307e-02\n",
      "Epoch: 2506 mean train loss:  1.90326168e-02, mean val. rec. loss:  1.75711363e-02\n",
      "Epoch: 2507 mean train loss:  1.90285628e-02, mean val. rec. loss:  1.75668476e-02\n",
      "Epoch: 2508 mean train loss:  1.90245107e-02, mean val. rec. loss:  1.75625578e-02\n",
      "Epoch: 2509 mean train loss:  1.90204586e-02, mean val. rec. loss:  1.75582577e-02\n",
      "Epoch: 2510 mean train loss:  1.90163990e-02, mean val. rec. loss:  1.75539668e-02\n",
      "Epoch: 2511 mean train loss:  1.90123413e-02, mean val. rec. loss:  1.75496781e-02\n",
      "Epoch: 2512 mean train loss:  1.90082929e-02, mean val. rec. loss:  1.75453848e-02\n",
      "Epoch: 2513 mean train loss:  1.90042408e-02, mean val. rec. loss:  1.75410995e-02\n",
      "Epoch: 2514 mean train loss:  1.90001849e-02, mean val. rec. loss:  1.75367972e-02\n",
      "Epoch: 2515 mean train loss:  1.89961272e-02, mean val. rec. loss:  1.75325097e-02\n",
      "Epoch: 2516 mean train loss:  1.89920788e-02, mean val. rec. loss:  1.75282176e-02\n",
      "Epoch: 2517 mean train loss:  1.89880304e-02, mean val. rec. loss:  1.75239187e-02\n",
      "Epoch: 2518 mean train loss:  1.89839764e-02, mean val. rec. loss:  1.75196288e-02\n",
      "Epoch: 2519 mean train loss:  1.89799206e-02, mean val. rec. loss:  1.75153390e-02\n",
      "Epoch: 2520 mean train loss:  1.89758740e-02, mean val. rec. loss:  1.75110492e-02\n",
      "Epoch: 2521 mean train loss:  1.89718256e-02, mean val. rec. loss:  1.75067525e-02\n",
      "Epoch: 2522 mean train loss:  1.89677698e-02, mean val. rec. loss:  1.75024683e-02\n",
      "Epoch: 2523 mean train loss:  1.89637251e-02, mean val. rec. loss:  1.74981694e-02\n",
      "Epoch: 2524 mean train loss:  1.89596674e-02, mean val. rec. loss:  1.74938841e-02\n",
      "Epoch: 2525 mean train loss:  1.89556246e-02, mean val. rec. loss:  1.74895886e-02\n",
      "Epoch: 2526 mean train loss:  1.89515687e-02, mean val. rec. loss:  1.74852999e-02\n",
      "Epoch: 2527 mean train loss:  1.89475240e-02, mean val. rec. loss:  1.74810022e-02\n",
      "Epoch: 2528 mean train loss:  1.89434682e-02, mean val. rec. loss:  1.74767180e-02\n",
      "Epoch: 2529 mean train loss:  1.89394310e-02, mean val. rec. loss:  1.74724191e-02\n",
      "Epoch: 2530 mean train loss:  1.89353732e-02, mean val. rec. loss:  1.74681338e-02\n",
      "Epoch: 2531 mean train loss:  1.89313304e-02, mean val. rec. loss:  1.74638360e-02\n",
      "Epoch: 2532 mean train loss:  1.89272783e-02, mean val. rec. loss:  1.74595507e-02\n",
      "Epoch: 2533 mean train loss:  1.89232373e-02, mean val. rec. loss:  1.74552541e-02\n",
      "Epoch: 2534 mean train loss:  1.89191834e-02, mean val. rec. loss:  1.74509688e-02\n",
      "Epoch: 2535 mean train loss:  1.89151443e-02, mean val. rec. loss:  1.74466767e-02\n",
      "Epoch: 2536 mean train loss:  1.89110959e-02, mean val. rec. loss:  1.74423823e-02\n",
      "Epoch: 2537 mean train loss:  1.89070437e-02, mean val. rec. loss:  1.74380891e-02\n",
      "Epoch: 2538 mean train loss:  1.89030028e-02, mean val. rec. loss:  1.74338061e-02\n",
      "Epoch: 2539 mean train loss:  1.88989600e-02, mean val. rec. loss:  1.74295151e-02\n",
      "Epoch: 2540 mean train loss:  1.88949116e-02, mean val. rec. loss:  1.74252196e-02\n",
      "Epoch: 2541 mean train loss:  1.88908613e-02, mean val. rec. loss:  1.74209377e-02\n",
      "Epoch: 2542 mean train loss:  1.88868204e-02, mean val. rec. loss:  1.74166445e-02\n",
      "Epoch: 2543 mean train loss:  1.88827813e-02, mean val. rec. loss:  1.74123524e-02\n",
      "Epoch: 2544 mean train loss:  1.88787347e-02, mean val. rec. loss:  1.74080580e-02\n",
      "Epoch: 2545 mean train loss:  1.88746863e-02, mean val. rec. loss:  1.74037670e-02\n",
      "Epoch: 2546 mean train loss:  1.88706491e-02, mean val. rec. loss:  1.73994840e-02\n",
      "Epoch: 2547 mean train loss:  1.88666119e-02, mean val. rec. loss:  1.73951919e-02\n",
      "Epoch: 2548 mean train loss:  1.88625672e-02, mean val. rec. loss:  1.73909032e-02\n",
      "Epoch: 2549 mean train loss:  1.88585244e-02, mean val. rec. loss:  1.73866145e-02\n",
      "Epoch: 2550 mean train loss:  1.88544816e-02, mean val. rec. loss:  1.73823258e-02\n",
      "Epoch: 2551 mean train loss:  1.88504406e-02, mean val. rec. loss:  1.73780360e-02\n",
      "Epoch: 2552 mean train loss:  1.88463996e-02, mean val. rec. loss:  1.73737484e-02\n",
      "Epoch: 2553 mean train loss:  1.88423606e-02, mean val. rec. loss:  1.73694608e-02\n",
      "Epoch: 2554 mean train loss:  1.88383196e-02, mean val. rec. loss:  1.73651721e-02\n",
      "Epoch: 2555 mean train loss:  1.88342805e-02, mean val. rec. loss:  1.73608834e-02\n",
      "Epoch: 2556 mean train loss:  1.88302414e-02, mean val. rec. loss:  1.73565936e-02\n",
      "Epoch: 2557 mean train loss:  1.88262023e-02, mean val. rec. loss:  1.73523049e-02\n",
      "Epoch: 2558 mean train loss:  1.88221651e-02, mean val. rec. loss:  1.73480185e-02\n",
      "Epoch: 2559 mean train loss:  1.88181279e-02, mean val. rec. loss:  1.73437298e-02\n",
      "Epoch: 2560 mean train loss:  1.88140906e-02, mean val. rec. loss:  1.73394411e-02\n",
      "Epoch: 2561 mean train loss:  1.88100534e-02, mean val. rec. loss:  1.73351546e-02\n",
      "Epoch: 2562 mean train loss:  1.88060162e-02, mean val. rec. loss:  1.73308682e-02\n",
      "Epoch: 2563 mean train loss:  1.88019827e-02, mean val. rec. loss:  1.73265750e-02\n",
      "Epoch: 2564 mean train loss:  1.87979454e-02, mean val. rec. loss:  1.73222919e-02\n",
      "Epoch: 2565 mean train loss:  1.87939045e-02, mean val. rec. loss:  1.73180078e-02\n",
      "Epoch: 2566 mean train loss:  1.87898710e-02, mean val. rec. loss:  1.73137225e-02\n",
      "Epoch: 2567 mean train loss:  1.87858412e-02, mean val. rec. loss:  1.73094383e-02\n",
      "Epoch: 2568 mean train loss:  1.87818040e-02, mean val. rec. loss:  1.73051451e-02\n",
      "Epoch: 2569 mean train loss:  1.87777668e-02, mean val. rec. loss:  1.73008632e-02\n",
      "Epoch: 2570 mean train loss:  1.87737351e-02, mean val. rec. loss:  1.72965790e-02\n",
      "Epoch: 2571 mean train loss:  1.87697072e-02, mean val. rec. loss:  1.72922858e-02\n",
      "Epoch: 2572 mean train loss:  1.87656700e-02, mean val. rec. loss:  1.72880028e-02\n",
      "Epoch: 2573 mean train loss:  1.87616365e-02, mean val. rec. loss:  1.72837220e-02\n",
      "Epoch: 2574 mean train loss:  1.87576067e-02, mean val. rec. loss:  1.72794378e-02\n",
      "Epoch: 2575 mean train loss:  1.87535806e-02, mean val. rec. loss:  1.72751491e-02\n",
      "Epoch: 2576 mean train loss:  1.87495415e-02, mean val. rec. loss:  1.72708718e-02\n",
      "Epoch: 2577 mean train loss:  1.87455173e-02, mean val. rec. loss:  1.72665819e-02\n",
      "Epoch: 2578 mean train loss:  1.87414838e-02, mean val. rec. loss:  1.72623046e-02\n",
      "Epoch: 2579 mean train loss:  1.87374578e-02, mean val. rec. loss:  1.72580170e-02\n",
      "Epoch: 2580 mean train loss:  1.87334243e-02, mean val. rec. loss:  1.72537374e-02\n",
      "Epoch: 2581 mean train loss:  1.87294019e-02, mean val. rec. loss:  1.72494510e-02\n",
      "Epoch: 2582 mean train loss:  1.87253666e-02, mean val. rec. loss:  1.72451736e-02\n",
      "Epoch: 2583 mean train loss:  1.87213480e-02, mean val. rec. loss:  1.72408849e-02\n",
      "Epoch: 2584 mean train loss:  1.87173126e-02, mean val. rec. loss:  1.72366098e-02\n",
      "Epoch: 2585 mean train loss:  1.87132940e-02, mean val. rec. loss:  1.72323211e-02\n",
      "Epoch: 2586 mean train loss:  1.87092605e-02, mean val. rec. loss:  1.72280437e-02\n",
      "Epoch: 2587 mean train loss:  1.87052437e-02, mean val. rec. loss:  1.72237573e-02\n",
      "Epoch: 2588 mean train loss:  1.87012121e-02, mean val. rec. loss:  1.72194800e-02\n",
      "Epoch: 2589 mean train loss:  1.86971954e-02, mean val. rec. loss:  1.72152015e-02\n",
      "Epoch: 2590 mean train loss:  1.86931674e-02, mean val. rec. loss:  1.72109162e-02\n",
      "Epoch: 2591 mean train loss:  1.86891395e-02, mean val. rec. loss:  1.72066343e-02\n",
      "Epoch: 2592 mean train loss:  1.86851209e-02, mean val. rec. loss:  1.72023637e-02\n",
      "Epoch: 2593 mean train loss:  1.86811004e-02, mean val. rec. loss:  1.71980829e-02\n",
      "Epoch: 2594 mean train loss:  1.86770744e-02, mean val. rec. loss:  1.71937999e-02\n",
      "Epoch: 2595 mean train loss:  1.86730521e-02, mean val. rec. loss:  1.71895316e-02\n",
      "Epoch: 2596 mean train loss:  1.86690334e-02, mean val. rec. loss:  1.71852463e-02\n",
      "Epoch: 2597 mean train loss:  1.86650186e-02, mean val. rec. loss:  1.71809712e-02\n",
      "Epoch: 2598 mean train loss:  1.86609944e-02, mean val. rec. loss:  1.71766871e-02\n",
      "Epoch: 2599 mean train loss:  1.86569720e-02, mean val. rec. loss:  1.71724075e-02\n",
      "Epoch: 2600 mean train loss:  1.86529590e-02, mean val. rec. loss:  1.71681358e-02\n",
      "Epoch: 2601 mean train loss:  1.86489441e-02, mean val. rec. loss:  1.71638561e-02\n",
      "Epoch: 2602 mean train loss:  1.86449218e-02, mean val. rec. loss:  1.71595833e-02\n",
      "Epoch: 2603 mean train loss:  1.86409069e-02, mean val. rec. loss:  1.71553071e-02\n",
      "Epoch: 2604 mean train loss:  1.86368883e-02, mean val. rec. loss:  1.71510309e-02\n",
      "Epoch: 2605 mean train loss:  1.86328734e-02, mean val. rec. loss:  1.71467546e-02\n",
      "Epoch: 2606 mean train loss:  1.86288567e-02, mean val. rec. loss:  1.71424784e-02\n",
      "Epoch: 2607 mean train loss:  1.86248418e-02, mean val. rec. loss:  1.71382056e-02\n",
      "Epoch: 2608 mean train loss:  1.86208269e-02, mean val. rec. loss:  1.71339328e-02\n",
      "Epoch: 2609 mean train loss:  1.86168139e-02, mean val. rec. loss:  1.71296577e-02\n",
      "Epoch: 2610 mean train loss:  1.86128009e-02, mean val. rec. loss:  1.71253848e-02\n",
      "Epoch: 2611 mean train loss:  1.86087897e-02, mean val. rec. loss:  1.71211120e-02\n",
      "Epoch: 2612 mean train loss:  1.86047804e-02, mean val. rec. loss:  1.71168392e-02\n",
      "Epoch: 2613 mean train loss:  1.86007655e-02, mean val. rec. loss:  1.71125686e-02\n",
      "Epoch: 2614 mean train loss:  1.85967562e-02, mean val. rec. loss:  1.71082947e-02\n",
      "Epoch: 2615 mean train loss:  1.85927469e-02, mean val. rec. loss:  1.71040253e-02\n",
      "Epoch: 2616 mean train loss:  1.85887395e-02, mean val. rec. loss:  1.70997456e-02\n",
      "Epoch: 2617 mean train loss:  1.85847284e-02, mean val. rec. loss:  1.70954796e-02\n",
      "Epoch: 2618 mean train loss:  1.85807153e-02, mean val. rec. loss:  1.70912136e-02\n",
      "Epoch: 2619 mean train loss:  1.85767116e-02, mean val. rec. loss:  1.70869464e-02\n",
      "Epoch: 2620 mean train loss:  1.85727098e-02, mean val. rec. loss:  1.70826804e-02\n",
      "Epoch: 2621 mean train loss:  1.85686986e-02, mean val. rec. loss:  1.70784031e-02\n",
      "Epoch: 2622 mean train loss:  1.85646893e-02, mean val. rec. loss:  1.70741382e-02\n",
      "Epoch: 2623 mean train loss:  1.85606875e-02, mean val. rec. loss:  1.70698722e-02\n",
      "Epoch: 2624 mean train loss:  1.85566838e-02, mean val. rec. loss:  1.70655959e-02\n",
      "Epoch: 2625 mean train loss:  1.85526763e-02, mean val. rec. loss:  1.70613310e-02\n",
      "Epoch: 2626 mean train loss:  1.85486708e-02, mean val. rec. loss:  1.70570673e-02\n",
      "Epoch: 2627 mean train loss:  1.85446726e-02, mean val. rec. loss:  1.70528103e-02\n",
      "Epoch: 2628 mean train loss:  1.85406689e-02, mean val. rec. loss:  1.70485375e-02\n",
      "Epoch: 2629 mean train loss:  1.85366671e-02, mean val. rec. loss:  1.70442692e-02\n",
      "Epoch: 2630 mean train loss:  1.85326671e-02, mean val. rec. loss:  1.70400077e-02\n",
      "Epoch: 2631 mean train loss:  1.85286652e-02, mean val. rec. loss:  1.70357485e-02\n",
      "Epoch: 2632 mean train loss:  1.85246653e-02, mean val. rec. loss:  1.70314814e-02\n",
      "Epoch: 2633 mean train loss:  1.85206653e-02, mean val. rec. loss:  1.70272120e-02\n",
      "Epoch: 2634 mean train loss:  1.85166690e-02, mean val. rec. loss:  1.70229527e-02\n",
      "Epoch: 2635 mean train loss:  1.85126690e-02, mean val. rec. loss:  1.70186981e-02\n",
      "Epoch: 2636 mean train loss:  1.85086728e-02, mean val. rec. loss:  1.70144298e-02\n",
      "Epoch: 2637 mean train loss:  1.85046765e-02, mean val. rec. loss:  1.70101615e-02\n",
      "Epoch: 2638 mean train loss:  1.85006821e-02, mean val. rec. loss:  1.70059057e-02\n",
      "Epoch: 2639 mean train loss:  1.84966859e-02, mean val. rec. loss:  1.70016521e-02\n",
      "Epoch: 2640 mean train loss:  1.84926915e-02, mean val. rec. loss:  1.69973850e-02\n",
      "Epoch: 2641 mean train loss:  1.84886989e-02, mean val. rec. loss:  1.69931246e-02\n",
      "Epoch: 2642 mean train loss:  1.84847027e-02, mean val. rec. loss:  1.69888631e-02\n",
      "Epoch: 2643 mean train loss:  1.84807064e-02, mean val. rec. loss:  1.69846017e-02\n",
      "Epoch: 2644 mean train loss:  1.84767176e-02, mean val. rec. loss:  1.69803526e-02\n",
      "Epoch: 2645 mean train loss:  1.84727325e-02, mean val. rec. loss:  1.69760923e-02\n",
      "Epoch: 2646 mean train loss:  1.84687362e-02, mean val. rec. loss:  1.69718331e-02\n",
      "Epoch: 2647 mean train loss:  1.84647437e-02, mean val. rec. loss:  1.69675875e-02\n",
      "Epoch: 2648 mean train loss:  1.84607586e-02, mean val. rec. loss:  1.69633260e-02\n",
      "Epoch: 2649 mean train loss:  1.84567735e-02, mean val. rec. loss:  1.69590713e-02\n",
      "Epoch: 2650 mean train loss:  1.84527828e-02, mean val. rec. loss:  1.69548121e-02\n",
      "Epoch: 2651 mean train loss:  1.84487959e-02, mean val. rec. loss:  1.69505551e-02\n",
      "Epoch: 2652 mean train loss:  1.84448108e-02, mean val. rec. loss:  1.69463084e-02\n",
      "Epoch: 2653 mean train loss:  1.84408294e-02, mean val. rec. loss:  1.69420571e-02\n",
      "Epoch: 2654 mean train loss:  1.84368481e-02, mean val. rec. loss:  1.69378058e-02\n",
      "Epoch: 2655 mean train loss:  1.84328630e-02, mean val. rec. loss:  1.69335546e-02\n",
      "Epoch: 2656 mean train loss:  1.84288798e-02, mean val. rec. loss:  1.69293044e-02\n",
      "Epoch: 2657 mean train loss:  1.84248965e-02, mean val. rec. loss:  1.69250565e-02\n",
      "Epoch: 2658 mean train loss:  1.84209170e-02, mean val. rec. loss:  1.69208064e-02\n",
      "Epoch: 2659 mean train loss:  1.84169357e-02, mean val. rec. loss:  1.69165585e-02\n",
      "Epoch: 2660 mean train loss:  1.84129562e-02, mean val. rec. loss:  1.69123084e-02\n",
      "Epoch: 2661 mean train loss:  1.84089767e-02, mean val. rec. loss:  1.69080639e-02\n",
      "Epoch: 2662 mean train loss:  1.84049990e-02, mean val. rec. loss:  1.69038160e-02\n",
      "Epoch: 2663 mean train loss:  1.84010233e-02, mean val. rec. loss:  1.68995693e-02\n",
      "Epoch: 2664 mean train loss:  1.83970456e-02, mean val. rec. loss:  1.68953237e-02\n",
      "Epoch: 2665 mean train loss:  1.83930698e-02, mean val. rec. loss:  1.68910735e-02\n",
      "Epoch: 2666 mean train loss:  1.83890903e-02, mean val. rec. loss:  1.68868313e-02\n",
      "Epoch: 2667 mean train loss:  1.83851108e-02, mean val. rec. loss:  1.68825891e-02\n",
      "Epoch: 2668 mean train loss:  1.83811425e-02, mean val. rec. loss:  1.68783492e-02\n",
      "Epoch: 2669 mean train loss:  1.83771723e-02, mean val. rec. loss:  1.68741149e-02\n",
      "Epoch: 2670 mean train loss:  1.83731947e-02, mean val. rec. loss:  1.68698614e-02\n",
      "Epoch: 2671 mean train loss:  1.83692226e-02, mean val. rec. loss:  1.68656248e-02\n",
      "Epoch: 2672 mean train loss:  1.83652580e-02, mean val. rec. loss:  1.68613860e-02\n",
      "Epoch: 2673 mean train loss:  1.83612916e-02, mean val. rec. loss:  1.68571393e-02\n",
      "Epoch: 2674 mean train loss:  1.83573177e-02, mean val. rec. loss:  1.68529016e-02\n",
      "Epoch: 2675 mean train loss:  1.83533456e-02, mean val. rec. loss:  1.68486673e-02\n",
      "Epoch: 2676 mean train loss:  1.83493810e-02, mean val. rec. loss:  1.68444319e-02\n",
      "Epoch: 2677 mean train loss:  1.83454201e-02, mean val. rec. loss:  1.68401988e-02\n",
      "Epoch: 2678 mean train loss:  1.83414518e-02, mean val. rec. loss:  1.68359577e-02\n",
      "Epoch: 2679 mean train loss:  1.83374872e-02, mean val. rec. loss:  1.68317167e-02\n",
      "Epoch: 2680 mean train loss:  1.83335244e-02, mean val. rec. loss:  1.68274869e-02\n",
      "Epoch: 2681 mean train loss:  1.83295598e-02, mean val. rec. loss:  1.68232617e-02\n",
      "Epoch: 2682 mean train loss:  1.83255990e-02, mean val. rec. loss:  1.68190207e-02\n",
      "Epoch: 2683 mean train loss:  1.83216344e-02, mean val. rec. loss:  1.68147841e-02\n",
      "Epoch: 2684 mean train loss:  1.83176772e-02, mean val. rec. loss:  1.68105578e-02\n",
      "Epoch: 2685 mean train loss:  1.83137145e-02, mean val. rec. loss:  1.68063326e-02\n",
      "Epoch: 2686 mean train loss:  1.83097592e-02, mean val. rec. loss:  1.68020972e-02\n",
      "Epoch: 2687 mean train loss:  1.83058020e-02, mean val. rec. loss:  1.67978607e-02\n",
      "Epoch: 2688 mean train loss:  1.83018430e-02, mean val. rec. loss:  1.67936377e-02\n",
      "Epoch: 2689 mean train loss:  1.82978877e-02, mean val. rec. loss:  1.67894159e-02\n",
      "Epoch: 2690 mean train loss:  1.82939324e-02, mean val. rec. loss:  1.67851851e-02\n",
      "Epoch: 2691 mean train loss:  1.82899808e-02, mean val. rec. loss:  1.67809576e-02\n",
      "Epoch: 2692 mean train loss:  1.82860256e-02, mean val. rec. loss:  1.67767256e-02\n",
      "Epoch: 2693 mean train loss:  1.82820665e-02, mean val. rec. loss:  1.67725015e-02\n",
      "Epoch: 2694 mean train loss:  1.82781187e-02, mean val. rec. loss:  1.67682854e-02\n",
      "Epoch: 2695 mean train loss:  1.82741708e-02, mean val. rec. loss:  1.67640591e-02\n",
      "Epoch: 2696 mean train loss:  1.82702193e-02, mean val. rec. loss:  1.67598327e-02\n",
      "Epoch: 2697 mean train loss:  1.82662658e-02, mean val. rec. loss:  1.67556200e-02\n",
      "Epoch: 2698 mean train loss:  1.82623199e-02, mean val. rec. loss:  1.67513948e-02\n",
      "Epoch: 2699 mean train loss:  1.82583776e-02, mean val. rec. loss:  1.67471742e-02\n",
      "Epoch: 2700 mean train loss:  1.82544279e-02, mean val. rec. loss:  1.67429524e-02\n",
      "Epoch: 2701 mean train loss:  1.82504801e-02, mean val. rec. loss:  1.67387306e-02\n",
      "Epoch: 2702 mean train loss:  1.82465397e-02, mean val. rec. loss:  1.67345145e-02\n",
      "Epoch: 2703 mean train loss:  1.82425955e-02, mean val. rec. loss:  1.67302983e-02\n",
      "Epoch: 2704 mean train loss:  1.82386533e-02, mean val. rec. loss:  1.67260822e-02\n",
      "Epoch: 2705 mean train loss:  1.82347110e-02, mean val. rec. loss:  1.67218695e-02\n",
      "Epoch: 2706 mean train loss:  1.82307706e-02, mean val. rec. loss:  1.67176545e-02\n",
      "Epoch: 2707 mean train loss:  1.82268302e-02, mean val. rec. loss:  1.67134384e-02\n",
      "Epoch: 2708 mean train loss:  1.82228898e-02, mean val. rec. loss:  1.67092302e-02\n",
      "Epoch: 2709 mean train loss:  1.82189532e-02, mean val. rec. loss:  1.67050175e-02\n",
      "Epoch: 2710 mean train loss:  1.82150165e-02, mean val. rec. loss:  1.67008059e-02\n",
      "Epoch: 2711 mean train loss:  1.82110798e-02, mean val. rec. loss:  1.66965965e-02\n",
      "Epoch: 2712 mean train loss:  1.82071432e-02, mean val. rec. loss:  1.66923884e-02\n",
      "Epoch: 2713 mean train loss:  1.82032102e-02, mean val. rec. loss:  1.66881779e-02\n",
      "Epoch: 2714 mean train loss:  1.81992791e-02, mean val. rec. loss:  1.66839731e-02\n",
      "Epoch: 2715 mean train loss:  1.81953406e-02, mean val. rec. loss:  1.66797706e-02\n",
      "Epoch: 2716 mean train loss:  1.81914132e-02, mean val. rec. loss:  1.66755658e-02\n",
      "Epoch: 2717 mean train loss:  1.81874859e-02, mean val. rec. loss:  1.66713678e-02\n",
      "Epoch: 2718 mean train loss:  1.81835529e-02, mean val. rec. loss:  1.66671551e-02\n",
      "Epoch: 2719 mean train loss:  1.81796200e-02, mean val. rec. loss:  1.66629583e-02\n",
      "Epoch: 2720 mean train loss:  1.81756982e-02, mean val. rec. loss:  1.66587580e-02\n",
      "Epoch: 2721 mean train loss:  1.81717764e-02, mean val. rec. loss:  1.66545487e-02\n",
      "Epoch: 2722 mean train loss:  1.81678454e-02, mean val. rec. loss:  1.66503507e-02\n",
      "Epoch: 2723 mean train loss:  1.81639180e-02, mean val. rec. loss:  1.66461584e-02\n",
      "Epoch: 2724 mean train loss:  1.81599999e-02, mean val. rec. loss:  1.66419616e-02\n",
      "Epoch: 2725 mean train loss:  1.81560819e-02, mean val. rec. loss:  1.66377681e-02\n",
      "Epoch: 2726 mean train loss:  1.81521564e-02, mean val. rec. loss:  1.66335667e-02\n",
      "Epoch: 2727 mean train loss:  1.81482365e-02, mean val. rec. loss:  1.66293665e-02\n",
      "Epoch: 2728 mean train loss:  1.81443166e-02, mean val. rec. loss:  1.66251787e-02\n",
      "Epoch: 2729 mean train loss:  1.81403985e-02, mean val. rec. loss:  1.66209921e-02\n",
      "Epoch: 2730 mean train loss:  1.81364805e-02, mean val. rec. loss:  1.66167963e-02\n",
      "Epoch: 2731 mean train loss:  1.81325643e-02, mean val. rec. loss:  1.66125984e-02\n",
      "Epoch: 2732 mean train loss:  1.81286518e-02, mean val. rec. loss:  1.66084140e-02\n",
      "Epoch: 2733 mean train loss:  1.81247356e-02, mean val. rec. loss:  1.66042307e-02\n",
      "Epoch: 2734 mean train loss:  1.81208213e-02, mean val. rec. loss:  1.66000362e-02\n",
      "Epoch: 2735 mean train loss:  1.81169126e-02, mean val. rec. loss:  1.65958427e-02\n",
      "Epoch: 2736 mean train loss:  1.81130020e-02, mean val. rec. loss:  1.65916652e-02\n",
      "Epoch: 2737 mean train loss:  1.81090914e-02, mean val. rec. loss:  1.65874830e-02\n",
      "Epoch: 2738 mean train loss:  1.81051845e-02, mean val. rec. loss:  1.65832941e-02\n",
      "Epoch: 2739 mean train loss:  1.81012776e-02, mean val. rec. loss:  1.65791075e-02\n",
      "Epoch: 2740 mean train loss:  1.80973670e-02, mean val. rec. loss:  1.65749367e-02\n",
      "Epoch: 2741 mean train loss:  1.80934639e-02, mean val. rec. loss:  1.65707535e-02\n",
      "Epoch: 2742 mean train loss:  1.80895626e-02, mean val. rec. loss:  1.65665703e-02\n",
      "Epoch: 2743 mean train loss:  1.80856576e-02, mean val. rec. loss:  1.65623881e-02\n",
      "Epoch: 2744 mean train loss:  1.80817507e-02, mean val. rec. loss:  1.65582072e-02\n",
      "Epoch: 2745 mean train loss:  1.80778531e-02, mean val. rec. loss:  1.65540398e-02\n",
      "Epoch: 2746 mean train loss:  1.80739574e-02, mean val. rec. loss:  1.65498622e-02\n",
      "Epoch: 2747 mean train loss:  1.80700561e-02, mean val. rec. loss:  1.65456847e-02\n",
      "Epoch: 2748 mean train loss:  1.80661530e-02, mean val. rec. loss:  1.65415184e-02\n",
      "Epoch: 2749 mean train loss:  1.80622610e-02, mean val. rec. loss:  1.65373409e-02\n",
      "Epoch: 2750 mean train loss:  1.80583690e-02, mean val. rec. loss:  1.65331678e-02\n",
      "Epoch: 2751 mean train loss:  1.80544733e-02, mean val. rec. loss:  1.65289993e-02\n",
      "Epoch: 2752 mean train loss:  1.80505776e-02, mean val. rec. loss:  1.65248308e-02\n",
      "Epoch: 2753 mean train loss:  1.80466894e-02, mean val. rec. loss:  1.65206623e-02\n",
      "Epoch: 2754 mean train loss:  1.80427937e-02, mean val. rec. loss:  1.65164961e-02\n",
      "Epoch: 2755 mean train loss:  1.80389054e-02, mean val. rec. loss:  1.65123321e-02\n",
      "Epoch: 2756 mean train loss:  1.80350172e-02, mean val. rec. loss:  1.65081671e-02\n",
      "Epoch: 2757 mean train loss:  1.80311289e-02, mean val. rec. loss:  1.65040042e-02\n",
      "Epoch: 2758 mean train loss:  1.80272425e-02, mean val. rec. loss:  1.64998425e-02\n",
      "Epoch: 2759 mean train loss:  1.80233580e-02, mean val. rec. loss:  1.64956797e-02\n",
      "Epoch: 2760 mean train loss:  1.80194753e-02, mean val. rec. loss:  1.64915191e-02\n",
      "Epoch: 2761 mean train loss:  1.80155927e-02, mean val. rec. loss:  1.64873597e-02\n",
      "Epoch: 2762 mean train loss:  1.80117100e-02, mean val. rec. loss:  1.64832048e-02\n",
      "Epoch: 2763 mean train loss:  1.80078348e-02, mean val. rec. loss:  1.64790409e-02\n",
      "Epoch: 2764 mean train loss:  1.80039502e-02, mean val. rec. loss:  1.64748882e-02\n",
      "Epoch: 2765 mean train loss:  1.80000676e-02, mean val. rec. loss:  1.64707356e-02\n",
      "Epoch: 2766 mean train loss:  1.79961942e-02, mean val. rec. loss:  1.64665864e-02\n",
      "Epoch: 2767 mean train loss:  1.79923209e-02, mean val. rec. loss:  1.64624394e-02\n",
      "Epoch: 2768 mean train loss:  1.79884438e-02, mean val. rec. loss:  1.64582778e-02\n",
      "Epoch: 2769 mean train loss:  1.79845667e-02, mean val. rec. loss:  1.64541342e-02\n",
      "Epoch: 2770 mean train loss:  1.79806989e-02, mean val. rec. loss:  1.64499895e-02\n",
      "Epoch: 2771 mean train loss:  1.79768312e-02, mean val. rec. loss:  1.64458358e-02\n",
      "Epoch: 2772 mean train loss:  1.79729615e-02, mean val. rec. loss:  1.64416888e-02\n",
      "Epoch: 2773 mean train loss:  1.79690863e-02, mean val. rec. loss:  1.64375487e-02\n",
      "Epoch: 2774 mean train loss:  1.79652223e-02, mean val. rec. loss:  1.64334006e-02\n",
      "Epoch: 2775 mean train loss:  1.79613564e-02, mean val. rec. loss:  1.64292627e-02\n",
      "Epoch: 2776 mean train loss:  1.79574905e-02, mean val. rec. loss:  1.64251305e-02\n",
      "Epoch: 2777 mean train loss:  1.79536283e-02, mean val. rec. loss:  1.64209858e-02\n",
      "Epoch: 2778 mean train loss:  1.79497661e-02, mean val. rec. loss:  1.64168388e-02\n",
      "Epoch: 2779 mean train loss:  1.79459039e-02, mean val. rec. loss:  1.64127078e-02\n",
      "Epoch: 2780 mean train loss:  1.79420417e-02, mean val. rec. loss:  1.64085767e-02\n",
      "Epoch: 2781 mean train loss:  1.79381851e-02, mean val. rec. loss:  1.64044365e-02\n",
      "Epoch: 2782 mean train loss:  1.79343248e-02, mean val. rec. loss:  1.64002941e-02\n",
      "Epoch: 2783 mean train loss:  1.79304701e-02, mean val. rec. loss:  1.63961699e-02\n",
      "Epoch: 2784 mean train loss:  1.79266153e-02, mean val. rec. loss:  1.63920433e-02\n",
      "Epoch: 2785 mean train loss:  1.79227625e-02, mean val. rec. loss:  1.63879088e-02\n",
      "Epoch: 2786 mean train loss:  1.79189096e-02, mean val. rec. loss:  1.63837755e-02\n",
      "Epoch: 2787 mean train loss:  1.79150549e-02, mean val. rec. loss:  1.63796444e-02\n",
      "Epoch: 2788 mean train loss:  1.79112076e-02, mean val. rec. loss:  1.63755292e-02\n",
      "Epoch: 2789 mean train loss:  1.79073603e-02, mean val. rec. loss:  1.63714015e-02\n",
      "Epoch: 2790 mean train loss:  1.79035111e-02, mean val. rec. loss:  1.63672761e-02\n",
      "Epoch: 2791 mean train loss:  1.78996583e-02, mean val. rec. loss:  1.63631621e-02\n",
      "Epoch: 2792 mean train loss:  1.78958166e-02, mean val. rec. loss:  1.63590367e-02\n",
      "Epoch: 2793 mean train loss:  1.78919786e-02, mean val. rec. loss:  1.63549181e-02\n",
      "Epoch: 2794 mean train loss:  1.78881313e-02, mean val. rec. loss:  1.63507960e-02\n",
      "Epoch: 2795 mean train loss:  1.78842877e-02, mean val. rec. loss:  1.63466740e-02\n",
      "Epoch: 2796 mean train loss:  1.78804535e-02, mean val. rec. loss:  1.63425668e-02\n",
      "Epoch: 2797 mean train loss:  1.78766155e-02, mean val. rec. loss:  1.63384527e-02\n",
      "Epoch: 2798 mean train loss:  1.78727775e-02, mean val. rec. loss:  1.63343398e-02\n",
      "Epoch: 2799 mean train loss:  1.78689433e-02, mean val. rec. loss:  1.63302291e-02\n",
      "Epoch: 2800 mean train loss:  1.78651072e-02, mean val. rec. loss:  1.63261207e-02\n",
      "Epoch: 2801 mean train loss:  1.78612729e-02, mean val. rec. loss:  1.63220112e-02\n",
      "Epoch: 2802 mean train loss:  1.78574405e-02, mean val. rec. loss:  1.63179039e-02\n",
      "Epoch: 2803 mean train loss:  1.78536100e-02, mean val. rec. loss:  1.63138001e-02\n",
      "Epoch: 2804 mean train loss:  1.78497814e-02, mean val. rec. loss:  1.63096973e-02\n",
      "Epoch: 2805 mean train loss:  1.78459527e-02, mean val. rec. loss:  1.63055935e-02\n",
      "Epoch: 2806 mean train loss:  1.78421277e-02, mean val. rec. loss:  1.63014907e-02\n",
      "Epoch: 2807 mean train loss:  1.78383028e-02, mean val. rec. loss:  1.62973914e-02\n",
      "Epoch: 2808 mean train loss:  1.78344779e-02, mean val. rec. loss:  1.62932921e-02\n",
      "Epoch: 2809 mean train loss:  1.78306567e-02, mean val. rec. loss:  1.62891928e-02\n",
      "Epoch: 2810 mean train loss:  1.78268354e-02, mean val. rec. loss:  1.62851014e-02\n",
      "Epoch: 2811 mean train loss:  1.78230198e-02, mean val. rec. loss:  1.62809975e-02\n",
      "Epoch: 2812 mean train loss:  1.78191967e-02, mean val. rec. loss:  1.62769084e-02\n",
      "Epoch: 2813 mean train loss:  1.78153755e-02, mean val. rec. loss:  1.62728204e-02\n",
      "Epoch: 2814 mean train loss:  1.78115636e-02, mean val. rec. loss:  1.62687313e-02\n",
      "Epoch: 2815 mean train loss:  1.78077517e-02, mean val. rec. loss:  1.62646490e-02\n",
      "Epoch: 2816 mean train loss:  1.78039379e-02, mean val. rec. loss:  1.62605508e-02\n",
      "Epoch: 2817 mean train loss:  1.78001205e-02, mean val. rec. loss:  1.62564662e-02\n",
      "Epoch: 2818 mean train loss:  1.77963141e-02, mean val. rec. loss:  1.62523839e-02\n",
      "Epoch: 2819 mean train loss:  1.77925078e-02, mean val. rec. loss:  1.62482925e-02\n",
      "Epoch: 2820 mean train loss:  1.77886978e-02, mean val. rec. loss:  1.62442147e-02\n",
      "Epoch: 2821 mean train loss:  1.77848877e-02, mean val. rec. loss:  1.62401369e-02\n",
      "Epoch: 2822 mean train loss:  1.77810870e-02, mean val. rec. loss:  1.62360648e-02\n",
      "Epoch: 2823 mean train loss:  1.77772826e-02, mean val. rec. loss:  1.62319814e-02\n",
      "Epoch: 2824 mean train loss:  1.77734800e-02, mean val. rec. loss:  1.62278991e-02\n",
      "Epoch: 2825 mean train loss:  1.77696811e-02, mean val. rec. loss:  1.62238304e-02\n",
      "Epoch: 2826 mean train loss:  1.77658804e-02, mean val. rec. loss:  1.62197651e-02\n",
      "Epoch: 2827 mean train loss:  1.77620834e-02, mean val. rec. loss:  1.62156861e-02\n",
      "Epoch: 2828 mean train loss:  1.77582863e-02, mean val. rec. loss:  1.62116106e-02\n",
      "Epoch: 2829 mean train loss:  1.77544931e-02, mean val. rec. loss:  1.62075521e-02\n",
      "Epoch: 2830 mean train loss:  1.77506998e-02, mean val. rec. loss:  1.62034891e-02\n",
      "Epoch: 2831 mean train loss:  1.77469065e-02, mean val. rec. loss:  1.61994181e-02\n",
      "Epoch: 2832 mean train loss:  1.77431188e-02, mean val. rec. loss:  1.61953494e-02\n",
      "Epoch: 2833 mean train loss:  1.77393292e-02, mean val. rec. loss:  1.61912920e-02\n",
      "Epoch: 2834 mean train loss:  1.77355416e-02, mean val. rec. loss:  1.61872267e-02\n",
      "Epoch: 2835 mean train loss:  1.77317501e-02, mean val. rec. loss:  1.61831660e-02\n",
      "Epoch: 2836 mean train loss:  1.77279680e-02, mean val. rec. loss:  1.61791154e-02\n",
      "Epoch: 2837 mean train loss:  1.77241878e-02, mean val. rec. loss:  1.61750580e-02\n",
      "Epoch: 2838 mean train loss:  1.77204038e-02, mean val. rec. loss:  1.61709995e-02\n",
      "Epoch: 2839 mean train loss:  1.77166198e-02, mean val. rec. loss:  1.61669592e-02\n",
      "Epoch: 2840 mean train loss:  1.77128433e-02, mean val. rec. loss:  1.61628995e-02\n",
      "Epoch: 2841 mean train loss:  1.77090705e-02, mean val. rec. loss:  1.61588501e-02\n",
      "Epoch: 2842 mean train loss:  1.77052921e-02, mean val. rec. loss:  1.61547961e-02\n",
      "Epoch: 2843 mean train loss:  1.77015137e-02, mean val. rec. loss:  1.61507478e-02\n",
      "Epoch: 2844 mean train loss:  1.76977428e-02, mean val. rec. loss:  1.61467086e-02\n",
      "Epoch: 2845 mean train loss:  1.76939756e-02, mean val. rec. loss:  1.61426649e-02\n",
      "Epoch: 2846 mean train loss:  1.76902028e-02, mean val. rec. loss:  1.61386222e-02\n",
      "Epoch: 2847 mean train loss:  1.76864337e-02, mean val. rec. loss:  1.61345819e-02\n",
      "Epoch: 2848 mean train loss:  1.76826684e-02, mean val. rec. loss:  1.61305426e-02\n",
      "Epoch: 2849 mean train loss:  1.76789030e-02, mean val. rec. loss:  1.61265034e-02\n",
      "Epoch: 2850 mean train loss:  1.76751377e-02, mean val. rec. loss:  1.61224710e-02\n",
      "Epoch: 2851 mean train loss:  1.76713742e-02, mean val. rec. loss:  1.61184374e-02\n",
      "Epoch: 2852 mean train loss:  1.76676144e-02, mean val. rec. loss:  1.61144039e-02\n",
      "Epoch: 2853 mean train loss:  1.76638547e-02, mean val. rec. loss:  1.61103737e-02\n",
      "Epoch: 2854 mean train loss:  1.76600949e-02, mean val. rec. loss:  1.61063458e-02\n",
      "Epoch: 2855 mean train loss:  1.76563389e-02, mean val. rec. loss:  1.61023191e-02\n",
      "Epoch: 2856 mean train loss:  1.76525865e-02, mean val. rec. loss:  1.60982923e-02\n",
      "Epoch: 2857 mean train loss:  1.76488324e-02, mean val. rec. loss:  1.60942667e-02\n",
      "Epoch: 2858 mean train loss:  1.76450801e-02, mean val. rec. loss:  1.60902456e-02\n",
      "Epoch: 2859 mean train loss:  1.76413352e-02, mean val. rec. loss:  1.60862178e-02\n",
      "Epoch: 2860 mean train loss:  1.76375792e-02, mean val. rec. loss:  1.60822035e-02\n",
      "Epoch: 2861 mean train loss:  1.76338287e-02, mean val. rec. loss:  1.60781892e-02\n",
      "Epoch: 2862 mean train loss:  1.76300876e-02, mean val. rec. loss:  1.60741749e-02\n",
      "Epoch: 2863 mean train loss:  1.76263446e-02, mean val. rec. loss:  1.60701652e-02\n",
      "Epoch: 2864 mean train loss:  1.76225978e-02, mean val. rec. loss:  1.60661441e-02\n",
      "Epoch: 2865 mean train loss:  1.76188548e-02, mean val. rec. loss:  1.60621389e-02\n",
      "Epoch: 2866 mean train loss:  1.76151193e-02, mean val. rec. loss:  1.60581314e-02\n",
      "Epoch: 2867 mean train loss:  1.76113837e-02, mean val. rec. loss:  1.60541149e-02\n",
      "Epoch: 2868 mean train loss:  1.76076444e-02, mean val. rec. loss:  1.60501119e-02\n",
      "Epoch: 2869 mean train loss:  1.76039070e-02, mean val. rec. loss:  1.60461101e-02\n",
      "Epoch: 2870 mean train loss:  1.76001789e-02, mean val. rec. loss:  1.60421038e-02\n",
      "Epoch: 2871 mean train loss:  1.75964452e-02, mean val. rec. loss:  1.60381099e-02\n",
      "Epoch: 2872 mean train loss:  1.75927171e-02, mean val. rec. loss:  1.60341195e-02\n",
      "Epoch: 2873 mean train loss:  1.75889872e-02, mean val. rec. loss:  1.60301177e-02\n",
      "Epoch: 2874 mean train loss:  1.75852609e-02, mean val. rec. loss:  1.60261170e-02\n",
      "Epoch: 2875 mean train loss:  1.75815365e-02, mean val. rec. loss:  1.60221288e-02\n",
      "Epoch: 2876 mean train loss:  1.75778084e-02, mean val. rec. loss:  1.60181406e-02\n",
      "Epoch: 2877 mean train loss:  1.75740878e-02, mean val. rec. loss:  1.60141467e-02\n",
      "Epoch: 2878 mean train loss:  1.75703671e-02, mean val. rec. loss:  1.60101517e-02\n",
      "Epoch: 2879 mean train loss:  1.75666465e-02, mean val. rec. loss:  1.60061704e-02\n",
      "Epoch: 2880 mean train loss:  1.75629295e-02, mean val. rec. loss:  1.60021924e-02\n",
      "Epoch: 2881 mean train loss:  1.75592145e-02, mean val. rec. loss:  1.59982030e-02\n",
      "Epoch: 2882 mean train loss:  1.75555012e-02, mean val. rec. loss:  1.59942205e-02\n",
      "Epoch: 2883 mean train loss:  1.75517843e-02, mean val. rec. loss:  1.59902380e-02\n",
      "Epoch: 2884 mean train loss:  1.75480767e-02, mean val. rec. loss:  1.59862702e-02\n",
      "Epoch: 2885 mean train loss:  1.75443709e-02, mean val. rec. loss:  1.59822911e-02\n",
      "Epoch: 2886 mean train loss:  1.75406577e-02, mean val. rec. loss:  1.59783154e-02\n",
      "Epoch: 2887 mean train loss:  1.75369482e-02, mean val. rec. loss:  1.59743532e-02\n",
      "Epoch: 2888 mean train loss:  1.75332481e-02, mean val. rec. loss:  1.59703753e-02\n",
      "Epoch: 2889 mean train loss:  1.75295479e-02, mean val. rec. loss:  1.59664086e-02\n",
      "Epoch: 2890 mean train loss:  1.75258459e-02, mean val. rec. loss:  1.59624385e-02\n",
      "Epoch: 2891 mean train loss:  1.75221420e-02, mean val. rec. loss:  1.59584685e-02\n",
      "Epoch: 2892 mean train loss:  1.75184455e-02, mean val. rec. loss:  1.59545155e-02\n",
      "Epoch: 2893 mean train loss:  1.75147547e-02, mean val. rec. loss:  1.59505511e-02\n",
      "Epoch: 2894 mean train loss:  1.75110582e-02, mean val. rec. loss:  1.59465924e-02\n",
      "Epoch: 2895 mean train loss:  1.75073636e-02, mean val. rec. loss:  1.59426348e-02\n",
      "Epoch: 2896 mean train loss:  1.75036709e-02, mean val. rec. loss:  1.59386783e-02\n",
      "Epoch: 2897 mean train loss:  1.74999800e-02, mean val. rec. loss:  1.59347253e-02\n",
      "Epoch: 2898 mean train loss:  1.74962929e-02, mean val. rec. loss:  1.59307723e-02\n",
      "Epoch: 2899 mean train loss:  1.74926058e-02, mean val. rec. loss:  1.59268260e-02\n",
      "Epoch: 2900 mean train loss:  1.74889224e-02, mean val. rec. loss:  1.59228752e-02\n",
      "Epoch: 2901 mean train loss:  1.74852389e-02, mean val. rec. loss:  1.59189290e-02\n",
      "Epoch: 2902 mean train loss:  1.74815592e-02, mean val. rec. loss:  1.59149839e-02\n",
      "Epoch: 2903 mean train loss:  1.74778777e-02, mean val. rec. loss:  1.59110399e-02\n",
      "Epoch: 2904 mean train loss:  1.74741999e-02, mean val. rec. loss:  1.59070971e-02\n",
      "Epoch: 2905 mean train loss:  1.74705239e-02, mean val. rec. loss:  1.59031599e-02\n",
      "Epoch: 2906 mean train loss:  1.74668479e-02, mean val. rec. loss:  1.58992228e-02\n",
      "Epoch: 2907 mean train loss:  1.74631794e-02, mean val. rec. loss:  1.58952833e-02\n",
      "Epoch: 2908 mean train loss:  1.74595053e-02, mean val. rec. loss:  1.58913507e-02\n",
      "Epoch: 2909 mean train loss:  1.74558294e-02, mean val. rec. loss:  1.58874226e-02\n",
      "Epoch: 2910 mean train loss:  1.74521664e-02, mean val. rec. loss:  1.58834968e-02\n",
      "Epoch: 2911 mean train loss:  1.74485035e-02, mean val. rec. loss:  1.58795721e-02\n",
      "Epoch: 2912 mean train loss:  1.74448369e-02, mean val. rec. loss:  1.58756349e-02\n",
      "Epoch: 2913 mean train loss:  1.74411683e-02, mean val. rec. loss:  1.58717170e-02\n",
      "Epoch: 2914 mean train loss:  1.74375110e-02, mean val. rec. loss:  1.58677957e-02\n",
      "Epoch: 2915 mean train loss:  1.74338555e-02, mean val. rec. loss:  1.58638654e-02\n",
      "Epoch: 2916 mean train loss:  1.74301926e-02, mean val. rec. loss:  1.58599520e-02\n",
      "Epoch: 2917 mean train loss:  1.74265352e-02, mean val. rec. loss:  1.58560409e-02\n",
      "Epoch: 2918 mean train loss:  1.74228854e-02, mean val. rec. loss:  1.58521310e-02\n",
      "Epoch: 2919 mean train loss:  1.74192317e-02, mean val. rec. loss:  1.58482142e-02\n",
      "Epoch: 2920 mean train loss:  1.74155818e-02, mean val. rec. loss:  1.58442997e-02\n",
      "Epoch: 2921 mean train loss:  1.74119338e-02, mean val. rec. loss:  1.58403977e-02\n",
      "Epoch: 2922 mean train loss:  1.74082821e-02, mean val. rec. loss:  1.58364980e-02\n",
      "Epoch: 2923 mean train loss:  1.74046396e-02, mean val. rec. loss:  1.58325869e-02\n",
      "Epoch: 2924 mean train loss:  1.74009934e-02, mean val. rec. loss:  1.58286781e-02\n",
      "Epoch: 2925 mean train loss:  1.73973510e-02, mean val. rec. loss:  1.58247851e-02\n",
      "Epoch: 2926 mean train loss:  1.73937123e-02, mean val. rec. loss:  1.58208922e-02\n",
      "Epoch: 2927 mean train loss:  1.73900736e-02, mean val. rec. loss:  1.58169902e-02\n",
      "Epoch: 2928 mean train loss:  1.73864386e-02, mean val. rec. loss:  1.58130893e-02\n",
      "Epoch: 2929 mean train loss:  1.73828036e-02, mean val. rec. loss:  1.58092032e-02\n",
      "Epoch: 2930 mean train loss:  1.73791686e-02, mean val. rec. loss:  1.58053091e-02\n",
      "Epoch: 2931 mean train loss:  1.73755336e-02, mean val. rec. loss:  1.58014195e-02\n",
      "Epoch: 2932 mean train loss:  1.73719060e-02, mean val. rec. loss:  1.57975391e-02\n",
      "Epoch: 2933 mean train loss:  1.73682822e-02, mean val. rec. loss:  1.57936563e-02\n",
      "Epoch: 2934 mean train loss:  1.73646528e-02, mean val. rec. loss:  1.57897702e-02\n",
      "Epoch: 2935 mean train loss:  1.73610234e-02, mean val. rec. loss:  1.57859011e-02\n",
      "Epoch: 2936 mean train loss:  1.73574051e-02, mean val. rec. loss:  1.57820183e-02\n",
      "Epoch: 2937 mean train loss:  1.73537869e-02, mean val. rec. loss:  1.57781390e-02\n",
      "Epoch: 2938 mean train loss:  1.73501668e-02, mean val. rec. loss:  1.57742608e-02\n",
      "Epoch: 2939 mean train loss:  1.73465430e-02, mean val. rec. loss:  1.57703849e-02\n",
      "Epoch: 2940 mean train loss:  1.73429303e-02, mean val. rec. loss:  1.57665225e-02\n",
      "Epoch: 2941 mean train loss:  1.73393196e-02, mean val. rec. loss:  1.57626512e-02\n",
      "Epoch: 2942 mean train loss:  1.73357050e-02, mean val. rec. loss:  1.57587798e-02\n",
      "Epoch: 2943 mean train loss:  1.73320924e-02, mean val. rec. loss:  1.57549197e-02\n",
      "Epoch: 2944 mean train loss:  1.73284835e-02, mean val. rec. loss:  1.57510551e-02\n",
      "Epoch: 2945 mean train loss:  1.73248764e-02, mean val. rec. loss:  1.57471973e-02\n",
      "Epoch: 2946 mean train loss:  1.73212693e-02, mean val. rec. loss:  1.57433407e-02\n",
      "Epoch: 2947 mean train loss:  1.73176679e-02, mean val. rec. loss:  1.57394840e-02\n",
      "Epoch: 2948 mean train loss:  1.73140645e-02, mean val. rec. loss:  1.57356285e-02\n",
      "Epoch: 2949 mean train loss:  1.73104649e-02, mean val. rec. loss:  1.57317775e-02\n",
      "Epoch: 2950 mean train loss:  1.73068672e-02, mean val. rec. loss:  1.57279288e-02\n",
      "Epoch: 2951 mean train loss:  1.73032731e-02, mean val. rec. loss:  1.57240823e-02\n",
      "Epoch: 2952 mean train loss:  1.72996791e-02, mean val. rec. loss:  1.57202359e-02\n",
      "Epoch: 2953 mean train loss:  1.72960869e-02, mean val. rec. loss:  1.57163940e-02\n",
      "Epoch: 2954 mean train loss:  1.72924985e-02, mean val. rec. loss:  1.57125521e-02\n",
      "Epoch: 2955 mean train loss:  1.72889082e-02, mean val. rec. loss:  1.57087147e-02\n",
      "Epoch: 2956 mean train loss:  1.72853253e-02, mean val. rec. loss:  1.57048796e-02\n",
      "Epoch: 2957 mean train loss:  1.72817425e-02, mean val. rec. loss:  1.57010377e-02\n",
      "Epoch: 2958 mean train loss:  1.72781559e-02, mean val. rec. loss:  1.56972094e-02\n",
      "Epoch: 2959 mean train loss:  1.72745675e-02, mean val. rec. loss:  1.56933811e-02\n",
      "Epoch: 2960 mean train loss:  1.72709939e-02, mean val. rec. loss:  1.56895528e-02\n",
      "Epoch: 2961 mean train loss:  1.72674185e-02, mean val. rec. loss:  1.56857335e-02\n",
      "Epoch: 2962 mean train loss:  1.72638357e-02, mean val. rec. loss:  1.56818995e-02\n",
      "Epoch: 2963 mean train loss:  1.72602603e-02, mean val. rec. loss:  1.56780803e-02\n",
      "Epoch: 2964 mean train loss:  1.72566904e-02, mean val. rec. loss:  1.56742599e-02\n",
      "Epoch: 2965 mean train loss:  1.72531243e-02, mean val. rec. loss:  1.56704350e-02\n",
      "Epoch: 2966 mean train loss:  1.72495527e-02, mean val. rec. loss:  1.56666192e-02\n",
      "Epoch: 2967 mean train loss:  1.72459810e-02, mean val. rec. loss:  1.56628090e-02\n",
      "Epoch: 2968 mean train loss:  1.72424205e-02, mean val. rec. loss:  1.56589921e-02\n",
      "Epoch: 2969 mean train loss:  1.72388562e-02, mean val. rec. loss:  1.56551899e-02\n",
      "Epoch: 2970 mean train loss:  1.72352957e-02, mean val. rec. loss:  1.56513888e-02\n",
      "Epoch: 2971 mean train loss:  1.72317352e-02, mean val. rec. loss:  1.56475797e-02\n",
      "Epoch: 2972 mean train loss:  1.72281784e-02, mean val. rec. loss:  1.56437662e-02\n",
      "Epoch: 2973 mean train loss:  1.72246217e-02, mean val. rec. loss:  1.56399753e-02\n",
      "Epoch: 2974 mean train loss:  1.72210686e-02, mean val. rec. loss:  1.56361810e-02\n",
      "Epoch: 2975 mean train loss:  1.72175155e-02, mean val. rec. loss:  1.56323788e-02\n",
      "Epoch: 2976 mean train loss:  1.72139662e-02, mean val. rec. loss:  1.56285788e-02\n",
      "Epoch: 2977 mean train loss:  1.72104169e-02, mean val. rec. loss:  1.56247913e-02\n",
      "Epoch: 2978 mean train loss:  1.72068713e-02, mean val. rec. loss:  1.56210095e-02\n",
      "Epoch: 2979 mean train loss:  1.72033294e-02, mean val. rec. loss:  1.56172141e-02\n",
      "Epoch: 2980 mean train loss:  1.71997875e-02, mean val. rec. loss:  1.56134221e-02\n",
      "Epoch: 2981 mean train loss:  1.71962456e-02, mean val. rec. loss:  1.56096448e-02\n",
      "Epoch: 2982 mean train loss:  1.71927093e-02, mean val. rec. loss:  1.56058653e-02\n",
      "Epoch: 2983 mean train loss:  1.71891693e-02, mean val. rec. loss:  1.56020801e-02\n",
      "Epoch: 2984 mean train loss:  1.71856292e-02, mean val. rec. loss:  1.55983141e-02\n",
      "Epoch: 2985 mean train loss:  1.71821004e-02, mean val. rec. loss:  1.55945335e-02\n",
      "Epoch: 2986 mean train loss:  1.71785697e-02, mean val. rec. loss:  1.55907607e-02\n",
      "Epoch: 2987 mean train loss:  1.71750390e-02, mean val. rec. loss:  1.55869868e-02\n",
      "Epoch: 2988 mean train loss:  1.71715083e-02, mean val. rec. loss:  1.55832152e-02\n",
      "Epoch: 2989 mean train loss:  1.71679850e-02, mean val. rec. loss:  1.55794572e-02\n",
      "Epoch: 2990 mean train loss:  1.71644654e-02, mean val. rec. loss:  1.55756902e-02\n",
      "Epoch: 2991 mean train loss:  1.71609385e-02, mean val. rec. loss:  1.55719254e-02\n",
      "Epoch: 2992 mean train loss:  1.71574152e-02, mean val. rec. loss:  1.55681764e-02\n",
      "Epoch: 2993 mean train loss:  1.71539012e-02, mean val. rec. loss:  1.55644184e-02\n",
      "Epoch: 2994 mean train loss:  1.71503836e-02, mean val. rec. loss:  1.55606650e-02\n",
      "Epoch: 2995 mean train loss:  1.71468696e-02, mean val. rec. loss:  1.55569138e-02\n",
      "Epoch: 2996 mean train loss:  1.71433557e-02, mean val. rec. loss:  1.55531626e-02\n",
      "Epoch: 2997 mean train loss:  1.71398454e-02, mean val. rec. loss:  1.55494159e-02\n",
      "Epoch: 2998 mean train loss:  1.71363389e-02, mean val. rec. loss:  1.55456715e-02\n",
      "Epoch: 2999 mean train loss:  1.71328306e-02, mean val. rec. loss:  1.55419271e-02\n",
      "Epoch: 3000 mean train loss:  1.71293259e-02, mean val. rec. loss:  1.55381850e-02\n",
      "Epoch: 3001 mean train loss:  1.71258213e-02, mean val. rec. loss:  1.55344452e-02\n",
      "Epoch: 3002 mean train loss:  1.71223204e-02, mean val. rec. loss:  1.55307087e-02\n",
      "Epoch: 3003 mean train loss:  1.71188232e-02, mean val. rec. loss:  1.55269723e-02\n",
      "Epoch: 3004 mean train loss:  1.71153241e-02, mean val. rec. loss:  1.55232415e-02\n",
      "Epoch: 3005 mean train loss:  1.71118288e-02, mean val. rec. loss:  1.55195107e-02\n",
      "Epoch: 3006 mean train loss:  1.71083353e-02, mean val. rec. loss:  1.55157731e-02\n",
      "Epoch: 3007 mean train loss:  1.71048400e-02, mean val. rec. loss:  1.55120525e-02\n",
      "Epoch: 3008 mean train loss:  1.71013521e-02, mean val. rec. loss:  1.55083286e-02\n",
      "Epoch: 3009 mean train loss:  1.70978698e-02, mean val. rec. loss:  1.55046023e-02\n",
      "Epoch: 3010 mean train loss:  1.70943800e-02, mean val. rec. loss:  1.55008840e-02\n",
      "Epoch: 3011 mean train loss:  1.70908922e-02, mean val. rec. loss:  1.54971736e-02\n",
      "Epoch: 3012 mean train loss:  1.70874136e-02, mean val. rec. loss:  1.54934610e-02\n",
      "Epoch: 3013 mean train loss:  1.70839369e-02, mean val. rec. loss:  1.54897551e-02\n",
      "Epoch: 3014 mean train loss:  1.70804546e-02, mean val. rec. loss:  1.54860334e-02\n",
      "Epoch: 3015 mean train loss:  1.70769760e-02, mean val. rec. loss:  1.54823310e-02\n",
      "Epoch: 3016 mean train loss:  1.70735049e-02, mean val. rec. loss:  1.54786240e-02\n",
      "Epoch: 3017 mean train loss:  1.70700356e-02, mean val. rec. loss:  1.54749159e-02\n",
      "Epoch: 3018 mean train loss:  1.70665626e-02, mean val. rec. loss:  1.54712180e-02\n",
      "Epoch: 3019 mean train loss:  1.70630915e-02, mean val. rec. loss:  1.54675201e-02\n",
      "Epoch: 3020 mean train loss:  1.70596260e-02, mean val. rec. loss:  1.54638313e-02\n",
      "Epoch: 3021 mean train loss:  1.70561642e-02, mean val. rec. loss:  1.54601345e-02\n",
      "Epoch: 3022 mean train loss:  1.70527005e-02, mean val. rec. loss:  1.54564344e-02\n",
      "Epoch: 3023 mean train loss:  1.70492368e-02, mean val. rec. loss:  1.54527546e-02\n",
      "Epoch: 3024 mean train loss:  1.70457806e-02, mean val. rec. loss:  1.54490703e-02\n",
      "Epoch: 3025 mean train loss:  1.70423225e-02, mean val. rec. loss:  1.54453826e-02\n",
      "Epoch: 3026 mean train loss:  1.70388663e-02, mean val. rec. loss:  1.54416950e-02\n",
      "Epoch: 3027 mean train loss:  1.70354156e-02, mean val. rec. loss:  1.54380209e-02\n",
      "Epoch: 3028 mean train loss:  1.70319631e-02, mean val. rec. loss:  1.54343479e-02\n",
      "Epoch: 3029 mean train loss:  1.70285144e-02, mean val. rec. loss:  1.54306682e-02\n",
      "Epoch: 3030 mean train loss:  1.70250675e-02, mean val. rec. loss:  1.54269884e-02\n",
      "Epoch: 3031 mean train loss:  1.70216224e-02, mean val. rec. loss:  1.54233245e-02\n",
      "Epoch: 3032 mean train loss:  1.70181792e-02, mean val. rec. loss:  1.54196618e-02\n",
      "Epoch: 3033 mean train loss:  1.70147397e-02, mean val. rec. loss:  1.54159922e-02\n",
      "Epoch: 3034 mean train loss:  1.70113021e-02, mean val. rec. loss:  1.54123284e-02\n",
      "Epoch: 3035 mean train loss:  1.70078627e-02, mean val. rec. loss:  1.54086622e-02\n",
      "Epoch: 3036 mean train loss:  1.70044213e-02, mean val. rec. loss:  1.54050040e-02\n",
      "Epoch: 3037 mean train loss:  1.70009912e-02, mean val. rec. loss:  1.54013549e-02\n",
      "Epoch: 3038 mean train loss:  1.69975629e-02, mean val. rec. loss:  1.53977023e-02\n",
      "Epoch: 3039 mean train loss:  1.69941309e-02, mean val. rec. loss:  1.53940475e-02\n",
      "Epoch: 3040 mean train loss:  1.69907007e-02, mean val. rec. loss:  1.53904086e-02\n",
      "Epoch: 3041 mean train loss:  1.69872780e-02, mean val. rec. loss:  1.53867583e-02\n",
      "Epoch: 3042 mean train loss:  1.69838609e-02, mean val. rec. loss:  1.53831137e-02\n",
      "Epoch: 3043 mean train loss:  1.69804345e-02, mean val. rec. loss:  1.53794691e-02\n",
      "Epoch: 3044 mean train loss:  1.69770118e-02, mean val. rec. loss:  1.53758291e-02\n",
      "Epoch: 3045 mean train loss:  1.69736002e-02, mean val. rec. loss:  1.53721969e-02\n",
      "Epoch: 3046 mean train loss:  1.69701868e-02, mean val. rec. loss:  1.53685625e-02\n",
      "Epoch: 3047 mean train loss:  1.69667716e-02, mean val. rec. loss:  1.53649338e-02\n",
      "Epoch: 3048 mean train loss:  1.69633600e-02, mean val. rec. loss:  1.53613017e-02\n",
      "Epoch: 3049 mean train loss:  1.69599485e-02, mean val. rec. loss:  1.53576775e-02\n",
      "Epoch: 3050 mean train loss:  1.69565426e-02, mean val. rec. loss:  1.53540522e-02\n",
      "Epoch: 3051 mean train loss:  1.69531366e-02, mean val. rec. loss:  1.53504325e-02\n",
      "Epoch: 3052 mean train loss:  1.69497325e-02, mean val. rec. loss:  1.53468106e-02\n",
      "Epoch: 3053 mean train loss:  1.69463322e-02, mean val. rec. loss:  1.53431932e-02\n",
      "Epoch: 3054 mean train loss:  1.69429318e-02, mean val. rec. loss:  1.53395804e-02\n",
      "Epoch: 3055 mean train loss:  1.69395333e-02, mean val. rec. loss:  1.53359664e-02\n",
      "Epoch: 3056 mean train loss:  1.69361385e-02, mean val. rec. loss:  1.53323569e-02\n",
      "Epoch: 3057 mean train loss:  1.69327456e-02, mean val. rec. loss:  1.53287497e-02\n",
      "Epoch: 3058 mean train loss:  1.69293546e-02, mean val. rec. loss:  1.53251426e-02\n",
      "Epoch: 3059 mean train loss:  1.69259635e-02, mean val. rec. loss:  1.53215411e-02\n",
      "Epoch: 3060 mean train loss:  1.69225781e-02, mean val. rec. loss:  1.53179373e-02\n",
      "Epoch: 3061 mean train loss:  1.69191926e-02, mean val. rec. loss:  1.53143403e-02\n",
      "Epoch: 3062 mean train loss:  1.69158109e-02, mean val. rec. loss:  1.53107365e-02\n",
      "Epoch: 3063 mean train loss:  1.69124254e-02, mean val. rec. loss:  1.53071475e-02\n",
      "Epoch: 3064 mean train loss:  1.69090418e-02, mean val. rec. loss:  1.53035585e-02\n",
      "Epoch: 3065 mean train loss:  1.69056657e-02, mean val. rec. loss:  1.52999728e-02\n",
      "Epoch: 3066 mean train loss:  1.69022951e-02, mean val. rec. loss:  1.52963917e-02\n",
      "Epoch: 3067 mean train loss:  1.68989171e-02, mean val. rec. loss:  1.52927970e-02\n",
      "Epoch: 3068 mean train loss:  1.68955428e-02, mean val. rec. loss:  1.52892182e-02\n",
      "Epoch: 3069 mean train loss:  1.68921778e-02, mean val. rec. loss:  1.52856394e-02\n",
      "Epoch: 3070 mean train loss:  1.68888128e-02, mean val. rec. loss:  1.52820594e-02\n",
      "Epoch: 3071 mean train loss:  1.68854460e-02, mean val. rec. loss:  1.52784862e-02\n",
      "Epoch: 3072 mean train loss:  1.68820754e-02, mean val. rec. loss:  1.52749187e-02\n",
      "Epoch: 3073 mean train loss:  1.68787216e-02, mean val. rec. loss:  1.52713535e-02\n",
      "Epoch: 3074 mean train loss:  1.68753641e-02, mean val. rec. loss:  1.52677906e-02\n",
      "Epoch: 3075 mean train loss:  1.68720047e-02, mean val. rec. loss:  1.52642185e-02\n",
      "Epoch: 3076 mean train loss:  1.68686490e-02, mean val. rec. loss:  1.52606522e-02\n",
      "Epoch: 3077 mean train loss:  1.68652934e-02, mean val. rec. loss:  1.52571006e-02\n",
      "Epoch: 3078 mean train loss:  1.68619414e-02, mean val. rec. loss:  1.52535501e-02\n",
      "Epoch: 3079 mean train loss:  1.68585913e-02, mean val. rec. loss:  1.52499894e-02\n",
      "Epoch: 3080 mean train loss:  1.68552431e-02, mean val. rec. loss:  1.52464310e-02\n",
      "Epoch: 3081 mean train loss:  1.68518986e-02, mean val. rec. loss:  1.52428873e-02\n",
      "Epoch: 3082 mean train loss:  1.68485541e-02, mean val. rec. loss:  1.52393459e-02\n",
      "Epoch: 3083 mean train loss:  1.68452115e-02, mean val. rec. loss:  1.52357954e-02\n",
      "Epoch: 3084 mean train loss:  1.68418745e-02, mean val. rec. loss:  1.52322438e-02\n",
      "Epoch: 3085 mean train loss:  1.68385337e-02, mean val. rec. loss:  1.52287115e-02\n",
      "Epoch: 3086 mean train loss:  1.68352004e-02, mean val. rec. loss:  1.52251814e-02\n",
      "Epoch: 3087 mean train loss:  1.68318670e-02, mean val. rec. loss:  1.52216400e-02\n",
      "Epoch: 3088 mean train loss:  1.68285375e-02, mean val. rec. loss:  1.52180997e-02\n",
      "Epoch: 3089 mean train loss:  1.68252079e-02, mean val. rec. loss:  1.52145753e-02\n",
      "Epoch: 3090 mean train loss:  1.68218801e-02, mean val. rec. loss:  1.52110464e-02\n",
      "Epoch: 3091 mean train loss:  1.68185505e-02, mean val. rec. loss:  1.52075333e-02\n",
      "Epoch: 3092 mean train loss:  1.68152302e-02, mean val. rec. loss:  1.52040044e-02\n",
      "Epoch: 3093 mean train loss:  1.68119118e-02, mean val. rec. loss:  1.52004823e-02\n",
      "Epoch: 3094 mean train loss:  1.68085878e-02, mean val. rec. loss:  1.51969601e-02\n",
      "Epoch: 3095 mean train loss:  1.68052657e-02, mean val. rec. loss:  1.51934437e-02\n",
      "Epoch: 3096 mean train loss:  1.68019547e-02, mean val. rec. loss:  1.51899385e-02\n",
      "Epoch: 3097 mean train loss:  1.67986418e-02, mean val. rec. loss:  1.51864255e-02\n",
      "Epoch: 3098 mean train loss:  1.67953271e-02, mean val. rec. loss:  1.51829124e-02\n",
      "Epoch: 3099 mean train loss:  1.67920162e-02, mean val. rec. loss:  1.51794186e-02\n",
      "Epoch: 3100 mean train loss:  1.67887108e-02, mean val. rec. loss:  1.51759112e-02\n",
      "Epoch: 3101 mean train loss:  1.67854110e-02, mean val. rec. loss:  1.51724118e-02\n",
      "Epoch: 3102 mean train loss:  1.67821037e-02, mean val. rec. loss:  1.51689101e-02\n",
      "Epoch: 3103 mean train loss:  1.67787983e-02, mean val. rec. loss:  1.51654118e-02\n",
      "Epoch: 3104 mean train loss:  1.67755041e-02, mean val. rec. loss:  1.51619248e-02\n",
      "Epoch: 3105 mean train loss:  1.67722099e-02, mean val. rec. loss:  1.51584355e-02\n",
      "Epoch: 3106 mean train loss:  1.67689138e-02, mean val. rec. loss:  1.51549486e-02\n",
      "Epoch: 3107 mean train loss:  1.67656215e-02, mean val. rec. loss:  1.51514627e-02\n",
      "Epoch: 3108 mean train loss:  1.67623291e-02, mean val. rec. loss:  1.51479814e-02\n",
      "Epoch: 3109 mean train loss:  1.67590405e-02, mean val. rec. loss:  1.51445001e-02\n",
      "Epoch: 3110 mean train loss:  1.67557537e-02, mean val. rec. loss:  1.51410211e-02\n",
      "Epoch: 3111 mean train loss:  1.67524670e-02, mean val. rec. loss:  1.51375488e-02\n",
      "Epoch: 3112 mean train loss:  1.67491821e-02, mean val. rec. loss:  1.51340743e-02\n",
      "Epoch: 3113 mean train loss:  1.67459009e-02, mean val. rec. loss:  1.51306021e-02\n",
      "Epoch: 3114 mean train loss:  1.67426234e-02, mean val. rec. loss:  1.51271344e-02\n",
      "Epoch: 3115 mean train loss:  1.67393460e-02, mean val. rec. loss:  1.51236712e-02\n",
      "Epoch: 3116 mean train loss:  1.67360722e-02, mean val. rec. loss:  1.51202081e-02\n",
      "Epoch: 3117 mean train loss:  1.67327985e-02, mean val. rec. loss:  1.51167483e-02\n",
      "Epoch: 3118 mean train loss:  1.67295285e-02, mean val. rec. loss:  1.51132897e-02\n",
      "Epoch: 3119 mean train loss:  1.67262604e-02, mean val. rec. loss:  1.51098356e-02\n",
      "Epoch: 3120 mean train loss:  1.67229959e-02, mean val. rec. loss:  1.51063815e-02\n",
      "Epoch: 3121 mean train loss:  1.67197297e-02, mean val. rec. loss:  1.51029240e-02\n",
      "Epoch: 3122 mean train loss:  1.67164615e-02, mean val. rec. loss:  1.50994778e-02\n",
      "Epoch: 3123 mean train loss:  1.67131952e-02, mean val. rec. loss:  1.50960362e-02\n",
      "Epoch: 3124 mean train loss:  1.67099401e-02, mean val. rec. loss:  1.50925980e-02\n",
      "Epoch: 3125 mean train loss:  1.67066869e-02, mean val. rec. loss:  1.50891609e-02\n",
      "Epoch: 3126 mean train loss:  1.67034280e-02, mean val. rec. loss:  1.50857148e-02\n",
      "Epoch: 3127 mean train loss:  1.67001729e-02, mean val. rec. loss:  1.50822856e-02\n",
      "Epoch: 3128 mean train loss:  1.66969253e-02, mean val. rec. loss:  1.50788531e-02\n",
      "Epoch: 3129 mean train loss:  1.66936795e-02, mean val. rec. loss:  1.50754160e-02\n",
      "Epoch: 3130 mean train loss:  1.66904300e-02, mean val. rec. loss:  1.50719891e-02\n",
      "Epoch: 3131 mean train loss:  1.66871804e-02, mean val. rec. loss:  1.50685668e-02\n",
      "Epoch: 3132 mean train loss:  1.66839421e-02, mean val. rec. loss:  1.50651455e-02\n",
      "Epoch: 3133 mean train loss:  1.66807037e-02, mean val. rec. loss:  1.50617311e-02\n",
      "Epoch: 3134 mean train loss:  1.66774635e-02, mean val. rec. loss:  1.50583031e-02\n",
      "Epoch: 3135 mean train loss:  1.66742233e-02, mean val. rec. loss:  1.50548910e-02\n",
      "Epoch: 3136 mean train loss:  1.66709924e-02, mean val. rec. loss:  1.50514789e-02\n",
      "Epoch: 3137 mean train loss:  1.66677652e-02, mean val. rec. loss:  1.50480656e-02\n",
      "Epoch: 3138 mean train loss:  1.66645269e-02, mean val. rec. loss:  1.50446648e-02\n",
      "Epoch: 3139 mean train loss:  1.66613053e-02, mean val. rec. loss:  1.50412561e-02\n",
      "Epoch: 3140 mean train loss:  1.66580725e-02, mean val. rec. loss:  1.50378587e-02\n",
      "Epoch: 3141 mean train loss:  1.66548547e-02, mean val. rec. loss:  1.50344556e-02\n",
      "Epoch: 3142 mean train loss:  1.66516256e-02, mean val. rec. loss:  1.50310639e-02\n",
      "Epoch: 3143 mean train loss:  1.66484115e-02, mean val. rec. loss:  1.50276642e-02\n",
      "Epoch: 3144 mean train loss:  1.66451862e-02, mean val. rec. loss:  1.50242782e-02\n",
      "Epoch: 3145 mean train loss:  1.66419776e-02, mean val. rec. loss:  1.50208819e-02\n",
      "Epoch: 3146 mean train loss:  1.66387542e-02, mean val. rec. loss:  1.50175015e-02\n",
      "Epoch: 3147 mean train loss:  1.66355475e-02, mean val. rec. loss:  1.50141121e-02\n",
      "Epoch: 3148 mean train loss:  1.66323333e-02, mean val. rec. loss:  1.50107351e-02\n",
      "Epoch: 3149 mean train loss:  1.66291304e-02, mean val. rec. loss:  1.50073502e-02\n",
      "Epoch: 3150 mean train loss:  1.66259200e-02, mean val. rec. loss:  1.50039789e-02\n",
      "Epoch: 3151 mean train loss:  1.66227226e-02, mean val. rec. loss:  1.50005985e-02\n",
      "Epoch: 3152 mean train loss:  1.66195140e-02, mean val. rec. loss:  1.49972306e-02\n",
      "Epoch: 3153 mean train loss:  1.66163204e-02, mean val. rec. loss:  1.49938558e-02\n",
      "Epoch: 3154 mean train loss:  1.66131155e-02, mean val. rec. loss:  1.49904993e-02\n",
      "Epoch: 3155 mean train loss:  1.66099219e-02, mean val. rec. loss:  1.49871291e-02\n",
      "Epoch: 3156 mean train loss:  1.66067301e-02, mean val. rec. loss:  1.49837703e-02\n",
      "Epoch: 3157 mean train loss:  1.66035364e-02, mean val. rec. loss:  1.49804080e-02\n",
      "Epoch: 3158 mean train loss:  1.66003409e-02, mean val. rec. loss:  1.49770480e-02\n",
      "Epoch: 3159 mean train loss:  1.65971566e-02, mean val. rec. loss:  1.49737028e-02\n",
      "Epoch: 3160 mean train loss:  1.65939759e-02, mean val. rec. loss:  1.49703508e-02\n",
      "Epoch: 3161 mean train loss:  1.65907860e-02, mean val. rec. loss:  1.49669965e-02\n",
      "Epoch: 3162 mean train loss:  1.65876035e-02, mean val. rec. loss:  1.49636592e-02\n",
      "Epoch: 3163 mean train loss:  1.65844248e-02, mean val. rec. loss:  1.49603105e-02\n",
      "Epoch: 3164 mean train loss:  1.65812497e-02, mean val. rec. loss:  1.49569676e-02\n",
      "Epoch: 3165 mean train loss:  1.65780710e-02, mean val. rec. loss:  1.49536246e-02\n",
      "Epoch: 3166 mean train loss:  1.65748941e-02, mean val. rec. loss:  1.49502839e-02\n",
      "Epoch: 3167 mean train loss:  1.65717265e-02, mean val. rec. loss:  1.49469591e-02\n",
      "Epoch: 3168 mean train loss:  1.65685608e-02, mean val. rec. loss:  1.49436241e-02\n",
      "Epoch: 3169 mean train loss:  1.65653876e-02, mean val. rec. loss:  1.49402913e-02\n",
      "Epoch: 3170 mean train loss:  1.65622181e-02, mean val. rec. loss:  1.49369767e-02\n",
      "Epoch: 3171 mean train loss:  1.65590599e-02, mean val. rec. loss:  1.49336474e-02\n",
      "Epoch: 3172 mean train loss:  1.65559035e-02, mean val. rec. loss:  1.49303259e-02\n",
      "Epoch: 3173 mean train loss:  1.65527433e-02, mean val. rec. loss:  1.49270079e-02\n",
      "Epoch: 3174 mean train loss:  1.65495850e-02, mean val. rec. loss:  1.49236910e-02\n",
      "Epoch: 3175 mean train loss:  1.65464268e-02, mean val. rec. loss:  1.49203776e-02\n",
      "Epoch: 3176 mean train loss:  1.65432722e-02, mean val. rec. loss:  1.49170675e-02\n",
      "Epoch: 3177 mean train loss:  1.65401214e-02, mean val. rec. loss:  1.49137585e-02\n",
      "Epoch: 3178 mean train loss:  1.65369706e-02, mean val. rec. loss:  1.49104541e-02\n",
      "Epoch: 3179 mean train loss:  1.65338235e-02, mean val. rec. loss:  1.49071509e-02\n",
      "Epoch: 3180 mean train loss:  1.65306782e-02, mean val. rec. loss:  1.49038499e-02\n",
      "Epoch: 3181 mean train loss:  1.65275367e-02, mean val. rec. loss:  1.49005522e-02\n",
      "Epoch: 3182 mean train loss:  1.65243933e-02, mean val. rec. loss:  1.48972546e-02\n",
      "Epoch: 3183 mean train loss:  1.65212537e-02, mean val. rec. loss:  1.48939638e-02\n",
      "Epoch: 3184 mean train loss:  1.65181159e-02, mean val. rec. loss:  1.48906730e-02\n",
      "Epoch: 3185 mean train loss:  1.65149818e-02, mean val. rec. loss:  1.48873822e-02\n",
      "Epoch: 3186 mean train loss:  1.65118459e-02, mean val. rec. loss:  1.48840994e-02\n",
      "Epoch: 3187 mean train loss:  1.65087174e-02, mean val. rec. loss:  1.48808165e-02\n",
      "Epoch: 3188 mean train loss:  1.65055889e-02, mean val. rec. loss:  1.48775371e-02\n",
      "Epoch: 3189 mean train loss:  1.65024604e-02, mean val. rec. loss:  1.48742565e-02\n",
      "Epoch: 3190 mean train loss:  1.64993357e-02, mean val. rec. loss:  1.48709827e-02\n",
      "Epoch: 3191 mean train loss:  1.64962128e-02, mean val. rec. loss:  1.48677032e-02\n",
      "Epoch: 3192 mean train loss:  1.64930862e-02, mean val. rec. loss:  1.48644351e-02\n",
      "Epoch: 3193 mean train loss:  1.64899633e-02, mean val. rec. loss:  1.48611738e-02\n",
      "Epoch: 3194 mean train loss:  1.64868516e-02, mean val. rec. loss:  1.48579091e-02\n",
      "Epoch: 3195 mean train loss:  1.64837380e-02, mean val. rec. loss:  1.48546534e-02\n",
      "Epoch: 3196 mean train loss:  1.64806225e-02, mean val. rec. loss:  1.48513876e-02\n",
      "Epoch: 3197 mean train loss:  1.64775071e-02, mean val. rec. loss:  1.48481285e-02\n",
      "Epoch: 3198 mean train loss:  1.64744010e-02, mean val. rec. loss:  1.48448763e-02\n",
      "Epoch: 3199 mean train loss:  1.64712967e-02, mean val. rec. loss:  1.48416149e-02\n",
      "Epoch: 3200 mean train loss:  1.64681868e-02, mean val. rec. loss:  1.48383695e-02\n",
      "Epoch: 3201 mean train loss:  1.64650788e-02, mean val. rec. loss:  1.48351252e-02\n",
      "Epoch: 3202 mean train loss:  1.64619839e-02, mean val. rec. loss:  1.48318843e-02\n",
      "Epoch: 3203 mean train loss:  1.64588889e-02, mean val. rec. loss:  1.48286468e-02\n",
      "Epoch: 3204 mean train loss:  1.64557865e-02, mean val. rec. loss:  1.48253968e-02\n",
      "Epoch: 3205 mean train loss:  1.64526897e-02, mean val. rec. loss:  1.48221638e-02\n",
      "Epoch: 3206 mean train loss:  1.64496003e-02, mean val. rec. loss:  1.48189275e-02\n",
      "Epoch: 3207 mean train loss:  1.64465128e-02, mean val. rec. loss:  1.48156877e-02\n",
      "Epoch: 3208 mean train loss:  1.64434197e-02, mean val. rec. loss:  1.48124649e-02\n",
      "Epoch: 3209 mean train loss:  1.64403303e-02, mean val. rec. loss:  1.48092411e-02\n",
      "Epoch: 3210 mean train loss:  1.64372484e-02, mean val. rec. loss:  1.48060172e-02\n",
      "Epoch: 3211 mean train loss:  1.64341702e-02, mean val. rec. loss:  1.48027921e-02\n",
      "Epoch: 3212 mean train loss:  1.64310808e-02, mean val. rec. loss:  1.47995784e-02\n",
      "Epoch: 3213 mean train loss:  1.64280101e-02, mean val. rec. loss:  1.47963557e-02\n",
      "Epoch: 3214 mean train loss:  1.64249226e-02, mean val. rec. loss:  1.47931465e-02\n",
      "Epoch: 3215 mean train loss:  1.64218555e-02, mean val. rec. loss:  1.47899328e-02\n",
      "Epoch: 3216 mean train loss:  1.64187736e-02, mean val. rec. loss:  1.47867282e-02\n",
      "Epoch: 3217 mean train loss:  1.64157103e-02, mean val. rec. loss:  1.47835191e-02\n",
      "Epoch: 3218 mean train loss:  1.64126358e-02, mean val. rec. loss:  1.47803212e-02\n",
      "Epoch: 3219 mean train loss:  1.64095744e-02, mean val. rec. loss:  1.47771144e-02\n",
      "Epoch: 3220 mean train loss:  1.64065018e-02, mean val. rec. loss:  1.47739211e-02\n",
      "Epoch: 3221 mean train loss:  1.64034478e-02, mean val. rec. loss:  1.47707187e-02\n",
      "Epoch: 3222 mean train loss:  1.64003770e-02, mean val. rec. loss:  1.47675311e-02\n",
      "Epoch: 3223 mean train loss:  1.63973249e-02, mean val. rec. loss:  1.47643367e-02\n",
      "Epoch: 3224 mean train loss:  1.63942635e-02, mean val. rec. loss:  1.47611525e-02\n",
      "Epoch: 3225 mean train loss:  1.63912132e-02, mean val. rec. loss:  1.47579638e-02\n",
      "Epoch: 3226 mean train loss:  1.63881555e-02, mean val. rec. loss:  1.47547830e-02\n",
      "Epoch: 3227 mean train loss:  1.63851108e-02, mean val. rec. loss:  1.47515976e-02\n",
      "Epoch: 3228 mean train loss:  1.63820549e-02, mean val. rec. loss:  1.47484225e-02\n",
      "Epoch: 3229 mean train loss:  1.63790140e-02, mean val. rec. loss:  1.47452417e-02\n",
      "Epoch: 3230 mean train loss:  1.63759618e-02, mean val. rec. loss:  1.47420722e-02\n",
      "Epoch: 3231 mean train loss:  1.63729265e-02, mean val. rec. loss:  1.47388982e-02\n",
      "Epoch: 3232 mean train loss:  1.63698781e-02, mean val. rec. loss:  1.47357310e-02\n",
      "Epoch: 3233 mean train loss:  1.63668464e-02, mean val. rec. loss:  1.47325672e-02\n",
      "Epoch: 3234 mean train loss:  1.63638073e-02, mean val. rec. loss:  1.47293989e-02\n",
      "Epoch: 3235 mean train loss:  1.63607664e-02, mean val. rec. loss:  1.47262362e-02\n",
      "Epoch: 3236 mean train loss:  1.63577366e-02, mean val. rec. loss:  1.47230872e-02\n",
      "Epoch: 3237 mean train loss:  1.63547068e-02, mean val. rec. loss:  1.47199325e-02\n",
      "Epoch: 3238 mean train loss:  1.63516752e-02, mean val. rec. loss:  1.47167766e-02\n",
      "Epoch: 3239 mean train loss:  1.63486435e-02, mean val. rec. loss:  1.47136355e-02\n",
      "Epoch: 3240 mean train loss:  1.63456212e-02, mean val. rec. loss:  1.47104830e-02\n",
      "Epoch: 3241 mean train loss:  1.63426026e-02, mean val. rec. loss:  1.47073362e-02\n",
      "Epoch: 3242 mean train loss:  1.63395746e-02, mean val. rec. loss:  1.47041929e-02\n",
      "Epoch: 3243 mean train loss:  1.63365542e-02, mean val. rec. loss:  1.47010495e-02\n",
      "Epoch: 3244 mean train loss:  1.63335411e-02, mean val. rec. loss:  1.46979197e-02\n",
      "Epoch: 3245 mean train loss:  1.63305281e-02, mean val. rec. loss:  1.46947843e-02\n",
      "Epoch: 3246 mean train loss:  1.63275114e-02, mean val. rec. loss:  1.46916477e-02\n",
      "Epoch: 3247 mean train loss:  1.63244946e-02, mean val. rec. loss:  1.46885258e-02\n",
      "Epoch: 3248 mean train loss:  1.63214890e-02, mean val. rec. loss:  1.46853961e-02\n",
      "Epoch: 3249 mean train loss:  1.63184872e-02, mean val. rec. loss:  1.46822720e-02\n",
      "Epoch: 3250 mean train loss:  1.63154779e-02, mean val. rec. loss:  1.46791433e-02\n",
      "Epoch: 3251 mean train loss:  1.63124723e-02, mean val. rec. loss:  1.46760226e-02\n",
      "Epoch: 3252 mean train loss:  1.63094742e-02, mean val. rec. loss:  1.46729121e-02\n",
      "Epoch: 3253 mean train loss:  1.63064779e-02, mean val. rec. loss:  1.46697959e-02\n",
      "Epoch: 3254 mean train loss:  1.63034779e-02, mean val. rec. loss:  1.46666809e-02\n",
      "Epoch: 3255 mean train loss:  1.63004798e-02, mean val. rec. loss:  1.46635795e-02\n",
      "Epoch: 3256 mean train loss:  1.62974891e-02, mean val. rec. loss:  1.46604667e-02\n",
      "Epoch: 3257 mean train loss:  1.62945003e-02, mean val. rec. loss:  1.46573619e-02\n",
      "Epoch: 3258 mean train loss:  1.62915115e-02, mean val. rec. loss:  1.46542639e-02\n",
      "Epoch: 3259 mean train loss:  1.62885208e-02, mean val. rec. loss:  1.46511613e-02\n",
      "Epoch: 3260 mean train loss:  1.62855339e-02, mean val. rec. loss:  1.46480633e-02\n",
      "Epoch: 3261 mean train loss:  1.62825506e-02, mean val. rec. loss:  1.46449721e-02\n",
      "Epoch: 3262 mean train loss:  1.62795693e-02, mean val. rec. loss:  1.46418797e-02\n",
      "Epoch: 3263 mean train loss:  1.62765879e-02, mean val. rec. loss:  1.46387896e-02\n",
      "Epoch: 3264 mean train loss:  1.62736084e-02, mean val. rec. loss:  1.46357052e-02\n",
      "Epoch: 3265 mean train loss:  1.62706345e-02, mean val. rec. loss:  1.46326208e-02\n",
      "Epoch: 3266 mean train loss:  1.62676606e-02, mean val. rec. loss:  1.46295386e-02\n",
      "Epoch: 3267 mean train loss:  1.62646866e-02, mean val. rec. loss:  1.46264610e-02\n",
      "Epoch: 3268 mean train loss:  1.62617165e-02, mean val. rec. loss:  1.46233823e-02\n",
      "Epoch: 3269 mean train loss:  1.62587481e-02, mean val. rec. loss:  1.46203092e-02\n",
      "Epoch: 3270 mean train loss:  1.62557835e-02, mean val. rec. loss:  1.46172373e-02\n",
      "Epoch: 3271 mean train loss:  1.62528189e-02, mean val. rec. loss:  1.46141676e-02\n",
      "Epoch: 3272 mean train loss:  1.62498543e-02, mean val. rec. loss:  1.46111025e-02\n",
      "Epoch: 3273 mean train loss:  1.62468953e-02, mean val. rec. loss:  1.46080362e-02\n",
      "Epoch: 3274 mean train loss:  1.62439363e-02, mean val. rec. loss:  1.46049722e-02\n",
      "Epoch: 3275 mean train loss:  1.62409791e-02, mean val. rec. loss:  1.46019150e-02\n",
      "Epoch: 3276 mean train loss:  1.62380257e-02, mean val. rec. loss:  1.45988555e-02\n",
      "Epoch: 3277 mean train loss:  1.62350722e-02, mean val. rec. loss:  1.45958028e-02\n",
      "Epoch: 3278 mean train loss:  1.62321225e-02, mean val. rec. loss:  1.45927490e-02\n",
      "Epoch: 3279 mean train loss:  1.62291728e-02, mean val. rec. loss:  1.45896998e-02\n",
      "Epoch: 3280 mean train loss:  1.62262268e-02, mean val. rec. loss:  1.45866517e-02\n",
      "Epoch: 3281 mean train loss:  1.62232827e-02, mean val. rec. loss:  1.45836081e-02\n",
      "Epoch: 3282 mean train loss:  1.62203404e-02, mean val. rec. loss:  1.45805645e-02\n",
      "Epoch: 3283 mean train loss:  1.62174000e-02, mean val. rec. loss:  1.45775265e-02\n",
      "Epoch: 3284 mean train loss:  1.62144596e-02, mean val. rec. loss:  1.45744943e-02\n",
      "Epoch: 3285 mean train loss:  1.62115174e-02, mean val. rec. loss:  1.45714518e-02\n",
      "Epoch: 3286 mean train loss:  1.62085788e-02, mean val. rec. loss:  1.45684252e-02\n",
      "Epoch: 3287 mean train loss:  1.62056478e-02, mean val. rec. loss:  1.45653953e-02\n",
      "Epoch: 3288 mean train loss:  1.62027204e-02, mean val. rec. loss:  1.45623619e-02\n",
      "Epoch: 3289 mean train loss:  1.61997874e-02, mean val. rec. loss:  1.45593421e-02\n",
      "Epoch: 3290 mean train loss:  1.61968545e-02, mean val. rec. loss:  1.45563234e-02\n",
      "Epoch: 3291 mean train loss:  1.61939327e-02, mean val. rec. loss:  1.45533048e-02\n",
      "Epoch: 3292 mean train loss:  1.61910128e-02, mean val. rec. loss:  1.45502930e-02\n",
      "Epoch: 3293 mean train loss:  1.61880854e-02, mean val. rec. loss:  1.45472720e-02\n",
      "Epoch: 3294 mean train loss:  1.61851637e-02, mean val. rec. loss:  1.45442636e-02\n",
      "Epoch: 3295 mean train loss:  1.61822493e-02, mean val. rec. loss:  1.45412574e-02\n",
      "Epoch: 3296 mean train loss:  1.61793387e-02, mean val. rec. loss:  1.45382422e-02\n",
      "Epoch: 3297 mean train loss:  1.61764207e-02, mean val. rec. loss:  1.45352394e-02\n",
      "Epoch: 3298 mean train loss:  1.61735045e-02, mean val. rec. loss:  1.45322435e-02\n",
      "Epoch: 3299 mean train loss:  1.61705995e-02, mean val. rec. loss:  1.45292475e-02\n",
      "Epoch: 3300 mean train loss:  1.61676945e-02, mean val. rec. loss:  1.45262549e-02\n",
      "Epoch: 3301 mean train loss:  1.61647839e-02, mean val. rec. loss:  1.45232510e-02\n",
      "Epoch: 3302 mean train loss:  1.61618751e-02, mean val. rec. loss:  1.45202641e-02\n",
      "Epoch: 3303 mean train loss:  1.61589757e-02, mean val. rec. loss:  1.45172738e-02\n",
      "Epoch: 3304 mean train loss:  1.61560818e-02, mean val. rec. loss:  1.45142790e-02\n",
      "Epoch: 3305 mean train loss:  1.61531805e-02, mean val. rec. loss:  1.45112989e-02\n",
      "Epoch: 3306 mean train loss:  1.61502793e-02, mean val. rec. loss:  1.45083199e-02\n",
      "Epoch: 3307 mean train loss:  1.61473891e-02, mean val. rec. loss:  1.45053433e-02\n",
      "Epoch: 3308 mean train loss:  1.61444990e-02, mean val. rec. loss:  1.45023700e-02\n",
      "Epoch: 3309 mean train loss:  1.61416033e-02, mean val. rec. loss:  1.44993865e-02\n",
      "Epoch: 3310 mean train loss:  1.61387113e-02, mean val. rec. loss:  1.44964177e-02\n",
      "Epoch: 3311 mean train loss:  1.61358287e-02, mean val. rec. loss:  1.44934501e-02\n",
      "Epoch: 3312 mean train loss:  1.61329460e-02, mean val. rec. loss:  1.44904757e-02\n",
      "Epoch: 3313 mean train loss:  1.61300614e-02, mean val. rec. loss:  1.44875137e-02\n",
      "Epoch: 3314 mean train loss:  1.61271769e-02, mean val. rec. loss:  1.44845541e-02\n",
      "Epoch: 3315 mean train loss:  1.61242998e-02, mean val. rec. loss:  1.44815967e-02\n",
      "Epoch: 3316 mean train loss:  1.61214283e-02, mean val. rec. loss:  1.44786449e-02\n",
      "Epoch: 3317 mean train loss:  1.61185475e-02, mean val. rec. loss:  1.44756784e-02\n",
      "Epoch: 3318 mean train loss:  1.61156704e-02, mean val. rec. loss:  1.44727324e-02\n",
      "Epoch: 3319 mean train loss:  1.61128083e-02, mean val. rec. loss:  1.44697784e-02\n",
      "Epoch: 3320 mean train loss:  1.61099274e-02, mean val. rec. loss:  1.44668357e-02\n",
      "Epoch: 3321 mean train loss:  1.61070671e-02, mean val. rec. loss:  1.44638896e-02\n",
      "Epoch: 3322 mean train loss:  1.61041938e-02, mean val. rec. loss:  1.44609549e-02\n",
      "Epoch: 3323 mean train loss:  1.61013372e-02, mean val. rec. loss:  1.44580088e-02\n",
      "Epoch: 3324 mean train loss:  1.60984675e-02, mean val. rec. loss:  1.44550775e-02\n",
      "Epoch: 3325 mean train loss:  1.60956128e-02, mean val. rec. loss:  1.44521382e-02\n",
      "Epoch: 3326 mean train loss:  1.60927450e-02, mean val. rec. loss:  1.44492114e-02\n",
      "Epoch: 3327 mean train loss:  1.60898977e-02, mean val. rec. loss:  1.44462767e-02\n",
      "Epoch: 3328 mean train loss:  1.60870337e-02, mean val. rec. loss:  1.44433567e-02\n",
      "Epoch: 3329 mean train loss:  1.60841901e-02, mean val. rec. loss:  1.44404265e-02\n",
      "Epoch: 3330 mean train loss:  1.60813298e-02, mean val. rec. loss:  1.44375076e-02\n",
      "Epoch: 3331 mean train loss:  1.60784881e-02, mean val. rec. loss:  1.44345831e-02\n",
      "Epoch: 3332 mean train loss:  1.60756315e-02, mean val. rec. loss:  1.44316699e-02\n",
      "Epoch: 3333 mean train loss:  1.60727954e-02, mean val. rec. loss:  1.44287499e-02\n",
      "Epoch: 3334 mean train loss:  1.60699444e-02, mean val. rec. loss:  1.44258458e-02\n",
      "Epoch: 3335 mean train loss:  1.60671101e-02, mean val. rec. loss:  1.44229304e-02\n",
      "Epoch: 3336 mean train loss:  1.60642628e-02, mean val. rec. loss:  1.44200263e-02\n",
      "Epoch: 3337 mean train loss:  1.60614323e-02, mean val. rec. loss:  1.44171176e-02\n",
      "Epoch: 3338 mean train loss:  1.60585887e-02, mean val. rec. loss:  1.44142180e-02\n",
      "Epoch: 3339 mean train loss:  1.60557619e-02, mean val. rec. loss:  1.44113117e-02\n",
      "Epoch: 3340 mean train loss:  1.60529221e-02, mean val. rec. loss:  1.44084200e-02\n",
      "Epoch: 3341 mean train loss:  1.60501009e-02, mean val. rec. loss:  1.44055193e-02\n",
      "Epoch: 3342 mean train loss:  1.60472647e-02, mean val. rec. loss:  1.44026299e-02\n",
      "Epoch: 3343 mean train loss:  1.60444472e-02, mean val. rec. loss:  1.43997349e-02\n",
      "Epoch: 3344 mean train loss:  1.60416130e-02, mean val. rec. loss:  1.43968512e-02\n",
      "Epoch: 3345 mean train loss:  1.60387955e-02, mean val. rec. loss:  1.43939595e-02\n",
      "Epoch: 3346 mean train loss:  1.60359687e-02, mean val. rec. loss:  1.43910815e-02\n",
      "Epoch: 3347 mean train loss:  1.60331549e-02, mean val. rec. loss:  1.43881944e-02\n",
      "Epoch: 3348 mean train loss:  1.60303300e-02, mean val. rec. loss:  1.43853209e-02\n",
      "Epoch: 3349 mean train loss:  1.60275237e-02, mean val. rec. loss:  1.43824406e-02\n",
      "Epoch: 3350 mean train loss:  1.60247043e-02, mean val. rec. loss:  1.43795705e-02\n",
      "Epoch: 3351 mean train loss:  1.60218998e-02, mean val. rec. loss:  1.43766959e-02\n",
      "Epoch: 3352 mean train loss:  1.60190823e-02, mean val. rec. loss:  1.43738292e-02\n",
      "Epoch: 3353 mean train loss:  1.60162816e-02, mean val. rec. loss:  1.43709568e-02\n",
      "Epoch: 3354 mean train loss:  1.60134678e-02, mean val. rec. loss:  1.43681003e-02\n",
      "Epoch: 3355 mean train loss:  1.60106727e-02, mean val. rec. loss:  1.43652314e-02\n",
      "Epoch: 3356 mean train loss:  1.60078626e-02, mean val. rec. loss:  1.43623783e-02\n",
      "Epoch: 3357 mean train loss:  1.60050694e-02, mean val. rec. loss:  1.43595161e-02\n",
      "Epoch: 3358 mean train loss:  1.60022649e-02, mean val. rec. loss:  1.43566653e-02\n",
      "Epoch: 3359 mean train loss:  1.59994716e-02, mean val. rec. loss:  1.43538111e-02\n",
      "Epoch: 3360 mean train loss:  1.59966727e-02, mean val. rec. loss:  1.43509648e-02\n",
      "Epoch: 3361 mean train loss:  1.59938869e-02, mean val. rec. loss:  1.43481129e-02\n",
      "Epoch: 3362 mean train loss:  1.59910880e-02, mean val. rec. loss:  1.43452734e-02\n",
      "Epoch: 3363 mean train loss:  1.59883078e-02, mean val. rec. loss:  1.43424226e-02\n",
      "Epoch: 3364 mean train loss:  1.59855108e-02, mean val. rec. loss:  1.43395876e-02\n",
      "Epoch: 3365 mean train loss:  1.59827324e-02, mean val. rec. loss:  1.43367459e-02\n",
      "Epoch: 3366 mean train loss:  1.59799410e-02, mean val. rec. loss:  1.43339087e-02\n",
      "Epoch: 3367 mean train loss:  1.59771644e-02, mean val. rec. loss:  1.43310839e-02\n",
      "Epoch: 3368 mean train loss:  1.59743879e-02, mean val. rec. loss:  1.43282558e-02\n",
      "Epoch: 3369 mean train loss:  1.59716058e-02, mean val. rec. loss:  1.43254243e-02\n",
      "Epoch: 3370 mean train loss:  1.59688237e-02, mean val. rec. loss:  1.43226086e-02\n",
      "Epoch: 3371 mean train loss:  1.59660509e-02, mean val. rec. loss:  1.43197816e-02\n",
      "Epoch: 3372 mean train loss:  1.59632818e-02, mean val. rec. loss:  1.43169625e-02\n",
      "Epoch: 3373 mean train loss:  1.59605071e-02, mean val. rec. loss:  1.43141401e-02\n",
      "Epoch: 3374 mean train loss:  1.59577325e-02, mean val. rec. loss:  1.43113233e-02\n",
      "Epoch: 3375 mean train loss:  1.59549708e-02, mean val. rec. loss:  1.43085167e-02\n",
      "Epoch: 3376 mean train loss:  1.59522055e-02, mean val. rec. loss:  1.43057067e-02\n",
      "Epoch: 3377 mean train loss:  1.59494420e-02, mean val. rec. loss:  1.43028956e-02\n",
      "Epoch: 3378 mean train loss:  1.59466729e-02, mean val. rec. loss:  1.43000980e-02\n",
      "Epoch: 3379 mean train loss:  1.59439150e-02, mean val. rec. loss:  1.42972892e-02\n",
      "Epoch: 3380 mean train loss:  1.59411627e-02, mean val. rec. loss:  1.42944883e-02\n",
      "Epoch: 3381 mean train loss:  1.59384011e-02, mean val. rec. loss:  1.42916851e-02\n",
      "Epoch: 3382 mean train loss:  1.59356432e-02, mean val. rec. loss:  1.42888864e-02\n",
      "Epoch: 3383 mean train loss:  1.59328946e-02, mean val. rec. loss:  1.42860991e-02\n",
      "Epoch: 3384 mean train loss:  1.59301460e-02, mean val. rec. loss:  1.42833061e-02\n",
      "Epoch: 3385 mean train loss:  1.59273918e-02, mean val. rec. loss:  1.42805131e-02\n",
      "Epoch: 3386 mean train loss:  1.59246414e-02, mean val. rec. loss:  1.42777383e-02\n",
      "Epoch: 3387 mean train loss:  1.59219002e-02, mean val. rec. loss:  1.42749476e-02\n",
      "Epoch: 3388 mean train loss:  1.59191591e-02, mean val. rec. loss:  1.42721614e-02\n",
      "Epoch: 3389 mean train loss:  1.59164142e-02, mean val. rec. loss:  1.42693786e-02\n",
      "Epoch: 3390 mean train loss:  1.59136693e-02, mean val. rec. loss:  1.42665981e-02\n",
      "Epoch: 3391 mean train loss:  1.59109356e-02, mean val. rec. loss:  1.42638335e-02\n",
      "Epoch: 3392 mean train loss:  1.59082019e-02, mean val. rec. loss:  1.42610586e-02\n",
      "Epoch: 3393 mean train loss:  1.59054645e-02, mean val. rec. loss:  1.42582838e-02\n",
      "Epoch: 3394 mean train loss:  1.59027252e-02, mean val. rec. loss:  1.42555237e-02\n",
      "Epoch: 3395 mean train loss:  1.58999971e-02, mean val. rec. loss:  1.42527545e-02\n",
      "Epoch: 3396 mean train loss:  1.58972727e-02, mean val. rec. loss:  1.42499899e-02\n",
      "Epoch: 3397 mean train loss:  1.58945428e-02, mean val. rec. loss:  1.42472230e-02\n",
      "Epoch: 3398 mean train loss:  1.58918128e-02, mean val. rec. loss:  1.42444629e-02\n",
      "Epoch: 3399 mean train loss:  1.58890940e-02, mean val. rec. loss:  1.42417141e-02\n",
      "Epoch: 3400 mean train loss:  1.58863733e-02, mean val. rec. loss:  1.42389586e-02\n",
      "Epoch: 3401 mean train loss:  1.58836490e-02, mean val. rec. loss:  1.42362030e-02\n",
      "Epoch: 3402 mean train loss:  1.58809283e-02, mean val. rec. loss:  1.42334633e-02\n",
      "Epoch: 3403 mean train loss:  1.58782132e-02, mean val. rec. loss:  1.42307111e-02\n",
      "Epoch: 3404 mean train loss:  1.58755037e-02, mean val. rec. loss:  1.42279658e-02\n",
      "Epoch: 3405 mean train loss:  1.58727868e-02, mean val. rec. loss:  1.42252182e-02\n",
      "Epoch: 3406 mean train loss:  1.58700717e-02, mean val. rec. loss:  1.42224773e-02\n",
      "Epoch: 3407 mean train loss:  1.58673678e-02, mean val. rec. loss:  1.42197467e-02\n",
      "Epoch: 3408 mean train loss:  1.58646602e-02, mean val. rec. loss:  1.42170104e-02\n",
      "Epoch: 3409 mean train loss:  1.58619526e-02, mean val. rec. loss:  1.42142719e-02\n",
      "Epoch: 3410 mean train loss:  1.58592431e-02, mean val. rec. loss:  1.42115503e-02\n",
      "Epoch: 3411 mean train loss:  1.58565448e-02, mean val. rec. loss:  1.42088175e-02\n",
      "Epoch: 3412 mean train loss:  1.58538483e-02, mean val. rec. loss:  1.42060914e-02\n",
      "Epoch: 3413 mean train loss:  1.58511463e-02, mean val. rec. loss:  1.42033630e-02\n",
      "Epoch: 3414 mean train loss:  1.58484442e-02, mean val. rec. loss:  1.42006381e-02\n",
      "Epoch: 3415 mean train loss:  1.58457552e-02, mean val. rec. loss:  1.41979267e-02\n",
      "Epoch: 3416 mean train loss:  1.58430662e-02, mean val. rec. loss:  1.41952063e-02\n",
      "Epoch: 3417 mean train loss:  1.58403679e-02, mean val. rec. loss:  1.41924905e-02\n",
      "Epoch: 3418 mean train loss:  1.58376733e-02, mean val. rec. loss:  1.41897871e-02\n",
      "Epoch: 3419 mean train loss:  1.58349918e-02, mean val. rec. loss:  1.41870735e-02\n",
      "Epoch: 3420 mean train loss:  1.58323084e-02, mean val. rec. loss:  1.41843621e-02\n",
      "Epoch: 3421 mean train loss:  1.58296212e-02, mean val. rec. loss:  1.41816542e-02\n",
      "Epoch: 3422 mean train loss:  1.58269341e-02, mean val. rec. loss:  1.41789496e-02\n",
      "Epoch: 3423 mean train loss:  1.58242562e-02, mean val. rec. loss:  1.41762553e-02\n",
      "Epoch: 3424 mean train loss:  1.58215803e-02, mean val. rec. loss:  1.41735576e-02\n",
      "Epoch: 3425 mean train loss:  1.58188987e-02, mean val. rec. loss:  1.41708565e-02\n",
      "Epoch: 3426 mean train loss:  1.58162209e-02, mean val. rec. loss:  1.41681735e-02\n",
      "Epoch: 3427 mean train loss:  1.58135505e-02, mean val. rec. loss:  1.41654757e-02\n",
      "Epoch: 3428 mean train loss:  1.58108801e-02, mean val. rec. loss:  1.41627871e-02\n",
      "Epoch: 3429 mean train loss:  1.58082079e-02, mean val. rec. loss:  1.41600973e-02\n",
      "Epoch: 3430 mean train loss:  1.58055319e-02, mean val. rec. loss:  1.41574098e-02\n",
      "Epoch: 3431 mean train loss:  1.58028727e-02, mean val. rec. loss:  1.41547336e-02\n",
      "Epoch: 3432 mean train loss:  1.58002098e-02, mean val. rec. loss:  1.41520551e-02\n",
      "Epoch: 3433 mean train loss:  1.57975431e-02, mean val. rec. loss:  1.41493767e-02\n",
      "Epoch: 3434 mean train loss:  1.57948802e-02, mean val. rec. loss:  1.41467084e-02\n",
      "Epoch: 3435 mean train loss:  1.57922210e-02, mean val. rec. loss:  1.41440311e-02\n",
      "Epoch: 3436 mean train loss:  1.57895692e-02, mean val. rec. loss:  1.41413595e-02\n",
      "Epoch: 3437 mean train loss:  1.57869063e-02, mean val. rec. loss:  1.41386855e-02\n",
      "Epoch: 3438 mean train loss:  1.57842489e-02, mean val. rec. loss:  1.41360184e-02\n",
      "Epoch: 3439 mean train loss:  1.57815990e-02, mean val. rec. loss:  1.41333615e-02\n",
      "Epoch: 3440 mean train loss:  1.57789547e-02, mean val. rec. loss:  1.41307001e-02\n",
      "Epoch: 3441 mean train loss:  1.57762993e-02, mean val. rec. loss:  1.41280375e-02\n",
      "Epoch: 3442 mean train loss:  1.57736475e-02, mean val. rec. loss:  1.41253908e-02\n",
      "Epoch: 3443 mean train loss:  1.57710069e-02, mean val. rec. loss:  1.41227316e-02\n",
      "Epoch: 3444 mean train loss:  1.57683663e-02, mean val. rec. loss:  1.41200770e-02\n",
      "Epoch: 3445 mean train loss:  1.57657202e-02, mean val. rec. loss:  1.41174246e-02\n",
      "Epoch: 3446 mean train loss:  1.57630777e-02, mean val. rec. loss:  1.41147745e-02\n",
      "Epoch: 3447 mean train loss:  1.57604408e-02, mean val. rec. loss:  1.41121357e-02\n",
      "Epoch: 3448 mean train loss:  1.57578077e-02, mean val. rec. loss:  1.41094902e-02\n",
      "Epoch: 3449 mean train loss:  1.57551690e-02, mean val. rec. loss:  1.41068457e-02\n",
      "Epoch: 3450 mean train loss:  1.57525303e-02, mean val. rec. loss:  1.41042183e-02\n",
      "Epoch: 3451 mean train loss:  1.57499008e-02, mean val. rec. loss:  1.41015750e-02\n",
      "Epoch: 3452 mean train loss:  1.57472770e-02, mean val. rec. loss:  1.40989397e-02\n",
      "Epoch: 3453 mean train loss:  1.57446457e-02, mean val. rec. loss:  1.40963066e-02\n",
      "Epoch: 3454 mean train loss:  1.57420145e-02, mean val. rec. loss:  1.40936757e-02\n",
      "Epoch: 3455 mean train loss:  1.57393925e-02, mean val. rec. loss:  1.40910540e-02\n",
      "Epoch: 3456 mean train loss:  1.57367724e-02, mean val. rec. loss:  1.40884277e-02\n",
      "Epoch: 3457 mean train loss:  1.57341467e-02, mean val. rec. loss:  1.40858014e-02\n",
      "Epoch: 3458 mean train loss:  1.57315247e-02, mean val. rec. loss:  1.40831910e-02\n",
      "Epoch: 3459 mean train loss:  1.57289102e-02, mean val. rec. loss:  1.40805670e-02\n",
      "Epoch: 3460 mean train loss:  1.57262994e-02, mean val. rec. loss:  1.40779498e-02\n",
      "Epoch: 3461 mean train loss:  1.57236812e-02, mean val. rec. loss:  1.40753337e-02\n",
      "Epoch: 3462 mean train loss:  1.57210648e-02, mean val. rec. loss:  1.40727210e-02\n",
      "Epoch: 3463 mean train loss:  1.57184596e-02, mean val. rec. loss:  1.40701174e-02\n",
      "Epoch: 3464 mean train loss:  1.57158526e-02, mean val. rec. loss:  1.40675104e-02\n",
      "Epoch: 3465 mean train loss:  1.57132399e-02, mean val. rec. loss:  1.40649022e-02\n",
      "Epoch: 3466 mean train loss:  1.57106291e-02, mean val. rec. loss:  1.40623077e-02\n",
      "Epoch: 3467 mean train loss:  1.57080276e-02, mean val. rec. loss:  1.40597029e-02\n",
      "Epoch: 3468 mean train loss:  1.57054299e-02, mean val. rec. loss:  1.40571050e-02\n",
      "Epoch: 3469 mean train loss:  1.57028247e-02, mean val. rec. loss:  1.40545048e-02\n",
      "Epoch: 3470 mean train loss:  1.57002232e-02, mean val. rec. loss:  1.40519114e-02\n",
      "Epoch: 3471 mean train loss:  1.56976292e-02, mean val. rec. loss:  1.40493270e-02\n",
      "Epoch: 3472 mean train loss:  1.56950407e-02, mean val. rec. loss:  1.40467359e-02\n",
      "Epoch: 3473 mean train loss:  1.56924411e-02, mean val. rec. loss:  1.40441470e-02\n",
      "Epoch: 3474 mean train loss:  1.56898452e-02, mean val. rec. loss:  1.40415729e-02\n",
      "Epoch: 3475 mean train loss:  1.56872586e-02, mean val. rec. loss:  1.40389840e-02\n",
      "Epoch: 3476 mean train loss:  1.56846720e-02, mean val. rec. loss:  1.40364031e-02\n",
      "Epoch: 3477 mean train loss:  1.56820817e-02, mean val. rec. loss:  1.40338245e-02\n",
      "Epoch: 3478 mean train loss:  1.56794933e-02, mean val. rec. loss:  1.40312458e-02\n",
      "Epoch: 3479 mean train loss:  1.56769141e-02, mean val. rec. loss:  1.40286807e-02\n",
      "Epoch: 3480 mean train loss:  1.56743332e-02, mean val. rec. loss:  1.40261066e-02\n",
      "Epoch: 3481 mean train loss:  1.56717484e-02, mean val. rec. loss:  1.40235347e-02\n",
      "Epoch: 3482 mean train loss:  1.56691674e-02, mean val. rec. loss:  1.40209765e-02\n",
      "Epoch: 3483 mean train loss:  1.56665939e-02, mean val. rec. loss:  1.40184092e-02\n",
      "Epoch: 3484 mean train loss:  1.56640241e-02, mean val. rec. loss:  1.40158441e-02\n",
      "Epoch: 3485 mean train loss:  1.56614449e-02, mean val. rec. loss:  1.40132791e-02\n",
      "Epoch: 3486 mean train loss:  1.56588695e-02, mean val. rec. loss:  1.40107208e-02\n",
      "Epoch: 3487 mean train loss:  1.56563034e-02, mean val. rec. loss:  1.40081716e-02\n",
      "Epoch: 3488 mean train loss:  1.56537355e-02, mean val. rec. loss:  1.40056202e-02\n",
      "Epoch: 3489 mean train loss:  1.56511694e-02, mean val. rec. loss:  1.40030631e-02\n",
      "Epoch: 3490 mean train loss:  1.56486051e-02, mean val. rec. loss:  1.40005071e-02\n",
      "Epoch: 3491 mean train loss:  1.56460428e-02, mean val. rec. loss:  1.39979647e-02\n",
      "Epoch: 3492 mean train loss:  1.56434822e-02, mean val. rec. loss:  1.39954257e-02\n",
      "Epoch: 3493 mean train loss:  1.56409217e-02, mean val. rec. loss:  1.39928766e-02\n",
      "Epoch: 3494 mean train loss:  1.56383668e-02, mean val. rec. loss:  1.39903296e-02\n",
      "Epoch: 3495 mean train loss:  1.56358119e-02, mean val. rec. loss:  1.39877963e-02\n",
      "Epoch: 3496 mean train loss:  1.56332570e-02, mean val. rec. loss:  1.39852653e-02\n",
      "Epoch: 3497 mean train loss:  1.56307020e-02, mean val. rec. loss:  1.39827252e-02\n",
      "Epoch: 3498 mean train loss:  1.56281527e-02, mean val. rec. loss:  1.39801874e-02\n",
      "Epoch: 3499 mean train loss:  1.56256033e-02, mean val. rec. loss:  1.39776643e-02\n",
      "Epoch: 3500 mean train loss:  1.56230559e-02, mean val. rec. loss:  1.39751434e-02\n",
      "Epoch: 3501 mean train loss:  1.56205112e-02, mean val. rec. loss:  1.39726090e-02\n",
      "Epoch: 3502 mean train loss:  1.56179665e-02, mean val. rec. loss:  1.39700814e-02\n",
      "Epoch: 3503 mean train loss:  1.56154255e-02, mean val. rec. loss:  1.39675662e-02\n",
      "Epoch: 3504 mean train loss:  1.56128837e-02, mean val. rec. loss:  1.39650544e-02\n",
      "Epoch: 3505 mean train loss:  1.56103436e-02, mean val. rec. loss:  1.39625302e-02\n",
      "Epoch: 3506 mean train loss:  1.56078064e-02, mean val. rec. loss:  1.39600105e-02\n",
      "Epoch: 3507 mean train loss:  1.56052701e-02, mean val. rec. loss:  1.39575033e-02\n",
      "Epoch: 3508 mean train loss:  1.56027357e-02, mean val. rec. loss:  1.39550006e-02\n",
      "Epoch: 3509 mean train loss:  1.56002040e-02, mean val. rec. loss:  1.39524866e-02\n",
      "Epoch: 3510 mean train loss:  1.55976724e-02, mean val. rec. loss:  1.39499726e-02\n",
      "Epoch: 3511 mean train loss:  1.55951435e-02, mean val. rec. loss:  1.39474767e-02\n",
      "Epoch: 3512 mean train loss:  1.55926165e-02, mean val. rec. loss:  1.39449808e-02\n",
      "Epoch: 3513 mean train loss:  1.55900877e-02, mean val. rec. loss:  1.39424758e-02\n",
      "Epoch: 3514 mean train loss:  1.55875653e-02, mean val. rec. loss:  1.39399731e-02\n",
      "Epoch: 3515 mean train loss:  1.55850411e-02, mean val. rec. loss:  1.39374852e-02\n",
      "Epoch: 3516 mean train loss:  1.55825216e-02, mean val. rec. loss:  1.39350007e-02\n",
      "Epoch: 3517 mean train loss:  1.55800020e-02, mean val. rec. loss:  1.39325014e-02\n",
      "Epoch: 3518 mean train loss:  1.55774843e-02, mean val. rec. loss:  1.39300100e-02\n",
      "Epoch: 3519 mean train loss:  1.55749695e-02, mean val. rec. loss:  1.39275278e-02\n",
      "Epoch: 3520 mean train loss:  1.55724536e-02, mean val. rec. loss:  1.39250523e-02\n",
      "Epoch: 3521 mean train loss:  1.55699406e-02, mean val. rec. loss:  1.39225655e-02\n",
      "Epoch: 3522 mean train loss:  1.55674304e-02, mean val. rec. loss:  1.39200798e-02\n",
      "Epoch: 3523 mean train loss:  1.55649211e-02, mean val. rec. loss:  1.39176089e-02\n",
      "Epoch: 3524 mean train loss:  1.55624127e-02, mean val. rec. loss:  1.39151413e-02\n",
      "Epoch: 3525 mean train loss:  1.55599090e-02, mean val. rec. loss:  1.39126613e-02\n",
      "Epoch: 3526 mean train loss:  1.55574025e-02, mean val. rec. loss:  1.39101847e-02\n",
      "Epoch: 3527 mean train loss:  1.55548988e-02, mean val. rec. loss:  1.39077229e-02\n",
      "Epoch: 3528 mean train loss:  1.55523978e-02, mean val. rec. loss:  1.39052633e-02\n",
      "Epoch: 3529 mean train loss:  1.55498979e-02, mean val. rec. loss:  1.39027946e-02\n",
      "Epoch: 3530 mean train loss:  1.55474007e-02, mean val. rec. loss:  1.39003282e-02\n",
      "Epoch: 3531 mean train loss:  1.55449035e-02, mean val. rec. loss:  1.38978731e-02\n",
      "Epoch: 3532 mean train loss:  1.55424091e-02, mean val. rec. loss:  1.38954237e-02\n",
      "Epoch: 3533 mean train loss:  1.55399184e-02, mean val. rec. loss:  1.38929596e-02\n",
      "Epoch: 3534 mean train loss:  1.55374258e-02, mean val. rec. loss:  1.38905068e-02\n",
      "Epoch: 3535 mean train loss:  1.55349407e-02, mean val. rec. loss:  1.38880472e-02\n",
      "Epoch: 3536 mean train loss:  1.55324491e-02, mean val. rec. loss:  1.38855990e-02\n",
      "Epoch: 3537 mean train loss:  1.55299603e-02, mean val. rec. loss:  1.38831553e-02\n",
      "Epoch: 3538 mean train loss:  1.55274789e-02, mean val. rec. loss:  1.38807104e-02\n",
      "Epoch: 3539 mean train loss:  1.55250004e-02, mean val. rec. loss:  1.38782701e-02\n",
      "Epoch: 3540 mean train loss:  1.55225162e-02, mean val. rec. loss:  1.38758207e-02\n",
      "Epoch: 3541 mean train loss:  1.55200339e-02, mean val. rec. loss:  1.38733838e-02\n",
      "Epoch: 3542 mean train loss:  1.55175600e-02, mean val. rec. loss:  1.38709469e-02\n",
      "Epoch: 3543 mean train loss:  1.55150870e-02, mean val. rec. loss:  1.38685043e-02\n",
      "Epoch: 3544 mean train loss:  1.55126103e-02, mean val. rec. loss:  1.38660753e-02\n",
      "Epoch: 3545 mean train loss:  1.55101345e-02, mean val. rec. loss:  1.38636486e-02\n",
      "Epoch: 3546 mean train loss:  1.55076671e-02, mean val. rec. loss:  1.38612219e-02\n",
      "Epoch: 3547 mean train loss:  1.55052006e-02, mean val. rec. loss:  1.38587986e-02\n",
      "Epoch: 3548 mean train loss:  1.55027286e-02, mean val. rec. loss:  1.38563662e-02\n",
      "Epoch: 3549 mean train loss:  1.55002612e-02, mean val. rec. loss:  1.38539463e-02\n",
      "Epoch: 3550 mean train loss:  1.54977994e-02, mean val. rec. loss:  1.38515275e-02\n",
      "Epoch: 3551 mean train loss:  1.54953394e-02, mean val. rec. loss:  1.38491019e-02\n",
      "Epoch: 3552 mean train loss:  1.54928748e-02, mean val. rec. loss:  1.38466888e-02\n",
      "Epoch: 3553 mean train loss:  1.54904111e-02, mean val. rec. loss:  1.38442825e-02\n",
      "Epoch: 3554 mean train loss:  1.54879577e-02, mean val. rec. loss:  1.38418694e-02\n",
      "Epoch: 3555 mean train loss:  1.54855052e-02, mean val. rec. loss:  1.38394677e-02\n",
      "Epoch: 3556 mean train loss:  1.54830452e-02, mean val. rec. loss:  1.38370512e-02\n",
      "Epoch: 3557 mean train loss:  1.54805899e-02, mean val. rec. loss:  1.38346483e-02\n",
      "Epoch: 3558 mean train loss:  1.54781411e-02, mean val. rec. loss:  1.38322488e-02\n",
      "Epoch: 3559 mean train loss:  1.54756942e-02, mean val. rec. loss:  1.38298402e-02\n",
      "Epoch: 3560 mean train loss:  1.54732417e-02, mean val. rec. loss:  1.38274452e-02\n",
      "Epoch: 3561 mean train loss:  1.54707920e-02, mean val. rec. loss:  1.38250526e-02\n",
      "Epoch: 3562 mean train loss:  1.54683507e-02, mean val. rec. loss:  1.38226587e-02\n",
      "Epoch: 3563 mean train loss:  1.54659093e-02, mean val. rec. loss:  1.38202740e-02\n",
      "Epoch: 3564 mean train loss:  1.54634634e-02, mean val. rec. loss:  1.38178734e-02\n",
      "Epoch: 3565 mean train loss:  1.54610192e-02, mean val. rec. loss:  1.38154909e-02\n",
      "Epoch: 3566 mean train loss:  1.54585853e-02, mean val. rec. loss:  1.38131050e-02\n",
      "Epoch: 3567 mean train loss:  1.54561496e-02, mean val. rec. loss:  1.38107202e-02\n",
      "Epoch: 3568 mean train loss:  1.54537139e-02, mean val. rec. loss:  1.38083355e-02\n",
      "Epoch: 3569 mean train loss:  1.54512800e-02, mean val. rec. loss:  1.38059541e-02\n",
      "Epoch: 3570 mean train loss:  1.54488470e-02, mean val. rec. loss:  1.38035784e-02\n",
      "Epoch: 3571 mean train loss:  1.54464178e-02, mean val. rec. loss:  1.38012005e-02\n",
      "Epoch: 3572 mean train loss:  1.54439904e-02, mean val. rec. loss:  1.37988259e-02\n",
      "Epoch: 3573 mean train loss:  1.54415649e-02, mean val. rec. loss:  1.37964537e-02\n",
      "Epoch: 3574 mean train loss:  1.54391366e-02, mean val. rec. loss:  1.37940825e-02\n",
      "Epoch: 3575 mean train loss:  1.54367139e-02, mean val. rec. loss:  1.37917159e-02\n",
      "Epoch: 3576 mean train loss:  1.54342931e-02, mean val. rec. loss:  1.37893482e-02\n",
      "Epoch: 3577 mean train loss:  1.54318722e-02, mean val. rec. loss:  1.37869861e-02\n",
      "Epoch: 3578 mean train loss:  1.54294523e-02, mean val. rec. loss:  1.37846229e-02\n",
      "Epoch: 3579 mean train loss:  1.54270333e-02, mean val. rec. loss:  1.37822631e-02\n",
      "Epoch: 3580 mean train loss:  1.54246190e-02, mean val. rec. loss:  1.37799067e-02\n",
      "Epoch: 3581 mean train loss:  1.54222047e-02, mean val. rec. loss:  1.37775525e-02\n",
      "Epoch: 3582 mean train loss:  1.54197922e-02, mean val. rec. loss:  1.37751984e-02\n",
      "Epoch: 3583 mean train loss:  1.54173816e-02, mean val. rec. loss:  1.37728477e-02\n",
      "Epoch: 3584 mean train loss:  1.54149728e-02, mean val. rec. loss:  1.37704992e-02\n",
      "Epoch: 3585 mean train loss:  1.54125632e-02, mean val. rec. loss:  1.37681530e-02\n",
      "Epoch: 3586 mean train loss:  1.54101581e-02, mean val. rec. loss:  1.37658091e-02\n",
      "Epoch: 3587 mean train loss:  1.54077531e-02, mean val. rec. loss:  1.37634663e-02\n",
      "Epoch: 3588 mean train loss:  1.54053518e-02, mean val. rec. loss:  1.37611246e-02\n",
      "Epoch: 3589 mean train loss:  1.54029487e-02, mean val. rec. loss:  1.37587875e-02\n",
      "Epoch: 3590 mean train loss:  1.54005483e-02, mean val. rec. loss:  1.37564515e-02\n",
      "Epoch: 3591 mean train loss:  1.53981507e-02, mean val. rec. loss:  1.37541178e-02\n",
      "Epoch: 3592 mean train loss:  1.53957550e-02, mean val. rec. loss:  1.37517807e-02\n",
      "Epoch: 3593 mean train loss:  1.53933547e-02, mean val. rec. loss:  1.37494594e-02\n",
      "Epoch: 3594 mean train loss:  1.53909664e-02, mean val. rec. loss:  1.37471268e-02\n",
      "Epoch: 3595 mean train loss:  1.53885791e-02, mean val. rec. loss:  1.37447965e-02\n",
      "Epoch: 3596 mean train loss:  1.53861852e-02, mean val. rec. loss:  1.37424696e-02\n",
      "Epoch: 3597 mean train loss:  1.53837923e-02, mean val. rec. loss:  1.37401427e-02\n",
      "Epoch: 3598 mean train loss:  1.53814106e-02, mean val. rec. loss:  1.37378305e-02\n",
      "Epoch: 3599 mean train loss:  1.53790298e-02, mean val. rec. loss:  1.37355092e-02\n",
      "Epoch: 3600 mean train loss:  1.53766397e-02, mean val. rec. loss:  1.37331902e-02\n",
      "Epoch: 3601 mean train loss:  1.53742570e-02, mean val. rec. loss:  1.37308849e-02\n",
      "Epoch: 3602 mean train loss:  1.53718780e-02, mean val. rec. loss:  1.37285670e-02\n",
      "Epoch: 3603 mean train loss:  1.53695028e-02, mean val. rec. loss:  1.37262560e-02\n",
      "Epoch: 3604 mean train loss:  1.53671220e-02, mean val. rec. loss:  1.37239461e-02\n",
      "Epoch: 3605 mean train loss:  1.53647440e-02, mean val. rec. loss:  1.37216384e-02\n",
      "Epoch: 3606 mean train loss:  1.53623734e-02, mean val. rec. loss:  1.37193410e-02\n",
      "Epoch: 3607 mean train loss:  1.53600028e-02, mean val. rec. loss:  1.37170390e-02\n",
      "Epoch: 3608 mean train loss:  1.53576304e-02, mean val. rec. loss:  1.37147370e-02\n",
      "Epoch: 3609 mean train loss:  1.53552561e-02, mean val. rec. loss:  1.37124498e-02\n",
      "Epoch: 3610 mean train loss:  1.53528911e-02, mean val. rec. loss:  1.37101467e-02\n",
      "Epoch: 3611 mean train loss:  1.53505280e-02, mean val. rec. loss:  1.37078538e-02\n",
      "Epoch: 3612 mean train loss:  1.53481593e-02, mean val. rec. loss:  1.37055586e-02\n",
      "Epoch: 3613 mean train loss:  1.53457934e-02, mean val. rec. loss:  1.37032725e-02\n",
      "Epoch: 3614 mean train loss:  1.53434405e-02, mean val. rec. loss:  1.37009796e-02\n",
      "Epoch: 3615 mean train loss:  1.53410737e-02, mean val. rec. loss:  1.36986992e-02\n",
      "Epoch: 3616 mean train loss:  1.53387217e-02, mean val. rec. loss:  1.36964097e-02\n",
      "Epoch: 3617 mean train loss:  1.53363586e-02, mean val. rec. loss:  1.36941327e-02\n",
      "Epoch: 3618 mean train loss:  1.53340113e-02, mean val. rec. loss:  1.36918455e-02\n",
      "Epoch: 3619 mean train loss:  1.53316510e-02, mean val. rec. loss:  1.36895718e-02\n",
      "Epoch: 3620 mean train loss:  1.53293065e-02, mean val. rec. loss:  1.36872925e-02\n",
      "Epoch: 3621 mean train loss:  1.53269490e-02, mean val. rec. loss:  1.36850223e-02\n",
      "Epoch: 3622 mean train loss:  1.53246073e-02, mean val. rec. loss:  1.36827464e-02\n",
      "Epoch: 3623 mean train loss:  1.53222544e-02, mean val. rec. loss:  1.36804819e-02\n",
      "Epoch: 3624 mean train loss:  1.53199155e-02, mean val. rec. loss:  1.36782083e-02\n",
      "Epoch: 3625 mean train loss:  1.53175635e-02, mean val. rec. loss:  1.36759482e-02\n",
      "Epoch: 3626 mean train loss:  1.53152283e-02, mean val. rec. loss:  1.36736780e-02\n",
      "Epoch: 3627 mean train loss:  1.53128820e-02, mean val. rec. loss:  1.36714214e-02\n",
      "Epoch: 3628 mean train loss:  1.53105486e-02, mean val. rec. loss:  1.36691580e-02\n",
      "Epoch: 3629 mean train loss:  1.53082041e-02, mean val. rec. loss:  1.36669036e-02\n",
      "Epoch: 3630 mean train loss:  1.53058745e-02, mean val. rec. loss:  1.36646436e-02\n",
      "Epoch: 3631 mean train loss:  1.53035338e-02, mean val. rec. loss:  1.36623972e-02\n",
      "Epoch: 3632 mean train loss:  1.53012070e-02, mean val. rec. loss:  1.36601406e-02\n",
      "Epoch: 3633 mean train loss:  1.52988671e-02, mean val. rec. loss:  1.36578953e-02\n",
      "Epoch: 3634 mean train loss:  1.52965422e-02, mean val. rec. loss:  1.36556467e-02\n",
      "Epoch: 3635 mean train loss:  1.52942191e-02, mean val. rec. loss:  1.36533934e-02\n",
      "Epoch: 3636 mean train loss:  1.52918876e-02, mean val. rec. loss:  1.36511527e-02\n",
      "Epoch: 3637 mean train loss:  1.52895599e-02, mean val. rec. loss:  1.36489120e-02\n",
      "Epoch: 3638 mean train loss:  1.52872396e-02, mean val. rec. loss:  1.36466769e-02\n",
      "Epoch: 3639 mean train loss:  1.52849240e-02, mean val. rec. loss:  1.36444418e-02\n",
      "Epoch: 3640 mean train loss:  1.52825990e-02, mean val. rec. loss:  1.36421966e-02\n",
      "Epoch: 3641 mean train loss:  1.52802769e-02, mean val. rec. loss:  1.36399660e-02\n",
      "Epoch: 3642 mean train loss:  1.52779631e-02, mean val. rec. loss:  1.36377310e-02\n",
      "Epoch: 3643 mean train loss:  1.52756503e-02, mean val. rec. loss:  1.36354948e-02\n",
      "Epoch: 3644 mean train loss:  1.52733346e-02, mean val. rec. loss:  1.36332676e-02\n",
      "Epoch: 3645 mean train loss:  1.52710171e-02, mean val. rec. loss:  1.36310484e-02\n",
      "Epoch: 3646 mean train loss:  1.52687099e-02, mean val. rec. loss:  1.36288270e-02\n",
      "Epoch: 3647 mean train loss:  1.52664054e-02, mean val. rec. loss:  1.36266089e-02\n",
      "Epoch: 3648 mean train loss:  1.52640926e-02, mean val. rec. loss:  1.36243795e-02\n",
      "Epoch: 3649 mean train loss:  1.52617825e-02, mean val. rec. loss:  1.36221660e-02\n",
      "Epoch: 3650 mean train loss:  1.52594809e-02, mean val. rec. loss:  1.36199513e-02\n",
      "Epoch: 3651 mean train loss:  1.52571820e-02, mean val. rec. loss:  1.36177344e-02\n",
      "Epoch: 3652 mean train loss:  1.52548813e-02, mean val. rec. loss:  1.36155220e-02\n",
      "Epoch: 3653 mean train loss:  1.52525805e-02, mean val. rec. loss:  1.36133119e-02\n",
      "Epoch: 3654 mean train loss:  1.52502826e-02, mean val. rec. loss:  1.36111018e-02\n",
      "Epoch: 3655 mean train loss:  1.52479865e-02, mean val. rec. loss:  1.36088962e-02\n",
      "Epoch: 3656 mean train loss:  1.52456913e-02, mean val. rec. loss:  1.36066918e-02\n",
      "Epoch: 3657 mean train loss:  1.52433981e-02, mean val. rec. loss:  1.36044873e-02\n",
      "Epoch: 3658 mean train loss:  1.52411066e-02, mean val. rec. loss:  1.36022863e-02\n",
      "Epoch: 3659 mean train loss:  1.52388152e-02, mean val. rec. loss:  1.36000863e-02\n",
      "Epoch: 3660 mean train loss:  1.52365256e-02, mean val. rec. loss:  1.35978921e-02\n",
      "Epoch: 3661 mean train loss:  1.52342407e-02, mean val. rec. loss:  1.35956967e-02\n",
      "Epoch: 3662 mean train loss:  1.52319530e-02, mean val. rec. loss:  1.35935047e-02\n",
      "Epoch: 3663 mean train loss:  1.52296700e-02, mean val. rec. loss:  1.35913139e-02\n",
      "Epoch: 3664 mean train loss:  1.52273869e-02, mean val. rec. loss:  1.35891253e-02\n",
      "Epoch: 3665 mean train loss:  1.52251058e-02, mean val. rec. loss:  1.35869402e-02\n",
      "Epoch: 3666 mean train loss:  1.52228264e-02, mean val. rec. loss:  1.35847538e-02\n",
      "Epoch: 3667 mean train loss:  1.52205471e-02, mean val. rec. loss:  1.35825698e-02\n",
      "Epoch: 3668 mean train loss:  1.52182669e-02, mean val. rec. loss:  1.35803835e-02\n",
      "Epoch: 3669 mean train loss:  1.52159903e-02, mean val. rec. loss:  1.35782153e-02\n",
      "Epoch: 3670 mean train loss:  1.52137203e-02, mean val. rec. loss:  1.35760336e-02\n",
      "Epoch: 3671 mean train loss:  1.52114522e-02, mean val. rec. loss:  1.35738552e-02\n",
      "Epoch: 3672 mean train loss:  1.52091803e-02, mean val. rec. loss:  1.35716802e-02\n",
      "Epoch: 3673 mean train loss:  1.52069084e-02, mean val. rec. loss:  1.35695053e-02\n",
      "Epoch: 3674 mean train loss:  1.52046459e-02, mean val. rec. loss:  1.35673428e-02\n",
      "Epoch: 3675 mean train loss:  1.52023833e-02, mean val. rec. loss:  1.35651723e-02\n",
      "Epoch: 3676 mean train loss:  1.52001152e-02, mean val. rec. loss:  1.35630042e-02\n",
      "Epoch: 3677 mean train loss:  1.51978489e-02, mean val. rec. loss:  1.35608530e-02\n",
      "Epoch: 3678 mean train loss:  1.51955919e-02, mean val. rec. loss:  1.35586883e-02\n",
      "Epoch: 3679 mean train loss:  1.51933368e-02, mean val. rec. loss:  1.35565269e-02\n",
      "Epoch: 3680 mean train loss:  1.51910742e-02, mean val. rec. loss:  1.35543655e-02\n",
      "Epoch: 3681 mean train loss:  1.51888145e-02, mean val. rec. loss:  1.35522087e-02\n",
      "Epoch: 3682 mean train loss:  1.51865631e-02, mean val. rec. loss:  1.35500587e-02\n",
      "Epoch: 3683 mean train loss:  1.51843098e-02, mean val. rec. loss:  1.35479132e-02\n",
      "Epoch: 3684 mean train loss:  1.51820575e-02, mean val. rec. loss:  1.35457564e-02\n",
      "Epoch: 3685 mean train loss:  1.51798098e-02, mean val. rec. loss:  1.35435996e-02\n",
      "Epoch: 3686 mean train loss:  1.51775603e-02, mean val. rec. loss:  1.35414598e-02\n",
      "Epoch: 3687 mean train loss:  1.51753126e-02, mean val. rec. loss:  1.35393222e-02\n",
      "Epoch: 3688 mean train loss:  1.51730668e-02, mean val. rec. loss:  1.35371745e-02\n",
      "Epoch: 3689 mean train loss:  1.51708248e-02, mean val. rec. loss:  1.35350267e-02\n",
      "Epoch: 3690 mean train loss:  1.51685827e-02, mean val. rec. loss:  1.35328926e-02\n",
      "Epoch: 3691 mean train loss:  1.51663406e-02, mean val. rec. loss:  1.35307630e-02\n",
      "Epoch: 3692 mean train loss:  1.51641023e-02, mean val. rec. loss:  1.35286231e-02\n",
      "Epoch: 3693 mean train loss:  1.51618648e-02, mean val. rec. loss:  1.35264822e-02\n",
      "Epoch: 3694 mean train loss:  1.51596283e-02, mean val. rec. loss:  1.35243605e-02\n",
      "Epoch: 3695 mean train loss:  1.51573928e-02, mean val. rec. loss:  1.35222377e-02\n",
      "Epoch: 3696 mean train loss:  1.51551600e-02, mean val. rec. loss:  1.35201058e-02\n",
      "Epoch: 3697 mean train loss:  1.51529273e-02, mean val. rec. loss:  1.35179808e-02\n",
      "Epoch: 3698 mean train loss:  1.51507019e-02, mean val. rec. loss:  1.35158489e-02\n",
      "Epoch: 3699 mean train loss:  1.51484710e-02, mean val. rec. loss:  1.35137284e-02\n",
      "Epoch: 3700 mean train loss:  1.51462401e-02, mean val. rec. loss:  1.35116124e-02\n",
      "Epoch: 3701 mean train loss:  1.51440185e-02, mean val. rec. loss:  1.35094941e-02\n",
      "Epoch: 3702 mean train loss:  1.51417988e-02, mean val. rec. loss:  1.35073826e-02\n",
      "Epoch: 3703 mean train loss:  1.51395716e-02, mean val. rec. loss:  1.35052576e-02\n",
      "Epoch: 3704 mean train loss:  1.51373463e-02, mean val. rec. loss:  1.35031495e-02\n",
      "Epoch: 3705 mean train loss:  1.51351331e-02, mean val. rec. loss:  1.35010403e-02\n",
      "Epoch: 3706 mean train loss:  1.51329180e-02, mean val. rec. loss:  1.34989277e-02\n",
      "Epoch: 3707 mean train loss:  1.51306983e-02, mean val. rec. loss:  1.34968230e-02\n",
      "Epoch: 3708 mean train loss:  1.51284804e-02, mean val. rec. loss:  1.34947218e-02\n",
      "Epoch: 3709 mean train loss:  1.51262700e-02, mean val. rec. loss:  1.34926205e-02\n",
      "Epoch: 3710 mean train loss:  1.51240614e-02, mean val. rec. loss:  1.34905193e-02\n",
      "Epoch: 3711 mean train loss:  1.51218529e-02, mean val. rec. loss:  1.34884214e-02\n",
      "Epoch: 3712 mean train loss:  1.51196453e-02, mean val. rec. loss:  1.34863224e-02\n",
      "Epoch: 3713 mean train loss:  1.51174395e-02, mean val. rec. loss:  1.34842291e-02\n",
      "Epoch: 3714 mean train loss:  1.51152347e-02, mean val. rec. loss:  1.34821358e-02\n",
      "Epoch: 3715 mean train loss:  1.51130289e-02, mean val. rec. loss:  1.34800447e-02\n",
      "Epoch: 3716 mean train loss:  1.51108287e-02, mean val. rec. loss:  1.34779548e-02\n",
      "Epoch: 3717 mean train loss:  1.51086267e-02, mean val. rec. loss:  1.34758683e-02\n",
      "Epoch: 3718 mean train loss:  1.51064274e-02, mean val. rec. loss:  1.34737818e-02\n",
      "Epoch: 3719 mean train loss:  1.51042300e-02, mean val. rec. loss:  1.34717009e-02\n",
      "Epoch: 3720 mean train loss:  1.51020336e-02, mean val. rec. loss:  1.34696189e-02\n",
      "Epoch: 3721 mean train loss:  1.50998399e-02, mean val. rec. loss:  1.34675415e-02\n",
      "Epoch: 3722 mean train loss:  1.50976463e-02, mean val. rec. loss:  1.34654629e-02\n",
      "Epoch: 3723 mean train loss:  1.50954545e-02, mean val. rec. loss:  1.34633843e-02\n",
      "Epoch: 3724 mean train loss:  1.50932589e-02, mean val. rec. loss:  1.34613069e-02\n",
      "Epoch: 3725 mean train loss:  1.50910690e-02, mean val. rec. loss:  1.34592283e-02\n",
      "Epoch: 3726 mean train loss:  1.50888847e-02, mean val. rec. loss:  1.34571656e-02\n",
      "Epoch: 3727 mean train loss:  1.50867040e-02, mean val. rec. loss:  1.34550950e-02\n",
      "Epoch: 3728 mean train loss:  1.50845160e-02, mean val. rec. loss:  1.34530243e-02\n",
      "Epoch: 3729 mean train loss:  1.50823298e-02, mean val. rec. loss:  1.34509696e-02\n",
      "Epoch: 3730 mean train loss:  1.50801529e-02, mean val. rec. loss:  1.34489012e-02\n",
      "Epoch: 3731 mean train loss:  1.50779769e-02, mean val. rec. loss:  1.34468385e-02\n",
      "Epoch: 3732 mean train loss:  1.50757953e-02, mean val. rec. loss:  1.34447769e-02\n",
      "Epoch: 3733 mean train loss:  1.50736147e-02, mean val. rec. loss:  1.34427165e-02\n",
      "Epoch: 3734 mean train loss:  1.50714434e-02, mean val. rec. loss:  1.34406685e-02\n",
      "Epoch: 3735 mean train loss:  1.50692749e-02, mean val. rec. loss:  1.34386126e-02\n",
      "Epoch: 3736 mean train loss:  1.50670970e-02, mean val. rec. loss:  1.34365658e-02\n",
      "Epoch: 3737 mean train loss:  1.50649276e-02, mean val. rec. loss:  1.34345099e-02\n",
      "Epoch: 3738 mean train loss:  1.50627600e-02, mean val. rec. loss:  1.34324551e-02\n",
      "Epoch: 3739 mean train loss:  1.50605924e-02, mean val. rec. loss:  1.34304151e-02\n",
      "Epoch: 3740 mean train loss:  1.50584285e-02, mean val. rec. loss:  1.34283739e-02\n",
      "Epoch: 3741 mean train loss:  1.50562628e-02, mean val. rec. loss:  1.34263271e-02\n",
      "Epoch: 3742 mean train loss:  1.50541008e-02, mean val. rec. loss:  1.34242791e-02\n",
      "Epoch: 3743 mean train loss:  1.50519407e-02, mean val. rec. loss:  1.34222459e-02\n",
      "Epoch: 3744 mean train loss:  1.50497805e-02, mean val. rec. loss:  1.34202138e-02\n",
      "Epoch: 3745 mean train loss:  1.50476204e-02, mean val. rec. loss:  1.34181727e-02\n",
      "Epoch: 3746 mean train loss:  1.50454649e-02, mean val. rec. loss:  1.34161338e-02\n",
      "Epoch: 3747 mean train loss:  1.50433113e-02, mean val. rec. loss:  1.34141108e-02\n",
      "Epoch: 3748 mean train loss:  1.50411567e-02, mean val. rec. loss:  1.34120832e-02\n",
      "Epoch: 3749 mean train loss:  1.50390096e-02, mean val. rec. loss:  1.34100466e-02\n",
      "Epoch: 3750 mean train loss:  1.50368551e-02, mean val. rec. loss:  1.34080213e-02\n",
      "Epoch: 3751 mean train loss:  1.50347024e-02, mean val. rec. loss:  1.34060017e-02\n",
      "Epoch: 3752 mean train loss:  1.50325590e-02, mean val. rec. loss:  1.34039798e-02\n",
      "Epoch: 3753 mean train loss:  1.50304175e-02, mean val. rec. loss:  1.34019636e-02\n",
      "Epoch: 3754 mean train loss:  1.50282685e-02, mean val. rec. loss:  1.33999383e-02\n",
      "Epoch: 3755 mean train loss:  1.50261233e-02, mean val. rec. loss:  1.33979244e-02\n",
      "Epoch: 3756 mean train loss:  1.50239855e-02, mean val. rec. loss:  1.33959105e-02\n",
      "Epoch: 3757 mean train loss:  1.50218486e-02, mean val. rec. loss:  1.33938920e-02\n",
      "Epoch: 3758 mean train loss:  1.50197071e-02, mean val. rec. loss:  1.33918826e-02\n",
      "Epoch: 3759 mean train loss:  1.50175646e-02, mean val. rec. loss:  1.33898754e-02\n",
      "Epoch: 3760 mean train loss:  1.50154315e-02, mean val. rec. loss:  1.33878660e-02\n",
      "Epoch: 3761 mean train loss:  1.50132965e-02, mean val. rec. loss:  1.33858600e-02\n",
      "Epoch: 3762 mean train loss:  1.50111661e-02, mean val. rec. loss:  1.33838552e-02\n",
      "Epoch: 3763 mean train loss:  1.50090358e-02, mean val. rec. loss:  1.33818514e-02\n",
      "Epoch: 3764 mean train loss:  1.50069054e-02, mean val. rec. loss:  1.33798533e-02\n",
      "Epoch: 3765 mean train loss:  1.50047770e-02, mean val. rec. loss:  1.33778530e-02\n",
      "Epoch: 3766 mean train loss:  1.50026522e-02, mean val. rec. loss:  1.33758572e-02\n",
      "Epoch: 3767 mean train loss:  1.50005265e-02, mean val. rec. loss:  1.33738614e-02\n",
      "Epoch: 3768 mean train loss:  1.49984036e-02, mean val. rec. loss:  1.33718702e-02\n",
      "Epoch: 3769 mean train loss:  1.49962817e-02, mean val. rec. loss:  1.33698800e-02\n",
      "Epoch: 3770 mean train loss:  1.49941606e-02, mean val. rec. loss:  1.33679001e-02\n",
      "Epoch: 3771 mean train loss:  1.49920470e-02, mean val. rec. loss:  1.33659066e-02\n",
      "Epoch: 3772 mean train loss:  1.49899325e-02, mean val. rec. loss:  1.33639187e-02\n",
      "Epoch: 3773 mean train loss:  1.49878152e-02, mean val. rec. loss:  1.33619320e-02\n",
      "Epoch: 3774 mean train loss:  1.49856951e-02, mean val. rec. loss:  1.33599453e-02\n",
      "Epoch: 3775 mean train loss:  1.49835871e-02, mean val. rec. loss:  1.33579733e-02\n",
      "Epoch: 3776 mean train loss:  1.49814810e-02, mean val. rec. loss:  1.33559956e-02\n",
      "Epoch: 3777 mean train loss:  1.49793674e-02, mean val. rec. loss:  1.33540168e-02\n",
      "Epoch: 3778 mean train loss:  1.49772557e-02, mean val. rec. loss:  1.33520505e-02\n",
      "Epoch: 3779 mean train loss:  1.49751532e-02, mean val. rec. loss:  1.33500740e-02\n",
      "Epoch: 3780 mean train loss:  1.49730527e-02, mean val. rec. loss:  1.33481020e-02\n",
      "Epoch: 3781 mean train loss:  1.49709438e-02, mean val. rec. loss:  1.33461357e-02\n",
      "Epoch: 3782 mean train loss:  1.49688404e-02, mean val. rec. loss:  1.33441626e-02\n",
      "Epoch: 3783 mean train loss:  1.49667417e-02, mean val. rec. loss:  1.33421894e-02\n",
      "Epoch: 3784 mean train loss:  1.49646430e-02, mean val. rec. loss:  1.33402322e-02\n",
      "Epoch: 3785 mean train loss:  1.49625462e-02, mean val. rec. loss:  1.33382772e-02\n",
      "Epoch: 3786 mean train loss:  1.49604494e-02, mean val. rec. loss:  1.33363109e-02\n",
      "Epoch: 3787 mean train loss:  1.49583553e-02, mean val. rec. loss:  1.33343435e-02\n",
      "Epoch: 3788 mean train loss:  1.49562613e-02, mean val. rec. loss:  1.33323942e-02\n",
      "Epoch: 3789 mean train loss:  1.49541719e-02, mean val. rec. loss:  1.33304460e-02\n",
      "Epoch: 3790 mean train loss:  1.49520807e-02, mean val. rec. loss:  1.33284887e-02\n",
      "Epoch: 3791 mean train loss:  1.49499932e-02, mean val. rec. loss:  1.33265315e-02\n",
      "Epoch: 3792 mean train loss:  1.49479057e-02, mean val. rec. loss:  1.33245833e-02\n",
      "Epoch: 3793 mean train loss:  1.49458163e-02, mean val. rec. loss:  1.33226397e-02\n",
      "Epoch: 3794 mean train loss:  1.49437372e-02, mean val. rec. loss:  1.33206938e-02\n",
      "Epoch: 3795 mean train loss:  1.49416580e-02, mean val. rec. loss:  1.33187547e-02\n",
      "Epoch: 3796 mean train loss:  1.49395724e-02, mean val. rec. loss:  1.33168042e-02\n",
      "Epoch: 3797 mean train loss:  1.49374895e-02, mean val. rec. loss:  1.33148674e-02\n",
      "Epoch: 3798 mean train loss:  1.49354160e-02, mean val. rec. loss:  1.33129295e-02\n",
      "Epoch: 3799 mean train loss:  1.49333433e-02, mean val. rec. loss:  1.33109847e-02\n",
      "Epoch: 3800 mean train loss:  1.49312642e-02, mean val. rec. loss:  1.33090547e-02\n",
      "Epoch: 3801 mean train loss:  1.49291869e-02, mean val. rec. loss:  1.33071235e-02\n",
      "Epoch: 3802 mean train loss:  1.49271171e-02, mean val. rec. loss:  1.33051935e-02\n",
      "Epoch: 3803 mean train loss:  1.49250473e-02, mean val. rec. loss:  1.33032623e-02\n",
      "Epoch: 3804 mean train loss:  1.49229793e-02, mean val. rec. loss:  1.33013323e-02\n",
      "Epoch: 3805 mean train loss:  1.49209132e-02, mean val. rec. loss:  1.32994056e-02\n",
      "Epoch: 3806 mean train loss:  1.49188453e-02, mean val. rec. loss:  1.32974836e-02\n",
      "Epoch: 3807 mean train loss:  1.49167829e-02, mean val. rec. loss:  1.32955603e-02\n",
      "Epoch: 3808 mean train loss:  1.49147186e-02, mean val. rec. loss:  1.32936394e-02\n",
      "Epoch: 3809 mean train loss:  1.49126572e-02, mean val. rec. loss:  1.32917207e-02\n",
      "Epoch: 3810 mean train loss:  1.49105967e-02, mean val. rec. loss:  1.32898043e-02\n",
      "Epoch: 3811 mean train loss:  1.49085390e-02, mean val. rec. loss:  1.32878901e-02\n",
      "Epoch: 3812 mean train loss:  1.49064812e-02, mean val. rec. loss:  1.32859692e-02\n",
      "Epoch: 3813 mean train loss:  1.49044207e-02, mean val. rec. loss:  1.32840550e-02\n",
      "Epoch: 3814 mean train loss:  1.49023732e-02, mean val. rec. loss:  1.32821499e-02\n",
      "Epoch: 3815 mean train loss:  1.49003220e-02, mean val. rec. loss:  1.32802403e-02\n",
      "Epoch: 3816 mean train loss:  1.48982699e-02, mean val. rec. loss:  1.32783296e-02\n",
      "Epoch: 3817 mean train loss:  1.48962150e-02, mean val. rec. loss:  1.32764347e-02\n",
      "Epoch: 3818 mean train loss:  1.48941703e-02, mean val. rec. loss:  1.32745251e-02\n",
      "Epoch: 3819 mean train loss:  1.48921284e-02, mean val. rec. loss:  1.32726245e-02\n",
      "Epoch: 3820 mean train loss:  1.48900809e-02, mean val. rec. loss:  1.32707206e-02\n",
      "Epoch: 3821 mean train loss:  1.48880316e-02, mean val. rec. loss:  1.32688280e-02\n",
      "Epoch: 3822 mean train loss:  1.48859971e-02, mean val. rec. loss:  1.32669240e-02\n",
      "Epoch: 3823 mean train loss:  1.48839497e-02, mean val. rec. loss:  1.32650337e-02\n",
      "Epoch: 3824 mean train loss:  1.48819189e-02, mean val. rec. loss:  1.32631365e-02\n",
      "Epoch: 3825 mean train loss:  1.48798724e-02, mean val. rec. loss:  1.32612507e-02\n",
      "Epoch: 3826 mean train loss:  1.48778445e-02, mean val. rec. loss:  1.32593559e-02\n",
      "Epoch: 3827 mean train loss:  1.48758026e-02, mean val. rec. loss:  1.32574757e-02\n",
      "Epoch: 3828 mean train loss:  1.48737774e-02, mean val. rec. loss:  1.32555831e-02\n",
      "Epoch: 3829 mean train loss:  1.48717374e-02, mean val. rec. loss:  1.32537075e-02\n",
      "Epoch: 3830 mean train loss:  1.48697160e-02, mean val. rec. loss:  1.32518206e-02\n",
      "Epoch: 3831 mean train loss:  1.48676778e-02, mean val. rec. loss:  1.32499427e-02\n",
      "Epoch: 3832 mean train loss:  1.48656564e-02, mean val. rec. loss:  1.32480626e-02\n",
      "Epoch: 3833 mean train loss:  1.48636341e-02, mean val. rec. loss:  1.32461915e-02\n",
      "Epoch: 3834 mean train loss:  1.48616062e-02, mean val. rec. loss:  1.32443080e-02\n",
      "Epoch: 3835 mean train loss:  1.48595801e-02, mean val. rec. loss:  1.32424381e-02\n",
      "Epoch: 3836 mean train loss:  1.48575624e-02, mean val. rec. loss:  1.32405670e-02\n",
      "Epoch: 3837 mean train loss:  1.48555466e-02, mean val. rec. loss:  1.32386914e-02\n",
      "Epoch: 3838 mean train loss:  1.48535243e-02, mean val. rec. loss:  1.32368249e-02\n",
      "Epoch: 3839 mean train loss:  1.48515038e-02, mean val. rec. loss:  1.32349629e-02\n",
      "Epoch: 3840 mean train loss:  1.48494926e-02, mean val. rec. loss:  1.32330975e-02\n",
      "Epoch: 3841 mean train loss:  1.48474787e-02, mean val. rec. loss:  1.32312332e-02\n",
      "Epoch: 3842 mean train loss:  1.48454666e-02, mean val. rec. loss:  1.32293724e-02\n",
      "Epoch: 3843 mean train loss:  1.48434554e-02, mean val. rec. loss:  1.32275127e-02\n",
      "Epoch: 3844 mean train loss:  1.48414480e-02, mean val. rec. loss:  1.32256529e-02\n",
      "Epoch: 3845 mean train loss:  1.48394396e-02, mean val. rec. loss:  1.32237978e-02\n",
      "Epoch: 3846 mean train loss:  1.48374312e-02, mean val. rec. loss:  1.32219448e-02\n",
      "Epoch: 3847 mean train loss:  1.48354275e-02, mean val. rec. loss:  1.32200931e-02\n",
      "Epoch: 3848 mean train loss:  1.48334238e-02, mean val. rec. loss:  1.32182435e-02\n",
      "Epoch: 3849 mean train loss:  1.48314229e-02, mean val. rec. loss:  1.32163895e-02\n",
      "Epoch: 3850 mean train loss:  1.48294163e-02, mean val. rec. loss:  1.32145513e-02\n",
      "Epoch: 3851 mean train loss:  1.48274229e-02, mean val. rec. loss:  1.32126984e-02\n",
      "Epoch: 3852 mean train loss:  1.48254294e-02, mean val. rec. loss:  1.32108545e-02\n",
      "Epoch: 3853 mean train loss:  1.48234285e-02, mean val. rec. loss:  1.32090073e-02\n",
      "Epoch: 3854 mean train loss:  1.48214313e-02, mean val. rec. loss:  1.32071680e-02\n",
      "Epoch: 3855 mean train loss:  1.48194425e-02, mean val. rec. loss:  1.32053355e-02\n",
      "Epoch: 3856 mean train loss:  1.48174536e-02, mean val. rec. loss:  1.32034973e-02\n",
      "Epoch: 3857 mean train loss:  1.48154602e-02, mean val. rec. loss:  1.32016603e-02\n",
      "Epoch: 3858 mean train loss:  1.48134667e-02, mean val. rec. loss:  1.31998300e-02\n",
      "Epoch: 3859 mean train loss:  1.48114872e-02, mean val. rec. loss:  1.31979930e-02\n",
      "Epoch: 3860 mean train loss:  1.48094946e-02, mean val. rec. loss:  1.31961661e-02\n",
      "Epoch: 3861 mean train loss:  1.48075188e-02, mean val. rec. loss:  1.31943348e-02\n",
      "Epoch: 3862 mean train loss:  1.48055291e-02, mean val. rec. loss:  1.31925113e-02\n",
      "Epoch: 3863 mean train loss:  1.48035542e-02, mean val. rec. loss:  1.31906811e-02\n",
      "Epoch: 3864 mean train loss:  1.48015664e-02, mean val. rec. loss:  1.31888645e-02\n",
      "Epoch: 3865 mean train loss:  1.47995971e-02, mean val. rec. loss:  1.31870410e-02\n",
      "Epoch: 3866 mean train loss:  1.47976120e-02, mean val. rec. loss:  1.31852244e-02\n",
      "Epoch: 3867 mean train loss:  1.47956437e-02, mean val. rec. loss:  1.31834112e-02\n",
      "Epoch: 3868 mean train loss:  1.47936679e-02, mean val. rec. loss:  1.31815832e-02\n",
      "Epoch: 3869 mean train loss:  1.47916902e-02, mean val. rec. loss:  1.31797734e-02\n",
      "Epoch: 3870 mean train loss:  1.47897247e-02, mean val. rec. loss:  1.31779602e-02\n",
      "Epoch: 3871 mean train loss:  1.47877582e-02, mean val. rec. loss:  1.31761435e-02\n",
      "Epoch: 3872 mean train loss:  1.47857889e-02, mean val. rec. loss:  1.31743382e-02\n",
      "Epoch: 3873 mean train loss:  1.47838169e-02, mean val. rec. loss:  1.31725341e-02\n",
      "Epoch: 3874 mean train loss:  1.47818560e-02, mean val. rec. loss:  1.31707299e-02\n",
      "Epoch: 3875 mean train loss:  1.47798970e-02, mean val. rec. loss:  1.31689246e-02\n",
      "Epoch: 3876 mean train loss:  1.47779352e-02, mean val. rec. loss:  1.31671216e-02\n",
      "Epoch: 3877 mean train loss:  1.47759743e-02, mean val. rec. loss:  1.31653209e-02\n",
      "Epoch: 3878 mean train loss:  1.47740162e-02, mean val. rec. loss:  1.31635212e-02\n",
      "Epoch: 3879 mean train loss:  1.47720590e-02, mean val. rec. loss:  1.31617239e-02\n",
      "Epoch: 3880 mean train loss:  1.47701019e-02, mean val. rec. loss:  1.31599288e-02\n",
      "Epoch: 3881 mean train loss:  1.47681466e-02, mean val. rec. loss:  1.31581360e-02\n",
      "Epoch: 3882 mean train loss:  1.47661940e-02, mean val. rec. loss:  1.31563443e-02\n",
      "Epoch: 3883 mean train loss:  1.47642434e-02, mean val. rec. loss:  1.31545571e-02\n",
      "Epoch: 3884 mean train loss:  1.47622928e-02, mean val. rec. loss:  1.31527655e-02\n",
      "Epoch: 3885 mean train loss:  1.47603403e-02, mean val. rec. loss:  1.31509738e-02\n",
      "Epoch: 3886 mean train loss:  1.47583887e-02, mean val. rec. loss:  1.31491866e-02\n",
      "Epoch: 3887 mean train loss:  1.47564473e-02, mean val. rec. loss:  1.31474097e-02\n",
      "Epoch: 3888 mean train loss:  1.47545079e-02, mean val. rec. loss:  1.31456271e-02\n",
      "Epoch: 3889 mean train loss:  1.47525600e-02, mean val. rec. loss:  1.31438456e-02\n",
      "Epoch: 3890 mean train loss:  1.47506150e-02, mean val. rec. loss:  1.31420766e-02\n",
      "Epoch: 3891 mean train loss:  1.47486792e-02, mean val. rec. loss:  1.31402974e-02\n",
      "Epoch: 3892 mean train loss:  1.47467444e-02, mean val. rec. loss:  1.31385159e-02\n",
      "Epoch: 3893 mean train loss:  1.47448003e-02, mean val. rec. loss:  1.31367469e-02\n",
      "Epoch: 3894 mean train loss:  1.47428729e-02, mean val. rec. loss:  1.31349700e-02\n",
      "Epoch: 3895 mean train loss:  1.47409306e-02, mean val. rec. loss:  1.31332044e-02\n",
      "Epoch: 3896 mean train loss:  1.47390070e-02, mean val. rec. loss:  1.31314308e-02\n",
      "Epoch: 3897 mean train loss:  1.47370666e-02, mean val. rec. loss:  1.31296698e-02\n",
      "Epoch: 3898 mean train loss:  1.47351467e-02, mean val. rec. loss:  1.31279019e-02\n",
      "Epoch: 3899 mean train loss:  1.47332100e-02, mean val. rec. loss:  1.31261442e-02\n",
      "Epoch: 3900 mean train loss:  1.47312919e-02, mean val. rec. loss:  1.31243752e-02\n",
      "Epoch: 3901 mean train loss:  1.47293627e-02, mean val. rec. loss:  1.31226176e-02\n",
      "Epoch: 3902 mean train loss:  1.47274363e-02, mean val. rec. loss:  1.31208622e-02\n",
      "Epoch: 3903 mean train loss:  1.47255192e-02, mean val. rec. loss:  1.31191068e-02\n",
      "Epoch: 3904 mean train loss:  1.47236011e-02, mean val. rec. loss:  1.31173570e-02\n",
      "Epoch: 3905 mean train loss:  1.47216775e-02, mean val. rec. loss:  1.31155960e-02\n",
      "Epoch: 3906 mean train loss:  1.47197575e-02, mean val. rec. loss:  1.31138485e-02\n",
      "Epoch: 3907 mean train loss:  1.47178451e-02, mean val. rec. loss:  1.31121022e-02\n",
      "Epoch: 3908 mean train loss:  1.47159326e-02, mean val. rec. loss:  1.31103525e-02\n",
      "Epoch: 3909 mean train loss:  1.47140201e-02, mean val. rec. loss:  1.31086061e-02\n",
      "Epoch: 3910 mean train loss:  1.47121086e-02, mean val. rec. loss:  1.31068598e-02\n",
      "Epoch: 3911 mean train loss:  1.47101970e-02, mean val. rec. loss:  1.31051158e-02\n",
      "Epoch: 3912 mean train loss:  1.47082883e-02, mean val. rec. loss:  1.31033762e-02\n",
      "Epoch: 3913 mean train loss:  1.47063814e-02, mean val. rec. loss:  1.31016379e-02\n",
      "Epoch: 3914 mean train loss:  1.47044755e-02, mean val. rec. loss:  1.30998995e-02\n",
      "Epoch: 3915 mean train loss:  1.47025704e-02, mean val. rec. loss:  1.30981645e-02\n",
      "Epoch: 3916 mean train loss:  1.47006673e-02, mean val. rec. loss:  1.30964295e-02\n",
      "Epoch: 3917 mean train loss:  1.46987623e-02, mean val. rec. loss:  1.30946911e-02\n",
      "Epoch: 3918 mean train loss:  1.46968582e-02, mean val. rec. loss:  1.30929697e-02\n",
      "Epoch: 3919 mean train loss:  1.46949634e-02, mean val. rec. loss:  1.30912348e-02\n",
      "Epoch: 3920 mean train loss:  1.46930705e-02, mean val. rec. loss:  1.30895054e-02\n",
      "Epoch: 3921 mean train loss:  1.46911701e-02, mean val. rec. loss:  1.30877750e-02\n",
      "Epoch: 3922 mean train loss:  1.46892725e-02, mean val. rec. loss:  1.30860491e-02\n",
      "Epoch: 3923 mean train loss:  1.46873834e-02, mean val. rec. loss:  1.30843368e-02\n",
      "Epoch: 3924 mean train loss:  1.46854951e-02, mean val. rec. loss:  1.30826086e-02\n",
      "Epoch: 3925 mean train loss:  1.46835975e-02, mean val. rec. loss:  1.30808918e-02\n",
      "Epoch: 3926 mean train loss:  1.46817176e-02, mean val. rec. loss:  1.30791715e-02\n",
      "Epoch: 3927 mean train loss:  1.46798238e-02, mean val. rec. loss:  1.30774581e-02\n",
      "Epoch: 3928 mean train loss:  1.46779448e-02, mean val. rec. loss:  1.30757367e-02\n",
      "Epoch: 3929 mean train loss:  1.46760519e-02, mean val. rec. loss:  1.30740312e-02\n",
      "Epoch: 3930 mean train loss:  1.46741776e-02, mean val. rec. loss:  1.30723166e-02\n",
      "Epoch: 3931 mean train loss:  1.46722884e-02, mean val. rec. loss:  1.30706089e-02\n",
      "Epoch: 3932 mean train loss:  1.46704132e-02, mean val. rec. loss:  1.30689022e-02\n",
      "Epoch: 3933 mean train loss:  1.46685371e-02, mean val. rec. loss:  1.30671967e-02\n",
      "Epoch: 3934 mean train loss:  1.46666572e-02, mean val. rec. loss:  1.30654844e-02\n",
      "Epoch: 3935 mean train loss:  1.46647782e-02, mean val. rec. loss:  1.30637835e-02\n",
      "Epoch: 3936 mean train loss:  1.46629086e-02, mean val. rec. loss:  1.30620836e-02\n",
      "Epoch: 3937 mean train loss:  1.46610408e-02, mean val. rec. loss:  1.30603781e-02\n",
      "Epoch: 3938 mean train loss:  1.46591656e-02, mean val. rec. loss:  1.30586806e-02\n",
      "Epoch: 3939 mean train loss:  1.46572913e-02, mean val. rec. loss:  1.30569830e-02\n",
      "Epoch: 3940 mean train loss:  1.46554226e-02, mean val. rec. loss:  1.30552888e-02\n",
      "Epoch: 3941 mean train loss:  1.46535567e-02, mean val. rec. loss:  1.30535947e-02\n",
      "Epoch: 3942 mean train loss:  1.46516917e-02, mean val. rec. loss:  1.30519005e-02\n",
      "Epoch: 3943 mean train loss:  1.46498286e-02, mean val. rec. loss:  1.30502132e-02\n",
      "Epoch: 3944 mean train loss:  1.46479664e-02, mean val. rec. loss:  1.30485258e-02\n",
      "Epoch: 3945 mean train loss:  1.46461042e-02, mean val. rec. loss:  1.30468384e-02\n",
      "Epoch: 3946 mean train loss:  1.46442448e-02, mean val. rec. loss:  1.30451488e-02\n",
      "Epoch: 3947 mean train loss:  1.46423900e-02, mean val. rec. loss:  1.30434728e-02\n",
      "Epoch: 3948 mean train loss:  1.46405381e-02, mean val. rec. loss:  1.30417900e-02\n",
      "Epoch: 3949 mean train loss:  1.46386796e-02, mean val. rec. loss:  1.30401072e-02\n",
      "Epoch: 3950 mean train loss:  1.46368221e-02, mean val. rec. loss:  1.30384379e-02\n",
      "Epoch: 3951 mean train loss:  1.46349729e-02, mean val. rec. loss:  1.30367574e-02\n",
      "Epoch: 3952 mean train loss:  1.46331266e-02, mean val. rec. loss:  1.30350814e-02\n",
      "Epoch: 3953 mean train loss:  1.46312737e-02, mean val. rec. loss:  1.30334054e-02\n",
      "Epoch: 3954 mean train loss:  1.46294227e-02, mean val. rec. loss:  1.30317384e-02\n",
      "Epoch: 3955 mean train loss:  1.46275828e-02, mean val. rec. loss:  1.30300635e-02\n",
      "Epoch: 3956 mean train loss:  1.46257309e-02, mean val. rec. loss:  1.30283988e-02\n",
      "Epoch: 3957 mean train loss:  1.46238948e-02, mean val. rec. loss:  1.30267251e-02\n",
      "Epoch: 3958 mean train loss:  1.46220447e-02, mean val. rec. loss:  1.30250672e-02\n",
      "Epoch: 3959 mean train loss:  1.46202123e-02, mean val. rec. loss:  1.30233991e-02\n",
      "Epoch: 3960 mean train loss:  1.46183650e-02, mean val. rec. loss:  1.30217435e-02\n",
      "Epoch: 3961 mean train loss:  1.46165335e-02, mean val. rec. loss:  1.30200732e-02\n",
      "Epoch: 3962 mean train loss:  1.46146937e-02, mean val. rec. loss:  1.30184153e-02\n",
      "Epoch: 3963 mean train loss:  1.46128548e-02, mean val. rec. loss:  1.30167608e-02\n",
      "Epoch: 3964 mean train loss:  1.46110261e-02, mean val. rec. loss:  1.30151075e-02\n",
      "Epoch: 3965 mean train loss:  1.46091974e-02, mean val. rec. loss:  1.30134598e-02\n",
      "Epoch: 3966 mean train loss:  1.46073641e-02, mean val. rec. loss:  1.30117963e-02\n",
      "Epoch: 3967 mean train loss:  1.46055298e-02, mean val. rec. loss:  1.30101520e-02\n",
      "Epoch: 3968 mean train loss:  1.46037068e-02, mean val. rec. loss:  1.30085032e-02\n",
      "Epoch: 3969 mean train loss:  1.46018809e-02, mean val. rec. loss:  1.30068522e-02\n",
      "Epoch: 3970 mean train loss:  1.46000569e-02, mean val. rec. loss:  1.30052056e-02\n",
      "Epoch: 3971 mean train loss:  1.45982328e-02, mean val. rec. loss:  1.30035602e-02\n",
      "Epoch: 3972 mean train loss:  1.45964116e-02, mean val. rec. loss:  1.30019171e-02\n",
      "Epoch: 3973 mean train loss:  1.45945913e-02, mean val. rec. loss:  1.30002774e-02\n",
      "Epoch: 3974 mean train loss:  1.45927720e-02, mean val. rec. loss:  1.29986388e-02\n",
      "Epoch: 3975 mean train loss:  1.45909535e-02, mean val. rec. loss:  1.29969956e-02\n",
      "Epoch: 3976 mean train loss:  1.45891323e-02, mean val. rec. loss:  1.29953582e-02\n",
      "Epoch: 3977 mean train loss:  1.45873241e-02, mean val. rec. loss:  1.29937275e-02\n",
      "Epoch: 3978 mean train loss:  1.45855150e-02, mean val. rec. loss:  1.29920946e-02\n",
      "Epoch: 3979 mean train loss:  1.45836984e-02, mean val. rec. loss:  1.29904560e-02\n",
      "Epoch: 3980 mean train loss:  1.45818847e-02, mean val. rec. loss:  1.29888389e-02\n",
      "Epoch: 3981 mean train loss:  1.45800783e-02, mean val. rec. loss:  1.29872083e-02\n",
      "Epoch: 3982 mean train loss:  1.45782767e-02, mean val. rec. loss:  1.29855754e-02\n",
      "Epoch: 3983 mean train loss:  1.45764629e-02, mean val. rec. loss:  1.29839526e-02\n",
      "Epoch: 3984 mean train loss:  1.45746650e-02, mean val. rec. loss:  1.29823265e-02\n",
      "Epoch: 3985 mean train loss:  1.45728549e-02, mean val. rec. loss:  1.29807083e-02\n",
      "Epoch: 3986 mean train loss:  1.45710616e-02, mean val. rec. loss:  1.29790822e-02\n",
      "Epoch: 3987 mean train loss:  1.45692534e-02, mean val. rec. loss:  1.29774686e-02\n",
      "Epoch: 3988 mean train loss:  1.45674620e-02, mean val. rec. loss:  1.29758481e-02\n",
      "Epoch: 3989 mean train loss:  1.45656557e-02, mean val. rec. loss:  1.29742345e-02\n",
      "Epoch: 3990 mean train loss:  1.45638643e-02, mean val. rec. loss:  1.29726208e-02\n",
      "Epoch: 3991 mean train loss:  1.45620747e-02, mean val. rec. loss:  1.29710015e-02\n",
      "Epoch: 3992 mean train loss:  1.45602777e-02, mean val. rec. loss:  1.29693912e-02\n",
      "Epoch: 3993 mean train loss:  1.45584807e-02, mean val. rec. loss:  1.29677867e-02\n",
      "Epoch: 3994 mean train loss:  1.45566958e-02, mean val. rec. loss:  1.29661810e-02\n",
      "Epoch: 3995 mean train loss:  1.45549108e-02, mean val. rec. loss:  1.29645809e-02\n",
      "Epoch: 3996 mean train loss:  1.45531185e-02, mean val. rec. loss:  1.29629763e-02\n",
      "Epoch: 3997 mean train loss:  1.45513336e-02, mean val. rec. loss:  1.29613718e-02\n",
      "Epoch: 3998 mean train loss:  1.45495477e-02, mean val. rec. loss:  1.29597717e-02\n",
      "Epoch: 3999 mean train loss:  1.45477656e-02, mean val. rec. loss:  1.29581694e-02\n",
      "Epoch: 4000 mean train loss:  1.45459835e-02, mean val. rec. loss:  1.29565739e-02\n",
      "Epoch: 4001 mean train loss:  1.45442032e-02, mean val. rec. loss:  1.29549750e-02\n",
      "Epoch: 4002 mean train loss:  1.45424249e-02, mean val. rec. loss:  1.29533818e-02\n",
      "Epoch: 4003 mean train loss:  1.45406474e-02, mean val. rec. loss:  1.29517851e-02\n",
      "Epoch: 4004 mean train loss:  1.45388755e-02, mean val. rec. loss:  1.29501919e-02\n",
      "Epoch: 4005 mean train loss:  1.45370971e-02, mean val. rec. loss:  1.29485964e-02\n",
      "Epoch: 4006 mean train loss:  1.45353206e-02, mean val. rec. loss:  1.29470054e-02\n",
      "Epoch: 4007 mean train loss:  1.45335534e-02, mean val. rec. loss:  1.29454269e-02\n",
      "Epoch: 4008 mean train loss:  1.45317862e-02, mean val. rec. loss:  1.29438393e-02\n",
      "Epoch: 4009 mean train loss:  1.45300134e-02, mean val. rec. loss:  1.29422586e-02\n",
      "Epoch: 4010 mean train loss:  1.45282461e-02, mean val. rec. loss:  1.29406721e-02\n",
      "Epoch: 4011 mean train loss:  1.45264808e-02, mean val. rec. loss:  1.29390857e-02\n",
      "Epoch: 4012 mean train loss:  1.45247164e-02, mean val. rec. loss:  1.29375118e-02\n",
      "Epoch: 4013 mean train loss:  1.45229529e-02, mean val. rec. loss:  1.29359401e-02\n",
      "Epoch: 4014 mean train loss:  1.45211903e-02, mean val. rec. loss:  1.29343570e-02\n",
      "Epoch: 4015 mean train loss:  1.45194305e-02, mean val. rec. loss:  1.29327808e-02\n",
      "Epoch: 4016 mean train loss:  1.45176708e-02, mean val. rec. loss:  1.29312080e-02\n",
      "Epoch: 4017 mean train loss:  1.45159110e-02, mean val. rec. loss:  1.29296397e-02\n",
      "Epoch: 4018 mean train loss:  1.45141587e-02, mean val. rec. loss:  1.29280703e-02\n",
      "Epoch: 4019 mean train loss:  1.45124063e-02, mean val. rec. loss:  1.29265076e-02\n",
      "Epoch: 4020 mean train loss:  1.45106503e-02, mean val. rec. loss:  1.29249326e-02\n",
      "Epoch: 4021 mean train loss:  1.45088943e-02, mean val. rec. loss:  1.29233711e-02\n",
      "Epoch: 4022 mean train loss:  1.45071475e-02, mean val. rec. loss:  1.29218107e-02\n",
      "Epoch: 4023 mean train loss:  1.45054017e-02, mean val. rec. loss:  1.29202481e-02\n",
      "Epoch: 4024 mean train loss:  1.45036540e-02, mean val. rec. loss:  1.29186877e-02\n",
      "Epoch: 4025 mean train loss:  1.45019092e-02, mean val. rec. loss:  1.29171274e-02\n",
      "Epoch: 4026 mean train loss:  1.45001643e-02, mean val. rec. loss:  1.29155693e-02\n",
      "Epoch: 4027 mean train loss:  1.44984204e-02, mean val. rec. loss:  1.29140146e-02\n",
      "Epoch: 4028 mean train loss:  1.44966783e-02, mean val. rec. loss:  1.29124611e-02\n",
      "Epoch: 4029 mean train loss:  1.44949390e-02, mean val. rec. loss:  1.29109064e-02\n",
      "Epoch: 4030 mean train loss:  1.44931960e-02, mean val. rec. loss:  1.29093506e-02\n",
      "Epoch: 4031 mean train loss:  1.44914548e-02, mean val. rec. loss:  1.29078084e-02\n",
      "Epoch: 4032 mean train loss:  1.44897221e-02, mean val. rec. loss:  1.29062560e-02\n",
      "Epoch: 4033 mean train loss:  1.44879930e-02, mean val. rec. loss:  1.29047081e-02\n",
      "Epoch: 4034 mean train loss:  1.44862556e-02, mean val. rec. loss:  1.29031613e-02\n",
      "Epoch: 4035 mean train loss:  1.44845182e-02, mean val. rec. loss:  1.29016146e-02\n",
      "Epoch: 4036 mean train loss:  1.44827919e-02, mean val. rec. loss:  1.29000815e-02\n",
      "Epoch: 4037 mean train loss:  1.44810638e-02, mean val. rec. loss:  1.28985461e-02\n",
      "Epoch: 4038 mean train loss:  1.44793357e-02, mean val. rec. loss:  1.28969993e-02\n",
      "Epoch: 4039 mean train loss:  1.44776085e-02, mean val. rec. loss:  1.28954571e-02\n",
      "Epoch: 4040 mean train loss:  1.44758841e-02, mean val. rec. loss:  1.28939296e-02\n",
      "Epoch: 4041 mean train loss:  1.44741616e-02, mean val. rec. loss:  1.28924010e-02\n",
      "Epoch: 4042 mean train loss:  1.44724382e-02, mean val. rec. loss:  1.28908577e-02\n",
      "Epoch: 4043 mean train loss:  1.44707138e-02, mean val. rec. loss:  1.28893314e-02\n",
      "Epoch: 4044 mean train loss:  1.44689987e-02, mean val. rec. loss:  1.28878039e-02\n",
      "Epoch: 4045 mean train loss:  1.44672864e-02, mean val. rec. loss:  1.28862685e-02\n",
      "Epoch: 4046 mean train loss:  1.44655657e-02, mean val. rec. loss:  1.28847433e-02\n",
      "Epoch: 4047 mean train loss:  1.44638460e-02, mean val. rec. loss:  1.28832238e-02\n",
      "Epoch: 4048 mean train loss:  1.44621365e-02, mean val. rec. loss:  1.28816997e-02\n",
      "Epoch: 4049 mean train loss:  1.44604252e-02, mean val. rec. loss:  1.28801768e-02\n",
      "Epoch: 4050 mean train loss:  1.44587147e-02, mean val. rec. loss:  1.28786550e-02\n",
      "Epoch: 4051 mean train loss:  1.44570043e-02, mean val. rec. loss:  1.28771377e-02\n",
      "Epoch: 4052 mean train loss:  1.44552976e-02, mean val. rec. loss:  1.28756204e-02\n",
      "Epoch: 4053 mean train loss:  1.44535928e-02, mean val. rec. loss:  1.28741066e-02\n",
      "Epoch: 4054 mean train loss:  1.44518852e-02, mean val. rec. loss:  1.28725927e-02\n",
      "Epoch: 4055 mean train loss:  1.44501822e-02, mean val. rec. loss:  1.28710755e-02\n",
      "Epoch: 4056 mean train loss:  1.44484755e-02, mean val. rec. loss:  1.28695616e-02\n",
      "Epoch: 4057 mean train loss:  1.44467800e-02, mean val. rec. loss:  1.28680580e-02\n",
      "Epoch: 4058 mean train loss:  1.44450844e-02, mean val. rec. loss:  1.28665498e-02\n",
      "Epoch: 4059 mean train loss:  1.44433824e-02, mean val. rec. loss:  1.28650393e-02\n",
      "Epoch: 4060 mean train loss:  1.44416803e-02, mean val. rec. loss:  1.28635447e-02\n",
      "Epoch: 4061 mean train loss:  1.44399895e-02, mean val. rec. loss:  1.28620343e-02\n",
      "Epoch: 4062 mean train loss:  1.44382958e-02, mean val. rec. loss:  1.28605261e-02\n",
      "Epoch: 4063 mean train loss:  1.44366031e-02, mean val. rec. loss:  1.28590315e-02\n",
      "Epoch: 4064 mean train loss:  1.44349131e-02, mean val. rec. loss:  1.28575392e-02\n",
      "Epoch: 4065 mean train loss:  1.44332232e-02, mean val. rec. loss:  1.28560355e-02\n",
      "Epoch: 4066 mean train loss:  1.44315342e-02, mean val. rec. loss:  1.28545342e-02\n",
      "Epoch: 4067 mean train loss:  1.44298480e-02, mean val. rec. loss:  1.28530441e-02\n",
      "Epoch: 4068 mean train loss:  1.44281590e-02, mean val. rec. loss:  1.28515518e-02\n",
      "Epoch: 4069 mean train loss:  1.44264793e-02, mean val. rec. loss:  1.28500618e-02\n",
      "Epoch: 4070 mean train loss:  1.44247996e-02, mean val. rec. loss:  1.28485774e-02\n",
      "Epoch: 4071 mean train loss:  1.44231143e-02, mean val. rec. loss:  1.28470817e-02\n",
      "Epoch: 4072 mean train loss:  1.44214309e-02, mean val. rec. loss:  1.28455996e-02\n",
      "Epoch: 4073 mean train loss:  1.44197568e-02, mean val. rec. loss:  1.28441129e-02\n",
      "Epoch: 4074 mean train loss:  1.44180808e-02, mean val. rec. loss:  1.28426297e-02\n",
      "Epoch: 4075 mean train loss:  1.44164058e-02, mean val. rec. loss:  1.28411498e-02\n",
      "Epoch: 4076 mean train loss:  1.44147326e-02, mean val. rec. loss:  1.28396700e-02\n",
      "Epoch: 4077 mean train loss:  1.44130594e-02, mean val. rec. loss:  1.28381879e-02\n",
      "Epoch: 4078 mean train loss:  1.44113881e-02, mean val. rec. loss:  1.28367103e-02\n",
      "Epoch: 4079 mean train loss:  1.44097177e-02, mean val. rec. loss:  1.28352361e-02\n",
      "Epoch: 4080 mean train loss:  1.44080492e-02, mean val. rec. loss:  1.28337586e-02\n",
      "Epoch: 4081 mean train loss:  1.44063788e-02, mean val. rec. loss:  1.28322833e-02\n",
      "Epoch: 4082 mean train loss:  1.44047177e-02, mean val. rec. loss:  1.28308170e-02\n",
      "Epoch: 4083 mean train loss:  1.44030567e-02, mean val. rec. loss:  1.28293474e-02\n",
      "Epoch: 4084 mean train loss:  1.44013900e-02, mean val. rec. loss:  1.28278744e-02\n",
      "Epoch: 4085 mean train loss:  1.43997233e-02, mean val. rec. loss:  1.28264206e-02\n",
      "Epoch: 4086 mean train loss:  1.43980678e-02, mean val. rec. loss:  1.28249464e-02\n",
      "Epoch: 4087 mean train loss:  1.43964077e-02, mean val. rec. loss:  1.28234768e-02\n",
      "Epoch: 4088 mean train loss:  1.43947513e-02, mean val. rec. loss:  1.28220208e-02\n",
      "Epoch: 4089 mean train loss:  1.43930958e-02, mean val. rec. loss:  1.28205670e-02\n",
      "Epoch: 4090 mean train loss:  1.43914403e-02, mean val. rec. loss:  1.28191031e-02\n",
      "Epoch: 4091 mean train loss:  1.43897876e-02, mean val. rec. loss:  1.28176391e-02\n",
      "Epoch: 4092 mean train loss:  1.43881349e-02, mean val. rec. loss:  1.28161831e-02\n",
      "Epoch: 4093 mean train loss:  1.43864813e-02, mean val. rec. loss:  1.28147316e-02\n",
      "Epoch: 4094 mean train loss:  1.43848361e-02, mean val. rec. loss:  1.28132801e-02\n",
      "Epoch: 4095 mean train loss:  1.43831927e-02, mean val. rec. loss:  1.28118331e-02\n",
      "Epoch: 4096 mean train loss:  1.43815409e-02, mean val. rec. loss:  1.28103748e-02\n",
      "Epoch: 4097 mean train loss:  1.43798929e-02, mean val. rec. loss:  1.28089279e-02\n",
      "Epoch: 4098 mean train loss:  1.43782523e-02, mean val. rec. loss:  1.28074809e-02\n",
      "Epoch: 4099 mean train loss:  1.43766098e-02, mean val. rec. loss:  1.28060351e-02\n",
      "Epoch: 4100 mean train loss:  1.43749692e-02, mean val. rec. loss:  1.28045893e-02\n",
      "Epoch: 4101 mean train loss:  1.43733305e-02, mean val. rec. loss:  1.28031469e-02\n",
      "Epoch: 4102 mean train loss:  1.43716909e-02, mean val. rec. loss:  1.28017045e-02\n",
      "Epoch: 4103 mean train loss:  1.43700549e-02, mean val. rec. loss:  1.28002666e-02\n",
      "Epoch: 4104 mean train loss:  1.43684199e-02, mean val. rec. loss:  1.27988219e-02\n",
      "Epoch: 4105 mean train loss:  1.43667812e-02, mean val. rec. loss:  1.27973953e-02\n",
      "Epoch: 4106 mean train loss:  1.43651527e-02, mean val. rec. loss:  1.27959541e-02\n",
      "Epoch: 4107 mean train loss:  1.43635261e-02, mean val. rec. loss:  1.27945173e-02\n",
      "Epoch: 4108 mean train loss:  1.43618929e-02, mean val. rec. loss:  1.27930817e-02\n",
      "Epoch: 4109 mean train loss:  1.43602598e-02, mean val. rec. loss:  1.27916529e-02\n",
      "Epoch: 4110 mean train loss:  1.43586397e-02, mean val. rec. loss:  1.27902173e-02\n",
      "Epoch: 4111 mean train loss:  1.43570065e-02, mean val. rec. loss:  1.27887930e-02\n",
      "Epoch: 4112 mean train loss:  1.43553920e-02, mean val. rec. loss:  1.27873596e-02\n",
      "Epoch: 4113 mean train loss:  1.43537598e-02, mean val. rec. loss:  1.27859376e-02\n",
      "Epoch: 4114 mean train loss:  1.43521444e-02, mean val. rec. loss:  1.27845111e-02\n",
      "Epoch: 4115 mean train loss:  1.43505187e-02, mean val. rec. loss:  1.27830914e-02\n",
      "Epoch: 4116 mean train loss:  1.43489042e-02, mean val. rec. loss:  1.27816694e-02\n",
      "Epoch: 4117 mean train loss:  1.43472896e-02, mean val. rec. loss:  1.27802564e-02\n",
      "Epoch: 4118 mean train loss:  1.43456695e-02, mean val. rec. loss:  1.27788287e-02\n",
      "Epoch: 4119 mean train loss:  1.43440522e-02, mean val. rec. loss:  1.27774147e-02\n",
      "Epoch: 4120 mean train loss:  1.43424414e-02, mean val. rec. loss:  1.27760017e-02\n",
      "Epoch: 4121 mean train loss:  1.43408344e-02, mean val. rec. loss:  1.27745854e-02\n",
      "Epoch: 4122 mean train loss:  1.43392226e-02, mean val. rec. loss:  1.27731725e-02\n",
      "Epoch: 4123 mean train loss:  1.43376156e-02, mean val. rec. loss:  1.27717618e-02\n",
      "Epoch: 4124 mean train loss:  1.43360066e-02, mean val. rec. loss:  1.27703523e-02\n",
      "Epoch: 4125 mean train loss:  1.43344005e-02, mean val. rec. loss:  1.27689439e-02\n",
      "Epoch: 4126 mean train loss:  1.43327943e-02, mean val. rec. loss:  1.27675400e-02\n",
      "Epoch: 4127 mean train loss:  1.43311910e-02, mean val. rec. loss:  1.27661316e-02\n",
      "Epoch: 4128 mean train loss:  1.43295839e-02, mean val. rec. loss:  1.27647232e-02\n",
      "Epoch: 4129 mean train loss:  1.43279806e-02, mean val. rec. loss:  1.27633307e-02\n",
      "Epoch: 4130 mean train loss:  1.43263828e-02, mean val. rec. loss:  1.27619246e-02\n",
      "Epoch: 4131 mean train loss:  1.43247888e-02, mean val. rec. loss:  1.27605252e-02\n",
      "Epoch: 4132 mean train loss:  1.43231873e-02, mean val. rec. loss:  1.27591259e-02\n",
      "Epoch: 4133 mean train loss:  1.43215886e-02, mean val. rec. loss:  1.27577334e-02\n",
      "Epoch: 4134 mean train loss:  1.43200030e-02, mean val. rec. loss:  1.27563318e-02\n",
      "Epoch: 4135 mean train loss:  1.43184015e-02, mean val. rec. loss:  1.27549415e-02\n",
      "Epoch: 4136 mean train loss:  1.43168186e-02, mean val. rec. loss:  1.27535445e-02\n",
      "Epoch: 4137 mean train loss:  1.43152199e-02, mean val. rec. loss:  1.27521610e-02\n",
      "Epoch: 4138 mean train loss:  1.43136399e-02, mean val. rec. loss:  1.27507628e-02\n",
      "Epoch: 4139 mean train loss:  1.43120486e-02, mean val. rec. loss:  1.27493771e-02\n",
      "Epoch: 4140 mean train loss:  1.43104583e-02, mean val. rec. loss:  1.27479948e-02\n",
      "Epoch: 4141 mean train loss:  1.43088782e-02, mean val. rec. loss:  1.27466113e-02\n",
      "Epoch: 4142 mean train loss:  1.43072991e-02, mean val. rec. loss:  1.27452324e-02\n",
      "Epoch: 4143 mean train loss:  1.43057125e-02, mean val. rec. loss:  1.27438422e-02\n",
      "Epoch: 4144 mean train loss:  1.43041259e-02, mean val. rec. loss:  1.27424587e-02\n",
      "Epoch: 4145 mean train loss:  1.43025486e-02, mean val. rec. loss:  1.27410809e-02\n",
      "Epoch: 4146 mean train loss:  1.43009704e-02, mean val. rec. loss:  1.27397031e-02\n",
      "Epoch: 4147 mean train loss:  1.42993941e-02, mean val. rec. loss:  1.27383288e-02\n",
      "Epoch: 4148 mean train loss:  1.42978187e-02, mean val. rec. loss:  1.27369544e-02\n",
      "Epoch: 4149 mean train loss:  1.42962461e-02, mean val. rec. loss:  1.27355800e-02\n",
      "Epoch: 4150 mean train loss:  1.42946734e-02, mean val. rec. loss:  1.27342181e-02\n",
      "Epoch: 4151 mean train loss:  1.42931055e-02, mean val. rec. loss:  1.27328426e-02\n",
      "Epoch: 4152 mean train loss:  1.42915394e-02, mean val. rec. loss:  1.27314716e-02\n",
      "Epoch: 4153 mean train loss:  1.42899677e-02, mean val. rec. loss:  1.27300984e-02\n",
      "Epoch: 4154 mean train loss:  1.42883969e-02, mean val. rec. loss:  1.27287285e-02\n",
      "Epoch: 4155 mean train loss:  1.42868355e-02, mean val. rec. loss:  1.27273700e-02\n",
      "Epoch: 4156 mean train loss:  1.42852703e-02, mean val. rec. loss:  1.27260104e-02\n",
      "Epoch: 4157 mean train loss:  1.42837061e-02, mean val. rec. loss:  1.27246417e-02\n",
      "Epoch: 4158 mean train loss:  1.42821446e-02, mean val. rec. loss:  1.27232764e-02\n",
      "Epoch: 4159 mean train loss:  1.42805860e-02, mean val. rec. loss:  1.27219213e-02\n",
      "Epoch: 4160 mean train loss:  1.42790273e-02, mean val. rec. loss:  1.27205696e-02\n",
      "Epoch: 4161 mean train loss:  1.42774687e-02, mean val. rec. loss:  1.27192020e-02\n",
      "Epoch: 4162 mean train loss:  1.42759082e-02, mean val. rec. loss:  1.27178503e-02\n",
      "Epoch: 4163 mean train loss:  1.42743579e-02, mean val. rec. loss:  1.27164975e-02\n",
      "Epoch: 4164 mean train loss:  1.42728076e-02, mean val. rec. loss:  1.27151367e-02\n",
      "Epoch: 4165 mean train loss:  1.42712508e-02, mean val. rec. loss:  1.27137872e-02\n",
      "Epoch: 4166 mean train loss:  1.42696950e-02, mean val. rec. loss:  1.27124446e-02\n",
      "Epoch: 4167 mean train loss:  1.42681493e-02, mean val. rec. loss:  1.27110941e-02\n",
      "Epoch: 4168 mean train loss:  1.42666028e-02, mean val. rec. loss:  1.27097469e-02\n",
      "Epoch: 4169 mean train loss:  1.42650553e-02, mean val. rec. loss:  1.27084009e-02\n",
      "Epoch: 4170 mean train loss:  1.42635097e-02, mean val. rec. loss:  1.27070594e-02\n",
      "Epoch: 4171 mean train loss:  1.42619650e-02, mean val. rec. loss:  1.27057167e-02\n",
      "Epoch: 4172 mean train loss:  1.42604222e-02, mean val. rec. loss:  1.27043696e-02\n",
      "Epoch: 4173 mean train loss:  1.42588766e-02, mean val. rec. loss:  1.27030371e-02\n",
      "Epoch: 4174 mean train loss:  1.42573402e-02, mean val. rec. loss:  1.27016957e-02\n",
      "Epoch: 4175 mean train loss:  1.42558058e-02, mean val. rec. loss:  1.27003553e-02\n",
      "Epoch: 4176 mean train loss:  1.42542639e-02, mean val. rec. loss:  1.26990161e-02\n",
      "Epoch: 4177 mean train loss:  1.42527229e-02, mean val. rec. loss:  1.26976802e-02\n",
      "Epoch: 4178 mean train loss:  1.42511913e-02, mean val. rec. loss:  1.26963524e-02\n",
      "Epoch: 4179 mean train loss:  1.42496587e-02, mean val. rec. loss:  1.26950267e-02\n",
      "Epoch: 4180 mean train loss:  1.42481261e-02, mean val. rec. loss:  1.26936898e-02\n",
      "Epoch: 4181 mean train loss:  1.42465963e-02, mean val. rec. loss:  1.26923551e-02\n",
      "Epoch: 4182 mean train loss:  1.42450666e-02, mean val. rec. loss:  1.26910363e-02\n",
      "Epoch: 4183 mean train loss:  1.42435377e-02, mean val. rec. loss:  1.26897118e-02\n",
      "Epoch: 4184 mean train loss:  1.42420154e-02, mean val. rec. loss:  1.26883896e-02\n",
      "Epoch: 4185 mean train loss:  1.42404846e-02, mean val. rec. loss:  1.26870572e-02\n",
      "Epoch: 4186 mean train loss:  1.42389567e-02, mean val. rec. loss:  1.26857395e-02\n",
      "Epoch: 4187 mean train loss:  1.42374381e-02, mean val. rec. loss:  1.26844195e-02\n",
      "Epoch: 4188 mean train loss:  1.42359204e-02, mean val. rec. loss:  1.26830939e-02\n",
      "Epoch: 4189 mean train loss:  1.42343943e-02, mean val. rec. loss:  1.26817728e-02\n",
      "Epoch: 4190 mean train loss:  1.42328767e-02, mean val. rec. loss:  1.26804597e-02\n",
      "Epoch: 4191 mean train loss:  1.42313590e-02, mean val. rec. loss:  1.26791431e-02\n",
      "Epoch: 4192 mean train loss:  1.42298422e-02, mean val. rec. loss:  1.26778300e-02\n",
      "Epoch: 4193 mean train loss:  1.42283264e-02, mean val. rec. loss:  1.26765180e-02\n",
      "Epoch: 4194 mean train loss:  1.42268115e-02, mean val. rec. loss:  1.26752150e-02\n",
      "Epoch: 4195 mean train loss:  1.42253031e-02, mean val. rec. loss:  1.26739030e-02\n",
      "Epoch: 4196 mean train loss:  1.42237892e-02, mean val. rec. loss:  1.26725921e-02\n",
      "Epoch: 4197 mean train loss:  1.42222771e-02, mean val. rec. loss:  1.26712926e-02\n",
      "Epoch: 4198 mean train loss:  1.42207715e-02, mean val. rec. loss:  1.26699840e-02\n",
      "Epoch: 4199 mean train loss:  1.42192678e-02, mean val. rec. loss:  1.26686777e-02\n",
      "Epoch: 4200 mean train loss:  1.42177575e-02, mean val. rec. loss:  1.26673702e-02\n",
      "Epoch: 4201 mean train loss:  1.42162538e-02, mean val. rec. loss:  1.26660740e-02\n",
      "Epoch: 4202 mean train loss:  1.42147501e-02, mean val. rec. loss:  1.26647790e-02\n",
      "Epoch: 4203 mean train loss:  1.42132473e-02, mean val. rec. loss:  1.26634750e-02\n",
      "Epoch: 4204 mean train loss:  1.42117464e-02, mean val. rec. loss:  1.26621720e-02\n",
      "Epoch: 4205 mean train loss:  1.42102482e-02, mean val. rec. loss:  1.26608759e-02\n",
      "Epoch: 4206 mean train loss:  1.42087464e-02, mean val. rec. loss:  1.26595854e-02\n",
      "Epoch: 4207 mean train loss:  1.42072529e-02, mean val. rec. loss:  1.26582916e-02\n",
      "Epoch: 4208 mean train loss:  1.42057613e-02, mean val. rec. loss:  1.26570068e-02\n",
      "Epoch: 4209 mean train loss:  1.42042641e-02, mean val. rec. loss:  1.26557061e-02\n",
      "Epoch: 4210 mean train loss:  1.42027669e-02, mean val. rec. loss:  1.26544145e-02\n",
      "Epoch: 4211 mean train loss:  1.42012753e-02, mean val. rec. loss:  1.26531263e-02\n",
      "Epoch: 4212 mean train loss:  1.41997855e-02, mean val. rec. loss:  1.26518381e-02\n",
      "Epoch: 4213 mean train loss:  1.41982958e-02, mean val. rec. loss:  1.26505544e-02\n",
      "Epoch: 4214 mean train loss:  1.41968097e-02, mean val. rec. loss:  1.26492708e-02\n",
      "Epoch: 4215 mean train loss:  1.41953237e-02, mean val. rec. loss:  1.26479871e-02\n",
      "Epoch: 4216 mean train loss:  1.41938377e-02, mean val. rec. loss:  1.26467148e-02\n",
      "Epoch: 4217 mean train loss:  1.41923572e-02, mean val. rec. loss:  1.26454300e-02\n",
      "Epoch: 4218 mean train loss:  1.41908805e-02, mean val. rec. loss:  1.26441531e-02\n",
      "Epoch: 4219 mean train loss:  1.41893954e-02, mean val. rec. loss:  1.26428729e-02\n",
      "Epoch: 4220 mean train loss:  1.41879113e-02, mean val. rec. loss:  1.26415960e-02\n",
      "Epoch: 4221 mean train loss:  1.41864364e-02, mean val. rec. loss:  1.26403260e-02\n",
      "Epoch: 4222 mean train loss:  1.41849597e-02, mean val. rec. loss:  1.26390582e-02\n",
      "Epoch: 4223 mean train loss:  1.41834839e-02, mean val. rec. loss:  1.26377813e-02\n",
      "Epoch: 4224 mean train loss:  1.41820091e-02, mean val. rec. loss:  1.26365067e-02\n",
      "Epoch: 4225 mean train loss:  1.41805370e-02, mean val. rec. loss:  1.26352435e-02\n",
      "Epoch: 4226 mean train loss:  1.41790631e-02, mean val. rec. loss:  1.26339746e-02\n",
      "Epoch: 4227 mean train loss:  1.41775966e-02, mean val. rec. loss:  1.26327125e-02\n",
      "Epoch: 4228 mean train loss:  1.41761245e-02, mean val. rec. loss:  1.26314367e-02\n",
      "Epoch: 4229 mean train loss:  1.41746525e-02, mean val. rec. loss:  1.26301746e-02\n",
      "Epoch: 4230 mean train loss:  1.41731888e-02, mean val. rec. loss:  1.26289159e-02\n",
      "Epoch: 4231 mean train loss:  1.41717270e-02, mean val. rec. loss:  1.26276481e-02\n",
      "Epoch: 4232 mean train loss:  1.41702586e-02, mean val. rec. loss:  1.26263860e-02\n",
      "Epoch: 4233 mean train loss:  1.41687959e-02, mean val. rec. loss:  1.26251273e-02\n",
      "Epoch: 4234 mean train loss:  1.41673341e-02, mean val. rec. loss:  1.26238697e-02\n",
      "Epoch: 4235 mean train loss:  1.41658750e-02, mean val. rec. loss:  1.26226155e-02\n",
      "Epoch: 4236 mean train loss:  1.41644151e-02, mean val. rec. loss:  1.26213614e-02\n",
      "Epoch: 4237 mean train loss:  1.41629570e-02, mean val. rec. loss:  1.26201117e-02\n",
      "Epoch: 4238 mean train loss:  1.41615045e-02, mean val. rec. loss:  1.26188598e-02\n",
      "Epoch: 4239 mean train loss:  1.41600454e-02, mean val. rec. loss:  1.26176056e-02\n",
      "Epoch: 4240 mean train loss:  1.41585883e-02, mean val. rec. loss:  1.26163650e-02\n",
      "Epoch: 4241 mean train loss:  1.41571386e-02, mean val. rec. loss:  1.26151120e-02\n",
      "Epoch: 4242 mean train loss:  1.41556916e-02, mean val. rec. loss:  1.26138601e-02\n",
      "Epoch: 4243 mean train loss:  1.41542345e-02, mean val. rec. loss:  1.26126173e-02\n",
      "Epoch: 4244 mean train loss:  1.41527931e-02, mean val. rec. loss:  1.26113676e-02\n",
      "Epoch: 4245 mean train loss:  1.41513369e-02, mean val. rec. loss:  1.26101304e-02\n",
      "Epoch: 4246 mean train loss:  1.41498993e-02, mean val. rec. loss:  1.26088831e-02\n",
      "Epoch: 4247 mean train loss:  1.41484468e-02, mean val. rec. loss:  1.26076470e-02\n",
      "Epoch: 4248 mean train loss:  1.41470073e-02, mean val. rec. loss:  1.26064065e-02\n",
      "Epoch: 4249 mean train loss:  1.41455678e-02, mean val. rec. loss:  1.26051625e-02\n",
      "Epoch: 4250 mean train loss:  1.41441228e-02, mean val. rec. loss:  1.26039287e-02\n",
      "Epoch: 4251 mean train loss:  1.41426796e-02, mean val. rec. loss:  1.26026984e-02\n",
      "Epoch: 4252 mean train loss:  1.41412448e-02, mean val. rec. loss:  1.26014669e-02\n",
      "Epoch: 4253 mean train loss:  1.41398109e-02, mean val. rec. loss:  1.26002354e-02\n",
      "Epoch: 4254 mean train loss:  1.41383751e-02, mean val. rec. loss:  1.25990016e-02\n",
      "Epoch: 4255 mean train loss:  1.41369403e-02, mean val. rec. loss:  1.25977769e-02\n",
      "Epoch: 4256 mean train loss:  1.41355064e-02, mean val. rec. loss:  1.25965454e-02\n",
      "Epoch: 4257 mean train loss:  1.41340763e-02, mean val. rec. loss:  1.25953184e-02\n",
      "Epoch: 4258 mean train loss:  1.41326442e-02, mean val. rec. loss:  1.25940915e-02\n",
      "Epoch: 4259 mean train loss:  1.41312104e-02, mean val. rec. loss:  1.25928623e-02\n",
      "Epoch: 4260 mean train loss:  1.41297783e-02, mean val. rec. loss:  1.25916489e-02\n",
      "Epoch: 4261 mean train loss:  1.41283556e-02, mean val. rec. loss:  1.25904231e-02\n",
      "Epoch: 4262 mean train loss:  1.41269338e-02, mean val. rec. loss:  1.25892018e-02\n",
      "Epoch: 4263 mean train loss:  1.41255046e-02, mean val. rec. loss:  1.25879862e-02\n",
      "Epoch: 4264 mean train loss:  1.41240810e-02, mean val. rec. loss:  1.25867603e-02\n",
      "Epoch: 4265 mean train loss:  1.41226592e-02, mean val. rec. loss:  1.25855390e-02\n",
      "Epoch: 4266 mean train loss:  1.41212393e-02, mean val. rec. loss:  1.25843279e-02\n",
      "Epoch: 4267 mean train loss:  1.41198193e-02, mean val. rec. loss:  1.25831214e-02\n",
      "Epoch: 4268 mean train loss:  1.41184004e-02, mean val. rec. loss:  1.25818978e-02\n",
      "Epoch: 4269 mean train loss:  1.41169804e-02, mean val. rec. loss:  1.25806901e-02\n",
      "Epoch: 4270 mean train loss:  1.41155680e-02, mean val. rec. loss:  1.25794779e-02\n",
      "Epoch: 4271 mean train loss:  1.41141574e-02, mean val. rec. loss:  1.25782634e-02\n",
      "Epoch: 4272 mean train loss:  1.41127402e-02, mean val. rec. loss:  1.25770592e-02\n",
      "Epoch: 4273 mean train loss:  1.41113240e-02, mean val. rec. loss:  1.25758503e-02\n",
      "Epoch: 4274 mean train loss:  1.41099143e-02, mean val. rec. loss:  1.25746449e-02\n",
      "Epoch: 4275 mean train loss:  1.41085037e-02, mean val. rec. loss:  1.25734395e-02\n",
      "Epoch: 4276 mean train loss:  1.41070959e-02, mean val. rec. loss:  1.25722375e-02\n",
      "Epoch: 4277 mean train loss:  1.41056890e-02, mean val. rec. loss:  1.25710355e-02\n",
      "Epoch: 4278 mean train loss:  1.41042831e-02, mean val. rec. loss:  1.25698312e-02\n",
      "Epoch: 4279 mean train loss:  1.41028827e-02, mean val. rec. loss:  1.25686382e-02\n",
      "Epoch: 4280 mean train loss:  1.41014842e-02, mean val. rec. loss:  1.25674396e-02\n",
      "Epoch: 4281 mean train loss:  1.41000783e-02, mean val. rec. loss:  1.25662399e-02\n",
      "Epoch: 4282 mean train loss:  1.40986732e-02, mean val. rec. loss:  1.25650560e-02\n",
      "Epoch: 4283 mean train loss:  1.40972775e-02, mean val. rec. loss:  1.25638551e-02\n",
      "Epoch: 4284 mean train loss:  1.40958809e-02, mean val. rec. loss:  1.25626542e-02\n",
      "Epoch: 4285 mean train loss:  1.40944833e-02, mean val. rec. loss:  1.25614692e-02\n",
      "Epoch: 4286 mean train loss:  1.40930885e-02, mean val. rec. loss:  1.25602842e-02\n",
      "Epoch: 4287 mean train loss:  1.40916928e-02, mean val. rec. loss:  1.25590924e-02\n",
      "Epoch: 4288 mean train loss:  1.40902999e-02, mean val. rec. loss:  1.25579040e-02\n",
      "Epoch: 4289 mean train loss:  1.40889126e-02, mean val. rec. loss:  1.25567201e-02\n",
      "Epoch: 4290 mean train loss:  1.40875169e-02, mean val. rec. loss:  1.25555261e-02\n",
      "Epoch: 4291 mean train loss:  1.40861239e-02, mean val. rec. loss:  1.25543456e-02\n",
      "Epoch: 4292 mean train loss:  1.40847385e-02, mean val. rec. loss:  1.25531629e-02\n",
      "Epoch: 4293 mean train loss:  1.40833558e-02, mean val. rec. loss:  1.25519756e-02\n",
      "Epoch: 4294 mean train loss:  1.40819666e-02, mean val. rec. loss:  1.25508076e-02\n",
      "Epoch: 4295 mean train loss:  1.40805802e-02, mean val. rec. loss:  1.25496147e-02\n",
      "Epoch: 4296 mean train loss:  1.40791984e-02, mean val. rec. loss:  1.25484501e-02\n",
      "Epoch: 4297 mean train loss:  1.40778167e-02, mean val. rec. loss:  1.25472582e-02\n",
      "Epoch: 4298 mean train loss:  1.40764359e-02, mean val. rec. loss:  1.25460971e-02\n",
      "Epoch: 4299 mean train loss:  1.40750541e-02, mean val. rec. loss:  1.25449155e-02\n",
      "Epoch: 4300 mean train loss:  1.40736789e-02, mean val. rec. loss:  1.25437407e-02\n",
      "Epoch: 4301 mean train loss:  1.40722990e-02, mean val. rec. loss:  1.25425636e-02\n",
      "Epoch: 4302 mean train loss:  1.40709182e-02, mean val. rec. loss:  1.25413922e-02\n",
      "Epoch: 4303 mean train loss:  1.40695476e-02, mean val. rec. loss:  1.25402299e-02\n",
      "Epoch: 4304 mean train loss:  1.40681780e-02, mean val. rec. loss:  1.25390573e-02\n",
      "Epoch: 4305 mean train loss:  1.40667981e-02, mean val. rec. loss:  1.25378939e-02\n",
      "Epoch: 4306 mean train loss:  1.40654341e-02, mean val. rec. loss:  1.25367213e-02\n",
      "Epoch: 4307 mean train loss:  1.40640579e-02, mean val. rec. loss:  1.25355624e-02\n",
      "Epoch: 4308 mean train loss:  1.40626966e-02, mean val. rec. loss:  1.25343956e-02\n",
      "Epoch: 4309 mean train loss:  1.40613205e-02, mean val. rec. loss:  1.25332400e-02\n",
      "Epoch: 4310 mean train loss:  1.40599592e-02, mean val. rec. loss:  1.25320788e-02\n",
      "Epoch: 4311 mean train loss:  1.40585980e-02, mean val. rec. loss:  1.25309108e-02\n",
      "Epoch: 4312 mean train loss:  1.40572311e-02, mean val. rec. loss:  1.25297565e-02\n",
      "Epoch: 4313 mean train loss:  1.40558643e-02, mean val. rec. loss:  1.25286021e-02\n",
      "Epoch: 4314 mean train loss:  1.40545067e-02, mean val. rec. loss:  1.25274477e-02\n",
      "Epoch: 4315 mean train loss:  1.40531464e-02, mean val. rec. loss:  1.25262910e-02\n",
      "Epoch: 4316 mean train loss:  1.40517879e-02, mean val. rec. loss:  1.25251378e-02\n",
      "Epoch: 4317 mean train loss:  1.40504295e-02, mean val. rec. loss:  1.25239868e-02\n",
      "Epoch: 4318 mean train loss:  1.40490738e-02, mean val. rec. loss:  1.25228347e-02\n",
      "Epoch: 4319 mean train loss:  1.40477190e-02, mean val. rec. loss:  1.25216825e-02\n",
      "Epoch: 4320 mean train loss:  1.40463606e-02, mean val. rec. loss:  1.25205304e-02\n",
      "Epoch: 4321 mean train loss:  1.40450105e-02, mean val. rec. loss:  1.25193896e-02\n",
      "Epoch: 4322 mean train loss:  1.40436632e-02, mean val. rec. loss:  1.25182421e-02\n",
      "Epoch: 4323 mean train loss:  1.40423094e-02, mean val. rec. loss:  1.25170945e-02\n",
      "Epoch: 4324 mean train loss:  1.40409556e-02, mean val. rec. loss:  1.25159526e-02\n",
      "Epoch: 4325 mean train loss:  1.40396157e-02, mean val. rec. loss:  1.25148061e-02\n",
      "Epoch: 4326 mean train loss:  1.40382610e-02, mean val. rec. loss:  1.25136699e-02\n",
      "Epoch: 4327 mean train loss:  1.40369230e-02, mean val. rec. loss:  1.25125245e-02\n",
      "Epoch: 4328 mean train loss:  1.40355710e-02, mean val. rec. loss:  1.25113906e-02\n",
      "Epoch: 4329 mean train loss:  1.40342368e-02, mean val. rec. loss:  1.25102441e-02\n",
      "Epoch: 4330 mean train loss:  1.40328904e-02, mean val. rec. loss:  1.25091090e-02\n",
      "Epoch: 4331 mean train loss:  1.40315459e-02, mean val. rec. loss:  1.25079796e-02\n",
      "Epoch: 4332 mean train loss:  1.40302107e-02, mean val. rec. loss:  1.25068479e-02\n",
      "Epoch: 4333 mean train loss:  1.40288755e-02, mean val. rec. loss:  1.25057184e-02\n",
      "Epoch: 4334 mean train loss:  1.40275348e-02, mean val. rec. loss:  1.25045742e-02\n",
      "Epoch: 4335 mean train loss:  1.40261996e-02, mean val. rec. loss:  1.25034550e-02\n",
      "Epoch: 4336 mean train loss:  1.40248644e-02, mean val. rec. loss:  1.25023131e-02\n",
      "Epoch: 4337 mean train loss:  1.40235311e-02, mean val. rec. loss:  1.25011961e-02\n",
      "Epoch: 4338 mean train loss:  1.40221977e-02, mean val. rec. loss:  1.25000644e-02\n",
      "Epoch: 4339 mean train loss:  1.40208709e-02, mean val. rec. loss:  1.24989361e-02\n",
      "Epoch: 4340 mean train loss:  1.40195385e-02, mean val. rec. loss:  1.24978078e-02\n",
      "Epoch: 4341 mean train loss:  1.40182052e-02, mean val. rec. loss:  1.24966806e-02\n",
      "Epoch: 4342 mean train loss:  1.40168830e-02, mean val. rec. loss:  1.24955648e-02\n",
      "Epoch: 4343 mean train loss:  1.40155590e-02, mean val. rec. loss:  1.24944399e-02\n",
      "Epoch: 4344 mean train loss:  1.40142257e-02, mean val. rec. loss:  1.24933241e-02\n",
      "Epoch: 4345 mean train loss:  1.40129110e-02, mean val. rec. loss:  1.24921991e-02\n",
      "Epoch: 4346 mean train loss:  1.40115795e-02, mean val. rec. loss:  1.24910879e-02\n",
      "Epoch: 4347 mean train loss:  1.40102667e-02, mean val. rec. loss:  1.24899698e-02\n",
      "Epoch: 4348 mean train loss:  1.40089380e-02, mean val. rec. loss:  1.24888585e-02\n",
      "Epoch: 4349 mean train loss:  1.40076233e-02, mean val. rec. loss:  1.24877449e-02\n",
      "Epoch: 4350 mean train loss:  1.40063104e-02, mean val. rec. loss:  1.24866257e-02\n",
      "Epoch: 4351 mean train loss:  1.40049892e-02, mean val. rec. loss:  1.24855155e-02\n",
      "Epoch: 4352 mean train loss:  1.40036689e-02, mean val. rec. loss:  1.24844087e-02\n",
      "Epoch: 4353 mean train loss:  1.40023589e-02, mean val. rec. loss:  1.24832997e-02\n",
      "Epoch: 4354 mean train loss:  1.40010470e-02, mean val. rec. loss:  1.24821918e-02\n",
      "Epoch: 4355 mean train loss:  1.39997341e-02, mean val. rec. loss:  1.24810839e-02\n",
      "Epoch: 4356 mean train loss:  1.39984231e-02, mean val. rec. loss:  1.24799771e-02\n",
      "Epoch: 4357 mean train loss:  1.39971140e-02, mean val. rec. loss:  1.24788749e-02\n",
      "Epoch: 4358 mean train loss:  1.39958049e-02, mean val. rec. loss:  1.24777659e-02\n",
      "Epoch: 4359 mean train loss:  1.39944939e-02, mean val. rec. loss:  1.24766603e-02\n",
      "Epoch: 4360 mean train loss:  1.39931922e-02, mean val. rec. loss:  1.24755671e-02\n",
      "Epoch: 4361 mean train loss:  1.39918915e-02, mean val. rec. loss:  1.24744660e-02\n",
      "Epoch: 4362 mean train loss:  1.39905833e-02, mean val. rec. loss:  1.24733638e-02\n",
      "Epoch: 4363 mean train loss:  1.39892761e-02, mean val. rec. loss:  1.24722695e-02\n",
      "Epoch: 4364 mean train loss:  1.39879818e-02, mean val. rec. loss:  1.24711684e-02\n",
      "Epoch: 4365 mean train loss:  1.39866737e-02, mean val. rec. loss:  1.24700775e-02\n",
      "Epoch: 4366 mean train loss:  1.39853832e-02, mean val. rec. loss:  1.24689776e-02\n",
      "Epoch: 4367 mean train loss:  1.39840768e-02, mean val. rec. loss:  1.24678912e-02\n",
      "Epoch: 4368 mean train loss:  1.39827873e-02, mean val. rec. loss:  1.24667901e-02\n",
      "Epoch: 4369 mean train loss:  1.39814884e-02, mean val. rec. loss:  1.24657027e-02\n",
      "Epoch: 4370 mean train loss:  1.39801914e-02, mean val. rec. loss:  1.24646174e-02\n",
      "Epoch: 4371 mean train loss:  1.39789018e-02, mean val. rec. loss:  1.24635288e-02\n",
      "Epoch: 4372 mean train loss:  1.39776132e-02, mean val. rec. loss:  1.24624481e-02\n",
      "Epoch: 4373 mean train loss:  1.39763180e-02, mean val. rec. loss:  1.24613504e-02\n",
      "Epoch: 4374 mean train loss:  1.39750284e-02, mean val. rec. loss:  1.24602766e-02\n",
      "Epoch: 4375 mean train loss:  1.39737398e-02, mean val. rec. loss:  1.24591800e-02\n",
      "Epoch: 4376 mean train loss:  1.39724530e-02, mean val. rec. loss:  1.24581107e-02\n",
      "Epoch: 4377 mean train loss:  1.39711663e-02, mean val. rec. loss:  1.24570198e-02\n",
      "Epoch: 4378 mean train loss:  1.39698813e-02, mean val. rec. loss:  1.24559459e-02\n",
      "Epoch: 4379 mean train loss:  1.39686020e-02, mean val. rec. loss:  1.24548652e-02\n",
      "Epoch: 4380 mean train loss:  1.39673152e-02, mean val. rec. loss:  1.24537823e-02\n",
      "Epoch: 4381 mean train loss:  1.39660294e-02, mean val. rec. loss:  1.24527141e-02\n",
      "Epoch: 4382 mean train loss:  1.39647519e-02, mean val. rec. loss:  1.24516323e-02\n",
      "Epoch: 4383 mean train loss:  1.39634745e-02, mean val. rec. loss:  1.24505482e-02\n",
      "Epoch: 4384 mean train loss:  1.39621961e-02, mean val. rec. loss:  1.24494822e-02\n",
      "Epoch: 4385 mean train loss:  1.39609186e-02, mean val. rec. loss:  1.24484163e-02\n",
      "Epoch: 4386 mean train loss:  1.39596430e-02, mean val. rec. loss:  1.24473413e-02\n",
      "Epoch: 4387 mean train loss:  1.39583674e-02, mean val. rec. loss:  1.24462674e-02\n",
      "Epoch: 4388 mean train loss:  1.39570983e-02, mean val. rec. loss:  1.24451890e-02\n",
      "Epoch: 4389 mean train loss:  1.39558218e-02, mean val. rec. loss:  1.24441253e-02\n",
      "Epoch: 4390 mean train loss:  1.39545462e-02, mean val. rec. loss:  1.24430617e-02\n",
      "Epoch: 4391 mean train loss:  1.39532809e-02, mean val. rec. loss:  1.24419957e-02\n",
      "Epoch: 4392 mean train loss:  1.39520155e-02, mean val. rec. loss:  1.24409275e-02\n",
      "Epoch: 4393 mean train loss:  1.39507464e-02, mean val. rec. loss:  1.24398650e-02\n",
      "Epoch: 4394 mean train loss:  1.39494811e-02, mean val. rec. loss:  1.24388002e-02\n",
      "Epoch: 4395 mean train loss:  1.39482138e-02, mean val. rec. loss:  1.24377411e-02\n",
      "Epoch: 4396 mean train loss:  1.39469503e-02, mean val. rec. loss:  1.24366797e-02\n",
      "Epoch: 4397 mean train loss:  1.39456859e-02, mean val. rec. loss:  1.24356183e-02\n",
      "Epoch: 4398 mean train loss:  1.39444205e-02, mean val. rec. loss:  1.24345546e-02\n",
      "Epoch: 4399 mean train loss:  1.39431552e-02, mean val. rec. loss:  1.24334988e-02\n",
      "Epoch: 4400 mean train loss:  1.39419001e-02, mean val. rec. loss:  1.24324477e-02\n",
      "Epoch: 4401 mean train loss:  1.39406431e-02, mean val. rec. loss:  1.24313931e-02\n",
      "Epoch: 4402 mean train loss:  1.39393833e-02, mean val. rec. loss:  1.24303441e-02\n",
      "Epoch: 4403 mean train loss:  1.39381254e-02, mean val. rec. loss:  1.24292861e-02\n",
      "Epoch: 4404 mean train loss:  1.39368703e-02, mean val. rec. loss:  1.24282293e-02\n",
      "Epoch: 4405 mean train loss:  1.39356161e-02, mean val. rec. loss:  1.24271826e-02\n",
      "Epoch: 4406 mean train loss:  1.39343619e-02, mean val. rec. loss:  1.24261337e-02\n",
      "Epoch: 4407 mean train loss:  1.39331143e-02, mean val. rec. loss:  1.24250768e-02\n",
      "Epoch: 4408 mean train loss:  1.39318591e-02, mean val. rec. loss:  1.24240313e-02\n",
      "Epoch: 4409 mean train loss:  1.39306059e-02, mean val. rec. loss:  1.24229869e-02\n",
      "Epoch: 4410 mean train loss:  1.39293601e-02, mean val. rec. loss:  1.24219448e-02\n",
      "Epoch: 4411 mean train loss:  1.39281171e-02, mean val. rec. loss:  1.24208992e-02\n",
      "Epoch: 4412 mean train loss:  1.39268713e-02, mean val. rec. loss:  1.24198548e-02\n",
      "Epoch: 4413 mean train loss:  1.39256245e-02, mean val. rec. loss:  1.24188150e-02\n",
      "Epoch: 4414 mean train loss:  1.39243824e-02, mean val. rec. loss:  1.24177717e-02\n",
      "Epoch: 4415 mean train loss:  1.39231366e-02, mean val. rec. loss:  1.24167330e-02\n",
      "Epoch: 4416 mean train loss:  1.39218955e-02, mean val. rec. loss:  1.24156920e-02\n",
      "Epoch: 4417 mean train loss:  1.39206506e-02, mean val. rec. loss:  1.24146510e-02\n",
      "Epoch: 4418 mean train loss:  1.39194067e-02, mean val. rec. loss:  1.24136236e-02\n",
      "Epoch: 4419 mean train loss:  1.39181730e-02, mean val. rec. loss:  1.24125849e-02\n",
      "Epoch: 4420 mean train loss:  1.39169383e-02, mean val. rec. loss:  1.24115496e-02\n",
      "Epoch: 4421 mean train loss:  1.39156972e-02, mean val. rec. loss:  1.24105211e-02\n",
      "Epoch: 4422 mean train loss:  1.39144616e-02, mean val. rec. loss:  1.24094824e-02\n",
      "Epoch: 4423 mean train loss:  1.39132279e-02, mean val. rec. loss:  1.24084459e-02\n",
      "Epoch: 4424 mean train loss:  1.39119952e-02, mean val. rec. loss:  1.24074231e-02\n",
      "Epoch: 4425 mean train loss:  1.39107633e-02, mean val. rec. loss:  1.24063934e-02\n",
      "Epoch: 4426 mean train loss:  1.39095361e-02, mean val. rec. loss:  1.24053706e-02\n",
      "Epoch: 4427 mean train loss:  1.39083024e-02, mean val. rec. loss:  1.24043318e-02\n",
      "Epoch: 4428 mean train loss:  1.39070706e-02, mean val. rec. loss:  1.24033101e-02\n",
      "Epoch: 4429 mean train loss:  1.39058471e-02, mean val. rec. loss:  1.24022850e-02\n",
      "Epoch: 4430 mean train loss:  1.39046227e-02, mean val. rec. loss:  1.24012622e-02\n",
      "Epoch: 4431 mean train loss:  1.39033984e-02, mean val. rec. loss:  1.24002382e-02\n",
      "Epoch: 4432 mean train loss:  1.39021730e-02, mean val. rec. loss:  1.23992176e-02\n",
      "Epoch: 4433 mean train loss:  1.39009514e-02, mean val. rec. loss:  1.23981982e-02\n",
      "Epoch: 4434 mean train loss:  1.38997289e-02, mean val. rec. loss:  1.23971798e-02\n",
      "Epoch: 4435 mean train loss:  1.38985073e-02, mean val. rec. loss:  1.23961581e-02\n",
      "Epoch: 4436 mean train loss:  1.38972839e-02, mean val. rec. loss:  1.23951353e-02\n",
      "Epoch: 4437 mean train loss:  1.38960613e-02, mean val. rec. loss:  1.23941283e-02\n",
      "Epoch: 4438 mean train loss:  1.38948472e-02, mean val. rec. loss:  1.23931111e-02\n",
      "Epoch: 4439 mean train loss:  1.38936358e-02, mean val. rec. loss:  1.23920962e-02\n",
      "Epoch: 4440 mean train loss:  1.38924161e-02, mean val. rec. loss:  1.23910779e-02\n",
      "Epoch: 4441 mean train loss:  1.38912019e-02, mean val. rec. loss:  1.23900709e-02\n",
      "Epoch: 4442 mean train loss:  1.38899878e-02, mean val. rec. loss:  1.23890640e-02\n",
      "Epoch: 4443 mean train loss:  1.38887755e-02, mean val. rec. loss:  1.23880502e-02\n",
      "Epoch: 4444 mean train loss:  1.38875651e-02, mean val. rec. loss:  1.23870387e-02\n",
      "Epoch: 4445 mean train loss:  1.38863584e-02, mean val. rec. loss:  1.23860363e-02\n",
      "Epoch: 4446 mean train loss:  1.38851442e-02, mean val. rec. loss:  1.23850168e-02\n",
      "Epoch: 4447 mean train loss:  1.38839338e-02, mean val. rec. loss:  1.23840178e-02\n",
      "Epoch: 4448 mean train loss:  1.38827318e-02, mean val. rec. loss:  1.23830131e-02\n",
      "Epoch: 4449 mean train loss:  1.38815279e-02, mean val. rec. loss:  1.23820095e-02\n",
      "Epoch: 4450 mean train loss:  1.38803249e-02, mean val. rec. loss:  1.23810048e-02\n",
      "Epoch: 4451 mean train loss:  1.38791200e-02, mean val. rec. loss:  1.23800035e-02\n",
      "Epoch: 4452 mean train loss:  1.38779189e-02, mean val. rec. loss:  1.23790033e-02\n",
      "Epoch: 4453 mean train loss:  1.38767160e-02, mean val. rec. loss:  1.23780043e-02\n",
      "Epoch: 4454 mean train loss:  1.38755158e-02, mean val. rec. loss:  1.23770053e-02\n",
      "Epoch: 4455 mean train loss:  1.38743137e-02, mean val. rec. loss:  1.23760017e-02\n",
      "Epoch: 4456 mean train loss:  1.38731108e-02, mean val. rec. loss:  1.23750027e-02\n",
      "Epoch: 4457 mean train loss:  1.38719171e-02, mean val. rec. loss:  1.23740138e-02\n",
      "Epoch: 4458 mean train loss:  1.38707253e-02, mean val. rec. loss:  1.23730137e-02\n",
      "Epoch: 4459 mean train loss:  1.38695223e-02, mean val. rec. loss:  1.23720226e-02\n",
      "Epoch: 4460 mean train loss:  1.38683361e-02, mean val. rec. loss:  1.23710247e-02\n",
      "Epoch: 4461 mean train loss:  1.38671350e-02, mean val. rec. loss:  1.23700381e-02\n",
      "Epoch: 4462 mean train loss:  1.38659516e-02, mean val. rec. loss:  1.23690448e-02\n",
      "Epoch: 4463 mean train loss:  1.38647542e-02, mean val. rec. loss:  1.23680559e-02\n",
      "Epoch: 4464 mean train loss:  1.38635670e-02, mean val. rec. loss:  1.23670671e-02\n",
      "Epoch: 4465 mean train loss:  1.38623827e-02, mean val. rec. loss:  1.23660737e-02\n",
      "Epoch: 4466 mean train loss:  1.38611927e-02, mean val. rec. loss:  1.23650906e-02\n",
      "Epoch: 4467 mean train loss:  1.38600019e-02, mean val. rec. loss:  1.23641097e-02\n",
      "Epoch: 4468 mean train loss:  1.38588203e-02, mean val. rec. loss:  1.23631231e-02\n",
      "Epoch: 4469 mean train loss:  1.38576360e-02, mean val. rec. loss:  1.23621388e-02\n",
      "Epoch: 4470 mean train loss:  1.38564525e-02, mean val. rec. loss:  1.23611568e-02\n",
      "Epoch: 4471 mean train loss:  1.38552700e-02, mean val. rec. loss:  1.23601748e-02\n",
      "Epoch: 4472 mean train loss:  1.38540885e-02, mean val. rec. loss:  1.23591905e-02\n",
      "Epoch: 4473 mean train loss:  1.38529051e-02, mean val. rec. loss:  1.23582085e-02\n",
      "Epoch: 4474 mean train loss:  1.38517300e-02, mean val. rec. loss:  1.23572355e-02\n",
      "Epoch: 4475 mean train loss:  1.38505559e-02, mean val. rec. loss:  1.23562603e-02\n",
      "Epoch: 4476 mean train loss:  1.38493753e-02, mean val. rec. loss:  1.23552783e-02\n",
      "Epoch: 4477 mean train loss:  1.38481946e-02, mean val. rec. loss:  1.23543065e-02\n",
      "Epoch: 4478 mean train loss:  1.38470289e-02, mean val. rec. loss:  1.23533278e-02\n",
      "Epoch: 4479 mean train loss:  1.38458464e-02, mean val. rec. loss:  1.23523617e-02\n",
      "Epoch: 4480 mean train loss:  1.38446826e-02, mean val. rec. loss:  1.23513831e-02\n",
      "Epoch: 4481 mean train loss:  1.38435029e-02, mean val. rec. loss:  1.23504181e-02\n",
      "Epoch: 4482 mean train loss:  1.38423381e-02, mean val. rec. loss:  1.23494406e-02\n",
      "Epoch: 4483 mean train loss:  1.38411667e-02, mean val. rec. loss:  1.23484722e-02\n",
      "Epoch: 4484 mean train loss:  1.38399945e-02, mean val. rec. loss:  1.23475083e-02\n",
      "Epoch: 4485 mean train loss:  1.38388306e-02, mean val. rec. loss:  1.23465421e-02\n",
      "Epoch: 4486 mean train loss:  1.38376677e-02, mean val. rec. loss:  1.23455760e-02\n",
      "Epoch: 4487 mean train loss:  1.38365020e-02, mean val. rec. loss:  1.23446064e-02\n",
      "Epoch: 4488 mean train loss:  1.38353372e-02, mean val. rec. loss:  1.23436437e-02\n",
      "Epoch: 4489 mean train loss:  1.38341742e-02, mean val. rec. loss:  1.23426809e-02\n",
      "Epoch: 4490 mean train loss:  1.38330131e-02, mean val. rec. loss:  1.23417171e-02\n",
      "Epoch: 4491 mean train loss:  1.38318511e-02, mean val. rec. loss:  1.23407520e-02\n",
      "Epoch: 4492 mean train loss:  1.38306873e-02, mean val. rec. loss:  1.23397882e-02\n",
      "Epoch: 4493 mean train loss:  1.38295243e-02, mean val. rec. loss:  1.23388356e-02\n",
      "Epoch: 4494 mean train loss:  1.38283698e-02, mean val. rec. loss:  1.23378740e-02\n",
      "Epoch: 4495 mean train loss:  1.38272171e-02, mean val. rec. loss:  1.23369147e-02\n",
      "Epoch: 4496 mean train loss:  1.38260560e-02, mean val. rec. loss:  1.23359644e-02\n",
      "Epoch: 4497 mean train loss:  1.38249014e-02, mean val. rec. loss:  1.23350016e-02\n",
      "Epoch: 4498 mean train loss:  1.38237459e-02, mean val. rec. loss:  1.23340389e-02\n",
      "Epoch: 4499 mean train loss:  1.38225933e-02, mean val. rec. loss:  1.23330943e-02\n",
      "Epoch: 4500 mean train loss:  1.38214424e-02, mean val. rec. loss:  1.23321395e-02\n",
      "Epoch: 4501 mean train loss:  1.38202953e-02, mean val. rec. loss:  1.23311903e-02\n",
      "Epoch: 4502 mean train loss:  1.38191408e-02, mean val. rec. loss:  1.23302287e-02\n",
      "Epoch: 4503 mean train loss:  1.38179890e-02, mean val. rec. loss:  1.23292841e-02\n",
      "Epoch: 4504 mean train loss:  1.38168447e-02, mean val. rec. loss:  1.23283361e-02\n",
      "Epoch: 4505 mean train loss:  1.38157013e-02, mean val. rec. loss:  1.23273893e-02\n",
      "Epoch: 4506 mean train loss:  1.38145551e-02, mean val. rec. loss:  1.23264424e-02\n",
      "Epoch: 4507 mean train loss:  1.38134108e-02, mean val. rec. loss:  1.23254955e-02\n",
      "Epoch: 4508 mean train loss:  1.38122665e-02, mean val. rec. loss:  1.23245498e-02\n",
      "Epoch: 4509 mean train loss:  1.38111240e-02, mean val. rec. loss:  1.23236108e-02\n",
      "Epoch: 4510 mean train loss:  1.38099825e-02, mean val. rec. loss:  1.23226651e-02\n",
      "Epoch: 4511 mean train loss:  1.38088382e-02, mean val. rec. loss:  1.23217194e-02\n",
      "Epoch: 4512 mean train loss:  1.38076939e-02, mean val. rec. loss:  1.23207872e-02\n",
      "Epoch: 4513 mean train loss:  1.38065598e-02, mean val. rec. loss:  1.23198415e-02\n",
      "Epoch: 4514 mean train loss:  1.38054257e-02, mean val. rec. loss:  1.23188980e-02\n",
      "Epoch: 4515 mean train loss:  1.38042805e-02, mean val. rec. loss:  1.23179659e-02\n",
      "Epoch: 4516 mean train loss:  1.38031538e-02, mean val. rec. loss:  1.23170224e-02\n",
      "Epoch: 4517 mean train loss:  1.38020105e-02, mean val. rec. loss:  1.23160926e-02\n",
      "Epoch: 4518 mean train loss:  1.38008848e-02, mean val. rec. loss:  1.23151525e-02\n",
      "Epoch: 4519 mean train loss:  1.37997442e-02, mean val. rec. loss:  1.23142215e-02\n",
      "Epoch: 4520 mean train loss:  1.37986175e-02, mean val. rec. loss:  1.23132883e-02\n",
      "Epoch: 4521 mean train loss:  1.37974891e-02, mean val. rec. loss:  1.23123482e-02\n",
      "Epoch: 4522 mean train loss:  1.37963568e-02, mean val. rec. loss:  1.23114206e-02\n",
      "Epoch: 4523 mean train loss:  1.37952237e-02, mean val. rec. loss:  1.23104930e-02\n",
      "Epoch: 4524 mean train loss:  1.37940999e-02, mean val. rec. loss:  1.23095620e-02\n",
      "Epoch: 4525 mean train loss:  1.37929732e-02, mean val. rec. loss:  1.23086333e-02\n",
      "Epoch: 4526 mean train loss:  1.37918485e-02, mean val. rec. loss:  1.23077057e-02\n",
      "Epoch: 4527 mean train loss:  1.37907237e-02, mean val. rec. loss:  1.23067792e-02\n",
      "Epoch: 4528 mean train loss:  1.37895990e-02, mean val. rec. loss:  1.23058482e-02\n",
      "Epoch: 4529 mean train loss:  1.37884723e-02, mean val. rec. loss:  1.23049206e-02\n",
      "Epoch: 4530 mean train loss:  1.37873550e-02, mean val. rec. loss:  1.23040033e-02\n",
      "Epoch: 4531 mean train loss:  1.37862377e-02, mean val. rec. loss:  1.23030802e-02\n",
      "Epoch: 4532 mean train loss:  1.37851148e-02, mean val. rec. loss:  1.23021537e-02\n",
      "Epoch: 4533 mean train loss:  1.37839910e-02, mean val. rec. loss:  1.23012375e-02\n",
      "Epoch: 4534 mean train loss:  1.37828820e-02, mean val. rec. loss:  1.23003122e-02\n",
      "Epoch: 4535 mean train loss:  1.37817582e-02, mean val. rec. loss:  1.22993959e-02\n",
      "Epoch: 4536 mean train loss:  1.37806493e-02, mean val. rec. loss:  1.22984729e-02\n",
      "Epoch: 4537 mean train loss:  1.37795264e-02, mean val. rec. loss:  1.22975589e-02\n",
      "Epoch: 4538 mean train loss:  1.37784165e-02, mean val. rec. loss:  1.22966403e-02\n",
      "Epoch: 4539 mean train loss:  1.37773066e-02, mean val. rec. loss:  1.22957264e-02\n",
      "Epoch: 4540 mean train loss:  1.37761903e-02, mean val. rec. loss:  1.22948033e-02\n",
      "Epoch: 4541 mean train loss:  1.37750767e-02, mean val. rec. loss:  1.22938916e-02\n",
      "Epoch: 4542 mean train loss:  1.37739705e-02, mean val. rec. loss:  1.22929787e-02\n",
      "Epoch: 4543 mean train loss:  1.37728616e-02, mean val. rec. loss:  1.22920648e-02\n",
      "Epoch: 4544 mean train loss:  1.37717536e-02, mean val. rec. loss:  1.22911553e-02\n",
      "Epoch: 4545 mean train loss:  1.37706465e-02, mean val. rec. loss:  1.22902447e-02\n",
      "Epoch: 4546 mean train loss:  1.37695394e-02, mean val. rec. loss:  1.22893364e-02\n",
      "Epoch: 4547 mean train loss:  1.37684352e-02, mean val. rec. loss:  1.22884213e-02\n",
      "Epoch: 4548 mean train loss:  1.37673272e-02, mean val. rec. loss:  1.22875209e-02\n",
      "Epoch: 4549 mean train loss:  1.37662275e-02, mean val. rec. loss:  1.22866115e-02\n",
      "Epoch: 4550 mean train loss:  1.37651298e-02, mean val. rec. loss:  1.22857065e-02\n",
      "Epoch: 4551 mean train loss:  1.37640236e-02, mean val. rec. loss:  1.22847926e-02\n",
      "Epoch: 4552 mean train loss:  1.37629249e-02, mean val. rec. loss:  1.22838922e-02\n",
      "Epoch: 4553 mean train loss:  1.37618253e-02, mean val. rec. loss:  1.22829941e-02\n",
      "Epoch: 4554 mean train loss:  1.37607266e-02, mean val. rec. loss:  1.22820880e-02\n",
      "Epoch: 4555 mean train loss:  1.37596289e-02, mean val. rec. loss:  1.22811797e-02\n",
      "Epoch: 4556 mean train loss:  1.37585320e-02, mean val. rec. loss:  1.22802805e-02\n",
      "Epoch: 4557 mean train loss:  1.37574333e-02, mean val. rec. loss:  1.22793835e-02\n",
      "Epoch: 4558 mean train loss:  1.37563440e-02, mean val. rec. loss:  1.22784854e-02\n",
      "Epoch: 4559 mean train loss:  1.37552536e-02, mean val. rec. loss:  1.22775941e-02\n",
      "Epoch: 4560 mean train loss:  1.37541578e-02, mean val. rec. loss:  1.22766846e-02\n",
      "Epoch: 4561 mean train loss:  1.37530656e-02, mean val. rec. loss:  1.22757956e-02\n",
      "Epoch: 4562 mean train loss:  1.37519753e-02, mean val. rec. loss:  1.22748895e-02\n",
      "Epoch: 4563 mean train loss:  1.37508850e-02, mean val. rec. loss:  1.22740062e-02\n",
      "Epoch: 4564 mean train loss:  1.37497965e-02, mean val. rec. loss:  1.22731013e-02\n",
      "Epoch: 4565 mean train loss:  1.37487090e-02, mean val. rec. loss:  1.22722145e-02\n",
      "Epoch: 4566 mean train loss:  1.37476270e-02, mean val. rec. loss:  1.22713209e-02\n",
      "Epoch: 4567 mean train loss:  1.37465358e-02, mean val. rec. loss:  1.22704262e-02\n",
      "Epoch: 4568 mean train loss:  1.37454464e-02, mean val. rec. loss:  1.22695474e-02\n",
      "Epoch: 4569 mean train loss:  1.37443673e-02, mean val. rec. loss:  1.22686504e-02\n",
      "Epoch: 4570 mean train loss:  1.37432844e-02, mean val. rec. loss:  1.22677534e-02\n",
      "Epoch: 4571 mean train loss:  1.37422025e-02, mean val. rec. loss:  1.22668712e-02\n",
      "Epoch: 4572 mean train loss:  1.37411206e-02, mean val. rec. loss:  1.22659935e-02\n",
      "Epoch: 4573 mean train loss:  1.37400405e-02, mean val. rec. loss:  1.22651022e-02\n",
      "Epoch: 4574 mean train loss:  1.37389623e-02, mean val. rec. loss:  1.22642177e-02\n",
      "Epoch: 4575 mean train loss:  1.37378878e-02, mean val. rec. loss:  1.22633253e-02\n",
      "Epoch: 4576 mean train loss:  1.37368059e-02, mean val. rec. loss:  1.22624442e-02\n",
      "Epoch: 4577 mean train loss:  1.37357258e-02, mean val. rec. loss:  1.22615653e-02\n",
      "Epoch: 4578 mean train loss:  1.37346532e-02, mean val. rec. loss:  1.22606888e-02\n",
      "Epoch: 4579 mean train loss:  1.37335833e-02, mean val. rec. loss:  1.22598054e-02\n",
      "Epoch: 4580 mean train loss:  1.37325098e-02, mean val. rec. loss:  1.22589266e-02\n",
      "Epoch: 4581 mean train loss:  1.37314372e-02, mean val. rec. loss:  1.22580466e-02\n",
      "Epoch: 4582 mean train loss:  1.37303655e-02, mean val. rec. loss:  1.22571700e-02\n",
      "Epoch: 4583 mean train loss:  1.37292938e-02, mean val. rec. loss:  1.22562889e-02\n",
      "Epoch: 4584 mean train loss:  1.37282212e-02, mean val. rec. loss:  1.22554101e-02\n",
      "Epoch: 4585 mean train loss:  1.37271476e-02, mean val. rec. loss:  1.22545335e-02\n",
      "Epoch: 4586 mean train loss:  1.37260852e-02, mean val. rec. loss:  1.22536660e-02\n",
      "Epoch: 4587 mean train loss:  1.37250210e-02, mean val. rec. loss:  1.22527906e-02\n",
      "Epoch: 4588 mean train loss:  1.37239493e-02, mean val. rec. loss:  1.22519220e-02\n",
      "Epoch: 4589 mean train loss:  1.37228851e-02, mean val. rec. loss:  1.22510420e-02\n",
      "Epoch: 4590 mean train loss:  1.37218208e-02, mean val. rec. loss:  1.22501655e-02\n",
      "Epoch: 4591 mean train loss:  1.37207575e-02, mean val. rec. loss:  1.22493002e-02\n",
      "Epoch: 4592 mean train loss:  1.37196942e-02, mean val. rec. loss:  1.22484305e-02\n",
      "Epoch: 4593 mean train loss:  1.37186374e-02, mean val. rec. loss:  1.22475528e-02\n",
      "Epoch: 4594 mean train loss:  1.37175722e-02, mean val. rec. loss:  1.22466898e-02\n",
      "Epoch: 4595 mean train loss:  1.37165089e-02, mean val. rec. loss:  1.22458269e-02\n",
      "Epoch: 4596 mean train loss:  1.37154540e-02, mean val. rec. loss:  1.22449594e-02\n",
      "Epoch: 4597 mean train loss:  1.37143991e-02, mean val. rec. loss:  1.22440964e-02\n",
      "Epoch: 4598 mean train loss:  1.37133432e-02, mean val. rec. loss:  1.22432323e-02\n",
      "Epoch: 4599 mean train loss:  1.37122864e-02, mean val. rec. loss:  1.22423671e-02\n",
      "Epoch: 4600 mean train loss:  1.37112315e-02, mean val. rec. loss:  1.22415041e-02\n",
      "Epoch: 4601 mean train loss:  1.37101775e-02, mean val. rec. loss:  1.22406400e-02\n",
      "Epoch: 4602 mean train loss:  1.37091197e-02, mean val. rec. loss:  1.22397737e-02\n",
      "Epoch: 4603 mean train loss:  1.37080648e-02, mean val. rec. loss:  1.22389096e-02\n",
      "Epoch: 4604 mean train loss:  1.37070173e-02, mean val. rec. loss:  1.22380591e-02\n",
      "Epoch: 4605 mean train loss:  1.37059698e-02, mean val. rec. loss:  1.22371996e-02\n",
      "Epoch: 4606 mean train loss:  1.37049168e-02, mean val. rec. loss:  1.22363343e-02\n",
      "Epoch: 4607 mean train loss:  1.37038674e-02, mean val. rec. loss:  1.22354827e-02\n",
      "Epoch: 4608 mean train loss:  1.37028200e-02, mean val. rec. loss:  1.22346300e-02\n",
      "Epoch: 4609 mean train loss:  1.37017725e-02, mean val. rec. loss:  1.22337682e-02\n",
      "Epoch: 4610 mean train loss:  1.37007259e-02, mean val. rec. loss:  1.22329131e-02\n",
      "Epoch: 4611 mean train loss:  1.36996850e-02, mean val. rec. loss:  1.22320513e-02\n",
      "Epoch: 4612 mean train loss:  1.36986365e-02, mean val. rec. loss:  1.22311997e-02\n",
      "Epoch: 4613 mean train loss:  1.36975881e-02, mean val. rec. loss:  1.22303503e-02\n",
      "Epoch: 4614 mean train loss:  1.36965509e-02, mean val. rec. loss:  1.22295021e-02\n",
      "Epoch: 4615 mean train loss:  1.36955136e-02, mean val. rec. loss:  1.22286505e-02\n",
      "Epoch: 4616 mean train loss:  1.36944745e-02, mean val. rec. loss:  1.22277989e-02\n",
      "Epoch: 4617 mean train loss:  1.36934336e-02, mean val. rec. loss:  1.22269507e-02\n",
      "Epoch: 4618 mean train loss:  1.36923945e-02, mean val. rec. loss:  1.22261036e-02\n",
      "Epoch: 4619 mean train loss:  1.36913563e-02, mean val. rec. loss:  1.22252543e-02\n",
      "Epoch: 4620 mean train loss:  1.36903153e-02, mean val. rec. loss:  1.22244038e-02\n",
      "Epoch: 4621 mean train loss:  1.36892753e-02, mean val. rec. loss:  1.22235556e-02\n",
      "Epoch: 4622 mean train loss:  1.36882455e-02, mean val. rec. loss:  1.22227187e-02\n",
      "Epoch: 4623 mean train loss:  1.36872139e-02, mean val. rec. loss:  1.22218739e-02\n",
      "Epoch: 4624 mean train loss:  1.36861766e-02, mean val. rec. loss:  1.22210347e-02\n",
      "Epoch: 4625 mean train loss:  1.36851431e-02, mean val. rec. loss:  1.22201854e-02\n",
      "Epoch: 4626 mean train loss:  1.36841115e-02, mean val. rec. loss:  1.22193394e-02\n",
      "Epoch: 4627 mean train loss:  1.36830817e-02, mean val. rec. loss:  1.22185048e-02\n",
      "Epoch: 4628 mean train loss:  1.36820509e-02, mean val. rec. loss:  1.22176691e-02\n",
      "Epoch: 4629 mean train loss:  1.36810202e-02, mean val. rec. loss:  1.22168209e-02\n",
      "Epoch: 4630 mean train loss:  1.36799904e-02, mean val. rec. loss:  1.22159851e-02\n",
      "Epoch: 4631 mean train loss:  1.36789644e-02, mean val. rec. loss:  1.22151494e-02\n",
      "Epoch: 4632 mean train loss:  1.36779429e-02, mean val. rec. loss:  1.22143091e-02\n",
      "Epoch: 4633 mean train loss:  1.36769141e-02, mean val. rec. loss:  1.22134836e-02\n",
      "Epoch: 4634 mean train loss:  1.36758890e-02, mean val. rec. loss:  1.22126354e-02\n",
      "Epoch: 4635 mean train loss:  1.36748657e-02, mean val. rec. loss:  1.22118132e-02\n",
      "Epoch: 4636 mean train loss:  1.36738415e-02, mean val. rec. loss:  1.22109684e-02\n",
      "Epoch: 4637 mean train loss:  1.36728191e-02, mean val. rec. loss:  1.22101486e-02\n",
      "Epoch: 4638 mean train loss:  1.36717977e-02, mean val. rec. loss:  1.22093105e-02\n",
      "Epoch: 4639 mean train loss:  1.36707810e-02, mean val. rec. loss:  1.22084782e-02\n",
      "Epoch: 4640 mean train loss:  1.36697586e-02, mean val. rec. loss:  1.22076459e-02\n",
      "Epoch: 4641 mean train loss:  1.36687363e-02, mean val. rec. loss:  1.22068124e-02\n",
      "Epoch: 4642 mean train loss:  1.36677205e-02, mean val. rec. loss:  1.22059880e-02\n",
      "Epoch: 4643 mean train loss:  1.36667037e-02, mean val. rec. loss:  1.22051647e-02\n",
      "Epoch: 4644 mean train loss:  1.36656888e-02, mean val. rec. loss:  1.22043324e-02\n",
      "Epoch: 4645 mean train loss:  1.36646720e-02, mean val. rec. loss:  1.22035023e-02\n",
      "Epoch: 4646 mean train loss:  1.36636590e-02, mean val. rec. loss:  1.22026813e-02\n",
      "Epoch: 4647 mean train loss:  1.36626451e-02, mean val. rec. loss:  1.22018569e-02\n",
      "Epoch: 4648 mean train loss:  1.36616367e-02, mean val. rec. loss:  1.22010370e-02\n",
      "Epoch: 4649 mean train loss:  1.36606218e-02, mean val. rec. loss:  1.22002024e-02\n",
      "Epoch: 4650 mean train loss:  1.36596050e-02, mean val. rec. loss:  1.21993837e-02\n",
      "Epoch: 4651 mean train loss:  1.36585976e-02, mean val. rec. loss:  1.21985593e-02\n",
      "Epoch: 4652 mean train loss:  1.36575883e-02, mean val. rec. loss:  1.21977394e-02\n",
      "Epoch: 4653 mean train loss:  1.36565799e-02, mean val. rec. loss:  1.21969184e-02\n",
      "Epoch: 4654 mean train loss:  1.36555725e-02, mean val. rec. loss:  1.21960974e-02\n",
      "Epoch: 4655 mean train loss:  1.36545659e-02, mean val. rec. loss:  1.21952810e-02\n",
      "Epoch: 4656 mean train loss:  1.36535604e-02, mean val. rec. loss:  1.21944577e-02\n",
      "Epoch: 4657 mean train loss:  1.36525510e-02, mean val. rec. loss:  1.21936481e-02\n",
      "Epoch: 4658 mean train loss:  1.36515511e-02, mean val. rec. loss:  1.21928282e-02\n",
      "Epoch: 4659 mean train loss:  1.36505520e-02, mean val. rec. loss:  1.21920117e-02\n",
      "Epoch: 4660 mean train loss:  1.36495436e-02, mean val. rec. loss:  1.21911885e-02\n",
      "Epoch: 4661 mean train loss:  1.36485436e-02, mean val. rec. loss:  1.21903777e-02\n",
      "Epoch: 4662 mean train loss:  1.36475418e-02, mean val. rec. loss:  1.21895691e-02\n",
      "Epoch: 4663 mean train loss:  1.36465418e-02, mean val. rec. loss:  1.21887515e-02\n",
      "Epoch: 4664 mean train loss:  1.36455436e-02, mean val. rec. loss:  1.21879362e-02\n",
      "Epoch: 4665 mean train loss:  1.36445446e-02, mean val. rec. loss:  1.21871232e-02\n",
      "Epoch: 4666 mean train loss:  1.36435418e-02, mean val. rec. loss:  1.21863158e-02\n",
      "Epoch: 4667 mean train loss:  1.36425492e-02, mean val. rec. loss:  1.21855061e-02\n",
      "Epoch: 4668 mean train loss:  1.36415567e-02, mean val. rec. loss:  1.21847021e-02\n",
      "Epoch: 4669 mean train loss:  1.36405576e-02, mean val. rec. loss:  1.21838811e-02\n",
      "Epoch: 4670 mean train loss:  1.36395641e-02, mean val. rec. loss:  1.21830851e-02\n",
      "Epoch: 4671 mean train loss:  1.36385697e-02, mean val. rec. loss:  1.21822629e-02\n",
      "Epoch: 4672 mean train loss:  1.36375790e-02, mean val. rec. loss:  1.21814691e-02\n",
      "Epoch: 4673 mean train loss:  1.36365865e-02, mean val. rec. loss:  1.21806538e-02\n",
      "Epoch: 4674 mean train loss:  1.36355958e-02, mean val. rec. loss:  1.21798566e-02\n",
      "Epoch: 4675 mean train loss:  1.36346088e-02, mean val. rec. loss:  1.21790492e-02\n",
      "Epoch: 4676 mean train loss:  1.36336163e-02, mean val. rec. loss:  1.21782441e-02\n",
      "Epoch: 4677 mean train loss:  1.36326237e-02, mean val. rec. loss:  1.21774503e-02\n",
      "Epoch: 4678 mean train loss:  1.36316396e-02, mean val. rec. loss:  1.21766418e-02\n",
      "Epoch: 4679 mean train loss:  1.36306536e-02, mean val. rec. loss:  1.21758333e-02\n",
      "Epoch: 4680 mean train loss:  1.36296675e-02, mean val. rec. loss:  1.21750372e-02\n",
      "Epoch: 4681 mean train loss:  1.36286815e-02, mean val. rec. loss:  1.21742435e-02\n",
      "Epoch: 4682 mean train loss:  1.36276964e-02, mean val. rec. loss:  1.21734406e-02\n",
      "Epoch: 4683 mean train loss:  1.36267150e-02, mean val. rec. loss:  1.21726411e-02\n",
      "Epoch: 4684 mean train loss:  1.36257355e-02, mean val. rec. loss:  1.21718372e-02\n",
      "Epoch: 4685 mean train loss:  1.36247495e-02, mean val. rec. loss:  1.21710411e-02\n",
      "Epoch: 4686 mean train loss:  1.36237634e-02, mean val. rec. loss:  1.21702530e-02\n",
      "Epoch: 4687 mean train loss:  1.36227877e-02, mean val. rec. loss:  1.21694535e-02\n",
      "Epoch: 4688 mean train loss:  1.36218081e-02, mean val. rec. loss:  1.21686597e-02\n",
      "Epoch: 4689 mean train loss:  1.36208296e-02, mean val. rec. loss:  1.21678660e-02\n",
      "Epoch: 4690 mean train loss:  1.36198519e-02, mean val. rec. loss:  1.21670722e-02\n",
      "Epoch: 4691 mean train loss:  1.36188752e-02, mean val. rec. loss:  1.21662818e-02\n",
      "Epoch: 4692 mean train loss:  1.36178985e-02, mean val. rec. loss:  1.21654823e-02\n",
      "Epoch: 4693 mean train loss:  1.36169180e-02, mean val. rec. loss:  1.21646886e-02\n",
      "Epoch: 4694 mean train loss:  1.36159488e-02, mean val. rec. loss:  1.21639050e-02\n",
      "Epoch: 4695 mean train loss:  1.36149786e-02, mean val. rec. loss:  1.21631146e-02\n",
      "Epoch: 4696 mean train loss:  1.36140000e-02, mean val. rec. loss:  1.21623299e-02\n",
      "Epoch: 4697 mean train loss:  1.36130288e-02, mean val. rec. loss:  1.21615350e-02\n",
      "Epoch: 4698 mean train loss:  1.36120577e-02, mean val. rec. loss:  1.21607423e-02\n",
      "Epoch: 4699 mean train loss:  1.36110875e-02, mean val. rec. loss:  1.21599599e-02\n",
      "Epoch: 4700 mean train loss:  1.36101173e-02, mean val. rec. loss:  1.21591808e-02\n",
      "Epoch: 4701 mean train loss:  1.36091480e-02, mean val. rec. loss:  1.21583848e-02\n",
      "Epoch: 4702 mean train loss:  1.36081769e-02, mean val. rec. loss:  1.21576012e-02\n",
      "Epoch: 4703 mean train loss:  1.36072123e-02, mean val. rec. loss:  1.21568188e-02\n",
      "Epoch: 4704 mean train loss:  1.36062486e-02, mean val. rec. loss:  1.21560307e-02\n",
      "Epoch: 4705 mean train loss:  1.36052803e-02, mean val. rec. loss:  1.21552584e-02\n",
      "Epoch: 4706 mean train loss:  1.36043147e-02, mean val. rec. loss:  1.21544624e-02\n",
      "Epoch: 4707 mean train loss:  1.36033501e-02, mean val. rec. loss:  1.21536935e-02\n",
      "Epoch: 4708 mean train loss:  1.36023874e-02, mean val. rec. loss:  1.21529009e-02\n",
      "Epoch: 4709 mean train loss:  1.36014246e-02, mean val. rec. loss:  1.21521332e-02\n",
      "Epoch: 4710 mean train loss:  1.36004619e-02, mean val. rec. loss:  1.21513485e-02\n",
      "Epoch: 4711 mean train loss:  1.35995047e-02, mean val. rec. loss:  1.21505706e-02\n",
      "Epoch: 4712 mean train loss:  1.35985419e-02, mean val. rec. loss:  1.21497904e-02\n",
      "Epoch: 4713 mean train loss:  1.35975783e-02, mean val. rec. loss:  1.21490125e-02\n",
      "Epoch: 4714 mean train loss:  1.35966239e-02, mean val. rec. loss:  1.21482414e-02\n",
      "Epoch: 4715 mean train loss:  1.35956649e-02, mean val. rec. loss:  1.21474703e-02\n",
      "Epoch: 4716 mean train loss:  1.35947068e-02, mean val. rec. loss:  1.21466890e-02\n",
      "Epoch: 4717 mean train loss:  1.35937515e-02, mean val. rec. loss:  1.21459099e-02\n",
      "Epoch: 4718 mean train loss:  1.35927971e-02, mean val. rec. loss:  1.21451422e-02\n",
      "Epoch: 4719 mean train loss:  1.35918418e-02, mean val. rec. loss:  1.21443711e-02\n",
      "Epoch: 4720 mean train loss:  1.35908921e-02, mean val. rec. loss:  1.21436023e-02\n",
      "Epoch: 4721 mean train loss:  1.35899340e-02, mean val. rec. loss:  1.21428232e-02\n",
      "Epoch: 4722 mean train loss:  1.35889777e-02, mean val. rec. loss:  1.21420567e-02\n",
      "Epoch: 4723 mean train loss:  1.35880299e-02, mean val. rec. loss:  1.21412844e-02\n",
      "Epoch: 4724 mean train loss:  1.35870792e-02, mean val. rec. loss:  1.21405145e-02\n",
      "Epoch: 4725 mean train loss:  1.35861295e-02, mean val. rec. loss:  1.21397467e-02\n",
      "Epoch: 4726 mean train loss:  1.35851789e-02, mean val. rec. loss:  1.21389813e-02\n",
      "Epoch: 4727 mean train loss:  1.35842310e-02, mean val. rec. loss:  1.21382125e-02\n",
      "Epoch: 4728 mean train loss:  1.35832831e-02, mean val. rec. loss:  1.21374414e-02\n",
      "Epoch: 4729 mean train loss:  1.35823325e-02, mean val. rec. loss:  1.21366861e-02\n",
      "Epoch: 4730 mean train loss:  1.35813893e-02, mean val. rec. loss:  1.21359139e-02\n",
      "Epoch: 4731 mean train loss:  1.35804489e-02, mean val. rec. loss:  1.21351485e-02\n",
      "Epoch: 4732 mean train loss:  1.35794992e-02, mean val. rec. loss:  1.21343785e-02\n",
      "Epoch: 4733 mean train loss:  1.35785550e-02, mean val. rec. loss:  1.21336199e-02\n",
      "Epoch: 4734 mean train loss:  1.35776109e-02, mean val. rec. loss:  1.21328646e-02\n",
      "Epoch: 4735 mean train loss:  1.35766686e-02, mean val. rec. loss:  1.21320958e-02\n",
      "Epoch: 4736 mean train loss:  1.35757264e-02, mean val. rec. loss:  1.21313304e-02\n",
      "Epoch: 4737 mean train loss:  1.35747860e-02, mean val. rec. loss:  1.21305672e-02\n",
      "Epoch: 4738 mean train loss:  1.35738418e-02, mean val. rec. loss:  1.21298097e-02\n",
      "Epoch: 4739 mean train loss:  1.35729061e-02, mean val. rec. loss:  1.21290545e-02\n",
      "Epoch: 4740 mean train loss:  1.35719703e-02, mean val. rec. loss:  1.21283015e-02\n",
      "Epoch: 4741 mean train loss:  1.35710290e-02, mean val. rec. loss:  1.21275293e-02\n",
      "Epoch: 4742 mean train loss:  1.35700923e-02, mean val. rec. loss:  1.21267843e-02\n",
      "Epoch: 4743 mean train loss:  1.35691556e-02, mean val. rec. loss:  1.21260132e-02\n",
      "Epoch: 4744 mean train loss:  1.35682199e-02, mean val. rec. loss:  1.21252693e-02\n",
      "Epoch: 4745 mean train loss:  1.35672841e-02, mean val. rec. loss:  1.21245027e-02\n",
      "Epoch: 4746 mean train loss:  1.35663503e-02, mean val. rec. loss:  1.21237531e-02\n",
      "Epoch: 4747 mean train loss:  1.35654192e-02, mean val. rec. loss:  1.21229979e-02\n",
      "Epoch: 4748 mean train loss:  1.35644834e-02, mean val. rec. loss:  1.21222393e-02\n",
      "Epoch: 4749 mean train loss:  1.35635477e-02, mean val. rec. loss:  1.21214954e-02\n",
      "Epoch: 4750 mean train loss:  1.35626194e-02, mean val. rec. loss:  1.21207379e-02\n",
      "Epoch: 4751 mean train loss:  1.35616901e-02, mean val. rec. loss:  1.21199781e-02\n",
      "Epoch: 4752 mean train loss:  1.35607600e-02, mean val. rec. loss:  1.21192331e-02\n",
      "Epoch: 4753 mean train loss:  1.35598307e-02, mean val. rec. loss:  1.21184892e-02\n",
      "Epoch: 4754 mean train loss:  1.35589024e-02, mean val. rec. loss:  1.21177351e-02\n",
      "Epoch: 4755 mean train loss:  1.35579741e-02, mean val. rec. loss:  1.21169844e-02\n",
      "Epoch: 4756 mean train loss:  1.35570514e-02, mean val. rec. loss:  1.21162292e-02\n",
      "Epoch: 4757 mean train loss:  1.35561212e-02, mean val. rec. loss:  1.21154831e-02\n",
      "Epoch: 4758 mean train loss:  1.35551920e-02, mean val. rec. loss:  1.21147414e-02\n",
      "Epoch: 4759 mean train loss:  1.35542712e-02, mean val. rec. loss:  1.21139953e-02\n",
      "Epoch: 4760 mean train loss:  1.35533466e-02, mean val. rec. loss:  1.21132491e-02\n",
      "Epoch: 4761 mean train loss:  1.35524248e-02, mean val. rec. loss:  1.21125052e-02\n",
      "Epoch: 4762 mean train loss:  1.35515021e-02, mean val. rec. loss:  1.21117591e-02\n",
      "Epoch: 4763 mean train loss:  1.35505794e-02, mean val. rec. loss:  1.21110152e-02\n",
      "Epoch: 4764 mean train loss:  1.35496576e-02, mean val. rec. loss:  1.21102690e-02\n",
      "Epoch: 4765 mean train loss:  1.35487358e-02, mean val. rec. loss:  1.21095229e-02\n",
      "Epoch: 4766 mean train loss:  1.35478187e-02, mean val. rec. loss:  1.21087881e-02\n",
      "Epoch: 4767 mean train loss:  1.35469043e-02, mean val. rec. loss:  1.21080464e-02\n",
      "Epoch: 4768 mean train loss:  1.35459816e-02, mean val. rec. loss:  1.21073071e-02\n",
      "Epoch: 4769 mean train loss:  1.35450617e-02, mean val. rec. loss:  1.21065632e-02\n",
      "Epoch: 4770 mean train loss:  1.35441474e-02, mean val. rec. loss:  1.21058170e-02\n",
      "Epoch: 4771 mean train loss:  1.35432321e-02, mean val. rec. loss:  1.21050845e-02\n",
      "Epoch: 4772 mean train loss:  1.35423168e-02, mean val. rec. loss:  1.21043519e-02\n",
      "Epoch: 4773 mean train loss:  1.35414006e-02, mean val. rec. loss:  1.21036035e-02\n",
      "Epoch: 4774 mean train loss:  1.35404844e-02, mean val. rec. loss:  1.21028687e-02\n",
      "Epoch: 4775 mean train loss:  1.35395747e-02, mean val. rec. loss:  1.21021316e-02\n",
      "Epoch: 4776 mean train loss:  1.35386660e-02, mean val. rec. loss:  1.21013911e-02\n",
      "Epoch: 4777 mean train loss:  1.35377498e-02, mean val. rec. loss:  1.21006654e-02\n",
      "Epoch: 4778 mean train loss:  1.35368392e-02, mean val. rec. loss:  1.20999192e-02\n",
      "Epoch: 4779 mean train loss:  1.35359295e-02, mean val. rec. loss:  1.20991969e-02\n",
      "Epoch: 4780 mean train loss:  1.35350189e-02, mean val. rec. loss:  1.20984541e-02\n",
      "Epoch: 4781 mean train loss:  1.35341111e-02, mean val. rec. loss:  1.20977340e-02\n",
      "Epoch: 4782 mean train loss:  1.35332014e-02, mean val. rec. loss:  1.20969947e-02\n",
      "Epoch: 4783 mean train loss:  1.35322982e-02, mean val. rec. loss:  1.20962644e-02\n",
      "Epoch: 4784 mean train loss:  1.35313876e-02, mean val. rec. loss:  1.20955296e-02\n",
      "Epoch: 4785 mean train loss:  1.35304770e-02, mean val. rec. loss:  1.20947971e-02\n",
      "Epoch: 4786 mean train loss:  1.35295757e-02, mean val. rec. loss:  1.20940713e-02\n",
      "Epoch: 4787 mean train loss:  1.35286716e-02, mean val. rec. loss:  1.20933490e-02\n",
      "Epoch: 4788 mean train loss:  1.35277675e-02, mean val. rec. loss:  1.20926175e-02\n",
      "Epoch: 4789 mean train loss:  1.35268644e-02, mean val. rec. loss:  1.20918827e-02\n",
      "Epoch: 4790 mean train loss:  1.35259612e-02, mean val. rec. loss:  1.20911615e-02\n",
      "Epoch: 4791 mean train loss:  1.35250599e-02, mean val. rec. loss:  1.20904369e-02\n",
      "Epoch: 4792 mean train loss:  1.35241642e-02, mean val. rec. loss:  1.20897123e-02\n",
      "Epoch: 4793 mean train loss:  1.35232573e-02, mean val. rec. loss:  1.20889763e-02\n",
      "Epoch: 4794 mean train loss:  1.35223532e-02, mean val. rec. loss:  1.20882585e-02\n",
      "Epoch: 4795 mean train loss:  1.35214594e-02, mean val. rec. loss:  1.20875328e-02\n",
      "Epoch: 4796 mean train loss:  1.35205599e-02, mean val. rec. loss:  1.20868082e-02\n",
      "Epoch: 4797 mean train loss:  1.35196623e-02, mean val. rec. loss:  1.20860836e-02\n",
      "Epoch: 4798 mean train loss:  1.35187657e-02, mean val. rec. loss:  1.20853612e-02\n",
      "Epoch: 4799 mean train loss:  1.35178700e-02, mean val. rec. loss:  1.20846344e-02\n",
      "Epoch: 4800 mean train loss:  1.35169705e-02, mean val. rec. loss:  1.20839211e-02\n",
      "Epoch: 4801 mean train loss:  1.35160786e-02, mean val. rec. loss:  1.20831942e-02\n",
      "Epoch: 4802 mean train loss:  1.35151884e-02, mean val. rec. loss:  1.20824730e-02\n",
      "Epoch: 4803 mean train loss:  1.35142909e-02, mean val. rec. loss:  1.20817518e-02\n",
      "Epoch: 4804 mean train loss:  1.35133951e-02, mean val. rec. loss:  1.20810374e-02\n",
      "Epoch: 4805 mean train loss:  1.35125106e-02, mean val. rec. loss:  1.20803128e-02\n",
      "Epoch: 4806 mean train loss:  1.35116093e-02, mean val. rec. loss:  1.20795995e-02\n",
      "Epoch: 4807 mean train loss:  1.35107266e-02, mean val. rec. loss:  1.20788783e-02\n",
      "Epoch: 4808 mean train loss:  1.35098290e-02, mean val. rec. loss:  1.20781650e-02\n",
      "Epoch: 4809 mean train loss:  1.35089445e-02, mean val. rec. loss:  1.20774483e-02\n",
      "Epoch: 4810 mean train loss:  1.35080581e-02, mean val. rec. loss:  1.20767260e-02\n",
      "Epoch: 4811 mean train loss:  1.35071661e-02, mean val. rec. loss:  1.20760127e-02\n",
      "Epoch: 4812 mean train loss:  1.35062741e-02, mean val. rec. loss:  1.20753040e-02\n",
      "Epoch: 4813 mean train loss:  1.35053914e-02, mean val. rec. loss:  1.20745896e-02\n",
      "Epoch: 4814 mean train loss:  1.35045069e-02, mean val. rec. loss:  1.20738752e-02\n",
      "Epoch: 4815 mean train loss:  1.35036205e-02, mean val. rec. loss:  1.20731585e-02\n",
      "Epoch: 4816 mean train loss:  1.35027350e-02, mean val. rec. loss:  1.20724464e-02\n",
      "Epoch: 4817 mean train loss:  1.35018496e-02, mean val. rec. loss:  1.20717274e-02\n",
      "Epoch: 4818 mean train loss:  1.35009613e-02, mean val. rec. loss:  1.20710176e-02\n",
      "Epoch: 4819 mean train loss:  1.35000833e-02, mean val. rec. loss:  1.20703122e-02\n",
      "Epoch: 4820 mean train loss:  1.34992052e-02, mean val. rec. loss:  1.20696024e-02\n",
      "Epoch: 4821 mean train loss:  1.34983198e-02, mean val. rec. loss:  1.20688891e-02\n",
      "Epoch: 4822 mean train loss:  1.34974352e-02, mean val. rec. loss:  1.20681838e-02\n",
      "Epoch: 4823 mean train loss:  1.34965628e-02, mean val. rec. loss:  1.20674682e-02\n",
      "Epoch: 4824 mean train loss:  1.34956745e-02, mean val. rec. loss:  1.20667674e-02\n",
      "Epoch: 4825 mean train loss:  1.34948049e-02, mean val. rec. loss:  1.20660530e-02\n",
      "Epoch: 4826 mean train loss:  1.34939166e-02, mean val. rec. loss:  1.20653522e-02\n",
      "Epoch: 4827 mean train loss:  1.34930442e-02, mean val. rec. loss:  1.20646446e-02\n",
      "Epoch: 4828 mean train loss:  1.34921708e-02, mean val. rec. loss:  1.20639449e-02\n",
      "Epoch: 4829 mean train loss:  1.34912900e-02, mean val. rec. loss:  1.20632294e-02\n",
      "Epoch: 4830 mean train loss:  1.34904111e-02, mean val. rec. loss:  1.20625309e-02\n",
      "Epoch: 4831 mean train loss:  1.34895405e-02, mean val. rec. loss:  1.20618255e-02\n",
      "Epoch: 4832 mean train loss:  1.34886652e-02, mean val. rec. loss:  1.20611213e-02\n",
      "Epoch: 4833 mean train loss:  1.34877919e-02, mean val. rec. loss:  1.20604194e-02\n",
      "Epoch: 4834 mean train loss:  1.34869185e-02, mean val. rec. loss:  1.20597186e-02\n",
      "Epoch: 4835 mean train loss:  1.34860470e-02, mean val. rec. loss:  1.20590110e-02\n",
      "Epoch: 4836 mean train loss:  1.34851708e-02, mean val. rec. loss:  1.20583170e-02\n",
      "Epoch: 4837 mean train loss:  1.34843021e-02, mean val. rec. loss:  1.20576117e-02\n",
      "Epoch: 4838 mean train loss:  1.34834381e-02, mean val. rec. loss:  1.20569109e-02\n",
      "Epoch: 4839 mean train loss:  1.34825629e-02, mean val. rec. loss:  1.20562078e-02\n",
      "Epoch: 4840 mean train loss:  1.34816895e-02, mean val. rec. loss:  1.20555138e-02\n",
      "Epoch: 4841 mean train loss:  1.34808301e-02, mean val. rec. loss:  1.20548096e-02\n",
      "Epoch: 4842 mean train loss:  1.34799530e-02, mean val. rec. loss:  1.20541168e-02\n",
      "Epoch: 4843 mean train loss:  1.34790936e-02, mean val. rec. loss:  1.20534171e-02\n",
      "Epoch: 4844 mean train loss:  1.34782193e-02, mean val. rec. loss:  1.20527208e-02\n",
      "Epoch: 4845 mean train loss:  1.34773571e-02, mean val. rec. loss:  1.20520235e-02\n",
      "Epoch: 4846 mean train loss:  1.34764968e-02, mean val. rec. loss:  1.20513227e-02\n",
      "Epoch: 4847 mean train loss:  1.34756281e-02, mean val. rec. loss:  1.20506287e-02\n",
      "Epoch: 4848 mean train loss:  1.34747584e-02, mean val. rec. loss:  1.20499392e-02\n",
      "Epoch: 4849 mean train loss:  1.34738999e-02, mean val. rec. loss:  1.20492452e-02\n",
      "Epoch: 4850 mean train loss:  1.34730377e-02, mean val. rec. loss:  1.20485478e-02\n",
      "Epoch: 4851 mean train loss:  1.34721737e-02, mean val. rec. loss:  1.20478572e-02\n",
      "Epoch: 4852 mean train loss:  1.34713134e-02, mean val. rec. loss:  1.20471655e-02\n",
      "Epoch: 4853 mean train loss:  1.34704530e-02, mean val. rec. loss:  1.20464692e-02\n",
      "Epoch: 4854 mean train loss:  1.34695880e-02, mean val. rec. loss:  1.20457752e-02\n",
      "Epoch: 4855 mean train loss:  1.34687324e-02, mean val. rec. loss:  1.20450915e-02\n",
      "Epoch: 4856 mean train loss:  1.34678776e-02, mean val. rec. loss:  1.20444031e-02\n",
      "Epoch: 4857 mean train loss:  1.34670154e-02, mean val. rec. loss:  1.20437091e-02\n",
      "Epoch: 4858 mean train loss:  1.34661532e-02, mean val. rec. loss:  1.20430265e-02\n",
      "Epoch: 4859 mean train loss:  1.34653041e-02, mean val. rec. loss:  1.20423325e-02\n",
      "Epoch: 4860 mean train loss:  1.34644400e-02, mean val. rec. loss:  1.20416487e-02\n",
      "Epoch: 4861 mean train loss:  1.34635918e-02, mean val. rec. loss:  1.20409558e-02\n",
      "Epoch: 4862 mean train loss:  1.34627287e-02, mean val. rec. loss:  1.20402721e-02\n",
      "Epoch: 4863 mean train loss:  1.34618776e-02, mean val. rec. loss:  1.20395837e-02\n",
      "Epoch: 4864 mean train loss:  1.34610266e-02, mean val. rec. loss:  1.20389022e-02\n",
      "Epoch: 4865 mean train loss:  1.34601700e-02, mean val. rec. loss:  1.20382094e-02\n",
      "Epoch: 4866 mean train loss:  1.34593134e-02, mean val. rec. loss:  1.20375301e-02\n",
      "Epoch: 4867 mean train loss:  1.34584661e-02, mean val. rec. loss:  1.20368440e-02\n",
      "Epoch: 4868 mean train loss:  1.34576151e-02, mean val. rec. loss:  1.20361569e-02\n",
      "Epoch: 4869 mean train loss:  1.34567622e-02, mean val. rec. loss:  1.20354742e-02\n",
      "Epoch: 4870 mean train loss:  1.34559130e-02, mean val. rec. loss:  1.20347915e-02\n",
      "Epoch: 4871 mean train loss:  1.34550620e-02, mean val. rec. loss:  1.20341066e-02\n",
      "Epoch: 4872 mean train loss:  1.34542101e-02, mean val. rec. loss:  1.20334330e-02\n",
      "Epoch: 4873 mean train loss:  1.34533656e-02, mean val. rec. loss:  1.20327481e-02\n",
      "Epoch: 4874 mean train loss:  1.34525211e-02, mean val. rec. loss:  1.20320655e-02\n",
      "Epoch: 4875 mean train loss:  1.34516691e-02, mean val. rec. loss:  1.20313805e-02\n",
      "Epoch: 4876 mean train loss:  1.34508181e-02, mean val. rec. loss:  1.20307047e-02\n",
      "Epoch: 4877 mean train loss:  1.34499801e-02, mean val. rec. loss:  1.20300186e-02\n",
      "Epoch: 4878 mean train loss:  1.34491272e-02, mean val. rec. loss:  1.20293462e-02\n",
      "Epoch: 4879 mean train loss:  1.34482902e-02, mean val. rec. loss:  1.20286635e-02\n",
      "Epoch: 4880 mean train loss:  1.34474373e-02, mean val. rec. loss:  1.20279877e-02\n",
      "Epoch: 4881 mean train loss:  1.34465984e-02, mean val. rec. loss:  1.20273096e-02\n",
      "Epoch: 4882 mean train loss:  1.34457604e-02, mean val. rec. loss:  1.20266269e-02\n",
      "Epoch: 4883 mean train loss:  1.34449131e-02, mean val. rec. loss:  1.20259499e-02\n",
      "Epoch: 4884 mean train loss:  1.34440658e-02, mean val. rec. loss:  1.20252797e-02\n",
      "Epoch: 4885 mean train loss:  1.34432297e-02, mean val. rec. loss:  1.20246028e-02\n",
      "Epoch: 4886 mean train loss:  1.34423880e-02, mean val. rec. loss:  1.20239280e-02\n",
      "Epoch: 4887 mean train loss:  1.34415490e-02, mean val. rec. loss:  1.20232533e-02\n",
      "Epoch: 4888 mean train loss:  1.34407083e-02, mean val. rec. loss:  1.20225820e-02\n",
      "Epoch: 4889 mean train loss:  1.34398703e-02, mean val. rec. loss:  1.20219050e-02\n",
      "Epoch: 4890 mean train loss:  1.34390286e-02, mean val. rec. loss:  1.20212303e-02\n",
      "Epoch: 4891 mean train loss:  1.34381943e-02, mean val. rec. loss:  1.20205624e-02\n",
      "Epoch: 4892 mean train loss:  1.34373600e-02, mean val. rec. loss:  1.20198888e-02\n",
      "Epoch: 4893 mean train loss:  1.34365202e-02, mean val. rec. loss:  1.20192141e-02\n",
      "Epoch: 4894 mean train loss:  1.34356803e-02, mean val. rec. loss:  1.20185462e-02\n",
      "Epoch: 4895 mean train loss:  1.34348526e-02, mean val. rec. loss:  1.20178692e-02\n",
      "Epoch: 4896 mean train loss:  1.34340100e-02, mean val. rec. loss:  1.20172070e-02\n",
      "Epoch: 4897 mean train loss:  1.34331841e-02, mean val. rec. loss:  1.20165323e-02\n",
      "Epoch: 4898 mean train loss:  1.34323433e-02, mean val. rec. loss:  1.20158666e-02\n",
      "Epoch: 4899 mean train loss:  1.34315137e-02, mean val. rec. loss:  1.20151987e-02\n",
      "Epoch: 4900 mean train loss:  1.34306850e-02, mean val. rec. loss:  1.20145353e-02\n",
      "Epoch: 4901 mean train loss:  1.34298480e-02, mean val. rec. loss:  1.20138561e-02\n",
      "Epoch: 4902 mean train loss:  1.34290137e-02, mean val. rec. loss:  1.20131938e-02\n",
      "Epoch: 4903 mean train loss:  1.34281878e-02, mean val. rec. loss:  1.20125293e-02\n",
      "Epoch: 4904 mean train loss:  1.34273582e-02, mean val. rec. loss:  1.20118625e-02\n",
      "Epoch: 4905 mean train loss:  1.34265277e-02, mean val. rec. loss:  1.20111980e-02\n",
      "Epoch: 4906 mean train loss:  1.34256981e-02, mean val. rec. loss:  1.20105335e-02\n",
      "Epoch: 4907 mean train loss:  1.34248703e-02, mean val. rec. loss:  1.20098645e-02\n",
      "Epoch: 4908 mean train loss:  1.34240379e-02, mean val. rec. loss:  1.20092102e-02\n",
      "Epoch: 4909 mean train loss:  1.34232148e-02, mean val. rec. loss:  1.20085423e-02\n",
      "Epoch: 4910 mean train loss:  1.34223918e-02, mean val. rec. loss:  1.20078800e-02\n",
      "Epoch: 4911 mean train loss:  1.34215621e-02, mean val. rec. loss:  1.20072144e-02\n",
      "Epoch: 4912 mean train loss:  1.34207325e-02, mean val. rec. loss:  1.20065578e-02\n",
      "Epoch: 4913 mean train loss:  1.34199150e-02, mean val. rec. loss:  1.20058899e-02\n",
      "Epoch: 4914 mean train loss:  1.34190826e-02, mean val. rec. loss:  1.20052333e-02\n",
      "Epoch: 4915 mean train loss:  1.34182670e-02, mean val. rec. loss:  1.20045665e-02\n",
      "Epoch: 4916 mean train loss:  1.34174355e-02, mean val. rec. loss:  1.20039088e-02\n",
      "Epoch: 4917 mean train loss:  1.34166171e-02, mean val. rec. loss:  1.20032511e-02\n",
      "Epoch: 4918 mean train loss:  1.34157996e-02, mean val. rec. loss:  1.20025821e-02\n",
      "Epoch: 4919 mean train loss:  1.34149737e-02, mean val. rec. loss:  1.20019244e-02\n",
      "Epoch: 4920 mean train loss:  1.34141478e-02, mean val. rec. loss:  1.20012723e-02\n",
      "Epoch: 4921 mean train loss:  1.34133322e-02, mean val. rec. loss:  1.20006112e-02\n",
      "Epoch: 4922 mean train loss:  1.34125110e-02, mean val. rec. loss:  1.19999535e-02\n",
      "Epoch: 4923 mean train loss:  1.34116916e-02, mean val. rec. loss:  1.19992958e-02\n",
      "Epoch: 4924 mean train loss:  1.34108732e-02, mean val. rec. loss:  1.19986404e-02\n",
      "Epoch: 4925 mean train loss:  1.34100547e-02, mean val. rec. loss:  1.19979793e-02\n",
      "Epoch: 4926 mean train loss:  1.34092335e-02, mean val. rec. loss:  1.19973204e-02\n",
      "Epoch: 4927 mean train loss:  1.34084207e-02, mean val. rec. loss:  1.19966707e-02\n",
      "Epoch: 4928 mean train loss:  1.34076078e-02, mean val. rec. loss:  1.19960152e-02\n",
      "Epoch: 4929 mean train loss:  1.34067875e-02, mean val. rec. loss:  1.19953564e-02\n",
      "Epoch: 4930 mean train loss:  1.34059682e-02, mean val. rec. loss:  1.19947066e-02\n",
      "Epoch: 4931 mean train loss:  1.34051618e-02, mean val. rec. loss:  1.19940478e-02\n",
      "Epoch: 4932 mean train loss:  1.34043387e-02, mean val. rec. loss:  1.19934014e-02\n",
      "Epoch: 4933 mean train loss:  1.34035333e-02, mean val. rec. loss:  1.19927426e-02\n",
      "Epoch: 4934 mean train loss:  1.34027121e-02, mean val. rec. loss:  1.19920951e-02\n",
      "Epoch: 4935 mean train loss:  1.34019039e-02, mean val. rec. loss:  1.19914442e-02\n",
      "Epoch: 4936 mean train loss:  1.34010957e-02, mean val. rec. loss:  1.19907955e-02\n",
      "Epoch: 4937 mean train loss:  1.34002792e-02, mean val. rec. loss:  1.19901355e-02\n",
      "Epoch: 4938 mean train loss:  1.33994645e-02, mean val. rec. loss:  1.19894892e-02\n",
      "Epoch: 4939 mean train loss:  1.33986572e-02, mean val. rec. loss:  1.19888383e-02\n",
      "Epoch: 4940 mean train loss:  1.33978471e-02, mean val. rec. loss:  1.19881874e-02\n",
      "Epoch: 4941 mean train loss:  1.33970380e-02, mean val. rec. loss:  1.19875365e-02\n",
      "Epoch: 4942 mean train loss:  1.33962280e-02, mean val. rec. loss:  1.19868901e-02\n",
      "Epoch: 4943 mean train loss:  1.33954207e-02, mean val. rec. loss:  1.19862369e-02\n",
      "Epoch: 4944 mean train loss:  1.33946097e-02, mean val. rec. loss:  1.19855985e-02\n",
      "Epoch: 4945 mean train loss:  1.33938053e-02, mean val. rec. loss:  1.19849442e-02\n",
      "Epoch: 4946 mean train loss:  1.33930027e-02, mean val. rec. loss:  1.19842978e-02\n",
      "Epoch: 4947 mean train loss:  1.33921926e-02, mean val. rec. loss:  1.19836492e-02\n",
      "Epoch: 4948 mean train loss:  1.33913826e-02, mean val. rec. loss:  1.19830074e-02\n",
      "Epoch: 4949 mean train loss:  1.33905855e-02, mean val. rec. loss:  1.19823587e-02\n",
      "Epoch: 4950 mean train loss:  1.33897746e-02, mean val. rec. loss:  1.19817169e-02\n",
      "Epoch: 4951 mean train loss:  1.33889766e-02, mean val. rec. loss:  1.19810671e-02\n",
      "Epoch: 4952 mean train loss:  1.33881647e-02, mean val. rec. loss:  1.19804287e-02\n",
      "Epoch: 4953 mean train loss:  1.33873667e-02, mean val. rec. loss:  1.19797857e-02\n",
      "Epoch: 4954 mean train loss:  1.33865679e-02, mean val. rec. loss:  1.19791360e-02\n",
      "Epoch: 4955 mean train loss:  1.33857625e-02, mean val. rec. loss:  1.19784964e-02\n",
      "Epoch: 4956 mean train loss:  1.33849571e-02, mean val. rec. loss:  1.19778568e-02\n",
      "Epoch: 4957 mean train loss:  1.33841600e-02, mean val. rec. loss:  1.19772150e-02\n",
      "Epoch: 4958 mean train loss:  1.33833593e-02, mean val. rec. loss:  1.19765743e-02\n",
      "Epoch: 4959 mean train loss:  1.33825595e-02, mean val. rec. loss:  1.19759336e-02\n",
      "Epoch: 4960 mean train loss:  1.33817606e-02, mean val. rec. loss:  1.19752941e-02\n",
      "Epoch: 4961 mean train loss:  1.33809608e-02, mean val. rec. loss:  1.19746477e-02\n",
      "Epoch: 4962 mean train loss:  1.33801591e-02, mean val. rec. loss:  1.19740059e-02\n",
      "Epoch: 4963 mean train loss:  1.33793658e-02, mean val. rec. loss:  1.19733742e-02\n",
      "Epoch: 4964 mean train loss:  1.33785725e-02, mean val. rec. loss:  1.19727335e-02\n",
      "Epoch: 4965 mean train loss:  1.33777718e-02, mean val. rec. loss:  1.19720917e-02\n",
      "Epoch: 4966 mean train loss:  1.33769720e-02, mean val. rec. loss:  1.19714544e-02\n",
      "Epoch: 4967 mean train loss:  1.33761834e-02, mean val. rec. loss:  1.19708126e-02\n",
      "Epoch: 4968 mean train loss:  1.33753798e-02, mean val. rec. loss:  1.19701798e-02\n",
      "Epoch: 4969 mean train loss:  1.33745930e-02, mean val. rec. loss:  1.19695403e-02\n",
      "Epoch: 4970 mean train loss:  1.33737914e-02, mean val. rec. loss:  1.19689064e-02\n",
      "Epoch: 4971 mean train loss:  1.33730018e-02, mean val. rec. loss:  1.19682691e-02\n",
      "Epoch: 4972 mean train loss:  1.33722132e-02, mean val. rec. loss:  1.19676375e-02\n",
      "Epoch: 4973 mean train loss:  1.33714161e-02, mean val. rec. loss:  1.19669922e-02\n",
      "Epoch: 4974 mean train loss:  1.33706191e-02, mean val. rec. loss:  1.19663629e-02\n",
      "Epoch: 4975 mean train loss:  1.33698333e-02, mean val. rec. loss:  1.19657278e-02\n",
      "Epoch: 4976 mean train loss:  1.33690409e-02, mean val. rec. loss:  1.19650928e-02\n",
      "Epoch: 4977 mean train loss:  1.33682504e-02, mean val. rec. loss:  1.19644600e-02\n",
      "Epoch: 4978 mean train loss:  1.33674609e-02, mean val. rec. loss:  1.19638273e-02\n",
      "Epoch: 4979 mean train loss:  1.33666704e-02, mean val. rec. loss:  1.19631900e-02\n",
      "Epoch: 4980 mean train loss:  1.33658780e-02, mean val. rec. loss:  1.19625663e-02\n",
      "Epoch: 4981 mean train loss:  1.33650921e-02, mean val. rec. loss:  1.19619290e-02\n",
      "Epoch: 4982 mean train loss:  1.33643091e-02, mean val. rec. loss:  1.19612974e-02\n",
      "Epoch: 4983 mean train loss:  1.33635167e-02, mean val. rec. loss:  1.19606635e-02\n",
      "Epoch: 4984 mean train loss:  1.33627253e-02, mean val. rec. loss:  1.19600364e-02\n",
      "Epoch: 4985 mean train loss:  1.33619469e-02, mean val. rec. loss:  1.19594002e-02\n",
      "Epoch: 4986 mean train loss:  1.33611527e-02, mean val. rec. loss:  1.19587743e-02\n",
      "Epoch: 4987 mean train loss:  1.33603752e-02, mean val. rec. loss:  1.19581404e-02\n",
      "Epoch: 4988 mean train loss:  1.33595819e-02, mean val. rec. loss:  1.19575144e-02\n",
      "Epoch: 4989 mean train loss:  1.33588017e-02, mean val. rec. loss:  1.19568851e-02\n",
      "Epoch: 4990 mean train loss:  1.33580214e-02, mean val. rec. loss:  1.19562489e-02\n",
      "Epoch: 4991 mean train loss:  1.33572337e-02, mean val. rec. loss:  1.19556218e-02\n",
      "Epoch: 4992 mean train loss:  1.33564460e-02, mean val. rec. loss:  1.19550015e-02\n",
      "Epoch: 4993 mean train loss:  1.33556676e-02, mean val. rec. loss:  1.19543699e-02\n",
      "Epoch: 4994 mean train loss:  1.33548864e-02, mean val. rec. loss:  1.19537417e-02\n",
      "Epoch: 4995 mean train loss:  1.33541043e-02, mean val. rec. loss:  1.19531157e-02\n",
      "Epoch: 4996 mean train loss:  1.33533231e-02, mean val. rec. loss:  1.19524909e-02\n",
      "Epoch: 4997 mean train loss:  1.33525428e-02, mean val. rec. loss:  1.19518604e-02\n",
      "Epoch: 4998 mean train loss:  1.33517589e-02, mean val. rec. loss:  1.19512311e-02\n",
      "Epoch: 4999 mean train loss:  1.33509823e-02, mean val. rec. loss:  1.19506142e-02\n",
      "Epoch: 5000 mean train loss:  1.33502077e-02, mean val. rec. loss:  1.19499882e-02\n",
      "Epoch: 5001 mean train loss:  1.33494255e-02, mean val. rec. loss:  1.19493623e-02\n",
      "Epoch: 5002 mean train loss:  1.33486425e-02, mean val. rec. loss:  1.19487431e-02\n",
      "Epoch: 5003 mean train loss:  1.33478725e-02, mean val. rec. loss:  1.19481138e-02\n",
      "Epoch: 5004 mean train loss:  1.33470876e-02, mean val. rec. loss:  1.19474969e-02\n",
      "Epoch: 5005 mean train loss:  1.33463194e-02, mean val. rec. loss:  1.19468698e-02\n",
      "Epoch: 5006 mean train loss:  1.33455345e-02, mean val. rec. loss:  1.19462507e-02\n",
      "Epoch: 5007 mean train loss:  1.33447645e-02, mean val. rec. loss:  1.19456315e-02\n",
      "Epoch: 5008 mean train loss:  1.33439917e-02, mean val. rec. loss:  1.19450135e-02\n",
      "Epoch: 5009 mean train loss:  1.33432114e-02, mean val. rec. loss:  1.19443841e-02\n",
      "Epoch: 5010 mean train loss:  1.33424349e-02, mean val. rec. loss:  1.19437695e-02\n",
      "Epoch: 5011 mean train loss:  1.33416639e-02, mean val. rec. loss:  1.19431481e-02\n",
      "Epoch: 5012 mean train loss:  1.33408911e-02, mean val. rec. loss:  1.19425267e-02\n",
      "Epoch: 5013 mean train loss:  1.33401183e-02, mean val. rec. loss:  1.19419098e-02\n",
      "Epoch: 5014 mean train loss:  1.33393455e-02, mean val. rec. loss:  1.19412872e-02\n",
      "Epoch: 5015 mean train loss:  1.33385718e-02, mean val. rec. loss:  1.19406647e-02\n",
      "Epoch: 5016 mean train loss:  1.33377971e-02, mean val. rec. loss:  1.19400546e-02\n",
      "Epoch: 5017 mean train loss:  1.33370299e-02, mean val. rec. loss:  1.19394321e-02\n",
      "Epoch: 5018 mean train loss:  1.33362636e-02, mean val. rec. loss:  1.19388129e-02\n",
      "Epoch: 5019 mean train loss:  1.33354889e-02, mean val. rec. loss:  1.19381926e-02\n",
      "Epoch: 5020 mean train loss:  1.33347152e-02, mean val. rec. loss:  1.19375780e-02\n",
      "Epoch: 5021 mean train loss:  1.33339535e-02, mean val. rec. loss:  1.19369566e-02\n",
      "Epoch: 5022 mean train loss:  1.33331761e-02, mean val. rec. loss:  1.19363465e-02\n",
      "Epoch: 5023 mean train loss:  1.33324163e-02, mean val. rec. loss:  1.19357251e-02\n",
      "Epoch: 5024 mean train loss:  1.33316398e-02, mean val. rec. loss:  1.19351127e-02\n",
      "Epoch: 5025 mean train loss:  1.33308763e-02, mean val. rec. loss:  1.19344981e-02\n",
      "Epoch: 5026 mean train loss:  1.33301137e-02, mean val. rec. loss:  1.19338756e-02\n",
      "Epoch: 5027 mean train loss:  1.33293437e-02, mean val. rec. loss:  1.19332644e-02\n",
      "Epoch: 5028 mean train loss:  1.33285737e-02, mean val. rec. loss:  1.19326543e-02\n",
      "Epoch: 5029 mean train loss:  1.33278120e-02, mean val. rec. loss:  1.19320385e-02\n",
      "Epoch: 5030 mean train loss:  1.33270467e-02, mean val. rec. loss:  1.19314250e-02\n",
      "Epoch: 5031 mean train loss:  1.33262822e-02, mean val. rec. loss:  1.19308116e-02\n",
      "Epoch: 5032 mean train loss:  1.33255169e-02, mean val. rec. loss:  1.19302003e-02\n",
      "Epoch: 5033 mean train loss:  1.33247543e-02, mean val. rec. loss:  1.19295823e-02\n",
      "Epoch: 5034 mean train loss:  1.33239871e-02, mean val. rec. loss:  1.19289666e-02\n",
      "Epoch: 5035 mean train loss:  1.33232282e-02, mean val. rec. loss:  1.19283610e-02\n",
      "Epoch: 5036 mean train loss:  1.33224694e-02, mean val. rec. loss:  1.19277487e-02\n",
      "Epoch: 5037 mean train loss:  1.33217031e-02, mean val. rec. loss:  1.19271363e-02\n",
      "Epoch: 5038 mean train loss:  1.33209377e-02, mean val. rec. loss:  1.19265297e-02\n",
      "Epoch: 5039 mean train loss:  1.33201854e-02, mean val. rec. loss:  1.19259139e-02\n",
      "Epoch: 5040 mean train loss:  1.33194163e-02, mean val. rec. loss:  1.19253095e-02\n",
      "Epoch: 5041 mean train loss:  1.33186640e-02, mean val. rec. loss:  1.19246960e-02\n",
      "Epoch: 5042 mean train loss:  1.33178968e-02, mean val. rec. loss:  1.19240893e-02\n",
      "Epoch: 5043 mean train loss:  1.33171417e-02, mean val. rec. loss:  1.19234804e-02\n",
      "Epoch: 5044 mean train loss:  1.33163866e-02, mean val. rec. loss:  1.19228771e-02\n",
      "Epoch: 5045 mean train loss:  1.33156249e-02, mean val. rec. loss:  1.19222591e-02\n",
      "Epoch: 5046 mean train loss:  1.33148633e-02, mean val. rec. loss:  1.19216547e-02\n",
      "Epoch: 5047 mean train loss:  1.33141082e-02, mean val. rec. loss:  1.19210480e-02\n",
      "Epoch: 5048 mean train loss:  1.33133530e-02, mean val. rec. loss:  1.19204413e-02\n",
      "Epoch: 5049 mean train loss:  1.33125951e-02, mean val. rec. loss:  1.19198358e-02\n",
      "Epoch: 5050 mean train loss:  1.33118391e-02, mean val. rec. loss:  1.19192303e-02\n",
      "Epoch: 5051 mean train loss:  1.33110840e-02, mean val. rec. loss:  1.19186179e-02\n",
      "Epoch: 5052 mean train loss:  1.33103251e-02, mean val. rec. loss:  1.19180203e-02\n",
      "Epoch: 5053 mean train loss:  1.33095737e-02, mean val. rec. loss:  1.19174125e-02\n",
      "Epoch: 5054 mean train loss:  1.33088223e-02, mean val. rec. loss:  1.19168058e-02\n",
      "Epoch: 5055 mean train loss:  1.33080644e-02, mean val. rec. loss:  1.19162003e-02\n",
      "Epoch: 5056 mean train loss:  1.33073084e-02, mean val. rec. loss:  1.19155993e-02\n",
      "Epoch: 5057 mean train loss:  1.33065626e-02, mean val. rec. loss:  1.19149881e-02\n",
      "Epoch: 5058 mean train loss:  1.33058009e-02, mean val. rec. loss:  1.19143916e-02\n",
      "Epoch: 5059 mean train loss:  1.33050579e-02, mean val. rec. loss:  1.19137849e-02\n",
      "Epoch: 5060 mean train loss:  1.33042972e-02, mean val. rec. loss:  1.19131884e-02\n",
      "Epoch: 5061 mean train loss:  1.33035542e-02, mean val. rec. loss:  1.19125886e-02\n",
      "Epoch: 5062 mean train loss:  1.33027982e-02, mean val. rec. loss:  1.19119773e-02\n",
      "Epoch: 5063 mean train loss:  1.33020440e-02, mean val. rec. loss:  1.19113809e-02\n",
      "Epoch: 5064 mean train loss:  1.33012972e-02, mean val. rec. loss:  1.19107821e-02\n",
      "Epoch: 5065 mean train loss:  1.33005523e-02, mean val. rec. loss:  1.19101789e-02\n",
      "Epoch: 5066 mean train loss:  1.32998037e-02, mean val. rec. loss:  1.19095778e-02\n",
      "Epoch: 5067 mean train loss:  1.32990551e-02, mean val. rec. loss:  1.19089780e-02\n",
      "Epoch: 5068 mean train loss:  1.32983065e-02, mean val. rec. loss:  1.19083781e-02\n",
      "Epoch: 5069 mean train loss:  1.32975598e-02, mean val. rec. loss:  1.19077794e-02\n",
      "Epoch: 5070 mean train loss:  1.32968112e-02, mean val. rec. loss:  1.19071772e-02\n",
      "Epoch: 5071 mean train loss:  1.32960589e-02, mean val. rec. loss:  1.19065728e-02\n",
      "Epoch: 5072 mean train loss:  1.32953094e-02, mean val. rec. loss:  1.19059729e-02\n",
      "Epoch: 5073 mean train loss:  1.32945673e-02, mean val. rec. loss:  1.19053810e-02\n",
      "Epoch: 5074 mean train loss:  1.32938243e-02, mean val. rec. loss:  1.19047777e-02\n",
      "Epoch: 5075 mean train loss:  1.32930719e-02, mean val. rec. loss:  1.19041835e-02\n",
      "Epoch: 5076 mean train loss:  1.32923345e-02, mean val. rec. loss:  1.19035814e-02\n",
      "Epoch: 5077 mean train loss:  1.32915822e-02, mean val. rec. loss:  1.19029894e-02\n",
      "Epoch: 5078 mean train loss:  1.32908448e-02, mean val. rec. loss:  1.19023884e-02\n",
      "Epoch: 5079 mean train loss:  1.32900934e-02, mean val. rec. loss:  1.19017942e-02\n",
      "Epoch: 5080 mean train loss:  1.32893550e-02, mean val. rec. loss:  1.19012000e-02\n",
      "Epoch: 5081 mean train loss:  1.32886157e-02, mean val. rec. loss:  1.19005968e-02\n",
      "Epoch: 5082 mean train loss:  1.32878690e-02, mean val. rec. loss:  1.19000048e-02\n",
      "Epoch: 5083 mean train loss:  1.32871241e-02, mean val. rec. loss:  1.18994072e-02\n",
      "Epoch: 5084 mean train loss:  1.32863811e-02, mean val. rec. loss:  1.18988119e-02\n",
      "Epoch: 5085 mean train loss:  1.32856399e-02, mean val. rec. loss:  1.18982177e-02\n",
      "Epoch: 5086 mean train loss:  1.32848997e-02, mean val. rec. loss:  1.18976235e-02\n",
      "Epoch: 5087 mean train loss:  1.32841576e-02, mean val. rec. loss:  1.18970304e-02\n",
      "Epoch: 5088 mean train loss:  1.32834183e-02, mean val. rec. loss:  1.18964339e-02\n",
      "Epoch: 5089 mean train loss:  1.32826828e-02, mean val. rec. loss:  1.18958477e-02\n",
      "Epoch: 5090 mean train loss:  1.32819472e-02, mean val. rec. loss:  1.18952535e-02\n",
      "Epoch: 5091 mean train loss:  1.32812042e-02, mean val. rec. loss:  1.18946570e-02\n",
      "Epoch: 5092 mean train loss:  1.32804630e-02, mean val. rec. loss:  1.18940685e-02\n",
      "Epoch: 5093 mean train loss:  1.32797312e-02, mean val. rec. loss:  1.18934708e-02\n",
      "Epoch: 5094 mean train loss:  1.32789873e-02, mean val. rec. loss:  1.18928834e-02\n",
      "Epoch: 5095 mean train loss:  1.32782582e-02, mean val. rec. loss:  1.18922858e-02\n",
      "Epoch: 5096 mean train loss:  1.32775115e-02, mean val. rec. loss:  1.18917030e-02\n",
      "Epoch: 5097 mean train loss:  1.32767843e-02, mean val. rec. loss:  1.18911076e-02\n",
      "Epoch: 5098 mean train loss:  1.32760441e-02, mean val. rec. loss:  1.18905168e-02\n",
      "Epoch: 5099 mean train loss:  1.32753048e-02, mean val. rec. loss:  1.18899306e-02\n",
      "Epoch: 5100 mean train loss:  1.32745739e-02, mean val. rec. loss:  1.18893432e-02\n",
      "Epoch: 5101 mean train loss:  1.32738430e-02, mean val. rec. loss:  1.18887512e-02\n",
      "Epoch: 5102 mean train loss:  1.32731083e-02, mean val. rec. loss:  1.18881616e-02\n",
      "Epoch: 5103 mean train loss:  1.32723746e-02, mean val. rec. loss:  1.18875719e-02\n",
      "Epoch: 5104 mean train loss:  1.32716400e-02, mean val. rec. loss:  1.18869868e-02\n",
      "Epoch: 5105 mean train loss:  1.32709072e-02, mean val. rec. loss:  1.18863971e-02\n",
      "Epoch: 5106 mean train loss:  1.32701744e-02, mean val. rec. loss:  1.18858074e-02\n",
      "Epoch: 5107 mean train loss:  1.32694370e-02, mean val. rec. loss:  1.18852144e-02\n",
      "Epoch: 5108 mean train loss:  1.32687014e-02, mean val. rec. loss:  1.18846383e-02\n",
      "Epoch: 5109 mean train loss:  1.32679733e-02, mean val. rec. loss:  1.18840464e-02\n",
      "Epoch: 5110 mean train loss:  1.32672471e-02, mean val. rec. loss:  1.18834533e-02\n",
      "Epoch: 5111 mean train loss:  1.32665087e-02, mean val. rec. loss:  1.18828727e-02\n",
      "Epoch: 5112 mean train loss:  1.32657853e-02, mean val. rec. loss:  1.18822808e-02\n",
      "Epoch: 5113 mean train loss:  1.32650478e-02, mean val. rec. loss:  1.18816990e-02\n",
      "Epoch: 5114 mean train loss:  1.32643253e-02, mean val. rec. loss:  1.18811094e-02\n",
      "Epoch: 5115 mean train loss:  1.32635879e-02, mean val. rec. loss:  1.18805276e-02\n",
      "Epoch: 5116 mean train loss:  1.32628635e-02, mean val. rec. loss:  1.18799402e-02\n",
      "Epoch: 5117 mean train loss:  1.32621391e-02, mean val. rec. loss:  1.18793630e-02\n",
      "Epoch: 5118 mean train loss:  1.32614072e-02, mean val. rec. loss:  1.18787700e-02\n",
      "Epoch: 5119 mean train loss:  1.32606754e-02, mean val. rec. loss:  1.18781803e-02\n",
      "Epoch: 5120 mean train loss:  1.32599473e-02, mean val. rec. loss:  1.18775974e-02\n",
      "Epoch: 5121 mean train loss:  1.32592210e-02, mean val. rec. loss:  1.18770146e-02\n",
      "Epoch: 5122 mean train loss:  1.32584948e-02, mean val. rec. loss:  1.18764294e-02\n",
      "Epoch: 5123 mean train loss:  1.32577685e-02, mean val. rec. loss:  1.18758477e-02\n",
      "Epoch: 5124 mean train loss:  1.32570423e-02, mean val. rec. loss:  1.18752739e-02\n",
      "Epoch: 5125 mean train loss:  1.32563207e-02, mean val. rec. loss:  1.18746854e-02\n",
      "Epoch: 5126 mean train loss:  1.32555991e-02, mean val. rec. loss:  1.18741025e-02\n",
      "Epoch: 5127 mean train loss:  1.32548710e-02, mean val. rec. loss:  1.18735174e-02\n",
      "Epoch: 5128 mean train loss:  1.32541428e-02, mean val. rec. loss:  1.18729391e-02\n",
      "Epoch: 5129 mean train loss:  1.32534259e-02, mean val. rec. loss:  1.18723517e-02\n",
      "Epoch: 5130 mean train loss:  1.32526950e-02, mean val. rec. loss:  1.18717756e-02\n",
      "Epoch: 5131 mean train loss:  1.32519799e-02, mean val. rec. loss:  1.18711905e-02\n",
      "Epoch: 5132 mean train loss:  1.32512490e-02, mean val. rec. loss:  1.18706144e-02\n",
      "Epoch: 5133 mean train loss:  1.32505339e-02, mean val. rec. loss:  1.18700406e-02\n",
      "Epoch: 5134 mean train loss:  1.32498067e-02, mean val. rec. loss:  1.18694510e-02\n",
      "Epoch: 5135 mean train loss:  1.32490823e-02, mean val. rec. loss:  1.18688749e-02\n",
      "Epoch: 5136 mean train loss:  1.32483645e-02, mean val. rec. loss:  1.18682988e-02\n",
      "Epoch: 5137 mean train loss:  1.32476494e-02, mean val. rec. loss:  1.18677160e-02\n",
      "Epoch: 5138 mean train loss:  1.32469241e-02, mean val. rec. loss:  1.18671467e-02\n",
      "Epoch: 5139 mean train loss:  1.32462025e-02, mean val. rec. loss:  1.18665570e-02\n",
      "Epoch: 5140 mean train loss:  1.32454837e-02, mean val. rec. loss:  1.18659901e-02\n",
      "Epoch: 5141 mean train loss:  1.32447630e-02, mean val. rec. loss:  1.18654027e-02\n",
      "Epoch: 5142 mean train loss:  1.32440442e-02, mean val. rec. loss:  1.18648311e-02\n",
      "Epoch: 5143 mean train loss:  1.32433300e-02, mean val. rec. loss:  1.18642505e-02\n",
      "Epoch: 5144 mean train loss:  1.32426066e-02, mean val. rec. loss:  1.18636722e-02\n",
      "Epoch: 5145 mean train loss:  1.32418840e-02, mean val. rec. loss:  1.18631052e-02\n",
      "Epoch: 5146 mean train loss:  1.32411708e-02, mean val. rec. loss:  1.18625258e-02\n",
      "Epoch: 5147 mean train loss:  1.32404576e-02, mean val. rec. loss:  1.18619440e-02\n",
      "Epoch: 5148 mean train loss:  1.32397332e-02, mean val. rec. loss:  1.18613725e-02\n",
      "Epoch: 5149 mean train loss:  1.32390246e-02, mean val. rec. loss:  1.18607942e-02\n",
      "Epoch: 5150 mean train loss:  1.32383012e-02, mean val. rec. loss:  1.18602249e-02\n",
      "Epoch: 5151 mean train loss:  1.32375917e-02, mean val. rec. loss:  1.18596421e-02\n",
      "Epoch: 5152 mean train loss:  1.32368738e-02, mean val. rec. loss:  1.18590671e-02\n",
      "Epoch: 5153 mean train loss:  1.32361541e-02, mean val. rec. loss:  1.18584967e-02\n",
      "Epoch: 5154 mean train loss:  1.32354427e-02, mean val. rec. loss:  1.18579229e-02\n",
      "Epoch: 5155 mean train loss:  1.32347323e-02, mean val. rec. loss:  1.18573514e-02\n",
      "Epoch: 5156 mean train loss:  1.32340126e-02, mean val. rec. loss:  1.18567776e-02\n",
      "Epoch: 5157 mean train loss:  1.32333003e-02, mean val. rec. loss:  1.18562038e-02\n",
      "Epoch: 5158 mean train loss:  1.32325861e-02, mean val. rec. loss:  1.18556300e-02\n",
      "Epoch: 5159 mean train loss:  1.32318720e-02, mean val. rec. loss:  1.18550563e-02\n",
      "Epoch: 5160 mean train loss:  1.32311597e-02, mean val. rec. loss:  1.18544757e-02\n",
      "Epoch: 5161 mean train loss:  1.32304511e-02, mean val. rec. loss:  1.18539041e-02\n",
      "Epoch: 5162 mean train loss:  1.32297370e-02, mean val. rec. loss:  1.18533269e-02\n",
      "Epoch: 5163 mean train loss:  1.32290200e-02, mean val. rec. loss:  1.18527520e-02\n",
      "Epoch: 5164 mean train loss:  1.32283124e-02, mean val. rec. loss:  1.18521884e-02\n",
      "Epoch: 5165 mean train loss:  1.32276057e-02, mean val. rec. loss:  1.18516101e-02\n",
      "Epoch: 5166 mean train loss:  1.32268878e-02, mean val. rec. loss:  1.18510454e-02\n",
      "Epoch: 5167 mean train loss:  1.32261839e-02, mean val. rec. loss:  1.18504705e-02\n",
      "Epoch: 5168 mean train loss:  1.32254670e-02, mean val. rec. loss:  1.18499057e-02\n",
      "Epoch: 5169 mean train loss:  1.32247649e-02, mean val. rec. loss:  1.18493387e-02\n",
      "Epoch: 5170 mean train loss:  1.32240517e-02, mean val. rec. loss:  1.18487581e-02\n",
      "Epoch: 5171 mean train loss:  1.32233385e-02, mean val. rec. loss:  1.18481934e-02\n",
      "Epoch: 5172 mean train loss:  1.32226346e-02, mean val. rec. loss:  1.18476242e-02\n",
      "Epoch: 5173 mean train loss:  1.32219307e-02, mean val. rec. loss:  1.18470481e-02\n",
      "Epoch: 5174 mean train loss:  1.32212174e-02, mean val. rec. loss:  1.18464902e-02\n",
      "Epoch: 5175 mean train loss:  1.32205107e-02, mean val. rec. loss:  1.18459096e-02\n",
      "Epoch: 5176 mean train loss:  1.32198050e-02, mean val. rec. loss:  1.18453540e-02\n",
      "Epoch: 5177 mean train loss:  1.32190964e-02, mean val. rec. loss:  1.18447756e-02\n",
      "Epoch: 5178 mean train loss:  1.32183906e-02, mean val. rec. loss:  1.18442177e-02\n",
      "Epoch: 5179 mean train loss:  1.32176830e-02, mean val. rec. loss:  1.18436473e-02\n",
      "Epoch: 5180 mean train loss:  1.32169828e-02, mean val. rec. loss:  1.18430781e-02\n",
      "Epoch: 5181 mean train loss:  1.32162733e-02, mean val. rec. loss:  1.18425065e-02\n",
      "Epoch: 5182 mean train loss:  1.32155629e-02, mean val. rec. loss:  1.18419384e-02\n",
      "Epoch: 5183 mean train loss:  1.32148637e-02, mean val. rec. loss:  1.18413748e-02\n",
      "Epoch: 5184 mean train loss:  1.32141579e-02, mean val. rec. loss:  1.18408135e-02\n",
      "Epoch: 5185 mean train loss:  1.32134530e-02, mean val. rec. loss:  1.18402420e-02\n",
      "Epoch: 5186 mean train loss:  1.32127501e-02, mean val. rec. loss:  1.18396705e-02\n",
      "Epoch: 5187 mean train loss:  1.32120471e-02, mean val. rec. loss:  1.18391125e-02\n",
      "Epoch: 5188 mean train loss:  1.32113423e-02, mean val. rec. loss:  1.18385501e-02\n",
      "Epoch: 5189 mean train loss:  1.32106439e-02, mean val. rec. loss:  1.18379876e-02\n",
      "Epoch: 5190 mean train loss:  1.32099382e-02, mean val. rec. loss:  1.18374138e-02\n",
      "Epoch: 5191 mean train loss:  1.32092315e-02, mean val. rec. loss:  1.18368525e-02\n",
      "Epoch: 5192 mean train loss:  1.32085331e-02, mean val. rec. loss:  1.18362867e-02\n",
      "Epoch: 5193 mean train loss:  1.32078320e-02, mean val. rec. loss:  1.18357220e-02\n",
      "Epoch: 5194 mean train loss:  1.32071300e-02, mean val. rec. loss:  1.18351572e-02\n",
      "Epoch: 5195 mean train loss:  1.32064298e-02, mean val. rec. loss:  1.18345925e-02\n",
      "Epoch: 5196 mean train loss:  1.32057296e-02, mean val. rec. loss:  1.18340301e-02\n",
      "Epoch: 5197 mean train loss:  1.32050285e-02, mean val. rec. loss:  1.18334608e-02\n",
      "Epoch: 5198 mean train loss:  1.32043246e-02, mean val. rec. loss:  1.18329074e-02\n",
      "Epoch: 5199 mean train loss:  1.32036291e-02, mean val. rec. loss:  1.18323382e-02\n",
      "Epoch: 5200 mean train loss:  1.32029345e-02, mean val. rec. loss:  1.18317734e-02\n",
      "Epoch: 5201 mean train loss:  1.32022306e-02, mean val. rec. loss:  1.18312042e-02\n",
      "Epoch: 5202 mean train loss:  1.32015322e-02, mean val. rec. loss:  1.18306485e-02\n",
      "Epoch: 5203 mean train loss:  1.32008339e-02, mean val. rec. loss:  1.18300929e-02\n",
      "Epoch: 5204 mean train loss:  1.32001356e-02, mean val. rec. loss:  1.18295225e-02\n",
      "Epoch: 5205 mean train loss:  1.31994373e-02, mean val. rec. loss:  1.18289555e-02\n",
      "Epoch: 5206 mean train loss:  1.31987408e-02, mean val. rec. loss:  1.18283965e-02\n",
      "Epoch: 5207 mean train loss:  1.31980388e-02, mean val. rec. loss:  1.18278385e-02\n",
      "Epoch: 5208 mean train loss:  1.31973460e-02, mean val. rec. loss:  1.18272784e-02\n",
      "Epoch: 5209 mean train loss:  1.31966542e-02, mean val. rec. loss:  1.18267216e-02\n",
      "Epoch: 5210 mean train loss:  1.31959531e-02, mean val. rec. loss:  1.18261512e-02\n",
      "Epoch: 5211 mean train loss:  1.31952585e-02, mean val. rec. loss:  1.18256012e-02\n",
      "Epoch: 5212 mean train loss:  1.31945611e-02, mean val. rec. loss:  1.18250297e-02\n",
      "Epoch: 5213 mean train loss:  1.31938656e-02, mean val. rec. loss:  1.18244820e-02\n",
      "Epoch: 5214 mean train loss:  1.31931710e-02, mean val. rec. loss:  1.18239116e-02\n",
      "Epoch: 5215 mean train loss:  1.31924764e-02, mean val. rec. loss:  1.18233593e-02\n",
      "Epoch: 5216 mean train loss:  1.31917855e-02, mean val. rec. loss:  1.18227969e-02\n",
      "Epoch: 5217 mean train loss:  1.31910881e-02, mean val. rec. loss:  1.18222344e-02\n",
      "Epoch: 5218 mean train loss:  1.31903907e-02, mean val. rec. loss:  1.18216856e-02\n",
      "Epoch: 5219 mean train loss:  1.31896999e-02, mean val. rec. loss:  1.18211254e-02\n",
      "Epoch: 5220 mean train loss:  1.31890118e-02, mean val. rec. loss:  1.18205618e-02\n",
      "Epoch: 5221 mean train loss:  1.31883116e-02, mean val. rec. loss:  1.18200084e-02\n",
      "Epoch: 5222 mean train loss:  1.31876273e-02, mean val. rec. loss:  1.18194449e-02\n",
      "Epoch: 5223 mean train loss:  1.31869271e-02, mean val. rec. loss:  1.18188937e-02\n",
      "Epoch: 5224 mean train loss:  1.31862418e-02, mean val. rec. loss:  1.18183290e-02\n",
      "Epoch: 5225 mean train loss:  1.31855472e-02, mean val. rec. loss:  1.18177722e-02\n",
      "Epoch: 5226 mean train loss:  1.31848526e-02, mean val. rec. loss:  1.18172200e-02\n",
      "Epoch: 5227 mean train loss:  1.31841654e-02, mean val. rec. loss:  1.18166655e-02\n",
      "Epoch: 5228 mean train loss:  1.31834783e-02, mean val. rec. loss:  1.18161155e-02\n",
      "Epoch: 5229 mean train loss:  1.31827828e-02, mean val. rec. loss:  1.18155598e-02\n",
      "Epoch: 5230 mean train loss:  1.31820938e-02, mean val. rec. loss:  1.18150042e-02\n",
      "Epoch: 5231 mean train loss:  1.31814029e-02, mean val. rec. loss:  1.18144486e-02\n",
      "Epoch: 5232 mean train loss:  1.31807148e-02, mean val. rec. loss:  1.18138918e-02\n",
      "Epoch: 5233 mean train loss:  1.31800239e-02, mean val. rec. loss:  1.18133327e-02\n",
      "Epoch: 5234 mean train loss:  1.31793405e-02, mean val. rec. loss:  1.18127759e-02\n",
      "Epoch: 5235 mean train loss:  1.31786478e-02, mean val. rec. loss:  1.18122169e-02\n",
      "Epoch: 5236 mean train loss:  1.31779550e-02, mean val. rec. loss:  1.18116612e-02\n",
      "Epoch: 5237 mean train loss:  1.31772716e-02, mean val. rec. loss:  1.18111147e-02\n",
      "Epoch: 5238 mean train loss:  1.31765882e-02, mean val. rec. loss:  1.18105545e-02\n",
      "Epoch: 5239 mean train loss:  1.31758917e-02, mean val. rec. loss:  1.18100056e-02\n",
      "Epoch: 5240 mean train loss:  1.31752120e-02, mean val. rec. loss:  1.18094477e-02\n",
      "Epoch: 5241 mean train loss:  1.31745184e-02, mean val. rec. loss:  1.18088989e-02\n",
      "Epoch: 5242 mean train loss:  1.31738396e-02, mean val. rec. loss:  1.18083500e-02\n",
      "Epoch: 5243 mean train loss:  1.31731487e-02, mean val. rec. loss:  1.18077864e-02\n",
      "Epoch: 5244 mean train loss:  1.31724597e-02, mean val. rec. loss:  1.18072387e-02\n",
      "Epoch: 5245 mean train loss:  1.31717782e-02, mean val. rec. loss:  1.18066876e-02\n",
      "Epoch: 5246 mean train loss:  1.31710966e-02, mean val. rec. loss:  1.18061286e-02\n",
      "Epoch: 5247 mean train loss:  1.31704076e-02, mean val. rec. loss:  1.18055854e-02\n",
      "Epoch: 5248 mean train loss:  1.31697223e-02, mean val. rec. loss:  1.18050207e-02\n",
      "Epoch: 5249 mean train loss:  1.31690389e-02, mean val. rec. loss:  1.18044809e-02\n",
      "Epoch: 5250 mean train loss:  1.31683536e-02, mean val. rec. loss:  1.18039196e-02\n",
      "Epoch: 5251 mean train loss:  1.31676702e-02, mean val. rec. loss:  1.18033787e-02\n",
      "Epoch: 5252 mean train loss:  1.31669867e-02, mean val. rec. loss:  1.18028219e-02\n",
      "Epoch: 5253 mean train loss:  1.31663070e-02, mean val. rec. loss:  1.18022696e-02\n",
      "Epoch: 5254 mean train loss:  1.31656199e-02, mean val. rec. loss:  1.18017163e-02\n",
      "Epoch: 5255 mean train loss:  1.31649327e-02, mean val. rec. loss:  1.18011652e-02\n",
      "Epoch: 5256 mean train loss:  1.31642549e-02, mean val. rec. loss:  1.18006186e-02\n",
      "Epoch: 5257 mean train loss:  1.31635724e-02, mean val. rec. loss:  1.18000731e-02\n",
      "Epoch: 5258 mean train loss:  1.31628899e-02, mean val. rec. loss:  1.17995186e-02\n",
      "Epoch: 5259 mean train loss:  1.31622093e-02, mean val. rec. loss:  1.17989618e-02\n",
      "Epoch: 5260 mean train loss:  1.31615277e-02, mean val. rec. loss:  1.17984187e-02\n",
      "Epoch: 5261 mean train loss:  1.31608462e-02, mean val. rec. loss:  1.17978675e-02\n",
      "Epoch: 5262 mean train loss:  1.31601693e-02, mean val. rec. loss:  1.17973244e-02\n",
      "Epoch: 5263 mean train loss:  1.31594849e-02, mean val. rec. loss:  1.17967653e-02\n",
      "Epoch: 5264 mean train loss:  1.31588005e-02, mean val. rec. loss:  1.17962221e-02\n",
      "Epoch: 5265 mean train loss:  1.31581246e-02, mean val. rec. loss:  1.17956710e-02\n",
      "Epoch: 5266 mean train loss:  1.31574449e-02, mean val. rec. loss:  1.17951211e-02\n",
      "Epoch: 5267 mean train loss:  1.31567652e-02, mean val. rec. loss:  1.17945733e-02\n",
      "Epoch: 5268 mean train loss:  1.31560864e-02, mean val. rec. loss:  1.17940256e-02\n",
      "Epoch: 5269 mean train loss:  1.31554076e-02, mean val. rec. loss:  1.17934779e-02\n",
      "Epoch: 5270 mean train loss:  1.31547279e-02, mean val. rec. loss:  1.17929257e-02\n",
      "Epoch: 5271 mean train loss:  1.31540464e-02, mean val. rec. loss:  1.17923882e-02\n",
      "Epoch: 5272 mean train loss:  1.31533713e-02, mean val. rec. loss:  1.17918348e-02\n",
      "Epoch: 5273 mean train loss:  1.31526981e-02, mean val. rec. loss:  1.17912882e-02\n",
      "Epoch: 5274 mean train loss:  1.31520166e-02, mean val. rec. loss:  1.17907348e-02\n",
      "Epoch: 5275 mean train loss:  1.31513387e-02, mean val. rec. loss:  1.17901928e-02\n",
      "Epoch: 5276 mean train loss:  1.31506628e-02, mean val. rec. loss:  1.17896530e-02\n",
      "Epoch: 5277 mean train loss:  1.31499849e-02, mean val. rec. loss:  1.17891019e-02\n",
      "Epoch: 5278 mean train loss:  1.31493090e-02, mean val. rec. loss:  1.17885508e-02\n",
      "Epoch: 5279 mean train loss:  1.31486330e-02, mean val. rec. loss:  1.17880065e-02\n",
      "Epoch: 5280 mean train loss:  1.31479533e-02, mean val. rec. loss:  1.17874644e-02\n",
      "Epoch: 5281 mean train loss:  1.31472810e-02, mean val. rec. loss:  1.17869190e-02\n",
      "Epoch: 5282 mean train loss:  1.31466097e-02, mean val. rec. loss:  1.17863804e-02\n",
      "Epoch: 5283 mean train loss:  1.31459300e-02, mean val. rec. loss:  1.17858270e-02\n",
      "Epoch: 5284 mean train loss:  1.31452512e-02, mean val. rec. loss:  1.17852781e-02\n",
      "Epoch: 5285 mean train loss:  1.31445771e-02, mean val. rec. loss:  1.17847361e-02\n",
      "Epoch: 5286 mean train loss:  1.31439021e-02, mean val. rec. loss:  1.17841895e-02\n",
      "Epoch: 5287 mean train loss:  1.31432280e-02, mean val. rec. loss:  1.17836475e-02\n",
      "Epoch: 5288 mean train loss:  1.31425548e-02, mean val. rec. loss:  1.17831100e-02\n",
      "Epoch: 5289 mean train loss:  1.31418844e-02, mean val. rec. loss:  1.17825600e-02\n",
      "Epoch: 5290 mean train loss:  1.31412159e-02, mean val. rec. loss:  1.17820157e-02\n",
      "Epoch: 5291 mean train loss:  1.31405380e-02, mean val. rec. loss:  1.17814680e-02\n",
      "Epoch: 5292 mean train loss:  1.31398621e-02, mean val. rec. loss:  1.17809248e-02\n",
      "Epoch: 5293 mean train loss:  1.31391935e-02, mean val. rec. loss:  1.17803850e-02\n",
      "Epoch: 5294 mean train loss:  1.31385194e-02, mean val. rec. loss:  1.17798464e-02\n",
      "Epoch: 5295 mean train loss:  1.31378481e-02, mean val. rec. loss:  1.17792975e-02\n",
      "Epoch: 5296 mean train loss:  1.31371768e-02, mean val. rec. loss:  1.17787510e-02\n",
      "Epoch: 5297 mean train loss:  1.31365064e-02, mean val. rec. loss:  1.17782089e-02\n",
      "Epoch: 5298 mean train loss:  1.31358304e-02, mean val. rec. loss:  1.17776669e-02\n",
      "Epoch: 5299 mean train loss:  1.31351638e-02, mean val. rec. loss:  1.17771271e-02\n",
      "Epoch: 5300 mean train loss:  1.31344962e-02, mean val. rec. loss:  1.17765896e-02\n",
      "Epoch: 5301 mean train loss:  1.31338211e-02, mean val. rec. loss:  1.17760385e-02\n",
      "Epoch: 5302 mean train loss:  1.31331470e-02, mean val. rec. loss:  1.17754965e-02\n",
      "Epoch: 5303 mean train loss:  1.31324766e-02, mean val. rec. loss:  1.17749544e-02\n",
      "Epoch: 5304 mean train loss:  1.31318081e-02, mean val. rec. loss:  1.17744158e-02\n",
      "Epoch: 5305 mean train loss:  1.31311377e-02, mean val. rec. loss:  1.17738737e-02\n",
      "Epoch: 5306 mean train loss:  1.31304692e-02, mean val. rec. loss:  1.17733328e-02\n",
      "Epoch: 5307 mean train loss:  1.31297988e-02, mean val. rec. loss:  1.17728021e-02\n",
      "Epoch: 5308 mean train loss:  1.31291340e-02, mean val. rec. loss:  1.17722578e-02\n",
      "Epoch: 5309 mean train loss:  1.31284701e-02, mean val. rec. loss:  1.17717169e-02\n",
      "Epoch: 5310 mean train loss:  1.31277979e-02, mean val. rec. loss:  1.17711737e-02\n",
      "Epoch: 5311 mean train loss:  1.31271256e-02, mean val. rec. loss:  1.17706385e-02\n",
      "Epoch: 5312 mean train loss:  1.31264655e-02, mean val. rec. loss:  1.17700942e-02\n",
      "Epoch: 5313 mean train loss:  1.31257904e-02, mean val. rec. loss:  1.17695590e-02\n",
      "Epoch: 5314 mean train loss:  1.31251312e-02, mean val. rec. loss:  1.17690158e-02\n",
      "Epoch: 5315 mean train loss:  1.31244562e-02, mean val. rec. loss:  1.17684817e-02\n",
      "Epoch: 5316 mean train loss:  1.31237970e-02, mean val. rec. loss:  1.17679487e-02\n",
      "Epoch: 5317 mean train loss:  1.31231266e-02, mean val. rec. loss:  1.17673965e-02\n",
      "Epoch: 5318 mean train loss:  1.31224562e-02, mean val. rec. loss:  1.17668612e-02\n",
      "Epoch: 5319 mean train loss:  1.31217932e-02, mean val. rec. loss:  1.17663249e-02\n",
      "Epoch: 5320 mean train loss:  1.31211322e-02, mean val. rec. loss:  1.17657828e-02\n",
      "Epoch: 5321 mean train loss:  1.31204655e-02, mean val. rec. loss:  1.17652442e-02\n",
      "Epoch: 5322 mean train loss:  1.31198016e-02, mean val. rec. loss:  1.17647044e-02\n",
      "Epoch: 5323 mean train loss:  1.31191359e-02, mean val. rec. loss:  1.17641658e-02\n",
      "Epoch: 5324 mean train loss:  1.31184711e-02, mean val. rec. loss:  1.17636305e-02\n",
      "Epoch: 5325 mean train loss:  1.31178063e-02, mean val. rec. loss:  1.17630885e-02\n",
      "Epoch: 5326 mean train loss:  1.31171378e-02, mean val. rec. loss:  1.17625464e-02\n",
      "Epoch: 5327 mean train loss:  1.31164692e-02, mean val. rec. loss:  1.17620089e-02\n",
      "Epoch: 5328 mean train loss:  1.31158100e-02, mean val. rec. loss:  1.17614782e-02\n",
      "Epoch: 5329 mean train loss:  1.31151489e-02, mean val. rec. loss:  1.17609385e-02\n",
      "Epoch: 5330 mean train loss:  1.31144823e-02, mean val. rec. loss:  1.17603964e-02\n",
      "Epoch: 5331 mean train loss:  1.31138193e-02, mean val. rec. loss:  1.17598646e-02\n",
      "Epoch: 5332 mean train loss:  1.31131564e-02, mean val. rec. loss:  1.17593362e-02\n",
      "Epoch: 5333 mean train loss:  1.31124935e-02, mean val. rec. loss:  1.17587952e-02\n",
      "Epoch: 5334 mean train loss:  1.31118305e-02, mean val. rec. loss:  1.17582600e-02\n",
      "Epoch: 5335 mean train loss:  1.31111732e-02, mean val. rec. loss:  1.17577157e-02\n",
      "Epoch: 5336 mean train loss:  1.31105065e-02, mean val. rec. loss:  1.17571850e-02\n",
      "Epoch: 5337 mean train loss:  1.31098408e-02, mean val. rec. loss:  1.17566520e-02\n",
      "Epoch: 5338 mean train loss:  1.31091843e-02, mean val. rec. loss:  1.17561202e-02\n",
      "Epoch: 5339 mean train loss:  1.31085270e-02, mean val. rec. loss:  1.17555838e-02\n",
      "Epoch: 5340 mean train loss:  1.31078650e-02, mean val. rec. loss:  1.17550486e-02\n",
      "Epoch: 5341 mean train loss:  1.31072039e-02, mean val. rec. loss:  1.17545145e-02\n",
      "Epoch: 5342 mean train loss:  1.31065438e-02, mean val. rec. loss:  1.17539804e-02\n",
      "Epoch: 5343 mean train loss:  1.31058827e-02, mean val. rec. loss:  1.17534429e-02\n",
      "Epoch: 5344 mean train loss:  1.31052179e-02, mean val. rec. loss:  1.17529042e-02\n",
      "Epoch: 5345 mean train loss:  1.31045531e-02, mean val. rec. loss:  1.17523667e-02\n",
      "Epoch: 5346 mean train loss:  1.31038994e-02, mean val. rec. loss:  1.17518406e-02\n",
      "Epoch: 5347 mean train loss:  1.31032430e-02, mean val. rec. loss:  1.17513065e-02\n",
      "Epoch: 5348 mean train loss:  1.31025792e-02, mean val. rec. loss:  1.17507780e-02\n",
      "Epoch: 5349 mean train loss:  1.31019199e-02, mean val. rec. loss:  1.17502383e-02\n",
      "Epoch: 5350 mean train loss:  1.31012617e-02, mean val. rec. loss:  1.17496996e-02\n",
      "Epoch: 5351 mean train loss:  1.31006034e-02, mean val. rec. loss:  1.17491723e-02\n",
      "Epoch: 5352 mean train loss:  1.30999442e-02, mean val. rec. loss:  1.17486439e-02\n",
      "Epoch: 5353 mean train loss:  1.30992849e-02, mean val. rec. loss:  1.17480996e-02\n",
      "Epoch: 5354 mean train loss:  1.30986229e-02, mean val. rec. loss:  1.17475711e-02\n",
      "Epoch: 5355 mean train loss:  1.30979702e-02, mean val. rec. loss:  1.17470404e-02\n",
      "Epoch: 5356 mean train loss:  1.30973157e-02, mean val. rec. loss:  1.17465007e-02\n",
      "Epoch: 5357 mean train loss:  1.30966555e-02, mean val. rec. loss:  1.17459779e-02\n",
      "Epoch: 5358 mean train loss:  1.30959972e-02, mean val. rec. loss:  1.17454325e-02\n",
      "Epoch: 5359 mean train loss:  1.30953408e-02, mean val. rec. loss:  1.17449086e-02\n",
      "Epoch: 5360 mean train loss:  1.30946835e-02, mean val. rec. loss:  1.17443654e-02\n",
      "Epoch: 5361 mean train loss:  1.30940280e-02, mean val. rec. loss:  1.17438472e-02\n",
      "Epoch: 5362 mean train loss:  1.30933706e-02, mean val. rec. loss:  1.17433097e-02\n",
      "Epoch: 5363 mean train loss:  1.30927198e-02, mean val. rec. loss:  1.17427756e-02\n",
      "Epoch: 5364 mean train loss:  1.30920596e-02, mean val. rec. loss:  1.17422392e-02\n",
      "Epoch: 5365 mean train loss:  1.30913995e-02, mean val. rec. loss:  1.17417074e-02\n",
      "Epoch: 5366 mean train loss:  1.30907477e-02, mean val. rec. loss:  1.17411801e-02\n",
      "Epoch: 5367 mean train loss:  1.30900932e-02, mean val. rec. loss:  1.17406539e-02\n",
      "Epoch: 5368 mean train loss:  1.30894377e-02, mean val. rec. loss:  1.17401153e-02\n",
      "Epoch: 5369 mean train loss:  1.30887822e-02, mean val. rec. loss:  1.17395812e-02\n",
      "Epoch: 5370 mean train loss:  1.30881267e-02, mean val. rec. loss:  1.17390561e-02\n",
      "Epoch: 5371 mean train loss:  1.30874731e-02, mean val. rec. loss:  1.17385243e-02\n",
      "Epoch: 5372 mean train loss:  1.30868222e-02, mean val. rec. loss:  1.17380004e-02\n",
      "Epoch: 5373 mean train loss:  1.30861658e-02, mean val. rec. loss:  1.17374583e-02\n",
      "Epoch: 5374 mean train loss:  1.30855066e-02, mean val. rec. loss:  1.17369344e-02\n",
      "Epoch: 5375 mean train loss:  1.30848585e-02, mean val. rec. loss:  1.17364083e-02\n",
      "Epoch: 5376 mean train loss:  1.30842086e-02, mean val. rec. loss:  1.17358764e-02\n",
      "Epoch: 5377 mean train loss:  1.30835541e-02, mean val. rec. loss:  1.17353469e-02\n",
      "Epoch: 5378 mean train loss:  1.30829014e-02, mean val. rec. loss:  1.17348162e-02\n",
      "Epoch: 5379 mean train loss:  1.30822487e-02, mean val. rec. loss:  1.17342900e-02\n",
      "Epoch: 5380 mean train loss:  1.30815960e-02, mean val. rec. loss:  1.17337570e-02\n",
      "Epoch: 5381 mean train loss:  1.30809396e-02, mean val. rec. loss:  1.17332195e-02\n",
      "Epoch: 5382 mean train loss:  1.30802831e-02, mean val. rec. loss:  1.17327024e-02\n",
      "Epoch: 5383 mean train loss:  1.30796351e-02, mean val. rec. loss:  1.17321683e-02\n",
      "Epoch: 5384 mean train loss:  1.30789871e-02, mean val. rec. loss:  1.17316388e-02\n",
      "Epoch: 5385 mean train loss:  1.30783316e-02, mean val. rec. loss:  1.17311035e-02\n",
      "Epoch: 5386 mean train loss:  1.30776807e-02, mean val. rec. loss:  1.17305796e-02\n",
      "Epoch: 5387 mean train loss:  1.30770299e-02, mean val. rec. loss:  1.17300580e-02\n",
      "Epoch: 5388 mean train loss:  1.30763781e-02, mean val. rec. loss:  1.17295205e-02\n",
      "Epoch: 5389 mean train loss:  1.30757264e-02, mean val. rec. loss:  1.17289932e-02\n",
      "Epoch: 5390 mean train loss:  1.30750802e-02, mean val. rec. loss:  1.17284682e-02\n",
      "Epoch: 5391 mean train loss:  1.30744266e-02, mean val. rec. loss:  1.17279284e-02\n",
      "Epoch: 5392 mean train loss:  1.30737720e-02, mean val. rec. loss:  1.17274045e-02\n",
      "Epoch: 5393 mean train loss:  1.30731267e-02, mean val. rec. loss:  1.17268761e-02\n",
      "Epoch: 5394 mean train loss:  1.30724787e-02, mean val. rec. loss:  1.17263476e-02\n",
      "Epoch: 5395 mean train loss:  1.30718307e-02, mean val. rec. loss:  1.17258203e-02\n",
      "Epoch: 5396 mean train loss:  1.30711808e-02, mean val. rec. loss:  1.17252953e-02\n",
      "Epoch: 5397 mean train loss:  1.30705327e-02, mean val. rec. loss:  1.17247669e-02\n",
      "Epoch: 5398 mean train loss:  1.30698828e-02, mean val. rec. loss:  1.17242418e-02\n",
      "Epoch: 5399 mean train loss:  1.30692329e-02, mean val. rec. loss:  1.17237123e-02\n",
      "Epoch: 5400 mean train loss:  1.30685811e-02, mean val. rec. loss:  1.17231827e-02\n",
      "Epoch: 5401 mean train loss:  1.30679294e-02, mean val. rec. loss:  1.17226531e-02\n",
      "Epoch: 5402 mean train loss:  1.30672851e-02, mean val. rec. loss:  1.17221361e-02\n",
      "Epoch: 5403 mean train loss:  1.30666389e-02, mean val. rec. loss:  1.17216042e-02\n",
      "Epoch: 5404 mean train loss:  1.30659834e-02, mean val. rec. loss:  1.17210837e-02\n",
      "Epoch: 5405 mean train loss:  1.30653447e-02, mean val. rec. loss:  1.17205530e-02\n",
      "Epoch: 5406 mean train loss:  1.30646892e-02, mean val. rec. loss:  1.17200337e-02\n",
      "Epoch: 5407 mean train loss:  1.30640486e-02, mean val. rec. loss:  1.17195052e-02\n",
      "Epoch: 5408 mean train loss:  1.30633940e-02, mean val. rec. loss:  1.17189825e-02\n",
      "Epoch: 5409 mean train loss:  1.30627506e-02, mean val. rec. loss:  1.17184597e-02\n",
      "Epoch: 5410 mean train loss:  1.30621091e-02, mean val. rec. loss:  1.17179267e-02\n",
      "Epoch: 5411 mean train loss:  1.30614583e-02, mean val. rec. loss:  1.17174028e-02\n",
      "Epoch: 5412 mean train loss:  1.30608084e-02, mean val. rec. loss:  1.17168778e-02\n",
      "Epoch: 5413 mean train loss:  1.30601622e-02, mean val. rec. loss:  1.17163505e-02\n",
      "Epoch: 5414 mean train loss:  1.30595160e-02, mean val. rec. loss:  1.17158243e-02\n",
      "Epoch: 5415 mean train loss:  1.30588698e-02, mean val. rec. loss:  1.17153004e-02\n",
      "Epoch: 5416 mean train loss:  1.30582255e-02, mean val. rec. loss:  1.17147731e-02\n",
      "Epoch: 5417 mean train loss:  1.30575784e-02, mean val. rec. loss:  1.17142481e-02\n",
      "Epoch: 5418 mean train loss:  1.30569387e-02, mean val. rec. loss:  1.17137288e-02\n",
      "Epoch: 5419 mean train loss:  1.30562953e-02, mean val. rec. loss:  1.17132026e-02\n",
      "Epoch: 5420 mean train loss:  1.30556473e-02, mean val. rec. loss:  1.17126742e-02\n",
      "Epoch: 5421 mean train loss:  1.30549983e-02, mean val. rec. loss:  1.17121593e-02\n",
      "Epoch: 5422 mean train loss:  1.30543577e-02, mean val. rec. loss:  1.17116309e-02\n",
      "Epoch: 5423 mean train loss:  1.30537134e-02, mean val. rec. loss:  1.17111002e-02\n",
      "Epoch: 5424 mean train loss:  1.30530691e-02, mean val. rec. loss:  1.17105831e-02\n",
      "Epoch: 5425 mean train loss:  1.30524248e-02, mean val. rec. loss:  1.17100637e-02\n",
      "Epoch: 5426 mean train loss:  1.30517805e-02, mean val. rec. loss:  1.17095308e-02\n",
      "Epoch: 5427 mean train loss:  1.30511333e-02, mean val. rec. loss:  1.17090114e-02\n",
      "Epoch: 5428 mean train loss:  1.30504937e-02, mean val. rec. loss:  1.17084886e-02\n",
      "Epoch: 5429 mean train loss:  1.30498550e-02, mean val. rec. loss:  1.17079591e-02\n",
      "Epoch: 5430 mean train loss:  1.30492078e-02, mean val. rec. loss:  1.17074386e-02\n",
      "Epoch: 5431 mean train loss:  1.30485598e-02, mean val. rec. loss:  1.17069147e-02\n",
      "Epoch: 5432 mean train loss:  1.30479173e-02, mean val. rec. loss:  1.17063897e-02\n",
      "Epoch: 5433 mean train loss:  1.30472740e-02, mean val. rec. loss:  1.17058680e-02\n",
      "Epoch: 5434 mean train loss:  1.30466315e-02, mean val. rec. loss:  1.17053464e-02\n",
      "Epoch: 5435 mean train loss:  1.30459900e-02, mean val. rec. loss:  1.17048202e-02\n",
      "Epoch: 5436 mean train loss:  1.30453522e-02, mean val. rec. loss:  1.17043043e-02\n",
      "Epoch: 5437 mean train loss:  1.30447125e-02, mean val. rec. loss:  1.17037804e-02\n",
      "Epoch: 5438 mean train loss:  1.30440682e-02, mean val. rec. loss:  1.17032531e-02\n",
      "Epoch: 5439 mean train loss:  1.30434220e-02, mean val. rec. loss:  1.17027417e-02\n",
      "Epoch: 5440 mean train loss:  1.30427842e-02, mean val. rec. loss:  1.17022098e-02\n",
      "Epoch: 5441 mean train loss:  1.30421427e-02, mean val. rec. loss:  1.17016814e-02\n",
      "Epoch: 5442 mean train loss:  1.30415030e-02, mean val. rec. loss:  1.17011643e-02\n",
      "Epoch: 5443 mean train loss:  1.30408606e-02, mean val. rec. loss:  1.17006495e-02\n",
      "Epoch: 5444 mean train loss:  1.30402200e-02, mean val. rec. loss:  1.17001233e-02\n",
      "Epoch: 5445 mean train loss:  1.30395803e-02, mean val. rec. loss:  1.16995994e-02\n",
      "Epoch: 5446 mean train loss:  1.30389444e-02, mean val. rec. loss:  1.16990710e-02\n",
      "Epoch: 5447 mean train loss:  1.30382991e-02, mean val. rec. loss:  1.16985527e-02\n",
      "Epoch: 5448 mean train loss:  1.30376557e-02, mean val. rec. loss:  1.16980379e-02\n",
      "Epoch: 5449 mean train loss:  1.30370207e-02, mean val. rec. loss:  1.16975152e-02\n",
      "Epoch: 5450 mean train loss:  1.30363801e-02, mean val. rec. loss:  1.16969935e-02\n",
      "Epoch: 5451 mean train loss:  1.30357405e-02, mean val. rec. loss:  1.16964730e-02\n",
      "Epoch: 5452 mean train loss:  1.30351008e-02, mean val. rec. loss:  1.16959514e-02\n",
      "Epoch: 5453 mean train loss:  1.30344621e-02, mean val. rec. loss:  1.16954332e-02\n",
      "Epoch: 5454 mean train loss:  1.30338234e-02, mean val. rec. loss:  1.16949093e-02\n",
      "Epoch: 5455 mean train loss:  1.30331790e-02, mean val. rec. loss:  1.16943876e-02\n",
      "Epoch: 5456 mean train loss:  1.30325450e-02, mean val. rec. loss:  1.16938717e-02\n",
      "Epoch: 5457 mean train loss:  1.30319100e-02, mean val. rec. loss:  1.16933523e-02\n",
      "Epoch: 5458 mean train loss:  1.30312684e-02, mean val. rec. loss:  1.16928296e-02\n",
      "Epoch: 5459 mean train loss:  1.30306260e-02, mean val. rec. loss:  1.16923147e-02\n",
      "Epoch: 5460 mean train loss:  1.30299956e-02, mean val. rec. loss:  1.16917920e-02\n",
      "Epoch: 5461 mean train loss:  1.30293485e-02, mean val. rec. loss:  1.16912760e-02\n",
      "Epoch: 5462 mean train loss:  1.30287200e-02, mean val. rec. loss:  1.16907533e-02\n",
      "Epoch: 5463 mean train loss:  1.30280738e-02, mean val. rec. loss:  1.16902362e-02\n",
      "Epoch: 5464 mean train loss:  1.30274416e-02, mean val. rec. loss:  1.16897157e-02\n",
      "Epoch: 5465 mean train loss:  1.30268066e-02, mean val. rec. loss:  1.16892008e-02\n",
      "Epoch: 5466 mean train loss:  1.30261660e-02, mean val. rec. loss:  1.16886713e-02\n",
      "Epoch: 5467 mean train loss:  1.30255236e-02, mean val. rec. loss:  1.16881576e-02\n",
      "Epoch: 5468 mean train loss:  1.30248914e-02, mean val. rec. loss:  1.16876371e-02\n",
      "Epoch: 5469 mean train loss:  1.30242554e-02, mean val. rec. loss:  1.16871166e-02\n",
      "Epoch: 5470 mean train loss:  1.30236185e-02, mean val. rec. loss:  1.16865972e-02\n",
      "Epoch: 5471 mean train loss:  1.30229817e-02, mean val. rec. loss:  1.16860779e-02\n",
      "Epoch: 5472 mean train loss:  1.30223457e-02, mean val. rec. loss:  1.16855562e-02\n",
      "Epoch: 5473 mean train loss:  1.30217061e-02, mean val. rec. loss:  1.16850459e-02\n",
      "Epoch: 5474 mean train loss:  1.30210729e-02, mean val. rec. loss:  1.16845232e-02\n",
      "Epoch: 5475 mean train loss:  1.30204417e-02, mean val. rec. loss:  1.16840038e-02\n",
      "Epoch: 5476 mean train loss:  1.30198029e-02, mean val. rec. loss:  1.16834833e-02\n",
      "Epoch: 5477 mean train loss:  1.30191633e-02, mean val. rec. loss:  1.16829708e-02\n",
      "Epoch: 5478 mean train loss:  1.30185357e-02, mean val. rec. loss:  1.16824446e-02\n",
      "Epoch: 5479 mean train loss:  1.30178923e-02, mean val. rec. loss:  1.16819309e-02\n",
      "Epoch: 5480 mean train loss:  1.30172648e-02, mean val. rec. loss:  1.16814082e-02\n",
      "Epoch: 5481 mean train loss:  1.30166223e-02, mean val. rec. loss:  1.16808956e-02\n",
      "Epoch: 5482 mean train loss:  1.30159957e-02, mean val. rec. loss:  1.16803717e-02\n",
      "Epoch: 5483 mean train loss:  1.30153579e-02, mean val. rec. loss:  1.16798569e-02\n",
      "Epoch: 5484 mean train loss:  1.30147182e-02, mean val. rec. loss:  1.16793420e-02\n",
      "Epoch: 5485 mean train loss:  1.30140879e-02, mean val. rec. loss:  1.16788272e-02\n",
      "Epoch: 5486 mean train loss:  1.30134584e-02, mean val. rec. loss:  1.16783101e-02\n",
      "Epoch: 5487 mean train loss:  1.30128234e-02, mean val. rec. loss:  1.16777908e-02\n",
      "Epoch: 5488 mean train loss:  1.30121903e-02, mean val. rec. loss:  1.16772748e-02\n",
      "Epoch: 5489 mean train loss:  1.30115562e-02, mean val. rec. loss:  1.16767588e-02\n",
      "Epoch: 5490 mean train loss:  1.30109240e-02, mean val. rec. loss:  1.16762429e-02\n",
      "Epoch: 5491 mean train loss:  1.30102890e-02, mean val. rec. loss:  1.16757247e-02\n",
      "Epoch: 5492 mean train loss:  1.30096521e-02, mean val. rec. loss:  1.16752030e-02\n",
      "Epoch: 5493 mean train loss:  1.30090162e-02, mean val. rec. loss:  1.16746859e-02\n",
      "Epoch: 5494 mean train loss:  1.30083868e-02, mean val. rec. loss:  1.16741756e-02\n",
      "Epoch: 5495 mean train loss:  1.30077564e-02, mean val. rec. loss:  1.16736540e-02\n",
      "Epoch: 5496 mean train loss:  1.30071167e-02, mean val. rec. loss:  1.16731392e-02\n",
      "Epoch: 5497 mean train loss:  1.30064920e-02, mean val. rec. loss:  1.16726198e-02\n",
      "Epoch: 5498 mean train loss:  1.30058514e-02, mean val. rec. loss:  1.16721084e-02\n",
      "Epoch: 5499 mean train loss:  1.30052266e-02, mean val. rec. loss:  1.16715856e-02\n",
      "Epoch: 5500 mean train loss:  1.30045870e-02, mean val. rec. loss:  1.16710708e-02\n",
      "Epoch: 5501 mean train loss:  1.30039594e-02, mean val. rec. loss:  1.16705560e-02\n",
      "Epoch: 5502 mean train loss:  1.30033309e-02, mean val. rec. loss:  1.16700434e-02\n",
      "Epoch: 5503 mean train loss:  1.30026950e-02, mean val. rec. loss:  1.16695150e-02\n",
      "Epoch: 5504 mean train loss:  1.30020600e-02, mean val. rec. loss:  1.16690036e-02\n",
      "Epoch: 5505 mean train loss:  1.30014324e-02, mean val. rec. loss:  1.16684854e-02\n",
      "Epoch: 5506 mean train loss:  1.30008011e-02, mean val. rec. loss:  1.16679683e-02\n",
      "Epoch: 5507 mean train loss:  1.30001698e-02, mean val. rec. loss:  1.16674512e-02\n",
      "Epoch: 5508 mean train loss:  1.29995386e-02, mean val. rec. loss:  1.16669375e-02\n",
      "Epoch: 5509 mean train loss:  1.29989073e-02, mean val. rec. loss:  1.16664158e-02\n",
      "Epoch: 5510 mean train loss:  1.29982723e-02, mean val. rec. loss:  1.16659112e-02\n",
      "Epoch: 5511 mean train loss:  1.29976456e-02, mean val. rec. loss:  1.16653907e-02\n",
      "Epoch: 5512 mean train loss:  1.29970200e-02, mean val. rec. loss:  1.16648770e-02\n",
      "Epoch: 5513 mean train loss:  1.29963859e-02, mean val. rec. loss:  1.16643599e-02\n",
      "Epoch: 5514 mean train loss:  1.29957509e-02, mean val. rec. loss:  1.16638497e-02\n",
      "Epoch: 5515 mean train loss:  1.29951289e-02, mean val. rec. loss:  1.16633280e-02\n",
      "Epoch: 5516 mean train loss:  1.29944911e-02, mean val. rec. loss:  1.16628189e-02\n",
      "Epoch: 5517 mean train loss:  1.29938691e-02, mean val. rec. loss:  1.16622995e-02\n",
      "Epoch: 5518 mean train loss:  1.29932313e-02, mean val. rec. loss:  1.16617926e-02\n",
      "Epoch: 5519 mean train loss:  1.29926103e-02, mean val. rec. loss:  1.16612710e-02\n",
      "Epoch: 5520 mean train loss:  1.29919771e-02, mean val. rec. loss:  1.16607596e-02\n",
      "Epoch: 5521 mean train loss:  1.29913449e-02, mean val. rec. loss:  1.16602481e-02\n",
      "Epoch: 5522 mean train loss:  1.29907174e-02, mean val. rec. loss:  1.16597356e-02\n",
      "Epoch: 5523 mean train loss:  1.29900935e-02, mean val. rec. loss:  1.16592196e-02\n",
      "Epoch: 5524 mean train loss:  1.29894641e-02, mean val. rec. loss:  1.16587059e-02\n",
      "Epoch: 5525 mean train loss:  1.29888356e-02, mean val. rec. loss:  1.16581911e-02\n",
      "Epoch: 5526 mean train loss:  1.29882071e-02, mean val. rec. loss:  1.16576774e-02\n",
      "Epoch: 5527 mean train loss:  1.29875777e-02, mean val. rec. loss:  1.16571649e-02\n",
      "Epoch: 5528 mean train loss:  1.29869502e-02, mean val. rec. loss:  1.16566489e-02\n",
      "Epoch: 5529 mean train loss:  1.29863170e-02, mean val. rec. loss:  1.16561307e-02\n",
      "Epoch: 5530 mean train loss:  1.29856848e-02, mean val. rec. loss:  1.16556136e-02\n",
      "Epoch: 5531 mean train loss:  1.29850610e-02, mean val. rec. loss:  1.16551067e-02\n",
      "Epoch: 5532 mean train loss:  1.29844371e-02, mean val. rec. loss:  1.16545953e-02\n",
      "Epoch: 5533 mean train loss:  1.29838058e-02, mean val. rec. loss:  1.16540850e-02\n",
      "Epoch: 5534 mean train loss:  1.29831764e-02, mean val. rec. loss:  1.16535656e-02\n",
      "Epoch: 5535 mean train loss:  1.29825498e-02, mean val. rec. loss:  1.16530474e-02\n",
      "Epoch: 5536 mean train loss:  1.29819222e-02, mean val. rec. loss:  1.16525416e-02\n",
      "Epoch: 5537 mean train loss:  1.29812947e-02, mean val. rec. loss:  1.16520245e-02\n",
      "Epoch: 5538 mean train loss:  1.29806708e-02, mean val. rec. loss:  1.16515052e-02\n",
      "Epoch: 5539 mean train loss:  1.29800414e-02, mean val. rec. loss:  1.16509915e-02\n",
      "Epoch: 5540 mean train loss:  1.29794092e-02, mean val. rec. loss:  1.16504846e-02\n",
      "Epoch: 5541 mean train loss:  1.29787872e-02, mean val. rec. loss:  1.16499721e-02\n",
      "Epoch: 5542 mean train loss:  1.29781634e-02, mean val. rec. loss:  1.16494595e-02\n",
      "Epoch: 5543 mean train loss:  1.29775377e-02, mean val. rec. loss:  1.16489458e-02\n",
      "Epoch: 5544 mean train loss:  1.29769111e-02, mean val. rec. loss:  1.16484321e-02\n",
      "Epoch: 5545 mean train loss:  1.29762835e-02, mean val. rec. loss:  1.16479207e-02\n",
      "Epoch: 5546 mean train loss:  1.29756578e-02, mean val. rec. loss:  1.16474070e-02\n",
      "Epoch: 5547 mean train loss:  1.29750275e-02, mean val. rec. loss:  1.16468899e-02\n",
      "Epoch: 5548 mean train loss:  1.29743971e-02, mean val. rec. loss:  1.16463751e-02\n",
      "Epoch: 5549 mean train loss:  1.29737752e-02, mean val. rec. loss:  1.16458693e-02\n",
      "Epoch: 5550 mean train loss:  1.29731532e-02, mean val. rec. loss:  1.16453568e-02\n",
      "Epoch: 5551 mean train loss:  1.29725238e-02, mean val. rec. loss:  1.16448374e-02\n",
      "Epoch: 5552 mean train loss:  1.29718990e-02, mean val. rec. loss:  1.16443294e-02\n",
      "Epoch: 5553 mean train loss:  1.29712742e-02, mean val. rec. loss:  1.16438236e-02\n",
      "Epoch: 5554 mean train loss:  1.29706467e-02, mean val. rec. loss:  1.16433077e-02\n",
      "Epoch: 5555 mean train loss:  1.29700228e-02, mean val. rec. loss:  1.16427906e-02\n",
      "Epoch: 5556 mean train loss:  1.29693971e-02, mean val. rec. loss:  1.16422769e-02\n",
      "Epoch: 5557 mean train loss:  1.29687677e-02, mean val. rec. loss:  1.16417689e-02\n",
      "Epoch: 5558 mean train loss:  1.29681467e-02, mean val. rec. loss:  1.16412586e-02\n",
      "Epoch: 5559 mean train loss:  1.29675256e-02, mean val. rec. loss:  1.16407528e-02\n",
      "Epoch: 5560 mean train loss:  1.29668962e-02, mean val. rec. loss:  1.16402380e-02\n",
      "Epoch: 5561 mean train loss:  1.29662724e-02, mean val. rec. loss:  1.16397243e-02\n",
      "Epoch: 5562 mean train loss:  1.29656476e-02, mean val. rec. loss:  1.16392117e-02\n",
      "Epoch: 5563 mean train loss:  1.29650229e-02, mean val. rec. loss:  1.16387015e-02\n",
      "Epoch: 5564 mean train loss:  1.29643990e-02, mean val. rec. loss:  1.16381889e-02\n",
      "Epoch: 5565 mean train loss:  1.29637752e-02, mean val. rec. loss:  1.16376718e-02\n",
      "Epoch: 5566 mean train loss:  1.29631551e-02, mean val. rec. loss:  1.16371604e-02\n",
      "Epoch: 5567 mean train loss:  1.29625285e-02, mean val. rec. loss:  1.16366444e-02\n",
      "Epoch: 5568 mean train loss:  1.29618990e-02, mean val. rec. loss:  1.16361307e-02\n",
      "Epoch: 5569 mean train loss:  1.29612799e-02, mean val. rec. loss:  1.16356284e-02\n",
      "Epoch: 5570 mean train loss:  1.29606616e-02, mean val. rec. loss:  1.16351113e-02\n",
      "Epoch: 5571 mean train loss:  1.29600285e-02, mean val. rec. loss:  1.16346044e-02\n",
      "Epoch: 5572 mean train loss:  1.29594139e-02, mean val. rec. loss:  1.16340884e-02\n",
      "Epoch: 5573 mean train loss:  1.29587827e-02, mean val. rec. loss:  1.16335816e-02\n",
      "Epoch: 5574 mean train loss:  1.29581663e-02, mean val. rec. loss:  1.16330747e-02\n",
      "Epoch: 5575 mean train loss:  1.29575406e-02, mean val. rec. loss:  1.16325530e-02\n",
      "Epoch: 5576 mean train loss:  1.29569130e-02, mean val. rec. loss:  1.16320473e-02\n",
      "Epoch: 5577 mean train loss:  1.29562929e-02, mean val. rec. loss:  1.16315359e-02\n",
      "Epoch: 5578 mean train loss:  1.29556747e-02, mean val. rec. loss:  1.16310199e-02\n",
      "Epoch: 5579 mean train loss:  1.29550490e-02, mean val. rec. loss:  1.16305187e-02\n",
      "Epoch: 5580 mean train loss:  1.29544251e-02, mean val. rec. loss:  1.16299971e-02\n",
      "Epoch: 5581 mean train loss:  1.29538032e-02, mean val. rec. loss:  1.16294947e-02\n",
      "Epoch: 5582 mean train loss:  1.29531803e-02, mean val. rec. loss:  1.16289753e-02\n",
      "Epoch: 5583 mean train loss:  1.29525592e-02, mean val. rec. loss:  1.16284696e-02\n",
      "Epoch: 5584 mean train loss:  1.29519391e-02, mean val. rec. loss:  1.16279570e-02\n",
      "Epoch: 5585 mean train loss:  1.29513134e-02, mean val. rec. loss:  1.16274399e-02\n",
      "Epoch: 5586 mean train loss:  1.29506877e-02, mean val. rec. loss:  1.16269410e-02\n",
      "Epoch: 5587 mean train loss:  1.29500695e-02, mean val. rec. loss:  1.16264250e-02\n",
      "Epoch: 5588 mean train loss:  1.29494531e-02, mean val. rec. loss:  1.16259113e-02\n",
      "Epoch: 5589 mean train loss:  1.29488227e-02, mean val. rec. loss:  1.16254033e-02\n",
      "Epoch: 5590 mean train loss:  1.29482091e-02, mean val. rec. loss:  1.16248908e-02\n",
      "Epoch: 5591 mean train loss:  1.29475797e-02, mean val. rec. loss:  1.16243861e-02\n",
      "Epoch: 5592 mean train loss:  1.29469661e-02, mean val. rec. loss:  1.16238724e-02\n",
      "Epoch: 5593 mean train loss:  1.29463376e-02, mean val. rec. loss:  1.16233656e-02\n",
      "Epoch: 5594 mean train loss:  1.29457194e-02, mean val. rec. loss:  1.16228564e-02\n",
      "Epoch: 5595 mean train loss:  1.29451030e-02, mean val. rec. loss:  1.16223529e-02\n",
      "Epoch: 5596 mean train loss:  1.29444773e-02, mean val. rec. loss:  1.16218347e-02\n",
      "Epoch: 5597 mean train loss:  1.29438535e-02, mean val. rec. loss:  1.16213233e-02\n",
      "Epoch: 5598 mean train loss:  1.29432315e-02, mean val. rec. loss:  1.16208141e-02\n",
      "Epoch: 5599 mean train loss:  1.29426114e-02, mean val. rec. loss:  1.16203027e-02\n",
      "Epoch: 5600 mean train loss:  1.29419894e-02, mean val. rec. loss:  1.16197947e-02\n",
      "Epoch: 5601 mean train loss:  1.29413693e-02, mean val. rec. loss:  1.16192866e-02\n",
      "Epoch: 5602 mean train loss:  1.29407492e-02, mean val. rec. loss:  1.16187854e-02\n",
      "Epoch: 5603 mean train loss:  1.29401328e-02, mean val. rec. loss:  1.16182706e-02\n",
      "Epoch: 5604 mean train loss:  1.29395164e-02, mean val. rec. loss:  1.16177603e-02\n",
      "Epoch: 5605 mean train loss:  1.29388926e-02, mean val. rec. loss:  1.16172478e-02\n",
      "Epoch: 5606 mean train loss:  1.29382669e-02, mean val. rec. loss:  1.16167363e-02\n",
      "Epoch: 5607 mean train loss:  1.29376524e-02, mean val. rec. loss:  1.16162317e-02\n",
      "Epoch: 5608 mean train loss:  1.29370323e-02, mean val. rec. loss:  1.16157260e-02\n",
      "Epoch: 5609 mean train loss:  1.29364122e-02, mean val. rec. loss:  1.16152123e-02\n",
      "Epoch: 5610 mean train loss:  1.29357930e-02, mean val. rec. loss:  1.16146974e-02\n",
      "Epoch: 5611 mean train loss:  1.29351738e-02, mean val. rec. loss:  1.16141883e-02\n",
      "Epoch: 5612 mean train loss:  1.29345490e-02, mean val. rec. loss:  1.16136814e-02\n",
      "Epoch: 5613 mean train loss:  1.29339336e-02, mean val. rec. loss:  1.16131745e-02\n",
      "Epoch: 5614 mean train loss:  1.29333191e-02, mean val. rec. loss:  1.16126699e-02\n",
      "Epoch: 5615 mean train loss:  1.29326952e-02, mean val. rec. loss:  1.16121539e-02\n",
      "Epoch: 5616 mean train loss:  1.29320723e-02, mean val. rec. loss:  1.16116414e-02\n",
      "Epoch: 5617 mean train loss:  1.29314532e-02, mean val. rec. loss:  1.16111334e-02\n",
      "Epoch: 5618 mean train loss:  1.29308349e-02, mean val. rec. loss:  1.16106242e-02\n",
      "Epoch: 5619 mean train loss:  1.29302157e-02, mean val. rec. loss:  1.16101173e-02\n",
      "Epoch: 5620 mean train loss:  1.29295966e-02, mean val. rec. loss:  1.16096161e-02\n",
      "Epoch: 5621 mean train loss:  1.29289802e-02, mean val. rec. loss:  1.16091035e-02\n",
      "Epoch: 5622 mean train loss:  1.29283675e-02, mean val. rec. loss:  1.16085932e-02\n",
      "Epoch: 5623 mean train loss:  1.29277446e-02, mean val. rec. loss:  1.16080784e-02\n",
      "Epoch: 5624 mean train loss:  1.29271217e-02, mean val. rec. loss:  1.16075727e-02\n",
      "Epoch: 5625 mean train loss:  1.29265072e-02, mean val. rec. loss:  1.16070692e-02\n",
      "Epoch: 5626 mean train loss:  1.29258880e-02, mean val. rec. loss:  1.16065668e-02\n",
      "Epoch: 5627 mean train loss:  1.29252707e-02, mean val. rec. loss:  1.16060531e-02\n",
      "Epoch: 5628 mean train loss:  1.29246524e-02, mean val. rec. loss:  1.16055406e-02\n",
      "Epoch: 5629 mean train loss:  1.29240342e-02, mean val. rec. loss:  1.16050394e-02\n",
      "Epoch: 5630 mean train loss:  1.29234141e-02, mean val. rec. loss:  1.16045302e-02\n",
      "Epoch: 5631 mean train loss:  1.29228014e-02, mean val. rec. loss:  1.16040279e-02\n",
      "Epoch: 5632 mean train loss:  1.29221794e-02, mean val. rec. loss:  1.16035074e-02\n",
      "Epoch: 5633 mean train loss:  1.29215575e-02, mean val. rec. loss:  1.16030050e-02\n",
      "Epoch: 5634 mean train loss:  1.29209439e-02, mean val. rec. loss:  1.16024947e-02\n",
      "Epoch: 5635 mean train loss:  1.29203266e-02, mean val. rec. loss:  1.16019844e-02\n",
      "Epoch: 5636 mean train loss:  1.29197083e-02, mean val. rec. loss:  1.16014741e-02\n",
      "Epoch: 5637 mean train loss:  1.29190919e-02, mean val. rec. loss:  1.16009673e-02\n",
      "Epoch: 5638 mean train loss:  1.29184737e-02, mean val. rec. loss:  1.16004626e-02\n",
      "Epoch: 5639 mean train loss:  1.29178573e-02, mean val. rec. loss:  1.15999455e-02\n",
      "Epoch: 5640 mean train loss:  1.29172353e-02, mean val. rec. loss:  1.15994489e-02\n",
      "Epoch: 5641 mean train loss:  1.29166217e-02, mean val. rec. loss:  1.15989352e-02\n",
      "Epoch: 5642 mean train loss:  1.29160100e-02, mean val. rec. loss:  1.15984271e-02\n",
      "Epoch: 5643 mean train loss:  1.29153880e-02, mean val. rec. loss:  1.15979169e-02\n",
      "Epoch: 5644 mean train loss:  1.29147670e-02, mean val. rec. loss:  1.15974122e-02\n",
      "Epoch: 5645 mean train loss:  1.29141590e-02, mean val. rec. loss:  1.15968997e-02\n",
      "Epoch: 5646 mean train loss:  1.29135333e-02, mean val. rec. loss:  1.15963973e-02\n",
      "Epoch: 5647 mean train loss:  1.29129244e-02, mean val. rec. loss:  1.15958859e-02\n",
      "Epoch: 5648 mean train loss:  1.29122987e-02, mean val. rec. loss:  1.15953824e-02\n",
      "Epoch: 5649 mean train loss:  1.29116869e-02, mean val. rec. loss:  1.15948721e-02\n",
      "Epoch: 5650 mean train loss:  1.29110752e-02, mean val. rec. loss:  1.15943596e-02\n",
      "Epoch: 5651 mean train loss:  1.29104551e-02, mean val. rec. loss:  1.15938515e-02\n",
      "Epoch: 5652 mean train loss:  1.29098331e-02, mean val. rec. loss:  1.15933503e-02\n",
      "Epoch: 5653 mean train loss:  1.29092205e-02, mean val. rec. loss:  1.15928423e-02\n",
      "Epoch: 5654 mean train loss:  1.29086041e-02, mean val. rec. loss:  1.15923320e-02\n",
      "Epoch: 5655 mean train loss:  1.29079886e-02, mean val. rec. loss:  1.15918240e-02\n",
      "Epoch: 5656 mean train loss:  1.29073722e-02, mean val. rec. loss:  1.15913171e-02\n",
      "Epoch: 5657 mean train loss:  1.29067568e-02, mean val. rec. loss:  1.15908125e-02\n",
      "Epoch: 5658 mean train loss:  1.29061404e-02, mean val. rec. loss:  1.15902999e-02\n",
      "Epoch: 5659 mean train loss:  1.29055212e-02, mean val. rec. loss:  1.15898033e-02\n",
      "Epoch: 5660 mean train loss:  1.29049095e-02, mean val. rec. loss:  1.15892907e-02\n",
      "Epoch: 5661 mean train loss:  1.29042968e-02, mean val. rec. loss:  1.15887827e-02\n",
      "Epoch: 5662 mean train loss:  1.29036767e-02, mean val. rec. loss:  1.15882792e-02\n",
      "Epoch: 5663 mean train loss:  1.29030613e-02, mean val. rec. loss:  1.15877666e-02\n",
      "Epoch: 5664 mean train loss:  1.29024449e-02, mean val. rec. loss:  1.15872541e-02\n",
      "Epoch: 5665 mean train loss:  1.29018294e-02, mean val. rec. loss:  1.15867517e-02\n",
      "Epoch: 5666 mean train loss:  1.29012149e-02, mean val. rec. loss:  1.15862516e-02\n",
      "Epoch: 5667 mean train loss:  1.29005985e-02, mean val. rec. loss:  1.15857334e-02\n",
      "Epoch: 5668 mean train loss:  1.28999793e-02, mean val. rec. loss:  1.15852311e-02\n",
      "Epoch: 5669 mean train loss:  1.28993676e-02, mean val. rec. loss:  1.15847253e-02\n",
      "Epoch: 5670 mean train loss:  1.28987568e-02, mean val. rec. loss:  1.15842127e-02\n",
      "Epoch: 5671 mean train loss:  1.28981376e-02, mean val. rec. loss:  1.15837093e-02\n",
      "Epoch: 5672 mean train loss:  1.28975175e-02, mean val. rec. loss:  1.15832012e-02\n",
      "Epoch: 5673 mean train loss:  1.28969021e-02, mean val. rec. loss:  1.15826944e-02\n",
      "Epoch: 5674 mean train loss:  1.28962885e-02, mean val. rec. loss:  1.15821875e-02\n",
      "Epoch: 5675 mean train loss:  1.28956730e-02, mean val. rec. loss:  1.15816817e-02\n",
      "Epoch: 5676 mean train loss:  1.28950585e-02, mean val. rec. loss:  1.15811714e-02\n",
      "Epoch: 5677 mean train loss:  1.28944477e-02, mean val. rec. loss:  1.15806679e-02\n",
      "Epoch: 5678 mean train loss:  1.28938369e-02, mean val. rec. loss:  1.15801599e-02\n",
      "Epoch: 5679 mean train loss:  1.28932177e-02, mean val. rec. loss:  1.15796496e-02\n",
      "Epoch: 5680 mean train loss:  1.28925985e-02, mean val. rec. loss:  1.15791529e-02\n",
      "Epoch: 5681 mean train loss:  1.28919878e-02, mean val. rec. loss:  1.15786381e-02\n",
      "Epoch: 5682 mean train loss:  1.28913732e-02, mean val. rec. loss:  1.15781256e-02\n",
      "Epoch: 5683 mean train loss:  1.28907596e-02, mean val. rec. loss:  1.15776232e-02\n",
      "Epoch: 5684 mean train loss:  1.28901451e-02, mean val. rec. loss:  1.15771231e-02\n",
      "Epoch: 5685 mean train loss:  1.28895297e-02, mean val. rec. loss:  1.15766060e-02\n",
      "Epoch: 5686 mean train loss:  1.28889114e-02, mean val. rec. loss:  1.15761025e-02\n",
      "Epoch: 5687 mean train loss:  1.28883025e-02, mean val. rec. loss:  1.15755979e-02\n",
      "Epoch: 5688 mean train loss:  1.28876917e-02, mean val. rec. loss:  1.15750854e-02\n",
      "Epoch: 5689 mean train loss:  1.28870725e-02, mean val. rec. loss:  1.15745830e-02\n",
      "Epoch: 5690 mean train loss:  1.28864552e-02, mean val. rec. loss:  1.15740750e-02\n",
      "Epoch: 5691 mean train loss:  1.28858407e-02, mean val. rec. loss:  1.15735670e-02\n",
      "Epoch: 5692 mean train loss:  1.28852271e-02, mean val. rec. loss:  1.15730612e-02\n",
      "Epoch: 5693 mean train loss:  1.28846135e-02, mean val. rec. loss:  1.15725555e-02\n",
      "Epoch: 5694 mean train loss:  1.28839990e-02, mean val. rec. loss:  1.15720508e-02\n",
      "Epoch: 5695 mean train loss:  1.28833844e-02, mean val. rec. loss:  1.15715530e-02\n",
      "Epoch: 5696 mean train loss:  1.28827764e-02, mean val. rec. loss:  1.15710427e-02\n",
      "Epoch: 5697 mean train loss:  1.28821656e-02, mean val. rec. loss:  1.15705359e-02\n",
      "Epoch: 5698 mean train loss:  1.28815492e-02, mean val. rec. loss:  1.15700267e-02\n",
      "Epoch: 5699 mean train loss:  1.28809319e-02, mean val. rec. loss:  1.15695209e-02\n",
      "Epoch: 5700 mean train loss:  1.28803249e-02, mean val. rec. loss:  1.15690061e-02\n",
      "Epoch: 5701 mean train loss:  1.28797029e-02, mean val. rec. loss:  1.15685049e-02\n",
      "Epoch: 5702 mean train loss:  1.28790986e-02, mean val. rec. loss:  1.15679912e-02\n",
      "Epoch: 5703 mean train loss:  1.28784766e-02, mean val. rec. loss:  1.15674911e-02\n",
      "Epoch: 5704 mean train loss:  1.28778705e-02, mean val. rec. loss:  1.15669774e-02\n",
      "Epoch: 5705 mean train loss:  1.28772541e-02, mean val. rec. loss:  1.15664728e-02\n",
      "Epoch: 5706 mean train loss:  1.28766377e-02, mean val. rec. loss:  1.15659693e-02\n",
      "Epoch: 5707 mean train loss:  1.28760269e-02, mean val. rec. loss:  1.15654636e-02\n",
      "Epoch: 5708 mean train loss:  1.28754189e-02, mean val. rec. loss:  1.15649646e-02\n",
      "Epoch: 5709 mean train loss:  1.28748016e-02, mean val. rec. loss:  1.15644566e-02\n",
      "Epoch: 5710 mean train loss:  1.28741880e-02, mean val. rec. loss:  1.15639509e-02\n",
      "Epoch: 5711 mean train loss:  1.28735763e-02, mean val. rec. loss:  1.15634451e-02\n",
      "Epoch: 5712 mean train loss:  1.28729636e-02, mean val. rec. loss:  1.15629416e-02\n",
      "Epoch: 5713 mean train loss:  1.28723491e-02, mean val. rec. loss:  1.15624291e-02\n",
      "Epoch: 5714 mean train loss:  1.28717411e-02, mean val. rec. loss:  1.15619233e-02\n",
      "Epoch: 5715 mean train loss:  1.28711247e-02, mean val. rec. loss:  1.15614119e-02\n",
      "Epoch: 5716 mean train loss:  1.28705074e-02, mean val. rec. loss:  1.15609050e-02\n",
      "Epoch: 5717 mean train loss:  1.28698994e-02, mean val. rec. loss:  1.15604072e-02\n",
      "Epoch: 5718 mean train loss:  1.28692904e-02, mean val. rec. loss:  1.15598958e-02\n",
      "Epoch: 5719 mean train loss:  1.28686694e-02, mean val. rec. loss:  1.15593945e-02\n",
      "Epoch: 5720 mean train loss:  1.28680642e-02, mean val. rec. loss:  1.15588842e-02\n",
      "Epoch: 5721 mean train loss:  1.28674441e-02, mean val. rec. loss:  1.15583830e-02\n",
      "Epoch: 5722 mean train loss:  1.28668389e-02, mean val. rec. loss:  1.15578795e-02\n",
      "Epoch: 5723 mean train loss:  1.28662225e-02, mean val. rec. loss:  1.15573613e-02\n",
      "Epoch: 5724 mean train loss:  1.28656061e-02, mean val. rec. loss:  1.15568590e-02\n",
      "Epoch: 5725 mean train loss:  1.28649990e-02, mean val. rec. loss:  1.15563543e-02\n",
      "Epoch: 5726 mean train loss:  1.28643892e-02, mean val. rec. loss:  1.15558407e-02\n",
      "Epoch: 5727 mean train loss:  1.28637737e-02, mean val. rec. loss:  1.15553440e-02\n",
      "Epoch: 5728 mean train loss:  1.28631610e-02, mean val. rec. loss:  1.15548280e-02\n",
      "Epoch: 5729 mean train loss:  1.28625484e-02, mean val. rec. loss:  1.15543325e-02\n",
      "Epoch: 5730 mean train loss:  1.28619357e-02, mean val. rec. loss:  1.15538131e-02\n",
      "Epoch: 5731 mean train loss:  1.28613249e-02, mean val. rec. loss:  1.15533210e-02\n",
      "Epoch: 5732 mean train loss:  1.28607113e-02, mean val. rec. loss:  1.15528073e-02\n",
      "Epoch: 5733 mean train loss:  1.28601043e-02, mean val. rec. loss:  1.15522992e-02\n",
      "Epoch: 5734 mean train loss:  1.28594879e-02, mean val. rec. loss:  1.15517901e-02\n",
      "Epoch: 5735 mean train loss:  1.28588724e-02, mean val. rec. loss:  1.15512843e-02\n",
      "Epoch: 5736 mean train loss:  1.28582644e-02, mean val. rec. loss:  1.15507820e-02\n",
      "Epoch: 5737 mean train loss:  1.28576527e-02, mean val. rec. loss:  1.15502819e-02\n",
      "Epoch: 5738 mean train loss:  1.28570400e-02, mean val. rec. loss:  1.15497716e-02\n",
      "Epoch: 5739 mean train loss:  1.28564292e-02, mean val. rec. loss:  1.15492591e-02\n",
      "Epoch: 5740 mean train loss:  1.28558166e-02, mean val. rec. loss:  1.15487601e-02\n",
      "Epoch: 5741 mean train loss:  1.28552039e-02, mean val. rec. loss:  1.15482521e-02\n",
      "Epoch: 5742 mean train loss:  1.28545978e-02, mean val. rec. loss:  1.15477520e-02\n",
      "Epoch: 5743 mean train loss:  1.28539804e-02, mean val. rec. loss:  1.15472360e-02\n",
      "Epoch: 5744 mean train loss:  1.28533650e-02, mean val. rec. loss:  1.15467360e-02\n",
      "Epoch: 5745 mean train loss:  1.28527588e-02, mean val. rec. loss:  1.15462313e-02\n",
      "Epoch: 5746 mean train loss:  1.28521499e-02, mean val. rec. loss:  1.15457244e-02\n",
      "Epoch: 5747 mean train loss:  1.28515391e-02, mean val. rec. loss:  1.15452176e-02\n",
      "Epoch: 5748 mean train loss:  1.28509274e-02, mean val. rec. loss:  1.15447107e-02\n",
      "Epoch: 5749 mean train loss:  1.28503157e-02, mean val. rec. loss:  1.15442061e-02\n",
      "Epoch: 5750 mean train loss:  1.28497049e-02, mean val. rec. loss:  1.15436958e-02\n",
      "Epoch: 5751 mean train loss:  1.28490885e-02, mean val. rec. loss:  1.15431855e-02\n",
      "Epoch: 5752 mean train loss:  1.28484730e-02, mean val. rec. loss:  1.15426888e-02\n",
      "Epoch: 5753 mean train loss:  1.28478650e-02, mean val. rec. loss:  1.15421796e-02\n",
      "Epoch: 5754 mean train loss:  1.28472589e-02, mean val. rec. loss:  1.15416750e-02\n",
      "Epoch: 5755 mean train loss:  1.28466425e-02, mean val. rec. loss:  1.15411625e-02\n",
      "Epoch: 5756 mean train loss:  1.28460326e-02, mean val. rec. loss:  1.15406624e-02\n",
      "Epoch: 5757 mean train loss:  1.28454209e-02, mean val. rec. loss:  1.15401623e-02\n",
      "Epoch: 5758 mean train loss:  1.28448092e-02, mean val. rec. loss:  1.15396497e-02\n",
      "Epoch: 5759 mean train loss:  1.28441974e-02, mean val. rec. loss:  1.15391406e-02\n",
      "Epoch: 5760 mean train loss:  1.28435866e-02, mean val. rec. loss:  1.15386360e-02\n",
      "Epoch: 5761 mean train loss:  1.28429712e-02, mean val. rec. loss:  1.15381347e-02\n",
      "Epoch: 5762 mean train loss:  1.28423641e-02, mean val. rec. loss:  1.15376290e-02\n",
      "Epoch: 5763 mean train loss:  1.28417570e-02, mean val. rec. loss:  1.15371278e-02\n",
      "Epoch: 5764 mean train loss:  1.28411406e-02, mean val. rec. loss:  1.15366095e-02\n",
      "Epoch: 5765 mean train loss:  1.28405308e-02, mean val. rec. loss:  1.15361129e-02\n",
      "Epoch: 5766 mean train loss:  1.28399190e-02, mean val. rec. loss:  1.15355946e-02\n",
      "Epoch: 5767 mean train loss:  1.28393082e-02, mean val. rec. loss:  1.15350991e-02\n",
      "Epoch: 5768 mean train loss:  1.28386974e-02, mean val. rec. loss:  1.15345843e-02\n",
      "Epoch: 5769 mean train loss:  1.28380866e-02, mean val. rec. loss:  1.15340819e-02\n",
      "Epoch: 5770 mean train loss:  1.28374786e-02, mean val. rec. loss:  1.15335762e-02\n",
      "Epoch: 5771 mean train loss:  1.28368650e-02, mean val. rec. loss:  1.15330659e-02\n",
      "Epoch: 5772 mean train loss:  1.28362487e-02, mean val. rec. loss:  1.15325726e-02\n",
      "Epoch: 5773 mean train loss:  1.28356425e-02, mean val. rec. loss:  1.15320612e-02\n",
      "Epoch: 5774 mean train loss:  1.28350354e-02, mean val. rec. loss:  1.15315486e-02\n",
      "Epoch: 5775 mean train loss:  1.28344163e-02, mean val. rec. loss:  1.15310474e-02\n",
      "Epoch: 5776 mean train loss:  1.28338129e-02, mean val. rec. loss:  1.15305371e-02\n",
      "Epoch: 5777 mean train loss:  1.28331947e-02, mean val. rec. loss:  1.15300382e-02\n",
      "Epoch: 5778 mean train loss:  1.28325913e-02, mean val. rec. loss:  1.15295245e-02\n",
      "Epoch: 5779 mean train loss:  1.28319768e-02, mean val. rec. loss:  1.15290198e-02\n",
      "Epoch: 5780 mean train loss:  1.28313623e-02, mean val. rec. loss:  1.15285175e-02\n",
      "Epoch: 5781 mean train loss:  1.28307552e-02, mean val. rec. loss:  1.15280117e-02\n",
      "Epoch: 5782 mean train loss:  1.28301481e-02, mean val. rec. loss:  1.15275128e-02\n",
      "Epoch: 5783 mean train loss:  1.28295336e-02, mean val. rec. loss:  1.15270036e-02\n",
      "Epoch: 5784 mean train loss:  1.28289228e-02, mean val. rec. loss:  1.15264979e-02\n",
      "Epoch: 5785 mean train loss:  1.28283120e-02, mean val. rec. loss:  1.15259921e-02\n",
      "Epoch: 5786 mean train loss:  1.28277012e-02, mean val. rec. loss:  1.15254864e-02\n",
      "Epoch: 5787 mean train loss:  1.28270904e-02, mean val. rec. loss:  1.15249727e-02\n",
      "Epoch: 5788 mean train loss:  1.28264843e-02, mean val. rec. loss:  1.15244658e-02\n",
      "Epoch: 5789 mean train loss:  1.28258697e-02, mean val. rec. loss:  1.15239555e-02\n",
      "Epoch: 5790 mean train loss:  1.28252552e-02, mean val. rec. loss:  1.15234475e-02\n",
      "Epoch: 5791 mean train loss:  1.28246491e-02, mean val. rec. loss:  1.15229497e-02\n",
      "Epoch: 5792 mean train loss:  1.28240420e-02, mean val. rec. loss:  1.15224360e-02\n",
      "Epoch: 5793 mean train loss:  1.28234238e-02, mean val. rec. loss:  1.15219336e-02\n",
      "Epoch: 5794 mean train loss:  1.28228204e-02, mean val. rec. loss:  1.15214211e-02\n",
      "Epoch: 5795 mean train loss:  1.28222022e-02, mean val. rec. loss:  1.15209221e-02\n",
      "Epoch: 5796 mean train loss:  1.28215988e-02, mean val. rec. loss:  1.15204141e-02\n",
      "Epoch: 5797 mean train loss:  1.28209815e-02, mean val. rec. loss:  1.15199095e-02\n",
      "Epoch: 5798 mean train loss:  1.28203754e-02, mean val. rec. loss:  1.15194037e-02\n",
      "Epoch: 5799 mean train loss:  1.28197692e-02, mean val. rec. loss:  1.15189014e-02\n",
      "Epoch: 5800 mean train loss:  1.28191538e-02, mean val. rec. loss:  1.15183877e-02\n",
      "Epoch: 5801 mean train loss:  1.28185392e-02, mean val. rec. loss:  1.15178797e-02\n",
      "Epoch: 5802 mean train loss:  1.28179303e-02, mean val. rec. loss:  1.15173739e-02\n",
      "Epoch: 5803 mean train loss:  1.28173195e-02, mean val. rec. loss:  1.15168659e-02\n",
      "Epoch: 5804 mean train loss:  1.28167087e-02, mean val. rec. loss:  1.15163624e-02\n",
      "Epoch: 5805 mean train loss:  1.28160979e-02, mean val. rec. loss:  1.15158566e-02\n",
      "Epoch: 5806 mean train loss:  1.28154880e-02, mean val. rec. loss:  1.15153486e-02\n",
      "Epoch: 5807 mean train loss:  1.28148819e-02, mean val. rec. loss:  1.15148485e-02\n",
      "Epoch: 5808 mean train loss:  1.28142748e-02, mean val. rec. loss:  1.15143416e-02\n",
      "Epoch: 5809 mean train loss:  1.28136603e-02, mean val. rec. loss:  1.15138325e-02\n",
      "Epoch: 5810 mean train loss:  1.28130458e-02, mean val. rec. loss:  1.15133358e-02\n",
      "Epoch: 5811 mean train loss:  1.28124396e-02, mean val. rec. loss:  1.15128233e-02\n",
      "Epoch: 5812 mean train loss:  1.28118298e-02, mean val. rec. loss:  1.15123096e-02\n",
      "Epoch: 5813 mean train loss:  1.28112199e-02, mean val. rec. loss:  1.15118061e-02\n",
      "Epoch: 5814 mean train loss:  1.28106082e-02, mean val. rec. loss:  1.15113071e-02\n",
      "Epoch: 5815 mean train loss:  1.28099992e-02, mean val. rec. loss:  1.15107878e-02\n",
      "Epoch: 5816 mean train loss:  1.28093838e-02, mean val. rec. loss:  1.15102865e-02\n",
      "Epoch: 5817 mean train loss:  1.28087776e-02, mean val. rec. loss:  1.15097774e-02\n",
      "Epoch: 5818 mean train loss:  1.28081715e-02, mean val. rec. loss:  1.15092682e-02\n",
      "Epoch: 5819 mean train loss:  1.28075579e-02, mean val. rec. loss:  1.15087636e-02\n",
      "Epoch: 5820 mean train loss:  1.28069434e-02, mean val. rec. loss:  1.15082556e-02\n",
      "Epoch: 5821 mean train loss:  1.28063326e-02, mean val. rec. loss:  1.15077487e-02\n",
      "Epoch: 5822 mean train loss:  1.28057218e-02, mean val. rec. loss:  1.15072430e-02\n",
      "Epoch: 5823 mean train loss:  1.28051119e-02, mean val. rec. loss:  1.15067372e-02\n",
      "Epoch: 5824 mean train loss:  1.28045020e-02, mean val. rec. loss:  1.15062314e-02\n",
      "Epoch: 5825 mean train loss:  1.28038912e-02, mean val. rec. loss:  1.15057246e-02\n",
      "Epoch: 5826 mean train loss:  1.28032851e-02, mean val. rec. loss:  1.15052222e-02\n",
      "Epoch: 5827 mean train loss:  1.28026790e-02, mean val. rec. loss:  1.15047165e-02\n",
      "Epoch: 5828 mean train loss:  1.28020654e-02, mean val. rec. loss:  1.15042062e-02\n",
      "Epoch: 5829 mean train loss:  1.28014499e-02, mean val. rec. loss:  1.15037038e-02\n",
      "Epoch: 5830 mean train loss:  1.28008484e-02, mean val. rec. loss:  1.15031901e-02\n",
      "Epoch: 5831 mean train loss:  1.28002302e-02, mean val. rec. loss:  1.15026889e-02\n",
      "Epoch: 5832 mean train loss:  1.27996278e-02, mean val. rec. loss:  1.15021775e-02\n",
      "Epoch: 5833 mean train loss:  1.27990104e-02, mean val. rec. loss:  1.15016751e-02\n",
      "Epoch: 5834 mean train loss:  1.27984080e-02, mean val. rec. loss:  1.15011614e-02\n",
      "Epoch: 5835 mean train loss:  1.27977944e-02, mean val. rec. loss:  1.15006568e-02\n",
      "Epoch: 5836 mean train loss:  1.27971799e-02, mean val. rec. loss:  1.15001522e-02\n",
      "Epoch: 5837 mean train loss:  1.27965728e-02, mean val. rec. loss:  1.14996476e-02\n",
      "Epoch: 5838 mean train loss:  1.27959658e-02, mean val. rec. loss:  1.14991396e-02\n",
      "Epoch: 5839 mean train loss:  1.27953559e-02, mean val. rec. loss:  1.14986315e-02\n",
      "Epoch: 5840 mean train loss:  1.27947460e-02, mean val. rec. loss:  1.14981224e-02\n",
      "Epoch: 5841 mean train loss:  1.27941352e-02, mean val. rec. loss:  1.14976166e-02\n",
      "Epoch: 5842 mean train loss:  1.27935244e-02, mean val. rec. loss:  1.14971131e-02\n",
      "Epoch: 5843 mean train loss:  1.27929155e-02, mean val. rec. loss:  1.14966040e-02\n",
      "Epoch: 5844 mean train loss:  1.27923010e-02, mean val. rec. loss:  1.14960914e-02\n",
      "Epoch: 5845 mean train loss:  1.27916864e-02, mean val. rec. loss:  1.14955925e-02\n",
      "Epoch: 5846 mean train loss:  1.27910794e-02, mean val. rec. loss:  1.14950845e-02\n",
      "Epoch: 5847 mean train loss:  1.27904742e-02, mean val. rec. loss:  1.14945764e-02\n",
      "Epoch: 5848 mean train loss:  1.27898596e-02, mean val. rec. loss:  1.14940729e-02\n",
      "Epoch: 5849 mean train loss:  1.27892488e-02, mean val. rec. loss:  1.14935604e-02\n",
      "Epoch: 5850 mean train loss:  1.27886399e-02, mean val. rec. loss:  1.14930501e-02\n",
      "Epoch: 5851 mean train loss:  1.27880300e-02, mean val. rec. loss:  1.14925466e-02\n",
      "Epoch: 5852 mean train loss:  1.27874202e-02, mean val. rec. loss:  1.14920397e-02\n",
      "Epoch: 5853 mean train loss:  1.27868131e-02, mean val. rec. loss:  1.14915340e-02\n",
      "Epoch: 5854 mean train loss:  1.27861967e-02, mean val. rec. loss:  1.14910203e-02\n",
      "Epoch: 5855 mean train loss:  1.27855840e-02, mean val. rec. loss:  1.14905168e-02\n",
      "Epoch: 5856 mean train loss:  1.27849770e-02, mean val. rec. loss:  1.14900133e-02\n",
      "Epoch: 5857 mean train loss:  1.27843708e-02, mean val. rec. loss:  1.14895030e-02\n",
      "Epoch: 5858 mean train loss:  1.27837610e-02, mean val. rec. loss:  1.14889973e-02\n",
      "Epoch: 5859 mean train loss:  1.27831511e-02, mean val. rec. loss:  1.14884881e-02\n",
      "Epoch: 5860 mean train loss:  1.27825384e-02, mean val. rec. loss:  1.14879812e-02\n",
      "Epoch: 5861 mean train loss:  1.27819295e-02, mean val. rec. loss:  1.14874755e-02\n",
      "Epoch: 5862 mean train loss:  1.27813178e-02, mean val. rec. loss:  1.14869675e-02\n",
      "Epoch: 5863 mean train loss:  1.27807051e-02, mean val. rec. loss:  1.14864549e-02\n",
      "Epoch: 5864 mean train loss:  1.27800887e-02, mean val. rec. loss:  1.14859559e-02\n",
      "Epoch: 5865 mean train loss:  1.27794835e-02, mean val. rec. loss:  1.14854445e-02\n",
      "Epoch: 5866 mean train loss:  1.27788764e-02, mean val. rec. loss:  1.14849308e-02\n",
      "Epoch: 5867 mean train loss:  1.27782582e-02, mean val. rec. loss:  1.14844273e-02\n",
      "Epoch: 5868 mean train loss:  1.27776558e-02, mean val. rec. loss:  1.14839137e-02\n",
      "Epoch: 5869 mean train loss:  1.27770366e-02, mean val. rec. loss:  1.14834136e-02\n",
      "Epoch: 5870 mean train loss:  1.27764351e-02, mean val. rec. loss:  1.14829010e-02\n",
      "Epoch: 5871 mean train loss:  1.27758169e-02, mean val. rec. loss:  1.14823941e-02\n",
      "Epoch: 5872 mean train loss:  1.27752098e-02, mean val. rec. loss:  1.14818861e-02\n",
      "Epoch: 5873 mean train loss:  1.27746027e-02, mean val. rec. loss:  1.14813701e-02\n",
      "Epoch: 5874 mean train loss:  1.27739882e-02, mean val. rec. loss:  1.14808644e-02\n",
      "Epoch: 5875 mean train loss:  1.27733737e-02, mean val. rec. loss:  1.14803632e-02\n",
      "Epoch: 5876 mean train loss:  1.27727666e-02, mean val. rec. loss:  1.14798552e-02\n",
      "Epoch: 5877 mean train loss:  1.27721558e-02, mean val. rec. loss:  1.14793471e-02\n",
      "Epoch: 5878 mean train loss:  1.27715459e-02, mean val. rec. loss:  1.14788391e-02\n",
      "Epoch: 5879 mean train loss:  1.27709351e-02, mean val. rec. loss:  1.14783334e-02\n",
      "Epoch: 5880 mean train loss:  1.27703253e-02, mean val. rec. loss:  1.14778197e-02\n",
      "Epoch: 5881 mean train loss:  1.27697098e-02, mean val. rec. loss:  1.14773082e-02\n",
      "Epoch: 5882 mean train loss:  1.27691037e-02, mean val. rec. loss:  1.14768093e-02\n",
      "Epoch: 5883 mean train loss:  1.27684966e-02, mean val. rec. loss:  1.14762979e-02\n",
      "Epoch: 5884 mean train loss:  1.27678802e-02, mean val. rec. loss:  1.14757887e-02\n",
      "Epoch: 5885 mean train loss:  1.27672666e-02, mean val. rec. loss:  1.14752830e-02\n",
      "Epoch: 5886 mean train loss:  1.27666642e-02, mean val. rec. loss:  1.14747727e-02\n",
      "Epoch: 5887 mean train loss:  1.27660459e-02, mean val. rec. loss:  1.14742692e-02\n",
      "Epoch: 5888 mean train loss:  1.27654426e-02, mean val. rec. loss:  1.14737600e-02\n",
      "Epoch: 5889 mean train loss:  1.27648234e-02, mean val. rec. loss:  1.14732531e-02\n",
      "Epoch: 5890 mean train loss:  1.27642173e-02, mean val. rec. loss:  1.14727451e-02\n",
      "Epoch: 5891 mean train loss:  1.27636102e-02, mean val. rec. loss:  1.14722405e-02\n",
      "Epoch: 5892 mean train loss:  1.27629947e-02, mean val. rec. loss:  1.14717223e-02\n",
      "Epoch: 5893 mean train loss:  1.27623793e-02, mean val. rec. loss:  1.14712188e-02\n",
      "Epoch: 5894 mean train loss:  1.27617741e-02, mean val. rec. loss:  1.14707108e-02\n",
      "Epoch: 5895 mean train loss:  1.27611633e-02, mean val. rec. loss:  1.14701993e-02\n",
      "Epoch: 5896 mean train loss:  1.27605516e-02, mean val. rec. loss:  1.14696902e-02\n",
      "Epoch: 5897 mean train loss:  1.27599398e-02, mean val. rec. loss:  1.14691822e-02\n",
      "Epoch: 5898 mean train loss:  1.27593300e-02, mean val. rec. loss:  1.14686741e-02\n",
      "Epoch: 5899 mean train loss:  1.27587182e-02, mean val. rec. loss:  1.14681593e-02\n",
      "Epoch: 5900 mean train loss:  1.27581037e-02, mean val. rec. loss:  1.14676604e-02\n",
      "Epoch: 5901 mean train loss:  1.27574966e-02, mean val. rec. loss:  1.14671455e-02\n",
      "Epoch: 5902 mean train loss:  1.27568905e-02, mean val. rec. loss:  1.14666398e-02\n",
      "Epoch: 5903 mean train loss:  1.27562750e-02, mean val. rec. loss:  1.14661227e-02\n",
      "Epoch: 5904 mean train loss:  1.27556633e-02, mean val. rec. loss:  1.14656192e-02\n",
      "Epoch: 5905 mean train loss:  1.27550516e-02, mean val. rec. loss:  1.14651169e-02\n",
      "Epoch: 5906 mean train loss:  1.27544408e-02, mean val. rec. loss:  1.14646032e-02\n",
      "Epoch: 5907 mean train loss:  1.27538300e-02, mean val. rec. loss:  1.14640906e-02\n",
      "Epoch: 5908 mean train loss:  1.27532192e-02, mean val. rec. loss:  1.14635826e-02\n",
      "Epoch: 5909 mean train loss:  1.27526028e-02, mean val. rec. loss:  1.14630780e-02\n",
      "Epoch: 5910 mean train loss:  1.27519966e-02, mean val. rec. loss:  1.14625688e-02\n",
      "Epoch: 5911 mean train loss:  1.27513886e-02, mean val. rec. loss:  1.14620653e-02\n",
      "Epoch: 5912 mean train loss:  1.27507741e-02, mean val. rec. loss:  1.14615482e-02\n",
      "Epoch: 5913 mean train loss:  1.27501577e-02, mean val. rec. loss:  1.14610368e-02\n",
      "Epoch: 5914 mean train loss:  1.27495469e-02, mean val. rec. loss:  1.14605277e-02\n",
      "Epoch: 5915 mean train loss:  1.27489352e-02, mean val. rec. loss:  1.14600162e-02\n",
      "Epoch: 5916 mean train loss:  1.27483244e-02, mean val. rec. loss:  1.14595071e-02\n",
      "Epoch: 5917 mean train loss:  1.27477127e-02, mean val. rec. loss:  1.14590070e-02\n",
      "Epoch: 5918 mean train loss:  1.27471056e-02, mean val. rec. loss:  1.14584910e-02\n",
      "Epoch: 5919 mean train loss:  1.27464967e-02, mean val. rec. loss:  1.14579785e-02\n",
      "Epoch: 5920 mean train loss:  1.27458821e-02, mean val. rec. loss:  1.14574682e-02\n",
      "Epoch: 5921 mean train loss:  1.27452658e-02, mean val. rec. loss:  1.14569568e-02\n",
      "Epoch: 5922 mean train loss:  1.27446578e-02, mean val. rec. loss:  1.14564544e-02\n",
      "Epoch: 5923 mean train loss:  1.27440460e-02, mean val. rec. loss:  1.14559475e-02\n",
      "Epoch: 5924 mean train loss:  1.27434343e-02, mean val. rec. loss:  1.14554316e-02\n",
      "Epoch: 5925 mean train loss:  1.27428226e-02, mean val. rec. loss:  1.14549179e-02\n",
      "Epoch: 5926 mean train loss:  1.27422118e-02, mean val. rec. loss:  1.14544098e-02\n",
      "Epoch: 5927 mean train loss:  1.27415954e-02, mean val. rec. loss:  1.14539030e-02\n",
      "Epoch: 5928 mean train loss:  1.27409874e-02, mean val. rec. loss:  1.14533938e-02\n",
      "Epoch: 5929 mean train loss:  1.27403794e-02, mean val. rec. loss:  1.14528869e-02\n",
      "Epoch: 5930 mean train loss:  1.27397630e-02, mean val. rec. loss:  1.14523698e-02\n",
      "Epoch: 5931 mean train loss:  1.27391475e-02, mean val. rec. loss:  1.14518584e-02\n",
      "Epoch: 5932 mean train loss:  1.27385358e-02, mean val. rec. loss:  1.14513470e-02\n",
      "Epoch: 5933 mean train loss:  1.27379231e-02, mean val. rec. loss:  1.14508390e-02\n",
      "Epoch: 5934 mean train loss:  1.27373114e-02, mean val. rec. loss:  1.14503275e-02\n",
      "Epoch: 5935 mean train loss:  1.27366997e-02, mean val. rec. loss:  1.14498195e-02\n",
      "Epoch: 5936 mean train loss:  1.27360870e-02, mean val. rec. loss:  1.14493160e-02\n",
      "Epoch: 5937 mean train loss:  1.27354781e-02, mean val. rec. loss:  1.14488023e-02\n",
      "Epoch: 5938 mean train loss:  1.27348692e-02, mean val. rec. loss:  1.14482909e-02\n",
      "Epoch: 5939 mean train loss:  1.27342546e-02, mean val. rec. loss:  1.14477772e-02\n",
      "Epoch: 5940 mean train loss:  1.27336373e-02, mean val. rec. loss:  1.14472681e-02\n",
      "Epoch: 5941 mean train loss:  1.27330340e-02, mean val. rec. loss:  1.14467544e-02\n",
      "Epoch: 5942 mean train loss:  1.27324139e-02, mean val. rec. loss:  1.14462486e-02\n",
      "Epoch: 5943 mean train loss:  1.27318086e-02, mean val. rec. loss:  1.14457338e-02\n",
      "Epoch: 5944 mean train loss:  1.27311876e-02, mean val. rec. loss:  1.14452326e-02\n",
      "Epoch: 5945 mean train loss:  1.27305833e-02, mean val. rec. loss:  1.14447246e-02\n",
      "Epoch: 5946 mean train loss:  1.27299669e-02, mean val. rec. loss:  1.14442018e-02\n",
      "Epoch: 5947 mean train loss:  1.27293496e-02, mean val. rec. loss:  1.14436960e-02\n",
      "Epoch: 5948 mean train loss:  1.27287425e-02, mean val. rec. loss:  1.14431892e-02\n",
      "Epoch: 5949 mean train loss:  1.27281336e-02, mean val. rec. loss:  1.14426721e-02\n",
      "Epoch: 5950 mean train loss:  1.27275172e-02, mean val. rec. loss:  1.14421708e-02\n",
      "Epoch: 5951 mean train loss:  1.27269036e-02, mean val. rec. loss:  1.14416469e-02\n",
      "Epoch: 5952 mean train loss:  1.27262919e-02, mean val. rec. loss:  1.14411457e-02\n",
      "Epoch: 5953 mean train loss:  1.27256783e-02, mean val. rec. loss:  1.14406264e-02\n",
      "Epoch: 5954 mean train loss:  1.27250666e-02, mean val. rec. loss:  1.14401195e-02\n",
      "Epoch: 5955 mean train loss:  1.27244576e-02, mean val. rec. loss:  1.14396058e-02\n",
      "Epoch: 5956 mean train loss:  1.27238403e-02, mean val. rec. loss:  1.14390887e-02\n",
      "Epoch: 5957 mean train loss:  1.27232221e-02, mean val. rec. loss:  1.14385875e-02\n",
      "Epoch: 5958 mean train loss:  1.27226141e-02, mean val. rec. loss:  1.14380749e-02\n",
      "Epoch: 5959 mean train loss:  1.27220061e-02, mean val. rec. loss:  1.14375567e-02\n",
      "Epoch: 5960 mean train loss:  1.27213841e-02, mean val. rec. loss:  1.14370509e-02\n",
      "Epoch: 5961 mean train loss:  1.27207789e-02, mean val. rec. loss:  1.14365361e-02\n",
      "Epoch: 5962 mean train loss:  1.27201578e-02, mean val. rec. loss:  1.14360315e-02\n",
      "Epoch: 5963 mean train loss:  1.27195536e-02, mean val. rec. loss:  1.14355099e-02\n",
      "Epoch: 5964 mean train loss:  1.27189362e-02, mean val. rec. loss:  1.14349984e-02\n",
      "Epoch: 5965 mean train loss:  1.27183180e-02, mean val. rec. loss:  1.14344916e-02\n",
      "Epoch: 5966 mean train loss:  1.27177091e-02, mean val. rec. loss:  1.14339801e-02\n",
      "Epoch: 5967 mean train loss:  1.27171001e-02, mean val. rec. loss:  1.14334755e-02\n",
      "Epoch: 5968 mean train loss:  1.27164819e-02, mean val. rec. loss:  1.14329618e-02\n",
      "Epoch: 5969 mean train loss:  1.27158674e-02, mean val. rec. loss:  1.14324493e-02\n",
      "Epoch: 5970 mean train loss:  1.27152547e-02, mean val. rec. loss:  1.14319367e-02\n",
      "Epoch: 5971 mean train loss:  1.27146402e-02, mean val. rec. loss:  1.14314275e-02\n",
      "Epoch: 5972 mean train loss:  1.27140275e-02, mean val. rec. loss:  1.14309161e-02\n",
      "Epoch: 5973 mean train loss:  1.27134130e-02, mean val. rec. loss:  1.14303979e-02\n",
      "Epoch: 5974 mean train loss:  1.27128041e-02, mean val. rec. loss:  1.14298842e-02\n",
      "Epoch: 5975 mean train loss:  1.27121858e-02, mean val. rec. loss:  1.14293682e-02\n",
      "Epoch: 5976 mean train loss:  1.27115694e-02, mean val. rec. loss:  1.14288557e-02\n",
      "Epoch: 5977 mean train loss:  1.27109586e-02, mean val. rec. loss:  1.14283477e-02\n",
      "Epoch: 5978 mean train loss:  1.27103450e-02, mean val. rec. loss:  1.14278396e-02\n",
      "Epoch: 5979 mean train loss:  1.27097296e-02, mean val. rec. loss:  1.14273226e-02\n",
      "Epoch: 5980 mean train loss:  1.27091169e-02, mean val. rec. loss:  1.14268043e-02\n",
      "Epoch: 5981 mean train loss:  1.27085024e-02, mean val. rec. loss:  1.14262997e-02\n",
      "Epoch: 5982 mean train loss:  1.27078879e-02, mean val. rec. loss:  1.14257849e-02\n",
      "Epoch: 5983 mean train loss:  1.27072771e-02, mean val. rec. loss:  1.14252667e-02\n",
      "Epoch: 5984 mean train loss:  1.27066598e-02, mean val. rec. loss:  1.14247541e-02\n",
      "Epoch: 5985 mean train loss:  1.27060406e-02, mean val. rec. loss:  1.14242472e-02\n",
      "Epoch: 5986 mean train loss:  1.27054316e-02, mean val. rec. loss:  1.14237346e-02\n",
      "Epoch: 5987 mean train loss:  1.27048208e-02, mean val. rec. loss:  1.14232187e-02\n",
      "Epoch: 5988 mean train loss:  1.27042054e-02, mean val. rec. loss:  1.14227061e-02\n",
      "Epoch: 5989 mean train loss:  1.27035918e-02, mean val. rec. loss:  1.14221947e-02\n",
      "Epoch: 5990 mean train loss:  1.27029773e-02, mean val. rec. loss:  1.14216799e-02\n",
      "Epoch: 5991 mean train loss:  1.27023628e-02, mean val. rec. loss:  1.14211639e-02\n",
      "Epoch: 5992 mean train loss:  1.27017426e-02, mean val. rec. loss:  1.14206457e-02\n",
      "Epoch: 5993 mean train loss:  1.27011244e-02, mean val. rec. loss:  1.14201320e-02\n",
      "Epoch: 5994 mean train loss:  1.27005155e-02, mean val. rec. loss:  1.14196251e-02\n",
      "Epoch: 5995 mean train loss:  1.26999028e-02, mean val. rec. loss:  1.14191126e-02\n",
      "Epoch: 5996 mean train loss:  1.26992846e-02, mean val. rec. loss:  1.14185932e-02\n",
      "Epoch: 5997 mean train loss:  1.26986700e-02, mean val. rec. loss:  1.14180863e-02\n",
      "Epoch: 5998 mean train loss:  1.26980536e-02, mean val. rec. loss:  1.14175794e-02\n",
      "Epoch: 5999 mean train loss:  1.26974382e-02, mean val. rec. loss:  1.14170601e-02\n",
      "Epoch: 6000 mean train loss:  1.26968237e-02, mean val. rec. loss:  1.14165452e-02\n",
      "Epoch: 6001 mean train loss:  1.26962119e-02, mean val. rec. loss:  1.14160247e-02\n",
      "Epoch: 6002 mean train loss:  1.26955937e-02, mean val. rec. loss:  1.14155111e-02\n",
      "Epoch: 6003 mean train loss:  1.26949736e-02, mean val. rec. loss:  1.14150030e-02\n",
      "Epoch: 6004 mean train loss:  1.26943619e-02, mean val. rec. loss:  1.14144882e-02\n",
      "Epoch: 6005 mean train loss:  1.26937501e-02, mean val. rec. loss:  1.14139734e-02\n",
      "Epoch: 6006 mean train loss:  1.26931337e-02, mean val. rec. loss:  1.14134586e-02\n",
      "Epoch: 6007 mean train loss:  1.26925192e-02, mean val. rec. loss:  1.14129437e-02\n",
      "Epoch: 6008 mean train loss:  1.26919047e-02, mean val. rec. loss:  1.14124312e-02\n",
      "Epoch: 6009 mean train loss:  1.26912883e-02, mean val. rec. loss:  1.14119163e-02\n",
      "Epoch: 6010 mean train loss:  1.26906719e-02, mean val. rec. loss:  1.14114015e-02\n",
      "Epoch: 6011 mean train loss:  1.26900518e-02, mean val. rec. loss:  1.14108833e-02\n",
      "Epoch: 6012 mean train loss:  1.26894317e-02, mean val. rec. loss:  1.14103662e-02\n",
      "Epoch: 6013 mean train loss:  1.26888209e-02, mean val. rec. loss:  1.14098593e-02\n",
      "Epoch: 6014 mean train loss:  1.26882092e-02, mean val. rec. loss:  1.14093388e-02\n",
      "Epoch: 6015 mean train loss:  1.26875844e-02, mean val. rec. loss:  1.14088297e-02\n",
      "Epoch: 6016 mean train loss:  1.26869764e-02, mean val. rec. loss:  1.14083058e-02\n",
      "Epoch: 6017 mean train loss:  1.26863516e-02, mean val. rec. loss:  1.14078000e-02\n",
      "Epoch: 6018 mean train loss:  1.26857446e-02, mean val. rec. loss:  1.14072795e-02\n",
      "Epoch: 6019 mean train loss:  1.26851189e-02, mean val. rec. loss:  1.14067670e-02\n",
      "Epoch: 6020 mean train loss:  1.26845071e-02, mean val. rec. loss:  1.14062533e-02\n",
      "Epoch: 6021 mean train loss:  1.26838945e-02, mean val. rec. loss:  1.14057430e-02\n",
      "Epoch: 6022 mean train loss:  1.26832734e-02, mean val. rec. loss:  1.14052191e-02\n",
      "Epoch: 6023 mean train loss:  1.26826533e-02, mean val. rec. loss:  1.14047099e-02\n",
      "Epoch: 6024 mean train loss:  1.26820407e-02, mean val. rec. loss:  1.14041940e-02\n",
      "Epoch: 6025 mean train loss:  1.26814243e-02, mean val. rec. loss:  1.14036780e-02\n",
      "Epoch: 6026 mean train loss:  1.26808079e-02, mean val. rec. loss:  1.14031620e-02\n",
      "Epoch: 6027 mean train loss:  1.26801906e-02, mean val. rec. loss:  1.14026472e-02\n",
      "Epoch: 6028 mean train loss:  1.26795733e-02, mean val. rec. loss:  1.14021279e-02\n",
      "Epoch: 6029 mean train loss:  1.26789532e-02, mean val. rec. loss:  1.14016198e-02\n",
      "Epoch: 6030 mean train loss:  1.26783396e-02, mean val. rec. loss:  1.14011005e-02\n",
      "Epoch: 6031 mean train loss:  1.26777260e-02, mean val. rec. loss:  1.14005857e-02\n",
      "Epoch: 6032 mean train loss:  1.26771059e-02, mean val. rec. loss:  1.14000674e-02\n",
      "Epoch: 6033 mean train loss:  1.26764848e-02, mean val. rec. loss:  1.13995560e-02\n",
      "Epoch: 6034 mean train loss:  1.26758759e-02, mean val. rec. loss:  1.13990344e-02\n",
      "Epoch: 6035 mean train loss:  1.26752493e-02, mean val. rec. loss:  1.13985241e-02\n",
      "Epoch: 6036 mean train loss:  1.26746394e-02, mean val. rec. loss:  1.13980025e-02\n",
      "Epoch: 6037 mean train loss:  1.26740146e-02, mean val. rec. loss:  1.13974888e-02\n",
      "Epoch: 6038 mean train loss:  1.26734010e-02, mean val. rec. loss:  1.13969728e-02\n",
      "Epoch: 6039 mean train loss:  1.26727874e-02, mean val. rec. loss:  1.13964512e-02\n",
      "Epoch: 6040 mean train loss:  1.26721655e-02, mean val. rec. loss:  1.13959341e-02\n",
      "Epoch: 6041 mean train loss:  1.26715435e-02, mean val. rec. loss:  1.13954238e-02\n",
      "Epoch: 6042 mean train loss:  1.26709299e-02, mean val. rec. loss:  1.13949033e-02\n",
      "Epoch: 6043 mean train loss:  1.26703107e-02, mean val. rec. loss:  1.13943851e-02\n",
      "Epoch: 6044 mean train loss:  1.26696925e-02, mean val. rec. loss:  1.13938702e-02\n",
      "Epoch: 6045 mean train loss:  1.26690761e-02, mean val. rec. loss:  1.13933554e-02\n",
      "Epoch: 6046 mean train loss:  1.26684579e-02, mean val. rec. loss:  1.13928383e-02\n",
      "Epoch: 6047 mean train loss:  1.26678377e-02, mean val. rec. loss:  1.13923156e-02\n",
      "Epoch: 6048 mean train loss:  1.26672158e-02, mean val. rec. loss:  1.13917962e-02\n",
      "Epoch: 6049 mean train loss:  1.26666012e-02, mean val. rec. loss:  1.13912870e-02\n",
      "Epoch: 6050 mean train loss:  1.26659886e-02, mean val. rec. loss:  1.13907722e-02\n",
      "Epoch: 6051 mean train loss:  1.26653648e-02, mean val. rec. loss:  1.13902574e-02\n",
      "Epoch: 6052 mean train loss:  1.26647465e-02, mean val. rec. loss:  1.13897335e-02\n",
      "Epoch: 6053 mean train loss:  1.26641273e-02, mean val. rec. loss:  1.13892096e-02\n",
      "Epoch: 6054 mean train loss:  1.26635100e-02, mean val. rec. loss:  1.13886970e-02\n",
      "Epoch: 6055 mean train loss:  1.26628899e-02, mean val. rec. loss:  1.13881890e-02\n",
      "Epoch: 6056 mean train loss:  1.26622707e-02, mean val. rec. loss:  1.13876595e-02\n",
      "Epoch: 6057 mean train loss:  1.26616478e-02, mean val. rec. loss:  1.13871458e-02\n",
      "Epoch: 6058 mean train loss:  1.26610333e-02, mean val. rec. loss:  1.13866287e-02\n",
      "Epoch: 6059 mean train loss:  1.26604169e-02, mean val. rec. loss:  1.13861048e-02\n",
      "Epoch: 6060 mean train loss:  1.26597949e-02, mean val. rec. loss:  1.13855899e-02\n",
      "Epoch: 6061 mean train loss:  1.26591702e-02, mean val. rec. loss:  1.13850717e-02\n",
      "Epoch: 6062 mean train loss:  1.26585510e-02, mean val. rec. loss:  1.13845558e-02\n",
      "Epoch: 6063 mean train loss:  1.26579309e-02, mean val. rec. loss:  1.13840387e-02\n",
      "Epoch: 6064 mean train loss:  1.26573126e-02, mean val. rec. loss:  1.13835193e-02\n",
      "Epoch: 6065 mean train loss:  1.26566925e-02, mean val. rec. loss:  1.13829988e-02\n",
      "Epoch: 6066 mean train loss:  1.26560771e-02, mean val. rec. loss:  1.13824874e-02\n",
      "Epoch: 6067 mean train loss:  1.26554598e-02, mean val. rec. loss:  1.13819669e-02\n",
      "Epoch: 6068 mean train loss:  1.26548359e-02, mean val. rec. loss:  1.13814464e-02\n",
      "Epoch: 6069 mean train loss:  1.26542121e-02, mean val. rec. loss:  1.13809361e-02\n",
      "Epoch: 6070 mean train loss:  1.26535957e-02, mean val. rec. loss:  1.13804111e-02\n",
      "Epoch: 6071 mean train loss:  1.26529765e-02, mean val. rec. loss:  1.13798872e-02\n",
      "Epoch: 6072 mean train loss:  1.26523574e-02, mean val. rec. loss:  1.13793712e-02\n",
      "Epoch: 6073 mean train loss:  1.26517363e-02, mean val. rec. loss:  1.13788598e-02\n",
      "Epoch: 6074 mean train loss:  1.26511153e-02, mean val. rec. loss:  1.13783291e-02\n",
      "Epoch: 6075 mean train loss:  1.26504914e-02, mean val. rec. loss:  1.13778143e-02\n",
      "Epoch: 6076 mean train loss:  1.26498741e-02, mean val. rec. loss:  1.13772960e-02\n",
      "Epoch: 6077 mean train loss:  1.26492587e-02, mean val. rec. loss:  1.13767676e-02\n",
      "Epoch: 6078 mean train loss:  1.26486320e-02, mean val. rec. loss:  1.13762516e-02\n",
      "Epoch: 6079 mean train loss:  1.26480073e-02, mean val. rec. loss:  1.13757300e-02\n",
      "Epoch: 6080 mean train loss:  1.26473872e-02, mean val. rec. loss:  1.13752095e-02\n",
      "Epoch: 6081 mean train loss:  1.26467661e-02, mean val. rec. loss:  1.13746913e-02\n",
      "Epoch: 6082 mean train loss:  1.26461442e-02, mean val. rec. loss:  1.13741708e-02\n",
      "Epoch: 6083 mean train loss:  1.26455231e-02, mean val. rec. loss:  1.13736537e-02\n",
      "Epoch: 6084 mean train loss:  1.26449021e-02, mean val. rec. loss:  1.13731343e-02\n",
      "Epoch: 6085 mean train loss:  1.26442848e-02, mean val. rec. loss:  1.13726207e-02\n",
      "Epoch: 6086 mean train loss:  1.26436674e-02, mean val. rec. loss:  1.13721013e-02\n",
      "Epoch: 6087 mean train loss:  1.26430417e-02, mean val. rec. loss:  1.13715774e-02\n",
      "Epoch: 6088 mean train loss:  1.26424160e-02, mean val. rec. loss:  1.13710614e-02\n",
      "Epoch: 6089 mean train loss:  1.26418025e-02, mean val. rec. loss:  1.13705353e-02\n",
      "Epoch: 6090 mean train loss:  1.26411730e-02, mean val. rec. loss:  1.13700204e-02\n",
      "Epoch: 6091 mean train loss:  1.26405585e-02, mean val. rec. loss:  1.13694954e-02\n",
      "Epoch: 6092 mean train loss:  1.26399291e-02, mean val. rec. loss:  1.13689817e-02\n",
      "Epoch: 6093 mean train loss:  1.26393146e-02, mean val. rec. loss:  1.13684533e-02\n",
      "Epoch: 6094 mean train loss:  1.26386889e-02, mean val. rec. loss:  1.13679328e-02\n",
      "Epoch: 6095 mean train loss:  1.26380613e-02, mean val. rec. loss:  1.13674157e-02\n",
      "Epoch: 6096 mean train loss:  1.26374431e-02, mean val. rec. loss:  1.13668963e-02\n",
      "Epoch: 6097 mean train loss:  1.26368248e-02, mean val. rec. loss:  1.13663736e-02\n",
      "Epoch: 6098 mean train loss:  1.26362028e-02, mean val. rec. loss:  1.13658542e-02\n",
      "Epoch: 6099 mean train loss:  1.26355809e-02, mean val. rec. loss:  1.13653292e-02\n",
      "Epoch: 6100 mean train loss:  1.26349580e-02, mean val. rec. loss:  1.13648098e-02\n",
      "Epoch: 6101 mean train loss:  1.26343351e-02, mean val. rec. loss:  1.13642905e-02\n",
      "Epoch: 6102 mean train loss:  1.26337122e-02, mean val. rec. loss:  1.13637654e-02\n",
      "Epoch: 6103 mean train loss:  1.26330855e-02, mean val. rec. loss:  1.13632393e-02\n",
      "Epoch: 6104 mean train loss:  1.26324571e-02, mean val. rec. loss:  1.13627267e-02\n",
      "Epoch: 6105 mean train loss:  1.26318388e-02, mean val. rec. loss:  1.13622039e-02\n",
      "Epoch: 6106 mean train loss:  1.26312196e-02, mean val. rec. loss:  1.13616812e-02\n",
      "Epoch: 6107 mean train loss:  1.26305921e-02, mean val. rec. loss:  1.13611664e-02\n",
      "Epoch: 6108 mean train loss:  1.26299682e-02, mean val. rec. loss:  1.13606379e-02\n",
      "Epoch: 6109 mean train loss:  1.26293444e-02, mean val. rec. loss:  1.13601095e-02\n",
      "Epoch: 6110 mean train loss:  1.26287215e-02, mean val. rec. loss:  1.13595947e-02\n",
      "Epoch: 6111 mean train loss:  1.26280967e-02, mean val. rec. loss:  1.13590685e-02\n",
      "Epoch: 6112 mean train loss:  1.26274766e-02, mean val. rec. loss:  1.13585525e-02\n",
      "Epoch: 6113 mean train loss:  1.26268491e-02, mean val. rec. loss:  1.13580207e-02\n",
      "Epoch: 6114 mean train loss:  1.26262224e-02, mean val. rec. loss:  1.13575036e-02\n",
      "Epoch: 6115 mean train loss:  1.26256014e-02, mean val. rec. loss:  1.13569808e-02\n",
      "Epoch: 6116 mean train loss:  1.26249813e-02, mean val. rec. loss:  1.13564581e-02\n",
      "Epoch: 6117 mean train loss:  1.26243575e-02, mean val. rec. loss:  1.13559365e-02\n",
      "Epoch: 6118 mean train loss:  1.26237336e-02, mean val. rec. loss:  1.13554126e-02\n",
      "Epoch: 6119 mean train loss:  1.26231079e-02, mean val. rec. loss:  1.13548909e-02\n",
      "Epoch: 6120 mean train loss:  1.26224841e-02, mean val. rec. loss:  1.13543670e-02\n",
      "Epoch: 6121 mean train loss:  1.26218556e-02, mean val. rec. loss:  1.13538363e-02\n",
      "Epoch: 6122 mean train loss:  1.26212271e-02, mean val. rec. loss:  1.13533238e-02\n",
      "Epoch: 6123 mean train loss:  1.26206051e-02, mean val. rec. loss:  1.13527942e-02\n",
      "Epoch: 6124 mean train loss:  1.26199832e-02, mean val. rec. loss:  1.13522714e-02\n",
      "Epoch: 6125 mean train loss:  1.26193556e-02, mean val. rec. loss:  1.13517407e-02\n",
      "Epoch: 6126 mean train loss:  1.26187299e-02, mean val. rec. loss:  1.13512225e-02\n",
      "Epoch: 6127 mean train loss:  1.26181042e-02, mean val. rec. loss:  1.13507043e-02\n",
      "Epoch: 6128 mean train loss:  1.26174785e-02, mean val. rec. loss:  1.13501759e-02\n",
      "Epoch: 6129 mean train loss:  1.26168528e-02, mean val. rec. loss:  1.13496474e-02\n",
      "Epoch: 6130 mean train loss:  1.26162271e-02, mean val. rec. loss:  1.13491224e-02\n",
      "Epoch: 6131 mean train loss:  1.26155968e-02, mean val. rec. loss:  1.13486030e-02\n",
      "Epoch: 6132 mean train loss:  1.26149757e-02, mean val. rec. loss:  1.13480814e-02\n",
      "Epoch: 6133 mean train loss:  1.26143538e-02, mean val. rec. loss:  1.13475632e-02\n",
      "Epoch: 6134 mean train loss:  1.26137234e-02, mean val. rec. loss:  1.13470268e-02\n",
      "Epoch: 6135 mean train loss:  1.26130968e-02, mean val. rec. loss:  1.13465131e-02\n",
      "Epoch: 6136 mean train loss:  1.26124702e-02, mean val. rec. loss:  1.13459779e-02\n",
      "Epoch: 6137 mean train loss:  1.26118435e-02, mean val. rec. loss:  1.13454653e-02\n",
      "Epoch: 6138 mean train loss:  1.26112178e-02, mean val. rec. loss:  1.13449301e-02\n",
      "Epoch: 6139 mean train loss:  1.26105912e-02, mean val. rec. loss:  1.13444119e-02\n",
      "Epoch: 6140 mean train loss:  1.26099683e-02, mean val. rec. loss:  1.13438846e-02\n",
      "Epoch: 6141 mean train loss:  1.26093370e-02, mean val. rec. loss:  1.13433527e-02\n",
      "Epoch: 6142 mean train loss:  1.26087067e-02, mean val. rec. loss:  1.13428379e-02\n",
      "Epoch: 6143 mean train loss:  1.26080828e-02, mean val. rec. loss:  1.13423061e-02\n",
      "Epoch: 6144 mean train loss:  1.26074562e-02, mean val. rec. loss:  1.13417731e-02\n",
      "Epoch: 6145 mean train loss:  1.26068305e-02, mean val. rec. loss:  1.13412515e-02\n",
      "Epoch: 6146 mean train loss:  1.26062020e-02, mean val. rec. loss:  1.13407321e-02\n",
      "Epoch: 6147 mean train loss:  1.26055735e-02, mean val. rec. loss:  1.13402025e-02\n",
      "Epoch: 6148 mean train loss:  1.26049469e-02, mean val. rec. loss:  1.13396775e-02\n",
      "Epoch: 6149 mean train loss:  1.26043240e-02, mean val. rec. loss:  1.13391445e-02\n",
      "Epoch: 6150 mean train loss:  1.26036909e-02, mean val. rec. loss:  1.13386184e-02\n",
      "Epoch: 6151 mean train loss:  1.26030596e-02, mean val. rec. loss:  1.13380979e-02\n",
      "Epoch: 6152 mean train loss:  1.26024348e-02, mean val. rec. loss:  1.13375751e-02\n",
      "Epoch: 6153 mean train loss:  1.26018110e-02, mean val. rec. loss:  1.13370444e-02\n",
      "Epoch: 6154 mean train loss:  1.26011825e-02, mean val. rec. loss:  1.13365205e-02\n",
      "Epoch: 6155 mean train loss:  1.26005549e-02, mean val. rec. loss:  1.13359943e-02\n",
      "Epoch: 6156 mean train loss:  1.25999264e-02, mean val. rec. loss:  1.13354682e-02\n",
      "Epoch: 6157 mean train loss:  1.25992980e-02, mean val. rec. loss:  1.13349397e-02\n",
      "Epoch: 6158 mean train loss:  1.25986648e-02, mean val. rec. loss:  1.13344068e-02\n",
      "Epoch: 6159 mean train loss:  1.25980317e-02, mean val. rec. loss:  1.13338795e-02\n",
      "Epoch: 6160 mean train loss:  1.25974078e-02, mean val. rec. loss:  1.13333590e-02\n",
      "Epoch: 6161 mean train loss:  1.25967821e-02, mean val. rec. loss:  1.13328317e-02\n",
      "Epoch: 6162 mean train loss:  1.25961490e-02, mean val. rec. loss:  1.13323112e-02\n",
      "Epoch: 6163 mean train loss:  1.25955196e-02, mean val. rec. loss:  1.13317782e-02\n",
      "Epoch: 6164 mean train loss:  1.25948902e-02, mean val. rec. loss:  1.13312441e-02\n",
      "Epoch: 6165 mean train loss:  1.25942598e-02, mean val. rec. loss:  1.13307236e-02\n",
      "Epoch: 6166 mean train loss:  1.25936304e-02, mean val. rec. loss:  1.13301975e-02\n",
      "Epoch: 6167 mean train loss:  1.25930047e-02, mean val. rec. loss:  1.13296622e-02\n",
      "Epoch: 6168 mean train loss:  1.25923725e-02, mean val. rec. loss:  1.13291349e-02\n",
      "Epoch: 6169 mean train loss:  1.25917375e-02, mean val. rec. loss:  1.13286110e-02\n",
      "Epoch: 6170 mean train loss:  1.25911108e-02, mean val. rec. loss:  1.13280871e-02\n",
      "Epoch: 6171 mean train loss:  1.25904851e-02, mean val. rec. loss:  1.13275576e-02\n",
      "Epoch: 6172 mean train loss:  1.25898548e-02, mean val. rec. loss:  1.13270303e-02\n",
      "Epoch: 6173 mean train loss:  1.25892244e-02, mean val. rec. loss:  1.13265007e-02\n",
      "Epoch: 6174 mean train loss:  1.25885941e-02, mean val. rec. loss:  1.13259745e-02\n",
      "Epoch: 6175 mean train loss:  1.25879628e-02, mean val. rec. loss:  1.13254461e-02\n",
      "Epoch: 6176 mean train loss:  1.25873334e-02, mean val. rec. loss:  1.13249143e-02\n",
      "Epoch: 6177 mean train loss:  1.25866974e-02, mean val. rec. loss:  1.13243824e-02\n",
      "Epoch: 6178 mean train loss:  1.25860634e-02, mean val. rec. loss:  1.13238631e-02\n",
      "Epoch: 6179 mean train loss:  1.25854358e-02, mean val. rec. loss:  1.13233301e-02\n",
      "Epoch: 6180 mean train loss:  1.25848083e-02, mean val. rec. loss:  1.13227949e-02\n",
      "Epoch: 6181 mean train loss:  1.25841677e-02, mean val. rec. loss:  1.13222676e-02\n",
      "Epoch: 6182 mean train loss:  1.25835457e-02, mean val. rec. loss:  1.13217357e-02\n",
      "Epoch: 6183 mean train loss:  1.25829051e-02, mean val. rec. loss:  1.13212118e-02\n",
      "Epoch: 6184 mean train loss:  1.25822822e-02, mean val. rec. loss:  1.13206766e-02\n",
      "Epoch: 6185 mean train loss:  1.25816425e-02, mean val. rec. loss:  1.13201493e-02\n",
      "Epoch: 6186 mean train loss:  1.25810131e-02, mean val. rec. loss:  1.13196197e-02\n",
      "Epoch: 6187 mean train loss:  1.25803856e-02, mean val. rec. loss:  1.13190958e-02\n",
      "Epoch: 6188 mean train loss:  1.25797487e-02, mean val. rec. loss:  1.13185561e-02\n",
      "Epoch: 6189 mean train loss:  1.25791118e-02, mean val. rec. loss:  1.13180254e-02\n",
      "Epoch: 6190 mean train loss:  1.25784796e-02, mean val. rec. loss:  1.13174935e-02\n",
      "Epoch: 6191 mean train loss:  1.25778474e-02, mean val. rec. loss:  1.13169651e-02\n",
      "Epoch: 6192 mean train loss:  1.25772152e-02, mean val. rec. loss:  1.13164332e-02\n",
      "Epoch: 6193 mean train loss:  1.25765811e-02, mean val. rec. loss:  1.13159025e-02\n",
      "Epoch: 6194 mean train loss:  1.25759480e-02, mean val. rec. loss:  1.13153809e-02\n",
      "Epoch: 6195 mean train loss:  1.25753195e-02, mean val. rec. loss:  1.13148457e-02\n",
      "Epoch: 6196 mean train loss:  1.25746910e-02, mean val. rec. loss:  1.13143138e-02\n",
      "Epoch: 6197 mean train loss:  1.25740522e-02, mean val. rec. loss:  1.13137786e-02\n",
      "Epoch: 6198 mean train loss:  1.25734154e-02, mean val. rec. loss:  1.13132490e-02\n",
      "Epoch: 6199 mean train loss:  1.25727860e-02, mean val. rec. loss:  1.13127206e-02\n",
      "Epoch: 6200 mean train loss:  1.25721519e-02, mean val. rec. loss:  1.13121956e-02\n",
      "Epoch: 6201 mean train loss:  1.25715178e-02, mean val. rec. loss:  1.13116569e-02\n",
      "Epoch: 6202 mean train loss:  1.25708837e-02, mean val. rec. loss:  1.13111217e-02\n",
      "Epoch: 6203 mean train loss:  1.25702496e-02, mean val. rec. loss:  1.13105865e-02\n",
      "Epoch: 6204 mean train loss:  1.25696109e-02, mean val. rec. loss:  1.13100603e-02\n",
      "Epoch: 6205 mean train loss:  1.25689815e-02, mean val. rec. loss:  1.13095273e-02\n",
      "Epoch: 6206 mean train loss:  1.25683511e-02, mean val. rec. loss:  1.13090034e-02\n",
      "Epoch: 6207 mean train loss:  1.25677115e-02, mean val. rec. loss:  1.13084637e-02\n",
      "Epoch: 6208 mean train loss:  1.25670728e-02, mean val. rec. loss:  1.13079307e-02\n",
      "Epoch: 6209 mean train loss:  1.25664377e-02, mean val. rec. loss:  1.13073966e-02\n",
      "Epoch: 6210 mean train loss:  1.25658046e-02, mean val. rec. loss:  1.13068670e-02\n",
      "Epoch: 6211 mean train loss:  1.25651677e-02, mean val. rec. loss:  1.13063352e-02\n",
      "Epoch: 6212 mean train loss:  1.25645327e-02, mean val. rec. loss:  1.13058124e-02\n",
      "Epoch: 6213 mean train loss:  1.25639014e-02, mean val. rec. loss:  1.13052704e-02\n",
      "Epoch: 6214 mean train loss:  1.25632692e-02, mean val. rec. loss:  1.13047385e-02\n",
      "Epoch: 6215 mean train loss:  1.25626296e-02, mean val. rec. loss:  1.13041999e-02\n",
      "Epoch: 6216 mean train loss:  1.25619899e-02, mean val. rec. loss:  1.13036692e-02\n",
      "Epoch: 6217 mean train loss:  1.25613577e-02, mean val. rec. loss:  1.13031362e-02\n",
      "Epoch: 6218 mean train loss:  1.25607227e-02, mean val. rec. loss:  1.13026101e-02\n",
      "Epoch: 6219 mean train loss:  1.25600858e-02, mean val. rec. loss:  1.13020680e-02\n",
      "Epoch: 6220 mean train loss:  1.25594499e-02, mean val. rec. loss:  1.13015305e-02\n",
      "Epoch: 6221 mean train loss:  1.25588130e-02, mean val. rec. loss:  1.13010055e-02\n",
      "Epoch: 6222 mean train loss:  1.25581761e-02, mean val. rec. loss:  1.13004714e-02\n",
      "Epoch: 6223 mean train loss:  1.25575439e-02, mean val. rec. loss:  1.12999430e-02\n",
      "Epoch: 6224 mean train loss:  1.25569015e-02, mean val. rec. loss:  1.12994009e-02\n",
      "Epoch: 6225 mean train loss:  1.25562627e-02, mean val. rec. loss:  1.12988714e-02\n",
      "Epoch: 6226 mean train loss:  1.25556277e-02, mean val. rec. loss:  1.12983350e-02\n",
      "Epoch: 6227 mean train loss:  1.25549909e-02, mean val. rec. loss:  1.12977975e-02\n",
      "Epoch: 6228 mean train loss:  1.25543540e-02, mean val. rec. loss:  1.12972645e-02\n",
      "Epoch: 6229 mean train loss:  1.25537153e-02, mean val. rec. loss:  1.12967293e-02\n",
      "Epoch: 6230 mean train loss:  1.25530775e-02, mean val. rec. loss:  1.12961986e-02\n",
      "Epoch: 6231 mean train loss:  1.25524397e-02, mean val. rec. loss:  1.12956577e-02\n",
      "Epoch: 6232 mean train loss:  1.25517981e-02, mean val. rec. loss:  1.12951281e-02\n",
      "Epoch: 6233 mean train loss:  1.25511631e-02, mean val. rec. loss:  1.12945883e-02\n",
      "Epoch: 6234 mean train loss:  1.25505290e-02, mean val. rec. loss:  1.12940531e-02\n",
      "Epoch: 6235 mean train loss:  1.25498875e-02, mean val. rec. loss:  1.12935099e-02\n",
      "Epoch: 6236 mean train loss:  1.25492488e-02, mean val. rec. loss:  1.12929792e-02\n",
      "Epoch: 6237 mean train loss:  1.25486091e-02, mean val. rec. loss:  1.12924485e-02\n",
      "Epoch: 6238 mean train loss:  1.25479695e-02, mean val. rec. loss:  1.12919099e-02\n",
      "Epoch: 6239 mean train loss:  1.25473317e-02, mean val. rec. loss:  1.12913712e-02\n",
      "Epoch: 6240 mean train loss:  1.25466929e-02, mean val. rec. loss:  1.12908337e-02\n",
      "Epoch: 6241 mean train loss:  1.25460486e-02, mean val. rec. loss:  1.12903019e-02\n",
      "Epoch: 6242 mean train loss:  1.25454118e-02, mean val. rec. loss:  1.12897667e-02\n",
      "Epoch: 6243 mean train loss:  1.25447777e-02, mean val. rec. loss:  1.12892371e-02\n",
      "Epoch: 6244 mean train loss:  1.25441334e-02, mean val. rec. loss:  1.12886871e-02\n",
      "Epoch: 6245 mean train loss:  1.25434937e-02, mean val. rec. loss:  1.12881609e-02\n",
      "Epoch: 6246 mean train loss:  1.25428531e-02, mean val. rec. loss:  1.12876144e-02\n",
      "Epoch: 6247 mean train loss:  1.25422134e-02, mean val. rec. loss:  1.12870871e-02\n",
      "Epoch: 6248 mean train loss:  1.25415728e-02, mean val. rec. loss:  1.12865371e-02\n",
      "Epoch: 6249 mean train loss:  1.25409323e-02, mean val. rec. loss:  1.12860064e-02\n",
      "Epoch: 6250 mean train loss:  1.25402954e-02, mean val. rec. loss:  1.12854655e-02\n",
      "Epoch: 6251 mean train loss:  1.25396501e-02, mean val. rec. loss:  1.12849246e-02\n",
      "Epoch: 6252 mean train loss:  1.25390058e-02, mean val. rec. loss:  1.12843939e-02\n",
      "Epoch: 6253 mean train loss:  1.25383680e-02, mean val. rec. loss:  1.12838541e-02\n",
      "Epoch: 6254 mean train loss:  1.25377311e-02, mean val. rec. loss:  1.12833098e-02\n",
      "Epoch: 6255 mean train loss:  1.25370822e-02, mean val. rec. loss:  1.12827768e-02\n",
      "Epoch: 6256 mean train loss:  1.25364490e-02, mean val. rec. loss:  1.12822325e-02\n",
      "Epoch: 6257 mean train loss:  1.25357991e-02, mean val. rec. loss:  1.12816995e-02\n",
      "Epoch: 6258 mean train loss:  1.25351641e-02, mean val. rec. loss:  1.12811530e-02\n",
      "Epoch: 6259 mean train loss:  1.25345198e-02, mean val. rec. loss:  1.12806177e-02\n",
      "Epoch: 6260 mean train loss:  1.25338727e-02, mean val. rec. loss:  1.12800825e-02\n",
      "Epoch: 6261 mean train loss:  1.25332349e-02, mean val. rec. loss:  1.12795450e-02\n",
      "Epoch: 6262 mean train loss:  1.25325962e-02, mean val. rec. loss:  1.12790132e-02\n",
      "Epoch: 6263 mean train loss:  1.25319481e-02, mean val. rec. loss:  1.12784722e-02\n",
      "Epoch: 6264 mean train loss:  1.25313057e-02, mean val. rec. loss:  1.12779336e-02\n",
      "Epoch: 6265 mean train loss:  1.25306632e-02, mean val. rec. loss:  1.12773950e-02\n",
      "Epoch: 6266 mean train loss:  1.25300198e-02, mean val. rec. loss:  1.12768575e-02\n",
      "Epoch: 6267 mean train loss:  1.25293774e-02, mean val. rec. loss:  1.12763109e-02\n",
      "Epoch: 6268 mean train loss:  1.25287377e-02, mean val. rec. loss:  1.12757688e-02\n",
      "Epoch: 6269 mean train loss:  1.25280906e-02, mean val. rec. loss:  1.12752245e-02\n",
      "Epoch: 6270 mean train loss:  1.25274425e-02, mean val. rec. loss:  1.12746848e-02\n",
      "Epoch: 6271 mean train loss:  1.25268038e-02, mean val. rec. loss:  1.12741518e-02\n",
      "Epoch: 6272 mean train loss:  1.25261632e-02, mean val. rec. loss:  1.12736064e-02\n",
      "Epoch: 6273 mean train loss:  1.25255105e-02, mean val. rec. loss:  1.12730666e-02\n",
      "Epoch: 6274 mean train loss:  1.25248746e-02, mean val. rec. loss:  1.12725211e-02\n",
      "Epoch: 6275 mean train loss:  1.25242228e-02, mean val. rec. loss:  1.12719893e-02\n",
      "Epoch: 6276 mean train loss:  1.25235859e-02, mean val. rec. loss:  1.12714495e-02\n",
      "Epoch: 6277 mean train loss:  1.25229360e-02, mean val. rec. loss:  1.12708973e-02\n",
      "Epoch: 6278 mean train loss:  1.25222871e-02, mean val. rec. loss:  1.12703575e-02\n",
      "Epoch: 6279 mean train loss:  1.25216465e-02, mean val. rec. loss:  1.12698177e-02\n",
      "Epoch: 6280 mean train loss:  1.25210050e-02, mean val. rec. loss:  1.12692712e-02\n",
      "Epoch: 6281 mean train loss:  1.25203560e-02, mean val. rec. loss:  1.12687393e-02\n",
      "Epoch: 6282 mean train loss:  1.25197107e-02, mean val. rec. loss:  1.12681871e-02\n",
      "Epoch: 6283 mean train loss:  1.25190655e-02, mean val. rec. loss:  1.12676564e-02\n",
      "Epoch: 6284 mean train loss:  1.25184193e-02, mean val. rec. loss:  1.12671053e-02\n",
      "Epoch: 6285 mean train loss:  1.25177731e-02, mean val. rec. loss:  1.12665757e-02\n",
      "Epoch: 6286 mean train loss:  1.25171269e-02, mean val. rec. loss:  1.12660280e-02\n",
      "Epoch: 6287 mean train loss:  1.25164845e-02, mean val. rec. loss:  1.12654848e-02\n",
      "Epoch: 6288 mean train loss:  1.25158346e-02, mean val. rec. loss:  1.12649382e-02\n",
      "Epoch: 6289 mean train loss:  1.25151828e-02, mean val. rec. loss:  1.12643951e-02\n",
      "Epoch: 6290 mean train loss:  1.25145404e-02, mean val. rec. loss:  1.12638576e-02\n",
      "Epoch: 6291 mean train loss:  1.25138942e-02, mean val. rec. loss:  1.12633201e-02\n",
      "Epoch: 6292 mean train loss:  1.25132461e-02, mean val. rec. loss:  1.12627689e-02\n",
      "Epoch: 6293 mean train loss:  1.25126000e-02, mean val. rec. loss:  1.12622190e-02\n",
      "Epoch: 6294 mean train loss:  1.25119519e-02, mean val. rec. loss:  1.12616826e-02\n",
      "Epoch: 6295 mean train loss:  1.25113039e-02, mean val. rec. loss:  1.12611394e-02\n",
      "Epoch: 6296 mean train loss:  1.25106614e-02, mean val. rec. loss:  1.12605985e-02\n",
      "Epoch: 6297 mean train loss:  1.25100078e-02, mean val. rec. loss:  1.12600440e-02\n",
      "Epoch: 6298 mean train loss:  1.25093560e-02, mean val. rec. loss:  1.12595076e-02\n",
      "Epoch: 6299 mean train loss:  1.25087117e-02, mean val. rec. loss:  1.12589610e-02\n",
      "Epoch: 6300 mean train loss:  1.25080637e-02, mean val. rec. loss:  1.12584145e-02\n",
      "Epoch: 6301 mean train loss:  1.25074147e-02, mean val. rec. loss:  1.12578679e-02\n",
      "Epoch: 6302 mean train loss:  1.25067657e-02, mean val. rec. loss:  1.12573270e-02\n",
      "Epoch: 6303 mean train loss:  1.25061177e-02, mean val. rec. loss:  1.12567849e-02\n",
      "Epoch: 6304 mean train loss:  1.25054687e-02, mean val. rec. loss:  1.12562350e-02\n",
      "Epoch: 6305 mean train loss:  1.25048151e-02, mean val. rec. loss:  1.12556975e-02\n",
      "Epoch: 6306 mean train loss:  1.25041689e-02, mean val. rec. loss:  1.12551486e-02\n",
      "Epoch: 6307 mean train loss:  1.25035236e-02, mean val. rec. loss:  1.12546043e-02\n",
      "Epoch: 6308 mean train loss:  1.25028700e-02, mean val. rec. loss:  1.12540555e-02\n",
      "Epoch: 6309 mean train loss:  1.25022164e-02, mean val. rec. loss:  1.12535134e-02\n",
      "Epoch: 6310 mean train loss:  1.25015739e-02, mean val. rec. loss:  1.12529623e-02\n",
      "Epoch: 6311 mean train loss:  1.25009147e-02, mean val. rec. loss:  1.12524225e-02\n",
      "Epoch: 6312 mean train loss:  1.25002732e-02, mean val. rec. loss:  1.12518714e-02\n",
      "Epoch: 6313 mean train loss:  1.24996149e-02, mean val. rec. loss:  1.12513294e-02\n",
      "Epoch: 6314 mean train loss:  1.24989687e-02, mean val. rec. loss:  1.12507828e-02\n",
      "Epoch: 6315 mean train loss:  1.24983207e-02, mean val. rec. loss:  1.12502306e-02\n",
      "Epoch: 6316 mean train loss:  1.24976652e-02, mean val. rec. loss:  1.12496828e-02\n",
      "Epoch: 6317 mean train loss:  1.24970106e-02, mean val. rec. loss:  1.12491419e-02\n",
      "Epoch: 6318 mean train loss:  1.24963626e-02, mean val. rec. loss:  1.12485942e-02\n",
      "Epoch: 6319 mean train loss:  1.24957108e-02, mean val. rec. loss:  1.12480454e-02\n",
      "Epoch: 6320 mean train loss:  1.24950590e-02, mean val. rec. loss:  1.12474988e-02\n",
      "Epoch: 6321 mean train loss:  1.24944082e-02, mean val. rec. loss:  1.12469534e-02\n",
      "Epoch: 6322 mean train loss:  1.24937555e-02, mean val. rec. loss:  1.12464011e-02\n",
      "Epoch: 6323 mean train loss:  1.24930991e-02, mean val. rec. loss:  1.12458511e-02\n",
      "Epoch: 6324 mean train loss:  1.24924511e-02, mean val. rec. loss:  1.12453091e-02\n",
      "Epoch: 6325 mean train loss:  1.24918030e-02, mean val. rec. loss:  1.12447614e-02\n",
      "Epoch: 6326 mean train loss:  1.24911457e-02, mean val. rec. loss:  1.12442103e-02\n",
      "Epoch: 6327 mean train loss:  1.24904883e-02, mean val. rec. loss:  1.12436660e-02\n",
      "Epoch: 6328 mean train loss:  1.24898431e-02, mean val. rec. loss:  1.12431137e-02\n",
      "Epoch: 6329 mean train loss:  1.24891810e-02, mean val. rec. loss:  1.12425694e-02\n",
      "Epoch: 6330 mean train loss:  1.24885367e-02, mean val. rec. loss:  1.12420149e-02\n",
      "Epoch: 6331 mean train loss:  1.24878747e-02, mean val. rec. loss:  1.12414683e-02\n",
      "Epoch: 6332 mean train loss:  1.24872257e-02, mean val. rec. loss:  1.12409195e-02\n",
      "Epoch: 6333 mean train loss:  1.24865740e-02, mean val. rec. loss:  1.12403752e-02\n",
      "Epoch: 6334 mean train loss:  1.24859166e-02, mean val. rec. loss:  1.12398150e-02\n",
      "Epoch: 6335 mean train loss:  1.24852565e-02, mean val. rec. loss:  1.12392729e-02\n",
      "Epoch: 6336 mean train loss:  1.24846075e-02, mean val. rec. loss:  1.12387196e-02\n",
      "Epoch: 6337 mean train loss:  1.24839529e-02, mean val. rec. loss:  1.12381685e-02\n",
      "Epoch: 6338 mean train loss:  1.24832975e-02, mean val. rec. loss:  1.12376173e-02\n",
      "Epoch: 6339 mean train loss:  1.24826420e-02, mean val. rec. loss:  1.12370708e-02\n",
      "Epoch: 6340 mean train loss:  1.24819865e-02, mean val. rec. loss:  1.12365219e-02\n",
      "Epoch: 6341 mean train loss:  1.24813319e-02, mean val. rec. loss:  1.12359640e-02\n",
      "Epoch: 6342 mean train loss:  1.24806708e-02, mean val. rec. loss:  1.12354242e-02\n",
      "Epoch: 6343 mean train loss:  1.24800200e-02, mean val. rec. loss:  1.12348686e-02\n",
      "Epoch: 6344 mean train loss:  1.24793692e-02, mean val. rec. loss:  1.12343197e-02\n",
      "Epoch: 6345 mean train loss:  1.24787081e-02, mean val. rec. loss:  1.12337618e-02\n",
      "Epoch: 6346 mean train loss:  1.24780517e-02, mean val. rec. loss:  1.12332152e-02\n",
      "Epoch: 6347 mean train loss:  1.24773943e-02, mean val. rec. loss:  1.12326709e-02\n",
      "Epoch: 6348 mean train loss:  1.24767379e-02, mean val. rec. loss:  1.12321153e-02\n",
      "Epoch: 6349 mean train loss:  1.24760805e-02, mean val. rec. loss:  1.12315574e-02\n",
      "Epoch: 6350 mean train loss:  1.24754241e-02, mean val. rec. loss:  1.12310051e-02\n",
      "Epoch: 6351 mean train loss:  1.24747621e-02, mean val. rec. loss:  1.12304597e-02\n",
      "Epoch: 6352 mean train loss:  1.24741085e-02, mean val. rec. loss:  1.12299074e-02\n",
      "Epoch: 6353 mean train loss:  1.24734548e-02, mean val. rec. loss:  1.12293620e-02\n",
      "Epoch: 6354 mean train loss:  1.24727919e-02, mean val. rec. loss:  1.12287950e-02\n",
      "Epoch: 6355 mean train loss:  1.24721346e-02, mean val. rec. loss:  1.12282541e-02\n",
      "Epoch: 6356 mean train loss:  1.24714763e-02, mean val. rec. loss:  1.12276882e-02\n",
      "Epoch: 6357 mean train loss:  1.24708180e-02, mean val. rec. loss:  1.12271462e-02\n",
      "Epoch: 6358 mean train loss:  1.24701588e-02, mean val. rec. loss:  1.12265849e-02\n",
      "Epoch: 6359 mean train loss:  1.24695014e-02, mean val. rec. loss:  1.12260349e-02\n",
      "Epoch: 6360 mean train loss:  1.24688459e-02, mean val. rec. loss:  1.12254815e-02\n",
      "Epoch: 6361 mean train loss:  1.24681821e-02, mean val. rec. loss:  1.12249213e-02\n",
      "Epoch: 6362 mean train loss:  1.24675191e-02, mean val. rec. loss:  1.12243793e-02\n",
      "Epoch: 6363 mean train loss:  1.24668627e-02, mean val. rec. loss:  1.12238168e-02\n",
      "Epoch: 6364 mean train loss:  1.24662035e-02, mean val. rec. loss:  1.12232567e-02\n",
      "Epoch: 6365 mean train loss:  1.24655443e-02, mean val. rec. loss:  1.12227078e-02\n",
      "Epoch: 6366 mean train loss:  1.24648832e-02, mean val. rec. loss:  1.12221624e-02\n",
      "Epoch: 6367 mean train loss:  1.24642230e-02, mean val. rec. loss:  1.12216022e-02\n",
      "Epoch: 6368 mean train loss:  1.24635629e-02, mean val. rec. loss:  1.12210477e-02\n",
      "Epoch: 6369 mean train loss:  1.24629055e-02, mean val. rec. loss:  1.12204875e-02\n",
      "Epoch: 6370 mean train loss:  1.24622426e-02, mean val. rec. loss:  1.12199318e-02\n",
      "Epoch: 6371 mean train loss:  1.24615759e-02, mean val. rec. loss:  1.12193819e-02\n",
      "Epoch: 6372 mean train loss:  1.24609186e-02, mean val. rec. loss:  1.12188251e-02\n",
      "Epoch: 6373 mean train loss:  1.24602575e-02, mean val. rec. loss:  1.12182683e-02\n",
      "Epoch: 6374 mean train loss:  1.24595964e-02, mean val. rec. loss:  1.12177115e-02\n",
      "Epoch: 6375 mean train loss:  1.24589335e-02, mean val. rec. loss:  1.12171559e-02\n",
      "Epoch: 6376 mean train loss:  1.24582724e-02, mean val. rec. loss:  1.12166014e-02\n",
      "Epoch: 6377 mean train loss:  1.24576095e-02, mean val. rec. loss:  1.12160378e-02\n",
      "Epoch: 6378 mean train loss:  1.24569437e-02, mean val. rec. loss:  1.12154776e-02\n",
      "Epoch: 6379 mean train loss:  1.24562845e-02, mean val. rec. loss:  1.12149265e-02\n",
      "Epoch: 6380 mean train loss:  1.24556244e-02, mean val. rec. loss:  1.12143708e-02\n",
      "Epoch: 6381 mean train loss:  1.24549586e-02, mean val. rec. loss:  1.12138106e-02\n",
      "Epoch: 6382 mean train loss:  1.24542910e-02, mean val. rec. loss:  1.12132584e-02\n",
      "Epoch: 6383 mean train loss:  1.24536356e-02, mean val. rec. loss:  1.12126959e-02\n",
      "Epoch: 6384 mean train loss:  1.24529642e-02, mean val. rec. loss:  1.12121437e-02\n",
      "Epoch: 6385 mean train loss:  1.24523087e-02, mean val. rec. loss:  1.12115812e-02\n",
      "Epoch: 6386 mean train loss:  1.24516365e-02, mean val. rec. loss:  1.12110256e-02\n",
      "Epoch: 6387 mean train loss:  1.24509763e-02, mean val. rec. loss:  1.12104677e-02\n",
      "Epoch: 6388 mean train loss:  1.24503162e-02, mean val. rec. loss:  1.12099143e-02\n",
      "Epoch: 6389 mean train loss:  1.24496467e-02, mean val. rec. loss:  1.12093496e-02\n",
      "Epoch: 6390 mean train loss:  1.24489782e-02, mean val. rec. loss:  1.12087962e-02\n",
      "Epoch: 6391 mean train loss:  1.24483171e-02, mean val. rec. loss:  1.12082349e-02\n",
      "Epoch: 6392 mean train loss:  1.24476523e-02, mean val. rec. loss:  1.12076770e-02\n",
      "Epoch: 6393 mean train loss:  1.24469866e-02, mean val. rec. loss:  1.12071179e-02\n",
      "Epoch: 6394 mean train loss:  1.24463209e-02, mean val. rec. loss:  1.12065611e-02\n",
      "Epoch: 6395 mean train loss:  1.24456551e-02, mean val. rec. loss:  1.12059975e-02\n",
      "Epoch: 6396 mean train loss:  1.24449857e-02, mean val. rec. loss:  1.12054464e-02\n",
      "Epoch: 6397 mean train loss:  1.24443218e-02, mean val. rec. loss:  1.12048794e-02\n",
      "Epoch: 6398 mean train loss:  1.24436598e-02, mean val. rec. loss:  1.12043204e-02\n",
      "Epoch: 6399 mean train loss:  1.24429894e-02, mean val. rec. loss:  1.12037579e-02\n",
      "Epoch: 6400 mean train loss:  1.24423190e-02, mean val. rec. loss:  1.12032011e-02\n",
      "Epoch: 6401 mean train loss:  1.24416598e-02, mean val. rec. loss:  1.12026319e-02\n",
      "Epoch: 6402 mean train loss:  1.24409838e-02, mean val. rec. loss:  1.12020796e-02\n",
      "Epoch: 6403 mean train loss:  1.24403246e-02, mean val. rec. loss:  1.12015115e-02\n",
      "Epoch: 6404 mean train loss:  1.24396486e-02, mean val. rec. loss:  1.12009547e-02\n",
      "Epoch: 6405 mean train loss:  1.24389857e-02, mean val. rec. loss:  1.12003934e-02\n",
      "Epoch: 6406 mean train loss:  1.24383218e-02, mean val. rec. loss:  1.11998253e-02\n",
      "Epoch: 6407 mean train loss:  1.24376486e-02, mean val. rec. loss:  1.11992663e-02\n",
      "Epoch: 6408 mean train loss:  1.24369773e-02, mean val. rec. loss:  1.11987095e-02\n",
      "Epoch: 6409 mean train loss:  1.24363116e-02, mean val. rec. loss:  1.11981459e-02\n",
      "Epoch: 6410 mean train loss:  1.24356431e-02, mean val. rec. loss:  1.11975812e-02\n",
      "Epoch: 6411 mean train loss:  1.24349736e-02, mean val. rec. loss:  1.11970221e-02\n",
      "Epoch: 6412 mean train loss:  1.24343042e-02, mean val. rec. loss:  1.11964608e-02\n",
      "Epoch: 6413 mean train loss:  1.24336356e-02, mean val. rec. loss:  1.11958972e-02\n",
      "Epoch: 6414 mean train loss:  1.24329652e-02, mean val. rec. loss:  1.11953291e-02\n",
      "Epoch: 6415 mean train loss:  1.24322911e-02, mean val. rec. loss:  1.11947644e-02\n",
      "Epoch: 6416 mean train loss:  1.24316263e-02, mean val. rec. loss:  1.11942087e-02\n",
      "Epoch: 6417 mean train loss:  1.24309587e-02, mean val. rec. loss:  1.11936440e-02\n",
      "Epoch: 6418 mean train loss:  1.24302837e-02, mean val. rec. loss:  1.11930861e-02\n",
      "Epoch: 6419 mean train loss:  1.24296133e-02, mean val. rec. loss:  1.11925146e-02\n",
      "Epoch: 6420 mean train loss:  1.24289429e-02, mean val. rec. loss:  1.11919476e-02\n",
      "Epoch: 6421 mean train loss:  1.24282716e-02, mean val. rec. loss:  1.11913896e-02\n",
      "Epoch: 6422 mean train loss:  1.24275993e-02, mean val. rec. loss:  1.11908340e-02\n",
      "Epoch: 6423 mean train loss:  1.24269289e-02, mean val. rec. loss:  1.11902579e-02\n",
      "Epoch: 6424 mean train loss:  1.24262520e-02, mean val. rec. loss:  1.11896955e-02\n",
      "Epoch: 6425 mean train loss:  1.24255854e-02, mean val. rec. loss:  1.11891319e-02\n",
      "Epoch: 6426 mean train loss:  1.24249159e-02, mean val. rec. loss:  1.11885592e-02\n",
      "Epoch: 6427 mean train loss:  1.24242409e-02, mean val. rec. loss:  1.11880047e-02\n",
      "Epoch: 6428 mean train loss:  1.24235668e-02, mean val. rec. loss:  1.11874253e-02\n",
      "Epoch: 6429 mean train loss:  1.24228945e-02, mean val. rec. loss:  1.11868708e-02\n",
      "Epoch: 6430 mean train loss:  1.24222204e-02, mean val. rec. loss:  1.11862936e-02\n",
      "Epoch: 6431 mean train loss:  1.24215481e-02, mean val. rec. loss:  1.11857402e-02\n",
      "Epoch: 6432 mean train loss:  1.24208740e-02, mean val. rec. loss:  1.11851687e-02\n",
      "Epoch: 6433 mean train loss:  1.24202046e-02, mean val. rec. loss:  1.11846017e-02\n",
      "Epoch: 6434 mean train loss:  1.24195267e-02, mean val. rec. loss:  1.11840301e-02\n",
      "Epoch: 6435 mean train loss:  1.24188480e-02, mean val. rec. loss:  1.11834666e-02\n",
      "Epoch: 6436 mean train loss:  1.24181794e-02, mean val. rec. loss:  1.11829030e-02\n",
      "Epoch: 6437 mean train loss:  1.24175035e-02, mean val. rec. loss:  1.11823428e-02\n",
      "Epoch: 6438 mean train loss:  1.24168275e-02, mean val. rec. loss:  1.11817713e-02\n",
      "Epoch: 6439 mean train loss:  1.24161534e-02, mean val. rec. loss:  1.11811997e-02\n",
      "Epoch: 6440 mean train loss:  1.24154793e-02, mean val. rec. loss:  1.11806373e-02\n",
      "Epoch: 6441 mean train loss:  1.24148033e-02, mean val. rec. loss:  1.11800703e-02\n",
      "Epoch: 6442 mean train loss:  1.24141301e-02, mean val. rec. loss:  1.11795056e-02\n",
      "Epoch: 6443 mean train loss:  1.24134495e-02, mean val. rec. loss:  1.11789284e-02\n",
      "Epoch: 6444 mean train loss:  1.24127707e-02, mean val. rec. loss:  1.11783648e-02\n",
      "Epoch: 6445 mean train loss:  1.24120975e-02, mean val. rec. loss:  1.11777955e-02\n",
      "Epoch: 6446 mean train loss:  1.24114216e-02, mean val. rec. loss:  1.11772240e-02\n",
      "Epoch: 6447 mean train loss:  1.24107456e-02, mean val. rec. loss:  1.11766548e-02\n",
      "Epoch: 6448 mean train loss:  1.24100677e-02, mean val. rec. loss:  1.11760855e-02\n",
      "Epoch: 6449 mean train loss:  1.24093918e-02, mean val. rec. loss:  1.11755174e-02\n",
      "Epoch: 6450 mean train loss:  1.24087121e-02, mean val. rec. loss:  1.11749402e-02\n",
      "Epoch: 6451 mean train loss:  1.24080305e-02, mean val. rec. loss:  1.11743800e-02\n",
      "Epoch: 6452 mean train loss:  1.24073564e-02, mean val. rec. loss:  1.11738051e-02\n",
      "Epoch: 6453 mean train loss:  1.24066823e-02, mean val. rec. loss:  1.11732347e-02\n",
      "Epoch: 6454 mean train loss:  1.24059998e-02, mean val. rec. loss:  1.11726552e-02\n",
      "Epoch: 6455 mean train loss:  1.24053210e-02, mean val. rec. loss:  1.11720894e-02\n",
      "Epoch: 6456 mean train loss:  1.24046423e-02, mean val. rec. loss:  1.11715246e-02\n",
      "Epoch: 6457 mean train loss:  1.24039635e-02, mean val. rec. loss:  1.11709486e-02\n",
      "Epoch: 6458 mean train loss:  1.24032838e-02, mean val. rec. loss:  1.11703725e-02\n",
      "Epoch: 6459 mean train loss:  1.24026041e-02, mean val. rec. loss:  1.11698021e-02\n",
      "Epoch: 6460 mean train loss:  1.24019207e-02, mean val. rec. loss:  1.11692363e-02\n",
      "Epoch: 6461 mean train loss:  1.24012447e-02, mean val. rec. loss:  1.11686636e-02\n",
      "Epoch: 6462 mean train loss:  1.24005687e-02, mean val. rec. loss:  1.11680978e-02\n",
      "Epoch: 6463 mean train loss:  1.23998834e-02, mean val. rec. loss:  1.11675183e-02\n",
      "Epoch: 6464 mean train loss:  1.23991981e-02, mean val. rec. loss:  1.11669434e-02\n",
      "Epoch: 6465 mean train loss:  1.23985184e-02, mean val. rec. loss:  1.11663719e-02\n",
      "Epoch: 6466 mean train loss:  1.23978369e-02, mean val. rec. loss:  1.11657981e-02\n",
      "Epoch: 6467 mean train loss:  1.23971562e-02, mean val. rec. loss:  1.11652288e-02\n",
      "Epoch: 6468 mean train loss:  1.23964738e-02, mean val. rec. loss:  1.11646618e-02\n",
      "Epoch: 6469 mean train loss:  1.23957959e-02, mean val. rec. loss:  1.11640824e-02\n",
      "Epoch: 6470 mean train loss:  1.23951181e-02, mean val. rec. loss:  1.11635097e-02\n",
      "Epoch: 6471 mean train loss:  1.23944319e-02, mean val. rec. loss:  1.11629336e-02\n",
      "Epoch: 6472 mean train loss:  1.23937456e-02, mean val. rec. loss:  1.11623576e-02\n",
      "Epoch: 6473 mean train loss:  1.23930678e-02, mean val. rec. loss:  1.11617872e-02\n",
      "Epoch: 6474 mean train loss:  1.23923844e-02, mean val. rec. loss:  1.11612191e-02\n",
      "Epoch: 6475 mean train loss:  1.23917010e-02, mean val. rec. loss:  1.11606373e-02\n",
      "Epoch: 6476 mean train loss:  1.23910175e-02, mean val. rec. loss:  1.11600579e-02\n",
      "Epoch: 6477 mean train loss:  1.23903341e-02, mean val. rec. loss:  1.11594807e-02\n",
      "Epoch: 6478 mean train loss:  1.23896451e-02, mean val. rec. loss:  1.11589103e-02\n",
      "Epoch: 6479 mean train loss:  1.23889663e-02, mean val. rec. loss:  1.11583342e-02\n",
      "Epoch: 6480 mean train loss:  1.23882857e-02, mean val. rec. loss:  1.11577627e-02\n",
      "Epoch: 6481 mean train loss:  1.23875967e-02, mean val. rec. loss:  1.11571787e-02\n",
      "Epoch: 6482 mean train loss:  1.23869086e-02, mean val. rec. loss:  1.11566015e-02\n",
      "Epoch: 6483 mean train loss:  1.23862224e-02, mean val. rec. loss:  1.11560255e-02\n",
      "Epoch: 6484 mean train loss:  1.23855371e-02, mean val. rec. loss:  1.11554471e-02\n",
      "Epoch: 6485 mean train loss:  1.23848518e-02, mean val. rec. loss:  1.11548745e-02\n",
      "Epoch: 6486 mean train loss:  1.23841665e-02, mean val. rec. loss:  1.11543052e-02\n",
      "Epoch: 6487 mean train loss:  1.23834840e-02, mean val. rec. loss:  1.11537246e-02\n",
      "Epoch: 6488 mean train loss:  1.23828016e-02, mean val. rec. loss:  1.11531463e-02\n",
      "Epoch: 6489 mean train loss:  1.23821125e-02, mean val. rec. loss:  1.11525657e-02\n",
      "Epoch: 6490 mean train loss:  1.23814207e-02, mean val. rec. loss:  1.11519896e-02\n",
      "Epoch: 6491 mean train loss:  1.23807382e-02, mean val. rec. loss:  1.11514158e-02\n",
      "Epoch: 6492 mean train loss:  1.23800511e-02, mean val. rec. loss:  1.11508432e-02\n",
      "Epoch: 6493 mean train loss:  1.23793630e-02, mean val. rec. loss:  1.11502603e-02\n",
      "Epoch: 6494 mean train loss:  1.23786749e-02, mean val. rec. loss:  1.11496775e-02\n",
      "Epoch: 6495 mean train loss:  1.23779878e-02, mean val. rec. loss:  1.11491059e-02\n",
      "Epoch: 6496 mean train loss:  1.23772988e-02, mean val. rec. loss:  1.11485276e-02\n",
      "Epoch: 6497 mean train loss:  1.23766135e-02, mean val. rec. loss:  1.11479425e-02\n",
      "Epoch: 6498 mean train loss:  1.23759226e-02, mean val. rec. loss:  1.11473641e-02\n",
      "Epoch: 6499 mean train loss:  1.23752280e-02, mean val. rec. loss:  1.11467903e-02\n",
      "Epoch: 6500 mean train loss:  1.23745446e-02, mean val. rec. loss:  1.11462109e-02\n",
      "Epoch: 6501 mean train loss:  1.23738556e-02, mean val. rec. loss:  1.11456280e-02\n",
      "Epoch: 6502 mean train loss:  1.23731647e-02, mean val. rec. loss:  1.11450486e-02\n",
      "Epoch: 6503 mean train loss:  1.23724738e-02, mean val. rec. loss:  1.11444680e-02\n",
      "Epoch: 6504 mean train loss:  1.23717848e-02, mean val. rec. loss:  1.11438874e-02\n",
      "Epoch: 6505 mean train loss:  1.23710949e-02, mean val. rec. loss:  1.11433000e-02\n",
      "Epoch: 6506 mean train loss:  1.23703994e-02, mean val. rec. loss:  1.11427307e-02\n",
      "Epoch: 6507 mean train loss:  1.23697122e-02, mean val. rec. loss:  1.11421433e-02\n",
      "Epoch: 6508 mean train loss:  1.23690251e-02, mean val. rec. loss:  1.11415582e-02\n",
      "Epoch: 6509 mean train loss:  1.23683305e-02, mean val. rec. loss:  1.11409833e-02\n",
      "Epoch: 6510 mean train loss:  1.23676387e-02, mean val. rec. loss:  1.11403947e-02\n",
      "Epoch: 6511 mean train loss:  1.23669478e-02, mean val. rec. loss:  1.11398051e-02\n",
      "Epoch: 6512 mean train loss:  1.23662551e-02, mean val. rec. loss:  1.11392290e-02\n",
      "Epoch: 6513 mean train loss:  1.23655614e-02, mean val. rec. loss:  1.11386529e-02\n",
      "Epoch: 6514 mean train loss:  1.23648687e-02, mean val. rec. loss:  1.11380610e-02\n",
      "Epoch: 6515 mean train loss:  1.23641731e-02, mean val. rec. loss:  1.11374804e-02\n",
      "Epoch: 6516 mean train loss:  1.23634841e-02, mean val. rec. loss:  1.11368987e-02\n",
      "Epoch: 6517 mean train loss:  1.23627942e-02, mean val. rec. loss:  1.11363101e-02\n",
      "Epoch: 6518 mean train loss:  1.23620968e-02, mean val. rec. loss:  1.11357250e-02\n",
      "Epoch: 6519 mean train loss:  1.23614041e-02, mean val. rec. loss:  1.11351399e-02\n",
      "Epoch: 6520 mean train loss:  1.23607085e-02, mean val. rec. loss:  1.11345559e-02\n",
      "Epoch: 6521 mean train loss:  1.23600139e-02, mean val. rec. loss:  1.11339730e-02\n",
      "Epoch: 6522 mean train loss:  1.23593193e-02, mean val. rec. loss:  1.11333913e-02\n",
      "Epoch: 6523 mean train loss:  1.23586257e-02, mean val. rec. loss:  1.11328130e-02\n",
      "Epoch: 6524 mean train loss:  1.23579348e-02, mean val. rec. loss:  1.11322256e-02\n",
      "Epoch: 6525 mean train loss:  1.23572337e-02, mean val. rec. loss:  1.11316370e-02\n",
      "Epoch: 6526 mean train loss:  1.23565354e-02, mean val. rec. loss:  1.11310621e-02\n",
      "Epoch: 6527 mean train loss:  1.23558435e-02, mean val. rec. loss:  1.11304690e-02\n",
      "Epoch: 6528 mean train loss:  1.23551462e-02, mean val. rec. loss:  1.11298782e-02\n",
      "Epoch: 6529 mean train loss:  1.23544506e-02, mean val. rec. loss:  1.11292976e-02\n",
      "Epoch: 6530 mean train loss:  1.23537542e-02, mean val. rec. loss:  1.11287170e-02\n",
      "Epoch: 6531 mean train loss:  1.23530577e-02, mean val. rec. loss:  1.11281262e-02\n",
      "Epoch: 6532 mean train loss:  1.23523603e-02, mean val. rec. loss:  1.11275377e-02\n",
      "Epoch: 6533 mean train loss:  1.23516657e-02, mean val. rec. loss:  1.11269548e-02\n",
      "Epoch: 6534 mean train loss:  1.23509646e-02, mean val. rec. loss:  1.11263572e-02\n",
      "Epoch: 6535 mean train loss:  1.23502635e-02, mean val. rec. loss:  1.11257744e-02\n",
      "Epoch: 6536 mean train loss:  1.23495689e-02, mean val. rec. loss:  1.11251836e-02\n",
      "Epoch: 6537 mean train loss:  1.23488687e-02, mean val. rec. loss:  1.11245939e-02\n",
      "Epoch: 6538 mean train loss:  1.23481713e-02, mean val. rec. loss:  1.11240065e-02\n",
      "Epoch: 6539 mean train loss:  1.23474721e-02, mean val. rec. loss:  1.11234168e-02\n",
      "Epoch: 6540 mean train loss:  1.23467738e-02, mean val. rec. loss:  1.11228294e-02\n",
      "Epoch: 6541 mean train loss:  1.23460736e-02, mean val. rec. loss:  1.11222352e-02\n",
      "Epoch: 6542 mean train loss:  1.23453687e-02, mean val. rec. loss:  1.11216444e-02\n",
      "Epoch: 6543 mean train loss:  1.23446741e-02, mean val. rec. loss:  1.11210582e-02\n",
      "Epoch: 6544 mean train loss:  1.23439767e-02, mean val. rec. loss:  1.11204719e-02\n",
      "Epoch: 6545 mean train loss:  1.23432728e-02, mean val. rec. loss:  1.11198811e-02\n",
      "Epoch: 6546 mean train loss:  1.23425680e-02, mean val. rec. loss:  1.11192926e-02\n",
      "Epoch: 6547 mean train loss:  1.23418743e-02, mean val. rec. loss:  1.11186995e-02\n",
      "Epoch: 6548 mean train loss:  1.23411648e-02, mean val. rec. loss:  1.11181143e-02\n",
      "Epoch: 6549 mean train loss:  1.23404721e-02, mean val. rec. loss:  1.11175190e-02\n",
      "Epoch: 6550 mean train loss:  1.23397607e-02, mean val. rec. loss:  1.11169316e-02\n",
      "Epoch: 6551 mean train loss:  1.23390643e-02, mean val. rec. loss:  1.11163397e-02\n",
      "Epoch: 6552 mean train loss:  1.23383650e-02, mean val. rec. loss:  1.11157534e-02\n",
      "Epoch: 6553 mean train loss:  1.23376583e-02, mean val. rec. loss:  1.11151524e-02\n",
      "Epoch: 6554 mean train loss:  1.23369516e-02, mean val. rec. loss:  1.11145684e-02\n",
      "Epoch: 6555 mean train loss:  1.23362524e-02, mean val. rec. loss:  1.11139765e-02\n",
      "Epoch: 6556 mean train loss:  1.23355485e-02, mean val. rec. loss:  1.11133834e-02\n",
      "Epoch: 6557 mean train loss:  1.23348446e-02, mean val. rec. loss:  1.11127915e-02\n",
      "Epoch: 6558 mean train loss:  1.23341407e-02, mean val. rec. loss:  1.11121995e-02\n",
      "Epoch: 6559 mean train loss:  1.23334367e-02, mean val. rec. loss:  1.11116008e-02\n",
      "Epoch: 6560 mean train loss:  1.23327282e-02, mean val. rec. loss:  1.11110157e-02\n",
      "Epoch: 6561 mean train loss:  1.23320271e-02, mean val. rec. loss:  1.11104192e-02\n",
      "Epoch: 6562 mean train loss:  1.23313269e-02, mean val. rec. loss:  1.11098238e-02\n",
      "Epoch: 6563 mean train loss:  1.23306174e-02, mean val. rec. loss:  1.11092274e-02\n",
      "Epoch: 6564 mean train loss:  1.23299079e-02, mean val. rec. loss:  1.11086377e-02\n",
      "Epoch: 6565 mean train loss:  1.23292096e-02, mean val. rec. loss:  1.11080378e-02\n",
      "Epoch: 6566 mean train loss:  1.23284954e-02, mean val. rec. loss:  1.11074482e-02\n",
      "Epoch: 6567 mean train loss:  1.23277980e-02, mean val. rec. loss:  1.11068472e-02\n",
      "Epoch: 6568 mean train loss:  1.23270829e-02, mean val. rec. loss:  1.11062541e-02\n",
      "Epoch: 6569 mean train loss:  1.23263800e-02, mean val. rec. loss:  1.11056576e-02\n",
      "Epoch: 6570 mean train loss:  1.23256761e-02, mean val. rec. loss:  1.11050555e-02\n",
      "Epoch: 6571 mean train loss:  1.23249656e-02, mean val. rec. loss:  1.11044613e-02\n",
      "Epoch: 6572 mean train loss:  1.23242543e-02, mean val. rec. loss:  1.11038716e-02\n",
      "Epoch: 6573 mean train loss:  1.23235494e-02, mean val. rec. loss:  1.11032740e-02\n",
      "Epoch: 6574 mean train loss:  1.23228409e-02, mean val. rec. loss:  1.11026764e-02\n",
      "Epoch: 6575 mean train loss:  1.23221332e-02, mean val. rec. loss:  1.11020822e-02\n",
      "Epoch: 6576 mean train loss:  1.23214238e-02, mean val. rec. loss:  1.11014846e-02\n",
      "Epoch: 6577 mean train loss:  1.23207143e-02, mean val. rec. loss:  1.11008915e-02\n",
      "Epoch: 6578 mean train loss:  1.23200057e-02, mean val. rec. loss:  1.11002882e-02\n",
      "Epoch: 6579 mean train loss:  1.23192915e-02, mean val. rec. loss:  1.10996997e-02\n",
      "Epoch: 6580 mean train loss:  1.23185858e-02, mean val. rec. loss:  1.10990976e-02\n",
      "Epoch: 6581 mean train loss:  1.23178791e-02, mean val. rec. loss:  1.10984988e-02\n",
      "Epoch: 6582 mean train loss:  1.23171640e-02, mean val. rec. loss:  1.10979058e-02\n",
      "Epoch: 6583 mean train loss:  1.23164536e-02, mean val. rec. loss:  1.10972991e-02\n",
      "Epoch: 6584 mean train loss:  1.23157422e-02, mean val. rec. loss:  1.10966935e-02\n",
      "Epoch: 6585 mean train loss:  1.23150318e-02, mean val. rec. loss:  1.10961016e-02\n",
      "Epoch: 6586 mean train loss:  1.23143195e-02, mean val. rec. loss:  1.10955097e-02\n",
      "Epoch: 6587 mean train loss:  1.23136063e-02, mean val. rec. loss:  1.10948973e-02\n",
      "Epoch: 6588 mean train loss:  1.23128912e-02, mean val. rec. loss:  1.10943031e-02\n",
      "Epoch: 6589 mean train loss:  1.23121836e-02, mean val. rec. loss:  1.10937032e-02\n",
      "Epoch: 6590 mean train loss:  1.23114741e-02, mean val. rec. loss:  1.10930988e-02\n",
      "Epoch: 6591 mean train loss:  1.23107571e-02, mean val. rec. loss:  1.10924967e-02\n",
      "Epoch: 6592 mean train loss:  1.23100439e-02, mean val. rec. loss:  1.10918968e-02\n",
      "Epoch: 6593 mean train loss:  1.23093307e-02, mean val. rec. loss:  1.10912924e-02\n",
      "Epoch: 6594 mean train loss:  1.23086165e-02, mean val. rec. loss:  1.10906948e-02\n",
      "Epoch: 6595 mean train loss:  1.23079024e-02, mean val. rec. loss:  1.10900949e-02\n",
      "Epoch: 6596 mean train loss:  1.23071882e-02, mean val. rec. loss:  1.10894950e-02\n",
      "Epoch: 6597 mean train loss:  1.23064778e-02, mean val. rec. loss:  1.10888929e-02\n",
      "Epoch: 6598 mean train loss:  1.23057581e-02, mean val. rec. loss:  1.10882862e-02\n",
      "Epoch: 6599 mean train loss:  1.23050383e-02, mean val. rec. loss:  1.10876932e-02\n",
      "Epoch: 6600 mean train loss:  1.23043270e-02, mean val. rec. loss:  1.10870808e-02\n",
      "Epoch: 6601 mean train loss:  1.23036119e-02, mean val. rec. loss:  1.10864696e-02\n",
      "Epoch: 6602 mean train loss:  1.23028950e-02, mean val. rec. loss:  1.10858709e-02\n",
      "Epoch: 6603 mean train loss:  1.23021789e-02, mean val. rec. loss:  1.10852721e-02\n",
      "Epoch: 6604 mean train loss:  1.23014620e-02, mean val. rec. loss:  1.10846643e-02\n",
      "Epoch: 6605 mean train loss:  1.23007460e-02, mean val. rec. loss:  1.10840610e-02\n",
      "Epoch: 6606 mean train loss:  1.23000328e-02, mean val. rec. loss:  1.10834578e-02\n",
      "Epoch: 6607 mean train loss:  1.22993102e-02, mean val. rec. loss:  1.10828431e-02\n",
      "Epoch: 6608 mean train loss:  1.22985877e-02, mean val. rec. loss:  1.10822467e-02\n",
      "Epoch: 6609 mean train loss:  1.22978745e-02, mean val. rec. loss:  1.10816434e-02\n",
      "Epoch: 6610 mean train loss:  1.22971594e-02, mean val. rec. loss:  1.10810379e-02\n",
      "Epoch: 6611 mean train loss:  1.22964406e-02, mean val. rec. loss:  1.10804300e-02\n",
      "Epoch: 6612 mean train loss:  1.22957218e-02, mean val. rec. loss:  1.10798268e-02\n",
      "Epoch: 6613 mean train loss:  1.22950030e-02, mean val. rec. loss:  1.10792212e-02\n",
      "Epoch: 6614 mean train loss:  1.22942833e-02, mean val. rec. loss:  1.10786134e-02\n",
      "Epoch: 6615 mean train loss:  1.22935598e-02, mean val. rec. loss:  1.10779999e-02\n",
      "Epoch: 6616 mean train loss:  1.22928354e-02, mean val. rec. loss:  1.10774046e-02\n",
      "Epoch: 6617 mean train loss:  1.22921185e-02, mean val. rec. loss:  1.10767934e-02\n",
      "Epoch: 6618 mean train loss:  1.22914024e-02, mean val. rec. loss:  1.10761844e-02\n",
      "Epoch: 6619 mean train loss:  1.22906771e-02, mean val. rec. loss:  1.10755823e-02\n",
      "Epoch: 6620 mean train loss:  1.22899555e-02, mean val. rec. loss:  1.10749688e-02\n",
      "Epoch: 6621 mean train loss:  1.22892339e-02, mean val. rec. loss:  1.10743542e-02\n",
      "Epoch: 6622 mean train loss:  1.22885123e-02, mean val. rec. loss:  1.10737509e-02\n",
      "Epoch: 6623 mean train loss:  1.22877907e-02, mean val. rec. loss:  1.10731408e-02\n",
      "Epoch: 6624 mean train loss:  1.22870710e-02, mean val. rec. loss:  1.10725342e-02\n",
      "Epoch: 6625 mean train loss:  1.22863438e-02, mean val. rec. loss:  1.10719161e-02\n",
      "Epoch: 6626 mean train loss:  1.22856185e-02, mean val. rec. loss:  1.10713117e-02\n",
      "Epoch: 6627 mean train loss:  1.22848978e-02, mean val. rec. loss:  1.10707039e-02\n",
      "Epoch: 6628 mean train loss:  1.22841781e-02, mean val. rec. loss:  1.10700916e-02\n",
      "Epoch: 6629 mean train loss:  1.22834537e-02, mean val. rec. loss:  1.10694804e-02\n",
      "Epoch: 6630 mean train loss:  1.22827302e-02, mean val. rec. loss:  1.10688703e-02\n",
      "Epoch: 6631 mean train loss:  1.22820058e-02, mean val. rec. loss:  1.10682591e-02\n",
      "Epoch: 6632 mean train loss:  1.22812805e-02, mean val. rec. loss:  1.10676490e-02\n",
      "Epoch: 6633 mean train loss:  1.22805505e-02, mean val. rec. loss:  1.10670310e-02\n",
      "Epoch: 6634 mean train loss:  1.22798224e-02, mean val. rec. loss:  1.10664300e-02\n",
      "Epoch: 6635 mean train loss:  1.22790999e-02, mean val. rec. loss:  1.10658153e-02\n",
      "Epoch: 6636 mean train loss:  1.22783774e-02, mean val. rec. loss:  1.10652041e-02\n",
      "Epoch: 6637 mean train loss:  1.22776474e-02, mean val. rec. loss:  1.10645861e-02\n",
      "Epoch: 6638 mean train loss:  1.22769211e-02, mean val. rec. loss:  1.10639783e-02\n",
      "Epoch: 6639 mean train loss:  1.22761930e-02, mean val. rec. loss:  1.10633705e-02\n",
      "Epoch: 6640 mean train loss:  1.22754658e-02, mean val. rec. loss:  1.10627536e-02\n",
      "Epoch: 6641 mean train loss:  1.22747386e-02, mean val. rec. loss:  1.10621356e-02\n",
      "Epoch: 6642 mean train loss:  1.22740114e-02, mean val. rec. loss:  1.10615210e-02\n",
      "Epoch: 6643 mean train loss:  1.22732787e-02, mean val. rec. loss:  1.10609098e-02\n",
      "Epoch: 6644 mean train loss:  1.22725543e-02, mean val. rec. loss:  1.10602974e-02\n",
      "Epoch: 6645 mean train loss:  1.22718290e-02, mean val. rec. loss:  1.10596862e-02\n",
      "Epoch: 6646 mean train loss:  1.22710962e-02, mean val. rec. loss:  1.10590591e-02\n",
      "Epoch: 6647 mean train loss:  1.22703662e-02, mean val. rec. loss:  1.10584547e-02\n",
      "Epoch: 6648 mean train loss:  1.22696362e-02, mean val. rec. loss:  1.10578276e-02\n",
      "Epoch: 6649 mean train loss:  1.22689072e-02, mean val. rec. loss:  1.10572221e-02\n",
      "Epoch: 6650 mean train loss:  1.22681763e-02, mean val. rec. loss:  1.10565961e-02\n",
      "Epoch: 6651 mean train loss:  1.22674472e-02, mean val. rec. loss:  1.10559860e-02\n",
      "Epoch: 6652 mean train loss:  1.22667191e-02, mean val. rec. loss:  1.10553669e-02\n",
      "Epoch: 6653 mean train loss:  1.22659835e-02, mean val. rec. loss:  1.10547455e-02\n",
      "Epoch: 6654 mean train loss:  1.22652480e-02, mean val. rec. loss:  1.10541411e-02\n",
      "Epoch: 6655 mean train loss:  1.22645208e-02, mean val. rec. loss:  1.10535140e-02\n",
      "Epoch: 6656 mean train loss:  1.22637880e-02, mean val. rec. loss:  1.10528903e-02\n",
      "Epoch: 6657 mean train loss:  1.22630553e-02, mean val. rec. loss:  1.10522791e-02\n",
      "Epoch: 6658 mean train loss:  1.22623234e-02, mean val. rec. loss:  1.10516656e-02\n",
      "Epoch: 6659 mean train loss:  1.22615897e-02, mean val. rec. loss:  1.10510453e-02\n",
      "Epoch: 6660 mean train loss:  1.22608569e-02, mean val. rec. loss:  1.10504262e-02\n",
      "Epoch: 6661 mean train loss:  1.22601270e-02, mean val. rec. loss:  1.10497991e-02\n",
      "Epoch: 6662 mean train loss:  1.22593886e-02, mean val. rec. loss:  1.10491810e-02\n",
      "Epoch: 6663 mean train loss:  1.22586502e-02, mean val. rec. loss:  1.10485664e-02\n",
      "Epoch: 6664 mean train loss:  1.22579193e-02, mean val. rec. loss:  1.10479450e-02\n",
      "Epoch: 6665 mean train loss:  1.22571856e-02, mean val. rec. loss:  1.10473225e-02\n",
      "Epoch: 6666 mean train loss:  1.22564501e-02, mean val. rec. loss:  1.10467033e-02\n",
      "Epoch: 6667 mean train loss:  1.22557145e-02, mean val. rec. loss:  1.10460808e-02\n",
      "Epoch: 6668 mean train loss:  1.22549771e-02, mean val. rec. loss:  1.10454627e-02\n",
      "Epoch: 6669 mean train loss:  1.22542415e-02, mean val. rec. loss:  1.10448345e-02\n",
      "Epoch: 6670 mean train loss:  1.22535013e-02, mean val. rec. loss:  1.10442131e-02\n",
      "Epoch: 6671 mean train loss:  1.22527694e-02, mean val. rec. loss:  1.10435962e-02\n",
      "Epoch: 6672 mean train loss:  1.22520348e-02, mean val. rec. loss:  1.10429748e-02\n",
      "Epoch: 6673 mean train loss:  1.22512937e-02, mean val. rec. loss:  1.10423500e-02\n",
      "Epoch: 6674 mean train loss:  1.22505525e-02, mean val. rec. loss:  1.10417320e-02\n",
      "Epoch: 6675 mean train loss:  1.22498225e-02, mean val. rec. loss:  1.10411026e-02\n",
      "Epoch: 6676 mean train loss:  1.22490758e-02, mean val. rec. loss:  1.10404868e-02\n",
      "Epoch: 6677 mean train loss:  1.22483458e-02, mean val. rec. loss:  1.10398586e-02\n",
      "Epoch: 6678 mean train loss:  1.22475972e-02, mean val. rec. loss:  1.10392372e-02\n",
      "Epoch: 6679 mean train loss:  1.22468616e-02, mean val. rec. loss:  1.10386158e-02\n",
      "Epoch: 6680 mean train loss:  1.22461261e-02, mean val. rec. loss:  1.10379944e-02\n",
      "Epoch: 6681 mean train loss:  1.22453821e-02, mean val. rec. loss:  1.10373616e-02\n",
      "Epoch: 6682 mean train loss:  1.22446382e-02, mean val. rec. loss:  1.10367436e-02\n",
      "Epoch: 6683 mean train loss:  1.22439017e-02, mean val. rec. loss:  1.10361142e-02\n",
      "Epoch: 6684 mean train loss:  1.22431605e-02, mean val. rec. loss:  1.10354883e-02\n",
      "Epoch: 6685 mean train loss:  1.22424194e-02, mean val. rec. loss:  1.10348646e-02\n",
      "Epoch: 6686 mean train loss:  1.22416782e-02, mean val. rec. loss:  1.10342375e-02\n",
      "Epoch: 6687 mean train loss:  1.22409362e-02, mean val. rec. loss:  1.10336036e-02\n",
      "Epoch: 6688 mean train loss:  1.22401885e-02, mean val. rec. loss:  1.10329845e-02\n",
      "Epoch: 6689 mean train loss:  1.22394511e-02, mean val. rec. loss:  1.10323517e-02\n",
      "Epoch: 6690 mean train loss:  1.22387127e-02, mean val. rec. loss:  1.10317257e-02\n",
      "Epoch: 6691 mean train loss:  1.22379660e-02, mean val. rec. loss:  1.10310930e-02\n",
      "Epoch: 6692 mean train loss:  1.22372174e-02, mean val. rec. loss:  1.10304693e-02\n",
      "Epoch: 6693 mean train loss:  1.22364818e-02, mean val. rec. loss:  1.10298365e-02\n",
      "Epoch: 6694 mean train loss:  1.22357295e-02, mean val. rec. loss:  1.10292117e-02\n",
      "Epoch: 6695 mean train loss:  1.22349930e-02, mean val. rec. loss:  1.10285790e-02\n",
      "Epoch: 6696 mean train loss:  1.22342397e-02, mean val. rec. loss:  1.10279587e-02\n",
      "Epoch: 6697 mean train loss:  1.22335032e-02, mean val. rec. loss:  1.10273225e-02\n",
      "Epoch: 6698 mean train loss:  1.22327537e-02, mean val. rec. loss:  1.10266954e-02\n",
      "Epoch: 6699 mean train loss:  1.22320032e-02, mean val. rec. loss:  1.10260695e-02\n",
      "Epoch: 6700 mean train loss:  1.22312621e-02, mean val. rec. loss:  1.10254412e-02\n",
      "Epoch: 6701 mean train loss:  1.22305209e-02, mean val. rec. loss:  1.10248108e-02\n",
      "Epoch: 6702 mean train loss:  1.22297723e-02, mean val. rec. loss:  1.10241780e-02\n",
      "Epoch: 6703 mean train loss:  1.22290256e-02, mean val. rec. loss:  1.10235475e-02\n",
      "Epoch: 6704 mean train loss:  1.22282788e-02, mean val. rec. loss:  1.10229181e-02\n",
      "Epoch: 6705 mean train loss:  1.22275312e-02, mean val. rec. loss:  1.10222888e-02\n",
      "Epoch: 6706 mean train loss:  1.22267835e-02, mean val. rec. loss:  1.10216560e-02\n",
      "Epoch: 6707 mean train loss:  1.22260302e-02, mean val. rec. loss:  1.10210165e-02\n",
      "Epoch: 6708 mean train loss:  1.22252770e-02, mean val. rec. loss:  1.10203837e-02\n",
      "Epoch: 6709 mean train loss:  1.22245321e-02, mean val. rec. loss:  1.10197589e-02\n",
      "Epoch: 6710 mean train loss:  1.22237872e-02, mean val. rec. loss:  1.10191182e-02\n",
      "Epoch: 6711 mean train loss:  1.22230293e-02, mean val. rec. loss:  1.10184877e-02\n",
      "Epoch: 6712 mean train loss:  1.22222863e-02, mean val. rec. loss:  1.10178481e-02\n",
      "Epoch: 6713 mean train loss:  1.22215284e-02, mean val. rec. loss:  1.10172210e-02\n",
      "Epoch: 6714 mean train loss:  1.22207873e-02, mean val. rec. loss:  1.10165804e-02\n",
      "Epoch: 6715 mean train loss:  1.22200265e-02, mean val. rec. loss:  1.10159476e-02\n",
      "Epoch: 6716 mean train loss:  1.22192789e-02, mean val. rec. loss:  1.10153148e-02\n",
      "Epoch: 6717 mean train loss:  1.22185321e-02, mean val. rec. loss:  1.10146855e-02\n",
      "Epoch: 6718 mean train loss:  1.22177761e-02, mean val. rec. loss:  1.10140402e-02\n",
      "Epoch: 6719 mean train loss:  1.22170200e-02, mean val. rec. loss:  1.10134120e-02\n",
      "Epoch: 6720 mean train loss:  1.22162714e-02, mean val. rec. loss:  1.10127725e-02\n",
      "Epoch: 6721 mean train loss:  1.22155173e-02, mean val. rec. loss:  1.10121374e-02\n",
      "Epoch: 6722 mean train loss:  1.22147649e-02, mean val. rec. loss:  1.10115001e-02\n",
      "Epoch: 6723 mean train loss:  1.22140107e-02, mean val. rec. loss:  1.10108617e-02\n",
      "Epoch: 6724 mean train loss:  1.22132566e-02, mean val. rec. loss:  1.10102165e-02\n",
      "Epoch: 6725 mean train loss:  1.22124986e-02, mean val. rec. loss:  1.10095883e-02\n",
      "Epoch: 6726 mean train loss:  1.22117472e-02, mean val. rec. loss:  1.10089464e-02\n",
      "Epoch: 6727 mean train loss:  1.22109959e-02, mean val. rec. loss:  1.10083103e-02\n",
      "Epoch: 6728 mean train loss:  1.22102379e-02, mean val. rec. loss:  1.10076673e-02\n",
      "Epoch: 6729 mean train loss:  1.22094763e-02, mean val. rec. loss:  1.10070345e-02\n",
      "Epoch: 6730 mean train loss:  1.22087296e-02, mean val. rec. loss:  1.10063893e-02\n",
      "Epoch: 6731 mean train loss:  1.22079642e-02, mean val. rec. loss:  1.10057577e-02\n",
      "Epoch: 6732 mean train loss:  1.22072156e-02, mean val. rec. loss:  1.10051136e-02\n",
      "Epoch: 6733 mean train loss:  1.22064512e-02, mean val. rec. loss:  1.10044786e-02\n",
      "Epoch: 6734 mean train loss:  1.22057016e-02, mean val. rec. loss:  1.10038322e-02\n",
      "Epoch: 6735 mean train loss:  1.22049400e-02, mean val. rec. loss:  1.10031904e-02\n",
      "Epoch: 6736 mean train loss:  1.22041765e-02, mean val. rec. loss:  1.10025565e-02\n",
      "Epoch: 6737 mean train loss:  1.22034223e-02, mean val. rec. loss:  1.10019180e-02\n",
      "Epoch: 6738 mean train loss:  1.22026681e-02, mean val. rec. loss:  1.10012751e-02\n",
      "Epoch: 6739 mean train loss:  1.22019093e-02, mean val. rec. loss:  1.10006298e-02\n",
      "Epoch: 6740 mean train loss:  1.22011495e-02, mean val. rec. loss:  1.09999880e-02\n",
      "Epoch: 6741 mean train loss:  1.22003897e-02, mean val. rec. loss:  1.09993462e-02\n",
      "Epoch: 6742 mean train loss:  1.21996300e-02, mean val. rec. loss:  1.09987032e-02\n",
      "Epoch: 6743 mean train loss:  1.21988693e-02, mean val. rec. loss:  1.09980602e-02\n",
      "Epoch: 6744 mean train loss:  1.21981039e-02, mean val. rec. loss:  1.09974093e-02\n",
      "Epoch: 6745 mean train loss:  1.21973385e-02, mean val. rec. loss:  1.09967652e-02\n",
      "Epoch: 6746 mean train loss:  1.21965825e-02, mean val. rec. loss:  1.09961268e-02\n",
      "Epoch: 6747 mean train loss:  1.21958237e-02, mean val. rec. loss:  1.09954759e-02\n",
      "Epoch: 6748 mean train loss:  1.21950536e-02, mean val. rec. loss:  1.09948352e-02\n",
      "Epoch: 6749 mean train loss:  1.21942995e-02, mean val. rec. loss:  1.09941866e-02\n",
      "Epoch: 6750 mean train loss:  1.21935266e-02, mean val. rec. loss:  1.09935459e-02\n",
      "Epoch: 6751 mean train loss:  1.21927725e-02, mean val. rec. loss:  1.09928950e-02\n",
      "Epoch: 6752 mean train loss:  1.21920006e-02, mean val. rec. loss:  1.09922509e-02\n",
      "Epoch: 6753 mean train loss:  1.21912399e-02, mean val. rec. loss:  1.09916057e-02\n",
      "Epoch: 6754 mean train loss:  1.21904801e-02, mean val. rec. loss:  1.09909661e-02\n",
      "Epoch: 6755 mean train loss:  1.21897110e-02, mean val. rec. loss:  1.09903072e-02\n",
      "Epoch: 6756 mean train loss:  1.21889419e-02, mean val. rec. loss:  1.09896666e-02\n",
      "Epoch: 6757 mean train loss:  1.21881803e-02, mean val. rec. loss:  1.09890179e-02\n",
      "Epoch: 6758 mean train loss:  1.21874149e-02, mean val. rec. loss:  1.09883704e-02\n",
      "Epoch: 6759 mean train loss:  1.21866496e-02, mean val. rec. loss:  1.09877229e-02\n",
      "Epoch: 6760 mean train loss:  1.21858824e-02, mean val. rec. loss:  1.09870754e-02\n",
      "Epoch: 6761 mean train loss:  1.21851161e-02, mean val. rec. loss:  1.09864211e-02\n",
      "Epoch: 6762 mean train loss:  1.21843442e-02, mean val. rec. loss:  1.09857815e-02\n",
      "Epoch: 6763 mean train loss:  1.21835807e-02, mean val. rec. loss:  1.09851261e-02\n",
      "Epoch: 6764 mean train loss:  1.21828172e-02, mean val. rec. loss:  1.09844741e-02\n",
      "Epoch: 6765 mean train loss:  1.21820453e-02, mean val. rec. loss:  1.09838220e-02\n",
      "Epoch: 6766 mean train loss:  1.21812725e-02, mean val. rec. loss:  1.09831757e-02\n",
      "Epoch: 6767 mean train loss:  1.21805127e-02, mean val. rec. loss:  1.09825214e-02\n",
      "Epoch: 6768 mean train loss:  1.21797343e-02, mean val. rec. loss:  1.09818739e-02\n",
      "Epoch: 6769 mean train loss:  1.21789736e-02, mean val. rec. loss:  1.09812196e-02\n",
      "Epoch: 6770 mean train loss:  1.21781952e-02, mean val. rec. loss:  1.09805755e-02\n",
      "Epoch: 6771 mean train loss:  1.21774327e-02, mean val. rec. loss:  1.09799155e-02\n",
      "Epoch: 6772 mean train loss:  1.21766589e-02, mean val. rec. loss:  1.09792635e-02\n",
      "Epoch: 6773 mean train loss:  1.21758833e-02, mean val. rec. loss:  1.09786171e-02\n",
      "Epoch: 6774 mean train loss:  1.21751152e-02, mean val. rec. loss:  1.09779684e-02\n",
      "Epoch: 6775 mean train loss:  1.21743479e-02, mean val. rec. loss:  1.09773107e-02\n",
      "Epoch: 6776 mean train loss:  1.21735761e-02, mean val. rec. loss:  1.09766564e-02\n",
      "Epoch: 6777 mean train loss:  1.21728033e-02, mean val. rec. loss:  1.09760021e-02\n",
      "Epoch: 6778 mean train loss:  1.21720314e-02, mean val. rec. loss:  1.09753490e-02\n",
      "Epoch: 6779 mean train loss:  1.21712576e-02, mean val. rec. loss:  1.09746958e-02\n",
      "Epoch: 6780 mean train loss:  1.21704839e-02, mean val. rec. loss:  1.09740381e-02\n",
      "Epoch: 6781 mean train loss:  1.21697055e-02, mean val. rec. loss:  1.09733781e-02\n",
      "Epoch: 6782 mean train loss:  1.21689271e-02, mean val. rec. loss:  1.09727181e-02\n",
      "Epoch: 6783 mean train loss:  1.21681571e-02, mean val. rec. loss:  1.09720706e-02\n",
      "Epoch: 6784 mean train loss:  1.21673861e-02, mean val. rec. loss:  1.09714141e-02\n",
      "Epoch: 6785 mean train loss:  1.21666068e-02, mean val. rec. loss:  1.09707620e-02\n",
      "Epoch: 6786 mean train loss:  1.21658321e-02, mean val. rec. loss:  1.09700998e-02\n",
      "Epoch: 6787 mean train loss:  1.21650556e-02, mean val. rec. loss:  1.09694375e-02\n",
      "Epoch: 6788 mean train loss:  1.21642791e-02, mean val. rec. loss:  1.09687855e-02\n",
      "Epoch: 6789 mean train loss:  1.21635025e-02, mean val. rec. loss:  1.09681255e-02\n",
      "Epoch: 6790 mean train loss:  1.21627288e-02, mean val. rec. loss:  1.09674610e-02\n",
      "Epoch: 6791 mean train loss:  1.21619476e-02, mean val. rec. loss:  1.09668033e-02\n",
      "Epoch: 6792 mean train loss:  1.21611664e-02, mean val. rec. loss:  1.09661501e-02\n",
      "Epoch: 6793 mean train loss:  1.21603908e-02, mean val. rec. loss:  1.09654924e-02\n",
      "Epoch: 6794 mean train loss:  1.21596171e-02, mean val. rec. loss:  1.09648313e-02\n",
      "Epoch: 6795 mean train loss:  1.21588378e-02, mean val. rec. loss:  1.09641714e-02\n",
      "Epoch: 6796 mean train loss:  1.21580594e-02, mean val. rec. loss:  1.09635102e-02\n",
      "Epoch: 6797 mean train loss:  1.21572791e-02, mean val. rec. loss:  1.09628503e-02\n",
      "Epoch: 6798 mean train loss:  1.21564988e-02, mean val. rec. loss:  1.09621903e-02\n",
      "Epoch: 6799 mean train loss:  1.21557186e-02, mean val. rec. loss:  1.09615281e-02\n",
      "Epoch: 6800 mean train loss:  1.21549327e-02, mean val. rec. loss:  1.09608647e-02\n",
      "Epoch: 6801 mean train loss:  1.21541488e-02, mean val. rec. loss:  1.09602115e-02\n",
      "Epoch: 6802 mean train loss:  1.21533713e-02, mean val. rec. loss:  1.09595470e-02\n",
      "Epoch: 6803 mean train loss:  1.21525929e-02, mean val. rec. loss:  1.09588768e-02\n",
      "Epoch: 6804 mean train loss:  1.21518024e-02, mean val. rec. loss:  1.09582168e-02\n",
      "Epoch: 6805 mean train loss:  1.21510277e-02, mean val. rec. loss:  1.09575512e-02\n",
      "Epoch: 6806 mean train loss:  1.21502363e-02, mean val. rec. loss:  1.09568924e-02\n",
      "Epoch: 6807 mean train loss:  1.21494616e-02, mean val. rec. loss:  1.09562256e-02\n",
      "Epoch: 6808 mean train loss:  1.21486702e-02, mean val. rec. loss:  1.09555633e-02\n",
      "Epoch: 6809 mean train loss:  1.21478890e-02, mean val. rec. loss:  1.09548988e-02\n",
      "Epoch: 6810 mean train loss:  1.21471088e-02, mean val. rec. loss:  1.09542411e-02\n",
      "Epoch: 6811 mean train loss:  1.21463210e-02, mean val. rec. loss:  1.09535630e-02\n",
      "Epoch: 6812 mean train loss:  1.21455305e-02, mean val. rec. loss:  1.09529019e-02\n",
      "Epoch: 6813 mean train loss:  1.21447494e-02, mean val. rec. loss:  1.09522363e-02\n",
      "Epoch: 6814 mean train loss:  1.21439644e-02, mean val. rec. loss:  1.09515695e-02\n",
      "Epoch: 6815 mean train loss:  1.21431786e-02, mean val. rec. loss:  1.09509027e-02\n",
      "Epoch: 6816 mean train loss:  1.21423909e-02, mean val. rec. loss:  1.09502359e-02\n",
      "Epoch: 6817 mean train loss:  1.21416032e-02, mean val. rec. loss:  1.09495623e-02\n",
      "Epoch: 6818 mean train loss:  1.21408127e-02, mean val. rec. loss:  1.09489058e-02\n",
      "Epoch: 6819 mean train loss:  1.21400278e-02, mean val. rec. loss:  1.09482310e-02\n",
      "Epoch: 6820 mean train loss:  1.21392438e-02, mean val. rec. loss:  1.09475643e-02\n",
      "Epoch: 6821 mean train loss:  1.21384524e-02, mean val. rec. loss:  1.09468907e-02\n",
      "Epoch: 6822 mean train loss:  1.21376591e-02, mean val. rec. loss:  1.09462262e-02\n",
      "Epoch: 6823 mean train loss:  1.21368788e-02, mean val. rec. loss:  1.09455503e-02\n",
      "Epoch: 6824 mean train loss:  1.21360799e-02, mean val. rec. loss:  1.09448858e-02\n",
      "Epoch: 6825 mean train loss:  1.21352978e-02, mean val. rec. loss:  1.09442111e-02\n",
      "Epoch: 6826 mean train loss:  1.21344989e-02, mean val. rec. loss:  1.09435477e-02\n",
      "Epoch: 6827 mean train loss:  1.21337168e-02, mean val. rec. loss:  1.09428809e-02\n",
      "Epoch: 6828 mean train loss:  1.21329217e-02, mean val. rec. loss:  1.09421983e-02\n",
      "Epoch: 6829 mean train loss:  1.21321246e-02, mean val. rec. loss:  1.09415315e-02\n",
      "Epoch: 6830 mean train loss:  1.21313369e-02, mean val. rec. loss:  1.09408625e-02\n",
      "Epoch: 6831 mean train loss:  1.21305492e-02, mean val. rec. loss:  1.09401889e-02\n",
      "Epoch: 6832 mean train loss:  1.21297569e-02, mean val. rec. loss:  1.09395153e-02\n",
      "Epoch: 6833 mean train loss:  1.21289645e-02, mean val. rec. loss:  1.09388440e-02\n",
      "Epoch: 6834 mean train loss:  1.21281693e-02, mean val. rec. loss:  1.09381704e-02\n",
      "Epoch: 6835 mean train loss:  1.21273761e-02, mean val. rec. loss:  1.09374980e-02\n",
      "Epoch: 6836 mean train loss:  1.21265818e-02, mean val. rec. loss:  1.09368210e-02\n",
      "Epoch: 6837 mean train loss:  1.21257829e-02, mean val. rec. loss:  1.09361429e-02\n",
      "Epoch: 6838 mean train loss:  1.21249841e-02, mean val. rec. loss:  1.09354681e-02\n",
      "Epoch: 6839 mean train loss:  1.21241936e-02, mean val. rec. loss:  1.09348025e-02\n",
      "Epoch: 6840 mean train loss:  1.21234012e-02, mean val. rec. loss:  1.09341278e-02\n",
      "Epoch: 6841 mean train loss:  1.21226014e-02, mean val. rec. loss:  1.09334440e-02\n",
      "Epoch: 6842 mean train loss:  1.21218044e-02, mean val. rec. loss:  1.09327727e-02\n",
      "Epoch: 6843 mean train loss:  1.21210074e-02, mean val. rec. loss:  1.09321036e-02\n",
      "Epoch: 6844 mean train loss:  1.21202103e-02, mean val. rec. loss:  1.09314210e-02\n",
      "Epoch: 6845 mean train loss:  1.21194124e-02, mean val. rec. loss:  1.09307451e-02\n",
      "Epoch: 6846 mean train loss:  1.21186172e-02, mean val. rec. loss:  1.09300579e-02\n",
      "Epoch: 6847 mean train loss:  1.21178156e-02, mean val. rec. loss:  1.09293798e-02\n",
      "Epoch: 6848 mean train loss:  1.21170130e-02, mean val. rec. loss:  1.09287085e-02\n",
      "Epoch: 6849 mean train loss:  1.21162178e-02, mean val. rec. loss:  1.09280304e-02\n",
      "Epoch: 6850 mean train loss:  1.21154217e-02, mean val. rec. loss:  1.09273500e-02\n",
      "Epoch: 6851 mean train loss:  1.21146219e-02, mean val. rec. loss:  1.09266719e-02\n",
      "Epoch: 6852 mean train loss:  1.21138221e-02, mean val. rec. loss:  1.09259892e-02\n",
      "Epoch: 6853 mean train loss:  1.21130204e-02, mean val. rec. loss:  1.09253111e-02\n",
      "Epoch: 6854 mean train loss:  1.21122188e-02, mean val. rec. loss:  1.09246307e-02\n",
      "Epoch: 6855 mean train loss:  1.21114171e-02, mean val. rec. loss:  1.09239481e-02\n",
      "Epoch: 6856 mean train loss:  1.21106117e-02, mean val. rec. loss:  1.09232609e-02\n",
      "Epoch: 6857 mean train loss:  1.21098044e-02, mean val. rec. loss:  1.09225805e-02\n",
      "Epoch: 6858 mean train loss:  1.21090074e-02, mean val. rec. loss:  1.09219069e-02\n",
      "Epoch: 6859 mean train loss:  1.21082085e-02, mean val. rec. loss:  1.09212254e-02\n",
      "Epoch: 6860 mean train loss:  1.21074003e-02, mean val. rec. loss:  1.09205484e-02\n",
      "Epoch: 6861 mean train loss:  1.21065959e-02, mean val. rec. loss:  1.09198601e-02\n",
      "Epoch: 6862 mean train loss:  1.21057914e-02, mean val. rec. loss:  1.09191718e-02\n",
      "Epoch: 6863 mean train loss:  1.21049869e-02, mean val. rec. loss:  1.09184936e-02\n",
      "Epoch: 6864 mean train loss:  1.21041825e-02, mean val. rec. loss:  1.09178087e-02\n",
      "Epoch: 6865 mean train loss:  1.21033799e-02, mean val. rec. loss:  1.09171170e-02\n",
      "Epoch: 6866 mean train loss:  1.21025707e-02, mean val. rec. loss:  1.09164343e-02\n",
      "Epoch: 6867 mean train loss:  1.21017607e-02, mean val. rec. loss:  1.09157528e-02\n",
      "Epoch: 6868 mean train loss:  1.21009572e-02, mean val. rec. loss:  1.09150690e-02\n",
      "Epoch: 6869 mean train loss:  1.21001545e-02, mean val. rec. loss:  1.09143841e-02\n",
      "Epoch: 6870 mean train loss:  1.20993473e-02, mean val. rec. loss:  1.09136958e-02\n",
      "Epoch: 6871 mean train loss:  1.20985400e-02, mean val. rec. loss:  1.09130097e-02\n",
      "Epoch: 6872 mean train loss:  1.20977318e-02, mean val. rec. loss:  1.09123237e-02\n",
      "Epoch: 6873 mean train loss:  1.20969218e-02, mean val. rec. loss:  1.09116376e-02\n",
      "Epoch: 6874 mean train loss:  1.20961136e-02, mean val. rec. loss:  1.09109482e-02\n",
      "Epoch: 6875 mean train loss:  1.20953008e-02, mean val. rec. loss:  1.09102542e-02\n",
      "Epoch: 6876 mean train loss:  1.20944870e-02, mean val. rec. loss:  1.09095761e-02\n",
      "Epoch: 6877 mean train loss:  1.20936797e-02, mean val. rec. loss:  1.09088821e-02\n",
      "Epoch: 6878 mean train loss:  1.20928725e-02, mean val. rec. loss:  1.09081881e-02\n",
      "Epoch: 6879 mean train loss:  1.20920540e-02, mean val. rec. loss:  1.09075031e-02\n",
      "Epoch: 6880 mean train loss:  1.20912496e-02, mean val. rec. loss:  1.09068046e-02\n",
      "Epoch: 6881 mean train loss:  1.20904302e-02, mean val. rec. loss:  1.09061208e-02\n",
      "Epoch: 6882 mean train loss:  1.20896257e-02, mean val. rec. loss:  1.09054280e-02\n",
      "Epoch: 6883 mean train loss:  1.20888064e-02, mean val. rec. loss:  1.09047385e-02\n",
      "Epoch: 6884 mean train loss:  1.20879972e-02, mean val. rec. loss:  1.09040468e-02\n",
      "Epoch: 6885 mean train loss:  1.20871863e-02, mean val. rec. loss:  1.09033619e-02\n",
      "Epoch: 6886 mean train loss:  1.20863688e-02, mean val. rec. loss:  1.09026599e-02\n",
      "Epoch: 6887 mean train loss:  1.20855513e-02, mean val. rec. loss:  1.09019739e-02\n",
      "Epoch: 6888 mean train loss:  1.20847403e-02, mean val. rec. loss:  1.09012799e-02\n",
      "Epoch: 6889 mean train loss:  1.20839256e-02, mean val. rec. loss:  1.09005882e-02\n",
      "Epoch: 6890 mean train loss:  1.20831099e-02, mean val. rec. loss:  1.08998930e-02\n",
      "Epoch: 6891 mean train loss:  1.20822943e-02, mean val. rec. loss:  1.08992002e-02\n",
      "Epoch: 6892 mean train loss:  1.20814786e-02, mean val. rec. loss:  1.08985062e-02\n",
      "Epoch: 6893 mean train loss:  1.20806611e-02, mean val. rec. loss:  1.08978065e-02\n",
      "Epoch: 6894 mean train loss:  1.20798399e-02, mean val. rec. loss:  1.08971205e-02\n",
      "Epoch: 6895 mean train loss:  1.20790261e-02, mean val. rec. loss:  1.08964208e-02\n",
      "Epoch: 6896 mean train loss:  1.20782133e-02, mean val. rec. loss:  1.08957257e-02\n",
      "Epoch: 6897 mean train loss:  1.20773902e-02, mean val. rec. loss:  1.08950215e-02\n",
      "Epoch: 6898 mean train loss:  1.20765718e-02, mean val. rec. loss:  1.08943286e-02\n",
      "Epoch: 6899 mean train loss:  1.20757524e-02, mean val. rec. loss:  1.08936391e-02\n",
      "Epoch: 6900 mean train loss:  1.20749330e-02, mean val. rec. loss:  1.08929349e-02\n",
      "Epoch: 6901 mean train loss:  1.20741137e-02, mean val. rec. loss:  1.08922353e-02\n",
      "Epoch: 6902 mean train loss:  1.20732943e-02, mean val. rec. loss:  1.08915356e-02\n",
      "Epoch: 6903 mean train loss:  1.20724675e-02, mean val. rec. loss:  1.08908428e-02\n",
      "Epoch: 6904 mean train loss:  1.20716519e-02, mean val. rec. loss:  1.08901408e-02\n",
      "Epoch: 6905 mean train loss:  1.20708334e-02, mean val. rec. loss:  1.08894502e-02\n",
      "Epoch: 6906 mean train loss:  1.20700075e-02, mean val. rec. loss:  1.08887415e-02\n",
      "Epoch: 6907 mean train loss:  1.20691817e-02, mean val. rec. loss:  1.08880396e-02\n",
      "Epoch: 6908 mean train loss:  1.20683586e-02, mean val. rec. loss:  1.08873388e-02\n",
      "Epoch: 6909 mean train loss:  1.20675355e-02, mean val. rec. loss:  1.08866380e-02\n",
      "Epoch: 6910 mean train loss:  1.20667124e-02, mean val. rec. loss:  1.08859349e-02\n",
      "Epoch: 6911 mean train loss:  1.20658875e-02, mean val. rec. loss:  1.08852364e-02\n",
      "Epoch: 6912 mean train loss:  1.20650634e-02, mean val. rec. loss:  1.08845310e-02\n",
      "Epoch: 6913 mean train loss:  1.20642431e-02, mean val. rec. loss:  1.08838348e-02\n",
      "Epoch: 6914 mean train loss:  1.20634210e-02, mean val. rec. loss:  1.08831329e-02\n",
      "Epoch: 6915 mean train loss:  1.20625923e-02, mean val. rec. loss:  1.08824264e-02\n",
      "Epoch: 6916 mean train loss:  1.20617618e-02, mean val. rec. loss:  1.08817233e-02\n",
      "Epoch: 6917 mean train loss:  1.20609433e-02, mean val. rec. loss:  1.08810146e-02\n",
      "Epoch: 6918 mean train loss:  1.20601081e-02, mean val. rec. loss:  1.08803149e-02\n",
      "Epoch: 6919 mean train loss:  1.20592888e-02, mean val. rec. loss:  1.08796073e-02\n",
      "Epoch: 6920 mean train loss:  1.20584527e-02, mean val. rec. loss:  1.08789099e-02\n",
      "Epoch: 6921 mean train loss:  1.20576333e-02, mean val. rec. loss:  1.08782080e-02\n",
      "Epoch: 6922 mean train loss:  1.20568009e-02, mean val. rec. loss:  1.08774913e-02\n",
      "Epoch: 6923 mean train loss:  1.20559666e-02, mean val. rec. loss:  1.08767905e-02\n",
      "Epoch: 6924 mean train loss:  1.20551417e-02, mean val. rec. loss:  1.08760829e-02\n",
      "Epoch: 6925 mean train loss:  1.20543167e-02, mean val. rec. loss:  1.08753731e-02\n",
      "Epoch: 6926 mean train loss:  1.20534834e-02, mean val. rec. loss:  1.08746643e-02\n",
      "Epoch: 6927 mean train loss:  1.20526519e-02, mean val. rec. loss:  1.08739556e-02\n",
      "Epoch: 6928 mean train loss:  1.20518214e-02, mean val. rec. loss:  1.08732491e-02\n",
      "Epoch: 6929 mean train loss:  1.20509890e-02, mean val. rec. loss:  1.08725426e-02\n",
      "Epoch: 6930 mean train loss:  1.20501585e-02, mean val. rec. loss:  1.08718328e-02\n",
      "Epoch: 6931 mean train loss:  1.20493261e-02, mean val. rec. loss:  1.08711297e-02\n",
      "Epoch: 6932 mean train loss:  1.20484955e-02, mean val. rec. loss:  1.08704198e-02\n",
      "Epoch: 6933 mean train loss:  1.20476594e-02, mean val. rec. loss:  1.08697066e-02\n",
      "Epoch: 6934 mean train loss:  1.20468214e-02, mean val. rec. loss:  1.08690046e-02\n",
      "Epoch: 6935 mean train loss:  1.20459909e-02, mean val. rec. loss:  1.08682857e-02\n",
      "Epoch: 6936 mean train loss:  1.20451566e-02, mean val. rec. loss:  1.08675679e-02\n",
      "Epoch: 6937 mean train loss:  1.20443224e-02, mean val. rec. loss:  1.08668603e-02\n",
      "Epoch: 6938 mean train loss:  1.20434881e-02, mean val. rec. loss:  1.08661550e-02\n",
      "Epoch: 6939 mean train loss:  1.20426510e-02, mean val. rec. loss:  1.08654371e-02\n",
      "Epoch: 6940 mean train loss:  1.20418158e-02, mean val. rec. loss:  1.08647227e-02\n",
      "Epoch: 6941 mean train loss:  1.20409834e-02, mean val. rec. loss:  1.08640140e-02\n",
      "Epoch: 6942 mean train loss:  1.20401417e-02, mean val. rec. loss:  1.08632883e-02\n",
      "Epoch: 6943 mean train loss:  1.20393000e-02, mean val. rec. loss:  1.08625784e-02\n",
      "Epoch: 6944 mean train loss:  1.20384667e-02, mean val. rec. loss:  1.08618674e-02\n",
      "Epoch: 6945 mean train loss:  1.20376334e-02, mean val. rec. loss:  1.08611530e-02\n",
      "Epoch: 6946 mean train loss:  1.20367935e-02, mean val. rec. loss:  1.08604363e-02\n",
      "Epoch: 6947 mean train loss:  1.20359537e-02, mean val. rec. loss:  1.08597219e-02\n",
      "Epoch: 6948 mean train loss:  1.20351157e-02, mean val. rec. loss:  1.08590052e-02\n",
      "Epoch: 6949 mean train loss:  1.20342758e-02, mean val. rec. loss:  1.08582908e-02\n",
      "Epoch: 6950 mean train loss:  1.20334360e-02, mean val. rec. loss:  1.08575742e-02\n",
      "Epoch: 6951 mean train loss:  1.20325906e-02, mean val. rec. loss:  1.08568518e-02\n",
      "Epoch: 6952 mean train loss:  1.20317451e-02, mean val. rec. loss:  1.08561340e-02\n",
      "Epoch: 6953 mean train loss:  1.20309090e-02, mean val. rec. loss:  1.08554241e-02\n",
      "Epoch: 6954 mean train loss:  1.20300710e-02, mean val. rec. loss:  1.08547052e-02\n",
      "Epoch: 6955 mean train loss:  1.20292237e-02, mean val. rec. loss:  1.08539772e-02\n",
      "Epoch: 6956 mean train loss:  1.20283801e-02, mean val. rec. loss:  1.08532628e-02\n",
      "Epoch: 6957 mean train loss:  1.20275375e-02, mean val. rec. loss:  1.08525484e-02\n",
      "Epoch: 6958 mean train loss:  1.20266949e-02, mean val. rec. loss:  1.08518238e-02\n",
      "Epoch: 6959 mean train loss:  1.20258494e-02, mean val. rec. loss:  1.08511026e-02\n",
      "Epoch: 6960 mean train loss:  1.20250086e-02, mean val. rec. loss:  1.08503734e-02\n",
      "Epoch: 6961 mean train loss:  1.20241595e-02, mean val. rec. loss:  1.08496533e-02\n",
      "Epoch: 6962 mean train loss:  1.20233103e-02, mean val. rec. loss:  1.08489367e-02\n",
      "Epoch: 6963 mean train loss:  1.20224677e-02, mean val. rec. loss:  1.08482143e-02\n",
      "Epoch: 6964 mean train loss:  1.20216260e-02, mean val. rec. loss:  1.08474908e-02\n",
      "Epoch: 6965 mean train loss:  1.20207787e-02, mean val. rec. loss:  1.08467674e-02\n",
      "Epoch: 6966 mean train loss:  1.20199314e-02, mean val. rec. loss:  1.08460439e-02\n",
      "Epoch: 6967 mean train loss:  1.20190841e-02, mean val. rec. loss:  1.08453227e-02\n",
      "Epoch: 6968 mean train loss:  1.20182359e-02, mean val. rec. loss:  1.08446037e-02\n",
      "Epoch: 6969 mean train loss:  1.20173886e-02, mean val. rec. loss:  1.08438757e-02\n",
      "Epoch: 6970 mean train loss:  1.20165347e-02, mean val. rec. loss:  1.08431454e-02\n",
      "Epoch: 6971 mean train loss:  1.20156819e-02, mean val. rec. loss:  1.08424186e-02\n",
      "Epoch: 6972 mean train loss:  1.20148364e-02, mean val. rec. loss:  1.08417008e-02\n",
      "Epoch: 6973 mean train loss:  1.20139891e-02, mean val. rec. loss:  1.08409739e-02\n",
      "Epoch: 6974 mean train loss:  1.20131353e-02, mean val. rec. loss:  1.08402538e-02\n",
      "Epoch: 6975 mean train loss:  1.20122834e-02, mean val. rec. loss:  1.08395201e-02\n",
      "Epoch: 6976 mean train loss:  1.20114323e-02, mean val. rec. loss:  1.08387887e-02\n",
      "Epoch: 6977 mean train loss:  1.20105795e-02, mean val. rec. loss:  1.08380641e-02\n",
      "Epoch: 6978 mean train loss:  1.20097284e-02, mean val. rec. loss:  1.08373338e-02\n",
      "Epoch: 6979 mean train loss:  1.20088793e-02, mean val. rec. loss:  1.08365979e-02\n",
      "Epoch: 6980 mean train loss:  1.20080208e-02, mean val. rec. loss:  1.08358687e-02\n",
      "Epoch: 6981 mean train loss:  1.20071633e-02, mean val. rec. loss:  1.08351418e-02\n",
      "Epoch: 6982 mean train loss:  1.20063132e-02, mean val. rec. loss:  1.08344150e-02\n",
      "Epoch: 6983 mean train loss:  1.20054622e-02, mean val. rec. loss:  1.08336835e-02\n",
      "Epoch: 6984 mean train loss:  1.20046083e-02, mean val. rec. loss:  1.08329533e-02\n",
      "Epoch: 6985 mean train loss:  1.20037517e-02, mean val. rec. loss:  1.08322218e-02\n",
      "Epoch: 6986 mean train loss:  1.20028961e-02, mean val. rec. loss:  1.08314916e-02\n",
      "Epoch: 6987 mean train loss:  1.20020395e-02, mean val. rec. loss:  1.08307601e-02\n",
      "Epoch: 6988 mean train loss:  1.20011828e-02, mean val. rec. loss:  1.08300287e-02\n",
      "Epoch: 6989 mean train loss:  1.20003216e-02, mean val. rec. loss:  1.08292905e-02\n",
      "Epoch: 6990 mean train loss:  1.19994603e-02, mean val. rec. loss:  1.08285670e-02\n",
      "Epoch: 6991 mean train loss:  1.19986056e-02, mean val. rec. loss:  1.08278277e-02\n",
      "Epoch: 6992 mean train loss:  1.19977508e-02, mean val. rec. loss:  1.08270929e-02\n",
      "Epoch: 6993 mean train loss:  1.19968896e-02, mean val. rec. loss:  1.08263626e-02\n",
      "Epoch: 6994 mean train loss:  1.19960292e-02, mean val. rec. loss:  1.08256187e-02\n",
      "Epoch: 6995 mean train loss:  1.19951698e-02, mean val. rec. loss:  1.08248771e-02\n",
      "Epoch: 6996 mean train loss:  1.19943095e-02, mean val. rec. loss:  1.08241468e-02\n",
      "Epoch: 6997 mean train loss:  1.19934482e-02, mean val. rec. loss:  1.08234188e-02\n",
      "Epoch: 6998 mean train loss:  1.19925870e-02, mean val. rec. loss:  1.08226692e-02\n",
      "Epoch: 6999 mean train loss:  1.19917211e-02, mean val. rec. loss:  1.08219367e-02\n",
      "Epoch: 7000 mean train loss:  1.19908626e-02, mean val. rec. loss:  1.08212007e-02\n",
      "Epoch: 7001 mean train loss:  1.19900041e-02, mean val. rec. loss:  1.08204591e-02\n",
      "Epoch: 7002 mean train loss:  1.19891373e-02, mean val. rec. loss:  1.08197231e-02\n",
      "Epoch: 7003 mean train loss:  1.19882695e-02, mean val. rec. loss:  1.08189849e-02\n",
      "Epoch: 7004 mean train loss:  1.19874064e-02, mean val. rec. loss:  1.08182467e-02\n",
      "Epoch: 7005 mean train loss:  1.19865414e-02, mean val. rec. loss:  1.08175062e-02\n",
      "Epoch: 7006 mean train loss:  1.19856764e-02, mean val. rec. loss:  1.08167669e-02\n",
      "Epoch: 7007 mean train loss:  1.19848105e-02, mean val. rec. loss:  1.08160230e-02\n",
      "Epoch: 7008 mean train loss:  1.19839492e-02, mean val. rec. loss:  1.08152870e-02\n",
      "Epoch: 7009 mean train loss:  1.19830879e-02, mean val. rec. loss:  1.08145443e-02\n",
      "Epoch: 7010 mean train loss:  1.19822164e-02, mean val. rec. loss:  1.08137970e-02\n",
      "Epoch: 7011 mean train loss:  1.19813449e-02, mean val. rec. loss:  1.08130667e-02\n",
      "Epoch: 7012 mean train loss:  1.19804809e-02, mean val. rec. loss:  1.08123160e-02\n",
      "Epoch: 7013 mean train loss:  1.19796150e-02, mean val. rec. loss:  1.08115653e-02\n",
      "Epoch: 7014 mean train loss:  1.19787453e-02, mean val. rec. loss:  1.08108271e-02\n",
      "Epoch: 7015 mean train loss:  1.19778766e-02, mean val. rec. loss:  1.08100889e-02\n",
      "Epoch: 7016 mean train loss:  1.19770079e-02, mean val. rec. loss:  1.08093416e-02\n",
      "Epoch: 7017 mean train loss:  1.19761382e-02, mean val. rec. loss:  1.08085966e-02\n",
      "Epoch: 7018 mean train loss:  1.19752714e-02, mean val. rec. loss:  1.08078561e-02\n",
      "Epoch: 7019 mean train loss:  1.19743971e-02, mean val. rec. loss:  1.08070997e-02\n",
      "Epoch: 7020 mean train loss:  1.19735219e-02, mean val. rec. loss:  1.08063581e-02\n",
      "Epoch: 7021 mean train loss:  1.19726541e-02, mean val. rec. loss:  1.08056165e-02\n",
      "Epoch: 7022 mean train loss:  1.19717854e-02, mean val. rec. loss:  1.08048692e-02\n",
      "Epoch: 7023 mean train loss:  1.19709139e-02, mean val. rec. loss:  1.08041208e-02\n",
      "Epoch: 7024 mean train loss:  1.19700414e-02, mean val. rec. loss:  1.08033746e-02\n",
      "Epoch: 7025 mean train loss:  1.19691671e-02, mean val. rec. loss:  1.08026262e-02\n",
      "Epoch: 7026 mean train loss:  1.19682928e-02, mean val. rec. loss:  1.08018800e-02\n",
      "Epoch: 7027 mean train loss:  1.19674195e-02, mean val. rec. loss:  1.08011282e-02\n",
      "Epoch: 7028 mean train loss:  1.19665414e-02, mean val. rec. loss:  1.08003741e-02\n",
      "Epoch: 7029 mean train loss:  1.19656606e-02, mean val. rec. loss:  1.07996211e-02\n",
      "Epoch: 7030 mean train loss:  1.19647901e-02, mean val. rec. loss:  1.07988773e-02\n",
      "Epoch: 7031 mean train loss:  1.19639186e-02, mean val. rec. loss:  1.07981175e-02\n",
      "Epoch: 7032 mean train loss:  1.19630331e-02, mean val. rec. loss:  1.07973713e-02\n",
      "Epoch: 7033 mean train loss:  1.19621644e-02, mean val. rec. loss:  1.07966138e-02\n",
      "Epoch: 7034 mean train loss:  1.19612780e-02, mean val. rec. loss:  1.07958677e-02\n",
      "Epoch: 7035 mean train loss:  1.19604102e-02, mean val. rec. loss:  1.07951102e-02\n",
      "Epoch: 7036 mean train loss:  1.19595219e-02, mean val. rec. loss:  1.07943618e-02\n",
      "Epoch: 7037 mean train loss:  1.19586476e-02, mean val. rec. loss:  1.07936043e-02\n",
      "Epoch: 7038 mean train loss:  1.19577715e-02, mean val. rec. loss:  1.07928445e-02\n",
      "Epoch: 7039 mean train loss:  1.19568888e-02, mean val. rec. loss:  1.07920938e-02\n",
      "Epoch: 7040 mean train loss:  1.19560042e-02, mean val. rec. loss:  1.07913443e-02\n",
      "Epoch: 7041 mean train loss:  1.19551272e-02, mean val. rec. loss:  1.07905879e-02\n",
      "Epoch: 7042 mean train loss:  1.19542463e-02, mean val. rec. loss:  1.07898349e-02\n",
      "Epoch: 7043 mean train loss:  1.19533655e-02, mean val. rec. loss:  1.07890763e-02\n",
      "Epoch: 7044 mean train loss:  1.19524828e-02, mean val. rec. loss:  1.07883188e-02\n",
      "Epoch: 7045 mean train loss:  1.19515992e-02, mean val. rec. loss:  1.07875636e-02\n",
      "Epoch: 7046 mean train loss:  1.19507175e-02, mean val. rec. loss:  1.07868004e-02\n",
      "Epoch: 7047 mean train loss:  1.19498292e-02, mean val. rec. loss:  1.07860395e-02\n",
      "Epoch: 7048 mean train loss:  1.19489503e-02, mean val. rec. loss:  1.07852854e-02\n",
      "Epoch: 7049 mean train loss:  1.19480695e-02, mean val. rec. loss:  1.07845268e-02\n",
      "Epoch: 7050 mean train loss:  1.19471803e-02, mean val. rec. loss:  1.07837659e-02\n",
      "Epoch: 7051 mean train loss:  1.19462911e-02, mean val. rec. loss:  1.07830095e-02\n",
      "Epoch: 7052 mean train loss:  1.19454130e-02, mean val. rec. loss:  1.07822430e-02\n",
      "Epoch: 7053 mean train loss:  1.19445183e-02, mean val. rec. loss:  1.07814889e-02\n",
      "Epoch: 7054 mean train loss:  1.19436402e-02, mean val. rec. loss:  1.07807223e-02\n",
      "Epoch: 7055 mean train loss:  1.19427445e-02, mean val. rec. loss:  1.07799693e-02\n",
      "Epoch: 7056 mean train loss:  1.19418656e-02, mean val. rec. loss:  1.07792005e-02\n",
      "Epoch: 7057 mean train loss:  1.19409726e-02, mean val. rec. loss:  1.07784362e-02\n",
      "Epoch: 7058 mean train loss:  1.19400797e-02, mean val. rec. loss:  1.07776798e-02\n",
      "Epoch: 7059 mean train loss:  1.19391943e-02, mean val. rec. loss:  1.07769167e-02\n",
      "Epoch: 7060 mean train loss:  1.19383097e-02, mean val. rec. loss:  1.07761524e-02\n",
      "Epoch: 7061 mean train loss:  1.19374196e-02, mean val. rec. loss:  1.07753881e-02\n",
      "Epoch: 7062 mean train loss:  1.19365295e-02, mean val. rec. loss:  1.07746238e-02\n",
      "Epoch: 7063 mean train loss:  1.19356375e-02, mean val. rec. loss:  1.07738572e-02\n",
      "Epoch: 7064 mean train loss:  1.19347455e-02, mean val. rec. loss:  1.07730929e-02\n",
      "Epoch: 7065 mean train loss:  1.19338554e-02, mean val. rec. loss:  1.07723241e-02\n",
      "Epoch: 7066 mean train loss:  1.19329587e-02, mean val. rec. loss:  1.07715507e-02\n",
      "Epoch: 7067 mean train loss:  1.19320621e-02, mean val. rec. loss:  1.07707932e-02\n",
      "Epoch: 7068 mean train loss:  1.19311710e-02, mean val. rec. loss:  1.07700210e-02\n",
      "Epoch: 7069 mean train loss:  1.19302837e-02, mean val. rec. loss:  1.07692533e-02\n",
      "Epoch: 7070 mean train loss:  1.19293842e-02, mean val. rec. loss:  1.07684890e-02\n",
      "Epoch: 7071 mean train loss:  1.19284885e-02, mean val. rec. loss:  1.07677122e-02\n",
      "Epoch: 7072 mean train loss:  1.19275947e-02, mean val. rec. loss:  1.07669377e-02\n",
      "Epoch: 7073 mean train loss:  1.19266990e-02, mean val. rec. loss:  1.07661745e-02\n",
      "Epoch: 7074 mean train loss:  1.19258032e-02, mean val. rec. loss:  1.07654113e-02\n",
      "Epoch: 7075 mean train loss:  1.19249057e-02, mean val. rec. loss:  1.07646312e-02\n",
      "Epoch: 7076 mean train loss:  1.19240044e-02, mean val. rec. loss:  1.07638646e-02\n",
      "Epoch: 7077 mean train loss:  1.19231114e-02, mean val. rec. loss:  1.07630935e-02\n",
      "Epoch: 7078 mean train loss:  1.19222167e-02, mean val. rec. loss:  1.07623179e-02\n",
      "Epoch: 7079 mean train loss:  1.19213144e-02, mean val. rec. loss:  1.07615479e-02\n",
      "Epoch: 7080 mean train loss:  1.19204113e-02, mean val. rec. loss:  1.07607711e-02\n",
      "Epoch: 7081 mean train loss:  1.19195118e-02, mean val. rec. loss:  1.07599977e-02\n",
      "Epoch: 7082 mean train loss:  1.19186115e-02, mean val. rec. loss:  1.07592221e-02\n",
      "Epoch: 7083 mean train loss:  1.19177102e-02, mean val. rec. loss:  1.07584476e-02\n",
      "Epoch: 7084 mean train loss:  1.19168107e-02, mean val. rec. loss:  1.07576731e-02\n",
      "Epoch: 7085 mean train loss:  1.19159085e-02, mean val. rec. loss:  1.07568940e-02\n",
      "Epoch: 7086 mean train loss:  1.19150109e-02, mean val. rec. loss:  1.07561241e-02\n",
      "Epoch: 7087 mean train loss:  1.19141133e-02, mean val. rec. loss:  1.07553484e-02\n",
      "Epoch: 7088 mean train loss:  1.19132065e-02, mean val. rec. loss:  1.07545683e-02\n",
      "Epoch: 7089 mean train loss:  1.19122986e-02, mean val. rec. loss:  1.07537926e-02\n",
      "Epoch: 7090 mean train loss:  1.19114020e-02, mean val. rec. loss:  1.07530102e-02\n",
      "Epoch: 7091 mean train loss:  1.19104904e-02, mean val. rec. loss:  1.07522379e-02\n",
      "Epoch: 7092 mean train loss:  1.19095938e-02, mean val. rec. loss:  1.07514544e-02\n",
      "Epoch: 7093 mean train loss:  1.19086804e-02, mean val. rec. loss:  1.07506810e-02\n",
      "Epoch: 7094 mean train loss:  1.19077828e-02, mean val. rec. loss:  1.07498951e-02\n",
      "Epoch: 7095 mean train loss:  1.19068731e-02, mean val. rec. loss:  1.07491138e-02\n",
      "Epoch: 7096 mean train loss:  1.19059616e-02, mean val. rec. loss:  1.07483405e-02\n",
      "Epoch: 7097 mean train loss:  1.19050584e-02, mean val. rec. loss:  1.07475591e-02\n",
      "Epoch: 7098 mean train loss:  1.19041534e-02, mean val. rec. loss:  1.07467846e-02\n",
      "Epoch: 7099 mean train loss:  1.19032419e-02, mean val. rec. loss:  1.07459909e-02\n",
      "Epoch: 7100 mean train loss:  1.19023341e-02, mean val. rec. loss:  1.07452175e-02\n",
      "Epoch: 7101 mean train loss:  1.19014253e-02, mean val. rec. loss:  1.07444237e-02\n",
      "Epoch: 7102 mean train loss:  1.19005147e-02, mean val. rec. loss:  1.07436515e-02\n",
      "Epoch: 7103 mean train loss:  1.18996050e-02, mean val. rec. loss:  1.07428588e-02\n",
      "Epoch: 7104 mean train loss:  1.18986944e-02, mean val. rec. loss:  1.07420786e-02\n",
      "Epoch: 7105 mean train loss:  1.18977884e-02, mean val. rec. loss:  1.07412939e-02\n",
      "Epoch: 7106 mean train loss:  1.18968722e-02, mean val. rec. loss:  1.07405035e-02\n",
      "Epoch: 7107 mean train loss:  1.18959560e-02, mean val. rec. loss:  1.07397290e-02\n",
      "Epoch: 7108 mean train loss:  1.18950473e-02, mean val. rec. loss:  1.07389421e-02\n",
      "Epoch: 7109 mean train loss:  1.18941386e-02, mean val. rec. loss:  1.07381483e-02\n",
      "Epoch: 7110 mean train loss:  1.18932168e-02, mean val. rec. loss:  1.07373647e-02\n",
      "Epoch: 7111 mean train loss:  1.18923108e-02, mean val. rec. loss:  1.07365743e-02\n",
      "Epoch: 7112 mean train loss:  1.18913881e-02, mean val. rec. loss:  1.07357896e-02\n",
      "Epoch: 7113 mean train loss:  1.18904821e-02, mean val. rec. loss:  1.07349970e-02\n",
      "Epoch: 7114 mean train loss:  1.18895576e-02, mean val. rec. loss:  1.07342134e-02\n",
      "Epoch: 7115 mean train loss:  1.18886470e-02, mean val. rec. loss:  1.07334230e-02\n",
      "Epoch: 7116 mean train loss:  1.18877345e-02, mean val. rec. loss:  1.07326371e-02\n",
      "Epoch: 7117 mean train loss:  1.18868136e-02, mean val. rec. loss:  1.07318388e-02\n",
      "Epoch: 7118 mean train loss:  1.18858928e-02, mean val. rec. loss:  1.07310473e-02\n",
      "Epoch: 7119 mean train loss:  1.18849747e-02, mean val. rec. loss:  1.07302524e-02\n",
      "Epoch: 7120 mean train loss:  1.18840548e-02, mean val. rec. loss:  1.07294597e-02\n",
      "Epoch: 7121 mean train loss:  1.18831367e-02, mean val. rec. loss:  1.07286682e-02\n",
      "Epoch: 7122 mean train loss:  1.18822168e-02, mean val. rec. loss:  1.07278744e-02\n",
      "Epoch: 7123 mean train loss:  1.18812969e-02, mean val. rec. loss:  1.07270875e-02\n",
      "Epoch: 7124 mean train loss:  1.18803807e-02, mean val. rec. loss:  1.07262891e-02\n",
      "Epoch: 7125 mean train loss:  1.18794645e-02, mean val. rec. loss:  1.07254931e-02\n",
      "Epoch: 7126 mean train loss:  1.18785390e-02, mean val. rec. loss:  1.07246959e-02\n",
      "Epoch: 7127 mean train loss:  1.18776125e-02, mean val. rec. loss:  1.07239010e-02\n",
      "Epoch: 7128 mean train loss:  1.18766954e-02, mean val. rec. loss:  1.07231095e-02\n",
      "Epoch: 7129 mean train loss:  1.18757718e-02, mean val. rec. loss:  1.07223180e-02\n",
      "Epoch: 7130 mean train loss:  1.18748481e-02, mean val. rec. loss:  1.07215174e-02\n",
      "Epoch: 7131 mean train loss:  1.18739245e-02, mean val. rec. loss:  1.07207168e-02\n",
      "Epoch: 7132 mean train loss:  1.18730018e-02, mean val. rec. loss:  1.07199253e-02\n",
      "Epoch: 7133 mean train loss:  1.18720772e-02, mean val. rec. loss:  1.07191258e-02\n",
      "Epoch: 7134 mean train loss:  1.18711545e-02, mean val. rec. loss:  1.07183196e-02\n",
      "Epoch: 7135 mean train loss:  1.18702243e-02, mean val. rec. loss:  1.07175235e-02\n",
      "Epoch: 7136 mean train loss:  1.18692941e-02, mean val. rec. loss:  1.07167275e-02\n",
      "Epoch: 7137 mean train loss:  1.18683724e-02, mean val. rec. loss:  1.07159303e-02\n",
      "Epoch: 7138 mean train loss:  1.18674478e-02, mean val. rec. loss:  1.07151308e-02\n",
      "Epoch: 7139 mean train loss:  1.18665204e-02, mean val. rec. loss:  1.07143280e-02\n",
      "Epoch: 7140 mean train loss:  1.18655930e-02, mean val. rec. loss:  1.07135274e-02\n",
      "Epoch: 7141 mean train loss:  1.18646638e-02, mean val. rec. loss:  1.07127257e-02\n",
      "Epoch: 7142 mean train loss:  1.18637346e-02, mean val. rec. loss:  1.07119262e-02\n",
      "Epoch: 7143 mean train loss:  1.18628035e-02, mean val. rec. loss:  1.07111211e-02\n",
      "Epoch: 7144 mean train loss:  1.18618705e-02, mean val. rec. loss:  1.07103126e-02\n",
      "Epoch: 7145 mean train loss:  1.18609348e-02, mean val. rec. loss:  1.07095142e-02\n",
      "Epoch: 7146 mean train loss:  1.18600074e-02, mean val. rec. loss:  1.07087057e-02\n",
      "Epoch: 7147 mean train loss:  1.18590800e-02, mean val. rec. loss:  1.07078960e-02\n",
      "Epoch: 7148 mean train loss:  1.18581396e-02, mean val. rec. loss:  1.07070932e-02\n",
      "Epoch: 7149 mean train loss:  1.18572150e-02, mean val. rec. loss:  1.07062801e-02\n",
      "Epoch: 7150 mean train loss:  1.18562728e-02, mean val. rec. loss:  1.07054784e-02\n",
      "Epoch: 7151 mean train loss:  1.18553473e-02, mean val. rec. loss:  1.07046665e-02\n",
      "Epoch: 7152 mean train loss:  1.18544050e-02, mean val. rec. loss:  1.07038625e-02\n",
      "Epoch: 7153 mean train loss:  1.18534748e-02, mean val. rec. loss:  1.07030540e-02\n",
      "Epoch: 7154 mean train loss:  1.18525428e-02, mean val. rec. loss:  1.07022511e-02\n",
      "Epoch: 7155 mean train loss:  1.18516024e-02, mean val. rec. loss:  1.07014312e-02\n",
      "Epoch: 7156 mean train loss:  1.18506639e-02, mean val. rec. loss:  1.07006307e-02\n",
      "Epoch: 7157 mean train loss:  1.18497309e-02, mean val. rec. loss:  1.06998187e-02\n",
      "Epoch: 7158 mean train loss:  1.18487924e-02, mean val. rec. loss:  1.06990079e-02\n",
      "Epoch: 7159 mean train loss:  1.18478557e-02, mean val. rec. loss:  1.06981994e-02\n",
      "Epoch: 7160 mean train loss:  1.18469171e-02, mean val. rec. loss:  1.06973875e-02\n",
      "Epoch: 7161 mean train loss:  1.18459786e-02, mean val. rec. loss:  1.06965767e-02\n",
      "Epoch: 7162 mean train loss:  1.18450382e-02, mean val. rec. loss:  1.06957602e-02\n",
      "Epoch: 7163 mean train loss:  1.18440940e-02, mean val. rec. loss:  1.06949551e-02\n",
      "Epoch: 7164 mean train loss:  1.18431583e-02, mean val. rec. loss:  1.06941375e-02\n",
      "Epoch: 7165 mean train loss:  1.18422225e-02, mean val. rec. loss:  1.06933244e-02\n",
      "Epoch: 7166 mean train loss:  1.18412765e-02, mean val. rec. loss:  1.06925046e-02\n",
      "Epoch: 7167 mean train loss:  1.18403296e-02, mean val. rec. loss:  1.06916961e-02\n",
      "Epoch: 7168 mean train loss:  1.18393967e-02, mean val. rec. loss:  1.06908739e-02\n",
      "Epoch: 7169 mean train loss:  1.18384451e-02, mean val. rec. loss:  1.06900631e-02\n",
      "Epoch: 7170 mean train loss:  1.18375103e-02, mean val. rec. loss:  1.06892444e-02\n",
      "Epoch: 7171 mean train loss:  1.18365578e-02, mean val. rec. loss:  1.06884302e-02\n",
      "Epoch: 7172 mean train loss:  1.18356174e-02, mean val. rec. loss:  1.06876103e-02\n",
      "Epoch: 7173 mean train loss:  1.18346760e-02, mean val. rec. loss:  1.06867859e-02\n",
      "Epoch: 7174 mean train loss:  1.18337263e-02, mean val. rec. loss:  1.06859661e-02\n",
      "Epoch: 7175 mean train loss:  1.18327756e-02, mean val. rec. loss:  1.06851530e-02\n",
      "Epoch: 7176 mean train loss:  1.18318343e-02, mean val. rec. loss:  1.06843320e-02\n",
      "Epoch: 7177 mean train loss:  1.18308874e-02, mean val. rec. loss:  1.06835099e-02\n",
      "Epoch: 7178 mean train loss:  1.18299386e-02, mean val. rec. loss:  1.06826889e-02\n",
      "Epoch: 7179 mean train loss:  1.18289917e-02, mean val. rec. loss:  1.06818702e-02\n",
      "Epoch: 7180 mean train loss:  1.18280429e-02, mean val. rec. loss:  1.06810503e-02\n",
      "Epoch: 7181 mean train loss:  1.18270941e-02, mean val. rec. loss:  1.06802236e-02\n",
      "Epoch: 7182 mean train loss:  1.18261397e-02, mean val. rec. loss:  1.06794117e-02\n",
      "Epoch: 7183 mean train loss:  1.18251937e-02, mean val. rec. loss:  1.06785850e-02\n",
      "Epoch: 7184 mean train loss:  1.18242468e-02, mean val. rec. loss:  1.06777640e-02\n",
      "Epoch: 7185 mean train loss:  1.18232915e-02, mean val. rec. loss:  1.06769351e-02\n",
      "Epoch: 7186 mean train loss:  1.18223362e-02, mean val. rec. loss:  1.06761152e-02\n",
      "Epoch: 7187 mean train loss:  1.18213930e-02, mean val. rec. loss:  1.06752840e-02\n",
      "Epoch: 7188 mean train loss:  1.18204312e-02, mean val. rec. loss:  1.06744641e-02\n",
      "Epoch: 7189 mean train loss:  1.18194871e-02, mean val. rec. loss:  1.06736329e-02\n",
      "Epoch: 7190 mean train loss:  1.18185243e-02, mean val. rec. loss:  1.06728131e-02\n",
      "Epoch: 7191 mean train loss:  1.18175783e-02, mean val. rec. loss:  1.06719785e-02\n",
      "Epoch: 7192 mean train loss:  1.18166193e-02, mean val. rec. loss:  1.06711484e-02\n",
      "Epoch: 7193 mean train loss:  1.18156603e-02, mean val. rec. loss:  1.06703229e-02\n",
      "Epoch: 7194 mean train loss:  1.18147087e-02, mean val. rec. loss:  1.06694962e-02\n",
      "Epoch: 7195 mean train loss:  1.18137562e-02, mean val. rec. loss:  1.06686627e-02\n",
      "Epoch: 7196 mean train loss:  1.18127981e-02, mean val. rec. loss:  1.06678327e-02\n",
      "Epoch: 7197 mean train loss:  1.18118428e-02, mean val. rec. loss:  1.06670026e-02\n",
      "Epoch: 7198 mean train loss:  1.18108837e-02, mean val. rec. loss:  1.06661725e-02\n",
      "Epoch: 7199 mean train loss:  1.18099247e-02, mean val. rec. loss:  1.06653447e-02\n",
      "Epoch: 7200 mean train loss:  1.18089666e-02, mean val. rec. loss:  1.06645101e-02\n",
      "Epoch: 7201 mean train loss:  1.18080029e-02, mean val. rec. loss:  1.06636732e-02\n",
      "Epoch: 7202 mean train loss:  1.18070383e-02, mean val. rec. loss:  1.06628409e-02\n",
      "Epoch: 7203 mean train loss:  1.18060821e-02, mean val. rec. loss:  1.06620131e-02\n",
      "Epoch: 7204 mean train loss:  1.18051258e-02, mean val. rec. loss:  1.06611807e-02\n",
      "Epoch: 7205 mean train loss:  1.18041594e-02, mean val. rec. loss:  1.06603518e-02\n",
      "Epoch: 7206 mean train loss:  1.18031985e-02, mean val. rec. loss:  1.06595115e-02\n",
      "Epoch: 7207 mean train loss:  1.18022357e-02, mean val. rec. loss:  1.06586701e-02\n",
      "Epoch: 7208 mean train loss:  1.18012711e-02, mean val. rec. loss:  1.06578412e-02\n",
      "Epoch: 7209 mean train loss:  1.18003084e-02, mean val. rec. loss:  1.06570100e-02\n",
      "Epoch: 7210 mean train loss:  1.17993437e-02, mean val. rec. loss:  1.06561618e-02\n",
      "Epoch: 7211 mean train loss:  1.17983754e-02, mean val. rec. loss:  1.06553294e-02\n",
      "Epoch: 7212 mean train loss:  1.17974145e-02, mean val. rec. loss:  1.06544926e-02\n",
      "Epoch: 7213 mean train loss:  1.17964518e-02, mean val. rec. loss:  1.06536466e-02\n",
      "Epoch: 7214 mean train loss:  1.17954816e-02, mean val. rec. loss:  1.06528165e-02\n",
      "Epoch: 7215 mean train loss:  1.17945151e-02, mean val. rec. loss:  1.06519615e-02\n",
      "Epoch: 7216 mean train loss:  1.17935486e-02, mean val. rec. loss:  1.06511337e-02\n",
      "Epoch: 7217 mean train loss:  1.17925793e-02, mean val. rec. loss:  1.06502798e-02\n",
      "Epoch: 7218 mean train loss:  1.17916119e-02, mean val. rec. loss:  1.06494532e-02\n",
      "Epoch: 7219 mean train loss:  1.17906427e-02, mean val. rec. loss:  1.06486038e-02\n",
      "Epoch: 7220 mean train loss:  1.17896780e-02, mean val. rec. loss:  1.06477624e-02\n",
      "Epoch: 7221 mean train loss:  1.17887032e-02, mean val. rec. loss:  1.06469131e-02\n",
      "Epoch: 7222 mean train loss:  1.17877274e-02, mean val. rec. loss:  1.06460705e-02\n",
      "Epoch: 7223 mean train loss:  1.17867609e-02, mean val. rec. loss:  1.06452348e-02\n",
      "Epoch: 7224 mean train loss:  1.17857944e-02, mean val. rec. loss:  1.06443843e-02\n",
      "Epoch: 7225 mean train loss:  1.17848131e-02, mean val. rec. loss:  1.06435440e-02\n",
      "Epoch: 7226 mean train loss:  1.17838494e-02, mean val. rec. loss:  1.06426958e-02\n",
      "Epoch: 7227 mean train loss:  1.17828671e-02, mean val. rec. loss:  1.06418555e-02\n",
      "Epoch: 7228 mean train loss:  1.17819025e-02, mean val. rec. loss:  1.06410050e-02\n",
      "Epoch: 7229 mean train loss:  1.17809192e-02, mean val. rec. loss:  1.06401614e-02\n",
      "Epoch: 7230 mean train loss:  1.17799490e-02, mean val. rec. loss:  1.06393154e-02\n",
      "Epoch: 7231 mean train loss:  1.17789770e-02, mean val. rec. loss:  1.06384604e-02\n",
      "Epoch: 7232 mean train loss:  1.17779993e-02, mean val. rec. loss:  1.06376144e-02\n",
      "Epoch: 7233 mean train loss:  1.17770170e-02, mean val. rec. loss:  1.06367617e-02\n",
      "Epoch: 7234 mean train loss:  1.17760394e-02, mean val. rec. loss:  1.06359112e-02\n",
      "Epoch: 7235 mean train loss:  1.17750626e-02, mean val. rec. loss:  1.06350596e-02\n",
      "Epoch: 7236 mean train loss:  1.17740850e-02, mean val. rec. loss:  1.06342057e-02\n",
      "Epoch: 7237 mean train loss:  1.17731055e-02, mean val. rec. loss:  1.06333552e-02\n",
      "Epoch: 7238 mean train loss:  1.17721260e-02, mean val. rec. loss:  1.06325002e-02\n",
      "Epoch: 7239 mean train loss:  1.17711511e-02, mean val. rec. loss:  1.06316543e-02\n",
      "Epoch: 7240 mean train loss:  1.17701744e-02, mean val. rec. loss:  1.06308015e-02\n",
      "Epoch: 7241 mean train loss:  1.17691912e-02, mean val. rec. loss:  1.06299442e-02\n",
      "Epoch: 7242 mean train loss:  1.17682042e-02, mean val. rec. loss:  1.06291040e-02\n",
      "Epoch: 7243 mean train loss:  1.17672265e-02, mean val. rec. loss:  1.06282410e-02\n",
      "Epoch: 7244 mean train loss:  1.17662442e-02, mean val. rec. loss:  1.06273803e-02\n",
      "Epoch: 7245 mean train loss:  1.17652610e-02, mean val. rec. loss:  1.06265332e-02\n",
      "Epoch: 7246 mean train loss:  1.17642778e-02, mean val. rec. loss:  1.06256850e-02\n",
      "Epoch: 7247 mean train loss:  1.17632936e-02, mean val. rec. loss:  1.06248141e-02\n",
      "Epoch: 7248 mean train loss:  1.17623057e-02, mean val. rec. loss:  1.06239614e-02\n",
      "Epoch: 7249 mean train loss:  1.17613243e-02, mean val. rec. loss:  1.06231041e-02\n",
      "Epoch: 7250 mean train loss:  1.17603439e-02, mean val. rec. loss:  1.06222400e-02\n",
      "Epoch: 7251 mean train loss:  1.17593532e-02, mean val. rec. loss:  1.06213827e-02\n",
      "Epoch: 7252 mean train loss:  1.17583616e-02, mean val. rec. loss:  1.06205209e-02\n",
      "Epoch: 7253 mean train loss:  1.17573756e-02, mean val. rec. loss:  1.06196591e-02\n",
      "Epoch: 7254 mean train loss:  1.17563867e-02, mean val. rec. loss:  1.06187995e-02\n",
      "Epoch: 7255 mean train loss:  1.17553988e-02, mean val. rec. loss:  1.06179377e-02\n",
      "Epoch: 7256 mean train loss:  1.17544091e-02, mean val. rec. loss:  1.06170770e-02\n",
      "Epoch: 7257 mean train loss:  1.17534203e-02, mean val. rec. loss:  1.06162242e-02\n",
      "Epoch: 7258 mean train loss:  1.17524352e-02, mean val. rec. loss:  1.06153568e-02\n",
      "Epoch: 7259 mean train loss:  1.17514482e-02, mean val. rec. loss:  1.06144927e-02\n",
      "Epoch: 7260 mean train loss:  1.17504519e-02, mean val. rec. loss:  1.06136240e-02\n",
      "Epoch: 7261 mean train loss:  1.17494557e-02, mean val. rec. loss:  1.06127622e-02\n",
      "Epoch: 7262 mean train loss:  1.17484696e-02, mean val. rec. loss:  1.06119015e-02\n",
      "Epoch: 7263 mean train loss:  1.17474762e-02, mean val. rec. loss:  1.06110408e-02\n",
      "Epoch: 7264 mean train loss:  1.17464827e-02, mean val. rec. loss:  1.06101699e-02\n",
      "Epoch: 7265 mean train loss:  1.17454892e-02, mean val. rec. loss:  1.06092991e-02\n",
      "Epoch: 7266 mean train loss:  1.17444948e-02, mean val. rec. loss:  1.06084304e-02\n",
      "Epoch: 7267 mean train loss:  1.17434957e-02, mean val. rec. loss:  1.06075663e-02\n",
      "Epoch: 7268 mean train loss:  1.17425041e-02, mean val. rec. loss:  1.06067000e-02\n",
      "Epoch: 7269 mean train loss:  1.17415125e-02, mean val. rec. loss:  1.06058359e-02\n",
      "Epoch: 7270 mean train loss:  1.17405116e-02, mean val. rec. loss:  1.06049605e-02\n",
      "Epoch: 7271 mean train loss:  1.17395116e-02, mean val. rec. loss:  1.06040873e-02\n",
      "Epoch: 7272 mean train loss:  1.17385116e-02, mean val. rec. loss:  1.06032141e-02\n",
      "Epoch: 7273 mean train loss:  1.17375144e-02, mean val. rec. loss:  1.06023444e-02\n",
      "Epoch: 7274 mean train loss:  1.17365153e-02, mean val. rec. loss:  1.06014746e-02\n",
      "Epoch: 7275 mean train loss:  1.17355162e-02, mean val. rec. loss:  1.06006083e-02\n",
      "Epoch: 7276 mean train loss:  1.17345200e-02, mean val. rec. loss:  1.05997294e-02\n",
      "Epoch: 7277 mean train loss:  1.17335246e-02, mean val. rec. loss:  1.05988551e-02\n",
      "Epoch: 7278 mean train loss:  1.17325190e-02, mean val. rec. loss:  1.05979774e-02\n",
      "Epoch: 7279 mean train loss:  1.17315135e-02, mean val. rec. loss:  1.05971020e-02\n",
      "Epoch: 7280 mean train loss:  1.17305153e-02, mean val. rec. loss:  1.05962334e-02\n",
      "Epoch: 7281 mean train loss:  1.17295125e-02, mean val. rec. loss:  1.05953602e-02\n",
      "Epoch: 7282 mean train loss:  1.17285097e-02, mean val. rec. loss:  1.05944802e-02\n",
      "Epoch: 7283 mean train loss:  1.17275060e-02, mean val. rec. loss:  1.05935980e-02\n",
      "Epoch: 7284 mean train loss:  1.17265014e-02, mean val. rec. loss:  1.05927271e-02\n",
      "Epoch: 7285 mean train loss:  1.17254967e-02, mean val. rec. loss:  1.05918517e-02\n",
      "Epoch: 7286 mean train loss:  1.17244949e-02, mean val. rec. loss:  1.05909672e-02\n",
      "Epoch: 7287 mean train loss:  1.17234856e-02, mean val. rec. loss:  1.05900895e-02\n",
      "Epoch: 7288 mean train loss:  1.17224744e-02, mean val. rec. loss:  1.05892175e-02\n",
      "Epoch: 7289 mean train loss:  1.17214716e-02, mean val. rec. loss:  1.05883386e-02\n",
      "Epoch: 7290 mean train loss:  1.17204688e-02, mean val. rec. loss:  1.05874575e-02\n",
      "Epoch: 7291 mean train loss:  1.17194595e-02, mean val. rec. loss:  1.05865776e-02\n",
      "Epoch: 7292 mean train loss:  1.17184520e-02, mean val. rec. loss:  1.05856987e-02\n",
      "Epoch: 7293 mean train loss:  1.17174409e-02, mean val. rec. loss:  1.05848176e-02\n",
      "Epoch: 7294 mean train loss:  1.17164316e-02, mean val. rec. loss:  1.05839320e-02\n",
      "Epoch: 7295 mean train loss:  1.17154167e-02, mean val. rec. loss:  1.05830418e-02\n",
      "Epoch: 7296 mean train loss:  1.17144018e-02, mean val. rec. loss:  1.05821539e-02\n",
      "Epoch: 7297 mean train loss:  1.17133943e-02, mean val. rec. loss:  1.05812762e-02\n",
      "Epoch: 7298 mean train loss:  1.17123869e-02, mean val. rec. loss:  1.05803929e-02\n",
      "Epoch: 7299 mean train loss:  1.17113701e-02, mean val. rec. loss:  1.05794993e-02\n",
      "Epoch: 7300 mean train loss:  1.17103562e-02, mean val. rec. loss:  1.05786193e-02\n",
      "Epoch: 7301 mean train loss:  1.17093422e-02, mean val. rec. loss:  1.05777382e-02\n",
      "Epoch: 7302 mean train loss:  1.17083273e-02, mean val. rec. loss:  1.05768469e-02\n",
      "Epoch: 7303 mean train loss:  1.17073143e-02, mean val. rec. loss:  1.05759579e-02\n",
      "Epoch: 7304 mean train loss:  1.17063013e-02, mean val. rec. loss:  1.05750632e-02\n",
      "Epoch: 7305 mean train loss:  1.17052808e-02, mean val. rec. loss:  1.05741753e-02\n",
      "Epoch: 7306 mean train loss:  1.17042603e-02, mean val. rec. loss:  1.05732908e-02\n",
      "Epoch: 7307 mean train loss:  1.17032473e-02, mean val. rec. loss:  1.05724040e-02\n",
      "Epoch: 7308 mean train loss:  1.17022342e-02, mean val. rec. loss:  1.05715150e-02\n",
      "Epoch: 7309 mean train loss:  1.17012156e-02, mean val. rec. loss:  1.05706214e-02\n",
      "Epoch: 7310 mean train loss:  1.17001961e-02, mean val. rec. loss:  1.05697301e-02\n",
      "Epoch: 7311 mean train loss:  1.16991774e-02, mean val. rec. loss:  1.05688388e-02\n",
      "Epoch: 7312 mean train loss:  1.16981560e-02, mean val. rec. loss:  1.05679475e-02\n",
      "Epoch: 7313 mean train loss:  1.16971365e-02, mean val. rec. loss:  1.05670539e-02\n",
      "Epoch: 7314 mean train loss:  1.16961113e-02, mean val. rec. loss:  1.05661535e-02\n",
      "Epoch: 7315 mean train loss:  1.16950853e-02, mean val. rec. loss:  1.05652577e-02\n",
      "Epoch: 7316 mean train loss:  1.16940667e-02, mean val. rec. loss:  1.05643686e-02\n",
      "Epoch: 7317 mean train loss:  1.16930480e-02, mean val. rec. loss:  1.05634660e-02\n",
      "Epoch: 7318 mean train loss:  1.16920164e-02, mean val. rec. loss:  1.05625747e-02\n",
      "Epoch: 7319 mean train loss:  1.16910015e-02, mean val. rec. loss:  1.05616709e-02\n",
      "Epoch: 7320 mean train loss:  1.16899661e-02, mean val. rec. loss:  1.05607796e-02\n",
      "Epoch: 7321 mean train loss:  1.16889503e-02, mean val. rec. loss:  1.05598781e-02\n",
      "Epoch: 7322 mean train loss:  1.16879158e-02, mean val. rec. loss:  1.05589811e-02\n",
      "Epoch: 7323 mean train loss:  1.16868944e-02, mean val. rec. loss:  1.05580841e-02\n",
      "Epoch: 7324 mean train loss:  1.16858712e-02, mean val. rec. loss:  1.05571906e-02\n",
      "Epoch: 7325 mean train loss:  1.16848404e-02, mean val. rec. loss:  1.05562788e-02\n",
      "Epoch: 7326 mean train loss:  1.16838079e-02, mean val. rec. loss:  1.05553751e-02\n",
      "Epoch: 7327 mean train loss:  1.16827781e-02, mean val. rec. loss:  1.05544747e-02\n",
      "Epoch: 7328 mean train loss:  1.16817501e-02, mean val. rec. loss:  1.05535709e-02\n",
      "Epoch: 7329 mean train loss:  1.16807194e-02, mean val. rec. loss:  1.05526683e-02\n",
      "Epoch: 7330 mean train loss:  1.16796887e-02, mean val. rec. loss:  1.05517679e-02\n",
      "Epoch: 7331 mean train loss:  1.16786570e-02, mean val. rec. loss:  1.05508596e-02\n",
      "Epoch: 7332 mean train loss:  1.16776310e-02, mean val. rec. loss:  1.05499615e-02\n",
      "Epoch: 7333 mean train loss:  1.16766030e-02, mean val. rec. loss:  1.05490554e-02\n",
      "Epoch: 7334 mean train loss:  1.16755649e-02, mean val. rec. loss:  1.05481460e-02\n",
      "Epoch: 7335 mean train loss:  1.16745276e-02, mean val. rec. loss:  1.05472479e-02\n",
      "Epoch: 7336 mean train loss:  1.16735025e-02, mean val. rec. loss:  1.05463327e-02\n",
      "Epoch: 7337 mean train loss:  1.16724597e-02, mean val. rec. loss:  1.05454312e-02\n",
      "Epoch: 7338 mean train loss:  1.16714317e-02, mean val. rec. loss:  1.05445184e-02\n",
      "Epoch: 7339 mean train loss:  1.16703880e-02, mean val. rec. loss:  1.05436169e-02\n",
      "Epoch: 7340 mean train loss:  1.16693610e-02, mean val. rec. loss:  1.05427097e-02\n",
      "Epoch: 7341 mean train loss:  1.16683200e-02, mean val. rec. loss:  1.05417878e-02\n",
      "Epoch: 7342 mean train loss:  1.16672772e-02, mean val. rec. loss:  1.05408817e-02\n",
      "Epoch: 7343 mean train loss:  1.16662437e-02, mean val. rec. loss:  1.05399745e-02\n",
      "Epoch: 7344 mean train loss:  1.16652102e-02, mean val. rec. loss:  1.05390560e-02\n",
      "Epoch: 7345 mean train loss:  1.16641673e-02, mean val. rec. loss:  1.05381420e-02\n",
      "Epoch: 7346 mean train loss:  1.16631273e-02, mean val. rec. loss:  1.05372326e-02\n",
      "Epoch: 7347 mean train loss:  1.16620873e-02, mean val. rec. loss:  1.05363186e-02\n",
      "Epoch: 7348 mean train loss:  1.16610454e-02, mean val. rec. loss:  1.05354057e-02\n",
      "Epoch: 7349 mean train loss:  1.16600053e-02, mean val. rec. loss:  1.05344963e-02\n",
      "Epoch: 7350 mean train loss:  1.16589662e-02, mean val. rec. loss:  1.05335812e-02\n",
      "Epoch: 7351 mean train loss:  1.16579197e-02, mean val. rec. loss:  1.05326592e-02\n",
      "Epoch: 7352 mean train loss:  1.16568722e-02, mean val. rec. loss:  1.05317555e-02\n",
      "Epoch: 7353 mean train loss:  1.16558322e-02, mean val. rec. loss:  1.05308335e-02\n",
      "Epoch: 7354 mean train loss:  1.16547921e-02, mean val. rec. loss:  1.05299116e-02\n",
      "Epoch: 7355 mean train loss:  1.16537391e-02, mean val. rec. loss:  1.05289988e-02\n",
      "Epoch: 7356 mean train loss:  1.16527018e-02, mean val. rec. loss:  1.05280746e-02\n",
      "Epoch: 7357 mean train loss:  1.16516469e-02, mean val. rec. loss:  1.05271617e-02\n",
      "Epoch: 7358 mean train loss:  1.16506097e-02, mean val. rec. loss:  1.05262330e-02\n",
      "Epoch: 7359 mean train loss:  1.16495575e-02, mean val. rec. loss:  1.05253145e-02\n",
      "Epoch: 7360 mean train loss:  1.16485063e-02, mean val. rec. loss:  1.05243960e-02\n",
      "Epoch: 7361 mean train loss:  1.16474616e-02, mean val. rec. loss:  1.05234752e-02\n",
      "Epoch: 7362 mean train loss:  1.16464151e-02, mean val. rec. loss:  1.05225578e-02\n",
      "Epoch: 7363 mean train loss:  1.16453611e-02, mean val. rec. loss:  1.05216245e-02\n",
      "Epoch: 7364 mean train loss:  1.16443127e-02, mean val. rec. loss:  1.05207105e-02\n",
      "Epoch: 7365 mean train loss:  1.16432596e-02, mean val. rec. loss:  1.05197750e-02\n",
      "Epoch: 7366 mean train loss:  1.16422084e-02, mean val. rec. loss:  1.05188633e-02\n",
      "Epoch: 7367 mean train loss:  1.16411572e-02, mean val. rec. loss:  1.05179312e-02\n",
      "Epoch: 7368 mean train loss:  1.16401078e-02, mean val. rec. loss:  1.05170047e-02\n",
      "Epoch: 7369 mean train loss:  1.16390501e-02, mean val. rec. loss:  1.05160748e-02\n",
      "Epoch: 7370 mean train loss:  1.16379924e-02, mean val. rec. loss:  1.05151484e-02\n",
      "Epoch: 7371 mean train loss:  1.16369421e-02, mean val. rec. loss:  1.05142310e-02\n",
      "Epoch: 7372 mean train loss:  1.16358918e-02, mean val. rec. loss:  1.05132966e-02\n",
      "Epoch: 7373 mean train loss:  1.16348267e-02, mean val. rec. loss:  1.05123713e-02\n",
      "Epoch: 7374 mean train loss:  1.16337801e-02, mean val. rec. loss:  1.05114403e-02\n",
      "Epoch: 7375 mean train loss:  1.16327149e-02, mean val. rec. loss:  1.05105138e-02\n",
      "Epoch: 7376 mean train loss:  1.16316647e-02, mean val. rec. loss:  1.05095840e-02\n",
      "Epoch: 7377 mean train loss:  1.16306032e-02, mean val. rec. loss:  1.05086428e-02\n",
      "Epoch: 7378 mean train loss:  1.16295399e-02, mean val. rec. loss:  1.05077152e-02\n",
      "Epoch: 7379 mean train loss:  1.16284850e-02, mean val. rec. loss:  1.05067853e-02\n",
      "Epoch: 7380 mean train loss:  1.16274300e-02, mean val. rec. loss:  1.05058498e-02\n",
      "Epoch: 7381 mean train loss:  1.16263686e-02, mean val. rec. loss:  1.05049142e-02\n",
      "Epoch: 7382 mean train loss:  1.16253071e-02, mean val. rec. loss:  1.05039776e-02\n",
      "Epoch: 7383 mean train loss:  1.16242466e-02, mean val. rec. loss:  1.05030432e-02\n",
      "Epoch: 7384 mean train loss:  1.16231833e-02, mean val. rec. loss:  1.05021099e-02\n",
      "Epoch: 7385 mean train loss:  1.16221209e-02, mean val. rec. loss:  1.05011721e-02\n",
      "Epoch: 7386 mean train loss:  1.16210539e-02, mean val. rec. loss:  1.05002320e-02\n",
      "Epoch: 7387 mean train loss:  1.16199850e-02, mean val. rec. loss:  1.04993045e-02\n",
      "Epoch: 7388 mean train loss:  1.16189236e-02, mean val. rec. loss:  1.04983610e-02\n",
      "Epoch: 7389 mean train loss:  1.16178630e-02, mean val. rec. loss:  1.04974232e-02\n",
      "Epoch: 7390 mean train loss:  1.16167932e-02, mean val. rec. loss:  1.04964775e-02\n",
      "Epoch: 7391 mean train loss:  1.16157271e-02, mean val. rec. loss:  1.04955453e-02\n",
      "Epoch: 7392 mean train loss:  1.16146591e-02, mean val. rec. loss:  1.04946075e-02\n",
      "Epoch: 7393 mean train loss:  1.16135921e-02, mean val. rec. loss:  1.04936607e-02\n",
      "Epoch: 7394 mean train loss:  1.16125241e-02, mean val. rec. loss:  1.04927206e-02\n",
      "Epoch: 7395 mean train loss:  1.16114580e-02, mean val. rec. loss:  1.04917828e-02\n",
      "Epoch: 7396 mean train loss:  1.16103863e-02, mean val. rec. loss:  1.04908302e-02\n",
      "Epoch: 7397 mean train loss:  1.16093109e-02, mean val. rec. loss:  1.04898936e-02\n",
      "Epoch: 7398 mean train loss:  1.16082439e-02, mean val. rec. loss:  1.04889456e-02\n",
      "Epoch: 7399 mean train loss:  1.16071731e-02, mean val. rec. loss:  1.04880010e-02\n",
      "Epoch: 7400 mean train loss:  1.16061005e-02, mean val. rec. loss:  1.04870564e-02\n",
      "Epoch: 7401 mean train loss:  1.16050288e-02, mean val. rec. loss:  1.04861106e-02\n",
      "Epoch: 7402 mean train loss:  1.16039553e-02, mean val. rec. loss:  1.04851660e-02\n",
      "Epoch: 7403 mean train loss:  1.16028817e-02, mean val. rec. loss:  1.04842135e-02\n",
      "Epoch: 7404 mean train loss:  1.16018035e-02, mean val. rec. loss:  1.04832621e-02\n",
      "Epoch: 7405 mean train loss:  1.16007327e-02, mean val. rec. loss:  1.04823198e-02\n",
      "Epoch: 7406 mean train loss:  1.15996611e-02, mean val. rec. loss:  1.04813706e-02\n",
      "Epoch: 7407 mean train loss:  1.15985810e-02, mean val. rec. loss:  1.04804158e-02\n",
      "Epoch: 7408 mean train loss:  1.15975046e-02, mean val. rec. loss:  1.04794678e-02\n",
      "Epoch: 7409 mean train loss:  1.15964264e-02, mean val. rec. loss:  1.04785209e-02\n",
      "Epoch: 7410 mean train loss:  1.15953482e-02, mean val. rec. loss:  1.04775661e-02\n",
      "Epoch: 7411 mean train loss:  1.15942691e-02, mean val. rec. loss:  1.04766113e-02\n",
      "Epoch: 7412 mean train loss:  1.15931899e-02, mean val. rec. loss:  1.04756576e-02\n",
      "Epoch: 7413 mean train loss:  1.15921061e-02, mean val. rec. loss:  1.04747096e-02\n",
      "Epoch: 7414 mean train loss:  1.15910298e-02, mean val. rec. loss:  1.04737548e-02\n",
      "Epoch: 7415 mean train loss:  1.15899516e-02, mean val. rec. loss:  1.04728068e-02\n",
      "Epoch: 7416 mean train loss:  1.15888669e-02, mean val. rec. loss:  1.04718509e-02\n",
      "Epoch: 7417 mean train loss:  1.15877840e-02, mean val. rec. loss:  1.04708938e-02\n",
      "Epoch: 7418 mean train loss:  1.15867011e-02, mean val. rec. loss:  1.04699379e-02\n",
      "Epoch: 7419 mean train loss:  1.15856183e-02, mean val. rec. loss:  1.04689797e-02\n",
      "Epoch: 7420 mean train loss:  1.15845326e-02, mean val. rec. loss:  1.04680169e-02\n",
      "Epoch: 7421 mean train loss:  1.15834525e-02, mean val. rec. loss:  1.04670564e-02\n",
      "Epoch: 7422 mean train loss:  1.15823632e-02, mean val. rec. loss:  1.04660925e-02\n",
      "Epoch: 7423 mean train loss:  1.15812719e-02, mean val. rec. loss:  1.04651343e-02\n",
      "Epoch: 7424 mean train loss:  1.15801900e-02, mean val. rec. loss:  1.04641818e-02\n",
      "Epoch: 7425 mean train loss:  1.15791062e-02, mean val. rec. loss:  1.04632145e-02\n",
      "Epoch: 7426 mean train loss:  1.15780094e-02, mean val. rec. loss:  1.04622552e-02\n",
      "Epoch: 7427 mean train loss:  1.15769293e-02, mean val. rec. loss:  1.04612902e-02\n",
      "Epoch: 7428 mean train loss:  1.15758315e-02, mean val. rec. loss:  1.04603342e-02\n",
      "Epoch: 7429 mean train loss:  1.15747515e-02, mean val. rec. loss:  1.04593760e-02\n",
      "Epoch: 7430 mean train loss:  1.15736565e-02, mean val. rec. loss:  1.04583985e-02\n",
      "Epoch: 7431 mean train loss:  1.15725625e-02, mean val. rec. loss:  1.04574403e-02\n",
      "Epoch: 7432 mean train loss:  1.15714731e-02, mean val. rec. loss:  1.04564753e-02\n",
      "Epoch: 7433 mean train loss:  1.15703856e-02, mean val. rec. loss:  1.04555035e-02\n",
      "Epoch: 7434 mean train loss:  1.15692878e-02, mean val. rec. loss:  1.04545475e-02\n",
      "Epoch: 7435 mean train loss:  1.15681947e-02, mean val. rec. loss:  1.04535689e-02\n",
      "Epoch: 7436 mean train loss:  1.15671016e-02, mean val. rec. loss:  1.04526107e-02\n",
      "Epoch: 7437 mean train loss:  1.15660057e-02, mean val. rec. loss:  1.04516332e-02\n",
      "Epoch: 7438 mean train loss:  1.15649107e-02, mean val. rec. loss:  1.04506671e-02\n",
      "Epoch: 7439 mean train loss:  1.15638185e-02, mean val. rec. loss:  1.04496964e-02\n",
      "Epoch: 7440 mean train loss:  1.15627171e-02, mean val. rec. loss:  1.04487212e-02\n",
      "Epoch: 7441 mean train loss:  1.15616146e-02, mean val. rec. loss:  1.04477607e-02\n",
      "Epoch: 7442 mean train loss:  1.15605215e-02, mean val. rec. loss:  1.04467821e-02\n",
      "Epoch: 7443 mean train loss:  1.15594228e-02, mean val. rec. loss:  1.04458023e-02\n",
      "Epoch: 7444 mean train loss:  1.15583241e-02, mean val. rec. loss:  1.04448339e-02\n",
      "Epoch: 7445 mean train loss:  1.15572245e-02, mean val. rec. loss:  1.04438666e-02\n",
      "Epoch: 7446 mean train loss:  1.15561230e-02, mean val. rec. loss:  1.04428880e-02\n",
      "Epoch: 7447 mean train loss:  1.15550225e-02, mean val. rec. loss:  1.04419139e-02\n",
      "Epoch: 7448 mean train loss:  1.15539247e-02, mean val. rec. loss:  1.04409307e-02\n",
      "Epoch: 7449 mean train loss:  1.15528177e-02, mean val. rec. loss:  1.04399567e-02\n",
      "Epoch: 7450 mean train loss:  1.15517115e-02, mean val. rec. loss:  1.04389848e-02\n",
      "Epoch: 7451 mean train loss:  1.15506110e-02, mean val. rec. loss:  1.04380062e-02\n",
      "Epoch: 7452 mean train loss:  1.15495067e-02, mean val. rec. loss:  1.04370276e-02\n",
      "Epoch: 7453 mean train loss:  1.15484024e-02, mean val. rec. loss:  1.04360456e-02\n",
      "Epoch: 7454 mean train loss:  1.15472963e-02, mean val. rec. loss:  1.04350692e-02\n",
      "Epoch: 7455 mean train loss:  1.15461910e-02, mean val. rec. loss:  1.04340895e-02\n",
      "Epoch: 7456 mean train loss:  1.15450840e-02, mean val. rec. loss:  1.04331040e-02\n",
      "Epoch: 7457 mean train loss:  1.15439722e-02, mean val. rec. loss:  1.04321220e-02\n",
      "Epoch: 7458 mean train loss:  1.15428680e-02, mean val. rec. loss:  1.04311468e-02\n",
      "Epoch: 7459 mean train loss:  1.15417637e-02, mean val. rec. loss:  1.04301648e-02\n",
      "Epoch: 7460 mean train loss:  1.15406501e-02, mean val. rec. loss:  1.04291839e-02\n",
      "Epoch: 7461 mean train loss:  1.15395402e-02, mean val. rec. loss:  1.04281951e-02\n",
      "Epoch: 7462 mean train loss:  1.15384285e-02, mean val. rec. loss:  1.04272051e-02\n",
      "Epoch: 7463 mean train loss:  1.15373186e-02, mean val. rec. loss:  1.04262276e-02\n",
      "Epoch: 7464 mean train loss:  1.15362051e-02, mean val. rec. loss:  1.04252376e-02\n",
      "Epoch: 7465 mean train loss:  1.15350970e-02, mean val. rec. loss:  1.04242454e-02\n",
      "Epoch: 7466 mean train loss:  1.15339797e-02, mean val. rec. loss:  1.04232600e-02\n",
      "Epoch: 7467 mean train loss:  1.15328615e-02, mean val. rec. loss:  1.04222757e-02\n",
      "Epoch: 7468 mean train loss:  1.15317507e-02, mean val. rec. loss:  1.04212891e-02\n",
      "Epoch: 7469 mean train loss:  1.15306408e-02, mean val. rec. loss:  1.04202969e-02\n",
      "Epoch: 7470 mean train loss:  1.15295244e-02, mean val. rec. loss:  1.04193081e-02\n",
      "Epoch: 7471 mean train loss:  1.15284081e-02, mean val. rec. loss:  1.04183170e-02\n",
      "Epoch: 7472 mean train loss:  1.15272907e-02, mean val. rec. loss:  1.04173259e-02\n",
      "Epoch: 7473 mean train loss:  1.15261734e-02, mean val. rec. loss:  1.04163325e-02\n",
      "Epoch: 7474 mean train loss:  1.15250505e-02, mean val. rec. loss:  1.04153346e-02\n",
      "Epoch: 7475 mean train loss:  1.15239276e-02, mean val. rec. loss:  1.04143401e-02\n",
      "Epoch: 7476 mean train loss:  1.15228122e-02, mean val. rec. loss:  1.04133536e-02\n",
      "Epoch: 7477 mean train loss:  1.15216949e-02, mean val. rec. loss:  1.04123523e-02\n",
      "Epoch: 7478 mean train loss:  1.15205673e-02, mean val. rec. loss:  1.04113612e-02\n",
      "Epoch: 7479 mean train loss:  1.15194528e-02, mean val. rec. loss:  1.04103610e-02\n",
      "Epoch: 7480 mean train loss:  1.15183224e-02, mean val. rec. loss:  1.04093699e-02\n",
      "Epoch: 7481 mean train loss:  1.15172070e-02, mean val. rec. loss:  1.04083697e-02\n",
      "Epoch: 7482 mean train loss:  1.15160757e-02, mean val. rec. loss:  1.04073752e-02\n",
      "Epoch: 7483 mean train loss:  1.15149556e-02, mean val. rec. loss:  1.04063751e-02\n",
      "Epoch: 7484 mean train loss:  1.15138346e-02, mean val. rec. loss:  1.04053704e-02\n",
      "Epoch: 7485 mean train loss:  1.15127051e-02, mean val. rec. loss:  1.04043759e-02\n",
      "Epoch: 7486 mean train loss:  1.15115757e-02, mean val. rec. loss:  1.04033734e-02\n",
      "Epoch: 7487 mean train loss:  1.15104491e-02, mean val. rec. loss:  1.04023710e-02\n",
      "Epoch: 7488 mean train loss:  1.15093206e-02, mean val. rec. loss:  1.04013686e-02\n",
      "Epoch: 7489 mean train loss:  1.15081921e-02, mean val. rec. loss:  1.04003661e-02\n",
      "Epoch: 7490 mean train loss:  1.15070636e-02, mean val. rec. loss:  1.03993592e-02\n",
      "Epoch: 7491 mean train loss:  1.15059389e-02, mean val. rec. loss:  1.03983601e-02\n",
      "Epoch: 7492 mean train loss:  1.15048113e-02, mean val. rec. loss:  1.03973554e-02\n",
      "Epoch: 7493 mean train loss:  1.15036763e-02, mean val. rec. loss:  1.03963451e-02\n",
      "Epoch: 7494 mean train loss:  1.15025422e-02, mean val. rec. loss:  1.03953528e-02\n",
      "Epoch: 7495 mean train loss:  1.15014137e-02, mean val. rec. loss:  1.03943402e-02\n",
      "Epoch: 7496 mean train loss:  1.15002806e-02, mean val. rec. loss:  1.03933287e-02\n",
      "Epoch: 7497 mean train loss:  1.14991465e-02, mean val. rec. loss:  1.03923240e-02\n",
      "Epoch: 7498 mean train loss:  1.14980134e-02, mean val. rec. loss:  1.03913238e-02\n",
      "Epoch: 7499 mean train loss:  1.14968784e-02, mean val. rec. loss:  1.03902998e-02\n",
      "Epoch: 7500 mean train loss:  1.14957387e-02, mean val. rec. loss:  1.03892951e-02\n",
      "Epoch: 7501 mean train loss:  1.14946074e-02, mean val. rec. loss:  1.03882847e-02\n",
      "Epoch: 7502 mean train loss:  1.14934743e-02, mean val. rec. loss:  1.03872698e-02\n",
      "Epoch: 7503 mean train loss:  1.14923337e-02, mean val. rec. loss:  1.03862640e-02\n",
      "Epoch: 7504 mean train loss:  1.14911940e-02, mean val. rec. loss:  1.03852389e-02\n",
      "Epoch: 7505 mean train loss:  1.14900572e-02, mean val. rec. loss:  1.03842376e-02\n",
      "Epoch: 7506 mean train loss:  1.14889175e-02, mean val. rec. loss:  1.03832136e-02\n",
      "Epoch: 7507 mean train loss:  1.14877779e-02, mean val. rec. loss:  1.03822089e-02\n",
      "Epoch: 7508 mean train loss:  1.14866354e-02, mean val. rec. loss:  1.03811917e-02\n",
      "Epoch: 7509 mean train loss:  1.14854995e-02, mean val. rec. loss:  1.03801700e-02\n",
      "Epoch: 7510 mean train loss:  1.14843524e-02, mean val. rec. loss:  1.03791517e-02\n",
      "Epoch: 7511 mean train loss:  1.14832071e-02, mean val. rec. loss:  1.03781357e-02\n",
      "Epoch: 7512 mean train loss:  1.14820684e-02, mean val. rec. loss:  1.03771196e-02\n",
      "Epoch: 7513 mean train loss:  1.14809241e-02, mean val. rec. loss:  1.03761058e-02\n",
      "Epoch: 7514 mean train loss:  1.14797779e-02, mean val. rec. loss:  1.03750830e-02\n",
      "Epoch: 7515 mean train loss:  1.14786336e-02, mean val. rec. loss:  1.03740579e-02\n",
      "Epoch: 7516 mean train loss:  1.14774874e-02, mean val. rec. loss:  1.03730350e-02\n",
      "Epoch: 7517 mean train loss:  1.14763356e-02, mean val. rec. loss:  1.03720190e-02\n",
      "Epoch: 7518 mean train loss:  1.14751941e-02, mean val. rec. loss:  1.03709961e-02\n",
      "Epoch: 7519 mean train loss:  1.14740498e-02, mean val. rec. loss:  1.03699790e-02\n",
      "Epoch: 7520 mean train loss:  1.14728962e-02, mean val. rec. loss:  1.03689482e-02\n",
      "Epoch: 7521 mean train loss:  1.14717435e-02, mean val. rec. loss:  1.03679231e-02\n",
      "Epoch: 7522 mean train loss:  1.14705926e-02, mean val. rec. loss:  1.03668991e-02\n",
      "Epoch: 7523 mean train loss:  1.14694437e-02, mean val. rec. loss:  1.03658728e-02\n",
      "Epoch: 7524 mean train loss:  1.14682910e-02, mean val. rec. loss:  1.03648477e-02\n",
      "Epoch: 7525 mean train loss:  1.14671392e-02, mean val. rec. loss:  1.03638294e-02\n",
      "Epoch: 7526 mean train loss:  1.14659912e-02, mean val. rec. loss:  1.03627952e-02\n",
      "Epoch: 7527 mean train loss:  1.14648422e-02, mean val. rec. loss:  1.03617656e-02\n",
      "Epoch: 7528 mean train loss:  1.14636820e-02, mean val. rec. loss:  1.03607336e-02\n",
      "Epoch: 7529 mean train loss:  1.14625238e-02, mean val. rec. loss:  1.03597040e-02\n",
      "Epoch: 7530 mean train loss:  1.14613776e-02, mean val. rec. loss:  1.03586675e-02\n",
      "Epoch: 7531 mean train loss:  1.14602128e-02, mean val. rec. loss:  1.03576402e-02\n",
      "Epoch: 7532 mean train loss:  1.14590647e-02, mean val. rec. loss:  1.03566060e-02\n",
      "Epoch: 7533 mean train loss:  1.14578990e-02, mean val. rec. loss:  1.03555752e-02\n",
      "Epoch: 7534 mean train loss:  1.14567454e-02, mean val. rec. loss:  1.03545421e-02\n",
      "Epoch: 7535 mean train loss:  1.14555890e-02, mean val. rec. loss:  1.03535011e-02\n",
      "Epoch: 7536 mean train loss:  1.14544270e-02, mean val. rec. loss:  1.03524704e-02\n",
      "Epoch: 7537 mean train loss:  1.14532622e-02, mean val. rec. loss:  1.03514430e-02\n",
      "Epoch: 7538 mean train loss:  1.14521057e-02, mean val. rec. loss:  1.03504054e-02\n",
      "Epoch: 7539 mean train loss:  1.14509437e-02, mean val. rec. loss:  1.03493689e-02\n",
      "Epoch: 7540 mean train loss:  1.14497827e-02, mean val. rec. loss:  1.03483302e-02\n",
      "Epoch: 7541 mean train loss:  1.14486188e-02, mean val. rec. loss:  1.03472938e-02\n",
      "Epoch: 7542 mean train loss:  1.14474549e-02, mean val. rec. loss:  1.03462482e-02\n",
      "Epoch: 7543 mean train loss:  1.14462873e-02, mean val. rec. loss:  1.03452050e-02\n",
      "Epoch: 7544 mean train loss:  1.14451272e-02, mean val. rec. loss:  1.03441674e-02\n",
      "Epoch: 7545 mean train loss:  1.14439652e-02, mean val. rec. loss:  1.03431309e-02\n",
      "Epoch: 7546 mean train loss:  1.14427948e-02, mean val. rec. loss:  1.03420786e-02\n",
      "Epoch: 7547 mean train loss:  1.14416281e-02, mean val. rec. loss:  1.03410410e-02\n",
      "Epoch: 7548 mean train loss:  1.14404596e-02, mean val. rec. loss:  1.03400023e-02\n",
      "Epoch: 7549 mean train loss:  1.14392911e-02, mean val. rec. loss:  1.03389534e-02\n",
      "Epoch: 7550 mean train loss:  1.14381225e-02, mean val. rec. loss:  1.03379022e-02\n",
      "Epoch: 7551 mean train loss:  1.14369522e-02, mean val. rec. loss:  1.03368589e-02\n",
      "Epoch: 7552 mean train loss:  1.14357780e-02, mean val. rec. loss:  1.03358168e-02\n",
      "Epoch: 7553 mean train loss:  1.14346114e-02, mean val. rec. loss:  1.03347735e-02\n",
      "Epoch: 7554 mean train loss:  1.14334438e-02, mean val. rec. loss:  1.03337314e-02\n",
      "Epoch: 7555 mean train loss:  1.14322669e-02, mean val. rec. loss:  1.03326813e-02\n",
      "Epoch: 7556 mean train loss:  1.14310937e-02, mean val. rec. loss:  1.03316335e-02\n",
      "Epoch: 7557 mean train loss:  1.14299196e-02, mean val. rec. loss:  1.03305835e-02\n",
      "Epoch: 7558 mean train loss:  1.14287455e-02, mean val. rec. loss:  1.03295346e-02\n",
      "Epoch: 7559 mean train loss:  1.14275704e-02, mean val. rec. loss:  1.03284777e-02\n",
      "Epoch: 7560 mean train loss:  1.14263982e-02, mean val. rec. loss:  1.03274265e-02\n",
      "Epoch: 7561 mean train loss:  1.14252176e-02, mean val. rec. loss:  1.03263685e-02\n",
      "Epoch: 7562 mean train loss:  1.14240369e-02, mean val. rec. loss:  1.03253150e-02\n",
      "Epoch: 7563 mean train loss:  1.14228619e-02, mean val. rec. loss:  1.03242684e-02\n",
      "Epoch: 7564 mean train loss:  1.14216878e-02, mean val. rec. loss:  1.03232081e-02\n",
      "Epoch: 7565 mean train loss:  1.14204997e-02, mean val. rec. loss:  1.03221592e-02\n",
      "Epoch: 7566 mean train loss:  1.14193284e-02, mean val. rec. loss:  1.03210966e-02\n",
      "Epoch: 7567 mean train loss:  1.14181403e-02, mean val. rec. loss:  1.03200466e-02\n",
      "Epoch: 7568 mean train loss:  1.14169662e-02, mean val. rec. loss:  1.03189920e-02\n",
      "Epoch: 7569 mean train loss:  1.14157791e-02, mean val. rec. loss:  1.03179238e-02\n",
      "Epoch: 7570 mean train loss:  1.14145928e-02, mean val. rec. loss:  1.03168692e-02\n",
      "Epoch: 7571 mean train loss:  1.14134141e-02, mean val. rec. loss:  1.03158100e-02\n",
      "Epoch: 7572 mean train loss:  1.14122344e-02, mean val. rec. loss:  1.03147486e-02\n",
      "Epoch: 7573 mean train loss:  1.14110482e-02, mean val. rec. loss:  1.03136872e-02\n",
      "Epoch: 7574 mean train loss:  1.14098629e-02, mean val. rec. loss:  1.03126270e-02\n",
      "Epoch: 7575 mean train loss:  1.14086776e-02, mean val. rec. loss:  1.03115633e-02\n",
      "Epoch: 7576 mean train loss:  1.14074904e-02, mean val. rec. loss:  1.03105008e-02\n",
      "Epoch: 7577 mean train loss:  1.14063005e-02, mean val. rec. loss:  1.03094359e-02\n",
      "Epoch: 7578 mean train loss:  1.14051087e-02, mean val. rec. loss:  1.03083655e-02\n",
      "Epoch: 7579 mean train loss:  1.14039160e-02, mean val. rec. loss:  1.03073007e-02\n",
      "Epoch: 7580 mean train loss:  1.14027307e-02, mean val. rec. loss:  1.03062393e-02\n",
      "Epoch: 7581 mean train loss:  1.14015445e-02, mean val. rec. loss:  1.03051677e-02\n",
      "Epoch: 7582 mean train loss:  1.14003434e-02, mean val. rec. loss:  1.03041051e-02\n",
      "Epoch: 7583 mean train loss:  1.13991609e-02, mean val. rec. loss:  1.03030335e-02\n",
      "Epoch: 7584 mean train loss:  1.13979598e-02, mean val. rec. loss:  1.03019687e-02\n",
      "Epoch: 7585 mean train loss:  1.13967754e-02, mean val. rec. loss:  1.03009028e-02\n",
      "Epoch: 7586 mean train loss:  1.13955761e-02, mean val. rec. loss:  1.02998187e-02\n",
      "Epoch: 7587 mean train loss:  1.13943769e-02, mean val. rec. loss:  1.02987528e-02\n",
      "Epoch: 7588 mean train loss:  1.13931860e-02, mean val. rec. loss:  1.02976823e-02\n",
      "Epoch: 7589 mean train loss:  1.13919933e-02, mean val. rec. loss:  1.02966050e-02\n",
      "Epoch: 7590 mean train loss:  1.13907931e-02, mean val. rec. loss:  1.02955323e-02\n",
      "Epoch: 7591 mean train loss:  1.13895957e-02, mean val. rec. loss:  1.02944550e-02\n",
      "Epoch: 7592 mean train loss:  1.13883974e-02, mean val. rec. loss:  1.02933811e-02\n",
      "Epoch: 7593 mean train loss:  1.13871982e-02, mean val. rec. loss:  1.02923072e-02\n",
      "Epoch: 7594 mean train loss:  1.13859989e-02, mean val. rec. loss:  1.02912345e-02\n",
      "Epoch: 7595 mean train loss:  1.13848024e-02, mean val. rec. loss:  1.02901561e-02\n",
      "Epoch: 7596 mean train loss:  1.13835967e-02, mean val. rec. loss:  1.02890743e-02\n",
      "Epoch: 7597 mean train loss:  1.13823918e-02, mean val. rec. loss:  1.02880049e-02\n",
      "Epoch: 7598 mean train loss:  1.13811926e-02, mean val. rec. loss:  1.02869186e-02\n",
      "Epoch: 7599 mean train loss:  1.13799896e-02, mean val. rec. loss:  1.02858322e-02\n",
      "Epoch: 7600 mean train loss:  1.13787857e-02, mean val. rec. loss:  1.02847549e-02\n",
      "Epoch: 7601 mean train loss:  1.13775809e-02, mean val. rec. loss:  1.02836799e-02\n",
      "Epoch: 7602 mean train loss:  1.13763751e-02, mean val. rec. loss:  1.02825959e-02\n",
      "Epoch: 7603 mean train loss:  1.13751703e-02, mean val. rec. loss:  1.02815129e-02\n",
      "Epoch: 7604 mean train loss:  1.13739673e-02, mean val. rec. loss:  1.02804345e-02\n",
      "Epoch: 7605 mean train loss:  1.13727550e-02, mean val. rec. loss:  1.02793391e-02\n",
      "Epoch: 7606 mean train loss:  1.13715427e-02, mean val. rec. loss:  1.02782618e-02\n",
      "Epoch: 7607 mean train loss:  1.13703379e-02, mean val. rec. loss:  1.02771754e-02\n",
      "Epoch: 7608 mean train loss:  1.13691275e-02, mean val. rec. loss:  1.02760891e-02\n",
      "Epoch: 7609 mean train loss:  1.13679170e-02, mean val. rec. loss:  1.02750005e-02\n",
      "Epoch: 7610 mean train loss:  1.13667066e-02, mean val. rec. loss:  1.02739141e-02\n",
      "Epoch: 7611 mean train loss:  1.13654943e-02, mean val. rec. loss:  1.02728255e-02\n",
      "Epoch: 7612 mean train loss:  1.13642820e-02, mean val. rec. loss:  1.02717301e-02\n",
      "Epoch: 7613 mean train loss:  1.13630642e-02, mean val. rec. loss:  1.02706369e-02\n",
      "Epoch: 7614 mean train loss:  1.13618547e-02, mean val. rec. loss:  1.02695506e-02\n",
      "Epoch: 7615 mean train loss:  1.13606452e-02, mean val. rec. loss:  1.02684574e-02\n",
      "Epoch: 7616 mean train loss:  1.13594236e-02, mean val. rec. loss:  1.02673586e-02\n",
      "Epoch: 7617 mean train loss:  1.13582085e-02, mean val. rec. loss:  1.02662689e-02\n",
      "Epoch: 7618 mean train loss:  1.13569916e-02, mean val. rec. loss:  1.02651814e-02\n",
      "Epoch: 7619 mean train loss:  1.13557737e-02, mean val. rec. loss:  1.02640837e-02\n",
      "Epoch: 7620 mean train loss:  1.13545558e-02, mean val. rec. loss:  1.02629871e-02\n",
      "Epoch: 7621 mean train loss:  1.13533398e-02, mean val. rec. loss:  1.02618826e-02\n",
      "Epoch: 7622 mean train loss:  1.13521163e-02, mean val. rec. loss:  1.02607872e-02\n",
      "Epoch: 7623 mean train loss:  1.13508910e-02, mean val. rec. loss:  1.02596963e-02\n",
      "Epoch: 7624 mean train loss:  1.13496741e-02, mean val. rec. loss:  1.02586009e-02\n",
      "Epoch: 7625 mean train loss:  1.13484571e-02, mean val. rec. loss:  1.02574987e-02\n",
      "Epoch: 7626 mean train loss:  1.13472337e-02, mean val. rec. loss:  1.02563976e-02\n",
      "Epoch: 7627 mean train loss:  1.13460102e-02, mean val. rec. loss:  1.02552965e-02\n",
      "Epoch: 7628 mean train loss:  1.13447868e-02, mean val. rec. loss:  1.02541943e-02\n",
      "Epoch: 7629 mean train loss:  1.13435624e-02, mean val. rec. loss:  1.02530898e-02\n",
      "Epoch: 7630 mean train loss:  1.13423315e-02, mean val. rec. loss:  1.02519785e-02\n",
      "Epoch: 7631 mean train loss:  1.13411006e-02, mean val. rec. loss:  1.02508763e-02\n",
      "Epoch: 7632 mean train loss:  1.13398790e-02, mean val. rec. loss:  1.02497774e-02\n",
      "Epoch: 7633 mean train loss:  1.13386546e-02, mean val. rec. loss:  1.02486729e-02\n",
      "Epoch: 7634 mean train loss:  1.13374218e-02, mean val. rec. loss:  1.02475719e-02\n",
      "Epoch: 7635 mean train loss:  1.13361928e-02, mean val. rec. loss:  1.02464628e-02\n",
      "Epoch: 7636 mean train loss:  1.13349628e-02, mean val. rec. loss:  1.02453493e-02\n",
      "Epoch: 7637 mean train loss:  1.13337328e-02, mean val. rec. loss:  1.02442493e-02\n",
      "Epoch: 7638 mean train loss:  1.13325019e-02, mean val. rec. loss:  1.02431380e-02\n",
      "Epoch: 7639 mean train loss:  1.13312719e-02, mean val. rec. loss:  1.02420199e-02\n",
      "Epoch: 7640 mean train loss:  1.13300345e-02, mean val. rec. loss:  1.02409109e-02\n",
      "Epoch: 7641 mean train loss:  1.13287980e-02, mean val. rec. loss:  1.02398030e-02\n",
      "Epoch: 7642 mean train loss:  1.13275662e-02, mean val. rec. loss:  1.02386872e-02\n",
      "Epoch: 7643 mean train loss:  1.13263315e-02, mean val. rec. loss:  1.02375725e-02\n",
      "Epoch: 7644 mean train loss:  1.13250969e-02, mean val. rec. loss:  1.02364566e-02\n",
      "Epoch: 7645 mean train loss:  1.13238604e-02, mean val. rec. loss:  1.02353431e-02\n",
      "Epoch: 7646 mean train loss:  1.13226220e-02, mean val. rec. loss:  1.02342284e-02\n",
      "Epoch: 7647 mean train loss:  1.13213828e-02, mean val. rec. loss:  1.02331046e-02\n",
      "Epoch: 7648 mean train loss:  1.13201388e-02, mean val. rec. loss:  1.02319990e-02\n",
      "Epoch: 7649 mean train loss:  1.13189042e-02, mean val. rec. loss:  1.02308775e-02\n",
      "Epoch: 7650 mean train loss:  1.13176695e-02, mean val. rec. loss:  1.02297628e-02\n",
      "Epoch: 7651 mean train loss:  1.13164237e-02, mean val. rec. loss:  1.02286458e-02\n",
      "Epoch: 7652 mean train loss:  1.13151817e-02, mean val. rec. loss:  1.02275186e-02\n",
      "Epoch: 7653 mean train loss:  1.13139396e-02, mean val. rec. loss:  1.02263937e-02\n",
      "Epoch: 7654 mean train loss:  1.13126975e-02, mean val. rec. loss:  1.02252745e-02\n",
      "Epoch: 7655 mean train loss:  1.13114517e-02, mean val. rec. loss:  1.02241586e-02\n",
      "Epoch: 7656 mean train loss:  1.13102077e-02, mean val. rec. loss:  1.02230224e-02\n",
      "Epoch: 7657 mean train loss:  1.13089573e-02, mean val. rec. loss:  1.02219032e-02\n",
      "Epoch: 7658 mean train loss:  1.13077161e-02, mean val. rec. loss:  1.02207783e-02\n",
      "Epoch: 7659 mean train loss:  1.13064722e-02, mean val. rec. loss:  1.02196488e-02\n",
      "Epoch: 7660 mean train loss:  1.13052217e-02, mean val. rec. loss:  1.02185216e-02\n",
      "Epoch: 7661 mean train loss:  1.13039722e-02, mean val. rec. loss:  1.02173945e-02\n",
      "Epoch: 7662 mean train loss:  1.13027255e-02, mean val. rec. loss:  1.02162673e-02\n",
      "Epoch: 7663 mean train loss:  1.13014731e-02, mean val. rec. loss:  1.02151424e-02\n",
      "Epoch: 7664 mean train loss:  1.13002227e-02, mean val. rec. loss:  1.02140152e-02\n",
      "Epoch: 7665 mean train loss:  1.12989713e-02, mean val. rec. loss:  1.02128892e-02\n",
      "Epoch: 7666 mean train loss:  1.12977236e-02, mean val. rec. loss:  1.02117541e-02\n",
      "Epoch: 7667 mean train loss:  1.12964648e-02, mean val. rec. loss:  1.02106178e-02\n",
      "Epoch: 7668 mean train loss:  1.12952078e-02, mean val. rec. loss:  1.02094929e-02\n",
      "Epoch: 7669 mean train loss:  1.12939564e-02, mean val. rec. loss:  1.02083589e-02\n",
      "Epoch: 7670 mean train loss:  1.12927069e-02, mean val. rec. loss:  1.02072148e-02\n",
      "Epoch: 7671 mean train loss:  1.12914415e-02, mean val. rec. loss:  1.02060842e-02\n",
      "Epoch: 7672 mean train loss:  1.12901948e-02, mean val. rec. loss:  1.02049434e-02\n",
      "Epoch: 7673 mean train loss:  1.12889285e-02, mean val. rec. loss:  1.02038140e-02\n",
      "Epoch: 7674 mean train loss:  1.12876771e-02, mean val. rec. loss:  1.02026664e-02\n",
      "Epoch: 7675 mean train loss:  1.12864155e-02, mean val. rec. loss:  1.02015301e-02\n",
      "Epoch: 7676 mean train loss:  1.12851510e-02, mean val. rec. loss:  1.02003962e-02\n",
      "Epoch: 7677 mean train loss:  1.12838950e-02, mean val. rec. loss:  1.01992588e-02\n",
      "Epoch: 7678 mean train loss:  1.12826399e-02, mean val. rec. loss:  1.01981248e-02\n",
      "Epoch: 7679 mean train loss:  1.12813708e-02, mean val. rec. loss:  1.01969716e-02\n",
      "Epoch: 7680 mean train loss:  1.12801101e-02, mean val. rec. loss:  1.01958398e-02\n",
      "Epoch: 7681 mean train loss:  1.12788447e-02, mean val. rec. loss:  1.01946855e-02\n",
      "Epoch: 7682 mean train loss:  1.12775822e-02, mean val. rec. loss:  1.01935537e-02\n",
      "Epoch: 7683 mean train loss:  1.12763159e-02, mean val. rec. loss:  1.01924039e-02\n",
      "Epoch: 7684 mean train loss:  1.12750552e-02, mean val. rec. loss:  1.01912540e-02\n",
      "Epoch: 7685 mean train loss:  1.12737833e-02, mean val. rec. loss:  1.01901042e-02\n",
      "Epoch: 7686 mean train loss:  1.12725114e-02, mean val. rec. loss:  1.01889566e-02\n",
      "Epoch: 7687 mean train loss:  1.12712489e-02, mean val. rec. loss:  1.01878158e-02\n",
      "Epoch: 7688 mean train loss:  1.12699844e-02, mean val. rec. loss:  1.01866614e-02\n",
      "Epoch: 7689 mean train loss:  1.12687051e-02, mean val. rec. loss:  1.01855184e-02\n",
      "Epoch: 7690 mean train loss:  1.12674435e-02, mean val. rec. loss:  1.01843629e-02\n",
      "Epoch: 7691 mean train loss:  1.12661632e-02, mean val. rec. loss:  1.01832164e-02\n",
      "Epoch: 7692 mean train loss:  1.12648997e-02, mean val. rec. loss:  1.01820666e-02\n",
      "Epoch: 7693 mean train loss:  1.12636232e-02, mean val. rec. loss:  1.01809054e-02\n",
      "Epoch: 7694 mean train loss:  1.12623467e-02, mean val. rec. loss:  1.01797555e-02\n",
      "Epoch: 7695 mean train loss:  1.12610757e-02, mean val. rec. loss:  1.01786034e-02\n",
      "Epoch: 7696 mean train loss:  1.12598057e-02, mean val. rec. loss:  1.01774445e-02\n",
      "Epoch: 7697 mean train loss:  1.12585254e-02, mean val. rec. loss:  1.01762856e-02\n",
      "Epoch: 7698 mean train loss:  1.12572489e-02, mean val. rec. loss:  1.01751289e-02\n",
      "Epoch: 7699 mean train loss:  1.12559714e-02, mean val. rec. loss:  1.01739734e-02\n",
      "Epoch: 7700 mean train loss:  1.12546940e-02, mean val. rec. loss:  1.01728156e-02\n",
      "Epoch: 7701 mean train loss:  1.12534147e-02, mean val. rec. loss:  1.01716555e-02\n",
      "Epoch: 7702 mean train loss:  1.12521372e-02, mean val. rec. loss:  1.01705023e-02\n",
      "Epoch: 7703 mean train loss:  1.12508597e-02, mean val. rec. loss:  1.01693388e-02\n",
      "Epoch: 7704 mean train loss:  1.12495748e-02, mean val. rec. loss:  1.01681708e-02\n",
      "Epoch: 7705 mean train loss:  1.12482881e-02, mean val. rec. loss:  1.01670210e-02\n",
      "Epoch: 7706 mean train loss:  1.12470106e-02, mean val. rec. loss:  1.01658564e-02\n",
      "Epoch: 7707 mean train loss:  1.12457303e-02, mean val. rec. loss:  1.01646850e-02\n",
      "Epoch: 7708 mean train loss:  1.12444389e-02, mean val. rec. loss:  1.01635238e-02\n",
      "Epoch: 7709 mean train loss:  1.12431614e-02, mean val. rec. loss:  1.01623535e-02\n",
      "Epoch: 7710 mean train loss:  1.12418672e-02, mean val. rec. loss:  1.01611912e-02\n",
      "Epoch: 7711 mean train loss:  1.12405898e-02, mean val. rec. loss:  1.01600175e-02\n",
      "Epoch: 7712 mean train loss:  1.12392993e-02, mean val. rec. loss:  1.01588461e-02\n",
      "Epoch: 7713 mean train loss:  1.12380060e-02, mean val. rec. loss:  1.01576815e-02\n",
      "Epoch: 7714 mean train loss:  1.12367201e-02, mean val. rec. loss:  1.01565135e-02\n",
      "Epoch: 7715 mean train loss:  1.12354371e-02, mean val. rec. loss:  1.01553489e-02\n",
      "Epoch: 7716 mean train loss:  1.12341419e-02, mean val. rec. loss:  1.01541662e-02\n",
      "Epoch: 7717 mean train loss:  1.12328505e-02, mean val. rec. loss:  1.01530061e-02\n",
      "Epoch: 7718 mean train loss:  1.12315581e-02, mean val. rec. loss:  1.01518211e-02\n",
      "Epoch: 7719 mean train loss:  1.12302658e-02, mean val. rec. loss:  1.01506599e-02\n",
      "Epoch: 7720 mean train loss:  1.12289725e-02, mean val. rec. loss:  1.01494783e-02\n",
      "Epoch: 7721 mean train loss:  1.12276792e-02, mean val. rec. loss:  1.01483058e-02\n",
      "Epoch: 7722 mean train loss:  1.12263878e-02, mean val. rec. loss:  1.01471287e-02\n",
      "Epoch: 7723 mean train loss:  1.12250870e-02, mean val. rec. loss:  1.01459471e-02\n",
      "Epoch: 7724 mean train loss:  1.12237863e-02, mean val. rec. loss:  1.01447814e-02\n",
      "Epoch: 7725 mean train loss:  1.12224930e-02, mean val. rec. loss:  1.01435987e-02\n",
      "Epoch: 7726 mean train loss:  1.12211997e-02, mean val. rec. loss:  1.01424114e-02\n",
      "Epoch: 7727 mean train loss:  1.12198924e-02, mean val. rec. loss:  1.01412366e-02\n",
      "Epoch: 7728 mean train loss:  1.12186019e-02, mean val. rec. loss:  1.01400482e-02\n",
      "Epoch: 7729 mean train loss:  1.12172919e-02, mean val. rec. loss:  1.01388700e-02\n",
      "Epoch: 7730 mean train loss:  1.12159995e-02, mean val. rec. loss:  1.01376816e-02\n",
      "Epoch: 7731 mean train loss:  1.12146904e-02, mean val. rec. loss:  1.01365023e-02\n",
      "Epoch: 7732 mean train loss:  1.12133915e-02, mean val. rec. loss:  1.01353150e-02\n",
      "Epoch: 7733 mean train loss:  1.12120917e-02, mean val. rec. loss:  1.01341356e-02\n",
      "Epoch: 7734 mean train loss:  1.12107817e-02, mean val. rec. loss:  1.01329393e-02\n",
      "Epoch: 7735 mean train loss:  1.12094744e-02, mean val. rec. loss:  1.01317611e-02\n",
      "Epoch: 7736 mean train loss:  1.12081727e-02, mean val. rec. loss:  1.01305727e-02\n",
      "Epoch: 7737 mean train loss:  1.12068646e-02, mean val. rec. loss:  1.01293831e-02\n",
      "Epoch: 7738 mean train loss:  1.12055582e-02, mean val. rec. loss:  1.01281959e-02\n",
      "Epoch: 7739 mean train loss:  1.12042510e-02, mean val. rec. loss:  1.01270052e-02\n",
      "Epoch: 7740 mean train loss:  1.12029409e-02, mean val. rec. loss:  1.01258157e-02\n",
      "Epoch: 7741 mean train loss:  1.12016309e-02, mean val. rec. loss:  1.01246170e-02\n",
      "Epoch: 7742 mean train loss:  1.12003162e-02, mean val. rec. loss:  1.01234343e-02\n",
      "Epoch: 7743 mean train loss:  1.11990089e-02, mean val. rec. loss:  1.01222334e-02\n",
      "Epoch: 7744 mean train loss:  1.11977016e-02, mean val. rec. loss:  1.01210382e-02\n",
      "Epoch: 7745 mean train loss:  1.11963832e-02, mean val. rec. loss:  1.01198362e-02\n",
      "Epoch: 7746 mean train loss:  1.11950704e-02, mean val. rec. loss:  1.01186455e-02\n",
      "Epoch: 7747 mean train loss:  1.11937557e-02, mean val. rec. loss:  1.01174537e-02\n",
      "Epoch: 7748 mean train loss:  1.11924391e-02, mean val. rec. loss:  1.01162506e-02\n",
      "Epoch: 7749 mean train loss:  1.11911244e-02, mean val. rec. loss:  1.01150485e-02\n",
      "Epoch: 7750 mean train loss:  1.11898069e-02, mean val. rec. loss:  1.01138545e-02\n",
      "Epoch: 7751 mean train loss:  1.11884885e-02, mean val. rec. loss:  1.01126559e-02\n",
      "Epoch: 7752 mean train loss:  1.11871756e-02, mean val. rec. loss:  1.01114482e-02\n",
      "Epoch: 7753 mean train loss:  1.11858516e-02, mean val. rec. loss:  1.01102450e-02\n",
      "Epoch: 7754 mean train loss:  1.11845266e-02, mean val. rec. loss:  1.01090475e-02\n",
      "Epoch: 7755 mean train loss:  1.11832101e-02, mean val. rec. loss:  1.01078455e-02\n",
      "Epoch: 7756 mean train loss:  1.11818944e-02, mean val. rec. loss:  1.01066390e-02\n",
      "Epoch: 7757 mean train loss:  1.11805704e-02, mean val. rec. loss:  1.01054336e-02\n",
      "Epoch: 7758 mean train loss:  1.11792473e-02, mean val. rec. loss:  1.01042259e-02\n",
      "Epoch: 7759 mean train loss:  1.11779233e-02, mean val. rec. loss:  1.01030205e-02\n",
      "Epoch: 7760 mean train loss:  1.11765993e-02, mean val. rec. loss:  1.01018128e-02\n",
      "Epoch: 7761 mean train loss:  1.11752725e-02, mean val. rec. loss:  1.01005994e-02\n",
      "Epoch: 7762 mean train loss:  1.11739420e-02, mean val. rec. loss:  1.00993861e-02\n",
      "Epoch: 7763 mean train loss:  1.11726096e-02, mean val. rec. loss:  1.00981863e-02\n",
      "Epoch: 7764 mean train loss:  1.11712865e-02, mean val. rec. loss:  1.00969662e-02\n",
      "Epoch: 7765 mean train loss:  1.11699615e-02, mean val. rec. loss:  1.00957551e-02\n",
      "Epoch: 7766 mean train loss:  1.11686273e-02, mean val. rec. loss:  1.00945440e-02\n",
      "Epoch: 7767 mean train loss:  1.11672967e-02, mean val. rec. loss:  1.00933238e-02\n",
      "Epoch: 7768 mean train loss:  1.11659662e-02, mean val. rec. loss:  1.00921071e-02\n",
      "Epoch: 7769 mean train loss:  1.11646338e-02, mean val. rec. loss:  1.00908948e-02\n",
      "Epoch: 7770 mean train loss:  1.11633014e-02, mean val. rec. loss:  1.00896838e-02\n",
      "Epoch: 7771 mean train loss:  1.11619681e-02, mean val. rec. loss:  1.00884568e-02\n",
      "Epoch: 7772 mean train loss:  1.11606292e-02, mean val. rec. loss:  1.00872389e-02\n",
      "Epoch: 7773 mean train loss:  1.11592977e-02, mean val. rec. loss:  1.00860210e-02\n",
      "Epoch: 7774 mean train loss:  1.11579653e-02, mean val. rec. loss:  1.00847963e-02\n",
      "Epoch: 7775 mean train loss:  1.11566254e-02, mean val. rec. loss:  1.00835762e-02\n",
      "Epoch: 7776 mean train loss:  1.11552828e-02, mean val. rec. loss:  1.00823605e-02\n",
      "Epoch: 7777 mean train loss:  1.11539495e-02, mean val. rec. loss:  1.00811370e-02\n",
      "Epoch: 7778 mean train loss:  1.11526087e-02, mean val. rec. loss:  1.00799134e-02\n",
      "Epoch: 7779 mean train loss:  1.11512698e-02, mean val. rec. loss:  1.00786865e-02\n",
      "Epoch: 7780 mean train loss:  1.11499290e-02, mean val. rec. loss:  1.00774618e-02\n",
      "Epoch: 7781 mean train loss:  1.11485864e-02, mean val. rec. loss:  1.00762371e-02\n",
      "Epoch: 7782 mean train loss:  1.11472447e-02, mean val. rec. loss:  1.00750056e-02\n",
      "Epoch: 7783 mean train loss:  1.11458974e-02, mean val. rec. loss:  1.00737763e-02\n",
      "Epoch: 7784 mean train loss:  1.11445585e-02, mean val. rec. loss:  1.00725516e-02\n",
      "Epoch: 7785 mean train loss:  1.11432158e-02, mean val. rec. loss:  1.00713224e-02\n",
      "Epoch: 7786 mean train loss:  1.11418657e-02, mean val. rec. loss:  1.00700875e-02\n",
      "Epoch: 7787 mean train loss:  1.11405156e-02, mean val. rec. loss:  1.00688571e-02\n",
      "Epoch: 7788 mean train loss:  1.11391767e-02, mean val. rec. loss:  1.00676200e-02\n",
      "Epoch: 7789 mean train loss:  1.11378192e-02, mean val. rec. loss:  1.00663896e-02\n",
      "Epoch: 7790 mean train loss:  1.11364784e-02, mean val. rec. loss:  1.00651536e-02\n",
      "Epoch: 7791 mean train loss:  1.11351209e-02, mean val. rec. loss:  1.00639243e-02\n",
      "Epoch: 7792 mean train loss:  1.11337782e-02, mean val. rec. loss:  1.00626860e-02\n",
      "Epoch: 7793 mean train loss:  1.11324188e-02, mean val. rec. loss:  1.00614500e-02\n",
      "Epoch: 7794 mean train loss:  1.11310706e-02, mean val. rec. loss:  1.00602117e-02\n",
      "Epoch: 7795 mean train loss:  1.11297214e-02, mean val. rec. loss:  1.00589711e-02\n",
      "Epoch: 7796 mean train loss:  1.11283639e-02, mean val. rec. loss:  1.00577306e-02\n",
      "Epoch: 7797 mean train loss:  1.11270045e-02, mean val. rec. loss:  1.00564968e-02\n",
      "Epoch: 7798 mean train loss:  1.11256544e-02, mean val. rec. loss:  1.00552517e-02\n",
      "Epoch: 7799 mean train loss:  1.11242978e-02, mean val. rec. loss:  1.00540111e-02\n",
      "Epoch: 7800 mean train loss:  1.11229403e-02, mean val. rec. loss:  1.00527683e-02\n",
      "Epoch: 7801 mean train loss:  1.11215827e-02, mean val. rec. loss:  1.00515243e-02\n",
      "Epoch: 7802 mean train loss:  1.11202233e-02, mean val. rec. loss:  1.00502803e-02\n",
      "Epoch: 7803 mean train loss:  1.11188630e-02, mean val. rec. loss:  1.00490352e-02\n",
      "Epoch: 7804 mean train loss:  1.11175045e-02, mean val. rec. loss:  1.00477833e-02\n",
      "Epoch: 7805 mean train loss:  1.11161377e-02, mean val. rec. loss:  1.00465314e-02\n",
      "Epoch: 7806 mean train loss:  1.11147802e-02, mean val. rec. loss:  1.00452920e-02\n",
      "Epoch: 7807 mean train loss:  1.11134217e-02, mean val. rec. loss:  1.00440401e-02\n",
      "Epoch: 7808 mean train loss:  1.11120539e-02, mean val. rec. loss:  1.00427882e-02\n",
      "Epoch: 7809 mean train loss:  1.11106852e-02, mean val. rec. loss:  1.00415476e-02\n",
      "Epoch: 7810 mean train loss:  1.11093239e-02, mean val. rec. loss:  1.00402877e-02\n",
      "Epoch: 7811 mean train loss:  1.11079571e-02, mean val. rec. loss:  1.00390290e-02\n",
      "Epoch: 7812 mean train loss:  1.11065921e-02, mean val. rec. loss:  1.00377771e-02\n",
      "Epoch: 7813 mean train loss:  1.11052234e-02, mean val. rec. loss:  1.00365286e-02\n",
      "Epoch: 7814 mean train loss:  1.11038547e-02, mean val. rec. loss:  1.00352710e-02\n",
      "Epoch: 7815 mean train loss:  1.11024851e-02, mean val. rec. loss:  1.00340123e-02\n",
      "Epoch: 7816 mean train loss:  1.11011154e-02, mean val. rec. loss:  1.00327536e-02\n",
      "Epoch: 7817 mean train loss:  1.10997393e-02, mean val. rec. loss:  1.00314994e-02\n",
      "Epoch: 7818 mean train loss:  1.10983724e-02, mean val. rec. loss:  1.00302407e-02\n",
      "Epoch: 7819 mean train loss:  1.10970037e-02, mean val. rec. loss:  1.00289865e-02\n",
      "Epoch: 7820 mean train loss:  1.10956266e-02, mean val. rec. loss:  1.00277165e-02\n",
      "Epoch: 7821 mean train loss:  1.10942477e-02, mean val. rec. loss:  1.00264578e-02\n",
      "Epoch: 7822 mean train loss:  1.10928752e-02, mean val. rec. loss:  1.00251945e-02\n",
      "Epoch: 7823 mean train loss:  1.10915009e-02, mean val. rec. loss:  1.00239313e-02\n",
      "Epoch: 7824 mean train loss:  1.10901238e-02, mean val. rec. loss:  1.00226635e-02\n",
      "Epoch: 7825 mean train loss:  1.10887467e-02, mean val. rec. loss:  1.00214014e-02\n",
      "Epoch: 7826 mean train loss:  1.10873687e-02, mean val. rec. loss:  1.00201347e-02\n",
      "Epoch: 7827 mean train loss:  1.10859888e-02, mean val. rec. loss:  1.00188692e-02\n",
      "Epoch: 7828 mean train loss:  1.10846090e-02, mean val. rec. loss:  1.00175969e-02\n",
      "Epoch: 7829 mean train loss:  1.10832235e-02, mean val. rec. loss:  1.00163359e-02\n",
      "Epoch: 7830 mean train loss:  1.10818455e-02, mean val. rec. loss:  1.00150602e-02\n",
      "Epoch: 7831 mean train loss:  1.10804674e-02, mean val. rec. loss:  1.00137901e-02\n",
      "Epoch: 7832 mean train loss:  1.10790801e-02, mean val. rec. loss:  1.00125144e-02\n",
      "Epoch: 7833 mean train loss:  1.10776909e-02, mean val. rec. loss:  1.00112443e-02\n",
      "Epoch: 7834 mean train loss:  1.10763101e-02, mean val. rec. loss:  1.00099743e-02\n",
      "Epoch: 7835 mean train loss:  1.10749256e-02, mean val. rec. loss:  1.00087020e-02\n",
      "Epoch: 7836 mean train loss:  1.10735373e-02, mean val. rec. loss:  1.00074217e-02\n",
      "Epoch: 7837 mean train loss:  1.10721509e-02, mean val. rec. loss:  1.00061403e-02\n",
      "Epoch: 7838 mean train loss:  1.10707617e-02, mean val. rec. loss:  1.00048680e-02\n",
      "Epoch: 7839 mean train loss:  1.10693725e-02, mean val. rec. loss:  1.00035991e-02\n",
      "Epoch: 7840 mean train loss:  1.10679815e-02, mean val. rec. loss:  1.00023063e-02\n",
      "Epoch: 7841 mean train loss:  1.10665885e-02, mean val. rec. loss:  1.00010317e-02\n",
      "Epoch: 7842 mean train loss:  1.10652012e-02, mean val. rec. loss:  9.99974808e-03\n",
      "Epoch: 7843 mean train loss:  1.10638120e-02, mean val. rec. loss:  9.99846215e-03\n",
      "Epoch: 7844 mean train loss:  1.10624154e-02, mean val. rec. loss:  9.99718075e-03\n",
      "Epoch: 7845 mean train loss:  1.10610169e-02, mean val. rec. loss:  9.99590163e-03\n",
      "Epoch: 7846 mean train loss:  1.10596249e-02, mean val. rec. loss:  9.99461797e-03\n",
      "Epoch: 7847 mean train loss:  1.10582329e-02, mean val. rec. loss:  9.99333430e-03\n",
      "Epoch: 7848 mean train loss:  1.10568362e-02, mean val. rec. loss:  9.99204611e-03\n",
      "Epoch: 7849 mean train loss:  1.10554377e-02, mean val. rec. loss:  9.99075791e-03\n",
      "Epoch: 7850 mean train loss:  1.10540392e-02, mean val. rec. loss:  9.98947085e-03\n",
      "Epoch: 7851 mean train loss:  1.10526407e-02, mean val. rec. loss:  9.98818265e-03\n",
      "Epoch: 7852 mean train loss:  1.10512422e-02, mean val. rec. loss:  9.98689785e-03\n",
      "Epoch: 7853 mean train loss:  1.10498400e-02, mean val. rec. loss:  9.98560285e-03\n",
      "Epoch: 7854 mean train loss:  1.10484340e-02, mean val. rec. loss:  9.98430331e-03\n",
      "Epoch: 7855 mean train loss:  1.10470272e-02, mean val. rec. loss:  9.98300945e-03\n",
      "Epoch: 7856 mean train loss:  1.10456296e-02, mean val. rec. loss:  9.98171785e-03\n",
      "Epoch: 7857 mean train loss:  1.10442283e-02, mean val. rec. loss:  9.98042058e-03\n",
      "Epoch: 7858 mean train loss:  1.10428186e-02, mean val. rec. loss:  9.97912104e-03\n",
      "Epoch: 7859 mean train loss:  1.10414080e-02, mean val. rec. loss:  9.97783625e-03\n",
      "Epoch: 7860 mean train loss:  1.10400076e-02, mean val. rec. loss:  9.97653104e-03\n",
      "Epoch: 7861 mean train loss:  1.10385989e-02, mean val. rec. loss:  9.97522470e-03\n",
      "Epoch: 7862 mean train loss:  1.10371892e-02, mean val. rec. loss:  9.97393310e-03\n",
      "Epoch: 7863 mean train loss:  1.10357805e-02, mean val. rec. loss:  9.97263810e-03\n",
      "Epoch: 7864 mean train loss:  1.10343708e-02, mean val. rec. loss:  9.97133289e-03\n",
      "Epoch: 7865 mean train loss:  1.10329592e-02, mean val. rec. loss:  9.97002655e-03\n",
      "Epoch: 7866 mean train loss:  1.10315477e-02, mean val. rec. loss:  9.96873155e-03\n",
      "Epoch: 7867 mean train loss:  1.10301352e-02, mean val. rec. loss:  9.96742407e-03\n",
      "Epoch: 7868 mean train loss:  1.10287255e-02, mean val. rec. loss:  9.96612340e-03\n",
      "Epoch: 7869 mean train loss:  1.10273075e-02, mean val. rec. loss:  9.96480572e-03\n",
      "Epoch: 7870 mean train loss:  1.10258866e-02, mean val. rec. loss:  9.96350165e-03\n",
      "Epoch: 7871 mean train loss:  1.10244760e-02, mean val. rec. loss:  9.96219531e-03\n",
      "Epoch: 7872 mean train loss:  1.10230636e-02, mean val. rec. loss:  9.96087989e-03\n",
      "Epoch: 7873 mean train loss:  1.10216408e-02, mean val. rec. loss:  9.95957355e-03\n",
      "Epoch: 7874 mean train loss:  1.10202181e-02, mean val. rec. loss:  9.95826948e-03\n",
      "Epoch: 7875 mean train loss:  1.10188029e-02, mean val. rec. loss:  9.95695747e-03\n",
      "Epoch: 7876 mean train loss:  1.10173839e-02, mean val. rec. loss:  9.95564319e-03\n",
      "Epoch: 7877 mean train loss:  1.10159621e-02, mean val. rec. loss:  9.95433005e-03\n",
      "Epoch: 7878 mean train loss:  1.10145403e-02, mean val. rec. loss:  9.95301463e-03\n",
      "Epoch: 7879 mean train loss:  1.10131167e-02, mean val. rec. loss:  9.95170035e-03\n",
      "Epoch: 7880 mean train loss:  1.10116930e-02, mean val. rec. loss:  9.95038607e-03\n",
      "Epoch: 7881 mean train loss:  1.10102694e-02, mean val. rec. loss:  9.94906386e-03\n",
      "Epoch: 7882 mean train loss:  1.10088420e-02, mean val. rec. loss:  9.94773937e-03\n",
      "Epoch: 7883 mean train loss:  1.10074137e-02, mean val. rec. loss:  9.94641942e-03\n",
      "Epoch: 7884 mean train loss:  1.10059901e-02, mean val. rec. loss:  9.94510174e-03\n",
      "Epoch: 7885 mean train loss:  1.10045664e-02, mean val. rec. loss:  9.94378066e-03\n",
      "Epoch: 7886 mean train loss:  1.10031335e-02, mean val. rec. loss:  9.94245504e-03\n",
      "Epoch: 7887 mean train loss:  1.10016986e-02, mean val. rec. loss:  9.94114417e-03\n",
      "Epoch: 7888 mean train loss:  1.10002722e-02, mean val. rec. loss:  9.93981741e-03\n",
      "Epoch: 7889 mean train loss:  1.09988458e-02, mean val. rec. loss:  9.93849520e-03\n",
      "Epoch: 7890 mean train loss:  1.09974100e-02, mean val. rec. loss:  9.93716164e-03\n",
      "Epoch: 7891 mean train loss:  1.09959771e-02, mean val. rec. loss:  9.93583829e-03\n",
      "Epoch: 7892 mean train loss:  1.09945422e-02, mean val. rec. loss:  9.93451267e-03\n",
      "Epoch: 7893 mean train loss:  1.09931074e-02, mean val. rec. loss:  9.93318138e-03\n",
      "Epoch: 7894 mean train loss:  1.09916726e-02, mean val. rec. loss:  9.93184442e-03\n",
      "Epoch: 7895 mean train loss:  1.09902350e-02, mean val. rec. loss:  9.93051880e-03\n",
      "Epoch: 7896 mean train loss:  1.09887974e-02, mean val. rec. loss:  9.92919205e-03\n",
      "Epoch: 7897 mean train loss:  1.09873588e-02, mean val. rec. loss:  9.92785736e-03\n",
      "Epoch: 7898 mean train loss:  1.09859212e-02, mean val. rec. loss:  9.92652267e-03\n",
      "Epoch: 7899 mean train loss:  1.09844864e-02, mean val. rec. loss:  9.92518911e-03\n",
      "Epoch: 7900 mean train loss:  1.09830395e-02, mean val. rec. loss:  9.92384422e-03\n",
      "Epoch: 7901 mean train loss:  1.09815944e-02, mean val. rec. loss:  9.92251293e-03\n",
      "Epoch: 7902 mean train loss:  1.09801568e-02, mean val. rec. loss:  9.92117711e-03\n",
      "Epoch: 7903 mean train loss:  1.09787174e-02, mean val. rec. loss:  9.91982881e-03\n",
      "Epoch: 7904 mean train loss:  1.09772704e-02, mean val. rec. loss:  9.91849185e-03\n",
      "Epoch: 7905 mean train loss:  1.09758189e-02, mean val. rec. loss:  9.91715829e-03\n",
      "Epoch: 7906 mean train loss:  1.09743784e-02, mean val. rec. loss:  9.91581793e-03\n",
      "Epoch: 7907 mean train loss:  1.09729362e-02, mean val. rec. loss:  9.91447304e-03\n",
      "Epoch: 7908 mean train loss:  1.09714874e-02, mean val. rec. loss:  9.91312701e-03\n",
      "Epoch: 7909 mean train loss:  1.09700405e-02, mean val. rec. loss:  9.91178098e-03\n",
      "Epoch: 7910 mean train loss:  1.09685908e-02, mean val. rec. loss:  9.91043608e-03\n",
      "Epoch: 7911 mean train loss:  1.09671401e-02, mean val. rec. loss:  9.90909118e-03\n",
      "Epoch: 7912 mean train loss:  1.09656895e-02, mean val. rec. loss:  9.90774175e-03\n",
      "Epoch: 7913 mean train loss:  1.09642360e-02, mean val. rec. loss:  9.90639572e-03\n",
      "Epoch: 7914 mean train loss:  1.09627835e-02, mean val. rec. loss:  9.90504969e-03\n",
      "Epoch: 7915 mean train loss:  1.09613310e-02, mean val. rec. loss:  9.90370026e-03\n",
      "Epoch: 7916 mean train loss:  1.09598757e-02, mean val. rec. loss:  9.90234402e-03\n",
      "Epoch: 7917 mean train loss:  1.09584158e-02, mean val. rec. loss:  9.90098779e-03\n",
      "Epoch: 7918 mean train loss:  1.09569558e-02, mean val. rec. loss:  9.89964289e-03\n",
      "Epoch: 7919 mean train loss:  1.09555024e-02, mean val. rec. loss:  9.89828439e-03\n",
      "Epoch: 7920 mean train loss:  1.09540480e-02, mean val. rec. loss:  9.89692815e-03\n",
      "Epoch: 7921 mean train loss:  1.09525871e-02, mean val. rec. loss:  9.89556511e-03\n",
      "Epoch: 7922 mean train loss:  1.09511216e-02, mean val. rec. loss:  9.89420661e-03\n",
      "Epoch: 7923 mean train loss:  1.09496644e-02, mean val. rec. loss:  9.89285491e-03\n",
      "Epoch: 7924 mean train loss:  1.09482072e-02, mean val. rec. loss:  9.89149527e-03\n",
      "Epoch: 7925 mean train loss:  1.09467426e-02, mean val. rec. loss:  9.89013336e-03\n",
      "Epoch: 7926 mean train loss:  1.09452743e-02, mean val. rec. loss:  9.88877259e-03\n",
      "Epoch: 7927 mean train loss:  1.09438180e-02, mean val. rec. loss:  9.88740615e-03\n",
      "Epoch: 7928 mean train loss:  1.09423450e-02, mean val. rec. loss:  9.88604878e-03\n",
      "Epoch: 7929 mean train loss:  1.09408870e-02, mean val. rec. loss:  9.88467780e-03\n",
      "Epoch: 7930 mean train loss:  1.09394121e-02, mean val. rec. loss:  9.88331589e-03\n",
      "Epoch: 7931 mean train loss:  1.09379521e-02, mean val. rec. loss:  9.88194378e-03\n",
      "Epoch: 7932 mean train loss:  1.09364754e-02, mean val. rec. loss:  9.88058301e-03\n",
      "Epoch: 7933 mean train loss:  1.09350155e-02, mean val. rec. loss:  9.87920863e-03\n",
      "Epoch: 7934 mean train loss:  1.09335369e-02, mean val. rec. loss:  9.87784673e-03\n",
      "Epoch: 7935 mean train loss:  1.09320732e-02, mean val. rec. loss:  9.87647348e-03\n",
      "Epoch: 7936 mean train loss:  1.09305928e-02, mean val. rec. loss:  9.87511157e-03\n",
      "Epoch: 7937 mean train loss:  1.09291300e-02, mean val. rec. loss:  9.87373152e-03\n",
      "Epoch: 7938 mean train loss:  1.09276514e-02, mean val. rec. loss:  9.87235374e-03\n",
      "Epoch: 7939 mean train loss:  1.09261729e-02, mean val. rec. loss:  9.87098503e-03\n",
      "Epoch: 7940 mean train loss:  1.09247008e-02, mean val. rec. loss:  9.86960612e-03\n",
      "Epoch: 7941 mean train loss:  1.09232297e-02, mean val. rec. loss:  9.86823287e-03\n",
      "Epoch: 7942 mean train loss:  1.09217483e-02, mean val. rec. loss:  9.86684602e-03\n",
      "Epoch: 7943 mean train loss:  1.09202660e-02, mean val. rec. loss:  9.86547164e-03\n",
      "Epoch: 7944 mean train loss:  1.09187921e-02, mean val. rec. loss:  9.86409272e-03\n",
      "Epoch: 7945 mean train loss:  1.09173163e-02, mean val. rec. loss:  9.86270814e-03\n",
      "Epoch: 7946 mean train loss:  1.09158312e-02, mean val. rec. loss:  9.86132809e-03\n",
      "Epoch: 7947 mean train loss:  1.09143452e-02, mean val. rec. loss:  9.85995144e-03\n",
      "Epoch: 7948 mean train loss:  1.09128656e-02, mean val. rec. loss:  9.85857366e-03\n",
      "Epoch: 7949 mean train loss:  1.09113880e-02, mean val. rec. loss:  9.85718794e-03\n",
      "Epoch: 7950 mean train loss:  1.09099029e-02, mean val. rec. loss:  9.85580109e-03\n",
      "Epoch: 7951 mean train loss:  1.09084187e-02, mean val. rec. loss:  9.85441310e-03\n",
      "Epoch: 7952 mean train loss:  1.09069327e-02, mean val. rec. loss:  9.85302284e-03\n",
      "Epoch: 7953 mean train loss:  1.09054439e-02, mean val. rec. loss:  9.85163145e-03\n",
      "Epoch: 7954 mean train loss:  1.09039579e-02, mean val. rec. loss:  9.85024007e-03\n",
      "Epoch: 7955 mean train loss:  1.09024690e-02, mean val. rec. loss:  9.84885094e-03\n",
      "Epoch: 7956 mean train loss:  1.09009802e-02, mean val. rec. loss:  9.84745729e-03\n",
      "Epoch: 7957 mean train loss:  1.08994886e-02, mean val. rec. loss:  9.84606703e-03\n",
      "Epoch: 7958 mean train loss:  1.08979979e-02, mean val. rec. loss:  9.84467678e-03\n",
      "Epoch: 7959 mean train loss:  1.08965054e-02, mean val. rec. loss:  9.84328199e-03\n",
      "Epoch: 7960 mean train loss:  1.08950119e-02, mean val. rec. loss:  9.84188946e-03\n",
      "Epoch: 7961 mean train loss:  1.08935184e-02, mean val. rec. loss:  9.84049467e-03\n",
      "Epoch: 7962 mean train loss:  1.08920231e-02, mean val. rec. loss:  9.83909648e-03\n",
      "Epoch: 7963 mean train loss:  1.08905277e-02, mean val. rec. loss:  9.83769261e-03\n",
      "Epoch: 7964 mean train loss:  1.08890259e-02, mean val. rec. loss:  9.83628762e-03\n",
      "Epoch: 7965 mean train loss:  1.08875240e-02, mean val. rec. loss:  9.83487922e-03\n",
      "Epoch: 7966 mean train loss:  1.08860296e-02, mean val. rec. loss:  9.83348216e-03\n",
      "Epoch: 7967 mean train loss:  1.08845352e-02, mean val. rec. loss:  9.83207603e-03\n",
      "Epoch: 7968 mean train loss:  1.08830306e-02, mean val. rec. loss:  9.83066763e-03\n",
      "Epoch: 7969 mean train loss:  1.08815259e-02, mean val. rec. loss:  9.82927057e-03\n",
      "Epoch: 7970 mean train loss:  1.08800268e-02, mean val. rec. loss:  9.82786104e-03\n",
      "Epoch: 7971 mean train loss:  1.08785287e-02, mean val. rec. loss:  9.82645377e-03\n",
      "Epoch: 7972 mean train loss:  1.08770213e-02, mean val. rec. loss:  9.82503857e-03\n",
      "Epoch: 7973 mean train loss:  1.08755120e-02, mean val. rec. loss:  9.82363244e-03\n",
      "Epoch: 7974 mean train loss:  1.08740110e-02, mean val. rec. loss:  9.82223084e-03\n",
      "Epoch: 7975 mean train loss:  1.08725092e-02, mean val. rec. loss:  9.82081791e-03\n",
      "Epoch: 7976 mean train loss:  1.08709971e-02, mean val. rec. loss:  9.81940271e-03\n",
      "Epoch: 7977 mean train loss:  1.08694841e-02, mean val. rec. loss:  9.81799998e-03\n",
      "Epoch: 7978 mean train loss:  1.08679794e-02, mean val. rec. loss:  9.81658137e-03\n",
      "Epoch: 7979 mean train loss:  1.08664748e-02, mean val. rec. loss:  9.81516730e-03\n",
      "Epoch: 7980 mean train loss:  1.08649599e-02, mean val. rec. loss:  9.81374530e-03\n",
      "Epoch: 7981 mean train loss:  1.08634422e-02, mean val. rec. loss:  9.81232669e-03\n",
      "Epoch: 7982 mean train loss:  1.08619357e-02, mean val. rec. loss:  9.81090922e-03\n",
      "Epoch: 7983 mean train loss:  1.08604198e-02, mean val. rec. loss:  9.80948948e-03\n",
      "Epoch: 7984 mean train loss:  1.08589068e-02, mean val. rec. loss:  9.80805840e-03\n",
      "Epoch: 7985 mean train loss:  1.08573919e-02, mean val. rec. loss:  9.80662959e-03\n",
      "Epoch: 7986 mean train loss:  1.08558752e-02, mean val. rec. loss:  9.80520872e-03\n",
      "Epoch: 7987 mean train loss:  1.08543565e-02, mean val. rec. loss:  9.80378785e-03\n",
      "Epoch: 7988 mean train loss:  1.08528389e-02, mean val. rec. loss:  9.80235904e-03\n",
      "Epoch: 7989 mean train loss:  1.08513193e-02, mean val. rec. loss:  9.80092909e-03\n",
      "Epoch: 7990 mean train loss:  1.08498016e-02, mean val. rec. loss:  9.79950595e-03\n",
      "Epoch: 7991 mean train loss:  1.08482784e-02, mean val. rec. loss:  9.79808395e-03\n",
      "Epoch: 7992 mean train loss:  1.08467560e-02, mean val. rec. loss:  9.79664947e-03\n",
      "Epoch: 7993 mean train loss:  1.08452346e-02, mean val. rec. loss:  9.79521725e-03\n",
      "Epoch: 7994 mean train loss:  1.08437123e-02, mean val. rec. loss:  9.79379071e-03\n",
      "Epoch: 7995 mean train loss:  1.08421871e-02, mean val. rec. loss:  9.79236417e-03\n",
      "Epoch: 7996 mean train loss:  1.08406601e-02, mean val. rec. loss:  9.79092402e-03\n",
      "Epoch: 7997 mean train loss:  1.08391350e-02, mean val. rec. loss:  9.78948273e-03\n",
      "Epoch: 7998 mean train loss:  1.08376080e-02, mean val. rec. loss:  9.78805166e-03\n",
      "Epoch: 7999 mean train loss:  1.08360801e-02, mean val. rec. loss:  9.78661944e-03\n",
      "Epoch: 8000 mean train loss:  1.08345494e-02, mean val. rec. loss:  9.78517362e-03\n",
      "Epoch: 8001 mean train loss:  1.08330205e-02, mean val. rec. loss:  9.78373007e-03\n",
      "Epoch: 8002 mean train loss:  1.08314888e-02, mean val. rec. loss:  9.78229446e-03\n",
      "Epoch: 8003 mean train loss:  1.08299581e-02, mean val. rec. loss:  9.78085544e-03\n",
      "Epoch: 8004 mean train loss:  1.08284237e-02, mean val. rec. loss:  9.77940622e-03\n",
      "Epoch: 8005 mean train loss:  1.08268911e-02, mean val. rec. loss:  9.77795586e-03\n",
      "Epoch: 8006 mean train loss:  1.08253567e-02, mean val. rec. loss:  9.77651798e-03\n",
      "Epoch: 8007 mean train loss:  1.08238222e-02, mean val. rec. loss:  9.77507783e-03\n",
      "Epoch: 8008 mean train loss:  1.08222859e-02, mean val. rec. loss:  9.77362407e-03\n",
      "Epoch: 8009 mean train loss:  1.08207477e-02, mean val. rec. loss:  9.77217258e-03\n",
      "Epoch: 8010 mean train loss:  1.08192096e-02, mean val. rec. loss:  9.77073243e-03\n",
      "Epoch: 8011 mean train loss:  1.08176714e-02, mean val. rec. loss:  9.76928548e-03\n",
      "Epoch: 8012 mean train loss:  1.08161304e-02, mean val. rec. loss:  9.76783285e-03\n",
      "Epoch: 8013 mean train loss:  1.08145904e-02, mean val. rec. loss:  9.76637569e-03\n",
      "Epoch: 8014 mean train loss:  1.08130476e-02, mean val. rec. loss:  9.76492647e-03\n",
      "Epoch: 8015 mean train loss:  1.08115047e-02, mean val. rec. loss:  9.76347725e-03\n",
      "Epoch: 8016 mean train loss:  1.08099619e-02, mean val. rec. loss:  9.76201669e-03\n",
      "Epoch: 8017 mean train loss:  1.08084182e-02, mean val. rec. loss:  9.76055159e-03\n",
      "Epoch: 8018 mean train loss:  1.08068707e-02, mean val. rec. loss:  9.75909783e-03\n",
      "Epoch: 8019 mean train loss:  1.08053251e-02, mean val. rec. loss:  9.75764634e-03\n",
      "Epoch: 8020 mean train loss:  1.08037767e-02, mean val. rec. loss:  9.75617897e-03\n",
      "Epoch: 8021 mean train loss:  1.08022301e-02, mean val. rec. loss:  9.75471161e-03\n",
      "Epoch: 8022 mean train loss:  1.08006808e-02, mean val. rec. loss:  9.75325558e-03\n",
      "Epoch: 8023 mean train loss:  1.07991296e-02, mean val. rec. loss:  9.75179729e-03\n",
      "Epoch: 8024 mean train loss:  1.07975784e-02, mean val. rec. loss:  9.75032652e-03\n",
      "Epoch: 8025 mean train loss:  1.07960262e-02, mean val. rec. loss:  9.74885349e-03\n",
      "Epoch: 8026 mean train loss:  1.07944741e-02, mean val. rec. loss:  9.74739292e-03\n",
      "Epoch: 8027 mean train loss:  1.07929182e-02, mean val. rec. loss:  9.74592783e-03\n",
      "Epoch: 8028 mean train loss:  1.07913633e-02, mean val. rec. loss:  9.74445366e-03\n",
      "Epoch: 8029 mean train loss:  1.07898074e-02, mean val. rec. loss:  9.74297835e-03\n",
      "Epoch: 8030 mean train loss:  1.07882525e-02, mean val. rec. loss:  9.74151099e-03\n",
      "Epoch: 8031 mean train loss:  1.07866939e-02, mean val. rec. loss:  9.74004702e-03\n",
      "Epoch: 8032 mean train loss:  1.07851361e-02, mean val. rec. loss:  9.73856832e-03\n",
      "Epoch: 8033 mean train loss:  1.07835765e-02, mean val. rec. loss:  9.73708508e-03\n",
      "Epoch: 8034 mean train loss:  1.07820160e-02, mean val. rec. loss:  9.73561771e-03\n",
      "Epoch: 8035 mean train loss:  1.07804555e-02, mean val. rec. loss:  9.73414127e-03\n",
      "Epoch: 8036 mean train loss:  1.07788913e-02, mean val. rec. loss:  9.73265576e-03\n",
      "Epoch: 8037 mean train loss:  1.07773289e-02, mean val. rec. loss:  9.73117026e-03\n",
      "Epoch: 8038 mean train loss:  1.07757647e-02, mean val. rec. loss:  9.72969268e-03\n",
      "Epoch: 8039 mean train loss:  1.07741986e-02, mean val. rec. loss:  9.72821738e-03\n",
      "Epoch: 8040 mean train loss:  1.07726325e-02, mean val. rec. loss:  9.72672733e-03\n",
      "Epoch: 8041 mean train loss:  1.07710664e-02, mean val. rec. loss:  9.72523729e-03\n",
      "Epoch: 8042 mean train loss:  1.07694993e-02, mean val. rec. loss:  9.72375858e-03\n",
      "Epoch: 8043 mean train loss:  1.07679295e-02, mean val. rec. loss:  9.72227421e-03\n",
      "Epoch: 8044 mean train loss:  1.07663587e-02, mean val. rec. loss:  9.72078190e-03\n",
      "Epoch: 8045 mean train loss:  1.07647889e-02, mean val. rec. loss:  9.71928618e-03\n",
      "Epoch: 8046 mean train loss:  1.07632172e-02, mean val. rec. loss:  9.71780067e-03\n",
      "Epoch: 8047 mean train loss:  1.07616437e-02, mean val. rec. loss:  9.71631289e-03\n",
      "Epoch: 8048 mean train loss:  1.07600692e-02, mean val. rec. loss:  9.71481151e-03\n",
      "Epoch: 8049 mean train loss:  1.07584947e-02, mean val. rec. loss:  9.71331466e-03\n",
      "Epoch: 8050 mean train loss:  1.07569202e-02, mean val. rec. loss:  9.71182235e-03\n",
      "Epoch: 8051 mean train loss:  1.07553429e-02, mean val. rec. loss:  9.71033003e-03\n",
      "Epoch: 8052 mean train loss:  1.07537647e-02, mean val. rec. loss:  9.70882638e-03\n",
      "Epoch: 8053 mean train loss:  1.07521856e-02, mean val. rec. loss:  9.70732386e-03\n",
      "Epoch: 8054 mean train loss:  1.07506083e-02, mean val. rec. loss:  9.70582815e-03\n",
      "Epoch: 8055 mean train loss:  1.07490264e-02, mean val. rec. loss:  9.70433130e-03\n",
      "Epoch: 8056 mean train loss:  1.07474445e-02, mean val. rec. loss:  9.70281857e-03\n",
      "Epoch: 8057 mean train loss:  1.07458635e-02, mean val. rec. loss:  9.70130698e-03\n",
      "Epoch: 8058 mean train loss:  1.07442797e-02, mean val. rec. loss:  9.69980446e-03\n",
      "Epoch: 8059 mean train loss:  1.07426940e-02, mean val. rec. loss:  9.69830421e-03\n",
      "Epoch: 8060 mean train loss:  1.07411102e-02, mean val. rec. loss:  9.69678582e-03\n",
      "Epoch: 8061 mean train loss:  1.07395227e-02, mean val. rec. loss:  9.69527763e-03\n",
      "Epoch: 8062 mean train loss:  1.07379324e-02, mean val. rec. loss:  9.69376944e-03\n",
      "Epoch: 8063 mean train loss:  1.07363477e-02, mean val. rec. loss:  9.69226805e-03\n",
      "Epoch: 8064 mean train loss:  1.07347629e-02, mean val. rec. loss:  9.69075873e-03\n",
      "Epoch: 8065 mean train loss:  1.07331680e-02, mean val. rec. loss:  9.68924260e-03\n",
      "Epoch: 8066 mean train loss:  1.07315721e-02, mean val. rec. loss:  9.68773895e-03\n",
      "Epoch: 8067 mean train loss:  1.07299846e-02, mean val. rec. loss:  9.68621829e-03\n",
      "Epoch: 8068 mean train loss:  1.07283980e-02, mean val. rec. loss:  9.68469989e-03\n",
      "Epoch: 8069 mean train loss:  1.07267984e-02, mean val. rec. loss:  9.68317129e-03\n",
      "Epoch: 8070 mean train loss:  1.07251978e-02, mean val. rec. loss:  9.68164950e-03\n",
      "Epoch: 8071 mean train loss:  1.07236084e-02, mean val. rec. loss:  9.68013337e-03\n",
      "Epoch: 8072 mean train loss:  1.07220144e-02, mean val. rec. loss:  9.67860590e-03\n",
      "Epoch: 8073 mean train loss:  1.07204120e-02, mean val. rec. loss:  9.67707617e-03\n",
      "Epoch: 8074 mean train loss:  1.07188077e-02, mean val. rec. loss:  9.67556344e-03\n",
      "Epoch: 8075 mean train loss:  1.07172127e-02, mean val. rec. loss:  9.67403144e-03\n",
      "Epoch: 8076 mean train loss:  1.07156150e-02, mean val. rec. loss:  9.67250965e-03\n",
      "Epoch: 8077 mean train loss:  1.07140154e-02, mean val. rec. loss:  9.67098558e-03\n",
      "Epoch: 8078 mean train loss:  1.07124120e-02, mean val. rec. loss:  9.66945925e-03\n",
      "Epoch: 8079 mean train loss:  1.07108068e-02, mean val. rec. loss:  9.66793065e-03\n",
      "Epoch: 8080 mean train loss:  1.07092025e-02, mean val. rec. loss:  9.66640205e-03\n",
      "Epoch: 8081 mean train loss:  1.07075954e-02, mean val. rec. loss:  9.66487118e-03\n",
      "Epoch: 8082 mean train loss:  1.07059902e-02, mean val. rec. loss:  9.66333691e-03\n",
      "Epoch: 8083 mean train loss:  1.07043813e-02, mean val. rec. loss:  9.66179584e-03\n",
      "Epoch: 8084 mean train loss:  1.07027724e-02, mean val. rec. loss:  9.66025816e-03\n",
      "Epoch: 8085 mean train loss:  1.07011634e-02, mean val. rec. loss:  9.65871709e-03\n",
      "Epoch: 8086 mean train loss:  1.06995517e-02, mean val. rec. loss:  9.65717375e-03\n",
      "Epoch: 8087 mean train loss:  1.06979391e-02, mean val. rec. loss:  9.65562927e-03\n",
      "Epoch: 8088 mean train loss:  1.06963264e-02, mean val. rec. loss:  9.65409613e-03\n",
      "Epoch: 8089 mean train loss:  1.06947082e-02, mean val. rec. loss:  9.65254032e-03\n",
      "Epoch: 8090 mean train loss:  1.06930890e-02, mean val. rec. loss:  9.65100491e-03\n",
      "Epoch: 8091 mean train loss:  1.06914782e-02, mean val. rec. loss:  9.64946384e-03\n",
      "Epoch: 8092 mean train loss:  1.06898655e-02, mean val. rec. loss:  9.64791143e-03\n",
      "Epoch: 8093 mean train loss:  1.06882436e-02, mean val. rec. loss:  9.64636241e-03\n",
      "Epoch: 8094 mean train loss:  1.06866197e-02, mean val. rec. loss:  9.64481567e-03\n",
      "Epoch: 8095 mean train loss:  1.06850034e-02, mean val. rec. loss:  9.64326439e-03\n",
      "Epoch: 8096 mean train loss:  1.06833870e-02, mean val. rec. loss:  9.64171311e-03\n",
      "Epoch: 8097 mean train loss:  1.06817603e-02, mean val. rec. loss:  9.64014595e-03\n",
      "Epoch: 8098 mean train loss:  1.06801328e-02, mean val. rec. loss:  9.63859921e-03\n",
      "Epoch: 8099 mean train loss:  1.06785183e-02, mean val. rec. loss:  9.63704113e-03\n",
      "Epoch: 8100 mean train loss:  1.06768842e-02, mean val. rec. loss:  9.63549211e-03\n",
      "Epoch: 8101 mean train loss:  1.06752660e-02, mean val. rec. loss:  9.63392949e-03\n",
      "Epoch: 8102 mean train loss:  1.06736310e-02, mean val. rec. loss:  9.63237368e-03\n",
      "Epoch: 8103 mean train loss:  1.06720118e-02, mean val. rec. loss:  9.63080652e-03\n",
      "Epoch: 8104 mean train loss:  1.06703731e-02, mean val. rec. loss:  9.62924844e-03\n",
      "Epoch: 8105 mean train loss:  1.06687520e-02, mean val. rec. loss:  9.62767561e-03\n",
      "Epoch: 8106 mean train loss:  1.06671124e-02, mean val. rec. loss:  9.62611186e-03\n",
      "Epoch: 8107 mean train loss:  1.06654876e-02, mean val. rec. loss:  9.62454697e-03\n",
      "Epoch: 8108 mean train loss:  1.06638526e-02, mean val. rec. loss:  9.62297415e-03\n",
      "Epoch: 8109 mean train loss:  1.06622120e-02, mean val. rec. loss:  9.62140472e-03\n",
      "Epoch: 8110 mean train loss:  1.06605807e-02, mean val. rec. loss:  9.61984097e-03\n",
      "Epoch: 8111 mean train loss:  1.06589476e-02, mean val. rec. loss:  9.61826928e-03\n",
      "Epoch: 8112 mean train loss:  1.06573070e-02, mean val. rec. loss:  9.61669078e-03\n",
      "Epoch: 8113 mean train loss:  1.06556636e-02, mean val. rec. loss:  9.61512363e-03\n",
      "Epoch: 8114 mean train loss:  1.06540295e-02, mean val. rec. loss:  9.61354059e-03\n",
      "Epoch: 8115 mean train loss:  1.06523927e-02, mean val. rec. loss:  9.61196097e-03\n",
      "Epoch: 8116 mean train loss:  1.06507502e-02, mean val. rec. loss:  9.61038474e-03\n",
      "Epoch: 8117 mean train loss:  1.06491078e-02, mean val. rec. loss:  9.60880738e-03\n",
      "Epoch: 8118 mean train loss:  1.06474644e-02, mean val. rec. loss:  9.60722548e-03\n",
      "Epoch: 8119 mean train loss:  1.06458191e-02, mean val. rec. loss:  9.60564358e-03\n",
      "Epoch: 8120 mean train loss:  1.06441730e-02, mean val. rec. loss:  9.60406055e-03\n",
      "Epoch: 8121 mean train loss:  1.06425259e-02, mean val. rec. loss:  9.60247412e-03\n",
      "Epoch: 8122 mean train loss:  1.06408787e-02, mean val. rec. loss:  9.60088315e-03\n",
      "Epoch: 8123 mean train loss:  1.06392251e-02, mean val. rec. loss:  9.59929785e-03\n",
      "Epoch: 8124 mean train loss:  1.06375706e-02, mean val. rec. loss:  9.59771028e-03\n",
      "Epoch: 8125 mean train loss:  1.06359225e-02, mean val. rec. loss:  9.59611818e-03\n",
      "Epoch: 8126 mean train loss:  1.06342754e-02, mean val. rec. loss:  9.59452834e-03\n",
      "Epoch: 8127 mean train loss:  1.06326162e-02, mean val. rec. loss:  9.59292150e-03\n",
      "Epoch: 8128 mean train loss:  1.06309589e-02, mean val. rec. loss:  9.59133280e-03\n",
      "Epoch: 8129 mean train loss:  1.06293108e-02, mean val. rec. loss:  9.58973162e-03\n",
      "Epoch: 8130 mean train loss:  1.06276460e-02, mean val. rec. loss:  9.58813952e-03\n",
      "Epoch: 8131 mean train loss:  1.06259961e-02, mean val. rec. loss:  9.58653381e-03\n",
      "Epoch: 8132 mean train loss:  1.06243295e-02, mean val. rec. loss:  9.58493717e-03\n",
      "Epoch: 8133 mean train loss:  1.06226777e-02, mean val. rec. loss:  9.58332806e-03\n",
      "Epoch: 8134 mean train loss:  1.06210092e-02, mean val. rec. loss:  9.58173482e-03\n",
      "Epoch: 8135 mean train loss:  1.06193518e-02, mean val. rec. loss:  9.58012684e-03\n",
      "Epoch: 8136 mean train loss:  1.06176908e-02, mean val. rec. loss:  9.57851886e-03\n",
      "Epoch: 8137 mean train loss:  1.06160241e-02, mean val. rec. loss:  9.57690975e-03\n",
      "Epoch: 8138 mean train loss:  1.06143537e-02, mean val. rec. loss:  9.57529837e-03\n",
      "Epoch: 8139 mean train loss:  1.06126926e-02, mean val. rec. loss:  9.57369266e-03\n",
      "Epoch: 8140 mean train loss:  1.06110288e-02, mean val. rec. loss:  9.57208241e-03\n",
      "Epoch: 8141 mean train loss:  1.06093593e-02, mean val. rec. loss:  9.57046990e-03\n",
      "Epoch: 8142 mean train loss:  1.06076889e-02, mean val. rec. loss:  9.56885511e-03\n",
      "Epoch: 8143 mean train loss:  1.06060176e-02, mean val. rec. loss:  9.56723353e-03\n",
      "Epoch: 8144 mean train loss:  1.06043463e-02, mean val. rec. loss:  9.56561761e-03\n",
      "Epoch: 8145 mean train loss:  1.06026740e-02, mean val. rec. loss:  9.56399489e-03\n",
      "Epoch: 8146 mean train loss:  1.06009953e-02, mean val. rec. loss:  9.56237217e-03\n",
      "Epoch: 8147 mean train loss:  1.05993156e-02, mean val. rec. loss:  9.56075398e-03\n",
      "Epoch: 8148 mean train loss:  1.05976424e-02, mean val. rec. loss:  9.55913013e-03\n",
      "Epoch: 8149 mean train loss:  1.05959702e-02, mean val. rec. loss:  9.55750514e-03\n",
      "Epoch: 8150 mean train loss:  1.05942849e-02, mean val. rec. loss:  9.55587675e-03\n",
      "Epoch: 8151 mean train loss:  1.05926061e-02, mean val. rec. loss:  9.55424496e-03\n",
      "Epoch: 8152 mean train loss:  1.05909255e-02, mean val. rec. loss:  9.55261657e-03\n",
      "Epoch: 8153 mean train loss:  1.05892430e-02, mean val. rec. loss:  9.55099271e-03\n",
      "Epoch: 8154 mean train loss:  1.05875596e-02, mean val. rec. loss:  9.54934958e-03\n",
      "Epoch: 8155 mean train loss:  1.05858752e-02, mean val. rec. loss:  9.54771325e-03\n",
      "Epoch: 8156 mean train loss:  1.05841844e-02, mean val. rec. loss:  9.54609053e-03\n",
      "Epoch: 8157 mean train loss:  1.05825028e-02, mean val. rec. loss:  9.54444740e-03\n",
      "Epoch: 8158 mean train loss:  1.05808194e-02, mean val. rec. loss:  9.54280994e-03\n",
      "Epoch: 8159 mean train loss:  1.05791248e-02, mean val. rec. loss:  9.54116227e-03\n",
      "Epoch: 8160 mean train loss:  1.05774311e-02, mean val. rec. loss:  9.53952141e-03\n",
      "Epoch: 8161 mean train loss:  1.05757384e-02, mean val. rec. loss:  9.53788054e-03\n",
      "Epoch: 8162 mean train loss:  1.05740457e-02, mean val. rec. loss:  9.53623514e-03\n",
      "Epoch: 8163 mean train loss:  1.05723520e-02, mean val. rec. loss:  9.53458634e-03\n",
      "Epoch: 8164 mean train loss:  1.05706574e-02, mean val. rec. loss:  9.53294548e-03\n",
      "Epoch: 8165 mean train loss:  1.05689665e-02, mean val. rec. loss:  9.53129781e-03\n",
      "Epoch: 8166 mean train loss:  1.05672719e-02, mean val. rec. loss:  9.52965127e-03\n",
      "Epoch: 8167 mean train loss:  1.05655699e-02, mean val. rec. loss:  9.52798773e-03\n",
      "Epoch: 8168 mean train loss:  1.05638651e-02, mean val. rec. loss:  9.52633553e-03\n",
      "Epoch: 8169 mean train loss:  1.05621677e-02, mean val. rec. loss:  9.52468672e-03\n",
      "Epoch: 8170 mean train loss:  1.05604675e-02, mean val. rec. loss:  9.52302205e-03\n",
      "Epoch: 8171 mean train loss:  1.05587645e-02, mean val. rec. loss:  9.52135510e-03\n",
      "Epoch: 8172 mean train loss:  1.05570588e-02, mean val. rec. loss:  9.51969836e-03\n",
      "Epoch: 8173 mean train loss:  1.05553521e-02, mean val. rec. loss:  9.51803028e-03\n",
      "Epoch: 8174 mean train loss:  1.05536416e-02, mean val. rec. loss:  9.51637581e-03\n",
      "Epoch: 8175 mean train loss:  1.05519377e-02, mean val. rec. loss:  9.51470546e-03\n",
      "Epoch: 8176 mean train loss:  1.05502329e-02, mean val. rec. loss:  9.51303738e-03\n",
      "Epoch: 8177 mean train loss:  1.05485169e-02, mean val. rec. loss:  9.51137044e-03\n",
      "Epoch: 8178 mean train loss:  1.05468065e-02, mean val. rec. loss:  9.50970066e-03\n",
      "Epoch: 8179 mean train loss:  1.05450942e-02, mean val. rec. loss:  9.50802861e-03\n",
      "Epoch: 8180 mean train loss:  1.05433782e-02, mean val. rec. loss:  9.50635883e-03\n",
      "Epoch: 8181 mean train loss:  1.05416687e-02, mean val. rec. loss:  9.50468621e-03\n",
      "Epoch: 8182 mean train loss:  1.05399462e-02, mean val. rec. loss:  9.50299942e-03\n",
      "Epoch: 8183 mean train loss:  1.05382246e-02, mean val. rec. loss:  9.50132454e-03\n",
      "Epoch: 8184 mean train loss:  1.05365086e-02, mean val. rec. loss:  9.49964058e-03\n",
      "Epoch: 8185 mean train loss:  1.05347907e-02, mean val. rec. loss:  9.49796513e-03\n",
      "Epoch: 8186 mean train loss:  1.05330672e-02, mean val. rec. loss:  9.49628571e-03\n",
      "Epoch: 8187 mean train loss:  1.05313447e-02, mean val. rec. loss:  9.49459269e-03\n",
      "Epoch: 8188 mean train loss:  1.05296194e-02, mean val. rec. loss:  9.49290136e-03\n",
      "Epoch: 8189 mean train loss:  1.05278903e-02, mean val. rec. loss:  9.49122534e-03\n",
      "Epoch: 8190 mean train loss:  1.05261678e-02, mean val. rec. loss:  9.48952835e-03\n",
      "Epoch: 8191 mean train loss:  1.05244453e-02, mean val. rec. loss:  9.48784269e-03\n",
      "Epoch: 8192 mean train loss:  1.05227153e-02, mean val. rec. loss:  9.48614966e-03\n",
      "Epoch: 8193 mean train loss:  1.05209853e-02, mean val. rec. loss:  9.48445664e-03\n",
      "Epoch: 8194 mean train loss:  1.05192535e-02, mean val. rec. loss:  9.48275624e-03\n",
      "Epoch: 8195 mean train loss:  1.05175161e-02, mean val. rec. loss:  9.48105924e-03\n",
      "Epoch: 8196 mean train loss:  1.05157768e-02, mean val. rec. loss:  9.47936281e-03\n",
      "Epoch: 8197 mean train loss:  1.05140440e-02, mean val. rec. loss:  9.47766072e-03\n",
      "Epoch: 8198 mean train loss:  1.05123131e-02, mean val. rec. loss:  9.47595181e-03\n",
      "Epoch: 8199 mean train loss:  1.05105664e-02, mean val. rec. loss:  9.47425085e-03\n",
      "Epoch: 8200 mean train loss:  1.05088355e-02, mean val. rec. loss:  9.47253571e-03\n",
      "Epoch: 8201 mean train loss:  1.05070841e-02, mean val. rec. loss:  9.47082737e-03\n",
      "Epoch: 8202 mean train loss:  1.05053467e-02, mean val. rec. loss:  9.46912074e-03\n",
      "Epoch: 8203 mean train loss:  1.05036083e-02, mean val. rec. loss:  9.46740730e-03\n",
      "Epoch: 8204 mean train loss:  1.05018606e-02, mean val. rec. loss:  9.46569613e-03\n",
      "Epoch: 8205 mean train loss:  1.05001139e-02, mean val. rec. loss:  9.46398042e-03\n",
      "Epoch: 8206 mean train loss:  1.04983662e-02, mean val. rec. loss:  9.46226131e-03\n",
      "Epoch: 8207 mean train loss:  1.04966167e-02, mean val. rec. loss:  9.46054334e-03\n",
      "Epoch: 8208 mean train loss:  1.04948709e-02, mean val. rec. loss:  9.45881459e-03\n",
      "Epoch: 8209 mean train loss:  1.04931158e-02, mean val. rec. loss:  9.45709038e-03\n",
      "Epoch: 8210 mean train loss:  1.04913579e-02, mean val. rec. loss:  9.45537241e-03\n",
      "Epoch: 8211 mean train loss:  1.04896130e-02, mean val. rec. loss:  9.45364423e-03\n",
      "Epoch: 8212 mean train loss:  1.04878477e-02, mean val. rec. loss:  9.45191945e-03\n",
      "Epoch: 8213 mean train loss:  1.04861000e-02, mean val. rec. loss:  9.45018957e-03\n",
      "Epoch: 8214 mean train loss:  1.04843365e-02, mean val. rec. loss:  9.44845288e-03\n",
      "Epoch: 8215 mean train loss:  1.04825721e-02, mean val. rec. loss:  9.44671790e-03\n",
      "Epoch: 8216 mean train loss:  1.04808179e-02, mean val. rec. loss:  9.44499312e-03\n",
      "Epoch: 8217 mean train loss:  1.04790544e-02, mean val. rec. loss:  9.44324566e-03\n",
      "Epoch: 8218 mean train loss:  1.04772918e-02, mean val. rec. loss:  9.44151351e-03\n",
      "Epoch: 8219 mean train loss:  1.04755255e-02, mean val. rec. loss:  9.43976435e-03\n",
      "Epoch: 8220 mean train loss:  1.04737546e-02, mean val. rec. loss:  9.43802597e-03\n",
      "Epoch: 8221 mean train loss:  1.04719911e-02, mean val. rec. loss:  9.43627964e-03\n",
      "Epoch: 8222 mean train loss:  1.04702257e-02, mean val. rec. loss:  9.43452651e-03\n",
      "Epoch: 8223 mean train loss:  1.04684455e-02, mean val. rec. loss:  9.43278132e-03\n",
      "Epoch: 8224 mean train loss:  1.04666820e-02, mean val. rec. loss:  9.43103387e-03\n",
      "Epoch: 8225 mean train loss:  1.04649045e-02, mean val. rec. loss:  9.42927734e-03\n",
      "Epoch: 8226 mean train loss:  1.04631243e-02, mean val. rec. loss:  9.42753101e-03\n",
      "Epoch: 8227 mean train loss:  1.04613524e-02, mean val. rec. loss:  9.42577788e-03\n",
      "Epoch: 8228 mean train loss:  1.04595740e-02, mean val. rec. loss:  9.42401795e-03\n",
      "Epoch: 8229 mean train loss:  1.04577947e-02, mean val. rec. loss:  9.42225689e-03\n",
      "Epoch: 8230 mean train loss:  1.04560144e-02, mean val. rec. loss:  9.42048675e-03\n",
      "Epoch: 8231 mean train loss:  1.04542277e-02, mean val. rec. loss:  9.41872568e-03\n",
      "Epoch: 8232 mean train loss:  1.04524474e-02, mean val. rec. loss:  9.41695555e-03\n",
      "Epoch: 8233 mean train loss:  1.04506634e-02, mean val. rec. loss:  9.41519165e-03\n",
      "Epoch: 8234 mean train loss:  1.04488748e-02, mean val. rec. loss:  9.41342434e-03\n",
      "Epoch: 8235 mean train loss:  1.04470880e-02, mean val. rec. loss:  9.41164910e-03\n",
      "Epoch: 8236 mean train loss:  1.04453022e-02, mean val. rec. loss:  9.40987386e-03\n",
      "Epoch: 8237 mean train loss:  1.04435061e-02, mean val. rec. loss:  9.40809749e-03\n",
      "Epoch: 8238 mean train loss:  1.04417156e-02, mean val. rec. loss:  9.40631941e-03\n",
      "Epoch: 8239 mean train loss:  1.04399205e-02, mean val. rec. loss:  9.40454190e-03\n",
      "Epoch: 8240 mean train loss:  1.04381281e-02, mean val. rec. loss:  9.40276383e-03\n",
      "Epoch: 8241 mean train loss:  1.04363264e-02, mean val. rec. loss:  9.40096874e-03\n",
      "Epoch: 8242 mean train loss:  1.04345229e-02, mean val. rec. loss:  9.39918840e-03\n",
      "Epoch: 8243 mean train loss:  1.04327324e-02, mean val. rec. loss:  9.39739558e-03\n",
      "Epoch: 8244 mean train loss:  1.04309205e-02, mean val. rec. loss:  9.39561751e-03\n",
      "Epoch: 8245 mean train loss:  1.04291225e-02, mean val. rec. loss:  9.39381959e-03\n",
      "Epoch: 8246 mean train loss:  1.04273209e-02, mean val. rec. loss:  9.39202677e-03\n",
      "Epoch: 8247 mean train loss:  1.04255164e-02, mean val. rec. loss:  9.39023395e-03\n",
      "Epoch: 8248 mean train loss:  1.04237082e-02, mean val. rec. loss:  9.38843150e-03\n",
      "Epoch: 8249 mean train loss:  1.04218972e-02, mean val. rec. loss:  9.38663528e-03\n",
      "Epoch: 8250 mean train loss:  1.04200816e-02, mean val. rec. loss:  9.38482148e-03\n",
      "Epoch: 8251 mean train loss:  1.04182650e-02, mean val. rec. loss:  9.38302300e-03\n",
      "Epoch: 8252 mean train loss:  1.04164615e-02, mean val. rec. loss:  9.38120864e-03\n",
      "Epoch: 8253 mean train loss:  1.04146347e-02, mean val. rec. loss:  9.37941185e-03\n",
      "Epoch: 8254 mean train loss:  1.04128219e-02, mean val. rec. loss:  9.37759635e-03\n",
      "Epoch: 8255 mean train loss:  1.04110062e-02, mean val. rec. loss:  9.37578483e-03\n",
      "Epoch: 8256 mean train loss:  1.04091869e-02, mean val. rec. loss:  9.37397443e-03\n",
      "Epoch: 8257 mean train loss:  1.04073647e-02, mean val. rec. loss:  9.37216120e-03\n",
      "Epoch: 8258 mean train loss:  1.04055351e-02, mean val. rec. loss:  9.37033210e-03\n",
      "Epoch: 8259 mean train loss:  1.04037055e-02, mean val. rec. loss:  9.36851660e-03\n",
      "Epoch: 8260 mean train loss:  1.04018862e-02, mean val. rec. loss:  9.36668636e-03\n",
      "Epoch: 8261 mean train loss:  1.04000482e-02, mean val. rec. loss:  9.36486236e-03\n",
      "Epoch: 8262 mean train loss:  1.03982279e-02, mean val. rec. loss:  9.36302816e-03\n",
      "Epoch: 8263 mean train loss:  1.03963899e-02, mean val. rec. loss:  9.36119565e-03\n",
      "Epoch: 8264 mean train loss:  1.03945528e-02, mean val. rec. loss:  9.35935861e-03\n",
      "Epoch: 8265 mean train loss:  1.03927158e-02, mean val. rec. loss:  9.35752610e-03\n",
      "Epoch: 8266 mean train loss:  1.03908797e-02, mean val. rec. loss:  9.35569643e-03\n",
      "Epoch: 8267 mean train loss:  1.03890454e-02, mean val. rec. loss:  9.35385428e-03\n",
      "Epoch: 8268 mean train loss:  1.03872074e-02, mean val. rec. loss:  9.35200930e-03\n",
      "Epoch: 8269 mean train loss:  1.03853573e-02, mean val. rec. loss:  9.35016773e-03\n",
      "Epoch: 8270 mean train loss:  1.03835166e-02, mean val. rec. loss:  9.34832898e-03\n",
      "Epoch: 8271 mean train loss:  1.03816767e-02, mean val. rec. loss:  9.34648344e-03\n",
      "Epoch: 8272 mean train loss:  1.03798276e-02, mean val. rec. loss:  9.34463109e-03\n",
      "Epoch: 8273 mean train loss:  1.03779784e-02, mean val. rec. loss:  9.34277250e-03\n",
      "Epoch: 8274 mean train loss:  1.03761227e-02, mean val. rec. loss:  9.34091901e-03\n",
      "Epoch: 8275 mean train loss:  1.03742661e-02, mean val. rec. loss:  9.33906723e-03\n",
      "Epoch: 8276 mean train loss:  1.03724207e-02, mean val. rec. loss:  9.33719844e-03\n",
      "Epoch: 8277 mean train loss:  1.03705548e-02, mean val. rec. loss:  9.33533418e-03\n",
      "Epoch: 8278 mean train loss:  1.03687019e-02, mean val. rec. loss:  9.33347219e-03\n",
      "Epoch: 8279 mean train loss:  1.03668453e-02, mean val. rec. loss:  9.33160566e-03\n",
      "Epoch: 8280 mean train loss:  1.03649841e-02, mean val. rec. loss:  9.32973574e-03\n",
      "Epoch: 8281 mean train loss:  1.03631219e-02, mean val. rec. loss:  9.32785730e-03\n",
      "Epoch: 8282 mean train loss:  1.03612504e-02, mean val. rec. loss:  9.32598171e-03\n",
      "Epoch: 8283 mean train loss:  1.03593779e-02, mean val. rec. loss:  9.32411121e-03\n",
      "Epoch: 8284 mean train loss:  1.03575176e-02, mean val. rec. loss:  9.32223335e-03\n",
      "Epoch: 8285 mean train loss:  1.03556414e-02, mean val. rec. loss:  9.32034868e-03\n",
      "Epoch: 8286 mean train loss:  1.03537662e-02, mean val. rec. loss:  9.31846231e-03\n",
      "Epoch: 8287 mean train loss:  1.03518919e-02, mean val. rec. loss:  9.31657424e-03\n",
      "Epoch: 8288 mean train loss:  1.03500139e-02, mean val. rec. loss:  9.31469297e-03\n",
      "Epoch: 8289 mean train loss:  1.03481424e-02, mean val. rec. loss:  9.31279810e-03\n",
      "Epoch: 8290 mean train loss:  1.03462653e-02, mean val. rec. loss:  9.31089982e-03\n",
      "Epoch: 8291 mean train loss:  1.03443743e-02, mean val. rec. loss:  9.30900154e-03\n",
      "Epoch: 8292 mean train loss:  1.03424953e-02, mean val. rec. loss:  9.30710270e-03\n",
      "Epoch: 8293 mean train loss:  1.03406126e-02, mean val. rec. loss:  9.30520215e-03\n",
      "Epoch: 8294 mean train loss:  1.03387244e-02, mean val. rec. loss:  9.30329367e-03\n",
      "Epoch: 8295 mean train loss:  1.03368352e-02, mean val. rec. loss:  9.30138405e-03\n",
      "Epoch: 8296 mean train loss:  1.03349395e-02, mean val. rec. loss:  9.29948238e-03\n",
      "Epoch: 8297 mean train loss:  1.03330447e-02, mean val. rec. loss:  9.29757616e-03\n",
      "Epoch: 8298 mean train loss:  1.03311481e-02, mean val. rec. loss:  9.29565861e-03\n",
      "Epoch: 8299 mean train loss:  1.03292579e-02, mean val. rec. loss:  9.29373765e-03\n",
      "Epoch: 8300 mean train loss:  1.03273501e-02, mean val. rec. loss:  9.29182066e-03\n",
      "Epoch: 8301 mean train loss:  1.03254497e-02, mean val. rec. loss:  9.28990141e-03\n",
      "Epoch: 8302 mean train loss:  1.03235512e-02, mean val. rec. loss:  9.28796798e-03\n",
      "Epoch: 8303 mean train loss:  1.03216406e-02, mean val. rec. loss:  9.28604702e-03\n",
      "Epoch: 8304 mean train loss:  1.03197328e-02, mean val. rec. loss:  9.28410905e-03\n",
      "Epoch: 8305 mean train loss:  1.03178185e-02, mean val. rec. loss:  9.28218696e-03\n",
      "Epoch: 8306 mean train loss:  1.03159097e-02, mean val. rec. loss:  9.28025013e-03\n",
      "Epoch: 8307 mean train loss:  1.03139973e-02, mean val. rec. loss:  9.27830139e-03\n",
      "Epoch: 8308 mean train loss:  1.03120755e-02, mean val. rec. loss:  9.27636626e-03\n",
      "Epoch: 8309 mean train loss:  1.03101630e-02, mean val. rec. loss:  9.27441639e-03\n",
      "Epoch: 8310 mean train loss:  1.03082431e-02, mean val. rec. loss:  9.27247445e-03\n",
      "Epoch: 8311 mean train loss:  1.03063213e-02, mean val. rec. loss:  9.27051891e-03\n",
      "Epoch: 8312 mean train loss:  1.03043921e-02, mean val. rec. loss:  9.26856564e-03\n",
      "Epoch: 8313 mean train loss:  1.03024647e-02, mean val. rec. loss:  9.26661860e-03\n",
      "Epoch: 8314 mean train loss:  1.03005392e-02, mean val. rec. loss:  9.26466249e-03\n",
      "Epoch: 8315 mean train loss:  1.02986147e-02, mean val. rec. loss:  9.26269391e-03\n",
      "Epoch: 8316 mean train loss:  1.02966715e-02, mean val. rec. loss:  9.26073156e-03\n",
      "Epoch: 8317 mean train loss:  1.02947422e-02, mean val. rec. loss:  9.25877035e-03\n",
      "Epoch: 8318 mean train loss:  1.02928093e-02, mean val. rec. loss:  9.25680233e-03\n",
      "Epoch: 8319 mean train loss:  1.02908689e-02, mean val. rec. loss:  9.25482638e-03\n",
      "Epoch: 8320 mean train loss:  1.02889229e-02, mean val. rec. loss:  9.25284872e-03\n",
      "Epoch: 8321 mean train loss:  1.02869750e-02, mean val. rec. loss:  9.25087674e-03\n",
      "Epoch: 8322 mean train loss:  1.02850384e-02, mean val. rec. loss:  9.24889228e-03\n",
      "Epoch: 8323 mean train loss:  1.02830849e-02, mean val. rec. loss:  9.24690101e-03\n",
      "Epoch: 8324 mean train loss:  1.02811287e-02, mean val. rec. loss:  9.24491372e-03\n",
      "Epoch: 8325 mean train loss:  1.02791762e-02, mean val. rec. loss:  9.24293210e-03\n",
      "Epoch: 8326 mean train loss:  1.02772265e-02, mean val. rec. loss:  9.24093176e-03\n",
      "Epoch: 8327 mean train loss:  1.02752684e-02, mean val. rec. loss:  9.23893880e-03\n",
      "Epoch: 8328 mean train loss:  1.02733075e-02, mean val. rec. loss:  9.23693336e-03\n",
      "Epoch: 8329 mean train loss:  1.02713419e-02, mean val. rec. loss:  9.23493132e-03\n",
      "Epoch: 8330 mean train loss:  1.02693755e-02, mean val. rec. loss:  9.23293439e-03\n",
      "Epoch: 8331 mean train loss:  1.02674146e-02, mean val. rec. loss:  9.23092328e-03\n",
      "Epoch: 8332 mean train loss:  1.02654490e-02, mean val. rec. loss:  9.22890594e-03\n",
      "Epoch: 8333 mean train loss:  1.02634686e-02, mean val. rec. loss:  9.22690164e-03\n",
      "Epoch: 8334 mean train loss:  1.02614984e-02, mean val. rec. loss:  9.22487295e-03\n",
      "Epoch: 8335 mean train loss:  1.02595226e-02, mean val. rec. loss:  9.22286184e-03\n",
      "Epoch: 8336 mean train loss:  1.02575431e-02, mean val. rec. loss:  9.22082976e-03\n",
      "Epoch: 8337 mean train loss:  1.02555580e-02, mean val. rec. loss:  9.21880674e-03\n",
      "Epoch: 8338 mean train loss:  1.02535822e-02, mean val. rec. loss:  9.21678033e-03\n",
      "Epoch: 8339 mean train loss:  1.02515925e-02, mean val. rec. loss:  9.21475108e-03\n",
      "Epoch: 8340 mean train loss:  1.02496055e-02, mean val. rec. loss:  9.21270992e-03\n",
      "Epoch: 8341 mean train loss:  1.02476139e-02, mean val. rec. loss:  9.21067273e-03\n",
      "Epoch: 8342 mean train loss:  1.02456251e-02, mean val. rec. loss:  9.20862420e-03\n",
      "Epoch: 8343 mean train loss:  1.02436186e-02, mean val. rec. loss:  9.20659155e-03\n",
      "Epoch: 8344 mean train loss:  1.02416232e-02, mean val. rec. loss:  9.20453678e-03\n",
      "Epoch: 8345 mean train loss:  1.02396251e-02, mean val. rec. loss:  9.20249222e-03\n",
      "Epoch: 8346 mean train loss:  1.02376195e-02, mean val. rec. loss:  9.20042385e-03\n",
      "Epoch: 8347 mean train loss:  1.02356102e-02, mean val. rec. loss:  9.19836682e-03\n",
      "Epoch: 8348 mean train loss:  1.02336084e-02, mean val. rec. loss:  9.19630525e-03\n",
      "Epoch: 8349 mean train loss:  1.02315935e-02, mean val. rec. loss:  9.19424084e-03\n",
      "Epoch: 8350 mean train loss:  1.02295786e-02, mean val. rec. loss:  9.19217133e-03\n",
      "Epoch: 8351 mean train loss:  1.02275674e-02, mean val. rec. loss:  9.19009219e-03\n",
      "Epoch: 8352 mean train loss:  1.02255432e-02, mean val. rec. loss:  9.18802041e-03\n",
      "Epoch: 8353 mean train loss:  1.02235218e-02, mean val. rec. loss:  9.18593843e-03\n",
      "Epoch: 8354 mean train loss:  1.02214930e-02, mean val. rec. loss:  9.18385588e-03\n",
      "Epoch: 8355 mean train loss:  1.02194650e-02, mean val. rec. loss:  9.18177504e-03\n",
      "Epoch: 8356 mean train loss:  1.02174390e-02, mean val. rec. loss:  9.17969136e-03\n",
      "Epoch: 8357 mean train loss:  1.02154073e-02, mean val. rec. loss:  9.17759463e-03\n",
      "Epoch: 8358 mean train loss:  1.02133747e-02, mean val. rec. loss:  9.17549394e-03\n",
      "Epoch: 8359 mean train loss:  1.02113319e-02, mean val. rec. loss:  9.17339041e-03\n",
      "Epoch: 8360 mean train loss:  1.02092910e-02, mean val. rec. loss:  9.17128745e-03\n",
      "Epoch: 8361 mean train loss:  1.02072528e-02, mean val. rec. loss:  9.16917259e-03\n",
      "Epoch: 8362 mean train loss:  1.02051951e-02, mean val. rec. loss:  9.16705942e-03\n",
      "Epoch: 8363 mean train loss:  1.02031522e-02, mean val. rec. loss:  9.16495476e-03\n",
      "Epoch: 8364 mean train loss:  1.02010992e-02, mean val. rec. loss:  9.16282686e-03\n",
      "Epoch: 8365 mean train loss:  1.01990387e-02, mean val. rec. loss:  9.16071142e-03\n",
      "Epoch: 8366 mean train loss:  1.01969893e-02, mean val. rec. loss:  9.15858522e-03\n",
      "Epoch: 8367 mean train loss:  1.01949251e-02, mean val. rec. loss:  9.15645788e-03\n",
      "Epoch: 8368 mean train loss:  1.01928618e-02, mean val. rec. loss:  9.15432997e-03\n",
      "Epoch: 8369 mean train loss:  1.01907985e-02, mean val. rec. loss:  9.15218789e-03\n",
      "Epoch: 8370 mean train loss:  1.01887203e-02, mean val. rec. loss:  9.15004864e-03\n",
      "Epoch: 8371 mean train loss:  1.01866532e-02, mean val. rec. loss:  9.14790089e-03\n",
      "Epoch: 8372 mean train loss:  1.01845750e-02, mean val. rec. loss:  9.14575370e-03\n",
      "Epoch: 8373 mean train loss:  1.01824959e-02, mean val. rec. loss:  9.14359801e-03\n",
      "Epoch: 8374 mean train loss:  1.01804186e-02, mean val. rec. loss:  9.14143609e-03\n",
      "Epoch: 8375 mean train loss:  1.01783255e-02, mean val. rec. loss:  9.13928550e-03\n",
      "Epoch: 8376 mean train loss:  1.01762399e-02, mean val. rec. loss:  9.13712187e-03\n",
      "Epoch: 8377 mean train loss:  1.01741496e-02, mean val. rec. loss:  9.13494124e-03\n",
      "Epoch: 8378 mean train loss:  1.01720509e-02, mean val. rec. loss:  9.13277874e-03\n",
      "Epoch: 8379 mean train loss:  1.01699624e-02, mean val. rec. loss:  9.13060208e-03\n",
      "Epoch: 8380 mean train loss:  1.01678591e-02, mean val. rec. loss:  9.12842484e-03\n",
      "Epoch: 8381 mean train loss:  1.01657557e-02, mean val. rec. loss:  9.12624250e-03\n",
      "Epoch: 8382 mean train loss:  1.01636533e-02, mean val. rec. loss:  9.12404996e-03\n",
      "Epoch: 8383 mean train loss:  1.01615342e-02, mean val. rec. loss:  9.12185742e-03\n",
      "Epoch: 8384 mean train loss:  1.01594262e-02, mean val. rec. loss:  9.11965750e-03\n",
      "Epoch: 8385 mean train loss:  1.01573107e-02, mean val. rec. loss:  9.11745475e-03\n",
      "Epoch: 8386 mean train loss:  1.01551859e-02, mean val. rec. loss:  9.11525314e-03\n",
      "Epoch: 8387 mean train loss:  1.01530714e-02, mean val. rec. loss:  9.11303905e-03\n",
      "Epoch: 8388 mean train loss:  1.01509401e-02, mean val. rec. loss:  9.11082666e-03\n",
      "Epoch: 8389 mean train loss:  1.01488107e-02, mean val. rec. loss:  9.10861030e-03\n",
      "Epoch: 8390 mean train loss:  1.01466776e-02, mean val. rec. loss:  9.10639791e-03\n",
      "Epoch: 8391 mean train loss:  1.01445482e-02, mean val. rec. loss:  9.10417418e-03\n",
      "Epoch: 8392 mean train loss:  1.01424067e-02, mean val. rec. loss:  9.10194309e-03\n",
      "Epoch: 8393 mean train loss:  1.01402605e-02, mean val. rec. loss:  9.09971652e-03\n",
      "Epoch: 8394 mean train loss:  1.01381134e-02, mean val. rec. loss:  9.09747408e-03\n",
      "Epoch: 8395 mean train loss:  1.01359598e-02, mean val. rec. loss:  9.09523108e-03\n",
      "Epoch: 8396 mean train loss:  1.01338061e-02, mean val. rec. loss:  9.09298751e-03\n",
      "Epoch: 8397 mean train loss:  1.01316544e-02, mean val. rec. loss:  9.09073656e-03\n",
      "Epoch: 8398 mean train loss:  1.01294933e-02, mean val. rec. loss:  9.08847541e-03\n",
      "Epoch: 8399 mean train loss:  1.01273341e-02, mean val. rec. loss:  9.08621313e-03\n",
      "Epoch: 8400 mean train loss:  1.01251674e-02, mean val. rec. loss:  9.08394858e-03\n",
      "Epoch: 8401 mean train loss:  1.01229868e-02, mean val. rec. loss:  9.08168233e-03\n",
      "Epoch: 8402 mean train loss:  1.01208174e-02, mean val. rec. loss:  9.07942628e-03\n",
      "Epoch: 8403 mean train loss:  1.01186395e-02, mean val. rec. loss:  9.07713678e-03\n",
      "Epoch: 8404 mean train loss:  1.01164533e-02, mean val. rec. loss:  9.07485919e-03\n",
      "Epoch: 8405 mean train loss:  1.01142755e-02, mean val. rec. loss:  9.07257139e-03\n",
      "Epoch: 8406 mean train loss:  1.01120837e-02, mean val. rec. loss:  9.07027849e-03\n",
      "Epoch: 8407 mean train loss:  1.01098891e-02, mean val. rec. loss:  9.06797652e-03\n",
      "Epoch: 8408 mean train loss:  1.01076945e-02, mean val. rec. loss:  9.06568588e-03\n",
      "Epoch: 8409 mean train loss:  1.01054990e-02, mean val. rec. loss:  9.06337711e-03\n",
      "Epoch: 8410 mean train loss:  1.01032951e-02, mean val. rec. loss:  9.06107003e-03\n",
      "Epoch: 8411 mean train loss:  1.01010819e-02, mean val. rec. loss:  9.05874822e-03\n",
      "Epoch: 8412 mean train loss:  1.00988696e-02, mean val. rec. loss:  9.05641789e-03\n",
      "Epoch: 8413 mean train loss:  1.00966489e-02, mean val. rec. loss:  9.05409834e-03\n",
      "Epoch: 8414 mean train loss:  1.00944338e-02, mean val. rec. loss:  9.05177653e-03\n",
      "Epoch: 8415 mean train loss:  1.00922122e-02, mean val. rec. loss:  9.04943543e-03\n",
      "Epoch: 8416 mean train loss:  1.00899888e-02, mean val. rec. loss:  9.04709887e-03\n",
      "Epoch: 8417 mean train loss:  1.00877579e-02, mean val. rec. loss:  9.04475268e-03\n",
      "Epoch: 8418 mean train loss:  1.00855130e-02, mean val. rec. loss:  9.04240535e-03\n",
      "Epoch: 8419 mean train loss:  1.00832775e-02, mean val. rec. loss:  9.04005234e-03\n",
      "Epoch: 8420 mean train loss:  1.00810344e-02, mean val. rec. loss:  9.03769254e-03\n",
      "Epoch: 8421 mean train loss:  1.00787821e-02, mean val. rec. loss:  9.03534407e-03\n",
      "Epoch: 8422 mean train loss:  1.00765363e-02, mean val. rec. loss:  9.03295932e-03\n",
      "Epoch: 8423 mean train loss:  1.00742803e-02, mean val. rec. loss:  9.03058194e-03\n",
      "Epoch: 8424 mean train loss:  1.00720149e-02, mean val. rec. loss:  9.02820796e-03\n",
      "Epoch: 8425 mean train loss:  1.00697617e-02, mean val. rec. loss:  9.02582150e-03\n",
      "Epoch: 8426 mean train loss:  1.00674917e-02, mean val. rec. loss:  9.02343845e-03\n",
      "Epoch: 8427 mean train loss:  1.00652151e-02, mean val. rec. loss:  9.02103556e-03\n",
      "Epoch: 8428 mean train loss:  1.00629405e-02, mean val. rec. loss:  9.01863606e-03\n",
      "Epoch: 8429 mean train loss:  1.00606574e-02, mean val. rec. loss:  9.01623430e-03\n",
      "Epoch: 8430 mean train loss:  1.00583762e-02, mean val. rec. loss:  9.01382120e-03\n",
      "Epoch: 8431 mean train loss:  1.00560783e-02, mean val. rec. loss:  9.01140696e-03\n",
      "Epoch: 8432 mean train loss:  1.00537850e-02, mean val. rec. loss:  9.00898592e-03\n",
      "Epoch: 8433 mean train loss:  1.00514917e-02, mean val. rec. loss:  9.00655127e-03\n",
      "Epoch: 8434 mean train loss:  1.00491882e-02, mean val. rec. loss:  9.00413023e-03\n",
      "Epoch: 8435 mean train loss:  1.00468809e-02, mean val. rec. loss:  9.00168198e-03\n",
      "Epoch: 8436 mean train loss:  1.00445644e-02, mean val. rec. loss:  8.99924506e-03\n",
      "Epoch: 8437 mean train loss:  1.00422571e-02, mean val. rec. loss:  8.99679681e-03\n",
      "Epoch: 8438 mean train loss:  1.00399368e-02, mean val. rec. loss:  8.99433721e-03\n",
      "Epoch: 8439 mean train loss:  1.00376063e-02, mean val. rec. loss:  8.99188669e-03\n",
      "Epoch: 8440 mean train loss:  1.00352814e-02, mean val. rec. loss:  8.98941915e-03\n",
      "Epoch: 8441 mean train loss:  1.00329462e-02, mean val. rec. loss:  8.98695276e-03\n",
      "Epoch: 8442 mean train loss:  1.00306017e-02, mean val. rec. loss:  8.98447161e-03\n",
      "Epoch: 8443 mean train loss:  1.00282628e-02, mean val. rec. loss:  8.98199161e-03\n",
      "Epoch: 8444 mean train loss:  1.00259136e-02, mean val. rec. loss:  8.97950196e-03\n",
      "Epoch: 8445 mean train loss:  1.00235514e-02, mean val. rec. loss:  8.97700721e-03\n",
      "Epoch: 8446 mean train loss:  1.00211976e-02, mean val. rec. loss:  8.97451417e-03\n",
      "Epoch: 8447 mean train loss:  1.00188326e-02, mean val. rec. loss:  8.97200411e-03\n",
      "Epoch: 8448 mean train loss:  1.00164667e-02, mean val. rec. loss:  8.96948215e-03\n",
      "Epoch: 8449 mean train loss:  1.00140896e-02, mean val. rec. loss:  8.96696812e-03\n",
      "Epoch: 8450 mean train loss:  1.00117172e-02, mean val. rec. loss:  8.96444445e-03\n",
      "Epoch: 8451 mean train loss:  1.00093317e-02, mean val. rec. loss:  8.96190435e-03\n",
      "Epoch: 8452 mean train loss:  1.00069444e-02, mean val. rec. loss:  8.95937104e-03\n",
      "Epoch: 8453 mean train loss:  1.00045496e-02, mean val. rec. loss:  8.95684114e-03\n",
      "Epoch: 8454 mean train loss:  1.00021530e-02, mean val. rec. loss:  8.95429310e-03\n",
      "Epoch: 8455 mean train loss:  9.99974983e-03, mean val. rec. loss:  8.95173598e-03\n",
      "Epoch: 8456 mean train loss:  9.99734295e-03, mean val. rec. loss:  8.94916752e-03\n",
      "Epoch: 8457 mean train loss:  9.99492583e-03, mean val. rec. loss:  8.94660360e-03\n",
      "Epoch: 8458 mean train loss:  9.99251057e-03, mean val. rec. loss:  8.94402947e-03\n",
      "Epoch: 8459 mean train loss:  9.99008413e-03, mean val. rec. loss:  8.94145081e-03\n",
      "Epoch: 8460 mean train loss:  9.98765304e-03, mean val. rec. loss:  8.93887215e-03\n",
      "Epoch: 8461 mean train loss:  9.98522847e-03, mean val. rec. loss:  8.93628215e-03\n",
      "Epoch: 8462 mean train loss:  9.98278435e-03, mean val. rec. loss:  8.93368931e-03\n",
      "Epoch: 8463 mean train loss:  9.98034488e-03, mean val. rec. loss:  8.93108059e-03\n",
      "Epoch: 8464 mean train loss:  9.97788399e-03, mean val. rec. loss:  8.92847018e-03\n",
      "Epoch: 8465 mean train loss:  9.97542776e-03, mean val. rec. loss:  8.92585863e-03\n",
      "Epoch: 8466 mean train loss:  9.97297712e-03, mean val. rec. loss:  8.92323234e-03\n",
      "Epoch: 8467 mean train loss:  9.97050506e-03, mean val. rec. loss:  8.92060265e-03\n",
      "Epoch: 8468 mean train loss:  9.96803580e-03, mean val. rec. loss:  8.91795708e-03\n",
      "Epoch: 8469 mean train loss:  9.96554977e-03, mean val. rec. loss:  8.91531208e-03\n",
      "Epoch: 8470 mean train loss:  9.96306561e-03, mean val. rec. loss:  8.91266935e-03\n",
      "Epoch: 8471 mean train loss:  9.96058704e-03, mean val. rec. loss:  8.91000961e-03\n",
      "Epoch: 8472 mean train loss:  9.95808704e-03, mean val. rec. loss:  8.90734250e-03\n",
      "Epoch: 8473 mean train loss:  9.95558892e-03, mean val. rec. loss:  8.90467425e-03\n",
      "Epoch: 8474 mean train loss:  9.95307868e-03, mean val. rec. loss:  8.90199466e-03\n",
      "Epoch: 8475 mean train loss:  9.95056752e-03, mean val. rec. loss:  8.89930430e-03\n",
      "Epoch: 8476 mean train loss:  9.94804704e-03, mean val. rec. loss:  8.89662585e-03\n",
      "Epoch: 8477 mean train loss:  9.94552564e-03, mean val. rec. loss:  8.89392245e-03\n",
      "Epoch: 8478 mean train loss:  9.94299399e-03, mean val. rec. loss:  8.89120998e-03\n",
      "Epoch: 8479 mean train loss:  9.94045117e-03, mean val. rec. loss:  8.88850544e-03\n",
      "Epoch: 8480 mean train loss:  9.93791580e-03, mean val. rec. loss:  8.88578163e-03\n",
      "Epoch: 8481 mean train loss:  9.93536367e-03, mean val. rec. loss:  8.88306349e-03\n",
      "Epoch: 8482 mean train loss:  9.93281340e-03, mean val. rec. loss:  8.88032437e-03\n",
      "Epoch: 8483 mean train loss:  9.93025288e-03, mean val. rec. loss:  8.87758355e-03\n",
      "Epoch: 8484 mean train loss:  9.92768492e-03, mean val. rec. loss:  8.87484103e-03\n",
      "Epoch: 8485 mean train loss:  9.92511324e-03, mean val. rec. loss:  8.87208716e-03\n",
      "Epoch: 8486 mean train loss:  9.92253503e-03, mean val. rec. loss:  8.86931969e-03\n",
      "Epoch: 8487 mean train loss:  9.91995218e-03, mean val. rec. loss:  8.86654599e-03\n",
      "Epoch: 8488 mean train loss:  9.91735628e-03, mean val. rec. loss:  8.86377228e-03\n",
      "Epoch: 8489 mean train loss:  9.91476411e-03, mean val. rec. loss:  8.86098723e-03\n",
      "Epoch: 8490 mean train loss:  9.91215425e-03, mean val. rec. loss:  8.85819482e-03\n",
      "Epoch: 8491 mean train loss:  9.90954812e-03, mean val. rec. loss:  8.85539446e-03\n",
      "Epoch: 8492 mean train loss:  9.90692615e-03, mean val. rec. loss:  8.85258220e-03\n",
      "Epoch: 8493 mean train loss:  9.90430047e-03, mean val. rec. loss:  8.84977220e-03\n",
      "Epoch: 8494 mean train loss:  9.90167105e-03, mean val. rec. loss:  8.84693500e-03\n",
      "Epoch: 8495 mean train loss:  9.89903419e-03, mean val. rec. loss:  8.84410742e-03\n",
      "Epoch: 8496 mean train loss:  9.89638988e-03, mean val. rec. loss:  8.84127078e-03\n",
      "Epoch: 8497 mean train loss:  9.89373812e-03, mean val. rec. loss:  8.83842053e-03\n",
      "Epoch: 8498 mean train loss:  9.89108077e-03, mean val. rec. loss:  8.83557709e-03\n",
      "Epoch: 8499 mean train loss:  9.88841412e-03, mean val. rec. loss:  8.83271153e-03\n",
      "Epoch: 8500 mean train loss:  9.88574374e-03, mean val. rec. loss:  8.82984143e-03\n",
      "Epoch: 8501 mean train loss:  9.88306404e-03, mean val. rec. loss:  8.82697134e-03\n",
      "Epoch: 8502 mean train loss:  9.88037877e-03, mean val. rec. loss:  8.82407459e-03\n",
      "Epoch: 8503 mean train loss:  9.87768883e-03, mean val. rec. loss:  8.82117672e-03\n",
      "Epoch: 8504 mean train loss:  9.87498400e-03, mean val. rec. loss:  8.81828224e-03\n",
      "Epoch: 8505 mean train loss:  9.87228010e-03, mean val. rec. loss:  8.81535658e-03\n",
      "Epoch: 8506 mean train loss:  9.86956316e-03, mean val. rec. loss:  8.81243603e-03\n",
      "Epoch: 8507 mean train loss:  9.86684623e-03, mean val. rec. loss:  8.80951150e-03\n",
      "Epoch: 8508 mean train loss:  9.86411346e-03, mean val. rec. loss:  8.80658244e-03\n",
      "Epoch: 8509 mean train loss:  9.86137977e-03, mean val. rec. loss:  8.80363410e-03\n",
      "Epoch: 8510 mean train loss:  9.85863862e-03, mean val. rec. loss:  8.80068009e-03\n",
      "Epoch: 8511 mean train loss:  9.85588910e-03, mean val. rec. loss:  8.79771758e-03\n",
      "Epoch: 8512 mean train loss:  9.85313026e-03, mean val. rec. loss:  8.79474769e-03\n",
      "Epoch: 8513 mean train loss:  9.85036584e-03, mean val. rec. loss:  8.79177554e-03\n",
      "Epoch: 8514 mean train loss:  9.84759304e-03, mean val. rec. loss:  8.78878921e-03\n",
      "Epoch: 8515 mean train loss:  9.84481186e-03, mean val. rec. loss:  8.78579778e-03\n",
      "Epoch: 8516 mean train loss:  9.84202323e-03, mean val. rec. loss:  8.78279501e-03\n",
      "Epoch: 8517 mean train loss:  9.83922808e-03, mean val. rec. loss:  8.77977353e-03\n",
      "Epoch: 8518 mean train loss:  9.83642455e-03, mean val. rec. loss:  8.77674865e-03\n",
      "Epoch: 8519 mean train loss:  9.83361264e-03, mean val. rec. loss:  8.77371129e-03\n",
      "Epoch: 8520 mean train loss:  9.83079422e-03, mean val. rec. loss:  8.77067053e-03\n",
      "Epoch: 8521 mean train loss:  9.82796183e-03, mean val. rec. loss:  8.76763488e-03\n",
      "Epoch: 8522 mean train loss:  9.82512944e-03, mean val. rec. loss:  8.76456917e-03\n",
      "Epoch: 8523 mean train loss:  9.82228215e-03, mean val. rec. loss:  8.76150347e-03\n",
      "Epoch: 8524 mean train loss:  9.81943486e-03, mean val. rec. loss:  8.75843039e-03\n",
      "Epoch: 8525 mean train loss:  9.81657081e-03, mean val. rec. loss:  8.75535278e-03\n",
      "Epoch: 8526 mean train loss:  9.81370955e-03, mean val. rec. loss:  8.75225362e-03\n",
      "Epoch: 8527 mean train loss:  9.81083340e-03, mean val. rec. loss:  8.74914652e-03\n",
      "Epoch: 8528 mean train loss:  9.80794887e-03, mean val. rec. loss:  8.74603603e-03\n",
      "Epoch: 8529 mean train loss:  9.80505502e-03, mean val. rec. loss:  8.74290058e-03\n",
      "Epoch: 8530 mean train loss:  9.80215559e-03, mean val. rec. loss:  8.73977534e-03\n",
      "Epoch: 8531 mean train loss:  9.79925057e-03, mean val. rec. loss:  8.73662969e-03\n",
      "Epoch: 8532 mean train loss:  9.79632880e-03, mean val. rec. loss:  8.73348858e-03\n",
      "Epoch: 8533 mean train loss:  9.79340702e-03, mean val. rec. loss:  8.73031911e-03\n",
      "Epoch: 8534 mean train loss:  9.79046755e-03, mean val. rec. loss:  8.72715191e-03\n",
      "Epoch: 8535 mean train loss:  9.78752250e-03, mean val. rec. loss:  8.72397735e-03\n",
      "Epoch: 8536 mean train loss:  9.78456906e-03, mean val. rec. loss:  8.72078804e-03\n",
      "Epoch: 8537 mean train loss:  9.78160911e-03, mean val. rec. loss:  8.71758342e-03\n",
      "Epoch: 8538 mean train loss:  9.77863426e-03, mean val. rec. loss:  8.71437427e-03\n",
      "Epoch: 8539 mean train loss:  9.77565848e-03, mean val. rec. loss:  8.71115491e-03\n",
      "Epoch: 8540 mean train loss:  9.77266780e-03, mean val. rec. loss:  8.70793555e-03\n",
      "Epoch: 8541 mean train loss:  9.76967806e-03, mean val. rec. loss:  8.70469691e-03\n",
      "Epoch: 8542 mean train loss:  9.76667248e-03, mean val. rec. loss:  8.70145204e-03\n",
      "Epoch: 8543 mean train loss:  9.76365201e-03, mean val. rec. loss:  8.69819355e-03\n",
      "Epoch: 8544 mean train loss:  9.76063433e-03, mean val. rec. loss:  8.69491863e-03\n",
      "Epoch: 8545 mean train loss:  9.75759430e-03, mean val. rec. loss:  8.69163180e-03\n",
      "Epoch: 8546 mean train loss:  9.75455242e-03, mean val. rec. loss:  8.68834837e-03\n",
      "Epoch: 8547 mean train loss:  9.75149842e-03, mean val. rec. loss:  8.68504453e-03\n",
      "Epoch: 8548 mean train loss:  9.74844164e-03, mean val. rec. loss:  8.68174069e-03\n",
      "Epoch: 8549 mean train loss:  9.74536623e-03, mean val. rec. loss:  8.67841247e-03\n",
      "Epoch: 8550 mean train loss:  9.74228617e-03, mean val. rec. loss:  8.67508765e-03\n",
      "Epoch: 8551 mean train loss:  9.73919586e-03, mean val. rec. loss:  8.67174922e-03\n",
      "Epoch: 8552 mean train loss:  9.73609997e-03, mean val. rec. loss:  8.66839435e-03\n",
      "Epoch: 8553 mean train loss:  9.73298452e-03, mean val. rec. loss:  8.66502701e-03\n",
      "Epoch: 8554 mean train loss:  9.72986349e-03, mean val. rec. loss:  8.66165116e-03\n",
      "Epoch: 8555 mean train loss:  9.72673594e-03, mean val. rec. loss:  8.65827134e-03\n",
      "Epoch: 8556 mean train loss:  9.72360095e-03, mean val. rec. loss:  8.65486941e-03\n",
      "Epoch: 8557 mean train loss:  9.72044640e-03, mean val. rec. loss:  8.65146691e-03\n",
      "Epoch: 8558 mean train loss:  9.71728719e-03, mean val. rec. loss:  8.64805535e-03\n",
      "Epoch: 8559 mean train loss:  9.71411681e-03, mean val. rec. loss:  8.64463187e-03\n",
      "Epoch: 8560 mean train loss:  9.71094178e-03, mean val. rec. loss:  8.64119876e-03\n",
      "Epoch: 8561 mean train loss:  9.70775184e-03, mean val. rec. loss:  8.63773786e-03\n",
      "Epoch: 8562 mean train loss:  9.70454143e-03, mean val. rec. loss:  8.63428263e-03\n",
      "Epoch: 8563 mean train loss:  9.70133753e-03, mean val. rec. loss:  8.63079849e-03\n",
      "Epoch: 8564 mean train loss:  9.69810942e-03, mean val. rec. loss:  8.62731832e-03\n",
      "Epoch: 8565 mean train loss:  9.69488783e-03, mean val. rec. loss:  8.62383361e-03\n",
      "Epoch: 8566 mean train loss:  9.69164110e-03, mean val. rec. loss:  8.62032281e-03\n",
      "Epoch: 8567 mean train loss:  9.68839530e-03, mean val. rec. loss:  8.61680068e-03\n",
      "Epoch: 8568 mean train loss:  9.68512064e-03, mean val. rec. loss:  8.61328139e-03\n",
      "Epoch: 8569 mean train loss:  9.68185250e-03, mean val. rec. loss:  8.60974451e-03\n",
      "Epoch: 8570 mean train loss:  9.67856573e-03, mean val. rec. loss:  8.60619290e-03\n",
      "Epoch: 8571 mean train loss:  9.67527152e-03, mean val. rec. loss:  8.60261747e-03\n",
      "Epoch: 8572 mean train loss:  9.67196054e-03, mean val. rec. loss:  8.59904544e-03\n",
      "Epoch: 8573 mean train loss:  9.66864212e-03, mean val. rec. loss:  8.59546605e-03\n",
      "Epoch: 8574 mean train loss:  9.66532183e-03, mean val. rec. loss:  8.59186624e-03\n",
      "Epoch: 8575 mean train loss:  9.66197827e-03, mean val. rec. loss:  8.58825963e-03\n",
      "Epoch: 8576 mean train loss:  9.65862819e-03, mean val. rec. loss:  8.58464394e-03\n",
      "Epoch: 8577 mean train loss:  9.65527159e-03, mean val. rec. loss:  8.58100501e-03\n",
      "Epoch: 8578 mean train loss:  9.65188892e-03, mean val. rec. loss:  8.57735417e-03\n",
      "Epoch: 8579 mean train loss:  9.64850253e-03, mean val. rec. loss:  8.57370787e-03\n",
      "Epoch: 8580 mean train loss:  9.64510962e-03, mean val. rec. loss:  8.57003492e-03\n",
      "Epoch: 8581 mean train loss:  9.64170367e-03, mean val. rec. loss:  8.56635517e-03\n",
      "Epoch: 8582 mean train loss:  9.63828655e-03, mean val. rec. loss:  8.56266351e-03\n",
      "Epoch: 8583 mean train loss:  9.63485453e-03, mean val. rec. loss:  8.55895824e-03\n",
      "Epoch: 8584 mean train loss:  9.63140948e-03, mean val. rec. loss:  8.55523709e-03\n",
      "Epoch: 8585 mean train loss:  9.62795698e-03, mean val. rec. loss:  8.55151481e-03\n",
      "Epoch: 8586 mean train loss:  9.62448958e-03, mean val. rec. loss:  8.54777042e-03\n",
      "Epoch: 8587 mean train loss:  9.62101380e-03, mean val. rec. loss:  8.54400902e-03\n",
      "Epoch: 8588 mean train loss:  9.61751475e-03, mean val. rec. loss:  8.54024706e-03\n",
      "Epoch: 8589 mean train loss:  9.61401755e-03, mean val. rec. loss:  8.53646695e-03\n",
      "Epoch: 8590 mean train loss:  9.61050453e-03, mean val. rec. loss:  8.53266812e-03\n",
      "Epoch: 8591 mean train loss:  9.60697102e-03, mean val. rec. loss:  8.52886930e-03\n",
      "Epoch: 8592 mean train loss:  9.60344124e-03, mean val. rec. loss:  8.52506254e-03\n",
      "Epoch: 8593 mean train loss:  9.59988632e-03, mean val. rec. loss:  8.52122403e-03\n",
      "Epoch: 8594 mean train loss:  9.59632395e-03, mean val. rec. loss:  8.51739233e-03\n",
      "Epoch: 8595 mean train loss:  9.59275227e-03, mean val. rec. loss:  8.51353284e-03\n",
      "Epoch: 8596 mean train loss:  9.58915265e-03, mean val. rec. loss:  8.50966711e-03\n",
      "Epoch: 8597 mean train loss:  9.58555676e-03, mean val. rec. loss:  8.50578154e-03\n",
      "Epoch: 8598 mean train loss:  9.58194318e-03, mean val. rec. loss:  8.50188179e-03\n",
      "Epoch: 8599 mean train loss:  9.57831284e-03, mean val. rec. loss:  8.49797921e-03\n",
      "Epoch: 8600 mean train loss:  9.57468343e-03, mean val. rec. loss:  8.49406076e-03\n",
      "Epoch: 8601 mean train loss:  9.57102609e-03, mean val. rec. loss:  8.49014060e-03\n",
      "Epoch: 8602 mean train loss:  9.56736130e-03, mean val. rec. loss:  8.48620173e-03\n",
      "Epoch: 8603 mean train loss:  9.56368161e-03, mean val. rec. loss:  8.48224245e-03\n",
      "Epoch: 8604 mean train loss:  9.55999727e-03, mean val. rec. loss:  8.47826390e-03\n",
      "Epoch: 8605 mean train loss:  9.55628313e-03, mean val. rec. loss:  8.47428420e-03\n",
      "Epoch: 8606 mean train loss:  9.55256992e-03, mean val. rec. loss:  8.47028580e-03\n",
      "Epoch: 8607 mean train loss:  9.54883995e-03, mean val. rec. loss:  8.46627266e-03\n",
      "Epoch: 8608 mean train loss:  9.54509136e-03, mean val. rec. loss:  8.46225328e-03\n",
      "Epoch: 8609 mean train loss:  9.54134464e-03, mean val. rec. loss:  8.45820612e-03\n",
      "Epoch: 8610 mean train loss:  9.53756811e-03, mean val. rec. loss:  8.45415725e-03\n",
      "Epoch: 8611 mean train loss:  9.53377949e-03, mean val. rec. loss:  8.45009365e-03\n",
      "Epoch: 8612 mean train loss:  9.52998155e-03, mean val. rec. loss:  8.44602834e-03\n",
      "Epoch: 8613 mean train loss:  9.52617710e-03, mean val. rec. loss:  8.44193696e-03\n",
      "Epoch: 8614 mean train loss:  9.52234936e-03, mean val. rec. loss:  8.43782799e-03\n",
      "Epoch: 8615 mean train loss:  9.51851604e-03, mean val. rec. loss:  8.43372186e-03\n",
      "Epoch: 8616 mean train loss:  9.51465945e-03, mean val. rec. loss:  8.42958171e-03\n",
      "Epoch: 8617 mean train loss:  9.51079261e-03, mean val. rec. loss:  8.42544327e-03\n",
      "Epoch: 8618 mean train loss:  9.50691646e-03, mean val. rec. loss:  8.42127250e-03\n",
      "Epoch: 8619 mean train loss:  9.50301331e-03, mean val. rec. loss:  8.41710854e-03\n",
      "Epoch: 8620 mean train loss:  9.49911574e-03, mean val. rec. loss:  8.41291282e-03\n",
      "Epoch: 8621 mean train loss:  9.49518838e-03, mean val. rec. loss:  8.40872335e-03\n",
      "Epoch: 8622 mean train loss:  9.49125171e-03, mean val. rec. loss:  8.40450892e-03\n",
      "Epoch: 8623 mean train loss:  9.48730387e-03, mean val. rec. loss:  8.40028713e-03\n",
      "Epoch: 8624 mean train loss:  9.48333554e-03, mean val. rec. loss:  8.39605229e-03\n",
      "Epoch: 8625 mean train loss:  9.47935418e-03, mean val. rec. loss:  8.39178230e-03\n",
      "Epoch: 8626 mean train loss:  9.47535512e-03, mean val. rec. loss:  8.38752139e-03\n",
      "Epoch: 8627 mean train loss:  9.47135234e-03, mean val. rec. loss:  8.38323722e-03\n",
      "Epoch: 8628 mean train loss:  9.46732815e-03, mean val. rec. loss:  8.37894739e-03\n",
      "Epoch: 8629 mean train loss:  9.46329092e-03, mean val. rec. loss:  8.37463601e-03\n",
      "Epoch: 8630 mean train loss:  9.45923228e-03, mean val. rec. loss:  8.37031726e-03\n",
      "Epoch: 8631 mean train loss:  9.45517363e-03, mean val. rec. loss:  8.36597583e-03\n",
      "Epoch: 8632 mean train loss:  9.45108333e-03, mean val. rec. loss:  8.36162930e-03\n",
      "Epoch: 8633 mean train loss:  9.44699117e-03, mean val. rec. loss:  8.35726122e-03\n",
      "Epoch: 8634 mean train loss:  9.44287945e-03, mean val. rec. loss:  8.35287840e-03\n",
      "Epoch: 8635 mean train loss:  9.43875191e-03, mean val. rec. loss:  8.34847744e-03\n",
      "Epoch: 8636 mean train loss:  9.43460574e-03, mean val. rec. loss:  8.34408101e-03\n",
      "Epoch: 8637 mean train loss:  9.43045212e-03, mean val. rec. loss:  8.33965624e-03\n",
      "Epoch: 8638 mean train loss:  9.42628175e-03, mean val. rec. loss:  8.33521672e-03\n",
      "Epoch: 8639 mean train loss:  9.42209741e-03, mean val. rec. loss:  8.33077380e-03\n",
      "Epoch: 8640 mean train loss:  9.41789072e-03, mean val. rec. loss:  8.32630480e-03\n",
      "Epoch: 8641 mean train loss:  9.41367379e-03, mean val. rec. loss:  8.32182956e-03\n",
      "Epoch: 8642 mean train loss:  9.40944103e-03, mean val. rec. loss:  8.31734695e-03\n",
      "Epoch: 8643 mean train loss:  9.40519616e-03, mean val. rec. loss:  8.31283939e-03\n",
      "Epoch: 8644 mean train loss:  9.40092709e-03, mean val. rec. loss:  8.30832219e-03\n",
      "Epoch: 8645 mean train loss:  9.39664685e-03, mean val. rec. loss:  8.30378459e-03\n",
      "Epoch: 8646 mean train loss:  9.39235357e-03, mean val. rec. loss:  8.29923564e-03\n",
      "Epoch: 8647 mean train loss:  9.38804539e-03, mean val. rec. loss:  8.29467025e-03\n",
      "Epoch: 8648 mean train loss:  9.38371393e-03, mean val. rec. loss:  8.29009239e-03\n",
      "Epoch: 8649 mean train loss:  9.37937410e-03, mean val. rec. loss:  8.28551112e-03\n",
      "Epoch: 8650 mean train loss:  9.37501564e-03, mean val. rec. loss:  8.28090151e-03\n",
      "Epoch: 8651 mean train loss:  9.37064601e-03, mean val. rec. loss:  8.27628282e-03\n",
      "Epoch: 8652 mean train loss:  9.36625217e-03, mean val. rec. loss:  8.27165676e-03\n",
      "Epoch: 8653 mean train loss:  9.36184437e-03, mean val. rec. loss:  8.26701143e-03\n",
      "Epoch: 8654 mean train loss:  9.35742353e-03, mean val. rec. loss:  8.26235759e-03\n",
      "Epoch: 8655 mean train loss:  9.35299058e-03, mean val. rec. loss:  8.25769354e-03\n",
      "Epoch: 8656 mean train loss:  9.34853436e-03, mean val. rec. loss:  8.25298867e-03\n",
      "Epoch: 8657 mean train loss:  9.34406417e-03, mean val. rec. loss:  8.24828494e-03\n",
      "Epoch: 8658 mean train loss:  9.33957909e-03, mean val. rec. loss:  8.24356986e-03\n",
      "Epoch: 8659 mean train loss:  9.33508376e-03, mean val. rec. loss:  8.23883664e-03\n",
      "Epoch: 8660 mean train loss:  9.33055957e-03, mean val. rec. loss:  8.23410116e-03\n",
      "Epoch: 8661 mean train loss:  9.32602513e-03, mean val. rec. loss:  8.22933619e-03\n",
      "Epoch: 8662 mean train loss:  9.32147394e-03, mean val. rec. loss:  8.22456442e-03\n",
      "Epoch: 8663 mean train loss:  9.31691437e-03, mean val. rec. loss:  8.21976996e-03\n",
      "Epoch: 8664 mean train loss:  9.31232686e-03, mean val. rec. loss:  8.21496814e-03\n",
      "Epoch: 8665 mean train loss:  9.30772166e-03, mean val. rec. loss:  8.21015838e-03\n",
      "Epoch: 8666 mean train loss:  9.30310902e-03, mean val. rec. loss:  8.20533161e-03\n",
      "Epoch: 8667 mean train loss:  9.29847775e-03, mean val. rec. loss:  8.20048896e-03\n",
      "Epoch: 8668 mean train loss:  9.29383158e-03, mean val. rec. loss:  8.19564121e-03\n",
      "Epoch: 8669 mean train loss:  9.28917238e-03, mean val. rec. loss:  8.19076171e-03\n",
      "Epoch: 8670 mean train loss:  9.28448153e-03, mean val. rec. loss:  8.18588617e-03\n",
      "Epoch: 8671 mean train loss:  9.27979532e-03, mean val. rec. loss:  8.18098966e-03\n",
      "Epoch: 8672 mean train loss:  9.27507188e-03, mean val. rec. loss:  8.17608918e-03\n",
      "Epoch: 8673 mean train loss:  9.27034657e-03, mean val. rec. loss:  8.17117226e-03\n",
      "Epoch: 8674 mean train loss:  9.26560264e-03, mean val. rec. loss:  8.16623266e-03\n",
      "Epoch: 8675 mean train loss:  9.26083916e-03, mean val. rec. loss:  8.16127718e-03\n",
      "Epoch: 8676 mean train loss:  9.25605891e-03, mean val. rec. loss:  8.15631377e-03\n",
      "Epoch: 8677 mean train loss:  9.25126005e-03, mean val. rec. loss:  8.15133844e-03\n",
      "Epoch: 8678 mean train loss:  9.24644256e-03, mean val. rec. loss:  8.14635745e-03\n",
      "Epoch: 8679 mean train loss:  9.24162042e-03, mean val. rec. loss:  8.14135038e-03\n",
      "Epoch: 8680 mean train loss:  9.23676383e-03, mean val. rec. loss:  8.13633763e-03\n",
      "Epoch: 8681 mean train loss:  9.23190630e-03, mean val. rec. loss:  8.13130731e-03\n",
      "Epoch: 8682 mean train loss:  9.22703388e-03, mean val. rec. loss:  8.12627699e-03\n",
      "Epoch: 8683 mean train loss:  9.22213166e-03, mean val. rec. loss:  8.12121039e-03\n",
      "Epoch: 8684 mean train loss:  9.21721827e-03, mean val. rec. loss:  8.11614888e-03\n",
      "Epoch: 8685 mean train loss:  9.21228999e-03, mean val. rec. loss:  8.11106810e-03\n",
      "Epoch: 8686 mean train loss:  9.20734960e-03, mean val. rec. loss:  8.10597428e-03\n",
      "Epoch: 8687 mean train loss:  9.20237941e-03, mean val. rec. loss:  8.10086798e-03\n",
      "Epoch: 8688 mean train loss:  9.19739991e-03, mean val. rec. loss:  8.09574921e-03\n",
      "Epoch: 8689 mean train loss:  9.19240552e-03, mean val. rec. loss:  8.09061853e-03\n",
      "Epoch: 8690 mean train loss:  9.18739715e-03, mean val. rec. loss:  8.08547425e-03\n",
      "Epoch: 8691 mean train loss:  9.18236459e-03, mean val. rec. loss:  8.08032316e-03\n",
      "Epoch: 8692 mean train loss:  9.17731805e-03, mean val. rec. loss:  8.07516640e-03\n",
      "Epoch: 8693 mean train loss:  9.17225568e-03, mean val. rec. loss:  8.06997846e-03\n",
      "Epoch: 8694 mean train loss:  9.16718215e-03, mean val. rec. loss:  8.06478825e-03\n",
      "Epoch: 8695 mean train loss:  9.16208161e-03, mean val. rec. loss:  8.05958783e-03\n",
      "Epoch: 8696 mean train loss:  9.15696803e-03, mean val. rec. loss:  8.05436927e-03\n",
      "Epoch: 8697 mean train loss:  9.15184049e-03, mean val. rec. loss:  8.04914731e-03\n",
      "Epoch: 8698 mean train loss:  9.14670084e-03, mean val. rec. loss:  8.04390834e-03\n",
      "Epoch: 8699 mean train loss:  9.14153978e-03, mean val. rec. loss:  8.03865802e-03\n",
      "Epoch: 8700 mean train loss:  9.13636289e-03, mean val. rec. loss:  8.03339297e-03\n",
      "Epoch: 8701 mean train loss:  9.13116366e-03, mean val. rec. loss:  8.02811488e-03\n",
      "Epoch: 8702 mean train loss:  9.12594952e-03, mean val. rec. loss:  8.02283565e-03\n",
      "Epoch: 8703 mean train loss:  9.12073073e-03, mean val. rec. loss:  8.01753374e-03\n",
      "Epoch: 8704 mean train loss:  9.11548401e-03, mean val. rec. loss:  8.01223070e-03\n",
      "Epoch: 8705 mean train loss:  9.11022146e-03, mean val. rec. loss:  8.00691178e-03\n",
      "Epoch: 8706 mean train loss:  9.10494495e-03, mean val. rec. loss:  8.00159060e-03\n",
      "Epoch: 8707 mean train loss:  9.09965539e-03, mean val. rec. loss:  7.99625297e-03\n",
      "Epoch: 8708 mean train loss:  9.09435187e-03, mean val. rec. loss:  7.99091194e-03\n",
      "Epoch: 8709 mean train loss:  9.08902787e-03, mean val. rec. loss:  7.98555107e-03\n",
      "Epoch: 8710 mean train loss:  9.08369363e-03, mean val. rec. loss:  7.98017432e-03\n",
      "Epoch: 8711 mean train loss:  9.07833331e-03, mean val. rec. loss:  7.97480380e-03\n",
      "Epoch: 8712 mean train loss:  9.07295996e-03, mean val. rec. loss:  7.96941174e-03\n",
      "Epoch: 8713 mean train loss:  9.06757916e-03, mean val. rec. loss:  7.96401855e-03\n",
      "Epoch: 8714 mean train loss:  9.06217043e-03, mean val. rec. loss:  7.95861459e-03\n",
      "Epoch: 8715 mean train loss:  9.05675052e-03, mean val. rec. loss:  7.95318681e-03\n",
      "Epoch: 8716 mean train loss:  9.05131479e-03, mean val. rec. loss:  7.94775733e-03\n",
      "Epoch: 8717 mean train loss:  9.04586323e-03, mean val. rec. loss:  7.94231310e-03\n",
      "Epoch: 8718 mean train loss:  9.04040328e-03, mean val. rec. loss:  7.93687569e-03\n",
      "Epoch: 8719 mean train loss:  9.03492658e-03, mean val. rec. loss:  7.93142183e-03\n",
      "Epoch: 8720 mean train loss:  9.02942102e-03, mean val. rec. loss:  7.92596967e-03\n",
      "Epoch: 8721 mean train loss:  9.02391266e-03, mean val. rec. loss:  7.92049710e-03\n",
      "Epoch: 8722 mean train loss:  9.01838568e-03, mean val. rec. loss:  7.91501205e-03\n",
      "Epoch: 8723 mean train loss:  9.01284100e-03, mean val. rec. loss:  7.90952417e-03\n",
      "Epoch: 8724 mean train loss:  9.00729074e-03, mean val. rec. loss:  7.90403572e-03\n",
      "Epoch: 8725 mean train loss:  9.00171535e-03, mean val. rec. loss:  7.89852800e-03\n",
      "Epoch: 8726 mean train loss:  8.99612877e-03, mean val. rec. loss:  7.89300780e-03\n",
      "Epoch: 8727 mean train loss:  8.99053196e-03, mean val. rec. loss:  7.88748420e-03\n",
      "Epoch: 8728 mean train loss:  8.98490535e-03, mean val. rec. loss:  7.88196344e-03\n",
      "Epoch: 8729 mean train loss:  8.97928154e-03, mean val. rec. loss:  7.87642566e-03\n",
      "Epoch: 8730 mean train loss:  8.97363165e-03, mean val. rec. loss:  7.87087825e-03\n",
      "Epoch: 8731 mean train loss:  8.96797059e-03, mean val. rec. loss:  7.86533991e-03\n",
      "Epoch: 8732 mean train loss:  8.96230395e-03, mean val. rec. loss:  7.85977889e-03\n",
      "Epoch: 8733 mean train loss:  8.95661123e-03, mean val. rec. loss:  7.85422013e-03\n",
      "Epoch: 8734 mean train loss:  8.95091293e-03, mean val. rec. loss:  7.84864607e-03\n",
      "Epoch: 8735 mean train loss:  8.94518669e-03, mean val. rec. loss:  7.84307654e-03\n",
      "Epoch: 8736 mean train loss:  8.93946232e-03, mean val. rec. loss:  7.83749851e-03\n",
      "Epoch: 8737 mean train loss:  8.93371746e-03, mean val. rec. loss:  7.83190801e-03\n",
      "Epoch: 8738 mean train loss:  8.92796143e-03, mean val. rec. loss:  7.82632091e-03\n",
      "Epoch: 8739 mean train loss:  8.92218584e-03, mean val. rec. loss:  7.82072133e-03\n",
      "Epoch: 8740 mean train loss:  8.91640002e-03, mean val. rec. loss:  7.81511495e-03\n",
      "Epoch: 8741 mean train loss:  8.91060767e-03, mean val. rec. loss:  7.80950517e-03\n",
      "Epoch: 8742 mean train loss:  8.90479019e-03, mean val. rec. loss:  7.80389539e-03\n",
      "Epoch: 8743 mean train loss:  8.89896805e-03, mean val. rec. loss:  7.79828730e-03\n",
      "Epoch: 8744 mean train loss:  8.89314219e-03, mean val. rec. loss:  7.79267582e-03\n",
      "Epoch: 8745 mean train loss:  8.88728746e-03, mean val. rec. loss:  7.78704279e-03\n",
      "Epoch: 8746 mean train loss:  8.88142901e-03, mean val. rec. loss:  7.78140750e-03\n",
      "Epoch: 8747 mean train loss:  8.87554542e-03, mean val. rec. loss:  7.77577617e-03\n",
      "Epoch: 8748 mean train loss:  8.86965811e-03, mean val. rec. loss:  7.77014484e-03\n",
      "Epoch: 8749 mean train loss:  8.86376613e-03, mean val. rec. loss:  7.76450557e-03\n",
      "Epoch: 8750 mean train loss:  8.85784809e-03, mean val. rec. loss:  7.75886688e-03\n",
      "Epoch: 8751 mean train loss:  8.85193098e-03, mean val. rec. loss:  7.75321627e-03\n",
      "Epoch: 8752 mean train loss:  8.84599525e-03, mean val. rec. loss:  7.74756963e-03\n",
      "Epoch: 8753 mean train loss:  8.84004835e-03, mean val. rec. loss:  7.74192186e-03\n",
      "Epoch: 8754 mean train loss:  8.83409213e-03, mean val. rec. loss:  7.73625708e-03\n",
      "Epoch: 8755 mean train loss:  8.82812381e-03, mean val. rec. loss:  7.73060818e-03\n",
      "Epoch: 8756 mean train loss:  8.82214245e-03, mean val. rec. loss:  7.72495247e-03\n",
      "Epoch: 8757 mean train loss:  8.81615551e-03, mean val. rec. loss:  7.71929506e-03\n",
      "Epoch: 8758 mean train loss:  8.81015088e-03, mean val. rec. loss:  7.71363482e-03\n",
      "Epoch: 8759 mean train loss:  8.80414438e-03, mean val. rec. loss:  7.70797344e-03\n",
      "Epoch: 8760 mean train loss:  8.79811647e-03, mean val. rec. loss:  7.70231603e-03\n",
      "Epoch: 8761 mean train loss:  8.79208670e-03, mean val. rec. loss:  7.69665862e-03\n",
      "Epoch: 8762 mean train loss:  8.78604203e-03, mean val. rec. loss:  7.69099157e-03\n",
      "Epoch: 8763 mean train loss:  8.77999177e-03, mean val. rec. loss:  7.68531602e-03\n",
      "Epoch: 8764 mean train loss:  8.77392289e-03, mean val. rec. loss:  7.67964330e-03\n",
      "Epoch: 8765 mean train loss:  8.76784936e-03, mean val. rec. loss:  7.67398873e-03\n",
      "Epoch: 8766 mean train loss:  8.76177768e-03, mean val. rec. loss:  7.66833245e-03\n",
      "Epoch: 8767 mean train loss:  8.75568832e-03, mean val. rec. loss:  7.66267334e-03\n",
      "Epoch: 8768 mean train loss:  8.74957847e-03, mean val. rec. loss:  7.65700800e-03\n",
      "Epoch: 8769 mean train loss:  8.74347608e-03, mean val. rec. loss:  7.65134151e-03\n",
      "Epoch: 8770 mean train loss:  8.73735506e-03, mean val. rec. loss:  7.64569034e-03\n",
      "Epoch: 8771 mean train loss:  8.73123776e-03, mean val. rec. loss:  7.64004427e-03\n",
      "Epoch: 8772 mean train loss:  8.72510557e-03, mean val. rec. loss:  7.63439423e-03\n",
      "Epoch: 8773 mean train loss:  8.71895475e-03, mean val. rec. loss:  7.62874590e-03\n",
      "Epoch: 8774 mean train loss:  8.71280859e-03, mean val. rec. loss:  7.62307942e-03\n",
      "Epoch: 8775 mean train loss:  8.70665312e-03, mean val. rec. loss:  7.61742314e-03\n",
      "Epoch: 8776 mean train loss:  8.70048834e-03, mean val. rec. loss:  7.61178387e-03\n",
      "Epoch: 8777 mean train loss:  8.69431425e-03, mean val. rec. loss:  7.60614574e-03\n",
      "Epoch: 8778 mean train loss:  8.68813457e-03, mean val. rec. loss:  7.60050421e-03\n",
      "Epoch: 8779 mean train loss:  8.68196047e-03, mean val. rec. loss:  7.59485927e-03\n",
      "Epoch: 8780 mean train loss:  8.67576124e-03, mean val. rec. loss:  7.58923362e-03\n",
      "Epoch: 8781 mean train loss:  8.66956387e-03, mean val. rec. loss:  7.58361476e-03\n",
      "Epoch: 8782 mean train loss:  8.66336464e-03, mean val. rec. loss:  7.57798684e-03\n",
      "Epoch: 8783 mean train loss:  8.65714492e-03, mean val. rec. loss:  7.57236458e-03\n",
      "Epoch: 8784 mean train loss:  8.65093638e-03, mean val. rec. loss:  7.56674686e-03\n",
      "Epoch: 8785 mean train loss:  8.64471201e-03, mean val. rec. loss:  7.56112290e-03\n",
      "Epoch: 8786 mean train loss:  8.63848391e-03, mean val. rec. loss:  7.55552333e-03\n",
      "Epoch: 8787 mean train loss:  8.63225861e-03, mean val. rec. loss:  7.54993963e-03\n",
      "Epoch: 8788 mean train loss:  8.62601655e-03, mean val. rec. loss:  7.54435196e-03\n",
      "Epoch: 8789 mean train loss:  8.61977728e-03, mean val. rec. loss:  7.53875408e-03\n",
      "Epoch: 8790 mean train loss:  8.61353056e-03, mean val. rec. loss:  7.53316528e-03\n",
      "Epoch: 8791 mean train loss:  8.60728477e-03, mean val. rec. loss:  7.52758328e-03\n",
      "Epoch: 8792 mean train loss:  8.60101943e-03, mean val. rec. loss:  7.52202396e-03\n",
      "Epoch: 8793 mean train loss:  8.59477272e-03, mean val. rec. loss:  7.51646464e-03\n",
      "Epoch: 8794 mean train loss:  8.58850551e-03, mean val. rec. loss:  7.51090305e-03\n",
      "Epoch: 8795 mean train loss:  8.58223831e-03, mean val. rec. loss:  7.50534997e-03\n",
      "Epoch: 8796 mean train loss:  8.57597111e-03, mean val. rec. loss:  7.49980142e-03\n",
      "Epoch: 8797 mean train loss:  8.56969646e-03, mean val. rec. loss:  7.49427385e-03\n",
      "Epoch: 8798 mean train loss:  8.56342832e-03, mean val. rec. loss:  7.48874685e-03\n",
      "Epoch: 8799 mean train loss:  8.55715647e-03, mean val. rec. loss:  7.48322041e-03\n",
      "Epoch: 8800 mean train loss:  8.55087530e-03, mean val. rec. loss:  7.47769965e-03\n",
      "Epoch: 8801 mean train loss:  8.54458947e-03, mean val. rec. loss:  7.47220496e-03\n",
      "Epoch: 8802 mean train loss:  8.53831669e-03, mean val. rec. loss:  7.46670745e-03\n",
      "Epoch: 8803 mean train loss:  8.53202434e-03, mean val. rec. loss:  7.46122920e-03\n",
      "Epoch: 8804 mean train loss:  8.52574876e-03, mean val. rec. loss:  7.45574983e-03\n",
      "Epoch: 8805 mean train loss:  8.51945270e-03, mean val. rec. loss:  7.45028179e-03\n",
      "Epoch: 8806 mean train loss:  8.51317711e-03, mean val. rec. loss:  7.44481376e-03\n",
      "Epoch: 8807 mean train loss:  8.50687732e-03, mean val. rec. loss:  7.43937634e-03\n",
      "Epoch: 8808 mean train loss:  8.50059988e-03, mean val. rec. loss:  7.43393666e-03\n",
      "Epoch: 8809 mean train loss:  8.49430661e-03, mean val. rec. loss:  7.42849810e-03\n",
      "Epoch: 8810 mean train loss:  8.48801520e-03, mean val. rec. loss:  7.42308053e-03\n",
      "Epoch: 8811 mean train loss:  8.48173030e-03, mean val. rec. loss:  7.41766863e-03\n",
      "Epoch: 8812 mean train loss:  8.47544820e-03, mean val. rec. loss:  7.41226296e-03\n",
      "Epoch: 8813 mean train loss:  8.46916145e-03, mean val. rec. loss:  7.40687827e-03\n",
      "Epoch: 8814 mean train loss:  8.46287562e-03, mean val. rec. loss:  7.40150209e-03\n",
      "Epoch: 8815 mean train loss:  8.45658701e-03, mean val. rec. loss:  7.39612874e-03\n",
      "Epoch: 8816 mean train loss:  8.45030025e-03, mean val. rec. loss:  7.39076787e-03\n",
      "Epoch: 8817 mean train loss:  8.44402653e-03, mean val. rec. loss:  7.38541947e-03\n",
      "Epoch: 8818 mean train loss:  8.43773512e-03, mean val. rec. loss:  7.38009488e-03\n",
      "Epoch: 8819 mean train loss:  8.43146233e-03, mean val. rec. loss:  7.37477029e-03\n",
      "Epoch: 8820 mean train loss:  8.42518768e-03, mean val. rec. loss:  7.36945931e-03\n",
      "Epoch: 8821 mean train loss:  8.41891489e-03, mean val. rec. loss:  7.36414493e-03\n",
      "Epoch: 8822 mean train loss:  8.41263931e-03, mean val. rec. loss:  7.35885890e-03\n",
      "Epoch: 8823 mean train loss:  8.40638235e-03, mean val. rec. loss:  7.35358421e-03\n",
      "Epoch: 8824 mean train loss:  8.40011422e-03, mean val. rec. loss:  7.34833616e-03\n",
      "Epoch: 8825 mean train loss:  8.39385446e-03, mean val. rec. loss:  7.34307735e-03\n",
      "Epoch: 8826 mean train loss:  8.38759936e-03, mean val. rec. loss:  7.33782534e-03\n",
      "Epoch: 8827 mean train loss:  8.38133961e-03, mean val. rec. loss:  7.33260224e-03\n",
      "Epoch: 8828 mean train loss:  8.37509289e-03, mean val. rec. loss:  7.32740182e-03\n",
      "Epoch: 8829 mean train loss:  8.36884804e-03, mean val. rec. loss:  7.32219800e-03\n",
      "Epoch: 8830 mean train loss:  8.36260598e-03, mean val. rec. loss:  7.31702253e-03\n",
      "Epoch: 8831 mean train loss:  8.35637695e-03, mean val. rec. loss:  7.31184140e-03\n",
      "Epoch: 8832 mean train loss:  8.35014420e-03, mean val. rec. loss:  7.30668294e-03\n",
      "Epoch: 8833 mean train loss:  8.34391424e-03, mean val. rec. loss:  7.30153355e-03\n",
      "Epoch: 8834 mean train loss:  8.33769545e-03, mean val. rec. loss:  7.29640344e-03\n",
      "Epoch: 8835 mean train loss:  8.33147853e-03, mean val. rec. loss:  7.29128580e-03\n",
      "Epoch: 8836 mean train loss:  8.32527464e-03, mean val. rec. loss:  7.28617610e-03\n",
      "Epoch: 8837 mean train loss:  8.31906796e-03, mean val. rec. loss:  7.28107378e-03\n",
      "Epoch: 8838 mean train loss:  8.31286873e-03, mean val. rec. loss:  7.27600377e-03\n",
      "Epoch: 8839 mean train loss:  8.30667508e-03, mean val. rec. loss:  7.27094680e-03\n",
      "Epoch: 8840 mean train loss:  8.30049633e-03, mean val. rec. loss:  7.26589890e-03\n",
      "Epoch: 8841 mean train loss:  8.29431293e-03, mean val. rec. loss:  7.26085837e-03\n",
      "Epoch: 8842 mean train loss:  8.28814815e-03, mean val. rec. loss:  7.25584223e-03\n",
      "Epoch: 8843 mean train loss:  8.28198430e-03, mean val. rec. loss:  7.25084026e-03\n",
      "Epoch: 8844 mean train loss:  8.27581859e-03, mean val. rec. loss:  7.24584679e-03\n",
      "Epoch: 8845 mean train loss:  8.26967056e-03, mean val. rec. loss:  7.24086977e-03\n",
      "Epoch: 8846 mean train loss:  8.26353837e-03, mean val. rec. loss:  7.23591032e-03\n",
      "Epoch: 8847 mean train loss:  8.25740618e-03, mean val. rec. loss:  7.23095881e-03\n",
      "Epoch: 8848 mean train loss:  8.25127119e-03, mean val. rec. loss:  7.22602431e-03\n",
      "Epoch: 8849 mean train loss:  8.24515203e-03, mean val. rec. loss:  7.22111136e-03\n",
      "Epoch: 8850 mean train loss:  8.23905150e-03, mean val. rec. loss:  7.21620861e-03\n",
      "Epoch: 8851 mean train loss:  8.23295468e-03, mean val. rec. loss:  7.21132571e-03\n",
      "Epoch: 8852 mean train loss:  8.22685415e-03, mean val. rec. loss:  7.20645358e-03\n",
      "Epoch: 8853 mean train loss:  8.22076944e-03, mean val. rec. loss:  7.20160810e-03\n",
      "Epoch: 8854 mean train loss:  8.21470429e-03, mean val. rec. loss:  7.19676942e-03\n",
      "Epoch: 8855 mean train loss:  8.20864006e-03, mean val. rec. loss:  7.19194492e-03\n",
      "Epoch: 8856 mean train loss:  8.20258422e-03, mean val. rec. loss:  7.18714196e-03\n",
      "Epoch: 8857 mean train loss:  8.19654327e-03, mean val. rec. loss:  7.18235601e-03\n",
      "Epoch: 8858 mean train loss:  8.19050605e-03, mean val. rec. loss:  7.17756326e-03\n",
      "Epoch: 8859 mean train loss:  8.18448000e-03, mean val. rec. loss:  7.17280453e-03\n",
      "Epoch: 8860 mean train loss:  8.17846513e-03, mean val. rec. loss:  7.16807244e-03\n",
      "Epoch: 8861 mean train loss:  8.17245956e-03, mean val. rec. loss:  7.16334036e-03\n",
      "Epoch: 8862 mean train loss:  8.16647169e-03, mean val. rec. loss:  7.15862528e-03\n",
      "Epoch: 8863 mean train loss:  8.16048475e-03, mean val. rec. loss:  7.15393629e-03\n",
      "Epoch: 8864 mean train loss:  8.15451084e-03, mean val. rec. loss:  7.14925126e-03\n",
      "Epoch: 8865 mean train loss:  8.14854345e-03, mean val. rec. loss:  7.14459119e-03\n",
      "Epoch: 8866 mean train loss:  8.14260120e-03, mean val. rec. loss:  7.13993678e-03\n",
      "Epoch: 8867 mean train loss:  8.13666361e-03, mean val. rec. loss:  7.13531979e-03\n",
      "Epoch: 8868 mean train loss:  8.13073346e-03, mean val. rec. loss:  7.13068977e-03\n",
      "Epoch: 8869 mean train loss:  8.12481356e-03, mean val. rec. loss:  7.12608696e-03\n",
      "Epoch: 8870 mean train loss:  8.11891879e-03, mean val. rec. loss:  7.12151703e-03\n",
      "Epoch: 8871 mean train loss:  8.11302310e-03, mean val. rec. loss:  7.11695845e-03\n",
      "Epoch: 8872 mean train loss:  8.10714696e-03, mean val. rec. loss:  7.11240213e-03\n",
      "Epoch: 8873 mean train loss:  8.10126988e-03, mean val. rec. loss:  7.10785942e-03\n",
      "Epoch: 8874 mean train loss:  8.09541981e-03, mean val. rec. loss:  7.10334846e-03\n",
      "Epoch: 8875 mean train loss:  8.08957719e-03, mean val. rec. loss:  7.09885224e-03\n",
      "Epoch: 8876 mean train loss:  8.08374015e-03, mean val. rec. loss:  7.09437700e-03\n",
      "Epoch: 8877 mean train loss:  8.07793384e-03, mean val. rec. loss:  7.08989893e-03\n",
      "Epoch: 8878 mean train loss:  8.07212288e-03, mean val. rec. loss:  7.08544013e-03\n",
      "Epoch: 8879 mean train loss:  8.06633053e-03, mean val. rec. loss:  7.08101592e-03\n",
      "Epoch: 8880 mean train loss:  8.06055681e-03, mean val. rec. loss:  7.07660022e-03\n",
      "Epoch: 8881 mean train loss:  8.05478030e-03, mean val. rec. loss:  7.07219529e-03\n",
      "Epoch: 8882 mean train loss:  8.04903171e-03, mean val. rec. loss:  7.06780907e-03\n",
      "Epoch: 8883 mean train loss:  8.04329244e-03, mean val. rec. loss:  7.06343929e-03\n",
      "Epoch: 8884 mean train loss:  8.03756248e-03, mean val. rec. loss:  7.05910126e-03\n",
      "Epoch: 8885 mean train loss:  8.03185952e-03, mean val. rec. loss:  7.05477060e-03\n",
      "Epoch: 8886 mean train loss:  8.02615657e-03, mean val. rec. loss:  7.05044391e-03\n",
      "Epoch: 8887 mean train loss:  8.02047316e-03, mean val. rec. loss:  7.04614841e-03\n",
      "Epoch: 8888 mean train loss:  8.01480745e-03, mean val. rec. loss:  7.04186992e-03\n",
      "Epoch: 8889 mean train loss:  8.00914359e-03, mean val. rec. loss:  7.03760617e-03\n",
      "Epoch: 8890 mean train loss:  8.00350861e-03, mean val. rec. loss:  7.03335659e-03\n",
      "Epoch: 8891 mean train loss:  7.99787548e-03, mean val. rec. loss:  7.02912005e-03\n",
      "Epoch: 8892 mean train loss:  7.99226284e-03, mean val. rec. loss:  7.02490449e-03\n",
      "Epoch: 8893 mean train loss:  7.98666882e-03, mean val. rec. loss:  7.02071332e-03\n",
      "Epoch: 8894 mean train loss:  7.98107666e-03, mean val. rec. loss:  7.01653348e-03\n",
      "Epoch: 8895 mean train loss:  7.97551430e-03, mean val. rec. loss:  7.01237008e-03\n",
      "Epoch: 8896 mean train loss:  7.96995566e-03, mean val. rec. loss:  7.00821746e-03\n",
      "Epoch: 8897 mean train loss:  7.96441471e-03, mean val. rec. loss:  7.00408922e-03\n",
      "Epoch: 8898 mean train loss:  7.95889238e-03, mean val. rec. loss:  6.99997855e-03\n",
      "Epoch: 8899 mean train loss:  7.95338682e-03, mean val. rec. loss:  6.99587129e-03\n",
      "Epoch: 8900 mean train loss:  7.94788125e-03, mean val. rec. loss:  6.99179578e-03\n",
      "Epoch: 8901 mean train loss:  7.94241200e-03, mean val. rec. loss:  6.98773671e-03\n",
      "Epoch: 8902 mean train loss:  7.93694181e-03, mean val. rec. loss:  6.98368104e-03\n",
      "Epoch: 8903 mean train loss:  7.93149025e-03, mean val. rec. loss:  6.97964635e-03\n",
      "Epoch: 8904 mean train loss:  7.92605638e-03, mean val. rec. loss:  6.97564965e-03\n",
      "Epoch: 8905 mean train loss:  7.92064485e-03, mean val. rec. loss:  6.97165522e-03\n",
      "Epoch: 8906 mean train loss:  7.91524357e-03, mean val. rec. loss:  6.96766816e-03\n",
      "Epoch: 8907 mean train loss:  7.90984973e-03, mean val. rec. loss:  6.96370604e-03\n",
      "Epoch: 8908 mean train loss:  7.90448569e-03, mean val. rec. loss:  6.95976207e-03\n",
      "Epoch: 8909 mean train loss:  7.89912817e-03, mean val. rec. loss:  6.95583171e-03\n",
      "Epoch: 8910 mean train loss:  7.89378462e-03, mean val. rec. loss:  6.95191042e-03\n",
      "Epoch: 8911 mean train loss:  7.88845875e-03, mean val. rec. loss:  6.94800500e-03\n",
      "Epoch: 8912 mean train loss:  7.88315244e-03, mean val. rec. loss:  6.94413701e-03\n",
      "Epoch: 8913 mean train loss:  7.87786661e-03, mean val. rec. loss:  6.94027922e-03\n",
      "Epoch: 8914 mean train loss:  7.87259568e-03, mean val. rec. loss:  6.93642937e-03\n",
      "Epoch: 8915 mean train loss:  7.86732754e-03, mean val. rec. loss:  6.93260560e-03\n",
      "Epoch: 8916 mean train loss:  7.86209293e-03, mean val. rec. loss:  6.92879884e-03\n",
      "Epoch: 8917 mean train loss:  7.85686110e-03, mean val. rec. loss:  6.92499322e-03\n",
      "Epoch: 8918 mean train loss:  7.85164604e-03, mean val. rec. loss:  6.92120120e-03\n",
      "Epoch: 8919 mean train loss:  7.84644773e-03, mean val. rec. loss:  6.91744037e-03\n",
      "Epoch: 8920 mean train loss:  7.84127364e-03, mean val. rec. loss:  6.91370561e-03\n",
      "Epoch: 8921 mean train loss:  7.83611444e-03, mean val. rec. loss:  6.90997256e-03\n",
      "Epoch: 8922 mean train loss:  7.83097293e-03, mean val. rec. loss:  6.90626389e-03\n",
      "Epoch: 8923 mean train loss:  7.82583701e-03, mean val. rec. loss:  6.90257337e-03\n",
      "Epoch: 8924 mean train loss:  7.82073274e-03, mean val. rec. loss:  6.89889134e-03\n",
      "Epoch: 8925 mean train loss:  7.81563220e-03, mean val. rec. loss:  6.89521556e-03\n",
      "Epoch: 8926 mean train loss:  7.81055029e-03, mean val. rec. loss:  6.89157606e-03\n",
      "Epoch: 8927 mean train loss:  7.80548513e-03, mean val. rec. loss:  6.88794564e-03\n",
      "Epoch: 8928 mean train loss:  7.80044045e-03, mean val. rec. loss:  6.88433732e-03\n",
      "Epoch: 8929 mean train loss:  7.79541347e-03, mean val. rec. loss:  6.88074318e-03\n",
      "Epoch: 8930 mean train loss:  7.79040558e-03, mean val. rec. loss:  6.87716605e-03\n",
      "Epoch: 8931 mean train loss:  7.78540001e-03, mean val. rec. loss:  6.87359403e-03\n",
      "Epoch: 8932 mean train loss:  7.78043029e-03, mean val. rec. loss:  6.87002824e-03\n",
      "Epoch: 8933 mean train loss:  7.77545963e-03, mean val. rec. loss:  6.86651461e-03\n",
      "Epoch: 8934 mean train loss:  7.77051971e-03, mean val. rec. loss:  6.86300892e-03\n",
      "Epoch: 8935 mean train loss:  7.76559375e-03, mean val. rec. loss:  6.85951061e-03\n",
      "Epoch: 8936 mean train loss:  7.76068082e-03, mean val. rec. loss:  6.85601059e-03\n",
      "Epoch: 8937 mean train loss:  7.75578699e-03, mean val. rec. loss:  6.85254686e-03\n",
      "Epoch: 8938 mean train loss:  7.75090153e-03, mean val. rec. loss:  6.84910183e-03\n",
      "Epoch: 8939 mean train loss:  7.74603563e-03, mean val. rec. loss:  6.84566192e-03\n",
      "Epoch: 8940 mean train loss:  7.74120092e-03, mean val. rec. loss:  6.84222540e-03\n",
      "Epoch: 8941 mean train loss:  7.73636248e-03, mean val. rec. loss:  6.83883594e-03\n",
      "Epoch: 8942 mean train loss:  7.73155430e-03, mean val. rec. loss:  6.83545046e-03\n",
      "Epoch: 8943 mean train loss:  7.72676428e-03, mean val. rec. loss:  6.83207461e-03\n",
      "Epoch: 8944 mean train loss:  7.72198590e-03, mean val. rec. loss:  6.82871237e-03\n",
      "Epoch: 8945 mean train loss:  7.71722661e-03, mean val. rec. loss:  6.82538245e-03\n",
      "Epoch: 8946 mean train loss:  7.71247709e-03, mean val. rec. loss:  6.82205933e-03\n",
      "Epoch: 8947 mean train loss:  7.70774620e-03, mean val. rec. loss:  6.81874755e-03\n",
      "Epoch: 8948 mean train loss:  7.70304417e-03, mean val. rec. loss:  6.81544427e-03\n",
      "Epoch: 8949 mean train loss:  7.69834213e-03, mean val. rec. loss:  6.81218409e-03\n",
      "Epoch: 8950 mean train loss:  7.69366711e-03, mean val. rec. loss:  6.80892901e-03\n",
      "Epoch: 8951 mean train loss:  7.68901303e-03, mean val. rec. loss:  6.80566713e-03\n",
      "Epoch: 8952 mean train loss:  7.68436872e-03, mean val. rec. loss:  6.80242282e-03\n",
      "Epoch: 8953 mean train loss:  7.67974071e-03, mean val. rec. loss:  6.79922501e-03\n",
      "Epoch: 8954 mean train loss:  7.67512807e-03, mean val. rec. loss:  6.79601926e-03\n",
      "Epoch: 8955 mean train loss:  7.67053218e-03, mean val. rec. loss:  6.79283165e-03\n",
      "Epoch: 8956 mean train loss:  7.66596376e-03, mean val. rec. loss:  6.78965481e-03\n",
      "Epoch: 8957 mean train loss:  7.66139534e-03, mean val. rec. loss:  6.78652334e-03\n",
      "Epoch: 8958 mean train loss:  7.65685765e-03, mean val. rec. loss:  6.78338562e-03\n",
      "Epoch: 8959 mean train loss:  7.65233206e-03, mean val. rec. loss:  6.78024337e-03\n",
      "Epoch: 8960 mean train loss:  7.64782184e-03, mean val. rec. loss:  6.77713288e-03\n",
      "Epoch: 8961 mean train loss:  7.64332558e-03, mean val. rec. loss:  6.77404619e-03\n",
      "Epoch: 8962 mean train loss:  7.63885213e-03, mean val. rec. loss:  6.77096971e-03\n",
      "Epoch: 8963 mean train loss:  7.63439172e-03, mean val. rec. loss:  6.76789437e-03\n",
      "Epoch: 8964 mean train loss:  7.62995040e-03, mean val. rec. loss:  6.76483944e-03\n",
      "Epoch: 8965 mean train loss:  7.62551931e-03, mean val. rec. loss:  6.76181399e-03\n",
      "Epoch: 8966 mean train loss:  7.62111058e-03, mean val. rec. loss:  6.75879761e-03\n",
      "Epoch: 8967 mean train loss:  7.61671860e-03, mean val. rec. loss:  6.75579257e-03\n",
      "Epoch: 8968 mean train loss:  7.61234478e-03, mean val. rec. loss:  6.75280284e-03\n",
      "Epoch: 8969 mean train loss:  7.60797422e-03, mean val. rec. loss:  6.74982332e-03\n",
      "Epoch: 8970 mean train loss:  7.60363951e-03, mean val. rec. loss:  6.74686477e-03\n",
      "Epoch: 8971 mean train loss:  7.59930526e-03, mean val. rec. loss:  6.74392210e-03\n",
      "Epoch: 8972 mean train loss:  7.59499335e-03, mean val. rec. loss:  6.74099644e-03\n",
      "Epoch: 8973 mean train loss:  7.59070101e-03, mean val. rec. loss:  6.73807532e-03\n",
      "Epoch: 8974 mean train loss:  7.58641983e-03, mean val. rec. loss:  6.73518084e-03\n",
      "Epoch: 8975 mean train loss:  7.58215541e-03, mean val. rec. loss:  6.73229317e-03\n",
      "Epoch: 8976 mean train loss:  7.57790264e-03, mean val. rec. loss:  6.72940153e-03\n",
      "Epoch: 8977 mean train loss:  7.57367639e-03, mean val. rec. loss:  6.72654675e-03\n",
      "Epoch: 8978 mean train loss:  7.56946086e-03, mean val. rec. loss:  6.72370500e-03\n",
      "Epoch: 8979 mean train loss:  7.56525790e-03, mean val. rec. loss:  6.72088480e-03\n",
      "Epoch: 8980 mean train loss:  7.56108287e-03, mean val. rec. loss:  6.71805723e-03\n",
      "Epoch: 8981 mean train loss:  7.55691296e-03, mean val. rec. loss:  6.71524724e-03\n",
      "Epoch: 8982 mean train loss:  7.55275701e-03, mean val. rec. loss:  6.71246559e-03\n",
      "Epoch: 8983 mean train loss:  7.54862248e-03, mean val. rec. loss:  6.70968225e-03\n",
      "Epoch: 8984 mean train loss:  7.54450425e-03, mean val. rec. loss:  6.70690911e-03\n",
      "Epoch: 8985 mean train loss:  7.54039998e-03, mean val. rec. loss:  6.70418870e-03\n",
      "Epoch: 8986 mean train loss:  7.53631759e-03, mean val. rec. loss:  6.70144561e-03\n",
      "Epoch: 8987 mean train loss:  7.53224219e-03, mean val. rec. loss:  6.69871386e-03\n",
      "Epoch: 8988 mean train loss:  7.52818401e-03, mean val. rec. loss:  6.69601216e-03\n",
      "Epoch: 8989 mean train loss:  7.52414725e-03, mean val. rec. loss:  6.69331273e-03\n",
      "Epoch: 8990 mean train loss:  7.52011281e-03, mean val. rec. loss:  6.69062690e-03\n",
      "Epoch: 8991 mean train loss:  7.51610538e-03, mean val. rec. loss:  6.68796036e-03\n",
      "Epoch: 8992 mean train loss:  7.51211191e-03, mean val. rec. loss:  6.68532840e-03\n",
      "Epoch: 8993 mean train loss:  7.50812543e-03, mean val. rec. loss:  6.68267603e-03\n",
      "Epoch: 8994 mean train loss:  7.50416967e-03, mean val. rec. loss:  6.68003046e-03\n",
      "Epoch: 8995 mean train loss:  7.50021717e-03, mean val. rec. loss:  6.67741324e-03\n",
      "Epoch: 8996 mean train loss:  7.49627957e-03, mean val. rec. loss:  6.67482324e-03\n",
      "Epoch: 8997 mean train loss:  7.49236059e-03, mean val. rec. loss:  6.67223437e-03\n",
      "Epoch: 8998 mean train loss:  7.48846069e-03, mean val. rec. loss:  6.66963303e-03\n",
      "Epoch: 8999 mean train loss:  7.48457244e-03, mean val. rec. loss:  6.66708839e-03\n",
      "Epoch: 9000 mean train loss:  7.48070421e-03, mean val. rec. loss:  6.66453750e-03\n",
      "Epoch: 9001 mean train loss:  7.47684202e-03, mean val. rec. loss:  6.66198889e-03\n",
      "Epoch: 9002 mean train loss:  7.47299148e-03, mean val. rec. loss:  6.65947997e-03\n",
      "Epoch: 9003 mean train loss:  7.46916514e-03, mean val. rec. loss:  6.65697331e-03\n",
      "Epoch: 9004 mean train loss:  7.46535231e-03, mean val. rec. loss:  6.65446609e-03\n",
      "Epoch: 9005 mean train loss:  7.46154692e-03, mean val. rec. loss:  6.65196511e-03\n",
      "Epoch: 9006 mean train loss:  7.45777086e-03, mean val. rec. loss:  6.64948850e-03\n",
      "Epoch: 9007 mean train loss:  7.45399201e-03, mean val. rec. loss:  6.64702210e-03\n",
      "Epoch: 9008 mean train loss:  7.45023877e-03, mean val. rec. loss:  6.64457725e-03\n",
      "Epoch: 9009 mean train loss:  7.44649856e-03, mean val. rec. loss:  6.64213296e-03\n",
      "Epoch: 9010 mean train loss:  7.44276300e-03, mean val. rec. loss:  6.63970342e-03\n",
      "Epoch: 9011 mean train loss:  7.43905492e-03, mean val. rec. loss:  6.63729088e-03\n",
      "Epoch: 9012 mean train loss:  7.43535614e-03, mean val. rec. loss:  6.63486984e-03\n",
      "Epoch: 9013 mean train loss:  7.43166575e-03, mean val. rec. loss:  6.63248452e-03\n",
      "Epoch: 9014 mean train loss:  7.42799909e-03, mean val. rec. loss:  6.63010714e-03\n",
      "Epoch: 9015 mean train loss:  7.42434222e-03, mean val. rec. loss:  6.62773033e-03\n",
      "Epoch: 9016 mean train loss:  7.42069512e-03, mean val. rec. loss:  6.62537619e-03\n",
      "Epoch: 9017 mean train loss:  7.41707455e-03, mean val. rec. loss:  6.62302886e-03\n",
      "Epoch: 9018 mean train loss:  7.41345678e-03, mean val. rec. loss:  6.62069173e-03\n",
      "Epoch: 9019 mean train loss:  7.40985996e-03, mean val. rec. loss:  6.61835631e-03\n",
      "Epoch: 9020 mean train loss:  7.40626221e-03, mean val. rec. loss:  6.61606454e-03\n",
      "Epoch: 9021 mean train loss:  7.40269146e-03, mean val. rec. loss:  6.61376200e-03\n",
      "Epoch: 9022 mean train loss:  7.39913235e-03, mean val. rec. loss:  6.61144926e-03\n",
      "Epoch: 9023 mean train loss:  7.39558068e-03, mean val. rec. loss:  6.60918754e-03\n",
      "Epoch: 9024 mean train loss:  7.39205276e-03, mean val. rec. loss:  6.60690711e-03\n",
      "Epoch: 9025 mean train loss:  7.38853043e-03, mean val. rec. loss:  6.60463576e-03\n",
      "Epoch: 9026 mean train loss:  7.38502113e-03, mean val. rec. loss:  6.60240863e-03\n",
      "Epoch: 9027 mean train loss:  7.38153185e-03, mean val. rec. loss:  6.60016902e-03\n",
      "Epoch: 9028 mean train loss:  7.37805002e-03, mean val. rec. loss:  6.59792148e-03\n",
      "Epoch: 9029 mean train loss:  7.37458076e-03, mean val. rec. loss:  6.59571647e-03\n",
      "Epoch: 9030 mean train loss:  7.37112779e-03, mean val. rec. loss:  6.59352222e-03\n",
      "Epoch: 9031 mean train loss:  7.36768973e-03, mean val. rec. loss:  6.59130360e-03\n",
      "Epoch: 9032 mean train loss:  7.36425771e-03, mean val. rec. loss:  6.58913770e-03\n",
      "Epoch: 9033 mean train loss:  7.36084431e-03, mean val. rec. loss:  6.58697407e-03\n",
      "Epoch: 9034 mean train loss:  7.35744209e-03, mean val. rec. loss:  6.58479740e-03\n",
      "Epoch: 9035 mean train loss:  7.35405477e-03, mean val. rec. loss:  6.58262754e-03\n",
      "Epoch: 9036 mean train loss:  7.35067303e-03, mean val. rec. loss:  6.58050757e-03\n",
      "Epoch: 9037 mean train loss:  7.34731364e-03, mean val. rec. loss:  6.57838647e-03\n",
      "Epoch: 9038 mean train loss:  7.34395890e-03, mean val. rec. loss:  6.57624325e-03\n",
      "Epoch: 9039 mean train loss:  7.34062465e-03, mean val. rec. loss:  6.57412952e-03\n",
      "Epoch: 9040 mean train loss:  7.33729319e-03, mean val. rec. loss:  6.57203110e-03\n",
      "Epoch: 9041 mean train loss:  7.33398035e-03, mean val. rec. loss:  6.56992473e-03\n",
      "Epoch: 9042 mean train loss:  7.33067683e-03, mean val. rec. loss:  6.56784502e-03\n",
      "Epoch: 9043 mean train loss:  7.32738959e-03, mean val. rec. loss:  6.56578175e-03\n",
      "Epoch: 9044 mean train loss:  7.32410516e-03, mean val. rec. loss:  6.56370941e-03\n",
      "Epoch: 9045 mean train loss:  7.32083887e-03, mean val. rec. loss:  6.56164104e-03\n",
      "Epoch: 9046 mean train loss:  7.31758470e-03, mean val. rec. loss:  6.55959988e-03\n",
      "Epoch: 9047 mean train loss:  7.31434774e-03, mean val. rec. loss:  6.55756552e-03\n",
      "Epoch: 9048 mean train loss:  7.31111265e-03, mean val. rec. loss:  6.55554251e-03\n",
      "Epoch: 9049 mean train loss:  7.30789665e-03, mean val. rec. loss:  6.55352063e-03\n",
      "Epoch: 9050 mean train loss:  7.30468996e-03, mean val. rec. loss:  6.55150782e-03\n",
      "Epoch: 9051 mean train loss:  7.30148559e-03, mean val. rec. loss:  6.54949955e-03\n",
      "Epoch: 9052 mean train loss:  7.29830637e-03, mean val. rec. loss:  6.54750829e-03\n",
      "Epoch: 9053 mean train loss:  7.29513040e-03, mean val. rec. loss:  6.54553346e-03\n",
      "Epoch: 9054 mean train loss:  7.29197399e-03, mean val. rec. loss:  6.54356431e-03\n",
      "Epoch: 9055 mean train loss:  7.28882130e-03, mean val. rec. loss:  6.54159006e-03\n",
      "Epoch: 9056 mean train loss:  7.28568398e-03, mean val. rec. loss:  6.53963622e-03\n",
      "Epoch: 9057 mean train loss:  7.28255782e-03, mean val. rec. loss:  6.53766934e-03\n",
      "Epoch: 9058 mean train loss:  7.27944331e-03, mean val. rec. loss:  6.53574044e-03\n",
      "Epoch: 9059 mean train loss:  7.27633438e-03, mean val. rec. loss:  6.53381779e-03\n",
      "Epoch: 9060 mean train loss:  7.27324687e-03, mean val. rec. loss:  6.53188549e-03\n",
      "Epoch: 9061 mean train loss:  7.27015098e-03, mean val. rec. loss:  6.52995660e-03\n",
      "Epoch: 9062 mean train loss:  7.26708069e-03, mean val. rec. loss:  6.52806739e-03\n",
      "Epoch: 9063 mean train loss:  7.26402810e-03, mean val. rec. loss:  6.52617592e-03\n",
      "Epoch: 9064 mean train loss:  7.26097271e-03, mean val. rec. loss:  6.52427424e-03\n",
      "Epoch: 9065 mean train loss:  7.25793873e-03, mean val. rec. loss:  6.52239070e-03\n",
      "Epoch: 9066 mean train loss:  7.25490476e-03, mean val. rec. loss:  6.52051964e-03\n",
      "Epoch: 9067 mean train loss:  7.25188755e-03, mean val. rec. loss:  6.51865028e-03\n",
      "Epoch: 9068 mean train loss:  7.24887592e-03, mean val. rec. loss:  6.51678999e-03\n",
      "Epoch: 9069 mean train loss:  7.24587919e-03, mean val. rec. loss:  6.51493934e-03\n",
      "Epoch: 9070 mean train loss:  7.24288804e-03, mean val. rec. loss:  6.51309833e-03\n",
      "Epoch: 9071 mean train loss:  7.23991273e-03, mean val. rec. loss:  6.51124768e-03\n",
      "Epoch: 9072 mean train loss:  7.23694719e-03, mean val. rec. loss:  6.50943389e-03\n",
      "Epoch: 9073 mean train loss:  7.23398724e-03, mean val. rec. loss:  6.50761159e-03\n",
      "Epoch: 9074 mean train loss:  7.23104498e-03, mean val. rec. loss:  6.50579439e-03\n",
      "Epoch: 9075 mean train loss:  7.22810365e-03, mean val. rec. loss:  6.50398967e-03\n",
      "Epoch: 9076 mean train loss:  7.22517954e-03, mean val. rec. loss:  6.50219968e-03\n",
      "Epoch: 9077 mean train loss:  7.22225963e-03, mean val. rec. loss:  6.50038475e-03\n",
      "Epoch: 9078 mean train loss:  7.21935461e-03, mean val. rec. loss:  6.49860951e-03\n",
      "Epoch: 9079 mean train loss:  7.21645565e-03, mean val. rec. loss:  6.49684505e-03\n",
      "Epoch: 9080 mean train loss:  7.21357158e-03, mean val. rec. loss:  6.49507718e-03\n",
      "Epoch: 9081 mean train loss:  7.21068984e-03, mean val. rec. loss:  6.49330250e-03\n",
      "Epoch: 9082 mean train loss:  7.20782439e-03, mean val. rec. loss:  6.49154994e-03\n",
      "Epoch: 9083 mean train loss:  7.20495848e-03, mean val. rec. loss:  6.48979568e-03\n",
      "Epoch: 9084 mean train loss:  7.20210793e-03, mean val. rec. loss:  6.48803858e-03\n",
      "Epoch: 9085 mean train loss:  7.19926716e-03, mean val. rec. loss:  6.48631381e-03\n",
      "Epoch: 9086 mean train loss:  7.19643710e-03, mean val. rec. loss:  6.48458279e-03\n",
      "Epoch: 9087 mean train loss:  7.19361495e-03, mean val. rec. loss:  6.48285234e-03\n",
      "Epoch: 9088 mean train loss:  7.19080165e-03, mean val. rec. loss:  6.48113380e-03\n",
      "Epoch: 9089 mean train loss:  7.18799672e-03, mean val. rec. loss:  6.47943397e-03\n",
      "Epoch: 9090 mean train loss:  7.18520204e-03, mean val. rec. loss:  6.47772847e-03\n",
      "Epoch: 9091 mean train loss:  7.18241667e-03, mean val. rec. loss:  6.47603431e-03\n",
      "Epoch: 9092 mean train loss:  7.17964387e-03, mean val. rec. loss:  6.47435035e-03\n",
      "Epoch: 9093 mean train loss:  7.17686827e-03, mean val. rec. loss:  6.47265449e-03\n",
      "Epoch: 9094 mean train loss:  7.17411409e-03, mean val. rec. loss:  6.47098414e-03\n",
      "Epoch: 9095 mean train loss:  7.17135991e-03, mean val. rec. loss:  6.46931890e-03\n",
      "Epoch: 9096 mean train loss:  7.16862296e-03, mean val. rec. loss:  6.46764118e-03\n",
      "Epoch: 9097 mean train loss:  7.16588600e-03, mean val. rec. loss:  6.46597423e-03\n",
      "Epoch: 9098 mean train loss:  7.16316162e-03, mean val. rec. loss:  6.46432203e-03\n",
      "Epoch: 9099 mean train loss:  7.16044096e-03, mean val. rec. loss:  6.46267606e-03\n",
      "Epoch: 9100 mean train loss:  7.15773985e-03, mean val. rec. loss:  6.46102556e-03\n",
      "Epoch: 9101 mean train loss:  7.15503316e-03, mean val. rec. loss:  6.45938583e-03\n",
      "Epoch: 9102 mean train loss:  7.15234648e-03, mean val. rec. loss:  6.45776935e-03\n",
      "Epoch: 9103 mean train loss:  7.14966353e-03, mean val. rec. loss:  6.45613132e-03\n",
      "Epoch: 9104 mean train loss:  7.14697872e-03, mean val. rec. loss:  6.45450973e-03\n",
      "Epoch: 9105 mean train loss:  7.14431252e-03, mean val. rec. loss:  6.45291082e-03\n",
      "Epoch: 9106 mean train loss:  7.14166077e-03, mean val. rec. loss:  6.45128130e-03\n",
      "Epoch: 9107 mean train loss:  7.13900947e-03, mean val. rec. loss:  6.44967672e-03\n",
      "Epoch: 9108 mean train loss:  7.13635818e-03, mean val. rec. loss:  6.44809312e-03\n",
      "Epoch: 9109 mean train loss:  7.13372597e-03, mean val. rec. loss:  6.44648458e-03\n",
      "Epoch: 9110 mean train loss:  7.13109330e-03, mean val. rec. loss:  6.44490268e-03\n",
      "Epoch: 9111 mean train loss:  7.12847739e-03, mean val. rec. loss:  6.44332475e-03\n",
      "Epoch: 9112 mean train loss:  7.12586054e-03, mean val. rec. loss:  6.44172301e-03\n",
      "Epoch: 9113 mean train loss:  7.12326046e-03, mean val. rec. loss:  6.44016719e-03\n",
      "Epoch: 9114 mean train loss:  7.12066503e-03, mean val. rec. loss:  6.43859494e-03\n",
      "Epoch: 9115 mean train loss:  7.11807147e-03, mean val. rec. loss:  6.43701701e-03\n",
      "Epoch: 9116 mean train loss:  7.11549047e-03, mean val. rec. loss:  6.43547707e-03\n",
      "Epoch: 9117 mean train loss:  7.11291692e-03, mean val. rec. loss:  6.43392919e-03\n",
      "Epoch: 9118 mean train loss:  7.11034617e-03, mean val. rec. loss:  6.43236430e-03\n",
      "Epoch: 9119 mean train loss:  7.10778845e-03, mean val. rec. loss:  6.43082323e-03\n",
      "Epoch: 9120 mean train loss:  7.10522608e-03, mean val. rec. loss:  6.42928669e-03\n",
      "Epoch: 9121 mean train loss:  7.10267907e-03, mean val. rec. loss:  6.42774448e-03\n",
      "Epoch: 9122 mean train loss:  7.10015161e-03, mean val. rec. loss:  6.42622892e-03\n",
      "Epoch: 9123 mean train loss:  7.09761344e-03, mean val. rec. loss:  6.42469749e-03\n",
      "Epoch: 9124 mean train loss:  7.09509110e-03, mean val. rec. loss:  6.42315528e-03\n",
      "Epoch: 9125 mean train loss:  7.09256923e-03, mean val. rec. loss:  6.42166693e-03\n",
      "Epoch: 9126 mean train loss:  7.09005714e-03, mean val. rec. loss:  6.42013607e-03\n",
      "Epoch: 9127 mean train loss:  7.08755110e-03, mean val. rec. loss:  6.41861597e-03\n",
      "Epoch: 9128 mean train loss:  7.08505529e-03, mean val. rec. loss:  6.41713500e-03\n",
      "Epoch: 9129 mean train loss:  7.08255949e-03, mean val. rec. loss:  6.41561320e-03\n",
      "Epoch: 9130 mean train loss:  7.08007626e-03, mean val. rec. loss:  6.41412769e-03\n",
      "Epoch: 9131 mean train loss:  7.07759815e-03, mean val. rec. loss:  6.41264615e-03\n",
      "Epoch: 9132 mean train loss:  7.07512423e-03, mean val. rec. loss:  6.41112946e-03\n",
      "Epoch: 9133 mean train loss:  7.07265590e-03, mean val. rec. loss:  6.40966549e-03\n",
      "Epoch: 9134 mean train loss:  7.07019827e-03, mean val. rec. loss:  6.40819643e-03\n",
      "Epoch: 9135 mean train loss:  7.06774204e-03, mean val. rec. loss:  6.40668313e-03\n",
      "Epoch: 9136 mean train loss:  7.06529373e-03, mean val. rec. loss:  6.40524015e-03\n",
      "Epoch: 9137 mean train loss:  7.06286450e-03, mean val. rec. loss:  6.40376258e-03\n",
      "Epoch: 9138 mean train loss:  7.06042550e-03, mean val. rec. loss:  6.40229011e-03\n",
      "Epoch: 9139 mean train loss:  7.05800046e-03, mean val. rec. loss:  6.40084145e-03\n",
      "Epoch: 9140 mean train loss:  7.05556937e-03, mean val. rec. loss:  6.39936445e-03\n",
      "Epoch: 9141 mean train loss:  7.05315364e-03, mean val. rec. loss:  6.39791239e-03\n",
      "Epoch: 9142 mean train loss:  7.05074723e-03, mean val. rec. loss:  6.39647905e-03\n",
      "Epoch: 9143 mean train loss:  7.04834361e-03, mean val. rec. loss:  6.39501905e-03\n",
      "Epoch: 9144 mean train loss:  7.04594324e-03, mean val. rec. loss:  6.39356983e-03\n",
      "Epoch: 9145 mean train loss:  7.04354753e-03, mean val. rec. loss:  6.39214329e-03\n",
      "Epoch: 9146 mean train loss:  7.04116486e-03, mean val. rec. loss:  6.39069066e-03\n",
      "Epoch: 9147 mean train loss:  7.03878126e-03, mean val. rec. loss:  6.38926525e-03\n",
      "Epoch: 9148 mean train loss:  7.03640417e-03, mean val. rec. loss:  6.38783588e-03\n",
      "Epoch: 9149 mean train loss:  7.03402895e-03, mean val. rec. loss:  6.38638495e-03\n",
      "Epoch: 9150 mean train loss:  7.03166490e-03, mean val. rec. loss:  6.38497996e-03\n",
      "Epoch: 9151 mean train loss:  7.02931388e-03, mean val. rec. loss:  6.38357836e-03\n",
      "Epoch: 9152 mean train loss:  7.02695449e-03, mean val. rec. loss:  6.38212687e-03\n",
      "Epoch: 9153 mean train loss:  7.02460906e-03, mean val. rec. loss:  6.38073208e-03\n",
      "Epoch: 9154 mean train loss:  7.02225758e-03, mean val. rec. loss:  6.37930440e-03\n",
      "Epoch: 9155 mean train loss:  7.01992378e-03, mean val. rec. loss:  6.37787219e-03\n",
      "Epoch: 9156 mean train loss:  7.01758301e-03, mean val. rec. loss:  6.37651142e-03\n",
      "Epoch: 9157 mean train loss:  7.01526784e-03, mean val. rec. loss:  6.37508147e-03\n",
      "Epoch: 9158 mean train loss:  7.01293964e-03, mean val. rec. loss:  6.37368612e-03\n",
      "Epoch: 9159 mean train loss:  7.01062354e-03, mean val. rec. loss:  6.37228849e-03\n",
      "Epoch: 9160 mean train loss:  7.00830372e-03, mean val. rec. loss:  6.37086421e-03\n",
      "Epoch: 9161 mean train loss:  7.00599926e-03, mean val. rec. loss:  6.36950288e-03\n",
      "Epoch: 9162 mean train loss:  7.00369759e-03, mean val. rec. loss:  6.36812453e-03\n",
      "Epoch: 9163 mean train loss:  7.00139639e-03, mean val. rec. loss:  6.36670139e-03\n",
      "Epoch: 9164 mean train loss:  6.99910077e-03, mean val. rec. loss:  6.36533891e-03\n",
      "Epoch: 9165 mean train loss:  6.99681447e-03, mean val. rec. loss:  6.36394129e-03\n",
      "Epoch: 9166 mean train loss:  6.99453282e-03, mean val. rec. loss:  6.36255784e-03\n",
      "Epoch: 9167 mean train loss:  6.99224605e-03, mean val. rec. loss:  6.36120047e-03\n",
      "Epoch: 9168 mean train loss:  6.98997417e-03, mean val. rec. loss:  6.35979547e-03\n",
      "Epoch: 9169 mean train loss:  6.98770929e-03, mean val. rec. loss:  6.35846021e-03\n",
      "Epoch: 9170 mean train loss:  6.98544253e-03, mean val. rec. loss:  6.35706145e-03\n",
      "Epoch: 9171 mean train loss:  6.98318370e-03, mean val. rec. loss:  6.35570295e-03\n",
      "Epoch: 9172 mean train loss:  6.98092393e-03, mean val. rec. loss:  6.35434501e-03\n",
      "Epoch: 9173 mean train loss:  6.97867114e-03, mean val. rec. loss:  6.35295646e-03\n",
      "Epoch: 9174 mean train loss:  6.97642069e-03, mean val. rec. loss:  6.35160816e-03\n",
      "Epoch: 9175 mean train loss:  6.97417395e-03, mean val. rec. loss:  6.35026156e-03\n",
      "Epoch: 9176 mean train loss:  6.97194165e-03, mean val. rec. loss:  6.34889058e-03\n",
      "Epoch: 9177 mean train loss:  6.96970377e-03, mean val. rec. loss:  6.34754739e-03\n",
      "Epoch: 9178 mean train loss:  6.96747612e-03, mean val. rec. loss:  6.34617868e-03\n",
      "Epoch: 9179 mean train loss:  6.96524615e-03, mean val. rec. loss:  6.34482301e-03\n",
      "Epoch: 9180 mean train loss:  6.96302129e-03, mean val. rec. loss:  6.34348265e-03\n",
      "Epoch: 9181 mean train loss:  6.96079784e-03, mean val. rec. loss:  6.34211451e-03\n",
      "Epoch: 9182 mean train loss:  6.95858183e-03, mean val. rec. loss:  6.34076678e-03\n",
      "Epoch: 9183 mean train loss:  6.95637560e-03, mean val. rec. loss:  6.33942982e-03\n",
      "Epoch: 9184 mean train loss:  6.95416798e-03, mean val. rec. loss:  6.33808209e-03\n",
      "Epoch: 9185 mean train loss:  6.95196780e-03, mean val. rec. loss:  6.33677007e-03\n",
      "Epoch: 9186 mean train loss:  6.94976902e-03, mean val. rec. loss:  6.33539910e-03\n",
      "Epoch: 9187 mean train loss:  6.94757023e-03, mean val. rec. loss:  6.33406781e-03\n",
      "Epoch: 9188 mean train loss:  6.94537611e-03, mean val. rec. loss:  6.33275466e-03\n",
      "Epoch: 9189 mean train loss:  6.94318431e-03, mean val. rec. loss:  6.33138482e-03\n",
      "Epoch: 9190 mean train loss:  6.94099577e-03, mean val. rec. loss:  6.33010853e-03\n",
      "Epoch: 9191 mean train loss:  6.93882166e-03, mean val. rec. loss:  6.32870750e-03\n",
      "Epoch: 9192 mean train loss:  6.93664383e-03, mean val. rec. loss:  6.32742724e-03\n",
      "Epoch: 9193 mean train loss:  6.93447252e-03, mean val. rec. loss:  6.32608348e-03\n",
      "Epoch: 9194 mean train loss:  6.93230586e-03, mean val. rec. loss:  6.32474482e-03\n",
      "Epoch: 9195 mean train loss:  6.93013175e-03, mean val. rec. loss:  6.32345776e-03\n",
      "Epoch: 9196 mean train loss:  6.92796509e-03, mean val. rec. loss:  6.32206807e-03\n",
      "Epoch: 9197 mean train loss:  6.92580030e-03, mean val. rec. loss:  6.32079858e-03\n",
      "Epoch: 9198 mean train loss:  6.92364528e-03, mean val. rec. loss:  6.31943327e-03\n",
      "Epoch: 9199 mean train loss:  6.92149352e-03, mean val. rec. loss:  6.31813544e-03\n",
      "Epoch: 9200 mean train loss:  6.91934222e-03, mean val. rec. loss:  6.31682229e-03\n",
      "Epoch: 9201 mean train loss:  6.91719884e-03, mean val. rec. loss:  6.31546549e-03\n",
      "Epoch: 9202 mean train loss:  6.91505313e-03, mean val. rec. loss:  6.31419544e-03\n",
      "Epoch: 9203 mean train loss:  6.91291301e-03, mean val. rec. loss:  6.31285111e-03\n",
      "Epoch: 9204 mean train loss:  6.91076963e-03, mean val. rec. loss:  6.31156801e-03\n",
      "Epoch: 9205 mean train loss:  6.90863369e-03, mean val. rec. loss:  6.31023389e-03\n",
      "Epoch: 9206 mean train loss:  6.90649776e-03, mean val. rec. loss:  6.30891791e-03\n",
      "Epoch: 9207 mean train loss:  6.90436741e-03, mean val. rec. loss:  6.30762801e-03\n",
      "Epoch: 9208 mean train loss:  6.90224452e-03, mean val. rec. loss:  6.30628141e-03\n",
      "Epoch: 9209 mean train loss:  6.90012162e-03, mean val. rec. loss:  6.30501079e-03\n",
      "Epoch: 9210 mean train loss:  6.89800059e-03, mean val. rec. loss:  6.30366249e-03\n",
      "Epoch: 9211 mean train loss:  6.89587862e-03, mean val. rec. loss:  6.30239811e-03\n",
      "Epoch: 9212 mean train loss:  6.89376271e-03, mean val. rec. loss:  6.30105548e-03\n",
      "Epoch: 9213 mean train loss:  6.89165610e-03, mean val. rec. loss:  6.29976785e-03\n",
      "Epoch: 9214 mean train loss:  6.88954438e-03, mean val. rec. loss:  6.29846661e-03\n",
      "Epoch: 9215 mean train loss:  6.88743731e-03, mean val. rec. loss:  6.29713192e-03\n",
      "Epoch: 9216 mean train loss:  6.88532652e-03, mean val. rec. loss:  6.29586640e-03\n",
      "Epoch: 9217 mean train loss:  6.88322690e-03, mean val. rec. loss:  6.29450790e-03\n",
      "Epoch: 9218 mean train loss:  6.88113147e-03, mean val. rec. loss:  6.29328151e-03\n",
      "Epoch: 9219 mean train loss:  6.87903976e-03, mean val. rec. loss:  6.29191563e-03\n",
      "Epoch: 9220 mean train loss:  6.87693456e-03, mean val. rec. loss:  6.29067279e-03\n",
      "Epoch: 9221 mean train loss:  6.87484844e-03, mean val. rec. loss:  6.28933697e-03\n",
      "Epoch: 9222 mean train loss:  6.87276232e-03, mean val. rec. loss:  6.28804650e-03\n",
      "Epoch: 9223 mean train loss:  6.87067387e-03, mean val. rec. loss:  6.28677588e-03\n",
      "Epoch: 9224 mean train loss:  6.86858636e-03, mean val. rec. loss:  6.28541794e-03\n",
      "Epoch: 9225 mean train loss:  6.86649884e-03, mean val. rec. loss:  6.28420062e-03\n",
      "Epoch: 9226 mean train loss:  6.86442855e-03, mean val. rec. loss:  6.28280866e-03\n",
      "Epoch: 9227 mean train loss:  6.86234941e-03, mean val. rec. loss:  6.28162082e-03\n",
      "Epoch: 9228 mean train loss:  6.86026842e-03, mean val. rec. loss:  6.28022660e-03\n",
      "Epoch: 9229 mean train loss:  6.85819440e-03, mean val. rec. loss:  6.27902175e-03\n",
      "Epoch: 9230 mean train loss:  6.85612737e-03, mean val. rec. loss:  6.27765361e-03\n",
      "Epoch: 9231 mean train loss:  6.85406267e-03, mean val. rec. loss:  6.27641530e-03\n",
      "Epoch: 9232 mean train loss:  6.85198819e-03, mean val. rec. loss:  6.27507948e-03\n",
      "Epoch: 9233 mean train loss:  6.84992674e-03, mean val. rec. loss:  6.27382814e-03\n",
      "Epoch: 9234 mean train loss:  6.84786623e-03, mean val. rec. loss:  6.27249628e-03\n",
      "Epoch: 9235 mean train loss:  6.84580804e-03, mean val. rec. loss:  6.27124097e-03\n",
      "Epoch: 9236 mean train loss:  6.84375032e-03, mean val. rec. loss:  6.26993349e-03\n",
      "Epoch: 9237 mean train loss:  6.84168515e-03, mean val. rec. loss:  6.26863963e-03\n",
      "Epoch: 9238 mean train loss:  6.83963162e-03, mean val. rec. loss:  6.26734009e-03\n",
      "Epoch: 9239 mean train loss:  6.83758135e-03, mean val. rec. loss:  6.26603602e-03\n",
      "Epoch: 9240 mean train loss:  6.83553154e-03, mean val. rec. loss:  6.26477447e-03\n",
      "Epoch: 9241 mean train loss:  6.83347941e-03, mean val. rec. loss:  6.26345905e-03\n",
      "Epoch: 9242 mean train loss:  6.83143193e-03, mean val. rec. loss:  6.26220091e-03\n",
      "Epoch: 9243 mean train loss:  6.82938678e-03, mean val. rec. loss:  6.26085148e-03\n",
      "Epoch: 9244 mean train loss:  6.82734768e-03, mean val. rec. loss:  6.25965513e-03\n",
      "Epoch: 9245 mean train loss:  6.82530160e-03, mean val. rec. loss:  6.25824900e-03\n",
      "Epoch: 9246 mean train loss:  6.82326343e-03, mean val. rec. loss:  6.25711559e-03\n",
      "Epoch: 9247 mean train loss:  6.82122340e-03, mean val. rec. loss:  6.25562158e-03\n",
      "Epoch: 9248 mean train loss:  6.81919036e-03, mean val. rec. loss:  6.25461574e-03\n",
      "Epoch: 9249 mean train loss:  6.81715265e-03, mean val. rec. loss:  6.25295219e-03\n",
      "Epoch: 9250 mean train loss:  6.81511914e-03, mean val. rec. loss:  6.25217429e-03\n",
      "Epoch: 9251 mean train loss:  6.81308656e-03, mean val. rec. loss:  6.25018529e-03\n",
      "Epoch: 9252 mean train loss:  6.81106096e-03, mean val. rec. loss:  6.24985417e-03\n",
      "Epoch: 9253 mean train loss:  6.80903350e-03, mean val. rec. loss:  6.24723809e-03\n",
      "Epoch: 9254 mean train loss:  6.80699720e-03, mean val. rec. loss:  6.24781188e-03\n",
      "Epoch: 9255 mean train loss:  6.80497812e-03, mean val. rec. loss:  6.24388719e-03\n",
      "Epoch: 9256 mean train loss:  6.80294973e-03, mean val. rec. loss:  6.24638023e-03\n",
      "Epoch: 9257 mean train loss:  6.80093623e-03, mean val. rec. loss:  6.23967333e-03\n",
      "Epoch: 9258 mean train loss:  6.79892833e-03, mean val. rec. loss:  6.24629802e-03\n",
      "Epoch: 9259 mean train loss:  6.79692926e-03, mean val. rec. loss:  6.23365985e-03\n",
      "Epoch: 9260 mean train loss:  6.79495162e-03, mean val. rec. loss:  6.24883529e-03\n",
      "Epoch: 9261 mean train loss:  6.79301960e-03, mean val. rec. loss:  6.22491292e-03\n",
      "Epoch: 9262 mean train loss:  6.79114809e-03, mean val. rec. loss:  6.25413040e-03\n",
      "Epoch: 9263 mean train loss:  6.78934736e-03, mean val. rec. loss:  6.21629187e-03\n",
      "Epoch: 9264 mean train loss:  6.78752055e-03, mean val. rec. loss:  6.25448703e-03\n",
      "Epoch: 9265 mean train loss:  6.78551404e-03, mean val. rec. loss:  6.21755568e-03\n",
      "Epoch: 9266 mean train loss:  6.78324868e-03, mean val. rec. loss:  6.23974023e-03\n",
      "Epoch: 9267 mean train loss:  6.78092653e-03, mean val. rec. loss:  6.23105114e-03\n",
      "Epoch: 9268 mean train loss:  6.77881387e-03, mean val. rec. loss:  6.22036511e-03\n",
      "Epoch: 9269 mean train loss:  6.77694330e-03, mean val. rec. loss:  6.24136182e-03\n",
      "Epoch: 9270 mean train loss:  6.77514164e-03, mean val. rec. loss:  6.21279185e-03\n",
      "Epoch: 9271 mean train loss:  6.77317051e-03, mean val. rec. loss:  6.23557967e-03\n",
      "Epoch: 9272 mean train loss:  6.77102713e-03, mean val. rec. loss:  6.22000961e-03\n",
      "Epoch: 9273 mean train loss:  6.76886047e-03, mean val. rec. loss:  6.21933829e-03\n",
      "Epoch: 9274 mean train loss:  6.76687165e-03, mean val. rec. loss:  6.22993587e-03\n",
      "Epoch: 9275 mean train loss:  6.76500015e-03, mean val. rec. loss:  6.20955378e-03\n",
      "Epoch: 9276 mean train loss:  6.76308768e-03, mean val. rec. loss:  6.22836418e-03\n",
      "Epoch: 9277 mean train loss:  6.76105696e-03, mean val. rec. loss:  6.21226625e-03\n",
      "Epoch: 9278 mean train loss:  6.75895594e-03, mean val. rec. loss:  6.21608718e-03\n",
      "Epoch: 9279 mean train loss:  6.75692196e-03, mean val. rec. loss:  6.21992739e-03\n",
      "Epoch: 9280 mean train loss:  6.75498529e-03, mean val. rec. loss:  6.20605716e-03\n",
      "Epoch: 9281 mean train loss:  6.75306490e-03, mean val. rec. loss:  6.22043882e-03\n",
      "Epoch: 9282 mean train loss:  6.75109377e-03, mean val. rec. loss:  6.20543744e-03\n",
      "Epoch: 9283 mean train loss:  6.74906585e-03, mean val. rec. loss:  6.21182570e-03\n",
      "Epoch: 9284 mean train loss:  6.74703885e-03, mean val. rec. loss:  6.21055111e-03\n",
      "Epoch: 9285 mean train loss:  6.74505609e-03, mean val. rec. loss:  6.20238251e-03\n",
      "Epoch: 9286 mean train loss:  6.74311242e-03, mean val. rec. loss:  6.21228212e-03\n",
      "Epoch: 9287 mean train loss:  6.74117528e-03, mean val. rec. loss:  6.19917392e-03\n",
      "Epoch: 9288 mean train loss:  6.73917901e-03, mean val. rec. loss:  6.20693656e-03\n",
      "Epoch: 9289 mean train loss:  6.73717250e-03, mean val. rec. loss:  6.20153770e-03\n",
      "Epoch: 9290 mean train loss:  6.73518601e-03, mean val. rec. loss:  6.19872090e-03\n",
      "Epoch: 9291 mean train loss:  6.73321814e-03, mean val. rec. loss:  6.20363612e-03\n",
      "Epoch: 9292 mean train loss:  6.73126656e-03, mean val. rec. loss:  6.19358852e-03\n",
      "Epoch: 9293 mean train loss:  6.72931033e-03, mean val. rec. loss:  6.20114761e-03\n",
      "Epoch: 9294 mean train loss:  6.72733315e-03, mean val. rec. loss:  6.19312132e-03\n",
      "Epoch: 9295 mean train loss:  6.72534852e-03, mean val. rec. loss:  6.19492434e-03\n",
      "Epoch: 9296 mean train loss:  6.72336157e-03, mean val. rec. loss:  6.19449400e-03\n",
      "Epoch: 9297 mean train loss:  6.72139882e-03, mean val. rec. loss:  6.18896643e-03\n",
      "Epoch: 9298 mean train loss:  6.71945003e-03, mean val. rec. loss:  6.19402283e-03\n",
      "Epoch: 9299 mean train loss:  6.71748542e-03, mean val. rec. loss:  6.18591547e-03\n",
      "Epoch: 9300 mean train loss:  6.71552035e-03, mean val. rec. loss:  6.19039411e-03\n",
      "Epoch: 9301 mean train loss:  6.71355713e-03, mean val. rec. loss:  6.18547435e-03\n",
      "Epoch: 9302 mean train loss:  6.71157437e-03, mean val. rec. loss:  6.18512225e-03\n",
      "Epoch: 9303 mean train loss:  6.70961115e-03, mean val. rec. loss:  6.18552708e-03\n",
      "Epoch: 9304 mean train loss:  6.70765771e-03, mean val. rec. loss:  6.18053985e-03\n",
      "Epoch: 9305 mean train loss:  6.70569962e-03, mean val. rec. loss:  6.18409714e-03\n",
      "Epoch: 9306 mean train loss:  6.70374153e-03, mean val. rec. loss:  6.17777011e-03\n",
      "Epoch: 9307 mean train loss:  6.70177878e-03, mean val. rec. loss:  6.18079896e-03\n",
      "Epoch: 9308 mean train loss:  6.69981789e-03, mean val. rec. loss:  6.17654655e-03\n",
      "Epoch: 9309 mean train loss:  6.69784816e-03, mean val. rec. loss:  6.17649099e-03\n",
      "Epoch: 9310 mean train loss:  6.69589146e-03, mean val. rec. loss:  6.17567509e-03\n",
      "Epoch: 9311 mean train loss:  6.69393802e-03, mean val. rec. loss:  6.17243702e-03\n",
      "Epoch: 9312 mean train loss:  6.69197993e-03, mean val. rec. loss:  6.17417144e-03\n",
      "Epoch: 9313 mean train loss:  6.69002370e-03, mean val. rec. loss:  6.16931235e-03\n",
      "Epoch: 9314 mean train loss:  6.68807492e-03, mean val. rec. loss:  6.17161092e-03\n",
      "Epoch: 9315 mean train loss:  6.68610844e-03, mean val. rec. loss:  6.16710166e-03\n",
      "Epoch: 9316 mean train loss:  6.68415408e-03, mean val. rec. loss:  6.16831218e-03\n",
      "Epoch: 9317 mean train loss:  6.68219505e-03, mean val. rec. loss:  6.16544776e-03\n",
      "Epoch: 9318 mean train loss:  6.68024254e-03, mean val. rec. loss:  6.16474469e-03\n",
      "Epoch: 9319 mean train loss:  6.67828911e-03, mean val. rec. loss:  6.16379328e-03\n",
      "Epoch: 9320 mean train loss:  6.67633008e-03, mean val. rec. loss:  6.16132802e-03\n",
      "Epoch: 9321 mean train loss:  6.67437664e-03, mean val. rec. loss:  6.16184852e-03\n",
      "Epoch: 9322 mean train loss:  6.67242507e-03, mean val. rec. loss:  6.15828329e-03\n",
      "Epoch: 9323 mean train loss:  6.67047349e-03, mean val. rec. loss:  6.15950062e-03\n",
      "Epoch: 9324 mean train loss:  6.66851447e-03, mean val. rec. loss:  6.15559237e-03\n",
      "Epoch: 9325 mean train loss:  6.66656289e-03, mean val. rec. loss:  6.15684087e-03\n",
      "Epoch: 9326 mean train loss:  6.66461411e-03, mean val. rec. loss:  6.15316169e-03\n",
      "Epoch: 9327 mean train loss:  6.66265881e-03, mean val. rec. loss:  6.15400366e-03\n",
      "Epoch: 9328 mean train loss:  6.66071003e-03, mean val. rec. loss:  6.15083420e-03\n",
      "Epoch: 9329 mean train loss:  6.65875566e-03, mean val. rec. loss:  6.15110635e-03\n",
      "Epoch: 9330 mean train loss:  6.65680595e-03, mean val. rec. loss:  6.14853450e-03\n",
      "Epoch: 9331 mean train loss:  6.65485623e-03, mean val. rec. loss:  6.14822605e-03\n",
      "Epoch: 9332 mean train loss:  6.65290233e-03, mean val. rec. loss:  6.14616222e-03\n",
      "Epoch: 9333 mean train loss:  6.65096053e-03, mean val. rec. loss:  6.14540189e-03\n",
      "Epoch: 9334 mean train loss:  6.64900616e-03, mean val. rec. loss:  6.14375422e-03\n",
      "Epoch: 9335 mean train loss:  6.64705319e-03, mean val. rec. loss:  6.14263101e-03\n",
      "Epoch: 9336 mean train loss:  6.64510254e-03, mean val. rec. loss:  6.14128725e-03\n",
      "Epoch: 9337 mean train loss:  6.64315330e-03, mean val. rec. loss:  6.13990663e-03\n",
      "Epoch: 9338 mean train loss:  6.64120451e-03, mean val. rec. loss:  6.13879590e-03\n",
      "Epoch: 9339 mean train loss:  6.63925666e-03, mean val. rec. loss:  6.13716525e-03\n",
      "Epoch: 9340 mean train loss:  6.63730881e-03, mean val. rec. loss:  6.13630626e-03\n",
      "Epoch: 9341 mean train loss:  6.63536562e-03, mean val. rec. loss:  6.13443803e-03\n",
      "Epoch: 9342 mean train loss:  6.63342056e-03, mean val. rec. loss:  6.13386424e-03\n",
      "Epoch: 9343 mean train loss:  6.63146898e-03, mean val. rec. loss:  6.13161783e-03\n",
      "Epoch: 9344 mean train loss:  6.62951461e-03, mean val. rec. loss:  6.13149196e-03\n",
      "Epoch: 9345 mean train loss:  6.62757747e-03, mean val. rec. loss:  6.12868594e-03\n",
      "Epoch: 9346 mean train loss:  6.62562310e-03, mean val. rec. loss:  6.12934988e-03\n",
      "Epoch: 9347 mean train loss:  6.62368316e-03, mean val. rec. loss:  6.12543086e-03\n",
      "Epoch: 9348 mean train loss:  6.62173345e-03, mean val. rec. loss:  6.12756670e-03\n",
      "Epoch: 9349 mean train loss:  6.61979212e-03, mean val. rec. loss:  6.12167286e-03\n",
      "Epoch: 9350 mean train loss:  6.61784613e-03, mean val. rec. loss:  6.12660112e-03\n",
      "Epoch: 9351 mean train loss:  6.61592202e-03, mean val. rec. loss:  6.11678996e-03\n",
      "Epoch: 9352 mean train loss:  6.61399325e-03, mean val. rec. loss:  6.12728491e-03\n",
      "Epoch: 9353 mean train loss:  6.61209941e-03, mean val. rec. loss:  6.10976214e-03\n",
      "Epoch: 9354 mean train loss:  6.61024141e-03, mean val. rec. loss:  6.13127877e-03\n",
      "Epoch: 9355 mean train loss:  6.60846348e-03, mean val. rec. loss:  6.09905003e-03\n",
      "Epoch: 9356 mean train loss:  6.60682987e-03, mean val. rec. loss:  6.14052409e-03\n",
      "Epoch: 9357 mean train loss:  6.60541787e-03, mean val. rec. loss:  6.08536066e-03\n",
      "Epoch: 9358 mean train loss:  6.60415204e-03, mean val. rec. loss:  6.14949044e-03\n",
      "Epoch: 9359 mean train loss:  6.60274842e-03, mean val. rec. loss:  6.08056961e-03\n",
      "Epoch: 9360 mean train loss:  6.60053986e-03, mean val. rec. loss:  6.13413866e-03\n",
      "Epoch: 9361 mean train loss:  6.59754406e-03, mean val. rec. loss:  6.10146936e-03\n",
      "Epoch: 9362 mean train loss:  6.59468048e-03, mean val. rec. loss:  6.09791548e-03\n",
      "Epoch: 9363 mean train loss:  6.59283598e-03, mean val. rec. loss:  6.12825106e-03\n",
      "Epoch: 9364 mean train loss:  6.59164324e-03, mean val. rec. loss:  6.08132484e-03\n",
      "Epoch: 9365 mean train loss:  6.59003152e-03, mean val. rec. loss:  6.12322641e-03\n",
      "Epoch: 9366 mean train loss:  6.58761067e-03, mean val. rec. loss:  6.09590324e-03\n",
      "Epoch: 9367 mean train loss:  6.58507763e-03, mean val. rec. loss:  6.09322252e-03\n",
      "Epoch: 9368 mean train loss:  6.58320426e-03, mean val. rec. loss:  6.11769090e-03\n",
      "Epoch: 9369 mean train loss:  6.58175827e-03, mean val. rec. loss:  6.08044034e-03\n",
      "Epoch: 9370 mean train loss:  6.57996405e-03, mean val. rec. loss:  6.10984775e-03\n",
      "Epoch: 9371 mean train loss:  6.57765912e-03, mean val. rec. loss:  6.09440299e-03\n",
      "Epoch: 9372 mean train loss:  6.57545243e-03, mean val. rec. loss:  6.08569745e-03\n",
      "Epoch: 9373 mean train loss:  6.57370616e-03, mean val. rec. loss:  6.10875233e-03\n",
      "Epoch: 9374 mean train loss:  6.57206464e-03, mean val. rec. loss:  6.07891457e-03\n",
      "Epoch: 9375 mean train loss:  6.57007861e-03, mean val. rec. loss:  6.09743411e-03\n",
      "Epoch: 9376 mean train loss:  6.56789799e-03, mean val. rec. loss:  6.09212313e-03\n",
      "Epoch: 9377 mean train loss:  6.56592034e-03, mean val. rec. loss:  6.07902400e-03\n",
      "Epoch: 9378 mean train loss:  6.56418431e-03, mean val. rec. loss:  6.09941857e-03\n",
      "Epoch: 9379 mean train loss:  6.56237985e-03, mean val. rec. loss:  6.07701346e-03\n",
      "Epoch: 9380 mean train loss:  6.56034541e-03, mean val. rec. loss:  6.08701853e-03\n",
      "Epoch: 9381 mean train loss:  6.55830072e-03, mean val. rec. loss:  6.08787922e-03\n",
      "Epoch: 9382 mean train loss:  6.55642270e-03, mean val. rec. loss:  6.07338587e-03\n",
      "Epoch: 9383 mean train loss:  6.55463128e-03, mean val. rec. loss:  6.08987162e-03\n",
      "Epoch: 9384 mean train loss:  6.55272998e-03, mean val. rec. loss:  6.07381678e-03\n",
      "Epoch: 9385 mean train loss:  6.55072068e-03, mean val. rec. loss:  6.07816218e-03\n",
      "Epoch: 9386 mean train loss:  6.54875653e-03, mean val. rec. loss:  6.08164575e-03\n",
      "Epoch: 9387 mean train loss:  6.54689853e-03, mean val. rec. loss:  6.06812875e-03\n",
      "Epoch: 9388 mean train loss:  6.54505124e-03, mean val. rec. loss:  6.08081568e-03\n",
      "Epoch: 9389 mean train loss:  6.54313132e-03, mean val. rec. loss:  6.06933474e-03\n",
      "Epoch: 9390 mean train loss:  6.54116112e-03, mean val. rec. loss:  6.07068247e-03\n",
      "Epoch: 9391 mean train loss:  6.53922537e-03, mean val. rec. loss:  6.07446768e-03\n",
      "Epoch: 9392 mean train loss:  6.53736039e-03, mean val. rec. loss:  6.06300091e-03\n",
      "Epoch: 9393 mean train loss:  6.53548889e-03, mean val. rec. loss:  6.07257507e-03\n",
      "Epoch: 9394 mean train loss:  6.53356338e-03, mean val. rec. loss:  6.06380660e-03\n",
      "Epoch: 9395 mean train loss:  6.53162019e-03, mean val. rec. loss:  6.06407592e-03\n",
      "Epoch: 9396 mean train loss:  6.52970446e-03, mean val. rec. loss:  6.06696076e-03\n",
      "Epoch: 9397 mean train loss:  6.52781992e-03, mean val. rec. loss:  6.05771998e-03\n",
      "Epoch: 9398 mean train loss:  6.52593259e-03, mean val. rec. loss:  6.06498367e-03\n",
      "Epoch: 9399 mean train loss:  6.52402152e-03, mean val. rec. loss:  6.05752267e-03\n",
      "Epoch: 9400 mean train loss:  6.52209229e-03, mean val. rec. loss:  6.05798760e-03\n",
      "Epoch: 9401 mean train loss:  6.52017051e-03, mean val. rec. loss:  6.05934610e-03\n",
      "Epoch: 9402 mean train loss:  6.51828085e-03, mean val. rec. loss:  6.05228653e-03\n",
      "Epoch: 9403 mean train loss:  6.51639026e-03, mean val. rec. loss:  6.05775683e-03\n",
      "Epoch: 9404 mean train loss:  6.51448152e-03, mean val. rec. loss:  6.05094220e-03\n",
      "Epoch: 9405 mean train loss:  6.51256486e-03, mean val. rec. loss:  6.05221906e-03\n",
      "Epoch: 9406 mean train loss:  6.51064261e-03, mean val. rec. loss:  6.05175867e-03\n",
      "Epoch: 9407 mean train loss:  6.50874318e-03, mean val. rec. loss:  6.04685762e-03\n",
      "Epoch: 9408 mean train loss:  6.50684467e-03, mean val. rec. loss:  6.05062752e-03\n",
      "Epoch: 9409 mean train loss:  6.50494617e-03, mean val. rec. loss:  6.04437591e-03\n",
      "Epoch: 9410 mean train loss:  6.50303463e-03, mean val. rec. loss:  6.04647490e-03\n",
      "Epoch: 9411 mean train loss:  6.50111658e-03, mean val. rec. loss:  6.04411736e-03\n",
      "Epoch: 9412 mean train loss:  6.49921249e-03, mean val. rec. loss:  6.04146499e-03\n",
      "Epoch: 9413 mean train loss:  6.49730560e-03, mean val. rec. loss:  6.04329410e-03\n",
      "Epoch: 9414 mean train loss:  6.49540012e-03, mean val. rec. loss:  6.03803641e-03\n",
      "Epoch: 9415 mean train loss:  6.49350255e-03, mean val. rec. loss:  6.04042967e-03\n",
      "Epoch: 9416 mean train loss:  6.49158495e-03, mean val. rec. loss:  6.03659116e-03\n",
      "Epoch: 9417 mean train loss:  6.48967993e-03, mean val. rec. loss:  6.03616082e-03\n",
      "Epoch: 9418 mean train loss:  6.48776840e-03, mean val. rec. loss:  6.03564713e-03\n",
      "Epoch: 9419 mean train loss:  6.48585639e-03, mean val. rec. loss:  6.03220664e-03\n",
      "Epoch: 9420 mean train loss:  6.48396022e-03, mean val. rec. loss:  6.03377380e-03\n",
      "Epoch: 9421 mean train loss:  6.48204914e-03, mean val. rec. loss:  6.02953386e-03\n",
      "Epoch: 9422 mean train loss:  6.48014738e-03, mean val. rec. loss:  6.03053913e-03\n",
      "Epoch: 9423 mean train loss:  6.47822932e-03, mean val. rec. loss:  6.02790830e-03\n",
      "Epoch: 9424 mean train loss:  6.47632244e-03, mean val. rec. loss:  6.02675505e-03\n",
      "Epoch: 9425 mean train loss:  6.47441556e-03, mean val. rec. loss:  6.02636666e-03\n",
      "Epoch: 9426 mean train loss:  6.47251147e-03, mean val. rec. loss:  6.02332874e-03\n",
      "Epoch: 9427 mean train loss:  6.47059853e-03, mean val. rec. loss:  6.02414860e-03\n",
      "Epoch: 9428 mean train loss:  6.46869398e-03, mean val. rec. loss:  6.02069168e-03\n",
      "Epoch: 9429 mean train loss:  6.46677965e-03, mean val. rec. loss:  6.02117362e-03\n",
      "Epoch: 9430 mean train loss:  6.46487975e-03, mean val. rec. loss:  6.01868737e-03\n",
      "Epoch: 9431 mean train loss:  6.46296867e-03, mean val. rec. loss:  6.01782328e-03\n",
      "Epoch: 9432 mean train loss:  6.46106039e-03, mean val. rec. loss:  6.01677549e-03\n",
      "Epoch: 9433 mean train loss:  6.45914699e-03, mean val. rec. loss:  6.01458918e-03\n",
      "Epoch: 9434 mean train loss:  6.45724244e-03, mean val. rec. loss:  6.01459145e-03\n",
      "Epoch: 9435 mean train loss:  6.45532717e-03, mean val. rec. loss:  6.01175027e-03\n",
      "Epoch: 9436 mean train loss:  6.45341936e-03, mean val. rec. loss:  6.01196176e-03\n",
      "Epoch: 9437 mean train loss:  6.45151899e-03, mean val. rec. loss:  6.00929975e-03\n",
      "Epoch: 9438 mean train loss:  6.44960466e-03, mean val. rec. loss:  6.00903723e-03\n",
      "Epoch: 9439 mean train loss:  6.44769871e-03, mean val. rec. loss:  6.00704937e-03\n",
      "Epoch: 9440 mean train loss:  6.44578531e-03, mean val. rec. loss:  6.00599987e-03\n",
      "Epoch: 9441 mean train loss:  6.44387563e-03, mean val. rec. loss:  6.00481544e-03\n",
      "Epoch: 9442 mean train loss:  6.44196316e-03, mean val. rec. loss:  6.00302659e-03\n",
      "Epoch: 9443 mean train loss:  6.44005349e-03, mean val. rec. loss:  6.00245450e-03\n",
      "Epoch: 9444 mean train loss:  6.43815033e-03, mean val. rec. loss:  6.00019051e-03\n",
      "Epoch: 9445 mean train loss:  6.43624158e-03, mean val. rec. loss:  5.99994841e-03\n",
      "Epoch: 9446 mean train loss:  6.43433005e-03, mean val. rec. loss:  5.99750015e-03\n",
      "Epoch: 9447 mean train loss:  6.43242223e-03, mean val. rec. loss:  5.99731021e-03\n",
      "Epoch: 9448 mean train loss:  6.43050883e-03, mean val. rec. loss:  5.99493963e-03\n",
      "Epoch: 9449 mean train loss:  6.42860102e-03, mean val. rec. loss:  5.99457449e-03\n",
      "Epoch: 9450 mean train loss:  6.42668529e-03, mean val. rec. loss:  5.99242447e-03\n",
      "Epoch: 9451 mean train loss:  6.42478073e-03, mean val. rec. loss:  5.99179852e-03\n",
      "Epoch: 9452 mean train loss:  6.42286826e-03, mean val. rec. loss:  5.98991045e-03\n",
      "Epoch: 9453 mean train loss:  6.42095579e-03, mean val. rec. loss:  5.98904522e-03\n",
      "Epoch: 9454 mean train loss:  6.41904239e-03, mean val. rec. loss:  5.98739132e-03\n",
      "Epoch: 9455 mean train loss:  6.41713178e-03, mean val. rec. loss:  5.98629817e-03\n",
      "Epoch: 9456 mean train loss:  6.41522211e-03, mean val. rec. loss:  5.98484838e-03\n",
      "Epoch: 9457 mean train loss:  6.41331802e-03, mean val. rec. loss:  5.98358059e-03\n",
      "Epoch: 9458 mean train loss:  6.41140508e-03, mean val. rec. loss:  5.98230260e-03\n",
      "Epoch: 9459 mean train loss:  6.40949494e-03, mean val. rec. loss:  5.98088286e-03\n",
      "Epoch: 9460 mean train loss:  6.40757968e-03, mean val. rec. loss:  5.97972167e-03\n",
      "Epoch: 9461 mean train loss:  6.40566767e-03, mean val. rec. loss:  5.97818796e-03\n",
      "Epoch: 9462 mean train loss:  6.40375614e-03, mean val. rec. loss:  5.97715491e-03\n",
      "Epoch: 9463 mean train loss:  6.40185065e-03, mean val. rec. loss:  5.97547776e-03\n",
      "Epoch: 9464 mean train loss:  6.39994097e-03, mean val. rec. loss:  5.97460460e-03\n",
      "Epoch: 9465 mean train loss:  6.39802431e-03, mean val. rec. loss:  5.97271823e-03\n",
      "Epoch: 9466 mean train loss:  6.39612162e-03, mean val. rec. loss:  5.97210361e-03\n",
      "Epoch: 9467 mean train loss:  6.39420450e-03, mean val. rec. loss:  5.96988442e-03\n",
      "Epoch: 9468 mean train loss:  6.39228830e-03, mean val. rec. loss:  5.96972850e-03\n",
      "Epoch: 9469 mean train loss:  6.39038142e-03, mean val. rec. loss:  5.96691680e-03\n",
      "Epoch: 9470 mean train loss:  6.38847081e-03, mean val. rec. loss:  5.96752518e-03\n",
      "Epoch: 9471 mean train loss:  6.38655741e-03, mean val. rec. loss:  5.96365776e-03\n",
      "Epoch: 9472 mean train loss:  6.38465099e-03, mean val. rec. loss:  5.96576128e-03\n",
      "Epoch: 9473 mean train loss:  6.38275109e-03, mean val. rec. loss:  5.95986801e-03\n",
      "Epoch: 9474 mean train loss:  6.38084048e-03, mean val. rec. loss:  5.96478776e-03\n",
      "Epoch: 9475 mean train loss:  6.37894571e-03, mean val. rec. loss:  5.95490572e-03\n",
      "Epoch: 9476 mean train loss:  6.37705698e-03, mean val. rec. loss:  5.96557077e-03\n",
      "Epoch: 9477 mean train loss:  6.37520363e-03, mean val. rec. loss:  5.94772198e-03\n",
      "Epoch: 9478 mean train loss:  6.37338753e-03, mean val. rec. loss:  5.96991164e-03\n",
      "Epoch: 9479 mean train loss:  6.37169620e-03, mean val. rec. loss:  5.93635954e-03\n",
      "Epoch: 9480 mean train loss:  6.37018503e-03, mean val. rec. loss:  5.98096961e-03\n",
      "Epoch: 9481 mean train loss:  6.36900626e-03, mean val. rec. loss:  5.92013233e-03\n",
      "Epoch: 9482 mean train loss:  6.36823625e-03, mean val. rec. loss:  5.99681523e-03\n",
      "Epoch: 9483 mean train loss:  6.36762545e-03, mean val. rec. loss:  5.90906472e-03\n",
      "Epoch: 9484 mean train loss:  6.36606866e-03, mean val. rec. loss:  5.98771394e-03\n",
      "Epoch: 9485 mean train loss:  6.36277118e-03, mean val. rec. loss:  5.92915425e-03\n",
      "Epoch: 9486 mean train loss:  6.35868507e-03, mean val. rec. loss:  5.93990095e-03\n",
      "Epoch: 9487 mean train loss:  6.35619067e-03, mean val. rec. loss:  5.96966670e-03\n",
      "Epoch: 9488 mean train loss:  6.35551469e-03, mean val. rec. loss:  5.91289643e-03\n",
      "Epoch: 9489 mean train loss:  6.35461805e-03, mean val. rec. loss:  5.96988896e-03\n",
      "Epoch: 9490 mean train loss:  6.35204124e-03, mean val. rec. loss:  5.93010849e-03\n",
      "Epoch: 9491 mean train loss:  6.34886016e-03, mean val. rec. loss:  5.92988680e-03\n",
      "Epoch: 9492 mean train loss:  6.34696445e-03, mean val. rec. loss:  5.96217451e-03\n",
      "Epoch: 9493 mean train loss:  6.34602404e-03, mean val. rec. loss:  5.91353089e-03\n",
      "Epoch: 9494 mean train loss:  6.34431641e-03, mean val. rec. loss:  5.94998427e-03\n",
      "Epoch: 9495 mean train loss:  6.34162043e-03, mean val. rec. loss:  5.93587873e-03\n",
      "Epoch: 9496 mean train loss:  6.33933319e-03, mean val. rec. loss:  5.91821421e-03\n",
      "Epoch: 9497 mean train loss:  6.33796029e-03, mean val. rec. loss:  5.95290709e-03\n",
      "Epoch: 9498 mean train loss:  6.33646867e-03, mean val. rec. loss:  5.91627681e-03\n",
      "Epoch: 9499 mean train loss:  6.33418656e-03, mean val. rec. loss:  5.93135757e-03\n",
      "Epoch: 9500 mean train loss:  6.33188443e-03, mean val. rec. loss:  5.93840523e-03\n",
      "Epoch: 9501 mean train loss:  6.33023918e-03, mean val. rec. loss:  5.91144040e-03\n",
      "Epoch: 9502 mean train loss:  6.32871172e-03, mean val. rec. loss:  5.93937421e-03\n",
      "Epoch: 9503 mean train loss:  6.32669171e-03, mean val. rec. loss:  5.91989930e-03\n",
      "Epoch 09505: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 9504 mean train loss:  6.32448315e-03, mean val. rec. loss:  5.91718796e-03\n",
      "Epoch: 9505 mean train loss:  6.32266286e-03, mean val. rec. loss:  5.91884980e-03\n",
      "Epoch: 9506 mean train loss:  6.32244405e-03, mean val. rec. loss:  5.92170856e-03\n",
      "Epoch: 9507 mean train loss:  6.32222525e-03, mean val. rec. loss:  5.92466257e-03\n",
      "Epoch: 9508 mean train loss:  6.32204461e-03, mean val. rec. loss:  5.92668445e-03\n",
      "Epoch: 9509 mean train loss:  6.32188260e-03, mean val. rec. loss:  5.92711252e-03\n",
      "Epoch: 9510 mean train loss:  6.32170663e-03, mean val. rec. loss:  5.92590200e-03\n",
      "Epoch: 9511 mean train loss:  6.32150085e-03, mean val. rec. loss:  5.92358302e-03\n",
      "Epoch: 9512 mean train loss:  6.32128810e-03, mean val. rec. loss:  5.92102477e-03\n",
      "Epoch: 9513 mean train loss:  6.32110328e-03, mean val. rec. loss:  5.91902160e-03\n",
      "Epoch: 9514 mean train loss:  6.32093754e-03, mean val. rec. loss:  5.91813994e-03\n",
      "Epoch: 9515 mean train loss:  6.32075132e-03, mean val. rec. loss:  5.91850564e-03\n",
      "Epoch: 9516 mean train loss:  6.32056324e-03, mean val. rec. loss:  5.91982502e-03\n",
      "Epoch: 9517 mean train loss:  6.32036771e-03, mean val. rec. loss:  5.92154753e-03\n",
      "Epoch: 9518 mean train loss:  6.32017684e-03, mean val. rec. loss:  5.92300923e-03\n",
      "Epoch: 9519 mean train loss:  6.31999341e-03, mean val. rec. loss:  5.92370379e-03\n",
      "Epoch: 9520 mean train loss:  6.31980812e-03, mean val. rec. loss:  5.92340726e-03\n",
      "Epoch: 9521 mean train loss:  6.31963214e-03, mean val. rec. loss:  5.92229766e-03\n",
      "Epoch: 9522 mean train loss:  6.31942730e-03, mean val. rec. loss:  5.92077756e-03\n",
      "Epoch: 9523 mean train loss:  6.31924202e-03, mean val. rec. loss:  5.91933685e-03\n",
      "Epoch: 9524 mean train loss:  6.31905300e-03, mean val. rec. loss:  5.91841096e-03\n",
      "Epoch 09526: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 9525 mean train loss:  6.31887982e-03, mean val. rec. loss:  5.91820684e-03\n",
      "Epoch: 9526 mean train loss:  6.31868476e-03, mean val. rec. loss:  5.91825390e-03\n",
      "Epoch: 9527 mean train loss:  6.31867451e-03, mean val. rec. loss:  5.91835482e-03\n",
      "Epoch: 9528 mean train loss:  6.31864612e-03, mean val. rec. loss:  5.91850848e-03\n",
      "Epoch: 9529 mean train loss:  6.31863308e-03, mean val. rec. loss:  5.91869502e-03\n",
      "Epoch: 9530 mean train loss:  6.31860608e-03, mean val. rec. loss:  5.91891501e-03\n",
      "Epoch: 9531 mean train loss:  6.31859630e-03, mean val. rec. loss:  5.91914577e-03\n",
      "Epoch: 9532 mean train loss:  6.31856558e-03, mean val. rec. loss:  5.91939298e-03\n",
      "Epoch: 9533 mean train loss:  6.31855394e-03, mean val. rec. loss:  5.91962544e-03\n",
      "Epoch: 9534 mean train loss:  6.31852787e-03, mean val. rec. loss:  5.91985678e-03\n",
      "Epoch: 9535 mean train loss:  6.31851576e-03, mean val. rec. loss:  5.92007110e-03\n",
      "Epoch: 9536 mean train loss:  6.31848690e-03, mean val. rec. loss:  5.92025934e-03\n",
      "Epoch: 9537 mean train loss:  6.31847852e-03, mean val. rec. loss:  5.92041696e-03\n",
      "Epoch: 9538 mean train loss:  6.31845198e-03, mean val. rec. loss:  5.92055417e-03\n",
      "Epoch: 9539 mean train loss:  6.31844174e-03, mean val. rec. loss:  5.92064943e-03\n",
      "Epoch: 9540 mean train loss:  6.31841520e-03, mean val. rec. loss:  5.92071633e-03\n",
      "Epoch: 9541 mean train loss:  6.31840403e-03, mean val. rec. loss:  5.92074922e-03\n",
      "Epoch: 9542 mean train loss:  6.31838029e-03, mean val. rec. loss:  5.92076396e-03\n",
      "Epoch: 9543 mean train loss:  6.31837005e-03, mean val. rec. loss:  5.92073504e-03\n",
      "Epoch: 9544 mean train loss:  6.31833885e-03, mean val. rec. loss:  5.92069649e-03\n",
      "Epoch: 9545 mean train loss:  6.31833001e-03, mean val. rec. loss:  5.92060974e-03\n",
      "Epoch 09547: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 9546 mean train loss:  6.31830208e-03, mean val. rec. loss:  5.92052412e-03\n",
      "Epoch: 9547 mean train loss:  6.31829416e-03, mean val. rec. loss:  5.92051335e-03\n",
      "Epoch: 9548 mean train loss:  6.31828997e-03, mean val. rec. loss:  5.92050484e-03\n",
      "Epoch: 9549 mean train loss:  6.31828857e-03, mean val. rec. loss:  5.92049520e-03\n",
      "Epoch: 9550 mean train loss:  6.31828764e-03, mean val. rec. loss:  5.92048727e-03\n",
      "Epoch: 9551 mean train loss:  6.31828578e-03, mean val. rec. loss:  5.92047593e-03\n",
      "Epoch: 9552 mean train loss:  6.31828485e-03, mean val. rec. loss:  5.92046345e-03\n",
      "Epoch: 9553 mean train loss:  6.31828392e-03, mean val. rec. loss:  5.92045325e-03\n",
      "Epoch: 9554 mean train loss:  6.31827787e-03, mean val. rec. loss:  5.92043737e-03\n",
      "Epoch: 9555 mean train loss:  6.31827694e-03, mean val. rec. loss:  5.92042603e-03\n",
      "Epoch: 9556 mean train loss:  6.31827461e-03, mean val. rec. loss:  5.92041526e-03\n",
      "Epoch: 9557 mean train loss:  6.31827368e-03, mean val. rec. loss:  5.92040335e-03\n",
      "Epoch: 9558 mean train loss:  6.31827321e-03, mean val. rec. loss:  5.92039201e-03\n",
      "Epoch: 9559 mean train loss:  6.31827181e-03, mean val. rec. loss:  5.92037954e-03\n",
      "Epoch: 9560 mean train loss:  6.31826856e-03, mean val. rec. loss:  5.92036536e-03\n",
      "Epoch: 9561 mean train loss:  6.31826530e-03, mean val. rec. loss:  5.92035573e-03\n",
      "Epoch: 9562 mean train loss:  6.31826437e-03, mean val. rec. loss:  5.92034439e-03\n",
      "Epoch: 9563 mean train loss:  6.31826250e-03, mean val. rec. loss:  5.92033191e-03\n",
      "Epoch: 9564 mean train loss:  6.31826157e-03, mean val. rec. loss:  5.92031830e-03\n",
      "Epoch: 9565 mean train loss:  6.31825971e-03, mean val. rec. loss:  5.92030810e-03\n",
      "Epoch: 9566 mean train loss:  6.31825878e-03, mean val. rec. loss:  5.92029449e-03\n",
      "Epoch 09568: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 9567 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92028032e-03\n",
      "Epoch: 9568 mean train loss:  6.31825319e-03, mean val. rec. loss:  5.92028032e-03\n",
      "Epoch: 9569 mean train loss:  6.31825319e-03, mean val. rec. loss:  5.92027975e-03\n",
      "Epoch: 9570 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027861e-03\n",
      "Epoch: 9571 mean train loss:  6.31825273e-03, mean val. rec. loss:  5.92027861e-03\n",
      "Epoch: 9572 mean train loss:  6.31825319e-03, mean val. rec. loss:  5.92027861e-03\n",
      "Epoch: 9573 mean train loss:  6.31825319e-03, mean val. rec. loss:  5.92027748e-03\n",
      "Epoch: 9574 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027805e-03\n",
      "Epoch: 9575 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027748e-03\n",
      "Epoch: 9576 mean train loss:  6.31825319e-03, mean val. rec. loss:  5.92027521e-03\n",
      "Epoch: 9577 mean train loss:  6.31825319e-03, mean val. rec. loss:  5.92027635e-03\n",
      "Epoch: 9578 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027748e-03\n",
      "Epoch: 9579 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027748e-03\n",
      "Epoch: 9580 mean train loss:  6.31825506e-03, mean val. rec. loss:  5.92027748e-03\n",
      "Epoch: 9581 mean train loss:  6.31825506e-03, mean val. rec. loss:  5.92027691e-03\n",
      "Epoch: 9582 mean train loss:  6.31825506e-03, mean val. rec. loss:  5.92027521e-03\n",
      "Epoch: 9583 mean train loss:  6.31825506e-03, mean val. rec. loss:  5.92027465e-03\n",
      "Epoch: 9584 mean train loss:  6.31825506e-03, mean val. rec. loss:  5.92027465e-03\n",
      "Epoch: 9585 mean train loss:  6.31825459e-03, mean val. rec. loss:  5.92027521e-03\n",
      "Epoch: 9586 mean train loss:  6.31825506e-03, mean val. rec. loss:  5.92027521e-03\n",
      "Epoch: 9587 mean train loss:  6.31825459e-03, mean val. rec. loss:  5.92027408e-03\n",
      "Epoch 09589: reducing learning rate of group 0 to 1.0000e-09.\n",
      "Epoch: 9588 mean train loss:  6.31825506e-03, mean val. rec. loss:  5.92027408e-03\n",
      "Epoch: 9589 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027408e-03\n",
      "Epoch: 9590 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027351e-03\n",
      "Epoch: 9591 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027351e-03\n",
      "Epoch: 9592 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027408e-03\n",
      "Epoch: 9593 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027408e-03\n",
      "Epoch: 9594 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027408e-03\n",
      "Epoch: 9595 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027295e-03\n",
      "Epoch: 9596 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027295e-03\n",
      "Epoch: 9597 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027295e-03\n",
      "Epoch: 9598 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027295e-03\n",
      "Epoch: 9599 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027295e-03\n",
      "Epoch: 9600 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9601 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9602 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9603 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027295e-03\n",
      "Epoch: 9604 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9605 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9606 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9607 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9608 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch 09610: reducing learning rate of group 0 to 1.0000e-10.\n",
      "Epoch: 9609 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9610 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9611 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9612 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9613 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9614 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9615 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9616 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9617 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9618 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9619 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9620 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9621 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9622 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9623 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9624 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9625 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9626 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9627 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9628 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9629 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9630 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9631 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9632 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9633 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9634 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9635 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9636 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9637 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9638 mean train loss:  6.31825459e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9639 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9640 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9641 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9642 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9643 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9644 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9645 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9646 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9647 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9648 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9649 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9650 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9651 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9652 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9653 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9654 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9655 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9656 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9657 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9658 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9659 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9660 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9661 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9662 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9663 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9664 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9665 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9666 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9667 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9668 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9669 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9670 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9671 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9672 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9673 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9674 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9675 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9676 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9677 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9678 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9679 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9680 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9681 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9682 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9683 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9684 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9685 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9686 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9687 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9688 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9689 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9690 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9691 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9692 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9693 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9694 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9695 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9696 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9697 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9698 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9699 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9700 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9701 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9702 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9703 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9704 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9705 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9706 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9707 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9708 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9709 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9710 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9711 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9712 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9713 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9714 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9715 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9716 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9717 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027238e-03\n",
      "Epoch: 9718 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9719 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9720 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9721 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9722 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9723 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9724 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9725 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9726 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9727 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9728 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9729 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9730 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9731 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9732 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9733 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9734 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9735 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9736 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9737 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9738 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9739 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9740 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9741 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9742 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9743 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9744 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9745 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9746 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9747 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9748 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9749 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9750 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9751 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9752 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9753 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9754 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9755 mean train loss:  6.31825459e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9756 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9757 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9758 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9759 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9760 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9761 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9762 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9763 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9764 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9765 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9766 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9767 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9768 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9769 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9770 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9771 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9772 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9773 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9774 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9775 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9776 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9777 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9778 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9779 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9780 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9781 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9782 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9783 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9784 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9785 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9786 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9787 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9788 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9789 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9790 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9791 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9792 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9793 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9794 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9795 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9796 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9797 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9798 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9799 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9800 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9801 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9802 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9803 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9804 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9805 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9806 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9807 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9808 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9809 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9810 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9811 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9812 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9813 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9814 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9815 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9816 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9817 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9818 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9819 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9820 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9821 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9822 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9823 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9824 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9825 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9826 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9827 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9828 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9829 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9830 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9831 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9832 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9833 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9834 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9835 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9836 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9837 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9838 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9839 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9840 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9841 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9842 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9843 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9844 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9845 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9846 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9847 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9848 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9849 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9850 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9851 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9852 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9853 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9854 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9855 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9856 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9857 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9858 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9859 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9860 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9861 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9862 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9863 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9864 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9865 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9866 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9867 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9868 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9869 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9870 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9871 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9872 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9873 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9874 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9875 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9876 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9877 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9878 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9879 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9880 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9881 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9882 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9883 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9884 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9885 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9886 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9887 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9888 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9889 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9890 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9891 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9892 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9893 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9894 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9895 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9896 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9897 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9898 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9899 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9900 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9901 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9902 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9903 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9904 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9905 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9906 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9907 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9908 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9909 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9910 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9911 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9912 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9913 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9914 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9915 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9916 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9917 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9918 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9919 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9920 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9921 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9922 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9923 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9924 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9925 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9926 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9927 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9928 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9929 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9930 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9931 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9932 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9933 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9934 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9935 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9936 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9937 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9938 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9939 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9940 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9941 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9942 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9943 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9944 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9945 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9946 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9947 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9948 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9949 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9950 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9951 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9952 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9953 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9954 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9955 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9956 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9957 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9958 mean train loss:  6.31825366e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9959 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9960 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9961 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9962 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9963 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9964 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9965 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9966 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9967 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9968 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9969 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9970 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9971 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9972 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9973 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9974 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9975 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9976 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9977 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9978 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9979 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9980 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9981 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9982 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9983 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9984 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9985 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9986 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9987 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9988 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9989 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9990 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9991 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9992 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9993 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9994 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9995 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9996 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9997 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9998 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n",
      "Epoch: 9999 mean train loss:  6.31825412e-03, mean val. rec. loss:  5.92027181e-03\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = train(net_H1, loaders, args, A, H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NN_library/training_data/PINN_H1_{total_params}', np.vstack([losses_train, losses_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 391\n"
     ]
    }
   ],
   "source": [
    "args = {'lr' : 0.0005, 'epochs' : 10000, 'dev' : dev, 'name' : f'NN_library/PINN/PINN_H2_{total_params}'}\n",
    "net_H2 = PINN(n_periodic=5, n_hidden=10, n_layers=2, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_H2.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_H2 = load_network(net_H2, args['name']+'_4999', args)\n",
    "net_H2 = net_H2.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 mean train loss:  1.00708445e+01, mean val. rec. loss:  9.40222705e+00\n",
      "Epoch: 1 mean train loss:  9.33938324e+00, mean val. rec. loss:  8.70565698e+00\n",
      "Epoch: 2 mean train loss:  8.64697111e+00, mean val. rec. loss:  8.04798800e+00\n",
      "Epoch: 3 mean train loss:  7.99333735e+00, mean val. rec. loss:  7.42878061e+00\n",
      "Epoch: 4 mean train loss:  7.37803671e+00, mean val. rec. loss:  6.84742982e+00\n",
      "Epoch: 5 mean train loss:  6.80046089e+00, mean val. rec. loss:  6.30319305e+00\n",
      "Epoch: 6 mean train loss:  6.25986431e+00, mean val. rec. loss:  5.79521159e+00\n",
      "Epoch: 7 mean train loss:  5.75538885e+00, mean val. rec. loss:  5.32253212e+00\n",
      "Epoch: 8 mean train loss:  5.28608300e+00, mean val. rec. loss:  4.88412291e+00\n",
      "Epoch: 9 mean train loss:  4.85091559e+00, mean val. rec. loss:  4.47887618e+00\n",
      "Epoch: 10 mean train loss:  4.44878017e+00, mean val. rec. loss:  4.10560896e+00\n",
      "Epoch: 11 mean train loss:  4.07849351e+00, mean val. rec. loss:  3.76306486e+00\n",
      "Epoch: 12 mean train loss:  3.73880254e+00, mean val. rec. loss:  3.44992127e+00\n",
      "Epoch: 13 mean train loss:  3.42838510e+00, mean val. rec. loss:  3.16478766e+00\n",
      "Epoch: 14 mean train loss:  3.14585320e+00, mean val. rec. loss:  2.90622097e+00\n",
      "Epoch: 15 mean train loss:  2.88976428e+00, mean val. rec. loss:  2.67272615e+00\n",
      "Epoch: 16 mean train loss:  2.65862502e+00, mean val. rec. loss:  2.46276812e+00\n",
      "Epoch: 17 mean train loss:  2.45090253e+00, mean val. rec. loss:  2.27478768e+00\n",
      "Epoch: 18 mean train loss:  2.26503792e+00, mean val. rec. loss:  2.10721199e+00\n",
      "Epoch: 19 mean train loss:  2.09946183e+00, mean val. rec. loss:  1.95847345e+00\n",
      "Epoch: 20 mean train loss:  1.95260812e+00, mean val. rec. loss:  1.82701635e+00\n",
      "Epoch: 21 mean train loss:  1.82292386e+00, mean val. rec. loss:  1.71130574e+00\n",
      "Epoch: 22 mean train loss:  1.70887592e+00, mean val. rec. loss:  1.60983686e+00\n",
      "Epoch: 23 mean train loss:  1.60896180e+00, mean val. rec. loss:  1.52114502e+00\n",
      "Epoch: 24 mean train loss:  1.52171868e+00, mean val. rec. loss:  1.44381282e+00\n",
      "Epoch: 25 mean train loss:  1.44573154e+00, mean val. rec. loss:  1.37648168e+00\n",
      "Epoch: 26 mean train loss:  1.37964374e+00, mean val. rec. loss:  1.31785919e+00\n",
      "Epoch: 27 mean train loss:  1.32216528e+00, mean val. rec. loss:  1.26673075e+00\n",
      "Epoch: 28 mean train loss:  1.27208433e+00, mean val. rec. loss:  1.22196578e+00\n",
      "Epoch: 29 mean train loss:  1.22827297e+00, mean val. rec. loss:  1.18252531e+00\n",
      "Epoch: 30 mean train loss:  1.18969387e+00, mean val. rec. loss:  1.14746645e+00\n",
      "Epoch: 31 mean train loss:  1.15540811e+00, mean val. rec. loss:  1.11594771e+00\n",
      "Epoch: 32 mean train loss:  1.12457715e+00, mean val. rec. loss:  1.08723034e+00\n",
      "Epoch: 33 mean train loss:  1.09646469e+00, mean val. rec. loss:  1.06067678e+00\n",
      "Epoch: 34 mean train loss:  1.07043680e+00, mean val. rec. loss:  1.03574875e+00\n",
      "Epoch: 35 mean train loss:  1.04595934e+00, mean val. rec. loss:  1.01200513e+00\n",
      "Epoch: 36 mean train loss:  1.02259382e+00, mean val. rec. loss:  9.89092343e-01\n",
      "Epoch: 37 mean train loss:  9.99991300e-01, mean val. rec. loss:  9.66740557e-01\n",
      "Epoch: 38 mean train loss:  9.77884804e-01, mean val. rec. loss:  9.44752443e-01\n",
      "Epoch: 39 mean train loss:  9.56081443e-01, mean val. rec. loss:  9.22996639e-01\n",
      "Epoch: 40 mean train loss:  9.34453933e-01, mean val. rec. loss:  9.01397378e-01\n",
      "Epoch: 41 mean train loss:  9.12930407e-01, mean val. rec. loss:  8.79924761e-01\n",
      "Epoch: 42 mean train loss:  8.91485243e-01, mean val. rec. loss:  8.58586986e-01\n",
      "Epoch: 43 mean train loss:  8.70130417e-01, mean val. rec. loss:  8.37420850e-01\n",
      "Epoch: 44 mean train loss:  8.48906450e-01, mean val. rec. loss:  8.16482744e-01\n",
      "Epoch: 45 mean train loss:  8.27874245e-01, mean val. rec. loss:  7.95843426e-01\n",
      "Epoch: 46 mean train loss:  8.07108526e-01, mean val. rec. loss:  7.75580335e-01\n",
      "Epoch: 47 mean train loss:  7.86690695e-01, mean val. rec. loss:  7.55772576e-01\n",
      "Epoch: 48 mean train loss:  7.66703104e-01, mean val. rec. loss:  7.36496353e-01\n",
      "Epoch: 49 mean train loss:  7.47226021e-01, mean val. rec. loss:  7.17820466e-01\n",
      "Epoch: 50 mean train loss:  7.28332088e-01, mean val. rec. loss:  6.99805442e-01\n",
      "Epoch: 51 mean train loss:  7.10084885e-01, mean val. rec. loss:  6.82500197e-01\n",
      "Epoch: 52 mean train loss:  6.92536912e-01, mean val. rec. loss:  6.65940873e-01\n",
      "Epoch: 53 mean train loss:  6.75727500e-01, mean val. rec. loss:  6.50151419e-01\n",
      "Epoch: 54 mean train loss:  6.59683462e-01, mean val. rec. loss:  6.35142867e-01\n",
      "Epoch: 55 mean train loss:  6.44418982e-01, mean val. rec. loss:  6.20914346e-01\n",
      "Epoch: 56 mean train loss:  6.29935609e-01, mean val. rec. loss:  6.07453663e-01\n",
      "Epoch: 57 mean train loss:  6.16223690e-01, mean val. rec. loss:  5.94738682e-01\n",
      "Epoch: 58 mean train loss:  6.03263380e-01, mean val. rec. loss:  5.82739359e-01\n",
      "Epoch: 59 mean train loss:  5.91026792e-01, mean val. rec. loss:  5.71418608e-01\n",
      "Epoch: 60 mean train loss:  5.79478529e-01, mean val. rec. loss:  5.60734444e-01\n",
      "Epoch: 61 mean train loss:  5.68578309e-01, mean val. rec. loss:  5.50641326e-01\n",
      "Epoch: 62 mean train loss:  5.58282213e-01, mean val. rec. loss:  5.41091865e-01\n",
      "Epoch: 63 mean train loss:  5.48543761e-01, mean val. rec. loss:  5.32038377e-01\n",
      "Epoch: 64 mean train loss:  5.39316473e-01, mean val. rec. loss:  5.23433508e-01\n",
      "Epoch: 65 mean train loss:  5.30553868e-01, mean val. rec. loss:  5.15232517e-01\n",
      "Epoch: 66 mean train loss:  5.22211374e-01, mean val. rec. loss:  5.07392729e-01\n",
      "Epoch: 67 mean train loss:  5.14247039e-01, mean val. rec. loss:  4.99875498e-01\n",
      "Epoch: 68 mean train loss:  5.06622307e-01, mean val. rec. loss:  4.92645808e-01\n",
      "Epoch: 69 mean train loss:  4.99302081e-01, mean val. rec. loss:  4.85672850e-01\n",
      "Epoch: 70 mean train loss:  4.92255492e-01, mean val. rec. loss:  4.78930461e-01\n",
      "Epoch: 71 mean train loss:  4.85456024e-01, mean val. rec. loss:  4.72396506e-01\n",
      "Epoch: 72 mean train loss:  4.78881091e-01, mean val. rec. loss:  4.66053240e-01\n",
      "Epoch: 73 mean train loss:  4.72512221e-01, mean val. rec. loss:  4.59886476e-01\n",
      "Epoch: 74 mean train loss:  4.66334635e-01, mean val. rec. loss:  4.53885581e-01\n",
      "Epoch: 75 mean train loss:  4.60336891e-01, mean val. rec. loss:  4.48043043e-01\n",
      "Epoch: 76 mean train loss:  4.54510500e-01, mean val. rec. loss:  4.42353783e-01\n",
      "Epoch: 77 mean train loss:  4.48849530e-01, mean val. rec. loss:  4.36814752e-01\n",
      "Epoch: 78 mean train loss:  4.43349781e-01, mean val. rec. loss:  4.31424498e-01\n",
      "Epoch: 79 mean train loss:  4.38008840e-01, mean val. rec. loss:  4.26182587e-01\n",
      "Epoch: 80 mean train loss:  4.32825157e-01, mean val. rec. loss:  4.21089599e-01\n",
      "Epoch: 81 mean train loss:  4.27798136e-01, mean val. rec. loss:  4.16145824e-01\n",
      "Epoch: 82 mean train loss:  4.22927093e-01, mean val. rec. loss:  4.11352205e-01\n",
      "Epoch: 83 mean train loss:  4.18211818e-01, mean val. rec. loss:  4.06708889e-01\n",
      "Epoch: 84 mean train loss:  4.13651507e-01, mean val. rec. loss:  4.02215946e-01\n",
      "Epoch: 85 mean train loss:  4.09245058e-01, mean val. rec. loss:  3.97872580e-01\n",
      "Epoch: 86 mean train loss:  4.04990802e-01, mean val. rec. loss:  3.93677484e-01\n",
      "Epoch: 87 mean train loss:  4.00886415e-01, mean val. rec. loss:  3.89628334e-01\n",
      "Epoch: 88 mean train loss:  3.96928738e-01, mean val. rec. loss:  3.85722375e-01\n",
      "Epoch: 89 mean train loss:  3.93114257e-01, mean val. rec. loss:  3.81956121e-01\n",
      "Epoch: 90 mean train loss:  3.89438621e-01, mean val. rec. loss:  3.78325291e-01\n",
      "Epoch: 91 mean train loss:  3.85897003e-01, mean val. rec. loss:  3.74825386e-01\n",
      "Epoch: 92 mean train loss:  3.82484278e-01, mean val. rec. loss:  3.71451288e-01\n",
      "Epoch: 93 mean train loss:  3.79194666e-01, mean val. rec. loss:  3.68197447e-01\n",
      "Epoch: 94 mean train loss:  3.76022447e-01, mean val. rec. loss:  3.65058600e-01\n",
      "Epoch: 95 mean train loss:  3.72961721e-01, mean val. rec. loss:  3.62029050e-01\n",
      "Epoch: 96 mean train loss:  3.70006529e-01, mean val. rec. loss:  3.59103208e-01\n",
      "Epoch: 97 mean train loss:  3.67151090e-01, mean val. rec. loss:  3.56275488e-01\n",
      "Epoch: 98 mean train loss:  3.64389745e-01, mean val. rec. loss:  3.53540663e-01\n",
      "Epoch: 99 mean train loss:  3.61717070e-01, mean val. rec. loss:  3.50893799e-01\n",
      "Epoch: 100 mean train loss:  3.59127999e-01, mean val. rec. loss:  3.48330104e-01\n",
      "Epoch: 101 mean train loss:  3.56617885e-01, mean val. rec. loss:  3.45845008e-01\n",
      "Epoch: 102 mean train loss:  3.54182259e-01, mean val. rec. loss:  3.43434373e-01\n",
      "Epoch: 103 mean train loss:  3.51817009e-01, mean val. rec. loss:  3.41094426e-01\n",
      "Epoch: 104 mean train loss:  3.49518560e-01, mean val. rec. loss:  3.38821611e-01\n",
      "Epoch: 105 mean train loss:  3.47283514e-01, mean val. rec. loss:  3.36612588e-01\n",
      "Epoch: 106 mean train loss:  3.45108833e-01, mean val. rec. loss:  3.34464383e-01\n",
      "Epoch: 107 mean train loss:  3.42991746e-01, mean val. rec. loss:  3.32374237e-01\n",
      "Epoch: 108 mean train loss:  3.40929750e-01, mean val. rec. loss:  3.30339466e-01\n",
      "Epoch: 109 mean train loss:  3.38920461e-01, mean val. rec. loss:  3.28357638e-01\n",
      "Epoch: 110 mean train loss:  3.36961794e-01, mean val. rec. loss:  3.26426503e-01\n",
      "Epoch: 111 mean train loss:  3.35051753e-01, mean val. rec. loss:  3.24543775e-01\n",
      "Epoch: 112 mean train loss:  3.33188430e-01, mean val. rec. loss:  3.22707495e-01\n",
      "Epoch: 113 mean train loss:  3.31370008e-01, mean val. rec. loss:  3.20915522e-01\n",
      "Epoch: 114 mean train loss:  3.29594730e-01, mean val. rec. loss:  3.19165932e-01\n",
      "Epoch: 115 mean train loss:  3.27861016e-01, mean val. rec. loss:  3.17456657e-01\n",
      "Epoch: 116 mean train loss:  3.26167018e-01, mean val. rec. loss:  3.15786028e-01\n",
      "Epoch: 117 mean train loss:  3.24511278e-01, mean val. rec. loss:  3.14152086e-01\n",
      "Epoch: 118 mean train loss:  3.22892125e-01, mean val. rec. loss:  3.12553016e-01\n",
      "Epoch: 119 mean train loss:  3.21307953e-01, mean val. rec. loss:  3.10987148e-01\n",
      "Epoch: 120 mean train loss:  3.19757269e-01, mean val. rec. loss:  3.09452814e-01\n",
      "Epoch: 121 mean train loss:  3.18238586e-01, mean val. rec. loss:  3.07948418e-01\n",
      "Epoch: 122 mean train loss:  3.16750383e-01, mean val. rec. loss:  3.06472397e-01\n",
      "Epoch: 123 mean train loss:  3.15291290e-01, mean val. rec. loss:  3.05023230e-01\n",
      "Epoch: 124 mean train loss:  3.13859935e-01, mean val. rec. loss:  3.03599663e-01\n",
      "Epoch: 125 mean train loss:  3.12455009e-01, mean val. rec. loss:  3.02200372e-01\n",
      "Epoch: 126 mean train loss:  3.11075320e-01, mean val. rec. loss:  3.00824033e-01\n",
      "Epoch: 127 mean train loss:  3.09719556e-01, mean val. rec. loss:  2.99469720e-01\n",
      "Epoch: 128 mean train loss:  3.08386794e-01, mean val. rec. loss:  2.98136091e-01\n",
      "Epoch: 129 mean train loss:  3.07075752e-01, mean val. rec. loss:  2.96822474e-01\n",
      "Epoch: 130 mean train loss:  3.05785626e-01, mean val. rec. loss:  2.95527890e-01\n",
      "Epoch: 131 mean train loss:  3.04515463e-01, mean val. rec. loss:  2.94251559e-01\n",
      "Epoch: 132 mean train loss:  3.03264340e-01, mean val. rec. loss:  2.92992700e-01\n",
      "Epoch: 133 mean train loss:  3.02031569e-01, mean val. rec. loss:  2.91750696e-01\n",
      "Epoch: 134 mean train loss:  3.00816378e-01, mean val. rec. loss:  2.90524913e-01\n",
      "Epoch: 135 mean train loss:  2.99618051e-01, mean val. rec. loss:  2.89314733e-01\n",
      "Epoch: 136 mean train loss:  2.98435903e-01, mean val. rec. loss:  2.88119757e-01\n",
      "Epoch: 137 mean train loss:  2.97269397e-01, mean val. rec. loss:  2.86939478e-01\n",
      "Epoch: 138 mean train loss:  2.96117967e-01, mean val. rec. loss:  2.85773387e-01\n",
      "Epoch: 139 mean train loss:  2.94981048e-01, mean val. rec. loss:  2.84621122e-01\n",
      "Epoch: 140 mean train loss:  2.93858132e-01, mean val. rec. loss:  2.83482355e-01\n",
      "Epoch: 141 mean train loss:  2.92748803e-01, mean val. rec. loss:  2.82356580e-01\n",
      "Epoch: 142 mean train loss:  2.91652524e-01, mean val. rec. loss:  2.81243541e-01\n",
      "Epoch: 143 mean train loss:  2.90568938e-01, mean val. rec. loss:  2.80142858e-01\n",
      "Epoch: 144 mean train loss:  2.89497567e-01, mean val. rec. loss:  2.79054258e-01\n",
      "Epoch: 145 mean train loss:  2.88438026e-01, mean val. rec. loss:  2.77977362e-01\n",
      "Epoch: 146 mean train loss:  2.87389985e-01, mean val. rec. loss:  2.76911895e-01\n",
      "Epoch: 147 mean train loss:  2.86353057e-01, mean val. rec. loss:  2.75857533e-01\n",
      "Epoch: 148 mean train loss:  2.85326916e-01, mean val. rec. loss:  2.74814093e-01\n",
      "Epoch: 149 mean train loss:  2.84311203e-01, mean val. rec. loss:  2.73781213e-01\n",
      "Epoch: 150 mean train loss:  2.83305680e-01, mean val. rec. loss:  2.72758638e-01\n",
      "Epoch: 151 mean train loss:  2.82309989e-01, mean val. rec. loss:  2.71746133e-01\n",
      "Epoch: 152 mean train loss:  2.81323892e-01, mean val. rec. loss:  2.70743426e-01\n",
      "Epoch: 153 mean train loss:  2.80347121e-01, mean val. rec. loss:  2.69750244e-01\n",
      "Epoch: 154 mean train loss:  2.79379407e-01, mean val. rec. loss:  2.68766405e-01\n",
      "Epoch: 155 mean train loss:  2.78420513e-01, mean val. rec. loss:  2.67791712e-01\n",
      "Epoch: 156 mean train loss:  2.77470319e-01, mean val. rec. loss:  2.66825854e-01\n",
      "Epoch: 157 mean train loss:  2.76528498e-01, mean val. rec. loss:  2.65868760e-01\n",
      "Epoch: 158 mean train loss:  2.75594930e-01, mean val. rec. loss:  2.64920066e-01\n",
      "Epoch: 159 mean train loss:  2.74669406e-01, mean val. rec. loss:  2.63979736e-01\n",
      "Epoch: 160 mean train loss:  2.73751748e-01, mean val. rec. loss:  2.63047408e-01\n",
      "Epoch: 161 mean train loss:  2.72841867e-01, mean val. rec. loss:  2.62123063e-01\n",
      "Epoch: 162 mean train loss:  2.71939494e-01, mean val. rec. loss:  2.61206447e-01\n",
      "Epoch: 163 mean train loss:  2.71044600e-01, mean val. rec. loss:  2.60297342e-01\n",
      "Epoch: 164 mean train loss:  2.70156946e-01, mean val. rec. loss:  2.59395785e-01\n",
      "Epoch: 165 mean train loss:  2.69276473e-01, mean val. rec. loss:  2.58501413e-01\n",
      "Epoch: 166 mean train loss:  2.68402971e-01, mean val. rec. loss:  2.57614208e-01\n",
      "Epoch: 167 mean train loss:  2.67536472e-01, mean val. rec. loss:  2.56734007e-01\n",
      "Epoch: 168 mean train loss:  2.66676706e-01, mean val. rec. loss:  2.55860572e-01\n",
      "Epoch: 169 mean train loss:  2.65823673e-01, mean val. rec. loss:  2.54993851e-01\n",
      "Epoch: 170 mean train loss:  2.64977136e-01, mean val. rec. loss:  2.54133807e-01\n",
      "Epoch: 171 mean train loss:  2.64137214e-01, mean val. rec. loss:  2.53280186e-01\n",
      "Epoch: 172 mean train loss:  2.63303549e-01, mean val. rec. loss:  2.52432951e-01\n",
      "Epoch: 173 mean train loss:  2.62476200e-01, mean val. rec. loss:  2.51592012e-01\n",
      "Epoch: 174 mean train loss:  2.61655048e-01, mean val. rec. loss:  2.50757297e-01\n",
      "Epoch: 175 mean train loss:  2.60840094e-01, mean val. rec. loss:  2.49928659e-01\n",
      "Epoch: 176 mean train loss:  2.60031099e-01, mean val. rec. loss:  2.49105918e-01\n",
      "Epoch: 177 mean train loss:  2.59228093e-01, mean val. rec. loss:  2.48289201e-01\n",
      "Epoch: 178 mean train loss:  2.58430926e-01, mean val. rec. loss:  2.47478254e-01\n",
      "Epoch: 179 mean train loss:  2.57639599e-01, mean val. rec. loss:  2.46673094e-01\n",
      "Epoch: 180 mean train loss:  2.56853993e-01, mean val. rec. loss:  2.45873740e-01\n",
      "Epoch: 181 mean train loss:  2.56074019e-01, mean val. rec. loss:  2.45079884e-01\n",
      "Epoch: 182 mean train loss:  2.55299675e-01, mean val. rec. loss:  2.44291653e-01\n",
      "Epoch: 183 mean train loss:  2.54530814e-01, mean val. rec. loss:  2.43509009e-01\n",
      "Epoch: 184 mean train loss:  2.53767465e-01, mean val. rec. loss:  2.42731809e-01\n",
      "Epoch: 185 mean train loss:  2.53009538e-01, mean val. rec. loss:  2.41959979e-01\n",
      "Epoch: 186 mean train loss:  2.52256915e-01, mean val. rec. loss:  2.41193592e-01\n",
      "Epoch: 187 mean train loss:  2.51509596e-01, mean val. rec. loss:  2.40432503e-01\n",
      "Epoch: 188 mean train loss:  2.50767520e-01, mean val. rec. loss:  2.39676676e-01\n",
      "Epoch: 189 mean train loss:  2.50030570e-01, mean val. rec. loss:  2.38926074e-01\n",
      "Epoch: 190 mean train loss:  2.49298774e-01, mean val. rec. loss:  2.38180680e-01\n",
      "Epoch: 191 mean train loss:  2.48572043e-01, mean val. rec. loss:  2.37440438e-01\n",
      "Epoch: 192 mean train loss:  2.47850347e-01, mean val. rec. loss:  2.36705313e-01\n",
      "Epoch: 193 mean train loss:  2.47133627e-01, mean val. rec. loss:  2.35975141e-01\n",
      "Epoch: 194 mean train loss:  2.46421793e-01, mean val. rec. loss:  2.35250031e-01\n",
      "Epoch: 195 mean train loss:  2.45714831e-01, mean val. rec. loss:  2.34529892e-01\n",
      "Epoch: 196 mean train loss:  2.45012666e-01, mean val. rec. loss:  2.33814689e-01\n",
      "Epoch: 197 mean train loss:  2.44315343e-01, mean val. rec. loss:  2.33104329e-01\n",
      "Epoch: 198 mean train loss:  2.43622713e-01, mean val. rec. loss:  2.32398869e-01\n",
      "Epoch: 199 mean train loss:  2.42934760e-01, mean val. rec. loss:  2.31698216e-01\n",
      "Epoch: 200 mean train loss:  2.42251500e-01, mean val. rec. loss:  2.31002336e-01\n",
      "Epoch: 201 mean train loss:  2.41572798e-01, mean val. rec. loss:  2.30311118e-01\n",
      "Epoch: 202 mean train loss:  2.40898685e-01, mean val. rec. loss:  2.29624618e-01\n",
      "Epoch: 203 mean train loss:  2.40229042e-01, mean val. rec. loss:  2.28942690e-01\n",
      "Epoch: 204 mean train loss:  2.39563912e-01, mean val. rec. loss:  2.28265443e-01\n",
      "Epoch: 205 mean train loss:  2.38903222e-01, mean val. rec. loss:  2.27592659e-01\n",
      "Epoch: 206 mean train loss:  2.38246896e-01, mean val. rec. loss:  2.26924465e-01\n",
      "Epoch: 207 mean train loss:  2.37594951e-01, mean val. rec. loss:  2.26260626e-01\n",
      "Epoch: 208 mean train loss:  2.36947296e-01, mean val. rec. loss:  2.25601341e-01\n",
      "Epoch: 209 mean train loss:  2.36303961e-01, mean val. rec. loss:  2.24946338e-01\n",
      "Epoch: 210 mean train loss:  2.35664828e-01, mean val. rec. loss:  2.24295762e-01\n",
      "Epoch: 211 mean train loss:  2.35029895e-01, mean val. rec. loss:  2.23649414e-01\n",
      "Epoch: 212 mean train loss:  2.34399164e-01, mean val. rec. loss:  2.23007474e-01\n",
      "Epoch: 213 mean train loss:  2.33772589e-01, mean val. rec. loss:  2.22369544e-01\n",
      "Epoch: 214 mean train loss:  2.33150066e-01, mean val. rec. loss:  2.21736005e-01\n",
      "Epoch: 215 mean train loss:  2.32531610e-01, mean val. rec. loss:  2.21106494e-01\n",
      "Epoch: 216 mean train loss:  2.31917236e-01, mean val. rec. loss:  2.20481138e-01\n",
      "Epoch: 217 mean train loss:  2.31306751e-01, mean val. rec. loss:  2.19859809e-01\n",
      "Epoch: 218 mean train loss:  2.30700272e-01, mean val. rec. loss:  2.19242599e-01\n",
      "Epoch: 219 mean train loss:  2.30097727e-01, mean val. rec. loss:  2.18629272e-01\n",
      "Epoch: 220 mean train loss:  2.29499040e-01, mean val. rec. loss:  2.18019973e-01\n",
      "Epoch: 221 mean train loss:  2.28904182e-01, mean val. rec. loss:  2.17414557e-01\n",
      "Epoch: 222 mean train loss:  2.28313212e-01, mean val. rec. loss:  2.16813095e-01\n",
      "Epoch: 223 mean train loss:  2.27725997e-01, mean val. rec. loss:  2.16215444e-01\n",
      "Epoch: 224 mean train loss:  2.27142535e-01, mean val. rec. loss:  2.15621712e-01\n",
      "Epoch: 225 mean train loss:  2.26562858e-01, mean val. rec. loss:  2.15031736e-01\n",
      "Epoch: 226 mean train loss:  2.25986860e-01, mean val. rec. loss:  2.14445534e-01\n",
      "Epoch: 227 mean train loss:  2.25414512e-01, mean val. rec. loss:  2.13862996e-01\n",
      "Epoch: 228 mean train loss:  2.24845814e-01, mean val. rec. loss:  2.13284305e-01\n",
      "Epoch: 229 mean train loss:  2.24280736e-01, mean val. rec. loss:  2.12709225e-01\n",
      "Epoch: 230 mean train loss:  2.23719248e-01, mean val. rec. loss:  2.12137810e-01\n",
      "Epoch: 231 mean train loss:  2.23161306e-01, mean val. rec. loss:  2.11570005e-01\n",
      "Epoch: 232 mean train loss:  2.22606850e-01, mean val. rec. loss:  2.11005793e-01\n",
      "Epoch: 233 mean train loss:  2.22055939e-01, mean val. rec. loss:  2.10445155e-01\n",
      "Epoch: 234 mean train loss:  2.21508455e-01, mean val. rec. loss:  2.09888036e-01\n",
      "Epoch: 235 mean train loss:  2.20964428e-01, mean val. rec. loss:  2.09334384e-01\n",
      "Epoch: 236 mean train loss:  2.20423767e-01, mean val. rec. loss:  2.08784251e-01\n",
      "Epoch: 237 mean train loss:  2.19886517e-01, mean val. rec. loss:  2.08237547e-01\n",
      "Epoch: 238 mean train loss:  2.19352620e-01, mean val. rec. loss:  2.07694254e-01\n",
      "Epoch: 239 mean train loss:  2.18822044e-01, mean val. rec. loss:  2.07154336e-01\n",
      "Epoch: 240 mean train loss:  2.18294702e-01, mean val. rec. loss:  2.06617866e-01\n",
      "Epoch: 241 mean train loss:  2.17770681e-01, mean val. rec. loss:  2.06084570e-01\n",
      "Epoch: 242 mean train loss:  2.17249849e-01, mean val. rec. loss:  2.05554667e-01\n",
      "Epoch: 243 mean train loss:  2.16732250e-01, mean val. rec. loss:  2.05028085e-01\n",
      "Epoch: 244 mean train loss:  2.16217853e-01, mean val. rec. loss:  2.04504732e-01\n",
      "Epoch: 245 mean train loss:  2.15706615e-01, mean val. rec. loss:  2.03984645e-01\n",
      "Epoch: 246 mean train loss:  2.15198549e-01, mean val. rec. loss:  2.03467661e-01\n",
      "Epoch: 247 mean train loss:  2.14693508e-01, mean val. rec. loss:  2.02953978e-01\n",
      "Epoch: 248 mean train loss:  2.14191641e-01, mean val. rec. loss:  2.02443453e-01\n",
      "Epoch: 249 mean train loss:  2.13692797e-01, mean val. rec. loss:  2.01935939e-01\n",
      "Epoch: 250 mean train loss:  2.13197007e-01, mean val. rec. loss:  2.01431619e-01\n",
      "Epoch: 251 mean train loss:  2.12704227e-01, mean val. rec. loss:  2.00930311e-01\n",
      "Epoch: 252 mean train loss:  2.12214411e-01, mean val. rec. loss:  2.00432087e-01\n",
      "Epoch: 253 mean train loss:  2.11727560e-01, mean val. rec. loss:  1.99936893e-01\n",
      "Epoch: 254 mean train loss:  2.11243674e-01, mean val. rec. loss:  1.99444656e-01\n",
      "Epoch: 255 mean train loss:  2.10762633e-01, mean val. rec. loss:  1.98955413e-01\n",
      "Epoch: 256 mean train loss:  2.10284512e-01, mean val. rec. loss:  1.98469128e-01\n",
      "Epoch: 257 mean train loss:  2.09809281e-01, mean val. rec. loss:  1.97985764e-01\n",
      "Epoch: 258 mean train loss:  2.09336865e-01, mean val. rec. loss:  1.97505320e-01\n",
      "Epoch: 259 mean train loss:  2.08867266e-01, mean val. rec. loss:  1.97027689e-01\n",
      "Epoch: 260 mean train loss:  2.08400452e-01, mean val. rec. loss:  1.96552962e-01\n",
      "Epoch: 261 mean train loss:  2.07936453e-01, mean val. rec. loss:  1.96081064e-01\n",
      "Epoch: 262 mean train loss:  2.07475181e-01, mean val. rec. loss:  1.95611906e-01\n",
      "Epoch: 263 mean train loss:  2.07016650e-01, mean val. rec. loss:  1.95145688e-01\n",
      "Epoch: 264 mean train loss:  2.06560831e-01, mean val. rec. loss:  1.94682082e-01\n",
      "Epoch: 265 mean train loss:  2.06107723e-01, mean val. rec. loss:  1.94221306e-01\n",
      "Epoch: 266 mean train loss:  2.05657236e-01, mean val. rec. loss:  1.93763216e-01\n",
      "Epoch: 267 mean train loss:  2.05209402e-01, mean val. rec. loss:  1.93307793e-01\n",
      "Epoch: 268 mean train loss:  2.04764145e-01, mean val. rec. loss:  1.92854982e-01\n",
      "Epoch: 269 mean train loss:  2.04321480e-01, mean val. rec. loss:  1.92404911e-01\n",
      "Epoch: 270 mean train loss:  2.03881407e-01, mean val. rec. loss:  1.91957435e-01\n",
      "Epoch: 271 mean train loss:  2.03443882e-01, mean val. rec. loss:  1.91512572e-01\n",
      "Epoch: 272 mean train loss:  2.03008904e-01, mean val. rec. loss:  1.91070230e-01\n",
      "Epoch: 273 mean train loss:  2.02576443e-01, mean val. rec. loss:  1.90630465e-01\n",
      "Epoch: 274 mean train loss:  2.02146441e-01, mean val. rec. loss:  1.90193312e-01\n",
      "Epoch: 275 mean train loss:  2.01718972e-01, mean val. rec. loss:  1.89758627e-01\n",
      "Epoch: 276 mean train loss:  2.01293930e-01, mean val. rec. loss:  1.89326446e-01\n",
      "Epoch: 277 mean train loss:  2.00871273e-01, mean val. rec. loss:  1.88896733e-01\n",
      "Epoch: 278 mean train loss:  2.00451043e-01, mean val. rec. loss:  1.88469450e-01\n",
      "Epoch: 279 mean train loss:  2.00033198e-01, mean val. rec. loss:  1.88044708e-01\n",
      "Epoch: 280 mean train loss:  1.99617765e-01, mean val. rec. loss:  1.87622252e-01\n",
      "Epoch: 281 mean train loss:  1.99204627e-01, mean val. rec. loss:  1.87202191e-01\n",
      "Epoch: 282 mean train loss:  1.98793813e-01, mean val. rec. loss:  1.86784561e-01\n",
      "Epoch: 283 mean train loss:  1.98385368e-01, mean val. rec. loss:  1.86369289e-01\n",
      "Epoch: 284 mean train loss:  1.97979157e-01, mean val. rec. loss:  1.85956377e-01\n",
      "Epoch: 285 mean train loss:  1.97575285e-01, mean val. rec. loss:  1.85545768e-01\n",
      "Epoch: 286 mean train loss:  1.97173618e-01, mean val. rec. loss:  1.85137373e-01\n",
      "Epoch: 287 mean train loss:  1.96774156e-01, mean val. rec. loss:  1.84731246e-01\n",
      "Epoch: 288 mean train loss:  1.96376928e-01, mean val. rec. loss:  1.84327478e-01\n",
      "Epoch: 289 mean train loss:  1.95981905e-01, mean val. rec. loss:  1.83925996e-01\n",
      "Epoch: 290 mean train loss:  1.95589073e-01, mean val. rec. loss:  1.83526655e-01\n",
      "Epoch: 291 mean train loss:  1.95198415e-01, mean val. rec. loss:  1.83129491e-01\n",
      "Epoch: 292 mean train loss:  1.94809843e-01, mean val. rec. loss:  1.82734522e-01\n",
      "Epoch: 293 mean train loss:  1.94423446e-01, mean val. rec. loss:  1.82341767e-01\n",
      "Epoch: 294 mean train loss:  1.94039149e-01, mean val. rec. loss:  1.81951099e-01\n",
      "Epoch: 295 mean train loss:  1.93656908e-01, mean val. rec. loss:  1.81562607e-01\n",
      "Epoch: 296 mean train loss:  1.93276768e-01, mean val. rec. loss:  1.81176239e-01\n",
      "Epoch: 297 mean train loss:  1.92898699e-01, mean val. rec. loss:  1.80791921e-01\n",
      "Epoch: 298 mean train loss:  1.92522641e-01, mean val. rec. loss:  1.80409707e-01\n",
      "Epoch: 299 mean train loss:  1.92148623e-01, mean val. rec. loss:  1.80029489e-01\n",
      "Epoch: 300 mean train loss:  1.91776543e-01, mean val. rec. loss:  1.79651322e-01\n",
      "Epoch: 301 mean train loss:  1.91406459e-01, mean val. rec. loss:  1.79275241e-01\n",
      "Epoch: 302 mean train loss:  1.91038400e-01, mean val. rec. loss:  1.78901119e-01\n",
      "Epoch: 303 mean train loss:  1.90672264e-01, mean val. rec. loss:  1.78529030e-01\n",
      "Epoch: 304 mean train loss:  1.90308079e-01, mean val. rec. loss:  1.78158936e-01\n",
      "Epoch: 305 mean train loss:  1.89945816e-01, mean val. rec. loss:  1.77790729e-01\n",
      "Epoch: 306 mean train loss:  1.89585445e-01, mean val. rec. loss:  1.77424373e-01\n",
      "Epoch: 307 mean train loss:  1.89226891e-01, mean val. rec. loss:  1.77060103e-01\n",
      "Epoch: 308 mean train loss:  1.88870319e-01, mean val. rec. loss:  1.76697685e-01\n",
      "Epoch: 309 mean train loss:  1.88515564e-01, mean val. rec. loss:  1.76337116e-01\n",
      "Epoch: 310 mean train loss:  1.88162612e-01, mean val. rec. loss:  1.75978471e-01\n",
      "Epoch: 311 mean train loss:  1.87811537e-01, mean val. rec. loss:  1.75621622e-01\n",
      "Epoch: 312 mean train loss:  1.87462220e-01, mean val. rec. loss:  1.75266679e-01\n",
      "Epoch: 313 mean train loss:  1.87114705e-01, mean val. rec. loss:  1.74913567e-01\n",
      "Epoch: 314 mean train loss:  1.86768978e-01, mean val. rec. loss:  1.74562216e-01\n",
      "Epoch: 315 mean train loss:  1.86424965e-01, mean val. rec. loss:  1.74212625e-01\n",
      "Epoch: 316 mean train loss:  1.86082739e-01, mean val. rec. loss:  1.73864920e-01\n",
      "Epoch: 317 mean train loss:  1.85742271e-01, mean val. rec. loss:  1.73518921e-01\n",
      "Epoch: 318 mean train loss:  1.85403501e-01, mean val. rec. loss:  1.73174664e-01\n",
      "Epoch: 319 mean train loss:  1.85066415e-01, mean val. rec. loss:  1.72832149e-01\n",
      "Epoch: 320 mean train loss:  1.84731012e-01, mean val. rec. loss:  1.72491393e-01\n",
      "Epoch: 321 mean train loss:  1.84397293e-01, mean val. rec. loss:  1.72152289e-01\n",
      "Epoch: 322 mean train loss:  1.84065257e-01, mean val. rec. loss:  1.71814872e-01\n",
      "Epoch: 323 mean train loss:  1.83734859e-01, mean val. rec. loss:  1.71479179e-01\n",
      "Epoch: 324 mean train loss:  1.83406041e-01, mean val. rec. loss:  1.71145046e-01\n",
      "Epoch: 325 mean train loss:  1.83078892e-01, mean val. rec. loss:  1.70812636e-01\n",
      "Epoch: 326 mean train loss:  1.82753321e-01, mean val. rec. loss:  1.70481751e-01\n",
      "Epoch: 327 mean train loss:  1.82429315e-01, mean val. rec. loss:  1.70152662e-01\n",
      "Epoch: 328 mean train loss:  1.82106932e-01, mean val. rec. loss:  1.69825079e-01\n",
      "Epoch: 329 mean train loss:  1.81786084e-01, mean val. rec. loss:  1.69499020e-01\n",
      "Epoch: 330 mean train loss:  1.81466771e-01, mean val. rec. loss:  1.69174684e-01\n",
      "Epoch: 331 mean train loss:  1.81149022e-01, mean val. rec. loss:  1.68851837e-01\n",
      "Epoch: 332 mean train loss:  1.80832777e-01, mean val. rec. loss:  1.68530531e-01\n",
      "Epoch: 333 mean train loss:  1.80518037e-01, mean val. rec. loss:  1.68210786e-01\n",
      "Epoch: 334 mean train loss:  1.80204831e-01, mean val. rec. loss:  1.67892529e-01\n",
      "Epoch: 335 mean train loss:  1.79893071e-01, mean val. rec. loss:  1.67575832e-01\n",
      "Epoch: 336 mean train loss:  1.79582770e-01, mean val. rec. loss:  1.67260568e-01\n",
      "Epoch: 337 mean train loss:  1.79273930e-01, mean val. rec. loss:  1.66946847e-01\n",
      "Epoch: 338 mean train loss:  1.78966534e-01, mean val. rec. loss:  1.66634522e-01\n",
      "Epoch: 339 mean train loss:  1.78660554e-01, mean val. rec. loss:  1.66323776e-01\n",
      "Epoch: 340 mean train loss:  1.78356049e-01, mean val. rec. loss:  1.66014355e-01\n",
      "Epoch: 341 mean train loss:  1.78052914e-01, mean val. rec. loss:  1.65706439e-01\n",
      "Epoch: 342 mean train loss:  1.77751179e-01, mean val. rec. loss:  1.65399867e-01\n",
      "Epoch: 343 mean train loss:  1.77450816e-01, mean val. rec. loss:  1.65094727e-01\n",
      "Epoch: 344 mean train loss:  1.77151822e-01, mean val. rec. loss:  1.64790985e-01\n",
      "Epoch: 345 mean train loss:  1.76854185e-01, mean val. rec. loss:  1.64488657e-01\n",
      "Epoch: 346 mean train loss:  1.76557903e-01, mean val. rec. loss:  1.64187655e-01\n",
      "Epoch: 347 mean train loss:  1.76262917e-01, mean val. rec. loss:  1.63888031e-01\n",
      "Epoch: 348 mean train loss:  1.75969316e-01, mean val. rec. loss:  1.63589804e-01\n",
      "Epoch: 349 mean train loss:  1.75677042e-01, mean val. rec. loss:  1.63292884e-01\n",
      "Epoch: 350 mean train loss:  1.75386004e-01, mean val. rec. loss:  1.62997288e-01\n",
      "Epoch: 351 mean train loss:  1.75096307e-01, mean val. rec. loss:  1.62703016e-01\n",
      "Epoch: 352 mean train loss:  1.74807846e-01, mean val. rec. loss:  1.62409942e-01\n",
      "Epoch: 353 mean train loss:  1.74520651e-01, mean val. rec. loss:  1.62118229e-01\n",
      "Epoch: 354 mean train loss:  1.74234738e-01, mean val. rec. loss:  1.61827750e-01\n",
      "Epoch: 355 mean train loss:  1.73950017e-01, mean val. rec. loss:  1.61538595e-01\n",
      "Epoch: 356 mean train loss:  1.73666606e-01, mean val. rec. loss:  1.61250710e-01\n",
      "Epoch: 357 mean train loss:  1.73384387e-01, mean val. rec. loss:  1.60964059e-01\n",
      "Epoch: 358 mean train loss:  1.73103390e-01, mean val. rec. loss:  1.60678605e-01\n",
      "Epoch: 359 mean train loss:  1.72823570e-01, mean val. rec. loss:  1.60394331e-01\n",
      "Epoch: 360 mean train loss:  1.72544912e-01, mean val. rec. loss:  1.60111363e-01\n",
      "Epoch: 361 mean train loss:  1.72267520e-01, mean val. rec. loss:  1.59829574e-01\n",
      "Epoch: 362 mean train loss:  1.71991260e-01, mean val. rec. loss:  1.59548856e-01\n",
      "Epoch: 363 mean train loss:  1.71716133e-01, mean val. rec. loss:  1.59269498e-01\n",
      "Epoch: 364 mean train loss:  1.71442167e-01, mean val. rec. loss:  1.58991212e-01\n",
      "Epoch: 365 mean train loss:  1.71169364e-01, mean val. rec. loss:  1.58714122e-01\n",
      "Epoch: 366 mean train loss:  1.70897693e-01, mean val. rec. loss:  1.58438212e-01\n",
      "Epoch: 367 mean train loss:  1.70627154e-01, mean val. rec. loss:  1.58163427e-01\n",
      "Epoch: 368 mean train loss:  1.70357687e-01, mean val. rec. loss:  1.57889712e-01\n",
      "Epoch: 369 mean train loss:  1.70089368e-01, mean val. rec. loss:  1.57617158e-01\n",
      "Epoch: 370 mean train loss:  1.69822091e-01, mean val. rec. loss:  1.57345802e-01\n",
      "Epoch: 371 mean train loss:  1.69555962e-01, mean val. rec. loss:  1.57075372e-01\n",
      "Epoch: 372 mean train loss:  1.69290831e-01, mean val. rec. loss:  1.56806102e-01\n",
      "Epoch: 373 mean train loss:  1.69026802e-01, mean val. rec. loss:  1.56537921e-01\n",
      "Epoch: 374 mean train loss:  1.68763816e-01, mean val. rec. loss:  1.56270883e-01\n",
      "Epoch: 375 mean train loss:  1.68501888e-01, mean val. rec. loss:  1.56004807e-01\n",
      "Epoch: 376 mean train loss:  1.68241017e-01, mean val. rec. loss:  1.55739801e-01\n",
      "Epoch: 377 mean train loss:  1.67981130e-01, mean val. rec. loss:  1.55475830e-01\n",
      "Epoch: 378 mean train loss:  1.67722256e-01, mean val. rec. loss:  1.55212983e-01\n",
      "Epoch: 379 mean train loss:  1.67464439e-01, mean val. rec. loss:  1.54951062e-01\n",
      "Epoch: 380 mean train loss:  1.67207606e-01, mean val. rec. loss:  1.54690193e-01\n",
      "Epoch: 381 mean train loss:  1.66951741e-01, mean val. rec. loss:  1.54430340e-01\n",
      "Epoch: 382 mean train loss:  1.66696904e-01, mean val. rec. loss:  1.54171503e-01\n",
      "Epoch: 383 mean train loss:  1.66443035e-01, mean val. rec. loss:  1.53913609e-01\n",
      "Epoch: 384 mean train loss:  1.66190149e-01, mean val. rec. loss:  1.53656786e-01\n",
      "Epoch: 385 mean train loss:  1.65938188e-01, mean val. rec. loss:  1.53400834e-01\n",
      "Epoch: 386 mean train loss:  1.65687179e-01, mean val. rec. loss:  1.53145843e-01\n",
      "Epoch: 387 mean train loss:  1.65437095e-01, mean val. rec. loss:  1.52891905e-01\n",
      "Epoch: 388 mean train loss:  1.65187978e-01, mean val. rec. loss:  1.52638838e-01\n",
      "Epoch: 389 mean train loss:  1.64939726e-01, mean val. rec. loss:  1.52386750e-01\n",
      "Epoch: 390 mean train loss:  1.64692457e-01, mean val. rec. loss:  1.52135552e-01\n",
      "Epoch: 391 mean train loss:  1.64446081e-01, mean val. rec. loss:  1.51885351e-01\n",
      "Epoch: 392 mean train loss:  1.64200630e-01, mean val. rec. loss:  1.51635958e-01\n",
      "Epoch: 393 mean train loss:  1.63956012e-01, mean val. rec. loss:  1.51387508e-01\n",
      "Epoch: 394 mean train loss:  1.63712289e-01, mean val. rec. loss:  1.51139975e-01\n",
      "Epoch: 395 mean train loss:  1.63469459e-01, mean val. rec. loss:  1.50893385e-01\n",
      "Epoch: 396 mean train loss:  1.63227583e-01, mean val. rec. loss:  1.50647657e-01\n",
      "Epoch: 397 mean train loss:  1.62986511e-01, mean val. rec. loss:  1.50402745e-01\n",
      "Epoch: 398 mean train loss:  1.62746259e-01, mean val. rec. loss:  1.50158768e-01\n",
      "Epoch: 399 mean train loss:  1.62506901e-01, mean val. rec. loss:  1.49915643e-01\n",
      "Epoch: 400 mean train loss:  1.62268376e-01, mean val. rec. loss:  1.49673389e-01\n",
      "Epoch: 401 mean train loss:  1.62030671e-01, mean val. rec. loss:  1.49431880e-01\n",
      "Epoch: 402 mean train loss:  1.61793786e-01, mean val. rec. loss:  1.49191277e-01\n",
      "Epoch: 403 mean train loss:  1.61557749e-01, mean val. rec. loss:  1.48951509e-01\n",
      "Epoch: 404 mean train loss:  1.61322518e-01, mean val. rec. loss:  1.48712539e-01\n",
      "Epoch: 405 mean train loss:  1.61088060e-01, mean val. rec. loss:  1.48474449e-01\n",
      "Epoch: 406 mean train loss:  1.60854437e-01, mean val. rec. loss:  1.48237086e-01\n",
      "Epoch: 407 mean train loss:  1.60621604e-01, mean val. rec. loss:  1.48000556e-01\n",
      "Epoch: 408 mean train loss:  1.60389545e-01, mean val. rec. loss:  1.47764825e-01\n",
      "Epoch: 409 mean train loss:  1.60158276e-01, mean val. rec. loss:  1.47529892e-01\n",
      "Epoch: 410 mean train loss:  1.59927752e-01, mean val. rec. loss:  1.47295713e-01\n",
      "Epoch: 411 mean train loss:  1.59698002e-01, mean val. rec. loss:  1.47062395e-01\n",
      "Epoch: 412 mean train loss:  1.59469072e-01, mean val. rec. loss:  1.46829721e-01\n",
      "Epoch: 413 mean train loss:  1.59240827e-01, mean val. rec. loss:  1.46597872e-01\n",
      "Epoch: 414 mean train loss:  1.59013342e-01, mean val. rec. loss:  1.46366750e-01\n",
      "Epoch: 415 mean train loss:  1.58786572e-01, mean val. rec. loss:  1.46136453e-01\n",
      "Epoch: 416 mean train loss:  1.58560576e-01, mean val. rec. loss:  1.45906800e-01\n",
      "Epoch: 417 mean train loss:  1.58335266e-01, mean val. rec. loss:  1.45677909e-01\n",
      "Epoch: 418 mean train loss:  1.58110686e-01, mean val. rec. loss:  1.45449825e-01\n",
      "Epoch: 419 mean train loss:  1.57886880e-01, mean val. rec. loss:  1.45222404e-01\n",
      "Epoch: 420 mean train loss:  1.57663730e-01, mean val. rec. loss:  1.44995636e-01\n",
      "Epoch: 421 mean train loss:  1.57441295e-01, mean val. rec. loss:  1.44769694e-01\n",
      "Epoch: 422 mean train loss:  1.57219546e-01, mean val. rec. loss:  1.44544386e-01\n",
      "Epoch: 423 mean train loss:  1.56998481e-01, mean val. rec. loss:  1.44319786e-01\n",
      "Epoch: 424 mean train loss:  1.56778117e-01, mean val. rec. loss:  1.44095912e-01\n",
      "Epoch: 425 mean train loss:  1.56558453e-01, mean val. rec. loss:  1.43872682e-01\n",
      "Epoch: 426 mean train loss:  1.56339415e-01, mean val. rec. loss:  1.43650214e-01\n",
      "Epoch: 427 mean train loss:  1.56121122e-01, mean val. rec. loss:  1.43428317e-01\n",
      "Epoch: 428 mean train loss:  1.55903409e-01, mean val. rec. loss:  1.43207128e-01\n",
      "Epoch: 429 mean train loss:  1.55686397e-01, mean val. rec. loss:  1.42986629e-01\n",
      "Epoch: 430 mean train loss:  1.55470040e-01, mean val. rec. loss:  1.42766801e-01\n",
      "Epoch: 431 mean train loss:  1.55254279e-01, mean val. rec. loss:  1.42547544e-01\n",
      "Epoch: 432 mean train loss:  1.55039219e-01, mean val. rec. loss:  1.42328977e-01\n",
      "Epoch: 433 mean train loss:  1.54824754e-01, mean val. rec. loss:  1.42111026e-01\n",
      "Epoch: 434 mean train loss:  1.54610915e-01, mean val. rec. loss:  1.41893729e-01\n",
      "Epoch: 435 mean train loss:  1.54397701e-01, mean val. rec. loss:  1.41677113e-01\n",
      "Epoch: 436 mean train loss:  1.54185144e-01, mean val. rec. loss:  1.41461104e-01\n",
      "Epoch: 437 mean train loss:  1.53973182e-01, mean val. rec. loss:  1.41245657e-01\n",
      "Epoch: 438 mean train loss:  1.53761845e-01, mean val. rec. loss:  1.41030873e-01\n",
      "Epoch: 439 mean train loss:  1.53551075e-01, mean val. rec. loss:  1.40816651e-01\n",
      "Epoch: 440 mean train loss:  1.53340901e-01, mean val. rec. loss:  1.40603073e-01\n",
      "Epoch: 441 mean train loss:  1.53131352e-01, mean val. rec. loss:  1.40390094e-01\n",
      "Epoch: 442 mean train loss:  1.52922355e-01, mean val. rec. loss:  1.40177687e-01\n",
      "Epoch: 443 mean train loss:  1.52713939e-01, mean val. rec. loss:  1.39965842e-01\n",
      "Epoch: 444 mean train loss:  1.52506103e-01, mean val. rec. loss:  1.39754687e-01\n",
      "Epoch: 445 mean train loss:  1.52298894e-01, mean val. rec. loss:  1.39544057e-01\n",
      "Epoch: 446 mean train loss:  1.52092205e-01, mean val. rec. loss:  1.39333927e-01\n",
      "Epoch: 447 mean train loss:  1.51886128e-01, mean val. rec. loss:  1.39124441e-01\n",
      "Epoch: 448 mean train loss:  1.51680587e-01, mean val. rec. loss:  1.38915444e-01\n",
      "Epoch: 449 mean train loss:  1.51475522e-01, mean val. rec. loss:  1.38707146e-01\n",
      "Epoch: 450 mean train loss:  1.51271128e-01, mean val. rec. loss:  1.38499284e-01\n",
      "Epoch: 451 mean train loss:  1.51067241e-01, mean val. rec. loss:  1.38292056e-01\n",
      "Epoch: 452 mean train loss:  1.50863875e-01, mean val. rec. loss:  1.38085264e-01\n",
      "Epoch: 453 mean train loss:  1.50661045e-01, mean val. rec. loss:  1.37879116e-01\n",
      "Epoch: 454 mean train loss:  1.50458796e-01, mean val. rec. loss:  1.37673459e-01\n",
      "Epoch: 455 mean train loss:  1.50257039e-01, mean val. rec. loss:  1.37468426e-01\n",
      "Epoch: 456 mean train loss:  1.50055833e-01, mean val. rec. loss:  1.37263785e-01\n",
      "Epoch: 457 mean train loss:  1.49855148e-01, mean val. rec. loss:  1.37059759e-01\n",
      "Epoch: 458 mean train loss:  1.49654926e-01, mean val. rec. loss:  1.36856152e-01\n",
      "Epoch: 459 mean train loss:  1.49455239e-01, mean val. rec. loss:  1.36653125e-01\n",
      "Epoch: 460 mean train loss:  1.49256059e-01, mean val. rec. loss:  1.36450551e-01\n",
      "Epoch: 461 mean train loss:  1.49057356e-01, mean val. rec. loss:  1.36248612e-01\n",
      "Epoch: 462 mean train loss:  1.48859189e-01, mean val. rec. loss:  1.36047046e-01\n",
      "Epoch: 463 mean train loss:  1.48661499e-01, mean val. rec. loss:  1.35846060e-01\n",
      "Epoch: 464 mean train loss:  1.48464286e-01, mean val. rec. loss:  1.35645482e-01\n",
      "Epoch: 465 mean train loss:  1.48267579e-01, mean val. rec. loss:  1.35445467e-01\n",
      "Epoch: 466 mean train loss:  1.48071378e-01, mean val. rec. loss:  1.35245860e-01\n",
      "Epoch: 467 mean train loss:  1.47875580e-01, mean val. rec. loss:  1.35046887e-01\n",
      "Epoch: 468 mean train loss:  1.47680348e-01, mean val. rec. loss:  1.34848269e-01\n",
      "Epoch: 469 mean train loss:  1.47485593e-01, mean val. rec. loss:  1.34650177e-01\n",
      "Epoch: 470 mean train loss:  1.47291225e-01, mean val. rec. loss:  1.34452457e-01\n",
      "Epoch: 471 mean train loss:  1.47097349e-01, mean val. rec. loss:  1.34255263e-01\n",
      "Epoch: 472 mean train loss:  1.46903919e-01, mean val. rec. loss:  1.34058523e-01\n",
      "Epoch: 473 mean train loss:  1.46710937e-01, mean val. rec. loss:  1.33862281e-01\n",
      "Epoch: 474 mean train loss:  1.46518431e-01, mean val. rec. loss:  1.33666412e-01\n",
      "Epoch: 475 mean train loss:  1.46326357e-01, mean val. rec. loss:  1.33471041e-01\n",
      "Epoch: 476 mean train loss:  1.46134745e-01, mean val. rec. loss:  1.33276079e-01\n",
      "Epoch: 477 mean train loss:  1.45943565e-01, mean val. rec. loss:  1.33081624e-01\n",
      "Epoch: 478 mean train loss:  1.45752817e-01, mean val. rec. loss:  1.32887633e-01\n",
      "Epoch: 479 mean train loss:  1.45562546e-01, mean val. rec. loss:  1.32694013e-01\n",
      "Epoch: 480 mean train loss:  1.45372662e-01, mean val. rec. loss:  1.32500865e-01\n",
      "Epoch: 481 mean train loss:  1.45183225e-01, mean val. rec. loss:  1.32308080e-01\n",
      "Epoch: 482 mean train loss:  1.44994176e-01, mean val. rec. loss:  1.32115803e-01\n",
      "Epoch: 483 mean train loss:  1.44805603e-01, mean val. rec. loss:  1.31923934e-01\n",
      "Epoch: 484 mean train loss:  1.44617447e-01, mean val. rec. loss:  1.31732446e-01\n",
      "Epoch: 485 mean train loss:  1.44429664e-01, mean val. rec. loss:  1.31541321e-01\n",
      "Epoch: 486 mean train loss:  1.44242298e-01, mean val. rec. loss:  1.31350759e-01\n",
      "Epoch: 487 mean train loss:  1.44055393e-01, mean val. rec. loss:  1.31160523e-01\n",
      "Epoch: 488 mean train loss:  1.43868876e-01, mean val. rec. loss:  1.30970695e-01\n",
      "Epoch: 489 mean train loss:  1.43682717e-01, mean val. rec. loss:  1.30781230e-01\n",
      "Epoch: 490 mean train loss:  1.43496930e-01, mean val. rec. loss:  1.30592228e-01\n",
      "Epoch: 491 mean train loss:  1.43311634e-01, mean val. rec. loss:  1.30403571e-01\n",
      "Epoch: 492 mean train loss:  1.43126652e-01, mean val. rec. loss:  1.30215376e-01\n",
      "Epoch: 493 mean train loss:  1.42942116e-01, mean val. rec. loss:  1.30027535e-01\n",
      "Epoch: 494 mean train loss:  1.42757938e-01, mean val. rec. loss:  1.29840084e-01\n",
      "Epoch: 495 mean train loss:  1.42574162e-01, mean val. rec. loss:  1.29652996e-01\n",
      "Epoch: 496 mean train loss:  1.42390669e-01, mean val. rec. loss:  1.29466199e-01\n",
      "Epoch: 497 mean train loss:  1.42207594e-01, mean val. rec. loss:  1.29279909e-01\n",
      "Epoch: 498 mean train loss:  1.42024950e-01, mean val. rec. loss:  1.29093946e-01\n",
      "Epoch: 499 mean train loss:  1.41842619e-01, mean val. rec. loss:  1.28908327e-01\n",
      "Epoch: 500 mean train loss:  1.41660676e-01, mean val. rec. loss:  1.28723072e-01\n",
      "Epoch: 501 mean train loss:  1.41479075e-01, mean val. rec. loss:  1.28538243e-01\n",
      "Epoch: 502 mean train loss:  1.41297862e-01, mean val. rec. loss:  1.28353777e-01\n",
      "Epoch: 503 mean train loss:  1.41117035e-01, mean val. rec. loss:  1.28169601e-01\n",
      "Epoch: 504 mean train loss:  1.40936477e-01, mean val. rec. loss:  1.27985842e-01\n",
      "Epoch: 505 mean train loss:  1.40756366e-01, mean val. rec. loss:  1.27802410e-01\n",
      "Epoch: 506 mean train loss:  1.40576553e-01, mean val. rec. loss:  1.27619304e-01\n",
      "Epoch: 507 mean train loss:  1.40397098e-01, mean val. rec. loss:  1.27436598e-01\n",
      "Epoch: 508 mean train loss:  1.40218000e-01, mean val. rec. loss:  1.27254237e-01\n",
      "Epoch: 509 mean train loss:  1.40039229e-01, mean val. rec. loss:  1.27072174e-01\n",
      "Epoch: 510 mean train loss:  1.39860787e-01, mean val. rec. loss:  1.26890493e-01\n",
      "Epoch: 511 mean train loss:  1.39682732e-01, mean val. rec. loss:  1.26709102e-01\n",
      "Epoch: 512 mean train loss:  1.39504945e-01, mean val. rec. loss:  1.26528074e-01\n",
      "Epoch: 513 mean train loss:  1.39327545e-01, mean val. rec. loss:  1.26347409e-01\n",
      "Epoch: 514 mean train loss:  1.39150473e-01, mean val. rec. loss:  1.26167061e-01\n",
      "Epoch: 515 mean train loss:  1.38973729e-01, mean val. rec. loss:  1.25987022e-01\n",
      "Epoch: 516 mean train loss:  1.38797297e-01, mean val. rec. loss:  1.25807310e-01\n",
      "Epoch: 517 mean train loss:  1.38621164e-01, mean val. rec. loss:  1.25627987e-01\n",
      "Epoch: 518 mean train loss:  1.38445433e-01, mean val. rec. loss:  1.25448910e-01\n",
      "Epoch: 519 mean train loss:  1.38269970e-01, mean val. rec. loss:  1.25270159e-01\n",
      "Epoch: 520 mean train loss:  1.38094790e-01, mean val. rec. loss:  1.25091716e-01\n",
      "Epoch: 521 mean train loss:  1.37919967e-01, mean val. rec. loss:  1.24913636e-01\n",
      "Epoch: 522 mean train loss:  1.37745413e-01, mean val. rec. loss:  1.24735756e-01\n",
      "Epoch: 523 mean train loss:  1.37571186e-01, mean val. rec. loss:  1.24558330e-01\n",
      "Epoch: 524 mean train loss:  1.37397302e-01, mean val. rec. loss:  1.24381085e-01\n",
      "Epoch: 525 mean train loss:  1.37223672e-01, mean val. rec. loss:  1.24204212e-01\n",
      "Epoch: 526 mean train loss:  1.37050354e-01, mean val. rec. loss:  1.24027611e-01\n",
      "Epoch: 527 mean train loss:  1.36877349e-01, mean val. rec. loss:  1.23851354e-01\n",
      "Epoch: 528 mean train loss:  1.36704657e-01, mean val. rec. loss:  1.23675406e-01\n",
      "Epoch: 529 mean train loss:  1.36532248e-01, mean val. rec. loss:  1.23499658e-01\n",
      "Epoch: 530 mean train loss:  1.36360092e-01, mean val. rec. loss:  1.23324246e-01\n",
      "Epoch: 531 mean train loss:  1.36188249e-01, mean val. rec. loss:  1.23149169e-01\n",
      "Epoch: 532 mean train loss:  1.36016749e-01, mean val. rec. loss:  1.22974382e-01\n",
      "Epoch: 533 mean train loss:  1.35845547e-01, mean val. rec. loss:  1.22799813e-01\n",
      "Epoch: 534 mean train loss:  1.35674553e-01, mean val. rec. loss:  1.22625625e-01\n",
      "Epoch: 535 mean train loss:  1.35503916e-01, mean val. rec. loss:  1.22451728e-01\n",
      "Epoch: 536 mean train loss:  1.35333548e-01, mean val. rec. loss:  1.22278048e-01\n",
      "Epoch: 537 mean train loss:  1.35163448e-01, mean val. rec. loss:  1.22104595e-01\n",
      "Epoch: 538 mean train loss:  1.34993617e-01, mean val. rec. loss:  1.21931568e-01\n",
      "Epoch: 539 mean train loss:  1.34824098e-01, mean val. rec. loss:  1.21758678e-01\n",
      "Epoch: 540 mean train loss:  1.34654817e-01, mean val. rec. loss:  1.21586159e-01\n",
      "Epoch: 541 mean train loss:  1.34485805e-01, mean val. rec. loss:  1.21413903e-01\n",
      "Epoch: 542 mean train loss:  1.34317090e-01, mean val. rec. loss:  1.21241856e-01\n",
      "Epoch: 543 mean train loss:  1.34148599e-01, mean val. rec. loss:  1.21070145e-01\n",
      "Epoch: 544 mean train loss:  1.33980406e-01, mean val. rec. loss:  1.20898652e-01\n",
      "Epoch: 545 mean train loss:  1.33812481e-01, mean val. rec. loss:  1.20727394e-01\n",
      "Epoch: 546 mean train loss:  1.33644765e-01, mean val. rec. loss:  1.20556445e-01\n",
      "Epoch: 547 mean train loss:  1.33477377e-01, mean val. rec. loss:  1.20385731e-01\n",
      "Epoch: 548 mean train loss:  1.33310182e-01, mean val. rec. loss:  1.20215308e-01\n",
      "Epoch: 549 mean train loss:  1.33143285e-01, mean val. rec. loss:  1.20045085e-01\n",
      "Epoch: 550 mean train loss:  1.32976626e-01, mean val. rec. loss:  1.19875115e-01\n",
      "Epoch: 551 mean train loss:  1.32810206e-01, mean val. rec. loss:  1.19705445e-01\n",
      "Epoch: 552 mean train loss:  1.32644069e-01, mean val. rec. loss:  1.19536020e-01\n",
      "Epoch: 553 mean train loss:  1.32478170e-01, mean val. rec. loss:  1.19366813e-01\n",
      "Epoch: 554 mean train loss:  1.32312509e-01, mean val. rec. loss:  1.19197923e-01\n",
      "Epoch: 555 mean train loss:  1.32147087e-01, mean val. rec. loss:  1.19029160e-01\n",
      "Epoch: 556 mean train loss:  1.31981933e-01, mean val. rec. loss:  1.18860714e-01\n",
      "Epoch: 557 mean train loss:  1.31816988e-01, mean val. rec. loss:  1.18692487e-01\n",
      "Epoch: 558 mean train loss:  1.31652311e-01, mean val. rec. loss:  1.18524513e-01\n",
      "Epoch: 559 mean train loss:  1.31487842e-01, mean val. rec. loss:  1.18356802e-01\n",
      "Epoch: 560 mean train loss:  1.31323656e-01, mean val. rec. loss:  1.18189337e-01\n",
      "Epoch: 561 mean train loss:  1.31159694e-01, mean val. rec. loss:  1.18021998e-01\n",
      "Epoch: 562 mean train loss:  1.30995926e-01, mean val. rec. loss:  1.17854968e-01\n",
      "Epoch: 563 mean train loss:  1.30832440e-01, mean val. rec. loss:  1.17688173e-01\n",
      "Epoch: 564 mean train loss:  1.30669134e-01, mean val. rec. loss:  1.17521615e-01\n",
      "Epoch: 565 mean train loss:  1.30506110e-01, mean val. rec. loss:  1.17355283e-01\n",
      "Epoch: 566 mean train loss:  1.30343295e-01, mean val. rec. loss:  1.17189206e-01\n",
      "Epoch: 567 mean train loss:  1.30180703e-01, mean val. rec. loss:  1.17023264e-01\n",
      "Epoch: 568 mean train loss:  1.30018305e-01, mean val. rec. loss:  1.16857576e-01\n",
      "Epoch: 569 mean train loss:  1.29856146e-01, mean val. rec. loss:  1.16692106e-01\n",
      "Epoch: 570 mean train loss:  1.29694269e-01, mean val. rec. loss:  1.16526891e-01\n",
      "Epoch: 571 mean train loss:  1.29532542e-01, mean val. rec. loss:  1.16361965e-01\n",
      "Epoch: 572 mean train loss:  1.29371068e-01, mean val. rec. loss:  1.16197212e-01\n",
      "Epoch: 573 mean train loss:  1.29209862e-01, mean val. rec. loss:  1.16032640e-01\n",
      "Epoch: 574 mean train loss:  1.29048804e-01, mean val. rec. loss:  1.15868350e-01\n",
      "Epoch: 575 mean train loss:  1.28888015e-01, mean val. rec. loss:  1.15704231e-01\n",
      "Epoch: 576 mean train loss:  1.28727420e-01, mean val. rec. loss:  1.15540340e-01\n",
      "Epoch: 577 mean train loss:  1.28567033e-01, mean val. rec. loss:  1.15376694e-01\n",
      "Epoch: 578 mean train loss:  1.28406840e-01, mean val. rec. loss:  1.15213138e-01\n",
      "Epoch: 579 mean train loss:  1.28246841e-01, mean val. rec. loss:  1.15049909e-01\n",
      "Epoch: 580 mean train loss:  1.28087110e-01, mean val. rec. loss:  1.14886834e-01\n",
      "Epoch: 581 mean train loss:  1.27927512e-01, mean val. rec. loss:  1.14723977e-01\n",
      "Epoch: 582 mean train loss:  1.27768169e-01, mean val. rec. loss:  1.14561328e-01\n",
      "Epoch: 583 mean train loss:  1.27609048e-01, mean val. rec. loss:  1.14398888e-01\n",
      "Epoch: 584 mean train loss:  1.27450092e-01, mean val. rec. loss:  1.14236603e-01\n",
      "Epoch: 585 mean train loss:  1.27291329e-01, mean val. rec. loss:  1.14074598e-01\n",
      "Epoch: 586 mean train loss:  1.27132819e-01, mean val. rec. loss:  1.13912757e-01\n",
      "Epoch: 587 mean train loss:  1.26974473e-01, mean val. rec. loss:  1.13751097e-01\n",
      "Epoch: 588 mean train loss:  1.26816306e-01, mean val. rec. loss:  1.13589655e-01\n",
      "Epoch: 589 mean train loss:  1.26658363e-01, mean val. rec. loss:  1.13428413e-01\n",
      "Epoch: 590 mean train loss:  1.26500598e-01, mean val. rec. loss:  1.13267397e-01\n",
      "Epoch: 591 mean train loss:  1.26343071e-01, mean val. rec. loss:  1.13106545e-01\n",
      "Epoch: 592 mean train loss:  1.26185724e-01, mean val. rec. loss:  1.12945829e-01\n",
      "Epoch: 593 mean train loss:  1.26028495e-01, mean val. rec. loss:  1.12785421e-01\n",
      "Epoch: 594 mean train loss:  1.25871535e-01, mean val. rec. loss:  1.12625131e-01\n",
      "Epoch: 595 mean train loss:  1.25714739e-01, mean val. rec. loss:  1.12464996e-01\n",
      "Epoch: 596 mean train loss:  1.25558151e-01, mean val. rec. loss:  1.12305132e-01\n",
      "Epoch: 597 mean train loss:  1.25401742e-01, mean val. rec. loss:  1.12145377e-01\n",
      "Epoch: 598 mean train loss:  1.25245481e-01, mean val. rec. loss:  1.11985958e-01\n",
      "Epoch: 599 mean train loss:  1.25089489e-01, mean val. rec. loss:  1.11826585e-01\n",
      "Epoch: 600 mean train loss:  1.24933661e-01, mean val. rec. loss:  1.11667502e-01\n",
      "Epoch: 601 mean train loss:  1.24778012e-01, mean val. rec. loss:  1.11508491e-01\n",
      "Epoch: 602 mean train loss:  1.24622519e-01, mean val. rec. loss:  1.11349734e-01\n",
      "Epoch: 603 mean train loss:  1.24467235e-01, mean val. rec. loss:  1.11191113e-01\n",
      "Epoch: 604 mean train loss:  1.24312092e-01, mean val. rec. loss:  1.11032783e-01\n",
      "Epoch: 605 mean train loss:  1.24157202e-01, mean val. rec. loss:  1.10874534e-01\n",
      "Epoch: 606 mean train loss:  1.24002425e-01, mean val. rec. loss:  1.10716413e-01\n",
      "Epoch: 607 mean train loss:  1.23847825e-01, mean val. rec. loss:  1.10558554e-01\n",
      "Epoch: 608 mean train loss:  1.23693428e-01, mean val. rec. loss:  1.10400895e-01\n",
      "Epoch: 609 mean train loss:  1.23539246e-01, mean val. rec. loss:  1.10243354e-01\n",
      "Epoch: 610 mean train loss:  1.23385168e-01, mean val. rec. loss:  1.10086003e-01\n",
      "Epoch: 611 mean train loss:  1.23231277e-01, mean val. rec. loss:  1.09928816e-01\n",
      "Epoch: 612 mean train loss:  1.23077564e-01, mean val. rec. loss:  1.09771801e-01\n",
      "Epoch: 613 mean train loss:  1.22924052e-01, mean val. rec. loss:  1.09615040e-01\n",
      "Epoch: 614 mean train loss:  1.22770712e-01, mean val. rec. loss:  1.09458370e-01\n",
      "Epoch: 615 mean train loss:  1.22617506e-01, mean val. rec. loss:  1.09301827e-01\n",
      "Epoch: 616 mean train loss:  1.22464486e-01, mean val. rec. loss:  1.09145574e-01\n",
      "Epoch: 617 mean train loss:  1.22311630e-01, mean val. rec. loss:  1.08989430e-01\n",
      "Epoch: 618 mean train loss:  1.22158930e-01, mean val. rec. loss:  1.08833413e-01\n",
      "Epoch: 619 mean train loss:  1.22006365e-01, mean val. rec. loss:  1.08677641e-01\n",
      "Epoch: 620 mean train loss:  1.21854053e-01, mean val. rec. loss:  1.08521959e-01\n",
      "Epoch: 621 mean train loss:  1.21701822e-01, mean val. rec. loss:  1.08366505e-01\n",
      "Epoch: 622 mean train loss:  1.21549808e-01, mean val. rec. loss:  1.08211186e-01\n",
      "Epoch: 623 mean train loss:  1.21397943e-01, mean val. rec. loss:  1.08056031e-01\n",
      "Epoch: 624 mean train loss:  1.21246241e-01, mean val. rec. loss:  1.07901003e-01\n",
      "Epoch: 625 mean train loss:  1.21094674e-01, mean val. rec. loss:  1.07746147e-01\n",
      "Epoch: 626 mean train loss:  1.20943278e-01, mean val. rec. loss:  1.07591491e-01\n",
      "Epoch: 627 mean train loss:  1.20792030e-01, mean val. rec. loss:  1.07436970e-01\n",
      "Epoch: 628 mean train loss:  1.20640962e-01, mean val. rec. loss:  1.07282659e-01\n",
      "Epoch: 629 mean train loss:  1.20490073e-01, mean val. rec. loss:  1.07128511e-01\n",
      "Epoch: 630 mean train loss:  1.20339369e-01, mean val. rec. loss:  1.06974517e-01\n",
      "Epoch: 631 mean train loss:  1.20188792e-01, mean val. rec. loss:  1.06820632e-01\n",
      "Epoch: 632 mean train loss:  1.20038335e-01, mean val. rec. loss:  1.06666955e-01\n",
      "Epoch: 633 mean train loss:  1.19888093e-01, mean val. rec. loss:  1.06513442e-01\n",
      "Epoch: 634 mean train loss:  1.19738016e-01, mean val. rec. loss:  1.06360056e-01\n",
      "Epoch: 635 mean train loss:  1.19588050e-01, mean val. rec. loss:  1.06206796e-01\n",
      "Epoch: 636 mean train loss:  1.19438218e-01, mean val. rec. loss:  1.06053719e-01\n",
      "Epoch: 637 mean train loss:  1.19288550e-01, mean val. rec. loss:  1.05900768e-01\n",
      "Epoch: 638 mean train loss:  1.19139009e-01, mean val. rec. loss:  1.05747971e-01\n",
      "Epoch: 639 mean train loss:  1.18989683e-01, mean val. rec. loss:  1.05595338e-01\n",
      "Epoch: 640 mean train loss:  1.18840485e-01, mean val. rec. loss:  1.05442913e-01\n",
      "Epoch: 641 mean train loss:  1.18691420e-01, mean val. rec. loss:  1.05290580e-01\n",
      "Epoch: 642 mean train loss:  1.18542512e-01, mean val. rec. loss:  1.05138482e-01\n",
      "Epoch: 643 mean train loss:  1.18393738e-01, mean val. rec. loss:  1.04986438e-01\n",
      "Epoch: 644 mean train loss:  1.18245135e-01, mean val. rec. loss:  1.04834549e-01\n",
      "Epoch: 645 mean train loss:  1.18096666e-01, mean val. rec. loss:  1.04682814e-01\n",
      "Epoch: 646 mean train loss:  1.17948354e-01, mean val. rec. loss:  1.04531305e-01\n",
      "Epoch: 647 mean train loss:  1.17800176e-01, mean val. rec. loss:  1.04379797e-01\n",
      "Epoch: 648 mean train loss:  1.17652109e-01, mean val. rec. loss:  1.04228506e-01\n",
      "Epoch: 649 mean train loss:  1.17504192e-01, mean val. rec. loss:  1.04077334e-01\n",
      "Epoch: 650 mean train loss:  1.17356483e-01, mean val. rec. loss:  1.03926324e-01\n",
      "Epoch: 651 mean train loss:  1.17208863e-01, mean val. rec. loss:  1.03775496e-01\n",
      "Epoch: 652 mean train loss:  1.17061378e-01, mean val. rec. loss:  1.03624813e-01\n",
      "Epoch: 653 mean train loss:  1.16914063e-01, mean val. rec. loss:  1.03474185e-01\n",
      "Epoch: 654 mean train loss:  1.16766854e-01, mean val. rec. loss:  1.03323738e-01\n",
      "Epoch: 655 mean train loss:  1.16619807e-01, mean val. rec. loss:  1.03173554e-01\n",
      "Epoch: 656 mean train loss:  1.16472918e-01, mean val. rec. loss:  1.03023370e-01\n",
      "Epoch: 657 mean train loss:  1.16326170e-01, mean val. rec. loss:  1.02873450e-01\n",
      "Epoch: 658 mean train loss:  1.16179586e-01, mean val. rec. loss:  1.02723619e-01\n",
      "Epoch: 659 mean train loss:  1.16033128e-01, mean val. rec. loss:  1.02573889e-01\n",
      "Epoch: 660 mean train loss:  1.15886767e-01, mean val. rec. loss:  1.02424313e-01\n",
      "Epoch: 661 mean train loss:  1.15740563e-01, mean val. rec. loss:  1.02274900e-01\n",
      "Epoch: 662 mean train loss:  1.15594508e-01, mean val. rec. loss:  1.02125642e-01\n",
      "Epoch: 663 mean train loss:  1.15448601e-01, mean val. rec. loss:  1.01976538e-01\n",
      "Epoch: 664 mean train loss:  1.15302814e-01, mean val. rec. loss:  1.01827470e-01\n",
      "Epoch: 665 mean train loss:  1.15157161e-01, mean val. rec. loss:  1.01678610e-01\n",
      "Epoch: 666 mean train loss:  1.15011627e-01, mean val. rec. loss:  1.01529860e-01\n",
      "Epoch: 667 mean train loss:  1.14866220e-01, mean val. rec. loss:  1.01381272e-01\n",
      "Epoch: 668 mean train loss:  1.14720991e-01, mean val. rec. loss:  1.01232839e-01\n",
      "Epoch: 669 mean train loss:  1.14575904e-01, mean val. rec. loss:  1.01084497e-01\n",
      "Epoch: 670 mean train loss:  1.14430892e-01, mean val. rec. loss:  1.00936282e-01\n",
      "Epoch: 671 mean train loss:  1.14285999e-01, mean val. rec. loss:  1.00788184e-01\n",
      "Epoch: 672 mean train loss:  1.14141239e-01, mean val. rec. loss:  1.00640278e-01\n",
      "Epoch: 673 mean train loss:  1.13996659e-01, mean val. rec. loss:  1.00492471e-01\n",
      "Epoch: 674 mean train loss:  1.13852190e-01, mean val. rec. loss:  1.00344800e-01\n",
      "Epoch: 675 mean train loss:  1.13707841e-01, mean val. rec. loss:  1.00197337e-01\n",
      "Epoch: 676 mean train loss:  1.13563633e-01, mean val. rec. loss:  1.00049920e-01\n",
      "Epoch: 677 mean train loss:  1.13419588e-01, mean val. rec. loss:  9.99026849e-02\n",
      "Epoch: 678 mean train loss:  1.13275596e-01, mean val. rec. loss:  9.97554947e-02\n",
      "Epoch: 679 mean train loss:  1.13131768e-01, mean val. rec. loss:  9.96085222e-02\n",
      "Epoch: 680 mean train loss:  1.12988082e-01, mean val. rec. loss:  9.94616133e-02\n",
      "Epoch: 681 mean train loss:  1.12844492e-01, mean val. rec. loss:  9.93148858e-02\n",
      "Epoch: 682 mean train loss:  1.12701051e-01, mean val. rec. loss:  9.91682853e-02\n",
      "Epoch: 683 mean train loss:  1.12557729e-01, mean val. rec. loss:  9.90218118e-02\n",
      "Epoch: 684 mean train loss:  1.12414542e-01, mean val. rec. loss:  9.88754653e-02\n",
      "Epoch: 685 mean train loss:  1.12271511e-01, mean val. rec. loss:  9.87292549e-02\n",
      "Epoch: 686 mean train loss:  1.12128569e-01, mean val. rec. loss:  9.85831534e-02\n",
      "Epoch: 687 mean train loss:  1.11985776e-01, mean val. rec. loss:  9.84372242e-02\n",
      "Epoch: 688 mean train loss:  1.11843103e-01, mean val. rec. loss:  9.82914402e-02\n",
      "Epoch: 689 mean train loss:  1.11700578e-01, mean val. rec. loss:  9.81457650e-02\n",
      "Epoch: 690 mean train loss:  1.11558180e-01, mean val. rec. loss:  9.80001624e-02\n",
      "Epoch: 691 mean train loss:  1.11415842e-01, mean val. rec. loss:  9.78546868e-02\n",
      "Epoch: 692 mean train loss:  1.11273630e-01, mean val. rec. loss:  9.77093745e-02\n",
      "Epoch: 693 mean train loss:  1.11131567e-01, mean val. rec. loss:  9.75641892e-02\n",
      "Epoch: 694 mean train loss:  1.10989631e-01, mean val. rec. loss:  9.74191128e-02\n",
      "Epoch: 695 mean train loss:  1.10847829e-01, mean val. rec. loss:  9.72741725e-02\n",
      "Epoch: 696 mean train loss:  1.10706131e-01, mean val. rec. loss:  9.71294135e-02\n",
      "Epoch: 697 mean train loss:  1.10564597e-01, mean val. rec. loss:  9.69847272e-02\n",
      "Epoch: 698 mean train loss:  1.10423182e-01, mean val. rec. loss:  9.68401679e-02\n",
      "Epoch: 699 mean train loss:  1.10281835e-01, mean val. rec. loss:  9.66956902e-02\n",
      "Epoch: 700 mean train loss:  1.10140584e-01, mean val. rec. loss:  9.65513939e-02\n",
      "Epoch: 701 mean train loss:  1.09999512e-01, mean val. rec. loss:  9.64071702e-02\n",
      "Epoch: 702 mean train loss:  1.09858514e-01, mean val. rec. loss:  9.62631099e-02\n",
      "Epoch: 703 mean train loss:  1.09717665e-01, mean val. rec. loss:  9.61191765e-02\n",
      "Epoch: 704 mean train loss:  1.09576951e-01, mean val. rec. loss:  9.59753701e-02\n",
      "Epoch: 705 mean train loss:  1.09436348e-01, mean val. rec. loss:  9.58316182e-02\n",
      "Epoch: 706 mean train loss:  1.09295812e-01, mean val. rec. loss:  9.56880839e-02\n",
      "Epoch: 707 mean train loss:  1.09155463e-01, mean val. rec. loss:  9.55446223e-02\n",
      "Epoch: 708 mean train loss:  1.09015195e-01, mean val. rec. loss:  9.54013058e-02\n",
      "Epoch: 709 mean train loss:  1.08875047e-01, mean val. rec. loss:  9.52580982e-02\n",
      "Epoch: 710 mean train loss:  1.08735025e-01, mean val. rec. loss:  9.51149812e-02\n",
      "Epoch: 711 mean train loss:  1.08595085e-01, mean val. rec. loss:  9.49720276e-02\n",
      "Epoch: 712 mean train loss:  1.08455331e-01, mean val. rec. loss:  9.48292645e-02\n",
      "Epoch: 713 mean train loss:  1.08315712e-01, mean val. rec. loss:  9.46865558e-02\n",
      "Epoch: 714 mean train loss:  1.08176159e-01, mean val. rec. loss:  9.45439469e-02\n",
      "Epoch: 715 mean train loss:  1.08036696e-01, mean val. rec. loss:  9.44014922e-02\n",
      "Epoch: 716 mean train loss:  1.07897389e-01, mean val. rec. loss:  9.42591101e-02\n",
      "Epoch: 717 mean train loss:  1.07758187e-01, mean val. rec. loss:  9.41169185e-02\n",
      "Epoch: 718 mean train loss:  1.07619148e-01, mean val. rec. loss:  9.39748358e-02\n",
      "Epoch: 719 mean train loss:  1.07480184e-01, mean val. rec. loss:  9.38327984e-02\n",
      "Epoch: 720 mean train loss:  1.07341302e-01, mean val. rec. loss:  9.36909697e-02\n",
      "Epoch: 721 mean train loss:  1.07202606e-01, mean val. rec. loss:  9.35491682e-02\n",
      "Epoch: 722 mean train loss:  1.07063947e-01, mean val. rec. loss:  9.34075663e-02\n",
      "Epoch: 723 mean train loss:  1.06925460e-01, mean val. rec. loss:  9.32660551e-02\n",
      "Epoch: 724 mean train loss:  1.06787062e-01, mean val. rec. loss:  9.31246981e-02\n",
      "Epoch: 725 mean train loss:  1.06648768e-01, mean val. rec. loss:  9.29833774e-02\n",
      "Epoch: 726 mean train loss:  1.06510608e-01, mean val. rec. loss:  9.28422472e-02\n",
      "Epoch: 727 mean train loss:  1.06372583e-01, mean val. rec. loss:  9.27012349e-02\n",
      "Epoch: 728 mean train loss:  1.06234647e-01, mean val. rec. loss:  9.25602862e-02\n",
      "Epoch: 729 mean train loss:  1.06096830e-01, mean val. rec. loss:  9.24195279e-02\n",
      "Epoch: 730 mean train loss:  1.05959147e-01, mean val. rec. loss:  9.22788060e-02\n",
      "Epoch: 731 mean train loss:  1.05821494e-01, mean val. rec. loss:  9.21382745e-02\n",
      "Epoch: 732 mean train loss:  1.05684034e-01, mean val. rec. loss:  9.19978066e-02\n",
      "Epoch: 733 mean train loss:  1.05546634e-01, mean val. rec. loss:  9.18574566e-02\n",
      "Epoch: 734 mean train loss:  1.05409383e-01, mean val. rec. loss:  9.17172608e-02\n",
      "Epoch: 735 mean train loss:  1.05272229e-01, mean val. rec. loss:  9.15772101e-02\n",
      "Epoch: 736 mean train loss:  1.05135217e-01, mean val. rec. loss:  9.14372139e-02\n",
      "Epoch: 737 mean train loss:  1.04998294e-01, mean val. rec. loss:  9.12974626e-02\n",
      "Epoch: 738 mean train loss:  1.04861527e-01, mean val. rec. loss:  9.11577204e-02\n",
      "Epoch: 739 mean train loss:  1.04724872e-01, mean val. rec. loss:  9.10181687e-02\n",
      "Epoch: 740 mean train loss:  1.04588299e-01, mean val. rec. loss:  9.08786080e-02\n",
      "Epoch: 741 mean train loss:  1.04451815e-01, mean val. rec. loss:  9.07392740e-02\n",
      "Epoch: 742 mean train loss:  1.04315488e-01, mean val. rec. loss:  9.05999672e-02\n",
      "Epoch: 743 mean train loss:  1.04179228e-01, mean val. rec. loss:  9.04608328e-02\n",
      "Epoch: 744 mean train loss:  1.04043079e-01, mean val. rec. loss:  9.03218164e-02\n",
      "Epoch: 745 mean train loss:  1.03907065e-01, mean val. rec. loss:  9.01829360e-02\n",
      "Epoch: 746 mean train loss:  1.03771170e-01, mean val. rec. loss:  9.00441101e-02\n",
      "Epoch: 747 mean train loss:  1.03635356e-01, mean val. rec. loss:  8.99054928e-02\n",
      "Epoch: 748 mean train loss:  1.03499714e-01, mean val. rec. loss:  8.97669299e-02\n",
      "Epoch: 749 mean train loss:  1.03364132e-01, mean val. rec. loss:  8.96285303e-02\n",
      "Epoch: 750 mean train loss:  1.03228684e-01, mean val. rec. loss:  8.94901307e-02\n",
      "Epoch: 751 mean train loss:  1.03093310e-01, mean val. rec. loss:  8.93519489e-02\n",
      "Epoch: 752 mean train loss:  1.02958055e-01, mean val. rec. loss:  8.92138396e-02\n",
      "Epoch: 753 mean train loss:  1.02822883e-01, mean val. rec. loss:  8.90759299e-02\n",
      "Epoch: 754 mean train loss:  1.02687844e-01, mean val. rec. loss:  8.89380656e-02\n",
      "Epoch: 755 mean train loss:  1.02552947e-01, mean val. rec. loss:  8.88002466e-02\n",
      "Epoch: 756 mean train loss:  1.02418110e-01, mean val. rec. loss:  8.86626544e-02\n",
      "Epoch: 757 mean train loss:  1.02283451e-01, mean val. rec. loss:  8.85251167e-02\n",
      "Epoch: 758 mean train loss:  1.02148837e-01, mean val. rec. loss:  8.83876968e-02\n",
      "Epoch: 759 mean train loss:  1.02014342e-01, mean val. rec. loss:  8.82504585e-02\n",
      "Epoch: 760 mean train loss:  1.01880018e-01, mean val. rec. loss:  8.81133380e-02\n",
      "Epoch: 761 mean train loss:  1.01745784e-01, mean val. rec. loss:  8.79763173e-02\n",
      "Epoch: 762 mean train loss:  1.01611669e-01, mean val. rec. loss:  8.78393511e-02\n",
      "Epoch: 763 mean train loss:  1.01477592e-01, mean val. rec. loss:  8.77025482e-02\n",
      "Epoch: 764 mean train loss:  1.01343655e-01, mean val. rec. loss:  8.75658722e-02\n",
      "Epoch: 765 mean train loss:  1.01209846e-01, mean val. rec. loss:  8.74292870e-02\n",
      "Epoch: 766 mean train loss:  1.01076155e-01, mean val. rec. loss:  8.72928560e-02\n",
      "Epoch: 767 mean train loss:  1.00942584e-01, mean val. rec. loss:  8.71564795e-02\n",
      "Epoch: 768 mean train loss:  1.00809073e-01, mean val. rec. loss:  8.70202390e-02\n",
      "Epoch: 769 mean train loss:  1.00675725e-01, mean val. rec. loss:  8.68842253e-02\n",
      "Epoch: 770 mean train loss:  1.00542467e-01, mean val. rec. loss:  8.67481753e-02\n",
      "Epoch: 771 mean train loss:  1.00409290e-01, mean val. rec. loss:  8.66123159e-02\n",
      "Epoch: 772 mean train loss:  1.00276263e-01, mean val. rec. loss:  8.64766197e-02\n",
      "Epoch: 773 mean train loss:  1.00143340e-01, mean val. rec. loss:  8.63409598e-02\n",
      "Epoch: 774 mean train loss:  1.00010536e-01, mean val. rec. loss:  8.62053997e-02\n",
      "Epoch: 775 mean train loss:  9.98777986e-02, mean val. rec. loss:  8.60699666e-02\n",
      "Epoch: 776 mean train loss:  9.97451510e-02, mean val. rec. loss:  8.59346424e-02\n",
      "Epoch: 777 mean train loss:  9.96126078e-02, mean val. rec. loss:  8.57994724e-02\n",
      "Epoch: 778 mean train loss:  9.94802284e-02, mean val. rec. loss:  8.56643840e-02\n",
      "Epoch: 779 mean train loss:  9.93479235e-02, mean val. rec. loss:  8.55294499e-02\n",
      "Epoch: 780 mean train loss:  9.92157601e-02, mean val. rec. loss:  8.53946246e-02\n",
      "Epoch: 781 mean train loss:  9.90837010e-02, mean val. rec. loss:  8.52599444e-02\n",
      "Epoch: 782 mean train loss:  9.89517759e-02, mean val. rec. loss:  8.51253187e-02\n",
      "Epoch: 783 mean train loss:  9.88199105e-02, mean val. rec. loss:  8.49909108e-02\n",
      "Epoch: 784 mean train loss:  9.86882387e-02, mean val. rec. loss:  8.48565482e-02\n",
      "Epoch: 785 mean train loss:  9.85566117e-02, mean val. rec. loss:  8.47223035e-02\n",
      "Epoch: 786 mean train loss:  9.84251038e-02, mean val. rec. loss:  8.45882130e-02\n",
      "Epoch: 787 mean train loss:  9.82937300e-02, mean val. rec. loss:  8.44541770e-02\n",
      "Epoch: 788 mean train loss:  9.81624008e-02, mean val. rec. loss:  8.43202770e-02\n",
      "Epoch: 789 mean train loss:  9.80312356e-02, mean val. rec. loss:  8.41865676e-02\n",
      "Epoch: 790 mean train loss:  9.79001895e-02, mean val. rec. loss:  8.40528763e-02\n",
      "Epoch: 791 mean train loss:  9.77691956e-02, mean val. rec. loss:  8.39192938e-02\n",
      "Epoch: 792 mean train loss:  9.76382836e-02, mean val. rec. loss:  8.37858837e-02\n",
      "Epoch: 793 mean train loss:  9.75075429e-02, mean val. rec. loss:  8.36525372e-02\n",
      "Epoch: 794 mean train loss:  9.73768618e-02, mean val. rec. loss:  8.35193902e-02\n",
      "Epoch: 795 mean train loss:  9.72463595e-02, mean val. rec. loss:  8.33863248e-02\n",
      "Epoch: 796 mean train loss:  9.71159466e-02, mean val. rec. loss:  8.32532958e-02\n",
      "Epoch: 797 mean train loss:  9.69856156e-02, mean val. rec. loss:  8.31204753e-02\n",
      "Epoch: 798 mean train loss:  9.68553963e-02, mean val. rec. loss:  8.29878182e-02\n",
      "Epoch: 799 mean train loss:  9.67253484e-02, mean val. rec. loss:  8.28552065e-02\n",
      "Epoch: 800 mean train loss:  9.65953526e-02, mean val. rec. loss:  8.27227036e-02\n",
      "Epoch: 801 mean train loss:  9.64654760e-02, mean val. rec. loss:  8.25903458e-02\n",
      "Epoch: 802 mean train loss:  9.63357260e-02, mean val. rec. loss:  8.24581423e-02\n",
      "Epoch: 803 mean train loss:  9.62060505e-02, mean val. rec. loss:  8.23259569e-02\n",
      "Epoch: 804 mean train loss:  9.60765091e-02, mean val. rec. loss:  8.21939348e-02\n",
      "Epoch: 805 mean train loss:  9.59470720e-02, mean val. rec. loss:  8.20620216e-02\n",
      "Epoch: 806 mean train loss:  9.58177391e-02, mean val. rec. loss:  8.19302898e-02\n",
      "Epoch: 807 mean train loss:  9.56885478e-02, mean val. rec. loss:  8.17986306e-02\n",
      "Epoch: 808 mean train loss:  9.55594533e-02, mean val. rec. loss:  8.16671619e-02\n",
      "Epoch: 809 mean train loss:  9.54304631e-02, mean val. rec. loss:  8.15356659e-02\n",
      "Epoch: 810 mean train loss:  9.53015697e-02, mean val. rec. loss:  8.14043605e-02\n",
      "Epoch: 811 mean train loss:  9.51727731e-02, mean val. rec. loss:  8.12731821e-02\n",
      "Epoch: 812 mean train loss:  9.50441032e-02, mean val. rec. loss:  8.11421398e-02\n",
      "Epoch: 813 mean train loss:  9.49155525e-02, mean val. rec. loss:  8.10111428e-02\n",
      "Epoch: 814 mean train loss:  9.47870688e-02, mean val. rec. loss:  8.08803635e-02\n",
      "Epoch: 815 mean train loss:  9.46587639e-02, mean val. rec. loss:  8.07496387e-02\n",
      "Epoch: 816 mean train loss:  9.45305260e-02, mean val. rec. loss:  8.06190591e-02\n",
      "Epoch: 817 mean train loss:  9.44024073e-02, mean val. rec. loss:  8.04885882e-02\n",
      "Epoch: 818 mean train loss:  9.42744152e-02, mean val. rec. loss:  8.03581900e-02\n",
      "Epoch: 819 mean train loss:  9.41465125e-02, mean val. rec. loss:  8.02280004e-02\n",
      "Epoch: 820 mean train loss:  9.40187141e-02, mean val. rec. loss:  8.00978562e-02\n",
      "Epoch: 821 mean train loss:  9.38910200e-02, mean val. rec. loss:  7.99678662e-02\n",
      "Epoch: 822 mean train loss:  9.37634525e-02, mean val. rec. loss:  7.98379850e-02\n",
      "Epoch: 823 mean train loss:  9.36359967e-02, mean val. rec. loss:  7.97082400e-02\n",
      "Epoch: 824 mean train loss:  9.35086601e-02, mean val. rec. loss:  7.95785493e-02\n",
      "Epoch: 825 mean train loss:  9.33814055e-02, mean val. rec. loss:  7.94490855e-02\n",
      "Epoch: 826 mean train loss:  9.32543147e-02, mean val. rec. loss:  7.93197033e-02\n",
      "Epoch: 827 mean train loss:  9.31272984e-02, mean val. rec. loss:  7.91904481e-02\n",
      "Epoch: 828 mean train loss:  9.30004311e-02, mean val. rec. loss:  7.90613018e-02\n",
      "Epoch: 829 mean train loss:  9.28736606e-02, mean val. rec. loss:  7.89322825e-02\n",
      "Epoch: 830 mean train loss:  9.27470093e-02, mean val. rec. loss:  7.88033720e-02\n",
      "Epoch: 831 mean train loss:  9.26204325e-02, mean val. rec. loss:  7.86745886e-02\n",
      "Epoch: 832 mean train loss:  9.24939823e-02, mean val. rec. loss:  7.85459140e-02\n",
      "Epoch: 833 mean train loss:  9.23676885e-02, mean val. rec. loss:  7.84173664e-02\n",
      "Epoch: 834 mean train loss:  9.22414395e-02, mean val. rec. loss:  7.82889639e-02\n",
      "Epoch: 835 mean train loss:  9.21153394e-02, mean val. rec. loss:  7.81606976e-02\n",
      "Epoch: 836 mean train loss:  9.19893734e-02, mean val. rec. loss:  7.80324857e-02\n",
      "Epoch: 837 mean train loss:  9.18634669e-02, mean val. rec. loss:  7.79044461e-02\n",
      "Epoch: 838 mean train loss:  9.17377095e-02, mean val. rec. loss:  7.77765335e-02\n",
      "Epoch: 839 mean train loss:  9.16120712e-02, mean val. rec. loss:  7.76487298e-02\n",
      "Epoch: 840 mean train loss:  9.14865298e-02, mean val. rec. loss:  7.75210350e-02\n",
      "Epoch: 841 mean train loss:  9.13611075e-02, mean val. rec. loss:  7.73935216e-02\n",
      "Epoch: 842 mean train loss:  9.12358044e-02, mean val. rec. loss:  7.72660808e-02\n",
      "Epoch: 843 mean train loss:  9.11106057e-02, mean val. rec. loss:  7.71388032e-02\n",
      "Epoch: 844 mean train loss:  9.09855186e-02, mean val. rec. loss:  7.70116346e-02\n",
      "Epoch: 845 mean train loss:  9.08605582e-02, mean val. rec. loss:  7.68845838e-02\n",
      "Epoch: 846 mean train loss:  9.07357169e-02, mean val. rec. loss:  7.67575875e-02\n",
      "Epoch: 847 mean train loss:  9.06109501e-02, mean val. rec. loss:  7.66307999e-02\n",
      "Epoch: 848 mean train loss:  9.04862951e-02, mean val. rec. loss:  7.65041029e-02\n",
      "Epoch: 849 mean train loss:  9.03617593e-02, mean val. rec. loss:  7.63775058e-02\n",
      "Epoch: 850 mean train loss:  9.02373277e-02, mean val. rec. loss:  7.62510719e-02\n",
      "Epoch: 851 mean train loss:  9.01130525e-02, mean val. rec. loss:  7.61247741e-02\n",
      "Epoch: 852 mean train loss:  8.99888817e-02, mean val. rec. loss:  7.59985217e-02\n",
      "Epoch: 853 mean train loss:  8.98647630e-02, mean val. rec. loss:  7.58724145e-02\n",
      "Epoch: 854 mean train loss:  8.97407783e-02, mean val. rec. loss:  7.57464297e-02\n",
      "Epoch: 855 mean train loss:  8.96169426e-02, mean val. rec. loss:  7.56206717e-02\n",
      "Epoch: 856 mean train loss:  8.94931815e-02, mean val. rec. loss:  7.54948547e-02\n",
      "Epoch: 857 mean train loss:  8.93695469e-02, mean val. rec. loss:  7.53693008e-02\n",
      "Epoch: 858 mean train loss:  8.92460613e-02, mean val. rec. loss:  7.52437877e-02\n",
      "Epoch: 859 mean train loss:  8.91226353e-02, mean val. rec. loss:  7.51185423e-02\n",
      "Epoch: 860 mean train loss:  8.89993881e-02, mean val. rec. loss:  7.49933059e-02\n",
      "Epoch: 861 mean train loss:  8.88762601e-02, mean val. rec. loss:  7.48682283e-02\n",
      "Epoch: 862 mean train loss:  8.87531693e-02, mean val. rec. loss:  7.47432051e-02\n",
      "Epoch: 863 mean train loss:  8.86302349e-02, mean val. rec. loss:  7.46184178e-02\n",
      "Epoch: 864 mean train loss:  8.85074421e-02, mean val. rec. loss:  7.44936713e-02\n",
      "Epoch: 865 mean train loss:  8.83847163e-02, mean val. rec. loss:  7.43690790e-02\n",
      "Epoch: 866 mean train loss:  8.82621097e-02, mean val. rec. loss:  7.42446183e-02\n",
      "Epoch: 867 mean train loss:  8.81396520e-02, mean val. rec. loss:  7.41202936e-02\n",
      "Epoch: 868 mean train loss:  8.80172837e-02, mean val. rec. loss:  7.39961096e-02\n",
      "Epoch: 869 mean train loss:  8.78950645e-02, mean val. rec. loss:  7.38720253e-02\n",
      "Epoch: 870 mean train loss:  8.77729346e-02, mean val. rec. loss:  7.37481270e-02\n",
      "Epoch: 871 mean train loss:  8.76509685e-02, mean val. rec. loss:  7.36243013e-02\n",
      "Epoch: 872 mean train loss:  8.75290919e-02, mean val. rec. loss:  7.35005981e-02\n",
      "Epoch: 873 mean train loss:  8.74073046e-02, mean val. rec. loss:  7.33770808e-02\n",
      "Epoch: 874 mean train loss:  8.72856663e-02, mean val. rec. loss:  7.32535817e-02\n",
      "Epoch: 875 mean train loss:  8.71640876e-02, mean val. rec. loss:  7.31303139e-02\n",
      "Epoch: 876 mean train loss:  8.70427175e-02, mean val. rec. loss:  7.30071005e-02\n",
      "Epoch: 877 mean train loss:  8.69214144e-02, mean val. rec. loss:  7.28840777e-02\n",
      "Epoch: 878 mean train loss:  8.68002305e-02, mean val. rec. loss:  7.27611728e-02\n",
      "Epoch: 879 mean train loss:  8.66792030e-02, mean val. rec. loss:  7.26384175e-02\n",
      "Epoch: 880 mean train loss:  8.65582947e-02, mean val. rec. loss:  7.25157893e-02\n",
      "Epoch: 881 mean train loss:  8.64375056e-02, mean val. rec. loss:  7.23932654e-02\n",
      "Epoch: 882 mean train loss:  8.63168059e-02, mean val. rec. loss:  7.22709139e-02\n",
      "Epoch: 883 mean train loss:  8.61962551e-02, mean val. rec. loss:  7.21486576e-02\n",
      "Epoch: 884 mean train loss:  8.60758086e-02, mean val. rec. loss:  7.20265328e-02\n",
      "Epoch: 885 mean train loss:  8.59554664e-02, mean val. rec. loss:  7.19045487e-02\n",
      "Epoch: 886 mean train loss:  8.58352657e-02, mean val. rec. loss:  7.17826508e-02\n",
      "Epoch: 887 mean train loss:  8.57151247e-02, mean val. rec. loss:  7.16609615e-02\n",
      "Epoch: 888 mean train loss:  8.55951847e-02, mean val. rec. loss:  7.15393629e-02\n",
      "Epoch: 889 mean train loss:  8.54753192e-02, mean val. rec. loss:  7.14179049e-02\n",
      "Epoch: 890 mean train loss:  8.53555729e-02, mean val. rec. loss:  7.12965649e-02\n",
      "Epoch: 891 mean train loss:  8.52359681e-02, mean val. rec. loss:  7.11753700e-02\n",
      "Epoch: 892 mean train loss:  8.51164825e-02, mean val. rec. loss:  7.10543293e-02\n",
      "Epoch: 893 mean train loss:  8.49971236e-02, mean val. rec. loss:  7.09333567e-02\n",
      "Epoch: 894 mean train loss:  8.48778317e-02, mean val. rec. loss:  7.08125565e-02\n",
      "Epoch: 895 mean train loss:  8.47587185e-02, mean val. rec. loss:  7.06918741e-02\n",
      "Epoch: 896 mean train loss:  8.46396947e-02, mean val. rec. loss:  7.05713279e-02\n",
      "Epoch: 897 mean train loss:  8.45208050e-02, mean val. rec. loss:  7.04509177e-02\n",
      "Epoch: 898 mean train loss:  8.44020420e-02, mean val. rec. loss:  7.03306618e-02\n",
      "Epoch: 899 mean train loss:  8.42833981e-02, mean val. rec. loss:  7.02104784e-02\n",
      "Epoch: 900 mean train loss:  8.41648287e-02, mean val. rec. loss:  7.00905037e-02\n",
      "Epoch: 901 mean train loss:  8.40464157e-02, mean val. rec. loss:  6.99706061e-02\n",
      "Epoch: 902 mean train loss:  8.39281443e-02, mean val. rec. loss:  6.98508445e-02\n",
      "Epoch: 903 mean train loss:  8.38099548e-02, mean val. rec. loss:  6.97312417e-02\n",
      "Epoch: 904 mean train loss:  8.36919217e-02, mean val. rec. loss:  6.96117750e-02\n",
      "Epoch: 905 mean train loss:  8.35739631e-02, mean val. rec. loss:  6.94923718e-02\n",
      "Epoch: 906 mean train loss:  8.34561460e-02, mean val. rec. loss:  6.93731954e-02\n",
      "Epoch: 907 mean train loss:  8.33384928e-02, mean val. rec. loss:  6.92541052e-02\n",
      "Epoch: 908 mean train loss:  8.32209067e-02, mean val. rec. loss:  6.91351239e-02\n",
      "Epoch: 909 mean train loss:  8.31034323e-02, mean val. rec. loss:  6.90162967e-02\n",
      "Epoch: 910 mean train loss:  8.29861143e-02, mean val. rec. loss:  6.88976329e-02\n",
      "Epoch: 911 mean train loss:  8.28689005e-02, mean val. rec. loss:  6.87790371e-02\n",
      "Epoch: 912 mean train loss:  8.27518283e-02, mean val. rec. loss:  6.86606862e-02\n",
      "Epoch: 913 mean train loss:  8.26348828e-02, mean val. rec. loss:  6.85423988e-02\n",
      "Epoch: 914 mean train loss:  8.25180415e-02, mean val. rec. loss:  6.84242974e-02\n",
      "Epoch: 915 mean train loss:  8.24013939e-02, mean val. rec. loss:  6.83063094e-02\n",
      "Epoch: 916 mean train loss:  8.22848208e-02, mean val. rec. loss:  6.81883759e-02\n",
      "Epoch: 917 mean train loss:  8.21683445e-02, mean val. rec. loss:  6.80706827e-02\n",
      "Epoch: 918 mean train loss:  8.20520023e-02, mean val. rec. loss:  6.79531302e-02\n",
      "Epoch: 919 mean train loss:  8.19358388e-02, mean val. rec. loss:  6.78356683e-02\n",
      "Epoch: 920 mean train loss:  8.18197424e-02, mean val. rec. loss:  6.77184151e-02\n",
      "Epoch: 921 mean train loss:  8.17038471e-02, mean val. rec. loss:  6.76012300e-02\n",
      "Epoch: 922 mean train loss:  8.15880040e-02, mean val. rec. loss:  6.74841900e-02\n",
      "Epoch: 923 mean train loss:  8.14723098e-02, mean val. rec. loss:  6.73673088e-02\n",
      "Epoch: 924 mean train loss:  8.13567348e-02, mean val. rec. loss:  6.72505183e-02\n",
      "Epoch: 925 mean train loss:  8.12412417e-02, mean val. rec. loss:  6.71339182e-02\n",
      "Epoch: 926 mean train loss:  8.11259424e-02, mean val. rec. loss:  6.70174044e-02\n",
      "Epoch: 927 mean train loss:  8.10107100e-02, mean val. rec. loss:  6.69010267e-02\n",
      "Epoch: 928 mean train loss:  8.08956415e-02, mean val. rec. loss:  6.67848893e-02\n",
      "Epoch: 929 mean train loss:  8.07806997e-02, mean val. rec. loss:  6.66688155e-02\n",
      "Epoch: 930 mean train loss:  8.06658770e-02, mean val. rec. loss:  6.65529185e-02\n",
      "Epoch: 931 mean train loss:  8.05511512e-02, mean val. rec. loss:  6.64370851e-02\n",
      "Epoch: 932 mean train loss:  8.04365818e-02, mean val. rec. loss:  6.63214603e-02\n",
      "Epoch: 933 mean train loss:  8.03221613e-02, mean val. rec. loss:  6.62059444e-02\n",
      "Epoch: 934 mean train loss:  8.02078377e-02, mean val. rec. loss:  6.60905872e-02\n",
      "Epoch: 935 mean train loss:  8.00936482e-02, mean val. rec. loss:  6.59753570e-02\n",
      "Epoch: 936 mean train loss:  7.99795853e-02, mean val. rec. loss:  6.58602629e-02\n",
      "Epoch: 937 mean train loss:  7.98656565e-02, mean val. rec. loss:  6.57453231e-02\n",
      "Epoch: 938 mean train loss:  7.97518469e-02, mean val. rec. loss:  6.56305465e-02\n",
      "Epoch: 939 mean train loss:  7.96382011e-02, mean val. rec. loss:  6.55158697e-02\n",
      "Epoch: 940 mean train loss:  7.95246373e-02, mean val. rec. loss:  6.54013925e-02\n",
      "Epoch: 941 mean train loss:  7.94112597e-02, mean val. rec. loss:  6.52869743e-02\n",
      "Epoch: 942 mean train loss:  7.92979491e-02, mean val. rec. loss:  6.51727465e-02\n",
      "Epoch: 943 mean train loss:  7.91847801e-02, mean val. rec. loss:  6.50586413e-02\n",
      "Epoch: 944 mean train loss:  7.90717228e-02, mean val. rec. loss:  6.49446630e-02\n",
      "Epoch: 945 mean train loss:  7.89588219e-02, mean val. rec. loss:  6.48309161e-02\n",
      "Epoch: 946 mean train loss:  7.88460700e-02, mean val. rec. loss:  6.47172100e-02\n",
      "Epoch: 947 mean train loss:  7.87333851e-02, mean val. rec. loss:  6.46037080e-02\n",
      "Epoch: 948 mean train loss:  7.86208790e-02, mean val. rec. loss:  6.44902922e-02\n",
      "Epoch: 949 mean train loss:  7.85084697e-02, mean val. rec. loss:  6.43770624e-02\n",
      "Epoch: 950 mean train loss:  7.83962020e-02, mean val. rec. loss:  6.42639686e-02\n",
      "Epoch: 951 mean train loss:  7.82840758e-02, mean val. rec. loss:  6.41510472e-02\n",
      "Epoch: 952 mean train loss:  7.81721060e-02, mean val. rec. loss:  6.40382710e-02\n",
      "Epoch: 953 mean train loss:  7.80602554e-02, mean val. rec. loss:  6.39255628e-02\n",
      "Epoch: 954 mean train loss:  7.79484941e-02, mean val. rec. loss:  6.38130678e-02\n",
      "Epoch: 955 mean train loss:  7.78369191e-02, mean val. rec. loss:  6.37006680e-02\n",
      "Epoch: 956 mean train loss:  7.77254112e-02, mean val. rec. loss:  6.35884860e-02\n",
      "Epoch: 957 mean train loss:  7.76140745e-02, mean val. rec. loss:  6.34764083e-02\n",
      "Epoch: 958 mean train loss:  7.75028943e-02, mean val. rec. loss:  6.33644712e-02\n",
      "Epoch: 959 mean train loss:  7.73918035e-02, mean val. rec. loss:  6.32526112e-02\n",
      "Epoch: 960 mean train loss:  7.72808318e-02, mean val. rec. loss:  6.31409871e-02\n",
      "Epoch: 961 mean train loss:  7.71699942e-02, mean val. rec. loss:  6.30294990e-02\n",
      "Epoch: 962 mean train loss:  7.70593280e-02, mean val. rec. loss:  6.29181244e-02\n",
      "Epoch: 963 mean train loss:  7.69487660e-02, mean val. rec. loss:  6.28069539e-02\n",
      "Epoch: 964 mean train loss:  7.68383902e-02, mean val. rec. loss:  6.26958922e-02\n",
      "Epoch: 965 mean train loss:  7.67281039e-02, mean val. rec. loss:  6.25849711e-02\n",
      "Epoch: 966 mean train loss:  7.66179516e-02, mean val. rec. loss:  6.24742270e-02\n",
      "Epoch: 967 mean train loss:  7.65079110e-02, mean val. rec. loss:  6.23636053e-02\n",
      "Epoch: 968 mean train loss:  7.63980343e-02, mean val. rec. loss:  6.22531650e-02\n",
      "Epoch: 969 mean train loss:  7.62883066e-02, mean val. rec. loss:  6.21428155e-02\n",
      "Epoch: 970 mean train loss:  7.61786534e-02, mean val. rec. loss:  6.20326474e-02\n",
      "Epoch: 971 mean train loss:  7.60691715e-02, mean val. rec. loss:  6.19225927e-02\n",
      "Epoch: 972 mean train loss:  7.59597938e-02, mean val. rec. loss:  6.18126832e-02\n",
      "Epoch: 973 mean train loss:  7.58505801e-02, mean val. rec. loss:  6.17030140e-02\n",
      "Epoch: 974 mean train loss:  7.57415079e-02, mean val. rec. loss:  6.15934447e-02\n",
      "Epoch: 975 mean train loss:  7.56325623e-02, mean val. rec. loss:  6.14840205e-02\n",
      "Epoch: 976 mean train loss:  7.55237135e-02, mean val. rec. loss:  6.13746416e-02\n",
      "Epoch: 977 mean train loss:  7.54150212e-02, mean val. rec. loss:  6.12655440e-02\n",
      "Epoch: 978 mean train loss:  7.53064927e-02, mean val. rec. loss:  6.11565371e-02\n",
      "Epoch: 979 mean train loss:  7.51980685e-02, mean val. rec. loss:  6.10477116e-02\n",
      "Epoch: 980 mean train loss:  7.50898082e-02, mean val. rec. loss:  6.09390041e-02\n",
      "Epoch: 981 mean train loss:  7.49816448e-02, mean val. rec. loss:  6.08304463e-02\n",
      "Epoch: 982 mean train loss:  7.48736154e-02, mean val. rec. loss:  6.07220426e-02\n",
      "Epoch: 983 mean train loss:  7.47657275e-02, mean val. rec. loss:  6.06137887e-02\n",
      "Epoch: 984 mean train loss:  7.46579812e-02, mean val. rec. loss:  6.05056890e-02\n",
      "Epoch: 985 mean train loss:  7.45503689e-02, mean val. rec. loss:  6.03977163e-02\n",
      "Epoch: 986 mean train loss:  7.44428684e-02, mean val. rec. loss:  6.02899340e-02\n",
      "Epoch: 987 mean train loss:  7.43355689e-02, mean val. rec. loss:  6.01822698e-02\n",
      "Epoch: 988 mean train loss:  7.42283440e-02, mean val. rec. loss:  6.00748005e-02\n",
      "Epoch: 989 mean train loss:  7.41213276e-02, mean val. rec. loss:  5.99674583e-02\n",
      "Epoch: 990 mean train loss:  7.40143858e-02, mean val. rec. loss:  5.98602522e-02\n",
      "Epoch: 991 mean train loss:  7.39075929e-02, mean val. rec. loss:  5.97532002e-02\n",
      "Epoch: 992 mean train loss:  7.38009266e-02, mean val. rec. loss:  5.96463161e-02\n",
      "Epoch: 993 mean train loss:  7.36943944e-02, mean val. rec. loss:  5.95395863e-02\n",
      "Epoch: 994 mean train loss:  7.35880186e-02, mean val. rec. loss:  5.94329607e-02\n",
      "Epoch: 995 mean train loss:  7.34817397e-02, mean val. rec. loss:  5.93265574e-02\n",
      "Epoch: 996 mean train loss:  7.33756693e-02, mean val. rec. loss:  5.92202494e-02\n",
      "Epoch: 997 mean train loss:  7.32696734e-02, mean val. rec. loss:  5.91141591e-02\n",
      "Epoch: 998 mean train loss:  7.31638861e-02, mean val. rec. loss:  5.90081867e-02\n",
      "Epoch: 999 mean train loss:  7.30581882e-02, mean val. rec. loss:  5.89023459e-02\n",
      "Epoch: 1000 mean train loss:  7.29526318e-02, mean val. rec. loss:  5.87966728e-02\n",
      "Epoch: 1001 mean train loss:  7.28472169e-02, mean val. rec. loss:  5.86911541e-02\n",
      "Epoch: 1002 mean train loss:  7.27419286e-02, mean val. rec. loss:  5.85858076e-02\n",
      "Epoch: 1003 mean train loss:  7.26367968e-02, mean val. rec. loss:  5.84806018e-02\n",
      "Epoch: 1004 mean train loss:  7.25318140e-02, mean val. rec. loss:  5.83755185e-02\n",
      "Epoch: 1005 mean train loss:  7.24269279e-02, mean val. rec. loss:  5.82706483e-02\n",
      "Epoch: 1006 mean train loss:  7.23222356e-02, mean val. rec. loss:  5.81658961e-02\n",
      "Epoch: 1007 mean train loss:  7.22176401e-02, mean val. rec. loss:  5.80612618e-02\n",
      "Epoch: 1008 mean train loss:  7.21131786e-02, mean val. rec. loss:  5.79568725e-02\n",
      "Epoch: 1009 mean train loss:  7.20089183e-02, mean val. rec. loss:  5.78525693e-02\n",
      "Epoch: 1010 mean train loss:  7.19047325e-02, mean val. rec. loss:  5.77484294e-02\n",
      "Epoch: 1011 mean train loss:  7.18007031e-02, mean val. rec. loss:  5.76444347e-02\n",
      "Epoch: 1012 mean train loss:  7.16967928e-02, mean val. rec. loss:  5.75405851e-02\n",
      "Epoch: 1013 mean train loss:  7.15930018e-02, mean val. rec. loss:  5.74369124e-02\n",
      "Epoch: 1014 mean train loss:  7.14894193e-02, mean val. rec. loss:  5.73334303e-02\n",
      "Epoch: 1015 mean train loss:  7.13859709e-02, mean val. rec. loss:  5.72300116e-02\n",
      "Epoch: 1016 mean train loss:  7.12826044e-02, mean val. rec. loss:  5.71268696e-02\n",
      "Epoch: 1017 mean train loss:  7.11794614e-02, mean val. rec. loss:  5.70238184e-02\n",
      "Epoch: 1018 mean train loss:  7.10764003e-02, mean val. rec. loss:  5.69208805e-02\n",
      "Epoch: 1019 mean train loss:  7.09734585e-02, mean val. rec. loss:  5.68181241e-02\n",
      "Epoch: 1020 mean train loss:  7.08706804e-02, mean val. rec. loss:  5.67155446e-02\n",
      "Epoch: 1021 mean train loss:  7.07680514e-02, mean val. rec. loss:  5.66131147e-02\n",
      "Epoch: 1022 mean train loss:  7.06655713e-02, mean val. rec. loss:  5.65108709e-02\n",
      "Epoch: 1023 mean train loss:  7.05632402e-02, mean val. rec. loss:  5.64087449e-02\n",
      "Epoch: 1024 mean train loss:  7.04610581e-02, mean val. rec. loss:  5.63068095e-02\n",
      "Epoch: 1025 mean train loss:  7.03590101e-02, mean val. rec. loss:  5.62049966e-02\n",
      "Epoch: 1026 mean train loss:  7.02570663e-02, mean val. rec. loss:  5.61033832e-02\n",
      "Epoch: 1027 mean train loss:  7.01553013e-02, mean val. rec. loss:  5.60018832e-02\n",
      "Epoch: 1028 mean train loss:  7.00536481e-02, mean val. rec. loss:  5.59005420e-02\n",
      "Epoch: 1029 mean train loss:  6.99521736e-02, mean val. rec. loss:  5.57993822e-02\n",
      "Epoch: 1030 mean train loss:  6.98507959e-02, mean val. rec. loss:  5.56983358e-02\n",
      "Epoch: 1031 mean train loss:  6.97495896e-02, mean val. rec. loss:  5.55974981e-02\n",
      "Epoch: 1032 mean train loss:  6.96485322e-02, mean val. rec. loss:  5.54967828e-02\n",
      "Epoch: 1033 mean train loss:  6.95476090e-02, mean val. rec. loss:  5.53962807e-02\n",
      "Epoch: 1034 mean train loss:  6.94468421e-02, mean val. rec. loss:  5.52958966e-02\n",
      "Epoch: 1035 mean train loss:  6.93462168e-02, mean val. rec. loss:  5.51956848e-02\n",
      "Epoch: 1036 mean train loss:  6.92457255e-02, mean val. rec. loss:  5.50955910e-02\n",
      "Epoch: 1037 mean train loss:  6.91453311e-02, mean val. rec. loss:  5.49956967e-02\n",
      "Epoch: 1038 mean train loss:  6.90451378e-02, mean val. rec. loss:  5.48959476e-02\n",
      "Epoch: 1039 mean train loss:  6.89450563e-02, mean val. rec. loss:  5.47963980e-02\n",
      "Epoch: 1040 mean train loss:  6.88451684e-02, mean val. rec. loss:  5.46969619e-02\n",
      "Epoch: 1041 mean train loss:  6.87453773e-02, mean val. rec. loss:  5.45976800e-02\n",
      "Epoch: 1042 mean train loss:  6.86457054e-02, mean val. rec. loss:  5.44985931e-02\n",
      "Epoch: 1043 mean train loss:  6.85462570e-02, mean val. rec. loss:  5.43996468e-02\n",
      "Epoch: 1044 mean train loss:  6.84468980e-02, mean val. rec. loss:  5.43008412e-02\n",
      "Epoch: 1045 mean train loss:  6.83476730e-02, mean val. rec. loss:  5.42022034e-02\n",
      "Epoch: 1046 mean train loss:  6.82486119e-02, mean val. rec. loss:  5.41037742e-02\n",
      "Epoch: 1047 mean train loss:  6.81497221e-02, mean val. rec. loss:  5.40054630e-02\n",
      "Epoch: 1048 mean train loss:  6.80509292e-02, mean val. rec. loss:  5.39072742e-02\n",
      "Epoch: 1049 mean train loss:  6.79522853e-02, mean val. rec. loss:  5.38092850e-02\n",
      "Epoch: 1050 mean train loss:  6.78538126e-02, mean val. rec. loss:  5.37114092e-02\n",
      "Epoch: 1051 mean train loss:  6.77554741e-02, mean val. rec. loss:  5.36137512e-02\n",
      "Epoch: 1052 mean train loss:  6.76572547e-02, mean val. rec. loss:  5.35162745e-02\n",
      "Epoch: 1053 mean train loss:  6.75592290e-02, mean val. rec. loss:  5.34189068e-02\n",
      "Epoch: 1054 mean train loss:  6.74613225e-02, mean val. rec. loss:  5.33216932e-02\n",
      "Epoch: 1055 mean train loss:  6.73635575e-02, mean val. rec. loss:  5.32247156e-02\n",
      "Epoch: 1056 mean train loss:  6.72659563e-02, mean val. rec. loss:  5.31278286e-02\n",
      "Epoch: 1057 mean train loss:  6.71684967e-02, mean val. rec. loss:  5.30311140e-02\n",
      "Epoch: 1058 mean train loss:  6.70711712e-02, mean val. rec. loss:  5.29345627e-02\n",
      "Epoch: 1059 mean train loss:  6.69739797e-02, mean val. rec. loss:  5.28381883e-02\n",
      "Epoch: 1060 mean train loss:  6.68769671e-02, mean val. rec. loss:  5.27419681e-02\n",
      "Epoch: 1061 mean train loss:  6.67800736e-02, mean val. rec. loss:  5.26459339e-02\n",
      "Epoch: 1062 mean train loss:  6.66833291e-02, mean val. rec. loss:  5.25499859e-02\n",
      "Epoch: 1063 mean train loss:  6.65867186e-02, mean val. rec. loss:  5.24542692e-02\n",
      "Epoch: 1064 mean train loss:  6.64902720e-02, mean val. rec. loss:  5.23587067e-02\n",
      "Epoch: 1065 mean train loss:  6.63939819e-02, mean val. rec. loss:  5.22632576e-02\n",
      "Epoch: 1066 mean train loss:  6.62977886e-02, mean val. rec. loss:  5.21680580e-02\n",
      "Epoch: 1067 mean train loss:  6.62018113e-02, mean val. rec. loss:  5.20729310e-02\n",
      "Epoch: 1068 mean train loss:  6.61059234e-02, mean val. rec. loss:  5.19780217e-02\n",
      "Epoch: 1069 mean train loss:  6.60102291e-02, mean val. rec. loss:  5.18832394e-02\n",
      "Epoch: 1070 mean train loss:  6.59146317e-02, mean val. rec. loss:  5.17886385e-02\n",
      "Epoch: 1071 mean train loss:  6.58191982e-02, mean val. rec. loss:  5.16941556e-02\n",
      "Epoch: 1072 mean train loss:  6.57239285e-02, mean val. rec. loss:  5.15999221e-02\n",
      "Epoch: 1073 mean train loss:  6.56287929e-02, mean val. rec. loss:  5.15058248e-02\n",
      "Epoch: 1074 mean train loss:  6.55338212e-02, mean val. rec. loss:  5.14118771e-02\n",
      "Epoch: 1075 mean train loss:  6.54389687e-02, mean val. rec. loss:  5.13180745e-02\n",
      "Epoch: 1076 mean train loss:  6.53442800e-02, mean val. rec. loss:  5.12244534e-02\n",
      "Epoch: 1077 mean train loss:  6.52497403e-02, mean val. rec. loss:  5.11309865e-02\n",
      "Epoch: 1078 mean train loss:  6.51553421e-02, mean val. rec. loss:  5.10376920e-02\n",
      "Epoch: 1079 mean train loss:  6.50610855e-02, mean val. rec. loss:  5.09445472e-02\n",
      "Epoch: 1080 mean train loss:  6.49669853e-02, mean val. rec. loss:  5.08515565e-02\n",
      "Epoch: 1081 mean train loss:  6.48730043e-02, mean val. rec. loss:  5.07587564e-02\n",
      "Epoch: 1082 mean train loss:  6.47792169e-02, mean val. rec. loss:  5.06661242e-02\n",
      "Epoch: 1083 mean train loss:  6.46855636e-02, mean val. rec. loss:  5.05736234e-02\n",
      "Epoch: 1084 mean train loss:  6.45920221e-02, mean val. rec. loss:  5.04813177e-02\n",
      "Epoch: 1085 mean train loss:  6.44986667e-02, mean val. rec. loss:  5.03891436e-02\n",
      "Epoch: 1086 mean train loss:  6.44054678e-02, mean val. rec. loss:  5.02971871e-02\n",
      "Epoch: 1087 mean train loss:  6.43124030e-02, mean val. rec. loss:  5.02053486e-02\n",
      "Epoch: 1088 mean train loss:  6.42194573e-02, mean val. rec. loss:  5.01137143e-02\n",
      "Epoch: 1089 mean train loss:  6.41267202e-02, mean val. rec. loss:  5.00222024e-02\n",
      "Epoch: 1090 mean train loss:  6.40340725e-02, mean val. rec. loss:  4.99308220e-02\n",
      "Epoch: 1091 mean train loss:  6.39415664e-02, mean val. rec. loss:  4.98397002e-02\n",
      "Epoch: 1092 mean train loss:  6.38492688e-02, mean val. rec. loss:  4.97486781e-02\n",
      "Epoch: 1093 mean train loss:  6.37570829e-02, mean val. rec. loss:  4.96578421e-02\n",
      "Epoch: 1094 mean train loss:  6.36650460e-02, mean val. rec. loss:  4.95671648e-02\n",
      "Epoch: 1095 mean train loss:  6.35731506e-02, mean val. rec. loss:  4.94766371e-02\n",
      "Epoch: 1096 mean train loss:  6.34814191e-02, mean val. rec. loss:  4.93862955e-02\n",
      "Epoch: 1097 mean train loss:  6.33898217e-02, mean val. rec. loss:  4.92960899e-02\n",
      "Epoch: 1098 mean train loss:  6.32983658e-02, mean val. rec. loss:  4.92060885e-02\n",
      "Epoch: 1099 mean train loss:  6.32070961e-02, mean val. rec. loss:  4.91162548e-02\n",
      "Epoch: 1100 mean train loss:  6.31159679e-02, mean val. rec. loss:  4.90265573e-02\n",
      "Epoch: 1101 mean train loss:  6.30249813e-02, mean val. rec. loss:  4.89370503e-02\n",
      "Epoch: 1102 mean train loss:  6.29341511e-02, mean val. rec. loss:  4.88476974e-02\n",
      "Epoch: 1103 mean train loss:  6.28434624e-02, mean val. rec. loss:  4.87585170e-02\n",
      "Epoch: 1104 mean train loss:  6.27529153e-02, mean val. rec. loss:  4.86694590e-02\n",
      "Epoch: 1105 mean train loss:  6.26624873e-02, mean val. rec. loss:  4.85806097e-02\n",
      "Epoch: 1106 mean train loss:  6.25722679e-02, mean val. rec. loss:  4.84919327e-02\n",
      "Epoch: 1107 mean train loss:  6.24821863e-02, mean val. rec. loss:  4.84034009e-02\n",
      "Epoch: 1108 mean train loss:  6.23922164e-02, mean val. rec. loss:  4.83149961e-02\n",
      "Epoch: 1109 mean train loss:  6.23024179e-02, mean val. rec. loss:  4.82268271e-02\n",
      "Epoch: 1110 mean train loss:  6.22127832e-02, mean val. rec. loss:  4.81387897e-02\n",
      "Epoch: 1111 mean train loss:  6.21232640e-02, mean val. rec. loss:  4.80509700e-02\n",
      "Epoch: 1112 mean train loss:  6.20339570e-02, mean val. rec. loss:  4.79632320e-02\n",
      "Epoch: 1113 mean train loss:  6.19447618e-02, mean val. rec. loss:  4.78757207e-02\n",
      "Epoch: 1114 mean train loss:  6.18557044e-02, mean val. rec. loss:  4.77883592e-02\n",
      "Epoch: 1115 mean train loss:  6.17667886e-02, mean val. rec. loss:  4.77011518e-02\n",
      "Epoch: 1116 mean train loss:  6.16780664e-02, mean val. rec. loss:  4.76140987e-02\n",
      "Epoch: 1117 mean train loss:  6.15894708e-02, mean val. rec. loss:  4.75272543e-02\n",
      "Epoch: 1118 mean train loss:  6.15010242e-02, mean val. rec. loss:  4.74405504e-02\n",
      "Epoch: 1119 mean train loss:  6.14127117e-02, mean val. rec. loss:  4.73540053e-02\n",
      "Epoch: 1120 mean train loss:  6.13245853e-02, mean val. rec. loss:  4.72676371e-02\n",
      "Epoch: 1121 mean train loss:  6.12365671e-02, mean val. rec. loss:  4.71814277e-02\n",
      "Epoch: 1122 mean train loss:  6.11487089e-02, mean val. rec. loss:  4.70954178e-02\n",
      "Epoch: 1123 mean train loss:  6.10610630e-02, mean val. rec. loss:  4.70095259e-02\n",
      "Epoch: 1124 mean train loss:  6.09734917e-02, mean val. rec. loss:  4.69238064e-02\n",
      "Epoch: 1125 mean train loss:  6.08860693e-02, mean val. rec. loss:  4.68383045e-02\n",
      "Epoch: 1126 mean train loss:  6.07988294e-02, mean val. rec. loss:  4.67528979e-02\n",
      "Epoch: 1127 mean train loss:  6.07117385e-02, mean val. rec. loss:  4.66676909e-02\n",
      "Epoch: 1128 mean train loss:  6.06247816e-02, mean val. rec. loss:  4.65826427e-02\n",
      "Epoch: 1129 mean train loss:  6.05379812e-02, mean val. rec. loss:  4.64977487e-02\n",
      "Epoch: 1130 mean train loss:  6.04513558e-02, mean val. rec. loss:  4.64130905e-02\n",
      "Epoch: 1131 mean train loss:  6.03648719e-02, mean val. rec. loss:  4.63285412e-02\n",
      "Epoch: 1132 mean train loss:  6.02785184e-02, mean val. rec. loss:  4.62441552e-02\n",
      "Epoch: 1133 mean train loss:  6.01923177e-02, mean val. rec. loss:  4.61599416e-02\n",
      "Epoch: 1134 mean train loss:  6.01062547e-02, mean val. rec. loss:  4.60759230e-02\n",
      "Epoch: 1135 mean train loss:  6.00203704e-02, mean val. rec. loss:  4.59920450e-02\n",
      "Epoch: 1136 mean train loss:  5.99346240e-02, mean val. rec. loss:  4.59083621e-02\n",
      "Epoch: 1137 mean train loss:  5.98490377e-02, mean val. rec. loss:  4.58248197e-02\n",
      "Epoch: 1138 mean train loss:  5.97636265e-02, mean val. rec. loss:  4.57414498e-02\n",
      "Epoch: 1139 mean train loss:  5.96783307e-02, mean val. rec. loss:  4.56582250e-02\n",
      "Epoch: 1140 mean train loss:  5.95931876e-02, mean val. rec. loss:  4.55751816e-02\n",
      "Epoch: 1141 mean train loss:  5.95081861e-02, mean val. rec. loss:  4.54922970e-02\n",
      "Epoch: 1142 mean train loss:  5.94233447e-02, mean val. rec. loss:  4.54095757e-02\n",
      "Epoch: 1143 mean train loss:  5.93386486e-02, mean val. rec. loss:  4.53270494e-02\n",
      "Epoch: 1144 mean train loss:  5.92541014e-02, mean val. rec. loss:  4.52446728e-02\n",
      "Epoch: 1145 mean train loss:  5.91697553e-02, mean val. rec. loss:  4.51625230e-02\n",
      "Epoch: 1146 mean train loss:  5.90855359e-02, mean val. rec. loss:  4.50804594e-02\n",
      "Epoch: 1147 mean train loss:  5.90014319e-02, mean val. rec. loss:  4.49985862e-02\n",
      "Epoch: 1148 mean train loss:  5.89174955e-02, mean val. rec. loss:  4.49169036e-02\n",
      "Epoch: 1149 mean train loss:  5.88337268e-02, mean val. rec. loss:  4.48353480e-02\n",
      "Epoch: 1150 mean train loss:  5.87500846e-02, mean val. rec. loss:  4.47539784e-02\n",
      "Epoch: 1151 mean train loss:  5.86666138e-02, mean val. rec. loss:  4.46727812e-02\n",
      "Epoch: 1152 mean train loss:  5.85832733e-02, mean val. rec. loss:  4.45917789e-02\n",
      "Epoch: 1153 mean train loss:  5.85001042e-02, mean val. rec. loss:  4.45109128e-02\n",
      "Epoch: 1154 mean train loss:  5.84171138e-02, mean val. rec. loss:  4.44302100e-02\n",
      "Epoch: 1155 mean train loss:  5.83342351e-02, mean val. rec. loss:  4.43496749e-02\n",
      "Epoch: 1156 mean train loss:  5.82515017e-02, mean val. rec. loss:  4.42693214e-02\n",
      "Epoch: 1157 mean train loss:  5.81689285e-02, mean val. rec. loss:  4.41890993e-02\n",
      "Epoch: 1158 mean train loss:  5.80864968e-02, mean val. rec. loss:  4.41090633e-02\n",
      "Epoch: 1159 mean train loss:  5.80042252e-02, mean val. rec. loss:  4.40292177e-02\n",
      "Epoch: 1160 mean train loss:  5.79221137e-02, mean val. rec. loss:  4.39494946e-02\n",
      "Epoch: 1161 mean train loss:  5.78401662e-02, mean val. rec. loss:  4.38699983e-02\n",
      "Epoch: 1162 mean train loss:  5.77583527e-02, mean val. rec. loss:  4.37906154e-02\n",
      "Epoch: 1163 mean train loss:  5.76766808e-02, mean val. rec. loss:  4.37114049e-02\n",
      "Epoch: 1164 mean train loss:  5.75951392e-02, mean val. rec. loss:  4.36323758e-02\n",
      "Epoch: 1165 mean train loss:  5.75137689e-02, mean val. rec. loss:  4.35535372e-02\n",
      "Epoch: 1166 mean train loss:  5.74325886e-02, mean val. rec. loss:  4.34748256e-02\n",
      "Epoch: 1167 mean train loss:  5.73515200e-02, mean val. rec. loss:  4.33962864e-02\n",
      "Epoch: 1168 mean train loss:  5.72705817e-02, mean val. rec. loss:  4.33178832e-02\n",
      "Epoch: 1169 mean train loss:  5.71897924e-02, mean val. rec. loss:  4.32397160e-02\n",
      "Epoch: 1170 mean train loss:  5.71092192e-02, mean val. rec. loss:  4.31616757e-02\n",
      "Epoch: 1171 mean train loss:  5.70287502e-02, mean val. rec. loss:  4.30837806e-02\n",
      "Epoch: 1172 mean train loss:  5.69484190e-02, mean val. rec. loss:  4.30061123e-02\n",
      "Epoch: 1173 mean train loss:  5.68682927e-02, mean val. rec. loss:  4.29285800e-02\n",
      "Epoch: 1174 mean train loss:  5.67882557e-02, mean val. rec. loss:  4.28511793e-02\n",
      "Epoch: 1175 mean train loss:  5.67084087e-02, mean val. rec. loss:  4.27740145e-02\n",
      "Epoch: 1176 mean train loss:  5.66287070e-02, mean val. rec. loss:  4.26969676e-02\n",
      "Epoch: 1177 mean train loss:  5.65491319e-02, mean val. rec. loss:  4.26201112e-02\n",
      "Epoch: 1178 mean train loss:  5.64697429e-02, mean val. rec. loss:  4.25433999e-02\n",
      "Epoch: 1179 mean train loss:  5.63904583e-02, mean val. rec. loss:  4.24668792e-02\n",
      "Epoch: 1180 mean train loss:  5.63113897e-02, mean val. rec. loss:  4.23905217e-02\n",
      "Epoch: 1181 mean train loss:  5.62324105e-02, mean val. rec. loss:  4.23143593e-02\n",
      "Epoch: 1182 mean train loss:  5.61536398e-02, mean val. rec. loss:  4.22383421e-02\n",
      "Epoch: 1183 mean train loss:  5.60750033e-02, mean val. rec. loss:  4.21624654e-02\n",
      "Epoch: 1184 mean train loss:  5.59964896e-02, mean val. rec. loss:  4.20867793e-02\n",
      "Epoch: 1185 mean train loss:  5.59181696e-02, mean val. rec. loss:  4.20112111e-02\n",
      "Epoch: 1186 mean train loss:  5.58399687e-02, mean val. rec. loss:  4.19358561e-02\n",
      "Epoch: 1187 mean train loss:  5.57619020e-02, mean val. rec. loss:  4.18606734e-02\n",
      "Epoch: 1188 mean train loss:  5.56840215e-02, mean val. rec. loss:  4.17856541e-02\n",
      "Epoch: 1189 mean train loss:  5.56062974e-02, mean val. rec. loss:  4.17108071e-02\n",
      "Epoch: 1190 mean train loss:  5.55286850e-02, mean val. rec. loss:  4.16360690e-02\n",
      "Epoch: 1191 mean train loss:  5.54512327e-02, mean val. rec. loss:  4.15615259e-02\n",
      "Epoch: 1192 mean train loss:  5.53739444e-02, mean val. rec. loss:  4.14871733e-02\n",
      "Epoch: 1193 mean train loss:  5.52968125e-02, mean val. rec. loss:  4.14129840e-02\n",
      "Epoch: 1194 mean train loss:  5.52198407e-02, mean val. rec. loss:  4.13389489e-02\n",
      "Epoch: 1195 mean train loss:  5.51430067e-02, mean val. rec. loss:  4.12650908e-02\n",
      "Epoch: 1196 mean train loss:  5.50663217e-02, mean val. rec. loss:  4.11914004e-02\n",
      "Epoch: 1197 mean train loss:  5.49897857e-02, mean val. rec. loss:  4.11178553e-02\n",
      "Epoch: 1198 mean train loss:  5.49133837e-02, mean val. rec. loss:  4.10444960e-02\n",
      "Epoch: 1199 mean train loss:  5.48371903e-02, mean val. rec. loss:  4.09713364e-02\n",
      "Epoch: 1200 mean train loss:  5.47611273e-02, mean val. rec. loss:  4.08982902e-02\n",
      "Epoch: 1201 mean train loss:  5.46851723e-02, mean val. rec. loss:  4.08254163e-02\n",
      "Epoch: 1202 mean train loss:  5.46093774e-02, mean val. rec. loss:  4.07527375e-02\n",
      "Epoch: 1203 mean train loss:  5.45337724e-02, mean val. rec. loss:  4.06802129e-02\n",
      "Epoch: 1204 mean train loss:  5.44582830e-02, mean val. rec. loss:  4.06078289e-02\n",
      "Epoch: 1205 mean train loss:  5.43829350e-02, mean val. rec. loss:  4.05356445e-02\n",
      "Epoch: 1206 mean train loss:  5.43077770e-02, mean val. rec. loss:  4.04635870e-02\n",
      "Epoch: 1207 mean train loss:  5.42327233e-02, mean val. rec. loss:  4.03917201e-02\n",
      "Epoch: 1208 mean train loss:  5.41578632e-02, mean val. rec. loss:  4.03199984e-02\n",
      "Epoch: 1209 mean train loss:  5.40831410e-02, mean val. rec. loss:  4.02484762e-02\n",
      "Epoch: 1210 mean train loss:  5.40085491e-02, mean val. rec. loss:  4.01771083e-02\n",
      "Epoch: 1211 mean train loss:  5.39341397e-02, mean val. rec. loss:  4.01059036e-02\n",
      "Epoch: 1212 mean train loss:  5.38598643e-02, mean val. rec. loss:  4.00348078e-02\n",
      "Epoch: 1213 mean train loss:  5.37856970e-02, mean val. rec. loss:  3.99639388e-02\n",
      "Epoch: 1214 mean train loss:  5.37117345e-02, mean val. rec. loss:  3.98932331e-02\n",
      "Epoch: 1215 mean train loss:  5.36379210e-02, mean val. rec. loss:  3.98226998e-02\n",
      "Epoch: 1216 mean train loss:  5.35642379e-02, mean val. rec. loss:  3.97523025e-02\n",
      "Epoch: 1217 mean train loss:  5.34906962e-02, mean val. rec. loss:  3.96820821e-02\n",
      "Epoch: 1218 mean train loss:  5.34173185e-02, mean val. rec. loss:  3.96120160e-02\n",
      "Epoch: 1219 mean train loss:  5.33440934e-02, mean val. rec. loss:  3.95421540e-02\n",
      "Epoch: 1220 mean train loss:  5.32710136e-02, mean val. rec. loss:  3.94724008e-02\n",
      "Epoch: 1221 mean train loss:  5.31980940e-02, mean val. rec. loss:  3.94028381e-02\n",
      "Epoch: 1222 mean train loss:  5.31253233e-02, mean val. rec. loss:  3.93334342e-02\n",
      "Epoch: 1223 mean train loss:  5.30526755e-02, mean val. rec. loss:  3.92642435e-02\n",
      "Epoch: 1224 mean train loss:  5.29802214e-02, mean val. rec. loss:  3.91951662e-02\n",
      "Epoch: 1225 mean train loss:  5.29078716e-02, mean val. rec. loss:  3.91262703e-02\n",
      "Epoch: 1226 mean train loss:  5.28357080e-02, mean val. rec. loss:  3.90575014e-02\n",
      "Epoch: 1227 mean train loss:  5.27636412e-02, mean val. rec. loss:  3.89889548e-02\n",
      "Epoch: 1228 mean train loss:  5.26917867e-02, mean val. rec. loss:  3.89205533e-02\n",
      "Epoch: 1229 mean train loss:  5.26200700e-02, mean val. rec. loss:  3.88522743e-02\n",
      "Epoch: 1230 mean train loss:  5.25484688e-02, mean val. rec. loss:  3.87841904e-02\n",
      "Epoch: 1231 mean train loss:  5.24770016e-02, mean val. rec. loss:  3.87162788e-02\n",
      "Epoch: 1232 mean train loss:  5.24057244e-02, mean val. rec. loss:  3.86485169e-02\n",
      "Epoch: 1233 mean train loss:  5.23345850e-02, mean val. rec. loss:  3.85809364e-02\n",
      "Epoch: 1234 mean train loss:  5.22635983e-02, mean val. rec. loss:  3.85134920e-02\n",
      "Epoch: 1235 mean train loss:  5.21927457e-02, mean val. rec. loss:  3.84462563e-02\n",
      "Epoch: 1236 mean train loss:  5.21220942e-02, mean val. rec. loss:  3.83791385e-02\n",
      "Epoch: 1237 mean train loss:  5.20515433e-02, mean val. rec. loss:  3.83122067e-02\n",
      "Epoch: 1238 mean train loss:  5.19811338e-02, mean val. rec. loss:  3.82454472e-02\n",
      "Epoch: 1239 mean train loss:  5.19109293e-02, mean val. rec. loss:  3.81788329e-02\n",
      "Epoch: 1240 mean train loss:  5.18408066e-02, mean val. rec. loss:  3.81124090e-02\n",
      "Epoch: 1241 mean train loss:  5.17708776e-02, mean val. rec. loss:  3.80461168e-02\n",
      "Epoch: 1242 mean train loss:  5.17010604e-02, mean val. rec. loss:  3.79800173e-02\n",
      "Epoch: 1243 mean train loss:  5.16314331e-02, mean val. rec. loss:  3.79140471e-02\n",
      "Epoch: 1244 mean train loss:  5.15619324e-02, mean val. rec. loss:  3.78482696e-02\n",
      "Epoch: 1245 mean train loss:  5.14925956e-02, mean val. rec. loss:  3.77826464e-02\n",
      "Epoch: 1246 mean train loss:  5.14234003e-02, mean val. rec. loss:  3.77171683e-02\n",
      "Epoch: 1247 mean train loss:  5.13543391e-02, mean val. rec. loss:  3.76518739e-02\n",
      "Epoch: 1248 mean train loss:  5.12854232e-02, mean val. rec. loss:  3.75867134e-02\n",
      "Epoch: 1249 mean train loss:  5.12166450e-02, mean val. rec. loss:  3.75217206e-02\n",
      "Epoch: 1250 mean train loss:  5.11480196e-02, mean val. rec. loss:  3.74569320e-02\n",
      "Epoch: 1251 mean train loss:  5.10795617e-02, mean val. rec. loss:  3.73922636e-02\n",
      "Epoch: 1252 mean train loss:  5.10112603e-02, mean val. rec. loss:  3.73277358e-02\n",
      "Epoch: 1253 mean train loss:  5.09430632e-02, mean val. rec. loss:  3.72634189e-02\n",
      "Epoch: 1254 mean train loss:  5.08750672e-02, mean val. rec. loss:  3.71992494e-02\n",
      "Epoch: 1255 mean train loss:  5.08072127e-02, mean val. rec. loss:  3.71352161e-02\n",
      "Epoch: 1256 mean train loss:  5.07394624e-02, mean val. rec. loss:  3.70713754e-02\n",
      "Epoch: 1257 mean train loss:  5.06718836e-02, mean val. rec. loss:  3.70076891e-02\n",
      "Epoch: 1258 mean train loss:  5.06044462e-02, mean val. rec. loss:  3.69441818e-02\n",
      "Epoch: 1259 mean train loss:  5.05371690e-02, mean val. rec. loss:  3.68808061e-02\n",
      "Epoch: 1260 mean train loss:  5.04700333e-02, mean val. rec. loss:  3.68176255e-02\n",
      "Epoch: 1261 mean train loss:  5.04030540e-02, mean val. rec. loss:  3.67545855e-02\n",
      "Epoch: 1262 mean train loss:  5.03362163e-02, mean val. rec. loss:  3.66916906e-02\n",
      "Epoch: 1263 mean train loss:  5.02695126e-02, mean val. rec. loss:  3.66289862e-02\n",
      "Epoch: 1264 mean train loss:  5.02029840e-02, mean val. rec. loss:  3.65664089e-02\n",
      "Epoch: 1265 mean train loss:  5.01365596e-02, mean val. rec. loss:  3.65040265e-02\n",
      "Epoch: 1266 mean train loss:  5.00703178e-02, mean val. rec. loss:  3.64417712e-02\n",
      "Epoch: 1267 mean train loss:  5.00041989e-02, mean val. rec. loss:  3.63796951e-02\n",
      "Epoch: 1268 mean train loss:  4.99382177e-02, mean val. rec. loss:  3.63177890e-02\n",
      "Epoch: 1269 mean train loss:  4.98724079e-02, mean val. rec. loss:  3.62560123e-02\n",
      "Epoch: 1270 mean train loss:  4.98067396e-02, mean val. rec. loss:  3.61944169e-02\n",
      "Epoch: 1271 mean train loss:  4.97412389e-02, mean val. rec. loss:  3.61329985e-02\n",
      "Epoch: 1272 mean train loss:  4.96758649e-02, mean val. rec. loss:  3.60717229e-02\n",
      "Epoch: 1273 mean train loss:  4.96106249e-02, mean val. rec. loss:  3.60106379e-02\n",
      "Epoch: 1274 mean train loss:  4.95455451e-02, mean val. rec. loss:  3.59496481e-02\n",
      "Epoch: 1275 mean train loss:  4.94805993e-02, mean val. rec. loss:  3.58888715e-02\n",
      "Epoch: 1276 mean train loss:  4.94158361e-02, mean val. rec. loss:  3.58282219e-02\n",
      "Epoch: 1277 mean train loss:  4.93511547e-02, mean val. rec. loss:  3.57677310e-02\n",
      "Epoch: 1278 mean train loss:  4.92866596e-02, mean val. rec. loss:  3.57074171e-02\n",
      "Epoch: 1279 mean train loss:  4.92223098e-02, mean val. rec. loss:  3.56472528e-02\n",
      "Epoch: 1280 mean train loss:  4.91580977e-02, mean val. rec. loss:  3.55872382e-02\n",
      "Epoch: 1281 mean train loss:  4.90940309e-02, mean val. rec. loss:  3.55274119e-02\n",
      "Epoch: 1282 mean train loss:  4.90301503e-02, mean val. rec. loss:  3.54676944e-02\n",
      "Epoch: 1283 mean train loss:  4.89663480e-02, mean val. rec. loss:  3.54082015e-02\n",
      "Epoch: 1284 mean train loss:  4.89027095e-02, mean val. rec. loss:  3.53488264e-02\n",
      "Epoch: 1285 mean train loss:  4.88392386e-02, mean val. rec. loss:  3.52896329e-02\n",
      "Epoch: 1286 mean train loss:  4.87759092e-02, mean val. rec. loss:  3.52305527e-02\n",
      "Epoch: 1287 mean train loss:  4.87126990e-02, mean val. rec. loss:  3.51716403e-02\n",
      "Epoch: 1288 mean train loss:  4.86496191e-02, mean val. rec. loss:  3.51129230e-02\n",
      "Epoch: 1289 mean train loss:  4.85867255e-02, mean val. rec. loss:  3.50543373e-02\n",
      "Epoch: 1290 mean train loss:  4.85239920e-02, mean val. rec. loss:  3.49959375e-02\n",
      "Epoch: 1291 mean train loss:  4.84613778e-02, mean val. rec. loss:  3.49376737e-02\n",
      "Epoch: 1292 mean train loss:  4.83988975e-02, mean val. rec. loss:  3.48795824e-02\n",
      "Epoch: 1293 mean train loss:  4.83365886e-02, mean val. rec. loss:  3.48216044e-02\n",
      "Epoch: 1294 mean train loss:  4.82743729e-02, mean val. rec. loss:  3.47638306e-02\n",
      "Epoch: 1295 mean train loss:  4.82123507e-02, mean val. rec. loss:  3.47061883e-02\n",
      "Epoch: 1296 mean train loss:  4.81504627e-02, mean val. rec. loss:  3.46486934e-02\n",
      "Epoch: 1297 mean train loss:  4.80886938e-02, mean val. rec. loss:  3.45913686e-02\n",
      "Epoch: 1298 mean train loss:  4.80270926e-02, mean val. rec. loss:  3.45341913e-02\n",
      "Epoch: 1299 mean train loss:  4.79656179e-02, mean val. rec. loss:  3.44771704e-02\n",
      "Epoch: 1300 mean train loss:  4.79042811e-02, mean val. rec. loss:  3.44203060e-02\n",
      "Epoch: 1301 mean train loss:  4.78430784e-02, mean val. rec. loss:  3.43636321e-02\n",
      "Epoch: 1302 mean train loss:  4.77820730e-02, mean val. rec. loss:  3.43070694e-02\n",
      "Epoch: 1303 mean train loss:  4.77211794e-02, mean val. rec. loss:  3.42506926e-02\n",
      "Epoch: 1304 mean train loss:  4.76604086e-02, mean val. rec. loss:  3.41944383e-02\n",
      "Epoch: 1305 mean train loss:  4.75997943e-02, mean val. rec. loss:  3.41383472e-02\n",
      "Epoch: 1306 mean train loss:  4.75393216e-02, mean val. rec. loss:  3.40824241e-02\n",
      "Epoch: 1307 mean train loss:  4.74789978e-02, mean val. rec. loss:  3.40266347e-02\n",
      "Epoch: 1308 mean train loss:  4.74188267e-02, mean val. rec. loss:  3.39710449e-02\n",
      "Epoch: 1309 mean train loss:  4.73587896e-02, mean val. rec. loss:  3.39155707e-02\n",
      "Epoch: 1310 mean train loss:  4.72988532e-02, mean val. rec. loss:  3.38602735e-02\n",
      "Epoch: 1311 mean train loss:  4.72391216e-02, mean val. rec. loss:  3.38051305e-02\n",
      "Epoch: 1312 mean train loss:  4.71795166e-02, mean val. rec. loss:  3.37501258e-02\n",
      "Epoch: 1313 mean train loss:  4.71200419e-02, mean val. rec. loss:  3.36952799e-02\n",
      "Epoch: 1314 mean train loss:  4.70607125e-02, mean val. rec. loss:  3.36405769e-02\n",
      "Epoch: 1315 mean train loss:  4.70015209e-02, mean val. rec. loss:  3.35860371e-02\n",
      "Epoch: 1316 mean train loss:  4.69424560e-02, mean val. rec. loss:  3.35316244e-02\n",
      "Epoch: 1317 mean train loss:  4.68835288e-02, mean val. rec. loss:  3.34774113e-02\n",
      "Epoch: 1318 mean train loss:  4.68247767e-02, mean val. rec. loss:  3.34233296e-02\n",
      "Epoch: 1319 mean train loss:  4.67661587e-02, mean val. rec. loss:  3.33694295e-02\n",
      "Epoch: 1320 mean train loss:  4.67076561e-02, mean val. rec. loss:  3.33156472e-02\n",
      "Epoch: 1321 mean train loss:  4.66493211e-02, mean val. rec. loss:  3.32620283e-02\n",
      "Epoch: 1322 mean train loss:  4.65911389e-02, mean val. rec. loss:  3.32085545e-02\n",
      "Epoch: 1323 mean train loss:  4.65330571e-02, mean val. rec. loss:  3.31552168e-02\n",
      "Epoch: 1324 mean train loss:  4.64751207e-02, mean val. rec. loss:  3.31020786e-02\n",
      "Epoch: 1325 mean train loss:  4.64173630e-02, mean val. rec. loss:  3.30490539e-02\n",
      "Epoch: 1326 mean train loss:  4.63596946e-02, mean val. rec. loss:  3.29961924e-02\n",
      "Epoch: 1327 mean train loss:  4.63021902e-02, mean val. rec. loss:  3.29434897e-02\n",
      "Epoch: 1328 mean train loss:  4.62448347e-02, mean val. rec. loss:  3.28909140e-02\n",
      "Epoch: 1329 mean train loss:  4.61876022e-02, mean val. rec. loss:  3.28384948e-02\n",
      "Epoch: 1330 mean train loss:  4.61305037e-02, mean val. rec. loss:  3.27862525e-02\n",
      "Epoch: 1331 mean train loss:  4.60735802e-02, mean val. rec. loss:  3.27341214e-02\n",
      "Epoch: 1332 mean train loss:  4.60167462e-02, mean val. rec. loss:  3.26821988e-02\n",
      "Epoch: 1333 mean train loss:  4.59601021e-02, mean val. rec. loss:  3.26303897e-02\n",
      "Epoch: 1334 mean train loss:  4.59035846e-02, mean val. rec. loss:  3.25787393e-02\n",
      "Epoch: 1335 mean train loss:  4.58471639e-02, mean val. rec. loss:  3.25272341e-02\n",
      "Epoch: 1336 mean train loss:  4.57909295e-02, mean val. rec. loss:  3.24758741e-02\n",
      "Epoch: 1337 mean train loss:  4.57348105e-02, mean val. rec. loss:  3.24246478e-02\n",
      "Epoch: 1338 mean train loss:  4.56788182e-02, mean val. rec. loss:  3.23736075e-02\n",
      "Epoch: 1339 mean train loss:  4.56230009e-02, mean val. rec. loss:  3.23226647e-02\n",
      "Epoch: 1340 mean train loss:  4.55672730e-02, mean val. rec. loss:  3.22719261e-02\n",
      "Epoch: 1341 mean train loss:  4.55117201e-02, mean val. rec. loss:  3.22213099e-02\n",
      "Epoch: 1342 mean train loss:  4.54563013e-02, mean val. rec. loss:  3.21708434e-02\n",
      "Epoch: 1343 mean train loss:  4.54010054e-02, mean val. rec. loss:  3.21205538e-02\n",
      "Epoch: 1344 mean train loss:  4.53458511e-02, mean val. rec. loss:  3.20703686e-02\n",
      "Epoch: 1345 mean train loss:  4.52908606e-02, mean val. rec. loss:  3.20203421e-02\n",
      "Epoch: 1346 mean train loss:  4.52359856e-02, mean val. rec. loss:  3.19704879e-02\n",
      "Epoch: 1347 mean train loss:  4.51812409e-02, mean val. rec. loss:  3.19207653e-02\n",
      "Epoch: 1348 mean train loss:  4.51266415e-02, mean val. rec. loss:  3.18711833e-02\n",
      "Epoch: 1349 mean train loss:  4.50721538e-02, mean val. rec. loss:  3.18217555e-02\n",
      "Epoch: 1350 mean train loss:  4.50178300e-02, mean val. rec. loss:  3.17724956e-02\n",
      "Epoch: 1351 mean train loss:  4.49636626e-02, mean val. rec. loss:  3.17233354e-02\n",
      "Epoch: 1352 mean train loss:  4.49095808e-02, mean val. rec. loss:  3.16743613e-02\n",
      "Epoch: 1353 mean train loss:  4.48556741e-02, mean val. rec. loss:  3.16255322e-02\n",
      "Epoch: 1354 mean train loss:  4.48018978e-02, mean val. rec. loss:  3.15768438e-02\n",
      "Epoch: 1355 mean train loss:  4.47482444e-02, mean val. rec. loss:  3.15282869e-02\n",
      "Epoch: 1356 mean train loss:  4.46947250e-02, mean val. rec. loss:  3.14798707e-02\n",
      "Epoch: 1357 mean train loss:  4.46413211e-02, mean val. rec. loss:  3.14316336e-02\n",
      "Epoch: 1358 mean train loss:  4.45881109e-02, mean val. rec. loss:  3.13834985e-02\n",
      "Epoch: 1359 mean train loss:  4.45349789e-02, mean val. rec. loss:  3.13355517e-02\n",
      "Epoch: 1360 mean train loss:  4.44820182e-02, mean val. rec. loss:  3.12877387e-02\n",
      "Epoch: 1361 mean train loss:  4.44291841e-02, mean val. rec. loss:  3.12400550e-02\n",
      "Epoch: 1362 mean train loss:  4.43764841e-02, mean val. rec. loss:  3.11925278e-02\n",
      "Epoch: 1363 mean train loss:  4.43238958e-02, mean val. rec. loss:  3.11451571e-02\n",
      "Epoch: 1364 mean train loss:  4.42714938e-02, mean val. rec. loss:  3.10978793e-02\n",
      "Epoch: 1365 mean train loss:  4.42191811e-02, mean val. rec. loss:  3.10507875e-02\n",
      "Epoch: 1366 mean train loss:  4.41669951e-02, mean val. rec. loss:  3.10038454e-02\n",
      "Epoch: 1367 mean train loss:  4.41149655e-02, mean val. rec. loss:  3.09570258e-02\n",
      "Epoch: 1368 mean train loss:  4.40630663e-02, mean val. rec. loss:  3.09103785e-02\n",
      "Epoch: 1369 mean train loss:  4.40113346e-02, mean val. rec. loss:  3.08638401e-02\n",
      "Epoch: 1370 mean train loss:  4.39596737e-02, mean val. rec. loss:  3.08174696e-02\n",
      "Epoch: 1371 mean train loss:  4.39081841e-02, mean val. rec. loss:  3.07712464e-02\n",
      "Epoch: 1372 mean train loss:  4.38568026e-02, mean val. rec. loss:  3.07251435e-02\n",
      "Epoch: 1373 mean train loss:  4.38055663e-02, mean val. rec. loss:  3.06791789e-02\n",
      "Epoch: 1374 mean train loss:  4.37544789e-02, mean val. rec. loss:  3.06333753e-02\n",
      "Epoch: 1375 mean train loss:  4.37035033e-02, mean val. rec. loss:  3.05876919e-02\n",
      "Epoch: 1376 mean train loss:  4.36526469e-02, mean val. rec. loss:  3.05421491e-02\n",
      "Epoch: 1377 mean train loss:  4.36019506e-02, mean val. rec. loss:  3.04967719e-02\n",
      "Epoch: 1378 mean train loss:  4.35513773e-02, mean val. rec. loss:  3.04515217e-02\n",
      "Epoch: 1379 mean train loss:  4.35009305e-02, mean val. rec. loss:  3.04064076e-02\n",
      "Epoch: 1380 mean train loss:  4.34506030e-02, mean val. rec. loss:  3.03614432e-02\n",
      "Epoch: 1381 mean train loss:  4.34004132e-02, mean val. rec. loss:  3.03166239e-02\n",
      "Epoch: 1382 mean train loss:  4.33503799e-02, mean val. rec. loss:  3.02719407e-02\n",
      "Epoch: 1383 mean train loss:  4.33004732e-02, mean val. rec. loss:  3.02273799e-02\n",
      "Epoch: 1384 mean train loss:  4.32506782e-02, mean val. rec. loss:  3.01829983e-02\n",
      "Epoch: 1385 mean train loss:  4.32010322e-02, mean val. rec. loss:  3.01387370e-02\n",
      "Epoch: 1386 mean train loss:  4.31514979e-02, mean val. rec. loss:  3.00945935e-02\n",
      "Epoch: 1387 mean train loss:  4.31020791e-02, mean val. rec. loss:  3.00506134e-02\n",
      "Epoch: 1388 mean train loss:  4.30528242e-02, mean val. rec. loss:  3.00067670e-02\n",
      "Epoch: 1389 mean train loss:  4.30036996e-02, mean val. rec. loss:  2.99630590e-02\n",
      "Epoch: 1390 mean train loss:  4.29546942e-02, mean val. rec. loss:  2.99194917e-02\n",
      "Epoch: 1391 mean train loss:  4.29058080e-02, mean val. rec. loss:  2.98760513e-02\n",
      "Epoch: 1392 mean train loss:  4.28570558e-02, mean val. rec. loss:  2.98327606e-02\n",
      "Epoch: 1393 mean train loss:  4.28084526e-02, mean val. rec. loss:  2.97896196e-02\n",
      "Epoch: 1394 mean train loss:  4.27599389e-02, mean val. rec. loss:  2.97465693e-02\n",
      "Epoch: 1395 mean train loss:  4.27115629e-02, mean val. rec. loss:  2.97037118e-02\n",
      "Epoch: 1396 mean train loss:  4.26633359e-02, mean val. rec. loss:  2.96609541e-02\n",
      "Epoch: 1397 mean train loss:  4.26152429e-02, mean val. rec. loss:  2.96183415e-02\n",
      "Epoch: 1398 mean train loss:  4.25672468e-02, mean val. rec. loss:  2.95758809e-02\n",
      "Epoch: 1399 mean train loss:  4.25194146e-02, mean val. rec. loss:  2.95335541e-02\n",
      "Epoch: 1400 mean train loss:  4.24716978e-02, mean val. rec. loss:  2.94913520e-02\n",
      "Epoch: 1401 mean train loss:  4.24241040e-02, mean val. rec. loss:  2.94492883e-02\n",
      "Epoch: 1402 mean train loss:  4.23766293e-02, mean val. rec. loss:  2.94073515e-02\n",
      "Epoch: 1403 mean train loss:  4.23292738e-02, mean val. rec. loss:  2.93655532e-02\n",
      "Epoch: 1404 mean train loss:  4.22820710e-02, mean val. rec. loss:  2.93238954e-02\n",
      "Epoch: 1405 mean train loss:  4.22349948e-02, mean val. rec. loss:  2.92823782e-02\n",
      "Epoch: 1406 mean train loss:  4.21880452e-02, mean val. rec. loss:  2.92409745e-02\n",
      "Epoch: 1407 mean train loss:  4.21412074e-02, mean val. rec. loss:  2.91997113e-02\n",
      "Epoch: 1408 mean train loss:  4.20944739e-02, mean val. rec. loss:  2.91586070e-02\n",
      "Epoch: 1409 mean train loss:  4.20479080e-02, mean val. rec. loss:  2.91176296e-02\n",
      "Epoch: 1410 mean train loss:  4.20014649e-02, mean val. rec. loss:  2.90767747e-02\n",
      "Epoch: 1411 mean train loss:  4.19551299e-02, mean val. rec. loss:  2.90360513e-02\n",
      "Epoch: 1412 mean train loss:  4.19089178e-02, mean val. rec. loss:  2.89954527e-02\n",
      "Epoch: 1413 mean train loss:  4.18628248e-02, mean val. rec. loss:  2.89550196e-02\n",
      "Epoch: 1414 mean train loss:  4.18168958e-02, mean val. rec. loss:  2.89146977e-02\n",
      "Epoch: 1415 mean train loss:  4.17710784e-02, mean val. rec. loss:  2.88745209e-02\n",
      "Epoch: 1416 mean train loss:  4.17253766e-02, mean val. rec. loss:  2.88344598e-02\n",
      "Epoch: 1417 mean train loss:  4.16797864e-02, mean val. rec. loss:  2.87945143e-02\n",
      "Epoch: 1418 mean train loss:  4.16343155e-02, mean val. rec. loss:  2.87547321e-02\n",
      "Epoch: 1419 mean train loss:  4.15890046e-02, mean val. rec. loss:  2.87150792e-02\n",
      "Epoch: 1420 mean train loss:  4.15437795e-02, mean val. rec. loss:  2.86755103e-02\n",
      "Epoch: 1421 mean train loss:  4.14987033e-02, mean val. rec. loss:  2.86361318e-02\n",
      "Epoch: 1422 mean train loss:  4.14537612e-02, mean val. rec. loss:  2.85968599e-02\n",
      "Epoch: 1423 mean train loss:  4.14089122e-02, mean val. rec. loss:  2.85577332e-02\n",
      "Epoch: 1424 mean train loss:  4.13642196e-02, mean val. rec. loss:  2.85187062e-02\n",
      "Epoch: 1425 mean train loss:  4.13195866e-02, mean val. rec. loss:  2.84798267e-02\n",
      "Epoch: 1426 mean train loss:  4.12751101e-02, mean val. rec. loss:  2.84411105e-02\n",
      "Epoch: 1427 mean train loss:  4.12308048e-02, mean val. rec. loss:  2.84024850e-02\n",
      "Epoch: 1428 mean train loss:  4.11865629e-02, mean val. rec. loss:  2.83640046e-02\n",
      "Epoch: 1429 mean train loss:  4.11424774e-02, mean val. rec. loss:  2.83256218e-02\n",
      "Epoch: 1430 mean train loss:  4.10984850e-02, mean val. rec. loss:  2.82874022e-02\n",
      "Epoch: 1431 mean train loss:  4.10546193e-02, mean val. rec. loss:  2.82492983e-02\n",
      "Epoch: 1432 mean train loss:  4.10109025e-02, mean val. rec. loss:  2.82113192e-02\n",
      "Epoch: 1433 mean train loss:  4.09672676e-02, mean val. rec. loss:  2.81734829e-02\n",
      "Epoch: 1434 mean train loss:  4.09237706e-02, mean val. rec. loss:  2.81357601e-02\n",
      "Epoch: 1435 mean train loss:  4.08804113e-02, mean val. rec. loss:  2.80981801e-02\n",
      "Epoch: 1436 mean train loss:  4.08371527e-02, mean val. rec. loss:  2.80607044e-02\n",
      "Epoch: 1437 mean train loss:  4.07940057e-02, mean val. rec. loss:  2.80233649e-02\n",
      "Epoch: 1438 mean train loss:  4.07510077e-02, mean val. rec. loss:  2.79861273e-02\n",
      "Epoch: 1439 mean train loss:  4.07080805e-02, mean val. rec. loss:  2.79490395e-02\n",
      "Epoch: 1440 mean train loss:  4.06653134e-02, mean val. rec. loss:  2.79120809e-02\n",
      "Epoch: 1441 mean train loss:  4.06226656e-02, mean val. rec. loss:  2.78752335e-02\n",
      "Epoch: 1442 mean train loss:  4.05801406e-02, mean val. rec. loss:  2.78384904e-02\n",
      "Epoch: 1443 mean train loss:  4.05377087e-02, mean val. rec. loss:  2.78019219e-02\n",
      "Epoch: 1444 mean train loss:  4.04953997e-02, mean val. rec. loss:  2.77654578e-02\n",
      "Epoch: 1445 mean train loss:  4.04532137e-02, mean val. rec. loss:  2.77291206e-02\n",
      "Epoch: 1446 mean train loss:  4.04111729e-02, mean val. rec. loss:  2.76929241e-02\n",
      "Epoch: 1447 mean train loss:  4.03692289e-02, mean val. rec. loss:  2.76568545e-02\n",
      "Epoch: 1448 mean train loss:  4.03274115e-02, mean val. rec. loss:  2.76208848e-02\n",
      "Epoch: 1449 mean train loss:  4.02857022e-02, mean val. rec. loss:  2.75850602e-02\n",
      "Epoch: 1450 mean train loss:  4.02441232e-02, mean val. rec. loss:  2.75493422e-02\n",
      "Epoch: 1451 mean train loss:  4.02026336e-02, mean val. rec. loss:  2.75137376e-02\n",
      "Epoch: 1452 mean train loss:  4.01612781e-02, mean val. rec. loss:  2.74782714e-02\n",
      "Epoch: 1453 mean train loss:  4.01200231e-02, mean val. rec. loss:  2.74429253e-02\n",
      "Epoch: 1454 mean train loss:  4.00789022e-02, mean val. rec. loss:  2.74076972e-02\n",
      "Epoch: 1455 mean train loss:  4.00379191e-02, mean val. rec. loss:  2.73726029e-02\n",
      "Epoch: 1456 mean train loss:  3.99970217e-02, mean val. rec. loss:  2.73376152e-02\n",
      "Epoch: 1457 mean train loss:  3.99562695e-02, mean val. rec. loss:  2.73027227e-02\n",
      "Epoch: 1458 mean train loss:  3.99156030e-02, mean val. rec. loss:  2.72679777e-02\n",
      "Epoch: 1459 mean train loss:  3.98750706e-02, mean val. rec. loss:  2.72333596e-02\n",
      "Epoch: 1460 mean train loss:  3.98346238e-02, mean val. rec. loss:  2.71988459e-02\n",
      "Epoch: 1461 mean train loss:  3.97943111e-02, mean val. rec. loss:  2.71644660e-02\n",
      "Epoch: 1462 mean train loss:  3.97541027e-02, mean val. rec. loss:  2.71301905e-02\n",
      "Epoch: 1463 mean train loss:  3.97140134e-02, mean val. rec. loss:  2.70960442e-02\n",
      "Epoch: 1464 mean train loss:  3.96740657e-02, mean val. rec. loss:  2.70620430e-02\n",
      "Epoch: 1465 mean train loss:  3.96342074e-02, mean val. rec. loss:  2.70281598e-02\n",
      "Epoch: 1466 mean train loss:  3.95944757e-02, mean val. rec. loss:  2.69943809e-02\n",
      "Epoch: 1467 mean train loss:  3.95548483e-02, mean val. rec. loss:  2.69607199e-02\n",
      "Epoch: 1468 mean train loss:  3.95153326e-02, mean val. rec. loss:  2.69271542e-02\n",
      "Epoch: 1469 mean train loss:  3.94759100e-02, mean val. rec. loss:  2.68937382e-02\n",
      "Epoch: 1470 mean train loss:  3.94366327e-02, mean val. rec. loss:  2.68604220e-02\n",
      "Epoch: 1471 mean train loss:  3.93974671e-02, mean val. rec. loss:  2.68272373e-02\n",
      "Epoch: 1472 mean train loss:  3.93584058e-02, mean val. rec. loss:  2.67941569e-02\n",
      "Epoch: 1473 mean train loss:  3.93194450e-02, mean val. rec. loss:  2.67612035e-02\n",
      "Epoch: 1474 mean train loss:  3.92805923e-02, mean val. rec. loss:  2.67283364e-02\n",
      "Epoch: 1475 mean train loss:  3.92418587e-02, mean val. rec. loss:  2.66956234e-02\n",
      "Epoch: 1476 mean train loss:  3.92032444e-02, mean val. rec. loss:  2.66629966e-02\n",
      "Epoch: 1477 mean train loss:  3.91647566e-02, mean val. rec. loss:  2.66305082e-02\n",
      "Epoch: 1478 mean train loss:  3.91263508e-02, mean val. rec. loss:  2.65981377e-02\n",
      "Epoch: 1479 mean train loss:  3.90880939e-02, mean val. rec. loss:  2.65658715e-02\n",
      "Epoch: 1480 mean train loss:  3.90499339e-02, mean val. rec. loss:  2.65337142e-02\n",
      "Epoch: 1481 mean train loss:  3.90118559e-02, mean val. rec. loss:  2.65016635e-02\n",
      "Epoch: 1482 mean train loss:  3.89738821e-02, mean val. rec. loss:  2.64697534e-02\n",
      "Epoch: 1483 mean train loss:  3.89360498e-02, mean val. rec. loss:  2.64379476e-02\n",
      "Epoch: 1484 mean train loss:  3.88983218e-02, mean val. rec. loss:  2.64062439e-02\n",
      "Epoch: 1485 mean train loss:  3.88606907e-02, mean val. rec. loss:  2.63746672e-02\n",
      "Epoch: 1486 mean train loss:  3.88231638e-02, mean val. rec. loss:  2.63432062e-02\n",
      "Epoch: 1487 mean train loss:  3.87857748e-02, mean val. rec. loss:  2.63118585e-02\n",
      "Epoch: 1488 mean train loss:  3.87484900e-02, mean val. rec. loss:  2.62806016e-02\n",
      "Epoch: 1489 mean train loss:  3.87113058e-02, mean val. rec. loss:  2.62494626e-02\n",
      "Epoch: 1490 mean train loss:  3.86742109e-02, mean val. rec. loss:  2.62184619e-02\n",
      "Epoch: 1491 mean train loss:  3.86372502e-02, mean val. rec. loss:  2.61875429e-02\n",
      "Epoch: 1492 mean train loss:  3.86003565e-02, mean val. rec. loss:  2.61567464e-02\n",
      "Epoch: 1493 mean train loss:  3.85636005e-02, mean val. rec. loss:  2.61260701e-02\n",
      "Epoch: 1494 mean train loss:  3.85269787e-02, mean val. rec. loss:  2.60954822e-02\n",
      "Epoch: 1495 mean train loss:  3.84904202e-02, mean val. rec. loss:  2.60650258e-02\n",
      "Epoch: 1496 mean train loss:  3.84539883e-02, mean val. rec. loss:  2.60346647e-02\n",
      "Epoch: 1497 mean train loss:  3.84176644e-02, mean val. rec. loss:  2.60044102e-02\n",
      "Epoch: 1498 mean train loss:  3.83814299e-02, mean val. rec. loss:  2.59742896e-02\n",
      "Epoch: 1499 mean train loss:  3.83453332e-02, mean val. rec. loss:  2.59442414e-02\n",
      "Epoch: 1500 mean train loss:  3.83092886e-02, mean val. rec. loss:  2.59143249e-02\n",
      "Epoch: 1501 mean train loss:  3.82733893e-02, mean val. rec. loss:  2.58845444e-02\n",
      "Epoch: 1502 mean train loss:  3.82376241e-02, mean val. rec. loss:  2.58548433e-02\n",
      "Epoch: 1503 mean train loss:  3.82019073e-02, mean val. rec. loss:  2.58252714e-02\n",
      "Epoch: 1504 mean train loss:  3.81663394e-02, mean val. rec. loss:  2.57957880e-02\n",
      "Epoch: 1505 mean train loss:  3.81308535e-02, mean val. rec. loss:  2.57664067e-02\n",
      "Epoch: 1506 mean train loss:  3.80954645e-02, mean val. rec. loss:  2.57371478e-02\n",
      "Epoch: 1507 mean train loss:  3.80602132e-02, mean val. rec. loss:  2.57079706e-02\n",
      "Epoch: 1508 mean train loss:  3.80250178e-02, mean val. rec. loss:  2.56789068e-02\n",
      "Epoch: 1509 mean train loss:  3.79899490e-02, mean val. rec. loss:  2.56499654e-02\n",
      "Epoch: 1510 mean train loss:  3.79550032e-02, mean val. rec. loss:  2.56211307e-02\n",
      "Epoch: 1511 mean train loss:  3.79201392e-02, mean val. rec. loss:  2.55924093e-02\n",
      "Epoch: 1512 mean train loss:  3.78853945e-02, mean val. rec. loss:  2.55637787e-02\n",
      "Epoch: 1513 mean train loss:  3.78507354e-02, mean val. rec. loss:  2.55352683e-02\n",
      "Epoch: 1514 mean train loss:  3.78161731e-02, mean val. rec. loss:  2.55068735e-02\n",
      "Epoch: 1515 mean train loss:  3.77817375e-02, mean val. rec. loss:  2.54785490e-02\n",
      "Epoch: 1516 mean train loss:  3.77473615e-02, mean val. rec. loss:  2.54503425e-02\n",
      "Epoch: 1517 mean train loss:  3.77131270e-02, mean val. rec. loss:  2.54222289e-02\n",
      "Epoch: 1518 mean train loss:  3.76790079e-02, mean val. rec. loss:  2.53942197e-02\n",
      "Epoch: 1519 mean train loss:  3.76449447e-02, mean val. rec. loss:  2.53663261e-02\n",
      "Epoch: 1520 mean train loss:  3.76110156e-02, mean val. rec. loss:  2.53385301e-02\n",
      "Epoch: 1521 mean train loss:  3.75771722e-02, mean val. rec. loss:  2.53108339e-02\n",
      "Epoch: 1522 mean train loss:  3.75434218e-02, mean val. rec. loss:  2.52832510e-02\n",
      "Epoch: 1523 mean train loss:  3.75097981e-02, mean val. rec. loss:  2.52557498e-02\n",
      "Epoch: 1524 mean train loss:  3.74762303e-02, mean val. rec. loss:  2.52283666e-02\n",
      "Epoch: 1525 mean train loss:  3.74427890e-02, mean val. rec. loss:  2.52010831e-02\n",
      "Epoch: 1526 mean train loss:  3.74094856e-02, mean val. rec. loss:  2.51739039e-02\n",
      "Epoch: 1527 mean train loss:  3.73762120e-02, mean val. rec. loss:  2.51468268e-02\n",
      "Epoch: 1528 mean train loss:  3.73430725e-02, mean val. rec. loss:  2.51198541e-02\n",
      "Epoch: 1529 mean train loss:  3.73100298e-02, mean val. rec. loss:  2.50929788e-02\n",
      "Epoch: 1530 mean train loss:  3.72770764e-02, mean val. rec. loss:  2.50661943e-02\n",
      "Epoch: 1531 mean train loss:  3.72442497e-02, mean val. rec. loss:  2.50395141e-02\n",
      "Epoch: 1532 mean train loss:  3.72114677e-02, mean val. rec. loss:  2.50129360e-02\n",
      "Epoch: 1533 mean train loss:  3.71788161e-02, mean val. rec. loss:  2.49864644e-02\n",
      "Epoch: 1534 mean train loss:  3.71462911e-02, mean val. rec. loss:  2.49600768e-02\n",
      "Epoch: 1535 mean train loss:  3.71138070e-02, mean val. rec. loss:  2.49337935e-02\n",
      "Epoch: 1536 mean train loss:  3.70814496e-02, mean val. rec. loss:  2.49076145e-02\n",
      "Epoch: 1537 mean train loss:  3.70491890e-02, mean val. rec. loss:  2.48815013e-02\n",
      "Epoch: 1538 mean train loss:  3.70170252e-02, mean val. rec. loss:  2.48555151e-02\n",
      "Epoch: 1539 mean train loss:  3.69849472e-02, mean val. rec. loss:  2.48296264e-02\n",
      "Epoch: 1540 mean train loss:  3.69529882e-02, mean val. rec. loss:  2.48038284e-02\n",
      "Epoch: 1541 mean train loss:  3.69211001e-02, mean val. rec. loss:  2.47781098e-02\n",
      "Epoch: 1542 mean train loss:  3.68893199e-02, mean val. rec. loss:  2.47525364e-02\n",
      "Epoch: 1543 mean train loss:  3.68576292e-02, mean val. rec. loss:  2.47270242e-02\n",
      "Epoch: 1544 mean train loss:  3.68260464e-02, mean val. rec. loss:  2.47016163e-02\n",
      "Epoch: 1545 mean train loss:  3.67945382e-02, mean val. rec. loss:  2.46762969e-02\n",
      "Epoch: 1546 mean train loss:  3.67631416e-02, mean val. rec. loss:  2.46510954e-02\n",
      "Epoch: 1547 mean train loss:  3.67318233e-02, mean val. rec. loss:  2.46259846e-02\n",
      "Epoch: 1548 mean train loss:  3.67006167e-02, mean val. rec. loss:  2.46009781e-02\n",
      "Epoch: 1549 mean train loss:  3.66695033e-02, mean val. rec. loss:  2.45760306e-02\n",
      "Epoch: 1550 mean train loss:  3.66384792e-02, mean val. rec. loss:  2.45512011e-02\n",
      "Epoch: 1551 mean train loss:  3.66075519e-02, mean val. rec. loss:  2.45264509e-02\n",
      "Epoch: 1552 mean train loss:  3.65767066e-02, mean val. rec. loss:  2.45018073e-02\n",
      "Epoch: 1553 mean train loss:  3.65459581e-02, mean val. rec. loss:  2.44772613e-02\n",
      "Epoch: 1554 mean train loss:  3.65152990e-02, mean val. rec. loss:  2.44527946e-02\n",
      "Epoch: 1555 mean train loss:  3.64847330e-02, mean val. rec. loss:  2.44284323e-02\n",
      "Epoch: 1556 mean train loss:  3.64542713e-02, mean val. rec. loss:  2.44041606e-02\n",
      "Epoch: 1557 mean train loss:  3.64238989e-02, mean val. rec. loss:  2.43799616e-02\n",
      "Epoch: 1558 mean train loss:  3.63936160e-02, mean val. rec. loss:  2.43558668e-02\n",
      "Epoch: 1559 mean train loss:  3.63634038e-02, mean val. rec. loss:  2.43318651e-02\n",
      "Epoch: 1560 mean train loss:  3.63333183e-02, mean val. rec. loss:  2.43079631e-02\n",
      "Epoch: 1561 mean train loss:  3.63033072e-02, mean val. rec. loss:  2.42841451e-02\n",
      "Epoch: 1562 mean train loss:  3.62733744e-02, mean val. rec. loss:  2.42603996e-02\n",
      "Epoch: 1563 mean train loss:  3.62435532e-02, mean val. rec. loss:  2.42367811e-02\n",
      "Epoch: 1564 mean train loss:  3.62138215e-02, mean val. rec. loss:  2.42132171e-02\n",
      "Epoch: 1565 mean train loss:  3.61841494e-02, mean val. rec. loss:  2.41897755e-02\n",
      "Epoch: 1566 mean train loss:  3.61546001e-02, mean val. rec. loss:  2.41664202e-02\n",
      "Epoch: 1567 mean train loss:  3.61251328e-02, mean val. rec. loss:  2.41431419e-02\n",
      "Epoch: 1568 mean train loss:  3.60957400e-02, mean val. rec. loss:  2.41199679e-02\n",
      "Epoch: 1569 mean train loss:  3.60664627e-02, mean val. rec. loss:  2.40968620e-02\n",
      "Epoch: 1570 mean train loss:  3.60372672e-02, mean val. rec. loss:  2.40738559e-02\n",
      "Epoch: 1571 mean train loss:  3.60081500e-02, mean val. rec. loss:  2.40509178e-02\n",
      "Epoch: 1572 mean train loss:  3.59791110e-02, mean val. rec. loss:  2.40280977e-02\n",
      "Epoch: 1573 mean train loss:  3.59501912e-02, mean val. rec. loss:  2.40053524e-02\n",
      "Epoch: 1574 mean train loss:  3.59213347e-02, mean val. rec. loss:  2.39826978e-02\n",
      "Epoch: 1575 mean train loss:  3.58925601e-02, mean val. rec. loss:  2.39601317e-02\n",
      "Epoch: 1576 mean train loss:  3.58638824e-02, mean val. rec. loss:  2.39376336e-02\n",
      "Epoch: 1577 mean train loss:  3.58352940e-02, mean val. rec. loss:  2.39152353e-02\n",
      "Epoch: 1578 mean train loss:  3.58067951e-02, mean val. rec. loss:  2.38929231e-02\n",
      "Epoch: 1579 mean train loss:  3.57783892e-02, mean val. rec. loss:  2.38707017e-02\n",
      "Epoch: 1580 mean train loss:  3.57500504e-02, mean val. rec. loss:  2.38485756e-02\n",
      "Epoch: 1581 mean train loss:  3.57218122e-02, mean val. rec. loss:  2.38265039e-02\n",
      "Epoch: 1582 mean train loss:  3.56936484e-02, mean val. rec. loss:  2.38045319e-02\n",
      "Epoch: 1583 mean train loss:  3.56655666e-02, mean val. rec. loss:  2.37826553e-02\n",
      "Epoch: 1584 mean train loss:  3.56375927e-02, mean val. rec. loss:  2.37608557e-02\n",
      "Epoch: 1585 mean train loss:  3.56096971e-02, mean val. rec. loss:  2.37391310e-02\n",
      "Epoch: 1586 mean train loss:  3.55818648e-02, mean val. rec. loss:  2.37175060e-02\n",
      "Epoch: 1587 mean train loss:  3.55541480e-02, mean val. rec. loss:  2.36959378e-02\n",
      "Epoch: 1588 mean train loss:  3.55264833e-02, mean val. rec. loss:  2.36744784e-02\n",
      "Epoch: 1589 mean train loss:  3.54989191e-02, mean val. rec. loss:  2.36531052e-02\n",
      "Epoch: 1590 mean train loss:  3.54714667e-02, mean val. rec. loss:  2.36318001e-02\n",
      "Epoch: 1591 mean train loss:  3.54440404e-02, mean val. rec. loss:  2.36105811e-02\n",
      "Epoch: 1592 mean train loss:  3.54167369e-02, mean val. rec. loss:  2.35894393e-02\n",
      "Epoch: 1593 mean train loss:  3.53895080e-02, mean val. rec. loss:  2.35683836e-02\n",
      "Epoch: 1594 mean train loss:  3.53623759e-02, mean val. rec. loss:  2.35473982e-02\n",
      "Epoch: 1595 mean train loss:  3.53352884e-02, mean val. rec. loss:  2.35265036e-02\n",
      "Epoch: 1596 mean train loss:  3.53083090e-02, mean val. rec. loss:  2.35057155e-02\n",
      "Epoch: 1597 mean train loss:  3.52814413e-02, mean val. rec. loss:  2.34849796e-02\n",
      "Epoch: 1598 mean train loss:  3.52546072e-02, mean val. rec. loss:  2.34643344e-02\n",
      "Epoch: 1599 mean train loss:  3.52278773e-02, mean val. rec. loss:  2.34437822e-02\n",
      "Epoch: 1600 mean train loss:  3.52012182e-02, mean val. rec. loss:  2.34232981e-02\n",
      "Epoch: 1601 mean train loss:  3.51746335e-02, mean val. rec. loss:  2.34029046e-02\n",
      "Epoch: 1602 mean train loss:  3.51481606e-02, mean val. rec. loss:  2.33825792e-02\n",
      "Epoch: 1603 mean train loss:  3.51217473e-02, mean val. rec. loss:  2.33623174e-02\n",
      "Epoch: 1604 mean train loss:  3.50954085e-02, mean val. rec. loss:  2.33421530e-02\n",
      "Epoch: 1605 mean train loss:  3.50691777e-02, mean val. rec. loss:  2.33220362e-02\n",
      "Epoch: 1606 mean train loss:  3.50429767e-02, mean val. rec. loss:  2.33020215e-02\n",
      "Epoch: 1607 mean train loss:  3.50168948e-02, mean val. rec. loss:  2.32820862e-02\n",
      "Epoch: 1608 mean train loss:  3.49909098e-02, mean val. rec. loss:  2.32622076e-02\n",
      "Epoch: 1609 mean train loss:  3.49649583e-02, mean val. rec. loss:  2.32424288e-02\n",
      "Epoch: 1610 mean train loss:  3.49391111e-02, mean val. rec. loss:  2.32227294e-02\n",
      "Epoch: 1611 mean train loss:  3.49133310e-02, mean val. rec. loss:  2.32031116e-02\n",
      "Epoch: 1612 mean train loss:  3.48876551e-02, mean val. rec. loss:  2.31835664e-02\n",
      "Epoch: 1613 mean train loss:  3.48620537e-02, mean val. rec. loss:  2.31641028e-02\n",
      "Epoch: 1614 mean train loss:  3.48365193e-02, mean val. rec. loss:  2.31447118e-02\n",
      "Epoch: 1615 mean train loss:  3.48110446e-02, mean val. rec. loss:  2.31253888e-02\n",
      "Epoch: 1616 mean train loss:  3.47856629e-02, mean val. rec. loss:  2.31061384e-02\n",
      "Epoch: 1617 mean train loss:  3.47603446e-02, mean val. rec. loss:  2.30869493e-02\n",
      "Epoch: 1618 mean train loss:  3.47351417e-02, mean val. rec. loss:  2.30678667e-02\n",
      "Epoch: 1619 mean train loss:  3.47099909e-02, mean val. rec. loss:  2.30488545e-02\n",
      "Epoch: 1620 mean train loss:  3.46849221e-02, mean val. rec. loss:  2.30298966e-02\n",
      "Epoch: 1621 mean train loss:  3.46599241e-02, mean val. rec. loss:  2.30110205e-02\n",
      "Epoch: 1622 mean train loss:  3.46350005e-02, mean val. rec. loss:  2.29922191e-02\n",
      "Epoch: 1623 mean train loss:  3.46101440e-02, mean val. rec. loss:  2.29734949e-02\n",
      "Epoch: 1624 mean train loss:  3.45853769e-02, mean val. rec. loss:  2.29548387e-02\n",
      "Epoch: 1625 mean train loss:  3.45606842e-02, mean val. rec. loss:  2.29362687e-02\n",
      "Epoch: 1626 mean train loss:  3.45360810e-02, mean val. rec. loss:  2.29177532e-02\n",
      "Epoch: 1627 mean train loss:  3.45115224e-02, mean val. rec. loss:  2.28993147e-02\n",
      "Epoch: 1628 mean train loss:  3.44870607e-02, mean val. rec. loss:  2.28809624e-02\n",
      "Epoch: 1629 mean train loss:  3.44626883e-02, mean val. rec. loss:  2.28626555e-02\n",
      "Epoch: 1630 mean train loss:  3.44383532e-02, mean val. rec. loss:  2.28444370e-02\n",
      "Epoch: 1631 mean train loss:  3.44141149e-02, mean val. rec. loss:  2.28262821e-02\n",
      "Epoch: 1632 mean train loss:  3.43899474e-02, mean val. rec. loss:  2.28082110e-02\n",
      "Epoch: 1633 mean train loss:  3.43658619e-02, mean val. rec. loss:  2.27902171e-02\n",
      "Epoch: 1634 mean train loss:  3.43418359e-02, mean val. rec. loss:  2.27722912e-02\n",
      "Epoch: 1635 mean train loss:  3.43178918e-02, mean val. rec. loss:  2.27544243e-02\n",
      "Epoch: 1636 mean train loss:  3.42940037e-02, mean val. rec. loss:  2.27366435e-02\n",
      "Epoch: 1637 mean train loss:  3.42702198e-02, mean val. rec. loss:  2.27189308e-02\n",
      "Epoch: 1638 mean train loss:  3.42464992e-02, mean val. rec. loss:  2.27012861e-02\n",
      "Epoch: 1639 mean train loss:  3.42228419e-02, mean val. rec. loss:  2.26836913e-02\n",
      "Epoch: 1640 mean train loss:  3.41992405e-02, mean val. rec. loss:  2.26661805e-02\n",
      "Epoch: 1641 mean train loss:  3.41757322e-02, mean val. rec. loss:  2.26487286e-02\n",
      "Epoch: 1642 mean train loss:  3.41522947e-02, mean val. rec. loss:  2.26313334e-02\n",
      "Epoch: 1643 mean train loss:  3.41289167e-02, mean val. rec. loss:  2.26140198e-02\n",
      "Epoch: 1644 mean train loss:  3.41056356e-02, mean val. rec. loss:  2.25967811e-02\n",
      "Epoch: 1645 mean train loss:  3.40823992e-02, mean val. rec. loss:  2.25795877e-02\n",
      "Epoch: 1646 mean train loss:  3.40592559e-02, mean val. rec. loss:  2.25624851e-02\n",
      "Epoch: 1647 mean train loss:  3.40361536e-02, mean val. rec. loss:  2.25454460e-02\n",
      "Epoch: 1648 mean train loss:  3.40131555e-02, mean val. rec. loss:  2.25284839e-02\n",
      "Epoch: 1649 mean train loss:  3.39902096e-02, mean val. rec. loss:  2.25115764e-02\n",
      "Epoch: 1650 mean train loss:  3.39673233e-02, mean val. rec. loss:  2.24947345e-02\n",
      "Epoch: 1651 mean train loss:  3.39445003e-02, mean val. rec. loss:  2.24779540e-02\n",
      "Epoch: 1652 mean train loss:  3.39217667e-02, mean val. rec. loss:  2.24612527e-02\n",
      "Epoch: 1653 mean train loss:  3.38991038e-02, mean val. rec. loss:  2.24446218e-02\n",
      "Epoch: 1654 mean train loss:  3.38765266e-02, mean val. rec. loss:  2.24280454e-02\n",
      "Epoch: 1655 mean train loss:  3.38539904e-02, mean val. rec. loss:  2.24115165e-02\n",
      "Epoch: 1656 mean train loss:  3.38315361e-02, mean val. rec. loss:  2.23950761e-02\n",
      "Epoch: 1657 mean train loss:  3.38091414e-02, mean val. rec. loss:  2.23786992e-02\n",
      "Epoch: 1658 mean train loss:  3.37868025e-02, mean val. rec. loss:  2.23623700e-02\n",
      "Epoch: 1659 mean train loss:  3.37645531e-02, mean val. rec. loss:  2.23461201e-02\n",
      "Epoch: 1660 mean train loss:  3.37423595e-02, mean val. rec. loss:  2.23299337e-02\n",
      "Epoch: 1661 mean train loss:  3.37202553e-02, mean val. rec. loss:  2.23137927e-02\n",
      "Epoch: 1662 mean train loss:  3.36981809e-02, mean val. rec. loss:  2.22977379e-02\n",
      "Epoch: 1663 mean train loss:  3.36761922e-02, mean val. rec. loss:  2.22817420e-02\n",
      "Epoch: 1664 mean train loss:  3.36542965e-02, mean val. rec. loss:  2.22657915e-02\n",
      "Epoch: 1665 mean train loss:  3.36324195e-02, mean val. rec. loss:  2.22499271e-02\n",
      "Epoch: 1666 mean train loss:  3.36106430e-02, mean val. rec. loss:  2.22341172e-02\n",
      "Epoch: 1667 mean train loss:  3.35889187e-02, mean val. rec. loss:  2.22183708e-02\n",
      "Epoch: 1668 mean train loss:  3.35672801e-02, mean val. rec. loss:  2.22026607e-02\n",
      "Epoch: 1669 mean train loss:  3.35456749e-02, mean val. rec. loss:  2.21870232e-02\n",
      "Epoch: 1670 mean train loss:  3.35241629e-02, mean val. rec. loss:  2.21714560e-02\n",
      "Epoch: 1671 mean train loss:  3.35027142e-02, mean val. rec. loss:  2.21559522e-02\n",
      "Epoch: 1672 mean train loss:  3.34812990e-02, mean val. rec. loss:  2.21405188e-02\n",
      "Epoch: 1673 mean train loss:  3.34599695e-02, mean val. rec. loss:  2.21251262e-02\n",
      "Epoch: 1674 mean train loss:  3.34386921e-02, mean val. rec. loss:  2.21098198e-02\n",
      "Epoch: 1675 mean train loss:  3.34175227e-02, mean val. rec. loss:  2.20945633e-02\n",
      "Epoch: 1676 mean train loss:  3.33963906e-02, mean val. rec. loss:  2.20793566e-02\n",
      "Epoch: 1677 mean train loss:  3.33753180e-02, mean val. rec. loss:  2.20642181e-02\n",
      "Epoch: 1678 mean train loss:  3.33543014e-02, mean val. rec. loss:  2.20491407e-02\n",
      "Epoch: 1679 mean train loss:  3.33333666e-02, mean val. rec. loss:  2.20341087e-02\n",
      "Epoch: 1680 mean train loss:  3.33124766e-02, mean val. rec. loss:  2.20191311e-02\n",
      "Epoch: 1681 mean train loss:  3.32916684e-02, mean val. rec. loss:  2.20042216e-02\n",
      "Epoch: 1682 mean train loss:  3.32709013e-02, mean val. rec. loss:  2.19893620e-02\n",
      "Epoch: 1683 mean train loss:  3.32502161e-02, mean val. rec. loss:  2.19745659e-02\n",
      "Epoch: 1684 mean train loss:  3.32295905e-02, mean val. rec. loss:  2.19598264e-02\n",
      "Epoch: 1685 mean train loss:  3.32090244e-02, mean val. rec. loss:  2.19451550e-02\n",
      "Epoch: 1686 mean train loss:  3.31885180e-02, mean val. rec. loss:  2.19305268e-02\n",
      "Epoch: 1687 mean train loss:  3.31680786e-02, mean val. rec. loss:  2.19159642e-02\n",
      "Epoch: 1688 mean train loss:  3.31476913e-02, mean val. rec. loss:  2.19014743e-02\n",
      "Epoch: 1689 mean train loss:  3.31273860e-02, mean val. rec. loss:  2.18870410e-02\n",
      "Epoch: 1690 mean train loss:  3.31071365e-02, mean val. rec. loss:  2.18726486e-02\n",
      "Epoch: 1691 mean train loss:  3.30869318e-02, mean val. rec. loss:  2.18583219e-02\n",
      "Epoch: 1692 mean train loss:  3.30667866e-02, mean val. rec. loss:  2.18440542e-02\n",
      "Epoch: 1693 mean train loss:  3.30467233e-02, mean val. rec. loss:  2.18298296e-02\n",
      "Epoch: 1694 mean train loss:  3.30267085e-02, mean val. rec. loss:  2.18156640e-02\n",
      "Epoch: 1695 mean train loss:  3.30067496e-02, mean val. rec. loss:  2.18015551e-02\n",
      "Epoch: 1696 mean train loss:  3.29868651e-02, mean val. rec. loss:  2.17874937e-02\n",
      "Epoch: 1697 mean train loss:  3.29670216e-02, mean val. rec. loss:  2.17734710e-02\n",
      "Epoch: 1698 mean train loss:  3.29472600e-02, mean val. rec. loss:  2.17595185e-02\n",
      "Epoch: 1699 mean train loss:  3.29275357e-02, mean val. rec. loss:  2.17456251e-02\n",
      "Epoch: 1700 mean train loss:  3.29078933e-02, mean val. rec. loss:  2.17317769e-02\n",
      "Epoch: 1701 mean train loss:  3.28882882e-02, mean val. rec. loss:  2.17179878e-02\n",
      "Epoch: 1702 mean train loss:  3.28687426e-02, mean val. rec. loss:  2.17042644e-02\n",
      "Epoch: 1703 mean train loss:  3.28492790e-02, mean val. rec. loss:  2.16905954e-02\n",
      "Epoch: 1704 mean train loss:  3.28298489e-02, mean val. rec. loss:  2.16769787e-02\n",
      "Epoch: 1705 mean train loss:  3.28104970e-02, mean val. rec. loss:  2.16634072e-02\n",
      "Epoch: 1706 mean train loss:  3.27911824e-02, mean val. rec. loss:  2.16498812e-02\n",
      "Epoch: 1707 mean train loss:  3.27719422e-02, mean val. rec. loss:  2.16364254e-02\n",
      "Epoch: 1708 mean train loss:  3.27527505e-02, mean val. rec. loss:  2.16230082e-02\n",
      "Epoch: 1709 mean train loss:  3.27336072e-02, mean val. rec. loss:  2.16096477e-02\n",
      "Epoch: 1710 mean train loss:  3.27145458e-02, mean val. rec. loss:  2.15963280e-02\n",
      "Epoch: 1711 mean train loss:  3.26955366e-02, mean val. rec. loss:  2.15830673e-02\n",
      "Epoch: 1712 mean train loss:  3.26765720e-02, mean val. rec. loss:  2.15698405e-02\n",
      "Epoch: 1713 mean train loss:  3.26576671e-02, mean val. rec. loss:  2.15567000e-02\n",
      "Epoch: 1714 mean train loss:  3.26388068e-02, mean val. rec. loss:  2.15435913e-02\n",
      "Epoch: 1715 mean train loss:  3.26200210e-02, mean val. rec. loss:  2.15305369e-02\n",
      "Epoch: 1716 mean train loss:  3.26013023e-02, mean val. rec. loss:  2.15175189e-02\n",
      "Epoch: 1717 mean train loss:  3.25826059e-02, mean val. rec. loss:  2.15045757e-02\n",
      "Epoch: 1718 mean train loss:  3.25639840e-02, mean val. rec. loss:  2.14916733e-02\n",
      "Epoch: 1719 mean train loss:  3.25454366e-02, mean val. rec. loss:  2.14788185e-02\n",
      "Epoch: 1720 mean train loss:  3.25269115e-02, mean val. rec. loss:  2.14660227e-02\n",
      "Epoch: 1721 mean train loss:  3.25084609e-02, mean val. rec. loss:  2.14532723e-02\n",
      "Epoch: 1722 mean train loss:  3.24900550e-02, mean val. rec. loss:  2.14405808e-02\n",
      "Epoch: 1723 mean train loss:  3.24717162e-02, mean val. rec. loss:  2.14279211e-02\n",
      "Epoch: 1724 mean train loss:  3.24534145e-02, mean val. rec. loss:  2.14152977e-02\n",
      "Epoch: 1725 mean train loss:  3.24351800e-02, mean val. rec. loss:  2.14027423e-02\n",
      "Epoch: 1726 mean train loss:  3.24170050e-02, mean val. rec. loss:  2.13902187e-02\n",
      "Epoch: 1727 mean train loss:  3.23988673e-02, mean val. rec. loss:  2.13777517e-02\n",
      "Epoch: 1728 mean train loss:  3.23807966e-02, mean val. rec. loss:  2.13653392e-02\n",
      "Epoch: 1729 mean train loss:  3.23627706e-02, mean val. rec. loss:  2.13529698e-02\n",
      "Epoch: 1730 mean train loss:  3.23448041e-02, mean val. rec. loss:  2.13406616e-02\n",
      "Epoch: 1731 mean train loss:  3.23268824e-02, mean val. rec. loss:  2.13283965e-02\n",
      "Epoch: 1732 mean train loss:  3.23090166e-02, mean val. rec. loss:  2.13161859e-02\n",
      "Epoch: 1733 mean train loss:  3.22912103e-02, mean val. rec. loss:  2.13039956e-02\n",
      "Epoch: 1734 mean train loss:  3.22734525e-02, mean val. rec. loss:  2.12918689e-02\n",
      "Epoch: 1735 mean train loss:  3.22557467e-02, mean val. rec. loss:  2.12797875e-02\n",
      "Epoch: 1736 mean train loss:  3.22380932e-02, mean val. rec. loss:  2.12677537e-02\n",
      "Epoch: 1737 mean train loss:  3.22205067e-02, mean val. rec. loss:  2.12557653e-02\n",
      "Epoch: 1738 mean train loss:  3.22029350e-02, mean val. rec. loss:  2.12438200e-02\n",
      "Epoch: 1739 mean train loss:  3.21854304e-02, mean val. rec. loss:  2.12319065e-02\n",
      "Epoch: 1740 mean train loss:  3.21680078e-02, mean val. rec. loss:  2.12200405e-02\n",
      "Epoch: 1741 mean train loss:  3.21506000e-02, mean val. rec. loss:  2.12082313e-02\n",
      "Epoch: 1742 mean train loss:  3.21332593e-02, mean val. rec. loss:  2.11964447e-02\n",
      "Epoch: 1743 mean train loss:  3.21159745e-02, mean val. rec. loss:  2.11847217e-02\n",
      "Epoch: 1744 mean train loss:  3.20987306e-02, mean val. rec. loss:  2.11730531e-02\n",
      "Epoch: 1745 mean train loss:  3.20815500e-02, mean val. rec. loss:  2.11614275e-02\n",
      "Epoch: 1746 mean train loss:  3.20644104e-02, mean val. rec. loss:  2.11498338e-02\n",
      "Epoch: 1747 mean train loss:  3.20473080e-02, mean val. rec. loss:  2.11382990e-02\n",
      "Epoch: 1748 mean train loss:  3.20302802e-02, mean val. rec. loss:  2.11268140e-02\n",
      "Epoch: 1749 mean train loss:  3.20132895e-02, mean val. rec. loss:  2.11153541e-02\n",
      "Epoch: 1750 mean train loss:  3.19963399e-02, mean val. rec. loss:  2.11039531e-02\n",
      "Epoch: 1751 mean train loss:  3.19794684e-02, mean val. rec. loss:  2.10925816e-02\n",
      "Epoch: 1752 mean train loss:  3.19626193e-02, mean val. rec. loss:  2.10812645e-02\n",
      "Epoch: 1753 mean train loss:  3.19458410e-02, mean val. rec. loss:  2.10699769e-02\n",
      "Epoch: 1754 mean train loss:  3.19290887e-02, mean val. rec. loss:  2.10587460e-02\n",
      "Epoch: 1755 mean train loss:  3.19124035e-02, mean val. rec. loss:  2.10475536e-02\n",
      "Epoch: 1756 mean train loss:  3.18957630e-02, mean val. rec. loss:  2.10363817e-02\n",
      "Epoch: 1757 mean train loss:  3.18791597e-02, mean val. rec. loss:  2.10252574e-02\n",
      "Epoch: 1758 mean train loss:  3.18625936e-02, mean val. rec. loss:  2.10141784e-02\n",
      "Epoch: 1759 mean train loss:  3.18461021e-02, mean val. rec. loss:  2.10031380e-02\n",
      "Epoch: 1760 mean train loss:  3.18296478e-02, mean val. rec. loss:  2.09921498e-02\n",
      "Epoch: 1761 mean train loss:  3.18132605e-02, mean val. rec. loss:  2.09812137e-02\n",
      "Epoch: 1762 mean train loss:  3.17968881e-02, mean val. rec. loss:  2.09703049e-02\n",
      "Epoch: 1763 mean train loss:  3.17805828e-02, mean val. rec. loss:  2.09594414e-02\n",
      "Epoch: 1764 mean train loss:  3.17643370e-02, mean val. rec. loss:  2.09486210e-02\n",
      "Epoch: 1765 mean train loss:  3.17481062e-02, mean val. rec. loss:  2.09378391e-02\n",
      "Epoch: 1766 mean train loss:  3.17319424e-02, mean val. rec. loss:  2.09270935e-02\n",
      "Epoch: 1767 mean train loss:  3.17158158e-02, mean val. rec. loss:  2.09164092e-02\n",
      "Epoch: 1768 mean train loss:  3.16997600e-02, mean val. rec. loss:  2.09057498e-02\n",
      "Epoch: 1769 mean train loss:  3.16837228e-02, mean val. rec. loss:  2.08951086e-02\n",
      "Epoch: 1770 mean train loss:  3.16677415e-02, mean val. rec. loss:  2.08845173e-02\n",
      "Epoch: 1771 mean train loss:  3.16518123e-02, mean val. rec. loss:  2.08739645e-02\n",
      "Epoch: 1772 mean train loss:  3.16359092e-02, mean val. rec. loss:  2.08634729e-02\n",
      "Epoch: 1773 mean train loss:  3.16200694e-02, mean val. rec. loss:  2.08529927e-02\n",
      "Epoch: 1774 mean train loss:  3.16042668e-02, mean val. rec. loss:  2.08425647e-02\n",
      "Epoch: 1775 mean train loss:  3.15885276e-02, mean val. rec. loss:  2.08321661e-02\n",
      "Epoch: 1776 mean train loss:  3.15728293e-02, mean val. rec. loss:  2.08218288e-02\n",
      "Epoch: 1777 mean train loss:  3.15571720e-02, mean val. rec. loss:  2.08115050e-02\n",
      "Epoch: 1778 mean train loss:  3.15415445e-02, mean val. rec. loss:  2.08012267e-02\n",
      "Epoch: 1779 mean train loss:  3.15259766e-02, mean val. rec. loss:  2.07909959e-02\n",
      "Epoch: 1780 mean train loss:  3.15104348e-02, mean val. rec. loss:  2.07808083e-02\n",
      "Epoch: 1781 mean train loss:  3.14949600e-02, mean val. rec. loss:  2.07706569e-02\n",
      "Epoch: 1782 mean train loss:  3.14795150e-02, mean val. rec. loss:  2.07605351e-02\n",
      "Epoch: 1783 mean train loss:  3.14641296e-02, mean val. rec. loss:  2.07504540e-02\n",
      "Epoch: 1784 mean train loss:  3.14487814e-02, mean val. rec. loss:  2.07404070e-02\n",
      "Epoch: 1785 mean train loss:  3.14334630e-02, mean val. rec. loss:  2.07303894e-02\n",
      "Epoch: 1786 mean train loss:  3.14182117e-02, mean val. rec. loss:  2.07204286e-02\n",
      "Epoch: 1787 mean train loss:  3.14029790e-02, mean val. rec. loss:  2.07104723e-02\n",
      "Epoch: 1788 mean train loss:  3.13878058e-02, mean val. rec. loss:  2.07005681e-02\n",
      "Epoch: 1789 mean train loss:  3.13726662e-02, mean val. rec. loss:  2.06907002e-02\n",
      "Epoch: 1790 mean train loss:  3.13575899e-02, mean val. rec. loss:  2.06808528e-02\n",
      "Epoch: 1791 mean train loss:  3.13425360e-02, mean val. rec. loss:  2.06710394e-02\n",
      "Epoch: 1792 mean train loss:  3.13275230e-02, mean val. rec. loss:  2.06612804e-02\n",
      "Epoch: 1793 mean train loss:  3.13125696e-02, mean val. rec. loss:  2.06515508e-02\n",
      "Epoch: 1794 mean train loss:  3.12976535e-02, mean val. rec. loss:  2.06418621e-02\n",
      "Epoch: 1795 mean train loss:  3.12827671e-02, mean val. rec. loss:  2.06322120e-02\n",
      "Epoch: 1796 mean train loss:  3.12679217e-02, mean val. rec. loss:  2.06226140e-02\n",
      "Epoch: 1797 mean train loss:  3.12531359e-02, mean val. rec. loss:  2.06130387e-02\n",
      "Epoch: 1798 mean train loss:  3.12383837e-02, mean val. rec. loss:  2.06034884e-02\n",
      "Epoch: 1799 mean train loss:  3.12236612e-02, mean val. rec. loss:  2.05939811e-02\n",
      "Epoch: 1800 mean train loss:  3.12090020e-02, mean val. rec. loss:  2.05845011e-02\n",
      "Epoch: 1801 mean train loss:  3.11943764e-02, mean val. rec. loss:  2.05750641e-02\n",
      "Epoch: 1802 mean train loss:  3.11797842e-02, mean val. rec. loss:  2.05656362e-02\n",
      "Epoch: 1803 mean train loss:  3.11652256e-02, mean val. rec. loss:  2.05562424e-02\n",
      "Epoch: 1804 mean train loss:  3.11507266e-02, mean val. rec. loss:  2.05468893e-02\n",
      "Epoch: 1805 mean train loss:  3.11362649e-02, mean val. rec. loss:  2.05375703e-02\n",
      "Epoch: 1806 mean train loss:  3.11218329e-02, mean val. rec. loss:  2.05282967e-02\n",
      "Epoch: 1807 mean train loss:  3.11074568e-02, mean val. rec. loss:  2.05190479e-02\n",
      "Epoch: 1808 mean train loss:  3.10931179e-02, mean val. rec. loss:  2.05098332e-02\n",
      "Epoch: 1809 mean train loss:  3.10788088e-02, mean val. rec. loss:  2.05006367e-02\n",
      "Epoch: 1810 mean train loss:  3.10645389e-02, mean val. rec. loss:  2.04914878e-02\n",
      "Epoch: 1811 mean train loss:  3.10503210e-02, mean val. rec. loss:  2.04823774e-02\n",
      "Epoch: 1812 mean train loss:  3.10361349e-02, mean val. rec. loss:  2.04732897e-02\n",
      "Epoch: 1813 mean train loss:  3.10219822e-02, mean val. rec. loss:  2.04642519e-02\n",
      "Epoch: 1814 mean train loss:  3.10078892e-02, mean val. rec. loss:  2.04552368e-02\n",
      "Epoch: 1815 mean train loss:  3.09938073e-02, mean val. rec. loss:  2.04462489e-02\n",
      "Epoch: 1816 mean train loss:  3.09797962e-02, mean val. rec. loss:  2.04372928e-02\n",
      "Epoch: 1817 mean train loss:  3.09657962e-02, mean val. rec. loss:  2.04283752e-02\n",
      "Epoch: 1818 mean train loss:  3.09518559e-02, mean val. rec. loss:  2.04194893e-02\n",
      "Epoch: 1819 mean train loss:  3.09379471e-02, mean val. rec. loss:  2.04106262e-02\n",
      "Epoch: 1820 mean train loss:  3.09240682e-02, mean val. rec. loss:  2.04017880e-02\n",
      "Epoch: 1821 mean train loss:  3.09102173e-02, mean val. rec. loss:  2.03929951e-02\n",
      "Epoch: 1822 mean train loss:  3.08964222e-02, mean val. rec. loss:  2.03842227e-02\n",
      "Epoch: 1823 mean train loss:  3.08826643e-02, mean val. rec. loss:  2.03754865e-02\n",
      "Epoch: 1824 mean train loss:  3.08689586e-02, mean val. rec. loss:  2.03667685e-02\n",
      "Epoch: 1825 mean train loss:  3.08552566e-02, mean val. rec. loss:  2.03580777e-02\n",
      "Epoch: 1826 mean train loss:  3.08416123e-02, mean val. rec. loss:  2.03494300e-02\n",
      "Epoch: 1827 mean train loss:  3.08280016e-02, mean val. rec. loss:  2.03408186e-02\n",
      "Epoch: 1828 mean train loss:  3.08144411e-02, mean val. rec. loss:  2.03322276e-02\n",
      "Epoch: 1829 mean train loss:  3.08008992e-02, mean val. rec. loss:  2.03236728e-02\n",
      "Epoch: 1830 mean train loss:  3.07873984e-02, mean val. rec. loss:  2.03151272e-02\n",
      "Epoch: 1831 mean train loss:  3.07739440e-02, mean val. rec. loss:  2.03066360e-02\n",
      "Epoch: 1832 mean train loss:  3.07605046e-02, mean val. rec. loss:  2.02981606e-02\n",
      "Epoch: 1833 mean train loss:  3.07471248e-02, mean val. rec. loss:  2.02897261e-02\n",
      "Epoch: 1834 mean train loss:  3.07337691e-02, mean val. rec. loss:  2.02813256e-02\n",
      "Epoch: 1835 mean train loss:  3.07204638e-02, mean val. rec. loss:  2.02729478e-02\n",
      "Epoch: 1836 mean train loss:  3.07071808e-02, mean val. rec. loss:  2.02645745e-02\n",
      "Epoch: 1837 mean train loss:  3.06939387e-02, mean val. rec. loss:  2.02562466e-02\n",
      "Epoch: 1838 mean train loss:  3.06807395e-02, mean val. rec. loss:  2.02479436e-02\n",
      "Epoch: 1839 mean train loss:  3.06675552e-02, mean val. rec. loss:  2.02396724e-02\n",
      "Epoch: 1840 mean train loss:  3.06544249e-02, mean val. rec. loss:  2.02314307e-02\n",
      "Epoch: 1841 mean train loss:  3.06413244e-02, mean val. rec. loss:  2.02232207e-02\n",
      "Epoch: 1842 mean train loss:  3.06282537e-02, mean val. rec. loss:  2.02150039e-02\n",
      "Epoch: 1843 mean train loss:  3.06152221e-02, mean val. rec. loss:  2.02068551e-02\n",
      "Epoch: 1844 mean train loss:  3.06022259e-02, mean val. rec. loss:  2.01987200e-02\n",
      "Epoch: 1845 mean train loss:  3.05892576e-02, mean val. rec. loss:  2.01906075e-02\n",
      "Epoch: 1846 mean train loss:  3.05763451e-02, mean val. rec. loss:  2.01825245e-02\n",
      "Epoch: 1847 mean train loss:  3.05634402e-02, mean val. rec. loss:  2.01744642e-02\n",
      "Epoch: 1848 mean train loss:  3.05505817e-02, mean val. rec. loss:  2.01664493e-02\n",
      "Epoch: 1849 mean train loss:  3.05377717e-02, mean val. rec. loss:  2.01584502e-02\n",
      "Epoch: 1850 mean train loss:  3.05249692e-02, mean val. rec. loss:  2.01504783e-02\n",
      "Epoch: 1851 mean train loss:  3.05122150e-02, mean val. rec. loss:  2.01425246e-02\n",
      "Epoch: 1852 mean train loss:  3.04994888e-02, mean val. rec. loss:  2.01346140e-02\n",
      "Epoch: 1853 mean train loss:  3.04868073e-02, mean val. rec. loss:  2.01267147e-02\n",
      "Epoch: 1854 mean train loss:  3.04741518e-02, mean val. rec. loss:  2.01188608e-02\n",
      "Epoch: 1855 mean train loss:  3.04615374e-02, mean val. rec. loss:  2.01110182e-02\n",
      "Epoch: 1856 mean train loss:  3.04489434e-02, mean val. rec. loss:  2.01032074e-02\n",
      "Epoch: 1857 mean train loss:  3.04364015e-02, mean val. rec. loss:  2.00954079e-02\n",
      "Epoch: 1858 mean train loss:  3.04238746e-02, mean val. rec. loss:  2.00876560e-02\n",
      "Epoch: 1859 mean train loss:  3.04113811e-02, mean val. rec. loss:  2.00799223e-02\n",
      "Epoch: 1860 mean train loss:  3.03989342e-02, mean val. rec. loss:  2.00721954e-02\n",
      "Epoch: 1861 mean train loss:  3.03864967e-02, mean val. rec. loss:  2.00645048e-02\n",
      "Epoch: 1862 mean train loss:  3.03741112e-02, mean val. rec. loss:  2.00568413e-02\n",
      "Epoch: 1863 mean train loss:  3.03617500e-02, mean val. rec. loss:  2.00492097e-02\n",
      "Epoch: 1864 mean train loss:  3.03494447e-02, mean val. rec. loss:  2.00415984e-02\n",
      "Epoch: 1865 mean train loss:  3.03371300e-02, mean val. rec. loss:  2.00340053e-02\n",
      "Epoch: 1866 mean train loss:  3.03248712e-02, mean val. rec. loss:  2.00264304e-02\n",
      "Epoch: 1867 mean train loss:  3.03126441e-02, mean val. rec. loss:  2.00188781e-02\n",
      "Epoch: 1868 mean train loss:  3.03004542e-02, mean val. rec. loss:  2.00113621e-02\n",
      "Epoch: 1869 mean train loss:  3.02882996e-02, mean val. rec. loss:  2.00038688e-02\n",
      "Epoch: 1870 mean train loss:  3.02761675e-02, mean val. rec. loss:  1.99963891e-02\n",
      "Epoch: 1871 mean train loss:  3.02640595e-02, mean val. rec. loss:  1.99889456e-02\n",
      "Epoch: 1872 mean train loss:  3.02519832e-02, mean val. rec. loss:  1.99815158e-02\n",
      "Epoch: 1873 mean train loss:  3.02399348e-02, mean val. rec. loss:  1.99741200e-02\n",
      "Epoch: 1874 mean train loss:  3.02279330e-02, mean val. rec. loss:  1.99667378e-02\n",
      "Epoch: 1875 mean train loss:  3.02159629e-02, mean val. rec. loss:  1.99593783e-02\n",
      "Epoch: 1876 mean train loss:  3.02040169e-02, mean val. rec. loss:  1.99520528e-02\n",
      "Epoch: 1877 mean train loss:  3.01920896e-02, mean val. rec. loss:  1.99447568e-02\n",
      "Epoch: 1878 mean train loss:  3.01802162e-02, mean val. rec. loss:  1.99374835e-02\n",
      "Epoch: 1879 mean train loss:  3.01683597e-02, mean val. rec. loss:  1.99302283e-02\n",
      "Epoch: 1880 mean train loss:  3.01565422e-02, mean val. rec. loss:  1.99229981e-02\n",
      "Epoch: 1881 mean train loss:  3.01447415e-02, mean val. rec. loss:  1.99157792e-02\n",
      "Epoch: 1882 mean train loss:  3.01329725e-02, mean val. rec. loss:  1.99085784e-02\n",
      "Epoch: 1883 mean train loss:  3.01212351e-02, mean val. rec. loss:  1.99014072e-02\n",
      "Epoch: 1884 mean train loss:  3.01095275e-02, mean val. rec. loss:  1.98942563e-02\n",
      "Epoch: 1885 mean train loss:  3.00978553e-02, mean val. rec. loss:  1.98871349e-02\n",
      "Epoch: 1886 mean train loss:  3.00862259e-02, mean val. rec. loss:  1.98800317e-02\n",
      "Epoch: 1887 mean train loss:  3.00745947e-02, mean val. rec. loss:  1.98729398e-02\n",
      "Epoch: 1888 mean train loss:  3.00630100e-02, mean val. rec. loss:  1.98658706e-02\n",
      "Epoch: 1889 mean train loss:  3.00514514e-02, mean val. rec. loss:  1.98588332e-02\n",
      "Epoch: 1890 mean train loss:  3.00399374e-02, mean val. rec. loss:  1.98518025e-02\n",
      "Epoch: 1891 mean train loss:  3.00284235e-02, mean val. rec. loss:  1.98448059e-02\n",
      "Epoch: 1892 mean train loss:  3.00169580e-02, mean val. rec. loss:  1.98378251e-02\n",
      "Epoch: 1893 mean train loss:  3.00055130e-02, mean val. rec. loss:  1.98308602e-02\n",
      "Epoch: 1894 mean train loss:  2.99941182e-02, mean val. rec. loss:  1.98239135e-02\n",
      "Epoch: 1895 mean train loss:  2.99827291e-02, mean val. rec. loss:  1.98170166e-02\n",
      "Epoch: 1896 mean train loss:  2.99713697e-02, mean val. rec. loss:  1.98101130e-02\n",
      "Epoch: 1897 mean train loss:  2.99600495e-02, mean val. rec. loss:  1.98032366e-02\n",
      "Epoch: 1898 mean train loss:  2.99487404e-02, mean val. rec. loss:  1.97963805e-02\n",
      "Epoch: 1899 mean train loss:  2.99374797e-02, mean val. rec. loss:  1.97895472e-02\n",
      "Epoch: 1900 mean train loss:  2.99262358e-02, mean val. rec. loss:  1.97827411e-02\n",
      "Epoch: 1901 mean train loss:  2.99150180e-02, mean val. rec. loss:  1.97759621e-02\n",
      "Epoch: 1902 mean train loss:  2.99038392e-02, mean val. rec. loss:  1.97691991e-02\n",
      "Epoch: 1903 mean train loss:  2.98926866e-02, mean val. rec. loss:  1.97624610e-02\n",
      "Epoch: 1904 mean train loss:  2.98815488e-02, mean val. rec. loss:  1.97557411e-02\n",
      "Epoch: 1905 mean train loss:  2.98704409e-02, mean val. rec. loss:  1.97490189e-02\n",
      "Epoch: 1906 mean train loss:  2.98593701e-02, mean val. rec. loss:  1.97423171e-02\n",
      "Epoch: 1907 mean train loss:  2.98483218e-02, mean val. rec. loss:  1.97356470e-02\n",
      "Epoch: 1908 mean train loss:  2.98372976e-02, mean val. rec. loss:  1.97289906e-02\n",
      "Epoch: 1909 mean train loss:  2.98262920e-02, mean val. rec. loss:  1.97223636e-02\n",
      "Epoch: 1910 mean train loss:  2.98153349e-02, mean val. rec. loss:  1.97157276e-02\n",
      "Epoch: 1911 mean train loss:  2.98043964e-02, mean val. rec. loss:  1.97091301e-02\n",
      "Epoch: 1912 mean train loss:  2.97934765e-02, mean val. rec. loss:  1.97025462e-02\n",
      "Epoch: 1913 mean train loss:  2.97825771e-02, mean val. rec. loss:  1.96959760e-02\n",
      "Epoch: 1914 mean train loss:  2.97717261e-02, mean val. rec. loss:  1.96894420e-02\n",
      "Epoch: 1915 mean train loss:  2.97608900e-02, mean val. rec. loss:  1.96829262e-02\n",
      "Epoch: 1916 mean train loss:  2.97500782e-02, mean val. rec. loss:  1.96764126e-02\n",
      "Epoch: 1917 mean train loss:  2.97393054e-02, mean val. rec. loss:  1.96699240e-02\n",
      "Epoch: 1918 mean train loss:  2.97285326e-02, mean val. rec. loss:  1.96634467e-02\n",
      "Epoch: 1919 mean train loss:  2.97178045e-02, mean val. rec. loss:  1.96569967e-02\n",
      "Epoch: 1920 mean train loss:  2.97071137e-02, mean val. rec. loss:  1.96505670e-02\n",
      "Epoch: 1921 mean train loss:  2.96964173e-02, mean val. rec. loss:  1.96441578e-02\n",
      "Epoch: 1922 mean train loss:  2.96857711e-02, mean val. rec. loss:  1.96377622e-02\n",
      "Epoch: 1923 mean train loss:  2.96751380e-02, mean val. rec. loss:  1.96313824e-02\n",
      "Epoch: 1924 mean train loss:  2.96645347e-02, mean val. rec. loss:  1.96250231e-02\n",
      "Epoch: 1925 mean train loss:  2.96539687e-02, mean val. rec. loss:  1.96186773e-02\n",
      "Epoch: 1926 mean train loss:  2.96433989e-02, mean val. rec. loss:  1.96123588e-02\n",
      "Epoch: 1927 mean train loss:  2.96328700e-02, mean val. rec. loss:  1.96060516e-02\n",
      "Epoch: 1928 mean train loss:  2.96223822e-02, mean val. rec. loss:  1.95997535e-02\n",
      "Epoch: 1929 mean train loss:  2.96118906e-02, mean val. rec. loss:  1.95934872e-02\n",
      "Epoch: 1930 mean train loss:  2.96014437e-02, mean val. rec. loss:  1.95872231e-02\n",
      "Epoch: 1931 mean train loss:  2.95910154e-02, mean val. rec. loss:  1.95809862e-02\n",
      "Epoch: 1932 mean train loss:  2.95806244e-02, mean val. rec. loss:  1.95747584e-02\n",
      "Epoch: 1933 mean train loss:  2.95702334e-02, mean val. rec. loss:  1.95685488e-02\n",
      "Epoch: 1934 mean train loss:  2.95598833e-02, mean val. rec. loss:  1.95623436e-02\n",
      "Epoch: 1935 mean train loss:  2.95495482e-02, mean val. rec. loss:  1.95561748e-02\n",
      "Epoch: 1936 mean train loss:  2.95392559e-02, mean val. rec. loss:  1.95500196e-02\n",
      "Epoch: 1937 mean train loss:  2.95289598e-02, mean val. rec. loss:  1.95438916e-02\n",
      "Epoch: 1938 mean train loss:  2.95187084e-02, mean val. rec. loss:  1.95377681e-02\n",
      "Epoch: 1939 mean train loss:  2.95084720e-02, mean val. rec. loss:  1.95316628e-02\n",
      "Epoch: 1940 mean train loss:  2.94982746e-02, mean val. rec. loss:  1.95255665e-02\n",
      "Epoch: 1941 mean train loss:  2.94880735e-02, mean val. rec. loss:  1.95194975e-02\n",
      "Epoch: 1942 mean train loss:  2.94779134e-02, mean val. rec. loss:  1.95134375e-02\n",
      "Epoch: 1943 mean train loss:  2.94677719e-02, mean val. rec. loss:  1.95073798e-02\n",
      "Epoch: 1944 mean train loss:  2.94576696e-02, mean val. rec. loss:  1.95013425e-02\n",
      "Epoch: 1945 mean train loss:  2.94475616e-02, mean val. rec. loss:  1.94953302e-02\n",
      "Epoch: 1946 mean train loss:  2.94374927e-02, mean val. rec. loss:  1.94893223e-02\n",
      "Epoch: 1947 mean train loss:  2.94274406e-02, mean val. rec. loss:  1.94833372e-02\n",
      "Epoch: 1948 mean train loss:  2.94174332e-02, mean val. rec. loss:  1.94773634e-02\n",
      "Epoch: 1949 mean train loss:  2.94074184e-02, mean val. rec. loss:  1.94714123e-02\n",
      "Epoch: 1950 mean train loss:  2.93974445e-02, mean val. rec. loss:  1.94654680e-02\n",
      "Epoch: 1951 mean train loss:  2.93874836e-02, mean val. rec. loss:  1.94595464e-02\n",
      "Epoch: 1952 mean train loss:  2.93775693e-02, mean val. rec. loss:  1.94536338e-02\n",
      "Epoch: 1953 mean train loss:  2.93676420e-02, mean val. rec. loss:  1.94477508e-02\n",
      "Epoch: 1954 mean train loss:  2.93577593e-02, mean val. rec. loss:  1.94418654e-02\n",
      "Epoch: 1955 mean train loss:  2.93478897e-02, mean val. rec. loss:  1.94360186e-02\n",
      "Epoch: 1956 mean train loss:  2.93380573e-02, mean val. rec. loss:  1.94301719e-02\n",
      "Epoch: 1957 mean train loss:  2.93282268e-02, mean val. rec. loss:  1.94243205e-02\n",
      "Epoch: 1958 mean train loss:  2.93184317e-02, mean val. rec. loss:  1.94185078e-02\n",
      "Epoch: 1959 mean train loss:  2.93086552e-02, mean val. rec. loss:  1.94126950e-02\n",
      "Epoch: 1960 mean train loss:  2.92989178e-02, mean val. rec. loss:  1.94068981e-02\n",
      "Epoch: 1961 mean train loss:  2.92891711e-02, mean val. rec. loss:  1.94011239e-02\n",
      "Epoch: 1962 mean train loss:  2.92794617e-02, mean val. rec. loss:  1.93953497e-02\n",
      "Epoch: 1963 mean train loss:  2.92697690e-02, mean val. rec. loss:  1.93896118e-02\n",
      "Epoch: 1964 mean train loss:  2.92601098e-02, mean val. rec. loss:  1.93838557e-02\n",
      "Epoch: 1965 mean train loss:  2.92504469e-02, mean val. rec. loss:  1.93781450e-02\n",
      "Epoch: 1966 mean train loss:  2.92408231e-02, mean val. rec. loss:  1.93724343e-02\n",
      "Epoch: 1967 mean train loss:  2.92312216e-02, mean val. rec. loss:  1.93667440e-02\n",
      "Epoch: 1968 mean train loss:  2.92216537e-02, mean val. rec. loss:  1.93610492e-02\n",
      "Epoch: 1969 mean train loss:  2.92120783e-02, mean val. rec. loss:  1.93553884e-02\n",
      "Epoch: 1970 mean train loss:  2.92025439e-02, mean val. rec. loss:  1.93497276e-02\n",
      "Epoch: 1971 mean train loss:  2.91930206e-02, mean val. rec. loss:  1.93440826e-02\n",
      "Epoch: 1972 mean train loss:  2.91835142e-02, mean val. rec. loss:  1.93384490e-02\n",
      "Epoch: 1973 mean train loss:  2.91740449e-02, mean val. rec. loss:  1.93328358e-02\n",
      "Epoch: 1974 mean train loss:  2.91645906e-02, mean val. rec. loss:  1.93272249e-02\n",
      "Epoch: 1975 mean train loss:  2.91551474e-02, mean val. rec. loss:  1.93216367e-02\n",
      "Epoch: 1976 mean train loss:  2.91457284e-02, mean val. rec. loss:  1.93160575e-02\n",
      "Epoch: 1977 mean train loss:  2.91363188e-02, mean val. rec. loss:  1.93104965e-02\n",
      "Epoch: 1978 mean train loss:  2.91269464e-02, mean val. rec. loss:  1.93049468e-02\n",
      "Epoch: 1979 mean train loss:  2.91175945e-02, mean val. rec. loss:  1.92994017e-02\n",
      "Epoch: 1980 mean train loss:  2.91082537e-02, mean val. rec. loss:  1.92938860e-02\n",
      "Epoch: 1981 mean train loss:  2.90989465e-02, mean val. rec. loss:  1.92883794e-02\n",
      "Epoch: 1982 mean train loss:  2.90896579e-02, mean val. rec. loss:  1.92828706e-02\n",
      "Epoch: 1983 mean train loss:  2.90803842e-02, mean val. rec. loss:  1.92773844e-02\n",
      "Epoch: 1984 mean train loss:  2.90711216e-02, mean val. rec. loss:  1.92719050e-02\n",
      "Epoch: 1985 mean train loss:  2.90618777e-02, mean val. rec. loss:  1.92664302e-02\n",
      "Epoch: 1986 mean train loss:  2.90526673e-02, mean val. rec. loss:  1.92609735e-02\n",
      "Epoch: 1987 mean train loss:  2.90434756e-02, mean val. rec. loss:  1.92555259e-02\n",
      "Epoch: 1988 mean train loss:  2.90342931e-02, mean val. rec. loss:  1.92500805e-02\n",
      "Epoch: 1989 mean train loss:  2.90251255e-02, mean val. rec. loss:  1.92446714e-02\n",
      "Epoch: 1990 mean train loss:  2.90159934e-02, mean val. rec. loss:  1.92392556e-02\n",
      "Epoch: 1991 mean train loss:  2.90068723e-02, mean val. rec. loss:  1.92338669e-02\n",
      "Epoch: 1992 mean train loss:  2.89977718e-02, mean val. rec. loss:  1.92284919e-02\n",
      "Epoch: 1993 mean train loss:  2.89886825e-02, mean val. rec. loss:  1.92231304e-02\n",
      "Epoch: 1994 mean train loss:  2.89796229e-02, mean val. rec. loss:  1.92177780e-02\n",
      "Epoch: 1995 mean train loss:  2.89705857e-02, mean val. rec. loss:  1.92124302e-02\n",
      "Epoch: 1996 mean train loss:  2.89615615e-02, mean val. rec. loss:  1.92070937e-02\n",
      "Epoch: 1997 mean train loss:  2.89525504e-02, mean val. rec. loss:  1.92017527e-02\n",
      "Epoch: 1998 mean train loss:  2.89435728e-02, mean val. rec. loss:  1.91964411e-02\n",
      "Epoch: 1999 mean train loss:  2.89345858e-02, mean val. rec. loss:  1.91911387e-02\n",
      "Epoch: 2000 mean train loss:  2.89256361e-02, mean val. rec. loss:  1.91858498e-02\n",
      "Epoch: 2001 mean train loss:  2.89167200e-02, mean val. rec. loss:  1.91805587e-02\n",
      "Epoch: 2002 mean train loss:  2.89077945e-02, mean val. rec. loss:  1.91752766e-02\n",
      "Epoch: 2003 mean train loss:  2.88988988e-02, mean val. rec. loss:  1.91700059e-02\n",
      "Epoch: 2004 mean train loss:  2.88900273e-02, mean val. rec. loss:  1.91647533e-02\n",
      "Epoch: 2005 mean train loss:  2.88811689e-02, mean val. rec. loss:  1.91595189e-02\n",
      "Epoch: 2006 mean train loss:  2.88723198e-02, mean val. rec. loss:  1.91542935e-02\n",
      "Epoch: 2007 mean train loss:  2.88635060e-02, mean val. rec. loss:  1.91490772e-02\n",
      "Epoch: 2008 mean train loss:  2.88547053e-02, mean val. rec. loss:  1.91438609e-02\n",
      "Epoch: 2009 mean train loss:  2.88459120e-02, mean val. rec. loss:  1.91386582e-02\n",
      "Epoch: 2010 mean train loss:  2.88371374e-02, mean val. rec. loss:  1.91334782e-02\n",
      "Epoch: 2011 mean train loss:  2.88283926e-02, mean val. rec. loss:  1.91283050e-02\n",
      "Epoch: 2012 mean train loss:  2.88196589e-02, mean val. rec. loss:  1.91231454e-02\n",
      "Epoch: 2013 mean train loss:  2.88109401e-02, mean val. rec. loss:  1.91179926e-02\n",
      "Epoch: 2014 mean train loss:  2.88022362e-02, mean val. rec. loss:  1.91128535e-02\n",
      "Epoch: 2015 mean train loss:  2.87935547e-02, mean val. rec. loss:  1.91077097e-02\n",
      "Epoch: 2016 mean train loss:  2.87848825e-02, mean val. rec. loss:  1.91025932e-02\n",
      "Epoch: 2017 mean train loss:  2.87762382e-02, mean val. rec. loss:  1.90974654e-02\n",
      "Epoch: 2018 mean train loss:  2.87675995e-02, mean val. rec. loss:  1.90923580e-02\n",
      "Epoch: 2019 mean train loss:  2.87589999e-02, mean val. rec. loss:  1.90872528e-02\n",
      "Epoch: 2020 mean train loss:  2.87503928e-02, mean val. rec. loss:  1.90821590e-02\n",
      "Epoch: 2021 mean train loss:  2.87418156e-02, mean val. rec. loss:  1.90770697e-02\n",
      "Epoch: 2022 mean train loss:  2.87332458e-02, mean val. rec. loss:  1.90719986e-02\n",
      "Epoch: 2023 mean train loss:  2.87247132e-02, mean val. rec. loss:  1.90669410e-02\n",
      "Epoch: 2024 mean train loss:  2.87161714e-02, mean val. rec. loss:  1.90618903e-02\n",
      "Epoch: 2025 mean train loss:  2.87076667e-02, mean val. rec. loss:  1.90568441e-02\n",
      "Epoch: 2026 mean train loss:  2.86991677e-02, mean val. rec. loss:  1.90518228e-02\n",
      "Epoch: 2027 mean train loss:  2.86906836e-02, mean val. rec. loss:  1.90468084e-02\n",
      "Epoch: 2028 mean train loss:  2.86822292e-02, mean val. rec. loss:  1.90417985e-02\n",
      "Epoch: 2029 mean train loss:  2.86737674e-02, mean val. rec. loss:  1.90367999e-02\n",
      "Epoch: 2030 mean train loss:  2.86653392e-02, mean val. rec. loss:  1.90318127e-02\n",
      "Epoch: 2031 mean train loss:  2.86569183e-02, mean val. rec. loss:  1.90268391e-02\n",
      "Epoch: 2032 mean train loss:  2.86485292e-02, mean val. rec. loss:  1.90218609e-02\n",
      "Epoch: 2033 mean train loss:  2.86401344e-02, mean val. rec. loss:  1.90168714e-02\n",
      "Epoch: 2034 mean train loss:  2.86317713e-02, mean val. rec. loss:  1.90118876e-02\n",
      "Epoch: 2035 mean train loss:  2.86234212e-02, mean val. rec. loss:  1.90069083e-02\n",
      "Epoch: 2036 mean train loss:  2.86150935e-02, mean val. rec. loss:  1.90019596e-02\n",
      "Epoch: 2037 mean train loss:  2.86067695e-02, mean val. rec. loss:  1.89970041e-02\n",
      "Epoch: 2038 mean train loss:  2.85984753e-02, mean val. rec. loss:  1.89920804e-02\n",
      "Epoch: 2039 mean train loss:  2.85901886e-02, mean val. rec. loss:  1.89871726e-02\n",
      "Epoch: 2040 mean train loss:  2.85819130e-02, mean val. rec. loss:  1.89822727e-02\n",
      "Epoch: 2041 mean train loss:  2.85736468e-02, mean val. rec. loss:  1.89773886e-02\n",
      "Epoch: 2042 mean train loss:  2.85654140e-02, mean val. rec. loss:  1.89725046e-02\n",
      "Epoch: 2043 mean train loss:  2.85571869e-02, mean val. rec. loss:  1.89676330e-02\n",
      "Epoch: 2044 mean train loss:  2.85489727e-02, mean val. rec. loss:  1.89627592e-02\n",
      "Epoch: 2045 mean train loss:  2.85407735e-02, mean val. rec. loss:  1.89578967e-02\n",
      "Epoch: 2046 mean train loss:  2.85325799e-02, mean val. rec. loss:  1.89530433e-02\n",
      "Epoch: 2047 mean train loss:  2.85244216e-02, mean val. rec. loss:  1.89481740e-02\n",
      "Epoch: 2048 mean train loss:  2.85162745e-02, mean val. rec. loss:  1.89433274e-02\n",
      "Epoch: 2049 mean train loss:  2.85081368e-02, mean val. rec. loss:  1.89384819e-02\n",
      "Epoch: 2050 mean train loss:  2.85000139e-02, mean val. rec. loss:  1.89336285e-02\n",
      "Epoch: 2051 mean train loss:  2.84919096e-02, mean val. rec. loss:  1.89288068e-02\n",
      "Epoch: 2052 mean train loss:  2.84838240e-02, mean val. rec. loss:  1.89239840e-02\n",
      "Epoch: 2053 mean train loss:  2.84757458e-02, mean val. rec. loss:  1.89191737e-02\n",
      "Epoch: 2054 mean train loss:  2.84676751e-02, mean val. rec. loss:  1.89143758e-02\n",
      "Epoch: 2055 mean train loss:  2.84596416e-02, mean val. rec. loss:  1.89095882e-02\n",
      "Epoch: 2056 mean train loss:  2.84515951e-02, mean val. rec. loss:  1.89048141e-02\n",
      "Epoch: 2057 mean train loss:  2.84435821e-02, mean val. rec. loss:  1.89000412e-02\n",
      "Epoch: 2058 mean train loss:  2.84355746e-02, mean val. rec. loss:  1.88952842e-02\n",
      "Epoch: 2059 mean train loss:  2.84275970e-02, mean val. rec. loss:  1.88905283e-02\n",
      "Epoch: 2060 mean train loss:  2.84196119e-02, mean val. rec. loss:  1.88857837e-02\n",
      "Epoch: 2061 mean train loss:  2.84116529e-02, mean val. rec. loss:  1.88810312e-02\n",
      "Epoch: 2062 mean train loss:  2.84037051e-02, mean val. rec. loss:  1.88763037e-02\n",
      "Epoch: 2063 mean train loss:  2.83957685e-02, mean val. rec. loss:  1.88715682e-02\n",
      "Epoch: 2064 mean train loss:  2.83878374e-02, mean val. rec. loss:  1.88668270e-02\n",
      "Epoch: 2065 mean train loss:  2.83799398e-02, mean val. rec. loss:  1.88621018e-02\n",
      "Epoch: 2066 mean train loss:  2.83720497e-02, mean val. rec. loss:  1.88573833e-02\n",
      "Epoch: 2067 mean train loss:  2.83641745e-02, mean val. rec. loss:  1.88526535e-02\n",
      "Epoch: 2068 mean train loss:  2.83563049e-02, mean val. rec. loss:  1.88479441e-02\n",
      "Epoch: 2069 mean train loss:  2.83484688e-02, mean val. rec. loss:  1.88432392e-02\n",
      "Epoch: 2070 mean train loss:  2.83406215e-02, mean val. rec. loss:  1.88385457e-02\n",
      "Epoch: 2071 mean train loss:  2.83327985e-02, mean val. rec. loss:  1.88338771e-02\n",
      "Epoch: 2072 mean train loss:  2.83249885e-02, mean val. rec. loss:  1.88292096e-02\n",
      "Epoch: 2073 mean train loss:  2.83172064e-02, mean val. rec. loss:  1.88245501e-02\n",
      "Epoch: 2074 mean train loss:  2.83094112e-02, mean val. rec. loss:  1.88199042e-02\n",
      "Epoch: 2075 mean train loss:  2.83016459e-02, mean val. rec. loss:  1.88152674e-02\n",
      "Epoch: 2076 mean train loss:  2.82938917e-02, mean val. rec. loss:  1.88106283e-02\n",
      "Epoch: 2077 mean train loss:  2.82861450e-02, mean val. rec. loss:  1.88059824e-02\n",
      "Epoch: 2078 mean train loss:  2.82784095e-02, mean val. rec. loss:  1.88013444e-02\n",
      "Epoch: 2079 mean train loss:  2.82707000e-02, mean val. rec. loss:  1.87967110e-02\n",
      "Epoch: 2080 mean train loss:  2.82629980e-02, mean val. rec. loss:  1.87920866e-02\n",
      "Epoch: 2081 mean train loss:  2.82553034e-02, mean val. rec. loss:  1.87874702e-02\n",
      "Epoch: 2082 mean train loss:  2.82476238e-02, mean val. rec. loss:  1.87828572e-02\n",
      "Epoch: 2083 mean train loss:  2.82399534e-02, mean val. rec. loss:  1.87782521e-02\n",
      "Epoch: 2084 mean train loss:  2.82323054e-02, mean val. rec. loss:  1.87736606e-02\n",
      "Epoch: 2085 mean train loss:  2.82246685e-02, mean val. rec. loss:  1.87690624e-02\n",
      "Epoch: 2086 mean train loss:  2.82170429e-02, mean val. rec. loss:  1.87644822e-02\n",
      "Epoch: 2087 mean train loss:  2.82094246e-02, mean val. rec. loss:  1.87599055e-02\n",
      "Epoch: 2088 mean train loss:  2.82018362e-02, mean val. rec. loss:  1.87553401e-02\n",
      "Epoch: 2089 mean train loss:  2.81942310e-02, mean val. rec. loss:  1.87507645e-02\n",
      "Epoch: 2090 mean train loss:  2.81866575e-02, mean val. rec. loss:  1.87462071e-02\n",
      "Epoch: 2091 mean train loss:  2.81790933e-02, mean val. rec. loss:  1.87416292e-02\n",
      "Epoch: 2092 mean train loss:  2.81715495e-02, mean val. rec. loss:  1.87370786e-02\n",
      "Epoch: 2093 mean train loss:  2.81640114e-02, mean val. rec. loss:  1.87325336e-02\n",
      "Epoch: 2094 mean train loss:  2.81564863e-02, mean val. rec. loss:  1.87279977e-02\n",
      "Epoch: 2095 mean train loss:  2.81489742e-02, mean val. rec. loss:  1.87234663e-02\n",
      "Epoch: 2096 mean train loss:  2.81414714e-02, mean val. rec. loss:  1.87189440e-02\n",
      "Epoch: 2097 mean train loss:  2.81339705e-02, mean val. rec. loss:  1.87144251e-02\n",
      "Epoch: 2098 mean train loss:  2.81264938e-02, mean val. rec. loss:  1.87099187e-02\n",
      "Epoch: 2099 mean train loss:  2.81190209e-02, mean val. rec. loss:  1.87054066e-02\n",
      "Epoch: 2100 mean train loss:  2.81115647e-02, mean val. rec. loss:  1.87009024e-02\n",
      "Epoch: 2101 mean train loss:  2.81041215e-02, mean val. rec. loss:  1.86963994e-02\n",
      "Epoch: 2102 mean train loss:  2.80967025e-02, mean val. rec. loss:  1.86919009e-02\n",
      "Epoch: 2103 mean train loss:  2.80892724e-02, mean val. rec. loss:  1.86874059e-02\n",
      "Epoch: 2104 mean train loss:  2.80818702e-02, mean val. rec. loss:  1.86829108e-02\n",
      "Epoch: 2105 mean train loss:  2.80744754e-02, mean val. rec. loss:  1.86784406e-02\n",
      "Epoch: 2106 mean train loss:  2.80670900e-02, mean val. rec. loss:  1.86739501e-02\n",
      "Epoch: 2107 mean train loss:  2.80597101e-02, mean val. rec. loss:  1.86694709e-02\n",
      "Epoch: 2108 mean train loss:  2.80523582e-02, mean val. rec. loss:  1.86649962e-02\n",
      "Epoch: 2109 mean train loss:  2.80450025e-02, mean val. rec. loss:  1.86605193e-02\n",
      "Epoch: 2110 mean train loss:  2.80376692e-02, mean val. rec. loss:  1.86560786e-02\n",
      "Epoch: 2111 mean train loss:  2.80303452e-02, mean val. rec. loss:  1.86516176e-02\n",
      "Epoch: 2112 mean train loss:  2.80230287e-02, mean val. rec. loss:  1.86471724e-02\n",
      "Epoch: 2113 mean train loss:  2.80157214e-02, mean val. rec. loss:  1.86427351e-02\n",
      "Epoch: 2114 mean train loss:  2.80084217e-02, mean val. rec. loss:  1.86383001e-02\n",
      "Epoch: 2115 mean train loss:  2.80011405e-02, mean val. rec. loss:  1.86338731e-02\n",
      "Epoch: 2116 mean train loss:  2.79938668e-02, mean val. rec. loss:  1.86294574e-02\n",
      "Epoch: 2117 mean train loss:  2.79866210e-02, mean val. rec. loss:  1.86250349e-02\n",
      "Epoch: 2118 mean train loss:  2.79793622e-02, mean val. rec. loss:  1.86206135e-02\n",
      "Epoch: 2119 mean train loss:  2.79721276e-02, mean val. rec. loss:  1.86162069e-02\n",
      "Epoch: 2120 mean train loss:  2.79649041e-02, mean val. rec. loss:  1.86118025e-02\n",
      "Epoch: 2121 mean train loss:  2.79576844e-02, mean val. rec. loss:  1.86073947e-02\n",
      "Epoch: 2122 mean train loss:  2.79504703e-02, mean val. rec. loss:  1.86029870e-02\n",
      "Epoch: 2123 mean train loss:  2.79432860e-02, mean val. rec. loss:  1.85985815e-02\n",
      "Epoch: 2124 mean train loss:  2.79360942e-02, mean val. rec. loss:  1.85941941e-02\n",
      "Epoch: 2125 mean train loss:  2.79289136e-02, mean val. rec. loss:  1.85897909e-02\n",
      "Epoch: 2126 mean train loss:  2.79217628e-02, mean val. rec. loss:  1.85854047e-02\n",
      "Epoch: 2127 mean train loss:  2.79146082e-02, mean val. rec. loss:  1.85810230e-02\n",
      "Epoch: 2128 mean train loss:  2.79074593e-02, mean val. rec. loss:  1.85766458e-02\n",
      "Epoch: 2129 mean train loss:  2.79003327e-02, mean val. rec. loss:  1.85722641e-02\n",
      "Epoch: 2130 mean train loss:  2.78932117e-02, mean val. rec. loss:  1.85678972e-02\n",
      "Epoch: 2131 mean train loss:  2.78861000e-02, mean val. rec. loss:  1.85635450e-02\n",
      "Epoch: 2132 mean train loss:  2.78789957e-02, mean val. rec. loss:  1.85591883e-02\n",
      "Epoch: 2133 mean train loss:  2.78719156e-02, mean val. rec. loss:  1.85548451e-02\n",
      "Epoch: 2134 mean train loss:  2.78648244e-02, mean val. rec. loss:  1.85504941e-02\n",
      "Epoch: 2135 mean train loss:  2.78577555e-02, mean val. rec. loss:  1.85461441e-02\n",
      "Epoch: 2136 mean train loss:  2.78506960e-02, mean val. rec. loss:  1.85418055e-02\n",
      "Epoch: 2137 mean train loss:  2.78436420e-02, mean val. rec. loss:  1.85374692e-02\n",
      "Epoch: 2138 mean train loss:  2.78366104e-02, mean val. rec. loss:  1.85331170e-02\n",
      "Epoch: 2139 mean train loss:  2.78295694e-02, mean val. rec. loss:  1.85287807e-02\n",
      "Epoch: 2140 mean train loss:  2.78225564e-02, mean val. rec. loss:  1.85244489e-02\n",
      "Epoch: 2141 mean train loss:  2.78155471e-02, mean val. rec. loss:  1.85201182e-02\n",
      "Epoch: 2142 mean train loss:  2.78085453e-02, mean val. rec. loss:  1.85158080e-02\n",
      "Epoch: 2143 mean train loss:  2.78015640e-02, mean val. rec. loss:  1.85114875e-02\n",
      "Epoch: 2144 mean train loss:  2.77945733e-02, mean val. rec. loss:  1.85071807e-02\n",
      "Epoch: 2145 mean train loss:  2.77876106e-02, mean val. rec. loss:  1.85028761e-02\n",
      "Epoch: 2146 mean train loss:  2.77806460e-02, mean val. rec. loss:  1.84985738e-02\n",
      "Epoch: 2147 mean train loss:  2.77736925e-02, mean val. rec. loss:  1.84942715e-02\n",
      "Epoch: 2148 mean train loss:  2.77667429e-02, mean val. rec. loss:  1.84899578e-02\n",
      "Epoch: 2149 mean train loss:  2.77598192e-02, mean val. rec. loss:  1.84856669e-02\n",
      "Epoch: 2150 mean train loss:  2.77528863e-02, mean val. rec. loss:  1.84813759e-02\n",
      "Epoch: 2151 mean train loss:  2.77459720e-02, mean val. rec. loss:  1.84770759e-02\n",
      "Epoch: 2152 mean train loss:  2.77390670e-02, mean val. rec. loss:  1.84727713e-02\n",
      "Epoch: 2153 mean train loss:  2.77321694e-02, mean val. rec. loss:  1.84684917e-02\n",
      "Epoch: 2154 mean train loss:  2.77252905e-02, mean val. rec. loss:  1.84642075e-02\n",
      "Epoch: 2155 mean train loss:  2.77184041e-02, mean val. rec. loss:  1.84599313e-02\n",
      "Epoch: 2156 mean train loss:  2.77115419e-02, mean val. rec. loss:  1.84556562e-02\n",
      "Epoch: 2157 mean train loss:  2.77046835e-02, mean val. rec. loss:  1.84513924e-02\n",
      "Epoch: 2158 mean train loss:  2.76978288e-02, mean val. rec. loss:  1.84471162e-02\n",
      "Epoch: 2159 mean train loss:  2.76910001e-02, mean val. rec. loss:  1.84428581e-02\n",
      "Epoch: 2160 mean train loss:  2.76841584e-02, mean val. rec. loss:  1.84385944e-02\n",
      "Epoch: 2161 mean train loss:  2.76773428e-02, mean val. rec. loss:  1.84343510e-02\n",
      "Epoch: 2162 mean train loss:  2.76705309e-02, mean val. rec. loss:  1.84301122e-02\n",
      "Epoch: 2163 mean train loss:  2.76637246e-02, mean val. rec. loss:  1.84258496e-02\n",
      "Epoch: 2164 mean train loss:  2.76569239e-02, mean val. rec. loss:  1.84216085e-02\n",
      "Epoch: 2165 mean train loss:  2.76501511e-02, mean val. rec. loss:  1.84173595e-02\n",
      "Epoch: 2166 mean train loss:  2.76433616e-02, mean val. rec. loss:  1.84131083e-02\n",
      "Epoch: 2167 mean train loss:  2.76365962e-02, mean val. rec. loss:  1.84088615e-02\n",
      "Epoch: 2168 mean train loss:  2.76298346e-02, mean val. rec. loss:  1.84046125e-02\n",
      "Epoch: 2169 mean train loss:  2.76230842e-02, mean val. rec. loss:  1.84003828e-02\n",
      "Epoch: 2170 mean train loss:  2.76163523e-02, mean val. rec. loss:  1.83961372e-02\n",
      "Epoch: 2171 mean train loss:  2.76096075e-02, mean val. rec. loss:  1.83919052e-02\n",
      "Epoch: 2172 mean train loss:  2.76028868e-02, mean val. rec. loss:  1.83876845e-02\n",
      "Epoch: 2173 mean train loss:  2.75961718e-02, mean val. rec. loss:  1.83834650e-02\n",
      "Epoch: 2174 mean train loss:  2.75894605e-02, mean val. rec. loss:  1.83792420e-02\n",
      "Epoch: 2175 mean train loss:  2.75827547e-02, mean val. rec. loss:  1.83750384e-02\n",
      "Epoch: 2176 mean train loss:  2.75760694e-02, mean val. rec. loss:  1.83708336e-02\n",
      "Epoch: 2177 mean train loss:  2.75693767e-02, mean val. rec. loss:  1.83666209e-02\n",
      "Epoch: 2178 mean train loss:  2.75627045e-02, mean val. rec. loss:  1.83624036e-02\n",
      "Epoch: 2179 mean train loss:  2.75560360e-02, mean val. rec. loss:  1.83581796e-02\n",
      "Epoch: 2180 mean train loss:  2.75493749e-02, mean val. rec. loss:  1.83539668e-02\n",
      "Epoch: 2181 mean train loss:  2.75427288e-02, mean val. rec. loss:  1.83497587e-02\n",
      "Epoch: 2182 mean train loss:  2.75360882e-02, mean val. rec. loss:  1.83455505e-02\n",
      "Epoch: 2183 mean train loss:  2.75294569e-02, mean val. rec. loss:  1.83413400e-02\n",
      "Epoch: 2184 mean train loss:  2.75228276e-02, mean val. rec. loss:  1.83371613e-02\n",
      "Epoch: 2185 mean train loss:  2.75162037e-02, mean val. rec. loss:  1.83329679e-02\n",
      "Epoch: 2186 mean train loss:  2.75096004e-02, mean val. rec. loss:  1.83287721e-02\n",
      "Epoch: 2187 mean train loss:  2.75029896e-02, mean val. rec. loss:  1.83245787e-02\n",
      "Epoch: 2188 mean train loss:  2.74964031e-02, mean val. rec. loss:  1.83204057e-02\n",
      "Epoch: 2189 mean train loss:  2.74898091e-02, mean val. rec. loss:  1.83162281e-02\n",
      "Epoch: 2190 mean train loss:  2.74832243e-02, mean val. rec. loss:  1.83120505e-02\n",
      "Epoch: 2191 mean train loss:  2.74766545e-02, mean val. rec. loss:  1.83078525e-02\n",
      "Epoch: 2192 mean train loss:  2.74700959e-02, mean val. rec. loss:  1.83036602e-02\n",
      "Epoch: 2193 mean train loss:  2.74635335e-02, mean val. rec. loss:  1.82994725e-02\n",
      "Epoch: 2194 mean train loss:  2.74569935e-02, mean val. rec. loss:  1.82952904e-02\n",
      "Epoch: 2195 mean train loss:  2.74504461e-02, mean val. rec. loss:  1.82911037e-02\n",
      "Epoch: 2196 mean train loss:  2.74439191e-02, mean val. rec. loss:  1.82869386e-02\n",
      "Epoch: 2197 mean train loss:  2.74373996e-02, mean val. rec. loss:  1.82827758e-02\n",
      "Epoch: 2198 mean train loss:  2.74308782e-02, mean val. rec. loss:  1.82786243e-02\n",
      "Epoch: 2199 mean train loss:  2.74243605e-02, mean val. rec. loss:  1.82744626e-02\n",
      "Epoch: 2200 mean train loss:  2.74178615e-02, mean val. rec. loss:  1.82703111e-02\n",
      "Epoch: 2201 mean train loss:  2.74113699e-02, mean val. rec. loss:  1.82661608e-02\n",
      "Epoch: 2202 mean train loss:  2.74048857e-02, mean val. rec. loss:  1.82620081e-02\n",
      "Epoch: 2203 mean train loss:  2.73984016e-02, mean val. rec. loss:  1.82578555e-02\n",
      "Epoch: 2204 mean train loss:  2.73919211e-02, mean val. rec. loss:  1.82537040e-02\n",
      "Epoch: 2205 mean train loss:  2.73854668e-02, mean val. rec. loss:  1.82495457e-02\n",
      "Epoch: 2206 mean train loss:  2.73790013e-02, mean val. rec. loss:  1.82453920e-02\n",
      "Epoch: 2207 mean train loss:  2.73725432e-02, mean val. rec. loss:  1.82412371e-02\n",
      "Epoch: 2208 mean train loss:  2.73661000e-02, mean val. rec. loss:  1.82370833e-02\n",
      "Epoch: 2209 mean train loss:  2.73596736e-02, mean val. rec. loss:  1.82329284e-02\n",
      "Epoch: 2210 mean train loss:  2.73532434e-02, mean val. rec. loss:  1.82287792e-02\n",
      "Epoch: 2211 mean train loss:  2.73468114e-02, mean val. rec. loss:  1.82246436e-02\n",
      "Epoch: 2212 mean train loss:  2.73404018e-02, mean val. rec. loss:  1.82204978e-02\n",
      "Epoch: 2213 mean train loss:  2.73339958e-02, mean val. rec. loss:  1.82163656e-02\n",
      "Epoch: 2214 mean train loss:  2.73275936e-02, mean val. rec. loss:  1.82122379e-02\n",
      "Epoch: 2215 mean train loss:  2.73211933e-02, mean val. rec. loss:  1.82080955e-02\n",
      "Epoch: 2216 mean train loss:  2.73148004e-02, mean val. rec. loss:  1.82039451e-02\n",
      "Epoch: 2217 mean train loss:  2.73084242e-02, mean val. rec. loss:  1.81998163e-02\n",
      "Epoch: 2218 mean train loss:  2.73020481e-02, mean val. rec. loss:  1.81956887e-02\n",
      "Epoch: 2219 mean train loss:  2.72956720e-02, mean val. rec. loss:  1.81915644e-02\n",
      "Epoch: 2220 mean train loss:  2.72893107e-02, mean val. rec. loss:  1.81874446e-02\n",
      "Epoch: 2221 mean train loss:  2.72829607e-02, mean val. rec. loss:  1.81833090e-02\n",
      "Epoch: 2222 mean train loss:  2.72766143e-02, mean val. rec. loss:  1.81791689e-02\n",
      "Epoch: 2223 mean train loss:  2.72702605e-02, mean val. rec. loss:  1.81750367e-02\n",
      "Epoch: 2224 mean train loss:  2.72639328e-02, mean val. rec. loss:  1.81709045e-02\n",
      "Epoch: 2225 mean train loss:  2.72576088e-02, mean val. rec. loss:  1.81667881e-02\n",
      "Epoch: 2226 mean train loss:  2.72512885e-02, mean val. rec. loss:  1.81626695e-02\n",
      "Epoch: 2227 mean train loss:  2.72449701e-02, mean val. rec. loss:  1.81585555e-02\n",
      "Epoch: 2228 mean train loss:  2.72386517e-02, mean val. rec. loss:  1.81544346e-02\n",
      "Epoch: 2229 mean train loss:  2.72323575e-02, mean val. rec. loss:  1.81503251e-02\n",
      "Epoch: 2230 mean train loss:  2.72260558e-02, mean val. rec. loss:  1.81461985e-02\n",
      "Epoch: 2231 mean train loss:  2.72197635e-02, mean val. rec. loss:  1.81420822e-02\n",
      "Epoch: 2232 mean train loss:  2.72134805e-02, mean val. rec. loss:  1.81379704e-02\n",
      "Epoch: 2233 mean train loss:  2.72072086e-02, mean val. rec. loss:  1.81338677e-02\n",
      "Epoch: 2234 mean train loss:  2.72009423e-02, mean val. rec. loss:  1.81297479e-02\n",
      "Epoch: 2235 mean train loss:  2.71946668e-02, mean val. rec. loss:  1.81256429e-02\n",
      "Epoch: 2236 mean train loss:  2.71884117e-02, mean val. rec. loss:  1.81215334e-02\n",
      "Epoch: 2237 mean train loss:  2.71821640e-02, mean val. rec. loss:  1.81174068e-02\n",
      "Epoch: 2238 mean train loss:  2.71759201e-02, mean val. rec. loss:  1.81132962e-02\n",
      "Epoch: 2239 mean train loss:  2.71696762e-02, mean val. rec. loss:  1.81091787e-02\n",
      "Epoch: 2240 mean train loss:  2.71634378e-02, mean val. rec. loss:  1.81050828e-02\n",
      "Epoch: 2241 mean train loss:  2.71572032e-02, mean val. rec. loss:  1.81009744e-02\n",
      "Epoch: 2242 mean train loss:  2.71509909e-02, mean val. rec. loss:  1.80968694e-02\n",
      "Epoch: 2243 mean train loss:  2.71447787e-02, mean val. rec. loss:  1.80927825e-02\n",
      "Epoch: 2244 mean train loss:  2.71385776e-02, mean val. rec. loss:  1.80886889e-02\n",
      "Epoch: 2245 mean train loss:  2.71323709e-02, mean val. rec. loss:  1.80845918e-02\n",
      "Epoch: 2246 mean train loss:  2.71261679e-02, mean val. rec. loss:  1.80804970e-02\n",
      "Epoch: 2247 mean train loss:  2.71199724e-02, mean val. rec. loss:  1.80764147e-02\n",
      "Epoch: 2248 mean train loss:  2.71137955e-02, mean val. rec. loss:  1.80723165e-02\n",
      "Epoch: 2249 mean train loss:  2.71076019e-02, mean val. rec. loss:  1.80682093e-02\n",
      "Epoch: 2250 mean train loss:  2.71014325e-02, mean val. rec. loss:  1.80641133e-02\n",
      "Epoch: 2251 mean train loss:  2.70952612e-02, mean val. rec. loss:  1.80600174e-02\n",
      "Epoch: 2252 mean train loss:  2.70890954e-02, mean val. rec. loss:  1.80559215e-02\n",
      "Epoch: 2253 mean train loss:  2.70829521e-02, mean val. rec. loss:  1.80518278e-02\n",
      "Epoch: 2254 mean train loss:  2.70767975e-02, mean val. rec. loss:  1.80477308e-02\n",
      "Epoch: 2255 mean train loss:  2.70706560e-02, mean val. rec. loss:  1.80436337e-02\n",
      "Epoch: 2256 mean train loss:  2.70645108e-02, mean val. rec. loss:  1.80395491e-02\n",
      "Epoch: 2257 mean train loss:  2.70583712e-02, mean val. rec. loss:  1.80354668e-02\n",
      "Epoch: 2258 mean train loss:  2.70522520e-02, mean val. rec. loss:  1.80313732e-02\n",
      "Epoch: 2259 mean train loss:  2.70461310e-02, mean val. rec. loss:  1.80272954e-02\n",
      "Epoch: 2260 mean train loss:  2.70400062e-02, mean val. rec. loss:  1.80232006e-02\n",
      "Epoch: 2261 mean train loss:  2.70339020e-02, mean val. rec. loss:  1.80191194e-02\n",
      "Epoch: 2262 mean train loss:  2.70277810e-02, mean val. rec. loss:  1.80150348e-02\n",
      "Epoch: 2263 mean train loss:  2.70216823e-02, mean val. rec. loss:  1.80109503e-02\n",
      "Epoch: 2264 mean train loss:  2.70155873e-02, mean val. rec. loss:  1.80068521e-02\n",
      "Epoch: 2265 mean train loss:  2.70094905e-02, mean val. rec. loss:  1.80027652e-02\n",
      "Epoch: 2266 mean train loss:  2.70034161e-02, mean val. rec. loss:  1.79986784e-02\n",
      "Epoch: 2267 mean train loss:  2.69973267e-02, mean val. rec. loss:  1.79945892e-02\n",
      "Epoch: 2268 mean train loss:  2.69912597e-02, mean val. rec. loss:  1.79904933e-02\n",
      "Epoch: 2269 mean train loss:  2.69851815e-02, mean val. rec. loss:  1.79864087e-02\n",
      "Epoch: 2270 mean train loss:  2.69791108e-02, mean val. rec. loss:  1.79823128e-02\n",
      "Epoch: 2271 mean train loss:  2.69730586e-02, mean val. rec. loss:  1.79782441e-02\n",
      "Epoch: 2272 mean train loss:  2.69670084e-02, mean val. rec. loss:  1.79741641e-02\n",
      "Epoch: 2273 mean train loss:  2.69609507e-02, mean val. rec. loss:  1.79700817e-02\n",
      "Epoch: 2274 mean train loss:  2.69549209e-02, mean val. rec. loss:  1.79660040e-02\n",
      "Epoch: 2275 mean train loss:  2.69488651e-02, mean val. rec. loss:  1.79619330e-02\n",
      "Epoch: 2276 mean train loss:  2.69428316e-02, mean val. rec. loss:  1.79578620e-02\n",
      "Epoch: 2277 mean train loss:  2.69368055e-02, mean val. rec. loss:  1.79537695e-02\n",
      "Epoch: 2278 mean train loss:  2.69307776e-02, mean val. rec. loss:  1.79496940e-02\n",
      "Epoch: 2279 mean train loss:  2.69247534e-02, mean val. rec. loss:  1.79456139e-02\n",
      "Epoch: 2280 mean train loss:  2.69187497e-02, mean val. rec. loss:  1.79415293e-02\n",
      "Epoch: 2281 mean train loss:  2.69127274e-02, mean val. rec. loss:  1.79374493e-02\n",
      "Epoch: 2282 mean train loss:  2.69067274e-02, mean val. rec. loss:  1.79333749e-02\n",
      "Epoch: 2283 mean train loss:  2.69007312e-02, mean val. rec. loss:  1.79292971e-02\n",
      "Epoch: 2284 mean train loss:  2.68947330e-02, mean val. rec. loss:  1.79252239e-02\n",
      "Epoch: 2285 mean train loss:  2.68887386e-02, mean val. rec. loss:  1.79211552e-02\n",
      "Epoch: 2286 mean train loss:  2.68827461e-02, mean val. rec. loss:  1.79170706e-02\n",
      "Epoch: 2287 mean train loss:  2.68767759e-02, mean val. rec. loss:  1.79129906e-02\n",
      "Epoch: 2288 mean train loss:  2.68707909e-02, mean val. rec. loss:  1.79088958e-02\n",
      "Epoch: 2289 mean train loss:  2.68648207e-02, mean val. rec. loss:  1.79048066e-02\n",
      "Epoch: 2290 mean train loss:  2.68588579e-02, mean val. rec. loss:  1.79007379e-02\n",
      "Epoch: 2291 mean train loss:  2.68528952e-02, mean val. rec. loss:  1.78966636e-02\n",
      "Epoch: 2292 mean train loss:  2.68469325e-02, mean val. rec. loss:  1.78925756e-02\n",
      "Epoch: 2293 mean train loss:  2.68409753e-02, mean val. rec. loss:  1.78885012e-02\n",
      "Epoch: 2294 mean train loss:  2.68350331e-02, mean val. rec. loss:  1.78844302e-02\n",
      "Epoch: 2295 mean train loss:  2.68290796e-02, mean val. rec. loss:  1.78803502e-02\n",
      "Epoch: 2296 mean train loss:  2.68231411e-02, mean val. rec. loss:  1.78762781e-02\n",
      "Epoch: 2297 mean train loss:  2.68172082e-02, mean val. rec. loss:  1.78722037e-02\n",
      "Epoch: 2298 mean train loss:  2.68112771e-02, mean val. rec. loss:  1.78681327e-02\n",
      "Epoch: 2299 mean train loss:  2.68053460e-02, mean val. rec. loss:  1.78640583e-02\n",
      "Epoch: 2300 mean train loss:  2.67994150e-02, mean val. rec. loss:  1.78599817e-02\n",
      "Epoch: 2301 mean train loss:  2.67935081e-02, mean val. rec. loss:  1.78558926e-02\n",
      "Epoch: 2302 mean train loss:  2.67875882e-02, mean val. rec. loss:  1.78518261e-02\n",
      "Epoch: 2303 mean train loss:  2.67816739e-02, mean val. rec. loss:  1.78477608e-02\n",
      "Epoch: 2304 mean train loss:  2.67757670e-02, mean val. rec. loss:  1.78436808e-02\n",
      "Epoch: 2305 mean train loss:  2.67698676e-02, mean val. rec. loss:  1.78395917e-02\n",
      "Epoch: 2306 mean train loss:  2.67639682e-02, mean val. rec. loss:  1.78355082e-02\n",
      "Epoch: 2307 mean train loss:  2.67580836e-02, mean val. rec. loss:  1.78314327e-02\n",
      "Epoch: 2308 mean train loss:  2.67521954e-02, mean val. rec. loss:  1.78273549e-02\n",
      "Epoch: 2309 mean train loss:  2.67463071e-02, mean val. rec. loss:  1.78232760e-02\n",
      "Epoch: 2310 mean train loss:  2.67404263e-02, mean val. rec. loss:  1.78191857e-02\n",
      "Epoch: 2311 mean train loss:  2.67345511e-02, mean val. rec. loss:  1.78151080e-02\n",
      "Epoch: 2312 mean train loss:  2.67286796e-02, mean val. rec. loss:  1.78110370e-02\n",
      "Epoch: 2313 mean train loss:  2.67228100e-02, mean val. rec. loss:  1.78069547e-02\n",
      "Epoch: 2314 mean train loss:  2.67169516e-02, mean val. rec. loss:  1.78028928e-02\n",
      "Epoch: 2315 mean train loss:  2.67110913e-02, mean val. rec. loss:  1.77988286e-02\n",
      "Epoch: 2316 mean train loss:  2.67052272e-02, mean val. rec. loss:  1.77947531e-02\n",
      "Epoch: 2317 mean train loss:  2.66993799e-02, mean val. rec. loss:  1.77906730e-02\n",
      "Epoch: 2318 mean train loss:  2.66935327e-02, mean val. rec. loss:  1.77865941e-02\n",
      "Epoch: 2319 mean train loss:  2.66876910e-02, mean val. rec. loss:  1.77825027e-02\n",
      "Epoch: 2320 mean train loss:  2.66818474e-02, mean val. rec. loss:  1.77784431e-02\n",
      "Epoch: 2321 mean train loss:  2.66760057e-02, mean val. rec. loss:  1.77743596e-02\n",
      "Epoch: 2322 mean train loss:  2.66701827e-02, mean val. rec. loss:  1.77702683e-02\n",
      "Epoch: 2323 mean train loss:  2.66643447e-02, mean val. rec. loss:  1.77661814e-02\n",
      "Epoch: 2324 mean train loss:  2.66585272e-02, mean val. rec. loss:  1.77620900e-02\n",
      "Epoch: 2325 mean train loss:  2.66527023e-02, mean val. rec. loss:  1.77580009e-02\n",
      "Epoch: 2326 mean train loss:  2.66468792e-02, mean val. rec. loss:  1.77539118e-02\n",
      "Epoch: 2327 mean train loss:  2.66410636e-02, mean val. rec. loss:  1.77498317e-02\n",
      "Epoch: 2328 mean train loss:  2.66352610e-02, mean val. rec. loss:  1.77457528e-02\n",
      "Epoch: 2329 mean train loss:  2.66294584e-02, mean val. rec. loss:  1.77416694e-02\n",
      "Epoch: 2330 mean train loss:  2.66236465e-02, mean val. rec. loss:  1.77375882e-02\n",
      "Epoch: 2331 mean train loss:  2.66178514e-02, mean val. rec. loss:  1.77335013e-02\n",
      "Epoch: 2332 mean train loss:  2.66120581e-02, mean val. rec. loss:  1.77294247e-02\n",
      "Epoch: 2333 mean train loss:  2.66062723e-02, mean val. rec. loss:  1.77253378e-02\n",
      "Epoch: 2334 mean train loss:  2.66004865e-02, mean val. rec. loss:  1.77212623e-02\n",
      "Epoch: 2335 mean train loss:  2.65946950e-02, mean val. rec. loss:  1.77171834e-02\n",
      "Epoch: 2336 mean train loss:  2.65889111e-02, mean val. rec. loss:  1.77131056e-02\n",
      "Epoch: 2337 mean train loss:  2.65831290e-02, mean val. rec. loss:  1.77090097e-02\n",
      "Epoch: 2338 mean train loss:  2.65773618e-02, mean val. rec. loss:  1.77049081e-02\n",
      "Epoch: 2339 mean train loss:  2.65715927e-02, mean val. rec. loss:  1.77008303e-02\n",
      "Epoch: 2340 mean train loss:  2.65658199e-02, mean val. rec. loss:  1.76967321e-02\n",
      "Epoch: 2341 mean train loss:  2.65600527e-02, mean val. rec. loss:  1.76926430e-02\n",
      "Epoch: 2342 mean train loss:  2.65542930e-02, mean val. rec. loss:  1.76885346e-02\n",
      "Epoch: 2343 mean train loss:  2.65485351e-02, mean val. rec. loss:  1.76844444e-02\n",
      "Epoch: 2344 mean train loss:  2.65427809e-02, mean val. rec. loss:  1.76803530e-02\n",
      "Epoch: 2345 mean train loss:  2.65370360e-02, mean val. rec. loss:  1.76762571e-02\n",
      "Epoch: 2346 mean train loss:  2.65312837e-02, mean val. rec. loss:  1.76721679e-02\n",
      "Epoch: 2347 mean train loss:  2.65255426e-02, mean val. rec. loss:  1.76680788e-02\n",
      "Epoch: 2348 mean train loss:  2.65198052e-02, mean val. rec. loss:  1.76639908e-02\n",
      "Epoch: 2349 mean train loss:  2.65140659e-02, mean val. rec. loss:  1.76599074e-02\n",
      "Epoch: 2350 mean train loss:  2.65083304e-02, mean val. rec. loss:  1.76558115e-02\n",
      "Epoch: 2351 mean train loss:  2.65026004e-02, mean val. rec. loss:  1.76517201e-02\n",
      "Epoch: 2352 mean train loss:  2.64968779e-02, mean val. rec. loss:  1.76476174e-02\n",
      "Epoch: 2353 mean train loss:  2.64911461e-02, mean val. rec. loss:  1.76435305e-02\n",
      "Epoch: 2354 mean train loss:  2.64854329e-02, mean val. rec. loss:  1.76394164e-02\n",
      "Epoch: 2355 mean train loss:  2.64797141e-02, mean val. rec. loss:  1.76353262e-02\n",
      "Epoch: 2356 mean train loss:  2.64739953e-02, mean val. rec. loss:  1.76312132e-02\n",
      "Epoch: 2357 mean train loss:  2.64682821e-02, mean val. rec. loss:  1.76271139e-02\n",
      "Epoch: 2358 mean train loss:  2.64625782e-02, mean val. rec. loss:  1.76230123e-02\n",
      "Epoch: 2359 mean train loss:  2.64568669e-02, mean val. rec. loss:  1.76189119e-02\n",
      "Epoch: 2360 mean train loss:  2.64511555e-02, mean val. rec. loss:  1.76148069e-02\n",
      "Epoch: 2361 mean train loss:  2.64454665e-02, mean val. rec. loss:  1.76106996e-02\n",
      "Epoch: 2362 mean train loss:  2.64397589e-02, mean val. rec. loss:  1.76065753e-02\n",
      "Epoch: 2363 mean train loss:  2.64340681e-02, mean val. rec. loss:  1.76024715e-02\n",
      "Epoch: 2364 mean train loss:  2.64283772e-02, mean val. rec. loss:  1.75983665e-02\n",
      "Epoch: 2365 mean train loss:  2.64226882e-02, mean val. rec. loss:  1.75942592e-02\n",
      "Epoch: 2366 mean train loss:  2.64170011e-02, mean val. rec. loss:  1.75901588e-02\n",
      "Epoch: 2367 mean train loss:  2.64113121e-02, mean val. rec. loss:  1.75860560e-02\n",
      "Epoch: 2368 mean train loss:  2.64056417e-02, mean val. rec. loss:  1.75819646e-02\n",
      "Epoch: 2369 mean train loss:  2.63999583e-02, mean val. rec. loss:  1.75778642e-02\n",
      "Epoch: 2370 mean train loss:  2.63942935e-02, mean val. rec. loss:  1.75737592e-02\n",
      "Epoch: 2371 mean train loss:  2.63886232e-02, mean val. rec. loss:  1.75696429e-02\n",
      "Epoch: 2372 mean train loss:  2.63829491e-02, mean val. rec. loss:  1.75655345e-02\n",
      "Epoch: 2373 mean train loss:  2.63772824e-02, mean val. rec. loss:  1.75614011e-02\n",
      "Epoch: 2374 mean train loss:  2.63716214e-02, mean val. rec. loss:  1.75572825e-02\n",
      "Epoch: 2375 mean train loss:  2.63659566e-02, mean val. rec. loss:  1.75531616e-02\n",
      "Epoch: 2376 mean train loss:  2.63602899e-02, mean val. rec. loss:  1.75490328e-02\n",
      "Epoch: 2377 mean train loss:  2.63546438e-02, mean val. rec. loss:  1.75449176e-02\n",
      "Epoch: 2378 mean train loss:  2.63489827e-02, mean val. rec. loss:  1.75407854e-02\n",
      "Epoch: 2379 mean train loss:  2.63433328e-02, mean val. rec. loss:  1.75366872e-02\n",
      "Epoch: 2380 mean train loss:  2.63376848e-02, mean val. rec. loss:  1.75325686e-02\n",
      "Epoch: 2381 mean train loss:  2.63320405e-02, mean val. rec. loss:  1.75284500e-02\n",
      "Epoch: 2382 mean train loss:  2.63263981e-02, mean val. rec. loss:  1.75243212e-02\n",
      "Epoch: 2383 mean train loss:  2.63207538e-02, mean val. rec. loss:  1.75202083e-02\n",
      "Epoch: 2384 mean train loss:  2.63151132e-02, mean val. rec. loss:  1.75160897e-02\n",
      "Epoch: 2385 mean train loss:  2.63094894e-02, mean val. rec. loss:  1.75119688e-02\n",
      "Epoch: 2386 mean train loss:  2.63038488e-02, mean val. rec. loss:  1.75078468e-02\n",
      "Epoch: 2387 mean train loss:  2.62982231e-02, mean val. rec. loss:  1.75037112e-02\n",
      "Epoch: 2388 mean train loss:  2.62925993e-02, mean val. rec. loss:  1.74995824e-02\n",
      "Epoch: 2389 mean train loss:  2.62869681e-02, mean val. rec. loss:  1.74954422e-02\n",
      "Epoch: 2390 mean train loss:  2.62813498e-02, mean val. rec. loss:  1.74913055e-02\n",
      "Epoch: 2391 mean train loss:  2.62757316e-02, mean val. rec. loss:  1.74871710e-02\n",
      "Epoch: 2392 mean train loss:  2.62701115e-02, mean val. rec. loss:  1.74830433e-02\n",
      "Epoch: 2393 mean train loss:  2.62644914e-02, mean val. rec. loss:  1.74789089e-02\n",
      "Epoch: 2394 mean train loss:  2.62588862e-02, mean val. rec. loss:  1.74747834e-02\n",
      "Epoch: 2395 mean train loss:  2.62532605e-02, mean val. rec. loss:  1.74706512e-02\n",
      "Epoch: 2396 mean train loss:  2.62476554e-02, mean val. rec. loss:  1.74665213e-02\n",
      "Epoch: 2397 mean train loss:  2.62420520e-02, mean val. rec. loss:  1.74623925e-02\n",
      "Epoch: 2398 mean train loss:  2.62364468e-02, mean val. rec. loss:  1.74582489e-02\n",
      "Epoch: 2399 mean train loss:  2.62308454e-02, mean val. rec. loss:  1.74541076e-02\n",
      "Epoch: 2400 mean train loss:  2.62252439e-02, mean val. rec. loss:  1.74499698e-02\n",
      "Epoch: 2401 mean train loss:  2.62196461e-02, mean val. rec. loss:  1.74458103e-02\n",
      "Epoch: 2402 mean train loss:  2.62140521e-02, mean val. rec. loss:  1.74416577e-02\n",
      "Epoch: 2403 mean train loss:  2.62084525e-02, mean val. rec. loss:  1.74374881e-02\n",
      "Epoch: 2404 mean train loss:  2.62028659e-02, mean val. rec. loss:  1.74333411e-02\n",
      "Epoch: 2405 mean train loss:  2.61972812e-02, mean val. rec. loss:  1.74292021e-02\n",
      "Epoch: 2406 mean train loss:  2.61917003e-02, mean val. rec. loss:  1.74250608e-02\n",
      "Epoch: 2407 mean train loss:  2.61861137e-02, mean val. rec. loss:  1.74209264e-02\n",
      "Epoch: 2408 mean train loss:  2.61805271e-02, mean val. rec. loss:  1.74167919e-02\n",
      "Epoch: 2409 mean train loss:  2.61749443e-02, mean val. rec. loss:  1.74126438e-02\n",
      "Epoch: 2410 mean train loss:  2.61693596e-02, mean val. rec. loss:  1.74084934e-02\n",
      "Epoch: 2411 mean train loss:  2.61637879e-02, mean val. rec. loss:  1.74043454e-02\n",
      "Epoch: 2412 mean train loss:  2.61582069e-02, mean val. rec. loss:  1.74001859e-02\n",
      "Epoch: 2413 mean train loss:  2.61526427e-02, mean val. rec. loss:  1.73960152e-02\n",
      "Epoch: 2414 mean train loss:  2.61470766e-02, mean val. rec. loss:  1.73918376e-02\n",
      "Epoch: 2415 mean train loss:  2.61415086e-02, mean val. rec. loss:  1.73876736e-02\n",
      "Epoch: 2416 mean train loss:  2.61359407e-02, mean val. rec. loss:  1.73835119e-02\n",
      "Epoch: 2417 mean train loss:  2.61303746e-02, mean val. rec. loss:  1.73793344e-02\n",
      "Epoch: 2418 mean train loss:  2.61248048e-02, mean val. rec. loss:  1.73751636e-02\n",
      "Epoch: 2419 mean train loss:  2.61192368e-02, mean val. rec. loss:  1.73709928e-02\n",
      "Epoch: 2420 mean train loss:  2.61136838e-02, mean val. rec. loss:  1.73668402e-02\n",
      "Epoch: 2421 mean train loss:  2.61081251e-02, mean val. rec. loss:  1.73626785e-02\n",
      "Epoch: 2422 mean train loss:  2.61025702e-02, mean val. rec. loss:  1.73585191e-02\n",
      "Epoch: 2423 mean train loss:  2.60970172e-02, mean val. rec. loss:  1.73543642e-02\n",
      "Epoch: 2424 mean train loss:  2.60914660e-02, mean val. rec. loss:  1.73501878e-02\n",
      "Epoch: 2425 mean train loss:  2.60859185e-02, mean val. rec. loss:  1.73460102e-02\n",
      "Epoch: 2426 mean train loss:  2.60803655e-02, mean val. rec. loss:  1.73418326e-02\n",
      "Epoch: 2427 mean train loss:  2.60748161e-02, mean val. rec. loss:  1.73376539e-02\n",
      "Epoch: 2428 mean train loss:  2.60692724e-02, mean val. rec. loss:  1.73334786e-02\n",
      "Epoch: 2429 mean train loss:  2.60637305e-02, mean val. rec. loss:  1.73293079e-02\n",
      "Epoch: 2430 mean train loss:  2.60581868e-02, mean val. rec. loss:  1.73251223e-02\n",
      "Epoch: 2431 mean train loss:  2.60526449e-02, mean val. rec. loss:  1.73209278e-02\n",
      "Epoch: 2432 mean train loss:  2.60471123e-02, mean val. rec. loss:  1.73167457e-02\n",
      "Epoch: 2433 mean train loss:  2.60415779e-02, mean val. rec. loss:  1.73125545e-02\n",
      "Epoch: 2434 mean train loss:  2.60360435e-02, mean val. rec. loss:  1.73083667e-02\n",
      "Epoch: 2435 mean train loss:  2.60305072e-02, mean val. rec. loss:  1.73041857e-02\n",
      "Epoch: 2436 mean train loss:  2.60249709e-02, mean val. rec. loss:  1.72999923e-02\n",
      "Epoch: 2437 mean train loss:  2.60194383e-02, mean val. rec. loss:  1.72957954e-02\n",
      "Epoch: 2438 mean train loss:  2.60139151e-02, mean val. rec. loss:  1.72915986e-02\n",
      "Epoch: 2439 mean train loss:  2.60083806e-02, mean val. rec. loss:  1.72874052e-02\n",
      "Epoch: 2440 mean train loss:  2.60028648e-02, mean val. rec. loss:  1.72832117e-02\n",
      "Epoch: 2441 mean train loss:  2.59973397e-02, mean val. rec. loss:  1.72790273e-02\n",
      "Epoch: 2442 mean train loss:  2.59918220e-02, mean val. rec. loss:  1.72748339e-02\n",
      "Epoch: 2443 mean train loss:  2.59863006e-02, mean val. rec. loss:  1.72706495e-02\n",
      "Epoch: 2444 mean train loss:  2.59807811e-02, mean val. rec. loss:  1.72664527e-02\n",
      "Epoch: 2445 mean train loss:  2.59752597e-02, mean val. rec. loss:  1.72622558e-02\n",
      "Epoch: 2446 mean train loss:  2.59697365e-02, mean val. rec. loss:  1.72580646e-02\n",
      "Epoch: 2447 mean train loss:  2.59642188e-02, mean val. rec. loss:  1.72538406e-02\n",
      "Epoch: 2448 mean train loss:  2.59587123e-02, mean val. rec. loss:  1.72496290e-02\n",
      "Epoch: 2449 mean train loss:  2.59531928e-02, mean val. rec. loss:  1.72454140e-02\n",
      "Epoch: 2450 mean train loss:  2.59476881e-02, mean val. rec. loss:  1.72411820e-02\n",
      "Epoch: 2451 mean train loss:  2.59421798e-02, mean val. rec. loss:  1.72369568e-02\n",
      "Epoch: 2452 mean train loss:  2.59366751e-02, mean val. rec. loss:  1.72327361e-02\n",
      "Epoch: 2453 mean train loss:  2.59311686e-02, mean val. rec. loss:  1.72285121e-02\n",
      "Epoch: 2454 mean train loss:  2.59256640e-02, mean val. rec. loss:  1.72242994e-02\n",
      "Epoch: 2455 mean train loss:  2.59201575e-02, mean val. rec. loss:  1.72200878e-02\n",
      "Epoch: 2456 mean train loss:  2.59146510e-02, mean val. rec. loss:  1.72158728e-02\n",
      "Epoch: 2457 mean train loss:  2.59091557e-02, mean val. rec. loss:  1.72116657e-02\n",
      "Epoch: 2458 mean train loss:  2.59036492e-02, mean val. rec. loss:  1.72074598e-02\n",
      "Epoch: 2459 mean train loss:  2.58981539e-02, mean val. rec. loss:  1.72032391e-02\n",
      "Epoch: 2460 mean train loss:  2.58926604e-02, mean val. rec. loss:  1.71990117e-02\n",
      "Epoch: 2461 mean train loss:  2.58871595e-02, mean val. rec. loss:  1.71947910e-02\n",
      "Epoch: 2462 mean train loss:  2.58816586e-02, mean val. rec. loss:  1.71905488e-02\n",
      "Epoch: 2463 mean train loss:  2.58761614e-02, mean val. rec. loss:  1.71863281e-02\n",
      "Epoch: 2464 mean train loss:  2.58706698e-02, mean val. rec. loss:  1.71820769e-02\n",
      "Epoch: 2465 mean train loss:  2.58651856e-02, mean val. rec. loss:  1.71778381e-02\n",
      "Epoch: 2466 mean train loss:  2.58596922e-02, mean val. rec. loss:  1.71736015e-02\n",
      "Epoch: 2467 mean train loss:  2.58541987e-02, mean val. rec. loss:  1.71693457e-02\n",
      "Epoch: 2468 mean train loss:  2.58487127e-02, mean val. rec. loss:  1.71651012e-02\n",
      "Epoch: 2469 mean train loss:  2.58432304e-02, mean val. rec. loss:  1.71608579e-02\n",
      "Epoch: 2470 mean train loss:  2.58377332e-02, mean val. rec. loss:  1.71566146e-02\n",
      "Epoch: 2471 mean train loss:  2.58322435e-02, mean val. rec. loss:  1.71523916e-02\n",
      "Epoch: 2472 mean train loss:  2.58267668e-02, mean val. rec. loss:  1.71481574e-02\n",
      "Epoch: 2473 mean train loss:  2.58212864e-02, mean val. rec. loss:  1.71439004e-02\n",
      "Epoch: 2474 mean train loss:  2.58158003e-02, mean val. rec. loss:  1.71396412e-02\n",
      "Epoch: 2475 mean train loss:  2.58103255e-02, mean val. rec. loss:  1.71353956e-02\n",
      "Epoch: 2476 mean train loss:  2.58048469e-02, mean val. rec. loss:  1.71311522e-02\n",
      "Epoch: 2477 mean train loss:  2.57993591e-02, mean val. rec. loss:  1.71268976e-02\n",
      "Epoch: 2478 mean train loss:  2.57938935e-02, mean val. rec. loss:  1.71226259e-02\n",
      "Epoch: 2479 mean train loss:  2.57884075e-02, mean val. rec. loss:  1.71183587e-02\n",
      "Epoch: 2480 mean train loss:  2.57829364e-02, mean val. rec. loss:  1.71140904e-02\n",
      "Epoch: 2481 mean train loss:  2.57774616e-02, mean val. rec. loss:  1.71098256e-02\n",
      "Epoch: 2482 mean train loss:  2.57719904e-02, mean val. rec. loss:  1.71055675e-02\n",
      "Epoch: 2483 mean train loss:  2.57665193e-02, mean val. rec. loss:  1.71013151e-02\n",
      "Epoch: 2484 mean train loss:  2.57610464e-02, mean val. rec. loss:  1.70970400e-02\n",
      "Epoch: 2485 mean train loss:  2.57555715e-02, mean val. rec. loss:  1.70927671e-02\n",
      "Epoch: 2486 mean train loss:  2.57500967e-02, mean val. rec. loss:  1.70884989e-02\n",
      "Epoch: 2487 mean train loss:  2.57446367e-02, mean val. rec. loss:  1.70842260e-02\n",
      "Epoch: 2488 mean train loss:  2.57391637e-02, mean val. rec. loss:  1.70799509e-02\n",
      "Epoch: 2489 mean train loss:  2.57337001e-02, mean val. rec. loss:  1.70756827e-02\n",
      "Epoch: 2490 mean train loss:  2.57282290e-02, mean val. rec. loss:  1.70714121e-02\n",
      "Epoch: 2491 mean train loss:  2.57227634e-02, mean val. rec. loss:  1.70671347e-02\n",
      "Epoch: 2492 mean train loss:  2.57172979e-02, mean val. rec. loss:  1.70628574e-02\n",
      "Epoch: 2493 mean train loss:  2.57118342e-02, mean val. rec. loss:  1.70585823e-02\n",
      "Epoch: 2494 mean train loss:  2.57063669e-02, mean val. rec. loss:  1.70543027e-02\n",
      "Epoch: 2495 mean train loss:  2.57009032e-02, mean val. rec. loss:  1.70500162e-02\n",
      "Epoch: 2496 mean train loss:  2.56954414e-02, mean val. rec. loss:  1.70457173e-02\n",
      "Epoch: 2497 mean train loss:  2.56899870e-02, mean val. rec. loss:  1.70414173e-02\n",
      "Epoch: 2498 mean train loss:  2.56845196e-02, mean val. rec. loss:  1.70371184e-02\n",
      "Epoch: 2499 mean train loss:  2.56790709e-02, mean val. rec. loss:  1.70328161e-02\n",
      "Epoch: 2500 mean train loss:  2.56735997e-02, mean val. rec. loss:  1.70285115e-02\n",
      "Epoch: 2501 mean train loss:  2.56681473e-02, mean val. rec. loss:  1.70242069e-02\n",
      "Epoch: 2502 mean train loss:  2.56626929e-02, mean val. rec. loss:  1.70199069e-02\n",
      "Epoch: 2503 mean train loss:  2.56572348e-02, mean val. rec. loss:  1.70156091e-02\n",
      "Epoch: 2504 mean train loss:  2.56517823e-02, mean val. rec. loss:  1.70113113e-02\n",
      "Epoch: 2505 mean train loss:  2.56463261e-02, mean val. rec. loss:  1.70070113e-02\n",
      "Epoch: 2506 mean train loss:  2.56408662e-02, mean val. rec. loss:  1.70027181e-02\n",
      "Epoch: 2507 mean train loss:  2.56354081e-02, mean val. rec. loss:  1.69984203e-02\n",
      "Epoch: 2508 mean train loss:  2.56299649e-02, mean val. rec. loss:  1.69941021e-02\n",
      "Epoch: 2509 mean train loss:  2.56245031e-02, mean val. rec. loss:  1.69897749e-02\n",
      "Epoch: 2510 mean train loss:  2.56190525e-02, mean val. rec. loss:  1.69854691e-02\n",
      "Epoch: 2511 mean train loss:  2.56135981e-02, mean val. rec. loss:  1.69811408e-02\n",
      "Epoch: 2512 mean train loss:  2.56081475e-02, mean val. rec. loss:  1.69768180e-02\n",
      "Epoch: 2513 mean train loss:  2.56026987e-02, mean val. rec. loss:  1.69725033e-02\n",
      "Epoch: 2514 mean train loss:  2.55972388e-02, mean val. rec. loss:  1.69681749e-02\n",
      "Epoch: 2515 mean train loss:  2.55917900e-02, mean val. rec. loss:  1.69638408e-02\n",
      "Epoch: 2516 mean train loss:  2.55863505e-02, mean val. rec. loss:  1.69595204e-02\n",
      "Epoch: 2517 mean train loss:  2.55808999e-02, mean val. rec. loss:  1.69551976e-02\n",
      "Epoch: 2518 mean train loss:  2.55754437e-02, mean val. rec. loss:  1.69508727e-02\n",
      "Epoch: 2519 mean train loss:  2.55699893e-02, mean val. rec. loss:  1.69465522e-02\n",
      "Epoch: 2520 mean train loss:  2.55645555e-02, mean val. rec. loss:  1.69422147e-02\n",
      "Epoch: 2521 mean train loss:  2.55590992e-02, mean val. rec. loss:  1.69378875e-02\n",
      "Epoch: 2522 mean train loss:  2.55536561e-02, mean val. rec. loss:  1.69335319e-02\n",
      "Epoch: 2523 mean train loss:  2.55482166e-02, mean val. rec. loss:  1.69291922e-02\n",
      "Epoch: 2524 mean train loss:  2.55427585e-02, mean val. rec. loss:  1.69248388e-02\n",
      "Epoch: 2525 mean train loss:  2.55373135e-02, mean val. rec. loss:  1.69204889e-02\n",
      "Epoch: 2526 mean train loss:  2.55318703e-02, mean val. rec. loss:  1.69161458e-02\n",
      "Epoch: 2527 mean train loss:  2.55264271e-02, mean val. rec. loss:  1.69118049e-02\n",
      "Epoch: 2528 mean train loss:  2.55209839e-02, mean val. rec. loss:  1.69074618e-02\n",
      "Epoch: 2529 mean train loss:  2.55155482e-02, mean val. rec. loss:  1.69031050e-02\n",
      "Epoch: 2530 mean train loss:  2.55101031e-02, mean val. rec. loss:  1.68987494e-02\n",
      "Epoch: 2531 mean train loss:  2.55046544e-02, mean val. rec. loss:  1.68943847e-02\n",
      "Epoch: 2532 mean train loss:  2.54992168e-02, mean val. rec. loss:  1.68900269e-02\n",
      "Epoch: 2533 mean train loss:  2.54937699e-02, mean val. rec. loss:  1.68856735e-02\n",
      "Epoch: 2534 mean train loss:  2.54883341e-02, mean val. rec. loss:  1.68812987e-02\n",
      "Epoch: 2535 mean train loss:  2.54828928e-02, mean val. rec. loss:  1.68769499e-02\n",
      "Epoch: 2536 mean train loss:  2.54774496e-02, mean val. rec. loss:  1.68725761e-02\n",
      "Epoch: 2537 mean train loss:  2.54720027e-02, mean val. rec. loss:  1.68682137e-02\n",
      "Epoch: 2538 mean train loss:  2.54665614e-02, mean val. rec. loss:  1.68638626e-02\n",
      "Epoch: 2539 mean train loss:  2.54611257e-02, mean val. rec. loss:  1.68594753e-02\n",
      "Epoch: 2540 mean train loss:  2.54556899e-02, mean val. rec. loss:  1.68551038e-02\n",
      "Epoch: 2541 mean train loss:  2.54502468e-02, mean val. rec. loss:  1.68507244e-02\n",
      "Epoch: 2542 mean train loss:  2.54448036e-02, mean val. rec. loss:  1.68463404e-02\n",
      "Epoch: 2543 mean train loss:  2.54393716e-02, mean val. rec. loss:  1.68419587e-02\n",
      "Epoch: 2544 mean train loss:  2.54339247e-02, mean val. rec. loss:  1.68375657e-02\n",
      "Epoch: 2545 mean train loss:  2.54284871e-02, mean val. rec. loss:  1.68331852e-02\n",
      "Epoch: 2546 mean train loss:  2.54230364e-02, mean val. rec. loss:  1.68288125e-02\n",
      "Epoch: 2547 mean train loss:  2.54176044e-02, mean val. rec. loss:  1.68244138e-02\n",
      "Epoch: 2548 mean train loss:  2.54121687e-02, mean val. rec. loss:  1.68200356e-02\n",
      "Epoch: 2549 mean train loss:  2.54067329e-02, mean val. rec. loss:  1.68156573e-02\n",
      "Epoch: 2550 mean train loss:  2.54012954e-02, mean val. rec. loss:  1.68112699e-02\n",
      "Epoch: 2551 mean train loss:  2.53958522e-02, mean val. rec. loss:  1.68068622e-02\n",
      "Epoch: 2552 mean train loss:  2.53904108e-02, mean val. rec. loss:  1.68024703e-02\n",
      "Epoch: 2553 mean train loss:  2.53849714e-02, mean val. rec. loss:  1.67980625e-02\n",
      "Epoch: 2554 mean train loss:  2.53795450e-02, mean val. rec. loss:  1.67936525e-02\n",
      "Epoch: 2555 mean train loss:  2.53741036e-02, mean val. rec. loss:  1.67892334e-02\n",
      "Epoch: 2556 mean train loss:  2.53686735e-02, mean val. rec. loss:  1.67848290e-02\n",
      "Epoch: 2557 mean train loss:  2.53632285e-02, mean val. rec. loss:  1.67804133e-02\n",
      "Epoch: 2558 mean train loss:  2.53577983e-02, mean val. rec. loss:  1.67759976e-02\n",
      "Epoch: 2559 mean train loss:  2.53523588e-02, mean val. rec. loss:  1.67715853e-02\n",
      "Epoch: 2560 mean train loss:  2.53469157e-02, mean val. rec. loss:  1.67671809e-02\n",
      "Epoch: 2561 mean train loss:  2.53414837e-02, mean val. rec. loss:  1.67627618e-02\n",
      "Epoch: 2562 mean train loss:  2.53360461e-02, mean val. rec. loss:  1.67583597e-02\n",
      "Epoch: 2563 mean train loss:  2.53306066e-02, mean val. rec. loss:  1.67539395e-02\n",
      "Epoch: 2564 mean train loss:  2.53251690e-02, mean val. rec. loss:  1.67495079e-02\n",
      "Epoch: 2565 mean train loss:  2.53197370e-02, mean val. rec. loss:  1.67450808e-02\n",
      "Epoch: 2566 mean train loss:  2.53143013e-02, mean val. rec. loss:  1.67406583e-02\n",
      "Epoch: 2567 mean train loss:  2.53088618e-02, mean val. rec. loss:  1.67362324e-02\n",
      "Epoch: 2568 mean train loss:  2.53034223e-02, mean val. rec. loss:  1.67317884e-02\n",
      "Epoch: 2569 mean train loss:  2.52979885e-02, mean val. rec. loss:  1.67273568e-02\n",
      "Epoch: 2570 mean train loss:  2.52925565e-02, mean val. rec. loss:  1.67229093e-02\n",
      "Epoch: 2571 mean train loss:  2.52871096e-02, mean val. rec. loss:  1.67184721e-02\n",
      "Epoch: 2572 mean train loss:  2.52816757e-02, mean val. rec. loss:  1.67140280e-02\n",
      "Epoch: 2573 mean train loss:  2.52762399e-02, mean val. rec. loss:  1.67095851e-02\n",
      "Epoch: 2574 mean train loss:  2.52708061e-02, mean val. rec. loss:  1.67051433e-02\n",
      "Epoch: 2575 mean train loss:  2.52653666e-02, mean val. rec. loss:  1.67006993e-02\n",
      "Epoch: 2576 mean train loss:  2.52599309e-02, mean val. rec. loss:  1.66962632e-02\n",
      "Epoch: 2577 mean train loss:  2.52544914e-02, mean val. rec. loss:  1.66918202e-02\n",
      "Epoch: 2578 mean train loss:  2.52490501e-02, mean val. rec. loss:  1.66873728e-02\n",
      "Epoch: 2579 mean train loss:  2.52436051e-02, mean val. rec. loss:  1.66829197e-02\n",
      "Epoch: 2580 mean train loss:  2.52381786e-02, mean val. rec. loss:  1.66784495e-02\n",
      "Epoch: 2581 mean train loss:  2.52327355e-02, mean val. rec. loss:  1.66739816e-02\n",
      "Epoch: 2582 mean train loss:  2.52273053e-02, mean val. rec. loss:  1.66695240e-02\n",
      "Epoch: 2583 mean train loss:  2.52218603e-02, mean val. rec. loss:  1.66650584e-02\n",
      "Epoch: 2584 mean train loss:  2.52164264e-02, mean val. rec. loss:  1.66605962e-02\n",
      "Epoch: 2585 mean train loss:  2.52109944e-02, mean val. rec. loss:  1.66561158e-02\n",
      "Epoch: 2586 mean train loss:  2.52055568e-02, mean val. rec. loss:  1.66516457e-02\n",
      "Epoch: 2587 mean train loss:  2.52001155e-02, mean val. rec. loss:  1.66471710e-02\n",
      "Epoch: 2588 mean train loss:  2.51946779e-02, mean val. rec. loss:  1.66426975e-02\n",
      "Epoch: 2589 mean train loss:  2.51892403e-02, mean val. rec. loss:  1.66382228e-02\n",
      "Epoch: 2590 mean train loss:  2.51838064e-02, mean val. rec. loss:  1.66337402e-02\n",
      "Epoch: 2591 mean train loss:  2.51783651e-02, mean val. rec. loss:  1.66292576e-02\n",
      "Epoch: 2592 mean train loss:  2.51729200e-02, mean val. rec. loss:  1.66247795e-02\n",
      "Epoch: 2593 mean train loss:  2.51674824e-02, mean val. rec. loss:  1.66202845e-02\n",
      "Epoch: 2594 mean train loss:  2.51620448e-02, mean val. rec. loss:  1.66158075e-02\n",
      "Epoch: 2595 mean train loss:  2.51565961e-02, mean val. rec. loss:  1.66113294e-02\n",
      "Epoch: 2596 mean train loss:  2.51511641e-02, mean val. rec. loss:  1.66068457e-02\n",
      "Epoch: 2597 mean train loss:  2.51457209e-02, mean val. rec. loss:  1.66023495e-02\n",
      "Epoch: 2598 mean train loss:  2.51402796e-02, mean val. rec. loss:  1.65978635e-02\n",
      "Epoch: 2599 mean train loss:  2.51348438e-02, mean val. rec. loss:  1.65933695e-02\n",
      "Epoch: 2600 mean train loss:  2.51294044e-02, mean val. rec. loss:  1.65888620e-02\n",
      "Epoch: 2601 mean train loss:  2.51239649e-02, mean val. rec. loss:  1.65843453e-02\n",
      "Epoch: 2602 mean train loss:  2.51185236e-02, mean val. rec. loss:  1.65798310e-02\n",
      "Epoch: 2603 mean train loss:  2.51130804e-02, mean val. rec. loss:  1.65753223e-02\n",
      "Epoch: 2604 mean train loss:  2.51076354e-02, mean val. rec. loss:  1.65708136e-02\n",
      "Epoch: 2605 mean train loss:  2.51021903e-02, mean val. rec. loss:  1.65663026e-02\n",
      "Epoch: 2606 mean train loss:  2.50967527e-02, mean val. rec. loss:  1.65618041e-02\n",
      "Epoch: 2607 mean train loss:  2.50913114e-02, mean val. rec. loss:  1.65572989e-02\n",
      "Epoch: 2608 mean train loss:  2.50858589e-02, mean val. rec. loss:  1.65527811e-02\n",
      "Epoch: 2609 mean train loss:  2.50804250e-02, mean val. rec. loss:  1.65482701e-02\n",
      "Epoch: 2610 mean train loss:  2.50749744e-02, mean val. rec. loss:  1.65437478e-02\n",
      "Epoch: 2611 mean train loss:  2.50695368e-02, mean val. rec. loss:  1.65392097e-02\n",
      "Epoch: 2612 mean train loss:  2.50640973e-02, mean val. rec. loss:  1.65346726e-02\n",
      "Epoch: 2613 mean train loss:  2.50586393e-02, mean val. rec. loss:  1.65301310e-02\n",
      "Epoch: 2614 mean train loss:  2.50531961e-02, mean val. rec. loss:  1.65255951e-02\n",
      "Epoch: 2615 mean train loss:  2.50477566e-02, mean val. rec. loss:  1.65210694e-02\n",
      "Epoch: 2616 mean train loss:  2.50423116e-02, mean val. rec. loss:  1.65165460e-02\n",
      "Epoch: 2617 mean train loss:  2.50368609e-02, mean val. rec. loss:  1.65120101e-02\n",
      "Epoch: 2618 mean train loss:  2.50314159e-02, mean val. rec. loss:  1.65074878e-02\n",
      "Epoch: 2619 mean train loss:  2.50259690e-02, mean val. rec. loss:  1.65029587e-02\n",
      "Epoch: 2620 mean train loss:  2.50205351e-02, mean val. rec. loss:  1.64984183e-02\n",
      "Epoch: 2621 mean train loss:  2.50150770e-02, mean val. rec. loss:  1.64938563e-02\n",
      "Epoch: 2622 mean train loss:  2.50096283e-02, mean val. rec. loss:  1.64893011e-02\n",
      "Epoch: 2623 mean train loss:  2.50041869e-02, mean val. rec. loss:  1.64847493e-02\n",
      "Epoch: 2624 mean train loss:  2.49987326e-02, mean val. rec. loss:  1.64801896e-02\n",
      "Epoch: 2625 mean train loss:  2.49932838e-02, mean val. rec. loss:  1.64756321e-02\n",
      "Epoch: 2626 mean train loss:  2.49878444e-02, mean val. rec. loss:  1.64710645e-02\n",
      "Epoch: 2627 mean train loss:  2.49823844e-02, mean val. rec. loss:  1.64665172e-02\n",
      "Epoch: 2628 mean train loss:  2.49769338e-02, mean val. rec. loss:  1.64619700e-02\n",
      "Epoch: 2629 mean train loss:  2.49714869e-02, mean val. rec. loss:  1.64574046e-02\n",
      "Epoch: 2630 mean train loss:  2.49660418e-02, mean val. rec. loss:  1.64528517e-02\n",
      "Epoch: 2631 mean train loss:  2.49605912e-02, mean val. rec. loss:  1.64482954e-02\n",
      "Epoch: 2632 mean train loss:  2.49551387e-02, mean val. rec. loss:  1.64437232e-02\n",
      "Epoch: 2633 mean train loss:  2.49496862e-02, mean val. rec. loss:  1.64391362e-02\n",
      "Epoch: 2634 mean train loss:  2.49442300e-02, mean val. rec. loss:  1.64345652e-02\n",
      "Epoch: 2635 mean train loss:  2.49387756e-02, mean val. rec. loss:  1.64299805e-02\n",
      "Epoch: 2636 mean train loss:  2.49333306e-02, mean val. rec. loss:  1.64253924e-02\n",
      "Epoch: 2637 mean train loss:  2.49278669e-02, mean val. rec. loss:  1.64208089e-02\n",
      "Epoch: 2638 mean train loss:  2.49224256e-02, mean val. rec. loss:  1.64162480e-02\n",
      "Epoch: 2639 mean train loss:  2.49169619e-02, mean val. rec. loss:  1.64116634e-02\n",
      "Epoch: 2640 mean train loss:  2.49115132e-02, mean val. rec. loss:  1.64070923e-02\n",
      "Epoch: 2641 mean train loss:  2.49060458e-02, mean val. rec. loss:  1.64025042e-02\n",
      "Epoch: 2642 mean train loss:  2.49005952e-02, mean val. rec. loss:  1.63979184e-02\n",
      "Epoch: 2643 mean train loss:  2.48951408e-02, mean val. rec. loss:  1.63933145e-02\n",
      "Epoch: 2644 mean train loss:  2.48896864e-02, mean val. rec. loss:  1.63887060e-02\n",
      "Epoch: 2645 mean train loss:  2.48842284e-02, mean val. rec. loss:  1.63841066e-02\n",
      "Epoch: 2646 mean train loss:  2.48787721e-02, mean val. rec. loss:  1.63794981e-02\n",
      "Epoch: 2647 mean train loss:  2.48733159e-02, mean val. rec. loss:  1.63748874e-02\n",
      "Epoch: 2648 mean train loss:  2.48678541e-02, mean val. rec. loss:  1.63702710e-02\n",
      "Epoch: 2649 mean train loss:  2.48623979e-02, mean val. rec. loss:  1.63656579e-02\n",
      "Epoch: 2650 mean train loss:  2.48569342e-02, mean val. rec. loss:  1.63610642e-02\n",
      "Epoch: 2651 mean train loss:  2.48514687e-02, mean val. rec. loss:  1.63564569e-02\n",
      "Epoch: 2652 mean train loss:  2.48460199e-02, mean val. rec. loss:  1.63518563e-02\n",
      "Epoch: 2653 mean train loss:  2.48405563e-02, mean val. rec. loss:  1.63472478e-02\n",
      "Epoch: 2654 mean train loss:  2.48350982e-02, mean val. rec. loss:  1.63426382e-02\n",
      "Epoch: 2655 mean train loss:  2.48296327e-02, mean val. rec. loss:  1.63380195e-02\n",
      "Epoch: 2656 mean train loss:  2.48241746e-02, mean val. rec. loss:  1.63334065e-02\n",
      "Epoch: 2657 mean train loss:  2.48187035e-02, mean val. rec. loss:  1.63287686e-02\n",
      "Epoch: 2658 mean train loss:  2.48132417e-02, mean val. rec. loss:  1.63241374e-02\n",
      "Epoch: 2659 mean train loss:  2.48077817e-02, mean val. rec. loss:  1.63194994e-02\n",
      "Epoch: 2660 mean train loss:  2.48023199e-02, mean val. rec. loss:  1.63148592e-02\n",
      "Epoch: 2661 mean train loss:  2.47968581e-02, mean val. rec. loss:  1.63102314e-02\n",
      "Epoch: 2662 mean train loss:  2.47913907e-02, mean val. rec. loss:  1.63055946e-02\n",
      "Epoch: 2663 mean train loss:  2.47859233e-02, mean val. rec. loss:  1.63009600e-02\n",
      "Epoch: 2664 mean train loss:  2.47804597e-02, mean val. rec. loss:  1.62963232e-02\n",
      "Epoch: 2665 mean train loss:  2.47749885e-02, mean val. rec. loss:  1.62916921e-02\n",
      "Epoch: 2666 mean train loss:  2.47695174e-02, mean val. rec. loss:  1.62870598e-02\n",
      "Epoch: 2667 mean train loss:  2.47640463e-02, mean val. rec. loss:  1.62824184e-02\n",
      "Epoch: 2668 mean train loss:  2.47585752e-02, mean val. rec. loss:  1.62777759e-02\n",
      "Epoch: 2669 mean train loss:  2.47531153e-02, mean val. rec. loss:  1.62731130e-02\n",
      "Epoch: 2670 mean train loss:  2.47476367e-02, mean val. rec. loss:  1.62684614e-02\n",
      "Epoch: 2671 mean train loss:  2.47421767e-02, mean val. rec. loss:  1.62638008e-02\n",
      "Epoch: 2672 mean train loss:  2.47366963e-02, mean val. rec. loss:  1.62591447e-02\n",
      "Epoch: 2673 mean train loss:  2.47312308e-02, mean val. rec. loss:  1.62544874e-02\n",
      "Epoch: 2674 mean train loss:  2.47257504e-02, mean val. rec. loss:  1.62498302e-02\n",
      "Epoch: 2675 mean train loss:  2.47202811e-02, mean val. rec. loss:  1.62451775e-02\n",
      "Epoch: 2676 mean train loss:  2.47148119e-02, mean val. rec. loss:  1.62405202e-02\n",
      "Epoch: 2677 mean train loss:  2.47093426e-02, mean val. rec. loss:  1.62358619e-02\n",
      "Epoch: 2678 mean train loss:  2.47038678e-02, mean val. rec. loss:  1.62311921e-02\n",
      "Epoch: 2679 mean train loss:  2.46983892e-02, mean val. rec. loss:  1.62265270e-02\n",
      "Epoch: 2680 mean train loss:  2.46929144e-02, mean val. rec. loss:  1.62218595e-02\n",
      "Epoch: 2681 mean train loss:  2.46874358e-02, mean val. rec. loss:  1.62171853e-02\n",
      "Epoch: 2682 mean train loss:  2.46819647e-02, mean val. rec. loss:  1.62124917e-02\n",
      "Epoch: 2683 mean train loss:  2.46764861e-02, mean val. rec. loss:  1.62077959e-02\n",
      "Epoch: 2684 mean train loss:  2.46710038e-02, mean val. rec. loss:  1.62031137e-02\n",
      "Epoch: 2685 mean train loss:  2.46655234e-02, mean val. rec. loss:  1.61984304e-02\n",
      "Epoch: 2686 mean train loss:  2.46600541e-02, mean val. rec. loss:  1.61937392e-02\n",
      "Epoch: 2687 mean train loss:  2.46545719e-02, mean val. rec. loss:  1.61890581e-02\n",
      "Epoch: 2688 mean train loss:  2.46490840e-02, mean val. rec. loss:  1.61843748e-02\n",
      "Epoch: 2689 mean train loss:  2.46436110e-02, mean val. rec. loss:  1.61796960e-02\n",
      "Epoch: 2690 mean train loss:  2.46381250e-02, mean val. rec. loss:  1.61749934e-02\n",
      "Epoch: 2691 mean train loss:  2.46326520e-02, mean val. rec. loss:  1.61703157e-02\n",
      "Epoch: 2692 mean train loss:  2.46271716e-02, mean val. rec. loss:  1.61656120e-02\n",
      "Epoch: 2693 mean train loss:  2.46216800e-02, mean val. rec. loss:  1.61609219e-02\n",
      "Epoch: 2694 mean train loss:  2.46161996e-02, mean val. rec. loss:  1.61562045e-02\n",
      "Epoch: 2695 mean train loss:  2.46107210e-02, mean val. rec. loss:  1.61514815e-02\n",
      "Epoch: 2696 mean train loss:  2.46052350e-02, mean val. rec. loss:  1.61467801e-02\n",
      "Epoch: 2697 mean train loss:  2.45997546e-02, mean val. rec. loss:  1.61420729e-02\n",
      "Epoch: 2698 mean train loss:  2.45942704e-02, mean val. rec. loss:  1.61373658e-02\n",
      "Epoch: 2699 mean train loss:  2.45887825e-02, mean val. rec. loss:  1.61326552e-02\n",
      "Epoch: 2700 mean train loss:  2.45832946e-02, mean val. rec. loss:  1.61279492e-02\n",
      "Epoch: 2701 mean train loss:  2.45778068e-02, mean val. rec. loss:  1.61232353e-02\n",
      "Epoch: 2702 mean train loss:  2.45723189e-02, mean val. rec. loss:  1.61185202e-02\n",
      "Epoch: 2703 mean train loss:  2.45668254e-02, mean val. rec. loss:  1.61137927e-02\n",
      "Epoch: 2704 mean train loss:  2.45613413e-02, mean val. rec. loss:  1.61090833e-02\n",
      "Epoch: 2705 mean train loss:  2.45558534e-02, mean val. rec. loss:  1.61043569e-02\n",
      "Epoch: 2706 mean train loss:  2.45503544e-02, mean val. rec. loss:  1.60996191e-02\n",
      "Epoch: 2707 mean train loss:  2.45448702e-02, mean val. rec. loss:  1.60948916e-02\n",
      "Epoch: 2708 mean train loss:  2.45393749e-02, mean val. rec. loss:  1.60901629e-02\n",
      "Epoch: 2709 mean train loss:  2.45338926e-02, mean val. rec. loss:  1.60854353e-02\n",
      "Epoch: 2710 mean train loss:  2.45283898e-02, mean val. rec. loss:  1.60807032e-02\n",
      "Epoch: 2711 mean train loss:  2.45229038e-02, mean val. rec. loss:  1.60759666e-02\n",
      "Epoch: 2712 mean train loss:  2.45174159e-02, mean val. rec. loss:  1.60712289e-02\n",
      "Epoch: 2713 mean train loss:  2.45119169e-02, mean val. rec. loss:  1.60664956e-02\n",
      "Epoch: 2714 mean train loss:  2.45064197e-02, mean val. rec. loss:  1.60617465e-02\n",
      "Epoch: 2715 mean train loss:  2.45009262e-02, mean val. rec. loss:  1.60570054e-02\n",
      "Epoch: 2716 mean train loss:  2.44954291e-02, mean val. rec. loss:  1.60522438e-02\n",
      "Epoch: 2717 mean train loss:  2.44899319e-02, mean val. rec. loss:  1.60474879e-02\n",
      "Epoch: 2718 mean train loss:  2.44844347e-02, mean val. rec. loss:  1.60427468e-02\n",
      "Epoch: 2719 mean train loss:  2.44789412e-02, mean val. rec. loss:  1.60379965e-02\n",
      "Epoch: 2720 mean train loss:  2.44734403e-02, mean val. rec. loss:  1.60332543e-02\n",
      "Epoch: 2721 mean train loss:  2.44679413e-02, mean val. rec. loss:  1.60285097e-02\n",
      "Epoch: 2722 mean train loss:  2.44624385e-02, mean val. rec. loss:  1.60237504e-02\n",
      "Epoch: 2723 mean train loss:  2.44569506e-02, mean val. rec. loss:  1.60189956e-02\n",
      "Epoch: 2724 mean train loss:  2.44514478e-02, mean val. rec. loss:  1.60142375e-02\n",
      "Epoch: 2725 mean train loss:  2.44459432e-02, mean val. rec. loss:  1.60094612e-02\n",
      "Epoch: 2726 mean train loss:  2.44404497e-02, mean val. rec. loss:  1.60047019e-02\n",
      "Epoch: 2727 mean train loss:  2.44349451e-02, mean val. rec. loss:  1.59999097e-02\n",
      "Epoch: 2728 mean train loss:  2.44294442e-02, mean val. rec. loss:  1.59951390e-02\n",
      "Epoch: 2729 mean train loss:  2.44239396e-02, mean val. rec. loss:  1.59903741e-02\n",
      "Epoch: 2730 mean train loss:  2.44184331e-02, mean val. rec. loss:  1.59855864e-02\n",
      "Epoch: 2731 mean train loss:  2.44129322e-02, mean val. rec. loss:  1.59808192e-02\n",
      "Epoch: 2732 mean train loss:  2.44074350e-02, mean val. rec. loss:  1.59760587e-02\n",
      "Epoch: 2733 mean train loss:  2.44019266e-02, mean val. rec. loss:  1.59712824e-02\n",
      "Epoch: 2734 mean train loss:  2.43964201e-02, mean val. rec. loss:  1.59665039e-02\n",
      "Epoch: 2735 mean train loss:  2.43909155e-02, mean val. rec. loss:  1.59617343e-02\n",
      "Epoch: 2736 mean train loss:  2.43854108e-02, mean val. rec. loss:  1.59569433e-02\n",
      "Epoch: 2737 mean train loss:  2.43799043e-02, mean val. rec. loss:  1.59521522e-02\n",
      "Epoch: 2738 mean train loss:  2.43743960e-02, mean val. rec. loss:  1.59473589e-02\n",
      "Epoch: 2739 mean train loss:  2.43688913e-02, mean val. rec. loss:  1.59425667e-02\n",
      "Epoch: 2740 mean train loss:  2.43633830e-02, mean val. rec. loss:  1.59377746e-02\n",
      "Epoch: 2741 mean train loss:  2.43578746e-02, mean val. rec. loss:  1.59329722e-02\n",
      "Epoch: 2742 mean train loss:  2.43523607e-02, mean val. rec. loss:  1.59281823e-02\n",
      "Epoch: 2743 mean train loss:  2.43468523e-02, mean val. rec. loss:  1.59233878e-02\n",
      "Epoch: 2744 mean train loss:  2.43413402e-02, mean val. rec. loss:  1.59185888e-02\n",
      "Epoch: 2745 mean train loss:  2.43358244e-02, mean val. rec. loss:  1.59138000e-02\n",
      "Epoch: 2746 mean train loss:  2.43303272e-02, mean val. rec. loss:  1.59089976e-02\n",
      "Epoch: 2747 mean train loss:  2.43248096e-02, mean val. rec. loss:  1.59041952e-02\n",
      "Epoch: 2748 mean train loss:  2.43192882e-02, mean val. rec. loss:  1.58993781e-02\n",
      "Epoch: 2749 mean train loss:  2.43137873e-02, mean val. rec. loss:  1.58945598e-02\n",
      "Epoch: 2750 mean train loss:  2.43082640e-02, mean val. rec. loss:  1.58897314e-02\n",
      "Epoch: 2751 mean train loss:  2.43027575e-02, mean val. rec. loss:  1.58849256e-02\n",
      "Epoch: 2752 mean train loss:  2.42972324e-02, mean val. rec. loss:  1.58801084e-02\n",
      "Epoch: 2753 mean train loss:  2.42917296e-02, mean val. rec. loss:  1.58752913e-02\n",
      "Epoch: 2754 mean train loss:  2.42862119e-02, mean val. rec. loss:  1.58704833e-02\n",
      "Epoch: 2755 mean train loss:  2.42806924e-02, mean val. rec. loss:  1.58656729e-02\n",
      "Epoch: 2756 mean train loss:  2.42751803e-02, mean val. rec. loss:  1.58608615e-02\n",
      "Epoch: 2757 mean train loss:  2.42696701e-02, mean val. rec. loss:  1.58560364e-02\n",
      "Epoch: 2758 mean train loss:  2.42641413e-02, mean val. rec. loss:  1.58512329e-02\n",
      "Epoch: 2759 mean train loss:  2.42586255e-02, mean val. rec. loss:  1.58464067e-02\n",
      "Epoch: 2760 mean train loss:  2.42531078e-02, mean val. rec. loss:  1.58415759e-02\n",
      "Epoch: 2761 mean train loss:  2.42475901e-02, mean val. rec. loss:  1.58367474e-02\n",
      "Epoch: 2762 mean train loss:  2.42420743e-02, mean val. rec. loss:  1.58319144e-02\n",
      "Epoch: 2763 mean train loss:  2.42365529e-02, mean val. rec. loss:  1.58270735e-02\n",
      "Epoch: 2764 mean train loss:  2.42310297e-02, mean val. rec. loss:  1.58222394e-02\n",
      "Epoch: 2765 mean train loss:  2.42255120e-02, mean val. rec. loss:  1.58173950e-02\n",
      "Epoch: 2766 mean train loss:  2.42199887e-02, mean val. rec. loss:  1.58125507e-02\n",
      "Epoch: 2767 mean train loss:  2.42144655e-02, mean val. rec. loss:  1.58077074e-02\n",
      "Epoch: 2768 mean train loss:  2.42089553e-02, mean val. rec. loss:  1.58028733e-02\n",
      "Epoch: 2769 mean train loss:  2.42034283e-02, mean val. rec. loss:  1.57980392e-02\n",
      "Epoch: 2770 mean train loss:  2.41979031e-02, mean val. rec. loss:  1.57932073e-02\n",
      "Epoch: 2771 mean train loss:  2.41923762e-02, mean val. rec. loss:  1.57883697e-02\n",
      "Epoch: 2772 mean train loss:  2.41868641e-02, mean val. rec. loss:  1.57835254e-02\n",
      "Epoch: 2773 mean train loss:  2.41813352e-02, mean val. rec. loss:  1.57786606e-02\n",
      "Epoch: 2774 mean train loss:  2.41758138e-02, mean val. rec. loss:  1.57738027e-02\n",
      "Epoch: 2775 mean train loss:  2.41702943e-02, mean val. rec. loss:  1.57689334e-02\n",
      "Epoch: 2776 mean train loss:  2.41647711e-02, mean val. rec. loss:  1.57640822e-02\n",
      "Epoch: 2777 mean train loss:  2.41592441e-02, mean val. rec. loss:  1.57592175e-02\n",
      "Epoch: 2778 mean train loss:  2.41537245e-02, mean val. rec. loss:  1.57543572e-02\n",
      "Epoch: 2779 mean train loss:  2.41481901e-02, mean val. rec. loss:  1.57494902e-02\n",
      "Epoch: 2780 mean train loss:  2.41426724e-02, mean val. rec. loss:  1.57446391e-02\n",
      "Epoch: 2781 mean train loss:  2.41371417e-02, mean val. rec. loss:  1.57397698e-02\n",
      "Epoch: 2782 mean train loss:  2.41316129e-02, mean val. rec. loss:  1.57349164e-02\n",
      "Epoch: 2783 mean train loss:  2.41260896e-02, mean val. rec. loss:  1.57300607e-02\n",
      "Epoch: 2784 mean train loss:  2.41205664e-02, mean val. rec. loss:  1.57251982e-02\n",
      "Epoch: 2785 mean train loss:  2.41150431e-02, mean val. rec. loss:  1.57203368e-02\n",
      "Epoch: 2786 mean train loss:  2.41095087e-02, mean val. rec. loss:  1.57154607e-02\n",
      "Epoch: 2787 mean train loss:  2.41039761e-02, mean val. rec. loss:  1.57105846e-02\n",
      "Epoch: 2788 mean train loss:  2.40984491e-02, mean val. rec. loss:  1.57057074e-02\n",
      "Epoch: 2789 mean train loss:  2.40929240e-02, mean val. rec. loss:  1.57008267e-02\n",
      "Epoch: 2790 mean train loss:  2.40873915e-02, mean val. rec. loss:  1.56959404e-02\n",
      "Epoch: 2791 mean train loss:  2.40818663e-02, mean val. rec. loss:  1.56910519e-02\n",
      "Epoch: 2792 mean train loss:  2.40763356e-02, mean val. rec. loss:  1.56861712e-02\n",
      "Epoch: 2793 mean train loss:  2.40708124e-02, mean val. rec. loss:  1.56813019e-02\n",
      "Epoch: 2794 mean train loss:  2.40652910e-02, mean val. rec. loss:  1.56764224e-02\n",
      "Epoch: 2795 mean train loss:  2.40597621e-02, mean val. rec. loss:  1.56715497e-02\n",
      "Epoch: 2796 mean train loss:  2.40542258e-02, mean val. rec. loss:  1.56666589e-02\n",
      "Epoch: 2797 mean train loss:  2.40486895e-02, mean val. rec. loss:  1.56617817e-02\n",
      "Epoch: 2798 mean train loss:  2.40431626e-02, mean val. rec. loss:  1.56569044e-02\n",
      "Epoch: 2799 mean train loss:  2.40376356e-02, mean val. rec. loss:  1.56520091e-02\n",
      "Epoch: 2800 mean train loss:  2.40320974e-02, mean val. rec. loss:  1.56471193e-02\n",
      "Epoch: 2801 mean train loss:  2.40265704e-02, mean val. rec. loss:  1.56422251e-02\n",
      "Epoch: 2802 mean train loss:  2.40210397e-02, mean val. rec. loss:  1.56373309e-02\n",
      "Epoch: 2803 mean train loss:  2.40155016e-02, mean val. rec. loss:  1.56324298e-02\n",
      "Epoch: 2804 mean train loss:  2.40099783e-02, mean val. rec. loss:  1.56275186e-02\n",
      "Epoch: 2805 mean train loss:  2.40044346e-02, mean val. rec. loss:  1.56226141e-02\n",
      "Epoch: 2806 mean train loss:  2.39989113e-02, mean val. rec. loss:  1.56177244e-02\n",
      "Epoch: 2807 mean train loss:  2.39933713e-02, mean val. rec. loss:  1.56128188e-02\n",
      "Epoch: 2808 mean train loss:  2.39878443e-02, mean val. rec. loss:  1.56079155e-02\n",
      "Epoch: 2809 mean train loss:  2.39823136e-02, mean val. rec. loss:  1.56030133e-02\n",
      "Epoch: 2810 mean train loss:  2.39767736e-02, mean val. rec. loss:  1.55981168e-02\n",
      "Epoch: 2811 mean train loss:  2.39712504e-02, mean val. rec. loss:  1.55932180e-02\n",
      "Epoch: 2812 mean train loss:  2.39657085e-02, mean val. rec. loss:  1.55883283e-02\n",
      "Epoch: 2813 mean train loss:  2.39601722e-02, mean val. rec. loss:  1.55834284e-02\n",
      "Epoch: 2814 mean train loss:  2.39546452e-02, mean val. rec. loss:  1.55785274e-02\n",
      "Epoch: 2815 mean train loss:  2.39491145e-02, mean val. rec. loss:  1.55736036e-02\n",
      "Epoch: 2816 mean train loss:  2.39435838e-02, mean val. rec. loss:  1.55686890e-02\n",
      "Epoch: 2817 mean train loss:  2.39380456e-02, mean val. rec. loss:  1.55637653e-02\n",
      "Epoch: 2818 mean train loss:  2.39325000e-02, mean val. rec. loss:  1.55588506e-02\n",
      "Epoch: 2819 mean train loss:  2.39269731e-02, mean val. rec. loss:  1.55539155e-02\n",
      "Epoch: 2820 mean train loss:  2.39214386e-02, mean val. rec. loss:  1.55490032e-02\n",
      "Epoch: 2821 mean train loss:  2.39159042e-02, mean val. rec. loss:  1.55440885e-02\n",
      "Epoch: 2822 mean train loss:  2.39103698e-02, mean val. rec. loss:  1.55391602e-02\n",
      "Epoch: 2823 mean train loss:  2.39048391e-02, mean val. rec. loss:  1.55342433e-02\n",
      "Epoch: 2824 mean train loss:  2.38993009e-02, mean val. rec. loss:  1.55293287e-02\n",
      "Epoch: 2825 mean train loss:  2.38937665e-02, mean val. rec. loss:  1.55244117e-02\n",
      "Epoch: 2826 mean train loss:  2.38882414e-02, mean val. rec. loss:  1.55194971e-02\n",
      "Epoch: 2827 mean train loss:  2.38827106e-02, mean val. rec. loss:  1.55145824e-02\n",
      "Epoch: 2828 mean train loss:  2.38771744e-02, mean val. rec. loss:  1.55096587e-02\n",
      "Epoch: 2829 mean train loss:  2.38716399e-02, mean val. rec. loss:  1.55047259e-02\n",
      "Epoch: 2830 mean train loss:  2.38660999e-02, mean val. rec. loss:  1.54997863e-02\n",
      "Epoch: 2831 mean train loss:  2.38605599e-02, mean val. rec. loss:  1.54948399e-02\n",
      "Epoch: 2832 mean train loss:  2.38550422e-02, mean val. rec. loss:  1.54899026e-02\n",
      "Epoch: 2833 mean train loss:  2.38495022e-02, mean val. rec. loss:  1.54849618e-02\n",
      "Epoch: 2834 mean train loss:  2.38439603e-02, mean val. rec. loss:  1.54800245e-02\n",
      "Epoch: 2835 mean train loss:  2.38384371e-02, mean val. rec. loss:  1.54750894e-02\n",
      "Epoch: 2836 mean train loss:  2.38328952e-02, mean val. rec. loss:  1.54701555e-02\n",
      "Epoch: 2837 mean train loss:  2.38273570e-02, mean val. rec. loss:  1.54652204e-02\n",
      "Epoch: 2838 mean train loss:  2.38218301e-02, mean val. rec. loss:  1.54602763e-02\n",
      "Epoch: 2839 mean train loss:  2.38162844e-02, mean val. rec. loss:  1.54553412e-02\n",
      "Epoch: 2840 mean train loss:  2.38107612e-02, mean val. rec. loss:  1.54504050e-02\n",
      "Epoch: 2841 mean train loss:  2.38052212e-02, mean val. rec. loss:  1.54454507e-02\n",
      "Epoch: 2842 mean train loss:  2.37996923e-02, mean val. rec. loss:  1.54405190e-02\n",
      "Epoch: 2843 mean train loss:  2.37941505e-02, mean val. rec. loss:  1.54355669e-02\n",
      "Epoch: 2844 mean train loss:  2.37886253e-02, mean val. rec. loss:  1.54306205e-02\n",
      "Epoch: 2845 mean train loss:  2.37830816e-02, mean val. rec. loss:  1.54256651e-02\n",
      "Epoch: 2846 mean train loss:  2.37775509e-02, mean val. rec. loss:  1.54207209e-02\n",
      "Epoch: 2847 mean train loss:  2.37720239e-02, mean val. rec. loss:  1.54157745e-02\n",
      "Epoch: 2848 mean train loss:  2.37664839e-02, mean val. rec. loss:  1.54108315e-02\n",
      "Epoch: 2849 mean train loss:  2.37609532e-02, mean val. rec. loss:  1.54058840e-02\n",
      "Epoch: 2850 mean train loss:  2.37554262e-02, mean val. rec. loss:  1.54009308e-02\n",
      "Epoch: 2851 mean train loss:  2.37498787e-02, mean val. rec. loss:  1.53959900e-02\n",
      "Epoch: 2852 mean train loss:  2.37443518e-02, mean val. rec. loss:  1.53910311e-02\n",
      "Epoch: 2853 mean train loss:  2.37388210e-02, mean val. rec. loss:  1.53860757e-02\n",
      "Epoch: 2854 mean train loss:  2.37332941e-02, mean val. rec. loss:  1.53811191e-02\n",
      "Epoch: 2855 mean train loss:  2.37277522e-02, mean val. rec. loss:  1.53761692e-02\n",
      "Epoch: 2856 mean train loss:  2.37222196e-02, mean val. rec. loss:  1.53711979e-02\n",
      "Epoch: 2857 mean train loss:  2.37166908e-02, mean val. rec. loss:  1.53662401e-02\n",
      "Epoch: 2858 mean train loss:  2.37111638e-02, mean val. rec. loss:  1.53612733e-02\n",
      "Epoch: 2859 mean train loss:  2.37056331e-02, mean val. rec. loss:  1.53563065e-02\n",
      "Epoch: 2860 mean train loss:  2.37001042e-02, mean val. rec. loss:  1.53513465e-02\n",
      "Epoch: 2861 mean train loss:  2.36945791e-02, mean val. rec. loss:  1.53463729e-02\n",
      "Epoch: 2862 mean train loss:  2.36890484e-02, mean val. rec. loss:  1.53414027e-02\n",
      "Epoch: 2863 mean train loss:  2.36835214e-02, mean val. rec. loss:  1.53364460e-02\n",
      "Epoch: 2864 mean train loss:  2.36779907e-02, mean val. rec. loss:  1.53314849e-02\n",
      "Epoch: 2865 mean train loss:  2.36724600e-02, mean val. rec. loss:  1.53265328e-02\n",
      "Epoch: 2866 mean train loss:  2.36669368e-02, mean val. rec. loss:  1.53215796e-02\n",
      "Epoch: 2867 mean train loss:  2.36614042e-02, mean val. rec. loss:  1.53166196e-02\n",
      "Epoch: 2868 mean train loss:  2.36558754e-02, mean val. rec. loss:  1.53116596e-02\n",
      "Epoch: 2869 mean train loss:  2.36503484e-02, mean val. rec. loss:  1.53066894e-02\n",
      "Epoch: 2870 mean train loss:  2.36448195e-02, mean val. rec. loss:  1.53017169e-02\n",
      "Epoch: 2871 mean train loss:  2.36392907e-02, mean val. rec. loss:  1.52967444e-02\n",
      "Epoch: 2872 mean train loss:  2.36337637e-02, mean val. rec. loss:  1.52917708e-02\n",
      "Epoch: 2873 mean train loss:  2.36282349e-02, mean val. rec. loss:  1.52867892e-02\n",
      "Epoch: 2874 mean train loss:  2.36227042e-02, mean val. rec. loss:  1.52818247e-02\n",
      "Epoch: 2875 mean train loss:  2.36171902e-02, mean val. rec. loss:  1.52768510e-02\n",
      "Epoch: 2876 mean train loss:  2.36116670e-02, mean val. rec. loss:  1.52718808e-02\n",
      "Epoch: 2877 mean train loss:  2.36061362e-02, mean val. rec. loss:  1.52669004e-02\n",
      "Epoch: 2878 mean train loss:  2.36006093e-02, mean val. rec. loss:  1.52619313e-02\n",
      "Epoch: 2879 mean train loss:  2.35950860e-02, mean val. rec. loss:  1.52569588e-02\n",
      "Epoch: 2880 mean train loss:  2.35895683e-02, mean val. rec. loss:  1.52519818e-02\n",
      "Epoch: 2881 mean train loss:  2.35840395e-02, mean val. rec. loss:  1.52470070e-02\n",
      "Epoch: 2882 mean train loss:  2.35785144e-02, mean val. rec. loss:  1.52420266e-02\n",
      "Epoch: 2883 mean train loss:  2.35729930e-02, mean val. rec. loss:  1.52370439e-02\n",
      "Epoch: 2884 mean train loss:  2.35674716e-02, mean val. rec. loss:  1.52320771e-02\n",
      "Epoch: 2885 mean train loss:  2.35619465e-02, mean val. rec. loss:  1.52270967e-02\n",
      "Epoch: 2886 mean train loss:  2.35564325e-02, mean val. rec. loss:  1.52221140e-02\n",
      "Epoch: 2887 mean train loss:  2.35509037e-02, mean val. rec. loss:  1.52171313e-02\n",
      "Epoch: 2888 mean train loss:  2.35453804e-02, mean val. rec. loss:  1.52121531e-02\n",
      "Epoch: 2889 mean train loss:  2.35398683e-02, mean val. rec. loss:  1.52071738e-02\n",
      "Epoch: 2890 mean train loss:  2.35343414e-02, mean val. rec. loss:  1.52021946e-02\n",
      "Epoch: 2891 mean train loss:  2.35288274e-02, mean val. rec. loss:  1.51972141e-02\n",
      "Epoch: 2892 mean train loss:  2.35233153e-02, mean val. rec. loss:  1.51922382e-02\n",
      "Epoch: 2893 mean train loss:  2.35177902e-02, mean val. rec. loss:  1.51872567e-02\n",
      "Epoch: 2894 mean train loss:  2.35122856e-02, mean val. rec. loss:  1.51822661e-02\n",
      "Epoch: 2895 mean train loss:  2.35067605e-02, mean val. rec. loss:  1.51772834e-02\n",
      "Epoch: 2896 mean train loss:  2.35012502e-02, mean val. rec. loss:  1.51722984e-02\n",
      "Epoch: 2897 mean train loss:  2.34957288e-02, mean val. rec. loss:  1.51673089e-02\n",
      "Epoch: 2898 mean train loss:  2.34902223e-02, mean val. rec. loss:  1.51623307e-02\n",
      "Epoch: 2899 mean train loss:  2.34847047e-02, mean val. rec. loss:  1.51573435e-02\n",
      "Epoch: 2900 mean train loss:  2.34791982e-02, mean val. rec. loss:  1.51523620e-02\n",
      "Epoch: 2901 mean train loss:  2.34736749e-02, mean val. rec. loss:  1.51473736e-02\n",
      "Epoch: 2902 mean train loss:  2.34681703e-02, mean val. rec. loss:  1.51423886e-02\n",
      "Epoch: 2903 mean train loss:  2.34626507e-02, mean val. rec. loss:  1.51373901e-02\n",
      "Epoch: 2904 mean train loss:  2.34571442e-02, mean val. rec. loss:  1.51324062e-02\n",
      "Epoch: 2905 mean train loss:  2.34516340e-02, mean val. rec. loss:  1.51274190e-02\n",
      "Epoch: 2906 mean train loss:  2.34461219e-02, mean val. rec. loss:  1.51224329e-02\n",
      "Epoch: 2907 mean train loss:  2.34406173e-02, mean val. rec. loss:  1.51174423e-02\n",
      "Epoch: 2908 mean train loss:  2.34350996e-02, mean val. rec. loss:  1.51124448e-02\n",
      "Epoch: 2909 mean train loss:  2.34295987e-02, mean val. rec. loss:  1.51074474e-02\n",
      "Epoch: 2910 mean train loss:  2.34240997e-02, mean val. rec. loss:  1.51024454e-02\n",
      "Epoch: 2911 mean train loss:  2.34185801e-02, mean val. rec. loss:  1.50974673e-02\n",
      "Epoch: 2912 mean train loss:  2.34130867e-02, mean val. rec. loss:  1.50924812e-02\n",
      "Epoch: 2913 mean train loss:  2.34075746e-02, mean val. rec. loss:  1.50874872e-02\n",
      "Epoch: 2914 mean train loss:  2.34020718e-02, mean val. rec. loss:  1.50824988e-02\n",
      "Epoch: 2915 mean train loss:  2.33965728e-02, mean val. rec. loss:  1.50775116e-02\n",
      "Epoch: 2916 mean train loss:  2.33910626e-02, mean val. rec. loss:  1.50725243e-02\n",
      "Epoch: 2917 mean train loss:  2.33855672e-02, mean val. rec. loss:  1.50675303e-02\n",
      "Epoch: 2918 mean train loss:  2.33800682e-02, mean val. rec. loss:  1.50625510e-02\n",
      "Epoch: 2919 mean train loss:  2.33745598e-02, mean val. rec. loss:  1.50575536e-02\n",
      "Epoch: 2920 mean train loss:  2.33690608e-02, mean val. rec. loss:  1.50525641e-02\n",
      "Epoch: 2921 mean train loss:  2.33635673e-02, mean val. rec. loss:  1.50475496e-02\n",
      "Epoch: 2922 mean train loss:  2.33580720e-02, mean val. rec. loss:  1.50425567e-02\n",
      "Epoch: 2923 mean train loss:  2.33525748e-02, mean val. rec. loss:  1.50375638e-02\n",
      "Epoch: 2924 mean train loss:  2.33470720e-02, mean val. rec. loss:  1.50325687e-02\n",
      "Epoch: 2925 mean train loss:  2.33415842e-02, mean val. rec. loss:  1.50275644e-02\n",
      "Epoch: 2926 mean train loss:  2.33360926e-02, mean val. rec. loss:  1.50225795e-02\n",
      "Epoch: 2927 mean train loss:  2.33305954e-02, mean val. rec. loss:  1.50175786e-02\n",
      "Epoch: 2928 mean train loss:  2.33250982e-02, mean val. rec. loss:  1.50125846e-02\n",
      "Epoch: 2929 mean train loss:  2.33196140e-02, mean val. rec. loss:  1.50075906e-02\n",
      "Epoch: 2930 mean train loss:  2.33141243e-02, mean val. rec. loss:  1.50025863e-02\n",
      "Epoch: 2931 mean train loss:  2.33086308e-02, mean val. rec. loss:  1.49975980e-02\n",
      "Epoch: 2932 mean train loss:  2.33031392e-02, mean val. rec. loss:  1.49926005e-02\n",
      "Epoch: 2933 mean train loss:  2.32976569e-02, mean val. rec. loss:  1.49875952e-02\n",
      "Epoch: 2934 mean train loss:  2.32921747e-02, mean val. rec. loss:  1.49825909e-02\n",
      "Epoch: 2935 mean train loss:  2.32866849e-02, mean val. rec. loss:  1.49775924e-02\n",
      "Epoch: 2936 mean train loss:  2.32811989e-02, mean val. rec. loss:  1.49725892e-02\n",
      "Epoch: 2937 mean train loss:  2.32757185e-02, mean val. rec. loss:  1.49675907e-02\n",
      "Epoch: 2938 mean train loss:  2.32702380e-02, mean val. rec. loss:  1.49625762e-02\n",
      "Epoch: 2939 mean train loss:  2.32647539e-02, mean val. rec. loss:  1.49575867e-02\n",
      "Epoch: 2940 mean train loss:  2.32592697e-02, mean val. rec. loss:  1.49525927e-02\n",
      "Epoch: 2941 mean train loss:  2.32537912e-02, mean val. rec. loss:  1.49476032e-02\n",
      "Epoch: 2942 mean train loss:  2.32483201e-02, mean val. rec. loss:  1.49426058e-02\n",
      "Epoch: 2943 mean train loss:  2.32428359e-02, mean val. rec. loss:  1.49376083e-02\n",
      "Epoch: 2944 mean train loss:  2.32373555e-02, mean val. rec. loss:  1.49326030e-02\n",
      "Epoch: 2945 mean train loss:  2.32318862e-02, mean val. rec. loss:  1.49276191e-02\n",
      "Epoch: 2946 mean train loss:  2.32264133e-02, mean val. rec. loss:  1.49226160e-02\n",
      "Epoch: 2947 mean train loss:  2.32209366e-02, mean val. rec. loss:  1.49176197e-02\n",
      "Epoch: 2948 mean train loss:  2.32154599e-02, mean val. rec. loss:  1.49126246e-02\n",
      "Epoch: 2949 mean train loss:  2.32099925e-02, mean val. rec. loss:  1.49076351e-02\n",
      "Epoch: 2950 mean train loss:  2.32045213e-02, mean val. rec. loss:  1.49026501e-02\n",
      "Epoch: 2951 mean train loss:  2.31990484e-02, mean val. rec. loss:  1.48976538e-02\n",
      "Epoch: 2952 mean train loss:  2.31935791e-02, mean val. rec. loss:  1.48926507e-02\n",
      "Epoch: 2953 mean train loss:  2.31881117e-02, mean val. rec. loss:  1.48876408e-02\n",
      "Epoch: 2954 mean train loss:  2.31826481e-02, mean val. rec. loss:  1.48826263e-02\n",
      "Epoch: 2955 mean train loss:  2.31771825e-02, mean val. rec. loss:  1.48776142e-02\n",
      "Epoch: 2956 mean train loss:  2.31717114e-02, mean val. rec. loss:  1.48726088e-02\n",
      "Epoch: 2957 mean train loss:  2.31662552e-02, mean val. rec. loss:  1.48676193e-02\n",
      "Epoch: 2958 mean train loss:  2.31607934e-02, mean val. rec. loss:  1.48626275e-02\n",
      "Epoch: 2959 mean train loss:  2.31553297e-02, mean val. rec. loss:  1.48576426e-02\n",
      "Epoch: 2960 mean train loss:  2.31498698e-02, mean val. rec. loss:  1.48526519e-02\n",
      "Epoch: 2961 mean train loss:  2.31444154e-02, mean val. rec. loss:  1.48476590e-02\n",
      "Epoch: 2962 mean train loss:  2.31389611e-02, mean val. rec. loss:  1.48426627e-02\n",
      "Epoch: 2963 mean train loss:  2.31335011e-02, mean val. rec. loss:  1.48376801e-02\n",
      "Epoch: 2964 mean train loss:  2.31280393e-02, mean val. rec. loss:  1.48326906e-02\n",
      "Epoch: 2965 mean train loss:  2.31225887e-02, mean val. rec. loss:  1.48276920e-02\n",
      "Epoch: 2966 mean train loss:  2.31171381e-02, mean val. rec. loss:  1.48226934e-02\n",
      "Epoch: 2967 mean train loss:  2.31116818e-02, mean val. rec. loss:  1.48177028e-02\n",
      "Epoch: 2968 mean train loss:  2.31062312e-02, mean val. rec. loss:  1.48127099e-02\n",
      "Epoch: 2969 mean train loss:  2.31007862e-02, mean val. rec. loss:  1.48077159e-02\n",
      "Epoch: 2970 mean train loss:  2.30953430e-02, mean val. rec. loss:  1.48027059e-02\n",
      "Epoch: 2971 mean train loss:  2.30898886e-02, mean val. rec. loss:  1.47977119e-02\n",
      "Epoch: 2972 mean train loss:  2.30844380e-02, mean val. rec. loss:  1.47927054e-02\n",
      "Epoch: 2973 mean train loss:  2.30789985e-02, mean val. rec. loss:  1.47877057e-02\n",
      "Epoch: 2974 mean train loss:  2.30735609e-02, mean val. rec. loss:  1.47827094e-02\n",
      "Epoch: 2975 mean train loss:  2.30681103e-02, mean val. rec. loss:  1.47777267e-02\n",
      "Epoch: 2976 mean train loss:  2.30626708e-02, mean val. rec. loss:  1.47727372e-02\n",
      "Epoch: 2977 mean train loss:  2.30572332e-02, mean val. rec. loss:  1.47677296e-02\n",
      "Epoch: 2978 mean train loss:  2.30517957e-02, mean val. rec. loss:  1.47627378e-02\n",
      "Epoch: 2979 mean train loss:  2.30463599e-02, mean val. rec. loss:  1.47577438e-02\n",
      "Epoch: 2980 mean train loss:  2.30409167e-02, mean val. rec. loss:  1.47527543e-02\n",
      "Epoch: 2981 mean train loss:  2.30354884e-02, mean val. rec. loss:  1.47477614e-02\n",
      "Epoch: 2982 mean train loss:  2.30300602e-02, mean val. rec. loss:  1.47427662e-02\n",
      "Epoch: 2983 mean train loss:  2.30246207e-02, mean val. rec. loss:  1.47377767e-02\n",
      "Epoch: 2984 mean train loss:  2.30191868e-02, mean val. rec. loss:  1.47327872e-02\n",
      "Epoch: 2985 mean train loss:  2.30137623e-02, mean val. rec. loss:  1.47277955e-02\n",
      "Epoch: 2986 mean train loss:  2.30083359e-02, mean val. rec. loss:  1.47228037e-02\n",
      "Epoch: 2987 mean train loss:  2.30029020e-02, mean val. rec. loss:  1.47178074e-02\n",
      "Epoch: 2988 mean train loss:  2.29974830e-02, mean val. rec. loss:  1.47128111e-02\n",
      "Epoch: 2989 mean train loss:  2.29920491e-02, mean val. rec. loss:  1.47078216e-02\n",
      "Epoch: 2990 mean train loss:  2.29866339e-02, mean val. rec. loss:  1.47028219e-02\n",
      "Epoch: 2991 mean train loss:  2.29812186e-02, mean val. rec. loss:  1.46978347e-02\n",
      "Epoch: 2992 mean train loss:  2.29757866e-02, mean val. rec. loss:  1.46928372e-02\n",
      "Epoch: 2993 mean train loss:  2.29703751e-02, mean val. rec. loss:  1.46878421e-02\n",
      "Epoch: 2994 mean train loss:  2.29649505e-02, mean val. rec. loss:  1.46828458e-02\n",
      "Epoch: 2995 mean train loss:  2.29595371e-02, mean val. rec. loss:  1.46778687e-02\n",
      "Epoch: 2996 mean train loss:  2.29541275e-02, mean val. rec. loss:  1.46728838e-02\n",
      "Epoch: 2997 mean train loss:  2.29487048e-02, mean val. rec. loss:  1.46678852e-02\n",
      "Epoch: 2998 mean train loss:  2.29432988e-02, mean val. rec. loss:  1.46629093e-02\n",
      "Epoch: 2999 mean train loss:  2.29378855e-02, mean val. rec. loss:  1.46579175e-02\n",
      "Epoch: 3000 mean train loss:  2.29324795e-02, mean val. rec. loss:  1.46529326e-02\n",
      "Epoch: 3001 mean train loss:  2.29270699e-02, mean val. rec. loss:  1.46479454e-02\n",
      "Epoch: 3002 mean train loss:  2.29216621e-02, mean val. rec. loss:  1.46429683e-02\n",
      "Epoch: 3003 mean train loss:  2.29162580e-02, mean val. rec. loss:  1.46379879e-02\n",
      "Epoch: 3004 mean train loss:  2.29108539e-02, mean val. rec. loss:  1.46329905e-02\n",
      "Epoch: 3005 mean train loss:  2.29054535e-02, mean val. rec. loss:  1.46280089e-02\n",
      "Epoch: 3006 mean train loss:  2.29000513e-02, mean val. rec. loss:  1.46230274e-02\n",
      "Epoch: 3007 mean train loss:  2.28946510e-02, mean val. rec. loss:  1.46180311e-02\n",
      "Epoch: 3008 mean train loss:  2.28892525e-02, mean val. rec. loss:  1.46130484e-02\n",
      "Epoch: 3009 mean train loss:  2.28838614e-02, mean val. rec. loss:  1.46080668e-02\n",
      "Epoch: 3010 mean train loss:  2.28784648e-02, mean val. rec. loss:  1.46030886e-02\n",
      "Epoch: 3011 mean train loss:  2.28730645e-02, mean val. rec. loss:  1.45981059e-02\n",
      "Epoch: 3012 mean train loss:  2.28676809e-02, mean val. rec. loss:  1.45931267e-02\n",
      "Epoch: 3013 mean train loss:  2.28622861e-02, mean val. rec. loss:  1.45881349e-02\n",
      "Epoch: 3014 mean train loss:  2.28568876e-02, mean val. rec. loss:  1.45831590e-02\n",
      "Epoch: 3015 mean train loss:  2.28515115e-02, mean val. rec. loss:  1.45781718e-02\n",
      "Epoch: 3016 mean train loss:  2.28461167e-02, mean val. rec. loss:  1.45731936e-02\n",
      "Epoch: 3017 mean train loss:  2.28407350e-02, mean val. rec. loss:  1.45681996e-02\n",
      "Epoch: 3018 mean train loss:  2.28353532e-02, mean val. rec. loss:  1.45632158e-02\n",
      "Epoch: 3019 mean train loss:  2.28299678e-02, mean val. rec. loss:  1.45582433e-02\n",
      "Epoch: 3020 mean train loss:  2.28245823e-02, mean val. rec. loss:  1.45532538e-02\n",
      "Epoch: 3021 mean train loss:  2.28192155e-02, mean val. rec. loss:  1.45482756e-02\n",
      "Epoch: 3022 mean train loss:  2.28138338e-02, mean val. rec. loss:  1.45432997e-02\n",
      "Epoch: 3023 mean train loss:  2.28084539e-02, mean val. rec. loss:  1.45383363e-02\n",
      "Epoch: 3024 mean train loss:  2.28030740e-02, mean val. rec. loss:  1.45333672e-02\n",
      "Epoch: 3025 mean train loss:  2.27977128e-02, mean val. rec. loss:  1.45283857e-02\n",
      "Epoch: 3026 mean train loss:  2.27923366e-02, mean val. rec. loss:  1.45234211e-02\n",
      "Epoch: 3027 mean train loss:  2.27869661e-02, mean val. rec. loss:  1.45184475e-02\n",
      "Epoch: 3028 mean train loss:  2.27815955e-02, mean val. rec. loss:  1.45134784e-02\n",
      "Epoch: 3029 mean train loss:  2.27762268e-02, mean val. rec. loss:  1.45084934e-02\n",
      "Epoch: 3030 mean train loss:  2.27708581e-02, mean val. rec. loss:  1.45035164e-02\n",
      "Epoch: 3031 mean train loss:  2.27654988e-02, mean val. rec. loss:  1.44985530e-02\n",
      "Epoch: 3032 mean train loss:  2.27601394e-02, mean val. rec. loss:  1.44935873e-02\n",
      "Epoch: 3033 mean train loss:  2.27547800e-02, mean val. rec. loss:  1.44886239e-02\n",
      "Epoch: 3034 mean train loss:  2.27494169e-02, mean val. rec. loss:  1.44836594e-02\n",
      "Epoch: 3035 mean train loss:  2.27440612e-02, mean val. rec. loss:  1.44786835e-02\n",
      "Epoch: 3036 mean train loss:  2.27387018e-02, mean val. rec. loss:  1.44737087e-02\n",
      "Epoch: 3037 mean train loss:  2.27333462e-02, mean val. rec. loss:  1.44687340e-02\n",
      "Epoch: 3038 mean train loss:  2.27279924e-02, mean val. rec. loss:  1.44637717e-02\n",
      "Epoch: 3039 mean train loss:  2.27226386e-02, mean val. rec. loss:  1.44588003e-02\n",
      "Epoch: 3040 mean train loss:  2.27172866e-02, mean val. rec. loss:  1.44538426e-02\n",
      "Epoch: 3041 mean train loss:  2.27119384e-02, mean val. rec. loss:  1.44488837e-02\n",
      "Epoch: 3042 mean train loss:  2.27065846e-02, mean val. rec. loss:  1.44439135e-02\n",
      "Epoch: 3043 mean train loss:  2.27012346e-02, mean val. rec. loss:  1.44389512e-02\n",
      "Epoch: 3044 mean train loss:  2.26958901e-02, mean val. rec. loss:  1.44339867e-02\n",
      "Epoch: 3045 mean train loss:  2.26905493e-02, mean val. rec. loss:  1.44290244e-02\n",
      "Epoch: 3046 mean train loss:  2.26852104e-02, mean val. rec. loss:  1.44240519e-02\n",
      "Epoch: 3047 mean train loss:  2.26798715e-02, mean val. rec. loss:  1.44190862e-02\n",
      "Epoch: 3048 mean train loss:  2.26745308e-02, mean val. rec. loss:  1.44141296e-02\n",
      "Epoch: 3049 mean train loss:  2.26691900e-02, mean val. rec. loss:  1.44091707e-02\n",
      "Epoch: 3050 mean train loss:  2.26638567e-02, mean val. rec. loss:  1.44042107e-02\n",
      "Epoch: 3051 mean train loss:  2.26585271e-02, mean val. rec. loss:  1.43992541e-02\n",
      "Epoch: 3052 mean train loss:  2.26531900e-02, mean val. rec. loss:  1.43942907e-02\n",
      "Epoch: 3053 mean train loss:  2.26478530e-02, mean val. rec. loss:  1.43893443e-02\n",
      "Epoch: 3054 mean train loss:  2.26425346e-02, mean val. rec. loss:  1.43843820e-02\n",
      "Epoch: 3055 mean train loss:  2.26371957e-02, mean val. rec. loss:  1.43794288e-02\n",
      "Epoch: 3056 mean train loss:  2.26318773e-02, mean val. rec. loss:  1.43744665e-02\n",
      "Epoch: 3057 mean train loss:  2.26265570e-02, mean val. rec. loss:  1.43694929e-02\n",
      "Epoch: 3058 mean train loss:  2.26212218e-02, mean val. rec. loss:  1.43645510e-02\n",
      "Epoch: 3059 mean train loss:  2.26159071e-02, mean val. rec. loss:  1.43595989e-02\n",
      "Epoch: 3060 mean train loss:  2.26105794e-02, mean val. rec. loss:  1.43546468e-02\n",
      "Epoch: 3061 mean train loss:  2.26052722e-02, mean val. rec. loss:  1.43497027e-02\n",
      "Epoch: 3062 mean train loss:  2.25999463e-02, mean val. rec. loss:  1.43447495e-02\n",
      "Epoch: 3063 mean train loss:  2.25946335e-02, mean val. rec. loss:  1.43397986e-02\n",
      "Epoch: 3064 mean train loss:  2.25893169e-02, mean val. rec. loss:  1.43348510e-02\n",
      "Epoch: 3065 mean train loss:  2.25840115e-02, mean val. rec. loss:  1.43298944e-02\n",
      "Epoch: 3066 mean train loss:  2.25786950e-02, mean val. rec. loss:  1.43249491e-02\n",
      "Epoch: 3067 mean train loss:  2.25733803e-02, mean val. rec. loss:  1.43200039e-02\n",
      "Epoch: 3068 mean train loss:  2.25680842e-02, mean val. rec. loss:  1.43150416e-02\n",
      "Epoch: 3069 mean train loss:  2.25627677e-02, mean val. rec. loss:  1.43100952e-02\n",
      "Epoch: 3070 mean train loss:  2.25574623e-02, mean val. rec. loss:  1.43051420e-02\n",
      "Epoch: 3071 mean train loss:  2.25521699e-02, mean val. rec. loss:  1.43001853e-02\n",
      "Epoch: 3072 mean train loss:  2.25468627e-02, mean val. rec. loss:  1.42952276e-02\n",
      "Epoch: 3073 mean train loss:  2.25415592e-02, mean val. rec. loss:  1.42902767e-02\n",
      "Epoch: 3074 mean train loss:  2.25362594e-02, mean val. rec. loss:  1.42853495e-02\n",
      "Epoch: 3075 mean train loss:  2.25309670e-02, mean val. rec. loss:  1.42804043e-02\n",
      "Epoch: 3076 mean train loss:  2.25256784e-02, mean val. rec. loss:  1.42754613e-02\n",
      "Epoch: 3077 mean train loss:  2.25203823e-02, mean val. rec. loss:  1.42705307e-02\n",
      "Epoch: 3078 mean train loss:  2.25150919e-02, mean val. rec. loss:  1.42655889e-02\n",
      "Epoch: 3079 mean train loss:  2.25097977e-02, mean val. rec. loss:  1.42606561e-02\n",
      "Epoch: 3080 mean train loss:  2.25045128e-02, mean val. rec. loss:  1.42557210e-02\n",
      "Epoch: 3081 mean train loss:  2.24992260e-02, mean val. rec. loss:  1.42507882e-02\n",
      "Epoch: 3082 mean train loss:  2.24939430e-02, mean val. rec. loss:  1.42458373e-02\n",
      "Epoch: 3083 mean train loss:  2.24886543e-02, mean val. rec. loss:  1.42408999e-02\n",
      "Epoch: 3084 mean train loss:  2.24833750e-02, mean val. rec. loss:  1.42359683e-02\n",
      "Epoch: 3085 mean train loss:  2.24780994e-02, mean val. rec. loss:  1.42310366e-02\n",
      "Epoch: 3086 mean train loss:  2.24728108e-02, mean val. rec. loss:  1.42260925e-02\n",
      "Epoch: 3087 mean train loss:  2.24675259e-02, mean val. rec. loss:  1.42211585e-02\n",
      "Epoch: 3088 mean train loss:  2.24622541e-02, mean val. rec. loss:  1.42162212e-02\n",
      "Epoch: 3089 mean train loss:  2.24569822e-02, mean val. rec. loss:  1.42112805e-02\n",
      "Epoch: 3090 mean train loss:  2.24517141e-02, mean val. rec. loss:  1.42063295e-02\n",
      "Epoch: 3091 mean train loss:  2.24464403e-02, mean val. rec. loss:  1.42013956e-02\n",
      "Epoch: 3092 mean train loss:  2.24411629e-02, mean val. rec. loss:  1.41964594e-02\n",
      "Epoch: 3093 mean train loss:  2.24359003e-02, mean val. rec. loss:  1.41915402e-02\n",
      "Epoch: 3094 mean train loss:  2.24306322e-02, mean val. rec. loss:  1.41866176e-02\n",
      "Epoch: 3095 mean train loss:  2.24253659e-02, mean val. rec. loss:  1.41816950e-02\n",
      "Epoch: 3096 mean train loss:  2.24201071e-02, mean val. rec. loss:  1.41767849e-02\n",
      "Epoch: 3097 mean train loss:  2.24148408e-02, mean val. rec. loss:  1.41718623e-02\n",
      "Epoch: 3098 mean train loss:  2.24095857e-02, mean val. rec. loss:  1.41669442e-02\n",
      "Epoch: 3099 mean train loss:  2.24043232e-02, mean val. rec. loss:  1.41620114e-02\n",
      "Epoch: 3100 mean train loss:  2.23990699e-02, mean val. rec. loss:  1.41570786e-02\n",
      "Epoch: 3101 mean train loss:  2.23938093e-02, mean val. rec. loss:  1.41521572e-02\n",
      "Epoch: 3102 mean train loss:  2.23885542e-02, mean val. rec. loss:  1.41472176e-02\n",
      "Epoch: 3103 mean train loss:  2.23833046e-02, mean val. rec. loss:  1.41422870e-02\n",
      "Epoch: 3104 mean train loss:  2.23780477e-02, mean val. rec. loss:  1.41373792e-02\n",
      "Epoch: 3105 mean train loss:  2.23728019e-02, mean val. rec. loss:  1.41324623e-02\n",
      "Epoch: 3106 mean train loss:  2.23675635e-02, mean val. rec. loss:  1.41275385e-02\n",
      "Epoch: 3107 mean train loss:  2.23623122e-02, mean val. rec. loss:  1.41226171e-02\n",
      "Epoch: 3108 mean train loss:  2.23570608e-02, mean val. rec. loss:  1.41176888e-02\n",
      "Epoch: 3109 mean train loss:  2.23518169e-02, mean val. rec. loss:  1.41127674e-02\n",
      "Epoch: 3110 mean train loss:  2.23465767e-02, mean val. rec. loss:  1.41078572e-02\n",
      "Epoch: 3111 mean train loss:  2.23413439e-02, mean val. rec. loss:  1.41029403e-02\n",
      "Epoch: 3112 mean train loss:  2.23361037e-02, mean val. rec. loss:  1.40980279e-02\n",
      "Epoch: 3113 mean train loss:  2.23308635e-02, mean val. rec. loss:  1.40931201e-02\n",
      "Epoch: 3114 mean train loss:  2.23256307e-02, mean val. rec. loss:  1.40882190e-02\n",
      "Epoch: 3115 mean train loss:  2.23203980e-02, mean val. rec. loss:  1.40833078e-02\n",
      "Epoch: 3116 mean train loss:  2.23151652e-02, mean val. rec. loss:  1.40783875e-02\n",
      "Epoch: 3117 mean train loss:  2.23099362e-02, mean val. rec. loss:  1.40734819e-02\n",
      "Epoch: 3118 mean train loss:  2.23046941e-02, mean val. rec. loss:  1.40685763e-02\n",
      "Epoch: 3119 mean train loss:  2.22994632e-02, mean val. rec. loss:  1.40636526e-02\n",
      "Epoch: 3120 mean train loss:  2.22942398e-02, mean val. rec. loss:  1.40587425e-02\n",
      "Epoch: 3121 mean train loss:  2.22890163e-02, mean val. rec. loss:  1.40538357e-02\n",
      "Epoch: 3122 mean train loss:  2.22837985e-02, mean val. rec. loss:  1.40489381e-02\n",
      "Epoch: 3123 mean train loss:  2.22785695e-02, mean val. rec. loss:  1.40440325e-02\n",
      "Epoch: 3124 mean train loss:  2.22733442e-02, mean val. rec. loss:  1.40391360e-02\n",
      "Epoch: 3125 mean train loss:  2.22681337e-02, mean val. rec. loss:  1.40342304e-02\n",
      "Epoch: 3126 mean train loss:  2.22629084e-02, mean val. rec. loss:  1.40293316e-02\n",
      "Epoch: 3127 mean train loss:  2.22576980e-02, mean val. rec. loss:  1.40244170e-02\n",
      "Epoch: 3128 mean train loss:  2.22524802e-02, mean val. rec. loss:  1.40195250e-02\n",
      "Epoch: 3129 mean train loss:  2.22472679e-02, mean val. rec. loss:  1.40146240e-02\n",
      "Epoch: 3130 mean train loss:  2.22420556e-02, mean val. rec. loss:  1.40097184e-02\n",
      "Epoch: 3131 mean train loss:  2.22368452e-02, mean val. rec. loss:  1.40048264e-02\n",
      "Epoch: 3132 mean train loss:  2.22316348e-02, mean val. rec. loss:  1.39999254e-02\n",
      "Epoch: 3133 mean train loss:  2.22264337e-02, mean val. rec. loss:  1.39950220e-02\n",
      "Epoch: 3134 mean train loss:  2.22212252e-02, mean val. rec. loss:  1.39901369e-02\n",
      "Epoch: 3135 mean train loss:  2.22160148e-02, mean val. rec. loss:  1.39852245e-02\n",
      "Epoch: 3136 mean train loss:  2.22108211e-02, mean val. rec. loss:  1.39803302e-02\n",
      "Epoch: 3137 mean train loss:  2.22056181e-02, mean val. rec. loss:  1.39754269e-02\n",
      "Epoch: 3138 mean train loss:  2.22004133e-02, mean val. rec. loss:  1.39705350e-02\n",
      "Epoch: 3139 mean train loss:  2.21952122e-02, mean val. rec. loss:  1.39656464e-02\n",
      "Epoch: 3140 mean train loss:  2.21900149e-02, mean val. rec. loss:  1.39607533e-02\n",
      "Epoch: 3141 mean train loss:  2.21848268e-02, mean val. rec. loss:  1.39558681e-02\n",
      "Epoch: 3142 mean train loss:  2.21796331e-02, mean val. rec. loss:  1.39509773e-02\n",
      "Epoch: 3143 mean train loss:  2.21744395e-02, mean val. rec. loss:  1.39460841e-02\n",
      "Epoch: 3144 mean train loss:  2.21692496e-02, mean val. rec. loss:  1.39412080e-02\n",
      "Epoch: 3145 mean train loss:  2.21640522e-02, mean val. rec. loss:  1.39363297e-02\n",
      "Epoch: 3146 mean train loss:  2.21588679e-02, mean val. rec. loss:  1.39314468e-02\n",
      "Epoch: 3147 mean train loss:  2.21536835e-02, mean val. rec. loss:  1.39265480e-02\n",
      "Epoch: 3148 mean train loss:  2.21485010e-02, mean val. rec. loss:  1.39216696e-02\n",
      "Epoch: 3149 mean train loss:  2.21433204e-02, mean val. rec. loss:  1.39167845e-02\n",
      "Epoch: 3150 mean train loss:  2.21381342e-02, mean val. rec. loss:  1.39119016e-02\n",
      "Epoch: 3151 mean train loss:  2.21329536e-02, mean val. rec. loss:  1.39070198e-02\n",
      "Epoch: 3152 mean train loss:  2.21277767e-02, mean val. rec. loss:  1.39021460e-02\n",
      "Epoch: 3153 mean train loss:  2.21225998e-02, mean val. rec. loss:  1.38972563e-02\n",
      "Epoch: 3154 mean train loss:  2.21174211e-02, mean val. rec. loss:  1.38923711e-02\n",
      "Epoch: 3155 mean train loss:  2.21122554e-02, mean val. rec. loss:  1.38874938e-02\n",
      "Epoch: 3156 mean train loss:  2.21070747e-02, mean val. rec. loss:  1.38826257e-02\n",
      "Epoch: 3157 mean train loss:  2.21019109e-02, mean val. rec. loss:  1.38777314e-02\n",
      "Epoch: 3158 mean train loss:  2.20967340e-02, mean val. rec. loss:  1.38728497e-02\n",
      "Epoch: 3159 mean train loss:  2.20915590e-02, mean val. rec. loss:  1.38679622e-02\n",
      "Epoch: 3160 mean train loss:  2.20864026e-02, mean val. rec. loss:  1.38630884e-02\n",
      "Epoch: 3161 mean train loss:  2.20812294e-02, mean val. rec. loss:  1.38582214e-02\n",
      "Epoch: 3162 mean train loss:  2.20760637e-02, mean val. rec. loss:  1.38533555e-02\n",
      "Epoch: 3163 mean train loss:  2.20709110e-02, mean val. rec. loss:  1.38484851e-02\n",
      "Epoch: 3164 mean train loss:  2.20657434e-02, mean val. rec. loss:  1.38436158e-02\n",
      "Epoch: 3165 mean train loss:  2.20605814e-02, mean val. rec. loss:  1.38387623e-02\n",
      "Epoch: 3166 mean train loss:  2.20554232e-02, mean val. rec. loss:  1.38338885e-02\n",
      "Epoch: 3167 mean train loss:  2.20502593e-02, mean val. rec. loss:  1.38290203e-02\n",
      "Epoch: 3168 mean train loss:  2.20451048e-02, mean val. rec. loss:  1.38241454e-02\n",
      "Epoch: 3169 mean train loss:  2.20399558e-02, mean val. rec. loss:  1.38192874e-02\n",
      "Epoch: 3170 mean train loss:  2.20347994e-02, mean val. rec. loss:  1.38144227e-02\n",
      "Epoch: 3171 mean train loss:  2.20296505e-02, mean val. rec. loss:  1.38095511e-02\n",
      "Epoch: 3172 mean train loss:  2.20244996e-02, mean val. rec. loss:  1.38046977e-02\n",
      "Epoch: 3173 mean train loss:  2.20193544e-02, mean val. rec. loss:  1.37998443e-02\n",
      "Epoch: 3174 mean train loss:  2.20142092e-02, mean val. rec. loss:  1.37949886e-02\n",
      "Epoch: 3175 mean train loss:  2.20090509e-02, mean val. rec. loss:  1.37901193e-02\n",
      "Epoch: 3176 mean train loss:  2.20039113e-02, mean val. rec. loss:  1.37852613e-02\n",
      "Epoch: 3177 mean train loss:  2.19987735e-02, mean val. rec. loss:  1.37804011e-02\n",
      "Epoch: 3178 mean train loss:  2.19936245e-02, mean val. rec. loss:  1.37755386e-02\n",
      "Epoch: 3179 mean train loss:  2.19884886e-02, mean val. rec. loss:  1.37706863e-02\n",
      "Epoch: 3180 mean train loss:  2.19833489e-02, mean val. rec. loss:  1.37658295e-02\n",
      "Epoch: 3181 mean train loss:  2.19782149e-02, mean val. rec. loss:  1.37609783e-02\n",
      "Epoch: 3182 mean train loss:  2.19730808e-02, mean val. rec. loss:  1.37561385e-02\n",
      "Epoch: 3183 mean train loss:  2.19679430e-02, mean val. rec. loss:  1.37512976e-02\n",
      "Epoch: 3184 mean train loss:  2.19628053e-02, mean val. rec. loss:  1.37464476e-02\n",
      "Epoch: 3185 mean train loss:  2.19576842e-02, mean val. rec. loss:  1.37415896e-02\n",
      "Epoch: 3186 mean train loss:  2.19525483e-02, mean val. rec. loss:  1.37367385e-02\n",
      "Epoch: 3187 mean train loss:  2.19474161e-02, mean val. rec. loss:  1.37318782e-02\n",
      "Epoch: 3188 mean train loss:  2.19422839e-02, mean val. rec. loss:  1.37270089e-02\n",
      "Epoch: 3189 mean train loss:  2.19371666e-02, mean val. rec. loss:  1.37221646e-02\n",
      "Epoch: 3190 mean train loss:  2.19320456e-02, mean val. rec. loss:  1.37173089e-02\n",
      "Epoch: 3191 mean train loss:  2.19269209e-02, mean val. rec. loss:  1.37124578e-02\n",
      "Epoch: 3192 mean train loss:  2.19217961e-02, mean val. rec. loss:  1.37076179e-02\n",
      "Epoch: 3193 mean train loss:  2.19166769e-02, mean val. rec. loss:  1.37028031e-02\n",
      "Epoch: 3194 mean train loss:  2.19115596e-02, mean val. rec. loss:  1.36979791e-02\n",
      "Epoch: 3195 mean train loss:  2.19064442e-02, mean val. rec. loss:  1.36931563e-02\n",
      "Epoch: 3196 mean train loss:  2.19013195e-02, mean val. rec. loss:  1.36883222e-02\n",
      "Epoch: 3197 mean train loss:  2.18962022e-02, mean val. rec. loss:  1.36834926e-02\n",
      "Epoch: 3198 mean train loss:  2.18910923e-02, mean val. rec. loss:  1.36786426e-02\n",
      "Epoch: 3199 mean train loss:  2.18859843e-02, mean val. rec. loss:  1.36737960e-02\n",
      "Epoch: 3200 mean train loss:  2.18808707e-02, mean val. rec. loss:  1.36689561e-02\n",
      "Epoch: 3201 mean train loss:  2.18757609e-02, mean val. rec. loss:  1.36641039e-02\n",
      "Epoch: 3202 mean train loss:  2.18706585e-02, mean val. rec. loss:  1.36592652e-02\n",
      "Epoch: 3203 mean train loss:  2.18655449e-02, mean val. rec. loss:  1.36544208e-02\n",
      "Epoch: 3204 mean train loss:  2.18604500e-02, mean val. rec. loss:  1.36495946e-02\n",
      "Epoch: 3205 mean train loss:  2.18553401e-02, mean val. rec. loss:  1.36447707e-02\n",
      "Epoch: 3206 mean train loss:  2.18502470e-02, mean val. rec. loss:  1.36399536e-02\n",
      "Epoch: 3207 mean train loss:  2.18451409e-02, mean val. rec. loss:  1.36351342e-02\n",
      "Epoch: 3208 mean train loss:  2.18400348e-02, mean val. rec. loss:  1.36303102e-02\n",
      "Epoch: 3209 mean train loss:  2.18349510e-02, mean val. rec. loss:  1.36254954e-02\n",
      "Epoch: 3210 mean train loss:  2.18298523e-02, mean val. rec. loss:  1.36206601e-02\n",
      "Epoch: 3211 mean train loss:  2.18247518e-02, mean val. rec. loss:  1.36158452e-02\n",
      "Epoch: 3212 mean train loss:  2.18196550e-02, mean val. rec. loss:  1.36110247e-02\n",
      "Epoch: 3213 mean train loss:  2.18145619e-02, mean val. rec. loss:  1.36062087e-02\n",
      "Epoch: 3214 mean train loss:  2.18094725e-02, mean val. rec. loss:  1.36013757e-02\n",
      "Epoch: 3215 mean train loss:  2.18043831e-02, mean val. rec. loss:  1.35965597e-02\n",
      "Epoch: 3216 mean train loss:  2.17992938e-02, mean val. rec. loss:  1.35917459e-02\n",
      "Epoch: 3217 mean train loss:  2.17942100e-02, mean val. rec. loss:  1.35869424e-02\n",
      "Epoch: 3218 mean train loss:  2.17891299e-02, mean val. rec. loss:  1.35821321e-02\n",
      "Epoch: 3219 mean train loss:  2.17840461e-02, mean val. rec. loss:  1.35773263e-02\n",
      "Epoch: 3220 mean train loss:  2.17789698e-02, mean val. rec. loss:  1.35725228e-02\n",
      "Epoch: 3221 mean train loss:  2.17738823e-02, mean val. rec. loss:  1.35677192e-02\n",
      "Epoch: 3222 mean train loss:  2.17687985e-02, mean val. rec. loss:  1.35628953e-02\n",
      "Epoch: 3223 mean train loss:  2.17637259e-02, mean val. rec. loss:  1.35580873e-02\n",
      "Epoch: 3224 mean train loss:  2.17586496e-02, mean val. rec. loss:  1.35532747e-02\n",
      "Epoch: 3225 mean train loss:  2.17535733e-02, mean val. rec. loss:  1.35484734e-02\n",
      "Epoch: 3226 mean train loss:  2.17484988e-02, mean val. rec. loss:  1.35436495e-02\n",
      "Epoch: 3227 mean train loss:  2.17434280e-02, mean val. rec. loss:  1.35388550e-02\n",
      "Epoch: 3228 mean train loss:  2.17383517e-02, mean val. rec. loss:  1.35340481e-02\n",
      "Epoch: 3229 mean train loss:  2.17332903e-02, mean val. rec. loss:  1.35292548e-02\n",
      "Epoch: 3230 mean train loss:  2.17282158e-02, mean val. rec. loss:  1.35244671e-02\n",
      "Epoch: 3231 mean train loss:  2.17231469e-02, mean val. rec. loss:  1.35196727e-02\n",
      "Epoch: 3232 mean train loss:  2.17180948e-02, mean val. rec. loss:  1.35148805e-02\n",
      "Epoch: 3233 mean train loss:  2.17130278e-02, mean val. rec. loss:  1.35100906e-02\n",
      "Epoch: 3234 mean train loss:  2.17079682e-02, mean val. rec. loss:  1.35052904e-02\n",
      "Epoch: 3235 mean train loss:  2.17029049e-02, mean val. rec. loss:  1.35005005e-02\n",
      "Epoch: 3236 mean train loss:  2.16978472e-02, mean val. rec. loss:  1.34957049e-02\n",
      "Epoch: 3237 mean train loss:  2.16927895e-02, mean val. rec. loss:  1.34908980e-02\n",
      "Epoch: 3238 mean train loss:  2.16877374e-02, mean val. rec. loss:  1.34861002e-02\n",
      "Epoch: 3239 mean train loss:  2.16826797e-02, mean val. rec. loss:  1.34813125e-02\n",
      "Epoch: 3240 mean train loss:  2.16776331e-02, mean val. rec. loss:  1.34765169e-02\n",
      "Epoch: 3241 mean train loss:  2.16725717e-02, mean val. rec. loss:  1.34717304e-02\n",
      "Epoch: 3242 mean train loss:  2.16675252e-02, mean val. rec. loss:  1.34669496e-02\n",
      "Epoch: 3243 mean train loss:  2.16624786e-02, mean val. rec. loss:  1.34621755e-02\n",
      "Epoch: 3244 mean train loss:  2.16574395e-02, mean val. rec. loss:  1.34574207e-02\n",
      "Epoch: 3245 mean train loss:  2.16523837e-02, mean val. rec. loss:  1.34526524e-02\n",
      "Epoch: 3246 mean train loss:  2.16473465e-02, mean val. rec. loss:  1.34478829e-02\n",
      "Epoch: 3247 mean train loss:  2.16422962e-02, mean val. rec. loss:  1.34431088e-02\n",
      "Epoch: 3248 mean train loss:  2.16372627e-02, mean val. rec. loss:  1.34383325e-02\n",
      "Epoch: 3249 mean train loss:  2.16322218e-02, mean val. rec. loss:  1.34335562e-02\n",
      "Epoch: 3250 mean train loss:  2.16271827e-02, mean val. rec. loss:  1.34287663e-02\n",
      "Epoch: 3251 mean train loss:  2.16221492e-02, mean val. rec. loss:  1.34239809e-02\n",
      "Epoch: 3252 mean train loss:  2.16171119e-02, mean val. rec. loss:  1.34192035e-02\n",
      "Epoch: 3253 mean train loss:  2.16120747e-02, mean val. rec. loss:  1.34144351e-02\n",
      "Epoch: 3254 mean train loss:  2.16070487e-02, mean val. rec. loss:  1.34096690e-02\n",
      "Epoch: 3255 mean train loss:  2.16020245e-02, mean val. rec. loss:  1.34049188e-02\n",
      "Epoch: 3256 mean train loss:  2.15969966e-02, mean val. rec. loss:  1.34001663e-02\n",
      "Epoch: 3257 mean train loss:  2.15919649e-02, mean val. rec. loss:  1.33954047e-02\n",
      "Epoch: 3258 mean train loss:  2.15869370e-02, mean val. rec. loss:  1.33906488e-02\n",
      "Epoch: 3259 mean train loss:  2.15819184e-02, mean val. rec. loss:  1.33858861e-02\n",
      "Epoch: 3260 mean train loss:  2.15768923e-02, mean val. rec. loss:  1.33811347e-02\n",
      "Epoch: 3261 mean train loss:  2.15718626e-02, mean val. rec. loss:  1.33763856e-02\n",
      "Epoch: 3262 mean train loss:  2.15668496e-02, mean val. rec. loss:  1.33716275e-02\n",
      "Epoch: 3263 mean train loss:  2.15618365e-02, mean val. rec. loss:  1.33668670e-02\n",
      "Epoch: 3264 mean train loss:  2.15568217e-02, mean val. rec. loss:  1.33621180e-02\n",
      "Epoch: 3265 mean train loss:  2.15517975e-02, mean val. rec. loss:  1.33573575e-02\n",
      "Epoch: 3266 mean train loss:  2.15467919e-02, mean val. rec. loss:  1.33526175e-02\n",
      "Epoch: 3267 mean train loss:  2.15417733e-02, mean val. rec. loss:  1.33478673e-02\n",
      "Epoch: 3268 mean train loss:  2.15367733e-02, mean val. rec. loss:  1.33431341e-02\n",
      "Epoch: 3269 mean train loss:  2.15317566e-02, mean val. rec. loss:  1.33383884e-02\n",
      "Epoch: 3270 mean train loss:  2.15267585e-02, mean val. rec. loss:  1.33336529e-02\n",
      "Epoch: 3271 mean train loss:  2.15217510e-02, mean val. rec. loss:  1.33289151e-02\n",
      "Epoch: 3272 mean train loss:  2.15167436e-02, mean val. rec. loss:  1.33241819e-02\n",
      "Epoch: 3273 mean train loss:  2.15117548e-02, mean val. rec. loss:  1.33194351e-02\n",
      "Epoch: 3274 mean train loss:  2.15067548e-02, mean val. rec. loss:  1.33146860e-02\n",
      "Epoch: 3275 mean train loss:  2.15017530e-02, mean val. rec. loss:  1.33099494e-02\n",
      "Epoch: 3276 mean train loss:  2.14967567e-02, mean val. rec. loss:  1.33052195e-02\n",
      "Epoch: 3277 mean train loss:  2.14917604e-02, mean val. rec. loss:  1.33004999e-02\n",
      "Epoch: 3278 mean train loss:  2.14867661e-02, mean val. rec. loss:  1.32957803e-02\n",
      "Epoch: 3279 mean train loss:  2.14817754e-02, mean val. rec. loss:  1.32910686e-02\n",
      "Epoch: 3280 mean train loss:  2.14767903e-02, mean val. rec. loss:  1.32863547e-02\n",
      "Epoch: 3281 mean train loss:  2.14718034e-02, mean val. rec. loss:  1.32816396e-02\n",
      "Epoch: 3282 mean train loss:  2.14668220e-02, mean val. rec. loss:  1.32769246e-02\n",
      "Epoch: 3283 mean train loss:  2.14618276e-02, mean val. rec. loss:  1.32722072e-02\n",
      "Epoch: 3284 mean train loss:  2.14568481e-02, mean val. rec. loss:  1.32674808e-02\n",
      "Epoch: 3285 mean train loss:  2.14518723e-02, mean val. rec. loss:  1.32627589e-02\n",
      "Epoch: 3286 mean train loss:  2.14468817e-02, mean val. rec. loss:  1.32580461e-02\n",
      "Epoch: 3287 mean train loss:  2.14419133e-02, mean val. rec. loss:  1.32533265e-02\n",
      "Epoch: 3288 mean train loss:  2.14369282e-02, mean val. rec. loss:  1.32486171e-02\n",
      "Epoch: 3289 mean train loss:  2.14319618e-02, mean val. rec. loss:  1.32439168e-02\n",
      "Epoch: 3290 mean train loss:  2.14269860e-02, mean val. rec. loss:  1.32392221e-02\n",
      "Epoch: 3291 mean train loss:  2.14220270e-02, mean val. rec. loss:  1.32345229e-02\n",
      "Epoch: 3292 mean train loss:  2.14170568e-02, mean val. rec. loss:  1.32298362e-02\n",
      "Epoch: 3293 mean train loss:  2.14120885e-02, mean val. rec. loss:  1.32251381e-02\n",
      "Epoch: 3294 mean train loss:  2.14071183e-02, mean val. rec. loss:  1.32204401e-02\n",
      "Epoch: 3295 mean train loss:  2.14021612e-02, mean val. rec. loss:  1.32157443e-02\n",
      "Epoch: 3296 mean train loss:  2.13972059e-02, mean val. rec. loss:  1.32110587e-02\n",
      "Epoch: 3297 mean train loss:  2.13922487e-02, mean val. rec. loss:  1.32063663e-02\n",
      "Epoch: 3298 mean train loss:  2.13872878e-02, mean val. rec. loss:  1.32016625e-02\n",
      "Epoch: 3299 mean train loss:  2.13823363e-02, mean val. rec. loss:  1.31969679e-02\n",
      "Epoch: 3300 mean train loss:  2.13773717e-02, mean val. rec. loss:  1.31922834e-02\n",
      "Epoch: 3301 mean train loss:  2.13724201e-02, mean val. rec. loss:  1.31875933e-02\n",
      "Epoch: 3302 mean train loss:  2.13674741e-02, mean val. rec. loss:  1.31829156e-02\n",
      "Epoch: 3303 mean train loss:  2.13625319e-02, mean val. rec. loss:  1.31782346e-02\n",
      "Epoch: 3304 mean train loss:  2.13575878e-02, mean val. rec. loss:  1.31735717e-02\n",
      "Epoch: 3305 mean train loss:  2.13526362e-02, mean val. rec. loss:  1.31689076e-02\n",
      "Epoch: 3306 mean train loss:  2.13477051e-02, mean val. rec. loss:  1.31642436e-02\n",
      "Epoch: 3307 mean train loss:  2.13427535e-02, mean val. rec. loss:  1.31595761e-02\n",
      "Epoch: 3308 mean train loss:  2.13378243e-02, mean val. rec. loss:  1.31549007e-02\n",
      "Epoch: 3309 mean train loss:  2.13328784e-02, mean val. rec. loss:  1.31502367e-02\n",
      "Epoch: 3310 mean train loss:  2.13279566e-02, mean val. rec. loss:  1.31455624e-02\n",
      "Epoch: 3311 mean train loss:  2.13230162e-02, mean val. rec. loss:  1.31408950e-02\n",
      "Epoch: 3312 mean train loss:  2.13180832e-02, mean val. rec. loss:  1.31362321e-02\n",
      "Epoch: 3313 mean train loss:  2.13131689e-02, mean val. rec. loss:  1.31315816e-02\n",
      "Epoch: 3314 mean train loss:  2.13082416e-02, mean val. rec. loss:  1.31269357e-02\n",
      "Epoch: 3315 mean train loss:  2.13033142e-02, mean val. rec. loss:  1.31222819e-02\n",
      "Epoch: 3316 mean train loss:  2.12983887e-02, mean val. rec. loss:  1.31176485e-02\n",
      "Epoch: 3317 mean train loss:  2.12934688e-02, mean val. rec. loss:  1.31130014e-02\n",
      "Epoch: 3318 mean train loss:  2.12885508e-02, mean val. rec. loss:  1.31083669e-02\n",
      "Epoch: 3319 mean train loss:  2.12836364e-02, mean val. rec. loss:  1.31037210e-02\n",
      "Epoch: 3320 mean train loss:  2.12787221e-02, mean val. rec. loss:  1.30990830e-02\n",
      "Epoch: 3321 mean train loss:  2.12738171e-02, mean val. rec. loss:  1.30944484e-02\n",
      "Epoch: 3322 mean train loss:  2.12689121e-02, mean val. rec. loss:  1.30898116e-02\n",
      "Epoch: 3323 mean train loss:  2.12639959e-02, mean val. rec. loss:  1.30851827e-02\n",
      "Epoch: 3324 mean train loss:  2.12590928e-02, mean val. rec. loss:  1.30805482e-02\n",
      "Epoch: 3325 mean train loss:  2.12541971e-02, mean val. rec. loss:  1.30759125e-02\n",
      "Epoch: 3326 mean train loss:  2.12492865e-02, mean val. rec. loss:  1.30712824e-02\n",
      "Epoch: 3327 mean train loss:  2.12443945e-02, mean val. rec. loss:  1.30666615e-02\n",
      "Epoch: 3328 mean train loss:  2.12394951e-02, mean val. rec. loss:  1.30620417e-02\n",
      "Epoch: 3329 mean train loss:  2.12346087e-02, mean val. rec. loss:  1.30574162e-02\n",
      "Epoch: 3330 mean train loss:  2.12297130e-02, mean val. rec. loss:  1.30528009e-02\n",
      "Epoch: 3331 mean train loss:  2.12248192e-02, mean val. rec. loss:  1.30481924e-02\n",
      "Epoch: 3332 mean train loss:  2.12199439e-02, mean val. rec. loss:  1.30435987e-02\n",
      "Epoch: 3333 mean train loss:  2.12150576e-02, mean val. rec. loss:  1.30389936e-02\n",
      "Epoch: 3334 mean train loss:  2.12101712e-02, mean val. rec. loss:  1.30343964e-02\n",
      "Epoch: 3335 mean train loss:  2.12052885e-02, mean val. rec. loss:  1.30297914e-02\n",
      "Epoch: 3336 mean train loss:  2.12004096e-02, mean val. rec. loss:  1.30251840e-02\n",
      "Epoch: 3337 mean train loss:  2.11955344e-02, mean val. rec. loss:  1.30205846e-02\n",
      "Epoch: 3338 mean train loss:  2.11906591e-02, mean val. rec. loss:  1.30159863e-02\n",
      "Epoch: 3339 mean train loss:  2.11857914e-02, mean val. rec. loss:  1.30113813e-02\n",
      "Epoch: 3340 mean train loss:  2.11809255e-02, mean val. rec. loss:  1.30068000e-02\n",
      "Epoch: 3341 mean train loss:  2.11760596e-02, mean val. rec. loss:  1.30022278e-02\n",
      "Epoch: 3342 mean train loss:  2.11712030e-02, mean val. rec. loss:  1.29976533e-02\n",
      "Epoch: 3343 mean train loss:  2.11663259e-02, mean val. rec. loss:  1.29930868e-02\n",
      "Epoch: 3344 mean train loss:  2.11614730e-02, mean val. rec. loss:  1.29885180e-02\n",
      "Epoch: 3345 mean train loss:  2.11566239e-02, mean val. rec. loss:  1.29839458e-02\n",
      "Epoch: 3346 mean train loss:  2.11517598e-02, mean val. rec. loss:  1.29793714e-02\n",
      "Epoch: 3347 mean train loss:  2.11469181e-02, mean val. rec. loss:  1.29747878e-02\n",
      "Epoch: 3348 mean train loss:  2.11420597e-02, mean val. rec. loss:  1.29702088e-02\n",
      "Epoch: 3349 mean train loss:  2.11372124e-02, mean val. rec. loss:  1.29656389e-02\n",
      "Epoch: 3350 mean train loss:  2.11323726e-02, mean val. rec. loss:  1.29610769e-02\n",
      "Epoch: 3351 mean train loss:  2.11275272e-02, mean val. rec. loss:  1.29565229e-02\n",
      "Epoch: 3352 mean train loss:  2.11226855e-02, mean val. rec. loss:  1.29519666e-02\n",
      "Epoch: 3353 mean train loss:  2.11178624e-02, mean val. rec. loss:  1.29474318e-02\n",
      "Epoch: 3354 mean train loss:  2.11130226e-02, mean val. rec. loss:  1.29428913e-02\n",
      "Epoch: 3355 mean train loss:  2.11081939e-02, mean val. rec. loss:  1.29383430e-02\n",
      "Epoch: 3356 mean train loss:  2.11033634e-02, mean val. rec. loss:  1.29338059e-02\n",
      "Epoch: 3357 mean train loss:  2.10985366e-02, mean val. rec. loss:  1.29292723e-02\n",
      "Epoch: 3358 mean train loss:  2.10937135e-02, mean val. rec. loss:  1.29247296e-02\n",
      "Epoch: 3359 mean train loss:  2.10888960e-02, mean val. rec. loss:  1.29202005e-02\n",
      "Epoch: 3360 mean train loss:  2.10840785e-02, mean val. rec. loss:  1.29156714e-02\n",
      "Epoch: 3361 mean train loss:  2.10792517e-02, mean val. rec. loss:  1.29111332e-02\n",
      "Epoch: 3362 mean train loss:  2.10744436e-02, mean val. rec. loss:  1.29065973e-02\n",
      "Epoch: 3363 mean train loss:  2.10696391e-02, mean val. rec. loss:  1.29020750e-02\n",
      "Epoch: 3364 mean train loss:  2.10648291e-02, mean val. rec. loss:  1.28975527e-02\n",
      "Epoch: 3365 mean train loss:  2.10600228e-02, mean val. rec. loss:  1.28930485e-02\n",
      "Epoch: 3366 mean train loss:  2.10552257e-02, mean val. rec. loss:  1.28885523e-02\n",
      "Epoch: 3367 mean train loss:  2.10504213e-02, mean val. rec. loss:  1.28840708e-02\n",
      "Epoch: 3368 mean train loss:  2.10456243e-02, mean val. rec. loss:  1.28795610e-02\n",
      "Epoch: 3369 mean train loss:  2.10408329e-02, mean val. rec. loss:  1.28750512e-02\n",
      "Epoch: 3370 mean train loss:  2.10360377e-02, mean val. rec. loss:  1.28705538e-02\n",
      "Epoch: 3371 mean train loss:  2.10312519e-02, mean val. rec. loss:  1.28660327e-02\n",
      "Epoch: 3372 mean train loss:  2.10264698e-02, mean val. rec. loss:  1.28615399e-02\n",
      "Epoch: 3373 mean train loss:  2.10216858e-02, mean val. rec. loss:  1.28570607e-02\n",
      "Epoch: 3374 mean train loss:  2.10169037e-02, mean val. rec. loss:  1.28525780e-02\n",
      "Epoch: 3375 mean train loss:  2.10121235e-02, mean val. rec. loss:  1.28481124e-02\n",
      "Epoch: 3376 mean train loss:  2.10073507e-02, mean val. rec. loss:  1.28436366e-02\n",
      "Epoch: 3377 mean train loss:  2.10025779e-02, mean val. rec. loss:  1.28391676e-02\n",
      "Epoch: 3378 mean train loss:  2.09978107e-02, mean val. rec. loss:  1.28346918e-02\n",
      "Epoch: 3379 mean train loss:  2.09930491e-02, mean val. rec. loss:  1.28302172e-02\n",
      "Epoch: 3380 mean train loss:  2.09882874e-02, mean val. rec. loss:  1.28257504e-02\n",
      "Epoch: 3381 mean train loss:  2.09835240e-02, mean val. rec. loss:  1.28212848e-02\n",
      "Epoch: 3382 mean train loss:  2.09787642e-02, mean val. rec. loss:  1.28168272e-02\n",
      "Epoch: 3383 mean train loss:  2.09740156e-02, mean val. rec. loss:  1.28123672e-02\n",
      "Epoch: 3384 mean train loss:  2.09692633e-02, mean val. rec. loss:  1.28079130e-02\n",
      "Epoch: 3385 mean train loss:  2.09645166e-02, mean val. rec. loss:  1.28034667e-02\n",
      "Epoch: 3386 mean train loss:  2.09597754e-02, mean val. rec. loss:  1.27990453e-02\n",
      "Epoch: 3387 mean train loss:  2.09550324e-02, mean val. rec. loss:  1.27946012e-02\n",
      "Epoch: 3388 mean train loss:  2.09502876e-02, mean val. rec. loss:  1.27901674e-02\n",
      "Epoch: 3389 mean train loss:  2.09455632e-02, mean val. rec. loss:  1.27857449e-02\n",
      "Epoch: 3390 mean train loss:  2.09408277e-02, mean val. rec. loss:  1.27813178e-02\n",
      "Epoch: 3391 mean train loss:  2.09360940e-02, mean val. rec. loss:  1.27768806e-02\n",
      "Epoch: 3392 mean train loss:  2.09313752e-02, mean val. rec. loss:  1.27724456e-02\n",
      "Epoch: 3393 mean train loss:  2.09266527e-02, mean val. rec. loss:  1.27680231e-02\n",
      "Epoch: 3394 mean train loss:  2.09219339e-02, mean val. rec. loss:  1.27635960e-02\n",
      "Epoch: 3395 mean train loss:  2.09172188e-02, mean val. rec. loss:  1.27591781e-02\n",
      "Epoch: 3396 mean train loss:  2.09125056e-02, mean val. rec. loss:  1.27547805e-02\n",
      "Epoch: 3397 mean train loss:  2.09077943e-02, mean val. rec. loss:  1.27503739e-02\n",
      "Epoch: 3398 mean train loss:  2.09030866e-02, mean val. rec. loss:  1.27459944e-02\n",
      "Epoch: 3399 mean train loss:  2.08983809e-02, mean val. rec. loss:  1.27416082e-02\n",
      "Epoch: 3400 mean train loss:  2.08936733e-02, mean val. rec. loss:  1.27372379e-02\n",
      "Epoch: 3401 mean train loss:  2.08889806e-02, mean val. rec. loss:  1.27328426e-02\n",
      "Epoch: 3402 mean train loss:  2.08842916e-02, mean val. rec. loss:  1.27284496e-02\n",
      "Epoch: 3403 mean train loss:  2.08795877e-02, mean val. rec. loss:  1.27240679e-02\n",
      "Epoch: 3404 mean train loss:  2.08749080e-02, mean val. rec. loss:  1.27196805e-02\n",
      "Epoch: 3405 mean train loss:  2.08702246e-02, mean val. rec. loss:  1.27153056e-02\n",
      "Epoch: 3406 mean train loss:  2.08655449e-02, mean val. rec. loss:  1.27109330e-02\n",
      "Epoch: 3407 mean train loss:  2.08608615e-02, mean val. rec. loss:  1.27065661e-02\n",
      "Epoch: 3408 mean train loss:  2.08561985e-02, mean val. rec. loss:  1.27022093e-02\n",
      "Epoch: 3409 mean train loss:  2.08515226e-02, mean val. rec. loss:  1.26978526e-02\n",
      "Epoch: 3410 mean train loss:  2.08468485e-02, mean val. rec. loss:  1.26935140e-02\n",
      "Epoch: 3411 mean train loss:  2.08421949e-02, mean val. rec. loss:  1.26891709e-02\n",
      "Epoch: 3412 mean train loss:  2.08375357e-02, mean val. rec. loss:  1.26848323e-02\n",
      "Epoch: 3413 mean train loss:  2.08328765e-02, mean val. rec. loss:  1.26804846e-02\n",
      "Epoch: 3414 mean train loss:  2.08282210e-02, mean val. rec. loss:  1.26761551e-02\n",
      "Epoch: 3415 mean train loss:  2.08235730e-02, mean val. rec. loss:  1.26718074e-02\n",
      "Epoch: 3416 mean train loss:  2.08189231e-02, mean val. rec. loss:  1.26674847e-02\n",
      "Epoch: 3417 mean train loss:  2.08142844e-02, mean val. rec. loss:  1.26631541e-02\n",
      "Epoch: 3418 mean train loss:  2.08096475e-02, mean val. rec. loss:  1.26588143e-02\n",
      "Epoch: 3419 mean train loss:  2.08050107e-02, mean val. rec. loss:  1.26544961e-02\n",
      "Epoch: 3420 mean train loss:  2.08003701e-02, mean val. rec. loss:  1.26501836e-02\n",
      "Epoch: 3421 mean train loss:  2.07957444e-02, mean val. rec. loss:  1.26458609e-02\n",
      "Epoch: 3422 mean train loss:  2.07911224e-02, mean val. rec. loss:  1.26415813e-02\n",
      "Epoch: 3423 mean train loss:  2.07864949e-02, mean val. rec. loss:  1.26372915e-02\n",
      "Epoch: 3424 mean train loss:  2.07818823e-02, mean val. rec. loss:  1.26329914e-02\n",
      "Epoch: 3425 mean train loss:  2.07772715e-02, mean val. rec. loss:  1.26287084e-02\n",
      "Epoch: 3426 mean train loss:  2.07726588e-02, mean val. rec. loss:  1.26244242e-02\n",
      "Epoch: 3427 mean train loss:  2.07680387e-02, mean val. rec. loss:  1.26201503e-02\n",
      "Epoch: 3428 mean train loss:  2.07634466e-02, mean val. rec. loss:  1.26158593e-02\n",
      "Epoch: 3429 mean train loss:  2.07588451e-02, mean val. rec. loss:  1.26115774e-02\n",
      "Epoch: 3430 mean train loss:  2.07542418e-02, mean val. rec. loss:  1.26072978e-02\n",
      "Epoch: 3431 mean train loss:  2.07496626e-02, mean val. rec. loss:  1.26030272e-02\n",
      "Epoch: 3432 mean train loss:  2.07450705e-02, mean val. rec. loss:  1.25987680e-02\n",
      "Epoch: 3433 mean train loss:  2.07404820e-02, mean val. rec. loss:  1.25945167e-02\n",
      "Epoch: 3434 mean train loss:  2.07359048e-02, mean val. rec. loss:  1.25902722e-02\n",
      "Epoch: 3435 mean train loss:  2.07313275e-02, mean val. rec. loss:  1.25860255e-02\n",
      "Epoch: 3436 mean train loss:  2.07267502e-02, mean val. rec. loss:  1.25817901e-02\n",
      "Epoch: 3437 mean train loss:  2.07221841e-02, mean val. rec. loss:  1.25775513e-02\n",
      "Epoch: 3438 mean train loss:  2.07176162e-02, mean val. rec. loss:  1.25733193e-02\n",
      "Epoch: 3439 mean train loss:  2.07130445e-02, mean val. rec. loss:  1.25690873e-02\n",
      "Epoch: 3440 mean train loss:  2.07084933e-02, mean val. rec. loss:  1.25648394e-02\n",
      "Epoch: 3441 mean train loss:  2.07039384e-02, mean val. rec. loss:  1.25606278e-02\n",
      "Epoch: 3442 mean train loss:  2.06993872e-02, mean val. rec. loss:  1.25563958e-02\n",
      "Epoch: 3443 mean train loss:  2.06948360e-02, mean val. rec. loss:  1.25521933e-02\n",
      "Epoch: 3444 mean train loss:  2.06902997e-02, mean val. rec. loss:  1.25479863e-02\n",
      "Epoch: 3445 mean train loss:  2.06857560e-02, mean val. rec. loss:  1.25437803e-02\n",
      "Epoch: 3446 mean train loss:  2.06812271e-02, mean val. rec. loss:  1.25395869e-02\n",
      "Epoch: 3447 mean train loss:  2.06766908e-02, mean val. rec. loss:  1.25353946e-02\n",
      "Epoch: 3448 mean train loss:  2.06721583e-02, mean val. rec. loss:  1.25312102e-02\n",
      "Epoch: 3449 mean train loss:  2.06676443e-02, mean val. rec. loss:  1.25270213e-02\n",
      "Epoch: 3450 mean train loss:  2.06631229e-02, mean val. rec. loss:  1.25228369e-02\n",
      "Epoch: 3451 mean train loss:  2.06586015e-02, mean val. rec. loss:  1.25186537e-02\n",
      "Epoch: 3452 mean train loss:  2.06540913e-02, mean val. rec. loss:  1.25144682e-02\n",
      "Epoch: 3453 mean train loss:  2.06495848e-02, mean val. rec. loss:  1.25103008e-02\n",
      "Epoch: 3454 mean train loss:  2.06450783e-02, mean val. rec. loss:  1.25061471e-02\n",
      "Epoch: 3455 mean train loss:  2.06405792e-02, mean val. rec. loss:  1.25019786e-02\n",
      "Epoch: 3456 mean train loss:  2.06360802e-02, mean val. rec. loss:  1.24978225e-02\n",
      "Epoch: 3457 mean train loss:  2.06315923e-02, mean val. rec. loss:  1.24936654e-02\n",
      "Epoch: 3458 mean train loss:  2.06271081e-02, mean val. rec. loss:  1.24895218e-02\n",
      "Epoch: 3459 mean train loss:  2.06226277e-02, mean val. rec. loss:  1.24853976e-02\n",
      "Epoch: 3460 mean train loss:  2.06181510e-02, mean val. rec. loss:  1.24812653e-02\n",
      "Epoch: 3461 mean train loss:  2.06136669e-02, mean val. rec. loss:  1.24771569e-02\n",
      "Epoch: 3462 mean train loss:  2.06092013e-02, mean val. rec. loss:  1.24730281e-02\n",
      "Epoch: 3463 mean train loss:  2.06047339e-02, mean val. rec. loss:  1.24689118e-02\n",
      "Epoch: 3464 mean train loss:  2.06002703e-02, mean val. rec. loss:  1.24648011e-02\n",
      "Epoch: 3465 mean train loss:  2.05958085e-02, mean val. rec. loss:  1.24606701e-02\n",
      "Epoch: 3466 mean train loss:  2.05913578e-02, mean val. rec. loss:  1.24565651e-02\n",
      "Epoch: 3467 mean train loss:  2.05868997e-02, mean val. rec. loss:  1.24524589e-02\n",
      "Epoch: 3468 mean train loss:  2.05824677e-02, mean val. rec. loss:  1.24483573e-02\n",
      "Epoch: 3469 mean train loss:  2.05780208e-02, mean val. rec. loss:  1.24442671e-02\n",
      "Epoch: 3470 mean train loss:  2.05735776e-02, mean val. rec. loss:  1.24401836e-02\n",
      "Epoch: 3471 mean train loss:  2.05691419e-02, mean val. rec. loss:  1.24361115e-02\n",
      "Epoch: 3472 mean train loss:  2.05647136e-02, mean val. rec. loss:  1.24320304e-02\n",
      "Epoch: 3473 mean train loss:  2.05602928e-02, mean val. rec. loss:  1.24279616e-02\n",
      "Epoch: 3474 mean train loss:  2.05558719e-02, mean val. rec. loss:  1.24238929e-02\n",
      "Epoch: 3475 mean train loss:  2.05514548e-02, mean val. rec. loss:  1.24198310e-02\n",
      "Epoch: 3476 mean train loss:  2.05470358e-02, mean val. rec. loss:  1.24157623e-02\n",
      "Epoch: 3477 mean train loss:  2.05426317e-02, mean val. rec. loss:  1.24117118e-02\n",
      "Epoch: 3478 mean train loss:  2.05382295e-02, mean val. rec. loss:  1.24076567e-02\n",
      "Epoch: 3479 mean train loss:  2.05338310e-02, mean val. rec. loss:  1.24036163e-02\n",
      "Epoch: 3480 mean train loss:  2.05294437e-02, mean val. rec. loss:  1.23995782e-02\n",
      "Epoch: 3481 mean train loss:  2.05250415e-02, mean val. rec. loss:  1.23955503e-02\n",
      "Epoch: 3482 mean train loss:  2.05206616e-02, mean val. rec. loss:  1.23915315e-02\n",
      "Epoch: 3483 mean train loss:  2.05162743e-02, mean val. rec. loss:  1.23875116e-02\n",
      "Epoch: 3484 mean train loss:  2.05119000e-02, mean val. rec. loss:  1.23834791e-02\n",
      "Epoch: 3485 mean train loss:  2.05075257e-02, mean val. rec. loss:  1.23794751e-02\n",
      "Epoch: 3486 mean train loss:  2.05031607e-02, mean val. rec. loss:  1.23754506e-02\n",
      "Epoch: 3487 mean train loss:  2.04987902e-02, mean val. rec. loss:  1.23714420e-02\n",
      "Epoch: 3488 mean train loss:  2.04944401e-02, mean val. rec. loss:  1.23674334e-02\n",
      "Epoch: 3489 mean train loss:  2.04900770e-02, mean val. rec. loss:  1.23634361e-02\n",
      "Epoch: 3490 mean train loss:  2.04857195e-02, mean val. rec. loss:  1.23594479e-02\n",
      "Epoch: 3491 mean train loss:  2.04813731e-02, mean val. rec. loss:  1.23554665e-02\n",
      "Epoch: 3492 mean train loss:  2.04770268e-02, mean val. rec. loss:  1.23515089e-02\n",
      "Epoch: 3493 mean train loss:  2.04726990e-02, mean val. rec. loss:  1.23475366e-02\n",
      "Epoch: 3494 mean train loss:  2.04683601e-02, mean val. rec. loss:  1.23435643e-02\n",
      "Epoch: 3495 mean train loss:  2.04640324e-02, mean val. rec. loss:  1.23395999e-02\n",
      "Epoch: 3496 mean train loss:  2.04597009e-02, mean val. rec. loss:  1.23356389e-02\n",
      "Epoch: 3497 mean train loss:  2.04553844e-02, mean val. rec. loss:  1.23316870e-02\n",
      "Epoch: 3498 mean train loss:  2.04510678e-02, mean val. rec. loss:  1.23277385e-02\n",
      "Epoch: 3499 mean train loss:  2.04467587e-02, mean val. rec. loss:  1.23237946e-02\n",
      "Epoch: 3500 mean train loss:  2.04424552e-02, mean val. rec. loss:  1.23198563e-02\n",
      "Epoch: 3501 mean train loss:  2.04381424e-02, mean val. rec. loss:  1.23159304e-02\n",
      "Epoch: 3502 mean train loss:  2.04338463e-02, mean val. rec. loss:  1.23119876e-02\n",
      "Epoch: 3503 mean train loss:  2.04295521e-02, mean val. rec. loss:  1.23080674e-02\n",
      "Epoch: 3504 mean train loss:  2.04252616e-02, mean val. rec. loss:  1.23041575e-02\n",
      "Epoch: 3505 mean train loss:  2.04209748e-02, mean val. rec. loss:  1.23002430e-02\n",
      "Epoch: 3506 mean train loss:  2.04166974e-02, mean val. rec. loss:  1.22963342e-02\n",
      "Epoch: 3507 mean train loss:  2.04124162e-02, mean val. rec. loss:  1.22924322e-02\n",
      "Epoch: 3508 mean train loss:  2.04081555e-02, mean val. rec. loss:  1.22885370e-02\n",
      "Epoch: 3509 mean train loss:  2.04038836e-02, mean val. rec. loss:  1.22846451e-02\n",
      "Epoch: 3510 mean train loss:  2.03996192e-02, mean val. rec. loss:  1.22807681e-02\n",
      "Epoch: 3511 mean train loss:  2.03953585e-02, mean val. rec. loss:  1.22768774e-02\n",
      "Epoch: 3512 mean train loss:  2.03911016e-02, mean val. rec. loss:  1.22729992e-02\n",
      "Epoch: 3513 mean train loss:  2.03868576e-02, mean val. rec. loss:  1.22691233e-02\n",
      "Epoch: 3514 mean train loss:  2.03826137e-02, mean val. rec. loss:  1.22652519e-02\n",
      "Epoch: 3515 mean train loss:  2.03783754e-02, mean val. rec. loss:  1.22613850e-02\n",
      "Epoch: 3516 mean train loss:  2.03741333e-02, mean val. rec. loss:  1.22575250e-02\n",
      "Epoch: 3517 mean train loss:  2.03699080e-02, mean val. rec. loss:  1.22536717e-02\n",
      "Epoch: 3518 mean train loss:  2.03656827e-02, mean val. rec. loss:  1.22498343e-02\n",
      "Epoch: 3519 mean train loss:  2.03614648e-02, mean val. rec. loss:  1.22459936e-02\n",
      "Epoch: 3520 mean train loss:  2.03572525e-02, mean val. rec. loss:  1.22421709e-02\n",
      "Epoch: 3521 mean train loss:  2.03530291e-02, mean val. rec. loss:  1.22383449e-02\n",
      "Epoch: 3522 mean train loss:  2.03488298e-02, mean val. rec. loss:  1.22345325e-02\n",
      "Epoch: 3523 mean train loss:  2.03446250e-02, mean val. rec. loss:  1.22306962e-02\n",
      "Epoch: 3524 mean train loss:  2.03404276e-02, mean val. rec. loss:  1.22268702e-02\n",
      "Epoch: 3525 mean train loss:  2.03362340e-02, mean val. rec. loss:  1.22230521e-02\n",
      "Epoch: 3526 mean train loss:  2.03320478e-02, mean val. rec. loss:  1.22192430e-02\n",
      "Epoch: 3527 mean train loss:  2.03278541e-02, mean val. rec. loss:  1.22154442e-02\n",
      "Epoch: 3528 mean train loss:  2.03236847e-02, mean val. rec. loss:  1.22116420e-02\n",
      "Epoch: 3529 mean train loss:  2.03195078e-02, mean val. rec. loss:  1.22078681e-02\n",
      "Epoch: 3530 mean train loss:  2.03153328e-02, mean val. rec. loss:  1.22040943e-02\n",
      "Epoch: 3531 mean train loss:  2.03111615e-02, mean val. rec. loss:  1.22003158e-02\n",
      "Epoch: 3532 mean train loss:  2.03069995e-02, mean val. rec. loss:  1.21965431e-02\n",
      "Epoch: 3533 mean train loss:  2.03028524e-02, mean val. rec. loss:  1.21927828e-02\n",
      "Epoch: 3534 mean train loss:  2.02986960e-02, mean val. rec. loss:  1.21890033e-02\n",
      "Epoch: 3535 mean train loss:  2.02945507e-02, mean val. rec. loss:  1.21852430e-02\n",
      "Epoch: 3536 mean train loss:  2.02903980e-02, mean val. rec. loss:  1.21814896e-02\n",
      "Epoch: 3537 mean train loss:  2.02862640e-02, mean val. rec. loss:  1.21777395e-02\n",
      "Epoch: 3538 mean train loss:  2.02821299e-02, mean val. rec. loss:  1.21739985e-02\n",
      "Epoch: 3539 mean train loss:  2.02780070e-02, mean val. rec. loss:  1.21702530e-02\n",
      "Epoch: 3540 mean train loss:  2.02738860e-02, mean val. rec. loss:  1.21665222e-02\n",
      "Epoch: 3541 mean train loss:  2.02697538e-02, mean val. rec. loss:  1.21628050e-02\n",
      "Epoch: 3542 mean train loss:  2.02656477e-02, mean val. rec. loss:  1.21590833e-02\n",
      "Epoch: 3543 mean train loss:  2.02615378e-02, mean val. rec. loss:  1.21553877e-02\n",
      "Epoch: 3544 mean train loss:  2.02574298e-02, mean val. rec. loss:  1.21516841e-02\n",
      "Epoch: 3545 mean train loss:  2.02533218e-02, mean val. rec. loss:  1.21479896e-02\n",
      "Epoch: 3546 mean train loss:  2.02492362e-02, mean val. rec. loss:  1.21442838e-02\n",
      "Epoch: 3547 mean train loss:  2.02451375e-02, mean val. rec. loss:  1.21405893e-02\n",
      "Epoch: 3548 mean train loss:  2.02410612e-02, mean val. rec. loss:  1.21368925e-02\n",
      "Epoch: 3549 mean train loss:  2.02369718e-02, mean val. rec. loss:  1.21332094e-02\n",
      "Epoch: 3550 mean train loss:  2.02328899e-02, mean val. rec. loss:  1.21295103e-02\n",
      "Epoch: 3551 mean train loss:  2.02288154e-02, mean val. rec. loss:  1.21258385e-02\n",
      "Epoch: 3552 mean train loss:  2.02247428e-02, mean val. rec. loss:  1.21221678e-02\n",
      "Epoch: 3553 mean train loss:  2.02206851e-02, mean val. rec. loss:  1.21184938e-02\n",
      "Epoch: 3554 mean train loss:  2.02166255e-02, mean val. rec. loss:  1.21148401e-02\n",
      "Epoch: 3555 mean train loss:  2.02125696e-02, mean val. rec. loss:  1.21111898e-02\n",
      "Epoch: 3556 mean train loss:  2.02085175e-02, mean val. rec. loss:  1.21075452e-02\n",
      "Epoch: 3557 mean train loss:  2.02044728e-02, mean val. rec. loss:  1.21039029e-02\n",
      "Epoch: 3558 mean train loss:  2.02004356e-02, mean val. rec. loss:  1.21002753e-02\n",
      "Epoch: 3559 mean train loss:  2.01964058e-02, mean val. rec. loss:  1.20966432e-02\n",
      "Epoch: 3560 mean train loss:  2.01923760e-02, mean val. rec. loss:  1.20930133e-02\n",
      "Epoch: 3561 mean train loss:  2.01883425e-02, mean val. rec. loss:  1.20893880e-02\n",
      "Epoch: 3562 mean train loss:  2.01843239e-02, mean val. rec. loss:  1.20857649e-02\n",
      "Epoch: 3563 mean train loss:  2.01803128e-02, mean val. rec. loss:  1.20821487e-02\n",
      "Epoch: 3564 mean train loss:  2.01762942e-02, mean val. rec. loss:  1.20785472e-02\n",
      "Epoch: 3565 mean train loss:  2.01722979e-02, mean val. rec. loss:  1.20749298e-02\n",
      "Epoch: 3566 mean train loss:  2.01682830e-02, mean val. rec. loss:  1.20713407e-02\n",
      "Epoch: 3567 mean train loss:  2.01642849e-02, mean val. rec. loss:  1.20677483e-02\n",
      "Epoch: 3568 mean train loss:  2.01602980e-02, mean val. rec. loss:  1.20641559e-02\n",
      "Epoch: 3569 mean train loss:  2.01563036e-02, mean val. rec. loss:  1.20605850e-02\n",
      "Epoch: 3570 mean train loss:  2.01523147e-02, mean val. rec. loss:  1.20570084e-02\n",
      "Epoch: 3571 mean train loss:  2.01483483e-02, mean val. rec. loss:  1.20534523e-02\n",
      "Epoch: 3572 mean train loss:  2.01443725e-02, mean val. rec. loss:  1.20498825e-02\n",
      "Epoch: 3573 mean train loss:  2.01403986e-02, mean val. rec. loss:  1.20463241e-02\n",
      "Epoch: 3574 mean train loss:  2.01364321e-02, mean val. rec. loss:  1.20427611e-02\n",
      "Epoch: 3575 mean train loss:  2.01324731e-02, mean val. rec. loss:  1.20391982e-02\n",
      "Epoch: 3576 mean train loss:  2.01285178e-02, mean val. rec. loss:  1.20356488e-02\n",
      "Epoch: 3577 mean train loss:  2.01245662e-02, mean val. rec. loss:  1.20320995e-02\n",
      "Epoch: 3578 mean train loss:  2.01206221e-02, mean val. rec. loss:  1.20285728e-02\n",
      "Epoch: 3579 mean train loss:  2.01166668e-02, mean val. rec. loss:  1.20250598e-02\n",
      "Epoch: 3580 mean train loss:  2.01127376e-02, mean val. rec. loss:  1.20215422e-02\n",
      "Epoch: 3581 mean train loss:  2.01088047e-02, mean val. rec. loss:  1.20180393e-02\n",
      "Epoch: 3582 mean train loss:  2.01048754e-02, mean val. rec. loss:  1.20145251e-02\n",
      "Epoch: 3583 mean train loss:  2.01009499e-02, mean val. rec. loss:  1.20110064e-02\n",
      "Epoch: 3584 mean train loss:  2.00970356e-02, mean val. rec. loss:  1.20074967e-02\n",
      "Epoch: 3585 mean train loss:  2.00931101e-02, mean val. rec. loss:  1.20039927e-02\n",
      "Epoch: 3586 mean train loss:  2.00892144e-02, mean val. rec. loss:  1.20005069e-02\n",
      "Epoch: 3587 mean train loss:  2.00853001e-02, mean val. rec. loss:  1.19970086e-02\n",
      "Epoch: 3588 mean train loss:  2.00813969e-02, mean val. rec. loss:  1.19935284e-02\n",
      "Epoch: 3589 mean train loss:  2.00775087e-02, mean val. rec. loss:  1.19900584e-02\n",
      "Epoch: 3590 mean train loss:  2.00736093e-02, mean val. rec. loss:  1.19865919e-02\n",
      "Epoch: 3591 mean train loss:  2.00697210e-02, mean val. rec. loss:  1.19831366e-02\n",
      "Epoch: 3592 mean train loss:  2.00658458e-02, mean val. rec. loss:  1.19796689e-02\n",
      "Epoch: 3593 mean train loss:  2.00619706e-02, mean val. rec. loss:  1.19762251e-02\n",
      "Epoch: 3594 mean train loss:  2.00580972e-02, mean val. rec. loss:  1.19727653e-02\n",
      "Epoch: 3595 mean train loss:  2.00542276e-02, mean val. rec. loss:  1.19693101e-02\n",
      "Epoch: 3596 mean train loss:  2.00503654e-02, mean val. rec. loss:  1.19658684e-02\n",
      "Epoch: 3597 mean train loss:  2.00465069e-02, mean val. rec. loss:  1.19624257e-02\n",
      "Epoch: 3598 mean train loss:  2.00426485e-02, mean val. rec. loss:  1.19589988e-02\n",
      "Epoch: 3599 mean train loss:  2.00387900e-02, mean val. rec. loss:  1.19555651e-02\n",
      "Epoch: 3600 mean train loss:  2.00349483e-02, mean val. rec. loss:  1.19521473e-02\n",
      "Epoch: 3601 mean train loss:  2.00311085e-02, mean val. rec. loss:  1.19487273e-02\n",
      "Epoch: 3602 mean train loss:  2.00272780e-02, mean val. rec. loss:  1.19453163e-02\n",
      "Epoch: 3603 mean train loss:  2.00234363e-02, mean val. rec. loss:  1.19419109e-02\n",
      "Epoch: 3604 mean train loss:  2.00196132e-02, mean val. rec. loss:  1.19385181e-02\n",
      "Epoch: 3605 mean train loss:  2.00157827e-02, mean val. rec. loss:  1.19351195e-02\n",
      "Epoch: 3606 mean train loss:  2.00119745e-02, mean val. rec. loss:  1.19317221e-02\n",
      "Epoch: 3607 mean train loss:  2.00081495e-02, mean val. rec. loss:  1.19283429e-02\n",
      "Epoch: 3608 mean train loss:  2.00043507e-02, mean val. rec. loss:  1.19249591e-02\n",
      "Epoch: 3609 mean train loss:  2.00005406e-02, mean val. rec. loss:  1.19215776e-02\n",
      "Epoch: 3610 mean train loss:  1.99967399e-02, mean val. rec. loss:  1.19182029e-02\n",
      "Epoch: 3611 mean train loss:  1.99929522e-02, mean val. rec. loss:  1.19148350e-02\n",
      "Epoch: 3612 mean train loss:  1.99891533e-02, mean val. rec. loss:  1.19114761e-02\n",
      "Epoch: 3613 mean train loss:  1.99853656e-02, mean val. rec. loss:  1.19081230e-02\n",
      "Epoch: 3614 mean train loss:  1.99815817e-02, mean val. rec. loss:  1.19047789e-02\n",
      "Epoch: 3615 mean train loss:  1.99778033e-02, mean val. rec. loss:  1.19014234e-02\n",
      "Epoch: 3616 mean train loss:  1.99740323e-02, mean val. rec. loss:  1.18980725e-02\n",
      "Epoch: 3617 mean train loss:  1.99702614e-02, mean val. rec. loss:  1.18947364e-02\n",
      "Epoch: 3618 mean train loss:  1.99665017e-02, mean val. rec. loss:  1.18914059e-02\n",
      "Epoch: 3619 mean train loss:  1.99627419e-02, mean val. rec. loss:  1.18880867e-02\n",
      "Epoch: 3620 mean train loss:  1.99589914e-02, mean val. rec. loss:  1.18847642e-02\n",
      "Epoch: 3621 mean train loss:  1.99552447e-02, mean val. rec. loss:  1.18814541e-02\n",
      "Epoch: 3622 mean train loss:  1.99514943e-02, mean val. rec. loss:  1.18781383e-02\n",
      "Epoch: 3623 mean train loss:  1.99477494e-02, mean val. rec. loss:  1.18748294e-02\n",
      "Epoch: 3624 mean train loss:  1.99440213e-02, mean val. rec. loss:  1.18715307e-02\n",
      "Epoch: 3625 mean train loss:  1.99402839e-02, mean val. rec. loss:  1.18682331e-02\n",
      "Epoch: 3626 mean train loss:  1.99365576e-02, mean val. rec. loss:  1.18649275e-02\n",
      "Epoch: 3627 mean train loss:  1.99328388e-02, mean val. rec. loss:  1.18616447e-02\n",
      "Epoch: 3628 mean train loss:  1.99291145e-02, mean val. rec. loss:  1.18583607e-02\n",
      "Epoch: 3629 mean train loss:  1.99253938e-02, mean val. rec. loss:  1.18550755e-02\n",
      "Epoch: 3630 mean train loss:  1.99216955e-02, mean val. rec. loss:  1.18518108e-02\n",
      "Epoch: 3631 mean train loss:  1.99179823e-02, mean val. rec. loss:  1.18485495e-02\n",
      "Epoch: 3632 mean train loss:  1.99142765e-02, mean val. rec. loss:  1.18452882e-02\n",
      "Epoch: 3633 mean train loss:  1.99105931e-02, mean val. rec. loss:  1.18420269e-02\n",
      "Epoch: 3634 mean train loss:  1.99068948e-02, mean val. rec. loss:  1.18387792e-02\n",
      "Epoch: 3635 mean train loss:  1.99032058e-02, mean val. rec. loss:  1.18355292e-02\n",
      "Epoch: 3636 mean train loss:  1.98995205e-02, mean val. rec. loss:  1.18322883e-02\n",
      "Epoch: 3637 mean train loss:  1.98958427e-02, mean val. rec. loss:  1.18290360e-02\n",
      "Epoch: 3638 mean train loss:  1.98921630e-02, mean val. rec. loss:  1.18257985e-02\n",
      "Epoch: 3639 mean train loss:  1.98884926e-02, mean val. rec. loss:  1.18225678e-02\n",
      "Epoch: 3640 mean train loss:  1.98848278e-02, mean val. rec. loss:  1.18193360e-02\n",
      "Epoch: 3641 mean train loss:  1.98811705e-02, mean val. rec. loss:  1.18161076e-02\n",
      "Epoch: 3642 mean train loss:  1.98775187e-02, mean val. rec. loss:  1.18129086e-02\n",
      "Epoch: 3643 mean train loss:  1.98738484e-02, mean val. rec. loss:  1.18096972e-02\n",
      "Epoch: 3644 mean train loss:  1.98702022e-02, mean val. rec. loss:  1.18064926e-02\n",
      "Epoch: 3645 mean train loss:  1.98665635e-02, mean val. rec. loss:  1.18032902e-02\n",
      "Epoch: 3646 mean train loss:  1.98629154e-02, mean val. rec. loss:  1.18000924e-02\n",
      "Epoch: 3647 mean train loss:  1.98592860e-02, mean val. rec. loss:  1.17969037e-02\n",
      "Epoch: 3648 mean train loss:  1.98556548e-02, mean val. rec. loss:  1.17937149e-02\n",
      "Epoch: 3649 mean train loss:  1.98520272e-02, mean val. rec. loss:  1.17905330e-02\n",
      "Epoch: 3650 mean train loss:  1.98483997e-02, mean val. rec. loss:  1.17873488e-02\n",
      "Epoch: 3651 mean train loss:  1.98447833e-02, mean val. rec. loss:  1.17841827e-02\n",
      "Epoch: 3652 mean train loss:  1.98411632e-02, mean val. rec. loss:  1.17810121e-02\n",
      "Epoch: 3653 mean train loss:  1.98375636e-02, mean val. rec. loss:  1.17778506e-02\n",
      "Epoch: 3654 mean train loss:  1.98339528e-02, mean val. rec. loss:  1.17746981e-02\n",
      "Epoch: 3655 mean train loss:  1.98303476e-02, mean val. rec. loss:  1.17715411e-02\n",
      "Epoch: 3656 mean train loss:  1.98267461e-02, mean val. rec. loss:  1.17683932e-02\n",
      "Epoch: 3657 mean train loss:  1.98231521e-02, mean val. rec. loss:  1.17652396e-02\n",
      "Epoch: 3658 mean train loss:  1.98195655e-02, mean val. rec. loss:  1.17620917e-02\n",
      "Epoch: 3659 mean train loss:  1.98159864e-02, mean val. rec. loss:  1.17589619e-02\n",
      "Epoch: 3660 mean train loss:  1.98123998e-02, mean val. rec. loss:  1.17558356e-02\n",
      "Epoch: 3661 mean train loss:  1.98088281e-02, mean val. rec. loss:  1.17527081e-02\n",
      "Epoch: 3662 mean train loss:  1.98052490e-02, mean val. rec. loss:  1.17495874e-02\n",
      "Epoch: 3663 mean train loss:  1.98016810e-02, mean val. rec. loss:  1.17464803e-02\n",
      "Epoch: 3664 mean train loss:  1.97981168e-02, mean val. rec. loss:  1.17433698e-02\n",
      "Epoch: 3665 mean train loss:  1.97945619e-02, mean val. rec. loss:  1.17402695e-02\n",
      "Epoch: 3666 mean train loss:  1.97910107e-02, mean val. rec. loss:  1.17371794e-02\n",
      "Epoch: 3667 mean train loss:  1.97874502e-02, mean val. rec. loss:  1.17340746e-02\n",
      "Epoch: 3668 mean train loss:  1.97839083e-02, mean val. rec. loss:  1.17309743e-02\n",
      "Epoch: 3669 mean train loss:  1.97803627e-02, mean val. rec. loss:  1.17278785e-02\n",
      "Epoch: 3670 mean train loss:  1.97768245e-02, mean val. rec. loss:  1.17247975e-02\n",
      "Epoch: 3671 mean train loss:  1.97732863e-02, mean val. rec. loss:  1.17217119e-02\n",
      "Epoch: 3672 mean train loss:  1.97697575e-02, mean val. rec. loss:  1.17186275e-02\n",
      "Epoch: 3673 mean train loss:  1.97662249e-02, mean val. rec. loss:  1.17155703e-02\n",
      "Epoch: 3674 mean train loss:  1.97627091e-02, mean val. rec. loss:  1.17125199e-02\n",
      "Epoch: 3675 mean train loss:  1.97591896e-02, mean val. rec. loss:  1.17094627e-02\n",
      "Epoch: 3676 mean train loss:  1.97556682e-02, mean val. rec. loss:  1.17064044e-02\n",
      "Epoch: 3677 mean train loss:  1.97521691e-02, mean val. rec. loss:  1.17033585e-02\n",
      "Epoch: 3678 mean train loss:  1.97486607e-02, mean val. rec. loss:  1.17003093e-02\n",
      "Epoch: 3679 mean train loss:  1.97451561e-02, mean val. rec. loss:  1.16972623e-02\n",
      "Epoch: 3680 mean train loss:  1.97416589e-02, mean val. rec. loss:  1.16942210e-02\n",
      "Epoch: 3681 mean train loss:  1.97381617e-02, mean val. rec. loss:  1.16911774e-02\n",
      "Epoch: 3682 mean train loss:  1.97346682e-02, mean val. rec. loss:  1.16881394e-02\n",
      "Epoch: 3683 mean train loss:  1.97311804e-02, mean val. rec. loss:  1.16851026e-02\n",
      "Epoch: 3684 mean train loss:  1.97276999e-02, mean val. rec. loss:  1.16820795e-02\n",
      "Epoch: 3685 mean train loss:  1.97242251e-02, mean val. rec. loss:  1.16790767e-02\n",
      "Epoch: 3686 mean train loss:  1.97207521e-02, mean val. rec. loss:  1.16760762e-02\n",
      "Epoch: 3687 mean train loss:  1.97172866e-02, mean val. rec. loss:  1.16730870e-02\n",
      "Epoch: 3688 mean train loss:  1.97138266e-02, mean val. rec. loss:  1.16700911e-02\n",
      "Epoch: 3689 mean train loss:  1.97103499e-02, mean val. rec. loss:  1.16670872e-02\n",
      "Epoch: 3690 mean train loss:  1.97068974e-02, mean val. rec. loss:  1.16640855e-02\n",
      "Epoch: 3691 mean train loss:  1.97034468e-02, mean val. rec. loss:  1.16610884e-02\n",
      "Epoch: 3692 mean train loss:  1.96999868e-02, mean val. rec. loss:  1.16580823e-02\n",
      "Epoch: 3693 mean train loss:  1.96965492e-02, mean val. rec. loss:  1.16550954e-02\n",
      "Epoch: 3694 mean train loss:  1.96931042e-02, mean val. rec. loss:  1.16521164e-02\n",
      "Epoch: 3695 mean train loss:  1.96896665e-02, mean val. rec. loss:  1.16491488e-02\n",
      "Epoch: 3696 mean train loss:  1.96862364e-02, mean val. rec. loss:  1.16461982e-02\n",
      "Epoch: 3697 mean train loss:  1.96828044e-02, mean val. rec. loss:  1.16432362e-02\n",
      "Epoch: 3698 mean train loss:  1.96793724e-02, mean val. rec. loss:  1.16402800e-02\n",
      "Epoch: 3699 mean train loss:  1.96759608e-02, mean val. rec. loss:  1.16373282e-02\n",
      "Epoch: 3700 mean train loss:  1.96725381e-02, mean val. rec. loss:  1.16343685e-02\n",
      "Epoch: 3701 mean train loss:  1.96691229e-02, mean val. rec. loss:  1.16314168e-02\n",
      "Epoch: 3702 mean train loss:  1.96657039e-02, mean val. rec. loss:  1.16284719e-02\n",
      "Epoch: 3703 mean train loss:  1.96623035e-02, mean val. rec. loss:  1.16255281e-02\n",
      "Epoch: 3704 mean train loss:  1.96589069e-02, mean val. rec. loss:  1.16225911e-02\n",
      "Epoch: 3705 mean train loss:  1.96555047e-02, mean val. rec. loss:  1.16196654e-02\n",
      "Epoch: 3706 mean train loss:  1.96521099e-02, mean val. rec. loss:  1.16167375e-02\n",
      "Epoch: 3707 mean train loss:  1.96487207e-02, mean val. rec. loss:  1.16138163e-02\n",
      "Epoch: 3708 mean train loss:  1.96453297e-02, mean val. rec. loss:  1.16109100e-02\n",
      "Epoch: 3709 mean train loss:  1.96419498e-02, mean val. rec. loss:  1.16080081e-02\n",
      "Epoch: 3710 mean train loss:  1.96385643e-02, mean val. rec. loss:  1.16050915e-02\n",
      "Epoch: 3711 mean train loss:  1.96351826e-02, mean val. rec. loss:  1.16021829e-02\n",
      "Epoch: 3712 mean train loss:  1.96318120e-02, mean val. rec. loss:  1.15992754e-02\n",
      "Epoch: 3713 mean train loss:  1.96284470e-02, mean val. rec. loss:  1.15963599e-02\n",
      "Epoch: 3714 mean train loss:  1.96250895e-02, mean val. rec. loss:  1.15934683e-02\n",
      "Epoch: 3715 mean train loss:  1.96217152e-02, mean val. rec. loss:  1.15905744e-02\n",
      "Epoch: 3716 mean train loss:  1.96183689e-02, mean val. rec. loss:  1.15876963e-02\n",
      "Epoch: 3717 mean train loss:  1.96150113e-02, mean val. rec. loss:  1.15848364e-02\n",
      "Epoch: 3718 mean train loss:  1.96116612e-02, mean val. rec. loss:  1.15819584e-02\n",
      "Epoch: 3719 mean train loss:  1.96083149e-02, mean val. rec. loss:  1.15790940e-02\n",
      "Epoch: 3720 mean train loss:  1.96049741e-02, mean val. rec. loss:  1.15762296e-02\n",
      "Epoch: 3721 mean train loss:  1.96016296e-02, mean val. rec. loss:  1.15733742e-02\n",
      "Epoch: 3722 mean train loss:  1.95983075e-02, mean val. rec. loss:  1.15705086e-02\n",
      "Epoch: 3723 mean train loss:  1.95949667e-02, mean val. rec. loss:  1.15676454e-02\n",
      "Epoch: 3724 mean train loss:  1.95916371e-02, mean val. rec. loss:  1.15647945e-02\n",
      "Epoch: 3725 mean train loss:  1.95883224e-02, mean val. rec. loss:  1.15619460e-02\n",
      "Epoch: 3726 mean train loss:  1.95850021e-02, mean val. rec. loss:  1.15590929e-02\n",
      "Epoch: 3727 mean train loss:  1.95816837e-02, mean val. rec. loss:  1.15562636e-02\n",
      "Epoch: 3728 mean train loss:  1.95783690e-02, mean val. rec. loss:  1.15534344e-02\n",
      "Epoch: 3729 mean train loss:  1.95750562e-02, mean val. rec. loss:  1.15505971e-02\n",
      "Epoch: 3730 mean train loss:  1.95717564e-02, mean val. rec. loss:  1.15477690e-02\n",
      "Epoch: 3731 mean train loss:  1.95684547e-02, mean val. rec. loss:  1.15449477e-02\n",
      "Epoch: 3732 mean train loss:  1.95651605e-02, mean val. rec. loss:  1.15421399e-02\n",
      "Epoch: 3733 mean train loss:  1.95618644e-02, mean val. rec. loss:  1.15393300e-02\n",
      "Epoch: 3734 mean train loss:  1.95585702e-02, mean val. rec. loss:  1.15365166e-02\n",
      "Epoch: 3735 mean train loss:  1.95552834e-02, mean val. rec. loss:  1.15337054e-02\n",
      "Epoch: 3736 mean train loss:  1.95520060e-02, mean val. rec. loss:  1.15308954e-02\n",
      "Epoch: 3737 mean train loss:  1.95487248e-02, mean val. rec. loss:  1.15281013e-02\n",
      "Epoch: 3738 mean train loss:  1.95454567e-02, mean val. rec. loss:  1.15253117e-02\n",
      "Epoch: 3739 mean train loss:  1.95421829e-02, mean val. rec. loss:  1.15225290e-02\n",
      "Epoch: 3740 mean train loss:  1.95389036e-02, mean val. rec. loss:  1.15197462e-02\n",
      "Epoch: 3741 mean train loss:  1.95356485e-02, mean val. rec. loss:  1.15169634e-02\n",
      "Epoch: 3742 mean train loss:  1.95323841e-02, mean val. rec. loss:  1.15141716e-02\n",
      "Epoch: 3743 mean train loss:  1.95291234e-02, mean val. rec. loss:  1.15113933e-02\n",
      "Epoch: 3744 mean train loss:  1.95258794e-02, mean val. rec. loss:  1.15086219e-02\n",
      "Epoch: 3745 mean train loss:  1.95226206e-02, mean val. rec. loss:  1.15058686e-02\n",
      "Epoch: 3746 mean train loss:  1.95193841e-02, mean val. rec. loss:  1.15031039e-02\n",
      "Epoch: 3747 mean train loss:  1.95161290e-02, mean val. rec. loss:  1.15003416e-02\n",
      "Epoch: 3748 mean train loss:  1.95128925e-02, mean val. rec. loss:  1.14975871e-02\n",
      "Epoch: 3749 mean train loss:  1.95096598e-02, mean val. rec. loss:  1.14948418e-02\n",
      "Epoch: 3750 mean train loss:  1.95064196e-02, mean val. rec. loss:  1.14921021e-02\n",
      "Epoch: 3751 mean train loss:  1.95031942e-02, mean val. rec. loss:  1.14893579e-02\n",
      "Epoch: 3752 mean train loss:  1.94999726e-02, mean val. rec. loss:  1.14866091e-02\n",
      "Epoch: 3753 mean train loss:  1.94967473e-02, mean val. rec. loss:  1.14838728e-02\n",
      "Epoch: 3754 mean train loss:  1.94935257e-02, mean val. rec. loss:  1.14811502e-02\n",
      "Epoch: 3755 mean train loss:  1.94903079e-02, mean val. rec. loss:  1.14784309e-02\n",
      "Epoch: 3756 mean train loss:  1.94871030e-02, mean val. rec. loss:  1.14756969e-02\n",
      "Epoch: 3757 mean train loss:  1.94838982e-02, mean val. rec. loss:  1.14729923e-02\n",
      "Epoch: 3758 mean train loss:  1.94806915e-02, mean val. rec. loss:  1.14702833e-02\n",
      "Epoch: 3759 mean train loss:  1.94774904e-02, mean val. rec. loss:  1.14675810e-02\n",
      "Epoch: 3760 mean train loss:  1.94742949e-02, mean val. rec. loss:  1.14648594e-02\n",
      "Epoch: 3761 mean train loss:  1.94710975e-02, mean val. rec. loss:  1.14621606e-02\n",
      "Epoch: 3762 mean train loss:  1.94679095e-02, mean val. rec. loss:  1.14594458e-02\n",
      "Epoch: 3763 mean train loss:  1.94647139e-02, mean val. rec. loss:  1.14567447e-02\n",
      "Epoch: 3764 mean train loss:  1.94615259e-02, mean val. rec. loss:  1.14540470e-02\n",
      "Epoch: 3765 mean train loss:  1.94583490e-02, mean val. rec. loss:  1.14513538e-02\n",
      "Epoch: 3766 mean train loss:  1.94551721e-02, mean val. rec. loss:  1.14486606e-02\n",
      "Epoch: 3767 mean train loss:  1.94520045e-02, mean val. rec. loss:  1.14459731e-02\n",
      "Epoch: 3768 mean train loss:  1.94488294e-02, mean val. rec. loss:  1.14433071e-02\n",
      "Epoch: 3769 mean train loss:  1.94456619e-02, mean val. rec. loss:  1.14406354e-02\n",
      "Epoch: 3770 mean train loss:  1.94425036e-02, mean val. rec. loss:  1.14379706e-02\n",
      "Epoch: 3771 mean train loss:  1.94393397e-02, mean val. rec. loss:  1.14353171e-02\n",
      "Epoch: 3772 mean train loss:  1.94361852e-02, mean val. rec. loss:  1.14326296e-02\n",
      "Epoch: 3773 mean train loss:  1.94330381e-02, mean val. rec. loss:  1.14299670e-02\n",
      "Epoch: 3774 mean train loss:  1.94298798e-02, mean val. rec. loss:  1.14273214e-02\n",
      "Epoch: 3775 mean train loss:  1.94267439e-02, mean val. rec. loss:  1.14246577e-02\n",
      "Epoch: 3776 mean train loss:  1.94235930e-02, mean val. rec. loss:  1.14219997e-02\n",
      "Epoch: 3777 mean train loss:  1.94204571e-02, mean val. rec. loss:  1.14193507e-02\n",
      "Epoch: 3778 mean train loss:  1.94173212e-02, mean val. rec. loss:  1.14167108e-02\n",
      "Epoch: 3779 mean train loss:  1.94141815e-02, mean val. rec. loss:  1.14140868e-02\n",
      "Epoch: 3780 mean train loss:  1.94110642e-02, mean val. rec. loss:  1.14114628e-02\n",
      "Epoch: 3781 mean train loss:  1.94079320e-02, mean val. rec. loss:  1.14088410e-02\n",
      "Epoch: 3782 mean train loss:  1.94048035e-02, mean val. rec. loss:  1.14062147e-02\n",
      "Epoch: 3783 mean train loss:  1.94016825e-02, mean val. rec. loss:  1.14035930e-02\n",
      "Epoch: 3784 mean train loss:  1.93985764e-02, mean val. rec. loss:  1.14009599e-02\n",
      "Epoch: 3785 mean train loss:  1.93954628e-02, mean val. rec. loss:  1.13983381e-02\n",
      "Epoch: 3786 mean train loss:  1.93923529e-02, mean val. rec. loss:  1.13957130e-02\n",
      "Epoch: 3787 mean train loss:  1.93892431e-02, mean val. rec. loss:  1.13931116e-02\n",
      "Epoch: 3788 mean train loss:  1.93861369e-02, mean val. rec. loss:  1.13905035e-02\n",
      "Epoch: 3789 mean train loss:  1.93830383e-02, mean val. rec. loss:  1.13879078e-02\n",
      "Epoch: 3790 mean train loss:  1.93799414e-02, mean val. rec. loss:  1.13853121e-02\n",
      "Epoch: 3791 mean train loss:  1.93768446e-02, mean val. rec. loss:  1.13827357e-02\n",
      "Epoch: 3792 mean train loss:  1.93737571e-02, mean val. rec. loss:  1.13801491e-02\n",
      "Epoch: 3793 mean train loss:  1.93706714e-02, mean val. rec. loss:  1.13775614e-02\n",
      "Epoch: 3794 mean train loss:  1.93675895e-02, mean val. rec. loss:  1.13749657e-02\n",
      "Epoch: 3795 mean train loss:  1.93645132e-02, mean val. rec. loss:  1.13723882e-02\n",
      "Epoch: 3796 mean train loss:  1.93614368e-02, mean val. rec. loss:  1.13698005e-02\n",
      "Epoch: 3797 mean train loss:  1.93583568e-02, mean val. rec. loss:  1.13672445e-02\n",
      "Epoch: 3798 mean train loss:  1.93552804e-02, mean val. rec. loss:  1.13646647e-02\n",
      "Epoch: 3799 mean train loss:  1.93522153e-02, mean val. rec. loss:  1.13621053e-02\n",
      "Epoch: 3800 mean train loss:  1.93491538e-02, mean val. rec. loss:  1.13595527e-02\n",
      "Epoch: 3801 mean train loss:  1.93460887e-02, mean val. rec. loss:  1.13569888e-02\n",
      "Epoch: 3802 mean train loss:  1.93430310e-02, mean val. rec. loss:  1.13544362e-02\n",
      "Epoch: 3803 mean train loss:  1.93399807e-02, mean val. rec. loss:  1.13518859e-02\n",
      "Epoch: 3804 mean train loss:  1.93369192e-02, mean val. rec. loss:  1.13493481e-02\n",
      "Epoch: 3805 mean train loss:  1.93338764e-02, mean val. rec. loss:  1.13468102e-02\n",
      "Epoch: 3806 mean train loss:  1.93308336e-02, mean val. rec. loss:  1.13442599e-02\n",
      "Epoch: 3807 mean train loss:  1.93277871e-02, mean val. rec. loss:  1.13417209e-02\n",
      "Epoch: 3808 mean train loss:  1.93247554e-02, mean val. rec. loss:  1.13391751e-02\n",
      "Epoch: 3809 mean train loss:  1.93217126e-02, mean val. rec. loss:  1.13366384e-02\n",
      "Epoch: 3810 mean train loss:  1.93186884e-02, mean val. rec. loss:  1.13341142e-02\n",
      "Epoch: 3811 mean train loss:  1.93156512e-02, mean val. rec. loss:  1.13315991e-02\n",
      "Epoch: 3812 mean train loss:  1.93126177e-02, mean val. rec. loss:  1.13290850e-02\n",
      "Epoch: 3813 mean train loss:  1.93096046e-02, mean val. rec. loss:  1.13265687e-02\n",
      "Epoch: 3814 mean train loss:  1.93065767e-02, mean val. rec. loss:  1.13240536e-02\n",
      "Epoch: 3815 mean train loss:  1.93035637e-02, mean val. rec. loss:  1.13215554e-02\n",
      "Epoch: 3816 mean train loss:  1.93005507e-02, mean val. rec. loss:  1.13190527e-02\n",
      "Epoch: 3817 mean train loss:  1.92975358e-02, mean val. rec. loss:  1.13165534e-02\n",
      "Epoch: 3818 mean train loss:  1.92945246e-02, mean val. rec. loss:  1.13140428e-02\n",
      "Epoch: 3819 mean train loss:  1.92915190e-02, mean val. rec. loss:  1.13115367e-02\n",
      "Epoch: 3820 mean train loss:  1.92885228e-02, mean val. rec. loss:  1.13090556e-02\n",
      "Epoch: 3821 mean train loss:  1.92855228e-02, mean val. rec. loss:  1.13065631e-02\n",
      "Epoch: 3822 mean train loss:  1.92825247e-02, mean val. rec. loss:  1.13040865e-02\n",
      "Epoch: 3823 mean train loss:  1.92795303e-02, mean val. rec. loss:  1.13015986e-02\n",
      "Epoch: 3824 mean train loss:  1.92765359e-02, mean val. rec. loss:  1.12991197e-02\n",
      "Epoch: 3825 mean train loss:  1.92735470e-02, mean val. rec. loss:  1.12966476e-02\n",
      "Epoch: 3826 mean train loss:  1.92705620e-02, mean val. rec. loss:  1.12941688e-02\n",
      "Epoch: 3827 mean train loss:  1.92675806e-02, mean val. rec. loss:  1.12917035e-02\n",
      "Epoch: 3828 mean train loss:  1.92646048e-02, mean val. rec. loss:  1.12892428e-02\n",
      "Epoch: 3829 mean train loss:  1.92616272e-02, mean val. rec. loss:  1.12867798e-02\n",
      "Epoch: 3830 mean train loss:  1.92586551e-02, mean val. rec. loss:  1.12843270e-02\n",
      "Epoch: 3831 mean train loss:  1.92556905e-02, mean val. rec. loss:  1.12818719e-02\n",
      "Epoch: 3832 mean train loss:  1.92527222e-02, mean val. rec. loss:  1.12794203e-02\n",
      "Epoch: 3833 mean train loss:  1.92497650e-02, mean val. rec. loss:  1.12769709e-02\n",
      "Epoch: 3834 mean train loss:  1.92468078e-02, mean val. rec. loss:  1.12745373e-02\n",
      "Epoch: 3835 mean train loss:  1.92438526e-02, mean val. rec. loss:  1.12720902e-02\n",
      "Epoch: 3836 mean train loss:  1.92408935e-02, mean val. rec. loss:  1.12696567e-02\n",
      "Epoch: 3837 mean train loss:  1.92379364e-02, mean val. rec. loss:  1.12672277e-02\n",
      "Epoch: 3838 mean train loss:  1.92349941e-02, mean val. rec. loss:  1.12647965e-02\n",
      "Epoch: 3839 mean train loss:  1.92320537e-02, mean val. rec. loss:  1.12623664e-02\n",
      "Epoch: 3840 mean train loss:  1.92291021e-02, mean val. rec. loss:  1.12599295e-02\n",
      "Epoch: 3841 mean train loss:  1.92261617e-02, mean val. rec. loss:  1.12575164e-02\n",
      "Epoch: 3842 mean train loss:  1.92232325e-02, mean val. rec. loss:  1.12550987e-02\n",
      "Epoch: 3843 mean train loss:  1.92202940e-02, mean val. rec. loss:  1.12526799e-02\n",
      "Epoch: 3844 mean train loss:  1.92173629e-02, mean val. rec. loss:  1.12502657e-02\n",
      "Epoch: 3845 mean train loss:  1.92144393e-02, mean val. rec. loss:  1.12478583e-02\n",
      "Epoch: 3846 mean train loss:  1.92115044e-02, mean val. rec. loss:  1.12454577e-02\n",
      "Epoch: 3847 mean train loss:  1.92085882e-02, mean val. rec. loss:  1.12430638e-02\n",
      "Epoch: 3848 mean train loss:  1.92056739e-02, mean val. rec. loss:  1.12406791e-02\n",
      "Epoch: 3849 mean train loss:  1.92027559e-02, mean val. rec. loss:  1.12382864e-02\n",
      "Epoch: 3850 mean train loss:  1.91998397e-02, mean val. rec. loss:  1.12358858e-02\n",
      "Epoch: 3851 mean train loss:  1.91969309e-02, mean val. rec. loss:  1.12334874e-02\n",
      "Epoch: 3852 mean train loss:  1.91940241e-02, mean val. rec. loss:  1.12311015e-02\n",
      "Epoch: 3853 mean train loss:  1.91911172e-02, mean val. rec. loss:  1.12287179e-02\n",
      "Epoch: 3854 mean train loss:  1.91882084e-02, mean val. rec. loss:  1.12263252e-02\n",
      "Epoch: 3855 mean train loss:  1.91853183e-02, mean val. rec. loss:  1.12239552e-02\n",
      "Epoch: 3856 mean train loss:  1.91824133e-02, mean val. rec. loss:  1.12215920e-02\n",
      "Epoch: 3857 mean train loss:  1.91795250e-02, mean val. rec. loss:  1.12192310e-02\n",
      "Epoch: 3858 mean train loss:  1.91766349e-02, mean val. rec. loss:  1.12168633e-02\n",
      "Epoch: 3859 mean train loss:  1.91737392e-02, mean val. rec. loss:  1.12144876e-02\n",
      "Epoch: 3860 mean train loss:  1.91708677e-02, mean val. rec. loss:  1.12121323e-02\n",
      "Epoch: 3861 mean train loss:  1.91679795e-02, mean val. rec. loss:  1.12097714e-02\n",
      "Epoch: 3862 mean train loss:  1.91650949e-02, mean val. rec. loss:  1.12074059e-02\n",
      "Epoch: 3863 mean train loss:  1.91622141e-02, mean val. rec. loss:  1.12050484e-02\n",
      "Epoch: 3864 mean train loss:  1.91593519e-02, mean val. rec. loss:  1.12026863e-02\n",
      "Epoch: 3865 mean train loss:  1.91564767e-02, mean val. rec. loss:  1.12003458e-02\n",
      "Epoch: 3866 mean train loss:  1.91536052e-02, mean val. rec. loss:  1.11979985e-02\n",
      "Epoch: 3867 mean train loss:  1.91507374e-02, mean val. rec. loss:  1.11956670e-02\n",
      "Epoch: 3868 mean train loss:  1.91478771e-02, mean val. rec. loss:  1.11933299e-02\n",
      "Epoch: 3869 mean train loss:  1.91450261e-02, mean val. rec. loss:  1.11910041e-02\n",
      "Epoch: 3870 mean train loss:  1.91421676e-02, mean val. rec. loss:  1.11886681e-02\n",
      "Epoch: 3871 mean train loss:  1.91393073e-02, mean val. rec. loss:  1.11863412e-02\n",
      "Epoch: 3872 mean train loss:  1.91364544e-02, mean val. rec. loss:  1.11840109e-02\n",
      "Epoch: 3873 mean train loss:  1.91336053e-02, mean val. rec. loss:  1.11816783e-02\n",
      "Epoch: 3874 mean train loss:  1.91307561e-02, mean val. rec. loss:  1.11793491e-02\n",
      "Epoch: 3875 mean train loss:  1.91279107e-02, mean val. rec. loss:  1.11770199e-02\n",
      "Epoch: 3876 mean train loss:  1.91250690e-02, mean val. rec. loss:  1.11747179e-02\n",
      "Epoch: 3877 mean train loss:  1.91222310e-02, mean val. rec. loss:  1.11724205e-02\n",
      "Epoch: 3878 mean train loss:  1.91193968e-02, mean val. rec. loss:  1.11701162e-02\n",
      "Epoch: 3879 mean train loss:  1.91165625e-02, mean val. rec. loss:  1.11678177e-02\n",
      "Epoch: 3880 mean train loss:  1.91137320e-02, mean val. rec. loss:  1.11655123e-02\n",
      "Epoch: 3881 mean train loss:  1.91109052e-02, mean val. rec. loss:  1.11632126e-02\n",
      "Epoch: 3882 mean train loss:  1.91080802e-02, mean val. rec. loss:  1.11609129e-02\n",
      "Epoch: 3883 mean train loss:  1.91052590e-02, mean val. rec. loss:  1.11586177e-02\n",
      "Epoch: 3884 mean train loss:  1.91024397e-02, mean val. rec. loss:  1.11563373e-02\n",
      "Epoch: 3885 mean train loss:  1.90996278e-02, mean val. rec. loss:  1.11540433e-02\n",
      "Epoch: 3886 mean train loss:  1.90968159e-02, mean val. rec. loss:  1.11517572e-02\n",
      "Epoch: 3887 mean train loss:  1.90940077e-02, mean val. rec. loss:  1.11494801e-02\n",
      "Epoch: 3888 mean train loss:  1.90911995e-02, mean val. rec. loss:  1.11471986e-02\n",
      "Epoch: 3889 mean train loss:  1.90883950e-02, mean val. rec. loss:  1.11449397e-02\n",
      "Epoch: 3890 mean train loss:  1.90855850e-02, mean val. rec. loss:  1.11426581e-02\n",
      "Epoch: 3891 mean train loss:  1.90827824e-02, mean val. rec. loss:  1.11403879e-02\n",
      "Epoch: 3892 mean train loss:  1.90799854e-02, mean val. rec. loss:  1.11381222e-02\n",
      "Epoch: 3893 mean train loss:  1.90771939e-02, mean val. rec. loss:  1.11358622e-02\n",
      "Epoch: 3894 mean train loss:  1.90744063e-02, mean val. rec. loss:  1.11336067e-02\n",
      "Epoch: 3895 mean train loss:  1.90716111e-02, mean val. rec. loss:  1.11313581e-02\n",
      "Epoch: 3896 mean train loss:  1.90688178e-02, mean val. rec. loss:  1.11291083e-02\n",
      "Epoch: 3897 mean train loss:  1.90660376e-02, mean val. rec. loss:  1.11268607e-02\n",
      "Epoch: 3898 mean train loss:  1.90632610e-02, mean val. rec. loss:  1.11246143e-02\n",
      "Epoch: 3899 mean train loss:  1.90604733e-02, mean val. rec. loss:  1.11223577e-02\n",
      "Epoch: 3900 mean train loss:  1.90576931e-02, mean val. rec. loss:  1.11201147e-02\n",
      "Epoch: 3901 mean train loss:  1.90549240e-02, mean val. rec. loss:  1.11178751e-02\n",
      "Epoch: 3902 mean train loss:  1.90521549e-02, mean val. rec. loss:  1.11156423e-02\n",
      "Epoch: 3903 mean train loss:  1.90493747e-02, mean val. rec. loss:  1.11134038e-02\n",
      "Epoch: 3904 mean train loss:  1.90466149e-02, mean val. rec. loss:  1.11111812e-02\n",
      "Epoch: 3905 mean train loss:  1.90438552e-02, mean val. rec. loss:  1.11089518e-02\n",
      "Epoch: 3906 mean train loss:  1.90410842e-02, mean val. rec. loss:  1.11067326e-02\n",
      "Epoch: 3907 mean train loss:  1.90383300e-02, mean val. rec. loss:  1.11045112e-02\n",
      "Epoch: 3908 mean train loss:  1.90355740e-02, mean val. rec. loss:  1.11022908e-02\n",
      "Epoch: 3909 mean train loss:  1.90328142e-02, mean val. rec. loss:  1.11000716e-02\n",
      "Epoch: 3910 mean train loss:  1.90300675e-02, mean val. rec. loss:  1.10978615e-02\n",
      "Epoch: 3911 mean train loss:  1.90273077e-02, mean val. rec. loss:  1.10956412e-02\n",
      "Epoch: 3912 mean train loss:  1.90245684e-02, mean val. rec. loss:  1.10934299e-02\n",
      "Epoch: 3913 mean train loss:  1.90218329e-02, mean val. rec. loss:  1.10912278e-02\n",
      "Epoch: 3914 mean train loss:  1.90190787e-02, mean val. rec. loss:  1.10890210e-02\n",
      "Epoch: 3915 mean train loss:  1.90163487e-02, mean val. rec. loss:  1.10868325e-02\n",
      "Epoch: 3916 mean train loss:  1.90136076e-02, mean val. rec. loss:  1.10846416e-02\n",
      "Epoch: 3917 mean train loss:  1.90108776e-02, mean val. rec. loss:  1.10824508e-02\n",
      "Epoch: 3918 mean train loss:  1.90081495e-02, mean val. rec. loss:  1.10802463e-02\n",
      "Epoch: 3919 mean train loss:  1.90054195e-02, mean val. rec. loss:  1.10780623e-02\n",
      "Epoch: 3920 mean train loss:  1.90026933e-02, mean val. rec. loss:  1.10758692e-02\n",
      "Epoch: 3921 mean train loss:  1.89999670e-02, mean val. rec. loss:  1.10736727e-02\n",
      "Epoch: 3922 mean train loss:  1.89972482e-02, mean val. rec. loss:  1.10714841e-02\n",
      "Epoch: 3923 mean train loss:  1.89945276e-02, mean val. rec. loss:  1.10693057e-02\n",
      "Epoch: 3924 mean train loss:  1.89918125e-02, mean val. rec. loss:  1.10671240e-02\n",
      "Epoch: 3925 mean train loss:  1.89890993e-02, mean val. rec. loss:  1.10649581e-02\n",
      "Epoch: 3926 mean train loss:  1.89863880e-02, mean val. rec. loss:  1.10627899e-02\n",
      "Epoch: 3927 mean train loss:  1.89836803e-02, mean val. rec. loss:  1.10606376e-02\n",
      "Epoch: 3928 mean train loss:  1.89809708e-02, mean val. rec. loss:  1.10584706e-02\n",
      "Epoch: 3929 mean train loss:  1.89782688e-02, mean val. rec. loss:  1.10563092e-02\n",
      "Epoch: 3930 mean train loss:  1.89755686e-02, mean val. rec. loss:  1.10541467e-02\n",
      "Epoch: 3931 mean train loss:  1.89728685e-02, mean val. rec. loss:  1.10519831e-02\n",
      "Epoch: 3932 mean train loss:  1.89701720e-02, mean val. rec. loss:  1.10498149e-02\n",
      "Epoch: 3933 mean train loss:  1.89674774e-02, mean val. rec. loss:  1.10476672e-02\n",
      "Epoch: 3934 mean train loss:  1.89647791e-02, mean val. rec. loss:  1.10455262e-02\n",
      "Epoch: 3935 mean train loss:  1.89621013e-02, mean val. rec. loss:  1.10433785e-02\n",
      "Epoch: 3936 mean train loss:  1.89594067e-02, mean val. rec. loss:  1.10412421e-02\n",
      "Epoch: 3937 mean train loss:  1.89567251e-02, mean val. rec. loss:  1.10391057e-02\n",
      "Epoch: 3938 mean train loss:  1.89540417e-02, mean val. rec. loss:  1.10369624e-02\n",
      "Epoch: 3939 mean train loss:  1.89513564e-02, mean val. rec. loss:  1.10348238e-02\n",
      "Epoch: 3940 mean train loss:  1.89486916e-02, mean val. rec. loss:  1.10326919e-02\n",
      "Epoch: 3941 mean train loss:  1.89460064e-02, mean val. rec. loss:  1.10305668e-02\n",
      "Epoch: 3942 mean train loss:  1.89433267e-02, mean val. rec. loss:  1.10284383e-02\n",
      "Epoch: 3943 mean train loss:  1.89406675e-02, mean val. rec. loss:  1.10263065e-02\n",
      "Epoch: 3944 mean train loss:  1.89379933e-02, mean val. rec. loss:  1.10241848e-02\n",
      "Epoch: 3945 mean train loss:  1.89353286e-02, mean val. rec. loss:  1.10220597e-02\n",
      "Epoch: 3946 mean train loss:  1.89326638e-02, mean val. rec. loss:  1.10199369e-02\n",
      "Epoch: 3947 mean train loss:  1.89299971e-02, mean val. rec. loss:  1.10178255e-02\n",
      "Epoch: 3948 mean train loss:  1.89273528e-02, mean val. rec. loss:  1.10157072e-02\n",
      "Epoch: 3949 mean train loss:  1.89246861e-02, mean val. rec. loss:  1.10136048e-02\n",
      "Epoch: 3950 mean train loss:  1.89220251e-02, mean val. rec. loss:  1.10114945e-02\n",
      "Epoch: 3951 mean train loss:  1.89193845e-02, mean val. rec. loss:  1.10093773e-02\n",
      "Epoch: 3952 mean train loss:  1.89167271e-02, mean val. rec. loss:  1.10072670e-02\n",
      "Epoch: 3953 mean train loss:  1.89140828e-02, mean val. rec. loss:  1.10051794e-02\n",
      "Epoch: 3954 mean train loss:  1.89114385e-02, mean val. rec. loss:  1.10030770e-02\n",
      "Epoch: 3955 mean train loss:  1.89087905e-02, mean val. rec. loss:  1.10009825e-02\n",
      "Epoch: 3956 mean train loss:  1.89061611e-02, mean val. rec. loss:  1.09989005e-02\n",
      "Epoch: 3957 mean train loss:  1.89035149e-02, mean val. rec. loss:  1.09968061e-02\n",
      "Epoch: 3958 mean train loss:  1.89008743e-02, mean val. rec. loss:  1.09947173e-02\n",
      "Epoch: 3959 mean train loss:  1.88982505e-02, mean val. rec. loss:  1.09926217e-02\n",
      "Epoch: 3960 mean train loss:  1.88956136e-02, mean val. rec. loss:  1.09905340e-02\n",
      "Epoch: 3961 mean train loss:  1.88929805e-02, mean val. rec. loss:  1.09884351e-02\n",
      "Epoch: 3962 mean train loss:  1.88903622e-02, mean val. rec. loss:  1.09863553e-02\n",
      "Epoch: 3963 mean train loss:  1.88877291e-02, mean val. rec. loss:  1.09842824e-02\n",
      "Epoch: 3964 mean train loss:  1.88851183e-02, mean val. rec. loss:  1.09822061e-02\n",
      "Epoch: 3965 mean train loss:  1.88824926e-02, mean val. rec. loss:  1.09801366e-02\n",
      "Epoch: 3966 mean train loss:  1.88798669e-02, mean val. rec. loss:  1.09780909e-02\n",
      "Epoch: 3967 mean train loss:  1.88772655e-02, mean val. rec. loss:  1.09760316e-02\n",
      "Epoch: 3968 mean train loss:  1.88746453e-02, mean val. rec. loss:  1.09739451e-02\n",
      "Epoch: 3969 mean train loss:  1.88720383e-02, mean val. rec. loss:  1.09718835e-02\n",
      "Epoch: 3970 mean train loss:  1.88694312e-02, mean val. rec. loss:  1.09698163e-02\n",
      "Epoch: 3971 mean train loss:  1.88668167e-02, mean val. rec. loss:  1.09677422e-02\n",
      "Epoch: 3972 mean train loss:  1.88642245e-02, mean val. rec. loss:  1.09656852e-02\n",
      "Epoch: 3973 mean train loss:  1.88616175e-02, mean val. rec. loss:  1.09636123e-02\n",
      "Epoch: 3974 mean train loss:  1.88590085e-02, mean val. rec. loss:  1.09615621e-02\n",
      "Epoch: 3975 mean train loss:  1.88564220e-02, mean val. rec. loss:  1.09595266e-02\n",
      "Epoch: 3976 mean train loss:  1.88538205e-02, mean val. rec. loss:  1.09574956e-02\n",
      "Epoch: 3977 mean train loss:  1.88512302e-02, mean val. rec. loss:  1.09554556e-02\n",
      "Epoch: 3978 mean train loss:  1.88486436e-02, mean val. rec. loss:  1.09534224e-02\n",
      "Epoch: 3979 mean train loss:  1.88460477e-02, mean val. rec. loss:  1.09513767e-02\n",
      "Epoch: 3980 mean train loss:  1.88434723e-02, mean val. rec. loss:  1.09493401e-02\n",
      "Epoch: 3981 mean train loss:  1.88408801e-02, mean val. rec. loss:  1.09473035e-02\n",
      "Epoch: 3982 mean train loss:  1.88382917e-02, mean val. rec. loss:  1.09452646e-02\n",
      "Epoch: 3983 mean train loss:  1.88357256e-02, mean val. rec. loss:  1.09432234e-02\n",
      "Epoch: 3984 mean train loss:  1.88331408e-02, mean val. rec. loss:  1.09411891e-02\n",
      "Epoch: 3985 mean train loss:  1.88305692e-02, mean val. rec. loss:  1.09391626e-02\n",
      "Epoch: 3986 mean train loss:  1.88279975e-02, mean val. rec. loss:  1.09371442e-02\n",
      "Epoch: 3987 mean train loss:  1.88254183e-02, mean val. rec. loss:  1.09351245e-02\n",
      "Epoch: 3988 mean train loss:  1.88228616e-02, mean val. rec. loss:  1.09331151e-02\n",
      "Epoch: 3989 mean train loss:  1.88202861e-02, mean val. rec. loss:  1.09310944e-02\n",
      "Epoch: 3990 mean train loss:  1.88177145e-02, mean val. rec. loss:  1.09290725e-02\n",
      "Epoch: 3991 mean train loss:  1.88151633e-02, mean val. rec. loss:  1.09270688e-02\n",
      "Epoch: 3992 mean train loss:  1.88125953e-02, mean val. rec. loss:  1.09250594e-02\n",
      "Epoch: 3993 mean train loss:  1.88100366e-02, mean val. rec. loss:  1.09230454e-02\n",
      "Epoch: 3994 mean train loss:  1.88074854e-02, mean val. rec. loss:  1.09210428e-02\n",
      "Epoch: 3995 mean train loss:  1.88049249e-02, mean val. rec. loss:  1.09190402e-02\n",
      "Epoch: 3996 mean train loss:  1.88023849e-02, mean val. rec. loss:  1.09170512e-02\n",
      "Epoch: 3997 mean train loss:  1.87998263e-02, mean val. rec. loss:  1.09150532e-02\n",
      "Epoch: 3998 mean train loss:  1.87972732e-02, mean val. rec. loss:  1.09130551e-02\n",
      "Epoch: 3999 mean train loss:  1.87947369e-02, mean val. rec. loss:  1.09110650e-02\n",
      "Epoch: 4000 mean train loss:  1.87921876e-02, mean val. rec. loss:  1.09090703e-02\n",
      "Epoch: 4001 mean train loss:  1.87896513e-02, mean val. rec. loss:  1.09070768e-02\n",
      "Epoch: 4002 mean train loss:  1.87871075e-02, mean val. rec. loss:  1.09050787e-02\n",
      "Epoch: 4003 mean train loss:  1.87845675e-02, mean val. rec. loss:  1.09030852e-02\n",
      "Epoch: 4004 mean train loss:  1.87820349e-02, mean val. rec. loss:  1.09010871e-02\n",
      "Epoch: 4005 mean train loss:  1.87795023e-02, mean val. rec. loss:  1.08991049e-02\n",
      "Epoch: 4006 mean train loss:  1.87769697e-02, mean val. rec. loss:  1.08971329e-02\n",
      "Epoch: 4007 mean train loss:  1.87744409e-02, mean val. rec. loss:  1.08951587e-02\n",
      "Epoch: 4008 mean train loss:  1.87719139e-02, mean val. rec. loss:  1.08931856e-02\n",
      "Epoch: 4009 mean train loss:  1.87693906e-02, mean val. rec. loss:  1.08912056e-02\n",
      "Epoch: 4010 mean train loss:  1.87668655e-02, mean val. rec. loss:  1.08892382e-02\n",
      "Epoch: 4011 mean train loss:  1.87643422e-02, mean val. rec. loss:  1.08872639e-02\n",
      "Epoch: 4012 mean train loss:  1.87618264e-02, mean val. rec. loss:  1.08852851e-02\n",
      "Epoch: 4013 mean train loss:  1.87593106e-02, mean val. rec. loss:  1.08833279e-02\n",
      "Epoch: 4014 mean train loss:  1.87567910e-02, mean val. rec. loss:  1.08813718e-02\n",
      "Epoch: 4015 mean train loss:  1.87542771e-02, mean val. rec. loss:  1.08794089e-02\n",
      "Epoch: 4016 mean train loss:  1.87517668e-02, mean val. rec. loss:  1.08774460e-02\n",
      "Epoch: 4017 mean train loss:  1.87492603e-02, mean val. rec. loss:  1.08754842e-02\n",
      "Epoch: 4018 mean train loss:  1.87467520e-02, mean val. rec. loss:  1.08735405e-02\n",
      "Epoch: 4019 mean train loss:  1.87442417e-02, mean val. rec. loss:  1.08715731e-02\n",
      "Epoch: 4020 mean train loss:  1.87417408e-02, mean val. rec. loss:  1.08696193e-02\n",
      "Epoch: 4021 mean train loss:  1.87392455e-02, mean val. rec. loss:  1.08676711e-02\n",
      "Epoch: 4022 mean train loss:  1.87367352e-02, mean val. rec. loss:  1.08657274e-02\n",
      "Epoch: 4023 mean train loss:  1.87342436e-02, mean val. rec. loss:  1.08637815e-02\n",
      "Epoch: 4024 mean train loss:  1.87317557e-02, mean val. rec. loss:  1.08618424e-02\n",
      "Epoch: 4025 mean train loss:  1.87292492e-02, mean val. rec. loss:  1.08598988e-02\n",
      "Epoch: 4026 mean train loss:  1.87267651e-02, mean val. rec. loss:  1.08579574e-02\n",
      "Epoch: 4027 mean train loss:  1.87242753e-02, mean val. rec. loss:  1.08560297e-02\n",
      "Epoch: 4028 mean train loss:  1.87217837e-02, mean val. rec. loss:  1.08541019e-02\n",
      "Epoch: 4029 mean train loss:  1.87193051e-02, mean val. rec. loss:  1.08521617e-02\n",
      "Epoch: 4030 mean train loss:  1.87168098e-02, mean val. rec. loss:  1.08502419e-02\n",
      "Epoch: 4031 mean train loss:  1.87143331e-02, mean val. rec. loss:  1.08483186e-02\n",
      "Epoch: 4032 mean train loss:  1.87118601e-02, mean val. rec. loss:  1.08463818e-02\n",
      "Epoch: 4033 mean train loss:  1.87093741e-02, mean val. rec. loss:  1.08444472e-02\n",
      "Epoch: 4034 mean train loss:  1.87068992e-02, mean val. rec. loss:  1.08425286e-02\n",
      "Epoch: 4035 mean train loss:  1.87044318e-02, mean val. rec. loss:  1.08406076e-02\n",
      "Epoch: 4036 mean train loss:  1.87019533e-02, mean val. rec. loss:  1.08386889e-02\n",
      "Epoch: 4037 mean train loss:  1.86994765e-02, mean val. rec. loss:  1.08367668e-02\n",
      "Epoch: 4038 mean train loss:  1.86970147e-02, mean val. rec. loss:  1.08348561e-02\n",
      "Epoch: 4039 mean train loss:  1.86945548e-02, mean val. rec. loss:  1.08329476e-02\n",
      "Epoch: 4040 mean train loss:  1.86920855e-02, mean val. rec. loss:  1.08310300e-02\n",
      "Epoch: 4041 mean train loss:  1.86896162e-02, mean val. rec. loss:  1.08291193e-02\n",
      "Epoch: 4042 mean train loss:  1.86871600e-02, mean val. rec. loss:  1.08272040e-02\n",
      "Epoch: 4043 mean train loss:  1.86847056e-02, mean val. rec. loss:  1.08252989e-02\n",
      "Epoch: 4044 mean train loss:  1.86822438e-02, mean val. rec. loss:  1.08233825e-02\n",
      "Epoch: 4045 mean train loss:  1.86797820e-02, mean val. rec. loss:  1.08214865e-02\n",
      "Epoch: 4046 mean train loss:  1.86773314e-02, mean val. rec. loss:  1.08195769e-02\n",
      "Epoch: 4047 mean train loss:  1.86748807e-02, mean val. rec. loss:  1.08176854e-02\n",
      "Epoch: 4048 mean train loss:  1.86724375e-02, mean val. rec. loss:  1.08157962e-02\n",
      "Epoch: 4049 mean train loss:  1.86699906e-02, mean val. rec. loss:  1.08138843e-02\n",
      "Epoch: 4050 mean train loss:  1.86675474e-02, mean val. rec. loss:  1.08120042e-02\n",
      "Epoch: 4051 mean train loss:  1.86651061e-02, mean val. rec. loss:  1.08100912e-02\n",
      "Epoch: 4052 mean train loss:  1.86626592e-02, mean val. rec. loss:  1.08082031e-02\n",
      "Epoch: 4053 mean train loss:  1.86602104e-02, mean val. rec. loss:  1.08063150e-02\n",
      "Epoch: 4054 mean train loss:  1.86577728e-02, mean val. rec. loss:  1.08044269e-02\n",
      "Epoch: 4055 mean train loss:  1.86553389e-02, mean val. rec. loss:  1.08025366e-02\n",
      "Epoch: 4056 mean train loss:  1.86529069e-02, mean val. rec. loss:  1.08006576e-02\n",
      "Epoch: 4057 mean train loss:  1.86504711e-02, mean val. rec. loss:  1.07987820e-02\n",
      "Epoch: 4058 mean train loss:  1.86480428e-02, mean val. rec. loss:  1.07968996e-02\n",
      "Epoch: 4059 mean train loss:  1.86456108e-02, mean val. rec. loss:  1.07950331e-02\n",
      "Epoch: 4060 mean train loss:  1.86431844e-02, mean val. rec. loss:  1.07931427e-02\n",
      "Epoch: 4061 mean train loss:  1.86407579e-02, mean val. rec. loss:  1.07912660e-02\n",
      "Epoch: 4062 mean train loss:  1.86383334e-02, mean val. rec. loss:  1.07893961e-02\n",
      "Epoch: 4063 mean train loss:  1.86359107e-02, mean val. rec. loss:  1.07875352e-02\n",
      "Epoch: 4064 mean train loss:  1.86334917e-02, mean val. rec. loss:  1.07856540e-02\n",
      "Epoch: 4065 mean train loss:  1.86310708e-02, mean val. rec. loss:  1.07837931e-02\n",
      "Epoch: 4066 mean train loss:  1.86286500e-02, mean val. rec. loss:  1.07819356e-02\n",
      "Epoch: 4067 mean train loss:  1.86262440e-02, mean val. rec. loss:  1.07800805e-02\n",
      "Epoch: 4068 mean train loss:  1.86238381e-02, mean val. rec. loss:  1.07782230e-02\n",
      "Epoch: 4069 mean train loss:  1.86214210e-02, mean val. rec. loss:  1.07763599e-02\n",
      "Epoch: 4070 mean train loss:  1.86190113e-02, mean val. rec. loss:  1.07744934e-02\n",
      "Epoch: 4071 mean train loss:  1.86166016e-02, mean val. rec. loss:  1.07726393e-02\n",
      "Epoch: 4072 mean train loss:  1.86141919e-02, mean val. rec. loss:  1.07707853e-02\n",
      "Epoch: 4073 mean train loss:  1.86117841e-02, mean val. rec. loss:  1.07689244e-02\n",
      "Epoch: 4074 mean train loss:  1.86093949e-02, mean val. rec. loss:  1.07670647e-02\n",
      "Epoch: 4075 mean train loss:  1.86069890e-02, mean val. rec. loss:  1.07652299e-02\n",
      "Epoch: 4076 mean train loss:  1.86045830e-02, mean val. rec. loss:  1.07633849e-02\n",
      "Epoch: 4077 mean train loss:  1.86021827e-02, mean val. rec. loss:  1.07615399e-02\n",
      "Epoch: 4078 mean train loss:  1.85998009e-02, mean val. rec. loss:  1.07596927e-02\n",
      "Epoch: 4079 mean train loss:  1.85973987e-02, mean val. rec. loss:  1.07578557e-02\n",
      "Epoch: 4080 mean train loss:  1.85950021e-02, mean val. rec. loss:  1.07560050e-02\n",
      "Epoch: 4081 mean train loss:  1.85926129e-02, mean val. rec. loss:  1.07541430e-02\n",
      "Epoch: 4082 mean train loss:  1.85902255e-02, mean val. rec. loss:  1.07523026e-02\n",
      "Epoch: 4083 mean train loss:  1.85878326e-02, mean val. rec. loss:  1.07504678e-02\n",
      "Epoch: 4084 mean train loss:  1.85854583e-02, mean val. rec. loss:  1.07486353e-02\n",
      "Epoch: 4085 mean train loss:  1.85830636e-02, mean val. rec. loss:  1.07468119e-02\n",
      "Epoch: 4086 mean train loss:  1.85806762e-02, mean val. rec. loss:  1.07449998e-02\n",
      "Epoch: 4087 mean train loss:  1.85783057e-02, mean val. rec. loss:  1.07431741e-02\n",
      "Epoch: 4088 mean train loss:  1.85759221e-02, mean val. rec. loss:  1.07413506e-02\n",
      "Epoch: 4089 mean train loss:  1.85735515e-02, mean val. rec. loss:  1.07395090e-02\n",
      "Epoch: 4090 mean train loss:  1.85711679e-02, mean val. rec. loss:  1.07376765e-02\n",
      "Epoch: 4091 mean train loss:  1.85688029e-02, mean val. rec. loss:  1.07358486e-02\n",
      "Epoch: 4092 mean train loss:  1.85664212e-02, mean val. rec. loss:  1.07340229e-02\n",
      "Epoch: 4093 mean train loss:  1.85640580e-02, mean val. rec. loss:  1.07322017e-02\n",
      "Epoch: 4094 mean train loss:  1.85616800e-02, mean val. rec. loss:  1.07303851e-02\n",
      "Epoch: 4095 mean train loss:  1.85593225e-02, mean val. rec. loss:  1.07285718e-02\n",
      "Epoch: 4096 mean train loss:  1.85569445e-02, mean val. rec. loss:  1.07267541e-02\n",
      "Epoch: 4097 mean train loss:  1.85545869e-02, mean val. rec. loss:  1.07249488e-02\n",
      "Epoch: 4098 mean train loss:  1.85522182e-02, mean val. rec. loss:  1.07231299e-02\n",
      "Epoch: 4099 mean train loss:  1.85498588e-02, mean val. rec. loss:  1.07213076e-02\n",
      "Epoch: 4100 mean train loss:  1.85475069e-02, mean val. rec. loss:  1.07194898e-02\n",
      "Epoch: 4101 mean train loss:  1.85451363e-02, mean val. rec. loss:  1.07176823e-02\n",
      "Epoch: 4102 mean train loss:  1.85427844e-02, mean val. rec. loss:  1.07158634e-02\n",
      "Epoch: 4103 mean train loss:  1.85404324e-02, mean val. rec. loss:  1.07140581e-02\n",
      "Epoch: 4104 mean train loss:  1.85380842e-02, mean val. rec. loss:  1.07122641e-02\n",
      "Epoch: 4105 mean train loss:  1.85357192e-02, mean val. rec. loss:  1.07104690e-02\n",
      "Epoch: 4106 mean train loss:  1.85333729e-02, mean val. rec. loss:  1.07086762e-02\n",
      "Epoch: 4107 mean train loss:  1.85310284e-02, mean val. rec. loss:  1.07068675e-02\n",
      "Epoch: 4108 mean train loss:  1.85286857e-02, mean val. rec. loss:  1.07050690e-02\n",
      "Epoch: 4109 mean train loss:  1.85263356e-02, mean val. rec. loss:  1.07032558e-02\n",
      "Epoch: 4110 mean train loss:  1.85239837e-02, mean val. rec. loss:  1.07014517e-02\n",
      "Epoch: 4111 mean train loss:  1.85216448e-02, mean val. rec. loss:  1.06996566e-02\n",
      "Epoch: 4112 mean train loss:  1.85193059e-02, mean val. rec. loss:  1.06978626e-02\n",
      "Epoch: 4113 mean train loss:  1.85169688e-02, mean val. rec. loss:  1.06960823e-02\n",
      "Epoch: 4114 mean train loss:  1.85146336e-02, mean val. rec. loss:  1.06942929e-02\n",
      "Epoch: 4115 mean train loss:  1.85122985e-02, mean val. rec. loss:  1.06925103e-02\n",
      "Epoch: 4116 mean train loss:  1.85099614e-02, mean val. rec. loss:  1.06907435e-02\n",
      "Epoch: 4117 mean train loss:  1.85076281e-02, mean val. rec. loss:  1.06889428e-02\n",
      "Epoch: 4118 mean train loss:  1.85052985e-02, mean val. rec. loss:  1.06871465e-02\n",
      "Epoch: 4119 mean train loss:  1.85029689e-02, mean val. rec. loss:  1.06853628e-02\n",
      "Epoch: 4120 mean train loss:  1.85006411e-02, mean val. rec. loss:  1.06835632e-02\n",
      "Epoch: 4121 mean train loss:  1.84983115e-02, mean val. rec. loss:  1.06817749e-02\n",
      "Epoch: 4122 mean train loss:  1.84959838e-02, mean val. rec. loss:  1.06800093e-02\n",
      "Epoch: 4123 mean train loss:  1.84936598e-02, mean val. rec. loss:  1.06782324e-02\n",
      "Epoch: 4124 mean train loss:  1.84913339e-02, mean val. rec. loss:  1.06764736e-02\n",
      "Epoch: 4125 mean train loss:  1.84890192e-02, mean val. rec. loss:  1.06746966e-02\n",
      "Epoch: 4126 mean train loss:  1.84867082e-02, mean val. rec. loss:  1.06729174e-02\n",
      "Epoch: 4127 mean train loss:  1.84843861e-02, mean val. rec. loss:  1.06711450e-02\n",
      "Epoch: 4128 mean train loss:  1.84820658e-02, mean val. rec. loss:  1.06693681e-02\n",
      "Epoch: 4129 mean train loss:  1.84797455e-02, mean val. rec. loss:  1.06675911e-02\n",
      "Epoch: 4130 mean train loss:  1.84774476e-02, mean val. rec. loss:  1.06658096e-02\n",
      "Epoch: 4131 mean train loss:  1.84751310e-02, mean val. rec. loss:  1.06640452e-02\n",
      "Epoch: 4132 mean train loss:  1.84728144e-02, mean val. rec. loss:  1.06622716e-02\n",
      "Epoch: 4133 mean train loss:  1.84705090e-02, mean val. rec. loss:  1.06605094e-02\n",
      "Epoch: 4134 mean train loss:  1.84682036e-02, mean val. rec. loss:  1.06587676e-02\n",
      "Epoch: 4135 mean train loss:  1.84658927e-02, mean val. rec. loss:  1.06570122e-02\n",
      "Epoch: 4136 mean train loss:  1.84635985e-02, mean val. rec. loss:  1.06552614e-02\n",
      "Epoch: 4137 mean train loss:  1.84612893e-02, mean val. rec. loss:  1.06535037e-02\n",
      "Epoch: 4138 mean train loss:  1.84589802e-02, mean val. rec. loss:  1.06517393e-02\n",
      "Epoch: 4139 mean train loss:  1.84566897e-02, mean val. rec. loss:  1.06499782e-02\n",
      "Epoch: 4140 mean train loss:  1.84543825e-02, mean val. rec. loss:  1.06482183e-02\n",
      "Epoch: 4141 mean train loss:  1.84520920e-02, mean val. rec. loss:  1.06464481e-02\n",
      "Epoch: 4142 mean train loss:  1.84497866e-02, mean val. rec. loss:  1.06447029e-02\n",
      "Epoch: 4143 mean train loss:  1.84475035e-02, mean val. rec. loss:  1.06429600e-02\n",
      "Epoch: 4144 mean train loss:  1.84452056e-02, mean val. rec. loss:  1.06412046e-02\n",
      "Epoch: 4145 mean train loss:  1.84429132e-02, mean val. rec. loss:  1.06394730e-02\n",
      "Epoch: 4146 mean train loss:  1.84406302e-02, mean val. rec. loss:  1.06377233e-02\n",
      "Epoch: 4147 mean train loss:  1.84383304e-02, mean val. rec. loss:  1.06359906e-02\n",
      "Epoch: 4148 mean train loss:  1.84360473e-02, mean val. rec. loss:  1.06342397e-02\n",
      "Epoch: 4149 mean train loss:  1.84337662e-02, mean val. rec. loss:  1.06324843e-02\n",
      "Epoch: 4150 mean train loss:  1.84314757e-02, mean val. rec. loss:  1.06307301e-02\n",
      "Epoch: 4151 mean train loss:  1.84291870e-02, mean val. rec. loss:  1.06289826e-02\n",
      "Epoch: 4152 mean train loss:  1.84269096e-02, mean val. rec. loss:  1.06272431e-02\n",
      "Epoch: 4153 mean train loss:  1.84246358e-02, mean val. rec. loss:  1.06255024e-02\n",
      "Epoch: 4154 mean train loss:  1.84223509e-02, mean val. rec. loss:  1.06237675e-02\n",
      "Epoch: 4155 mean train loss:  1.84200660e-02, mean val. rec. loss:  1.06220381e-02\n",
      "Epoch: 4156 mean train loss:  1.84177904e-02, mean val. rec. loss:  1.06203032e-02\n",
      "Epoch: 4157 mean train loss:  1.84155186e-02, mean val. rec. loss:  1.06185670e-02\n",
      "Epoch: 4158 mean train loss:  1.84132467e-02, mean val. rec. loss:  1.06168275e-02\n",
      "Epoch: 4159 mean train loss:  1.84109785e-02, mean val. rec. loss:  1.06150801e-02\n",
      "Epoch: 4160 mean train loss:  1.84087048e-02, mean val. rec. loss:  1.06133371e-02\n",
      "Epoch: 4161 mean train loss:  1.84064367e-02, mean val. rec. loss:  1.06116010e-02\n",
      "Epoch: 4162 mean train loss:  1.84041666e-02, mean val. rec. loss:  1.06098842e-02\n",
      "Epoch: 4163 mean train loss:  1.84018985e-02, mean val. rec. loss:  1.06081651e-02\n",
      "Epoch: 4164 mean train loss:  1.83996341e-02, mean val. rec. loss:  1.06064448e-02\n",
      "Epoch: 4165 mean train loss:  1.83973659e-02, mean val. rec. loss:  1.06047246e-02\n",
      "Epoch: 4166 mean train loss:  1.83950996e-02, mean val. rec. loss:  1.06030043e-02\n",
      "Epoch: 4167 mean train loss:  1.83928371e-02, mean val. rec. loss:  1.06012705e-02\n",
      "Epoch: 4168 mean train loss:  1.83905745e-02, mean val. rec. loss:  1.05995446e-02\n",
      "Epoch: 4169 mean train loss:  1.83883231e-02, mean val. rec. loss:  1.05978062e-02\n",
      "Epoch: 4170 mean train loss:  1.83860680e-02, mean val. rec. loss:  1.05960757e-02\n",
      "Epoch: 4171 mean train loss:  1.83838073e-02, mean val. rec. loss:  1.05943623e-02\n",
      "Epoch: 4172 mean train loss:  1.83815466e-02, mean val. rec. loss:  1.05926523e-02\n",
      "Epoch: 4173 mean train loss:  1.83792952e-02, mean val. rec. loss:  1.05909388e-02\n",
      "Epoch: 4174 mean train loss:  1.83770494e-02, mean val. rec. loss:  1.05892277e-02\n",
      "Epoch: 4175 mean train loss:  1.83747906e-02, mean val. rec. loss:  1.05875154e-02\n",
      "Epoch: 4176 mean train loss:  1.83725429e-02, mean val. rec. loss:  1.05858053e-02\n",
      "Epoch: 4177 mean train loss:  1.83702953e-02, mean val. rec. loss:  1.05840839e-02\n",
      "Epoch: 4178 mean train loss:  1.83680420e-02, mean val. rec. loss:  1.05823853e-02\n",
      "Epoch: 4179 mean train loss:  1.83658055e-02, mean val. rec. loss:  1.05806627e-02\n",
      "Epoch: 4180 mean train loss:  1.83635523e-02, mean val. rec. loss:  1.05789470e-02\n",
      "Epoch: 4181 mean train loss:  1.83613158e-02, mean val. rec. loss:  1.05772393e-02\n",
      "Epoch: 4182 mean train loss:  1.83590644e-02, mean val. rec. loss:  1.05755258e-02\n",
      "Epoch: 4183 mean train loss:  1.83568335e-02, mean val. rec. loss:  1.05738237e-02\n",
      "Epoch: 4184 mean train loss:  1.83545802e-02, mean val. rec. loss:  1.05721160e-02\n",
      "Epoch: 4185 mean train loss:  1.83523493e-02, mean val. rec. loss:  1.05704173e-02\n",
      "Epoch: 4186 mean train loss:  1.83501073e-02, mean val. rec. loss:  1.05687208e-02\n",
      "Epoch: 4187 mean train loss:  1.83478708e-02, mean val. rec. loss:  1.05670323e-02\n",
      "Epoch: 4188 mean train loss:  1.83456417e-02, mean val. rec. loss:  1.05653212e-02\n",
      "Epoch: 4189 mean train loss:  1.83434015e-02, mean val. rec. loss:  1.05636191e-02\n",
      "Epoch: 4190 mean train loss:  1.83411650e-02, mean val. rec. loss:  1.05619034e-02\n",
      "Epoch: 4191 mean train loss:  1.83389397e-02, mean val. rec. loss:  1.05602001e-02\n",
      "Epoch: 4192 mean train loss:  1.83367144e-02, mean val. rec. loss:  1.05584946e-02\n",
      "Epoch: 4193 mean train loss:  1.83344853e-02, mean val. rec. loss:  1.05568050e-02\n",
      "Epoch: 4194 mean train loss:  1.83322526e-02, mean val. rec. loss:  1.05550961e-02\n",
      "Epoch: 4195 mean train loss:  1.83300179e-02, mean val. rec. loss:  1.05534031e-02\n",
      "Epoch: 4196 mean train loss:  1.83277982e-02, mean val. rec. loss:  1.05517191e-02\n",
      "Epoch: 4197 mean train loss:  1.83255748e-02, mean val. rec. loss:  1.05500295e-02\n",
      "Epoch: 4198 mean train loss:  1.83233513e-02, mean val. rec. loss:  1.05483433e-02\n",
      "Epoch: 4199 mean train loss:  1.83211297e-02, mean val. rec. loss:  1.05466423e-02\n",
      "Epoch: 4200 mean train loss:  1.83189081e-02, mean val. rec. loss:  1.05449515e-02\n",
      "Epoch: 4201 mean train loss:  1.83166884e-02, mean val. rec. loss:  1.05432563e-02\n",
      "Epoch: 4202 mean train loss:  1.83144705e-02, mean val. rec. loss:  1.05415508e-02\n",
      "Epoch: 4203 mean train loss:  1.83122508e-02, mean val. rec. loss:  1.05398498e-02\n",
      "Epoch: 4204 mean train loss:  1.83100366e-02, mean val. rec. loss:  1.05381715e-02\n",
      "Epoch: 4205 mean train loss:  1.83078318e-02, mean val. rec. loss:  1.05364978e-02\n",
      "Epoch: 4206 mean train loss:  1.83056158e-02, mean val. rec. loss:  1.05348229e-02\n",
      "Epoch: 4207 mean train loss:  1.83033979e-02, mean val. rec. loss:  1.05331457e-02\n",
      "Epoch: 4208 mean train loss:  1.83011838e-02, mean val. rec. loss:  1.05314844e-02\n",
      "Epoch: 4209 mean train loss:  1.82989752e-02, mean val. rec. loss:  1.05297880e-02\n",
      "Epoch: 4210 mean train loss:  1.82967704e-02, mean val. rec. loss:  1.05280973e-02\n",
      "Epoch: 4211 mean train loss:  1.82945562e-02, mean val. rec. loss:  1.05263997e-02\n",
      "Epoch: 4212 mean train loss:  1.82923495e-02, mean val. rec. loss:  1.05247101e-02\n",
      "Epoch: 4213 mean train loss:  1.82901447e-02, mean val. rec. loss:  1.05230329e-02\n",
      "Epoch: 4214 mean train loss:  1.82879343e-02, mean val. rec. loss:  1.05213580e-02\n",
      "Epoch: 4215 mean train loss:  1.82857425e-02, mean val. rec. loss:  1.05196877e-02\n",
      "Epoch: 4216 mean train loss:  1.82835302e-02, mean val. rec. loss:  1.05180253e-02\n",
      "Epoch: 4217 mean train loss:  1.82813384e-02, mean val. rec. loss:  1.05163583e-02\n",
      "Epoch: 4218 mean train loss:  1.82791280e-02, mean val. rec. loss:  1.05146800e-02\n",
      "Epoch: 4219 mean train loss:  1.82769381e-02, mean val. rec. loss:  1.05130018e-02\n",
      "Epoch: 4220 mean train loss:  1.82747276e-02, mean val. rec. loss:  1.05113257e-02\n",
      "Epoch: 4221 mean train loss:  1.82725358e-02, mean val. rec. loss:  1.05096418e-02\n",
      "Epoch: 4222 mean train loss:  1.82703385e-02, mean val. rec. loss:  1.05079737e-02\n",
      "Epoch: 4223 mean train loss:  1.82681392e-02, mean val. rec. loss:  1.05062966e-02\n",
      "Epoch: 4224 mean train loss:  1.82659493e-02, mean val. rec. loss:  1.05046149e-02\n",
      "Epoch: 4225 mean train loss:  1.82637593e-02, mean val. rec. loss:  1.05029513e-02\n",
      "Epoch: 4226 mean train loss:  1.82615564e-02, mean val. rec. loss:  1.05012855e-02\n",
      "Epoch: 4227 mean train loss:  1.82593702e-02, mean val. rec. loss:  1.04996276e-02\n",
      "Epoch: 4228 mean train loss:  1.82571802e-02, mean val. rec. loss:  1.04979494e-02\n",
      "Epoch: 4229 mean train loss:  1.82549977e-02, mean val. rec. loss:  1.04962858e-02\n",
      "Epoch: 4230 mean train loss:  1.82528115e-02, mean val. rec. loss:  1.04946257e-02\n",
      "Epoch: 4231 mean train loss:  1.82506272e-02, mean val. rec. loss:  1.04929576e-02\n",
      "Epoch: 4232 mean train loss:  1.82484354e-02, mean val. rec. loss:  1.04912872e-02\n",
      "Epoch: 4233 mean train loss:  1.82462529e-02, mean val. rec. loss:  1.04896158e-02\n",
      "Epoch: 4234 mean train loss:  1.82440667e-02, mean val. rec. loss:  1.04879431e-02\n",
      "Epoch: 4235 mean train loss:  1.82418842e-02, mean val. rec. loss:  1.04862875e-02\n",
      "Epoch: 4236 mean train loss:  1.82397036e-02, mean val. rec. loss:  1.04846342e-02\n",
      "Epoch: 4237 mean train loss:  1.82375285e-02, mean val. rec. loss:  1.04829752e-02\n",
      "Epoch: 4238 mean train loss:  1.82353479e-02, mean val. rec. loss:  1.04813366e-02\n",
      "Epoch: 4239 mean train loss:  1.82331691e-02, mean val. rec. loss:  1.04796923e-02\n",
      "Epoch: 4240 mean train loss:  1.82309885e-02, mean val. rec. loss:  1.04780254e-02\n",
      "Epoch: 4241 mean train loss:  1.82288060e-02, mean val. rec. loss:  1.04763584e-02\n",
      "Epoch: 4242 mean train loss:  1.82266366e-02, mean val. rec. loss:  1.04746824e-02\n",
      "Epoch: 4243 mean train loss:  1.82244653e-02, mean val. rec. loss:  1.04730030e-02\n",
      "Epoch: 4244 mean train loss:  1.82222884e-02, mean val. rec. loss:  1.04713395e-02\n",
      "Epoch: 4245 mean train loss:  1.82201096e-02, mean val. rec. loss:  1.04696907e-02\n",
      "Epoch: 4246 mean train loss:  1.82179513e-02, mean val. rec. loss:  1.04680441e-02\n",
      "Epoch: 4247 mean train loss:  1.82157726e-02, mean val. rec. loss:  1.04664157e-02\n",
      "Epoch: 4248 mean train loss:  1.82136012e-02, mean val. rec. loss:  1.04647873e-02\n",
      "Epoch: 4249 mean train loss:  1.82114374e-02, mean val. rec. loss:  1.04631385e-02\n",
      "Epoch: 4250 mean train loss:  1.82092698e-02, mean val. rec. loss:  1.04614761e-02\n",
      "Epoch: 4251 mean train loss:  1.82071022e-02, mean val. rec. loss:  1.04598251e-02\n",
      "Epoch: 4252 mean train loss:  1.82049365e-02, mean val. rec. loss:  1.04581490e-02\n",
      "Epoch: 4253 mean train loss:  1.82027707e-02, mean val. rec. loss:  1.04564946e-02\n",
      "Epoch: 4254 mean train loss:  1.82006143e-02, mean val. rec. loss:  1.04548333e-02\n",
      "Epoch: 4255 mean train loss:  1.81984411e-02, mean val. rec. loss:  1.04531890e-02\n",
      "Epoch: 4256 mean train loss:  1.81962847e-02, mean val. rec. loss:  1.04515538e-02\n",
      "Epoch: 4257 mean train loss:  1.81941283e-02, mean val. rec. loss:  1.04499209e-02\n",
      "Epoch: 4258 mean train loss:  1.81919626e-02, mean val. rec. loss:  1.04482868e-02\n",
      "Epoch: 4259 mean train loss:  1.81898006e-02, mean val. rec. loss:  1.04466505e-02\n",
      "Epoch: 4260 mean train loss:  1.81876479e-02, mean val. rec. loss:  1.04449994e-02\n",
      "Epoch: 4261 mean train loss:  1.81854915e-02, mean val. rec. loss:  1.04433370e-02\n",
      "Epoch: 4262 mean train loss:  1.81833406e-02, mean val. rec. loss:  1.04416792e-02\n",
      "Epoch: 4263 mean train loss:  1.81811861e-02, mean val. rec. loss:  1.04400270e-02\n",
      "Epoch: 4264 mean train loss:  1.81790278e-02, mean val. rec. loss:  1.04383725e-02\n",
      "Epoch: 4265 mean train loss:  1.81768714e-02, mean val. rec. loss:  1.04367192e-02\n",
      "Epoch: 4266 mean train loss:  1.81747224e-02, mean val. rec. loss:  1.04350953e-02\n",
      "Epoch: 4267 mean train loss:  1.81725697e-02, mean val. rec. loss:  1.04334692e-02\n",
      "Epoch: 4268 mean train loss:  1.81704208e-02, mean val. rec. loss:  1.04318442e-02\n",
      "Epoch: 4269 mean train loss:  1.81682755e-02, mean val. rec. loss:  1.04302181e-02\n",
      "Epoch: 4270 mean train loss:  1.81661228e-02, mean val. rec. loss:  1.04285795e-02\n",
      "Epoch: 4271 mean train loss:  1.81639739e-02, mean val. rec. loss:  1.04269329e-02\n",
      "Epoch: 4272 mean train loss:  1.81618249e-02, mean val. rec. loss:  1.04252649e-02\n",
      "Epoch: 4273 mean train loss:  1.81596741e-02, mean val. rec. loss:  1.04236240e-02\n",
      "Epoch: 4274 mean train loss:  1.81575325e-02, mean val. rec. loss:  1.04219831e-02\n",
      "Epoch: 4275 mean train loss:  1.81553947e-02, mean val. rec. loss:  1.04203491e-02\n",
      "Epoch: 4276 mean train loss:  1.81532458e-02, mean val. rec. loss:  1.04187195e-02\n",
      "Epoch: 4277 mean train loss:  1.81510950e-02, mean val. rec. loss:  1.04171025e-02\n",
      "Epoch: 4278 mean train loss:  1.81489646e-02, mean val. rec. loss:  1.04154764e-02\n",
      "Epoch: 4279 mean train loss:  1.81468156e-02, mean val. rec. loss:  1.04138514e-02\n",
      "Epoch: 4280 mean train loss:  1.81446778e-02, mean val. rec. loss:  1.04122037e-02\n",
      "Epoch: 4281 mean train loss:  1.81425382e-02, mean val. rec. loss:  1.04105504e-02\n",
      "Epoch: 4282 mean train loss:  1.81404004e-02, mean val. rec. loss:  1.04089118e-02\n",
      "Epoch: 4283 mean train loss:  1.81382645e-02, mean val. rec. loss:  1.04072686e-02\n",
      "Epoch: 4284 mean train loss:  1.81361322e-02, mean val. rec. loss:  1.04056516e-02\n",
      "Epoch: 4285 mean train loss:  1.81339833e-02, mean val. rec. loss:  1.04040255e-02\n",
      "Epoch: 4286 mean train loss:  1.81318585e-02, mean val. rec. loss:  1.04024062e-02\n",
      "Epoch: 4287 mean train loss:  1.81297207e-02, mean val. rec. loss:  1.04007880e-02\n",
      "Epoch: 4288 mean train loss:  1.81275811e-02, mean val. rec. loss:  1.03991437e-02\n",
      "Epoch: 4289 mean train loss:  1.81254526e-02, mean val. rec. loss:  1.03975221e-02\n",
      "Epoch: 4290 mean train loss:  1.81233241e-02, mean val. rec. loss:  1.03959005e-02\n",
      "Epoch: 4291 mean train loss:  1.81211900e-02, mean val. rec. loss:  1.03942495e-02\n",
      "Epoch: 4292 mean train loss:  1.81190541e-02, mean val. rec. loss:  1.03926109e-02\n",
      "Epoch: 4293 mean train loss:  1.81169238e-02, mean val. rec. loss:  1.03909813e-02\n",
      "Epoch: 4294 mean train loss:  1.81147971e-02, mean val. rec. loss:  1.03893632e-02\n",
      "Epoch: 4295 mean train loss:  1.81126724e-02, mean val. rec. loss:  1.03877382e-02\n",
      "Epoch: 4296 mean train loss:  1.81105458e-02, mean val. rec. loss:  1.03861268e-02\n",
      "Epoch: 4297 mean train loss:  1.81084210e-02, mean val. rec. loss:  1.03845041e-02\n",
      "Epoch: 4298 mean train loss:  1.81062944e-02, mean val. rec. loss:  1.03828745e-02\n",
      "Epoch: 4299 mean train loss:  1.81041696e-02, mean val. rec. loss:  1.03812337e-02\n",
      "Epoch: 4300 mean train loss:  1.81020449e-02, mean val. rec. loss:  1.03796076e-02\n",
      "Epoch: 4301 mean train loss:  1.80999182e-02, mean val. rec. loss:  1.03779814e-02\n",
      "Epoch: 4302 mean train loss:  1.80977935e-02, mean val. rec. loss:  1.03763530e-02\n",
      "Epoch: 4303 mean train loss:  1.80956873e-02, mean val. rec. loss:  1.03747417e-02\n",
      "Epoch: 4304 mean train loss:  1.80935626e-02, mean val. rec. loss:  1.03731223e-02\n",
      "Epoch: 4305 mean train loss:  1.80914378e-02, mean val. rec. loss:  1.03715019e-02\n",
      "Epoch: 4306 mean train loss:  1.80893131e-02, mean val. rec. loss:  1.03698735e-02\n",
      "Epoch: 4307 mean train loss:  1.80872069e-02, mean val. rec. loss:  1.03682564e-02\n",
      "Epoch: 4308 mean train loss:  1.80850822e-02, mean val. rec. loss:  1.03666315e-02\n",
      "Epoch: 4309 mean train loss:  1.80829648e-02, mean val. rec. loss:  1.03650133e-02\n",
      "Epoch: 4310 mean train loss:  1.80808531e-02, mean val. rec. loss:  1.03633883e-02\n",
      "Epoch: 4311 mean train loss:  1.80787302e-02, mean val. rec. loss:  1.03617588e-02\n",
      "Epoch: 4312 mean train loss:  1.80766241e-02, mean val. rec. loss:  1.03601428e-02\n",
      "Epoch: 4313 mean train loss:  1.80745012e-02, mean val. rec. loss:  1.03585258e-02\n",
      "Epoch: 4314 mean train loss:  1.80723932e-02, mean val. rec. loss:  1.03569008e-02\n",
      "Epoch: 4315 mean train loss:  1.80702871e-02, mean val. rec. loss:  1.03553030e-02\n",
      "Epoch: 4316 mean train loss:  1.80681642e-02, mean val. rec. loss:  1.03536905e-02\n",
      "Epoch: 4317 mean train loss:  1.80660599e-02, mean val. rec. loss:  1.03520587e-02\n",
      "Epoch: 4318 mean train loss:  1.80639556e-02, mean val. rec. loss:  1.03504258e-02\n",
      "Epoch: 4319 mean train loss:  1.80618327e-02, mean val. rec. loss:  1.03488031e-02\n",
      "Epoch: 4320 mean train loss:  1.80597266e-02, mean val. rec. loss:  1.03471894e-02\n",
      "Epoch: 4321 mean train loss:  1.80576242e-02, mean val. rec. loss:  1.03455837e-02\n",
      "Epoch: 4322 mean train loss:  1.80555162e-02, mean val. rec. loss:  1.03439825e-02\n",
      "Epoch: 4323 mean train loss:  1.80534119e-02, mean val. rec. loss:  1.03423916e-02\n",
      "Epoch: 4324 mean train loss:  1.80512964e-02, mean val. rec. loss:  1.03407802e-02\n",
      "Epoch: 4325 mean train loss:  1.80491959e-02, mean val. rec. loss:  1.03391631e-02\n",
      "Epoch: 4326 mean train loss:  1.80470860e-02, mean val. rec. loss:  1.03375211e-02\n",
      "Epoch: 4327 mean train loss:  1.80449836e-02, mean val. rec. loss:  1.03358950e-02\n",
      "Epoch: 4328 mean train loss:  1.80428775e-02, mean val. rec. loss:  1.03342791e-02\n",
      "Epoch: 4329 mean train loss:  1.80407844e-02, mean val. rec. loss:  1.03326575e-02\n",
      "Epoch: 4330 mean train loss:  1.80386782e-02, mean val. rec. loss:  1.03310586e-02\n",
      "Epoch: 4331 mean train loss:  1.80365740e-02, mean val. rec. loss:  1.03294518e-02\n",
      "Epoch: 4332 mean train loss:  1.80344734e-02, mean val. rec. loss:  1.03278574e-02\n",
      "Epoch: 4333 mean train loss:  1.80323654e-02, mean val. rec. loss:  1.03262472e-02\n",
      "Epoch: 4334 mean train loss:  1.80302723e-02, mean val. rec. loss:  1.03246210e-02\n",
      "Epoch: 4335 mean train loss:  1.80281792e-02, mean val. rec. loss:  1.03229938e-02\n",
      "Epoch: 4336 mean train loss:  1.80260749e-02, mean val. rec. loss:  1.03213688e-02\n",
      "Epoch: 4337 mean train loss:  1.80239781e-02, mean val. rec. loss:  1.03197495e-02\n",
      "Epoch: 4338 mean train loss:  1.80218813e-02, mean val. rec. loss:  1.03181426e-02\n",
      "Epoch: 4339 mean train loss:  1.80197789e-02, mean val. rec. loss:  1.03165324e-02\n",
      "Epoch: 4340 mean train loss:  1.80176932e-02, mean val. rec. loss:  1.03149323e-02\n",
      "Epoch: 4341 mean train loss:  1.80155871e-02, mean val. rec. loss:  1.03133266e-02\n",
      "Epoch: 4342 mean train loss:  1.80135014e-02, mean val. rec. loss:  1.03117277e-02\n",
      "Epoch: 4343 mean train loss:  1.80114009e-02, mean val. rec. loss:  1.03101118e-02\n",
      "Epoch: 4344 mean train loss:  1.80093152e-02, mean val. rec. loss:  1.03084936e-02\n",
      "Epoch: 4345 mean train loss:  1.80072202e-02, mean val. rec. loss:  1.03068868e-02\n",
      "Epoch: 4346 mean train loss:  1.80051253e-02, mean val. rec. loss:  1.03052629e-02\n",
      "Epoch: 4347 mean train loss:  1.80030396e-02, mean val. rec. loss:  1.03036549e-02\n",
      "Epoch: 4348 mean train loss:  1.80009540e-02, mean val. rec. loss:  1.03020492e-02\n",
      "Epoch: 4349 mean train loss:  1.79988497e-02, mean val. rec. loss:  1.03004458e-02\n",
      "Epoch: 4350 mean train loss:  1.79967640e-02, mean val. rec. loss:  1.02988435e-02\n",
      "Epoch: 4351 mean train loss:  1.79946784e-02, mean val. rec. loss:  1.02972480e-02\n",
      "Epoch: 4352 mean train loss:  1.79925927e-02, mean val. rec. loss:  1.02956411e-02\n",
      "Epoch: 4353 mean train loss:  1.79905071e-02, mean val. rec. loss:  1.02940331e-02\n",
      "Epoch: 4354 mean train loss:  1.79884214e-02, mean val. rec. loss:  1.02924218e-02\n",
      "Epoch: 4355 mean train loss:  1.79863358e-02, mean val. rec. loss:  1.02908127e-02\n",
      "Epoch: 4356 mean train loss:  1.79842501e-02, mean val. rec. loss:  1.02892047e-02\n",
      "Epoch: 4357 mean train loss:  1.79821645e-02, mean val. rec. loss:  1.02876001e-02\n",
      "Epoch: 4358 mean train loss:  1.79800770e-02, mean val. rec. loss:  1.02859978e-02\n",
      "Epoch: 4359 mean train loss:  1.79779913e-02, mean val. rec. loss:  1.02844011e-02\n",
      "Epoch: 4360 mean train loss:  1.79759075e-02, mean val. rec. loss:  1.02828034e-02\n",
      "Epoch: 4361 mean train loss:  1.79738219e-02, mean val. rec. loss:  1.02811909e-02\n",
      "Epoch: 4362 mean train loss:  1.79717325e-02, mean val. rec. loss:  1.02795908e-02\n",
      "Epoch: 4363 mean train loss:  1.79696655e-02, mean val. rec. loss:  1.02779851e-02\n",
      "Epoch: 4364 mean train loss:  1.79675798e-02, mean val. rec. loss:  1.02763771e-02\n",
      "Epoch: 4365 mean train loss:  1.79654923e-02, mean val. rec. loss:  1.02747669e-02\n",
      "Epoch: 4366 mean train loss:  1.79634234e-02, mean val. rec. loss:  1.02731668e-02\n",
      "Epoch: 4367 mean train loss:  1.79613377e-02, mean val. rec. loss:  1.02715645e-02\n",
      "Epoch: 4368 mean train loss:  1.79592484e-02, mean val. rec. loss:  1.02699543e-02\n",
      "Epoch: 4369 mean train loss:  1.79571776e-02, mean val. rec. loss:  1.02683440e-02\n",
      "Epoch: 4370 mean train loss:  1.79550919e-02, mean val. rec. loss:  1.02667383e-02\n",
      "Epoch: 4371 mean train loss:  1.79530212e-02, mean val. rec. loss:  1.02651360e-02\n",
      "Epoch: 4372 mean train loss:  1.79509430e-02, mean val. rec. loss:  1.02635382e-02\n",
      "Epoch: 4373 mean train loss:  1.79488648e-02, mean val. rec. loss:  1.02619461e-02\n",
      "Epoch: 4374 mean train loss:  1.79467940e-02, mean val. rec. loss:  1.02603336e-02\n",
      "Epoch: 4375 mean train loss:  1.79447084e-02, mean val. rec. loss:  1.02587302e-02\n",
      "Epoch: 4376 mean train loss:  1.79426395e-02, mean val. rec. loss:  1.02571245e-02\n",
      "Epoch: 4377 mean train loss:  1.79405687e-02, mean val. rec. loss:  1.02555210e-02\n",
      "Epoch: 4378 mean train loss:  1.79384998e-02, mean val. rec. loss:  1.02539040e-02\n",
      "Epoch: 4379 mean train loss:  1.79364216e-02, mean val. rec. loss:  1.02522983e-02\n",
      "Epoch: 4380 mean train loss:  1.79343397e-02, mean val. rec. loss:  1.02506880e-02\n",
      "Epoch: 4381 mean train loss:  1.79322689e-02, mean val. rec. loss:  1.02490948e-02\n",
      "Epoch: 4382 mean train loss:  1.79301982e-02, mean val. rec. loss:  1.02474993e-02\n",
      "Epoch: 4383 mean train loss:  1.79281274e-02, mean val. rec. loss:  1.02459049e-02\n",
      "Epoch: 4384 mean train loss:  1.79260585e-02, mean val. rec. loss:  1.02443003e-02\n",
      "Epoch: 4385 mean train loss:  1.79239859e-02, mean val. rec. loss:  1.02426787e-02\n",
      "Epoch: 4386 mean train loss:  1.79219133e-02, mean val. rec. loss:  1.02410696e-02\n",
      "Epoch: 4387 mean train loss:  1.79198425e-02, mean val. rec. loss:  1.02394685e-02\n",
      "Epoch: 4388 mean train loss:  1.79177811e-02, mean val. rec. loss:  1.02378718e-02\n",
      "Epoch: 4389 mean train loss:  1.79157197e-02, mean val. rec. loss:  1.02362786e-02\n",
      "Epoch: 4390 mean train loss:  1.79136470e-02, mean val. rec. loss:  1.02346899e-02\n",
      "Epoch: 4391 mean train loss:  1.79115763e-02, mean val. rec. loss:  1.02330921e-02\n",
      "Epoch: 4392 mean train loss:  1.79095037e-02, mean val. rec. loss:  1.02314807e-02\n",
      "Epoch: 4393 mean train loss:  1.79074497e-02, mean val. rec. loss:  1.02298716e-02\n",
      "Epoch: 4394 mean train loss:  1.79053752e-02, mean val. rec. loss:  1.02282580e-02\n",
      "Epoch: 4395 mean train loss:  1.79033044e-02, mean val. rec. loss:  1.02266557e-02\n",
      "Epoch: 4396 mean train loss:  1.79012523e-02, mean val. rec. loss:  1.02250670e-02\n",
      "Epoch: 4397 mean train loss:  1.78991778e-02, mean val. rec. loss:  1.02234749e-02\n",
      "Epoch: 4398 mean train loss:  1.78971220e-02, mean val. rec. loss:  1.02218918e-02\n",
      "Epoch: 4399 mean train loss:  1.78950512e-02, mean val. rec. loss:  1.02202952e-02\n",
      "Epoch: 4400 mean train loss:  1.78929953e-02, mean val. rec. loss:  1.02186917e-02\n",
      "Epoch: 4401 mean train loss:  1.78909190e-02, mean val. rec. loss:  1.02170792e-02\n",
      "Epoch: 4402 mean train loss:  1.78888650e-02, mean val. rec. loss:  1.02154758e-02\n",
      "Epoch: 4403 mean train loss:  1.78868091e-02, mean val. rec. loss:  1.02138689e-02\n",
      "Epoch: 4404 mean train loss:  1.78847347e-02, mean val. rec. loss:  1.02122666e-02\n",
      "Epoch: 4405 mean train loss:  1.78826751e-02, mean val. rec. loss:  1.02106700e-02\n",
      "Epoch: 4406 mean train loss:  1.78806192e-02, mean val. rec. loss:  1.02090734e-02\n",
      "Epoch: 4407 mean train loss:  1.78785634e-02, mean val. rec. loss:  1.02074744e-02\n",
      "Epoch: 4408 mean train loss:  1.78764963e-02, mean val. rec. loss:  1.02058778e-02\n",
      "Epoch: 4409 mean train loss:  1.78744293e-02, mean val. rec. loss:  1.02042744e-02\n",
      "Epoch: 4410 mean train loss:  1.78723734e-02, mean val. rec. loss:  1.02026664e-02\n",
      "Epoch: 4411 mean train loss:  1.78703139e-02, mean val. rec. loss:  1.02010754e-02\n",
      "Epoch: 4412 mean train loss:  1.78682561e-02, mean val. rec. loss:  1.01994708e-02\n",
      "Epoch: 4413 mean train loss:  1.78661984e-02, mean val. rec. loss:  1.01978629e-02\n",
      "Epoch: 4414 mean train loss:  1.78641426e-02, mean val. rec. loss:  1.01962628e-02\n",
      "Epoch: 4415 mean train loss:  1.78620811e-02, mean val. rec. loss:  1.01946684e-02\n",
      "Epoch: 4416 mean train loss:  1.78600308e-02, mean val. rec. loss:  1.01930605e-02\n",
      "Epoch: 4417 mean train loss:  1.78579806e-02, mean val. rec. loss:  1.01914627e-02\n",
      "Epoch: 4418 mean train loss:  1.78559191e-02, mean val. rec. loss:  1.01898627e-02\n",
      "Epoch: 4419 mean train loss:  1.78538595e-02, mean val. rec. loss:  1.01882762e-02\n",
      "Epoch: 4420 mean train loss:  1.78518018e-02, mean val. rec. loss:  1.01866830e-02\n",
      "Epoch: 4421 mean train loss:  1.78497590e-02, mean val. rec. loss:  1.01850818e-02\n",
      "Epoch: 4422 mean train loss:  1.78476994e-02, mean val. rec. loss:  1.01834761e-02\n",
      "Epoch: 4423 mean train loss:  1.78456380e-02, mean val. rec. loss:  1.01818749e-02\n",
      "Epoch: 4424 mean train loss:  1.78435970e-02, mean val. rec. loss:  1.01802635e-02\n",
      "Epoch: 4425 mean train loss:  1.78415356e-02, mean val. rec. loss:  1.01786578e-02\n",
      "Epoch: 4426 mean train loss:  1.78394909e-02, mean val. rec. loss:  1.01770635e-02\n",
      "Epoch: 4427 mean train loss:  1.78374276e-02, mean val. rec. loss:  1.01754600e-02\n",
      "Epoch: 4428 mean train loss:  1.78353866e-02, mean val. rec. loss:  1.01738520e-02\n",
      "Epoch: 4429 mean train loss:  1.78333233e-02, mean val. rec. loss:  1.01722520e-02\n",
      "Epoch: 4430 mean train loss:  1.78312786e-02, mean val. rec. loss:  1.01706656e-02\n",
      "Epoch: 4431 mean train loss:  1.78292302e-02, mean val. rec. loss:  1.01690735e-02\n",
      "Epoch: 4432 mean train loss:  1.78271706e-02, mean val. rec. loss:  1.01674814e-02\n",
      "Epoch: 4433 mean train loss:  1.78251222e-02, mean val. rec. loss:  1.01658870e-02\n",
      "Epoch: 4434 mean train loss:  1.78230775e-02, mean val. rec. loss:  1.01642903e-02\n",
      "Epoch: 4435 mean train loss:  1.78210329e-02, mean val. rec. loss:  1.01626846e-02\n",
      "Epoch: 4436 mean train loss:  1.78189751e-02, mean val. rec. loss:  1.01610631e-02\n",
      "Epoch: 4437 mean train loss:  1.78169193e-02, mean val. rec. loss:  1.01594573e-02\n",
      "Epoch: 4438 mean train loss:  1.78148727e-02, mean val. rec. loss:  1.01578562e-02\n",
      "Epoch: 4439 mean train loss:  1.78128225e-02, mean val. rec. loss:  1.01562607e-02\n",
      "Epoch: 4440 mean train loss:  1.78107759e-02, mean val. rec. loss:  1.01546618e-02\n",
      "Epoch: 4441 mean train loss:  1.78087275e-02, mean val. rec. loss:  1.01530640e-02\n",
      "Epoch: 4442 mean train loss:  1.78066791e-02, mean val. rec. loss:  1.01514730e-02\n",
      "Epoch: 4443 mean train loss:  1.78046307e-02, mean val. rec. loss:  1.01498718e-02\n",
      "Epoch: 4444 mean train loss:  1.78025897e-02, mean val. rec. loss:  1.01482627e-02\n",
      "Epoch: 4445 mean train loss:  1.78005506e-02, mean val. rec. loss:  1.01466627e-02\n",
      "Epoch: 4446 mean train loss:  1.77984985e-02, mean val. rec. loss:  1.01450502e-02\n",
      "Epoch: 4447 mean train loss:  1.77964501e-02, mean val. rec. loss:  1.01434524e-02\n",
      "Epoch: 4448 mean train loss:  1.77943979e-02, mean val. rec. loss:  1.01418433e-02\n",
      "Epoch: 4449 mean train loss:  1.77923682e-02, mean val. rec. loss:  1.01402546e-02\n",
      "Epoch: 4450 mean train loss:  1.77903179e-02, mean val. rec. loss:  1.01386534e-02\n",
      "Epoch: 4451 mean train loss:  1.77882639e-02, mean val. rec. loss:  1.01370590e-02\n",
      "Epoch: 4452 mean train loss:  1.77862322e-02, mean val. rec. loss:  1.01354522e-02\n",
      "Epoch: 4453 mean train loss:  1.77841782e-02, mean val. rec. loss:  1.01338567e-02\n",
      "Epoch: 4454 mean train loss:  1.77821447e-02, mean val. rec. loss:  1.01322510e-02\n",
      "Epoch: 4455 mean train loss:  1.77800907e-02, mean val. rec. loss:  1.01306453e-02\n",
      "Epoch: 4456 mean train loss:  1.77780572e-02, mean val. rec. loss:  1.01290361e-02\n",
      "Epoch: 4457 mean train loss:  1.77760051e-02, mean val. rec. loss:  1.01274293e-02\n",
      "Epoch: 4458 mean train loss:  1.77739679e-02, mean val. rec. loss:  1.01258213e-02\n",
      "Epoch: 4459 mean train loss:  1.77719325e-02, mean val. rec. loss:  1.01242111e-02\n",
      "Epoch: 4460 mean train loss:  1.77698785e-02, mean val. rec. loss:  1.01226190e-02\n",
      "Epoch: 4461 mean train loss:  1.77678412e-02, mean val. rec. loss:  1.01210269e-02\n",
      "Epoch: 4462 mean train loss:  1.77658040e-02, mean val. rec. loss:  1.01194416e-02\n",
      "Epoch: 4463 mean train loss:  1.77637668e-02, mean val. rec. loss:  1.01178427e-02\n",
      "Epoch: 4464 mean train loss:  1.77617146e-02, mean val. rec. loss:  1.01162392e-02\n",
      "Epoch: 4465 mean train loss:  1.77596700e-02, mean val. rec. loss:  1.01146324e-02\n",
      "Epoch: 4466 mean train loss:  1.77576327e-02, mean val. rec. loss:  1.01130142e-02\n",
      "Epoch: 4467 mean train loss:  1.77555918e-02, mean val. rec. loss:  1.01113960e-02\n",
      "Epoch: 4468 mean train loss:  1.77535508e-02, mean val. rec. loss:  1.01097948e-02\n",
      "Epoch: 4469 mean train loss:  1.77515136e-02, mean val. rec. loss:  1.01081971e-02\n",
      "Epoch: 4470 mean train loss:  1.77494726e-02, mean val. rec. loss:  1.01066004e-02\n",
      "Epoch: 4471 mean train loss:  1.77474279e-02, mean val. rec. loss:  1.01050095e-02\n",
      "Epoch: 4472 mean train loss:  1.77453963e-02, mean val. rec. loss:  1.01034162e-02\n",
      "Epoch: 4473 mean train loss:  1.77433665e-02, mean val. rec. loss:  1.01018105e-02\n",
      "Epoch: 4474 mean train loss:  1.77413218e-02, mean val. rec. loss:  1.01001946e-02\n",
      "Epoch: 4475 mean train loss:  1.77392790e-02, mean val. rec. loss:  1.00985866e-02\n",
      "Epoch: 4476 mean train loss:  1.77372380e-02, mean val. rec. loss:  1.00969809e-02\n",
      "Epoch: 4477 mean train loss:  1.77352138e-02, mean val. rec. loss:  1.00953729e-02\n",
      "Epoch: 4478 mean train loss:  1.77331691e-02, mean val. rec. loss:  1.00937842e-02\n",
      "Epoch: 4479 mean train loss:  1.77311226e-02, mean val. rec. loss:  1.00921830e-02\n",
      "Epoch: 4480 mean train loss:  1.77290965e-02, mean val. rec. loss:  1.00905751e-02\n",
      "Epoch: 4481 mean train loss:  1.77270518e-02, mean val. rec. loss:  1.00889705e-02\n",
      "Epoch: 4482 mean train loss:  1.77250239e-02, mean val. rec. loss:  1.00873602e-02\n",
      "Epoch: 4483 mean train loss:  1.77229792e-02, mean val. rec. loss:  1.00857579e-02\n",
      "Epoch: 4484 mean train loss:  1.77209494e-02, mean val. rec. loss:  1.00841375e-02\n",
      "Epoch: 4485 mean train loss:  1.77189066e-02, mean val. rec. loss:  1.00825476e-02\n",
      "Epoch: 4486 mean train loss:  1.77168731e-02, mean val. rec. loss:  1.00809261e-02\n",
      "Epoch: 4487 mean train loss:  1.77148452e-02, mean val. rec. loss:  1.00793192e-02\n",
      "Epoch: 4488 mean train loss:  1.77127986e-02, mean val. rec. loss:  1.00777214e-02\n",
      "Epoch: 4489 mean train loss:  1.77107688e-02, mean val. rec. loss:  1.00761169e-02\n",
      "Epoch: 4490 mean train loss:  1.77087390e-02, mean val. rec. loss:  1.00744987e-02\n",
      "Epoch: 4491 mean train loss:  1.77067018e-02, mean val. rec. loss:  1.00728918e-02\n",
      "Epoch: 4492 mean train loss:  1.77046571e-02, mean val. rec. loss:  1.00712952e-02\n",
      "Epoch: 4493 mean train loss:  1.77026236e-02, mean val. rec. loss:  1.00696929e-02\n",
      "Epoch: 4494 mean train loss:  1.77005938e-02, mean val. rec. loss:  1.00680838e-02\n",
      "Epoch: 4495 mean train loss:  1.76985640e-02, mean val. rec. loss:  1.00664701e-02\n",
      "Epoch: 4496 mean train loss:  1.76965287e-02, mean val. rec. loss:  1.00648587e-02\n",
      "Epoch: 4497 mean train loss:  1.76944951e-02, mean val. rec. loss:  1.00632508e-02\n",
      "Epoch: 4498 mean train loss:  1.76924598e-02, mean val. rec. loss:  1.00616292e-02\n",
      "Epoch: 4499 mean train loss:  1.76904244e-02, mean val. rec. loss:  1.00600212e-02\n",
      "Epoch: 4500 mean train loss:  1.76883872e-02, mean val. rec. loss:  1.00584200e-02\n",
      "Epoch: 4501 mean train loss:  1.76863518e-02, mean val. rec. loss:  1.00568279e-02\n",
      "Epoch: 4502 mean train loss:  1.76843257e-02, mean val. rec. loss:  1.00552256e-02\n",
      "Epoch: 4503 mean train loss:  1.76822997e-02, mean val. rec. loss:  1.00536267e-02\n",
      "Epoch: 4504 mean train loss:  1.76802606e-02, mean val. rec. loss:  1.00520210e-02\n",
      "Epoch: 4505 mean train loss:  1.76782252e-02, mean val. rec. loss:  1.00504028e-02\n",
      "Epoch: 4506 mean train loss:  1.76762028e-02, mean val. rec. loss:  1.00487767e-02\n",
      "Epoch: 4507 mean train loss:  1.76741638e-02, mean val. rec. loss:  1.00471630e-02\n",
      "Epoch: 4508 mean train loss:  1.76721265e-02, mean val. rec. loss:  1.00455505e-02\n",
      "Epoch: 4509 mean train loss:  1.76701060e-02, mean val. rec. loss:  1.00439459e-02\n",
      "Epoch: 4510 mean train loss:  1.76680651e-02, mean val. rec. loss:  1.00423357e-02\n",
      "Epoch: 4511 mean train loss:  1.76660427e-02, mean val. rec. loss:  1.00407334e-02\n",
      "Epoch: 4512 mean train loss:  1.76640018e-02, mean val. rec. loss:  1.00391311e-02\n",
      "Epoch: 4513 mean train loss:  1.76619794e-02, mean val. rec. loss:  1.00375163e-02\n",
      "Epoch: 4514 mean train loss:  1.76599385e-02, mean val. rec. loss:  1.00358970e-02\n",
      "Epoch: 4515 mean train loss:  1.76579161e-02, mean val. rec. loss:  1.00342822e-02\n",
      "Epoch: 4516 mean train loss:  1.76558901e-02, mean val. rec. loss:  1.00326583e-02\n",
      "Epoch: 4517 mean train loss:  1.76538491e-02, mean val. rec. loss:  1.00310526e-02\n",
      "Epoch: 4518 mean train loss:  1.76518193e-02, mean val. rec. loss:  1.00294515e-02\n",
      "Epoch: 4519 mean train loss:  1.76497970e-02, mean val. rec. loss:  1.00278548e-02\n",
      "Epoch: 4520 mean train loss:  1.76477672e-02, mean val. rec. loss:  1.00262468e-02\n",
      "Epoch: 4521 mean train loss:  1.76457337e-02, mean val. rec. loss:  1.00246196e-02\n",
      "Epoch: 4522 mean train loss:  1.76436965e-02, mean val. rec. loss:  1.00230105e-02\n",
      "Epoch: 4523 mean train loss:  1.76416667e-02, mean val. rec. loss:  1.00213934e-02\n",
      "Epoch: 4524 mean train loss:  1.76396369e-02, mean val. rec. loss:  1.00197809e-02\n",
      "Epoch: 4525 mean train loss:  1.76376071e-02, mean val. rec. loss:  1.00181661e-02\n",
      "Epoch: 4526 mean train loss:  1.76355773e-02, mean val. rec. loss:  1.00165423e-02\n",
      "Epoch: 4527 mean train loss:  1.76335512e-02, mean val. rec. loss:  1.00149207e-02\n",
      "Epoch: 4528 mean train loss:  1.76315177e-02, mean val. rec. loss:  1.00133002e-02\n",
      "Epoch: 4529 mean train loss:  1.76294954e-02, mean val. rec. loss:  1.00116979e-02\n",
      "Epoch: 4530 mean train loss:  1.76274730e-02, mean val. rec. loss:  1.00100945e-02\n",
      "Epoch: 4531 mean train loss:  1.76254395e-02, mean val. rec. loss:  1.00084956e-02\n",
      "Epoch: 4532 mean train loss:  1.76234079e-02, mean val. rec. loss:  1.00068876e-02\n",
      "Epoch: 4533 mean train loss:  1.76213725e-02, mean val. rec. loss:  1.00052683e-02\n",
      "Epoch: 4534 mean train loss:  1.76193576e-02, mean val. rec. loss:  1.00036365e-02\n",
      "Epoch: 4535 mean train loss:  1.76173241e-02, mean val. rec. loss:  1.00020070e-02\n",
      "Epoch: 4536 mean train loss:  1.76152869e-02, mean val. rec. loss:  1.00003706e-02\n",
      "Epoch: 4537 mean train loss:  1.76132701e-02, mean val. rec. loss:  9.99876152e-03\n",
      "Epoch: 4538 mean train loss:  1.76112347e-02, mean val. rec. loss:  9.99715807e-03\n",
      "Epoch: 4539 mean train loss:  1.76092161e-02, mean val. rec. loss:  9.99554329e-03\n",
      "Epoch: 4540 mean train loss:  1.76071789e-02, mean val. rec. loss:  9.99392737e-03\n",
      "Epoch: 4541 mean train loss:  1.76051640e-02, mean val. rec. loss:  9.99231599e-03\n",
      "Epoch: 4542 mean train loss:  1.76031230e-02, mean val. rec. loss:  9.99069214e-03\n",
      "Epoch: 4543 mean train loss:  1.76011026e-02, mean val. rec. loss:  9.98906715e-03\n",
      "Epoch: 4544 mean train loss:  1.75990746e-02, mean val. rec. loss:  9.98744216e-03\n",
      "Epoch: 4545 mean train loss:  1.75970430e-02, mean val. rec. loss:  9.98582738e-03\n",
      "Epoch: 4546 mean train loss:  1.75950188e-02, mean val. rec. loss:  9.98421713e-03\n",
      "Epoch: 4547 mean train loss:  1.75929946e-02, mean val. rec. loss:  9.98259781e-03\n",
      "Epoch: 4548 mean train loss:  1.75909555e-02, mean val. rec. loss:  9.98097623e-03\n",
      "Epoch: 4549 mean train loss:  1.75889313e-02, mean val. rec. loss:  9.97936144e-03\n",
      "Epoch: 4550 mean train loss:  1.75869071e-02, mean val. rec. loss:  9.97773079e-03\n",
      "Epoch: 4551 mean train loss:  1.75848810e-02, mean val. rec. loss:  9.97611827e-03\n",
      "Epoch: 4552 mean train loss:  1.75828587e-02, mean val. rec. loss:  9.97449215e-03\n",
      "Epoch: 4553 mean train loss:  1.75808326e-02, mean val. rec. loss:  9.97287056e-03\n",
      "Epoch: 4554 mean train loss:  1.75788028e-02, mean val. rec. loss:  9.97124671e-03\n",
      "Epoch: 4555 mean train loss:  1.75767749e-02, mean val. rec. loss:  9.96963193e-03\n",
      "Epoch: 4556 mean train loss:  1.75747488e-02, mean val. rec. loss:  9.96801828e-03\n",
      "Epoch: 4557 mean train loss:  1.75727209e-02, mean val. rec. loss:  9.96639782e-03\n",
      "Epoch: 4558 mean train loss:  1.75706911e-02, mean val. rec. loss:  9.96477851e-03\n",
      "Epoch: 4559 mean train loss:  1.75686613e-02, mean val. rec. loss:  9.96315465e-03\n",
      "Epoch: 4560 mean train loss:  1.75666334e-02, mean val. rec. loss:  9.96152513e-03\n",
      "Epoch: 4561 mean train loss:  1.75646129e-02, mean val. rec. loss:  9.95989787e-03\n",
      "Epoch: 4562 mean train loss:  1.75625869e-02, mean val. rec. loss:  9.95827515e-03\n",
      "Epoch: 4563 mean train loss:  1.75605571e-02, mean val. rec. loss:  9.95664563e-03\n",
      "Epoch: 4564 mean train loss:  1.75585254e-02, mean val. rec. loss:  9.95502404e-03\n",
      "Epoch: 4565 mean train loss:  1.75565087e-02, mean val. rec. loss:  9.95340699e-03\n",
      "Epoch: 4566 mean train loss:  1.75544770e-02, mean val. rec. loss:  9.95178313e-03\n",
      "Epoch: 4567 mean train loss:  1.75524528e-02, mean val. rec. loss:  9.95016382e-03\n",
      "Epoch: 4568 mean train loss:  1.75504268e-02, mean val. rec. loss:  9.94855130e-03\n",
      "Epoch: 4569 mean train loss:  1.75483970e-02, mean val. rec. loss:  9.94692178e-03\n",
      "Epoch: 4570 mean train loss:  1.75463709e-02, mean val. rec. loss:  9.94528998e-03\n",
      "Epoch: 4571 mean train loss:  1.75443486e-02, mean val. rec. loss:  9.94366159e-03\n",
      "Epoch: 4572 mean train loss:  1.75423188e-02, mean val. rec. loss:  9.94203094e-03\n",
      "Epoch: 4573 mean train loss:  1.75402983e-02, mean val. rec. loss:  9.94039801e-03\n",
      "Epoch: 4574 mean train loss:  1.75382611e-02, mean val. rec. loss:  9.93876168e-03\n",
      "Epoch: 4575 mean train loss:  1.75362406e-02, mean val. rec. loss:  9.93712762e-03\n",
      "Epoch: 4576 mean train loss:  1.75342183e-02, mean val. rec. loss:  9.93550377e-03\n",
      "Epoch: 4577 mean train loss:  1.75321978e-02, mean val. rec. loss:  9.93387878e-03\n",
      "Epoch: 4578 mean train loss:  1.75301587e-02, mean val. rec. loss:  9.93224585e-03\n",
      "Epoch: 4579 mean train loss:  1.75281326e-02, mean val. rec. loss:  9.93060952e-03\n",
      "Epoch: 4580 mean train loss:  1.75261103e-02, mean val. rec. loss:  9.92897433e-03\n",
      "Epoch: 4581 mean train loss:  1.75240880e-02, mean val. rec. loss:  9.92735501e-03\n",
      "Epoch: 4582 mean train loss:  1.75220638e-02, mean val. rec. loss:  9.92573909e-03\n",
      "Epoch: 4583 mean train loss:  1.75200377e-02, mean val. rec. loss:  9.92410050e-03\n",
      "Epoch: 4584 mean train loss:  1.75180135e-02, mean val. rec. loss:  9.92247551e-03\n",
      "Epoch: 4585 mean train loss:  1.75159874e-02, mean val. rec. loss:  9.92084372e-03\n",
      "Epoch: 4586 mean train loss:  1.75139614e-02, mean val. rec. loss:  9.91920058e-03\n",
      "Epoch: 4587 mean train loss:  1.75119316e-02, mean val. rec. loss:  9.91756652e-03\n",
      "Epoch: 4588 mean train loss:  1.75099037e-02, mean val. rec. loss:  9.91594607e-03\n",
      "Epoch: 4589 mean train loss:  1.75078757e-02, mean val. rec. loss:  9.91430634e-03\n",
      "Epoch: 4590 mean train loss:  1.75058552e-02, mean val. rec. loss:  9.91269042e-03\n",
      "Epoch: 4591 mean train loss:  1.75038329e-02, mean val. rec. loss:  9.91105069e-03\n",
      "Epoch: 4592 mean train loss:  1.75018013e-02, mean val. rec. loss:  9.90942684e-03\n",
      "Epoch: 4593 mean train loss:  1.74997715e-02, mean val. rec. loss:  9.90777690e-03\n",
      "Epoch: 4594 mean train loss:  1.74977566e-02, mean val. rec. loss:  9.90615305e-03\n",
      "Epoch: 4595 mean train loss:  1.74957231e-02, mean val. rec. loss:  9.90452353e-03\n",
      "Epoch: 4596 mean train loss:  1.74936896e-02, mean val. rec. loss:  9.90289854e-03\n",
      "Epoch: 4597 mean train loss:  1.74916765e-02, mean val. rec. loss:  9.90126448e-03\n",
      "Epoch: 4598 mean train loss:  1.74896412e-02, mean val. rec. loss:  9.89962021e-03\n",
      "Epoch: 4599 mean train loss:  1.74876225e-02, mean val. rec. loss:  9.89797935e-03\n",
      "Epoch: 4600 mean train loss:  1.74855890e-02, mean val. rec. loss:  9.89633962e-03\n",
      "Epoch: 4601 mean train loss:  1.74835723e-02, mean val. rec. loss:  9.89469648e-03\n",
      "Epoch: 4602 mean train loss:  1.74815369e-02, mean val. rec. loss:  9.89306696e-03\n",
      "Epoch: 4603 mean train loss:  1.74795146e-02, mean val. rec. loss:  9.89141702e-03\n",
      "Epoch: 4604 mean train loss:  1.74774959e-02, mean val. rec. loss:  9.88977276e-03\n",
      "Epoch: 4605 mean train loss:  1.74754569e-02, mean val. rec. loss:  9.88814890e-03\n",
      "Epoch: 4606 mean train loss:  1.74734345e-02, mean val. rec. loss:  9.88651824e-03\n",
      "Epoch: 4607 mean train loss:  1.74714122e-02, mean val. rec. loss:  9.88489439e-03\n",
      "Epoch: 4608 mean train loss:  1.74693898e-02, mean val. rec. loss:  9.88326373e-03\n",
      "Epoch: 4609 mean train loss:  1.74673582e-02, mean val. rec. loss:  9.88162400e-03\n",
      "Epoch: 4610 mean train loss:  1.74653247e-02, mean val. rec. loss:  9.87997974e-03\n",
      "Epoch: 4611 mean train loss:  1.74633005e-02, mean val. rec. loss:  9.87831733e-03\n",
      "Epoch: 4612 mean train loss:  1.74612763e-02, mean val. rec. loss:  9.87665945e-03\n",
      "Epoch: 4613 mean train loss:  1.74592483e-02, mean val. rec. loss:  9.87501065e-03\n",
      "Epoch: 4614 mean train loss:  1.74572241e-02, mean val. rec. loss:  9.87337546e-03\n",
      "Epoch: 4615 mean train loss:  1.74551962e-02, mean val. rec. loss:  9.87175614e-03\n",
      "Epoch: 4616 mean train loss:  1.74531683e-02, mean val. rec. loss:  9.87012548e-03\n",
      "Epoch: 4617 mean train loss:  1.74511404e-02, mean val. rec. loss:  9.86849142e-03\n",
      "Epoch: 4618 mean train loss:  1.74491124e-02, mean val. rec. loss:  9.86685736e-03\n",
      "Epoch: 4619 mean train loss:  1.74470938e-02, mean val. rec. loss:  9.86519835e-03\n",
      "Epoch: 4620 mean train loss:  1.74450715e-02, mean val. rec. loss:  9.86354841e-03\n",
      "Epoch: 4621 mean train loss:  1.74430380e-02, mean val. rec. loss:  9.86190188e-03\n",
      "Epoch: 4622 mean train loss:  1.74410101e-02, mean val. rec. loss:  9.86025081e-03\n",
      "Epoch: 4623 mean train loss:  1.74389784e-02, mean val. rec. loss:  9.85861335e-03\n",
      "Epoch: 4624 mean train loss:  1.74369654e-02, mean val. rec. loss:  9.85697815e-03\n",
      "Epoch: 4625 mean train loss:  1.74349319e-02, mean val. rec. loss:  9.85533502e-03\n",
      "Epoch: 4626 mean train loss:  1.74328965e-02, mean val. rec. loss:  9.85368735e-03\n",
      "Epoch: 4627 mean train loss:  1.74308835e-02, mean val. rec. loss:  9.85203061e-03\n",
      "Epoch: 4628 mean train loss:  1.74288462e-02, mean val. rec. loss:  9.85036820e-03\n",
      "Epoch: 4629 mean train loss:  1.74268295e-02, mean val. rec. loss:  9.84872961e-03\n",
      "Epoch: 4630 mean train loss:  1.74247885e-02, mean val. rec. loss:  9.84709215e-03\n",
      "Epoch: 4631 mean train loss:  1.74227718e-02, mean val. rec. loss:  9.84544901e-03\n",
      "Epoch: 4632 mean train loss:  1.74207327e-02, mean val. rec. loss:  9.84380815e-03\n",
      "Epoch: 4633 mean train loss:  1.74187140e-02, mean val. rec. loss:  9.84216615e-03\n",
      "Epoch: 4634 mean train loss:  1.74166936e-02, mean val. rec. loss:  9.84051735e-03\n",
      "Epoch: 4635 mean train loss:  1.74146545e-02, mean val. rec. loss:  9.83886401e-03\n",
      "Epoch: 4636 mean train loss:  1.74126284e-02, mean val. rec. loss:  9.83721294e-03\n",
      "Epoch: 4637 mean train loss:  1.74106061e-02, mean val. rec. loss:  9.83555280e-03\n",
      "Epoch: 4638 mean train loss:  1.74085726e-02, mean val. rec. loss:  9.83389719e-03\n",
      "Epoch: 4639 mean train loss:  1.74065428e-02, mean val. rec. loss:  9.83225179e-03\n",
      "Epoch: 4640 mean train loss:  1.74045167e-02, mean val. rec. loss:  9.83059959e-03\n",
      "Epoch: 4641 mean train loss:  1.74024906e-02, mean val. rec. loss:  9.82896326e-03\n",
      "Epoch: 4642 mean train loss:  1.74004646e-02, mean val. rec. loss:  9.82733147e-03\n",
      "Epoch: 4643 mean train loss:  1.73984385e-02, mean val. rec. loss:  9.82568040e-03\n",
      "Epoch: 4644 mean train loss:  1.73964106e-02, mean val. rec. loss:  9.82403160e-03\n",
      "Epoch: 4645 mean train loss:  1.73943827e-02, mean val. rec. loss:  9.82238166e-03\n",
      "Epoch: 4646 mean train loss:  1.73923566e-02, mean val. rec. loss:  9.82072492e-03\n",
      "Epoch: 4647 mean train loss:  1.73903268e-02, mean val. rec. loss:  9.81905571e-03\n",
      "Epoch: 4648 mean train loss:  1.73882933e-02, mean val. rec. loss:  9.81739784e-03\n",
      "Epoch: 4649 mean train loss:  1.73862635e-02, mean val. rec. loss:  9.81574110e-03\n",
      "Epoch: 4650 mean train loss:  1.73842300e-02, mean val. rec. loss:  9.81409116e-03\n",
      "Epoch: 4651 mean train loss:  1.73822002e-02, mean val. rec. loss:  9.81242421e-03\n",
      "Epoch: 4652 mean train loss:  1.73801779e-02, mean val. rec. loss:  9.81077768e-03\n",
      "Epoch: 4653 mean train loss:  1.73781518e-02, mean val. rec. loss:  9.80912888e-03\n",
      "Epoch: 4654 mean train loss:  1.73761183e-02, mean val. rec. loss:  9.80747554e-03\n",
      "Epoch: 4655 mean train loss:  1.73740848e-02, mean val. rec. loss:  9.80582674e-03\n",
      "Epoch: 4656 mean train loss:  1.73720662e-02, mean val. rec. loss:  9.80417227e-03\n",
      "Epoch: 4657 mean train loss:  1.73700327e-02, mean val. rec. loss:  9.80251326e-03\n",
      "Epoch: 4658 mean train loss:  1.73679954e-02, mean val. rec. loss:  9.80086105e-03\n",
      "Epoch: 4659 mean train loss:  1.73659768e-02, mean val. rec. loss:  9.79918617e-03\n",
      "Epoch: 4660 mean train loss:  1.73639377e-02, mean val. rec. loss:  9.79752603e-03\n",
      "Epoch: 4661 mean train loss:  1.73619172e-02, mean val. rec. loss:  9.79586022e-03\n",
      "Epoch: 4662 mean train loss:  1.73598763e-02, mean val. rec. loss:  9.79420575e-03\n",
      "Epoch: 4663 mean train loss:  1.73578558e-02, mean val. rec. loss:  9.79255241e-03\n",
      "Epoch: 4664 mean train loss:  1.73558167e-02, mean val. rec. loss:  9.79089227e-03\n",
      "Epoch: 4665 mean train loss:  1.73537925e-02, mean val. rec. loss:  9.78923666e-03\n",
      "Epoch: 4666 mean train loss:  1.73517683e-02, mean val. rec. loss:  9.78758219e-03\n",
      "Epoch: 4667 mean train loss:  1.73497292e-02, mean val. rec. loss:  9.78592772e-03\n",
      "Epoch: 4668 mean train loss:  1.73477013e-02, mean val. rec. loss:  9.78426644e-03\n",
      "Epoch: 4669 mean train loss:  1.73456752e-02, mean val. rec. loss:  9.78260290e-03\n",
      "Epoch: 4670 mean train loss:  1.73436398e-02, mean val. rec. loss:  9.78094049e-03\n",
      "Epoch: 4671 mean train loss:  1.73416045e-02, mean val. rec. loss:  9.77927694e-03\n",
      "Epoch: 4672 mean train loss:  1.73395765e-02, mean val. rec. loss:  9.77759979e-03\n",
      "Epoch: 4673 mean train loss:  1.73375467e-02, mean val. rec. loss:  9.77591697e-03\n",
      "Epoch: 4674 mean train loss:  1.73355188e-02, mean val. rec. loss:  9.77426363e-03\n",
      "Epoch: 4675 mean train loss:  1.73334890e-02, mean val. rec. loss:  9.77259555e-03\n",
      "Epoch: 4676 mean train loss:  1.73314574e-02, mean val. rec. loss:  9.77094448e-03\n",
      "Epoch: 4677 mean train loss:  1.73294276e-02, mean val. rec. loss:  9.76930135e-03\n",
      "Epoch: 4678 mean train loss:  1.73273959e-02, mean val. rec. loss:  9.76764348e-03\n",
      "Epoch: 4679 mean train loss:  1.73253643e-02, mean val. rec. loss:  9.76598787e-03\n",
      "Epoch: 4680 mean train loss:  1.73233326e-02, mean val. rec. loss:  9.76431752e-03\n",
      "Epoch: 4681 mean train loss:  1.73212991e-02, mean val. rec. loss:  9.76264264e-03\n",
      "Epoch: 4682 mean train loss:  1.73192656e-02, mean val. rec. loss:  9.76096549e-03\n",
      "Epoch: 4683 mean train loss:  1.73172302e-02, mean val. rec. loss:  9.75930195e-03\n",
      "Epoch: 4684 mean train loss:  1.73151949e-02, mean val. rec. loss:  9.75761913e-03\n",
      "Epoch: 4685 mean train loss:  1.73131688e-02, mean val. rec. loss:  9.75596352e-03\n",
      "Epoch: 4686 mean train loss:  1.73111390e-02, mean val. rec. loss:  9.75430678e-03\n",
      "Epoch: 4687 mean train loss:  1.73090999e-02, mean val. rec. loss:  9.75264550e-03\n",
      "Epoch: 4688 mean train loss:  1.73070627e-02, mean val. rec. loss:  9.75098536e-03\n",
      "Epoch: 4689 mean train loss:  1.73050422e-02, mean val. rec. loss:  9.74931728e-03\n",
      "Epoch: 4690 mean train loss:  1.73030031e-02, mean val. rec. loss:  9.74764127e-03\n",
      "Epoch: 4691 mean train loss:  1.73009640e-02, mean val. rec. loss:  9.74596411e-03\n",
      "Epoch: 4692 mean train loss:  1.72989398e-02, mean val. rec. loss:  9.74427902e-03\n",
      "Epoch: 4693 mean train loss:  1.72968970e-02, mean val. rec. loss:  9.74261888e-03\n",
      "Epoch: 4694 mean train loss:  1.72948746e-02, mean val. rec. loss:  9.74095421e-03\n",
      "Epoch: 4695 mean train loss:  1.72928300e-02, mean val. rec. loss:  9.73928499e-03\n",
      "Epoch: 4696 mean train loss:  1.72908076e-02, mean val. rec. loss:  9.73760671e-03\n",
      "Epoch: 4697 mean train loss:  1.72887629e-02, mean val. rec. loss:  9.73593749e-03\n",
      "Epoch: 4698 mean train loss:  1.72867369e-02, mean val. rec. loss:  9.73427055e-03\n",
      "Epoch: 4699 mean train loss:  1.72847052e-02, mean val. rec. loss:  9.73261041e-03\n",
      "Epoch: 4700 mean train loss:  1.72826624e-02, mean val. rec. loss:  9.73093666e-03\n",
      "Epoch: 4701 mean train loss:  1.72806308e-02, mean val. rec. loss:  9.72925610e-03\n",
      "Epoch: 4702 mean train loss:  1.72786010e-02, mean val. rec. loss:  9.72757555e-03\n",
      "Epoch: 4703 mean train loss:  1.72765600e-02, mean val. rec. loss:  9.72589953e-03\n",
      "Epoch: 4704 mean train loss:  1.72745228e-02, mean val. rec. loss:  9.72422805e-03\n",
      "Epoch: 4705 mean train loss:  1.72724911e-02, mean val. rec. loss:  9.72254750e-03\n",
      "Epoch: 4706 mean train loss:  1.72704576e-02, mean val. rec. loss:  9.72086241e-03\n",
      "Epoch: 4707 mean train loss:  1.72684260e-02, mean val. rec. loss:  9.71918185e-03\n",
      "Epoch: 4708 mean train loss:  1.72663943e-02, mean val. rec. loss:  9.71752625e-03\n",
      "Epoch: 4709 mean train loss:  1.72643533e-02, mean val. rec. loss:  9.71585023e-03\n",
      "Epoch: 4710 mean train loss:  1.72623161e-02, mean val. rec. loss:  9.71416741e-03\n",
      "Epoch: 4711 mean train loss:  1.72602826e-02, mean val. rec. loss:  9.71249479e-03\n",
      "Epoch: 4712 mean train loss:  1.72582491e-02, mean val. rec. loss:  9.71081084e-03\n",
      "Epoch: 4713 mean train loss:  1.72562119e-02, mean val. rec. loss:  9.70913482e-03\n",
      "Epoch: 4714 mean train loss:  1.72541783e-02, mean val. rec. loss:  9.70746107e-03\n",
      "Epoch: 4715 mean train loss:  1.72521411e-02, mean val. rec. loss:  9.70578279e-03\n",
      "Epoch: 4716 mean train loss:  1.72501001e-02, mean val. rec. loss:  9.70410450e-03\n",
      "Epoch: 4717 mean train loss:  1.72480610e-02, mean val. rec. loss:  9.70240807e-03\n",
      "Epoch: 4718 mean train loss:  1.72460238e-02, mean val. rec. loss:  9.70072525e-03\n",
      "Epoch: 4719 mean train loss:  1.72439810e-02, mean val. rec. loss:  9.69904470e-03\n",
      "Epoch: 4720 mean train loss:  1.72419586e-02, mean val. rec. loss:  9.69737435e-03\n",
      "Epoch: 4721 mean train loss:  1.72399177e-02, mean val. rec. loss:  9.69571988e-03\n",
      "Epoch: 4722 mean train loss:  1.72378730e-02, mean val. rec. loss:  9.69403933e-03\n",
      "Epoch: 4723 mean train loss:  1.72358395e-02, mean val. rec. loss:  9.69235537e-03\n",
      "Epoch: 4724 mean train loss:  1.72338022e-02, mean val. rec. loss:  9.69067595e-03\n",
      "Epoch: 4725 mean train loss:  1.72317613e-02, mean val. rec. loss:  9.68899767e-03\n",
      "Epoch: 4726 mean train loss:  1.72297334e-02, mean val. rec. loss:  9.68731258e-03\n",
      "Epoch: 4727 mean train loss:  1.72276868e-02, mean val. rec. loss:  9.68561275e-03\n",
      "Epoch: 4728 mean train loss:  1.72256570e-02, mean val. rec. loss:  9.68391745e-03\n",
      "Epoch: 4729 mean train loss:  1.72236105e-02, mean val. rec. loss:  9.68222783e-03\n",
      "Epoch: 4730 mean train loss:  1.72215807e-02, mean val. rec. loss:  9.68053820e-03\n",
      "Epoch: 4731 mean train loss:  1.72195341e-02, mean val. rec. loss:  9.67886899e-03\n",
      "Epoch: 4732 mean train loss:  1.72175006e-02, mean val. rec. loss:  9.67719864e-03\n",
      "Epoch: 4733 mean train loss:  1.72154504e-02, mean val. rec. loss:  9.67552376e-03\n",
      "Epoch: 4734 mean train loss:  1.72134187e-02, mean val. rec. loss:  9.67383980e-03\n",
      "Epoch: 4735 mean train loss:  1.72113852e-02, mean val. rec. loss:  9.67214791e-03\n",
      "Epoch: 4736 mean train loss:  1.72093293e-02, mean val. rec. loss:  9.67045715e-03\n",
      "Epoch: 4737 mean train loss:  1.72072958e-02, mean val. rec. loss:  9.66876185e-03\n",
      "Epoch: 4738 mean train loss:  1.72052605e-02, mean val. rec. loss:  9.66706429e-03\n",
      "Epoch: 4739 mean train loss:  1.72032251e-02, mean val. rec. loss:  9.66537013e-03\n",
      "Epoch: 4740 mean train loss:  1.72011804e-02, mean val. rec. loss:  9.66369751e-03\n",
      "Epoch: 4741 mean train loss:  1.71991320e-02, mean val. rec. loss:  9.66201809e-03\n",
      "Epoch: 4742 mean train loss:  1.71970948e-02, mean val. rec. loss:  9.66034094e-03\n",
      "Epoch: 4743 mean train loss:  1.71950557e-02, mean val. rec. loss:  9.65865472e-03\n",
      "Epoch: 4744 mean train loss:  1.71930166e-02, mean val. rec. loss:  9.65696283e-03\n",
      "Epoch: 4745 mean train loss:  1.71909775e-02, mean val. rec. loss:  9.65526867e-03\n",
      "Epoch: 4746 mean train loss:  1.71889365e-02, mean val. rec. loss:  9.65357337e-03\n",
      "Epoch: 4747 mean train loss:  1.71868937e-02, mean val. rec. loss:  9.65188261e-03\n",
      "Epoch: 4748 mean train loss:  1.71848527e-02, mean val. rec. loss:  9.65017711e-03\n",
      "Epoch: 4749 mean train loss:  1.71828080e-02, mean val. rec. loss:  9.64848862e-03\n",
      "Epoch: 4750 mean train loss:  1.71807652e-02, mean val. rec. loss:  9.64680807e-03\n",
      "Epoch: 4751 mean train loss:  1.71787224e-02, mean val. rec. loss:  9.64512411e-03\n",
      "Epoch: 4752 mean train loss:  1.71766870e-02, mean val. rec. loss:  9.64342768e-03\n",
      "Epoch: 4753 mean train loss:  1.71746516e-02, mean val. rec. loss:  9.64173579e-03\n",
      "Epoch: 4754 mean train loss:  1.71726051e-02, mean val. rec. loss:  9.64003709e-03\n",
      "Epoch: 4755 mean train loss:  1.71705585e-02, mean val. rec. loss:  9.63834520e-03\n",
      "Epoch: 4756 mean train loss:  1.71685101e-02, mean val. rec. loss:  9.63664197e-03\n",
      "Epoch: 4757 mean train loss:  1.71664841e-02, mean val. rec. loss:  9.63493873e-03\n",
      "Epoch: 4758 mean train loss:  1.71644319e-02, mean val. rec. loss:  9.63325024e-03\n",
      "Epoch: 4759 mean train loss:  1.71623817e-02, mean val. rec. loss:  9.63155155e-03\n",
      "Epoch: 4760 mean train loss:  1.71603500e-02, mean val. rec. loss:  9.62986079e-03\n",
      "Epoch: 4761 mean train loss:  1.71583016e-02, mean val. rec. loss:  9.62817229e-03\n",
      "Epoch: 4762 mean train loss:  1.71562569e-02, mean val. rec. loss:  9.62646906e-03\n",
      "Epoch: 4763 mean train loss:  1.71542160e-02, mean val. rec. loss:  9.62476470e-03\n",
      "Epoch: 4764 mean train loss:  1.71521731e-02, mean val. rec. loss:  9.62308074e-03\n",
      "Epoch: 4765 mean train loss:  1.71501284e-02, mean val. rec. loss:  9.62138545e-03\n",
      "Epoch: 4766 mean train loss:  1.71480819e-02, mean val. rec. loss:  9.61970376e-03\n",
      "Epoch: 4767 mean train loss:  1.71460372e-02, mean val. rec. loss:  9.61800619e-03\n",
      "Epoch: 4768 mean train loss:  1.71440000e-02, mean val. rec. loss:  9.61630296e-03\n",
      "Epoch: 4769 mean train loss:  1.71419460e-02, mean val. rec. loss:  9.61459519e-03\n",
      "Epoch: 4770 mean train loss:  1.71399087e-02, mean val. rec. loss:  9.61288969e-03\n",
      "Epoch: 4771 mean train loss:  1.71378585e-02, mean val. rec. loss:  9.61118646e-03\n",
      "Epoch: 4772 mean train loss:  1.71358138e-02, mean val. rec. loss:  9.60948890e-03\n",
      "Epoch: 4773 mean train loss:  1.71337710e-02, mean val. rec. loss:  9.60779927e-03\n",
      "Epoch: 4774 mean train loss:  1.71317300e-02, mean val. rec. loss:  9.60609490e-03\n",
      "Epoch: 4775 mean train loss:  1.71296797e-02, mean val. rec. loss:  9.60440528e-03\n",
      "Epoch: 4776 mean train loss:  1.71276276e-02, mean val. rec. loss:  9.60269751e-03\n",
      "Epoch: 4777 mean train loss:  1.71255848e-02, mean val. rec. loss:  9.60099881e-03\n",
      "Epoch: 4778 mean train loss:  1.71235419e-02, mean val. rec. loss:  9.59929105e-03\n",
      "Epoch: 4779 mean train loss:  1.71214973e-02, mean val. rec. loss:  9.59758781e-03\n",
      "Epoch: 4780 mean train loss:  1.71194526e-02, mean val. rec. loss:  9.59589365e-03\n",
      "Epoch: 4781 mean train loss:  1.71174079e-02, mean val. rec. loss:  9.59419155e-03\n",
      "Epoch: 4782 mean train loss:  1.71153613e-02, mean val. rec. loss:  9.59248492e-03\n",
      "Epoch: 4783 mean train loss:  1.71133148e-02, mean val. rec. loss:  9.59078849e-03\n",
      "Epoch: 4784 mean train loss:  1.71112664e-02, mean val. rec. loss:  9.58909093e-03\n",
      "Epoch: 4785 mean train loss:  1.71092217e-02, mean val. rec. loss:  9.58738656e-03\n",
      "Epoch: 4786 mean train loss:  1.71071733e-02, mean val. rec. loss:  9.58568106e-03\n",
      "Epoch: 4787 mean train loss:  1.71051212e-02, mean val. rec. loss:  9.58398463e-03\n",
      "Epoch: 4788 mean train loss:  1.71030746e-02, mean val. rec. loss:  9.58227232e-03\n",
      "Epoch: 4789 mean train loss:  1.71010243e-02, mean val. rec. loss:  9.58057136e-03\n",
      "Epoch: 4790 mean train loss:  1.70989741e-02, mean val. rec. loss:  9.57886586e-03\n",
      "Epoch: 4791 mean train loss:  1.70969201e-02, mean val. rec. loss:  9.57714675e-03\n",
      "Epoch: 4792 mean train loss:  1.70948828e-02, mean val. rec. loss:  9.57544465e-03\n",
      "Epoch: 4793 mean train loss:  1.70928326e-02, mean val. rec. loss:  9.57373915e-03\n",
      "Epoch: 4794 mean train loss:  1.70907767e-02, mean val. rec. loss:  9.57203025e-03\n",
      "Epoch: 4795 mean train loss:  1.70887246e-02, mean val. rec. loss:  9.57030887e-03\n",
      "Epoch: 4796 mean train loss:  1.70866855e-02, mean val. rec. loss:  9.56859543e-03\n",
      "Epoch: 4797 mean train loss:  1.70846315e-02, mean val. rec. loss:  9.56689447e-03\n",
      "Epoch: 4798 mean train loss:  1.70825849e-02, mean val. rec. loss:  9.56520824e-03\n",
      "Epoch: 4799 mean train loss:  1.70805365e-02, mean val. rec. loss:  9.56351408e-03\n",
      "Epoch: 4800 mean train loss:  1.70784807e-02, mean val. rec. loss:  9.56180518e-03\n",
      "Epoch: 4801 mean train loss:  1.70764397e-02, mean val. rec. loss:  9.56008721e-03\n",
      "Epoch: 4802 mean train loss:  1.70743820e-02, mean val. rec. loss:  9.55836356e-03\n",
      "Epoch: 4803 mean train loss:  1.70723410e-02, mean val. rec. loss:  9.55663992e-03\n",
      "Epoch: 4804 mean train loss:  1.70702814e-02, mean val. rec. loss:  9.55492761e-03\n",
      "Epoch: 4805 mean train loss:  1.70682386e-02, mean val. rec. loss:  9.55321531e-03\n",
      "Epoch: 4806 mean train loss:  1.70661790e-02, mean val. rec. loss:  9.55150300e-03\n",
      "Epoch: 4807 mean train loss:  1.70641343e-02, mean val. rec. loss:  9.54979750e-03\n",
      "Epoch: 4808 mean train loss:  1.70620878e-02, mean val. rec. loss:  9.54810221e-03\n",
      "Epoch: 4809 mean train loss:  1.70600264e-02, mean val. rec. loss:  9.54640805e-03\n",
      "Epoch: 4810 mean train loss:  1.70579817e-02, mean val. rec. loss:  9.54469574e-03\n",
      "Epoch: 4811 mean train loss:  1.70559370e-02, mean val. rec. loss:  9.54297777e-03\n",
      "Epoch: 4812 mean train loss:  1.70538700e-02, mean val. rec. loss:  9.54126206e-03\n",
      "Epoch: 4813 mean train loss:  1.70518215e-02, mean val. rec. loss:  9.53953728e-03\n",
      "Epoch: 4814 mean train loss:  1.70497750e-02, mean val. rec. loss:  9.53781364e-03\n",
      "Epoch: 4815 mean train loss:  1.70477284e-02, mean val. rec. loss:  9.53609680e-03\n",
      "Epoch: 4816 mean train loss:  1.70456689e-02, mean val. rec. loss:  9.53438222e-03\n",
      "Epoch: 4817 mean train loss:  1.70436093e-02, mean val. rec. loss:  9.53268693e-03\n",
      "Epoch: 4818 mean train loss:  1.70415590e-02, mean val. rec. loss:  9.53097689e-03\n",
      "Epoch: 4819 mean train loss:  1.70395087e-02, mean val. rec. loss:  9.52927026e-03\n",
      "Epoch: 4820 mean train loss:  1.70374566e-02, mean val. rec. loss:  9.52755682e-03\n",
      "Epoch: 4821 mean train loss:  1.70354045e-02, mean val. rec. loss:  9.52583544e-03\n",
      "Epoch: 4822 mean train loss:  1.70333523e-02, mean val. rec. loss:  9.52411520e-03\n",
      "Epoch: 4823 mean train loss:  1.70312983e-02, mean val. rec. loss:  9.52238929e-03\n",
      "Epoch: 4824 mean train loss:  1.70292443e-02, mean val. rec. loss:  9.52067018e-03\n",
      "Epoch: 4825 mean train loss:  1.70271885e-02, mean val. rec. loss:  9.51895107e-03\n",
      "Epoch: 4826 mean train loss:  1.70251326e-02, mean val. rec. loss:  9.51723310e-03\n",
      "Epoch: 4827 mean train loss:  1.70230768e-02, mean val. rec. loss:  9.51552646e-03\n",
      "Epoch: 4828 mean train loss:  1.70210209e-02, mean val. rec. loss:  9.51383003e-03\n",
      "Epoch: 4829 mean train loss:  1.70189762e-02, mean val. rec. loss:  9.51211830e-03\n",
      "Epoch: 4830 mean train loss:  1.70169260e-02, mean val. rec. loss:  9.51040883e-03\n",
      "Epoch: 4831 mean train loss:  1.70148682e-02, mean val. rec. loss:  9.50867781e-03\n",
      "Epoch: 4832 mean train loss:  1.70128124e-02, mean val. rec. loss:  9.50695020e-03\n",
      "Epoch: 4833 mean train loss:  1.70107491e-02, mean val. rec. loss:  9.50522032e-03\n",
      "Epoch: 4834 mean train loss:  1.70087007e-02, mean val. rec. loss:  9.50350291e-03\n",
      "Epoch: 4835 mean train loss:  1.70066485e-02, mean val. rec. loss:  9.50177529e-03\n",
      "Epoch: 4836 mean train loss:  1.70045889e-02, mean val. rec. loss:  9.50007036e-03\n",
      "Epoch: 4837 mean train loss:  1.70025256e-02, mean val. rec. loss:  9.49834898e-03\n",
      "Epoch: 4838 mean train loss:  1.70004810e-02, mean val. rec. loss:  9.49664859e-03\n",
      "Epoch: 4839 mean train loss:  1.69984176e-02, mean val. rec. loss:  9.49493288e-03\n",
      "Epoch: 4840 mean train loss:  1.69963674e-02, mean val. rec. loss:  9.49321547e-03\n",
      "Epoch: 4841 mean train loss:  1.69943097e-02, mean val. rec. loss:  9.49149636e-03\n",
      "Epoch: 4842 mean train loss:  1.69922463e-02, mean val. rec. loss:  9.48976251e-03\n",
      "Epoch: 4843 mean train loss:  1.69901998e-02, mean val. rec. loss:  9.48803547e-03\n",
      "Epoch: 4844 mean train loss:  1.69881346e-02, mean val. rec. loss:  9.48629765e-03\n",
      "Epoch: 4845 mean train loss:  1.69860862e-02, mean val. rec. loss:  9.48457060e-03\n",
      "Epoch: 4846 mean train loss:  1.69840192e-02, mean val. rec. loss:  9.48284752e-03\n",
      "Epoch: 4847 mean train loss:  1.69819689e-02, mean val. rec. loss:  9.48114769e-03\n",
      "Epoch: 4848 mean train loss:  1.69799205e-02, mean val. rec. loss:  9.47943992e-03\n",
      "Epoch: 4849 mean train loss:  1.69778479e-02, mean val. rec. loss:  9.47771741e-03\n",
      "Epoch: 4850 mean train loss:  1.69757995e-02, mean val. rec. loss:  9.47599490e-03\n",
      "Epoch: 4851 mean train loss:  1.69737380e-02, mean val. rec. loss:  9.47426616e-03\n",
      "Epoch: 4852 mean train loss:  1.69716803e-02, mean val. rec. loss:  9.47253514e-03\n",
      "Epoch: 4853 mean train loss:  1.69696282e-02, mean val. rec. loss:  9.47080299e-03\n",
      "Epoch: 4854 mean train loss:  1.69675723e-02, mean val. rec. loss:  9.46908445e-03\n",
      "Epoch: 4855 mean train loss:  1.69655016e-02, mean val. rec. loss:  9.46736081e-03\n",
      "Epoch: 4856 mean train loss:  1.69634494e-02, mean val. rec. loss:  9.46564510e-03\n",
      "Epoch: 4857 mean train loss:  1.69613954e-02, mean val. rec. loss:  9.46392202e-03\n",
      "Epoch: 4858 mean train loss:  1.69593377e-02, mean val. rec. loss:  9.46219044e-03\n",
      "Epoch: 4859 mean train loss:  1.69572819e-02, mean val. rec. loss:  9.46046283e-03\n",
      "Epoch: 4860 mean train loss:  1.69552148e-02, mean val. rec. loss:  9.45873124e-03\n",
      "Epoch: 4861 mean train loss:  1.69531478e-02, mean val. rec. loss:  9.45700533e-03\n",
      "Epoch: 4862 mean train loss:  1.69510919e-02, mean val. rec. loss:  9.45528622e-03\n",
      "Epoch: 4863 mean train loss:  1.69490342e-02, mean val. rec. loss:  9.45356938e-03\n",
      "Epoch: 4864 mean train loss:  1.69469765e-02, mean val. rec. loss:  9.45184290e-03\n",
      "Epoch: 4865 mean train loss:  1.69449169e-02, mean val. rec. loss:  9.45011756e-03\n",
      "Epoch: 4866 mean train loss:  1.69428573e-02, mean val. rec. loss:  9.44838144e-03\n",
      "Epoch: 4867 mean train loss:  1.69407959e-02, mean val. rec. loss:  9.44664532e-03\n",
      "Epoch: 4868 mean train loss:  1.69387363e-02, mean val. rec. loss:  9.44490694e-03\n",
      "Epoch: 4869 mean train loss:  1.69366749e-02, mean val. rec. loss:  9.44317138e-03\n",
      "Epoch: 4870 mean train loss:  1.69346115e-02, mean val. rec. loss:  9.44146758e-03\n",
      "Epoch: 4871 mean train loss:  1.69325557e-02, mean val. rec. loss:  9.43975641e-03\n",
      "Epoch: 4872 mean train loss:  1.69305017e-02, mean val. rec. loss:  9.43804354e-03\n",
      "Epoch: 4873 mean train loss:  1.69284402e-02, mean val. rec. loss:  9.43633124e-03\n",
      "Epoch: 4874 mean train loss:  1.69263769e-02, mean val. rec. loss:  9.43459682e-03\n",
      "Epoch: 4875 mean train loss:  1.69243136e-02, mean val. rec. loss:  9.43283349e-03\n",
      "Epoch: 4876 mean train loss:  1.69222466e-02, mean val. rec. loss:  9.43108376e-03\n",
      "Epoch: 4877 mean train loss:  1.69201982e-02, mean val. rec. loss:  9.42934424e-03\n",
      "Epoch: 4878 mean train loss:  1.69181330e-02, mean val. rec. loss:  9.42762400e-03\n",
      "Epoch: 4879 mean train loss:  1.69160660e-02, mean val. rec. loss:  9.42591113e-03\n",
      "Epoch: 4880 mean train loss:  1.69140064e-02, mean val. rec. loss:  9.42419315e-03\n",
      "Epoch: 4881 mean train loss:  1.69119487e-02, mean val. rec. loss:  9.42247971e-03\n",
      "Epoch: 4882 mean train loss:  1.69098779e-02, mean val. rec. loss:  9.42074586e-03\n",
      "Epoch: 4883 mean train loss:  1.69078277e-02, mean val. rec. loss:  9.41901031e-03\n",
      "Epoch: 4884 mean train loss:  1.69057588e-02, mean val. rec. loss:  9.41727193e-03\n",
      "Epoch: 4885 mean train loss:  1.69036899e-02, mean val. rec. loss:  9.41552787e-03\n",
      "Epoch: 4886 mean train loss:  1.69016396e-02, mean val. rec. loss:  9.41378948e-03\n",
      "Epoch: 4887 mean train loss:  1.68995651e-02, mean val. rec. loss:  9.41207094e-03\n",
      "Epoch: 4888 mean train loss:  1.68975130e-02, mean val. rec. loss:  9.41033879e-03\n",
      "Epoch: 4889 mean train loss:  1.68954404e-02, mean val. rec. loss:  9.40861742e-03\n",
      "Epoch: 4890 mean train loss:  1.68933864e-02, mean val. rec. loss:  9.40688357e-03\n",
      "Epoch: 4891 mean train loss:  1.68913156e-02, mean val. rec. loss:  9.40515765e-03\n",
      "Epoch: 4892 mean train loss:  1.68892560e-02, mean val. rec. loss:  9.40344365e-03\n",
      "Epoch: 4893 mean train loss:  1.68872002e-02, mean val. rec. loss:  9.40170016e-03\n",
      "Epoch: 4894 mean train loss:  1.68851257e-02, mean val. rec. loss:  9.39995951e-03\n",
      "Epoch: 4895 mean train loss:  1.68830698e-02, mean val. rec. loss:  9.39822906e-03\n",
      "Epoch: 4896 mean train loss:  1.68810009e-02, mean val. rec. loss:  9.39648727e-03\n",
      "Epoch: 4897 mean train loss:  1.68789395e-02, mean val. rec. loss:  9.39474208e-03\n",
      "Epoch: 4898 mean train loss:  1.68768781e-02, mean val. rec. loss:  9.39301276e-03\n",
      "Epoch: 4899 mean train loss:  1.68748185e-02, mean val. rec. loss:  9.39128742e-03\n",
      "Epoch: 4900 mean train loss:  1.68727403e-02, mean val. rec. loss:  9.38957511e-03\n",
      "Epoch: 4901 mean train loss:  1.68706825e-02, mean val. rec. loss:  9.38784183e-03\n",
      "Epoch: 4902 mean train loss:  1.68686211e-02, mean val. rec. loss:  9.38609778e-03\n",
      "Epoch: 4903 mean train loss:  1.68665634e-02, mean val. rec. loss:  9.38433898e-03\n",
      "Epoch: 4904 mean train loss:  1.68644889e-02, mean val. rec. loss:  9.38260513e-03\n",
      "Epoch: 4905 mean train loss:  1.68624181e-02, mean val. rec. loss:  9.38086958e-03\n",
      "Epoch: 4906 mean train loss:  1.68603586e-02, mean val. rec. loss:  9.37912325e-03\n",
      "Epoch: 4907 mean train loss:  1.68582953e-02, mean val. rec. loss:  9.37740358e-03\n",
      "Epoch: 4908 mean train loss:  1.68562320e-02, mean val. rec. loss:  9.37567426e-03\n",
      "Epoch: 4909 mean train loss:  1.68541686e-02, mean val. rec. loss:  9.37395402e-03\n",
      "Epoch: 4910 mean train loss:  1.68521035e-02, mean val. rec. loss:  9.37222244e-03\n",
      "Epoch: 4911 mean train loss:  1.68500383e-02, mean val. rec. loss:  9.37048745e-03\n",
      "Epoch: 4912 mean train loss:  1.68479731e-02, mean val. rec. loss:  9.36874850e-03\n",
      "Epoch: 4913 mean train loss:  1.68459080e-02, mean val. rec. loss:  9.36701522e-03\n",
      "Epoch: 4914 mean train loss:  1.68438428e-02, mean val. rec. loss:  9.36526946e-03\n",
      "Epoch: 4915 mean train loss:  1.68417758e-02, mean val. rec. loss:  9.36353278e-03\n",
      "Epoch: 4916 mean train loss:  1.68397087e-02, mean val. rec. loss:  9.36179722e-03\n",
      "Epoch: 4917 mean train loss:  1.68376398e-02, mean val. rec. loss:  9.36005997e-03\n",
      "Epoch: 4918 mean train loss:  1.68355728e-02, mean val. rec. loss:  9.35832896e-03\n",
      "Epoch: 4919 mean train loss:  1.68335058e-02, mean val. rec. loss:  9.35659794e-03\n",
      "Epoch: 4920 mean train loss:  1.68314425e-02, mean val. rec. loss:  9.35485956e-03\n",
      "Epoch: 4921 mean train loss:  1.68293847e-02, mean val. rec. loss:  9.35313648e-03\n",
      "Epoch: 4922 mean train loss:  1.68273140e-02, mean val. rec. loss:  9.35140490e-03\n",
      "Epoch: 4923 mean train loss:  1.68252451e-02, mean val. rec. loss:  9.34966934e-03\n",
      "Epoch: 4924 mean train loss:  1.68231706e-02, mean val. rec. loss:  9.34792245e-03\n",
      "Epoch: 4925 mean train loss:  1.68211185e-02, mean val. rec. loss:  9.34617556e-03\n",
      "Epoch: 4926 mean train loss:  1.68190496e-02, mean val. rec. loss:  9.34442187e-03\n",
      "Epoch: 4927 mean train loss:  1.68169770e-02, mean val. rec. loss:  9.34267951e-03\n",
      "Epoch: 4928 mean train loss:  1.68149118e-02, mean val. rec. loss:  9.34094339e-03\n",
      "Epoch: 4929 mean train loss:  1.68128504e-02, mean val. rec. loss:  9.33921748e-03\n",
      "Epoch: 4930 mean train loss:  1.68107722e-02, mean val. rec. loss:  9.33750178e-03\n",
      "Epoch: 4931 mean train loss:  1.68087182e-02, mean val. rec. loss:  9.33577019e-03\n",
      "Epoch: 4932 mean train loss:  1.68066437e-02, mean val. rec. loss:  9.33404258e-03\n",
      "Epoch: 4933 mean train loss:  1.68045711e-02, mean val. rec. loss:  9.33230419e-03\n",
      "Epoch: 4934 mean train loss:  1.68025152e-02, mean val. rec. loss:  9.33055050e-03\n",
      "Epoch: 4935 mean train loss:  1.68004370e-02, mean val. rec. loss:  9.32880531e-03\n",
      "Epoch: 4936 mean train loss:  1.67983774e-02, mean val. rec. loss:  9.32705955e-03\n",
      "Epoch: 4937 mean train loss:  1.67963029e-02, mean val. rec. loss:  9.32531890e-03\n",
      "Epoch: 4938 mean train loss:  1.67942434e-02, mean val. rec. loss:  9.32359412e-03\n",
      "Epoch: 4939 mean train loss:  1.67921651e-02, mean val. rec. loss:  9.32187898e-03\n",
      "Epoch: 4940 mean train loss:  1.67901074e-02, mean val. rec. loss:  9.32014626e-03\n",
      "Epoch: 4941 mean train loss:  1.67880274e-02, mean val. rec. loss:  9.31840901e-03\n",
      "Epoch: 4942 mean train loss:  1.67859678e-02, mean val. rec. loss:  9.31666212e-03\n",
      "Epoch: 4943 mean train loss:  1.67839063e-02, mean val. rec. loss:  9.31490105e-03\n",
      "Epoch: 4944 mean train loss:  1.67818263e-02, mean val. rec. loss:  9.31316494e-03\n",
      "Epoch: 4945 mean train loss:  1.67797667e-02, mean val. rec. loss:  9.31142258e-03\n",
      "Epoch: 4946 mean train loss:  1.67776997e-02, mean val. rec. loss:  9.30970177e-03\n",
      "Epoch: 4947 mean train loss:  1.67756252e-02, mean val. rec. loss:  9.30796565e-03\n",
      "Epoch: 4948 mean train loss:  1.67735619e-02, mean val. rec. loss:  9.30621820e-03\n",
      "Epoch: 4949 mean train loss:  1.67714967e-02, mean val. rec. loss:  9.30447074e-03\n",
      "Epoch: 4950 mean train loss:  1.67694166e-02, mean val. rec. loss:  9.30271931e-03\n",
      "Epoch: 4951 mean train loss:  1.67673496e-02, mean val. rec. loss:  9.30097526e-03\n",
      "Epoch: 4952 mean train loss:  1.67652863e-02, mean val. rec. loss:  9.29924197e-03\n",
      "Epoch: 4953 mean train loss:  1.67632230e-02, mean val. rec. loss:  9.29751152e-03\n",
      "Epoch: 4954 mean train loss:  1.67611522e-02, mean val. rec. loss:  9.29578845e-03\n",
      "Epoch: 4955 mean train loss:  1.67590722e-02, mean val. rec. loss:  9.29404723e-03\n",
      "Epoch: 4956 mean train loss:  1.67570070e-02, mean val. rec. loss:  9.29228786e-03\n",
      "Epoch: 4957 mean train loss:  1.67549400e-02, mean val. rec. loss:  9.29054551e-03\n",
      "Epoch: 4958 mean train loss:  1.67528729e-02, mean val. rec. loss:  9.28879918e-03\n",
      "Epoch: 4959 mean train loss:  1.67508078e-02, mean val. rec. loss:  9.28706817e-03\n",
      "Epoch: 4960 mean train loss:  1.67487389e-02, mean val. rec. loss:  9.28534282e-03\n",
      "Epoch: 4961 mean train loss:  1.67466681e-02, mean val. rec. loss:  9.28360160e-03\n",
      "Epoch: 4962 mean train loss:  1.67446011e-02, mean val. rec. loss:  9.28186832e-03\n",
      "Epoch: 4963 mean train loss:  1.67425303e-02, mean val. rec. loss:  9.28013333e-03\n",
      "Epoch: 4964 mean train loss:  1.67404633e-02, mean val. rec. loss:  9.27839608e-03\n",
      "Epoch: 4965 mean train loss:  1.67383963e-02, mean val. rec. loss:  9.27665996e-03\n",
      "Epoch: 4966 mean train loss:  1.67363255e-02, mean val. rec. loss:  9.27490797e-03\n",
      "Epoch: 4967 mean train loss:  1.67342585e-02, mean val. rec. loss:  9.27315994e-03\n",
      "Epoch: 4968 mean train loss:  1.67321840e-02, mean val. rec. loss:  9.27140965e-03\n",
      "Epoch: 4969 mean train loss:  1.67301132e-02, mean val. rec. loss:  9.26966503e-03\n",
      "Epoch: 4970 mean train loss:  1.67280425e-02, mean val. rec. loss:  9.26794025e-03\n",
      "Epoch: 4971 mean train loss:  1.67259699e-02, mean val. rec. loss:  9.26620300e-03\n",
      "Epoch: 4972 mean train loss:  1.67238972e-02, mean val. rec. loss:  9.26447595e-03\n",
      "Epoch: 4973 mean train loss:  1.67218377e-02, mean val. rec. loss:  9.26274380e-03\n",
      "Epoch: 4974 mean train loss:  1.67197744e-02, mean val. rec. loss:  9.26100315e-03\n",
      "Epoch: 4975 mean train loss:  1.67176999e-02, mean val. rec. loss:  9.25926363e-03\n",
      "Epoch: 4976 mean train loss:  1.67156254e-02, mean val. rec. loss:  9.25752014e-03\n",
      "Epoch: 4977 mean train loss:  1.67135509e-02, mean val. rec. loss:  9.25577438e-03\n",
      "Epoch: 4978 mean train loss:  1.67114951e-02, mean val. rec. loss:  9.25403259e-03\n",
      "Epoch: 4979 mean train loss:  1.67094206e-02, mean val. rec. loss:  9.25229194e-03\n",
      "Epoch: 4980 mean train loss:  1.67073498e-02, mean val. rec. loss:  9.25055015e-03\n",
      "Epoch: 4981 mean train loss:  1.67052716e-02, mean val. rec. loss:  9.24881403e-03\n",
      "Epoch: 4982 mean train loss:  1.67032139e-02, mean val. rec. loss:  9.24708358e-03\n",
      "Epoch: 4983 mean train loss:  1.67011376e-02, mean val. rec. loss:  9.24535087e-03\n",
      "Epoch: 4984 mean train loss:  1.66990687e-02, mean val. rec. loss:  9.24360795e-03\n",
      "Epoch: 4985 mean train loss:  1.66970035e-02, mean val. rec. loss:  9.24187240e-03\n",
      "Epoch: 4986 mean train loss:  1.66949271e-02, mean val. rec. loss:  9.24011416e-03\n",
      "Epoch: 4987 mean train loss:  1.66928657e-02, mean val. rec. loss:  9.23837975e-03\n",
      "Epoch: 4988 mean train loss:  1.66907912e-02, mean val. rec. loss:  9.23663853e-03\n",
      "Epoch: 4989 mean train loss:  1.66887205e-02, mean val. rec. loss:  9.23490921e-03\n",
      "Epoch: 4990 mean train loss:  1.66866553e-02, mean val. rec. loss:  9.23317026e-03\n",
      "Epoch: 4991 mean train loss:  1.66845845e-02, mean val. rec. loss:  9.23143131e-03\n",
      "Epoch: 4992 mean train loss:  1.66825156e-02, mean val. rec. loss:  9.22968158e-03\n",
      "Epoch: 4993 mean train loss:  1.66804412e-02, mean val. rec. loss:  9.22794319e-03\n",
      "Epoch: 4994 mean train loss:  1.66783741e-02, mean val. rec. loss:  9.22619630e-03\n",
      "Epoch: 4995 mean train loss:  1.66763052e-02, mean val. rec. loss:  9.22447719e-03\n",
      "Epoch: 4996 mean train loss:  1.66742345e-02, mean val. rec. loss:  9.22273257e-03\n",
      "Epoch: 4997 mean train loss:  1.66721712e-02, mean val. rec. loss:  9.22098398e-03\n",
      "Epoch: 4998 mean train loss:  1.66700911e-02, mean val. rec. loss:  9.21923425e-03\n",
      "Epoch: 4999 mean train loss:  1.66680278e-02, mean val. rec. loss:  9.21750721e-03\n",
      "Epoch: 5000 mean train loss:  1.66659533e-02, mean val. rec. loss:  9.21576825e-03\n",
      "Epoch: 5001 mean train loss:  1.66638826e-02, mean val. rec. loss:  9.21402703e-03\n",
      "Epoch: 5002 mean train loss:  1.66618193e-02, mean val. rec. loss:  9.21229091e-03\n",
      "Epoch: 5003 mean train loss:  1.66597373e-02, mean val. rec. loss:  9.21056047e-03\n",
      "Epoch: 5004 mean train loss:  1.66576740e-02, mean val. rec. loss:  9.20882095e-03\n",
      "Epoch: 5005 mean train loss:  1.66556089e-02, mean val. rec. loss:  9.20708086e-03\n",
      "Epoch: 5006 mean train loss:  1.66535437e-02, mean val. rec. loss:  9.20533057e-03\n",
      "Epoch: 5007 mean train loss:  1.66514599e-02, mean val. rec. loss:  9.20358311e-03\n",
      "Epoch: 5008 mean train loss:  1.66493947e-02, mean val. rec. loss:  9.20184812e-03\n",
      "Epoch: 5009 mean train loss:  1.66473314e-02, mean val. rec. loss:  9.20011087e-03\n",
      "Epoch: 5010 mean train loss:  1.66452644e-02, mean val. rec. loss:  9.19838269e-03\n",
      "Epoch: 5011 mean train loss:  1.66431825e-02, mean val. rec. loss:  9.19664147e-03\n",
      "Epoch: 5012 mean train loss:  1.66411154e-02, mean val. rec. loss:  9.19490422e-03\n",
      "Epoch: 5013 mean train loss:  1.66390484e-02, mean val. rec. loss:  9.19316527e-03\n",
      "Epoch: 5014 mean train loss:  1.66369795e-02, mean val. rec. loss:  9.19142631e-03\n",
      "Epoch: 5015 mean train loss:  1.66349143e-02, mean val. rec. loss:  9.18969530e-03\n",
      "Epoch: 5016 mean train loss:  1.66328436e-02, mean val. rec. loss:  9.18795294e-03\n",
      "Epoch: 5017 mean train loss:  1.66307784e-02, mean val. rec. loss:  9.18621682e-03\n",
      "Epoch: 5018 mean train loss:  1.66287002e-02, mean val. rec. loss:  9.18448467e-03\n",
      "Epoch: 5019 mean train loss:  1.66266239e-02, mean val. rec. loss:  9.18275082e-03\n",
      "Epoch: 5020 mean train loss:  1.66245568e-02, mean val. rec. loss:  9.18101471e-03\n",
      "Epoch: 5021 mean train loss:  1.66224898e-02, mean val. rec. loss:  9.17927122e-03\n",
      "Epoch: 5022 mean train loss:  1.66204190e-02, mean val. rec. loss:  9.17753340e-03\n",
      "Epoch: 5023 mean train loss:  1.66183520e-02, mean val. rec. loss:  9.17580182e-03\n",
      "Epoch: 5024 mean train loss:  1.66162813e-02, mean val. rec. loss:  9.17404982e-03\n",
      "Epoch: 5025 mean train loss:  1.66142124e-02, mean val. rec. loss:  9.17231370e-03\n",
      "Epoch: 5026 mean train loss:  1.66121416e-02, mean val. rec. loss:  9.17058382e-03\n",
      "Epoch: 5027 mean train loss:  1.66100727e-02, mean val. rec. loss:  9.16885678e-03\n",
      "Epoch: 5028 mean train loss:  1.66080113e-02, mean val. rec. loss:  9.16711839e-03\n",
      "Epoch: 5029 mean train loss:  1.66059498e-02, mean val. rec. loss:  9.16538851e-03\n",
      "Epoch: 5030 mean train loss:  1.66038791e-02, mean val. rec. loss:  9.16366090e-03\n",
      "Epoch: 5031 mean train loss:  1.66018083e-02, mean val. rec. loss:  9.16192931e-03\n",
      "Epoch: 5032 mean train loss:  1.65997376e-02, mean val. rec. loss:  9.16018129e-03\n",
      "Epoch: 5033 mean train loss:  1.65976631e-02, mean val. rec. loss:  9.15846048e-03\n",
      "Epoch: 5034 mean train loss:  1.65955942e-02, mean val. rec. loss:  9.15670905e-03\n",
      "Epoch: 5035 mean train loss:  1.65935253e-02, mean val. rec. loss:  9.15497010e-03\n",
      "Epoch: 5036 mean train loss:  1.65914657e-02, mean val. rec. loss:  9.15321073e-03\n",
      "Epoch: 5037 mean train loss:  1.65893949e-02, mean val. rec. loss:  9.15148709e-03\n",
      "Epoch: 5038 mean train loss:  1.65873223e-02, mean val. rec. loss:  9.14976628e-03\n",
      "Epoch: 5039 mean train loss:  1.65852497e-02, mean val. rec. loss:  9.14804093e-03\n",
      "Epoch: 5040 mean train loss:  1.65831957e-02, mean val. rec. loss:  9.14630765e-03\n",
      "Epoch: 5041 mean train loss:  1.65811231e-02, mean val. rec. loss:  9.14458401e-03\n",
      "Epoch: 5042 mean train loss:  1.65790486e-02, mean val. rec. loss:  9.14283712e-03\n",
      "Epoch: 5043 mean train loss:  1.65769779e-02, mean val. rec. loss:  9.14110440e-03\n",
      "Epoch: 5044 mean train loss:  1.65749257e-02, mean val. rec. loss:  9.13936715e-03\n",
      "Epoch: 5045 mean train loss:  1.65728475e-02, mean val. rec. loss:  9.13762593e-03\n",
      "Epoch: 5046 mean train loss:  1.65707730e-02, mean val. rec. loss:  9.13588300e-03\n",
      "Epoch: 5047 mean train loss:  1.65687172e-02, mean val. rec. loss:  9.13415029e-03\n",
      "Epoch: 5048 mean train loss:  1.65666427e-02, mean val. rec. loss:  9.13242778e-03\n",
      "Epoch: 5049 mean train loss:  1.65645757e-02, mean val. rec. loss:  9.13069903e-03\n",
      "Epoch: 5050 mean train loss:  1.65625124e-02, mean val. rec. loss:  9.12896121e-03\n",
      "Epoch: 5051 mean train loss:  1.65604379e-02, mean val. rec. loss:  9.12724664e-03\n",
      "Epoch: 5052 mean train loss:  1.65583820e-02, mean val. rec. loss:  9.12550655e-03\n",
      "Epoch: 5053 mean train loss:  1.65563075e-02, mean val. rec. loss:  9.12376760e-03\n",
      "Epoch: 5054 mean train loss:  1.65542480e-02, mean val. rec. loss:  9.12202978e-03\n",
      "Epoch: 5055 mean train loss:  1.65521735e-02, mean val. rec. loss:  9.12029026e-03\n",
      "Epoch: 5056 mean train loss:  1.65501158e-02, mean val. rec. loss:  9.11855641e-03\n",
      "Epoch: 5057 mean train loss:  1.65480394e-02, mean val. rec. loss:  9.11682199e-03\n",
      "Epoch: 5058 mean train loss:  1.65459817e-02, mean val. rec. loss:  9.11509098e-03\n",
      "Epoch: 5059 mean train loss:  1.65439054e-02, mean val. rec. loss:  9.11337640e-03\n",
      "Epoch: 5060 mean train loss:  1.65418458e-02, mean val. rec. loss:  9.11165900e-03\n",
      "Epoch: 5061 mean train loss:  1.65397713e-02, mean val. rec. loss:  9.10992912e-03\n",
      "Epoch: 5062 mean train loss:  1.65377136e-02, mean val. rec. loss:  9.10819867e-03\n",
      "Epoch: 5063 mean train loss:  1.65356335e-02, mean val. rec. loss:  9.10646085e-03\n",
      "Epoch: 5064 mean train loss:  1.65335739e-02, mean val. rec. loss:  9.10471396e-03\n",
      "Epoch: 5065 mean train loss:  1.65315143e-02, mean val. rec. loss:  9.10297557e-03\n",
      "Epoch: 5066 mean train loss:  1.65294399e-02, mean val. rec. loss:  9.10124059e-03\n",
      "Epoch: 5067 mean train loss:  1.65273803e-02, mean val. rec. loss:  9.09952601e-03\n",
      "Epoch: 5068 mean train loss:  1.65253132e-02, mean val. rec. loss:  9.09781258e-03\n",
      "Epoch: 5069 mean train loss:  1.65232425e-02, mean val. rec. loss:  9.09609687e-03\n",
      "Epoch: 5070 mean train loss:  1.65211866e-02, mean val. rec. loss:  9.09437436e-03\n",
      "Epoch: 5071 mean train loss:  1.65191140e-02, mean val. rec. loss:  9.09263881e-03\n",
      "Epoch: 5072 mean train loss:  1.65170451e-02, mean val. rec. loss:  9.09088851e-03\n",
      "Epoch: 5073 mean train loss:  1.65149837e-02, mean val. rec. loss:  9.08915807e-03\n",
      "Epoch: 5074 mean train loss:  1.65129222e-02, mean val. rec. loss:  9.08741741e-03\n",
      "Epoch: 5075 mean train loss:  1.65108552e-02, mean val. rec. loss:  9.08570171e-03\n",
      "Epoch: 5076 mean train loss:  1.65087844e-02, mean val. rec. loss:  9.08397353e-03\n",
      "Epoch: 5077 mean train loss:  1.65067230e-02, mean val. rec. loss:  9.08225101e-03\n",
      "Epoch: 5078 mean train loss:  1.65046634e-02, mean val. rec. loss:  9.08052964e-03\n",
      "Epoch: 5079 mean train loss:  1.65025927e-02, mean val. rec. loss:  9.07881166e-03\n",
      "Epoch: 5080 mean train loss:  1.65005238e-02, mean val. rec. loss:  9.07707214e-03\n",
      "Epoch: 5081 mean train loss:  1.64984642e-02, mean val. rec. loss:  9.07535757e-03\n",
      "Epoch: 5082 mean train loss:  1.64964009e-02, mean val. rec. loss:  9.07362826e-03\n",
      "Epoch: 5083 mean train loss:  1.64943413e-02, mean val. rec. loss:  9.07191028e-03\n",
      "Epoch: 5084 mean train loss:  1.64922798e-02, mean val. rec. loss:  9.07019458e-03\n",
      "Epoch: 5085 mean train loss:  1.64902184e-02, mean val. rec. loss:  9.06846866e-03\n",
      "Epoch: 5086 mean train loss:  1.64881495e-02, mean val. rec. loss:  9.06673935e-03\n",
      "Epoch: 5087 mean train loss:  1.64860769e-02, mean val. rec. loss:  9.06500833e-03\n",
      "Epoch: 5088 mean train loss:  1.64840173e-02, mean val. rec. loss:  9.06327505e-03\n",
      "Epoch: 5089 mean train loss:  1.64819540e-02, mean val. rec. loss:  9.06155708e-03\n",
      "Epoch: 5090 mean train loss:  1.64798907e-02, mean val. rec. loss:  9.05983286e-03\n",
      "Epoch: 5091 mean train loss:  1.64778293e-02, mean val. rec. loss:  9.05811376e-03\n",
      "Epoch: 5092 mean train loss:  1.64757678e-02, mean val. rec. loss:  9.05638898e-03\n",
      "Epoch: 5093 mean train loss:  1.64737026e-02, mean val. rec. loss:  9.05466193e-03\n",
      "Epoch: 5094 mean train loss:  1.64716412e-02, mean val. rec. loss:  9.05293545e-03\n",
      "Epoch: 5095 mean train loss:  1.64695797e-02, mean val. rec. loss:  9.05121748e-03\n",
      "Epoch: 5096 mean train loss:  1.64675183e-02, mean val. rec. loss:  9.04949156e-03\n",
      "Epoch: 5097 mean train loss:  1.64654550e-02, mean val. rec. loss:  9.04776735e-03\n",
      "Epoch: 5098 mean train loss:  1.64633917e-02, mean val. rec. loss:  9.04604201e-03\n",
      "Epoch: 5099 mean train loss:  1.64613321e-02, mean val. rec. loss:  9.04433537e-03\n",
      "Epoch: 5100 mean train loss:  1.64592707e-02, mean val. rec. loss:  9.04260776e-03\n",
      "Epoch: 5101 mean train loss:  1.64572055e-02, mean val. rec. loss:  9.04088638e-03\n",
      "Epoch: 5102 mean train loss:  1.64551440e-02, mean val. rec. loss:  9.03916614e-03\n",
      "Epoch: 5103 mean train loss:  1.64530919e-02, mean val. rec. loss:  9.03745214e-03\n",
      "Epoch: 5104 mean train loss:  1.64510361e-02, mean val. rec. loss:  9.03573700e-03\n",
      "Epoch: 5105 mean train loss:  1.64489709e-02, mean val. rec. loss:  9.03403206e-03\n",
      "Epoch: 5106 mean train loss:  1.64469076e-02, mean val. rec. loss:  9.03230331e-03\n",
      "Epoch: 5107 mean train loss:  1.64448480e-02, mean val. rec. loss:  9.03058251e-03\n",
      "Epoch: 5108 mean train loss:  1.64427866e-02, mean val. rec. loss:  9.02886170e-03\n",
      "Epoch: 5109 mean train loss:  1.64407214e-02, mean val. rec. loss:  9.02713522e-03\n",
      "Epoch: 5110 mean train loss:  1.64386581e-02, mean val. rec. loss:  9.02541271e-03\n",
      "Epoch: 5111 mean train loss:  1.64366134e-02, mean val. rec. loss:  9.02370267e-03\n",
      "Epoch: 5112 mean train loss:  1.64345501e-02, mean val. rec. loss:  9.02198923e-03\n",
      "Epoch: 5113 mean train loss:  1.64324849e-02, mean val. rec. loss:  9.02027806e-03\n",
      "Epoch: 5114 mean train loss:  1.64304235e-02, mean val. rec. loss:  9.01856065e-03\n",
      "Epoch: 5115 mean train loss:  1.64283769e-02, mean val. rec. loss:  9.01684211e-03\n",
      "Epoch: 5116 mean train loss:  1.64263136e-02, mean val. rec. loss:  9.01512697e-03\n",
      "Epoch: 5117 mean train loss:  1.64242522e-02, mean val. rec. loss:  9.01340503e-03\n",
      "Epoch: 5118 mean train loss:  1.64221889e-02, mean val. rec. loss:  9.01168592e-03\n",
      "Epoch: 5119 mean train loss:  1.64201442e-02, mean val. rec. loss:  9.00998099e-03\n",
      "Epoch: 5120 mean train loss:  1.64180809e-02, mean val. rec. loss:  9.00828399e-03\n",
      "Epoch: 5121 mean train loss:  1.64160176e-02, mean val. rec. loss:  9.00656828e-03\n",
      "Epoch: 5122 mean train loss:  1.64139636e-02, mean val. rec. loss:  9.00485541e-03\n",
      "Epoch: 5123 mean train loss:  1.64119077e-02, mean val. rec. loss:  9.00313347e-03\n",
      "Epoch: 5124 mean train loss:  1.64098425e-02, mean val. rec. loss:  9.00142910e-03\n",
      "Epoch: 5125 mean train loss:  1.64077979e-02, mean val. rec. loss:  8.99971169e-03\n",
      "Epoch: 5126 mean train loss:  1.64057345e-02, mean val. rec. loss:  8.99799485e-03\n",
      "Epoch: 5127 mean train loss:  1.64036694e-02, mean val. rec. loss:  8.99627348e-03\n",
      "Epoch: 5128 mean train loss:  1.64016247e-02, mean val. rec. loss:  8.99455947e-03\n",
      "Epoch: 5129 mean train loss:  1.63995614e-02, mean val. rec. loss:  8.99286588e-03\n",
      "Epoch: 5130 mean train loss:  1.63975055e-02, mean val. rec. loss:  8.99116434e-03\n",
      "Epoch: 5131 mean train loss:  1.63954553e-02, mean val. rec. loss:  8.98945658e-03\n",
      "Epoch: 5132 mean train loss:  1.63933919e-02, mean val. rec. loss:  8.98774767e-03\n",
      "Epoch: 5133 mean train loss:  1.63913473e-02, mean val. rec. loss:  8.98603083e-03\n",
      "Epoch: 5134 mean train loss:  1.63892821e-02, mean val. rec. loss:  8.98431513e-03\n",
      "Epoch: 5135 mean train loss:  1.63872355e-02, mean val. rec. loss:  8.98261076e-03\n",
      "Epoch: 5136 mean train loss:  1.63851722e-02, mean val. rec. loss:  8.98088768e-03\n",
      "Epoch: 5137 mean train loss:  1.63831238e-02, mean val. rec. loss:  8.97918445e-03\n",
      "Epoch: 5138 mean train loss:  1.63810624e-02, mean val. rec. loss:  8.97750446e-03\n",
      "Epoch: 5139 mean train loss:  1.63790177e-02, mean val. rec. loss:  8.97579443e-03\n",
      "Epoch: 5140 mean train loss:  1.63769563e-02, mean val. rec. loss:  8.97408269e-03\n",
      "Epoch: 5141 mean train loss:  1.63749078e-02, mean val. rec. loss:  8.97237775e-03\n",
      "Epoch: 5142 mean train loss:  1.63728483e-02, mean val. rec. loss:  8.97067792e-03\n",
      "Epoch: 5143 mean train loss:  1.63708036e-02, mean val. rec. loss:  8.96896222e-03\n",
      "Epoch: 5144 mean train loss:  1.63687421e-02, mean val. rec. loss:  8.96724651e-03\n",
      "Epoch: 5145 mean train loss:  1.63666956e-02, mean val. rec. loss:  8.96553194e-03\n",
      "Epoch: 5146 mean train loss:  1.63646360e-02, mean val. rec. loss:  8.96382870e-03\n",
      "Epoch: 5147 mean train loss:  1.63625857e-02, mean val. rec. loss:  8.96213114e-03\n",
      "Epoch: 5148 mean train loss:  1.63605392e-02, mean val. rec. loss:  8.96043868e-03\n",
      "Epoch: 5149 mean train loss:  1.63584759e-02, mean val. rec. loss:  8.95873715e-03\n",
      "Epoch: 5150 mean train loss:  1.63564312e-02, mean val. rec. loss:  8.95703278e-03\n",
      "Epoch: 5151 mean train loss:  1.63543884e-02, mean val. rec. loss:  8.95532275e-03\n",
      "Epoch: 5152 mean train loss:  1.63523251e-02, mean val. rec. loss:  8.95361781e-03\n",
      "Epoch: 5153 mean train loss:  1.63502785e-02, mean val. rec. loss:  8.95191401e-03\n",
      "Epoch: 5154 mean train loss:  1.63482282e-02, mean val. rec. loss:  8.95022099e-03\n",
      "Epoch: 5155 mean train loss:  1.63461724e-02, mean val. rec. loss:  8.94852229e-03\n",
      "Epoch: 5156 mean train loss:  1.63441296e-02, mean val. rec. loss:  8.94681055e-03\n",
      "Epoch: 5157 mean train loss:  1.63420756e-02, mean val. rec. loss:  8.94511242e-03\n",
      "Epoch: 5158 mean train loss:  1.63400216e-02, mean val. rec. loss:  8.94340692e-03\n",
      "Epoch: 5159 mean train loss:  1.63379750e-02, mean val. rec. loss:  8.94171730e-03\n",
      "Epoch: 5160 mean train loss:  1.63359322e-02, mean val. rec. loss:  8.94002427e-03\n",
      "Epoch: 5161 mean train loss:  1.63338745e-02, mean val. rec. loss:  8.93834201e-03\n",
      "Epoch: 5162 mean train loss:  1.63318242e-02, mean val. rec. loss:  8.93664842e-03\n",
      "Epoch: 5163 mean train loss:  1.63297795e-02, mean val. rec. loss:  8.93495256e-03\n",
      "Epoch: 5164 mean train loss:  1.63277348e-02, mean val. rec. loss:  8.93323855e-03\n",
      "Epoch: 5165 mean train loss:  1.63256827e-02, mean val. rec. loss:  8.93154212e-03\n",
      "Epoch: 5166 mean train loss:  1.63236306e-02, mean val. rec. loss:  8.92984286e-03\n",
      "Epoch: 5167 mean train loss:  1.63215877e-02, mean val. rec. loss:  8.92813339e-03\n",
      "Epoch: 5168 mean train loss:  1.63195449e-02, mean val. rec. loss:  8.92644943e-03\n",
      "Epoch: 5169 mean train loss:  1.63174891e-02, mean val. rec. loss:  8.92477172e-03\n",
      "Epoch: 5170 mean train loss:  1.63154407e-02, mean val. rec. loss:  8.92308266e-03\n",
      "Epoch: 5171 mean train loss:  1.63133941e-02, mean val. rec. loss:  8.92139303e-03\n",
      "Epoch: 5172 mean train loss:  1.63113513e-02, mean val. rec. loss:  8.91969320e-03\n",
      "Epoch: 5173 mean train loss:  1.63093066e-02, mean val. rec. loss:  8.91798430e-03\n",
      "Epoch: 5174 mean train loss:  1.63072619e-02, mean val. rec. loss:  8.91628277e-03\n",
      "Epoch: 5175 mean train loss:  1.63052191e-02, mean val. rec. loss:  8.91459201e-03\n",
      "Epoch: 5176 mean train loss:  1.63031763e-02, mean val. rec. loss:  8.91292109e-03\n",
      "Epoch: 5177 mean train loss:  1.63011279e-02, mean val. rec. loss:  8.91123714e-03\n",
      "Epoch: 5178 mean train loss:  1.62990720e-02, mean val. rec. loss:  8.90954298e-03\n",
      "Epoch: 5179 mean train loss:  1.62970292e-02, mean val. rec. loss:  8.90785392e-03\n",
      "Epoch: 5180 mean train loss:  1.62949882e-02, mean val. rec. loss:  8.90615636e-03\n",
      "Epoch: 5181 mean train loss:  1.62929454e-02, mean val. rec. loss:  8.90446446e-03\n",
      "Epoch: 5182 mean train loss:  1.62909044e-02, mean val. rec. loss:  8.90277881e-03\n",
      "Epoch: 5183 mean train loss:  1.62888635e-02, mean val. rec. loss:  8.90108918e-03\n",
      "Epoch: 5184 mean train loss:  1.62868188e-02, mean val. rec. loss:  8.89942280e-03\n",
      "Epoch: 5185 mean train loss:  1.62847760e-02, mean val. rec. loss:  8.89773941e-03\n",
      "Epoch: 5186 mean train loss:  1.62827331e-02, mean val. rec. loss:  8.89605659e-03\n",
      "Epoch: 5187 mean train loss:  1.62806922e-02, mean val. rec. loss:  8.89436413e-03\n",
      "Epoch: 5188 mean train loss:  1.62786475e-02, mean val. rec. loss:  8.89267111e-03\n",
      "Epoch: 5189 mean train loss:  1.62766065e-02, mean val. rec. loss:  8.89096731e-03\n",
      "Epoch: 5190 mean train loss:  1.62745656e-02, mean val. rec. loss:  8.88927938e-03\n",
      "Epoch: 5191 mean train loss:  1.62725246e-02, mean val. rec. loss:  8.88760450e-03\n",
      "Epoch: 5192 mean train loss:  1.62704836e-02, mean val. rec. loss:  8.88593699e-03\n",
      "Epoch: 5193 mean train loss:  1.62684427e-02, mean val. rec. loss:  8.88425020e-03\n",
      "Epoch: 5194 mean train loss:  1.62663999e-02, mean val. rec. loss:  8.88257248e-03\n",
      "Epoch: 5195 mean train loss:  1.62643608e-02, mean val. rec. loss:  8.88088625e-03\n",
      "Epoch: 5196 mean train loss:  1.62623235e-02, mean val. rec. loss:  8.87920797e-03\n",
      "Epoch: 5197 mean train loss:  1.62602826e-02, mean val. rec. loss:  8.87751778e-03\n",
      "Epoch: 5198 mean train loss:  1.62582397e-02, mean val. rec. loss:  8.87582985e-03\n",
      "Epoch: 5199 mean train loss:  1.62561988e-02, mean val. rec. loss:  8.87414703e-03\n",
      "Epoch: 5200 mean train loss:  1.62541560e-02, mean val. rec. loss:  8.87247272e-03\n",
      "Epoch: 5201 mean train loss:  1.62521150e-02, mean val. rec. loss:  8.87078763e-03\n",
      "Epoch: 5202 mean train loss:  1.62500740e-02, mean val. rec. loss:  8.86910367e-03\n",
      "Epoch: 5203 mean train loss:  1.62480480e-02, mean val. rec. loss:  8.86743559e-03\n",
      "Epoch: 5204 mean train loss:  1.62460126e-02, mean val. rec. loss:  8.86575447e-03\n",
      "Epoch: 5205 mean train loss:  1.62439716e-02, mean val. rec. loss:  8.86408242e-03\n",
      "Epoch: 5206 mean train loss:  1.62419344e-02, mean val. rec. loss:  8.86241037e-03\n",
      "Epoch: 5207 mean train loss:  1.62398953e-02, mean val. rec. loss:  8.86074570e-03\n",
      "Epoch: 5208 mean train loss:  1.62378581e-02, mean val. rec. loss:  8.85906401e-03\n",
      "Epoch: 5209 mean train loss:  1.62358171e-02, mean val. rec. loss:  8.85737438e-03\n",
      "Epoch: 5210 mean train loss:  1.62337799e-02, mean val. rec. loss:  8.85569780e-03\n",
      "Epoch: 5211 mean train loss:  1.62317501e-02, mean val. rec. loss:  8.85402462e-03\n",
      "Epoch: 5212 mean train loss:  1.62297203e-02, mean val. rec. loss:  8.85235030e-03\n",
      "Epoch: 5213 mean train loss:  1.62276849e-02, mean val. rec. loss:  8.85068562e-03\n",
      "Epoch: 5214 mean train loss:  1.62256440e-02, mean val. rec. loss:  8.84902265e-03\n",
      "Epoch: 5215 mean train loss:  1.62236049e-02, mean val. rec. loss:  8.84736024e-03\n",
      "Epoch: 5216 mean train loss:  1.62215751e-02, mean val. rec. loss:  8.84568535e-03\n",
      "Epoch: 5217 mean train loss:  1.62195472e-02, mean val. rec. loss:  8.84401047e-03\n",
      "Epoch: 5218 mean train loss:  1.62175062e-02, mean val. rec. loss:  8.84232765e-03\n",
      "Epoch: 5219 mean train loss:  1.62154708e-02, mean val. rec. loss:  8.84065447e-03\n",
      "Epoch: 5220 mean train loss:  1.62134448e-02, mean val. rec. loss:  8.83897902e-03\n",
      "Epoch: 5221 mean train loss:  1.62114131e-02, mean val. rec. loss:  8.83732001e-03\n",
      "Epoch: 5222 mean train loss:  1.62093740e-02, mean val. rec. loss:  8.83565987e-03\n",
      "Epoch: 5223 mean train loss:  1.62073386e-02, mean val. rec. loss:  8.83400823e-03\n",
      "Epoch: 5224 mean train loss:  1.62053088e-02, mean val. rec. loss:  8.83234185e-03\n",
      "Epoch: 5225 mean train loss:  1.62032828e-02, mean val. rec. loss:  8.83066470e-03\n",
      "Epoch: 5226 mean train loss:  1.62012493e-02, mean val. rec. loss:  8.82898188e-03\n",
      "Epoch: 5227 mean train loss:  1.61992139e-02, mean val. rec. loss:  8.82731040e-03\n",
      "Epoch: 5228 mean train loss:  1.61971878e-02, mean val. rec. loss:  8.82565819e-03\n",
      "Epoch: 5229 mean train loss:  1.61951599e-02, mean val. rec. loss:  8.82400712e-03\n",
      "Epoch: 5230 mean train loss:  1.61931245e-02, mean val. rec. loss:  8.82234585e-03\n",
      "Epoch: 5231 mean train loss:  1.61910910e-02, mean val. rec. loss:  8.82069421e-03\n",
      "Epoch: 5232 mean train loss:  1.61890724e-02, mean val. rec. loss:  8.81904427e-03\n",
      "Epoch: 5233 mean train loss:  1.61870352e-02, mean val. rec. loss:  8.81737676e-03\n",
      "Epoch: 5234 mean train loss:  1.61850128e-02, mean val. rec. loss:  8.81570755e-03\n",
      "Epoch: 5235 mean train loss:  1.61829830e-02, mean val. rec. loss:  8.81402246e-03\n",
      "Epoch: 5236 mean train loss:  1.61809495e-02, mean val. rec. loss:  8.81234474e-03\n",
      "Epoch: 5237 mean train loss:  1.61789309e-02, mean val. rec. loss:  8.81069310e-03\n",
      "Epoch: 5238 mean train loss:  1.61768974e-02, mean val. rec. loss:  8.80904714e-03\n",
      "Epoch: 5239 mean train loss:  1.61748639e-02, mean val. rec. loss:  8.80738529e-03\n",
      "Epoch: 5240 mean train loss:  1.61728490e-02, mean val. rec. loss:  8.80573139e-03\n",
      "Epoch: 5241 mean train loss:  1.61708155e-02, mean val. rec. loss:  8.80407578e-03\n",
      "Epoch: 5242 mean train loss:  1.61687894e-02, mean val. rec. loss:  8.80241054e-03\n",
      "Epoch: 5243 mean train loss:  1.61667671e-02, mean val. rec. loss:  8.80075493e-03\n",
      "Epoch: 5244 mean train loss:  1.61647336e-02, mean val. rec. loss:  8.79909933e-03\n",
      "Epoch: 5245 mean train loss:  1.61627187e-02, mean val. rec. loss:  8.79745279e-03\n",
      "Epoch: 5246 mean train loss:  1.61606870e-02, mean val. rec. loss:  8.79580399e-03\n",
      "Epoch: 5247 mean train loss:  1.61586554e-02, mean val. rec. loss:  8.79414215e-03\n",
      "Epoch: 5248 mean train loss:  1.61566424e-02, mean val. rec. loss:  8.79247464e-03\n",
      "Epoch: 5249 mean train loss:  1.61546107e-02, mean val. rec. loss:  8.79081223e-03\n",
      "Epoch: 5250 mean train loss:  1.61525940e-02, mean val. rec. loss:  8.78915208e-03\n",
      "Epoch: 5251 mean train loss:  1.61505604e-02, mean val. rec. loss:  8.78749478e-03\n",
      "Epoch: 5252 mean train loss:  1.61485474e-02, mean val. rec. loss:  8.78586695e-03\n",
      "Epoch: 5253 mean train loss:  1.61465158e-02, mean val. rec. loss:  8.78422836e-03\n",
      "Epoch: 5254 mean train loss:  1.61445009e-02, mean val. rec. loss:  8.78258352e-03\n",
      "Epoch: 5255 mean train loss:  1.61424692e-02, mean val. rec. loss:  8.78092282e-03\n",
      "Epoch: 5256 mean train loss:  1.61404581e-02, mean val. rec. loss:  8.77927005e-03\n",
      "Epoch: 5257 mean train loss:  1.61384264e-02, mean val. rec. loss:  8.77760367e-03\n",
      "Epoch: 5258 mean train loss:  1.61364134e-02, mean val. rec. loss:  8.77594806e-03\n",
      "Epoch: 5259 mean train loss:  1.61343855e-02, mean val. rec. loss:  8.77429756e-03\n",
      "Epoch: 5260 mean train loss:  1.61323724e-02, mean val. rec. loss:  8.77266123e-03\n",
      "Epoch: 5261 mean train loss:  1.61303445e-02, mean val. rec. loss:  8.77102547e-03\n",
      "Epoch: 5262 mean train loss:  1.61283315e-02, mean val. rec. loss:  8.76939594e-03\n",
      "Epoch: 5263 mean train loss:  1.61263035e-02, mean val. rec. loss:  8.76776018e-03\n",
      "Epoch: 5264 mean train loss:  1.61242942e-02, mean val. rec. loss:  8.76610911e-03\n",
      "Epoch: 5265 mean train loss:  1.61222645e-02, mean val. rec. loss:  8.76445691e-03\n",
      "Epoch: 5266 mean train loss:  1.61202552e-02, mean val. rec. loss:  8.76280470e-03\n",
      "Epoch: 5267 mean train loss:  1.61182272e-02, mean val. rec. loss:  8.76115874e-03\n",
      "Epoch: 5268 mean train loss:  1.61162179e-02, mean val. rec. loss:  8.75952241e-03\n",
      "Epoch: 5269 mean train loss:  1.61141900e-02, mean val. rec. loss:  8.75788608e-03\n",
      "Epoch: 5270 mean train loss:  1.61121826e-02, mean val. rec. loss:  8.75625939e-03\n",
      "Epoch: 5271 mean train loss:  1.61101602e-02, mean val. rec. loss:  8.75460605e-03\n",
      "Epoch: 5272 mean train loss:  1.61081435e-02, mean val. rec. loss:  8.75294761e-03\n",
      "Epoch: 5273 mean train loss:  1.61061342e-02, mean val. rec. loss:  8.75130902e-03\n",
      "Epoch: 5274 mean train loss:  1.61041044e-02, mean val. rec. loss:  8.74967722e-03\n",
      "Epoch: 5275 mean train loss:  1.61020969e-02, mean val. rec. loss:  8.74805054e-03\n",
      "Epoch: 5276 mean train loss:  1.61000858e-02, mean val. rec. loss:  8.74641874e-03\n",
      "Epoch: 5277 mean train loss:  1.60980597e-02, mean val. rec. loss:  8.74478355e-03\n",
      "Epoch: 5278 mean train loss:  1.60960541e-02, mean val. rec. loss:  8.74314892e-03\n",
      "Epoch: 5279 mean train loss:  1.60940336e-02, mean val. rec. loss:  8.74150069e-03\n",
      "Epoch: 5280 mean train loss:  1.60920206e-02, mean val. rec. loss:  8.73984678e-03\n",
      "Epoch: 5281 mean train loss:  1.60900113e-02, mean val. rec. loss:  8.73821159e-03\n",
      "Epoch: 5282 mean train loss:  1.60879871e-02, mean val. rec. loss:  8.73658433e-03\n",
      "Epoch: 5283 mean train loss:  1.60859815e-02, mean val. rec. loss:  8.73497408e-03\n",
      "Epoch: 5284 mean train loss:  1.60839741e-02, mean val. rec. loss:  8.73334173e-03\n",
      "Epoch: 5285 mean train loss:  1.60819518e-02, mean val. rec. loss:  8.73171390e-03\n",
      "Epoch: 5286 mean train loss:  1.60799443e-02, mean val. rec. loss:  8.73006680e-03\n",
      "Epoch: 5287 mean train loss:  1.60779294e-02, mean val. rec. loss:  8.72842877e-03\n",
      "Epoch: 5288 mean train loss:  1.60759145e-02, mean val. rec. loss:  8.72678337e-03\n",
      "Epoch: 5289 mean train loss:  1.60739108e-02, mean val. rec. loss:  8.72515668e-03\n",
      "Epoch: 5290 mean train loss:  1.60718885e-02, mean val. rec. loss:  8.72353226e-03\n",
      "Epoch: 5291 mean train loss:  1.60698848e-02, mean val. rec. loss:  8.72191521e-03\n",
      "Epoch: 5292 mean train loss:  1.60678810e-02, mean val. rec. loss:  8.72029136e-03\n",
      "Epoch: 5293 mean train loss:  1.60658587e-02, mean val. rec. loss:  8.71865446e-03\n",
      "Epoch: 5294 mean train loss:  1.60638531e-02, mean val. rec. loss:  8.71701756e-03\n",
      "Epoch: 5295 mean train loss:  1.60618494e-02, mean val. rec. loss:  8.71539371e-03\n",
      "Epoch: 5296 mean train loss:  1.60598327e-02, mean val. rec. loss:  8.71376305e-03\n",
      "Epoch: 5297 mean train loss:  1.60578196e-02, mean val. rec. loss:  8.71213466e-03\n",
      "Epoch: 5298 mean train loss:  1.60558178e-02, mean val. rec. loss:  8.71051308e-03\n",
      "Epoch: 5299 mean train loss:  1.60538141e-02, mean val. rec. loss:  8.70888922e-03\n",
      "Epoch: 5300 mean train loss:  1.60518029e-02, mean val. rec. loss:  8.70726877e-03\n",
      "Epoch: 5301 mean train loss:  1.60497880e-02, mean val. rec. loss:  8.70564151e-03\n",
      "Epoch: 5302 mean train loss:  1.60477861e-02, mean val. rec. loss:  8.70402333e-03\n",
      "Epoch: 5303 mean train loss:  1.60457843e-02, mean val. rec. loss:  8.70239040e-03\n",
      "Epoch: 5304 mean train loss:  1.60437713e-02, mean val. rec. loss:  8.70076258e-03\n",
      "Epoch: 5305 mean train loss:  1.60417601e-02, mean val. rec. loss:  8.69914326e-03\n",
      "Epoch: 5306 mean train loss:  1.60397582e-02, mean val. rec. loss:  8.69752564e-03\n",
      "Epoch: 5307 mean train loss:  1.60377583e-02, mean val. rec. loss:  8.69590519e-03\n",
      "Epoch: 5308 mean train loss:  1.60357489e-02, mean val. rec. loss:  8.69428927e-03\n",
      "Epoch: 5309 mean train loss:  1.60337378e-02, mean val. rec. loss:  8.69268073e-03\n",
      "Epoch: 5310 mean train loss:  1.60317397e-02, mean val. rec. loss:  8.69106594e-03\n",
      "Epoch: 5311 mean train loss:  1.60297359e-02, mean val. rec. loss:  8.68945626e-03\n",
      "Epoch: 5312 mean train loss:  1.60277285e-02, mean val. rec. loss:  8.68783354e-03\n",
      "Epoch: 5313 mean train loss:  1.60257211e-02, mean val. rec. loss:  8.68620345e-03\n",
      "Epoch: 5314 mean train loss:  1.60237211e-02, mean val. rec. loss:  8.68458243e-03\n",
      "Epoch: 5315 mean train loss:  1.60217248e-02, mean val. rec. loss:  8.68296255e-03\n",
      "Epoch: 5316 mean train loss:  1.60197173e-02, mean val. rec. loss:  8.68135967e-03\n",
      "Epoch: 5317 mean train loss:  1.60177099e-02, mean val. rec. loss:  8.67975396e-03\n",
      "Epoch: 5318 mean train loss:  1.60157099e-02, mean val. rec. loss:  8.67815108e-03\n",
      "Epoch: 5319 mean train loss:  1.60137136e-02, mean val. rec. loss:  8.67654537e-03\n",
      "Epoch: 5320 mean train loss:  1.60117062e-02, mean val. rec. loss:  8.67491982e-03\n",
      "Epoch: 5321 mean train loss:  1.60097025e-02, mean val. rec. loss:  8.67329313e-03\n",
      "Epoch: 5322 mean train loss:  1.60077062e-02, mean val. rec. loss:  8.67168175e-03\n",
      "Epoch: 5323 mean train loss:  1.60057062e-02, mean val. rec. loss:  8.67008454e-03\n",
      "Epoch: 5324 mean train loss:  1.60037062e-02, mean val. rec. loss:  8.66847600e-03\n",
      "Epoch: 5325 mean train loss:  1.60017100e-02, mean val. rec. loss:  8.66686632e-03\n",
      "Epoch: 5326 mean train loss:  1.59997063e-02, mean val. rec. loss:  8.66525890e-03\n",
      "Epoch: 5327 mean train loss:  1.59976988e-02, mean val. rec. loss:  8.66364299e-03\n",
      "Epoch: 5328 mean train loss:  1.59957025e-02, mean val. rec. loss:  8.66202877e-03\n",
      "Epoch: 5329 mean train loss:  1.59937044e-02, mean val. rec. loss:  8.66042760e-03\n",
      "Epoch: 5330 mean train loss:  1.59917063e-02, mean val. rec. loss:  8.65881281e-03\n",
      "Epoch: 5331 mean train loss:  1.59897100e-02, mean val. rec. loss:  8.65721391e-03\n",
      "Epoch: 5332 mean train loss:  1.59877137e-02, mean val. rec. loss:  8.65561557e-03\n",
      "Epoch: 5333 mean train loss:  1.59857193e-02, mean val. rec. loss:  8.65401269e-03\n",
      "Epoch: 5334 mean train loss:  1.59837138e-02, mean val. rec. loss:  8.65240358e-03\n",
      "Epoch: 5335 mean train loss:  1.59817100e-02, mean val. rec. loss:  8.65079050e-03\n",
      "Epoch: 5336 mean train loss:  1.59797175e-02, mean val. rec. loss:  8.64918649e-03\n",
      "Epoch: 5337 mean train loss:  1.59777231e-02, mean val. rec. loss:  8.64757624e-03\n",
      "Epoch: 5338 mean train loss:  1.59757287e-02, mean val. rec. loss:  8.64598074e-03\n",
      "Epoch: 5339 mean train loss:  1.59737361e-02, mean val. rec. loss:  8.64438466e-03\n",
      "Epoch: 5340 mean train loss:  1.59717399e-02, mean val. rec. loss:  8.64279143e-03\n",
      "Epoch: 5341 mean train loss:  1.59697492e-02, mean val. rec. loss:  8.64119252e-03\n",
      "Epoch: 5342 mean train loss:  1.59677473e-02, mean val. rec. loss:  8.63959305e-03\n",
      "Epoch: 5343 mean train loss:  1.59657455e-02, mean val. rec. loss:  8.63798053e-03\n",
      "Epoch: 5344 mean train loss:  1.59637511e-02, mean val. rec. loss:  8.63637595e-03\n",
      "Epoch: 5345 mean train loss:  1.59617623e-02, mean val. rec. loss:  8.63477988e-03\n",
      "Epoch: 5346 mean train loss:  1.59597679e-02, mean val. rec. loss:  8.63317871e-03\n",
      "Epoch: 5347 mean train loss:  1.59577772e-02, mean val. rec. loss:  8.63157696e-03\n",
      "Epoch: 5348 mean train loss:  1.59557865e-02, mean val. rec. loss:  8.63000981e-03\n",
      "Epoch: 5349 mean train loss:  1.59537958e-02, mean val. rec. loss:  8.62841771e-03\n",
      "Epoch: 5350 mean train loss:  1.59517958e-02, mean val. rec. loss:  8.62682050e-03\n",
      "Epoch: 5351 mean train loss:  1.59497958e-02, mean val. rec. loss:  8.62521252e-03\n",
      "Epoch: 5352 mean train loss:  1.59478070e-02, mean val. rec. loss:  8.62361248e-03\n",
      "Epoch: 5353 mean train loss:  1.59458163e-02, mean val. rec. loss:  8.62200677e-03\n",
      "Epoch: 5354 mean train loss:  1.59438275e-02, mean val. rec. loss:  8.62041126e-03\n",
      "Epoch: 5355 mean train loss:  1.59418405e-02, mean val. rec. loss:  8.61883844e-03\n",
      "Epoch: 5356 mean train loss:  1.59398480e-02, mean val. rec. loss:  8.61726391e-03\n",
      "Epoch: 5357 mean train loss:  1.59378629e-02, mean val. rec. loss:  8.61566897e-03\n",
      "Epoch: 5358 mean train loss:  1.59358666e-02, mean val. rec. loss:  8.61407687e-03\n",
      "Epoch: 5359 mean train loss:  1.59338666e-02, mean val. rec. loss:  8.61248363e-03\n",
      "Epoch: 5360 mean train loss:  1.59318797e-02, mean val. rec. loss:  8.61088813e-03\n",
      "Epoch: 5361 mean train loss:  1.59298909e-02, mean val. rec. loss:  8.60929092e-03\n",
      "Epoch: 5362 mean train loss:  1.59279002e-02, mean val. rec. loss:  8.60770165e-03\n",
      "Epoch: 5363 mean train loss:  1.59259114e-02, mean val. rec. loss:  8.60612940e-03\n",
      "Epoch: 5364 mean train loss:  1.59239244e-02, mean val. rec. loss:  8.60453219e-03\n",
      "Epoch: 5365 mean train loss:  1.59219337e-02, mean val. rec. loss:  8.60295426e-03\n",
      "Epoch: 5366 mean train loss:  1.59199449e-02, mean val. rec. loss:  8.60136896e-03\n",
      "Epoch: 5367 mean train loss:  1.59179598e-02, mean val. rec. loss:  8.59977062e-03\n",
      "Epoch: 5368 mean train loss:  1.59159710e-02, mean val. rec. loss:  8.59818192e-03\n",
      "Epoch: 5369 mean train loss:  1.59139840e-02, mean val. rec. loss:  8.59659832e-03\n",
      "Epoch: 5370 mean train loss:  1.59119971e-02, mean val. rec. loss:  8.59501302e-03\n",
      "Epoch: 5371 mean train loss:  1.59100083e-02, mean val. rec. loss:  8.59342262e-03\n",
      "Epoch: 5372 mean train loss:  1.59080194e-02, mean val. rec. loss:  8.59184696e-03\n",
      "Epoch: 5373 mean train loss:  1.59060343e-02, mean val. rec. loss:  8.59026280e-03\n",
      "Epoch: 5374 mean train loss:  1.59040492e-02, mean val. rec. loss:  8.58868600e-03\n",
      "Epoch: 5375 mean train loss:  1.59020623e-02, mean val. rec. loss:  8.58710467e-03\n",
      "Epoch: 5376 mean train loss:  1.59000791e-02, mean val. rec. loss:  8.58551710e-03\n",
      "Epoch: 5377 mean train loss:  1.58980940e-02, mean val. rec. loss:  8.58394371e-03\n",
      "Epoch: 5378 mean train loss:  1.58961089e-02, mean val. rec. loss:  8.58236125e-03\n",
      "Epoch: 5379 mean train loss:  1.58941238e-02, mean val. rec. loss:  8.58077538e-03\n",
      "Epoch: 5380 mean train loss:  1.58921387e-02, mean val. rec. loss:  8.57919235e-03\n",
      "Epoch: 5381 mean train loss:  1.58901536e-02, mean val. rec. loss:  8.57760818e-03\n",
      "Epoch: 5382 mean train loss:  1.58881685e-02, mean val. rec. loss:  8.57604783e-03\n",
      "Epoch: 5383 mean train loss:  1.58861871e-02, mean val. rec. loss:  8.57447444e-03\n",
      "Epoch: 5384 mean train loss:  1.58842020e-02, mean val. rec. loss:  8.57290161e-03\n",
      "Epoch: 5385 mean train loss:  1.58822207e-02, mean val. rec. loss:  8.57132765e-03\n",
      "Epoch: 5386 mean train loss:  1.58802356e-02, mean val. rec. loss:  8.56975596e-03\n",
      "Epoch: 5387 mean train loss:  1.58782542e-02, mean val. rec. loss:  8.56817520e-03\n",
      "Epoch: 5388 mean train loss:  1.58762672e-02, mean val. rec. loss:  8.56659217e-03\n",
      "Epoch: 5389 mean train loss:  1.58742877e-02, mean val. rec. loss:  8.56501140e-03\n",
      "Epoch: 5390 mean train loss:  1.58723063e-02, mean val. rec. loss:  8.56344085e-03\n",
      "Epoch: 5391 mean train loss:  1.58703231e-02, mean val. rec. loss:  8.56186632e-03\n",
      "Epoch: 5392 mean train loss:  1.58683399e-02, mean val. rec. loss:  8.56030087e-03\n",
      "Epoch: 5393 mean train loss:  1.58663604e-02, mean val. rec. loss:  8.55873654e-03\n",
      "Epoch: 5394 mean train loss:  1.58643809e-02, mean val. rec. loss:  8.55716996e-03\n",
      "Epoch: 5395 mean train loss:  1.58623976e-02, mean val. rec. loss:  8.55559883e-03\n",
      "Epoch: 5396 mean train loss:  1.58604181e-02, mean val. rec. loss:  8.55403054e-03\n",
      "Epoch: 5397 mean train loss:  1.58584368e-02, mean val. rec. loss:  8.55244864e-03\n",
      "Epoch: 5398 mean train loss:  1.58564554e-02, mean val. rec. loss:  8.55087412e-03\n",
      "Epoch: 5399 mean train loss:  1.58544759e-02, mean val. rec. loss:  8.54930016e-03\n",
      "Epoch: 5400 mean train loss:  1.58524964e-02, mean val. rec. loss:  8.54773527e-03\n",
      "Epoch: 5401 mean train loss:  1.58505169e-02, mean val. rec. loss:  8.54617152e-03\n",
      "Epoch: 5402 mean train loss:  1.58485373e-02, mean val. rec. loss:  8.54459359e-03\n",
      "Epoch: 5403 mean train loss:  1.58465578e-02, mean val. rec. loss:  8.54302757e-03\n",
      "Epoch: 5404 mean train loss:  1.58445802e-02, mean val. rec. loss:  8.54147175e-03\n",
      "Epoch: 5405 mean train loss:  1.58426025e-02, mean val. rec. loss:  8.53991027e-03\n",
      "Epoch: 5406 mean train loss:  1.58406230e-02, mean val. rec. loss:  8.53836125e-03\n",
      "Epoch: 5407 mean train loss:  1.58386435e-02, mean val. rec. loss:  8.53679353e-03\n",
      "Epoch: 5408 mean train loss:  1.58366677e-02, mean val. rec. loss:  8.53522581e-03\n",
      "Epoch: 5409 mean train loss:  1.58346901e-02, mean val. rec. loss:  8.53365298e-03\n",
      "Epoch: 5410 mean train loss:  1.58327125e-02, mean val. rec. loss:  8.53207959e-03\n",
      "Epoch: 5411 mean train loss:  1.58307348e-02, mean val. rec. loss:  8.53052491e-03\n",
      "Epoch: 5412 mean train loss:  1.58287572e-02, mean val. rec. loss:  8.52896229e-03\n",
      "Epoch: 5413 mean train loss:  1.58267795e-02, mean val. rec. loss:  8.52740250e-03\n",
      "Epoch: 5414 mean train loss:  1.58248019e-02, mean val. rec. loss:  8.52583591e-03\n",
      "Epoch: 5415 mean train loss:  1.58228261e-02, mean val. rec. loss:  8.52428577e-03\n",
      "Epoch: 5416 mean train loss:  1.58208503e-02, mean val. rec. loss:  8.52272825e-03\n",
      "Epoch: 5417 mean train loss:  1.58188727e-02, mean val. rec. loss:  8.52115940e-03\n",
      "Epoch: 5418 mean train loss:  1.58168969e-02, mean val. rec. loss:  8.51960528e-03\n",
      "Epoch: 5419 mean train loss:  1.58149211e-02, mean val. rec. loss:  8.51805117e-03\n",
      "Epoch: 5420 mean train loss:  1.58129472e-02, mean val. rec. loss:  8.51650159e-03\n",
      "Epoch: 5421 mean train loss:  1.58109695e-02, mean val. rec. loss:  8.51494747e-03\n",
      "Epoch: 5422 mean train loss:  1.58089956e-02, mean val. rec. loss:  8.51338258e-03\n",
      "Epoch: 5423 mean train loss:  1.58070179e-02, mean val. rec. loss:  8.51183187e-03\n",
      "Epoch: 5424 mean train loss:  1.58050440e-02, mean val. rec. loss:  8.51025508e-03\n",
      "Epoch: 5425 mean train loss:  1.58030701e-02, mean val. rec. loss:  8.50869132e-03\n",
      "Epoch: 5426 mean train loss:  1.58010962e-02, mean val. rec. loss:  8.50714344e-03\n",
      "Epoch: 5427 mean train loss:  1.57991204e-02, mean val. rec. loss:  8.50559557e-03\n",
      "Epoch: 5428 mean train loss:  1.57971483e-02, mean val. rec. loss:  8.50405449e-03\n",
      "Epoch: 5429 mean train loss:  1.57951744e-02, mean val. rec. loss:  8.50250945e-03\n",
      "Epoch: 5430 mean train loss:  1.57931986e-02, mean val. rec. loss:  8.50095760e-03\n",
      "Epoch: 5431 mean train loss:  1.57912266e-02, mean val. rec. loss:  8.49939158e-03\n",
      "Epoch: 5432 mean train loss:  1.57892508e-02, mean val. rec. loss:  8.49783406e-03\n",
      "Epoch: 5433 mean train loss:  1.57872787e-02, mean val. rec. loss:  8.49627768e-03\n",
      "Epoch: 5434 mean train loss:  1.57853048e-02, mean val. rec. loss:  8.49473207e-03\n",
      "Epoch: 5435 mean train loss:  1.57833346e-02, mean val. rec. loss:  8.49318759e-03\n",
      "Epoch: 5436 mean train loss:  1.57813607e-02, mean val. rec. loss:  8.49163178e-03\n",
      "Epoch: 5437 mean train loss:  1.57793868e-02, mean val. rec. loss:  8.49008730e-03\n",
      "Epoch: 5438 mean train loss:  1.57774166e-02, mean val. rec. loss:  8.48853546e-03\n",
      "Epoch: 5439 mean train loss:  1.57754426e-02, mean val. rec. loss:  8.48699155e-03\n",
      "Epoch: 5440 mean train loss:  1.57734687e-02, mean val. rec. loss:  8.48544140e-03\n",
      "Epoch: 5441 mean train loss:  1.57714985e-02, mean val. rec. loss:  8.48388558e-03\n",
      "Epoch: 5442 mean train loss:  1.57695339e-02, mean val. rec. loss:  8.48235018e-03\n",
      "Epoch: 5443 mean train loss:  1.57675619e-02, mean val. rec. loss:  8.48079947e-03\n",
      "Epoch: 5444 mean train loss:  1.57655898e-02, mean val. rec. loss:  8.47925102e-03\n",
      "Epoch: 5445 mean train loss:  1.57636177e-02, mean val. rec. loss:  8.47771222e-03\n",
      "Epoch: 5446 mean train loss:  1.57616475e-02, mean val. rec. loss:  8.47615980e-03\n",
      "Epoch: 5447 mean train loss:  1.57596811e-02, mean val. rec. loss:  8.47461533e-03\n",
      "Epoch: 5448 mean train loss:  1.57577071e-02, mean val. rec. loss:  8.47306631e-03\n",
      "Epoch: 5449 mean train loss:  1.57557369e-02, mean val. rec. loss:  8.47153091e-03\n",
      "Epoch: 5450 mean train loss:  1.57537630e-02, mean val. rec. loss:  8.46998076e-03\n",
      "Epoch: 5451 mean train loss:  1.57517891e-02, mean val. rec. loss:  8.46842268e-03\n",
      "Epoch: 5452 mean train loss:  1.57498189e-02, mean val. rec. loss:  8.46687253e-03\n",
      "Epoch: 5453 mean train loss:  1.57478450e-02, mean val. rec. loss:  8.46532579e-03\n",
      "Epoch: 5454 mean train loss:  1.57458804e-02, mean val. rec. loss:  8.46378188e-03\n",
      "Epoch: 5455 mean train loss:  1.57439176e-02, mean val. rec. loss:  8.46224194e-03\n",
      "Epoch: 5456 mean train loss:  1.57419474e-02, mean val. rec. loss:  8.46071504e-03\n",
      "Epoch: 5457 mean train loss:  1.57399791e-02, mean val. rec. loss:  8.45917737e-03\n",
      "Epoch: 5458 mean train loss:  1.57380089e-02, mean val. rec. loss:  8.45763516e-03\n",
      "Epoch: 5459 mean train loss:  1.57360387e-02, mean val. rec. loss:  8.45608275e-03\n",
      "Epoch: 5460 mean train loss:  1.57340722e-02, mean val. rec. loss:  8.45453714e-03\n",
      "Epoch: 5461 mean train loss:  1.57321002e-02, mean val. rec. loss:  8.45299379e-03\n",
      "Epoch: 5462 mean train loss:  1.57301318e-02, mean val. rec. loss:  8.45146633e-03\n",
      "Epoch: 5463 mean train loss:  1.57281635e-02, mean val. rec. loss:  8.44993489e-03\n",
      "Epoch: 5464 mean train loss:  1.57261933e-02, mean val. rec. loss:  8.44840459e-03\n",
      "Epoch: 5465 mean train loss:  1.57242268e-02, mean val. rec. loss:  8.44686238e-03\n",
      "Epoch: 5466 mean train loss:  1.57222566e-02, mean val. rec. loss:  8.44532301e-03\n",
      "Epoch: 5467 mean train loss:  1.57202864e-02, mean val. rec. loss:  8.44378590e-03\n",
      "Epoch: 5468 mean train loss:  1.57183200e-02, mean val. rec. loss:  8.44224086e-03\n",
      "Epoch: 5469 mean train loss:  1.57163516e-02, mean val. rec. loss:  8.44070602e-03\n",
      "Epoch: 5470 mean train loss:  1.57143814e-02, mean val. rec. loss:  8.43917686e-03\n",
      "Epoch: 5471 mean train loss:  1.57124131e-02, mean val. rec. loss:  8.43764032e-03\n",
      "Epoch: 5472 mean train loss:  1.57104429e-02, mean val. rec. loss:  8.43610265e-03\n",
      "Epoch: 5473 mean train loss:  1.57084746e-02, mean val. rec. loss:  8.43456554e-03\n",
      "Epoch: 5474 mean train loss:  1.57065081e-02, mean val. rec. loss:  8.43303127e-03\n",
      "Epoch: 5475 mean train loss:  1.57045416e-02, mean val. rec. loss:  8.43150040e-03\n",
      "Epoch: 5476 mean train loss:  1.57025751e-02, mean val. rec. loss:  8.42997634e-03\n",
      "Epoch: 5477 mean train loss:  1.57006031e-02, mean val. rec. loss:  8.42842903e-03\n",
      "Epoch: 5478 mean train loss:  1.56986366e-02, mean val. rec. loss:  8.42688625e-03\n",
      "Epoch: 5479 mean train loss:  1.56966683e-02, mean val. rec. loss:  8.42535708e-03\n",
      "Epoch: 5480 mean train loss:  1.56947018e-02, mean val. rec. loss:  8.42382225e-03\n",
      "Epoch: 5481 mean train loss:  1.56927353e-02, mean val. rec. loss:  8.42229818e-03\n",
      "Epoch: 5482 mean train loss:  1.56907670e-02, mean val. rec. loss:  8.42077298e-03\n",
      "Epoch: 5483 mean train loss:  1.56887986e-02, mean val. rec. loss:  8.41923134e-03\n",
      "Epoch: 5484 mean train loss:  1.56868340e-02, mean val. rec. loss:  8.41769480e-03\n",
      "Epoch: 5485 mean train loss:  1.56848657e-02, mean val. rec. loss:  8.41617924e-03\n",
      "Epoch: 5486 mean train loss:  1.56828974e-02, mean val. rec. loss:  8.41464724e-03\n",
      "Epoch: 5487 mean train loss:  1.56809290e-02, mean val. rec. loss:  8.41311978e-03\n",
      "Epoch: 5488 mean train loss:  1.56789644e-02, mean val. rec. loss:  8.41159174e-03\n",
      "Epoch: 5489 mean train loss:  1.56769998e-02, mean val. rec. loss:  8.41005237e-03\n",
      "Epoch: 5490 mean train loss:  1.56750296e-02, mean val. rec. loss:  8.40851696e-03\n",
      "Epoch: 5491 mean train loss:  1.56730631e-02, mean val. rec. loss:  8.40699063e-03\n",
      "Epoch: 5492 mean train loss:  1.56710967e-02, mean val. rec. loss:  8.40546430e-03\n",
      "Epoch: 5493 mean train loss:  1.56691302e-02, mean val. rec. loss:  8.40394647e-03\n",
      "Epoch: 5494 mean train loss:  1.56671619e-02, mean val. rec. loss:  8.40242127e-03\n",
      "Epoch: 5495 mean train loss:  1.56651935e-02, mean val. rec. loss:  8.40088757e-03\n",
      "Epoch: 5496 mean train loss:  1.56632289e-02, mean val. rec. loss:  8.39935954e-03\n",
      "Epoch: 5497 mean train loss:  1.56612624e-02, mean val. rec. loss:  8.39783831e-03\n",
      "Epoch: 5498 mean train loss:  1.56592960e-02, mean val. rec. loss:  8.39631878e-03\n",
      "Epoch: 5499 mean train loss:  1.56573295e-02, mean val. rec. loss:  8.39478281e-03\n",
      "Epoch: 5500 mean train loss:  1.56553630e-02, mean val. rec. loss:  8.39325364e-03\n",
      "Epoch: 5501 mean train loss:  1.56533947e-02, mean val. rec. loss:  8.39172957e-03\n",
      "Epoch: 5502 mean train loss:  1.56514282e-02, mean val. rec. loss:  8.39019984e-03\n",
      "Epoch: 5503 mean train loss:  1.56494617e-02, mean val. rec. loss:  8.38868485e-03\n",
      "Epoch: 5504 mean train loss:  1.56474953e-02, mean val. rec. loss:  8.38716078e-03\n",
      "Epoch: 5505 mean train loss:  1.56455288e-02, mean val. rec. loss:  8.38564012e-03\n",
      "Epoch: 5506 mean train loss:  1.56435623e-02, mean val. rec. loss:  8.38410245e-03\n",
      "Epoch: 5507 mean train loss:  1.56415958e-02, mean val. rec. loss:  8.38257668e-03\n",
      "Epoch: 5508 mean train loss:  1.56396294e-02, mean val. rec. loss:  8.38105772e-03\n",
      "Epoch: 5509 mean train loss:  1.56376629e-02, mean val. rec. loss:  8.37954613e-03\n",
      "Epoch: 5510 mean train loss:  1.56356964e-02, mean val. rec. loss:  8.37802887e-03\n",
      "Epoch: 5511 mean train loss:  1.56337299e-02, mean val. rec. loss:  8.37650027e-03\n",
      "Epoch: 5512 mean train loss:  1.56317598e-02, mean val. rec. loss:  8.37496430e-03\n",
      "Epoch: 5513 mean train loss:  1.56297970e-02, mean val. rec. loss:  8.37344477e-03\n",
      "Epoch: 5514 mean train loss:  1.56278287e-02, mean val. rec. loss:  8.37193148e-03\n",
      "Epoch: 5515 mean train loss:  1.56258622e-02, mean val. rec. loss:  8.37040741e-03\n",
      "Epoch: 5516 mean train loss:  1.56238976e-02, mean val. rec. loss:  8.36887938e-03\n",
      "Epoch: 5517 mean train loss:  1.56219292e-02, mean val. rec. loss:  8.36736892e-03\n",
      "Epoch: 5518 mean train loss:  1.56199628e-02, mean val. rec. loss:  8.36584826e-03\n",
      "Epoch: 5519 mean train loss:  1.56179963e-02, mean val. rec. loss:  8.36431796e-03\n",
      "Epoch: 5520 mean train loss:  1.56160298e-02, mean val. rec. loss:  8.36279616e-03\n",
      "Epoch: 5521 mean train loss:  1.56140606e-02, mean val. rec. loss:  8.36127493e-03\n",
      "Epoch: 5522 mean train loss:  1.56120950e-02, mean val. rec. loss:  8.35974803e-03\n",
      "Epoch: 5523 mean train loss:  1.56101285e-02, mean val. rec. loss:  8.35823191e-03\n",
      "Epoch: 5524 mean train loss:  1.56081602e-02, mean val. rec. loss:  8.35670898e-03\n",
      "Epoch: 5525 mean train loss:  1.56061937e-02, mean val. rec. loss:  8.35520135e-03\n",
      "Epoch: 5526 mean train loss:  1.56042282e-02, mean val. rec. loss:  8.35368806e-03\n",
      "Epoch: 5527 mean train loss:  1.56022589e-02, mean val. rec. loss:  8.35215833e-03\n",
      "Epoch: 5528 mean train loss:  1.56002934e-02, mean val. rec. loss:  8.35063029e-03\n",
      "Epoch: 5529 mean train loss:  1.55983269e-02, mean val. rec. loss:  8.34910510e-03\n",
      "Epoch: 5530 mean train loss:  1.55963604e-02, mean val. rec. loss:  8.34758103e-03\n",
      "Epoch: 5531 mean train loss:  1.55943930e-02, mean val. rec. loss:  8.34606604e-03\n",
      "Epoch: 5532 mean train loss:  1.55924238e-02, mean val. rec. loss:  8.34455898e-03\n",
      "Epoch: 5533 mean train loss:  1.55904564e-02, mean val. rec. loss:  8.34305363e-03\n",
      "Epoch: 5534 mean train loss:  1.55884899e-02, mean val. rec. loss:  8.34153750e-03\n",
      "Epoch: 5535 mean train loss:  1.55865216e-02, mean val. rec. loss:  8.34000040e-03\n",
      "Epoch: 5536 mean train loss:  1.55845551e-02, mean val. rec. loss:  8.33848030e-03\n",
      "Epoch: 5537 mean train loss:  1.55825867e-02, mean val. rec. loss:  8.33695624e-03\n",
      "Epoch: 5538 mean train loss:  1.55806184e-02, mean val. rec. loss:  8.33544124e-03\n",
      "Epoch: 5539 mean train loss:  1.55786501e-02, mean val. rec. loss:  8.33393702e-03\n",
      "Epoch: 5540 mean train loss:  1.55766827e-02, mean val. rec. loss:  8.33242997e-03\n",
      "Epoch: 5541 mean train loss:  1.55747153e-02, mean val. rec. loss:  8.33090534e-03\n",
      "Epoch: 5542 mean train loss:  1.55727479e-02, mean val. rec. loss:  8.32937730e-03\n",
      "Epoch: 5543 mean train loss:  1.55707777e-02, mean val. rec. loss:  8.32785721e-03\n",
      "Epoch: 5544 mean train loss:  1.55688093e-02, mean val. rec. loss:  8.32634448e-03\n",
      "Epoch: 5545 mean train loss:  1.55668419e-02, mean val. rec. loss:  8.32484877e-03\n",
      "Epoch: 5546 mean train loss:  1.55648717e-02, mean val. rec. loss:  8.32332641e-03\n",
      "Epoch: 5547 mean train loss:  1.55629053e-02, mean val. rec. loss:  8.32179724e-03\n",
      "Epoch: 5548 mean train loss:  1.55609369e-02, mean val. rec. loss:  8.32028565e-03\n",
      "Epoch: 5549 mean train loss:  1.55589667e-02, mean val. rec. loss:  8.31877065e-03\n",
      "Epoch: 5550 mean train loss:  1.55569993e-02, mean val. rec. loss:  8.31726643e-03\n",
      "Epoch: 5551 mean train loss:  1.55550291e-02, mean val. rec. loss:  8.31574861e-03\n",
      "Epoch: 5552 mean train loss:  1.55530589e-02, mean val. rec. loss:  8.31423645e-03\n",
      "Epoch: 5553 mean train loss:  1.55510822e-02, mean val. rec. loss:  8.31271635e-03\n",
      "Epoch: 5554 mean train loss:  1.55491046e-02, mean val. rec. loss:  8.31119626e-03\n",
      "Epoch: 5555 mean train loss:  1.55471409e-02, mean val. rec. loss:  8.30967503e-03\n",
      "Epoch: 5556 mean train loss:  1.55451716e-02, mean val. rec. loss:  8.30816457e-03\n",
      "Epoch: 5557 mean train loss:  1.55432042e-02, mean val. rec. loss:  8.30666659e-03\n",
      "Epoch: 5558 mean train loss:  1.55412359e-02, mean val. rec. loss:  8.30516350e-03\n",
      "Epoch: 5559 mean train loss:  1.55392694e-02, mean val. rec. loss:  8.30365815e-03\n",
      "Epoch: 5560 mean train loss:  1.55373001e-02, mean val. rec. loss:  8.30214542e-03\n",
      "Epoch: 5561 mean train loss:  1.55353244e-02, mean val. rec. loss:  8.30063497e-03\n",
      "Epoch: 5562 mean train loss:  1.55333458e-02, mean val. rec. loss:  8.29910977e-03\n",
      "Epoch: 5563 mean train loss:  1.55313784e-02, mean val. rec. loss:  8.29758797e-03\n",
      "Epoch: 5564 mean train loss:  1.55294082e-02, mean val. rec. loss:  8.29607695e-03\n",
      "Epoch: 5565 mean train loss:  1.55274398e-02, mean val. rec. loss:  8.29457783e-03\n",
      "Epoch: 5566 mean train loss:  1.55254715e-02, mean val. rec. loss:  8.29306397e-03\n",
      "Epoch: 5567 mean train loss:  1.55235013e-02, mean val. rec. loss:  8.29155975e-03\n",
      "Epoch: 5568 mean train loss:  1.55215320e-02, mean val. rec. loss:  8.29006687e-03\n",
      "Epoch: 5569 mean train loss:  1.55195516e-02, mean val. rec. loss:  8.28855131e-03\n",
      "Epoch: 5570 mean train loss:  1.55175758e-02, mean val. rec. loss:  8.28702611e-03\n",
      "Epoch: 5571 mean train loss:  1.55156065e-02, mean val. rec. loss:  8.28551339e-03\n",
      "Epoch: 5572 mean train loss:  1.55136345e-02, mean val. rec. loss:  8.28399046e-03\n",
      "Epoch: 5573 mean train loss:  1.55116643e-02, mean val. rec. loss:  8.28248454e-03\n",
      "Epoch: 5574 mean train loss:  1.55096960e-02, mean val. rec. loss:  8.28098542e-03\n",
      "Epoch: 5575 mean train loss:  1.55077267e-02, mean val. rec. loss:  8.27949594e-03\n",
      "Epoch: 5576 mean train loss:  1.55057537e-02, mean val. rec. loss:  8.27797018e-03\n",
      "Epoch: 5577 mean train loss:  1.55037761e-02, mean val. rec. loss:  8.27645462e-03\n",
      "Epoch: 5578 mean train loss:  1.55017938e-02, mean val. rec. loss:  8.27493566e-03\n",
      "Epoch: 5579 mean train loss:  1.54998226e-02, mean val. rec. loss:  8.27342520e-03\n",
      "Epoch: 5580 mean train loss:  1.54978515e-02, mean val. rec. loss:  8.27191814e-03\n",
      "Epoch: 5581 mean train loss:  1.54958813e-02, mean val. rec. loss:  8.27041279e-03\n",
      "Epoch: 5582 mean train loss:  1.54939092e-02, mean val. rec. loss:  8.26891878e-03\n",
      "Epoch: 5583 mean train loss:  1.54919353e-02, mean val. rec. loss:  8.26741002e-03\n",
      "Epoch: 5584 mean train loss:  1.54899651e-02, mean val. rec. loss:  8.26589673e-03\n",
      "Epoch: 5585 mean train loss:  1.54879800e-02, mean val. rec. loss:  8.26437833e-03\n",
      "Epoch: 5586 mean train loss:  1.54859987e-02, mean val. rec. loss:  8.26286618e-03\n",
      "Epoch: 5587 mean train loss:  1.54840266e-02, mean val. rec. loss:  8.26134041e-03\n",
      "Epoch: 5588 mean train loss:  1.54820545e-02, mean val. rec. loss:  8.25984356e-03\n",
      "Epoch: 5589 mean train loss:  1.54800806e-02, mean val. rec. loss:  8.25834444e-03\n",
      "Epoch: 5590 mean train loss:  1.54781086e-02, mean val. rec. loss:  8.25683172e-03\n",
      "Epoch: 5591 mean train loss:  1.54761337e-02, mean val. rec. loss:  8.25532126e-03\n",
      "Epoch: 5592 mean train loss:  1.54741588e-02, mean val. rec. loss:  8.25381818e-03\n",
      "Epoch: 5593 mean train loss:  1.54721803e-02, mean val. rec. loss:  8.25232416e-03\n",
      "Epoch: 5594 mean train loss:  1.54701905e-02, mean val. rec. loss:  8.25080520e-03\n",
      "Epoch: 5595 mean train loss:  1.54682175e-02, mean val. rec. loss:  8.24929418e-03\n",
      "Epoch: 5596 mean train loss:  1.54662427e-02, mean val. rec. loss:  8.24778485e-03\n",
      "Epoch: 5597 mean train loss:  1.54642678e-02, mean val. rec. loss:  8.24626986e-03\n",
      "Epoch: 5598 mean train loss:  1.54622939e-02, mean val. rec. loss:  8.24477358e-03\n",
      "Epoch: 5599 mean train loss:  1.54603097e-02, mean val. rec. loss:  8.24326312e-03\n",
      "Epoch: 5600 mean train loss:  1.54583312e-02, mean val. rec. loss:  8.24176174e-03\n",
      "Epoch: 5601 mean train loss:  1.54563544e-02, mean val. rec. loss:  8.24025638e-03\n",
      "Epoch: 5602 mean train loss:  1.54543805e-02, mean val. rec. loss:  8.23874763e-03\n",
      "Epoch: 5603 mean train loss:  1.54523973e-02, mean val. rec. loss:  8.23723604e-03\n",
      "Epoch: 5604 mean train loss:  1.54504122e-02, mean val. rec. loss:  8.23573408e-03\n",
      "Epoch: 5605 mean train loss:  1.54484364e-02, mean val. rec. loss:  8.23421512e-03\n",
      "Epoch: 5606 mean train loss:  1.54464643e-02, mean val. rec. loss:  8.23270297e-03\n",
      "Epoch: 5607 mean train loss:  1.54444755e-02, mean val. rec. loss:  8.23120725e-03\n",
      "Epoch: 5608 mean train loss:  1.54424914e-02, mean val. rec. loss:  8.22970303e-03\n",
      "Epoch: 5609 mean train loss:  1.54405156e-02, mean val. rec. loss:  8.22819938e-03\n",
      "Epoch: 5610 mean train loss:  1.54385389e-02, mean val. rec. loss:  8.22669629e-03\n",
      "Epoch: 5611 mean train loss:  1.54365556e-02, mean val. rec. loss:  8.22517563e-03\n",
      "Epoch: 5612 mean train loss:  1.54345677e-02, mean val. rec. loss:  8.22366744e-03\n",
      "Epoch: 5613 mean train loss:  1.54325910e-02, mean val. rec. loss:  8.22214904e-03\n",
      "Epoch: 5614 mean train loss:  1.54306115e-02, mean val. rec. loss:  8.22063859e-03\n",
      "Epoch: 5615 mean train loss:  1.54286227e-02, mean val. rec. loss:  8.21912700e-03\n",
      "Epoch: 5616 mean train loss:  1.54266385e-02, mean val. rec. loss:  8.21762448e-03\n",
      "Epoch: 5617 mean train loss:  1.54246600e-02, mean val. rec. loss:  8.21611402e-03\n",
      "Epoch: 5618 mean train loss:  1.54226814e-02, mean val. rec. loss:  8.21460810e-03\n",
      "Epoch: 5619 mean train loss:  1.54206954e-02, mean val. rec. loss:  8.21310615e-03\n",
      "Epoch: 5620 mean train loss:  1.54187028e-02, mean val. rec. loss:  8.21159286e-03\n",
      "Epoch: 5621 mean train loss:  1.54167252e-02, mean val. rec. loss:  8.21007106e-03\n",
      "Epoch: 5622 mean train loss:  1.54147429e-02, mean val. rec. loss:  8.20856514e-03\n",
      "Epoch: 5623 mean train loss:  1.54127531e-02, mean val. rec. loss:  8.20706092e-03\n",
      "Epoch: 5624 mean train loss:  1.54107634e-02, mean val. rec. loss:  8.20556010e-03\n",
      "Epoch: 5625 mean train loss:  1.54087829e-02, mean val. rec. loss:  8.20404964e-03\n",
      "Epoch: 5626 mean train loss:  1.54067997e-02, mean val. rec. loss:  8.20254316e-03\n",
      "Epoch: 5627 mean train loss:  1.54048109e-02, mean val. rec. loss:  8.20105538e-03\n",
      "Epoch: 5628 mean train loss:  1.54028165e-02, mean val. rec. loss:  8.19952054e-03\n",
      "Epoch: 5629 mean train loss:  1.54008360e-02, mean val. rec. loss:  8.19801349e-03\n",
      "Epoch: 5630 mean train loss:  1.53988528e-02, mean val. rec. loss:  8.19649509e-03\n",
      "Epoch: 5631 mean train loss:  1.53968602e-02, mean val. rec. loss:  8.19498350e-03\n",
      "Epoch: 5632 mean train loss:  1.53948705e-02, mean val. rec. loss:  8.19348212e-03\n",
      "Epoch: 5633 mean train loss:  1.53928882e-02, mean val. rec. loss:  8.19198016e-03\n",
      "Epoch: 5634 mean train loss:  1.53908873e-02, mean val. rec. loss:  8.19047935e-03\n",
      "Epoch: 5635 mean train loss:  1.53889059e-02, mean val. rec. loss:  8.18898023e-03\n",
      "Epoch: 5636 mean train loss:  1.53869227e-02, mean val. rec. loss:  8.18746637e-03\n",
      "Epoch: 5637 mean train loss:  1.53849199e-02, mean val. rec. loss:  8.18595932e-03\n",
      "Epoch: 5638 mean train loss:  1.53829376e-02, mean val. rec. loss:  8.18442845e-03\n",
      "Epoch: 5639 mean train loss:  1.53809478e-02, mean val. rec. loss:  8.18291232e-03\n",
      "Epoch: 5640 mean train loss:  1.53789506e-02, mean val. rec. loss:  8.18140923e-03\n",
      "Epoch: 5641 mean train loss:  1.53769674e-02, mean val. rec. loss:  8.17991692e-03\n",
      "Epoch: 5642 mean train loss:  1.53749637e-02, mean val. rec. loss:  8.17841213e-03\n",
      "Epoch: 5643 mean train loss:  1.53729786e-02, mean val. rec. loss:  8.17689487e-03\n",
      "Epoch: 5644 mean train loss:  1.53709916e-02, mean val. rec. loss:  8.17540199e-03\n",
      "Epoch: 5645 mean train loss:  1.53689870e-02, mean val. rec. loss:  8.17388133e-03\n",
      "Epoch: 5646 mean train loss:  1.53670010e-02, mean val. rec. loss:  8.17237314e-03\n",
      "Epoch: 5647 mean train loss:  1.53650028e-02, mean val. rec. loss:  8.17085248e-03\n",
      "Epoch: 5648 mean train loss:  1.53630066e-02, mean val. rec. loss:  8.16934259e-03\n",
      "Epoch: 5649 mean train loss:  1.53610205e-02, mean val. rec. loss:  8.16782873e-03\n",
      "Epoch: 5650 mean train loss:  1.53590140e-02, mean val. rec. loss:  8.16632621e-03\n",
      "Epoch: 5651 mean train loss:  1.53570233e-02, mean val. rec. loss:  8.16482143e-03\n",
      "Epoch: 5652 mean train loss:  1.53550364e-02, mean val. rec. loss:  8.16330700e-03\n",
      "Epoch: 5653 mean train loss:  1.53530261e-02, mean val. rec. loss:  8.16179598e-03\n",
      "Epoch: 5654 mean train loss:  1.53510392e-02, mean val. rec. loss:  8.16028098e-03\n",
      "Epoch: 5655 mean train loss:  1.53490383e-02, mean val. rec. loss:  8.15876769e-03\n",
      "Epoch: 5656 mean train loss:  1.53470373e-02, mean val. rec. loss:  8.15725780e-03\n",
      "Epoch: 5657 mean train loss:  1.53450467e-02, mean val. rec. loss:  8.15575131e-03\n",
      "Epoch: 5658 mean train loss:  1.53430346e-02, mean val. rec. loss:  8.15423972e-03\n",
      "Epoch: 5659 mean train loss:  1.53410439e-02, mean val. rec. loss:  8.15274061e-03\n",
      "Epoch: 5660 mean train loss:  1.53390467e-02, mean val. rec. loss:  8.15123809e-03\n",
      "Epoch: 5661 mean train loss:  1.53370439e-02, mean val. rec. loss:  8.14973613e-03\n",
      "Epoch: 5662 mean train loss:  1.53350430e-02, mean val. rec. loss:  8.14821774e-03\n",
      "Epoch: 5663 mean train loss:  1.53330420e-02, mean val. rec. loss:  8.14670275e-03\n",
      "Epoch: 5664 mean train loss:  1.53310448e-02, mean val. rec. loss:  8.14518322e-03\n",
      "Epoch: 5665 mean train loss:  1.53290411e-02, mean val. rec. loss:  8.14367276e-03\n",
      "Epoch: 5666 mean train loss:  1.53270365e-02, mean val. rec. loss:  8.14216684e-03\n",
      "Epoch: 5667 mean train loss:  1.53250365e-02, mean val. rec. loss:  8.14066375e-03\n",
      "Epoch: 5668 mean train loss:  1.53230346e-02, mean val. rec. loss:  8.13915046e-03\n",
      "Epoch: 5669 mean train loss:  1.53210300e-02, mean val. rec. loss:  8.13763604e-03\n",
      "Epoch: 5670 mean train loss:  1.53190235e-02, mean val. rec. loss:  8.13612445e-03\n",
      "Epoch: 5671 mean train loss:  1.53170216e-02, mean val. rec. loss:  8.13460889e-03\n",
      "Epoch: 5672 mean train loss:  1.53150179e-02, mean val. rec. loss:  8.13310580e-03\n",
      "Epoch: 5673 mean train loss:  1.53130105e-02, mean val. rec. loss:  8.13158911e-03\n",
      "Epoch: 5674 mean train loss:  1.53110012e-02, mean val. rec. loss:  8.13007468e-03\n",
      "Epoch: 5675 mean train loss:  1.53089974e-02, mean val. rec. loss:  8.12856082e-03\n",
      "Epoch: 5676 mean train loss:  1.53069919e-02, mean val. rec. loss:  8.12704073e-03\n",
      "Epoch: 5677 mean train loss:  1.53049844e-02, mean val. rec. loss:  8.12553537e-03\n",
      "Epoch: 5678 mean train loss:  1.53029770e-02, mean val. rec. loss:  8.12402775e-03\n",
      "Epoch: 5679 mean train loss:  1.53009677e-02, mean val. rec. loss:  8.12252467e-03\n",
      "Epoch: 5680 mean train loss:  1.52989602e-02, mean val. rec. loss:  8.12100570e-03\n",
      "Epoch: 5681 mean train loss:  1.52969481e-02, mean val. rec. loss:  8.11949695e-03\n",
      "Epoch: 5682 mean train loss:  1.52949370e-02, mean val. rec. loss:  8.11797062e-03\n",
      "Epoch: 5683 mean train loss:  1.52929277e-02, mean val. rec. loss:  8.11643975e-03\n",
      "Epoch: 5684 mean train loss:  1.52909184e-02, mean val. rec. loss:  8.11493383e-03\n",
      "Epoch: 5685 mean train loss:  1.52889053e-02, mean val. rec. loss:  8.11343811e-03\n",
      "Epoch: 5686 mean train loss:  1.52868877e-02, mean val. rec. loss:  8.11192085e-03\n",
      "Epoch: 5687 mean train loss:  1.52848839e-02, mean val. rec. loss:  8.11042457e-03\n",
      "Epoch: 5688 mean train loss:  1.52828653e-02, mean val. rec. loss:  8.10890107e-03\n",
      "Epoch: 5689 mean train loss:  1.52808514e-02, mean val. rec. loss:  8.10737474e-03\n",
      "Epoch: 5690 mean train loss:  1.52788411e-02, mean val. rec. loss:  8.10585521e-03\n",
      "Epoch: 5691 mean train loss:  1.52768207e-02, mean val. rec. loss:  8.10433908e-03\n",
      "Epoch: 5692 mean train loss:  1.52748169e-02, mean val. rec. loss:  8.10282919e-03\n",
      "Epoch: 5693 mean train loss:  1.52727927e-02, mean val. rec. loss:  8.10131760e-03\n",
      "Epoch: 5694 mean train loss:  1.52707723e-02, mean val. rec. loss:  8.09982075e-03\n",
      "Epoch: 5695 mean train loss:  1.52687648e-02, mean val. rec. loss:  8.09829612e-03\n",
      "Epoch: 5696 mean train loss:  1.52667425e-02, mean val. rec. loss:  8.09677036e-03\n",
      "Epoch: 5697 mean train loss:  1.52647276e-02, mean val. rec. loss:  8.09525139e-03\n",
      "Epoch: 5698 mean train loss:  1.52627080e-02, mean val. rec. loss:  8.09372960e-03\n",
      "Epoch: 5699 mean train loss:  1.52606820e-02, mean val. rec. loss:  8.09223048e-03\n",
      "Epoch: 5700 mean train loss:  1.52586755e-02, mean val. rec. loss:  8.09072343e-03\n",
      "Epoch: 5701 mean train loss:  1.52566494e-02, mean val. rec. loss:  8.08921240e-03\n",
      "Epoch: 5702 mean train loss:  1.52546196e-02, mean val. rec. loss:  8.08767076e-03\n",
      "Epoch: 5703 mean train loss:  1.52526085e-02, mean val. rec. loss:  8.08613989e-03\n",
      "Epoch: 5704 mean train loss:  1.52505815e-02, mean val. rec. loss:  8.08463284e-03\n",
      "Epoch: 5705 mean train loss:  1.52485601e-02, mean val. rec. loss:  8.08310821e-03\n",
      "Epoch: 5706 mean train loss:  1.52465386e-02, mean val. rec. loss:  8.08160455e-03\n",
      "Epoch: 5707 mean train loss:  1.52445089e-02, mean val. rec. loss:  8.08007935e-03\n",
      "Epoch: 5708 mean train loss:  1.52424977e-02, mean val. rec. loss:  8.07856153e-03\n",
      "Epoch: 5709 mean train loss:  1.52404623e-02, mean val. rec. loss:  8.07703633e-03\n",
      "Epoch: 5710 mean train loss:  1.52384316e-02, mean val. rec. loss:  8.07551567e-03\n",
      "Epoch: 5711 mean train loss:  1.52364186e-02, mean val. rec. loss:  8.07400011e-03\n",
      "Epoch: 5712 mean train loss:  1.52343888e-02, mean val. rec. loss:  8.07249532e-03\n",
      "Epoch: 5713 mean train loss:  1.52323581e-02, mean val. rec. loss:  8.07097749e-03\n",
      "Epoch: 5714 mean train loss:  1.52303199e-02, mean val. rec. loss:  8.06945116e-03\n",
      "Epoch: 5715 mean train loss:  1.52283078e-02, mean val. rec. loss:  8.06791916e-03\n",
      "Epoch: 5716 mean train loss:  1.52262734e-02, mean val. rec. loss:  8.06640190e-03\n",
      "Epoch: 5717 mean train loss:  1.52242398e-02, mean val. rec. loss:  8.06489314e-03\n",
      "Epoch: 5718 mean train loss:  1.52222035e-02, mean val. rec. loss:  8.06337645e-03\n",
      "Epoch: 5719 mean train loss:  1.52201886e-02, mean val. rec. loss:  8.06186372e-03\n",
      "Epoch: 5720 mean train loss:  1.52181514e-02, mean val. rec. loss:  8.06034476e-03\n",
      "Epoch: 5721 mean train loss:  1.52161132e-02, mean val. rec. loss:  8.05881843e-03\n",
      "Epoch: 5722 mean train loss:  1.52140760e-02, mean val. rec. loss:  8.05727849e-03\n",
      "Epoch: 5723 mean train loss:  1.52120555e-02, mean val. rec. loss:  8.05575159e-03\n",
      "Epoch: 5724 mean train loss:  1.52100174e-02, mean val. rec. loss:  8.05421902e-03\n",
      "Epoch: 5725 mean train loss:  1.52079773e-02, mean val. rec. loss:  8.05271707e-03\n",
      "Epoch: 5726 mean train loss:  1.52059364e-02, mean val. rec. loss:  8.05120377e-03\n",
      "Epoch: 5727 mean train loss:  1.52039140e-02, mean val. rec. loss:  8.04968368e-03\n",
      "Epoch: 5728 mean train loss:  1.52018740e-02, mean val. rec. loss:  8.04815565e-03\n",
      "Epoch: 5729 mean train loss:  1.51998312e-02, mean val. rec. loss:  8.04662478e-03\n",
      "Epoch: 5730 mean train loss:  1.51977893e-02, mean val. rec. loss:  8.04509221e-03\n",
      "Epoch: 5731 mean train loss:  1.51957632e-02, mean val. rec. loss:  8.04355567e-03\n",
      "Epoch: 5732 mean train loss:  1.51937204e-02, mean val. rec. loss:  8.04202367e-03\n",
      "Epoch: 5733 mean train loss:  1.51916739e-02, mean val. rec. loss:  8.04050244e-03\n",
      "Epoch: 5734 mean train loss:  1.51896282e-02, mean val. rec. loss:  8.03898971e-03\n",
      "Epoch: 5735 mean train loss:  1.51875919e-02, mean val. rec. loss:  8.03746848e-03\n",
      "Epoch: 5736 mean train loss:  1.51855575e-02, mean val. rec. loss:  8.03595236e-03\n",
      "Epoch: 5737 mean train loss:  1.51835119e-02, mean val. rec. loss:  8.03442772e-03\n",
      "Epoch: 5738 mean train loss:  1.51814663e-02, mean val. rec. loss:  8.03289402e-03\n",
      "Epoch: 5739 mean train loss:  1.51794197e-02, mean val. rec. loss:  8.03135578e-03\n",
      "Epoch: 5740 mean train loss:  1.51773722e-02, mean val. rec. loss:  8.02982945e-03\n",
      "Epoch: 5741 mean train loss:  1.51753238e-02, mean val. rec. loss:  8.02829801e-03\n",
      "Epoch: 5742 mean train loss:  1.51732763e-02, mean val. rec. loss:  8.02676828e-03\n",
      "Epoch: 5743 mean train loss:  1.51712382e-02, mean val. rec. loss:  8.02524592e-03\n",
      "Epoch: 5744 mean train loss:  1.51691954e-02, mean val. rec. loss:  8.02373319e-03\n",
      "Epoch: 5745 mean train loss:  1.51671469e-02, mean val. rec. loss:  8.02221083e-03\n",
      "Epoch: 5746 mean train loss:  1.51650948e-02, mean val. rec. loss:  8.02067712e-03\n",
      "Epoch: 5747 mean train loss:  1.51630399e-02, mean val. rec. loss:  8.01913378e-03\n",
      "Epoch: 5748 mean train loss:  1.51609877e-02, mean val. rec. loss:  8.01759157e-03\n",
      "Epoch: 5749 mean train loss:  1.51589347e-02, mean val. rec. loss:  8.01605560e-03\n",
      "Epoch: 5750 mean train loss:  1.51568770e-02, mean val. rec. loss:  8.01453607e-03\n",
      "Epoch: 5751 mean train loss:  1.51548304e-02, mean val. rec. loss:  8.01300180e-03\n",
      "Epoch: 5752 mean train loss:  1.51527857e-02, mean val. rec. loss:  8.01148114e-03\n",
      "Epoch: 5753 mean train loss:  1.51507280e-02, mean val. rec. loss:  8.00995651e-03\n",
      "Epoch: 5754 mean train loss:  1.51486703e-02, mean val. rec. loss:  8.00842054e-03\n",
      "Epoch: 5755 mean train loss:  1.51466144e-02, mean val. rec. loss:  8.00688570e-03\n",
      "Epoch: 5756 mean train loss:  1.51445548e-02, mean val. rec. loss:  8.00534179e-03\n",
      "Epoch: 5757 mean train loss:  1.51424953e-02, mean val. rec. loss:  8.00381036e-03\n",
      "Epoch: 5758 mean train loss:  1.51404375e-02, mean val. rec. loss:  8.00228176e-03\n",
      "Epoch: 5759 mean train loss:  1.51383798e-02, mean val. rec. loss:  8.00075032e-03\n",
      "Epoch: 5760 mean train loss:  1.51363184e-02, mean val. rec. loss:  7.99922285e-03\n",
      "Epoch: 5761 mean train loss:  1.51342597e-02, mean val. rec. loss:  7.99768745e-03\n",
      "Epoch: 5762 mean train loss:  1.51321964e-02, mean val. rec. loss:  7.99615205e-03\n",
      "Epoch: 5763 mean train loss:  1.51301350e-02, mean val. rec. loss:  7.99461381e-03\n",
      "Epoch: 5764 mean train loss:  1.51280735e-02, mean val. rec. loss:  7.99307840e-03\n",
      "Epoch: 5765 mean train loss:  1.51260093e-02, mean val. rec. loss:  7.99153619e-03\n",
      "Epoch: 5766 mean train loss:  1.51239460e-02, mean val. rec. loss:  7.99000476e-03\n",
      "Epoch: 5767 mean train loss:  1.51218808e-02, mean val. rec. loss:  7.98845688e-03\n",
      "Epoch: 5768 mean train loss:  1.51198138e-02, mean val. rec. loss:  7.98691978e-03\n",
      "Epoch: 5769 mean train loss:  1.51177486e-02, mean val. rec. loss:  7.98538777e-03\n",
      "Epoch: 5770 mean train loss:  1.51156788e-02, mean val. rec. loss:  7.98385180e-03\n",
      "Epoch: 5771 mean train loss:  1.51136118e-02, mean val. rec. loss:  7.98231866e-03\n",
      "Epoch: 5772 mean train loss:  1.51115419e-02, mean val. rec. loss:  7.98077079e-03\n",
      "Epoch: 5773 mean train loss:  1.51094712e-02, mean val. rec. loss:  7.97923028e-03\n",
      "Epoch: 5774 mean train loss:  1.51074041e-02, mean val. rec. loss:  7.97767787e-03\n",
      "Epoch: 5775 mean train loss:  1.51053325e-02, mean val. rec. loss:  7.97613452e-03\n",
      "Epoch: 5776 mean train loss:  1.51032580e-02, mean val. rec. loss:  7.97460422e-03\n",
      "Epoch: 5777 mean train loss:  1.51011872e-02, mean val. rec. loss:  7.97307222e-03\n",
      "Epoch: 5778 mean train loss:  1.50991155e-02, mean val. rec. loss:  7.97153795e-03\n",
      "Epoch: 5779 mean train loss:  1.50970439e-02, mean val. rec. loss:  7.96999688e-03\n",
      "Epoch: 5780 mean train loss:  1.50949703e-02, mean val. rec. loss:  7.96845920e-03\n",
      "Epoch: 5781 mean train loss:  1.50928968e-02, mean val. rec. loss:  7.96691926e-03\n",
      "Epoch: 5782 mean train loss:  1.50908232e-02, mean val. rec. loss:  7.96536798e-03\n",
      "Epoch: 5783 mean train loss:  1.50887338e-02, mean val. rec. loss:  7.96382351e-03\n",
      "Epoch: 5784 mean train loss:  1.50866519e-02, mean val. rec. loss:  7.96227336e-03\n",
      "Epoch: 5785 mean train loss:  1.50845737e-02, mean val. rec. loss:  7.96073002e-03\n",
      "Epoch: 5786 mean train loss:  1.50824973e-02, mean val. rec. loss:  7.95919802e-03\n",
      "Epoch: 5787 mean train loss:  1.50804164e-02, mean val. rec. loss:  7.95766715e-03\n",
      "Epoch: 5788 mean train loss:  1.50783372e-02, mean val. rec. loss:  7.95612891e-03\n",
      "Epoch: 5789 mean train loss:  1.50762571e-02, mean val. rec. loss:  7.95458840e-03\n",
      "Epoch: 5790 mean train loss:  1.50741780e-02, mean val. rec. loss:  7.95302011e-03\n",
      "Epoch: 5791 mean train loss:  1.50720849e-02, mean val. rec. loss:  7.95145636e-03\n",
      "Epoch: 5792 mean train loss:  1.50699927e-02, mean val. rec. loss:  7.94988920e-03\n",
      "Epoch: 5793 mean train loss:  1.50679080e-02, mean val. rec. loss:  7.94835040e-03\n",
      "Epoch: 5794 mean train loss:  1.50658233e-02, mean val. rec. loss:  7.94681272e-03\n",
      "Epoch: 5795 mean train loss:  1.50637395e-02, mean val. rec. loss:  7.94527675e-03\n",
      "Epoch: 5796 mean train loss:  1.50616529e-02, mean val. rec. loss:  7.94373228e-03\n",
      "Epoch: 5797 mean train loss:  1.50595635e-02, mean val. rec. loss:  7.94218667e-03\n",
      "Epoch: 5798 mean train loss:  1.50574770e-02, mean val. rec. loss:  7.94061894e-03\n",
      "Epoch: 5799 mean train loss:  1.50553764e-02, mean val. rec. loss:  7.93905746e-03\n",
      "Epoch: 5800 mean train loss:  1.50532824e-02, mean val. rec. loss:  7.93750448e-03\n",
      "Epoch: 5801 mean train loss:  1.50511930e-02, mean val. rec. loss:  7.93596170e-03\n",
      "Epoch: 5802 mean train loss:  1.50491036e-02, mean val. rec. loss:  7.93442290e-03\n",
      "Epoch: 5803 mean train loss:  1.50470059e-02, mean val. rec. loss:  7.93287842e-03\n",
      "Epoch: 5804 mean train loss:  1.50449044e-02, mean val. rec. loss:  7.93132941e-03\n",
      "Epoch: 5805 mean train loss:  1.50428131e-02, mean val. rec. loss:  7.92975772e-03\n",
      "Epoch: 5806 mean train loss:  1.50407191e-02, mean val. rec. loss:  7.92821494e-03\n",
      "Epoch: 5807 mean train loss:  1.50386139e-02, mean val. rec. loss:  7.92666366e-03\n",
      "Epoch: 5808 mean train loss:  1.50365106e-02, mean val. rec. loss:  7.92510955e-03\n",
      "Epoch: 5809 mean train loss:  1.50344156e-02, mean val. rec. loss:  7.92356394e-03\n",
      "Epoch: 5810 mean train loss:  1.50323197e-02, mean val. rec. loss:  7.92200018e-03\n",
      "Epoch: 5811 mean train loss:  1.50302145e-02, mean val. rec. loss:  7.92045571e-03\n",
      "Epoch: 5812 mean train loss:  1.50281046e-02, mean val. rec. loss:  7.91889252e-03\n",
      "Epoch: 5813 mean train loss:  1.50260059e-02, mean val. rec. loss:  7.91733444e-03\n",
      "Epoch: 5814 mean train loss:  1.50239072e-02, mean val. rec. loss:  7.91577125e-03\n",
      "Epoch: 5815 mean train loss:  1.50217937e-02, mean val. rec. loss:  7.91420239e-03\n",
      "Epoch: 5816 mean train loss:  1.50196838e-02, mean val. rec. loss:  7.91266188e-03\n",
      "Epoch: 5817 mean train loss:  1.50175814e-02, mean val. rec. loss:  7.91110040e-03\n",
      "Epoch: 5818 mean train loss:  1.50154753e-02, mean val. rec. loss:  7.90953211e-03\n",
      "Epoch: 5819 mean train loss:  1.50133635e-02, mean val. rec. loss:  7.90797913e-03\n",
      "Epoch: 5820 mean train loss:  1.50112499e-02, mean val. rec. loss:  7.90642955e-03\n",
      "Epoch: 5821 mean train loss:  1.50091457e-02, mean val. rec. loss:  7.90486013e-03\n",
      "Epoch: 5822 mean train loss:  1.50070228e-02, mean val. rec. loss:  7.90329410e-03\n",
      "Epoch: 5823 mean train loss:  1.50049148e-02, mean val. rec. loss:  7.90174112e-03\n",
      "Epoch: 5824 mean train loss:  1.50028068e-02, mean val. rec. loss:  7.90017680e-03\n",
      "Epoch: 5825 mean train loss:  1.50006801e-02, mean val. rec. loss:  7.89860795e-03\n",
      "Epoch: 5826 mean train loss:  1.49985721e-02, mean val. rec. loss:  7.89706347e-03\n",
      "Epoch: 5827 mean train loss:  1.49964530e-02, mean val. rec. loss:  7.89550425e-03\n",
      "Epoch: 5828 mean train loss:  1.49943338e-02, mean val. rec. loss:  7.89393540e-03\n",
      "Epoch: 5829 mean train loss:  1.49922211e-02, mean val. rec. loss:  7.89235577e-03\n",
      "Epoch: 5830 mean train loss:  1.49900899e-02, mean val. rec. loss:  7.89078974e-03\n",
      "Epoch: 5831 mean train loss:  1.49879781e-02, mean val. rec. loss:  7.88923109e-03\n",
      "Epoch: 5832 mean train loss:  1.49858627e-02, mean val. rec. loss:  7.88767074e-03\n",
      "Epoch: 5833 mean train loss:  1.49837277e-02, mean val. rec. loss:  7.88610529e-03\n",
      "Epoch: 5834 mean train loss:  1.49816095e-02, mean val. rec. loss:  7.88452112e-03\n",
      "Epoch: 5835 mean train loss:  1.49794847e-02, mean val. rec. loss:  7.88296701e-03\n",
      "Epoch: 5836 mean train loss:  1.49773553e-02, mean val. rec. loss:  7.88139985e-03\n",
      "Epoch: 5837 mean train loss:  1.49752370e-02, mean val. rec. loss:  7.87982816e-03\n",
      "Epoch: 5838 mean train loss:  1.49730974e-02, mean val. rec. loss:  7.87826384e-03\n",
      "Epoch: 5839 mean train loss:  1.49709726e-02, mean val. rec. loss:  7.87669838e-03\n",
      "Epoch: 5840 mean train loss:  1.49688423e-02, mean val. rec. loss:  7.87512386e-03\n",
      "Epoch: 5841 mean train loss:  1.49667138e-02, mean val. rec. loss:  7.87354763e-03\n",
      "Epoch: 5842 mean train loss:  1.49645853e-02, mean val. rec. loss:  7.87199011e-03\n",
      "Epoch: 5843 mean train loss:  1.49624531e-02, mean val. rec. loss:  7.87041445e-03\n",
      "Epoch: 5844 mean train loss:  1.49603172e-02, mean val. rec. loss:  7.86885013e-03\n",
      "Epoch: 5845 mean train loss:  1.49581831e-02, mean val. rec. loss:  7.86728241e-03\n",
      "Epoch: 5846 mean train loss:  1.49560481e-02, mean val. rec. loss:  7.86570958e-03\n",
      "Epoch: 5847 mean train loss:  1.49539112e-02, mean val. rec. loss:  7.86413676e-03\n",
      "Epoch: 5848 mean train loss:  1.49517734e-02, mean val. rec. loss:  7.86254806e-03\n",
      "Epoch: 5849 mean train loss:  1.49496356e-02, mean val. rec. loss:  7.86097240e-03\n",
      "Epoch: 5850 mean train loss:  1.49474978e-02, mean val. rec. loss:  7.85939900e-03\n",
      "Epoch: 5851 mean train loss:  1.49453563e-02, mean val. rec. loss:  7.85782845e-03\n",
      "Epoch: 5852 mean train loss:  1.49432111e-02, mean val. rec. loss:  7.85625222e-03\n",
      "Epoch: 5853 mean train loss:  1.49410705e-02, mean val. rec. loss:  7.85468280e-03\n",
      "Epoch: 5854 mean train loss:  1.49389290e-02, mean val. rec. loss:  7.85310543e-03\n",
      "Epoch: 5855 mean train loss:  1.49367828e-02, mean val. rec. loss:  7.85152807e-03\n",
      "Epoch: 5856 mean train loss:  1.49346348e-02, mean val. rec. loss:  7.84993484e-03\n",
      "Epoch: 5857 mean train loss:  1.49324905e-02, mean val. rec. loss:  7.84834217e-03\n",
      "Epoch: 5858 mean train loss:  1.49303452e-02, mean val. rec. loss:  7.84676991e-03\n",
      "Epoch: 5859 mean train loss:  1.49281944e-02, mean val. rec. loss:  7.84520162e-03\n",
      "Epoch: 5860 mean train loss:  1.49260380e-02, mean val. rec. loss:  7.84362879e-03\n",
      "Epoch: 5861 mean train loss:  1.49238983e-02, mean val. rec. loss:  7.84205030e-03\n",
      "Epoch: 5862 mean train loss:  1.49217419e-02, mean val. rec. loss:  7.84047293e-03\n",
      "Epoch: 5863 mean train loss:  1.49195799e-02, mean val. rec. loss:  7.83888310e-03\n",
      "Epoch: 5864 mean train loss:  1.49174384e-02, mean val. rec. loss:  7.83728816e-03\n",
      "Epoch: 5865 mean train loss:  1.49152764e-02, mean val. rec. loss:  7.83571534e-03\n",
      "Epoch: 5866 mean train loss:  1.49131199e-02, mean val. rec. loss:  7.83413854e-03\n",
      "Epoch: 5867 mean train loss:  1.49109663e-02, mean val. rec. loss:  7.83256231e-03\n",
      "Epoch: 5868 mean train loss:  1.49088034e-02, mean val. rec. loss:  7.83095944e-03\n",
      "Epoch: 5869 mean train loss:  1.49066535e-02, mean val. rec. loss:  7.82936563e-03\n",
      "Epoch: 5870 mean train loss:  1.49044859e-02, mean val. rec. loss:  7.82779961e-03\n",
      "Epoch: 5871 mean train loss:  1.49023174e-02, mean val. rec. loss:  7.82621771e-03\n",
      "Epoch: 5872 mean train loss:  1.49001675e-02, mean val. rec. loss:  7.82463298e-03\n",
      "Epoch: 5873 mean train loss:  1.48979980e-02, mean val. rec. loss:  7.82303861e-03\n",
      "Epoch: 5874 mean train loss:  1.48958370e-02, mean val. rec. loss:  7.82143687e-03\n",
      "Epoch: 5875 mean train loss:  1.48936722e-02, mean val. rec. loss:  7.81984250e-03\n",
      "Epoch: 5876 mean train loss:  1.48914962e-02, mean val. rec. loss:  7.81826854e-03\n",
      "Epoch: 5877 mean train loss:  1.48893407e-02, mean val. rec. loss:  7.81669912e-03\n",
      "Epoch: 5878 mean train loss:  1.48871666e-02, mean val. rec. loss:  7.81510361e-03\n",
      "Epoch: 5879 mean train loss:  1.48849916e-02, mean val. rec. loss:  7.81351264e-03\n",
      "Epoch: 5880 mean train loss:  1.48828165e-02, mean val. rec. loss:  7.81190296e-03\n",
      "Epoch: 5881 mean train loss:  1.48806582e-02, mean val. rec. loss:  7.81030349e-03\n",
      "Epoch: 5882 mean train loss:  1.48784795e-02, mean val. rec. loss:  7.80872726e-03\n",
      "Epoch: 5883 mean train loss:  1.48763026e-02, mean val. rec. loss:  7.80714990e-03\n",
      "Epoch: 5884 mean train loss:  1.48741220e-02, mean val. rec. loss:  7.80557254e-03\n",
      "Epoch: 5885 mean train loss:  1.48719581e-02, mean val. rec. loss:  7.80396909e-03\n",
      "Epoch: 5886 mean train loss:  1.48697775e-02, mean val. rec. loss:  7.80235318e-03\n",
      "Epoch: 5887 mean train loss:  1.48675931e-02, mean val. rec. loss:  7.80074803e-03\n",
      "Epoch: 5888 mean train loss:  1.48654078e-02, mean val. rec. loss:  7.79915310e-03\n",
      "Epoch: 5889 mean train loss:  1.48632402e-02, mean val. rec. loss:  7.79757687e-03\n",
      "Epoch: 5890 mean train loss:  1.48610531e-02, mean val. rec. loss:  7.79599384e-03\n",
      "Epoch: 5891 mean train loss:  1.48588669e-02, mean val. rec. loss:  7.79439209e-03\n",
      "Epoch: 5892 mean train loss:  1.48566760e-02, mean val. rec. loss:  7.79277448e-03\n",
      "Epoch: 5893 mean train loss:  1.48545056e-02, mean val. rec. loss:  7.79116877e-03\n",
      "Epoch: 5894 mean train loss:  1.48523129e-02, mean val. rec. loss:  7.78956702e-03\n",
      "Epoch: 5895 mean train loss:  1.48501230e-02, mean val. rec. loss:  7.78798399e-03\n",
      "Epoch: 5896 mean train loss:  1.48479302e-02, mean val. rec. loss:  7.78639529e-03\n",
      "Epoch: 5897 mean train loss:  1.48457524e-02, mean val. rec. loss:  7.78478618e-03\n",
      "Epoch: 5898 mean train loss:  1.48435597e-02, mean val. rec. loss:  7.78319408e-03\n",
      "Epoch: 5899 mean train loss:  1.48413623e-02, mean val. rec. loss:  7.78158893e-03\n",
      "Epoch: 5900 mean train loss:  1.48391686e-02, mean val. rec. loss:  7.77998946e-03\n",
      "Epoch: 5901 mean train loss:  1.48369713e-02, mean val. rec. loss:  7.77838375e-03\n",
      "Epoch: 5902 mean train loss:  1.48347729e-02, mean val. rec. loss:  7.77678427e-03\n",
      "Epoch: 5903 mean train loss:  1.48325756e-02, mean val. rec. loss:  7.77518253e-03\n",
      "Epoch: 5904 mean train loss:  1.48303810e-02, mean val. rec. loss:  7.77358476e-03\n",
      "Epoch: 5905 mean train loss:  1.48281901e-02, mean val. rec. loss:  7.77197224e-03\n",
      "Epoch: 5906 mean train loss:  1.48259871e-02, mean val. rec. loss:  7.77037390e-03\n",
      "Epoch: 5907 mean train loss:  1.48237851e-02, mean val. rec. loss:  7.76878010e-03\n",
      "Epoch: 5908 mean train loss:  1.48215784e-02, mean val. rec. loss:  7.76716362e-03\n",
      "Epoch: 5909 mean train loss:  1.48193754e-02, mean val. rec. loss:  7.76554997e-03\n",
      "Epoch: 5910 mean train loss:  1.48171669e-02, mean val. rec. loss:  7.76393122e-03\n",
      "Epoch: 5911 mean train loss:  1.48149583e-02, mean val. rec. loss:  7.76230793e-03\n",
      "Epoch: 5912 mean train loss:  1.48127591e-02, mean val. rec. loss:  7.76071582e-03\n",
      "Epoch: 5913 mean train loss:  1.48105561e-02, mean val. rec. loss:  7.75912372e-03\n",
      "Epoch: 5914 mean train loss:  1.48083457e-02, mean val. rec. loss:  7.75751688e-03\n",
      "Epoch: 5915 mean train loss:  1.48061315e-02, mean val. rec. loss:  7.75591117e-03\n",
      "Epoch: 5916 mean train loss:  1.48039174e-02, mean val. rec. loss:  7.75428958e-03\n",
      "Epoch: 5917 mean train loss:  1.48017032e-02, mean val. rec. loss:  7.75266402e-03\n",
      "Epoch: 5918 mean train loss:  1.47994854e-02, mean val. rec. loss:  7.75105378e-03\n",
      "Epoch: 5919 mean train loss:  1.47972694e-02, mean val. rec. loss:  7.74944296e-03\n",
      "Epoch: 5920 mean train loss:  1.47950533e-02, mean val. rec. loss:  7.74783272e-03\n",
      "Epoch: 5921 mean train loss:  1.47928355e-02, mean val. rec. loss:  7.74622134e-03\n",
      "Epoch: 5922 mean train loss:  1.47906157e-02, mean val. rec. loss:  7.74461563e-03\n",
      "Epoch: 5923 mean train loss:  1.47883979e-02, mean val. rec. loss:  7.74300708e-03\n",
      "Epoch: 5924 mean train loss:  1.47861754e-02, mean val. rec. loss:  7.74138209e-03\n",
      "Epoch: 5925 mean train loss:  1.47839528e-02, mean val. rec. loss:  7.73978205e-03\n",
      "Epoch: 5926 mean train loss:  1.47817284e-02, mean val. rec. loss:  7.73815309e-03\n",
      "Epoch: 5927 mean train loss:  1.47795059e-02, mean val. rec. loss:  7.73652584e-03\n",
      "Epoch: 5928 mean train loss:  1.47772787e-02, mean val. rec. loss:  7.73490198e-03\n",
      "Epoch: 5929 mean train loss:  1.47750497e-02, mean val. rec. loss:  7.73329514e-03\n",
      "Epoch: 5930 mean train loss:  1.47728234e-02, mean val. rec. loss:  7.73169736e-03\n",
      "Epoch: 5931 mean train loss:  1.47705916e-02, mean val. rec. loss:  7.73007918e-03\n",
      "Epoch: 5932 mean train loss:  1.47683626e-02, mean val. rec. loss:  7.72843605e-03\n",
      "Epoch: 5933 mean train loss:  1.47661307e-02, mean val. rec. loss:  7.72681389e-03\n",
      "Epoch: 5934 mean train loss:  1.47638970e-02, mean val. rec. loss:  7.72517473e-03\n",
      "Epoch: 5935 mean train loss:  1.47616624e-02, mean val. rec. loss:  7.72356278e-03\n",
      "Epoch: 5936 mean train loss:  1.47594278e-02, mean val. rec. loss:  7.72194233e-03\n",
      "Epoch: 5937 mean train loss:  1.47571894e-02, mean val. rec. loss:  7.72031564e-03\n",
      "Epoch: 5938 mean train loss:  1.47549511e-02, mean val. rec. loss:  7.71867818e-03\n",
      "Epoch: 5939 mean train loss:  1.47527127e-02, mean val. rec. loss:  7.71705546e-03\n",
      "Epoch: 5940 mean train loss:  1.47504744e-02, mean val. rec. loss:  7.71543387e-03\n",
      "Epoch: 5941 mean train loss:  1.47482342e-02, mean val. rec. loss:  7.71380662e-03\n",
      "Epoch: 5942 mean train loss:  1.47459939e-02, mean val. rec. loss:  7.71218276e-03\n",
      "Epoch: 5943 mean train loss:  1.47437537e-02, mean val. rec. loss:  7.71056004e-03\n",
      "Epoch: 5944 mean train loss:  1.47415079e-02, mean val. rec. loss:  7.70892768e-03\n",
      "Epoch: 5945 mean train loss:  1.47392565e-02, mean val. rec. loss:  7.70730269e-03\n",
      "Epoch: 5946 mean train loss:  1.47369986e-02, mean val. rec. loss:  7.70566069e-03\n",
      "Epoch: 5947 mean train loss:  1.47347519e-02, mean val. rec. loss:  7.70403684e-03\n",
      "Epoch: 5948 mean train loss:  1.47325061e-02, mean val. rec. loss:  7.70239201e-03\n",
      "Epoch: 5949 mean train loss:  1.47302547e-02, mean val. rec. loss:  7.70077382e-03\n",
      "Epoch: 5950 mean train loss:  1.47280052e-02, mean val. rec. loss:  7.69915337e-03\n",
      "Epoch: 5951 mean train loss:  1.47257519e-02, mean val. rec. loss:  7.69751874e-03\n",
      "Epoch: 5952 mean train loss:  1.47235015e-02, mean val. rec. loss:  7.69589092e-03\n",
      "Epoch: 5953 mean train loss:  1.47212361e-02, mean val. rec. loss:  7.69424835e-03\n",
      "Epoch: 5954 mean train loss:  1.47189717e-02, mean val. rec. loss:  7.69260295e-03\n",
      "Epoch: 5955 mean train loss:  1.47167147e-02, mean val. rec. loss:  7.69096266e-03\n",
      "Epoch: 5956 mean train loss:  1.47144577e-02, mean val. rec. loss:  7.68933086e-03\n",
      "Epoch: 5957 mean train loss:  1.47121970e-02, mean val. rec. loss:  7.68769907e-03\n",
      "Epoch: 5958 mean train loss:  1.47099382e-02, mean val. rec. loss:  7.68605934e-03\n",
      "Epoch: 5959 mean train loss:  1.47076766e-02, mean val. rec. loss:  7.68443095e-03\n",
      "Epoch: 5960 mean train loss:  1.47054000e-02, mean val. rec. loss:  7.68279122e-03\n",
      "Epoch: 5961 mean train loss:  1.47031375e-02, mean val. rec. loss:  7.68116113e-03\n",
      "Epoch: 5962 mean train loss:  1.47008712e-02, mean val. rec. loss:  7.67950439e-03\n",
      "Epoch: 5963 mean train loss:  1.46986086e-02, mean val. rec. loss:  7.67786579e-03\n",
      "Epoch: 5964 mean train loss:  1.46963256e-02, mean val. rec. loss:  7.67621813e-03\n",
      "Epoch: 5965 mean train loss:  1.46940574e-02, mean val. rec. loss:  7.67459257e-03\n",
      "Epoch: 5966 mean train loss:  1.46917902e-02, mean val. rec. loss:  7.67295284e-03\n",
      "Epoch: 5967 mean train loss:  1.46895202e-02, mean val. rec. loss:  7.67131538e-03\n",
      "Epoch: 5968 mean train loss:  1.46872307e-02, mean val. rec. loss:  7.66967338e-03\n",
      "Epoch: 5969 mean train loss:  1.46849569e-02, mean val. rec. loss:  7.66802231e-03\n",
      "Epoch: 5970 mean train loss:  1.46826850e-02, mean val. rec. loss:  7.66637237e-03\n",
      "Epoch: 5971 mean train loss:  1.46804113e-02, mean val. rec. loss:  7.66472697e-03\n",
      "Epoch: 5972 mean train loss:  1.46781152e-02, mean val. rec. loss:  7.66309461e-03\n",
      "Epoch: 5973 mean train loss:  1.46758396e-02, mean val. rec. loss:  7.66145092e-03\n",
      "Epoch: 5974 mean train loss:  1.46735603e-02, mean val. rec. loss:  7.65979644e-03\n",
      "Epoch: 5975 mean train loss:  1.46712810e-02, mean val. rec. loss:  7.65814311e-03\n",
      "Epoch: 5976 mean train loss:  1.46689812e-02, mean val. rec. loss:  7.65649544e-03\n",
      "Epoch: 5977 mean train loss:  1.46667000e-02, mean val. rec. loss:  7.65485911e-03\n",
      "Epoch: 5978 mean train loss:  1.46644179e-02, mean val. rec. loss:  7.65321825e-03\n",
      "Epoch: 5979 mean train loss:  1.46621181e-02, mean val. rec. loss:  7.65155754e-03\n",
      "Epoch: 5980 mean train loss:  1.46598341e-02, mean val. rec. loss:  7.64990250e-03\n",
      "Epoch: 5981 mean train loss:  1.46575501e-02, mean val. rec. loss:  7.64826617e-03\n",
      "Epoch: 5982 mean train loss:  1.46552438e-02, mean val. rec. loss:  7.64662644e-03\n",
      "Epoch: 5983 mean train loss:  1.46529570e-02, mean val. rec. loss:  7.64497424e-03\n",
      "Epoch: 5984 mean train loss:  1.46506572e-02, mean val. rec. loss:  7.64333054e-03\n",
      "Epoch: 5985 mean train loss:  1.46483584e-02, mean val. rec. loss:  7.64168117e-03\n",
      "Epoch: 5986 mean train loss:  1.46460707e-02, mean val. rec. loss:  7.64001989e-03\n",
      "Epoch: 5987 mean train loss:  1.46437560e-02, mean val. rec. loss:  7.63836088e-03\n",
      "Epoch: 5988 mean train loss:  1.46414636e-02, mean val. rec. loss:  7.63670414e-03\n",
      "Epoch: 5989 mean train loss:  1.46391675e-02, mean val. rec. loss:  7.63505648e-03\n",
      "Epoch: 5990 mean train loss:  1.46368528e-02, mean val. rec. loss:  7.63340371e-03\n",
      "Epoch: 5991 mean train loss:  1.46345539e-02, mean val. rec. loss:  7.63175660e-03\n",
      "Epoch: 5992 mean train loss:  1.46322476e-02, mean val. rec. loss:  7.63009419e-03\n",
      "Epoch: 5993 mean train loss:  1.46299366e-02, mean val. rec. loss:  7.62843859e-03\n",
      "Epoch: 5994 mean train loss:  1.46276350e-02, mean val. rec. loss:  7.62677675e-03\n",
      "Epoch: 5995 mean train loss:  1.46253128e-02, mean val. rec. loss:  7.62510753e-03\n",
      "Epoch: 5996 mean train loss:  1.46230093e-02, mean val. rec. loss:  7.62344342e-03\n",
      "Epoch: 5997 mean train loss:  1.46207048e-02, mean val. rec. loss:  7.62180029e-03\n",
      "Epoch: 5998 mean train loss:  1.46183780e-02, mean val. rec. loss:  7.62015035e-03\n",
      "Epoch: 5999 mean train loss:  1.46160708e-02, mean val. rec. loss:  7.61849361e-03\n",
      "Epoch: 6000 mean train loss:  1.46137468e-02, mean val. rec. loss:  7.61682497e-03\n",
      "Epoch: 6001 mean train loss:  1.46114376e-02, mean val. rec. loss:  7.61515292e-03\n",
      "Epoch: 6002 mean train loss:  1.46091090e-02, mean val. rec. loss:  7.61349958e-03\n",
      "Epoch: 6003 mean train loss:  1.46067989e-02, mean val. rec. loss:  7.61183944e-03\n",
      "Epoch: 6004 mean train loss:  1.46044656e-02, mean val. rec. loss:  7.61018270e-03\n",
      "Epoch: 6005 mean train loss:  1.46021537e-02, mean val. rec. loss:  7.60852256e-03\n",
      "Epoch: 6006 mean train loss:  1.45998204e-02, mean val. rec. loss:  7.60685108e-03\n",
      "Epoch: 6007 mean train loss:  1.45975029e-02, mean val. rec. loss:  7.60518640e-03\n",
      "Epoch: 6008 mean train loss:  1.45951658e-02, mean val. rec. loss:  7.60351832e-03\n",
      "Epoch: 6009 mean train loss:  1.45928474e-02, mean val. rec. loss:  7.60185421e-03\n",
      "Epoch: 6010 mean train loss:  1.45905085e-02, mean val. rec. loss:  7.60019180e-03\n",
      "Epoch: 6011 mean train loss:  1.45881873e-02, mean val. rec. loss:  7.59852315e-03\n",
      "Epoch: 6012 mean train loss:  1.45858474e-02, mean val. rec. loss:  7.59684430e-03\n",
      "Epoch: 6013 mean train loss:  1.45835215e-02, mean val. rec. loss:  7.59517112e-03\n",
      "Epoch: 6014 mean train loss:  1.45811770e-02, mean val. rec. loss:  7.59350984e-03\n",
      "Epoch: 6015 mean train loss:  1.45788512e-02, mean val. rec. loss:  7.59185083e-03\n",
      "Epoch: 6016 mean train loss:  1.45765039e-02, mean val. rec. loss:  7.59018275e-03\n",
      "Epoch: 6017 mean train loss:  1.45741743e-02, mean val. rec. loss:  7.58850220e-03\n",
      "Epoch: 6018 mean train loss:  1.45718251e-02, mean val. rec. loss:  7.58683185e-03\n",
      "Epoch: 6019 mean train loss:  1.45694927e-02, mean val. rec. loss:  7.58515981e-03\n",
      "Epoch: 6020 mean train loss:  1.45671417e-02, mean val. rec. loss:  7.58348606e-03\n",
      "Epoch: 6021 mean train loss:  1.45648075e-02, mean val. rec. loss:  7.58181174e-03\n",
      "Epoch: 6022 mean train loss:  1.45624536e-02, mean val. rec. loss:  7.58014763e-03\n",
      "Epoch: 6023 mean train loss:  1.45601194e-02, mean val. rec. loss:  7.57847898e-03\n",
      "Epoch: 6024 mean train loss:  1.45577647e-02, mean val. rec. loss:  7.57680467e-03\n",
      "Epoch: 6025 mean train loss:  1.45554080e-02, mean val. rec. loss:  7.57511901e-03\n",
      "Epoch: 6026 mean train loss:  1.45530701e-02, mean val. rec. loss:  7.57345206e-03\n",
      "Epoch: 6027 mean train loss:  1.45507116e-02, mean val. rec. loss:  7.57176811e-03\n",
      "Epoch: 6028 mean train loss:  1.45483615e-02, mean val. rec. loss:  7.57009946e-03\n",
      "Epoch: 6029 mean train loss:  1.45460077e-02, mean val. rec. loss:  7.56841664e-03\n",
      "Epoch: 6030 mean train loss:  1.45436446e-02, mean val. rec. loss:  7.56674686e-03\n",
      "Epoch: 6031 mean train loss:  1.45412982e-02, mean val. rec. loss:  7.56505950e-03\n",
      "Epoch: 6032 mean train loss:  1.45389332e-02, mean val. rec. loss:  7.56338235e-03\n",
      "Epoch: 6033 mean train loss:  1.45365664e-02, mean val. rec. loss:  7.56170974e-03\n",
      "Epoch: 6034 mean train loss:  1.45342154e-02, mean val. rec. loss:  7.56002351e-03\n",
      "Epoch: 6035 mean train loss:  1.45318476e-02, mean val. rec. loss:  7.55834466e-03\n",
      "Epoch: 6036 mean train loss:  1.45294845e-02, mean val. rec. loss:  7.55665617e-03\n",
      "Epoch: 6037 mean train loss:  1.45271251e-02, mean val. rec. loss:  7.55497221e-03\n",
      "Epoch: 6038 mean train loss:  1.45247499e-02, mean val. rec. loss:  7.55328883e-03\n",
      "Epoch: 6039 mean train loss:  1.45223951e-02, mean val. rec. loss:  7.55161678e-03\n",
      "Epoch: 6040 mean train loss:  1.45200190e-02, mean val. rec. loss:  7.54994133e-03\n",
      "Epoch: 6041 mean train loss:  1.45176400e-02, mean val. rec. loss:  7.54824943e-03\n",
      "Epoch: 6042 mean train loss:  1.45152797e-02, mean val. rec. loss:  7.54655187e-03\n",
      "Epoch: 6043 mean train loss:  1.45129008e-02, mean val. rec. loss:  7.54487415e-03\n",
      "Epoch: 6044 mean train loss:  1.45105237e-02, mean val. rec. loss:  7.54318453e-03\n",
      "Epoch: 6045 mean train loss:  1.45081568e-02, mean val. rec. loss:  7.54150738e-03\n",
      "Epoch: 6046 mean train loss:  1.45057797e-02, mean val. rec. loss:  7.53983646e-03\n",
      "Epoch: 6047 mean train loss:  1.45033952e-02, mean val. rec. loss:  7.53814514e-03\n",
      "Epoch: 6048 mean train loss:  1.45010107e-02, mean val. rec. loss:  7.53645494e-03\n",
      "Epoch: 6049 mean train loss:  1.44986429e-02, mean val. rec. loss:  7.53477212e-03\n",
      "Epoch: 6050 mean train loss:  1.44962565e-02, mean val. rec. loss:  7.53308193e-03\n",
      "Epoch: 6051 mean train loss:  1.44938673e-02, mean val. rec. loss:  7.53139514e-03\n",
      "Epoch: 6052 mean train loss:  1.44914790e-02, mean val. rec. loss:  7.52970381e-03\n",
      "Epoch: 6053 mean train loss:  1.44891066e-02, mean val. rec. loss:  7.52802156e-03\n",
      "Epoch: 6054 mean train loss:  1.44867156e-02, mean val. rec. loss:  7.52632853e-03\n",
      "Epoch: 6055 mean train loss:  1.44843199e-02, mean val. rec. loss:  7.52464968e-03\n",
      "Epoch: 6056 mean train loss:  1.44819279e-02, mean val. rec. loss:  7.52295212e-03\n",
      "Epoch: 6057 mean train loss:  1.44795480e-02, mean val. rec. loss:  7.52124945e-03\n",
      "Epoch: 6058 mean train loss:  1.44771523e-02, mean val. rec. loss:  7.51955869e-03\n",
      "Epoch: 6059 mean train loss:  1.44747538e-02, mean val. rec. loss:  7.51786907e-03\n",
      "Epoch: 6060 mean train loss:  1.44723516e-02, mean val. rec. loss:  7.51617491e-03\n",
      "Epoch: 6061 mean train loss:  1.44699698e-02, mean val. rec. loss:  7.51447734e-03\n",
      "Epoch: 6062 mean train loss:  1.44675667e-02, mean val. rec. loss:  7.51279169e-03\n",
      "Epoch: 6063 mean train loss:  1.44651607e-02, mean val. rec. loss:  7.51109186e-03\n",
      "Epoch: 6064 mean train loss:  1.44627566e-02, mean val. rec. loss:  7.50938806e-03\n",
      "Epoch: 6065 mean train loss:  1.44603693e-02, mean val. rec. loss:  7.50769163e-03\n",
      "Epoch: 6066 mean train loss:  1.44579633e-02, mean val. rec. loss:  7.50599690e-03\n",
      "Epoch: 6067 mean train loss:  1.44555555e-02, mean val. rec. loss:  7.50431408e-03\n",
      "Epoch: 6068 mean train loss:  1.44531449e-02, mean val. rec. loss:  7.50262445e-03\n",
      "Epoch: 6069 mean train loss:  1.44507362e-02, mean val. rec. loss:  7.50092859e-03\n",
      "Epoch: 6070 mean train loss:  1.44483246e-02, mean val. rec. loss:  7.49921288e-03\n",
      "Epoch: 6071 mean train loss:  1.44459112e-02, mean val. rec. loss:  7.49750171e-03\n",
      "Epoch: 6072 mean train loss:  1.44435100e-02, mean val. rec. loss:  7.49581152e-03\n",
      "Epoch: 6073 mean train loss:  1.44411031e-02, mean val. rec. loss:  7.49412587e-03\n",
      "Epoch: 6074 mean train loss:  1.44386878e-02, mean val. rec. loss:  7.49243737e-03\n",
      "Epoch: 6075 mean train loss:  1.44362670e-02, mean val. rec. loss:  7.49071827e-03\n",
      "Epoch: 6076 mean train loss:  1.44338480e-02, mean val. rec. loss:  7.48900880e-03\n",
      "Epoch: 6077 mean train loss:  1.44314271e-02, mean val. rec. loss:  7.48731293e-03\n",
      "Epoch: 6078 mean train loss:  1.44290081e-02, mean val. rec. loss:  7.48560290e-03\n",
      "Epoch: 6079 mean train loss:  1.44265817e-02, mean val. rec. loss:  7.48390874e-03\n",
      "Epoch: 6080 mean train loss:  1.44241646e-02, mean val. rec. loss:  7.48220720e-03\n",
      "Epoch: 6081 mean train loss:  1.44217512e-02, mean val. rec. loss:  7.48050567e-03\n",
      "Epoch: 6082 mean train loss:  1.44193248e-02, mean val. rec. loss:  7.47880301e-03\n",
      "Epoch: 6083 mean train loss:  1.44168965e-02, mean val. rec. loss:  7.47709240e-03\n",
      "Epoch: 6084 mean train loss:  1.44144654e-02, mean val. rec. loss:  7.47539030e-03\n",
      "Epoch: 6085 mean train loss:  1.44120361e-02, mean val. rec. loss:  7.47367687e-03\n",
      "Epoch: 6086 mean train loss:  1.44096013e-02, mean val. rec. loss:  7.47197363e-03\n",
      "Epoch: 6087 mean train loss:  1.44071702e-02, mean val. rec. loss:  7.47025906e-03\n",
      "Epoch: 6088 mean train loss:  1.44047410e-02, mean val. rec. loss:  7.46856376e-03\n",
      "Epoch: 6089 mean train loss:  1.44023043e-02, mean val. rec. loss:  7.46685259e-03\n",
      "Epoch: 6090 mean train loss:  1.43998695e-02, mean val. rec. loss:  7.46514823e-03\n",
      "Epoch: 6091 mean train loss:  1.43974338e-02, mean val. rec. loss:  7.46343989e-03\n",
      "Epoch: 6092 mean train loss:  1.43949952e-02, mean val. rec. loss:  7.46173496e-03\n",
      "Epoch: 6093 mean train loss:  1.43925558e-02, mean val. rec. loss:  7.46002605e-03\n",
      "Epoch: 6094 mean train loss:  1.43901163e-02, mean val. rec. loss:  7.45831942e-03\n",
      "Epoch: 6095 mean train loss:  1.43876731e-02, mean val. rec. loss:  7.45660712e-03\n",
      "Epoch: 6096 mean train loss:  1.43852318e-02, mean val. rec. loss:  7.45489538e-03\n",
      "Epoch: 6097 mean train loss:  1.43827886e-02, mean val. rec. loss:  7.45318648e-03\n",
      "Epoch: 6098 mean train loss:  1.43803417e-02, mean val. rec. loss:  7.45148664e-03\n",
      "Epoch: 6099 mean train loss:  1.43778966e-02, mean val. rec. loss:  7.44977037e-03\n",
      "Epoch: 6100 mean train loss:  1.43754469e-02, mean val. rec. loss:  7.44805013e-03\n",
      "Epoch: 6101 mean train loss:  1.43729962e-02, mean val. rec. loss:  7.44634293e-03\n",
      "Epoch: 6102 mean train loss:  1.43705456e-02, mean val. rec. loss:  7.44464310e-03\n",
      "Epoch: 6103 mean train loss:  1.43680950e-02, mean val. rec. loss:  7.44293249e-03\n",
      "Epoch: 6104 mean train loss:  1.43656378e-02, mean val. rec. loss:  7.44121849e-03\n",
      "Epoch: 6105 mean train loss:  1.43631872e-02, mean val. rec. loss:  7.43950108e-03\n",
      "Epoch: 6106 mean train loss:  1.43607300e-02, mean val. rec. loss:  7.43779274e-03\n",
      "Epoch: 6107 mean train loss:  1.43582728e-02, mean val. rec. loss:  7.43607193e-03\n",
      "Epoch: 6108 mean train loss:  1.43558166e-02, mean val. rec. loss:  7.43436190e-03\n",
      "Epoch: 6109 mean train loss:  1.43533567e-02, mean val. rec. loss:  7.43265243e-03\n",
      "Epoch: 6110 mean train loss:  1.43509004e-02, mean val. rec. loss:  7.43093105e-03\n",
      "Epoch: 6111 mean train loss:  1.43484386e-02, mean val. rec. loss:  7.42921704e-03\n",
      "Epoch: 6112 mean train loss:  1.43459777e-02, mean val. rec. loss:  7.42751381e-03\n",
      "Epoch: 6113 mean train loss:  1.43435075e-02, mean val. rec. loss:  7.42580264e-03\n",
      "Epoch: 6114 mean train loss:  1.43410327e-02, mean val. rec. loss:  7.42407559e-03\n",
      "Epoch: 6115 mean train loss:  1.43385671e-02, mean val. rec. loss:  7.42235082e-03\n",
      "Epoch: 6116 mean train loss:  1.43361007e-02, mean val. rec. loss:  7.42062547e-03\n",
      "Epoch: 6117 mean train loss:  1.43336342e-02, mean val. rec. loss:  7.41891147e-03\n",
      "Epoch: 6118 mean train loss:  1.43311621e-02, mean val. rec. loss:  7.41720596e-03\n",
      "Epoch: 6119 mean train loss:  1.43286919e-02, mean val. rec. loss:  7.41549309e-03\n",
      "Epoch: 6120 mean train loss:  1.43262236e-02, mean val. rec. loss:  7.41379383e-03\n",
      "Epoch: 6121 mean train loss:  1.43237385e-02, mean val. rec. loss:  7.41205488e-03\n",
      "Epoch: 6122 mean train loss:  1.43212534e-02, mean val. rec. loss:  7.41031366e-03\n",
      "Epoch: 6123 mean train loss:  1.43187776e-02, mean val. rec. loss:  7.40858321e-03\n",
      "Epoch: 6124 mean train loss:  1.43163046e-02, mean val. rec. loss:  7.40686693e-03\n",
      "Epoch: 6125 mean train loss:  1.43138242e-02, mean val. rec. loss:  7.40516313e-03\n",
      "Epoch: 6126 mean train loss:  1.43113475e-02, mean val. rec. loss:  7.40346784e-03\n",
      "Epoch: 6127 mean train loss:  1.43088671e-02, mean val. rec. loss:  7.40176347e-03\n",
      "Epoch: 6128 mean train loss:  1.43063680e-02, mean val. rec. loss:  7.40000808e-03\n",
      "Epoch: 6129 mean train loss:  1.43038894e-02, mean val. rec. loss:  7.39825381e-03\n",
      "Epoch: 6130 mean train loss:  1.43014071e-02, mean val. rec. loss:  7.39653584e-03\n",
      "Epoch: 6131 mean train loss:  1.42989230e-02, mean val. rec. loss:  7.39484792e-03\n",
      "Epoch: 6132 mean train loss:  1.42964211e-02, mean val. rec. loss:  7.39314525e-03\n",
      "Epoch: 6133 mean train loss:  1.42939332e-02, mean val. rec. loss:  7.39141764e-03\n",
      "Epoch: 6134 mean train loss:  1.42914472e-02, mean val. rec. loss:  7.38968492e-03\n",
      "Epoch: 6135 mean train loss:  1.42889602e-02, mean val. rec. loss:  7.38794710e-03\n",
      "Epoch: 6136 mean train loss:  1.42864500e-02, mean val. rec. loss:  7.38622913e-03\n",
      "Epoch: 6137 mean train loss:  1.42839621e-02, mean val. rec. loss:  7.38451625e-03\n",
      "Epoch: 6138 mean train loss:  1.42814687e-02, mean val. rec. loss:  7.38280565e-03\n",
      "Epoch: 6139 mean train loss:  1.42789752e-02, mean val. rec. loss:  7.38106953e-03\n",
      "Epoch: 6140 mean train loss:  1.42764640e-02, mean val. rec. loss:  7.37935779e-03\n",
      "Epoch: 6141 mean train loss:  1.42739677e-02, mean val. rec. loss:  7.37763302e-03\n",
      "Epoch: 6142 mean train loss:  1.42714696e-02, mean val. rec. loss:  7.37590314e-03\n",
      "Epoch: 6143 mean train loss:  1.42689752e-02, mean val. rec. loss:  7.37418119e-03\n",
      "Epoch: 6144 mean train loss:  1.42664557e-02, mean val. rec. loss:  7.37245131e-03\n",
      "Epoch: 6145 mean train loss:  1.42639538e-02, mean val. rec. loss:  7.37072653e-03\n",
      "Epoch: 6146 mean train loss:  1.42614501e-02, mean val. rec. loss:  7.36900629e-03\n",
      "Epoch: 6147 mean train loss:  1.42589427e-02, mean val. rec. loss:  7.36728094e-03\n",
      "Epoch: 6148 mean train loss:  1.42564315e-02, mean val. rec. loss:  7.36556354e-03\n",
      "Epoch: 6149 mean train loss:  1.42539259e-02, mean val. rec. loss:  7.36384216e-03\n",
      "Epoch: 6150 mean train loss:  1.42514054e-02, mean val. rec. loss:  7.36211228e-03\n",
      "Epoch: 6151 mean train loss:  1.42488989e-02, mean val. rec. loss:  7.36038410e-03\n",
      "Epoch: 6152 mean train loss:  1.42463924e-02, mean val. rec. loss:  7.35865422e-03\n",
      "Epoch: 6153 mean train loss:  1.42438636e-02, mean val. rec. loss:  7.35693511e-03\n",
      "Epoch: 6154 mean train loss:  1.42413571e-02, mean val. rec. loss:  7.35522110e-03\n",
      "Epoch: 6155 mean train loss:  1.42388394e-02, mean val. rec. loss:  7.35350823e-03\n",
      "Epoch: 6156 mean train loss:  1.42363161e-02, mean val. rec. loss:  7.35177325e-03\n",
      "Epoch: 6157 mean train loss:  1.42338022e-02, mean val. rec. loss:  7.35002692e-03\n",
      "Epoch: 6158 mean train loss:  1.42312714e-02, mean val. rec. loss:  7.34828627e-03\n",
      "Epoch: 6159 mean train loss:  1.42287547e-02, mean val. rec. loss:  7.34656943e-03\n",
      "Epoch: 6160 mean train loss:  1.42262398e-02, mean val. rec. loss:  7.34485429e-03\n",
      "Epoch: 6161 mean train loss:  1.42237035e-02, mean val. rec. loss:  7.34314765e-03\n",
      "Epoch: 6162 mean train loss:  1.42211858e-02, mean val. rec. loss:  7.34140757e-03\n",
      "Epoch: 6163 mean train loss:  1.42186560e-02, mean val. rec. loss:  7.33966011e-03\n",
      "Epoch: 6164 mean train loss:  1.42161244e-02, mean val. rec. loss:  7.33791379e-03\n",
      "Epoch: 6165 mean train loss:  1.42136030e-02, mean val. rec. loss:  7.33619127e-03\n",
      "Epoch: 6166 mean train loss:  1.42110648e-02, mean val. rec. loss:  7.33448010e-03\n",
      "Epoch: 6167 mean train loss:  1.42085388e-02, mean val. rec. loss:  7.33275816e-03\n",
      "Epoch: 6168 mean train loss:  1.42059978e-02, mean val. rec. loss:  7.33102715e-03\n",
      "Epoch: 6169 mean train loss:  1.42034745e-02, mean val. rec. loss:  7.32929897e-03\n",
      "Epoch: 6170 mean train loss:  1.42009289e-02, mean val. rec. loss:  7.32756228e-03\n",
      "Epoch: 6171 mean train loss:  1.41984038e-02, mean val. rec. loss:  7.32582163e-03\n",
      "Epoch: 6172 mean train loss:  1.41958563e-02, mean val. rec. loss:  7.32409345e-03\n",
      "Epoch: 6173 mean train loss:  1.41933274e-02, mean val. rec. loss:  7.32236243e-03\n",
      "Epoch: 6174 mean train loss:  1.41907790e-02, mean val. rec. loss:  7.32063652e-03\n",
      "Epoch: 6175 mean train loss:  1.41882465e-02, mean val. rec. loss:  7.31892081e-03\n",
      "Epoch: 6176 mean train loss:  1.41856962e-02, mean val. rec. loss:  7.31719263e-03\n",
      "Epoch: 6177 mean train loss:  1.41831608e-02, mean val. rec. loss:  7.31546275e-03\n",
      "Epoch: 6178 mean train loss:  1.41806106e-02, mean val. rec. loss:  7.31371869e-03\n",
      "Epoch: 6179 mean train loss:  1.41780724e-02, mean val. rec. loss:  7.31197861e-03\n",
      "Epoch: 6180 mean train loss:  1.41755156e-02, mean val. rec. loss:  7.31024532e-03\n",
      "Epoch: 6181 mean train loss:  1.41729774e-02, mean val. rec. loss:  7.30852678e-03\n",
      "Epoch: 6182 mean train loss:  1.41704197e-02, mean val. rec. loss:  7.30682695e-03\n",
      "Epoch: 6183 mean train loss:  1.41678825e-02, mean val. rec. loss:  7.30508289e-03\n",
      "Epoch: 6184 mean train loss:  1.41653238e-02, mean val. rec. loss:  7.30333204e-03\n",
      "Epoch: 6185 mean train loss:  1.41627670e-02, mean val. rec. loss:  7.30159025e-03\n",
      "Epoch: 6186 mean train loss:  1.41602242e-02, mean val. rec. loss:  7.29986774e-03\n",
      "Epoch: 6187 mean train loss:  1.41576646e-02, mean val. rec. loss:  7.29815203e-03\n",
      "Epoch: 6188 mean train loss:  1.41551097e-02, mean val. rec. loss:  7.29643235e-03\n",
      "Epoch: 6189 mean train loss:  1.41525557e-02, mean val. rec. loss:  7.29470474e-03\n",
      "Epoch: 6190 mean train loss:  1.41499924e-02, mean val. rec. loss:  7.29296919e-03\n",
      "Epoch: 6191 mean train loss:  1.41474449e-02, mean val. rec. loss:  7.29124668e-03\n",
      "Epoch: 6192 mean train loss:  1.41448770e-02, mean val. rec. loss:  7.28950546e-03\n",
      "Epoch: 6193 mean train loss:  1.41423090e-02, mean val. rec. loss:  7.28777218e-03\n",
      "Epoch: 6194 mean train loss:  1.41397615e-02, mean val. rec. loss:  7.28602982e-03\n",
      "Epoch: 6195 mean train loss:  1.41371889e-02, mean val. rec. loss:  7.28430844e-03\n",
      "Epoch: 6196 mean train loss:  1.41346312e-02, mean val. rec. loss:  7.28258650e-03\n",
      "Epoch: 6197 mean train loss:  1.41320688e-02, mean val. rec. loss:  7.28086512e-03\n",
      "Epoch: 6198 mean train loss:  1.41294916e-02, mean val. rec. loss:  7.27913638e-03\n",
      "Epoch: 6199 mean train loss:  1.41269264e-02, mean val. rec. loss:  7.27739459e-03\n",
      "Epoch: 6200 mean train loss:  1.41243649e-02, mean val. rec. loss:  7.27565904e-03\n",
      "Epoch: 6201 mean train loss:  1.41217933e-02, mean val. rec. loss:  7.27392972e-03\n",
      "Epoch: 6202 mean train loss:  1.41192179e-02, mean val. rec. loss:  7.27218737e-03\n",
      "Epoch: 6203 mean train loss:  1.41166527e-02, mean val. rec. loss:  7.27047903e-03\n",
      "Epoch: 6204 mean train loss:  1.41140847e-02, mean val. rec. loss:  7.26875482e-03\n",
      "Epoch: 6205 mean train loss:  1.41115047e-02, mean val. rec. loss:  7.26703004e-03\n",
      "Epoch: 6206 mean train loss:  1.41089255e-02, mean val. rec. loss:  7.26529506e-03\n",
      "Epoch: 6207 mean train loss:  1.41063538e-02, mean val. rec. loss:  7.26353059e-03\n",
      "Epoch: 6208 mean train loss:  1.41037822e-02, mean val. rec. loss:  7.26182055e-03\n",
      "Epoch: 6209 mean train loss:  1.41012012e-02, mean val. rec. loss:  7.26009804e-03\n",
      "Epoch: 6210 mean train loss:  1.40986183e-02, mean val. rec. loss:  7.25837893e-03\n",
      "Epoch: 6211 mean train loss:  1.40960448e-02, mean val. rec. loss:  7.25664622e-03\n",
      "Epoch: 6212 mean train loss:  1.40934656e-02, mean val. rec. loss:  7.25490953e-03\n",
      "Epoch: 6213 mean train loss:  1.40908790e-02, mean val. rec. loss:  7.25317909e-03\n",
      "Epoch: 6214 mean train loss:  1.40882925e-02, mean val. rec. loss:  7.25144353e-03\n",
      "Epoch: 6215 mean train loss:  1.40857059e-02, mean val. rec. loss:  7.24971535e-03\n",
      "Epoch: 6216 mean train loss:  1.40831277e-02, mean val. rec. loss:  7.24800078e-03\n",
      "Epoch: 6217 mean train loss:  1.40805532e-02, mean val. rec. loss:  7.24628564e-03\n",
      "Epoch: 6218 mean train loss:  1.40779620e-02, mean val. rec. loss:  7.24456767e-03\n",
      "Epoch: 6219 mean train loss:  1.40753707e-02, mean val. rec. loss:  7.24282191e-03\n",
      "Epoch: 6220 mean train loss:  1.40727832e-02, mean val. rec. loss:  7.24108296e-03\n",
      "Epoch: 6221 mean train loss:  1.40701901e-02, mean val. rec. loss:  7.23934854e-03\n",
      "Epoch: 6222 mean train loss:  1.40675979e-02, mean val. rec. loss:  7.23762433e-03\n",
      "Epoch: 6223 mean train loss:  1.40650030e-02, mean val. rec. loss:  7.23589671e-03\n",
      "Epoch: 6224 mean train loss:  1.40624210e-02, mean val. rec. loss:  7.23417250e-03\n",
      "Epoch: 6225 mean train loss:  1.40598326e-02, mean val. rec. loss:  7.23245510e-03\n",
      "Epoch: 6226 mean train loss:  1.40572367e-02, mean val. rec. loss:  7.23072238e-03\n",
      "Epoch: 6227 mean train loss:  1.40546389e-02, mean val. rec. loss:  7.22898229e-03\n",
      "Epoch: 6228 mean train loss:  1.40520412e-02, mean val. rec. loss:  7.22724447e-03\n",
      "Epoch: 6229 mean train loss:  1.40494416e-02, mean val. rec. loss:  7.22551856e-03\n",
      "Epoch: 6230 mean train loss:  1.40468448e-02, mean val. rec. loss:  7.22380626e-03\n",
      "Epoch: 6231 mean train loss:  1.40442442e-02, mean val. rec. loss:  7.22208318e-03\n",
      "Epoch: 6232 mean train loss:  1.40416464e-02, mean val. rec. loss:  7.22037258e-03\n",
      "Epoch: 6233 mean train loss:  1.40390450e-02, mean val. rec. loss:  7.21864213e-03\n",
      "Epoch: 6234 mean train loss:  1.40364435e-02, mean val. rec. loss:  7.21689467e-03\n",
      "Epoch: 6235 mean train loss:  1.40338401e-02, mean val. rec. loss:  7.21516422e-03\n",
      "Epoch: 6236 mean train loss:  1.40312396e-02, mean val. rec. loss:  7.21343944e-03\n",
      "Epoch: 6237 mean train loss:  1.40286335e-02, mean val. rec. loss:  7.21171353e-03\n",
      "Epoch: 6238 mean train loss:  1.40260301e-02, mean val. rec. loss:  7.20998251e-03\n",
      "Epoch: 6239 mean train loss:  1.40234221e-02, mean val. rec. loss:  7.20826738e-03\n",
      "Epoch: 6240 mean train loss:  1.40208141e-02, mean val. rec. loss:  7.20655677e-03\n",
      "Epoch: 6241 mean train loss:  1.40182071e-02, mean val. rec. loss:  7.20482916e-03\n",
      "Epoch: 6242 mean train loss:  1.40156018e-02, mean val. rec. loss:  7.20309531e-03\n",
      "Epoch: 6243 mean train loss:  1.40129939e-02, mean val. rec. loss:  7.20136826e-03\n",
      "Epoch: 6244 mean train loss:  1.40103877e-02, mean val. rec. loss:  7.19964348e-03\n",
      "Epoch: 6245 mean train loss:  1.40077704e-02, mean val. rec. loss:  7.19795046e-03\n",
      "Epoch: 6246 mean train loss:  1.40051512e-02, mean val. rec. loss:  7.19623078e-03\n",
      "Epoch: 6247 mean train loss:  1.40025404e-02, mean val. rec. loss:  7.19449806e-03\n",
      "Epoch: 6248 mean train loss:  1.39999259e-02, mean val. rec. loss:  7.19276535e-03\n",
      "Epoch: 6249 mean train loss:  1.39973151e-02, mean val. rec. loss:  7.19104681e-03\n",
      "Epoch: 6250 mean train loss:  1.39947025e-02, mean val. rec. loss:  7.18933110e-03\n",
      "Epoch: 6251 mean train loss:  1.39920880e-02, mean val. rec. loss:  7.18761880e-03\n",
      "Epoch: 6252 mean train loss:  1.39894753e-02, mean val. rec. loss:  7.18590876e-03\n",
      "Epoch: 6253 mean train loss:  1.39868487e-02, mean val. rec. loss:  7.18419192e-03\n",
      "Epoch: 6254 mean train loss:  1.39842249e-02, mean val. rec. loss:  7.18245693e-03\n",
      "Epoch: 6255 mean train loss:  1.39816104e-02, mean val. rec. loss:  7.18072195e-03\n",
      "Epoch: 6256 mean train loss:  1.39789968e-02, mean val. rec. loss:  7.17899717e-03\n",
      "Epoch: 6257 mean train loss:  1.39763729e-02, mean val. rec. loss:  7.17729961e-03\n",
      "Epoch: 6258 mean train loss:  1.39737445e-02, mean val. rec. loss:  7.17557596e-03\n",
      "Epoch: 6259 mean train loss:  1.39711234e-02, mean val. rec. loss:  7.17385515e-03\n",
      "Epoch: 6260 mean train loss:  1.39685071e-02, mean val. rec. loss:  7.17214455e-03\n",
      "Epoch: 6261 mean train loss:  1.39658758e-02, mean val. rec. loss:  7.17041637e-03\n",
      "Epoch: 6262 mean train loss:  1.39632482e-02, mean val. rec. loss:  7.16868422e-03\n",
      "Epoch: 6263 mean train loss:  1.39606281e-02, mean val. rec. loss:  7.16696681e-03\n",
      "Epoch: 6264 mean train loss:  1.39580062e-02, mean val. rec. loss:  7.16526131e-03\n",
      "Epoch: 6265 mean train loss:  1.39553693e-02, mean val. rec. loss:  7.16355978e-03\n",
      "Epoch: 6266 mean train loss:  1.39527483e-02, mean val. rec. loss:  7.16184351e-03\n",
      "Epoch: 6267 mean train loss:  1.39501282e-02, mean val. rec. loss:  7.16012723e-03\n",
      "Epoch: 6268 mean train loss:  1.39474866e-02, mean val. rec. loss:  7.15840416e-03\n",
      "Epoch: 6269 mean train loss:  1.39448628e-02, mean val. rec. loss:  7.15669582e-03\n",
      "Epoch: 6270 mean train loss:  1.39422315e-02, mean val. rec. loss:  7.15498068e-03\n",
      "Epoch: 6271 mean train loss:  1.39395965e-02, mean val. rec. loss:  7.15326838e-03\n",
      "Epoch: 6272 mean train loss:  1.39369727e-02, mean val. rec. loss:  7.15155947e-03\n",
      "Epoch: 6273 mean train loss:  1.39343284e-02, mean val. rec. loss:  7.14985057e-03\n",
      "Epoch: 6274 mean train loss:  1.39317009e-02, mean val. rec. loss:  7.14814167e-03\n",
      "Epoch: 6275 mean train loss:  1.39290752e-02, mean val. rec. loss:  7.14642710e-03\n",
      "Epoch: 6276 mean train loss:  1.39264327e-02, mean val. rec. loss:  7.14470288e-03\n",
      "Epoch: 6277 mean train loss:  1.39238070e-02, mean val. rec. loss:  7.14300192e-03\n",
      "Epoch: 6278 mean train loss:  1.39211590e-02, mean val. rec. loss:  7.14129925e-03\n",
      "Epoch: 6279 mean train loss:  1.39185342e-02, mean val. rec. loss:  7.13959035e-03\n",
      "Epoch: 6280 mean train loss:  1.39158871e-02, mean val. rec. loss:  7.13787578e-03\n",
      "Epoch: 6281 mean train loss:  1.39132568e-02, mean val. rec. loss:  7.13614646e-03\n",
      "Epoch: 6282 mean train loss:  1.39106097e-02, mean val. rec. loss:  7.13444493e-03\n",
      "Epoch: 6283 mean train loss:  1.39079803e-02, mean val. rec. loss:  7.13273773e-03\n",
      "Epoch: 6284 mean train loss:  1.39053304e-02, mean val. rec. loss:  7.13103903e-03\n",
      "Epoch: 6285 mean train loss:  1.39027028e-02, mean val. rec. loss:  7.12933467e-03\n",
      "Epoch: 6286 mean train loss:  1.39000520e-02, mean val. rec. loss:  7.12764391e-03\n",
      "Epoch: 6287 mean train loss:  1.38974030e-02, mean val. rec. loss:  7.12591119e-03\n",
      "Epoch: 6288 mean train loss:  1.38947718e-02, mean val. rec. loss:  7.12419662e-03\n",
      "Epoch: 6289 mean train loss:  1.38921256e-02, mean val. rec. loss:  7.12248772e-03\n",
      "Epoch: 6290 mean train loss:  1.38894850e-02, mean val. rec. loss:  7.12081510e-03\n",
      "Epoch: 6291 mean train loss:  1.38868435e-02, mean val. rec. loss:  7.11910847e-03\n",
      "Epoch: 6292 mean train loss:  1.38841908e-02, mean val. rec. loss:  7.11739786e-03\n",
      "Epoch: 6293 mean train loss:  1.38815577e-02, mean val. rec. loss:  7.11568726e-03\n",
      "Epoch: 6294 mean train loss:  1.38789059e-02, mean val. rec. loss:  7.11398573e-03\n",
      "Epoch: 6295 mean train loss:  1.38762560e-02, mean val. rec. loss:  7.11228533e-03\n",
      "Epoch: 6296 mean train loss:  1.38736163e-02, mean val. rec. loss:  7.11059344e-03\n",
      "Epoch: 6297 mean train loss:  1.38709730e-02, mean val. rec. loss:  7.10889474e-03\n",
      "Epoch: 6298 mean train loss:  1.38683212e-02, mean val. rec. loss:  7.10719661e-03\n",
      "Epoch: 6299 mean train loss:  1.38656685e-02, mean val. rec. loss:  7.10550018e-03\n",
      "Epoch: 6300 mean train loss:  1.38630214e-02, mean val. rec. loss:  7.10380885e-03\n",
      "Epoch: 6301 mean train loss:  1.38603799e-02, mean val. rec. loss:  7.10209598e-03\n",
      "Epoch: 6302 mean train loss:  1.38577272e-02, mean val. rec. loss:  7.10040182e-03\n",
      "Epoch: 6303 mean train loss:  1.38550699e-02, mean val. rec. loss:  7.09871049e-03\n",
      "Epoch: 6304 mean train loss:  1.38524181e-02, mean val. rec. loss:  7.09702654e-03\n",
      "Epoch: 6305 mean train loss:  1.38497719e-02, mean val. rec. loss:  7.09532557e-03\n",
      "Epoch: 6306 mean train loss:  1.38471323e-02, mean val. rec. loss:  7.09363482e-03\n",
      "Epoch: 6307 mean train loss:  1.38444777e-02, mean val. rec. loss:  7.09195086e-03\n",
      "Epoch: 6308 mean train loss:  1.38418222e-02, mean val. rec. loss:  7.09025216e-03\n",
      "Epoch: 6309 mean train loss:  1.38391658e-02, mean val. rec. loss:  7.08856254e-03\n",
      "Epoch: 6310 mean train loss:  1.38365113e-02, mean val. rec. loss:  7.08685704e-03\n",
      "Epoch: 6311 mean train loss:  1.38338558e-02, mean val. rec. loss:  7.08517422e-03\n",
      "Epoch: 6312 mean train loss:  1.38312022e-02, mean val. rec. loss:  7.08350614e-03\n",
      "Epoch: 6313 mean train loss:  1.38285467e-02, mean val. rec. loss:  7.08180574e-03\n",
      "Epoch: 6314 mean train loss:  1.38258940e-02, mean val. rec. loss:  7.08011101e-03\n",
      "Epoch: 6315 mean train loss:  1.38232376e-02, mean val. rec. loss:  7.07841572e-03\n",
      "Epoch: 6316 mean train loss:  1.38205821e-02, mean val. rec. loss:  7.07673913e-03\n",
      "Epoch: 6317 mean train loss:  1.38179247e-02, mean val. rec. loss:  7.07507219e-03\n",
      "Epoch: 6318 mean train loss:  1.38152730e-02, mean val. rec. loss:  7.07338369e-03\n",
      "Epoch: 6319 mean train loss:  1.38126138e-02, mean val. rec. loss:  7.07169804e-03\n",
      "Epoch: 6320 mean train loss:  1.38099592e-02, mean val. rec. loss:  7.07000104e-03\n",
      "Epoch: 6321 mean train loss:  1.38073037e-02, mean val. rec. loss:  7.06833069e-03\n",
      "Epoch: 6322 mean train loss:  1.38046473e-02, mean val. rec. loss:  7.06664731e-03\n",
      "Epoch: 6323 mean train loss:  1.38019956e-02, mean val. rec. loss:  7.06498206e-03\n",
      "Epoch: 6324 mean train loss:  1.37993382e-02, mean val. rec. loss:  7.06330264e-03\n",
      "Epoch: 6325 mean train loss:  1.37966716e-02, mean val. rec. loss:  7.06160848e-03\n",
      "Epoch: 6326 mean train loss:  1.37940058e-02, mean val. rec. loss:  7.05992226e-03\n",
      "Epoch: 6327 mean train loss:  1.37913513e-02, mean val. rec. loss:  7.05823944e-03\n",
      "Epoch: 6328 mean train loss:  1.37886958e-02, mean val. rec. loss:  7.05656455e-03\n",
      "Epoch: 6329 mean train loss:  1.37860394e-02, mean val. rec. loss:  7.05490271e-03\n",
      "Epoch: 6330 mean train loss:  1.37833718e-02, mean val. rec. loss:  7.05324710e-03\n",
      "Epoch: 6331 mean train loss:  1.37807089e-02, mean val. rec. loss:  7.05156995e-03\n",
      "Epoch: 6332 mean train loss:  1.37780552e-02, mean val. rec. loss:  7.04988657e-03\n",
      "Epoch: 6333 mean train loss:  1.37754016e-02, mean val. rec. loss:  7.04820658e-03\n",
      "Epoch: 6334 mean train loss:  1.37727359e-02, mean val. rec. loss:  7.04651412e-03\n",
      "Epoch: 6335 mean train loss:  1.37700683e-02, mean val. rec. loss:  7.04485965e-03\n",
      "Epoch: 6336 mean train loss:  1.37674138e-02, mean val. rec. loss:  7.04318023e-03\n",
      "Epoch: 6337 mean train loss:  1.37647518e-02, mean val. rec. loss:  7.04152746e-03\n",
      "Epoch: 6338 mean train loss:  1.37620860e-02, mean val. rec. loss:  7.03985881e-03\n",
      "Epoch: 6339 mean train loss:  1.37594315e-02, mean val. rec. loss:  7.03820264e-03\n",
      "Epoch: 6340 mean train loss:  1.37567555e-02, mean val. rec. loss:  7.03651868e-03\n",
      "Epoch: 6341 mean train loss:  1.37541010e-02, mean val. rec. loss:  7.03484040e-03\n",
      "Epoch: 6342 mean train loss:  1.37514464e-02, mean val. rec. loss:  7.03317629e-03\n",
      "Epoch: 6343 mean train loss:  1.37487732e-02, mean val. rec. loss:  7.03153372e-03\n",
      "Epoch: 6344 mean train loss:  1.37461224e-02, mean val. rec. loss:  7.02987471e-03\n",
      "Epoch: 6345 mean train loss:  1.37434483e-02, mean val. rec. loss:  7.02821004e-03\n",
      "Epoch: 6346 mean train loss:  1.37407928e-02, mean val. rec. loss:  7.02653969e-03\n",
      "Epoch: 6347 mean train loss:  1.37381206e-02, mean val. rec. loss:  7.02486877e-03\n",
      "Epoch: 6348 mean train loss:  1.37354669e-02, mean val. rec. loss:  7.02322224e-03\n",
      "Epoch: 6349 mean train loss:  1.37327966e-02, mean val. rec. loss:  7.02156607e-03\n",
      "Epoch: 6350 mean train loss:  1.37301262e-02, mean val. rec. loss:  7.01991216e-03\n",
      "Epoch: 6351 mean train loss:  1.37274744e-02, mean val. rec. loss:  7.01825486e-03\n",
      "Epoch: 6352 mean train loss:  1.37248040e-02, mean val. rec. loss:  7.01658621e-03\n",
      "Epoch: 6353 mean train loss:  1.37221430e-02, mean val. rec. loss:  7.01494591e-03\n",
      "Epoch: 6354 mean train loss:  1.37194800e-02, mean val. rec. loss:  7.01330278e-03\n",
      "Epoch: 6355 mean train loss:  1.37168078e-02, mean val. rec. loss:  7.01163980e-03\n",
      "Epoch: 6356 mean train loss:  1.37141477e-02, mean val. rec. loss:  7.00998760e-03\n",
      "Epoch: 6357 mean train loss:  1.37114912e-02, mean val. rec. loss:  7.00833710e-03\n",
      "Epoch: 6358 mean train loss:  1.37088227e-02, mean val. rec. loss:  7.00668716e-03\n",
      "Epoch: 6359 mean train loss:  1.37061523e-02, mean val. rec. loss:  7.00503439e-03\n",
      "Epoch: 6360 mean train loss:  1.37034931e-02, mean val. rec. loss:  7.00339693e-03\n",
      "Epoch: 6361 mean train loss:  1.37008349e-02, mean val. rec. loss:  7.00174075e-03\n",
      "Epoch: 6362 mean train loss:  1.36981673e-02, mean val. rec. loss:  7.00009989e-03\n",
      "Epoch: 6363 mean train loss:  1.36954997e-02, mean val. rec. loss:  6.99845449e-03\n",
      "Epoch: 6364 mean train loss:  1.36928340e-02, mean val. rec. loss:  6.99680569e-03\n",
      "Epoch: 6365 mean train loss:  1.36901692e-02, mean val. rec. loss:  6.99515972e-03\n",
      "Epoch: 6366 mean train loss:  1.36875006e-02, mean val. rec. loss:  6.99351375e-03\n",
      "Epoch: 6367 mean train loss:  1.36848424e-02, mean val. rec. loss:  6.99188650e-03\n",
      "Epoch: 6368 mean train loss:  1.36821785e-02, mean val. rec. loss:  6.99027001e-03\n",
      "Epoch: 6369 mean train loss:  1.36795128e-02, mean val. rec. loss:  6.98863369e-03\n",
      "Epoch: 6370 mean train loss:  1.36768508e-02, mean val. rec. loss:  6.98698658e-03\n",
      "Epoch: 6371 mean train loss:  1.36741860e-02, mean val. rec. loss:  6.98532758e-03\n",
      "Epoch: 6372 mean train loss:  1.36715221e-02, mean val. rec. loss:  6.98369805e-03\n",
      "Epoch: 6373 mean train loss:  1.36688564e-02, mean val. rec. loss:  6.98208214e-03\n",
      "Epoch: 6374 mean train loss:  1.36661879e-02, mean val. rec. loss:  6.98045034e-03\n",
      "Epoch: 6375 mean train loss:  1.36635268e-02, mean val. rec. loss:  6.97881005e-03\n",
      "Epoch: 6376 mean train loss:  1.36608657e-02, mean val. rec. loss:  6.97717428e-03\n",
      "Epoch: 6377 mean train loss:  1.36582047e-02, mean val. rec. loss:  6.97554930e-03\n",
      "Epoch: 6378 mean train loss:  1.36555436e-02, mean val. rec. loss:  6.97391694e-03\n",
      "Epoch: 6379 mean train loss:  1.36528835e-02, mean val. rec. loss:  6.97231066e-03\n",
      "Epoch: 6380 mean train loss:  1.36502177e-02, mean val. rec. loss:  6.97066413e-03\n",
      "Epoch: 6381 mean train loss:  1.36475492e-02, mean val. rec. loss:  6.96902553e-03\n",
      "Epoch: 6382 mean train loss:  1.36448919e-02, mean val. rec. loss:  6.96741358e-03\n",
      "Epoch: 6383 mean train loss:  1.36422354e-02, mean val. rec. loss:  6.96581127e-03\n",
      "Epoch: 6384 mean train loss:  1.36395623e-02, mean val. rec. loss:  6.96418798e-03\n",
      "Epoch: 6385 mean train loss:  1.36369012e-02, mean val. rec. loss:  6.96255392e-03\n",
      "Epoch: 6386 mean train loss:  1.36342476e-02, mean val. rec. loss:  6.96092213e-03\n",
      "Epoch: 6387 mean train loss:  1.36315753e-02, mean val. rec. loss:  6.95932209e-03\n",
      "Epoch: 6388 mean train loss:  1.36289199e-02, mean val. rec. loss:  6.95771751e-03\n",
      "Epoch: 6389 mean train loss:  1.36262662e-02, mean val. rec. loss:  6.95611294e-03\n",
      "Epoch: 6390 mean train loss:  1.36235968e-02, mean val. rec. loss:  6.95448341e-03\n",
      "Epoch: 6391 mean train loss:  1.36209460e-02, mean val. rec. loss:  6.95287203e-03\n",
      "Epoch: 6392 mean train loss:  1.36182737e-02, mean val. rec. loss:  6.95125498e-03\n",
      "Epoch: 6393 mean train loss:  1.36156238e-02, mean val. rec. loss:  6.94965494e-03\n",
      "Epoch: 6394 mean train loss:  1.36129553e-02, mean val. rec. loss:  6.94804073e-03\n",
      "Epoch: 6395 mean train loss:  1.36102998e-02, mean val. rec. loss:  6.94644465e-03\n",
      "Epoch: 6396 mean train loss:  1.36076406e-02, mean val. rec. loss:  6.94482874e-03\n",
      "Epoch: 6397 mean train loss:  1.36049758e-02, mean val. rec. loss:  6.94321509e-03\n",
      "Epoch: 6398 mean train loss:  1.36023296e-02, mean val. rec. loss:  6.94161108e-03\n",
      "Epoch: 6399 mean train loss:  1.35996630e-02, mean val. rec. loss:  6.94002464e-03\n",
      "Epoch: 6400 mean train loss:  1.35969982e-02, mean val. rec. loss:  6.93841326e-03\n",
      "Epoch: 6401 mean train loss:  1.35943557e-02, mean val. rec. loss:  6.93681152e-03\n",
      "Epoch: 6402 mean train loss:  1.35916909e-02, mean val. rec. loss:  6.93521375e-03\n",
      "Epoch: 6403 mean train loss:  1.35890317e-02, mean val. rec. loss:  6.93361938e-03\n",
      "Epoch: 6404 mean train loss:  1.35863707e-02, mean val. rec. loss:  6.93201934e-03\n",
      "Epoch: 6405 mean train loss:  1.35837198e-02, mean val. rec. loss:  6.93040796e-03\n",
      "Epoch: 6406 mean train loss:  1.35810746e-02, mean val. rec. loss:  6.92882833e-03\n",
      "Epoch: 6407 mean train loss:  1.35784145e-02, mean val. rec. loss:  6.92724643e-03\n",
      "Epoch: 6408 mean train loss:  1.35757580e-02, mean val. rec. loss:  6.92566397e-03\n",
      "Epoch: 6409 mean train loss:  1.35730998e-02, mean val. rec. loss:  6.92406336e-03\n",
      "Epoch: 6410 mean train loss:  1.35704471e-02, mean val. rec. loss:  6.92245991e-03\n",
      "Epoch: 6411 mean train loss:  1.35677934e-02, mean val. rec. loss:  6.92086895e-03\n",
      "Epoch: 6412 mean train loss:  1.35651417e-02, mean val. rec. loss:  6.91930292e-03\n",
      "Epoch: 6413 mean train loss:  1.35624881e-02, mean val. rec. loss:  6.91772329e-03\n",
      "Epoch: 6414 mean train loss:  1.35598344e-02, mean val. rec. loss:  6.91614366e-03\n",
      "Epoch: 6415 mean train loss:  1.35571873e-02, mean val. rec. loss:  6.91456574e-03\n",
      "Epoch: 6416 mean train loss:  1.35545365e-02, mean val. rec. loss:  6.91298044e-03\n",
      "Epoch: 6417 mean train loss:  1.35518903e-02, mean val. rec. loss:  6.91138493e-03\n",
      "Epoch: 6418 mean train loss:  1.35492339e-02, mean val. rec. loss:  6.90980133e-03\n",
      "Epoch: 6419 mean train loss:  1.35465719e-02, mean val. rec. loss:  6.90822057e-03\n",
      "Epoch: 6420 mean train loss:  1.35439295e-02, mean val. rec. loss:  6.90663867e-03\n",
      "Epoch: 6421 mean train loss:  1.35412852e-02, mean val. rec. loss:  6.90508796e-03\n",
      "Epoch: 6422 mean train loss:  1.35386418e-02, mean val. rec. loss:  6.90352024e-03\n",
      "Epoch: 6423 mean train loss:  1.35359816e-02, mean val. rec. loss:  6.90193834e-03\n",
      "Epoch: 6424 mean train loss:  1.35333373e-02, mean val. rec. loss:  6.90035247e-03\n",
      "Epoch: 6425 mean train loss:  1.35306995e-02, mean val. rec. loss:  6.89879269e-03\n",
      "Epoch: 6426 mean train loss:  1.35280422e-02, mean val. rec. loss:  6.89723574e-03\n",
      "Epoch: 6427 mean train loss:  1.35254035e-02, mean val. rec. loss:  6.89567822e-03\n",
      "Epoch: 6428 mean train loss:  1.35227545e-02, mean val. rec. loss:  6.89410313e-03\n",
      "Epoch: 6429 mean train loss:  1.35201139e-02, mean val. rec. loss:  6.89252974e-03\n",
      "Epoch: 6430 mean train loss:  1.35174724e-02, mean val. rec. loss:  6.89096145e-03\n",
      "Epoch: 6431 mean train loss:  1.35148244e-02, mean val. rec. loss:  6.88940336e-03\n",
      "Epoch: 6432 mean train loss:  1.35121810e-02, mean val. rec. loss:  6.88785662e-03\n",
      "Epoch: 6433 mean train loss:  1.35095423e-02, mean val. rec. loss:  6.88631101e-03\n",
      "Epoch: 6434 mean train loss:  1.35068942e-02, mean val. rec. loss:  6.88475349e-03\n",
      "Epoch: 6435 mean train loss:  1.35042676e-02, mean val. rec. loss:  6.88319427e-03\n",
      "Epoch: 6436 mean train loss:  1.35016177e-02, mean val. rec. loss:  6.88164243e-03\n",
      "Epoch: 6437 mean train loss:  1.34989753e-02, mean val. rec. loss:  6.88008661e-03\n",
      "Epoch: 6438 mean train loss:  1.34963393e-02, mean val. rec. loss:  6.87852286e-03\n",
      "Epoch: 6439 mean train loss:  1.34937099e-02, mean val. rec. loss:  6.87697895e-03\n",
      "Epoch: 6440 mean train loss:  1.34910656e-02, mean val. rec. loss:  6.87545035e-03\n",
      "Epoch: 6441 mean train loss:  1.34884278e-02, mean val. rec. loss:  6.87390077e-03\n",
      "Epoch: 6442 mean train loss:  1.34857872e-02, mean val. rec. loss:  6.87234779e-03\n",
      "Epoch: 6443 mean train loss:  1.34831504e-02, mean val. rec. loss:  6.87078687e-03\n",
      "Epoch: 6444 mean train loss:  1.34805256e-02, mean val. rec. loss:  6.86925373e-03\n",
      "Epoch: 6445 mean train loss:  1.34778925e-02, mean val. rec. loss:  6.86773137e-03\n",
      "Epoch: 6446 mean train loss:  1.34752593e-02, mean val. rec. loss:  6.86619767e-03\n",
      "Epoch: 6447 mean train loss:  1.34726281e-02, mean val. rec. loss:  6.86465773e-03\n",
      "Epoch: 6448 mean train loss:  1.34699949e-02, mean val. rec. loss:  6.86309794e-03\n",
      "Epoch: 6449 mean train loss:  1.34673655e-02, mean val. rec. loss:  6.86156424e-03\n",
      "Epoch: 6450 mean train loss:  1.34647249e-02, mean val. rec. loss:  6.86003337e-03\n",
      "Epoch: 6451 mean train loss:  1.34621011e-02, mean val. rec. loss:  6.85849796e-03\n",
      "Epoch: 6452 mean train loss:  1.34594736e-02, mean val. rec. loss:  6.85698297e-03\n",
      "Epoch: 6453 mean train loss:  1.34568507e-02, mean val. rec. loss:  6.85547138e-03\n",
      "Epoch: 6454 mean train loss:  1.34542259e-02, mean val. rec. loss:  6.85395299e-03\n",
      "Epoch: 6455 mean train loss:  1.34515890e-02, mean val. rec. loss:  6.85240964e-03\n",
      "Epoch: 6456 mean train loss:  1.34489671e-02, mean val. rec. loss:  6.85087140e-03\n",
      "Epoch: 6457 mean train loss:  1.34463507e-02, mean val. rec. loss:  6.84933713e-03\n",
      "Epoch: 6458 mean train loss:  1.34437129e-02, mean val. rec. loss:  6.84783178e-03\n",
      "Epoch: 6459 mean train loss:  1.34410984e-02, mean val. rec. loss:  6.84633040e-03\n",
      "Epoch: 6460 mean train loss:  1.34384867e-02, mean val. rec. loss:  6.84481767e-03\n",
      "Epoch: 6461 mean train loss:  1.34358535e-02, mean val. rec. loss:  6.84329020e-03\n",
      "Epoch: 6462 mean train loss:  1.34332427e-02, mean val. rec. loss:  6.84175537e-03\n",
      "Epoch: 6463 mean train loss:  1.34306133e-02, mean val. rec. loss:  6.84023187e-03\n",
      "Epoch: 6464 mean train loss:  1.34280063e-02, mean val. rec. loss:  6.83873899e-03\n",
      "Epoch: 6465 mean train loss:  1.34253824e-02, mean val. rec. loss:  6.83721663e-03\n",
      "Epoch: 6466 mean train loss:  1.34227567e-02, mean val. rec. loss:  6.83569880e-03\n",
      "Epoch: 6467 mean train loss:  1.34201525e-02, mean val. rec. loss:  6.83419231e-03\n",
      "Epoch: 6468 mean train loss:  1.34175324e-02, mean val. rec. loss:  6.83269886e-03\n",
      "Epoch: 6469 mean train loss:  1.34149132e-02, mean val. rec. loss:  6.83119748e-03\n",
      "Epoch: 6470 mean train loss:  1.34123145e-02, mean val. rec. loss:  6.82968929e-03\n",
      "Epoch: 6471 mean train loss:  1.34096991e-02, mean val. rec. loss:  6.82816579e-03\n",
      "Epoch: 6472 mean train loss:  1.34070836e-02, mean val. rec. loss:  6.82667745e-03\n",
      "Epoch: 6473 mean train loss:  1.34044738e-02, mean val. rec. loss:  6.82516983e-03\n",
      "Epoch: 6474 mean train loss:  1.34018592e-02, mean val. rec. loss:  6.82368091e-03\n",
      "Epoch: 6475 mean train loss:  1.33992559e-02, mean val. rec. loss:  6.82218010e-03\n",
      "Epoch: 6476 mean train loss:  1.33966563e-02, mean val. rec. loss:  6.82068155e-03\n",
      "Epoch: 6477 mean train loss:  1.33940436e-02, mean val. rec. loss:  6.81918243e-03\n",
      "Epoch: 6478 mean train loss:  1.33914431e-02, mean val. rec. loss:  6.81769635e-03\n",
      "Epoch: 6479 mean train loss:  1.33888323e-02, mean val. rec. loss:  6.81620914e-03\n",
      "Epoch: 6480 mean train loss:  1.33862261e-02, mean val. rec. loss:  6.81470776e-03\n",
      "Epoch: 6481 mean train loss:  1.33836256e-02, mean val. rec. loss:  6.81320581e-03\n",
      "Epoch: 6482 mean train loss:  1.33810306e-02, mean val. rec. loss:  6.81173164e-03\n",
      "Epoch: 6483 mean train loss:  1.33784329e-02, mean val. rec. loss:  6.81024726e-03\n",
      "Epoch: 6484 mean train loss:  1.33758407e-02, mean val. rec. loss:  6.80876175e-03\n",
      "Epoch: 6485 mean train loss:  1.33732281e-02, mean val. rec. loss:  6.80727341e-03\n",
      "Epoch: 6486 mean train loss:  1.33706350e-02, mean val. rec. loss:  6.80578393e-03\n",
      "Epoch: 6487 mean train loss:  1.33680474e-02, mean val. rec. loss:  6.80433527e-03\n",
      "Epoch: 6488 mean train loss:  1.33654404e-02, mean val. rec. loss:  6.80283786e-03\n",
      "Epoch: 6489 mean train loss:  1.33628519e-02, mean val. rec. loss:  6.80136539e-03\n",
      "Epoch: 6490 mean train loss:  1.33602598e-02, mean val. rec. loss:  6.79987591e-03\n",
      "Epoch: 6491 mean train loss:  1.33576713e-02, mean val. rec. loss:  6.79841081e-03\n",
      "Epoch: 6492 mean train loss:  1.33550810e-02, mean val. rec. loss:  6.79697633e-03\n",
      "Epoch: 6493 mean train loss:  1.33524926e-02, mean val. rec. loss:  6.79550046e-03\n",
      "Epoch: 6494 mean train loss:  1.33498985e-02, mean val. rec. loss:  6.79400531e-03\n",
      "Epoch: 6495 mean train loss:  1.33473203e-02, mean val. rec. loss:  6.79253568e-03\n",
      "Epoch: 6496 mean train loss:  1.33447272e-02, mean val. rec. loss:  6.79107795e-03\n",
      "Epoch: 6497 mean train loss:  1.33421369e-02, mean val. rec. loss:  6.78961853e-03\n",
      "Epoch: 6498 mean train loss:  1.33395624e-02, mean val. rec. loss:  6.78815570e-03\n",
      "Epoch: 6499 mean train loss:  1.33369824e-02, mean val. rec. loss:  6.78669287e-03\n",
      "Epoch: 6500 mean train loss:  1.33343995e-02, mean val. rec. loss:  6.78524875e-03\n",
      "Epoch: 6501 mean train loss:  1.33318185e-02, mean val. rec. loss:  6.78378932e-03\n",
      "Epoch: 6502 mean train loss:  1.33292319e-02, mean val. rec. loss:  6.78231742e-03\n",
      "Epoch: 6503 mean train loss:  1.33266565e-02, mean val. rec. loss:  6.78083474e-03\n",
      "Epoch: 6504 mean train loss:  1.33240774e-02, mean val. rec. loss:  6.77939232e-03\n",
      "Epoch: 6505 mean train loss:  1.33215020e-02, mean val. rec. loss:  6.77796748e-03\n",
      "Epoch: 6506 mean train loss:  1.33189303e-02, mean val. rec. loss:  6.77652280e-03\n",
      "Epoch: 6507 mean train loss:  1.33163586e-02, mean val. rec. loss:  6.77508151e-03\n",
      "Epoch: 6508 mean train loss:  1.33137888e-02, mean val. rec. loss:  6.77360791e-03\n",
      "Epoch: 6509 mean train loss:  1.33112208e-02, mean val. rec. loss:  6.77215132e-03\n",
      "Epoch: 6510 mean train loss:  1.33086380e-02, mean val. rec. loss:  6.77070380e-03\n",
      "Epoch: 6511 mean train loss:  1.33060737e-02, mean val. rec. loss:  6.76927328e-03\n",
      "Epoch: 6512 mean train loss:  1.33035095e-02, mean val. rec. loss:  6.76786092e-03\n",
      "Epoch: 6513 mean train loss:  1.33009397e-02, mean val. rec. loss:  6.76641850e-03\n",
      "Epoch: 6514 mean train loss:  1.32983754e-02, mean val. rec. loss:  6.76496020e-03\n",
      "Epoch: 6515 mean train loss:  1.32958168e-02, mean val. rec. loss:  6.76350474e-03\n",
      "Epoch: 6516 mean train loss:  1.32932451e-02, mean val. rec. loss:  6.76207253e-03\n",
      "Epoch: 6517 mean train loss:  1.32906939e-02, mean val. rec. loss:  6.76064996e-03\n",
      "Epoch: 6518 mean train loss:  1.32881250e-02, mean val. rec. loss:  6.75923249e-03\n",
      "Epoch: 6519 mean train loss:  1.32855748e-02, mean val. rec. loss:  6.75780765e-03\n",
      "Epoch: 6520 mean train loss:  1.32830152e-02, mean val. rec. loss:  6.75635842e-03\n",
      "Epoch: 6521 mean train loss:  1.32804491e-02, mean val. rec. loss:  6.75490013e-03\n",
      "Epoch: 6522 mean train loss:  1.32779100e-02, mean val. rec. loss:  6.75349513e-03\n",
      "Epoch: 6523 mean train loss:  1.32753476e-02, mean val. rec. loss:  6.75208673e-03\n",
      "Epoch: 6524 mean train loss:  1.32727908e-02, mean val. rec. loss:  6.75065736e-03\n",
      "Epoch: 6525 mean train loss:  1.32702396e-02, mean val. rec. loss:  6.74922344e-03\n",
      "Epoch: 6526 mean train loss:  1.32676865e-02, mean val. rec. loss:  6.74778726e-03\n",
      "Epoch: 6527 mean train loss:  1.32651456e-02, mean val. rec. loss:  6.74636015e-03\n",
      "Epoch: 6528 mean train loss:  1.32625953e-02, mean val. rec. loss:  6.74495232e-03\n",
      "Epoch: 6529 mean train loss:  1.32600534e-02, mean val. rec. loss:  6.74354449e-03\n",
      "Epoch: 6530 mean train loss:  1.32575060e-02, mean val. rec. loss:  6.74214630e-03\n",
      "Epoch: 6531 mean train loss:  1.32549566e-02, mean val. rec. loss:  6.74072202e-03\n",
      "Epoch: 6532 mean train loss:  1.32524147e-02, mean val. rec. loss:  6.73929491e-03\n",
      "Epoch: 6533 mean train loss:  1.32498784e-02, mean val. rec. loss:  6.73787574e-03\n",
      "Epoch: 6534 mean train loss:  1.32473440e-02, mean val. rec. loss:  6.73646848e-03\n",
      "Epoch: 6535 mean train loss:  1.32448105e-02, mean val. rec. loss:  6.73507879e-03\n",
      "Epoch: 6536 mean train loss:  1.32422565e-02, mean val. rec. loss:  6.73366472e-03\n",
      "Epoch: 6537 mean train loss:  1.32397258e-02, mean val. rec. loss:  6.73224725e-03\n",
      "Epoch: 6538 mean train loss:  1.32372025e-02, mean val. rec. loss:  6.73083488e-03\n",
      "Epoch: 6539 mean train loss:  1.32346587e-02, mean val. rec. loss:  6.72943669e-03\n",
      "Epoch: 6540 mean train loss:  1.32321355e-02, mean val. rec. loss:  6.72804813e-03\n",
      "Epoch: 6541 mean train loss:  1.32295964e-02, mean val. rec. loss:  6.72664654e-03\n",
      "Epoch: 6542 mean train loss:  1.32270759e-02, mean val. rec. loss:  6.72525288e-03\n",
      "Epoch: 6543 mean train loss:  1.32245415e-02, mean val. rec. loss:  6.72384278e-03\n",
      "Epoch: 6544 mean train loss:  1.32220182e-02, mean val. rec. loss:  6.72244005e-03\n",
      "Epoch: 6545 mean train loss:  1.32194949e-02, mean val. rec. loss:  6.72104413e-03\n",
      "Epoch: 6546 mean train loss:  1.32169670e-02, mean val. rec. loss:  6.71966578e-03\n",
      "Epoch: 6547 mean train loss:  1.32144409e-02, mean val. rec. loss:  6.71826702e-03\n",
      "Epoch: 6548 mean train loss:  1.32119344e-02, mean val. rec. loss:  6.71687563e-03\n",
      "Epoch: 6549 mean train loss:  1.32094130e-02, mean val. rec. loss:  6.71549955e-03\n",
      "Epoch: 6550 mean train loss:  1.32068916e-02, mean val. rec. loss:  6.71410986e-03\n",
      "Epoch: 6551 mean train loss:  1.32043739e-02, mean val. rec. loss:  6.71271620e-03\n",
      "Epoch: 6552 mean train loss:  1.32018600e-02, mean val. rec. loss:  6.71132084e-03\n",
      "Epoch: 6553 mean train loss:  1.31993451e-02, mean val. rec. loss:  6.70994987e-03\n",
      "Epoch: 6554 mean train loss:  1.31968349e-02, mean val. rec. loss:  6.70857549e-03\n",
      "Epoch: 6555 mean train loss:  1.31943256e-02, mean val. rec. loss:  6.70720848e-03\n",
      "Epoch: 6556 mean train loss:  1.31918079e-02, mean val. rec. loss:  6.70581482e-03\n",
      "Epoch: 6557 mean train loss:  1.31892976e-02, mean val. rec. loss:  6.70442570e-03\n",
      "Epoch: 6558 mean train loss:  1.31867911e-02, mean val. rec. loss:  6.70304565e-03\n",
      "Epoch: 6559 mean train loss:  1.31842921e-02, mean val. rec. loss:  6.70167921e-03\n",
      "Epoch: 6560 mean train loss:  1.31817772e-02, mean val. rec. loss:  6.70029576e-03\n",
      "Epoch: 6561 mean train loss:  1.31792781e-02, mean val. rec. loss:  6.69891571e-03\n",
      "Epoch: 6562 mean train loss:  1.31767856e-02, mean val. rec. loss:  6.69756514e-03\n",
      "Epoch: 6563 mean train loss:  1.31742753e-02, mean val. rec. loss:  6.69619473e-03\n",
      "Epoch: 6564 mean train loss:  1.31717875e-02, mean val. rec. loss:  6.69482432e-03\n",
      "Epoch: 6565 mean train loss:  1.31692847e-02, mean val. rec. loss:  6.69344484e-03\n",
      "Epoch: 6566 mean train loss:  1.31667791e-02, mean val. rec. loss:  6.69206989e-03\n",
      "Epoch: 6567 mean train loss:  1.31642968e-02, mean val. rec. loss:  6.69071932e-03\n",
      "Epoch: 6568 mean train loss:  1.31618005e-02, mean val. rec. loss:  6.68936876e-03\n",
      "Epoch: 6569 mean train loss:  1.31593033e-02, mean val. rec. loss:  6.68798984e-03\n",
      "Epoch: 6570 mean train loss:  1.31568182e-02, mean val. rec. loss:  6.68662680e-03\n",
      "Epoch: 6571 mean train loss:  1.31543332e-02, mean val. rec. loss:  6.68527397e-03\n",
      "Epoch: 6572 mean train loss:  1.31518471e-02, mean val. rec. loss:  6.68391660e-03\n",
      "Epoch: 6573 mean train loss:  1.31493592e-02, mean val. rec. loss:  6.68254789e-03\n",
      "Epoch: 6574 mean train loss:  1.31468714e-02, mean val. rec. loss:  6.68118485e-03\n",
      "Epoch: 6575 mean train loss:  1.31443900e-02, mean val. rec. loss:  6.67983655e-03\n",
      "Epoch: 6576 mean train loss:  1.31419114e-02, mean val. rec. loss:  6.67850526e-03\n",
      "Epoch: 6577 mean train loss:  1.31394338e-02, mean val. rec. loss:  6.67713882e-03\n",
      "Epoch: 6578 mean train loss:  1.31369477e-02, mean val. rec. loss:  6.67575990e-03\n",
      "Epoch: 6579 mean train loss:  1.31344617e-02, mean val. rec. loss:  6.67441161e-03\n",
      "Epoch: 6580 mean train loss:  1.31319925e-02, mean val. rec. loss:  6.67307692e-03\n",
      "Epoch: 6581 mean train loss:  1.31295232e-02, mean val. rec. loss:  6.67174222e-03\n",
      "Epoch: 6582 mean train loss:  1.31270372e-02, mean val. rec. loss:  6.67037578e-03\n",
      "Epoch: 6583 mean train loss:  1.31245735e-02, mean val. rec. loss:  6.66900764e-03\n",
      "Epoch: 6584 mean train loss:  1.31221033e-02, mean val. rec. loss:  6.66768996e-03\n",
      "Epoch: 6585 mean train loss:  1.31196322e-02, mean val. rec. loss:  6.66635357e-03\n",
      "Epoch: 6586 mean train loss:  1.31171610e-02, mean val. rec. loss:  6.66499620e-03\n",
      "Epoch: 6587 mean train loss:  1.31147020e-02, mean val. rec. loss:  6.66364450e-03\n",
      "Epoch: 6588 mean train loss:  1.31122318e-02, mean val. rec. loss:  6.66232568e-03\n",
      "Epoch: 6589 mean train loss:  1.31097756e-02, mean val. rec. loss:  6.66099269e-03\n",
      "Epoch: 6590 mean train loss:  1.31073147e-02, mean val. rec. loss:  6.65965517e-03\n",
      "Epoch: 6591 mean train loss:  1.31048510e-02, mean val. rec. loss:  6.65830347e-03\n",
      "Epoch: 6592 mean train loss:  1.31023892e-02, mean val. rec. loss:  6.65695801e-03\n",
      "Epoch: 6593 mean train loss:  1.30999274e-02, mean val. rec. loss:  6.65563295e-03\n",
      "Epoch: 6594 mean train loss:  1.30974674e-02, mean val. rec. loss:  6.65431471e-03\n",
      "Epoch: 6595 mean train loss:  1.30950168e-02, mean val. rec. loss:  6.65297264e-03\n",
      "Epoch: 6596 mean train loss:  1.30925615e-02, mean val. rec. loss:  6.65163115e-03\n",
      "Epoch: 6597 mean train loss:  1.30901118e-02, mean val. rec. loss:  6.65029533e-03\n",
      "Epoch: 6598 mean train loss:  1.30876621e-02, mean val. rec. loss:  6.64896744e-03\n",
      "Epoch: 6599 mean train loss:  1.30852161e-02, mean val. rec. loss:  6.64766847e-03\n",
      "Epoch: 6600 mean train loss:  1.30827748e-02, mean val. rec. loss:  6.64633888e-03\n",
      "Epoch: 6601 mean train loss:  1.30803223e-02, mean val. rec. loss:  6.64500646e-03\n",
      "Epoch: 6602 mean train loss:  1.30778735e-02, mean val. rec. loss:  6.64367687e-03\n",
      "Epoch: 6603 mean train loss:  1.30754387e-02, mean val. rec. loss:  6.64236316e-03\n",
      "Epoch: 6604 mean train loss:  1.30729843e-02, mean val. rec. loss:  6.64103697e-03\n",
      "Epoch: 6605 mean train loss:  1.30705541e-02, mean val. rec. loss:  6.63970455e-03\n",
      "Epoch: 6606 mean train loss:  1.30681026e-02, mean val. rec. loss:  6.63840615e-03\n",
      "Epoch: 6607 mean train loss:  1.30656771e-02, mean val. rec. loss:  6.63710151e-03\n",
      "Epoch: 6608 mean train loss:  1.30632329e-02, mean val. rec. loss:  6.63577022e-03\n",
      "Epoch: 6609 mean train loss:  1.30608084e-02, mean val. rec. loss:  6.63443666e-03\n",
      "Epoch: 6610 mean train loss:  1.30583707e-02, mean val. rec. loss:  6.63312465e-03\n",
      "Epoch: 6611 mean train loss:  1.30559331e-02, mean val. rec. loss:  6.63181944e-03\n",
      "Epoch: 6612 mean train loss:  1.30534955e-02, mean val. rec. loss:  6.63052104e-03\n",
      "Epoch: 6613 mean train loss:  1.30510728e-02, mean val. rec. loss:  6.62920223e-03\n",
      "Epoch: 6614 mean train loss:  1.30486510e-02, mean val. rec. loss:  6.62788455e-03\n",
      "Epoch: 6615 mean train loss:  1.30462246e-02, mean val. rec. loss:  6.62657027e-03\n",
      "Epoch: 6616 mean train loss:  1.30437991e-02, mean val. rec. loss:  6.62527640e-03\n",
      "Epoch: 6617 mean train loss:  1.30413745e-02, mean val. rec. loss:  6.62397119e-03\n",
      "Epoch: 6618 mean train loss:  1.30389453e-02, mean val. rec. loss:  6.62264557e-03\n",
      "Epoch: 6619 mean train loss:  1.30365152e-02, mean val. rec. loss:  6.62131429e-03\n",
      "Epoch: 6620 mean train loss:  1.30341008e-02, mean val. rec. loss:  6.62001191e-03\n",
      "Epoch: 6621 mean train loss:  1.30316893e-02, mean val. rec. loss:  6.61871748e-03\n",
      "Epoch: 6622 mean train loss:  1.30292759e-02, mean val. rec. loss:  6.61743098e-03\n",
      "Epoch: 6623 mean train loss:  1.30268476e-02, mean val. rec. loss:  6.61610650e-03\n",
      "Epoch: 6624 mean train loss:  1.30244407e-02, mean val. rec. loss:  6.61479505e-03\n",
      "Epoch: 6625 mean train loss:  1.30220161e-02, mean val. rec. loss:  6.61348985e-03\n",
      "Epoch: 6626 mean train loss:  1.30196148e-02, mean val. rec. loss:  6.61218634e-03\n",
      "Epoch: 6627 mean train loss:  1.30171940e-02, mean val. rec. loss:  6.61088624e-03\n",
      "Epoch: 6628 mean train loss:  1.30147974e-02, mean val. rec. loss:  6.60959634e-03\n",
      "Epoch: 6629 mean train loss:  1.30123821e-02, mean val. rec. loss:  6.60831324e-03\n",
      "Epoch: 6630 mean train loss:  1.30099706e-02, mean val. rec. loss:  6.60699670e-03\n",
      "Epoch: 6631 mean train loss:  1.30075795e-02, mean val. rec. loss:  6.60568695e-03\n",
      "Epoch: 6632 mean train loss:  1.30051717e-02, mean val. rec. loss:  6.60438402e-03\n",
      "Epoch: 6633 mean train loss:  1.30027657e-02, mean val. rec. loss:  6.60311056e-03\n",
      "Epoch: 6634 mean train loss:  1.30003617e-02, mean val. rec. loss:  6.60183200e-03\n",
      "Epoch: 6635 mean train loss:  1.29979622e-02, mean val. rec. loss:  6.60052339e-03\n",
      "Epoch: 6636 mean train loss:  1.29955665e-02, mean val. rec. loss:  6.59920628e-03\n",
      "Epoch: 6637 mean train loss:  1.29931652e-02, mean val. rec. loss:  6.59791411e-03\n",
      "Epoch: 6638 mean train loss:  1.29907732e-02, mean val. rec. loss:  6.59664292e-03\n",
      "Epoch: 6639 mean train loss:  1.29883812e-02, mean val. rec. loss:  6.59538138e-03\n",
      "Epoch: 6640 mean train loss:  1.29859911e-02, mean val. rec. loss:  6.59408467e-03\n",
      "Epoch: 6641 mean train loss:  1.29835861e-02, mean val. rec. loss:  6.59275395e-03\n",
      "Epoch: 6642 mean train loss:  1.29811997e-02, mean val. rec. loss:  6.59144704e-03\n",
      "Epoch: 6643 mean train loss:  1.29788180e-02, mean val. rec. loss:  6.59019287e-03\n",
      "Epoch: 6644 mean train loss:  1.29764167e-02, mean val. rec. loss:  6.58891998e-03\n",
      "Epoch: 6645 mean train loss:  1.29740387e-02, mean val. rec. loss:  6.58763291e-03\n",
      "Epoch: 6646 mean train loss:  1.29716439e-02, mean val. rec. loss:  6.58633905e-03\n",
      "Epoch: 6647 mean train loss:  1.29692724e-02, mean val. rec. loss:  6.58505255e-03\n",
      "Epoch: 6648 mean train loss:  1.29668813e-02, mean val. rec. loss:  6.58376662e-03\n",
      "Epoch: 6649 mean train loss:  1.29644977e-02, mean val. rec. loss:  6.58247899e-03\n",
      "Epoch: 6650 mean train loss:  1.29621262e-02, mean val. rec. loss:  6.58120667e-03\n",
      "Epoch: 6651 mean train loss:  1.29597398e-02, mean val. rec. loss:  6.57994228e-03\n",
      "Epoch: 6652 mean train loss:  1.29573627e-02, mean val. rec. loss:  6.57865012e-03\n",
      "Epoch: 6653 mean train loss:  1.29549819e-02, mean val. rec. loss:  6.57733981e-03\n",
      "Epoch: 6654 mean train loss:  1.29526039e-02, mean val. rec. loss:  6.57605615e-03\n",
      "Epoch: 6655 mean train loss:  1.29502278e-02, mean val. rec. loss:  6.57480537e-03\n",
      "Epoch: 6656 mean train loss:  1.29478553e-02, mean val. rec. loss:  6.57354439e-03\n",
      "Epoch: 6657 mean train loss:  1.29454857e-02, mean val. rec. loss:  6.57225336e-03\n",
      "Epoch: 6658 mean train loss:  1.29431170e-02, mean val. rec. loss:  6.57095325e-03\n",
      "Epoch: 6659 mean train loss:  1.29407492e-02, mean val. rec. loss:  6.56967413e-03\n",
      "Epoch: 6660 mean train loss:  1.29383861e-02, mean val. rec. loss:  6.56841314e-03\n",
      "Epoch: 6661 mean train loss:  1.29360127e-02, mean val. rec. loss:  6.56715159e-03\n",
      "Epoch: 6662 mean train loss:  1.29336450e-02, mean val. rec. loss:  6.56587644e-03\n",
      "Epoch: 6663 mean train loss:  1.29312874e-02, mean val. rec. loss:  6.56459108e-03\n",
      "Epoch: 6664 mean train loss:  1.29289131e-02, mean val. rec. loss:  6.56330401e-03\n",
      "Epoch: 6665 mean train loss:  1.29265575e-02, mean val. rec. loss:  6.56203396e-03\n",
      "Epoch: 6666 mean train loss:  1.29241887e-02, mean val. rec. loss:  6.56076561e-03\n",
      "Epoch: 6667 mean train loss:  1.29218415e-02, mean val. rec. loss:  6.55951199e-03\n",
      "Epoch: 6668 mean train loss:  1.29194737e-02, mean val. rec. loss:  6.55823627e-03\n",
      "Epoch: 6669 mean train loss:  1.29171124e-02, mean val. rec. loss:  6.55693503e-03\n",
      "Epoch: 6670 mean train loss:  1.29147698e-02, mean val. rec. loss:  6.55567292e-03\n",
      "Epoch: 6671 mean train loss:  1.29124076e-02, mean val. rec. loss:  6.55441760e-03\n",
      "Epoch: 6672 mean train loss:  1.29100510e-02, mean val. rec. loss:  6.55314075e-03\n",
      "Epoch: 6673 mean train loss:  1.29076916e-02, mean val. rec. loss:  6.55186672e-03\n",
      "Epoch: 6674 mean train loss:  1.29053434e-02, mean val. rec. loss:  6.55060744e-03\n",
      "Epoch: 6675 mean train loss:  1.29029933e-02, mean val. rec. loss:  6.54932832e-03\n",
      "Epoch: 6676 mean train loss:  1.29006414e-02, mean val. rec. loss:  6.54806847e-03\n",
      "Epoch: 6677 mean train loss:  1.28982969e-02, mean val. rec. loss:  6.54678651e-03\n",
      "Epoch: 6678 mean train loss:  1.28959514e-02, mean val. rec. loss:  6.54554707e-03\n",
      "Epoch: 6679 mean train loss:  1.28936097e-02, mean val. rec. loss:  6.54428552e-03\n",
      "Epoch: 6680 mean train loss:  1.28912457e-02, mean val. rec. loss:  6.54299166e-03\n",
      "Epoch: 6681 mean train loss:  1.28889077e-02, mean val. rec. loss:  6.54171140e-03\n",
      "Epoch: 6682 mean train loss:  1.28865706e-02, mean val. rec. loss:  6.54046289e-03\n",
      "Epoch: 6683 mean train loss:  1.28842178e-02, mean val. rec. loss:  6.53922005e-03\n",
      "Epoch: 6684 mean train loss:  1.28818854e-02, mean val. rec. loss:  6.53794149e-03\n",
      "Epoch: 6685 mean train loss:  1.28795362e-02, mean val. rec. loss:  6.53666350e-03\n",
      "Epoch: 6686 mean train loss:  1.28772103e-02, mean val. rec. loss:  6.53540479e-03\n",
      "Epoch: 6687 mean train loss:  1.28748612e-02, mean val. rec. loss:  6.53413870e-03\n",
      "Epoch: 6688 mean train loss:  1.28725260e-02, mean val. rec. loss:  6.53288112e-03\n",
      "Epoch: 6689 mean train loss:  1.28701945e-02, mean val. rec. loss:  6.53162467e-03\n",
      "Epoch: 6690 mean train loss:  1.28678575e-02, mean val. rec. loss:  6.53035916e-03\n",
      "Epoch: 6691 mean train loss:  1.28655214e-02, mean val. rec. loss:  6.52910725e-03\n",
      "Epoch: 6692 mean train loss:  1.28631853e-02, mean val. rec. loss:  6.52783266e-03\n",
      "Epoch: 6693 mean train loss:  1.28608501e-02, mean val. rec. loss:  6.52655920e-03\n",
      "Epoch: 6694 mean train loss:  1.28585167e-02, mean val. rec. loss:  6.52531410e-03\n",
      "Epoch: 6695 mean train loss:  1.28561871e-02, mean val. rec. loss:  6.52406729e-03\n",
      "Epoch: 6696 mean train loss:  1.28538594e-02, mean val. rec. loss:  6.52279780e-03\n",
      "Epoch: 6697 mean train loss:  1.28515354e-02, mean val. rec. loss:  6.52152094e-03\n",
      "Epoch: 6698 mean train loss:  1.28492095e-02, mean val. rec. loss:  6.52026790e-03\n",
      "Epoch: 6699 mean train loss:  1.28468874e-02, mean val. rec. loss:  6.51901939e-03\n",
      "Epoch: 6700 mean train loss:  1.28445522e-02, mean val. rec. loss:  6.51775501e-03\n",
      "Epoch: 6701 mean train loss:  1.28422319e-02, mean val. rec. loss:  6.51647815e-03\n",
      "Epoch: 6702 mean train loss:  1.28399135e-02, mean val. rec. loss:  6.51520469e-03\n",
      "Epoch: 6703 mean train loss:  1.28375801e-02, mean val. rec. loss:  6.51396979e-03\n",
      "Epoch: 6704 mean train loss:  1.28352692e-02, mean val. rec. loss:  6.51273092e-03\n",
      "Epoch: 6705 mean train loss:  1.28329358e-02, mean val. rec. loss:  6.51145123e-03\n",
      "Epoch: 6706 mean train loss:  1.28306304e-02, mean val. rec. loss:  6.51018515e-03\n",
      "Epoch: 6707 mean train loss:  1.28283018e-02, mean val. rec. loss:  6.50894628e-03\n",
      "Epoch: 6708 mean train loss:  1.28259778e-02, mean val. rec. loss:  6.50767679e-03\n",
      "Epoch: 6709 mean train loss:  1.28236770e-02, mean val. rec. loss:  6.50642658e-03\n",
      "Epoch: 6710 mean train loss:  1.28213530e-02, mean val. rec. loss:  6.50515766e-03\n",
      "Epoch: 6711 mean train loss:  1.28190364e-02, mean val. rec. loss:  6.50390122e-03\n",
      "Epoch: 6712 mean train loss:  1.28167189e-02, mean val. rec. loss:  6.50264930e-03\n",
      "Epoch: 6713 mean train loss:  1.28144052e-02, mean val. rec. loss:  6.50138265e-03\n",
      "Epoch: 6714 mean train loss:  1.28120923e-02, mean val. rec. loss:  6.50012167e-03\n",
      "Epoch: 6715 mean train loss:  1.28097776e-02, mean val. rec. loss:  6.49886749e-03\n",
      "Epoch: 6716 mean train loss:  1.28074666e-02, mean val. rec. loss:  6.49761728e-03\n",
      "Epoch: 6717 mean train loss:  1.28051594e-02, mean val. rec. loss:  6.49636934e-03\n",
      "Epoch: 6718 mean train loss:  1.28028559e-02, mean val. rec. loss:  6.49510269e-03\n",
      "Epoch: 6719 mean train loss:  1.28005309e-02, mean val. rec. loss:  6.49382470e-03\n",
      "Epoch: 6720 mean train loss:  1.27982265e-02, mean val. rec. loss:  6.49258413e-03\n",
      "Epoch: 6721 mean train loss:  1.27959285e-02, mean val. rec. loss:  6.49133562e-03\n",
      "Epoch: 6722 mean train loss:  1.27936082e-02, mean val. rec. loss:  6.49007464e-03\n",
      "Epoch: 6723 mean train loss:  1.27913121e-02, mean val. rec. loss:  6.48880062e-03\n",
      "Epoch: 6724 mean train loss:  1.27889946e-02, mean val. rec. loss:  6.48754474e-03\n",
      "Epoch: 6725 mean train loss:  1.27867014e-02, mean val. rec. loss:  6.48630417e-03\n",
      "Epoch: 6726 mean train loss:  1.27843913e-02, mean val. rec. loss:  6.48504035e-03\n",
      "Epoch: 6727 mean train loss:  1.27820980e-02, mean val. rec. loss:  6.48379298e-03\n",
      "Epoch: 6728 mean train loss:  1.27797908e-02, mean val. rec. loss:  6.48252349e-03\n",
      "Epoch: 6729 mean train loss:  1.27774854e-02, mean val. rec. loss:  6.48125910e-03\n",
      "Epoch: 6730 mean train loss:  1.27751800e-02, mean val. rec. loss:  6.48000266e-03\n",
      "Epoch: 6731 mean train loss:  1.27728895e-02, mean val. rec. loss:  6.47874735e-03\n",
      "Epoch: 6732 mean train loss:  1.27705953e-02, mean val. rec. loss:  6.47751131e-03\n",
      "Epoch: 6733 mean train loss:  1.27682936e-02, mean val. rec. loss:  6.47625827e-03\n",
      "Epoch: 6734 mean train loss:  1.27659938e-02, mean val. rec. loss:  6.47498708e-03\n",
      "Epoch: 6735 mean train loss:  1.27636977e-02, mean val. rec. loss:  6.47371646e-03\n",
      "Epoch: 6736 mean train loss:  1.27613905e-02, mean val. rec. loss:  6.47244584e-03\n",
      "Epoch: 6737 mean train loss:  1.27590888e-02, mean val. rec. loss:  6.47119053e-03\n",
      "Epoch: 6738 mean train loss:  1.27567983e-02, mean val. rec. loss:  6.46993918e-03\n",
      "Epoch: 6739 mean train loss:  1.27545078e-02, mean val. rec. loss:  6.46868784e-03\n",
      "Epoch: 6740 mean train loss:  1.27522192e-02, mean val. rec. loss:  6.46743933e-03\n",
      "Epoch: 6741 mean train loss:  1.27499129e-02, mean val. rec. loss:  6.46617268e-03\n",
      "Epoch: 6742 mean train loss:  1.27476270e-02, mean val. rec. loss:  6.46489469e-03\n",
      "Epoch: 6743 mean train loss:  1.27453207e-02, mean val. rec. loss:  6.46364958e-03\n",
      "Epoch: 6744 mean train loss:  1.27430404e-02, mean val. rec. loss:  6.46241128e-03\n",
      "Epoch: 6745 mean train loss:  1.27407378e-02, mean val. rec. loss:  6.46113612e-03\n",
      "Epoch: 6746 mean train loss:  1.27384613e-02, mean val. rec. loss:  6.45987174e-03\n",
      "Epoch: 6747 mean train loss:  1.27361606e-02, mean val. rec. loss:  6.45862153e-03\n",
      "Epoch: 6748 mean train loss:  1.27338654e-02, mean val. rec. loss:  6.45736508e-03\n",
      "Epoch: 6749 mean train loss:  1.27315917e-02, mean val. rec. loss:  6.45610070e-03\n",
      "Epoch: 6750 mean train loss:  1.27292975e-02, mean val. rec. loss:  6.45483291e-03\n",
      "Epoch: 6751 mean train loss:  1.27270061e-02, mean val. rec. loss:  6.45358384e-03\n",
      "Epoch: 6752 mean train loss:  1.27247146e-02, mean val. rec. loss:  6.45234384e-03\n",
      "Epoch: 6753 mean train loss:  1.27224306e-02, mean val. rec. loss:  6.45105110e-03\n",
      "Epoch: 6754 mean train loss:  1.27201411e-02, mean val. rec. loss:  6.44977198e-03\n",
      "Epoch: 6755 mean train loss:  1.27178543e-02, mean val. rec. loss:  6.44853254e-03\n",
      "Epoch: 6756 mean train loss:  1.27155694e-02, mean val. rec. loss:  6.44729424e-03\n",
      "Epoch: 6757 mean train loss:  1.27132864e-02, mean val. rec. loss:  6.44603552e-03\n",
      "Epoch: 6758 mean train loss:  1.27110089e-02, mean val. rec. loss:  6.44476774e-03\n",
      "Epoch: 6759 mean train loss:  1.27087082e-02, mean val. rec. loss:  6.44348067e-03\n",
      "Epoch: 6760 mean train loss:  1.27064288e-02, mean val. rec. loss:  6.44221629e-03\n",
      "Epoch: 6761 mean train loss:  1.27041532e-02, mean val. rec. loss:  6.44096041e-03\n",
      "Epoch: 6762 mean train loss:  1.27018562e-02, mean val. rec. loss:  6.43970510e-03\n",
      "Epoch: 6763 mean train loss:  1.26995806e-02, mean val. rec. loss:  6.43844979e-03\n",
      "Epoch: 6764 mean train loss:  1.26972995e-02, mean val. rec. loss:  6.43718427e-03\n",
      "Epoch: 6765 mean train loss:  1.26950145e-02, mean val. rec. loss:  6.43589664e-03\n",
      "Epoch: 6766 mean train loss:  1.26927278e-02, mean val. rec. loss:  6.43461184e-03\n",
      "Epoch: 6767 mean train loss:  1.26904559e-02, mean val. rec. loss:  6.43338034e-03\n",
      "Epoch: 6768 mean train loss:  1.26881673e-02, mean val. rec. loss:  6.43212333e-03\n",
      "Epoch: 6769 mean train loss:  1.26858805e-02, mean val. rec. loss:  6.43083343e-03\n",
      "Epoch: 6770 mean train loss:  1.26836152e-02, mean val. rec. loss:  6.42957812e-03\n",
      "Epoch: 6771 mean train loss:  1.26813349e-02, mean val. rec. loss:  6.42831147e-03\n",
      "Epoch: 6772 mean train loss:  1.26790500e-02, mean val. rec. loss:  6.42704935e-03\n",
      "Epoch: 6773 mean train loss:  1.26767707e-02, mean val. rec. loss:  6.42578610e-03\n",
      "Epoch: 6774 mean train loss:  1.26744895e-02, mean val. rec. loss:  6.42452285e-03\n",
      "Epoch: 6775 mean train loss:  1.26722120e-02, mean val. rec. loss:  6.42325847e-03\n",
      "Epoch: 6776 mean train loss:  1.26699336e-02, mean val. rec. loss:  6.42200259e-03\n",
      "Epoch: 6777 mean train loss:  1.26676571e-02, mean val. rec. loss:  6.42071042e-03\n",
      "Epoch: 6778 mean train loss:  1.26653834e-02, mean val. rec. loss:  6.41945455e-03\n",
      "Epoch: 6779 mean train loss:  1.26630966e-02, mean val. rec. loss:  6.41818676e-03\n",
      "Epoch: 6780 mean train loss:  1.26608154e-02, mean val. rec. loss:  6.41691841e-03\n",
      "Epoch: 6781 mean train loss:  1.26585454e-02, mean val. rec. loss:  6.41565175e-03\n",
      "Epoch: 6782 mean train loss:  1.26562745e-02, mean val. rec. loss:  6.41439701e-03\n",
      "Epoch: 6783 mean train loss:  1.26539858e-02, mean val. rec. loss:  6.41312072e-03\n",
      "Epoch: 6784 mean train loss:  1.26517140e-02, mean val. rec. loss:  6.41184386e-03\n",
      "Epoch: 6785 mean train loss:  1.26494281e-02, mean val. rec. loss:  6.41054546e-03\n",
      "Epoch: 6786 mean train loss:  1.26471637e-02, mean val. rec. loss:  6.40928561e-03\n",
      "Epoch: 6787 mean train loss:  1.26448769e-02, mean val. rec. loss:  6.40802916e-03\n",
      "Epoch: 6788 mean train loss:  1.26426134e-02, mean val. rec. loss:  6.40676875e-03\n",
      "Epoch: 6789 mean train loss:  1.26403323e-02, mean val. rec. loss:  6.40548339e-03\n",
      "Epoch: 6790 mean train loss:  1.26380501e-02, mean val. rec. loss:  6.40418668e-03\n",
      "Epoch: 6791 mean train loss:  1.26357894e-02, mean val. rec. loss:  6.40290756e-03\n",
      "Epoch: 6792 mean train loss:  1.26335120e-02, mean val. rec. loss:  6.40166529e-03\n",
      "Epoch: 6793 mean train loss:  1.26312327e-02, mean val. rec. loss:  6.40039637e-03\n",
      "Epoch: 6794 mean train loss:  1.26289571e-02, mean val. rec. loss:  6.39910647e-03\n",
      "Epoch: 6795 mean train loss:  1.26266750e-02, mean val. rec. loss:  6.39783018e-03\n",
      "Epoch: 6796 mean train loss:  1.26244012e-02, mean val. rec. loss:  6.39654028e-03\n",
      "Epoch: 6797 mean train loss:  1.26221284e-02, mean val. rec. loss:  6.39525719e-03\n",
      "Epoch: 6798 mean train loss:  1.26198528e-02, mean val. rec. loss:  6.39400131e-03\n",
      "Epoch: 6799 mean train loss:  1.26175809e-02, mean val. rec. loss:  6.39271821e-03\n",
      "Epoch: 6800 mean train loss:  1.26153100e-02, mean val. rec. loss:  6.39142321e-03\n",
      "Epoch: 6801 mean train loss:  1.26130372e-02, mean val. rec. loss:  6.39014522e-03\n",
      "Epoch: 6802 mean train loss:  1.26107691e-02, mean val. rec. loss:  6.38887687e-03\n",
      "Epoch: 6803 mean train loss:  1.26084823e-02, mean val. rec. loss:  6.38758243e-03\n",
      "Epoch: 6804 mean train loss:  1.26062151e-02, mean val. rec. loss:  6.38630161e-03\n",
      "Epoch: 6805 mean train loss:  1.26039488e-02, mean val. rec. loss:  6.38501964e-03\n",
      "Epoch: 6806 mean train loss:  1.26016601e-02, mean val. rec. loss:  6.38374052e-03\n",
      "Epoch: 6807 mean train loss:  1.25993967e-02, mean val. rec. loss:  6.38245232e-03\n",
      "Epoch: 6808 mean train loss:  1.25971117e-02, mean val. rec. loss:  6.38115845e-03\n",
      "Epoch: 6809 mean train loss:  1.25948492e-02, mean val. rec. loss:  6.37988330e-03\n",
      "Epoch: 6810 mean train loss:  1.25925661e-02, mean val. rec. loss:  6.37860020e-03\n",
      "Epoch: 6811 mean train loss:  1.25902933e-02, mean val. rec. loss:  6.37730974e-03\n",
      "Epoch: 6812 mean train loss:  1.25880243e-02, mean val. rec. loss:  6.37601927e-03\n",
      "Epoch: 6813 mean train loss:  1.25857440e-02, mean val. rec. loss:  6.37473391e-03\n",
      "Epoch: 6814 mean train loss:  1.25834656e-02, mean val. rec. loss:  6.37344741e-03\n",
      "Epoch: 6815 mean train loss:  1.25812068e-02, mean val. rec. loss:  6.37215241e-03\n",
      "Epoch: 6816 mean train loss:  1.25789293e-02, mean val. rec. loss:  6.37086535e-03\n",
      "Epoch: 6817 mean train loss:  1.25766519e-02, mean val. rec. loss:  6.36958055e-03\n",
      "Epoch: 6818 mean train loss:  1.25743809e-02, mean val. rec. loss:  6.36829122e-03\n",
      "Epoch: 6819 mean train loss:  1.25721044e-02, mean val. rec. loss:  6.36698261e-03\n",
      "Epoch: 6820 mean train loss:  1.25698307e-02, mean val. rec. loss:  6.36569158e-03\n",
      "Epoch: 6821 mean train loss:  1.25675569e-02, mean val. rec. loss:  6.36442663e-03\n",
      "Epoch: 6822 mean train loss:  1.25652832e-02, mean val. rec. loss:  6.36312482e-03\n",
      "Epoch: 6823 mean train loss:  1.25630020e-02, mean val. rec. loss:  6.36180828e-03\n",
      "Epoch: 6824 mean train loss:  1.25607208e-02, mean val. rec. loss:  6.36051668e-03\n",
      "Epoch: 6825 mean train loss:  1.25584527e-02, mean val. rec. loss:  6.35924322e-03\n",
      "Epoch: 6826 mean train loss:  1.25561808e-02, mean val. rec. loss:  6.35796070e-03\n",
      "Epoch: 6827 mean train loss:  1.25538922e-02, mean val. rec. loss:  6.35664301e-03\n",
      "Epoch: 6828 mean train loss:  1.25516231e-02, mean val. rec. loss:  6.35535142e-03\n",
      "Epoch: 6829 mean train loss:  1.25493559e-02, mean val. rec. loss:  6.35406435e-03\n",
      "Epoch: 6830 mean train loss:  1.25470719e-02, mean val. rec. loss:  6.35274780e-03\n",
      "Epoch: 6831 mean train loss:  1.25448010e-02, mean val. rec. loss:  6.35145394e-03\n",
      "Epoch: 6832 mean train loss:  1.25425179e-02, mean val. rec. loss:  6.35015780e-03\n",
      "Epoch: 6833 mean train loss:  1.25402498e-02, mean val. rec. loss:  6.34886053e-03\n",
      "Epoch: 6834 mean train loss:  1.25379639e-02, mean val. rec. loss:  6.34753605e-03\n",
      "Epoch: 6835 mean train loss:  1.25356799e-02, mean val. rec. loss:  6.34621553e-03\n",
      "Epoch: 6836 mean train loss:  1.25334174e-02, mean val. rec. loss:  6.34495682e-03\n",
      "Epoch: 6837 mean train loss:  1.25311343e-02, mean val. rec. loss:  6.34365275e-03\n",
      "Epoch: 6838 mean train loss:  1.25288494e-02, mean val. rec. loss:  6.34231352e-03\n",
      "Epoch: 6839 mean train loss:  1.25265813e-02, mean val. rec. loss:  6.34100321e-03\n",
      "Epoch: 6840 mean train loss:  1.25243075e-02, mean val. rec. loss:  6.33973259e-03\n",
      "Epoch: 6841 mean train loss:  1.25220264e-02, mean val. rec. loss:  6.33841377e-03\n",
      "Epoch: 6842 mean train loss:  1.25197470e-02, mean val. rec. loss:  6.33708078e-03\n",
      "Epoch: 6843 mean train loss:  1.25174649e-02, mean val. rec. loss:  6.33579542e-03\n",
      "Epoch: 6844 mean train loss:  1.25151865e-02, mean val. rec. loss:  6.33448511e-03\n",
      "Epoch: 6845 mean train loss:  1.25129035e-02, mean val. rec. loss:  6.33315496e-03\n",
      "Epoch: 6846 mean train loss:  1.25106270e-02, mean val. rec. loss:  6.33183274e-03\n",
      "Epoch: 6847 mean train loss:  1.25083504e-02, mean val. rec. loss:  6.33053944e-03\n",
      "Epoch: 6848 mean train loss:  1.25060665e-02, mean val. rec. loss:  6.32923140e-03\n",
      "Epoch: 6849 mean train loss:  1.25037760e-02, mean val. rec. loss:  6.32788310e-03\n",
      "Epoch: 6850 mean train loss:  1.25014985e-02, mean val. rec. loss:  6.32656882e-03\n",
      "Epoch: 6851 mean train loss:  1.24992220e-02, mean val. rec. loss:  6.32528289e-03\n",
      "Epoch: 6852 mean train loss:  1.24969268e-02, mean val. rec. loss:  6.32395557e-03\n",
      "Epoch: 6853 mean train loss:  1.24946503e-02, mean val. rec. loss:  6.32262202e-03\n",
      "Epoch: 6854 mean train loss:  1.24923766e-02, mean val. rec. loss:  6.32131851e-03\n",
      "Epoch: 6855 mean train loss:  1.24900823e-02, mean val. rec. loss:  6.31997928e-03\n",
      "Epoch: 6856 mean train loss:  1.24878067e-02, mean val. rec. loss:  6.31867238e-03\n",
      "Epoch: 6857 mean train loss:  1.24855116e-02, mean val. rec. loss:  6.31734619e-03\n",
      "Epoch: 6858 mean train loss:  1.24832388e-02, mean val. rec. loss:  6.31601320e-03\n",
      "Epoch: 6859 mean train loss:  1.24809474e-02, mean val. rec. loss:  6.31470062e-03\n",
      "Epoch: 6860 mean train loss:  1.24786587e-02, mean val. rec. loss:  6.31336707e-03\n",
      "Epoch: 6861 mean train loss:  1.24763822e-02, mean val. rec. loss:  6.31204712e-03\n",
      "Epoch: 6862 mean train loss:  1.24740880e-02, mean val. rec. loss:  6.31072093e-03\n",
      "Epoch: 6863 mean train loss:  1.24717975e-02, mean val. rec. loss:  6.30937717e-03\n",
      "Epoch: 6864 mean train loss:  1.24695238e-02, mean val. rec. loss:  6.30805495e-03\n",
      "Epoch: 6865 mean train loss:  1.24672333e-02, mean val. rec. loss:  6.30676052e-03\n",
      "Epoch: 6866 mean train loss:  1.24649409e-02, mean val. rec. loss:  6.30542980e-03\n",
      "Epoch: 6867 mean train loss:  1.24626504e-02, mean val. rec. loss:  6.30405542e-03\n",
      "Epoch: 6868 mean train loss:  1.24603581e-02, mean val. rec. loss:  6.30271109e-03\n",
      "Epoch: 6869 mean train loss:  1.24580694e-02, mean val. rec. loss:  6.30140248e-03\n",
      "Epoch: 6870 mean train loss:  1.24557771e-02, mean val. rec. loss:  6.30008593e-03\n",
      "Epoch: 6871 mean train loss:  1.24534884e-02, mean val. rec. loss:  6.29874444e-03\n",
      "Epoch: 6872 mean train loss:  1.24511961e-02, mean val. rec. loss:  6.29740805e-03\n",
      "Epoch: 6873 mean train loss:  1.24489093e-02, mean val. rec. loss:  6.29606485e-03\n",
      "Epoch: 6874 mean train loss:  1.24466188e-02, mean val. rec. loss:  6.29471372e-03\n",
      "Epoch: 6875 mean train loss:  1.24443283e-02, mean val. rec. loss:  6.29339830e-03\n",
      "Epoch: 6876 mean train loss:  1.24420425e-02, mean val. rec. loss:  6.29205681e-03\n",
      "Epoch: 6877 mean train loss:  1.24397324e-02, mean val. rec. loss:  6.29069094e-03\n",
      "Epoch: 6878 mean train loss:  1.24374429e-02, mean val. rec. loss:  6.28934377e-03\n",
      "Epoch: 6879 mean train loss:  1.24351570e-02, mean val. rec. loss:  6.28803006e-03\n",
      "Epoch: 6880 mean train loss:  1.24328498e-02, mean val. rec. loss:  6.28668913e-03\n",
      "Epoch: 6881 mean train loss:  1.24305593e-02, mean val. rec. loss:  6.28532099e-03\n",
      "Epoch: 6882 mean train loss:  1.24282707e-02, mean val. rec. loss:  6.28396702e-03\n",
      "Epoch: 6883 mean train loss:  1.24259643e-02, mean val. rec. loss:  6.28263176e-03\n",
      "Epoch: 6884 mean train loss:  1.24236748e-02, mean val. rec. loss:  6.28130161e-03\n",
      "Epoch: 6885 mean train loss:  1.24213666e-02, mean val. rec. loss:  6.27993630e-03\n",
      "Epoch: 6886 mean train loss:  1.24190780e-02, mean val. rec. loss:  6.27858744e-03\n",
      "Epoch: 6887 mean train loss:  1.24167716e-02, mean val. rec. loss:  6.27722893e-03\n",
      "Epoch: 6888 mean train loss:  1.24144625e-02, mean val. rec. loss:  6.27586079e-03\n",
      "Epoch: 6889 mean train loss:  1.24121739e-02, mean val. rec. loss:  6.27451816e-03\n",
      "Epoch: 6890 mean train loss:  1.24098676e-02, mean val. rec. loss:  6.27318347e-03\n",
      "Epoch: 6891 mean train loss:  1.24075594e-02, mean val. rec. loss:  6.27179832e-03\n",
      "Epoch: 6892 mean train loss:  1.24052642e-02, mean val. rec. loss:  6.27044322e-03\n",
      "Epoch: 6893 mean train loss:  1.24029635e-02, mean val. rec. loss:  6.26910683e-03\n",
      "Epoch: 6894 mean train loss:  1.24006572e-02, mean val. rec. loss:  6.26775569e-03\n",
      "Epoch: 6895 mean train loss:  1.23983508e-02, mean val. rec. loss:  6.26639776e-03\n",
      "Epoch: 6896 mean train loss:  1.23960436e-02, mean val. rec. loss:  6.26502167e-03\n",
      "Epoch: 6897 mean train loss:  1.23937335e-02, mean val. rec. loss:  6.26363539e-03\n",
      "Epoch: 6898 mean train loss:  1.23914272e-02, mean val. rec. loss:  6.26229219e-03\n",
      "Epoch: 6899 mean train loss:  1.23891162e-02, mean val. rec. loss:  6.26094106e-03\n",
      "Epoch: 6900 mean train loss:  1.23868090e-02, mean val. rec. loss:  6.25957348e-03\n",
      "Epoch: 6901 mean train loss:  1.23845055e-02, mean val. rec. loss:  6.25819230e-03\n",
      "Epoch: 6902 mean train loss:  1.23821982e-02, mean val. rec. loss:  6.25683153e-03\n",
      "Epoch: 6903 mean train loss:  1.23798882e-02, mean val. rec. loss:  6.25547246e-03\n",
      "Epoch: 6904 mean train loss:  1.23775781e-02, mean val. rec. loss:  6.25410545e-03\n",
      "Epoch: 6905 mean train loss:  1.23752634e-02, mean val. rec. loss:  6.25272427e-03\n",
      "Epoch: 6906 mean train loss:  1.23729468e-02, mean val. rec. loss:  6.25135556e-03\n",
      "Epoch: 6907 mean train loss:  1.23706387e-02, mean val. rec. loss:  6.24997324e-03\n",
      "Epoch: 6908 mean train loss:  1.23683286e-02, mean val. rec. loss:  6.24860566e-03\n",
      "Epoch: 6909 mean train loss:  1.23660027e-02, mean val. rec. loss:  6.24725169e-03\n",
      "Epoch: 6910 mean train loss:  1.23636936e-02, mean val. rec. loss:  6.24585577e-03\n",
      "Epoch: 6911 mean train loss:  1.23613845e-02, mean val. rec. loss:  6.24448082e-03\n",
      "Epoch: 6912 mean train loss:  1.23590521e-02, mean val. rec. loss:  6.24310361e-03\n",
      "Epoch: 6913 mean train loss:  1.23567439e-02, mean val. rec. loss:  6.24172186e-03\n",
      "Epoch: 6914 mean train loss:  1.23544152e-02, mean val. rec. loss:  6.24032763e-03\n",
      "Epoch: 6915 mean train loss:  1.23521052e-02, mean val. rec. loss:  6.23893965e-03\n",
      "Epoch: 6916 mean train loss:  1.23497747e-02, mean val. rec. loss:  6.23756016e-03\n",
      "Epoch: 6917 mean train loss:  1.23474656e-02, mean val. rec. loss:  6.23620336e-03\n",
      "Epoch: 6918 mean train loss:  1.23451369e-02, mean val. rec. loss:  6.23480403e-03\n",
      "Epoch: 6919 mean train loss:  1.23428064e-02, mean val. rec. loss:  6.23338203e-03\n",
      "Epoch: 6920 mean train loss:  1.23404944e-02, mean val. rec. loss:  6.23202239e-03\n",
      "Epoch: 6921 mean train loss:  1.23381630e-02, mean val. rec. loss:  6.23064858e-03\n",
      "Epoch: 6922 mean train loss:  1.23358334e-02, mean val. rec. loss:  6.22925095e-03\n",
      "Epoch: 6923 mean train loss:  1.23335140e-02, mean val. rec. loss:  6.22783518e-03\n",
      "Epoch: 6924 mean train loss:  1.23311909e-02, mean val. rec. loss:  6.22646534e-03\n",
      "Epoch: 6925 mean train loss:  1.23288595e-02, mean val. rec. loss:  6.22507678e-03\n",
      "Epoch: 6926 mean train loss:  1.23265308e-02, mean val. rec. loss:  6.22368256e-03\n",
      "Epoch: 6927 mean train loss:  1.23241966e-02, mean val. rec. loss:  6.22228266e-03\n",
      "Epoch: 6928 mean train loss:  1.23218632e-02, mean val. rec. loss:  6.22085385e-03\n",
      "Epoch: 6929 mean train loss:  1.23195327e-02, mean val. rec. loss:  6.21946133e-03\n",
      "Epoch: 6930 mean train loss:  1.23172003e-02, mean val. rec. loss:  6.21809886e-03\n",
      "Epoch: 6931 mean train loss:  1.23148670e-02, mean val. rec. loss:  6.21667855e-03\n",
      "Epoch: 6932 mean train loss:  1.23125327e-02, mean val. rec. loss:  6.21525768e-03\n",
      "Epoch: 6933 mean train loss:  1.23101985e-02, mean val. rec. loss:  6.21386686e-03\n",
      "Epoch: 6934 mean train loss:  1.23078651e-02, mean val. rec. loss:  6.21246923e-03\n",
      "Epoch: 6935 mean train loss:  1.23055327e-02, mean val. rec. loss:  6.21107671e-03\n",
      "Epoch: 6936 mean train loss:  1.23031985e-02, mean val. rec. loss:  6.20965697e-03\n",
      "Epoch: 6937 mean train loss:  1.23008503e-02, mean val. rec. loss:  6.20824120e-03\n",
      "Epoch: 6938 mean train loss:  1.22985076e-02, mean val. rec. loss:  6.20684357e-03\n",
      "Epoch: 6939 mean train loss:  1.22961724e-02, mean val. rec. loss:  6.20545105e-03\n",
      "Epoch: 6940 mean train loss:  1.22938354e-02, mean val. rec. loss:  6.20402848e-03\n",
      "Epoch: 6941 mean train loss:  1.22914797e-02, mean val. rec. loss:  6.20261271e-03\n",
      "Epoch: 6942 mean train loss:  1.22891455e-02, mean val. rec. loss:  6.20119013e-03\n",
      "Epoch: 6943 mean train loss:  1.22868056e-02, mean val. rec. loss:  6.19977663e-03\n",
      "Epoch: 6944 mean train loss:  1.22844509e-02, mean val. rec. loss:  6.19836880e-03\n",
      "Epoch: 6945 mean train loss:  1.22821101e-02, mean val. rec. loss:  6.19694793e-03\n",
      "Epoch: 6946 mean train loss:  1.22797535e-02, mean val. rec. loss:  6.19552649e-03\n",
      "Epoch: 6947 mean train loss:  1.22774146e-02, mean val. rec. loss:  6.19409994e-03\n",
      "Epoch: 6948 mean train loss:  1.22750561e-02, mean val. rec. loss:  6.19267227e-03\n",
      "Epoch: 6949 mean train loss:  1.22727163e-02, mean val. rec. loss:  6.19127181e-03\n",
      "Epoch: 6950 mean train loss:  1.22703541e-02, mean val. rec. loss:  6.18986964e-03\n",
      "Epoch: 6951 mean train loss:  1.22680143e-02, mean val. rec. loss:  6.18843630e-03\n",
      "Epoch: 6952 mean train loss:  1.22656549e-02, mean val. rec. loss:  6.18697007e-03\n",
      "Epoch: 6953 mean train loss:  1.22632936e-02, mean val. rec. loss:  6.18557584e-03\n",
      "Epoch: 6954 mean train loss:  1.22609473e-02, mean val. rec. loss:  6.18417992e-03\n",
      "Epoch: 6955 mean train loss:  1.22585860e-02, mean val. rec. loss:  6.18271879e-03\n",
      "Epoch: 6956 mean train loss:  1.22562266e-02, mean val. rec. loss:  6.18124575e-03\n",
      "Epoch: 6957 mean train loss:  1.22538700e-02, mean val. rec. loss:  6.17985436e-03\n",
      "Epoch: 6958 mean train loss:  1.22515190e-02, mean val. rec. loss:  6.17845844e-03\n",
      "Epoch: 6959 mean train loss:  1.22491521e-02, mean val. rec. loss:  6.17699334e-03\n",
      "Epoch: 6960 mean train loss:  1.22467890e-02, mean val. rec. loss:  6.17554468e-03\n",
      "Epoch: 6961 mean train loss:  1.22444268e-02, mean val. rec. loss:  6.17410340e-03\n",
      "Epoch: 6962 mean train loss:  1.22420553e-02, mean val. rec. loss:  6.17264908e-03\n",
      "Epoch: 6963 mean train loss:  1.22396876e-02, mean val. rec. loss:  6.17123557e-03\n",
      "Epoch: 6964 mean train loss:  1.22373235e-02, mean val. rec. loss:  6.16982491e-03\n",
      "Epoch: 6965 mean train loss:  1.22349567e-02, mean val. rec. loss:  6.16834167e-03\n",
      "Epoch: 6966 mean train loss:  1.22325861e-02, mean val. rec. loss:  6.16685332e-03\n",
      "Epoch: 6967 mean train loss:  1.22302183e-02, mean val. rec. loss:  6.16546703e-03\n",
      "Epoch: 6968 mean train loss:  1.22278505e-02, mean val. rec. loss:  6.16406034e-03\n",
      "Epoch: 6969 mean train loss:  1.22254846e-02, mean val. rec. loss:  6.16256292e-03\n",
      "Epoch: 6970 mean train loss:  1.22231113e-02, mean val. rec. loss:  6.16110122e-03\n",
      "Epoch: 6971 mean train loss:  1.22207454e-02, mean val. rec. loss:  6.15966504e-03\n",
      "Epoch: 6972 mean train loss:  1.22183711e-02, mean val. rec. loss:  6.15823000e-03\n",
      "Epoch: 6973 mean train loss:  1.22159875e-02, mean val. rec. loss:  6.15675299e-03\n",
      "Epoch: 6974 mean train loss:  1.22136048e-02, mean val. rec. loss:  6.15529186e-03\n",
      "Epoch: 6975 mean train loss:  1.22112342e-02, mean val. rec. loss:  6.15385795e-03\n",
      "Epoch: 6976 mean train loss:  1.22088599e-02, mean val. rec. loss:  6.15241326e-03\n",
      "Epoch: 6977 mean train loss:  1.22064670e-02, mean val. rec. loss:  6.15095213e-03\n",
      "Epoch: 6978 mean train loss:  1.22040909e-02, mean val. rec. loss:  6.14950121e-03\n",
      "Epoch: 6979 mean train loss:  1.22017203e-02, mean val. rec. loss:  6.14804405e-03\n",
      "Epoch: 6980 mean train loss:  1.21993199e-02, mean val. rec. loss:  6.14657442e-03\n",
      "Epoch: 6981 mean train loss:  1.21969456e-02, mean val. rec. loss:  6.14514277e-03\n",
      "Epoch: 6982 mean train loss:  1.21945499e-02, mean val. rec. loss:  6.14365159e-03\n",
      "Epoch: 6983 mean train loss:  1.21921691e-02, mean val. rec. loss:  6.14217402e-03\n",
      "Epoch: 6984 mean train loss:  1.21897715e-02, mean val. rec. loss:  6.14071233e-03\n",
      "Epoch: 6985 mean train loss:  1.21873926e-02, mean val. rec. loss:  6.13927388e-03\n",
      "Epoch: 6986 mean train loss:  1.21849922e-02, mean val. rec. loss:  6.13779460e-03\n",
      "Epoch: 6987 mean train loss:  1.21826142e-02, mean val. rec. loss:  6.13631817e-03\n",
      "Epoch: 6988 mean train loss:  1.21802129e-02, mean val. rec. loss:  6.13485534e-03\n",
      "Epoch: 6989 mean train loss:  1.21778228e-02, mean val. rec. loss:  6.13340385e-03\n",
      "Epoch: 6990 mean train loss:  1.21754262e-02, mean val. rec. loss:  6.13191380e-03\n",
      "Epoch: 6991 mean train loss:  1.21730239e-02, mean val. rec. loss:  6.13043283e-03\n",
      "Epoch: 6992 mean train loss:  1.21706347e-02, mean val. rec. loss:  6.12896093e-03\n",
      "Epoch: 6993 mean train loss:  1.21682381e-02, mean val. rec. loss:  6.12747031e-03\n",
      "Epoch: 6994 mean train loss:  1.21658331e-02, mean val. rec. loss:  6.12600578e-03\n",
      "Epoch: 6995 mean train loss:  1.21634271e-02, mean val. rec. loss:  6.12454352e-03\n",
      "Epoch: 6996 mean train loss:  1.21610202e-02, mean val. rec. loss:  6.12303193e-03\n",
      "Epoch: 6997 mean train loss:  1.21586236e-02, mean val. rec. loss:  6.12155719e-03\n",
      "Epoch: 6998 mean train loss:  1.21562288e-02, mean val. rec. loss:  6.12008926e-03\n",
      "Epoch: 6999 mean train loss:  1.21538201e-02, mean val. rec. loss:  6.11860772e-03\n",
      "Epoch: 7000 mean train loss:  1.21514058e-02, mean val. rec. loss:  6.11709840e-03\n",
      "Epoch: 7001 mean train loss:  1.21490017e-02, mean val. rec. loss:  6.11562366e-03\n",
      "Epoch: 7002 mean train loss:  1.21465892e-02, mean val. rec. loss:  6.11414439e-03\n",
      "Epoch: 7003 mean train loss:  1.21441833e-02, mean val. rec. loss:  6.11266568e-03\n",
      "Epoch: 7004 mean train loss:  1.21417671e-02, mean val. rec. loss:  6.11115693e-03\n",
      "Epoch: 7005 mean train loss:  1.21393574e-02, mean val. rec. loss:  6.10967198e-03\n",
      "Epoch: 7006 mean train loss:  1.21369412e-02, mean val. rec. loss:  6.10817060e-03\n",
      "Epoch: 7007 mean train loss:  1.21345269e-02, mean val. rec. loss:  6.10669359e-03\n",
      "Epoch: 7008 mean train loss:  1.21321032e-02, mean val. rec. loss:  6.10518200e-03\n",
      "Epoch: 7009 mean train loss:  1.21296777e-02, mean val. rec. loss:  6.10367552e-03\n",
      "Epoch: 7010 mean train loss:  1.21272587e-02, mean val. rec. loss:  6.10218887e-03\n",
      "Epoch: 7011 mean train loss:  1.21248435e-02, mean val. rec. loss:  6.10069599e-03\n",
      "Epoch: 7012 mean train loss:  1.21224263e-02, mean val. rec. loss:  6.09920198e-03\n",
      "Epoch: 7013 mean train loss:  1.21199953e-02, mean val. rec. loss:  6.09768188e-03\n",
      "Epoch: 7014 mean train loss:  1.21175679e-02, mean val. rec. loss:  6.09617256e-03\n",
      "Epoch: 7015 mean train loss:  1.21151480e-02, mean val. rec. loss:  6.09467741e-03\n",
      "Epoch: 7016 mean train loss:  1.21127262e-02, mean val. rec. loss:  6.09318453e-03\n",
      "Epoch: 7017 mean train loss:  1.21102886e-02, mean val. rec. loss:  6.09165990e-03\n",
      "Epoch: 7018 mean train loss:  1.21078668e-02, mean val. rec. loss:  6.09013924e-03\n",
      "Epoch: 7019 mean train loss:  1.21054283e-02, mean val. rec. loss:  6.08865373e-03\n",
      "Epoch: 7020 mean train loss:  1.21029981e-02, mean val. rec. loss:  6.08715461e-03\n",
      "Epoch: 7021 mean train loss:  1.21005680e-02, mean val. rec. loss:  6.08563281e-03\n",
      "Epoch: 7022 mean train loss:  1.20981341e-02, mean val. rec. loss:  6.08410081e-03\n",
      "Epoch: 7023 mean train loss:  1.20956909e-02, mean val. rec. loss:  6.08258639e-03\n",
      "Epoch: 7024 mean train loss:  1.20932561e-02, mean val. rec. loss:  6.08109577e-03\n",
      "Epoch: 7025 mean train loss:  1.20908101e-02, mean val. rec. loss:  6.07958532e-03\n",
      "Epoch: 7026 mean train loss:  1.20883855e-02, mean val. rec. loss:  6.07805331e-03\n",
      "Epoch: 7027 mean train loss:  1.20859339e-02, mean val. rec. loss:  6.07651451e-03\n",
      "Epoch: 7028 mean train loss:  1.20834917e-02, mean val. rec. loss:  6.07501086e-03\n",
      "Epoch: 7029 mean train loss:  1.20810531e-02, mean val. rec. loss:  6.07349473e-03\n",
      "Epoch: 7030 mean train loss:  1.20786016e-02, mean val. rec. loss:  6.07196273e-03\n",
      "Epoch: 7031 mean train loss:  1.20761491e-02, mean val. rec. loss:  6.07045907e-03\n",
      "Epoch: 7032 mean train loss:  1.20737161e-02, mean val. rec. loss:  6.06893671e-03\n",
      "Epoch: 7033 mean train loss:  1.20712627e-02, mean val. rec. loss:  6.06738316e-03\n",
      "Epoch: 7034 mean train loss:  1.20688083e-02, mean val. rec. loss:  6.06588234e-03\n",
      "Epoch: 7035 mean train loss:  1.20663521e-02, mean val. rec. loss:  6.06434694e-03\n",
      "Epoch: 7036 mean train loss:  1.20638968e-02, mean val. rec. loss:  6.06278545e-03\n",
      "Epoch: 7037 mean train loss:  1.20614489e-02, mean val. rec. loss:  6.06126139e-03\n",
      "Epoch: 7038 mean train loss:  1.20590039e-02, mean val. rec. loss:  6.05977985e-03\n",
      "Epoch: 7039 mean train loss:  1.20565448e-02, mean val. rec. loss:  6.05821099e-03\n",
      "Epoch: 7040 mean train loss:  1.20540858e-02, mean val. rec. loss:  6.05665574e-03\n",
      "Epoch: 7041 mean train loss:  1.20516277e-02, mean val. rec. loss:  6.05515719e-03\n",
      "Epoch: 7042 mean train loss:  1.20491650e-02, mean val. rec. loss:  6.05360705e-03\n",
      "Epoch: 7043 mean train loss:  1.20467041e-02, mean val. rec. loss:  6.05204783e-03\n",
      "Epoch: 7044 mean train loss:  1.20442274e-02, mean val. rec. loss:  6.05052433e-03\n",
      "Epoch: 7045 mean train loss:  1.20417563e-02, mean val. rec. loss:  6.04899346e-03\n",
      "Epoch: 7046 mean train loss:  1.20392935e-02, mean val. rec. loss:  6.04745012e-03\n",
      "Epoch: 7047 mean train loss:  1.20368298e-02, mean val. rec. loss:  6.04589827e-03\n",
      "Epoch: 7048 mean train loss:  1.20343596e-02, mean val. rec. loss:  6.04435153e-03\n",
      "Epoch: 7049 mean train loss:  1.20318960e-02, mean val. rec. loss:  6.04279685e-03\n",
      "Epoch: 7050 mean train loss:  1.20294211e-02, mean val. rec. loss:  6.04127902e-03\n",
      "Epoch: 7051 mean train loss:  1.20269407e-02, mean val. rec. loss:  6.03970506e-03\n",
      "Epoch: 7052 mean train loss:  1.20244695e-02, mean val. rec. loss:  6.03814528e-03\n",
      "Epoch: 7053 mean train loss:  1.20220031e-02, mean val. rec. loss:  6.03661441e-03\n",
      "Epoch: 7054 mean train loss:  1.20195105e-02, mean val. rec. loss:  6.03505406e-03\n",
      "Epoch: 7055 mean train loss:  1.20170422e-02, mean val. rec. loss:  6.03350164e-03\n",
      "Epoch: 7056 mean train loss:  1.20145618e-02, mean val. rec. loss:  6.03193392e-03\n",
      "Epoch: 7057 mean train loss:  1.20120739e-02, mean val. rec. loss:  6.03037924e-03\n",
      "Epoch: 7058 mean train loss:  1.20096018e-02, mean val. rec. loss:  6.02885234e-03\n",
      "Epoch: 7059 mean train loss:  1.20071083e-02, mean val. rec. loss:  6.02729596e-03\n",
      "Epoch: 7060 mean train loss:  1.20046316e-02, mean val. rec. loss:  6.02571236e-03\n",
      "Epoch: 7061 mean train loss:  1.20021344e-02, mean val. rec. loss:  6.02413896e-03\n",
      "Epoch: 7062 mean train loss:  1.19996484e-02, mean val. rec. loss:  6.02259959e-03\n",
      "Epoch: 7063 mean train loss:  1.19971605e-02, mean val. rec. loss:  6.02104264e-03\n",
      "Epoch: 7064 mean train loss:  1.19946615e-02, mean val. rec. loss:  6.01946074e-03\n",
      "Epoch: 7065 mean train loss:  1.19921810e-02, mean val. rec. loss:  6.01791627e-03\n",
      "Epoch: 7066 mean train loss:  1.19896782e-02, mean val. rec. loss:  6.01634117e-03\n",
      "Epoch: 7067 mean train loss:  1.19871810e-02, mean val. rec. loss:  6.01475644e-03\n",
      "Epoch: 7068 mean train loss:  1.19846866e-02, mean val. rec. loss:  6.01321253e-03\n",
      "Epoch: 7069 mean train loss:  1.19821960e-02, mean val. rec. loss:  6.01163517e-03\n",
      "Epoch: 7070 mean train loss:  1.19796913e-02, mean val. rec. loss:  6.01005044e-03\n",
      "Epoch: 7071 mean train loss:  1.19771885e-02, mean val. rec. loss:  6.00848612e-03\n",
      "Epoch: 7072 mean train loss:  1.19746802e-02, mean val. rec. loss:  6.00690762e-03\n",
      "Epoch: 7073 mean train loss:  1.19721755e-02, mean val. rec. loss:  6.00530872e-03\n",
      "Epoch: 7074 mean train loss:  1.19696783e-02, mean val. rec. loss:  6.00377104e-03\n",
      "Epoch: 7075 mean train loss:  1.19671718e-02, mean val. rec. loss:  6.00221466e-03\n",
      "Epoch: 7076 mean train loss:  1.19646653e-02, mean val. rec. loss:  6.00059648e-03\n",
      "Epoch: 7077 mean train loss:  1.19621513e-02, mean val. rec. loss:  5.99901458e-03\n",
      "Epoch: 7078 mean train loss:  1.19596458e-02, mean val. rec. loss:  5.99745536e-03\n",
      "Epoch: 7079 mean train loss:  1.19571318e-02, mean val. rec. loss:  5.99586779e-03\n",
      "Epoch: 7080 mean train loss:  1.19546113e-02, mean val. rec. loss:  5.99428590e-03\n",
      "Epoch: 7081 mean train loss:  1.19520974e-02, mean val. rec. loss:  5.99270627e-03\n",
      "Epoch: 7082 mean train loss:  1.19495843e-02, mean val. rec. loss:  5.99111700e-03\n",
      "Epoch: 7083 mean train loss:  1.19470704e-02, mean val. rec. loss:  5.98952319e-03\n",
      "Epoch: 7084 mean train loss:  1.19445536e-02, mean val. rec. loss:  5.98795094e-03\n",
      "Epoch: 7085 mean train loss:  1.19420322e-02, mean val. rec. loss:  5.98635430e-03\n",
      "Epoch: 7086 mean train loss:  1.19395052e-02, mean val. rec. loss:  5.98475312e-03\n",
      "Epoch: 7087 mean train loss:  1.19369801e-02, mean val. rec. loss:  5.98319901e-03\n",
      "Epoch: 7088 mean train loss:  1.19344680e-02, mean val. rec. loss:  5.98159953e-03\n",
      "Epoch: 7089 mean train loss:  1.19319261e-02, mean val. rec. loss:  5.97996321e-03\n",
      "Epoch: 7090 mean train loss:  1.19294084e-02, mean val. rec. loss:  5.97837620e-03\n",
      "Epoch: 7091 mean train loss:  1.19268777e-02, mean val. rec. loss:  5.97682776e-03\n",
      "Epoch: 7092 mean train loss:  1.19243470e-02, mean val. rec. loss:  5.97522375e-03\n",
      "Epoch: 7093 mean train loss:  1.19218098e-02, mean val. rec. loss:  5.97360216e-03\n",
      "Epoch: 7094 mean train loss:  1.19192818e-02, mean val. rec. loss:  5.97200836e-03\n",
      "Epoch: 7095 mean train loss:  1.19167474e-02, mean val. rec. loss:  5.97040832e-03\n",
      "Epoch: 7096 mean train loss:  1.19142130e-02, mean val. rec. loss:  5.96882075e-03\n",
      "Epoch: 7097 mean train loss:  1.19116655e-02, mean val. rec. loss:  5.96720937e-03\n",
      "Epoch: 7098 mean train loss:  1.19091385e-02, mean val. rec. loss:  5.96558495e-03\n",
      "Epoch: 7099 mean train loss:  1.19065910e-02, mean val. rec. loss:  5.96401496e-03\n",
      "Epoch: 7100 mean train loss:  1.19040454e-02, mean val. rec. loss:  5.96241378e-03\n",
      "Epoch: 7101 mean train loss:  1.19015054e-02, mean val. rec. loss:  5.96079673e-03\n",
      "Epoch: 7102 mean train loss:  1.18989672e-02, mean val. rec. loss:  5.95917061e-03\n",
      "Epoch: 7103 mean train loss:  1.18964132e-02, mean val. rec. loss:  5.95758418e-03\n",
      "Epoch: 7104 mean train loss:  1.18938629e-02, mean val. rec. loss:  5.95599661e-03\n",
      "Epoch: 7105 mean train loss:  1.18913136e-02, mean val. rec. loss:  5.95435008e-03\n",
      "Epoch: 7106 mean train loss:  1.18887578e-02, mean val. rec. loss:  5.95272055e-03\n",
      "Epoch: 7107 mean train loss:  1.18862112e-02, mean val. rec. loss:  5.95114999e-03\n",
      "Epoch: 7108 mean train loss:  1.18836619e-02, mean val. rec. loss:  5.94955562e-03\n",
      "Epoch: 7109 mean train loss:  1.18811051e-02, mean val. rec. loss:  5.94790795e-03\n",
      "Epoch: 7110 mean train loss:  1.18785446e-02, mean val. rec. loss:  5.94627049e-03\n",
      "Epoch: 7111 mean train loss:  1.18759822e-02, mean val. rec. loss:  5.94465741e-03\n",
      "Epoch: 7112 mean train loss:  1.18734301e-02, mean val. rec. loss:  5.94307211e-03\n",
      "Epoch: 7113 mean train loss:  1.18708695e-02, mean val. rec. loss:  5.94146243e-03\n",
      "Epoch: 7114 mean train loss:  1.18683109e-02, mean val. rec. loss:  5.93983291e-03\n",
      "Epoch: 7115 mean train loss:  1.18657485e-02, mean val. rec. loss:  5.93816993e-03\n",
      "Epoch: 7116 mean train loss:  1.18631815e-02, mean val. rec. loss:  5.93657896e-03\n",
      "Epoch: 7117 mean train loss:  1.18606107e-02, mean val. rec. loss:  5.93499933e-03\n",
      "Epoch: 7118 mean train loss:  1.18580512e-02, mean val. rec. loss:  5.93332898e-03\n",
      "Epoch: 7119 mean train loss:  1.18554776e-02, mean val. rec. loss:  5.93168982e-03\n",
      "Epoch: 7120 mean train loss:  1.18529050e-02, mean val. rec. loss:  5.93010112e-03\n",
      "Epoch: 7121 mean train loss:  1.18503398e-02, mean val. rec. loss:  5.92847953e-03\n",
      "Epoch: 7122 mean train loss:  1.18477588e-02, mean val. rec. loss:  5.92683357e-03\n",
      "Epoch: 7123 mean train loss:  1.18451937e-02, mean val. rec. loss:  5.92521368e-03\n",
      "Epoch: 7124 mean train loss:  1.18426043e-02, mean val. rec. loss:  5.92356204e-03\n",
      "Epoch: 7125 mean train loss:  1.18400428e-02, mean val. rec. loss:  5.92196427e-03\n",
      "Epoch: 7126 mean train loss:  1.18374535e-02, mean val. rec. loss:  5.92032341e-03\n",
      "Epoch: 7127 mean train loss:  1.18348669e-02, mean val. rec. loss:  5.91867234e-03\n",
      "Epoch: 7128 mean train loss:  1.18322952e-02, mean val. rec. loss:  5.91705755e-03\n",
      "Epoch: 7129 mean train loss:  1.18297049e-02, mean val. rec. loss:  5.91542803e-03\n",
      "Epoch: 7130 mean train loss:  1.18271165e-02, mean val. rec. loss:  5.91377809e-03\n",
      "Epoch: 7131 mean train loss:  1.18245355e-02, mean val. rec. loss:  5.91212476e-03\n",
      "Epoch: 7132 mean train loss:  1.18219563e-02, mean val. rec. loss:  5.91052812e-03\n",
      "Epoch: 7133 mean train loss:  1.18193623e-02, mean val. rec. loss:  5.90886911e-03\n",
      "Epoch: 7134 mean train loss:  1.18167683e-02, mean val. rec. loss:  5.90722654e-03\n",
      "Epoch: 7135 mean train loss:  1.18141770e-02, mean val. rec. loss:  5.90559872e-03\n",
      "Epoch: 7136 mean train loss:  1.18115867e-02, mean val. rec. loss:  5.90395162e-03\n",
      "Epoch: 7137 mean train loss:  1.18089871e-02, mean val. rec. loss:  5.90230905e-03\n",
      "Epoch: 7138 mean train loss:  1.18063959e-02, mean val. rec. loss:  5.90068010e-03\n",
      "Epoch: 7139 mean train loss:  1.18037962e-02, mean val. rec. loss:  5.89901258e-03\n",
      "Epoch: 7140 mean train loss:  1.18011994e-02, mean val. rec. loss:  5.89739497e-03\n",
      "Epoch: 7141 mean train loss:  1.17985951e-02, mean val. rec. loss:  5.89575240e-03\n",
      "Epoch: 7142 mean train loss:  1.17959853e-02, mean val. rec. loss:  5.89406505e-03\n",
      "Epoch: 7143 mean train loss:  1.17933847e-02, mean val. rec. loss:  5.89241738e-03\n",
      "Epoch: 7144 mean train loss:  1.17907851e-02, mean val. rec. loss:  5.89083151e-03\n",
      "Epoch: 7145 mean train loss:  1.17881622e-02, mean val. rec. loss:  5.88916570e-03\n",
      "Epoch: 7146 mean train loss:  1.17855691e-02, mean val. rec. loss:  5.88748571e-03\n",
      "Epoch: 7147 mean train loss:  1.17829639e-02, mean val. rec. loss:  5.88583521e-03\n",
      "Epoch: 7148 mean train loss:  1.17803420e-02, mean val. rec. loss:  5.88418924e-03\n",
      "Epoch: 7149 mean train loss:  1.17777405e-02, mean val. rec. loss:  5.88255008e-03\n",
      "Epoch: 7150 mean train loss:  1.17751139e-02, mean val. rec. loss:  5.88089391e-03\n",
      "Epoch: 7151 mean train loss:  1.17725068e-02, mean val. rec. loss:  5.87921165e-03\n",
      "Epoch: 7152 mean train loss:  1.17698895e-02, mean val. rec. loss:  5.87760254e-03\n",
      "Epoch: 7153 mean train loss:  1.17672601e-02, mean val. rec. loss:  5.87592879e-03\n",
      "Epoch: 7154 mean train loss:  1.17646474e-02, mean val. rec. loss:  5.87424824e-03\n",
      "Epoch: 7155 mean train loss:  1.17620357e-02, mean val. rec. loss:  5.87261304e-03\n",
      "Epoch: 7156 mean train loss:  1.17594035e-02, mean val. rec. loss:  5.87096821e-03\n",
      "Epoch: 7157 mean train loss:  1.17567787e-02, mean val. rec. loss:  5.86931260e-03\n",
      "Epoch: 7158 mean train loss:  1.17541502e-02, mean val. rec. loss:  5.86760654e-03\n",
      "Epoch: 7159 mean train loss:  1.17515208e-02, mean val. rec. loss:  5.86595773e-03\n",
      "Epoch: 7160 mean train loss:  1.17488933e-02, mean val. rec. loss:  5.86432481e-03\n",
      "Epoch: 7161 mean train loss:  1.17462620e-02, mean val. rec. loss:  5.86266070e-03\n",
      "Epoch: 7162 mean train loss:  1.17436400e-02, mean val. rec. loss:  5.86097221e-03\n",
      "Epoch: 7163 mean train loss:  1.17410050e-02, mean val. rec. loss:  5.85932170e-03\n",
      "Epoch: 7164 mean train loss:  1.17383738e-02, mean val. rec. loss:  5.85768481e-03\n",
      "Epoch: 7165 mean train loss:  1.17357471e-02, mean val. rec. loss:  5.85600709e-03\n",
      "Epoch: 7166 mean train loss:  1.17331000e-02, mean val. rec. loss:  5.85432427e-03\n",
      "Epoch: 7167 mean train loss:  1.17304585e-02, mean val. rec. loss:  5.85264315e-03\n",
      "Epoch: 7168 mean train loss:  1.17278282e-02, mean val. rec. loss:  5.85102496e-03\n",
      "Epoch: 7169 mean train loss:  1.17251876e-02, mean val. rec. loss:  5.84935462e-03\n",
      "Epoch: 7170 mean train loss:  1.17225396e-02, mean val. rec. loss:  5.84765819e-03\n",
      "Epoch: 7171 mean train loss:  1.17199083e-02, mean val. rec. loss:  5.84599578e-03\n",
      "Epoch: 7172 mean train loss:  1.17172519e-02, mean val. rec. loss:  5.84432259e-03\n",
      "Epoch: 7173 mean train loss:  1.17146085e-02, mean val. rec. loss:  5.84265281e-03\n",
      "Epoch: 7174 mean train loss:  1.17119614e-02, mean val. rec. loss:  5.84099607e-03\n",
      "Epoch: 7175 mean train loss:  1.17093040e-02, mean val. rec. loss:  5.83930928e-03\n",
      "Epoch: 7176 mean train loss:  1.17066607e-02, mean val. rec. loss:  5.83765708e-03\n",
      "Epoch: 7177 mean train loss:  1.17040126e-02, mean val. rec. loss:  5.83597879e-03\n",
      "Epoch: 7178 mean train loss:  1.17013553e-02, mean val. rec. loss:  5.83427670e-03\n",
      "Epoch: 7179 mean train loss:  1.16987007e-02, mean val. rec. loss:  5.83262563e-03\n",
      "Epoch: 7180 mean train loss:  1.16960387e-02, mean val. rec. loss:  5.83097172e-03\n",
      "Epoch: 7181 mean train loss:  1.16933795e-02, mean val. rec. loss:  5.82927473e-03\n",
      "Epoch: 7182 mean train loss:  1.16907203e-02, mean val. rec. loss:  5.82760381e-03\n",
      "Epoch: 7183 mean train loss:  1.16880630e-02, mean val. rec. loss:  5.82591305e-03\n",
      "Epoch: 7184 mean train loss:  1.16854038e-02, mean val. rec. loss:  5.82423080e-03\n",
      "Epoch: 7185 mean train loss:  1.16827427e-02, mean val. rec. loss:  5.82259390e-03\n",
      "Epoch: 7186 mean train loss:  1.16800853e-02, mean val. rec. loss:  5.82090541e-03\n",
      "Epoch: 7187 mean train loss:  1.16773963e-02, mean val. rec. loss:  5.81917326e-03\n",
      "Epoch: 7188 mean train loss:  1.16747381e-02, mean val. rec. loss:  5.81752900e-03\n",
      "Epoch: 7189 mean train loss:  1.16720761e-02, mean val. rec. loss:  5.81585581e-03\n",
      "Epoch: 7190 mean train loss:  1.16693908e-02, mean val. rec. loss:  5.81415315e-03\n",
      "Epoch: 7191 mean train loss:  1.16667334e-02, mean val. rec. loss:  5.81251002e-03\n",
      "Epoch: 7192 mean train loss:  1.16640482e-02, mean val. rec. loss:  5.81078297e-03\n",
      "Epoch: 7193 mean train loss:  1.16613889e-02, mean val. rec. loss:  5.80911829e-03\n",
      "Epoch: 7194 mean train loss:  1.16587027e-02, mean val. rec. loss:  5.80746382e-03\n",
      "Epoch: 7195 mean train loss:  1.16560249e-02, mean val. rec. loss:  5.80574471e-03\n",
      "Epoch: 7196 mean train loss:  1.16533555e-02, mean val. rec. loss:  5.80407493e-03\n",
      "Epoch: 7197 mean train loss:  1.16506702e-02, mean val. rec. loss:  5.80239268e-03\n",
      "Epoch: 7198 mean train loss:  1.16479840e-02, mean val. rec. loss:  5.80067640e-03\n",
      "Epoch: 7199 mean train loss:  1.16453024e-02, mean val. rec. loss:  5.79900322e-03\n",
      "Epoch: 7200 mean train loss:  1.16426143e-02, mean val. rec. loss:  5.79734478e-03\n",
      "Epoch: 7201 mean train loss:  1.16399309e-02, mean val. rec. loss:  5.79564211e-03\n",
      "Epoch: 7202 mean train loss:  1.16372475e-02, mean val. rec. loss:  5.79392641e-03\n",
      "Epoch: 7203 mean train loss:  1.16345641e-02, mean val. rec. loss:  5.79228044e-03\n",
      "Epoch: 7204 mean train loss:  1.16318751e-02, mean val. rec. loss:  5.79059308e-03\n",
      "Epoch: 7205 mean train loss:  1.16291898e-02, mean val. rec. loss:  5.78891423e-03\n",
      "Epoch: 7206 mean train loss:  1.16264813e-02, mean val. rec. loss:  5.78718265e-03\n",
      "Epoch: 7207 mean train loss:  1.16237932e-02, mean val. rec. loss:  5.78548112e-03\n",
      "Epoch: 7208 mean train loss:  1.16211107e-02, mean val. rec. loss:  5.78387087e-03\n",
      "Epoch: 7209 mean train loss:  1.16184012e-02, mean val. rec. loss:  5.78213418e-03\n",
      "Epoch: 7210 mean train loss:  1.16157131e-02, mean val. rec. loss:  5.78042358e-03\n",
      "Epoch: 7211 mean train loss:  1.16130092e-02, mean val. rec. loss:  5.77876060e-03\n",
      "Epoch: 7212 mean train loss:  1.16102979e-02, mean val. rec. loss:  5.77703866e-03\n",
      "Epoch: 7213 mean train loss:  1.16076182e-02, mean val. rec. loss:  5.77540630e-03\n",
      "Epoch: 7214 mean train loss:  1.16049041e-02, mean val. rec. loss:  5.77368096e-03\n",
      "Epoch: 7215 mean train loss:  1.16021927e-02, mean val. rec. loss:  5.77194824e-03\n",
      "Epoch: 7216 mean train loss:  1.15994869e-02, mean val. rec. loss:  5.77028640e-03\n",
      "Epoch: 7217 mean train loss:  1.15967784e-02, mean val. rec. loss:  5.76859280e-03\n",
      "Epoch: 7218 mean train loss:  1.15940698e-02, mean val. rec. loss:  5.76691168e-03\n",
      "Epoch: 7219 mean train loss:  1.15913603e-02, mean val. rec. loss:  5.76518237e-03\n",
      "Epoch: 7220 mean train loss:  1.15886518e-02, mean val. rec. loss:  5.76350748e-03\n",
      "Epoch: 7221 mean train loss:  1.15859386e-02, mean val. rec. loss:  5.76182466e-03\n",
      "Epoch: 7222 mean train loss:  1.15832319e-02, mean val. rec. loss:  5.76014411e-03\n",
      "Epoch: 7223 mean train loss:  1.15805224e-02, mean val. rec. loss:  5.75842443e-03\n",
      "Epoch: 7224 mean train loss:  1.15777915e-02, mean val. rec. loss:  5.75669285e-03\n",
      "Epoch: 7225 mean train loss:  1.15750857e-02, mean val. rec. loss:  5.75505539e-03\n",
      "Epoch: 7226 mean train loss:  1.15723539e-02, mean val. rec. loss:  5.75332664e-03\n",
      "Epoch: 7227 mean train loss:  1.15696444e-02, mean val. rec. loss:  5.75163758e-03\n",
      "Epoch: 7228 mean train loss:  1.15669154e-02, mean val. rec. loss:  5.74993719e-03\n",
      "Epoch: 7229 mean train loss:  1.15642049e-02, mean val. rec. loss:  5.74823339e-03\n",
      "Epoch: 7230 mean train loss:  1.15614787e-02, mean val. rec. loss:  5.74655510e-03\n",
      "Epoch: 7231 mean train loss:  1.15587441e-02, mean val. rec. loss:  5.74484620e-03\n",
      "Epoch: 7232 mean train loss:  1.15560178e-02, mean val. rec. loss:  5.74312993e-03\n",
      "Epoch: 7233 mean train loss:  1.15532832e-02, mean val. rec. loss:  5.74145107e-03\n",
      "Epoch: 7234 mean train loss:  1.15505570e-02, mean val. rec. loss:  5.73972913e-03\n",
      "Epoch: 7235 mean train loss:  1.15478288e-02, mean val. rec. loss:  5.73804177e-03\n",
      "Epoch: 7236 mean train loss:  1.15450970e-02, mean val. rec. loss:  5.73638390e-03\n",
      "Epoch: 7237 mean train loss:  1.15423670e-02, mean val. rec. loss:  5.73465685e-03\n",
      "Epoch: 7238 mean train loss:  1.15396389e-02, mean val. rec. loss:  5.73296042e-03\n",
      "Epoch: 7239 mean train loss:  1.15368922e-02, mean val. rec. loss:  5.73123564e-03\n",
      "Epoch: 7240 mean train loss:  1.15341622e-02, mean val. rec. loss:  5.72955566e-03\n",
      "Epoch: 7241 mean train loss:  1.15314174e-02, mean val. rec. loss:  5.72787964e-03\n",
      "Epoch: 7242 mean train loss:  1.15286818e-02, mean val. rec. loss:  5.72615486e-03\n",
      "Epoch: 7243 mean train loss:  1.15259313e-02, mean val. rec. loss:  5.72442838e-03\n",
      "Epoch: 7244 mean train loss:  1.15232070e-02, mean val. rec. loss:  5.72278242e-03\n",
      "Epoch: 7245 mean train loss:  1.15204584e-02, mean val. rec. loss:  5.72106614e-03\n",
      "Epoch: 7246 mean train loss:  1.15177088e-02, mean val. rec. loss:  5.71937028e-03\n",
      "Epoch: 7247 mean train loss:  1.15149575e-02, mean val. rec. loss:  5.71762622e-03\n",
      "Epoch: 7248 mean train loss:  1.15122117e-02, mean val. rec. loss:  5.71595474e-03\n",
      "Epoch: 7249 mean train loss:  1.15094659e-02, mean val. rec. loss:  5.71428666e-03\n",
      "Epoch: 7250 mean train loss:  1.15067154e-02, mean val. rec. loss:  5.71255451e-03\n",
      "Epoch: 7251 mean train loss:  1.15039668e-02, mean val. rec. loss:  5.71081216e-03\n",
      "Epoch: 7252 mean train loss:  1.15012182e-02, mean val. rec. loss:  5.70917526e-03\n",
      "Epoch: 7253 mean train loss:  1.14984733e-02, mean val. rec. loss:  5.70745842e-03\n",
      "Epoch: 7254 mean train loss:  1.14957043e-02, mean val. rec. loss:  5.70574725e-03\n",
      "Epoch: 7255 mean train loss:  1.14929575e-02, mean val. rec. loss:  5.70405366e-03\n",
      "Epoch: 7256 mean train loss:  1.14901940e-02, mean val. rec. loss:  5.70233398e-03\n",
      "Epoch: 7257 mean train loss:  1.14874445e-02, mean val. rec. loss:  5.70067384e-03\n",
      "Epoch: 7258 mean train loss:  1.14846801e-02, mean val. rec. loss:  5.69893999e-03\n",
      "Epoch: 7259 mean train loss:  1.14819110e-02, mean val. rec. loss:  5.69723562e-03\n",
      "Epoch: 7260 mean train loss:  1.14791503e-02, mean val. rec. loss:  5.69560270e-03\n",
      "Epoch: 7261 mean train loss:  1.14763989e-02, mean val. rec. loss:  5.69382235e-03\n",
      "Epoch: 7262 mean train loss:  1.14736410e-02, mean val. rec. loss:  5.69215768e-03\n",
      "Epoch: 7263 mean train loss:  1.14708738e-02, mean val. rec. loss:  5.69048279e-03\n",
      "Epoch: 7264 mean train loss:  1.14680954e-02, mean val. rec. loss:  5.68872286e-03\n",
      "Epoch: 7265 mean train loss:  1.14653245e-02, mean val. rec. loss:  5.68703834e-03\n",
      "Epoch: 7266 mean train loss:  1.14625601e-02, mean val. rec. loss:  5.68532774e-03\n",
      "Epoch: 7267 mean train loss:  1.14597975e-02, mean val. rec. loss:  5.68364945e-03\n",
      "Epoch: 7268 mean train loss:  1.14570098e-02, mean val. rec. loss:  5.68196209e-03\n",
      "Epoch: 7269 mean train loss:  1.14542463e-02, mean val. rec. loss:  5.68020783e-03\n",
      "Epoch: 7270 mean train loss:  1.14514614e-02, mean val. rec. loss:  5.67854202e-03\n",
      "Epoch: 7271 mean train loss:  1.14487035e-02, mean val. rec. loss:  5.67686714e-03\n",
      "Epoch: 7272 mean train loss:  1.14459186e-02, mean val. rec. loss:  5.67511798e-03\n",
      "Epoch: 7273 mean train loss:  1.14431384e-02, mean val. rec. loss:  5.67345047e-03\n",
      "Epoch: 7274 mean train loss:  1.14403562e-02, mean val. rec. loss:  5.67171945e-03\n",
      "Epoch: 7275 mean train loss:  1.14375779e-02, mean val. rec. loss:  5.67002699e-03\n",
      "Epoch: 7276 mean train loss:  1.14347939e-02, mean val. rec. loss:  5.66837422e-03\n",
      "Epoch: 7277 mean train loss:  1.14320164e-02, mean val. rec. loss:  5.66663413e-03\n",
      "Epoch: 7278 mean train loss:  1.14292362e-02, mean val. rec. loss:  5.66494281e-03\n",
      "Epoch: 7279 mean train loss:  1.14264634e-02, mean val. rec. loss:  5.66326509e-03\n",
      "Epoch: 7280 mean train loss:  1.14236580e-02, mean val. rec. loss:  5.66151990e-03\n",
      "Epoch: 7281 mean train loss:  1.14208777e-02, mean val. rec. loss:  5.65988017e-03\n",
      "Epoch: 7282 mean train loss:  1.14180938e-02, mean val. rec. loss:  5.65816276e-03\n",
      "Epoch: 7283 mean train loss:  1.14153079e-02, mean val. rec. loss:  5.65646576e-03\n",
      "Epoch: 7284 mean train loss:  1.14125100e-02, mean val. rec. loss:  5.65476877e-03\n",
      "Epoch: 7285 mean train loss:  1.14097232e-02, mean val. rec. loss:  5.65304626e-03\n",
      "Epoch: 7286 mean train loss:  1.14069430e-02, mean val. rec. loss:  5.65140766e-03\n",
      "Epoch: 7287 mean train loss:  1.14041459e-02, mean val. rec. loss:  5.64968458e-03\n",
      "Epoch: 7288 mean train loss:  1.14013480e-02, mean val. rec. loss:  5.64794847e-03\n",
      "Epoch: 7289 mean train loss:  1.13985594e-02, mean val. rec. loss:  5.64631667e-03\n",
      "Epoch: 7290 mean train loss:  1.13957456e-02, mean val. rec. loss:  5.64459416e-03\n",
      "Epoch: 7291 mean train loss:  1.13929477e-02, mean val. rec. loss:  5.64286031e-03\n",
      "Epoch: 7292 mean train loss:  1.13901581e-02, mean val. rec. loss:  5.64118770e-03\n",
      "Epoch: 7293 mean train loss:  1.13873583e-02, mean val. rec. loss:  5.63951848e-03\n",
      "Epoch: 7294 mean train loss:  1.13845529e-02, mean val. rec. loss:  5.63780731e-03\n",
      "Epoch: 7295 mean train loss:  1.13817401e-02, mean val. rec. loss:  5.63608650e-03\n",
      "Epoch: 7296 mean train loss:  1.13789533e-02, mean val. rec. loss:  5.63441105e-03\n",
      "Epoch: 7297 mean train loss:  1.13761405e-02, mean val. rec. loss:  5.63274694e-03\n",
      "Epoch: 7298 mean train loss:  1.13733323e-02, mean val. rec. loss:  5.63099608e-03\n",
      "Epoch: 7299 mean train loss:  1.13705260e-02, mean val. rec. loss:  5.62933084e-03\n",
      "Epoch: 7300 mean train loss:  1.13677196e-02, mean val. rec. loss:  5.62768147e-03\n",
      "Epoch: 7301 mean train loss:  1.13649115e-02, mean val. rec. loss:  5.62597143e-03\n",
      "Epoch: 7302 mean train loss:  1.13621126e-02, mean val. rec. loss:  5.62424269e-03\n",
      "Epoch: 7303 mean train loss:  1.13592904e-02, mean val. rec. loss:  5.62256667e-03\n",
      "Epoch: 7304 mean train loss:  1.13564795e-02, mean val. rec. loss:  5.62089632e-03\n",
      "Epoch: 7305 mean train loss:  1.13536741e-02, mean val. rec. loss:  5.61921690e-03\n",
      "Epoch: 7306 mean train loss:  1.13508566e-02, mean val. rec. loss:  5.61747058e-03\n",
      "Epoch: 7307 mean train loss:  1.13480270e-02, mean val. rec. loss:  5.61582064e-03\n",
      "Epoch: 7308 mean train loss:  1.13452290e-02, mean val. rec. loss:  5.61414803e-03\n",
      "Epoch: 7309 mean train loss:  1.13424106e-02, mean val. rec. loss:  5.61244933e-03\n",
      "Epoch: 7310 mean train loss:  1.13395950e-02, mean val. rec. loss:  5.61075857e-03\n",
      "Epoch: 7311 mean train loss:  1.13367700e-02, mean val. rec. loss:  5.60909673e-03\n",
      "Epoch: 7312 mean train loss:  1.13339507e-02, mean val. rec. loss:  5.60738159e-03\n",
      "Epoch: 7313 mean train loss:  1.13311229e-02, mean val. rec. loss:  5.60568459e-03\n",
      "Epoch: 7314 mean train loss:  1.13283008e-02, mean val. rec. loss:  5.60402388e-03\n",
      "Epoch: 7315 mean train loss:  1.13254814e-02, mean val. rec. loss:  5.60234560e-03\n",
      "Epoch: 7316 mean train loss:  1.13226453e-02, mean val. rec. loss:  5.60063499e-03\n",
      "Epoch: 7317 mean train loss:  1.13198353e-02, mean val. rec. loss:  5.59897882e-03\n",
      "Epoch: 7318 mean train loss:  1.13169982e-02, mean val. rec. loss:  5.59728409e-03\n",
      "Epoch: 7319 mean train loss:  1.13141658e-02, mean val. rec. loss:  5.59560864e-03\n",
      "Epoch: 7320 mean train loss:  1.13113437e-02, mean val. rec. loss:  5.59391788e-03\n",
      "Epoch: 7321 mean train loss:  1.13085197e-02, mean val. rec. loss:  5.59228382e-03\n",
      "Epoch: 7322 mean train loss:  1.13056808e-02, mean val. rec. loss:  5.59053069e-03\n",
      "Epoch: 7323 mean train loss:  1.13028446e-02, mean val. rec. loss:  5.58886148e-03\n",
      "Epoch: 7324 mean train loss:  1.13000178e-02, mean val. rec. loss:  5.58723989e-03\n",
      "Epoch: 7325 mean train loss:  1.12971910e-02, mean val. rec. loss:  5.58553043e-03\n",
      "Epoch: 7326 mean train loss:  1.12943447e-02, mean val. rec. loss:  5.58384590e-03\n",
      "Epoch: 7327 mean train loss:  1.12915207e-02, mean val. rec. loss:  5.58213360e-03\n",
      "Epoch: 7328 mean train loss:  1.12886780e-02, mean val. rec. loss:  5.58051258e-03\n",
      "Epoch: 7329 mean train loss:  1.12858410e-02, mean val. rec. loss:  5.57885074e-03\n",
      "Epoch: 7330 mean train loss:  1.12830132e-02, mean val. rec. loss:  5.57712823e-03\n",
      "Epoch: 7331 mean train loss:  1.12801688e-02, mean val. rec. loss:  5.57551288e-03\n",
      "Epoch: 7332 mean train loss:  1.12773280e-02, mean val. rec. loss:  5.57378073e-03\n",
      "Epoch: 7333 mean train loss:  1.12744909e-02, mean val. rec. loss:  5.57213986e-03\n",
      "Epoch: 7334 mean train loss:  1.12716381e-02, mean val. rec. loss:  5.57045988e-03\n",
      "Epoch: 7335 mean train loss:  1.12687945e-02, mean val. rec. loss:  5.56879577e-03\n",
      "Epoch: 7336 mean train loss:  1.12659584e-02, mean val. rec. loss:  5.56714016e-03\n",
      "Epoch: 7337 mean train loss:  1.12631027e-02, mean val. rec. loss:  5.56542332e-03\n",
      "Epoch: 7338 mean train loss:  1.12602675e-02, mean val. rec. loss:  5.56380684e-03\n",
      "Epoch: 7339 mean train loss:  1.12574165e-02, mean val. rec. loss:  5.56210360e-03\n",
      "Epoch: 7340 mean train loss:  1.12545646e-02, mean val. rec. loss:  5.56044119e-03\n",
      "Epoch: 7341 mean train loss:  1.12517108e-02, mean val. rec. loss:  5.55878956e-03\n",
      "Epoch: 7342 mean train loss:  1.12488597e-02, mean val. rec. loss:  5.55714075e-03\n",
      "Epoch: 7343 mean train loss:  1.12460125e-02, mean val. rec. loss:  5.55544262e-03\n",
      "Epoch: 7344 mean train loss:  1.12431689e-02, mean val. rec. loss:  5.55380119e-03\n",
      "Epoch: 7345 mean train loss:  1.12403207e-02, mean val. rec. loss:  5.55214899e-03\n",
      "Epoch: 7346 mean train loss:  1.12374520e-02, mean val. rec. loss:  5.55047524e-03\n",
      "Epoch: 7347 mean train loss:  1.12346103e-02, mean val. rec. loss:  5.54882077e-03\n",
      "Epoch: 7348 mean train loss:  1.12317453e-02, mean val. rec. loss:  5.54711583e-03\n",
      "Epoch: 7349 mean train loss:  1.12288989e-02, mean val. rec. loss:  5.54552827e-03\n",
      "Epoch: 7350 mean train loss:  1.12260479e-02, mean val. rec. loss:  5.54385055e-03\n",
      "Epoch: 7351 mean train loss:  1.12231867e-02, mean val. rec. loss:  5.54219891e-03\n",
      "Epoch: 7352 mean train loss:  1.12203273e-02, mean val. rec. loss:  5.54054047e-03\n",
      "Epoch: 7353 mean train loss:  1.12174688e-02, mean val. rec. loss:  5.53885368e-03\n",
      "Epoch: 7354 mean train loss:  1.12145936e-02, mean val. rec. loss:  5.53722529e-03\n",
      "Epoch: 7355 mean train loss:  1.12117444e-02, mean val. rec. loss:  5.53559293e-03\n",
      "Epoch: 7356 mean train loss:  1.12088767e-02, mean val. rec. loss:  5.53391918e-03\n",
      "Epoch: 7357 mean train loss:  1.12060117e-02, mean val. rec. loss:  5.53229079e-03\n",
      "Epoch: 7358 mean train loss:  1.12031430e-02, mean val. rec. loss:  5.53058756e-03\n",
      "Epoch: 7359 mean train loss:  1.12002966e-02, mean val. rec. loss:  5.52900169e-03\n",
      "Epoch: 7360 mean train loss:  1.11974298e-02, mean val. rec. loss:  5.52735686e-03\n",
      "Epoch: 7361 mean train loss:  1.11945601e-02, mean val. rec. loss:  5.52567404e-03\n",
      "Epoch: 7362 mean train loss:  1.11916924e-02, mean val. rec. loss:  5.52402580e-03\n",
      "Epoch: 7363 mean train loss:  1.11888171e-02, mean val. rec. loss:  5.52239344e-03\n",
      "Epoch: 7364 mean train loss:  1.11859428e-02, mean val. rec. loss:  5.52074974e-03\n",
      "Epoch: 7365 mean train loss:  1.11830797e-02, mean val. rec. loss:  5.51910321e-03\n",
      "Epoch: 7366 mean train loss:  1.11801989e-02, mean val. rec. loss:  5.51742833e-03\n",
      "Epoch: 7367 mean train loss:  1.11773405e-02, mean val. rec. loss:  5.51589236e-03\n",
      "Epoch: 7368 mean train loss:  1.11744634e-02, mean val. rec. loss:  5.51415057e-03\n",
      "Epoch: 7369 mean train loss:  1.11715863e-02, mean val. rec. loss:  5.51255960e-03\n",
      "Epoch: 7370 mean train loss:  1.11687120e-02, mean val. rec. loss:  5.51090002e-03\n",
      "Epoch: 7371 mean train loss:  1.11658396e-02, mean val. rec. loss:  5.50931529e-03\n",
      "Epoch: 7372 mean train loss:  1.11629662e-02, mean val. rec. loss:  5.50766365e-03\n",
      "Epoch: 7373 mean train loss:  1.11600938e-02, mean val. rec. loss:  5.50600805e-03\n",
      "Epoch: 7374 mean train loss:  1.11571990e-02, mean val. rec. loss:  5.50438589e-03\n",
      "Epoch: 7375 mean train loss:  1.11543331e-02, mean val. rec. loss:  5.50274503e-03\n",
      "Epoch: 7376 mean train loss:  1.11514448e-02, mean val. rec. loss:  5.50114839e-03\n",
      "Epoch: 7377 mean train loss:  1.11485696e-02, mean val. rec. loss:  5.49952057e-03\n",
      "Epoch: 7378 mean train loss:  1.11457000e-02, mean val. rec. loss:  5.49788027e-03\n",
      "Epoch: 7379 mean train loss:  1.11428201e-02, mean val. rec. loss:  5.49623317e-03\n",
      "Epoch: 7380 mean train loss:  1.11399365e-02, mean val. rec. loss:  5.49465864e-03\n",
      "Epoch: 7381 mean train loss:  1.11370389e-02, mean val. rec. loss:  5.49303422e-03\n",
      "Epoch: 7382 mean train loss:  1.11341507e-02, mean val. rec. loss:  5.49136784e-03\n",
      "Epoch: 7383 mean train loss:  1.11312782e-02, mean val. rec. loss:  5.48974002e-03\n",
      "Epoch: 7384 mean train loss:  1.11283816e-02, mean val. rec. loss:  5.48817060e-03\n",
      "Epoch: 7385 mean train loss:  1.11255073e-02, mean val. rec. loss:  5.48654844e-03\n",
      "Epoch: 7386 mean train loss:  1.11226125e-02, mean val. rec. loss:  5.48491438e-03\n",
      "Epoch: 7387 mean train loss:  1.11197206e-02, mean val. rec. loss:  5.48329166e-03\n",
      "Epoch: 7388 mean train loss:  1.11168342e-02, mean val. rec. loss:  5.48170069e-03\n",
      "Epoch: 7389 mean train loss:  1.11139440e-02, mean val. rec. loss:  5.48006947e-03\n",
      "Epoch: 7390 mean train loss:  1.11110614e-02, mean val. rec. loss:  5.47845469e-03\n",
      "Epoch: 7391 mean train loss:  1.11081713e-02, mean val. rec. loss:  5.47691928e-03\n",
      "Epoch: 7392 mean train loss:  1.11052653e-02, mean val. rec. loss:  5.47517126e-03\n",
      "Epoch: 7393 mean train loss:  1.11023836e-02, mean val. rec. loss:  5.47370049e-03\n",
      "Epoch: 7394 mean train loss:  1.10994795e-02, mean val. rec. loss:  5.47195643e-03\n",
      "Epoch: 7395 mean train loss:  1.10965791e-02, mean val. rec. loss:  5.47052876e-03\n",
      "Epoch: 7396 mean train loss:  1.10937002e-02, mean val. rec. loss:  5.46872290e-03\n",
      "Epoch: 7397 mean train loss:  1.10907970e-02, mean val. rec. loss:  5.46735532e-03\n",
      "Epoch: 7398 mean train loss:  1.10878957e-02, mean val. rec. loss:  5.46552112e-03\n",
      "Epoch: 7399 mean train loss:  1.10850037e-02, mean val. rec. loss:  5.46410705e-03\n",
      "Epoch: 7400 mean train loss:  1.10821145e-02, mean val. rec. loss:  5.46241402e-03\n",
      "Epoch: 7401 mean train loss:  1.10792012e-02, mean val. rec. loss:  5.46089846e-03\n",
      "Epoch: 7402 mean train loss:  1.10763148e-02, mean val. rec. loss:  5.45922471e-03\n",
      "Epoch: 7403 mean train loss:  1.10734060e-02, mean val. rec. loss:  5.45763374e-03\n",
      "Epoch: 7404 mean train loss:  1.10705047e-02, mean val. rec. loss:  5.45614540e-03\n",
      "Epoch: 7405 mean train loss:  1.10676016e-02, mean val. rec. loss:  5.45442119e-03\n",
      "Epoch: 7406 mean train loss:  1.10646984e-02, mean val. rec. loss:  5.45295609e-03\n",
      "Epoch: 7407 mean train loss:  1.10618036e-02, mean val. rec. loss:  5.45123414e-03\n",
      "Epoch: 7408 mean train loss:  1.10589014e-02, mean val. rec. loss:  5.44984162e-03\n",
      "Epoch: 7409 mean train loss:  1.10559768e-02, mean val. rec. loss:  5.44802272e-03\n",
      "Epoch: 7410 mean train loss:  1.10530811e-02, mean val. rec. loss:  5.44667612e-03\n",
      "Epoch: 7411 mean train loss:  1.10501677e-02, mean val. rec. loss:  5.44485609e-03\n",
      "Epoch: 7412 mean train loss:  1.10472515e-02, mean val. rec. loss:  5.44355372e-03\n",
      "Epoch: 7413 mean train loss:  1.10443540e-02, mean val. rec. loss:  5.44161972e-03\n",
      "Epoch: 7414 mean train loss:  1.10414527e-02, mean val. rec. loss:  5.44051693e-03\n",
      "Epoch: 7415 mean train loss:  1.10385318e-02, mean val. rec. loss:  5.43837825e-03\n",
      "Epoch: 7416 mean train loss:  1.10356259e-02, mean val. rec. loss:  5.43742741e-03\n",
      "Epoch: 7417 mean train loss:  1.10327265e-02, mean val. rec. loss:  5.43516116e-03\n",
      "Epoch: 7418 mean train loss:  1.10298010e-02, mean val. rec. loss:  5.43446093e-03\n",
      "Epoch: 7419 mean train loss:  1.10269015e-02, mean val. rec. loss:  5.43177737e-03\n",
      "Epoch: 7420 mean train loss:  1.10239798e-02, mean val. rec. loss:  5.43160784e-03\n",
      "Epoch: 7421 mean train loss:  1.10210710e-02, mean val. rec. loss:  5.42837714e-03\n",
      "Epoch: 7422 mean train loss:  1.10181464e-02, mean val. rec. loss:  5.42878878e-03\n",
      "Epoch: 7423 mean train loss:  1.10152405e-02, mean val. rec. loss:  5.42479548e-03\n",
      "Epoch: 7424 mean train loss:  1.10123290e-02, mean val. rec. loss:  5.42633145e-03\n",
      "Epoch: 7425 mean train loss:  1.10093951e-02, mean val. rec. loss:  5.42083280e-03\n",
      "Epoch: 7426 mean train loss:  1.10064845e-02, mean val. rec. loss:  5.42430107e-03\n",
      "Epoch: 7427 mean train loss:  1.10035720e-02, mean val. rec. loss:  5.41629236e-03\n",
      "Epoch: 7428 mean train loss:  1.10006633e-02, mean val. rec. loss:  5.42316142e-03\n",
      "Epoch: 7429 mean train loss:  1.09977694e-02, mean val. rec. loss:  5.41061964e-03\n",
      "Epoch: 7430 mean train loss:  1.09948784e-02, mean val. rec. loss:  5.42374202e-03\n",
      "Epoch: 7431 mean train loss:  1.09920078e-02, mean val. rec. loss:  5.40278953e-03\n",
      "Epoch: 7432 mean train loss:  1.09891810e-02, mean val. rec. loss:  5.42745806e-03\n",
      "Epoch: 7433 mean train loss:  1.09864203e-02, mean val. rec. loss:  5.39125585e-03\n",
      "Epoch: 7434 mean train loss:  1.09838142e-02, mean val. rec. loss:  5.43747447e-03\n",
      "Epoch: 7435 mean train loss:  1.09814706e-02, mean val. rec. loss:  5.37322619e-03\n",
      "Epoch: 7436 mean train loss:  1.09795758e-02, mean val. rec. loss:  5.45895369e-03\n",
      "Epoch: 7437 mean train loss:  1.09784622e-02, mean val. rec. loss:  5.34751327e-03\n",
      "Epoch: 7438 mean train loss:  1.09785442e-02, mean val. rec. loss:  5.49528800e-03\n",
      "Epoch: 7439 mean train loss:  1.09799380e-02, mean val. rec. loss:  5.32077127e-03\n",
      "Epoch: 7440 mean train loss:  1.09823635e-02, mean val. rec. loss:  5.52946662e-03\n",
      "Epoch: 7441 mean train loss:  1.09838905e-02, mean val. rec. loss:  5.31109391e-03\n",
      "Epoch: 7442 mean train loss:  1.09820423e-02, mean val. rec. loss:  5.51141485e-03\n",
      "Epoch: 7443 mean train loss:  1.09748896e-02, mean val. rec. loss:  5.33372922e-03\n",
      "Epoch: 7444 mean train loss:  1.09639995e-02, mean val. rec. loss:  5.43283254e-03\n",
      "Epoch: 7445 mean train loss:  1.09538087e-02, mean val. rec. loss:  5.39215283e-03\n",
      "Epoch: 7446 mean train loss:  1.09482808e-02, mean val. rec. loss:  5.35803941e-03\n",
      "Epoch: 7447 mean train loss:  1.09477482e-02, mean val. rec. loss:  5.45520306e-03\n",
      "Epoch: 7448 mean train loss:  1.09493506e-02, mean val. rec. loss:  5.32565191e-03\n",
      "Epoch: 7449 mean train loss:  1.09493022e-02, mean val. rec. loss:  5.46196326e-03\n",
      "Epoch: 7450 mean train loss:  1.09455778e-02, mean val. rec. loss:  5.33885139e-03\n",
      "Epoch: 7451 mean train loss:  1.09389614e-02, mean val. rec. loss:  5.40945958e-03\n",
      "Epoch: 7452 mean train loss:  1.09323358e-02, mean val. rec. loss:  5.38292453e-03\n",
      "Epoch: 7453 mean train loss:  1.09281188e-02, mean val. rec. loss:  5.35568811e-03\n",
      "Epoch: 7454 mean train loss:  1.09265891e-02, mean val. rec. loss:  5.42518387e-03\n",
      "Epoch: 7455 mean train loss:  1.09260444e-02, mean val. rec. loss:  5.33195795e-03\n",
      "Epoch: 7456 mean train loss:  1.09244038e-02, mean val. rec. loss:  5.42684287e-03\n",
      "Epoch: 7457 mean train loss:  1.09208097e-02, mean val. rec. loss:  5.34338050e-03\n",
      "Epoch: 7458 mean train loss:  1.09159597e-02, mean val. rec. loss:  5.38867209e-03\n",
      "Epoch: 7459 mean train loss:  1.09113172e-02, mean val. rec. loss:  5.37591995e-03\n",
      "Epoch: 7460 mean train loss:  1.09079951e-02, mean val. rec. loss:  5.35104618e-03\n",
      "Epoch: 7461 mean train loss:  1.09059178e-02, mean val. rec. loss:  5.40196853e-03\n",
      "Epoch 07463: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 7462 mean train loss:  1.09041869e-02, mean val. rec. loss:  5.33426786e-03\n",
      "Epoch: 7463 mean train loss:  1.09018517e-02, mean val. rec. loss:  5.34011634e-03\n",
      "Epoch: 7464 mean train loss:  1.09006804e-02, mean val. rec. loss:  5.35519993e-03\n",
      "Epoch: 7465 mean train loss:  1.08990510e-02, mean val. rec. loss:  5.37419121e-03\n",
      "Epoch: 7466 mean train loss:  1.08985538e-02, mean val. rec. loss:  5.38971365e-03\n",
      "Epoch: 7467 mean train loss:  1.08991329e-02, mean val. rec. loss:  5.39550203e-03\n",
      "Epoch: 7468 mean train loss:  1.08993713e-02, mean val. rec. loss:  5.38982024e-03\n",
      "Epoch: 7469 mean train loss:  1.08985947e-02, mean val. rec. loss:  5.37624881e-03\n",
      "Epoch: 7470 mean train loss:  1.08975026e-02, mean val. rec. loss:  5.36107393e-03\n",
      "Epoch: 7471 mean train loss:  1.08970752e-02, mean val. rec. loss:  5.34987875e-03\n",
      "Epoch: 7472 mean train loss:  1.08972763e-02, mean val. rec. loss:  5.34568587e-03\n",
      "Epoch: 7473 mean train loss:  1.08973089e-02, mean val. rec. loss:  5.34894719e-03\n",
      "Epoch: 7474 mean train loss:  1.08967549e-02, mean val. rec. loss:  5.35793055e-03\n",
      "Epoch: 7475 mean train loss:  1.08959830e-02, mean val. rec. loss:  5.36915408e-03\n",
      "Epoch: 7476 mean train loss:  1.08956115e-02, mean val. rec. loss:  5.37826785e-03\n",
      "Epoch: 7477 mean train loss:  1.08956078e-02, mean val. rec. loss:  5.38177468e-03\n",
      "Epoch: 7478 mean train loss:  1.08955072e-02, mean val. rec. loss:  5.37873335e-03\n",
      "Epoch: 7479 mean train loss:  1.08950584e-02, mean val. rec. loss:  5.37096107e-03\n",
      "Epoch: 7480 mean train loss:  1.08944905e-02, mean val. rec. loss:  5.36195220e-03\n",
      "Epoch: 7481 mean train loss:  1.08941460e-02, mean val. rec. loss:  5.35504515e-03\n",
      "Epoch: 7482 mean train loss:  1.08940249e-02, mean val. rec. loss:  5.35228221e-03\n",
      "Epoch 07484: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch: 7483 mean train loss:  1.08938555e-02, mean val. rec. loss:  5.35412152e-03\n",
      "Epoch: 7484 mean train loss:  1.08934681e-02, mean val. rec. loss:  5.35463691e-03\n",
      "Epoch: 7485 mean train loss:  1.08934262e-02, mean val. rec. loss:  5.35544374e-03\n",
      "Epoch: 7486 mean train loss:  1.08933545e-02, mean val. rec. loss:  5.35649607e-03\n",
      "Epoch: 7487 mean train loss:  1.08932856e-02, mean val. rec. loss:  5.35772700e-03\n",
      "Epoch: 7488 mean train loss:  1.08932205e-02, mean val. rec. loss:  5.35906112e-03\n",
      "Epoch: 7489 mean train loss:  1.08931562e-02, mean val. rec. loss:  5.36046385e-03\n",
      "Epoch: 7490 mean train loss:  1.08931050e-02, mean val. rec. loss:  5.36186318e-03\n",
      "Epoch: 7491 mean train loss:  1.08930696e-02, mean val. rec. loss:  5.36320978e-03\n",
      "Epoch: 7492 mean train loss:  1.08930305e-02, mean val. rec. loss:  5.36446282e-03\n",
      "Epoch: 7493 mean train loss:  1.08929989e-02, mean val. rec. loss:  5.36557525e-03\n",
      "Epoch: 7494 mean train loss:  1.08929691e-02, mean val. rec. loss:  5.36652042e-03\n",
      "Epoch: 7495 mean train loss:  1.08929411e-02, mean val. rec. loss:  5.36728472e-03\n",
      "Epoch: 7496 mean train loss:  1.08929318e-02, mean val. rec. loss:  5.36785795e-03\n",
      "Epoch: 7497 mean train loss:  1.08929141e-02, mean val. rec. loss:  5.36823953e-03\n",
      "Epoch: 7498 mean train loss:  1.08928834e-02, mean val. rec. loss:  5.36840056e-03\n",
      "Epoch: 7499 mean train loss:  1.08928499e-02, mean val. rec. loss:  5.36841360e-03\n",
      "Epoch: 7500 mean train loss:  1.08928257e-02, mean val. rec. loss:  5.36825371e-03\n",
      "Epoch: 7501 mean train loss:  1.08928043e-02, mean val. rec. loss:  5.36794130e-03\n",
      "Epoch: 7502 mean train loss:  1.08927624e-02, mean val. rec. loss:  5.36752059e-03\n",
      "Epoch: 7503 mean train loss:  1.08927177e-02, mean val. rec. loss:  5.36701370e-03\n",
      "Epoch 07505: reducing learning rate of group 0 to 5.0000e-07.\n",
      "Epoch: 7504 mean train loss:  1.08926972e-02, mean val. rec. loss:  5.36644501e-03\n",
      "Epoch: 7505 mean train loss:  1.08926599e-02, mean val. rec. loss:  5.36638094e-03\n",
      "Epoch: 7506 mean train loss:  1.08926609e-02, mean val. rec. loss:  5.36631517e-03\n",
      "Epoch: 7507 mean train loss:  1.08926506e-02, mean val. rec. loss:  5.36623693e-03\n",
      "Epoch: 7508 mean train loss:  1.08926385e-02, mean val. rec. loss:  5.36616719e-03\n",
      "Epoch: 7509 mean train loss:  1.08926376e-02, mean val. rec. loss:  5.36608554e-03\n",
      "Epoch: 7510 mean train loss:  1.08926367e-02, mean val. rec. loss:  5.36602487e-03\n",
      "Epoch: 7511 mean train loss:  1.08926395e-02, mean val. rec. loss:  5.36594720e-03\n",
      "Epoch: 7512 mean train loss:  1.08926376e-02, mean val. rec. loss:  5.36587689e-03\n",
      "Epoch: 7513 mean train loss:  1.08926339e-02, mean val. rec. loss:  5.36579184e-03\n",
      "Epoch: 7514 mean train loss:  1.08926339e-02, mean val. rec. loss:  5.36572607e-03\n",
      "Epoch: 7515 mean train loss:  1.08926246e-02, mean val. rec. loss:  5.36565350e-03\n",
      "Epoch: 7516 mean train loss:  1.08926097e-02, mean val. rec. loss:  5.36558432e-03\n",
      "Epoch: 7517 mean train loss:  1.08926087e-02, mean val. rec. loss:  5.36550211e-03\n",
      "Epoch: 7518 mean train loss:  1.08926087e-02, mean val. rec. loss:  5.36543861e-03\n",
      "Epoch: 7519 mean train loss:  1.08926106e-02, mean val. rec. loss:  5.36537284e-03\n",
      "Epoch: 7520 mean train loss:  1.08926078e-02, mean val. rec. loss:  5.36528892e-03\n",
      "Epoch: 7521 mean train loss:  1.08926041e-02, mean val. rec. loss:  5.36522089e-03\n",
      "Epoch: 7522 mean train loss:  1.08926069e-02, mean val. rec. loss:  5.36515852e-03\n",
      "Epoch: 7523 mean train loss:  1.08925948e-02, mean val. rec. loss:  5.36508821e-03\n",
      "Epoch: 7524 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36502187e-03\n",
      "Epoch 07526: reducing learning rate of group 0 to 5.0000e-08.\n",
      "Epoch: 7525 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36496404e-03\n",
      "Epoch: 7526 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36496064e-03\n",
      "Epoch: 7527 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36495950e-03\n",
      "Epoch: 7528 mean train loss:  1.08925836e-02, mean val. rec. loss:  5.36495894e-03\n",
      "Epoch: 7529 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36495837e-03\n",
      "Epoch: 7530 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36495440e-03\n",
      "Epoch: 7531 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36495270e-03\n",
      "Epoch: 7532 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36495100e-03\n",
      "Epoch: 7533 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36494986e-03\n",
      "Epoch: 7534 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36494760e-03\n",
      "Epoch: 7535 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36494590e-03\n",
      "Epoch: 7536 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36494419e-03\n",
      "Epoch: 7537 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36494249e-03\n",
      "Epoch: 7538 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36493909e-03\n",
      "Epoch: 7539 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36493739e-03\n",
      "Epoch: 7540 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36493909e-03\n",
      "Epoch: 7541 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36493739e-03\n",
      "Epoch: 7542 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36493512e-03\n",
      "Epoch: 7543 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36493229e-03\n",
      "Epoch: 7544 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36493059e-03\n",
      "Epoch: 7545 mean train loss:  1.08925817e-02, mean val. rec. loss:  5.36493059e-03\n",
      "Epoch 07547: reducing learning rate of group 0 to 5.0000e-09.\n",
      "Epoch: 7546 mean train loss:  1.08925808e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 7547 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7548 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7549 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7550 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7551 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 7552 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7553 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7554 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 7555 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 7556 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 7557 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7558 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7559 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7560 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7561 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7562 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7563 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7564 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7565 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7566 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch 07568: reducing learning rate of group 0 to 5.0000e-10.\n",
      "Epoch: 7567 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7568 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7569 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7570 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7571 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7572 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7573 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7574 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7575 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7576 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7577 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7578 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7579 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7580 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7581 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7582 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7583 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7584 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7585 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7586 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7587 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch 07589: reducing learning rate of group 0 to 5.0000e-11.\n",
      "Epoch: 7588 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7589 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7590 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7591 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7592 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7593 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7594 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7595 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7596 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7597 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7598 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7599 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7600 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7601 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7602 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7603 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7604 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7605 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7606 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7607 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7608 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7609 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7610 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7611 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7612 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7613 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7614 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7615 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7616 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7617 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7618 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7619 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7620 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7621 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7622 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7623 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7624 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7625 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7626 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7627 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7628 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7629 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7630 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7631 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7632 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7633 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7634 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7635 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7636 mean train loss:  1.08925808e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7637 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7638 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7639 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7640 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7641 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7642 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7643 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7644 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7645 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7646 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7647 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7648 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7649 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7650 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7651 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7652 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7653 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7654 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7655 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7656 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7657 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7658 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7659 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7660 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7661 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7662 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7663 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7664 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7665 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7666 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7667 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7668 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7669 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7670 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7671 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7672 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7673 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7674 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7675 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7676 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7677 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7678 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7679 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7680 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7681 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7682 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7683 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7684 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7685 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7686 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7687 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7688 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7689 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7690 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7691 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7692 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7693 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7694 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7695 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7696 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7697 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7698 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7699 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7700 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7701 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7702 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7703 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7704 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7705 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7706 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7707 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7708 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7709 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7710 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7711 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7712 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7713 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7714 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7715 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7716 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7717 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7718 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7719 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7720 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7721 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7722 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7723 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7724 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7725 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7726 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7727 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7728 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7729 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7730 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7731 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7732 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7733 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7734 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7735 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7736 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7737 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7738 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7739 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7740 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7741 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7742 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7743 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7744 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7745 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7746 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7747 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7748 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7749 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7750 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7751 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7752 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7753 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7754 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7755 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7756 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7757 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7758 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7759 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7760 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7761 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7762 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7763 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7764 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7765 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7766 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7767 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7768 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7769 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7770 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7771 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7772 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7773 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7774 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7775 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7776 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7777 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7778 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7779 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7780 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7781 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7782 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7783 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7784 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7785 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7786 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7787 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7788 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7789 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7790 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7791 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7792 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7793 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7794 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7795 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7796 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7797 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7798 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7799 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7800 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7801 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7802 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7803 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7804 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7805 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7806 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7807 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7808 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7809 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7810 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7811 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7812 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7813 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7814 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7815 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7816 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7817 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7818 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7819 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7820 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7821 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7822 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7823 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7824 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7825 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7826 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7827 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7828 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7829 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7830 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7831 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7832 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7833 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7834 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7835 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7836 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7837 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7838 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7839 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7840 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7841 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7842 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7843 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7844 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7845 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7846 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7847 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7848 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7849 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7850 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7851 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7852 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7853 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7854 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7855 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7856 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7857 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7858 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7859 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7860 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7861 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7862 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7863 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7864 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7865 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7866 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7867 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7868 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7869 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7870 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7871 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7872 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7873 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7874 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7875 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7876 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7877 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7878 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7879 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7880 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7881 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7882 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7883 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7884 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7885 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7886 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7887 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7888 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7889 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7890 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7891 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7892 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7893 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7894 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7895 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7896 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7897 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7898 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7899 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7900 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7901 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7902 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7903 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7904 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7905 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7906 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7907 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7908 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7909 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7910 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7911 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7912 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7913 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7914 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7915 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7916 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7917 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7918 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7919 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7920 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7921 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7922 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7923 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7924 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7925 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7926 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7927 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7928 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7929 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7930 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7931 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7932 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7933 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7934 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7935 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7936 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7937 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7938 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7939 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7940 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7941 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7942 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7943 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7944 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7945 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7946 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7947 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7948 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7949 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7950 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7951 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7952 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7953 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7954 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7955 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7956 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7957 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7958 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7959 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7960 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7961 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7962 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7963 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7964 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7965 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7966 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7967 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7968 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7969 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7970 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7971 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7972 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7973 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7974 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7975 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7976 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7977 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7978 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7979 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7980 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7981 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7982 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7983 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7984 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7985 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7986 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7987 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7988 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7989 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7990 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7991 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7992 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7993 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7994 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7995 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7996 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7997 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7998 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 7999 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8000 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8001 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8002 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8003 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8004 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8005 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8006 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8007 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8008 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8009 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8010 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8011 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8012 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8013 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8014 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8015 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8016 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8017 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8018 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8019 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8020 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8021 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8022 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8023 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8024 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8025 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8026 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8027 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8028 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8029 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8030 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8031 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8032 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8033 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8034 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8035 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8036 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8037 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8038 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8039 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8040 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8041 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8042 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8043 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8044 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8045 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8046 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8047 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8048 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8049 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8050 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8051 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8052 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8053 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8054 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8055 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8056 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8057 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8058 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8059 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8060 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8061 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8062 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8063 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8064 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8065 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8066 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8067 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8068 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8069 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8070 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8071 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8072 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8073 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8074 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8075 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8076 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8077 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8078 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8079 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8080 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8081 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8082 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8083 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8084 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8085 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8086 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8087 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8088 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8089 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8090 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8091 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8092 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8093 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8094 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8095 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8096 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8097 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8098 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8099 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8100 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8101 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8102 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8103 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8104 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8105 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8106 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8107 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8108 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8109 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8110 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8111 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8112 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8113 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8114 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8115 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8116 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8117 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8118 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8119 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8120 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8121 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8122 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8123 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8124 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8125 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8126 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8127 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8128 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8129 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8130 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8131 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8132 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8133 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8134 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8135 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8136 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8137 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8138 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8139 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8140 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8141 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8142 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8143 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8144 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8145 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8146 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8147 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8148 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8149 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8150 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8151 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8152 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8153 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8154 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8155 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8156 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8157 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8158 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8159 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8160 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8161 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8162 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8163 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8164 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8165 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8166 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8167 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8168 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8169 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8170 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8171 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8172 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8173 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8174 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8175 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8176 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8177 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8178 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8179 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8180 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8181 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8182 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8183 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8184 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8185 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8186 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8187 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8188 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8189 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8190 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8191 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8192 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8193 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8194 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8195 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8196 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8197 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8198 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8199 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8200 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8201 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8202 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8203 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8204 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8205 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8206 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8207 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8208 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8209 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8210 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8211 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8212 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8213 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8214 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8215 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8216 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8217 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8218 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8219 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8220 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8221 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8222 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8223 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8224 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8225 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8226 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8227 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8228 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8229 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8230 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8231 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8232 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8233 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8234 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8235 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8236 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8237 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8238 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8239 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8240 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8241 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8242 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8243 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8244 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8245 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8246 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8247 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8248 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8249 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8250 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8251 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8252 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8253 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8254 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8255 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8256 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8257 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8258 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8259 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8260 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8261 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8262 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8263 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8264 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8265 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8266 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8267 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8268 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8269 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8270 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8271 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8272 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8273 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8274 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8275 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8276 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8277 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8278 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8279 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8280 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8281 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8282 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8283 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8284 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8285 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8286 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8287 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8288 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8289 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8290 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8291 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8292 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8293 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8294 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8295 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8296 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8297 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8298 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8299 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8300 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8301 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8302 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8303 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8304 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8305 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8306 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8307 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8308 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8309 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8310 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8311 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8312 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8313 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8314 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8315 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8316 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8317 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8318 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8319 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8320 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8321 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8322 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8323 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8324 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8325 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8326 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8327 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8328 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8329 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8330 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8331 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8332 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8333 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8334 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8335 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8336 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8337 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8338 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8339 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8340 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8341 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8342 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8343 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8344 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8345 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8346 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8347 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8348 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8349 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8350 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8351 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8352 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8353 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8354 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8355 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8356 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8357 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8358 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8359 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8360 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8361 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8362 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8363 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8364 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8365 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8366 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8367 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8368 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8369 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8370 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8371 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8372 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8373 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8374 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8375 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8376 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8377 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8378 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8379 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8380 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8381 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8382 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8383 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8384 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8385 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8386 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8387 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8388 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8389 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8390 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8391 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8392 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8393 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8394 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8395 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8396 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8397 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8398 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8399 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8400 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8401 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8402 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8403 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8404 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8405 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8406 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8407 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8408 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8409 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8410 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8411 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8412 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8413 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8414 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8415 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8416 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8417 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8418 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8419 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8420 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8421 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8422 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8423 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8424 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8425 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8426 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8427 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8428 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8429 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8430 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8431 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8432 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8433 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8434 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8435 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8436 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8437 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8438 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8439 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8440 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8441 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8442 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8443 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8444 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8445 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8446 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8447 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8448 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8449 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8450 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8451 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8452 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8453 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8454 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8455 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8456 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8457 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8458 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8459 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8460 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8461 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8462 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8463 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8464 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8465 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8466 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8467 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8468 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8469 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8470 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8471 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8472 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8473 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8474 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8475 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8476 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8477 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8478 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8479 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8480 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8481 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8482 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8483 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8484 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8485 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8486 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8487 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8488 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8489 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8490 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8491 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8492 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8493 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8494 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8495 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8496 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8497 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8498 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8499 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8500 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8501 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8502 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8503 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8504 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8505 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8506 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8507 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8508 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8509 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8510 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8511 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8512 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8513 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8514 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8515 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8516 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8517 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8518 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8519 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8520 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8521 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8522 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8523 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8524 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8525 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8526 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8527 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8528 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8529 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8530 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8531 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8532 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8533 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8534 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8535 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8536 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8537 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8538 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8539 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8540 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8541 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8542 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8543 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8544 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8545 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8546 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8547 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8548 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8549 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8550 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8551 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8552 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8553 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8554 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8555 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8556 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8557 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8558 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8559 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8560 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8561 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8562 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8563 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8564 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8565 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8566 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8567 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8568 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8569 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8570 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8571 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8572 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8573 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8574 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8575 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8576 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8577 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8578 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8579 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8580 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8581 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8582 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8583 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8584 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8585 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8586 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8587 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8588 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8589 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8590 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8591 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8592 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8593 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8594 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8595 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8596 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8597 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8598 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8599 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8600 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8601 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8602 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8603 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8604 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8605 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8606 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8607 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8608 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8609 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8610 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8611 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8612 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8613 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8614 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8615 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8616 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8617 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8618 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8619 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8620 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8621 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8622 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8623 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8624 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8625 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8626 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8627 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8628 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8629 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8630 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8631 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8632 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8633 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8634 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8635 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8636 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8637 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8638 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8639 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8640 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8641 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8642 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8643 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8644 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8645 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8646 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8647 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8648 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8649 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8650 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8651 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8652 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8653 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8654 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8655 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8656 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8657 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8658 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8659 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8660 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8661 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8662 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8663 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8664 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8665 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8666 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8667 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8668 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8669 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8670 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8671 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8672 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8673 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8674 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8675 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8676 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8677 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8678 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8679 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8680 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8681 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8682 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8683 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8684 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8685 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8686 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8687 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8688 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8689 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8690 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8691 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8692 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8693 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8694 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8695 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8696 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8697 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8698 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8699 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8700 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8701 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8702 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8703 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8704 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8705 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8706 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8707 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8708 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8709 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8710 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8711 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8712 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8713 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8714 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8715 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8716 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8717 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8718 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8719 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8720 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8721 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8722 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8723 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8724 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8725 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8726 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8727 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8728 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8729 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8730 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8731 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8732 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8733 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8734 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8735 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8736 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8737 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8738 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8739 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8740 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8741 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8742 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8743 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8744 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8745 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8746 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8747 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8748 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8749 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8750 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8751 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8752 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8753 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8754 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8755 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8756 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8757 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8758 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8759 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8760 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8761 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8762 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8763 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8764 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8765 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8766 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8767 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8768 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8769 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8770 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8771 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8772 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8773 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8774 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8775 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8776 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8777 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8778 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8779 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8780 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8781 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8782 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8783 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8784 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8785 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8786 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8787 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8788 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8789 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8790 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8791 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8792 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8793 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8794 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8795 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8796 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8797 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8798 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8799 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8800 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8801 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8802 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8803 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8804 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8805 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8806 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8807 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8808 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8809 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8810 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8811 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8812 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8813 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8814 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8815 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8816 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8817 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8818 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8819 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8820 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8821 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8822 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8823 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8824 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8825 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8826 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8827 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8828 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8829 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8830 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8831 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8832 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8833 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8834 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8835 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8836 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8837 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8838 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8839 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8840 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8841 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8842 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8843 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8844 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8845 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8846 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8847 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8848 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8849 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8850 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8851 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8852 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8853 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8854 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8855 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8856 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8857 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8858 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8859 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8860 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8861 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8862 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8863 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8864 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8865 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8866 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8867 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8868 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8869 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8870 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8871 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8872 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8873 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8874 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8875 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8876 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8877 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8878 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8879 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8880 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8881 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8882 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8883 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8884 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8885 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8886 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8887 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8888 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8889 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8890 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8891 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8892 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8893 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8894 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8895 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8896 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8897 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8898 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8899 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8900 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8901 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8902 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8903 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8904 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8905 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8906 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8907 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8908 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8909 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8910 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8911 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8912 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8913 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8914 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8915 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8916 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8917 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8918 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8919 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8920 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8921 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8922 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8923 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8924 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8925 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8926 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8927 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8928 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8929 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8930 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8931 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8932 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8933 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8934 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8935 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8936 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8937 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8938 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8939 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8940 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8941 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8942 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8943 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8944 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8945 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8946 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8947 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8948 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8949 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8950 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8951 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8952 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8953 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8954 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8955 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8956 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8957 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8958 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8959 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8960 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8961 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8962 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8963 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8964 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8965 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8966 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8967 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8968 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8969 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8970 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8971 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8972 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8973 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8974 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8975 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8976 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8977 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8978 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8979 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8980 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8981 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8982 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8983 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8984 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8985 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8986 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8987 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8988 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8989 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8990 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8991 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8992 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8993 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8994 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8995 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8996 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8997 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8998 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 8999 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9000 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9001 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9002 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9003 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9004 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9005 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9006 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9007 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9008 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9009 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9010 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9011 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9012 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9013 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9014 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9015 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9016 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9017 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9018 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9019 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9020 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9021 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9022 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9023 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9024 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9025 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9026 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9027 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9028 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9029 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9030 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9031 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9032 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9033 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9034 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9035 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9036 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9037 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9038 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9039 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9040 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9041 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9042 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9043 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9044 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9045 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9046 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9047 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9048 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9049 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9050 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9051 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9052 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9053 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9054 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9055 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9056 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9057 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9058 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9059 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9060 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9061 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9062 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9063 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9064 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9065 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9066 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9067 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9068 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9069 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9070 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9071 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9072 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9073 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9074 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9075 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9076 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9077 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9078 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9079 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9080 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9081 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9082 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9083 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9084 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9085 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9086 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9087 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9088 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9089 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9090 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9091 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9092 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9093 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9094 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9095 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9096 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9097 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9098 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9099 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9100 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9101 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9102 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9103 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9104 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9105 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9106 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9107 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9108 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9109 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9110 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9111 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9112 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9113 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9114 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9115 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9116 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9117 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9118 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9119 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9120 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9121 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9122 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9123 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9124 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9125 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9126 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9127 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9128 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9129 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9130 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9131 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9132 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9133 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9134 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9135 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9136 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9137 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9138 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9139 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9140 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9141 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9142 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9143 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9144 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9145 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9146 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9147 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9148 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9149 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9150 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9151 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9152 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9153 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9154 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9155 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9156 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9157 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9158 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9159 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9160 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9161 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9162 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9163 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9164 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9165 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9166 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9167 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9168 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9169 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9170 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9171 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9172 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9173 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9174 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9175 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9176 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9177 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9178 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9179 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9180 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9181 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9182 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9183 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9184 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9185 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9186 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9187 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9188 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9189 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9190 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9191 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9192 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9193 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9194 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9195 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9196 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9197 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9198 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9199 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9200 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9201 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9202 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9203 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9204 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9205 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9206 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9207 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9208 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9209 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9210 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9211 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9212 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9213 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9214 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9215 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9216 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9217 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9218 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9219 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9220 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9221 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9222 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9223 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9224 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9225 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9226 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9227 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9228 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9229 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9230 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9231 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9232 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9233 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9234 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9235 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9236 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9237 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9238 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9239 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9240 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9241 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9242 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9243 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9244 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9245 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9246 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9247 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9248 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9249 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9250 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9251 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9252 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9253 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9254 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9255 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9256 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9257 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9258 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9259 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9260 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9261 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9262 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9263 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9264 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9265 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9266 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9267 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9268 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9269 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9270 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9271 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9272 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9273 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9274 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9275 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9276 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9277 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9278 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9279 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9280 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9281 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9282 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9283 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9284 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9285 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9286 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9287 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9288 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9289 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9290 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9291 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9292 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9293 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9294 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9295 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9296 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9297 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9298 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9299 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9300 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9301 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9302 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9303 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9304 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9305 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9306 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9307 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9308 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9309 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9310 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9311 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9312 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9313 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9314 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9315 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9316 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9317 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9318 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9319 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9320 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9321 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9322 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9323 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9324 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9325 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9326 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9327 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9328 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9329 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9330 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9331 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9332 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9333 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9334 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9335 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9336 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9337 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9338 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9339 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9340 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9341 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9342 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9343 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9344 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9345 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9346 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9347 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9348 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9349 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9350 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9351 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9352 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9353 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9354 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9355 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9356 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9357 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9358 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9359 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9360 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9361 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9362 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9363 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9364 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9365 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9366 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9367 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9368 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9369 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9370 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9371 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9372 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9373 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9374 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9375 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9376 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9377 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9378 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9379 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9380 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9381 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9382 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9383 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9384 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9385 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9386 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9387 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9388 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9389 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9390 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9391 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9392 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9393 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9394 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9395 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9396 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9397 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9398 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9399 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9400 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9401 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9402 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9403 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9404 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9405 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9406 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9407 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9408 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9409 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9410 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9411 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9412 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9413 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9414 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9415 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9416 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9417 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9418 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9419 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9420 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9421 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9422 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9423 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9424 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9425 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9426 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9427 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9428 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9429 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9430 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9431 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9432 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9433 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9434 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9435 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9436 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9437 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9438 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9439 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9440 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9441 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9442 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9443 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9444 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9445 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9446 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9447 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9448 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9449 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9450 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9451 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9452 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9453 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9454 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9455 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9456 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9457 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9458 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9459 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9460 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9461 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9462 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9463 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9464 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9465 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9466 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9467 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9468 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9469 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9470 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9471 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9472 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9473 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9474 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9475 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9476 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9477 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9478 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9479 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9480 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9481 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9482 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9483 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9484 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9485 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9486 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9487 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9488 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9489 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9490 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9491 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9492 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9493 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9494 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9495 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9496 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9497 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9498 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9499 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9500 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9501 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9502 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9503 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9504 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9505 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9506 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9507 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9508 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9509 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9510 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9511 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9512 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9513 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9514 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9515 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9516 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9517 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9518 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9519 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9520 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9521 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9522 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9523 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9524 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9525 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9526 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9527 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9528 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9529 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9530 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9531 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9532 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9533 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9534 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9535 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9536 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9537 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9538 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9539 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9540 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9541 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9542 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9543 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9544 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9545 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9546 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9547 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9548 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9549 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9550 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9551 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9552 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9553 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9554 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9555 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9556 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9557 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9558 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9559 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9560 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9561 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9562 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9563 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9564 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9565 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9566 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9567 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9568 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9569 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9570 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9571 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9572 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9573 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9574 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9575 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9576 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9577 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9578 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9579 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9580 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9581 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9582 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9583 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9584 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9585 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9586 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9587 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9588 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9589 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9590 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9591 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9592 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9593 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9594 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9595 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9596 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9597 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9598 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9599 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9600 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9601 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9602 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9603 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9604 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9605 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9606 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9607 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9608 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9609 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9610 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9611 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9612 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9613 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9614 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9615 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9616 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9617 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9618 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9619 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9620 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9621 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9622 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9623 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9624 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9625 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9626 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9627 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9628 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9629 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9630 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9631 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9632 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9633 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9634 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9635 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9636 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9637 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9638 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9639 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9640 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9641 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9642 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9643 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9644 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9645 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9646 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9647 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9648 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9649 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9650 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9651 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9652 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9653 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9654 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9655 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9656 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9657 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9658 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9659 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9660 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9661 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9662 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9663 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9664 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9665 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9666 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9667 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9668 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9669 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9670 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9671 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9672 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9673 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9674 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9675 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9676 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9677 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9678 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9679 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9680 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9681 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9682 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9683 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9684 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9685 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9686 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9687 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9688 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492492e-03\n",
      "Epoch: 9689 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9690 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9691 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9692 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9693 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9694 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9695 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9696 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9697 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9698 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9699 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9700 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9701 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9702 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9703 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9704 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9705 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9706 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9707 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9708 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9709 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9710 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9711 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9712 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9713 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9714 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9715 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9716 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9717 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9718 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9719 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9720 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9721 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9722 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9723 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9724 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9725 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9726 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9727 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9728 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9729 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9730 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9731 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9732 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9733 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9734 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9735 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9736 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9737 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9738 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9739 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9740 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9741 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9742 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9743 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9744 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9745 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9746 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9747 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9748 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9749 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9750 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9751 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9752 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9753 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9754 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9755 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9756 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9757 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9758 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9759 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9760 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9761 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9762 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9763 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9764 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9765 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9766 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9767 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9768 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9769 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9770 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9771 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9772 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9773 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9774 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9775 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9776 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9777 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9778 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9779 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9780 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9781 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9782 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9783 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9784 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9785 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9786 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9787 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9788 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9789 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9790 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9791 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9792 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9793 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9794 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9795 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9796 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9797 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9798 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9799 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9800 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9801 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9802 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9803 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9804 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9805 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9806 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9807 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9808 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9809 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9810 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9811 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9812 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9813 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9814 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9815 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9816 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9817 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9818 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9819 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9820 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9821 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9822 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9823 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9824 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9825 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9826 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9827 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9828 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9829 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9830 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9831 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9832 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9833 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9834 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9835 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9836 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9837 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9838 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9839 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9840 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9841 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9842 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9843 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9844 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9845 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9846 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9847 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9848 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9849 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9850 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9851 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9852 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9853 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9854 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9855 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9856 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9857 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9858 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9859 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9860 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9861 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9862 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9863 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9864 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9865 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9866 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9867 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9868 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9869 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9870 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9871 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9872 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9873 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9874 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9875 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9876 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9877 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9878 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9879 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9880 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9881 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9882 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9883 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9884 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9885 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9886 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9887 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9888 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9889 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9890 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9891 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9892 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9893 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9894 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9895 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9896 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9897 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9898 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9899 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9900 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9901 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9902 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9903 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9904 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9905 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9906 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9907 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9908 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9909 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9910 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9911 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9912 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9913 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9914 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9915 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9916 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9917 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492548e-03\n",
      "Epoch: 9918 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9919 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9920 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9921 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9922 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9923 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9924 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9925 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9926 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9927 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9928 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9929 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9930 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9931 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9932 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9933 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9934 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9935 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9936 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9937 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9938 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9939 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9940 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9941 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9942 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9943 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9944 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9945 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9946 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9947 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9948 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9949 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9950 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9951 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9952 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9953 mean train loss:  1.08925789e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9954 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9955 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9956 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9957 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9958 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9959 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9960 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9961 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9962 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9963 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9964 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9965 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9966 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9967 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9968 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9969 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9970 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9971 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9972 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9973 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9974 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9975 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9976 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9977 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9978 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9979 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9980 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9981 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9982 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9983 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9984 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9985 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9986 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9987 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9988 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9989 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9990 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9991 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9992 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9993 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9994 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9995 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9996 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9997 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9998 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n",
      "Epoch: 9999 mean train loss:  1.08925799e-02, mean val. rec. loss:  5.36492605e-03\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = train(net_H2, loaders, args, A, H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NN_library/training_data/PINN_H2_{total_params}', np.vstack([losses_train, losses_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1 = net_H1(grid_data).detach().cpu()\n",
    "U2 = net_H2(grid_data).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x13e24b38a90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n",
    "pos1 = axs[0].pcolormesh(XY[0], XY[1], U1.reshape(N, N), cmap='inferno')\n",
    "fig.colorbar(pos1, ax=axs[0])\n",
    "pos2 = axs[1].pcolormesh(XY[0], XY[1], U2.reshape(N, N), cmap='inferno')\n",
    "fig.colorbar(pos2, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x13e60add4d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_1 = PDE_loss(grid_data, net_H1, A, H1).detach().cpu()\n",
    "error_2 = PDE_loss(grid_data, net_H2, A, H2).detach().cpu()\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n",
    "pos1 = axs[0].pcolormesh(XY[0], XY[1], error_1.reshape(N, N), cmap='hot')\n",
    "#axs[0].scatter(data[:,0], data[:,1], s=0.5, c='g')\n",
    "fig.colorbar(pos1, ax=axs[0])\n",
    "pos2 = axs[1].pcolormesh(XY[0], XY[1], error_2.reshape(N, N), cmap='hot')\n",
    "#axs[1].scatter(data[:,0], data[:,1], s=0.5, c='g')\n",
    "fig.colorbar(pos2, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data[:].to(dev).requires_grad_(True)\n",
    "T1 = net_H1(inputs)\n",
    "T2 = net_H2(inputs)\n",
    "dq1 = torch.autograd.grad(\n",
    "            outputs=T1, inputs=inputs,\n",
    "            grad_outputs=torch.ones_like(T1),\n",
    "            create_graph=True, retain_graph=True\n",
    "    )[0].detach()\n",
    "dq2 = torch.autograd.grad(\n",
    "            outputs=T2, inputs=inputs,\n",
    "            grad_outputs=torch.ones_like(T2),\n",
    "            create_graph=True, retain_graph=True\n",
    "    )[0].detach()\n",
    "triang = Triangulation(data[:,0], data[:,1])\n",
    "n_elem = len(triang.triangles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bound(dq1, dq2, tri, data):\n",
    "    # Compute the A_h\n",
    "    A_h = np.zeros((2,2))\n",
    "    n_elem = len(tri.triangles)\n",
    "    for i in range(n_elem):\n",
    "        elem = tri.triangles[i]\n",
    "        node_coords = data[elem]\n",
    "        vector = node_coords[1:]-node_coords[:-1]\n",
    "        vectors = torch.hstack((vector, torch.tensor([[0], [0]], device=dev)))\n",
    "        area = (1/2) * abs(torch.cross(vectors[0], vectors[1])[-1])\n",
    "        A_loc = A(node_coords)\n",
    "        Q1 = A_loc @ (dq1[elem] + H1).view(3,2,1)\n",
    "        Q2 = A_loc @ (dq2[elem] + H2).view(3,2,1)\n",
    "        Q1_mean = Q1.mean(dim=0)\n",
    "        Q2_mean = Q2.mean(dim=0)\n",
    "        A_h_elem_1 = (area * Q1_mean).squeeze().detach().cpu().numpy()\n",
    "        A_h_elem_2 = (area * Q2_mean).squeeze().detach().cpu().numpy()\n",
    "        A_h[:,0] += A_h_elem_1\n",
    "        A_h[:,1] += A_h_elem_2\n",
    "    return A_h / L**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.08769387e+00 8.06598855e-05]\n",
      " [1.24026602e-05 1.07214200e+00]]\n"
     ]
    }
   ],
   "source": [
    "A_bound = compute_bound(dq1, dq2, triang, inputs)\n",
    "print(A_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'bounds/A_u_NN_{total_params}.npy', A_bound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
