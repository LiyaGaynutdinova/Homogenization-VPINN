{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dataset import *\n",
    "from save_load import *\n",
    "from NN_library.PINN import *\n",
    "from NN_library.train_primal_PINN import *\n",
    "from NN_library.train_dual_PINN import *\n",
    "from NN_library.MN.material_NN import *\n",
    "from matplotlib.tri import Triangulation\n",
    "from utility import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_grid(129, [0, 2*np.pi], [0, 2*np.pi])\n",
    "loaders = get_loaders_Sobol(data, 129**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x20add83bd10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = 2*np.pi\n",
    "N = 512 # number of nodes in each direction including the border\n",
    "x = np.linspace(0, L, N, endpoint=True)\n",
    "y = np.linspace(0, L, N, endpoint=True)\n",
    "\n",
    "XY = np.meshgrid(x, y)\n",
    "grid_data = torch.tensor(np.vstack((XY[0].flatten(), XY[1].flatten())).T, dtype=torch.float, device=dev)\n",
    "\n",
    "mat_net = Material_NN()\n",
    "args = {'lr' : 0.001, 'epochs' : 30000, 'dev' : dev, 'name' : f'NN_library/MN/MN_16'}\n",
    "mat_net = load_network(mat_net, args['name']+'_29999', args).to(dev)\n",
    "\n",
    "def A_interp(x): \n",
    "    a = 0.1 + 0.9*mat_net(x)\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = a.view(-1,1,1) * I\n",
    "    return A\n",
    "\n",
    "def A_interp_inv(x):  \n",
    "    a = 1 / (0.1 + 0.9*mat_net(x))\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = a.view(-1,1,1) * I\n",
    "    return A\n",
    "\n",
    "def A(x):  \n",
    "    x = x - torch.pi/2\n",
    "    a = torch.where((x[:,0]<torch.pi)&(x[:,1]<torch.pi)&(x[:,0]>0)&(x[:,1]>0), 0.1, 1.).view(-1,1,1)\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = a * I\n",
    "    return A\n",
    "\n",
    "def A_inv(x):  \n",
    "    x = x - torch.pi/2\n",
    "    a = torch.where((x[:,0]<torch.pi)&(x[:,1]<torch.pi)&(x[:,0]>0)&(x[:,1]>0), 0.1, 1.).view(-1,1,1)\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = (1 / a) * I\n",
    "    return A\n",
    "\n",
    "def H1(x):\n",
    "    H = torch.zeros_like(x)\n",
    "    H[:,0] = 1.\n",
    "    return H\n",
    "\n",
    "Z = 0.1 + 0.9*mat_net(grid_data).detach().cpu()\n",
    "plt.pcolormesh(XY[0], XY[1], Z.reshape(N, N), vmin=0.1)\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:,0], data[:,1], s = 0.05, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primal PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 391\n"
     ]
    }
   ],
   "source": [
    "net_primal = PINN(n_periodic=5, n_hidden=10, n_layers=2, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_primal.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "args = {'lr' : 0.0001, 'epochs' : 40000, 'dev' : dev, 'name' : f'NN_library/PINN/square/MNPINN_primal_{total_params}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_primal = load_network(net_primal, args['name']+'_39999', args)\n",
    "net_primal = net_primal.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train, losses_val = train_primal(net_primal, loaders, args, A_interp, H1, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NN_library/training_data/square/MNPINN_primal_{total_params}', np.vstack([losses_train, losses_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1 = net_primal(grid_data).detach().cpu()\n",
    "error_1, _ = PDE_loss(grid_data, net_primal, A_interp, H1)\n",
    "error_2, _ = PDE_loss(grid_data, net_primal, A, H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1b1156fd550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 5850x1500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(19.5,5), dpi=300)\n",
    "pos1 = axs[0].pcolormesh(XY[0], XY[1], U1.reshape(N, N), cmap='inferno')\n",
    "fig.colorbar(pos1, ax=axs[0])\n",
    "pos2 = axs[1].pcolormesh(XY[0], XY[1], error_1.detach().cpu().reshape(N, N), cmap='hot')\n",
    "fig.colorbar(pos2, ax=axs[1])\n",
    "pos3 = axs[2].pcolormesh(XY[0], XY[1], error_2.detach().cpu().reshape(N, N), cmap='hot')\n",
    "fig.colorbar(pos3, ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7776840925216675\n",
      "0.7707076258297628\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(loaders['train']):\n",
    "    areas, tri = get_areas(x)\n",
    "    areas = areas.to(args['dev'])\n",
    "x = x.to(args['dev'])\n",
    "x_rot = torch.fliplr(x)\n",
    "\n",
    "_, q1, gH = PDE_loss(x, net_primal, A_interp, H1)\n",
    "bound_1 = compute_estimate(areas, tri, q1, gH, L).detach()[0].item()\n",
    "print(bound_1)\n",
    "\n",
    "U1 = net_primal(x)\n",
    "U2 = net_primal(x_rot)\n",
    "bound_2 = compute_bound_primal(U1, U2, tri, x, A, L)[0,0]\n",
    "print(bound_2)\n",
    "\n",
    "np.save(f'bounds/square/A_u_MNPINN_{total_params}_{N}.npy', (bound_1, bound_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 391\n"
     ]
    }
   ],
   "source": [
    "net_dual = PINN(n_periodic=5, n_hidden=10, n_layers=2, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_dual.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "args = {'lr' : 0.0001, 'epochs' : 40000, 'dev' : dev, 'name' : f'NN_library/PINN/square/MNPINN_dual_{total_params}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dual = load_network(net_dual, args['name']+'_39999', args)\n",
    "net_dual = net_dual.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 mean train loss:  2.56384516e+00, bound:  3.13343257e-01\n",
      "Epoch: 1 mean train loss:  2.56133556e+00, bound:  3.13348204e-01\n",
      "Epoch: 2 mean train loss:  2.55886817e+00, bound:  3.13353151e-01\n",
      "Epoch: 3 mean train loss:  2.55644488e+00, bound:  3.13358605e-01\n",
      "Epoch: 4 mean train loss:  2.55406451e+00, bound:  3.13364863e-01\n",
      "Epoch: 5 mean train loss:  2.55172801e+00, bound:  3.13372314e-01\n",
      "Epoch: 6 mean train loss:  2.54943514e+00, bound:  3.13381165e-01\n",
      "Epoch: 7 mean train loss:  2.54718518e+00, bound:  3.13391149e-01\n",
      "Epoch: 8 mean train loss:  2.54497790e+00, bound:  3.13401878e-01\n",
      "Epoch: 9 mean train loss:  2.54281378e+00, bound:  3.13413203e-01\n",
      "Epoch: 10 mean train loss:  2.54069257e+00, bound:  3.13424766e-01\n",
      "Epoch: 11 mean train loss:  2.53861356e+00, bound:  3.13436568e-01\n",
      "Epoch: 12 mean train loss:  2.53657651e+00, bound:  3.13448489e-01\n",
      "Epoch: 13 mean train loss:  2.53457999e+00, bound:  3.13460529e-01\n",
      "Epoch: 14 mean train loss:  2.53262520e+00, bound:  3.13472688e-01\n",
      "Epoch: 15 mean train loss:  2.53070951e+00, bound:  3.13485026e-01\n",
      "Epoch: 16 mean train loss:  2.52883506e+00, bound:  3.13497514e-01\n",
      "Epoch: 17 mean train loss:  2.52699852e+00, bound:  3.13510209e-01\n",
      "Epoch: 18 mean train loss:  2.52520084e+00, bound:  3.13523054e-01\n",
      "Epoch: 19 mean train loss:  2.52344036e+00, bound:  3.13536137e-01\n",
      "Epoch: 20 mean train loss:  2.52171588e+00, bound:  3.13549399e-01\n",
      "Epoch: 21 mean train loss:  2.52002740e+00, bound:  3.13562751e-01\n",
      "Epoch: 22 mean train loss:  2.51837492e+00, bound:  3.13576251e-01\n",
      "Epoch: 23 mean train loss:  2.51675582e+00, bound:  3.13589782e-01\n",
      "Epoch: 24 mean train loss:  2.51517034e+00, bound:  3.13603520e-01\n",
      "Epoch: 25 mean train loss:  2.51361775e+00, bound:  3.13617289e-01\n",
      "Epoch: 26 mean train loss:  2.51209641e+00, bound:  3.13631147e-01\n",
      "Epoch: 27 mean train loss:  2.51060534e+00, bound:  3.13645005e-01\n",
      "Epoch: 28 mean train loss:  2.50914478e+00, bound:  3.13658953e-01\n",
      "Epoch: 29 mean train loss:  2.50771284e+00, bound:  3.13672960e-01\n",
      "Epoch: 30 mean train loss:  2.50630856e+00, bound:  3.13686997e-01\n",
      "Epoch: 31 mean train loss:  2.50493217e+00, bound:  3.13701063e-01\n",
      "Epoch: 32 mean train loss:  2.50358152e+00, bound:  3.13715130e-01\n",
      "Epoch: 33 mean train loss:  2.50225616e+00, bound:  3.13729227e-01\n",
      "Epoch: 34 mean train loss:  2.50095606e+00, bound:  3.13743293e-01\n",
      "Epoch: 35 mean train loss:  2.49967885e+00, bound:  3.13757390e-01\n",
      "Epoch: 36 mean train loss:  2.49842358e+00, bound:  3.13771456e-01\n",
      "Epoch: 37 mean train loss:  2.49719143e+00, bound:  3.13785464e-01\n",
      "Epoch: 38 mean train loss:  2.49598002e+00, bound:  3.13799500e-01\n",
      "Epoch: 39 mean train loss:  2.49478936e+00, bound:  3.13813478e-01\n",
      "Epoch: 40 mean train loss:  2.49361753e+00, bound:  3.13827395e-01\n",
      "Epoch: 41 mean train loss:  2.49246502e+00, bound:  3.13841283e-01\n",
      "Epoch: 42 mean train loss:  2.49133015e+00, bound:  3.13855112e-01\n",
      "Epoch: 43 mean train loss:  2.49021244e+00, bound:  3.13868910e-01\n",
      "Epoch: 44 mean train loss:  2.48911238e+00, bound:  3.13882738e-01\n",
      "Epoch: 45 mean train loss:  2.48802710e+00, bound:  3.13896447e-01\n",
      "Epoch: 46 mean train loss:  2.48695731e+00, bound:  3.13910097e-01\n",
      "Epoch: 47 mean train loss:  2.48590255e+00, bound:  3.13923746e-01\n",
      "Epoch: 48 mean train loss:  2.48486280e+00, bound:  3.13937366e-01\n",
      "Epoch: 49 mean train loss:  2.48383594e+00, bound:  3.13950866e-01\n",
      "Epoch: 50 mean train loss:  2.48282218e+00, bound:  3.13964456e-01\n",
      "Epoch: 51 mean train loss:  2.48182130e+00, bound:  3.13977957e-01\n",
      "Epoch: 52 mean train loss:  2.48083138e+00, bound:  3.13991487e-01\n",
      "Epoch: 53 mean train loss:  2.47985387e+00, bound:  3.14004928e-01\n",
      "Epoch: 54 mean train loss:  2.47888780e+00, bound:  3.14018428e-01\n",
      "Epoch: 55 mean train loss:  2.47793198e+00, bound:  3.14031869e-01\n",
      "Epoch: 56 mean train loss:  2.47698641e+00, bound:  3.14045340e-01\n",
      "Epoch: 57 mean train loss:  2.47605038e+00, bound:  3.14058810e-01\n",
      "Epoch: 58 mean train loss:  2.47512388e+00, bound:  3.14072311e-01\n",
      "Epoch: 59 mean train loss:  2.47420764e+00, bound:  3.14085811e-01\n",
      "Epoch: 60 mean train loss:  2.47329926e+00, bound:  3.14099312e-01\n",
      "Epoch: 61 mean train loss:  2.47239923e+00, bound:  3.14112812e-01\n",
      "Epoch: 62 mean train loss:  2.47150803e+00, bound:  3.14126343e-01\n",
      "Epoch: 63 mean train loss:  2.47062421e+00, bound:  3.14139903e-01\n",
      "Epoch: 64 mean train loss:  2.46974754e+00, bound:  3.14153522e-01\n",
      "Epoch: 65 mean train loss:  2.46887898e+00, bound:  3.14167142e-01\n",
      "Epoch: 66 mean train loss:  2.46801734e+00, bound:  3.14180791e-01\n",
      "Epoch: 67 mean train loss:  2.46716237e+00, bound:  3.14194530e-01\n",
      "Epoch: 68 mean train loss:  2.46631360e+00, bound:  3.14208239e-01\n",
      "Epoch: 69 mean train loss:  2.46547103e+00, bound:  3.14222038e-01\n",
      "Epoch: 70 mean train loss:  2.46463561e+00, bound:  3.14235836e-01\n",
      "Epoch: 71 mean train loss:  2.46380448e+00, bound:  3.14249665e-01\n",
      "Epoch: 72 mean train loss:  2.46297956e+00, bound:  3.14263582e-01\n",
      "Epoch: 73 mean train loss:  2.46215987e+00, bound:  3.14277470e-01\n",
      "Epoch: 74 mean train loss:  2.46134639e+00, bound:  3.14291447e-01\n",
      "Epoch: 75 mean train loss:  2.46053696e+00, bound:  3.14305425e-01\n",
      "Epoch: 76 mean train loss:  2.45973277e+00, bound:  3.14319491e-01\n",
      "Epoch: 77 mean train loss:  2.45893335e+00, bound:  3.14333558e-01\n",
      "Epoch: 78 mean train loss:  2.45813894e+00, bound:  3.14347625e-01\n",
      "Epoch: 79 mean train loss:  2.45734859e+00, bound:  3.14361751e-01\n",
      "Epoch: 80 mean train loss:  2.45656180e+00, bound:  3.14375877e-01\n",
      "Epoch: 81 mean train loss:  2.45577908e+00, bound:  3.14390093e-01\n",
      "Epoch: 82 mean train loss:  2.45500040e+00, bound:  3.14404279e-01\n",
      "Epoch: 83 mean train loss:  2.45422626e+00, bound:  3.14418525e-01\n",
      "Epoch: 84 mean train loss:  2.45345592e+00, bound:  3.14432770e-01\n",
      "Epoch: 85 mean train loss:  2.45268798e+00, bound:  3.14447045e-01\n",
      "Epoch: 86 mean train loss:  2.45192409e+00, bound:  3.14461321e-01\n",
      "Epoch: 87 mean train loss:  2.45116282e+00, bound:  3.14475656e-01\n",
      "Epoch: 88 mean train loss:  2.45040536e+00, bound:  3.14489961e-01\n",
      "Epoch: 89 mean train loss:  2.44965005e+00, bound:  3.14504325e-01\n",
      "Epoch: 90 mean train loss:  2.44889784e+00, bound:  3.14518660e-01\n",
      "Epoch: 91 mean train loss:  2.44814944e+00, bound:  3.14532995e-01\n",
      "Epoch: 92 mean train loss:  2.44740272e+00, bound:  3.14547420e-01\n",
      "Epoch: 93 mean train loss:  2.44665885e+00, bound:  3.14561784e-01\n",
      "Epoch: 94 mean train loss:  2.44591713e+00, bound:  3.14576179e-01\n",
      "Epoch: 95 mean train loss:  2.44517827e+00, bound:  3.14590603e-01\n",
      "Epoch: 96 mean train loss:  2.44444060e+00, bound:  3.14604998e-01\n",
      "Epoch: 97 mean train loss:  2.44370604e+00, bound:  3.14619452e-01\n",
      "Epoch: 98 mean train loss:  2.44297314e+00, bound:  3.14633876e-01\n",
      "Epoch: 99 mean train loss:  2.44224167e+00, bound:  3.14648300e-01\n",
      "Epoch: 100 mean train loss:  2.44151306e+00, bound:  3.14662755e-01\n",
      "Epoch: 101 mean train loss:  2.44078517e+00, bound:  3.14677238e-01\n",
      "Epoch: 102 mean train loss:  2.44005942e+00, bound:  3.14691693e-01\n",
      "Epoch: 103 mean train loss:  2.43933511e+00, bound:  3.14706177e-01\n",
      "Epoch: 104 mean train loss:  2.43861294e+00, bound:  3.14720631e-01\n",
      "Epoch: 105 mean train loss:  2.43789101e+00, bound:  3.14735144e-01\n",
      "Epoch: 106 mean train loss:  2.43717074e+00, bound:  3.14749688e-01\n",
      "Epoch: 107 mean train loss:  2.43645215e+00, bound:  3.14764172e-01\n",
      "Epoch: 108 mean train loss:  2.43573332e+00, bound:  3.14778745e-01\n",
      "Epoch: 109 mean train loss:  2.43501759e+00, bound:  3.14793289e-01\n",
      "Epoch: 110 mean train loss:  2.43430114e+00, bound:  3.14807832e-01\n",
      "Epoch: 111 mean train loss:  2.43358636e+00, bound:  3.14822465e-01\n",
      "Epoch: 112 mean train loss:  2.43287206e+00, bound:  3.14837068e-01\n",
      "Epoch: 113 mean train loss:  2.43215919e+00, bound:  3.14851701e-01\n",
      "Epoch: 114 mean train loss:  2.43144584e+00, bound:  3.14866364e-01\n",
      "Epoch: 115 mean train loss:  2.43073392e+00, bound:  3.14881027e-01\n",
      "Epoch: 116 mean train loss:  2.43002248e+00, bound:  3.14895749e-01\n",
      "Epoch: 117 mean train loss:  2.42931175e+00, bound:  3.14910471e-01\n",
      "Epoch: 118 mean train loss:  2.42860031e+00, bound:  3.14925283e-01\n",
      "Epoch: 119 mean train loss:  2.42788982e+00, bound:  3.14940065e-01\n",
      "Epoch: 120 mean train loss:  2.42717934e+00, bound:  3.14954877e-01\n",
      "Epoch: 121 mean train loss:  2.42646980e+00, bound:  3.14969748e-01\n",
      "Epoch: 122 mean train loss:  2.42575979e+00, bound:  3.14984679e-01\n",
      "Epoch: 123 mean train loss:  2.42505002e+00, bound:  3.14999610e-01\n",
      "Epoch: 124 mean train loss:  2.42434049e+00, bound:  3.15014601e-01\n",
      "Epoch: 125 mean train loss:  2.42363071e+00, bound:  3.15029591e-01\n",
      "Epoch: 126 mean train loss:  2.42292070e+00, bound:  3.15044671e-01\n",
      "Epoch: 127 mean train loss:  2.42221022e+00, bound:  3.15059721e-01\n",
      "Epoch: 128 mean train loss:  2.42149997e+00, bound:  3.15074891e-01\n",
      "Epoch: 129 mean train loss:  2.42078924e+00, bound:  3.15090090e-01\n",
      "Epoch: 130 mean train loss:  2.42007804e+00, bound:  3.15105349e-01\n",
      "Epoch: 131 mean train loss:  2.41936707e+00, bound:  3.15120608e-01\n",
      "Epoch: 132 mean train loss:  2.41865563e+00, bound:  3.15135926e-01\n",
      "Epoch: 133 mean train loss:  2.41794252e+00, bound:  3.15151304e-01\n",
      "Epoch: 134 mean train loss:  2.41722989e+00, bound:  3.15166771e-01\n",
      "Epoch: 135 mean train loss:  2.41651678e+00, bound:  3.15182209e-01\n",
      "Epoch: 136 mean train loss:  2.41580224e+00, bound:  3.15197766e-01\n",
      "Epoch: 137 mean train loss:  2.41508794e+00, bound:  3.15213352e-01\n",
      "Epoch: 138 mean train loss:  2.41437173e+00, bound:  3.15228969e-01\n",
      "Epoch: 139 mean train loss:  2.41365647e+00, bound:  3.15244704e-01\n",
      "Epoch: 140 mean train loss:  2.41293907e+00, bound:  3.15260440e-01\n",
      "Epoch: 141 mean train loss:  2.41222095e+00, bound:  3.15276265e-01\n",
      "Epoch: 142 mean train loss:  2.41150188e+00, bound:  3.15292150e-01\n",
      "Epoch: 143 mean train loss:  2.41078234e+00, bound:  3.15308064e-01\n",
      "Epoch: 144 mean train loss:  2.41006160e+00, bound:  3.15324038e-01\n",
      "Epoch: 145 mean train loss:  2.40933967e+00, bound:  3.15340102e-01\n",
      "Epoch: 146 mean train loss:  2.40861630e+00, bound:  3.15356225e-01\n",
      "Epoch: 147 mean train loss:  2.40789247e+00, bound:  3.15372407e-01\n",
      "Epoch: 148 mean train loss:  2.40716720e+00, bound:  3.15388620e-01\n",
      "Epoch: 149 mean train loss:  2.40644073e+00, bound:  3.15404981e-01\n",
      "Epoch: 150 mean train loss:  2.40571260e+00, bound:  3.15421313e-01\n",
      "Epoch: 151 mean train loss:  2.40498400e+00, bound:  3.15437764e-01\n",
      "Epoch: 152 mean train loss:  2.40425324e+00, bound:  3.15454245e-01\n",
      "Epoch: 153 mean train loss:  2.40352178e+00, bound:  3.15470815e-01\n",
      "Epoch: 154 mean train loss:  2.40278912e+00, bound:  3.15487504e-01\n",
      "Epoch: 155 mean train loss:  2.40205479e+00, bound:  3.15504193e-01\n",
      "Epoch: 156 mean train loss:  2.40131783e+00, bound:  3.15520972e-01\n",
      "Epoch: 157 mean train loss:  2.40058112e+00, bound:  3.15537840e-01\n",
      "Epoch: 158 mean train loss:  2.39984179e+00, bound:  3.15554738e-01\n",
      "Epoch: 159 mean train loss:  2.39910102e+00, bound:  3.15571785e-01\n",
      "Epoch: 160 mean train loss:  2.39835906e+00, bound:  3.15588832e-01\n",
      "Epoch: 161 mean train loss:  2.39761424e+00, bound:  3.15605968e-01\n",
      "Epoch: 162 mean train loss:  2.39686918e+00, bound:  3.15623164e-01\n",
      "Epoch: 163 mean train loss:  2.39612150e+00, bound:  3.15640450e-01\n",
      "Epoch: 164 mean train loss:  2.39537263e+00, bound:  3.15657824e-01\n",
      "Epoch: 165 mean train loss:  2.39462209e+00, bound:  3.15675259e-01\n",
      "Epoch: 166 mean train loss:  2.39386940e+00, bound:  3.15692812e-01\n",
      "Epoch: 167 mean train loss:  2.39311457e+00, bound:  3.15710366e-01\n",
      "Epoch: 168 mean train loss:  2.39235830e+00, bound:  3.15728068e-01\n",
      "Epoch: 169 mean train loss:  2.39159966e+00, bound:  3.15745801e-01\n",
      "Epoch: 170 mean train loss:  2.39083886e+00, bound:  3.15763652e-01\n",
      "Epoch: 171 mean train loss:  2.39007664e+00, bound:  3.15781564e-01\n",
      "Epoch: 172 mean train loss:  2.38931203e+00, bound:  3.15799564e-01\n",
      "Epoch: 173 mean train loss:  2.38854575e+00, bound:  3.15817624e-01\n",
      "Epoch: 174 mean train loss:  2.38777709e+00, bound:  3.15835834e-01\n",
      "Epoch: 175 mean train loss:  2.38700676e+00, bound:  3.15854043e-01\n",
      "Epoch: 176 mean train loss:  2.38623357e+00, bound:  3.15872431e-01\n",
      "Epoch: 177 mean train loss:  2.38545871e+00, bound:  3.15890819e-01\n",
      "Epoch: 178 mean train loss:  2.38468146e+00, bound:  3.15909296e-01\n",
      "Epoch: 179 mean train loss:  2.38390160e+00, bound:  3.15927893e-01\n",
      "Epoch: 180 mean train loss:  2.38311958e+00, bound:  3.15946579e-01\n",
      "Epoch: 181 mean train loss:  2.38233542e+00, bound:  3.15965354e-01\n",
      "Epoch: 182 mean train loss:  2.38154817e+00, bound:  3.15984190e-01\n",
      "Epoch: 183 mean train loss:  2.38075995e+00, bound:  3.16003084e-01\n",
      "Epoch: 184 mean train loss:  2.37996769e+00, bound:  3.16022158e-01\n",
      "Epoch: 185 mean train loss:  2.37917423e+00, bound:  3.16041261e-01\n",
      "Epoch: 186 mean train loss:  2.37837791e+00, bound:  3.16060513e-01\n",
      "Epoch: 187 mean train loss:  2.37757826e+00, bound:  3.16079825e-01\n",
      "Epoch: 188 mean train loss:  2.37677670e+00, bound:  3.16099197e-01\n",
      "Epoch: 189 mean train loss:  2.37597322e+00, bound:  3.16118747e-01\n",
      "Epoch: 190 mean train loss:  2.37516594e+00, bound:  3.16138297e-01\n",
      "Epoch: 191 mean train loss:  2.37435651e+00, bound:  3.16157967e-01\n",
      "Epoch: 192 mean train loss:  2.37354422e+00, bound:  3.16177785e-01\n",
      "Epoch: 193 mean train loss:  2.37272906e+00, bound:  3.16197604e-01\n",
      "Epoch: 194 mean train loss:  2.37191176e+00, bound:  3.16217631e-01\n",
      "Epoch: 195 mean train loss:  2.37109065e+00, bound:  3.16237658e-01\n",
      "Epoch: 196 mean train loss:  2.37026715e+00, bound:  3.16257864e-01\n",
      "Epoch: 197 mean train loss:  2.36944056e+00, bound:  3.16278100e-01\n",
      "Epoch: 198 mean train loss:  2.36861157e+00, bound:  3.16298485e-01\n",
      "Epoch: 199 mean train loss:  2.36777949e+00, bound:  3.16318989e-01\n",
      "Epoch: 200 mean train loss:  2.36694431e+00, bound:  3.16339493e-01\n",
      "Epoch: 201 mean train loss:  2.36610579e+00, bound:  3.16360176e-01\n",
      "Epoch: 202 mean train loss:  2.36526442e+00, bound:  3.16380918e-01\n",
      "Epoch: 203 mean train loss:  2.36442089e+00, bound:  3.16401809e-01\n",
      "Epoch: 204 mean train loss:  2.36357355e+00, bound:  3.16422790e-01\n",
      "Epoch: 205 mean train loss:  2.36272240e+00, bound:  3.16443831e-01\n",
      "Epoch: 206 mean train loss:  2.36186790e+00, bound:  3.16465020e-01\n",
      "Epoch: 207 mean train loss:  2.36101151e+00, bound:  3.16486329e-01\n",
      "Epoch: 208 mean train loss:  2.36015129e+00, bound:  3.16507727e-01\n",
      "Epoch: 209 mean train loss:  2.35928798e+00, bound:  3.16529214e-01\n",
      "Epoch: 210 mean train loss:  2.35842037e+00, bound:  3.16550821e-01\n",
      "Epoch: 211 mean train loss:  2.35754991e+00, bound:  3.16572517e-01\n",
      "Epoch: 212 mean train loss:  2.35667562e+00, bound:  3.16594362e-01\n",
      "Epoch: 213 mean train loss:  2.35579896e+00, bound:  3.16616267e-01\n",
      "Epoch: 214 mean train loss:  2.35491776e+00, bound:  3.16638350e-01\n",
      "Epoch: 215 mean train loss:  2.35403275e+00, bound:  3.16660464e-01\n",
      "Epoch: 216 mean train loss:  2.35314512e+00, bound:  3.16682726e-01\n",
      "Epoch: 217 mean train loss:  2.35225344e+00, bound:  3.16705108e-01\n",
      "Epoch: 218 mean train loss:  2.35135889e+00, bound:  3.16727608e-01\n",
      "Epoch: 219 mean train loss:  2.35045958e+00, bound:  3.16750169e-01\n",
      "Epoch: 220 mean train loss:  2.34955716e+00, bound:  3.16772908e-01\n",
      "Epoch: 221 mean train loss:  2.34865069e+00, bound:  3.16795737e-01\n",
      "Epoch: 222 mean train loss:  2.34774065e+00, bound:  3.16818714e-01\n",
      "Epoch: 223 mean train loss:  2.34682631e+00, bound:  3.16841722e-01\n",
      "Epoch: 224 mean train loss:  2.34590864e+00, bound:  3.16864878e-01\n",
      "Epoch: 225 mean train loss:  2.34498668e+00, bound:  3.16888154e-01\n",
      "Epoch: 226 mean train loss:  2.34406114e+00, bound:  3.16911608e-01\n",
      "Epoch: 227 mean train loss:  2.34313154e+00, bound:  3.16935092e-01\n",
      "Epoch: 228 mean train loss:  2.34219694e+00, bound:  3.16958755e-01\n",
      "Epoch: 229 mean train loss:  2.34125996e+00, bound:  3.16982538e-01\n",
      "Epoch: 230 mean train loss:  2.34031725e+00, bound:  3.17006439e-01\n",
      "Epoch: 231 mean train loss:  2.33937073e+00, bound:  3.17030400e-01\n",
      "Epoch: 232 mean train loss:  2.33842063e+00, bound:  3.17054570e-01\n",
      "Epoch: 233 mean train loss:  2.33746624e+00, bound:  3.17078829e-01\n",
      "Epoch: 234 mean train loss:  2.33650684e+00, bound:  3.17103207e-01\n",
      "Epoch: 235 mean train loss:  2.33554339e+00, bound:  3.17127734e-01\n",
      "Epoch: 236 mean train loss:  2.33457565e+00, bound:  3.17152321e-01\n",
      "Epoch: 237 mean train loss:  2.33360362e+00, bound:  3.17177117e-01\n",
      "Epoch: 238 mean train loss:  2.33262658e+00, bound:  3.17201972e-01\n",
      "Epoch: 239 mean train loss:  2.33164597e+00, bound:  3.17226946e-01\n",
      "Epoch: 240 mean train loss:  2.33066010e+00, bound:  3.17252129e-01\n",
      "Epoch: 241 mean train loss:  2.32966971e+00, bound:  3.17277372e-01\n",
      "Epoch: 242 mean train loss:  2.32867479e+00, bound:  3.17302793e-01\n",
      "Epoch: 243 mean train loss:  2.32767534e+00, bound:  3.17328334e-01\n",
      "Epoch: 244 mean train loss:  2.32667112e+00, bound:  3.17354023e-01\n",
      "Epoch: 245 mean train loss:  2.32566166e+00, bound:  3.17379832e-01\n",
      "Epoch: 246 mean train loss:  2.32464790e+00, bound:  3.17405760e-01\n",
      "Epoch: 247 mean train loss:  2.32362914e+00, bound:  3.17431867e-01\n",
      "Epoch: 248 mean train loss:  2.32260513e+00, bound:  3.17458034e-01\n",
      "Epoch: 249 mean train loss:  2.32157731e+00, bound:  3.17484379e-01\n",
      "Epoch: 250 mean train loss:  2.32054353e+00, bound:  3.17510873e-01\n",
      "Epoch: 251 mean train loss:  2.31950474e+00, bound:  3.17537487e-01\n",
      "Epoch: 252 mean train loss:  2.31846142e+00, bound:  3.17564249e-01\n",
      "Epoch: 253 mean train loss:  2.31741261e+00, bound:  3.17591190e-01\n",
      "Epoch: 254 mean train loss:  2.31635809e+00, bound:  3.17618221e-01\n",
      "Epoch: 255 mean train loss:  2.31529927e+00, bound:  3.17645401e-01\n",
      "Epoch: 256 mean train loss:  2.31423521e+00, bound:  3.17672729e-01\n",
      "Epoch: 257 mean train loss:  2.31316471e+00, bound:  3.17700207e-01\n",
      "Epoch: 258 mean train loss:  2.31208968e+00, bound:  3.17727834e-01\n",
      "Epoch: 259 mean train loss:  2.31100941e+00, bound:  3.17755610e-01\n",
      "Epoch: 260 mean train loss:  2.30992317e+00, bound:  3.17783535e-01\n",
      "Epoch: 261 mean train loss:  2.30883145e+00, bound:  3.17811608e-01\n",
      "Epoch: 262 mean train loss:  2.30773449e+00, bound:  3.17839831e-01\n",
      "Epoch: 263 mean train loss:  2.30663228e+00, bound:  3.17868233e-01\n",
      "Epoch: 264 mean train loss:  2.30552435e+00, bound:  3.17896694e-01\n",
      "Epoch: 265 mean train loss:  2.30440974e+00, bound:  3.17925394e-01\n",
      "Epoch: 266 mean train loss:  2.30328989e+00, bound:  3.17954212e-01\n",
      "Epoch: 267 mean train loss:  2.30216479e+00, bound:  3.17983240e-01\n",
      "Epoch: 268 mean train loss:  2.30103302e+00, bound:  3.18012297e-01\n",
      "Epoch: 269 mean train loss:  2.29989600e+00, bound:  3.18041623e-01\n",
      "Epoch: 270 mean train loss:  2.29875302e+00, bound:  3.18071097e-01\n",
      "Epoch: 271 mean train loss:  2.29760408e+00, bound:  3.18100691e-01\n",
      "Epoch: 272 mean train loss:  2.29644918e+00, bound:  3.18130493e-01\n",
      "Epoch: 273 mean train loss:  2.29528761e+00, bound:  3.18160415e-01\n",
      "Epoch: 274 mean train loss:  2.29412031e+00, bound:  3.18190515e-01\n",
      "Epoch: 275 mean train loss:  2.29294682e+00, bound:  3.18220764e-01\n",
      "Epoch: 276 mean train loss:  2.29176712e+00, bound:  3.18251252e-01\n",
      "Epoch: 277 mean train loss:  2.29058123e+00, bound:  3.18281800e-01\n",
      "Epoch: 278 mean train loss:  2.28938866e+00, bound:  3.18312556e-01\n",
      "Epoch: 279 mean train loss:  2.28819036e+00, bound:  3.18343490e-01\n",
      "Epoch: 280 mean train loss:  2.28698540e+00, bound:  3.18374634e-01\n",
      "Epoch: 281 mean train loss:  2.28577375e+00, bound:  3.18405867e-01\n",
      "Epoch: 282 mean train loss:  2.28455591e+00, bound:  3.18437338e-01\n",
      "Epoch: 283 mean train loss:  2.28333163e+00, bound:  3.18468958e-01\n",
      "Epoch: 284 mean train loss:  2.28209996e+00, bound:  3.18500757e-01\n",
      "Epoch: 285 mean train loss:  2.28086185e+00, bound:  3.18532705e-01\n",
      "Epoch: 286 mean train loss:  2.27961731e+00, bound:  3.18564862e-01\n",
      "Epoch: 287 mean train loss:  2.27836657e+00, bound:  3.18597138e-01\n",
      "Epoch: 288 mean train loss:  2.27710819e+00, bound:  3.18629682e-01\n",
      "Epoch: 289 mean train loss:  2.27584362e+00, bound:  3.18662345e-01\n",
      "Epoch: 290 mean train loss:  2.27457094e+00, bound:  3.18695217e-01\n",
      "Epoch: 291 mean train loss:  2.27329183e+00, bound:  3.18728268e-01\n",
      "Epoch: 292 mean train loss:  2.27200651e+00, bound:  3.18761468e-01\n",
      "Epoch: 293 mean train loss:  2.27071333e+00, bound:  3.18794906e-01\n",
      "Epoch: 294 mean train loss:  2.26941347e+00, bound:  3.18828553e-01\n",
      "Epoch: 295 mean train loss:  2.26810622e+00, bound:  3.18862259e-01\n",
      "Epoch: 296 mean train loss:  2.26679230e+00, bound:  3.18896264e-01\n",
      "Epoch: 297 mean train loss:  2.26547027e+00, bound:  3.18930417e-01\n",
      "Epoch: 298 mean train loss:  2.26414108e+00, bound:  3.18964779e-01\n",
      "Epoch: 299 mean train loss:  2.26280427e+00, bound:  3.18999320e-01\n",
      "Epoch: 300 mean train loss:  2.26146173e+00, bound:  3.19034100e-01\n",
      "Epoch: 301 mean train loss:  2.26011038e+00, bound:  3.19068998e-01\n",
      "Epoch: 302 mean train loss:  2.25875187e+00, bound:  3.19104135e-01\n",
      "Epoch: 303 mean train loss:  2.25738573e+00, bound:  3.19139481e-01\n",
      "Epoch: 304 mean train loss:  2.25601172e+00, bound:  3.19175005e-01\n",
      "Epoch: 305 mean train loss:  2.25462985e+00, bound:  3.19210738e-01\n",
      "Epoch: 306 mean train loss:  2.25324082e+00, bound:  3.19246680e-01\n",
      "Epoch: 307 mean train loss:  2.25184393e+00, bound:  3.19282800e-01\n",
      "Epoch: 308 mean train loss:  2.25043941e+00, bound:  3.19319159e-01\n",
      "Epoch: 309 mean train loss:  2.24902678e+00, bound:  3.19355667e-01\n",
      "Epoch: 310 mean train loss:  2.24760580e+00, bound:  3.19392413e-01\n",
      "Epoch: 311 mean train loss:  2.24617767e+00, bound:  3.19429398e-01\n",
      "Epoch: 312 mean train loss:  2.24474072e+00, bound:  3.19466531e-01\n",
      "Epoch: 313 mean train loss:  2.24329686e+00, bound:  3.19503903e-01\n",
      "Epoch: 314 mean train loss:  2.24184370e+00, bound:  3.19541484e-01\n",
      "Epoch: 315 mean train loss:  2.24038363e+00, bound:  3.19579303e-01\n",
      "Epoch: 316 mean train loss:  2.23891377e+00, bound:  3.19617301e-01\n",
      "Epoch: 317 mean train loss:  2.23743701e+00, bound:  3.19655538e-01\n",
      "Epoch: 318 mean train loss:  2.23595166e+00, bound:  3.19693953e-01\n",
      "Epoch: 319 mean train loss:  2.23445702e+00, bound:  3.19732606e-01\n",
      "Epoch: 320 mean train loss:  2.23295546e+00, bound:  3.19771469e-01\n",
      "Epoch: 321 mean train loss:  2.23144484e+00, bound:  3.19810599e-01\n",
      "Epoch: 322 mean train loss:  2.22992563e+00, bound:  3.19849908e-01\n",
      "Epoch: 323 mean train loss:  2.22839761e+00, bound:  3.19889426e-01\n",
      "Epoch: 324 mean train loss:  2.22686148e+00, bound:  3.19929242e-01\n",
      "Epoch: 325 mean train loss:  2.22531676e+00, bound:  3.19969177e-01\n",
      "Epoch: 326 mean train loss:  2.22376299e+00, bound:  3.20009470e-01\n",
      "Epoch: 327 mean train loss:  2.22220063e+00, bound:  3.20049882e-01\n",
      "Epoch: 328 mean train loss:  2.22062945e+00, bound:  3.20090562e-01\n",
      "Epoch: 329 mean train loss:  2.21904969e+00, bound:  3.20131451e-01\n",
      "Epoch: 330 mean train loss:  2.21746039e+00, bound:  3.20172638e-01\n",
      "Epoch: 331 mean train loss:  2.21586275e+00, bound:  3.20213974e-01\n",
      "Epoch: 332 mean train loss:  2.21425557e+00, bound:  3.20255488e-01\n",
      "Epoch: 333 mean train loss:  2.21263957e+00, bound:  3.20297420e-01\n",
      "Epoch: 334 mean train loss:  2.21101546e+00, bound:  3.20339501e-01\n",
      "Epoch: 335 mean train loss:  2.20938063e+00, bound:  3.20381790e-01\n",
      "Epoch: 336 mean train loss:  2.20773792e+00, bound:  3.20424348e-01\n",
      "Epoch: 337 mean train loss:  2.20608521e+00, bound:  3.20467114e-01\n",
      "Epoch: 338 mean train loss:  2.20442319e+00, bound:  3.20510119e-01\n",
      "Epoch: 339 mean train loss:  2.20275211e+00, bound:  3.20553422e-01\n",
      "Epoch: 340 mean train loss:  2.20107222e+00, bound:  3.20596963e-01\n",
      "Epoch: 341 mean train loss:  2.19938231e+00, bound:  3.20640773e-01\n",
      "Epoch: 342 mean train loss:  2.19768310e+00, bound:  3.20684761e-01\n",
      "Epoch: 343 mean train loss:  2.19597387e+00, bound:  3.20729047e-01\n",
      "Epoch: 344 mean train loss:  2.19425559e+00, bound:  3.20773542e-01\n",
      "Epoch: 345 mean train loss:  2.19252801e+00, bound:  3.20818335e-01\n",
      "Epoch: 346 mean train loss:  2.19078994e+00, bound:  3.20863336e-01\n",
      "Epoch: 347 mean train loss:  2.18904328e+00, bound:  3.20908636e-01\n",
      "Epoch: 348 mean train loss:  2.18728590e+00, bound:  3.20954144e-01\n",
      "Epoch: 349 mean train loss:  2.18551898e+00, bound:  3.20999950e-01\n",
      "Epoch: 350 mean train loss:  2.18374228e+00, bound:  3.21046025e-01\n",
      "Epoch: 351 mean train loss:  2.18195534e+00, bound:  3.21092337e-01\n",
      "Epoch: 352 mean train loss:  2.18015909e+00, bound:  3.21138918e-01\n",
      "Epoch: 353 mean train loss:  2.17835259e+00, bound:  3.21185708e-01\n",
      "Epoch: 354 mean train loss:  2.17653561e+00, bound:  3.21232826e-01\n",
      "Epoch: 355 mean train loss:  2.17470884e+00, bound:  3.21280211e-01\n",
      "Epoch: 356 mean train loss:  2.17287254e+00, bound:  3.21327865e-01\n",
      "Epoch: 357 mean train loss:  2.17102528e+00, bound:  3.21375757e-01\n",
      "Epoch: 358 mean train loss:  2.16916871e+00, bound:  3.21423918e-01\n",
      "Epoch: 359 mean train loss:  2.16730046e+00, bound:  3.21472377e-01\n",
      "Epoch: 360 mean train loss:  2.16542315e+00, bound:  3.21521074e-01\n",
      "Epoch: 361 mean train loss:  2.16353464e+00, bound:  3.21570098e-01\n",
      "Epoch: 362 mean train loss:  2.16163611e+00, bound:  3.21619391e-01\n",
      "Epoch: 363 mean train loss:  2.15972638e+00, bound:  3.21668923e-01\n",
      "Epoch: 364 mean train loss:  2.15780735e+00, bound:  3.21718752e-01\n",
      "Epoch: 365 mean train loss:  2.15587664e+00, bound:  3.21768880e-01\n",
      "Epoch: 366 mean train loss:  2.15393615e+00, bound:  3.21819276e-01\n",
      "Epoch: 367 mean train loss:  2.15198445e+00, bound:  3.21869940e-01\n",
      "Epoch: 368 mean train loss:  2.15002251e+00, bound:  3.21920902e-01\n",
      "Epoch: 369 mean train loss:  2.14804912e+00, bound:  3.21972162e-01\n",
      "Epoch: 370 mean train loss:  2.14606524e+00, bound:  3.22023720e-01\n",
      "Epoch: 371 mean train loss:  2.14407086e+00, bound:  3.22075546e-01\n",
      "Epoch: 372 mean train loss:  2.14206529e+00, bound:  3.22127640e-01\n",
      "Epoch: 373 mean train loss:  2.14004850e+00, bound:  3.22180063e-01\n",
      "Epoch: 374 mean train loss:  2.13802171e+00, bound:  3.22232813e-01\n",
      "Epoch: 375 mean train loss:  2.13598251e+00, bound:  3.22285831e-01\n",
      "Epoch: 376 mean train loss:  2.13393259e+00, bound:  3.22339147e-01\n",
      "Epoch: 377 mean train loss:  2.13187218e+00, bound:  3.22392732e-01\n",
      "Epoch: 378 mean train loss:  2.12979984e+00, bound:  3.22446704e-01\n",
      "Epoch: 379 mean train loss:  2.12771654e+00, bound:  3.22500885e-01\n",
      "Epoch: 380 mean train loss:  2.12562156e+00, bound:  3.22555423e-01\n",
      "Epoch: 381 mean train loss:  2.12351561e+00, bound:  3.22610289e-01\n",
      "Epoch: 382 mean train loss:  2.12139821e+00, bound:  3.22665423e-01\n",
      "Epoch: 383 mean train loss:  2.11926937e+00, bound:  3.22720915e-01\n",
      "Epoch: 384 mean train loss:  2.11712933e+00, bound:  3.22776645e-01\n",
      "Epoch: 385 mean train loss:  2.11497712e+00, bound:  3.22832733e-01\n",
      "Epoch: 386 mean train loss:  2.11281395e+00, bound:  3.22889179e-01\n",
      "Epoch: 387 mean train loss:  2.11063862e+00, bound:  3.22945863e-01\n",
      "Epoch: 388 mean train loss:  2.10845160e+00, bound:  3.23002934e-01\n",
      "Epoch: 389 mean train loss:  2.10625315e+00, bound:  3.23060304e-01\n",
      "Epoch: 390 mean train loss:  2.10404277e+00, bound:  3.23117971e-01\n",
      "Epoch: 391 mean train loss:  2.10181999e+00, bound:  3.23175967e-01\n",
      "Epoch: 392 mean train loss:  2.09958553e+00, bound:  3.23234320e-01\n",
      "Epoch: 393 mean train loss:  2.09733939e+00, bound:  3.23292971e-01\n",
      "Epoch: 394 mean train loss:  2.09508085e+00, bound:  3.23352009e-01\n",
      "Epoch: 395 mean train loss:  2.09281087e+00, bound:  3.23411345e-01\n",
      "Epoch: 396 mean train loss:  2.09052873e+00, bound:  3.23471040e-01\n",
      "Epoch: 397 mean train loss:  2.08823371e+00, bound:  3.23531032e-01\n",
      "Epoch: 398 mean train loss:  2.08592725e+00, bound:  3.23591381e-01\n",
      "Epoch: 399 mean train loss:  2.08360767e+00, bound:  3.23652059e-01\n",
      "Epoch: 400 mean train loss:  2.08127642e+00, bound:  3.23713064e-01\n",
      "Epoch: 401 mean train loss:  2.07893300e+00, bound:  3.23774427e-01\n",
      "Epoch: 402 mean train loss:  2.07657671e+00, bound:  3.23836148e-01\n",
      "Epoch: 403 mean train loss:  2.07420754e+00, bound:  3.23898256e-01\n",
      "Epoch: 404 mean train loss:  2.07182622e+00, bound:  3.23960632e-01\n",
      "Epoch: 405 mean train loss:  2.06943250e+00, bound:  3.24023396e-01\n",
      "Epoch: 406 mean train loss:  2.06702614e+00, bound:  3.24086517e-01\n",
      "Epoch: 407 mean train loss:  2.06460667e+00, bound:  3.24149966e-01\n",
      "Epoch: 408 mean train loss:  2.06217456e+00, bound:  3.24213773e-01\n",
      "Epoch: 409 mean train loss:  2.05973005e+00, bound:  3.24277997e-01\n",
      "Epoch: 410 mean train loss:  2.05727291e+00, bound:  3.24342519e-01\n",
      "Epoch: 411 mean train loss:  2.05480194e+00, bound:  3.24407429e-01\n",
      "Epoch: 412 mean train loss:  2.05231857e+00, bound:  3.24472666e-01\n",
      "Epoch: 413 mean train loss:  2.04982233e+00, bound:  3.24538320e-01\n",
      "Epoch: 414 mean train loss:  2.04731297e+00, bound:  3.24604273e-01\n",
      "Epoch: 415 mean train loss:  2.04479003e+00, bound:  3.24670643e-01\n",
      "Epoch: 416 mean train loss:  2.04225445e+00, bound:  3.24737340e-01\n",
      "Epoch: 417 mean train loss:  2.03970551e+00, bound:  3.24804455e-01\n",
      "Epoch: 418 mean train loss:  2.03714371e+00, bound:  3.24871927e-01\n",
      "Epoch: 419 mean train loss:  2.03456759e+00, bound:  3.24939758e-01\n",
      "Epoch: 420 mean train loss:  2.03197908e+00, bound:  3.25007975e-01\n",
      "Epoch: 421 mean train loss:  2.02937698e+00, bound:  3.25076520e-01\n",
      "Epoch: 422 mean train loss:  2.02676129e+00, bound:  3.25145543e-01\n",
      "Epoch: 423 mean train loss:  2.02413177e+00, bound:  3.25214922e-01\n",
      "Epoch: 424 mean train loss:  2.02148890e+00, bound:  3.25284570e-01\n",
      "Epoch: 425 mean train loss:  2.01883245e+00, bound:  3.25354725e-01\n",
      "Epoch: 426 mean train loss:  2.01616240e+00, bound:  3.25425178e-01\n",
      "Epoch: 427 mean train loss:  2.01347899e+00, bound:  3.25496078e-01\n",
      "Epoch: 428 mean train loss:  2.01078153e+00, bound:  3.25567305e-01\n",
      "Epoch: 429 mean train loss:  2.00806999e+00, bound:  3.25638980e-01\n",
      "Epoch: 430 mean train loss:  2.00534487e+00, bound:  3.25710982e-01\n",
      "Epoch: 431 mean train loss:  2.00260615e+00, bound:  3.25783402e-01\n",
      "Epoch: 432 mean train loss:  1.99985278e+00, bound:  3.25856209e-01\n",
      "Epoch: 433 mean train loss:  1.99708593e+00, bound:  3.25929433e-01\n",
      "Epoch: 434 mean train loss:  1.99430454e+00, bound:  3.26003045e-01\n",
      "Epoch: 435 mean train loss:  1.99151027e+00, bound:  3.26077044e-01\n",
      "Epoch: 436 mean train loss:  1.98870111e+00, bound:  3.26151341e-01\n",
      "Epoch: 437 mean train loss:  1.98587823e+00, bound:  3.26226205e-01\n",
      "Epoch: 438 mean train loss:  1.98304045e+00, bound:  3.26301336e-01\n",
      "Epoch: 439 mean train loss:  1.98018897e+00, bound:  3.26376915e-01\n",
      "Epoch: 440 mean train loss:  1.97732329e+00, bound:  3.26452881e-01\n",
      "Epoch: 441 mean train loss:  1.97444332e+00, bound:  3.26529264e-01\n",
      "Epoch: 442 mean train loss:  1.97154891e+00, bound:  3.26606005e-01\n",
      "Epoch: 443 mean train loss:  1.96864033e+00, bound:  3.26683223e-01\n",
      "Epoch: 444 mean train loss:  1.96571779e+00, bound:  3.26760769e-01\n",
      "Epoch: 445 mean train loss:  1.96277940e+00, bound:  3.26838702e-01\n",
      "Epoch: 446 mean train loss:  1.95982814e+00, bound:  3.26917082e-01\n",
      "Epoch: 447 mean train loss:  1.95686197e+00, bound:  3.26995879e-01\n",
      "Epoch: 448 mean train loss:  1.95388091e+00, bound:  3.27075064e-01\n",
      "Epoch: 449 mean train loss:  1.95088577e+00, bound:  3.27154607e-01\n",
      "Epoch: 450 mean train loss:  1.94787621e+00, bound:  3.27234596e-01\n",
      "Epoch: 451 mean train loss:  1.94485211e+00, bound:  3.27315003e-01\n",
      "Epoch: 452 mean train loss:  1.94181299e+00, bound:  3.27395797e-01\n",
      "Epoch: 453 mean train loss:  1.93876004e+00, bound:  3.27477038e-01\n",
      "Epoch: 454 mean train loss:  1.93569207e+00, bound:  3.27558607e-01\n",
      "Epoch: 455 mean train loss:  1.93260980e+00, bound:  3.27640623e-01\n",
      "Epoch: 456 mean train loss:  1.92951274e+00, bound:  3.27723026e-01\n",
      "Epoch: 457 mean train loss:  1.92640090e+00, bound:  3.27805877e-01\n",
      "Epoch: 458 mean train loss:  1.92327476e+00, bound:  3.27889115e-01\n",
      "Epoch: 459 mean train loss:  1.92013407e+00, bound:  3.27972740e-01\n",
      "Epoch: 460 mean train loss:  1.91697907e+00, bound:  3.28056723e-01\n",
      "Epoch: 461 mean train loss:  1.91380918e+00, bound:  3.28141212e-01\n",
      "Epoch: 462 mean train loss:  1.91062415e+00, bound:  3.28226030e-01\n",
      "Epoch: 463 mean train loss:  1.90742540e+00, bound:  3.28311324e-01\n",
      "Epoch: 464 mean train loss:  1.90421224e+00, bound:  3.28397006e-01\n",
      "Epoch: 465 mean train loss:  1.90098476e+00, bound:  3.28483045e-01\n",
      "Epoch: 466 mean train loss:  1.89774168e+00, bound:  3.28569531e-01\n",
      "Epoch: 467 mean train loss:  1.89448559e+00, bound:  3.28656405e-01\n",
      "Epoch: 468 mean train loss:  1.89121389e+00, bound:  3.28743696e-01\n",
      "Epoch: 469 mean train loss:  1.88792896e+00, bound:  3.28831375e-01\n",
      "Epoch: 470 mean train loss:  1.88462913e+00, bound:  3.28919470e-01\n",
      "Epoch: 471 mean train loss:  1.88131523e+00, bound:  3.29007983e-01\n",
      "Epoch: 472 mean train loss:  1.87798703e+00, bound:  3.29096824e-01\n",
      "Epoch: 473 mean train loss:  1.87464392e+00, bound:  3.29186112e-01\n",
      "Epoch: 474 mean train loss:  1.87128711e+00, bound:  3.29275817e-01\n",
      "Epoch: 475 mean train loss:  1.86791706e+00, bound:  3.29365909e-01\n",
      "Epoch: 476 mean train loss:  1.86453259e+00, bound:  3.29456419e-01\n",
      "Epoch: 477 mean train loss:  1.86113334e+00, bound:  3.29547256e-01\n",
      "Epoch: 478 mean train loss:  1.85772145e+00, bound:  3.29638541e-01\n",
      "Epoch: 479 mean train loss:  1.85429561e+00, bound:  3.29730213e-01\n",
      "Epoch: 480 mean train loss:  1.85085595e+00, bound:  3.29822302e-01\n",
      "Epoch: 481 mean train loss:  1.84740222e+00, bound:  3.29914778e-01\n",
      "Epoch: 482 mean train loss:  1.84393537e+00, bound:  3.30007643e-01\n",
      "Epoch: 483 mean train loss:  1.84045577e+00, bound:  3.30100864e-01\n",
      "Epoch: 484 mean train loss:  1.83696187e+00, bound:  3.30194473e-01\n",
      "Epoch: 485 mean train loss:  1.83345520e+00, bound:  3.30288500e-01\n",
      "Epoch: 486 mean train loss:  1.82993543e+00, bound:  3.30382884e-01\n",
      "Epoch: 487 mean train loss:  1.82640314e+00, bound:  3.30477655e-01\n",
      "Epoch: 488 mean train loss:  1.82285774e+00, bound:  3.30572844e-01\n",
      "Epoch: 489 mean train loss:  1.81929934e+00, bound:  3.30668360e-01\n",
      "Epoch: 490 mean train loss:  1.81572926e+00, bound:  3.30764323e-01\n",
      "Epoch: 491 mean train loss:  1.81214643e+00, bound:  3.30860615e-01\n",
      "Epoch: 492 mean train loss:  1.80855036e+00, bound:  3.30957234e-01\n",
      "Epoch: 493 mean train loss:  1.80494392e+00, bound:  3.31054300e-01\n",
      "Epoch: 494 mean train loss:  1.80132461e+00, bound:  3.31151724e-01\n",
      "Epoch: 495 mean train loss:  1.79769397e+00, bound:  3.31249475e-01\n",
      "Epoch: 496 mean train loss:  1.79405129e+00, bound:  3.31347615e-01\n",
      "Epoch: 497 mean train loss:  1.79039824e+00, bound:  3.31446141e-01\n",
      "Epoch: 498 mean train loss:  1.78673291e+00, bound:  3.31544936e-01\n",
      "Epoch: 499 mean train loss:  1.78305745e+00, bound:  3.31644177e-01\n",
      "Epoch: 500 mean train loss:  1.77937102e+00, bound:  3.31743717e-01\n",
      "Epoch: 501 mean train loss:  1.77567387e+00, bound:  3.31843644e-01\n",
      "Epoch: 502 mean train loss:  1.77196705e+00, bound:  3.31943929e-01\n",
      "Epoch: 503 mean train loss:  1.76824903e+00, bound:  3.32044572e-01\n",
      "Epoch: 504 mean train loss:  1.76452160e+00, bound:  3.32145482e-01\n",
      "Epoch: 505 mean train loss:  1.76078451e+00, bound:  3.32246810e-01\n",
      "Epoch: 506 mean train loss:  1.75703859e+00, bound:  3.32348406e-01\n",
      "Epoch: 507 mean train loss:  1.75328279e+00, bound:  3.32450330e-01\n",
      "Epoch: 508 mean train loss:  1.74951839e+00, bound:  3.32552671e-01\n",
      "Epoch: 509 mean train loss:  1.74574590e+00, bound:  3.32655311e-01\n",
      "Epoch: 510 mean train loss:  1.74196434e+00, bound:  3.32758218e-01\n",
      "Epoch: 511 mean train loss:  1.73817492e+00, bound:  3.32861453e-01\n",
      "Epoch: 512 mean train loss:  1.73437810e+00, bound:  3.32965016e-01\n",
      "Epoch: 513 mean train loss:  1.73057330e+00, bound:  3.33068907e-01\n",
      "Epoch: 514 mean train loss:  1.72676158e+00, bound:  3.33173096e-01\n",
      "Epoch: 515 mean train loss:  1.72294259e+00, bound:  3.33277553e-01\n",
      "Epoch: 516 mean train loss:  1.71911728e+00, bound:  3.33382368e-01\n",
      "Epoch: 517 mean train loss:  1.71528649e+00, bound:  3.33487451e-01\n",
      "Epoch: 518 mean train loss:  1.71144938e+00, bound:  3.33592832e-01\n",
      "Epoch: 519 mean train loss:  1.70760608e+00, bound:  3.33698481e-01\n",
      "Epoch: 520 mean train loss:  1.70375788e+00, bound:  3.33804399e-01\n",
      "Epoch: 521 mean train loss:  1.69990528e+00, bound:  3.33910644e-01\n",
      "Epoch: 522 mean train loss:  1.69604790e+00, bound:  3.34017128e-01\n",
      "Epoch: 523 mean train loss:  1.69218647e+00, bound:  3.34123820e-01\n",
      "Epoch: 524 mean train loss:  1.68832076e+00, bound:  3.34230870e-01\n",
      "Epoch: 525 mean train loss:  1.68445206e+00, bound:  3.34338188e-01\n",
      "Epoch: 526 mean train loss:  1.68058026e+00, bound:  3.34445685e-01\n",
      "Epoch: 527 mean train loss:  1.67670596e+00, bound:  3.34553510e-01\n",
      "Epoch: 528 mean train loss:  1.67282927e+00, bound:  3.34661484e-01\n",
      "Epoch: 529 mean train loss:  1.66895080e+00, bound:  3.34769756e-01\n",
      "Epoch: 530 mean train loss:  1.66507089e+00, bound:  3.34878236e-01\n",
      "Epoch: 531 mean train loss:  1.66118979e+00, bound:  3.34986925e-01\n",
      "Epoch: 532 mean train loss:  1.65730810e+00, bound:  3.35095882e-01\n",
      "Epoch: 533 mean train loss:  1.65342653e+00, bound:  3.35205019e-01\n",
      "Epoch: 534 mean train loss:  1.64954472e+00, bound:  3.35314393e-01\n",
      "Epoch: 535 mean train loss:  1.64566338e+00, bound:  3.35423946e-01\n",
      "Epoch: 536 mean train loss:  1.64178360e+00, bound:  3.35533708e-01\n",
      "Epoch: 537 mean train loss:  1.63790512e+00, bound:  3.35643619e-01\n",
      "Epoch: 538 mean train loss:  1.63402843e+00, bound:  3.35753769e-01\n",
      "Epoch: 539 mean train loss:  1.63015413e+00, bound:  3.35864067e-01\n",
      "Epoch: 540 mean train loss:  1.62628245e+00, bound:  3.35974514e-01\n",
      "Epoch: 541 mean train loss:  1.62241423e+00, bound:  3.36085141e-01\n",
      "Epoch: 542 mean train loss:  1.61854994e+00, bound:  3.36195946e-01\n",
      "Epoch: 543 mean train loss:  1.61468935e+00, bound:  3.36306840e-01\n",
      "Epoch: 544 mean train loss:  1.61083400e+00, bound:  3.36417943e-01\n",
      "Epoch: 545 mean train loss:  1.60698295e+00, bound:  3.36529166e-01\n",
      "Epoch: 546 mean train loss:  1.60313773e+00, bound:  3.36640507e-01\n",
      "Epoch: 547 mean train loss:  1.59929872e+00, bound:  3.36751878e-01\n",
      "Epoch: 548 mean train loss:  1.59546602e+00, bound:  3.36863488e-01\n",
      "Epoch: 549 mean train loss:  1.59163988e+00, bound:  3.36975127e-01\n",
      "Epoch: 550 mean train loss:  1.58782113e+00, bound:  3.37086886e-01\n",
      "Epoch: 551 mean train loss:  1.58401060e+00, bound:  3.37198734e-01\n",
      "Epoch: 552 mean train loss:  1.58020830e+00, bound:  3.37310582e-01\n",
      "Epoch: 553 mean train loss:  1.57641399e+00, bound:  3.37422520e-01\n",
      "Epoch: 554 mean train loss:  1.57262933e+00, bound:  3.37534577e-01\n",
      "Epoch: 555 mean train loss:  1.56885386e+00, bound:  3.37646633e-01\n",
      "Epoch: 556 mean train loss:  1.56508815e+00, bound:  3.37758720e-01\n",
      "Epoch: 557 mean train loss:  1.56133378e+00, bound:  3.37870836e-01\n",
      "Epoch: 558 mean train loss:  1.55759037e+00, bound:  3.37983042e-01\n",
      "Epoch: 559 mean train loss:  1.55385745e+00, bound:  3.38095218e-01\n",
      "Epoch: 560 mean train loss:  1.55013645e+00, bound:  3.38207334e-01\n",
      "Epoch: 561 mean train loss:  1.54642820e+00, bound:  3.38319421e-01\n",
      "Epoch: 562 mean train loss:  1.54273176e+00, bound:  3.38431567e-01\n",
      "Epoch: 563 mean train loss:  1.53904915e+00, bound:  3.38543683e-01\n",
      "Epoch: 564 mean train loss:  1.53537941e+00, bound:  3.38655680e-01\n",
      "Epoch: 565 mean train loss:  1.53172374e+00, bound:  3.38767648e-01\n",
      "Epoch: 566 mean train loss:  1.52808225e+00, bound:  3.38879615e-01\n",
      "Epoch: 567 mean train loss:  1.52445483e+00, bound:  3.38991374e-01\n",
      "Epoch: 568 mean train loss:  1.52084339e+00, bound:  3.39103192e-01\n",
      "Epoch: 569 mean train loss:  1.51724637e+00, bound:  3.39214802e-01\n",
      "Epoch: 570 mean train loss:  1.51366556e+00, bound:  3.39326322e-01\n",
      "Epoch: 571 mean train loss:  1.51010120e+00, bound:  3.39437753e-01\n",
      "Epoch: 572 mean train loss:  1.50655198e+00, bound:  3.39549065e-01\n",
      "Epoch: 573 mean train loss:  1.50302076e+00, bound:  3.39660138e-01\n",
      "Epoch: 574 mean train loss:  1.49950635e+00, bound:  3.39771092e-01\n",
      "Epoch: 575 mean train loss:  1.49600899e+00, bound:  3.39881927e-01\n",
      "Epoch: 576 mean train loss:  1.49252963e+00, bound:  3.39992583e-01\n",
      "Epoch: 577 mean train loss:  1.48906827e+00, bound:  3.40103000e-01\n",
      "Epoch: 578 mean train loss:  1.48562443e+00, bound:  3.40213209e-01\n",
      "Epoch: 579 mean train loss:  1.48220015e+00, bound:  3.40323269e-01\n",
      "Epoch: 580 mean train loss:  1.47879434e+00, bound:  3.40433031e-01\n",
      "Epoch: 581 mean train loss:  1.47540796e+00, bound:  3.40542585e-01\n",
      "Epoch: 582 mean train loss:  1.47204030e+00, bound:  3.40651870e-01\n",
      "Epoch: 583 mean train loss:  1.46869218e+00, bound:  3.40760916e-01\n",
      "Epoch: 584 mean train loss:  1.46536434e+00, bound:  3.40869695e-01\n",
      "Epoch: 585 mean train loss:  1.46205640e+00, bound:  3.40978146e-01\n",
      "Epoch: 586 mean train loss:  1.45876825e+00, bound:  3.41086388e-01\n",
      "Epoch: 587 mean train loss:  1.45550072e+00, bound:  3.41194242e-01\n",
      "Epoch: 588 mean train loss:  1.45225310e+00, bound:  3.41301799e-01\n",
      "Epoch: 589 mean train loss:  1.44902706e+00, bound:  3.41408998e-01\n",
      "Epoch: 590 mean train loss:  1.44582105e+00, bound:  3.41515899e-01\n",
      "Epoch: 591 mean train loss:  1.44263625e+00, bound:  3.41622442e-01\n",
      "Epoch: 592 mean train loss:  1.43947268e+00, bound:  3.41728598e-01\n",
      "Epoch: 593 mean train loss:  1.43632984e+00, bound:  3.41834426e-01\n",
      "Epoch: 594 mean train loss:  1.43320835e+00, bound:  3.41939867e-01\n",
      "Epoch: 595 mean train loss:  1.43010831e+00, bound:  3.42044890e-01\n",
      "Epoch: 596 mean train loss:  1.42702961e+00, bound:  3.42149526e-01\n",
      "Epoch: 597 mean train loss:  1.42397213e+00, bound:  3.42253774e-01\n",
      "Epoch: 598 mean train loss:  1.42093635e+00, bound:  3.42357546e-01\n",
      "Epoch: 599 mean train loss:  1.41792190e+00, bound:  3.42460960e-01\n",
      "Epoch: 600 mean train loss:  1.41492891e+00, bound:  3.42563838e-01\n",
      "Epoch: 601 mean train loss:  1.41195786e+00, bound:  3.42666328e-01\n",
      "Epoch: 602 mean train loss:  1.40900815e+00, bound:  3.42768371e-01\n",
      "Epoch: 603 mean train loss:  1.40607953e+00, bound:  3.42869967e-01\n",
      "Epoch: 604 mean train loss:  1.40317261e+00, bound:  3.42971087e-01\n",
      "Epoch: 605 mean train loss:  1.40028703e+00, bound:  3.43071669e-01\n",
      "Epoch: 606 mean train loss:  1.39742339e+00, bound:  3.43171835e-01\n",
      "Epoch: 607 mean train loss:  1.39458060e+00, bound:  3.43271464e-01\n",
      "Epoch: 608 mean train loss:  1.39175904e+00, bound:  3.43370616e-01\n",
      "Epoch: 609 mean train loss:  1.38895857e+00, bound:  3.43469232e-01\n",
      "Epoch: 610 mean train loss:  1.38617945e+00, bound:  3.43567371e-01\n",
      "Epoch: 611 mean train loss:  1.38342106e+00, bound:  3.43665004e-01\n",
      "Epoch: 612 mean train loss:  1.38068354e+00, bound:  3.43762070e-01\n",
      "Epoch: 613 mean train loss:  1.37796676e+00, bound:  3.43858659e-01\n",
      "Epoch: 614 mean train loss:  1.37527061e+00, bound:  3.43954742e-01\n",
      "Epoch: 615 mean train loss:  1.37259495e+00, bound:  3.44050229e-01\n",
      "Epoch: 616 mean train loss:  1.36994004e+00, bound:  3.44145238e-01\n",
      "Epoch: 617 mean train loss:  1.36730480e+00, bound:  3.44239652e-01\n",
      "Epoch: 618 mean train loss:  1.36468923e+00, bound:  3.44333559e-01\n",
      "Epoch: 619 mean train loss:  1.36209452e+00, bound:  3.44426900e-01\n",
      "Epoch: 620 mean train loss:  1.35951912e+00, bound:  3.44519794e-01\n",
      "Epoch: 621 mean train loss:  1.35696375e+00, bound:  3.44612002e-01\n",
      "Epoch: 622 mean train loss:  1.35442674e+00, bound:  3.44703704e-01\n",
      "Epoch: 623 mean train loss:  1.35191000e+00, bound:  3.44794869e-01\n",
      "Epoch: 624 mean train loss:  1.34941161e+00, bound:  3.44885528e-01\n",
      "Epoch: 625 mean train loss:  1.34693217e+00, bound:  3.44975591e-01\n",
      "Epoch: 626 mean train loss:  1.34447145e+00, bound:  3.45065087e-01\n",
      "Epoch: 627 mean train loss:  1.34202886e+00, bound:  3.45154047e-01\n",
      "Epoch: 628 mean train loss:  1.33960474e+00, bound:  3.45242530e-01\n",
      "Epoch: 629 mean train loss:  1.33719897e+00, bound:  3.45330358e-01\n",
      "Epoch: 630 mean train loss:  1.33481026e+00, bound:  3.45417678e-01\n",
      "Epoch: 631 mean train loss:  1.33243942e+00, bound:  3.45504433e-01\n",
      "Epoch: 632 mean train loss:  1.33008635e+00, bound:  3.45590770e-01\n",
      "Epoch: 633 mean train loss:  1.32774997e+00, bound:  3.45676452e-01\n",
      "Epoch: 634 mean train loss:  1.32543051e+00, bound:  3.45761627e-01\n",
      "Epoch: 635 mean train loss:  1.32312846e+00, bound:  3.45846325e-01\n",
      "Epoch: 636 mean train loss:  1.32084262e+00, bound:  3.45930457e-01\n",
      "Epoch: 637 mean train loss:  1.31857276e+00, bound:  3.46013963e-01\n",
      "Epoch: 638 mean train loss:  1.31631911e+00, bound:  3.46097082e-01\n",
      "Epoch: 639 mean train loss:  1.31408143e+00, bound:  3.46179664e-01\n",
      "Epoch: 640 mean train loss:  1.31185949e+00, bound:  3.46261710e-01\n",
      "Epoch: 641 mean train loss:  1.30965304e+00, bound:  3.46343219e-01\n",
      "Epoch: 642 mean train loss:  1.30746198e+00, bound:  3.46424311e-01\n",
      "Epoch: 643 mean train loss:  1.30528557e+00, bound:  3.46504837e-01\n",
      "Epoch: 644 mean train loss:  1.30312407e+00, bound:  3.46584886e-01\n",
      "Epoch: 645 mean train loss:  1.30097675e+00, bound:  3.46664488e-01\n",
      "Epoch: 646 mean train loss:  1.29884386e+00, bound:  3.46743613e-01\n",
      "Epoch: 647 mean train loss:  1.29672563e+00, bound:  3.46822232e-01\n",
      "Epoch: 648 mean train loss:  1.29462123e+00, bound:  3.46900344e-01\n",
      "Epoch: 649 mean train loss:  1.29253042e+00, bound:  3.46978039e-01\n",
      "Epoch: 650 mean train loss:  1.29045296e+00, bound:  3.47055316e-01\n",
      "Epoch: 651 mean train loss:  1.28838837e+00, bound:  3.47132057e-01\n",
      "Epoch: 652 mean train loss:  1.28633773e+00, bound:  3.47208411e-01\n",
      "Epoch: 653 mean train loss:  1.28429961e+00, bound:  3.47284347e-01\n",
      "Epoch: 654 mean train loss:  1.28227425e+00, bound:  3.47359806e-01\n",
      "Epoch: 655 mean train loss:  1.28026116e+00, bound:  3.47434878e-01\n",
      "Epoch: 656 mean train loss:  1.27826047e+00, bound:  3.47509503e-01\n",
      "Epoch: 657 mean train loss:  1.27627146e+00, bound:  3.47583711e-01\n",
      "Epoch: 658 mean train loss:  1.27429497e+00, bound:  3.47657531e-01\n",
      "Epoch: 659 mean train loss:  1.27233005e+00, bound:  3.47730964e-01\n",
      "Epoch: 660 mean train loss:  1.27037609e+00, bound:  3.47804040e-01\n",
      "Epoch: 661 mean train loss:  1.26843393e+00, bound:  3.47876668e-01\n",
      "Epoch: 662 mean train loss:  1.26650286e+00, bound:  3.47948939e-01\n",
      "Epoch: 663 mean train loss:  1.26458204e+00, bound:  3.48020881e-01\n",
      "Epoch: 664 mean train loss:  1.26267266e+00, bound:  3.48092377e-01\n",
      "Epoch: 665 mean train loss:  1.26077402e+00, bound:  3.48163545e-01\n",
      "Epoch: 666 mean train loss:  1.25888538e+00, bound:  3.48234415e-01\n",
      "Epoch: 667 mean train loss:  1.25700712e+00, bound:  3.48304957e-01\n",
      "Epoch: 668 mean train loss:  1.25513864e+00, bound:  3.48375052e-01\n",
      "Epoch: 669 mean train loss:  1.25328028e+00, bound:  3.48444879e-01\n",
      "Epoch: 670 mean train loss:  1.25143170e+00, bound:  3.48514378e-01\n",
      "Epoch: 671 mean train loss:  1.24959266e+00, bound:  3.48583609e-01\n",
      "Epoch: 672 mean train loss:  1.24776292e+00, bound:  3.48652452e-01\n",
      "Epoch: 673 mean train loss:  1.24594247e+00, bound:  3.48721087e-01\n",
      "Epoch: 674 mean train loss:  1.24413109e+00, bound:  3.48789364e-01\n",
      "Epoch: 675 mean train loss:  1.24232900e+00, bound:  3.48857343e-01\n",
      "Epoch: 676 mean train loss:  1.24053538e+00, bound:  3.48925114e-01\n",
      "Epoch: 677 mean train loss:  1.23874998e+00, bound:  3.48992527e-01\n",
      "Epoch: 678 mean train loss:  1.23697376e+00, bound:  3.49059701e-01\n",
      "Epoch: 679 mean train loss:  1.23520589e+00, bound:  3.49126577e-01\n",
      "Epoch: 680 mean train loss:  1.23344564e+00, bound:  3.49193245e-01\n",
      "Epoch: 681 mean train loss:  1.23169422e+00, bound:  3.49259585e-01\n",
      "Epoch: 682 mean train loss:  1.22995043e+00, bound:  3.49325806e-01\n",
      "Epoch: 683 mean train loss:  1.22821450e+00, bound:  3.49391639e-01\n",
      "Epoch: 684 mean train loss:  1.22648597e+00, bound:  3.49457383e-01\n",
      "Epoch: 685 mean train loss:  1.22476530e+00, bound:  3.49522769e-01\n",
      "Epoch: 686 mean train loss:  1.22305191e+00, bound:  3.49587977e-01\n",
      "Epoch: 687 mean train loss:  1.22134590e+00, bound:  3.49653035e-01\n",
      "Epoch: 688 mean train loss:  1.21964729e+00, bound:  3.49717826e-01\n",
      "Epoch: 689 mean train loss:  1.21795547e+00, bound:  3.49782437e-01\n",
      "Epoch: 690 mean train loss:  1.21627128e+00, bound:  3.49846840e-01\n",
      "Epoch: 691 mean train loss:  1.21459317e+00, bound:  3.49911064e-01\n",
      "Epoch: 692 mean train loss:  1.21292222e+00, bound:  3.49975109e-01\n",
      "Epoch: 693 mean train loss:  1.21125770e+00, bound:  3.50038975e-01\n",
      "Epoch: 694 mean train loss:  1.20959997e+00, bound:  3.50102574e-01\n",
      "Epoch: 695 mean train loss:  1.20794868e+00, bound:  3.50166082e-01\n",
      "Epoch: 696 mean train loss:  1.20630336e+00, bound:  3.50229442e-01\n",
      "Epoch: 697 mean train loss:  1.20466495e+00, bound:  3.50292534e-01\n",
      "Epoch: 698 mean train loss:  1.20303226e+00, bound:  3.50355566e-01\n",
      "Epoch: 699 mean train loss:  1.20140553e+00, bound:  3.50418448e-01\n",
      "Epoch: 700 mean train loss:  1.19978511e+00, bound:  3.50481153e-01\n",
      "Epoch: 701 mean train loss:  1.19817054e+00, bound:  3.50543678e-01\n",
      "Epoch: 702 mean train loss:  1.19656146e+00, bound:  3.50606114e-01\n",
      "Epoch: 703 mean train loss:  1.19495821e+00, bound:  3.50668460e-01\n",
      "Epoch: 704 mean train loss:  1.19336057e+00, bound:  3.50730628e-01\n",
      "Epoch: 705 mean train loss:  1.19176841e+00, bound:  3.50792676e-01\n",
      "Epoch: 706 mean train loss:  1.19018209e+00, bound:  3.50854605e-01\n",
      "Epoch: 707 mean train loss:  1.18860078e+00, bound:  3.50916386e-01\n",
      "Epoch: 708 mean train loss:  1.18702459e+00, bound:  3.50978106e-01\n",
      "Epoch: 709 mean train loss:  1.18545437e+00, bound:  3.51039767e-01\n",
      "Epoch: 710 mean train loss:  1.18388867e+00, bound:  3.51101220e-01\n",
      "Epoch: 711 mean train loss:  1.18232810e+00, bound:  3.51162642e-01\n",
      "Epoch: 712 mean train loss:  1.18077290e+00, bound:  3.51223975e-01\n",
      "Epoch: 713 mean train loss:  1.17922223e+00, bound:  3.51285160e-01\n",
      "Epoch: 714 mean train loss:  1.17767656e+00, bound:  3.51346344e-01\n",
      "Epoch: 715 mean train loss:  1.17613542e+00, bound:  3.51407379e-01\n",
      "Epoch: 716 mean train loss:  1.17459965e+00, bound:  3.51468325e-01\n",
      "Epoch: 717 mean train loss:  1.17306817e+00, bound:  3.51529241e-01\n",
      "Epoch: 718 mean train loss:  1.17154157e+00, bound:  3.51590097e-01\n",
      "Epoch: 719 mean train loss:  1.17001879e+00, bound:  3.51650864e-01\n",
      "Epoch: 720 mean train loss:  1.16850114e+00, bound:  3.51711571e-01\n",
      "Epoch: 721 mean train loss:  1.16698813e+00, bound:  3.51772219e-01\n",
      "Epoch: 722 mean train loss:  1.16547894e+00, bound:  3.51832807e-01\n",
      "Epoch: 723 mean train loss:  1.16397393e+00, bound:  3.51893306e-01\n",
      "Epoch: 724 mean train loss:  1.16247356e+00, bound:  3.51953864e-01\n",
      "Epoch: 725 mean train loss:  1.16097736e+00, bound:  3.52014244e-01\n",
      "Epoch: 726 mean train loss:  1.15948534e+00, bound:  3.52074683e-01\n",
      "Epoch: 727 mean train loss:  1.15799725e+00, bound:  3.52135062e-01\n",
      "Epoch: 728 mean train loss:  1.15651321e+00, bound:  3.52195352e-01\n",
      "Epoch: 729 mean train loss:  1.15503323e+00, bound:  3.52255553e-01\n",
      "Epoch: 730 mean train loss:  1.15355754e+00, bound:  3.52315903e-01\n",
      "Epoch: 731 mean train loss:  1.15208530e+00, bound:  3.52376014e-01\n",
      "Epoch: 732 mean train loss:  1.15061700e+00, bound:  3.52436274e-01\n",
      "Epoch: 733 mean train loss:  1.14915228e+00, bound:  3.52496415e-01\n",
      "Epoch: 734 mean train loss:  1.14769173e+00, bound:  3.52556556e-01\n",
      "Epoch: 735 mean train loss:  1.14623439e+00, bound:  3.52616698e-01\n",
      "Epoch: 736 mean train loss:  1.14478123e+00, bound:  3.52676839e-01\n",
      "Epoch: 737 mean train loss:  1.14333165e+00, bound:  3.52736890e-01\n",
      "Epoch: 738 mean train loss:  1.14188528e+00, bound:  3.52796942e-01\n",
      "Epoch: 739 mean train loss:  1.14044273e+00, bound:  3.52857023e-01\n",
      "Epoch: 740 mean train loss:  1.13900375e+00, bound:  3.52917105e-01\n",
      "Epoch: 741 mean train loss:  1.13756835e+00, bound:  3.52977186e-01\n",
      "Epoch: 742 mean train loss:  1.13613594e+00, bound:  3.53037208e-01\n",
      "Epoch: 743 mean train loss:  1.13470685e+00, bound:  3.53097260e-01\n",
      "Epoch: 744 mean train loss:  1.13328171e+00, bound:  3.53157341e-01\n",
      "Epoch: 745 mean train loss:  1.13185930e+00, bound:  3.53217423e-01\n",
      "Epoch: 746 mean train loss:  1.13044083e+00, bound:  3.53277475e-01\n",
      "Epoch: 747 mean train loss:  1.12902498e+00, bound:  3.53337586e-01\n",
      "Epoch: 748 mean train loss:  1.12761271e+00, bound:  3.53397697e-01\n",
      "Epoch: 749 mean train loss:  1.12620330e+00, bound:  3.53457779e-01\n",
      "Epoch: 750 mean train loss:  1.12479722e+00, bound:  3.53517890e-01\n",
      "Epoch: 751 mean train loss:  1.12339461e+00, bound:  3.53578031e-01\n",
      "Epoch: 752 mean train loss:  1.12199426e+00, bound:  3.53638142e-01\n",
      "Epoch: 753 mean train loss:  1.12059700e+00, bound:  3.53698343e-01\n",
      "Epoch: 754 mean train loss:  1.11920345e+00, bound:  3.53758514e-01\n",
      "Epoch: 755 mean train loss:  1.11781263e+00, bound:  3.53818715e-01\n",
      "Epoch: 756 mean train loss:  1.11642444e+00, bound:  3.53878945e-01\n",
      "Epoch: 757 mean train loss:  1.11503971e+00, bound:  3.53939235e-01\n",
      "Epoch: 758 mean train loss:  1.11365759e+00, bound:  3.53999496e-01\n",
      "Epoch: 759 mean train loss:  1.11227822e+00, bound:  3.54059815e-01\n",
      "Epoch: 760 mean train loss:  1.11090171e+00, bound:  3.54120165e-01\n",
      "Epoch: 761 mean train loss:  1.10952842e+00, bound:  3.54180515e-01\n",
      "Epoch: 762 mean train loss:  1.10815763e+00, bound:  3.54240894e-01\n",
      "Epoch: 763 mean train loss:  1.10678995e+00, bound:  3.54301393e-01\n",
      "Epoch: 764 mean train loss:  1.10542464e+00, bound:  3.54361832e-01\n",
      "Epoch: 765 mean train loss:  1.10406196e+00, bound:  3.54422331e-01\n",
      "Epoch: 766 mean train loss:  1.10270238e+00, bound:  3.54482859e-01\n",
      "Epoch: 767 mean train loss:  1.10134506e+00, bound:  3.54543477e-01\n",
      "Epoch: 768 mean train loss:  1.09999049e+00, bound:  3.54604125e-01\n",
      "Epoch: 769 mean train loss:  1.09863913e+00, bound:  3.54664743e-01\n",
      "Epoch: 770 mean train loss:  1.09728980e+00, bound:  3.54725480e-01\n",
      "Epoch: 771 mean train loss:  1.09594285e+00, bound:  3.54786187e-01\n",
      "Epoch: 772 mean train loss:  1.09459925e+00, bound:  3.54846984e-01\n",
      "Epoch: 773 mean train loss:  1.09325767e+00, bound:  3.54907840e-01\n",
      "Epoch: 774 mean train loss:  1.09191883e+00, bound:  3.54968697e-01\n",
      "Epoch: 775 mean train loss:  1.09058237e+00, bound:  3.55029643e-01\n",
      "Epoch: 776 mean train loss:  1.08924842e+00, bound:  3.55090588e-01\n",
      "Epoch: 777 mean train loss:  1.08791721e+00, bound:  3.55151594e-01\n",
      "Epoch: 778 mean train loss:  1.08658779e+00, bound:  3.55212718e-01\n",
      "Epoch: 779 mean train loss:  1.08526146e+00, bound:  3.55273783e-01\n",
      "Epoch: 780 mean train loss:  1.08393764e+00, bound:  3.55334938e-01\n",
      "Epoch: 781 mean train loss:  1.08261573e+00, bound:  3.55396152e-01\n",
      "Epoch: 782 mean train loss:  1.08129644e+00, bound:  3.55457395e-01\n",
      "Epoch: 783 mean train loss:  1.07997942e+00, bound:  3.55518728e-01\n",
      "Epoch: 784 mean train loss:  1.07866514e+00, bound:  3.55580091e-01\n",
      "Epoch: 785 mean train loss:  1.07735324e+00, bound:  3.55641484e-01\n",
      "Epoch: 786 mean train loss:  1.07604361e+00, bound:  3.55702996e-01\n",
      "Epoch: 787 mean train loss:  1.07473600e+00, bound:  3.55764508e-01\n",
      "Epoch: 788 mean train loss:  1.07343102e+00, bound:  3.55826050e-01\n",
      "Epoch: 789 mean train loss:  1.07212830e+00, bound:  3.55887681e-01\n",
      "Epoch: 790 mean train loss:  1.07082832e+00, bound:  3.55949372e-01\n",
      "Epoch: 791 mean train loss:  1.06952977e+00, bound:  3.56011093e-01\n",
      "Epoch: 792 mean train loss:  1.06823409e+00, bound:  3.56072873e-01\n",
      "Epoch: 793 mean train loss:  1.06694043e+00, bound:  3.56134683e-01\n",
      "Epoch: 794 mean train loss:  1.06564939e+00, bound:  3.56196582e-01\n",
      "Epoch: 795 mean train loss:  1.06436026e+00, bound:  3.56258541e-01\n",
      "Epoch: 796 mean train loss:  1.06307364e+00, bound:  3.56320500e-01\n",
      "Epoch: 797 mean train loss:  1.06178892e+00, bound:  3.56382549e-01\n",
      "Epoch: 798 mean train loss:  1.06050682e+00, bound:  3.56444657e-01\n",
      "Epoch: 799 mean train loss:  1.05922675e+00, bound:  3.56506795e-01\n",
      "Epoch: 800 mean train loss:  1.05794871e+00, bound:  3.56569022e-01\n",
      "Epoch: 801 mean train loss:  1.05667341e+00, bound:  3.56631249e-01\n",
      "Epoch: 802 mean train loss:  1.05539966e+00, bound:  3.56693566e-01\n",
      "Epoch: 803 mean train loss:  1.05412889e+00, bound:  3.56755942e-01\n",
      "Epoch: 804 mean train loss:  1.05285931e+00, bound:  3.56818318e-01\n",
      "Epoch: 805 mean train loss:  1.05159247e+00, bound:  3.56880784e-01\n",
      "Epoch: 806 mean train loss:  1.05032837e+00, bound:  3.56943309e-01\n",
      "Epoch: 807 mean train loss:  1.04906583e+00, bound:  3.57005864e-01\n",
      "Epoch: 808 mean train loss:  1.04780507e+00, bound:  3.57068479e-01\n",
      "Epoch: 809 mean train loss:  1.04654729e+00, bound:  3.57131153e-01\n",
      "Epoch: 810 mean train loss:  1.04529130e+00, bound:  3.57193798e-01\n",
      "Epoch: 811 mean train loss:  1.04403722e+00, bound:  3.57256621e-01\n",
      "Epoch: 812 mean train loss:  1.04278564e+00, bound:  3.57319415e-01\n",
      "Epoch: 813 mean train loss:  1.04153597e+00, bound:  3.57382238e-01\n",
      "Epoch: 814 mean train loss:  1.04028857e+00, bound:  3.57445180e-01\n",
      "Epoch: 815 mean train loss:  1.03904343e+00, bound:  3.57508123e-01\n",
      "Epoch: 816 mean train loss:  1.03780031e+00, bound:  3.57571125e-01\n",
      "Epoch: 817 mean train loss:  1.03655946e+00, bound:  3.57634127e-01\n",
      "Epoch: 818 mean train loss:  1.03532040e+00, bound:  3.57697219e-01\n",
      "Epoch: 819 mean train loss:  1.03408384e+00, bound:  3.57760340e-01\n",
      "Epoch: 820 mean train loss:  1.03284883e+00, bound:  3.57823491e-01\n",
      "Epoch: 821 mean train loss:  1.03161669e+00, bound:  3.57886732e-01\n",
      "Epoch: 822 mean train loss:  1.03038597e+00, bound:  3.57949913e-01\n",
      "Epoch: 823 mean train loss:  1.02915716e+00, bound:  3.58013272e-01\n",
      "Epoch: 824 mean train loss:  1.02793133e+00, bound:  3.58076572e-01\n",
      "Epoch: 825 mean train loss:  1.02670693e+00, bound:  3.58139962e-01\n",
      "Epoch: 826 mean train loss:  1.02548516e+00, bound:  3.58203351e-01\n",
      "Epoch: 827 mean train loss:  1.02426505e+00, bound:  3.58266771e-01\n",
      "Epoch: 828 mean train loss:  1.02304709e+00, bound:  3.58330190e-01\n",
      "Epoch: 829 mean train loss:  1.02183139e+00, bound:  3.58393699e-01\n",
      "Epoch: 830 mean train loss:  1.02061737e+00, bound:  3.58457267e-01\n",
      "Epoch: 831 mean train loss:  1.01940608e+00, bound:  3.58520806e-01\n",
      "Epoch: 832 mean train loss:  1.01819623e+00, bound:  3.58584374e-01\n",
      "Epoch: 833 mean train loss:  1.01698852e+00, bound:  3.58648002e-01\n",
      "Epoch: 834 mean train loss:  1.01578271e+00, bound:  3.58711660e-01\n",
      "Epoch: 835 mean train loss:  1.01457965e+00, bound:  3.58775318e-01\n",
      "Epoch: 836 mean train loss:  1.01337802e+00, bound:  3.58839035e-01\n",
      "Epoch: 837 mean train loss:  1.01217842e+00, bound:  3.58902752e-01\n",
      "Epoch: 838 mean train loss:  1.01098144e+00, bound:  3.58966470e-01\n",
      "Epoch: 839 mean train loss:  1.00978577e+00, bound:  3.59030247e-01\n",
      "Epoch: 840 mean train loss:  1.00859272e+00, bound:  3.59094054e-01\n",
      "Epoch: 841 mean train loss:  1.00740147e+00, bound:  3.59157860e-01\n",
      "Epoch: 842 mean train loss:  1.00621200e+00, bound:  3.59221637e-01\n",
      "Epoch: 843 mean train loss:  1.00502515e+00, bound:  3.59285533e-01\n",
      "Epoch: 844 mean train loss:  1.00383985e+00, bound:  3.59349370e-01\n",
      "Epoch: 845 mean train loss:  1.00265622e+00, bound:  3.59413207e-01\n",
      "Epoch: 846 mean train loss:  1.00147510e+00, bound:  3.59477133e-01\n",
      "Epoch: 847 mean train loss:  1.00029600e+00, bound:  3.59540969e-01\n",
      "Epoch: 848 mean train loss:  9.99118686e-01, bound:  3.59604865e-01\n",
      "Epoch: 849 mean train loss:  9.97943580e-01, bound:  3.59668761e-01\n",
      "Epoch: 850 mean train loss:  9.96769786e-01, bound:  3.59732687e-01\n",
      "Epoch: 851 mean train loss:  9.95598674e-01, bound:  3.59796643e-01\n",
      "Epoch: 852 mean train loss:  9.94429231e-01, bound:  3.59860510e-01\n",
      "Epoch: 853 mean train loss:  9.93261635e-01, bound:  3.59924436e-01\n",
      "Epoch: 854 mean train loss:  9.92095947e-01, bound:  3.59988332e-01\n",
      "Epoch: 855 mean train loss:  9.90932345e-01, bound:  3.60052288e-01\n",
      "Epoch: 856 mean train loss:  9.89771128e-01, bound:  3.60116184e-01\n",
      "Epoch: 857 mean train loss:  9.88611042e-01, bound:  3.60180140e-01\n",
      "Epoch: 858 mean train loss:  9.87452745e-01, bound:  3.60244036e-01\n",
      "Epoch: 859 mean train loss:  9.86296415e-01, bound:  3.60307932e-01\n",
      "Epoch: 860 mean train loss:  9.85142589e-01, bound:  3.60371858e-01\n",
      "Epoch: 861 mean train loss:  9.83989894e-01, bound:  3.60435724e-01\n",
      "Epoch: 862 mean train loss:  9.82839704e-01, bound:  3.60499620e-01\n",
      "Epoch: 863 mean train loss:  9.81691241e-01, bound:  3.60563487e-01\n",
      "Epoch: 864 mean train loss:  9.80544508e-01, bound:  3.60627383e-01\n",
      "Epoch: 865 mean train loss:  9.79399443e-01, bound:  3.60691220e-01\n",
      "Epoch: 866 mean train loss:  9.78256643e-01, bound:  3.60754967e-01\n",
      "Epoch: 867 mean train loss:  9.77114916e-01, bound:  3.60818803e-01\n",
      "Epoch: 868 mean train loss:  9.75975633e-01, bound:  3.60882610e-01\n",
      "Epoch: 869 mean train loss:  9.74837899e-01, bound:  3.60946357e-01\n",
      "Epoch: 870 mean train loss:  9.73701954e-01, bound:  3.61010104e-01\n",
      "Epoch: 871 mean train loss:  9.72567618e-01, bound:  3.61073822e-01\n",
      "Epoch: 872 mean train loss:  9.71435130e-01, bound:  3.61137539e-01\n",
      "Epoch: 873 mean train loss:  9.70304489e-01, bound:  3.61201197e-01\n",
      "Epoch: 874 mean train loss:  9.69175339e-01, bound:  3.61264825e-01\n",
      "Epoch: 875 mean train loss:  9.68048394e-01, bound:  3.61328453e-01\n",
      "Epoch: 876 mean train loss:  9.66922820e-01, bound:  3.61392021e-01\n",
      "Epoch: 877 mean train loss:  9.65798736e-01, bound:  3.61455619e-01\n",
      "Epoch: 878 mean train loss:  9.64676380e-01, bound:  3.61519128e-01\n",
      "Epoch: 879 mean train loss:  9.63556349e-01, bound:  3.61582607e-01\n",
      "Epoch: 880 mean train loss:  9.62437093e-01, bound:  3.61646116e-01\n",
      "Epoch: 881 mean train loss:  9.61319983e-01, bound:  3.61709505e-01\n",
      "Epoch: 882 mean train loss:  9.60204303e-01, bound:  3.61772925e-01\n",
      "Epoch: 883 mean train loss:  9.59090412e-01, bound:  3.61836255e-01\n",
      "Epoch: 884 mean train loss:  9.57977891e-01, bound:  3.61899555e-01\n",
      "Epoch: 885 mean train loss:  9.56866622e-01, bound:  3.61962855e-01\n",
      "Epoch: 886 mean train loss:  9.55757618e-01, bound:  3.62026066e-01\n",
      "Epoch: 887 mean train loss:  9.54650104e-01, bound:  3.62089276e-01\n",
      "Epoch: 888 mean train loss:  9.53543723e-01, bound:  3.62152368e-01\n",
      "Epoch: 889 mean train loss:  9.52439249e-01, bound:  3.62215519e-01\n",
      "Epoch: 890 mean train loss:  9.51335609e-01, bound:  3.62278551e-01\n",
      "Epoch: 891 mean train loss:  9.50233698e-01, bound:  3.62341553e-01\n",
      "Epoch: 892 mean train loss:  9.49133873e-01, bound:  3.62404525e-01\n",
      "Epoch: 893 mean train loss:  9.48035002e-01, bound:  3.62467438e-01\n",
      "Epoch: 894 mean train loss:  9.46937680e-01, bound:  3.62530291e-01\n",
      "Epoch: 895 mean train loss:  9.45841551e-01, bound:  3.62593085e-01\n",
      "Epoch: 896 mean train loss:  9.44747567e-01, bound:  3.62655848e-01\n",
      "Epoch: 897 mean train loss:  9.43654537e-01, bound:  3.62718552e-01\n",
      "Epoch: 898 mean train loss:  9.42562521e-01, bound:  3.62781227e-01\n",
      "Epoch: 899 mean train loss:  9.41472054e-01, bound:  3.62843841e-01\n",
      "Epoch: 900 mean train loss:  9.40383494e-01, bound:  3.62906337e-01\n",
      "Epoch: 901 mean train loss:  9.39295948e-01, bound:  3.62968832e-01\n",
      "Epoch: 902 mean train loss:  9.38209176e-01, bound:  3.63031298e-01\n",
      "Epoch: 903 mean train loss:  9.37124372e-01, bound:  3.63093704e-01\n",
      "Epoch: 904 mean train loss:  9.36040699e-01, bound:  3.63156050e-01\n",
      "Epoch: 905 mean train loss:  9.34958518e-01, bound:  3.63218278e-01\n",
      "Epoch: 906 mean train loss:  9.33877051e-01, bound:  3.63280475e-01\n",
      "Epoch: 907 mean train loss:  9.32797313e-01, bound:  3.63342553e-01\n",
      "Epoch: 908 mean train loss:  9.31718767e-01, bound:  3.63404632e-01\n",
      "Epoch: 909 mean train loss:  9.30641353e-01, bound:  3.63466680e-01\n",
      "Epoch: 910 mean train loss:  9.29565012e-01, bound:  3.63528639e-01\n",
      "Epoch: 911 mean train loss:  9.28489864e-01, bound:  3.63590568e-01\n",
      "Epoch: 912 mean train loss:  9.27415967e-01, bound:  3.63652378e-01\n",
      "Epoch: 913 mean train loss:  9.26343560e-01, bound:  3.63714129e-01\n",
      "Epoch: 914 mean train loss:  9.25271690e-01, bound:  3.63775820e-01\n",
      "Epoch: 915 mean train loss:  9.24201548e-01, bound:  3.63837451e-01\n",
      "Epoch: 916 mean train loss:  9.23132241e-01, bound:  3.63899022e-01\n",
      "Epoch: 917 mean train loss:  9.22063887e-01, bound:  3.63960534e-01\n",
      "Epoch: 918 mean train loss:  9.20997083e-01, bound:  3.64021987e-01\n",
      "Epoch: 919 mean train loss:  9.19931114e-01, bound:  3.64083320e-01\n",
      "Epoch: 920 mean train loss:  9.18865919e-01, bound:  3.64144593e-01\n",
      "Epoch: 921 mean train loss:  9.17801857e-01, bound:  3.64205837e-01\n",
      "Epoch: 922 mean train loss:  9.16739106e-01, bound:  3.64267021e-01\n",
      "Epoch: 923 mean train loss:  9.15677190e-01, bound:  3.64328116e-01\n",
      "Epoch: 924 mean train loss:  9.14616823e-01, bound:  3.64389151e-01\n",
      "Epoch: 925 mean train loss:  9.13556874e-01, bound:  3.64450097e-01\n",
      "Epoch: 926 mean train loss:  9.12498236e-01, bound:  3.64510983e-01\n",
      "Epoch: 927 mean train loss:  9.11440253e-01, bound:  3.64571810e-01\n",
      "Epoch: 928 mean train loss:  9.10383165e-01, bound:  3.64632517e-01\n",
      "Epoch: 929 mean train loss:  9.09327149e-01, bound:  3.64693224e-01\n",
      "Epoch: 930 mean train loss:  9.08272564e-01, bound:  3.64753813e-01\n",
      "Epoch: 931 mean train loss:  9.07218397e-01, bound:  3.64814401e-01\n",
      "Epoch: 932 mean train loss:  9.06165421e-01, bound:  3.64874810e-01\n",
      "Epoch: 933 mean train loss:  9.05112863e-01, bound:  3.64935219e-01\n",
      "Epoch: 934 mean train loss:  9.04061556e-01, bound:  3.64995509e-01\n",
      "Epoch: 935 mean train loss:  9.03011024e-01, bound:  3.65055740e-01\n",
      "Epoch: 936 mean train loss:  9.01961267e-01, bound:  3.65115911e-01\n",
      "Epoch: 937 mean train loss:  9.00912881e-01, bound:  3.65175992e-01\n",
      "Epoch: 938 mean train loss:  8.99864972e-01, bound:  3.65236074e-01\n",
      "Epoch: 939 mean train loss:  8.98818076e-01, bound:  3.65295976e-01\n",
      "Epoch: 940 mean train loss:  8.97771955e-01, bound:  3.65355879e-01\n",
      "Epoch: 941 mean train loss:  8.96726608e-01, bound:  3.65415663e-01\n",
      "Epoch: 942 mean train loss:  8.95681918e-01, bound:  3.65475386e-01\n",
      "Epoch: 943 mean train loss:  8.94638181e-01, bound:  3.65535080e-01\n",
      "Epoch: 944 mean train loss:  8.93595338e-01, bound:  3.65594625e-01\n",
      "Epoch: 945 mean train loss:  8.92552972e-01, bound:  3.65654141e-01\n",
      "Epoch: 946 mean train loss:  8.91511858e-01, bound:  3.65713567e-01\n",
      "Epoch: 947 mean train loss:  8.90470862e-01, bound:  3.65772903e-01\n",
      "Epoch: 948 mean train loss:  8.89431119e-01, bound:  3.65832239e-01\n",
      "Epoch: 949 mean train loss:  8.88392091e-01, bound:  3.65891457e-01\n",
      "Epoch: 950 mean train loss:  8.87353599e-01, bound:  3.65950584e-01\n",
      "Epoch: 951 mean train loss:  8.86315763e-01, bound:  3.66009593e-01\n",
      "Epoch: 952 mean train loss:  8.85279357e-01, bound:  3.66068602e-01\n",
      "Epoch: 953 mean train loss:  8.84242833e-01, bound:  3.66127551e-01\n",
      "Epoch: 954 mean train loss:  8.83207560e-01, bound:  3.66186380e-01\n",
      "Epoch: 955 mean train loss:  8.82172823e-01, bound:  3.66245151e-01\n",
      "Epoch: 956 mean train loss:  8.81138921e-01, bound:  3.66303831e-01\n",
      "Epoch: 957 mean train loss:  8.80105317e-01, bound:  3.66362453e-01\n",
      "Epoch: 958 mean train loss:  8.79072964e-01, bound:  3.66420984e-01\n",
      "Epoch: 959 mean train loss:  8.78040969e-01, bound:  3.66479427e-01\n",
      "Epoch: 960 mean train loss:  8.77010167e-01, bound:  3.66537839e-01\n",
      "Epoch: 961 mean train loss:  8.75978947e-01, bound:  3.66596162e-01\n",
      "Epoch: 962 mean train loss:  8.74949753e-01, bound:  3.66654366e-01\n",
      "Epoch: 963 mean train loss:  8.73920023e-01, bound:  3.66712511e-01\n",
      "Epoch: 964 mean train loss:  8.72891784e-01, bound:  3.66770655e-01\n",
      "Epoch: 965 mean train loss:  8.71863902e-01, bound:  3.66828650e-01\n",
      "Epoch: 966 mean train loss:  8.70836675e-01, bound:  3.66886586e-01\n",
      "Epoch: 967 mean train loss:  8.69810402e-01, bound:  3.66944432e-01\n",
      "Epoch: 968 mean train loss:  8.68784189e-01, bound:  3.67002249e-01\n",
      "Epoch: 969 mean train loss:  8.67758930e-01, bound:  3.67060006e-01\n",
      "Epoch: 970 mean train loss:  8.66734624e-01, bound:  3.67117614e-01\n",
      "Epoch: 971 mean train loss:  8.65710258e-01, bound:  3.67175162e-01\n",
      "Epoch: 972 mean train loss:  8.64687443e-01, bound:  3.67232651e-01\n",
      "Epoch: 973 mean train loss:  8.63664448e-01, bound:  3.67290050e-01\n",
      "Epoch: 974 mean train loss:  8.62642407e-01, bound:  3.67347389e-01\n",
      "Epoch: 975 mean train loss:  8.61621439e-01, bound:  3.67404670e-01\n",
      "Epoch: 976 mean train loss:  8.60600531e-01, bound:  3.67461920e-01\n",
      "Epoch: 977 mean train loss:  8.59580576e-01, bound:  3.67518991e-01\n",
      "Epoch: 978 mean train loss:  8.58561158e-01, bound:  3.67576003e-01\n",
      "Epoch: 979 mean train loss:  8.57542396e-01, bound:  3.67632955e-01\n",
      "Epoch: 980 mean train loss:  8.56524110e-01, bound:  3.67689818e-01\n",
      "Epoch: 981 mean train loss:  8.55506599e-01, bound:  3.67746681e-01\n",
      "Epoch: 982 mean train loss:  8.54489386e-01, bound:  3.67803395e-01\n",
      "Epoch: 983 mean train loss:  8.53472948e-01, bound:  3.67860019e-01\n",
      "Epoch: 984 mean train loss:  8.52457404e-01, bound:  3.67916644e-01\n",
      "Epoch: 985 mean train loss:  8.51442933e-01, bound:  3.67973119e-01\n",
      "Epoch: 986 mean train loss:  8.50428343e-01, bound:  3.68029565e-01\n",
      "Epoch: 987 mean train loss:  8.49414647e-01, bound:  3.68085891e-01\n",
      "Epoch: 988 mean train loss:  8.48401785e-01, bound:  3.68142158e-01\n",
      "Epoch: 989 mean train loss:  8.47388983e-01, bound:  3.68198365e-01\n",
      "Epoch: 990 mean train loss:  8.46377194e-01, bound:  3.68254513e-01\n",
      "Epoch: 991 mean train loss:  8.45366120e-01, bound:  3.68310571e-01\n",
      "Epoch: 992 mean train loss:  8.44355702e-01, bound:  3.68366510e-01\n",
      "Epoch: 993 mean train loss:  8.43346119e-01, bound:  3.68422449e-01\n",
      "Epoch: 994 mean train loss:  8.42336833e-01, bound:  3.68478268e-01\n",
      "Epoch: 995 mean train loss:  8.41328442e-01, bound:  3.68533969e-01\n",
      "Epoch: 996 mean train loss:  8.40320528e-01, bound:  3.68589669e-01\n",
      "Epoch: 997 mean train loss:  8.39313567e-01, bound:  3.68645281e-01\n",
      "Epoch: 998 mean train loss:  8.38307261e-01, bound:  3.68700743e-01\n",
      "Epoch: 999 mean train loss:  8.37301135e-01, bound:  3.68756205e-01\n",
      "Epoch: 1000 mean train loss:  8.36295784e-01, bound:  3.68811518e-01\n",
      "Epoch: 1001 mean train loss:  8.35291266e-01, bound:  3.68866831e-01\n",
      "Epoch: 1002 mean train loss:  8.34287882e-01, bound:  3.68922025e-01\n",
      "Epoch: 1003 mean train loss:  8.33284616e-01, bound:  3.68977159e-01\n",
      "Epoch: 1004 mean train loss:  8.32282543e-01, bound:  3.69032204e-01\n",
      "Epoch: 1005 mean train loss:  8.31280470e-01, bound:  3.69087100e-01\n",
      "Epoch: 1006 mean train loss:  8.30280006e-01, bound:  3.69141996e-01\n",
      "Epoch: 1007 mean train loss:  8.29279304e-01, bound:  3.69196802e-01\n",
      "Epoch: 1008 mean train loss:  8.28280032e-01, bound:  3.69251549e-01\n",
      "Epoch: 1009 mean train loss:  8.27280998e-01, bound:  3.69306147e-01\n",
      "Epoch: 1010 mean train loss:  8.26283514e-01, bound:  3.69360745e-01\n",
      "Epoch: 1011 mean train loss:  8.25285912e-01, bound:  3.69415253e-01\n",
      "Epoch: 1012 mean train loss:  8.24289083e-01, bound:  3.69469613e-01\n",
      "Epoch: 1013 mean train loss:  8.23293030e-01, bound:  3.69523942e-01\n",
      "Epoch: 1014 mean train loss:  8.22298169e-01, bound:  3.69578212e-01\n",
      "Epoch: 1015 mean train loss:  8.21303666e-01, bound:  3.69632334e-01\n",
      "Epoch: 1016 mean train loss:  8.20309997e-01, bound:  3.69686455e-01\n",
      "Epoch: 1017 mean train loss:  8.19317460e-01, bound:  3.69740456e-01\n",
      "Epoch: 1018 mean train loss:  8.18324983e-01, bound:  3.69794339e-01\n",
      "Epoch: 1019 mean train loss:  8.17334056e-01, bound:  3.69848192e-01\n",
      "Epoch: 1020 mean train loss:  8.16343665e-01, bound:  3.69901955e-01\n",
      "Epoch: 1021 mean train loss:  8.15353990e-01, bound:  3.69955599e-01\n",
      "Epoch: 1022 mean train loss:  8.14365506e-01, bound:  3.70009154e-01\n",
      "Epoch: 1023 mean train loss:  8.13377202e-01, bound:  3.70062679e-01\n",
      "Epoch: 1024 mean train loss:  8.12390268e-01, bound:  3.70116025e-01\n",
      "Epoch: 1025 mean train loss:  8.11403930e-01, bound:  3.70169401e-01\n",
      "Epoch: 1026 mean train loss:  8.10418606e-01, bound:  3.70222598e-01\n",
      "Epoch: 1027 mean train loss:  8.09434354e-01, bound:  3.70275766e-01\n",
      "Epoch: 1028 mean train loss:  8.08450103e-01, bound:  3.70328814e-01\n",
      "Epoch: 1029 mean train loss:  8.07467103e-01, bound:  3.70381802e-01\n",
      "Epoch: 1030 mean train loss:  8.06485593e-01, bound:  3.70434672e-01\n",
      "Epoch: 1031 mean train loss:  8.05504501e-01, bound:  3.70487452e-01\n",
      "Epoch: 1032 mean train loss:  8.04524004e-01, bound:  3.70540231e-01\n",
      "Epoch: 1033 mean train loss:  8.03544760e-01, bound:  3.70592833e-01\n",
      "Epoch: 1034 mean train loss:  8.02566588e-01, bound:  3.70645374e-01\n",
      "Epoch: 1035 mean train loss:  8.01589072e-01, bound:  3.70697796e-01\n",
      "Epoch: 1036 mean train loss:  8.00612688e-01, bound:  3.70750129e-01\n",
      "Epoch: 1037 mean train loss:  7.99637496e-01, bound:  3.70802402e-01\n",
      "Epoch: 1038 mean train loss:  7.98662782e-01, bound:  3.70854527e-01\n",
      "Epoch: 1039 mean train loss:  7.97689497e-01, bound:  3.70906681e-01\n",
      "Epoch: 1040 mean train loss:  7.96716392e-01, bound:  3.70958596e-01\n",
      "Epoch: 1041 mean train loss:  7.95745373e-01, bound:  3.71010512e-01\n",
      "Epoch: 1042 mean train loss:  7.94774711e-01, bound:  3.71062309e-01\n",
      "Epoch: 1043 mean train loss:  7.93805003e-01, bound:  3.71113986e-01\n",
      "Epoch: 1044 mean train loss:  7.92836607e-01, bound:  3.71165603e-01\n",
      "Epoch: 1045 mean train loss:  7.91868865e-01, bound:  3.71217072e-01\n",
      "Epoch: 1046 mean train loss:  7.90902555e-01, bound:  3.71268451e-01\n",
      "Epoch: 1047 mean train loss:  7.89937258e-01, bound:  3.71319771e-01\n",
      "Epoch: 1048 mean train loss:  7.88973033e-01, bound:  3.71371001e-01\n",
      "Epoch: 1049 mean train loss:  7.88010061e-01, bound:  3.71422112e-01\n",
      "Epoch: 1050 mean train loss:  7.87048042e-01, bound:  3.71473163e-01\n",
      "Epoch: 1051 mean train loss:  7.86086738e-01, bound:  3.71524036e-01\n",
      "Epoch: 1052 mean train loss:  7.85127103e-01, bound:  3.71574819e-01\n",
      "Epoch: 1053 mean train loss:  7.84168124e-01, bound:  3.71625543e-01\n",
      "Epoch: 1054 mean train loss:  7.83210695e-01, bound:  3.71676147e-01\n",
      "Epoch: 1055 mean train loss:  7.82254279e-01, bound:  3.71726602e-01\n",
      "Epoch: 1056 mean train loss:  7.81298578e-01, bound:  3.71777028e-01\n",
      "Epoch: 1057 mean train loss:  7.80344963e-01, bound:  3.71827364e-01\n",
      "Epoch: 1058 mean train loss:  7.79392183e-01, bound:  3.71877462e-01\n",
      "Epoch: 1059 mean train loss:  7.78440416e-01, bound:  3.71927559e-01\n",
      "Epoch: 1060 mean train loss:  7.77489841e-01, bound:  3.71977508e-01\n",
      "Epoch: 1061 mean train loss:  7.76540697e-01, bound:  3.72027338e-01\n",
      "Epoch: 1062 mean train loss:  7.75592744e-01, bound:  3.72077107e-01\n",
      "Epoch: 1063 mean train loss:  7.74646044e-01, bound:  3.72126698e-01\n",
      "Epoch: 1064 mean train loss:  7.73700655e-01, bound:  3.72176230e-01\n",
      "Epoch: 1065 mean train loss:  7.72756636e-01, bound:  3.72225642e-01\n",
      "Epoch: 1066 mean train loss:  7.71813273e-01, bound:  3.72274905e-01\n",
      "Epoch: 1067 mean train loss:  7.70872176e-01, bound:  3.72324109e-01\n",
      "Epoch: 1068 mean train loss:  7.69931614e-01, bound:  3.72373164e-01\n",
      "Epoch: 1069 mean train loss:  7.68992722e-01, bound:  3.72422069e-01\n",
      "Epoch: 1070 mean train loss:  7.68055201e-01, bound:  3.72470915e-01\n",
      "Epoch: 1071 mean train loss:  7.67119288e-01, bound:  3.72519642e-01\n",
      "Epoch: 1072 mean train loss:  7.66184449e-01, bound:  3.72568220e-01\n",
      "Epoch: 1073 mean train loss:  7.65251279e-01, bound:  3.72616619e-01\n",
      "Epoch: 1074 mean train loss:  7.64319241e-01, bound:  3.72664988e-01\n",
      "Epoch: 1075 mean train loss:  7.63388515e-01, bound:  3.72713208e-01\n",
      "Epoch: 1076 mean train loss:  7.62459338e-01, bound:  3.72761279e-01\n",
      "Epoch: 1077 mean train loss:  7.61531830e-01, bound:  3.72809261e-01\n",
      "Epoch: 1078 mean train loss:  7.60605693e-01, bound:  3.72857124e-01\n",
      "Epoch: 1079 mean train loss:  7.59680688e-01, bound:  3.72904778e-01\n",
      "Epoch: 1080 mean train loss:  7.58756936e-01, bound:  3.72952402e-01\n",
      "Epoch: 1081 mean train loss:  7.57835984e-01, bound:  3.72999817e-01\n",
      "Epoch: 1082 mean train loss:  7.56915569e-01, bound:  3.73047143e-01\n",
      "Epoch: 1083 mean train loss:  7.55996764e-01, bound:  3.73094350e-01\n",
      "Epoch: 1084 mean train loss:  7.55079269e-01, bound:  3.73141408e-01\n",
      "Epoch: 1085 mean train loss:  7.54163980e-01, bound:  3.73188287e-01\n",
      "Epoch: 1086 mean train loss:  7.53249824e-01, bound:  3.73235106e-01\n",
      "Epoch: 1087 mean train loss:  7.52337575e-01, bound:  3.73281747e-01\n",
      "Epoch: 1088 mean train loss:  7.51426399e-01, bound:  3.73328269e-01\n",
      "Epoch: 1089 mean train loss:  7.50516891e-01, bound:  3.73374611e-01\n",
      "Epoch: 1090 mean train loss:  7.49609828e-01, bound:  3.73420864e-01\n",
      "Epoch: 1091 mean train loss:  7.48703778e-01, bound:  3.73466969e-01\n",
      "Epoch: 1092 mean train loss:  7.47799456e-01, bound:  3.73512894e-01\n",
      "Epoch: 1093 mean train loss:  7.46896505e-01, bound:  3.73558700e-01\n",
      "Epoch: 1094 mean train loss:  7.45996058e-01, bound:  3.73604357e-01\n",
      "Epoch: 1095 mean train loss:  7.45096326e-01, bound:  3.73649865e-01\n",
      "Epoch: 1096 mean train loss:  7.44198859e-01, bound:  3.73695225e-01\n",
      "Epoch: 1097 mean train loss:  7.43302882e-01, bound:  3.73740464e-01\n",
      "Epoch: 1098 mean train loss:  7.42408991e-01, bound:  3.73785496e-01\n",
      "Epoch: 1099 mean train loss:  7.41516471e-01, bound:  3.73830408e-01\n",
      "Epoch: 1100 mean train loss:  7.40626156e-01, bound:  3.73875171e-01\n",
      "Epoch: 1101 mean train loss:  7.39737213e-01, bound:  3.73919785e-01\n",
      "Epoch: 1102 mean train loss:  7.38850415e-01, bound:  3.73964220e-01\n",
      "Epoch: 1103 mean train loss:  7.37964988e-01, bound:  3.74008507e-01\n",
      "Epoch: 1104 mean train loss:  7.37081647e-01, bound:  3.74052614e-01\n",
      "Epoch: 1105 mean train loss:  7.36200035e-01, bound:  3.74096602e-01\n",
      "Epoch: 1106 mean train loss:  7.35320508e-01, bound:  3.74140441e-01\n",
      "Epoch: 1107 mean train loss:  7.34442592e-01, bound:  3.74184072e-01\n",
      "Epoch: 1108 mean train loss:  7.33566701e-01, bound:  3.74227524e-01\n",
      "Epoch: 1109 mean train loss:  7.32692599e-01, bound:  3.74270827e-01\n",
      "Epoch: 1110 mean train loss:  7.31820524e-01, bound:  3.74313980e-01\n",
      "Epoch: 1111 mean train loss:  7.30949938e-01, bound:  3.74356955e-01\n",
      "Epoch: 1112 mean train loss:  7.30081677e-01, bound:  3.74399781e-01\n",
      "Epoch: 1113 mean train loss:  7.29215026e-01, bound:  3.74442428e-01\n",
      "Epoch: 1114 mean train loss:  7.28350937e-01, bound:  3.74484897e-01\n",
      "Epoch: 1115 mean train loss:  7.27488577e-01, bound:  3.74527156e-01\n",
      "Epoch: 1116 mean train loss:  7.26627946e-01, bound:  3.74569327e-01\n",
      "Epoch: 1117 mean train loss:  7.25769281e-01, bound:  3.74611259e-01\n",
      "Epoch: 1118 mean train loss:  7.24912882e-01, bound:  3.74652982e-01\n",
      "Epoch: 1119 mean train loss:  7.24058628e-01, bound:  3.74694556e-01\n",
      "Epoch: 1120 mean train loss:  7.23206282e-01, bound:  3.74735981e-01\n",
      "Epoch: 1121 mean train loss:  7.22355902e-01, bound:  3.74777168e-01\n",
      "Epoch: 1122 mean train loss:  7.21507192e-01, bound:  3.74818236e-01\n",
      "Epoch: 1123 mean train loss:  7.20661223e-01, bound:  3.74859065e-01\n",
      "Epoch: 1124 mean train loss:  7.19816983e-01, bound:  3.74899745e-01\n",
      "Epoch: 1125 mean train loss:  7.18975127e-01, bound:  3.74940187e-01\n",
      "Epoch: 1126 mean train loss:  7.18134999e-01, bound:  3.74980450e-01\n",
      "Epoch: 1127 mean train loss:  7.17296958e-01, bound:  3.75020564e-01\n",
      "Epoch: 1128 mean train loss:  7.16461360e-01, bound:  3.75060439e-01\n",
      "Epoch: 1129 mean train loss:  7.15627849e-01, bound:  3.75100166e-01\n",
      "Epoch: 1130 mean train loss:  7.14796424e-01, bound:  3.75139654e-01\n",
      "Epoch: 1131 mean train loss:  7.13967204e-01, bound:  3.75178993e-01\n",
      "Epoch: 1132 mean train loss:  7.13140011e-01, bound:  3.75218093e-01\n",
      "Epoch: 1133 mean train loss:  7.12315261e-01, bound:  3.75257015e-01\n",
      "Epoch: 1134 mean train loss:  7.11492479e-01, bound:  3.75295669e-01\n",
      "Epoch: 1135 mean train loss:  7.10671902e-01, bound:  3.75334203e-01\n",
      "Epoch: 1136 mean train loss:  7.09853709e-01, bound:  3.75372529e-01\n",
      "Epoch: 1137 mean train loss:  7.09037423e-01, bound:  3.75410616e-01\n",
      "Epoch: 1138 mean train loss:  7.08223701e-01, bound:  3.75448465e-01\n",
      "Epoch: 1139 mean train loss:  7.07412004e-01, bound:  3.75486135e-01\n",
      "Epoch: 1140 mean train loss:  7.06602752e-01, bound:  3.75523627e-01\n",
      "Epoch: 1141 mean train loss:  7.05795646e-01, bound:  3.75560910e-01\n",
      "Epoch: 1142 mean train loss:  7.04990804e-01, bound:  3.75597894e-01\n",
      "Epoch: 1143 mean train loss:  7.04188585e-01, bound:  3.75634730e-01\n",
      "Epoch: 1144 mean train loss:  7.03388572e-01, bound:  3.75671357e-01\n",
      "Epoch: 1145 mean train loss:  7.02590644e-01, bound:  3.75707775e-01\n",
      "Epoch: 1146 mean train loss:  7.01795340e-01, bound:  3.75743926e-01\n",
      "Epoch: 1147 mean train loss:  7.01001585e-01, bound:  3.75779897e-01\n",
      "Epoch: 1148 mean train loss:  7.00211108e-01, bound:  3.75815600e-01\n",
      "Epoch: 1149 mean train loss:  6.99422598e-01, bound:  3.75851125e-01\n",
      "Epoch: 1150 mean train loss:  6.98636472e-01, bound:  3.75886410e-01\n",
      "Epoch: 1151 mean train loss:  6.97852671e-01, bound:  3.75921488e-01\n",
      "Epoch: 1152 mean train loss:  6.97071075e-01, bound:  3.75956297e-01\n",
      "Epoch: 1153 mean train loss:  6.96292400e-01, bound:  3.75990897e-01\n",
      "Epoch: 1154 mean train loss:  6.95515037e-01, bound:  3.76025259e-01\n",
      "Epoch: 1155 mean train loss:  6.94741309e-01, bound:  3.76059413e-01\n",
      "Epoch: 1156 mean train loss:  6.93969727e-01, bound:  3.76093298e-01\n",
      "Epoch: 1157 mean train loss:  6.93200111e-01, bound:  3.76126975e-01\n",
      "Epoch: 1158 mean train loss:  6.92432940e-01, bound:  3.76160383e-01\n",
      "Epoch: 1159 mean train loss:  6.91668391e-01, bound:  3.76193643e-01\n",
      "Epoch: 1160 mean train loss:  6.90906286e-01, bound:  3.76226515e-01\n",
      "Epoch: 1161 mean train loss:  6.90146506e-01, bound:  3.76259238e-01\n",
      "Epoch: 1162 mean train loss:  6.89389110e-01, bound:  3.76291752e-01\n",
      "Epoch: 1163 mean train loss:  6.88634515e-01, bound:  3.76323998e-01\n",
      "Epoch: 1164 mean train loss:  6.87882483e-01, bound:  3.76356006e-01\n",
      "Epoch: 1165 mean train loss:  6.87132418e-01, bound:  3.76387715e-01\n",
      "Epoch: 1166 mean train loss:  6.86385214e-01, bound:  3.76419216e-01\n",
      "Epoch: 1167 mean train loss:  6.85640454e-01, bound:  3.76450449e-01\n",
      "Epoch: 1168 mean train loss:  6.84897661e-01, bound:  3.76481444e-01\n",
      "Epoch: 1169 mean train loss:  6.84157610e-01, bound:  3.76512229e-01\n",
      "Epoch: 1170 mean train loss:  6.83420122e-01, bound:  3.76542717e-01\n",
      "Epoch: 1171 mean train loss:  6.82685435e-01, bound:  3.76572996e-01\n",
      "Epoch: 1172 mean train loss:  6.81953132e-01, bound:  3.76602978e-01\n",
      "Epoch: 1173 mean train loss:  6.81223571e-01, bound:  3.76632750e-01\n",
      "Epoch: 1174 mean train loss:  6.80495918e-01, bound:  3.76662254e-01\n",
      "Epoch: 1175 mean train loss:  6.79771066e-01, bound:  3.76691461e-01\n",
      "Epoch: 1176 mean train loss:  6.79048777e-01, bound:  3.76720399e-01\n",
      "Epoch: 1177 mean train loss:  6.78329051e-01, bound:  3.76749128e-01\n",
      "Epoch: 1178 mean train loss:  6.77611768e-01, bound:  3.76777589e-01\n",
      "Epoch: 1179 mean train loss:  6.76896572e-01, bound:  3.76805782e-01\n",
      "Epoch: 1180 mean train loss:  6.76184654e-01, bound:  3.76833677e-01\n",
      "Epoch: 1181 mean train loss:  6.75474584e-01, bound:  3.76861334e-01\n",
      "Epoch: 1182 mean train loss:  6.74767852e-01, bound:  3.76888752e-01\n",
      "Epoch: 1183 mean train loss:  6.74063206e-01, bound:  3.76915902e-01\n",
      "Epoch: 1184 mean train loss:  6.73361182e-01, bound:  3.76942784e-01\n",
      "Epoch: 1185 mean train loss:  6.72661781e-01, bound:  3.76969367e-01\n",
      "Epoch: 1186 mean train loss:  6.71964407e-01, bound:  3.76995683e-01\n",
      "Epoch: 1187 mean train loss:  6.71270669e-01, bound:  3.77021760e-01\n",
      "Epoch: 1188 mean train loss:  6.70578659e-01, bound:  3.77047569e-01\n",
      "Epoch: 1189 mean train loss:  6.69889390e-01, bound:  3.77073139e-01\n",
      "Epoch: 1190 mean train loss:  6.69202805e-01, bound:  3.77098352e-01\n",
      "Epoch: 1191 mean train loss:  6.68518424e-01, bound:  3.77123386e-01\n",
      "Epoch: 1192 mean train loss:  6.67836905e-01, bound:  3.77148062e-01\n",
      "Epoch: 1193 mean train loss:  6.67157948e-01, bound:  3.77172500e-01\n",
      "Epoch: 1194 mean train loss:  6.66481555e-01, bound:  3.77196670e-01\n",
      "Epoch: 1195 mean train loss:  6.65807843e-01, bound:  3.77220571e-01\n",
      "Epoch: 1196 mean train loss:  6.65136576e-01, bound:  3.77244174e-01\n",
      "Epoch: 1197 mean train loss:  6.64467871e-01, bound:  3.77267510e-01\n",
      "Epoch: 1198 mean train loss:  6.63801551e-01, bound:  3.77290577e-01\n",
      "Epoch: 1199 mean train loss:  6.63137972e-01, bound:  3.77313375e-01\n",
      "Epoch: 1200 mean train loss:  6.62476838e-01, bound:  3.77335906e-01\n",
      "Epoch: 1201 mean train loss:  6.61818326e-01, bound:  3.77358109e-01\n",
      "Epoch: 1202 mean train loss:  6.61162496e-01, bound:  3.77380073e-01\n",
      "Epoch: 1203 mean train loss:  6.60509408e-01, bound:  3.77401739e-01\n",
      "Epoch: 1204 mean train loss:  6.59858644e-01, bound:  3.77423137e-01\n",
      "Epoch: 1205 mean train loss:  6.59210145e-01, bound:  3.77444237e-01\n",
      "Epoch: 1206 mean train loss:  6.58564806e-01, bound:  3.77465069e-01\n",
      "Epoch: 1207 mean train loss:  6.57921791e-01, bound:  3.77485603e-01\n",
      "Epoch: 1208 mean train loss:  6.57281399e-01, bound:  3.77505898e-01\n",
      "Epoch: 1209 mean train loss:  6.56643391e-01, bound:  3.77525866e-01\n",
      "Epoch: 1210 mean train loss:  6.56007886e-01, bound:  3.77545536e-01\n",
      "Epoch: 1211 mean train loss:  6.55374825e-01, bound:  3.77564967e-01\n",
      "Epoch: 1212 mean train loss:  6.54744864e-01, bound:  3.77584100e-01\n",
      "Epoch: 1213 mean train loss:  6.54117286e-01, bound:  3.77602994e-01\n",
      "Epoch: 1214 mean train loss:  6.53491735e-01, bound:  3.77621502e-01\n",
      "Epoch: 1215 mean train loss:  6.52868927e-01, bound:  3.77639800e-01\n",
      "Epoch: 1216 mean train loss:  6.52248800e-01, bound:  3.77657771e-01\n",
      "Epoch: 1217 mean train loss:  6.51631176e-01, bound:  3.77675563e-01\n",
      "Epoch: 1218 mean train loss:  6.51016533e-01, bound:  3.77692968e-01\n",
      "Epoch: 1219 mean train loss:  6.50403619e-01, bound:  3.77710104e-01\n",
      "Epoch: 1220 mean train loss:  6.49793983e-01, bound:  3.77726942e-01\n",
      "Epoch: 1221 mean train loss:  6.49185956e-01, bound:  3.77743542e-01\n",
      "Epoch: 1222 mean train loss:  6.48581207e-01, bound:  3.77759844e-01\n",
      "Epoch: 1223 mean train loss:  6.47979021e-01, bound:  3.77775818e-01\n",
      "Epoch: 1224 mean train loss:  6.47378743e-01, bound:  3.77791524e-01\n",
      "Epoch: 1225 mean train loss:  6.46781564e-01, bound:  3.77806991e-01\n",
      "Epoch: 1226 mean train loss:  6.46186829e-01, bound:  3.77822191e-01\n",
      "Epoch: 1227 mean train loss:  6.45594120e-01, bound:  3.77837032e-01\n",
      "Epoch: 1228 mean train loss:  6.45004213e-01, bound:  3.77851665e-01\n",
      "Epoch: 1229 mean train loss:  6.44417048e-01, bound:  3.77865940e-01\n",
      "Epoch: 1230 mean train loss:  6.43831432e-01, bound:  3.77879947e-01\n",
      "Epoch: 1231 mean train loss:  6.43249333e-01, bound:  3.77893716e-01\n",
      "Epoch: 1232 mean train loss:  6.42669201e-01, bound:  3.77907157e-01\n",
      "Epoch: 1233 mean train loss:  6.42091870e-01, bound:  3.77920300e-01\n",
      "Epoch: 1234 mean train loss:  6.41516626e-01, bound:  3.77933174e-01\n",
      "Epoch: 1235 mean train loss:  6.40943646e-01, bound:  3.77945811e-01\n",
      "Epoch: 1236 mean train loss:  6.40373826e-01, bound:  3.77958119e-01\n",
      "Epoch: 1237 mean train loss:  6.39805973e-01, bound:  3.77970159e-01\n",
      "Epoch: 1238 mean train loss:  6.39240921e-01, bound:  3.77981931e-01\n",
      "Epoch: 1239 mean train loss:  6.38677955e-01, bound:  3.77993405e-01\n",
      "Epoch: 1240 mean train loss:  6.38117671e-01, bound:  3.78004611e-01\n",
      "Epoch: 1241 mean train loss:  6.37559474e-01, bound:  3.78015608e-01\n",
      "Epoch: 1242 mean train loss:  6.37003541e-01, bound:  3.78026217e-01\n",
      "Epoch: 1243 mean train loss:  6.36450291e-01, bound:  3.78036588e-01\n",
      "Epoch: 1244 mean train loss:  6.35899901e-01, bound:  3.78046691e-01\n",
      "Epoch: 1245 mean train loss:  6.35351419e-01, bound:  3.78056496e-01\n",
      "Epoch: 1246 mean train loss:  6.34805262e-01, bound:  3.78066003e-01\n",
      "Epoch: 1247 mean train loss:  6.34261668e-01, bound:  3.78075272e-01\n",
      "Epoch: 1248 mean train loss:  6.33720219e-01, bound:  3.78084242e-01\n",
      "Epoch: 1249 mean train loss:  6.33181632e-01, bound:  3.78092974e-01\n",
      "Epoch: 1250 mean train loss:  6.32644892e-01, bound:  3.78101408e-01\n",
      "Epoch: 1251 mean train loss:  6.32110596e-01, bound:  3.78109604e-01\n",
      "Epoch: 1252 mean train loss:  6.31578803e-01, bound:  3.78117442e-01\n",
      "Epoch: 1253 mean train loss:  6.31049216e-01, bound:  3.78125072e-01\n",
      "Epoch: 1254 mean train loss:  6.30522013e-01, bound:  3.78132433e-01\n",
      "Epoch: 1255 mean train loss:  6.29997432e-01, bound:  3.78139496e-01\n",
      "Epoch: 1256 mean train loss:  6.29474640e-01, bound:  3.78146321e-01\n",
      "Epoch: 1257 mean train loss:  6.28954113e-01, bound:  3.78152847e-01\n",
      "Epoch: 1258 mean train loss:  6.28436506e-01, bound:  3.78159106e-01\n",
      "Epoch: 1259 mean train loss:  6.27920687e-01, bound:  3.78165126e-01\n",
      "Epoch: 1260 mean train loss:  6.27407193e-01, bound:  3.78170878e-01\n",
      "Epoch: 1261 mean train loss:  6.26895964e-01, bound:  3.78176332e-01\n",
      "Epoch: 1262 mean train loss:  6.26387179e-01, bound:  3.78181547e-01\n",
      "Epoch: 1263 mean train loss:  6.25880778e-01, bound:  3.78186494e-01\n",
      "Epoch: 1264 mean train loss:  6.25376165e-01, bound:  3.78191203e-01\n",
      "Epoch: 1265 mean train loss:  6.24873936e-01, bound:  3.78195614e-01\n",
      "Epoch: 1266 mean train loss:  6.24374270e-01, bound:  3.78199786e-01\n",
      "Epoch: 1267 mean train loss:  6.23876095e-01, bound:  3.78203750e-01\n",
      "Epoch: 1268 mean train loss:  6.23381078e-01, bound:  3.78207386e-01\n",
      "Epoch: 1269 mean train loss:  6.22887850e-01, bound:  3.78210783e-01\n",
      "Epoch: 1270 mean train loss:  6.22396648e-01, bound:  3.78213972e-01\n",
      "Epoch: 1271 mean train loss:  6.21907830e-01, bound:  3.78216863e-01\n",
      "Epoch: 1272 mean train loss:  6.21421039e-01, bound:  3.78219485e-01\n",
      "Epoch: 1273 mean train loss:  6.20936394e-01, bound:  3.78221869e-01\n",
      "Epoch: 1274 mean train loss:  6.20453954e-01, bound:  3.78223985e-01\n",
      "Epoch: 1275 mean train loss:  6.19973719e-01, bound:  3.78225863e-01\n",
      "Epoch: 1276 mean train loss:  6.19495511e-01, bound:  3.78227472e-01\n",
      "Epoch: 1277 mean train loss:  6.19019747e-01, bound:  3.78228843e-01\n",
      "Epoch: 1278 mean train loss:  6.18545890e-01, bound:  3.78230065e-01\n",
      "Epoch: 1279 mean train loss:  6.18074179e-01, bound:  3.78230929e-01\n",
      "Epoch: 1280 mean train loss:  6.17604434e-01, bound:  3.78231615e-01\n",
      "Epoch: 1281 mean train loss:  6.17136896e-01, bound:  3.78232032e-01\n",
      "Epoch: 1282 mean train loss:  6.16671562e-01, bound:  3.78232211e-01\n",
      "Epoch: 1283 mean train loss:  6.16207898e-01, bound:  3.78232092e-01\n",
      "Epoch: 1284 mean train loss:  6.15746975e-01, bound:  3.78231853e-01\n",
      "Epoch: 1285 mean train loss:  6.15287602e-01, bound:  3.78231317e-01\n",
      "Epoch: 1286 mean train loss:  6.14830613e-01, bound:  3.78230572e-01\n",
      "Epoch: 1287 mean train loss:  6.14375353e-01, bound:  3.78229529e-01\n",
      "Epoch: 1288 mean train loss:  6.13922119e-01, bound:  3.78228277e-01\n",
      "Epoch: 1289 mean train loss:  6.13471508e-01, bound:  3.78226817e-01\n",
      "Epoch: 1290 mean train loss:  6.13022327e-01, bound:  3.78225088e-01\n",
      "Epoch: 1291 mean train loss:  6.12575173e-01, bound:  3.78223211e-01\n",
      "Epoch: 1292 mean train loss:  6.12129927e-01, bound:  3.78221065e-01\n",
      "Epoch: 1293 mean train loss:  6.11687124e-01, bound:  3.78218681e-01\n",
      "Epoch: 1294 mean train loss:  6.11245573e-01, bound:  3.78216058e-01\n",
      "Epoch: 1295 mean train loss:  6.10807061e-01, bound:  3.78213257e-01\n",
      "Epoch: 1296 mean train loss:  6.10369384e-01, bound:  3.78210247e-01\n",
      "Epoch: 1297 mean train loss:  6.09934151e-01, bound:  3.78206998e-01\n",
      "Epoch: 1298 mean train loss:  6.09500945e-01, bound:  3.78203511e-01\n",
      "Epoch: 1299 mean train loss:  6.09069347e-01, bound:  3.78199816e-01\n",
      "Epoch: 1300 mean train loss:  6.08639777e-01, bound:  3.78195912e-01\n",
      "Epoch: 1301 mean train loss:  6.08212471e-01, bound:  3.78191859e-01\n",
      "Epoch: 1302 mean train loss:  6.07786536e-01, bound:  3.78187507e-01\n",
      "Epoch: 1303 mean train loss:  6.07362807e-01, bound:  3.78182948e-01\n",
      "Epoch: 1304 mean train loss:  6.06940985e-01, bound:  3.78178239e-01\n",
      "Epoch: 1305 mean train loss:  6.06520951e-01, bound:  3.78173262e-01\n",
      "Epoch: 1306 mean train loss:  6.06102407e-01, bound:  3.78168106e-01\n",
      "Epoch: 1307 mean train loss:  6.05686486e-01, bound:  3.78162712e-01\n",
      "Epoch: 1308 mean train loss:  6.05272114e-01, bound:  3.78157198e-01\n",
      "Epoch: 1309 mean train loss:  6.04858994e-01, bound:  3.78151417e-01\n",
      "Epoch: 1310 mean train loss:  6.04448438e-01, bound:  3.78145486e-01\n",
      "Epoch: 1311 mean train loss:  6.04039311e-01, bound:  3.78139317e-01\n",
      "Epoch: 1312 mean train loss:  6.03632152e-01, bound:  3.78132999e-01\n",
      "Epoch: 1313 mean train loss:  6.03226662e-01, bound:  3.78126472e-01\n",
      "Epoch: 1314 mean train loss:  6.02822900e-01, bound:  3.78119707e-01\n",
      "Epoch: 1315 mean train loss:  6.02420866e-01, bound:  3.78112823e-01\n",
      "Epoch: 1316 mean train loss:  6.02020800e-01, bound:  3.78105670e-01\n",
      "Epoch: 1317 mean train loss:  6.01622462e-01, bound:  3.78098398e-01\n",
      "Epoch: 1318 mean train loss:  6.01225793e-01, bound:  3.78090918e-01\n",
      "Epoch: 1319 mean train loss:  6.00831091e-01, bound:  3.78083229e-01\n",
      "Epoch: 1320 mean train loss:  6.00437641e-01, bound:  3.78075391e-01\n",
      "Epoch: 1321 mean train loss:  6.00046337e-01, bound:  3.78067344e-01\n",
      "Epoch: 1322 mean train loss:  5.99656403e-01, bound:  3.78059179e-01\n",
      "Epoch: 1323 mean train loss:  5.99268556e-01, bound:  3.78050774e-01\n",
      "Epoch: 1324 mean train loss:  5.98881662e-01, bound:  3.78042221e-01\n",
      "Epoch: 1325 mean train loss:  5.98497212e-01, bound:  3.78033459e-01\n",
      "Epoch: 1326 mean train loss:  5.98113835e-01, bound:  3.78024548e-01\n",
      "Epoch: 1327 mean train loss:  5.97732902e-01, bound:  3.78015399e-01\n",
      "Epoch: 1328 mean train loss:  5.97353160e-01, bound:  3.78006220e-01\n",
      "Epoch: 1329 mean train loss:  5.96974611e-01, bound:  3.77996802e-01\n",
      "Epoch: 1330 mean train loss:  5.96598566e-01, bound:  3.77987206e-01\n",
      "Epoch: 1331 mean train loss:  5.96223474e-01, bound:  3.77977431e-01\n",
      "Epoch: 1332 mean train loss:  5.95850587e-01, bound:  3.77967536e-01\n",
      "Epoch: 1333 mean train loss:  5.95478654e-01, bound:  3.77957493e-01\n",
      "Epoch: 1334 mean train loss:  5.95108390e-01, bound:  3.77947271e-01\n",
      "Epoch: 1335 mean train loss:  5.94740391e-01, bound:  3.77936870e-01\n",
      "Epoch: 1336 mean train loss:  5.94373107e-01, bound:  3.77926320e-01\n",
      "Epoch: 1337 mean train loss:  5.94007790e-01, bound:  3.77915621e-01\n",
      "Epoch: 1338 mean train loss:  5.93644023e-01, bound:  3.77904773e-01\n",
      "Epoch: 1339 mean train loss:  5.93281746e-01, bound:  3.77893776e-01\n",
      "Epoch: 1340 mean train loss:  5.92920661e-01, bound:  3.77882570e-01\n",
      "Epoch: 1341 mean train loss:  5.92561722e-01, bound:  3.77871215e-01\n",
      "Epoch: 1342 mean train loss:  5.92203736e-01, bound:  3.77859771e-01\n",
      "Epoch: 1343 mean train loss:  5.91848016e-01, bound:  3.77848178e-01\n",
      "Epoch: 1344 mean train loss:  5.91493547e-01, bound:  3.77836466e-01\n",
      "Epoch: 1345 mean train loss:  5.91140151e-01, bound:  3.77824575e-01\n",
      "Epoch: 1346 mean train loss:  5.90788960e-01, bound:  3.77812564e-01\n",
      "Epoch: 1347 mean train loss:  5.90438366e-01, bound:  3.77800375e-01\n",
      "Epoch: 1348 mean train loss:  5.90089500e-01, bound:  3.77788126e-01\n",
      "Epoch: 1349 mean train loss:  5.89742005e-01, bound:  3.77775669e-01\n",
      "Epoch: 1350 mean train loss:  5.89396119e-01, bound:  3.77763093e-01\n",
      "Epoch: 1351 mean train loss:  5.89051843e-01, bound:  3.77750427e-01\n",
      "Epoch: 1352 mean train loss:  5.88708937e-01, bound:  3.77737612e-01\n",
      "Epoch: 1353 mean train loss:  5.88367283e-01, bound:  3.77724618e-01\n",
      "Epoch: 1354 mean train loss:  5.88027358e-01, bound:  3.77711535e-01\n",
      "Epoch: 1355 mean train loss:  5.87688506e-01, bound:  3.77698302e-01\n",
      "Epoch: 1356 mean train loss:  5.87351084e-01, bound:  3.77684921e-01\n",
      "Epoch: 1357 mean train loss:  5.87014973e-01, bound:  3.77671450e-01\n",
      "Epoch: 1358 mean train loss:  5.86680412e-01, bound:  3.77657831e-01\n",
      "Epoch: 1359 mean train loss:  5.86347401e-01, bound:  3.77644122e-01\n",
      "Epoch: 1360 mean train loss:  5.86015522e-01, bound:  3.77630293e-01\n",
      "Epoch: 1361 mean train loss:  5.85685015e-01, bound:  3.77616316e-01\n",
      "Epoch: 1362 mean train loss:  5.85355818e-01, bound:  3.77602249e-01\n",
      "Epoch: 1363 mean train loss:  5.85027754e-01, bound:  3.77588063e-01\n",
      "Epoch: 1364 mean train loss:  5.84701240e-01, bound:  3.77573758e-01\n",
      "Epoch: 1365 mean train loss:  5.84375799e-01, bound:  3.77559334e-01\n",
      "Epoch: 1366 mean train loss:  5.84052324e-01, bound:  3.77544820e-01\n",
      "Epoch: 1367 mean train loss:  5.83729625e-01, bound:  3.77530158e-01\n",
      "Epoch: 1368 mean train loss:  5.83408296e-01, bound:  3.77515435e-01\n",
      "Epoch: 1369 mean train loss:  5.83088398e-01, bound:  3.77500594e-01\n",
      "Epoch: 1370 mean train loss:  5.82769632e-01, bound:  3.77485603e-01\n",
      "Epoch: 1371 mean train loss:  5.82451999e-01, bound:  3.77470553e-01\n",
      "Epoch: 1372 mean train loss:  5.82135916e-01, bound:  3.77455413e-01\n",
      "Epoch: 1373 mean train loss:  5.81820905e-01, bound:  3.77440125e-01\n",
      "Epoch: 1374 mean train loss:  5.81507206e-01, bound:  3.77424777e-01\n",
      "Epoch: 1375 mean train loss:  5.81194639e-01, bound:  3.77409309e-01\n",
      "Epoch: 1376 mean train loss:  5.80883265e-01, bound:  3.77393752e-01\n",
      "Epoch: 1377 mean train loss:  5.80573380e-01, bound:  3.77378076e-01\n",
      "Epoch: 1378 mean train loss:  5.80264509e-01, bound:  3.77362311e-01\n",
      "Epoch: 1379 mean train loss:  5.79956949e-01, bound:  3.77346456e-01\n",
      "Epoch: 1380 mean train loss:  5.79650521e-01, bound:  3.77330512e-01\n",
      "Epoch: 1381 mean train loss:  5.79345167e-01, bound:  3.77314478e-01\n",
      "Epoch: 1382 mean train loss:  5.79041004e-01, bound:  3.77298415e-01\n",
      "Epoch: 1383 mean train loss:  5.78738153e-01, bound:  3.77282172e-01\n",
      "Epoch: 1384 mean train loss:  5.78436613e-01, bound:  3.77265841e-01\n",
      "Epoch: 1385 mean train loss:  5.78136146e-01, bound:  3.77249479e-01\n",
      "Epoch: 1386 mean train loss:  5.77836514e-01, bound:  3.77232999e-01\n",
      "Epoch: 1387 mean train loss:  5.77538550e-01, bound:  3.77216429e-01\n",
      "Epoch: 1388 mean train loss:  5.77241063e-01, bound:  3.77199799e-01\n",
      "Epoch: 1389 mean train loss:  5.76945186e-01, bound:  3.77183139e-01\n",
      "Epoch: 1390 mean train loss:  5.76650262e-01, bound:  3.77166331e-01\n",
      "Epoch: 1391 mean train loss:  5.76356351e-01, bound:  3.77149463e-01\n",
      "Epoch: 1392 mean train loss:  5.76063633e-01, bound:  3.77132505e-01\n",
      "Epoch: 1393 mean train loss:  5.75772464e-01, bound:  3.77115458e-01\n",
      "Epoch: 1394 mean train loss:  5.75481951e-01, bound:  3.77098352e-01\n",
      "Epoch: 1395 mean train loss:  5.75192332e-01, bound:  3.77081156e-01\n",
      "Epoch: 1396 mean train loss:  5.74904501e-01, bound:  3.77063930e-01\n",
      "Epoch: 1397 mean train loss:  5.74617207e-01, bound:  3.77046585e-01\n",
      "Epoch: 1398 mean train loss:  5.74330688e-01, bound:  3.77029240e-01\n",
      "Epoch: 1399 mean train loss:  5.74045658e-01, bound:  3.77011746e-01\n",
      "Epoch: 1400 mean train loss:  5.73761523e-01, bound:  3.76994193e-01\n",
      "Epoch: 1401 mean train loss:  5.73478639e-01, bound:  3.76976579e-01\n",
      "Epoch: 1402 mean train loss:  5.73196709e-01, bound:  3.76958936e-01\n",
      "Epoch: 1403 mean train loss:  5.72915554e-01, bound:  3.76941204e-01\n",
      "Epoch: 1404 mean train loss:  5.72635889e-01, bound:  3.76923412e-01\n",
      "Epoch: 1405 mean train loss:  5.72356939e-01, bound:  3.76905590e-01\n",
      "Epoch: 1406 mean train loss:  5.72078943e-01, bound:  3.76887619e-01\n",
      "Epoch: 1407 mean train loss:  5.71801782e-01, bound:  3.76869678e-01\n",
      "Epoch: 1408 mean train loss:  5.71526229e-01, bound:  3.76851588e-01\n",
      "Epoch: 1409 mean train loss:  5.71251392e-01, bound:  3.76833498e-01\n",
      "Epoch: 1410 mean train loss:  5.70977688e-01, bound:  3.76815319e-01\n",
      "Epoch: 1411 mean train loss:  5.70704401e-01, bound:  3.76797110e-01\n",
      "Epoch: 1412 mean train loss:  5.70432544e-01, bound:  3.76778841e-01\n",
      "Epoch: 1413 mean train loss:  5.70161402e-01, bound:  3.76760483e-01\n",
      "Epoch: 1414 mean train loss:  5.69891393e-01, bound:  3.76742095e-01\n",
      "Epoch: 1415 mean train loss:  5.69622278e-01, bound:  3.76723677e-01\n",
      "Epoch: 1416 mean train loss:  5.69354057e-01, bound:  3.76705140e-01\n",
      "Epoch: 1417 mean train loss:  5.69087148e-01, bound:  3.76686633e-01\n",
      "Epoch: 1418 mean train loss:  5.68820477e-01, bound:  3.76668036e-01\n",
      "Epoch: 1419 mean train loss:  5.68555236e-01, bound:  3.76649380e-01\n",
      "Epoch: 1420 mean train loss:  5.68291008e-01, bound:  3.76630694e-01\n",
      "Epoch: 1421 mean train loss:  5.68027258e-01, bound:  3.76611948e-01\n",
      "Epoch: 1422 mean train loss:  5.67764640e-01, bound:  3.76593113e-01\n",
      "Epoch: 1423 mean train loss:  5.67502916e-01, bound:  3.76574278e-01\n",
      "Epoch: 1424 mean train loss:  5.67242146e-01, bound:  3.76555473e-01\n",
      "Epoch: 1425 mean train loss:  5.66982031e-01, bound:  3.76536489e-01\n",
      "Epoch: 1426 mean train loss:  5.66722989e-01, bound:  3.76517534e-01\n",
      "Epoch: 1427 mean train loss:  5.66464722e-01, bound:  3.76498550e-01\n",
      "Epoch: 1428 mean train loss:  5.66207528e-01, bound:  3.76479417e-01\n",
      "Epoch: 1429 mean train loss:  5.65951467e-01, bound:  3.76460344e-01\n",
      "Epoch: 1430 mean train loss:  5.65695405e-01, bound:  3.76441211e-01\n",
      "Epoch: 1431 mean train loss:  5.65440893e-01, bound:  3.76422048e-01\n",
      "Epoch: 1432 mean train loss:  5.65187156e-01, bound:  3.76402825e-01\n",
      "Epoch: 1433 mean train loss:  5.64934015e-01, bound:  3.76383543e-01\n",
      "Epoch: 1434 mean train loss:  5.64682126e-01, bound:  3.76364231e-01\n",
      "Epoch: 1435 mean train loss:  5.64430773e-01, bound:  3.76344919e-01\n",
      "Epoch: 1436 mean train loss:  5.64180076e-01, bound:  3.76325548e-01\n",
      "Epoch: 1437 mean train loss:  5.63930094e-01, bound:  3.76306146e-01\n",
      "Epoch: 1438 mean train loss:  5.63681304e-01, bound:  3.76286715e-01\n",
      "Epoch: 1439 mean train loss:  5.63432992e-01, bound:  3.76267225e-01\n",
      "Epoch: 1440 mean train loss:  5.63185930e-01, bound:  3.76247734e-01\n",
      "Epoch: 1441 mean train loss:  5.62939346e-01, bound:  3.76228213e-01\n",
      "Epoch: 1442 mean train loss:  5.62693894e-01, bound:  3.76208633e-01\n",
      "Epoch: 1443 mean train loss:  5.62448800e-01, bound:  3.76189053e-01\n",
      "Epoch: 1444 mean train loss:  5.62204719e-01, bound:  3.76169443e-01\n",
      "Epoch: 1445 mean train loss:  5.61961412e-01, bound:  3.76149744e-01\n",
      "Epoch: 1446 mean train loss:  5.61718583e-01, bound:  3.76130044e-01\n",
      "Epoch: 1447 mean train loss:  5.61476648e-01, bound:  3.76110345e-01\n",
      "Epoch: 1448 mean train loss:  5.61235845e-01, bound:  3.76090616e-01\n",
      "Epoch: 1449 mean train loss:  5.60995877e-01, bound:  3.76070827e-01\n",
      "Epoch: 1450 mean train loss:  5.60756147e-01, bound:  3.76051009e-01\n",
      "Epoch: 1451 mean train loss:  5.60517490e-01, bound:  3.76031220e-01\n",
      "Epoch: 1452 mean train loss:  5.60279131e-01, bound:  3.76011372e-01\n",
      "Epoch: 1453 mean train loss:  5.60041845e-01, bound:  3.75991464e-01\n",
      "Epoch: 1454 mean train loss:  5.59805274e-01, bound:  3.75971586e-01\n",
      "Epoch: 1455 mean train loss:  5.59569597e-01, bound:  3.75951678e-01\n",
      "Epoch: 1456 mean train loss:  5.59334517e-01, bound:  3.75931740e-01\n",
      "Epoch: 1457 mean train loss:  5.59099972e-01, bound:  3.75911802e-01\n",
      "Epoch: 1458 mean train loss:  5.58866322e-01, bound:  3.75891805e-01\n",
      "Epoch: 1459 mean train loss:  5.58633268e-01, bound:  3.75871807e-01\n",
      "Epoch: 1460 mean train loss:  5.58401048e-01, bound:  3.75851780e-01\n",
      "Epoch: 1461 mean train loss:  5.58169186e-01, bound:  3.75831693e-01\n",
      "Epoch: 1462 mean train loss:  5.57938576e-01, bound:  3.75811666e-01\n",
      "Epoch: 1463 mean train loss:  5.57708025e-01, bound:  3.75791579e-01\n",
      "Epoch: 1464 mean train loss:  5.57478666e-01, bound:  3.75771493e-01\n",
      "Epoch: 1465 mean train loss:  5.57249904e-01, bound:  3.75751346e-01\n",
      "Epoch: 1466 mean train loss:  5.57021797e-01, bound:  3.75731200e-01\n",
      "Epoch: 1467 mean train loss:  5.56794226e-01, bound:  3.75711054e-01\n",
      "Epoch: 1468 mean train loss:  5.56567252e-01, bound:  3.75690937e-01\n",
      "Epoch: 1469 mean train loss:  5.56341171e-01, bound:  3.75670731e-01\n",
      "Epoch: 1470 mean train loss:  5.56115687e-01, bound:  3.75650525e-01\n",
      "Epoch: 1471 mean train loss:  5.55890739e-01, bound:  3.75630289e-01\n",
      "Epoch: 1472 mean train loss:  5.55666566e-01, bound:  3.75610113e-01\n",
      "Epoch: 1473 mean train loss:  5.55442989e-01, bound:  3.75589877e-01\n",
      "Epoch: 1474 mean train loss:  5.55220008e-01, bound:  3.75569552e-01\n",
      "Epoch: 1475 mean train loss:  5.54997802e-01, bound:  3.75549316e-01\n",
      "Epoch: 1476 mean train loss:  5.54776073e-01, bound:  3.75529110e-01\n",
      "Epoch: 1477 mean train loss:  5.54555058e-01, bound:  3.75508785e-01\n",
      "Epoch: 1478 mean train loss:  5.54334700e-01, bound:  3.75488520e-01\n",
      "Epoch: 1479 mean train loss:  5.54114819e-01, bound:  3.75468194e-01\n",
      "Epoch: 1480 mean train loss:  5.53895473e-01, bound:  3.75447869e-01\n",
      "Epoch: 1481 mean train loss:  5.53676963e-01, bound:  3.75427544e-01\n",
      "Epoch: 1482 mean train loss:  5.53459287e-01, bound:  3.75407249e-01\n",
      "Epoch: 1483 mean train loss:  5.53241670e-01, bound:  3.75386894e-01\n",
      "Epoch: 1484 mean train loss:  5.53024888e-01, bound:  3.75366539e-01\n",
      "Epoch: 1485 mean train loss:  5.52808583e-01, bound:  3.75346214e-01\n",
      "Epoch: 1486 mean train loss:  5.52593112e-01, bound:  3.75325799e-01\n",
      "Epoch: 1487 mean train loss:  5.52378118e-01, bound:  3.75305474e-01\n",
      "Epoch: 1488 mean train loss:  5.52163363e-01, bound:  3.75285089e-01\n",
      "Epoch: 1489 mean train loss:  5.51949859e-01, bound:  3.75264704e-01\n",
      "Epoch: 1490 mean train loss:  5.51737010e-01, bound:  3.75244319e-01\n",
      "Epoch: 1491 mean train loss:  5.51524043e-01, bound:  3.75223905e-01\n",
      "Epoch: 1492 mean train loss:  5.51311910e-01, bound:  3.75203520e-01\n",
      "Epoch: 1493 mean train loss:  5.51100492e-01, bound:  3.75183105e-01\n",
      "Epoch: 1494 mean train loss:  5.50889134e-01, bound:  3.75162750e-01\n",
      "Epoch: 1495 mean train loss:  5.50678909e-01, bound:  3.75142276e-01\n",
      "Epoch: 1496 mean train loss:  5.50468922e-01, bound:  3.75121951e-01\n",
      "Epoch: 1497 mean train loss:  5.50259471e-01, bound:  3.75101477e-01\n",
      "Epoch: 1498 mean train loss:  5.50050855e-01, bound:  3.75081062e-01\n",
      "Epoch: 1499 mean train loss:  5.49842536e-01, bound:  3.75060618e-01\n",
      "Epoch: 1500 mean train loss:  5.49634874e-01, bound:  3.75040203e-01\n",
      "Epoch: 1501 mean train loss:  5.49427569e-01, bound:  3.75019819e-01\n",
      "Epoch: 1502 mean train loss:  5.49220800e-01, bound:  3.74999374e-01\n",
      "Epoch: 1503 mean train loss:  5.49014807e-01, bound:  3.74978960e-01\n",
      "Epoch: 1504 mean train loss:  5.48809052e-01, bound:  3.74958545e-01\n",
      "Epoch: 1505 mean train loss:  5.48604131e-01, bound:  3.74938101e-01\n",
      "Epoch: 1506 mean train loss:  5.48399627e-01, bound:  3.74917746e-01\n",
      "Epoch: 1507 mean train loss:  5.48195481e-01, bound:  3.74897301e-01\n",
      "Epoch: 1508 mean train loss:  5.47991812e-01, bound:  3.74876887e-01\n",
      "Epoch: 1509 mean train loss:  5.47788799e-01, bound:  3.74856442e-01\n",
      "Epoch: 1510 mean train loss:  5.47586322e-01, bound:  3.74836057e-01\n",
      "Epoch: 1511 mean train loss:  5.47384143e-01, bound:  3.74815643e-01\n",
      "Epoch: 1512 mean train loss:  5.47182500e-01, bound:  3.74795228e-01\n",
      "Epoch: 1513 mean train loss:  5.46981454e-01, bound:  3.74774814e-01\n",
      "Epoch: 1514 mean train loss:  5.46781123e-01, bound:  3.74754399e-01\n",
      "Epoch: 1515 mean train loss:  5.46580791e-01, bound:  3.74734014e-01\n",
      "Epoch: 1516 mean train loss:  5.46380997e-01, bound:  3.74713659e-01\n",
      "Epoch: 1517 mean train loss:  5.46181977e-01, bound:  3.74693245e-01\n",
      "Epoch: 1518 mean train loss:  5.45983374e-01, bound:  3.74672890e-01\n",
      "Epoch: 1519 mean train loss:  5.45785129e-01, bound:  3.74652535e-01\n",
      "Epoch: 1520 mean train loss:  5.45587480e-01, bound:  3.74632120e-01\n",
      "Epoch: 1521 mean train loss:  5.45390010e-01, bound:  3.74611706e-01\n",
      "Epoch: 1522 mean train loss:  5.45193493e-01, bound:  3.74591380e-01\n",
      "Epoch: 1523 mean train loss:  5.44997036e-01, bound:  3.74571025e-01\n",
      "Epoch: 1524 mean train loss:  5.44801533e-01, bound:  3.74550641e-01\n",
      "Epoch: 1525 mean train loss:  5.44605553e-01, bound:  3.74530315e-01\n",
      "Epoch: 1526 mean train loss:  5.44410765e-01, bound:  3.74509960e-01\n",
      "Epoch: 1527 mean train loss:  5.44216275e-01, bound:  3.74489665e-01\n",
      "Epoch: 1528 mean train loss:  5.44022143e-01, bound:  3.74469340e-01\n",
      "Epoch: 1529 mean train loss:  5.43828428e-01, bound:  3.74449015e-01\n",
      "Epoch: 1530 mean train loss:  5.43635428e-01, bound:  3.74428749e-01\n",
      "Epoch: 1531 mean train loss:  5.43442547e-01, bound:  3.74408424e-01\n",
      "Epoch: 1532 mean train loss:  5.43250322e-01, bound:  3.74388158e-01\n",
      "Epoch: 1533 mean train loss:  5.43058395e-01, bound:  3.74367863e-01\n",
      "Epoch: 1534 mean train loss:  5.42867005e-01, bound:  3.74347568e-01\n",
      "Epoch: 1535 mean train loss:  5.42676151e-01, bound:  3.74327272e-01\n",
      "Epoch: 1536 mean train loss:  5.42485416e-01, bound:  3.74307066e-01\n",
      "Epoch: 1537 mean train loss:  5.42295098e-01, bound:  3.74286801e-01\n",
      "Epoch: 1538 mean train loss:  5.42105377e-01, bound:  3.74266535e-01\n",
      "Epoch: 1539 mean train loss:  5.41915953e-01, bound:  3.74246359e-01\n",
      "Epoch: 1540 mean train loss:  5.41726887e-01, bound:  3.74226153e-01\n",
      "Epoch: 1541 mean train loss:  5.41538537e-01, bound:  3.74205917e-01\n",
      "Epoch: 1542 mean train loss:  5.41350484e-01, bound:  3.74185771e-01\n",
      "Epoch: 1543 mean train loss:  5.41162491e-01, bound:  3.74165565e-01\n",
      "Epoch: 1544 mean train loss:  5.40975273e-01, bound:  3.74145389e-01\n",
      "Epoch: 1545 mean train loss:  5.40788412e-01, bound:  3.74125242e-01\n",
      "Epoch: 1546 mean train loss:  5.40601552e-01, bound:  3.74105096e-01\n",
      "Epoch: 1547 mean train loss:  5.40415883e-01, bound:  3.74084949e-01\n",
      "Epoch: 1548 mean train loss:  5.40229857e-01, bound:  3.74064863e-01\n",
      "Epoch: 1549 mean train loss:  5.40044546e-01, bound:  3.74044716e-01\n",
      "Epoch: 1550 mean train loss:  5.39859474e-01, bound:  3.74024600e-01\n",
      "Epoch: 1551 mean train loss:  5.39674759e-01, bound:  3.74004483e-01\n",
      "Epoch: 1552 mean train loss:  5.39490521e-01, bound:  3.73984486e-01\n",
      "Epoch: 1553 mean train loss:  5.39306700e-01, bound:  3.73964369e-01\n",
      "Epoch: 1554 mean train loss:  5.39123237e-01, bound:  3.73944312e-01\n",
      "Epoch: 1555 mean train loss:  5.38940310e-01, bound:  3.73924315e-01\n",
      "Epoch: 1556 mean train loss:  5.38757503e-01, bound:  3.73904288e-01\n",
      "Epoch: 1557 mean train loss:  5.38575053e-01, bound:  3.73884261e-01\n",
      "Epoch: 1558 mean train loss:  5.38392901e-01, bound:  3.73864323e-01\n",
      "Epoch: 1559 mean train loss:  5.38211405e-01, bound:  3.73844326e-01\n",
      "Epoch: 1560 mean train loss:  5.38030148e-01, bound:  3.73824358e-01\n",
      "Epoch: 1561 mean train loss:  5.37849247e-01, bound:  3.73804450e-01\n",
      "Epoch: 1562 mean train loss:  5.37668705e-01, bound:  3.73784482e-01\n",
      "Epoch: 1563 mean train loss:  5.37488520e-01, bound:  3.73764575e-01\n",
      "Epoch: 1564 mean train loss:  5.37308574e-01, bound:  3.73744667e-01\n",
      "Epoch: 1565 mean train loss:  5.37128985e-01, bound:  3.73724788e-01\n",
      "Epoch: 1566 mean train loss:  5.36949873e-01, bound:  3.73704910e-01\n",
      "Epoch: 1567 mean train loss:  5.36771119e-01, bound:  3.73685032e-01\n",
      "Epoch: 1568 mean train loss:  5.36592364e-01, bound:  3.73665273e-01\n",
      "Epoch: 1569 mean train loss:  5.36414683e-01, bound:  3.73645425e-01\n",
      "Epoch: 1570 mean train loss:  5.36236465e-01, bound:  3.73625606e-01\n",
      "Epoch: 1571 mean train loss:  5.36059201e-01, bound:  3.73605818e-01\n",
      "Epoch: 1572 mean train loss:  5.35882056e-01, bound:  3.73586059e-01\n",
      "Epoch: 1573 mean train loss:  5.35705209e-01, bound:  3.73566270e-01\n",
      "Epoch: 1574 mean train loss:  5.35528719e-01, bound:  3.73546511e-01\n",
      "Epoch: 1575 mean train loss:  5.35352767e-01, bound:  3.73526752e-01\n",
      "Epoch: 1576 mean train loss:  5.35176635e-01, bound:  3.73507112e-01\n",
      "Epoch: 1577 mean train loss:  5.35001218e-01, bound:  3.73487383e-01\n",
      "Epoch: 1578 mean train loss:  5.34825981e-01, bound:  3.73467743e-01\n",
      "Epoch: 1579 mean train loss:  5.34651220e-01, bound:  3.73448044e-01\n",
      "Epoch: 1580 mean train loss:  5.34476817e-01, bound:  3.73428434e-01\n",
      "Epoch: 1581 mean train loss:  5.34302354e-01, bound:  3.73408794e-01\n",
      "Epoch: 1582 mean train loss:  5.34128666e-01, bound:  3.73389184e-01\n",
      "Epoch: 1583 mean train loss:  5.33954620e-01, bound:  3.73369575e-01\n",
      "Epoch: 1584 mean train loss:  5.33781528e-01, bound:  3.73349994e-01\n",
      "Epoch: 1585 mean train loss:  5.33608496e-01, bound:  3.73330444e-01\n",
      "Epoch: 1586 mean train loss:  5.33435881e-01, bound:  3.73310983e-01\n",
      "Epoch: 1587 mean train loss:  5.33263445e-01, bound:  3.73291433e-01\n",
      "Epoch: 1588 mean train loss:  5.33091366e-01, bound:  3.73271912e-01\n",
      "Epoch: 1589 mean train loss:  5.32919586e-01, bound:  3.73252451e-01\n",
      "Epoch: 1590 mean train loss:  5.32747984e-01, bound:  3.73233020e-01\n",
      "Epoch: 1591 mean train loss:  5.32576978e-01, bound:  3.73213559e-01\n",
      "Epoch: 1592 mean train loss:  5.32405972e-01, bound:  3.73194158e-01\n",
      "Epoch: 1593 mean train loss:  5.32235205e-01, bound:  3.73174787e-01\n",
      "Epoch: 1594 mean train loss:  5.32065213e-01, bound:  3.73155385e-01\n",
      "Epoch: 1595 mean train loss:  5.31894982e-01, bound:  3.73136014e-01\n",
      "Epoch: 1596 mean train loss:  5.31725168e-01, bound:  3.73116672e-01\n",
      "Epoch: 1597 mean train loss:  5.31555831e-01, bound:  3.73097360e-01\n",
      "Epoch: 1598 mean train loss:  5.31386554e-01, bound:  3.73078018e-01\n",
      "Epoch: 1599 mean train loss:  5.31217575e-01, bound:  3.73058736e-01\n",
      "Epoch: 1600 mean train loss:  5.31049073e-01, bound:  3.73039454e-01\n",
      "Epoch: 1601 mean train loss:  5.30880511e-01, bound:  3.73020262e-01\n",
      "Epoch: 1602 mean train loss:  5.30712366e-01, bound:  3.73001069e-01\n",
      "Epoch: 1603 mean train loss:  5.30544579e-01, bound:  3.72981787e-01\n",
      "Epoch: 1604 mean train loss:  5.30377150e-01, bound:  3.72962683e-01\n",
      "Epoch: 1605 mean train loss:  5.30210078e-01, bound:  3.72943491e-01\n",
      "Epoch: 1606 mean train loss:  5.30043006e-01, bound:  3.72924298e-01\n",
      "Epoch: 1607 mean train loss:  5.29876292e-01, bound:  3.72905225e-01\n",
      "Epoch: 1608 mean train loss:  5.29709637e-01, bound:  3.72886091e-01\n",
      "Epoch: 1609 mean train loss:  5.29543579e-01, bound:  3.72867048e-01\n",
      "Epoch: 1610 mean train loss:  5.29377639e-01, bound:  3.72847974e-01\n",
      "Epoch: 1611 mean train loss:  5.29211760e-01, bound:  3.72828871e-01\n",
      "Epoch: 1612 mean train loss:  5.29046178e-01, bound:  3.72809887e-01\n",
      "Epoch: 1613 mean train loss:  5.28881252e-01, bound:  3.72790903e-01\n",
      "Epoch: 1614 mean train loss:  5.28716266e-01, bound:  3.72771949e-01\n",
      "Epoch: 1615 mean train loss:  5.28551817e-01, bound:  3.72752964e-01\n",
      "Epoch: 1616 mean train loss:  5.28387308e-01, bound:  3.72734070e-01\n",
      "Epoch: 1617 mean train loss:  5.28223395e-01, bound:  3.72715116e-01\n",
      "Epoch: 1618 mean train loss:  5.28059423e-01, bound:  3.72696251e-01\n",
      "Epoch: 1619 mean train loss:  5.27895749e-01, bound:  3.72677356e-01\n",
      "Epoch: 1620 mean train loss:  5.27732313e-01, bound:  3.72658521e-01\n",
      "Epoch: 1621 mean train loss:  5.27569294e-01, bound:  3.72639656e-01\n",
      "Epoch: 1622 mean train loss:  5.27406156e-01, bound:  3.72620910e-01\n",
      "Epoch: 1623 mean train loss:  5.27243912e-01, bound:  3.72602105e-01\n",
      "Epoch: 1624 mean train loss:  5.27081192e-01, bound:  3.72583359e-01\n",
      "Epoch: 1625 mean train loss:  5.26918888e-01, bound:  3.72564584e-01\n",
      "Epoch: 1626 mean train loss:  5.26757121e-01, bound:  3.72545868e-01\n",
      "Epoch: 1627 mean train loss:  5.26595294e-01, bound:  3.72527152e-01\n",
      "Epoch: 1628 mean train loss:  5.26433766e-01, bound:  3.72508526e-01\n",
      "Epoch: 1629 mean train loss:  5.26272655e-01, bound:  3.72489840e-01\n",
      "Epoch: 1630 mean train loss:  5.26111603e-01, bound:  3.72471213e-01\n",
      "Epoch: 1631 mean train loss:  5.25950670e-01, bound:  3.72452587e-01\n",
      "Epoch: 1632 mean train loss:  5.25790155e-01, bound:  3.72433990e-01\n",
      "Epoch: 1633 mean train loss:  5.25629818e-01, bound:  3.72415423e-01\n",
      "Epoch: 1634 mean train loss:  5.25469720e-01, bound:  3.72396886e-01\n",
      "Epoch: 1635 mean train loss:  5.25309980e-01, bound:  3.72378379e-01\n",
      "Epoch: 1636 mean train loss:  5.25150239e-01, bound:  3.72359902e-01\n",
      "Epoch: 1637 mean train loss:  5.24990618e-01, bound:  3.72341454e-01\n",
      "Epoch: 1638 mean train loss:  5.24831474e-01, bound:  3.72322977e-01\n",
      "Epoch: 1639 mean train loss:  5.24672449e-01, bound:  3.72304559e-01\n",
      "Epoch: 1640 mean train loss:  5.24513841e-01, bound:  3.72286141e-01\n",
      "Epoch: 1641 mean train loss:  5.24354994e-01, bound:  3.72267783e-01\n",
      "Epoch: 1642 mean train loss:  5.24196804e-01, bound:  3.72249395e-01\n",
      "Epoch: 1643 mean train loss:  5.24038851e-01, bound:  3.72231036e-01\n",
      "Epoch: 1644 mean train loss:  5.23880720e-01, bound:  3.72212708e-01\n",
      "Epoch: 1645 mean train loss:  5.23723304e-01, bound:  3.72194409e-01\n",
      "Epoch: 1646 mean train loss:  5.23565650e-01, bound:  3.72176200e-01\n",
      "Epoch: 1647 mean train loss:  5.23408353e-01, bound:  3.72157931e-01\n",
      "Epoch: 1648 mean train loss:  5.23251116e-01, bound:  3.72139722e-01\n",
      "Epoch: 1649 mean train loss:  5.23094356e-01, bound:  3.72121483e-01\n",
      "Epoch: 1650 mean train loss:  5.22937477e-01, bound:  3.72103304e-01\n",
      "Epoch: 1651 mean train loss:  5.22781193e-01, bound:  3.72085154e-01\n",
      "Epoch: 1652 mean train loss:  5.22624850e-01, bound:  3.72067004e-01\n",
      "Epoch: 1653 mean train loss:  5.22468984e-01, bound:  3.72048914e-01\n",
      "Epoch: 1654 mean train loss:  5.22312999e-01, bound:  3.72030795e-01\n",
      "Epoch: 1655 mean train loss:  5.22157371e-01, bound:  3.72012705e-01\n",
      "Epoch: 1656 mean train loss:  5.22002101e-01, bound:  3.71994674e-01\n",
      "Epoch: 1657 mean train loss:  5.21846652e-01, bound:  3.71976703e-01\n",
      "Epoch: 1658 mean train loss:  5.21691501e-01, bound:  3.71958673e-01\n",
      "Epoch: 1659 mean train loss:  5.21536648e-01, bound:  3.71940732e-01\n",
      "Epoch: 1660 mean train loss:  5.21382391e-01, bound:  3.71922791e-01\n",
      "Epoch: 1661 mean train loss:  5.21227837e-01, bound:  3.71904820e-01\n",
      "Epoch: 1662 mean train loss:  5.21073341e-01, bound:  3.71886969e-01\n",
      "Epoch: 1663 mean train loss:  5.20919442e-01, bound:  3.71869057e-01\n",
      "Epoch: 1664 mean train loss:  5.20765662e-01, bound:  3.71851176e-01\n",
      "Epoch: 1665 mean train loss:  5.20611703e-01, bound:  3.71833324e-01\n",
      "Epoch: 1666 mean train loss:  5.20458221e-01, bound:  3.71815532e-01\n",
      "Epoch: 1667 mean train loss:  5.20304918e-01, bound:  3.71797740e-01\n",
      "Epoch: 1668 mean train loss:  5.20151794e-01, bound:  3.71779978e-01\n",
      "Epoch: 1669 mean train loss:  5.19998968e-01, bound:  3.71762246e-01\n",
      "Epoch: 1670 mean train loss:  5.19846439e-01, bound:  3.71744484e-01\n",
      "Epoch: 1671 mean train loss:  5.19693553e-01, bound:  3.71726781e-01\n",
      "Epoch: 1672 mean train loss:  5.19541144e-01, bound:  3.71709168e-01\n",
      "Epoch: 1673 mean train loss:  5.19388855e-01, bound:  3.71691525e-01\n",
      "Epoch: 1674 mean train loss:  5.19236982e-01, bound:  3.71673882e-01\n",
      "Epoch: 1675 mean train loss:  5.19085169e-01, bound:  3.71656239e-01\n",
      "Epoch: 1676 mean train loss:  5.18933296e-01, bound:  3.71638656e-01\n",
      "Epoch: 1677 mean train loss:  5.18782079e-01, bound:  3.71621132e-01\n",
      "Epoch: 1678 mean train loss:  5.18630564e-01, bound:  3.71603578e-01\n",
      "Epoch: 1679 mean train loss:  5.18479645e-01, bound:  3.71586084e-01\n",
      "Epoch: 1680 mean train loss:  5.18328488e-01, bound:  3.71568590e-01\n",
      "Epoch: 1681 mean train loss:  5.18177867e-01, bound:  3.71551126e-01\n",
      "Epoch: 1682 mean train loss:  5.18027365e-01, bound:  3.71533722e-01\n",
      "Epoch: 1683 mean train loss:  5.17876923e-01, bound:  3.71516317e-01\n",
      "Epoch: 1684 mean train loss:  5.17726600e-01, bound:  3.71498913e-01\n",
      "Epoch: 1685 mean train loss:  5.17576635e-01, bound:  3.71481538e-01\n",
      "Epoch: 1686 mean train loss:  5.17426431e-01, bound:  3.71464163e-01\n",
      "Epoch: 1687 mean train loss:  5.17276824e-01, bound:  3.71446878e-01\n",
      "Epoch: 1688 mean train loss:  5.17127097e-01, bound:  3.71429563e-01\n",
      "Epoch: 1689 mean train loss:  5.16977549e-01, bound:  3.71412247e-01\n",
      "Epoch: 1690 mean train loss:  5.16828597e-01, bound:  3.71395022e-01\n",
      "Epoch: 1691 mean train loss:  5.16679227e-01, bound:  3.71377736e-01\n",
      "Epoch: 1692 mean train loss:  5.16530454e-01, bound:  3.71360570e-01\n",
      "Epoch: 1693 mean train loss:  5.16381741e-01, bound:  3.71343404e-01\n",
      "Epoch: 1694 mean train loss:  5.16233087e-01, bound:  3.71326208e-01\n",
      "Epoch: 1695 mean train loss:  5.16084969e-01, bound:  3.71309072e-01\n",
      "Epoch: 1696 mean train loss:  5.15936673e-01, bound:  3.71291965e-01\n",
      "Epoch: 1697 mean train loss:  5.15788734e-01, bound:  3.71274889e-01\n",
      "Epoch: 1698 mean train loss:  5.15640736e-01, bound:  3.71257812e-01\n",
      "Epoch: 1699 mean train loss:  5.15492737e-01, bound:  3.71240765e-01\n",
      "Epoch: 1700 mean train loss:  5.15344858e-01, bound:  3.71223718e-01\n",
      "Epoch: 1701 mean train loss:  5.15197515e-01, bound:  3.71206731e-01\n",
      "Epoch: 1702 mean train loss:  5.15050232e-01, bound:  3.71189743e-01\n",
      "Epoch: 1703 mean train loss:  5.14903009e-01, bound:  3.71172786e-01\n",
      "Epoch: 1704 mean train loss:  5.14756382e-01, bound:  3.71155888e-01\n",
      "Epoch: 1705 mean train loss:  5.14609396e-01, bound:  3.71138930e-01\n",
      "Epoch: 1706 mean train loss:  5.14462709e-01, bound:  3.71122032e-01\n",
      "Epoch: 1707 mean train loss:  5.14316082e-01, bound:  3.71105194e-01\n",
      "Epoch: 1708 mean train loss:  5.14169812e-01, bound:  3.71088326e-01\n",
      "Epoch: 1709 mean train loss:  5.14023840e-01, bound:  3.71071577e-01\n",
      "Epoch: 1710 mean train loss:  5.13877630e-01, bound:  3.71054739e-01\n",
      "Epoch: 1711 mean train loss:  5.13731837e-01, bound:  3.71037960e-01\n",
      "Epoch: 1712 mean train loss:  5.13586044e-01, bound:  3.71021211e-01\n",
      "Epoch: 1713 mean train loss:  5.13440371e-01, bound:  3.71004522e-01\n",
      "Epoch: 1714 mean train loss:  5.13294756e-01, bound:  3.70987803e-01\n",
      "Epoch: 1715 mean train loss:  5.13149321e-01, bound:  3.70971113e-01\n",
      "Epoch: 1716 mean train loss:  5.13004482e-01, bound:  3.70954424e-01\n",
      "Epoch: 1717 mean train loss:  5.12859583e-01, bound:  3.70937824e-01\n",
      "Epoch: 1718 mean train loss:  5.12714565e-01, bound:  3.70921224e-01\n",
      "Epoch: 1719 mean train loss:  5.12569785e-01, bound:  3.70904565e-01\n",
      "Epoch: 1720 mean train loss:  5.12425601e-01, bound:  3.70887995e-01\n",
      "Epoch: 1721 mean train loss:  5.12281120e-01, bound:  3.70871484e-01\n",
      "Epoch: 1722 mean train loss:  5.12136579e-01, bound:  3.70854914e-01\n",
      "Epoch: 1723 mean train loss:  5.11992574e-01, bound:  3.70838434e-01\n",
      "Epoch: 1724 mean train loss:  5.11848509e-01, bound:  3.70821953e-01\n",
      "Epoch: 1725 mean train loss:  5.11704803e-01, bound:  3.70805562e-01\n",
      "Epoch: 1726 mean train loss:  5.11560977e-01, bound:  3.70789111e-01\n",
      "Epoch: 1727 mean train loss:  5.11417329e-01, bound:  3.70772690e-01\n",
      "Epoch: 1728 mean train loss:  5.11274040e-01, bound:  3.70756298e-01\n",
      "Epoch: 1729 mean train loss:  5.11130750e-01, bound:  3.70739937e-01\n",
      "Epoch: 1730 mean train loss:  5.10987639e-01, bound:  3.70723605e-01\n",
      "Epoch: 1731 mean train loss:  5.10844827e-01, bound:  3.70707244e-01\n",
      "Epoch: 1732 mean train loss:  5.10702133e-01, bound:  3.70690942e-01\n",
      "Epoch: 1733 mean train loss:  5.10559261e-01, bound:  3.70674670e-01\n",
      "Epoch: 1734 mean train loss:  5.10416687e-01, bound:  3.70658368e-01\n",
      "Epoch: 1735 mean train loss:  5.10273933e-01, bound:  3.70642185e-01\n",
      "Epoch: 1736 mean train loss:  5.10131896e-01, bound:  3.70625973e-01\n",
      "Epoch: 1737 mean train loss:  5.09989738e-01, bound:  3.70609760e-01\n",
      "Epoch: 1738 mean train loss:  5.09847760e-01, bound:  3.70593637e-01\n",
      "Epoch: 1739 mean train loss:  5.09706020e-01, bound:  3.70577484e-01\n",
      "Epoch: 1740 mean train loss:  5.09564281e-01, bound:  3.70561391e-01\n",
      "Epoch: 1741 mean train loss:  5.09422719e-01, bound:  3.70545268e-01\n",
      "Epoch: 1742 mean train loss:  5.09281099e-01, bound:  3.70529145e-01\n",
      "Epoch: 1743 mean train loss:  5.09139895e-01, bound:  3.70513141e-01\n",
      "Epoch: 1744 mean train loss:  5.08998513e-01, bound:  3.70497108e-01\n",
      "Epoch: 1745 mean train loss:  5.08857608e-01, bound:  3.70481044e-01\n",
      "Epoch: 1746 mean train loss:  5.08716583e-01, bound:  3.70465070e-01\n",
      "Epoch: 1747 mean train loss:  5.08575976e-01, bound:  3.70449126e-01\n",
      "Epoch: 1748 mean train loss:  5.08435488e-01, bound:  3.70433122e-01\n",
      "Epoch: 1749 mean train loss:  5.08294523e-01, bound:  3.70417237e-01\n",
      "Epoch: 1750 mean train loss:  5.08154511e-01, bound:  3.70401323e-01\n",
      "Epoch: 1751 mean train loss:  5.08014143e-01, bound:  3.70385438e-01\n",
      "Epoch: 1752 mean train loss:  5.07873952e-01, bound:  3.70369583e-01\n",
      "Epoch: 1753 mean train loss:  5.07734001e-01, bound:  3.70353758e-01\n",
      "Epoch: 1754 mean train loss:  5.07593870e-01, bound:  3.70337903e-01\n",
      "Epoch: 1755 mean train loss:  5.07454276e-01, bound:  3.70322138e-01\n",
      "Epoch: 1756 mean train loss:  5.07314622e-01, bound:  3.70306343e-01\n",
      "Epoch: 1757 mean train loss:  5.07175326e-01, bound:  3.70290607e-01\n",
      "Epoch: 1758 mean train loss:  5.07035971e-01, bound:  3.70274872e-01\n",
      "Epoch: 1759 mean train loss:  5.06896734e-01, bound:  3.70259166e-01\n",
      "Epoch: 1760 mean train loss:  5.06757557e-01, bound:  3.70243460e-01\n",
      "Epoch: 1761 mean train loss:  5.06618619e-01, bound:  3.70227754e-01\n",
      "Epoch: 1762 mean train loss:  5.06479621e-01, bound:  3.70212168e-01\n",
      "Epoch: 1763 mean train loss:  5.06341040e-01, bound:  3.70196521e-01\n",
      "Epoch: 1764 mean train loss:  5.06202579e-01, bound:  3.70180875e-01\n",
      "Epoch: 1765 mean train loss:  5.06063819e-01, bound:  3.70165318e-01\n",
      "Epoch: 1766 mean train loss:  5.05926013e-01, bound:  3.70149761e-01\n",
      "Epoch: 1767 mean train loss:  5.05787432e-01, bound:  3.70134234e-01\n",
      "Epoch: 1768 mean train loss:  5.05649388e-01, bound:  3.70118678e-01\n",
      "Epoch: 1769 mean train loss:  5.05511224e-01, bound:  3.70103210e-01\n",
      "Epoch: 1770 mean train loss:  5.05373418e-01, bound:  3.70087653e-01\n",
      "Epoch: 1771 mean train loss:  5.05235791e-01, bound:  3.70072246e-01\n",
      "Epoch: 1772 mean train loss:  5.05098104e-01, bound:  3.70056808e-01\n",
      "Epoch: 1773 mean train loss:  5.04960656e-01, bound:  3.70041370e-01\n",
      "Epoch: 1774 mean train loss:  5.04823506e-01, bound:  3.70025933e-01\n",
      "Epoch: 1775 mean train loss:  5.04686296e-01, bound:  3.70010585e-01\n",
      "Epoch: 1776 mean train loss:  5.04549146e-01, bound:  3.69995207e-01\n",
      "Epoch: 1777 mean train loss:  5.04411995e-01, bound:  3.69979888e-01\n",
      "Epoch: 1778 mean train loss:  5.04275143e-01, bound:  3.69964570e-01\n",
      "Epoch: 1779 mean train loss:  5.04138529e-01, bound:  3.69949251e-01\n",
      "Epoch: 1780 mean train loss:  5.04001796e-01, bound:  3.69933993e-01\n",
      "Epoch: 1781 mean train loss:  5.03865421e-01, bound:  3.69918764e-01\n",
      "Epoch: 1782 mean train loss:  5.03728926e-01, bound:  3.69903475e-01\n",
      "Epoch: 1783 mean train loss:  5.03592968e-01, bound:  3.69888276e-01\n",
      "Epoch: 1784 mean train loss:  5.03456533e-01, bound:  3.69873077e-01\n",
      "Epoch: 1785 mean train loss:  5.03320694e-01, bound:  3.69857907e-01\n",
      "Epoch: 1786 mean train loss:  5.03184855e-01, bound:  3.69842738e-01\n",
      "Epoch: 1787 mean train loss:  5.03048837e-01, bound:  3.69827598e-01\n",
      "Epoch: 1788 mean train loss:  5.02913296e-01, bound:  3.69812518e-01\n",
      "Epoch: 1789 mean train loss:  5.02777874e-01, bound:  3.69797409e-01\n",
      "Epoch: 1790 mean train loss:  5.02642393e-01, bound:  3.69782299e-01\n",
      "Epoch: 1791 mean train loss:  5.02506912e-01, bound:  3.69767219e-01\n",
      "Epoch: 1792 mean train loss:  5.02372026e-01, bound:  3.69752198e-01\n",
      "Epoch: 1793 mean train loss:  5.02236426e-01, bound:  3.69737148e-01\n",
      "Epoch: 1794 mean train loss:  5.02102196e-01, bound:  3.69722188e-01\n",
      "Epoch: 1795 mean train loss:  5.01967072e-01, bound:  3.69707197e-01\n",
      "Epoch: 1796 mean train loss:  5.01832128e-01, bound:  3.69692236e-01\n",
      "Epoch: 1797 mean train loss:  5.01697958e-01, bound:  3.69677305e-01\n",
      "Epoch: 1798 mean train loss:  5.01563072e-01, bound:  3.69662344e-01\n",
      "Epoch: 1799 mean train loss:  5.01428843e-01, bound:  3.69647473e-01\n",
      "Epoch: 1800 mean train loss:  5.01294672e-01, bound:  3.69632602e-01\n",
      "Epoch: 1801 mean train loss:  5.01160622e-01, bound:  3.69617701e-01\n",
      "Epoch: 1802 mean train loss:  5.01026630e-01, bound:  3.69602829e-01\n",
      "Epoch: 1803 mean train loss:  5.00892937e-01, bound:  3.69588017e-01\n",
      "Epoch: 1804 mean train loss:  5.00758886e-01, bound:  3.69573236e-01\n",
      "Epoch: 1805 mean train loss:  5.00625312e-01, bound:  3.69558424e-01\n",
      "Epoch: 1806 mean train loss:  5.00491679e-01, bound:  3.69543701e-01\n",
      "Epoch: 1807 mean train loss:  5.00358224e-01, bound:  3.69528890e-01\n",
      "Epoch: 1808 mean train loss:  5.00225067e-01, bound:  3.69514167e-01\n",
      "Epoch: 1809 mean train loss:  5.00091791e-01, bound:  3.69499415e-01\n",
      "Epoch: 1810 mean train loss:  4.99958962e-01, bound:  3.69484812e-01\n",
      "Epoch: 1811 mean train loss:  4.99825567e-01, bound:  3.69470090e-01\n",
      "Epoch: 1812 mean train loss:  4.99692649e-01, bound:  3.69455457e-01\n",
      "Epoch: 1813 mean train loss:  4.99560088e-01, bound:  3.69440764e-01\n",
      "Epoch: 1814 mean train loss:  4.99427289e-01, bound:  3.69426191e-01\n",
      "Epoch: 1815 mean train loss:  4.99294847e-01, bound:  3.69411588e-01\n",
      "Epoch: 1816 mean train loss:  4.99162406e-01, bound:  3.69396985e-01\n",
      "Epoch: 1817 mean train loss:  4.99030054e-01, bound:  3.69382441e-01\n",
      "Epoch: 1818 mean train loss:  4.98897851e-01, bound:  3.69367868e-01\n",
      "Epoch: 1819 mean train loss:  4.98765916e-01, bound:  3.69353324e-01\n",
      "Epoch: 1820 mean train loss:  4.98633951e-01, bound:  3.69338751e-01\n",
      "Epoch: 1821 mean train loss:  4.98502076e-01, bound:  3.69324297e-01\n",
      "Epoch: 1822 mean train loss:  4.98370379e-01, bound:  3.69309843e-01\n",
      "Epoch: 1823 mean train loss:  4.98238772e-01, bound:  3.69295359e-01\n",
      "Epoch: 1824 mean train loss:  4.98107314e-01, bound:  3.69280934e-01\n",
      "Epoch: 1825 mean train loss:  4.97975737e-01, bound:  3.69266450e-01\n",
      "Epoch: 1826 mean train loss:  4.97844696e-01, bound:  3.69252115e-01\n",
      "Epoch: 1827 mean train loss:  4.97713417e-01, bound:  3.69237691e-01\n",
      "Epoch: 1828 mean train loss:  4.97582316e-01, bound:  3.69223326e-01\n",
      "Epoch: 1829 mean train loss:  4.97451395e-01, bound:  3.69208992e-01\n",
      "Epoch: 1830 mean train loss:  4.97320414e-01, bound:  3.69194657e-01\n",
      "Epoch: 1831 mean train loss:  4.97189730e-01, bound:  3.69180381e-01\n",
      "Epoch: 1832 mean train loss:  4.97058988e-01, bound:  3.69166076e-01\n",
      "Epoch: 1833 mean train loss:  4.96928513e-01, bound:  3.69151711e-01\n",
      "Epoch: 1834 mean train loss:  4.96798009e-01, bound:  3.69137496e-01\n",
      "Epoch: 1835 mean train loss:  4.96667743e-01, bound:  3.69123220e-01\n",
      "Epoch: 1836 mean train loss:  4.96537805e-01, bound:  3.69109005e-01\n",
      "Epoch: 1837 mean train loss:  4.96407866e-01, bound:  3.69094789e-01\n",
      "Epoch: 1838 mean train loss:  4.96277660e-01, bound:  3.69080573e-01\n",
      "Epoch: 1839 mean train loss:  4.96147901e-01, bound:  3.69066417e-01\n",
      "Epoch: 1840 mean train loss:  4.96018201e-01, bound:  3.69052261e-01\n",
      "Epoch: 1841 mean train loss:  4.95888352e-01, bound:  3.69038105e-01\n",
      "Epoch: 1842 mean train loss:  4.95759130e-01, bound:  3.69023979e-01\n",
      "Epoch: 1843 mean train loss:  4.95629430e-01, bound:  3.69009852e-01\n",
      "Epoch: 1844 mean train loss:  4.95500207e-01, bound:  3.68995786e-01\n",
      "Epoch: 1845 mean train loss:  4.95370984e-01, bound:  3.68981659e-01\n",
      "Epoch: 1846 mean train loss:  4.95241821e-01, bound:  3.68967623e-01\n",
      "Epoch: 1847 mean train loss:  4.95113015e-01, bound:  3.68953556e-01\n",
      "Epoch: 1848 mean train loss:  4.94983733e-01, bound:  3.68939549e-01\n",
      "Epoch: 1849 mean train loss:  4.94855374e-01, bound:  3.68925512e-01\n",
      "Epoch: 1850 mean train loss:  4.94726390e-01, bound:  3.68911505e-01\n",
      "Epoch: 1851 mean train loss:  4.94598061e-01, bound:  3.68897527e-01\n",
      "Epoch: 1852 mean train loss:  4.94469494e-01, bound:  3.68883580e-01\n",
      "Epoch: 1853 mean train loss:  4.94341314e-01, bound:  3.68869603e-01\n",
      "Epoch: 1854 mean train loss:  4.94213104e-01, bound:  3.68855655e-01\n",
      "Epoch: 1855 mean train loss:  4.94084924e-01, bound:  3.68841767e-01\n",
      "Epoch: 1856 mean train loss:  4.93956804e-01, bound:  3.68827820e-01\n",
      "Epoch: 1857 mean train loss:  4.93829012e-01, bound:  3.68813962e-01\n",
      "Epoch: 1858 mean train loss:  4.93700832e-01, bound:  3.68800104e-01\n",
      "Epoch: 1859 mean train loss:  4.93573487e-01, bound:  3.68786275e-01\n",
      "Epoch: 1860 mean train loss:  4.93445784e-01, bound:  3.68772388e-01\n",
      "Epoch: 1861 mean train loss:  4.93318379e-01, bound:  3.68758559e-01\n",
      "Epoch: 1862 mean train loss:  4.93191332e-01, bound:  3.68744761e-01\n",
      "Epoch: 1863 mean train loss:  4.93063599e-01, bound:  3.68730962e-01\n",
      "Epoch: 1864 mean train loss:  4.92936462e-01, bound:  3.68717164e-01\n",
      "Epoch: 1865 mean train loss:  4.92809534e-01, bound:  3.68703425e-01\n",
      "Epoch: 1866 mean train loss:  4.92682308e-01, bound:  3.68689686e-01\n",
      "Epoch: 1867 mean train loss:  4.92555678e-01, bound:  3.68675977e-01\n",
      "Epoch: 1868 mean train loss:  4.92429048e-01, bound:  3.68662268e-01\n",
      "Epoch: 1869 mean train loss:  4.92302090e-01, bound:  3.68648529e-01\n",
      "Epoch: 1870 mean train loss:  4.92175817e-01, bound:  3.68634880e-01\n",
      "Epoch: 1871 mean train loss:  4.92049545e-01, bound:  3.68621171e-01\n",
      "Epoch: 1872 mean train loss:  4.91923064e-01, bound:  3.68607491e-01\n",
      "Epoch: 1873 mean train loss:  4.91796613e-01, bound:  3.68593901e-01\n",
      "Epoch: 1874 mean train loss:  4.91670936e-01, bound:  3.68580252e-01\n",
      "Epoch: 1875 mean train loss:  4.91544664e-01, bound:  3.68566602e-01\n",
      "Epoch: 1876 mean train loss:  4.91418958e-01, bound:  3.68553042e-01\n",
      "Epoch: 1877 mean train loss:  4.91293013e-01, bound:  3.68539482e-01\n",
      "Epoch: 1878 mean train loss:  4.91167277e-01, bound:  3.68525892e-01\n",
      "Epoch: 1879 mean train loss:  4.91042048e-01, bound:  3.68512362e-01\n",
      "Epoch: 1880 mean train loss:  4.90916163e-01, bound:  3.68498802e-01\n",
      "Epoch: 1881 mean train loss:  4.90790933e-01, bound:  3.68485272e-01\n",
      "Epoch: 1882 mean train loss:  4.90665644e-01, bound:  3.68471801e-01\n",
      "Epoch: 1883 mean train loss:  4.90540415e-01, bound:  3.68458271e-01\n",
      "Epoch: 1884 mean train loss:  4.90415514e-01, bound:  3.68444800e-01\n",
      "Epoch: 1885 mean train loss:  4.90290523e-01, bound:  3.68431330e-01\n",
      "Epoch: 1886 mean train loss:  4.90165740e-01, bound:  3.68417859e-01\n",
      "Epoch: 1887 mean train loss:  4.90041226e-01, bound:  3.68404478e-01\n",
      "Epoch: 1888 mean train loss:  4.89916593e-01, bound:  3.68391007e-01\n",
      "Epoch: 1889 mean train loss:  4.89791781e-01, bound:  3.68377596e-01\n",
      "Epoch: 1890 mean train loss:  4.89667565e-01, bound:  3.68364185e-01\n",
      "Epoch: 1891 mean train loss:  4.89543468e-01, bound:  3.68350834e-01\n",
      "Epoch: 1892 mean train loss:  4.89418834e-01, bound:  3.68337423e-01\n",
      "Epoch: 1893 mean train loss:  4.89294946e-01, bound:  3.68324071e-01\n",
      "Epoch: 1894 mean train loss:  4.89170790e-01, bound:  3.68310750e-01\n",
      "Epoch: 1895 mean train loss:  4.89046961e-01, bound:  3.68297458e-01\n",
      "Epoch: 1896 mean train loss:  4.88923132e-01, bound:  3.68284076e-01\n",
      "Epoch: 1897 mean train loss:  4.88799423e-01, bound:  3.68270814e-01\n",
      "Epoch: 1898 mean train loss:  4.88675565e-01, bound:  3.68257552e-01\n",
      "Epoch: 1899 mean train loss:  4.88552541e-01, bound:  3.68244261e-01\n",
      "Epoch: 1900 mean train loss:  4.88429159e-01, bound:  3.68230969e-01\n",
      "Epoch: 1901 mean train loss:  4.88305718e-01, bound:  3.68217736e-01\n",
      "Epoch: 1902 mean train loss:  4.88182724e-01, bound:  3.68204504e-01\n",
      "Epoch: 1903 mean train loss:  4.88059640e-01, bound:  3.68191302e-01\n",
      "Epoch: 1904 mean train loss:  4.87936348e-01, bound:  3.68178070e-01\n",
      "Epoch: 1905 mean train loss:  4.87813473e-01, bound:  3.68164897e-01\n",
      "Epoch: 1906 mean train loss:  4.87690598e-01, bound:  3.68151695e-01\n",
      "Epoch: 1907 mean train loss:  4.87568259e-01, bound:  3.68138522e-01\n",
      "Epoch: 1908 mean train loss:  4.87445593e-01, bound:  3.68125379e-01\n",
      "Epoch: 1909 mean train loss:  4.87323105e-01, bound:  3.68112236e-01\n",
      "Epoch: 1910 mean train loss:  4.87200707e-01, bound:  3.68099123e-01\n",
      "Epoch: 1911 mean train loss:  4.87078547e-01, bound:  3.68085980e-01\n",
      "Epoch: 1912 mean train loss:  4.86956388e-01, bound:  3.68072897e-01\n",
      "Epoch: 1913 mean train loss:  4.86834437e-01, bound:  3.68059784e-01\n",
      "Epoch: 1914 mean train loss:  4.86712486e-01, bound:  3.68046671e-01\n",
      "Epoch: 1915 mean train loss:  4.86590654e-01, bound:  3.68033618e-01\n",
      "Epoch: 1916 mean train loss:  4.86469090e-01, bound:  3.68020624e-01\n",
      "Epoch: 1917 mean train loss:  4.86346990e-01, bound:  3.68007511e-01\n",
      "Epoch: 1918 mean train loss:  4.86225367e-01, bound:  3.67994487e-01\n",
      "Epoch: 1919 mean train loss:  4.86104101e-01, bound:  3.67981464e-01\n",
      "Epoch: 1920 mean train loss:  4.85982746e-01, bound:  3.67968410e-01\n",
      "Epoch: 1921 mean train loss:  4.85861570e-01, bound:  3.67955416e-01\n",
      "Epoch: 1922 mean train loss:  4.85740542e-01, bound:  3.67942452e-01\n",
      "Epoch: 1923 mean train loss:  4.85619366e-01, bound:  3.67929459e-01\n",
      "Epoch: 1924 mean train loss:  4.85498399e-01, bound:  3.67916465e-01\n",
      "Epoch: 1925 mean train loss:  4.85377640e-01, bound:  3.67903560e-01\n",
      "Epoch: 1926 mean train loss:  4.85257238e-01, bound:  3.67890626e-01\n",
      "Epoch: 1927 mean train loss:  4.85136092e-01, bound:  3.67877692e-01\n",
      "Epoch: 1928 mean train loss:  4.85015839e-01, bound:  3.67864788e-01\n",
      "Epoch: 1929 mean train loss:  4.84895259e-01, bound:  3.67851853e-01\n",
      "Epoch: 1930 mean train loss:  4.84775096e-01, bound:  3.67838979e-01\n",
      "Epoch: 1931 mean train loss:  4.84654903e-01, bound:  3.67826045e-01\n",
      "Epoch: 1932 mean train loss:  4.84534562e-01, bound:  3.67813170e-01\n",
      "Epoch: 1933 mean train loss:  4.84414846e-01, bound:  3.67800355e-01\n",
      "Epoch: 1934 mean train loss:  4.84294832e-01, bound:  3.67787480e-01\n",
      "Epoch: 1935 mean train loss:  4.84175086e-01, bound:  3.67774636e-01\n",
      "Epoch: 1936 mean train loss:  4.84055161e-01, bound:  3.67761850e-01\n",
      "Epoch: 1937 mean train loss:  4.83935684e-01, bound:  3.67748976e-01\n",
      "Epoch: 1938 mean train loss:  4.83816087e-01, bound:  3.67736191e-01\n",
      "Epoch: 1939 mean train loss:  4.83696640e-01, bound:  3.67723435e-01\n",
      "Epoch: 1940 mean train loss:  4.83577251e-01, bound:  3.67710650e-01\n",
      "Epoch: 1941 mean train loss:  4.83458102e-01, bound:  3.67697865e-01\n",
      "Epoch: 1942 mean train loss:  4.83339071e-01, bound:  3.67685139e-01\n",
      "Epoch: 1943 mean train loss:  4.83219981e-01, bound:  3.67672354e-01\n",
      "Epoch: 1944 mean train loss:  4.83101100e-01, bound:  3.67659599e-01\n",
      "Epoch: 1945 mean train loss:  4.82982576e-01, bound:  3.67646873e-01\n",
      "Epoch: 1946 mean train loss:  4.82863456e-01, bound:  3.67634118e-01\n",
      "Epoch: 1947 mean train loss:  4.82745051e-01, bound:  3.67621392e-01\n",
      "Epoch: 1948 mean train loss:  4.82626379e-01, bound:  3.67608726e-01\n",
      "Epoch: 1949 mean train loss:  4.82507825e-01, bound:  3.67595971e-01\n",
      "Epoch: 1950 mean train loss:  4.82389659e-01, bound:  3.67583364e-01\n",
      "Epoch: 1951 mean train loss:  4.82271522e-01, bound:  3.67570668e-01\n",
      "Epoch: 1952 mean train loss:  4.82153147e-01, bound:  3.67558032e-01\n",
      "Epoch: 1953 mean train loss:  4.82035220e-01, bound:  3.67545336e-01\n",
      "Epoch: 1954 mean train loss:  4.81917411e-01, bound:  3.67532730e-01\n",
      "Epoch: 1955 mean train loss:  4.81799781e-01, bound:  3.67520064e-01\n",
      "Epoch: 1956 mean train loss:  4.81681973e-01, bound:  3.67507458e-01\n",
      "Epoch: 1957 mean train loss:  4.81564194e-01, bound:  3.67494851e-01\n",
      "Epoch: 1958 mean train loss:  4.81446713e-01, bound:  3.67482245e-01\n",
      "Epoch: 1959 mean train loss:  4.81329471e-01, bound:  3.67469668e-01\n",
      "Epoch: 1960 mean train loss:  4.81211871e-01, bound:  3.67457122e-01\n",
      "Epoch: 1961 mean train loss:  4.81094360e-01, bound:  3.67444485e-01\n",
      "Epoch: 1962 mean train loss:  4.80977684e-01, bound:  3.67431939e-01\n",
      "Epoch: 1963 mean train loss:  4.80860621e-01, bound:  3.67419392e-01\n",
      "Epoch: 1964 mean train loss:  4.80743676e-01, bound:  3.67406845e-01\n",
      "Epoch: 1965 mean train loss:  4.80626822e-01, bound:  3.67394298e-01\n",
      "Epoch: 1966 mean train loss:  4.80510116e-01, bound:  3.67381752e-01\n",
      "Epoch: 1967 mean train loss:  4.80393320e-01, bound:  3.67369264e-01\n",
      "Epoch: 1968 mean train loss:  4.80276704e-01, bound:  3.67356718e-01\n",
      "Epoch: 1969 mean train loss:  4.80160475e-01, bound:  3.67344230e-01\n",
      "Epoch: 1970 mean train loss:  4.80044186e-01, bound:  3.67331684e-01\n",
      "Epoch: 1971 mean train loss:  4.79927719e-01, bound:  3.67319256e-01\n",
      "Epoch: 1972 mean train loss:  4.79811817e-01, bound:  3.67306799e-01\n",
      "Epoch: 1973 mean train loss:  4.79695618e-01, bound:  3.67294282e-01\n",
      "Epoch: 1974 mean train loss:  4.79579747e-01, bound:  3.67281824e-01\n",
      "Epoch: 1975 mean train loss:  4.79463965e-01, bound:  3.67269427e-01\n",
      "Epoch: 1976 mean train loss:  4.79348153e-01, bound:  3.67256910e-01\n",
      "Epoch: 1977 mean train loss:  4.79232550e-01, bound:  3.67244482e-01\n",
      "Epoch: 1978 mean train loss:  4.79116678e-01, bound:  3.67232084e-01\n",
      "Epoch: 1979 mean train loss:  4.79001403e-01, bound:  3.67219687e-01\n",
      "Epoch: 1980 mean train loss:  4.78886068e-01, bound:  3.67207259e-01\n",
      "Epoch: 1981 mean train loss:  4.78770763e-01, bound:  3.67194861e-01\n",
      "Epoch: 1982 mean train loss:  4.78655636e-01, bound:  3.67182463e-01\n",
      "Epoch: 1983 mean train loss:  4.78540570e-01, bound:  3.67170095e-01\n",
      "Epoch: 1984 mean train loss:  4.78425473e-01, bound:  3.67157698e-01\n",
      "Epoch: 1985 mean train loss:  4.78310585e-01, bound:  3.67145330e-01\n",
      "Epoch: 1986 mean train loss:  4.78195727e-01, bound:  3.67132962e-01\n",
      "Epoch: 1987 mean train loss:  4.78081107e-01, bound:  3.67120653e-01\n",
      "Epoch: 1988 mean train loss:  4.77966398e-01, bound:  3.67108256e-01\n",
      "Epoch: 1989 mean train loss:  4.77851927e-01, bound:  3.67095947e-01\n",
      "Epoch: 1990 mean train loss:  4.77737457e-01, bound:  3.67083609e-01\n",
      "Epoch: 1991 mean train loss:  4.77623343e-01, bound:  3.67071211e-01\n",
      "Epoch: 1992 mean train loss:  4.77509022e-01, bound:  3.67058992e-01\n",
      "Epoch: 1993 mean train loss:  4.77395356e-01, bound:  3.67046654e-01\n",
      "Epoch: 1994 mean train loss:  4.77280647e-01, bound:  3.67034346e-01\n",
      "Epoch: 1995 mean train loss:  4.77167279e-01, bound:  3.67022067e-01\n",
      "Epoch: 1996 mean train loss:  4.77053255e-01, bound:  3.67009759e-01\n",
      "Epoch: 1997 mean train loss:  4.76939380e-01, bound:  3.66997480e-01\n",
      "Epoch: 1998 mean train loss:  4.76825625e-01, bound:  3.66985232e-01\n",
      "Epoch: 1999 mean train loss:  4.76712197e-01, bound:  3.66972983e-01\n",
      "Epoch: 2000 mean train loss:  4.76598591e-01, bound:  3.66960704e-01\n",
      "Epoch: 2001 mean train loss:  4.76485372e-01, bound:  3.66948456e-01\n",
      "Epoch: 2002 mean train loss:  4.76371914e-01, bound:  3.66936207e-01\n",
      "Epoch: 2003 mean train loss:  4.76258963e-01, bound:  3.66923988e-01\n",
      "Epoch: 2004 mean train loss:  4.76145953e-01, bound:  3.66911739e-01\n",
      "Epoch: 2005 mean train loss:  4.76032853e-01, bound:  3.66899520e-01\n",
      "Epoch: 2006 mean train loss:  4.75920022e-01, bound:  3.66887301e-01\n",
      "Epoch: 2007 mean train loss:  4.75807220e-01, bound:  3.66875112e-01\n",
      "Epoch: 2008 mean train loss:  4.75694448e-01, bound:  3.66862893e-01\n",
      "Epoch: 2009 mean train loss:  4.75582212e-01, bound:  3.66850704e-01\n",
      "Epoch: 2010 mean train loss:  4.75469291e-01, bound:  3.66838515e-01\n",
      "Epoch: 2011 mean train loss:  4.75356758e-01, bound:  3.66826355e-01\n",
      "Epoch: 2012 mean train loss:  4.75244671e-01, bound:  3.66814137e-01\n",
      "Epoch: 2013 mean train loss:  4.75132406e-01, bound:  3.66802007e-01\n",
      "Epoch: 2014 mean train loss:  4.75020438e-01, bound:  3.66789788e-01\n",
      "Epoch: 2015 mean train loss:  4.74908292e-01, bound:  3.66777688e-01\n",
      "Epoch: 2016 mean train loss:  4.74796325e-01, bound:  3.66765469e-01\n",
      "Epoch: 2017 mean train loss:  4.74684417e-01, bound:  3.66753340e-01\n",
      "Epoch: 2018 mean train loss:  4.74572510e-01, bound:  3.66741240e-01\n",
      "Epoch: 2019 mean train loss:  4.74461079e-01, bound:  3.66729081e-01\n",
      "Epoch: 2020 mean train loss:  4.74349707e-01, bound:  3.66716981e-01\n",
      "Epoch: 2021 mean train loss:  4.74237829e-01, bound:  3.66704792e-01\n",
      "Epoch: 2022 mean train loss:  4.74126637e-01, bound:  3.66692722e-01\n",
      "Epoch: 2023 mean train loss:  4.74015266e-01, bound:  3.66680622e-01\n",
      "Epoch: 2024 mean train loss:  4.73904043e-01, bound:  3.66668493e-01\n",
      "Epoch: 2025 mean train loss:  4.73792791e-01, bound:  3.66656423e-01\n",
      "Epoch: 2026 mean train loss:  4.73681957e-01, bound:  3.66644293e-01\n",
      "Epoch: 2027 mean train loss:  4.73571122e-01, bound:  3.66632193e-01\n",
      "Epoch: 2028 mean train loss:  4.73460048e-01, bound:  3.66620153e-01\n",
      "Epoch: 2029 mean train loss:  4.73349273e-01, bound:  3.66608024e-01\n",
      "Epoch: 2030 mean train loss:  4.73238736e-01, bound:  3.66595954e-01\n",
      "Epoch: 2031 mean train loss:  4.73128259e-01, bound:  3.66583914e-01\n",
      "Epoch: 2032 mean train loss:  4.73017573e-01, bound:  3.66571814e-01\n",
      "Epoch: 2033 mean train loss:  4.72907186e-01, bound:  3.66559803e-01\n",
      "Epoch: 2034 mean train loss:  4.72797096e-01, bound:  3.66547763e-01\n",
      "Epoch: 2035 mean train loss:  4.72686708e-01, bound:  3.66535693e-01\n",
      "Epoch: 2036 mean train loss:  4.72576886e-01, bound:  3.66523623e-01\n",
      "Epoch: 2037 mean train loss:  4.72466648e-01, bound:  3.66511613e-01\n",
      "Epoch: 2038 mean train loss:  4.72356558e-01, bound:  3.66499543e-01\n",
      "Epoch: 2039 mean train loss:  4.72246975e-01, bound:  3.66487533e-01\n",
      "Epoch: 2040 mean train loss:  4.72137004e-01, bound:  3.66475523e-01\n",
      "Epoch: 2041 mean train loss:  4.72027630e-01, bound:  3.66463512e-01\n",
      "Epoch: 2042 mean train loss:  4.71918017e-01, bound:  3.66451502e-01\n",
      "Epoch: 2043 mean train loss:  4.71808702e-01, bound:  3.66439432e-01\n",
      "Epoch: 2044 mean train loss:  4.71698970e-01, bound:  3.66427481e-01\n",
      "Epoch: 2045 mean train loss:  4.71589625e-01, bound:  3.66415471e-01\n",
      "Epoch: 2046 mean train loss:  4.71480697e-01, bound:  3.66403490e-01\n",
      "Epoch: 2047 mean train loss:  4.71371472e-01, bound:  3.66391480e-01\n",
      "Epoch: 2048 mean train loss:  4.71262306e-01, bound:  3.66379470e-01\n",
      "Epoch: 2049 mean train loss:  4.71153408e-01, bound:  3.66367519e-01\n",
      "Epoch: 2050 mean train loss:  4.71044660e-01, bound:  3.66355509e-01\n",
      "Epoch: 2051 mean train loss:  4.70936030e-01, bound:  3.66343528e-01\n",
      "Epoch: 2052 mean train loss:  4.70827311e-01, bound:  3.66331547e-01\n",
      "Epoch: 2053 mean train loss:  4.70718563e-01, bound:  3.66319656e-01\n",
      "Epoch: 2054 mean train loss:  4.70609993e-01, bound:  3.66307676e-01\n",
      "Epoch: 2055 mean train loss:  4.70501542e-01, bound:  3.66295695e-01\n",
      "Epoch: 2056 mean train loss:  4.70393389e-01, bound:  3.66283745e-01\n",
      "Epoch: 2057 mean train loss:  4.70285177e-01, bound:  3.66271794e-01\n",
      "Epoch: 2058 mean train loss:  4.70176846e-01, bound:  3.66259843e-01\n",
      "Epoch: 2059 mean train loss:  4.70068991e-01, bound:  3.66247922e-01\n",
      "Epoch: 2060 mean train loss:  4.69961047e-01, bound:  3.66235971e-01\n",
      "Epoch: 2061 mean train loss:  4.69853073e-01, bound:  3.66224051e-01\n",
      "Epoch: 2062 mean train loss:  4.69745457e-01, bound:  3.66212130e-01\n",
      "Epoch: 2063 mean train loss:  4.69637841e-01, bound:  3.66200179e-01\n",
      "Epoch: 2064 mean train loss:  4.69530016e-01, bound:  3.66188258e-01\n",
      "Epoch: 2065 mean train loss:  4.69422579e-01, bound:  3.66176337e-01\n",
      "Epoch: 2066 mean train loss:  4.69315469e-01, bound:  3.66164416e-01\n",
      "Epoch: 2067 mean train loss:  4.69207823e-01, bound:  3.66152525e-01\n",
      "Epoch: 2068 mean train loss:  4.69100386e-01, bound:  3.66140604e-01\n",
      "Epoch: 2069 mean train loss:  4.68993634e-01, bound:  3.66128653e-01\n",
      "Epoch: 2070 mean train loss:  4.68886197e-01, bound:  3.66116732e-01\n",
      "Epoch: 2071 mean train loss:  4.68779236e-01, bound:  3.66104871e-01\n",
      "Epoch: 2072 mean train loss:  4.68672544e-01, bound:  3.66092950e-01\n",
      "Epoch: 2073 mean train loss:  4.68565792e-01, bound:  3.66081089e-01\n",
      "Epoch: 2074 mean train loss:  4.68458921e-01, bound:  3.66069198e-01\n",
      "Epoch: 2075 mean train loss:  4.68352228e-01, bound:  3.66057307e-01\n",
      "Epoch: 2076 mean train loss:  4.68245775e-01, bound:  3.66045445e-01\n",
      "Epoch: 2077 mean train loss:  4.68139142e-01, bound:  3.66033554e-01\n",
      "Epoch: 2078 mean train loss:  4.68032569e-01, bound:  3.66021693e-01\n",
      "Epoch: 2079 mean train loss:  4.67926621e-01, bound:  3.66009831e-01\n",
      "Epoch: 2080 mean train loss:  4.67820376e-01, bound:  3.65997940e-01\n",
      "Epoch: 2081 mean train loss:  4.67714339e-01, bound:  3.65986049e-01\n",
      "Epoch: 2082 mean train loss:  4.67608333e-01, bound:  3.65974188e-01\n",
      "Epoch: 2083 mean train loss:  4.67502117e-01, bound:  3.65962297e-01\n",
      "Epoch: 2084 mean train loss:  4.67396498e-01, bound:  3.65950406e-01\n",
      "Epoch: 2085 mean train loss:  4.67290461e-01, bound:  3.65938634e-01\n",
      "Epoch: 2086 mean train loss:  4.67184871e-01, bound:  3.65926743e-01\n",
      "Epoch: 2087 mean train loss:  4.67079163e-01, bound:  3.65914881e-01\n",
      "Epoch: 2088 mean train loss:  4.66973722e-01, bound:  3.65903020e-01\n",
      "Epoch: 2089 mean train loss:  4.66868192e-01, bound:  3.65891188e-01\n",
      "Epoch: 2090 mean train loss:  4.66762990e-01, bound:  3.65879327e-01\n",
      "Epoch: 2091 mean train loss:  4.66657758e-01, bound:  3.65867466e-01\n",
      "Epoch: 2092 mean train loss:  4.66552198e-01, bound:  3.65855664e-01\n",
      "Epoch: 2093 mean train loss:  4.66447175e-01, bound:  3.65843803e-01\n",
      "Epoch: 2094 mean train loss:  4.66342151e-01, bound:  3.65832001e-01\n",
      "Epoch: 2095 mean train loss:  4.66237068e-01, bound:  3.65820110e-01\n",
      "Epoch: 2096 mean train loss:  4.66132641e-01, bound:  3.65808278e-01\n",
      "Epoch: 2097 mean train loss:  4.66027796e-01, bound:  3.65796477e-01\n",
      "Epoch: 2098 mean train loss:  4.65922982e-01, bound:  3.65784645e-01\n",
      "Epoch: 2099 mean train loss:  4.65818495e-01, bound:  3.65772814e-01\n",
      "Epoch: 2100 mean train loss:  4.65713829e-01, bound:  3.65761012e-01\n",
      "Epoch: 2101 mean train loss:  4.65609401e-01, bound:  3.65749180e-01\n",
      "Epoch: 2102 mean train loss:  4.65504825e-01, bound:  3.65737319e-01\n",
      "Epoch: 2103 mean train loss:  4.65400577e-01, bound:  3.65725577e-01\n",
      "Epoch: 2104 mean train loss:  4.65296656e-01, bound:  3.65713686e-01\n",
      "Epoch: 2105 mean train loss:  4.65192378e-01, bound:  3.65701914e-01\n",
      "Epoch: 2106 mean train loss:  4.65088278e-01, bound:  3.65690082e-01\n",
      "Epoch: 2107 mean train loss:  4.64984477e-01, bound:  3.65678251e-01\n",
      "Epoch: 2108 mean train loss:  4.64880466e-01, bound:  3.65666419e-01\n",
      "Epoch: 2109 mean train loss:  4.64776695e-01, bound:  3.65654647e-01\n",
      "Epoch: 2110 mean train loss:  4.64673102e-01, bound:  3.65642786e-01\n",
      "Epoch: 2111 mean train loss:  4.64569539e-01, bound:  3.65631014e-01\n",
      "Epoch: 2112 mean train loss:  4.64466065e-01, bound:  3.65619183e-01\n",
      "Epoch: 2113 mean train loss:  4.64362472e-01, bound:  3.65607440e-01\n",
      "Epoch: 2114 mean train loss:  4.64259118e-01, bound:  3.65595579e-01\n",
      "Epoch: 2115 mean train loss:  4.64155793e-01, bound:  3.65583807e-01\n",
      "Epoch: 2116 mean train loss:  4.64052528e-01, bound:  3.65572035e-01\n",
      "Epoch: 2117 mean train loss:  4.63949412e-01, bound:  3.65560204e-01\n",
      "Epoch: 2118 mean train loss:  4.63846356e-01, bound:  3.65548432e-01\n",
      "Epoch: 2119 mean train loss:  4.63743538e-01, bound:  3.65536630e-01\n",
      "Epoch: 2120 mean train loss:  4.63640422e-01, bound:  3.65524799e-01\n",
      "Epoch: 2121 mean train loss:  4.63537782e-01, bound:  3.65513057e-01\n",
      "Epoch: 2122 mean train loss:  4.63434964e-01, bound:  3.65501255e-01\n",
      "Epoch: 2123 mean train loss:  4.63332385e-01, bound:  3.65489453e-01\n",
      "Epoch: 2124 mean train loss:  4.63229865e-01, bound:  3.65477651e-01\n",
      "Epoch: 2125 mean train loss:  4.63127106e-01, bound:  3.65465909e-01\n",
      "Epoch: 2126 mean train loss:  4.63024676e-01, bound:  3.65454078e-01\n",
      "Epoch: 2127 mean train loss:  4.62922364e-01, bound:  3.65442306e-01\n",
      "Epoch: 2128 mean train loss:  4.62820441e-01, bound:  3.65430474e-01\n",
      "Epoch: 2129 mean train loss:  4.62718248e-01, bound:  3.65418702e-01\n",
      "Epoch: 2130 mean train loss:  4.62616026e-01, bound:  3.65406930e-01\n",
      "Epoch: 2131 mean train loss:  4.62513804e-01, bound:  3.65395129e-01\n",
      "Epoch: 2132 mean train loss:  4.62412000e-01, bound:  3.65383357e-01\n",
      "Epoch: 2133 mean train loss:  4.62310135e-01, bound:  3.65371585e-01\n",
      "Epoch: 2134 mean train loss:  4.62208301e-01, bound:  3.65359813e-01\n",
      "Epoch: 2135 mean train loss:  4.62106586e-01, bound:  3.65347981e-01\n",
      "Epoch: 2136 mean train loss:  4.62005019e-01, bound:  3.65336210e-01\n",
      "Epoch: 2137 mean train loss:  4.61903483e-01, bound:  3.65324408e-01\n",
      "Epoch: 2138 mean train loss:  4.61801857e-01, bound:  3.65312606e-01\n",
      "Epoch: 2139 mean train loss:  4.61700737e-01, bound:  3.65300894e-01\n",
      "Epoch: 2140 mean train loss:  4.61599320e-01, bound:  3.65289062e-01\n",
      "Epoch: 2141 mean train loss:  4.61498052e-01, bound:  3.65277320e-01\n",
      "Epoch: 2142 mean train loss:  4.61396754e-01, bound:  3.65265518e-01\n",
      "Epoch: 2143 mean train loss:  4.61295485e-01, bound:  3.65253747e-01\n",
      "Epoch: 2144 mean train loss:  4.61194873e-01, bound:  3.65242004e-01\n",
      "Epoch: 2145 mean train loss:  4.61093932e-01, bound:  3.65230203e-01\n",
      "Epoch: 2146 mean train loss:  4.60992992e-01, bound:  3.65218431e-01\n",
      "Epoch: 2147 mean train loss:  4.60892171e-01, bound:  3.65206659e-01\n",
      "Epoch: 2148 mean train loss:  4.60791498e-01, bound:  3.65194857e-01\n",
      "Epoch: 2149 mean train loss:  4.60690826e-01, bound:  3.65183115e-01\n",
      "Epoch: 2150 mean train loss:  4.60590392e-01, bound:  3.65171313e-01\n",
      "Epoch: 2151 mean train loss:  4.60490048e-01, bound:  3.65159512e-01\n",
      "Epoch: 2152 mean train loss:  4.60389495e-01, bound:  3.65147769e-01\n",
      "Epoch: 2153 mean train loss:  4.60289270e-01, bound:  3.65135968e-01\n",
      "Epoch: 2154 mean train loss:  4.60188985e-01, bound:  3.65124226e-01\n",
      "Epoch: 2155 mean train loss:  4.60088909e-01, bound:  3.65112424e-01\n",
      "Epoch: 2156 mean train loss:  4.59989041e-01, bound:  3.65100652e-01\n",
      "Epoch: 2157 mean train loss:  4.59888726e-01, bound:  3.65088880e-01\n",
      "Epoch: 2158 mean train loss:  4.59788680e-01, bound:  3.65077138e-01\n",
      "Epoch: 2159 mean train loss:  4.59689081e-01, bound:  3.65065336e-01\n",
      "Epoch: 2160 mean train loss:  4.59588975e-01, bound:  3.65053564e-01\n",
      "Epoch: 2161 mean train loss:  4.59489197e-01, bound:  3.65041733e-01\n",
      "Epoch: 2162 mean train loss:  4.59389597e-01, bound:  3.65030020e-01\n",
      "Epoch: 2163 mean train loss:  4.59289998e-01, bound:  3.65018189e-01\n",
      "Epoch: 2164 mean train loss:  4.59190696e-01, bound:  3.65006447e-01\n",
      "Epoch: 2165 mean train loss:  4.59091216e-01, bound:  3.64994645e-01\n",
      "Epoch: 2166 mean train loss:  4.58991826e-01, bound:  3.64982903e-01\n",
      "Epoch: 2167 mean train loss:  4.58892435e-01, bound:  3.64971131e-01\n",
      "Epoch: 2168 mean train loss:  4.58793253e-01, bound:  3.64959359e-01\n",
      "Epoch: 2169 mean train loss:  4.58694190e-01, bound:  3.64947528e-01\n",
      "Epoch: 2170 mean train loss:  4.58595127e-01, bound:  3.64935756e-01\n",
      "Epoch: 2171 mean train loss:  4.58496362e-01, bound:  3.64923954e-01\n",
      "Epoch: 2172 mean train loss:  4.58397061e-01, bound:  3.64912182e-01\n",
      "Epoch: 2173 mean train loss:  4.58298594e-01, bound:  3.64900380e-01\n",
      "Epoch: 2174 mean train loss:  4.58199710e-01, bound:  3.64888579e-01\n",
      "Epoch: 2175 mean train loss:  4.58101094e-01, bound:  3.64876807e-01\n",
      "Epoch: 2176 mean train loss:  4.58002418e-01, bound:  3.64865035e-01\n",
      "Epoch: 2177 mean train loss:  4.57903981e-01, bound:  3.64853263e-01\n",
      "Epoch: 2178 mean train loss:  4.57805514e-01, bound:  3.64841491e-01\n",
      "Epoch: 2179 mean train loss:  4.57707405e-01, bound:  3.64829630e-01\n",
      "Epoch: 2180 mean train loss:  4.57608759e-01, bound:  3.64817888e-01\n",
      "Epoch: 2181 mean train loss:  4.57510442e-01, bound:  3.64806086e-01\n",
      "Epoch: 2182 mean train loss:  4.57412452e-01, bound:  3.64794344e-01\n",
      "Epoch: 2183 mean train loss:  4.57314193e-01, bound:  3.64782542e-01\n",
      "Epoch: 2184 mean train loss:  4.57216203e-01, bound:  3.64770740e-01\n",
      "Epoch: 2185 mean train loss:  4.57118332e-01, bound:  3.64758939e-01\n",
      "Epoch: 2186 mean train loss:  4.57020313e-01, bound:  3.64747137e-01\n",
      "Epoch: 2187 mean train loss:  4.56922442e-01, bound:  3.64735365e-01\n",
      "Epoch: 2188 mean train loss:  4.56824750e-01, bound:  3.64723533e-01\n",
      "Epoch: 2189 mean train loss:  4.56727386e-01, bound:  3.64711761e-01\n",
      "Epoch: 2190 mean train loss:  4.56629604e-01, bound:  3.64699960e-01\n",
      "Epoch: 2191 mean train loss:  4.56531942e-01, bound:  3.64688188e-01\n",
      "Epoch: 2192 mean train loss:  4.56434309e-01, bound:  3.64676356e-01\n",
      "Epoch: 2193 mean train loss:  4.56337005e-01, bound:  3.64664584e-01\n",
      "Epoch: 2194 mean train loss:  4.56239998e-01, bound:  3.64652753e-01\n",
      "Epoch: 2195 mean train loss:  4.56142485e-01, bound:  3.64640921e-01\n",
      "Epoch: 2196 mean train loss:  4.56045240e-01, bound:  3.64629149e-01\n",
      "Epoch: 2197 mean train loss:  4.55948174e-01, bound:  3.64617348e-01\n",
      "Epoch: 2198 mean train loss:  4.55850929e-01, bound:  3.64605546e-01\n",
      "Epoch: 2199 mean train loss:  4.55754280e-01, bound:  3.64593714e-01\n",
      "Epoch: 2200 mean train loss:  4.55657035e-01, bound:  3.64581913e-01\n",
      "Epoch: 2201 mean train loss:  4.55560267e-01, bound:  3.64570141e-01\n",
      "Epoch: 2202 mean train loss:  4.55463469e-01, bound:  3.64558280e-01\n",
      "Epoch: 2203 mean train loss:  4.55366880e-01, bound:  3.64546478e-01\n",
      "Epoch: 2204 mean train loss:  4.55270261e-01, bound:  3.64534646e-01\n",
      "Epoch: 2205 mean train loss:  4.55173522e-01, bound:  3.64522845e-01\n",
      "Epoch: 2206 mean train loss:  4.55076963e-01, bound:  3.64510983e-01\n",
      "Epoch: 2207 mean train loss:  4.54980701e-01, bound:  3.64499182e-01\n",
      "Epoch: 2208 mean train loss:  4.54884171e-01, bound:  3.64487380e-01\n",
      "Epoch: 2209 mean train loss:  4.54787999e-01, bound:  3.64475578e-01\n",
      "Epoch: 2210 mean train loss:  4.54691738e-01, bound:  3.64463747e-01\n",
      "Epoch: 2211 mean train loss:  4.54595476e-01, bound:  3.64451885e-01\n",
      "Epoch: 2212 mean train loss:  4.54499513e-01, bound:  3.64440054e-01\n",
      "Epoch: 2213 mean train loss:  4.54403341e-01, bound:  3.64428222e-01\n",
      "Epoch: 2214 mean train loss:  4.54307228e-01, bound:  3.64416391e-01\n",
      "Epoch: 2215 mean train loss:  4.54211324e-01, bound:  3.64404589e-01\n",
      "Epoch: 2216 mean train loss:  4.54115570e-01, bound:  3.64392728e-01\n",
      "Epoch: 2217 mean train loss:  4.54019994e-01, bound:  3.64380926e-01\n",
      "Epoch: 2218 mean train loss:  4.53924179e-01, bound:  3.64369065e-01\n",
      "Epoch: 2219 mean train loss:  4.53828663e-01, bound:  3.64357233e-01\n",
      "Epoch: 2220 mean train loss:  4.53732848e-01, bound:  3.64345402e-01\n",
      "Epoch: 2221 mean train loss:  4.53637600e-01, bound:  3.64333540e-01\n",
      "Epoch: 2222 mean train loss:  4.53541875e-01, bound:  3.64321709e-01\n",
      "Epoch: 2223 mean train loss:  4.53446686e-01, bound:  3.64309847e-01\n",
      "Epoch: 2224 mean train loss:  4.53351498e-01, bound:  3.64298016e-01\n",
      "Epoch: 2225 mean train loss:  4.53255922e-01, bound:  3.64286184e-01\n",
      "Epoch: 2226 mean train loss:  4.53160942e-01, bound:  3.64274293e-01\n",
      "Epoch: 2227 mean train loss:  4.53065783e-01, bound:  3.64262491e-01\n",
      "Epoch: 2228 mean train loss:  4.52970803e-01, bound:  3.64250571e-01\n",
      "Epoch: 2229 mean train loss:  4.52875763e-01, bound:  3.64238769e-01\n",
      "Epoch: 2230 mean train loss:  4.52781022e-01, bound:  3.64226907e-01\n",
      "Epoch: 2231 mean train loss:  4.52685982e-01, bound:  3.64215046e-01\n",
      "Epoch: 2232 mean train loss:  4.52591032e-01, bound:  3.64203155e-01\n",
      "Epoch: 2233 mean train loss:  4.52496439e-01, bound:  3.64191294e-01\n",
      "Epoch: 2234 mean train loss:  4.52401727e-01, bound:  3.64179432e-01\n",
      "Epoch: 2235 mean train loss:  4.52307254e-01, bound:  3.64167541e-01\n",
      "Epoch: 2236 mean train loss:  4.52212453e-01, bound:  3.64155680e-01\n",
      "Epoch: 2237 mean train loss:  4.52118099e-01, bound:  3.64143819e-01\n",
      "Epoch: 2238 mean train loss:  4.52023625e-01, bound:  3.64131868e-01\n",
      "Epoch: 2239 mean train loss:  4.51929241e-01, bound:  3.64120007e-01\n",
      "Epoch: 2240 mean train loss:  4.51835185e-01, bound:  3.64108175e-01\n",
      "Epoch: 2241 mean train loss:  4.51740652e-01, bound:  3.64096254e-01\n",
      "Epoch: 2242 mean train loss:  4.51646715e-01, bound:  3.64084393e-01\n",
      "Epoch: 2243 mean train loss:  4.51552778e-01, bound:  3.64072502e-01\n",
      "Epoch: 2244 mean train loss:  4.51458931e-01, bound:  3.64060640e-01\n",
      "Epoch: 2245 mean train loss:  4.51364905e-01, bound:  3.64048719e-01\n",
      "Epoch: 2246 mean train loss:  4.51270700e-01, bound:  3.64036858e-01\n",
      "Epoch: 2247 mean train loss:  4.51176941e-01, bound:  3.64024967e-01\n",
      "Epoch: 2248 mean train loss:  4.51083124e-01, bound:  3.64013016e-01\n",
      "Epoch: 2249 mean train loss:  4.50989366e-01, bound:  3.64001155e-01\n",
      "Epoch: 2250 mean train loss:  4.50895697e-01, bound:  3.63989204e-01\n",
      "Epoch: 2251 mean train loss:  4.50801969e-01, bound:  3.63977313e-01\n",
      "Epoch: 2252 mean train loss:  4.50708568e-01, bound:  3.63965422e-01\n",
      "Epoch: 2253 mean train loss:  4.50614929e-01, bound:  3.63953501e-01\n",
      "Epoch: 2254 mean train loss:  4.50521648e-01, bound:  3.63941610e-01\n",
      "Epoch: 2255 mean train loss:  4.50428337e-01, bound:  3.63929689e-01\n",
      "Epoch: 2256 mean train loss:  4.50334907e-01, bound:  3.63917738e-01\n",
      "Epoch: 2257 mean train loss:  4.50241745e-01, bound:  3.63905847e-01\n",
      "Epoch: 2258 mean train loss:  4.50148523e-01, bound:  3.63893926e-01\n",
      "Epoch: 2259 mean train loss:  4.50055301e-01, bound:  3.63882005e-01\n",
      "Epoch: 2260 mean train loss:  4.49962378e-01, bound:  3.63870054e-01\n",
      "Epoch: 2261 mean train loss:  4.49869245e-01, bound:  3.63858134e-01\n",
      "Epoch: 2262 mean train loss:  4.49776411e-01, bound:  3.63846183e-01\n",
      "Epoch: 2263 mean train loss:  4.49683428e-01, bound:  3.63834292e-01\n",
      "Epoch: 2264 mean train loss:  4.49590653e-01, bound:  3.63822311e-01\n",
      "Epoch: 2265 mean train loss:  4.49497759e-01, bound:  3.63810360e-01\n",
      "Epoch: 2266 mean train loss:  4.49405193e-01, bound:  3.63798380e-01\n",
      "Epoch: 2267 mean train loss:  4.49312449e-01, bound:  3.63786489e-01\n",
      "Epoch: 2268 mean train loss:  4.49219972e-01, bound:  3.63774508e-01\n",
      "Epoch: 2269 mean train loss:  4.49127376e-01, bound:  3.63762587e-01\n",
      "Epoch: 2270 mean train loss:  4.49034780e-01, bound:  3.63750607e-01\n",
      "Epoch: 2271 mean train loss:  4.48942393e-01, bound:  3.63738656e-01\n",
      "Epoch: 2272 mean train loss:  4.48850036e-01, bound:  3.63726705e-01\n",
      "Epoch: 2273 mean train loss:  4.48757946e-01, bound:  3.63714755e-01\n",
      "Epoch: 2274 mean train loss:  4.48665589e-01, bound:  3.63702744e-01\n",
      "Epoch: 2275 mean train loss:  4.48573619e-01, bound:  3.63690794e-01\n",
      "Epoch: 2276 mean train loss:  4.48481292e-01, bound:  3.63678843e-01\n",
      "Epoch: 2277 mean train loss:  4.48389202e-01, bound:  3.63666892e-01\n",
      "Epoch: 2278 mean train loss:  4.48297113e-01, bound:  3.63654882e-01\n",
      "Epoch: 2279 mean train loss:  4.48205203e-01, bound:  3.63642901e-01\n",
      "Epoch: 2280 mean train loss:  4.48113322e-01, bound:  3.63630861e-01\n",
      "Epoch: 2281 mean train loss:  4.48021650e-01, bound:  3.63618970e-01\n",
      "Epoch: 2282 mean train loss:  4.47929621e-01, bound:  3.63606960e-01\n",
      "Epoch: 2283 mean train loss:  4.47838008e-01, bound:  3.63594979e-01\n",
      "Epoch: 2284 mean train loss:  4.47746247e-01, bound:  3.63582939e-01\n",
      "Epoch: 2285 mean train loss:  4.47654665e-01, bound:  3.63570958e-01\n",
      "Epoch: 2286 mean train loss:  4.47563142e-01, bound:  3.63558948e-01\n",
      "Epoch: 2287 mean train loss:  4.47471619e-01, bound:  3.63546968e-01\n",
      "Epoch: 2288 mean train loss:  4.47380096e-01, bound:  3.63534957e-01\n",
      "Epoch: 2289 mean train loss:  4.47288692e-01, bound:  3.63522947e-01\n",
      "Epoch: 2290 mean train loss:  4.47197437e-01, bound:  3.63510936e-01\n",
      "Epoch: 2291 mean train loss:  4.47106123e-01, bound:  3.63498896e-01\n",
      "Epoch: 2292 mean train loss:  4.47014928e-01, bound:  3.63486886e-01\n",
      "Epoch: 2293 mean train loss:  4.46923912e-01, bound:  3.63474935e-01\n",
      "Epoch: 2294 mean train loss:  4.46832776e-01, bound:  3.63462836e-01\n",
      "Epoch: 2295 mean train loss:  4.46741521e-01, bound:  3.63450855e-01\n",
      "Epoch: 2296 mean train loss:  4.46650505e-01, bound:  3.63438815e-01\n",
      "Epoch: 2297 mean train loss:  4.46559638e-01, bound:  3.63426775e-01\n",
      "Epoch: 2298 mean train loss:  4.46468771e-01, bound:  3.63414764e-01\n",
      "Epoch: 2299 mean train loss:  4.46377665e-01, bound:  3.63402724e-01\n",
      "Epoch: 2300 mean train loss:  4.46287006e-01, bound:  3.63390595e-01\n",
      "Epoch: 2301 mean train loss:  4.46196347e-01, bound:  3.63378644e-01\n",
      "Epoch: 2302 mean train loss:  4.46105629e-01, bound:  3.63366544e-01\n",
      "Epoch: 2303 mean train loss:  4.46015060e-01, bound:  3.63354474e-01\n",
      "Epoch: 2304 mean train loss:  4.45924640e-01, bound:  3.63342464e-01\n",
      "Epoch: 2305 mean train loss:  4.45834011e-01, bound:  3.63330424e-01\n",
      "Epoch: 2306 mean train loss:  4.45743591e-01, bound:  3.63318384e-01\n",
      "Epoch: 2307 mean train loss:  4.45652813e-01, bound:  3.63306284e-01\n",
      "Epoch: 2308 mean train loss:  4.45562661e-01, bound:  3.63294244e-01\n",
      "Epoch: 2309 mean train loss:  4.45472240e-01, bound:  3.63282114e-01\n",
      "Epoch: 2310 mean train loss:  4.45382029e-01, bound:  3.63270104e-01\n",
      "Epoch: 2311 mean train loss:  4.45291728e-01, bound:  3.63257974e-01\n",
      "Epoch: 2312 mean train loss:  4.45201933e-01, bound:  3.63245875e-01\n",
      "Epoch: 2313 mean train loss:  4.45111841e-01, bound:  3.63233835e-01\n",
      "Epoch: 2314 mean train loss:  4.45021898e-01, bound:  3.63221735e-01\n",
      "Epoch: 2315 mean train loss:  4.44931895e-01, bound:  3.63209695e-01\n",
      "Epoch: 2316 mean train loss:  4.44841743e-01, bound:  3.63197595e-01\n",
      "Epoch: 2317 mean train loss:  4.44751769e-01, bound:  3.63185555e-01\n",
      "Epoch: 2318 mean train loss:  4.44661915e-01, bound:  3.63173425e-01\n",
      "Epoch: 2319 mean train loss:  4.44572061e-01, bound:  3.63161325e-01\n",
      "Epoch: 2320 mean train loss:  4.44482327e-01, bound:  3.63149196e-01\n",
      "Epoch: 2321 mean train loss:  4.44392949e-01, bound:  3.63137066e-01\n",
      "Epoch: 2322 mean train loss:  4.44303185e-01, bound:  3.63124967e-01\n",
      "Epoch: 2323 mean train loss:  4.44213748e-01, bound:  3.63112777e-01\n",
      "Epoch: 2324 mean train loss:  4.44123954e-01, bound:  3.63100678e-01\n",
      "Epoch: 2325 mean train loss:  4.44034278e-01, bound:  3.63088608e-01\n",
      "Epoch: 2326 mean train loss:  4.43945080e-01, bound:  3.63076478e-01\n",
      "Epoch: 2327 mean train loss:  4.43855613e-01, bound:  3.63064319e-01\n",
      "Epoch: 2328 mean train loss:  4.43766236e-01, bound:  3.63052249e-01\n",
      "Epoch: 2329 mean train loss:  4.43677068e-01, bound:  3.63040090e-01\n",
      "Epoch: 2330 mean train loss:  4.43587840e-01, bound:  3.63027930e-01\n",
      "Epoch: 2331 mean train loss:  4.43498552e-01, bound:  3.63015831e-01\n",
      "Epoch: 2332 mean train loss:  4.43409264e-01, bound:  3.63003641e-01\n",
      "Epoch: 2333 mean train loss:  4.43320334e-01, bound:  3.62991512e-01\n",
      "Epoch: 2334 mean train loss:  4.43231374e-01, bound:  3.62979352e-01\n",
      "Epoch: 2335 mean train loss:  4.43142354e-01, bound:  3.62967193e-01\n",
      "Epoch: 2336 mean train loss:  4.43053573e-01, bound:  3.62955034e-01\n",
      "Epoch: 2337 mean train loss:  4.42964375e-01, bound:  3.62942934e-01\n",
      "Epoch: 2338 mean train loss:  4.42875654e-01, bound:  3.62930715e-01\n",
      "Epoch: 2339 mean train loss:  4.42786694e-01, bound:  3.62918586e-01\n",
      "Epoch: 2340 mean train loss:  4.42698240e-01, bound:  3.62906396e-01\n",
      "Epoch: 2341 mean train loss:  4.42609280e-01, bound:  3.62894207e-01\n",
      "Epoch: 2342 mean train loss:  4.42520887e-01, bound:  3.62882048e-01\n",
      "Epoch: 2343 mean train loss:  4.42431986e-01, bound:  3.62869829e-01\n",
      "Epoch: 2344 mean train loss:  4.42343563e-01, bound:  3.62857640e-01\n",
      "Epoch: 2345 mean train loss:  4.42255080e-01, bound:  3.62845480e-01\n",
      "Epoch: 2346 mean train loss:  4.42166775e-01, bound:  3.62833261e-01\n",
      "Epoch: 2347 mean train loss:  4.42078292e-01, bound:  3.62821102e-01\n",
      "Epoch: 2348 mean train loss:  4.41989720e-01, bound:  3.62808853e-01\n",
      "Epoch: 2349 mean train loss:  4.41901416e-01, bound:  3.62796694e-01\n",
      "Epoch: 2350 mean train loss:  4.41813231e-01, bound:  3.62784445e-01\n",
      "Epoch: 2351 mean train loss:  4.41724896e-01, bound:  3.62772256e-01\n",
      "Epoch: 2352 mean train loss:  4.41636950e-01, bound:  3.62760037e-01\n",
      "Epoch: 2353 mean train loss:  4.41548496e-01, bound:  3.62747818e-01\n",
      "Epoch: 2354 mean train loss:  4.41460639e-01, bound:  3.62735659e-01\n",
      "Epoch: 2355 mean train loss:  4.41372603e-01, bound:  3.62723410e-01\n",
      "Epoch: 2356 mean train loss:  4.41284478e-01, bound:  3.62711161e-01\n",
      "Epoch: 2357 mean train loss:  4.41196710e-01, bound:  3.62698913e-01\n",
      "Epoch: 2358 mean train loss:  4.41108793e-01, bound:  3.62686694e-01\n",
      "Epoch: 2359 mean train loss:  4.41020936e-01, bound:  3.62674475e-01\n",
      "Epoch: 2360 mean train loss:  4.40933198e-01, bound:  3.62662226e-01\n",
      "Epoch: 2361 mean train loss:  4.40845400e-01, bound:  3.62649947e-01\n",
      "Epoch: 2362 mean train loss:  4.40757900e-01, bound:  3.62637728e-01\n",
      "Epoch: 2363 mean train loss:  4.40670133e-01, bound:  3.62625480e-01\n",
      "Epoch: 2364 mean train loss:  4.40582454e-01, bound:  3.62613261e-01\n",
      "Epoch: 2365 mean train loss:  4.40494806e-01, bound:  3.62600982e-01\n",
      "Epoch: 2366 mean train loss:  4.40407276e-01, bound:  3.62588733e-01\n",
      "Epoch: 2367 mean train loss:  4.40319598e-01, bound:  3.62576455e-01\n",
      "Epoch: 2368 mean train loss:  4.40232337e-01, bound:  3.62564176e-01\n",
      "Epoch: 2369 mean train loss:  4.40145075e-01, bound:  3.62551898e-01\n",
      "Epoch: 2370 mean train loss:  4.40057337e-01, bound:  3.62539649e-01\n",
      "Epoch: 2371 mean train loss:  4.39970136e-01, bound:  3.62527370e-01\n",
      "Epoch: 2372 mean train loss:  4.39882934e-01, bound:  3.62515062e-01\n",
      "Epoch: 2373 mean train loss:  4.39796180e-01, bound:  3.62502784e-01\n",
      "Epoch: 2374 mean train loss:  4.39708799e-01, bound:  3.62490505e-01\n",
      "Epoch: 2375 mean train loss:  4.39621449e-01, bound:  3.62478167e-01\n",
      "Epoch: 2376 mean train loss:  4.39534515e-01, bound:  3.62465918e-01\n",
      "Epoch: 2377 mean train loss:  4.39447492e-01, bound:  3.62453580e-01\n",
      "Epoch: 2378 mean train loss:  4.39360410e-01, bound:  3.62441301e-01\n",
      "Epoch: 2379 mean train loss:  4.39273417e-01, bound:  3.62429053e-01\n",
      "Epoch: 2380 mean train loss:  4.39186960e-01, bound:  3.62416685e-01\n",
      "Epoch: 2381 mean train loss:  4.39099729e-01, bound:  3.62404376e-01\n",
      "Epoch: 2382 mean train loss:  4.39012796e-01, bound:  3.62392068e-01\n",
      "Epoch: 2383 mean train loss:  4.38926250e-01, bound:  3.62379700e-01\n",
      "Epoch: 2384 mean train loss:  4.38839555e-01, bound:  3.62367421e-01\n",
      "Epoch: 2385 mean train loss:  4.38752800e-01, bound:  3.62355113e-01\n",
      "Epoch: 2386 mean train loss:  4.38665926e-01, bound:  3.62342775e-01\n",
      "Epoch: 2387 mean train loss:  4.38579500e-01, bound:  3.62330407e-01\n",
      "Epoch: 2388 mean train loss:  4.38492864e-01, bound:  3.62318099e-01\n",
      "Epoch: 2389 mean train loss:  4.38406438e-01, bound:  3.62305731e-01\n",
      "Epoch: 2390 mean train loss:  4.38319802e-01, bound:  3.62293392e-01\n",
      "Epoch: 2391 mean train loss:  4.38233376e-01, bound:  3.62281024e-01\n",
      "Epoch: 2392 mean train loss:  4.38146919e-01, bound:  3.62268686e-01\n",
      "Epoch: 2393 mean train loss:  4.38060641e-01, bound:  3.62256318e-01\n",
      "Epoch: 2394 mean train loss:  4.37974334e-01, bound:  3.62243921e-01\n",
      "Epoch: 2395 mean train loss:  4.37888265e-01, bound:  3.62231642e-01\n",
      "Epoch: 2396 mean train loss:  4.37801838e-01, bound:  3.62219214e-01\n",
      "Epoch: 2397 mean train loss:  4.37715590e-01, bound:  3.62206906e-01\n",
      "Epoch: 2398 mean train loss:  4.37629521e-01, bound:  3.62194508e-01\n",
      "Epoch: 2399 mean train loss:  4.37543482e-01, bound:  3.62182081e-01\n",
      "Epoch: 2400 mean train loss:  4.37457323e-01, bound:  3.62169743e-01\n",
      "Epoch: 2401 mean train loss:  4.37371492e-01, bound:  3.62157345e-01\n",
      "Epoch: 2402 mean train loss:  4.37285393e-01, bound:  3.62144947e-01\n",
      "Epoch: 2403 mean train loss:  4.37199652e-01, bound:  3.62132579e-01\n",
      "Epoch: 2404 mean train loss:  4.37113494e-01, bound:  3.62120122e-01\n",
      "Epoch: 2405 mean train loss:  4.37027454e-01, bound:  3.62107724e-01\n",
      "Epoch: 2406 mean train loss:  4.36941862e-01, bound:  3.62095356e-01\n",
      "Epoch: 2407 mean train loss:  4.36856061e-01, bound:  3.62082899e-01\n",
      "Epoch: 2408 mean train loss:  4.36770320e-01, bound:  3.62070531e-01\n",
      "Epoch: 2409 mean train loss:  4.36684579e-01, bound:  3.62058103e-01\n",
      "Epoch: 2410 mean train loss:  4.36598986e-01, bound:  3.62045676e-01\n",
      "Epoch: 2411 mean train loss:  4.36513841e-01, bound:  3.62033278e-01\n",
      "Epoch: 2412 mean train loss:  4.36427921e-01, bound:  3.62020880e-01\n",
      "Epoch: 2413 mean train loss:  4.36342388e-01, bound:  3.62008423e-01\n",
      "Epoch: 2414 mean train loss:  4.36257184e-01, bound:  3.61995995e-01\n",
      "Epoch: 2415 mean train loss:  4.36171323e-01, bound:  3.61983567e-01\n",
      "Epoch: 2416 mean train loss:  4.36086327e-01, bound:  3.61971140e-01\n",
      "Epoch: 2417 mean train loss:  4.36000496e-01, bound:  3.61958653e-01\n",
      "Epoch: 2418 mean train loss:  4.35915172e-01, bound:  3.61946195e-01\n",
      "Epoch: 2419 mean train loss:  4.35829818e-01, bound:  3.61933768e-01\n",
      "Epoch: 2420 mean train loss:  4.35744733e-01, bound:  3.61921281e-01\n",
      "Epoch: 2421 mean train loss:  4.35659319e-01, bound:  3.61908853e-01\n",
      "Epoch: 2422 mean train loss:  4.35574085e-01, bound:  3.61896366e-01\n",
      "Epoch: 2423 mean train loss:  4.35488999e-01, bound:  3.61883879e-01\n",
      "Epoch: 2424 mean train loss:  4.35404122e-01, bound:  3.61871421e-01\n",
      "Epoch: 2425 mean train loss:  4.35319006e-01, bound:  3.61858994e-01\n",
      "Epoch: 2426 mean train loss:  4.35233831e-01, bound:  3.61846507e-01\n",
      "Epoch: 2427 mean train loss:  4.35148865e-01, bound:  3.61834049e-01\n",
      "Epoch: 2428 mean train loss:  4.35064107e-01, bound:  3.61821532e-01\n",
      "Epoch: 2429 mean train loss:  4.34979081e-01, bound:  3.61809075e-01\n",
      "Epoch: 2430 mean train loss:  4.34894383e-01, bound:  3.61796558e-01\n",
      "Epoch: 2431 mean train loss:  4.34809327e-01, bound:  3.61784041e-01\n",
      "Epoch: 2432 mean train loss:  4.34724689e-01, bound:  3.61771554e-01\n",
      "Epoch: 2433 mean train loss:  4.34639901e-01, bound:  3.61759096e-01\n",
      "Epoch: 2434 mean train loss:  4.34555054e-01, bound:  3.61746579e-01\n",
      "Epoch: 2435 mean train loss:  4.34470594e-01, bound:  3.61734062e-01\n",
      "Epoch: 2436 mean train loss:  4.34385747e-01, bound:  3.61721545e-01\n",
      "Epoch: 2437 mean train loss:  4.34301227e-01, bound:  3.61709028e-01\n",
      "Epoch: 2438 mean train loss:  4.34216619e-01, bound:  3.61696512e-01\n",
      "Epoch: 2439 mean train loss:  4.34131980e-01, bound:  3.61683935e-01\n",
      "Epoch: 2440 mean train loss:  4.34047610e-01, bound:  3.61671448e-01\n",
      "Epoch: 2441 mean train loss:  4.33962971e-01, bound:  3.61658931e-01\n",
      "Epoch: 2442 mean train loss:  4.33878809e-01, bound:  3.61646354e-01\n",
      "Epoch: 2443 mean train loss:  4.33794409e-01, bound:  3.61633807e-01\n",
      "Epoch: 2444 mean train loss:  4.33710068e-01, bound:  3.61621290e-01\n",
      "Epoch: 2445 mean train loss:  4.33625609e-01, bound:  3.61608773e-01\n",
      "Epoch: 2446 mean train loss:  4.33541507e-01, bound:  3.61596197e-01\n",
      "Epoch: 2447 mean train loss:  4.33457077e-01, bound:  3.61583680e-01\n",
      "Epoch: 2448 mean train loss:  4.33372915e-01, bound:  3.61571074e-01\n",
      "Epoch: 2449 mean train loss:  4.33288872e-01, bound:  3.61558527e-01\n",
      "Epoch: 2450 mean train loss:  4.33204740e-01, bound:  3.61545980e-01\n",
      "Epoch: 2451 mean train loss:  4.33120638e-01, bound:  3.61533374e-01\n",
      "Epoch: 2452 mean train loss:  4.33036774e-01, bound:  3.61520827e-01\n",
      "Epoch: 2453 mean train loss:  4.32952732e-01, bound:  3.61508280e-01\n",
      "Epoch: 2454 mean train loss:  4.32868451e-01, bound:  3.61495674e-01\n",
      "Epoch: 2455 mean train loss:  4.32784557e-01, bound:  3.61483067e-01\n",
      "Epoch: 2456 mean train loss:  4.32700276e-01, bound:  3.61470520e-01\n",
      "Epoch: 2457 mean train loss:  4.32616860e-01, bound:  3.61457884e-01\n",
      "Epoch: 2458 mean train loss:  4.32532966e-01, bound:  3.61445338e-01\n",
      "Epoch: 2459 mean train loss:  4.32448864e-01, bound:  3.61432701e-01\n",
      "Epoch: 2460 mean train loss:  4.32365358e-01, bound:  3.61420125e-01\n",
      "Epoch: 2461 mean train loss:  4.32281584e-01, bound:  3.61407548e-01\n",
      "Epoch: 2462 mean train loss:  4.32197660e-01, bound:  3.61394942e-01\n",
      "Epoch: 2463 mean train loss:  4.32113677e-01, bound:  3.61382335e-01\n",
      "Epoch: 2464 mean train loss:  4.32030618e-01, bound:  3.61369699e-01\n",
      "Epoch: 2465 mean train loss:  4.31946784e-01, bound:  3.61357033e-01\n",
      "Epoch: 2466 mean train loss:  4.31863397e-01, bound:  3.61344457e-01\n",
      "Epoch: 2467 mean train loss:  4.31780100e-01, bound:  3.61331791e-01\n",
      "Epoch: 2468 mean train loss:  4.31696475e-01, bound:  3.61319184e-01\n",
      "Epoch: 2469 mean train loss:  4.31612998e-01, bound:  3.61306518e-01\n",
      "Epoch: 2470 mean train loss:  4.31529403e-01, bound:  3.61293912e-01\n",
      "Epoch: 2471 mean train loss:  4.31445956e-01, bound:  3.61281276e-01\n",
      "Epoch: 2472 mean train loss:  4.31362391e-01, bound:  3.61268640e-01\n",
      "Epoch: 2473 mean train loss:  4.31279361e-01, bound:  3.61256003e-01\n",
      "Epoch: 2474 mean train loss:  4.31196094e-01, bound:  3.61243337e-01\n",
      "Epoch: 2475 mean train loss:  4.31112647e-01, bound:  3.61230671e-01\n",
      "Epoch: 2476 mean train loss:  4.31029469e-01, bound:  3.61217946e-01\n",
      "Epoch: 2477 mean train loss:  4.30946141e-01, bound:  3.61205310e-01\n",
      "Epoch: 2478 mean train loss:  4.30863351e-01, bound:  3.61192673e-01\n",
      "Epoch: 2479 mean train loss:  4.30780083e-01, bound:  3.61180007e-01\n",
      "Epoch: 2480 mean train loss:  4.30696875e-01, bound:  3.61167341e-01\n",
      "Epoch: 2481 mean train loss:  4.30613846e-01, bound:  3.61154675e-01\n",
      "Epoch: 2482 mean train loss:  4.30530727e-01, bound:  3.61141980e-01\n",
      "Epoch: 2483 mean train loss:  4.30447966e-01, bound:  3.61129344e-01\n",
      "Epoch: 2484 mean train loss:  4.30364996e-01, bound:  3.61116588e-01\n",
      "Epoch: 2485 mean train loss:  4.30281967e-01, bound:  3.61103922e-01\n",
      "Epoch: 2486 mean train loss:  4.30199265e-01, bound:  3.61091226e-01\n",
      "Epoch: 2487 mean train loss:  4.30116296e-01, bound:  3.61078531e-01\n",
      "Epoch: 2488 mean train loss:  4.30033267e-01, bound:  3.61065835e-01\n",
      "Epoch: 2489 mean train loss:  4.29950505e-01, bound:  3.61053079e-01\n",
      "Epoch: 2490 mean train loss:  4.29867953e-01, bound:  3.61040413e-01\n",
      "Epoch: 2491 mean train loss:  4.29785132e-01, bound:  3.61027688e-01\n",
      "Epoch: 2492 mean train loss:  4.29702461e-01, bound:  3.61014992e-01\n",
      "Epoch: 2493 mean train loss:  4.29619879e-01, bound:  3.61002266e-01\n",
      "Epoch: 2494 mean train loss:  4.29536849e-01, bound:  3.60989481e-01\n",
      "Epoch: 2495 mean train loss:  4.29454446e-01, bound:  3.60976815e-01\n",
      "Epoch: 2496 mean train loss:  4.29371297e-01, bound:  3.60964030e-01\n",
      "Epoch: 2497 mean train loss:  4.29289401e-01, bound:  3.60951334e-01\n",
      "Epoch: 2498 mean train loss:  4.29206908e-01, bound:  3.60938549e-01\n",
      "Epoch: 2499 mean train loss:  4.29124057e-01, bound:  3.60925853e-01\n",
      "Epoch: 2500 mean train loss:  4.29041952e-01, bound:  3.60913098e-01\n",
      "Epoch: 2501 mean train loss:  4.28959697e-01, bound:  3.60900372e-01\n",
      "Epoch: 2502 mean train loss:  4.28876996e-01, bound:  3.60887617e-01\n",
      "Epoch: 2503 mean train loss:  4.28794563e-01, bound:  3.60874891e-01\n",
      "Epoch: 2504 mean train loss:  4.28712159e-01, bound:  3.60862136e-01\n",
      "Epoch: 2505 mean train loss:  4.28629816e-01, bound:  3.60849321e-01\n",
      "Epoch: 2506 mean train loss:  4.28547621e-01, bound:  3.60836565e-01\n",
      "Epoch: 2507 mean train loss:  4.28465337e-01, bound:  3.60823750e-01\n",
      "Epoch: 2508 mean train loss:  4.28383321e-01, bound:  3.60810995e-01\n",
      "Epoch: 2509 mean train loss:  4.28300858e-01, bound:  3.60798210e-01\n",
      "Epoch: 2510 mean train loss:  4.28218961e-01, bound:  3.60785425e-01\n",
      "Epoch: 2511 mean train loss:  4.28137064e-01, bound:  3.60772640e-01\n",
      "Epoch: 2512 mean train loss:  4.28054720e-01, bound:  3.60759854e-01\n",
      "Epoch: 2513 mean train loss:  4.27972704e-01, bound:  3.60747129e-01\n",
      "Epoch: 2514 mean train loss:  4.27890599e-01, bound:  3.60734314e-01\n",
      "Epoch: 2515 mean train loss:  4.27808732e-01, bound:  3.60721469e-01\n",
      "Epoch: 2516 mean train loss:  4.27726686e-01, bound:  3.60708684e-01\n",
      "Epoch: 2517 mean train loss:  4.27644819e-01, bound:  3.60695839e-01\n",
      "Epoch: 2518 mean train loss:  4.27562714e-01, bound:  3.60682994e-01\n",
      "Epoch: 2519 mean train loss:  4.27480906e-01, bound:  3.60670209e-01\n",
      "Epoch: 2520 mean train loss:  4.27398920e-01, bound:  3.60657424e-01\n",
      "Epoch: 2521 mean train loss:  4.27316993e-01, bound:  3.60644579e-01\n",
      "Epoch: 2522 mean train loss:  4.27235484e-01, bound:  3.60631794e-01\n",
      "Epoch: 2523 mean train loss:  4.27153528e-01, bound:  3.60618949e-01\n",
      "Epoch: 2524 mean train loss:  4.27072108e-01, bound:  3.60606134e-01\n",
      "Epoch: 2525 mean train loss:  4.26990330e-01, bound:  3.60593349e-01\n",
      "Epoch: 2526 mean train loss:  4.26908433e-01, bound:  3.60580444e-01\n",
      "Epoch: 2527 mean train loss:  4.26826864e-01, bound:  3.60567629e-01\n",
      "Epoch: 2528 mean train loss:  4.26745057e-01, bound:  3.60554785e-01\n",
      "Epoch: 2529 mean train loss:  4.26663786e-01, bound:  3.60541910e-01\n",
      "Epoch: 2530 mean train loss:  4.26582396e-01, bound:  3.60529035e-01\n",
      "Epoch: 2531 mean train loss:  4.26500648e-01, bound:  3.60516191e-01\n",
      "Epoch: 2532 mean train loss:  4.26418900e-01, bound:  3.60503346e-01\n",
      "Epoch: 2533 mean train loss:  4.26337898e-01, bound:  3.60490471e-01\n",
      "Epoch: 2534 mean train loss:  4.26256120e-01, bound:  3.60477626e-01\n",
      "Epoch: 2535 mean train loss:  4.26174670e-01, bound:  3.60464722e-01\n",
      "Epoch: 2536 mean train loss:  4.26093310e-01, bound:  3.60451877e-01\n",
      "Epoch: 2537 mean train loss:  4.26011950e-01, bound:  3.60439003e-01\n",
      "Epoch: 2538 mean train loss:  4.25930709e-01, bound:  3.60426098e-01\n",
      "Epoch: 2539 mean train loss:  4.25849259e-01, bound:  3.60413224e-01\n",
      "Epoch: 2540 mean train loss:  4.25768226e-01, bound:  3.60400349e-01\n",
      "Epoch: 2541 mean train loss:  4.25686479e-01, bound:  3.60387474e-01\n",
      "Epoch: 2542 mean train loss:  4.25605416e-01, bound:  3.60374570e-01\n",
      "Epoch: 2543 mean train loss:  4.25524235e-01, bound:  3.60361636e-01\n",
      "Epoch: 2544 mean train loss:  4.25443172e-01, bound:  3.60348731e-01\n",
      "Epoch: 2545 mean train loss:  4.25361991e-01, bound:  3.60335857e-01\n",
      "Epoch: 2546 mean train loss:  4.25280929e-01, bound:  3.60322952e-01\n",
      "Epoch: 2547 mean train loss:  4.25199658e-01, bound:  3.60310018e-01\n",
      "Epoch: 2548 mean train loss:  4.25118685e-01, bound:  3.60297084e-01\n",
      "Epoch: 2549 mean train loss:  4.25037861e-01, bound:  3.60284179e-01\n",
      "Epoch: 2550 mean train loss:  4.24956501e-01, bound:  3.60271275e-01\n",
      "Epoch: 2551 mean train loss:  4.24875766e-01, bound:  3.60258311e-01\n",
      "Epoch: 2552 mean train loss:  4.24794853e-01, bound:  3.60245407e-01\n",
      "Epoch: 2553 mean train loss:  4.24714029e-01, bound:  3.60232443e-01\n",
      "Epoch: 2554 mean train loss:  4.24632967e-01, bound:  3.60219538e-01\n",
      "Epoch: 2555 mean train loss:  4.24552232e-01, bound:  3.60206604e-01\n",
      "Epoch: 2556 mean train loss:  4.24470931e-01, bound:  3.60193640e-01\n",
      "Epoch: 2557 mean train loss:  4.24390197e-01, bound:  3.60180676e-01\n",
      "Epoch: 2558 mean train loss:  4.24309433e-01, bound:  3.60167772e-01\n",
      "Epoch: 2559 mean train loss:  4.24228609e-01, bound:  3.60154778e-01\n",
      "Epoch: 2560 mean train loss:  4.24148053e-01, bound:  3.60141814e-01\n",
      "Epoch: 2561 mean train loss:  4.24067259e-01, bound:  3.60128880e-01\n",
      "Epoch: 2562 mean train loss:  4.23986673e-01, bound:  3.60115945e-01\n",
      "Epoch: 2563 mean train loss:  4.23906177e-01, bound:  3.60102952e-01\n",
      "Epoch: 2564 mean train loss:  4.23825175e-01, bound:  3.60089988e-01\n",
      "Epoch: 2565 mean train loss:  4.23744678e-01, bound:  3.60076994e-01\n",
      "Epoch: 2566 mean train loss:  4.23663974e-01, bound:  3.60064000e-01\n",
      "Epoch: 2567 mean train loss:  4.23583388e-01, bound:  3.60051036e-01\n",
      "Epoch: 2568 mean train loss:  4.23503131e-01, bound:  3.60038042e-01\n",
      "Epoch: 2569 mean train loss:  4.23422307e-01, bound:  3.60025048e-01\n",
      "Epoch: 2570 mean train loss:  4.23341870e-01, bound:  3.60012084e-01\n",
      "Epoch: 2571 mean train loss:  4.23261613e-01, bound:  3.59999061e-01\n",
      "Epoch: 2572 mean train loss:  4.23180789e-01, bound:  3.59986037e-01\n",
      "Epoch: 2573 mean train loss:  4.23100412e-01, bound:  3.59973103e-01\n",
      "Epoch: 2574 mean train loss:  4.23020095e-01, bound:  3.59960049e-01\n",
      "Epoch: 2575 mean train loss:  4.22939777e-01, bound:  3.59947056e-01\n",
      "Epoch: 2576 mean train loss:  4.22859520e-01, bound:  3.59934062e-01\n",
      "Epoch: 2577 mean train loss:  4.22779262e-01, bound:  3.59921008e-01\n",
      "Epoch: 2578 mean train loss:  4.22698677e-01, bound:  3.59908015e-01\n",
      "Epoch: 2579 mean train loss:  4.22618508e-01, bound:  3.59894991e-01\n",
      "Epoch: 2580 mean train loss:  4.22538161e-01, bound:  3.59881938e-01\n",
      "Epoch: 2581 mean train loss:  4.22458112e-01, bound:  3.59868944e-01\n",
      "Epoch: 2582 mean train loss:  4.22377914e-01, bound:  3.59855920e-01\n",
      "Epoch: 2583 mean train loss:  4.22297597e-01, bound:  3.59842896e-01\n",
      "Epoch: 2584 mean train loss:  4.22217518e-01, bound:  3.59829843e-01\n",
      "Epoch: 2585 mean train loss:  4.22137052e-01, bound:  3.59816819e-01\n",
      "Epoch: 2586 mean train loss:  4.22057241e-01, bound:  3.59803736e-01\n",
      "Epoch: 2587 mean train loss:  4.21977252e-01, bound:  3.59790683e-01\n",
      "Epoch: 2588 mean train loss:  4.21897143e-01, bound:  3.59777659e-01\n",
      "Epoch: 2589 mean train loss:  4.21816975e-01, bound:  3.59764576e-01\n",
      "Epoch: 2590 mean train loss:  4.21736956e-01, bound:  3.59751523e-01\n",
      "Epoch: 2591 mean train loss:  4.21656996e-01, bound:  3.59738499e-01\n",
      "Epoch: 2592 mean train loss:  4.21577036e-01, bound:  3.59725386e-01\n",
      "Epoch: 2593 mean train loss:  4.21496928e-01, bound:  3.59712303e-01\n",
      "Epoch: 2594 mean train loss:  4.21417028e-01, bound:  3.59699279e-01\n",
      "Epoch: 2595 mean train loss:  4.21337157e-01, bound:  3.59686166e-01\n",
      "Epoch: 2596 mean train loss:  4.21257347e-01, bound:  3.59673053e-01\n",
      "Epoch: 2597 mean train loss:  4.21177596e-01, bound:  3.59660000e-01\n",
      "Epoch: 2598 mean train loss:  4.21097547e-01, bound:  3.59646916e-01\n",
      "Epoch: 2599 mean train loss:  4.21017557e-01, bound:  3.59633833e-01\n",
      "Epoch: 2600 mean train loss:  4.20938045e-01, bound:  3.59620750e-01\n",
      "Epoch: 2601 mean train loss:  4.20858502e-01, bound:  3.59607637e-01\n",
      "Epoch: 2602 mean train loss:  4.20778722e-01, bound:  3.59594584e-01\n",
      "Epoch: 2603 mean train loss:  4.20698732e-01, bound:  3.59581500e-01\n",
      "Epoch: 2604 mean train loss:  4.20619071e-01, bound:  3.59568357e-01\n",
      "Epoch: 2605 mean train loss:  4.20539200e-01, bound:  3.59555274e-01\n",
      "Epoch: 2606 mean train loss:  4.20459539e-01, bound:  3.59542131e-01\n",
      "Epoch: 2607 mean train loss:  4.20380086e-01, bound:  3.59529018e-01\n",
      "Epoch: 2608 mean train loss:  4.20300543e-01, bound:  3.59515905e-01\n",
      "Epoch: 2609 mean train loss:  4.20220912e-01, bound:  3.59502792e-01\n",
      "Epoch: 2610 mean train loss:  4.20141280e-01, bound:  3.59489679e-01\n",
      "Epoch: 2611 mean train loss:  4.20061976e-01, bound:  3.59476537e-01\n",
      "Epoch: 2612 mean train loss:  4.19982195e-01, bound:  3.59463364e-01\n",
      "Epoch: 2613 mean train loss:  4.19902563e-01, bound:  3.59450281e-01\n",
      "Epoch: 2614 mean train loss:  4.19823080e-01, bound:  3.59437108e-01\n",
      "Epoch: 2615 mean train loss:  4.19743717e-01, bound:  3.59423965e-01\n",
      "Epoch: 2616 mean train loss:  4.19664115e-01, bound:  3.59410852e-01\n",
      "Epoch: 2617 mean train loss:  4.19584781e-01, bound:  3.59397680e-01\n",
      "Epoch: 2618 mean train loss:  4.19505447e-01, bound:  3.59384537e-01\n",
      "Epoch: 2619 mean train loss:  4.19426173e-01, bound:  3.59371424e-01\n",
      "Epoch: 2620 mean train loss:  4.19346392e-01, bound:  3.59358221e-01\n",
      "Epoch: 2621 mean train loss:  4.19267446e-01, bound:  3.59345108e-01\n",
      "Epoch: 2622 mean train loss:  4.19187993e-01, bound:  3.59331936e-01\n",
      "Epoch: 2623 mean train loss:  4.19108838e-01, bound:  3.59318793e-01\n",
      "Epoch: 2624 mean train loss:  4.19029355e-01, bound:  3.59305561e-01\n",
      "Epoch: 2625 mean train loss:  4.18949991e-01, bound:  3.59292418e-01\n",
      "Epoch: 2626 mean train loss:  4.18870747e-01, bound:  3.59279215e-01\n",
      "Epoch: 2627 mean train loss:  4.18791652e-01, bound:  3.59266043e-01\n",
      "Epoch: 2628 mean train loss:  4.18712378e-01, bound:  3.59252870e-01\n",
      "Epoch: 2629 mean train loss:  4.18633163e-01, bound:  3.59239668e-01\n",
      "Epoch: 2630 mean train loss:  4.18553948e-01, bound:  3.59226525e-01\n",
      "Epoch: 2631 mean train loss:  4.18474853e-01, bound:  3.59213352e-01\n",
      "Epoch: 2632 mean train loss:  4.18396026e-01, bound:  3.59200150e-01\n",
      "Epoch: 2633 mean train loss:  4.18316692e-01, bound:  3.59186947e-01\n",
      "Epoch: 2634 mean train loss:  4.18237597e-01, bound:  3.59173745e-01\n",
      "Epoch: 2635 mean train loss:  4.18158710e-01, bound:  3.59160542e-01\n",
      "Epoch: 2636 mean train loss:  4.18079406e-01, bound:  3.59147310e-01\n",
      "Epoch: 2637 mean train loss:  4.18000609e-01, bound:  3.59134138e-01\n",
      "Epoch: 2638 mean train loss:  4.17921573e-01, bound:  3.59120935e-01\n",
      "Epoch: 2639 mean train loss:  4.17842567e-01, bound:  3.59107733e-01\n",
      "Epoch: 2640 mean train loss:  4.17763591e-01, bound:  3.59094501e-01\n",
      "Epoch: 2641 mean train loss:  4.17684644e-01, bound:  3.59081268e-01\n",
      "Epoch: 2642 mean train loss:  4.17605668e-01, bound:  3.59068096e-01\n",
      "Epoch: 2643 mean train loss:  4.17526722e-01, bound:  3.59054863e-01\n",
      "Epoch: 2644 mean train loss:  4.17447686e-01, bound:  3.59041601e-01\n",
      "Epoch: 2645 mean train loss:  4.17368889e-01, bound:  3.59028399e-01\n",
      "Epoch: 2646 mean train loss:  4.17289764e-01, bound:  3.59015167e-01\n",
      "Epoch: 2647 mean train loss:  4.17211145e-01, bound:  3.59001935e-01\n",
      "Epoch: 2648 mean train loss:  4.17132139e-01, bound:  3.58988672e-01\n",
      "Epoch: 2649 mean train loss:  4.17053491e-01, bound:  3.58975410e-01\n",
      "Epoch: 2650 mean train loss:  4.16974694e-01, bound:  3.58962178e-01\n",
      "Epoch: 2651 mean train loss:  4.16895866e-01, bound:  3.58948916e-01\n",
      "Epoch: 2652 mean train loss:  4.16817069e-01, bound:  3.58935654e-01\n",
      "Epoch: 2653 mean train loss:  4.16738361e-01, bound:  3.58922482e-01\n",
      "Epoch: 2654 mean train loss:  4.16659802e-01, bound:  3.58909130e-01\n",
      "Epoch: 2655 mean train loss:  4.16581273e-01, bound:  3.58895928e-01\n",
      "Epoch: 2656 mean train loss:  4.16502416e-01, bound:  3.58882666e-01\n",
      "Epoch: 2657 mean train loss:  4.16423708e-01, bound:  3.58869404e-01\n",
      "Epoch: 2658 mean train loss:  4.16344941e-01, bound:  3.58856112e-01\n",
      "Epoch: 2659 mean train loss:  4.16266441e-01, bound:  3.58842909e-01\n",
      "Epoch: 2660 mean train loss:  4.16187912e-01, bound:  3.58829588e-01\n",
      "Epoch: 2661 mean train loss:  4.16109174e-01, bound:  3.58816326e-01\n",
      "Epoch: 2662 mean train loss:  4.16030765e-01, bound:  3.58803034e-01\n",
      "Epoch: 2663 mean train loss:  4.15952057e-01, bound:  3.58789742e-01\n",
      "Epoch: 2664 mean train loss:  4.15873289e-01, bound:  3.58776420e-01\n",
      "Epoch: 2665 mean train loss:  4.15794700e-01, bound:  3.58763158e-01\n",
      "Epoch: 2666 mean train loss:  4.15716261e-01, bound:  3.58749866e-01\n",
      "Epoch: 2667 mean train loss:  4.15637702e-01, bound:  3.58736604e-01\n",
      "Epoch: 2668 mean train loss:  4.15559232e-01, bound:  3.58723283e-01\n",
      "Epoch: 2669 mean train loss:  4.15481031e-01, bound:  3.58709991e-01\n",
      "Epoch: 2670 mean train loss:  4.15402323e-01, bound:  3.58696699e-01\n",
      "Epoch: 2671 mean train loss:  4.15323615e-01, bound:  3.58683378e-01\n",
      "Epoch: 2672 mean train loss:  4.15245384e-01, bound:  3.58670086e-01\n",
      "Epoch: 2673 mean train loss:  4.15167183e-01, bound:  3.58656764e-01\n",
      "Epoch: 2674 mean train loss:  4.15088892e-01, bound:  3.58643472e-01\n",
      "Epoch: 2675 mean train loss:  4.15010363e-01, bound:  3.58630151e-01\n",
      "Epoch: 2676 mean train loss:  4.14932013e-01, bound:  3.58616829e-01\n",
      "Epoch: 2677 mean train loss:  4.14853781e-01, bound:  3.58603567e-01\n",
      "Epoch: 2678 mean train loss:  4.14775223e-01, bound:  3.58590215e-01\n",
      "Epoch: 2679 mean train loss:  4.14697051e-01, bound:  3.58576894e-01\n",
      "Epoch: 2680 mean train loss:  4.14618641e-01, bound:  3.58563572e-01\n",
      "Epoch: 2681 mean train loss:  4.14540350e-01, bound:  3.58550251e-01\n",
      "Epoch: 2682 mean train loss:  4.14462239e-01, bound:  3.58536929e-01\n",
      "Epoch: 2683 mean train loss:  4.14383858e-01, bound:  3.58523577e-01\n",
      "Epoch: 2684 mean train loss:  4.14305598e-01, bound:  3.58510166e-01\n",
      "Epoch: 2685 mean train loss:  4.14227337e-01, bound:  3.58496875e-01\n",
      "Epoch: 2686 mean train loss:  4.14149284e-01, bound:  3.58483523e-01\n",
      "Epoch: 2687 mean train loss:  4.14070934e-01, bound:  3.58470201e-01\n",
      "Epoch: 2688 mean train loss:  4.13992882e-01, bound:  3.58456820e-01\n",
      "Epoch: 2689 mean train loss:  4.13914740e-01, bound:  3.58443469e-01\n",
      "Epoch: 2690 mean train loss:  4.13836628e-01, bound:  3.58430177e-01\n",
      "Epoch: 2691 mean train loss:  4.13758159e-01, bound:  3.58416796e-01\n",
      "Epoch: 2692 mean train loss:  4.13680226e-01, bound:  3.58403414e-01\n",
      "Epoch: 2693 mean train loss:  4.13602114e-01, bound:  3.58390063e-01\n",
      "Epoch: 2694 mean train loss:  4.13523883e-01, bound:  3.58376712e-01\n",
      "Epoch: 2695 mean train loss:  4.13445801e-01, bound:  3.58363360e-01\n",
      "Epoch: 2696 mean train loss:  4.13367897e-01, bound:  3.58349979e-01\n",
      "Epoch: 2697 mean train loss:  4.13289636e-01, bound:  3.58336598e-01\n",
      "Epoch: 2698 mean train loss:  4.13211703e-01, bound:  3.58323216e-01\n",
      "Epoch: 2699 mean train loss:  4.13133889e-01, bound:  3.58309865e-01\n",
      "Epoch: 2700 mean train loss:  4.13055807e-01, bound:  3.58296514e-01\n",
      "Epoch: 2701 mean train loss:  4.12977368e-01, bound:  3.58283103e-01\n",
      "Epoch: 2702 mean train loss:  4.12899554e-01, bound:  3.58269751e-01\n",
      "Epoch: 2703 mean train loss:  4.12821531e-01, bound:  3.58256340e-01\n",
      "Epoch: 2704 mean train loss:  4.12743837e-01, bound:  3.58242929e-01\n",
      "Epoch: 2705 mean train loss:  4.12665576e-01, bound:  3.58229548e-01\n",
      "Epoch: 2706 mean train loss:  4.12587821e-01, bound:  3.58216166e-01\n",
      "Epoch: 2707 mean train loss:  4.12509829e-01, bound:  3.58202785e-01\n",
      "Epoch: 2708 mean train loss:  4.12431657e-01, bound:  3.58189434e-01\n",
      "Epoch: 2709 mean train loss:  4.12353754e-01, bound:  3.58176023e-01\n",
      "Epoch: 2710 mean train loss:  4.12276059e-01, bound:  3.58162582e-01\n",
      "Epoch: 2711 mean train loss:  4.12198246e-01, bound:  3.58149201e-01\n",
      "Epoch: 2712 mean train loss:  4.12120253e-01, bound:  3.58135760e-01\n",
      "Epoch: 2713 mean train loss:  4.12042320e-01, bound:  3.58122349e-01\n",
      "Epoch: 2714 mean train loss:  4.11964595e-01, bound:  3.58108968e-01\n",
      "Epoch: 2715 mean train loss:  4.11886901e-01, bound:  3.58095586e-01\n",
      "Epoch: 2716 mean train loss:  4.11809057e-01, bound:  3.58082145e-01\n",
      "Epoch: 2717 mean train loss:  4.11731184e-01, bound:  3.58068734e-01\n",
      "Epoch: 2718 mean train loss:  4.11653161e-01, bound:  3.58055323e-01\n",
      "Epoch: 2719 mean train loss:  4.11575437e-01, bound:  3.58041942e-01\n",
      "Epoch: 2720 mean train loss:  4.11497802e-01, bound:  3.58028501e-01\n",
      "Epoch: 2721 mean train loss:  4.11420077e-01, bound:  3.58015031e-01\n",
      "Epoch: 2722 mean train loss:  4.11342084e-01, bound:  3.58001590e-01\n",
      "Epoch: 2723 mean train loss:  4.11264420e-01, bound:  3.57988209e-01\n",
      "Epoch: 2724 mean train loss:  4.11186695e-01, bound:  3.57974738e-01\n",
      "Epoch: 2725 mean train loss:  4.11109149e-01, bound:  3.57961327e-01\n",
      "Epoch: 2726 mean train loss:  4.11031336e-01, bound:  3.57947916e-01\n",
      "Epoch: 2727 mean train loss:  4.10953492e-01, bound:  3.57934505e-01\n",
      "Epoch: 2728 mean train loss:  4.10875887e-01, bound:  3.57921064e-01\n",
      "Epoch: 2729 mean train loss:  4.10798192e-01, bound:  3.57907593e-01\n",
      "Epoch: 2730 mean train loss:  4.10720736e-01, bound:  3.57894152e-01\n",
      "Epoch: 2731 mean train loss:  4.10642862e-01, bound:  3.57880682e-01\n",
      "Epoch: 2732 mean train loss:  4.10565078e-01, bound:  3.57867211e-01\n",
      "Epoch: 2733 mean train loss:  4.10487354e-01, bound:  3.57853770e-01\n",
      "Epoch: 2734 mean train loss:  4.10410106e-01, bound:  3.57840329e-01\n",
      "Epoch: 2735 mean train loss:  4.10332412e-01, bound:  3.57826918e-01\n",
      "Epoch: 2736 mean train loss:  4.10254866e-01, bound:  3.57813448e-01\n",
      "Epoch: 2737 mean train loss:  4.10177052e-01, bound:  3.57800007e-01\n",
      "Epoch: 2738 mean train loss:  4.10099477e-01, bound:  3.57786566e-01\n",
      "Epoch: 2739 mean train loss:  4.10021931e-01, bound:  3.57773095e-01\n",
      "Epoch: 2740 mean train loss:  4.09944087e-01, bound:  3.57759625e-01\n",
      "Epoch: 2741 mean train loss:  4.09866631e-01, bound:  3.57746154e-01\n",
      "Epoch: 2742 mean train loss:  4.09789175e-01, bound:  3.57732713e-01\n",
      "Epoch: 2743 mean train loss:  4.09711808e-01, bound:  3.57719213e-01\n",
      "Epoch: 2744 mean train loss:  4.09634143e-01, bound:  3.57705742e-01\n",
      "Epoch: 2745 mean train loss:  4.09556597e-01, bound:  3.57692301e-01\n",
      "Epoch: 2746 mean train loss:  4.09479141e-01, bound:  3.57678860e-01\n",
      "Epoch: 2747 mean train loss:  4.09401357e-01, bound:  3.57665360e-01\n",
      "Epoch: 2748 mean train loss:  4.09324050e-01, bound:  3.57651889e-01\n",
      "Epoch: 2749 mean train loss:  4.09246594e-01, bound:  3.57638359e-01\n",
      "Epoch: 2750 mean train loss:  4.09169078e-01, bound:  3.57624888e-01\n",
      "Epoch: 2751 mean train loss:  4.09091353e-01, bound:  3.57611388e-01\n",
      "Epoch: 2752 mean train loss:  4.09014136e-01, bound:  3.57597917e-01\n",
      "Epoch: 2753 mean train loss:  4.08936799e-01, bound:  3.57584447e-01\n",
      "Epoch: 2754 mean train loss:  4.08859074e-01, bound:  3.57570946e-01\n",
      "Epoch: 2755 mean train loss:  4.08781737e-01, bound:  3.57557505e-01\n",
      "Epoch: 2756 mean train loss:  4.08704281e-01, bound:  3.57543975e-01\n",
      "Epoch: 2757 mean train loss:  4.08626884e-01, bound:  3.57530445e-01\n",
      "Epoch: 2758 mean train loss:  4.08549458e-01, bound:  3.57516974e-01\n",
      "Epoch: 2759 mean train loss:  4.08471882e-01, bound:  3.57503474e-01\n",
      "Epoch: 2760 mean train loss:  4.08394456e-01, bound:  3.57489973e-01\n",
      "Epoch: 2761 mean train loss:  4.08317208e-01, bound:  3.57476473e-01\n",
      "Epoch: 2762 mean train loss:  4.08239365e-01, bound:  3.57462972e-01\n",
      "Epoch: 2763 mean train loss:  4.08162236e-01, bound:  3.57449502e-01\n",
      "Epoch: 2764 mean train loss:  4.08084869e-01, bound:  3.57436001e-01\n",
      "Epoch: 2765 mean train loss:  4.08007354e-01, bound:  3.57422471e-01\n",
      "Epoch: 2766 mean train loss:  4.07930225e-01, bound:  3.57408941e-01\n",
      "Epoch: 2767 mean train loss:  4.07852948e-01, bound:  3.57395470e-01\n",
      "Epoch: 2768 mean train loss:  4.07775491e-01, bound:  3.57381940e-01\n",
      "Epoch: 2769 mean train loss:  4.07697797e-01, bound:  3.57368439e-01\n",
      "Epoch: 2770 mean train loss:  4.07620668e-01, bound:  3.57354909e-01\n",
      "Epoch: 2771 mean train loss:  4.07543421e-01, bound:  3.57341409e-01\n",
      "Epoch: 2772 mean train loss:  4.07465935e-01, bound:  3.57327878e-01\n",
      "Epoch: 2773 mean train loss:  4.07388598e-01, bound:  3.57314348e-01\n",
      "Epoch: 2774 mean train loss:  4.07311410e-01, bound:  3.57300848e-01\n",
      "Epoch: 2775 mean train loss:  4.07233804e-01, bound:  3.57287347e-01\n",
      "Epoch: 2776 mean train loss:  4.07156736e-01, bound:  3.57273847e-01\n",
      "Epoch: 2777 mean train loss:  4.07079428e-01, bound:  3.57260287e-01\n",
      "Epoch: 2778 mean train loss:  4.07002151e-01, bound:  3.57246757e-01\n",
      "Epoch: 2779 mean train loss:  4.06924635e-01, bound:  3.57233256e-01\n",
      "Epoch: 2780 mean train loss:  4.06847566e-01, bound:  3.57219696e-01\n",
      "Epoch: 2781 mean train loss:  4.06770051e-01, bound:  3.57206136e-01\n",
      "Epoch: 2782 mean train loss:  4.06692892e-01, bound:  3.57192606e-01\n",
      "Epoch: 2783 mean train loss:  4.06615555e-01, bound:  3.57179075e-01\n",
      "Epoch: 2784 mean train loss:  4.06538427e-01, bound:  3.57165545e-01\n",
      "Epoch: 2785 mean train loss:  4.06461090e-01, bound:  3.57152015e-01\n",
      "Epoch: 2786 mean train loss:  4.06383604e-01, bound:  3.57138485e-01\n",
      "Epoch: 2787 mean train loss:  4.06306565e-01, bound:  3.57124984e-01\n",
      "Epoch: 2788 mean train loss:  4.06229109e-01, bound:  3.57111394e-01\n",
      "Epoch: 2789 mean train loss:  4.06152219e-01, bound:  3.57097894e-01\n",
      "Epoch: 2790 mean train loss:  4.06074703e-01, bound:  3.57084364e-01\n",
      "Epoch: 2791 mean train loss:  4.05997306e-01, bound:  3.57070804e-01\n",
      "Epoch: 2792 mean train loss:  4.05920088e-01, bound:  3.57057273e-01\n",
      "Epoch: 2793 mean train loss:  4.05842930e-01, bound:  3.57043684e-01\n",
      "Epoch: 2794 mean train loss:  4.05765772e-01, bound:  3.57030123e-01\n",
      "Epoch: 2795 mean train loss:  4.05688494e-01, bound:  3.57016623e-01\n",
      "Epoch: 2796 mean train loss:  4.05611038e-01, bound:  3.57003033e-01\n",
      "Epoch: 2797 mean train loss:  4.05534118e-01, bound:  3.56989533e-01\n",
      "Epoch: 2798 mean train loss:  4.05456454e-01, bound:  3.56975973e-01\n",
      "Epoch: 2799 mean train loss:  4.05379593e-01, bound:  3.56962413e-01\n",
      "Epoch: 2800 mean train loss:  4.05302465e-01, bound:  3.56948823e-01\n",
      "Epoch: 2801 mean train loss:  4.05225128e-01, bound:  3.56935292e-01\n",
      "Epoch: 2802 mean train loss:  4.05147970e-01, bound:  3.56921762e-01\n",
      "Epoch: 2803 mean train loss:  4.05070990e-01, bound:  3.56908143e-01\n",
      "Epoch: 2804 mean train loss:  4.04993504e-01, bound:  3.56894583e-01\n",
      "Epoch: 2805 mean train loss:  4.04916286e-01, bound:  3.56881022e-01\n",
      "Epoch: 2806 mean train loss:  4.04839039e-01, bound:  3.56867522e-01\n",
      "Epoch: 2807 mean train loss:  4.04761910e-01, bound:  3.56853932e-01\n",
      "Epoch: 2808 mean train loss:  4.04684603e-01, bound:  3.56840372e-01\n",
      "Epoch: 2809 mean train loss:  4.04607415e-01, bound:  3.56826812e-01\n",
      "Epoch: 2810 mean train loss:  4.04530317e-01, bound:  3.56813252e-01\n",
      "Epoch: 2811 mean train loss:  4.04452890e-01, bound:  3.56799662e-01\n",
      "Epoch: 2812 mean train loss:  4.04375792e-01, bound:  3.56786102e-01\n",
      "Epoch: 2813 mean train loss:  4.04298961e-01, bound:  3.56772512e-01\n",
      "Epoch: 2814 mean train loss:  4.04221296e-01, bound:  3.56758952e-01\n",
      "Epoch: 2815 mean train loss:  4.04144228e-01, bound:  3.56745392e-01\n",
      "Epoch: 2816 mean train loss:  4.04067039e-01, bound:  3.56731832e-01\n",
      "Epoch: 2817 mean train loss:  4.03990000e-01, bound:  3.56718272e-01\n",
      "Epoch: 2818 mean train loss:  4.03912783e-01, bound:  3.56704682e-01\n",
      "Epoch: 2819 mean train loss:  4.03835207e-01, bound:  3.56691092e-01\n",
      "Epoch: 2820 mean train loss:  4.03758258e-01, bound:  3.56677502e-01\n",
      "Epoch: 2821 mean train loss:  4.03681040e-01, bound:  3.56663942e-01\n",
      "Epoch: 2822 mean train loss:  4.03603911e-01, bound:  3.56650382e-01\n",
      "Epoch: 2823 mean train loss:  4.03526574e-01, bound:  3.56636792e-01\n",
      "Epoch: 2824 mean train loss:  4.03449416e-01, bound:  3.56623232e-01\n",
      "Epoch: 2825 mean train loss:  4.03371871e-01, bound:  3.56609613e-01\n",
      "Epoch: 2826 mean train loss:  4.03294891e-01, bound:  3.56596082e-01\n",
      "Epoch: 2827 mean train loss:  4.03217673e-01, bound:  3.56582463e-01\n",
      "Epoch: 2828 mean train loss:  4.03140575e-01, bound:  3.56568903e-01\n",
      "Epoch: 2829 mean train loss:  4.03063029e-01, bound:  3.56555283e-01\n",
      "Epoch: 2830 mean train loss:  4.02986288e-01, bound:  3.56541723e-01\n",
      "Epoch: 2831 mean train loss:  4.02908713e-01, bound:  3.56528103e-01\n",
      "Epoch: 2832 mean train loss:  4.02831763e-01, bound:  3.56514513e-01\n",
      "Epoch: 2833 mean train loss:  4.02754486e-01, bound:  3.56500983e-01\n",
      "Epoch: 2834 mean train loss:  4.02677387e-01, bound:  3.56487364e-01\n",
      "Epoch: 2835 mean train loss:  4.02600169e-01, bound:  3.56473774e-01\n",
      "Epoch: 2836 mean train loss:  4.02522743e-01, bound:  3.56460184e-01\n",
      "Epoch: 2837 mean train loss:  4.02445614e-01, bound:  3.56446594e-01\n",
      "Epoch: 2838 mean train loss:  4.02368367e-01, bound:  3.56433004e-01\n",
      "Epoch: 2839 mean train loss:  4.02291149e-01, bound:  3.56419384e-01\n",
      "Epoch: 2840 mean train loss:  4.02214110e-01, bound:  3.56405824e-01\n",
      "Epoch: 2841 mean train loss:  4.02136832e-01, bound:  3.56392235e-01\n",
      "Epoch: 2842 mean train loss:  4.02059585e-01, bound:  3.56378645e-01\n",
      "Epoch: 2843 mean train loss:  4.01982188e-01, bound:  3.56365085e-01\n",
      "Epoch: 2844 mean train loss:  4.01905179e-01, bound:  3.56351435e-01\n",
      "Epoch: 2845 mean train loss:  4.01827723e-01, bound:  3.56337875e-01\n",
      "Epoch: 2846 mean train loss:  4.01750833e-01, bound:  3.56324315e-01\n",
      "Epoch: 2847 mean train loss:  4.01673317e-01, bound:  3.56310636e-01\n",
      "Epoch: 2848 mean train loss:  4.01595920e-01, bound:  3.56297046e-01\n",
      "Epoch: 2849 mean train loss:  4.01518703e-01, bound:  3.56283456e-01\n",
      "Epoch: 2850 mean train loss:  4.01441902e-01, bound:  3.56269836e-01\n",
      "Epoch: 2851 mean train loss:  4.01364535e-01, bound:  3.56256276e-01\n",
      "Epoch: 2852 mean train loss:  4.01286930e-01, bound:  3.56242716e-01\n",
      "Epoch: 2853 mean train loss:  4.01210010e-01, bound:  3.56229097e-01\n",
      "Epoch: 2854 mean train loss:  4.01132613e-01, bound:  3.56215477e-01\n",
      "Epoch: 2855 mean train loss:  4.01055276e-01, bound:  3.56201887e-01\n",
      "Epoch: 2856 mean train loss:  4.00978059e-01, bound:  3.56188267e-01\n",
      "Epoch: 2857 mean train loss:  4.00900632e-01, bound:  3.56174618e-01\n",
      "Epoch: 2858 mean train loss:  4.00823176e-01, bound:  3.56161028e-01\n",
      "Epoch: 2859 mean train loss:  4.00746047e-01, bound:  3.56147438e-01\n",
      "Epoch: 2860 mean train loss:  4.00668949e-01, bound:  3.56133848e-01\n",
      "Epoch: 2861 mean train loss:  4.00591612e-01, bound:  3.56120259e-01\n",
      "Epoch: 2862 mean train loss:  4.00514305e-01, bound:  3.56106669e-01\n",
      "Epoch: 2863 mean train loss:  4.00437027e-01, bound:  3.56093079e-01\n",
      "Epoch: 2864 mean train loss:  4.00359750e-01, bound:  3.56079459e-01\n",
      "Epoch: 2865 mean train loss:  4.00282383e-01, bound:  3.56065840e-01\n",
      "Epoch: 2866 mean train loss:  4.00204897e-01, bound:  3.56052250e-01\n",
      "Epoch: 2867 mean train loss:  4.00127530e-01, bound:  3.56038600e-01\n",
      "Epoch: 2868 mean train loss:  4.00050133e-01, bound:  3.56025010e-01\n",
      "Epoch: 2869 mean train loss:  3.99972975e-01, bound:  3.56011391e-01\n",
      "Epoch: 2870 mean train loss:  3.99895698e-01, bound:  3.55997801e-01\n",
      "Epoch: 2871 mean train loss:  3.99818093e-01, bound:  3.55984211e-01\n",
      "Epoch: 2872 mean train loss:  3.99740487e-01, bound:  3.55970591e-01\n",
      "Epoch: 2873 mean train loss:  3.99663627e-01, bound:  3.55956972e-01\n",
      "Epoch: 2874 mean train loss:  3.99585843e-01, bound:  3.55943352e-01\n",
      "Epoch: 2875 mean train loss:  3.99508655e-01, bound:  3.55929762e-01\n",
      "Epoch: 2876 mean train loss:  3.99431348e-01, bound:  3.55916142e-01\n",
      "Epoch: 2877 mean train loss:  3.99353623e-01, bound:  3.55902493e-01\n",
      "Epoch: 2878 mean train loss:  3.99276316e-01, bound:  3.55888903e-01\n",
      "Epoch: 2879 mean train loss:  3.99199009e-01, bound:  3.55875283e-01\n",
      "Epoch: 2880 mean train loss:  3.99121702e-01, bound:  3.55861664e-01\n",
      "Epoch: 2881 mean train loss:  3.99044186e-01, bound:  3.55848044e-01\n",
      "Epoch: 2882 mean train loss:  3.98966759e-01, bound:  3.55834424e-01\n",
      "Epoch: 2883 mean train loss:  3.98889184e-01, bound:  3.55820835e-01\n",
      "Epoch: 2884 mean train loss:  3.98811817e-01, bound:  3.55807215e-01\n",
      "Epoch: 2885 mean train loss:  3.98734242e-01, bound:  3.55793625e-01\n",
      "Epoch: 2886 mean train loss:  3.98656666e-01, bound:  3.55780005e-01\n",
      "Epoch: 2887 mean train loss:  3.98579538e-01, bound:  3.55766386e-01\n",
      "Epoch: 2888 mean train loss:  3.98502052e-01, bound:  3.55752766e-01\n",
      "Epoch: 2889 mean train loss:  3.98424327e-01, bound:  3.55739176e-01\n",
      "Epoch: 2890 mean train loss:  3.98346990e-01, bound:  3.55725557e-01\n",
      "Epoch: 2891 mean train loss:  3.98269176e-01, bound:  3.55711937e-01\n",
      "Epoch: 2892 mean train loss:  3.98191869e-01, bound:  3.55698317e-01\n",
      "Epoch: 2893 mean train loss:  3.98114324e-01, bound:  3.55684698e-01\n",
      "Epoch: 2894 mean train loss:  3.98036808e-01, bound:  3.55671048e-01\n",
      "Epoch: 2895 mean train loss:  3.97959113e-01, bound:  3.55657488e-01\n",
      "Epoch: 2896 mean train loss:  3.97881478e-01, bound:  3.55643839e-01\n",
      "Epoch: 2897 mean train loss:  3.97803932e-01, bound:  3.55630219e-01\n",
      "Epoch: 2898 mean train loss:  3.97726238e-01, bound:  3.55616599e-01\n",
      "Epoch: 2899 mean train loss:  3.97648901e-01, bound:  3.55602980e-01\n",
      "Epoch: 2900 mean train loss:  3.97571117e-01, bound:  3.55589330e-01\n",
      "Epoch: 2901 mean train loss:  3.97493422e-01, bound:  3.55575770e-01\n",
      "Epoch: 2902 mean train loss:  3.97415727e-01, bound:  3.55562180e-01\n",
      "Epoch: 2903 mean train loss:  3.97338241e-01, bound:  3.55548531e-01\n",
      "Epoch: 2904 mean train loss:  3.97260636e-01, bound:  3.55534911e-01\n",
      "Epoch: 2905 mean train loss:  3.97183090e-01, bound:  3.55521321e-01\n",
      "Epoch: 2906 mean train loss:  3.97105306e-01, bound:  3.55507702e-01\n",
      "Epoch: 2907 mean train loss:  3.97027612e-01, bound:  3.55494082e-01\n",
      "Epoch: 2908 mean train loss:  3.96949798e-01, bound:  3.55480462e-01\n",
      "Epoch: 2909 mean train loss:  3.96872342e-01, bound:  3.55466783e-01\n",
      "Epoch: 2910 mean train loss:  3.96794587e-01, bound:  3.55453223e-01\n",
      "Epoch: 2911 mean train loss:  3.96716744e-01, bound:  3.55439574e-01\n",
      "Epoch: 2912 mean train loss:  3.96638811e-01, bound:  3.55425924e-01\n",
      "Epoch: 2913 mean train loss:  3.96561265e-01, bound:  3.55412334e-01\n",
      "Epoch: 2914 mean train loss:  3.96483421e-01, bound:  3.55398715e-01\n",
      "Epoch: 2915 mean train loss:  3.96405697e-01, bound:  3.55385125e-01\n",
      "Epoch: 2916 mean train loss:  3.96327764e-01, bound:  3.55371565e-01\n",
      "Epoch: 2917 mean train loss:  3.96249920e-01, bound:  3.55357945e-01\n",
      "Epoch: 2918 mean train loss:  3.96172047e-01, bound:  3.55344266e-01\n",
      "Epoch: 2919 mean train loss:  3.96094233e-01, bound:  3.55330616e-01\n",
      "Epoch: 2920 mean train loss:  3.96016210e-01, bound:  3.55317026e-01\n",
      "Epoch: 2921 mean train loss:  3.95938784e-01, bound:  3.55303407e-01\n",
      "Epoch: 2922 mean train loss:  3.95860910e-01, bound:  3.55289787e-01\n",
      "Epoch: 2923 mean train loss:  3.95782977e-01, bound:  3.55276167e-01\n",
      "Epoch: 2924 mean train loss:  3.95704925e-01, bound:  3.55262578e-01\n",
      "Epoch: 2925 mean train loss:  3.95626754e-01, bound:  3.55248958e-01\n",
      "Epoch: 2926 mean train loss:  3.95549238e-01, bound:  3.55235338e-01\n",
      "Epoch: 2927 mean train loss:  3.95471007e-01, bound:  3.55221719e-01\n",
      "Epoch: 2928 mean train loss:  3.95392835e-01, bound:  3.55208039e-01\n",
      "Epoch: 2929 mean train loss:  3.95314932e-01, bound:  3.55194479e-01\n",
      "Epoch: 2930 mean train loss:  3.95237356e-01, bound:  3.55180830e-01\n",
      "Epoch: 2931 mean train loss:  3.95159096e-01, bound:  3.55167240e-01\n",
      "Epoch: 2932 mean train loss:  3.95080835e-01, bound:  3.55153650e-01\n",
      "Epoch: 2933 mean train loss:  3.95003170e-01, bound:  3.55140030e-01\n",
      "Epoch: 2934 mean train loss:  3.94924998e-01, bound:  3.55126381e-01\n",
      "Epoch: 2935 mean train loss:  3.94846976e-01, bound:  3.55112761e-01\n",
      "Epoch: 2936 mean train loss:  3.94768775e-01, bound:  3.55099142e-01\n",
      "Epoch: 2937 mean train loss:  3.94690752e-01, bound:  3.55085462e-01\n",
      "Epoch: 2938 mean train loss:  3.94612461e-01, bound:  3.55071902e-01\n",
      "Epoch: 2939 mean train loss:  3.94534409e-01, bound:  3.55058312e-01\n",
      "Epoch: 2940 mean train loss:  3.94456208e-01, bound:  3.55044693e-01\n",
      "Epoch: 2941 mean train loss:  3.94378185e-01, bound:  3.55031103e-01\n",
      "Epoch: 2942 mean train loss:  3.94299954e-01, bound:  3.55017453e-01\n",
      "Epoch: 2943 mean train loss:  3.94221753e-01, bound:  3.55003834e-01\n",
      "Epoch: 2944 mean train loss:  3.94143373e-01, bound:  3.54990244e-01\n",
      "Epoch: 2945 mean train loss:  3.94065410e-01, bound:  3.54976594e-01\n",
      "Epoch: 2946 mean train loss:  3.93987060e-01, bound:  3.54963005e-01\n",
      "Epoch: 2947 mean train loss:  3.93908769e-01, bound:  3.54949415e-01\n",
      "Epoch: 2948 mean train loss:  3.93830091e-01, bound:  3.54935795e-01\n",
      "Epoch: 2949 mean train loss:  3.93751889e-01, bound:  3.54922175e-01\n",
      "Epoch: 2950 mean train loss:  3.93673539e-01, bound:  3.54908556e-01\n",
      "Epoch: 2951 mean train loss:  3.93595397e-01, bound:  3.54894936e-01\n",
      "Epoch: 2952 mean train loss:  3.93517017e-01, bound:  3.54881316e-01\n",
      "Epoch: 2953 mean train loss:  3.93438309e-01, bound:  3.54867697e-01\n",
      "Epoch: 2954 mean train loss:  3.93360347e-01, bound:  3.54854107e-01\n",
      "Epoch: 2955 mean train loss:  3.93281817e-01, bound:  3.54840487e-01\n",
      "Epoch: 2956 mean train loss:  3.93203348e-01, bound:  3.54826897e-01\n",
      "Epoch: 2957 mean train loss:  3.93124640e-01, bound:  3.54813278e-01\n",
      "Epoch: 2958 mean train loss:  3.93046230e-01, bound:  3.54799658e-01\n",
      "Epoch: 2959 mean train loss:  3.92967910e-01, bound:  3.54786009e-01\n",
      "Epoch: 2960 mean train loss:  3.92889082e-01, bound:  3.54772389e-01\n",
      "Epoch: 2961 mean train loss:  3.92810553e-01, bound:  3.54758829e-01\n",
      "Epoch: 2962 mean train loss:  3.92732292e-01, bound:  3.54745179e-01\n",
      "Epoch: 2963 mean train loss:  3.92653584e-01, bound:  3.54731590e-01\n",
      "Epoch: 2964 mean train loss:  3.92574996e-01, bound:  3.54718000e-01\n",
      "Epoch: 2965 mean train loss:  3.92496407e-01, bound:  3.54704380e-01\n",
      "Epoch: 2966 mean train loss:  3.92417550e-01, bound:  3.54690760e-01\n",
      "Epoch: 2967 mean train loss:  3.92338842e-01, bound:  3.54677141e-01\n",
      "Epoch: 2968 mean train loss:  3.92260224e-01, bound:  3.54663521e-01\n",
      "Epoch: 2969 mean train loss:  3.92181307e-01, bound:  3.54649901e-01\n",
      "Epoch: 2970 mean train loss:  3.92102689e-01, bound:  3.54636312e-01\n",
      "Epoch: 2971 mean train loss:  3.92023951e-01, bound:  3.54622751e-01\n",
      "Epoch: 2972 mean train loss:  3.91945392e-01, bound:  3.54609102e-01\n",
      "Epoch: 2973 mean train loss:  3.91866624e-01, bound:  3.54595482e-01\n",
      "Epoch: 2974 mean train loss:  3.91787618e-01, bound:  3.54581922e-01\n",
      "Epoch: 2975 mean train loss:  3.91709000e-01, bound:  3.54568273e-01\n",
      "Epoch: 2976 mean train loss:  3.91629845e-01, bound:  3.54554713e-01\n",
      "Epoch: 2977 mean train loss:  3.91551107e-01, bound:  3.54541093e-01\n",
      "Epoch: 2978 mean train loss:  3.91471982e-01, bound:  3.54527473e-01\n",
      "Epoch: 2979 mean train loss:  3.91393155e-01, bound:  3.54513913e-01\n",
      "Epoch: 2980 mean train loss:  3.91314298e-01, bound:  3.54500294e-01\n",
      "Epoch: 2981 mean train loss:  3.91235352e-01, bound:  3.54486674e-01\n",
      "Epoch: 2982 mean train loss:  3.91156346e-01, bound:  3.54473054e-01\n",
      "Epoch: 2983 mean train loss:  3.91076982e-01, bound:  3.54459405e-01\n",
      "Epoch: 2984 mean train loss:  3.90998155e-01, bound:  3.54445815e-01\n",
      "Epoch: 2985 mean train loss:  3.90919179e-01, bound:  3.54432255e-01\n",
      "Epoch: 2986 mean train loss:  3.90840083e-01, bound:  3.54418635e-01\n",
      "Epoch: 2987 mean train loss:  3.90760809e-01, bound:  3.54405075e-01\n",
      "Epoch: 2988 mean train loss:  3.90681565e-01, bound:  3.54391426e-01\n",
      "Epoch: 2989 mean train loss:  3.90602469e-01, bound:  3.54377836e-01\n",
      "Epoch: 2990 mean train loss:  3.90523523e-01, bound:  3.54364246e-01\n",
      "Epoch: 2991 mean train loss:  3.90443891e-01, bound:  3.54350626e-01\n",
      "Epoch: 2992 mean train loss:  3.90364796e-01, bound:  3.54337007e-01\n",
      "Epoch: 2993 mean train loss:  3.90285492e-01, bound:  3.54323417e-01\n",
      "Epoch: 2994 mean train loss:  3.90206099e-01, bound:  3.54309827e-01\n",
      "Epoch: 2995 mean train loss:  3.90126973e-01, bound:  3.54296237e-01\n",
      "Epoch: 2996 mean train loss:  3.90047312e-01, bound:  3.54282647e-01\n",
      "Epoch: 2997 mean train loss:  3.89967978e-01, bound:  3.54269028e-01\n",
      "Epoch: 2998 mean train loss:  3.89888912e-01, bound:  3.54255438e-01\n",
      "Epoch: 2999 mean train loss:  3.89809370e-01, bound:  3.54241848e-01\n",
      "Epoch: 3000 mean train loss:  3.89729828e-01, bound:  3.54228199e-01\n",
      "Epoch: 3001 mean train loss:  3.89650255e-01, bound:  3.54214668e-01\n",
      "Epoch: 3002 mean train loss:  3.89570802e-01, bound:  3.54201108e-01\n",
      "Epoch: 3003 mean train loss:  3.89491111e-01, bound:  3.54187489e-01\n",
      "Epoch: 3004 mean train loss:  3.89411628e-01, bound:  3.54173869e-01\n",
      "Epoch: 3005 mean train loss:  3.89332175e-01, bound:  3.54160279e-01\n",
      "Epoch: 3006 mean train loss:  3.89252543e-01, bound:  3.54146659e-01\n",
      "Epoch: 3007 mean train loss:  3.89172673e-01, bound:  3.54133070e-01\n",
      "Epoch: 3008 mean train loss:  3.89093071e-01, bound:  3.54119509e-01\n",
      "Epoch: 3009 mean train loss:  3.89013529e-01, bound:  3.54105920e-01\n",
      "Epoch: 3010 mean train loss:  3.88933718e-01, bound:  3.54092300e-01\n",
      "Epoch: 3011 mean train loss:  3.88854027e-01, bound:  3.54078740e-01\n",
      "Epoch: 3012 mean train loss:  3.88774037e-01, bound:  3.54065120e-01\n",
      "Epoch: 3013 mean train loss:  3.88694286e-01, bound:  3.54051530e-01\n",
      "Epoch: 3014 mean train loss:  3.88614357e-01, bound:  3.54037970e-01\n",
      "Epoch: 3015 mean train loss:  3.88534486e-01, bound:  3.54024380e-01\n",
      "Epoch: 3016 mean train loss:  3.88454556e-01, bound:  3.54010791e-01\n",
      "Epoch: 3017 mean train loss:  3.88374716e-01, bound:  3.53997171e-01\n",
      "Epoch: 3018 mean train loss:  3.88294816e-01, bound:  3.53983611e-01\n",
      "Epoch: 3019 mean train loss:  3.88214767e-01, bound:  3.53970021e-01\n",
      "Epoch: 3020 mean train loss:  3.88134539e-01, bound:  3.53956431e-01\n",
      "Epoch: 3021 mean train loss:  3.88054729e-01, bound:  3.53942871e-01\n",
      "Epoch: 3022 mean train loss:  3.87974828e-01, bound:  3.53929311e-01\n",
      "Epoch: 3023 mean train loss:  3.87894422e-01, bound:  3.53915691e-01\n",
      "Epoch: 3024 mean train loss:  3.87814075e-01, bound:  3.53902102e-01\n",
      "Epoch: 3025 mean train loss:  3.87734085e-01, bound:  3.53888512e-01\n",
      "Epoch: 3026 mean train loss:  3.87654066e-01, bound:  3.53874922e-01\n",
      "Epoch: 3027 mean train loss:  3.87573510e-01, bound:  3.53861332e-01\n",
      "Epoch: 3028 mean train loss:  3.87493491e-01, bound:  3.53847742e-01\n",
      "Epoch: 3029 mean train loss:  3.87413085e-01, bound:  3.53834182e-01\n",
      "Epoch: 3030 mean train loss:  3.87332797e-01, bound:  3.53820622e-01\n",
      "Epoch: 3031 mean train loss:  3.87252569e-01, bound:  3.53807032e-01\n",
      "Epoch: 3032 mean train loss:  3.87171894e-01, bound:  3.53793442e-01\n",
      "Epoch: 3033 mean train loss:  3.87091458e-01, bound:  3.53779882e-01\n",
      "Epoch: 3034 mean train loss:  3.87011409e-01, bound:  3.53766233e-01\n",
      "Epoch: 3035 mean train loss:  3.86930645e-01, bound:  3.53752673e-01\n",
      "Epoch: 3036 mean train loss:  3.86850059e-01, bound:  3.53739113e-01\n",
      "Epoch: 3037 mean train loss:  3.86769414e-01, bound:  3.53725553e-01\n",
      "Epoch: 3038 mean train loss:  3.86688977e-01, bound:  3.53711993e-01\n",
      "Epoch: 3039 mean train loss:  3.86608243e-01, bound:  3.53698462e-01\n",
      "Epoch: 3040 mean train loss:  3.86527479e-01, bound:  3.53684843e-01\n",
      "Epoch: 3041 mean train loss:  3.86447012e-01, bound:  3.53671253e-01\n",
      "Epoch: 3042 mean train loss:  3.86366099e-01, bound:  3.53657693e-01\n",
      "Epoch: 3043 mean train loss:  3.86285454e-01, bound:  3.53644103e-01\n",
      "Epoch: 3044 mean train loss:  3.86204630e-01, bound:  3.53630573e-01\n",
      "Epoch: 3045 mean train loss:  3.86123896e-01, bound:  3.53617013e-01\n",
      "Epoch: 3046 mean train loss:  3.86042833e-01, bound:  3.53603363e-01\n",
      "Epoch: 3047 mean train loss:  3.85962129e-01, bound:  3.53589863e-01\n",
      "Epoch: 3048 mean train loss:  3.85881126e-01, bound:  3.53576243e-01\n",
      "Epoch: 3049 mean train loss:  3.85799974e-01, bound:  3.53562683e-01\n",
      "Epoch: 3050 mean train loss:  3.85718912e-01, bound:  3.53549123e-01\n",
      "Epoch: 3051 mean train loss:  3.85638028e-01, bound:  3.53535533e-01\n",
      "Epoch: 3052 mean train loss:  3.85557055e-01, bound:  3.53521973e-01\n",
      "Epoch: 3053 mean train loss:  3.85475993e-01, bound:  3.53508383e-01\n",
      "Epoch: 3054 mean train loss:  3.85394722e-01, bound:  3.53494823e-01\n",
      "Epoch: 3055 mean train loss:  3.85313421e-01, bound:  3.53481323e-01\n",
      "Epoch: 3056 mean train loss:  3.85232389e-01, bound:  3.53467703e-01\n",
      "Epoch: 3057 mean train loss:  3.85151148e-01, bound:  3.53454173e-01\n",
      "Epoch: 3058 mean train loss:  3.85069668e-01, bound:  3.53440642e-01\n",
      "Epoch: 3059 mean train loss:  3.84988546e-01, bound:  3.53427052e-01\n",
      "Epoch: 3060 mean train loss:  3.84907365e-01, bound:  3.53413522e-01\n",
      "Epoch: 3061 mean train loss:  3.84825796e-01, bound:  3.53399932e-01\n",
      "Epoch: 3062 mean train loss:  3.84744555e-01, bound:  3.53386372e-01\n",
      "Epoch: 3063 mean train loss:  3.84662926e-01, bound:  3.53372842e-01\n",
      "Epoch: 3064 mean train loss:  3.84581476e-01, bound:  3.53359222e-01\n",
      "Epoch: 3065 mean train loss:  3.84499967e-01, bound:  3.53345692e-01\n",
      "Epoch: 3066 mean train loss:  3.84418219e-01, bound:  3.53332162e-01\n",
      "Epoch: 3067 mean train loss:  3.84336531e-01, bound:  3.53318602e-01\n",
      "Epoch: 3068 mean train loss:  3.84254903e-01, bound:  3.53305012e-01\n",
      "Epoch: 3069 mean train loss:  3.84173274e-01, bound:  3.53291482e-01\n",
      "Epoch: 3070 mean train loss:  3.84091496e-01, bound:  3.53277951e-01\n",
      "Epoch: 3071 mean train loss:  3.84009659e-01, bound:  3.53264362e-01\n",
      "Epoch: 3072 mean train loss:  3.83927852e-01, bound:  3.53250802e-01\n",
      "Epoch: 3073 mean train loss:  3.83846343e-01, bound:  3.53237271e-01\n",
      "Epoch: 3074 mean train loss:  3.83764178e-01, bound:  3.53223711e-01\n",
      "Epoch: 3075 mean train loss:  3.83681983e-01, bound:  3.53210181e-01\n",
      "Epoch: 3076 mean train loss:  3.83599877e-01, bound:  3.53196621e-01\n",
      "Epoch: 3077 mean train loss:  3.83517951e-01, bound:  3.53183061e-01\n",
      "Epoch: 3078 mean train loss:  3.83435905e-01, bound:  3.53169531e-01\n",
      "Epoch: 3079 mean train loss:  3.83353978e-01, bound:  3.53155971e-01\n",
      "Epoch: 3080 mean train loss:  3.83271486e-01, bound:  3.53142440e-01\n",
      "Epoch: 3081 mean train loss:  3.83189470e-01, bound:  3.53128850e-01\n",
      "Epoch: 3082 mean train loss:  3.83107334e-01, bound:  3.53115290e-01\n",
      "Epoch: 3083 mean train loss:  3.83025110e-01, bound:  3.53101760e-01\n",
      "Epoch: 3084 mean train loss:  3.82942587e-01, bound:  3.53088230e-01\n",
      "Epoch: 3085 mean train loss:  3.82860482e-01, bound:  3.53074670e-01\n",
      "Epoch: 3086 mean train loss:  3.82778138e-01, bound:  3.53061199e-01\n",
      "Epoch: 3087 mean train loss:  3.82695317e-01, bound:  3.53047639e-01\n",
      "Epoch: 3088 mean train loss:  3.82612944e-01, bound:  3.53034109e-01\n",
      "Epoch: 3089 mean train loss:  3.82530451e-01, bound:  3.53020549e-01\n",
      "Epoch: 3090 mean train loss:  3.82448018e-01, bound:  3.53007019e-01\n",
      "Epoch: 3091 mean train loss:  3.82365674e-01, bound:  3.52993488e-01\n",
      "Epoch: 3092 mean train loss:  3.82282645e-01, bound:  3.52979988e-01\n",
      "Epoch: 3093 mean train loss:  3.82200092e-01, bound:  3.52966428e-01\n",
      "Epoch: 3094 mean train loss:  3.82117033e-01, bound:  3.52952838e-01\n",
      "Epoch: 3095 mean train loss:  3.82034719e-01, bound:  3.52939367e-01\n",
      "Epoch: 3096 mean train loss:  3.81951720e-01, bound:  3.52925777e-01\n",
      "Epoch: 3097 mean train loss:  3.81868899e-01, bound:  3.52912247e-01\n",
      "Epoch: 3098 mean train loss:  3.81785870e-01, bound:  3.52898747e-01\n",
      "Epoch: 3099 mean train loss:  3.81703138e-01, bound:  3.52885216e-01\n",
      "Epoch: 3100 mean train loss:  3.81620109e-01, bound:  3.52871656e-01\n",
      "Epoch: 3101 mean train loss:  3.81537050e-01, bound:  3.52858156e-01\n",
      "Epoch: 3102 mean train loss:  3.81453842e-01, bound:  3.52844626e-01\n",
      "Epoch: 3103 mean train loss:  3.81370813e-01, bound:  3.52831125e-01\n",
      "Epoch: 3104 mean train loss:  3.81287515e-01, bound:  3.52817595e-01\n",
      "Epoch: 3105 mean train loss:  3.81204545e-01, bound:  3.52804005e-01\n",
      "Epoch: 3106 mean train loss:  3.81121337e-01, bound:  3.52790475e-01\n",
      "Epoch: 3107 mean train loss:  3.81037891e-01, bound:  3.52776974e-01\n",
      "Epoch: 3108 mean train loss:  3.80954534e-01, bound:  3.52763444e-01\n",
      "Epoch: 3109 mean train loss:  3.80870998e-01, bound:  3.52749944e-01\n",
      "Epoch: 3110 mean train loss:  3.80787790e-01, bound:  3.52736413e-01\n",
      "Epoch: 3111 mean train loss:  3.80704403e-01, bound:  3.52722973e-01\n",
      "Epoch: 3112 mean train loss:  3.80620748e-01, bound:  3.52709413e-01\n",
      "Epoch: 3113 mean train loss:  3.80537003e-01, bound:  3.52695823e-01\n",
      "Epoch: 3114 mean train loss:  3.80453497e-01, bound:  3.52682412e-01\n",
      "Epoch: 3115 mean train loss:  3.80369693e-01, bound:  3.52668822e-01\n",
      "Epoch: 3116 mean train loss:  3.80286187e-01, bound:  3.52655321e-01\n",
      "Epoch: 3117 mean train loss:  3.80202293e-01, bound:  3.52641821e-01\n",
      "Epoch: 3118 mean train loss:  3.80118430e-01, bound:  3.52628320e-01\n",
      "Epoch: 3119 mean train loss:  3.80034655e-01, bound:  3.52614790e-01\n",
      "Epoch: 3120 mean train loss:  3.79950821e-01, bound:  3.52601290e-01\n",
      "Epoch: 3121 mean train loss:  3.79866987e-01, bound:  3.52587759e-01\n",
      "Epoch: 3122 mean train loss:  3.79782677e-01, bound:  3.52574229e-01\n",
      "Epoch: 3123 mean train loss:  3.79698664e-01, bound:  3.52560788e-01\n",
      "Epoch: 3124 mean train loss:  3.79614562e-01, bound:  3.52547258e-01\n",
      "Epoch: 3125 mean train loss:  3.79530489e-01, bound:  3.52533728e-01\n",
      "Epoch: 3126 mean train loss:  3.79446328e-01, bound:  3.52520257e-01\n",
      "Epoch: 3127 mean train loss:  3.79362166e-01, bound:  3.52506727e-01\n",
      "Epoch: 3128 mean train loss:  3.79277438e-01, bound:  3.52493227e-01\n",
      "Epoch: 3129 mean train loss:  3.79193395e-01, bound:  3.52479726e-01\n",
      "Epoch: 3130 mean train loss:  3.79109055e-01, bound:  3.52466226e-01\n",
      "Epoch: 3131 mean train loss:  3.79024535e-01, bound:  3.52452666e-01\n",
      "Epoch: 3132 mean train loss:  3.78939956e-01, bound:  3.52439225e-01\n",
      "Epoch: 3133 mean train loss:  3.78855318e-01, bound:  3.52425754e-01\n",
      "Epoch: 3134 mean train loss:  3.78770977e-01, bound:  3.52412254e-01\n",
      "Epoch: 3135 mean train loss:  3.78686368e-01, bound:  3.52398723e-01\n",
      "Epoch: 3136 mean train loss:  3.78601521e-01, bound:  3.52385283e-01\n",
      "Epoch: 3137 mean train loss:  3.78516883e-01, bound:  3.52371752e-01\n",
      "Epoch: 3138 mean train loss:  3.78432065e-01, bound:  3.52358252e-01\n",
      "Epoch: 3139 mean train loss:  3.78347218e-01, bound:  3.52344781e-01\n",
      "Epoch: 3140 mean train loss:  3.78262430e-01, bound:  3.52331311e-01\n",
      "Epoch: 3141 mean train loss:  3.78177255e-01, bound:  3.52317780e-01\n",
      "Epoch: 3142 mean train loss:  3.78092498e-01, bound:  3.52304310e-01\n",
      "Epoch: 3143 mean train loss:  3.78007472e-01, bound:  3.52290839e-01\n",
      "Epoch: 3144 mean train loss:  3.77922177e-01, bound:  3.52277309e-01\n",
      "Epoch: 3145 mean train loss:  3.77837181e-01, bound:  3.52263838e-01\n",
      "Epoch: 3146 mean train loss:  3.77752095e-01, bound:  3.52250338e-01\n",
      "Epoch: 3147 mean train loss:  3.77666652e-01, bound:  3.52236867e-01\n",
      "Epoch: 3148 mean train loss:  3.77581239e-01, bound:  3.52223426e-01\n",
      "Epoch: 3149 mean train loss:  3.77495944e-01, bound:  3.52209955e-01\n",
      "Epoch: 3150 mean train loss:  3.77410620e-01, bound:  3.52196455e-01\n",
      "Epoch: 3151 mean train loss:  3.77324998e-01, bound:  3.52182955e-01\n",
      "Epoch: 3152 mean train loss:  3.77239376e-01, bound:  3.52169514e-01\n",
      "Epoch: 3153 mean train loss:  3.77154052e-01, bound:  3.52156013e-01\n",
      "Epoch: 3154 mean train loss:  3.77068371e-01, bound:  3.52142543e-01\n",
      "Epoch: 3155 mean train loss:  3.76982749e-01, bound:  3.52129072e-01\n",
      "Epoch: 3156 mean train loss:  3.76896888e-01, bound:  3.52115571e-01\n",
      "Epoch: 3157 mean train loss:  3.76811385e-01, bound:  3.52102131e-01\n",
      "Epoch: 3158 mean train loss:  3.76725376e-01, bound:  3.52088720e-01\n",
      "Epoch: 3159 mean train loss:  3.76639158e-01, bound:  3.52075219e-01\n",
      "Epoch: 3160 mean train loss:  3.76553297e-01, bound:  3.52061749e-01\n",
      "Epoch: 3161 mean train loss:  3.76467377e-01, bound:  3.52048248e-01\n",
      "Epoch: 3162 mean train loss:  3.76381278e-01, bound:  3.52034777e-01\n",
      "Epoch: 3163 mean train loss:  3.76295090e-01, bound:  3.52021307e-01\n",
      "Epoch: 3164 mean train loss:  3.76209170e-01, bound:  3.52007955e-01\n",
      "Epoch: 3165 mean train loss:  3.76122802e-01, bound:  3.51994425e-01\n",
      "Epoch: 3166 mean train loss:  3.76036584e-01, bound:  3.51980954e-01\n",
      "Epoch: 3167 mean train loss:  3.75950158e-01, bound:  3.51967543e-01\n",
      "Epoch: 3168 mean train loss:  3.75863850e-01, bound:  3.51954013e-01\n",
      "Epoch: 3169 mean train loss:  3.75777155e-01, bound:  3.51940602e-01\n",
      "Epoch: 3170 mean train loss:  3.75690639e-01, bound:  3.51927161e-01\n",
      "Epoch: 3171 mean train loss:  3.75604391e-01, bound:  3.51913691e-01\n",
      "Epoch: 3172 mean train loss:  3.75517428e-01, bound:  3.51900250e-01\n",
      "Epoch: 3173 mean train loss:  3.75430852e-01, bound:  3.51886809e-01\n",
      "Epoch: 3174 mean train loss:  3.75344396e-01, bound:  3.51873338e-01\n",
      "Epoch: 3175 mean train loss:  3.75257373e-01, bound:  3.51859897e-01\n",
      "Epoch: 3176 mean train loss:  3.75170559e-01, bound:  3.51846397e-01\n",
      "Epoch: 3177 mean train loss:  3.75083476e-01, bound:  3.51833016e-01\n",
      "Epoch: 3178 mean train loss:  3.74996692e-01, bound:  3.51819575e-01\n",
      "Epoch: 3179 mean train loss:  3.74909490e-01, bound:  3.51806134e-01\n",
      "Epoch: 3180 mean train loss:  3.74822229e-01, bound:  3.51792693e-01\n",
      "Epoch: 3181 mean train loss:  3.74735177e-01, bound:  3.51779252e-01\n",
      "Epoch: 3182 mean train loss:  3.74648005e-01, bound:  3.51765811e-01\n",
      "Epoch: 3183 mean train loss:  3.74560684e-01, bound:  3.51752400e-01\n",
      "Epoch: 3184 mean train loss:  3.74473244e-01, bound:  3.51738930e-01\n",
      "Epoch: 3185 mean train loss:  3.74385864e-01, bound:  3.51725549e-01\n",
      "Epoch: 3186 mean train loss:  3.74298602e-01, bound:  3.51712108e-01\n",
      "Epoch: 3187 mean train loss:  3.74210894e-01, bound:  3.51698667e-01\n",
      "Epoch: 3188 mean train loss:  3.74123693e-01, bound:  3.51685196e-01\n",
      "Epoch: 3189 mean train loss:  3.74035925e-01, bound:  3.51671785e-01\n",
      "Epoch: 3190 mean train loss:  3.73948246e-01, bound:  3.51658344e-01\n",
      "Epoch: 3191 mean train loss:  3.73860568e-01, bound:  3.51644933e-01\n",
      "Epoch: 3192 mean train loss:  3.73772800e-01, bound:  3.51631492e-01\n",
      "Epoch: 3193 mean train loss:  3.73684824e-01, bound:  3.51618111e-01\n",
      "Epoch: 3194 mean train loss:  3.73596847e-01, bound:  3.51604730e-01\n",
      "Epoch: 3195 mean train loss:  3.73508841e-01, bound:  3.51591259e-01\n",
      "Epoch: 3196 mean train loss:  3.73421073e-01, bound:  3.51577878e-01\n",
      "Epoch: 3197 mean train loss:  3.73332620e-01, bound:  3.51564407e-01\n",
      "Epoch: 3198 mean train loss:  3.73244613e-01, bound:  3.51551026e-01\n",
      "Epoch: 3199 mean train loss:  3.73156309e-01, bound:  3.51537645e-01\n",
      "Epoch: 3200 mean train loss:  3.73067915e-01, bound:  3.51524204e-01\n",
      "Epoch: 3201 mean train loss:  3.72979462e-01, bound:  3.51510823e-01\n",
      "Epoch: 3202 mean train loss:  3.72891188e-01, bound:  3.51497382e-01\n",
      "Epoch: 3203 mean train loss:  3.72802556e-01, bound:  3.51484001e-01\n",
      "Epoch: 3204 mean train loss:  3.72714043e-01, bound:  3.51470590e-01\n",
      "Epoch: 3205 mean train loss:  3.72625351e-01, bound:  3.51457208e-01\n",
      "Epoch: 3206 mean train loss:  3.72536838e-01, bound:  3.51443797e-01\n",
      "Epoch: 3207 mean train loss:  3.72447938e-01, bound:  3.51430416e-01\n",
      "Epoch: 3208 mean train loss:  3.72359216e-01, bound:  3.51417005e-01\n",
      "Epoch: 3209 mean train loss:  3.72270316e-01, bound:  3.51403624e-01\n",
      "Epoch: 3210 mean train loss:  3.72181475e-01, bound:  3.51390183e-01\n",
      "Epoch: 3211 mean train loss:  3.72092396e-01, bound:  3.51376802e-01\n",
      "Epoch: 3212 mean train loss:  3.72003347e-01, bound:  3.51363391e-01\n",
      "Epoch: 3213 mean train loss:  3.71914178e-01, bound:  3.51349980e-01\n",
      "Epoch: 3214 mean train loss:  3.71824950e-01, bound:  3.51336628e-01\n",
      "Epoch: 3215 mean train loss:  3.71735692e-01, bound:  3.51323277e-01\n",
      "Epoch: 3216 mean train loss:  3.71646613e-01, bound:  3.51309836e-01\n",
      "Epoch: 3217 mean train loss:  3.71557534e-01, bound:  3.51296544e-01\n",
      "Epoch: 3218 mean train loss:  3.71468008e-01, bound:  3.51283044e-01\n",
      "Epoch: 3219 mean train loss:  3.71379048e-01, bound:  3.51269811e-01\n",
      "Epoch: 3220 mean train loss:  3.71289998e-01, bound:  3.51256311e-01\n",
      "Epoch: 3221 mean train loss:  3.71202141e-01, bound:  3.51243109e-01\n",
      "Epoch: 3222 mean train loss:  3.71114701e-01, bound:  3.51229489e-01\n",
      "Epoch: 3223 mean train loss:  3.71029228e-01, bound:  3.51216465e-01\n",
      "Epoch: 3224 mean train loss:  3.70945156e-01, bound:  3.51202667e-01\n",
      "Epoch: 3225 mean train loss:  3.70862365e-01, bound:  3.51190001e-01\n",
      "Epoch: 3226 mean train loss:  3.70776027e-01, bound:  3.51176053e-01\n",
      "Epoch: 3227 mean train loss:  3.70682001e-01, bound:  3.51163656e-01\n",
      "Epoch: 3228 mean train loss:  3.70581836e-01, bound:  3.51150125e-01\n",
      "Epoch: 3229 mean train loss:  3.70483726e-01, bound:  3.51137608e-01\n",
      "Epoch: 3230 mean train loss:  3.70394707e-01, bound:  3.51124704e-01\n",
      "Epoch: 3231 mean train loss:  3.70311439e-01, bound:  3.51111025e-01\n",
      "Epoch: 3232 mean train loss:  3.70226651e-01, bound:  3.51097882e-01\n",
      "Epoch: 3233 mean train loss:  3.70134324e-01, bound:  3.51083338e-01\n",
      "Epoch: 3234 mean train loss:  3.70038509e-01, bound:  3.51069778e-01\n",
      "Epoch: 3235 mean train loss:  3.69946063e-01, bound:  3.51055861e-01\n",
      "Epoch: 3236 mean train loss:  3.69858861e-01, bound:  3.51042241e-01\n",
      "Epoch: 3237 mean train loss:  3.69773060e-01, bound:  3.51029247e-01\n",
      "Epoch: 3238 mean train loss:  3.69683146e-01, bound:  3.51015478e-01\n",
      "Epoch: 3239 mean train loss:  3.69589835e-01, bound:  3.51002455e-01\n",
      "Epoch: 3240 mean train loss:  3.69497389e-01, bound:  3.50988775e-01\n",
      "Epoch: 3241 mean train loss:  3.69408488e-01, bound:  3.50975275e-01\n",
      "Epoch: 3242 mean train loss:  3.69321257e-01, bound:  3.50962192e-01\n",
      "Epoch: 3243 mean train loss:  3.69231880e-01, bound:  3.50948632e-01\n",
      "Epoch: 3244 mean train loss:  3.69139791e-01, bound:  3.50935876e-01\n",
      "Epoch: 3245 mean train loss:  3.69048178e-01, bound:  3.50922823e-01\n",
      "Epoch: 3246 mean train loss:  3.68958235e-01, bound:  3.50909710e-01\n",
      "Epoch: 3247 mean train loss:  3.68869454e-01, bound:  3.50896776e-01\n",
      "Epoch: 3248 mean train loss:  3.68779629e-01, bound:  3.50883305e-01\n",
      "Epoch: 3249 mean train loss:  3.68688285e-01, bound:  3.50870222e-01\n",
      "Epoch: 3250 mean train loss:  3.68596762e-01, bound:  3.50856900e-01\n",
      "Epoch: 3251 mean train loss:  3.68506432e-01, bound:  3.50843728e-01\n",
      "Epoch: 3252 mean train loss:  3.68416667e-01, bound:  3.50830764e-01\n",
      "Epoch: 3253 mean train loss:  3.68326366e-01, bound:  3.50817323e-01\n",
      "Epoch: 3254 mean train loss:  3.68235499e-01, bound:  3.50804299e-01\n",
      "Epoch: 3255 mean train loss:  3.68143976e-01, bound:  3.50790799e-01\n",
      "Epoch: 3256 mean train loss:  3.68052959e-01, bound:  3.50777447e-01\n",
      "Epoch: 3257 mean train loss:  3.67962390e-01, bound:  3.50764126e-01\n",
      "Epoch: 3258 mean train loss:  3.67872298e-01, bound:  3.50750685e-01\n",
      "Epoch: 3259 mean train loss:  3.67780954e-01, bound:  3.50737572e-01\n",
      "Epoch: 3260 mean train loss:  3.67689490e-01, bound:  3.50724220e-01\n",
      "Epoch: 3261 mean train loss:  3.67598057e-01, bound:  3.50711107e-01\n",
      "Epoch: 3262 mean train loss:  3.67506832e-01, bound:  3.50697845e-01\n",
      "Epoch: 3263 mean train loss:  3.67416054e-01, bound:  3.50684553e-01\n",
      "Epoch: 3264 mean train loss:  3.67324710e-01, bound:  3.50671500e-01\n",
      "Epoch: 3265 mean train loss:  3.67233247e-01, bound:  3.50658268e-01\n",
      "Epoch: 3266 mean train loss:  3.67141306e-01, bound:  3.50645244e-01\n",
      "Epoch: 3267 mean train loss:  3.67049813e-01, bound:  3.50632221e-01\n",
      "Epoch: 3268 mean train loss:  3.66958410e-01, bound:  3.50619137e-01\n",
      "Epoch: 3269 mean train loss:  3.66866618e-01, bound:  3.50606114e-01\n",
      "Epoch: 3270 mean train loss:  3.66774887e-01, bound:  3.50592911e-01\n",
      "Epoch: 3271 mean train loss:  3.66682798e-01, bound:  3.50579798e-01\n",
      "Epoch: 3272 mean train loss:  3.66590917e-01, bound:  3.50566626e-01\n",
      "Epoch: 3273 mean train loss:  3.66498798e-01, bound:  3.50553483e-01\n",
      "Epoch: 3274 mean train loss:  3.66406828e-01, bound:  3.50540400e-01\n",
      "Epoch: 3275 mean train loss:  3.66314828e-01, bound:  3.50527197e-01\n",
      "Epoch: 3276 mean train loss:  3.66222441e-01, bound:  3.50514084e-01\n",
      "Epoch: 3277 mean train loss:  3.66129905e-01, bound:  3.50500852e-01\n",
      "Epoch: 3278 mean train loss:  3.66037697e-01, bound:  3.50487709e-01\n",
      "Epoch: 3279 mean train loss:  3.65945131e-01, bound:  3.50474507e-01\n",
      "Epoch: 3280 mean train loss:  3.65852505e-01, bound:  3.50461364e-01\n",
      "Epoch: 3281 mean train loss:  3.65759879e-01, bound:  3.50448251e-01\n",
      "Epoch: 3282 mean train loss:  3.65667224e-01, bound:  3.50435108e-01\n",
      "Epoch: 3283 mean train loss:  3.65574241e-01, bound:  3.50422084e-01\n",
      "Epoch: 3284 mean train loss:  3.65481555e-01, bound:  3.50408912e-01\n",
      "Epoch: 3285 mean train loss:  3.65388244e-01, bound:  3.50395799e-01\n",
      "Epoch: 3286 mean train loss:  3.65295351e-01, bound:  3.50382686e-01\n",
      "Epoch: 3287 mean train loss:  3.65202397e-01, bound:  3.50369513e-01\n",
      "Epoch: 3288 mean train loss:  3.65108997e-01, bound:  3.50356400e-01\n",
      "Epoch: 3289 mean train loss:  3.65015775e-01, bound:  3.50343317e-01\n",
      "Epoch: 3290 mean train loss:  3.64922434e-01, bound:  3.50330234e-01\n",
      "Epoch: 3291 mean train loss:  3.64829212e-01, bound:  3.50317061e-01\n",
      "Epoch: 3292 mean train loss:  3.64735544e-01, bound:  3.50303948e-01\n",
      "Epoch: 3293 mean train loss:  3.64642084e-01, bound:  3.50290775e-01\n",
      "Epoch: 3294 mean train loss:  3.64548177e-01, bound:  3.50277662e-01\n",
      "Epoch: 3295 mean train loss:  3.64454597e-01, bound:  3.50264579e-01\n",
      "Epoch: 3296 mean train loss:  3.64360601e-01, bound:  3.50251406e-01\n",
      "Epoch: 3297 mean train loss:  3.64266604e-01, bound:  3.50238323e-01\n",
      "Epoch: 3298 mean train loss:  3.64172667e-01, bound:  3.50225210e-01\n",
      "Epoch: 3299 mean train loss:  3.64078701e-01, bound:  3.50212127e-01\n",
      "Epoch: 3300 mean train loss:  3.63984406e-01, bound:  3.50198984e-01\n",
      "Epoch: 3301 mean train loss:  3.63890141e-01, bound:  3.50185871e-01\n",
      "Epoch: 3302 mean train loss:  3.63795966e-01, bound:  3.50172758e-01\n",
      "Epoch: 3303 mean train loss:  3.63701433e-01, bound:  3.50159675e-01\n",
      "Epoch: 3304 mean train loss:  3.63606542e-01, bound:  3.50146562e-01\n",
      "Epoch: 3305 mean train loss:  3.63511950e-01, bound:  3.50133508e-01\n",
      "Epoch: 3306 mean train loss:  3.63417745e-01, bound:  3.50120425e-01\n",
      "Epoch: 3307 mean train loss:  3.63322884e-01, bound:  3.50107312e-01\n",
      "Epoch: 3308 mean train loss:  3.63227844e-01, bound:  3.50094259e-01\n",
      "Epoch: 3309 mean train loss:  3.63132775e-01, bound:  3.50081146e-01\n",
      "Epoch: 3310 mean train loss:  3.63038242e-01, bound:  3.50068122e-01\n",
      "Epoch: 3311 mean train loss:  3.62943202e-01, bound:  3.50054979e-01\n",
      "Epoch: 3312 mean train loss:  3.62847507e-01, bound:  3.50041896e-01\n",
      "Epoch: 3313 mean train loss:  3.62752438e-01, bound:  3.50028813e-01\n",
      "Epoch: 3314 mean train loss:  3.62657070e-01, bound:  3.50015730e-01\n",
      "Epoch: 3315 mean train loss:  3.62561613e-01, bound:  3.50002646e-01\n",
      "Epoch: 3316 mean train loss:  3.62466335e-01, bound:  3.49989533e-01\n",
      "Epoch: 3317 mean train loss:  3.62370372e-01, bound:  3.49976450e-01\n",
      "Epoch: 3318 mean train loss:  3.62274915e-01, bound:  3.49963367e-01\n",
      "Epoch: 3319 mean train loss:  3.62178862e-01, bound:  3.49950254e-01\n",
      "Epoch: 3320 mean train loss:  3.62083226e-01, bound:  3.49937201e-01\n",
      "Epoch: 3321 mean train loss:  3.61986995e-01, bound:  3.49924088e-01\n",
      "Epoch: 3322 mean train loss:  3.61891121e-01, bound:  3.49911064e-01\n",
      "Epoch: 3323 mean train loss:  3.61794680e-01, bound:  3.49897951e-01\n",
      "Epoch: 3324 mean train loss:  3.61698776e-01, bound:  3.49884897e-01\n",
      "Epoch: 3325 mean train loss:  3.61602277e-01, bound:  3.49871844e-01\n",
      "Epoch: 3326 mean train loss:  3.61505747e-01, bound:  3.49858791e-01\n",
      "Epoch: 3327 mean train loss:  3.61409515e-01, bound:  3.49845707e-01\n",
      "Epoch: 3328 mean train loss:  3.61312866e-01, bound:  3.49832654e-01\n",
      "Epoch: 3329 mean train loss:  3.61216098e-01, bound:  3.49819571e-01\n",
      "Epoch: 3330 mean train loss:  3.61119330e-01, bound:  3.49806517e-01\n",
      "Epoch: 3331 mean train loss:  3.61022502e-01, bound:  3.49793464e-01\n",
      "Epoch: 3332 mean train loss:  3.60925555e-01, bound:  3.49780381e-01\n",
      "Epoch: 3333 mean train loss:  3.60828489e-01, bound:  3.49767298e-01\n",
      "Epoch: 3334 mean train loss:  3.60731244e-01, bound:  3.49754304e-01\n",
      "Epoch: 3335 mean train loss:  3.60634267e-01, bound:  3.49741220e-01\n",
      "Epoch: 3336 mean train loss:  3.60536963e-01, bound:  3.49728197e-01\n",
      "Epoch: 3337 mean train loss:  3.60439271e-01, bound:  3.49715114e-01\n",
      "Epoch: 3338 mean train loss:  3.60342056e-01, bound:  3.49702060e-01\n",
      "Epoch: 3339 mean train loss:  3.60244334e-01, bound:  3.49689007e-01\n",
      "Epoch: 3340 mean train loss:  3.60146731e-01, bound:  3.49675953e-01\n",
      "Epoch: 3341 mean train loss:  3.60048860e-01, bound:  3.49662870e-01\n",
      "Epoch: 3342 mean train loss:  3.59951168e-01, bound:  3.49649906e-01\n",
      "Epoch: 3343 mean train loss:  3.59853238e-01, bound:  3.49636793e-01\n",
      "Epoch: 3344 mean train loss:  3.59755099e-01, bound:  3.49623799e-01\n",
      "Epoch: 3345 mean train loss:  3.59657079e-01, bound:  3.49610716e-01\n",
      "Epoch: 3346 mean train loss:  3.59558880e-01, bound:  3.49597752e-01\n",
      "Epoch: 3347 mean train loss:  3.59460920e-01, bound:  3.49584550e-01\n",
      "Epoch: 3348 mean train loss:  3.59363467e-01, bound:  3.49571794e-01\n",
      "Epoch: 3349 mean train loss:  3.59265953e-01, bound:  3.49558353e-01\n",
      "Epoch: 3350 mean train loss:  3.59169424e-01, bound:  3.49545777e-01\n",
      "Epoch: 3351 mean train loss:  3.59074473e-01, bound:  3.49532187e-01\n",
      "Epoch: 3352 mean train loss:  3.58981878e-01, bound:  3.49519968e-01\n",
      "Epoch: 3353 mean train loss:  3.58893007e-01, bound:  3.49505961e-01\n",
      "Epoch: 3354 mean train loss:  3.58806461e-01, bound:  3.49494278e-01\n",
      "Epoch: 3355 mean train loss:  3.58717322e-01, bound:  3.49479854e-01\n",
      "Epoch: 3356 mean train loss:  3.58616740e-01, bound:  3.49468619e-01\n",
      "Epoch: 3357 mean train loss:  3.58502269e-01, bound:  3.49454522e-01\n",
      "Epoch: 3358 mean train loss:  3.58384430e-01, bound:  3.49442810e-01\n",
      "Epoch: 3359 mean train loss:  3.58279765e-01, bound:  3.49429965e-01\n",
      "Epoch: 3360 mean train loss:  3.58190328e-01, bound:  3.49416673e-01\n",
      "Epoch: 3361 mean train loss:  3.58103573e-01, bound:  3.49404395e-01\n",
      "Epoch: 3362 mean train loss:  3.58005673e-01, bound:  3.49389881e-01\n",
      "Epoch: 3363 mean train loss:  3.57896745e-01, bound:  3.49377275e-01\n",
      "Epoch: 3364 mean train loss:  3.57789636e-01, bound:  3.49363446e-01\n",
      "Epoch: 3365 mean train loss:  3.57692748e-01, bound:  3.49350154e-01\n",
      "Epoch: 3366 mean train loss:  3.57601762e-01, bound:  3.49337757e-01\n",
      "Epoch: 3367 mean train loss:  3.57506365e-01, bound:  3.49323958e-01\n",
      "Epoch: 3368 mean train loss:  3.57402563e-01, bound:  3.49311739e-01\n",
      "Epoch: 3369 mean train loss:  3.57297659e-01, bound:  3.49298239e-01\n",
      "Epoch: 3370 mean train loss:  3.57198924e-01, bound:  3.49285245e-01\n",
      "Epoch: 3371 mean train loss:  3.57104599e-01, bound:  3.49272788e-01\n",
      "Epoch: 3372 mean train loss:  3.57008159e-01, bound:  3.49259377e-01\n",
      "Epoch: 3373 mean train loss:  3.56906831e-01, bound:  3.49247277e-01\n",
      "Epoch: 3374 mean train loss:  3.56803983e-01, bound:  3.49234283e-01\n",
      "Epoch: 3375 mean train loss:  3.56704712e-01, bound:  3.49221617e-01\n",
      "Epoch: 3376 mean train loss:  3.56608212e-01, bound:  3.49209130e-01\n",
      "Epoch: 3377 mean train loss:  3.56510401e-01, bound:  3.49195808e-01\n",
      "Epoch: 3378 mean train loss:  3.56409878e-01, bound:  3.49183410e-01\n",
      "Epoch: 3379 mean train loss:  3.56308103e-01, bound:  3.49170238e-01\n",
      "Epoch: 3380 mean train loss:  3.56208265e-01, bound:  3.49157453e-01\n",
      "Epoch: 3381 mean train loss:  3.56109858e-01, bound:  3.49144816e-01\n",
      "Epoch: 3382 mean train loss:  3.56011689e-01, bound:  3.49131554e-01\n",
      "Epoch: 3383 mean train loss:  3.55911344e-01, bound:  3.49119008e-01\n",
      "Epoch: 3384 mean train loss:  3.55810314e-01, bound:  3.49105775e-01\n",
      "Epoch: 3385 mean train loss:  3.55709404e-01, bound:  3.49092931e-01\n",
      "Epoch: 3386 mean train loss:  3.55610311e-01, bound:  3.49080056e-01\n",
      "Epoch: 3387 mean train loss:  3.55510920e-01, bound:  3.49066883e-01\n",
      "Epoch: 3388 mean train loss:  3.55410904e-01, bound:  3.49054337e-01\n",
      "Epoch: 3389 mean train loss:  3.55309874e-01, bound:  3.49041253e-01\n",
      "Epoch: 3390 mean train loss:  3.55208904e-01, bound:  3.49028647e-01\n",
      "Epoch: 3391 mean train loss:  3.55108321e-01, bound:  3.49015862e-01\n",
      "Epoch: 3392 mean train loss:  3.55008543e-01, bound:  3.49002928e-01\n",
      "Epoch: 3393 mean train loss:  3.54907900e-01, bound:  3.48990381e-01\n",
      "Epoch: 3394 mean train loss:  3.54807228e-01, bound:  3.48977447e-01\n",
      "Epoch: 3395 mean train loss:  3.54705781e-01, bound:  3.48964930e-01\n",
      "Epoch: 3396 mean train loss:  3.54604572e-01, bound:  3.48952115e-01\n",
      "Epoch: 3397 mean train loss:  3.54503810e-01, bound:  3.48939359e-01\n",
      "Epoch: 3398 mean train loss:  3.54402930e-01, bound:  3.48926693e-01\n",
      "Epoch: 3399 mean train loss:  3.54301572e-01, bound:  3.48913670e-01\n",
      "Epoch: 3400 mean train loss:  3.54199976e-01, bound:  3.48901063e-01\n",
      "Epoch: 3401 mean train loss:  3.54098529e-01, bound:  3.48888189e-01\n",
      "Epoch: 3402 mean train loss:  3.53997171e-01, bound:  3.48875374e-01\n",
      "Epoch: 3403 mean train loss:  3.53895277e-01, bound:  3.48862678e-01\n",
      "Epoch: 3404 mean train loss:  3.53793591e-01, bound:  3.48849773e-01\n",
      "Epoch: 3405 mean train loss:  3.53691727e-01, bound:  3.48837107e-01\n",
      "Epoch: 3406 mean train loss:  3.53589892e-01, bound:  3.48824114e-01\n",
      "Epoch: 3407 mean train loss:  3.53487283e-01, bound:  3.48811388e-01\n",
      "Epoch: 3408 mean train loss:  3.53385270e-01, bound:  3.48798603e-01\n",
      "Epoch: 3409 mean train loss:  3.53282839e-01, bound:  3.48785758e-01\n",
      "Epoch: 3410 mean train loss:  3.53180319e-01, bound:  3.48773062e-01\n",
      "Epoch: 3411 mean train loss:  3.53078306e-01, bound:  3.48760217e-01\n",
      "Epoch: 3412 mean train loss:  3.52975488e-01, bound:  3.48747581e-01\n",
      "Epoch: 3413 mean train loss:  3.52872610e-01, bound:  3.48734587e-01\n",
      "Epoch: 3414 mean train loss:  3.52769881e-01, bound:  3.48721862e-01\n",
      "Epoch: 3415 mean train loss:  3.52666676e-01, bound:  3.48709047e-01\n",
      "Epoch: 3416 mean train loss:  3.52563888e-01, bound:  3.48696202e-01\n",
      "Epoch: 3417 mean train loss:  3.52460682e-01, bound:  3.48683506e-01\n",
      "Epoch: 3418 mean train loss:  3.52357268e-01, bound:  3.48670721e-01\n",
      "Epoch: 3419 mean train loss:  3.52254063e-01, bound:  3.48657966e-01\n",
      "Epoch: 3420 mean train loss:  3.52150470e-01, bound:  3.48645121e-01\n",
      "Epoch: 3421 mean train loss:  3.52046758e-01, bound:  3.48632306e-01\n",
      "Epoch: 3422 mean train loss:  3.51943135e-01, bound:  3.48619491e-01\n",
      "Epoch: 3423 mean train loss:  3.51839155e-01, bound:  3.48606706e-01\n",
      "Epoch: 3424 mean train loss:  3.51735562e-01, bound:  3.48593920e-01\n",
      "Epoch: 3425 mean train loss:  3.51631343e-01, bound:  3.48581105e-01\n",
      "Epoch: 3426 mean train loss:  3.51527065e-01, bound:  3.48568380e-01\n",
      "Epoch: 3427 mean train loss:  3.51423025e-01, bound:  3.48555565e-01\n",
      "Epoch: 3428 mean train loss:  3.51318717e-01, bound:  3.48542720e-01\n",
      "Epoch: 3429 mean train loss:  3.51213992e-01, bound:  3.48529935e-01\n",
      "Epoch: 3430 mean train loss:  3.51109445e-01, bound:  3.48517179e-01\n",
      "Epoch: 3431 mean train loss:  3.51004928e-01, bound:  3.48504305e-01\n",
      "Epoch: 3432 mean train loss:  3.50899994e-01, bound:  3.48491579e-01\n",
      "Epoch: 3433 mean train loss:  3.50795120e-01, bound:  3.48478794e-01\n",
      "Epoch: 3434 mean train loss:  3.50690037e-01, bound:  3.48466009e-01\n",
      "Epoch: 3435 mean train loss:  3.50585043e-01, bound:  3.48453194e-01\n",
      "Epoch: 3436 mean train loss:  3.50479841e-01, bound:  3.48440349e-01\n",
      "Epoch: 3437 mean train loss:  3.50374073e-01, bound:  3.48427564e-01\n",
      "Epoch: 3438 mean train loss:  3.50268990e-01, bound:  3.48414719e-01\n",
      "Epoch: 3439 mean train loss:  3.50163102e-01, bound:  3.48401934e-01\n",
      "Epoch: 3440 mean train loss:  3.50057364e-01, bound:  3.48389149e-01\n",
      "Epoch: 3441 mean train loss:  3.49951655e-01, bound:  3.48376334e-01\n",
      "Epoch: 3442 mean train loss:  3.49845499e-01, bound:  3.48363489e-01\n",
      "Epoch: 3443 mean train loss:  3.49739552e-01, bound:  3.48350644e-01\n",
      "Epoch: 3444 mean train loss:  3.49633485e-01, bound:  3.48337859e-01\n",
      "Epoch: 3445 mean train loss:  3.49527180e-01, bound:  3.48325044e-01\n",
      "Epoch: 3446 mean train loss:  3.49420846e-01, bound:  3.48312289e-01\n",
      "Epoch: 3447 mean train loss:  3.49314302e-01, bound:  3.48299414e-01\n",
      "Epoch: 3448 mean train loss:  3.49207997e-01, bound:  3.48286629e-01\n",
      "Epoch: 3449 mean train loss:  3.49100947e-01, bound:  3.48273814e-01\n",
      "Epoch: 3450 mean train loss:  3.48994255e-01, bound:  3.48261058e-01\n",
      "Epoch: 3451 mean train loss:  3.48887146e-01, bound:  3.48248184e-01\n",
      "Epoch: 3452 mean train loss:  3.48779947e-01, bound:  3.48235369e-01\n",
      "Epoch: 3453 mean train loss:  3.48672956e-01, bound:  3.48222554e-01\n",
      "Epoch: 3454 mean train loss:  3.48565340e-01, bound:  3.48209739e-01\n",
      "Epoch: 3455 mean train loss:  3.48458052e-01, bound:  3.48196894e-01\n",
      "Epoch: 3456 mean train loss:  3.48350585e-01, bound:  3.48184109e-01\n",
      "Epoch: 3457 mean train loss:  3.48242849e-01, bound:  3.48171204e-01\n",
      "Epoch: 3458 mean train loss:  3.48135233e-01, bound:  3.48158479e-01\n",
      "Epoch: 3459 mean train loss:  3.48027289e-01, bound:  3.48145545e-01\n",
      "Epoch: 3460 mean train loss:  3.47919554e-01, bound:  3.48132789e-01\n",
      "Epoch: 3461 mean train loss:  3.47811550e-01, bound:  3.48119825e-01\n",
      "Epoch: 3462 mean train loss:  3.47703397e-01, bound:  3.48107219e-01\n",
      "Epoch: 3463 mean train loss:  3.47594917e-01, bound:  3.48094195e-01\n",
      "Epoch: 3464 mean train loss:  3.47487062e-01, bound:  3.48081559e-01\n",
      "Epoch: 3465 mean train loss:  3.47378910e-01, bound:  3.48068446e-01\n",
      "Epoch: 3466 mean train loss:  3.47270787e-01, bound:  3.48056018e-01\n",
      "Epoch: 3467 mean train loss:  3.47163528e-01, bound:  3.48042637e-01\n",
      "Epoch: 3468 mean train loss:  3.47056955e-01, bound:  3.48030448e-01\n",
      "Epoch: 3469 mean train loss:  3.46952021e-01, bound:  3.48016798e-01\n",
      "Epoch: 3470 mean train loss:  3.46849680e-01, bound:  3.48005116e-01\n",
      "Epoch: 3471 mean train loss:  3.46750855e-01, bound:  3.47990841e-01\n",
      "Epoch: 3472 mean train loss:  3.46655607e-01, bound:  3.47979784e-01\n",
      "Epoch: 3473 mean train loss:  3.46560657e-01, bound:  3.47964942e-01\n",
      "Epoch: 3474 mean train loss:  3.46458793e-01, bound:  3.47954661e-01\n",
      "Epoch: 3475 mean train loss:  3.46340030e-01, bound:  3.47939640e-01\n",
      "Epoch: 3476 mean train loss:  3.46207798e-01, bound:  3.47929090e-01\n",
      "Epoch: 3477 mean train loss:  3.46077710e-01, bound:  3.47915322e-01\n",
      "Epoch: 3478 mean train loss:  3.45966041e-01, bound:  3.47902894e-01\n",
      "Epoch: 3479 mean train loss:  3.45870107e-01, bound:  3.47890764e-01\n",
      "Epoch: 3480 mean train loss:  3.45774591e-01, bound:  3.47876370e-01\n",
      "Epoch: 3481 mean train loss:  3.45666260e-01, bound:  3.47864866e-01\n",
      "Epoch: 3482 mean train loss:  3.45544904e-01, bound:  3.47850233e-01\n",
      "Epoch: 3483 mean train loss:  3.45424861e-01, bound:  3.47837985e-01\n",
      "Epoch: 3484 mean train loss:  3.45316023e-01, bound:  3.47825110e-01\n",
      "Epoch: 3485 mean train loss:  3.45216036e-01, bound:  3.47811580e-01\n",
      "Epoch: 3486 mean train loss:  3.45112771e-01, bound:  3.47800076e-01\n",
      "Epoch: 3487 mean train loss:  3.45000744e-01, bound:  3.47786039e-01\n",
      "Epoch: 3488 mean train loss:  3.44883978e-01, bound:  3.47774148e-01\n",
      "Epoch: 3489 mean train loss:  3.44771147e-01, bound:  3.47761095e-01\n",
      "Epoch: 3490 mean train loss:  3.44664842e-01, bound:  3.47748190e-01\n",
      "Epoch: 3491 mean train loss:  3.44561011e-01, bound:  3.47736388e-01\n",
      "Epoch: 3492 mean train loss:  3.44452143e-01, bound:  3.47722828e-01\n",
      "Epoch: 3493 mean train loss:  3.44338894e-01, bound:  3.47711027e-01\n",
      "Epoch: 3494 mean train loss:  3.44225496e-01, bound:  3.47697854e-01\n",
      "Epoch: 3495 mean train loss:  3.44116122e-01, bound:  3.47685099e-01\n",
      "Epoch: 3496 mean train loss:  3.44009250e-01, bound:  3.47672850e-01\n",
      "Epoch: 3497 mean train loss:  3.43901515e-01, bound:  3.47659439e-01\n",
      "Epoch: 3498 mean train loss:  3.43790859e-01, bound:  3.47647429e-01\n",
      "Epoch: 3499 mean train loss:  3.43678325e-01, bound:  3.47634077e-01\n",
      "Epoch: 3500 mean train loss:  3.43566567e-01, bound:  3.47621530e-01\n",
      "Epoch: 3501 mean train loss:  3.43457282e-01, bound:  3.47608864e-01\n",
      "Epoch: 3502 mean train loss:  3.43349010e-01, bound:  3.47595572e-01\n",
      "Epoch: 3503 mean train loss:  3.43238711e-01, bound:  3.47583383e-01\n",
      "Epoch: 3504 mean train loss:  3.43127877e-01, bound:  3.47570121e-01\n",
      "Epoch: 3505 mean train loss:  3.43015909e-01, bound:  3.47557813e-01\n",
      "Epoch: 3506 mean train loss:  3.42904598e-01, bound:  3.47545028e-01\n",
      "Epoch: 3507 mean train loss:  3.42794418e-01, bound:  3.47532183e-01\n",
      "Epoch: 3508 mean train loss:  3.42684299e-01, bound:  3.47519785e-01\n",
      "Epoch: 3509 mean train loss:  3.42573702e-01, bound:  3.47506672e-01\n",
      "Epoch: 3510 mean train loss:  3.42462122e-01, bound:  3.47494453e-01\n",
      "Epoch: 3511 mean train loss:  3.42350066e-01, bound:  3.47481549e-01\n",
      "Epoch: 3512 mean train loss:  3.42238277e-01, bound:  3.47469032e-01\n",
      "Epoch: 3513 mean train loss:  3.42127144e-01, bound:  3.47456485e-01\n",
      "Epoch: 3514 mean train loss:  3.42016011e-01, bound:  3.47443521e-01\n",
      "Epoch: 3515 mean train loss:  3.41904551e-01, bound:  3.47431093e-01\n",
      "Epoch: 3516 mean train loss:  3.41792583e-01, bound:  3.47418010e-01\n",
      "Epoch: 3517 mean train loss:  3.41680318e-01, bound:  3.47405493e-01\n",
      "Epoch: 3518 mean train loss:  3.41567755e-01, bound:  3.47392619e-01\n",
      "Epoch: 3519 mean train loss:  3.41456026e-01, bound:  3.47379923e-01\n",
      "Epoch: 3520 mean train loss:  3.41343582e-01, bound:  3.47367257e-01\n",
      "Epoch: 3521 mean train loss:  3.41231585e-01, bound:  3.47354352e-01\n",
      "Epoch: 3522 mean train loss:  3.41119260e-01, bound:  3.47341806e-01\n",
      "Epoch: 3523 mean train loss:  3.41006517e-01, bound:  3.47328842e-01\n",
      "Epoch: 3524 mean train loss:  3.40893656e-01, bound:  3.47316325e-01\n",
      "Epoch: 3525 mean train loss:  3.40780586e-01, bound:  3.47303391e-01\n",
      "Epoch: 3526 mean train loss:  3.40667546e-01, bound:  3.47290725e-01\n",
      "Epoch: 3527 mean train loss:  3.40554327e-01, bound:  3.47278088e-01\n",
      "Epoch: 3528 mean train loss:  3.40441227e-01, bound:  3.47265214e-01\n",
      "Epoch: 3529 mean train loss:  3.40328157e-01, bound:  3.47252667e-01\n",
      "Epoch: 3530 mean train loss:  3.40214789e-01, bound:  3.47239673e-01\n",
      "Epoch: 3531 mean train loss:  3.40101242e-01, bound:  3.47227097e-01\n",
      "Epoch: 3532 mean train loss:  3.39987338e-01, bound:  3.47214162e-01\n",
      "Epoch: 3533 mean train loss:  3.39873642e-01, bound:  3.47201496e-01\n",
      "Epoch: 3534 mean train loss:  3.39759707e-01, bound:  3.47188741e-01\n",
      "Epoch: 3535 mean train loss:  3.39645505e-01, bound:  3.47176015e-01\n",
      "Epoch: 3536 mean train loss:  3.39531332e-01, bound:  3.47163230e-01\n",
      "Epoch: 3537 mean train loss:  3.39417219e-01, bound:  3.47150356e-01\n",
      "Epoch: 3538 mean train loss:  3.39302838e-01, bound:  3.47137660e-01\n",
      "Epoch: 3539 mean train loss:  3.39188337e-01, bound:  3.47124755e-01\n",
      "Epoch: 3540 mean train loss:  3.39073479e-01, bound:  3.47112089e-01\n",
      "Epoch: 3541 mean train loss:  3.38958830e-01, bound:  3.47099245e-01\n",
      "Epoch: 3542 mean train loss:  3.38844001e-01, bound:  3.47086549e-01\n",
      "Epoch: 3543 mean train loss:  3.38729084e-01, bound:  3.47073674e-01\n",
      "Epoch: 3544 mean train loss:  3.38614076e-01, bound:  3.47060859e-01\n",
      "Epoch: 3545 mean train loss:  3.38498384e-01, bound:  3.47048044e-01\n",
      "Epoch: 3546 mean train loss:  3.38383257e-01, bound:  3.47035259e-01\n",
      "Epoch: 3547 mean train loss:  3.38267714e-01, bound:  3.47022504e-01\n",
      "Epoch: 3548 mean train loss:  3.38152289e-01, bound:  3.47009629e-01\n",
      "Epoch: 3549 mean train loss:  3.38036746e-01, bound:  3.46996844e-01\n",
      "Epoch: 3550 mean train loss:  3.37921053e-01, bound:  3.46983969e-01\n",
      "Epoch: 3551 mean train loss:  3.37804854e-01, bound:  3.46971184e-01\n",
      "Epoch: 3552 mean train loss:  3.37688863e-01, bound:  3.46958220e-01\n",
      "Epoch: 3553 mean train loss:  3.37572902e-01, bound:  3.46945465e-01\n",
      "Epoch: 3554 mean train loss:  3.37456197e-01, bound:  3.46932590e-01\n",
      "Epoch: 3555 mean train loss:  3.37339997e-01, bound:  3.46919835e-01\n",
      "Epoch: 3556 mean train loss:  3.37223470e-01, bound:  3.46906900e-01\n",
      "Epoch: 3557 mean train loss:  3.37106854e-01, bound:  3.46894115e-01\n",
      "Epoch: 3558 mean train loss:  3.36990356e-01, bound:  3.46881121e-01\n",
      "Epoch: 3559 mean train loss:  3.36873263e-01, bound:  3.46868396e-01\n",
      "Epoch: 3560 mean train loss:  3.36756289e-01, bound:  3.46855402e-01\n",
      "Epoch: 3561 mean train loss:  3.36639434e-01, bound:  3.46842736e-01\n",
      "Epoch: 3562 mean train loss:  3.36522102e-01, bound:  3.46829683e-01\n",
      "Epoch: 3563 mean train loss:  3.36404920e-01, bound:  3.46817017e-01\n",
      "Epoch: 3564 mean train loss:  3.36287946e-01, bound:  3.46803844e-01\n",
      "Epoch: 3565 mean train loss:  3.36170763e-01, bound:  3.46791327e-01\n",
      "Epoch: 3566 mean train loss:  3.36053699e-01, bound:  3.46777946e-01\n",
      "Epoch: 3567 mean train loss:  3.35937202e-01, bound:  3.46765697e-01\n",
      "Epoch: 3568 mean train loss:  3.35821092e-01, bound:  3.46752018e-01\n",
      "Epoch: 3569 mean train loss:  3.35705727e-01, bound:  3.46740097e-01\n",
      "Epoch: 3570 mean train loss:  3.35592717e-01, bound:  3.46725941e-01\n",
      "Epoch: 3571 mean train loss:  3.35482329e-01, bound:  3.46714646e-01\n",
      "Epoch: 3572 mean train loss:  3.35375667e-01, bound:  3.46699774e-01\n",
      "Epoch: 3573 mean train loss:  3.35273176e-01, bound:  3.46689433e-01\n",
      "Epoch: 3574 mean train loss:  3.35171878e-01, bound:  3.46673638e-01\n",
      "Epoch: 3575 mean train loss:  3.35064799e-01, bound:  3.46664280e-01\n",
      "Epoch: 3576 mean train loss:  3.34940523e-01, bound:  3.46648067e-01\n",
      "Epoch: 3577 mean train loss:  3.34798098e-01, bound:  3.46638530e-01\n",
      "Epoch: 3578 mean train loss:  3.34652305e-01, bound:  3.46623808e-01\n",
      "Epoch: 3579 mean train loss:  3.34522784e-01, bound:  3.46611917e-01\n",
      "Epoch: 3580 mean train loss:  3.34414184e-01, bound:  3.46599698e-01\n",
      "Epoch: 3581 mean train loss:  3.34313810e-01, bound:  3.46584976e-01\n",
      "Epoch: 3582 mean train loss:  3.34205210e-01, bound:  3.46574187e-01\n",
      "Epoch: 3583 mean train loss:  3.34079713e-01, bound:  3.46558481e-01\n",
      "Epoch: 3584 mean train loss:  3.33946705e-01, bound:  3.46547186e-01\n",
      "Epoch: 3585 mean train loss:  3.33820164e-01, bound:  3.46533388e-01\n",
      "Epoch: 3586 mean train loss:  3.33707333e-01, bound:  3.46520185e-01\n",
      "Epoch: 3587 mean train loss:  3.33599627e-01, bound:  3.46508682e-01\n",
      "Epoch: 3588 mean train loss:  3.33486438e-01, bound:  3.46494108e-01\n",
      "Epoch: 3589 mean train loss:  3.33363652e-01, bound:  3.46483082e-01\n",
      "Epoch: 3590 mean train loss:  3.33237141e-01, bound:  3.46468955e-01\n",
      "Epoch: 3591 mean train loss:  3.33116144e-01, bound:  3.46456647e-01\n",
      "Epoch: 3592 mean train loss:  3.33001643e-01, bound:  3.46444398e-01\n",
      "Epoch: 3593 mean train loss:  3.32889318e-01, bound:  3.46430629e-01\n",
      "Epoch: 3594 mean train loss:  3.32772791e-01, bound:  3.46419305e-01\n",
      "Epoch: 3595 mean train loss:  3.32651109e-01, bound:  3.46405238e-01\n",
      "Epoch: 3596 mean train loss:  3.32528532e-01, bound:  3.46393257e-01\n",
      "Epoch: 3597 mean train loss:  3.32409054e-01, bound:  3.46380144e-01\n",
      "Epoch: 3598 mean train loss:  3.32293361e-01, bound:  3.46366942e-01\n",
      "Epoch: 3599 mean train loss:  3.32178086e-01, bound:  3.46354932e-01\n",
      "Epoch: 3600 mean train loss:  3.32060069e-01, bound:  3.46341014e-01\n",
      "Epoch: 3601 mean train loss:  3.31939340e-01, bound:  3.46329093e-01\n",
      "Epoch: 3602 mean train loss:  3.31818432e-01, bound:  3.46315533e-01\n",
      "Epoch: 3603 mean train loss:  3.31698805e-01, bound:  3.46302837e-01\n",
      "Epoch: 3604 mean train loss:  3.31581146e-01, bound:  3.46290261e-01\n",
      "Epoch: 3605 mean train loss:  3.31464052e-01, bound:  3.46276879e-01\n",
      "Epoch: 3606 mean train loss:  3.31345767e-01, bound:  3.46264899e-01\n",
      "Epoch: 3607 mean train loss:  3.31225991e-01, bound:  3.46251369e-01\n",
      "Epoch: 3608 mean train loss:  3.31105500e-01, bound:  3.46239150e-01\n",
      "Epoch: 3609 mean train loss:  3.30985248e-01, bound:  3.46226066e-01\n",
      "Epoch: 3610 mean train loss:  3.30866486e-01, bound:  3.46213222e-01\n",
      "Epoch: 3611 mean train loss:  3.30747455e-01, bound:  3.46200764e-01\n",
      "Epoch: 3612 mean train loss:  3.30628633e-01, bound:  3.46187472e-01\n",
      "Epoch: 3613 mean train loss:  3.30509305e-01, bound:  3.46175313e-01\n",
      "Epoch: 3614 mean train loss:  3.30389082e-01, bound:  3.46161991e-01\n",
      "Epoch: 3615 mean train loss:  3.30268681e-01, bound:  3.46149504e-01\n",
      "Epoch: 3616 mean train loss:  3.30148518e-01, bound:  3.46136451e-01\n",
      "Epoch: 3617 mean train loss:  3.30028474e-01, bound:  3.46123546e-01\n",
      "Epoch: 3618 mean train loss:  3.29908937e-01, bound:  3.46110910e-01\n",
      "Epoch: 3619 mean train loss:  3.29788953e-01, bound:  3.46097678e-01\n",
      "Epoch: 3620 mean train loss:  3.29668850e-01, bound:  3.46085191e-01\n",
      "Epoch: 3621 mean train loss:  3.29548270e-01, bound:  3.46071899e-01\n",
      "Epoch: 3622 mean train loss:  3.29427302e-01, bound:  3.46059412e-01\n",
      "Epoch: 3623 mean train loss:  3.29306543e-01, bound:  3.46046329e-01\n",
      "Epoch: 3624 mean train loss:  3.29185814e-01, bound:  3.46033573e-01\n",
      "Epoch: 3625 mean train loss:  3.29065293e-01, bound:  3.46020728e-01\n",
      "Epoch: 3626 mean train loss:  3.28944474e-01, bound:  3.46007705e-01\n",
      "Epoch: 3627 mean train loss:  3.28823507e-01, bound:  3.45995069e-01\n",
      "Epoch: 3628 mean train loss:  3.28702569e-01, bound:  3.45981896e-01\n",
      "Epoch: 3629 mean train loss:  3.28581333e-01, bound:  3.45969319e-01\n",
      "Epoch: 3630 mean train loss:  3.28459918e-01, bound:  3.45956147e-01\n",
      "Epoch: 3631 mean train loss:  3.28338355e-01, bound:  3.45943451e-01\n",
      "Epoch: 3632 mean train loss:  3.28217000e-01, bound:  3.45930368e-01\n",
      "Epoch: 3633 mean train loss:  3.28095406e-01, bound:  3.45917553e-01\n",
      "Epoch: 3634 mean train loss:  3.27973574e-01, bound:  3.45904648e-01\n",
      "Epoch: 3635 mean train loss:  3.27851981e-01, bound:  3.45891714e-01\n",
      "Epoch: 3636 mean train loss:  3.27730149e-01, bound:  3.45878899e-01\n",
      "Epoch: 3637 mean train loss:  3.27608049e-01, bound:  3.45865786e-01\n",
      "Epoch: 3638 mean train loss:  3.27486217e-01, bound:  3.45853001e-01\n",
      "Epoch: 3639 mean train loss:  3.27364504e-01, bound:  3.45839888e-01\n",
      "Epoch: 3640 mean train loss:  3.27241719e-01, bound:  3.45827162e-01\n",
      "Epoch: 3641 mean train loss:  3.27119648e-01, bound:  3.45813990e-01\n",
      "Epoch: 3642 mean train loss:  3.26997399e-01, bound:  3.45801324e-01\n",
      "Epoch: 3643 mean train loss:  3.26874673e-01, bound:  3.45788121e-01\n",
      "Epoch: 3644 mean train loss:  3.26752126e-01, bound:  3.45775425e-01\n",
      "Epoch: 3645 mean train loss:  3.26629549e-01, bound:  3.45762223e-01\n",
      "Epoch: 3646 mean train loss:  3.26506823e-01, bound:  3.45749497e-01\n",
      "Epoch: 3647 mean train loss:  3.26383978e-01, bound:  3.45736325e-01\n",
      "Epoch: 3648 mean train loss:  3.26260835e-01, bound:  3.45723540e-01\n",
      "Epoch: 3649 mean train loss:  3.26137751e-01, bound:  3.45710307e-01\n",
      "Epoch: 3650 mean train loss:  3.26014578e-01, bound:  3.45697552e-01\n",
      "Epoch: 3651 mean train loss:  3.25891465e-01, bound:  3.45684350e-01\n",
      "Epoch: 3652 mean train loss:  3.25768143e-01, bound:  3.45671564e-01\n",
      "Epoch: 3653 mean train loss:  3.25644970e-01, bound:  3.45658302e-01\n",
      "Epoch: 3654 mean train loss:  3.25521648e-01, bound:  3.45645666e-01\n",
      "Epoch: 3655 mean train loss:  3.25397998e-01, bound:  3.45632285e-01\n",
      "Epoch: 3656 mean train loss:  3.25274706e-01, bound:  3.45619708e-01\n",
      "Epoch: 3657 mean train loss:  3.25151354e-01, bound:  3.45606148e-01\n",
      "Epoch: 3658 mean train loss:  3.25028360e-01, bound:  3.45593721e-01\n",
      "Epoch: 3659 mean train loss:  3.24905932e-01, bound:  3.45579863e-01\n",
      "Epoch: 3660 mean train loss:  3.24784070e-01, bound:  3.45567942e-01\n",
      "Epoch: 3661 mean train loss:  3.24663877e-01, bound:  3.45553547e-01\n",
      "Epoch: 3662 mean train loss:  3.24545830e-01, bound:  3.45542341e-01\n",
      "Epoch: 3663 mean train loss:  3.24431896e-01, bound:  3.45527053e-01\n",
      "Epoch: 3664 mean train loss:  3.24323744e-01, bound:  3.45516950e-01\n",
      "Epoch: 3665 mean train loss:  3.24221641e-01, bound:  3.45500320e-01\n",
      "Epoch: 3666 mean train loss:  3.24123263e-01, bound:  3.45491856e-01\n",
      "Epoch: 3667 mean train loss:  3.24016005e-01, bound:  3.45473915e-01\n",
      "Epoch: 3668 mean train loss:  3.23884994e-01, bound:  3.45466554e-01\n",
      "Epoch: 3669 mean train loss:  3.23725700e-01, bound:  3.45449209e-01\n",
      "Epoch: 3670 mean train loss:  3.23561579e-01, bound:  3.45439821e-01\n",
      "Epoch: 3671 mean train loss:  3.23422402e-01, bound:  3.45426083e-01\n",
      "Epoch: 3672 mean train loss:  3.23315263e-01, bound:  3.45412165e-01\n",
      "Epoch: 3673 mean train loss:  3.23218167e-01, bound:  3.45401853e-01\n",
      "Epoch: 3674 mean train loss:  3.23104441e-01, bound:  3.45384896e-01\n",
      "Epoch: 3675 mean train loss:  3.22965771e-01, bound:  3.45375091e-01\n",
      "Epoch: 3676 mean train loss:  3.22820246e-01, bound:  3.45359445e-01\n",
      "Epoch: 3677 mean train loss:  3.22690755e-01, bound:  3.45347047e-01\n",
      "Epoch: 3678 mean train loss:  3.22580099e-01, bound:  3.45335275e-01\n",
      "Epoch: 3679 mean train loss:  3.22471529e-01, bound:  3.45320016e-01\n",
      "Epoch: 3680 mean train loss:  3.22349042e-01, bound:  3.45309973e-01\n",
      "Epoch: 3681 mean train loss:  3.22214037e-01, bound:  3.45294625e-01\n",
      "Epoch: 3682 mean train loss:  3.22081119e-01, bound:  3.45283061e-01\n",
      "Epoch: 3683 mean train loss:  3.21960509e-01, bound:  3.45270425e-01\n",
      "Epoch: 3684 mean train loss:  3.21847498e-01, bound:  3.45256299e-01\n",
      "Epoch: 3685 mean train loss:  3.21729660e-01, bound:  3.45245659e-01\n",
      "Epoch: 3686 mean train loss:  3.21602851e-01, bound:  3.45230758e-01\n",
      "Epoch: 3687 mean train loss:  3.21472853e-01, bound:  3.45219374e-01\n",
      "Epoch: 3688 mean train loss:  3.21347326e-01, bound:  3.45206052e-01\n",
      "Epoch: 3689 mean train loss:  3.21228564e-01, bound:  3.45192552e-01\n",
      "Epoch: 3690 mean train loss:  3.21111023e-01, bound:  3.45181048e-01\n",
      "Epoch: 3691 mean train loss:  3.20989013e-01, bound:  3.45166415e-01\n",
      "Epoch: 3692 mean train loss:  3.20862472e-01, bound:  3.45154941e-01\n",
      "Epoch: 3693 mean train loss:  3.20736110e-01, bound:  3.45141083e-01\n",
      "Epoch: 3694 mean train loss:  3.20613354e-01, bound:  3.45128238e-01\n",
      "Epoch: 3695 mean train loss:  3.20493519e-01, bound:  3.45115989e-01\n",
      "Epoch: 3696 mean train loss:  3.20372880e-01, bound:  3.45102042e-01\n",
      "Epoch: 3697 mean train loss:  3.20249766e-01, bound:  3.45090479e-01\n",
      "Epoch: 3698 mean train loss:  3.20124745e-01, bound:  3.45076561e-01\n",
      "Epoch: 3699 mean train loss:  3.19999576e-01, bound:  3.45064372e-01\n",
      "Epoch: 3700 mean train loss:  3.19876581e-01, bound:  3.45051527e-01\n",
      "Epoch: 3701 mean train loss:  3.19755107e-01, bound:  3.45038205e-01\n",
      "Epoch: 3702 mean train loss:  3.19633394e-01, bound:  3.45026255e-01\n",
      "Epoch: 3703 mean train loss:  3.19510549e-01, bound:  3.45012486e-01\n",
      "Epoch: 3704 mean train loss:  3.19385886e-01, bound:  3.45000565e-01\n",
      "Epoch: 3705 mean train loss:  3.19261819e-01, bound:  3.44987184e-01\n",
      "Epoch: 3706 mean train loss:  3.19137990e-01, bound:  3.44974518e-01\n",
      "Epoch: 3707 mean train loss:  3.19015414e-01, bound:  3.44961852e-01\n",
      "Epoch: 3708 mean train loss:  3.18892896e-01, bound:  3.44948351e-01\n",
      "Epoch: 3709 mean train loss:  3.18770111e-01, bound:  3.44936132e-01\n",
      "Epoch: 3710 mean train loss:  3.18646133e-01, bound:  3.44922483e-01\n",
      "Epoch: 3711 mean train loss:  3.18521857e-01, bound:  3.44910234e-01\n",
      "Epoch: 3712 mean train loss:  3.18398148e-01, bound:  3.44897002e-01\n",
      "Epoch: 3713 mean train loss:  3.18274170e-01, bound:  3.44884157e-01\n",
      "Epoch: 3714 mean train loss:  3.18150640e-01, bound:  3.44871432e-01\n",
      "Epoch: 3715 mean train loss:  3.18027437e-01, bound:  3.44858140e-01\n",
      "Epoch: 3716 mean train loss:  3.17903936e-01, bound:  3.44845772e-01\n",
      "Epoch: 3717 mean train loss:  3.17779928e-01, bound:  3.44832361e-01\n",
      "Epoch: 3718 mean train loss:  3.17656159e-01, bound:  3.44819993e-01\n",
      "Epoch: 3719 mean train loss:  3.17531735e-01, bound:  3.44806790e-01\n",
      "Epoch: 3720 mean train loss:  3.17407399e-01, bound:  3.44794065e-01\n",
      "Epoch: 3721 mean train loss:  3.17283452e-01, bound:  3.44781101e-01\n",
      "Epoch: 3722 mean train loss:  3.17159384e-01, bound:  3.44768077e-01\n",
      "Epoch: 3723 mean train loss:  3.17035347e-01, bound:  3.44755381e-01\n",
      "Epoch: 3724 mean train loss:  3.16911340e-01, bound:  3.44742090e-01\n",
      "Epoch: 3725 mean train loss:  3.16787094e-01, bound:  3.44729573e-01\n",
      "Epoch: 3726 mean train loss:  3.16662639e-01, bound:  3.44716191e-01\n",
      "Epoch: 3727 mean train loss:  3.16538215e-01, bound:  3.44703674e-01\n",
      "Epoch: 3728 mean train loss:  3.16413820e-01, bound:  3.44690412e-01\n",
      "Epoch: 3729 mean train loss:  3.16289306e-01, bound:  3.44677687e-01\n",
      "Epoch: 3730 mean train loss:  3.16164821e-01, bound:  3.44664633e-01\n",
      "Epoch: 3731 mean train loss:  3.16040218e-01, bound:  3.44651759e-01\n",
      "Epoch: 3732 mean train loss:  3.15915763e-01, bound:  3.44638824e-01\n",
      "Epoch: 3733 mean train loss:  3.15790951e-01, bound:  3.44625771e-01\n",
      "Epoch: 3734 mean train loss:  3.15666288e-01, bound:  3.44613016e-01\n",
      "Epoch: 3735 mean train loss:  3.15541595e-01, bound:  3.44599843e-01\n",
      "Epoch: 3736 mean train loss:  3.15416962e-01, bound:  3.44587147e-01\n",
      "Epoch: 3737 mean train loss:  3.15292239e-01, bound:  3.44573915e-01\n",
      "Epoch: 3738 mean train loss:  3.15167129e-01, bound:  3.44561249e-01\n",
      "Epoch: 3739 mean train loss:  3.15042317e-01, bound:  3.44547987e-01\n",
      "Epoch: 3740 mean train loss:  3.14917564e-01, bound:  3.44535321e-01\n",
      "Epoch: 3741 mean train loss:  3.14792484e-01, bound:  3.44521999e-01\n",
      "Epoch: 3742 mean train loss:  3.14667255e-01, bound:  3.44509274e-01\n",
      "Epoch: 3743 mean train loss:  3.14542413e-01, bound:  3.44495982e-01\n",
      "Epoch: 3744 mean train loss:  3.14417243e-01, bound:  3.44483316e-01\n",
      "Epoch: 3745 mean train loss:  3.14291984e-01, bound:  3.44469994e-01\n",
      "Epoch: 3746 mean train loss:  3.14167202e-01, bound:  3.44457388e-01\n",
      "Epoch: 3747 mean train loss:  3.14041972e-01, bound:  3.44443977e-01\n",
      "Epoch: 3748 mean train loss:  3.13916832e-01, bound:  3.44431460e-01\n",
      "Epoch: 3749 mean train loss:  3.13791692e-01, bound:  3.44417870e-01\n",
      "Epoch: 3750 mean train loss:  3.13666970e-01, bound:  3.44405502e-01\n",
      "Epoch: 3751 mean train loss:  3.13541710e-01, bound:  3.44391763e-01\n",
      "Epoch: 3752 mean train loss:  3.13417763e-01, bound:  3.44379663e-01\n",
      "Epoch: 3753 mean train loss:  3.13293546e-01, bound:  3.44365507e-01\n",
      "Epoch: 3754 mean train loss:  3.13170522e-01, bound:  3.44353974e-01\n",
      "Epoch: 3755 mean train loss:  3.13049138e-01, bound:  3.44339162e-01\n",
      "Epoch: 3756 mean train loss:  3.12930495e-01, bound:  3.44328493e-01\n",
      "Epoch: 3757 mean train loss:  3.12815785e-01, bound:  3.44312608e-01\n",
      "Epoch: 3758 mean train loss:  3.12706947e-01, bound:  3.44303250e-01\n",
      "Epoch: 3759 mean train loss:  3.12605143e-01, bound:  3.44285756e-01\n",
      "Epoch: 3760 mean train loss:  3.12508047e-01, bound:  3.44278574e-01\n",
      "Epoch: 3761 mean train loss:  3.12405646e-01, bound:  3.44259262e-01\n",
      "Epoch: 3762 mean train loss:  3.12281817e-01, bound:  3.44253540e-01\n",
      "Epoch: 3763 mean train loss:  3.12125653e-01, bound:  3.44234526e-01\n",
      "Epoch: 3764 mean train loss:  3.11953098e-01, bound:  3.44227105e-01\n",
      "Epoch: 3765 mean train loss:  3.11797857e-01, bound:  3.44212145e-01\n",
      "Epoch: 3766 mean train loss:  3.11679423e-01, bound:  3.44199210e-01\n",
      "Epoch: 3767 mean train loss:  3.11583817e-01, bound:  3.44189137e-01\n",
      "Epoch: 3768 mean train loss:  3.11481506e-01, bound:  3.44171584e-01\n",
      "Epoch: 3769 mean train loss:  3.11352611e-01, bound:  3.44163120e-01\n",
      "Epoch: 3770 mean train loss:  3.11204344e-01, bound:  3.44146013e-01\n",
      "Epoch: 3771 mean train loss:  3.11061412e-01, bound:  3.44135135e-01\n",
      "Epoch: 3772 mean train loss:  3.10941160e-01, bound:  3.44122529e-01\n",
      "Epoch: 3773 mean train loss:  3.10835063e-01, bound:  3.44107628e-01\n",
      "Epoch: 3774 mean train loss:  3.10723275e-01, bound:  3.44098210e-01\n",
      "Epoch: 3775 mean train loss:  3.10594648e-01, bound:  3.44081849e-01\n",
      "Epoch: 3776 mean train loss:  3.10457021e-01, bound:  3.44071776e-01\n",
      "Epoch: 3777 mean train loss:  3.10326427e-01, bound:  3.44057947e-01\n",
      "Epoch: 3778 mean train loss:  3.10208082e-01, bound:  3.44044745e-01\n",
      "Epoch: 3779 mean train loss:  3.10095429e-01, bound:  3.44034165e-01\n",
      "Epoch: 3780 mean train loss:  3.09976697e-01, bound:  3.44018757e-01\n",
      "Epoch: 3781 mean train loss:  3.09849173e-01, bound:  3.44008625e-01\n",
      "Epoch: 3782 mean train loss:  3.09718549e-01, bound:  3.43994081e-01\n",
      "Epoch: 3783 mean train loss:  3.09593648e-01, bound:  3.43981802e-01\n",
      "Epoch: 3784 mean train loss:  3.09474736e-01, bound:  3.43969792e-01\n",
      "Epoch: 3785 mean train loss:  3.09357584e-01, bound:  3.43955368e-01\n",
      "Epoch: 3786 mean train loss:  3.09236616e-01, bound:  3.43944639e-01\n",
      "Epoch: 3787 mean train loss:  3.09110790e-01, bound:  3.43929827e-01\n",
      "Epoch: 3788 mean train loss:  3.08984458e-01, bound:  3.43918294e-01\n",
      "Epoch: 3789 mean train loss:  3.08860809e-01, bound:  3.43905181e-01\n",
      "Epoch: 3790 mean train loss:  3.08740735e-01, bound:  3.43892008e-01\n",
      "Epoch: 3791 mean train loss:  3.08621258e-01, bound:  3.43880594e-01\n",
      "Epoch: 3792 mean train loss:  3.08500022e-01, bound:  3.43866348e-01\n",
      "Epoch: 3793 mean train loss:  3.08376372e-01, bound:  3.43855143e-01\n",
      "Epoch: 3794 mean train loss:  3.08251500e-01, bound:  3.43841314e-01\n",
      "Epoch: 3795 mean train loss:  3.08128029e-01, bound:  3.43829125e-01\n",
      "Epoch: 3796 mean train loss:  3.08006346e-01, bound:  3.43816727e-01\n",
      "Epoch: 3797 mean train loss:  3.07885677e-01, bound:  3.43803436e-01\n",
      "Epoch: 3798 mean train loss:  3.07764113e-01, bound:  3.43791872e-01\n",
      "Epoch: 3799 mean train loss:  3.07642132e-01, bound:  3.43778044e-01\n",
      "Epoch: 3800 mean train loss:  3.07518840e-01, bound:  3.43766272e-01\n",
      "Epoch: 3801 mean train loss:  3.07395548e-01, bound:  3.43752801e-01\n",
      "Epoch: 3802 mean train loss:  3.07272762e-01, bound:  3.43740344e-01\n",
      "Epoch: 3803 mean train loss:  3.07150811e-01, bound:  3.43727827e-01\n",
      "Epoch: 3804 mean train loss:  3.07029039e-01, bound:  3.43714565e-01\n",
      "Epoch: 3805 mean train loss:  3.06907535e-01, bound:  3.43702585e-01\n",
      "Epoch: 3806 mean train loss:  3.06785047e-01, bound:  3.43688965e-01\n",
      "Epoch: 3807 mean train loss:  3.06662560e-01, bound:  3.43677014e-01\n",
      "Epoch: 3808 mean train loss:  3.06539595e-01, bound:  3.43663692e-01\n",
      "Epoch: 3809 mean train loss:  3.06416780e-01, bound:  3.43651384e-01\n",
      "Epoch: 3810 mean train loss:  3.06294501e-01, bound:  3.43638569e-01\n",
      "Epoch: 3811 mean train loss:  3.06172520e-01, bound:  3.43625724e-01\n",
      "Epoch: 3812 mean train loss:  3.06050360e-01, bound:  3.43613356e-01\n",
      "Epoch: 3813 mean train loss:  3.05928320e-01, bound:  3.43600124e-01\n",
      "Epoch: 3814 mean train loss:  3.05806011e-01, bound:  3.43587965e-01\n",
      "Epoch: 3815 mean train loss:  3.05683643e-01, bound:  3.43574673e-01\n",
      "Epoch: 3816 mean train loss:  3.05561066e-01, bound:  3.43562484e-01\n",
      "Epoch: 3817 mean train loss:  3.05438459e-01, bound:  3.43549252e-01\n",
      "Epoch: 3818 mean train loss:  3.05316001e-01, bound:  3.43536884e-01\n",
      "Epoch: 3819 mean train loss:  3.05193394e-01, bound:  3.43523949e-01\n",
      "Epoch: 3820 mean train loss:  3.05070877e-01, bound:  3.43511224e-01\n",
      "Epoch: 3821 mean train loss:  3.04948509e-01, bound:  3.43498588e-01\n",
      "Epoch: 3822 mean train loss:  3.04826319e-01, bound:  3.43485624e-01\n",
      "Epoch: 3823 mean train loss:  3.04704100e-01, bound:  3.43473136e-01\n",
      "Epoch: 3824 mean train loss:  3.04581791e-01, bound:  3.43460053e-01\n",
      "Epoch: 3825 mean train loss:  3.04459274e-01, bound:  3.43447715e-01\n",
      "Epoch: 3826 mean train loss:  3.04336876e-01, bound:  3.43434483e-01\n",
      "Epoch: 3827 mean train loss:  3.04214686e-01, bound:  3.43422204e-01\n",
      "Epoch: 3828 mean train loss:  3.04091990e-01, bound:  3.43409061e-01\n",
      "Epoch: 3829 mean train loss:  3.03969532e-01, bound:  3.43396753e-01\n",
      "Epoch: 3830 mean train loss:  3.03847015e-01, bound:  3.43383610e-01\n",
      "Epoch: 3831 mean train loss:  3.03724468e-01, bound:  3.43371212e-01\n",
      "Epoch: 3832 mean train loss:  3.03602070e-01, bound:  3.43358099e-01\n",
      "Epoch: 3833 mean train loss:  3.03479582e-01, bound:  3.43345672e-01\n",
      "Epoch: 3834 mean train loss:  3.03357065e-01, bound:  3.43332559e-01\n",
      "Epoch: 3835 mean train loss:  3.03234488e-01, bound:  3.43320131e-01\n",
      "Epoch: 3836 mean train loss:  3.03112239e-01, bound:  3.43306959e-01\n",
      "Epoch: 3837 mean train loss:  3.02989513e-01, bound:  3.43294621e-01\n",
      "Epoch: 3838 mean train loss:  3.02867442e-01, bound:  3.43281329e-01\n",
      "Epoch: 3839 mean train loss:  3.02744895e-01, bound:  3.43269050e-01\n",
      "Epoch: 3840 mean train loss:  3.02622676e-01, bound:  3.43255699e-01\n",
      "Epoch: 3841 mean train loss:  3.02500457e-01, bound:  3.43243659e-01\n",
      "Epoch: 3842 mean train loss:  3.02378863e-01, bound:  3.43230009e-01\n",
      "Epoch: 3843 mean train loss:  3.02257329e-01, bound:  3.43218297e-01\n",
      "Epoch: 3844 mean train loss:  3.02136779e-01, bound:  3.43204170e-01\n",
      "Epoch: 3845 mean train loss:  3.02017182e-01, bound:  3.43193173e-01\n",
      "Epoch: 3846 mean train loss:  3.01899582e-01, bound:  3.43178183e-01\n",
      "Epoch: 3847 mean train loss:  3.01786155e-01, bound:  3.43168259e-01\n",
      "Epoch: 3848 mean train loss:  3.01677465e-01, bound:  3.43151867e-01\n",
      "Epoch: 3849 mean train loss:  3.01578254e-01, bound:  3.43143910e-01\n",
      "Epoch: 3850 mean train loss:  3.01489264e-01, bound:  3.43125135e-01\n",
      "Epoch: 3851 mean train loss:  3.01408470e-01, bound:  3.43120188e-01\n",
      "Epoch: 3852 mean train loss:  3.01319957e-01, bound:  3.43098879e-01\n",
      "Epoch: 3853 mean train loss:  3.01197618e-01, bound:  3.43096167e-01\n",
      "Epoch: 3854 mean train loss:  3.01025778e-01, bound:  3.43075305e-01\n",
      "Epoch: 3855 mean train loss:  3.00834656e-01, bound:  3.43069673e-01\n",
      "Epoch: 3856 mean train loss:  3.00678730e-01, bound:  3.43055129e-01\n",
      "Epoch: 3857 mean train loss:  3.00578445e-01, bound:  3.43041509e-01\n",
      "Epoch: 3858 mean train loss:  3.00502270e-01, bound:  3.43033642e-01\n",
      "Epoch: 3859 mean train loss:  3.00400972e-01, bound:  3.43014359e-01\n",
      "Epoch: 3860 mean train loss:  3.00256938e-01, bound:  3.43007654e-01\n",
      "Epoch: 3861 mean train loss:  3.00098538e-01, bound:  3.42990547e-01\n",
      "Epoch: 3862 mean train loss:  2.99967289e-01, bound:  3.42979103e-01\n",
      "Epoch: 3863 mean train loss:  2.99868941e-01, bound:  3.42968702e-01\n",
      "Epoch: 3864 mean train loss:  2.99772888e-01, bound:  3.42952043e-01\n",
      "Epoch: 3865 mean train loss:  2.99651772e-01, bound:  3.42944503e-01\n",
      "Epoch: 3866 mean train loss:  2.99510717e-01, bound:  3.42927933e-01\n",
      "Epoch: 3867 mean train loss:  2.99377352e-01, bound:  3.42917413e-01\n",
      "Epoch: 3868 mean train loss:  2.99265444e-01, bound:  3.42905849e-01\n",
      "Epoch: 3869 mean train loss:  2.99163043e-01, bound:  3.42890888e-01\n",
      "Epoch: 3870 mean train loss:  2.99048841e-01, bound:  3.42882425e-01\n",
      "Epoch: 3871 mean train loss:  2.98920214e-01, bound:  3.42866570e-01\n",
      "Epoch: 3872 mean train loss:  2.98790336e-01, bound:  3.42856258e-01\n",
      "Epoch: 3873 mean train loss:  2.98671842e-01, bound:  3.42843771e-01\n",
      "Epoch: 3874 mean train loss:  2.98562706e-01, bound:  3.42829645e-01\n",
      "Epoch: 3875 mean train loss:  2.98451006e-01, bound:  3.42820019e-01\n",
      "Epoch: 3876 mean train loss:  2.98329830e-01, bound:  3.42804670e-01\n",
      "Epoch: 3877 mean train loss:  2.98204452e-01, bound:  3.42794299e-01\n",
      "Epoch: 3878 mean train loss:  2.98083544e-01, bound:  3.42781037e-01\n",
      "Epoch: 3879 mean train loss:  2.97968715e-01, bound:  3.42767954e-01\n",
      "Epoch: 3880 mean train loss:  2.97856361e-01, bound:  3.42757434e-01\n",
      "Epoch: 3881 mean train loss:  2.97739714e-01, bound:  3.42742831e-01\n",
      "Epoch: 3882 mean train loss:  2.97618419e-01, bound:  3.42732519e-01\n",
      "Epoch: 3883 mean train loss:  2.97496974e-01, bound:  3.42718929e-01\n",
      "Epoch: 3884 mean train loss:  2.97378987e-01, bound:  3.42706770e-01\n",
      "Epoch: 3885 mean train loss:  2.97264189e-01, bound:  3.42695326e-01\n",
      "Epoch: 3886 mean train loss:  2.97149122e-01, bound:  3.42681527e-01\n",
      "Epoch: 3887 mean train loss:  2.97031552e-01, bound:  3.42671126e-01\n",
      "Epoch: 3888 mean train loss:  2.96912014e-01, bound:  3.42657387e-01\n",
      "Epoch: 3889 mean train loss:  2.96792537e-01, bound:  3.42646003e-01\n",
      "Epoch: 3890 mean train loss:  2.96674997e-01, bound:  3.42633575e-01\n",
      "Epoch: 3891 mean train loss:  2.96559125e-01, bound:  3.42620462e-01\n",
      "Epoch: 3892 mean train loss:  2.96443224e-01, bound:  3.42609227e-01\n",
      "Epoch: 3893 mean train loss:  2.96325892e-01, bound:  3.42595518e-01\n",
      "Epoch: 3894 mean train loss:  2.96207547e-01, bound:  3.42584372e-01\n",
      "Epoch: 3895 mean train loss:  2.96089023e-01, bound:  3.42571259e-01\n",
      "Epoch: 3896 mean train loss:  2.95971453e-01, bound:  3.42559099e-01\n",
      "Epoch: 3897 mean train loss:  2.95854568e-01, bound:  3.42546970e-01\n",
      "Epoch: 3898 mean train loss:  2.95738220e-01, bound:  3.42533886e-01\n",
      "Epoch: 3899 mean train loss:  2.95621365e-01, bound:  3.42522502e-01\n",
      "Epoch: 3900 mean train loss:  2.95503855e-01, bound:  3.42509180e-01\n",
      "Epoch: 3901 mean train loss:  2.95385957e-01, bound:  3.42497677e-01\n",
      "Epoch: 3902 mean train loss:  2.95267999e-01, bound:  3.42484802e-01\n",
      "Epoch: 3903 mean train loss:  2.95150667e-01, bound:  3.42472583e-01\n",
      "Epoch: 3904 mean train loss:  2.95033723e-01, bound:  3.42460364e-01\n",
      "Epoch: 3905 mean train loss:  2.94916481e-01, bound:  3.42447609e-01\n",
      "Epoch: 3906 mean train loss:  2.94799685e-01, bound:  3.42435867e-01\n",
      "Epoch: 3907 mean train loss:  2.94682652e-01, bound:  3.42422754e-01\n",
      "Epoch: 3908 mean train loss:  2.94565141e-01, bound:  3.42410982e-01\n",
      "Epoch: 3909 mean train loss:  2.94447660e-01, bound:  3.42398047e-01\n",
      "Epoch: 3910 mean train loss:  2.94330537e-01, bound:  3.42386097e-01\n",
      "Epoch: 3911 mean train loss:  2.94212997e-01, bound:  3.42373490e-01\n",
      "Epoch: 3912 mean train loss:  2.94095755e-01, bound:  3.42361033e-01\n",
      "Epoch: 3913 mean train loss:  2.93978631e-01, bound:  3.42348933e-01\n",
      "Epoch: 3914 mean train loss:  2.93861777e-01, bound:  3.42336059e-01\n",
      "Epoch: 3915 mean train loss:  2.93744713e-01, bound:  3.42324138e-01\n",
      "Epoch: 3916 mean train loss:  2.93627381e-01, bound:  3.42311233e-01\n",
      "Epoch: 3917 mean train loss:  2.93510109e-01, bound:  3.42299312e-01\n",
      "Epoch: 3918 mean train loss:  2.93392718e-01, bound:  3.42286497e-01\n",
      "Epoch: 3919 mean train loss:  2.93275356e-01, bound:  3.42274457e-01\n",
      "Epoch: 3920 mean train loss:  2.93157995e-01, bound:  3.42261791e-01\n",
      "Epoch: 3921 mean train loss:  2.93040991e-01, bound:  3.42249513e-01\n",
      "Epoch: 3922 mean train loss:  2.92923689e-01, bound:  3.42237025e-01\n",
      "Epoch: 3923 mean train loss:  2.92806476e-01, bound:  3.42224538e-01\n",
      "Epoch: 3924 mean train loss:  2.92688996e-01, bound:  3.42212260e-01\n",
      "Epoch: 3925 mean train loss:  2.92572200e-01, bound:  3.42199624e-01\n",
      "Epoch: 3926 mean train loss:  2.92455137e-01, bound:  3.42187434e-01\n",
      "Epoch: 3927 mean train loss:  2.92337894e-01, bound:  3.42174649e-01\n",
      "Epoch: 3928 mean train loss:  2.92220592e-01, bound:  3.42162579e-01\n",
      "Epoch: 3929 mean train loss:  2.92103499e-01, bound:  3.42149764e-01\n",
      "Epoch: 3930 mean train loss:  2.91986346e-01, bound:  3.42137724e-01\n",
      "Epoch: 3931 mean train loss:  2.91869193e-01, bound:  3.42124850e-01\n",
      "Epoch: 3932 mean train loss:  2.91752100e-01, bound:  3.42112809e-01\n",
      "Epoch: 3933 mean train loss:  2.91635156e-01, bound:  3.42099935e-01\n",
      "Epoch: 3934 mean train loss:  2.91517437e-01, bound:  3.42088044e-01\n",
      "Epoch: 3935 mean train loss:  2.91400522e-01, bound:  3.42074990e-01\n",
      "Epoch: 3936 mean train loss:  2.91283399e-01, bound:  3.42063159e-01\n",
      "Epoch: 3937 mean train loss:  2.91166723e-01, bound:  3.42049986e-01\n",
      "Epoch: 3938 mean train loss:  2.91050076e-01, bound:  3.42038393e-01\n",
      "Epoch: 3939 mean train loss:  2.90932983e-01, bound:  3.42024922e-01\n",
      "Epoch: 3940 mean train loss:  2.90816635e-01, bound:  3.42013657e-01\n",
      "Epoch: 3941 mean train loss:  2.90700734e-01, bound:  3.41999710e-01\n",
      "Epoch: 3942 mean train loss:  2.90585876e-01, bound:  3.41989040e-01\n",
      "Epoch: 3943 mean train loss:  2.90472388e-01, bound:  3.41974407e-01\n",
      "Epoch: 3944 mean train loss:  2.90361136e-01, bound:  3.41964722e-01\n",
      "Epoch: 3945 mean train loss:  2.90253967e-01, bound:  3.41948777e-01\n",
      "Epoch: 3946 mean train loss:  2.90152341e-01, bound:  3.41940850e-01\n",
      "Epoch: 3947 mean train loss:  2.90059477e-01, bound:  3.41922790e-01\n",
      "Epoch: 3948 mean train loss:  2.89977074e-01, bound:  3.41917574e-01\n",
      "Epoch: 3949 mean train loss:  2.89900422e-01, bound:  3.41896713e-01\n",
      "Epoch: 3950 mean train loss:  2.89817393e-01, bound:  3.41894627e-01\n",
      "Epoch: 3951 mean train loss:  2.89701819e-01, bound:  3.41872066e-01\n",
      "Epoch: 3952 mean train loss:  2.89540023e-01, bound:  3.41870189e-01\n",
      "Epoch: 3953 mean train loss:  2.89353460e-01, bound:  3.41851026e-01\n",
      "Epoch: 3954 mean train loss:  2.89192945e-01, bound:  3.41843188e-01\n",
      "Epoch: 3955 mean train loss:  2.89085150e-01, bound:  3.41831774e-01\n",
      "Epoch: 3956 mean train loss:  2.89010793e-01, bound:  3.41815382e-01\n",
      "Epoch: 3957 mean train loss:  2.88926750e-01, bound:  3.41809511e-01\n",
      "Epoch: 3958 mean train loss:  2.88803905e-01, bound:  3.41789633e-01\n",
      "Epoch: 3959 mean train loss:  2.88652748e-01, bound:  3.41782987e-01\n",
      "Epoch: 3960 mean train loss:  2.88510174e-01, bound:  3.41767639e-01\n",
      "Epoch: 3961 mean train loss:  2.88400650e-01, bound:  3.41755241e-01\n",
      "Epoch: 3962 mean train loss:  2.88311601e-01, bound:  3.41746479e-01\n",
      "Epoch: 3963 mean train loss:  2.88212568e-01, bound:  3.41729224e-01\n",
      "Epoch: 3964 mean train loss:  2.88089871e-01, bound:  3.41722250e-01\n",
      "Epoch: 3965 mean train loss:  2.87955016e-01, bound:  3.41706157e-01\n",
      "Epoch: 3966 mean train loss:  2.87830055e-01, bound:  3.41695815e-01\n",
      "Epoch: 3967 mean train loss:  2.87724018e-01, bound:  3.41684937e-01\n",
      "Epoch: 3968 mean train loss:  2.87624866e-01, bound:  3.41669917e-01\n",
      "Epoch: 3969 mean train loss:  2.87516534e-01, bound:  3.41661900e-01\n",
      "Epoch: 3970 mean train loss:  2.87394732e-01, bound:  3.41645867e-01\n",
      "Epoch: 3971 mean train loss:  2.87270039e-01, bound:  3.41636270e-01\n",
      "Epoch: 3972 mean train loss:  2.87153631e-01, bound:  3.41623485e-01\n",
      "Epoch: 3973 mean train loss:  2.87046999e-01, bound:  3.41610223e-01\n",
      "Epoch: 3974 mean train loss:  2.86941469e-01, bound:  3.41600537e-01\n",
      "Epoch: 3975 mean train loss:  2.86829323e-01, bound:  3.41585219e-01\n",
      "Epoch: 3976 mean train loss:  2.86711067e-01, bound:  3.41575742e-01\n",
      "Epoch: 3977 mean train loss:  2.86592066e-01, bound:  3.41561884e-01\n",
      "Epoch: 3978 mean train loss:  2.86478251e-01, bound:  3.41550201e-01\n",
      "Epoch: 3979 mean train loss:  2.86368966e-01, bound:  3.41539085e-01\n",
      "Epoch: 3980 mean train loss:  2.86260217e-01, bound:  3.41525048e-01\n",
      "Epoch: 3981 mean train loss:  2.86148161e-01, bound:  3.41515243e-01\n",
      "Epoch: 3982 mean train loss:  2.86032706e-01, bound:  3.41500968e-01\n",
      "Epoch: 3983 mean train loss:  2.85916597e-01, bound:  3.41490269e-01\n",
      "Epoch: 3984 mean train loss:  2.85802782e-01, bound:  3.41477871e-01\n",
      "Epoch: 3985 mean train loss:  2.85691500e-01, bound:  3.41465473e-01\n",
      "Epoch: 3986 mean train loss:  2.85580814e-01, bound:  3.41454744e-01\n",
      "Epoch: 3987 mean train loss:  2.85469383e-01, bound:  3.41440916e-01\n",
      "Epoch: 3988 mean train loss:  2.85355955e-01, bound:  3.41430426e-01\n",
      "Epoch: 3989 mean train loss:  2.85241574e-01, bound:  3.41416776e-01\n",
      "Epoch: 3990 mean train loss:  2.85127461e-01, bound:  3.41405481e-01\n",
      "Epoch: 3991 mean train loss:  2.85014629e-01, bound:  3.41393203e-01\n",
      "Epoch: 3992 mean train loss:  2.84902751e-01, bound:  3.41380626e-01\n",
      "Epoch: 3993 mean train loss:  2.84791410e-01, bound:  3.41369361e-01\n",
      "Epoch: 3994 mean train loss:  2.84679204e-01, bound:  3.41355920e-01\n",
      "Epoch: 3995 mean train loss:  2.84566224e-01, bound:  3.41344982e-01\n",
      "Epoch: 3996 mean train loss:  2.84452736e-01, bound:  3.41331780e-01\n",
      "Epoch: 3997 mean train loss:  2.84339428e-01, bound:  3.41320395e-01\n",
      "Epoch: 3998 mean train loss:  2.84226090e-01, bound:  3.41307938e-01\n",
      "Epoch: 3999 mean train loss:  2.84113646e-01, bound:  3.41295660e-01\n",
      "Epoch: 4000 mean train loss:  2.84001827e-01, bound:  3.41283947e-01\n",
      "Epoch: 4001 mean train loss:  2.83889204e-01, bound:  3.41270983e-01\n",
      "Epoch: 4002 mean train loss:  2.83776760e-01, bound:  3.41259658e-01\n",
      "Epoch: 4003 mean train loss:  2.83663869e-01, bound:  3.41246545e-01\n",
      "Epoch: 4004 mean train loss:  2.83550829e-01, bound:  3.41235191e-01\n",
      "Epoch: 4005 mean train loss:  2.83437610e-01, bound:  3.41222316e-01\n",
      "Epoch: 4006 mean train loss:  2.83324599e-01, bound:  3.41210514e-01\n",
      "Epoch: 4007 mean train loss:  2.83211857e-01, bound:  3.41198057e-01\n",
      "Epoch: 4008 mean train loss:  2.83098966e-01, bound:  3.41185778e-01\n",
      "Epoch: 4009 mean train loss:  2.82986343e-01, bound:  3.41173857e-01\n",
      "Epoch: 4010 mean train loss:  2.82873720e-01, bound:  3.41161221e-01\n",
      "Epoch: 4011 mean train loss:  2.82760710e-01, bound:  3.41149479e-01\n",
      "Epoch: 4012 mean train loss:  2.82648325e-01, bound:  3.41136515e-01\n",
      "Epoch: 4013 mean train loss:  2.82535106e-01, bound:  3.41124892e-01\n",
      "Epoch: 4014 mean train loss:  2.82422334e-01, bound:  3.41111988e-01\n",
      "Epoch: 4015 mean train loss:  2.82309473e-01, bound:  3.41100365e-01\n",
      "Epoch: 4016 mean train loss:  2.82196283e-01, bound:  3.41087610e-01\n",
      "Epoch: 4017 mean train loss:  2.82083362e-01, bound:  3.41075748e-01\n",
      "Epoch: 4018 mean train loss:  2.81970382e-01, bound:  3.41063112e-01\n",
      "Epoch: 4019 mean train loss:  2.81857371e-01, bound:  3.41051042e-01\n",
      "Epoch: 4020 mean train loss:  2.81744301e-01, bound:  3.41038525e-01\n",
      "Epoch: 4021 mean train loss:  2.81631023e-01, bound:  3.41026336e-01\n",
      "Epoch: 4022 mean train loss:  2.81518012e-01, bound:  3.41013938e-01\n",
      "Epoch: 4023 mean train loss:  2.81405091e-01, bound:  3.41001630e-01\n",
      "Epoch: 4024 mean train loss:  2.81292081e-01, bound:  3.40989321e-01\n",
      "Epoch: 4025 mean train loss:  2.81178981e-01, bound:  3.40976894e-01\n",
      "Epoch: 4026 mean train loss:  2.81065822e-01, bound:  3.40964735e-01\n",
      "Epoch: 4027 mean train loss:  2.80952990e-01, bound:  3.40952098e-01\n",
      "Epoch: 4028 mean train loss:  2.80839860e-01, bound:  3.40940088e-01\n",
      "Epoch: 4029 mean train loss:  2.80726850e-01, bound:  3.40927362e-01\n",
      "Epoch: 4030 mean train loss:  2.80613333e-01, bound:  3.40915501e-01\n",
      "Epoch: 4031 mean train loss:  2.80500650e-01, bound:  3.40902507e-01\n",
      "Epoch: 4032 mean train loss:  2.80387819e-01, bound:  3.40890855e-01\n",
      "Epoch: 4033 mean train loss:  2.80275017e-01, bound:  3.40877652e-01\n",
      "Epoch: 4034 mean train loss:  2.80162305e-01, bound:  3.40866357e-01\n",
      "Epoch: 4035 mean train loss:  2.80050486e-01, bound:  3.40852588e-01\n",
      "Epoch: 4036 mean train loss:  2.79939413e-01, bound:  3.40841949e-01\n",
      "Epoch: 4037 mean train loss:  2.79829770e-01, bound:  3.40827286e-01\n",
      "Epoch: 4038 mean train loss:  2.79723227e-01, bound:  3.40817869e-01\n",
      "Epoch: 4039 mean train loss:  2.79621512e-01, bound:  3.40801686e-01\n",
      "Epoch: 4040 mean train loss:  2.79527485e-01, bound:  3.40794295e-01\n",
      "Epoch: 4041 mean train loss:  2.79446512e-01, bound:  3.40775460e-01\n",
      "Epoch: 4042 mean train loss:  2.79383034e-01, bound:  3.40771616e-01\n",
      "Epoch: 4043 mean train loss:  2.79334188e-01, bound:  3.40748936e-01\n",
      "Epoch: 4044 mean train loss:  2.79278666e-01, bound:  3.40749472e-01\n",
      "Epoch: 4045 mean train loss:  2.79172629e-01, bound:  3.40724140e-01\n",
      "Epoch: 4046 mean train loss:  2.78985560e-01, bound:  3.40725273e-01\n",
      "Epoch: 4047 mean train loss:  2.78760672e-01, bound:  3.40704739e-01\n",
      "Epoch: 4048 mean train loss:  2.78591067e-01, bound:  3.40697169e-01\n",
      "Epoch: 4049 mean train loss:  2.78513491e-01, bound:  3.40687722e-01\n",
      "Epoch: 4050 mean train loss:  2.78472036e-01, bound:  3.40668768e-01\n",
      "Epoch: 4051 mean train loss:  2.78384447e-01, bound:  3.40665579e-01\n",
      "Epoch: 4052 mean train loss:  2.78226286e-01, bound:  3.40644330e-01\n",
      "Epoch: 4053 mean train loss:  2.78056353e-01, bound:  3.40637296e-01\n",
      "Epoch: 4054 mean train loss:  2.77940363e-01, bound:  3.40624630e-01\n",
      "Epoch: 4055 mean train loss:  2.77870804e-01, bound:  3.40608746e-01\n",
      "Epoch: 4056 mean train loss:  2.77788579e-01, bound:  3.40603322e-01\n",
      "Epoch: 4057 mean train loss:  2.77659357e-01, bound:  3.40584278e-01\n",
      "Epoch: 4058 mean train loss:  2.77511716e-01, bound:  3.40576738e-01\n",
      "Epoch: 4059 mean train loss:  2.77393401e-01, bound:  3.40563864e-01\n",
      "Epoch: 4060 mean train loss:  2.77307212e-01, bound:  3.40549380e-01\n",
      "Epoch: 4061 mean train loss:  2.77216941e-01, bound:  3.40542674e-01\n",
      "Epoch: 4062 mean train loss:  2.77098030e-01, bound:  3.40525359e-01\n",
      "Epoch: 4063 mean train loss:  2.76966453e-01, bound:  3.40517074e-01\n",
      "Epoch: 4064 mean train loss:  2.76850373e-01, bound:  3.40504080e-01\n",
      "Epoch: 4065 mean train loss:  2.76754677e-01, bound:  3.40490192e-01\n",
      "Epoch: 4066 mean train loss:  2.76657969e-01, bound:  3.40481967e-01\n",
      "Epoch: 4067 mean train loss:  2.76544988e-01, bound:  3.40465724e-01\n",
      "Epoch: 4068 mean train loss:  2.76422292e-01, bound:  3.40456605e-01\n",
      "Epoch: 4069 mean train loss:  2.76308209e-01, bound:  3.40443403e-01\n",
      "Epoch: 4070 mean train loss:  2.76205957e-01, bound:  3.40430081e-01\n",
      "Epoch: 4071 mean train loss:  2.76105970e-01, bound:  3.40420842e-01\n",
      "Epoch: 4072 mean train loss:  2.75996417e-01, bound:  3.40405554e-01\n",
      "Epoch: 4073 mean train loss:  2.75879920e-01, bound:  3.40396136e-01\n",
      "Epoch: 4074 mean train loss:  2.75766432e-01, bound:  3.40382814e-01\n",
      "Epoch: 4075 mean train loss:  2.75659680e-01, bound:  3.40370178e-01\n",
      "Epoch: 4076 mean train loss:  2.75556743e-01, bound:  3.40359986e-01\n",
      "Epoch: 4077 mean train loss:  2.75449306e-01, bound:  3.40345472e-01\n",
      "Epoch: 4078 mean train loss:  2.75337607e-01, bound:  3.40335995e-01\n",
      "Epoch: 4079 mean train loss:  2.75224239e-01, bound:  3.40322644e-01\n",
      "Epoch: 4080 mean train loss:  2.75115222e-01, bound:  3.40310782e-01\n",
      "Epoch: 4081 mean train loss:  2.75009215e-01, bound:  3.40299636e-01\n",
      "Epoch: 4082 mean train loss:  2.74903089e-01, bound:  3.40285689e-01\n",
      "Epoch: 4083 mean train loss:  2.74793833e-01, bound:  3.40275526e-01\n",
      "Epoch: 4084 mean train loss:  2.74682403e-01, bound:  3.40261817e-01\n",
      "Epoch: 4085 mean train loss:  2.74571866e-01, bound:  3.40250492e-01\n",
      "Epoch: 4086 mean train loss:  2.74463266e-01, bound:  3.40238482e-01\n",
      "Epoch: 4087 mean train loss:  2.74356127e-01, bound:  3.40225399e-01\n",
      "Epoch: 4088 mean train loss:  2.74248570e-01, bound:  3.40214580e-01\n",
      "Epoch: 4089 mean train loss:  2.74139136e-01, bound:  3.40200961e-01\n",
      "Epoch: 4090 mean train loss:  2.74028867e-01, bound:  3.40189934e-01\n",
      "Epoch: 4091 mean train loss:  2.73919046e-01, bound:  3.40177208e-01\n",
      "Epoch: 4092 mean train loss:  2.73809910e-01, bound:  3.40165049e-01\n",
      "Epoch: 4093 mean train loss:  2.73701936e-01, bound:  3.40153426e-01\n",
      "Epoch: 4094 mean train loss:  2.73593605e-01, bound:  3.40140253e-01\n",
      "Epoch: 4095 mean train loss:  2.73484498e-01, bound:  3.40129137e-01\n",
      "Epoch: 4096 mean train loss:  2.73374736e-01, bound:  3.40115935e-01\n",
      "Epoch: 4097 mean train loss:  2.73265302e-01, bound:  3.40104371e-01\n",
      "Epoch: 4098 mean train loss:  2.73155600e-01, bound:  3.40091884e-01\n",
      "Epoch: 4099 mean train loss:  2.73046762e-01, bound:  3.40079397e-01\n",
      "Epoch: 4100 mean train loss:  2.72937924e-01, bound:  3.40067685e-01\n",
      "Epoch: 4101 mean train loss:  2.72828937e-01, bound:  3.40054661e-01\n",
      "Epoch: 4102 mean train loss:  2.72719711e-01, bound:  3.40043187e-01\n",
      "Epoch: 4103 mean train loss:  2.72610128e-01, bound:  3.40030164e-01\n",
      "Epoch: 4104 mean train loss:  2.72500247e-01, bound:  3.40018362e-01\n",
      "Epoch: 4105 mean train loss:  2.72390634e-01, bound:  3.40005755e-01\n",
      "Epoch: 4106 mean train loss:  2.72281349e-01, bound:  3.39993387e-01\n",
      "Epoch: 4107 mean train loss:  2.72171974e-01, bound:  3.39981347e-01\n",
      "Epoch: 4108 mean train loss:  2.72062749e-01, bound:  3.39968592e-01\n",
      "Epoch: 4109 mean train loss:  2.71953195e-01, bound:  3.39956850e-01\n",
      "Epoch: 4110 mean train loss:  2.71843553e-01, bound:  3.39943886e-01\n",
      "Epoch: 4111 mean train loss:  2.71734118e-01, bound:  3.39932024e-01\n",
      "Epoch: 4112 mean train loss:  2.71624058e-01, bound:  3.39919209e-01\n",
      "Epoch: 4113 mean train loss:  2.71513879e-01, bound:  3.39907080e-01\n",
      "Epoch: 4114 mean train loss:  2.71404445e-01, bound:  3.39894563e-01\n",
      "Epoch: 4115 mean train loss:  2.71294266e-01, bound:  3.39882106e-01\n",
      "Epoch: 4116 mean train loss:  2.71184683e-01, bound:  3.39869946e-01\n",
      "Epoch: 4117 mean train loss:  2.71074861e-01, bound:  3.39857191e-01\n",
      "Epoch: 4118 mean train loss:  2.70965308e-01, bound:  3.39845061e-01\n",
      "Epoch: 4119 mean train loss:  2.70855159e-01, bound:  3.39832217e-01\n",
      "Epoch: 4120 mean train loss:  2.70745069e-01, bound:  3.39820147e-01\n",
      "Epoch: 4121 mean train loss:  2.70634979e-01, bound:  3.39807332e-01\n",
      "Epoch: 4122 mean train loss:  2.70524830e-01, bound:  3.39795202e-01\n",
      "Epoch: 4123 mean train loss:  2.70414501e-01, bound:  3.39782417e-01\n",
      "Epoch: 4124 mean train loss:  2.70304412e-01, bound:  3.39770228e-01\n",
      "Epoch: 4125 mean train loss:  2.70194203e-01, bound:  3.39757502e-01\n",
      "Epoch: 4126 mean train loss:  2.70083874e-01, bound:  3.39745134e-01\n",
      "Epoch: 4127 mean train loss:  2.69973695e-01, bound:  3.39732558e-01\n",
      "Epoch: 4128 mean train loss:  2.69863099e-01, bound:  3.39720070e-01\n",
      "Epoch: 4129 mean train loss:  2.69752771e-01, bound:  3.39707583e-01\n",
      "Epoch: 4130 mean train loss:  2.69642383e-01, bound:  3.39694947e-01\n",
      "Epoch: 4131 mean train loss:  2.69531935e-01, bound:  3.39682519e-01\n",
      "Epoch: 4132 mean train loss:  2.69421071e-01, bound:  3.39669794e-01\n",
      "Epoch: 4133 mean train loss:  2.69310832e-01, bound:  3.39657426e-01\n",
      "Epoch: 4134 mean train loss:  2.69200027e-01, bound:  3.39644551e-01\n",
      "Epoch: 4135 mean train loss:  2.69089609e-01, bound:  3.39632332e-01\n",
      "Epoch: 4136 mean train loss:  2.68978447e-01, bound:  3.39619398e-01\n",
      "Epoch: 4137 mean train loss:  2.68868178e-01, bound:  3.39607179e-01\n",
      "Epoch: 4138 mean train loss:  2.68757433e-01, bound:  3.39594036e-01\n",
      "Epoch: 4139 mean train loss:  2.68646419e-01, bound:  3.39582086e-01\n",
      "Epoch: 4140 mean train loss:  2.68535912e-01, bound:  3.39568704e-01\n",
      "Epoch: 4141 mean train loss:  2.68425226e-01, bound:  3.39556962e-01\n",
      "Epoch: 4142 mean train loss:  2.68314958e-01, bound:  3.39543194e-01\n",
      "Epoch: 4143 mean train loss:  2.68205404e-01, bound:  3.39531958e-01\n",
      "Epoch: 4144 mean train loss:  2.68096417e-01, bound:  3.39517504e-01\n",
      "Epoch: 4145 mean train loss:  2.67988890e-01, bound:  3.39507133e-01\n",
      "Epoch: 4146 mean train loss:  2.67883748e-01, bound:  3.39491516e-01\n",
      "Epoch: 4147 mean train loss:  2.67784208e-01, bound:  3.39482695e-01\n",
      "Epoch: 4148 mean train loss:  2.67692387e-01, bound:  3.39464933e-01\n",
      "Epoch: 4149 mean train loss:  2.67613769e-01, bound:  3.39459002e-01\n",
      "Epoch: 4150 mean train loss:  2.67554522e-01, bound:  3.39437783e-01\n",
      "Epoch: 4151 mean train loss:  2.67516136e-01, bound:  3.39436352e-01\n",
      "Epoch: 4152 mean train loss:  2.67482519e-01, bound:  3.39410663e-01\n",
      "Epoch: 4153 mean train loss:  2.67407984e-01, bound:  3.39413524e-01\n",
      "Epoch: 4154 mean train loss:  2.67239213e-01, bound:  3.39386851e-01\n",
      "Epoch: 4155 mean train loss:  2.66995370e-01, bound:  3.39387149e-01\n",
      "Epoch: 4156 mean train loss:  2.66782671e-01, bound:  3.39369297e-01\n",
      "Epoch: 4157 mean train loss:  2.66681165e-01, bound:  3.39357227e-01\n",
      "Epoch: 4158 mean train loss:  2.66654044e-01, bound:  3.39351326e-01\n",
      "Epoch: 4159 mean train loss:  2.66598046e-01, bound:  3.39328855e-01\n",
      "Epoch: 4160 mean train loss:  2.66453743e-01, bound:  3.39326262e-01\n",
      "Epoch: 4161 mean train loss:  2.66266376e-01, bound:  3.39306265e-01\n",
      "Epoch: 4162 mean train loss:  2.66128302e-01, bound:  3.39296132e-01\n",
      "Epoch: 4163 mean train loss:  2.66060382e-01, bound:  3.39287013e-01\n",
      "Epoch: 4164 mean train loss:  2.65996754e-01, bound:  3.39267790e-01\n",
      "Epoch: 4165 mean train loss:  2.65879184e-01, bound:  3.39263141e-01\n",
      "Epoch: 4166 mean train loss:  2.65724421e-01, bound:  3.39244604e-01\n",
      "Epoch: 4167 mean train loss:  2.65594363e-01, bound:  3.39234471e-01\n",
      "Epoch: 4168 mean train loss:  2.65508950e-01, bound:  3.39224547e-01\n",
      "Epoch: 4169 mean train loss:  2.65429586e-01, bound:  3.39207321e-01\n",
      "Epoch: 4170 mean train loss:  2.65318006e-01, bound:  3.39201152e-01\n",
      "Epoch: 4171 mean train loss:  2.65182793e-01, bound:  3.39184076e-01\n",
      "Epoch: 4172 mean train loss:  2.65061229e-01, bound:  3.39173436e-01\n",
      "Epoch: 4173 mean train loss:  2.64966935e-01, bound:  3.39162678e-01\n",
      "Epoch: 4174 mean train loss:  2.64877051e-01, bound:  3.39146435e-01\n",
      "Epoch: 4175 mean train loss:  2.64768094e-01, bound:  3.39138776e-01\n",
      "Epoch: 4176 mean train loss:  2.64644414e-01, bound:  3.39122504e-01\n",
      "Epoch: 4177 mean train loss:  2.64527857e-01, bound:  3.39111477e-01\n",
      "Epoch: 4178 mean train loss:  2.64427662e-01, bound:  3.39100063e-01\n",
      "Epoch: 4179 mean train loss:  2.64331192e-01, bound:  3.39084715e-01\n",
      "Epoch: 4180 mean train loss:  2.64225036e-01, bound:  3.39076102e-01\n",
      "Epoch: 4181 mean train loss:  2.64107943e-01, bound:  3.39060545e-01\n",
      "Epoch: 4182 mean train loss:  2.63993680e-01, bound:  3.39049518e-01\n",
      "Epoch: 4183 mean train loss:  2.63889462e-01, bound:  3.39037418e-01\n",
      "Epoch: 4184 mean train loss:  2.63788879e-01, bound:  3.39022994e-01\n",
      "Epoch: 4185 mean train loss:  2.63683915e-01, bound:  3.39013577e-01\n",
      "Epoch: 4186 mean train loss:  2.63572425e-01, bound:  3.38998705e-01\n",
      "Epoch: 4187 mean train loss:  2.63459712e-01, bound:  3.38988036e-01\n",
      "Epoch: 4188 mean train loss:  2.63352096e-01, bound:  3.38975310e-01\n",
      "Epoch: 4189 mean train loss:  2.63247818e-01, bound:  3.38961631e-01\n",
      "Epoch: 4190 mean train loss:  2.63143659e-01, bound:  3.38951141e-01\n",
      "Epoch: 4191 mean train loss:  2.63035089e-01, bound:  3.38936448e-01\n",
      "Epoch: 4192 mean train loss:  2.62924522e-01, bound:  3.38925719e-01\n",
      "Epoch: 4193 mean train loss:  2.62814850e-01, bound:  3.38912427e-01\n",
      "Epoch: 4194 mean train loss:  2.62707889e-01, bound:  3.38899583e-01\n",
      "Epoch: 4195 mean train loss:  2.62602538e-01, bound:  3.38888139e-01\n",
      "Epoch: 4196 mean train loss:  2.62495995e-01, bound:  3.38873923e-01\n",
      "Epoch: 4197 mean train loss:  2.62387395e-01, bound:  3.38863015e-01\n",
      "Epoch: 4198 mean train loss:  2.62278020e-01, bound:  3.38849276e-01\n",
      "Epoch: 4199 mean train loss:  2.62168884e-01, bound:  3.38837266e-01\n",
      "Epoch: 4200 mean train loss:  2.62061626e-01, bound:  3.38824898e-01\n",
      "Epoch: 4201 mean train loss:  2.61954784e-01, bound:  3.38811457e-01\n",
      "Epoch: 4202 mean train loss:  2.61847556e-01, bound:  3.38799983e-01\n",
      "Epoch: 4203 mean train loss:  2.61738926e-01, bound:  3.38786095e-01\n",
      "Epoch: 4204 mean train loss:  2.61629909e-01, bound:  3.38774532e-01\n",
      "Epoch: 4205 mean train loss:  2.61521161e-01, bound:  3.38761300e-01\n",
      "Epoch: 4206 mean train loss:  2.61412710e-01, bound:  3.38748664e-01\n",
      "Epoch: 4207 mean train loss:  2.61304915e-01, bound:  3.38736385e-01\n",
      "Epoch: 4208 mean train loss:  2.61197269e-01, bound:  3.38722885e-01\n",
      "Epoch: 4209 mean train loss:  2.61089087e-01, bound:  3.38711143e-01\n",
      "Epoch: 4210 mean train loss:  2.60980278e-01, bound:  3.38697553e-01\n",
      "Epoch: 4211 mean train loss:  2.60871291e-01, bound:  3.38685542e-01\n",
      "Epoch: 4212 mean train loss:  2.60762304e-01, bound:  3.38672340e-01\n",
      "Epoch: 4213 mean train loss:  2.60653496e-01, bound:  3.38659555e-01\n",
      "Epoch: 4214 mean train loss:  2.60544837e-01, bound:  3.38647038e-01\n",
      "Epoch: 4215 mean train loss:  2.60436267e-01, bound:  3.38633835e-01\n",
      "Epoch: 4216 mean train loss:  2.60327607e-01, bound:  3.38621736e-01\n",
      "Epoch: 4217 mean train loss:  2.60218590e-01, bound:  3.38608205e-01\n",
      "Epoch: 4218 mean train loss:  2.60109603e-01, bound:  3.38595957e-01\n",
      "Epoch: 4219 mean train loss:  2.60000229e-01, bound:  3.38582635e-01\n",
      "Epoch: 4220 mean train loss:  2.59890735e-01, bound:  3.38570088e-01\n",
      "Epoch: 4221 mean train loss:  2.59781897e-01, bound:  3.38557154e-01\n",
      "Epoch: 4222 mean train loss:  2.59672552e-01, bound:  3.38544190e-01\n",
      "Epoch: 4223 mean train loss:  2.59563029e-01, bound:  3.38531584e-01\n",
      "Epoch: 4224 mean train loss:  2.59453744e-01, bound:  3.38518202e-01\n",
      "Epoch: 4225 mean train loss:  2.59344548e-01, bound:  3.38505715e-01\n",
      "Epoch: 4226 mean train loss:  2.59234935e-01, bound:  3.38492364e-01\n",
      "Epoch: 4227 mean train loss:  2.59125292e-01, bound:  3.38479936e-01\n",
      "Epoch: 4228 mean train loss:  2.59015769e-01, bound:  3.38466614e-01\n",
      "Epoch: 4229 mean train loss:  2.58905739e-01, bound:  3.38453948e-01\n",
      "Epoch: 4230 mean train loss:  2.58795857e-01, bound:  3.38440716e-01\n",
      "Epoch: 4231 mean train loss:  2.58685827e-01, bound:  3.38427901e-01\n",
      "Epoch: 4232 mean train loss:  2.58576035e-01, bound:  3.38414878e-01\n",
      "Epoch: 4233 mean train loss:  2.58465618e-01, bound:  3.38401824e-01\n",
      "Epoch: 4234 mean train loss:  2.58355677e-01, bound:  3.38388950e-01\n",
      "Epoch: 4235 mean train loss:  2.58245289e-01, bound:  3.38375717e-01\n",
      "Epoch: 4236 mean train loss:  2.58135319e-01, bound:  3.38362932e-01\n",
      "Epoch: 4237 mean train loss:  2.58024842e-01, bound:  3.38349581e-01\n",
      "Epoch: 4238 mean train loss:  2.57914662e-01, bound:  3.38336885e-01\n",
      "Epoch: 4239 mean train loss:  2.57804036e-01, bound:  3.38323414e-01\n",
      "Epoch: 4240 mean train loss:  2.57693440e-01, bound:  3.38310748e-01\n",
      "Epoch: 4241 mean train loss:  2.57583052e-01, bound:  3.38297158e-01\n",
      "Epoch: 4242 mean train loss:  2.57472128e-01, bound:  3.38284552e-01\n",
      "Epoch: 4243 mean train loss:  2.57361501e-01, bound:  3.38270962e-01\n",
      "Epoch: 4244 mean train loss:  2.57250756e-01, bound:  3.38258386e-01\n",
      "Epoch: 4245 mean train loss:  2.57139683e-01, bound:  3.38244557e-01\n",
      "Epoch: 4246 mean train loss:  2.57029325e-01, bound:  3.38232130e-01\n",
      "Epoch: 4247 mean train loss:  2.56918609e-01, bound:  3.38218153e-01\n",
      "Epoch: 4248 mean train loss:  2.56807417e-01, bound:  3.38205963e-01\n",
      "Epoch: 4249 mean train loss:  2.56696969e-01, bound:  3.38191658e-01\n",
      "Epoch: 4250 mean train loss:  2.56586939e-01, bound:  3.38179827e-01\n",
      "Epoch: 4251 mean train loss:  2.56477356e-01, bound:  3.38164896e-01\n",
      "Epoch: 4252 mean train loss:  2.56368637e-01, bound:  3.38153839e-01\n",
      "Epoch: 4253 mean train loss:  2.56262094e-01, bound:  3.38137835e-01\n",
      "Epoch: 4254 mean train loss:  2.56158739e-01, bound:  3.38128120e-01\n",
      "Epoch: 4255 mean train loss:  2.56060719e-01, bound:  3.38110447e-01\n",
      "Epoch: 4256 mean train loss:  2.55971938e-01, bound:  3.38103026e-01\n",
      "Epoch: 4257 mean train loss:  2.55897105e-01, bound:  3.38082403e-01\n",
      "Epoch: 4258 mean train loss:  2.55841643e-01, bound:  3.38078678e-01\n",
      "Epoch: 4259 mean train loss:  2.55804479e-01, bound:  3.38054031e-01\n",
      "Epoch: 4260 mean train loss:  2.55767167e-01, bound:  3.38055015e-01\n",
      "Epoch: 4261 mean train loss:  2.55683780e-01, bound:  3.38027030e-01\n",
      "Epoch: 4262 mean train loss:  2.55510777e-01, bound:  3.38029593e-01\n",
      "Epoch: 4263 mean train loss:  2.55267501e-01, bound:  3.38005334e-01\n",
      "Epoch: 4264 mean train loss:  2.55051583e-01, bound:  3.37999821e-01\n",
      "Epoch: 4265 mean train loss:  2.54938930e-01, bound:  3.37987155e-01\n",
      "Epoch: 4266 mean train loss:  2.54905492e-01, bound:  3.37968826e-01\n",
      "Epoch: 4267 mean train loss:  2.54858524e-01, bound:  3.37965131e-01\n",
      "Epoch: 4268 mean train loss:  2.54733950e-01, bound:  3.37941319e-01\n",
      "Epoch: 4269 mean train loss:  2.54550368e-01, bound:  3.37936461e-01\n",
      "Epoch: 4270 mean train loss:  2.54389882e-01, bound:  3.37919682e-01\n",
      "Epoch: 4271 mean train loss:  2.54298270e-01, bound:  3.37905616e-01\n",
      "Epoch: 4272 mean train loss:  2.54238993e-01, bound:  3.37898195e-01\n",
      "Epoch: 4273 mean train loss:  2.54148304e-01, bound:  3.37877393e-01\n",
      "Epoch: 4274 mean train loss:  2.54007757e-01, bound:  3.37871581e-01\n",
      "Epoch: 4275 mean train loss:  2.53858119e-01, bound:  3.37854147e-01\n",
      "Epoch: 4276 mean train loss:  2.53744572e-01, bound:  3.37842017e-01\n",
      "Epoch: 4277 mean train loss:  2.53662944e-01, bound:  3.37832481e-01\n",
      "Epoch: 4278 mean train loss:  2.53575236e-01, bound:  3.37814063e-01\n",
      "Epoch: 4279 mean train loss:  2.53456831e-01, bound:  3.37806880e-01\n",
      "Epoch: 4280 mean train loss:  2.53323048e-01, bound:  3.37789476e-01\n",
      "Epoch: 4281 mean train loss:  2.53202498e-01, bound:  3.37778062e-01\n",
      "Epoch: 4282 mean train loss:  2.53105491e-01, bound:  3.37766618e-01\n",
      "Epoch: 4283 mean train loss:  2.53013074e-01, bound:  3.37749928e-01\n",
      "Epoch: 4284 mean train loss:  2.52905965e-01, bound:  3.37741375e-01\n",
      "Epoch: 4285 mean train loss:  2.52784431e-01, bound:  3.37724268e-01\n",
      "Epoch: 4286 mean train loss:  2.52664268e-01, bound:  3.37713420e-01\n",
      "Epoch: 4287 mean train loss:  2.52556801e-01, bound:  3.37700516e-01\n",
      "Epoch: 4288 mean train loss:  2.52457798e-01, bound:  3.37685436e-01\n",
      "Epoch: 4289 mean train loss:  2.52355099e-01, bound:  3.37675631e-01\n",
      "Epoch: 4290 mean train loss:  2.52243072e-01, bound:  3.37659031e-01\n",
      "Epoch: 4291 mean train loss:  2.52126187e-01, bound:  3.37648392e-01\n",
      "Epoch: 4292 mean train loss:  2.52012968e-01, bound:  3.37634444e-01\n",
      "Epoch: 4293 mean train loss:  2.51907289e-01, bound:  3.37620884e-01\n",
      "Epoch: 4294 mean train loss:  2.51803607e-01, bound:  3.37609947e-01\n",
      "Epoch: 4295 mean train loss:  2.51696944e-01, bound:  3.37594271e-01\n",
      "Epoch: 4296 mean train loss:  2.51585186e-01, bound:  3.37583542e-01\n",
      "Epoch: 4297 mean train loss:  2.51471549e-01, bound:  3.37568581e-01\n",
      "Epoch: 4298 mean train loss:  2.51360565e-01, bound:  3.37556064e-01\n",
      "Epoch: 4299 mean train loss:  2.51253128e-01, bound:  3.37543488e-01\n",
      "Epoch: 4300 mean train loss:  2.51146942e-01, bound:  3.37528914e-01\n",
      "Epoch: 4301 mean train loss:  2.51039058e-01, bound:  3.37517649e-01\n",
      "Epoch: 4302 mean train loss:  2.50928640e-01, bound:  3.37502420e-01\n",
      "Epoch: 4303 mean train loss:  2.50816762e-01, bound:  3.37490678e-01\n",
      "Epoch: 4304 mean train loss:  2.50705749e-01, bound:  3.37476701e-01\n",
      "Epoch: 4305 mean train loss:  2.50596255e-01, bound:  3.37463498e-01\n",
      "Epoch: 4306 mean train loss:  2.50488102e-01, bound:  3.37451071e-01\n",
      "Epoch: 4307 mean train loss:  2.50379860e-01, bound:  3.37436527e-01\n",
      "Epoch: 4308 mean train loss:  2.50270009e-01, bound:  3.37424666e-01\n",
      "Epoch: 4309 mean train loss:  2.50159323e-01, bound:  3.37410003e-01\n",
      "Epoch: 4310 mean train loss:  2.50048131e-01, bound:  3.37397724e-01\n",
      "Epoch: 4311 mean train loss:  2.49937177e-01, bound:  3.37383986e-01\n",
      "Epoch: 4312 mean train loss:  2.49827310e-01, bound:  3.37370634e-01\n",
      "Epoch: 4313 mean train loss:  2.49717787e-01, bound:  3.37357759e-01\n",
      "Epoch: 4314 mean train loss:  2.49608159e-01, bound:  3.37343514e-01\n",
      "Epoch: 4315 mean train loss:  2.49498367e-01, bound:  3.37331206e-01\n",
      "Epoch: 4316 mean train loss:  2.49387249e-01, bound:  3.37316751e-01\n",
      "Epoch: 4317 mean train loss:  2.49276593e-01, bound:  3.37304264e-01\n",
      "Epoch: 4318 mean train loss:  2.49165177e-01, bound:  3.37290138e-01\n",
      "Epoch: 4319 mean train loss:  2.49054357e-01, bound:  3.37276995e-01\n",
      "Epoch: 4320 mean train loss:  2.48943448e-01, bound:  3.37263554e-01\n",
      "Epoch: 4321 mean train loss:  2.48832941e-01, bound:  3.37249815e-01\n",
      "Epoch: 4322 mean train loss:  2.48722389e-01, bound:  3.37236881e-01\n",
      "Epoch: 4323 mean train loss:  2.48611212e-01, bound:  3.37222785e-01\n",
      "Epoch: 4324 mean train loss:  2.48500198e-01, bound:  3.37210000e-01\n",
      "Epoch: 4325 mean train loss:  2.48389080e-01, bound:  3.37195724e-01\n",
      "Epoch: 4326 mean train loss:  2.48277262e-01, bound:  3.37182879e-01\n",
      "Epoch: 4327 mean train loss:  2.48165533e-01, bound:  3.37168783e-01\n",
      "Epoch: 4328 mean train loss:  2.48054028e-01, bound:  3.37155670e-01\n",
      "Epoch: 4329 mean train loss:  2.47942120e-01, bound:  3.37141812e-01\n",
      "Epoch: 4330 mean train loss:  2.47830525e-01, bound:  3.37128341e-01\n",
      "Epoch: 4331 mean train loss:  2.47718379e-01, bound:  3.37114781e-01\n",
      "Epoch: 4332 mean train loss:  2.47606769e-01, bound:  3.37100983e-01\n",
      "Epoch: 4333 mean train loss:  2.47495010e-01, bound:  3.37087601e-01\n",
      "Epoch: 4334 mean train loss:  2.47382984e-01, bound:  3.37073594e-01\n",
      "Epoch: 4335 mean train loss:  2.47270942e-01, bound:  3.37060422e-01\n",
      "Epoch: 4336 mean train loss:  2.47158810e-01, bound:  3.37046236e-01\n",
      "Epoch: 4337 mean train loss:  2.47046456e-01, bound:  3.37033093e-01\n",
      "Epoch: 4338 mean train loss:  2.46934101e-01, bound:  3.37018818e-01\n",
      "Epoch: 4339 mean train loss:  2.46821254e-01, bound:  3.37005734e-01\n",
      "Epoch: 4340 mean train loss:  2.46708751e-01, bound:  3.36991400e-01\n",
      "Epoch: 4341 mean train loss:  2.46596321e-01, bound:  3.36978346e-01\n",
      "Epoch: 4342 mean train loss:  2.46483728e-01, bound:  3.36963862e-01\n",
      "Epoch: 4343 mean train loss:  2.46370763e-01, bound:  3.36950928e-01\n",
      "Epoch: 4344 mean train loss:  2.46257767e-01, bound:  3.36936235e-01\n",
      "Epoch: 4345 mean train loss:  2.46145010e-01, bound:  3.36923480e-01\n",
      "Epoch: 4346 mean train loss:  2.46032208e-01, bound:  3.36908489e-01\n",
      "Epoch: 4347 mean train loss:  2.45919704e-01, bound:  3.36896002e-01\n",
      "Epoch: 4348 mean train loss:  2.45807603e-01, bound:  3.36880624e-01\n",
      "Epoch: 4349 mean train loss:  2.45695889e-01, bound:  3.36868674e-01\n",
      "Epoch: 4350 mean train loss:  2.45585263e-01, bound:  3.36852521e-01\n",
      "Epoch: 4351 mean train loss:  2.45476425e-01, bound:  3.36841553e-01\n",
      "Epoch: 4352 mean train loss:  2.45370716e-01, bound:  3.36824089e-01\n",
      "Epoch: 4353 mean train loss:  2.45270655e-01, bound:  3.36814731e-01\n",
      "Epoch: 4354 mean train loss:  2.45178744e-01, bound:  3.36795211e-01\n",
      "Epoch: 4355 mean train loss:  2.45100304e-01, bound:  3.36788684e-01\n",
      "Epoch: 4356 mean train loss:  2.45040581e-01, bound:  3.36765677e-01\n",
      "Epoch: 4357 mean train loss:  2.45000467e-01, bound:  3.36763412e-01\n",
      "Epoch: 4358 mean train loss:  2.44966805e-01, bound:  3.36736232e-01\n",
      "Epoch: 4359 mean train loss:  2.44902164e-01, bound:  3.36738139e-01\n",
      "Epoch: 4360 mean train loss:  2.44755521e-01, bound:  3.36709470e-01\n",
      "Epoch: 4361 mean train loss:  2.44519949e-01, bound:  3.36710036e-01\n",
      "Epoch: 4362 mean train loss:  2.44273260e-01, bound:  3.36688191e-01\n",
      "Epoch: 4363 mean train loss:  2.44112819e-01, bound:  3.36678058e-01\n",
      "Epoch: 4364 mean train loss:  2.44054303e-01, bound:  3.36668134e-01\n",
      "Epoch: 4365 mean train loss:  2.44025394e-01, bound:  3.36646378e-01\n",
      "Epoch: 4366 mean train loss:  2.43940026e-01, bound:  3.36642981e-01\n",
      "Epoch: 4367 mean train loss:  2.43774086e-01, bound:  3.36619347e-01\n",
      "Epoch: 4368 mean train loss:  2.43586496e-01, bound:  3.36612374e-01\n",
      "Epoch: 4369 mean train loss:  2.43451431e-01, bound:  3.36597174e-01\n",
      "Epoch: 4370 mean train loss:  2.43378058e-01, bound:  3.36580604e-01\n",
      "Epoch: 4371 mean train loss:  2.43312091e-01, bound:  3.36573511e-01\n",
      "Epoch: 4372 mean train loss:  2.43201584e-01, bound:  3.36551875e-01\n",
      "Epoch: 4373 mean train loss:  2.43049756e-01, bound:  3.36544931e-01\n",
      "Epoch: 4374 mean train loss:  2.42903352e-01, bound:  3.36527914e-01\n",
      "Epoch: 4375 mean train loss:  2.42795706e-01, bound:  3.36514294e-01\n",
      "Epoch: 4376 mean train loss:  2.42713735e-01, bound:  3.36504459e-01\n",
      "Epoch: 4377 mean train loss:  2.42619708e-01, bound:  3.36485237e-01\n",
      "Epoch: 4378 mean train loss:  2.42496178e-01, bound:  3.36477190e-01\n",
      "Epoch: 4379 mean train loss:  2.42359668e-01, bound:  3.36459368e-01\n",
      "Epoch: 4380 mean train loss:  2.42237553e-01, bound:  3.36447269e-01\n",
      "Epoch: 4381 mean train loss:  2.42136508e-01, bound:  3.36435080e-01\n",
      "Epoch: 4382 mean train loss:  2.42041603e-01, bound:  3.36417913e-01\n",
      "Epoch: 4383 mean train loss:  2.41934076e-01, bound:  3.36408615e-01\n",
      "Epoch: 4384 mean train loss:  2.41811827e-01, bound:  3.36390823e-01\n",
      "Epoch: 4385 mean train loss:  2.41687328e-01, bound:  3.36379707e-01\n",
      "Epoch: 4386 mean train loss:  2.41573185e-01, bound:  3.36365610e-01\n",
      "Epoch: 4387 mean train loss:  2.41469666e-01, bound:  3.36350441e-01\n",
      "Epoch: 4388 mean train loss:  2.41366208e-01, bound:  3.36339563e-01\n",
      "Epoch: 4389 mean train loss:  2.41255224e-01, bound:  3.36322397e-01\n",
      "Epoch: 4390 mean train loss:  2.41137132e-01, bound:  3.36311698e-01\n",
      "Epoch: 4391 mean train loss:  2.41018683e-01, bound:  3.36296290e-01\n",
      "Epoch: 4392 mean train loss:  2.40905404e-01, bound:  3.36283058e-01\n",
      "Epoch: 4393 mean train loss:  2.40797833e-01, bound:  3.36270452e-01\n",
      "Epoch: 4394 mean train loss:  2.40690738e-01, bound:  3.36254507e-01\n",
      "Epoch: 4395 mean train loss:  2.40580216e-01, bound:  3.36243153e-01\n",
      "Epoch: 4396 mean train loss:  2.40465060e-01, bound:  3.36227000e-01\n",
      "Epoch: 4397 mean train loss:  2.40349039e-01, bound:  3.36214960e-01\n",
      "Epoch: 4398 mean train loss:  2.40234777e-01, bound:  3.36200446e-01\n",
      "Epoch: 4399 mean train loss:  2.40123510e-01, bound:  3.36186290e-01\n",
      "Epoch: 4400 mean train loss:  2.40013570e-01, bound:  3.36173534e-01\n",
      "Epoch: 4401 mean train loss:  2.39903182e-01, bound:  3.36157918e-01\n",
      "Epoch: 4402 mean train loss:  2.39790544e-01, bound:  3.36145937e-01\n",
      "Epoch: 4403 mean train loss:  2.39676222e-01, bound:  3.36130470e-01\n",
      "Epoch: 4404 mean train loss:  2.39561811e-01, bound:  3.36117864e-01\n",
      "Epoch: 4405 mean train loss:  2.39447922e-01, bound:  3.36103350e-01\n",
      "Epoch: 4406 mean train loss:  2.39335313e-01, bound:  3.36089283e-01\n",
      "Epoch: 4407 mean train loss:  2.39223272e-01, bound:  3.36076081e-01\n",
      "Epoch: 4408 mean train loss:  2.39111423e-01, bound:  3.36061060e-01\n",
      "Epoch: 4409 mean train loss:  2.38998637e-01, bound:  3.36048484e-01\n",
      "Epoch: 4410 mean train loss:  2.38885224e-01, bound:  3.36033255e-01\n",
      "Epoch: 4411 mean train loss:  2.38770664e-01, bound:  3.36020410e-01\n",
      "Epoch: 4412 mean train loss:  2.38656417e-01, bound:  3.36005509e-01\n",
      "Epoch: 4413 mean train loss:  2.38542110e-01, bound:  3.35991919e-01\n",
      "Epoch: 4414 mean train loss:  2.38428861e-01, bound:  3.35977912e-01\n",
      "Epoch: 4415 mean train loss:  2.38315135e-01, bound:  3.35963517e-01\n",
      "Epoch: 4416 mean train loss:  2.38202050e-01, bound:  3.35950136e-01\n",
      "Epoch: 4417 mean train loss:  2.38088235e-01, bound:  3.35935146e-01\n",
      "Epoch: 4418 mean train loss:  2.37974435e-01, bound:  3.35922033e-01\n",
      "Epoch: 4419 mean train loss:  2.37860158e-01, bound:  3.35906893e-01\n",
      "Epoch: 4420 mean train loss:  2.37745658e-01, bound:  3.35893780e-01\n",
      "Epoch: 4421 mean train loss:  2.37630978e-01, bound:  3.35878819e-01\n",
      "Epoch: 4422 mean train loss:  2.37515941e-01, bound:  3.35865349e-01\n",
      "Epoch: 4423 mean train loss:  2.37401217e-01, bound:  3.35850716e-01\n",
      "Epoch: 4424 mean train loss:  2.37286568e-01, bound:  3.35836768e-01\n",
      "Epoch: 4425 mean train loss:  2.37171739e-01, bound:  3.35822552e-01\n",
      "Epoch: 4426 mean train loss:  2.37056687e-01, bound:  3.35808307e-01\n",
      "Epoch: 4427 mean train loss:  2.36941740e-01, bound:  3.35794330e-01\n",
      "Epoch: 4428 mean train loss:  2.36826673e-01, bound:  3.35779667e-01\n",
      "Epoch: 4429 mean train loss:  2.36711740e-01, bound:  3.35765988e-01\n",
      "Epoch: 4430 mean train loss:  2.36596331e-01, bound:  3.35751086e-01\n",
      "Epoch: 4431 mean train loss:  2.36480862e-01, bound:  3.35737526e-01\n",
      "Epoch: 4432 mean train loss:  2.36365438e-01, bound:  3.35722536e-01\n",
      "Epoch: 4433 mean train loss:  2.36249909e-01, bound:  3.35709065e-01\n",
      "Epoch: 4434 mean train loss:  2.36134514e-01, bound:  3.35693777e-01\n",
      "Epoch: 4435 mean train loss:  2.36018762e-01, bound:  3.35680485e-01\n",
      "Epoch: 4436 mean train loss:  2.35903159e-01, bound:  3.35665077e-01\n",
      "Epoch: 4437 mean train loss:  2.35787481e-01, bound:  3.35652024e-01\n",
      "Epoch: 4438 mean train loss:  2.35671803e-01, bound:  3.35636288e-01\n",
      "Epoch: 4439 mean train loss:  2.35556901e-01, bound:  3.35623503e-01\n",
      "Epoch: 4440 mean train loss:  2.35442266e-01, bound:  3.35607260e-01\n",
      "Epoch: 4441 mean train loss:  2.35328317e-01, bound:  3.35595161e-01\n",
      "Epoch: 4442 mean train loss:  2.35215962e-01, bound:  3.35578024e-01\n",
      "Epoch: 4443 mean train loss:  2.35106081e-01, bound:  3.35566968e-01\n",
      "Epoch: 4444 mean train loss:  2.35000372e-01, bound:  3.35548431e-01\n",
      "Epoch: 4445 mean train loss:  2.34900877e-01, bound:  3.35539192e-01\n",
      "Epoch: 4446 mean train loss:  2.34811738e-01, bound:  3.35518330e-01\n",
      "Epoch: 4447 mean train loss:  2.34737217e-01, bound:  3.35512072e-01\n",
      "Epoch: 4448 mean train loss:  2.34680921e-01, bound:  3.35487813e-01\n",
      "Epoch: 4449 mean train loss:  2.34640345e-01, bound:  3.35485667e-01\n",
      "Epoch: 4450 mean train loss:  2.34596059e-01, bound:  3.35457534e-01\n",
      "Epoch: 4451 mean train loss:  2.34506413e-01, bound:  3.35458905e-01\n",
      "Epoch: 4452 mean train loss:  2.34332576e-01, bound:  3.35430324e-01\n",
      "Epoch: 4453 mean train loss:  2.34086558e-01, bound:  3.35429251e-01\n",
      "Epoch: 4454 mean train loss:  2.33849749e-01, bound:  3.35407943e-01\n",
      "Epoch: 4455 mean train loss:  2.33701453e-01, bound:  3.35396379e-01\n",
      "Epoch: 4456 mean train loss:  2.33642161e-01, bound:  3.35386097e-01\n",
      "Epoch: 4457 mean train loss:  2.33605117e-01, bound:  3.35363954e-01\n",
      "Epoch: 4458 mean train loss:  2.33516768e-01, bound:  3.35359663e-01\n",
      "Epoch: 4459 mean train loss:  2.33355328e-01, bound:  3.35335821e-01\n",
      "Epoch: 4460 mean train loss:  2.33167097e-01, bound:  3.35328609e-01\n",
      "Epoch: 4461 mean train loss:  2.33018309e-01, bound:  3.35312039e-01\n",
      "Epoch: 4462 mean train loss:  2.32928753e-01, bound:  3.35296035e-01\n",
      "Epoch: 4463 mean train loss:  2.32860163e-01, bound:  3.35287333e-01\n",
      "Epoch: 4464 mean train loss:  2.32763112e-01, bound:  3.35265785e-01\n",
      "Epoch: 4465 mean train loss:  2.32622951e-01, bound:  3.35258663e-01\n",
      "Epoch: 4466 mean train loss:  2.32468128e-01, bound:  3.35239679e-01\n",
      "Epoch: 4467 mean train loss:  2.32336715e-01, bound:  3.35227489e-01\n",
      "Epoch: 4468 mean train loss:  2.32237935e-01, bound:  3.35215002e-01\n",
      "Epoch: 4469 mean train loss:  2.32148930e-01, bound:  3.35196584e-01\n",
      "Epoch: 4470 mean train loss:  2.32042924e-01, bound:  3.35187823e-01\n",
      "Epoch: 4471 mean train loss:  2.31913581e-01, bound:  3.35168362e-01\n",
      "Epoch: 4472 mean train loss:  2.31777132e-01, bound:  3.35157961e-01\n",
      "Epoch: 4473 mean train loss:  2.31653288e-01, bound:  3.35142553e-01\n",
      "Epoch: 4474 mean train loss:  2.31547028e-01, bound:  3.35127264e-01\n",
      "Epoch: 4475 mean train loss:  2.31446475e-01, bound:  3.35116059e-01\n",
      "Epoch: 4476 mean train loss:  2.31337368e-01, bound:  3.35097760e-01\n",
      "Epoch: 4477 mean train loss:  2.31216267e-01, bound:  3.35087538e-01\n",
      "Epoch: 4478 mean train loss:  2.31089652e-01, bound:  3.35070372e-01\n",
      "Epoch: 4479 mean train loss:  2.30967656e-01, bound:  3.35057586e-01\n",
      "Epoch: 4480 mean train loss:  2.30854884e-01, bound:  3.35043788e-01\n",
      "Epoch: 4481 mean train loss:  2.30746880e-01, bound:  3.35027695e-01\n",
      "Epoch: 4482 mean train loss:  2.30637491e-01, bound:  3.35016280e-01\n",
      "Epoch: 4483 mean train loss:  2.30521962e-01, bound:  3.34998965e-01\n",
      "Epoch: 4484 mean train loss:  2.30401576e-01, bound:  3.34987551e-01\n",
      "Epoch: 4485 mean train loss:  2.30280444e-01, bound:  3.34971488e-01\n",
      "Epoch: 4486 mean train loss:  2.30162397e-01, bound:  3.34957808e-01\n",
      "Epoch: 4487 mean train loss:  2.30048835e-01, bound:  3.34943950e-01\n",
      "Epoch: 4488 mean train loss:  2.29936808e-01, bound:  3.34928185e-01\n",
      "Epoch: 4489 mean train loss:  2.29823872e-01, bound:  3.34915966e-01\n",
      "Epoch: 4490 mean train loss:  2.29708076e-01, bound:  3.34899366e-01\n",
      "Epoch: 4491 mean train loss:  2.29590014e-01, bound:  3.34887058e-01\n",
      "Epoch: 4492 mean train loss:  2.29471013e-01, bound:  3.34871083e-01\n",
      "Epoch: 4493 mean train loss:  2.29353443e-01, bound:  3.34857553e-01\n",
      "Epoch: 4494 mean train loss:  2.29236946e-01, bound:  3.34843099e-01\n",
      "Epoch: 4495 mean train loss:  2.29121953e-01, bound:  3.34828258e-01\n",
      "Epoch: 4496 mean train loss:  2.29007065e-01, bound:  3.34814996e-01\n",
      "Epoch: 4497 mean train loss:  2.28891671e-01, bound:  3.34799111e-01\n",
      "Epoch: 4498 mean train loss:  2.28775352e-01, bound:  3.34786266e-01\n",
      "Epoch: 4499 mean train loss:  2.28657722e-01, bound:  3.34770352e-01\n",
      "Epoch: 4500 mean train loss:  2.28540301e-01, bound:  3.34757298e-01\n",
      "Epoch: 4501 mean train loss:  2.28421837e-01, bound:  3.34741890e-01\n",
      "Epoch: 4502 mean train loss:  2.28303999e-01, bound:  3.34728003e-01\n",
      "Epoch: 4503 mean train loss:  2.28187159e-01, bound:  3.34713399e-01\n",
      "Epoch: 4504 mean train loss:  2.28070140e-01, bound:  3.34698617e-01\n",
      "Epoch: 4505 mean train loss:  2.27953315e-01, bound:  3.34684730e-01\n",
      "Epoch: 4506 mean train loss:  2.27836266e-01, bound:  3.34669381e-01\n",
      "Epoch: 4507 mean train loss:  2.27719024e-01, bound:  3.34655881e-01\n",
      "Epoch: 4508 mean train loss:  2.27601916e-01, bound:  3.34640145e-01\n",
      "Epoch: 4509 mean train loss:  2.27484018e-01, bound:  3.34626794e-01\n",
      "Epoch: 4510 mean train loss:  2.27366090e-01, bound:  3.34611058e-01\n",
      "Epoch: 4511 mean train loss:  2.27247581e-01, bound:  3.34597558e-01\n",
      "Epoch: 4512 mean train loss:  2.27129251e-01, bound:  3.34582001e-01\n",
      "Epoch: 4513 mean train loss:  2.27010787e-01, bound:  3.34568292e-01\n",
      "Epoch: 4514 mean train loss:  2.26891935e-01, bound:  3.34552884e-01\n",
      "Epoch: 4515 mean train loss:  2.26773262e-01, bound:  3.34538937e-01\n",
      "Epoch: 4516 mean train loss:  2.26654723e-01, bound:  3.34523767e-01\n",
      "Epoch: 4517 mean train loss:  2.26536304e-01, bound:  3.34509581e-01\n",
      "Epoch: 4518 mean train loss:  2.26416856e-01, bound:  3.34494531e-01\n",
      "Epoch: 4519 mean train loss:  2.26298004e-01, bound:  3.34480196e-01\n",
      "Epoch: 4520 mean train loss:  2.26178974e-01, bound:  3.34465176e-01\n",
      "Epoch: 4521 mean train loss:  2.26059511e-01, bound:  3.34450781e-01\n",
      "Epoch: 4522 mean train loss:  2.25940526e-01, bound:  3.34435821e-01\n",
      "Epoch: 4523 mean train loss:  2.25821137e-01, bound:  3.34421396e-01\n",
      "Epoch: 4524 mean train loss:  2.25701794e-01, bound:  3.34406376e-01\n",
      "Epoch: 4525 mean train loss:  2.25581780e-01, bound:  3.34391981e-01\n",
      "Epoch: 4526 mean train loss:  2.25462660e-01, bound:  3.34376872e-01\n",
      "Epoch: 4527 mean train loss:  2.25342587e-01, bound:  3.34362566e-01\n",
      "Epoch: 4528 mean train loss:  2.25222945e-01, bound:  3.34347278e-01\n",
      "Epoch: 4529 mean train loss:  2.25103289e-01, bound:  3.34333181e-01\n",
      "Epoch: 4530 mean train loss:  2.24983618e-01, bound:  3.34317476e-01\n",
      "Epoch: 4531 mean train loss:  2.24864259e-01, bound:  3.34303796e-01\n",
      "Epoch: 4532 mean train loss:  2.24745929e-01, bound:  3.34287494e-01\n",
      "Epoch: 4533 mean train loss:  2.24628821e-01, bound:  3.34274650e-01\n",
      "Epoch: 4534 mean train loss:  2.24515095e-01, bound:  3.34257185e-01\n",
      "Epoch: 4535 mean train loss:  2.24406958e-01, bound:  3.34245950e-01\n",
      "Epoch: 4536 mean train loss:  2.24310875e-01, bound:  3.34226161e-01\n",
      "Epoch: 4537 mean train loss:  2.24236727e-01, bound:  3.34218144e-01\n",
      "Epoch: 4538 mean train loss:  2.24202141e-01, bound:  3.34194094e-01\n",
      "Epoch: 4539 mean train loss:  2.24228531e-01, bound:  3.34191978e-01\n",
      "Epoch: 4540 mean train loss:  2.24321231e-01, bound:  3.34161043e-01\n",
      "Epoch: 4541 mean train loss:  2.24408656e-01, bound:  3.34166914e-01\n",
      "Epoch: 4542 mean train loss:  2.24303454e-01, bound:  3.34131181e-01\n",
      "Epoch: 4543 mean train loss:  2.23883078e-01, bound:  3.34137291e-01\n",
      "Epoch: 4544 mean train loss:  2.23395735e-01, bound:  3.34111989e-01\n",
      "Epoch: 4545 mean train loss:  2.23220557e-01, bound:  3.34100634e-01\n",
      "Epoch: 4546 mean train loss:  2.23335579e-01, bound:  3.34095478e-01\n",
      "Epoch: 4547 mean train loss:  2.23369986e-01, bound:  3.34066987e-01\n",
      "Epoch: 4548 mean train loss:  2.23104477e-01, bound:  3.34067017e-01\n",
      "Epoch: 4549 mean train loss:  2.22766861e-01, bound:  3.34044397e-01\n",
      "Epoch: 4550 mean train loss:  2.22668096e-01, bound:  3.34030092e-01\n",
      "Epoch: 4551 mean train loss:  2.22717360e-01, bound:  3.34024608e-01\n",
      "Epoch: 4552 mean train loss:  2.22619459e-01, bound:  3.33998710e-01\n",
      "Epoch: 4553 mean train loss:  2.22355083e-01, bound:  3.33993495e-01\n",
      "Epoch: 4554 mean train loss:  2.22172633e-01, bound:  3.33976328e-01\n",
      "Epoch: 4555 mean train loss:  2.22150296e-01, bound:  3.33957970e-01\n",
      "Epoch: 4556 mean train loss:  2.22097442e-01, bound:  3.33952934e-01\n",
      "Epoch: 4557 mean train loss:  2.21908972e-01, bound:  3.33930433e-01\n",
      "Epoch: 4558 mean train loss:  2.21718773e-01, bound:  3.33920300e-01\n",
      "Epoch: 4559 mean train loss:  2.21641138e-01, bound:  3.33907932e-01\n",
      "Epoch: 4560 mean train loss:  2.21587241e-01, bound:  3.33887428e-01\n",
      "Epoch: 4561 mean train loss:  2.21446261e-01, bound:  3.33880693e-01\n",
      "Epoch: 4562 mean train loss:  2.21271098e-01, bound:  3.33862215e-01\n",
      "Epoch: 4563 mean train loss:  2.21161097e-01, bound:  3.33847851e-01\n",
      "Epoch: 4564 mean train loss:  2.21092030e-01, bound:  3.33838046e-01\n",
      "Epoch: 4565 mean train loss:  2.20978096e-01, bound:  3.33817959e-01\n",
      "Epoch: 4566 mean train loss:  2.20822841e-01, bound:  3.33808422e-01\n",
      "Epoch: 4567 mean train loss:  2.20695943e-01, bound:  3.33793432e-01\n",
      "Epoch: 4568 mean train loss:  2.20608562e-01, bound:  3.33776772e-01\n",
      "Epoch: 4569 mean train loss:  2.20507622e-01, bound:  3.33767265e-01\n",
      "Epoch: 4570 mean train loss:  2.20371425e-01, bound:  3.33748788e-01\n",
      "Epoch: 4571 mean train loss:  2.20238358e-01, bound:  3.33736688e-01\n",
      "Epoch: 4572 mean train loss:  2.20134586e-01, bound:  3.33723843e-01\n",
      "Epoch: 4573 mean train loss:  2.20035762e-01, bound:  3.33706677e-01\n",
      "Epoch: 4574 mean train loss:  2.19915196e-01, bound:  3.33696425e-01\n",
      "Epoch: 4575 mean train loss:  2.19783947e-01, bound:  3.33679795e-01\n",
      "Epoch: 4576 mean train loss:  2.19667867e-01, bound:  3.33665788e-01\n",
      "Epoch: 4577 mean train loss:  2.19564348e-01, bound:  3.33653599e-01\n",
      "Epoch: 4578 mean train loss:  2.19452858e-01, bound:  3.33636463e-01\n",
      "Epoch: 4579 mean train loss:  2.19329000e-01, bound:  3.33625078e-01\n",
      "Epoch: 4580 mean train loss:  2.19206259e-01, bound:  3.33609641e-01\n",
      "Epoch: 4581 mean train loss:  2.19095260e-01, bound:  3.33594739e-01\n",
      "Epoch: 4582 mean train loss:  2.18986437e-01, bound:  3.33582520e-01\n",
      "Epoch: 4583 mean train loss:  2.18870148e-01, bound:  3.33565921e-01\n",
      "Epoch: 4584 mean train loss:  2.18748078e-01, bound:  3.33553582e-01\n",
      "Epoch: 4585 mean train loss:  2.18630284e-01, bound:  3.33538920e-01\n",
      "Epoch: 4586 mean train loss:  2.18518168e-01, bound:  3.33523899e-01\n",
      "Epoch: 4587 mean train loss:  2.18405411e-01, bound:  3.33511353e-01\n",
      "Epoch: 4588 mean train loss:  2.18287975e-01, bound:  3.33495229e-01\n",
      "Epoch: 4589 mean train loss:  2.18168050e-01, bound:  3.33482414e-01\n",
      "Epoch: 4590 mean train loss:  2.18051538e-01, bound:  3.33467960e-01\n",
      "Epoch: 4591 mean train loss:  2.17937186e-01, bound:  3.33453119e-01\n",
      "Epoch: 4592 mean train loss:  2.17822999e-01, bound:  3.33440304e-01\n",
      "Epoch: 4593 mean train loss:  2.17705920e-01, bound:  3.33424568e-01\n",
      "Epoch: 4594 mean train loss:  2.17586800e-01, bound:  3.33411366e-01\n",
      "Epoch: 4595 mean train loss:  2.17469528e-01, bound:  3.33396882e-01\n",
      "Epoch: 4596 mean train loss:  2.17353925e-01, bound:  3.33382130e-01\n",
      "Epoch: 4597 mean train loss:  2.17238143e-01, bound:  3.33368897e-01\n",
      "Epoch: 4598 mean train loss:  2.17121214e-01, bound:  3.33353400e-01\n",
      "Epoch: 4599 mean train loss:  2.17002973e-01, bound:  3.33339989e-01\n",
      "Epoch: 4600 mean train loss:  2.16885030e-01, bound:  3.33325267e-01\n",
      "Epoch: 4601 mean train loss:  2.16767997e-01, bound:  3.33310753e-01\n",
      "Epoch: 4602 mean train loss:  2.16651633e-01, bound:  3.33297104e-01\n",
      "Epoch: 4603 mean train loss:  2.16534808e-01, bound:  3.33281845e-01\n",
      "Epoch: 4604 mean train loss:  2.16416165e-01, bound:  3.33268344e-01\n",
      "Epoch: 4605 mean train loss:  2.16297969e-01, bound:  3.33253443e-01\n",
      "Epoch: 4606 mean train loss:  2.16180310e-01, bound:  3.33239198e-01\n",
      "Epoch: 4607 mean train loss:  2.16062993e-01, bound:  3.33225071e-01\n",
      "Epoch: 4608 mean train loss:  2.15945274e-01, bound:  3.33210170e-01\n",
      "Epoch: 4609 mean train loss:  2.15827182e-01, bound:  3.33196461e-01\n",
      "Epoch: 4610 mean train loss:  2.15708822e-01, bound:  3.33181441e-01\n",
      "Epoch: 4611 mean train loss:  2.15590298e-01, bound:  3.33167434e-01\n",
      "Epoch: 4612 mean train loss:  2.15471938e-01, bound:  3.33152950e-01\n",
      "Epoch: 4613 mean train loss:  2.15353131e-01, bound:  3.33138376e-01\n",
      "Epoch: 4614 mean train loss:  2.15234995e-01, bound:  3.33124369e-01\n",
      "Epoch: 4615 mean train loss:  2.15116397e-01, bound:  3.33109379e-01\n",
      "Epoch: 4616 mean train loss:  2.14997604e-01, bound:  3.33095461e-01\n",
      "Epoch: 4617 mean train loss:  2.14878336e-01, bound:  3.33080620e-01\n",
      "Epoch: 4618 mean train loss:  2.14759097e-01, bound:  3.33066434e-01\n",
      "Epoch: 4619 mean train loss:  2.14639783e-01, bound:  3.33051980e-01\n",
      "Epoch: 4620 mean train loss:  2.14520812e-01, bound:  3.33037347e-01\n",
      "Epoch: 4621 mean train loss:  2.14401498e-01, bound:  3.33023190e-01\n",
      "Epoch: 4622 mean train loss:  2.14282140e-01, bound:  3.33008349e-01\n",
      "Epoch: 4623 mean train loss:  2.14162752e-01, bound:  3.32994282e-01\n",
      "Epoch: 4624 mean train loss:  2.14042708e-01, bound:  3.32979470e-01\n",
      "Epoch: 4625 mean train loss:  2.13922977e-01, bound:  3.32965136e-01\n",
      "Epoch: 4626 mean train loss:  2.13802695e-01, bound:  3.32950592e-01\n",
      "Epoch: 4627 mean train loss:  2.13683113e-01, bound:  3.32936019e-01\n",
      "Epoch: 4628 mean train loss:  2.13562697e-01, bound:  3.32921684e-01\n",
      "Epoch: 4629 mean train loss:  2.13442802e-01, bound:  3.32906932e-01\n",
      "Epoch: 4630 mean train loss:  2.13322118e-01, bound:  3.32892627e-01\n",
      "Epoch: 4631 mean train loss:  2.13201836e-01, bound:  3.32877815e-01\n",
      "Epoch: 4632 mean train loss:  2.13081270e-01, bound:  3.32863480e-01\n",
      "Epoch: 4633 mean train loss:  2.12960631e-01, bound:  3.32848758e-01\n",
      "Epoch: 4634 mean train loss:  2.12839648e-01, bound:  3.32834363e-01\n",
      "Epoch: 4635 mean train loss:  2.12718859e-01, bound:  3.32819730e-01\n",
      "Epoch: 4636 mean train loss:  2.12597802e-01, bound:  3.32805157e-01\n",
      "Epoch: 4637 mean train loss:  2.12476730e-01, bound:  3.32790613e-01\n",
      "Epoch: 4638 mean train loss:  2.12355480e-01, bound:  3.32776010e-01\n",
      "Epoch: 4639 mean train loss:  2.12233886e-01, bound:  3.32761556e-01\n",
      "Epoch: 4640 mean train loss:  2.12112531e-01, bound:  3.32746804e-01\n",
      "Epoch: 4641 mean train loss:  2.11991310e-01, bound:  3.32732350e-01\n",
      "Epoch: 4642 mean train loss:  2.11869419e-01, bound:  3.32717597e-01\n",
      "Epoch: 4643 mean train loss:  2.11747304e-01, bound:  3.32703173e-01\n",
      "Epoch: 4644 mean train loss:  2.11625680e-01, bound:  3.32688421e-01\n",
      "Epoch: 4645 mean train loss:  2.11503714e-01, bound:  3.32673937e-01\n",
      "Epoch: 4646 mean train loss:  2.11381271e-01, bound:  3.32659155e-01\n",
      "Epoch: 4647 mean train loss:  2.11259663e-01, bound:  3.32644671e-01\n",
      "Epoch: 4648 mean train loss:  2.11137086e-01, bound:  3.32629919e-01\n",
      "Epoch: 4649 mean train loss:  2.11014479e-01, bound:  3.32615435e-01\n",
      "Epoch: 4650 mean train loss:  2.10891694e-01, bound:  3.32600623e-01\n",
      "Epoch: 4651 mean train loss:  2.10769221e-01, bound:  3.32586110e-01\n",
      "Epoch: 4652 mean train loss:  2.10646495e-01, bound:  3.32571268e-01\n",
      "Epoch: 4653 mean train loss:  2.10523427e-01, bound:  3.32556844e-01\n",
      "Epoch: 4654 mean train loss:  2.10400745e-01, bound:  3.32541943e-01\n",
      "Epoch: 4655 mean train loss:  2.10277855e-01, bound:  3.32527548e-01\n",
      "Epoch: 4656 mean train loss:  2.10154414e-01, bound:  3.32512528e-01\n",
      "Epoch: 4657 mean train loss:  2.10030973e-01, bound:  3.32498252e-01\n",
      "Epoch: 4658 mean train loss:  2.09907815e-01, bound:  3.32483083e-01\n",
      "Epoch: 4659 mean train loss:  2.09784880e-01, bound:  3.32468987e-01\n",
      "Epoch: 4660 mean train loss:  2.09661320e-01, bound:  3.32453519e-01\n",
      "Epoch: 4661 mean train loss:  2.09538624e-01, bound:  3.32439810e-01\n",
      "Epoch: 4662 mean train loss:  2.09416077e-01, bound:  3.32423836e-01\n",
      "Epoch: 4663 mean train loss:  2.09295347e-01, bound:  3.32410753e-01\n",
      "Epoch: 4664 mean train loss:  2.09176555e-01, bound:  3.32393885e-01\n",
      "Epoch: 4665 mean train loss:  2.09062845e-01, bound:  3.32382143e-01\n",
      "Epoch: 4666 mean train loss:  2.08958358e-01, bound:  3.32363397e-01\n",
      "Epoch: 4667 mean train loss:  2.08871961e-01, bound:  3.32354277e-01\n",
      "Epoch: 4668 mean train loss:  2.08819792e-01, bound:  3.32331866e-01\n",
      "Epoch: 4669 mean train loss:  2.08825842e-01, bound:  3.32327902e-01\n",
      "Epoch: 4670 mean train loss:  2.08916664e-01, bound:  3.32299262e-01\n",
      "Epoch: 4671 mean train loss:  2.09072903e-01, bound:  3.32303137e-01\n",
      "Epoch: 4672 mean train loss:  2.09149569e-01, bound:  3.32267821e-01\n",
      "Epoch: 4673 mean train loss:  2.08891794e-01, bound:  3.32276016e-01\n",
      "Epoch: 4674 mean train loss:  2.08301589e-01, bound:  3.32245648e-01\n",
      "Epoch: 4675 mean train loss:  2.07836777e-01, bound:  3.32241088e-01\n",
      "Epoch: 4676 mean train loss:  2.07829192e-01, bound:  3.32230747e-01\n",
      "Epoch: 4677 mean train loss:  2.08017021e-01, bound:  3.32205743e-01\n",
      "Epoch: 4678 mean train loss:  2.07930699e-01, bound:  3.32206994e-01\n",
      "Epoch: 4679 mean train loss:  2.07527101e-01, bound:  3.32180619e-01\n",
      "Epoch: 4680 mean train loss:  2.07237080e-01, bound:  3.32171828e-01\n",
      "Epoch: 4681 mean train loss:  2.07265496e-01, bound:  3.32162738e-01\n",
      "Epoch: 4682 mean train loss:  2.07305625e-01, bound:  3.32138419e-01\n",
      "Epoch: 4683 mean train loss:  2.07086578e-01, bound:  3.32135499e-01\n",
      "Epoch: 4684 mean train loss:  2.06791669e-01, bound:  3.32114697e-01\n",
      "Epoch: 4685 mean train loss:  2.06700355e-01, bound:  3.32100123e-01\n",
      "Epoch: 4686 mean train loss:  2.06714928e-01, bound:  3.32093686e-01\n",
      "Epoch: 4687 mean train loss:  2.06586331e-01, bound:  3.32070857e-01\n",
      "Epoch: 4688 mean train loss:  2.06344575e-01, bound:  3.32063556e-01\n",
      "Epoch: 4689 mean train loss:  2.06204236e-01, bound:  3.32048446e-01\n",
      "Epoch: 4690 mean train loss:  2.06173196e-01, bound:  3.32029909e-01\n",
      "Epoch: 4691 mean train loss:  2.06080213e-01, bound:  3.32023621e-01\n",
      "Epoch: 4692 mean train loss:  2.05887750e-01, bound:  3.32003891e-01\n",
      "Epoch: 4693 mean train loss:  2.05732316e-01, bound:  3.31992149e-01\n",
      "Epoch: 4694 mean train loss:  2.05663532e-01, bound:  3.31981093e-01\n",
      "Epoch: 4695 mean train loss:  2.05580339e-01, bound:  3.31961572e-01\n",
      "Epoch: 4696 mean train loss:  2.05424130e-01, bound:  3.31953317e-01\n",
      "Epoch: 4697 mean train loss:  2.05268487e-01, bound:  3.31937224e-01\n",
      "Epoch: 4698 mean train loss:  2.05172047e-01, bound:  3.31922293e-01\n",
      "Epoch: 4699 mean train loss:  2.05087110e-01, bound:  3.31912786e-01\n",
      "Epoch: 4700 mean train loss:  2.04956383e-01, bound:  3.31894219e-01\n",
      "Epoch: 4701 mean train loss:  2.04808295e-01, bound:  3.31883460e-01\n",
      "Epoch: 4702 mean train loss:  2.04692855e-01, bound:  3.31870019e-01\n",
      "Epoch: 4703 mean train loss:  2.04599634e-01, bound:  3.31853807e-01\n",
      "Epoch: 4704 mean train loss:  2.04485178e-01, bound:  3.31844062e-01\n",
      "Epoch: 4705 mean train loss:  2.04347625e-01, bound:  3.31827343e-01\n",
      "Epoch: 4706 mean train loss:  2.04221621e-01, bound:  3.31814468e-01\n",
      "Epoch: 4707 mean train loss:  2.04117149e-01, bound:  3.31802219e-01\n",
      "Epoch: 4708 mean train loss:  2.04009891e-01, bound:  3.31785709e-01\n",
      "Epoch: 4709 mean train loss:  2.03884080e-01, bound:  3.31774890e-01\n",
      "Epoch: 4710 mean train loss:  2.03755066e-01, bound:  3.31759512e-01\n",
      "Epoch: 4711 mean train loss:  2.03639463e-01, bound:  3.31745446e-01\n",
      "Epoch: 4712 mean train loss:  2.03531995e-01, bound:  3.31733465e-01\n",
      "Epoch: 4713 mean train loss:  2.03415662e-01, bound:  3.31717312e-01\n",
      "Epoch: 4714 mean train loss:  2.03290433e-01, bound:  3.31705689e-01\n",
      "Epoch: 4715 mean train loss:  2.03168035e-01, bound:  3.31691206e-01\n",
      "Epoch: 4716 mean train loss:  2.03054428e-01, bound:  3.31676781e-01\n",
      "Epoch: 4717 mean train loss:  2.02941820e-01, bound:  3.31664741e-01\n",
      "Epoch: 4718 mean train loss:  2.02822447e-01, bound:  3.31649065e-01\n",
      "Epoch: 4719 mean train loss:  2.02699557e-01, bound:  3.31636846e-01\n",
      "Epoch: 4720 mean train loss:  2.02579737e-01, bound:  3.31622809e-01\n",
      "Epoch: 4721 mean train loss:  2.02464998e-01, bound:  3.31608444e-01\n",
      "Epoch: 4722 mean train loss:  2.02349484e-01, bound:  3.31596106e-01\n",
      "Epoch: 4723 mean train loss:  2.02229828e-01, bound:  3.31580877e-01\n",
      "Epoch: 4724 mean train loss:  2.02108786e-01, bound:  3.31568241e-01\n",
      "Epoch: 4725 mean train loss:  2.01989725e-01, bound:  3.31554353e-01\n",
      "Epoch: 4726 mean train loss:  2.01872543e-01, bound:  3.31540078e-01\n",
      "Epoch: 4727 mean train loss:  2.01756224e-01, bound:  3.31527442e-01\n",
      "Epoch: 4728 mean train loss:  2.01636642e-01, bound:  3.31512481e-01\n",
      "Epoch: 4729 mean train loss:  2.01516286e-01, bound:  3.31499636e-01\n",
      "Epoch: 4730 mean train loss:  2.01396570e-01, bound:  3.31485569e-01\n",
      "Epoch: 4731 mean train loss:  2.01278955e-01, bound:  3.31471562e-01\n",
      "Epoch: 4732 mean train loss:  2.01160729e-01, bound:  3.31458628e-01\n",
      "Epoch: 4733 mean train loss:  2.01041684e-01, bound:  3.31443846e-01\n",
      "Epoch: 4734 mean train loss:  2.00921401e-01, bound:  3.31430912e-01\n",
      "Epoch: 4735 mean train loss:  2.00801671e-01, bound:  3.31416667e-01\n",
      "Epoch: 4736 mean train loss:  2.00682104e-01, bound:  3.31402928e-01\n",
      "Epoch: 4737 mean train loss:  2.00563490e-01, bound:  3.31389546e-01\n",
      "Epoch: 4738 mean train loss:  2.00444445e-01, bound:  3.31375092e-01\n",
      "Epoch: 4739 mean train loss:  2.00324520e-01, bound:  3.31361979e-01\n",
      "Epoch: 4740 mean train loss:  2.00204253e-01, bound:  3.31347674e-01\n",
      "Epoch: 4741 mean train loss:  2.00084016e-01, bound:  3.31334203e-01\n",
      "Epoch: 4742 mean train loss:  1.99963972e-01, bound:  3.31320465e-01\n",
      "Epoch: 4743 mean train loss:  1.99844584e-01, bound:  3.31306398e-01\n",
      "Epoch: 4744 mean train loss:  1.99724793e-01, bound:  3.31293106e-01\n",
      "Epoch: 4745 mean train loss:  1.99604541e-01, bound:  3.31278801e-01\n",
      "Epoch: 4746 mean train loss:  1.99483946e-01, bound:  3.31265479e-01\n",
      "Epoch: 4747 mean train loss:  1.99363425e-01, bound:  3.31251472e-01\n",
      "Epoch: 4748 mean train loss:  1.99242666e-01, bound:  3.31237823e-01\n",
      "Epoch: 4749 mean train loss:  1.99122414e-01, bound:  3.31224144e-01\n",
      "Epoch: 4750 mean train loss:  1.99002013e-01, bound:  3.31210077e-01\n",
      "Epoch: 4751 mean train loss:  1.98881254e-01, bound:  3.31196755e-01\n",
      "Epoch: 4752 mean train loss:  1.98760390e-01, bound:  3.31182539e-01\n",
      "Epoch: 4753 mean train loss:  1.98639244e-01, bound:  3.31169099e-01\n",
      "Epoch: 4754 mean train loss:  1.98518246e-01, bound:  3.31155151e-01\n",
      "Epoch: 4755 mean train loss:  1.98396921e-01, bound:  3.31141442e-01\n",
      "Epoch: 4756 mean train loss:  1.98275864e-01, bound:  3.31127763e-01\n",
      "Epoch: 4757 mean train loss:  1.98154792e-01, bound:  3.31113786e-01\n",
      "Epoch: 4758 mean train loss:  1.98033303e-01, bound:  3.31100315e-01\n",
      "Epoch: 4759 mean train loss:  1.97911859e-01, bound:  3.31086218e-01\n",
      "Epoch: 4760 mean train loss:  1.97790116e-01, bound:  3.31072688e-01\n",
      "Epoch: 4761 mean train loss:  1.97668374e-01, bound:  3.31058711e-01\n",
      "Epoch: 4762 mean train loss:  1.97546527e-01, bound:  3.31045091e-01\n",
      "Epoch: 4763 mean train loss:  1.97424814e-01, bound:  3.31031263e-01\n",
      "Epoch: 4764 mean train loss:  1.97302639e-01, bound:  3.31017435e-01\n",
      "Epoch: 4765 mean train loss:  1.97180554e-01, bound:  3.31003726e-01\n",
      "Epoch: 4766 mean train loss:  1.97058454e-01, bound:  3.30989838e-01\n",
      "Epoch: 4767 mean train loss:  1.96936607e-01, bound:  3.30976218e-01\n",
      "Epoch: 4768 mean train loss:  1.96813941e-01, bound:  3.30962300e-01\n",
      "Epoch: 4769 mean train loss:  1.96691632e-01, bound:  3.30948710e-01\n",
      "Epoch: 4770 mean train loss:  1.96569085e-01, bound:  3.30934733e-01\n",
      "Epoch: 4771 mean train loss:  1.96446478e-01, bound:  3.30921143e-01\n",
      "Epoch: 4772 mean train loss:  1.96323797e-01, bound:  3.30907226e-01\n",
      "Epoch: 4773 mean train loss:  1.96201131e-01, bound:  3.30893546e-01\n",
      "Epoch: 4774 mean train loss:  1.96077853e-01, bound:  3.30879688e-01\n",
      "Epoch: 4775 mean train loss:  1.95955262e-01, bound:  3.30866009e-01\n",
      "Epoch: 4776 mean train loss:  1.95832059e-01, bound:  3.30852181e-01\n",
      "Epoch: 4777 mean train loss:  1.95708632e-01, bound:  3.30838442e-01\n",
      "Epoch: 4778 mean train loss:  1.95585519e-01, bound:  3.30824703e-01\n",
      "Epoch: 4779 mean train loss:  1.95462018e-01, bound:  3.30810905e-01\n",
      "Epoch: 4780 mean train loss:  1.95338503e-01, bound:  3.30797225e-01\n",
      "Epoch: 4781 mean train loss:  1.95214853e-01, bound:  3.30783397e-01\n",
      "Epoch: 4782 mean train loss:  1.95091590e-01, bound:  3.30769688e-01\n",
      "Epoch: 4783 mean train loss:  1.94967538e-01, bound:  3.30755860e-01\n",
      "Epoch: 4784 mean train loss:  1.94843903e-01, bound:  3.30742210e-01\n",
      "Epoch: 4785 mean train loss:  1.94720134e-01, bound:  3.30728292e-01\n",
      "Epoch: 4786 mean train loss:  1.94596216e-01, bound:  3.30714703e-01\n",
      "Epoch: 4787 mean train loss:  1.94472268e-01, bound:  3.30700845e-01\n",
      "Epoch: 4788 mean train loss:  1.94347635e-01, bound:  3.30687225e-01\n",
      "Epoch: 4789 mean train loss:  1.94223642e-01, bound:  3.30673277e-01\n",
      "Epoch: 4790 mean train loss:  1.94098994e-01, bound:  3.30659807e-01\n",
      "Epoch: 4791 mean train loss:  1.93974823e-01, bound:  3.30645710e-01\n",
      "Epoch: 4792 mean train loss:  1.93850607e-01, bound:  3.30632418e-01\n",
      "Epoch: 4793 mean train loss:  1.93726435e-01, bound:  3.30618173e-01\n",
      "Epoch: 4794 mean train loss:  1.93602160e-01, bound:  3.30605090e-01\n",
      "Epoch: 4795 mean train loss:  1.93478271e-01, bound:  3.30590457e-01\n",
      "Epoch: 4796 mean train loss:  1.93355173e-01, bound:  3.30577910e-01\n",
      "Epoch: 4797 mean train loss:  1.93233639e-01, bound:  3.30562651e-01\n",
      "Epoch: 4798 mean train loss:  1.93115368e-01, bound:  3.30551058e-01\n",
      "Epoch: 4799 mean train loss:  1.93003401e-01, bound:  3.30534399e-01\n",
      "Epoch: 4800 mean train loss:  1.92903101e-01, bound:  3.30524743e-01\n",
      "Epoch: 4801 mean train loss:  1.92827061e-01, bound:  3.30505490e-01\n",
      "Epoch: 4802 mean train loss:  1.92797214e-01, bound:  3.30499589e-01\n",
      "Epoch: 4803 mean train loss:  1.92848578e-01, bound:  3.30475271e-01\n",
      "Epoch: 4804 mean train loss:  1.93015680e-01, bound:  3.30476344e-01\n",
      "Epoch: 4805 mean train loss:  1.93269536e-01, bound:  3.30444515e-01\n",
      "Epoch: 4806 mean train loss:  1.93380937e-01, bound:  3.30453336e-01\n",
      "Epoch: 4807 mean train loss:  1.93008199e-01, bound:  3.30419481e-01\n",
      "Epoch: 4808 mean train loss:  1.92235038e-01, bound:  3.30423683e-01\n",
      "Epoch: 4809 mean train loss:  1.91755369e-01, bound:  3.30405712e-01\n",
      "Epoch: 4810 mean train loss:  1.91895112e-01, bound:  3.30389023e-01\n",
      "Epoch: 4811 mean train loss:  1.92148194e-01, bound:  3.30389559e-01\n",
      "Epoch: 4812 mean train loss:  1.91927776e-01, bound:  3.30361784e-01\n",
      "Epoch: 4813 mean train loss:  1.91401288e-01, bound:  3.30359638e-01\n",
      "Epoch: 4814 mean train loss:  1.91202432e-01, bound:  3.30345809e-01\n",
      "Epoch: 4815 mean train loss:  1.91356212e-01, bound:  3.30326468e-01\n",
      "Epoch: 4816 mean train loss:  1.91327423e-01, bound:  3.30325246e-01\n",
      "Epoch: 4817 mean train loss:  1.90968648e-01, bound:  3.30302298e-01\n",
      "Epoch: 4818 mean train loss:  1.90713212e-01, bound:  3.30292732e-01\n",
      "Epoch: 4819 mean train loss:  1.90750659e-01, bound:  3.30284536e-01\n",
      "Epoch: 4820 mean train loss:  1.90739855e-01, bound:  3.30263436e-01\n",
      "Epoch: 4821 mean train loss:  1.90485686e-01, bound:  3.30259234e-01\n",
      "Epoch: 4822 mean train loss:  1.90253600e-01, bound:  3.30242902e-01\n",
      "Epoch: 4823 mean train loss:  1.90224200e-01, bound:  3.30227226e-01\n",
      "Epoch: 4824 mean train loss:  1.90193191e-01, bound:  3.30221415e-01\n",
      "Epoch: 4825 mean train loss:  1.89998224e-01, bound:  3.30202103e-01\n",
      "Epoch: 4826 mean train loss:  1.89796180e-01, bound:  3.30192983e-01\n",
      "Epoch: 4827 mean train loss:  1.89729467e-01, bound:  3.30182195e-01\n",
      "Epoch: 4828 mean train loss:  1.89676881e-01, bound:  3.30164105e-01\n",
      "Epoch: 4829 mean train loss:  1.89516991e-01, bound:  3.30157220e-01\n",
      "Epoch: 4830 mean train loss:  1.89338952e-01, bound:  3.30141872e-01\n",
      "Epoch: 4831 mean train loss:  1.89248681e-01, bound:  3.30128491e-01\n",
      "Epoch: 4832 mean train loss:  1.89179778e-01, bound:  3.30120325e-01\n",
      "Epoch: 4833 mean train loss:  1.89042449e-01, bound:  3.30103010e-01\n",
      "Epoch: 4834 mean train loss:  1.88882500e-01, bound:  3.30093235e-01\n",
      "Epoch: 4835 mean train loss:  1.88776314e-01, bound:  3.30081314e-01\n",
      "Epoch: 4836 mean train loss:  1.88694388e-01, bound:  3.30065995e-01\n",
      "Epoch: 4837 mean train loss:  1.88572273e-01, bound:  3.30057681e-01\n",
      "Epoch: 4838 mean train loss:  1.88425526e-01, bound:  3.30042720e-01\n",
      "Epoch: 4839 mean train loss:  1.88308656e-01, bound:  3.30030382e-01\n",
      "Epoch: 4840 mean train loss:  1.88216150e-01, bound:  3.30020159e-01\n",
      "Epoch: 4841 mean train loss:  1.88103706e-01, bound:  3.30004632e-01\n",
      "Epoch: 4842 mean train loss:  1.87968358e-01, bound:  3.29994887e-01\n",
      "Epoch: 4843 mean train loss:  1.87845260e-01, bound:  3.29981804e-01\n",
      "Epoch: 4844 mean train loss:  1.87742740e-01, bound:  3.29968035e-01\n",
      "Epoch: 4845 mean train loss:  1.87635213e-01, bound:  3.29958022e-01\n",
      "Epoch: 4846 mean train loss:  1.87509730e-01, bound:  3.29943269e-01\n",
      "Epoch: 4847 mean train loss:  1.87384278e-01, bound:  3.29932094e-01\n",
      "Epoch: 4848 mean train loss:  1.87273204e-01, bound:  3.29920262e-01\n",
      "Epoch: 4849 mean train loss:  1.87166259e-01, bound:  3.29906166e-01\n",
      "Epoch: 4850 mean train loss:  1.87048361e-01, bound:  3.29895765e-01\n",
      "Epoch: 4851 mean train loss:  1.86924830e-01, bound:  3.29881907e-01\n",
      "Epoch: 4852 mean train loss:  1.86807707e-01, bound:  3.29869837e-01\n",
      "Epoch: 4853 mean train loss:  1.86697170e-01, bound:  3.29858512e-01\n",
      "Epoch: 4854 mean train loss:  1.86584324e-01, bound:  3.29844594e-01\n",
      "Epoch: 4855 mean train loss:  1.86465010e-01, bound:  3.29833716e-01\n",
      "Epoch: 4856 mean train loss:  1.86344653e-01, bound:  3.29820603e-01\n",
      "Epoch: 4857 mean train loss:  1.86230108e-01, bound:  3.29808056e-01\n",
      "Epoch: 4858 mean train loss:  1.86117724e-01, bound:  3.29796791e-01\n",
      "Epoch: 4859 mean train loss:  1.86001778e-01, bound:  3.29783231e-01\n",
      "Epoch: 4860 mean train loss:  1.85882866e-01, bound:  3.29771966e-01\n",
      "Epoch: 4861 mean train loss:  1.85765177e-01, bound:  3.29759181e-01\n",
      "Epoch: 4862 mean train loss:  1.85650468e-01, bound:  3.29746574e-01\n",
      "Epoch: 4863 mean train loss:  1.85536072e-01, bound:  3.29735160e-01\n",
      "Epoch: 4864 mean train loss:  1.85419828e-01, bound:  3.29721808e-01\n",
      "Epoch: 4865 mean train loss:  1.85301825e-01, bound:  3.29710335e-01\n",
      "Epoch: 4866 mean train loss:  1.85184762e-01, bound:  3.29697669e-01\n",
      "Epoch: 4867 mean train loss:  1.85069382e-01, bound:  3.29685092e-01\n",
      "Epoch: 4868 mean train loss:  1.84953496e-01, bound:  3.29673499e-01\n",
      "Epoch: 4869 mean train loss:  1.84836850e-01, bound:  3.29660416e-01\n",
      "Epoch: 4870 mean train loss:  1.84719816e-01, bound:  3.29648763e-01\n",
      "Epoch: 4871 mean train loss:  1.84602931e-01, bound:  3.29636127e-01\n",
      "Epoch: 4872 mean train loss:  1.84486300e-01, bound:  3.29623759e-01\n",
      "Epoch: 4873 mean train loss:  1.84370264e-01, bound:  3.29611897e-01\n",
      "Epoch: 4874 mean train loss:  1.84254006e-01, bound:  3.29599023e-01\n",
      "Epoch: 4875 mean train loss:  1.84136912e-01, bound:  3.29587281e-01\n",
      "Epoch: 4876 mean train loss:  1.84019580e-01, bound:  3.29574645e-01\n",
      "Epoch: 4877 mean train loss:  1.83902949e-01, bound:  3.29562455e-01\n",
      "Epoch: 4878 mean train loss:  1.83786288e-01, bound:  3.29550445e-01\n",
      "Epoch: 4879 mean train loss:  1.83669761e-01, bound:  3.29537809e-01\n",
      "Epoch: 4880 mean train loss:  1.83552817e-01, bound:  3.29526037e-01\n",
      "Epoch: 4881 mean train loss:  1.83435977e-01, bound:  3.29513401e-01\n",
      "Epoch: 4882 mean train loss:  1.83318511e-01, bound:  3.29501420e-01\n",
      "Epoch: 4883 mean train loss:  1.83201402e-01, bound:  3.29489201e-01\n",
      "Epoch: 4884 mean train loss:  1.83084577e-01, bound:  3.29476833e-01\n",
      "Epoch: 4885 mean train loss:  1.82967439e-01, bound:  3.29464942e-01\n",
      "Epoch: 4886 mean train loss:  1.82850525e-01, bound:  3.29452366e-01\n",
      "Epoch: 4887 mean train loss:  1.82733148e-01, bound:  3.29440475e-01\n",
      "Epoch: 4888 mean train loss:  1.82615653e-01, bound:  3.29428107e-01\n",
      "Epoch: 4889 mean train loss:  1.82498217e-01, bound:  3.29416007e-01\n",
      "Epoch: 4890 mean train loss:  1.82381362e-01, bound:  3.29403907e-01\n",
      "Epoch: 4891 mean train loss:  1.82264149e-01, bound:  3.29391569e-01\n",
      "Epoch: 4892 mean train loss:  1.82146519e-01, bound:  3.29379648e-01\n",
      "Epoch: 4893 mean train loss:  1.82029322e-01, bound:  3.29367250e-01\n",
      "Epoch: 4894 mean train loss:  1.81911618e-01, bound:  3.29355329e-01\n",
      "Epoch: 4895 mean train loss:  1.81794211e-01, bound:  3.29343051e-01\n",
      "Epoch: 4896 mean train loss:  1.81676507e-01, bound:  3.29330951e-01\n",
      "Epoch: 4897 mean train loss:  1.81558847e-01, bound:  3.29318881e-01\n",
      "Epoch: 4898 mean train loss:  1.81441501e-01, bound:  3.29306573e-01\n",
      "Epoch: 4899 mean train loss:  1.81323752e-01, bound:  3.29294652e-01\n",
      "Epoch: 4900 mean train loss:  1.81206390e-01, bound:  3.29282343e-01\n",
      "Epoch: 4901 mean train loss:  1.81088433e-01, bound:  3.29270452e-01\n",
      "Epoch: 4902 mean train loss:  1.80970192e-01, bound:  3.29258204e-01\n",
      "Epoch: 4903 mean train loss:  1.80852920e-01, bound:  3.29246193e-01\n",
      "Epoch: 4904 mean train loss:  1.80734903e-01, bound:  3.29234034e-01\n",
      "Epoch: 4905 mean train loss:  1.80617362e-01, bound:  3.29221964e-01\n",
      "Epoch: 4906 mean train loss:  1.80499077e-01, bound:  3.29209983e-01\n",
      "Epoch: 4907 mean train loss:  1.80381462e-01, bound:  3.29197824e-01\n",
      "Epoch: 4908 mean train loss:  1.80263340e-01, bound:  3.29185873e-01\n",
      "Epoch: 4909 mean train loss:  1.80145636e-01, bound:  3.29173654e-01\n",
      "Epoch: 4910 mean train loss:  1.80028066e-01, bound:  3.29161823e-01\n",
      "Epoch: 4911 mean train loss:  1.79909647e-01, bound:  3.29149693e-01\n",
      "Epoch: 4912 mean train loss:  1.79791629e-01, bound:  3.29137743e-01\n",
      "Epoch: 4913 mean train loss:  1.79673791e-01, bound:  3.29125643e-01\n",
      "Epoch: 4914 mean train loss:  1.79555759e-01, bound:  3.29113662e-01\n",
      "Epoch: 4915 mean train loss:  1.79437339e-01, bound:  3.29101682e-01\n",
      "Epoch: 4916 mean train loss:  1.79319218e-01, bound:  3.29089642e-01\n",
      "Epoch: 4917 mean train loss:  1.79201305e-01, bound:  3.29077721e-01\n",
      "Epoch: 4918 mean train loss:  1.79083273e-01, bound:  3.29065681e-01\n",
      "Epoch: 4919 mean train loss:  1.78964943e-01, bound:  3.29053760e-01\n",
      "Epoch: 4920 mean train loss:  1.78846881e-01, bound:  3.29041749e-01\n",
      "Epoch: 4921 mean train loss:  1.78728685e-01, bound:  3.29029918e-01\n",
      "Epoch: 4922 mean train loss:  1.78610429e-01, bound:  3.29017848e-01\n",
      "Epoch: 4923 mean train loss:  1.78491980e-01, bound:  3.29006016e-01\n",
      "Epoch: 4924 mean train loss:  1.78373888e-01, bound:  3.28993946e-01\n",
      "Epoch: 4925 mean train loss:  1.78255633e-01, bound:  3.28982204e-01\n",
      "Epoch: 4926 mean train loss:  1.78137377e-01, bound:  3.28970134e-01\n",
      "Epoch: 4927 mean train loss:  1.78018808e-01, bound:  3.28958362e-01\n",
      "Epoch: 4928 mean train loss:  1.77900776e-01, bound:  3.28946263e-01\n",
      "Epoch: 4929 mean train loss:  1.77782521e-01, bound:  3.28934610e-01\n",
      "Epoch: 4930 mean train loss:  1.77664071e-01, bound:  3.28922451e-01\n",
      "Epoch: 4931 mean train loss:  1.77545741e-01, bound:  3.28910887e-01\n",
      "Epoch: 4932 mean train loss:  1.77427575e-01, bound:  3.28898698e-01\n",
      "Epoch: 4933 mean train loss:  1.77309185e-01, bound:  3.28887254e-01\n",
      "Epoch: 4934 mean train loss:  1.77190989e-01, bound:  3.28874916e-01\n",
      "Epoch: 4935 mean train loss:  1.77073047e-01, bound:  3.28863710e-01\n",
      "Epoch: 4936 mean train loss:  1.76955745e-01, bound:  3.28851074e-01\n",
      "Epoch: 4937 mean train loss:  1.76838890e-01, bound:  3.28840286e-01\n",
      "Epoch: 4938 mean train loss:  1.76722944e-01, bound:  3.28827173e-01\n",
      "Epoch: 4939 mean train loss:  1.76609397e-01, bound:  3.28817070e-01\n",
      "Epoch: 4940 mean train loss:  1.76500559e-01, bound:  3.28803003e-01\n",
      "Epoch: 4941 mean train loss:  1.76398873e-01, bound:  3.28794271e-01\n",
      "Epoch: 4942 mean train loss:  1.76312163e-01, bound:  3.28778446e-01\n",
      "Epoch: 4943 mean train loss:  1.76252455e-01, bound:  3.28772217e-01\n",
      "Epoch: 4944 mean train loss:  1.76242292e-01, bound:  3.28753114e-01\n",
      "Epoch: 4945 mean train loss:  1.76312581e-01, bound:  3.28751475e-01\n",
      "Epoch: 4946 mean train loss:  1.76489770e-01, bound:  3.28726798e-01\n",
      "Epoch: 4947 mean train loss:  1.76728129e-01, bound:  3.28731984e-01\n",
      "Epoch: 4948 mean train loss:  1.76825255e-01, bound:  3.28702182e-01\n",
      "Epoch: 4949 mean train loss:  1.76475450e-01, bound:  3.28709990e-01\n",
      "Epoch: 4950 mean train loss:  1.75749376e-01, bound:  3.28685373e-01\n",
      "Epoch: 4951 mean train loss:  1.75223112e-01, bound:  3.28681886e-01\n",
      "Epoch: 4952 mean train loss:  1.75262183e-01, bound:  3.28674108e-01\n",
      "Epoch: 4953 mean train loss:  1.75536975e-01, bound:  3.28653812e-01\n",
      "Epoch: 4954 mean train loss:  1.75483271e-01, bound:  3.28655869e-01\n",
      "Epoch: 4955 mean train loss:  1.75026357e-01, bound:  3.28634292e-01\n",
      "Epoch: 4956 mean train loss:  1.74662888e-01, bound:  3.28628451e-01\n",
      "Epoch: 4957 mean train loss:  1.74694568e-01, bound:  3.28620672e-01\n",
      "Epoch: 4958 mean train loss:  1.74811542e-01, bound:  3.28601509e-01\n",
      "Epoch: 4959 mean train loss:  1.74634904e-01, bound:  3.28600019e-01\n",
      "Epoch: 4960 mean train loss:  1.74287647e-01, bound:  3.28582376e-01\n",
      "Epoch: 4961 mean train loss:  1.74136370e-01, bound:  3.28572690e-01\n",
      "Epoch: 4962 mean train loss:  1.74187377e-01, bound:  3.28566879e-01\n",
      "Epoch: 4963 mean train loss:  1.74137101e-01, bound:  3.28548640e-01\n",
      "Epoch: 4964 mean train loss:  1.73894867e-01, bound:  3.28544229e-01\n",
      "Epoch: 4965 mean train loss:  1.73687518e-01, bound:  3.28530431e-01\n",
      "Epoch: 4966 mean train loss:  1.73648641e-01, bound:  3.28517675e-01\n",
      "Epoch: 4967 mean train loss:  1.73624605e-01, bound:  3.28512520e-01\n",
      "Epoch: 4968 mean train loss:  1.73469380e-01, bound:  3.28495979e-01\n",
      "Epoch: 4969 mean train loss:  1.73270330e-01, bound:  3.28488737e-01\n",
      "Epoch: 4970 mean train loss:  1.73168838e-01, bound:  3.28478098e-01\n",
      "Epoch: 4971 mean train loss:  1.73128694e-01, bound:  3.28463942e-01\n",
      "Epoch: 4972 mean train loss:  1.73025146e-01, bound:  3.28458339e-01\n",
      "Epoch: 4973 mean train loss:  1.72857523e-01, bound:  3.28444004e-01\n",
      "Epoch: 4974 mean train loss:  1.72723845e-01, bound:  3.28434497e-01\n",
      "Epoch: 4975 mean train loss:  1.72652990e-01, bound:  3.28425646e-01\n",
      "Epoch: 4976 mean train loss:  1.72572583e-01, bound:  3.28411460e-01\n",
      "Epoch: 4977 mean train loss:  1.72439113e-01, bound:  3.28404635e-01\n",
      "Epoch: 4978 mean train loss:  1.72298551e-01, bound:  3.28392327e-01\n",
      "Epoch: 4979 mean train loss:  1.72198787e-01, bound:  3.28381509e-01\n",
      "Epoch: 4980 mean train loss:  1.72118530e-01, bound:  3.28373283e-01\n",
      "Epoch: 4981 mean train loss:  1.72011733e-01, bound:  3.28359723e-01\n",
      "Epoch: 4982 mean train loss:  1.71881005e-01, bound:  3.28351647e-01\n",
      "Epoch: 4983 mean train loss:  1.71762601e-01, bound:  3.28340501e-01\n",
      "Epoch: 4984 mean train loss:  1.71669438e-01, bound:  3.28329086e-01\n",
      "Epoch: 4985 mean train loss:  1.71575189e-01, bound:  3.28320771e-01\n",
      "Epoch: 4986 mean train loss:  1.71461046e-01, bound:  3.28307837e-01\n",
      "Epoch: 4987 mean train loss:  1.71339720e-01, bound:  3.28298956e-01\n",
      "Epoch: 4988 mean train loss:  1.71231806e-01, bound:  3.28288376e-01\n",
      "Epoch: 4989 mean train loss:  1.71135142e-01, bound:  3.28276843e-01\n",
      "Epoch: 4990 mean train loss:  1.71033666e-01, bound:  3.28268290e-01\n",
      "Epoch: 4991 mean train loss:  1.70920417e-01, bound:  3.28256041e-01\n",
      "Epoch: 4992 mean train loss:  1.70805961e-01, bound:  3.28246593e-01\n",
      "Epoch: 4993 mean train loss:  1.70700297e-01, bound:  3.28236282e-01\n",
      "Epoch: 4994 mean train loss:  1.70600235e-01, bound:  3.28224868e-01\n",
      "Epoch: 4995 mean train loss:  1.70495942e-01, bound:  3.28216016e-01\n",
      "Epoch: 4996 mean train loss:  1.70385271e-01, bound:  3.28204244e-01\n",
      "Epoch: 4997 mean train loss:  1.70274168e-01, bound:  3.28194588e-01\n",
      "Epoch: 4998 mean train loss:  1.70168489e-01, bound:  3.28184277e-01\n",
      "Epoch: 4999 mean train loss:  1.70066118e-01, bound:  3.28173190e-01\n",
      "Epoch: 5000 mean train loss:  1.69961542e-01, bound:  3.28164071e-01\n",
      "Epoch: 5001 mean train loss:  1.69853404e-01, bound:  3.28152597e-01\n",
      "Epoch: 5002 mean train loss:  1.69743896e-01, bound:  3.28142941e-01\n",
      "Epoch: 5003 mean train loss:  1.69637904e-01, bound:  3.28132540e-01\n",
      "Epoch: 5004 mean train loss:  1.69533506e-01, bound:  3.28121781e-01\n",
      "Epoch: 5005 mean train loss:  1.69429466e-01, bound:  3.28112364e-01\n",
      "Epoch: 5006 mean train loss:  1.69322506e-01, bound:  3.28101158e-01\n",
      "Epoch: 5007 mean train loss:  1.69215083e-01, bound:  3.28091532e-01\n",
      "Epoch: 5008 mean train loss:  1.69108182e-01, bound:  3.28080952e-01\n",
      "Epoch: 5009 mean train loss:  1.69002891e-01, bound:  3.28070492e-01\n",
      "Epoch: 5010 mean train loss:  1.68898374e-01, bound:  3.28060836e-01\n",
      "Epoch: 5011 mean train loss:  1.68792903e-01, bound:  3.28049868e-01\n",
      "Epoch: 5012 mean train loss:  1.68686301e-01, bound:  3.28040302e-01\n",
      "Epoch: 5013 mean train loss:  1.68579996e-01, bound:  3.28029603e-01\n",
      "Epoch: 5014 mean train loss:  1.68474033e-01, bound:  3.28019530e-01\n",
      "Epoch: 5015 mean train loss:  1.68368965e-01, bound:  3.28009546e-01\n",
      "Epoch: 5016 mean train loss:  1.68263927e-01, bound:  3.27998936e-01\n",
      "Epoch: 5017 mean train loss:  1.68158606e-01, bound:  3.27989250e-01\n",
      "Epoch: 5018 mean train loss:  1.68052867e-01, bound:  3.27978522e-01\n",
      "Epoch: 5019 mean train loss:  1.67946711e-01, bound:  3.27968687e-01\n",
      "Epoch: 5020 mean train loss:  1.67841256e-01, bound:  3.27958435e-01\n",
      "Epoch: 5021 mean train loss:  1.67735949e-01, bound:  3.27948153e-01\n",
      "Epoch: 5022 mean train loss:  1.67630970e-01, bound:  3.27938288e-01\n",
      "Epoch: 5023 mean train loss:  1.67526022e-01, bound:  3.27927798e-01\n",
      "Epoch: 5024 mean train loss:  1.67420760e-01, bound:  3.27918082e-01\n",
      "Epoch: 5025 mean train loss:  1.67315260e-01, bound:  3.27907652e-01\n",
      "Epoch: 5026 mean train loss:  1.67209789e-01, bound:  3.27897787e-01\n",
      "Epoch: 5027 mean train loss:  1.67104736e-01, bound:  3.27887654e-01\n",
      "Epoch: 5028 mean train loss:  1.66999713e-01, bound:  3.27877492e-01\n",
      "Epoch: 5029 mean train loss:  1.66895062e-01, bound:  3.27867627e-01\n",
      "Epoch: 5030 mean train loss:  1.66789845e-01, bound:  3.27857316e-01\n",
      "Epoch: 5031 mean train loss:  1.66685179e-01, bound:  3.27847570e-01\n",
      "Epoch: 5032 mean train loss:  1.66580305e-01, bound:  3.27837318e-01\n",
      "Epoch: 5033 mean train loss:  1.66475251e-01, bound:  3.27827454e-01\n",
      "Epoch: 5034 mean train loss:  1.66370332e-01, bound:  3.27817351e-01\n",
      "Epoch: 5035 mean train loss:  1.66265503e-01, bound:  3.27807337e-01\n",
      "Epoch: 5036 mean train loss:  1.66160882e-01, bound:  3.27797443e-01\n",
      "Epoch: 5037 mean train loss:  1.66056603e-01, bound:  3.27787280e-01\n",
      "Epoch: 5038 mean train loss:  1.65951625e-01, bound:  3.27777505e-01\n",
      "Epoch: 5039 mean train loss:  1.65847376e-01, bound:  3.27767372e-01\n",
      "Epoch: 5040 mean train loss:  1.65742815e-01, bound:  3.27757597e-01\n",
      "Epoch: 5041 mean train loss:  1.65638462e-01, bound:  3.27747494e-01\n",
      "Epoch: 5042 mean train loss:  1.65534139e-01, bound:  3.27737689e-01\n",
      "Epoch: 5043 mean train loss:  1.65429428e-01, bound:  3.27727705e-01\n",
      "Epoch: 5044 mean train loss:  1.65324882e-01, bound:  3.27717811e-01\n",
      "Epoch: 5045 mean train loss:  1.65221035e-01, bound:  3.27707946e-01\n",
      "Epoch: 5046 mean train loss:  1.65116757e-01, bound:  3.27697992e-01\n",
      "Epoch: 5047 mean train loss:  1.65013060e-01, bound:  3.27688217e-01\n",
      "Epoch: 5048 mean train loss:  1.64908871e-01, bound:  3.27678233e-01\n",
      "Epoch: 5049 mean train loss:  1.64804399e-01, bound:  3.27668488e-01\n",
      "Epoch: 5050 mean train loss:  1.64700732e-01, bound:  3.27658534e-01\n",
      "Epoch: 5051 mean train loss:  1.64596930e-01, bound:  3.27648848e-01\n",
      "Epoch: 5052 mean train loss:  1.64493084e-01, bound:  3.27638894e-01\n",
      "Epoch: 5053 mean train loss:  1.64389461e-01, bound:  3.27629179e-01\n",
      "Epoch: 5054 mean train loss:  1.64285243e-01, bound:  3.27619314e-01\n",
      "Epoch: 5055 mean train loss:  1.64181873e-01, bound:  3.27609569e-01\n",
      "Epoch: 5056 mean train loss:  1.64078325e-01, bound:  3.27599764e-01\n",
      "Epoch: 5057 mean train loss:  1.63974687e-01, bound:  3.27590019e-01\n",
      "Epoch: 5058 mean train loss:  1.63871005e-01, bound:  3.27580273e-01\n",
      "Epoch: 5059 mean train loss:  1.63767576e-01, bound:  3.27570528e-01\n",
      "Epoch: 5060 mean train loss:  1.63664147e-01, bound:  3.27560782e-01\n",
      "Epoch: 5061 mean train loss:  1.63560808e-01, bound:  3.27551037e-01\n",
      "Epoch: 5062 mean train loss:  1.63457572e-01, bound:  3.27541381e-01\n",
      "Epoch: 5063 mean train loss:  1.63354352e-01, bound:  3.27531636e-01\n",
      "Epoch: 5064 mean train loss:  1.63251400e-01, bound:  3.27522010e-01\n",
      "Epoch: 5065 mean train loss:  1.63148373e-01, bound:  3.27512234e-01\n",
      "Epoch: 5066 mean train loss:  1.63044885e-01, bound:  3.27502638e-01\n",
      "Epoch: 5067 mean train loss:  1.62942126e-01, bound:  3.27492923e-01\n",
      "Epoch: 5068 mean train loss:  1.62838906e-01, bound:  3.27483356e-01\n",
      "Epoch: 5069 mean train loss:  1.62736133e-01, bound:  3.27473611e-01\n",
      "Epoch: 5070 mean train loss:  1.62633389e-01, bound:  3.27464163e-01\n",
      "Epoch: 5071 mean train loss:  1.62530705e-01, bound:  3.27454299e-01\n",
      "Epoch: 5072 mean train loss:  1.62428617e-01, bound:  3.27444971e-01\n",
      "Epoch: 5073 mean train loss:  1.62325993e-01, bound:  3.27435017e-01\n",
      "Epoch: 5074 mean train loss:  1.62224233e-01, bound:  3.27425957e-01\n",
      "Epoch: 5075 mean train loss:  1.62122294e-01, bound:  3.27415735e-01\n",
      "Epoch: 5076 mean train loss:  1.62021026e-01, bound:  3.27406973e-01\n",
      "Epoch: 5077 mean train loss:  1.61921456e-01, bound:  3.27396333e-01\n",
      "Epoch: 5078 mean train loss:  1.61823958e-01, bound:  3.27388197e-01\n",
      "Epoch: 5079 mean train loss:  1.61730692e-01, bound:  3.27376842e-01\n",
      "Epoch: 5080 mean train loss:  1.61645293e-01, bound:  3.27369869e-01\n",
      "Epoch: 5081 mean train loss:  1.61574259e-01, bound:  3.27356935e-01\n",
      "Epoch: 5082 mean train loss:  1.61532521e-01, bound:  3.27352196e-01\n",
      "Epoch: 5083 mean train loss:  1.61542460e-01, bound:  3.27336311e-01\n",
      "Epoch: 5084 mean train loss:  1.61641136e-01, bound:  3.27335685e-01\n",
      "Epoch: 5085 mean train loss:  1.61867663e-01, bound:  3.27314764e-01\n",
      "Epoch: 5086 mean train loss:  1.62196368e-01, bound:  3.27320337e-01\n",
      "Epoch: 5087 mean train loss:  1.62426561e-01, bound:  3.27294141e-01\n",
      "Epoch: 5088 mean train loss:  1.62168667e-01, bound:  3.27303082e-01\n",
      "Epoch: 5089 mean train loss:  1.61371499e-01, bound:  3.27280521e-01\n",
      "Epoch: 5090 mean train loss:  1.60673276e-01, bound:  3.27280045e-01\n",
      "Epoch: 5091 mean train loss:  1.60657063e-01, bound:  3.27272981e-01\n",
      "Epoch: 5092 mean train loss:  1.61037743e-01, bound:  3.27256501e-01\n",
      "Epoch: 5093 mean train loss:  1.61091059e-01, bound:  3.27259749e-01\n",
      "Epoch: 5094 mean train loss:  1.60607606e-01, bound:  3.27240676e-01\n",
      "Epoch: 5095 mean train loss:  1.60162151e-01, bound:  3.27237248e-01\n",
      "Epoch: 5096 mean train loss:  1.60207450e-01, bound:  3.27230722e-01\n",
      "Epoch: 5097 mean train loss:  1.60403982e-01, bound:  3.27214569e-01\n",
      "Epoch: 5098 mean train loss:  1.60248190e-01, bound:  3.27214658e-01\n",
      "Epoch: 5099 mean train loss:  1.59861505e-01, bound:  3.27199489e-01\n",
      "Epoch: 5100 mean train loss:  1.59713298e-01, bound:  3.27191949e-01\n",
      "Epoch: 5101 mean train loss:  1.59817636e-01, bound:  3.27187955e-01\n",
      "Epoch: 5102 mean train loss:  1.59789249e-01, bound:  3.27172339e-01\n",
      "Epoch: 5103 mean train loss:  1.59525767e-01, bound:  3.27169478e-01\n",
      "Epoch: 5104 mean train loss:  1.59326389e-01, bound:  3.27158421e-01\n",
      "Epoch: 5105 mean train loss:  1.59335017e-01, bound:  3.27147454e-01\n",
      "Epoch: 5106 mean train loss:  1.59332559e-01, bound:  3.27144206e-01\n",
      "Epoch: 5107 mean train loss:  1.59161255e-01, bound:  3.27130258e-01\n",
      "Epoch: 5108 mean train loss:  1.58968955e-01, bound:  3.27124357e-01\n",
      "Epoch: 5109 mean train loss:  1.58913150e-01, bound:  3.27116609e-01\n",
      "Epoch: 5110 mean train loss:  1.58899501e-01, bound:  3.27104390e-01\n",
      "Epoch: 5111 mean train loss:  1.58785269e-01, bound:  3.27100486e-01\n",
      "Epoch: 5112 mean train loss:  1.58617944e-01, bound:  3.27089101e-01\n",
      "Epoch: 5113 mean train loss:  1.58524618e-01, bound:  3.27080697e-01\n",
      "Epoch: 5114 mean train loss:  1.58488736e-01, bound:  3.27074707e-01\n",
      "Epoch: 5115 mean train loss:  1.58405811e-01, bound:  3.27062756e-01\n",
      "Epoch: 5116 mean train loss:  1.58267334e-01, bound:  3.27057391e-01\n",
      "Epoch: 5117 mean train loss:  1.58155739e-01, bound:  3.27048272e-01\n",
      "Epoch: 5118 mean train loss:  1.58096090e-01, bound:  3.27038586e-01\n",
      "Epoch: 5119 mean train loss:  1.58026651e-01, bound:  3.27032954e-01\n",
      "Epoch: 5120 mean train loss:  1.57914191e-01, bound:  3.27021897e-01\n",
      "Epoch: 5121 mean train loss:  1.57798886e-01, bound:  3.27015042e-01\n",
      "Epoch: 5122 mean train loss:  1.57718390e-01, bound:  3.27007294e-01\n",
      "Epoch: 5123 mean train loss:  1.57649562e-01, bound:  3.26997131e-01\n",
      "Epoch: 5124 mean train loss:  1.57556951e-01, bound:  3.26991051e-01\n",
      "Epoch: 5125 mean train loss:  1.57447621e-01, bound:  3.26981097e-01\n",
      "Epoch: 5126 mean train loss:  1.57353610e-01, bound:  3.26973170e-01\n",
      "Epoch: 5127 mean train loss:  1.57277673e-01, bound:  3.26966047e-01\n",
      "Epoch: 5128 mean train loss:  1.57195255e-01, bound:  3.26956034e-01\n",
      "Epoch: 5129 mean train loss:  1.57097116e-01, bound:  3.26949388e-01\n",
      "Epoch: 5130 mean train loss:  1.56998172e-01, bound:  3.26940209e-01\n",
      "Epoch: 5131 mean train loss:  1.56912699e-01, bound:  3.26931715e-01\n",
      "Epoch: 5132 mean train loss:  1.56832144e-01, bound:  3.26924652e-01\n",
      "Epoch: 5133 mean train loss:  1.56743646e-01, bound:  3.26914966e-01\n",
      "Epoch: 5134 mean train loss:  1.56647965e-01, bound:  3.26907903e-01\n",
      "Epoch: 5135 mean train loss:  1.56555533e-01, bound:  3.26899230e-01\n",
      "Epoch: 5136 mean train loss:  1.56471327e-01, bound:  3.26890618e-01\n",
      "Epoch: 5137 mean train loss:  1.56387135e-01, bound:  3.26883495e-01\n",
      "Epoch: 5138 mean train loss:  1.56297311e-01, bound:  3.26874167e-01\n",
      "Epoch: 5139 mean train loss:  1.56204864e-01, bound:  3.26866776e-01\n",
      "Epoch: 5140 mean train loss:  1.56115353e-01, bound:  3.26858431e-01\n",
      "Epoch: 5141 mean train loss:  1.56030327e-01, bound:  3.26849818e-01\n",
      "Epoch: 5142 mean train loss:  1.55944660e-01, bound:  3.26842517e-01\n",
      "Epoch: 5143 mean train loss:  1.55855432e-01, bound:  3.26833546e-01\n",
      "Epoch: 5144 mean train loss:  1.55765072e-01, bound:  3.26826036e-01\n",
      "Epoch: 5145 mean train loss:  1.55676663e-01, bound:  3.26817811e-01\n",
      "Epoch: 5146 mean train loss:  1.55590624e-01, bound:  3.26809376e-01\n",
      "Epoch: 5147 mean train loss:  1.55504823e-01, bound:  3.26801926e-01\n",
      "Epoch: 5148 mean train loss:  1.55416980e-01, bound:  3.26793164e-01\n",
      "Epoch: 5149 mean train loss:  1.55327797e-01, bound:  3.26785594e-01\n",
      "Epoch: 5150 mean train loss:  1.55239776e-01, bound:  3.26777369e-01\n",
      "Epoch: 5151 mean train loss:  1.55153453e-01, bound:  3.26769114e-01\n",
      "Epoch: 5152 mean train loss:  1.55066893e-01, bound:  3.26761574e-01\n",
      "Epoch: 5153 mean train loss:  1.54979929e-01, bound:  3.26752990e-01\n",
      "Epoch: 5154 mean train loss:  1.54892340e-01, bound:  3.26745450e-01\n",
      "Epoch: 5155 mean train loss:  1.54804662e-01, bound:  3.26737195e-01\n",
      "Epoch: 5156 mean train loss:  1.54717848e-01, bound:  3.26729119e-01\n",
      "Epoch: 5157 mean train loss:  1.54631704e-01, bound:  3.26721400e-01\n",
      "Epoch: 5158 mean train loss:  1.54545277e-01, bound:  3.26712996e-01\n",
      "Epoch: 5159 mean train loss:  1.54458210e-01, bound:  3.26705426e-01\n",
      "Epoch: 5160 mean train loss:  1.54370993e-01, bound:  3.26697201e-01\n",
      "Epoch: 5161 mean train loss:  1.54284373e-01, bound:  3.26689303e-01\n",
      "Epoch: 5162 mean train loss:  1.54198229e-01, bound:  3.26681525e-01\n",
      "Epoch: 5163 mean train loss:  1.54112101e-01, bound:  3.26673299e-01\n",
      "Epoch: 5164 mean train loss:  1.54025778e-01, bound:  3.26665699e-01\n",
      "Epoch: 5165 mean train loss:  1.53939426e-01, bound:  3.26657504e-01\n",
      "Epoch: 5166 mean train loss:  1.53852984e-01, bound:  3.26649785e-01\n",
      "Epoch: 5167 mean train loss:  1.53766602e-01, bound:  3.26641887e-01\n",
      "Epoch: 5168 mean train loss:  1.53680891e-01, bound:  3.26633871e-01\n",
      "Epoch: 5169 mean train loss:  1.53595328e-01, bound:  3.26626241e-01\n",
      "Epoch: 5170 mean train loss:  1.53509125e-01, bound:  3.26618105e-01\n",
      "Epoch: 5171 mean train loss:  1.53423071e-01, bound:  3.26610506e-01\n",
      "Epoch: 5172 mean train loss:  1.53337061e-01, bound:  3.26602489e-01\n",
      "Epoch: 5173 mean train loss:  1.53251499e-01, bound:  3.26594740e-01\n",
      "Epoch: 5174 mean train loss:  1.53165996e-01, bound:  3.26586932e-01\n",
      "Epoch: 5175 mean train loss:  1.53080910e-01, bound:  3.26579034e-01\n",
      "Epoch: 5176 mean train loss:  1.52995333e-01, bound:  3.26571405e-01\n",
      "Epoch: 5177 mean train loss:  1.52909577e-01, bound:  3.26563448e-01\n",
      "Epoch: 5178 mean train loss:  1.52824223e-01, bound:  3.26555789e-01\n",
      "Epoch: 5179 mean train loss:  1.52739048e-01, bound:  3.26547951e-01\n",
      "Epoch: 5180 mean train loss:  1.52653888e-01, bound:  3.26540202e-01\n",
      "Epoch: 5181 mean train loss:  1.52568683e-01, bound:  3.26532513e-01\n",
      "Epoch: 5182 mean train loss:  1.52483508e-01, bound:  3.26524705e-01\n",
      "Epoch: 5183 mean train loss:  1.52398616e-01, bound:  3.26517045e-01\n",
      "Epoch: 5184 mean train loss:  1.52313769e-01, bound:  3.26509207e-01\n",
      "Epoch: 5185 mean train loss:  1.52228907e-01, bound:  3.26501608e-01\n",
      "Epoch: 5186 mean train loss:  1.52144119e-01, bound:  3.26493829e-01\n",
      "Epoch: 5187 mean train loss:  1.52059376e-01, bound:  3.26486140e-01\n",
      "Epoch: 5188 mean train loss:  1.51974946e-01, bound:  3.26478481e-01\n",
      "Epoch: 5189 mean train loss:  1.51890382e-01, bound:  3.26470792e-01\n",
      "Epoch: 5190 mean train loss:  1.51805550e-01, bound:  3.26463193e-01\n",
      "Epoch: 5191 mean train loss:  1.51721016e-01, bound:  3.26455414e-01\n",
      "Epoch: 5192 mean train loss:  1.51636779e-01, bound:  3.26447815e-01\n",
      "Epoch: 5193 mean train loss:  1.51552558e-01, bound:  3.26440126e-01\n",
      "Epoch: 5194 mean train loss:  1.51468575e-01, bound:  3.26432556e-01\n",
      "Epoch: 5195 mean train loss:  1.51384085e-01, bound:  3.26424927e-01\n",
      "Epoch: 5196 mean train loss:  1.51300386e-01, bound:  3.26417327e-01\n",
      "Epoch: 5197 mean train loss:  1.51216030e-01, bound:  3.26409727e-01\n",
      "Epoch: 5198 mean train loss:  1.51132092e-01, bound:  3.26402098e-01\n",
      "Epoch: 5199 mean train loss:  1.51048020e-01, bound:  3.26394588e-01\n",
      "Epoch: 5200 mean train loss:  1.50964692e-01, bound:  3.26386958e-01\n",
      "Epoch: 5201 mean train loss:  1.50880873e-01, bound:  3.26379448e-01\n",
      "Epoch: 5202 mean train loss:  1.50797278e-01, bound:  3.26371819e-01\n",
      "Epoch: 5203 mean train loss:  1.50713906e-01, bound:  3.26364368e-01\n",
      "Epoch: 5204 mean train loss:  1.50630146e-01, bound:  3.26356769e-01\n",
      "Epoch: 5205 mean train loss:  1.50546655e-01, bound:  3.26349288e-01\n",
      "Epoch: 5206 mean train loss:  1.50463387e-01, bound:  3.26341748e-01\n",
      "Epoch: 5207 mean train loss:  1.50379956e-01, bound:  3.26334238e-01\n",
      "Epoch: 5208 mean train loss:  1.50296554e-01, bound:  3.26326758e-01\n",
      "Epoch: 5209 mean train loss:  1.50213599e-01, bound:  3.26319277e-01\n",
      "Epoch: 5210 mean train loss:  1.50130585e-01, bound:  3.26311797e-01\n",
      "Epoch: 5211 mean train loss:  1.50047556e-01, bound:  3.26304287e-01\n",
      "Epoch: 5212 mean train loss:  1.49964809e-01, bound:  3.26296896e-01\n",
      "Epoch: 5213 mean train loss:  1.49882033e-01, bound:  3.26289415e-01\n",
      "Epoch: 5214 mean train loss:  1.49799183e-01, bound:  3.26282024e-01\n",
      "Epoch: 5215 mean train loss:  1.49716347e-01, bound:  3.26274574e-01\n",
      "Epoch: 5216 mean train loss:  1.49633572e-01, bound:  3.26267183e-01\n",
      "Epoch: 5217 mean train loss:  1.49551451e-01, bound:  3.26259762e-01\n",
      "Epoch: 5218 mean train loss:  1.49468556e-01, bound:  3.26252431e-01\n",
      "Epoch: 5219 mean train loss:  1.49386138e-01, bound:  3.26244920e-01\n",
      "Epoch: 5220 mean train loss:  1.49303854e-01, bound:  3.26237649e-01\n",
      "Epoch: 5221 mean train loss:  1.49221689e-01, bound:  3.26230168e-01\n",
      "Epoch: 5222 mean train loss:  1.49139538e-01, bound:  3.26222867e-01\n",
      "Epoch: 5223 mean train loss:  1.49057075e-01, bound:  3.26215446e-01\n",
      "Epoch: 5224 mean train loss:  1.48975104e-01, bound:  3.26208204e-01\n",
      "Epoch: 5225 mean train loss:  1.48893490e-01, bound:  3.26200753e-01\n",
      "Epoch: 5226 mean train loss:  1.48811549e-01, bound:  3.26193541e-01\n",
      "Epoch: 5227 mean train loss:  1.48729309e-01, bound:  3.26186061e-01\n",
      "Epoch: 5228 mean train loss:  1.48647651e-01, bound:  3.26178968e-01\n",
      "Epoch: 5229 mean train loss:  1.48566365e-01, bound:  3.26171428e-01\n",
      "Epoch: 5230 mean train loss:  1.48484915e-01, bound:  3.26164424e-01\n",
      "Epoch: 5231 mean train loss:  1.48403585e-01, bound:  3.26156855e-01\n",
      "Epoch: 5232 mean train loss:  1.48322150e-01, bound:  3.26149911e-01\n",
      "Epoch: 5233 mean train loss:  1.48241699e-01, bound:  3.26142192e-01\n",
      "Epoch: 5234 mean train loss:  1.48161352e-01, bound:  3.26135546e-01\n",
      "Epoch: 5235 mean train loss:  1.48080871e-01, bound:  3.26127589e-01\n",
      "Epoch: 5236 mean train loss:  1.48003116e-01, bound:  3.26121271e-01\n",
      "Epoch: 5237 mean train loss:  1.47926405e-01, bound:  3.26112837e-01\n",
      "Epoch: 5238 mean train loss:  1.47852227e-01, bound:  3.26107174e-01\n",
      "Epoch: 5239 mean train loss:  1.47784218e-01, bound:  3.26097965e-01\n",
      "Epoch: 5240 mean train loss:  1.47725403e-01, bound:  3.26093346e-01\n",
      "Epoch: 5241 mean train loss:  1.47683725e-01, bound:  3.26082766e-01\n",
      "Epoch: 5242 mean train loss:  1.47669792e-01, bound:  3.26080143e-01\n",
      "Epoch: 5243 mean train loss:  1.47702381e-01, bound:  3.26067120e-01\n",
      "Epoch: 5244 mean train loss:  1.47803202e-01, bound:  3.26067597e-01\n",
      "Epoch: 5245 mean train loss:  1.47984892e-01, bound:  3.26050967e-01\n",
      "Epoch: 5246 mean train loss:  1.48205727e-01, bound:  3.26055676e-01\n",
      "Epoch: 5247 mean train loss:  1.48323610e-01, bound:  3.26035619e-01\n",
      "Epoch: 5248 mean train loss:  1.48121193e-01, bound:  3.26042175e-01\n",
      "Epoch: 5249 mean train loss:  1.47573665e-01, bound:  3.26024592e-01\n",
      "Epoch: 5250 mean train loss:  1.47012204e-01, bound:  3.26025277e-01\n",
      "Epoch: 5251 mean train loss:  1.46825895e-01, bound:  3.26017678e-01\n",
      "Epoch: 5252 mean train loss:  1.47008106e-01, bound:  3.26007456e-01\n",
      "Epoch: 5253 mean train loss:  1.47203714e-01, bound:  3.26008558e-01\n",
      "Epoch: 5254 mean train loss:  1.47094280e-01, bound:  3.25993061e-01\n",
      "Epoch: 5255 mean train loss:  1.46725431e-01, bound:  3.25993389e-01\n",
      "Epoch: 5256 mean train loss:  1.46443754e-01, bound:  3.25983435e-01\n",
      "Epoch: 5257 mean train loss:  1.46446630e-01, bound:  3.25975448e-01\n",
      "Epoch: 5258 mean train loss:  1.46565646e-01, bound:  3.25973570e-01\n",
      "Epoch: 5259 mean train loss:  1.46524414e-01, bound:  3.25960279e-01\n",
      "Epoch: 5260 mean train loss:  1.46289408e-01, bound:  3.25959265e-01\n",
      "Epoch: 5261 mean train loss:  1.46076247e-01, bound:  3.25949878e-01\n",
      "Epoch: 5262 mean train loss:  1.46039099e-01, bound:  3.25942576e-01\n",
      "Epoch: 5263 mean train loss:  1.46083504e-01, bound:  3.25939536e-01\n",
      "Epoch: 5264 mean train loss:  1.46032527e-01, bound:  3.25927734e-01\n",
      "Epoch: 5265 mean train loss:  1.45862252e-01, bound:  3.25925231e-01\n",
      "Epoch: 5266 mean train loss:  1.45707145e-01, bound:  3.25916469e-01\n",
      "Epoch: 5267 mean train loss:  1.45659283e-01, bound:  3.25909197e-01\n",
      "Epoch: 5268 mean train loss:  1.45657450e-01, bound:  3.25905442e-01\n",
      "Epoch: 5269 mean train loss:  1.45594493e-01, bound:  3.25894982e-01\n",
      "Epoch: 5270 mean train loss:  1.45462051e-01, bound:  3.25891644e-01\n",
      "Epoch: 5271 mean train loss:  1.45342261e-01, bound:  3.25883597e-01\n",
      "Epoch: 5272 mean train loss:  1.45287171e-01, bound:  3.25876534e-01\n",
      "Epoch: 5273 mean train loss:  1.45257026e-01, bound:  3.25872242e-01\n",
      "Epoch: 5274 mean train loss:  1.45189598e-01, bound:  3.25862706e-01\n",
      "Epoch: 5275 mean train loss:  1.45081311e-01, bound:  3.25858742e-01\n",
      "Epoch: 5276 mean train loss:  1.44981593e-01, bound:  3.25851202e-01\n",
      "Epoch: 5277 mean train loss:  1.44918889e-01, bound:  3.25844377e-01\n",
      "Epoch: 5278 mean train loss:  1.44872665e-01, bound:  3.25839639e-01\n",
      "Epoch: 5279 mean train loss:  1.44804955e-01, bound:  3.25830847e-01\n",
      "Epoch: 5280 mean train loss:  1.44712850e-01, bound:  3.25826436e-01\n",
      "Epoch: 5281 mean train loss:  1.44623145e-01, bound:  3.25819105e-01\n",
      "Epoch: 5282 mean train loss:  1.44554749e-01, bound:  3.25812519e-01\n",
      "Epoch: 5283 mean train loss:  1.44498035e-01, bound:  3.25807303e-01\n",
      "Epoch: 5284 mean train loss:  1.44432113e-01, bound:  3.25799108e-01\n",
      "Epoch: 5285 mean train loss:  1.44350186e-01, bound:  3.25794369e-01\n",
      "Epoch: 5286 mean train loss:  1.44266814e-01, bound:  3.25787038e-01\n",
      "Epoch: 5287 mean train loss:  1.44194320e-01, bound:  3.25780809e-01\n",
      "Epoch: 5288 mean train loss:  1.44131675e-01, bound:  3.25775206e-01\n",
      "Epoch: 5289 mean train loss:  1.44065946e-01, bound:  3.25767487e-01\n",
      "Epoch: 5290 mean train loss:  1.43991739e-01, bound:  3.25762451e-01\n",
      "Epoch: 5291 mean train loss:  1.43912926e-01, bound:  3.25755060e-01\n",
      "Epoch: 5292 mean train loss:  1.43838346e-01, bound:  3.25749099e-01\n",
      "Epoch: 5293 mean train loss:  1.43770754e-01, bound:  3.25743049e-01\n",
      "Epoch: 5294 mean train loss:  1.43704355e-01, bound:  3.25735867e-01\n",
      "Epoch: 5295 mean train loss:  1.43634230e-01, bound:  3.25730562e-01\n",
      "Epoch: 5296 mean train loss:  1.43560261e-01, bound:  3.25723350e-01\n",
      "Epoch: 5297 mean train loss:  1.43485814e-01, bound:  3.25717568e-01\n",
      "Epoch: 5298 mean train loss:  1.43414989e-01, bound:  3.25711191e-01\n",
      "Epoch: 5299 mean train loss:  1.43346980e-01, bound:  3.25704396e-01\n",
      "Epoch: 5300 mean train loss:  1.43278629e-01, bound:  3.25698853e-01\n",
      "Epoch: 5301 mean train loss:  1.43207416e-01, bound:  3.25691760e-01\n",
      "Epoch: 5302 mean train loss:  1.43134803e-01, bound:  3.25686097e-01\n",
      "Epoch: 5303 mean train loss:  1.43063083e-01, bound:  3.25679511e-01\n",
      "Epoch: 5304 mean train loss:  1.42993227e-01, bound:  3.25673252e-01\n",
      "Epoch: 5305 mean train loss:  1.42924324e-01, bound:  3.25667322e-01\n",
      "Epoch: 5306 mean train loss:  1.42855331e-01, bound:  3.25660586e-01\n",
      "Epoch: 5307 mean train loss:  1.42784998e-01, bound:  3.25654924e-01\n",
      "Epoch: 5308 mean train loss:  1.42713785e-01, bound:  3.25648159e-01\n",
      "Epoch: 5309 mean train loss:  1.42643243e-01, bound:  3.25642198e-01\n",
      "Epoch: 5310 mean train loss:  1.42573014e-01, bound:  3.25636029e-01\n",
      "Epoch: 5311 mean train loss:  1.42504051e-01, bound:  3.25629592e-01\n",
      "Epoch: 5312 mean train loss:  1.42434984e-01, bound:  3.25623780e-01\n",
      "Epoch: 5313 mean train loss:  1.42365381e-01, bound:  3.25617135e-01\n",
      "Epoch: 5314 mean train loss:  1.42295673e-01, bound:  3.25611323e-01\n",
      "Epoch: 5315 mean train loss:  1.42225251e-01, bound:  3.25604916e-01\n",
      "Epoch: 5316 mean train loss:  1.42155692e-01, bound:  3.25598836e-01\n",
      "Epoch: 5317 mean train loss:  1.42086521e-01, bound:  3.25592756e-01\n",
      "Epoch: 5318 mean train loss:  1.42017484e-01, bound:  3.25586349e-01\n",
      "Epoch: 5319 mean train loss:  1.41948491e-01, bound:  3.25580508e-01\n",
      "Epoch: 5320 mean train loss:  1.41878963e-01, bound:  3.25574070e-01\n",
      "Epoch: 5321 mean train loss:  1.41809985e-01, bound:  3.25568169e-01\n",
      "Epoch: 5322 mean train loss:  1.41740605e-01, bound:  3.25561881e-01\n",
      "Epoch: 5323 mean train loss:  1.41671509e-01, bound:  3.25555801e-01\n",
      "Epoch: 5324 mean train loss:  1.41602471e-01, bound:  3.25549781e-01\n",
      "Epoch: 5325 mean train loss:  1.41533807e-01, bound:  3.25543493e-01\n",
      "Epoch: 5326 mean train loss:  1.41465068e-01, bound:  3.25537592e-01\n",
      "Epoch: 5327 mean train loss:  1.41396463e-01, bound:  3.25531304e-01\n",
      "Epoch: 5328 mean train loss:  1.41327441e-01, bound:  3.25525403e-01\n",
      "Epoch: 5329 mean train loss:  1.41258791e-01, bound:  3.25519174e-01\n",
      "Epoch: 5330 mean train loss:  1.41190156e-01, bound:  3.25513184e-01\n",
      "Epoch: 5331 mean train loss:  1.41121432e-01, bound:  3.25507164e-01\n",
      "Epoch: 5332 mean train loss:  1.41053125e-01, bound:  3.25501025e-01\n",
      "Epoch: 5333 mean train loss:  1.40984192e-01, bound:  3.25495094e-01\n",
      "Epoch: 5334 mean train loss:  1.40916660e-01, bound:  3.25488895e-01\n",
      "Epoch: 5335 mean train loss:  1.40848175e-01, bound:  3.25483024e-01\n",
      "Epoch: 5336 mean train loss:  1.40779540e-01, bound:  3.25476855e-01\n",
      "Epoch: 5337 mean train loss:  1.40711471e-01, bound:  3.25470924e-01\n",
      "Epoch: 5338 mean train loss:  1.40643761e-01, bound:  3.25464845e-01\n",
      "Epoch: 5339 mean train loss:  1.40575558e-01, bound:  3.25458884e-01\n",
      "Epoch: 5340 mean train loss:  1.40506983e-01, bound:  3.25452894e-01\n",
      "Epoch: 5341 mean train loss:  1.40439227e-01, bound:  3.25446874e-01\n",
      "Epoch: 5342 mean train loss:  1.40371189e-01, bound:  3.25440943e-01\n",
      "Epoch: 5343 mean train loss:  1.40303507e-01, bound:  3.25434864e-01\n",
      "Epoch: 5344 mean train loss:  1.40235692e-01, bound:  3.25428993e-01\n",
      "Epoch: 5345 mean train loss:  1.40167817e-01, bound:  3.25422972e-01\n",
      "Epoch: 5346 mean train loss:  1.40100092e-01, bound:  3.25417072e-01\n",
      "Epoch: 5347 mean train loss:  1.40032604e-01, bound:  3.25411052e-01\n",
      "Epoch: 5348 mean train loss:  1.39964744e-01, bound:  3.25405151e-01\n",
      "Epoch: 5349 mean train loss:  1.39897138e-01, bound:  3.25399220e-01\n",
      "Epoch: 5350 mean train loss:  1.39829740e-01, bound:  3.25393260e-01\n",
      "Epoch: 5351 mean train loss:  1.39762461e-01, bound:  3.25387329e-01\n",
      "Epoch: 5352 mean train loss:  1.39695376e-01, bound:  3.25381398e-01\n",
      "Epoch: 5353 mean train loss:  1.39627814e-01, bound:  3.25375527e-01\n",
      "Epoch: 5354 mean train loss:  1.39560580e-01, bound:  3.25369537e-01\n",
      "Epoch: 5355 mean train loss:  1.39493436e-01, bound:  3.25363696e-01\n",
      "Epoch: 5356 mean train loss:  1.39425963e-01, bound:  3.25357735e-01\n",
      "Epoch: 5357 mean train loss:  1.39359400e-01, bound:  3.25351954e-01\n",
      "Epoch: 5358 mean train loss:  1.39292344e-01, bound:  3.25345993e-01\n",
      "Epoch: 5359 mean train loss:  1.39225468e-01, bound:  3.25340152e-01\n",
      "Epoch: 5360 mean train loss:  1.39158428e-01, bound:  3.25334251e-01\n",
      "Epoch: 5361 mean train loss:  1.39091551e-01, bound:  3.25328439e-01\n",
      "Epoch: 5362 mean train loss:  1.39024571e-01, bound:  3.25322539e-01\n",
      "Epoch: 5363 mean train loss:  1.38957962e-01, bound:  3.25316697e-01\n",
      "Epoch: 5364 mean train loss:  1.38891488e-01, bound:  3.25310826e-01\n",
      "Epoch: 5365 mean train loss:  1.38824627e-01, bound:  3.25305015e-01\n",
      "Epoch: 5366 mean train loss:  1.38758138e-01, bound:  3.25299174e-01\n",
      "Epoch: 5367 mean train loss:  1.38691515e-01, bound:  3.25293362e-01\n",
      "Epoch: 5368 mean train loss:  1.38625160e-01, bound:  3.25287521e-01\n",
      "Epoch: 5369 mean train loss:  1.38558820e-01, bound:  3.25281709e-01\n",
      "Epoch: 5370 mean train loss:  1.38492152e-01, bound:  3.25275958e-01\n",
      "Epoch: 5371 mean train loss:  1.38425976e-01, bound:  3.25270116e-01\n",
      "Epoch: 5372 mean train loss:  1.38359934e-01, bound:  3.25264335e-01\n",
      "Epoch: 5373 mean train loss:  1.38293445e-01, bound:  3.25258523e-01\n",
      "Epoch: 5374 mean train loss:  1.38227418e-01, bound:  3.25252771e-01\n",
      "Epoch: 5375 mean train loss:  1.38161302e-01, bound:  3.25246960e-01\n",
      "Epoch: 5376 mean train loss:  1.38095438e-01, bound:  3.25241238e-01\n",
      "Epoch: 5377 mean train loss:  1.38029426e-01, bound:  3.25235426e-01\n",
      "Epoch: 5378 mean train loss:  1.37963220e-01, bound:  3.25229704e-01\n",
      "Epoch: 5379 mean train loss:  1.37897447e-01, bound:  3.25223953e-01\n",
      "Epoch: 5380 mean train loss:  1.37831733e-01, bound:  3.25218230e-01\n",
      "Epoch: 5381 mean train loss:  1.37765989e-01, bound:  3.25212479e-01\n",
      "Epoch: 5382 mean train loss:  1.37700334e-01, bound:  3.25206786e-01\n",
      "Epoch: 5383 mean train loss:  1.37635127e-01, bound:  3.25200975e-01\n",
      "Epoch: 5384 mean train loss:  1.37569487e-01, bound:  3.25195342e-01\n",
      "Epoch: 5385 mean train loss:  1.37503594e-01, bound:  3.25189531e-01\n",
      "Epoch: 5386 mean train loss:  1.37438014e-01, bound:  3.25183958e-01\n",
      "Epoch: 5387 mean train loss:  1.37372792e-01, bound:  3.25178087e-01\n",
      "Epoch: 5388 mean train loss:  1.37307271e-01, bound:  3.25172544e-01\n",
      "Epoch: 5389 mean train loss:  1.37242362e-01, bound:  3.25166702e-01\n",
      "Epoch: 5390 mean train loss:  1.37177452e-01, bound:  3.25161248e-01\n",
      "Epoch: 5391 mean train loss:  1.37112454e-01, bound:  3.25155199e-01\n",
      "Epoch: 5392 mean train loss:  1.37048244e-01, bound:  3.25150013e-01\n",
      "Epoch: 5393 mean train loss:  1.36984706e-01, bound:  3.25143725e-01\n",
      "Epoch: 5394 mean train loss:  1.36922061e-01, bound:  3.25138867e-01\n",
      "Epoch: 5395 mean train loss:  1.36862099e-01, bound:  3.25132191e-01\n",
      "Epoch: 5396 mean train loss:  1.36806130e-01, bound:  3.25127870e-01\n",
      "Epoch: 5397 mean train loss:  1.36757061e-01, bound:  3.25120419e-01\n",
      "Epoch: 5398 mean train loss:  1.36721149e-01, bound:  3.25117201e-01\n",
      "Epoch: 5399 mean train loss:  1.36709422e-01, bound:  3.25108349e-01\n",
      "Epoch: 5400 mean train loss:  1.36740297e-01, bound:  3.25107127e-01\n",
      "Epoch: 5401 mean train loss:  1.36842743e-01, bound:  3.25095654e-01\n",
      "Epoch: 5402 mean train loss:  1.37050197e-01, bound:  3.25097799e-01\n",
      "Epoch: 5403 mean train loss:  1.37370050e-01, bound:  3.25082451e-01\n",
      "Epoch: 5404 mean train loss:  1.37697592e-01, bound:  3.25088769e-01\n",
      "Epoch: 5405 mean train loss:  1.37760684e-01, bound:  3.25070560e-01\n",
      "Epoch: 5406 mean train loss:  1.37286484e-01, bound:  3.25077444e-01\n",
      "Epoch: 5407 mean train loss:  1.36495963e-01, bound:  3.25063765e-01\n",
      "Epoch: 5408 mean train loss:  1.36034057e-01, bound:  3.25062722e-01\n",
      "Epoch: 5409 mean train loss:  1.36190042e-01, bound:  3.25060129e-01\n",
      "Epoch: 5410 mean train loss:  1.36561379e-01, bound:  3.25048655e-01\n",
      "Epoch: 5411 mean train loss:  1.36567459e-01, bound:  3.25052023e-01\n",
      "Epoch: 5412 mean train loss:  1.36129498e-01, bound:  3.25039417e-01\n",
      "Epoch: 5413 mean train loss:  1.35747492e-01, bound:  3.25037748e-01\n",
      "Epoch: 5414 mean train loss:  1.35788620e-01, bound:  3.25033516e-01\n",
      "Epoch: 5415 mean train loss:  1.36002287e-01, bound:  3.25023234e-01\n",
      "Epoch: 5416 mean train loss:  1.35952562e-01, bound:  3.25024217e-01\n",
      "Epoch: 5417 mean train loss:  1.35639966e-01, bound:  3.25013846e-01\n",
      "Epoch: 5418 mean train loss:  1.35439605e-01, bound:  3.25010628e-01\n",
      "Epoch: 5419 mean train loss:  1.35506362e-01, bound:  3.25007319e-01\n",
      "Epoch: 5420 mean train loss:  1.35587811e-01, bound:  3.24997693e-01\n",
      "Epoch: 5421 mean train loss:  1.35458529e-01, bound:  3.24997187e-01\n",
      "Epoch: 5422 mean train loss:  1.35240376e-01, bound:  3.24988693e-01\n",
      "Epoch: 5423 mean train loss:  1.35166600e-01, bound:  3.24983716e-01\n",
      "Epoch: 5424 mean train loss:  1.35217339e-01, bound:  3.24980825e-01\n",
      "Epoch: 5425 mean train loss:  1.35195300e-01, bound:  3.24971646e-01\n",
      "Epoch: 5426 mean train loss:  1.35048330e-01, bound:  3.24969798e-01\n",
      "Epoch: 5427 mean train loss:  1.34919897e-01, bound:  3.24963152e-01\n",
      "Epoch: 5428 mean train loss:  1.34900704e-01, bound:  3.24957252e-01\n",
      "Epoch: 5429 mean train loss:  1.34904116e-01, bound:  3.24954689e-01\n",
      "Epoch: 5430 mean train loss:  1.34827882e-01, bound:  3.24946463e-01\n",
      "Epoch: 5431 mean train loss:  1.34706885e-01, bound:  3.24943334e-01\n",
      "Epoch: 5432 mean train loss:  1.34636506e-01, bound:  3.24937969e-01\n",
      "Epoch: 5433 mean train loss:  1.34620145e-01, bound:  3.24931562e-01\n",
      "Epoch: 5434 mean train loss:  1.34582698e-01, bound:  3.24928790e-01\n",
      "Epoch: 5435 mean train loss:  1.34494245e-01, bound:  3.24921519e-01\n",
      "Epoch: 5436 mean train loss:  1.34405360e-01, bound:  3.24917555e-01\n",
      "Epoch: 5437 mean train loss:  1.34358555e-01, bound:  3.24912876e-01\n",
      "Epoch: 5438 mean train loss:  1.34327024e-01, bound:  3.24906379e-01\n",
      "Epoch: 5439 mean train loss:  1.34269044e-01, bound:  3.24903220e-01\n",
      "Epoch: 5440 mean train loss:  1.34187162e-01, bound:  3.24896663e-01\n",
      "Epoch: 5441 mean train loss:  1.34119630e-01, bound:  3.24892044e-01\n",
      "Epoch: 5442 mean train loss:  1.34076148e-01, bound:  3.24887693e-01\n",
      "Epoch: 5443 mean train loss:  1.34031430e-01, bound:  3.24881345e-01\n",
      "Epoch: 5444 mean train loss:  1.33966967e-01, bound:  3.24877769e-01\n",
      "Epoch: 5445 mean train loss:  1.33896038e-01, bound:  3.24871838e-01\n",
      "Epoch: 5446 mean train loss:  1.33837357e-01, bound:  3.24866891e-01\n",
      "Epoch: 5447 mean train loss:  1.33791000e-01, bound:  3.24862629e-01\n",
      "Epoch: 5448 mean train loss:  1.33738950e-01, bound:  3.24856430e-01\n",
      "Epoch: 5449 mean train loss:  1.33675069e-01, bound:  3.24852407e-01\n",
      "Epoch: 5450 mean train loss:  1.33610964e-01, bound:  3.24846774e-01\n",
      "Epoch: 5451 mean train loss:  1.33555681e-01, bound:  3.24841678e-01\n",
      "Epoch: 5452 mean train loss:  1.33505061e-01, bound:  3.24837357e-01\n",
      "Epoch: 5453 mean train loss:  1.33450195e-01, bound:  3.24831426e-01\n",
      "Epoch: 5454 mean train loss:  1.33389100e-01, bound:  3.24827224e-01\n",
      "Epoch: 5455 mean train loss:  1.33328646e-01, bound:  3.24821800e-01\n",
      "Epoch: 5456 mean train loss:  1.33273348e-01, bound:  3.24816734e-01\n",
      "Epoch: 5457 mean train loss:  1.33221000e-01, bound:  3.24812293e-01\n",
      "Epoch: 5458 mean train loss:  1.33165702e-01, bound:  3.24806511e-01\n",
      "Epoch: 5459 mean train loss:  1.33106545e-01, bound:  3.24802130e-01\n",
      "Epoch: 5460 mean train loss:  1.33047819e-01, bound:  3.24796796e-01\n",
      "Epoch: 5461 mean train loss:  1.32992446e-01, bound:  3.24791789e-01\n",
      "Epoch: 5462 mean train loss:  1.32938534e-01, bound:  3.24787259e-01\n",
      "Epoch: 5463 mean train loss:  1.32882923e-01, bound:  3.24781746e-01\n",
      "Epoch: 5464 mean train loss:  1.32825240e-01, bound:  3.24777275e-01\n",
      "Epoch: 5465 mean train loss:  1.32768452e-01, bound:  3.24772090e-01\n",
      "Epoch: 5466 mean train loss:  1.32712424e-01, bound:  3.24767113e-01\n",
      "Epoch: 5467 mean train loss:  1.32658079e-01, bound:  3.24762493e-01\n",
      "Epoch: 5468 mean train loss:  1.32602736e-01, bound:  3.24757099e-01\n",
      "Epoch: 5469 mean train loss:  1.32546112e-01, bound:  3.24752629e-01\n",
      "Epoch: 5470 mean train loss:  1.32489249e-01, bound:  3.24747443e-01\n",
      "Epoch: 5471 mean train loss:  1.32433683e-01, bound:  3.24742615e-01\n",
      "Epoch: 5472 mean train loss:  1.32378846e-01, bound:  3.24737906e-01\n",
      "Epoch: 5473 mean train loss:  1.32323489e-01, bound:  3.24732691e-01\n",
      "Epoch: 5474 mean train loss:  1.32267788e-01, bound:  3.24728131e-01\n",
      "Epoch: 5475 mean train loss:  1.32211447e-01, bound:  3.24723005e-01\n",
      "Epoch: 5476 mean train loss:  1.32155642e-01, bound:  3.24718207e-01\n",
      "Epoch: 5477 mean train loss:  1.32100642e-01, bound:  3.24713409e-01\n",
      "Epoch: 5478 mean train loss:  1.32045388e-01, bound:  3.24708343e-01\n",
      "Epoch: 5479 mean train loss:  1.31990209e-01, bound:  3.24703723e-01\n",
      "Epoch: 5480 mean train loss:  1.31934240e-01, bound:  3.24698597e-01\n",
      "Epoch: 5481 mean train loss:  1.31878585e-01, bound:  3.24693888e-01\n",
      "Epoch: 5482 mean train loss:  1.31823406e-01, bound:  3.24689001e-01\n",
      "Epoch: 5483 mean train loss:  1.31768569e-01, bound:  3.24684054e-01\n",
      "Epoch: 5484 mean train loss:  1.31713495e-01, bound:  3.24679375e-01\n",
      "Epoch: 5485 mean train loss:  1.31658450e-01, bound:  3.24674398e-01\n",
      "Epoch: 5486 mean train loss:  1.31602541e-01, bound:  3.24669689e-01\n",
      "Epoch: 5487 mean train loss:  1.31547540e-01, bound:  3.24664801e-01\n",
      "Epoch: 5488 mean train loss:  1.31492466e-01, bound:  3.24659914e-01\n",
      "Epoch: 5489 mean train loss:  1.31437495e-01, bound:  3.24655205e-01\n",
      "Epoch: 5490 mean train loss:  1.31382391e-01, bound:  3.24650228e-01\n",
      "Epoch: 5491 mean train loss:  1.31327584e-01, bound:  3.24645549e-01\n",
      "Epoch: 5492 mean train loss:  1.31272271e-01, bound:  3.24640602e-01\n",
      "Epoch: 5493 mean train loss:  1.31217405e-01, bound:  3.24635863e-01\n",
      "Epoch: 5494 mean train loss:  1.31162658e-01, bound:  3.24631065e-01\n",
      "Epoch: 5495 mean train loss:  1.31107733e-01, bound:  3.24626207e-01\n",
      "Epoch: 5496 mean train loss:  1.31052792e-01, bound:  3.24621499e-01\n",
      "Epoch: 5497 mean train loss:  1.30997956e-01, bound:  3.24616641e-01\n",
      "Epoch: 5498 mean train loss:  1.30943090e-01, bound:  3.24611902e-01\n",
      "Epoch: 5499 mean train loss:  1.30888045e-01, bound:  3.24607074e-01\n",
      "Epoch: 5500 mean train loss:  1.30833551e-01, bound:  3.24602336e-01\n",
      "Epoch: 5501 mean train loss:  1.30779222e-01, bound:  3.24597597e-01\n",
      "Epoch: 5502 mean train loss:  1.30724564e-01, bound:  3.24592710e-01\n",
      "Epoch: 5503 mean train loss:  1.30669624e-01, bound:  3.24588060e-01\n",
      "Epoch: 5504 mean train loss:  1.30615309e-01, bound:  3.24583203e-01\n",
      "Epoch: 5505 mean train loss:  1.30560443e-01, bound:  3.24578494e-01\n",
      "Epoch: 5506 mean train loss:  1.30506009e-01, bound:  3.24573696e-01\n",
      "Epoch: 5507 mean train loss:  1.30451277e-01, bound:  3.24568927e-01\n",
      "Epoch: 5508 mean train loss:  1.30397066e-01, bound:  3.24564219e-01\n",
      "Epoch: 5509 mean train loss:  1.30342633e-01, bound:  3.24559420e-01\n",
      "Epoch: 5510 mean train loss:  1.30288333e-01, bound:  3.24554741e-01\n",
      "Epoch: 5511 mean train loss:  1.30234063e-01, bound:  3.24549943e-01\n",
      "Epoch: 5512 mean train loss:  1.30179629e-01, bound:  3.24545264e-01\n",
      "Epoch: 5513 mean train loss:  1.30125463e-01, bound:  3.24540526e-01\n",
      "Epoch: 5514 mean train loss:  1.30071014e-01, bound:  3.24535787e-01\n",
      "Epoch: 5515 mean train loss:  1.30016625e-01, bound:  3.24531049e-01\n",
      "Epoch: 5516 mean train loss:  1.29962444e-01, bound:  3.24526310e-01\n",
      "Epoch: 5517 mean train loss:  1.29908234e-01, bound:  3.24521631e-01\n",
      "Epoch: 5518 mean train loss:  1.29853874e-01, bound:  3.24516892e-01\n",
      "Epoch: 5519 mean train loss:  1.29799619e-01, bound:  3.24512184e-01\n",
      "Epoch: 5520 mean train loss:  1.29745528e-01, bound:  3.24507475e-01\n",
      "Epoch: 5521 mean train loss:  1.29691526e-01, bound:  3.24502766e-01\n",
      "Epoch: 5522 mean train loss:  1.29637569e-01, bound:  3.24498087e-01\n",
      "Epoch: 5523 mean train loss:  1.29583672e-01, bound:  3.24493408e-01\n",
      "Epoch: 5524 mean train loss:  1.29529417e-01, bound:  3.24488670e-01\n",
      "Epoch: 5525 mean train loss:  1.29475072e-01, bound:  3.24483961e-01\n",
      "Epoch: 5526 mean train loss:  1.29421487e-01, bound:  3.24479342e-01\n",
      "Epoch: 5527 mean train loss:  1.29367754e-01, bound:  3.24474603e-01\n",
      "Epoch: 5528 mean train loss:  1.29313573e-01, bound:  3.24469954e-01\n",
      "Epoch: 5529 mean train loss:  1.29259810e-01, bound:  3.24465275e-01\n",
      "Epoch: 5530 mean train loss:  1.29206032e-01, bound:  3.24460596e-01\n",
      "Epoch: 5531 mean train loss:  1.29152283e-01, bound:  3.24455947e-01\n",
      "Epoch: 5532 mean train loss:  1.29098266e-01, bound:  3.24451268e-01\n",
      "Epoch: 5533 mean train loss:  1.29044726e-01, bound:  3.24446619e-01\n",
      "Epoch: 5534 mean train loss:  1.28991023e-01, bound:  3.24441940e-01\n",
      "Epoch: 5535 mean train loss:  1.28936887e-01, bound:  3.24437320e-01\n",
      "Epoch: 5536 mean train loss:  1.28883272e-01, bound:  3.24432641e-01\n",
      "Epoch: 5537 mean train loss:  1.28829762e-01, bound:  3.24427992e-01\n",
      "Epoch: 5538 mean train loss:  1.28775969e-01, bound:  3.24423343e-01\n",
      "Epoch: 5539 mean train loss:  1.28722534e-01, bound:  3.24418724e-01\n",
      "Epoch: 5540 mean train loss:  1.28668711e-01, bound:  3.24414074e-01\n",
      "Epoch: 5541 mean train loss:  1.28615335e-01, bound:  3.24409425e-01\n",
      "Epoch: 5542 mean train loss:  1.28561601e-01, bound:  3.24404806e-01\n",
      "Epoch: 5543 mean train loss:  1.28508136e-01, bound:  3.24400157e-01\n",
      "Epoch: 5544 mean train loss:  1.28454730e-01, bound:  3.24395537e-01\n",
      "Epoch: 5545 mean train loss:  1.28401384e-01, bound:  3.24390948e-01\n",
      "Epoch: 5546 mean train loss:  1.28347471e-01, bound:  3.24386299e-01\n",
      "Epoch: 5547 mean train loss:  1.28294557e-01, bound:  3.24381649e-01\n",
      "Epoch: 5548 mean train loss:  1.28240973e-01, bound:  3.24377060e-01\n",
      "Epoch: 5549 mean train loss:  1.28187686e-01, bound:  3.24372441e-01\n",
      "Epoch: 5550 mean train loss:  1.28134564e-01, bound:  3.24367821e-01\n",
      "Epoch: 5551 mean train loss:  1.28080919e-01, bound:  3.24363232e-01\n",
      "Epoch: 5552 mean train loss:  1.28027946e-01, bound:  3.24358612e-01\n",
      "Epoch: 5553 mean train loss:  1.27974719e-01, bound:  3.24354023e-01\n",
      "Epoch: 5554 mean train loss:  1.27921209e-01, bound:  3.24349403e-01\n",
      "Epoch: 5555 mean train loss:  1.27868131e-01, bound:  3.24344844e-01\n",
      "Epoch: 5556 mean train loss:  1.27814591e-01, bound:  3.24340254e-01\n",
      "Epoch: 5557 mean train loss:  1.27761766e-01, bound:  3.24335694e-01\n",
      "Epoch: 5558 mean train loss:  1.27708271e-01, bound:  3.24331045e-01\n",
      "Epoch: 5559 mean train loss:  1.27655402e-01, bound:  3.24326515e-01\n",
      "Epoch: 5560 mean train loss:  1.27602324e-01, bound:  3.24321896e-01\n",
      "Epoch: 5561 mean train loss:  1.27549380e-01, bound:  3.24317366e-01\n",
      "Epoch: 5562 mean train loss:  1.27495870e-01, bound:  3.24312806e-01\n",
      "Epoch: 5563 mean train loss:  1.27443105e-01, bound:  3.24308246e-01\n",
      "Epoch: 5564 mean train loss:  1.27389893e-01, bound:  3.24303716e-01\n",
      "Epoch: 5565 mean train loss:  1.27336860e-01, bound:  3.24299097e-01\n",
      "Epoch: 5566 mean train loss:  1.27284482e-01, bound:  3.24294567e-01\n",
      "Epoch: 5567 mean train loss:  1.27231166e-01, bound:  3.24289978e-01\n",
      "Epoch: 5568 mean train loss:  1.27178058e-01, bound:  3.24285448e-01\n",
      "Epoch: 5569 mean train loss:  1.27125368e-01, bound:  3.24280858e-01\n",
      "Epoch: 5570 mean train loss:  1.27072319e-01, bound:  3.24276328e-01\n",
      "Epoch: 5571 mean train loss:  1.27019763e-01, bound:  3.24271798e-01\n",
      "Epoch: 5572 mean train loss:  1.26967072e-01, bound:  3.24267209e-01\n",
      "Epoch: 5573 mean train loss:  1.26913726e-01, bound:  3.24262738e-01\n",
      "Epoch: 5574 mean train loss:  1.26861155e-01, bound:  3.24258149e-01\n",
      "Epoch: 5575 mean train loss:  1.26808599e-01, bound:  3.24253649e-01\n",
      "Epoch: 5576 mean train loss:  1.26755640e-01, bound:  3.24249089e-01\n",
      "Epoch: 5577 mean train loss:  1.26702651e-01, bound:  3.24244589e-01\n",
      "Epoch: 5578 mean train loss:  1.26649871e-01, bound:  3.24239969e-01\n",
      "Epoch: 5579 mean train loss:  1.26597434e-01, bound:  3.24235529e-01\n",
      "Epoch: 5580 mean train loss:  1.26544476e-01, bound:  3.24230969e-01\n",
      "Epoch: 5581 mean train loss:  1.26491770e-01, bound:  3.24226469e-01\n",
      "Epoch: 5582 mean train loss:  1.26439169e-01, bound:  3.24221879e-01\n",
      "Epoch: 5583 mean train loss:  1.26386851e-01, bound:  3.24217469e-01\n",
      "Epoch: 5584 mean train loss:  1.26334384e-01, bound:  3.24212849e-01\n",
      "Epoch: 5585 mean train loss:  1.26282051e-01, bound:  3.24208498e-01\n",
      "Epoch: 5586 mean train loss:  1.26229227e-01, bound:  3.24203759e-01\n",
      "Epoch: 5587 mean train loss:  1.26177043e-01, bound:  3.24199528e-01\n",
      "Epoch: 5588 mean train loss:  1.26125231e-01, bound:  3.24194729e-01\n",
      "Epoch: 5589 mean train loss:  1.26073316e-01, bound:  3.24190617e-01\n",
      "Epoch: 5590 mean train loss:  1.26021892e-01, bound:  3.24185610e-01\n",
      "Epoch: 5591 mean train loss:  1.25971615e-01, bound:  3.24181706e-01\n",
      "Epoch: 5592 mean train loss:  1.25922337e-01, bound:  3.24176461e-01\n",
      "Epoch: 5593 mean train loss:  1.25875399e-01, bound:  3.24172944e-01\n",
      "Epoch: 5594 mean train loss:  1.25831440e-01, bound:  3.24167222e-01\n",
      "Epoch: 5595 mean train loss:  1.25793397e-01, bound:  3.24164301e-01\n",
      "Epoch: 5596 mean train loss:  1.25765204e-01, bound:  3.24157804e-01\n",
      "Epoch: 5597 mean train loss:  1.25752971e-01, bound:  3.24155927e-01\n",
      "Epoch: 5598 mean train loss:  1.25766382e-01, bound:  3.24148118e-01\n",
      "Epoch: 5599 mean train loss:  1.25820369e-01, bound:  3.24147910e-01\n",
      "Epoch: 5600 mean train loss:  1.25930905e-01, bound:  3.24138105e-01\n",
      "Epoch: 5601 mean train loss:  1.26103550e-01, bound:  3.24140310e-01\n",
      "Epoch: 5602 mean train loss:  1.26310870e-01, bound:  3.24127942e-01\n",
      "Epoch: 5603 mean train loss:  1.26447767e-01, bound:  3.24132591e-01\n",
      "Epoch: 5604 mean train loss:  1.26360759e-01, bound:  3.24119061e-01\n",
      "Epoch: 5605 mean train loss:  1.25972211e-01, bound:  3.24123323e-01\n",
      "Epoch: 5606 mean train loss:  1.25462726e-01, bound:  3.24113250e-01\n",
      "Epoch: 5607 mean train loss:  1.25152782e-01, bound:  3.24112147e-01\n",
      "Epoch: 5608 mean train loss:  1.25183269e-01, bound:  3.24109167e-01\n",
      "Epoch: 5609 mean train loss:  1.25393569e-01, bound:  3.24101210e-01\n",
      "Epoch: 5610 mean train loss:  1.25498265e-01, bound:  3.24102700e-01\n",
      "Epoch: 5611 mean train loss:  1.25344053e-01, bound:  3.24092478e-01\n",
      "Epoch: 5612 mean train loss:  1.25043511e-01, bound:  3.24092507e-01\n",
      "Epoch: 5613 mean train loss:  1.24850012e-01, bound:  3.24086249e-01\n",
      "Epoch: 5614 mean train loss:  1.24870218e-01, bound:  3.24080974e-01\n",
      "Epoch: 5615 mean train loss:  1.24974027e-01, bound:  3.24079901e-01\n",
      "Epoch: 5616 mean train loss:  1.24969028e-01, bound:  3.24071348e-01\n",
      "Epoch: 5617 mean train loss:  1.24812543e-01, bound:  3.24071139e-01\n",
      "Epoch: 5618 mean train loss:  1.24632828e-01, bound:  3.24064434e-01\n",
      "Epoch: 5619 mean train loss:  1.24563888e-01, bound:  3.24060559e-01\n",
      "Epoch: 5620 mean train loss:  1.24595910e-01, bound:  3.24057937e-01\n",
      "Epoch: 5621 mean train loss:  1.24610923e-01, bound:  3.24050665e-01\n",
      "Epoch: 5622 mean train loss:  1.24533825e-01, bound:  3.24049503e-01\n",
      "Epoch: 5623 mean train loss:  1.24403916e-01, bound:  3.24042767e-01\n",
      "Epoch: 5624 mean train loss:  1.24313660e-01, bound:  3.24039400e-01\n",
      "Epoch: 5625 mean train loss:  1.24295816e-01, bound:  3.24035913e-01\n",
      "Epoch: 5626 mean train loss:  1.24296144e-01, bound:  3.24029773e-01\n",
      "Epoch: 5627 mean train loss:  1.24252424e-01, bound:  3.24028015e-01\n",
      "Epoch: 5628 mean train loss:  1.24162413e-01, bound:  3.24021697e-01\n",
      "Epoch: 5629 mean train loss:  1.24076352e-01, bound:  3.24018657e-01\n",
      "Epoch: 5630 mean train loss:  1.24031149e-01, bound:  3.24014515e-01\n",
      "Epoch: 5631 mean train loss:  1.24011524e-01, bound:  3.24009210e-01\n",
      "Epoch: 5632 mean train loss:  1.23978920e-01, bound:  3.24006826e-01\n",
      "Epoch: 5633 mean train loss:  1.23915382e-01, bound:  3.24000865e-01\n",
      "Epoch: 5634 mean train loss:  1.23841129e-01, bound:  3.23997885e-01\n",
      "Epoch: 5635 mean train loss:  1.23783536e-01, bound:  3.23993355e-01\n",
      "Epoch: 5636 mean train loss:  1.23747431e-01, bound:  3.23988676e-01\n",
      "Epoch: 5637 mean train loss:  1.23714373e-01, bound:  3.23985666e-01\n",
      "Epoch: 5638 mean train loss:  1.23665638e-01, bound:  3.23980093e-01\n",
      "Epoch: 5639 mean train loss:  1.23603962e-01, bound:  3.23977143e-01\n",
      "Epoch: 5640 mean train loss:  1.23543940e-01, bound:  3.23972315e-01\n",
      "Epoch: 5641 mean train loss:  1.23496331e-01, bound:  3.23968083e-01\n",
      "Epoch: 5642 mean train loss:  1.23457342e-01, bound:  3.23964596e-01\n",
      "Epoch: 5643 mean train loss:  1.23415060e-01, bound:  3.23959470e-01\n",
      "Epoch: 5644 mean train loss:  1.23363383e-01, bound:  3.23956370e-01\n",
      "Epoch: 5645 mean train loss:  1.23306826e-01, bound:  3.23951423e-01\n",
      "Epoch: 5646 mean train loss:  1.23254307e-01, bound:  3.23947549e-01\n",
      "Epoch: 5647 mean train loss:  1.23208374e-01, bound:  3.23943496e-01\n",
      "Epoch: 5648 mean train loss:  1.23165213e-01, bound:  3.23938757e-01\n",
      "Epoch: 5649 mean train loss:  1.23119257e-01, bound:  3.23935330e-01\n",
      "Epoch: 5650 mean train loss:  1.23068810e-01, bound:  3.23930413e-01\n",
      "Epoch: 5651 mean train loss:  1.23015858e-01, bound:  3.23926866e-01\n",
      "Epoch: 5652 mean train loss:  1.22965842e-01, bound:  3.23922485e-01\n",
      "Epoch: 5653 mean train loss:  1.22919299e-01, bound:  3.23918194e-01\n",
      "Epoch: 5654 mean train loss:  1.22873709e-01, bound:  3.23914438e-01\n",
      "Epoch: 5655 mean train loss:  1.22826681e-01, bound:  3.23909760e-01\n",
      "Epoch: 5656 mean train loss:  1.22777089e-01, bound:  3.23906124e-01\n",
      "Epoch: 5657 mean train loss:  1.22726478e-01, bound:  3.23901564e-01\n",
      "Epoch: 5658 mean train loss:  1.22677311e-01, bound:  3.23897570e-01\n",
      "Epoch: 5659 mean train loss:  1.22629732e-01, bound:  3.23893517e-01\n",
      "Epoch: 5660 mean train loss:  1.22583278e-01, bound:  3.23889047e-01\n",
      "Epoch: 5661 mean train loss:  1.22535706e-01, bound:  3.23885322e-01\n",
      "Epoch: 5662 mean train loss:  1.22487418e-01, bound:  3.23880792e-01\n",
      "Epoch: 5663 mean train loss:  1.22438036e-01, bound:  3.23877007e-01\n",
      "Epoch: 5664 mean train loss:  1.22389086e-01, bound:  3.23872715e-01\n",
      "Epoch: 5665 mean train loss:  1.22340649e-01, bound:  3.23868543e-01\n",
      "Epoch: 5666 mean train loss:  1.22293353e-01, bound:  3.23864579e-01\n",
      "Epoch: 5667 mean train loss:  1.22246124e-01, bound:  3.23860228e-01\n",
      "Epoch: 5668 mean train loss:  1.22197933e-01, bound:  3.23856384e-01\n",
      "Epoch: 5669 mean train loss:  1.22149371e-01, bound:  3.23852003e-01\n",
      "Epoch: 5670 mean train loss:  1.22100711e-01, bound:  3.23848069e-01\n",
      "Epoch: 5671 mean train loss:  1.22052319e-01, bound:  3.23843896e-01\n",
      "Epoch: 5672 mean train loss:  1.22004397e-01, bound:  3.23839754e-01\n",
      "Epoch: 5673 mean train loss:  1.21956505e-01, bound:  3.23835820e-01\n",
      "Epoch: 5674 mean train loss:  1.21908702e-01, bound:  3.23831499e-01\n",
      "Epoch: 5675 mean train loss:  1.21860705e-01, bound:  3.23827595e-01\n",
      "Epoch: 5676 mean train loss:  1.21811964e-01, bound:  3.23823303e-01\n",
      "Epoch: 5677 mean train loss:  1.21763743e-01, bound:  3.23819369e-01\n",
      "Epoch: 5678 mean train loss:  1.21715508e-01, bound:  3.23815227e-01\n",
      "Epoch: 5679 mean train loss:  1.21667527e-01, bound:  3.23811054e-01\n",
      "Epoch: 5680 mean train loss:  1.21619686e-01, bound:  3.23807031e-01\n",
      "Epoch: 5681 mean train loss:  1.21571369e-01, bound:  3.23802859e-01\n",
      "Epoch: 5682 mean train loss:  1.21523388e-01, bound:  3.23798895e-01\n",
      "Epoch: 5683 mean train loss:  1.21474870e-01, bound:  3.23794693e-01\n",
      "Epoch: 5684 mean train loss:  1.21426597e-01, bound:  3.23790699e-01\n",
      "Epoch: 5685 mean train loss:  1.21378429e-01, bound:  3.23786557e-01\n",
      "Epoch: 5686 mean train loss:  1.21330038e-01, bound:  3.23782474e-01\n",
      "Epoch: 5687 mean train loss:  1.21282287e-01, bound:  3.23778421e-01\n",
      "Epoch: 5688 mean train loss:  1.21234313e-01, bound:  3.23774248e-01\n",
      "Epoch: 5689 mean train loss:  1.21186092e-01, bound:  3.23770255e-01\n",
      "Epoch: 5690 mean train loss:  1.21137656e-01, bound:  3.23766083e-01\n",
      "Epoch: 5691 mean train loss:  1.21089227e-01, bound:  3.23762119e-01\n",
      "Epoch: 5692 mean train loss:  1.21041082e-01, bound:  3.23758006e-01\n",
      "Epoch: 5693 mean train loss:  1.20992944e-01, bound:  3.23753923e-01\n",
      "Epoch: 5694 mean train loss:  1.20944150e-01, bound:  3.23749870e-01\n",
      "Epoch: 5695 mean train loss:  1.20896146e-01, bound:  3.23745787e-01\n",
      "Epoch: 5696 mean train loss:  1.20848276e-01, bound:  3.23741734e-01\n",
      "Epoch: 5697 mean train loss:  1.20799690e-01, bound:  3.23737562e-01\n",
      "Epoch: 5698 mean train loss:  1.20751373e-01, bound:  3.23733568e-01\n",
      "Epoch: 5699 mean train loss:  1.20703191e-01, bound:  3.23729485e-01\n",
      "Epoch: 5700 mean train loss:  1.20654836e-01, bound:  3.23725462e-01\n",
      "Epoch: 5701 mean train loss:  1.20606624e-01, bound:  3.23721319e-01\n",
      "Epoch: 5702 mean train loss:  1.20557800e-01, bound:  3.23717296e-01\n",
      "Epoch: 5703 mean train loss:  1.20509841e-01, bound:  3.23713243e-01\n",
      "Epoch: 5704 mean train loss:  1.20461337e-01, bound:  3.23709130e-01\n",
      "Epoch: 5705 mean train loss:  1.20413229e-01, bound:  3.23705137e-01\n",
      "Epoch: 5706 mean train loss:  1.20364614e-01, bound:  3.23700994e-01\n",
      "Epoch: 5707 mean train loss:  1.20316453e-01, bound:  3.23697001e-01\n",
      "Epoch: 5708 mean train loss:  1.20267771e-01, bound:  3.23692888e-01\n",
      "Epoch: 5709 mean train loss:  1.20219558e-01, bound:  3.23688895e-01\n",
      "Epoch: 5710 mean train loss:  1.20170735e-01, bound:  3.23684812e-01\n",
      "Epoch: 5711 mean train loss:  1.20122813e-01, bound:  3.23680758e-01\n",
      "Epoch: 5712 mean train loss:  1.20073974e-01, bound:  3.23676705e-01\n",
      "Epoch: 5713 mean train loss:  1.20025449e-01, bound:  3.23672622e-01\n",
      "Epoch: 5714 mean train loss:  1.19976848e-01, bound:  3.23668569e-01\n",
      "Epoch: 5715 mean train loss:  1.19928636e-01, bound:  3.23664546e-01\n",
      "Epoch: 5716 mean train loss:  1.19880229e-01, bound:  3.23660523e-01\n",
      "Epoch: 5717 mean train loss:  1.19831286e-01, bound:  3.23656380e-01\n",
      "Epoch: 5718 mean train loss:  1.19782977e-01, bound:  3.23652387e-01\n",
      "Epoch: 5719 mean train loss:  1.19734444e-01, bound:  3.23648304e-01\n",
      "Epoch: 5720 mean train loss:  1.19685531e-01, bound:  3.23644340e-01\n",
      "Epoch: 5721 mean train loss:  1.19637072e-01, bound:  3.23640198e-01\n",
      "Epoch: 5722 mean train loss:  1.19588368e-01, bound:  3.23636174e-01\n",
      "Epoch: 5723 mean train loss:  1.19539954e-01, bound:  3.23632151e-01\n",
      "Epoch: 5724 mean train loss:  1.19491138e-01, bound:  3.23628068e-01\n",
      "Epoch: 5725 mean train loss:  1.19442329e-01, bound:  3.23624074e-01\n",
      "Epoch: 5726 mean train loss:  1.19393684e-01, bound:  3.23619992e-01\n",
      "Epoch: 5727 mean train loss:  1.19344927e-01, bound:  3.23615968e-01\n",
      "Epoch: 5728 mean train loss:  1.19296819e-01, bound:  3.23611885e-01\n",
      "Epoch: 5729 mean train loss:  1.19247586e-01, bound:  3.23607922e-01\n",
      "Epoch: 5730 mean train loss:  1.19198561e-01, bound:  3.23603839e-01\n",
      "Epoch: 5731 mean train loss:  1.19149968e-01, bound:  3.23599845e-01\n",
      "Epoch: 5732 mean train loss:  1.19101025e-01, bound:  3.23595732e-01\n",
      "Epoch: 5733 mean train loss:  1.19052388e-01, bound:  3.23591739e-01\n",
      "Epoch: 5734 mean train loss:  1.19003586e-01, bound:  3.23587656e-01\n",
      "Epoch: 5735 mean train loss:  1.18954912e-01, bound:  3.23583663e-01\n",
      "Epoch: 5736 mean train loss:  1.18906096e-01, bound:  3.23579550e-01\n",
      "Epoch: 5737 mean train loss:  1.18856549e-01, bound:  3.23575616e-01\n",
      "Epoch: 5738 mean train loss:  1.18807890e-01, bound:  3.23571503e-01\n",
      "Epoch: 5739 mean train loss:  1.18758798e-01, bound:  3.23567510e-01\n",
      "Epoch: 5740 mean train loss:  1.18709937e-01, bound:  3.23563457e-01\n",
      "Epoch: 5741 mean train loss:  1.18660942e-01, bound:  3.23559463e-01\n",
      "Epoch: 5742 mean train loss:  1.18612289e-01, bound:  3.23555350e-01\n",
      "Epoch: 5743 mean train loss:  1.18563086e-01, bound:  3.23551416e-01\n",
      "Epoch: 5744 mean train loss:  1.18514381e-01, bound:  3.23547304e-01\n",
      "Epoch: 5745 mean train loss:  1.18465088e-01, bound:  3.23543400e-01\n",
      "Epoch: 5746 mean train loss:  1.18416116e-01, bound:  3.23539227e-01\n",
      "Epoch: 5747 mean train loss:  1.18367419e-01, bound:  3.23535353e-01\n",
      "Epoch: 5748 mean train loss:  1.18317604e-01, bound:  3.23531151e-01\n",
      "Epoch: 5749 mean train loss:  1.18269093e-01, bound:  3.23527306e-01\n",
      "Epoch: 5750 mean train loss:  1.18219621e-01, bound:  3.23523074e-01\n",
      "Epoch: 5751 mean train loss:  1.18170612e-01, bound:  3.23519289e-01\n",
      "Epoch: 5752 mean train loss:  1.18122041e-01, bound:  3.23514938e-01\n",
      "Epoch: 5753 mean train loss:  1.18073992e-01, bound:  3.23511273e-01\n",
      "Epoch: 5754 mean train loss:  1.18025474e-01, bound:  3.23506832e-01\n",
      "Epoch: 5755 mean train loss:  1.17977798e-01, bound:  3.23503375e-01\n",
      "Epoch: 5756 mean train loss:  1.17931046e-01, bound:  3.23498696e-01\n",
      "Epoch: 5757 mean train loss:  1.17885627e-01, bound:  3.23495448e-01\n",
      "Epoch: 5758 mean train loss:  1.17842540e-01, bound:  3.23490471e-01\n",
      "Epoch: 5759 mean train loss:  1.17803521e-01, bound:  3.23487669e-01\n",
      "Epoch: 5760 mean train loss:  1.17770694e-01, bound:  3.23482096e-01\n",
      "Epoch: 5761 mean train loss:  1.17747143e-01, bound:  3.23480040e-01\n",
      "Epoch: 5762 mean train loss:  1.17740124e-01, bound:  3.23473603e-01\n",
      "Epoch: 5763 mean train loss:  1.17756315e-01, bound:  3.23472649e-01\n",
      "Epoch: 5764 mean train loss:  1.17808446e-01, bound:  3.23464811e-01\n",
      "Epoch: 5765 mean train loss:  1.17906392e-01, bound:  3.23465556e-01\n",
      "Epoch: 5766 mean train loss:  1.18051499e-01, bound:  3.23455811e-01\n",
      "Epoch: 5767 mean train loss:  1.18211523e-01, bound:  3.23458642e-01\n",
      "Epoch: 5768 mean train loss:  1.18309438e-01, bound:  3.23447198e-01\n",
      "Epoch: 5769 mean train loss:  1.18224926e-01, bound:  3.23451132e-01\n",
      "Epoch: 5770 mean train loss:  1.17910713e-01, bound:  3.23440343e-01\n",
      "Epoch: 5771 mean train loss:  1.17481641e-01, bound:  3.23442072e-01\n",
      "Epoch: 5772 mean train loss:  1.17175058e-01, bound:  3.23435724e-01\n",
      "Epoch: 5773 mean train loss:  1.17128119e-01, bound:  3.23432207e-01\n",
      "Epoch: 5774 mean train loss:  1.17268592e-01, bound:  3.23431164e-01\n",
      "Epoch: 5775 mean train loss:  1.17399074e-01, bound:  3.23423058e-01\n",
      "Epoch: 5776 mean train loss:  1.17359415e-01, bound:  3.23424101e-01\n",
      "Epoch: 5777 mean train loss:  1.17148057e-01, bound:  3.23415667e-01\n",
      "Epoch: 5778 mean train loss:  1.16915740e-01, bound:  3.23414534e-01\n",
      "Epoch: 5779 mean train loss:  1.16814762e-01, bound:  3.23409885e-01\n",
      "Epoch: 5780 mean train loss:  1.16853967e-01, bound:  3.23404700e-01\n",
      "Epoch: 5781 mean train loss:  1.16916262e-01, bound:  3.23403716e-01\n",
      "Epoch: 5782 mean train loss:  1.16885558e-01, bound:  3.23396415e-01\n",
      "Epoch: 5783 mean train loss:  1.16751231e-01, bound:  3.23395789e-01\n",
      "Epoch: 5784 mean train loss:  1.16604514e-01, bound:  3.23389947e-01\n",
      "Epoch: 5785 mean train loss:  1.16531931e-01, bound:  3.23386550e-01\n",
      "Epoch: 5786 mean train loss:  1.16536118e-01, bound:  3.23383719e-01\n",
      "Epoch: 5787 mean train loss:  1.16547018e-01, bound:  3.23377609e-01\n",
      "Epoch: 5788 mean train loss:  1.16501912e-01, bound:  3.23376238e-01\n",
      "Epoch: 5789 mean train loss:  1.16403200e-01, bound:  3.23370069e-01\n",
      "Epoch: 5790 mean train loss:  1.16304651e-01, bound:  3.23367625e-01\n",
      "Epoch: 5791 mean train loss:  1.16250016e-01, bound:  3.23363632e-01\n",
      "Epoch: 5792 mean train loss:  1.16234094e-01, bound:  3.23358953e-01\n",
      "Epoch: 5793 mean train loss:  1.16216652e-01, bound:  3.23356837e-01\n",
      "Epoch: 5794 mean train loss:  1.16166674e-01, bound:  3.23351085e-01\n",
      "Epoch: 5795 mean train loss:  1.16089851e-01, bound:  3.23348910e-01\n",
      "Epoch: 5796 mean train loss:  1.16015866e-01, bound:  3.23344171e-01\n",
      "Epoch: 5797 mean train loss:  1.15966417e-01, bound:  3.23340476e-01\n",
      "Epoch: 5798 mean train loss:  1.15936488e-01, bound:  3.23337376e-01\n",
      "Epoch: 5799 mean train loss:  1.15904741e-01, bound:  3.23332280e-01\n",
      "Epoch: 5800 mean train loss:  1.15855590e-01, bound:  3.23329896e-01\n",
      "Epoch: 5801 mean train loss:  1.15792371e-01, bound:  3.23324919e-01\n",
      "Epoch: 5802 mean train loss:  1.15730867e-01, bound:  3.23321849e-01\n",
      "Epoch: 5803 mean train loss:  1.15681365e-01, bound:  3.23317975e-01\n",
      "Epoch: 5804 mean train loss:  1.15642637e-01, bound:  3.23313683e-01\n",
      "Epoch: 5805 mean train loss:  1.15604334e-01, bound:  3.23310763e-01\n",
      "Epoch: 5806 mean train loss:  1.15557425e-01, bound:  3.23305935e-01\n",
      "Epoch: 5807 mean train loss:  1.15502648e-01, bound:  3.23303074e-01\n",
      "Epoch: 5808 mean train loss:  1.15446813e-01, bound:  3.23298633e-01\n",
      "Epoch: 5809 mean train loss:  1.15396008e-01, bound:  3.23295057e-01\n",
      "Epoch: 5810 mean train loss:  1.15351796e-01, bound:  3.23291481e-01\n",
      "Epoch: 5811 mean train loss:  1.15309648e-01, bound:  3.23287070e-01\n",
      "Epoch: 5812 mean train loss:  1.15264244e-01, bound:  3.23284000e-01\n",
      "Epoch: 5813 mean train loss:  1.15214378e-01, bound:  3.23279440e-01\n",
      "Epoch: 5814 mean train loss:  1.15161806e-01, bound:  3.23276162e-01\n",
      "Epoch: 5815 mean train loss:  1.15111373e-01, bound:  3.23272079e-01\n",
      "Epoch: 5816 mean train loss:  1.15063749e-01, bound:  3.23268324e-01\n",
      "Epoch: 5817 mean train loss:  1.15018331e-01, bound:  3.23264807e-01\n",
      "Epoch: 5818 mean train loss:  1.14973202e-01, bound:  3.23260516e-01\n",
      "Epoch: 5819 mean train loss:  1.14925750e-01, bound:  3.23257208e-01\n",
      "Epoch: 5820 mean train loss:  1.14876300e-01, bound:  3.23252887e-01\n",
      "Epoch: 5821 mean train loss:  1.14826471e-01, bound:  3.23249459e-01\n",
      "Epoch: 5822 mean train loss:  1.14777490e-01, bound:  3.23245466e-01\n",
      "Epoch: 5823 mean train loss:  1.14730015e-01, bound:  3.23241621e-01\n",
      "Epoch: 5824 mean train loss:  1.14683144e-01, bound:  3.23238045e-01\n",
      "Epoch: 5825 mean train loss:  1.14636421e-01, bound:  3.23233873e-01\n",
      "Epoch: 5826 mean train loss:  1.14589073e-01, bound:  3.23230505e-01\n",
      "Epoch: 5827 mean train loss:  1.14540078e-01, bound:  3.23226362e-01\n",
      "Epoch: 5828 mean train loss:  1.14491493e-01, bound:  3.23222876e-01\n",
      "Epoch: 5829 mean train loss:  1.14442676e-01, bound:  3.23218912e-01\n",
      "Epoch: 5830 mean train loss:  1.14394814e-01, bound:  3.23215127e-01\n",
      "Epoch: 5831 mean train loss:  1.14347287e-01, bound:  3.23211432e-01\n",
      "Epoch: 5832 mean train loss:  1.14299737e-01, bound:  3.23207468e-01\n",
      "Epoch: 5833 mean train loss:  1.14252351e-01, bound:  3.23203951e-01\n",
      "Epoch: 5834 mean train loss:  1.14204042e-01, bound:  3.23199868e-01\n",
      "Epoch: 5835 mean train loss:  1.14155799e-01, bound:  3.23196352e-01\n",
      "Epoch: 5836 mean train loss:  1.14107184e-01, bound:  3.23192388e-01\n",
      "Epoch: 5837 mean train loss:  1.14058986e-01, bound:  3.23188722e-01\n",
      "Epoch: 5838 mean train loss:  1.14010975e-01, bound:  3.23184937e-01\n",
      "Epoch: 5839 mean train loss:  1.13962978e-01, bound:  3.23181063e-01\n",
      "Epoch: 5840 mean train loss:  1.13915056e-01, bound:  3.23177457e-01\n",
      "Epoch: 5841 mean train loss:  1.13867238e-01, bound:  3.23173493e-01\n",
      "Epoch: 5842 mean train loss:  1.13819078e-01, bound:  3.23169917e-01\n",
      "Epoch: 5843 mean train loss:  1.13770612e-01, bound:  3.23165953e-01\n",
      "Epoch: 5844 mean train loss:  1.13722824e-01, bound:  3.23162317e-01\n",
      "Epoch: 5845 mean train loss:  1.13674194e-01, bound:  3.23158443e-01\n",
      "Epoch: 5846 mean train loss:  1.13625824e-01, bound:  3.23154718e-01\n",
      "Epoch: 5847 mean train loss:  1.13577835e-01, bound:  3.23150992e-01\n",
      "Epoch: 5848 mean train loss:  1.13529630e-01, bound:  3.23147148e-01\n",
      "Epoch: 5849 mean train loss:  1.13481350e-01, bound:  3.23143482e-01\n",
      "Epoch: 5850 mean train loss:  1.13433227e-01, bound:  3.23139578e-01\n",
      "Epoch: 5851 mean train loss:  1.13384955e-01, bound:  3.23135942e-01\n",
      "Epoch: 5852 mean train loss:  1.13336749e-01, bound:  3.23132038e-01\n",
      "Epoch: 5853 mean train loss:  1.13288596e-01, bound:  3.23128372e-01\n",
      "Epoch: 5854 mean train loss:  1.13239907e-01, bound:  3.23124528e-01\n",
      "Epoch: 5855 mean train loss:  1.13191530e-01, bound:  3.23120832e-01\n",
      "Epoch: 5856 mean train loss:  1.13143265e-01, bound:  3.23117018e-01\n",
      "Epoch: 5857 mean train loss:  1.13095075e-01, bound:  3.23113292e-01\n",
      "Epoch: 5858 mean train loss:  1.13046445e-01, bound:  3.23109508e-01\n",
      "Epoch: 5859 mean train loss:  1.12998202e-01, bound:  3.23105693e-01\n",
      "Epoch: 5860 mean train loss:  1.12949595e-01, bound:  3.23101997e-01\n",
      "Epoch: 5861 mean train loss:  1.12901531e-01, bound:  3.23098153e-01\n",
      "Epoch: 5862 mean train loss:  1.12853572e-01, bound:  3.23094457e-01\n",
      "Epoch: 5863 mean train loss:  1.12804756e-01, bound:  3.23090613e-01\n",
      "Epoch: 5864 mean train loss:  1.12756737e-01, bound:  3.23086917e-01\n",
      "Epoch: 5865 mean train loss:  1.12707898e-01, bound:  3.23083103e-01\n",
      "Epoch: 5866 mean train loss:  1.12659506e-01, bound:  3.23079377e-01\n",
      "Epoch: 5867 mean train loss:  1.12611264e-01, bound:  3.23075563e-01\n",
      "Epoch: 5868 mean train loss:  1.12562574e-01, bound:  3.23071867e-01\n",
      "Epoch: 5869 mean train loss:  1.12514220e-01, bound:  3.23068053e-01\n",
      "Epoch: 5870 mean train loss:  1.12465873e-01, bound:  3.23064327e-01\n",
      "Epoch: 5871 mean train loss:  1.12417243e-01, bound:  3.23060572e-01\n",
      "Epoch: 5872 mean train loss:  1.12368509e-01, bound:  3.23056787e-01\n",
      "Epoch: 5873 mean train loss:  1.12320282e-01, bound:  3.23053002e-01\n",
      "Epoch: 5874 mean train loss:  1.12271897e-01, bound:  3.23049277e-01\n",
      "Epoch: 5875 mean train loss:  1.12223260e-01, bound:  3.23045552e-01\n",
      "Epoch: 5876 mean train loss:  1.12174556e-01, bound:  3.23041737e-01\n",
      "Epoch: 5877 mean train loss:  1.12126179e-01, bound:  3.23038012e-01\n",
      "Epoch: 5878 mean train loss:  1.12077639e-01, bound:  3.23034197e-01\n",
      "Epoch: 5879 mean train loss:  1.12028979e-01, bound:  3.23030472e-01\n",
      "Epoch: 5880 mean train loss:  1.11980632e-01, bound:  3.23026657e-01\n",
      "Epoch: 5881 mean train loss:  1.11931764e-01, bound:  3.23022932e-01\n",
      "Epoch: 5882 mean train loss:  1.11883454e-01, bound:  3.23019117e-01\n",
      "Epoch: 5883 mean train loss:  1.11834772e-01, bound:  3.23015451e-01\n",
      "Epoch: 5884 mean train loss:  1.11786582e-01, bound:  3.23011637e-01\n",
      "Epoch: 5885 mean train loss:  1.11737959e-01, bound:  3.23007911e-01\n",
      "Epoch: 5886 mean train loss:  1.11689299e-01, bound:  3.23004097e-01\n",
      "Epoch: 5887 mean train loss:  1.11640863e-01, bound:  3.23000431e-01\n",
      "Epoch: 5888 mean train loss:  1.11592032e-01, bound:  3.22996557e-01\n",
      "Epoch: 5889 mean train loss:  1.11543469e-01, bound:  3.22992891e-01\n",
      "Epoch: 5890 mean train loss:  1.11495003e-01, bound:  3.22989017e-01\n",
      "Epoch: 5891 mean train loss:  1.11446403e-01, bound:  3.22985351e-01\n",
      "Epoch: 5892 mean train loss:  1.11398369e-01, bound:  3.22981477e-01\n",
      "Epoch: 5893 mean train loss:  1.11349680e-01, bound:  3.22977871e-01\n",
      "Epoch: 5894 mean train loss:  1.11301154e-01, bound:  3.22973907e-01\n",
      "Epoch: 5895 mean train loss:  1.11253090e-01, bound:  3.22970361e-01\n",
      "Epoch: 5896 mean train loss:  1.11204825e-01, bound:  3.22966337e-01\n",
      "Epoch: 5897 mean train loss:  1.11156754e-01, bound:  3.22962880e-01\n",
      "Epoch: 5898 mean train loss:  1.11109130e-01, bound:  3.22958767e-01\n",
      "Epoch: 5899 mean train loss:  1.11062035e-01, bound:  3.22955459e-01\n",
      "Epoch: 5900 mean train loss:  1.11015968e-01, bound:  3.22951138e-01\n",
      "Epoch: 5901 mean train loss:  1.10971585e-01, bound:  3.22948068e-01\n",
      "Epoch: 5902 mean train loss:  1.10929556e-01, bound:  3.22943389e-01\n",
      "Epoch: 5903 mean train loss:  1.10891484e-01, bound:  3.22940767e-01\n",
      "Epoch: 5904 mean train loss:  1.10860467e-01, bound:  3.22935581e-01\n",
      "Epoch: 5905 mean train loss:  1.10841118e-01, bound:  3.22933584e-01\n",
      "Epoch: 5906 mean train loss:  1.10840477e-01, bound:  3.22927594e-01\n",
      "Epoch: 5907 mean train loss:  1.10869713e-01, bound:  3.22926700e-01\n",
      "Epoch: 5908 mean train loss:  1.10944696e-01, bound:  3.22919279e-01\n",
      "Epoch: 5909 mean train loss:  1.11080110e-01, bound:  3.22920173e-01\n",
      "Epoch: 5910 mean train loss:  1.11279942e-01, bound:  3.22910726e-01\n",
      "Epoch: 5911 mean train loss:  1.11499347e-01, bound:  3.22913826e-01\n",
      "Epoch: 5912 mean train loss:  1.11626707e-01, bound:  3.22902620e-01\n",
      "Epoch: 5913 mean train loss:  1.11493409e-01, bound:  3.22906673e-01\n",
      "Epoch: 5914 mean train loss:  1.11060537e-01, bound:  3.22896510e-01\n",
      "Epoch: 5915 mean train loss:  1.10536717e-01, bound:  3.22897911e-01\n",
      "Epoch: 5916 mean train loss:  1.10248789e-01, bound:  3.22892755e-01\n",
      "Epoch: 5917 mean train loss:  1.10310845e-01, bound:  3.22888583e-01\n",
      "Epoch: 5918 mean train loss:  1.10538378e-01, bound:  3.22888613e-01\n",
      "Epoch: 5919 mean train loss:  1.10644102e-01, bound:  3.22880298e-01\n",
      "Epoch: 5920 mean train loss:  1.10481597e-01, bound:  3.22881520e-01\n",
      "Epoch: 5921 mean train loss:  1.10173158e-01, bound:  3.22874129e-01\n",
      "Epoch: 5922 mean train loss:  1.09973133e-01, bound:  3.22872013e-01\n",
      "Epoch: 5923 mean train loss:  1.09992839e-01, bound:  3.22869062e-01\n",
      "Epoch: 5924 mean train loss:  1.10106058e-01, bound:  3.22862893e-01\n",
      "Epoch: 5925 mean train loss:  1.10118866e-01, bound:  3.22862864e-01\n",
      "Epoch: 5926 mean train loss:  1.09972693e-01, bound:  3.22855949e-01\n",
      "Epoch: 5927 mean train loss:  1.09785222e-01, bound:  3.22854638e-01\n",
      "Epoch: 5928 mean train loss:  1.09699920e-01, bound:  3.22850585e-01\n",
      "Epoch: 5929 mean train loss:  1.09727763e-01, bound:  3.22845846e-01\n",
      "Epoch: 5930 mean train loss:  1.09760113e-01, bound:  3.22844535e-01\n",
      "Epoch: 5931 mean train loss:  1.09705225e-01, bound:  3.22838128e-01\n",
      "Epoch: 5932 mean train loss:  1.09580301e-01, bound:  3.22836637e-01\n",
      "Epoch: 5933 mean train loss:  1.09474838e-01, bound:  3.22831899e-01\n",
      "Epoch: 5934 mean train loss:  1.09442078e-01, bound:  3.22828054e-01\n",
      "Epoch: 5935 mean train loss:  1.09448940e-01, bound:  3.22825879e-01\n",
      "Epoch: 5936 mean train loss:  1.09427556e-01, bound:  3.22820365e-01\n",
      "Epoch: 5937 mean train loss:  1.09352946e-01, bound:  3.22818756e-01\n",
      "Epoch: 5938 mean train loss:  1.09261498e-01, bound:  3.22813898e-01\n",
      "Epoch: 5939 mean train loss:  1.09200120e-01, bound:  3.22810709e-01\n",
      "Epoch: 5940 mean train loss:  1.09176666e-01, bound:  3.22807789e-01\n",
      "Epoch: 5941 mean train loss:  1.09157838e-01, bound:  3.22802991e-01\n",
      "Epoch: 5942 mean train loss:  1.09113060e-01, bound:  3.22800905e-01\n",
      "Epoch: 5943 mean train loss:  1.09044112e-01, bound:  3.22796047e-01\n",
      "Epoch: 5944 mean train loss:  1.08977303e-01, bound:  3.22793216e-01\n",
      "Epoch: 5945 mean train loss:  1.08932346e-01, bound:  3.22789669e-01\n",
      "Epoch: 5946 mean train loss:  1.08902864e-01, bound:  3.22785467e-01\n",
      "Epoch: 5947 mean train loss:  1.08868875e-01, bound:  3.22783023e-01\n",
      "Epoch: 5948 mean train loss:  1.08818978e-01, bound:  3.22778374e-01\n",
      "Epoch: 5949 mean train loss:  1.08759165e-01, bound:  3.22775751e-01\n",
      "Epoch: 5950 mean train loss:  1.08705163e-01, bound:  3.22771758e-01\n",
      "Epoch: 5951 mean train loss:  1.08662762e-01, bound:  3.22768152e-01\n",
      "Epoch: 5952 mean train loss:  1.08626492e-01, bound:  3.22765201e-01\n",
      "Epoch: 5953 mean train loss:  1.08586162e-01, bound:  3.22760820e-01\n",
      "Epoch: 5954 mean train loss:  1.08537532e-01, bound:  3.22758108e-01\n",
      "Epoch: 5955 mean train loss:  1.08484566e-01, bound:  3.22753847e-01\n",
      "Epoch: 5956 mean train loss:  1.08434938e-01, bound:  3.22750539e-01\n",
      "Epoch: 5957 mean train loss:  1.08391695e-01, bound:  3.22747111e-01\n",
      "Epoch: 5958 mean train loss:  1.08351529e-01, bound:  3.22743177e-01\n",
      "Epoch: 5959 mean train loss:  1.08309403e-01, bound:  3.22740227e-01\n",
      "Epoch: 5960 mean train loss:  1.08262666e-01, bound:  3.22736025e-01\n",
      "Epoch: 5961 mean train loss:  1.08213834e-01, bound:  3.22732985e-01\n",
      "Epoch: 5962 mean train loss:  1.08166106e-01, bound:  3.22729170e-01\n",
      "Epoch: 5963 mean train loss:  1.08121634e-01, bound:  3.22725564e-01\n",
      "Epoch: 5964 mean train loss:  1.08078778e-01, bound:  3.22722286e-01\n",
      "Epoch: 5965 mean train loss:  1.08035900e-01, bound:  3.22718263e-01\n",
      "Epoch: 5966 mean train loss:  1.07990742e-01, bound:  3.22715133e-01\n",
      "Epoch: 5967 mean train loss:  1.07943915e-01, bound:  3.22711200e-01\n",
      "Epoch: 5968 mean train loss:  1.07897386e-01, bound:  3.22707862e-01\n",
      "Epoch: 5969 mean train loss:  1.07851736e-01, bound:  3.22704285e-01\n",
      "Epoch: 5970 mean train loss:  1.07808061e-01, bound:  3.22700620e-01\n",
      "Epoch: 5971 mean train loss:  1.07764475e-01, bound:  3.22697282e-01\n",
      "Epoch: 5972 mean train loss:  1.07720256e-01, bound:  3.22693437e-01\n",
      "Epoch: 5973 mean train loss:  1.07674927e-01, bound:  3.22690189e-01\n",
      "Epoch: 5974 mean train loss:  1.07629031e-01, bound:  3.22686404e-01\n",
      "Epoch: 5975 mean train loss:  1.07583478e-01, bound:  3.22682977e-01\n",
      "Epoch: 5976 mean train loss:  1.07538499e-01, bound:  3.22679460e-01\n",
      "Epoch: 5977 mean train loss:  1.07494578e-01, bound:  3.22675794e-01\n",
      "Epoch: 5978 mean train loss:  1.07450396e-01, bound:  3.22672457e-01\n",
      "Epoch: 5979 mean train loss:  1.07405998e-01, bound:  3.22668642e-01\n",
      "Epoch: 5980 mean train loss:  1.07361004e-01, bound:  3.22665364e-01\n",
      "Epoch: 5981 mean train loss:  1.07315935e-01, bound:  3.22661608e-01\n",
      "Epoch: 5982 mean train loss:  1.07270762e-01, bound:  3.22658211e-01\n",
      "Epoch: 5983 mean train loss:  1.07226193e-01, bound:  3.22654665e-01\n",
      "Epoch: 5984 mean train loss:  1.07181527e-01, bound:  3.22651088e-01\n",
      "Epoch: 5985 mean train loss:  1.07137032e-01, bound:  3.22647601e-01\n",
      "Epoch: 5986 mean train loss:  1.07092999e-01, bound:  3.22643906e-01\n",
      "Epoch: 5987 mean train loss:  1.07048169e-01, bound:  3.22640568e-01\n",
      "Epoch: 5988 mean train loss:  1.07003272e-01, bound:  3.22636873e-01\n",
      "Epoch: 5989 mean train loss:  1.06958613e-01, bound:  3.22633475e-01\n",
      "Epoch: 5990 mean train loss:  1.06913596e-01, bound:  3.22629869e-01\n",
      "Epoch: 5991 mean train loss:  1.06869303e-01, bound:  3.22626352e-01\n",
      "Epoch: 5992 mean train loss:  1.06824450e-01, bound:  3.22622865e-01\n",
      "Epoch: 5993 mean train loss:  1.06780238e-01, bound:  3.22619200e-01\n",
      "Epoch: 5994 mean train loss:  1.06735870e-01, bound:  3.22615772e-01\n",
      "Epoch: 5995 mean train loss:  1.06691301e-01, bound:  3.22612166e-01\n",
      "Epoch: 5996 mean train loss:  1.06646702e-01, bound:  3.22608739e-01\n",
      "Epoch: 5997 mean train loss:  1.06602155e-01, bound:  3.22605073e-01\n",
      "Epoch: 5998 mean train loss:  1.06557399e-01, bound:  3.22601616e-01\n",
      "Epoch: 5999 mean train loss:  1.06512882e-01, bound:  3.22598040e-01\n",
      "Epoch: 6000 mean train loss:  1.06468670e-01, bound:  3.22594523e-01\n",
      "Epoch: 6001 mean train loss:  1.06424280e-01, bound:  3.22591007e-01\n",
      "Epoch: 6002 mean train loss:  1.06379665e-01, bound:  3.22587430e-01\n",
      "Epoch: 6003 mean train loss:  1.06335483e-01, bound:  3.22583973e-01\n",
      "Epoch: 6004 mean train loss:  1.06290974e-01, bound:  3.22580367e-01\n",
      "Epoch: 6005 mean train loss:  1.06246427e-01, bound:  3.22576880e-01\n",
      "Epoch: 6006 mean train loss:  1.06202148e-01, bound:  3.22573274e-01\n",
      "Epoch: 6007 mean train loss:  1.06158055e-01, bound:  3.22569788e-01\n",
      "Epoch: 6008 mean train loss:  1.06113352e-01, bound:  3.22566241e-01\n",
      "Epoch: 6009 mean train loss:  1.06068820e-01, bound:  3.22562724e-01\n",
      "Epoch: 6010 mean train loss:  1.06024586e-01, bound:  3.22559148e-01\n",
      "Epoch: 6011 mean train loss:  1.05980247e-01, bound:  3.22555631e-01\n",
      "Epoch: 6012 mean train loss:  1.05936185e-01, bound:  3.22552115e-01\n",
      "Epoch: 6013 mean train loss:  1.05891563e-01, bound:  3.22548568e-01\n",
      "Epoch: 6014 mean train loss:  1.05847560e-01, bound:  3.22545052e-01\n",
      "Epoch: 6015 mean train loss:  1.05802938e-01, bound:  3.22541475e-01\n",
      "Epoch: 6016 mean train loss:  1.05758943e-01, bound:  3.22538018e-01\n",
      "Epoch: 6017 mean train loss:  1.05714440e-01, bound:  3.22534412e-01\n",
      "Epoch: 6018 mean train loss:  1.05670482e-01, bound:  3.22530925e-01\n",
      "Epoch: 6019 mean train loss:  1.05626069e-01, bound:  3.22527379e-01\n",
      "Epoch: 6020 mean train loss:  1.05581805e-01, bound:  3.22523832e-01\n",
      "Epoch: 6021 mean train loss:  1.05537765e-01, bound:  3.22520286e-01\n",
      "Epoch: 6022 mean train loss:  1.05493456e-01, bound:  3.22516769e-01\n",
      "Epoch: 6023 mean train loss:  1.05449148e-01, bound:  3.22513223e-01\n",
      "Epoch: 6024 mean train loss:  1.05404764e-01, bound:  3.22509676e-01\n",
      "Epoch: 6025 mean train loss:  1.05361044e-01, bound:  3.22506189e-01\n",
      "Epoch: 6026 mean train loss:  1.05316475e-01, bound:  3.22502583e-01\n",
      "Epoch: 6027 mean train loss:  1.05272524e-01, bound:  3.22499067e-01\n",
      "Epoch: 6028 mean train loss:  1.05228312e-01, bound:  3.22495520e-01\n",
      "Epoch: 6029 mean train loss:  1.05184101e-01, bound:  3.22492003e-01\n",
      "Epoch: 6030 mean train loss:  1.05140045e-01, bound:  3.22488487e-01\n",
      "Epoch: 6031 mean train loss:  1.05096146e-01, bound:  3.22484970e-01\n",
      "Epoch: 6032 mean train loss:  1.05051883e-01, bound:  3.22481364e-01\n",
      "Epoch: 6033 mean train loss:  1.05007879e-01, bound:  3.22477907e-01\n",
      "Epoch: 6034 mean train loss:  1.04963638e-01, bound:  3.22474331e-01\n",
      "Epoch: 6035 mean train loss:  1.04919665e-01, bound:  3.22470814e-01\n",
      "Epoch: 6036 mean train loss:  1.04875728e-01, bound:  3.22467268e-01\n",
      "Epoch: 6037 mean train loss:  1.04831688e-01, bound:  3.22463751e-01\n",
      "Epoch: 6038 mean train loss:  1.04787409e-01, bound:  3.22460204e-01\n",
      "Epoch: 6039 mean train loss:  1.04743458e-01, bound:  3.22456688e-01\n",
      "Epoch: 6040 mean train loss:  1.04699440e-01, bound:  3.22453141e-01\n",
      "Epoch: 6041 mean train loss:  1.04655907e-01, bound:  3.22449654e-01\n",
      "Epoch: 6042 mean train loss:  1.04611531e-01, bound:  3.22446048e-01\n",
      "Epoch: 6043 mean train loss:  1.04567595e-01, bound:  3.22442561e-01\n",
      "Epoch: 6044 mean train loss:  1.04523785e-01, bound:  3.22438985e-01\n",
      "Epoch: 6045 mean train loss:  1.04479730e-01, bound:  3.22435498e-01\n",
      "Epoch: 6046 mean train loss:  1.04435921e-01, bound:  3.22431892e-01\n",
      "Epoch: 6047 mean train loss:  1.04392238e-01, bound:  3.22428465e-01\n",
      "Epoch: 6048 mean train loss:  1.04348235e-01, bound:  3.22424829e-01\n",
      "Epoch: 6049 mean train loss:  1.04304478e-01, bound:  3.22421402e-01\n",
      "Epoch: 6050 mean train loss:  1.04260847e-01, bound:  3.22417706e-01\n",
      "Epoch: 6051 mean train loss:  1.04217209e-01, bound:  3.22414368e-01\n",
      "Epoch: 6052 mean train loss:  1.04173586e-01, bound:  3.22410583e-01\n",
      "Epoch: 6053 mean train loss:  1.04130268e-01, bound:  3.22407305e-01\n",
      "Epoch: 6054 mean train loss:  1.04087502e-01, bound:  3.22403491e-01\n",
      "Epoch: 6055 mean train loss:  1.04044944e-01, bound:  3.22400331e-01\n",
      "Epoch: 6056 mean train loss:  1.04003787e-01, bound:  3.22396338e-01\n",
      "Epoch: 6057 mean train loss:  1.03963710e-01, bound:  3.22393358e-01\n",
      "Epoch: 6058 mean train loss:  1.03926413e-01, bound:  3.22389096e-01\n",
      "Epoch: 6059 mean train loss:  1.03892766e-01, bound:  3.22386533e-01\n",
      "Epoch: 6060 mean train loss:  1.03866585e-01, bound:  3.22381765e-01\n",
      "Epoch: 6061 mean train loss:  1.03852153e-01, bound:  3.22379798e-01\n",
      "Epoch: 6062 mean train loss:  1.03858039e-01, bound:  3.22374195e-01\n",
      "Epoch: 6063 mean train loss:  1.03896596e-01, bound:  3.22373301e-01\n",
      "Epoch: 6064 mean train loss:  1.03986263e-01, bound:  3.22366416e-01\n",
      "Epoch: 6065 mean train loss:  1.04147032e-01, bound:  3.22367191e-01\n",
      "Epoch: 6066 mean train loss:  1.04387149e-01, bound:  3.22358340e-01\n",
      "Epoch: 6067 mean train loss:  1.04662232e-01, bound:  3.22361290e-01\n",
      "Epoch: 6068 mean train loss:  1.04843609e-01, bound:  3.22350591e-01\n",
      "Epoch: 6069 mean train loss:  1.04720689e-01, bound:  3.22354645e-01\n",
      "Epoch: 6070 mean train loss:  1.04231827e-01, bound:  3.22344840e-01\n",
      "Epoch: 6071 mean train loss:  1.03626505e-01, bound:  3.22346389e-01\n",
      "Epoch: 6072 mean train loss:  1.03313237e-01, bound:  3.22341621e-01\n",
      "Epoch: 6073 mean train loss:  1.03423290e-01, bound:  3.22337568e-01\n",
      "Epoch: 6074 mean train loss:  1.03710495e-01, bound:  3.22337955e-01\n",
      "Epoch: 6075 mean train loss:  1.03816204e-01, bound:  3.22329968e-01\n",
      "Epoch: 6076 mean train loss:  1.03590325e-01, bound:  3.22331220e-01\n",
      "Epoch: 6077 mean train loss:  1.03235312e-01, bound:  3.22324485e-01\n",
      "Epoch: 6078 mean train loss:  1.03065491e-01, bound:  3.22322100e-01\n",
      "Epoch: 6079 mean train loss:  1.03155263e-01, bound:  3.22319895e-01\n",
      "Epoch: 6080 mean train loss:  1.03295140e-01, bound:  3.22313607e-01\n",
      "Epoch: 6081 mean train loss:  1.03260331e-01, bound:  3.22313845e-01\n",
      "Epoch: 6082 mean train loss:  1.03054881e-01, bound:  3.22307497e-01\n",
      "Epoch: 6083 mean train loss:  1.02874406e-01, bound:  3.22305828e-01\n",
      "Epoch: 6084 mean train loss:  1.02853991e-01, bound:  3.22302818e-01\n",
      "Epoch: 6085 mean train loss:  1.02927506e-01, bound:  3.22297752e-01\n",
      "Epoch: 6086 mean train loss:  1.02937073e-01, bound:  3.22296947e-01\n",
      "Epoch: 6087 mean train loss:  1.02826595e-01, bound:  3.22290957e-01\n",
      "Epoch: 6088 mean train loss:  1.02686301e-01, bound:  3.22289139e-01\n",
      "Epoch: 6089 mean train loss:  1.02625169e-01, bound:  3.22285444e-01\n",
      "Epoch: 6090 mean train loss:  1.02643341e-01, bound:  3.22281033e-01\n",
      "Epoch: 6091 mean train loss:  1.02651760e-01, bound:  3.22279513e-01\n",
      "Epoch: 6092 mean train loss:  1.02589920e-01, bound:  3.22274208e-01\n",
      "Epoch: 6093 mean train loss:  1.02489315e-01, bound:  3.22272420e-01\n",
      "Epoch: 6094 mean train loss:  1.02419831e-01, bound:  3.22268575e-01\n",
      "Epoch: 6095 mean train loss:  1.02403328e-01, bound:  3.22264791e-01\n",
      "Epoch: 6096 mean train loss:  1.02399185e-01, bound:  3.22262824e-01\n",
      "Epoch: 6097 mean train loss:  1.02360718e-01, bound:  3.22257966e-01\n",
      "Epoch: 6098 mean train loss:  1.02288805e-01, bound:  3.22255939e-01\n",
      "Epoch: 6099 mean train loss:  1.02221802e-01, bound:  3.22251916e-01\n",
      "Epoch: 6100 mean train loss:  1.02185391e-01, bound:  3.22248489e-01\n",
      "Epoch: 6101 mean train loss:  1.02167159e-01, bound:  3.22245955e-01\n",
      "Epoch: 6102 mean train loss:  1.02137469e-01, bound:  3.22241545e-01\n",
      "Epoch: 6103 mean train loss:  1.02085456e-01, bound:  3.22239399e-01\n",
      "Epoch: 6104 mean train loss:  1.02025859e-01, bound:  3.22235346e-01\n",
      "Epoch: 6105 mean train loss:  1.01979524e-01, bound:  3.22232306e-01\n",
      "Epoch: 6106 mean train loss:  1.01948686e-01, bound:  3.22229415e-01\n",
      "Epoch: 6107 mean train loss:  1.01919822e-01, bound:  3.22225392e-01\n",
      "Epoch: 6108 mean train loss:  1.01879813e-01, bound:  3.22223008e-01\n",
      "Epoch: 6109 mean train loss:  1.01829320e-01, bound:  3.22218925e-01\n",
      "Epoch: 6110 mean train loss:  1.01780646e-01, bound:  3.22216064e-01\n",
      "Epoch: 6111 mean train loss:  1.01741381e-01, bound:  3.22212696e-01\n",
      "Epoch: 6112 mean train loss:  1.01707779e-01, bound:  3.22208971e-01\n",
      "Epoch: 6113 mean train loss:  1.01672307e-01, bound:  3.22206318e-01\n",
      "Epoch: 6114 mean train loss:  1.01630129e-01, bound:  3.22202384e-01\n",
      "Epoch: 6115 mean train loss:  1.01584114e-01, bound:  3.22199583e-01\n",
      "Epoch: 6116 mean train loss:  1.01540461e-01, bound:  3.22196066e-01\n",
      "Epoch: 6117 mean train loss:  1.01502106e-01, bound:  3.22192699e-01\n",
      "Epoch: 6118 mean train loss:  1.01465613e-01, bound:  3.22189718e-01\n",
      "Epoch: 6119 mean train loss:  1.01427719e-01, bound:  3.22185904e-01\n",
      "Epoch: 6120 mean train loss:  1.01385996e-01, bound:  3.22183102e-01\n",
      "Epoch: 6121 mean train loss:  1.01342767e-01, bound:  3.22179407e-01\n",
      "Epoch: 6122 mean train loss:  1.01301454e-01, bound:  3.22176218e-01\n",
      "Epoch: 6123 mean train loss:  1.01262309e-01, bound:  3.22172999e-01\n",
      "Epoch: 6124 mean train loss:  1.01224601e-01, bound:  3.22169453e-01\n",
      "Epoch: 6125 mean train loss:  1.01185903e-01, bound:  3.22166532e-01\n",
      "Epoch: 6126 mean train loss:  1.01145156e-01, bound:  3.22162896e-01\n",
      "Epoch: 6127 mean train loss:  1.01103529e-01, bound:  3.22159857e-01\n",
      "Epoch: 6128 mean train loss:  1.01062611e-01, bound:  3.22156489e-01\n",
      "Epoch: 6129 mean train loss:  1.01023294e-01, bound:  3.22153121e-01\n",
      "Epoch: 6130 mean train loss:  1.00984596e-01, bound:  3.22150022e-01\n",
      "Epoch: 6131 mean train loss:  1.00945771e-01, bound:  3.22146505e-01\n",
      "Epoch: 6132 mean train loss:  1.00905932e-01, bound:  3.22143465e-01\n",
      "Epoch: 6133 mean train loss:  1.00865141e-01, bound:  3.22139949e-01\n",
      "Epoch: 6134 mean train loss:  1.00824505e-01, bound:  3.22136819e-01\n",
      "Epoch: 6135 mean train loss:  1.00784674e-01, bound:  3.22133511e-01\n",
      "Epoch: 6136 mean train loss:  1.00745469e-01, bound:  3.22130144e-01\n",
      "Epoch: 6137 mean train loss:  1.00706272e-01, bound:  3.22127044e-01\n",
      "Epoch: 6138 mean train loss:  1.00666568e-01, bound:  3.22123557e-01\n",
      "Epoch: 6139 mean train loss:  1.00626692e-01, bound:  3.22120458e-01\n",
      "Epoch: 6140 mean train loss:  1.00586630e-01, bound:  3.22117060e-01\n",
      "Epoch: 6141 mean train loss:  1.00546613e-01, bound:  3.22113842e-01\n",
      "Epoch: 6142 mean train loss:  1.00506827e-01, bound:  3.22110564e-01\n",
      "Epoch: 6143 mean train loss:  1.00467727e-01, bound:  3.22107196e-01\n",
      "Epoch: 6144 mean train loss:  1.00428201e-01, bound:  3.22104067e-01\n",
      "Epoch: 6145 mean train loss:  1.00388728e-01, bound:  3.22100669e-01\n",
      "Epoch: 6146 mean train loss:  1.00348972e-01, bound:  3.22097540e-01\n",
      "Epoch: 6147 mean train loss:  1.00308970e-01, bound:  3.22094113e-01\n",
      "Epoch: 6148 mean train loss:  1.00269504e-01, bound:  3.22090954e-01\n",
      "Epoch: 6149 mean train loss:  1.00230023e-01, bound:  3.22087675e-01\n",
      "Epoch: 6150 mean train loss:  1.00190252e-01, bound:  3.22084367e-01\n",
      "Epoch: 6151 mean train loss:  1.00150913e-01, bound:  3.22081149e-01\n",
      "Epoch: 6152 mean train loss:  1.00111224e-01, bound:  3.22077781e-01\n",
      "Epoch: 6153 mean train loss:  1.00072056e-01, bound:  3.22074622e-01\n",
      "Epoch: 6154 mean train loss:  1.00032337e-01, bound:  3.22071284e-01\n",
      "Epoch: 6155 mean train loss:  9.99924913e-02, bound:  3.22068065e-01\n",
      "Epoch: 6156 mean train loss:  9.99531597e-02, bound:  3.22064757e-01\n",
      "Epoch: 6157 mean train loss:  9.99138430e-02, bound:  3.22061539e-01\n",
      "Epoch: 6158 mean train loss:  9.98741686e-02, bound:  3.22058260e-01\n",
      "Epoch: 6159 mean train loss:  9.98346955e-02, bound:  3.22054982e-01\n",
      "Epoch: 6160 mean train loss:  9.97954309e-02, bound:  3.22051764e-01\n",
      "Epoch: 6161 mean train loss:  9.97557193e-02, bound:  3.22048426e-01\n",
      "Epoch: 6162 mean train loss:  9.97162163e-02, bound:  3.22045237e-01\n",
      "Epoch: 6163 mean train loss:  9.96768996e-02, bound:  3.22041899e-01\n",
      "Epoch: 6164 mean train loss:  9.96371955e-02, bound:  3.22038680e-01\n",
      "Epoch: 6165 mean train loss:  9.95978490e-02, bound:  3.22035402e-01\n",
      "Epoch: 6166 mean train loss:  9.95585546e-02, bound:  3.22032124e-01\n",
      "Epoch: 6167 mean train loss:  9.95188951e-02, bound:  3.22028905e-01\n",
      "Epoch: 6168 mean train loss:  9.94794220e-02, bound:  3.22025627e-01\n",
      "Epoch: 6169 mean train loss:  9.94400755e-02, bound:  3.22022378e-01\n",
      "Epoch: 6170 mean train loss:  9.94007289e-02, bound:  3.22019100e-01\n",
      "Epoch: 6171 mean train loss:  9.93614197e-02, bound:  3.22015852e-01\n",
      "Epoch: 6172 mean train loss:  9.93220732e-02, bound:  3.22012603e-01\n",
      "Epoch: 6173 mean train loss:  9.92826596e-02, bound:  3.22009355e-01\n",
      "Epoch: 6174 mean train loss:  9.92435440e-02, bound:  3.22006047e-01\n",
      "Epoch: 6175 mean train loss:  9.92037058e-02, bound:  3.22002828e-01\n",
      "Epoch: 6176 mean train loss:  9.91644561e-02, bound:  3.21999580e-01\n",
      "Epoch: 6177 mean train loss:  9.91253033e-02, bound:  3.21996331e-01\n",
      "Epoch: 6178 mean train loss:  9.90855470e-02, bound:  3.21993083e-01\n",
      "Epoch: 6179 mean train loss:  9.90462899e-02, bound:  3.21989805e-01\n",
      "Epoch: 6180 mean train loss:  9.90070105e-02, bound:  3.21986556e-01\n",
      "Epoch: 6181 mean train loss:  9.89679918e-02, bound:  3.21983278e-01\n",
      "Epoch: 6182 mean train loss:  9.89284888e-02, bound:  3.21980059e-01\n",
      "Epoch: 6183 mean train loss:  9.88890603e-02, bound:  3.21976781e-01\n",
      "Epoch: 6184 mean train loss:  9.88499895e-02, bound:  3.21973562e-01\n",
      "Epoch: 6185 mean train loss:  9.88106728e-02, bound:  3.21970284e-01\n",
      "Epoch: 6186 mean train loss:  9.87714082e-02, bound:  3.21967036e-01\n",
      "Epoch: 6187 mean train loss:  9.87321362e-02, bound:  3.21963757e-01\n",
      "Epoch: 6188 mean train loss:  9.86927822e-02, bound:  3.21960509e-01\n",
      "Epoch: 6189 mean train loss:  9.86535028e-02, bound:  3.21957290e-01\n",
      "Epoch: 6190 mean train loss:  9.86141786e-02, bound:  3.21953982e-01\n",
      "Epoch: 6191 mean train loss:  9.85751599e-02, bound:  3.21950793e-01\n",
      "Epoch: 6192 mean train loss:  9.85356718e-02, bound:  3.21947485e-01\n",
      "Epoch: 6193 mean train loss:  9.84964296e-02, bound:  3.21944267e-01\n",
      "Epoch: 6194 mean train loss:  9.84570831e-02, bound:  3.21941018e-01\n",
      "Epoch: 6195 mean train loss:  9.84180123e-02, bound:  3.21937799e-01\n",
      "Epoch: 6196 mean train loss:  9.83790308e-02, bound:  3.21934491e-01\n",
      "Epoch: 6197 mean train loss:  9.83397886e-02, bound:  3.21931273e-01\n",
      "Epoch: 6198 mean train loss:  9.83004346e-02, bound:  3.21928024e-01\n",
      "Epoch: 6199 mean train loss:  9.82614011e-02, bound:  3.21924746e-01\n",
      "Epoch: 6200 mean train loss:  9.82223079e-02, bound:  3.21921527e-01\n",
      "Epoch: 6201 mean train loss:  9.81829688e-02, bound:  3.21918279e-01\n",
      "Epoch: 6202 mean train loss:  9.81440470e-02, bound:  3.21915001e-01\n",
      "Epoch: 6203 mean train loss:  9.81049687e-02, bound:  3.21911812e-01\n",
      "Epoch: 6204 mean train loss:  9.80656147e-02, bound:  3.21908504e-01\n",
      "Epoch: 6205 mean train loss:  9.80264395e-02, bound:  3.21905315e-01\n",
      "Epoch: 6206 mean train loss:  9.79872569e-02, bound:  3.21902037e-01\n",
      "Epoch: 6207 mean train loss:  9.79486182e-02, bound:  3.21898818e-01\n",
      "Epoch: 6208 mean train loss:  9.79092196e-02, bound:  3.21895540e-01\n",
      "Epoch: 6209 mean train loss:  9.78701785e-02, bound:  3.21892321e-01\n",
      "Epoch: 6210 mean train loss:  9.78309885e-02, bound:  3.21889043e-01\n",
      "Epoch: 6211 mean train loss:  9.77922752e-02, bound:  3.21885854e-01\n",
      "Epoch: 6212 mean train loss:  9.77529883e-02, bound:  3.21882546e-01\n",
      "Epoch: 6213 mean train loss:  9.77138206e-02, bound:  3.21879387e-01\n",
      "Epoch: 6214 mean train loss:  9.76747200e-02, bound:  3.21876079e-01\n",
      "Epoch: 6215 mean train loss:  9.76359695e-02, bound:  3.21872920e-01\n",
      "Epoch: 6216 mean train loss:  9.75969508e-02, bound:  3.21869582e-01\n",
      "Epoch: 6217 mean train loss:  9.75583419e-02, bound:  3.21866393e-01\n",
      "Epoch: 6218 mean train loss:  9.75191370e-02, bound:  3.21863055e-01\n",
      "Epoch: 6219 mean train loss:  9.74803939e-02, bound:  3.21859956e-01\n",
      "Epoch: 6220 mean train loss:  9.74418446e-02, bound:  3.21856529e-01\n",
      "Epoch: 6221 mean train loss:  9.74034816e-02, bound:  3.21853548e-01\n",
      "Epoch: 6222 mean train loss:  9.73654687e-02, bound:  3.21850032e-01\n",
      "Epoch: 6223 mean train loss:  9.73283798e-02, bound:  3.21847171e-01\n",
      "Epoch: 6224 mean train loss:  9.72920582e-02, bound:  3.21843445e-01\n",
      "Epoch: 6225 mean train loss:  9.72574726e-02, bound:  3.21840793e-01\n",
      "Epoch: 6226 mean train loss:  9.72257853e-02, bound:  3.21836799e-01\n",
      "Epoch: 6227 mean train loss:  9.71987918e-02, bound:  3.21834505e-01\n",
      "Epoch: 6228 mean train loss:  9.71793383e-02, bound:  3.21830094e-01\n",
      "Epoch: 6229 mean train loss:  9.71723869e-02, bound:  3.21828395e-01\n",
      "Epoch: 6230 mean train loss:  9.71867368e-02, bound:  3.21823210e-01\n",
      "Epoch: 6231 mean train loss:  9.72339734e-02, bound:  3.21822464e-01\n",
      "Epoch: 6232 mean train loss:  9.73319858e-02, bound:  3.21816057e-01\n",
      "Epoch: 6233 mean train loss:  9.74962488e-02, bound:  3.21816832e-01\n",
      "Epoch: 6234 mean train loss:  9.77302641e-02, bound:  3.21808726e-01\n",
      "Epoch: 6235 mean train loss:  9.79863778e-02, bound:  3.21811378e-01\n",
      "Epoch: 6236 mean train loss:  9.81422812e-02, bound:  3.21801722e-01\n",
      "Epoch: 6237 mean train loss:  9.80135128e-02, bound:  3.21805298e-01\n",
      "Epoch: 6238 mean train loss:  9.75557268e-02, bound:  3.21796447e-01\n",
      "Epoch: 6239 mean train loss:  9.69910398e-02, bound:  3.21797788e-01\n",
      "Epoch: 6240 mean train loss:  9.66796800e-02, bound:  3.21793288e-01\n",
      "Epoch: 6241 mean train loss:  9.67532620e-02, bound:  3.21789801e-01\n",
      "Epoch: 6242 mean train loss:  9.70163271e-02, bound:  3.21789891e-01\n",
      "Epoch: 6243 mean train loss:  9.71547142e-02, bound:  3.21782827e-01\n",
      "Epoch: 6244 mean train loss:  9.69989970e-02, bound:  3.21784049e-01\n",
      "Epoch: 6245 mean train loss:  9.66755450e-02, bound:  3.21777552e-01\n",
      "Epoch: 6246 mean train loss:  9.64598432e-02, bound:  3.21775854e-01\n",
      "Epoch: 6247 mean train loss:  9.64839160e-02, bound:  3.21773261e-01\n",
      "Epoch: 6248 mean train loss:  9.66223776e-02, bound:  3.21767986e-01\n",
      "Epoch: 6249 mean train loss:  9.66597050e-02, bound:  3.21768016e-01\n",
      "Epoch: 6250 mean train loss:  9.65216458e-02, bound:  3.21761996e-01\n",
      "Epoch: 6251 mean train loss:  9.63272452e-02, bound:  3.21761042e-01\n",
      "Epoch: 6252 mean train loss:  9.62376371e-02, bound:  3.21757525e-01\n",
      "Epoch: 6253 mean train loss:  9.62753147e-02, bound:  3.21753621e-01\n",
      "Epoch: 6254 mean train loss:  9.63288397e-02, bound:  3.21752548e-01\n",
      "Epoch: 6255 mean train loss:  9.62914526e-02, bound:  3.21747065e-01\n",
      "Epoch: 6256 mean train loss:  9.61707979e-02, bound:  3.21745872e-01\n",
      "Epoch: 6257 mean train loss:  9.60627794e-02, bound:  3.21741670e-01\n",
      "Epoch: 6258 mean train loss:  9.60328206e-02, bound:  3.21738452e-01\n",
      "Epoch: 6259 mean train loss:  9.60528255e-02, bound:  3.21736515e-01\n",
      "Epoch: 6260 mean train loss:  9.60484222e-02, bound:  3.21731776e-01\n",
      "Epoch: 6261 mean train loss:  9.59854648e-02, bound:  3.21730435e-01\n",
      "Epoch: 6262 mean train loss:  9.58975703e-02, bound:  3.21726203e-01\n",
      "Epoch: 6263 mean train loss:  9.58393887e-02, bound:  3.21723640e-01\n",
      "Epoch: 6264 mean train loss:  9.58241001e-02, bound:  3.21721077e-01\n",
      "Epoch: 6265 mean train loss:  9.58187431e-02, bound:  3.21717054e-01\n",
      "Epoch: 6266 mean train loss:  9.57865864e-02, bound:  3.21715325e-01\n",
      "Epoch: 6267 mean train loss:  9.57259685e-02, bound:  3.21711063e-01\n",
      "Epoch: 6268 mean train loss:  9.56645384e-02, bound:  3.21708709e-01\n",
      "Epoch: 6269 mean train loss:  9.56259966e-02, bound:  3.21705610e-01\n",
      "Epoch: 6270 mean train loss:  9.56060737e-02, bound:  3.21702093e-01\n",
      "Epoch: 6271 mean train loss:  9.55836400e-02, bound:  3.21700007e-01\n",
      "Epoch: 6272 mean train loss:  9.55444574e-02, bound:  3.21696043e-01\n",
      "Epoch: 6273 mean train loss:  9.54927653e-02, bound:  3.21693838e-01\n",
      "Epoch: 6274 mean train loss:  9.54453722e-02, bound:  3.21690470e-01\n",
      "Epoch: 6275 mean train loss:  9.54112560e-02, bound:  3.21687430e-01\n",
      "Epoch: 6276 mean train loss:  9.53850076e-02, bound:  3.21684897e-01\n",
      "Epoch: 6277 mean train loss:  9.53548327e-02, bound:  3.21681172e-01\n",
      "Epoch: 6278 mean train loss:  9.53158140e-02, bound:  3.21678847e-01\n",
      "Epoch: 6279 mean train loss:  9.52710882e-02, bound:  3.21675241e-01\n",
      "Epoch: 6280 mean train loss:  9.52298492e-02, bound:  3.21672499e-01\n",
      "Epoch: 6281 mean train loss:  9.51954499e-02, bound:  3.21669549e-01\n",
      "Epoch: 6282 mean train loss:  9.51650888e-02, bound:  3.21666181e-01\n",
      "Epoch: 6283 mean train loss:  9.51318890e-02, bound:  3.21663678e-01\n",
      "Epoch: 6284 mean train loss:  9.50943604e-02, bound:  3.21660161e-01\n",
      "Epoch: 6285 mean train loss:  9.50538889e-02, bound:  3.21657628e-01\n",
      "Epoch: 6286 mean train loss:  9.50146019e-02, bound:  3.21654379e-01\n",
      "Epoch: 6287 mean train loss:  9.49794203e-02, bound:  3.21651310e-01\n",
      "Epoch: 6288 mean train loss:  9.49461907e-02, bound:  3.21648508e-01\n",
      "Epoch: 6289 mean train loss:  9.49123576e-02, bound:  3.21645081e-01\n",
      "Epoch: 6290 mean train loss:  9.48763415e-02, bound:  3.21642458e-01\n",
      "Epoch: 6291 mean train loss:  9.48380008e-02, bound:  3.21639121e-01\n",
      "Epoch: 6292 mean train loss:  9.48004499e-02, bound:  3.21636319e-01\n",
      "Epoch: 6293 mean train loss:  9.47640464e-02, bound:  3.21633250e-01\n",
      "Epoch: 6294 mean train loss:  9.47295353e-02, bound:  3.21630120e-01\n",
      "Epoch: 6295 mean train loss:  9.46950316e-02, bound:  3.21627408e-01\n",
      "Epoch: 6296 mean train loss:  9.46596861e-02, bound:  3.21624100e-01\n",
      "Epoch: 6297 mean train loss:  9.46231484e-02, bound:  3.21621388e-01\n",
      "Epoch: 6298 mean train loss:  9.45865288e-02, bound:  3.21618170e-01\n",
      "Epoch: 6299 mean train loss:  9.45497006e-02, bound:  3.21615219e-01\n",
      "Epoch: 6300 mean train loss:  9.45139602e-02, bound:  3.21612239e-01\n",
      "Epoch: 6301 mean train loss:  9.44789127e-02, bound:  3.21609139e-01\n",
      "Epoch: 6302 mean train loss:  9.44439024e-02, bound:  3.21606308e-01\n",
      "Epoch: 6303 mean train loss:  9.44080651e-02, bound:  3.21603090e-01\n",
      "Epoch: 6304 mean train loss:  9.43720639e-02, bound:  3.21600348e-01\n",
      "Epoch: 6305 mean train loss:  9.43358243e-02, bound:  3.21597189e-01\n",
      "Epoch: 6306 mean train loss:  9.42997783e-02, bound:  3.21594298e-01\n",
      "Epoch: 6307 mean train loss:  9.42640379e-02, bound:  3.21591288e-01\n",
      "Epoch: 6308 mean train loss:  9.42286402e-02, bound:  3.21588218e-01\n",
      "Epoch: 6309 mean train loss:  9.41930786e-02, bound:  3.21585327e-01\n",
      "Epoch: 6310 mean train loss:  9.41577628e-02, bound:  3.21582168e-01\n",
      "Epoch: 6311 mean train loss:  9.41217691e-02, bound:  3.21579337e-01\n",
      "Epoch: 6312 mean train loss:  9.40860286e-02, bound:  3.21576208e-01\n",
      "Epoch: 6313 mean train loss:  9.40499008e-02, bound:  3.21573317e-01\n",
      "Epoch: 6314 mean train loss:  9.40141529e-02, bound:  3.21570307e-01\n",
      "Epoch: 6315 mean train loss:  9.39787477e-02, bound:  3.21567327e-01\n",
      "Epoch: 6316 mean train loss:  9.39429551e-02, bound:  3.21564376e-01\n",
      "Epoch: 6317 mean train loss:  9.39073488e-02, bound:  3.21561277e-01\n",
      "Epoch: 6318 mean train loss:  9.38717946e-02, bound:  3.21558446e-01\n",
      "Epoch: 6319 mean train loss:  9.38359797e-02, bound:  3.21555376e-01\n",
      "Epoch: 6320 mean train loss:  9.38003361e-02, bound:  3.21552455e-01\n",
      "Epoch: 6321 mean train loss:  9.37648490e-02, bound:  3.21549416e-01\n",
      "Epoch: 6322 mean train loss:  9.37292129e-02, bound:  3.21546435e-01\n",
      "Epoch: 6323 mean train loss:  9.36933830e-02, bound:  3.21543455e-01\n",
      "Epoch: 6324 mean train loss:  9.36575830e-02, bound:  3.21540445e-01\n",
      "Epoch: 6325 mean train loss:  9.36225057e-02, bound:  3.21537524e-01\n",
      "Epoch: 6326 mean train loss:  9.35867950e-02, bound:  3.21534485e-01\n",
      "Epoch: 6327 mean train loss:  9.35509205e-02, bound:  3.21531594e-01\n",
      "Epoch: 6328 mean train loss:  9.35152173e-02, bound:  3.21528554e-01\n",
      "Epoch: 6329 mean train loss:  9.34799165e-02, bound:  3.21525633e-01\n",
      "Epoch: 6330 mean train loss:  9.34445336e-02, bound:  3.21522623e-01\n",
      "Epoch: 6331 mean train loss:  9.34085771e-02, bound:  3.21519613e-01\n",
      "Epoch: 6332 mean train loss:  9.33729410e-02, bound:  3.21516693e-01\n",
      "Epoch: 6333 mean train loss:  9.33371931e-02, bound:  3.21513683e-01\n",
      "Epoch: 6334 mean train loss:  9.33017358e-02, bound:  3.21510702e-01\n",
      "Epoch: 6335 mean train loss:  9.32662264e-02, bound:  3.21507722e-01\n",
      "Epoch: 6336 mean train loss:  9.32307616e-02, bound:  3.21504802e-01\n",
      "Epoch: 6337 mean train loss:  9.31954309e-02, bound:  3.21501762e-01\n",
      "Epoch: 6338 mean train loss:  9.31598097e-02, bound:  3.21498841e-01\n",
      "Epoch: 6339 mean train loss:  9.31238830e-02, bound:  3.21495831e-01\n",
      "Epoch: 6340 mean train loss:  9.30885226e-02, bound:  3.21492851e-01\n",
      "Epoch: 6341 mean train loss:  9.30529758e-02, bound:  3.21489871e-01\n",
      "Epoch: 6342 mean train loss:  9.30174589e-02, bound:  3.21486950e-01\n",
      "Epoch: 6343 mean train loss:  9.29821655e-02, bound:  3.21483940e-01\n",
      "Epoch: 6344 mean train loss:  9.29464251e-02, bound:  3.21480960e-01\n",
      "Epoch: 6345 mean train loss:  9.29109752e-02, bound:  3.21478039e-01\n",
      "Epoch: 6346 mean train loss:  9.28753540e-02, bound:  3.21475029e-01\n",
      "Epoch: 6347 mean train loss:  9.28397402e-02, bound:  3.21472079e-01\n",
      "Epoch: 6348 mean train loss:  9.28043053e-02, bound:  3.21469098e-01\n",
      "Epoch: 6349 mean train loss:  9.27688554e-02, bound:  3.21466148e-01\n",
      "Epoch: 6350 mean train loss:  9.27330554e-02, bound:  3.21463168e-01\n",
      "Epoch: 6351 mean train loss:  9.26974565e-02, bound:  3.21460187e-01\n",
      "Epoch: 6352 mean train loss:  9.26620141e-02, bound:  3.21457237e-01\n",
      "Epoch: 6353 mean train loss:  9.26265046e-02, bound:  3.21454257e-01\n",
      "Epoch: 6354 mean train loss:  9.25912187e-02, bound:  3.21451306e-01\n",
      "Epoch: 6355 mean train loss:  9.25558954e-02, bound:  3.21448326e-01\n",
      "Epoch: 6356 mean train loss:  9.25200731e-02, bound:  3.21445376e-01\n",
      "Epoch: 6357 mean train loss:  9.24846530e-02, bound:  3.21442366e-01\n",
      "Epoch: 6358 mean train loss:  9.24490318e-02, bound:  3.21439445e-01\n",
      "Epoch: 6359 mean train loss:  9.24139246e-02, bound:  3.21436465e-01\n",
      "Epoch: 6360 mean train loss:  9.23782438e-02, bound:  3.21433485e-01\n",
      "Epoch: 6361 mean train loss:  9.23426077e-02, bound:  3.21430534e-01\n",
      "Epoch: 6362 mean train loss:  9.23074260e-02, bound:  3.21427584e-01\n",
      "Epoch: 6363 mean train loss:  9.22721997e-02, bound:  3.21424603e-01\n",
      "Epoch: 6364 mean train loss:  9.22363326e-02, bound:  3.21421653e-01\n",
      "Epoch: 6365 mean train loss:  9.22008380e-02, bound:  3.21418643e-01\n",
      "Epoch: 6366 mean train loss:  9.21655297e-02, bound:  3.21415722e-01\n",
      "Epoch: 6367 mean train loss:  9.21300724e-02, bound:  3.21412742e-01\n",
      "Epoch: 6368 mean train loss:  9.20946375e-02, bound:  3.21409792e-01\n",
      "Epoch: 6369 mean train loss:  9.20592323e-02, bound:  3.21406811e-01\n",
      "Epoch: 6370 mean train loss:  9.20238271e-02, bound:  3.21403891e-01\n",
      "Epoch: 6371 mean train loss:  9.19884741e-02, bound:  3.21400911e-01\n",
      "Epoch: 6372 mean train loss:  9.19529423e-02, bound:  3.21397990e-01\n",
      "Epoch: 6373 mean train loss:  9.19177309e-02, bound:  3.21394980e-01\n",
      "Epoch: 6374 mean train loss:  9.18822885e-02, bound:  3.21392059e-01\n",
      "Epoch: 6375 mean train loss:  9.18467641e-02, bound:  3.21389079e-01\n",
      "Epoch: 6376 mean train loss:  9.18114185e-02, bound:  3.21386129e-01\n",
      "Epoch: 6377 mean train loss:  9.17759761e-02, bound:  3.21383148e-01\n",
      "Epoch: 6378 mean train loss:  9.17404816e-02, bound:  3.21380228e-01\n",
      "Epoch: 6379 mean train loss:  9.17052105e-02, bound:  3.21377218e-01\n",
      "Epoch: 6380 mean train loss:  9.16699097e-02, bound:  3.21374327e-01\n",
      "Epoch: 6381 mean train loss:  9.16345790e-02, bound:  3.21371317e-01\n",
      "Epoch: 6382 mean train loss:  9.15991515e-02, bound:  3.21368396e-01\n",
      "Epoch: 6383 mean train loss:  9.15637538e-02, bound:  3.21365386e-01\n",
      "Epoch: 6384 mean train loss:  9.15288702e-02, bound:  3.21362495e-01\n",
      "Epoch: 6385 mean train loss:  9.14933532e-02, bound:  3.21359456e-01\n",
      "Epoch: 6386 mean train loss:  9.14584696e-02, bound:  3.21356654e-01\n",
      "Epoch: 6387 mean train loss:  9.14239734e-02, bound:  3.21353495e-01\n",
      "Epoch: 6388 mean train loss:  9.13892016e-02, bound:  3.21350753e-01\n",
      "Epoch: 6389 mean train loss:  9.13552046e-02, bound:  3.21347505e-01\n",
      "Epoch: 6390 mean train loss:  9.13222730e-02, bound:  3.21344972e-01\n",
      "Epoch: 6391 mean train loss:  9.12905708e-02, bound:  3.21341515e-01\n",
      "Epoch: 6392 mean train loss:  9.12612602e-02, bound:  3.21339160e-01\n",
      "Epoch: 6393 mean train loss:  9.12362039e-02, bound:  3.21335435e-01\n",
      "Epoch: 6394 mean train loss:  9.12182033e-02, bound:  3.21333498e-01\n",
      "Epoch: 6395 mean train loss:  9.12125856e-02, bound:  3.21329236e-01\n",
      "Epoch: 6396 mean train loss:  9.12272409e-02, bound:  3.21328014e-01\n",
      "Epoch: 6397 mean train loss:  9.12772715e-02, bound:  3.21322829e-01\n",
      "Epoch: 6398 mean train loss:  9.13812891e-02, bound:  3.21322739e-01\n",
      "Epoch: 6399 mean train loss:  9.15659964e-02, bound:  3.21316153e-01\n",
      "Epoch: 6400 mean train loss:  9.18435529e-02, bound:  3.21317852e-01\n",
      "Epoch: 6401 mean train loss:  9.21854228e-02, bound:  3.21309298e-01\n",
      "Epoch: 6402 mean train loss:  9.24450681e-02, bound:  3.21312904e-01\n",
      "Epoch: 6403 mean train loss:  9.23951119e-02, bound:  3.21303338e-01\n",
      "Epoch: 6404 mean train loss:  9.18883905e-02, bound:  3.21306765e-01\n",
      "Epoch: 6405 mean train loss:  9.11854357e-02, bound:  3.21299762e-01\n",
      "Epoch: 6406 mean train loss:  9.07745957e-02, bound:  3.21299314e-01\n",
      "Epoch: 6407 mean train loss:  9.08751339e-02, bound:  3.21297646e-01\n",
      "Epoch: 6408 mean train loss:  9.12287086e-02, bound:  3.21292192e-01\n",
      "Epoch: 6409 mean train loss:  9.13921744e-02, bound:  3.21293890e-01\n",
      "Epoch: 6410 mean train loss:  9.11637247e-02, bound:  3.21286857e-01\n",
      "Epoch: 6411 mean train loss:  9.07583684e-02, bound:  3.21287066e-01\n",
      "Epoch: 6412 mean train loss:  9.05641690e-02, bound:  3.21283162e-01\n",
      "Epoch: 6413 mean train loss:  9.06822011e-02, bound:  3.21279228e-01\n",
      "Epoch: 6414 mean train loss:  9.08619165e-02, bound:  3.21279049e-01\n",
      "Epoch: 6415 mean train loss:  9.08324271e-02, bound:  3.21272999e-01\n",
      "Epoch: 6416 mean train loss:  9.06015933e-02, bound:  3.21272969e-01\n",
      "Epoch: 6417 mean train loss:  9.04098377e-02, bound:  3.21268916e-01\n",
      "Epoch: 6418 mean train loss:  9.04131010e-02, bound:  3.21265966e-01\n",
      "Epoch: 6419 mean train loss:  9.05162469e-02, bound:  3.21264982e-01\n",
      "Epoch: 6420 mean train loss:  9.05300453e-02, bound:  3.21259856e-01\n",
      "Epoch: 6421 mean train loss:  9.04020220e-02, bound:  3.21259290e-01\n",
      "Epoch: 6422 mean train loss:  9.02566537e-02, bound:  3.21255058e-01\n",
      "Epoch: 6423 mean train loss:  9.02165845e-02, bound:  3.21252286e-01\n",
      "Epoch: 6424 mean train loss:  9.02595520e-02, bound:  3.21250528e-01\n",
      "Epoch: 6425 mean train loss:  9.02732760e-02, bound:  3.21245968e-01\n",
      "Epoch: 6426 mean train loss:  9.01999399e-02, bound:  3.21244955e-01\n",
      "Epoch: 6427 mean train loss:  9.00964513e-02, bound:  3.21240991e-01\n",
      "Epoch: 6428 mean train loss:  9.00442824e-02, bound:  3.21238488e-01\n",
      "Epoch: 6429 mean train loss:  9.00507495e-02, bound:  3.21236461e-01\n",
      "Epoch: 6430 mean train loss:  9.00541618e-02, bound:  3.21232527e-01\n",
      "Epoch: 6431 mean train loss:  9.00086388e-02, bound:  3.21231246e-01\n",
      "Epoch: 6432 mean train loss:  8.99340585e-02, bound:  3.21227461e-01\n",
      "Epoch: 6433 mean train loss:  8.98803920e-02, bound:  3.21225107e-01\n",
      "Epoch: 6434 mean train loss:  8.98639709e-02, bound:  3.21222663e-01\n",
      "Epoch: 6435 mean train loss:  8.98566023e-02, bound:  3.21218997e-01\n",
      "Epoch: 6436 mean train loss:  8.98256451e-02, bound:  3.21217358e-01\n",
      "Epoch: 6437 mean train loss:  8.97705182e-02, bound:  3.21213663e-01\n",
      "Epoch: 6438 mean train loss:  8.97198096e-02, bound:  3.21211487e-01\n",
      "Epoch: 6439 mean train loss:  8.96901786e-02, bound:  3.21208864e-01\n",
      "Epoch: 6440 mean train loss:  8.96733552e-02, bound:  3.21205646e-01\n",
      "Epoch: 6441 mean train loss:  8.96477848e-02, bound:  3.21203798e-01\n",
      "Epoch: 6442 mean train loss:  8.96061808e-02, bound:  3.21200311e-01\n",
      "Epoch: 6443 mean train loss:  8.95607397e-02, bound:  3.21198136e-01\n",
      "Epoch: 6444 mean train loss:  8.95241871e-02, bound:  3.21195275e-01\n",
      "Epoch: 6445 mean train loss:  8.94985870e-02, bound:  3.21192205e-01\n",
      "Epoch: 6446 mean train loss:  8.94738883e-02, bound:  3.21190000e-01\n",
      "Epoch: 6447 mean train loss:  8.94406065e-02, bound:  3.21186602e-01\n",
      "Epoch: 6448 mean train loss:  8.94006565e-02, bound:  3.21184397e-01\n",
      "Epoch: 6449 mean train loss:  8.93625766e-02, bound:  3.21181387e-01\n",
      "Epoch: 6450 mean train loss:  8.93308371e-02, bound:  3.21178645e-01\n",
      "Epoch: 6451 mean train loss:  8.93032998e-02, bound:  3.21176261e-01\n",
      "Epoch: 6452 mean train loss:  8.92736763e-02, bound:  3.21173131e-01\n",
      "Epoch: 6453 mean train loss:  8.92388299e-02, bound:  3.21170837e-01\n",
      "Epoch: 6454 mean train loss:  8.92024487e-02, bound:  3.21167797e-01\n",
      "Epoch: 6455 mean train loss:  8.91678333e-02, bound:  3.21165144e-01\n",
      "Epoch: 6456 mean train loss:  8.91366452e-02, bound:  3.21162492e-01\n",
      "Epoch: 6457 mean train loss:  8.91069099e-02, bound:  3.21159482e-01\n",
      "Epoch: 6458 mean train loss:  8.90755281e-02, bound:  3.21157098e-01\n",
      "Epoch: 6459 mean train loss:  8.90415460e-02, bound:  3.21154058e-01\n",
      "Epoch: 6460 mean train loss:  8.90070572e-02, bound:  3.21151525e-01\n",
      "Epoch: 6461 mean train loss:  8.89737979e-02, bound:  3.21148753e-01\n",
      "Epoch: 6462 mean train loss:  8.89420882e-02, bound:  3.21146011e-01\n",
      "Epoch: 6463 mean train loss:  8.89114887e-02, bound:  3.21143508e-01\n",
      "Epoch: 6464 mean train loss:  8.88795033e-02, bound:  3.21140498e-01\n",
      "Epoch: 6465 mean train loss:  8.88465196e-02, bound:  3.21138024e-01\n",
      "Epoch: 6466 mean train loss:  8.88127685e-02, bound:  3.21135104e-01\n",
      "Epoch: 6467 mean train loss:  8.87797475e-02, bound:  3.21132451e-01\n",
      "Epoch: 6468 mean train loss:  8.87478739e-02, bound:  3.21129769e-01\n",
      "Epoch: 6469 mean train loss:  8.87161493e-02, bound:  3.21126938e-01\n",
      "Epoch: 6470 mean train loss:  8.86844248e-02, bound:  3.21124405e-01\n",
      "Epoch: 6471 mean train loss:  8.86516124e-02, bound:  3.21121514e-01\n",
      "Epoch: 6472 mean train loss:  8.86190385e-02, bound:  3.21118951e-01\n",
      "Epoch: 6473 mean train loss:  8.85862038e-02, bound:  3.21116179e-01\n",
      "Epoch: 6474 mean train loss:  8.85536224e-02, bound:  3.21113497e-01\n",
      "Epoch: 6475 mean train loss:  8.85214061e-02, bound:  3.21110845e-01\n",
      "Epoch: 6476 mean train loss:  8.84901285e-02, bound:  3.21108013e-01\n",
      "Epoch: 6477 mean train loss:  8.84575397e-02, bound:  3.21105421e-01\n",
      "Epoch: 6478 mean train loss:  8.84248838e-02, bound:  3.21102589e-01\n",
      "Epoch: 6479 mean train loss:  8.83924589e-02, bound:  3.21099937e-01\n",
      "Epoch: 6480 mean train loss:  8.83599073e-02, bound:  3.21097195e-01\n",
      "Epoch: 6481 mean train loss:  8.83277357e-02, bound:  3.21094483e-01\n",
      "Epoch: 6482 mean train loss:  8.82954821e-02, bound:  3.21091861e-01\n",
      "Epoch: 6483 mean train loss:  8.82637352e-02, bound:  3.21089089e-01\n",
      "Epoch: 6484 mean train loss:  8.82312506e-02, bound:  3.21086466e-01\n",
      "Epoch: 6485 mean train loss:  8.81989151e-02, bound:  3.21083695e-01\n",
      "Epoch: 6486 mean train loss:  8.81664082e-02, bound:  3.21081072e-01\n",
      "Epoch: 6487 mean train loss:  8.81340504e-02, bound:  3.21078330e-01\n",
      "Epoch: 6488 mean train loss:  8.81014764e-02, bound:  3.21075618e-01\n",
      "Epoch: 6489 mean train loss:  8.80694464e-02, bound:  3.21072936e-01\n",
      "Epoch: 6490 mean train loss:  8.80375803e-02, bound:  3.21070164e-01\n",
      "Epoch: 6491 mean train loss:  8.80049989e-02, bound:  3.21067572e-01\n",
      "Epoch: 6492 mean train loss:  8.79726484e-02, bound:  3.21064770e-01\n",
      "Epoch: 6493 mean train loss:  8.79402384e-02, bound:  3.21062118e-01\n",
      "Epoch: 6494 mean train loss:  8.79079401e-02, bound:  3.21059406e-01\n",
      "Epoch: 6495 mean train loss:  8.78758430e-02, bound:  3.21056724e-01\n",
      "Epoch: 6496 mean train loss:  8.78434032e-02, bound:  3.21054071e-01\n",
      "Epoch: 6497 mean train loss:  8.78112763e-02, bound:  3.21051300e-01\n",
      "Epoch: 6498 mean train loss:  8.77790377e-02, bound:  3.21048647e-01\n",
      "Epoch: 6499 mean train loss:  8.77467021e-02, bound:  3.21045905e-01\n",
      "Epoch: 6500 mean train loss:  8.77148136e-02, bound:  3.21043253e-01\n",
      "Epoch: 6501 mean train loss:  8.76823068e-02, bound:  3.21040541e-01\n",
      "Epoch: 6502 mean train loss:  8.76497999e-02, bound:  3.21037829e-01\n",
      "Epoch: 6503 mean train loss:  8.76177102e-02, bound:  3.21035147e-01\n",
      "Epoch: 6504 mean train loss:  8.75857025e-02, bound:  3.21032465e-01\n",
      "Epoch: 6505 mean train loss:  8.75532925e-02, bound:  3.21029812e-01\n",
      "Epoch: 6506 mean train loss:  8.75210539e-02, bound:  3.21027040e-01\n",
      "Epoch: 6507 mean train loss:  8.74887556e-02, bound:  3.21024418e-01\n",
      "Epoch: 6508 mean train loss:  8.74566585e-02, bound:  3.21021676e-01\n",
      "Epoch: 6509 mean train loss:  8.74245763e-02, bound:  3.21019024e-01\n",
      "Epoch: 6510 mean train loss:  8.73922110e-02, bound:  3.21016282e-01\n",
      "Epoch: 6511 mean train loss:  8.73598084e-02, bound:  3.21013600e-01\n",
      "Epoch: 6512 mean train loss:  8.73276815e-02, bound:  3.21010917e-01\n",
      "Epoch: 6513 mean train loss:  8.72954354e-02, bound:  3.21008235e-01\n",
      "Epoch: 6514 mean train loss:  8.72628912e-02, bound:  3.21005553e-01\n",
      "Epoch: 6515 mean train loss:  8.72308686e-02, bound:  3.21002811e-01\n",
      "Epoch: 6516 mean train loss:  8.71986523e-02, bound:  3.21000159e-01\n",
      "Epoch: 6517 mean train loss:  8.71664733e-02, bound:  3.20997477e-01\n",
      "Epoch: 6518 mean train loss:  8.71342048e-02, bound:  3.20994794e-01\n",
      "Epoch: 6519 mean train loss:  8.71021301e-02, bound:  3.20992112e-01\n",
      "Epoch: 6520 mean train loss:  8.70698094e-02, bound:  3.20989430e-01\n",
      "Epoch: 6521 mean train loss:  8.70375037e-02, bound:  3.20986718e-01\n",
      "Epoch: 6522 mean train loss:  8.70053470e-02, bound:  3.20984036e-01\n",
      "Epoch: 6523 mean train loss:  8.69731680e-02, bound:  3.20981324e-01\n",
      "Epoch: 6524 mean train loss:  8.69410411e-02, bound:  3.20978671e-01\n",
      "Epoch: 6525 mean train loss:  8.69086757e-02, bound:  3.20975989e-01\n",
      "Epoch: 6526 mean train loss:  8.68765190e-02, bound:  3.20973247e-01\n",
      "Epoch: 6527 mean train loss:  8.68444219e-02, bound:  3.20970595e-01\n",
      "Epoch: 6528 mean train loss:  8.68120715e-02, bound:  3.20967883e-01\n",
      "Epoch: 6529 mean train loss:  8.67798552e-02, bound:  3.20965230e-01\n",
      "Epoch: 6530 mean train loss:  8.67476910e-02, bound:  3.20962518e-01\n",
      "Epoch: 6531 mean train loss:  8.67158696e-02, bound:  3.20959836e-01\n",
      "Epoch: 6532 mean train loss:  8.66833031e-02, bound:  3.20957184e-01\n",
      "Epoch: 6533 mean train loss:  8.66510421e-02, bound:  3.20954502e-01\n",
      "Epoch: 6534 mean train loss:  8.66189003e-02, bound:  3.20951819e-01\n",
      "Epoch: 6535 mean train loss:  8.65871012e-02, bound:  3.20949107e-01\n",
      "Epoch: 6536 mean train loss:  8.65546763e-02, bound:  3.20946425e-01\n",
      "Epoch: 6537 mean train loss:  8.65223035e-02, bound:  3.20943713e-01\n",
      "Epoch: 6538 mean train loss:  8.64902064e-02, bound:  3.20941061e-01\n",
      "Epoch: 6539 mean train loss:  8.64582434e-02, bound:  3.20938349e-01\n",
      "Epoch: 6540 mean train loss:  8.64258856e-02, bound:  3.20935696e-01\n",
      "Epoch: 6541 mean train loss:  8.63937661e-02, bound:  3.20933014e-01\n",
      "Epoch: 6542 mean train loss:  8.63615200e-02, bound:  3.20930332e-01\n",
      "Epoch: 6543 mean train loss:  8.63295794e-02, bound:  3.20927650e-01\n",
      "Epoch: 6544 mean train loss:  8.62971619e-02, bound:  3.20924968e-01\n",
      "Epoch: 6545 mean train loss:  8.62651318e-02, bound:  3.20922285e-01\n",
      "Epoch: 6546 mean train loss:  8.62330496e-02, bound:  3.20919603e-01\n",
      "Epoch: 6547 mean train loss:  8.62007588e-02, bound:  3.20916921e-01\n",
      "Epoch: 6548 mean train loss:  8.61683488e-02, bound:  3.20914239e-01\n",
      "Epoch: 6549 mean train loss:  8.61365274e-02, bound:  3.20911527e-01\n",
      "Epoch: 6550 mean train loss:  8.61042067e-02, bound:  3.20908844e-01\n",
      "Epoch: 6551 mean train loss:  8.60724449e-02, bound:  3.20906222e-01\n",
      "Epoch: 6552 mean train loss:  8.60398710e-02, bound:  3.20903480e-01\n",
      "Epoch: 6553 mean train loss:  8.60079229e-02, bound:  3.20900857e-01\n",
      "Epoch: 6554 mean train loss:  8.59758481e-02, bound:  3.20898175e-01\n",
      "Epoch: 6555 mean train loss:  8.59434679e-02, bound:  3.20895433e-01\n",
      "Epoch: 6556 mean train loss:  8.59112889e-02, bound:  3.20892811e-01\n",
      "Epoch: 6557 mean train loss:  8.58793259e-02, bound:  3.20890129e-01\n",
      "Epoch: 6558 mean train loss:  8.58469829e-02, bound:  3.20887446e-01\n",
      "Epoch: 6559 mean train loss:  8.58147070e-02, bound:  3.20884794e-01\n",
      "Epoch: 6560 mean train loss:  8.57826471e-02, bound:  3.20882112e-01\n",
      "Epoch: 6561 mean train loss:  8.57505202e-02, bound:  3.20879430e-01\n",
      "Epoch: 6562 mean train loss:  8.57183561e-02, bound:  3.20876747e-01\n",
      "Epoch: 6563 mean train loss:  8.56865197e-02, bound:  3.20874065e-01\n",
      "Epoch: 6564 mean train loss:  8.56540576e-02, bound:  3.20871383e-01\n",
      "Epoch: 6565 mean train loss:  8.56220797e-02, bound:  3.20868701e-01\n",
      "Epoch: 6566 mean train loss:  8.55899379e-02, bound:  3.20866078e-01\n",
      "Epoch: 6567 mean train loss:  8.55576769e-02, bound:  3.20863336e-01\n",
      "Epoch: 6568 mean train loss:  8.55259448e-02, bound:  3.20860714e-01\n",
      "Epoch: 6569 mean train loss:  8.54937285e-02, bound:  3.20857972e-01\n",
      "Epoch: 6570 mean train loss:  8.54617134e-02, bound:  3.20855379e-01\n",
      "Epoch: 6571 mean train loss:  8.54299888e-02, bound:  3.20852607e-01\n",
      "Epoch: 6572 mean train loss:  8.53979588e-02, bound:  3.20850044e-01\n",
      "Epoch: 6573 mean train loss:  8.53666812e-02, bound:  3.20847213e-01\n",
      "Epoch: 6574 mean train loss:  8.53355005e-02, bound:  3.20844769e-01\n",
      "Epoch: 6575 mean train loss:  8.53046402e-02, bound:  3.20841819e-01\n",
      "Epoch: 6576 mean train loss:  8.52752030e-02, bound:  3.20839524e-01\n",
      "Epoch: 6577 mean train loss:  8.52477178e-02, bound:  3.20836335e-01\n",
      "Epoch: 6578 mean train loss:  8.52231085e-02, bound:  3.20834279e-01\n",
      "Epoch: 6579 mean train loss:  8.52043107e-02, bound:  3.20830822e-01\n",
      "Epoch: 6580 mean train loss:  8.51949677e-02, bound:  3.20829183e-01\n",
      "Epoch: 6581 mean train loss:  8.52029473e-02, bound:  3.20825130e-01\n",
      "Epoch: 6582 mean train loss:  8.52402300e-02, bound:  3.20824295e-01\n",
      "Epoch: 6583 mean train loss:  8.53267312e-02, bound:  3.20819259e-01\n",
      "Epoch: 6584 mean train loss:  8.54908526e-02, bound:  3.20819676e-01\n",
      "Epoch: 6585 mean train loss:  8.57661068e-02, bound:  3.20813030e-01\n",
      "Epoch: 6586 mean train loss:  8.61517414e-02, bound:  3.20815414e-01\n",
      "Epoch: 6587 mean train loss:  8.65652859e-02, bound:  3.20806742e-01\n",
      "Epoch: 6588 mean train loss:  8.67457166e-02, bound:  3.20810854e-01\n",
      "Epoch: 6589 mean train loss:  8.64159986e-02, bound:  3.20801854e-01\n",
      "Epoch: 6590 mean train loss:  8.56159553e-02, bound:  3.20804834e-01\n",
      "Epoch: 6591 mean train loss:  8.49171951e-02, bound:  3.20799559e-01\n",
      "Epoch: 6592 mean train loss:  8.48233625e-02, bound:  3.20797712e-01\n",
      "Epoch: 6593 mean train loss:  8.52205232e-02, bound:  3.20797920e-01\n",
      "Epoch: 6594 mean train loss:  8.55543986e-02, bound:  3.20791662e-01\n",
      "Epoch: 6595 mean train loss:  8.54054019e-02, bound:  3.20793748e-01\n",
      "Epoch: 6596 mean train loss:  8.49251375e-02, bound:  3.20787847e-01\n",
      "Epoch: 6597 mean train loss:  8.46207812e-02, bound:  3.20786744e-01\n",
      "Epoch: 6598 mean train loss:  8.47330987e-02, bound:  3.20784956e-01\n",
      "Epoch: 6599 mean train loss:  8.49815235e-02, bound:  3.20779860e-01\n",
      "Epoch: 6600 mean train loss:  8.49802867e-02, bound:  3.20780516e-01\n",
      "Epoch: 6601 mean train loss:  8.47074911e-02, bound:  3.20775241e-01\n",
      "Epoch: 6602 mean train loss:  8.44793320e-02, bound:  3.20774108e-01\n",
      "Epoch: 6603 mean train loss:  8.45058486e-02, bound:  3.20772111e-01\n",
      "Epoch: 6604 mean train loss:  8.46486315e-02, bound:  3.20767879e-01\n",
      "Epoch: 6605 mean train loss:  8.46522152e-02, bound:  3.20767939e-01\n",
      "Epoch: 6606 mean train loss:  8.44832286e-02, bound:  3.20763350e-01\n",
      "Epoch: 6607 mean train loss:  8.43293890e-02, bound:  3.20761949e-01\n",
      "Epoch: 6608 mean train loss:  8.43286142e-02, bound:  3.20759624e-01\n",
      "Epoch: 6609 mean train loss:  8.44017044e-02, bound:  3.20755661e-01\n",
      "Epoch: 6610 mean train loss:  8.43940526e-02, bound:  3.20755035e-01\n",
      "Epoch: 6611 mean train loss:  8.42822120e-02, bound:  3.20750803e-01\n",
      "Epoch: 6612 mean train loss:  8.41792300e-02, bound:  3.20749134e-01\n",
      "Epoch: 6613 mean train loss:  8.41675997e-02, bound:  3.20746809e-01\n",
      "Epoch: 6614 mean train loss:  8.41997862e-02, bound:  3.20743233e-01\n",
      "Epoch: 6615 mean train loss:  8.41827393e-02, bound:  3.20742309e-01\n",
      "Epoch: 6616 mean train loss:  8.41033161e-02, bound:  3.20738643e-01\n",
      "Epoch: 6617 mean train loss:  8.40302482e-02, bound:  3.20736885e-01\n",
      "Epoch: 6618 mean train loss:  8.40117633e-02, bound:  3.20734650e-01\n",
      "Epoch: 6619 mean train loss:  8.40189978e-02, bound:  3.20731312e-01\n",
      "Epoch: 6620 mean train loss:  8.39975774e-02, bound:  3.20729971e-01\n",
      "Epoch: 6621 mean train loss:  8.39392319e-02, bound:  3.20726484e-01\n",
      "Epoch: 6622 mean train loss:  8.38831142e-02, bound:  3.20724517e-01\n",
      "Epoch: 6623 mean train loss:  8.38584080e-02, bound:  3.20722133e-01\n",
      "Epoch: 6624 mean train loss:  8.38509426e-02, bound:  3.20719063e-01\n",
      "Epoch: 6625 mean train loss:  8.38282704e-02, bound:  3.20717543e-01\n",
      "Epoch: 6626 mean train loss:  8.37832689e-02, bound:  3.20714295e-01\n",
      "Epoch: 6627 mean train loss:  8.37366357e-02, bound:  3.20712388e-01\n",
      "Epoch: 6628 mean train loss:  8.37076008e-02, bound:  3.20709974e-01\n",
      "Epoch: 6629 mean train loss:  8.36908966e-02, bound:  3.20707142e-01\n",
      "Epoch: 6630 mean train loss:  8.36677104e-02, bound:  3.20705324e-01\n",
      "Epoch: 6631 mean train loss:  8.36315528e-02, bound:  3.20702225e-01\n",
      "Epoch: 6632 mean train loss:  8.35906640e-02, bound:  3.20700169e-01\n",
      "Epoch: 6633 mean train loss:  8.35586563e-02, bound:  3.20697576e-01\n",
      "Epoch: 6634 mean train loss:  8.35357532e-02, bound:  3.20694804e-01\n",
      "Epoch: 6635 mean train loss:  8.35118890e-02, bound:  3.20692867e-01\n",
      "Epoch: 6636 mean train loss:  8.34804848e-02, bound:  3.20689887e-01\n",
      "Epoch: 6637 mean train loss:  8.34447518e-02, bound:  3.20687830e-01\n",
      "Epoch: 6638 mean train loss:  8.34117606e-02, bound:  3.20685238e-01\n",
      "Epoch: 6639 mean train loss:  8.33849609e-02, bound:  3.20682704e-01\n",
      "Epoch: 6640 mean train loss:  8.33593309e-02, bound:  3.20680559e-01\n",
      "Epoch: 6641 mean train loss:  8.33307505e-02, bound:  3.20677698e-01\n",
      "Epoch: 6642 mean train loss:  8.32981616e-02, bound:  3.20675552e-01\n",
      "Epoch: 6643 mean train loss:  8.32654908e-02, bound:  3.20672929e-01\n",
      "Epoch: 6644 mean train loss:  8.32360312e-02, bound:  3.20670456e-01\n",
      "Epoch: 6645 mean train loss:  8.32088888e-02, bound:  3.20668191e-01\n",
      "Epoch: 6646 mean train loss:  8.31813887e-02, bound:  3.20665479e-01\n",
      "Epoch: 6647 mean train loss:  8.31511915e-02, bound:  3.20663273e-01\n",
      "Epoch: 6648 mean train loss:  8.31196904e-02, bound:  3.20660651e-01\n",
      "Epoch: 6649 mean train loss:  8.30891505e-02, bound:  3.20658296e-01\n",
      "Epoch: 6650 mean train loss:  8.30606967e-02, bound:  3.20655912e-01\n",
      "Epoch: 6651 mean train loss:  8.30324590e-02, bound:  3.20653319e-01\n",
      "Epoch: 6652 mean train loss:  8.30034763e-02, bound:  3.20651084e-01\n",
      "Epoch: 6653 mean train loss:  8.29734877e-02, bound:  3.20648432e-01\n",
      "Epoch: 6654 mean train loss:  8.29432309e-02, bound:  3.20646137e-01\n",
      "Epoch: 6655 mean train loss:  8.29131305e-02, bound:  3.20643634e-01\n",
      "Epoch: 6656 mean train loss:  8.28844458e-02, bound:  3.20641160e-01\n",
      "Epoch: 6657 mean train loss:  8.28555450e-02, bound:  3.20638835e-01\n",
      "Epoch: 6658 mean train loss:  8.28265399e-02, bound:  3.20636213e-01\n",
      "Epoch: 6659 mean train loss:  8.27966854e-02, bound:  3.20633918e-01\n",
      "Epoch: 6660 mean train loss:  8.27671066e-02, bound:  3.20631385e-01\n",
      "Epoch: 6661 mean train loss:  8.27375650e-02, bound:  3.20628971e-01\n",
      "Epoch: 6662 mean train loss:  8.27084631e-02, bound:  3.20626557e-01\n",
      "Epoch: 6663 mean train loss:  8.26793835e-02, bound:  3.20624053e-01\n",
      "Epoch: 6664 mean train loss:  8.26501772e-02, bound:  3.20621729e-01\n",
      "Epoch: 6665 mean train loss:  8.26208740e-02, bound:  3.20619166e-01\n",
      "Epoch: 6666 mean train loss:  8.25913250e-02, bound:  3.20616841e-01\n",
      "Epoch: 6667 mean train loss:  8.25616494e-02, bound:  3.20614338e-01\n",
      "Epoch: 6668 mean train loss:  8.25323388e-02, bound:  3.20611894e-01\n",
      "Epoch: 6669 mean train loss:  8.25033486e-02, bound:  3.20609480e-01\n",
      "Epoch: 6670 mean train loss:  8.24738294e-02, bound:  3.20606977e-01\n",
      "Epoch: 6671 mean train loss:  8.24447200e-02, bound:  3.20604652e-01\n",
      "Epoch: 6672 mean train loss:  8.24149698e-02, bound:  3.20602149e-01\n",
      "Epoch: 6673 mean train loss:  8.23859498e-02, bound:  3.20599765e-01\n",
      "Epoch: 6674 mean train loss:  8.23565722e-02, bound:  3.20597291e-01\n",
      "Epoch: 6675 mean train loss:  8.23272839e-02, bound:  3.20594847e-01\n",
      "Epoch: 6676 mean train loss:  8.22979063e-02, bound:  3.20592463e-01\n",
      "Epoch: 6677 mean train loss:  8.22686926e-02, bound:  3.20589960e-01\n",
      "Epoch: 6678 mean train loss:  8.22393000e-02, bound:  3.20587605e-01\n",
      "Epoch: 6679 mean train loss:  8.22101086e-02, bound:  3.20585132e-01\n",
      "Epoch: 6680 mean train loss:  8.21808204e-02, bound:  3.20582718e-01\n",
      "Epoch: 6681 mean train loss:  8.21515396e-02, bound:  3.20580274e-01\n",
      "Epoch: 6682 mean train loss:  8.21220651e-02, bound:  3.20577800e-01\n",
      "Epoch: 6683 mean train loss:  8.20926800e-02, bound:  3.20575446e-01\n",
      "Epoch: 6684 mean train loss:  8.20635855e-02, bound:  3.20572942e-01\n",
      "Epoch: 6685 mean train loss:  8.20343345e-02, bound:  3.20570558e-01\n",
      "Epoch: 6686 mean train loss:  8.20049196e-02, bound:  3.20568115e-01\n",
      "Epoch: 6687 mean train loss:  8.19754973e-02, bound:  3.20565701e-01\n",
      "Epoch: 6688 mean train loss:  8.19464326e-02, bound:  3.20563257e-01\n",
      "Epoch: 6689 mean train loss:  8.19169283e-02, bound:  3.20560783e-01\n",
      "Epoch: 6690 mean train loss:  8.18875730e-02, bound:  3.20558429e-01\n",
      "Epoch: 6691 mean train loss:  8.18580464e-02, bound:  3.20555955e-01\n",
      "Epoch: 6692 mean train loss:  8.18291828e-02, bound:  3.20553571e-01\n",
      "Epoch: 6693 mean train loss:  8.17996860e-02, bound:  3.20551127e-01\n",
      "Epoch: 6694 mean train loss:  8.17705393e-02, bound:  3.20548683e-01\n",
      "Epoch: 6695 mean train loss:  8.17414820e-02, bound:  3.20546269e-01\n",
      "Epoch: 6696 mean train loss:  8.17118213e-02, bound:  3.20543855e-01\n",
      "Epoch: 6697 mean train loss:  8.16827640e-02, bound:  3.20541412e-01\n",
      "Epoch: 6698 mean train loss:  8.16531628e-02, bound:  3.20538998e-01\n",
      "Epoch: 6699 mean train loss:  8.16239193e-02, bound:  3.20536554e-01\n",
      "Epoch: 6700 mean train loss:  8.15949142e-02, bound:  3.20534110e-01\n",
      "Epoch: 6701 mean train loss:  8.15655738e-02, bound:  3.20531696e-01\n",
      "Epoch: 6702 mean train loss:  8.15361291e-02, bound:  3.20529282e-01\n",
      "Epoch: 6703 mean train loss:  8.15067887e-02, bound:  3.20526838e-01\n",
      "Epoch: 6704 mean train loss:  8.14773068e-02, bound:  3.20524395e-01\n",
      "Epoch: 6705 mean train loss:  8.14483240e-02, bound:  3.20521981e-01\n",
      "Epoch: 6706 mean train loss:  8.14189166e-02, bound:  3.20519567e-01\n",
      "Epoch: 6707 mean train loss:  8.13895613e-02, bound:  3.20517123e-01\n",
      "Epoch: 6708 mean train loss:  8.13602656e-02, bound:  3.20514709e-01\n",
      "Epoch: 6709 mean train loss:  8.13311189e-02, bound:  3.20512265e-01\n",
      "Epoch: 6710 mean train loss:  8.13016519e-02, bound:  3.20509851e-01\n",
      "Epoch: 6711 mean train loss:  8.12722966e-02, bound:  3.20507407e-01\n",
      "Epoch: 6712 mean train loss:  8.12433138e-02, bound:  3.20504993e-01\n",
      "Epoch: 6713 mean train loss:  8.12137052e-02, bound:  3.20502579e-01\n",
      "Epoch: 6714 mean train loss:  8.11844841e-02, bound:  3.20500165e-01\n",
      "Epoch: 6715 mean train loss:  8.11554939e-02, bound:  3.20497751e-01\n",
      "Epoch: 6716 mean train loss:  8.11258033e-02, bound:  3.20495307e-01\n",
      "Epoch: 6717 mean train loss:  8.10967833e-02, bound:  3.20492893e-01\n",
      "Epoch: 6718 mean train loss:  8.10672790e-02, bound:  3.20490450e-01\n",
      "Epoch: 6719 mean train loss:  8.10377970e-02, bound:  3.20488036e-01\n",
      "Epoch: 6720 mean train loss:  8.10087398e-02, bound:  3.20485592e-01\n",
      "Epoch: 6721 mean train loss:  8.09794664e-02, bound:  3.20483178e-01\n",
      "Epoch: 6722 mean train loss:  8.09500664e-02, bound:  3.20480734e-01\n",
      "Epoch: 6723 mean train loss:  8.09207708e-02, bound:  3.20478350e-01\n",
      "Epoch: 6724 mean train loss:  8.08914900e-02, bound:  3.20475936e-01\n",
      "Epoch: 6725 mean train loss:  8.08619931e-02, bound:  3.20473522e-01\n",
      "Epoch: 6726 mean train loss:  8.08327869e-02, bound:  3.20471078e-01\n",
      "Epoch: 6727 mean train loss:  8.08038116e-02, bound:  3.20468634e-01\n",
      "Epoch: 6728 mean train loss:  8.07743967e-02, bound:  3.20466220e-01\n",
      "Epoch: 6729 mean train loss:  8.07455555e-02, bound:  3.20463777e-01\n",
      "Epoch: 6730 mean train loss:  8.07155669e-02, bound:  3.20461363e-01\n",
      "Epoch: 6731 mean train loss:  8.06865916e-02, bound:  3.20458978e-01\n",
      "Epoch: 6732 mean train loss:  8.06569979e-02, bound:  3.20456535e-01\n",
      "Epoch: 6733 mean train loss:  8.06278065e-02, bound:  3.20454121e-01\n",
      "Epoch: 6734 mean train loss:  8.05987716e-02, bound:  3.20451677e-01\n",
      "Epoch: 6735 mean train loss:  8.05691779e-02, bound:  3.20449263e-01\n",
      "Epoch: 6736 mean train loss:  8.05400237e-02, bound:  3.20446819e-01\n",
      "Epoch: 6737 mean train loss:  8.05110931e-02, bound:  3.20444435e-01\n",
      "Epoch: 6738 mean train loss:  8.04813877e-02, bound:  3.20442021e-01\n",
      "Epoch: 6739 mean train loss:  8.04519281e-02, bound:  3.20439577e-01\n",
      "Epoch: 6740 mean train loss:  8.04227814e-02, bound:  3.20437163e-01\n",
      "Epoch: 6741 mean train loss:  8.03935602e-02, bound:  3.20434779e-01\n",
      "Epoch: 6742 mean train loss:  8.03642347e-02, bound:  3.20432335e-01\n",
      "Epoch: 6743 mean train loss:  8.03346634e-02, bound:  3.20429921e-01\n",
      "Epoch: 6744 mean train loss:  8.03057775e-02, bound:  3.20427507e-01\n",
      "Epoch: 6745 mean train loss:  8.02764967e-02, bound:  3.20425063e-01\n",
      "Epoch: 6746 mean train loss:  8.02470148e-02, bound:  3.20422679e-01\n",
      "Epoch: 6747 mean train loss:  8.02177191e-02, bound:  3.20420235e-01\n",
      "Epoch: 6748 mean train loss:  8.01886022e-02, bound:  3.20417792e-01\n",
      "Epoch: 6749 mean train loss:  8.01592842e-02, bound:  3.20415378e-01\n",
      "Epoch: 6750 mean train loss:  8.01301003e-02, bound:  3.20412964e-01\n",
      "Epoch: 6751 mean train loss:  8.01005438e-02, bound:  3.20410579e-01\n",
      "Epoch: 6752 mean train loss:  8.00712705e-02, bound:  3.20408136e-01\n",
      "Epoch: 6753 mean train loss:  8.00420493e-02, bound:  3.20405751e-01\n",
      "Epoch: 6754 mean train loss:  8.00126716e-02, bound:  3.20403337e-01\n",
      "Epoch: 6755 mean train loss:  7.99834132e-02, bound:  3.20400923e-01\n",
      "Epoch: 6756 mean train loss:  7.99540579e-02, bound:  3.20398480e-01\n",
      "Epoch: 6757 mean train loss:  7.99250975e-02, bound:  3.20396066e-01\n",
      "Epoch: 6758 mean train loss:  7.98954517e-02, bound:  3.20393622e-01\n",
      "Epoch: 6759 mean train loss:  7.98664093e-02, bound:  3.20391238e-01\n",
      "Epoch: 6760 mean train loss:  7.98371360e-02, bound:  3.20388824e-01\n",
      "Epoch: 6761 mean train loss:  7.98077583e-02, bound:  3.20386380e-01\n",
      "Epoch: 6762 mean train loss:  7.97783509e-02, bound:  3.20383966e-01\n",
      "Epoch: 6763 mean train loss:  7.97492266e-02, bound:  3.20381552e-01\n",
      "Epoch: 6764 mean train loss:  7.97198266e-02, bound:  3.20379138e-01\n",
      "Epoch: 6765 mean train loss:  7.96903819e-02, bound:  3.20376724e-01\n",
      "Epoch: 6766 mean train loss:  7.96611160e-02, bound:  3.20374340e-01\n",
      "Epoch: 6767 mean train loss:  7.96317533e-02, bound:  3.20371896e-01\n",
      "Epoch: 6768 mean train loss:  7.96026736e-02, bound:  3.20369482e-01\n",
      "Epoch: 6769 mean train loss:  7.95736164e-02, bound:  3.20367038e-01\n",
      "Epoch: 6770 mean train loss:  7.95441940e-02, bound:  3.20364684e-01\n",
      "Epoch: 6771 mean train loss:  7.95146674e-02, bound:  3.20362240e-01\n",
      "Epoch: 6772 mean train loss:  7.94856176e-02, bound:  3.20359856e-01\n",
      "Epoch: 6773 mean train loss:  7.94560462e-02, bound:  3.20357412e-01\n",
      "Epoch: 6774 mean train loss:  7.94270709e-02, bound:  3.20354998e-01\n",
      "Epoch: 6775 mean train loss:  7.93975815e-02, bound:  3.20352554e-01\n",
      "Epoch: 6776 mean train loss:  7.93683380e-02, bound:  3.20350170e-01\n",
      "Epoch: 6777 mean train loss:  7.93389454e-02, bound:  3.20347726e-01\n",
      "Epoch: 6778 mean train loss:  7.93099180e-02, bound:  3.20345372e-01\n",
      "Epoch: 6779 mean train loss:  7.92804882e-02, bound:  3.20342928e-01\n",
      "Epoch: 6780 mean train loss:  7.92511478e-02, bound:  3.20340514e-01\n",
      "Epoch: 6781 mean train loss:  7.92220980e-02, bound:  3.20338070e-01\n",
      "Epoch: 6782 mean train loss:  7.91929439e-02, bound:  3.20335716e-01\n",
      "Epoch: 6783 mean train loss:  7.91641846e-02, bound:  3.20333242e-01\n",
      "Epoch: 6784 mean train loss:  7.91350678e-02, bound:  3.20330918e-01\n",
      "Epoch: 6785 mean train loss:  7.91067109e-02, bound:  3.20328355e-01\n",
      "Epoch: 6786 mean train loss:  7.90791586e-02, bound:  3.20326179e-01\n",
      "Epoch: 6787 mean train loss:  7.90532082e-02, bound:  3.20323437e-01\n",
      "Epoch: 6788 mean train loss:  7.90299848e-02, bound:  3.20321470e-01\n",
      "Epoch: 6789 mean train loss:  7.90129080e-02, bound:  3.20318401e-01\n",
      "Epoch: 6790 mean train loss:  7.90063217e-02, bound:  3.20316911e-01\n",
      "Epoch: 6791 mean train loss:  7.90205747e-02, bound:  3.20313275e-01\n",
      "Epoch: 6792 mean train loss:  7.90754333e-02, bound:  3.20312560e-01\n",
      "Epoch: 6793 mean train loss:  7.92052075e-02, bound:  3.20307791e-01\n",
      "Epoch: 6794 mean train loss:  7.94672593e-02, bound:  3.20308596e-01\n",
      "Epoch: 6795 mean train loss:  7.99397603e-02, bound:  3.20301831e-01\n",
      "Epoch: 6796 mean train loss:  8.06485489e-02, bound:  3.20305139e-01\n",
      "Epoch: 6797 mean train loss:  8.14126283e-02, bound:  3.20295781e-01\n",
      "Epoch: 6798 mean train loss:  8.16070214e-02, bound:  3.20301175e-01\n",
      "Epoch: 6799 mean train loss:  8.06979612e-02, bound:  3.20291847e-01\n",
      "Epoch: 6800 mean train loss:  7.92351738e-02, bound:  3.20294827e-01\n",
      "Epoch: 6801 mean train loss:  7.86638483e-02, bound:  3.20291579e-01\n",
      "Epoch: 6802 mean train loss:  7.93142989e-02, bound:  3.20287764e-01\n",
      "Epoch: 6803 mean train loss:  8.00074711e-02, bound:  3.20290625e-01\n",
      "Epoch: 6804 mean train loss:  7.96852782e-02, bound:  3.20283562e-01\n",
      "Epoch: 6805 mean train loss:  7.87916407e-02, bound:  3.20285231e-01\n",
      "Epoch: 6806 mean train loss:  7.85651207e-02, bound:  3.20282519e-01\n",
      "Epoch: 6807 mean train loss:  7.90820196e-02, bound:  3.20278049e-01\n",
      "Epoch: 6808 mean train loss:  7.93169439e-02, bound:  3.20279598e-01\n",
      "Epoch: 6809 mean train loss:  7.88475201e-02, bound:  3.20273757e-01\n",
      "Epoch: 6810 mean train loss:  7.84160420e-02, bound:  3.20273161e-01\n",
      "Epoch: 6811 mean train loss:  7.85919651e-02, bound:  3.20271730e-01\n",
      "Epoch: 6812 mean train loss:  7.88875371e-02, bound:  3.20266783e-01\n",
      "Epoch: 6813 mean train loss:  7.87163451e-02, bound:  3.20267737e-01\n",
      "Epoch: 6814 mean train loss:  7.83530176e-02, bound:  3.20263743e-01\n",
      "Epoch: 6815 mean train loss:  7.83383176e-02, bound:  3.20261568e-01\n",
      "Epoch: 6816 mean train loss:  7.85521045e-02, bound:  3.20261180e-01\n",
      "Epoch: 6817 mean train loss:  7.85293505e-02, bound:  3.20256561e-01\n",
      "Epoch: 6818 mean train loss:  7.82775283e-02, bound:  3.20256233e-01\n",
      "Epoch: 6819 mean train loss:  7.81833157e-02, bound:  3.20253640e-01\n",
      "Epoch: 6820 mean train loss:  7.83049390e-02, bound:  3.20250183e-01\n",
      "Epoch: 6821 mean train loss:  7.83343911e-02, bound:  3.20249856e-01\n",
      "Epoch: 6822 mean train loss:  7.81777874e-02, bound:  3.20245922e-01\n",
      "Epoch: 6823 mean train loss:  7.80685768e-02, bound:  3.20244282e-01\n",
      "Epoch: 6824 mean train loss:  7.81206712e-02, bound:  3.20242703e-01\n",
      "Epoch: 6825 mean train loss:  7.81587064e-02, bound:  3.20239037e-01\n",
      "Epoch: 6826 mean train loss:  7.80652985e-02, bound:  3.20238441e-01\n",
      "Epoch: 6827 mean train loss:  7.79660270e-02, bound:  3.20235640e-01\n",
      "Epoch: 6828 mean train loss:  7.79731721e-02, bound:  3.20233256e-01\n",
      "Epoch: 6829 mean train loss:  7.80024230e-02, bound:  3.20232183e-01\n",
      "Epoch: 6830 mean train loss:  7.79493302e-02, bound:  3.20228815e-01\n",
      "Epoch: 6831 mean train loss:  7.78663605e-02, bound:  3.20227444e-01\n",
      "Epoch: 6832 mean train loss:  7.78460652e-02, bound:  3.20225328e-01\n",
      "Epoch: 6833 mean train loss:  7.78609067e-02, bound:  3.20222318e-01\n",
      "Epoch: 6834 mean train loss:  7.78321177e-02, bound:  3.20221215e-01\n",
      "Epoch: 6835 mean train loss:  7.77674317e-02, bound:  3.20218295e-01\n",
      "Epoch: 6836 mean train loss:  7.77315572e-02, bound:  3.20216298e-01\n",
      "Epoch: 6837 mean train loss:  7.77317509e-02, bound:  3.20214659e-01\n",
      "Epoch: 6838 mean train loss:  7.77142793e-02, bound:  3.20211738e-01\n",
      "Epoch: 6839 mean train loss:  7.76659548e-02, bound:  3.20210367e-01\n",
      "Epoch: 6840 mean train loss:  7.76246414e-02, bound:  3.20207983e-01\n",
      "Epoch: 6841 mean train loss:  7.76113868e-02, bound:  3.20205539e-01\n",
      "Epoch: 6842 mean train loss:  7.75975734e-02, bound:  3.20204020e-01\n",
      "Epoch: 6843 mean train loss:  7.75623545e-02, bound:  3.20201188e-01\n",
      "Epoch: 6844 mean train loss:  7.75219053e-02, bound:  3.20199370e-01\n",
      "Epoch: 6845 mean train loss:  7.74978995e-02, bound:  3.20197254e-01\n",
      "Epoch: 6846 mean train loss:  7.74829835e-02, bound:  3.20194662e-01\n",
      "Epoch: 6847 mean train loss:  7.74559528e-02, bound:  3.20193052e-01\n",
      "Epoch: 6848 mean train loss:  7.74199814e-02, bound:  3.20190459e-01\n",
      "Epoch: 6849 mean train loss:  7.73903579e-02, bound:  3.20188403e-01\n",
      "Epoch: 6850 mean train loss:  7.73706511e-02, bound:  3.20186466e-01\n",
      "Epoch: 6851 mean train loss:  7.73480386e-02, bound:  3.20183933e-01\n",
      "Epoch: 6852 mean train loss:  7.73173049e-02, bound:  3.20182174e-01\n",
      "Epoch: 6853 mean train loss:  7.72861615e-02, bound:  3.20179790e-01\n",
      "Epoch: 6854 mean train loss:  7.72615671e-02, bound:  3.20177615e-01\n",
      "Epoch: 6855 mean train loss:  7.72396326e-02, bound:  3.20175737e-01\n",
      "Epoch: 6856 mean train loss:  7.72129893e-02, bound:  3.20173204e-01\n",
      "Epoch: 6857 mean train loss:  7.71828815e-02, bound:  3.20171297e-01\n",
      "Epoch: 6858 mean train loss:  7.71555007e-02, bound:  3.20169121e-01\n",
      "Epoch: 6859 mean train loss:  7.71320313e-02, bound:  3.20166826e-01\n",
      "Epoch: 6860 mean train loss:  7.71071985e-02, bound:  3.20164919e-01\n",
      "Epoch: 6861 mean train loss:  7.70795792e-02, bound:  3.20162505e-01\n",
      "Epoch: 6862 mean train loss:  7.70510510e-02, bound:  3.20160508e-01\n",
      "Epoch: 6863 mean train loss:  7.70257488e-02, bound:  3.20158422e-01\n",
      "Epoch: 6864 mean train loss:  7.70007968e-02, bound:  3.20156127e-01\n",
      "Epoch: 6865 mean train loss:  7.69749656e-02, bound:  3.20154190e-01\n",
      "Epoch: 6866 mean train loss:  7.69476742e-02, bound:  3.20151865e-01\n",
      "Epoch: 6867 mean train loss:  7.69204870e-02, bound:  3.20149809e-01\n",
      "Epoch: 6868 mean train loss:  7.68953189e-02, bound:  3.20147663e-01\n",
      "Epoch: 6869 mean train loss:  7.68700689e-02, bound:  3.20145369e-01\n",
      "Epoch: 6870 mean train loss:  7.68434256e-02, bound:  3.20143372e-01\n",
      "Epoch: 6871 mean train loss:  7.68164769e-02, bound:  3.20141137e-01\n",
      "Epoch: 6872 mean train loss:  7.67900795e-02, bound:  3.20139021e-01\n",
      "Epoch: 6873 mean train loss:  7.67645463e-02, bound:  3.20136935e-01\n",
      "Epoch: 6874 mean train loss:  7.67386556e-02, bound:  3.20134670e-01\n",
      "Epoch: 6875 mean train loss:  7.67123401e-02, bound:  3.20132673e-01\n",
      "Epoch: 6876 mean train loss:  7.66856000e-02, bound:  3.20130438e-01\n",
      "Epoch: 6877 mean train loss:  7.66594931e-02, bound:  3.20128322e-01\n",
      "Epoch: 6878 mean train loss:  7.66336694e-02, bound:  3.20126206e-01\n",
      "Epoch: 6879 mean train loss:  7.66075626e-02, bound:  3.20123971e-01\n",
      "Epoch: 6880 mean train loss:  7.65814558e-02, bound:  3.20121914e-01\n",
      "Epoch: 6881 mean train loss:  7.65551403e-02, bound:  3.20119739e-01\n",
      "Epoch: 6882 mean train loss:  7.65289366e-02, bound:  3.20117593e-01\n",
      "Epoch: 6883 mean train loss:  7.65030459e-02, bound:  3.20115477e-01\n",
      "Epoch: 6884 mean train loss:  7.64767826e-02, bound:  3.20113271e-01\n",
      "Epoch: 6885 mean train loss:  7.64505938e-02, bound:  3.20111185e-01\n",
      "Epoch: 6886 mean train loss:  7.64243826e-02, bound:  3.20109010e-01\n",
      "Epoch: 6887 mean train loss:  7.63981119e-02, bound:  3.20106894e-01\n",
      "Epoch: 6888 mean train loss:  7.63723254e-02, bound:  3.20104778e-01\n",
      "Epoch: 6889 mean train loss:  7.63461888e-02, bound:  3.20102572e-01\n",
      "Epoch: 6890 mean train loss:  7.63200969e-02, bound:  3.20100486e-01\n",
      "Epoch: 6891 mean train loss:  7.62936547e-02, bound:  3.20098311e-01\n",
      "Epoch: 6892 mean train loss:  7.62675256e-02, bound:  3.20096195e-01\n",
      "Epoch: 6893 mean train loss:  7.62415603e-02, bound:  3.20094049e-01\n",
      "Epoch: 6894 mean train loss:  7.62152523e-02, bound:  3.20091844e-01\n",
      "Epoch: 6895 mean train loss:  7.61889368e-02, bound:  3.20089757e-01\n",
      "Epoch: 6896 mean train loss:  7.61629865e-02, bound:  3.20087582e-01\n",
      "Epoch: 6897 mean train loss:  7.61369616e-02, bound:  3.20085466e-01\n",
      "Epoch: 6898 mean train loss:  7.61107281e-02, bound:  3.20083380e-01\n",
      "Epoch: 6899 mean train loss:  7.60844499e-02, bound:  3.20081204e-01\n",
      "Epoch: 6900 mean train loss:  7.60583654e-02, bound:  3.20079058e-01\n",
      "Epoch: 6901 mean train loss:  7.60320872e-02, bound:  3.20076913e-01\n",
      "Epoch: 6902 mean train loss:  7.60059431e-02, bound:  3.20074767e-01\n",
      "Epoch: 6903 mean train loss:  7.59799331e-02, bound:  3.20072651e-01\n",
      "Epoch: 6904 mean train loss:  7.59536177e-02, bound:  3.20070475e-01\n",
      "Epoch: 6905 mean train loss:  7.59277493e-02, bound:  3.20068330e-01\n",
      "Epoch: 6906 mean train loss:  7.59016424e-02, bound:  3.20066184e-01\n",
      "Epoch: 6907 mean train loss:  7.58751035e-02, bound:  3.20064038e-01\n",
      "Epoch: 6908 mean train loss:  7.58488923e-02, bound:  3.20061922e-01\n",
      "Epoch: 6909 mean train loss:  7.58231729e-02, bound:  3.20059747e-01\n",
      "Epoch: 6910 mean train loss:  7.57966787e-02, bound:  3.20057660e-01\n",
      "Epoch: 6911 mean train loss:  7.57706091e-02, bound:  3.20055485e-01\n",
      "Epoch: 6912 mean train loss:  7.57446215e-02, bound:  3.20053369e-01\n",
      "Epoch: 6913 mean train loss:  7.57181123e-02, bound:  3.20051193e-01\n",
      "Epoch: 6914 mean train loss:  7.56922513e-02, bound:  3.20049107e-01\n",
      "Epoch: 6915 mean train loss:  7.56659731e-02, bound:  3.20046932e-01\n",
      "Epoch: 6916 mean train loss:  7.56398141e-02, bound:  3.20044786e-01\n",
      "Epoch: 6917 mean train loss:  7.56137222e-02, bound:  3.20042670e-01\n",
      "Epoch: 6918 mean train loss:  7.55872279e-02, bound:  3.20040494e-01\n",
      "Epoch: 6919 mean train loss:  7.55614191e-02, bound:  3.20038378e-01\n",
      "Epoch: 6920 mean train loss:  7.55349845e-02, bound:  3.20036232e-01\n",
      "Epoch: 6921 mean train loss:  7.55089000e-02, bound:  3.20034117e-01\n",
      "Epoch: 6922 mean train loss:  7.54824951e-02, bound:  3.20031941e-01\n",
      "Epoch: 6923 mean train loss:  7.54565299e-02, bound:  3.20029825e-01\n",
      "Epoch: 6924 mean train loss:  7.54301026e-02, bound:  3.20027679e-01\n",
      "Epoch: 6925 mean train loss:  7.54039735e-02, bound:  3.20025563e-01\n",
      "Epoch: 6926 mean train loss:  7.53780380e-02, bound:  3.20023417e-01\n",
      "Epoch: 6927 mean train loss:  7.53515437e-02, bound:  3.20021302e-01\n",
      "Epoch: 6928 mean train loss:  7.53254443e-02, bound:  3.20019126e-01\n",
      "Epoch: 6929 mean train loss:  7.52993003e-02, bound:  3.20017010e-01\n",
      "Epoch: 6930 mean train loss:  7.52730072e-02, bound:  3.20014834e-01\n",
      "Epoch: 6931 mean train loss:  7.52468407e-02, bound:  3.20012718e-01\n",
      "Epoch: 6932 mean train loss:  7.52209276e-02, bound:  3.20010602e-01\n",
      "Epoch: 6933 mean train loss:  7.51943141e-02, bound:  3.20008427e-01\n",
      "Epoch: 6934 mean train loss:  7.51683414e-02, bound:  3.20006341e-01\n",
      "Epoch: 6935 mean train loss:  7.51425400e-02, bound:  3.20004165e-01\n",
      "Epoch: 6936 mean train loss:  7.51156583e-02, bound:  3.20002019e-01\n",
      "Epoch: 6937 mean train loss:  7.50896335e-02, bound:  3.19999874e-01\n",
      "Epoch: 6938 mean train loss:  7.50633925e-02, bound:  3.19997728e-01\n",
      "Epoch: 6939 mean train loss:  7.50374943e-02, bound:  3.19995582e-01\n",
      "Epoch: 6940 mean train loss:  7.50108510e-02, bound:  3.19993436e-01\n",
      "Epoch: 6941 mean train loss:  7.49844313e-02, bound:  3.19991320e-01\n",
      "Epoch: 6942 mean train loss:  7.49584883e-02, bound:  3.19989175e-01\n",
      "Epoch: 6943 mean train loss:  7.49326721e-02, bound:  3.19987029e-01\n",
      "Epoch: 6944 mean train loss:  7.49061927e-02, bound:  3.19984913e-01\n",
      "Epoch: 6945 mean train loss:  7.48795047e-02, bound:  3.19982767e-01\n",
      "Epoch: 6946 mean train loss:  7.48537108e-02, bound:  3.19980651e-01\n",
      "Epoch: 6947 mean train loss:  7.48274922e-02, bound:  3.19978505e-01\n",
      "Epoch: 6948 mean train loss:  7.48012885e-02, bound:  3.19976389e-01\n",
      "Epoch: 6949 mean train loss:  7.47752935e-02, bound:  3.19974214e-01\n",
      "Epoch: 6950 mean train loss:  7.47487620e-02, bound:  3.19972098e-01\n",
      "Epoch: 6951 mean train loss:  7.47226626e-02, bound:  3.19969952e-01\n",
      "Epoch: 6952 mean train loss:  7.46963024e-02, bound:  3.19967806e-01\n",
      "Epoch: 6953 mean train loss:  7.46701732e-02, bound:  3.19965661e-01\n",
      "Epoch: 6954 mean train loss:  7.46438131e-02, bound:  3.19963545e-01\n",
      "Epoch: 6955 mean train loss:  7.46175945e-02, bound:  3.19961429e-01\n",
      "Epoch: 6956 mean train loss:  7.45912865e-02, bound:  3.19959253e-01\n",
      "Epoch: 6957 mean train loss:  7.45650902e-02, bound:  3.19957137e-01\n",
      "Epoch: 6958 mean train loss:  7.45388195e-02, bound:  3.19954962e-01\n",
      "Epoch: 6959 mean train loss:  7.45124891e-02, bound:  3.19952816e-01\n",
      "Epoch: 6960 mean train loss:  7.44863600e-02, bound:  3.19950670e-01\n",
      "Epoch: 6961 mean train loss:  7.44598210e-02, bound:  3.19948554e-01\n",
      "Epoch: 6962 mean train loss:  7.44337440e-02, bound:  3.19946408e-01\n",
      "Epoch: 6963 mean train loss:  7.44075477e-02, bound:  3.19944292e-01\n",
      "Epoch: 6964 mean train loss:  7.43811876e-02, bound:  3.19942117e-01\n",
      "Epoch: 6965 mean train loss:  7.43551254e-02, bound:  3.19940001e-01\n",
      "Epoch: 6966 mean train loss:  7.43287653e-02, bound:  3.19937855e-01\n",
      "Epoch: 6967 mean train loss:  7.43023679e-02, bound:  3.19935739e-01\n",
      "Epoch: 6968 mean train loss:  7.42765293e-02, bound:  3.19933593e-01\n",
      "Epoch: 6969 mean train loss:  7.42502138e-02, bound:  3.19931448e-01\n",
      "Epoch: 6970 mean train loss:  7.42236227e-02, bound:  3.19929332e-01\n",
      "Epoch: 6971 mean train loss:  7.41972774e-02, bound:  3.19927216e-01\n",
      "Epoch: 6972 mean train loss:  7.41710812e-02, bound:  3.19925040e-01\n",
      "Epoch: 6973 mean train loss:  7.41451830e-02, bound:  3.19922924e-01\n",
      "Epoch: 6974 mean train loss:  7.41184205e-02, bound:  3.19920778e-01\n",
      "Epoch: 6975 mean train loss:  7.40922391e-02, bound:  3.19918633e-01\n",
      "Epoch: 6976 mean train loss:  7.40660653e-02, bound:  3.19916487e-01\n",
      "Epoch: 6977 mean train loss:  7.40398467e-02, bound:  3.19914371e-01\n",
      "Epoch: 6978 mean train loss:  7.40134716e-02, bound:  3.19912255e-01\n",
      "Epoch: 6979 mean train loss:  7.39871040e-02, bound:  3.19910049e-01\n",
      "Epoch: 6980 mean train loss:  7.39610270e-02, bound:  3.19907963e-01\n",
      "Epoch: 6981 mean train loss:  7.39348680e-02, bound:  3.19905788e-01\n",
      "Epoch: 6982 mean train loss:  7.39081949e-02, bound:  3.19903672e-01\n",
      "Epoch: 6983 mean train loss:  7.38819465e-02, bound:  3.19901496e-01\n",
      "Epoch: 6984 mean train loss:  7.38559812e-02, bound:  3.19899380e-01\n",
      "Epoch: 6985 mean train loss:  7.38297477e-02, bound:  3.19897205e-01\n",
      "Epoch: 6986 mean train loss:  7.38030300e-02, bound:  3.19895118e-01\n",
      "Epoch: 6987 mean train loss:  7.37769604e-02, bound:  3.19892973e-01\n",
      "Epoch: 6988 mean train loss:  7.37507269e-02, bound:  3.19890827e-01\n",
      "Epoch: 6989 mean train loss:  7.37242475e-02, bound:  3.19888711e-01\n",
      "Epoch: 6990 mean train loss:  7.36977533e-02, bound:  3.19886565e-01\n",
      "Epoch: 6991 mean train loss:  7.36717358e-02, bound:  3.19884419e-01\n",
      "Epoch: 6992 mean train loss:  7.36453682e-02, bound:  3.19882303e-01\n",
      "Epoch: 6993 mean train loss:  7.36189410e-02, bound:  3.19880128e-01\n",
      "Epoch: 6994 mean train loss:  7.35927299e-02, bound:  3.19878012e-01\n",
      "Epoch: 6995 mean train loss:  7.35663176e-02, bound:  3.19875896e-01\n",
      "Epoch: 6996 mean train loss:  7.35402107e-02, bound:  3.19873720e-01\n",
      "Epoch: 6997 mean train loss:  7.35136867e-02, bound:  3.19871604e-01\n",
      "Epoch: 6998 mean train loss:  7.34873340e-02, bound:  3.19869488e-01\n",
      "Epoch: 6999 mean train loss:  7.34610707e-02, bound:  3.19867343e-01\n",
      "Epoch: 7000 mean train loss:  7.34348074e-02, bound:  3.19865197e-01\n",
      "Epoch: 7001 mean train loss:  7.34084472e-02, bound:  3.19863081e-01\n",
      "Epoch: 7002 mean train loss:  7.33820274e-02, bound:  3.19860905e-01\n",
      "Epoch: 7003 mean train loss:  7.33559206e-02, bound:  3.19858789e-01\n",
      "Epoch: 7004 mean train loss:  7.33293071e-02, bound:  3.19856614e-01\n",
      "Epoch: 7005 mean train loss:  7.33030140e-02, bound:  3.19854498e-01\n",
      "Epoch: 7006 mean train loss:  7.32769147e-02, bound:  3.19852322e-01\n",
      "Epoch: 7007 mean train loss:  7.32502937e-02, bound:  3.19850236e-01\n",
      "Epoch: 7008 mean train loss:  7.32240230e-02, bound:  3.19848090e-01\n",
      "Epoch: 7009 mean train loss:  7.31979311e-02, bound:  3.19845974e-01\n",
      "Epoch: 7010 mean train loss:  7.31710643e-02, bound:  3.19843799e-01\n",
      "Epoch: 7011 mean train loss:  7.31449649e-02, bound:  3.19841683e-01\n",
      "Epoch: 7012 mean train loss:  7.31189027e-02, bound:  3.19839537e-01\n",
      "Epoch: 7013 mean train loss:  7.30924085e-02, bound:  3.19837391e-01\n",
      "Epoch: 7014 mean train loss:  7.30656385e-02, bound:  3.19835275e-01\n",
      "Epoch: 7015 mean train loss:  7.30394721e-02, bound:  3.19833100e-01\n",
      "Epoch: 7016 mean train loss:  7.30134323e-02, bound:  3.19830984e-01\n",
      "Epoch: 7017 mean train loss:  7.29867741e-02, bound:  3.19828868e-01\n",
      "Epoch: 7018 mean train loss:  7.29604512e-02, bound:  3.19826752e-01\n",
      "Epoch: 7019 mean train loss:  7.29342699e-02, bound:  3.19824576e-01\n",
      "Epoch: 7020 mean train loss:  7.29078650e-02, bound:  3.19822460e-01\n",
      "Epoch: 7021 mean train loss:  7.28814527e-02, bound:  3.19820315e-01\n",
      "Epoch: 7022 mean train loss:  7.28549883e-02, bound:  3.19818199e-01\n",
      "Epoch: 7023 mean train loss:  7.28284940e-02, bound:  3.19816023e-01\n",
      "Epoch: 7024 mean train loss:  7.28021637e-02, bound:  3.19813937e-01\n",
      "Epoch: 7025 mean train loss:  7.27760866e-02, bound:  3.19811732e-01\n",
      "Epoch: 7026 mean train loss:  7.27497712e-02, bound:  3.19809645e-01\n",
      "Epoch: 7027 mean train loss:  7.27233440e-02, bound:  3.19807470e-01\n",
      "Epoch: 7028 mean train loss:  7.26969242e-02, bound:  3.19805354e-01\n",
      "Epoch: 7029 mean train loss:  7.26704672e-02, bound:  3.19803208e-01\n",
      "Epoch: 7030 mean train loss:  7.26440251e-02, bound:  3.19801122e-01\n",
      "Epoch: 7031 mean train loss:  7.26176277e-02, bound:  3.19798917e-01\n",
      "Epoch: 7032 mean train loss:  7.25913495e-02, bound:  3.19796830e-01\n",
      "Epoch: 7033 mean train loss:  7.25650042e-02, bound:  3.19794625e-01\n",
      "Epoch: 7034 mean train loss:  7.25389346e-02, bound:  3.19792598e-01\n",
      "Epoch: 7035 mean train loss:  7.25126043e-02, bound:  3.19790334e-01\n",
      "Epoch: 7036 mean train loss:  7.24866539e-02, bound:  3.19788367e-01\n",
      "Epoch: 7037 mean train loss:  7.24613443e-02, bound:  3.19786012e-01\n",
      "Epoch: 7038 mean train loss:  7.24358782e-02, bound:  3.19784105e-01\n",
      "Epoch: 7039 mean train loss:  7.24115595e-02, bound:  3.19781691e-01\n",
      "Epoch: 7040 mean train loss:  7.23886415e-02, bound:  3.19779903e-01\n",
      "Epoch: 7041 mean train loss:  7.23669156e-02, bound:  3.19777310e-01\n",
      "Epoch: 7042 mean train loss:  7.23495856e-02, bound:  3.19775760e-01\n",
      "Epoch: 7043 mean train loss:  7.23385662e-02, bound:  3.19772899e-01\n",
      "Epoch: 7044 mean train loss:  7.23383799e-02, bound:  3.19771707e-01\n",
      "Epoch: 7045 mean train loss:  7.23558962e-02, bound:  3.19768310e-01\n",
      "Epoch: 7046 mean train loss:  7.24045709e-02, bound:  3.19767803e-01\n",
      "Epoch: 7047 mean train loss:  7.25042298e-02, bound:  3.19763571e-01\n",
      "Epoch: 7048 mean train loss:  7.26811439e-02, bound:  3.19764167e-01\n",
      "Epoch: 7049 mean train loss:  7.29653612e-02, bound:  3.19758594e-01\n",
      "Epoch: 7050 mean train loss:  7.33543038e-02, bound:  3.19760740e-01\n",
      "Epoch: 7051 mean train loss:  7.37695619e-02, bound:  3.19753557e-01\n",
      "Epoch: 7052 mean train loss:  7.39683509e-02, bound:  3.19757193e-01\n",
      "Epoch: 7053 mean train loss:  7.36869350e-02, bound:  3.19749594e-01\n",
      "Epoch: 7054 mean train loss:  7.29248747e-02, bound:  3.19752455e-01\n",
      "Epoch: 7055 mean train loss:  7.21856877e-02, bound:  3.19747746e-01\n",
      "Epoch: 7056 mean train loss:  7.19885454e-02, bound:  3.19746703e-01\n",
      "Epoch: 7057 mean train loss:  7.23267421e-02, bound:  3.19746584e-01\n",
      "Epoch: 7058 mean train loss:  7.27180168e-02, bound:  3.19741607e-01\n",
      "Epoch: 7059 mean train loss:  7.27031901e-02, bound:  3.19743633e-01\n",
      "Epoch: 7060 mean train loss:  7.22852871e-02, bound:  3.19738448e-01\n",
      "Epoch: 7061 mean train loss:  7.18922615e-02, bound:  3.19738269e-01\n",
      "Epoch: 7062 mean train loss:  7.18714744e-02, bound:  3.19736272e-01\n",
      "Epoch: 7063 mean train loss:  7.21132904e-02, bound:  3.19732547e-01\n",
      "Epoch: 7064 mean train loss:  7.22488612e-02, bound:  3.19733113e-01\n",
      "Epoch: 7065 mean train loss:  7.20920190e-02, bound:  3.19728404e-01\n",
      "Epoch: 7066 mean train loss:  7.18137696e-02, bound:  3.19728255e-01\n",
      "Epoch: 7067 mean train loss:  7.16986507e-02, bound:  3.19725752e-01\n",
      "Epoch: 7068 mean train loss:  7.17969984e-02, bound:  3.19722950e-01\n",
      "Epoch: 7069 mean train loss:  7.19111338e-02, bound:  3.19722742e-01\n",
      "Epoch: 7070 mean train loss:  7.18626380e-02, bound:  3.19718689e-01\n",
      "Epoch: 7071 mean train loss:  7.16924742e-02, bound:  3.19718391e-01\n",
      "Epoch: 7072 mean train loss:  7.15750754e-02, bound:  3.19715649e-01\n",
      "Epoch: 7073 mean train loss:  7.15943053e-02, bound:  3.19713295e-01\n",
      "Epoch: 7074 mean train loss:  7.16622397e-02, bound:  3.19712520e-01\n",
      "Epoch: 7075 mean train loss:  7.16524199e-02, bound:  3.19708914e-01\n",
      "Epoch: 7076 mean train loss:  7.15524256e-02, bound:  3.19708258e-01\n",
      "Epoch: 7077 mean train loss:  7.14578927e-02, bound:  3.19705456e-01\n",
      "Epoch: 7078 mean train loss:  7.14414939e-02, bound:  3.19703400e-01\n",
      "Epoch: 7079 mean train loss:  7.14720413e-02, bound:  3.19702208e-01\n",
      "Epoch: 7080 mean train loss:  7.14712143e-02, bound:  3.19699079e-01\n",
      "Epoch: 7081 mean train loss:  7.14116171e-02, bound:  3.19698274e-01\n",
      "Epoch: 7082 mean train loss:  7.13400692e-02, bound:  3.19695622e-01\n",
      "Epoch: 7083 mean train loss:  7.13078007e-02, bound:  3.19693834e-01\n",
      "Epoch: 7084 mean train loss:  7.13122860e-02, bound:  3.19692373e-01\n",
      "Epoch: 7085 mean train loss:  7.13100135e-02, bound:  3.19689572e-01\n",
      "Epoch: 7086 mean train loss:  7.12739527e-02, bound:  3.19688529e-01\n",
      "Epoch: 7087 mean train loss:  7.12207034e-02, bound:  3.19685847e-01\n",
      "Epoch: 7088 mean train loss:  7.11826533e-02, bound:  3.19684148e-01\n",
      "Epoch: 7089 mean train loss:  7.11694956e-02, bound:  3.19682389e-01\n",
      "Epoch: 7090 mean train loss:  7.11623728e-02, bound:  3.19679856e-01\n",
      "Epoch: 7091 mean train loss:  7.11388364e-02, bound:  3.19678605e-01\n",
      "Epoch: 7092 mean train loss:  7.10999221e-02, bound:  3.19676042e-01\n",
      "Epoch: 7093 mean train loss:  7.10622445e-02, bound:  3.19674432e-01\n",
      "Epoch: 7094 mean train loss:  7.10382462e-02, bound:  3.19672495e-01\n",
      "Epoch: 7095 mean train loss:  7.10234717e-02, bound:  3.19670200e-01\n",
      "Epoch: 7096 mean train loss:  7.10053071e-02, bound:  3.19668710e-01\n",
      "Epoch: 7097 mean train loss:  7.09766150e-02, bound:  3.19666237e-01\n",
      "Epoch: 7098 mean train loss:  7.09430277e-02, bound:  3.19664598e-01\n",
      "Epoch: 7099 mean train loss:  7.09139705e-02, bound:  3.19662482e-01\n",
      "Epoch: 7100 mean train loss:  7.08924755e-02, bound:  3.19660395e-01\n",
      "Epoch: 7101 mean train loss:  7.08736181e-02, bound:  3.19658697e-01\n",
      "Epoch: 7102 mean train loss:  7.08507672e-02, bound:  3.19656312e-01\n",
      "Epoch: 7103 mean train loss:  7.08225444e-02, bound:  3.19654673e-01\n",
      "Epoch: 7104 mean train loss:  7.07932115e-02, bound:  3.19652468e-01\n",
      "Epoch: 7105 mean train loss:  7.07674772e-02, bound:  3.19650531e-01\n",
      "Epoch: 7106 mean train loss:  7.07454309e-02, bound:  3.19648713e-01\n",
      "Epoch: 7107 mean train loss:  7.07230642e-02, bound:  3.19646508e-01\n",
      "Epoch: 7108 mean train loss:  7.06993118e-02, bound:  3.19644809e-01\n",
      "Epoch: 7109 mean train loss:  7.06722885e-02, bound:  3.19642574e-01\n",
      "Epoch: 7110 mean train loss:  7.06452653e-02, bound:  3.19640785e-01\n",
      "Epoch: 7111 mean train loss:  7.06205666e-02, bound:  3.19638819e-01\n",
      "Epoch: 7112 mean train loss:  7.05976859e-02, bound:  3.19636703e-01\n",
      "Epoch: 7113 mean train loss:  7.05742165e-02, bound:  3.19634944e-01\n",
      "Epoch: 7114 mean train loss:  7.05496371e-02, bound:  3.19632739e-01\n",
      "Epoch: 7115 mean train loss:  7.05242753e-02, bound:  3.19630951e-01\n",
      "Epoch: 7116 mean train loss:  7.04983920e-02, bound:  3.19628894e-01\n",
      "Epoch: 7117 mean train loss:  7.04737827e-02, bound:  3.19626957e-01\n",
      "Epoch: 7118 mean train loss:  7.04496726e-02, bound:  3.19625050e-01\n",
      "Epoch: 7119 mean train loss:  7.04258457e-02, bound:  3.19622934e-01\n",
      "Epoch: 7120 mean train loss:  7.04016760e-02, bound:  3.19621116e-01\n",
      "Epoch: 7121 mean train loss:  7.03763738e-02, bound:  3.19619000e-01\n",
      "Epoch: 7122 mean train loss:  7.03511760e-02, bound:  3.19617152e-01\n",
      "Epoch: 7123 mean train loss:  7.03265518e-02, bound:  3.19615126e-01\n",
      "Epoch: 7124 mean train loss:  7.03020245e-02, bound:  3.19613129e-01\n",
      "Epoch: 7125 mean train loss:  7.02781081e-02, bound:  3.19611281e-01\n",
      "Epoch: 7126 mean train loss:  7.02536255e-02, bound:  3.19609135e-01\n",
      "Epoch: 7127 mean train loss:  7.02287555e-02, bound:  3.19607317e-01\n",
      "Epoch: 7128 mean train loss:  7.02039897e-02, bound:  3.19605231e-01\n",
      "Epoch: 7129 mean train loss:  7.01793432e-02, bound:  3.19603354e-01\n",
      "Epoch: 7130 mean train loss:  7.01547340e-02, bound:  3.19601357e-01\n",
      "Epoch: 7131 mean train loss:  7.01300800e-02, bound:  3.19599360e-01\n",
      "Epoch: 7132 mean train loss:  7.01062381e-02, bound:  3.19597483e-01\n",
      "Epoch: 7133 mean train loss:  7.00813308e-02, bound:  3.19595397e-01\n",
      "Epoch: 7134 mean train loss:  7.00569376e-02, bound:  3.19593489e-01\n",
      "Epoch: 7135 mean train loss:  7.00321198e-02, bound:  3.19591463e-01\n",
      "Epoch: 7136 mean train loss:  7.00075477e-02, bound:  3.19589525e-01\n",
      "Epoch: 7137 mean train loss:  6.99829161e-02, bound:  3.19587559e-01\n",
      "Epoch: 7138 mean train loss:  6.99582696e-02, bound:  3.19585562e-01\n",
      "Epoch: 7139 mean train loss:  6.99342489e-02, bound:  3.19583625e-01\n",
      "Epoch: 7140 mean train loss:  6.99097514e-02, bound:  3.19581598e-01\n",
      "Epoch: 7141 mean train loss:  6.98848814e-02, bound:  3.19579691e-01\n",
      "Epoch: 7142 mean train loss:  6.98601604e-02, bound:  3.19577664e-01\n",
      "Epoch: 7143 mean train loss:  6.98354691e-02, bound:  3.19575727e-01\n",
      "Epoch: 7144 mean train loss:  6.98107406e-02, bound:  3.19573760e-01\n",
      "Epoch: 7145 mean train loss:  6.97861761e-02, bound:  3.19571763e-01\n",
      "Epoch: 7146 mean train loss:  6.97620437e-02, bound:  3.19569826e-01\n",
      "Epoch: 7147 mean train loss:  6.97374493e-02, bound:  3.19567800e-01\n",
      "Epoch: 7148 mean train loss:  6.97124973e-02, bound:  3.19565862e-01\n",
      "Epoch: 7149 mean train loss:  6.96878359e-02, bound:  3.19563866e-01\n",
      "Epoch: 7150 mean train loss:  6.96632639e-02, bound:  3.19561869e-01\n",
      "Epoch: 7151 mean train loss:  6.96385875e-02, bound:  3.19559902e-01\n",
      "Epoch: 7152 mean train loss:  6.96140081e-02, bound:  3.19557935e-01\n",
      "Epoch: 7153 mean train loss:  6.95894063e-02, bound:  3.19555998e-01\n",
      "Epoch: 7154 mean train loss:  6.95649534e-02, bound:  3.19553971e-01\n",
      "Epoch: 7155 mean train loss:  6.95401728e-02, bound:  3.19552034e-01\n",
      "Epoch: 7156 mean train loss:  6.95155933e-02, bound:  3.19550008e-01\n",
      "Epoch: 7157 mean train loss:  6.94911405e-02, bound:  3.19548100e-01\n",
      "Epoch: 7158 mean train loss:  6.94665387e-02, bound:  3.19546074e-01\n",
      "Epoch: 7159 mean train loss:  6.94417879e-02, bound:  3.19544137e-01\n",
      "Epoch: 7160 mean train loss:  6.94173127e-02, bound:  3.19542170e-01\n",
      "Epoch: 7161 mean train loss:  6.93925396e-02, bound:  3.19540173e-01\n",
      "Epoch: 7162 mean train loss:  6.93679154e-02, bound:  3.19538206e-01\n",
      "Epoch: 7163 mean train loss:  6.93434626e-02, bound:  3.19536209e-01\n",
      "Epoch: 7164 mean train loss:  6.93187714e-02, bound:  3.19534272e-01\n",
      "Epoch: 7165 mean train loss:  6.92942142e-02, bound:  3.19532275e-01\n",
      "Epoch: 7166 mean train loss:  6.92693517e-02, bound:  3.19530308e-01\n",
      "Epoch: 7167 mean train loss:  6.92446530e-02, bound:  3.19528311e-01\n",
      "Epoch: 7168 mean train loss:  6.92198128e-02, bound:  3.19526345e-01\n",
      "Epoch: 7169 mean train loss:  6.91951662e-02, bound:  3.19524407e-01\n",
      "Epoch: 7170 mean train loss:  6.91706762e-02, bound:  3.19522411e-01\n",
      "Epoch: 7171 mean train loss:  6.91459775e-02, bound:  3.19520444e-01\n",
      "Epoch: 7172 mean train loss:  6.91215619e-02, bound:  3.19518447e-01\n",
      "Epoch: 7173 mean train loss:  6.90968484e-02, bound:  3.19516480e-01\n",
      "Epoch: 7174 mean train loss:  6.90719634e-02, bound:  3.19514483e-01\n",
      "Epoch: 7175 mean train loss:  6.90473244e-02, bound:  3.19512546e-01\n",
      "Epoch: 7176 mean train loss:  6.90225959e-02, bound:  3.19510520e-01\n",
      "Epoch: 7177 mean train loss:  6.89980611e-02, bound:  3.19508582e-01\n",
      "Epoch: 7178 mean train loss:  6.89731687e-02, bound:  3.19506615e-01\n",
      "Epoch: 7179 mean train loss:  6.89485446e-02, bound:  3.19504619e-01\n",
      "Epoch: 7180 mean train loss:  6.89237416e-02, bound:  3.19502652e-01\n",
      "Epoch: 7181 mean train loss:  6.88991472e-02, bound:  3.19500655e-01\n",
      "Epoch: 7182 mean train loss:  6.88740909e-02, bound:  3.19498688e-01\n",
      "Epoch: 7183 mean train loss:  6.88496232e-02, bound:  3.19496691e-01\n",
      "Epoch: 7184 mean train loss:  6.88252673e-02, bound:  3.19494754e-01\n",
      "Epoch: 7185 mean train loss:  6.88000768e-02, bound:  3.19492787e-01\n",
      "Epoch: 7186 mean train loss:  6.87755197e-02, bound:  3.19490790e-01\n",
      "Epoch: 7187 mean train loss:  6.87509999e-02, bound:  3.19488823e-01\n",
      "Epoch: 7188 mean train loss:  6.87259883e-02, bound:  3.19486827e-01\n",
      "Epoch: 7189 mean train loss:  6.87012449e-02, bound:  3.19484860e-01\n",
      "Epoch: 7190 mean train loss:  6.86768666e-02, bound:  3.19482863e-01\n",
      "Epoch: 7191 mean train loss:  6.86516911e-02, bound:  3.19480896e-01\n",
      "Epoch: 7192 mean train loss:  6.86269999e-02, bound:  3.19478899e-01\n",
      "Epoch: 7193 mean train loss:  6.86022639e-02, bound:  3.19476902e-01\n",
      "Epoch: 7194 mean train loss:  6.85774907e-02, bound:  3.19474995e-01\n",
      "Epoch: 7195 mean train loss:  6.85526058e-02, bound:  3.19472969e-01\n",
      "Epoch: 7196 mean train loss:  6.85280859e-02, bound:  3.19470972e-01\n",
      "Epoch: 7197 mean train loss:  6.85034022e-02, bound:  3.19469005e-01\n",
      "Epoch: 7198 mean train loss:  6.84784800e-02, bound:  3.19467068e-01\n",
      "Epoch: 7199 mean train loss:  6.84536323e-02, bound:  3.19465041e-01\n",
      "Epoch: 7200 mean train loss:  6.84289411e-02, bound:  3.19463074e-01\n",
      "Epoch: 7201 mean train loss:  6.84042498e-02, bound:  3.19461077e-01\n",
      "Epoch: 7202 mean train loss:  6.83794469e-02, bound:  3.19459110e-01\n",
      "Epoch: 7203 mean train loss:  6.83546513e-02, bound:  3.19457144e-01\n",
      "Epoch: 7204 mean train loss:  6.83299154e-02, bound:  3.19455147e-01\n",
      "Epoch: 7205 mean train loss:  6.83050007e-02, bound:  3.19453150e-01\n",
      "Epoch: 7206 mean train loss:  6.82805181e-02, bound:  3.19451183e-01\n",
      "Epoch: 7207 mean train loss:  6.82553723e-02, bound:  3.19449216e-01\n",
      "Epoch: 7208 mean train loss:  6.82307705e-02, bound:  3.19447219e-01\n",
      "Epoch: 7209 mean train loss:  6.82058409e-02, bound:  3.19445282e-01\n",
      "Epoch: 7210 mean train loss:  6.81811050e-02, bound:  3.19443256e-01\n",
      "Epoch: 7211 mean train loss:  6.81566000e-02, bound:  3.19441319e-01\n",
      "Epoch: 7212 mean train loss:  6.81315213e-02, bound:  3.19439292e-01\n",
      "Epoch: 7213 mean train loss:  6.81068376e-02, bound:  3.19437325e-01\n",
      "Epoch: 7214 mean train loss:  6.80821687e-02, bound:  3.19435328e-01\n",
      "Epoch: 7215 mean train loss:  6.80571124e-02, bound:  3.19433391e-01\n",
      "Epoch: 7216 mean train loss:  6.80325329e-02, bound:  3.19431335e-01\n",
      "Epoch: 7217 mean train loss:  6.80080727e-02, bound:  3.19429457e-01\n",
      "Epoch: 7218 mean train loss:  6.79834187e-02, bound:  3.19427371e-01\n",
      "Epoch: 7219 mean train loss:  6.79587722e-02, bound:  3.19425523e-01\n",
      "Epoch: 7220 mean train loss:  6.79343417e-02, bound:  3.19423378e-01\n",
      "Epoch: 7221 mean train loss:  6.79109097e-02, bound:  3.19421560e-01\n",
      "Epoch: 7222 mean train loss:  6.78870305e-02, bound:  3.19419384e-01\n",
      "Epoch: 7223 mean train loss:  6.78643659e-02, bound:  3.19417655e-01\n",
      "Epoch: 7224 mean train loss:  6.78427070e-02, bound:  3.19415331e-01\n",
      "Epoch: 7225 mean train loss:  6.78234994e-02, bound:  3.19413811e-01\n",
      "Epoch: 7226 mean train loss:  6.78071603e-02, bound:  3.19411278e-01\n",
      "Epoch: 7227 mean train loss:  6.77965358e-02, bound:  3.19409966e-01\n",
      "Epoch: 7228 mean train loss:  6.77955821e-02, bound:  3.19407105e-01\n",
      "Epoch: 7229 mean train loss:  6.78095594e-02, bound:  3.19406241e-01\n",
      "Epoch: 7230 mean train loss:  6.78484738e-02, bound:  3.19402844e-01\n",
      "Epoch: 7231 mean train loss:  6.79278821e-02, bound:  3.19402665e-01\n",
      "Epoch: 7232 mean train loss:  6.80683181e-02, bound:  3.19398373e-01\n",
      "Epoch: 7233 mean train loss:  6.82914555e-02, bound:  3.19399357e-01\n",
      "Epoch: 7234 mean train loss:  6.86071664e-02, bound:  3.19393784e-01\n",
      "Epoch: 7235 mean train loss:  6.89659193e-02, bound:  3.19396138e-01\n",
      "Epoch: 7236 mean train loss:  6.92293942e-02, bound:  3.19389343e-01\n",
      "Epoch: 7237 mean train loss:  6.91576675e-02, bound:  3.19392562e-01\n",
      "Epoch: 7238 mean train loss:  6.86489493e-02, bound:  3.19386154e-01\n",
      "Epoch: 7239 mean train loss:  6.79291934e-02, bound:  3.19387883e-01\n",
      "Epoch: 7240 mean train loss:  6.74761385e-02, bound:  3.19384605e-01\n",
      "Epoch: 7241 mean train loss:  6.75290823e-02, bound:  3.19382727e-01\n",
      "Epoch: 7242 mean train loss:  6.78850263e-02, bound:  3.19383115e-01\n",
      "Epoch: 7243 mean train loss:  6.81299791e-02, bound:  3.19378346e-01\n",
      "Epoch: 7244 mean train loss:  6.80015832e-02, bound:  3.19379836e-01\n",
      "Epoch: 7245 mean train loss:  6.76173121e-02, bound:  3.19375426e-01\n",
      "Epoch: 7246 mean train loss:  6.73254952e-02, bound:  3.19374859e-01\n",
      "Epoch: 7247 mean train loss:  6.73372895e-02, bound:  3.19373190e-01\n",
      "Epoch: 7248 mean train loss:  6.75341785e-02, bound:  3.19369793e-01\n",
      "Epoch: 7249 mean train loss:  6.76452443e-02, bound:  3.19370121e-01\n",
      "Epoch: 7250 mean train loss:  6.75317645e-02, bound:  3.19365919e-01\n",
      "Epoch: 7251 mean train loss:  6.73011839e-02, bound:  3.19365740e-01\n",
      "Epoch: 7252 mean train loss:  6.71642721e-02, bound:  3.19363147e-01\n",
      "Epoch: 7253 mean train loss:  6.72013760e-02, bound:  3.19360882e-01\n",
      "Epoch: 7254 mean train loss:  6.73037469e-02, bound:  3.19360316e-01\n",
      "Epoch: 7255 mean train loss:  6.73198551e-02, bound:  3.19356710e-01\n",
      "Epoch: 7256 mean train loss:  6.72129616e-02, bound:  3.19356501e-01\n",
      "Epoch: 7257 mean train loss:  6.70790225e-02, bound:  3.19353521e-01\n",
      "Epoch: 7258 mean train loss:  6.70262873e-02, bound:  3.19351941e-01\n",
      "Epoch: 7259 mean train loss:  6.70597255e-02, bound:  3.19350660e-01\n",
      "Epoch: 7260 mean train loss:  6.70973659e-02, bound:  3.19347650e-01\n",
      "Epoch: 7261 mean train loss:  6.70686513e-02, bound:  3.19347054e-01\n",
      "Epoch: 7262 mean train loss:  6.69852570e-02, bound:  3.19344074e-01\n",
      "Epoch: 7263 mean train loss:  6.69126213e-02, bound:  3.19342822e-01\n",
      "Epoch: 7264 mean train loss:  6.68942556e-02, bound:  3.19341063e-01\n",
      "Epoch: 7265 mean train loss:  6.69100508e-02, bound:  3.19338620e-01\n",
      "Epoch: 7266 mean train loss:  6.69096261e-02, bound:  3.19337755e-01\n",
      "Epoch: 7267 mean train loss:  6.68695122e-02, bound:  3.19335014e-01\n",
      "Epoch: 7268 mean train loss:  6.68101832e-02, bound:  3.19333881e-01\n",
      "Epoch: 7269 mean train loss:  6.67682290e-02, bound:  3.19331825e-01\n",
      "Epoch: 7270 mean train loss:  6.67555183e-02, bound:  3.19329798e-01\n",
      "Epoch: 7271 mean train loss:  6.67540878e-02, bound:  3.19328517e-01\n",
      "Epoch: 7272 mean train loss:  6.67381808e-02, bound:  3.19325954e-01\n",
      "Epoch: 7273 mean train loss:  6.67019114e-02, bound:  3.19324821e-01\n",
      "Epoch: 7274 mean train loss:  6.66594431e-02, bound:  3.19322467e-01\n",
      "Epoch: 7275 mean train loss:  6.66286796e-02, bound:  3.19320798e-01\n",
      "Epoch: 7276 mean train loss:  6.66126087e-02, bound:  3.19319099e-01\n",
      "Epoch: 7277 mean train loss:  6.66011572e-02, bound:  3.19316834e-01\n",
      "Epoch: 7278 mean train loss:  6.65807202e-02, bound:  3.19315583e-01\n",
      "Epoch: 7279 mean train loss:  6.65502101e-02, bound:  3.19313169e-01\n",
      "Epoch: 7280 mean train loss:  6.65165335e-02, bound:  3.19311708e-01\n",
      "Epoch: 7281 mean train loss:  6.64892048e-02, bound:  3.19309741e-01\n",
      "Epoch: 7282 mean train loss:  6.64691180e-02, bound:  3.19307774e-01\n",
      "Epoch: 7283 mean train loss:  6.64523989e-02, bound:  3.19306225e-01\n",
      "Epoch: 7284 mean train loss:  6.64310902e-02, bound:  3.19303960e-01\n",
      "Epoch: 7285 mean train loss:  6.64043874e-02, bound:  3.19302469e-01\n",
      "Epoch: 7286 mean train loss:  6.63755909e-02, bound:  3.19300294e-01\n",
      "Epoch: 7287 mean train loss:  6.63493276e-02, bound:  3.19298565e-01\n",
      "Epoch: 7288 mean train loss:  6.63272962e-02, bound:  3.19296777e-01\n",
      "Epoch: 7289 mean train loss:  6.63072914e-02, bound:  3.19294691e-01\n",
      "Epoch: 7290 mean train loss:  6.62857890e-02, bound:  3.19293112e-01\n",
      "Epoch: 7291 mean train loss:  6.62615746e-02, bound:  3.19291025e-01\n",
      "Epoch: 7292 mean train loss:  6.62356317e-02, bound:  3.19289386e-01\n",
      "Epoch: 7293 mean train loss:  6.62102029e-02, bound:  3.19287419e-01\n",
      "Epoch: 7294 mean train loss:  6.61861077e-02, bound:  3.19285631e-01\n",
      "Epoch: 7295 mean train loss:  6.61642104e-02, bound:  3.19283843e-01\n",
      "Epoch: 7296 mean train loss:  6.61422834e-02, bound:  3.19281816e-01\n",
      "Epoch: 7297 mean train loss:  6.61196485e-02, bound:  3.19280207e-01\n",
      "Epoch: 7298 mean train loss:  6.60950616e-02, bound:  3.19278181e-01\n",
      "Epoch: 7299 mean train loss:  6.60705790e-02, bound:  3.19276452e-01\n",
      "Epoch: 7300 mean train loss:  6.60461187e-02, bound:  3.19274545e-01\n",
      "Epoch: 7301 mean train loss:  6.60229921e-02, bound:  3.19272697e-01\n",
      "Epoch: 7302 mean train loss:  6.60004988e-02, bound:  3.19270909e-01\n",
      "Epoch: 7303 mean train loss:  6.59774989e-02, bound:  3.19268942e-01\n",
      "Epoch: 7304 mean train loss:  6.59546256e-02, bound:  3.19267273e-01\n",
      "Epoch: 7305 mean train loss:  6.59304708e-02, bound:  3.19265246e-01\n",
      "Epoch: 7306 mean train loss:  6.59063756e-02, bound:  3.19263488e-01\n",
      "Epoch: 7307 mean train loss:  6.58827275e-02, bound:  3.19261611e-01\n",
      "Epoch: 7308 mean train loss:  6.58590943e-02, bound:  3.19259763e-01\n",
      "Epoch: 7309 mean train loss:  6.58361316e-02, bound:  3.19257945e-01\n",
      "Epoch: 7310 mean train loss:  6.58134297e-02, bound:  3.19256037e-01\n",
      "Epoch: 7311 mean train loss:  6.57898337e-02, bound:  3.19254309e-01\n",
      "Epoch: 7312 mean train loss:  6.57663122e-02, bound:  3.19252342e-01\n",
      "Epoch: 7313 mean train loss:  6.57424852e-02, bound:  3.19250554e-01\n",
      "Epoch: 7314 mean train loss:  6.57188371e-02, bound:  3.19248646e-01\n",
      "Epoch: 7315 mean train loss:  6.56953231e-02, bound:  3.19246799e-01\n",
      "Epoch: 7316 mean train loss:  6.56720698e-02, bound:  3.19244951e-01\n",
      "Epoch: 7317 mean train loss:  6.56487793e-02, bound:  3.19243103e-01\n",
      "Epoch: 7318 mean train loss:  6.56254292e-02, bound:  3.19241315e-01\n",
      "Epoch: 7319 mean train loss:  6.56020194e-02, bound:  3.19239378e-01\n",
      "Epoch: 7320 mean train loss:  6.55788183e-02, bound:  3.19237560e-01\n",
      "Epoch: 7321 mean train loss:  6.55548275e-02, bound:  3.19235682e-01\n",
      "Epoch: 7322 mean train loss:  6.55314773e-02, bound:  3.19233865e-01\n",
      "Epoch: 7323 mean train loss:  6.55078217e-02, bound:  3.19231987e-01\n",
      "Epoch: 7324 mean train loss:  6.54844344e-02, bound:  3.19230139e-01\n",
      "Epoch: 7325 mean train loss:  6.54611140e-02, bound:  3.19228292e-01\n",
      "Epoch: 7326 mean train loss:  6.54378459e-02, bound:  3.19226414e-01\n",
      "Epoch: 7327 mean train loss:  6.54141679e-02, bound:  3.19224626e-01\n",
      "Epoch: 7328 mean train loss:  6.53906539e-02, bound:  3.19222718e-01\n",
      "Epoch: 7329 mean train loss:  6.53673410e-02, bound:  3.19220871e-01\n",
      "Epoch: 7330 mean train loss:  6.53435290e-02, bound:  3.19218993e-01\n",
      "Epoch: 7331 mean train loss:  6.53200299e-02, bound:  3.19217205e-01\n",
      "Epoch: 7332 mean train loss:  6.52965233e-02, bound:  3.19215298e-01\n",
      "Epoch: 7333 mean train loss:  6.52733073e-02, bound:  3.19213450e-01\n",
      "Epoch: 7334 mean train loss:  6.52494133e-02, bound:  3.19211602e-01\n",
      "Epoch: 7335 mean train loss:  6.52261376e-02, bound:  3.19209725e-01\n",
      "Epoch: 7336 mean train loss:  6.52027726e-02, bound:  3.19207937e-01\n",
      "Epoch: 7337 mean train loss:  6.51790425e-02, bound:  3.19206029e-01\n",
      "Epoch: 7338 mean train loss:  6.51558116e-02, bound:  3.19204181e-01\n",
      "Epoch: 7339 mean train loss:  6.51319623e-02, bound:  3.19202304e-01\n",
      "Epoch: 7340 mean train loss:  6.51083142e-02, bound:  3.19200456e-01\n",
      "Epoch: 7341 mean train loss:  6.50851652e-02, bound:  3.19198579e-01\n",
      "Epoch: 7342 mean train loss:  6.50613755e-02, bound:  3.19196731e-01\n",
      "Epoch: 7343 mean train loss:  6.50379285e-02, bound:  3.19194883e-01\n",
      "Epoch: 7344 mean train loss:  6.50147125e-02, bound:  3.19193006e-01\n",
      "Epoch: 7345 mean train loss:  6.49908930e-02, bound:  3.19191188e-01\n",
      "Epoch: 7346 mean train loss:  6.49671704e-02, bound:  3.19189310e-01\n",
      "Epoch: 7347 mean train loss:  6.49438351e-02, bound:  3.19187462e-01\n",
      "Epoch: 7348 mean train loss:  6.49201348e-02, bound:  3.19185555e-01\n",
      "Epoch: 7349 mean train loss:  6.48967475e-02, bound:  3.19183707e-01\n",
      "Epoch: 7350 mean train loss:  6.48727790e-02, bound:  3.19181830e-01\n",
      "Epoch: 7351 mean train loss:  6.48493394e-02, bound:  3.19179982e-01\n",
      "Epoch: 7352 mean train loss:  6.48259968e-02, bound:  3.19178134e-01\n",
      "Epoch: 7353 mean train loss:  6.48023635e-02, bound:  3.19176286e-01\n",
      "Epoch: 7354 mean train loss:  6.47786185e-02, bound:  3.19174409e-01\n",
      "Epoch: 7355 mean train loss:  6.47549927e-02, bound:  3.19172561e-01\n",
      "Epoch: 7356 mean train loss:  6.47314787e-02, bound:  3.19170713e-01\n",
      "Epoch: 7357 mean train loss:  6.47077858e-02, bound:  3.19168836e-01\n",
      "Epoch: 7358 mean train loss:  6.46842197e-02, bound:  3.19166988e-01\n",
      "Epoch: 7359 mean train loss:  6.46604970e-02, bound:  3.19165111e-01\n",
      "Epoch: 7360 mean train loss:  6.46371022e-02, bound:  3.19163263e-01\n",
      "Epoch: 7361 mean train loss:  6.46131709e-02, bound:  3.19161326e-01\n",
      "Epoch: 7362 mean train loss:  6.45900890e-02, bound:  3.19159478e-01\n",
      "Epoch: 7363 mean train loss:  6.45659566e-02, bound:  3.19157630e-01\n",
      "Epoch: 7364 mean train loss:  6.45427704e-02, bound:  3.19155812e-01\n",
      "Epoch: 7365 mean train loss:  6.45187646e-02, bound:  3.19153905e-01\n",
      "Epoch: 7366 mean train loss:  6.44951835e-02, bound:  3.19152057e-01\n",
      "Epoch: 7367 mean train loss:  6.44715428e-02, bound:  3.19150180e-01\n",
      "Epoch: 7368 mean train loss:  6.44476786e-02, bound:  3.19148332e-01\n",
      "Epoch: 7369 mean train loss:  6.44242689e-02, bound:  3.19146484e-01\n",
      "Epoch: 7370 mean train loss:  6.44004643e-02, bound:  3.19144607e-01\n",
      "Epoch: 7371 mean train loss:  6.43767342e-02, bound:  3.19142729e-01\n",
      "Epoch: 7372 mean train loss:  6.43533319e-02, bound:  3.19140881e-01\n",
      "Epoch: 7373 mean train loss:  6.43293411e-02, bound:  3.19139004e-01\n",
      "Epoch: 7374 mean train loss:  6.43056929e-02, bound:  3.19137126e-01\n",
      "Epoch: 7375 mean train loss:  6.42823502e-02, bound:  3.19135278e-01\n",
      "Epoch: 7376 mean train loss:  6.42583966e-02, bound:  3.19133401e-01\n",
      "Epoch: 7377 mean train loss:  6.42343983e-02, bound:  3.19131553e-01\n",
      "Epoch: 7378 mean train loss:  6.42112195e-02, bound:  3.19129646e-01\n",
      "Epoch: 7379 mean train loss:  6.41870201e-02, bound:  3.19127828e-01\n",
      "Epoch: 7380 mean train loss:  6.41634613e-02, bound:  3.19125921e-01\n",
      "Epoch: 7381 mean train loss:  6.41401038e-02, bound:  3.19124073e-01\n",
      "Epoch: 7382 mean train loss:  6.41163513e-02, bound:  3.19122165e-01\n",
      "Epoch: 7383 mean train loss:  6.40923306e-02, bound:  3.19120347e-01\n",
      "Epoch: 7384 mean train loss:  6.40689209e-02, bound:  3.19118440e-01\n",
      "Epoch: 7385 mean train loss:  6.40455931e-02, bound:  3.19116652e-01\n",
      "Epoch: 7386 mean train loss:  6.40217960e-02, bound:  3.19114715e-01\n",
      "Epoch: 7387 mean train loss:  6.39984086e-02, bound:  3.19112927e-01\n",
      "Epoch: 7388 mean train loss:  6.39754534e-02, bound:  3.19110930e-01\n",
      "Epoch: 7389 mean train loss:  6.39521778e-02, bound:  3.19109231e-01\n",
      "Epoch: 7390 mean train loss:  6.39297441e-02, bound:  3.19107145e-01\n",
      "Epoch: 7391 mean train loss:  6.39080480e-02, bound:  3.19105506e-01\n",
      "Epoch: 7392 mean train loss:  6.38876185e-02, bound:  3.19103360e-01\n",
      "Epoch: 7393 mean train loss:  6.38691112e-02, bound:  3.19101870e-01\n",
      "Epoch: 7394 mean train loss:  6.38544559e-02, bound:  3.19099516e-01\n",
      "Epoch: 7395 mean train loss:  6.38456568e-02, bound:  3.19098294e-01\n",
      "Epoch: 7396 mean train loss:  6.38465062e-02, bound:  3.19095612e-01\n",
      "Epoch: 7397 mean train loss:  6.38646707e-02, bound:  3.19094747e-01\n",
      "Epoch: 7398 mean train loss:  6.39116764e-02, bound:  3.19091529e-01\n",
      "Epoch: 7399 mean train loss:  6.40052035e-02, bound:  3.19091439e-01\n",
      "Epoch: 7400 mean train loss:  6.41715750e-02, bound:  3.19087297e-01\n",
      "Epoch: 7401 mean train loss:  6.44383505e-02, bound:  3.19088340e-01\n",
      "Epoch: 7402 mean train loss:  6.48203567e-02, bound:  3.19082886e-01\n",
      "Epoch: 7403 mean train loss:  6.52536228e-02, bound:  3.19085389e-01\n",
      "Epoch: 7404 mean train loss:  6.55555576e-02, bound:  3.19078654e-01\n",
      "Epoch: 7405 mean train loss:  6.54231012e-02, bound:  3.19081992e-01\n",
      "Epoch: 7406 mean train loss:  6.47625774e-02, bound:  3.19075823e-01\n",
      "Epoch: 7407 mean train loss:  6.39269352e-02, bound:  3.19077402e-01\n",
      "Epoch: 7408 mean train loss:  6.35147840e-02, bound:  3.19074690e-01\n",
      "Epoch: 7409 mean train loss:  6.37168139e-02, bound:  3.19072396e-01\n",
      "Epoch: 7410 mean train loss:  6.41643628e-02, bound:  3.19073319e-01\n",
      "Epoch: 7411 mean train loss:  6.43336326e-02, bound:  3.19068462e-01\n",
      "Epoch: 7412 mean train loss:  6.40302449e-02, bound:  3.19069952e-01\n",
      "Epoch: 7413 mean train loss:  6.35647923e-02, bound:  3.19066107e-01\n",
      "Epoch: 7414 mean train loss:  6.33746833e-02, bound:  3.19065034e-01\n",
      "Epoch: 7415 mean train loss:  6.35451898e-02, bound:  3.19064170e-01\n",
      "Epoch: 7416 mean train loss:  6.37789965e-02, bound:  3.19060385e-01\n",
      "Epoch: 7417 mean train loss:  6.37701601e-02, bound:  3.19061011e-01\n",
      "Epoch: 7418 mean train loss:  6.35191798e-02, bound:  3.19057167e-01\n",
      "Epoch: 7419 mean train loss:  6.32872581e-02, bound:  3.19056481e-01\n",
      "Epoch: 7420 mean train loss:  6.32725507e-02, bound:  3.19054723e-01\n",
      "Epoch: 7421 mean train loss:  6.34067878e-02, bound:  3.19051832e-01\n",
      "Epoch: 7422 mean train loss:  6.34769499e-02, bound:  3.19051772e-01\n",
      "Epoch: 7423 mean train loss:  6.33780882e-02, bound:  3.19048226e-01\n",
      "Epoch: 7424 mean train loss:  6.32092729e-02, bound:  3.19047779e-01\n",
      "Epoch: 7425 mean train loss:  6.31290376e-02, bound:  3.19045663e-01\n",
      "Epoch: 7426 mean train loss:  6.31712973e-02, bound:  3.19043398e-01\n",
      "Epoch: 7427 mean train loss:  6.32317886e-02, bound:  3.19042832e-01\n",
      "Epoch: 7428 mean train loss:  6.32065833e-02, bound:  3.19039702e-01\n",
      "Epoch: 7429 mean train loss:  6.31055161e-02, bound:  3.19039077e-01\n",
      "Epoch: 7430 mean train loss:  6.30219802e-02, bound:  3.19036752e-01\n",
      "Epoch: 7431 mean train loss:  6.30124733e-02, bound:  3.19034904e-01\n",
      "Epoch: 7432 mean train loss:  6.30425885e-02, bound:  3.19033891e-01\n",
      "Epoch: 7433 mean train loss:  6.30420372e-02, bound:  3.19031179e-01\n",
      "Epoch: 7434 mean train loss:  6.29877374e-02, bound:  3.19030464e-01\n",
      "Epoch: 7435 mean train loss:  6.29199743e-02, bound:  3.19028109e-01\n",
      "Epoch: 7436 mean train loss:  6.28859922e-02, bound:  3.19026560e-01\n",
      "Epoch: 7437 mean train loss:  6.28888756e-02, bound:  3.19025248e-01\n",
      "Epoch: 7438 mean train loss:  6.28910288e-02, bound:  3.19022804e-01\n",
      "Epoch: 7439 mean train loss:  6.28643930e-02, bound:  3.19021970e-01\n",
      "Epoch: 7440 mean train loss:  6.28166124e-02, bound:  3.19019556e-01\n",
      "Epoch: 7441 mean train loss:  6.27752841e-02, bound:  3.19018185e-01\n",
      "Epoch: 7442 mean train loss:  6.27566501e-02, bound:  3.19016546e-01\n",
      "Epoch: 7443 mean train loss:  6.27509356e-02, bound:  3.19014430e-01\n",
      "Epoch: 7444 mean train loss:  6.27374649e-02, bound:  3.19013238e-01\n",
      "Epoch: 7445 mean train loss:  6.27074912e-02, bound:  3.19010913e-01\n",
      "Epoch: 7446 mean train loss:  6.26705512e-02, bound:  3.19009602e-01\n",
      "Epoch: 7447 mean train loss:  6.26408085e-02, bound:  3.19007725e-01\n",
      "Epoch: 7448 mean train loss:  6.26232848e-02, bound:  3.19005847e-01\n",
      "Epoch: 7449 mean train loss:  6.26102462e-02, bound:  3.19004476e-01\n",
      "Epoch: 7450 mean train loss:  6.25912249e-02, bound:  3.19002241e-01\n",
      "Epoch: 7451 mean train loss:  6.25633225e-02, bound:  3.19000930e-01\n",
      "Epoch: 7452 mean train loss:  6.25328124e-02, bound:  3.18998903e-01\n",
      "Epoch: 7453 mean train loss:  6.25074804e-02, bound:  3.18997234e-01\n",
      "Epoch: 7454 mean train loss:  6.24882318e-02, bound:  3.18995565e-01\n",
      "Epoch: 7455 mean train loss:  6.24711178e-02, bound:  3.18993568e-01\n",
      "Epoch: 7456 mean train loss:  6.24500364e-02, bound:  3.18992138e-01\n",
      "Epoch: 7457 mean train loss:  6.24245442e-02, bound:  3.18990052e-01\n",
      "Epoch: 7458 mean train loss:  6.23978451e-02, bound:  3.18988502e-01\n",
      "Epoch: 7459 mean train loss:  6.23741746e-02, bound:  3.18986714e-01\n",
      "Epoch: 7460 mean train loss:  6.23532012e-02, bound:  3.18984956e-01\n",
      "Epoch: 7461 mean train loss:  6.23336285e-02, bound:  3.18983376e-01\n",
      "Epoch: 7462 mean train loss:  6.23123571e-02, bound:  3.18981409e-01\n",
      "Epoch: 7463 mean train loss:  6.22888394e-02, bound:  3.18979919e-01\n",
      "Epoch: 7464 mean train loss:  6.22638762e-02, bound:  3.18978012e-01\n",
      "Epoch: 7465 mean train loss:  6.22404814e-02, bound:  3.18976372e-01\n",
      "Epoch: 7466 mean train loss:  6.22183606e-02, bound:  3.18974674e-01\n",
      "Epoch: 7467 mean train loss:  6.21975698e-02, bound:  3.18972796e-01\n",
      "Epoch: 7468 mean train loss:  6.21761940e-02, bound:  3.18971246e-01\n",
      "Epoch: 7469 mean train loss:  6.21536598e-02, bound:  3.18969309e-01\n",
      "Epoch: 7470 mean train loss:  6.21304065e-02, bound:  3.18967730e-01\n",
      "Epoch: 7471 mean train loss:  6.21066913e-02, bound:  3.18965912e-01\n",
      "Epoch: 7472 mean train loss:  6.20842353e-02, bound:  3.18964154e-01\n",
      "Epoch: 7473 mean train loss:  6.20623194e-02, bound:  3.18962485e-01\n",
      "Epoch: 7474 mean train loss:  6.20405748e-02, bound:  3.18960637e-01\n",
      "Epoch: 7475 mean train loss:  6.20186552e-02, bound:  3.18958998e-01\n",
      "Epoch: 7476 mean train loss:  6.19958080e-02, bound:  3.18957150e-01\n",
      "Epoch: 7477 mean train loss:  6.19729422e-02, bound:  3.18955511e-01\n",
      "Epoch: 7478 mean train loss:  6.19499534e-02, bound:  3.18953723e-01\n",
      "Epoch: 7479 mean train loss:  6.19275197e-02, bound:  3.18951964e-01\n",
      "Epoch: 7480 mean train loss:  6.19054027e-02, bound:  3.18950266e-01\n",
      "Epoch: 7481 mean train loss:  6.18835650e-02, bound:  3.18948478e-01\n",
      "Epoch: 7482 mean train loss:  6.18610233e-02, bound:  3.18946809e-01\n",
      "Epoch: 7483 mean train loss:  6.18390404e-02, bound:  3.18944961e-01\n",
      "Epoch: 7484 mean train loss:  6.18160069e-02, bound:  3.18943292e-01\n",
      "Epoch: 7485 mean train loss:  6.17933534e-02, bound:  3.18941504e-01\n",
      "Epoch: 7486 mean train loss:  6.17709793e-02, bound:  3.18939745e-01\n",
      "Epoch: 7487 mean train loss:  6.17486797e-02, bound:  3.18938047e-01\n",
      "Epoch: 7488 mean train loss:  6.17262945e-02, bound:  3.18936288e-01\n",
      "Epoch: 7489 mean train loss:  6.17043748e-02, bound:  3.18934590e-01\n",
      "Epoch: 7490 mean train loss:  6.16817363e-02, bound:  3.18932801e-01\n",
      "Epoch: 7491 mean train loss:  6.16589896e-02, bound:  3.18931073e-01\n",
      "Epoch: 7492 mean train loss:  6.16365224e-02, bound:  3.18929285e-01\n",
      "Epoch: 7493 mean train loss:  6.16141632e-02, bound:  3.18927526e-01\n",
      "Epoch: 7494 mean train loss:  6.15914352e-02, bound:  3.18925828e-01\n",
      "Epoch: 7495 mean train loss:  6.15691207e-02, bound:  3.18924069e-01\n",
      "Epoch: 7496 mean train loss:  6.15467504e-02, bound:  3.18922371e-01\n",
      "Epoch: 7497 mean train loss:  6.15242943e-02, bound:  3.18920583e-01\n",
      "Epoch: 7498 mean train loss:  6.15019090e-02, bound:  3.18918854e-01\n",
      "Epoch: 7499 mean train loss:  6.14794828e-02, bound:  3.18917036e-01\n",
      "Epoch: 7500 mean train loss:  6.14569001e-02, bound:  3.18915367e-01\n",
      "Epoch: 7501 mean train loss:  6.14344478e-02, bound:  3.18913579e-01\n",
      "Epoch: 7502 mean train loss:  6.14115559e-02, bound:  3.18911821e-01\n",
      "Epoch: 7503 mean train loss:  6.13892004e-02, bound:  3.18910062e-01\n",
      "Epoch: 7504 mean train loss:  6.13666438e-02, bound:  3.18908304e-01\n",
      "Epoch: 7505 mean train loss:  6.13441207e-02, bound:  3.18906605e-01\n",
      "Epoch: 7506 mean train loss:  6.13219216e-02, bound:  3.18904817e-01\n",
      "Epoch: 7507 mean train loss:  6.12996668e-02, bound:  3.18903118e-01\n",
      "Epoch: 7508 mean train loss:  6.12769350e-02, bound:  3.18901330e-01\n",
      "Epoch: 7509 mean train loss:  6.12542629e-02, bound:  3.18899602e-01\n",
      "Epoch: 7510 mean train loss:  6.12315126e-02, bound:  3.18897784e-01\n",
      "Epoch: 7511 mean train loss:  6.12089559e-02, bound:  3.18896055e-01\n",
      "Epoch: 7512 mean train loss:  6.11862242e-02, bound:  3.18894297e-01\n",
      "Epoch: 7513 mean train loss:  6.11639172e-02, bound:  3.18892568e-01\n",
      "Epoch: 7514 mean train loss:  6.11413307e-02, bound:  3.18890780e-01\n",
      "Epoch: 7515 mean train loss:  6.11187890e-02, bound:  3.18889022e-01\n",
      "Epoch: 7516 mean train loss:  6.10965602e-02, bound:  3.18887293e-01\n",
      "Epoch: 7517 mean train loss:  6.10741302e-02, bound:  3.18885505e-01\n",
      "Epoch: 7518 mean train loss:  6.10511638e-02, bound:  3.18883806e-01\n",
      "Epoch: 7519 mean train loss:  6.10286519e-02, bound:  3.18882018e-01\n",
      "Epoch: 7520 mean train loss:  6.10059611e-02, bound:  3.18880260e-01\n",
      "Epoch: 7521 mean train loss:  6.09833896e-02, bound:  3.18878531e-01\n",
      "Epoch: 7522 mean train loss:  6.09608367e-02, bound:  3.18876773e-01\n",
      "Epoch: 7523 mean train loss:  6.09382242e-02, bound:  3.18875015e-01\n",
      "Epoch: 7524 mean train loss:  6.09154403e-02, bound:  3.18873227e-01\n",
      "Epoch: 7525 mean train loss:  6.08929843e-02, bound:  3.18871468e-01\n",
      "Epoch: 7526 mean train loss:  6.08702786e-02, bound:  3.18869710e-01\n",
      "Epoch: 7527 mean train loss:  6.08476587e-02, bound:  3.18867952e-01\n",
      "Epoch: 7528 mean train loss:  6.08251318e-02, bound:  3.18866193e-01\n",
      "Epoch: 7529 mean train loss:  6.08026460e-02, bound:  3.18864465e-01\n",
      "Epoch: 7530 mean train loss:  6.07798398e-02, bound:  3.18862677e-01\n",
      "Epoch: 7531 mean train loss:  6.07572533e-02, bound:  3.18860918e-01\n",
      "Epoch: 7532 mean train loss:  6.07347228e-02, bound:  3.18859190e-01\n",
      "Epoch: 7533 mean train loss:  6.07121438e-02, bound:  3.18857431e-01\n",
      "Epoch: 7534 mean train loss:  6.06893077e-02, bound:  3.18855643e-01\n",
      "Epoch: 7535 mean train loss:  6.06667027e-02, bound:  3.18853885e-01\n",
      "Epoch: 7536 mean train loss:  6.06437400e-02, bound:  3.18852097e-01\n",
      "Epoch: 7537 mean train loss:  6.06208667e-02, bound:  3.18850338e-01\n",
      "Epoch: 7538 mean train loss:  6.05983175e-02, bound:  3.18848610e-01\n",
      "Epoch: 7539 mean train loss:  6.05755337e-02, bound:  3.18846852e-01\n",
      "Epoch: 7540 mean train loss:  6.05528913e-02, bound:  3.18845093e-01\n",
      "Epoch: 7541 mean train loss:  6.05306663e-02, bound:  3.18843305e-01\n",
      "Epoch: 7542 mean train loss:  6.05076924e-02, bound:  3.18841577e-01\n",
      "Epoch: 7543 mean train loss:  6.04848377e-02, bound:  3.18839818e-01\n",
      "Epoch: 7544 mean train loss:  6.04620725e-02, bound:  3.18838030e-01\n",
      "Epoch: 7545 mean train loss:  6.04394302e-02, bound:  3.18836242e-01\n",
      "Epoch: 7546 mean train loss:  6.04168810e-02, bound:  3.18834484e-01\n",
      "Epoch: 7547 mean train loss:  6.03939593e-02, bound:  3.18832695e-01\n",
      "Epoch: 7548 mean train loss:  6.03712015e-02, bound:  3.18830967e-01\n",
      "Epoch: 7549 mean train loss:  6.03483804e-02, bound:  3.18829179e-01\n",
      "Epoch: 7550 mean train loss:  6.03255928e-02, bound:  3.18827450e-01\n",
      "Epoch: 7551 mean train loss:  6.03029244e-02, bound:  3.18825632e-01\n",
      "Epoch: 7552 mean train loss:  6.02799580e-02, bound:  3.18823934e-01\n",
      "Epoch: 7553 mean train loss:  6.02574423e-02, bound:  3.18822086e-01\n",
      "Epoch: 7554 mean train loss:  6.02346696e-02, bound:  3.18820387e-01\n",
      "Epoch: 7555 mean train loss:  6.02118000e-02, bound:  3.18818539e-01\n",
      "Epoch: 7556 mean train loss:  6.01893589e-02, bound:  3.18816841e-01\n",
      "Epoch: 7557 mean train loss:  6.01668134e-02, bound:  3.18814993e-01\n",
      "Epoch: 7558 mean train loss:  6.01439364e-02, bound:  3.18813294e-01\n",
      "Epoch: 7559 mean train loss:  6.01216368e-02, bound:  3.18811476e-01\n",
      "Epoch: 7560 mean train loss:  6.00996092e-02, bound:  3.18809807e-01\n",
      "Epoch: 7561 mean train loss:  6.00770861e-02, bound:  3.18807870e-01\n",
      "Epoch: 7562 mean train loss:  6.00557700e-02, bound:  3.18806320e-01\n",
      "Epoch: 7563 mean train loss:  6.00348786e-02, bound:  3.18804294e-01\n",
      "Epoch: 7564 mean train loss:  6.00150414e-02, bound:  3.18802804e-01\n",
      "Epoch: 7565 mean train loss:  5.99977784e-02, bound:  3.18800718e-01\n",
      "Epoch: 7566 mean train loss:  5.99836856e-02, bound:  3.18799406e-01\n",
      "Epoch: 7567 mean train loss:  5.99750429e-02, bound:  3.18797052e-01\n",
      "Epoch: 7568 mean train loss:  5.99759407e-02, bound:  3.18796009e-01\n",
      "Epoch: 7569 mean train loss:  5.99928237e-02, bound:  3.18793297e-01\n",
      "Epoch: 7570 mean train loss:  6.00363091e-02, bound:  3.18792760e-01\n",
      "Epoch: 7571 mean train loss:  6.01219162e-02, bound:  3.18789423e-01\n",
      "Epoch: 7572 mean train loss:  6.02732189e-02, bound:  3.18789691e-01\n",
      "Epoch: 7573 mean train loss:  6.05189316e-02, bound:  3.18785340e-01\n",
      "Epoch: 7574 mean train loss:  6.08727112e-02, bound:  3.18786800e-01\n",
      "Epoch: 7575 mean train loss:  6.13002405e-02, bound:  3.18781167e-01\n",
      "Epoch: 7576 mean train loss:  6.16416782e-02, bound:  3.18783879e-01\n",
      "Epoch: 7577 mean train loss:  6.16361685e-02, bound:  3.18777561e-01\n",
      "Epoch: 7578 mean train loss:  6.10957518e-02, bound:  3.18780273e-01\n",
      "Epoch: 7579 mean train loss:  6.02646843e-02, bound:  3.18775475e-01\n",
      "Epoch: 7580 mean train loss:  5.97018301e-02, bound:  3.18775743e-01\n",
      "Epoch: 7581 mean train loss:  5.97422086e-02, bound:  3.18774492e-01\n",
      "Epoch: 7582 mean train loss:  6.01650216e-02, bound:  3.18771243e-01\n",
      "Epoch: 7583 mean train loss:  6.04601204e-02, bound:  3.18772644e-01\n",
      "Epoch: 7584 mean train loss:  6.03109524e-02, bound:  3.18768024e-01\n",
      "Epoch: 7585 mean train loss:  5.98594099e-02, bound:  3.18768829e-01\n",
      "Epoch: 7586 mean train loss:  5.95377199e-02, bound:  3.18766028e-01\n",
      "Epoch: 7587 mean train loss:  5.95822744e-02, bound:  3.18764120e-01\n",
      "Epoch: 7588 mean train loss:  5.98267131e-02, bound:  3.18763912e-01\n",
      "Epoch: 7589 mean train loss:  5.99461384e-02, bound:  3.18760067e-01\n",
      "Epoch: 7590 mean train loss:  5.97948022e-02, bound:  3.18760514e-01\n",
      "Epoch: 7591 mean train loss:  5.95268197e-02, bound:  3.18757236e-01\n",
      "Epoch: 7592 mean train loss:  5.93908913e-02, bound:  3.18756044e-01\n",
      "Epoch: 7593 mean train loss:  5.94587512e-02, bound:  3.18754792e-01\n",
      "Epoch: 7594 mean train loss:  5.95844314e-02, bound:  3.18751782e-01\n",
      "Epoch: 7595 mean train loss:  5.95904998e-02, bound:  3.18751752e-01\n",
      "Epoch: 7596 mean train loss:  5.94560020e-02, bound:  3.18748564e-01\n",
      "Epoch: 7597 mean train loss:  5.93097955e-02, bound:  3.18747878e-01\n",
      "Epoch: 7598 mean train loss:  5.92704639e-02, bound:  3.18746120e-01\n",
      "Epoch: 7599 mean train loss:  5.93258329e-02, bound:  3.18743885e-01\n",
      "Epoch: 7600 mean train loss:  5.93681522e-02, bound:  3.18743318e-01\n",
      "Epoch: 7601 mean train loss:  5.93246259e-02, bound:  3.18740457e-01\n",
      "Epoch: 7602 mean train loss:  5.92263229e-02, bound:  3.18739772e-01\n",
      "Epoch: 7603 mean train loss:  5.91571108e-02, bound:  3.18737686e-01\n",
      "Epoch: 7604 mean train loss:  5.91545999e-02, bound:  3.18735927e-01\n",
      "Epoch: 7605 mean train loss:  5.91813475e-02, bound:  3.18734974e-01\n",
      "Epoch: 7606 mean train loss:  5.91761358e-02, bound:  3.18732411e-01\n",
      "Epoch: 7607 mean train loss:  5.91249578e-02, bound:  3.18731695e-01\n",
      "Epoch: 7608 mean train loss:  5.90616390e-02, bound:  3.18729550e-01\n",
      "Epoch: 7609 mean train loss:  5.90280928e-02, bound:  3.18728089e-01\n",
      "Epoch: 7610 mean train loss:  5.90266697e-02, bound:  3.18726748e-01\n",
      "Epoch: 7611 mean train loss:  5.90292215e-02, bound:  3.18724573e-01\n",
      "Epoch: 7612 mean train loss:  5.90081699e-02, bound:  3.18723649e-01\n",
      "Epoch: 7613 mean train loss:  5.89658618e-02, bound:  3.18721414e-01\n",
      "Epoch: 7614 mean train loss:  5.89240827e-02, bound:  3.18720162e-01\n",
      "Epoch: 7615 mean train loss:  5.89000173e-02, bound:  3.18718463e-01\n",
      "Epoch: 7616 mean train loss:  5.88916726e-02, bound:  3.18716586e-01\n",
      "Epoch: 7617 mean train loss:  5.88819794e-02, bound:  3.18715364e-01\n",
      "Epoch: 7618 mean train loss:  5.88589050e-02, bound:  3.18713218e-01\n",
      "Epoch: 7619 mean train loss:  5.88254668e-02, bound:  3.18711996e-01\n",
      "Epoch: 7620 mean train loss:  5.87930791e-02, bound:  3.18710059e-01\n",
      "Epoch: 7621 mean train loss:  5.87707125e-02, bound:  3.18708390e-01\n",
      "Epoch: 7622 mean train loss:  5.87565526e-02, bound:  3.18706959e-01\n",
      "Epoch: 7623 mean train loss:  5.87410033e-02, bound:  3.18704993e-01\n",
      "Epoch: 7624 mean train loss:  5.87192066e-02, bound:  3.18703681e-01\n",
      "Epoch: 7625 mean train loss:  5.86912893e-02, bound:  3.18701714e-01\n",
      "Epoch: 7626 mean train loss:  5.86643033e-02, bound:  3.18700194e-01\n",
      "Epoch: 7627 mean train loss:  5.86412214e-02, bound:  3.18698525e-01\n",
      "Epoch: 7628 mean train loss:  5.86227812e-02, bound:  3.18696737e-01\n",
      "Epoch: 7629 mean train loss:  5.86053543e-02, bound:  3.18695337e-01\n",
      "Epoch: 7630 mean train loss:  5.85846305e-02, bound:  3.18693340e-01\n",
      "Epoch: 7631 mean train loss:  5.85604757e-02, bound:  3.18691969e-01\n",
      "Epoch: 7632 mean train loss:  5.85352927e-02, bound:  3.18690151e-01\n",
      "Epoch: 7633 mean train loss:  5.85119389e-02, bound:  3.18688571e-01\n",
      "Epoch: 7634 mean train loss:  5.84911034e-02, bound:  3.18686932e-01\n",
      "Epoch: 7635 mean train loss:  5.84715083e-02, bound:  3.18685144e-01\n",
      "Epoch: 7636 mean train loss:  5.84514253e-02, bound:  3.18683684e-01\n",
      "Epoch: 7637 mean train loss:  5.84297255e-02, bound:  3.18681836e-01\n",
      "Epoch: 7638 mean train loss:  5.84065095e-02, bound:  3.18680346e-01\n",
      "Epoch: 7639 mean train loss:  5.83834089e-02, bound:  3.18678647e-01\n",
      "Epoch: 7640 mean train loss:  5.83610833e-02, bound:  3.18677008e-01\n",
      "Epoch: 7641 mean train loss:  5.83401397e-02, bound:  3.18675399e-01\n",
      "Epoch: 7642 mean train loss:  5.83193488e-02, bound:  3.18673640e-01\n",
      "Epoch: 7643 mean train loss:  5.82986325e-02, bound:  3.18672091e-01\n",
      "Epoch: 7644 mean train loss:  5.82763553e-02, bound:  3.18670303e-01\n",
      "Epoch: 7645 mean train loss:  5.82542419e-02, bound:  3.18668783e-01\n",
      "Epoch: 7646 mean train loss:  5.82319386e-02, bound:  3.18667024e-01\n",
      "Epoch: 7647 mean train loss:  5.82100973e-02, bound:  3.18665415e-01\n",
      "Epoch: 7648 mean train loss:  5.81883080e-02, bound:  3.18663806e-01\n",
      "Epoch: 7649 mean train loss:  5.81672750e-02, bound:  3.18662107e-01\n",
      "Epoch: 7650 mean train loss:  5.81462234e-02, bound:  3.18660527e-01\n",
      "Epoch: 7651 mean train loss:  5.81242181e-02, bound:  3.18658739e-01\n",
      "Epoch: 7652 mean train loss:  5.81021085e-02, bound:  3.18657160e-01\n",
      "Epoch: 7653 mean train loss:  5.80805130e-02, bound:  3.18655461e-01\n",
      "Epoch: 7654 mean train loss:  5.80583885e-02, bound:  3.18653792e-01\n",
      "Epoch: 7655 mean train loss:  5.80364987e-02, bound:  3.18652183e-01\n",
      "Epoch: 7656 mean train loss:  5.80154322e-02, bound:  3.18650484e-01\n",
      "Epoch: 7657 mean train loss:  5.79937212e-02, bound:  3.18648875e-01\n",
      "Epoch: 7658 mean train loss:  5.79720065e-02, bound:  3.18647116e-01\n",
      "Epoch: 7659 mean train loss:  5.79504259e-02, bound:  3.18645537e-01\n",
      "Epoch: 7660 mean train loss:  5.79285100e-02, bound:  3.18643808e-01\n",
      "Epoch: 7661 mean train loss:  5.79068847e-02, bound:  3.18642229e-01\n",
      "Epoch: 7662 mean train loss:  5.78848943e-02, bound:  3.18640530e-01\n",
      "Epoch: 7663 mean train loss:  5.78631759e-02, bound:  3.18638861e-01\n",
      "Epoch: 7664 mean train loss:  5.78413196e-02, bound:  3.18637222e-01\n",
      "Epoch: 7665 mean train loss:  5.78199252e-02, bound:  3.18635553e-01\n",
      "Epoch: 7666 mean train loss:  5.77981435e-02, bound:  3.18633884e-01\n",
      "Epoch: 7667 mean train loss:  5.77764958e-02, bound:  3.18632185e-01\n",
      "Epoch: 7668 mean train loss:  5.77547476e-02, bound:  3.18630576e-01\n",
      "Epoch: 7669 mean train loss:  5.77328466e-02, bound:  3.18628907e-01\n",
      "Epoch: 7670 mean train loss:  5.77112734e-02, bound:  3.18627238e-01\n",
      "Epoch: 7671 mean train loss:  5.76893240e-02, bound:  3.18625569e-01\n",
      "Epoch: 7672 mean train loss:  5.76674640e-02, bound:  3.18623900e-01\n",
      "Epoch: 7673 mean train loss:  5.76457009e-02, bound:  3.18622202e-01\n",
      "Epoch: 7674 mean train loss:  5.76242395e-02, bound:  3.18620533e-01\n",
      "Epoch: 7675 mean train loss:  5.76020740e-02, bound:  3.18618923e-01\n",
      "Epoch: 7676 mean train loss:  5.75802587e-02, bound:  3.18617225e-01\n",
      "Epoch: 7677 mean train loss:  5.75589500e-02, bound:  3.18615586e-01\n",
      "Epoch: 7678 mean train loss:  5.75371198e-02, bound:  3.18613917e-01\n",
      "Epoch: 7679 mean train loss:  5.75148351e-02, bound:  3.18612248e-01\n",
      "Epoch: 7680 mean train loss:  5.74930795e-02, bound:  3.18610549e-01\n",
      "Epoch: 7681 mean train loss:  5.74712493e-02, bound:  3.18608880e-01\n",
      "Epoch: 7682 mean train loss:  5.74497730e-02, bound:  3.18607241e-01\n",
      "Epoch: 7683 mean train loss:  5.74277230e-02, bound:  3.18605542e-01\n",
      "Epoch: 7684 mean train loss:  5.74057475e-02, bound:  3.18603873e-01\n",
      "Epoch: 7685 mean train loss:  5.73839769e-02, bound:  3.18602204e-01\n",
      "Epoch: 7686 mean train loss:  5.73621131e-02, bound:  3.18600535e-01\n",
      "Epoch: 7687 mean train loss:  5.73403165e-02, bound:  3.18598866e-01\n",
      "Epoch: 7688 mean train loss:  5.73185422e-02, bound:  3.18597198e-01\n",
      "Epoch: 7689 mean train loss:  5.72966076e-02, bound:  3.18595499e-01\n",
      "Epoch: 7690 mean train loss:  5.72746471e-02, bound:  3.18593860e-01\n",
      "Epoch: 7691 mean train loss:  5.72528988e-02, bound:  3.18592161e-01\n",
      "Epoch: 7692 mean train loss:  5.72310053e-02, bound:  3.18590522e-01\n",
      "Epoch: 7693 mean train loss:  5.72090149e-02, bound:  3.18588823e-01\n",
      "Epoch: 7694 mean train loss:  5.71873412e-02, bound:  3.18587154e-01\n",
      "Epoch: 7695 mean train loss:  5.71652949e-02, bound:  3.18585485e-01\n",
      "Epoch: 7696 mean train loss:  5.71434647e-02, bound:  3.18583816e-01\n",
      "Epoch: 7697 mean train loss:  5.71215264e-02, bound:  3.18582118e-01\n",
      "Epoch: 7698 mean train loss:  5.70993349e-02, bound:  3.18580419e-01\n",
      "Epoch: 7699 mean train loss:  5.70773855e-02, bound:  3.18578780e-01\n",
      "Epoch: 7700 mean train loss:  5.70554025e-02, bound:  3.18577111e-01\n",
      "Epoch: 7701 mean train loss:  5.70335351e-02, bound:  3.18575412e-01\n",
      "Epoch: 7702 mean train loss:  5.70118092e-02, bound:  3.18573773e-01\n",
      "Epoch: 7703 mean train loss:  5.69901466e-02, bound:  3.18572074e-01\n",
      "Epoch: 7704 mean train loss:  5.69679588e-02, bound:  3.18570375e-01\n",
      "Epoch: 7705 mean train loss:  5.69458529e-02, bound:  3.18568736e-01\n",
      "Epoch: 7706 mean train loss:  5.69235981e-02, bound:  3.18567067e-01\n",
      "Epoch: 7707 mean train loss:  5.69017380e-02, bound:  3.18565369e-01\n",
      "Epoch: 7708 mean train loss:  5.68798631e-02, bound:  3.18563670e-01\n",
      "Epoch: 7709 mean train loss:  5.68577833e-02, bound:  3.18562031e-01\n",
      "Epoch: 7710 mean train loss:  5.68355732e-02, bound:  3.18560332e-01\n",
      "Epoch: 7711 mean train loss:  5.68135381e-02, bound:  3.18558663e-01\n",
      "Epoch: 7712 mean train loss:  5.67916222e-02, bound:  3.18556994e-01\n",
      "Epoch: 7713 mean train loss:  5.67695685e-02, bound:  3.18555266e-01\n",
      "Epoch: 7714 mean train loss:  5.67477494e-02, bound:  3.18553627e-01\n",
      "Epoch: 7715 mean train loss:  5.67254014e-02, bound:  3.18551958e-01\n",
      "Epoch: 7716 mean train loss:  5.67037240e-02, bound:  3.18550289e-01\n",
      "Epoch: 7717 mean train loss:  5.66817261e-02, bound:  3.18548590e-01\n",
      "Epoch: 7718 mean train loss:  5.66595756e-02, bound:  3.18546891e-01\n",
      "Epoch: 7719 mean train loss:  5.66376261e-02, bound:  3.18545222e-01\n",
      "Epoch: 7720 mean train loss:  5.66155277e-02, bound:  3.18543583e-01\n",
      "Epoch: 7721 mean train loss:  5.65937161e-02, bound:  3.18541855e-01\n",
      "Epoch: 7722 mean train loss:  5.65715879e-02, bound:  3.18540215e-01\n",
      "Epoch: 7723 mean train loss:  5.65497205e-02, bound:  3.18538487e-01\n",
      "Epoch: 7724 mean train loss:  5.65280356e-02, bound:  3.18536848e-01\n",
      "Epoch: 7725 mean train loss:  5.65062948e-02, bound:  3.18535089e-01\n",
      "Epoch: 7726 mean train loss:  5.64849675e-02, bound:  3.18533510e-01\n",
      "Epoch: 7727 mean train loss:  5.64644597e-02, bound:  3.18531662e-01\n",
      "Epoch: 7728 mean train loss:  5.64440824e-02, bound:  3.18530202e-01\n",
      "Epoch: 7729 mean train loss:  5.64251989e-02, bound:  3.18528265e-01\n",
      "Epoch: 7730 mean train loss:  5.64088374e-02, bound:  3.18526894e-01\n",
      "Epoch: 7731 mean train loss:  5.63961975e-02, bound:  3.18524778e-01\n",
      "Epoch: 7732 mean train loss:  5.63905723e-02, bound:  3.18523645e-01\n",
      "Epoch: 7733 mean train loss:  5.63966744e-02, bound:  3.18521202e-01\n",
      "Epoch: 7734 mean train loss:  5.64229637e-02, bound:  3.18520486e-01\n",
      "Epoch: 7735 mean train loss:  5.64848110e-02, bound:  3.18517596e-01\n",
      "Epoch: 7736 mean train loss:  5.66060729e-02, bound:  3.18517506e-01\n",
      "Epoch: 7737 mean train loss:  5.68220392e-02, bound:  3.18513781e-01\n",
      "Epoch: 7738 mean train loss:  5.71732149e-02, bound:  3.18514764e-01\n",
      "Epoch: 7739 mean train loss:  5.76791354e-02, bound:  3.18509698e-01\n",
      "Epoch: 7740 mean train loss:  5.82517534e-02, bound:  3.18512172e-01\n",
      "Epoch: 7741 mean train loss:  5.86121269e-02, bound:  3.18505883e-01\n",
      "Epoch: 7742 mean train loss:  5.83455376e-02, bound:  3.18509072e-01\n",
      "Epoch: 7743 mean train loss:  5.73952608e-02, bound:  3.18503499e-01\n",
      "Epoch: 7744 mean train loss:  5.63869812e-02, bound:  3.18504751e-01\n",
      "Epoch: 7745 mean train loss:  5.61028868e-02, bound:  3.18502784e-01\n",
      "Epoch: 7746 mean train loss:  5.65701574e-02, bound:  3.18500161e-01\n",
      "Epoch: 7747 mean train loss:  5.70959710e-02, bound:  3.18501562e-01\n",
      "Epoch: 7748 mean train loss:  5.70397861e-02, bound:  3.18496972e-01\n",
      "Epoch: 7749 mean train loss:  5.64588495e-02, bound:  3.18498164e-01\n",
      "Epoch: 7750 mean train loss:  5.59990071e-02, bound:  3.18495363e-01\n",
      "Epoch: 7751 mean train loss:  5.60739115e-02, bound:  3.18493545e-01\n",
      "Epoch: 7752 mean train loss:  5.64314537e-02, bound:  3.18493664e-01\n",
      "Epoch: 7753 mean train loss:  5.65576404e-02, bound:  3.18489730e-01\n",
      "Epoch: 7754 mean train loss:  5.62886074e-02, bound:  3.18490356e-01\n",
      "Epoch: 7755 mean train loss:  5.59394099e-02, bound:  3.18487346e-01\n",
      "Epoch: 7756 mean train loss:  5.58665134e-02, bound:  3.18485856e-01\n",
      "Epoch: 7757 mean train loss:  5.60485944e-02, bound:  3.18485051e-01\n",
      "Epoch: 7758 mean train loss:  5.61817884e-02, bound:  3.18481714e-01\n",
      "Epoch: 7759 mean train loss:  5.60744964e-02, bound:  3.18481833e-01\n",
      "Epoch: 7760 mean train loss:  5.58480471e-02, bound:  3.18478942e-01\n",
      "Epoch: 7761 mean train loss:  5.57437576e-02, bound:  3.18477809e-01\n",
      "Epoch: 7762 mean train loss:  5.58179021e-02, bound:  3.18476737e-01\n",
      "Epoch: 7763 mean train loss:  5.59095778e-02, bound:  3.18474054e-01\n",
      "Epoch: 7764 mean train loss:  5.58687337e-02, bound:  3.18473935e-01\n",
      "Epoch: 7765 mean train loss:  5.57307079e-02, bound:  3.18471313e-01\n",
      "Epoch: 7766 mean train loss:  5.56407124e-02, bound:  3.18470210e-01\n",
      "Epoch: 7767 mean train loss:  5.56619242e-02, bound:  3.18468928e-01\n",
      "Epoch: 7768 mean train loss:  5.57141490e-02, bound:  3.18466604e-01\n",
      "Epoch: 7769 mean train loss:  5.56968451e-02, bound:  3.18466127e-01\n",
      "Epoch: 7770 mean train loss:  5.56115210e-02, bound:  3.18463773e-01\n",
      "Epoch: 7771 mean train loss:  5.55404052e-02, bound:  3.18462700e-01\n",
      "Epoch: 7772 mean train loss:  5.55342510e-02, bound:  3.18461329e-01\n",
      "Epoch: 7773 mean train loss:  5.55579886e-02, bound:  3.18459272e-01\n",
      "Epoch: 7774 mean train loss:  5.55501655e-02, bound:  3.18458557e-01\n",
      "Epoch: 7775 mean train loss:  5.54977916e-02, bound:  3.18456292e-01\n",
      "Epoch: 7776 mean train loss:  5.54411560e-02, bound:  3.18455249e-01\n",
      "Epoch: 7777 mean train loss:  5.54179922e-02, bound:  3.18453699e-01\n",
      "Epoch: 7778 mean train loss:  5.54218479e-02, bound:  3.18451852e-01\n",
      "Epoch: 7779 mean train loss:  5.54175340e-02, bound:  3.18450898e-01\n",
      "Epoch: 7780 mean train loss:  5.53871989e-02, bound:  3.18448752e-01\n",
      "Epoch: 7781 mean train loss:  5.53432368e-02, bound:  3.18447679e-01\n",
      "Epoch: 7782 mean train loss:  5.53118102e-02, bound:  3.18445951e-01\n",
      "Epoch: 7783 mean train loss:  5.52998371e-02, bound:  3.18444252e-01\n",
      "Epoch: 7784 mean train loss:  5.52928522e-02, bound:  3.18443060e-01\n",
      "Epoch: 7785 mean train loss:  5.52740507e-02, bound:  3.18441004e-01\n",
      "Epoch: 7786 mean train loss:  5.52422963e-02, bound:  3.18439871e-01\n",
      "Epoch: 7787 mean train loss:  5.52104861e-02, bound:  3.18438053e-01\n",
      "Epoch: 7788 mean train loss:  5.51891662e-02, bound:  3.18436503e-01\n",
      "Epoch: 7789 mean train loss:  5.51762618e-02, bound:  3.18435133e-01\n",
      "Epoch: 7790 mean train loss:  5.51613569e-02, bound:  3.18433225e-01\n",
      "Epoch: 7791 mean train loss:  5.51383942e-02, bound:  3.18432003e-01\n",
      "Epoch: 7792 mean train loss:  5.51103987e-02, bound:  3.18430126e-01\n",
      "Epoch: 7793 mean train loss:  5.50850704e-02, bound:  3.18428695e-01\n",
      "Epoch: 7794 mean train loss:  5.50656654e-02, bound:  3.18427175e-01\n",
      "Epoch: 7795 mean train loss:  5.50496280e-02, bound:  3.18425477e-01\n",
      "Epoch: 7796 mean train loss:  5.50310872e-02, bound:  3.18424106e-01\n",
      "Epoch: 7797 mean train loss:  5.50082698e-02, bound:  3.18422318e-01\n",
      "Epoch: 7798 mean train loss:  5.49838208e-02, bound:  3.18420947e-01\n",
      "Epoch: 7799 mean train loss:  5.49608059e-02, bound:  3.18419278e-01\n",
      "Epoch: 7800 mean train loss:  5.49411811e-02, bound:  3.18417728e-01\n",
      "Epoch: 7801 mean train loss:  5.49230240e-02, bound:  3.18416268e-01\n",
      "Epoch: 7802 mean train loss:  5.49032725e-02, bound:  3.18414569e-01\n",
      "Epoch: 7803 mean train loss:  5.48816472e-02, bound:  3.18413228e-01\n",
      "Epoch: 7804 mean train loss:  5.48586585e-02, bound:  3.18411529e-01\n",
      "Epoch: 7805 mean train loss:  5.48366494e-02, bound:  3.18410039e-01\n",
      "Epoch: 7806 mean train loss:  5.48163429e-02, bound:  3.18408519e-01\n",
      "Epoch: 7807 mean train loss:  5.47967851e-02, bound:  3.18406850e-01\n",
      "Epoch: 7808 mean train loss:  5.47770448e-02, bound:  3.18405390e-01\n",
      "Epoch: 7809 mean train loss:  5.47559932e-02, bound:  3.18403691e-01\n",
      "Epoch: 7810 mean train loss:  5.47344461e-02, bound:  3.18402231e-01\n",
      "Epoch: 7811 mean train loss:  5.47124445e-02, bound:  3.18400681e-01\n",
      "Epoch: 7812 mean train loss:  5.46916872e-02, bound:  3.18399101e-01\n",
      "Epoch: 7813 mean train loss:  5.46712577e-02, bound:  3.18397582e-01\n",
      "Epoch: 7814 mean train loss:  5.46511710e-02, bound:  3.18395913e-01\n",
      "Epoch: 7815 mean train loss:  5.46304695e-02, bound:  3.18394452e-01\n",
      "Epoch: 7816 mean train loss:  5.46095222e-02, bound:  3.18392783e-01\n",
      "Epoch: 7817 mean train loss:  5.45880869e-02, bound:  3.18391323e-01\n",
      "Epoch: 7818 mean train loss:  5.45669980e-02, bound:  3.18389714e-01\n",
      "Epoch: 7819 mean train loss:  5.45461550e-02, bound:  3.18388134e-01\n",
      "Epoch: 7820 mean train loss:  5.45258112e-02, bound:  3.18386614e-01\n",
      "Epoch: 7821 mean train loss:  5.45052700e-02, bound:  3.18385035e-01\n",
      "Epoch: 7822 mean train loss:  5.44844605e-02, bound:  3.18383485e-01\n",
      "Epoch: 7823 mean train loss:  5.44634201e-02, bound:  3.18381816e-01\n",
      "Epoch: 7824 mean train loss:  5.44422679e-02, bound:  3.18380356e-01\n",
      "Epoch: 7825 mean train loss:  5.44210710e-02, bound:  3.18378717e-01\n",
      "Epoch: 7826 mean train loss:  5.44002652e-02, bound:  3.18377167e-01\n",
      "Epoch: 7827 mean train loss:  5.43793850e-02, bound:  3.18375647e-01\n",
      "Epoch: 7828 mean train loss:  5.43590859e-02, bound:  3.18374038e-01\n",
      "Epoch: 7829 mean train loss:  5.43382317e-02, bound:  3.18372548e-01\n",
      "Epoch: 7830 mean train loss:  5.43173105e-02, bound:  3.18370908e-01\n",
      "Epoch: 7831 mean train loss:  5.42965494e-02, bound:  3.18369359e-01\n",
      "Epoch: 7832 mean train loss:  5.42752929e-02, bound:  3.18367749e-01\n",
      "Epoch: 7833 mean train loss:  5.42543866e-02, bound:  3.18366230e-01\n",
      "Epoch: 7834 mean train loss:  5.42338453e-02, bound:  3.18364650e-01\n",
      "Epoch: 7835 mean train loss:  5.42127118e-02, bound:  3.18363100e-01\n",
      "Epoch: 7836 mean train loss:  5.41917123e-02, bound:  3.18361580e-01\n",
      "Epoch: 7837 mean train loss:  5.41711934e-02, bound:  3.18359911e-01\n",
      "Epoch: 7838 mean train loss:  5.41504286e-02, bound:  3.18358392e-01\n",
      "Epoch: 7839 mean train loss:  5.41291162e-02, bound:  3.18356782e-01\n",
      "Epoch: 7840 mean train loss:  5.41082658e-02, bound:  3.18355262e-01\n",
      "Epoch: 7841 mean train loss:  5.40872701e-02, bound:  3.18353683e-01\n",
      "Epoch: 7842 mean train loss:  5.40662855e-02, bound:  3.18352133e-01\n",
      "Epoch: 7843 mean train loss:  5.40453158e-02, bound:  3.18350524e-01\n",
      "Epoch: 7844 mean train loss:  5.40244989e-02, bound:  3.18348944e-01\n",
      "Epoch: 7845 mean train loss:  5.40036373e-02, bound:  3.18347394e-01\n",
      "Epoch: 7846 mean train loss:  5.39827086e-02, bound:  3.18345815e-01\n",
      "Epoch: 7847 mean train loss:  5.39618842e-02, bound:  3.18344235e-01\n",
      "Epoch: 7848 mean train loss:  5.39406687e-02, bound:  3.18342656e-01\n",
      "Epoch: 7849 mean train loss:  5.39199188e-02, bound:  3.18341076e-01\n",
      "Epoch: 7850 mean train loss:  5.38983643e-02, bound:  3.18339497e-01\n",
      "Epoch: 7851 mean train loss:  5.38780876e-02, bound:  3.18337947e-01\n",
      "Epoch: 7852 mean train loss:  5.38568199e-02, bound:  3.18336368e-01\n",
      "Epoch: 7853 mean train loss:  5.38359098e-02, bound:  3.18334758e-01\n",
      "Epoch: 7854 mean train loss:  5.38149662e-02, bound:  3.18333209e-01\n",
      "Epoch: 7855 mean train loss:  5.37937023e-02, bound:  3.18331629e-01\n",
      "Epoch: 7856 mean train loss:  5.37726507e-02, bound:  3.18330050e-01\n",
      "Epoch: 7857 mean train loss:  5.37516251e-02, bound:  3.18328470e-01\n",
      "Epoch: 7858 mean train loss:  5.37307635e-02, bound:  3.18326890e-01\n",
      "Epoch: 7859 mean train loss:  5.37096672e-02, bound:  3.18325311e-01\n",
      "Epoch: 7860 mean train loss:  5.36886491e-02, bound:  3.18323731e-01\n",
      "Epoch: 7861 mean train loss:  5.36675900e-02, bound:  3.18322182e-01\n",
      "Epoch: 7862 mean train loss:  5.36466278e-02, bound:  3.18320572e-01\n",
      "Epoch: 7863 mean train loss:  5.36255650e-02, bound:  3.18319023e-01\n",
      "Epoch: 7864 mean train loss:  5.36047854e-02, bound:  3.18317413e-01\n",
      "Epoch: 7865 mean train loss:  5.35830930e-02, bound:  3.18315834e-01\n",
      "Epoch: 7866 mean train loss:  5.35622463e-02, bound:  3.18314224e-01\n",
      "Epoch: 7867 mean train loss:  5.35413660e-02, bound:  3.18312705e-01\n",
      "Epoch: 7868 mean train loss:  5.35201095e-02, bound:  3.18311095e-01\n",
      "Epoch: 7869 mean train loss:  5.34994453e-02, bound:  3.18309546e-01\n",
      "Epoch: 7870 mean train loss:  5.34780435e-02, bound:  3.18307936e-01\n",
      "Epoch: 7871 mean train loss:  5.34570105e-02, bound:  3.18306357e-01\n",
      "Epoch: 7872 mean train loss:  5.34360483e-02, bound:  3.18304807e-01\n",
      "Epoch: 7873 mean train loss:  5.34146689e-02, bound:  3.18303198e-01\n",
      "Epoch: 7874 mean train loss:  5.33938222e-02, bound:  3.18301618e-01\n",
      "Epoch: 7875 mean train loss:  5.33726253e-02, bound:  3.18300068e-01\n",
      "Epoch: 7876 mean train loss:  5.33513799e-02, bound:  3.18298459e-01\n",
      "Epoch: 7877 mean train loss:  5.33305667e-02, bound:  3.18296880e-01\n",
      "Epoch: 7878 mean train loss:  5.33091947e-02, bound:  3.18295270e-01\n",
      "Epoch: 7879 mean train loss:  5.32881245e-02, bound:  3.18293720e-01\n",
      "Epoch: 7880 mean train loss:  5.32672144e-02, bound:  3.18292111e-01\n",
      "Epoch: 7881 mean train loss:  5.32457381e-02, bound:  3.18290532e-01\n",
      "Epoch: 7882 mean train loss:  5.32245860e-02, bound:  3.18288982e-01\n",
      "Epoch: 7883 mean train loss:  5.32034002e-02, bound:  3.18287402e-01\n",
      "Epoch: 7884 mean train loss:  5.31824157e-02, bound:  3.18285793e-01\n",
      "Epoch: 7885 mean train loss:  5.31614460e-02, bound:  3.18284243e-01\n",
      "Epoch: 7886 mean train loss:  5.31403013e-02, bound:  3.18282604e-01\n",
      "Epoch: 7887 mean train loss:  5.31188250e-02, bound:  3.18281025e-01\n",
      "Epoch: 7888 mean train loss:  5.30978255e-02, bound:  3.18279445e-01\n",
      "Epoch: 7889 mean train loss:  5.30766994e-02, bound:  3.18277895e-01\n",
      "Epoch: 7890 mean train loss:  5.30553386e-02, bound:  3.18276286e-01\n",
      "Epoch: 7891 mean train loss:  5.30344509e-02, bound:  3.18274707e-01\n",
      "Epoch: 7892 mean train loss:  5.30129336e-02, bound:  3.18273067e-01\n",
      "Epoch: 7893 mean train loss:  5.29919788e-02, bound:  3.18271548e-01\n",
      "Epoch: 7894 mean train loss:  5.29709719e-02, bound:  3.18269908e-01\n",
      "Epoch: 7895 mean train loss:  5.29500805e-02, bound:  3.18268418e-01\n",
      "Epoch: 7896 mean train loss:  5.29293977e-02, bound:  3.18266720e-01\n",
      "Epoch: 7897 mean train loss:  5.29090464e-02, bound:  3.18265229e-01\n",
      "Epoch: 7898 mean train loss:  5.28887361e-02, bound:  3.18263501e-01\n",
      "Epoch: 7899 mean train loss:  5.28686605e-02, bound:  3.18262130e-01\n",
      "Epoch: 7900 mean train loss:  5.28499223e-02, bound:  3.18260312e-01\n",
      "Epoch: 7901 mean train loss:  5.28324582e-02, bound:  3.18258971e-01\n",
      "Epoch: 7902 mean train loss:  5.28174825e-02, bound:  3.18257034e-01\n",
      "Epoch: 7903 mean train loss:  5.28062992e-02, bound:  3.18255901e-01\n",
      "Epoch: 7904 mean train loss:  5.28008379e-02, bound:  3.18253785e-01\n",
      "Epoch: 7905 mean train loss:  5.28065227e-02, bound:  3.18252861e-01\n",
      "Epoch: 7906 mean train loss:  5.28296567e-02, bound:  3.18250448e-01\n",
      "Epoch: 7907 mean train loss:  5.28811514e-02, bound:  3.18249941e-01\n",
      "Epoch: 7908 mean train loss:  5.29803447e-02, bound:  3.18246931e-01\n",
      "Epoch: 7909 mean train loss:  5.31515814e-02, bound:  3.18247139e-01\n",
      "Epoch: 7910 mean train loss:  5.34241721e-02, bound:  3.18243295e-01\n",
      "Epoch: 7911 mean train loss:  5.38161062e-02, bound:  3.18244547e-01\n",
      "Epoch: 7912 mean train loss:  5.42834736e-02, bound:  3.18239570e-01\n",
      "Epoch: 7913 mean train loss:  5.46596684e-02, bound:  3.18241894e-01\n",
      "Epoch: 7914 mean train loss:  5.46523295e-02, bound:  3.18236321e-01\n",
      "Epoch: 7915 mean train loss:  5.40684611e-02, bound:  3.18238646e-01\n",
      "Epoch: 7916 mean train loss:  5.31632006e-02, bound:  3.18234414e-01\n",
      "Epoch: 7917 mean train loss:  5.25504537e-02, bound:  3.18234533e-01\n",
      "Epoch: 7918 mean train loss:  5.25961295e-02, bound:  3.18233401e-01\n",
      "Epoch: 7919 mean train loss:  5.30601889e-02, bound:  3.18230540e-01\n",
      "Epoch: 7920 mean train loss:  5.33841141e-02, bound:  3.18231702e-01\n",
      "Epoch: 7921 mean train loss:  5.32225072e-02, bound:  3.18227649e-01\n",
      "Epoch: 7922 mean train loss:  5.27358279e-02, bound:  3.18228424e-01\n",
      "Epoch: 7923 mean train loss:  5.23907095e-02, bound:  3.18225950e-01\n",
      "Epoch: 7924 mean train loss:  5.24413846e-02, bound:  3.18224281e-01\n",
      "Epoch: 7925 mean train loss:  5.27119264e-02, bound:  3.18224102e-01\n",
      "Epoch: 7926 mean train loss:  5.28522320e-02, bound:  3.18220675e-01\n",
      "Epoch: 7927 mean train loss:  5.27027585e-02, bound:  3.18221092e-01\n",
      "Epoch: 7928 mean train loss:  5.24138026e-02, bound:  3.18218112e-01\n",
      "Epoch: 7929 mean train loss:  5.22516705e-02, bound:  3.18217069e-01\n",
      "Epoch: 7930 mean train loss:  5.23105413e-02, bound:  3.18215787e-01\n",
      "Epoch: 7931 mean train loss:  5.24541773e-02, bound:  3.18213105e-01\n",
      "Epoch: 7932 mean train loss:  5.24902716e-02, bound:  3.18212986e-01\n",
      "Epoch: 7933 mean train loss:  5.23669906e-02, bound:  3.18210125e-01\n",
      "Epoch: 7934 mean train loss:  5.22008017e-02, bound:  3.18209559e-01\n",
      "Epoch: 7935 mean train loss:  5.21319099e-02, bound:  3.18207830e-01\n",
      "Epoch: 7936 mean train loss:  5.21784611e-02, bound:  3.18205982e-01\n",
      "Epoch: 7937 mean train loss:  5.22416793e-02, bound:  3.18205416e-01\n",
      "Epoch: 7938 mean train loss:  5.22258654e-02, bound:  3.18202883e-01\n",
      "Epoch: 7939 mean train loss:  5.21331988e-02, bound:  3.18202406e-01\n",
      "Epoch: 7940 mean train loss:  5.20428158e-02, bound:  3.18200439e-01\n",
      "Epoch: 7941 mean train loss:  5.20187132e-02, bound:  3.18199039e-01\n",
      "Epoch: 7942 mean train loss:  5.20465299e-02, bound:  3.18198085e-01\n",
      "Epoch: 7943 mean train loss:  5.20630628e-02, bound:  3.18195879e-01\n",
      "Epoch: 7944 mean train loss:  5.20301424e-02, bound:  3.18195283e-01\n",
      "Epoch: 7945 mean train loss:  5.19649126e-02, bound:  3.18193257e-01\n",
      "Epoch: 7946 mean train loss:  5.19133732e-02, bound:  3.18192124e-01\n",
      "Epoch: 7947 mean train loss:  5.19005843e-02, bound:  3.18190753e-01\n",
      "Epoch: 7948 mean train loss:  5.19079342e-02, bound:  3.18188995e-01\n",
      "Epoch: 7949 mean train loss:  5.19034415e-02, bound:  3.18188101e-01\n",
      "Epoch: 7950 mean train loss:  5.18726408e-02, bound:  3.18186074e-01\n",
      "Epoch: 7951 mean train loss:  5.18282764e-02, bound:  3.18185151e-01\n",
      "Epoch: 7952 mean train loss:  5.17931245e-02, bound:  3.18183452e-01\n",
      "Epoch: 7953 mean train loss:  5.17769605e-02, bound:  3.18181932e-01\n",
      "Epoch: 7954 mean train loss:  5.17719090e-02, bound:  3.18180799e-01\n",
      "Epoch: 7955 mean train loss:  5.17608896e-02, bound:  3.18178892e-01\n",
      "Epoch: 7956 mean train loss:  5.17362915e-02, bound:  3.18177849e-01\n",
      "Epoch: 7957 mean train loss:  5.17036803e-02, bound:  3.18175972e-01\n",
      "Epoch: 7958 mean train loss:  5.16738594e-02, bound:  3.18174720e-01\n",
      "Epoch: 7959 mean train loss:  5.16536161e-02, bound:  3.18173200e-01\n",
      "Epoch: 7960 mean train loss:  5.16408607e-02, bound:  3.18171501e-01\n",
      "Epoch: 7961 mean train loss:  5.16268276e-02, bound:  3.18170309e-01\n",
      "Epoch: 7962 mean train loss:  5.16067147e-02, bound:  3.18168461e-01\n",
      "Epoch: 7963 mean train loss:  5.15812226e-02, bound:  3.18167269e-01\n",
      "Epoch: 7964 mean train loss:  5.15548512e-02, bound:  3.18165541e-01\n",
      "Epoch: 7965 mean train loss:  5.15326336e-02, bound:  3.18164140e-01\n",
      "Epoch: 7966 mean train loss:  5.15148975e-02, bound:  3.18162709e-01\n",
      "Epoch: 7967 mean train loss:  5.14985286e-02, bound:  3.18161070e-01\n",
      "Epoch: 7968 mean train loss:  5.14804646e-02, bound:  3.18159819e-01\n",
      "Epoch: 7969 mean train loss:  5.14587536e-02, bound:  3.18158120e-01\n",
      "Epoch: 7970 mean train loss:  5.14354743e-02, bound:  3.18156838e-01\n",
      "Epoch: 7971 mean train loss:  5.14125787e-02, bound:  3.18155229e-01\n",
      "Epoch: 7972 mean train loss:  5.13921604e-02, bound:  3.18153769e-01\n",
      "Epoch: 7973 mean train loss:  5.13735637e-02, bound:  3.18152368e-01\n",
      "Epoch: 7974 mean train loss:  5.13554215e-02, bound:  3.18150759e-01\n",
      "Epoch: 7975 mean train loss:  5.13361990e-02, bound:  3.18149507e-01\n",
      "Epoch: 7976 mean train loss:  5.13155349e-02, bound:  3.18147838e-01\n",
      "Epoch: 7977 mean train loss:  5.12934886e-02, bound:  3.18146497e-01\n",
      "Epoch: 7978 mean train loss:  5.12723997e-02, bound:  3.18144947e-01\n",
      "Epoch: 7979 mean train loss:  5.12515642e-02, bound:  3.18143457e-01\n",
      "Epoch: 7980 mean train loss:  5.12320884e-02, bound:  3.18142027e-01\n",
      "Epoch: 7981 mean train loss:  5.12127839e-02, bound:  3.18140477e-01\n",
      "Epoch: 7982 mean train loss:  5.11933528e-02, bound:  3.18139106e-01\n",
      "Epoch: 7983 mean train loss:  5.11731505e-02, bound:  3.18137527e-01\n",
      "Epoch: 7984 mean train loss:  5.11525646e-02, bound:  3.18136126e-01\n",
      "Epoch: 7985 mean train loss:  5.11316732e-02, bound:  3.18134546e-01\n",
      "Epoch: 7986 mean train loss:  5.11109717e-02, bound:  3.18133116e-01\n",
      "Epoch: 7987 mean train loss:  5.10907844e-02, bound:  3.18131655e-01\n",
      "Epoch: 7988 mean train loss:  5.10710552e-02, bound:  3.18130106e-01\n",
      "Epoch: 7989 mean train loss:  5.10515533e-02, bound:  3.18128705e-01\n",
      "Epoch: 7990 mean train loss:  5.10314256e-02, bound:  3.18127126e-01\n",
      "Epoch: 7991 mean train loss:  5.10113239e-02, bound:  3.18125725e-01\n",
      "Epoch: 7992 mean train loss:  5.09907864e-02, bound:  3.18124175e-01\n",
      "Epoch: 7993 mean train loss:  5.09706289e-02, bound:  3.18122774e-01\n",
      "Epoch: 7994 mean train loss:  5.09498306e-02, bound:  3.18121225e-01\n",
      "Epoch: 7995 mean train loss:  5.09296097e-02, bound:  3.18119735e-01\n",
      "Epoch: 7996 mean train loss:  5.09094670e-02, bound:  3.18118274e-01\n",
      "Epoch: 7997 mean train loss:  5.08897118e-02, bound:  3.18116784e-01\n",
      "Epoch: 7998 mean train loss:  5.08697778e-02, bound:  3.18115324e-01\n",
      "Epoch: 7999 mean train loss:  5.08494638e-02, bound:  3.18113744e-01\n",
      "Epoch: 8000 mean train loss:  5.08293286e-02, bound:  3.18112344e-01\n",
      "Epoch: 8001 mean train loss:  5.08089066e-02, bound:  3.18110794e-01\n",
      "Epoch: 8002 mean train loss:  5.07887527e-02, bound:  3.18109363e-01\n",
      "Epoch: 8003 mean train loss:  5.07686771e-02, bound:  3.18107843e-01\n",
      "Epoch: 8004 mean train loss:  5.07482700e-02, bound:  3.18106353e-01\n",
      "Epoch: 8005 mean train loss:  5.07279113e-02, bound:  3.18104893e-01\n",
      "Epoch: 8006 mean train loss:  5.07077575e-02, bound:  3.18103373e-01\n",
      "Epoch: 8007 mean train loss:  5.06875552e-02, bound:  3.18101943e-01\n",
      "Epoch: 8008 mean train loss:  5.06674945e-02, bound:  3.18100423e-01\n",
      "Epoch: 8009 mean train loss:  5.06472513e-02, bound:  3.18098933e-01\n",
      "Epoch: 8010 mean train loss:  5.06269522e-02, bound:  3.18097413e-01\n",
      "Epoch: 8011 mean train loss:  5.06070852e-02, bound:  3.18095982e-01\n",
      "Epoch: 8012 mean train loss:  5.05864993e-02, bound:  3.18094462e-01\n",
      "Epoch: 8013 mean train loss:  5.05662076e-02, bound:  3.18093002e-01\n",
      "Epoch: 8014 mean train loss:  5.05456999e-02, bound:  3.18091482e-01\n",
      "Epoch: 8015 mean train loss:  5.05255833e-02, bound:  3.18089992e-01\n",
      "Epoch: 8016 mean train loss:  5.05054556e-02, bound:  3.18088531e-01\n",
      "Epoch: 8017 mean train loss:  5.04852459e-02, bound:  3.18086982e-01\n",
      "Epoch: 8018 mean train loss:  5.04651628e-02, bound:  3.18085492e-01\n",
      "Epoch: 8019 mean train loss:  5.04447259e-02, bound:  3.18084031e-01\n",
      "Epoch: 8020 mean train loss:  5.04244864e-02, bound:  3.18082511e-01\n",
      "Epoch: 8021 mean train loss:  5.04040569e-02, bound:  3.18081051e-01\n",
      "Epoch: 8022 mean train loss:  5.03836535e-02, bound:  3.18079561e-01\n",
      "Epoch: 8023 mean train loss:  5.03638126e-02, bound:  3.18078041e-01\n",
      "Epoch: 8024 mean train loss:  5.03436178e-02, bound:  3.18076551e-01\n",
      "Epoch: 8025 mean train loss:  5.03229350e-02, bound:  3.18075031e-01\n",
      "Epoch: 8026 mean train loss:  5.03029265e-02, bound:  3.18073571e-01\n",
      "Epoch: 8027 mean train loss:  5.02826013e-02, bound:  3.18072051e-01\n",
      "Epoch: 8028 mean train loss:  5.02623729e-02, bound:  3.18070620e-01\n",
      "Epoch: 8029 mean train loss:  5.02420813e-02, bound:  3.18069041e-01\n",
      "Epoch: 8030 mean train loss:  5.02216816e-02, bound:  3.18067580e-01\n",
      "Epoch: 8031 mean train loss:  5.02012670e-02, bound:  3.18066061e-01\n",
      "Epoch: 8032 mean train loss:  5.01812100e-02, bound:  3.18064630e-01\n",
      "Epoch: 8033 mean train loss:  5.01606539e-02, bound:  3.18063051e-01\n",
      "Epoch: 8034 mean train loss:  5.01403287e-02, bound:  3.18061620e-01\n",
      "Epoch: 8035 mean train loss:  5.01203202e-02, bound:  3.18060100e-01\n",
      "Epoch: 8036 mean train loss:  5.00994995e-02, bound:  3.18058640e-01\n",
      "Epoch: 8037 mean train loss:  5.00794910e-02, bound:  3.18057120e-01\n",
      "Epoch: 8038 mean train loss:  5.00595383e-02, bound:  3.18055630e-01\n",
      "Epoch: 8039 mean train loss:  5.00390157e-02, bound:  3.18054110e-01\n",
      "Epoch: 8040 mean train loss:  5.00185713e-02, bound:  3.18052679e-01\n",
      "Epoch: 8041 mean train loss:  4.99984771e-02, bound:  3.18051130e-01\n",
      "Epoch: 8042 mean train loss:  4.99782301e-02, bound:  3.18049669e-01\n",
      "Epoch: 8043 mean train loss:  4.99582253e-02, bound:  3.18048120e-01\n",
      "Epoch: 8044 mean train loss:  4.99380752e-02, bound:  3.18046689e-01\n",
      "Epoch: 8045 mean train loss:  4.99183349e-02, bound:  3.18045110e-01\n",
      "Epoch: 8046 mean train loss:  4.98989038e-02, bound:  3.18043739e-01\n",
      "Epoch: 8047 mean train loss:  4.98801246e-02, bound:  3.18042099e-01\n",
      "Epoch: 8048 mean train loss:  4.98616919e-02, bound:  3.18040758e-01\n",
      "Epoch: 8049 mean train loss:  4.98446487e-02, bound:  3.18039060e-01\n",
      "Epoch: 8050 mean train loss:  4.98295315e-02, bound:  3.18037838e-01\n",
      "Epoch: 8051 mean train loss:  4.98172268e-02, bound:  3.18035990e-01\n",
      "Epoch: 8052 mean train loss:  4.98098359e-02, bound:  3.18034947e-01\n",
      "Epoch: 8053 mean train loss:  4.98098619e-02, bound:  3.18032920e-01\n",
      "Epoch: 8054 mean train loss:  4.98232804e-02, bound:  3.18032116e-01\n",
      "Epoch: 8055 mean train loss:  4.98563498e-02, bound:  3.18029702e-01\n",
      "Epoch: 8056 mean train loss:  4.99220453e-02, bound:  3.18029344e-01\n",
      "Epoch: 8057 mean train loss:  5.00387624e-02, bound:  3.18026423e-01\n",
      "Epoch: 8058 mean train loss:  5.02310283e-02, bound:  3.18026721e-01\n",
      "Epoch: 8059 mean train loss:  5.05202152e-02, bound:  3.18022996e-01\n",
      "Epoch: 8060 mean train loss:  5.09091057e-02, bound:  3.18024248e-01\n",
      "Epoch: 8061 mean train loss:  5.13290055e-02, bound:  3.18019509e-01\n",
      "Epoch: 8062 mean train loss:  5.16000278e-02, bound:  3.18021655e-01\n",
      "Epoch: 8063 mean train loss:  5.14663383e-02, bound:  3.18016529e-01\n",
      "Epoch: 8064 mean train loss:  5.08365296e-02, bound:  3.18018496e-01\n",
      "Epoch: 8065 mean train loss:  5.00145741e-02, bound:  3.18014711e-01\n",
      "Epoch: 8066 mean train loss:  4.95282151e-02, bound:  3.18014592e-01\n",
      "Epoch: 8067 mean train loss:  4.96211313e-02, bound:  3.18013608e-01\n",
      "Epoch: 8068 mean train loss:  5.00419848e-02, bound:  3.18010896e-01\n",
      "Epoch: 8069 mean train loss:  5.03209569e-02, bound:  3.18011880e-01\n",
      "Epoch: 8070 mean train loss:  5.01782894e-02, bound:  3.18008244e-01\n",
      "Epoch: 8071 mean train loss:  4.97479849e-02, bound:  3.18008840e-01\n",
      "Epoch: 8072 mean train loss:  4.94155623e-02, bound:  3.18006486e-01\n",
      "Epoch: 8073 mean train loss:  4.94159348e-02, bound:  3.18005055e-01\n",
      "Epoch: 8074 mean train loss:  4.96396944e-02, bound:  3.18004727e-01\n",
      "Epoch: 8075 mean train loss:  4.98041026e-02, bound:  3.18001688e-01\n",
      "Epoch: 8076 mean train loss:  4.97331657e-02, bound:  3.18002015e-01\n",
      "Epoch: 8077 mean train loss:  4.94916588e-02, bound:  3.17999125e-01\n",
      "Epoch: 8078 mean train loss:  4.92912009e-02, bound:  3.17998350e-01\n",
      "Epoch: 8079 mean train loss:  4.92682494e-02, bound:  3.17996860e-01\n",
      "Epoch: 8080 mean train loss:  4.93782796e-02, bound:  3.17994654e-01\n",
      "Epoch: 8081 mean train loss:  4.94673625e-02, bound:  3.17994326e-01\n",
      "Epoch: 8082 mean train loss:  4.94311042e-02, bound:  3.17991585e-01\n",
      "Epoch: 8083 mean train loss:  4.92944010e-02, bound:  3.17991257e-01\n",
      "Epoch: 8084 mean train loss:  4.91698682e-02, bound:  3.17989230e-01\n",
      "Epoch: 8085 mean train loss:  4.91395444e-02, bound:  3.17987919e-01\n",
      "Epoch: 8086 mean train loss:  4.91863713e-02, bound:  3.17987025e-01\n",
      "Epoch: 8087 mean train loss:  4.92293462e-02, bound:  3.17984849e-01\n",
      "Epoch: 8088 mean train loss:  4.92065549e-02, bound:  3.17984462e-01\n",
      "Epoch: 8089 mean train loss:  4.91265170e-02, bound:  3.17982286e-01\n",
      "Epoch: 8090 mean train loss:  4.90487143e-02, bound:  3.17981422e-01\n",
      "Epoch: 8091 mean train loss:  4.90196943e-02, bound:  3.17980051e-01\n",
      "Epoch: 8092 mean train loss:  4.90341634e-02, bound:  3.17978352e-01\n",
      "Epoch: 8093 mean train loss:  4.90502380e-02, bound:  3.17977637e-01\n",
      "Epoch: 8094 mean train loss:  4.90326881e-02, bound:  3.17975581e-01\n",
      "Epoch: 8095 mean train loss:  4.89836447e-02, bound:  3.17974865e-01\n",
      "Epoch: 8096 mean train loss:  4.89315242e-02, bound:  3.17973137e-01\n",
      "Epoch: 8097 mean train loss:  4.89017740e-02, bound:  3.17971915e-01\n",
      "Epoch: 8098 mean train loss:  4.88973558e-02, bound:  3.17970753e-01\n",
      "Epoch: 8099 mean train loss:  4.88983616e-02, bound:  3.17969024e-01\n",
      "Epoch: 8100 mean train loss:  4.88857664e-02, bound:  3.17968160e-01\n",
      "Epoch: 8101 mean train loss:  4.88551483e-02, bound:  3.17966342e-01\n",
      "Epoch: 8102 mean train loss:  4.88177426e-02, bound:  3.17965358e-01\n",
      "Epoch: 8103 mean train loss:  4.87870239e-02, bound:  3.17963809e-01\n",
      "Epoch: 8104 mean train loss:  4.87698205e-02, bound:  3.17962408e-01\n",
      "Epoch: 8105 mean train loss:  4.87612970e-02, bound:  3.17961186e-01\n",
      "Epoch: 8106 mean train loss:  4.87501174e-02, bound:  3.17959458e-01\n",
      "Epoch: 8107 mean train loss:  4.87303138e-02, bound:  3.17958444e-01\n",
      "Epoch: 8108 mean train loss:  4.87032235e-02, bound:  3.17956716e-01\n",
      "Epoch: 8109 mean train loss:  4.86757942e-02, bound:  3.17955494e-01\n",
      "Epoch: 8110 mean train loss:  4.86520119e-02, bound:  3.17953974e-01\n",
      "Epoch: 8111 mean train loss:  4.86348569e-02, bound:  3.17952514e-01\n",
      "Epoch: 8112 mean train loss:  4.86203842e-02, bound:  3.17951262e-01\n",
      "Epoch: 8113 mean train loss:  4.86050472e-02, bound:  3.17949593e-01\n",
      "Epoch: 8114 mean train loss:  4.85854931e-02, bound:  3.17948461e-01\n",
      "Epoch: 8115 mean train loss:  4.85629924e-02, bound:  3.17946821e-01\n",
      "Epoch: 8116 mean train loss:  4.85391133e-02, bound:  3.17945570e-01\n",
      "Epoch: 8117 mean train loss:  4.85174395e-02, bound:  3.17944139e-01\n",
      "Epoch: 8118 mean train loss:  4.84983698e-02, bound:  3.17942709e-01\n",
      "Epoch: 8119 mean train loss:  4.84813601e-02, bound:  3.17941397e-01\n",
      "Epoch: 8120 mean train loss:  4.84642833e-02, bound:  3.17939848e-01\n",
      "Epoch: 8121 mean train loss:  4.84458990e-02, bound:  3.17938656e-01\n",
      "Epoch: 8122 mean train loss:  4.84257415e-02, bound:  3.17937106e-01\n",
      "Epoch: 8123 mean train loss:  4.84043881e-02, bound:  3.17935854e-01\n",
      "Epoch: 8124 mean train loss:  4.83831875e-02, bound:  3.17934334e-01\n",
      "Epoch: 8125 mean train loss:  4.83630672e-02, bound:  3.17933023e-01\n",
      "Epoch: 8126 mean train loss:  4.83440980e-02, bound:  3.17931682e-01\n",
      "Epoch: 8127 mean train loss:  4.83263880e-02, bound:  3.17930192e-01\n",
      "Epoch: 8128 mean train loss:  4.83076982e-02, bound:  3.17928910e-01\n",
      "Epoch: 8129 mean train loss:  4.82887290e-02, bound:  3.17927390e-01\n",
      "Epoch: 8130 mean train loss:  4.82692830e-02, bound:  3.17926079e-01\n",
      "Epoch: 8131 mean train loss:  4.82489467e-02, bound:  3.17924619e-01\n",
      "Epoch: 8132 mean train loss:  4.82288040e-02, bound:  3.17923307e-01\n",
      "Epoch: 8133 mean train loss:  4.82088812e-02, bound:  3.17921877e-01\n",
      "Epoch: 8134 mean train loss:  4.81893122e-02, bound:  3.17920476e-01\n",
      "Epoch: 8135 mean train loss:  4.81706075e-02, bound:  3.17919105e-01\n",
      "Epoch: 8136 mean train loss:  4.81514372e-02, bound:  3.17917615e-01\n",
      "Epoch: 8137 mean train loss:  4.81325686e-02, bound:  3.17916334e-01\n",
      "Epoch: 8138 mean train loss:  4.81133237e-02, bound:  3.17914844e-01\n",
      "Epoch: 8139 mean train loss:  4.80939187e-02, bound:  3.17913502e-01\n",
      "Epoch: 8140 mean train loss:  4.80741933e-02, bound:  3.17912042e-01\n",
      "Epoch: 8141 mean train loss:  4.80544455e-02, bound:  3.17910731e-01\n",
      "Epoch: 8142 mean train loss:  4.80351076e-02, bound:  3.17909271e-01\n",
      "Epoch: 8143 mean train loss:  4.80153933e-02, bound:  3.17907870e-01\n",
      "Epoch: 8144 mean train loss:  4.79961373e-02, bound:  3.17906469e-01\n",
      "Epoch: 8145 mean train loss:  4.79771160e-02, bound:  3.17905039e-01\n",
      "Epoch: 8146 mean train loss:  4.79578078e-02, bound:  3.17903697e-01\n",
      "Epoch: 8147 mean train loss:  4.79383096e-02, bound:  3.17902237e-01\n",
      "Epoch: 8148 mean train loss:  4.79189195e-02, bound:  3.17900926e-01\n",
      "Epoch: 8149 mean train loss:  4.78999801e-02, bound:  3.17899406e-01\n",
      "Epoch: 8150 mean train loss:  4.78804298e-02, bound:  3.17898095e-01\n",
      "Epoch: 8151 mean train loss:  4.78607602e-02, bound:  3.17896634e-01\n",
      "Epoch: 8152 mean train loss:  4.78415675e-02, bound:  3.17895293e-01\n",
      "Epoch: 8153 mean train loss:  4.78217192e-02, bound:  3.17893893e-01\n",
      "Epoch: 8154 mean train loss:  4.78023179e-02, bound:  3.17892462e-01\n",
      "Epoch: 8155 mean train loss:  4.77830395e-02, bound:  3.17891061e-01\n",
      "Epoch: 8156 mean train loss:  4.77634780e-02, bound:  3.17889661e-01\n",
      "Epoch: 8157 mean train loss:  4.77444045e-02, bound:  3.17888290e-01\n",
      "Epoch: 8158 mean train loss:  4.77251299e-02, bound:  3.17886859e-01\n",
      "Epoch: 8159 mean train loss:  4.77055497e-02, bound:  3.17885488e-01\n",
      "Epoch: 8160 mean train loss:  4.76864055e-02, bound:  3.17884058e-01\n",
      "Epoch: 8161 mean train loss:  4.76667061e-02, bound:  3.17882657e-01\n",
      "Epoch: 8162 mean train loss:  4.76474948e-02, bound:  3.17881227e-01\n",
      "Epoch: 8163 mean train loss:  4.76282872e-02, bound:  3.17879885e-01\n",
      "Epoch: 8164 mean train loss:  4.76086214e-02, bound:  3.17878455e-01\n",
      "Epoch: 8165 mean train loss:  4.75892834e-02, bound:  3.17877054e-01\n",
      "Epoch: 8166 mean train loss:  4.75699976e-02, bound:  3.17875654e-01\n",
      "Epoch: 8167 mean train loss:  4.75504696e-02, bound:  3.17874253e-01\n",
      "Epoch: 8168 mean train loss:  4.75312620e-02, bound:  3.17872822e-01\n",
      "Epoch: 8169 mean train loss:  4.75117154e-02, bound:  3.17871481e-01\n",
      "Epoch: 8170 mean train loss:  4.74921726e-02, bound:  3.17870021e-01\n",
      "Epoch: 8171 mean train loss:  4.74727415e-02, bound:  3.17868650e-01\n",
      "Epoch: 8172 mean train loss:  4.74538170e-02, bound:  3.17867190e-01\n",
      "Epoch: 8173 mean train loss:  4.74341102e-02, bound:  3.17865878e-01\n",
      "Epoch: 8174 mean train loss:  4.74146307e-02, bound:  3.17864388e-01\n",
      "Epoch: 8175 mean train loss:  4.73952703e-02, bound:  3.17863077e-01\n",
      "Epoch: 8176 mean train loss:  4.73761559e-02, bound:  3.17861557e-01\n",
      "Epoch: 8177 mean train loss:  4.73565944e-02, bound:  3.17860246e-01\n",
      "Epoch: 8178 mean train loss:  4.73372452e-02, bound:  3.17858785e-01\n",
      "Epoch: 8179 mean train loss:  4.73185033e-02, bound:  3.17857444e-01\n",
      "Epoch: 8180 mean train loss:  4.72991504e-02, bound:  3.17855954e-01\n",
      "Epoch: 8181 mean train loss:  4.72803153e-02, bound:  3.17854673e-01\n",
      "Epoch: 8182 mean train loss:  4.72615846e-02, bound:  3.17853123e-01\n",
      "Epoch: 8183 mean train loss:  4.72433381e-02, bound:  3.17851841e-01\n",
      "Epoch: 8184 mean train loss:  4.72257026e-02, bound:  3.17850292e-01\n",
      "Epoch: 8185 mean train loss:  4.72090803e-02, bound:  3.17849070e-01\n",
      "Epoch: 8186 mean train loss:  4.71934639e-02, bound:  3.17847431e-01\n",
      "Epoch: 8187 mean train loss:  4.71803918e-02, bound:  3.17846328e-01\n",
      "Epoch: 8188 mean train loss:  4.71707582e-02, bound:  3.17844540e-01\n",
      "Epoch: 8189 mean train loss:  4.71667461e-02, bound:  3.17843616e-01\n",
      "Epoch: 8190 mean train loss:  4.71717939e-02, bound:  3.17841619e-01\n",
      "Epoch: 8191 mean train loss:  4.71914187e-02, bound:  3.17840964e-01\n",
      "Epoch: 8192 mean train loss:  4.72337045e-02, bound:  3.17838639e-01\n",
      "Epoch: 8193 mean train loss:  4.73118797e-02, bound:  3.17838371e-01\n",
      "Epoch: 8194 mean train loss:  4.74443696e-02, bound:  3.17835540e-01\n",
      "Epoch: 8195 mean train loss:  4.76535410e-02, bound:  3.17835927e-01\n",
      "Epoch: 8196 mean train loss:  4.79564071e-02, bound:  3.17832291e-01\n",
      "Epoch: 8197 mean train loss:  4.83431071e-02, bound:  3.17833573e-01\n",
      "Epoch: 8198 mean train loss:  4.87249233e-02, bound:  3.17829072e-01\n",
      "Epoch: 8199 mean train loss:  4.89198677e-02, bound:  3.17831069e-01\n",
      "Epoch: 8200 mean train loss:  4.87025753e-02, bound:  3.17826301e-01\n",
      "Epoch: 8201 mean train loss:  4.80481535e-02, bound:  3.17827970e-01\n",
      "Epoch: 8202 mean train loss:  4.72827889e-02, bound:  3.17824602e-01\n",
      "Epoch: 8203 mean train loss:  4.68762293e-02, bound:  3.17824304e-01\n",
      "Epoch: 8204 mean train loss:  4.69981283e-02, bound:  3.17823470e-01\n",
      "Epoch: 8205 mean train loss:  4.73937057e-02, bound:  3.17820877e-01\n",
      "Epoch: 8206 mean train loss:  4.76435050e-02, bound:  3.17821771e-01\n",
      "Epoch: 8207 mean train loss:  4.75096218e-02, bound:  3.17818403e-01\n",
      "Epoch: 8208 mean train loss:  4.71125096e-02, bound:  3.17818999e-01\n",
      "Epoch: 8209 mean train loss:  4.67914678e-02, bound:  3.17816764e-01\n",
      "Epoch: 8210 mean train loss:  4.67661619e-02, bound:  3.17815512e-01\n",
      "Epoch: 8211 mean train loss:  4.69611846e-02, bound:  3.17815065e-01\n",
      "Epoch: 8212 mean train loss:  4.71320562e-02, bound:  3.17812353e-01\n",
      "Epoch: 8213 mean train loss:  4.71024625e-02, bound:  3.17812592e-01\n",
      "Epoch: 8214 mean train loss:  4.68970798e-02, bound:  3.17809820e-01\n",
      "Epoch: 8215 mean train loss:  4.66892794e-02, bound:  3.17809314e-01\n",
      "Epoch: 8216 mean train loss:  4.66230623e-02, bound:  3.17807674e-01\n",
      "Epoch: 8217 mean train loss:  4.66981120e-02, bound:  3.17805797e-01\n",
      "Epoch: 8218 mean train loss:  4.67985347e-02, bound:  3.17805380e-01\n",
      "Epoch: 8219 mean train loss:  4.68110554e-02, bound:  3.17802846e-01\n",
      "Epoch: 8220 mean train loss:  4.67147976e-02, bound:  3.17802608e-01\n",
      "Epoch: 8221 mean train loss:  4.65847477e-02, bound:  3.17800522e-01\n",
      "Epoch: 8222 mean train loss:  4.65098061e-02, bound:  3.17799538e-01\n",
      "Epoch: 8223 mean train loss:  4.65182513e-02, bound:  3.17798406e-01\n",
      "Epoch: 8224 mean train loss:  4.65656444e-02, bound:  3.17796528e-01\n",
      "Epoch: 8225 mean train loss:  4.65840846e-02, bound:  3.17796081e-01\n",
      "Epoch: 8226 mean train loss:  4.65433747e-02, bound:  3.17793965e-01\n",
      "Epoch: 8227 mean train loss:  4.64673452e-02, bound:  3.17793399e-01\n",
      "Epoch: 8228 mean train loss:  4.64055985e-02, bound:  3.17791760e-01\n",
      "Epoch: 8229 mean train loss:  4.63853329e-02, bound:  3.17790508e-01\n",
      "Epoch: 8230 mean train loss:  4.63978611e-02, bound:  3.17789584e-01\n",
      "Epoch: 8231 mean train loss:  4.64078970e-02, bound:  3.17787826e-01\n",
      "Epoch: 8232 mean train loss:  4.63912673e-02, bound:  3.17787141e-01\n",
      "Epoch: 8233 mean train loss:  4.63481285e-02, bound:  3.17785382e-01\n",
      "Epoch: 8234 mean train loss:  4.63012941e-02, bound:  3.17784458e-01\n",
      "Epoch: 8235 mean train loss:  4.62704934e-02, bound:  3.17783117e-01\n",
      "Epoch: 8236 mean train loss:  4.62607294e-02, bound:  3.17781776e-01\n",
      "Epoch: 8237 mean train loss:  4.62596007e-02, bound:  3.17780823e-01\n",
      "Epoch: 8238 mean train loss:  4.62514497e-02, bound:  3.17779124e-01\n",
      "Epoch: 8239 mean train loss:  4.62291948e-02, bound:  3.17778319e-01\n",
      "Epoch: 8240 mean train loss:  4.61966135e-02, bound:  3.17776650e-01\n",
      "Epoch: 8241 mean train loss:  4.61650826e-02, bound:  3.17775607e-01\n",
      "Epoch: 8242 mean train loss:  4.61420938e-02, bound:  3.17774266e-01\n",
      "Epoch: 8243 mean train loss:  4.61284183e-02, bound:  3.17772835e-01\n",
      "Epoch: 8244 mean train loss:  4.61181439e-02, bound:  3.17771792e-01\n",
      "Epoch: 8245 mean train loss:  4.61051017e-02, bound:  3.17770153e-01\n",
      "Epoch: 8246 mean train loss:  4.60854061e-02, bound:  3.17769140e-01\n",
      "Epoch: 8247 mean train loss:  4.60607894e-02, bound:  3.17767531e-01\n",
      "Epoch: 8248 mean train loss:  4.60358709e-02, bound:  3.17766368e-01\n",
      "Epoch: 8249 mean train loss:  4.60137241e-02, bound:  3.17764938e-01\n",
      "Epoch: 8250 mean train loss:  4.59959023e-02, bound:  3.17763597e-01\n",
      "Epoch: 8251 mean train loss:  4.59810868e-02, bound:  3.17762405e-01\n",
      "Epoch: 8252 mean train loss:  4.59663123e-02, bound:  3.17760885e-01\n",
      "Epoch: 8253 mean train loss:  4.59485874e-02, bound:  3.17759782e-01\n",
      "Epoch: 8254 mean train loss:  4.59287837e-02, bound:  3.17758232e-01\n",
      "Epoch: 8255 mean train loss:  4.59072068e-02, bound:  3.17757130e-01\n",
      "Epoch: 8256 mean train loss:  4.58861217e-02, bound:  3.17755699e-01\n",
      "Epoch: 8257 mean train loss:  4.58663516e-02, bound:  3.17754447e-01\n",
      "Epoch: 8258 mean train loss:  4.58481684e-02, bound:  3.17753136e-01\n",
      "Epoch: 8259 mean train loss:  4.58314642e-02, bound:  3.17751765e-01\n",
      "Epoch: 8260 mean train loss:  4.58145514e-02, bound:  3.17750573e-01\n",
      "Epoch: 8261 mean train loss:  4.57967408e-02, bound:  3.17749143e-01\n",
      "Epoch: 8262 mean train loss:  4.57779281e-02, bound:  3.17747980e-01\n",
      "Epoch: 8263 mean train loss:  4.57582250e-02, bound:  3.17746550e-01\n",
      "Epoch: 8264 mean train loss:  4.57390063e-02, bound:  3.17745388e-01\n",
      "Epoch: 8265 mean train loss:  4.57196683e-02, bound:  3.17743987e-01\n",
      "Epoch: 8266 mean train loss:  4.57006767e-02, bound:  3.17742705e-01\n",
      "Epoch: 8267 mean train loss:  4.56822887e-02, bound:  3.17741424e-01\n",
      "Epoch: 8268 mean train loss:  4.56643365e-02, bound:  3.17740053e-01\n",
      "Epoch: 8269 mean train loss:  4.56468873e-02, bound:  3.17738831e-01\n",
      "Epoch: 8270 mean train loss:  4.56284843e-02, bound:  3.17737460e-01\n",
      "Epoch: 8271 mean train loss:  4.56099734e-02, bound:  3.17736268e-01\n",
      "Epoch: 8272 mean train loss:  4.55911867e-02, bound:  3.17734838e-01\n",
      "Epoch: 8273 mean train loss:  4.55724932e-02, bound:  3.17733645e-01\n",
      "Epoch: 8274 mean train loss:  4.55534309e-02, bound:  3.17732245e-01\n",
      "Epoch: 8275 mean train loss:  4.55347784e-02, bound:  3.17730993e-01\n",
      "Epoch: 8276 mean train loss:  4.55161668e-02, bound:  3.17729682e-01\n",
      "Epoch: 8277 mean train loss:  4.54982631e-02, bound:  3.17728311e-01\n",
      "Epoch: 8278 mean train loss:  4.54797335e-02, bound:  3.17727029e-01\n",
      "Epoch: 8279 mean train loss:  4.54611890e-02, bound:  3.17725688e-01\n",
      "Epoch: 8280 mean train loss:  4.54428419e-02, bound:  3.17724407e-01\n",
      "Epoch: 8281 mean train loss:  4.54248004e-02, bound:  3.17723066e-01\n",
      "Epoch: 8282 mean train loss:  4.54062149e-02, bound:  3.17721784e-01\n",
      "Epoch: 8283 mean train loss:  4.53876555e-02, bound:  3.17720413e-01\n",
      "Epoch: 8284 mean train loss:  4.53688279e-02, bound:  3.17719162e-01\n",
      "Epoch: 8285 mean train loss:  4.53503951e-02, bound:  3.17717820e-01\n",
      "Epoch: 8286 mean train loss:  4.53318730e-02, bound:  3.17716479e-01\n",
      "Epoch: 8287 mean train loss:  4.53130566e-02, bound:  3.17715138e-01\n",
      "Epoch: 8288 mean train loss:  4.52946201e-02, bound:  3.17713886e-01\n",
      "Epoch: 8289 mean train loss:  4.52763550e-02, bound:  3.17712545e-01\n",
      "Epoch: 8290 mean train loss:  4.52577174e-02, bound:  3.17711234e-01\n",
      "Epoch: 8291 mean train loss:  4.52392176e-02, bound:  3.17709953e-01\n",
      "Epoch: 8292 mean train loss:  4.52207960e-02, bound:  3.17708641e-01\n",
      "Epoch: 8293 mean train loss:  4.52023111e-02, bound:  3.17707330e-01\n",
      "Epoch: 8294 mean train loss:  4.51840535e-02, bound:  3.17705989e-01\n",
      "Epoch: 8295 mean train loss:  4.51654643e-02, bound:  3.17704707e-01\n",
      "Epoch: 8296 mean train loss:  4.51471657e-02, bound:  3.17703396e-01\n",
      "Epoch: 8297 mean train loss:  4.51287739e-02, bound:  3.17702085e-01\n",
      "Epoch: 8298 mean train loss:  4.51100506e-02, bound:  3.17700714e-01\n",
      "Epoch: 8299 mean train loss:  4.50916849e-02, bound:  3.17699462e-01\n",
      "Epoch: 8300 mean train loss:  4.50729281e-02, bound:  3.17698121e-01\n",
      "Epoch: 8301 mean train loss:  4.50544395e-02, bound:  3.17696840e-01\n",
      "Epoch: 8302 mean train loss:  4.50359955e-02, bound:  3.17695469e-01\n",
      "Epoch: 8303 mean train loss:  4.50175665e-02, bound:  3.17694217e-01\n",
      "Epoch: 8304 mean train loss:  4.49991599e-02, bound:  3.17692876e-01\n",
      "Epoch: 8305 mean train loss:  4.49808016e-02, bound:  3.17691565e-01\n",
      "Epoch: 8306 mean train loss:  4.49623354e-02, bound:  3.17690223e-01\n",
      "Epoch: 8307 mean train loss:  4.49441001e-02, bound:  3.17688972e-01\n",
      "Epoch: 8308 mean train loss:  4.49256264e-02, bound:  3.17687601e-01\n",
      "Epoch: 8309 mean train loss:  4.49069813e-02, bound:  3.17686379e-01\n",
      "Epoch: 8310 mean train loss:  4.48884219e-02, bound:  3.17684978e-01\n",
      "Epoch: 8311 mean train loss:  4.48705330e-02, bound:  3.17683727e-01\n",
      "Epoch: 8312 mean train loss:  4.48521525e-02, bound:  3.17682296e-01\n",
      "Epoch: 8313 mean train loss:  4.48343903e-02, bound:  3.17681134e-01\n",
      "Epoch: 8314 mean train loss:  4.48165648e-02, bound:  3.17679673e-01\n",
      "Epoch: 8315 mean train loss:  4.47996594e-02, bound:  3.17678481e-01\n",
      "Epoch: 8316 mean train loss:  4.47828323e-02, bound:  3.17677021e-01\n",
      "Epoch: 8317 mean train loss:  4.47674319e-02, bound:  3.17675918e-01\n",
      "Epoch: 8318 mean train loss:  4.47536707e-02, bound:  3.17674339e-01\n",
      "Epoch: 8319 mean train loss:  4.47426736e-02, bound:  3.17673326e-01\n",
      "Epoch: 8320 mean train loss:  4.47358638e-02, bound:  3.17671657e-01\n",
      "Epoch: 8321 mean train loss:  4.47363108e-02, bound:  3.17670822e-01\n",
      "Epoch: 8322 mean train loss:  4.47477847e-02, bound:  3.17668885e-01\n",
      "Epoch: 8323 mean train loss:  4.47783023e-02, bound:  3.17668319e-01\n",
      "Epoch: 8324 mean train loss:  4.48388234e-02, bound:  3.17666084e-01\n",
      "Epoch: 8325 mean train loss:  4.49473821e-02, bound:  3.17665964e-01\n",
      "Epoch: 8326 mean train loss:  4.51277532e-02, bound:  3.17663103e-01\n",
      "Epoch: 8327 mean train loss:  4.54088226e-02, bound:  3.17663729e-01\n",
      "Epoch: 8328 mean train loss:  4.58042733e-02, bound:  3.17660034e-01\n",
      "Epoch: 8329 mean train loss:  4.62799296e-02, bound:  3.17661554e-01\n",
      "Epoch: 8330 mean train loss:  4.66812924e-02, bound:  3.17656964e-01\n",
      "Epoch: 8331 mean train loss:  4.67462651e-02, bound:  3.17659110e-01\n",
      "Epoch: 8332 mean train loss:  4.62513231e-02, bound:  3.17654550e-01\n",
      "Epoch: 8333 mean train loss:  4.53555658e-02, bound:  3.17655951e-01\n",
      "Epoch: 8334 mean train loss:  4.46105972e-02, bound:  3.17653239e-01\n",
      "Epoch: 8335 mean train loss:  4.44773696e-02, bound:  3.17652315e-01\n",
      "Epoch: 8336 mean train loss:  4.48725000e-02, bound:  3.17652255e-01\n",
      "Epoch: 8337 mean train loss:  4.52975594e-02, bound:  3.17649275e-01\n",
      "Epoch: 8338 mean train loss:  4.53107879e-02, bound:  3.17650437e-01\n",
      "Epoch: 8339 mean train loss:  4.48983498e-02, bound:  3.17647338e-01\n",
      "Epoch: 8340 mean train loss:  4.44549099e-02, bound:  3.17647487e-01\n",
      "Epoch: 8341 mean train loss:  4.43487950e-02, bound:  3.17646146e-01\n",
      "Epoch: 8342 mean train loss:  4.45657708e-02, bound:  3.17644238e-01\n",
      "Epoch: 8343 mean train loss:  4.47972789e-02, bound:  3.17644477e-01\n",
      "Epoch: 8344 mean train loss:  4.47827503e-02, bound:  3.17641616e-01\n",
      "Epoch: 8345 mean train loss:  4.45377938e-02, bound:  3.17641824e-01\n",
      "Epoch: 8346 mean train loss:  4.42916118e-02, bound:  3.17639679e-01\n",
      "Epoch: 8347 mean train loss:  4.42362465e-02, bound:  3.17638516e-01\n",
      "Epoch: 8348 mean train loss:  4.43517715e-02, bound:  3.17637742e-01\n",
      "Epoch: 8349 mean train loss:  4.44705114e-02, bound:  3.17635417e-01\n",
      "Epoch: 8350 mean train loss:  4.44562212e-02, bound:  3.17635357e-01\n",
      "Epoch: 8351 mean train loss:  4.43158932e-02, bound:  3.17633003e-01\n",
      "Epoch: 8352 mean train loss:  4.41710204e-02, bound:  3.17632437e-01\n",
      "Epoch: 8353 mean train loss:  4.41242382e-02, bound:  3.17631036e-01\n",
      "Epoch: 8354 mean train loss:  4.41738367e-02, bound:  3.17629457e-01\n",
      "Epoch: 8355 mean train loss:  4.42333557e-02, bound:  3.17628980e-01\n",
      "Epoch: 8356 mean train loss:  4.42253202e-02, bound:  3.17626894e-01\n",
      "Epoch: 8357 mean train loss:  4.41468991e-02, bound:  3.17626536e-01\n",
      "Epoch: 8358 mean train loss:  4.40576486e-02, bound:  3.17624807e-01\n",
      "Epoch: 8359 mean train loss:  4.40172441e-02, bound:  3.17623794e-01\n",
      "Epoch: 8360 mean train loss:  4.40307632e-02, bound:  3.17622870e-01\n",
      "Epoch: 8361 mean train loss:  4.40565646e-02, bound:  3.17621201e-01\n",
      "Epoch: 8362 mean train loss:  4.40511853e-02, bound:  3.17620695e-01\n",
      "Epoch: 8363 mean train loss:  4.40063626e-02, bound:  3.17618936e-01\n",
      "Epoch: 8364 mean train loss:  4.39488180e-02, bound:  3.17618221e-01\n",
      "Epoch: 8365 mean train loss:  4.39125448e-02, bound:  3.17616880e-01\n",
      "Epoch: 8366 mean train loss:  4.39062119e-02, bound:  3.17615628e-01\n",
      "Epoch: 8367 mean train loss:  4.39126082e-02, bound:  3.17614824e-01\n",
      "Epoch: 8368 mean train loss:  4.39074896e-02, bound:  3.17613214e-01\n",
      "Epoch: 8369 mean train loss:  4.38812301e-02, bound:  3.17612559e-01\n",
      "Epoch: 8370 mean train loss:  4.38429527e-02, bound:  3.17611009e-01\n",
      "Epoch: 8371 mean train loss:  4.38102894e-02, bound:  3.17610085e-01\n",
      "Epoch: 8372 mean train loss:  4.37928438e-02, bound:  3.17608893e-01\n",
      "Epoch: 8373 mean train loss:  4.37859856e-02, bound:  3.17607552e-01\n",
      "Epoch: 8374 mean train loss:  4.37788703e-02, bound:  3.17606658e-01\n",
      "Epoch: 8375 mean train loss:  4.37629521e-02, bound:  3.17605108e-01\n",
      "Epoch: 8376 mean train loss:  4.37376648e-02, bound:  3.17604244e-01\n",
      "Epoch: 8377 mean train loss:  4.37100269e-02, bound:  3.17602783e-01\n",
      "Epoch: 8378 mean train loss:  4.36870940e-02, bound:  3.17601681e-01\n",
      "Epoch: 8379 mean train loss:  4.36707176e-02, bound:  3.17600459e-01\n",
      "Epoch: 8380 mean train loss:  4.36589681e-02, bound:  3.17599088e-01\n",
      "Epoch: 8381 mean train loss:  4.36465517e-02, bound:  3.17598045e-01\n",
      "Epoch: 8382 mean train loss:  4.36293781e-02, bound:  3.17596585e-01\n",
      "Epoch: 8383 mean train loss:  4.36082296e-02, bound:  3.17595571e-01\n",
      "Epoch: 8384 mean train loss:  4.35859300e-02, bound:  3.17594141e-01\n",
      "Epoch: 8385 mean train loss:  4.35654111e-02, bound:  3.17593008e-01\n",
      "Epoch: 8386 mean train loss:  4.35479619e-02, bound:  3.17591816e-01\n",
      "Epoch: 8387 mean train loss:  4.35327627e-02, bound:  3.17590475e-01\n",
      "Epoch: 8388 mean train loss:  4.35180105e-02, bound:  3.17589432e-01\n",
      "Epoch: 8389 mean train loss:  4.35016006e-02, bound:  3.17588001e-01\n",
      "Epoch: 8390 mean train loss:  4.34832200e-02, bound:  3.17586988e-01\n",
      "Epoch: 8391 mean train loss:  4.34636697e-02, bound:  3.17585617e-01\n",
      "Epoch: 8392 mean train loss:  4.34441417e-02, bound:  3.17584515e-01\n",
      "Epoch: 8393 mean train loss:  4.34256941e-02, bound:  3.17583263e-01\n",
      "Epoch: 8394 mean train loss:  4.34084237e-02, bound:  3.17582041e-01\n",
      "Epoch: 8395 mean train loss:  4.33922596e-02, bound:  3.17580909e-01\n",
      "Epoch: 8396 mean train loss:  4.33758944e-02, bound:  3.17579627e-01\n",
      "Epoch: 8397 mean train loss:  4.33590636e-02, bound:  3.17578495e-01\n",
      "Epoch: 8398 mean train loss:  4.33411784e-02, bound:  3.17577183e-01\n",
      "Epoch: 8399 mean train loss:  4.33226824e-02, bound:  3.17576110e-01\n",
      "Epoch: 8400 mean train loss:  4.33047824e-02, bound:  3.17574799e-01\n",
      "Epoch: 8401 mean train loss:  4.32863981e-02, bound:  3.17573667e-01\n",
      "Epoch: 8402 mean train loss:  4.32691760e-02, bound:  3.17572474e-01\n",
      "Epoch: 8403 mean train loss:  4.32522520e-02, bound:  3.17571223e-01\n",
      "Epoch: 8404 mean train loss:  4.32348624e-02, bound:  3.17570090e-01\n",
      "Epoch: 8405 mean train loss:  4.32178602e-02, bound:  3.17568809e-01\n",
      "Epoch: 8406 mean train loss:  4.32005748e-02, bound:  3.17567676e-01\n",
      "Epoch: 8407 mean train loss:  4.31830734e-02, bound:  3.17566395e-01\n",
      "Epoch: 8408 mean train loss:  4.31650840e-02, bound:  3.17565262e-01\n",
      "Epoch: 8409 mean train loss:  4.31476459e-02, bound:  3.17563981e-01\n",
      "Epoch: 8410 mean train loss:  4.31300066e-02, bound:  3.17562789e-01\n",
      "Epoch: 8411 mean train loss:  4.31122817e-02, bound:  3.17561537e-01\n",
      "Epoch: 8412 mean train loss:  4.30946685e-02, bound:  3.17560315e-01\n",
      "Epoch: 8413 mean train loss:  4.30773459e-02, bound:  3.17559123e-01\n",
      "Epoch: 8414 mean train loss:  4.30601873e-02, bound:  3.17557842e-01\n",
      "Epoch: 8415 mean train loss:  4.30427045e-02, bound:  3.17556679e-01\n",
      "Epoch: 8416 mean train loss:  4.30255756e-02, bound:  3.17555398e-01\n",
      "Epoch: 8417 mean train loss:  4.30080406e-02, bound:  3.17554265e-01\n",
      "Epoch: 8418 mean train loss:  4.29905951e-02, bound:  3.17553014e-01\n",
      "Epoch: 8419 mean train loss:  4.29732874e-02, bound:  3.17551821e-01\n",
      "Epoch: 8420 mean train loss:  4.29553539e-02, bound:  3.17550540e-01\n",
      "Epoch: 8421 mean train loss:  4.29377072e-02, bound:  3.17549407e-01\n",
      "Epoch: 8422 mean train loss:  4.29202542e-02, bound:  3.17548156e-01\n",
      "Epoch: 8423 mean train loss:  4.29028086e-02, bound:  3.17546934e-01\n",
      "Epoch: 8424 mean train loss:  4.28854562e-02, bound:  3.17545712e-01\n",
      "Epoch: 8425 mean train loss:  4.28677835e-02, bound:  3.17544520e-01\n",
      "Epoch: 8426 mean train loss:  4.28504199e-02, bound:  3.17543298e-01\n",
      "Epoch: 8427 mean train loss:  4.28327806e-02, bound:  3.17542046e-01\n",
      "Epoch: 8428 mean train loss:  4.28156927e-02, bound:  3.17540884e-01\n",
      "Epoch: 8429 mean train loss:  4.27982435e-02, bound:  3.17539662e-01\n",
      "Epoch: 8430 mean train loss:  4.27805930e-02, bound:  3.17538470e-01\n",
      "Epoch: 8431 mean train loss:  4.27630916e-02, bound:  3.17537189e-01\n",
      "Epoch: 8432 mean train loss:  4.27459553e-02, bound:  3.17536026e-01\n",
      "Epoch: 8433 mean train loss:  4.27284688e-02, bound:  3.17534804e-01\n",
      "Epoch: 8434 mean train loss:  4.27107103e-02, bound:  3.17533612e-01\n",
      "Epoch: 8435 mean train loss:  4.26931754e-02, bound:  3.17532331e-01\n",
      "Epoch: 8436 mean train loss:  4.26760055e-02, bound:  3.17531198e-01\n",
      "Epoch: 8437 mean train loss:  4.26585227e-02, bound:  3.17529917e-01\n",
      "Epoch: 8438 mean train loss:  4.26405594e-02, bound:  3.17528725e-01\n",
      "Epoch: 8439 mean train loss:  4.26233336e-02, bound:  3.17527473e-01\n",
      "Epoch: 8440 mean train loss:  4.26059142e-02, bound:  3.17526281e-01\n",
      "Epoch: 8441 mean train loss:  4.25886661e-02, bound:  3.17525059e-01\n",
      "Epoch: 8442 mean train loss:  4.25707623e-02, bound:  3.17523897e-01\n",
      "Epoch: 8443 mean train loss:  4.25532125e-02, bound:  3.17522615e-01\n",
      "Epoch: 8444 mean train loss:  4.25362326e-02, bound:  3.17521453e-01\n",
      "Epoch: 8445 mean train loss:  4.25185449e-02, bound:  3.17520201e-01\n",
      "Epoch: 8446 mean train loss:  4.25010435e-02, bound:  3.17518979e-01\n",
      "Epoch: 8447 mean train loss:  4.24839519e-02, bound:  3.17517728e-01\n",
      "Epoch: 8448 mean train loss:  4.24663536e-02, bound:  3.17516565e-01\n",
      "Epoch: 8449 mean train loss:  4.24492322e-02, bound:  3.17515314e-01\n",
      "Epoch: 8450 mean train loss:  4.24321145e-02, bound:  3.17514181e-01\n",
      "Epoch: 8451 mean train loss:  4.24148999e-02, bound:  3.17512870e-01\n",
      "Epoch: 8452 mean train loss:  4.23982702e-02, bound:  3.17511737e-01\n",
      "Epoch: 8453 mean train loss:  4.23816293e-02, bound:  3.17510426e-01\n",
      "Epoch: 8454 mean train loss:  4.23655957e-02, bound:  3.17509323e-01\n",
      "Epoch: 8455 mean train loss:  4.23503742e-02, bound:  3.17507952e-01\n",
      "Epoch: 8456 mean train loss:  4.23361994e-02, bound:  3.17506909e-01\n",
      "Epoch: 8457 mean train loss:  4.23243791e-02, bound:  3.17505479e-01\n",
      "Epoch: 8458 mean train loss:  4.23152000e-02, bound:  3.17504585e-01\n",
      "Epoch: 8459 mean train loss:  4.23108786e-02, bound:  3.17502946e-01\n",
      "Epoch: 8460 mean train loss:  4.23148312e-02, bound:  3.17502260e-01\n",
      "Epoch: 8461 mean train loss:  4.23317328e-02, bound:  3.17500412e-01\n",
      "Epoch: 8462 mean train loss:  4.23705764e-02, bound:  3.17499965e-01\n",
      "Epoch: 8463 mean train loss:  4.24432680e-02, bound:  3.17497760e-01\n",
      "Epoch: 8464 mean train loss:  4.25701737e-02, bound:  3.17497760e-01\n",
      "Epoch: 8465 mean train loss:  4.27776501e-02, bound:  3.17495018e-01\n",
      "Epoch: 8466 mean train loss:  4.30959612e-02, bound:  3.17495704e-01\n",
      "Epoch: 8467 mean train loss:  4.35309932e-02, bound:  3.17492098e-01\n",
      "Epoch: 8468 mean train loss:  4.40330543e-02, bound:  3.17493737e-01\n",
      "Epoch: 8469 mean train loss:  4.44144979e-02, bound:  3.17489296e-01\n",
      "Epoch: 8470 mean train loss:  4.43930998e-02, bound:  3.17491412e-01\n",
      "Epoch: 8471 mean train loss:  4.37771119e-02, bound:  3.17487091e-01\n",
      "Epoch: 8472 mean train loss:  4.28211503e-02, bound:  3.17488372e-01\n",
      "Epoch: 8473 mean train loss:  4.21316065e-02, bound:  3.17485929e-01\n",
      "Epoch: 8474 mean train loss:  4.21120077e-02, bound:  3.17484915e-01\n",
      "Epoch: 8475 mean train loss:  4.25743982e-02, bound:  3.17485034e-01\n",
      "Epoch: 8476 mean train loss:  4.29678820e-02, bound:  3.17482114e-01\n",
      "Epoch: 8477 mean train loss:  4.28869091e-02, bound:  3.17483276e-01\n",
      "Epoch: 8478 mean train loss:  4.24129367e-02, bound:  3.17480475e-01\n",
      "Epoch: 8479 mean train loss:  4.19996753e-02, bound:  3.17480505e-01\n",
      "Epoch: 8480 mean train loss:  4.19748761e-02, bound:  3.17479432e-01\n",
      "Epoch: 8481 mean train loss:  4.22375165e-02, bound:  3.17477494e-01\n",
      "Epoch: 8482 mean train loss:  4.24403995e-02, bound:  3.17477882e-01\n",
      "Epoch: 8483 mean train loss:  4.23575044e-02, bound:  3.17475170e-01\n",
      "Epoch: 8484 mean train loss:  4.20767888e-02, bound:  3.17475349e-01\n",
      "Epoch: 8485 mean train loss:  4.18592133e-02, bound:  3.17473501e-01\n",
      "Epoch: 8486 mean train loss:  4.18609194e-02, bound:  3.17472249e-01\n",
      "Epoch: 8487 mean train loss:  4.20042798e-02, bound:  3.17471743e-01\n",
      "Epoch: 8488 mean train loss:  4.21017185e-02, bound:  3.17469448e-01\n",
      "Epoch: 8489 mean train loss:  4.20440212e-02, bound:  3.17469448e-01\n",
      "Epoch: 8490 mean train loss:  4.18824628e-02, bound:  3.17467332e-01\n",
      "Epoch: 8491 mean train loss:  4.17546406e-02, bound:  3.17466646e-01\n",
      "Epoch: 8492 mean train loss:  4.17422764e-02, bound:  3.17465514e-01\n",
      "Epoch: 8493 mean train loss:  4.18096706e-02, bound:  3.17463934e-01\n",
      "Epoch: 8494 mean train loss:  4.18600701e-02, bound:  3.17463607e-01\n",
      "Epoch: 8495 mean train loss:  4.18302603e-02, bound:  3.17461640e-01\n",
      "Epoch: 8496 mean train loss:  4.17389348e-02, bound:  3.17461252e-01\n",
      "Epoch: 8497 mean train loss:  4.16563079e-02, bound:  3.17459732e-01\n",
      "Epoch: 8498 mean train loss:  4.16312069e-02, bound:  3.17458749e-01\n",
      "Epoch: 8499 mean train loss:  4.16549779e-02, bound:  3.17457944e-01\n",
      "Epoch: 8500 mean train loss:  4.16790731e-02, bound:  3.17456365e-01\n",
      "Epoch: 8501 mean train loss:  4.16648239e-02, bound:  3.17455918e-01\n",
      "Epoch: 8502 mean train loss:  4.16137874e-02, bound:  3.17454278e-01\n",
      "Epoch: 8503 mean train loss:  4.15585116e-02, bound:  3.17453623e-01\n",
      "Epoch: 8504 mean train loss:  4.15289998e-02, bound:  3.17452401e-01\n",
      "Epoch: 8505 mean train loss:  4.15287539e-02, bound:  3.17451209e-01\n",
      "Epoch: 8506 mean train loss:  4.15363722e-02, bound:  3.17450523e-01\n",
      "Epoch: 8507 mean train loss:  4.15279977e-02, bound:  3.17449003e-01\n",
      "Epoch: 8508 mean train loss:  4.14984189e-02, bound:  3.17448437e-01\n",
      "Epoch: 8509 mean train loss:  4.14608307e-02, bound:  3.17447007e-01\n",
      "Epoch: 8510 mean train loss:  4.14316356e-02, bound:  3.17446113e-01\n",
      "Epoch: 8511 mean train loss:  4.14180756e-02, bound:  3.17445070e-01\n",
      "Epoch: 8512 mean train loss:  4.14129980e-02, bound:  3.17443788e-01\n",
      "Epoch: 8513 mean train loss:  4.14061733e-02, bound:  3.17443043e-01\n",
      "Epoch: 8514 mean train loss:  4.13888022e-02, bound:  3.17441612e-01\n",
      "Epoch: 8515 mean train loss:  4.13635224e-02, bound:  3.17440808e-01\n",
      "Epoch: 8516 mean train loss:  4.13369723e-02, bound:  3.17439526e-01\n",
      "Epoch: 8517 mean train loss:  4.13163118e-02, bound:  3.17438453e-01\n",
      "Epoch: 8518 mean train loss:  4.13023755e-02, bound:  3.17437410e-01\n",
      "Epoch: 8519 mean train loss:  4.12916914e-02, bound:  3.17436099e-01\n",
      "Epoch: 8520 mean train loss:  4.12794985e-02, bound:  3.17435145e-01\n",
      "Epoch: 8521 mean train loss:  4.12623994e-02, bound:  3.17433804e-01\n",
      "Epoch: 8522 mean train loss:  4.12413813e-02, bound:  3.17432880e-01\n",
      "Epoch: 8523 mean train loss:  4.12206501e-02, bound:  3.17431599e-01\n",
      "Epoch: 8524 mean train loss:  4.12013531e-02, bound:  3.17430556e-01\n",
      "Epoch: 8525 mean train loss:  4.11857627e-02, bound:  3.17429423e-01\n",
      "Epoch: 8526 mean train loss:  4.11718413e-02, bound:  3.17428201e-01\n",
      "Epoch: 8527 mean train loss:  4.11579683e-02, bound:  3.17427248e-01\n",
      "Epoch: 8528 mean train loss:  4.11419347e-02, bound:  3.17425936e-01\n",
      "Epoch: 8529 mean train loss:  4.11243811e-02, bound:  3.17424983e-01\n",
      "Epoch: 8530 mean train loss:  4.11058404e-02, bound:  3.17423731e-01\n",
      "Epoch: 8531 mean train loss:  4.10874188e-02, bound:  3.17422658e-01\n",
      "Epoch: 8532 mean train loss:  4.10703458e-02, bound:  3.17421526e-01\n",
      "Epoch: 8533 mean train loss:  4.10545543e-02, bound:  3.17420393e-01\n",
      "Epoch: 8534 mean train loss:  4.10391428e-02, bound:  3.17419350e-01\n",
      "Epoch: 8535 mean train loss:  4.10237983e-02, bound:  3.17418158e-01\n",
      "Epoch: 8536 mean train loss:  4.10075672e-02, bound:  3.17417145e-01\n",
      "Epoch: 8537 mean train loss:  4.09904644e-02, bound:  3.17415923e-01\n",
      "Epoch: 8538 mean train loss:  4.09733579e-02, bound:  3.17414910e-01\n",
      "Epoch: 8539 mean train loss:  4.09559757e-02, bound:  3.17413747e-01\n",
      "Epoch: 8540 mean train loss:  4.09390554e-02, bound:  3.17412674e-01\n",
      "Epoch: 8541 mean train loss:  4.09224406e-02, bound:  3.17411572e-01\n",
      "Epoch: 8542 mean train loss:  4.09068614e-02, bound:  3.17410439e-01\n",
      "Epoch: 8543 mean train loss:  4.08905819e-02, bound:  3.17409366e-01\n",
      "Epoch: 8544 mean train loss:  4.08745557e-02, bound:  3.17408204e-01\n",
      "Epoch: 8545 mean train loss:  4.08585370e-02, bound:  3.17407161e-01\n",
      "Epoch: 8546 mean train loss:  4.08418663e-02, bound:  3.17405999e-01\n",
      "Epoch: 8547 mean train loss:  4.08248417e-02, bound:  3.17404956e-01\n",
      "Epoch: 8548 mean train loss:  4.08082381e-02, bound:  3.17403793e-01\n",
      "Epoch: 8549 mean train loss:  4.07917500e-02, bound:  3.17402691e-01\n",
      "Epoch: 8550 mean train loss:  4.07747403e-02, bound:  3.17401558e-01\n",
      "Epoch: 8551 mean train loss:  4.07584906e-02, bound:  3.17400426e-01\n",
      "Epoch: 8552 mean train loss:  4.07420918e-02, bound:  3.17399353e-01\n",
      "Epoch: 8553 mean train loss:  4.07260768e-02, bound:  3.17398190e-01\n",
      "Epoch: 8554 mean train loss:  4.07096818e-02, bound:  3.17397088e-01\n",
      "Epoch: 8555 mean train loss:  4.06933837e-02, bound:  3.17395926e-01\n",
      "Epoch: 8556 mean train loss:  4.06771190e-02, bound:  3.17394853e-01\n",
      "Epoch: 8557 mean train loss:  4.06603068e-02, bound:  3.17393661e-01\n",
      "Epoch: 8558 mean train loss:  4.06436101e-02, bound:  3.17392588e-01\n",
      "Epoch: 8559 mean train loss:  4.06272002e-02, bound:  3.17391455e-01\n",
      "Epoch: 8560 mean train loss:  4.06107605e-02, bound:  3.17390323e-01\n",
      "Epoch: 8561 mean train loss:  4.05943766e-02, bound:  3.17389220e-01\n",
      "Epoch: 8562 mean train loss:  4.05776910e-02, bound:  3.17388088e-01\n",
      "Epoch: 8563 mean train loss:  4.05611806e-02, bound:  3.17386985e-01\n",
      "Epoch: 8564 mean train loss:  4.05451618e-02, bound:  3.17385823e-01\n",
      "Epoch: 8565 mean train loss:  4.05287966e-02, bound:  3.17384750e-01\n",
      "Epoch: 8566 mean train loss:  4.05122451e-02, bound:  3.17383587e-01\n",
      "Epoch: 8567 mean train loss:  4.04959805e-02, bound:  3.17382514e-01\n",
      "Epoch: 8568 mean train loss:  4.04794849e-02, bound:  3.17381352e-01\n",
      "Epoch: 8569 mean train loss:  4.04630676e-02, bound:  3.17380250e-01\n",
      "Epoch: 8570 mean train loss:  4.04468067e-02, bound:  3.17379087e-01\n",
      "Epoch: 8571 mean train loss:  4.04301994e-02, bound:  3.17377985e-01\n",
      "Epoch: 8572 mean train loss:  4.04137559e-02, bound:  3.17376822e-01\n",
      "Epoch: 8573 mean train loss:  4.03971896e-02, bound:  3.17375809e-01\n",
      "Epoch: 8574 mean train loss:  4.03811522e-02, bound:  3.17374617e-01\n",
      "Epoch: 8575 mean train loss:  4.03642170e-02, bound:  3.17373544e-01\n",
      "Epoch: 8576 mean train loss:  4.03478295e-02, bound:  3.17372352e-01\n",
      "Epoch: 8577 mean train loss:  4.03312445e-02, bound:  3.17371279e-01\n",
      "Epoch: 8578 mean train loss:  4.03149948e-02, bound:  3.17370147e-01\n",
      "Epoch: 8579 mean train loss:  4.02987227e-02, bound:  3.17369044e-01\n",
      "Epoch: 8580 mean train loss:  4.02816534e-02, bound:  3.17367941e-01\n",
      "Epoch: 8581 mean train loss:  4.02653404e-02, bound:  3.17366838e-01\n",
      "Epoch: 8582 mean train loss:  4.02491055e-02, bound:  3.17365676e-01\n",
      "Epoch: 8583 mean train loss:  4.02327143e-02, bound:  3.17364603e-01\n",
      "Epoch: 8584 mean train loss:  4.02160957e-02, bound:  3.17363411e-01\n",
      "Epoch: 8585 mean train loss:  4.01994251e-02, bound:  3.17362338e-01\n",
      "Epoch: 8586 mean train loss:  4.01834920e-02, bound:  3.17361206e-01\n",
      "Epoch: 8587 mean train loss:  4.01670113e-02, bound:  3.17360103e-01\n",
      "Epoch: 8588 mean train loss:  4.01504748e-02, bound:  3.17358971e-01\n",
      "Epoch: 8589 mean train loss:  4.01340351e-02, bound:  3.17357868e-01\n",
      "Epoch: 8590 mean train loss:  4.01173346e-02, bound:  3.17356706e-01\n",
      "Epoch: 8591 mean train loss:  4.01011929e-02, bound:  3.17355633e-01\n",
      "Epoch: 8592 mean train loss:  4.00849693e-02, bound:  3.17354441e-01\n",
      "Epoch: 8593 mean train loss:  4.00689393e-02, bound:  3.17353427e-01\n",
      "Epoch: 8594 mean train loss:  4.00528759e-02, bound:  3.17352206e-01\n",
      "Epoch: 8595 mean train loss:  4.00375500e-02, bound:  3.17351192e-01\n",
      "Epoch: 8596 mean train loss:  4.00219858e-02, bound:  3.17349941e-01\n",
      "Epoch: 8597 mean train loss:  4.00073007e-02, bound:  3.17348987e-01\n",
      "Epoch: 8598 mean train loss:  3.99941318e-02, bound:  3.17347705e-01\n",
      "Epoch: 8599 mean train loss:  3.99824679e-02, bound:  3.17346781e-01\n",
      "Epoch: 8600 mean train loss:  3.99736874e-02, bound:  3.17345411e-01\n",
      "Epoch: 8601 mean train loss:  3.99698019e-02, bound:  3.17344606e-01\n",
      "Epoch: 8602 mean train loss:  3.99735123e-02, bound:  3.17343056e-01\n",
      "Epoch: 8603 mean train loss:  3.99916619e-02, bound:  3.17342520e-01\n",
      "Epoch: 8604 mean train loss:  4.00310010e-02, bound:  3.17340672e-01\n",
      "Epoch: 8605 mean train loss:  4.01080362e-02, bound:  3.17340434e-01\n",
      "Epoch: 8606 mean train loss:  4.02447172e-02, bound:  3.17338169e-01\n",
      "Epoch: 8607 mean train loss:  4.04757075e-02, bound:  3.17338526e-01\n",
      "Epoch: 8608 mean train loss:  4.08388823e-02, bound:  3.17335516e-01\n",
      "Epoch: 8609 mean train loss:  4.13607396e-02, bound:  3.17336738e-01\n",
      "Epoch: 8610 mean train loss:  4.19840477e-02, bound:  3.17332745e-01\n",
      "Epoch: 8611 mean train loss:  4.25008573e-02, bound:  3.17334861e-01\n",
      "Epoch: 8612 mean train loss:  4.25125286e-02, bound:  3.17330271e-01\n",
      "Epoch: 8613 mean train loss:  4.17691246e-02, bound:  3.17332357e-01\n",
      "Epoch: 8614 mean train loss:  4.05853949e-02, bound:  3.17328811e-01\n",
      "Epoch: 8615 mean train loss:  3.97886001e-02, bound:  3.17329019e-01\n",
      "Epoch: 8616 mean train loss:  3.98721769e-02, bound:  3.17328185e-01\n",
      "Epoch: 8617 mean train loss:  4.04992327e-02, bound:  3.17325890e-01\n",
      "Epoch: 8618 mean train loss:  4.08983789e-02, bound:  3.17327082e-01\n",
      "Epoch: 8619 mean train loss:  4.06245813e-02, bound:  3.17324013e-01\n",
      "Epoch: 8620 mean train loss:  3.99822332e-02, bound:  3.17324758e-01\n",
      "Epoch: 8621 mean train loss:  3.96329314e-02, bound:  3.17323208e-01\n",
      "Epoch: 8622 mean train loss:  3.98234203e-02, bound:  3.17321867e-01\n",
      "Epoch: 8623 mean train loss:  4.01890315e-02, bound:  3.17322224e-01\n",
      "Epoch: 8624 mean train loss:  4.02515046e-02, bound:  3.17319632e-01\n",
      "Epoch: 8625 mean train loss:  3.99431065e-02, bound:  3.17320138e-01\n",
      "Epoch: 8626 mean train loss:  3.96071672e-02, bound:  3.17318201e-01\n",
      "Epoch: 8627 mean train loss:  3.95597406e-02, bound:  3.17317247e-01\n",
      "Epoch: 8628 mean train loss:  3.97522189e-02, bound:  3.17316860e-01\n",
      "Epoch: 8629 mean train loss:  3.98993269e-02, bound:  3.17314595e-01\n",
      "Epoch: 8630 mean train loss:  3.98204103e-02, bound:  3.17314744e-01\n",
      "Epoch: 8631 mean train loss:  3.96006033e-02, bound:  3.17312717e-01\n",
      "Epoch: 8632 mean train loss:  3.94583978e-02, bound:  3.17312032e-01\n",
      "Epoch: 8633 mean train loss:  3.94909121e-02, bound:  3.17311168e-01\n",
      "Epoch: 8634 mean train loss:  3.96029949e-02, bound:  3.17309439e-01\n",
      "Epoch: 8635 mean train loss:  3.96408662e-02, bound:  3.17309320e-01\n",
      "Epoch: 8636 mean train loss:  3.95533331e-02, bound:  3.17307383e-01\n",
      "Epoch: 8637 mean train loss:  3.94248664e-02, bound:  3.17306995e-01\n",
      "Epoch: 8638 mean train loss:  3.93630639e-02, bound:  3.17305833e-01\n",
      "Epoch: 8639 mean train loss:  3.93923000e-02, bound:  3.17304581e-01\n",
      "Epoch: 8640 mean train loss:  3.94439436e-02, bound:  3.17304194e-01\n",
      "Epoch: 8641 mean train loss:  3.94424647e-02, bound:  3.17302495e-01\n",
      "Epoch: 8642 mean train loss:  3.93781848e-02, bound:  3.17302197e-01\n",
      "Epoch: 8643 mean train loss:  3.93040739e-02, bound:  3.17300797e-01\n",
      "Epoch: 8644 mean train loss:  3.92720252e-02, bound:  3.17299932e-01\n",
      "Epoch: 8645 mean train loss:  3.92856821e-02, bound:  3.17299187e-01\n",
      "Epoch: 8646 mean train loss:  3.93045731e-02, bound:  3.17297786e-01\n",
      "Epoch: 8647 mean train loss:  3.92928347e-02, bound:  3.17297369e-01\n",
      "Epoch: 8648 mean train loss:  3.92486230e-02, bound:  3.17295909e-01\n",
      "Epoch: 8649 mean train loss:  3.92025858e-02, bound:  3.17295283e-01\n",
      "Epoch: 8650 mean train loss:  3.91807258e-02, bound:  3.17294240e-01\n",
      "Epoch: 8651 mean train loss:  3.91817316e-02, bound:  3.17293108e-01\n",
      "Epoch: 8652 mean train loss:  3.91845293e-02, bound:  3.17292482e-01\n",
      "Epoch: 8653 mean train loss:  3.91708538e-02, bound:  3.17291170e-01\n",
      "Epoch: 8654 mean train loss:  3.91401201e-02, bound:  3.17290545e-01\n",
      "Epoch: 8655 mean train loss:  3.91081274e-02, bound:  3.17289352e-01\n",
      "Epoch: 8656 mean train loss:  3.90879437e-02, bound:  3.17288399e-01\n",
      "Epoch: 8657 mean train loss:  3.90807129e-02, bound:  3.17287564e-01\n",
      "Epoch: 8658 mean train loss:  3.90757248e-02, bound:  3.17286342e-01\n",
      "Epoch: 8659 mean train loss:  3.90631370e-02, bound:  3.17285627e-01\n",
      "Epoch: 8660 mean train loss:  3.90416719e-02, bound:  3.17284346e-01\n",
      "Epoch: 8661 mean train loss:  3.90168726e-02, bound:  3.17283571e-01\n",
      "Epoch: 8662 mean train loss:  3.89963202e-02, bound:  3.17282438e-01\n",
      "Epoch: 8663 mean train loss:  3.89827564e-02, bound:  3.17281395e-01\n",
      "Epoch: 8664 mean train loss:  3.89726758e-02, bound:  3.17280471e-01\n",
      "Epoch: 8665 mean train loss:  3.89609821e-02, bound:  3.17279279e-01\n",
      "Epoch: 8666 mean train loss:  3.89448144e-02, bound:  3.17278445e-01\n",
      "Epoch: 8667 mean train loss:  3.89254689e-02, bound:  3.17277223e-01\n",
      "Epoch: 8668 mean train loss:  3.89057845e-02, bound:  3.17276299e-01\n",
      "Epoch: 8669 mean train loss:  3.88892815e-02, bound:  3.17275256e-01\n",
      "Epoch: 8670 mean train loss:  3.88755277e-02, bound:  3.17274183e-01\n",
      "Epoch: 8671 mean train loss:  3.88629138e-02, bound:  3.17273289e-01\n",
      "Epoch: 8672 mean train loss:  3.88488881e-02, bound:  3.17272097e-01\n",
      "Epoch: 8673 mean train loss:  3.88330743e-02, bound:  3.17271233e-01\n",
      "Epoch: 8674 mean train loss:  3.88155058e-02, bound:  3.17270070e-01\n",
      "Epoch: 8675 mean train loss:  3.87984216e-02, bound:  3.17269146e-01\n",
      "Epoch: 8676 mean train loss:  3.87821831e-02, bound:  3.17268074e-01\n",
      "Epoch: 8677 mean train loss:  3.87676209e-02, bound:  3.17267060e-01\n",
      "Epoch: 8678 mean train loss:  3.87535952e-02, bound:  3.17266107e-01\n",
      "Epoch: 8679 mean train loss:  3.87392193e-02, bound:  3.17264974e-01\n",
      "Epoch: 8680 mean train loss:  3.87237370e-02, bound:  3.17264110e-01\n",
      "Epoch: 8681 mean train loss:  3.87078933e-02, bound:  3.17263007e-01\n",
      "Epoch: 8682 mean train loss:  3.86913083e-02, bound:  3.17262053e-01\n",
      "Epoch: 8683 mean train loss:  3.86750661e-02, bound:  3.17261010e-01\n",
      "Epoch: 8684 mean train loss:  3.86596993e-02, bound:  3.17260027e-01\n",
      "Epoch: 8685 mean train loss:  3.86447720e-02, bound:  3.17259014e-01\n",
      "Epoch: 8686 mean train loss:  3.86301018e-02, bound:  3.17258030e-01\n",
      "Epoch: 8687 mean train loss:  3.86153758e-02, bound:  3.17257077e-01\n",
      "Epoch: 8688 mean train loss:  3.85998785e-02, bound:  3.17256033e-01\n",
      "Epoch: 8689 mean train loss:  3.85842472e-02, bound:  3.17255020e-01\n",
      "Epoch: 8690 mean train loss:  3.85685414e-02, bound:  3.17253977e-01\n",
      "Epoch: 8691 mean train loss:  3.85528170e-02, bound:  3.17253023e-01\n",
      "Epoch: 8692 mean train loss:  3.85372192e-02, bound:  3.17252010e-01\n",
      "Epoch: 8693 mean train loss:  3.85223143e-02, bound:  3.17250937e-01\n",
      "Epoch: 8694 mean train loss:  3.85068469e-02, bound:  3.17249984e-01\n",
      "Epoch: 8695 mean train loss:  3.84917669e-02, bound:  3.17248940e-01\n",
      "Epoch: 8696 mean train loss:  3.84767242e-02, bound:  3.17247957e-01\n",
      "Epoch: 8697 mean train loss:  3.84616479e-02, bound:  3.17246854e-01\n",
      "Epoch: 8698 mean train loss:  3.84458154e-02, bound:  3.17245901e-01\n",
      "Epoch: 8699 mean train loss:  3.84302773e-02, bound:  3.17244858e-01\n",
      "Epoch: 8700 mean train loss:  3.84148993e-02, bound:  3.17243814e-01\n",
      "Epoch: 8701 mean train loss:  3.83993760e-02, bound:  3.17242771e-01\n",
      "Epoch: 8702 mean train loss:  3.83841321e-02, bound:  3.17241758e-01\n",
      "Epoch: 8703 mean train loss:  3.83689553e-02, bound:  3.17240745e-01\n",
      "Epoch: 8704 mean train loss:  3.83536071e-02, bound:  3.17239732e-01\n",
      "Epoch: 8705 mean train loss:  3.83385196e-02, bound:  3.17238748e-01\n",
      "Epoch: 8706 mean train loss:  3.83231975e-02, bound:  3.17237675e-01\n",
      "Epoch: 8707 mean train loss:  3.83079015e-02, bound:  3.17236692e-01\n",
      "Epoch: 8708 mean train loss:  3.82925756e-02, bound:  3.17235649e-01\n",
      "Epoch: 8709 mean train loss:  3.82770933e-02, bound:  3.17234665e-01\n",
      "Epoch: 8710 mean train loss:  3.82617116e-02, bound:  3.17233622e-01\n",
      "Epoch: 8711 mean train loss:  3.82465273e-02, bound:  3.17232609e-01\n",
      "Epoch: 8712 mean train loss:  3.82311158e-02, bound:  3.17231566e-01\n",
      "Epoch: 8713 mean train loss:  3.82155068e-02, bound:  3.17230582e-01\n",
      "Epoch: 8714 mean train loss:  3.82002331e-02, bound:  3.17229539e-01\n",
      "Epoch: 8715 mean train loss:  3.81851308e-02, bound:  3.17228526e-01\n",
      "Epoch: 8716 mean train loss:  3.81696969e-02, bound:  3.17227542e-01\n",
      "Epoch: 8717 mean train loss:  3.81545611e-02, bound:  3.17226499e-01\n",
      "Epoch: 8718 mean train loss:  3.81391346e-02, bound:  3.17225486e-01\n",
      "Epoch: 8719 mean train loss:  3.81237604e-02, bound:  3.17224443e-01\n",
      "Epoch: 8720 mean train loss:  3.81086692e-02, bound:  3.17223489e-01\n",
      "Epoch: 8721 mean train loss:  3.80931757e-02, bound:  3.17222416e-01\n",
      "Epoch: 8722 mean train loss:  3.80776711e-02, bound:  3.17221433e-01\n",
      "Epoch: 8723 mean train loss:  3.80623937e-02, bound:  3.17220360e-01\n",
      "Epoch: 8724 mean train loss:  3.80470864e-02, bound:  3.17219406e-01\n",
      "Epoch: 8725 mean train loss:  3.80317122e-02, bound:  3.17218363e-01\n",
      "Epoch: 8726 mean train loss:  3.80162857e-02, bound:  3.17217380e-01\n",
      "Epoch: 8727 mean train loss:  3.80009487e-02, bound:  3.17216337e-01\n",
      "Epoch: 8728 mean train loss:  3.79858613e-02, bound:  3.17215323e-01\n",
      "Epoch: 8729 mean train loss:  3.79702486e-02, bound:  3.17214280e-01\n",
      "Epoch: 8730 mean train loss:  3.79548892e-02, bound:  3.17213297e-01\n",
      "Epoch: 8731 mean train loss:  3.79396491e-02, bound:  3.17212254e-01\n",
      "Epoch: 8732 mean train loss:  3.79245616e-02, bound:  3.17211241e-01\n",
      "Epoch: 8733 mean train loss:  3.79090980e-02, bound:  3.17210227e-01\n",
      "Epoch: 8734 mean train loss:  3.78934145e-02, bound:  3.17209214e-01\n",
      "Epoch: 8735 mean train loss:  3.78783196e-02, bound:  3.17208171e-01\n",
      "Epoch: 8736 mean train loss:  3.78629640e-02, bound:  3.17207158e-01\n",
      "Epoch: 8737 mean train loss:  3.78478020e-02, bound:  3.17206144e-01\n",
      "Epoch: 8738 mean train loss:  3.78325619e-02, bound:  3.17205131e-01\n",
      "Epoch: 8739 mean train loss:  3.78170796e-02, bound:  3.17204118e-01\n",
      "Epoch: 8740 mean train loss:  3.78017388e-02, bound:  3.17203045e-01\n",
      "Epoch: 8741 mean train loss:  3.77861075e-02, bound:  3.17202091e-01\n",
      "Epoch: 8742 mean train loss:  3.77709195e-02, bound:  3.17201048e-01\n",
      "Epoch: 8743 mean train loss:  3.77554633e-02, bound:  3.17200035e-01\n",
      "Epoch: 8744 mean train loss:  3.77402902e-02, bound:  3.17199022e-01\n",
      "Epoch: 8745 mean train loss:  3.77249345e-02, bound:  3.17198008e-01\n",
      "Epoch: 8746 mean train loss:  3.77099887e-02, bound:  3.17196965e-01\n",
      "Epoch: 8747 mean train loss:  3.76944654e-02, bound:  3.17195982e-01\n",
      "Epoch: 8748 mean train loss:  3.76792476e-02, bound:  3.17194939e-01\n",
      "Epoch: 8749 mean train loss:  3.76644209e-02, bound:  3.17193985e-01\n",
      "Epoch: 8750 mean train loss:  3.76489833e-02, bound:  3.17192882e-01\n",
      "Epoch: 8751 mean train loss:  3.76342945e-02, bound:  3.17191929e-01\n",
      "Epoch: 8752 mean train loss:  3.76197584e-02, bound:  3.17190856e-01\n",
      "Epoch: 8753 mean train loss:  3.76059078e-02, bound:  3.17189932e-01\n",
      "Epoch: 8754 mean train loss:  3.75925489e-02, bound:  3.17188770e-01\n",
      "Epoch: 8755 mean train loss:  3.75807248e-02, bound:  3.17187905e-01\n",
      "Epoch: 8756 mean train loss:  3.75709534e-02, bound:  3.17186683e-01\n",
      "Epoch: 8757 mean train loss:  3.75650264e-02, bound:  3.17185938e-01\n",
      "Epoch: 8758 mean train loss:  3.75651270e-02, bound:  3.17184567e-01\n",
      "Epoch: 8759 mean train loss:  3.75755727e-02, bound:  3.17184031e-01\n",
      "Epoch: 8760 mean train loss:  3.76034565e-02, bound:  3.17182451e-01\n",
      "Epoch: 8761 mean train loss:  3.76615413e-02, bound:  3.17182153e-01\n",
      "Epoch: 8762 mean train loss:  3.77677232e-02, bound:  3.17180157e-01\n",
      "Epoch: 8763 mean train loss:  3.79544050e-02, bound:  3.17180395e-01\n",
      "Epoch: 8764 mean train loss:  3.82632874e-02, bound:  3.17177773e-01\n",
      "Epoch: 8765 mean train loss:  3.87407765e-02, bound:  3.17178756e-01\n",
      "Epoch: 8766 mean train loss:  3.93887758e-02, bound:  3.17175180e-01\n",
      "Epoch: 8767 mean train loss:  4.00947668e-02, bound:  3.17177176e-01\n",
      "Epoch: 8768 mean train loss:  4.05021831e-02, bound:  3.17172706e-01\n",
      "Epoch: 8769 mean train loss:  4.01728190e-02, bound:  3.17175090e-01\n",
      "Epoch: 8770 mean train loss:  3.90219316e-02, bound:  3.17171037e-01\n",
      "Epoch: 8771 mean train loss:  3.77711840e-02, bound:  3.17172021e-01\n",
      "Epoch: 8772 mean train loss:  3.73359472e-02, bound:  3.17170531e-01\n",
      "Epoch: 8773 mean train loss:  3.78387310e-02, bound:  3.17168832e-01\n",
      "Epoch: 8774 mean train loss:  3.85237448e-02, bound:  3.17169815e-01\n",
      "Epoch: 8775 mean train loss:  3.85741442e-02, bound:  3.17166805e-01\n",
      "Epoch: 8776 mean train loss:  3.79400626e-02, bound:  3.17167968e-01\n",
      "Epoch: 8777 mean train loss:  3.73330452e-02, bound:  3.17166060e-01\n",
      "Epoch: 8778 mean train loss:  3.73407938e-02, bound:  3.17165226e-01\n",
      "Epoch: 8779 mean train loss:  3.77652720e-02, bound:  3.17165494e-01\n",
      "Epoch: 8780 mean train loss:  3.79866920e-02, bound:  3.17163050e-01\n",
      "Epoch: 8781 mean train loss:  3.77287902e-02, bound:  3.17163795e-01\n",
      "Epoch: 8782 mean train loss:  3.73086408e-02, bound:  3.17161828e-01\n",
      "Epoch: 8783 mean train loss:  3.71770151e-02, bound:  3.17161143e-01\n",
      "Epoch: 8784 mean train loss:  3.73763666e-02, bound:  3.17160815e-01\n",
      "Epoch: 8785 mean train loss:  3.75771075e-02, bound:  3.17158639e-01\n",
      "Epoch: 8786 mean train loss:  3.75150144e-02, bound:  3.17158967e-01\n",
      "Epoch: 8787 mean train loss:  3.72651294e-02, bound:  3.17156971e-01\n",
      "Epoch: 8788 mean train loss:  3.70975584e-02, bound:  3.17156434e-01\n",
      "Epoch: 8789 mean train loss:  3.71431820e-02, bound:  3.17155659e-01\n",
      "Epoch: 8790 mean train loss:  3.72808836e-02, bound:  3.17153960e-01\n",
      "Epoch: 8791 mean train loss:  3.73175405e-02, bound:  3.17153960e-01\n",
      "Epoch: 8792 mean train loss:  3.72049771e-02, bound:  3.17152113e-01\n",
      "Epoch: 8793 mean train loss:  3.70598324e-02, bound:  3.17151785e-01\n",
      "Epoch: 8794 mean train loss:  3.70116681e-02, bound:  3.17150712e-01\n",
      "Epoch: 8795 mean train loss:  3.70660946e-02, bound:  3.17149520e-01\n",
      "Epoch: 8796 mean train loss:  3.71249430e-02, bound:  3.17149282e-01\n",
      "Epoch: 8797 mean train loss:  3.71051058e-02, bound:  3.17147672e-01\n",
      "Epoch: 8798 mean train loss:  3.70194092e-02, bound:  3.17147434e-01\n",
      "Epoch: 8799 mean train loss:  3.69448327e-02, bound:  3.17146212e-01\n",
      "Epoch: 8800 mean train loss:  3.69323976e-02, bound:  3.17145377e-01\n",
      "Epoch: 8801 mean train loss:  3.69628929e-02, bound:  3.17144841e-01\n",
      "Epoch: 8802 mean train loss:  3.69793586e-02, bound:  3.17143440e-01\n",
      "Epoch: 8803 mean train loss:  3.69489454e-02, bound:  3.17143142e-01\n",
      "Epoch: 8804 mean train loss:  3.68926525e-02, bound:  3.17141831e-01\n",
      "Epoch: 8805 mean train loss:  3.68525870e-02, bound:  3.17141175e-01\n",
      "Epoch: 8806 mean train loss:  3.68479379e-02, bound:  3.17140341e-01\n",
      "Epoch: 8807 mean train loss:  3.68591174e-02, bound:  3.17139238e-01\n",
      "Epoch: 8808 mean train loss:  3.68568487e-02, bound:  3.17138761e-01\n",
      "Epoch: 8809 mean train loss:  3.68299149e-02, bound:  3.17137480e-01\n",
      "Epoch: 8810 mean train loss:  3.67928930e-02, bound:  3.17136884e-01\n",
      "Epoch: 8811 mean train loss:  3.67670543e-02, bound:  3.17135930e-01\n",
      "Epoch: 8812 mean train loss:  3.67594287e-02, bound:  3.17134976e-01\n",
      "Epoch: 8813 mean train loss:  3.67590263e-02, bound:  3.17134321e-01\n",
      "Epoch: 8814 mean train loss:  3.67506035e-02, bound:  3.17133158e-01\n",
      "Epoch: 8815 mean train loss:  3.67291570e-02, bound:  3.17132562e-01\n",
      "Epoch: 8816 mean train loss:  3.67026478e-02, bound:  3.17131460e-01\n",
      "Epoch: 8817 mean train loss:  3.66819650e-02, bound:  3.17130655e-01\n",
      "Epoch: 8818 mean train loss:  3.66709903e-02, bound:  3.17129791e-01\n",
      "Epoch: 8819 mean train loss:  3.66636030e-02, bound:  3.17128748e-01\n",
      "Epoch: 8820 mean train loss:  3.66534516e-02, bound:  3.17128062e-01\n",
      "Epoch: 8821 mean train loss:  3.66366021e-02, bound:  3.17126930e-01\n",
      "Epoch: 8822 mean train loss:  3.66162881e-02, bound:  3.17126215e-01\n",
      "Epoch: 8823 mean train loss:  3.65976542e-02, bound:  3.17125171e-01\n",
      "Epoch: 8824 mean train loss:  3.65831330e-02, bound:  3.17124248e-01\n",
      "Epoch: 8825 mean train loss:  3.65720615e-02, bound:  3.17123413e-01\n",
      "Epoch: 8826 mean train loss:  3.65611836e-02, bound:  3.17122340e-01\n",
      "Epoch: 8827 mean train loss:  3.65473479e-02, bound:  3.17121595e-01\n",
      "Epoch: 8828 mean train loss:  3.65306549e-02, bound:  3.17120522e-01\n",
      "Epoch: 8829 mean train loss:  3.65132689e-02, bound:  3.17119688e-01\n",
      "Epoch: 8830 mean train loss:  3.64975072e-02, bound:  3.17118734e-01\n",
      "Epoch: 8831 mean train loss:  3.64841595e-02, bound:  3.17117780e-01\n",
      "Epoch: 8832 mean train loss:  3.64715569e-02, bound:  3.17116976e-01\n",
      "Epoch: 8833 mean train loss:  3.64589132e-02, bound:  3.17115903e-01\n",
      "Epoch: 8834 mean train loss:  3.64447013e-02, bound:  3.17115098e-01\n",
      "Epoch: 8835 mean train loss:  3.64294238e-02, bound:  3.17114025e-01\n",
      "Epoch: 8836 mean train loss:  3.64133306e-02, bound:  3.17113221e-01\n",
      "Epoch: 8837 mean train loss:  3.63982022e-02, bound:  3.17112267e-01\n",
      "Epoch: 8838 mean train loss:  3.63844559e-02, bound:  3.17111373e-01\n",
      "Epoch: 8839 mean train loss:  3.63709070e-02, bound:  3.17110509e-01\n",
      "Epoch: 8840 mean train loss:  3.63573693e-02, bound:  3.17109525e-01\n",
      "Epoch: 8841 mean train loss:  3.63432951e-02, bound:  3.17108721e-01\n",
      "Epoch: 8842 mean train loss:  3.63285877e-02, bound:  3.17107737e-01\n",
      "Epoch: 8843 mean train loss:  3.63136567e-02, bound:  3.17106873e-01\n",
      "Epoch: 8844 mean train loss:  3.62988450e-02, bound:  3.17105919e-01\n",
      "Epoch: 8845 mean train loss:  3.62848788e-02, bound:  3.17104995e-01\n",
      "Epoch: 8846 mean train loss:  3.62710431e-02, bound:  3.17104161e-01\n",
      "Epoch: 8847 mean train loss:  3.62569578e-02, bound:  3.17103207e-01\n",
      "Epoch: 8848 mean train loss:  3.62429135e-02, bound:  3.17102343e-01\n",
      "Epoch: 8849 mean train loss:  3.62286940e-02, bound:  3.17101389e-01\n",
      "Epoch: 8850 mean train loss:  3.62144373e-02, bound:  3.17100525e-01\n",
      "Epoch: 8851 mean train loss:  3.61998156e-02, bound:  3.17099571e-01\n",
      "Epoch: 8852 mean train loss:  3.61852460e-02, bound:  3.17098677e-01\n",
      "Epoch: 8853 mean train loss:  3.61714661e-02, bound:  3.17097783e-01\n",
      "Epoch: 8854 mean train loss:  3.61569934e-02, bound:  3.17096829e-01\n",
      "Epoch: 8855 mean train loss:  3.61430198e-02, bound:  3.17095935e-01\n",
      "Epoch: 8856 mean train loss:  3.61292213e-02, bound:  3.17094982e-01\n",
      "Epoch: 8857 mean train loss:  3.61149944e-02, bound:  3.17094088e-01\n",
      "Epoch: 8858 mean train loss:  3.61004733e-02, bound:  3.17093164e-01\n",
      "Epoch: 8859 mean train loss:  3.60861272e-02, bound:  3.17092240e-01\n",
      "Epoch: 8860 mean train loss:  3.60716246e-02, bound:  3.17091286e-01\n",
      "Epoch: 8861 mean train loss:  3.60575505e-02, bound:  3.17090392e-01\n",
      "Epoch: 8862 mean train loss:  3.60432006e-02, bound:  3.17089468e-01\n",
      "Epoch: 8863 mean train loss:  3.60292494e-02, bound:  3.17088544e-01\n",
      "Epoch: 8864 mean train loss:  3.60151269e-02, bound:  3.17087620e-01\n",
      "Epoch: 8865 mean train loss:  3.60007733e-02, bound:  3.17086726e-01\n",
      "Epoch: 8866 mean train loss:  3.59867737e-02, bound:  3.17085803e-01\n",
      "Epoch: 8867 mean train loss:  3.59723419e-02, bound:  3.17084849e-01\n",
      "Epoch: 8868 mean train loss:  3.59581336e-02, bound:  3.17083925e-01\n",
      "Epoch: 8869 mean train loss:  3.59437317e-02, bound:  3.17083031e-01\n",
      "Epoch: 8870 mean train loss:  3.59297805e-02, bound:  3.17082107e-01\n",
      "Epoch: 8871 mean train loss:  3.59156244e-02, bound:  3.17081183e-01\n",
      "Epoch: 8872 mean train loss:  3.59013341e-02, bound:  3.17080230e-01\n",
      "Epoch: 8873 mean train loss:  3.58871147e-02, bound:  3.17079365e-01\n",
      "Epoch: 8874 mean train loss:  3.58727053e-02, bound:  3.17078441e-01\n",
      "Epoch: 8875 mean train loss:  3.58585678e-02, bound:  3.17077488e-01\n",
      "Epoch: 8876 mean train loss:  3.58446054e-02, bound:  3.17076623e-01\n",
      "Epoch: 8877 mean train loss:  3.58301699e-02, bound:  3.17075670e-01\n",
      "Epoch: 8878 mean train loss:  3.58160287e-02, bound:  3.17074746e-01\n",
      "Epoch: 8879 mean train loss:  3.58020738e-02, bound:  3.17073852e-01\n",
      "Epoch: 8880 mean train loss:  3.57873812e-02, bound:  3.17072928e-01\n",
      "Epoch: 8881 mean train loss:  3.57734933e-02, bound:  3.17072034e-01\n",
      "Epoch: 8882 mean train loss:  3.57589088e-02, bound:  3.17071110e-01\n",
      "Epoch: 8883 mean train loss:  3.57451662e-02, bound:  3.17070186e-01\n",
      "Epoch: 8884 mean train loss:  3.57306711e-02, bound:  3.17069262e-01\n",
      "Epoch: 8885 mean train loss:  3.57164480e-02, bound:  3.17068338e-01\n",
      "Epoch: 8886 mean train loss:  3.57026234e-02, bound:  3.17067444e-01\n",
      "Epoch: 8887 mean train loss:  3.56881022e-02, bound:  3.17066491e-01\n",
      "Epoch: 8888 mean train loss:  3.56738903e-02, bound:  3.17065567e-01\n",
      "Epoch: 8889 mean train loss:  3.56596597e-02, bound:  3.17064643e-01\n",
      "Epoch: 8890 mean train loss:  3.56454402e-02, bound:  3.17063749e-01\n",
      "Epoch: 8891 mean train loss:  3.56315523e-02, bound:  3.17062825e-01\n",
      "Epoch: 8892 mean train loss:  3.56170908e-02, bound:  3.17061931e-01\n",
      "Epoch: 8893 mean train loss:  3.56030203e-02, bound:  3.17061007e-01\n",
      "Epoch: 8894 mean train loss:  3.55887935e-02, bound:  3.17060083e-01\n",
      "Epoch: 8895 mean train loss:  3.55744176e-02, bound:  3.17059159e-01\n",
      "Epoch: 8896 mean train loss:  3.55603807e-02, bound:  3.17058235e-01\n",
      "Epoch: 8897 mean train loss:  3.55459638e-02, bound:  3.17057312e-01\n",
      "Epoch: 8898 mean train loss:  3.55318449e-02, bound:  3.17056388e-01\n",
      "Epoch: 8899 mean train loss:  3.55175510e-02, bound:  3.17055464e-01\n",
      "Epoch: 8900 mean train loss:  3.55036743e-02, bound:  3.17054570e-01\n",
      "Epoch: 8901 mean train loss:  3.54889967e-02, bound:  3.17053616e-01\n",
      "Epoch: 8902 mean train loss:  3.54749486e-02, bound:  3.17052692e-01\n",
      "Epoch: 8903 mean train loss:  3.54607031e-02, bound:  3.17051798e-01\n",
      "Epoch: 8904 mean train loss:  3.54466513e-02, bound:  3.17050874e-01\n",
      "Epoch: 8905 mean train loss:  3.54323424e-02, bound:  3.17049921e-01\n",
      "Epoch: 8906 mean train loss:  3.54181565e-02, bound:  3.17049056e-01\n",
      "Epoch: 8907 mean train loss:  3.54039483e-02, bound:  3.17048103e-01\n",
      "Epoch: 8908 mean train loss:  3.53897065e-02, bound:  3.17047179e-01\n",
      "Epoch: 8909 mean train loss:  3.53752673e-02, bound:  3.17046255e-01\n",
      "Epoch: 8910 mean train loss:  3.53613682e-02, bound:  3.17045361e-01\n",
      "Epoch: 8911 mean train loss:  3.53470743e-02, bound:  3.17044407e-01\n",
      "Epoch: 8912 mean train loss:  3.53331156e-02, bound:  3.17043513e-01\n",
      "Epoch: 8913 mean train loss:  3.53188999e-02, bound:  3.17042559e-01\n",
      "Epoch: 8914 mean train loss:  3.53046507e-02, bound:  3.17041695e-01\n",
      "Epoch: 8915 mean train loss:  3.52908187e-02, bound:  3.17040741e-01\n",
      "Epoch: 8916 mean train loss:  3.52770947e-02, bound:  3.17039877e-01\n",
      "Epoch: 8917 mean train loss:  3.52635607e-02, bound:  3.17038864e-01\n",
      "Epoch: 8918 mean train loss:  3.52503732e-02, bound:  3.17038089e-01\n",
      "Epoch: 8919 mean train loss:  3.52379382e-02, bound:  3.17037046e-01\n",
      "Epoch: 8920 mean train loss:  3.52264084e-02, bound:  3.17036271e-01\n",
      "Epoch: 8921 mean train loss:  3.52162682e-02, bound:  3.17035168e-01\n",
      "Epoch: 8922 mean train loss:  3.52090597e-02, bound:  3.17034483e-01\n",
      "Epoch: 8923 mean train loss:  3.52067798e-02, bound:  3.17033261e-01\n",
      "Epoch: 8924 mean train loss:  3.52119096e-02, bound:  3.17032725e-01\n",
      "Epoch: 8925 mean train loss:  3.52311209e-02, bound:  3.17031324e-01\n",
      "Epoch: 8926 mean train loss:  3.52724865e-02, bound:  3.17031026e-01\n",
      "Epoch: 8927 mean train loss:  3.53531353e-02, bound:  3.17029297e-01\n",
      "Epoch: 8928 mean train loss:  3.54984663e-02, bound:  3.17029446e-01\n",
      "Epoch: 8929 mean train loss:  3.57460380e-02, bound:  3.17027152e-01\n",
      "Epoch: 8930 mean train loss:  3.61487456e-02, bound:  3.17027956e-01\n",
      "Epoch: 8931 mean train loss:  3.67440805e-02, bound:  3.17024827e-01\n",
      "Epoch: 8932 mean train loss:  3.75062972e-02, bound:  3.17026585e-01\n",
      "Epoch: 8933 mean train loss:  3.82006988e-02, bound:  3.17022473e-01\n",
      "Epoch: 8934 mean train loss:  3.83759178e-02, bound:  3.17024887e-01\n",
      "Epoch: 8935 mean train loss:  3.75995487e-02, bound:  3.17020684e-01\n",
      "Epoch: 8936 mean train loss:  3.61711122e-02, bound:  3.17022264e-01\n",
      "Epoch: 8937 mean train loss:  3.51109728e-02, bound:  3.17020029e-01\n",
      "Epoch: 8938 mean train loss:  3.51530835e-02, bound:  3.17019105e-01\n",
      "Epoch: 8939 mean train loss:  3.59286405e-02, bound:  3.17019612e-01\n",
      "Epoch: 8940 mean train loss:  3.64244059e-02, bound:  3.17016840e-01\n",
      "Epoch: 8941 mean train loss:  3.60585041e-02, bound:  3.17018270e-01\n",
      "Epoch: 8942 mean train loss:  3.52639072e-02, bound:  3.17016035e-01\n",
      "Epoch: 8943 mean train loss:  3.49228457e-02, bound:  3.17015827e-01\n",
      "Epoch: 8944 mean train loss:  3.52574959e-02, bound:  3.17015737e-01\n",
      "Epoch: 8945 mean train loss:  3.56794409e-02, bound:  3.17013592e-01\n",
      "Epoch: 8946 mean train loss:  3.56127694e-02, bound:  3.17014545e-01\n",
      "Epoch: 8947 mean train loss:  3.51536497e-02, bound:  3.17012429e-01\n",
      "Epoch: 8948 mean train loss:  3.48529443e-02, bound:  3.17012221e-01\n",
      "Epoch: 8949 mean train loss:  3.49763036e-02, bound:  3.17011714e-01\n",
      "Epoch: 8950 mean train loss:  3.52483019e-02, bound:  3.17009866e-01\n",
      "Epoch: 8951 mean train loss:  3.52807567e-02, bound:  3.17010283e-01\n",
      "Epoch: 8952 mean train loss:  3.50344926e-02, bound:  3.17008287e-01\n",
      "Epoch: 8953 mean train loss:  3.48017886e-02, bound:  3.17007989e-01\n",
      "Epoch: 8954 mean train loss:  3.48046198e-02, bound:  3.17007154e-01\n",
      "Epoch: 8955 mean train loss:  3.49603258e-02, bound:  3.17005634e-01\n",
      "Epoch: 8956 mean train loss:  3.50353979e-02, bound:  3.17005664e-01\n",
      "Epoch: 8957 mean train loss:  3.49310227e-02, bound:  3.17003876e-01\n",
      "Epoch: 8958 mean train loss:  3.47654931e-02, bound:  3.17003697e-01\n",
      "Epoch: 8959 mean train loss:  3.46999876e-02, bound:  3.17002684e-01\n",
      "Epoch: 8960 mean train loss:  3.47602405e-02, bound:  3.17001551e-01\n",
      "Epoch: 8961 mean train loss:  3.48342918e-02, bound:  3.17001402e-01\n",
      "Epoch: 8962 mean train loss:  3.48192602e-02, bound:  3.16999853e-01\n",
      "Epoch: 8963 mean train loss:  3.47244591e-02, bound:  3.16999733e-01\n",
      "Epoch: 8964 mean train loss:  3.46410312e-02, bound:  3.16998601e-01\n",
      "Epoch: 8965 mean train loss:  3.46314944e-02, bound:  3.16997826e-01\n",
      "Epoch: 8966 mean train loss:  3.46713141e-02, bound:  3.16997409e-01\n",
      "Epoch: 8967 mean train loss:  3.46917994e-02, bound:  3.16996127e-01\n",
      "Epoch: 8968 mean train loss:  3.46569717e-02, bound:  3.16995919e-01\n",
      "Epoch: 8969 mean train loss:  3.45949531e-02, bound:  3.16994697e-01\n",
      "Epoch: 8970 mean train loss:  3.45544219e-02, bound:  3.16994131e-01\n",
      "Epoch: 8971 mean train loss:  3.45547572e-02, bound:  3.16993415e-01\n",
      "Epoch: 8972 mean train loss:  3.45713310e-02, bound:  3.16992342e-01\n",
      "Epoch: 8973 mean train loss:  3.45688090e-02, bound:  3.16991955e-01\n",
      "Epoch: 8974 mean train loss:  3.45376432e-02, bound:  3.16990793e-01\n",
      "Epoch: 8975 mean train loss:  3.44986208e-02, bound:  3.16990316e-01\n",
      "Epoch: 8976 mean train loss:  3.44753340e-02, bound:  3.16989422e-01\n",
      "Epoch: 8977 mean train loss:  3.44733186e-02, bound:  3.16988528e-01\n",
      "Epoch: 8978 mean train loss:  3.44764441e-02, bound:  3.16987991e-01\n",
      "Epoch: 8979 mean train loss:  3.44657414e-02, bound:  3.16986889e-01\n",
      "Epoch: 8980 mean train loss:  3.44412811e-02, bound:  3.16986382e-01\n",
      "Epoch: 8981 mean train loss:  3.44143659e-02, bound:  3.16985399e-01\n",
      "Epoch: 8982 mean train loss:  3.43971848e-02, bound:  3.16984653e-01\n",
      "Epoch: 8983 mean train loss:  3.43904942e-02, bound:  3.16983968e-01\n",
      "Epoch: 8984 mean train loss:  3.43854353e-02, bound:  3.16983014e-01\n",
      "Epoch: 8985 mean train loss:  3.43743488e-02, bound:  3.16982418e-01\n",
      "Epoch: 8986 mean train loss:  3.43551859e-02, bound:  3.16981465e-01\n",
      "Epoch: 8987 mean train loss:  3.43347229e-02, bound:  3.16980779e-01\n",
      "Epoch: 8988 mean train loss:  3.43183689e-02, bound:  3.16979945e-01\n",
      "Epoch: 8989 mean train loss:  3.43083963e-02, bound:  3.16979051e-01\n",
      "Epoch: 8990 mean train loss:  3.42996009e-02, bound:  3.16978395e-01\n",
      "Epoch: 8991 mean train loss:  3.42886262e-02, bound:  3.16977352e-01\n",
      "Epoch: 8992 mean train loss:  3.42730768e-02, bound:  3.16976756e-01\n",
      "Epoch: 8993 mean train loss:  3.42556350e-02, bound:  3.16975772e-01\n",
      "Epoch: 8994 mean train loss:  3.42401452e-02, bound:  3.16974998e-01\n",
      "Epoch: 8995 mean train loss:  3.42272781e-02, bound:  3.16974193e-01\n",
      "Epoch: 8996 mean train loss:  3.42165343e-02, bound:  3.16973269e-01\n",
      "Epoch: 8997 mean train loss:  3.42053697e-02, bound:  3.16972554e-01\n",
      "Epoch: 8998 mean train loss:  3.41921858e-02, bound:  3.16971600e-01\n",
      "Epoch: 8999 mean train loss:  3.41774300e-02, bound:  3.16970885e-01\n",
      "Epoch: 9000 mean train loss:  3.41623165e-02, bound:  3.16969991e-01\n",
      "Epoch: 9001 mean train loss:  3.41483764e-02, bound:  3.16969186e-01\n",
      "Epoch: 9002 mean train loss:  3.41356806e-02, bound:  3.16968352e-01\n",
      "Epoch: 9003 mean train loss:  3.41237038e-02, bound:  3.16967487e-01\n",
      "Epoch: 9004 mean train loss:  3.41116413e-02, bound:  3.16966772e-01\n",
      "Epoch: 9005 mean train loss:  3.40985060e-02, bound:  3.16965878e-01\n",
      "Epoch: 9006 mean train loss:  3.40841822e-02, bound:  3.16965133e-01\n",
      "Epoch: 9007 mean train loss:  3.40705775e-02, bound:  3.16964239e-01\n",
      "Epoch: 9008 mean train loss:  3.40564810e-02, bound:  3.16963464e-01\n",
      "Epoch: 9009 mean train loss:  3.40436064e-02, bound:  3.16962630e-01\n",
      "Epoch: 9010 mean train loss:  3.40310819e-02, bound:  3.16961795e-01\n",
      "Epoch: 9011 mean train loss:  3.40185240e-02, bound:  3.16961050e-01\n",
      "Epoch: 9012 mean train loss:  3.40053737e-02, bound:  3.16960186e-01\n",
      "Epoch: 9013 mean train loss:  3.39920893e-02, bound:  3.16959441e-01\n",
      "Epoch: 9014 mean train loss:  3.39786783e-02, bound:  3.16958576e-01\n",
      "Epoch: 9015 mean train loss:  3.39651369e-02, bound:  3.16957772e-01\n",
      "Epoch: 9016 mean train loss:  3.39517444e-02, bound:  3.16956967e-01\n",
      "Epoch: 9017 mean train loss:  3.39390785e-02, bound:  3.16956162e-01\n",
      "Epoch: 9018 mean train loss:  3.39262113e-02, bound:  3.16955328e-01\n",
      "Epoch: 9019 mean train loss:  3.39131951e-02, bound:  3.16954494e-01\n",
      "Epoch: 9020 mean train loss:  3.39001678e-02, bound:  3.16953689e-01\n",
      "Epoch: 9021 mean train loss:  3.38867344e-02, bound:  3.16952854e-01\n",
      "Epoch: 9022 mean train loss:  3.38738002e-02, bound:  3.16952050e-01\n",
      "Epoch: 9023 mean train loss:  3.38602774e-02, bound:  3.16951215e-01\n",
      "Epoch: 9024 mean train loss:  3.38470712e-02, bound:  3.16950411e-01\n",
      "Epoch: 9025 mean train loss:  3.38340886e-02, bound:  3.16949606e-01\n",
      "Epoch: 9026 mean train loss:  3.38214859e-02, bound:  3.16948742e-01\n",
      "Epoch: 9027 mean train loss:  3.38081345e-02, bound:  3.16947937e-01\n",
      "Epoch: 9028 mean train loss:  3.37951370e-02, bound:  3.16947073e-01\n",
      "Epoch: 9029 mean train loss:  3.37823145e-02, bound:  3.16946298e-01\n",
      "Epoch: 9030 mean train loss:  3.37687656e-02, bound:  3.16945463e-01\n",
      "Epoch: 9031 mean train loss:  3.37555744e-02, bound:  3.16944629e-01\n",
      "Epoch: 9032 mean train loss:  3.37427258e-02, bound:  3.16943794e-01\n",
      "Epoch: 9033 mean train loss:  3.37292366e-02, bound:  3.16942960e-01\n",
      "Epoch: 9034 mean train loss:  3.37162241e-02, bound:  3.16942155e-01\n",
      "Epoch: 9035 mean train loss:  3.37035619e-02, bound:  3.16941291e-01\n",
      "Epoch: 9036 mean train loss:  3.36904749e-02, bound:  3.16940516e-01\n",
      "Epoch: 9037 mean train loss:  3.36771682e-02, bound:  3.16939652e-01\n",
      "Epoch: 9038 mean train loss:  3.36640067e-02, bound:  3.16938847e-01\n",
      "Epoch: 9039 mean train loss:  3.36511545e-02, bound:  3.16938013e-01\n",
      "Epoch: 9040 mean train loss:  3.36378962e-02, bound:  3.16937208e-01\n",
      "Epoch: 9041 mean train loss:  3.36244404e-02, bound:  3.16936374e-01\n",
      "Epoch: 9042 mean train loss:  3.36114541e-02, bound:  3.16935569e-01\n",
      "Epoch: 9043 mean train loss:  3.35987285e-02, bound:  3.16934735e-01\n",
      "Epoch: 9044 mean train loss:  3.35853323e-02, bound:  3.16933900e-01\n",
      "Epoch: 9045 mean train loss:  3.35723013e-02, bound:  3.16933095e-01\n",
      "Epoch: 9046 mean train loss:  3.35591361e-02, bound:  3.16932231e-01\n",
      "Epoch: 9047 mean train loss:  3.35462205e-02, bound:  3.16931456e-01\n",
      "Epoch: 9048 mean train loss:  3.35329510e-02, bound:  3.16930622e-01\n",
      "Epoch: 9049 mean train loss:  3.35199907e-02, bound:  3.16929787e-01\n",
      "Epoch: 9050 mean train loss:  3.35068107e-02, bound:  3.16928953e-01\n",
      "Epoch: 9051 mean train loss:  3.34939621e-02, bound:  3.16928178e-01\n",
      "Epoch: 9052 mean train loss:  3.34806927e-02, bound:  3.16927314e-01\n",
      "Epoch: 9053 mean train loss:  3.34675312e-02, bound:  3.16926509e-01\n",
      "Epoch: 9054 mean train loss:  3.34544107e-02, bound:  3.16925675e-01\n",
      "Epoch: 9055 mean train loss:  3.34414057e-02, bound:  3.16924870e-01\n",
      "Epoch: 9056 mean train loss:  3.34281065e-02, bound:  3.16924065e-01\n",
      "Epoch: 9057 mean train loss:  3.34151573e-02, bound:  3.16923231e-01\n",
      "Epoch: 9058 mean train loss:  3.34020853e-02, bound:  3.16922426e-01\n",
      "Epoch: 9059 mean train loss:  3.33892182e-02, bound:  3.16921592e-01\n",
      "Epoch: 9060 mean train loss:  3.33758853e-02, bound:  3.16920757e-01\n",
      "Epoch: 9061 mean train loss:  3.33627686e-02, bound:  3.16919923e-01\n",
      "Epoch: 9062 mean train loss:  3.33497934e-02, bound:  3.16919088e-01\n",
      "Epoch: 9063 mean train loss:  3.33369523e-02, bound:  3.16918314e-01\n",
      "Epoch: 9064 mean train loss:  3.33234370e-02, bound:  3.16917449e-01\n",
      "Epoch: 9065 mean train loss:  3.33105735e-02, bound:  3.16916615e-01\n",
      "Epoch: 9066 mean train loss:  3.32973897e-02, bound:  3.16915780e-01\n",
      "Epoch: 9067 mean train loss:  3.32846083e-02, bound:  3.16915005e-01\n",
      "Epoch: 9068 mean train loss:  3.32713164e-02, bound:  3.16914171e-01\n",
      "Epoch: 9069 mean train loss:  3.32580544e-02, bound:  3.16913337e-01\n",
      "Epoch: 9070 mean train loss:  3.32449675e-02, bound:  3.16912502e-01\n",
      "Epoch: 9071 mean train loss:  3.32321450e-02, bound:  3.16911668e-01\n",
      "Epoch: 9072 mean train loss:  3.32188606e-02, bound:  3.16910863e-01\n",
      "Epoch: 9073 mean train loss:  3.32058221e-02, bound:  3.16910058e-01\n",
      "Epoch: 9074 mean train loss:  3.31927650e-02, bound:  3.16909224e-01\n",
      "Epoch: 9075 mean train loss:  3.31794955e-02, bound:  3.16908389e-01\n",
      "Epoch: 9076 mean train loss:  3.31665874e-02, bound:  3.16907585e-01\n",
      "Epoch: 9077 mean train loss:  3.31536196e-02, bound:  3.16906750e-01\n",
      "Epoch: 9078 mean train loss:  3.31404917e-02, bound:  3.16905916e-01\n",
      "Epoch: 9079 mean train loss:  3.31272744e-02, bound:  3.16905141e-01\n",
      "Epoch: 9080 mean train loss:  3.31144445e-02, bound:  3.16904306e-01\n",
      "Epoch: 9081 mean train loss:  3.31013948e-02, bound:  3.16903502e-01\n",
      "Epoch: 9082 mean train loss:  3.30888815e-02, bound:  3.16902637e-01\n",
      "Epoch: 9083 mean train loss:  3.30756269e-02, bound:  3.16901863e-01\n",
      "Epoch: 9084 mean train loss:  3.30628417e-02, bound:  3.16900998e-01\n",
      "Epoch: 9085 mean train loss:  3.30500901e-02, bound:  3.16900194e-01\n",
      "Epoch: 9086 mean train loss:  3.30377743e-02, bound:  3.16899329e-01\n",
      "Epoch: 9087 mean train loss:  3.30258012e-02, bound:  3.16898614e-01\n",
      "Epoch: 9088 mean train loss:  3.30138169e-02, bound:  3.16897660e-01\n",
      "Epoch: 9089 mean train loss:  3.30030173e-02, bound:  3.16897005e-01\n",
      "Epoch: 9090 mean train loss:  3.29933949e-02, bound:  3.16896021e-01\n",
      "Epoch: 9091 mean train loss:  3.29858176e-02, bound:  3.16895396e-01\n",
      "Epoch: 9092 mean train loss:  3.29813808e-02, bound:  3.16894323e-01\n",
      "Epoch: 9093 mean train loss:  3.29821706e-02, bound:  3.16893816e-01\n",
      "Epoch: 9094 mean train loss:  3.29920352e-02, bound:  3.16892624e-01\n",
      "Epoch: 9095 mean train loss:  3.30157615e-02, bound:  3.16892266e-01\n",
      "Epoch: 9096 mean train loss:  3.30629945e-02, bound:  3.16890776e-01\n",
      "Epoch: 9097 mean train loss:  3.31482999e-02, bound:  3.16890776e-01\n",
      "Epoch: 9098 mean train loss:  3.32940556e-02, bound:  3.16888928e-01\n",
      "Epoch: 9099 mean train loss:  3.35338712e-02, bound:  3.16889435e-01\n",
      "Epoch: 9100 mean train loss:  3.39054726e-02, bound:  3.16886932e-01\n",
      "Epoch: 9101 mean train loss:  3.44358347e-02, bound:  3.16888154e-01\n",
      "Epoch: 9102 mean train loss:  3.50786895e-02, bound:  3.16884816e-01\n",
      "Epoch: 9103 mean train loss:  3.56551223e-02, bound:  3.16886783e-01\n",
      "Epoch: 9104 mean train loss:  3.57876755e-02, bound:  3.16882879e-01\n",
      "Epoch: 9105 mean train loss:  3.51773538e-02, bound:  3.16884935e-01\n",
      "Epoch: 9106 mean train loss:  3.39880623e-02, bound:  3.16881686e-01\n",
      "Epoch: 9107 mean train loss:  3.29846554e-02, bound:  3.16882282e-01\n",
      "Epoch: 9108 mean train loss:  3.28150839e-02, bound:  3.16881299e-01\n",
      "Epoch: 9109 mean train loss:  3.33682336e-02, bound:  3.16879660e-01\n",
      "Epoch: 9110 mean train loss:  3.39314900e-02, bound:  3.16880673e-01\n",
      "Epoch: 9111 mean train loss:  3.38895842e-02, bound:  3.16878080e-01\n",
      "Epoch: 9112 mean train loss:  3.32976021e-02, bound:  3.16879094e-01\n",
      "Epoch: 9113 mean train loss:  3.27702910e-02, bound:  3.16877574e-01\n",
      "Epoch: 9114 mean train loss:  3.27720903e-02, bound:  3.16876858e-01\n",
      "Epoch: 9115 mean train loss:  3.31392549e-02, bound:  3.16877067e-01\n",
      "Epoch: 9116 mean train loss:  3.33657451e-02, bound:  3.16875041e-01\n",
      "Epoch: 9117 mean train loss:  3.31870317e-02, bound:  3.16875756e-01\n",
      "Epoch: 9118 mean train loss:  3.28128599e-02, bound:  3.16874027e-01\n",
      "Epoch: 9119 mean train loss:  3.26276012e-02, bound:  3.16873640e-01\n",
      "Epoch: 9120 mean train loss:  3.27471867e-02, bound:  3.16873223e-01\n",
      "Epoch: 9121 mean train loss:  3.29523534e-02, bound:  3.16871583e-01\n",
      "Epoch: 9122 mean train loss:  3.29821482e-02, bound:  3.16871911e-01\n",
      "Epoch: 9123 mean train loss:  3.28046009e-02, bound:  3.16870093e-01\n",
      "Epoch: 9124 mean train loss:  3.26061398e-02, bound:  3.16869885e-01\n",
      "Epoch: 9125 mean train loss:  3.25601622e-02, bound:  3.16868991e-01\n",
      "Epoch: 9126 mean train loss:  3.26551832e-02, bound:  3.16867828e-01\n",
      "Epoch: 9127 mean train loss:  3.27477790e-02, bound:  3.16867769e-01\n",
      "Epoch: 9128 mean train loss:  3.27253044e-02, bound:  3.16866189e-01\n",
      "Epoch: 9129 mean train loss:  3.26094404e-02, bound:  3.16866100e-01\n",
      "Epoch: 9130 mean train loss:  3.25045772e-02, bound:  3.16864967e-01\n",
      "Epoch: 9131 mean train loss:  3.24868895e-02, bound:  3.16864282e-01\n",
      "Epoch: 9132 mean train loss:  3.25369723e-02, bound:  3.16863894e-01\n",
      "Epoch: 9133 mean train loss:  3.25785503e-02, bound:  3.16862673e-01\n",
      "Epoch: 9134 mean train loss:  3.25590633e-02, bound:  3.16862613e-01\n",
      "Epoch: 9135 mean train loss:  3.24909464e-02, bound:  3.16861391e-01\n",
      "Epoch: 9136 mean train loss:  3.24275196e-02, bound:  3.16861004e-01\n",
      "Epoch: 9137 mean train loss:  3.24077718e-02, bound:  3.16860259e-01\n",
      "Epoch: 9138 mean train loss:  3.24258097e-02, bound:  3.16859365e-01\n",
      "Epoch: 9139 mean train loss:  3.24446298e-02, bound:  3.16859037e-01\n",
      "Epoch: 9140 mean train loss:  3.24343853e-02, bound:  3.16857904e-01\n",
      "Epoch: 9141 mean train loss:  3.23958471e-02, bound:  3.16857606e-01\n",
      "Epoch: 9142 mean train loss:  3.23541202e-02, bound:  3.16856593e-01\n",
      "Epoch: 9143 mean train loss:  3.23318355e-02, bound:  3.16855967e-01\n",
      "Epoch: 9144 mean train loss:  3.23314369e-02, bound:  3.16855341e-01\n",
      "Epoch: 9145 mean train loss:  3.23371515e-02, bound:  3.16854417e-01\n",
      "Epoch: 9146 mean train loss:  3.23319547e-02, bound:  3.16854000e-01\n",
      "Epoch: 9147 mean train loss:  3.23100239e-02, bound:  3.16852957e-01\n",
      "Epoch: 9148 mean train loss:  3.22816670e-02, bound:  3.16852480e-01\n",
      "Epoch: 9149 mean train loss:  3.22598889e-02, bound:  3.16851646e-01\n",
      "Epoch: 9150 mean train loss:  3.22493911e-02, bound:  3.16850901e-01\n",
      "Epoch: 9151 mean train loss:  3.22464630e-02, bound:  3.16850364e-01\n",
      "Epoch: 9152 mean train loss:  3.22410949e-02, bound:  3.16849411e-01\n",
      "Epoch: 9153 mean train loss:  3.22281122e-02, bound:  3.16848934e-01\n",
      "Epoch: 9154 mean train loss:  3.22088450e-02, bound:  3.16848040e-01\n",
      "Epoch: 9155 mean train loss:  3.21893618e-02, bound:  3.16847473e-01\n",
      "Epoch: 9156 mean train loss:  3.21741700e-02, bound:  3.16846728e-01\n",
      "Epoch: 9157 mean train loss:  3.21650021e-02, bound:  3.16845983e-01\n",
      "Epoch: 9158 mean train loss:  3.21570709e-02, bound:  3.16845357e-01\n",
      "Epoch: 9159 mean train loss:  3.21482159e-02, bound:  3.16844493e-01\n",
      "Epoch: 9160 mean train loss:  3.21351886e-02, bound:  3.16843897e-01\n",
      "Epoch: 9161 mean train loss:  3.21189463e-02, bound:  3.16843033e-01\n",
      "Epoch: 9162 mean train loss:  3.21033858e-02, bound:  3.16842407e-01\n",
      "Epoch: 9163 mean train loss:  3.20896879e-02, bound:  3.16841632e-01\n",
      "Epoch: 9164 mean train loss:  3.20788398e-02, bound:  3.16840827e-01\n",
      "Epoch: 9165 mean train loss:  3.20686735e-02, bound:  3.16840172e-01\n",
      "Epoch: 9166 mean train loss:  3.20582502e-02, bound:  3.16839337e-01\n",
      "Epoch: 9167 mean train loss:  3.20461839e-02, bound:  3.16838712e-01\n",
      "Epoch: 9168 mean train loss:  3.20324562e-02, bound:  3.16837847e-01\n",
      "Epoch: 9169 mean train loss:  3.20185944e-02, bound:  3.16837192e-01\n",
      "Epoch: 9170 mean train loss:  3.20051536e-02, bound:  3.16836417e-01\n",
      "Epoch: 9171 mean train loss:  3.19928043e-02, bound:  3.16835672e-01\n",
      "Epoch: 9172 mean train loss:  3.19815949e-02, bound:  3.16834956e-01\n",
      "Epoch: 9173 mean train loss:  3.19704488e-02, bound:  3.16834122e-01\n",
      "Epoch: 9174 mean train loss:  3.19587700e-02, bound:  3.16833466e-01\n",
      "Epoch: 9175 mean train loss:  3.19465995e-02, bound:  3.16832662e-01\n",
      "Epoch: 9176 mean train loss:  3.19335163e-02, bound:  3.16831976e-01\n",
      "Epoch: 9177 mean train loss:  3.19204293e-02, bound:  3.16831201e-01\n",
      "Epoch: 9178 mean train loss:  3.19079570e-02, bound:  3.16830486e-01\n",
      "Epoch: 9179 mean train loss:  3.18957269e-02, bound:  3.16829741e-01\n",
      "Epoch: 9180 mean train loss:  3.18839848e-02, bound:  3.16829026e-01\n",
      "Epoch: 9181 mean train loss:  3.18726897e-02, bound:  3.16828310e-01\n",
      "Epoch: 9182 mean train loss:  3.18605974e-02, bound:  3.16827536e-01\n",
      "Epoch: 9183 mean train loss:  3.18483338e-02, bound:  3.16826850e-01\n",
      "Epoch: 9184 mean train loss:  3.18361819e-02, bound:  3.16826046e-01\n",
      "Epoch: 9185 mean train loss:  3.18237022e-02, bound:  3.16825360e-01\n",
      "Epoch: 9186 mean train loss:  3.18111964e-02, bound:  3.16824615e-01\n",
      "Epoch: 9187 mean train loss:  3.17988954e-02, bound:  3.16823900e-01\n",
      "Epoch: 9188 mean train loss:  3.17869522e-02, bound:  3.16823125e-01\n",
      "Epoch: 9189 mean train loss:  3.17750312e-02, bound:  3.16822380e-01\n",
      "Epoch: 9190 mean train loss:  3.17631103e-02, bound:  3.16821665e-01\n",
      "Epoch: 9191 mean train loss:  3.17509994e-02, bound:  3.16820920e-01\n",
      "Epoch: 9192 mean train loss:  3.17391977e-02, bound:  3.16820234e-01\n",
      "Epoch: 9193 mean train loss:  3.17267329e-02, bound:  3.16819429e-01\n",
      "Epoch: 9194 mean train loss:  3.17145325e-02, bound:  3.16818684e-01\n",
      "Epoch: 9195 mean train loss:  3.17022838e-02, bound:  3.16817939e-01\n",
      "Epoch: 9196 mean train loss:  3.16902213e-02, bound:  3.16817194e-01\n",
      "Epoch: 9197 mean train loss:  3.16778794e-02, bound:  3.16816509e-01\n",
      "Epoch: 9198 mean train loss:  3.16660181e-02, bound:  3.16815764e-01\n",
      "Epoch: 9199 mean train loss:  3.16538140e-02, bound:  3.16815019e-01\n",
      "Epoch: 9200 mean train loss:  3.16420160e-02, bound:  3.16814274e-01\n",
      "Epoch: 9201 mean train loss:  3.16300504e-02, bound:  3.16813529e-01\n",
      "Epoch: 9202 mean train loss:  3.16178165e-02, bound:  3.16812724e-01\n",
      "Epoch: 9203 mean train loss:  3.16057019e-02, bound:  3.16812068e-01\n",
      "Epoch: 9204 mean train loss:  3.15933488e-02, bound:  3.16811293e-01\n",
      "Epoch: 9205 mean train loss:  3.15812901e-02, bound:  3.16810548e-01\n",
      "Epoch: 9206 mean train loss:  3.15691605e-02, bound:  3.16809803e-01\n",
      "Epoch: 9207 mean train loss:  3.15571725e-02, bound:  3.16809058e-01\n",
      "Epoch: 9208 mean train loss:  3.15451100e-02, bound:  3.16808313e-01\n",
      "Epoch: 9209 mean train loss:  3.15329805e-02, bound:  3.16807568e-01\n",
      "Epoch: 9210 mean train loss:  3.15208994e-02, bound:  3.16806853e-01\n",
      "Epoch: 9211 mean train loss:  3.15086693e-02, bound:  3.16806108e-01\n",
      "Epoch: 9212 mean train loss:  3.14966701e-02, bound:  3.16805363e-01\n",
      "Epoch: 9213 mean train loss:  3.14846709e-02, bound:  3.16804618e-01\n",
      "Epoch: 9214 mean train loss:  3.14726233e-02, bound:  3.16803873e-01\n",
      "Epoch: 9215 mean train loss:  3.14603932e-02, bound:  3.16803128e-01\n",
      "Epoch: 9216 mean train loss:  3.14483754e-02, bound:  3.16802382e-01\n",
      "Epoch: 9217 mean train loss:  3.14364396e-02, bound:  3.16801637e-01\n",
      "Epoch: 9218 mean train loss:  3.14241797e-02, bound:  3.16800922e-01\n",
      "Epoch: 9219 mean train loss:  3.14121135e-02, bound:  3.16800207e-01\n",
      "Epoch: 9220 mean train loss:  3.14001665e-02, bound:  3.16799462e-01\n",
      "Epoch: 9221 mean train loss:  3.13878432e-02, bound:  3.16798717e-01\n",
      "Epoch: 9222 mean train loss:  3.13760154e-02, bound:  3.16798002e-01\n",
      "Epoch: 9223 mean train loss:  3.13637815e-02, bound:  3.16797227e-01\n",
      "Epoch: 9224 mean train loss:  3.13515961e-02, bound:  3.16796482e-01\n",
      "Epoch: 9225 mean train loss:  3.13397534e-02, bound:  3.16795737e-01\n",
      "Epoch: 9226 mean train loss:  3.13274376e-02, bound:  3.16795021e-01\n",
      "Epoch: 9227 mean train loss:  3.13153900e-02, bound:  3.16794246e-01\n",
      "Epoch: 9228 mean train loss:  3.13036293e-02, bound:  3.16793501e-01\n",
      "Epoch: 9229 mean train loss:  3.12910415e-02, bound:  3.16792756e-01\n",
      "Epoch: 9230 mean train loss:  3.12791504e-02, bound:  3.16792041e-01\n",
      "Epoch: 9231 mean train loss:  3.12671959e-02, bound:  3.16791296e-01\n",
      "Epoch: 9232 mean train loss:  3.12548280e-02, bound:  3.16790551e-01\n",
      "Epoch: 9233 mean train loss:  3.12433969e-02, bound:  3.16789806e-01\n",
      "Epoch: 9234 mean train loss:  3.12308688e-02, bound:  3.16789061e-01\n",
      "Epoch: 9235 mean train loss:  3.12193148e-02, bound:  3.16788316e-01\n",
      "Epoch: 9236 mean train loss:  3.12069394e-02, bound:  3.16787630e-01\n",
      "Epoch: 9237 mean train loss:  3.11950706e-02, bound:  3.16786826e-01\n",
      "Epoch: 9238 mean train loss:  3.11830956e-02, bound:  3.16786140e-01\n",
      "Epoch: 9239 mean train loss:  3.11710481e-02, bound:  3.16785395e-01\n",
      "Epoch: 9240 mean train loss:  3.11592594e-02, bound:  3.16784680e-01\n",
      "Epoch: 9241 mean train loss:  3.11474893e-02, bound:  3.16783845e-01\n",
      "Epoch: 9242 mean train loss:  3.11355684e-02, bound:  3.16783190e-01\n",
      "Epoch: 9243 mean train loss:  3.11242361e-02, bound:  3.16782355e-01\n",
      "Epoch: 9244 mean train loss:  3.11131850e-02, bound:  3.16781700e-01\n",
      "Epoch: 9245 mean train loss:  3.11026517e-02, bound:  3.16780865e-01\n",
      "Epoch: 9246 mean train loss:  3.10925022e-02, bound:  3.16780299e-01\n",
      "Epoch: 9247 mean train loss:  3.10837403e-02, bound:  3.16779375e-01\n",
      "Epoch: 9248 mean train loss:  3.10765952e-02, bound:  3.16778839e-01\n",
      "Epoch: 9249 mean train loss:  3.10724284e-02, bound:  3.16777855e-01\n",
      "Epoch: 9250 mean train loss:  3.10729779e-02, bound:  3.16777408e-01\n",
      "Epoch: 9251 mean train loss:  3.10811289e-02, bound:  3.16776305e-01\n",
      "Epoch: 9252 mean train loss:  3.11012864e-02, bound:  3.16776037e-01\n",
      "Epoch: 9253 mean train loss:  3.11404988e-02, bound:  3.16774696e-01\n",
      "Epoch: 9254 mean train loss:  3.12108863e-02, bound:  3.16774696e-01\n",
      "Epoch: 9255 mean train loss:  3.13296989e-02, bound:  3.16773057e-01\n",
      "Epoch: 9256 mean train loss:  3.15243453e-02, bound:  3.16773444e-01\n",
      "Epoch: 9257 mean train loss:  3.18260416e-02, bound:  3.16771269e-01\n",
      "Epoch: 9258 mean train loss:  3.22658531e-02, bound:  3.16772282e-01\n",
      "Epoch: 9259 mean train loss:  3.28287184e-02, bound:  3.16769332e-01\n",
      "Epoch: 9260 mean train loss:  3.34158316e-02, bound:  3.16771090e-01\n",
      "Epoch: 9261 mean train loss:  3.37506905e-02, bound:  3.16767514e-01\n",
      "Epoch: 9262 mean train loss:  3.35163809e-02, bound:  3.16769540e-01\n",
      "Epoch: 9263 mean train loss:  3.26141566e-02, bound:  3.16766262e-01\n",
      "Epoch: 9264 mean train loss:  3.15042473e-02, bound:  3.16767305e-01\n",
      "Epoch: 9265 mean train loss:  3.08928695e-02, bound:  3.16765726e-01\n",
      "Epoch: 9266 mean train loss:  3.10737900e-02, bound:  3.16764832e-01\n",
      "Epoch: 9267 mean train loss:  3.16659994e-02, bound:  3.16765368e-01\n",
      "Epoch: 9268 mean train loss:  3.20090167e-02, bound:  3.16763043e-01\n",
      "Epoch: 9269 mean train loss:  3.17618474e-02, bound:  3.16764235e-01\n",
      "Epoch: 9270 mean train loss:  3.11771873e-02, bound:  3.16762298e-01\n",
      "Epoch: 9271 mean train loss:  3.08169369e-02, bound:  3.16762328e-01\n",
      "Epoch: 9272 mean train loss:  3.09335385e-02, bound:  3.16761971e-01\n",
      "Epoch: 9273 mean train loss:  3.12695652e-02, bound:  3.16760480e-01\n",
      "Epoch: 9274 mean train loss:  3.14057618e-02, bound:  3.16761136e-01\n",
      "Epoch: 9275 mean train loss:  3.11967395e-02, bound:  3.16759229e-01\n",
      "Epoch: 9276 mean train loss:  3.08708809e-02, bound:  3.16759497e-01\n",
      "Epoch: 9277 mean train loss:  3.07343528e-02, bound:  3.16758513e-01\n",
      "Epoch: 9278 mean train loss:  3.08499578e-02, bound:  3.16757470e-01\n",
      "Epoch: 9279 mean train loss:  3.10251806e-02, bound:  3.16757560e-01\n",
      "Epoch: 9280 mean train loss:  3.10506206e-02, bound:  3.16755861e-01\n",
      "Epoch: 9281 mean train loss:  3.09012141e-02, bound:  3.16756010e-01\n",
      "Epoch: 9282 mean train loss:  3.07244565e-02, bound:  3.16754699e-01\n",
      "Epoch: 9283 mean train loss:  3.06656118e-02, bound:  3.16754133e-01\n",
      "Epoch: 9284 mean train loss:  3.07330694e-02, bound:  3.16753745e-01\n",
      "Epoch: 9285 mean train loss:  3.08212824e-02, bound:  3.16752404e-01\n",
      "Epoch: 9286 mean train loss:  3.08278184e-02, bound:  3.16752464e-01\n",
      "Epoch: 9287 mean train loss:  3.07434872e-02, bound:  3.16751093e-01\n",
      "Epoch: 9288 mean train loss:  3.06410398e-02, bound:  3.16750944e-01\n",
      "Epoch: 9289 mean train loss:  3.05954218e-02, bound:  3.16750109e-01\n",
      "Epoch: 9290 mean train loss:  3.06183118e-02, bound:  3.16749364e-01\n",
      "Epoch: 9291 mean train loss:  3.06622237e-02, bound:  3.16749126e-01\n",
      "Epoch: 9292 mean train loss:  3.06724906e-02, bound:  3.16747993e-01\n",
      "Epoch: 9293 mean train loss:  3.06329262e-02, bound:  3.16747874e-01\n",
      "Epoch: 9294 mean train loss:  3.05717103e-02, bound:  3.16746831e-01\n",
      "Epoch: 9295 mean train loss:  3.05296853e-02, bound:  3.16746414e-01\n",
      "Epoch: 9296 mean train loss:  3.05241682e-02, bound:  3.16745847e-01\n",
      "Epoch: 9297 mean train loss:  3.05404235e-02, bound:  3.16744983e-01\n",
      "Epoch: 9298 mean train loss:  3.05502750e-02, bound:  3.16744685e-01\n",
      "Epoch: 9299 mean train loss:  3.05358693e-02, bound:  3.16743642e-01\n",
      "Epoch: 9300 mean train loss:  3.05024087e-02, bound:  3.16743374e-01\n",
      "Epoch: 9301 mean train loss:  3.04681435e-02, bound:  3.16742480e-01\n",
      "Epoch: 9302 mean train loss:  3.04490719e-02, bound:  3.16741884e-01\n",
      "Epoch: 9303 mean train loss:  3.04464940e-02, bound:  3.16741288e-01\n",
      "Epoch: 9304 mean train loss:  3.04492954e-02, bound:  3.16740423e-01\n",
      "Epoch: 9305 mean train loss:  3.04450281e-02, bound:  3.16740066e-01\n",
      "Epoch: 9306 mean train loss:  3.04285884e-02, bound:  3.16739142e-01\n",
      "Epoch: 9307 mean train loss:  3.04056816e-02, bound:  3.16738725e-01\n",
      "Epoch: 9308 mean train loss:  3.03841885e-02, bound:  3.16737950e-01\n",
      "Epoch: 9309 mean train loss:  3.03711127e-02, bound:  3.16737324e-01\n",
      "Epoch: 9310 mean train loss:  3.03646103e-02, bound:  3.16736758e-01\n",
      "Epoch: 9311 mean train loss:  3.03606596e-02, bound:  3.16735953e-01\n",
      "Epoch: 9312 mean train loss:  3.03522367e-02, bound:  3.16735536e-01\n",
      "Epoch: 9313 mean train loss:  3.03384941e-02, bound:  3.16734672e-01\n",
      "Epoch: 9314 mean train loss:  3.03214397e-02, bound:  3.16734195e-01\n",
      "Epoch: 9315 mean train loss:  3.03051062e-02, bound:  3.16733479e-01\n",
      "Epoch: 9316 mean train loss:  3.02925389e-02, bound:  3.16732854e-01\n",
      "Epoch: 9317 mean train loss:  3.02829519e-02, bound:  3.16732287e-01\n",
      "Epoch: 9318 mean train loss:  3.02753709e-02, bound:  3.16731513e-01\n",
      "Epoch: 9319 mean train loss:  3.02666258e-02, bound:  3.16731006e-01\n",
      "Epoch: 9320 mean train loss:  3.02546062e-02, bound:  3.16730171e-01\n",
      "Epoch: 9321 mean train loss:  3.02412752e-02, bound:  3.16729695e-01\n",
      "Epoch: 9322 mean train loss:  3.02271638e-02, bound:  3.16728890e-01\n",
      "Epoch: 9323 mean train loss:  3.02141216e-02, bound:  3.16728264e-01\n",
      "Epoch: 9324 mean train loss:  3.02032810e-02, bound:  3.16727608e-01\n",
      "Epoch: 9325 mean train loss:  3.01930290e-02, bound:  3.16726863e-01\n",
      "Epoch: 9326 mean train loss:  3.01832277e-02, bound:  3.16726267e-01\n",
      "Epoch: 9327 mean train loss:  3.01729925e-02, bound:  3.16725522e-01\n",
      "Epoch: 9328 mean train loss:  3.01616024e-02, bound:  3.16724926e-01\n",
      "Epoch: 9329 mean train loss:  3.01490668e-02, bound:  3.16724181e-01\n",
      "Epoch: 9330 mean train loss:  3.01366802e-02, bound:  3.16723555e-01\n",
      "Epoch: 9331 mean train loss:  3.01247649e-02, bound:  3.16722870e-01\n",
      "Epoch: 9332 mean train loss:  3.01134195e-02, bound:  3.16722214e-01\n",
      "Epoch: 9333 mean train loss:  3.01029142e-02, bound:  3.16721588e-01\n",
      "Epoch: 9334 mean train loss:  3.00921705e-02, bound:  3.16720873e-01\n",
      "Epoch: 9335 mean train loss:  3.00814118e-02, bound:  3.16720247e-01\n",
      "Epoch: 9336 mean train loss:  3.00703347e-02, bound:  3.16719502e-01\n",
      "Epoch: 9337 mean train loss:  3.00587509e-02, bound:  3.16718936e-01\n",
      "Epoch: 9338 mean train loss:  3.00471131e-02, bound:  3.16718161e-01\n",
      "Epoch: 9339 mean train loss:  3.00355237e-02, bound:  3.16717595e-01\n",
      "Epoch: 9340 mean train loss:  3.00241355e-02, bound:  3.16716880e-01\n",
      "Epoch: 9341 mean train loss:  3.00128423e-02, bound:  3.16716224e-01\n",
      "Epoch: 9342 mean train loss:  3.00018992e-02, bound:  3.16715568e-01\n",
      "Epoch: 9343 mean train loss:  2.99908686e-02, bound:  3.16714853e-01\n",
      "Epoch: 9344 mean train loss:  2.99798138e-02, bound:  3.16714227e-01\n",
      "Epoch: 9345 mean train loss:  2.99686752e-02, bound:  3.16713542e-01\n",
      "Epoch: 9346 mean train loss:  2.99574845e-02, bound:  3.16712916e-01\n",
      "Epoch: 9347 mean train loss:  2.99462732e-02, bound:  3.16712201e-01\n",
      "Epoch: 9348 mean train loss:  2.99348291e-02, bound:  3.16711575e-01\n",
      "Epoch: 9349 mean train loss:  2.99235005e-02, bound:  3.16710889e-01\n",
      "Epoch: 9350 mean train loss:  2.99125351e-02, bound:  3.16710234e-01\n",
      "Epoch: 9351 mean train loss:  2.99011227e-02, bound:  3.16709578e-01\n",
      "Epoch: 9352 mean train loss:  2.98900213e-02, bound:  3.16708893e-01\n",
      "Epoch: 9353 mean train loss:  2.98789572e-02, bound:  3.16708237e-01\n",
      "Epoch: 9354 mean train loss:  2.98679862e-02, bound:  3.16707551e-01\n",
      "Epoch: 9355 mean train loss:  2.98568029e-02, bound:  3.16706926e-01\n",
      "Epoch: 9356 mean train loss:  2.98459269e-02, bound:  3.16706210e-01\n",
      "Epoch: 9357 mean train loss:  2.98348311e-02, bound:  3.16705585e-01\n",
      "Epoch: 9358 mean train loss:  2.98235863e-02, bound:  3.16704899e-01\n",
      "Epoch: 9359 mean train loss:  2.98121646e-02, bound:  3.16704243e-01\n",
      "Epoch: 9360 mean train loss:  2.98010465e-02, bound:  3.16703558e-01\n",
      "Epoch: 9361 mean train loss:  2.97897365e-02, bound:  3.16702902e-01\n",
      "Epoch: 9362 mean train loss:  2.97784284e-02, bound:  3.16702217e-01\n",
      "Epoch: 9363 mean train loss:  2.97673699e-02, bound:  3.16701561e-01\n",
      "Epoch: 9364 mean train loss:  2.97562722e-02, bound:  3.16700876e-01\n",
      "Epoch: 9365 mean train loss:  2.97449306e-02, bound:  3.16700220e-01\n",
      "Epoch: 9366 mean train loss:  2.97337640e-02, bound:  3.16699564e-01\n",
      "Epoch: 9367 mean train loss:  2.97230650e-02, bound:  3.16698879e-01\n",
      "Epoch: 9368 mean train loss:  2.97116581e-02, bound:  3.16698253e-01\n",
      "Epoch: 9369 mean train loss:  2.97007430e-02, bound:  3.16697538e-01\n",
      "Epoch: 9370 mean train loss:  2.96896566e-02, bound:  3.16696882e-01\n",
      "Epoch: 9371 mean train loss:  2.96786185e-02, bound:  3.16696197e-01\n",
      "Epoch: 9372 mean train loss:  2.96672862e-02, bound:  3.16695571e-01\n",
      "Epoch: 9373 mean train loss:  2.96561457e-02, bound:  3.16694885e-01\n",
      "Epoch: 9374 mean train loss:  2.96452567e-02, bound:  3.16694200e-01\n",
      "Epoch: 9375 mean train loss:  2.96340175e-02, bound:  3.16693574e-01\n",
      "Epoch: 9376 mean train loss:  2.96226162e-02, bound:  3.16692919e-01\n",
      "Epoch: 9377 mean train loss:  2.96114441e-02, bound:  3.16692203e-01\n",
      "Epoch: 9378 mean train loss:  2.96003968e-02, bound:  3.16691577e-01\n",
      "Epoch: 9379 mean train loss:  2.95892488e-02, bound:  3.16690892e-01\n",
      "Epoch: 9380 mean train loss:  2.95782480e-02, bound:  3.16690236e-01\n",
      "Epoch: 9381 mean train loss:  2.95669623e-02, bound:  3.16689551e-01\n",
      "Epoch: 9382 mean train loss:  2.95557976e-02, bound:  3.16688895e-01\n",
      "Epoch: 9383 mean train loss:  2.95447242e-02, bound:  3.16688210e-01\n",
      "Epoch: 9384 mean train loss:  2.95338035e-02, bound:  3.16687584e-01\n",
      "Epoch: 9385 mean train loss:  2.95226015e-02, bound:  3.16686898e-01\n",
      "Epoch: 9386 mean train loss:  2.95116976e-02, bound:  3.16686213e-01\n",
      "Epoch: 9387 mean train loss:  2.95005962e-02, bound:  3.16685528e-01\n",
      "Epoch: 9388 mean train loss:  2.94894390e-02, bound:  3.16684902e-01\n",
      "Epoch: 9389 mean train loss:  2.94786375e-02, bound:  3.16684216e-01\n",
      "Epoch: 9390 mean train loss:  2.94678807e-02, bound:  3.16683590e-01\n",
      "Epoch: 9391 mean train loss:  2.94569228e-02, bound:  3.16682845e-01\n",
      "Epoch: 9392 mean train loss:  2.94461381e-02, bound:  3.16682309e-01\n",
      "Epoch: 9393 mean train loss:  2.94358190e-02, bound:  3.16681564e-01\n",
      "Epoch: 9394 mean train loss:  2.94254478e-02, bound:  3.16680968e-01\n",
      "Epoch: 9395 mean train loss:  2.94157583e-02, bound:  3.16680193e-01\n",
      "Epoch: 9396 mean train loss:  2.94065345e-02, bound:  3.16679657e-01\n",
      "Epoch: 9397 mean train loss:  2.93984730e-02, bound:  3.16678822e-01\n",
      "Epoch: 9398 mean train loss:  2.93913111e-02, bound:  3.16678375e-01\n",
      "Epoch: 9399 mean train loss:  2.93866619e-02, bound:  3.16677481e-01\n",
      "Epoch: 9400 mean train loss:  2.93851495e-02, bound:  3.16677064e-01\n",
      "Epoch: 9401 mean train loss:  2.93891989e-02, bound:  3.16676050e-01\n",
      "Epoch: 9402 mean train loss:  2.94012185e-02, bound:  3.16675812e-01\n",
      "Epoch: 9403 mean train loss:  2.94256620e-02, bound:  3.16674620e-01\n",
      "Epoch: 9404 mean train loss:  2.94698495e-02, bound:  3.16674560e-01\n",
      "Epoch: 9405 mean train loss:  2.95438170e-02, bound:  3.16673160e-01\n",
      "Epoch: 9406 mean train loss:  2.96625029e-02, bound:  3.16673428e-01\n",
      "Epoch: 9407 mean train loss:  2.98452191e-02, bound:  3.16671610e-01\n",
      "Epoch: 9408 mean train loss:  3.01171057e-02, bound:  3.16672295e-01\n",
      "Epoch: 9409 mean train loss:  3.04899942e-02, bound:  3.16669941e-01\n",
      "Epoch: 9410 mean train loss:  3.09488904e-02, bound:  3.16671282e-01\n",
      "Epoch: 9411 mean train loss:  3.13930772e-02, bound:  3.16668302e-01\n",
      "Epoch: 9412 mean train loss:  3.16376612e-02, bound:  3.16670060e-01\n",
      "Epoch: 9413 mean train loss:  3.14404741e-02, bound:  3.16666812e-01\n",
      "Epoch: 9414 mean train loss:  3.07543278e-02, bound:  3.16668391e-01\n",
      "Epoch: 9415 mean train loss:  2.98617687e-02, bound:  3.16665977e-01\n",
      "Epoch: 9416 mean train loss:  2.92652454e-02, bound:  3.16666245e-01\n",
      "Epoch: 9417 mean train loss:  2.92479321e-02, bound:  3.16665620e-01\n",
      "Epoch: 9418 mean train loss:  2.96508633e-02, bound:  3.16664249e-01\n",
      "Epoch: 9419 mean train loss:  3.00408415e-02, bound:  3.16665024e-01\n",
      "Epoch: 9420 mean train loss:  3.00598536e-02, bound:  3.16662937e-01\n",
      "Epoch: 9421 mean train loss:  2.97008455e-02, bound:  3.16663831e-01\n",
      "Epoch: 9422 mean train loss:  2.92770397e-02, bound:  3.16662341e-01\n",
      "Epoch: 9423 mean train loss:  2.91188210e-02, bound:  3.16662073e-01\n",
      "Epoch: 9424 mean train loss:  2.92718448e-02, bound:  3.16661924e-01\n",
      "Epoch: 9425 mean train loss:  2.95098312e-02, bound:  3.16660494e-01\n",
      "Epoch: 9426 mean train loss:  2.95812506e-02, bound:  3.16661030e-01\n",
      "Epoch: 9427 mean train loss:  2.94229053e-02, bound:  3.16659331e-01\n",
      "Epoch: 9428 mean train loss:  2.91825049e-02, bound:  3.16659510e-01\n",
      "Epoch: 9429 mean train loss:  2.90529598e-02, bound:  3.16658556e-01\n",
      "Epoch: 9430 mean train loss:  2.90975533e-02, bound:  3.16657752e-01\n",
      "Epoch: 9431 mean train loss:  2.92232595e-02, bound:  3.16657633e-01\n",
      "Epoch: 9432 mean train loss:  2.92916466e-02, bound:  3.16656202e-01\n",
      "Epoch: 9433 mean train loss:  2.92384587e-02, bound:  3.16656351e-01\n",
      "Epoch: 9434 mean train loss:  2.91096531e-02, bound:  3.16655040e-01\n",
      "Epoch: 9435 mean train loss:  2.90036723e-02, bound:  3.16654742e-01\n",
      "Epoch: 9436 mean train loss:  2.89816055e-02, bound:  3.16654086e-01\n",
      "Epoch: 9437 mean train loss:  2.90286988e-02, bound:  3.16653222e-01\n",
      "Epoch: 9438 mean train loss:  2.90822648e-02, bound:  3.16653103e-01\n",
      "Epoch: 9439 mean train loss:  2.90885419e-02, bound:  3.16651911e-01\n",
      "Epoch: 9440 mean train loss:  2.90394016e-02, bound:  3.16651911e-01\n",
      "Epoch: 9441 mean train loss:  2.89678909e-02, bound:  3.16650867e-01\n",
      "Epoch: 9442 mean train loss:  2.89171096e-02, bound:  3.16650540e-01\n",
      "Epoch: 9443 mean train loss:  2.89077945e-02, bound:  3.16649973e-01\n",
      "Epoch: 9444 mean train loss:  2.89271846e-02, bound:  3.16649228e-01\n",
      "Epoch: 9445 mean train loss:  2.89468113e-02, bound:  3.16649020e-01\n",
      "Epoch: 9446 mean train loss:  2.89439820e-02, bound:  3.16648036e-01\n",
      "Epoch: 9447 mean train loss:  2.89156511e-02, bound:  3.16647857e-01\n",
      "Epoch: 9448 mean train loss:  2.88762152e-02, bound:  3.16646963e-01\n",
      "Epoch: 9449 mean train loss:  2.88440827e-02, bound:  3.16646576e-01\n",
      "Epoch: 9450 mean train loss:  2.88301930e-02, bound:  3.16645950e-01\n",
      "Epoch: 9451 mean train loss:  2.88312864e-02, bound:  3.16645205e-01\n",
      "Epoch: 9452 mean train loss:  2.88361907e-02, bound:  3.16644847e-01\n",
      "Epoch: 9453 mean train loss:  2.88336687e-02, bound:  3.16643924e-01\n",
      "Epoch: 9454 mean train loss:  2.88189407e-02, bound:  3.16643625e-01\n",
      "Epoch: 9455 mean train loss:  2.87965462e-02, bound:  3.16642791e-01\n",
      "Epoch: 9456 mean train loss:  2.87732873e-02, bound:  3.16642314e-01\n",
      "Epoch: 9457 mean train loss:  2.87567023e-02, bound:  3.16641659e-01\n",
      "Epoch: 9458 mean train loss:  2.87478268e-02, bound:  3.16641062e-01\n",
      "Epoch: 9459 mean train loss:  2.87439115e-02, bound:  3.16640586e-01\n",
      "Epoch: 9460 mean train loss:  2.87396945e-02, bound:  3.16639841e-01\n",
      "Epoch: 9461 mean train loss:  2.87313797e-02, bound:  3.16639483e-01\n",
      "Epoch: 9462 mean train loss:  2.87182238e-02, bound:  3.16638649e-01\n",
      "Epoch: 9463 mean train loss:  2.87020970e-02, bound:  3.16638291e-01\n",
      "Epoch: 9464 mean train loss:  2.86866128e-02, bound:  3.16637576e-01\n",
      "Epoch: 9465 mean train loss:  2.86733489e-02, bound:  3.16637069e-01\n",
      "Epoch: 9466 mean train loss:  2.86634658e-02, bound:  3.16636473e-01\n",
      "Epoch: 9467 mean train loss:  2.86557097e-02, bound:  3.16635847e-01\n",
      "Epoch: 9468 mean train loss:  2.86478177e-02, bound:  3.16635340e-01\n",
      "Epoch: 9469 mean train loss:  2.86389757e-02, bound:  3.16634655e-01\n",
      "Epoch: 9470 mean train loss:  2.86283810e-02, bound:  3.16634208e-01\n",
      "Epoch: 9471 mean train loss:  2.86157429e-02, bound:  3.16633493e-01\n",
      "Epoch: 9472 mean train loss:  2.86031812e-02, bound:  3.16632956e-01\n",
      "Epoch: 9473 mean train loss:  2.85909530e-02, bound:  3.16632301e-01\n",
      "Epoch: 9474 mean train loss:  2.85799261e-02, bound:  3.16631705e-01\n",
      "Epoch: 9475 mean train loss:  2.85699330e-02, bound:  3.16631109e-01\n",
      "Epoch: 9476 mean train loss:  2.85606608e-02, bound:  3.16630453e-01\n",
      "Epoch: 9477 mean train loss:  2.85511240e-02, bound:  3.16629946e-01\n",
      "Epoch: 9478 mean train loss:  2.85414439e-02, bound:  3.16629201e-01\n",
      "Epoch: 9479 mean train loss:  2.85312589e-02, bound:  3.16628695e-01\n",
      "Epoch: 9480 mean train loss:  2.85198484e-02, bound:  3.16628009e-01\n",
      "Epoch: 9481 mean train loss:  2.85086911e-02, bound:  3.16627473e-01\n",
      "Epoch: 9482 mean train loss:  2.84973122e-02, bound:  3.16626817e-01\n",
      "Epoch: 9483 mean train loss:  2.84865126e-02, bound:  3.16626221e-01\n",
      "Epoch: 9484 mean train loss:  2.84759253e-02, bound:  3.16625625e-01\n",
      "Epoch: 9485 mean train loss:  2.84658559e-02, bound:  3.16625029e-01\n",
      "Epoch: 9486 mean train loss:  2.84559429e-02, bound:  3.16624463e-01\n",
      "Epoch: 9487 mean train loss:  2.84460410e-02, bound:  3.16623777e-01\n",
      "Epoch: 9488 mean train loss:  2.84358803e-02, bound:  3.16623271e-01\n",
      "Epoch: 9489 mean train loss:  2.84257550e-02, bound:  3.16622585e-01\n",
      "Epoch: 9490 mean train loss:  2.84152348e-02, bound:  3.16622019e-01\n",
      "Epoch: 9491 mean train loss:  2.84045059e-02, bound:  3.16621393e-01\n",
      "Epoch: 9492 mean train loss:  2.83940993e-02, bound:  3.16620827e-01\n",
      "Epoch: 9493 mean train loss:  2.83831544e-02, bound:  3.16620201e-01\n",
      "Epoch: 9494 mean train loss:  2.83724703e-02, bound:  3.16619575e-01\n",
      "Epoch: 9495 mean train loss:  2.83620507e-02, bound:  3.16618979e-01\n",
      "Epoch: 9496 mean train loss:  2.83515714e-02, bound:  3.16618383e-01\n",
      "Epoch: 9497 mean train loss:  2.83413790e-02, bound:  3.16617787e-01\n",
      "Epoch: 9498 mean train loss:  2.83311363e-02, bound:  3.16617161e-01\n",
      "Epoch: 9499 mean train loss:  2.83207595e-02, bound:  3.16616565e-01\n",
      "Epoch: 9500 mean train loss:  2.83107087e-02, bound:  3.16615939e-01\n",
      "Epoch: 9501 mean train loss:  2.83003915e-02, bound:  3.16615373e-01\n",
      "Epoch: 9502 mean train loss:  2.82901004e-02, bound:  3.16614717e-01\n",
      "Epoch: 9503 mean train loss:  2.82799359e-02, bound:  3.16614181e-01\n",
      "Epoch: 9504 mean train loss:  2.82693990e-02, bound:  3.16613525e-01\n",
      "Epoch: 9505 mean train loss:  2.82589551e-02, bound:  3.16612959e-01\n",
      "Epoch: 9506 mean train loss:  2.82485504e-02, bound:  3.16612333e-01\n",
      "Epoch: 9507 mean train loss:  2.82379892e-02, bound:  3.16611737e-01\n",
      "Epoch: 9508 mean train loss:  2.82275323e-02, bound:  3.16611111e-01\n",
      "Epoch: 9509 mean train loss:  2.82171294e-02, bound:  3.16610545e-01\n",
      "Epoch: 9510 mean train loss:  2.82068197e-02, bound:  3.16609919e-01\n",
      "Epoch: 9511 mean train loss:  2.81962957e-02, bound:  3.16609293e-01\n",
      "Epoch: 9512 mean train loss:  2.81861871e-02, bound:  3.16608727e-01\n",
      "Epoch: 9513 mean train loss:  2.81759594e-02, bound:  3.16608101e-01\n",
      "Epoch: 9514 mean train loss:  2.81653963e-02, bound:  3.16607505e-01\n",
      "Epoch: 9515 mean train loss:  2.81549618e-02, bound:  3.16606909e-01\n",
      "Epoch: 9516 mean train loss:  2.81444769e-02, bound:  3.16606283e-01\n",
      "Epoch: 9517 mean train loss:  2.81342790e-02, bound:  3.16605687e-01\n",
      "Epoch: 9518 mean train loss:  2.81240102e-02, bound:  3.16605061e-01\n",
      "Epoch: 9519 mean train loss:  2.81138830e-02, bound:  3.16604495e-01\n",
      "Epoch: 9520 mean train loss:  2.81034466e-02, bound:  3.16603869e-01\n",
      "Epoch: 9521 mean train loss:  2.80930512e-02, bound:  3.16603243e-01\n",
      "Epoch: 9522 mean train loss:  2.80825719e-02, bound:  3.16602677e-01\n",
      "Epoch: 9523 mean train loss:  2.80723907e-02, bound:  3.16602051e-01\n",
      "Epoch: 9524 mean train loss:  2.80619189e-02, bound:  3.16601425e-01\n",
      "Epoch: 9525 mean train loss:  2.80514583e-02, bound:  3.16600859e-01\n",
      "Epoch: 9526 mean train loss:  2.80412436e-02, bound:  3.16600233e-01\n",
      "Epoch: 9527 mean train loss:  2.80310661e-02, bound:  3.16599607e-01\n",
      "Epoch: 9528 mean train loss:  2.80207973e-02, bound:  3.16599041e-01\n",
      "Epoch: 9529 mean train loss:  2.80102342e-02, bound:  3.16598415e-01\n",
      "Epoch: 9530 mean train loss:  2.80000102e-02, bound:  3.16597849e-01\n",
      "Epoch: 9531 mean train loss:  2.79896632e-02, bound:  3.16597223e-01\n",
      "Epoch: 9532 mean train loss:  2.79793888e-02, bound:  3.16596657e-01\n",
      "Epoch: 9533 mean train loss:  2.79693250e-02, bound:  3.16595972e-01\n",
      "Epoch: 9534 mean train loss:  2.79590245e-02, bound:  3.16595405e-01\n",
      "Epoch: 9535 mean train loss:  2.79484745e-02, bound:  3.16594779e-01\n",
      "Epoch: 9536 mean train loss:  2.79384609e-02, bound:  3.16594213e-01\n",
      "Epoch: 9537 mean train loss:  2.79283952e-02, bound:  3.16593587e-01\n",
      "Epoch: 9538 mean train loss:  2.79185493e-02, bound:  3.16593021e-01\n",
      "Epoch: 9539 mean train loss:  2.79088560e-02, bound:  3.16592366e-01\n",
      "Epoch: 9540 mean train loss:  2.78990287e-02, bound:  3.16591829e-01\n",
      "Epoch: 9541 mean train loss:  2.78901123e-02, bound:  3.16591114e-01\n",
      "Epoch: 9542 mean train loss:  2.78818831e-02, bound:  3.16590637e-01\n",
      "Epoch: 9543 mean train loss:  2.78748050e-02, bound:  3.16589892e-01\n",
      "Epoch: 9544 mean train loss:  2.78694462e-02, bound:  3.16589475e-01\n",
      "Epoch: 9545 mean train loss:  2.78677493e-02, bound:  3.16588640e-01\n",
      "Epoch: 9546 mean train loss:  2.78713387e-02, bound:  3.16588312e-01\n",
      "Epoch: 9547 mean train loss:  2.78841220e-02, bound:  3.16587359e-01\n",
      "Epoch: 9548 mean train loss:  2.79120523e-02, bound:  3.16587210e-01\n",
      "Epoch: 9549 mean train loss:  2.79645789e-02, bound:  3.16586047e-01\n",
      "Epoch: 9550 mean train loss:  2.80583575e-02, bound:  3.16586167e-01\n",
      "Epoch: 9551 mean train loss:  2.82190703e-02, bound:  3.16584617e-01\n",
      "Epoch: 9552 mean train loss:  2.84851119e-02, bound:  3.16585213e-01\n",
      "Epoch: 9553 mean train loss:  2.89000571e-02, bound:  3.16583037e-01\n",
      "Epoch: 9554 mean train loss:  2.95033623e-02, bound:  3.16584319e-01\n",
      "Epoch: 9555 mean train loss:  3.02531030e-02, bound:  3.16581339e-01\n",
      "Epoch: 9556 mean train loss:  3.09579317e-02, bound:  3.16583395e-01\n",
      "Epoch: 9557 mean train loss:  3.11766695e-02, bound:  3.16579819e-01\n",
      "Epoch: 9558 mean train loss:  3.05237994e-02, bound:  3.16581964e-01\n",
      "Epoch: 9559 mean train loss:  2.91494336e-02, bound:  3.16578984e-01\n",
      "Epoch: 9560 mean train loss:  2.79574133e-02, bound:  3.16579729e-01\n",
      "Epoch: 9561 mean train loss:  2.77562998e-02, bound:  3.16578895e-01\n",
      "Epoch: 9562 mean train loss:  2.84211859e-02, bound:  3.16577464e-01\n",
      "Epoch: 9563 mean train loss:  2.90710200e-02, bound:  3.16578627e-01\n",
      "Epoch: 9564 mean train loss:  2.89641730e-02, bound:  3.16576272e-01\n",
      "Epoch: 9565 mean train loss:  2.82373186e-02, bound:  3.16577375e-01\n",
      "Epoch: 9566 mean train loss:  2.76859775e-02, bound:  3.16576153e-01\n",
      "Epoch: 9567 mean train loss:  2.78031286e-02, bound:  3.16575527e-01\n",
      "Epoch: 9568 mean train loss:  2.82623656e-02, bound:  3.16576093e-01\n",
      "Epoch: 9569 mean train loss:  2.84240060e-02, bound:  3.16574216e-01\n",
      "Epoch: 9570 mean train loss:  2.80976780e-02, bound:  3.16575080e-01\n",
      "Epoch: 9571 mean train loss:  2.76857745e-02, bound:  3.16573739e-01\n",
      "Epoch: 9572 mean train loss:  2.76223067e-02, bound:  3.16573292e-01\n",
      "Epoch: 9573 mean train loss:  2.78663896e-02, bound:  3.16573352e-01\n",
      "Epoch: 9574 mean train loss:  2.80430876e-02, bound:  3.16571772e-01\n",
      "Epoch: 9575 mean train loss:  2.79276986e-02, bound:  3.16572279e-01\n",
      "Epoch: 9576 mean train loss:  2.76651606e-02, bound:  3.16570908e-01\n",
      "Epoch: 9577 mean train loss:  2.75391396e-02, bound:  3.16570580e-01\n",
      "Epoch: 9578 mean train loss:  2.76299603e-02, bound:  3.16570193e-01\n",
      "Epoch: 9579 mean train loss:  2.77708340e-02, bound:  3.16568971e-01\n",
      "Epoch: 9580 mean train loss:  2.77755428e-02, bound:  3.16569179e-01\n",
      "Epoch: 9581 mean train loss:  2.76395846e-02, bound:  3.16567898e-01\n",
      "Epoch: 9582 mean train loss:  2.75078528e-02, bound:  3.16567749e-01\n",
      "Epoch: 9583 mean train loss:  2.74930727e-02, bound:  3.16567183e-01\n",
      "Epoch: 9584 mean train loss:  2.75683738e-02, bound:  3.16566288e-01\n",
      "Epoch: 9585 mean train loss:  2.76222862e-02, bound:  3.16566378e-01\n",
      "Epoch: 9586 mean train loss:  2.75873244e-02, bound:  3.16565305e-01\n",
      "Epoch: 9587 mean train loss:  2.74976771e-02, bound:  3.16565305e-01\n",
      "Epoch: 9588 mean train loss:  2.74344701e-02, bound:  3.16564560e-01\n",
      "Epoch: 9589 mean train loss:  2.74372734e-02, bound:  3.16564023e-01\n",
      "Epoch: 9590 mean train loss:  2.74765342e-02, bound:  3.16563815e-01\n",
      "Epoch: 9591 mean train loss:  2.74944399e-02, bound:  3.16562891e-01\n",
      "Epoch: 9592 mean train loss:  2.74649430e-02, bound:  3.16562831e-01\n",
      "Epoch: 9593 mean train loss:  2.74115950e-02, bound:  3.16561997e-01\n",
      "Epoch: 9594 mean train loss:  2.73751412e-02, bound:  3.16561639e-01\n",
      "Epoch: 9595 mean train loss:  2.73736268e-02, bound:  3.16561192e-01\n",
      "Epoch: 9596 mean train loss:  2.73902006e-02, bound:  3.16560388e-01\n",
      "Epoch: 9597 mean train loss:  2.73958538e-02, bound:  3.16560239e-01\n",
      "Epoch: 9598 mean train loss:  2.73767412e-02, bound:  3.16559315e-01\n",
      "Epoch: 9599 mean train loss:  2.73438822e-02, bound:  3.16559076e-01\n",
      "Epoch: 9600 mean train loss:  2.73182504e-02, bound:  3.16558421e-01\n",
      "Epoch: 9601 mean train loss:  2.73104198e-02, bound:  3.16557825e-01\n",
      "Epoch: 9602 mean train loss:  2.73143817e-02, bound:  3.16557437e-01\n",
      "Epoch: 9603 mean train loss:  2.73147356e-02, bound:  3.16556722e-01\n",
      "Epoch: 9604 mean train loss:  2.73029823e-02, bound:  3.16556424e-01\n",
      "Epoch: 9605 mean train loss:  2.72823181e-02, bound:  3.16555679e-01\n",
      "Epoch: 9606 mean train loss:  2.72622071e-02, bound:  3.16555321e-01\n",
      "Epoch: 9607 mean train loss:  2.72506792e-02, bound:  3.16554785e-01\n",
      "Epoch: 9608 mean train loss:  2.72470042e-02, bound:  3.16554219e-01\n",
      "Epoch: 9609 mean train loss:  2.72440817e-02, bound:  3.16553861e-01\n",
      "Epoch: 9610 mean train loss:  2.72357743e-02, bound:  3.16553205e-01\n",
      "Epoch: 9611 mean train loss:  2.72218734e-02, bound:  3.16552877e-01\n",
      "Epoch: 9612 mean train loss:  2.72061881e-02, bound:  3.16552281e-01\n",
      "Epoch: 9613 mean train loss:  2.71936227e-02, bound:  3.16551805e-01\n",
      "Epoch: 9614 mean train loss:  2.71855853e-02, bound:  3.16551358e-01\n",
      "Epoch: 9615 mean train loss:  2.71797180e-02, bound:  3.16550732e-01\n",
      "Epoch: 9616 mean train loss:  2.71725561e-02, bound:  3.16550344e-01\n",
      "Epoch: 9617 mean train loss:  2.71626692e-02, bound:  3.16549659e-01\n",
      "Epoch: 9618 mean train loss:  2.71502491e-02, bound:  3.16549271e-01\n",
      "Epoch: 9619 mean train loss:  2.71378588e-02, bound:  3.16548645e-01\n",
      "Epoch: 9620 mean train loss:  2.71272678e-02, bound:  3.16548109e-01\n",
      "Epoch: 9621 mean train loss:  2.71188654e-02, bound:  3.16547602e-01\n",
      "Epoch: 9622 mean train loss:  2.71108523e-02, bound:  3.16547006e-01\n",
      "Epoch: 9623 mean train loss:  2.71024071e-02, bound:  3.16546559e-01\n",
      "Epoch: 9624 mean train loss:  2.70926319e-02, bound:  3.16545933e-01\n",
      "Epoch: 9625 mean train loss:  2.70816777e-02, bound:  3.16545486e-01\n",
      "Epoch: 9626 mean train loss:  2.70711053e-02, bound:  3.16544861e-01\n",
      "Epoch: 9627 mean train loss:  2.70613935e-02, bound:  3.16544324e-01\n",
      "Epoch: 9628 mean train loss:  2.70518716e-02, bound:  3.16543818e-01\n",
      "Epoch: 9629 mean train loss:  2.70431284e-02, bound:  3.16543251e-01\n",
      "Epoch: 9630 mean train loss:  2.70342398e-02, bound:  3.16542774e-01\n",
      "Epoch: 9631 mean train loss:  2.70249452e-02, bound:  3.16542178e-01\n",
      "Epoch: 9632 mean train loss:  2.70149931e-02, bound:  3.16541702e-01\n",
      "Epoch: 9633 mean train loss:  2.70048138e-02, bound:  3.16541135e-01\n",
      "Epoch: 9634 mean train loss:  2.69948374e-02, bound:  3.16540599e-01\n",
      "Epoch: 9635 mean train loss:  2.69855745e-02, bound:  3.16540062e-01\n",
      "Epoch: 9636 mean train loss:  2.69762948e-02, bound:  3.16539526e-01\n",
      "Epoch: 9637 mean train loss:  2.69669648e-02, bound:  3.16538990e-01\n",
      "Epoch: 9638 mean train loss:  2.69576479e-02, bound:  3.16538453e-01\n",
      "Epoch: 9639 mean train loss:  2.69483440e-02, bound:  3.16537976e-01\n",
      "Epoch: 9640 mean train loss:  2.69387979e-02, bound:  3.16537380e-01\n",
      "Epoch: 9641 mean train loss:  2.69290097e-02, bound:  3.16536874e-01\n",
      "Epoch: 9642 mean train loss:  2.69191321e-02, bound:  3.16536278e-01\n",
      "Epoch: 9643 mean train loss:  2.69098599e-02, bound:  3.16535771e-01\n",
      "Epoch: 9644 mean train loss:  2.69006100e-02, bound:  3.16535264e-01\n",
      "Epoch: 9645 mean train loss:  2.68911775e-02, bound:  3.16534698e-01\n",
      "Epoch: 9646 mean train loss:  2.68817544e-02, bound:  3.16534132e-01\n",
      "Epoch: 9647 mean train loss:  2.68724561e-02, bound:  3.16533595e-01\n",
      "Epoch: 9648 mean train loss:  2.68628839e-02, bound:  3.16533118e-01\n",
      "Epoch: 9649 mean train loss:  2.68531870e-02, bound:  3.16532522e-01\n",
      "Epoch: 9650 mean train loss:  2.68440396e-02, bound:  3.16532046e-01\n",
      "Epoch: 9651 mean train loss:  2.68340632e-02, bound:  3.16531479e-01\n",
      "Epoch: 9652 mean train loss:  2.68248506e-02, bound:  3.16530973e-01\n",
      "Epoch: 9653 mean train loss:  2.68156994e-02, bound:  3.16530377e-01\n",
      "Epoch: 9654 mean train loss:  2.68058684e-02, bound:  3.16529840e-01\n",
      "Epoch: 9655 mean train loss:  2.67966539e-02, bound:  3.16529304e-01\n",
      "Epoch: 9656 mean train loss:  2.67872438e-02, bound:  3.16528767e-01\n",
      "Epoch: 9657 mean train loss:  2.67776214e-02, bound:  3.16528231e-01\n",
      "Epoch: 9658 mean train loss:  2.67685428e-02, bound:  3.16527665e-01\n",
      "Epoch: 9659 mean train loss:  2.67588552e-02, bound:  3.16527158e-01\n",
      "Epoch: 9660 mean train loss:  2.67494228e-02, bound:  3.16526622e-01\n",
      "Epoch: 9661 mean train loss:  2.67397854e-02, bound:  3.16526085e-01\n",
      "Epoch: 9662 mean train loss:  2.67305076e-02, bound:  3.16525519e-01\n",
      "Epoch: 9663 mean train loss:  2.67209932e-02, bound:  3.16524982e-01\n",
      "Epoch: 9664 mean train loss:  2.67117321e-02, bound:  3.16524476e-01\n",
      "Epoch: 9665 mean train loss:  2.67025158e-02, bound:  3.16523910e-01\n",
      "Epoch: 9666 mean train loss:  2.66928189e-02, bound:  3.16523373e-01\n",
      "Epoch: 9667 mean train loss:  2.66834460e-02, bound:  3.16522837e-01\n",
      "Epoch: 9668 mean train loss:  2.66741384e-02, bound:  3.16522300e-01\n",
      "Epoch: 9669 mean train loss:  2.66645737e-02, bound:  3.16521764e-01\n",
      "Epoch: 9670 mean train loss:  2.66552866e-02, bound:  3.16521227e-01\n",
      "Epoch: 9671 mean train loss:  2.66456343e-02, bound:  3.16520691e-01\n",
      "Epoch: 9672 mean train loss:  2.66364329e-02, bound:  3.16520154e-01\n",
      "Epoch: 9673 mean train loss:  2.66267844e-02, bound:  3.16519588e-01\n",
      "Epoch: 9674 mean train loss:  2.66174823e-02, bound:  3.16519111e-01\n",
      "Epoch: 9675 mean train loss:  2.66079791e-02, bound:  3.16518545e-01\n",
      "Epoch: 9676 mean train loss:  2.65986733e-02, bound:  3.16518009e-01\n",
      "Epoch: 9677 mean train loss:  2.65891645e-02, bound:  3.16517442e-01\n",
      "Epoch: 9678 mean train loss:  2.65798923e-02, bound:  3.16516906e-01\n",
      "Epoch: 9679 mean train loss:  2.65704319e-02, bound:  3.16516399e-01\n",
      "Epoch: 9680 mean train loss:  2.65612677e-02, bound:  3.16515833e-01\n",
      "Epoch: 9681 mean train loss:  2.65514366e-02, bound:  3.16515326e-01\n",
      "Epoch: 9682 mean train loss:  2.65424177e-02, bound:  3.16514760e-01\n",
      "Epoch: 9683 mean train loss:  2.65328456e-02, bound:  3.16514283e-01\n",
      "Epoch: 9684 mean train loss:  2.65236609e-02, bound:  3.16513687e-01\n",
      "Epoch: 9685 mean train loss:  2.65138429e-02, bound:  3.16513151e-01\n",
      "Epoch: 9686 mean train loss:  2.65046507e-02, bound:  3.16512614e-01\n",
      "Epoch: 9687 mean train loss:  2.64950693e-02, bound:  3.16512108e-01\n",
      "Epoch: 9688 mean train loss:  2.64858101e-02, bound:  3.16511512e-01\n",
      "Epoch: 9689 mean train loss:  2.64761858e-02, bound:  3.16511035e-01\n",
      "Epoch: 9690 mean train loss:  2.64670551e-02, bound:  3.16510469e-01\n",
      "Epoch: 9691 mean train loss:  2.64573712e-02, bound:  3.16509962e-01\n",
      "Epoch: 9692 mean train loss:  2.64479071e-02, bound:  3.16509396e-01\n",
      "Epoch: 9693 mean train loss:  2.64387149e-02, bound:  3.16508889e-01\n",
      "Epoch: 9694 mean train loss:  2.64290608e-02, bound:  3.16508353e-01\n",
      "Epoch: 9695 mean train loss:  2.64199469e-02, bound:  3.16507757e-01\n",
      "Epoch: 9696 mean train loss:  2.64102817e-02, bound:  3.16507220e-01\n",
      "Epoch: 9697 mean train loss:  2.64009386e-02, bound:  3.16506743e-01\n",
      "Epoch: 9698 mean train loss:  2.63912138e-02, bound:  3.16506207e-01\n",
      "Epoch: 9699 mean train loss:  2.63821408e-02, bound:  3.16505671e-01\n",
      "Epoch: 9700 mean train loss:  2.63726227e-02, bound:  3.16505134e-01\n",
      "Epoch: 9701 mean train loss:  2.63635051e-02, bound:  3.16504598e-01\n",
      "Epoch: 9702 mean train loss:  2.63538584e-02, bound:  3.16504061e-01\n",
      "Epoch: 9703 mean train loss:  2.63446588e-02, bound:  3.16503495e-01\n",
      "Epoch: 9704 mean train loss:  2.63350718e-02, bound:  3.16502959e-01\n",
      "Epoch: 9705 mean train loss:  2.63258778e-02, bound:  3.16502452e-01\n",
      "Epoch: 9706 mean train loss:  2.63164602e-02, bound:  3.16501915e-01\n",
      "Epoch: 9707 mean train loss:  2.63072886e-02, bound:  3.16501349e-01\n",
      "Epoch: 9708 mean train loss:  2.62977891e-02, bound:  3.16500843e-01\n",
      "Epoch: 9709 mean train loss:  2.62887608e-02, bound:  3.16500276e-01\n",
      "Epoch: 9710 mean train loss:  2.62796339e-02, bound:  3.16499770e-01\n",
      "Epoch: 9711 mean train loss:  2.62704547e-02, bound:  3.16499203e-01\n",
      "Epoch: 9712 mean train loss:  2.62614246e-02, bound:  3.16498727e-01\n",
      "Epoch: 9713 mean train loss:  2.62522381e-02, bound:  3.16498131e-01\n",
      "Epoch: 9714 mean train loss:  2.62437668e-02, bound:  3.16497624e-01\n",
      "Epoch: 9715 mean train loss:  2.62351222e-02, bound:  3.16496998e-01\n",
      "Epoch: 9716 mean train loss:  2.62274854e-02, bound:  3.16496611e-01\n",
      "Epoch: 9717 mean train loss:  2.62199454e-02, bound:  3.16495925e-01\n",
      "Epoch: 9718 mean train loss:  2.62135770e-02, bound:  3.16495568e-01\n",
      "Epoch: 9719 mean train loss:  2.62089595e-02, bound:  3.16494823e-01\n",
      "Epoch: 9720 mean train loss:  2.62064580e-02, bound:  3.16494495e-01\n",
      "Epoch: 9721 mean train loss:  2.62078531e-02, bound:  3.16493750e-01\n",
      "Epoch: 9722 mean train loss:  2.62151416e-02, bound:  3.16493511e-01\n",
      "Epoch: 9723 mean train loss:  2.62318011e-02, bound:  3.16492558e-01\n",
      "Epoch: 9724 mean train loss:  2.62624398e-02, bound:  3.16492528e-01\n",
      "Epoch: 9725 mean train loss:  2.63162050e-02, bound:  3.16491395e-01\n",
      "Epoch: 9726 mean train loss:  2.64060292e-02, bound:  3.16491574e-01\n",
      "Epoch: 9727 mean train loss:  2.65504643e-02, bound:  3.16490144e-01\n",
      "Epoch: 9728 mean train loss:  2.67751124e-02, bound:  3.16490710e-01\n",
      "Epoch: 9729 mean train loss:  2.71037277e-02, bound:  3.16488773e-01\n",
      "Epoch: 9730 mean train loss:  2.75519975e-02, bound:  3.16489935e-01\n",
      "Epoch: 9731 mean train loss:  2.80792918e-02, bound:  3.16487342e-01\n",
      "Epoch: 9732 mean train loss:  2.85603870e-02, bound:  3.16489071e-01\n",
      "Epoch: 9733 mean train loss:  2.87339129e-02, bound:  3.16485971e-01\n",
      "Epoch: 9734 mean train loss:  2.83675659e-02, bound:  3.16487819e-01\n",
      "Epoch: 9735 mean train loss:  2.74766721e-02, bound:  3.16485137e-01\n",
      "Epoch: 9736 mean train loss:  2.65168101e-02, bound:  3.16486031e-01\n",
      "Epoch: 9737 mean train loss:  2.60455254e-02, bound:  3.16484898e-01\n",
      "Epoch: 9738 mean train loss:  2.62389872e-02, bound:  3.16484123e-01\n",
      "Epoch: 9739 mean train loss:  2.67570447e-02, bound:  3.16484690e-01\n",
      "Epoch: 9740 mean train loss:  2.70658340e-02, bound:  3.16482812e-01\n",
      "Epoch: 9741 mean train loss:  2.68813837e-02, bound:  3.16483945e-01\n",
      "Epoch: 9742 mean train loss:  2.63832156e-02, bound:  3.16482306e-01\n",
      "Epoch: 9743 mean train loss:  2.60196030e-02, bound:  3.16482514e-01\n",
      "Epoch: 9744 mean train loss:  2.60483418e-02, bound:  3.16482157e-01\n",
      "Epoch: 9745 mean train loss:  2.63262354e-02, bound:  3.16481084e-01\n",
      "Epoch: 9746 mean train loss:  2.65145320e-02, bound:  3.16481680e-01\n",
      "Epoch: 9747 mean train loss:  2.64192019e-02, bound:  3.16480041e-01\n",
      "Epoch: 9748 mean train loss:  2.61455178e-02, bound:  3.16480488e-01\n",
      "Epoch: 9749 mean train loss:  2.59464961e-02, bound:  3.16479564e-01\n",
      "Epoch: 9750 mean train loss:  2.59615108e-02, bound:  3.16478968e-01\n",
      "Epoch: 9751 mean train loss:  2.61101164e-02, bound:  3.16478968e-01\n",
      "Epoch: 9752 mean train loss:  2.62100976e-02, bound:  3.16477656e-01\n",
      "Epoch: 9753 mean train loss:  2.61605009e-02, bound:  3.16477984e-01\n",
      "Epoch: 9754 mean train loss:  2.60111839e-02, bound:  3.16476732e-01\n",
      "Epoch: 9755 mean train loss:  2.58903485e-02, bound:  3.16476583e-01\n",
      "Epoch: 9756 mean train loss:  2.58774385e-02, bound:  3.16476047e-01\n",
      "Epoch: 9757 mean train loss:  2.59460118e-02, bound:  3.16475183e-01\n",
      "Epoch: 9758 mean train loss:  2.60083657e-02, bound:  3.16475272e-01\n",
      "Epoch: 9759 mean train loss:  2.60011815e-02, bound:  3.16474110e-01\n",
      "Epoch: 9760 mean train loss:  2.59293225e-02, bound:  3.16474229e-01\n",
      "Epoch: 9761 mean train loss:  2.58487538e-02, bound:  3.16473365e-01\n",
      "Epoch: 9762 mean train loss:  2.58114114e-02, bound:  3.16473037e-01\n",
      "Epoch: 9763 mean train loss:  2.58261822e-02, bound:  3.16472709e-01\n",
      "Epoch: 9764 mean train loss:  2.58610267e-02, bound:  3.16471964e-01\n",
      "Epoch: 9765 mean train loss:  2.58761477e-02, bound:  3.16471964e-01\n",
      "Epoch: 9766 mean train loss:  2.58546919e-02, bound:  3.16471070e-01\n",
      "Epoch: 9767 mean train loss:  2.58092955e-02, bound:  3.16470981e-01\n",
      "Epoch: 9768 mean train loss:  2.57681347e-02, bound:  3.16470325e-01\n",
      "Epoch: 9769 mean train loss:  2.57501379e-02, bound:  3.16469908e-01\n",
      "Epoch: 9770 mean train loss:  2.57554650e-02, bound:  3.16469550e-01\n",
      "Epoch: 9771 mean train loss:  2.57673990e-02, bound:  3.16468835e-01\n",
      "Epoch: 9772 mean train loss:  2.57694591e-02, bound:  3.16468686e-01\n",
      "Epoch: 9773 mean train loss:  2.57544145e-02, bound:  3.16467851e-01\n",
      "Epoch: 9774 mean train loss:  2.57282145e-02, bound:  3.16467643e-01\n",
      "Epoch: 9775 mean train loss:  2.57030837e-02, bound:  3.16466987e-01\n",
      "Epoch: 9776 mean train loss:  2.56882124e-02, bound:  3.16466510e-01\n",
      "Epoch: 9777 mean train loss:  2.56848298e-02, bound:  3.16466093e-01\n",
      "Epoch: 9778 mean train loss:  2.56858133e-02, bound:  3.16465467e-01\n",
      "Epoch: 9779 mean train loss:  2.56837830e-02, bound:  3.16465229e-01\n",
      "Epoch: 9780 mean train loss:  2.56745778e-02, bound:  3.16464514e-01\n",
      "Epoch: 9781 mean train loss:  2.56586000e-02, bound:  3.16464245e-01\n",
      "Epoch: 9782 mean train loss:  2.56418474e-02, bound:  3.16463649e-01\n",
      "Epoch: 9783 mean train loss:  2.56277453e-02, bound:  3.16463292e-01\n",
      "Epoch: 9784 mean train loss:  2.56188959e-02, bound:  3.16462785e-01\n",
      "Epoch: 9785 mean train loss:  2.56135147e-02, bound:  3.16462308e-01\n",
      "Epoch: 9786 mean train loss:  2.56091058e-02, bound:  3.16461980e-01\n",
      "Epoch: 9787 mean train loss:  2.56019570e-02, bound:  3.16461384e-01\n",
      "Epoch: 9788 mean train loss:  2.55920403e-02, bound:  3.16461056e-01\n",
      "Epoch: 9789 mean train loss:  2.55797189e-02, bound:  3.16460490e-01\n",
      "Epoch: 9790 mean train loss:  2.55677924e-02, bound:  3.16460162e-01\n",
      "Epoch: 9791 mean train loss:  2.55573504e-02, bound:  3.16459626e-01\n",
      "Epoch: 9792 mean train loss:  2.55486146e-02, bound:  3.16459119e-01\n",
      "Epoch: 9793 mean train loss:  2.55413037e-02, bound:  3.16458702e-01\n",
      "Epoch: 9794 mean train loss:  2.55344491e-02, bound:  3.16458166e-01\n",
      "Epoch: 9795 mean train loss:  2.55268272e-02, bound:  3.16457778e-01\n",
      "Epoch: 9796 mean train loss:  2.55181026e-02, bound:  3.16457182e-01\n",
      "Epoch: 9797 mean train loss:  2.55077872e-02, bound:  3.16456795e-01\n",
      "Epoch: 9798 mean train loss:  2.54974365e-02, bound:  3.16456228e-01\n",
      "Epoch: 9799 mean train loss:  2.54875962e-02, bound:  3.16455781e-01\n",
      "Epoch: 9800 mean train loss:  2.54783779e-02, bound:  3.16455305e-01\n",
      "Epoch: 9801 mean train loss:  2.54698861e-02, bound:  3.16454798e-01\n",
      "Epoch: 9802 mean train loss:  2.54619773e-02, bound:  3.16454351e-01\n",
      "Epoch: 9803 mean train loss:  2.54540183e-02, bound:  3.16453815e-01\n",
      "Epoch: 9804 mean train loss:  2.54453439e-02, bound:  3.16453397e-01\n",
      "Epoch: 9805 mean train loss:  2.54364815e-02, bound:  3.16452861e-01\n",
      "Epoch: 9806 mean train loss:  2.54272688e-02, bound:  3.16452414e-01\n",
      "Epoch: 9807 mean train loss:  2.54179109e-02, bound:  3.16451877e-01\n",
      "Epoch: 9808 mean train loss:  2.54088081e-02, bound:  3.16451460e-01\n",
      "Epoch: 9809 mean train loss:  2.53998227e-02, bound:  3.16450953e-01\n",
      "Epoch: 9810 mean train loss:  2.53909025e-02, bound:  3.16450447e-01\n",
      "Epoch: 9811 mean train loss:  2.53824964e-02, bound:  3.16450000e-01\n",
      "Epoch: 9812 mean train loss:  2.53740065e-02, bound:  3.16449493e-01\n",
      "Epoch: 9813 mean train loss:  2.53655445e-02, bound:  3.16449076e-01\n",
      "Epoch: 9814 mean train loss:  2.53566876e-02, bound:  3.16448510e-01\n",
      "Epoch: 9815 mean train loss:  2.53479946e-02, bound:  3.16448063e-01\n",
      "Epoch: 9816 mean train loss:  2.53392253e-02, bound:  3.16447556e-01\n",
      "Epoch: 9817 mean train loss:  2.53300983e-02, bound:  3.16447079e-01\n",
      "Epoch: 9818 mean train loss:  2.53212973e-02, bound:  3.16446573e-01\n",
      "Epoch: 9819 mean train loss:  2.53123436e-02, bound:  3.16446126e-01\n",
      "Epoch: 9820 mean train loss:  2.53034681e-02, bound:  3.16445619e-01\n",
      "Epoch: 9821 mean train loss:  2.52950061e-02, bound:  3.16445142e-01\n",
      "Epoch: 9822 mean train loss:  2.52860598e-02, bound:  3.16444635e-01\n",
      "Epoch: 9823 mean train loss:  2.52775289e-02, bound:  3.16444159e-01\n",
      "Epoch: 9824 mean train loss:  2.52691098e-02, bound:  3.16443682e-01\n",
      "Epoch: 9825 mean train loss:  2.52602436e-02, bound:  3.16443145e-01\n",
      "Epoch: 9826 mean train loss:  2.52515655e-02, bound:  3.16442728e-01\n",
      "Epoch: 9827 mean train loss:  2.52430420e-02, bound:  3.16442221e-01\n",
      "Epoch: 9828 mean train loss:  2.52340175e-02, bound:  3.16441715e-01\n",
      "Epoch: 9829 mean train loss:  2.52255090e-02, bound:  3.16441238e-01\n",
      "Epoch: 9830 mean train loss:  2.52164118e-02, bound:  3.16440761e-01\n",
      "Epoch: 9831 mean train loss:  2.52078250e-02, bound:  3.16440284e-01\n",
      "Epoch: 9832 mean train loss:  2.51990724e-02, bound:  3.16439778e-01\n",
      "Epoch: 9833 mean train loss:  2.51900796e-02, bound:  3.16439301e-01\n",
      "Epoch: 9834 mean train loss:  2.51814201e-02, bound:  3.16438824e-01\n",
      "Epoch: 9835 mean train loss:  2.51727458e-02, bound:  3.16438347e-01\n",
      "Epoch: 9836 mean train loss:  2.51640417e-02, bound:  3.16437840e-01\n",
      "Epoch: 9837 mean train loss:  2.51553059e-02, bound:  3.16437364e-01\n",
      "Epoch: 9838 mean train loss:  2.51466110e-02, bound:  3.16436857e-01\n",
      "Epoch: 9839 mean train loss:  2.51378808e-02, bound:  3.16436410e-01\n",
      "Epoch: 9840 mean train loss:  2.51293778e-02, bound:  3.16435903e-01\n",
      "Epoch: 9841 mean train loss:  2.51205880e-02, bound:  3.16435426e-01\n",
      "Epoch: 9842 mean train loss:  2.51118671e-02, bound:  3.16434920e-01\n",
      "Epoch: 9843 mean train loss:  2.51031723e-02, bound:  3.16434473e-01\n",
      "Epoch: 9844 mean train loss:  2.50943359e-02, bound:  3.16433966e-01\n",
      "Epoch: 9845 mean train loss:  2.50855777e-02, bound:  3.16433489e-01\n",
      "Epoch: 9846 mean train loss:  2.50767563e-02, bound:  3.16432983e-01\n",
      "Epoch: 9847 mean train loss:  2.50680652e-02, bound:  3.16432536e-01\n",
      "Epoch: 9848 mean train loss:  2.50593890e-02, bound:  3.16432029e-01\n",
      "Epoch: 9849 mean train loss:  2.50507947e-02, bound:  3.16431552e-01\n",
      "Epoch: 9850 mean train loss:  2.50419453e-02, bound:  3.16431046e-01\n",
      "Epoch: 9851 mean train loss:  2.50332560e-02, bound:  3.16430598e-01\n",
      "Epoch: 9852 mean train loss:  2.50246860e-02, bound:  3.16430122e-01\n",
      "Epoch: 9853 mean train loss:  2.50157528e-02, bound:  3.16429615e-01\n",
      "Epoch: 9854 mean train loss:  2.50071660e-02, bound:  3.16429138e-01\n",
      "Epoch: 9855 mean train loss:  2.49985456e-02, bound:  3.16428661e-01\n",
      "Epoch: 9856 mean train loss:  2.49897353e-02, bound:  3.16428125e-01\n",
      "Epoch: 9857 mean train loss:  2.49809995e-02, bound:  3.16427678e-01\n",
      "Epoch: 9858 mean train loss:  2.49725115e-02, bound:  3.16427201e-01\n",
      "Epoch: 9859 mean train loss:  2.49636434e-02, bound:  3.16426724e-01\n",
      "Epoch: 9860 mean train loss:  2.49551013e-02, bound:  3.16426218e-01\n",
      "Epoch: 9861 mean train loss:  2.49462649e-02, bound:  3.16425741e-01\n",
      "Epoch: 9862 mean train loss:  2.49376055e-02, bound:  3.16425264e-01\n",
      "Epoch: 9863 mean train loss:  2.49289963e-02, bound:  3.16424787e-01\n",
      "Epoch: 9864 mean train loss:  2.49201842e-02, bound:  3.16424280e-01\n",
      "Epoch: 9865 mean train loss:  2.49117613e-02, bound:  3.16423804e-01\n",
      "Epoch: 9866 mean train loss:  2.49028895e-02, bound:  3.16423327e-01\n",
      "Epoch: 9867 mean train loss:  2.48945821e-02, bound:  3.16422850e-01\n",
      "Epoch: 9868 mean train loss:  2.48858016e-02, bound:  3.16422343e-01\n",
      "Epoch: 9869 mean train loss:  2.48774569e-02, bound:  3.16421896e-01\n",
      "Epoch: 9870 mean train loss:  2.48689111e-02, bound:  3.16421360e-01\n",
      "Epoch: 9871 mean train loss:  2.48610508e-02, bound:  3.16420943e-01\n",
      "Epoch: 9872 mean train loss:  2.48528421e-02, bound:  3.16420406e-01\n",
      "Epoch: 9873 mean train loss:  2.48455759e-02, bound:  3.16419989e-01\n",
      "Epoch: 9874 mean train loss:  2.48385873e-02, bound:  3.16419363e-01\n",
      "Epoch: 9875 mean train loss:  2.48324014e-02, bound:  3.16419035e-01\n",
      "Epoch: 9876 mean train loss:  2.48281211e-02, bound:  3.16418409e-01\n",
      "Epoch: 9877 mean train loss:  2.48261709e-02, bound:  3.16418082e-01\n",
      "Epoch: 9878 mean train loss:  2.48288102e-02, bound:  3.16417396e-01\n",
      "Epoch: 9879 mean train loss:  2.48376094e-02, bound:  3.16417187e-01\n",
      "Epoch: 9880 mean train loss:  2.48578172e-02, bound:  3.16416323e-01\n",
      "Epoch: 9881 mean train loss:  2.48964317e-02, bound:  3.16416323e-01\n",
      "Epoch: 9882 mean train loss:  2.49645934e-02, bound:  3.16415220e-01\n",
      "Epoch: 9883 mean train loss:  2.50815433e-02, bound:  3.16415519e-01\n",
      "Epoch: 9884 mean train loss:  2.52739340e-02, bound:  3.16414028e-01\n",
      "Epoch: 9885 mean train loss:  2.55800206e-02, bound:  3.16414803e-01\n",
      "Epoch: 9886 mean train loss:  2.60374099e-02, bound:  3.16412747e-01\n",
      "Epoch: 9887 mean train loss:  2.66637281e-02, bound:  3.16414148e-01\n",
      "Epoch: 9888 mean train loss:  2.73759589e-02, bound:  3.16411316e-01\n",
      "Epoch: 9889 mean train loss:  2.79346164e-02, bound:  3.16413373e-01\n",
      "Epoch: 9890 mean train loss:  2.79141665e-02, bound:  3.16410065e-01\n",
      "Epoch: 9891 mean train loss:  2.70743594e-02, bound:  3.16412061e-01\n",
      "Epoch: 9892 mean train loss:  2.57457644e-02, bound:  3.16409588e-01\n",
      "Epoch: 9893 mean train loss:  2.47974303e-02, bound:  3.16410065e-01\n",
      "Epoch: 9894 mean train loss:  2.47997995e-02, bound:  3.16409647e-01\n",
      "Epoch: 9895 mean train loss:  2.54689064e-02, bound:  3.16408277e-01\n",
      "Epoch: 9896 mean train loss:  2.59925518e-02, bound:  3.16409439e-01\n",
      "Epoch: 9897 mean train loss:  2.58060899e-02, bound:  3.16407382e-01\n",
      "Epoch: 9898 mean train loss:  2.51257233e-02, bound:  3.16408396e-01\n",
      "Epoch: 9899 mean train loss:  2.46583745e-02, bound:  3.16407412e-01\n",
      "Epoch: 9900 mean train loss:  2.47873385e-02, bound:  3.16406816e-01\n",
      "Epoch: 9901 mean train loss:  2.52014007e-02, bound:  3.16407382e-01\n",
      "Epoch: 9902 mean train loss:  2.53483113e-02, bound:  3.16405803e-01\n",
      "Epoch: 9903 mean train loss:  2.50646155e-02, bound:  3.16406578e-01\n",
      "Epoch: 9904 mean train loss:  2.46879701e-02, bound:  3.16405386e-01\n",
      "Epoch: 9905 mean train loss:  2.46021189e-02, bound:  3.16405147e-01\n",
      "Epoch: 9906 mean train loss:  2.48033013e-02, bound:  3.16405147e-01\n",
      "Epoch: 9907 mean train loss:  2.49830429e-02, bound:  3.16403836e-01\n",
      "Epoch: 9908 mean train loss:  2.49195192e-02, bound:  3.16404313e-01\n",
      "Epoch: 9909 mean train loss:  2.46926658e-02, bound:  3.16403091e-01\n",
      "Epoch: 9910 mean train loss:  2.45447811e-02, bound:  3.16402912e-01\n",
      "Epoch: 9911 mean train loss:  2.45885439e-02, bound:  3.16402555e-01\n",
      "Epoch: 9912 mean train loss:  2.47178189e-02, bound:  3.16401571e-01\n",
      "Epoch: 9913 mean train loss:  2.47629751e-02, bound:  3.16401809e-01\n",
      "Epoch: 9914 mean train loss:  2.46735346e-02, bound:  3.16400617e-01\n",
      "Epoch: 9915 mean train loss:  2.45439336e-02, bound:  3.16400647e-01\n",
      "Epoch: 9916 mean train loss:  2.44896617e-02, bound:  3.16400081e-01\n",
      "Epoch: 9917 mean train loss:  2.45320350e-02, bound:  3.16399455e-01\n",
      "Epoch: 9918 mean train loss:  2.45961081e-02, bound:  3.16399485e-01\n",
      "Epoch: 9919 mean train loss:  2.46033072e-02, bound:  3.16398561e-01\n",
      "Epoch: 9920 mean train loss:  2.45432816e-02, bound:  3.16398680e-01\n",
      "Epoch: 9921 mean train loss:  2.44706795e-02, bound:  3.16397935e-01\n",
      "Epoch: 9922 mean train loss:  2.44403277e-02, bound:  3.16397637e-01\n",
      "Epoch: 9923 mean train loss:  2.44594943e-02, bound:  3.16397399e-01\n",
      "Epoch: 9924 mean train loss:  2.44896207e-02, bound:  3.16396654e-01\n",
      "Epoch: 9925 mean train loss:  2.44925879e-02, bound:  3.16396713e-01\n",
      "Epoch: 9926 mean train loss:  2.44600940e-02, bound:  3.16395879e-01\n",
      "Epoch: 9927 mean train loss:  2.44166255e-02, bound:  3.16395730e-01\n",
      "Epoch: 9928 mean train loss:  2.43916661e-02, bound:  3.16395223e-01\n",
      "Epoch: 9929 mean train loss:  2.43931953e-02, bound:  3.16394687e-01\n",
      "Epoch: 9930 mean train loss:  2.44061649e-02, bound:  3.16394448e-01\n",
      "Epoch: 9931 mean train loss:  2.44093668e-02, bound:  3.16393733e-01\n",
      "Epoch: 9932 mean train loss:  2.43935939e-02, bound:  3.16393584e-01\n",
      "Epoch: 9933 mean train loss:  2.43672542e-02, bound:  3.16392928e-01\n",
      "Epoch: 9934 mean train loss:  2.43450888e-02, bound:  3.16392601e-01\n",
      "Epoch: 9935 mean train loss:  2.43362281e-02, bound:  3.16392183e-01\n",
      "Epoch: 9936 mean train loss:  2.43371390e-02, bound:  3.16391647e-01\n",
      "Epoch: 9937 mean train loss:  2.43384074e-02, bound:  3.16391408e-01\n",
      "Epoch: 9938 mean train loss:  2.43315231e-02, bound:  3.16390783e-01\n",
      "Epoch: 9939 mean train loss:  2.43167914e-02, bound:  3.16390574e-01\n",
      "Epoch: 9940 mean train loss:  2.42993291e-02, bound:  3.16390038e-01\n",
      "Epoch: 9941 mean train loss:  2.42866799e-02, bound:  3.16389740e-01\n",
      "Epoch: 9942 mean train loss:  2.42805202e-02, bound:  3.16389352e-01\n",
      "Epoch: 9943 mean train loss:  2.42775008e-02, bound:  3.16388845e-01\n",
      "Epoch: 9944 mean train loss:  2.42728665e-02, bound:  3.16388607e-01\n",
      "Epoch: 9945 mean train loss:  2.42641382e-02, bound:  3.16388100e-01\n",
      "Epoch: 9946 mean train loss:  2.42523104e-02, bound:  3.16387832e-01\n",
      "Epoch: 9947 mean train loss:  2.42403299e-02, bound:  3.16387326e-01\n",
      "Epoch: 9948 mean train loss:  2.42303554e-02, bound:  3.16386968e-01\n",
      "Epoch: 9949 mean train loss:  2.42232177e-02, bound:  3.16386551e-01\n",
      "Epoch: 9950 mean train loss:  2.42176466e-02, bound:  3.16386044e-01\n",
      "Epoch: 9951 mean train loss:  2.42112651e-02, bound:  3.16385776e-01\n",
      "Epoch: 9952 mean train loss:  2.42032446e-02, bound:  3.16385180e-01\n",
      "Epoch: 9953 mean train loss:  2.41933577e-02, bound:  3.16384852e-01\n",
      "Epoch: 9954 mean train loss:  2.41833013e-02, bound:  3.16384375e-01\n",
      "Epoch: 9955 mean train loss:  2.41742637e-02, bound:  3.16383958e-01\n",
      "Epoch: 9956 mean train loss:  2.41665505e-02, bound:  3.16383541e-01\n",
      "Epoch: 9957 mean train loss:  2.41595302e-02, bound:  3.16383094e-01\n",
      "Epoch: 9958 mean train loss:  2.41524037e-02, bound:  3.16382706e-01\n",
      "Epoch: 9959 mean train loss:  2.41444446e-02, bound:  3.16382200e-01\n",
      "Epoch: 9960 mean train loss:  2.41357442e-02, bound:  3.16381842e-01\n",
      "Epoch: 9961 mean train loss:  2.41269469e-02, bound:  3.16381365e-01\n",
      "Epoch: 9962 mean train loss:  2.41182167e-02, bound:  3.16380948e-01\n",
      "Epoch: 9963 mean train loss:  2.41099056e-02, bound:  3.16380531e-01\n",
      "Epoch: 9964 mean train loss:  2.41021607e-02, bound:  3.16380084e-01\n",
      "Epoch: 9965 mean train loss:  2.40946598e-02, bound:  3.16379726e-01\n",
      "Epoch: 9966 mean train loss:  2.40869168e-02, bound:  3.16379219e-01\n",
      "Epoch: 9967 mean train loss:  2.40787398e-02, bound:  3.16378862e-01\n",
      "Epoch: 9968 mean train loss:  2.40704250e-02, bound:  3.16378385e-01\n",
      "Epoch: 9969 mean train loss:  2.40623578e-02, bound:  3.16377997e-01\n",
      "Epoch: 9970 mean train loss:  2.40538195e-02, bound:  3.16377491e-01\n",
      "Epoch: 9971 mean train loss:  2.40456946e-02, bound:  3.16377103e-01\n",
      "Epoch: 9972 mean train loss:  2.40377970e-02, bound:  3.16376686e-01\n",
      "Epoch: 9973 mean train loss:  2.40298323e-02, bound:  3.16376179e-01\n",
      "Epoch: 9974 mean train loss:  2.40222290e-02, bound:  3.16375822e-01\n",
      "Epoch: 9975 mean train loss:  2.40139645e-02, bound:  3.16375315e-01\n",
      "Epoch: 9976 mean train loss:  2.40059644e-02, bound:  3.16374898e-01\n",
      "Epoch: 9977 mean train loss:  2.39977706e-02, bound:  3.16374451e-01\n",
      "Epoch: 9978 mean train loss:  2.39895899e-02, bound:  3.16374034e-01\n",
      "Epoch: 9979 mean train loss:  2.39816979e-02, bound:  3.16373587e-01\n",
      "Epoch: 9980 mean train loss:  2.39734463e-02, bound:  3.16373169e-01\n",
      "Epoch: 9981 mean train loss:  2.39655953e-02, bound:  3.16372722e-01\n",
      "Epoch: 9982 mean train loss:  2.39574946e-02, bound:  3.16372305e-01\n",
      "Epoch: 9983 mean train loss:  2.39495877e-02, bound:  3.16371858e-01\n",
      "Epoch: 9984 mean train loss:  2.39416342e-02, bound:  3.16371411e-01\n",
      "Epoch: 9985 mean train loss:  2.39334963e-02, bound:  3.16370994e-01\n",
      "Epoch: 9986 mean train loss:  2.39257533e-02, bound:  3.16370547e-01\n",
      "Epoch: 9987 mean train loss:  2.39174366e-02, bound:  3.16370130e-01\n",
      "Epoch: 9988 mean train loss:  2.39095148e-02, bound:  3.16369683e-01\n",
      "Epoch: 9989 mean train loss:  2.39013322e-02, bound:  3.16369236e-01\n",
      "Epoch: 9990 mean train loss:  2.38933694e-02, bound:  3.16368818e-01\n",
      "Epoch: 9991 mean train loss:  2.38853712e-02, bound:  3.16368371e-01\n",
      "Epoch: 9992 mean train loss:  2.38773841e-02, bound:  3.16367954e-01\n",
      "Epoch: 9993 mean train loss:  2.38693040e-02, bound:  3.16367507e-01\n",
      "Epoch: 9994 mean train loss:  2.38612611e-02, bound:  3.16367060e-01\n",
      "Epoch: 9995 mean train loss:  2.38531809e-02, bound:  3.16366643e-01\n",
      "Epoch: 9996 mean train loss:  2.38454062e-02, bound:  3.16366225e-01\n",
      "Epoch: 9997 mean train loss:  2.38373466e-02, bound:  3.16365778e-01\n",
      "Epoch: 9998 mean train loss:  2.38293912e-02, bound:  3.16365361e-01\n",
      "Epoch: 9999 mean train loss:  2.38215346e-02, bound:  3.16364914e-01\n",
      "Epoch: 10000 mean train loss:  2.38132365e-02, bound:  3.16364467e-01\n",
      "Epoch: 10001 mean train loss:  2.38051768e-02, bound:  3.16364050e-01\n",
      "Epoch: 10002 mean train loss:  2.37971861e-02, bound:  3.16363603e-01\n",
      "Epoch: 10003 mean train loss:  2.37891432e-02, bound:  3.16363156e-01\n",
      "Epoch: 10004 mean train loss:  2.37812698e-02, bound:  3.16362739e-01\n",
      "Epoch: 10005 mean train loss:  2.37732641e-02, bound:  3.16362292e-01\n",
      "Epoch: 10006 mean train loss:  2.37651542e-02, bound:  3.16361874e-01\n",
      "Epoch: 10007 mean train loss:  2.37573944e-02, bound:  3.16361427e-01\n",
      "Epoch: 10008 mean train loss:  2.37491112e-02, bound:  3.16360980e-01\n",
      "Epoch: 10009 mean train loss:  2.37409491e-02, bound:  3.16360563e-01\n",
      "Epoch: 10010 mean train loss:  2.37330515e-02, bound:  3.16360116e-01\n",
      "Epoch: 10011 mean train loss:  2.37249620e-02, bound:  3.16359699e-01\n",
      "Epoch: 10012 mean train loss:  2.37172469e-02, bound:  3.16359252e-01\n",
      "Epoch: 10013 mean train loss:  2.37091184e-02, bound:  3.16358835e-01\n",
      "Epoch: 10014 mean train loss:  2.37012170e-02, bound:  3.16358387e-01\n",
      "Epoch: 10015 mean train loss:  2.36931387e-02, bound:  3.16357940e-01\n",
      "Epoch: 10016 mean train loss:  2.36851983e-02, bound:  3.16357493e-01\n",
      "Epoch: 10017 mean train loss:  2.36769896e-02, bound:  3.16357046e-01\n",
      "Epoch: 10018 mean train loss:  2.36693230e-02, bound:  3.16356629e-01\n",
      "Epoch: 10019 mean train loss:  2.36610379e-02, bound:  3.16356182e-01\n",
      "Epoch: 10020 mean train loss:  2.36531459e-02, bound:  3.16355765e-01\n",
      "Epoch: 10021 mean train loss:  2.36451048e-02, bound:  3.16355318e-01\n",
      "Epoch: 10022 mean train loss:  2.36370657e-02, bound:  3.16354901e-01\n",
      "Epoch: 10023 mean train loss:  2.36290637e-02, bound:  3.16354454e-01\n",
      "Epoch: 10024 mean train loss:  2.36209538e-02, bound:  3.16354007e-01\n",
      "Epoch: 10025 mean train loss:  2.36132313e-02, bound:  3.16353589e-01\n",
      "Epoch: 10026 mean train loss:  2.36050617e-02, bound:  3.16353172e-01\n",
      "Epoch: 10027 mean train loss:  2.35969257e-02, bound:  3.16352725e-01\n",
      "Epoch: 10028 mean train loss:  2.35892031e-02, bound:  3.16352248e-01\n",
      "Epoch: 10029 mean train loss:  2.35809106e-02, bound:  3.16351861e-01\n",
      "Epoch: 10030 mean train loss:  2.35730093e-02, bound:  3.16351414e-01\n",
      "Epoch: 10031 mean train loss:  2.35650912e-02, bound:  3.16350996e-01\n",
      "Epoch: 10032 mean train loss:  2.35569905e-02, bound:  3.16350549e-01\n",
      "Epoch: 10033 mean train loss:  2.35491358e-02, bound:  3.16350132e-01\n",
      "Epoch: 10034 mean train loss:  2.35411022e-02, bound:  3.16349685e-01\n",
      "Epoch: 10035 mean train loss:  2.35331208e-02, bound:  3.16349268e-01\n",
      "Epoch: 10036 mean train loss:  2.35252660e-02, bound:  3.16348791e-01\n",
      "Epoch: 10037 mean train loss:  2.35171784e-02, bound:  3.16348374e-01\n",
      "Epoch: 10038 mean train loss:  2.35093553e-02, bound:  3.16347927e-01\n",
      "Epoch: 10039 mean train loss:  2.35011876e-02, bound:  3.16347539e-01\n",
      "Epoch: 10040 mean train loss:  2.34934967e-02, bound:  3.16347063e-01\n",
      "Epoch: 10041 mean train loss:  2.34855227e-02, bound:  3.16346675e-01\n",
      "Epoch: 10042 mean train loss:  2.34776232e-02, bound:  3.16346198e-01\n",
      "Epoch: 10043 mean train loss:  2.34701522e-02, bound:  3.16345811e-01\n",
      "Epoch: 10044 mean train loss:  2.34624259e-02, bound:  3.16345304e-01\n",
      "Epoch: 10045 mean train loss:  2.34550238e-02, bound:  3.16344947e-01\n",
      "Epoch: 10046 mean train loss:  2.34478582e-02, bound:  3.16344470e-01\n",
      "Epoch: 10047 mean train loss:  2.34412085e-02, bound:  3.16344082e-01\n",
      "Epoch: 10048 mean train loss:  2.34351754e-02, bound:  3.16343546e-01\n",
      "Epoch: 10049 mean train loss:  2.34296173e-02, bound:  3.16343278e-01\n",
      "Epoch: 10050 mean train loss:  2.34255586e-02, bound:  3.16342652e-01\n",
      "Epoch: 10051 mean train loss:  2.34238636e-02, bound:  3.16342443e-01\n",
      "Epoch: 10052 mean train loss:  2.34245751e-02, bound:  3.16341758e-01\n",
      "Epoch: 10053 mean train loss:  2.34303232e-02, bound:  3.16341639e-01\n",
      "Epoch: 10054 mean train loss:  2.34437268e-02, bound:  3.16340804e-01\n",
      "Epoch: 10055 mean train loss:  2.34689917e-02, bound:  3.16340804e-01\n",
      "Epoch: 10056 mean train loss:  2.35126000e-02, bound:  3.16339910e-01\n",
      "Epoch: 10057 mean train loss:  2.35851668e-02, bound:  3.16340059e-01\n",
      "Epoch: 10058 mean train loss:  2.37009879e-02, bound:  3.16338837e-01\n",
      "Epoch: 10059 mean train loss:  2.38810610e-02, bound:  3.16339403e-01\n",
      "Epoch: 10060 mean train loss:  2.41494179e-02, bound:  3.16337764e-01\n",
      "Epoch: 10061 mean train loss:  2.45288145e-02, bound:  3.16338748e-01\n",
      "Epoch: 10062 mean train loss:  2.50104032e-02, bound:  3.16336513e-01\n",
      "Epoch: 10063 mean train loss:  2.55280901e-02, bound:  3.16338092e-01\n",
      "Epoch: 10064 mean train loss:  2.58919653e-02, bound:  3.16335320e-01\n",
      "Epoch: 10065 mean train loss:  2.58553494e-02, bound:  3.16337228e-01\n",
      "Epoch: 10066 mean train loss:  2.52519902e-02, bound:  3.16334516e-01\n",
      "Epoch: 10067 mean train loss:  2.42903605e-02, bound:  3.16335827e-01\n",
      "Epoch: 10068 mean train loss:  2.34895777e-02, bound:  3.16334277e-01\n",
      "Epoch: 10069 mean train loss:  2.32864469e-02, bound:  3.16334099e-01\n",
      "Epoch: 10070 mean train loss:  2.36466955e-02, bound:  3.16334277e-01\n",
      "Epoch: 10071 mean train loss:  2.41290275e-02, bound:  3.16332757e-01\n",
      "Epoch: 10072 mean train loss:  2.42703240e-02, bound:  3.16333860e-01\n",
      "Epoch: 10073 mean train loss:  2.39492506e-02, bound:  3.16332161e-01\n",
      "Epoch: 10074 mean train loss:  2.34674066e-02, bound:  3.16332817e-01\n",
      "Epoch: 10075 mean train loss:  2.32311487e-02, bound:  3.16332132e-01\n",
      "Epoch: 10076 mean train loss:  2.33640894e-02, bound:  3.16331506e-01\n",
      "Epoch: 10077 mean train loss:  2.36377530e-02, bound:  3.16331923e-01\n",
      "Epoch: 10078 mean train loss:  2.37430278e-02, bound:  3.16330582e-01\n",
      "Epoch: 10079 mean train loss:  2.35823821e-02, bound:  3.16331178e-01\n",
      "Epoch: 10080 mean train loss:  2.33183354e-02, bound:  3.16330075e-01\n",
      "Epoch: 10081 mean train loss:  2.31826156e-02, bound:  3.16329956e-01\n",
      "Epoch: 10082 mean train loss:  2.32473966e-02, bound:  3.16329718e-01\n",
      "Epoch: 10083 mean train loss:  2.33927611e-02, bound:  3.16328764e-01\n",
      "Epoch: 10084 mean train loss:  2.34542619e-02, bound:  3.16329062e-01\n",
      "Epoch: 10085 mean train loss:  2.33737100e-02, bound:  3.16327840e-01\n",
      "Epoch: 10086 mean train loss:  2.32283715e-02, bound:  3.16328019e-01\n",
      "Epoch: 10087 mean train loss:  2.31372584e-02, bound:  3.16327274e-01\n",
      "Epoch: 10088 mean train loss:  2.31501162e-02, bound:  3.16326797e-01\n",
      "Epoch: 10089 mean train loss:  2.32213438e-02, bound:  3.16326708e-01\n",
      "Epoch: 10090 mean train loss:  2.32686643e-02, bound:  3.16325754e-01\n",
      "Epoch: 10091 mean train loss:  2.32460126e-02, bound:  3.16325963e-01\n",
      "Epoch: 10092 mean train loss:  2.31730398e-02, bound:  3.16325098e-01\n",
      "Epoch: 10093 mean train loss:  2.31036972e-02, bound:  3.16325009e-01\n",
      "Epoch: 10094 mean train loss:  2.30796840e-02, bound:  3.16324621e-01\n",
      "Epoch: 10095 mean train loss:  2.31004115e-02, bound:  3.16324085e-01\n",
      "Epoch: 10096 mean train loss:  2.31324919e-02, bound:  3.16324085e-01\n",
      "Epoch: 10097 mean train loss:  2.31419466e-02, bound:  3.16323280e-01\n",
      "Epoch: 10098 mean train loss:  2.31180117e-02, bound:  3.16323340e-01\n",
      "Epoch: 10099 mean train loss:  2.30756048e-02, bound:  3.16322595e-01\n",
      "Epoch: 10100 mean train loss:  2.30398439e-02, bound:  3.16322446e-01\n",
      "Epoch: 10101 mean train loss:  2.30266806e-02, bound:  3.16322029e-01\n",
      "Epoch: 10102 mean train loss:  2.30332986e-02, bound:  3.16321522e-01\n",
      "Epoch: 10103 mean train loss:  2.30449699e-02, bound:  3.16321313e-01\n",
      "Epoch: 10104 mean train loss:  2.30461638e-02, bound:  3.16320658e-01\n",
      "Epoch: 10105 mean train loss:  2.30322126e-02, bound:  3.16320539e-01\n",
      "Epoch: 10106 mean train loss:  2.30085813e-02, bound:  3.16319883e-01\n",
      "Epoch: 10107 mean train loss:  2.29862779e-02, bound:  3.16319644e-01\n",
      "Epoch: 10108 mean train loss:  2.29733661e-02, bound:  3.16319197e-01\n",
      "Epoch: 10109 mean train loss:  2.29699109e-02, bound:  3.16318750e-01\n",
      "Epoch: 10110 mean train loss:  2.29711961e-02, bound:  3.16318482e-01\n",
      "Epoch: 10111 mean train loss:  2.29699854e-02, bound:  3.16317946e-01\n",
      "Epoch: 10112 mean train loss:  2.29627043e-02, bound:  3.16317737e-01\n",
      "Epoch: 10113 mean train loss:  2.29497645e-02, bound:  3.16317230e-01\n",
      "Epoch: 10114 mean train loss:  2.29347497e-02, bound:  3.16316962e-01\n",
      "Epoch: 10115 mean train loss:  2.29220185e-02, bound:  3.16316515e-01\n",
      "Epoch: 10116 mean train loss:  2.29131505e-02, bound:  3.16316217e-01\n",
      "Epoch: 10117 mean train loss:  2.29083057e-02, bound:  3.16315889e-01\n",
      "Epoch: 10118 mean train loss:  2.29043644e-02, bound:  3.16315442e-01\n",
      "Epoch: 10119 mean train loss:  2.28993595e-02, bound:  3.16315204e-01\n",
      "Epoch: 10120 mean train loss:  2.28920635e-02, bound:  3.16314697e-01\n",
      "Epoch: 10121 mean train loss:  2.28822716e-02, bound:  3.16314459e-01\n",
      "Epoch: 10122 mean train loss:  2.28719898e-02, bound:  3.16313982e-01\n",
      "Epoch: 10123 mean train loss:  2.28618067e-02, bound:  3.16313684e-01\n",
      "Epoch: 10124 mean train loss:  2.28537172e-02, bound:  3.16313267e-01\n",
      "Epoch: 10125 mean train loss:  2.28462908e-02, bound:  3.16312879e-01\n",
      "Epoch: 10126 mean train loss:  2.28401367e-02, bound:  3.16312522e-01\n",
      "Epoch: 10127 mean train loss:  2.28338651e-02, bound:  3.16312075e-01\n",
      "Epoch: 10128 mean train loss:  2.28267014e-02, bound:  3.16311747e-01\n",
      "Epoch: 10129 mean train loss:  2.28189435e-02, bound:  3.16311300e-01\n",
      "Epoch: 10130 mean train loss:  2.28102058e-02, bound:  3.16311002e-01\n",
      "Epoch: 10131 mean train loss:  2.28017755e-02, bound:  3.16310525e-01\n",
      "Epoch: 10132 mean train loss:  2.27937419e-02, bound:  3.16310197e-01\n",
      "Epoch: 10133 mean train loss:  2.27860976e-02, bound:  3.16309780e-01\n",
      "Epoch: 10134 mean train loss:  2.27787737e-02, bound:  3.16309363e-01\n",
      "Epoch: 10135 mean train loss:  2.27716900e-02, bound:  3.16309005e-01\n",
      "Epoch: 10136 mean train loss:  2.27646567e-02, bound:  3.16308618e-01\n",
      "Epoch: 10137 mean train loss:  2.27575786e-02, bound:  3.16308260e-01\n",
      "Epoch: 10138 mean train loss:  2.27501150e-02, bound:  3.16307813e-01\n",
      "Epoch: 10139 mean train loss:  2.27423292e-02, bound:  3.16307485e-01\n",
      "Epoch: 10140 mean train loss:  2.27343217e-02, bound:  3.16307038e-01\n",
      "Epoch: 10141 mean train loss:  2.27264930e-02, bound:  3.16306710e-01\n",
      "Epoch: 10142 mean train loss:  2.27188934e-02, bound:  3.16306293e-01\n",
      "Epoch: 10143 mean train loss:  2.27111764e-02, bound:  3.16305876e-01\n",
      "Epoch: 10144 mean train loss:  2.27037948e-02, bound:  3.16305488e-01\n",
      "Epoch: 10145 mean train loss:  2.26965435e-02, bound:  3.16305101e-01\n",
      "Epoch: 10146 mean train loss:  2.26893201e-02, bound:  3.16304743e-01\n",
      "Epoch: 10147 mean train loss:  2.26817708e-02, bound:  3.16304296e-01\n",
      "Epoch: 10148 mean train loss:  2.26743519e-02, bound:  3.16303939e-01\n",
      "Epoch: 10149 mean train loss:  2.26667318e-02, bound:  3.16303492e-01\n",
      "Epoch: 10150 mean train loss:  2.26590037e-02, bound:  3.16303164e-01\n",
      "Epoch: 10151 mean train loss:  2.26516537e-02, bound:  3.16302717e-01\n",
      "Epoch: 10152 mean train loss:  2.26439983e-02, bound:  3.16302359e-01\n",
      "Epoch: 10153 mean train loss:  2.26363707e-02, bound:  3.16301942e-01\n",
      "Epoch: 10154 mean train loss:  2.26288904e-02, bound:  3.16301554e-01\n",
      "Epoch: 10155 mean train loss:  2.26213764e-02, bound:  3.16301167e-01\n",
      "Epoch: 10156 mean train loss:  2.26139855e-02, bound:  3.16300750e-01\n",
      "Epoch: 10157 mean train loss:  2.26064529e-02, bound:  3.16300362e-01\n",
      "Epoch: 10158 mean train loss:  2.25992780e-02, bound:  3.16299975e-01\n",
      "Epoch: 10159 mean train loss:  2.25918889e-02, bound:  3.16299558e-01\n",
      "Epoch: 10160 mean train loss:  2.25841440e-02, bound:  3.16299170e-01\n",
      "Epoch: 10161 mean train loss:  2.25767139e-02, bound:  3.16298783e-01\n",
      "Epoch: 10162 mean train loss:  2.25691646e-02, bound:  3.16298366e-01\n",
      "Epoch: 10163 mean train loss:  2.25616321e-02, bound:  3.16298008e-01\n",
      "Epoch: 10164 mean train loss:  2.25542784e-02, bound:  3.16297591e-01\n",
      "Epoch: 10165 mean train loss:  2.25466713e-02, bound:  3.16297203e-01\n",
      "Epoch: 10166 mean train loss:  2.25394238e-02, bound:  3.16296816e-01\n",
      "Epoch: 10167 mean train loss:  2.25317329e-02, bound:  3.16296399e-01\n",
      "Epoch: 10168 mean train loss:  2.25242972e-02, bound:  3.16295981e-01\n",
      "Epoch: 10169 mean train loss:  2.25166995e-02, bound:  3.16295624e-01\n",
      "Epoch: 10170 mean train loss:  2.25092117e-02, bound:  3.16295207e-01\n",
      "Epoch: 10171 mean train loss:  2.25019325e-02, bound:  3.16294819e-01\n",
      "Epoch: 10172 mean train loss:  2.24941820e-02, bound:  3.16294461e-01\n",
      "Epoch: 10173 mean train loss:  2.24868692e-02, bound:  3.16294044e-01\n",
      "Epoch: 10174 mean train loss:  2.24792641e-02, bound:  3.16293627e-01\n",
      "Epoch: 10175 mean train loss:  2.24719774e-02, bound:  3.16293269e-01\n",
      "Epoch: 10176 mean train loss:  2.24643294e-02, bound:  3.16292852e-01\n",
      "Epoch: 10177 mean train loss:  2.24568751e-02, bound:  3.16292435e-01\n",
      "Epoch: 10178 mean train loss:  2.24495344e-02, bound:  3.16292048e-01\n",
      "Epoch: 10179 mean train loss:  2.24421769e-02, bound:  3.16291660e-01\n",
      "Epoch: 10180 mean train loss:  2.24344563e-02, bound:  3.16291273e-01\n",
      "Epoch: 10181 mean train loss:  2.24268902e-02, bound:  3.16290885e-01\n",
      "Epoch: 10182 mean train loss:  2.24195328e-02, bound:  3.16290468e-01\n",
      "Epoch: 10183 mean train loss:  2.24122293e-02, bound:  3.16290081e-01\n",
      "Epoch: 10184 mean train loss:  2.24045273e-02, bound:  3.16289663e-01\n",
      "Epoch: 10185 mean train loss:  2.23970693e-02, bound:  3.16289276e-01\n",
      "Epoch: 10186 mean train loss:  2.23897137e-02, bound:  3.16288888e-01\n",
      "Epoch: 10187 mean train loss:  2.23823786e-02, bound:  3.16288501e-01\n",
      "Epoch: 10188 mean train loss:  2.23747157e-02, bound:  3.16288084e-01\n",
      "Epoch: 10189 mean train loss:  2.23672669e-02, bound:  3.16287726e-01\n",
      "Epoch: 10190 mean train loss:  2.23598685e-02, bound:  3.16287309e-01\n",
      "Epoch: 10191 mean train loss:  2.23524403e-02, bound:  3.16286951e-01\n",
      "Epoch: 10192 mean train loss:  2.23449562e-02, bound:  3.16286504e-01\n",
      "Epoch: 10193 mean train loss:  2.23376714e-02, bound:  3.16286176e-01\n",
      "Epoch: 10194 mean train loss:  2.23302469e-02, bound:  3.16285729e-01\n",
      "Epoch: 10195 mean train loss:  2.23230459e-02, bound:  3.16285372e-01\n",
      "Epoch: 10196 mean train loss:  2.23158523e-02, bound:  3.16284925e-01\n",
      "Epoch: 10197 mean train loss:  2.23085377e-02, bound:  3.16284567e-01\n",
      "Epoch: 10198 mean train loss:  2.23016404e-02, bound:  3.16284150e-01\n",
      "Epoch: 10199 mean train loss:  2.22949199e-02, bound:  3.16283822e-01\n",
      "Epoch: 10200 mean train loss:  2.22884603e-02, bound:  3.16283345e-01\n",
      "Epoch: 10201 mean train loss:  2.22823825e-02, bound:  3.16283047e-01\n",
      "Epoch: 10202 mean train loss:  2.22770330e-02, bound:  3.16282511e-01\n",
      "Epoch: 10203 mean train loss:  2.22727731e-02, bound:  3.16282272e-01\n",
      "Epoch: 10204 mean train loss:  2.22700816e-02, bound:  3.16281706e-01\n",
      "Epoch: 10205 mean train loss:  2.22699270e-02, bound:  3.16281527e-01\n",
      "Epoch: 10206 mean train loss:  2.22734790e-02, bound:  3.16280872e-01\n",
      "Epoch: 10207 mean train loss:  2.22828556e-02, bound:  3.16280752e-01\n",
      "Epoch: 10208 mean train loss:  2.23010965e-02, bound:  3.16280007e-01\n",
      "Epoch: 10209 mean train loss:  2.23336313e-02, bound:  3.16280067e-01\n",
      "Epoch: 10210 mean train loss:  2.23867800e-02, bound:  3.16279143e-01\n",
      "Epoch: 10211 mean train loss:  2.24724300e-02, bound:  3.16279382e-01\n",
      "Epoch: 10212 mean train loss:  2.26059109e-02, bound:  3.16278160e-01\n",
      "Epoch: 10213 mean train loss:  2.28088163e-02, bound:  3.16278815e-01\n",
      "Epoch: 10214 mean train loss:  2.31017228e-02, bound:  3.16277146e-01\n",
      "Epoch: 10215 mean train loss:  2.34995931e-02, bound:  3.16278249e-01\n",
      "Epoch: 10216 mean train loss:  2.39746161e-02, bound:  3.16276014e-01\n",
      "Epoch: 10217 mean train loss:  2.44343635e-02, bound:  3.16277623e-01\n",
      "Epoch: 10218 mean train loss:  2.46720146e-02, bound:  3.16274971e-01\n",
      "Epoch: 10219 mean train loss:  2.44714804e-02, bound:  3.16276729e-01\n",
      "Epoch: 10220 mean train loss:  2.37644613e-02, bound:  3.16274285e-01\n",
      "Epoch: 10221 mean train loss:  2.28495803e-02, bound:  3.16275388e-01\n",
      "Epoch: 10222 mean train loss:  2.22221632e-02, bound:  3.16274136e-01\n",
      "Epoch: 10223 mean train loss:  2.21816525e-02, bound:  3.16273838e-01\n",
      "Epoch: 10224 mean train loss:  2.25845184e-02, bound:  3.16274136e-01\n",
      "Epoch: 10225 mean train loss:  2.29930039e-02, bound:  3.16272676e-01\n",
      "Epoch: 10226 mean train loss:  2.30363421e-02, bound:  3.16273719e-01\n",
      "Epoch: 10227 mean train loss:  2.26852968e-02, bound:  3.16272199e-01\n",
      "Epoch: 10228 mean train loss:  2.22540349e-02, bound:  3.16272736e-01\n",
      "Epoch: 10229 mean train loss:  2.20843758e-02, bound:  3.16272169e-01\n",
      "Epoch: 10230 mean train loss:  2.22361721e-02, bound:  3.16271544e-01\n",
      "Epoch: 10231 mean train loss:  2.24820543e-02, bound:  3.16272050e-01\n",
      "Epoch: 10232 mean train loss:  2.25589648e-02, bound:  3.16270679e-01\n",
      "Epoch: 10233 mean train loss:  2.24007349e-02, bound:  3.16271305e-01\n",
      "Epoch: 10234 mean train loss:  2.21607815e-02, bound:  3.16270232e-01\n",
      "Epoch: 10235 mean train loss:  2.20396519e-02, bound:  3.16270143e-01\n",
      "Epoch: 10236 mean train loss:  2.20967773e-02, bound:  3.16269904e-01\n",
      "Epoch: 10237 mean train loss:  2.22284254e-02, bound:  3.16269010e-01\n",
      "Epoch: 10238 mean train loss:  2.22899802e-02, bound:  3.16269308e-01\n",
      "Epoch: 10239 mean train loss:  2.22249757e-02, bound:  3.16268146e-01\n",
      "Epoch: 10240 mean train loss:  2.20942423e-02, bound:  3.16268355e-01\n",
      "Epoch: 10241 mean train loss:  2.20016614e-02, bound:  3.16267639e-01\n",
      "Epoch: 10242 mean train loss:  2.20003985e-02, bound:  3.16267252e-01\n",
      "Epoch: 10243 mean train loss:  2.20607147e-02, bound:  3.16267163e-01\n",
      "Epoch: 10244 mean train loss:  2.21118890e-02, bound:  3.16266298e-01\n",
      "Epoch: 10245 mean train loss:  2.21055523e-02, bound:  3.16266507e-01\n",
      "Epoch: 10246 mean train loss:  2.20468529e-02, bound:  3.16265672e-01\n",
      "Epoch: 10247 mean train loss:  2.19785385e-02, bound:  3.16265672e-01\n",
      "Epoch: 10248 mean train loss:  2.19427086e-02, bound:  3.16265196e-01\n",
      "Epoch: 10249 mean train loss:  2.19499227e-02, bound:  3.16264778e-01\n",
      "Epoch: 10250 mean train loss:  2.19779741e-02, bound:  3.16264689e-01\n",
      "Epoch: 10251 mean train loss:  2.19958723e-02, bound:  3.16264004e-01\n",
      "Epoch: 10252 mean train loss:  2.19862256e-02, bound:  3.16264063e-01\n",
      "Epoch: 10253 mean train loss:  2.19532140e-02, bound:  3.16263348e-01\n",
      "Epoch: 10254 mean train loss:  2.19161827e-02, bound:  3.16263258e-01\n",
      "Epoch: 10255 mean train loss:  2.18932945e-02, bound:  3.16262782e-01\n",
      "Epoch: 10256 mean train loss:  2.18902379e-02, bound:  3.16262394e-01\n",
      "Epoch: 10257 mean train loss:  2.18990073e-02, bound:  3.16262186e-01\n",
      "Epoch: 10258 mean train loss:  2.19058227e-02, bound:  3.16261590e-01\n",
      "Epoch: 10259 mean train loss:  2.19016802e-02, bound:  3.16261441e-01\n",
      "Epoch: 10260 mean train loss:  2.18856893e-02, bound:  3.16260844e-01\n",
      "Epoch: 10261 mean train loss:  2.18646880e-02, bound:  3.16260695e-01\n",
      "Epoch: 10262 mean train loss:  2.18464769e-02, bound:  3.16260189e-01\n",
      "Epoch: 10263 mean train loss:  2.18362231e-02, bound:  3.16259891e-01\n",
      "Epoch: 10264 mean train loss:  2.18333192e-02, bound:  3.16259563e-01\n",
      "Epoch: 10265 mean train loss:  2.18332876e-02, bound:  3.16259116e-01\n",
      "Epoch: 10266 mean train loss:  2.18313485e-02, bound:  3.16258937e-01\n",
      "Epoch: 10267 mean train loss:  2.18246244e-02, bound:  3.16258430e-01\n",
      "Epoch: 10268 mean train loss:  2.18130890e-02, bound:  3.16258252e-01\n",
      "Epoch: 10269 mean train loss:  2.18000915e-02, bound:  3.16257805e-01\n",
      "Epoch: 10270 mean train loss:  2.17883866e-02, bound:  3.16257566e-01\n",
      "Epoch: 10271 mean train loss:  2.17799842e-02, bound:  3.16257209e-01\n",
      "Epoch: 10272 mean train loss:  2.17743125e-02, bound:  3.16256851e-01\n",
      "Epoch: 10273 mean train loss:  2.17699166e-02, bound:  3.16256613e-01\n",
      "Epoch: 10274 mean train loss:  2.17653364e-02, bound:  3.16256166e-01\n",
      "Epoch: 10275 mean train loss:  2.17590965e-02, bound:  3.16255927e-01\n",
      "Epoch: 10276 mean train loss:  2.17508730e-02, bound:  3.16255450e-01\n",
      "Epoch: 10277 mean train loss:  2.17414517e-02, bound:  3.16255242e-01\n",
      "Epoch: 10278 mean train loss:  2.17322409e-02, bound:  3.16254795e-01\n",
      "Epoch: 10279 mean train loss:  2.17239894e-02, bound:  3.16254497e-01\n",
      "Epoch: 10280 mean train loss:  2.17169896e-02, bound:  3.16254139e-01\n",
      "Epoch: 10281 mean train loss:  2.17106026e-02, bound:  3.16253781e-01\n",
      "Epoch: 10282 mean train loss:  2.17049755e-02, bound:  3.16253453e-01\n",
      "Epoch: 10283 mean train loss:  2.16987170e-02, bound:  3.16253036e-01\n",
      "Epoch: 10284 mean train loss:  2.16917489e-02, bound:  3.16252738e-01\n",
      "Epoch: 10285 mean train loss:  2.16840915e-02, bound:  3.16252351e-01\n",
      "Epoch: 10286 mean train loss:  2.16762871e-02, bound:  3.16252053e-01\n",
      "Epoch: 10287 mean train loss:  2.16681939e-02, bound:  3.16251636e-01\n",
      "Epoch: 10288 mean train loss:  2.16605626e-02, bound:  3.16251308e-01\n",
      "Epoch: 10289 mean train loss:  2.16534771e-02, bound:  3.16250980e-01\n",
      "Epoch: 10290 mean train loss:  2.16467008e-02, bound:  3.16250592e-01\n",
      "Epoch: 10291 mean train loss:  2.16401909e-02, bound:  3.16250294e-01\n",
      "Epoch: 10292 mean train loss:  2.16335393e-02, bound:  3.16249877e-01\n",
      "Epoch: 10293 mean train loss:  2.16265153e-02, bound:  3.16249579e-01\n",
      "Epoch: 10294 mean train loss:  2.16195770e-02, bound:  3.16249192e-01\n",
      "Epoch: 10295 mean train loss:  2.16122959e-02, bound:  3.16248864e-01\n",
      "Epoch: 10296 mean train loss:  2.16049012e-02, bound:  3.16248477e-01\n",
      "Epoch: 10297 mean train loss:  2.15975028e-02, bound:  3.16248119e-01\n",
      "Epoch: 10298 mean train loss:  2.15903725e-02, bound:  3.16247791e-01\n",
      "Epoch: 10299 mean train loss:  2.15832237e-02, bound:  3.16247374e-01\n",
      "Epoch: 10300 mean train loss:  2.15762332e-02, bound:  3.16247046e-01\n",
      "Epoch: 10301 mean train loss:  2.15693116e-02, bound:  3.16246718e-01\n",
      "Epoch: 10302 mean train loss:  2.15626806e-02, bound:  3.16246361e-01\n",
      "Epoch: 10303 mean train loss:  2.15555299e-02, bound:  3.16245914e-01\n",
      "Epoch: 10304 mean train loss:  2.15486623e-02, bound:  3.16245615e-01\n",
      "Epoch: 10305 mean train loss:  2.15413645e-02, bound:  3.16245228e-01\n",
      "Epoch: 10306 mean train loss:  2.15343796e-02, bound:  3.16244870e-01\n",
      "Epoch: 10307 mean train loss:  2.15274878e-02, bound:  3.16244483e-01\n",
      "Epoch: 10308 mean train loss:  2.15201862e-02, bound:  3.16244125e-01\n",
      "Epoch: 10309 mean train loss:  2.15130094e-02, bound:  3.16243768e-01\n",
      "Epoch: 10310 mean train loss:  2.15060469e-02, bound:  3.16243410e-01\n",
      "Epoch: 10311 mean train loss:  2.14989483e-02, bound:  3.16243023e-01\n",
      "Epoch: 10312 mean train loss:  2.14918498e-02, bound:  3.16242695e-01\n",
      "Epoch: 10313 mean train loss:  2.14848928e-02, bound:  3.16242337e-01\n",
      "Epoch: 10314 mean train loss:  2.14779414e-02, bound:  3.16241980e-01\n",
      "Epoch: 10315 mean train loss:  2.14710645e-02, bound:  3.16241592e-01\n",
      "Epoch: 10316 mean train loss:  2.14640833e-02, bound:  3.16241235e-01\n",
      "Epoch: 10317 mean train loss:  2.14568116e-02, bound:  3.16240877e-01\n",
      "Epoch: 10318 mean train loss:  2.14499347e-02, bound:  3.16240489e-01\n",
      "Epoch: 10319 mean train loss:  2.14430206e-02, bound:  3.16240162e-01\n",
      "Epoch: 10320 mean train loss:  2.14358401e-02, bound:  3.16239774e-01\n",
      "Epoch: 10321 mean train loss:  2.14288626e-02, bound:  3.16239417e-01\n",
      "Epoch: 10322 mean train loss:  2.14217547e-02, bound:  3.16239059e-01\n",
      "Epoch: 10323 mean train loss:  2.14150585e-02, bound:  3.16238701e-01\n",
      "Epoch: 10324 mean train loss:  2.14078650e-02, bound:  3.16238344e-01\n",
      "Epoch: 10325 mean train loss:  2.14007050e-02, bound:  3.16237986e-01\n",
      "Epoch: 10326 mean train loss:  2.13937070e-02, bound:  3.16237599e-01\n",
      "Epoch: 10327 mean train loss:  2.13866197e-02, bound:  3.16237241e-01\n",
      "Epoch: 10328 mean train loss:  2.13794708e-02, bound:  3.16236883e-01\n",
      "Epoch: 10329 mean train loss:  2.13725679e-02, bound:  3.16236496e-01\n",
      "Epoch: 10330 mean train loss:  2.13654954e-02, bound:  3.16236168e-01\n",
      "Epoch: 10331 mean train loss:  2.13585161e-02, bound:  3.16235811e-01\n",
      "Epoch: 10332 mean train loss:  2.13517342e-02, bound:  3.16235423e-01\n",
      "Epoch: 10333 mean train loss:  2.13444307e-02, bound:  3.16235095e-01\n",
      "Epoch: 10334 mean train loss:  2.13374365e-02, bound:  3.16234708e-01\n",
      "Epoch: 10335 mean train loss:  2.13304441e-02, bound:  3.16234380e-01\n",
      "Epoch: 10336 mean train loss:  2.13233884e-02, bound:  3.16233993e-01\n",
      "Epoch: 10337 mean train loss:  2.13163998e-02, bound:  3.16233665e-01\n",
      "Epoch: 10338 mean train loss:  2.13095192e-02, bound:  3.16233277e-01\n",
      "Epoch: 10339 mean train loss:  2.13024709e-02, bound:  3.16232920e-01\n",
      "Epoch: 10340 mean train loss:  2.12954339e-02, bound:  3.16232532e-01\n",
      "Epoch: 10341 mean train loss:  2.12886240e-02, bound:  3.16232204e-01\n",
      "Epoch: 10342 mean train loss:  2.12816615e-02, bound:  3.16231817e-01\n",
      "Epoch: 10343 mean train loss:  2.12748218e-02, bound:  3.16231489e-01\n",
      "Epoch: 10344 mean train loss:  2.12682057e-02, bound:  3.16231072e-01\n",
      "Epoch: 10345 mean train loss:  2.12614108e-02, bound:  3.16230804e-01\n",
      "Epoch: 10346 mean train loss:  2.12549623e-02, bound:  3.16230386e-01\n",
      "Epoch: 10347 mean train loss:  2.12486777e-02, bound:  3.16230088e-01\n",
      "Epoch: 10348 mean train loss:  2.12424416e-02, bound:  3.16229641e-01\n",
      "Epoch: 10349 mean train loss:  2.12366506e-02, bound:  3.16229373e-01\n",
      "Epoch: 10350 mean train loss:  2.12315116e-02, bound:  3.16228867e-01\n",
      "Epoch: 10351 mean train loss:  2.12269612e-02, bound:  3.16228658e-01\n",
      "Epoch: 10352 mean train loss:  2.12239530e-02, bound:  3.16228122e-01\n",
      "Epoch: 10353 mean train loss:  2.12223791e-02, bound:  3.16227973e-01\n",
      "Epoch: 10354 mean train loss:  2.12236866e-02, bound:  3.16227376e-01\n",
      "Epoch: 10355 mean train loss:  2.12290660e-02, bound:  3.16227317e-01\n",
      "Epoch: 10356 mean train loss:  2.12400723e-02, bound:  3.16226602e-01\n",
      "Epoch: 10357 mean train loss:  2.12599840e-02, bound:  3.16226602e-01\n",
      "Epoch: 10358 mean train loss:  2.12934092e-02, bound:  3.16225797e-01\n",
      "Epoch: 10359 mean train loss:  2.13467274e-02, bound:  3.16226006e-01\n",
      "Epoch: 10360 mean train loss:  2.14294605e-02, bound:  3.16224962e-01\n",
      "Epoch: 10361 mean train loss:  2.15556994e-02, bound:  3.16225380e-01\n",
      "Epoch: 10362 mean train loss:  2.17409972e-02, bound:  3.16224068e-01\n",
      "Epoch: 10363 mean train loss:  2.20016856e-02, bound:  3.16224903e-01\n",
      "Epoch: 10364 mean train loss:  2.23415606e-02, bound:  3.16223085e-01\n",
      "Epoch: 10365 mean train loss:  2.27401555e-02, bound:  3.16224307e-01\n",
      "Epoch: 10366 mean train loss:  2.31100693e-02, bound:  3.16222042e-01\n",
      "Epoch: 10367 mean train loss:  2.33067907e-02, bound:  3.16223711e-01\n",
      "Epoch: 10368 mean train loss:  2.31517479e-02, bound:  3.16221237e-01\n",
      "Epoch: 10369 mean train loss:  2.26006899e-02, bound:  3.16222668e-01\n",
      "Epoch: 10370 mean train loss:  2.18401514e-02, bound:  3.16220820e-01\n",
      "Epoch: 10371 mean train loss:  2.12444011e-02, bound:  3.16221356e-01\n",
      "Epoch: 10372 mean train loss:  2.10872963e-02, bound:  3.16220790e-01\n",
      "Epoch: 10373 mean train loss:  2.13375986e-02, bound:  3.16220045e-01\n",
      "Epoch: 10374 mean train loss:  2.17077453e-02, bound:  3.16220701e-01\n",
      "Epoch: 10375 mean train loss:  2.18762588e-02, bound:  3.16219211e-01\n",
      "Epoch: 10376 mean train loss:  2.17138845e-02, bound:  3.16220134e-01\n",
      "Epoch: 10377 mean train loss:  2.13581417e-02, bound:  3.16218913e-01\n",
      "Epoch: 10378 mean train loss:  2.10834369e-02, bound:  3.16219151e-01\n",
      "Epoch: 10379 mean train loss:  2.10588444e-02, bound:  3.16218853e-01\n",
      "Epoch: 10380 mean train loss:  2.12283246e-02, bound:  3.16218078e-01\n",
      "Epoch: 10381 mean train loss:  2.13978514e-02, bound:  3.16218585e-01\n",
      "Epoch: 10382 mean train loss:  2.14102659e-02, bound:  3.16217333e-01\n",
      "Epoch: 10383 mean train loss:  2.12632474e-02, bound:  3.16217840e-01\n",
      "Epoch: 10384 mean train loss:  2.10803654e-02, bound:  3.16216916e-01\n",
      "Epoch: 10385 mean train loss:  2.09923852e-02, bound:  3.16216767e-01\n",
      "Epoch: 10386 mean train loss:  2.10313089e-02, bound:  3.16216528e-01\n",
      "Epoch: 10387 mean train loss:  2.11280249e-02, bound:  3.16215724e-01\n",
      "Epoch: 10388 mean train loss:  2.11851932e-02, bound:  3.16215992e-01\n",
      "Epoch: 10389 mean train loss:  2.11545508e-02, bound:  3.16214949e-01\n",
      "Epoch: 10390 mean train loss:  2.10628435e-02, bound:  3.16215128e-01\n",
      "Epoch: 10391 mean train loss:  2.09755655e-02, bound:  3.16214442e-01\n",
      "Epoch: 10392 mean train loss:  2.09425632e-02, bound:  3.16214174e-01\n",
      "Epoch: 10393 mean train loss:  2.09663231e-02, bound:  3.16213995e-01\n",
      "Epoch: 10394 mean train loss:  2.10107639e-02, bound:  3.16213369e-01\n",
      "Epoch: 10395 mean train loss:  2.10337937e-02, bound:  3.16213459e-01\n",
      "Epoch: 10396 mean train loss:  2.10166145e-02, bound:  3.16212654e-01\n",
      "Epoch: 10397 mean train loss:  2.09704675e-02, bound:  3.16212744e-01\n",
      "Epoch: 10398 mean train loss:  2.09221914e-02, bound:  3.16212147e-01\n",
      "Epoch: 10399 mean train loss:  2.08954103e-02, bound:  3.16211969e-01\n",
      "Epoch: 10400 mean train loss:  2.08956338e-02, bound:  3.16211700e-01\n",
      "Epoch: 10401 mean train loss:  2.09112428e-02, bound:  3.16211194e-01\n",
      "Epoch: 10402 mean train loss:  2.09250208e-02, bound:  3.16211134e-01\n",
      "Epoch: 10403 mean train loss:  2.09239852e-02, bound:  3.16210479e-01\n",
      "Epoch: 10404 mean train loss:  2.09062584e-02, bound:  3.16210479e-01\n",
      "Epoch: 10405 mean train loss:  2.08796747e-02, bound:  3.16209882e-01\n",
      "Epoch: 10406 mean train loss:  2.08555330e-02, bound:  3.16209704e-01\n",
      "Epoch: 10407 mean train loss:  2.08411608e-02, bound:  3.16209286e-01\n",
      "Epoch: 10408 mean train loss:  2.08378378e-02, bound:  3.16208899e-01\n",
      "Epoch: 10409 mean train loss:  2.08409075e-02, bound:  3.16208690e-01\n",
      "Epoch: 10410 mean train loss:  2.08435729e-02, bound:  3.16208154e-01\n",
      "Epoch: 10411 mean train loss:  2.08407287e-02, bound:  3.16208065e-01\n",
      "Epoch: 10412 mean train loss:  2.08309628e-02, bound:  3.16207528e-01\n",
      "Epoch: 10413 mean train loss:  2.08163057e-02, bound:  3.16207379e-01\n",
      "Epoch: 10414 mean train loss:  2.08011735e-02, bound:  3.16206932e-01\n",
      "Epoch: 10415 mean train loss:  2.07889508e-02, bound:  3.16206723e-01\n",
      "Epoch: 10416 mean train loss:  2.07810309e-02, bound:  3.16206396e-01\n",
      "Epoch: 10417 mean train loss:  2.07771864e-02, bound:  3.16206008e-01\n",
      "Epoch: 10418 mean train loss:  2.07747668e-02, bound:  3.16205829e-01\n",
      "Epoch: 10419 mean train loss:  2.07715277e-02, bound:  3.16205412e-01\n",
      "Epoch: 10420 mean train loss:  2.07659788e-02, bound:  3.16205233e-01\n",
      "Epoch: 10421 mean train loss:  2.07577012e-02, bound:  3.16204786e-01\n",
      "Epoch: 10422 mean train loss:  2.07479037e-02, bound:  3.16204578e-01\n",
      "Epoch: 10423 mean train loss:  2.07378287e-02, bound:  3.16204190e-01\n",
      "Epoch: 10424 mean train loss:  2.07286254e-02, bound:  3.16203952e-01\n",
      "Epoch: 10425 mean train loss:  2.07210705e-02, bound:  3.16203594e-01\n",
      "Epoch: 10426 mean train loss:  2.07150653e-02, bound:  3.16203266e-01\n",
      "Epoch: 10427 mean train loss:  2.07098853e-02, bound:  3.16202998e-01\n",
      "Epoch: 10428 mean train loss:  2.07049306e-02, bound:  3.16202581e-01\n",
      "Epoch: 10429 mean train loss:  2.06995010e-02, bound:  3.16202372e-01\n",
      "Epoch: 10430 mean train loss:  2.06927303e-02, bound:  3.16201925e-01\n",
      "Epoch: 10431 mean train loss:  2.06853431e-02, bound:  3.16201717e-01\n",
      "Epoch: 10432 mean train loss:  2.06775218e-02, bound:  3.16201299e-01\n",
      "Epoch: 10433 mean train loss:  2.06695888e-02, bound:  3.16201061e-01\n",
      "Epoch: 10434 mean train loss:  2.06618700e-02, bound:  3.16200703e-01\n",
      "Epoch: 10435 mean train loss:  2.06545908e-02, bound:  3.16200405e-01\n",
      "Epoch: 10436 mean train loss:  2.06481535e-02, bound:  3.16200078e-01\n",
      "Epoch: 10437 mean train loss:  2.06417311e-02, bound:  3.16199750e-01\n",
      "Epoch: 10438 mean train loss:  2.06355453e-02, bound:  3.16199422e-01\n",
      "Epoch: 10439 mean train loss:  2.06294376e-02, bound:  3.16199064e-01\n",
      "Epoch: 10440 mean train loss:  2.06231773e-02, bound:  3.16198796e-01\n",
      "Epoch: 10441 mean train loss:  2.06162781e-02, bound:  3.16198438e-01\n",
      "Epoch: 10442 mean train loss:  2.06094496e-02, bound:  3.16198140e-01\n",
      "Epoch: 10443 mean train loss:  2.06027068e-02, bound:  3.16197753e-01\n",
      "Epoch: 10444 mean train loss:  2.05954537e-02, bound:  3.16197455e-01\n",
      "Epoch: 10445 mean train loss:  2.05884408e-02, bound:  3.16197127e-01\n",
      "Epoch: 10446 mean train loss:  2.05815509e-02, bound:  3.16196799e-01\n",
      "Epoch: 10447 mean train loss:  2.05749068e-02, bound:  3.16196442e-01\n",
      "Epoch: 10448 mean train loss:  2.05678977e-02, bound:  3.16196144e-01\n",
      "Epoch: 10449 mean train loss:  2.05615051e-02, bound:  3.16195816e-01\n",
      "Epoch: 10450 mean train loss:  2.05547456e-02, bound:  3.16195428e-01\n",
      "Epoch: 10451 mean train loss:  2.05482952e-02, bound:  3.16195130e-01\n",
      "Epoch: 10452 mean train loss:  2.05417499e-02, bound:  3.16194803e-01\n",
      "Epoch: 10453 mean train loss:  2.05351524e-02, bound:  3.16194504e-01\n",
      "Epoch: 10454 mean train loss:  2.05285698e-02, bound:  3.16194087e-01\n",
      "Epoch: 10455 mean train loss:  2.05220114e-02, bound:  3.16193849e-01\n",
      "Epoch: 10456 mean train loss:  2.05151550e-02, bound:  3.16193432e-01\n",
      "Epoch: 10457 mean train loss:  2.05086041e-02, bound:  3.16193134e-01\n",
      "Epoch: 10458 mean train loss:  2.05016546e-02, bound:  3.16192776e-01\n",
      "Epoch: 10459 mean train loss:  2.04950292e-02, bound:  3.16192448e-01\n",
      "Epoch: 10460 mean train loss:  2.04881150e-02, bound:  3.16192120e-01\n",
      "Epoch: 10461 mean train loss:  2.04815082e-02, bound:  3.16191792e-01\n",
      "Epoch: 10462 mean train loss:  2.04750039e-02, bound:  3.16191435e-01\n",
      "Epoch: 10463 mean train loss:  2.04681531e-02, bound:  3.16191107e-01\n",
      "Epoch: 10464 mean train loss:  2.04613078e-02, bound:  3.16190779e-01\n",
      "Epoch: 10465 mean train loss:  2.04547718e-02, bound:  3.16190451e-01\n",
      "Epoch: 10466 mean train loss:  2.04481632e-02, bound:  3.16190094e-01\n",
      "Epoch: 10467 mean train loss:  2.04414874e-02, bound:  3.16189766e-01\n",
      "Epoch: 10468 mean train loss:  2.04348825e-02, bound:  3.16189438e-01\n",
      "Epoch: 10469 mean train loss:  2.04281732e-02, bound:  3.16189080e-01\n",
      "Epoch: 10470 mean train loss:  2.04214882e-02, bound:  3.16188753e-01\n",
      "Epoch: 10471 mean train loss:  2.04148050e-02, bound:  3.16188425e-01\n",
      "Epoch: 10472 mean train loss:  2.04082672e-02, bound:  3.16188097e-01\n",
      "Epoch: 10473 mean train loss:  2.04015821e-02, bound:  3.16187739e-01\n",
      "Epoch: 10474 mean train loss:  2.03950126e-02, bound:  3.16187441e-01\n",
      "Epoch: 10475 mean train loss:  2.03884859e-02, bound:  3.16187114e-01\n",
      "Epoch: 10476 mean train loss:  2.03819349e-02, bound:  3.16186786e-01\n",
      "Epoch: 10477 mean train loss:  2.03753226e-02, bound:  3.16186428e-01\n",
      "Epoch: 10478 mean train loss:  2.03686338e-02, bound:  3.16186130e-01\n",
      "Epoch: 10479 mean train loss:  2.03623716e-02, bound:  3.16185743e-01\n",
      "Epoch: 10480 mean train loss:  2.03557517e-02, bound:  3.16185445e-01\n",
      "Epoch: 10481 mean train loss:  2.03491822e-02, bound:  3.16185117e-01\n",
      "Epoch: 10482 mean train loss:  2.03426778e-02, bound:  3.16184789e-01\n",
      "Epoch: 10483 mean train loss:  2.03366037e-02, bound:  3.16184402e-01\n",
      "Epoch: 10484 mean train loss:  2.03301683e-02, bound:  3.16184133e-01\n",
      "Epoch: 10485 mean train loss:  2.03242302e-02, bound:  3.16183716e-01\n",
      "Epoch: 10486 mean train loss:  2.03183852e-02, bound:  3.16183478e-01\n",
      "Epoch: 10487 mean train loss:  2.03130953e-02, bound:  3.16183060e-01\n",
      "Epoch: 10488 mean train loss:  2.03081146e-02, bound:  3.16182822e-01\n",
      "Epoch: 10489 mean train loss:  2.03041416e-02, bound:  3.16182375e-01\n",
      "Epoch: 10490 mean train loss:  2.03010608e-02, bound:  3.16182196e-01\n",
      "Epoch: 10491 mean train loss:  2.02998687e-02, bound:  3.16181660e-01\n",
      "Epoch: 10492 mean train loss:  2.03007720e-02, bound:  3.16181540e-01\n",
      "Epoch: 10493 mean train loss:  2.03055888e-02, bound:  3.16180974e-01\n",
      "Epoch: 10494 mean train loss:  2.03159079e-02, bound:  3.16180915e-01\n",
      "Epoch: 10495 mean train loss:  2.03342102e-02, bound:  3.16180259e-01\n",
      "Epoch: 10496 mean train loss:  2.03646347e-02, bound:  3.16180319e-01\n",
      "Epoch: 10497 mean train loss:  2.04127692e-02, bound:  3.16179484e-01\n",
      "Epoch: 10498 mean train loss:  2.04873998e-02, bound:  3.16179782e-01\n",
      "Epoch: 10499 mean train loss:  2.05994602e-02, bound:  3.16178679e-01\n",
      "Epoch: 10500 mean train loss:  2.07633991e-02, bound:  3.16179246e-01\n",
      "Epoch: 10501 mean train loss:  2.09916364e-02, bound:  3.16177785e-01\n",
      "Epoch: 10502 mean train loss:  2.12937128e-02, bound:  3.16178799e-01\n",
      "Epoch: 10503 mean train loss:  2.16505416e-02, bound:  3.16176891e-01\n",
      "Epoch: 10504 mean train loss:  2.20056027e-02, bound:  3.16178262e-01\n",
      "Epoch: 10505 mean train loss:  2.22305767e-02, bound:  3.16175997e-01\n",
      "Epoch: 10506 mean train loss:  2.21795719e-02, bound:  3.16177577e-01\n",
      "Epoch: 10507 mean train loss:  2.17600577e-02, bound:  3.16175342e-01\n",
      "Epoch: 10508 mean train loss:  2.10880358e-02, bound:  3.16176504e-01\n",
      "Epoch: 10509 mean train loss:  2.04589330e-02, bound:  3.16175103e-01\n",
      "Epoch: 10510 mean train loss:  2.01680642e-02, bound:  3.16175222e-01\n",
      "Epoch: 10511 mean train loss:  2.02825125e-02, bound:  3.16175133e-01\n",
      "Epoch: 10512 mean train loss:  2.06120554e-02, bound:  3.16174150e-01\n",
      "Epoch: 10513 mean train loss:  2.08629202e-02, bound:  3.16174984e-01\n",
      "Epoch: 10514 mean train loss:  2.08399687e-02, bound:  3.16173494e-01\n",
      "Epoch: 10515 mean train loss:  2.05707662e-02, bound:  3.16174299e-01\n",
      "Epoch: 10516 mean train loss:  2.02606898e-02, bound:  3.16173345e-01\n",
      "Epoch: 10517 mean train loss:  2.01205742e-02, bound:  3.16173345e-01\n",
      "Epoch: 10518 mean train loss:  2.01971699e-02, bound:  3.16173255e-01\n",
      "Epoch: 10519 mean train loss:  2.03691162e-02, bound:  3.16172391e-01\n",
      "Epoch: 10520 mean train loss:  2.04691794e-02, bound:  3.16172868e-01\n",
      "Epoch: 10521 mean train loss:  2.04163641e-02, bound:  3.16171736e-01\n",
      "Epoch: 10522 mean train loss:  2.02596541e-02, bound:  3.16172153e-01\n",
      "Epoch: 10523 mean train loss:  2.01170854e-02, bound:  3.16171378e-01\n",
      "Epoch: 10524 mean train loss:  2.00746711e-02, bound:  3.16171139e-01\n",
      "Epoch: 10525 mean train loss:  2.01293845e-02, bound:  3.16171020e-01\n",
      "Epoch: 10526 mean train loss:  2.02106796e-02, bound:  3.16170186e-01\n",
      "Epoch: 10527 mean train loss:  2.02447157e-02, bound:  3.16170424e-01\n",
      "Epoch: 10528 mean train loss:  2.02061012e-02, bound:  3.16169530e-01\n",
      "Epoch: 10529 mean train loss:  2.01241449e-02, bound:  3.16169679e-01\n",
      "Epoch: 10530 mean train loss:  2.00520940e-02, bound:  3.16169053e-01\n",
      "Epoch: 10531 mean train loss:  2.00265441e-02, bound:  3.16168815e-01\n",
      "Epoch: 10532 mean train loss:  2.00462323e-02, bound:  3.16168636e-01\n",
      "Epoch: 10533 mean train loss:  2.00822446e-02, bound:  3.16168040e-01\n",
      "Epoch: 10534 mean train loss:  2.01021060e-02, bound:  3.16168189e-01\n",
      "Epoch: 10535 mean train loss:  2.00902224e-02, bound:  3.16167414e-01\n",
      "Epoch: 10536 mean train loss:  2.00530682e-02, bound:  3.16167504e-01\n",
      "Epoch: 10537 mean train loss:  2.00110972e-02, bound:  3.16166937e-01\n",
      "Epoch: 10538 mean train loss:  1.99833345e-02, bound:  3.16166788e-01\n",
      "Epoch: 10539 mean train loss:  1.99768730e-02, bound:  3.16166461e-01\n",
      "Epoch: 10540 mean train loss:  1.99858248e-02, bound:  3.16166043e-01\n",
      "Epoch: 10541 mean train loss:  1.99978724e-02, bound:  3.16165924e-01\n",
      "Epoch: 10542 mean train loss:  2.00014990e-02, bound:  3.16165358e-01\n",
      "Epoch: 10543 mean train loss:  1.99920703e-02, bound:  3.16165298e-01\n",
      "Epoch: 10544 mean train loss:  1.99722387e-02, bound:  3.16164762e-01\n",
      "Epoch: 10545 mean train loss:  1.99500024e-02, bound:  3.16164613e-01\n",
      "Epoch: 10546 mean train loss:  1.99324433e-02, bound:  3.16164196e-01\n",
      "Epoch: 10547 mean train loss:  1.99230723e-02, bound:  3.16163927e-01\n",
      "Epoch: 10548 mean train loss:  1.99210979e-02, bound:  3.16163629e-01\n",
      "Epoch: 10549 mean train loss:  1.99227426e-02, bound:  3.16163212e-01\n",
      "Epoch: 10550 mean train loss:  1.99226569e-02, bound:  3.16163093e-01\n",
      "Epoch: 10551 mean train loss:  1.99186280e-02, bound:  3.16162616e-01\n",
      "Epoch: 10552 mean train loss:  1.99096948e-02, bound:  3.16162527e-01\n",
      "Epoch: 10553 mean train loss:  1.98973902e-02, bound:  3.16162080e-01\n",
      "Epoch: 10554 mean train loss:  1.98847000e-02, bound:  3.16161901e-01\n",
      "Epoch: 10555 mean train loss:  1.98739301e-02, bound:  3.16161543e-01\n",
      "Epoch: 10556 mean train loss:  1.98659152e-02, bound:  3.16161305e-01\n",
      "Epoch: 10557 mean train loss:  1.98608283e-02, bound:  3.16161036e-01\n",
      "Epoch: 10558 mean train loss:  1.98570043e-02, bound:  3.16160679e-01\n",
      "Epoch: 10559 mean train loss:  1.98535342e-02, bound:  3.16160500e-01\n",
      "Epoch: 10560 mean train loss:  1.98485181e-02, bound:  3.16160113e-01\n",
      "Epoch: 10561 mean train loss:  1.98419802e-02, bound:  3.16159934e-01\n",
      "Epoch: 10562 mean train loss:  1.98341515e-02, bound:  3.16159517e-01\n",
      "Epoch: 10563 mean train loss:  1.98254324e-02, bound:  3.16159338e-01\n",
      "Epoch: 10564 mean train loss:  1.98169239e-02, bound:  3.16158980e-01\n",
      "Epoch: 10565 mean train loss:  1.98092386e-02, bound:  3.16158712e-01\n",
      "Epoch: 10566 mean train loss:  1.98022928e-02, bound:  3.16158384e-01\n",
      "Epoch: 10567 mean train loss:  1.97964720e-02, bound:  3.16158056e-01\n",
      "Epoch: 10568 mean train loss:  1.97909866e-02, bound:  3.16157848e-01\n",
      "Epoch: 10569 mean train loss:  1.97856743e-02, bound:  3.16157430e-01\n",
      "Epoch: 10570 mean train loss:  1.97799131e-02, bound:  3.16157281e-01\n",
      "Epoch: 10571 mean train loss:  1.97737291e-02, bound:  3.16156864e-01\n",
      "Epoch: 10572 mean train loss:  1.97669100e-02, bound:  3.16156656e-01\n",
      "Epoch: 10573 mean train loss:  1.97601710e-02, bound:  3.16156298e-01\n",
      "Epoch: 10574 mean train loss:  1.97531302e-02, bound:  3.16156089e-01\n",
      "Epoch: 10575 mean train loss:  1.97461806e-02, bound:  3.16155732e-01\n",
      "Epoch: 10576 mean train loss:  1.97391603e-02, bound:  3.16155434e-01\n",
      "Epoch: 10577 mean train loss:  1.97326876e-02, bound:  3.16155136e-01\n",
      "Epoch: 10578 mean train loss:  1.97261907e-02, bound:  3.16154867e-01\n",
      "Epoch: 10579 mean train loss:  1.97199136e-02, bound:  3.16154540e-01\n",
      "Epoch: 10580 mean train loss:  1.97139867e-02, bound:  3.16154242e-01\n",
      "Epoch: 10581 mean train loss:  1.97078232e-02, bound:  3.16153944e-01\n",
      "Epoch: 10582 mean train loss:  1.97016187e-02, bound:  3.16153586e-01\n",
      "Epoch: 10583 mean train loss:  1.96954254e-02, bound:  3.16153347e-01\n",
      "Epoch: 10584 mean train loss:  1.96890775e-02, bound:  3.16152960e-01\n",
      "Epoch: 10585 mean train loss:  1.96827427e-02, bound:  3.16152722e-01\n",
      "Epoch: 10586 mean train loss:  1.96762811e-02, bound:  3.16152364e-01\n",
      "Epoch: 10587 mean train loss:  1.96699370e-02, bound:  3.16152066e-01\n",
      "Epoch: 10588 mean train loss:  1.96633376e-02, bound:  3.16151738e-01\n",
      "Epoch: 10589 mean train loss:  1.96567867e-02, bound:  3.16151470e-01\n",
      "Epoch: 10590 mean train loss:  1.96504276e-02, bound:  3.16151142e-01\n",
      "Epoch: 10591 mean train loss:  1.96437389e-02, bound:  3.16150844e-01\n",
      "Epoch: 10592 mean train loss:  1.96375549e-02, bound:  3.16150516e-01\n",
      "Epoch: 10593 mean train loss:  1.96309108e-02, bound:  3.16150218e-01\n",
      "Epoch: 10594 mean train loss:  1.96248256e-02, bound:  3.16149920e-01\n",
      "Epoch: 10595 mean train loss:  1.96182057e-02, bound:  3.16149622e-01\n",
      "Epoch: 10596 mean train loss:  1.96120292e-02, bound:  3.16149294e-01\n",
      "Epoch: 10597 mean train loss:  1.96054839e-02, bound:  3.16148996e-01\n",
      "Epoch: 10598 mean train loss:  1.95991937e-02, bound:  3.16148669e-01\n",
      "Epoch: 10599 mean train loss:  1.95927434e-02, bound:  3.16148400e-01\n",
      "Epoch: 10600 mean train loss:  1.95865966e-02, bound:  3.16148043e-01\n",
      "Epoch: 10601 mean train loss:  1.95801146e-02, bound:  3.16147774e-01\n",
      "Epoch: 10602 mean train loss:  1.95738636e-02, bound:  3.16147447e-01\n",
      "Epoch: 10603 mean train loss:  1.95674393e-02, bound:  3.16147119e-01\n",
      "Epoch: 10604 mean train loss:  1.95612777e-02, bound:  3.16146821e-01\n",
      "Epoch: 10605 mean train loss:  1.95547175e-02, bound:  3.16146493e-01\n",
      "Epoch: 10606 mean train loss:  1.95484050e-02, bound:  3.16146195e-01\n",
      "Epoch: 10607 mean train loss:  1.95420925e-02, bound:  3.16145897e-01\n",
      "Epoch: 10608 mean train loss:  1.95356607e-02, bound:  3.16145599e-01\n",
      "Epoch: 10609 mean train loss:  1.95292607e-02, bound:  3.16145271e-01\n",
      "Epoch: 10610 mean train loss:  1.95229109e-02, bound:  3.16144973e-01\n",
      "Epoch: 10611 mean train loss:  1.95165742e-02, bound:  3.16144615e-01\n",
      "Epoch: 10612 mean train loss:  1.95101202e-02, bound:  3.16144317e-01\n",
      "Epoch: 10613 mean train loss:  1.95038971e-02, bound:  3.16143990e-01\n",
      "Epoch: 10614 mean train loss:  1.94976032e-02, bound:  3.16143721e-01\n",
      "Epoch: 10615 mean train loss:  1.94912683e-02, bound:  3.16143394e-01\n",
      "Epoch: 10616 mean train loss:  1.94851123e-02, bound:  3.16143125e-01\n",
      "Epoch: 10617 mean train loss:  1.94790419e-02, bound:  3.16142768e-01\n",
      "Epoch: 10618 mean train loss:  1.94727741e-02, bound:  3.16142529e-01\n",
      "Epoch: 10619 mean train loss:  1.94667000e-02, bound:  3.16142201e-01\n",
      "Epoch: 10620 mean train loss:  1.94607936e-02, bound:  3.16141903e-01\n",
      "Epoch: 10621 mean train loss:  1.94549840e-02, bound:  3.16141546e-01\n",
      "Epoch: 10622 mean train loss:  1.94493327e-02, bound:  3.16141337e-01\n",
      "Epoch: 10623 mean train loss:  1.94442384e-02, bound:  3.16140920e-01\n",
      "Epoch: 10624 mean train loss:  1.94393396e-02, bound:  3.16140682e-01\n",
      "Epoch: 10625 mean train loss:  1.94353219e-02, bound:  3.16140294e-01\n",
      "Epoch: 10626 mean train loss:  1.94325112e-02, bound:  3.16140145e-01\n",
      "Epoch: 10627 mean train loss:  1.94311813e-02, bound:  3.16139668e-01\n",
      "Epoch: 10628 mean train loss:  1.94324777e-02, bound:  3.16139519e-01\n",
      "Epoch: 10629 mean train loss:  1.94372479e-02, bound:  3.16138953e-01\n",
      "Epoch: 10630 mean train loss:  1.94474831e-02, bound:  3.16138953e-01\n",
      "Epoch: 10631 mean train loss:  1.94657240e-02, bound:  3.16138297e-01\n",
      "Epoch: 10632 mean train loss:  1.94964148e-02, bound:  3.16138417e-01\n",
      "Epoch: 10633 mean train loss:  1.95451565e-02, bound:  3.16137582e-01\n",
      "Epoch: 10634 mean train loss:  1.96217746e-02, bound:  3.16137910e-01\n",
      "Epoch: 10635 mean train loss:  1.97377708e-02, bound:  3.16136837e-01\n",
      "Epoch: 10636 mean train loss:  1.99100822e-02, bound:  3.16137433e-01\n",
      "Epoch: 10637 mean train loss:  2.01544613e-02, bound:  3.16136003e-01\n",
      "Epoch: 10638 mean train loss:  2.04822253e-02, bound:  3.16137046e-01\n",
      "Epoch: 10639 mean train loss:  2.08753459e-02, bound:  3.16135108e-01\n",
      "Epoch: 10640 mean train loss:  2.12733671e-02, bound:  3.16136539e-01\n",
      "Epoch: 10641 mean train loss:  2.15309318e-02, bound:  3.16134244e-01\n",
      "Epoch: 10642 mean train loss:  2.14766320e-02, bound:  3.16135913e-01\n",
      "Epoch: 10643 mean train loss:  2.10029259e-02, bound:  3.16133678e-01\n",
      "Epoch: 10644 mean train loss:  2.02515535e-02, bound:  3.16134900e-01\n",
      "Epoch: 10645 mean train loss:  1.95733048e-02, bound:  3.16133559e-01\n",
      "Epoch: 10646 mean train loss:  1.93019100e-02, bound:  3.16133618e-01\n",
      "Epoch: 10647 mean train loss:  1.94832627e-02, bound:  3.16133618e-01\n",
      "Epoch: 10648 mean train loss:  1.98641755e-02, bound:  3.16132575e-01\n",
      "Epoch: 10649 mean train loss:  2.00994499e-02, bound:  3.16133469e-01\n",
      "Epoch: 10650 mean train loss:  1.99991316e-02, bound:  3.16132039e-01\n",
      "Epoch: 10651 mean train loss:  1.96563508e-02, bound:  3.16132843e-01\n",
      "Epoch: 10652 mean train loss:  1.93433184e-02, bound:  3.16131979e-01\n",
      "Epoch: 10653 mean train loss:  1.92708615e-02, bound:  3.16131860e-01\n",
      "Epoch: 10654 mean train loss:  1.94226298e-02, bound:  3.16131979e-01\n",
      "Epoch: 10655 mean train loss:  1.96078587e-02, bound:  3.16130996e-01\n",
      "Epoch: 10656 mean train loss:  1.96483470e-02, bound:  3.16131592e-01\n",
      "Epoch: 10657 mean train loss:  1.95155293e-02, bound:  3.16130519e-01\n",
      "Epoch: 10658 mean train loss:  1.93269365e-02, bound:  3.16130817e-01\n",
      "Epoch: 10659 mean train loss:  1.92251578e-02, bound:  3.16130221e-01\n",
      "Epoch: 10660 mean train loss:  1.92570444e-02, bound:  3.16129833e-01\n",
      "Epoch: 10661 mean train loss:  1.93562321e-02, bound:  3.16129953e-01\n",
      "Epoch: 10662 mean train loss:  1.94195174e-02, bound:  3.16128999e-01\n",
      "Epoch: 10663 mean train loss:  1.93926189e-02, bound:  3.16129297e-01\n",
      "Epoch: 10664 mean train loss:  1.92997400e-02, bound:  3.16128463e-01\n",
      "Epoch: 10665 mean train loss:  1.92103442e-02, bound:  3.16128522e-01\n",
      "Epoch: 10666 mean train loss:  1.91780012e-02, bound:  3.16128105e-01\n",
      "Epoch: 10667 mean train loss:  1.92048047e-02, bound:  3.16127658e-01\n",
      "Epoch: 10668 mean train loss:  1.92513671e-02, bound:  3.16127688e-01\n",
      "Epoch: 10669 mean train loss:  1.92735847e-02, bound:  3.16126972e-01\n",
      "Epoch: 10670 mean train loss:  1.92533508e-02, bound:  3.16127211e-01\n",
      "Epoch: 10671 mean train loss:  1.92040596e-02, bound:  3.16126496e-01\n",
      "Epoch: 10672 mean train loss:  1.91567671e-02, bound:  3.16126466e-01\n",
      "Epoch: 10673 mean train loss:  1.91339031e-02, bound:  3.16126138e-01\n",
      "Epoch: 10674 mean train loss:  1.91392507e-02, bound:  3.16125751e-01\n",
      "Epoch: 10675 mean train loss:  1.91581324e-02, bound:  3.16125661e-01\n",
      "Epoch: 10676 mean train loss:  1.91707928e-02, bound:  3.16125095e-01\n",
      "Epoch: 10677 mean train loss:  1.91659257e-02, bound:  3.16125154e-01\n",
      "Epoch: 10678 mean train loss:  1.91445649e-02, bound:  3.16124529e-01\n",
      "Epoch: 10679 mean train loss:  1.91171598e-02, bound:  3.16124469e-01\n",
      "Epoch: 10680 mean train loss:  1.90951135e-02, bound:  3.16124052e-01\n",
      "Epoch: 10681 mean train loss:  1.90852210e-02, bound:  3.16123784e-01\n",
      "Epoch: 10682 mean train loss:  1.90860685e-02, bound:  3.16123545e-01\n",
      "Epoch: 10683 mean train loss:  1.90909281e-02, bound:  3.16123098e-01\n",
      "Epoch: 10684 mean train loss:  1.90927554e-02, bound:  3.16123039e-01\n",
      "Epoch: 10685 mean train loss:  1.90876424e-02, bound:  3.16122532e-01\n",
      "Epoch: 10686 mean train loss:  1.90753583e-02, bound:  3.16122472e-01\n",
      "Epoch: 10687 mean train loss:  1.90598369e-02, bound:  3.16122025e-01\n",
      "Epoch: 10688 mean train loss:  1.90459173e-02, bound:  3.16121876e-01\n",
      "Epoch: 10689 mean train loss:  1.90364104e-02, bound:  3.16121548e-01\n",
      "Epoch: 10690 mean train loss:  1.90315079e-02, bound:  3.16121310e-01\n",
      "Epoch: 10691 mean train loss:  1.90295838e-02, bound:  3.16121131e-01\n",
      "Epoch: 10692 mean train loss:  1.90278813e-02, bound:  3.16120774e-01\n",
      "Epoch: 10693 mean train loss:  1.90240610e-02, bound:  3.16120654e-01\n",
      "Epoch: 10694 mean train loss:  1.90172587e-02, bound:  3.16120237e-01\n",
      "Epoch: 10695 mean train loss:  1.90083198e-02, bound:  3.16120118e-01\n",
      "Epoch: 10696 mean train loss:  1.89985260e-02, bound:  3.16119730e-01\n",
      "Epoch: 10697 mean train loss:  1.89896841e-02, bound:  3.16119581e-01\n",
      "Epoch: 10698 mean train loss:  1.89820826e-02, bound:  3.16119254e-01\n",
      "Epoch: 10699 mean train loss:  1.89760756e-02, bound:  3.16119015e-01\n",
      "Epoch: 10700 mean train loss:  1.89711545e-02, bound:  3.16118747e-01\n",
      "Epoch: 10701 mean train loss:  1.89667232e-02, bound:  3.16118419e-01\n",
      "Epoch: 10702 mean train loss:  1.89616289e-02, bound:  3.16118240e-01\n",
      "Epoch: 10703 mean train loss:  1.89560018e-02, bound:  3.16117853e-01\n",
      "Epoch: 10704 mean train loss:  1.89495031e-02, bound:  3.16117704e-01\n",
      "Epoch: 10705 mean train loss:  1.89425386e-02, bound:  3.16117316e-01\n",
      "Epoch: 10706 mean train loss:  1.89351887e-02, bound:  3.16117138e-01\n",
      "Epoch: 10707 mean train loss:  1.89280398e-02, bound:  3.16116840e-01\n",
      "Epoch: 10708 mean train loss:  1.89213268e-02, bound:  3.16116601e-01\n",
      "Epoch: 10709 mean train loss:  1.89152863e-02, bound:  3.16116303e-01\n",
      "Epoch: 10710 mean train loss:  1.89096965e-02, bound:  3.16116005e-01\n",
      "Epoch: 10711 mean train loss:  1.89041346e-02, bound:  3.16115767e-01\n",
      "Epoch: 10712 mean train loss:  1.88985188e-02, bound:  3.16115469e-01\n",
      "Epoch: 10713 mean train loss:  1.88926719e-02, bound:  3.16115230e-01\n",
      "Epoch: 10714 mean train loss:  1.88867114e-02, bound:  3.16114962e-01\n",
      "Epoch: 10715 mean train loss:  1.88803319e-02, bound:  3.16114694e-01\n",
      "Epoch: 10716 mean train loss:  1.88739207e-02, bound:  3.16114396e-01\n",
      "Epoch: 10717 mean train loss:  1.88674517e-02, bound:  3.16114157e-01\n",
      "Epoch: 10718 mean train loss:  1.88610479e-02, bound:  3.16113800e-01\n",
      "Epoch: 10719 mean train loss:  1.88547894e-02, bound:  3.16113591e-01\n",
      "Epoch: 10720 mean train loss:  1.88487675e-02, bound:  3.16113263e-01\n",
      "Epoch: 10721 mean train loss:  1.88426115e-02, bound:  3.16113025e-01\n",
      "Epoch: 10722 mean train loss:  1.88365076e-02, bound:  3.16112727e-01\n",
      "Epoch: 10723 mean train loss:  1.88305695e-02, bound:  3.16112429e-01\n",
      "Epoch: 10724 mean train loss:  1.88246090e-02, bound:  3.16112161e-01\n",
      "Epoch: 10725 mean train loss:  1.88187584e-02, bound:  3.16111833e-01\n",
      "Epoch: 10726 mean train loss:  1.88127141e-02, bound:  3.16111594e-01\n",
      "Epoch: 10727 mean train loss:  1.88067984e-02, bound:  3.16111267e-01\n",
      "Epoch: 10728 mean train loss:  1.88007280e-02, bound:  3.16111028e-01\n",
      "Epoch: 10729 mean train loss:  1.87945981e-02, bound:  3.16110730e-01\n",
      "Epoch: 10730 mean train loss:  1.87884700e-02, bound:  3.16110462e-01\n",
      "Epoch: 10731 mean train loss:  1.87821873e-02, bound:  3.16110164e-01\n",
      "Epoch: 10732 mean train loss:  1.87761001e-02, bound:  3.16109866e-01\n",
      "Epoch: 10733 mean train loss:  1.87698863e-02, bound:  3.16109598e-01\n",
      "Epoch: 10734 mean train loss:  1.87638365e-02, bound:  3.16109300e-01\n",
      "Epoch: 10735 mean train loss:  1.87576432e-02, bound:  3.16109002e-01\n",
      "Epoch: 10736 mean train loss:  1.87515840e-02, bound:  3.16108733e-01\n",
      "Epoch: 10737 mean train loss:  1.87455192e-02, bound:  3.16108435e-01\n",
      "Epoch: 10738 mean train loss:  1.87393557e-02, bound:  3.16108167e-01\n",
      "Epoch: 10739 mean train loss:  1.87333375e-02, bound:  3.16107869e-01\n",
      "Epoch: 10740 mean train loss:  1.87270977e-02, bound:  3.16107631e-01\n",
      "Epoch: 10741 mean train loss:  1.87210273e-02, bound:  3.16107303e-01\n",
      "Epoch: 10742 mean train loss:  1.87152606e-02, bound:  3.16107005e-01\n",
      "Epoch: 10743 mean train loss:  1.87090077e-02, bound:  3.16106766e-01\n",
      "Epoch: 10744 mean train loss:  1.87030379e-02, bound:  3.16106439e-01\n",
      "Epoch: 10745 mean train loss:  1.86970327e-02, bound:  3.16106170e-01\n",
      "Epoch: 10746 mean train loss:  1.86909307e-02, bound:  3.16105872e-01\n",
      "Epoch: 10747 mean train loss:  1.86846219e-02, bound:  3.16105604e-01\n",
      "Epoch: 10748 mean train loss:  1.86786298e-02, bound:  3.16105336e-01\n",
      "Epoch: 10749 mean train loss:  1.86726283e-02, bound:  3.16105038e-01\n",
      "Epoch: 10750 mean train loss:  1.86666586e-02, bound:  3.16104740e-01\n",
      "Epoch: 10751 mean train loss:  1.86604429e-02, bound:  3.16104472e-01\n",
      "Epoch: 10752 mean train loss:  1.86544023e-02, bound:  3.16104174e-01\n",
      "Epoch: 10753 mean train loss:  1.86483655e-02, bound:  3.16103905e-01\n",
      "Epoch: 10754 mean train loss:  1.86422877e-02, bound:  3.16103607e-01\n",
      "Epoch: 10755 mean train loss:  1.86361633e-02, bound:  3.16103339e-01\n",
      "Epoch: 10756 mean train loss:  1.86301991e-02, bound:  3.16103041e-01\n",
      "Epoch: 10757 mean train loss:  1.86240952e-02, bound:  3.16102803e-01\n",
      "Epoch: 10758 mean train loss:  1.86179820e-02, bound:  3.16102475e-01\n",
      "Epoch: 10759 mean train loss:  1.86118744e-02, bound:  3.16102177e-01\n",
      "Epoch: 10760 mean train loss:  1.86060145e-02, bound:  3.16101879e-01\n",
      "Epoch: 10761 mean train loss:  1.85997915e-02, bound:  3.16101640e-01\n",
      "Epoch: 10762 mean train loss:  1.85940340e-02, bound:  3.16101342e-01\n",
      "Epoch: 10763 mean train loss:  1.85880717e-02, bound:  3.16101074e-01\n",
      "Epoch: 10764 mean train loss:  1.85820833e-02, bound:  3.16100776e-01\n",
      "Epoch: 10765 mean train loss:  1.85761768e-02, bound:  3.16100538e-01\n",
      "Epoch: 10766 mean train loss:  1.85705908e-02, bound:  3.16100210e-01\n",
      "Epoch: 10767 mean train loss:  1.85650159e-02, bound:  3.16099972e-01\n",
      "Epoch: 10768 mean train loss:  1.85595136e-02, bound:  3.16099644e-01\n",
      "Epoch: 10769 mean train loss:  1.85545105e-02, bound:  3.16099435e-01\n",
      "Epoch: 10770 mean train loss:  1.85499508e-02, bound:  3.16099048e-01\n",
      "Epoch: 10771 mean train loss:  1.85460765e-02, bound:  3.16098869e-01\n",
      "Epoch: 10772 mean train loss:  1.85429230e-02, bound:  3.16098452e-01\n",
      "Epoch: 10773 mean train loss:  1.85414217e-02, bound:  3.16098332e-01\n",
      "Epoch: 10774 mean train loss:  1.85422134e-02, bound:  3.16097885e-01\n",
      "Epoch: 10775 mean train loss:  1.85462311e-02, bound:  3.16097796e-01\n",
      "Epoch: 10776 mean train loss:  1.85555536e-02, bound:  3.16097230e-01\n",
      "Epoch: 10777 mean train loss:  1.85728800e-02, bound:  3.16097289e-01\n",
      "Epoch: 10778 mean train loss:  1.86025221e-02, bound:  3.16096604e-01\n",
      "Epoch: 10779 mean train loss:  1.86511762e-02, bound:  3.16096812e-01\n",
      "Epoch: 10780 mean train loss:  1.87284797e-02, bound:  3.16095918e-01\n",
      "Epoch: 10781 mean train loss:  1.88485887e-02, bound:  3.16096395e-01\n",
      "Epoch: 10782 mean train loss:  1.90290101e-02, bound:  3.16095173e-01\n",
      "Epoch: 10783 mean train loss:  1.92906447e-02, bound:  3.16095978e-01\n",
      "Epoch: 10784 mean train loss:  1.96457952e-02, bound:  3.16094398e-01\n",
      "Epoch: 10785 mean train loss:  2.00856198e-02, bound:  3.16095620e-01\n",
      "Epoch: 10786 mean train loss:  2.05332115e-02, bound:  3.16093534e-01\n",
      "Epoch: 10787 mean train loss:  2.08387226e-02, bound:  3.16095173e-01\n",
      "Epoch: 10788 mean train loss:  2.07791999e-02, bound:  3.16092789e-01\n",
      "Epoch: 10789 mean train loss:  2.02482250e-02, bound:  3.16094398e-01\n",
      "Epoch: 10790 mean train loss:  1.93996727e-02, bound:  3.16092521e-01\n",
      "Epoch: 10791 mean train loss:  1.86660364e-02, bound:  3.16093206e-01\n",
      "Epoch: 10792 mean train loss:  1.84212197e-02, bound:  3.16092640e-01\n",
      "Epoch: 10793 mean train loss:  1.86810121e-02, bound:  3.16091985e-01\n",
      "Epoch: 10794 mean train loss:  1.91122666e-02, bound:  3.16092670e-01\n",
      "Epoch: 10795 mean train loss:  1.93119477e-02, bound:  3.16091299e-01\n",
      "Epoch: 10796 mean train loss:  1.91158503e-02, bound:  3.16092312e-01\n",
      "Epoch: 10797 mean train loss:  1.87012888e-02, bound:  3.16091150e-01\n",
      "Epoch: 10798 mean train loss:  1.84135549e-02, bound:  3.16091448e-01\n",
      "Epoch: 10799 mean train loss:  1.84374731e-02, bound:  3.16091239e-01\n",
      "Epoch: 10800 mean train loss:  1.86608098e-02, bound:  3.16090524e-01\n",
      "Epoch: 10801 mean train loss:  1.88254844e-02, bound:  3.16091150e-01\n",
      "Epoch: 10802 mean train loss:  1.87722296e-02, bound:  3.16089988e-01\n",
      "Epoch: 10803 mean train loss:  1.85611323e-02, bound:  3.16090465e-01\n",
      "Epoch: 10804 mean train loss:  1.83807146e-02, bound:  3.16089779e-01\n",
      "Epoch: 10805 mean train loss:  1.83595531e-02, bound:  3.16089571e-01\n",
      "Epoch: 10806 mean train loss:  1.84659716e-02, bound:  3.16089571e-01\n",
      "Epoch: 10807 mean train loss:  1.85686219e-02, bound:  3.16088706e-01\n",
      "Epoch: 10808 mean train loss:  1.85658839e-02, bound:  3.16089064e-01\n",
      "Epoch: 10809 mean train loss:  1.84643697e-02, bound:  3.16088170e-01\n",
      "Epoch: 10810 mean train loss:  1.83516741e-02, bound:  3.16088289e-01\n",
      "Epoch: 10811 mean train loss:  1.83075443e-02, bound:  3.16087842e-01\n",
      "Epoch: 10812 mean train loss:  1.83418579e-02, bound:  3.16087425e-01\n",
      "Epoch: 10813 mean train loss:  1.84020679e-02, bound:  3.16087544e-01\n",
      "Epoch: 10814 mean train loss:  1.84273385e-02, bound:  3.16086829e-01\n",
      "Epoch: 10815 mean train loss:  1.83956157e-02, bound:  3.16087037e-01\n",
      "Epoch: 10816 mean train loss:  1.83323063e-02, bound:  3.16086411e-01\n",
      "Epoch: 10817 mean train loss:  1.82805881e-02, bound:  3.16086352e-01\n",
      "Epoch: 10818 mean train loss:  1.82670094e-02, bound:  3.16086084e-01\n",
      "Epoch: 10819 mean train loss:  1.82862002e-02, bound:  3.16085666e-01\n",
      "Epoch: 10820 mean train loss:  1.83115844e-02, bound:  3.16085666e-01\n",
      "Epoch: 10821 mean train loss:  1.83178559e-02, bound:  3.16085070e-01\n",
      "Epoch: 10822 mean train loss:  1.82987694e-02, bound:  3.16085130e-01\n",
      "Epoch: 10823 mean train loss:  1.82657205e-02, bound:  3.16084594e-01\n",
      "Epoch: 10824 mean train loss:  1.82367936e-02, bound:  3.16084504e-01\n",
      "Epoch: 10825 mean train loss:  1.82240270e-02, bound:  3.16084146e-01\n",
      "Epoch: 10826 mean train loss:  1.82268992e-02, bound:  3.16083819e-01\n",
      "Epoch: 10827 mean train loss:  1.82359535e-02, bound:  3.16083759e-01\n",
      "Epoch: 10828 mean train loss:  1.82398688e-02, bound:  3.16083223e-01\n",
      "Epoch: 10829 mean train loss:  1.82326045e-02, bound:  3.16083223e-01\n",
      "Epoch: 10830 mean train loss:  1.82168465e-02, bound:  3.16082716e-01\n",
      "Epoch: 10831 mean train loss:  1.81985181e-02, bound:  3.16082656e-01\n",
      "Epoch: 10832 mean train loss:  1.81844346e-02, bound:  3.16082329e-01\n",
      "Epoch: 10833 mean train loss:  1.81779563e-02, bound:  3.16082060e-01\n",
      "Epoch: 10834 mean train loss:  1.81769226e-02, bound:  3.16081911e-01\n",
      "Epoch: 10835 mean train loss:  1.81772206e-02, bound:  3.16081583e-01\n",
      "Epoch: 10836 mean train loss:  1.81750860e-02, bound:  3.16081494e-01\n",
      "Epoch: 10837 mean train loss:  1.81684028e-02, bound:  3.16081077e-01\n",
      "Epoch: 10838 mean train loss:  1.81584526e-02, bound:  3.16081017e-01\n",
      "Epoch: 10839 mean train loss:  1.81473959e-02, bound:  3.16080689e-01\n",
      "Epoch: 10840 mean train loss:  1.81379095e-02, bound:  3.16080511e-01\n",
      "Epoch: 10841 mean train loss:  1.81311984e-02, bound:  3.16080272e-01\n",
      "Epoch: 10842 mean train loss:  1.81271117e-02, bound:  3.16079974e-01\n",
      "Epoch: 10843 mean train loss:  1.81239713e-02, bound:  3.16079825e-01\n",
      "Epoch: 10844 mean train loss:  1.81203503e-02, bound:  3.16079468e-01\n",
      "Epoch: 10845 mean train loss:  1.81152243e-02, bound:  3.16079378e-01\n",
      "Epoch: 10846 mean train loss:  1.81081332e-02, bound:  3.16079021e-01\n",
      "Epoch: 10847 mean train loss:  1.81005038e-02, bound:  3.16078842e-01\n",
      "Epoch: 10848 mean train loss:  1.80923473e-02, bound:  3.16078544e-01\n",
      "Epoch: 10849 mean train loss:  1.80852208e-02, bound:  3.16078335e-01\n",
      "Epoch: 10850 mean train loss:  1.80791747e-02, bound:  3.16078097e-01\n",
      "Epoch: 10851 mean train loss:  1.80740152e-02, bound:  3.16077858e-01\n",
      "Epoch: 10852 mean train loss:  1.80694386e-02, bound:  3.16077650e-01\n",
      "Epoch: 10853 mean train loss:  1.80644169e-02, bound:  3.16077352e-01\n",
      "Epoch: 10854 mean train loss:  1.80589706e-02, bound:  3.16077173e-01\n",
      "Epoch: 10855 mean train loss:  1.80527736e-02, bound:  3.16076845e-01\n",
      "Epoch: 10856 mean train loss:  1.80464573e-02, bound:  3.16076696e-01\n",
      "Epoch: 10857 mean train loss:  1.80397779e-02, bound:  3.16076398e-01\n",
      "Epoch: 10858 mean train loss:  1.80331860e-02, bound:  3.16076219e-01\n",
      "Epoch: 10859 mean train loss:  1.80272888e-02, bound:  3.16075921e-01\n",
      "Epoch: 10860 mean train loss:  1.80214345e-02, bound:  3.16075683e-01\n",
      "Epoch: 10861 mean train loss:  1.80159081e-02, bound:  3.16075474e-01\n",
      "Epoch: 10862 mean train loss:  1.80104468e-02, bound:  3.16075146e-01\n",
      "Epoch: 10863 mean train loss:  1.80049613e-02, bound:  3.16074967e-01\n",
      "Epoch: 10864 mean train loss:  1.79993585e-02, bound:  3.16074669e-01\n",
      "Epoch: 10865 mean train loss:  1.79933328e-02, bound:  3.16074461e-01\n",
      "Epoch: 10866 mean train loss:  1.79874133e-02, bound:  3.16074163e-01\n",
      "Epoch: 10867 mean train loss:  1.79813504e-02, bound:  3.16073924e-01\n",
      "Epoch: 10868 mean train loss:  1.79753862e-02, bound:  3.16073656e-01\n",
      "Epoch: 10869 mean train loss:  1.79694518e-02, bound:  3.16073388e-01\n",
      "Epoch: 10870 mean train loss:  1.79637149e-02, bound:  3.16073149e-01\n",
      "Epoch: 10871 mean train loss:  1.79579612e-02, bound:  3.16072851e-01\n",
      "Epoch: 10872 mean train loss:  1.79521162e-02, bound:  3.16072613e-01\n",
      "Epoch: 10873 mean train loss:  1.79464482e-02, bound:  3.16072375e-01\n",
      "Epoch: 10874 mean train loss:  1.79406162e-02, bound:  3.16072136e-01\n",
      "Epoch: 10875 mean train loss:  1.79351065e-02, bound:  3.16071838e-01\n",
      "Epoch: 10876 mean train loss:  1.79294180e-02, bound:  3.16071600e-01\n",
      "Epoch: 10877 mean train loss:  1.79235246e-02, bound:  3.16071302e-01\n",
      "Epoch: 10878 mean train loss:  1.79176703e-02, bound:  3.16071093e-01\n",
      "Epoch: 10879 mean train loss:  1.79116987e-02, bound:  3.16070825e-01\n",
      "Epoch: 10880 mean train loss:  1.79059003e-02, bound:  3.16070557e-01\n",
      "Epoch: 10881 mean train loss:  1.79002825e-02, bound:  3.16070288e-01\n",
      "Epoch: 10882 mean train loss:  1.78943146e-02, bound:  3.16069990e-01\n",
      "Epoch: 10883 mean train loss:  1.78884566e-02, bound:  3.16069752e-01\n",
      "Epoch: 10884 mean train loss:  1.78827830e-02, bound:  3.16069514e-01\n",
      "Epoch: 10885 mean train loss:  1.78768970e-02, bound:  3.16069245e-01\n",
      "Epoch: 10886 mean train loss:  1.78712141e-02, bound:  3.16068977e-01\n",
      "Epoch: 10887 mean train loss:  1.78652219e-02, bound:  3.16068769e-01\n",
      "Epoch: 10888 mean train loss:  1.78597569e-02, bound:  3.16068470e-01\n",
      "Epoch: 10889 mean train loss:  1.78540070e-02, bound:  3.16068202e-01\n",
      "Epoch: 10890 mean train loss:  1.78481303e-02, bound:  3.16067934e-01\n",
      "Epoch: 10891 mean train loss:  1.78422797e-02, bound:  3.16067696e-01\n",
      "Epoch: 10892 mean train loss:  1.78365689e-02, bound:  3.16067457e-01\n",
      "Epoch: 10893 mean train loss:  1.78307015e-02, bound:  3.16067219e-01\n",
      "Epoch: 10894 mean train loss:  1.78251117e-02, bound:  3.16066921e-01\n",
      "Epoch: 10895 mean train loss:  1.78192277e-02, bound:  3.16066653e-01\n",
      "Epoch: 10896 mean train loss:  1.78133678e-02, bound:  3.16066384e-01\n",
      "Epoch: 10897 mean train loss:  1.78075992e-02, bound:  3.16066146e-01\n",
      "Epoch: 10898 mean train loss:  1.78016946e-02, bound:  3.16065907e-01\n",
      "Epoch: 10899 mean train loss:  1.77960768e-02, bound:  3.16065669e-01\n",
      "Epoch: 10900 mean train loss:  1.77901126e-02, bound:  3.16065401e-01\n",
      "Epoch: 10901 mean train loss:  1.77843180e-02, bound:  3.16065162e-01\n",
      "Epoch: 10902 mean train loss:  1.77786648e-02, bound:  3.16064894e-01\n",
      "Epoch: 10903 mean train loss:  1.77727453e-02, bound:  3.16064626e-01\n",
      "Epoch: 10904 mean train loss:  1.77670922e-02, bound:  3.16064358e-01\n",
      "Epoch: 10905 mean train loss:  1.77613217e-02, bound:  3.16064119e-01\n",
      "Epoch: 10906 mean train loss:  1.77555680e-02, bound:  3.16063851e-01\n",
      "Epoch: 10907 mean train loss:  1.77497398e-02, bound:  3.16063613e-01\n",
      "Epoch: 10908 mean train loss:  1.77439265e-02, bound:  3.16063344e-01\n",
      "Epoch: 10909 mean train loss:  1.77382268e-02, bound:  3.16063106e-01\n",
      "Epoch: 10910 mean train loss:  1.77325550e-02, bound:  3.16062838e-01\n",
      "Epoch: 10911 mean train loss:  1.77268423e-02, bound:  3.16062599e-01\n",
      "Epoch: 10912 mean train loss:  1.77211743e-02, bound:  3.16062301e-01\n",
      "Epoch: 10913 mean train loss:  1.77155156e-02, bound:  3.16062063e-01\n",
      "Epoch: 10914 mean train loss:  1.77098829e-02, bound:  3.16061795e-01\n",
      "Epoch: 10915 mean train loss:  1.77044347e-02, bound:  3.16061556e-01\n",
      "Epoch: 10916 mean train loss:  1.76989231e-02, bound:  3.16061229e-01\n",
      "Epoch: 10917 mean train loss:  1.76936928e-02, bound:  3.16061080e-01\n",
      "Epoch: 10918 mean train loss:  1.76884197e-02, bound:  3.16060752e-01\n",
      "Epoch: 10919 mean train loss:  1.76835023e-02, bound:  3.16060543e-01\n",
      "Epoch: 10920 mean train loss:  1.76788531e-02, bound:  3.16060215e-01\n",
      "Epoch: 10921 mean train loss:  1.76747236e-02, bound:  3.16060066e-01\n",
      "Epoch: 10922 mean train loss:  1.76714528e-02, bound:  3.16059679e-01\n",
      "Epoch: 10923 mean train loss:  1.76692754e-02, bound:  3.16059589e-01\n",
      "Epoch: 10924 mean train loss:  1.76684093e-02, bound:  3.16059142e-01\n",
      "Epoch: 10925 mean train loss:  1.76695865e-02, bound:  3.16059083e-01\n",
      "Epoch: 10926 mean train loss:  1.76740419e-02, bound:  3.16058576e-01\n",
      "Epoch: 10927 mean train loss:  1.76832583e-02, bound:  3.16058606e-01\n",
      "Epoch: 10928 mean train loss:  1.76996402e-02, bound:  3.16058040e-01\n",
      "Epoch: 10929 mean train loss:  1.77266616e-02, bound:  3.16058159e-01\n",
      "Epoch: 10930 mean train loss:  1.77685432e-02, bound:  3.16057473e-01\n",
      "Epoch: 10931 mean train loss:  1.78323258e-02, bound:  3.16057771e-01\n",
      "Epoch: 10932 mean train loss:  1.79263707e-02, bound:  3.16056818e-01\n",
      "Epoch: 10933 mean train loss:  1.80628579e-02, bound:  3.16057354e-01\n",
      "Epoch: 10934 mean train loss:  1.82524268e-02, bound:  3.16056132e-01\n",
      "Epoch: 10935 mean train loss:  1.85043029e-02, bound:  3.16057026e-01\n",
      "Epoch: 10936 mean train loss:  1.88093968e-02, bound:  3.16055387e-01\n",
      "Epoch: 10937 mean train loss:  1.91336889e-02, bound:  3.16056669e-01\n",
      "Epoch: 10938 mean train loss:  1.93863213e-02, bound:  3.16054702e-01\n",
      "Epoch: 10939 mean train loss:  1.94491949e-02, bound:  3.16056162e-01\n",
      "Epoch: 10940 mean train loss:  1.92094743e-02, bound:  3.16054165e-01\n",
      "Epoch: 10941 mean train loss:  1.86898075e-02, bound:  3.16055387e-01\n",
      "Epoch: 10942 mean train loss:  1.80753693e-02, bound:  3.16053957e-01\n",
      "Epoch: 10943 mean train loss:  1.76417362e-02, bound:  3.16054314e-01\n",
      "Epoch: 10944 mean train loss:  1.75549742e-02, bound:  3.16053987e-01\n",
      "Epoch: 10945 mean train loss:  1.77639108e-02, bound:  3.16053391e-01\n",
      "Epoch: 10946 mean train loss:  1.80527736e-02, bound:  3.16053987e-01\n",
      "Epoch: 10947 mean train loss:  1.81915890e-02, bound:  3.16052794e-01\n",
      "Epoch: 10948 mean train loss:  1.80844367e-02, bound:  3.16053629e-01\n",
      "Epoch: 10949 mean train loss:  1.78157929e-02, bound:  3.16052556e-01\n",
      "Epoch: 10950 mean train loss:  1.75766610e-02, bound:  3.16052943e-01\n",
      "Epoch: 10951 mean train loss:  1.75071638e-02, bound:  3.16052556e-01\n",
      "Epoch: 10952 mean train loss:  1.76035017e-02, bound:  3.16052109e-01\n",
      "Epoch: 10953 mean train loss:  1.77465733e-02, bound:  3.16052437e-01\n",
      "Epoch: 10954 mean train loss:  1.78086683e-02, bound:  3.16051483e-01\n",
      "Epoch: 10955 mean train loss:  1.77454874e-02, bound:  3.16051960e-01\n",
      "Epoch: 10956 mean train loss:  1.76091623e-02, bound:  3.16051126e-01\n",
      "Epoch: 10957 mean train loss:  1.74953975e-02, bound:  3.16051215e-01\n",
      "Epoch: 10958 mean train loss:  1.74658056e-02, bound:  3.16050887e-01\n",
      "Epoch: 10959 mean train loss:  1.75128244e-02, bound:  3.16050410e-01\n",
      "Epoch: 10960 mean train loss:  1.75795667e-02, bound:  3.16050559e-01\n",
      "Epoch: 10961 mean train loss:  1.76081900e-02, bound:  3.16049784e-01\n",
      "Epoch: 10962 mean train loss:  1.75787918e-02, bound:  3.16050023e-01\n",
      "Epoch: 10963 mean train loss:  1.75125822e-02, bound:  3.16049337e-01\n",
      "Epoch: 10964 mean train loss:  1.74503252e-02, bound:  3.16049367e-01\n",
      "Epoch: 10965 mean train loss:  1.74223986e-02, bound:  3.16049039e-01\n",
      "Epoch: 10966 mean train loss:  1.74320005e-02, bound:  3.16048712e-01\n",
      "Epoch: 10967 mean train loss:  1.74601078e-02, bound:  3.16048712e-01\n",
      "Epoch: 10968 mean train loss:  1.74813140e-02, bound:  3.16048145e-01\n",
      "Epoch: 10969 mean train loss:  1.74795743e-02, bound:  3.16048294e-01\n",
      "Epoch: 10970 mean train loss:  1.74550824e-02, bound:  3.16047698e-01\n",
      "Epoch: 10971 mean train loss:  1.74205601e-02, bound:  3.16047698e-01\n",
      "Epoch: 10972 mean train loss:  1.73910819e-02, bound:  3.16047281e-01\n",
      "Epoch: 10973 mean train loss:  1.73769966e-02, bound:  3.16047072e-01\n",
      "Epoch: 10974 mean train loss:  1.73778590e-02, bound:  3.16046894e-01\n",
      "Epoch: 10975 mean train loss:  1.73868462e-02, bound:  3.16046506e-01\n",
      "Epoch: 10976 mean train loss:  1.73938703e-02, bound:  3.16046476e-01\n",
      "Epoch: 10977 mean train loss:  1.73926819e-02, bound:  3.16045970e-01\n",
      "Epoch: 10978 mean train loss:  1.73820425e-02, bound:  3.16045970e-01\n",
      "Epoch: 10979 mean train loss:  1.73651651e-02, bound:  3.16045523e-01\n",
      "Epoch: 10980 mean train loss:  1.73477959e-02, bound:  3.16045433e-01\n",
      "Epoch: 10981 mean train loss:  1.73340999e-02, bound:  3.16045105e-01\n",
      "Epoch: 10982 mean train loss:  1.73267834e-02, bound:  3.16044867e-01\n",
      "Epoch: 10983 mean train loss:  1.73247065e-02, bound:  3.16044718e-01\n",
      "Epoch: 10984 mean train loss:  1.73248183e-02, bound:  3.16044420e-01\n",
      "Epoch: 10985 mean train loss:  1.73240658e-02, bound:  3.16044301e-01\n",
      "Epoch: 10986 mean train loss:  1.73207950e-02, bound:  3.16043973e-01\n",
      "Epoch: 10987 mean train loss:  1.73137486e-02, bound:  3.16043913e-01\n",
      "Epoch: 10988 mean train loss:  1.73044447e-02, bound:  3.16043556e-01\n",
      "Epoch: 10989 mean train loss:  1.72942113e-02, bound:  3.16043437e-01\n",
      "Epoch: 10990 mean train loss:  1.72845647e-02, bound:  3.16043198e-01\n",
      "Epoch: 10991 mean train loss:  1.72768999e-02, bound:  3.16042989e-01\n",
      "Epoch: 10992 mean train loss:  1.72711778e-02, bound:  3.16042781e-01\n",
      "Epoch: 10993 mean train loss:  1.72671657e-02, bound:  3.16042513e-01\n",
      "Epoch: 10994 mean train loss:  1.72637273e-02, bound:  3.16042364e-01\n",
      "Epoch: 10995 mean train loss:  1.72597338e-02, bound:  3.16042036e-01\n",
      "Epoch: 10996 mean train loss:  1.72550753e-02, bound:  3.16041917e-01\n",
      "Epoch: 10997 mean train loss:  1.72493309e-02, bound:  3.16041589e-01\n",
      "Epoch: 10998 mean train loss:  1.72429103e-02, bound:  3.16041470e-01\n",
      "Epoch: 10999 mean train loss:  1.72358863e-02, bound:  3.16041172e-01\n",
      "Epoch: 11000 mean train loss:  1.72287561e-02, bound:  3.16041023e-01\n",
      "Epoch: 11001 mean train loss:  1.72220208e-02, bound:  3.16040754e-01\n",
      "Epoch: 11002 mean train loss:  1.72158405e-02, bound:  3.16040546e-01\n",
      "Epoch: 11003 mean train loss:  1.72101278e-02, bound:  3.16040337e-01\n",
      "Epoch: 11004 mean train loss:  1.72049180e-02, bound:  3.16040099e-01\n",
      "Epoch: 11005 mean train loss:  1.71999037e-02, bound:  3.16039920e-01\n",
      "Epoch: 11006 mean train loss:  1.71951260e-02, bound:  3.16039652e-01\n",
      "Epoch: 11007 mean train loss:  1.71898212e-02, bound:  3.16039503e-01\n",
      "Epoch: 11008 mean train loss:  1.71846207e-02, bound:  3.16039234e-01\n",
      "Epoch: 11009 mean train loss:  1.71788726e-02, bound:  3.16039056e-01\n",
      "Epoch: 11010 mean train loss:  1.71731710e-02, bound:  3.16038787e-01\n",
      "Epoch: 11011 mean train loss:  1.71671677e-02, bound:  3.16038609e-01\n",
      "Epoch: 11012 mean train loss:  1.71610862e-02, bound:  3.16038311e-01\n",
      "Epoch: 11013 mean train loss:  1.71554387e-02, bound:  3.16038132e-01\n",
      "Epoch: 11014 mean train loss:  1.71493851e-02, bound:  3.16037863e-01\n",
      "Epoch: 11015 mean train loss:  1.71436723e-02, bound:  3.16037685e-01\n",
      "Epoch: 11016 mean train loss:  1.71381291e-02, bound:  3.16037387e-01\n",
      "Epoch: 11017 mean train loss:  1.71323679e-02, bound:  3.16037178e-01\n",
      "Epoch: 11018 mean train loss:  1.71269700e-02, bound:  3.16036969e-01\n",
      "Epoch: 11019 mean train loss:  1.71216652e-02, bound:  3.16036701e-01\n",
      "Epoch: 11020 mean train loss:  1.71162840e-02, bound:  3.16036493e-01\n",
      "Epoch: 11021 mean train loss:  1.71107780e-02, bound:  3.16036254e-01\n",
      "Epoch: 11022 mean train loss:  1.71054062e-02, bound:  3.16036046e-01\n",
      "Epoch: 11023 mean train loss:  1.70998387e-02, bound:  3.16035748e-01\n",
      "Epoch: 11024 mean train loss:  1.70943327e-02, bound:  3.16035539e-01\n",
      "Epoch: 11025 mean train loss:  1.70886479e-02, bound:  3.16035271e-01\n",
      "Epoch: 11026 mean train loss:  1.70830581e-02, bound:  3.16035092e-01\n",
      "Epoch: 11027 mean train loss:  1.70775317e-02, bound:  3.16034824e-01\n",
      "Epoch: 11028 mean train loss:  1.70718972e-02, bound:  3.16034645e-01\n",
      "Epoch: 11029 mean train loss:  1.70661900e-02, bound:  3.16034347e-01\n",
      "Epoch: 11030 mean train loss:  1.70606058e-02, bound:  3.16034108e-01\n",
      "Epoch: 11031 mean train loss:  1.70549210e-02, bound:  3.16033870e-01\n",
      "Epoch: 11032 mean train loss:  1.70493405e-02, bound:  3.16033661e-01\n",
      "Epoch: 11033 mean train loss:  1.70437787e-02, bound:  3.16033423e-01\n",
      "Epoch: 11034 mean train loss:  1.70382056e-02, bound:  3.16033125e-01\n",
      "Epoch: 11035 mean train loss:  1.70326382e-02, bound:  3.16032916e-01\n",
      "Epoch: 11036 mean train loss:  1.70270447e-02, bound:  3.16032678e-01\n",
      "Epoch: 11037 mean train loss:  1.70216076e-02, bound:  3.16032469e-01\n",
      "Epoch: 11038 mean train loss:  1.70160159e-02, bound:  3.16032231e-01\n",
      "Epoch: 11039 mean train loss:  1.70102362e-02, bound:  3.16031992e-01\n",
      "Epoch: 11040 mean train loss:  1.70047581e-02, bound:  3.16031724e-01\n",
      "Epoch: 11041 mean train loss:  1.69992242e-02, bound:  3.16031545e-01\n",
      "Epoch: 11042 mean train loss:  1.69936623e-02, bound:  3.16031247e-01\n",
      "Epoch: 11043 mean train loss:  1.69881471e-02, bound:  3.16031039e-01\n",
      "Epoch: 11044 mean train loss:  1.69825573e-02, bound:  3.16030800e-01\n",
      "Epoch: 11045 mean train loss:  1.69769749e-02, bound:  3.16030562e-01\n",
      "Epoch: 11046 mean train loss:  1.69716235e-02, bound:  3.16030353e-01\n",
      "Epoch: 11047 mean train loss:  1.69662219e-02, bound:  3.16030115e-01\n",
      "Epoch: 11048 mean train loss:  1.69605073e-02, bound:  3.16029847e-01\n",
      "Epoch: 11049 mean train loss:  1.69549063e-02, bound:  3.16029638e-01\n",
      "Epoch: 11050 mean train loss:  1.69494208e-02, bound:  3.16029370e-01\n",
      "Epoch: 11051 mean train loss:  1.69438440e-02, bound:  3.16029161e-01\n",
      "Epoch: 11052 mean train loss:  1.69384126e-02, bound:  3.16028923e-01\n",
      "Epoch: 11053 mean train loss:  1.69329066e-02, bound:  3.16028714e-01\n",
      "Epoch: 11054 mean train loss:  1.69275366e-02, bound:  3.16028416e-01\n",
      "Epoch: 11055 mean train loss:  1.69221722e-02, bound:  3.16028267e-01\n",
      "Epoch: 11056 mean train loss:  1.69166587e-02, bound:  3.16027969e-01\n",
      "Epoch: 11057 mean train loss:  1.69113968e-02, bound:  3.16027790e-01\n",
      "Epoch: 11058 mean train loss:  1.69062242e-02, bound:  3.16027492e-01\n",
      "Epoch: 11059 mean train loss:  1.69012211e-02, bound:  3.16027313e-01\n",
      "Epoch: 11060 mean train loss:  1.68969110e-02, bound:  3.16026986e-01\n",
      "Epoch: 11061 mean train loss:  1.68927889e-02, bound:  3.16026866e-01\n",
      "Epoch: 11062 mean train loss:  1.68891661e-02, bound:  3.16026539e-01\n",
      "Epoch: 11063 mean train loss:  1.68870743e-02, bound:  3.16026419e-01\n",
      "Epoch: 11064 mean train loss:  1.68863665e-02, bound:  3.16026062e-01\n",
      "Epoch: 11065 mean train loss:  1.68885123e-02, bound:  3.16025972e-01\n",
      "Epoch: 11066 mean train loss:  1.68947522e-02, bound:  3.16025555e-01\n",
      "Epoch: 11067 mean train loss:  1.69075783e-02, bound:  3.16025555e-01\n",
      "Epoch: 11068 mean train loss:  1.69303026e-02, bound:  3.16024989e-01\n",
      "Epoch: 11069 mean train loss:  1.69691481e-02, bound:  3.16025138e-01\n",
      "Epoch: 11070 mean train loss:  1.70323178e-02, bound:  3.16024423e-01\n",
      "Epoch: 11071 mean train loss:  1.71331950e-02, bound:  3.16024840e-01\n",
      "Epoch: 11072 mean train loss:  1.72900259e-02, bound:  3.16023856e-01\n",
      "Epoch: 11073 mean train loss:  1.75278634e-02, bound:  3.16024542e-01\n",
      "Epoch: 11074 mean train loss:  1.78696290e-02, bound:  3.16023111e-01\n",
      "Epoch: 11075 mean train loss:  1.83278769e-02, bound:  3.16024244e-01\n",
      "Epoch: 11076 mean train loss:  1.88586712e-02, bound:  3.16022336e-01\n",
      "Epoch: 11077 mean train loss:  1.93366334e-02, bound:  3.16023976e-01\n",
      "Epoch: 11078 mean train loss:  1.95066165e-02, bound:  3.16021651e-01\n",
      "Epoch: 11079 mean train loss:  1.91470459e-02, bound:  3.16023350e-01\n",
      "Epoch: 11080 mean train loss:  1.82680767e-02, bound:  3.16021353e-01\n",
      "Epoch: 11081 mean train loss:  1.73017737e-02, bound:  3.16022336e-01\n",
      "Epoch: 11082 mean train loss:  1.67973395e-02, bound:  3.16021562e-01\n",
      "Epoch: 11083 mean train loss:  1.69557501e-02, bound:  3.16021144e-01\n",
      "Epoch: 11084 mean train loss:  1.74652860e-02, bound:  3.16021770e-01\n",
      "Epoch: 11085 mean train loss:  1.77934002e-02, bound:  3.16020459e-01\n",
      "Epoch: 11086 mean train loss:  1.76365599e-02, bound:  3.16021502e-01\n",
      "Epoch: 11087 mean train loss:  1.71533134e-02, bound:  3.16020370e-01\n",
      "Epoch: 11088 mean train loss:  1.67892519e-02, bound:  3.16020727e-01\n",
      "Epoch: 11089 mean train loss:  1.68145113e-02, bound:  3.16020608e-01\n",
      "Epoch: 11090 mean train loss:  1.70914251e-02, bound:  3.16019922e-01\n",
      "Epoch: 11091 mean train loss:  1.72748193e-02, bound:  3.16020608e-01\n",
      "Epoch: 11092 mean train loss:  1.71720125e-02, bound:  3.16019475e-01\n",
      "Epoch: 11093 mean train loss:  1.69023704e-02, bound:  3.16020042e-01\n",
      "Epoch: 11094 mean train loss:  1.67275574e-02, bound:  3.16019475e-01\n",
      "Epoch: 11095 mean train loss:  1.67716332e-02, bound:  3.16019148e-01\n",
      "Epoch: 11096 mean train loss:  1.69277936e-02, bound:  3.16019356e-01\n",
      "Epoch: 11097 mean train loss:  1.70068797e-02, bound:  3.16018432e-01\n",
      "Epoch: 11098 mean train loss:  1.69288069e-02, bound:  3.16018850e-01\n",
      "Epoch: 11099 mean train loss:  1.67767573e-02, bound:  3.16018105e-01\n",
      "Epoch: 11100 mean train loss:  1.66875049e-02, bound:  3.16018075e-01\n",
      "Epoch: 11101 mean train loss:  1.67142898e-02, bound:  3.16017926e-01\n",
      "Epoch: 11102 mean train loss:  1.67964734e-02, bound:  3.16017330e-01\n",
      "Epoch: 11103 mean train loss:  1.68379303e-02, bound:  3.16017628e-01\n",
      "Epoch: 11104 mean train loss:  1.67979300e-02, bound:  3.16016883e-01\n",
      "Epoch: 11105 mean train loss:  1.67146605e-02, bound:  3.16017061e-01\n",
      "Epoch: 11106 mean train loss:  1.66560672e-02, bound:  3.16016674e-01\n",
      "Epoch: 11107 mean train loss:  1.66561641e-02, bound:  3.16016406e-01\n",
      "Epoch: 11108 mean train loss:  1.66944768e-02, bound:  3.16016406e-01\n",
      "Epoch: 11109 mean train loss:  1.67241301e-02, bound:  3.16015810e-01\n",
      "Epoch: 11110 mean train loss:  1.67156812e-02, bound:  3.16015989e-01\n",
      "Epoch: 11111 mean train loss:  1.66753270e-02, bound:  3.16015393e-01\n",
      "Epoch: 11112 mean train loss:  1.66326724e-02, bound:  3.16015393e-01\n",
      "Epoch: 11113 mean train loss:  1.66133028e-02, bound:  3.16015095e-01\n",
      "Epoch: 11114 mean train loss:  1.66207105e-02, bound:  3.16014767e-01\n",
      "Epoch: 11115 mean train loss:  1.66378561e-02, bound:  3.16014737e-01\n",
      "Epoch: 11116 mean train loss:  1.66449435e-02, bound:  3.16014230e-01\n",
      "Epoch: 11117 mean train loss:  1.66335199e-02, bound:  3.16014260e-01\n",
      "Epoch: 11118 mean train loss:  1.66094787e-02, bound:  3.16013813e-01\n",
      "Epoch: 11119 mean train loss:  1.65871438e-02, bound:  3.16013753e-01\n",
      "Epoch: 11120 mean train loss:  1.65761672e-02, bound:  3.16013485e-01\n",
      "Epoch: 11121 mean train loss:  1.65772922e-02, bound:  3.16013247e-01\n",
      "Epoch: 11122 mean train loss:  1.65828858e-02, bound:  3.16013217e-01\n",
      "Epoch: 11123 mean train loss:  1.65846255e-02, bound:  3.16012800e-01\n",
      "Epoch: 11124 mean train loss:  1.65775474e-02, bound:  3.16012800e-01\n",
      "Epoch: 11125 mean train loss:  1.65638998e-02, bound:  3.16012472e-01\n",
      "Epoch: 11126 mean train loss:  1.65495276e-02, bound:  3.16012383e-01\n",
      "Epoch: 11127 mean train loss:  1.65395364e-02, bound:  3.16012144e-01\n",
      "Epoch: 11128 mean train loss:  1.65353995e-02, bound:  3.16011935e-01\n",
      "Epoch: 11129 mean train loss:  1.65349711e-02, bound:  3.16011876e-01\n",
      "Epoch: 11130 mean train loss:  1.65342763e-02, bound:  3.16011578e-01\n",
      "Epoch: 11131 mean train loss:  1.65304728e-02, bound:  3.16011488e-01\n",
      "Epoch: 11132 mean train loss:  1.65229850e-02, bound:  3.16011190e-01\n",
      "Epoch: 11133 mean train loss:  1.65133886e-02, bound:  3.16011131e-01\n",
      "Epoch: 11134 mean train loss:  1.65043827e-02, bound:  3.16010863e-01\n",
      "Epoch: 11135 mean train loss:  1.64977219e-02, bound:  3.16010684e-01\n",
      "Epoch: 11136 mean train loss:  1.64932422e-02, bound:  3.16010505e-01\n",
      "Epoch: 11137 mean train loss:  1.64901707e-02, bound:  3.16010296e-01\n",
      "Epoch: 11138 mean train loss:  1.64868049e-02, bound:  3.16010177e-01\n",
      "Epoch: 11139 mean train loss:  1.64820906e-02, bound:  3.16009909e-01\n",
      "Epoch: 11140 mean train loss:  1.64761506e-02, bound:  3.16009790e-01\n",
      "Epoch: 11141 mean train loss:  1.64692998e-02, bound:  3.16009521e-01\n",
      "Epoch: 11142 mean train loss:  1.64623056e-02, bound:  3.16009372e-01\n",
      "Epoch: 11143 mean train loss:  1.64561104e-02, bound:  3.16009164e-01\n",
      "Epoch: 11144 mean train loss:  1.64509322e-02, bound:  3.16008985e-01\n",
      "Epoch: 11145 mean train loss:  1.64464414e-02, bound:  3.16008836e-01\n",
      "Epoch: 11146 mean train loss:  1.64419860e-02, bound:  3.16008598e-01\n",
      "Epoch: 11147 mean train loss:  1.64376218e-02, bound:  3.16008478e-01\n",
      "Epoch: 11148 mean train loss:  1.64324231e-02, bound:  3.16008180e-01\n",
      "Epoch: 11149 mean train loss:  1.64267272e-02, bound:  3.16008061e-01\n",
      "Epoch: 11150 mean train loss:  1.64207630e-02, bound:  3.16007823e-01\n",
      "Epoch: 11151 mean train loss:  1.64148491e-02, bound:  3.16007644e-01\n",
      "Epoch: 11152 mean train loss:  1.64092183e-02, bound:  3.16007435e-01\n",
      "Epoch: 11153 mean train loss:  1.64039694e-02, bound:  3.16007197e-01\n",
      "Epoch: 11154 mean train loss:  1.63990725e-02, bound:  3.16007048e-01\n",
      "Epoch: 11155 mean train loss:  1.63941905e-02, bound:  3.16006839e-01\n",
      "Epoch: 11156 mean train loss:  1.63890794e-02, bound:  3.16006631e-01\n",
      "Epoch: 11157 mean train loss:  1.63842551e-02, bound:  3.16006392e-01\n",
      "Epoch: 11158 mean train loss:  1.63789075e-02, bound:  3.16006184e-01\n",
      "Epoch: 11159 mean train loss:  1.63734723e-02, bound:  3.16005975e-01\n",
      "Epoch: 11160 mean train loss:  1.63678788e-02, bound:  3.16005796e-01\n",
      "Epoch: 11161 mean train loss:  1.63626410e-02, bound:  3.16005558e-01\n",
      "Epoch: 11162 mean train loss:  1.63571108e-02, bound:  3.16005349e-01\n",
      "Epoch: 11163 mean train loss:  1.63519848e-02, bound:  3.16005141e-01\n",
      "Epoch: 11164 mean train loss:  1.63468253e-02, bound:  3.16004932e-01\n",
      "Epoch: 11165 mean train loss:  1.63416807e-02, bound:  3.16004753e-01\n",
      "Epoch: 11166 mean train loss:  1.63366199e-02, bound:  3.16004515e-01\n",
      "Epoch: 11167 mean train loss:  1.63315013e-02, bound:  3.16004306e-01\n",
      "Epoch: 11168 mean train loss:  1.63263120e-02, bound:  3.16004097e-01\n",
      "Epoch: 11169 mean train loss:  1.63213164e-02, bound:  3.16003889e-01\n",
      "Epoch: 11170 mean train loss:  1.63159892e-02, bound:  3.16003650e-01\n",
      "Epoch: 11171 mean train loss:  1.63106304e-02, bound:  3.16003442e-01\n",
      "Epoch: 11172 mean train loss:  1.63052548e-02, bound:  3.16003233e-01\n",
      "Epoch: 11173 mean train loss:  1.62999965e-02, bound:  3.16002995e-01\n",
      "Epoch: 11174 mean train loss:  1.62947699e-02, bound:  3.16002786e-01\n",
      "Epoch: 11175 mean train loss:  1.62895247e-02, bound:  3.16002578e-01\n",
      "Epoch: 11176 mean train loss:  1.62844397e-02, bound:  3.16002369e-01\n",
      "Epoch: 11177 mean train loss:  1.62792914e-02, bound:  3.16002131e-01\n",
      "Epoch: 11178 mean train loss:  1.62742082e-02, bound:  3.16001922e-01\n",
      "Epoch: 11179 mean train loss:  1.62688866e-02, bound:  3.16001713e-01\n",
      "Epoch: 11180 mean train loss:  1.62638910e-02, bound:  3.16001505e-01\n",
      "Epoch: 11181 mean train loss:  1.62585806e-02, bound:  3.16001266e-01\n",
      "Epoch: 11182 mean train loss:  1.62533950e-02, bound:  3.16001117e-01\n",
      "Epoch: 11183 mean train loss:  1.62481554e-02, bound:  3.16000909e-01\n",
      "Epoch: 11184 mean train loss:  1.62428729e-02, bound:  3.16000700e-01\n",
      "Epoch: 11185 mean train loss:  1.62376128e-02, bound:  3.16000462e-01\n",
      "Epoch: 11186 mean train loss:  1.62324533e-02, bound:  3.16000253e-01\n",
      "Epoch: 11187 mean train loss:  1.62271466e-02, bound:  3.16000044e-01\n",
      "Epoch: 11188 mean train loss:  1.62220597e-02, bound:  3.15999836e-01\n",
      "Epoch: 11189 mean train loss:  1.62168648e-02, bound:  3.15999597e-01\n",
      "Epoch: 11190 mean train loss:  1.62115246e-02, bound:  3.15999389e-01\n",
      "Epoch: 11191 mean train loss:  1.62063353e-02, bound:  3.15999180e-01\n",
      "Epoch: 11192 mean train loss:  1.62011795e-02, bound:  3.15998971e-01\n",
      "Epoch: 11193 mean train loss:  1.61959603e-02, bound:  3.15998793e-01\n",
      "Epoch: 11194 mean train loss:  1.61907729e-02, bound:  3.15998554e-01\n",
      "Epoch: 11195 mean train loss:  1.61855649e-02, bound:  3.15998375e-01\n",
      "Epoch: 11196 mean train loss:  1.61804967e-02, bound:  3.15998167e-01\n",
      "Epoch: 11197 mean train loss:  1.61752403e-02, bound:  3.15997958e-01\n",
      "Epoch: 11198 mean train loss:  1.61699709e-02, bound:  3.15997720e-01\n",
      "Epoch: 11199 mean train loss:  1.61647834e-02, bound:  3.15997511e-01\n",
      "Epoch: 11200 mean train loss:  1.61596108e-02, bound:  3.15997303e-01\n",
      "Epoch: 11201 mean train loss:  1.61544625e-02, bound:  3.15997094e-01\n",
      "Epoch: 11202 mean train loss:  1.61492061e-02, bound:  3.15996885e-01\n",
      "Epoch: 11203 mean train loss:  1.61438901e-02, bound:  3.15996677e-01\n",
      "Epoch: 11204 mean train loss:  1.61387846e-02, bound:  3.15996498e-01\n",
      "Epoch: 11205 mean train loss:  1.61336474e-02, bound:  3.15996230e-01\n",
      "Epoch: 11206 mean train loss:  1.61284134e-02, bound:  3.15996021e-01\n",
      "Epoch: 11207 mean train loss:  1.61232110e-02, bound:  3.15995842e-01\n",
      "Epoch: 11208 mean train loss:  1.61180701e-02, bound:  3.15995634e-01\n",
      "Epoch: 11209 mean train loss:  1.61128268e-02, bound:  3.15995425e-01\n",
      "Epoch: 11210 mean train loss:  1.61075592e-02, bound:  3.15995187e-01\n",
      "Epoch: 11211 mean train loss:  1.61023680e-02, bound:  3.15994978e-01\n",
      "Epoch: 11212 mean train loss:  1.60972420e-02, bound:  3.15994799e-01\n",
      "Epoch: 11213 mean train loss:  1.60920322e-02, bound:  3.15994591e-01\n",
      "Epoch: 11214 mean train loss:  1.60869323e-02, bound:  3.15994352e-01\n",
      "Epoch: 11215 mean train loss:  1.60816256e-02, bound:  3.15994173e-01\n",
      "Epoch: 11216 mean train loss:  1.60764307e-02, bound:  3.15993935e-01\n",
      "Epoch: 11217 mean train loss:  1.60713661e-02, bound:  3.15993756e-01\n",
      "Epoch: 11218 mean train loss:  1.60661209e-02, bound:  3.15993518e-01\n",
      "Epoch: 11219 mean train loss:  1.60608385e-02, bound:  3.15993309e-01\n",
      "Epoch: 11220 mean train loss:  1.60557963e-02, bound:  3.15993100e-01\n",
      "Epoch: 11221 mean train loss:  1.60506219e-02, bound:  3.15992892e-01\n",
      "Epoch: 11222 mean train loss:  1.60454269e-02, bound:  3.15992653e-01\n",
      "Epoch: 11223 mean train loss:  1.60402227e-02, bound:  3.15992445e-01\n",
      "Epoch: 11224 mean train loss:  1.60351805e-02, bound:  3.15992236e-01\n",
      "Epoch: 11225 mean train loss:  1.60298888e-02, bound:  3.15992057e-01\n",
      "Epoch: 11226 mean train loss:  1.60249937e-02, bound:  3.15991819e-01\n",
      "Epoch: 11227 mean train loss:  1.60198677e-02, bound:  3.15991580e-01\n",
      "Epoch: 11228 mean train loss:  1.60149969e-02, bound:  3.15991431e-01\n",
      "Epoch: 11229 mean train loss:  1.60104595e-02, bound:  3.15991163e-01\n",
      "Epoch: 11230 mean train loss:  1.60056017e-02, bound:  3.15991014e-01\n",
      "Epoch: 11231 mean train loss:  1.60013437e-02, bound:  3.15990716e-01\n",
      "Epoch: 11232 mean train loss:  1.59974247e-02, bound:  3.15990627e-01\n",
      "Epoch: 11233 mean train loss:  1.59940869e-02, bound:  3.15990329e-01\n",
      "Epoch: 11234 mean train loss:  1.59917381e-02, bound:  3.15990180e-01\n",
      "Epoch: 11235 mean train loss:  1.59906056e-02, bound:  3.15989822e-01\n",
      "Epoch: 11236 mean train loss:  1.59917306e-02, bound:  3.15989792e-01\n",
      "Epoch: 11237 mean train loss:  1.59958862e-02, bound:  3.15989375e-01\n",
      "Epoch: 11238 mean train loss:  1.60052869e-02, bound:  3.15989405e-01\n",
      "Epoch: 11239 mean train loss:  1.60224345e-02, bound:  3.15988928e-01\n",
      "Epoch: 11240 mean train loss:  1.60522591e-02, bound:  3.15989077e-01\n",
      "Epoch: 11241 mean train loss:  1.61016211e-02, bound:  3.15988392e-01\n",
      "Epoch: 11242 mean train loss:  1.61810778e-02, bound:  3.15988749e-01\n",
      "Epoch: 11243 mean train loss:  1.63060185e-02, bound:  3.15987855e-01\n",
      "Epoch: 11244 mean train loss:  1.64985731e-02, bound:  3.15988481e-01\n",
      "Epoch: 11245 mean train loss:  1.67823974e-02, bound:  3.15987200e-01\n",
      "Epoch: 11246 mean train loss:  1.71799380e-02, bound:  3.15988272e-01\n",
      "Epoch: 11247 mean train loss:  1.76771060e-02, bound:  3.15986514e-01\n",
      "Epoch: 11248 mean train loss:  1.82022378e-02, bound:  3.15988034e-01\n",
      "Epoch: 11249 mean train loss:  1.85566507e-02, bound:  3.15985858e-01\n",
      "Epoch: 11250 mean train loss:  1.84970051e-02, bound:  3.15987617e-01\n",
      "Epoch: 11251 mean train loss:  1.78645607e-02, bound:  3.15985441e-01\n",
      "Epoch: 11252 mean train loss:  1.68844797e-02, bound:  3.15986723e-01\n",
      "Epoch: 11253 mean train loss:  1.60884243e-02, bound:  3.15985560e-01\n",
      "Epoch: 11254 mean train loss:  1.59082469e-02, bound:  3.15985560e-01\n",
      "Epoch: 11255 mean train loss:  1.62907094e-02, bound:  3.15985888e-01\n",
      "Epoch: 11256 mean train loss:  1.67708732e-02, bound:  3.15984756e-01\n",
      "Epoch: 11257 mean train loss:  1.68794896e-02, bound:  3.15985858e-01\n",
      "Epoch: 11258 mean train loss:  1.65242795e-02, bound:  3.15984577e-01\n",
      "Epoch: 11259 mean train loss:  1.60495043e-02, bound:  3.15985203e-01\n",
      "Epoch: 11260 mean train loss:  1.58642568e-02, bound:  3.15984845e-01\n",
      "Epoch: 11261 mean train loss:  1.60452258e-02, bound:  3.15984428e-01\n",
      "Epoch: 11262 mean train loss:  1.63145419e-02, bound:  3.15984935e-01\n",
      "Epoch: 11263 mean train loss:  1.63640603e-02, bound:  3.15983891e-01\n",
      "Epoch: 11264 mean train loss:  1.61542576e-02, bound:  3.15984517e-01\n",
      "Epoch: 11265 mean train loss:  1.59055609e-02, bound:  3.15983772e-01\n",
      "Epoch: 11266 mean train loss:  1.58383772e-02, bound:  3.15983772e-01\n",
      "Epoch: 11267 mean train loss:  1.59582235e-02, bound:  3.15983772e-01\n",
      "Epoch: 11268 mean train loss:  1.60933007e-02, bound:  3.15982997e-01\n",
      "Epoch: 11269 mean train loss:  1.60933323e-02, bound:  3.15983474e-01\n",
      "Epoch: 11270 mean train loss:  1.59645751e-02, bound:  3.15982670e-01\n",
      "Epoch: 11271 mean train loss:  1.58335995e-02, bound:  3.15982819e-01\n",
      "Epoch: 11272 mean train loss:  1.58052482e-02, bound:  3.15982521e-01\n",
      "Epoch: 11273 mean train loss:  1.58714224e-02, bound:  3.15982044e-01\n",
      "Epoch: 11274 mean train loss:  1.59418024e-02, bound:  3.15982312e-01\n",
      "Epoch: 11275 mean train loss:  1.59414839e-02, bound:  3.15981567e-01\n",
      "Epoch: 11276 mean train loss:  1.58724301e-02, bound:  3.15981865e-01\n",
      "Epoch: 11277 mean train loss:  1.57964434e-02, bound:  3.15981328e-01\n",
      "Epoch: 11278 mean train loss:  1.57685112e-02, bound:  3.15981209e-01\n",
      "Epoch: 11279 mean train loss:  1.57930646e-02, bound:  3.15981150e-01\n",
      "Epoch: 11280 mean train loss:  1.58317983e-02, bound:  3.15980613e-01\n",
      "Epoch: 11281 mean train loss:  1.58430990e-02, bound:  3.15980792e-01\n",
      "Epoch: 11282 mean train loss:  1.58159677e-02, bound:  3.15980226e-01\n",
      "Epoch: 11283 mean train loss:  1.57716479e-02, bound:  3.15980226e-01\n",
      "Epoch: 11284 mean train loss:  1.57400575e-02, bound:  3.15979898e-01\n",
      "Epoch: 11285 mean train loss:  1.57364160e-02, bound:  3.15979689e-01\n",
      "Epoch: 11286 mean train loss:  1.57519877e-02, bound:  3.15979570e-01\n",
      "Epoch: 11287 mean train loss:  1.57663822e-02, bound:  3.15979153e-01\n",
      "Epoch: 11288 mean train loss:  1.57638788e-02, bound:  3.15979183e-01\n",
      "Epoch: 11289 mean train loss:  1.57446712e-02, bound:  3.15978795e-01\n",
      "Epoch: 11290 mean train loss:  1.57202221e-02, bound:  3.15978706e-01\n",
      "Epoch: 11291 mean train loss:  1.57034229e-02, bound:  3.15978467e-01\n",
      "Epoch: 11292 mean train loss:  1.56997666e-02, bound:  3.15978259e-01\n",
      "Epoch: 11293 mean train loss:  1.57046020e-02, bound:  3.15978169e-01\n",
      "Epoch: 11294 mean train loss:  1.57093033e-02, bound:  3.15977842e-01\n",
      "Epoch: 11295 mean train loss:  1.57068018e-02, bound:  3.15977842e-01\n",
      "Epoch: 11296 mean train loss:  1.56964213e-02, bound:  3.15977514e-01\n",
      "Epoch: 11297 mean train loss:  1.56823304e-02, bound:  3.15977484e-01\n",
      "Epoch: 11298 mean train loss:  1.56701971e-02, bound:  3.15977246e-01\n",
      "Epoch: 11299 mean train loss:  1.56633593e-02, bound:  3.15977097e-01\n",
      "Epoch: 11300 mean train loss:  1.56617668e-02, bound:  3.15976977e-01\n",
      "Epoch: 11301 mean train loss:  1.56615730e-02, bound:  3.15976739e-01\n",
      "Epoch: 11302 mean train loss:  1.56597737e-02, bound:  3.15976679e-01\n",
      "Epoch: 11303 mean train loss:  1.56545825e-02, bound:  3.15976411e-01\n",
      "Epoch: 11304 mean train loss:  1.56464707e-02, bound:  3.15976381e-01\n",
      "Epoch: 11305 mean train loss:  1.56375840e-02, bound:  3.15976083e-01\n",
      "Epoch: 11306 mean train loss:  1.56299043e-02, bound:  3.15975964e-01\n",
      "Epoch: 11307 mean train loss:  1.56245949e-02, bound:  3.15975815e-01\n",
      "Epoch: 11308 mean train loss:  1.56214526e-02, bound:  3.15975606e-01\n",
      "Epoch: 11309 mean train loss:  1.56185944e-02, bound:  3.15975517e-01\n",
      "Epoch: 11310 mean train loss:  1.56152109e-02, bound:  3.15975249e-01\n",
      "Epoch: 11311 mean train loss:  1.56105161e-02, bound:  3.15975189e-01\n",
      "Epoch: 11312 mean train loss:  1.56043498e-02, bound:  3.15974951e-01\n",
      "Epoch: 11313 mean train loss:  1.55977001e-02, bound:  3.15974832e-01\n",
      "Epoch: 11314 mean train loss:  1.55913243e-02, bound:  3.15974623e-01\n",
      "Epoch: 11315 mean train loss:  1.55860148e-02, bound:  3.15974444e-01\n",
      "Epoch: 11316 mean train loss:  1.55815333e-02, bound:  3.15974325e-01\n",
      "Epoch: 11317 mean train loss:  1.55773908e-02, bound:  3.15974116e-01\n",
      "Epoch: 11318 mean train loss:  1.55732436e-02, bound:  3.15973997e-01\n",
      "Epoch: 11319 mean train loss:  1.55691486e-02, bound:  3.15973759e-01\n",
      "Epoch: 11320 mean train loss:  1.55639900e-02, bound:  3.15973639e-01\n",
      "Epoch: 11321 mean train loss:  1.55586740e-02, bound:  3.15973431e-01\n",
      "Epoch: 11322 mean train loss:  1.55530525e-02, bound:  3.15973282e-01\n",
      "Epoch: 11323 mean train loss:  1.55473426e-02, bound:  3.15973014e-01\n",
      "Epoch: 11324 mean train loss:  1.55423172e-02, bound:  3.15972894e-01\n",
      "Epoch: 11325 mean train loss:  1.55376559e-02, bound:  3.15972686e-01\n",
      "Epoch: 11326 mean train loss:  1.55331884e-02, bound:  3.15972477e-01\n",
      "Epoch: 11327 mean train loss:  1.55289192e-02, bound:  3.15972358e-01\n",
      "Epoch: 11328 mean train loss:  1.55240810e-02, bound:  3.15972120e-01\n",
      "Epoch: 11329 mean train loss:  1.55192344e-02, bound:  3.15972000e-01\n",
      "Epoch: 11330 mean train loss:  1.55143142e-02, bound:  3.15971762e-01\n",
      "Epoch: 11331 mean train loss:  1.55092049e-02, bound:  3.15971583e-01\n",
      "Epoch: 11332 mean train loss:  1.55039895e-02, bound:  3.15971345e-01\n",
      "Epoch: 11333 mean train loss:  1.54990228e-02, bound:  3.15971196e-01\n",
      "Epoch: 11334 mean train loss:  1.54941306e-02, bound:  3.15971017e-01\n",
      "Epoch: 11335 mean train loss:  1.54894190e-02, bound:  3.15970778e-01\n",
      "Epoch: 11336 mean train loss:  1.54848034e-02, bound:  3.15970629e-01\n",
      "Epoch: 11337 mean train loss:  1.54800816e-02, bound:  3.15970421e-01\n",
      "Epoch: 11338 mean train loss:  1.54753607e-02, bound:  3.15970242e-01\n",
      "Epoch: 11339 mean train loss:  1.54705746e-02, bound:  3.15970033e-01\n",
      "Epoch: 11340 mean train loss:  1.54657019e-02, bound:  3.15969884e-01\n",
      "Epoch: 11341 mean train loss:  1.54609112e-02, bound:  3.15969616e-01\n",
      "Epoch: 11342 mean train loss:  1.54557433e-02, bound:  3.15969467e-01\n",
      "Epoch: 11343 mean train loss:  1.54507719e-02, bound:  3.15969259e-01\n",
      "Epoch: 11344 mean train loss:  1.54458741e-02, bound:  3.15969050e-01\n",
      "Epoch: 11345 mean train loss:  1.54411057e-02, bound:  3.15968871e-01\n",
      "Epoch: 11346 mean train loss:  1.54364640e-02, bound:  3.15968692e-01\n",
      "Epoch: 11347 mean train loss:  1.54314963e-02, bound:  3.15968484e-01\n",
      "Epoch: 11348 mean train loss:  1.54265594e-02, bound:  3.15968275e-01\n",
      "Epoch: 11349 mean train loss:  1.54218813e-02, bound:  3.15968126e-01\n",
      "Epoch: 11350 mean train loss:  1.54171549e-02, bound:  3.15967888e-01\n",
      "Epoch: 11351 mean train loss:  1.54123232e-02, bound:  3.15967709e-01\n",
      "Epoch: 11352 mean train loss:  1.54075380e-02, bound:  3.15967500e-01\n",
      "Epoch: 11353 mean train loss:  1.54026840e-02, bound:  3.15967351e-01\n",
      "Epoch: 11354 mean train loss:  1.53980386e-02, bound:  3.15967143e-01\n",
      "Epoch: 11355 mean train loss:  1.53929815e-02, bound:  3.15966964e-01\n",
      "Epoch: 11356 mean train loss:  1.53881172e-02, bound:  3.15966725e-01\n",
      "Epoch: 11357 mean train loss:  1.53832678e-02, bound:  3.15966576e-01\n",
      "Epoch: 11358 mean train loss:  1.53785283e-02, bound:  3.15966368e-01\n",
      "Epoch: 11359 mean train loss:  1.53735932e-02, bound:  3.15966159e-01\n",
      "Epoch: 11360 mean train loss:  1.53688379e-02, bound:  3.15965980e-01\n",
      "Epoch: 11361 mean train loss:  1.53640574e-02, bound:  3.15965801e-01\n",
      "Epoch: 11362 mean train loss:  1.53591484e-02, bound:  3.15965623e-01\n",
      "Epoch: 11363 mean train loss:  1.53544126e-02, bound:  3.15965414e-01\n",
      "Epoch: 11364 mean train loss:  1.53496396e-02, bound:  3.15965205e-01\n",
      "Epoch: 11365 mean train loss:  1.53447967e-02, bound:  3.15965056e-01\n",
      "Epoch: 11366 mean train loss:  1.53400451e-02, bound:  3.15964848e-01\n",
      "Epoch: 11367 mean train loss:  1.53352423e-02, bound:  3.15964639e-01\n",
      "Epoch: 11368 mean train loss:  1.53304311e-02, bound:  3.15964431e-01\n",
      "Epoch: 11369 mean train loss:  1.53256906e-02, bound:  3.15964252e-01\n",
      "Epoch: 11370 mean train loss:  1.53208179e-02, bound:  3.15964103e-01\n",
      "Epoch: 11371 mean train loss:  1.53159825e-02, bound:  3.15963864e-01\n",
      "Epoch: 11372 mean train loss:  1.53111657e-02, bound:  3.15963715e-01\n",
      "Epoch: 11373 mean train loss:  1.53064197e-02, bound:  3.15963507e-01\n",
      "Epoch: 11374 mean train loss:  1.53015526e-02, bound:  3.15963328e-01\n",
      "Epoch: 11375 mean train loss:  1.52967246e-02, bound:  3.15963089e-01\n",
      "Epoch: 11376 mean train loss:  1.52920587e-02, bound:  3.15962940e-01\n",
      "Epoch: 11377 mean train loss:  1.52873341e-02, bound:  3.15962702e-01\n",
      "Epoch: 11378 mean train loss:  1.52823795e-02, bound:  3.15962553e-01\n",
      "Epoch: 11379 mean train loss:  1.52776614e-02, bound:  3.15962315e-01\n",
      "Epoch: 11380 mean train loss:  1.52727365e-02, bound:  3.15962195e-01\n",
      "Epoch: 11381 mean train loss:  1.52680222e-02, bound:  3.15961957e-01\n",
      "Epoch: 11382 mean train loss:  1.52631206e-02, bound:  3.15961778e-01\n",
      "Epoch: 11383 mean train loss:  1.52582955e-02, bound:  3.15961570e-01\n",
      "Epoch: 11384 mean train loss:  1.52534395e-02, bound:  3.15961391e-01\n",
      "Epoch: 11385 mean train loss:  1.52486293e-02, bound:  3.15961182e-01\n",
      "Epoch: 11386 mean train loss:  1.52438190e-02, bound:  3.15961003e-01\n",
      "Epoch: 11387 mean train loss:  1.52389407e-02, bound:  3.15960795e-01\n",
      "Epoch: 11388 mean train loss:  1.52342739e-02, bound:  3.15960646e-01\n",
      "Epoch: 11389 mean train loss:  1.52295511e-02, bound:  3.15960437e-01\n",
      "Epoch: 11390 mean train loss:  1.52245900e-02, bound:  3.15960228e-01\n",
      "Epoch: 11391 mean train loss:  1.52199836e-02, bound:  3.15960020e-01\n",
      "Epoch: 11392 mean train loss:  1.52153419e-02, bound:  3.15959871e-01\n",
      "Epoch: 11393 mean train loss:  1.52105736e-02, bound:  3.15959632e-01\n",
      "Epoch: 11394 mean train loss:  1.52058443e-02, bound:  3.15959454e-01\n",
      "Epoch: 11395 mean train loss:  1.52012417e-02, bound:  3.15959245e-01\n",
      "Epoch: 11396 mean train loss:  1.51967201e-02, bound:  3.15959096e-01\n",
      "Epoch: 11397 mean train loss:  1.51923625e-02, bound:  3.15958858e-01\n",
      "Epoch: 11398 mean train loss:  1.51881492e-02, bound:  3.15958709e-01\n",
      "Epoch: 11399 mean train loss:  1.51841911e-02, bound:  3.15958470e-01\n",
      "Epoch: 11400 mean train loss:  1.51807675e-02, bound:  3.15958321e-01\n",
      "Epoch: 11401 mean train loss:  1.51776616e-02, bound:  3.15958053e-01\n",
      "Epoch: 11402 mean train loss:  1.51756266e-02, bound:  3.15957993e-01\n",
      "Epoch: 11403 mean train loss:  1.51746925e-02, bound:  3.15957636e-01\n",
      "Epoch: 11404 mean train loss:  1.51757970e-02, bound:  3.15957606e-01\n",
      "Epoch: 11405 mean train loss:  1.51800225e-02, bound:  3.15957218e-01\n",
      "Epoch: 11406 mean train loss:  1.51885739e-02, bound:  3.15957248e-01\n",
      "Epoch: 11407 mean train loss:  1.52045004e-02, bound:  3.15956801e-01\n",
      "Epoch: 11408 mean train loss:  1.52310841e-02, bound:  3.15956920e-01\n",
      "Epoch: 11409 mean train loss:  1.52746877e-02, bound:  3.15956324e-01\n",
      "Epoch: 11410 mean train loss:  1.53432917e-02, bound:  3.15956622e-01\n",
      "Epoch: 11411 mean train loss:  1.54499374e-02, bound:  3.15955818e-01\n",
      "Epoch: 11412 mean train loss:  1.56118963e-02, bound:  3.15956384e-01\n",
      "Epoch: 11413 mean train loss:  1.58489887e-02, bound:  3.15955281e-01\n",
      "Epoch: 11414 mean train loss:  1.61817502e-02, bound:  3.15956175e-01\n",
      "Epoch: 11415 mean train loss:  1.66062228e-02, bound:  3.15954596e-01\n",
      "Epoch: 11416 mean train loss:  1.70790087e-02, bound:  3.15955967e-01\n",
      "Epoch: 11417 mean train loss:  1.74568649e-02, bound:  3.15953970e-01\n",
      "Epoch: 11418 mean train loss:  1.75415631e-02, bound:  3.15955609e-01\n",
      "Epoch: 11419 mean train loss:  1.71442963e-02, bound:  3.15953583e-01\n",
      "Epoch: 11420 mean train loss:  1.63413249e-02, bound:  3.15954864e-01\n",
      "Epoch: 11421 mean train loss:  1.55059826e-02, bound:  3.15953612e-01\n",
      "Epoch: 11422 mean train loss:  1.50903985e-02, bound:  3.15953851e-01\n",
      "Epoch: 11423 mean train loss:  1.52363861e-02, bound:  3.15953851e-01\n",
      "Epoch: 11424 mean train loss:  1.56761575e-02, bound:  3.15953046e-01\n",
      "Epoch: 11425 mean train loss:  1.59734506e-02, bound:  3.15953940e-01\n",
      "Epoch: 11426 mean train loss:  1.58641934e-02, bound:  3.15952718e-01\n",
      "Epoch: 11427 mean train loss:  1.54593633e-02, bound:  3.15953523e-01\n",
      "Epoch: 11428 mean train loss:  1.51132466e-02, bound:  3.15952867e-01\n",
      "Epoch: 11429 mean train loss:  1.50816627e-02, bound:  3.15952778e-01\n",
      "Epoch: 11430 mean train loss:  1.52995391e-02, bound:  3.15953046e-01\n",
      "Epoch: 11431 mean train loss:  1.54942079e-02, bound:  3.15952182e-01\n",
      "Epoch: 11432 mean train loss:  1.54665466e-02, bound:  3.15952837e-01\n",
      "Epoch: 11433 mean train loss:  1.52545730e-02, bound:  3.15951943e-01\n",
      "Epoch: 11434 mean train loss:  1.50616830e-02, bound:  3.15952212e-01\n",
      "Epoch: 11435 mean train loss:  1.50368493e-02, bound:  3.15951943e-01\n",
      "Epoch: 11436 mean train loss:  1.51506402e-02, bound:  3.15951496e-01\n",
      "Epoch: 11437 mean train loss:  1.52576389e-02, bound:  3.15951765e-01\n",
      "Epoch: 11438 mean train loss:  1.52486917e-02, bound:  3.15950960e-01\n",
      "Epoch: 11439 mean train loss:  1.51378615e-02, bound:  3.15951318e-01\n",
      "Epoch: 11440 mean train loss:  1.50259705e-02, bound:  3.15950751e-01\n",
      "Epoch: 11441 mean train loss:  1.49950087e-02, bound:  3.15950632e-01\n",
      "Epoch: 11442 mean train loss:  1.50438873e-02, bound:  3.15950632e-01\n",
      "Epoch: 11443 mean train loss:  1.51057569e-02, bound:  3.15950066e-01\n",
      "Epoch: 11444 mean train loss:  1.51180755e-02, bound:  3.15950334e-01\n",
      "Epoch: 11445 mean train loss:  1.50709739e-02, bound:  3.15949678e-01\n",
      "Epoch: 11446 mean train loss:  1.50041142e-02, bound:  3.15949768e-01\n",
      "Epoch: 11447 mean train loss:  1.49650471e-02, bound:  3.15949470e-01\n",
      "Epoch: 11448 mean train loss:  1.49700791e-02, bound:  3.15949202e-01\n",
      "Epoch: 11449 mean train loss:  1.50003424e-02, bound:  3.15949202e-01\n",
      "Epoch: 11450 mean train loss:  1.50221465e-02, bound:  3.15948665e-01\n",
      "Epoch: 11451 mean train loss:  1.50155174e-02, bound:  3.15948844e-01\n",
      "Epoch: 11452 mean train loss:  1.49845723e-02, bound:  3.15948337e-01\n",
      "Epoch: 11453 mean train loss:  1.49499383e-02, bound:  3.15948278e-01\n",
      "Epoch: 11454 mean train loss:  1.49310175e-02, bound:  3.15948009e-01\n",
      "Epoch: 11455 mean train loss:  1.49325505e-02, bound:  3.15947771e-01\n",
      "Epoch: 11456 mean train loss:  1.49454819e-02, bound:  3.15947682e-01\n",
      "Epoch: 11457 mean train loss:  1.49544617e-02, bound:  3.15947324e-01\n",
      "Epoch: 11458 mean train loss:  1.49504272e-02, bound:  3.15947354e-01\n",
      "Epoch: 11459 mean train loss:  1.49343386e-02, bound:  3.15946966e-01\n",
      "Epoch: 11460 mean train loss:  1.49147324e-02, bound:  3.15946996e-01\n",
      "Epoch: 11461 mean train loss:  1.49000287e-02, bound:  3.15946698e-01\n",
      "Epoch: 11462 mean train loss:  1.48947826e-02, bound:  3.15946579e-01\n",
      "Epoch: 11463 mean train loss:  1.48966759e-02, bound:  3.15946460e-01\n",
      "Epoch: 11464 mean train loss:  1.48999793e-02, bound:  3.15946192e-01\n",
      "Epoch: 11465 mean train loss:  1.48995994e-02, bound:  3.15946221e-01\n",
      "Epoch: 11466 mean train loss:  1.48934107e-02, bound:  3.15945894e-01\n",
      "Epoch: 11467 mean train loss:  1.48827862e-02, bound:  3.15945894e-01\n",
      "Epoch: 11468 mean train loss:  1.48716886e-02, bound:  3.15945596e-01\n",
      "Epoch: 11469 mean train loss:  1.48629760e-02, bound:  3.15945536e-01\n",
      "Epoch: 11470 mean train loss:  1.48583725e-02, bound:  3.15945357e-01\n",
      "Epoch: 11471 mean train loss:  1.48565108e-02, bound:  3.15945208e-01\n",
      "Epoch: 11472 mean train loss:  1.48556707e-02, bound:  3.15945119e-01\n",
      "Epoch: 11473 mean train loss:  1.48535203e-02, bound:  3.15944850e-01\n",
      "Epoch: 11474 mean train loss:  1.48491552e-02, bound:  3.15944791e-01\n",
      "Epoch: 11475 mean train loss:  1.48425279e-02, bound:  3.15944552e-01\n",
      "Epoch: 11476 mean train loss:  1.48348529e-02, bound:  3.15944463e-01\n",
      "Epoch: 11477 mean train loss:  1.48278577e-02, bound:  3.15944284e-01\n",
      "Epoch: 11478 mean train loss:  1.48221357e-02, bound:  3.15944165e-01\n",
      "Epoch: 11479 mean train loss:  1.48179997e-02, bound:  3.15944016e-01\n",
      "Epoch: 11480 mean train loss:  1.48149170e-02, bound:  3.15943807e-01\n",
      "Epoch: 11481 mean train loss:  1.48119554e-02, bound:  3.15943718e-01\n",
      "Epoch: 11482 mean train loss:  1.48083940e-02, bound:  3.15943480e-01\n",
      "Epoch: 11483 mean train loss:  1.48039358e-02, bound:  3.15943390e-01\n",
      "Epoch: 11484 mean train loss:  1.47984764e-02, bound:  3.15943152e-01\n",
      "Epoch: 11485 mean train loss:  1.47928707e-02, bound:  3.15943062e-01\n",
      "Epoch: 11486 mean train loss:  1.47871887e-02, bound:  3.15942883e-01\n",
      "Epoch: 11487 mean train loss:  1.47821046e-02, bound:  3.15942734e-01\n",
      "Epoch: 11488 mean train loss:  1.47775086e-02, bound:  3.15942556e-01\n",
      "Epoch: 11489 mean train loss:  1.47733632e-02, bound:  3.15942407e-01\n",
      "Epoch: 11490 mean train loss:  1.47693623e-02, bound:  3.15942228e-01\n",
      "Epoch: 11491 mean train loss:  1.47654284e-02, bound:  3.15942049e-01\n",
      "Epoch: 11492 mean train loss:  1.47612123e-02, bound:  3.15941930e-01\n",
      "Epoch: 11493 mean train loss:  1.47568248e-02, bound:  3.15941721e-01\n",
      "Epoch: 11494 mean train loss:  1.47521319e-02, bound:  3.15941602e-01\n",
      "Epoch: 11495 mean train loss:  1.47471745e-02, bound:  3.15941393e-01\n",
      "Epoch: 11496 mean train loss:  1.47423791e-02, bound:  3.15941244e-01\n",
      "Epoch: 11497 mean train loss:  1.47377159e-02, bound:  3.15941036e-01\n",
      "Epoch: 11498 mean train loss:  1.47329522e-02, bound:  3.15940857e-01\n",
      "Epoch: 11499 mean train loss:  1.47284949e-02, bound:  3.15940708e-01\n",
      "Epoch: 11500 mean train loss:  1.47239529e-02, bound:  3.15940499e-01\n",
      "Epoch: 11501 mean train loss:  1.47196911e-02, bound:  3.15940320e-01\n",
      "Epoch: 11502 mean train loss:  1.47152599e-02, bound:  3.15940142e-01\n",
      "Epoch: 11503 mean train loss:  1.47109646e-02, bound:  3.15939993e-01\n",
      "Epoch: 11504 mean train loss:  1.47066209e-02, bound:  3.15939754e-01\n",
      "Epoch: 11505 mean train loss:  1.47021329e-02, bound:  3.15939605e-01\n",
      "Epoch: 11506 mean train loss:  1.46976421e-02, bound:  3.15939397e-01\n",
      "Epoch: 11507 mean train loss:  1.46931475e-02, bound:  3.15939277e-01\n",
      "Epoch: 11508 mean train loss:  1.46887330e-02, bound:  3.15939039e-01\n",
      "Epoch: 11509 mean train loss:  1.46840820e-02, bound:  3.15938860e-01\n",
      "Epoch: 11510 mean train loss:  1.46795055e-02, bound:  3.15938711e-01\n",
      "Epoch: 11511 mean train loss:  1.46750929e-02, bound:  3.15938532e-01\n",
      "Epoch: 11512 mean train loss:  1.46706356e-02, bound:  3.15938383e-01\n",
      "Epoch: 11513 mean train loss:  1.46661429e-02, bound:  3.15938175e-01\n",
      "Epoch: 11514 mean train loss:  1.46615291e-02, bound:  3.15937966e-01\n",
      "Epoch: 11515 mean train loss:  1.46571184e-02, bound:  3.15937787e-01\n",
      "Epoch: 11516 mean train loss:  1.46526555e-02, bound:  3.15937608e-01\n",
      "Epoch: 11517 mean train loss:  1.46482922e-02, bound:  3.15937430e-01\n",
      "Epoch: 11518 mean train loss:  1.46438144e-02, bound:  3.15937281e-01\n",
      "Epoch: 11519 mean train loss:  1.46395173e-02, bound:  3.15937042e-01\n",
      "Epoch: 11520 mean train loss:  1.46350320e-02, bound:  3.15936893e-01\n",
      "Epoch: 11521 mean train loss:  1.46305701e-02, bound:  3.15936714e-01\n",
      "Epoch: 11522 mean train loss:  1.46261090e-02, bound:  3.15936536e-01\n",
      "Epoch: 11523 mean train loss:  1.46216843e-02, bound:  3.15936327e-01\n",
      "Epoch: 11524 mean train loss:  1.46172615e-02, bound:  3.15936178e-01\n",
      "Epoch: 11525 mean train loss:  1.46127297e-02, bound:  3.15935969e-01\n",
      "Epoch: 11526 mean train loss:  1.46082202e-02, bound:  3.15935850e-01\n",
      "Epoch: 11527 mean train loss:  1.46038039e-02, bound:  3.15935642e-01\n",
      "Epoch: 11528 mean train loss:  1.45993242e-02, bound:  3.15935463e-01\n",
      "Epoch: 11529 mean train loss:  1.45949870e-02, bound:  3.15935284e-01\n",
      "Epoch: 11530 mean train loss:  1.45904357e-02, bound:  3.15935105e-01\n",
      "Epoch: 11531 mean train loss:  1.45860305e-02, bound:  3.15934926e-01\n",
      "Epoch: 11532 mean train loss:  1.45815471e-02, bound:  3.15934747e-01\n",
      "Epoch: 11533 mean train loss:  1.45772221e-02, bound:  3.15934569e-01\n",
      "Epoch: 11534 mean train loss:  1.45727741e-02, bound:  3.15934420e-01\n",
      "Epoch: 11535 mean train loss:  1.45682599e-02, bound:  3.15934211e-01\n",
      "Epoch: 11536 mean train loss:  1.45638334e-02, bound:  3.15934002e-01\n",
      "Epoch: 11537 mean train loss:  1.45593695e-02, bound:  3.15933853e-01\n",
      "Epoch: 11538 mean train loss:  1.45549262e-02, bound:  3.15933675e-01\n",
      "Epoch: 11539 mean train loss:  1.45505136e-02, bound:  3.15933466e-01\n",
      "Epoch: 11540 mean train loss:  1.45461857e-02, bound:  3.15933317e-01\n",
      "Epoch: 11541 mean train loss:  1.45417042e-02, bound:  3.15933138e-01\n",
      "Epoch: 11542 mean train loss:  1.45372096e-02, bound:  3.15932959e-01\n",
      "Epoch: 11543 mean train loss:  1.45327430e-02, bound:  3.15932781e-01\n",
      "Epoch: 11544 mean train loss:  1.45284450e-02, bound:  3.15932572e-01\n",
      "Epoch: 11545 mean train loss:  1.45240016e-02, bound:  3.15932423e-01\n",
      "Epoch: 11546 mean train loss:  1.45195173e-02, bound:  3.15932244e-01\n",
      "Epoch: 11547 mean train loss:  1.45153562e-02, bound:  3.15932035e-01\n",
      "Epoch: 11548 mean train loss:  1.45108160e-02, bound:  3.15931857e-01\n",
      "Epoch: 11549 mean train loss:  1.45064220e-02, bound:  3.15931678e-01\n",
      "Epoch: 11550 mean train loss:  1.45020466e-02, bound:  3.15931499e-01\n",
      "Epoch: 11551 mean train loss:  1.44979218e-02, bound:  3.15931350e-01\n",
      "Epoch: 11552 mean train loss:  1.44935148e-02, bound:  3.15931141e-01\n",
      "Epoch: 11553 mean train loss:  1.44894328e-02, bound:  3.15930992e-01\n",
      "Epoch: 11554 mean train loss:  1.44852977e-02, bound:  3.15930784e-01\n",
      "Epoch: 11555 mean train loss:  1.44815715e-02, bound:  3.15930605e-01\n",
      "Epoch: 11556 mean train loss:  1.44776301e-02, bound:  3.15930396e-01\n",
      "Epoch: 11557 mean train loss:  1.44741246e-02, bound:  3.15930247e-01\n",
      "Epoch: 11558 mean train loss:  1.44708753e-02, bound:  3.15930039e-01\n",
      "Epoch: 11559 mean train loss:  1.44680096e-02, bound:  3.15929919e-01\n",
      "Epoch: 11560 mean train loss:  1.44662503e-02, bound:  3.15929621e-01\n",
      "Epoch: 11561 mean train loss:  1.44653907e-02, bound:  3.15929592e-01\n",
      "Epoch: 11562 mean train loss:  1.44665400e-02, bound:  3.15929234e-01\n",
      "Epoch: 11563 mean train loss:  1.44703751e-02, bound:  3.15929234e-01\n",
      "Epoch: 11564 mean train loss:  1.44786071e-02, bound:  3.15928817e-01\n",
      "Epoch: 11565 mean train loss:  1.44934962e-02, bound:  3.15928906e-01\n",
      "Epoch: 11566 mean train loss:  1.45181259e-02, bound:  3.15928400e-01\n",
      "Epoch: 11567 mean train loss:  1.45577574e-02, bound:  3.15928608e-01\n",
      "Epoch: 11568 mean train loss:  1.46200471e-02, bound:  3.15927982e-01\n",
      "Epoch: 11569 mean train loss:  1.47170136e-02, bound:  3.15928370e-01\n",
      "Epoch: 11570 mean train loss:  1.48630785e-02, bound:  3.15927476e-01\n",
      "Epoch: 11571 mean train loss:  1.50780370e-02, bound:  3.15928161e-01\n",
      "Epoch: 11572 mean train loss:  1.53786512e-02, bound:  3.15926850e-01\n",
      "Epoch: 11573 mean train loss:  1.57732144e-02, bound:  3.15927953e-01\n",
      "Epoch: 11574 mean train loss:  1.62220746e-02, bound:  3.15926254e-01\n",
      "Epoch: 11575 mean train loss:  1.66256819e-02, bound:  3.15927744e-01\n",
      "Epoch: 11576 mean train loss:  1.67855043e-02, bound:  3.15925717e-01\n",
      "Epoch: 11577 mean train loss:  1.65251885e-02, bound:  3.15927297e-01\n",
      "Epoch: 11578 mean train loss:  1.58230364e-02, bound:  3.15925449e-01\n",
      "Epoch: 11579 mean train loss:  1.49813471e-02, bound:  3.15926492e-01\n",
      "Epoch: 11580 mean train loss:  1.44411260e-02, bound:  3.15925628e-01\n",
      "Epoch: 11581 mean train loss:  1.44415936e-02, bound:  3.15925479e-01\n",
      "Epoch: 11582 mean train loss:  1.48242461e-02, bound:  3.15925956e-01\n",
      "Epoch: 11583 mean train loss:  1.51860891e-02, bound:  3.15924823e-01\n",
      "Epoch: 11584 mean train loss:  1.52049558e-02, bound:  3.15925837e-01\n",
      "Epoch: 11585 mean train loss:  1.48740634e-02, bound:  3.15924734e-01\n",
      "Epoch: 11586 mean train loss:  1.44891981e-02, bound:  3.15925270e-01\n",
      "Epoch: 11587 mean train loss:  1.43529987e-02, bound:  3.15924942e-01\n",
      "Epoch: 11588 mean train loss:  1.45041235e-02, bound:  3.15924585e-01\n",
      "Epoch: 11589 mean train loss:  1.47260213e-02, bound:  3.15925032e-01\n",
      "Epoch: 11590 mean train loss:  1.47821177e-02, bound:  3.15924078e-01\n",
      "Epoch: 11591 mean train loss:  1.46277770e-02, bound:  3.15924674e-01\n",
      "Epoch: 11592 mean train loss:  1.44155622e-02, bound:  3.15923959e-01\n",
      "Epoch: 11593 mean train loss:  1.43248988e-02, bound:  3.15924019e-01\n",
      "Epoch: 11594 mean train loss:  1.43937981e-02, bound:  3.15923959e-01\n",
      "Epoch: 11595 mean train loss:  1.45140840e-02, bound:  3.15923333e-01\n",
      "Epoch: 11596 mean train loss:  1.45559749e-02, bound:  3.15923721e-01\n",
      "Epoch: 11597 mean train loss:  1.44833438e-02, bound:  3.15922946e-01\n",
      "Epoch: 11598 mean train loss:  1.43662952e-02, bound:  3.15923214e-01\n",
      "Epoch: 11599 mean train loss:  1.42991766e-02, bound:  3.15922797e-01\n",
      "Epoch: 11600 mean train loss:  1.43166846e-02, bound:  3.15922529e-01\n",
      "Epoch: 11601 mean train loss:  1.43780857e-02, bound:  3.15922648e-01\n",
      "Epoch: 11602 mean train loss:  1.44164348e-02, bound:  3.15922022e-01\n",
      "Epoch: 11603 mean train loss:  1.43968910e-02, bound:  3.15922290e-01\n",
      "Epoch: 11604 mean train loss:  1.43370926e-02, bound:  3.15921724e-01\n",
      "Epoch: 11605 mean train loss:  1.42828738e-02, bound:  3.15921724e-01\n",
      "Epoch: 11606 mean train loss:  1.42667806e-02, bound:  3.15921515e-01\n",
      "Epoch: 11607 mean train loss:  1.42864352e-02, bound:  3.15921158e-01\n",
      "Epoch: 11608 mean train loss:  1.43147865e-02, bound:  3.15921247e-01\n",
      "Epoch: 11609 mean train loss:  1.43241026e-02, bound:  3.15920711e-01\n",
      "Epoch: 11610 mean train loss:  1.43064484e-02, bound:  3.15920770e-01\n",
      "Epoch: 11611 mean train loss:  1.42737227e-02, bound:  3.15920383e-01\n",
      "Epoch: 11612 mean train loss:  1.42457318e-02, bound:  3.15920293e-01\n",
      "Epoch: 11613 mean train loss:  1.42356828e-02, bound:  3.15920085e-01\n",
      "Epoch: 11614 mean train loss:  1.42423641e-02, bound:  3.15919816e-01\n",
      "Epoch: 11615 mean train loss:  1.42542925e-02, bound:  3.15919787e-01\n",
      "Epoch: 11616 mean train loss:  1.42590497e-02, bound:  3.15919429e-01\n",
      "Epoch: 11617 mean train loss:  1.42515106e-02, bound:  3.15919489e-01\n",
      "Epoch: 11618 mean train loss:  1.42349107e-02, bound:  3.15919101e-01\n",
      "Epoch: 11619 mean train loss:  1.42175239e-02, bound:  3.15919101e-01\n",
      "Epoch: 11620 mean train loss:  1.42061934e-02, bound:  3.15918863e-01\n",
      "Epoch: 11621 mean train loss:  1.42030064e-02, bound:  3.15918714e-01\n",
      "Epoch: 11622 mean train loss:  1.42050292e-02, bound:  3.15918654e-01\n",
      "Epoch: 11623 mean train loss:  1.42075000e-02, bound:  3.15918356e-01\n",
      "Epoch: 11624 mean train loss:  1.42061291e-02, bound:  3.15918386e-01\n",
      "Epoch: 11625 mean train loss:  1.41997328e-02, bound:  3.15918088e-01\n",
      "Epoch: 11626 mean train loss:  1.41899800e-02, bound:  3.15918088e-01\n",
      "Epoch: 11627 mean train loss:  1.41801639e-02, bound:  3.15917850e-01\n",
      "Epoch: 11628 mean train loss:  1.41727645e-02, bound:  3.15917760e-01\n",
      "Epoch: 11629 mean train loss:  1.41685884e-02, bound:  3.15917641e-01\n",
      "Epoch: 11630 mean train loss:  1.41669493e-02, bound:  3.15917403e-01\n",
      "Epoch: 11631 mean train loss:  1.41660450e-02, bound:  3.15917343e-01\n",
      "Epoch: 11632 mean train loss:  1.41640790e-02, bound:  3.15917104e-01\n",
      "Epoch: 11633 mean train loss:  1.41600752e-02, bound:  3.15917075e-01\n",
      "Epoch: 11634 mean train loss:  1.41542451e-02, bound:  3.15916836e-01\n",
      "Epoch: 11635 mean train loss:  1.41475657e-02, bound:  3.15916777e-01\n",
      "Epoch: 11636 mean train loss:  1.41408592e-02, bound:  3.15916568e-01\n",
      "Epoch: 11637 mean train loss:  1.41354175e-02, bound:  3.15916449e-01\n",
      "Epoch: 11638 mean train loss:  1.41310599e-02, bound:  3.15916330e-01\n",
      "Epoch: 11639 mean train loss:  1.41279660e-02, bound:  3.15916151e-01\n",
      "Epoch: 11640 mean train loss:  1.41251544e-02, bound:  3.15916032e-01\n",
      "Epoch: 11641 mean train loss:  1.41219981e-02, bound:  3.15915853e-01\n",
      "Epoch: 11642 mean train loss:  1.41183650e-02, bound:  3.15915763e-01\n",
      "Epoch: 11643 mean train loss:  1.41138434e-02, bound:  3.15915525e-01\n",
      "Epoch: 11644 mean train loss:  1.41088916e-02, bound:  3.15915436e-01\n",
      "Epoch: 11645 mean train loss:  1.41036306e-02, bound:  3.15915227e-01\n",
      "Epoch: 11646 mean train loss:  1.40986936e-02, bound:  3.15915108e-01\n",
      "Epoch: 11647 mean train loss:  1.40941422e-02, bound:  3.15914929e-01\n",
      "Epoch: 11648 mean train loss:  1.40897958e-02, bound:  3.15914780e-01\n",
      "Epoch: 11649 mean train loss:  1.40859904e-02, bound:  3.15914661e-01\n",
      "Epoch: 11650 mean train loss:  1.40823247e-02, bound:  3.15914452e-01\n",
      "Epoch: 11651 mean train loss:  1.40787717e-02, bound:  3.15914333e-01\n",
      "Epoch: 11652 mean train loss:  1.40750241e-02, bound:  3.15914094e-01\n",
      "Epoch: 11653 mean train loss:  1.40707875e-02, bound:  3.15914005e-01\n",
      "Epoch: 11654 mean train loss:  1.40666142e-02, bound:  3.15913796e-01\n",
      "Epoch: 11655 mean train loss:  1.40621951e-02, bound:  3.15913677e-01\n",
      "Epoch: 11656 mean train loss:  1.40577136e-02, bound:  3.15913469e-01\n",
      "Epoch: 11657 mean train loss:  1.40532190e-02, bound:  3.15913349e-01\n",
      "Epoch: 11658 mean train loss:  1.40490383e-02, bound:  3.15913141e-01\n",
      "Epoch: 11659 mean train loss:  1.40446844e-02, bound:  3.15912992e-01\n",
      "Epoch: 11660 mean train loss:  1.40404664e-02, bound:  3.15912813e-01\n",
      "Epoch: 11661 mean train loss:  1.40364906e-02, bound:  3.15912604e-01\n",
      "Epoch: 11662 mean train loss:  1.40324319e-02, bound:  3.15912485e-01\n",
      "Epoch: 11663 mean train loss:  1.40285967e-02, bound:  3.15912277e-01\n",
      "Epoch: 11664 mean train loss:  1.40244905e-02, bound:  3.15912157e-01\n",
      "Epoch: 11665 mean train loss:  1.40205007e-02, bound:  3.15911919e-01\n",
      "Epoch: 11666 mean train loss:  1.40162949e-02, bound:  3.15911800e-01\n",
      "Epoch: 11667 mean train loss:  1.40122194e-02, bound:  3.15911591e-01\n",
      "Epoch: 11668 mean train loss:  1.40080322e-02, bound:  3.15911472e-01\n",
      "Epoch: 11669 mean train loss:  1.40037723e-02, bound:  3.15911233e-01\n",
      "Epoch: 11670 mean train loss:  1.39996419e-02, bound:  3.15911114e-01\n",
      "Epoch: 11671 mean train loss:  1.39954602e-02, bound:  3.15910906e-01\n",
      "Epoch: 11672 mean train loss:  1.39911948e-02, bound:  3.15910727e-01\n",
      "Epoch: 11673 mean train loss:  1.39870178e-02, bound:  3.15910578e-01\n",
      "Epoch: 11674 mean train loss:  1.39831044e-02, bound:  3.15910399e-01\n",
      "Epoch: 11675 mean train loss:  1.39788957e-02, bound:  3.15910250e-01\n",
      "Epoch: 11676 mean train loss:  1.39747979e-02, bound:  3.15910041e-01\n",
      "Epoch: 11677 mean train loss:  1.39706712e-02, bound:  3.15909892e-01\n",
      "Epoch: 11678 mean train loss:  1.39666013e-02, bound:  3.15909714e-01\n",
      "Epoch: 11679 mean train loss:  1.39625985e-02, bound:  3.15909564e-01\n",
      "Epoch: 11680 mean train loss:  1.39585137e-02, bound:  3.15909356e-01\n",
      "Epoch: 11681 mean train loss:  1.39543945e-02, bound:  3.15909237e-01\n",
      "Epoch: 11682 mean train loss:  1.39503637e-02, bound:  3.15909028e-01\n",
      "Epoch: 11683 mean train loss:  1.39461420e-02, bound:  3.15908849e-01\n",
      "Epoch: 11684 mean train loss:  1.39421951e-02, bound:  3.15908700e-01\n",
      "Epoch: 11685 mean train loss:  1.39379967e-02, bound:  3.15908492e-01\n",
      "Epoch: 11686 mean train loss:  1.39339594e-02, bound:  3.15908372e-01\n",
      "Epoch: 11687 mean train loss:  1.39298104e-02, bound:  3.15908194e-01\n",
      "Epoch: 11688 mean train loss:  1.39256725e-02, bound:  3.15908015e-01\n",
      "Epoch: 11689 mean train loss:  1.39216399e-02, bound:  3.15907866e-01\n",
      "Epoch: 11690 mean train loss:  1.39175775e-02, bound:  3.15907687e-01\n",
      "Epoch: 11691 mean train loss:  1.39135085e-02, bound:  3.15907508e-01\n",
      "Epoch: 11692 mean train loss:  1.39093995e-02, bound:  3.15907300e-01\n",
      "Epoch: 11693 mean train loss:  1.39052337e-02, bound:  3.15907180e-01\n",
      "Epoch: 11694 mean train loss:  1.39011992e-02, bound:  3.15906972e-01\n",
      "Epoch: 11695 mean train loss:  1.38970762e-02, bound:  3.15906823e-01\n",
      "Epoch: 11696 mean train loss:  1.38928704e-02, bound:  3.15906614e-01\n",
      "Epoch: 11697 mean train loss:  1.38889700e-02, bound:  3.15906495e-01\n",
      "Epoch: 11698 mean train loss:  1.38847874e-02, bound:  3.15906286e-01\n",
      "Epoch: 11699 mean train loss:  1.38807315e-02, bound:  3.15906167e-01\n",
      "Epoch: 11700 mean train loss:  1.38767762e-02, bound:  3.15905958e-01\n",
      "Epoch: 11701 mean train loss:  1.38726989e-02, bound:  3.15905839e-01\n",
      "Epoch: 11702 mean train loss:  1.38688302e-02, bound:  3.15905601e-01\n",
      "Epoch: 11703 mean train loss:  1.38648991e-02, bound:  3.15905482e-01\n",
      "Epoch: 11704 mean train loss:  1.38608469e-02, bound:  3.15905273e-01\n",
      "Epoch: 11705 mean train loss:  1.38571253e-02, bound:  3.15905154e-01\n",
      "Epoch: 11706 mean train loss:  1.38532287e-02, bound:  3.15904886e-01\n",
      "Epoch: 11707 mean train loss:  1.38496300e-02, bound:  3.15904766e-01\n",
      "Epoch: 11708 mean train loss:  1.38462484e-02, bound:  3.15904528e-01\n",
      "Epoch: 11709 mean train loss:  1.38429217e-02, bound:  3.15904438e-01\n",
      "Epoch: 11710 mean train loss:  1.38400039e-02, bound:  3.15904200e-01\n",
      "Epoch: 11711 mean train loss:  1.38376551e-02, bound:  3.15904111e-01\n",
      "Epoch: 11712 mean train loss:  1.38360290e-02, bound:  3.15903842e-01\n",
      "Epoch: 11713 mean train loss:  1.38356602e-02, bound:  3.15903753e-01\n",
      "Epoch: 11714 mean train loss:  1.38370181e-02, bound:  3.15903455e-01\n",
      "Epoch: 11715 mean train loss:  1.38409492e-02, bound:  3.15903455e-01\n",
      "Epoch: 11716 mean train loss:  1.38489911e-02, bound:  3.15903097e-01\n",
      "Epoch: 11717 mean train loss:  1.38627579e-02, bound:  3.15903157e-01\n",
      "Epoch: 11718 mean train loss:  1.38854580e-02, bound:  3.15902650e-01\n",
      "Epoch: 11719 mean train loss:  1.39217358e-02, bound:  3.15902859e-01\n",
      "Epoch: 11720 mean train loss:  1.39777372e-02, bound:  3.15902233e-01\n",
      "Epoch: 11721 mean train loss:  1.40631972e-02, bound:  3.15902621e-01\n",
      "Epoch: 11722 mean train loss:  1.41911255e-02, bound:  3.15901786e-01\n",
      "Epoch: 11723 mean train loss:  1.43790059e-02, bound:  3.15902412e-01\n",
      "Epoch: 11724 mean train loss:  1.46403788e-02, bound:  3.15901250e-01\n",
      "Epoch: 11725 mean train loss:  1.49850473e-02, bound:  3.15902203e-01\n",
      "Epoch: 11726 mean train loss:  1.53864669e-02, bound:  3.15900654e-01\n",
      "Epoch: 11727 mean train loss:  1.57728028e-02, bound:  3.15901995e-01\n",
      "Epoch: 11728 mean train loss:  1.59836076e-02, bound:  3.15900147e-01\n",
      "Epoch: 11729 mean train loss:  1.58560127e-02, bound:  3.15901607e-01\n",
      "Epoch: 11730 mean train loss:  1.53145213e-02, bound:  3.15899879e-01\n",
      "Epoch: 11731 mean train loss:  1.45499129e-02, bound:  3.15900892e-01\n",
      "Epoch: 11732 mean train loss:  1.39313685e-02, bound:  3.15899968e-01\n",
      "Epoch: 11733 mean train loss:  1.37541164e-02, bound:  3.15899998e-01\n",
      "Epoch: 11734 mean train loss:  1.40014049e-02, bound:  3.15900236e-01\n",
      "Epoch: 11735 mean train loss:  1.43784191e-02, bound:  3.15899342e-01\n",
      "Epoch: 11736 mean train loss:  1.45487105e-02, bound:  3.15900236e-01\n",
      "Epoch: 11737 mean train loss:  1.43754259e-02, bound:  3.15899134e-01\n",
      "Epoch: 11738 mean train loss:  1.40155414e-02, bound:  3.15899789e-01\n",
      "Epoch: 11739 mean train loss:  1.37572177e-02, bound:  3.15899253e-01\n",
      "Epoch: 11740 mean train loss:  1.37644364e-02, bound:  3.15899134e-01\n",
      "Epoch: 11741 mean train loss:  1.39536709e-02, bound:  3.15899372e-01\n",
      "Epoch: 11742 mean train loss:  1.41075766e-02, bound:  3.15898567e-01\n",
      "Epoch: 11743 mean train loss:  1.40813142e-02, bound:  3.15899134e-01\n",
      "Epoch: 11744 mean train loss:  1.39080193e-02, bound:  3.15898359e-01\n",
      "Epoch: 11745 mean train loss:  1.37427673e-02, bound:  3.15898567e-01\n",
      "Epoch: 11746 mean train loss:  1.37061086e-02, bound:  3.15898299e-01\n",
      "Epoch: 11747 mean train loss:  1.37883658e-02, bound:  3.15897912e-01\n",
      "Epoch: 11748 mean train loss:  1.38848945e-02, bound:  3.15898210e-01\n",
      "Epoch: 11749 mean train loss:  1.39015168e-02, bound:  3.15897435e-01\n",
      "Epoch: 11750 mean train loss:  1.38281099e-02, bound:  3.15897793e-01\n",
      "Epoch: 11751 mean train loss:  1.37286996e-02, bound:  3.15897226e-01\n",
      "Epoch: 11752 mean train loss:  1.36757772e-02, bound:  3.15897226e-01\n",
      "Epoch: 11753 mean train loss:  1.36924256e-02, bound:  3.15897107e-01\n",
      "Epoch: 11754 mean train loss:  1.37437098e-02, bound:  3.15896660e-01\n",
      "Epoch: 11755 mean train loss:  1.37764569e-02, bound:  3.15896869e-01\n",
      "Epoch: 11756 mean train loss:  1.37628289e-02, bound:  3.15896213e-01\n",
      "Epoch: 11757 mean train loss:  1.37146544e-02, bound:  3.15896392e-01\n",
      "Epoch: 11758 mean train loss:  1.36666987e-02, bound:  3.15895975e-01\n",
      "Epoch: 11759 mean train loss:  1.36460150e-02, bound:  3.15895826e-01\n",
      "Epoch: 11760 mean train loss:  1.36560136e-02, bound:  3.15895706e-01\n",
      "Epoch: 11761 mean train loss:  1.36788823e-02, bound:  3.15895289e-01\n",
      "Epoch: 11762 mean train loss:  1.36923986e-02, bound:  3.15895379e-01\n",
      "Epoch: 11763 mean train loss:  1.36851799e-02, bound:  3.15894902e-01\n",
      "Epoch: 11764 mean train loss:  1.36617087e-02, bound:  3.15894961e-01\n",
      "Epoch: 11765 mean train loss:  1.36354305e-02, bound:  3.15894604e-01\n",
      "Epoch: 11766 mean train loss:  1.36189321e-02, bound:  3.15894485e-01\n",
      "Epoch: 11767 mean train loss:  1.36166913e-02, bound:  3.15894365e-01\n",
      "Epoch: 11768 mean train loss:  1.36239557e-02, bound:  3.15894067e-01\n",
      "Epoch: 11769 mean train loss:  1.36314929e-02, bound:  3.15894037e-01\n",
      "Epoch: 11770 mean train loss:  1.36321038e-02, bound:  3.15893739e-01\n",
      "Epoch: 11771 mean train loss:  1.36239063e-02, bound:  3.15893739e-01\n",
      "Epoch: 11772 mean train loss:  1.36099597e-02, bound:  3.15893471e-01\n",
      "Epoch: 11773 mean train loss:  1.35964323e-02, bound:  3.15893471e-01\n",
      "Epoch: 11774 mean train loss:  1.35871861e-02, bound:  3.15893233e-01\n",
      "Epoch: 11775 mean train loss:  1.35835884e-02, bound:  3.15893084e-01\n",
      "Epoch: 11776 mean train loss:  1.35840634e-02, bound:  3.15892965e-01\n",
      "Epoch: 11777 mean train loss:  1.35850180e-02, bound:  3.15892756e-01\n",
      "Epoch: 11778 mean train loss:  1.35839069e-02, bound:  3.15892726e-01\n",
      "Epoch: 11779 mean train loss:  1.35796061e-02, bound:  3.15892488e-01\n",
      "Epoch: 11780 mean train loss:  1.35727031e-02, bound:  3.15892488e-01\n",
      "Epoch: 11781 mean train loss:  1.35647226e-02, bound:  3.15892249e-01\n",
      "Epoch: 11782 mean train loss:  1.35574993e-02, bound:  3.15892160e-01\n",
      "Epoch: 11783 mean train loss:  1.35521339e-02, bound:  3.15892041e-01\n",
      "Epoch: 11784 mean train loss:  1.35485120e-02, bound:  3.15891862e-01\n",
      "Epoch: 11785 mean train loss:  1.35463132e-02, bound:  3.15891743e-01\n",
      "Epoch: 11786 mean train loss:  1.35443239e-02, bound:  3.15891594e-01\n",
      "Epoch: 11787 mean train loss:  1.35414368e-02, bound:  3.15891504e-01\n",
      "Epoch: 11788 mean train loss:  1.35378409e-02, bound:  3.15891296e-01\n",
      "Epoch: 11789 mean train loss:  1.35329366e-02, bound:  3.15891236e-01\n",
      "Epoch: 11790 mean train loss:  1.35279112e-02, bound:  3.15891027e-01\n",
      "Epoch: 11791 mean train loss:  1.35226026e-02, bound:  3.15890938e-01\n",
      "Epoch: 11792 mean train loss:  1.35177923e-02, bound:  3.15890759e-01\n",
      "Epoch: 11793 mean train loss:  1.35137625e-02, bound:  3.15890640e-01\n",
      "Epoch: 11794 mean train loss:  1.35102281e-02, bound:  3.15890491e-01\n",
      "Epoch: 11795 mean train loss:  1.35069052e-02, bound:  3.15890312e-01\n",
      "Epoch: 11796 mean train loss:  1.35036167e-02, bound:  3.15890193e-01\n",
      "Epoch: 11797 mean train loss:  1.35003757e-02, bound:  3.15890044e-01\n",
      "Epoch: 11798 mean train loss:  1.34966420e-02, bound:  3.15889925e-01\n",
      "Epoch: 11799 mean train loss:  1.34925917e-02, bound:  3.15889716e-01\n",
      "Epoch: 11800 mean train loss:  1.34884864e-02, bound:  3.15889627e-01\n",
      "Epoch: 11801 mean train loss:  1.34841641e-02, bound:  3.15889418e-01\n",
      "Epoch: 11802 mean train loss:  1.34799266e-02, bound:  3.15889299e-01\n",
      "Epoch: 11803 mean train loss:  1.34758353e-02, bound:  3.15889090e-01\n",
      "Epoch: 11804 mean train loss:  1.34717291e-02, bound:  3.15888941e-01\n",
      "Epoch: 11805 mean train loss:  1.34679433e-02, bound:  3.15888822e-01\n",
      "Epoch: 11806 mean train loss:  1.34642730e-02, bound:  3.15888643e-01\n",
      "Epoch: 11807 mean train loss:  1.34605030e-02, bound:  3.15888494e-01\n",
      "Epoch: 11808 mean train loss:  1.34569416e-02, bound:  3.15888315e-01\n",
      "Epoch: 11809 mean train loss:  1.34532470e-02, bound:  3.15888196e-01\n",
      "Epoch: 11810 mean train loss:  1.34494286e-02, bound:  3.15887988e-01\n",
      "Epoch: 11811 mean train loss:  1.34458020e-02, bound:  3.15887868e-01\n",
      "Epoch: 11812 mean train loss:  1.34418616e-02, bound:  3.15887660e-01\n",
      "Epoch: 11813 mean train loss:  1.34380301e-02, bound:  3.15887511e-01\n",
      "Epoch: 11814 mean train loss:  1.34340646e-02, bound:  3.15887332e-01\n",
      "Epoch: 11815 mean train loss:  1.34302275e-02, bound:  3.15887183e-01\n",
      "Epoch: 11816 mean train loss:  1.34260515e-02, bound:  3.15887004e-01\n",
      "Epoch: 11817 mean train loss:  1.34223038e-02, bound:  3.15886885e-01\n",
      "Epoch: 11818 mean train loss:  1.34184109e-02, bound:  3.15886676e-01\n",
      "Epoch: 11819 mean train loss:  1.34146120e-02, bound:  3.15886527e-01\n",
      "Epoch: 11820 mean train loss:  1.34108793e-02, bound:  3.15886378e-01\n",
      "Epoch: 11821 mean train loss:  1.34070143e-02, bound:  3.15886199e-01\n",
      "Epoch: 11822 mean train loss:  1.34031754e-02, bound:  3.15886021e-01\n",
      "Epoch: 11823 mean train loss:  1.33994436e-02, bound:  3.15885872e-01\n",
      "Epoch: 11824 mean train loss:  1.33956559e-02, bound:  3.15885723e-01\n",
      "Epoch: 11825 mean train loss:  1.33918114e-02, bound:  3.15885514e-01\n",
      "Epoch: 11826 mean train loss:  1.33880554e-02, bound:  3.15885395e-01\n",
      "Epoch: 11827 mean train loss:  1.33842947e-02, bound:  3.15885216e-01\n",
      "Epoch: 11828 mean train loss:  1.33804530e-02, bound:  3.15885067e-01\n",
      "Epoch: 11829 mean train loss:  1.33766662e-02, bound:  3.15884888e-01\n",
      "Epoch: 11830 mean train loss:  1.33728767e-02, bound:  3.15884739e-01\n",
      "Epoch: 11831 mean train loss:  1.33691877e-02, bound:  3.15884560e-01\n",
      "Epoch: 11832 mean train loss:  1.33653199e-02, bound:  3.15884382e-01\n",
      "Epoch: 11833 mean train loss:  1.33616682e-02, bound:  3.15884203e-01\n",
      "Epoch: 11834 mean train loss:  1.33578498e-02, bound:  3.15884084e-01\n",
      "Epoch: 11835 mean train loss:  1.33538926e-02, bound:  3.15883875e-01\n",
      "Epoch: 11836 mean train loss:  1.33500518e-02, bound:  3.15883756e-01\n",
      "Epoch: 11837 mean train loss:  1.33462930e-02, bound:  3.15883577e-01\n",
      "Epoch: 11838 mean train loss:  1.33424457e-02, bound:  3.15883428e-01\n",
      "Epoch: 11839 mean train loss:  1.33387027e-02, bound:  3.15883249e-01\n",
      "Epoch: 11840 mean train loss:  1.33348852e-02, bound:  3.15883070e-01\n",
      "Epoch: 11841 mean train loss:  1.33311125e-02, bound:  3.15882891e-01\n",
      "Epoch: 11842 mean train loss:  1.33273173e-02, bound:  3.15882742e-01\n",
      "Epoch: 11843 mean train loss:  1.33235278e-02, bound:  3.15882564e-01\n",
      "Epoch: 11844 mean train loss:  1.33197522e-02, bound:  3.15882444e-01\n",
      "Epoch: 11845 mean train loss:  1.33160967e-02, bound:  3.15882236e-01\n",
      "Epoch: 11846 mean train loss:  1.33124422e-02, bound:  3.15882146e-01\n",
      "Epoch: 11847 mean train loss:  1.33085893e-02, bound:  3.15881908e-01\n",
      "Epoch: 11848 mean train loss:  1.33049320e-02, bound:  3.15881789e-01\n",
      "Epoch: 11849 mean train loss:  1.33014405e-02, bound:  3.15881580e-01\n",
      "Epoch: 11850 mean train loss:  1.32977804e-02, bound:  3.15881431e-01\n",
      "Epoch: 11851 mean train loss:  1.32943504e-02, bound:  3.15881252e-01\n",
      "Epoch: 11852 mean train loss:  1.32912602e-02, bound:  3.15881133e-01\n",
      "Epoch: 11853 mean train loss:  1.32880500e-02, bound:  3.15880895e-01\n",
      "Epoch: 11854 mean train loss:  1.32852634e-02, bound:  3.15880805e-01\n",
      "Epoch: 11855 mean train loss:  1.32830320e-02, bound:  3.15880567e-01\n",
      "Epoch: 11856 mean train loss:  1.32812560e-02, bound:  3.15880477e-01\n",
      "Epoch: 11857 mean train loss:  1.32806487e-02, bound:  3.15880209e-01\n",
      "Epoch: 11858 mean train loss:  1.32813742e-02, bound:  3.15880209e-01\n",
      "Epoch: 11859 mean train loss:  1.32842604e-02, bound:  3.15879881e-01\n",
      "Epoch: 11860 mean train loss:  1.32901622e-02, bound:  3.15879881e-01\n",
      "Epoch: 11861 mean train loss:  1.33010568e-02, bound:  3.15879464e-01\n",
      "Epoch: 11862 mean train loss:  1.33193536e-02, bound:  3.15879583e-01\n",
      "Epoch: 11863 mean train loss:  1.33485347e-02, bound:  3.15879107e-01\n",
      "Epoch: 11864 mean train loss:  1.33943623e-02, bound:  3.15879315e-01\n",
      "Epoch: 11865 mean train loss:  1.34645123e-02, bound:  3.15878689e-01\n",
      "Epoch: 11866 mean train loss:  1.35707259e-02, bound:  3.15879107e-01\n",
      "Epoch: 11867 mean train loss:  1.37272039e-02, bound:  3.15878242e-01\n",
      "Epoch: 11868 mean train loss:  1.39519284e-02, bound:  3.15878898e-01\n",
      "Epoch: 11869 mean train loss:  1.42563563e-02, bound:  3.15877706e-01\n",
      "Epoch: 11870 mean train loss:  1.46401208e-02, bound:  3.15878749e-01\n",
      "Epoch: 11871 mean train loss:  1.50494352e-02, bound:  3.15877140e-01\n",
      "Epoch: 11872 mean train loss:  1.53769637e-02, bound:  3.15878510e-01\n",
      "Epoch: 11873 mean train loss:  1.54359303e-02, bound:  3.15876663e-01\n",
      "Epoch: 11874 mean train loss:  1.50962705e-02, bound:  3.15878034e-01\n",
      "Epoch: 11875 mean train loss:  1.43969962e-02, bound:  3.15876454e-01\n",
      "Epoch: 11876 mean train loss:  1.36500057e-02, bound:  3.15877318e-01\n",
      "Epoch: 11877 mean train loss:  1.32303685e-02, bound:  3.15876663e-01\n",
      "Epoch: 11878 mean train loss:  1.32928286e-02, bound:  3.15876454e-01\n",
      "Epoch: 11879 mean train loss:  1.36551354e-02, bound:  3.15876871e-01\n",
      "Epoch: 11880 mean train loss:  1.39607545e-02, bound:  3.15875918e-01\n",
      "Epoch: 11881 mean train loss:  1.39503861e-02, bound:  3.15876782e-01\n",
      "Epoch: 11882 mean train loss:  1.36421602e-02, bound:  3.15875828e-01\n",
      "Epoch: 11883 mean train loss:  1.33009301e-02, bound:  3.15876275e-01\n",
      "Epoch: 11884 mean train loss:  1.31812366e-02, bound:  3.15876007e-01\n",
      "Epoch: 11885 mean train loss:  1.33123687e-02, bound:  3.15875679e-01\n",
      "Epoch: 11886 mean train loss:  1.35114631e-02, bound:  3.15876037e-01\n",
      "Epoch: 11887 mean train loss:  1.35734584e-02, bound:  3.15875232e-01\n",
      "Epoch: 11888 mean train loss:  1.34495813e-02, bound:  3.15875709e-01\n",
      "Epoch: 11889 mean train loss:  1.32588493e-02, bound:  3.15875083e-01\n",
      "Epoch: 11890 mean train loss:  1.31585756e-02, bound:  3.15875113e-01\n",
      "Epoch: 11891 mean train loss:  1.31999543e-02, bound:  3.15875024e-01\n",
      "Epoch: 11892 mean train loss:  1.33059220e-02, bound:  3.15874517e-01\n",
      "Epoch: 11893 mean train loss:  1.33633045e-02, bound:  3.15874845e-01\n",
      "Epoch: 11894 mean train loss:  1.33218700e-02, bound:  3.15874130e-01\n",
      "Epoch: 11895 mean train loss:  1.32225212e-02, bound:  3.15874398e-01\n",
      "Epoch: 11896 mean train loss:  1.31450212e-02, bound:  3.15873951e-01\n",
      "Epoch: 11897 mean train loss:  1.31363990e-02, bound:  3.15873802e-01\n",
      "Epoch: 11898 mean train loss:  1.31813437e-02, bound:  3.15873802e-01\n",
      "Epoch: 11899 mean train loss:  1.32273370e-02, bound:  3.15873265e-01\n",
      "Epoch: 11900 mean train loss:  1.32324239e-02, bound:  3.15873533e-01\n",
      "Epoch: 11901 mean train loss:  1.31940264e-02, bound:  3.15872937e-01\n",
      "Epoch: 11902 mean train loss:  1.31413694e-02, bound:  3.15873027e-01\n",
      "Epoch: 11903 mean train loss:  1.31083559e-02, bound:  3.15872699e-01\n",
      "Epoch: 11904 mean train loss:  1.31087434e-02, bound:  3.15872461e-01\n",
      "Epoch: 11905 mean train loss:  1.31305410e-02, bound:  3.15872401e-01\n",
      "Epoch: 11906 mean train loss:  1.31501453e-02, bound:  3.15871954e-01\n",
      "Epoch: 11907 mean train loss:  1.31507302e-02, bound:  3.15872073e-01\n",
      "Epoch: 11908 mean train loss:  1.31315915e-02, bound:  3.15871596e-01\n",
      "Epoch: 11909 mean train loss:  1.31043699e-02, bound:  3.15871626e-01\n",
      "Epoch: 11910 mean train loss:  1.30837420e-02, bound:  3.15871358e-01\n",
      "Epoch: 11911 mean train loss:  1.30771697e-02, bound:  3.15871179e-01\n",
      "Epoch: 11912 mean train loss:  1.30822426e-02, bound:  3.15871090e-01\n",
      "Epoch: 11913 mean train loss:  1.30911302e-02, bound:  3.15870821e-01\n",
      "Epoch: 11914 mean train loss:  1.30945770e-02, bound:  3.15870821e-01\n",
      "Epoch: 11915 mean train loss:  1.30892126e-02, bound:  3.15870494e-01\n",
      "Epoch: 11916 mean train loss:  1.30767133e-02, bound:  3.15870494e-01\n",
      "Epoch: 11917 mean train loss:  1.30624082e-02, bound:  3.15870225e-01\n",
      "Epoch: 11918 mean train loss:  1.30521171e-02, bound:  3.15870196e-01\n",
      "Epoch: 11919 mean train loss:  1.30475527e-02, bound:  3.15870017e-01\n",
      "Epoch: 11920 mean train loss:  1.30477911e-02, bound:  3.15869838e-01\n",
      "Epoch: 11921 mean train loss:  1.30494833e-02, bound:  3.15869808e-01\n",
      "Epoch: 11922 mean train loss:  1.30495168e-02, bound:  3.15869540e-01\n",
      "Epoch: 11923 mean train loss:  1.30461976e-02, bound:  3.15869540e-01\n",
      "Epoch: 11924 mean train loss:  1.30396793e-02, bound:  3.15869302e-01\n",
      "Epoch: 11925 mean train loss:  1.30316401e-02, bound:  3.15869302e-01\n",
      "Epoch: 11926 mean train loss:  1.30242128e-02, bound:  3.15869093e-01\n",
      "Epoch: 11927 mean train loss:  1.30187897e-02, bound:  3.15868974e-01\n",
      "Epoch: 11928 mean train loss:  1.30154639e-02, bound:  3.15868855e-01\n",
      "Epoch: 11929 mean train loss:  1.30137829e-02, bound:  3.15868706e-01\n",
      "Epoch: 11930 mean train loss:  1.30124986e-02, bound:  3.15868646e-01\n",
      "Epoch: 11931 mean train loss:  1.30103510e-02, bound:  3.15868407e-01\n",
      "Epoch: 11932 mean train loss:  1.30070709e-02, bound:  3.15868407e-01\n",
      "Epoch: 11933 mean train loss:  1.30026396e-02, bound:  3.15868199e-01\n",
      "Epoch: 11934 mean train loss:  1.29973134e-02, bound:  3.15868080e-01\n",
      "Epoch: 11935 mean train loss:  1.29921222e-02, bound:  3.15867931e-01\n",
      "Epoch: 11936 mean train loss:  1.29873706e-02, bound:  3.15867841e-01\n",
      "Epoch: 11937 mean train loss:  1.29831498e-02, bound:  3.15867662e-01\n",
      "Epoch: 11938 mean train loss:  1.29796090e-02, bound:  3.15867543e-01\n",
      "Epoch: 11939 mean train loss:  1.29764788e-02, bound:  3.15867394e-01\n",
      "Epoch: 11940 mean train loss:  1.29735908e-02, bound:  3.15867275e-01\n",
      "Epoch: 11941 mean train loss:  1.29705220e-02, bound:  3.15867156e-01\n",
      "Epoch: 11942 mean train loss:  1.29672475e-02, bound:  3.15866977e-01\n",
      "Epoch: 11943 mean train loss:  1.29638091e-02, bound:  3.15866858e-01\n",
      "Epoch: 11944 mean train loss:  1.29599571e-02, bound:  3.15866649e-01\n",
      "Epoch: 11945 mean train loss:  1.29560661e-02, bound:  3.15866560e-01\n",
      "Epoch: 11946 mean train loss:  1.29519943e-02, bound:  3.15866381e-01\n",
      "Epoch: 11947 mean train loss:  1.29481517e-02, bound:  3.15866232e-01\n",
      "Epoch: 11948 mean train loss:  1.29443090e-02, bound:  3.15866083e-01\n",
      "Epoch: 11949 mean train loss:  1.29406573e-02, bound:  3.15865964e-01\n",
      "Epoch: 11950 mean train loss:  1.29369479e-02, bound:  3.15865785e-01\n",
      "Epoch: 11951 mean train loss:  1.29336547e-02, bound:  3.15865636e-01\n",
      "Epoch: 11952 mean train loss:  1.29302889e-02, bound:  3.15865517e-01\n",
      "Epoch: 11953 mean train loss:  1.29267387e-02, bound:  3.15865338e-01\n",
      "Epoch: 11954 mean train loss:  1.29234120e-02, bound:  3.15865219e-01\n",
      "Epoch: 11955 mean train loss:  1.29198302e-02, bound:  3.15865010e-01\n",
      "Epoch: 11956 mean train loss:  1.29162660e-02, bound:  3.15864891e-01\n",
      "Epoch: 11957 mean train loss:  1.29127866e-02, bound:  3.15864712e-01\n",
      "Epoch: 11958 mean train loss:  1.29090343e-02, bound:  3.15864563e-01\n",
      "Epoch: 11959 mean train loss:  1.29055101e-02, bound:  3.15864414e-01\n",
      "Epoch: 11960 mean train loss:  1.29017923e-02, bound:  3.15864235e-01\n",
      "Epoch: 11961 mean train loss:  1.28982551e-02, bound:  3.15864086e-01\n",
      "Epoch: 11962 mean train loss:  1.28946640e-02, bound:  3.15863907e-01\n",
      "Epoch: 11963 mean train loss:  1.28911287e-02, bound:  3.15863758e-01\n",
      "Epoch: 11964 mean train loss:  1.28875207e-02, bound:  3.15863639e-01\n",
      "Epoch: 11965 mean train loss:  1.28839808e-02, bound:  3.15863460e-01\n",
      "Epoch: 11966 mean train loss:  1.28803598e-02, bound:  3.15863281e-01\n",
      "Epoch: 11967 mean train loss:  1.28769567e-02, bound:  3.15863103e-01\n",
      "Epoch: 11968 mean train loss:  1.28733823e-02, bound:  3.15862983e-01\n",
      "Epoch: 11969 mean train loss:  1.28699010e-02, bound:  3.15862805e-01\n",
      "Epoch: 11970 mean train loss:  1.28664002e-02, bound:  3.15862656e-01\n",
      "Epoch: 11971 mean train loss:  1.28628677e-02, bound:  3.15862477e-01\n",
      "Epoch: 11972 mean train loss:  1.28591992e-02, bound:  3.15862358e-01\n",
      "Epoch: 11973 mean train loss:  1.28557635e-02, bound:  3.15862209e-01\n",
      "Epoch: 11974 mean train loss:  1.28522469e-02, bound:  3.15862030e-01\n",
      "Epoch: 11975 mean train loss:  1.28486706e-02, bound:  3.15861881e-01\n",
      "Epoch: 11976 mean train loss:  1.28452685e-02, bound:  3.15861702e-01\n",
      "Epoch: 11977 mean train loss:  1.28418505e-02, bound:  3.15861553e-01\n",
      "Epoch: 11978 mean train loss:  1.28381271e-02, bound:  3.15861374e-01\n",
      "Epoch: 11979 mean train loss:  1.28346151e-02, bound:  3.15861255e-01\n",
      "Epoch: 11980 mean train loss:  1.28310481e-02, bound:  3.15861076e-01\n",
      "Epoch: 11981 mean train loss:  1.28276078e-02, bound:  3.15860957e-01\n",
      "Epoch: 11982 mean train loss:  1.28240269e-02, bound:  3.15860778e-01\n",
      "Epoch: 11983 mean train loss:  1.28206071e-02, bound:  3.15860629e-01\n",
      "Epoch: 11984 mean train loss:  1.28170876e-02, bound:  3.15860450e-01\n",
      "Epoch: 11985 mean train loss:  1.28137479e-02, bound:  3.15860331e-01\n",
      "Epoch: 11986 mean train loss:  1.28101520e-02, bound:  3.15860152e-01\n",
      "Epoch: 11987 mean train loss:  1.28066577e-02, bound:  3.15860033e-01\n",
      "Epoch: 11988 mean train loss:  1.28032956e-02, bound:  3.15859884e-01\n",
      "Epoch: 11989 mean train loss:  1.27997855e-02, bound:  3.15859675e-01\n",
      "Epoch: 11990 mean train loss:  1.27963331e-02, bound:  3.15859497e-01\n",
      "Epoch: 11991 mean train loss:  1.27929607e-02, bound:  3.15859377e-01\n",
      "Epoch: 11992 mean train loss:  1.27895866e-02, bound:  3.15859199e-01\n",
      "Epoch: 11993 mean train loss:  1.27862748e-02, bound:  3.15859079e-01\n",
      "Epoch: 11994 mean train loss:  1.27830030e-02, bound:  3.15858901e-01\n",
      "Epoch: 11995 mean train loss:  1.27798021e-02, bound:  3.15858781e-01\n",
      "Epoch: 11996 mean train loss:  1.27767818e-02, bound:  3.15858573e-01\n",
      "Epoch: 11997 mean train loss:  1.27739459e-02, bound:  3.15858454e-01\n",
      "Epoch: 11998 mean train loss:  1.27713354e-02, bound:  3.15858215e-01\n",
      "Epoch: 11999 mean train loss:  1.27691859e-02, bound:  3.15858155e-01\n",
      "Epoch: 12000 mean train loss:  1.27674257e-02, bound:  3.15857917e-01\n",
      "Epoch: 12001 mean train loss:  1.27667449e-02, bound:  3.15857828e-01\n",
      "Epoch: 12002 mean train loss:  1.27673419e-02, bound:  3.15857589e-01\n",
      "Epoch: 12003 mean train loss:  1.27700306e-02, bound:  3.15857530e-01\n",
      "Epoch: 12004 mean train loss:  1.27757657e-02, bound:  3.15857232e-01\n",
      "Epoch: 12005 mean train loss:  1.27862170e-02, bound:  3.15857261e-01\n",
      "Epoch: 12006 mean train loss:  1.28036803e-02, bound:  3.15856874e-01\n",
      "Epoch: 12007 mean train loss:  1.28320158e-02, bound:  3.15857023e-01\n",
      "Epoch: 12008 mean train loss:  1.28763737e-02, bound:  3.15856457e-01\n",
      "Epoch: 12009 mean train loss:  1.29451789e-02, bound:  3.15856725e-01\n",
      "Epoch: 12010 mean train loss:  1.30493650e-02, bound:  3.15856040e-01\n",
      "Epoch: 12011 mean train loss:  1.32045345e-02, bound:  3.15856576e-01\n",
      "Epoch: 12012 mean train loss:  1.34270499e-02, bound:  3.15855592e-01\n",
      "Epoch: 12013 mean train loss:  1.37341926e-02, bound:  3.15856397e-01\n",
      "Epoch: 12014 mean train loss:  1.41204111e-02, bound:  3.15855026e-01\n",
      "Epoch: 12015 mean train loss:  1.45468181e-02, bound:  3.15856218e-01\n",
      "Epoch: 12016 mean train loss:  1.48907388e-02, bound:  3.15854490e-01\n",
      "Epoch: 12017 mean train loss:  1.49835879e-02, bound:  3.15855950e-01\n",
      "Epoch: 12018 mean train loss:  1.46559523e-02, bound:  3.15854162e-01\n",
      "Epoch: 12019 mean train loss:  1.39550772e-02, bound:  3.15855354e-01\n",
      "Epoch: 12020 mean train loss:  1.31799076e-02, bound:  3.15854162e-01\n",
      "Epoch: 12021 mean train loss:  1.27298646e-02, bound:  3.15854520e-01\n",
      "Epoch: 12022 mean train loss:  1.27790822e-02, bound:  3.15854460e-01\n",
      "Epoch: 12023 mean train loss:  1.31488657e-02, bound:  3.15853804e-01\n",
      "Epoch: 12024 mean train loss:  1.34695796e-02, bound:  3.15854549e-01\n",
      "Epoch: 12025 mean train loss:  1.34629002e-02, bound:  3.15853477e-01\n",
      "Epoch: 12026 mean train loss:  1.31486710e-02, bound:  3.15854251e-01\n",
      "Epoch: 12027 mean train loss:  1.27967903e-02, bound:  3.15853596e-01\n",
      "Epoch: 12028 mean train loss:  1.26757799e-02, bound:  3.15853626e-01\n",
      "Epoch: 12029 mean train loss:  1.28156841e-02, bound:  3.15853745e-01\n",
      "Epoch: 12030 mean train loss:  1.30211832e-02, bound:  3.15853119e-01\n",
      "Epoch: 12031 mean train loss:  1.30793443e-02, bound:  3.15853596e-01\n",
      "Epoch: 12032 mean train loss:  1.29432855e-02, bound:  3.15852851e-01\n",
      "Epoch: 12033 mean train loss:  1.27469385e-02, bound:  3.15853149e-01\n",
      "Epoch: 12034 mean train loss:  1.26531227e-02, bound:  3.15852791e-01\n",
      "Epoch: 12035 mean train loss:  1.27065508e-02, bound:  3.15852582e-01\n",
      "Epoch: 12036 mean train loss:  1.28183691e-02, bound:  3.15852731e-01\n",
      "Epoch: 12037 mean train loss:  1.28690694e-02, bound:  3.15852046e-01\n",
      "Epoch: 12038 mean train loss:  1.28152203e-02, bound:  3.15852404e-01\n",
      "Epoch: 12039 mean train loss:  1.27092302e-02, bound:  3.15851808e-01\n",
      "Epoch: 12040 mean train loss:  1.26368832e-02, bound:  3.15851867e-01\n",
      "Epoch: 12041 mean train loss:  1.26403365e-02, bound:  3.15851718e-01\n",
      "Epoch: 12042 mean train loss:  1.26929898e-02, bound:  3.15851301e-01\n",
      "Epoch: 12043 mean train loss:  1.27359899e-02, bound:  3.15851480e-01\n",
      "Epoch: 12044 mean train loss:  1.27307018e-02, bound:  3.15850914e-01\n",
      "Epoch: 12045 mean train loss:  1.26832640e-02, bound:  3.15851063e-01\n",
      "Epoch: 12046 mean train loss:  1.26304077e-02, bound:  3.15850586e-01\n",
      "Epoch: 12047 mean train loss:  1.26054380e-02, bound:  3.15850496e-01\n",
      "Epoch: 12048 mean train loss:  1.26153547e-02, bound:  3.15850377e-01\n",
      "Epoch: 12049 mean train loss:  1.26410099e-02, bound:  3.15849960e-01\n",
      "Epoch: 12050 mean train loss:  1.26569699e-02, bound:  3.15850079e-01\n",
      "Epoch: 12051 mean train loss:  1.26498183e-02, bound:  3.15849602e-01\n",
      "Epoch: 12052 mean train loss:  1.26245106e-02, bound:  3.15849632e-01\n",
      "Epoch: 12053 mean train loss:  1.25971157e-02, bound:  3.15849304e-01\n",
      "Epoch: 12054 mean train loss:  1.25815719e-02, bound:  3.15849185e-01\n",
      "Epoch: 12055 mean train loss:  1.25814071e-02, bound:  3.15849066e-01\n",
      "Epoch: 12056 mean train loss:  1.25907389e-02, bound:  3.15848768e-01\n",
      "Epoch: 12057 mean train loss:  1.25988340e-02, bound:  3.15848798e-01\n",
      "Epoch: 12058 mean train loss:  1.25983329e-02, bound:  3.15848470e-01\n",
      "Epoch: 12059 mean train loss:  1.25883203e-02, bound:  3.15848500e-01\n",
      "Epoch: 12060 mean train loss:  1.25734722e-02, bound:  3.15848261e-01\n",
      "Epoch: 12061 mean train loss:  1.25606125e-02, bound:  3.15848172e-01\n",
      "Epoch: 12062 mean train loss:  1.25537459e-02, bound:  3.15847993e-01\n",
      "Epoch: 12063 mean train loss:  1.25527317e-02, bound:  3.15847844e-01\n",
      "Epoch: 12064 mean train loss:  1.25551457e-02, bound:  3.15847844e-01\n",
      "Epoch: 12065 mean train loss:  1.25569990e-02, bound:  3.15847605e-01\n",
      "Epoch: 12066 mean train loss:  1.25555471e-02, bound:  3.15847605e-01\n",
      "Epoch: 12067 mean train loss:  1.25501109e-02, bound:  3.15847337e-01\n",
      "Epoch: 12068 mean train loss:  1.25418752e-02, bound:  3.15847337e-01\n",
      "Epoch: 12069 mean train loss:  1.25336917e-02, bound:  3.15847158e-01\n",
      "Epoch: 12070 mean train loss:  1.25274798e-02, bound:  3.15847039e-01\n",
      "Epoch: 12071 mean train loss:  1.25237899e-02, bound:  3.15846920e-01\n",
      "Epoch: 12072 mean train loss:  1.25222281e-02, bound:  3.15846771e-01\n",
      "Epoch: 12073 mean train loss:  1.25215100e-02, bound:  3.15846741e-01\n",
      "Epoch: 12074 mean train loss:  1.25199761e-02, bound:  3.15846503e-01\n",
      "Epoch: 12075 mean train loss:  1.25171747e-02, bound:  3.15846503e-01\n",
      "Epoch: 12076 mean train loss:  1.25130126e-02, bound:  3.15846294e-01\n",
      "Epoch: 12077 mean train loss:  1.25078075e-02, bound:  3.15846205e-01\n",
      "Epoch: 12078 mean train loss:  1.25025529e-02, bound:  3.15846086e-01\n",
      "Epoch: 12079 mean train loss:  1.24977482e-02, bound:  3.15845966e-01\n",
      "Epoch: 12080 mean train loss:  1.24939680e-02, bound:  3.15845788e-01\n",
      "Epoch: 12081 mean train loss:  1.24908509e-02, bound:  3.15845728e-01\n",
      "Epoch: 12082 mean train loss:  1.24883093e-02, bound:  3.15845609e-01\n",
      "Epoch: 12083 mean train loss:  1.24859689e-02, bound:  3.15845430e-01\n",
      "Epoch: 12084 mean train loss:  1.24832494e-02, bound:  3.15845311e-01\n",
      "Epoch: 12085 mean train loss:  1.24802990e-02, bound:  3.15845132e-01\n",
      "Epoch: 12086 mean train loss:  1.24766175e-02, bound:  3.15845072e-01\n",
      "Epoch: 12087 mean train loss:  1.24729248e-02, bound:  3.15844864e-01\n",
      "Epoch: 12088 mean train loss:  1.24690961e-02, bound:  3.15844774e-01\n",
      "Epoch: 12089 mean train loss:  1.24649685e-02, bound:  3.15844625e-01\n",
      "Epoch: 12090 mean train loss:  1.24613391e-02, bound:  3.15844446e-01\n",
      "Epoch: 12091 mean train loss:  1.24577498e-02, bound:  3.15844327e-01\n",
      "Epoch: 12092 mean train loss:  1.24545973e-02, bound:  3.15844178e-01\n",
      "Epoch: 12093 mean train loss:  1.24513311e-02, bound:  3.15844059e-01\n",
      "Epoch: 12094 mean train loss:  1.24483034e-02, bound:  3.15843880e-01\n",
      "Epoch: 12095 mean train loss:  1.24451742e-02, bound:  3.15843761e-01\n",
      "Epoch: 12096 mean train loss:  1.24421194e-02, bound:  3.15843612e-01\n",
      "Epoch: 12097 mean train loss:  1.24386838e-02, bound:  3.15843463e-01\n",
      "Epoch: 12098 mean train loss:  1.24355117e-02, bound:  3.15843314e-01\n",
      "Epoch: 12099 mean train loss:  1.24321654e-02, bound:  3.15843195e-01\n",
      "Epoch: 12100 mean train loss:  1.24286683e-02, bound:  3.15842986e-01\n",
      "Epoch: 12101 mean train loss:  1.24252634e-02, bound:  3.15842867e-01\n",
      "Epoch: 12102 mean train loss:  1.24218473e-02, bound:  3.15842718e-01\n",
      "Epoch: 12103 mean train loss:  1.24184424e-02, bound:  3.15842569e-01\n",
      "Epoch: 12104 mean train loss:  1.24150757e-02, bound:  3.15842420e-01\n",
      "Epoch: 12105 mean train loss:  1.24117984e-02, bound:  3.15842241e-01\n",
      "Epoch: 12106 mean train loss:  1.24083953e-02, bound:  3.15842122e-01\n",
      "Epoch: 12107 mean train loss:  1.24051096e-02, bound:  3.15841973e-01\n",
      "Epoch: 12108 mean train loss:  1.24019245e-02, bound:  3.15841854e-01\n",
      "Epoch: 12109 mean train loss:  1.23985661e-02, bound:  3.15841645e-01\n",
      "Epoch: 12110 mean train loss:  1.23954862e-02, bound:  3.15841526e-01\n",
      "Epoch: 12111 mean train loss:  1.23920674e-02, bound:  3.15841377e-01\n",
      "Epoch: 12112 mean train loss:  1.23889185e-02, bound:  3.15841198e-01\n",
      "Epoch: 12113 mean train loss:  1.23856617e-02, bound:  3.15841079e-01\n",
      "Epoch: 12114 mean train loss:  1.23824328e-02, bound:  3.15840900e-01\n",
      "Epoch: 12115 mean train loss:  1.23791164e-02, bound:  3.15840751e-01\n",
      "Epoch: 12116 mean train loss:  1.23758418e-02, bound:  3.15840632e-01\n",
      "Epoch: 12117 mean train loss:  1.23726707e-02, bound:  3.15840453e-01\n",
      "Epoch: 12118 mean train loss:  1.23693701e-02, bound:  3.15840334e-01\n",
      "Epoch: 12119 mean train loss:  1.23659456e-02, bound:  3.15840185e-01\n",
      "Epoch: 12120 mean train loss:  1.23627679e-02, bound:  3.15840006e-01\n",
      "Epoch: 12121 mean train loss:  1.23594450e-02, bound:  3.15839887e-01\n",
      "Epoch: 12122 mean train loss:  1.23561239e-02, bound:  3.15839708e-01\n",
      "Epoch: 12123 mean train loss:  1.23528047e-02, bound:  3.15839559e-01\n",
      "Epoch: 12124 mean train loss:  1.23494584e-02, bound:  3.15839440e-01\n",
      "Epoch: 12125 mean train loss:  1.23462286e-02, bound:  3.15839291e-01\n",
      "Epoch: 12126 mean train loss:  1.23429438e-02, bound:  3.15839142e-01\n",
      "Epoch: 12127 mean train loss:  1.23396525e-02, bound:  3.15838993e-01\n",
      "Epoch: 12128 mean train loss:  1.23364050e-02, bound:  3.15838844e-01\n",
      "Epoch: 12129 mean train loss:  1.23330606e-02, bound:  3.15838665e-01\n",
      "Epoch: 12130 mean train loss:  1.23299332e-02, bound:  3.15838546e-01\n",
      "Epoch: 12131 mean train loss:  1.23265339e-02, bound:  3.15838367e-01\n",
      "Epoch: 12132 mean train loss:  1.23232827e-02, bound:  3.15838218e-01\n",
      "Epoch: 12133 mean train loss:  1.23200770e-02, bound:  3.15838069e-01\n",
      "Epoch: 12134 mean train loss:  1.23168034e-02, bound:  3.15837920e-01\n",
      "Epoch: 12135 mean train loss:  1.23133920e-02, bound:  3.15837771e-01\n",
      "Epoch: 12136 mean train loss:  1.23103093e-02, bound:  3.15837651e-01\n",
      "Epoch: 12137 mean train loss:  1.23071065e-02, bound:  3.15837473e-01\n",
      "Epoch: 12138 mean train loss:  1.23038664e-02, bound:  3.15837324e-01\n",
      "Epoch: 12139 mean train loss:  1.23007558e-02, bound:  3.15837145e-01\n",
      "Epoch: 12140 mean train loss:  1.22976750e-02, bound:  3.15837026e-01\n",
      "Epoch: 12141 mean train loss:  1.22944321e-02, bound:  3.15836877e-01\n",
      "Epoch: 12142 mean train loss:  1.22915730e-02, bound:  3.15836757e-01\n",
      "Epoch: 12143 mean train loss:  1.22886002e-02, bound:  3.15836549e-01\n",
      "Epoch: 12144 mean train loss:  1.22858118e-02, bound:  3.15836430e-01\n",
      "Epoch: 12145 mean train loss:  1.22830523e-02, bound:  3.15836251e-01\n",
      "Epoch: 12146 mean train loss:  1.22810276e-02, bound:  3.15836132e-01\n",
      "Epoch: 12147 mean train loss:  1.22793810e-02, bound:  3.15835923e-01\n",
      "Epoch: 12148 mean train loss:  1.22783994e-02, bound:  3.15835834e-01\n",
      "Epoch: 12149 mean train loss:  1.22786285e-02, bound:  3.15835595e-01\n",
      "Epoch: 12150 mean train loss:  1.22806206e-02, bound:  3.15835565e-01\n",
      "Epoch: 12151 mean train loss:  1.22850528e-02, bound:  3.15835267e-01\n",
      "Epoch: 12152 mean train loss:  1.22936340e-02, bound:  3.15835267e-01\n",
      "Epoch: 12153 mean train loss:  1.23080518e-02, bound:  3.15834910e-01\n",
      "Epoch: 12154 mean train loss:  1.23321526e-02, bound:  3.15835029e-01\n",
      "Epoch: 12155 mean train loss:  1.23707885e-02, bound:  3.15834552e-01\n",
      "Epoch: 12156 mean train loss:  1.24320956e-02, bound:  3.15834820e-01\n",
      "Epoch: 12157 mean train loss:  1.25275515e-02, bound:  3.15834165e-01\n",
      "Epoch: 12158 mean train loss:  1.26740681e-02, bound:  3.15834612e-01\n",
      "Epoch: 12159 mean train loss:  1.28912488e-02, bound:  3.15833688e-01\n",
      "Epoch: 12160 mean train loss:  1.32035865e-02, bound:  3.15834463e-01\n",
      "Epoch: 12161 mean train loss:  1.36186443e-02, bound:  3.15833151e-01\n",
      "Epoch: 12162 mean train loss:  1.41134234e-02, bound:  3.15834343e-01\n",
      "Epoch: 12163 mean train loss:  1.45716835e-02, bound:  3.15832615e-01\n",
      "Epoch: 12164 mean train loss:  1.48009118e-02, bound:  3.15834135e-01\n",
      "Epoch: 12165 mean train loss:  1.45612387e-02, bound:  3.15832257e-01\n",
      "Epoch: 12166 mean train loss:  1.38246603e-02, bound:  3.15833569e-01\n",
      "Epoch: 12167 mean train loss:  1.28937643e-02, bound:  3.15832287e-01\n",
      "Epoch: 12168 mean train loss:  1.22863688e-02, bound:  3.15832704e-01\n",
      "Epoch 12170: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 12169 mean train loss:  1.22883841e-02, bound:  3.15832585e-01\n",
      "Epoch: 12170 mean train loss:  1.27195306e-02, bound:  3.15831929e-01\n",
      "Epoch: 12171 mean train loss:  1.25087900e-02, bound:  3.15832019e-01\n",
      "Epoch: 12172 mean train loss:  1.22488439e-02, bound:  3.15832198e-01\n",
      "Epoch: 12173 mean train loss:  1.22407936e-02, bound:  3.15832376e-01\n",
      "Epoch: 12174 mean train loss:  1.24211386e-02, bound:  3.15832555e-01\n",
      "Epoch: 12175 mean train loss:  1.25087574e-02, bound:  3.15832585e-01\n",
      "Epoch: 12176 mean train loss:  1.23925852e-02, bound:  3.15832555e-01\n",
      "Epoch: 12177 mean train loss:  1.22354599e-02, bound:  3.15832376e-01\n",
      "Epoch: 12178 mean train loss:  1.22215860e-02, bound:  3.15832257e-01\n",
      "Epoch: 12179 mean train loss:  1.23255253e-02, bound:  3.15832138e-01\n",
      "Epoch: 12180 mean train loss:  1.23831201e-02, bound:  3.15832049e-01\n",
      "Epoch: 12181 mean train loss:  1.23194698e-02, bound:  3.15832108e-01\n",
      "Epoch: 12182 mean train loss:  1.22247841e-02, bound:  3.15832198e-01\n",
      "Epoch: 12183 mean train loss:  1.22111719e-02, bound:  3.15832317e-01\n",
      "Epoch: 12184 mean train loss:  1.22707970e-02, bound:  3.15832376e-01\n",
      "Epoch: 12185 mean train loss:  1.23079112e-02, bound:  3.15832436e-01\n",
      "Epoch: 12186 mean train loss:  1.22729558e-02, bound:  3.15832376e-01\n",
      "Epoch: 12187 mean train loss:  1.22160120e-02, bound:  3.15832317e-01\n",
      "Epoch: 12188 mean train loss:  1.22049125e-02, bound:  3.15832227e-01\n",
      "Epoch: 12189 mean train loss:  1.22390855e-02, bound:  3.15832138e-01\n",
      "Epoch: 12190 mean train loss:  1.22623593e-02, bound:  3.15832138e-01\n",
      "Epoch: 12191 mean train loss:  1.22432038e-02, bound:  3.15832138e-01\n",
      "Epoch: 12192 mean train loss:  1.22090150e-02, bound:  3.15832168e-01\n",
      "Epoch: 12193 mean train loss:  1.22009404e-02, bound:  3.15832257e-01\n",
      "Epoch: 12194 mean train loss:  1.22203687e-02, bound:  3.15832257e-01\n",
      "Epoch: 12195 mean train loss:  1.22348089e-02, bound:  3.15832287e-01\n",
      "Epoch: 12196 mean train loss:  1.22241098e-02, bound:  3.15832257e-01\n",
      "Epoch: 12197 mean train loss:  1.22034438e-02, bound:  3.15832227e-01\n",
      "Epoch: 12198 mean train loss:  1.21978708e-02, bound:  3.15832168e-01\n",
      "Epoch: 12199 mean train loss:  1.22089647e-02, bound:  3.15832108e-01\n",
      "Epoch: 12200 mean train loss:  1.22176288e-02, bound:  3.15832049e-01\n",
      "Epoch: 12201 mean train loss:  1.22115733e-02, bound:  3.15832049e-01\n",
      "Epoch: 12202 mean train loss:  1.21990694e-02, bound:  3.15832108e-01\n",
      "Epoch: 12203 mean train loss:  1.21953553e-02, bound:  3.15832138e-01\n",
      "Epoch: 12204 mean train loss:  1.22016128e-02, bound:  3.15832138e-01\n",
      "Epoch: 12205 mean train loss:  1.22067528e-02, bound:  3.15832168e-01\n",
      "Epoch: 12206 mean train loss:  1.22031551e-02, bound:  3.15832138e-01\n",
      "Epoch: 12207 mean train loss:  1.21956281e-02, bound:  3.15832078e-01\n",
      "Epoch: 12208 mean train loss:  1.21930279e-02, bound:  3.15832049e-01\n",
      "Epoch: 12209 mean train loss:  1.21965511e-02, bound:  3.15831989e-01\n",
      "Epoch: 12210 mean train loss:  1.21995453e-02, bound:  3.15831959e-01\n",
      "Epoch: 12211 mean train loss:  1.21973809e-02, bound:  3.15831959e-01\n",
      "Epoch: 12212 mean train loss:  1.21927522e-02, bound:  3.15831959e-01\n",
      "Epoch: 12213 mean train loss:  1.21909557e-02, bound:  3.15831989e-01\n",
      "Epoch: 12214 mean train loss:  1.21929618e-02, bound:  3.15831989e-01\n",
      "Epoch: 12215 mean train loss:  1.21944519e-02, bound:  3.15831959e-01\n",
      "Epoch: 12216 mean train loss:  1.21930167e-02, bound:  3.15831959e-01\n",
      "Epoch: 12217 mean train loss:  1.21901222e-02, bound:  3.15831929e-01\n",
      "Epoch: 12218 mean train loss:  1.21889198e-02, bound:  3.15831900e-01\n",
      "Epoch: 12219 mean train loss:  1.21897962e-02, bound:  3.15831870e-01\n",
      "Epoch: 12220 mean train loss:  1.21908393e-02, bound:  3.15831840e-01\n",
      "Epoch: 12221 mean train loss:  1.21897170e-02, bound:  3.15831840e-01\n",
      "Epoch: 12222 mean train loss:  1.21879401e-02, bound:  3.15831840e-01\n",
      "Epoch: 12223 mean train loss:  1.21870935e-02, bound:  3.15831840e-01\n",
      "Epoch: 12224 mean train loss:  1.21874744e-02, bound:  3.15831810e-01\n",
      "Epoch: 12225 mean train loss:  1.21878041e-02, bound:  3.15831810e-01\n",
      "Epoch: 12226 mean train loss:  1.21870348e-02, bound:  3.15831780e-01\n",
      "Epoch: 12227 mean train loss:  1.21858343e-02, bound:  3.15831780e-01\n",
      "Epoch: 12228 mean train loss:  1.21851657e-02, bound:  3.15831721e-01\n",
      "Epoch: 12229 mean train loss:  1.21852867e-02, bound:  3.15831691e-01\n",
      "Epoch: 12230 mean train loss:  1.21853314e-02, bound:  3.15831691e-01\n",
      "Epoch: 12231 mean train loss:  1.21847438e-02, bound:  3.15831691e-01\n",
      "Epoch: 12232 mean train loss:  1.21839549e-02, bound:  3.15831661e-01\n",
      "Epoch: 12233 mean train loss:  1.21833794e-02, bound:  3.15831661e-01\n",
      "Epoch: 12234 mean train loss:  1.21833906e-02, bound:  3.15831661e-01\n",
      "Epoch: 12235 mean train loss:  1.21832313e-02, bound:  3.15831602e-01\n",
      "Epoch: 12236 mean train loss:  1.21827396e-02, bound:  3.15831602e-01\n",
      "Epoch: 12237 mean train loss:  1.21820886e-02, bound:  3.15831602e-01\n",
      "Epoch: 12238 mean train loss:  1.21816117e-02, bound:  3.15831572e-01\n",
      "Epoch: 12239 mean train loss:  1.21814115e-02, bound:  3.15831572e-01\n",
      "Epoch: 12240 mean train loss:  1.21812597e-02, bound:  3.15831542e-01\n",
      "Epoch: 12241 mean train loss:  1.21807354e-02, bound:  3.15831512e-01\n",
      "Epoch: 12242 mean train loss:  1.21801607e-02, bound:  3.15831512e-01\n",
      "Epoch: 12243 mean train loss:  1.21799130e-02, bound:  3.15831482e-01\n",
      "Epoch: 12244 mean train loss:  1.21796215e-02, bound:  3.15831453e-01\n",
      "Epoch: 12245 mean train loss:  1.21792946e-02, bound:  3.15831453e-01\n",
      "Epoch: 12246 mean train loss:  1.21788494e-02, bound:  3.15831453e-01\n",
      "Epoch: 12247 mean train loss:  1.21783549e-02, bound:  3.15831453e-01\n",
      "Epoch: 12248 mean train loss:  1.21782059e-02, bound:  3.15831393e-01\n",
      "Epoch: 12249 mean train loss:  1.21778212e-02, bound:  3.15831363e-01\n",
      "Epoch: 12250 mean train loss:  1.21775530e-02, bound:  3.15831333e-01\n",
      "Epoch: 12251 mean train loss:  1.21770548e-02, bound:  3.15831333e-01\n",
      "Epoch: 12252 mean train loss:  1.21767493e-02, bound:  3.15831333e-01\n",
      "Epoch: 12253 mean train loss:  1.21762929e-02, bound:  3.15831333e-01\n",
      "Epoch: 12254 mean train loss:  1.21759977e-02, bound:  3.15831274e-01\n",
      "Epoch: 12255 mean train loss:  1.21756531e-02, bound:  3.15831274e-01\n",
      "Epoch: 12256 mean train loss:  1.21754324e-02, bound:  3.15831244e-01\n",
      "Epoch: 12257 mean train loss:  1.21749407e-02, bound:  3.15831244e-01\n",
      "Epoch: 12258 mean train loss:  1.21745896e-02, bound:  3.15831244e-01\n",
      "Epoch: 12259 mean train loss:  1.21742589e-02, bound:  3.15831214e-01\n",
      "Epoch: 12260 mean train loss:  1.21740270e-02, bound:  3.15831155e-01\n",
      "Epoch: 12261 mean train loss:  1.21735903e-02, bound:  3.15831155e-01\n",
      "Epoch: 12262 mean train loss:  1.21732214e-02, bound:  3.15831155e-01\n",
      "Epoch: 12263 mean train loss:  1.21729467e-02, bound:  3.15831125e-01\n",
      "Epoch: 12264 mean train loss:  1.21726291e-02, bound:  3.15831125e-01\n",
      "Epoch: 12265 mean train loss:  1.21721430e-02, bound:  3.15831125e-01\n",
      "Epoch: 12266 mean train loss:  1.21718142e-02, bound:  3.15831065e-01\n",
      "Epoch: 12267 mean train loss:  1.21714557e-02, bound:  3.15831035e-01\n",
      "Epoch: 12268 mean train loss:  1.21712312e-02, bound:  3.15831035e-01\n",
      "Epoch: 12269 mean train loss:  1.21708252e-02, bound:  3.15831006e-01\n",
      "Epoch: 12270 mean train loss:  1.21704452e-02, bound:  3.15831006e-01\n",
      "Epoch: 12271 mean train loss:  1.21701164e-02, bound:  3.15831006e-01\n",
      "Epoch: 12272 mean train loss:  1.21698594e-02, bound:  3.15830946e-01\n",
      "Epoch: 12273 mean train loss:  1.21694896e-02, bound:  3.15830946e-01\n",
      "Epoch: 12274 mean train loss:  1.21690677e-02, bound:  3.15830946e-01\n",
      "Epoch: 12275 mean train loss:  1.21687585e-02, bound:  3.15830946e-01\n",
      "Epoch: 12276 mean train loss:  1.21686496e-02, bound:  3.15830886e-01\n",
      "Epoch: 12277 mean train loss:  1.21681578e-02, bound:  3.15830886e-01\n",
      "Epoch: 12278 mean train loss:  1.21678254e-02, bound:  3.15830886e-01\n",
      "Epoch: 12279 mean train loss:  1.21674780e-02, bound:  3.15830827e-01\n",
      "Epoch: 12280 mean train loss:  1.21671166e-02, bound:  3.15830827e-01\n",
      "Epoch: 12281 mean train loss:  1.21667963e-02, bound:  3.15830827e-01\n",
      "Epoch: 12282 mean train loss:  1.21663995e-02, bound:  3.15830827e-01\n",
      "Epoch: 12283 mean train loss:  1.21661201e-02, bound:  3.15830767e-01\n",
      "Epoch: 12284 mean train loss:  1.21657541e-02, bound:  3.15830767e-01\n",
      "Epoch: 12285 mean train loss:  1.21655101e-02, bound:  3.15830737e-01\n",
      "Epoch: 12286 mean train loss:  1.21651217e-02, bound:  3.15830737e-01\n",
      "Epoch: 12287 mean train loss:  1.21646989e-02, bound:  3.15830737e-01\n",
      "Epoch: 12288 mean train loss:  1.21644353e-02, bound:  3.15830708e-01\n",
      "Epoch: 12289 mean train loss:  1.21641057e-02, bound:  3.15830678e-01\n",
      "Epoch: 12290 mean train loss:  1.21637862e-02, bound:  3.15830678e-01\n",
      "Epoch: 12291 mean train loss:  1.21634072e-02, bound:  3.15830678e-01\n",
      "Epoch: 12292 mean train loss:  1.21630626e-02, bound:  3.15830618e-01\n",
      "Epoch: 12293 mean train loss:  1.21626640e-02, bound:  3.15830618e-01\n",
      "Epoch: 12294 mean train loss:  1.21623743e-02, bound:  3.15830588e-01\n",
      "Epoch: 12295 mean train loss:  1.21619999e-02, bound:  3.15830588e-01\n",
      "Epoch: 12296 mean train loss:  1.21618053e-02, bound:  3.15830588e-01\n",
      "Epoch: 12297 mean train loss:  1.21613909e-02, bound:  3.15830559e-01\n",
      "Epoch: 12298 mean train loss:  1.21610286e-02, bound:  3.15830559e-01\n",
      "Epoch: 12299 mean train loss:  1.21606141e-02, bound:  3.15830499e-01\n",
      "Epoch: 12300 mean train loss:  1.21602984e-02, bound:  3.15830499e-01\n",
      "Epoch: 12301 mean train loss:  1.21600339e-02, bound:  3.15830469e-01\n",
      "Epoch: 12302 mean train loss:  1.21595515e-02, bound:  3.15830469e-01\n",
      "Epoch: 12303 mean train loss:  1.21593317e-02, bound:  3.15830439e-01\n",
      "Epoch: 12304 mean train loss:  1.21590989e-02, bound:  3.15830439e-01\n",
      "Epoch: 12305 mean train loss:  1.21586807e-02, bound:  3.15830439e-01\n",
      "Epoch: 12306 mean train loss:  1.21583445e-02, bound:  3.15830380e-01\n",
      "Epoch: 12307 mean train loss:  1.21580185e-02, bound:  3.15830380e-01\n",
      "Epoch: 12308 mean train loss:  1.21576590e-02, bound:  3.15830350e-01\n",
      "Epoch: 12309 mean train loss:  1.21574197e-02, bound:  3.15830350e-01\n",
      "Epoch: 12310 mean train loss:  1.21569699e-02, bound:  3.15830350e-01\n",
      "Epoch: 12311 mean train loss:  1.21567529e-02, bound:  3.15830320e-01\n",
      "Epoch: 12312 mean train loss:  1.21563282e-02, bound:  3.15830320e-01\n",
      "Epoch: 12313 mean train loss:  1.21559650e-02, bound:  3.15830261e-01\n",
      "Epoch: 12314 mean train loss:  1.21556735e-02, bound:  3.15830261e-01\n",
      "Epoch: 12315 mean train loss:  1.21552553e-02, bound:  3.15830261e-01\n",
      "Epoch: 12316 mean train loss:  1.21549666e-02, bound:  3.15830231e-01\n",
      "Epoch: 12317 mean train loss:  1.21545931e-02, bound:  3.15830231e-01\n",
      "Epoch: 12318 mean train loss:  1.21543016e-02, bound:  3.15830171e-01\n",
      "Epoch: 12319 mean train loss:  1.21539645e-02, bound:  3.15830171e-01\n",
      "Epoch: 12320 mean train loss:  1.21536413e-02, bound:  3.15830141e-01\n",
      "Epoch: 12321 mean train loss:  1.21533675e-02, bound:  3.15830141e-01\n",
      "Epoch: 12322 mean train loss:  1.21529866e-02, bound:  3.15830141e-01\n",
      "Epoch: 12323 mean train loss:  1.21525796e-02, bound:  3.15830141e-01\n",
      "Epoch: 12324 mean train loss:  1.21523635e-02, bound:  3.15830141e-01\n",
      "Epoch: 12325 mean train loss:  1.21519817e-02, bound:  3.15830082e-01\n",
      "Epoch: 12326 mean train loss:  1.21516306e-02, bound:  3.15830082e-01\n",
      "Epoch: 12327 mean train loss:  1.21512972e-02, bound:  3.15830082e-01\n",
      "Epoch: 12328 mean train loss:  1.21508939e-02, bound:  3.15830022e-01\n",
      "Epoch: 12329 mean train loss:  1.21506015e-02, bound:  3.15830022e-01\n",
      "Epoch: 12330 mean train loss:  1.21502597e-02, bound:  3.15830022e-01\n",
      "Epoch: 12331 mean train loss:  1.21499756e-02, bound:  3.15830022e-01\n",
      "Epoch: 12332 mean train loss:  1.21496459e-02, bound:  3.15830022e-01\n",
      "Epoch: 12333 mean train loss:  1.21492222e-02, bound:  3.15829962e-01\n",
      "Epoch: 12334 mean train loss:  1.21489372e-02, bound:  3.15829962e-01\n",
      "Epoch: 12335 mean train loss:  1.21485731e-02, bound:  3.15829903e-01\n",
      "Epoch: 12336 mean train loss:  1.21482471e-02, bound:  3.15829903e-01\n",
      "Epoch: 12337 mean train loss:  1.21479211e-02, bound:  3.15829903e-01\n",
      "Epoch: 12338 mean train loss:  1.21476054e-02, bound:  3.15829903e-01\n",
      "Epoch: 12339 mean train loss:  1.21472329e-02, bound:  3.15829903e-01\n",
      "Epoch: 12340 mean train loss:  1.21469432e-02, bound:  3.15829843e-01\n",
      "Epoch: 12341 mean train loss:  1.21465325e-02, bound:  3.15829843e-01\n",
      "Epoch: 12342 mean train loss:  1.21462150e-02, bound:  3.15829813e-01\n",
      "Epoch: 12343 mean train loss:  1.21459169e-02, bound:  3.15829813e-01\n",
      "Epoch: 12344 mean train loss:  1.21455574e-02, bound:  3.15829813e-01\n",
      "Epoch: 12345 mean train loss:  1.21452482e-02, bound:  3.15829784e-01\n",
      "Epoch: 12346 mean train loss:  1.21448888e-02, bound:  3.15829784e-01\n",
      "Epoch: 12347 mean train loss:  1.21446457e-02, bound:  3.15829724e-01\n",
      "Epoch: 12348 mean train loss:  1.21442610e-02, bound:  3.15829724e-01\n",
      "Epoch: 12349 mean train loss:  1.21438559e-02, bound:  3.15829724e-01\n",
      "Epoch: 12350 mean train loss:  1.21435439e-02, bound:  3.15829694e-01\n",
      "Epoch: 12351 mean train loss:  1.21432142e-02, bound:  3.15829694e-01\n",
      "Epoch: 12352 mean train loss:  1.21428510e-02, bound:  3.15829664e-01\n",
      "Epoch: 12353 mean train loss:  1.21425232e-02, bound:  3.15829664e-01\n",
      "Epoch: 12354 mean train loss:  1.21421302e-02, bound:  3.15829664e-01\n",
      "Epoch: 12355 mean train loss:  1.21419150e-02, bound:  3.15829605e-01\n",
      "Epoch: 12356 mean train loss:  1.21415202e-02, bound:  3.15829605e-01\n",
      "Epoch: 12357 mean train loss:  1.21412035e-02, bound:  3.15829575e-01\n",
      "Epoch: 12358 mean train loss:  1.21408887e-02, bound:  3.15829575e-01\n",
      "Epoch: 12359 mean train loss:  1.21405385e-02, bound:  3.15829575e-01\n",
      "Epoch: 12360 mean train loss:  1.21401455e-02, bound:  3.15829515e-01\n",
      "Epoch: 12361 mean train loss:  1.21398484e-02, bound:  3.15829515e-01\n",
      "Epoch: 12362 mean train loss:  1.21395439e-02, bound:  3.15829515e-01\n",
      "Epoch: 12363 mean train loss:  1.21390997e-02, bound:  3.15829515e-01\n",
      "Epoch: 12364 mean train loss:  1.21388473e-02, bound:  3.15829515e-01\n",
      "Epoch: 12365 mean train loss:  1.21384822e-02, bound:  3.15829456e-01\n",
      "Epoch: 12366 mean train loss:  1.21381124e-02, bound:  3.15829456e-01\n",
      "Epoch: 12367 mean train loss:  1.21378405e-02, bound:  3.15829396e-01\n",
      "Epoch: 12368 mean train loss:  1.21375164e-02, bound:  3.15829396e-01\n",
      "Epoch: 12369 mean train loss:  1.21371606e-02, bound:  3.15829396e-01\n",
      "Epoch: 12370 mean train loss:  1.21368077e-02, bound:  3.15829396e-01\n",
      "Epoch: 12371 mean train loss:  1.21364836e-02, bound:  3.15829396e-01\n",
      "Epoch: 12372 mean train loss:  1.21361585e-02, bound:  3.15829337e-01\n",
      "Epoch: 12373 mean train loss:  1.21357273e-02, bound:  3.15829337e-01\n",
      "Epoch: 12374 mean train loss:  1.21353958e-02, bound:  3.15829337e-01\n",
      "Epoch: 12375 mean train loss:  1.21350707e-02, bound:  3.15829337e-01\n",
      "Epoch: 12376 mean train loss:  1.21348603e-02, bound:  3.15829337e-01\n",
      "Epoch: 12377 mean train loss:  1.21344794e-02, bound:  3.15829277e-01\n",
      "Epoch: 12378 mean train loss:  1.21340612e-02, bound:  3.15829277e-01\n",
      "Epoch: 12379 mean train loss:  1.21337110e-02, bound:  3.15829277e-01\n",
      "Epoch: 12380 mean train loss:  1.21334540e-02, bound:  3.15829247e-01\n",
      "Epoch: 12381 mean train loss:  1.21331075e-02, bound:  3.15829247e-01\n",
      "Epoch: 12382 mean train loss:  1.21327043e-02, bound:  3.15829247e-01\n",
      "Epoch: 12383 mean train loss:  1.21325115e-02, bound:  3.15829188e-01\n",
      "Epoch: 12384 mean train loss:  1.21320346e-02, bound:  3.15829188e-01\n",
      "Epoch: 12385 mean train loss:  1.21316547e-02, bound:  3.15829158e-01\n",
      "Epoch: 12386 mean train loss:  1.21313753e-02, bound:  3.15829158e-01\n",
      "Epoch: 12387 mean train loss:  1.21309571e-02, bound:  3.15829128e-01\n",
      "Epoch: 12388 mean train loss:  1.21307103e-02, bound:  3.15829128e-01\n",
      "Epoch: 12389 mean train loss:  1.21303294e-02, bound:  3.15829128e-01\n",
      "Epoch: 12390 mean train loss:  1.21300025e-02, bound:  3.15829068e-01\n",
      "Epoch: 12391 mean train loss:  1.21296234e-02, bound:  3.15829068e-01\n",
      "Epoch: 12392 mean train loss:  1.21293627e-02, bound:  3.15829039e-01\n",
      "Epoch: 12393 mean train loss:  1.21289659e-02, bound:  3.15829039e-01\n",
      "Epoch: 12394 mean train loss:  1.21287052e-02, bound:  3.15829039e-01\n",
      "Epoch: 12395 mean train loss:  1.21283587e-02, bound:  3.15829009e-01\n",
      "Epoch: 12396 mean train loss:  1.21280039e-02, bound:  3.15829009e-01\n",
      "Epoch: 12397 mean train loss:  1.21275708e-02, bound:  3.15828949e-01\n",
      "Epoch: 12398 mean train loss:  1.21273119e-02, bound:  3.15828949e-01\n",
      "Epoch: 12399 mean train loss:  1.21269580e-02, bound:  3.15828949e-01\n",
      "Epoch: 12400 mean train loss:  1.21266125e-02, bound:  3.15828919e-01\n",
      "Epoch: 12401 mean train loss:  1.21261738e-02, bound:  3.15828919e-01\n",
      "Epoch: 12402 mean train loss:  1.21259214e-02, bound:  3.15828919e-01\n",
      "Epoch: 12403 mean train loss:  1.21255815e-02, bound:  3.15828890e-01\n",
      "Epoch: 12404 mean train loss:  1.21252667e-02, bound:  3.15828860e-01\n",
      "Epoch: 12405 mean train loss:  1.21249137e-02, bound:  3.15828830e-01\n",
      "Epoch: 12406 mean train loss:  1.21245347e-02, bound:  3.15828830e-01\n",
      "Epoch: 12407 mean train loss:  1.21241361e-02, bound:  3.15828800e-01\n",
      "Epoch: 12408 mean train loss:  1.21238669e-02, bound:  3.15828800e-01\n",
      "Epoch: 12409 mean train loss:  1.21235410e-02, bound:  3.15828800e-01\n",
      "Epoch: 12410 mean train loss:  1.21232644e-02, bound:  3.15828770e-01\n",
      "Epoch: 12411 mean train loss:  1.21227885e-02, bound:  3.15828770e-01\n",
      "Epoch: 12412 mean train loss:  1.21225575e-02, bound:  3.15828741e-01\n",
      "Epoch: 12413 mean train loss:  1.21220928e-02, bound:  3.15828711e-01\n",
      "Epoch: 12414 mean train loss:  1.21218236e-02, bound:  3.15828711e-01\n",
      "Epoch: 12415 mean train loss:  1.21214809e-02, bound:  3.15828711e-01\n",
      "Epoch: 12416 mean train loss:  1.21212145e-02, bound:  3.15828711e-01\n",
      "Epoch: 12417 mean train loss:  1.21207703e-02, bound:  3.15828681e-01\n",
      "Epoch: 12418 mean train loss:  1.21206911e-02, bound:  3.15828681e-01\n",
      "Epoch: 12419 mean train loss:  1.21201314e-02, bound:  3.15828621e-01\n",
      "Epoch: 12420 mean train loss:  1.21197933e-02, bound:  3.15828592e-01\n",
      "Epoch: 12421 mean train loss:  1.21193901e-02, bound:  3.15828592e-01\n",
      "Epoch: 12422 mean train loss:  1.21192727e-02, bound:  3.15828592e-01\n",
      "Epoch: 12423 mean train loss:  1.21186934e-02, bound:  3.15828592e-01\n",
      "Epoch: 12424 mean train loss:  1.21184792e-02, bound:  3.15828532e-01\n",
      "Epoch: 12425 mean train loss:  1.21180732e-02, bound:  3.15828532e-01\n",
      "Epoch: 12426 mean train loss:  1.21178441e-02, bound:  3.15828502e-01\n",
      "Epoch: 12427 mean train loss:  1.21173896e-02, bound:  3.15828502e-01\n",
      "Epoch: 12428 mean train loss:  1.21171270e-02, bound:  3.15828502e-01\n",
      "Epoch: 12429 mean train loss:  1.21166846e-02, bound:  3.15828502e-01\n",
      "Epoch: 12430 mean train loss:  1.21164080e-02, bound:  3.15828472e-01\n",
      "Epoch: 12431 mean train loss:  1.21159973e-02, bound:  3.15828472e-01\n",
      "Epoch: 12432 mean train loss:  1.21156275e-02, bound:  3.15828413e-01\n",
      "Epoch: 12433 mean train loss:  1.21153183e-02, bound:  3.15828413e-01\n",
      "Epoch: 12434 mean train loss:  1.21150361e-02, bound:  3.15828413e-01\n",
      "Epoch: 12435 mean train loss:  1.21146459e-02, bound:  3.15828383e-01\n",
      "Epoch: 12436 mean train loss:  1.21143200e-02, bound:  3.15828383e-01\n",
      "Epoch: 12437 mean train loss:  1.21140424e-02, bound:  3.15828383e-01\n",
      "Epoch: 12438 mean train loss:  1.21136662e-02, bound:  3.15828353e-01\n",
      "Epoch: 12439 mean train loss:  1.21132443e-02, bound:  3.15828353e-01\n",
      "Epoch: 12440 mean train loss:  1.21130003e-02, bound:  3.15828294e-01\n",
      "Epoch: 12441 mean train loss:  1.21125216e-02, bound:  3.15828294e-01\n",
      "Epoch: 12442 mean train loss:  1.21122729e-02, bound:  3.15828294e-01\n",
      "Epoch: 12443 mean train loss:  1.21119609e-02, bound:  3.15828264e-01\n",
      "Epoch: 12444 mean train loss:  1.21115064e-02, bound:  3.15828264e-01\n",
      "Epoch: 12445 mean train loss:  1.21111739e-02, bound:  3.15828234e-01\n",
      "Epoch: 12446 mean train loss:  1.21109467e-02, bound:  3.15828234e-01\n",
      "Epoch: 12447 mean train loss:  1.21104699e-02, bound:  3.15828174e-01\n",
      "Epoch: 12448 mean train loss:  1.21101364e-02, bound:  3.15828174e-01\n",
      "Epoch: 12449 mean train loss:  1.21097574e-02, bound:  3.15828174e-01\n",
      "Epoch: 12450 mean train loss:  1.21095534e-02, bound:  3.15828145e-01\n",
      "Epoch: 12451 mean train loss:  1.21090468e-02, bound:  3.15828145e-01\n",
      "Epoch: 12452 mean train loss:  1.21087832e-02, bound:  3.15828145e-01\n",
      "Epoch: 12453 mean train loss:  1.21083381e-02, bound:  3.15828115e-01\n",
      "Epoch: 12454 mean train loss:  1.21081611e-02, bound:  3.15828115e-01\n",
      "Epoch: 12455 mean train loss:  1.21077728e-02, bound:  3.15828085e-01\n",
      "Epoch: 12456 mean train loss:  1.21073490e-02, bound:  3.15828085e-01\n",
      "Epoch: 12457 mean train loss:  1.21070277e-02, bound:  3.15828085e-01\n",
      "Epoch: 12458 mean train loss:  1.21067083e-02, bound:  3.15828025e-01\n",
      "Epoch: 12459 mean train loss:  1.21064289e-02, bound:  3.15828025e-01\n",
      "Epoch: 12460 mean train loss:  1.21059408e-02, bound:  3.15827996e-01\n",
      "Epoch: 12461 mean train loss:  1.21056689e-02, bound:  3.15827996e-01\n",
      "Epoch: 12462 mean train loss:  1.21053662e-02, bound:  3.15827996e-01\n",
      "Epoch: 12463 mean train loss:  1.21049769e-02, bound:  3.15827966e-01\n",
      "Epoch: 12464 mean train loss:  1.21047096e-02, bound:  3.15827966e-01\n",
      "Epoch: 12465 mean train loss:  1.21041788e-02, bound:  3.15827906e-01\n",
      "Epoch: 12466 mean train loss:  1.21039506e-02, bound:  3.15827906e-01\n",
      "Epoch: 12467 mean train loss:  1.21036200e-02, bound:  3.15827906e-01\n",
      "Epoch: 12468 mean train loss:  1.21032195e-02, bound:  3.15827906e-01\n",
      "Epoch: 12469 mean train loss:  1.21028796e-02, bound:  3.15827906e-01\n",
      "Epoch: 12470 mean train loss:  1.21025676e-02, bound:  3.15827847e-01\n",
      "Epoch: 12471 mean train loss:  1.21021336e-02, bound:  3.15827847e-01\n",
      "Epoch: 12472 mean train loss:  1.21019240e-02, bound:  3.15827847e-01\n",
      "Epoch: 12473 mean train loss:  1.21014882e-02, bound:  3.15827817e-01\n",
      "Epoch: 12474 mean train loss:  1.21012088e-02, bound:  3.15827817e-01\n",
      "Epoch: 12475 mean train loss:  1.21007636e-02, bound:  3.15827787e-01\n",
      "Epoch: 12476 mean train loss:  1.21005280e-02, bound:  3.15827787e-01\n",
      "Epoch: 12477 mean train loss:  1.21001052e-02, bound:  3.15827787e-01\n",
      "Epoch: 12478 mean train loss:  1.20998565e-02, bound:  3.15827727e-01\n",
      "Epoch: 12479 mean train loss:  1.20993443e-02, bound:  3.15827727e-01\n",
      "Epoch: 12480 mean train loss:  1.20990342e-02, bound:  3.15827698e-01\n",
      "Epoch: 12481 mean train loss:  1.20986747e-02, bound:  3.15827698e-01\n",
      "Epoch: 12482 mean train loss:  1.20983887e-02, bound:  3.15827698e-01\n",
      "Epoch: 12483 mean train loss:  1.20979445e-02, bound:  3.15827668e-01\n",
      "Epoch: 12484 mean train loss:  1.20976605e-02, bound:  3.15827668e-01\n",
      "Epoch: 12485 mean train loss:  1.20972721e-02, bound:  3.15827608e-01\n",
      "Epoch: 12486 mean train loss:  1.20969852e-02, bound:  3.15827608e-01\n",
      "Epoch: 12487 mean train loss:  1.20965745e-02, bound:  3.15827608e-01\n",
      "Epoch: 12488 mean train loss:  1.20962663e-02, bound:  3.15827578e-01\n",
      "Epoch: 12489 mean train loss:  1.20958667e-02, bound:  3.15827578e-01\n",
      "Epoch: 12490 mean train loss:  1.20955780e-02, bound:  3.15827519e-01\n",
      "Epoch: 12491 mean train loss:  1.20950779e-02, bound:  3.15827519e-01\n",
      "Epoch: 12492 mean train loss:  1.20948311e-02, bound:  3.15827519e-01\n",
      "Epoch: 12493 mean train loss:  1.20943636e-02, bound:  3.15827489e-01\n",
      "Epoch: 12494 mean train loss:  1.20942239e-02, bound:  3.15827489e-01\n",
      "Epoch: 12495 mean train loss:  1.20937573e-02, bound:  3.15827459e-01\n",
      "Epoch: 12496 mean train loss:  1.20935589e-02, bound:  3.15827459e-01\n",
      "Epoch: 12497 mean train loss:  1.20929908e-02, bound:  3.15827459e-01\n",
      "Epoch: 12498 mean train loss:  1.20927598e-02, bound:  3.15827399e-01\n",
      "Epoch: 12499 mean train loss:  1.20923240e-02, bound:  3.15827399e-01\n",
      "Epoch: 12500 mean train loss:  1.20919812e-02, bound:  3.15827399e-01\n",
      "Epoch: 12501 mean train loss:  1.20916162e-02, bound:  3.15827399e-01\n",
      "Epoch: 12502 mean train loss:  1.20913461e-02, bound:  3.15827340e-01\n",
      "Epoch: 12503 mean train loss:  1.20908236e-02, bound:  3.15827340e-01\n",
      "Epoch: 12504 mean train loss:  1.20906243e-02, bound:  3.15827340e-01\n",
      "Epoch: 12505 mean train loss:  1.20901717e-02, bound:  3.15827280e-01\n",
      "Epoch: 12506 mean train loss:  1.20899230e-02, bound:  3.15827280e-01\n",
      "Epoch: 12507 mean train loss:  1.20894285e-02, bound:  3.15827280e-01\n",
      "Epoch: 12508 mean train loss:  1.20891351e-02, bound:  3.15827280e-01\n",
      "Epoch: 12509 mean train loss:  1.20888520e-02, bound:  3.15827280e-01\n",
      "Epoch: 12510 mean train loss:  1.20885987e-02, bound:  3.15827250e-01\n",
      "Epoch: 12511 mean train loss:  1.20880753e-02, bound:  3.15827250e-01\n",
      "Epoch: 12512 mean train loss:  1.20877083e-02, bound:  3.15827161e-01\n",
      "Epoch: 12513 mean train loss:  1.20873442e-02, bound:  3.15827161e-01\n",
      "Epoch: 12514 mean train loss:  1.20870471e-02, bound:  3.15827161e-01\n",
      "Epoch: 12515 mean train loss:  1.20865637e-02, bound:  3.15827161e-01\n",
      "Epoch: 12516 mean train loss:  1.20862909e-02, bound:  3.15827161e-01\n",
      "Epoch: 12517 mean train loss:  1.20860096e-02, bound:  3.15827131e-01\n",
      "Epoch: 12518 mean train loss:  1.20856585e-02, bound:  3.15827131e-01\n",
      "Epoch: 12519 mean train loss:  1.20852971e-02, bound:  3.15827072e-01\n",
      "Epoch: 12520 mean train loss:  1.20849181e-02, bound:  3.15827072e-01\n",
      "Epoch: 12521 mean train loss:  1.20845651e-02, bound:  3.15827072e-01\n",
      "Epoch: 12522 mean train loss:  1.20842513e-02, bound:  3.15827042e-01\n",
      "Epoch: 12523 mean train loss:  1.20838350e-02, bound:  3.15827042e-01\n",
      "Epoch: 12524 mean train loss:  1.20835155e-02, bound:  3.15827012e-01\n",
      "Epoch: 12525 mean train loss:  1.20830871e-02, bound:  3.15826982e-01\n",
      "Epoch: 12526 mean train loss:  1.20828580e-02, bound:  3.15826982e-01\n",
      "Epoch: 12527 mean train loss:  1.20824194e-02, bound:  3.15826952e-01\n",
      "Epoch: 12528 mean train loss:  1.20820953e-02, bound:  3.15826952e-01\n",
      "Epoch: 12529 mean train loss:  1.20816194e-02, bound:  3.15826952e-01\n",
      "Epoch: 12530 mean train loss:  1.20812962e-02, bound:  3.15826923e-01\n",
      "Epoch: 12531 mean train loss:  1.20808985e-02, bound:  3.15826923e-01\n",
      "Epoch: 12532 mean train loss:  1.20806014e-02, bound:  3.15826863e-01\n",
      "Epoch: 12533 mean train loss:  1.20802438e-02, bound:  3.15826863e-01\n",
      "Epoch: 12534 mean train loss:  1.20799523e-02, bound:  3.15826863e-01\n",
      "Epoch: 12535 mean train loss:  1.20796058e-02, bound:  3.15826833e-01\n",
      "Epoch: 12536 mean train loss:  1.20792370e-02, bound:  3.15826833e-01\n",
      "Epoch: 12537 mean train loss:  1.20789185e-02, bound:  3.15826803e-01\n",
      "Epoch: 12538 mean train loss:  1.20785097e-02, bound:  3.15826803e-01\n",
      "Epoch: 12539 mean train loss:  1.20781548e-02, bound:  3.15826774e-01\n",
      "Epoch: 12540 mean train loss:  1.20776892e-02, bound:  3.15826774e-01\n",
      "Epoch: 12541 mean train loss:  1.20773511e-02, bound:  3.15826714e-01\n",
      "Epoch: 12542 mean train loss:  1.20771499e-02, bound:  3.15826714e-01\n",
      "Epoch: 12543 mean train loss:  1.20767141e-02, bound:  3.15826714e-01\n",
      "Epoch: 12544 mean train loss:  1.20762801e-02, bound:  3.15826684e-01\n",
      "Epoch: 12545 mean train loss:  1.20759457e-02, bound:  3.15826684e-01\n",
      "Epoch: 12546 mean train loss:  1.20756021e-02, bound:  3.15826684e-01\n",
      "Epoch: 12547 mean train loss:  1.20752510e-02, bound:  3.15826654e-01\n",
      "Epoch: 12548 mean train loss:  1.20748691e-02, bound:  3.15826654e-01\n",
      "Epoch: 12549 mean train loss:  1.20745553e-02, bound:  3.15826595e-01\n",
      "Epoch: 12550 mean train loss:  1.20742796e-02, bound:  3.15826595e-01\n",
      "Epoch: 12551 mean train loss:  1.20737897e-02, bound:  3.15826565e-01\n",
      "Epoch: 12552 mean train loss:  1.20734498e-02, bound:  3.15826565e-01\n",
      "Epoch: 12553 mean train loss:  1.20731648e-02, bound:  3.15826565e-01\n",
      "Epoch: 12554 mean train loss:  1.20728333e-02, bound:  3.15826535e-01\n",
      "Epoch: 12555 mean train loss:  1.20724039e-02, bound:  3.15826535e-01\n",
      "Epoch: 12556 mean train loss:  1.20720416e-02, bound:  3.15826476e-01\n",
      "Epoch: 12557 mean train loss:  1.20716672e-02, bound:  3.15826476e-01\n",
      "Epoch: 12558 mean train loss:  1.20713422e-02, bound:  3.15826476e-01\n",
      "Epoch: 12559 mean train loss:  1.20709483e-02, bound:  3.15826476e-01\n",
      "Epoch: 12560 mean train loss:  1.20706204e-02, bound:  3.15826476e-01\n",
      "Epoch: 12561 mean train loss:  1.20702600e-02, bound:  3.15826416e-01\n",
      "Epoch: 12562 mean train loss:  1.20699238e-02, bound:  3.15826416e-01\n",
      "Epoch: 12563 mean train loss:  1.20694768e-02, bound:  3.15826386e-01\n",
      "Epoch: 12564 mean train loss:  1.20691741e-02, bound:  3.15826386e-01\n",
      "Epoch: 12565 mean train loss:  1.20687764e-02, bound:  3.15826386e-01\n",
      "Epoch: 12566 mean train loss:  1.20684626e-02, bound:  3.15826356e-01\n",
      "Epoch: 12567 mean train loss:  1.20680537e-02, bound:  3.15826356e-01\n",
      "Epoch: 12568 mean train loss:  1.20677128e-02, bound:  3.15826297e-01\n",
      "Epoch: 12569 mean train loss:  1.20673059e-02, bound:  3.15826297e-01\n",
      "Epoch: 12570 mean train loss:  1.20669156e-02, bound:  3.15826297e-01\n",
      "Epoch: 12571 mean train loss:  1.20666185e-02, bound:  3.15826267e-01\n",
      "Epoch: 12572 mean train loss:  1.20662777e-02, bound:  3.15826267e-01\n",
      "Epoch: 12573 mean train loss:  1.20657515e-02, bound:  3.15826237e-01\n",
      "Epoch: 12574 mean train loss:  1.20655820e-02, bound:  3.15826237e-01\n",
      "Epoch: 12575 mean train loss:  1.20650781e-02, bound:  3.15826178e-01\n",
      "Epoch: 12576 mean train loss:  1.20646711e-02, bound:  3.15826178e-01\n",
      "Epoch: 12577 mean train loss:  1.20643610e-02, bound:  3.15826178e-01\n",
      "Epoch: 12578 mean train loss:  1.20641189e-02, bound:  3.15826148e-01\n",
      "Epoch: 12579 mean train loss:  1.20636448e-02, bound:  3.15826148e-01\n",
      "Epoch: 12580 mean train loss:  1.20633142e-02, bound:  3.15826118e-01\n",
      "Epoch: 12581 mean train loss:  1.20630693e-02, bound:  3.15826118e-01\n",
      "Epoch: 12582 mean train loss:  1.20627284e-02, bound:  3.15826088e-01\n",
      "Epoch: 12583 mean train loss:  1.20622227e-02, bound:  3.15826058e-01\n",
      "Epoch: 12584 mean train loss:  1.20618679e-02, bound:  3.15826058e-01\n",
      "Epoch: 12585 mean train loss:  1.20614087e-02, bound:  3.15826029e-01\n",
      "Epoch: 12586 mean train loss:  1.20611060e-02, bound:  3.15826029e-01\n",
      "Epoch: 12587 mean train loss:  1.20607000e-02, bound:  3.15825969e-01\n",
      "Epoch: 12588 mean train loss:  1.20604085e-02, bound:  3.15825969e-01\n",
      "Epoch: 12589 mean train loss:  1.20599903e-02, bound:  3.15825969e-01\n",
      "Epoch: 12590 mean train loss:  1.20597286e-02, bound:  3.15825969e-01\n",
      "Epoch: 12591 mean train loss:  1.20591726e-02, bound:  3.15825969e-01\n",
      "Epoch: 12592 mean train loss:  1.20590059e-02, bound:  3.15825909e-01\n",
      "Epoch: 12593 mean train loss:  1.20586520e-02, bound:  3.15825909e-01\n",
      "Epoch: 12594 mean train loss:  1.20583242e-02, bound:  3.15825850e-01\n",
      "Epoch: 12595 mean train loss:  1.20578548e-02, bound:  3.15825850e-01\n",
      "Epoch: 12596 mean train loss:  1.20574273e-02, bound:  3.15825850e-01\n",
      "Epoch: 12597 mean train loss:  1.20570883e-02, bound:  3.15825850e-01\n",
      "Epoch: 12598 mean train loss:  1.20568061e-02, bound:  3.15825850e-01\n",
      "Epoch: 12599 mean train loss:  1.20563004e-02, bound:  3.15825820e-01\n",
      "Epoch: 12600 mean train loss:  1.20560508e-02, bound:  3.15825820e-01\n",
      "Epoch: 12601 mean train loss:  1.20556094e-02, bound:  3.15825731e-01\n",
      "Epoch: 12602 mean train loss:  1.20553197e-02, bound:  3.15825731e-01\n",
      "Epoch: 12603 mean train loss:  1.20549370e-02, bound:  3.15825731e-01\n",
      "Epoch: 12604 mean train loss:  1.20545253e-02, bound:  3.15825731e-01\n",
      "Epoch: 12605 mean train loss:  1.20541370e-02, bound:  3.15825731e-01\n",
      "Epoch: 12606 mean train loss:  1.20538417e-02, bound:  3.15825701e-01\n",
      "Epoch: 12607 mean train loss:  1.20534310e-02, bound:  3.15825701e-01\n",
      "Epoch: 12608 mean train loss:  1.20530985e-02, bound:  3.15825701e-01\n",
      "Epoch: 12609 mean train loss:  1.20527353e-02, bound:  3.15825641e-01\n",
      "Epoch: 12610 mean train loss:  1.20523954e-02, bound:  3.15825611e-01\n",
      "Epoch: 12611 mean train loss:  1.20520210e-02, bound:  3.15825611e-01\n",
      "Epoch: 12612 mean train loss:  1.20515507e-02, bound:  3.15825611e-01\n",
      "Epoch: 12613 mean train loss:  1.20512238e-02, bound:  3.15825582e-01\n",
      "Epoch: 12614 mean train loss:  1.20508652e-02, bound:  3.15825582e-01\n",
      "Epoch: 12615 mean train loss:  1.20504415e-02, bound:  3.15825522e-01\n",
      "Epoch: 12616 mean train loss:  1.20501397e-02, bound:  3.15825522e-01\n",
      "Epoch: 12617 mean train loss:  1.20497281e-02, bound:  3.15825522e-01\n",
      "Epoch: 12618 mean train loss:  1.20494608e-02, bound:  3.15825522e-01\n",
      "Epoch: 12619 mean train loss:  1.20490110e-02, bound:  3.15825462e-01\n",
      "Epoch: 12620 mean train loss:  1.20486403e-02, bound:  3.15825462e-01\n",
      "Epoch: 12621 mean train loss:  1.20483097e-02, bound:  3.15825462e-01\n",
      "Epoch: 12622 mean train loss:  1.20479185e-02, bound:  3.15825462e-01\n",
      "Epoch: 12623 mean train loss:  1.20474771e-02, bound:  3.15825403e-01\n",
      "Epoch: 12624 mean train loss:  1.20471604e-02, bound:  3.15825403e-01\n",
      "Epoch: 12625 mean train loss:  1.20468074e-02, bound:  3.15825373e-01\n",
      "Epoch: 12626 mean train loss:  1.20465141e-02, bound:  3.15825373e-01\n",
      "Epoch: 12627 mean train loss:  1.20460875e-02, bound:  3.15825343e-01\n",
      "Epoch: 12628 mean train loss:  1.20456992e-02, bound:  3.15825343e-01\n",
      "Epoch: 12629 mean train loss:  1.20452829e-02, bound:  3.15825313e-01\n",
      "Epoch: 12630 mean train loss:  1.20450193e-02, bound:  3.15825284e-01\n",
      "Epoch: 12631 mean train loss:  1.20445117e-02, bound:  3.15825284e-01\n",
      "Epoch: 12632 mean train loss:  1.20441327e-02, bound:  3.15825254e-01\n",
      "Epoch: 12633 mean train loss:  1.20438095e-02, bound:  3.15825254e-01\n",
      "Epoch: 12634 mean train loss:  1.20435441e-02, bound:  3.15825254e-01\n",
      "Epoch: 12635 mean train loss:  1.20430514e-02, bound:  3.15825254e-01\n",
      "Epoch: 12636 mean train loss:  1.20427208e-02, bound:  3.15825194e-01\n",
      "Epoch: 12637 mean train loss:  1.20423762e-02, bound:  3.15825164e-01\n",
      "Epoch: 12638 mean train loss:  1.20419357e-02, bound:  3.15825135e-01\n",
      "Epoch: 12639 mean train loss:  1.20415343e-02, bound:  3.15825135e-01\n",
      "Epoch: 12640 mean train loss:  1.20411599e-02, bound:  3.15825135e-01\n",
      "Epoch: 12641 mean train loss:  1.20408637e-02, bound:  3.15825135e-01\n",
      "Epoch: 12642 mean train loss:  1.20405499e-02, bound:  3.15825135e-01\n",
      "Epoch: 12643 mean train loss:  1.20401029e-02, bound:  3.15825045e-01\n",
      "Epoch: 12644 mean train loss:  1.20397368e-02, bound:  3.15825045e-01\n",
      "Epoch: 12645 mean train loss:  1.20393243e-02, bound:  3.15825045e-01\n",
      "Epoch: 12646 mean train loss:  1.20390663e-02, bound:  3.15825045e-01\n",
      "Epoch: 12647 mean train loss:  1.20385252e-02, bound:  3.15825015e-01\n",
      "Epoch: 12648 mean train loss:  1.20382132e-02, bound:  3.15824986e-01\n",
      "Epoch: 12649 mean train loss:  1.20378435e-02, bound:  3.15824956e-01\n",
      "Epoch: 12650 mean train loss:  1.20376116e-02, bound:  3.15824956e-01\n",
      "Epoch: 12651 mean train loss:  1.20370947e-02, bound:  3.15824956e-01\n",
      "Epoch: 12652 mean train loss:  1.20366933e-02, bound:  3.15824956e-01\n",
      "Epoch: 12653 mean train loss:  1.20364036e-02, bound:  3.15824926e-01\n",
      "Epoch: 12654 mean train loss:  1.20359873e-02, bound:  3.15824896e-01\n",
      "Epoch: 12655 mean train loss:  1.20356036e-02, bound:  3.15824866e-01\n",
      "Epoch: 12656 mean train loss:  1.20351622e-02, bound:  3.15824836e-01\n",
      "Epoch: 12657 mean train loss:  1.20348735e-02, bound:  3.15824836e-01\n",
      "Epoch: 12658 mean train loss:  1.20346081e-02, bound:  3.15824836e-01\n",
      "Epoch: 12659 mean train loss:  1.20340716e-02, bound:  3.15824807e-01\n",
      "Epoch: 12660 mean train loss:  1.20337671e-02, bound:  3.15824807e-01\n",
      "Epoch: 12661 mean train loss:  1.20334141e-02, bound:  3.15824777e-01\n",
      "Epoch: 12662 mean train loss:  1.20330108e-02, bound:  3.15824747e-01\n",
      "Epoch: 12663 mean train loss:  1.20326066e-02, bound:  3.15824717e-01\n",
      "Epoch: 12664 mean train loss:  1.20322369e-02, bound:  3.15824717e-01\n",
      "Epoch: 12665 mean train loss:  1.20318681e-02, bound:  3.15824717e-01\n",
      "Epoch: 12666 mean train loss:  1.20315086e-02, bound:  3.15824717e-01\n",
      "Epoch: 12667 mean train loss:  1.20311091e-02, bound:  3.15824687e-01\n",
      "Epoch: 12668 mean train loss:  1.20307198e-02, bound:  3.15824658e-01\n",
      "Epoch: 12669 mean train loss:  1.20304124e-02, bound:  3.15824628e-01\n",
      "Epoch: 12670 mean train loss:  1.20300194e-02, bound:  3.15824628e-01\n",
      "Epoch: 12671 mean train loss:  1.20295817e-02, bound:  3.15824598e-01\n",
      "Epoch: 12672 mean train loss:  1.20291403e-02, bound:  3.15824598e-01\n",
      "Epoch: 12673 mean train loss:  1.20287761e-02, bound:  3.15824568e-01\n",
      "Epoch: 12674 mean train loss:  1.20284930e-02, bound:  3.15824568e-01\n",
      "Epoch: 12675 mean train loss:  1.20280767e-02, bound:  3.15824538e-01\n",
      "Epoch: 12676 mean train loss:  1.20277544e-02, bound:  3.15824538e-01\n",
      "Epoch: 12677 mean train loss:  1.20273512e-02, bound:  3.15824509e-01\n",
      "Epoch: 12678 mean train loss:  1.20270392e-02, bound:  3.15824479e-01\n",
      "Epoch: 12679 mean train loss:  1.20266164e-02, bound:  3.15824479e-01\n",
      "Epoch: 12680 mean train loss:  1.20261861e-02, bound:  3.15824449e-01\n",
      "Epoch: 12681 mean train loss:  1.20258583e-02, bound:  3.15824419e-01\n",
      "Epoch: 12682 mean train loss:  1.20255193e-02, bound:  3.15824419e-01\n",
      "Epoch: 12683 mean train loss:  1.20250350e-02, bound:  3.15824389e-01\n",
      "Epoch: 12684 mean train loss:  1.20247081e-02, bound:  3.15824389e-01\n",
      "Epoch: 12685 mean train loss:  1.20242378e-02, bound:  3.15824389e-01\n",
      "Epoch: 12686 mean train loss:  1.20239211e-02, bound:  3.15824360e-01\n",
      "Epoch: 12687 mean train loss:  1.20235439e-02, bound:  3.15824300e-01\n",
      "Epoch: 12688 mean train loss:  1.20231789e-02, bound:  3.15824300e-01\n",
      "Epoch: 12689 mean train loss:  1.20227244e-02, bound:  3.15824300e-01\n",
      "Epoch: 12690 mean train loss:  1.20225316e-02, bound:  3.15824270e-01\n",
      "Epoch: 12691 mean train loss:  1.20219728e-02, bound:  3.15824270e-01\n",
      "Epoch: 12692 mean train loss:  1.20216254e-02, bound:  3.15824240e-01\n",
      "Epoch: 12693 mean train loss:  1.20212352e-02, bound:  3.15824240e-01\n",
      "Epoch: 12694 mean train loss:  1.20208608e-02, bound:  3.15824181e-01\n",
      "Epoch: 12695 mean train loss:  1.20204762e-02, bound:  3.15824181e-01\n",
      "Epoch: 12696 mean train loss:  1.20200459e-02, bound:  3.15824181e-01\n",
      "Epoch: 12697 mean train loss:  1.20197004e-02, bound:  3.15824151e-01\n",
      "Epoch: 12698 mean train loss:  1.20194089e-02, bound:  3.15824151e-01\n",
      "Epoch: 12699 mean train loss:  1.20189972e-02, bound:  3.15824151e-01\n",
      "Epoch: 12700 mean train loss:  1.20185511e-02, bound:  3.15824121e-01\n",
      "Epoch: 12701 mean train loss:  1.20181637e-02, bound:  3.15824091e-01\n",
      "Epoch: 12702 mean train loss:  1.20178936e-02, bound:  3.15824062e-01\n",
      "Epoch: 12703 mean train loss:  1.20174289e-02, bound:  3.15824062e-01\n",
      "Epoch: 12704 mean train loss:  1.20170312e-02, bound:  3.15824032e-01\n",
      "Epoch: 12705 mean train loss:  1.20165320e-02, bound:  3.15824032e-01\n",
      "Epoch: 12706 mean train loss:  1.20163420e-02, bound:  3.15824032e-01\n",
      "Epoch: 12707 mean train loss:  1.20158140e-02, bound:  3.15823972e-01\n",
      "Epoch: 12708 mean train loss:  1.20155131e-02, bound:  3.15823972e-01\n",
      "Epoch: 12709 mean train loss:  1.20151555e-02, bound:  3.15823942e-01\n",
      "Epoch: 12710 mean train loss:  1.20148426e-02, bound:  3.15823942e-01\n",
      "Epoch: 12711 mean train loss:  1.20143639e-02, bound:  3.15823913e-01\n",
      "Epoch: 12712 mean train loss:  1.20139550e-02, bound:  3.15823913e-01\n",
      "Epoch: 12713 mean train loss:  1.20135779e-02, bound:  3.15823853e-01\n",
      "Epoch: 12714 mean train loss:  1.20132435e-02, bound:  3.15823853e-01\n",
      "Epoch: 12715 mean train loss:  1.20128170e-02, bound:  3.15823823e-01\n",
      "Epoch: 12716 mean train loss:  1.20124556e-02, bound:  3.15823823e-01\n",
      "Epoch: 12717 mean train loss:  1.20120244e-02, bound:  3.15823823e-01\n",
      "Epoch: 12718 mean train loss:  1.20117227e-02, bound:  3.15823823e-01\n",
      "Epoch: 12719 mean train loss:  1.20113436e-02, bound:  3.15823734e-01\n",
      "Epoch: 12720 mean train loss:  1.20108481e-02, bound:  3.15823734e-01\n",
      "Epoch: 12721 mean train loss:  1.20105520e-02, bound:  3.15823734e-01\n",
      "Epoch: 12722 mean train loss:  1.20101590e-02, bound:  3.15823704e-01\n",
      "Epoch: 12723 mean train loss:  1.20097278e-02, bound:  3.15823704e-01\n",
      "Epoch: 12724 mean train loss:  1.20093478e-02, bound:  3.15823704e-01\n",
      "Epoch: 12725 mean train loss:  1.20089222e-02, bound:  3.15823704e-01\n",
      "Epoch: 12726 mean train loss:  1.20086046e-02, bound:  3.15823615e-01\n",
      "Epoch: 12727 mean train loss:  1.20081836e-02, bound:  3.15823615e-01\n",
      "Epoch: 12728 mean train loss:  1.20078316e-02, bound:  3.15823615e-01\n",
      "Epoch: 12729 mean train loss:  1.20074535e-02, bound:  3.15823615e-01\n",
      "Epoch: 12730 mean train loss:  1.20070688e-02, bound:  3.15823615e-01\n",
      "Epoch: 12731 mean train loss:  1.20066246e-02, bound:  3.15823585e-01\n",
      "Epoch: 12732 mean train loss:  1.20062791e-02, bound:  3.15823585e-01\n",
      "Epoch: 12733 mean train loss:  1.20058889e-02, bound:  3.15823525e-01\n",
      "Epoch: 12734 mean train loss:  1.20055303e-02, bound:  3.15823525e-01\n",
      "Epoch: 12735 mean train loss:  1.20051028e-02, bound:  3.15823495e-01\n",
      "Epoch: 12736 mean train loss:  1.20047154e-02, bound:  3.15823495e-01\n",
      "Epoch: 12737 mean train loss:  1.20043298e-02, bound:  3.15823436e-01\n",
      "Epoch: 12738 mean train loss:  1.20040365e-02, bound:  3.15823436e-01\n",
      "Epoch: 12739 mean train loss:  1.20035382e-02, bound:  3.15823406e-01\n",
      "Epoch: 12740 mean train loss:  1.20031526e-02, bound:  3.15823406e-01\n",
      "Epoch: 12741 mean train loss:  1.20026805e-02, bound:  3.15823376e-01\n",
      "Epoch: 12742 mean train loss:  1.20024979e-02, bound:  3.15823376e-01\n",
      "Epoch: 12743 mean train loss:  1.20019959e-02, bound:  3.15823346e-01\n",
      "Epoch: 12744 mean train loss:  1.20016364e-02, bound:  3.15823317e-01\n",
      "Epoch: 12745 mean train loss:  1.20012229e-02, bound:  3.15823287e-01\n",
      "Epoch: 12746 mean train loss:  1.20008700e-02, bound:  3.15823287e-01\n",
      "Epoch: 12747 mean train loss:  1.20003968e-02, bound:  3.15823287e-01\n",
      "Epoch: 12748 mean train loss:  1.20000327e-02, bound:  3.15823257e-01\n",
      "Epoch: 12749 mean train loss:  1.19996313e-02, bound:  3.15823257e-01\n",
      "Epoch: 12750 mean train loss:  1.19992383e-02, bound:  3.15823257e-01\n",
      "Epoch: 12751 mean train loss:  1.19989356e-02, bound:  3.15823197e-01\n",
      "Epoch: 12752 mean train loss:  1.19985584e-02, bound:  3.15823168e-01\n",
      "Epoch: 12753 mean train loss:  1.19980862e-02, bound:  3.15823168e-01\n",
      "Epoch: 12754 mean train loss:  1.19977593e-02, bound:  3.15823168e-01\n",
      "Epoch: 12755 mean train loss:  1.19973226e-02, bound:  3.15823138e-01\n",
      "Epoch: 12756 mean train loss:  1.19969575e-02, bound:  3.15823138e-01\n",
      "Epoch: 12757 mean train loss:  1.19965179e-02, bound:  3.15823108e-01\n",
      "Epoch: 12758 mean train loss:  1.19961388e-02, bound:  3.15823108e-01\n",
      "Epoch: 12759 mean train loss:  1.19957458e-02, bound:  3.15823048e-01\n",
      "Epoch: 12760 mean train loss:  1.19954711e-02, bound:  3.15823048e-01\n",
      "Epoch: 12761 mean train loss:  1.19949887e-02, bound:  3.15823019e-01\n",
      "Epoch: 12762 mean train loss:  1.19946729e-02, bound:  3.15823019e-01\n",
      "Epoch: 12763 mean train loss:  1.19941598e-02, bound:  3.15822989e-01\n",
      "Epoch: 12764 mean train loss:  1.19938990e-02, bound:  3.15822989e-01\n",
      "Epoch: 12765 mean train loss:  1.19934101e-02, bound:  3.15822959e-01\n",
      "Epoch: 12766 mean train loss:  1.19929975e-02, bound:  3.15822959e-01\n",
      "Epoch: 12767 mean train loss:  1.19925756e-02, bound:  3.15822929e-01\n",
      "Epoch: 12768 mean train loss:  1.19922105e-02, bound:  3.15822899e-01\n",
      "Epoch: 12769 mean train loss:  1.19918734e-02, bound:  3.15822899e-01\n",
      "Epoch: 12770 mean train loss:  1.19914701e-02, bound:  3.15822870e-01\n",
      "Epoch: 12771 mean train loss:  1.19911339e-02, bound:  3.15822870e-01\n",
      "Epoch: 12772 mean train loss:  1.19907185e-02, bound:  3.15822840e-01\n",
      "Epoch: 12773 mean train loss:  1.19901877e-02, bound:  3.15822810e-01\n",
      "Epoch: 12774 mean train loss:  1.19898962e-02, bound:  3.15822810e-01\n",
      "Epoch: 12775 mean train loss:  1.19895255e-02, bound:  3.15822810e-01\n",
      "Epoch: 12776 mean train loss:  1.19891660e-02, bound:  3.15822750e-01\n",
      "Epoch: 12777 mean train loss:  1.19886659e-02, bound:  3.15822750e-01\n",
      "Epoch: 12778 mean train loss:  1.19883120e-02, bound:  3.15822721e-01\n",
      "Epoch: 12779 mean train loss:  1.19879013e-02, bound:  3.15822721e-01\n",
      "Epoch: 12780 mean train loss:  1.19875418e-02, bound:  3.15822691e-01\n",
      "Epoch: 12781 mean train loss:  1.19870985e-02, bound:  3.15822691e-01\n",
      "Epoch: 12782 mean train loss:  1.19867874e-02, bound:  3.15822631e-01\n",
      "Epoch: 12783 mean train loss:  1.19863199e-02, bound:  3.15822631e-01\n",
      "Epoch: 12784 mean train loss:  1.19860396e-02, bound:  3.15822601e-01\n",
      "Epoch: 12785 mean train loss:  1.19855478e-02, bound:  3.15822601e-01\n",
      "Epoch: 12786 mean train loss:  1.19851772e-02, bound:  3.15822601e-01\n",
      "Epoch: 12787 mean train loss:  1.19847851e-02, bound:  3.15822572e-01\n",
      "Epoch: 12788 mean train loss:  1.19844088e-02, bound:  3.15822542e-01\n",
      "Epoch: 12789 mean train loss:  1.19840475e-02, bound:  3.15822512e-01\n",
      "Epoch: 12790 mean train loss:  1.19835725e-02, bound:  3.15822512e-01\n",
      "Epoch: 12791 mean train loss:  1.19832112e-02, bound:  3.15822482e-01\n",
      "Epoch: 12792 mean train loss:  1.19827641e-02, bound:  3.15822482e-01\n",
      "Epoch: 12793 mean train loss:  1.19824056e-02, bound:  3.15822423e-01\n",
      "Epoch: 12794 mean train loss:  1.19820340e-02, bound:  3.15822423e-01\n",
      "Epoch: 12795 mean train loss:  1.19815962e-02, bound:  3.15822393e-01\n",
      "Epoch: 12796 mean train loss:  1.19812936e-02, bound:  3.15822393e-01\n",
      "Epoch: 12797 mean train loss:  1.19808149e-02, bound:  3.15822393e-01\n",
      "Epoch: 12798 mean train loss:  1.19804610e-02, bound:  3.15822393e-01\n",
      "Epoch: 12799 mean train loss:  1.19799757e-02, bound:  3.15822303e-01\n",
      "Epoch: 12800 mean train loss:  1.19797122e-02, bound:  3.15822303e-01\n",
      "Epoch: 12801 mean train loss:  1.19792083e-02, bound:  3.15822303e-01\n",
      "Epoch: 12802 mean train loss:  1.19788637e-02, bound:  3.15822303e-01\n",
      "Epoch: 12803 mean train loss:  1.19784325e-02, bound:  3.15822273e-01\n",
      "Epoch: 12804 mean train loss:  1.19780032e-02, bound:  3.15822273e-01\n",
      "Epoch: 12805 mean train loss:  1.19776344e-02, bound:  3.15822244e-01\n",
      "Epoch: 12806 mean train loss:  1.19772088e-02, bound:  3.15822184e-01\n",
      "Epoch: 12807 mean train loss:  1.19768642e-02, bound:  3.15822184e-01\n",
      "Epoch: 12808 mean train loss:  1.19764656e-02, bound:  3.15822184e-01\n",
      "Epoch: 12809 mean train loss:  1.19760856e-02, bound:  3.15822154e-01\n",
      "Epoch: 12810 mean train loss:  1.19756730e-02, bound:  3.15822154e-01\n",
      "Epoch: 12811 mean train loss:  1.19752837e-02, bound:  3.15822095e-01\n",
      "Epoch: 12812 mean train loss:  1.19748032e-02, bound:  3.15822095e-01\n",
      "Epoch: 12813 mean train loss:  1.19745908e-02, bound:  3.15822065e-01\n",
      "Epoch: 12814 mean train loss:  1.19740115e-02, bound:  3.15822065e-01\n",
      "Epoch: 12815 mean train loss:  1.19736530e-02, bound:  3.15822035e-01\n",
      "Epoch: 12816 mean train loss:  1.19733345e-02, bound:  3.15822035e-01\n",
      "Epoch: 12817 mean train loss:  1.19728139e-02, bound:  3.15821975e-01\n",
      "Epoch: 12818 mean train loss:  1.19725438e-02, bound:  3.15821975e-01\n",
      "Epoch: 12819 mean train loss:  1.19720288e-02, bound:  3.15821946e-01\n",
      "Epoch: 12820 mean train loss:  1.19716311e-02, bound:  3.15821946e-01\n",
      "Epoch: 12821 mean train loss:  1.19712008e-02, bound:  3.15821946e-01\n",
      "Epoch: 12822 mean train loss:  1.19709065e-02, bound:  3.15821916e-01\n",
      "Epoch: 12823 mean train loss:  1.19704958e-02, bound:  3.15821916e-01\n",
      "Epoch: 12824 mean train loss:  1.19699771e-02, bound:  3.15821856e-01\n",
      "Epoch: 12825 mean train loss:  1.19696157e-02, bound:  3.15821856e-01\n",
      "Epoch: 12826 mean train loss:  1.19692506e-02, bound:  3.15821826e-01\n",
      "Epoch: 12827 mean train loss:  1.19688027e-02, bound:  3.15821826e-01\n",
      "Epoch: 12828 mean train loss:  1.19683081e-02, bound:  3.15821767e-01\n",
      "Epoch: 12829 mean train loss:  1.19680371e-02, bound:  3.15821767e-01\n",
      "Epoch: 12830 mean train loss:  1.19676329e-02, bound:  3.15821737e-01\n",
      "Epoch: 12831 mean train loss:  1.19672036e-02, bound:  3.15821737e-01\n",
      "Epoch: 12832 mean train loss:  1.19667687e-02, bound:  3.15821707e-01\n",
      "Epoch: 12833 mean train loss:  1.19663915e-02, bound:  3.15821707e-01\n",
      "Epoch: 12834 mean train loss:  1.19659789e-02, bound:  3.15821707e-01\n",
      "Epoch: 12835 mean train loss:  1.19656557e-02, bound:  3.15821648e-01\n",
      "Epoch: 12836 mean train loss:  1.19651975e-02, bound:  3.15821648e-01\n",
      "Epoch: 12837 mean train loss:  1.19648678e-02, bound:  3.15821618e-01\n",
      "Epoch: 12838 mean train loss:  1.19644348e-02, bound:  3.15821588e-01\n",
      "Epoch: 12839 mean train loss:  1.19639821e-02, bound:  3.15821588e-01\n",
      "Epoch: 12840 mean train loss:  1.19634997e-02, bound:  3.15821588e-01\n",
      "Epoch: 12841 mean train loss:  1.19632073e-02, bound:  3.15821588e-01\n",
      "Epoch: 12842 mean train loss:  1.19627360e-02, bound:  3.15821528e-01\n",
      "Epoch: 12843 mean train loss:  1.19623169e-02, bound:  3.15821528e-01\n",
      "Epoch: 12844 mean train loss:  1.19620748e-02, bound:  3.15821469e-01\n",
      "Epoch: 12845 mean train loss:  1.19614946e-02, bound:  3.15821469e-01\n",
      "Epoch: 12846 mean train loss:  1.19611351e-02, bound:  3.15821439e-01\n",
      "Epoch: 12847 mean train loss:  1.19607961e-02, bound:  3.15821439e-01\n",
      "Epoch: 12848 mean train loss:  1.19604589e-02, bound:  3.15821409e-01\n",
      "Epoch: 12849 mean train loss:  1.19599411e-02, bound:  3.15821409e-01\n",
      "Epoch: 12850 mean train loss:  1.19596226e-02, bound:  3.15821379e-01\n",
      "Epoch: 12851 mean train loss:  1.19591625e-02, bound:  3.15821379e-01\n",
      "Epoch: 12852 mean train loss:  1.19587053e-02, bound:  3.15821320e-01\n",
      "Epoch: 12853 mean train loss:  1.19583933e-02, bound:  3.15821320e-01\n",
      "Epoch: 12854 mean train loss:  1.19579323e-02, bound:  3.15821290e-01\n",
      "Epoch: 12855 mean train loss:  1.19574415e-02, bound:  3.15821290e-01\n",
      "Epoch: 12856 mean train loss:  1.19570829e-02, bound:  3.15821260e-01\n",
      "Epoch: 12857 mean train loss:  1.19567560e-02, bound:  3.15821260e-01\n",
      "Epoch: 12858 mean train loss:  1.19562587e-02, bound:  3.15821201e-01\n",
      "Epoch: 12859 mean train loss:  1.19559290e-02, bound:  3.15821201e-01\n",
      "Epoch: 12860 mean train loss:  1.19554335e-02, bound:  3.15821171e-01\n",
      "Epoch: 12861 mean train loss:  1.19551504e-02, bound:  3.15821171e-01\n",
      "Epoch: 12862 mean train loss:  1.19546726e-02, bound:  3.15821141e-01\n",
      "Epoch: 12863 mean train loss:  1.19542824e-02, bound:  3.15821141e-01\n",
      "Epoch: 12864 mean train loss:  1.19538652e-02, bound:  3.15821081e-01\n",
      "Epoch: 12865 mean train loss:  1.19533846e-02, bound:  3.15821081e-01\n",
      "Epoch: 12866 mean train loss:  1.19530223e-02, bound:  3.15821081e-01\n",
      "Epoch: 12867 mean train loss:  1.19526321e-02, bound:  3.15821081e-01\n",
      "Epoch: 12868 mean train loss:  1.19523089e-02, bound:  3.15821052e-01\n",
      "Epoch: 12869 mean train loss:  1.19518191e-02, bound:  3.15821022e-01\n",
      "Epoch: 12870 mean train loss:  1.19514726e-02, bound:  3.15820962e-01\n",
      "Epoch: 12871 mean train loss:  1.19509464e-02, bound:  3.15820962e-01\n",
      "Epoch: 12872 mean train loss:  1.19506046e-02, bound:  3.15820962e-01\n",
      "Epoch: 12873 mean train loss:  1.19501827e-02, bound:  3.15820962e-01\n",
      "Epoch: 12874 mean train loss:  1.19497199e-02, bound:  3.15820903e-01\n",
      "Epoch: 12875 mean train loss:  1.19493911e-02, bound:  3.15820903e-01\n",
      "Epoch: 12876 mean train loss:  1.19488956e-02, bound:  3.15820873e-01\n",
      "Epoch: 12877 mean train loss:  1.19486200e-02, bound:  3.15820873e-01\n",
      "Epoch: 12878 mean train loss:  1.19481059e-02, bound:  3.15820843e-01\n",
      "Epoch: 12879 mean train loss:  1.19477650e-02, bound:  3.15820843e-01\n",
      "Epoch: 12880 mean train loss:  1.19473254e-02, bound:  3.15820813e-01\n",
      "Epoch: 12881 mean train loss:  1.19468458e-02, bound:  3.15820783e-01\n",
      "Epoch: 12882 mean train loss:  1.19464807e-02, bound:  3.15820754e-01\n",
      "Epoch: 12883 mean train loss:  1.19461101e-02, bound:  3.15820754e-01\n",
      "Epoch: 12884 mean train loss:  1.19456835e-02, bound:  3.15820724e-01\n",
      "Epoch: 12885 mean train loss:  1.19453147e-02, bound:  3.15820724e-01\n",
      "Epoch: 12886 mean train loss:  1.19448667e-02, bound:  3.15820664e-01\n",
      "Epoch: 12887 mean train loss:  1.19444374e-02, bound:  3.15820664e-01\n",
      "Epoch: 12888 mean train loss:  1.19440090e-02, bound:  3.15820664e-01\n",
      "Epoch: 12889 mean train loss:  1.19436421e-02, bound:  3.15820634e-01\n",
      "Epoch: 12890 mean train loss:  1.19432034e-02, bound:  3.15820605e-01\n",
      "Epoch: 12891 mean train loss:  1.19427927e-02, bound:  3.15820605e-01\n",
      "Epoch: 12892 mean train loss:  1.19423550e-02, bound:  3.15820545e-01\n",
      "Epoch: 12893 mean train loss:  1.19420066e-02, bound:  3.15820545e-01\n",
      "Epoch: 12894 mean train loss:  1.19415922e-02, bound:  3.15820545e-01\n",
      "Epoch: 12895 mean train loss:  1.19410744e-02, bound:  3.15820515e-01\n",
      "Epoch: 12896 mean train loss:  1.19406478e-02, bound:  3.15820515e-01\n",
      "Epoch: 12897 mean train loss:  1.19403023e-02, bound:  3.15820485e-01\n",
      "Epoch: 12898 mean train loss:  1.19398460e-02, bound:  3.15820485e-01\n",
      "Epoch: 12899 mean train loss:  1.19394641e-02, bound:  3.15820426e-01\n",
      "Epoch: 12900 mean train loss:  1.19390823e-02, bound:  3.15820426e-01\n",
      "Epoch: 12901 mean train loss:  1.19387293e-02, bound:  3.15820396e-01\n",
      "Epoch: 12902 mean train loss:  1.19382218e-02, bound:  3.15820396e-01\n",
      "Epoch: 12903 mean train loss:  1.19377682e-02, bound:  3.15820366e-01\n",
      "Epoch: 12904 mean train loss:  1.19374553e-02, bound:  3.15820366e-01\n",
      "Epoch: 12905 mean train loss:  1.19369095e-02, bound:  3.15820307e-01\n",
      "Epoch: 12906 mean train loss:  1.19366981e-02, bound:  3.15820307e-01\n",
      "Epoch: 12907 mean train loss:  1.19361440e-02, bound:  3.15820277e-01\n",
      "Epoch: 12908 mean train loss:  1.19357435e-02, bound:  3.15820277e-01\n",
      "Epoch: 12909 mean train loss:  1.19353617e-02, bound:  3.15820277e-01\n",
      "Epoch: 12910 mean train loss:  1.19349854e-02, bound:  3.15820217e-01\n",
      "Epoch: 12911 mean train loss:  1.19344462e-02, bound:  3.15820217e-01\n",
      "Epoch: 12912 mean train loss:  1.19340913e-02, bound:  3.15820187e-01\n",
      "Epoch: 12913 mean train loss:  1.19336750e-02, bound:  3.15820158e-01\n",
      "Epoch: 12914 mean train loss:  1.19332541e-02, bound:  3.15820158e-01\n",
      "Epoch: 12915 mean train loss:  1.19327633e-02, bound:  3.15820158e-01\n",
      "Epoch: 12916 mean train loss:  1.19323879e-02, bound:  3.15820098e-01\n",
      "Epoch: 12917 mean train loss:  1.19320815e-02, bound:  3.15820068e-01\n",
      "Epoch: 12918 mean train loss:  1.19315870e-02, bound:  3.15820068e-01\n",
      "Epoch: 12919 mean train loss:  1.19312694e-02, bound:  3.15820038e-01\n",
      "Epoch: 12920 mean train loss:  1.19306967e-02, bound:  3.15820038e-01\n",
      "Epoch: 12921 mean train loss:  1.19303623e-02, bound:  3.15820038e-01\n",
      "Epoch: 12922 mean train loss:  1.19300215e-02, bound:  3.15819979e-01\n",
      "Epoch: 12923 mean train loss:  1.19295577e-02, bound:  3.15819949e-01\n",
      "Epoch: 12924 mean train loss:  1.19291302e-02, bound:  3.15819949e-01\n",
      "Epoch: 12925 mean train loss:  1.19286589e-02, bound:  3.15819949e-01\n",
      "Epoch: 12926 mean train loss:  1.19282156e-02, bound:  3.15819889e-01\n",
      "Epoch: 12927 mean train loss:  1.19278170e-02, bound:  3.15819860e-01\n",
      "Epoch: 12928 mean train loss:  1.19274305e-02, bound:  3.15819860e-01\n",
      "Epoch: 12929 mean train loss:  1.19270375e-02, bound:  3.15819860e-01\n",
      "Epoch: 12930 mean train loss:  1.19266110e-02, bound:  3.15819830e-01\n",
      "Epoch: 12931 mean train loss:  1.19261574e-02, bound:  3.15819830e-01\n",
      "Epoch: 12932 mean train loss:  1.19257839e-02, bound:  3.15819770e-01\n",
      "Epoch: 12933 mean train loss:  1.19252717e-02, bound:  3.15819740e-01\n",
      "Epoch: 12934 mean train loss:  1.19249504e-02, bound:  3.15819740e-01\n",
      "Epoch: 12935 mean train loss:  1.19244084e-02, bound:  3.15819740e-01\n",
      "Epoch: 12936 mean train loss:  1.19241057e-02, bound:  3.15819710e-01\n",
      "Epoch: 12937 mean train loss:  1.19236223e-02, bound:  3.15819651e-01\n",
      "Epoch: 12938 mean train loss:  1.19231762e-02, bound:  3.15819651e-01\n",
      "Epoch: 12939 mean train loss:  1.19228140e-02, bound:  3.15819651e-01\n",
      "Epoch: 12940 mean train loss:  1.19224070e-02, bound:  3.15819651e-01\n",
      "Epoch: 12941 mean train loss:  1.19219488e-02, bound:  3.15819591e-01\n",
      "Epoch: 12942 mean train loss:  1.19215185e-02, bound:  3.15819591e-01\n",
      "Epoch: 12943 mean train loss:  1.19210659e-02, bound:  3.15819532e-01\n",
      "Epoch: 12944 mean train loss:  1.19207660e-02, bound:  3.15819532e-01\n",
      "Epoch: 12945 mean train loss:  1.19202016e-02, bound:  3.15819532e-01\n",
      "Epoch: 12946 mean train loss:  1.19198402e-02, bound:  3.15819532e-01\n",
      "Epoch: 12947 mean train loss:  1.19194994e-02, bound:  3.15819472e-01\n",
      "Epoch: 12948 mean train loss:  1.19190281e-02, bound:  3.15819472e-01\n",
      "Epoch: 12949 mean train loss:  1.19185876e-02, bound:  3.15819442e-01\n",
      "Epoch: 12950 mean train loss:  1.19182207e-02, bound:  3.15819412e-01\n",
      "Epoch: 12951 mean train loss:  1.19176963e-02, bound:  3.15819412e-01\n",
      "Epoch: 12952 mean train loss:  1.19173238e-02, bound:  3.15819412e-01\n",
      "Epoch: 12953 mean train loss:  1.19169019e-02, bound:  3.15819383e-01\n",
      "Epoch: 12954 mean train loss:  1.19164372e-02, bound:  3.15819383e-01\n",
      "Epoch: 12955 mean train loss:  1.19160814e-02, bound:  3.15819323e-01\n",
      "Epoch: 12956 mean train loss:  1.19156418e-02, bound:  3.15819293e-01\n",
      "Epoch: 12957 mean train loss:  1.19152060e-02, bound:  3.15819293e-01\n",
      "Epoch: 12958 mean train loss:  1.19148558e-02, bound:  3.15819263e-01\n",
      "Epoch: 12959 mean train loss:  1.19143091e-02, bound:  3.15819234e-01\n",
      "Epoch: 12960 mean train loss:  1.19140428e-02, bound:  3.15819204e-01\n",
      "Epoch: 12961 mean train loss:  1.19134374e-02, bound:  3.15819204e-01\n",
      "Epoch: 12962 mean train loss:  1.19130053e-02, bound:  3.15819174e-01\n",
      "Epoch: 12963 mean train loss:  1.19126411e-02, bound:  3.15819174e-01\n",
      "Epoch: 12964 mean train loss:  1.19122723e-02, bound:  3.15819144e-01\n",
      "Epoch: 12965 mean train loss:  1.19116846e-02, bound:  3.15819144e-01\n",
      "Epoch: 12966 mean train loss:  1.19114183e-02, bound:  3.15819085e-01\n",
      "Epoch: 12967 mean train loss:  1.19109293e-02, bound:  3.15819085e-01\n",
      "Epoch: 12968 mean train loss:  1.19105652e-02, bound:  3.15819055e-01\n",
      "Epoch: 12969 mean train loss:  1.19100157e-02, bound:  3.15819055e-01\n",
      "Epoch: 12970 mean train loss:  1.19096125e-02, bound:  3.15819025e-01\n",
      "Epoch: 12971 mean train loss:  1.19092790e-02, bound:  3.15819025e-01\n",
      "Epoch: 12972 mean train loss:  1.19088478e-02, bound:  3.15818965e-01\n",
      "Epoch: 12973 mean train loss:  1.19083859e-02, bound:  3.15818965e-01\n",
      "Epoch: 12974 mean train loss:  1.19080497e-02, bound:  3.15818936e-01\n",
      "Epoch: 12975 mean train loss:  1.19076055e-02, bound:  3.15818936e-01\n",
      "Epoch: 12976 mean train loss:  1.19072013e-02, bound:  3.15818876e-01\n",
      "Epoch: 12977 mean train loss:  1.19066676e-02, bound:  3.15818876e-01\n",
      "Epoch: 12978 mean train loss:  1.19062755e-02, bound:  3.15818846e-01\n",
      "Epoch: 12979 mean train loss:  1.19058574e-02, bound:  3.15818846e-01\n",
      "Epoch: 12980 mean train loss:  1.19054299e-02, bound:  3.15818846e-01\n",
      "Epoch: 12981 mean train loss:  1.19050182e-02, bound:  3.15818846e-01\n",
      "Epoch: 12982 mean train loss:  1.19045749e-02, bound:  3.15818787e-01\n",
      "Epoch: 12983 mean train loss:  1.19040851e-02, bound:  3.15818757e-01\n",
      "Epoch: 12984 mean train loss:  1.19036520e-02, bound:  3.15818727e-01\n",
      "Epoch: 12985 mean train loss:  1.19033316e-02, bound:  3.15818727e-01\n",
      "Epoch: 12986 mean train loss:  1.19028836e-02, bound:  3.15818727e-01\n",
      "Epoch: 12987 mean train loss:  1.19024236e-02, bound:  3.15818667e-01\n",
      "Epoch: 12988 mean train loss:  1.19020129e-02, bound:  3.15818667e-01\n",
      "Epoch: 12989 mean train loss:  1.19015817e-02, bound:  3.15818638e-01\n",
      "Epoch: 12990 mean train loss:  1.19011290e-02, bound:  3.15818638e-01\n",
      "Epoch: 12991 mean train loss:  1.19006997e-02, bound:  3.15818608e-01\n",
      "Epoch: 12992 mean train loss:  1.19001996e-02, bound:  3.15818548e-01\n",
      "Epoch: 12993 mean train loss:  1.18998867e-02, bound:  3.15818548e-01\n",
      "Epoch: 12994 mean train loss:  1.18995132e-02, bound:  3.15818548e-01\n",
      "Epoch: 12995 mean train loss:  1.18990168e-02, bound:  3.15818518e-01\n",
      "Epoch: 12996 mean train loss:  1.18985465e-02, bound:  3.15818518e-01\n",
      "Epoch: 12997 mean train loss:  1.18982159e-02, bound:  3.15818489e-01\n",
      "Epoch: 12998 mean train loss:  1.18977055e-02, bound:  3.15818489e-01\n",
      "Epoch: 12999 mean train loss:  1.18972575e-02, bound:  3.15818429e-01\n",
      "Epoch: 13000 mean train loss:  1.18968887e-02, bound:  3.15818429e-01\n",
      "Epoch: 13001 mean train loss:  1.18964706e-02, bound:  3.15818399e-01\n",
      "Epoch: 13002 mean train loss:  1.18959397e-02, bound:  3.15818399e-01\n",
      "Epoch: 13003 mean train loss:  1.18954759e-02, bound:  3.15818369e-01\n",
      "Epoch: 13004 mean train loss:  1.18950699e-02, bound:  3.15818310e-01\n",
      "Epoch: 13005 mean train loss:  1.18946685e-02, bound:  3.15818310e-01\n",
      "Epoch: 13006 mean train loss:  1.18942494e-02, bound:  3.15818280e-01\n",
      "Epoch: 13007 mean train loss:  1.18938023e-02, bound:  3.15818280e-01\n",
      "Epoch: 13008 mean train loss:  1.18933599e-02, bound:  3.15818280e-01\n",
      "Epoch: 13009 mean train loss:  1.18928729e-02, bound:  3.15818220e-01\n",
      "Epoch: 13010 mean train loss:  1.18926195e-02, bound:  3.15818220e-01\n",
      "Epoch: 13011 mean train loss:  1.18921278e-02, bound:  3.15818220e-01\n",
      "Epoch: 13012 mean train loss:  1.18916631e-02, bound:  3.15818161e-01\n",
      "Epoch: 13013 mean train loss:  1.18911834e-02, bound:  3.15818161e-01\n",
      "Epoch: 13014 mean train loss:  1.18908267e-02, bound:  3.15818161e-01\n",
      "Epoch: 13015 mean train loss:  1.18903276e-02, bound:  3.15818101e-01\n",
      "Epoch: 13016 mean train loss:  1.18899476e-02, bound:  3.15818101e-01\n",
      "Epoch: 13017 mean train loss:  1.18895136e-02, bound:  3.15818101e-01\n",
      "Epoch: 13018 mean train loss:  1.18890256e-02, bound:  3.15818042e-01\n",
      "Epoch: 13019 mean train loss:  1.18886214e-02, bound:  3.15818042e-01\n",
      "Epoch: 13020 mean train loss:  1.18881743e-02, bound:  3.15818042e-01\n",
      "Epoch: 13021 mean train loss:  1.18877869e-02, bound:  3.15818042e-01\n",
      "Epoch: 13022 mean train loss:  1.18873073e-02, bound:  3.15817982e-01\n",
      "Epoch: 13023 mean train loss:  1.18868249e-02, bound:  3.15817952e-01\n",
      "Epoch: 13024 mean train loss:  1.18863992e-02, bound:  3.15817952e-01\n",
      "Epoch: 13025 mean train loss:  1.18860491e-02, bound:  3.15817952e-01\n",
      "Epoch: 13026 mean train loss:  1.18855732e-02, bound:  3.15817893e-01\n",
      "Epoch: 13027 mean train loss:  1.18850218e-02, bound:  3.15817863e-01\n",
      "Epoch: 13028 mean train loss:  1.18847201e-02, bound:  3.15817863e-01\n",
      "Epoch: 13029 mean train loss:  1.18842795e-02, bound:  3.15817833e-01\n",
      "Epoch: 13030 mean train loss:  1.18838884e-02, bound:  3.15817833e-01\n",
      "Epoch: 13031 mean train loss:  1.18834190e-02, bound:  3.15817833e-01\n",
      "Epoch: 13032 mean train loss:  1.18829329e-02, bound:  3.15817773e-01\n",
      "Epoch: 13033 mean train loss:  1.18825668e-02, bound:  3.15817744e-01\n",
      "Epoch: 13034 mean train loss:  1.18821450e-02, bound:  3.15817744e-01\n",
      "Epoch: 13035 mean train loss:  1.18816616e-02, bound:  3.15817714e-01\n",
      "Epoch: 13036 mean train loss:  1.18812220e-02, bound:  3.15817714e-01\n",
      "Epoch: 13037 mean train loss:  1.18807433e-02, bound:  3.15817654e-01\n",
      "Epoch: 13038 mean train loss:  1.18802832e-02, bound:  3.15817654e-01\n",
      "Epoch: 13039 mean train loss:  1.18799014e-02, bound:  3.15817624e-01\n",
      "Epoch: 13040 mean train loss:  1.18794795e-02, bound:  3.15817624e-01\n",
      "Epoch: 13041 mean train loss:  1.18789868e-02, bound:  3.15817595e-01\n",
      "Epoch: 13042 mean train loss:  1.18786888e-02, bound:  3.15817565e-01\n",
      "Epoch: 13043 mean train loss:  1.18781924e-02, bound:  3.15817535e-01\n",
      "Epoch: 13044 mean train loss:  1.18777156e-02, bound:  3.15817535e-01\n",
      "Epoch: 13045 mean train loss:  1.18772266e-02, bound:  3.15817505e-01\n",
      "Epoch: 13046 mean train loss:  1.18767973e-02, bound:  3.15817475e-01\n",
      "Epoch: 13047 mean train loss:  1.18763791e-02, bound:  3.15817475e-01\n",
      "Epoch: 13048 mean train loss:  1.18759647e-02, bound:  3.15817416e-01\n",
      "Epoch: 13049 mean train loss:  1.18755279e-02, bound:  3.15817416e-01\n",
      "Epoch: 13050 mean train loss:  1.18751414e-02, bound:  3.15817416e-01\n",
      "Epoch: 13051 mean train loss:  1.18746841e-02, bound:  3.15817416e-01\n",
      "Epoch: 13052 mean train loss:  1.18741523e-02, bound:  3.15817386e-01\n",
      "Epoch: 13053 mean train loss:  1.18738525e-02, bound:  3.15817386e-01\n",
      "Epoch: 13054 mean train loss:  1.18733020e-02, bound:  3.15817297e-01\n",
      "Epoch: 13055 mean train loss:  1.18729332e-02, bound:  3.15817297e-01\n",
      "Epoch: 13056 mean train loss:  1.18724601e-02, bound:  3.15817297e-01\n",
      "Epoch: 13057 mean train loss:  1.18720429e-02, bound:  3.15817267e-01\n",
      "Epoch: 13058 mean train loss:  1.18715744e-02, bound:  3.15817237e-01\n",
      "Epoch: 13059 mean train loss:  1.18710902e-02, bound:  3.15817207e-01\n",
      "Epoch: 13060 mean train loss:  1.18707288e-02, bound:  3.15817207e-01\n",
      "Epoch: 13061 mean train loss:  1.18702650e-02, bound:  3.15817177e-01\n",
      "Epoch: 13062 mean train loss:  1.18697621e-02, bound:  3.15817177e-01\n",
      "Epoch: 13063 mean train loss:  1.18693877e-02, bound:  3.15817118e-01\n",
      "Epoch: 13064 mean train loss:  1.18689891e-02, bound:  3.15817118e-01\n",
      "Epoch: 13065 mean train loss:  1.18685113e-02, bound:  3.15817088e-01\n",
      "Epoch: 13066 mean train loss:  1.18680876e-02, bound:  3.15817088e-01\n",
      "Epoch: 13067 mean train loss:  1.18676703e-02, bound:  3.15817058e-01\n",
      "Epoch: 13068 mean train loss:  1.18671637e-02, bound:  3.15817028e-01\n",
      "Epoch: 13069 mean train loss:  1.18667064e-02, bound:  3.15816998e-01\n",
      "Epoch: 13070 mean train loss:  1.18664047e-02, bound:  3.15816998e-01\n",
      "Epoch: 13071 mean train loss:  1.18658682e-02, bound:  3.15816969e-01\n",
      "Epoch: 13072 mean train loss:  1.18654510e-02, bound:  3.15816939e-01\n",
      "Epoch: 13073 mean train loss:  1.18650179e-02, bound:  3.15816939e-01\n",
      "Epoch: 13074 mean train loss:  1.18645877e-02, bound:  3.15816879e-01\n",
      "Epoch: 13075 mean train loss:  1.18641658e-02, bound:  3.15816879e-01\n",
      "Epoch: 13076 mean train loss:  1.18636405e-02, bound:  3.15816849e-01\n",
      "Epoch: 13077 mean train loss:  1.18632084e-02, bound:  3.15816849e-01\n",
      "Epoch: 13078 mean train loss:  1.18627390e-02, bound:  3.15816820e-01\n",
      "Epoch: 13079 mean train loss:  1.18623646e-02, bound:  3.15816820e-01\n",
      "Epoch: 13080 mean train loss:  1.18619222e-02, bound:  3.15816790e-01\n",
      "Epoch: 13081 mean train loss:  1.18614109e-02, bound:  3.15816760e-01\n",
      "Epoch: 13082 mean train loss:  1.18609406e-02, bound:  3.15816730e-01\n",
      "Epoch: 13083 mean train loss:  1.18606891e-02, bound:  3.15816671e-01\n",
      "Epoch: 13084 mean train loss:  1.18600689e-02, bound:  3.15816671e-01\n",
      "Epoch: 13085 mean train loss:  1.18596610e-02, bound:  3.15816671e-01\n",
      "Epoch: 13086 mean train loss:  1.18591823e-02, bound:  3.15816671e-01\n",
      "Epoch: 13087 mean train loss:  1.18588340e-02, bound:  3.15816611e-01\n",
      "Epoch: 13088 mean train loss:  1.18583813e-02, bound:  3.15816611e-01\n",
      "Epoch: 13089 mean train loss:  1.18579082e-02, bound:  3.15816611e-01\n",
      "Epoch: 13090 mean train loss:  1.18574491e-02, bound:  3.15816551e-01\n",
      "Epoch: 13091 mean train loss:  1.18569927e-02, bound:  3.15816551e-01\n",
      "Epoch: 13092 mean train loss:  1.18566174e-02, bound:  3.15816522e-01\n",
      "Epoch: 13093 mean train loss:  1.18561452e-02, bound:  3.15816522e-01\n",
      "Epoch: 13094 mean train loss:  1.18556814e-02, bound:  3.15816522e-01\n",
      "Epoch: 13095 mean train loss:  1.18551916e-02, bound:  3.15816492e-01\n",
      "Epoch: 13096 mean train loss:  1.18547082e-02, bound:  3.15816432e-01\n",
      "Epoch: 13097 mean train loss:  1.18543459e-02, bound:  3.15816402e-01\n",
      "Epoch: 13098 mean train loss:  1.18539222e-02, bound:  3.15816402e-01\n",
      "Epoch: 13099 mean train loss:  1.18534332e-02, bound:  3.15816402e-01\n",
      "Epoch: 13100 mean train loss:  1.18529201e-02, bound:  3.15816343e-01\n",
      "Epoch: 13101 mean train loss:  1.18524767e-02, bound:  3.15816313e-01\n",
      "Epoch: 13102 mean train loss:  1.18521117e-02, bound:  3.15816313e-01\n",
      "Epoch: 13103 mean train loss:  1.18516525e-02, bound:  3.15816283e-01\n",
      "Epoch: 13104 mean train loss:  1.18512269e-02, bound:  3.15816283e-01\n",
      "Epoch: 13105 mean train loss:  1.18506867e-02, bound:  3.15816283e-01\n",
      "Epoch: 13106 mean train loss:  1.18503580e-02, bound:  3.15816224e-01\n",
      "Epoch: 13107 mean train loss:  1.18499286e-02, bound:  3.15816194e-01\n",
      "Epoch: 13108 mean train loss:  1.18493782e-02, bound:  3.15816194e-01\n",
      "Epoch: 13109 mean train loss:  1.18489005e-02, bound:  3.15816164e-01\n",
      "Epoch: 13110 mean train loss:  1.18486090e-02, bound:  3.15816164e-01\n",
      "Epoch: 13111 mean train loss:  1.18480902e-02, bound:  3.15816104e-01\n",
      "Epoch: 13112 mean train loss:  1.18476190e-02, bound:  3.15816104e-01\n",
      "Epoch: 13113 mean train loss:  1.18471673e-02, bound:  3.15816075e-01\n",
      "Epoch: 13114 mean train loss:  1.18468106e-02, bound:  3.15816075e-01\n",
      "Epoch: 13115 mean train loss:  1.18462741e-02, bound:  3.15816015e-01\n",
      "Epoch: 13116 mean train loss:  1.18458373e-02, bound:  3.15815985e-01\n",
      "Epoch: 13117 mean train loss:  1.18453596e-02, bound:  3.15815985e-01\n",
      "Epoch: 13118 mean train loss:  1.18449191e-02, bound:  3.15815985e-01\n",
      "Epoch: 13119 mean train loss:  1.18445279e-02, bound:  3.15815955e-01\n",
      "Epoch: 13120 mean train loss:  1.18440352e-02, bound:  3.15815955e-01\n",
      "Epoch: 13121 mean train loss:  1.18435957e-02, bound:  3.15815896e-01\n",
      "Epoch: 13122 mean train loss:  1.18431561e-02, bound:  3.15815866e-01\n",
      "Epoch: 13123 mean train loss:  1.18425945e-02, bound:  3.15815866e-01\n",
      "Epoch: 13124 mean train loss:  1.18421959e-02, bound:  3.15815836e-01\n",
      "Epoch: 13125 mean train loss:  1.18417647e-02, bound:  3.15815836e-01\n",
      "Epoch: 13126 mean train loss:  1.18413651e-02, bound:  3.15815777e-01\n",
      "Epoch: 13127 mean train loss:  1.18409349e-02, bound:  3.15815777e-01\n",
      "Epoch: 13128 mean train loss:  1.18405065e-02, bound:  3.15815747e-01\n",
      "Epoch: 13129 mean train loss:  1.18399849e-02, bound:  3.15815747e-01\n",
      "Epoch: 13130 mean train loss:  1.18395267e-02, bound:  3.15815717e-01\n",
      "Epoch: 13131 mean train loss:  1.18390191e-02, bound:  3.15815687e-01\n",
      "Epoch: 13132 mean train loss:  1.18387407e-02, bound:  3.15815657e-01\n",
      "Epoch: 13133 mean train loss:  1.18381921e-02, bound:  3.15815657e-01\n",
      "Epoch: 13134 mean train loss:  1.18376678e-02, bound:  3.15815657e-01\n",
      "Epoch: 13135 mean train loss:  1.18372124e-02, bound:  3.15815598e-01\n",
      "Epoch: 13136 mean train loss:  1.18368845e-02, bound:  3.15815598e-01\n",
      "Epoch: 13137 mean train loss:  1.18363937e-02, bound:  3.15815568e-01\n",
      "Epoch: 13138 mean train loss:  1.18358620e-02, bound:  3.15815538e-01\n",
      "Epoch: 13139 mean train loss:  1.18354484e-02, bound:  3.15815538e-01\n",
      "Epoch: 13140 mean train loss:  1.18351076e-02, bound:  3.15815508e-01\n",
      "Epoch: 13141 mean train loss:  1.18346633e-02, bound:  3.15815479e-01\n",
      "Epoch: 13142 mean train loss:  1.18341269e-02, bound:  3.15815449e-01\n",
      "Epoch: 13143 mean train loss:  1.18336389e-02, bound:  3.15815449e-01\n",
      "Epoch: 13144 mean train loss:  1.18331779e-02, bound:  3.15815419e-01\n",
      "Epoch: 13145 mean train loss:  1.18327718e-02, bound:  3.15815419e-01\n",
      "Epoch: 13146 mean train loss:  1.18322736e-02, bound:  3.15815389e-01\n",
      "Epoch: 13147 mean train loss:  1.18317781e-02, bound:  3.15815389e-01\n",
      "Epoch: 13148 mean train loss:  1.18313571e-02, bound:  3.15815330e-01\n",
      "Epoch: 13149 mean train loss:  1.18308971e-02, bound:  3.15815300e-01\n",
      "Epoch: 13150 mean train loss:  1.18304174e-02, bound:  3.15815300e-01\n",
      "Epoch: 13151 mean train loss:  1.18300477e-02, bound:  3.15815270e-01\n",
      "Epoch: 13152 mean train loss:  1.18295746e-02, bound:  3.15815270e-01\n",
      "Epoch: 13153 mean train loss:  1.18291704e-02, bound:  3.15815210e-01\n",
      "Epoch: 13154 mean train loss:  1.18288342e-02, bound:  3.15815210e-01\n",
      "Epoch: 13155 mean train loss:  1.18283089e-02, bound:  3.15815181e-01\n",
      "Epoch: 13156 mean train loss:  1.18277306e-02, bound:  3.15815181e-01\n",
      "Epoch: 13157 mean train loss:  1.18272854e-02, bound:  3.15815151e-01\n",
      "Epoch: 13158 mean train loss:  1.18267769e-02, bound:  3.15815121e-01\n",
      "Epoch: 13159 mean train loss:  1.18264072e-02, bound:  3.15815091e-01\n",
      "Epoch: 13160 mean train loss:  1.18258651e-02, bound:  3.15815091e-01\n",
      "Epoch: 13161 mean train loss:  1.18254302e-02, bound:  3.15815091e-01\n",
      "Epoch: 13162 mean train loss:  1.18249832e-02, bound:  3.15815002e-01\n",
      "Epoch: 13163 mean train loss:  1.18245063e-02, bound:  3.15815002e-01\n",
      "Epoch: 13164 mean train loss:  1.18240891e-02, bound:  3.15814972e-01\n",
      "Epoch: 13165 mean train loss:  1.18236532e-02, bound:  3.15814972e-01\n",
      "Epoch: 13166 mean train loss:  1.18231019e-02, bound:  3.15814972e-01\n",
      "Epoch: 13167 mean train loss:  1.18227322e-02, bound:  3.15814942e-01\n",
      "Epoch: 13168 mean train loss:  1.18222302e-02, bound:  3.15814883e-01\n",
      "Epoch: 13169 mean train loss:  1.18217384e-02, bound:  3.15814883e-01\n",
      "Epoch: 13170 mean train loss:  1.18213408e-02, bound:  3.15814853e-01\n",
      "Epoch: 13171 mean train loss:  1.18209040e-02, bound:  3.15814853e-01\n",
      "Epoch: 13172 mean train loss:  1.18204104e-02, bound:  3.15814853e-01\n",
      "Epoch: 13173 mean train loss:  1.18198274e-02, bound:  3.15814793e-01\n",
      "Epoch: 13174 mean train loss:  1.18194949e-02, bound:  3.15814763e-01\n",
      "Epoch: 13175 mean train loss:  1.18190451e-02, bound:  3.15814763e-01\n",
      "Epoch: 13176 mean train loss:  1.18185719e-02, bound:  3.15814734e-01\n",
      "Epoch: 13177 mean train loss:  1.18181212e-02, bound:  3.15814734e-01\n",
      "Epoch: 13178 mean train loss:  1.18176527e-02, bound:  3.15814674e-01\n",
      "Epoch: 13179 mean train loss:  1.18172877e-02, bound:  3.15814674e-01\n",
      "Epoch: 13180 mean train loss:  1.18167503e-02, bound:  3.15814644e-01\n",
      "Epoch: 13181 mean train loss:  1.18163088e-02, bound:  3.15814644e-01\n",
      "Epoch: 13182 mean train loss:  1.18158758e-02, bound:  3.15814614e-01\n",
      "Epoch: 13183 mean train loss:  1.18154464e-02, bound:  3.15814614e-01\n",
      "Epoch: 13184 mean train loss:  1.18149240e-02, bound:  3.15814555e-01\n",
      "Epoch: 13185 mean train loss:  1.18144471e-02, bound:  3.15814555e-01\n",
      "Epoch: 13186 mean train loss:  1.18140234e-02, bound:  3.15814525e-01\n",
      "Epoch: 13187 mean train loss:  1.18135568e-02, bound:  3.15814495e-01\n",
      "Epoch: 13188 mean train loss:  1.18130315e-02, bound:  3.15814495e-01\n",
      "Epoch: 13189 mean train loss:  1.18125184e-02, bound:  3.15814435e-01\n",
      "Epoch: 13190 mean train loss:  1.18121058e-02, bound:  3.15814435e-01\n",
      "Epoch: 13191 mean train loss:  1.18117044e-02, bound:  3.15814406e-01\n",
      "Epoch: 13192 mean train loss:  1.18112341e-02, bound:  3.15814406e-01\n",
      "Epoch: 13193 mean train loss:  1.18107339e-02, bound:  3.15814346e-01\n",
      "Epoch: 13194 mean train loss:  1.18102999e-02, bound:  3.15814316e-01\n",
      "Epoch: 13195 mean train loss:  1.18098203e-02, bound:  3.15814316e-01\n",
      "Epoch: 13196 mean train loss:  1.18094040e-02, bound:  3.15814286e-01\n",
      "Epoch: 13197 mean train loss:  1.18089169e-02, bound:  3.15814286e-01\n",
      "Epoch: 13198 mean train loss:  1.18084829e-02, bound:  3.15814286e-01\n",
      "Epoch: 13199 mean train loss:  1.18079195e-02, bound:  3.15814227e-01\n",
      "Epoch: 13200 mean train loss:  1.18075171e-02, bound:  3.15814227e-01\n",
      "Epoch: 13201 mean train loss:  1.18070105e-02, bound:  3.15814197e-01\n",
      "Epoch: 13202 mean train loss:  1.18065048e-02, bound:  3.15814167e-01\n",
      "Epoch: 13203 mean train loss:  1.18061313e-02, bound:  3.15814167e-01\n",
      "Epoch: 13204 mean train loss:  1.18056526e-02, bound:  3.15814108e-01\n",
      "Epoch: 13205 mean train loss:  1.18051544e-02, bound:  3.15814108e-01\n",
      "Epoch: 13206 mean train loss:  1.18047474e-02, bound:  3.15814108e-01\n",
      "Epoch: 13207 mean train loss:  1.18042482e-02, bound:  3.15814048e-01\n",
      "Epoch: 13208 mean train loss:  1.18037835e-02, bound:  3.15814048e-01\n",
      "Epoch: 13209 mean train loss:  1.18033169e-02, bound:  3.15814018e-01\n",
      "Epoch: 13210 mean train loss:  1.18028289e-02, bound:  3.15813988e-01\n",
      "Epoch: 13211 mean train loss:  1.18022924e-02, bound:  3.15813988e-01\n",
      "Epoch: 13212 mean train loss:  1.18019693e-02, bound:  3.15813959e-01\n",
      "Epoch: 13213 mean train loss:  1.18014896e-02, bound:  3.15813959e-01\n",
      "Epoch: 13214 mean train loss:  1.18010342e-02, bound:  3.15813899e-01\n",
      "Epoch: 13215 mean train loss:  1.18005779e-02, bound:  3.15813869e-01\n",
      "Epoch: 13216 mean train loss:  1.18001709e-02, bound:  3.15813869e-01\n",
      "Epoch: 13217 mean train loss:  1.17995711e-02, bound:  3.15813839e-01\n",
      "Epoch: 13218 mean train loss:  1.17991222e-02, bound:  3.15813839e-01\n",
      "Epoch: 13219 mean train loss:  1.17985746e-02, bound:  3.15813780e-01\n",
      "Epoch: 13220 mean train loss:  1.17982142e-02, bound:  3.15813750e-01\n",
      "Epoch: 13221 mean train loss:  1.17976852e-02, bound:  3.15813750e-01\n",
      "Epoch: 13222 mean train loss:  1.17972475e-02, bound:  3.15813750e-01\n",
      "Epoch: 13223 mean train loss:  1.17968712e-02, bound:  3.15813720e-01\n",
      "Epoch: 13224 mean train loss:  1.17963506e-02, bound:  3.15813661e-01\n",
      "Epoch: 13225 mean train loss:  1.17959008e-02, bound:  3.15813661e-01\n",
      "Epoch: 13226 mean train loss:  1.17954118e-02, bound:  3.15813661e-01\n",
      "Epoch: 13227 mean train loss:  1.17948828e-02, bound:  3.15813631e-01\n",
      "Epoch: 13228 mean train loss:  1.17944842e-02, bound:  3.15813601e-01\n",
      "Epoch: 13229 mean train loss:  1.17940400e-02, bound:  3.15813571e-01\n",
      "Epoch: 13230 mean train loss:  1.17935436e-02, bound:  3.15813541e-01\n",
      "Epoch: 13231 mean train loss:  1.17930332e-02, bound:  3.15813541e-01\n",
      "Epoch: 13232 mean train loss:  1.17925508e-02, bound:  3.15813541e-01\n",
      "Epoch: 13233 mean train loss:  1.17921196e-02, bound:  3.15813482e-01\n",
      "Epoch: 13234 mean train loss:  1.17916260e-02, bound:  3.15813452e-01\n",
      "Epoch: 13235 mean train loss:  1.17911492e-02, bound:  3.15813422e-01\n",
      "Epoch: 13236 mean train loss:  1.17907021e-02, bound:  3.15813422e-01\n",
      "Epoch: 13237 mean train loss:  1.17902216e-02, bound:  3.15813422e-01\n",
      "Epoch: 13238 mean train loss:  1.17898071e-02, bound:  3.15813363e-01\n",
      "Epoch: 13239 mean train loss:  1.17892679e-02, bound:  3.15813333e-01\n",
      "Epoch: 13240 mean train loss:  1.17888898e-02, bound:  3.15813333e-01\n",
      "Epoch: 13241 mean train loss:  1.17884260e-02, bound:  3.15813303e-01\n",
      "Epoch: 13242 mean train loss:  1.17879398e-02, bound:  3.15813303e-01\n",
      "Epoch: 13243 mean train loss:  1.17874471e-02, bound:  3.15813243e-01\n",
      "Epoch: 13244 mean train loss:  1.17869610e-02, bound:  3.15813243e-01\n",
      "Epoch: 13245 mean train loss:  1.17865717e-02, bound:  3.15813214e-01\n",
      "Epoch: 13246 mean train loss:  1.17859207e-02, bound:  3.15813214e-01\n",
      "Epoch: 13247 mean train loss:  1.17855472e-02, bound:  3.15813184e-01\n",
      "Epoch: 13248 mean train loss:  1.17850248e-02, bound:  3.15813154e-01\n",
      "Epoch: 13249 mean train loss:  1.17846318e-02, bound:  3.15813124e-01\n",
      "Epoch: 13250 mean train loss:  1.17841382e-02, bound:  3.15813094e-01\n",
      "Epoch: 13251 mean train loss:  1.17836120e-02, bound:  3.15813094e-01\n",
      "Epoch: 13252 mean train loss:  1.17831873e-02, bound:  3.15813065e-01\n",
      "Epoch: 13253 mean train loss:  1.17827244e-02, bound:  3.15813005e-01\n",
      "Epoch: 13254 mean train loss:  1.17822066e-02, bound:  3.15813005e-01\n",
      "Epoch: 13255 mean train loss:  1.17817530e-02, bound:  3.15813005e-01\n",
      "Epoch: 13256 mean train loss:  1.17812557e-02, bound:  3.15812975e-01\n",
      "Epoch: 13257 mean train loss:  1.17808860e-02, bound:  3.15812945e-01\n",
      "Epoch: 13258 mean train loss:  1.17802862e-02, bound:  3.15812945e-01\n",
      "Epoch: 13259 mean train loss:  1.17798299e-02, bound:  3.15812886e-01\n",
      "Epoch: 13260 mean train loss:  1.17793912e-02, bound:  3.15812886e-01\n",
      "Epoch: 13261 mean train loss:  1.17790019e-02, bound:  3.15812856e-01\n",
      "Epoch: 13262 mean train loss:  1.17783993e-02, bound:  3.15812856e-01\n",
      "Epoch: 13263 mean train loss:  1.17779849e-02, bound:  3.15812856e-01\n",
      "Epoch: 13264 mean train loss:  1.17775155e-02, bound:  3.15812796e-01\n",
      "Epoch: 13265 mean train loss:  1.17770266e-02, bound:  3.15812767e-01\n",
      "Epoch: 13266 mean train loss:  1.17765088e-02, bound:  3.15812737e-01\n",
      "Epoch: 13267 mean train loss:  1.17760673e-02, bound:  3.15812737e-01\n",
      "Epoch: 13268 mean train loss:  1.17754964e-02, bound:  3.15812677e-01\n",
      "Epoch: 13269 mean train loss:  1.17751136e-02, bound:  3.15812677e-01\n",
      "Epoch: 13270 mean train loss:  1.17746666e-02, bound:  3.15812677e-01\n",
      "Epoch: 13271 mean train loss:  1.17741674e-02, bound:  3.15812647e-01\n",
      "Epoch: 13272 mean train loss:  1.17736747e-02, bound:  3.15812618e-01\n",
      "Epoch: 13273 mean train loss:  1.17732137e-02, bound:  3.15812618e-01\n",
      "Epoch: 13274 mean train loss:  1.17727341e-02, bound:  3.15812558e-01\n",
      "Epoch: 13275 mean train loss:  1.17722629e-02, bound:  3.15812558e-01\n",
      "Epoch: 13276 mean train loss:  1.17718736e-02, bound:  3.15812528e-01\n",
      "Epoch: 13277 mean train loss:  1.17713269e-02, bound:  3.15812528e-01\n",
      "Epoch: 13278 mean train loss:  1.17708575e-02, bound:  3.15812469e-01\n",
      "Epoch: 13279 mean train loss:  1.17703695e-02, bound:  3.15812439e-01\n",
      "Epoch: 13280 mean train loss:  1.17698731e-02, bound:  3.15812439e-01\n",
      "Epoch: 13281 mean train loss:  1.17693990e-02, bound:  3.15812439e-01\n",
      "Epoch: 13282 mean train loss:  1.17688999e-02, bound:  3.15812409e-01\n",
      "Epoch: 13283 mean train loss:  1.17683969e-02, bound:  3.15812349e-01\n",
      "Epoch: 13284 mean train loss:  1.17680021e-02, bound:  3.15812320e-01\n",
      "Epoch: 13285 mean train loss:  1.17675308e-02, bound:  3.15812320e-01\n",
      "Epoch: 13286 mean train loss:  1.17670093e-02, bound:  3.15812320e-01\n",
      "Epoch: 13287 mean train loss:  1.17665846e-02, bound:  3.15812290e-01\n",
      "Epoch: 13288 mean train loss:  1.17660630e-02, bound:  3.15812290e-01\n",
      "Epoch: 13289 mean train loss:  1.17655853e-02, bound:  3.15812230e-01\n",
      "Epoch: 13290 mean train loss:  1.17650516e-02, bound:  3.15812200e-01\n",
      "Epoch: 13291 mean train loss:  1.17647164e-02, bound:  3.15812200e-01\n",
      "Epoch: 13292 mean train loss:  1.17642069e-02, bound:  3.15812171e-01\n",
      "Epoch: 13293 mean train loss:  1.17636649e-02, bound:  3.15812141e-01\n",
      "Epoch: 13294 mean train loss:  1.17632216e-02, bound:  3.15812111e-01\n",
      "Epoch: 13295 mean train loss:  1.17627271e-02, bound:  3.15812111e-01\n",
      "Epoch: 13296 mean train loss:  1.17622549e-02, bound:  3.15812081e-01\n",
      "Epoch: 13297 mean train loss:  1.17617352e-02, bound:  3.15812051e-01\n",
      "Epoch: 13298 mean train loss:  1.17612807e-02, bound:  3.15812021e-01\n",
      "Epoch: 13299 mean train loss:  1.17607899e-02, bound:  3.15811992e-01\n",
      "Epoch: 13300 mean train loss:  1.17603075e-02, bound:  3.15811992e-01\n",
      "Epoch: 13301 mean train loss:  1.17599349e-02, bound:  3.15811962e-01\n",
      "Epoch: 13302 mean train loss:  1.17593426e-02, bound:  3.15811962e-01\n",
      "Epoch: 13303 mean train loss:  1.17589207e-02, bound:  3.15811902e-01\n",
      "Epoch: 13304 mean train loss:  1.17584188e-02, bound:  3.15811872e-01\n",
      "Epoch: 13305 mean train loss:  1.17579186e-02, bound:  3.15811872e-01\n",
      "Epoch: 13306 mean train loss:  1.17574614e-02, bound:  3.15811843e-01\n",
      "Epoch: 13307 mean train loss:  1.17569380e-02, bound:  3.15811813e-01\n",
      "Epoch: 13308 mean train loss:  1.17564909e-02, bound:  3.15811813e-01\n",
      "Epoch: 13309 mean train loss:  1.17560364e-02, bound:  3.15811753e-01\n",
      "Epoch: 13310 mean train loss:  1.17555391e-02, bound:  3.15811753e-01\n",
      "Epoch: 13311 mean train loss:  1.17551042e-02, bound:  3.15811723e-01\n",
      "Epoch: 13312 mean train loss:  1.17545435e-02, bound:  3.15811723e-01\n",
      "Epoch: 13313 mean train loss:  1.17540089e-02, bound:  3.15811694e-01\n",
      "Epoch: 13314 mean train loss:  1.17534949e-02, bound:  3.15811664e-01\n",
      "Epoch: 13315 mean train loss:  1.17531354e-02, bound:  3.15811664e-01\n",
      "Epoch: 13316 mean train loss:  1.17525989e-02, bound:  3.15811604e-01\n",
      "Epoch: 13317 mean train loss:  1.17520848e-02, bound:  3.15811604e-01\n",
      "Epoch: 13318 mean train loss:  1.17516555e-02, bound:  3.15811574e-01\n",
      "Epoch: 13319 mean train loss:  1.17511908e-02, bound:  3.15811545e-01\n",
      "Epoch: 13320 mean train loss:  1.17505910e-02, bound:  3.15811545e-01\n",
      "Epoch: 13321 mean train loss:  1.17502445e-02, bound:  3.15811515e-01\n",
      "Epoch: 13322 mean train loss:  1.17497118e-02, bound:  3.15811515e-01\n",
      "Epoch: 13323 mean train loss:  1.17491996e-02, bound:  3.15811455e-01\n",
      "Epoch: 13324 mean train loss:  1.17488019e-02, bound:  3.15811425e-01\n",
      "Epoch: 13325 mean train loss:  1.17483307e-02, bound:  3.15811425e-01\n",
      "Epoch: 13326 mean train loss:  1.17478864e-02, bound:  3.15811396e-01\n",
      "Epoch: 13327 mean train loss:  1.17473323e-02, bound:  3.15811396e-01\n",
      "Epoch: 13328 mean train loss:  1.17467847e-02, bound:  3.15811336e-01\n",
      "Epoch: 13329 mean train loss:  1.17463535e-02, bound:  3.15811306e-01\n",
      "Epoch: 13330 mean train loss:  1.17458235e-02, bound:  3.15811306e-01\n",
      "Epoch: 13331 mean train loss:  1.17453048e-02, bound:  3.15811276e-01\n",
      "Epoch: 13332 mean train loss:  1.17448866e-02, bound:  3.15811276e-01\n",
      "Epoch: 13333 mean train loss:  1.17444200e-02, bound:  3.15811217e-01\n",
      "Epoch: 13334 mean train loss:  1.17439199e-02, bound:  3.15811187e-01\n",
      "Epoch: 13335 mean train loss:  1.17434552e-02, bound:  3.15811187e-01\n",
      "Epoch: 13336 mean train loss:  1.17428871e-02, bound:  3.15811127e-01\n",
      "Epoch: 13337 mean train loss:  1.17424391e-02, bound:  3.15811127e-01\n",
      "Epoch: 13338 mean train loss:  1.17418533e-02, bound:  3.15811098e-01\n",
      "Epoch: 13339 mean train loss:  1.17413523e-02, bound:  3.15811098e-01\n",
      "Epoch: 13340 mean train loss:  1.17408829e-02, bound:  3.15811098e-01\n",
      "Epoch: 13341 mean train loss:  1.17404442e-02, bound:  3.15811008e-01\n",
      "Epoch: 13342 mean train loss:  1.17399143e-02, bound:  3.15811008e-01\n",
      "Epoch: 13343 mean train loss:  1.17395008e-02, bound:  3.15811008e-01\n",
      "Epoch: 13344 mean train loss:  1.17389373e-02, bound:  3.15810978e-01\n",
      "Epoch: 13345 mean train loss:  1.17385229e-02, bound:  3.15810978e-01\n",
      "Epoch: 13346 mean train loss:  1.17379725e-02, bound:  3.15810889e-01\n",
      "Epoch: 13347 mean train loss:  1.17376149e-02, bound:  3.15810889e-01\n",
      "Epoch: 13348 mean train loss:  1.17371120e-02, bound:  3.15810889e-01\n",
      "Epoch: 13349 mean train loss:  1.17365252e-02, bound:  3.15810859e-01\n",
      "Epoch: 13350 mean train loss:  1.17360866e-02, bound:  3.15810859e-01\n",
      "Epoch: 13351 mean train loss:  1.17356200e-02, bound:  3.15810800e-01\n",
      "Epoch: 13352 mean train loss:  1.17351022e-02, bound:  3.15810800e-01\n",
      "Epoch: 13353 mean train loss:  1.17345545e-02, bound:  3.15810770e-01\n",
      "Epoch: 13354 mean train loss:  1.17340880e-02, bound:  3.15810740e-01\n",
      "Epoch: 13355 mean train loss:  1.17336102e-02, bound:  3.15810740e-01\n",
      "Epoch: 13356 mean train loss:  1.17330728e-02, bound:  3.15810680e-01\n",
      "Epoch: 13357 mean train loss:  1.17326472e-02, bound:  3.15810680e-01\n",
      "Epoch: 13358 mean train loss:  1.17320819e-02, bound:  3.15810651e-01\n",
      "Epoch: 13359 mean train loss:  1.17316265e-02, bound:  3.15810621e-01\n",
      "Epoch: 13360 mean train loss:  1.17311766e-02, bound:  3.15810621e-01\n",
      "Epoch: 13361 mean train loss:  1.17307352e-02, bound:  3.15810561e-01\n",
      "Epoch: 13362 mean train loss:  1.17302341e-02, bound:  3.15810561e-01\n",
      "Epoch: 13363 mean train loss:  1.17297824e-02, bound:  3.15810531e-01\n",
      "Epoch: 13364 mean train loss:  1.17292311e-02, bound:  3.15810472e-01\n",
      "Epoch: 13365 mean train loss:  1.17287142e-02, bound:  3.15810472e-01\n",
      "Epoch: 13366 mean train loss:  1.17281433e-02, bound:  3.15810442e-01\n",
      "Epoch: 13367 mean train loss:  1.17277335e-02, bound:  3.15810442e-01\n",
      "Epoch: 13368 mean train loss:  1.17273126e-02, bound:  3.15810412e-01\n",
      "Epoch: 13369 mean train loss:  1.17267827e-02, bound:  3.15810412e-01\n",
      "Epoch: 13370 mean train loss:  1.17263654e-02, bound:  3.15810353e-01\n",
      "Epoch: 13371 mean train loss:  1.17258299e-02, bound:  3.15810323e-01\n",
      "Epoch: 13372 mean train loss:  1.17251910e-02, bound:  3.15810293e-01\n",
      "Epoch: 13373 mean train loss:  1.17247039e-02, bound:  3.15810293e-01\n",
      "Epoch: 13374 mean train loss:  1.17243165e-02, bound:  3.15810293e-01\n",
      "Epoch: 13375 mean train loss:  1.17237614e-02, bound:  3.15810233e-01\n",
      "Epoch: 13376 mean train loss:  1.17231878e-02, bound:  3.15810233e-01\n",
      "Epoch: 13377 mean train loss:  1.17227957e-02, bound:  3.15810233e-01\n",
      "Epoch: 13378 mean train loss:  1.17223505e-02, bound:  3.15810174e-01\n",
      "Epoch: 13379 mean train loss:  1.17218252e-02, bound:  3.15810144e-01\n",
      "Epoch: 13380 mean train loss:  1.17212767e-02, bound:  3.15810144e-01\n",
      "Epoch: 13381 mean train loss:  1.17207682e-02, bound:  3.15810114e-01\n",
      "Epoch: 13382 mean train loss:  1.17203174e-02, bound:  3.15810114e-01\n",
      "Epoch: 13383 mean train loss:  1.17198043e-02, bound:  3.15810084e-01\n",
      "Epoch: 13384 mean train loss:  1.17194680e-02, bound:  3.15810025e-01\n",
      "Epoch: 13385 mean train loss:  1.17188785e-02, bound:  3.15810025e-01\n",
      "Epoch: 13386 mean train loss:  1.17183765e-02, bound:  3.15809995e-01\n",
      "Epoch: 13387 mean train loss:  1.17179491e-02, bound:  3.15809995e-01\n",
      "Epoch: 13388 mean train loss:  1.17173586e-02, bound:  3.15809965e-01\n",
      "Epoch: 13389 mean train loss:  1.17168473e-02, bound:  3.15809906e-01\n",
      "Epoch: 13390 mean train loss:  1.17163574e-02, bound:  3.15809906e-01\n",
      "Epoch: 13391 mean train loss:  1.17158722e-02, bound:  3.15809876e-01\n",
      "Epoch: 13392 mean train loss:  1.17153097e-02, bound:  3.15809846e-01\n",
      "Epoch: 13393 mean train loss:  1.17148599e-02, bound:  3.15809846e-01\n",
      "Epoch: 13394 mean train loss:  1.17144044e-02, bound:  3.15809786e-01\n",
      "Epoch: 13395 mean train loss:  1.17139192e-02, bound:  3.15809786e-01\n",
      "Epoch: 13396 mean train loss:  1.17133837e-02, bound:  3.15809757e-01\n",
      "Epoch: 13397 mean train loss:  1.17128361e-02, bound:  3.15809727e-01\n",
      "Epoch: 13398 mean train loss:  1.17123388e-02, bound:  3.15809727e-01\n",
      "Epoch: 13399 mean train loss:  1.17118871e-02, bound:  3.15809667e-01\n",
      "Epoch: 13400 mean train loss:  1.17114475e-02, bound:  3.15809667e-01\n",
      "Epoch: 13401 mean train loss:  1.17108291e-02, bound:  3.15809667e-01\n",
      "Epoch: 13402 mean train loss:  1.17104305e-02, bound:  3.15809608e-01\n",
      "Epoch: 13403 mean train loss:  1.17099984e-02, bound:  3.15809608e-01\n",
      "Epoch: 13404 mean train loss:  1.17094815e-02, bound:  3.15809578e-01\n",
      "Epoch: 13405 mean train loss:  1.17088025e-02, bound:  3.15809578e-01\n",
      "Epoch: 13406 mean train loss:  1.17084002e-02, bound:  3.15809548e-01\n",
      "Epoch: 13407 mean train loss:  1.17079429e-02, bound:  3.15809458e-01\n",
      "Epoch: 13408 mean train loss:  1.17074056e-02, bound:  3.15809458e-01\n",
      "Epoch: 13409 mean train loss:  1.17068700e-02, bound:  3.15809458e-01\n",
      "Epoch: 13410 mean train loss:  1.17063904e-02, bound:  3.15809429e-01\n",
      "Epoch: 13411 mean train loss:  1.17058596e-02, bound:  3.15809429e-01\n",
      "Epoch: 13412 mean train loss:  1.17054246e-02, bound:  3.15809369e-01\n",
      "Epoch: 13413 mean train loss:  1.17049757e-02, bound:  3.15809369e-01\n",
      "Epoch: 13414 mean train loss:  1.17043797e-02, bound:  3.15809339e-01\n",
      "Epoch: 13415 mean train loss:  1.17039541e-02, bound:  3.15809309e-01\n",
      "Epoch: 13416 mean train loss:  1.17033133e-02, bound:  3.15809309e-01\n",
      "Epoch: 13417 mean train loss:  1.17028682e-02, bound:  3.15809280e-01\n",
      "Epoch: 13418 mean train loss:  1.17024481e-02, bound:  3.15809250e-01\n",
      "Epoch: 13419 mean train loss:  1.17019443e-02, bound:  3.15809220e-01\n",
      "Epoch: 13420 mean train loss:  1.17014544e-02, bound:  3.15809190e-01\n",
      "Epoch: 13421 mean train loss:  1.17008463e-02, bound:  3.15809190e-01\n",
      "Epoch: 13422 mean train loss:  1.17003303e-02, bound:  3.15809131e-01\n",
      "Epoch: 13423 mean train loss:  1.16998432e-02, bound:  3.15809131e-01\n",
      "Epoch: 13424 mean train loss:  1.16994018e-02, bound:  3.15809101e-01\n",
      "Epoch: 13425 mean train loss:  1.16988914e-02, bound:  3.15809071e-01\n",
      "Epoch: 13426 mean train loss:  1.16983056e-02, bound:  3.15809071e-01\n",
      "Epoch: 13427 mean train loss:  1.16978418e-02, bound:  3.15809011e-01\n",
      "Epoch: 13428 mean train loss:  1.16973538e-02, bound:  3.15808982e-01\n",
      "Epoch: 13429 mean train loss:  1.16969319e-02, bound:  3.15808982e-01\n",
      "Epoch: 13430 mean train loss:  1.16963265e-02, bound:  3.15808982e-01\n",
      "Epoch: 13431 mean train loss:  1.16957892e-02, bound:  3.15808952e-01\n",
      "Epoch: 13432 mean train loss:  1.16953775e-02, bound:  3.15808892e-01\n",
      "Epoch: 13433 mean train loss:  1.16948141e-02, bound:  3.15808862e-01\n",
      "Epoch: 13434 mean train loss:  1.16944173e-02, bound:  3.15808862e-01\n",
      "Epoch: 13435 mean train loss:  1.16937784e-02, bound:  3.15808862e-01\n",
      "Epoch: 13436 mean train loss:  1.16933249e-02, bound:  3.15808803e-01\n",
      "Epoch: 13437 mean train loss:  1.16927875e-02, bound:  3.15808803e-01\n",
      "Epoch: 13438 mean train loss:  1.16923051e-02, bound:  3.15808743e-01\n",
      "Epoch: 13439 mean train loss:  1.16918413e-02, bound:  3.15808743e-01\n",
      "Epoch: 13440 mean train loss:  1.16912751e-02, bound:  3.15808743e-01\n",
      "Epoch: 13441 mean train loss:  1.16907628e-02, bound:  3.15808684e-01\n",
      "Epoch: 13442 mean train loss:  1.16903372e-02, bound:  3.15808684e-01\n",
      "Epoch: 13443 mean train loss:  1.16897980e-02, bound:  3.15808654e-01\n",
      "Epoch: 13444 mean train loss:  1.16893416e-02, bound:  3.15808624e-01\n",
      "Epoch: 13445 mean train loss:  1.16887037e-02, bound:  3.15808624e-01\n",
      "Epoch: 13446 mean train loss:  1.16882324e-02, bound:  3.15808564e-01\n",
      "Epoch: 13447 mean train loss:  1.16877109e-02, bound:  3.15808564e-01\n",
      "Epoch: 13448 mean train loss:  1.16871009e-02, bound:  3.15808535e-01\n",
      "Epoch: 13449 mean train loss:  1.16867041e-02, bound:  3.15808475e-01\n",
      "Epoch: 13450 mean train loss:  1.16863390e-02, bound:  3.15808475e-01\n",
      "Epoch: 13451 mean train loss:  1.16856927e-02, bound:  3.15808445e-01\n",
      "Epoch: 13452 mean train loss:  1.16851386e-02, bound:  3.15808415e-01\n",
      "Epoch: 13453 mean train loss:  1.16846450e-02, bound:  3.15808415e-01\n",
      "Epoch: 13454 mean train loss:  1.16841774e-02, bound:  3.15808356e-01\n",
      "Epoch: 13455 mean train loss:  1.16837267e-02, bound:  3.15808326e-01\n",
      "Epoch: 13456 mean train loss:  1.16831372e-02, bound:  3.15808326e-01\n",
      "Epoch: 13457 mean train loss:  1.16826613e-02, bound:  3.15808296e-01\n",
      "Epoch: 13458 mean train loss:  1.16822096e-02, bound:  3.15808237e-01\n",
      "Epoch: 13459 mean train loss:  1.16816303e-02, bound:  3.15808237e-01\n",
      "Epoch: 13460 mean train loss:  1.16812028e-02, bound:  3.15808237e-01\n",
      "Epoch: 13461 mean train loss:  1.16805723e-02, bound:  3.15808237e-01\n",
      "Epoch: 13462 mean train loss:  1.16802016e-02, bound:  3.15808177e-01\n",
      "Epoch: 13463 mean train loss:  1.16794948e-02, bound:  3.15808147e-01\n",
      "Epoch: 13464 mean train loss:  1.16791558e-02, bound:  3.15808147e-01\n",
      "Epoch: 13465 mean train loss:  1.16785662e-02, bound:  3.15808117e-01\n",
      "Epoch: 13466 mean train loss:  1.16780838e-02, bound:  3.15808058e-01\n",
      "Epoch: 13467 mean train loss:  1.16775148e-02, bound:  3.15808058e-01\n",
      "Epoch: 13468 mean train loss:  1.16770733e-02, bound:  3.15808028e-01\n",
      "Epoch: 13469 mean train loss:  1.16764810e-02, bound:  3.15808028e-01\n",
      "Epoch: 13470 mean train loss:  1.16760740e-02, bound:  3.15807998e-01\n",
      "Epoch: 13471 mean train loss:  1.16754612e-02, bound:  3.15807939e-01\n",
      "Epoch: 13472 mean train loss:  1.16750486e-02, bound:  3.15807939e-01\n",
      "Epoch: 13473 mean train loss:  1.16744833e-02, bound:  3.15807909e-01\n",
      "Epoch: 13474 mean train loss:  1.16740167e-02, bound:  3.15807879e-01\n",
      "Epoch: 13475 mean train loss:  1.16734263e-02, bound:  3.15807879e-01\n",
      "Epoch: 13476 mean train loss:  1.16729727e-02, bound:  3.15807849e-01\n",
      "Epoch: 13477 mean train loss:  1.16724111e-02, bound:  3.15807849e-01\n",
      "Epoch: 13478 mean train loss:  1.16720609e-02, bound:  3.15807790e-01\n",
      "Epoch: 13479 mean train loss:  1.16714202e-02, bound:  3.15807760e-01\n",
      "Epoch: 13480 mean train loss:  1.16709694e-02, bound:  3.15807760e-01\n",
      "Epoch: 13481 mean train loss:  1.16704097e-02, bound:  3.15807730e-01\n",
      "Epoch: 13482 mean train loss:  1.16698220e-02, bound:  3.15807670e-01\n",
      "Epoch: 13483 mean train loss:  1.16693294e-02, bound:  3.15807670e-01\n",
      "Epoch: 13484 mean train loss:  1.16688320e-02, bound:  3.15807641e-01\n",
      "Epoch: 13485 mean train loss:  1.16683859e-02, bound:  3.15807581e-01\n",
      "Epoch: 13486 mean train loss:  1.16678867e-02, bound:  3.15807581e-01\n",
      "Epoch: 13487 mean train loss:  1.16673457e-02, bound:  3.15807551e-01\n",
      "Epoch: 13488 mean train loss:  1.16667747e-02, bound:  3.15807551e-01\n",
      "Epoch: 13489 mean train loss:  1.16662662e-02, bound:  3.15807551e-01\n",
      "Epoch: 13490 mean train loss:  1.16657652e-02, bound:  3.15807462e-01\n",
      "Epoch: 13491 mean train loss:  1.16653843e-02, bound:  3.15807462e-01\n",
      "Epoch: 13492 mean train loss:  1.16647352e-02, bound:  3.15807432e-01\n",
      "Epoch: 13493 mean train loss:  1.16642611e-02, bound:  3.15807432e-01\n",
      "Epoch: 13494 mean train loss:  1.16637228e-02, bound:  3.15807432e-01\n",
      "Epoch: 13495 mean train loss:  1.16631174e-02, bound:  3.15807372e-01\n",
      "Epoch: 13496 mean train loss:  1.16627794e-02, bound:  3.15807343e-01\n",
      "Epoch: 13497 mean train loss:  1.16621517e-02, bound:  3.15807343e-01\n",
      "Epoch: 13498 mean train loss:  1.16617177e-02, bound:  3.15807313e-01\n",
      "Epoch: 13499 mean train loss:  1.16610480e-02, bound:  3.15807253e-01\n",
      "Epoch: 13500 mean train loss:  1.16606141e-02, bound:  3.15807253e-01\n",
      "Epoch: 13501 mean train loss:  1.16601642e-02, bound:  3.15807223e-01\n",
      "Epoch: 13502 mean train loss:  1.16595561e-02, bound:  3.15807223e-01\n",
      "Epoch: 13503 mean train loss:  1.16589870e-02, bound:  3.15807194e-01\n",
      "Epoch: 13504 mean train loss:  1.16584729e-02, bound:  3.15807134e-01\n",
      "Epoch: 13505 mean train loss:  1.16579728e-02, bound:  3.15807134e-01\n",
      "Epoch: 13506 mean train loss:  1.16575286e-02, bound:  3.15807104e-01\n",
      "Epoch: 13507 mean train loss:  1.16569968e-02, bound:  3.15807104e-01\n",
      "Epoch: 13508 mean train loss:  1.16564455e-02, bound:  3.15807074e-01\n",
      "Epoch: 13509 mean train loss:  1.16559137e-02, bound:  3.15807015e-01\n",
      "Epoch: 13510 mean train loss:  1.16555300e-02, bound:  3.15807015e-01\n",
      "Epoch: 13511 mean train loss:  1.16548529e-02, bound:  3.15806985e-01\n",
      "Epoch: 13512 mean train loss:  1.16543835e-02, bound:  3.15806925e-01\n",
      "Epoch: 13513 mean train loss:  1.16537837e-02, bound:  3.15806925e-01\n",
      "Epoch: 13514 mean train loss:  1.16532883e-02, bound:  3.15806895e-01\n",
      "Epoch: 13515 mean train loss:  1.16527677e-02, bound:  3.15806866e-01\n",
      "Epoch: 13516 mean train loss:  1.16522964e-02, bound:  3.15806866e-01\n",
      "Epoch: 13517 mean train loss:  1.16517488e-02, bound:  3.15806806e-01\n",
      "Epoch: 13518 mean train loss:  1.16513185e-02, bound:  3.15806806e-01\n",
      "Epoch: 13519 mean train loss:  1.16507020e-02, bound:  3.15806806e-01\n",
      "Epoch: 13520 mean train loss:  1.16501888e-02, bound:  3.15806746e-01\n",
      "Epoch: 13521 mean train loss:  1.16497036e-02, bound:  3.15806746e-01\n",
      "Epoch: 13522 mean train loss:  1.16492696e-02, bound:  3.15806746e-01\n",
      "Epoch: 13523 mean train loss:  1.16486121e-02, bound:  3.15806687e-01\n",
      "Epoch: 13524 mean train loss:  1.16481176e-02, bound:  3.15806657e-01\n",
      "Epoch: 13525 mean train loss:  1.16476147e-02, bound:  3.15806627e-01\n",
      "Epoch: 13526 mean train loss:  1.16472021e-02, bound:  3.15806597e-01\n",
      "Epoch: 13527 mean train loss:  1.16465753e-02, bound:  3.15806568e-01\n",
      "Epoch: 13528 mean train loss:  1.16461022e-02, bound:  3.15806568e-01\n",
      "Epoch: 13529 mean train loss:  1.16455154e-02, bound:  3.15806538e-01\n",
      "Epoch: 13530 mean train loss:  1.16449660e-02, bound:  3.15806538e-01\n",
      "Epoch: 13531 mean train loss:  1.16444193e-02, bound:  3.15806478e-01\n",
      "Epoch: 13532 mean train loss:  1.16439238e-02, bound:  3.15806448e-01\n",
      "Epoch: 13533 mean train loss:  1.16434731e-02, bound:  3.15806419e-01\n",
      "Epoch: 13534 mean train loss:  1.16428891e-02, bound:  3.15806419e-01\n",
      "Epoch: 13535 mean train loss:  1.16423620e-02, bound:  3.15806359e-01\n",
      "Epoch: 13536 mean train loss:  1.16418079e-02, bound:  3.15806329e-01\n",
      "Epoch: 13537 mean train loss:  1.16413115e-02, bound:  3.15806329e-01\n",
      "Epoch: 13538 mean train loss:  1.16409147e-02, bound:  3.15806299e-01\n",
      "Epoch: 13539 mean train loss:  1.16403261e-02, bound:  3.15806299e-01\n",
      "Epoch: 13540 mean train loss:  1.16397655e-02, bound:  3.15806240e-01\n",
      "Epoch: 13541 mean train loss:  1.16392178e-02, bound:  3.15806240e-01\n",
      "Epoch: 13542 mean train loss:  1.16388332e-02, bound:  3.15806210e-01\n",
      "Epoch: 13543 mean train loss:  1.16381571e-02, bound:  3.15806180e-01\n",
      "Epoch: 13544 mean train loss:  1.16376756e-02, bound:  3.15806150e-01\n",
      "Epoch: 13545 mean train loss:  1.16371913e-02, bound:  3.15806121e-01\n",
      "Epoch: 13546 mean train loss:  1.16366250e-02, bound:  3.15806121e-01\n",
      "Epoch: 13547 mean train loss:  1.16361175e-02, bound:  3.15806091e-01\n",
      "Epoch: 13548 mean train loss:  1.16354767e-02, bound:  3.15806061e-01\n",
      "Epoch: 13549 mean train loss:  1.16351238e-02, bound:  3.15806001e-01\n",
      "Epoch: 13550 mean train loss:  1.16346106e-02, bound:  3.15806001e-01\n",
      "Epoch: 13551 mean train loss:  1.16340136e-02, bound:  3.15806001e-01\n",
      "Epoch: 13552 mean train loss:  1.16334762e-02, bound:  3.15805942e-01\n",
      "Epoch: 13553 mean train loss:  1.16328849e-02, bound:  3.15805912e-01\n",
      "Epoch: 13554 mean train loss:  1.16324164e-02, bound:  3.15805912e-01\n",
      "Epoch: 13555 mean train loss:  1.16318129e-02, bound:  3.15805882e-01\n",
      "Epoch: 13556 mean train loss:  1.16313212e-02, bound:  3.15805852e-01\n",
      "Epoch: 13557 mean train loss:  1.16307735e-02, bound:  3.15805823e-01\n",
      "Epoch: 13558 mean train loss:  1.16303526e-02, bound:  3.15805793e-01\n",
      "Epoch: 13559 mean train loss:  1.16297975e-02, bound:  3.15805793e-01\n",
      "Epoch: 13560 mean train loss:  1.16291922e-02, bound:  3.15805763e-01\n",
      "Epoch: 13561 mean train loss:  1.16286920e-02, bound:  3.15805733e-01\n",
      "Epoch: 13562 mean train loss:  1.16282254e-02, bound:  3.15805703e-01\n",
      "Epoch: 13563 mean train loss:  1.16276769e-02, bound:  3.15805674e-01\n",
      "Epoch: 13564 mean train loss:  1.16270846e-02, bound:  3.15805674e-01\n",
      "Epoch: 13565 mean train loss:  1.16265090e-02, bound:  3.15805644e-01\n",
      "Epoch: 13566 mean train loss:  1.16261551e-02, bound:  3.15805584e-01\n",
      "Epoch: 13567 mean train loss:  1.16255116e-02, bound:  3.15805584e-01\n",
      "Epoch: 13568 mean train loss:  1.16250552e-02, bound:  3.15805554e-01\n",
      "Epoch: 13569 mean train loss:  1.16245197e-02, bound:  3.15805525e-01\n",
      "Epoch: 13570 mean train loss:  1.16239917e-02, bound:  3.15805525e-01\n",
      "Epoch: 13571 mean train loss:  1.16234245e-02, bound:  3.15805495e-01\n",
      "Epoch: 13572 mean train loss:  1.16228890e-02, bound:  3.15805435e-01\n",
      "Epoch: 13573 mean train loss:  1.16223395e-02, bound:  3.15805435e-01\n",
      "Epoch: 13574 mean train loss:  1.16218729e-02, bound:  3.15805405e-01\n",
      "Epoch: 13575 mean train loss:  1.16213327e-02, bound:  3.15805376e-01\n",
      "Epoch: 13576 mean train loss:  1.16207516e-02, bound:  3.15805346e-01\n",
      "Epoch: 13577 mean train loss:  1.16202701e-02, bound:  3.15805316e-01\n",
      "Epoch: 13578 mean train loss:  1.16197607e-02, bound:  3.15805316e-01\n",
      "Epoch: 13579 mean train loss:  1.16191907e-02, bound:  3.15805256e-01\n",
      "Epoch: 13580 mean train loss:  1.16186729e-02, bound:  3.15805256e-01\n",
      "Epoch: 13581 mean train loss:  1.16180964e-02, bound:  3.15805227e-01\n",
      "Epoch: 13582 mean train loss:  1.16176745e-02, bound:  3.15805227e-01\n",
      "Epoch: 13583 mean train loss:  1.16170114e-02, bound:  3.15805197e-01\n",
      "Epoch: 13584 mean train loss:  1.16165634e-02, bound:  3.15805137e-01\n",
      "Epoch: 13585 mean train loss:  1.16159525e-02, bound:  3.15805107e-01\n",
      "Epoch: 13586 mean train loss:  1.16155064e-02, bound:  3.15805107e-01\n",
      "Epoch: 13587 mean train loss:  1.16149401e-02, bound:  3.15805078e-01\n",
      "Epoch: 13588 mean train loss:  1.16144279e-02, bound:  3.15805078e-01\n",
      "Epoch: 13589 mean train loss:  1.16137741e-02, bound:  3.15805018e-01\n",
      "Epoch: 13590 mean train loss:  1.16132963e-02, bound:  3.15804988e-01\n",
      "Epoch: 13591 mean train loss:  1.16127655e-02, bound:  3.15804988e-01\n",
      "Epoch: 13592 mean train loss:  1.16121825e-02, bound:  3.15804929e-01\n",
      "Epoch: 13593 mean train loss:  1.16117103e-02, bound:  3.15804929e-01\n",
      "Epoch: 13594 mean train loss:  1.16112186e-02, bound:  3.15804899e-01\n",
      "Epoch: 13595 mean train loss:  1.16106225e-02, bound:  3.15804869e-01\n",
      "Epoch: 13596 mean train loss:  1.16101764e-02, bound:  3.15804869e-01\n",
      "Epoch: 13597 mean train loss:  1.16095804e-02, bound:  3.15804809e-01\n",
      "Epoch: 13598 mean train loss:  1.16091054e-02, bound:  3.15804809e-01\n",
      "Epoch: 13599 mean train loss:  1.16085634e-02, bound:  3.15804780e-01\n",
      "Epoch: 13600 mean train loss:  1.16079152e-02, bound:  3.15804750e-01\n",
      "Epoch: 13601 mean train loss:  1.16074150e-02, bound:  3.15804690e-01\n",
      "Epoch: 13602 mean train loss:  1.16069773e-02, bound:  3.15804690e-01\n",
      "Epoch: 13603 mean train loss:  1.16063850e-02, bound:  3.15804690e-01\n",
      "Epoch: 13604 mean train loss:  1.16058253e-02, bound:  3.15804660e-01\n",
      "Epoch: 13605 mean train loss:  1.16052721e-02, bound:  3.15804571e-01\n",
      "Epoch: 13606 mean train loss:  1.16048129e-02, bound:  3.15804571e-01\n",
      "Epoch: 13607 mean train loss:  1.16041759e-02, bound:  3.15804571e-01\n",
      "Epoch: 13608 mean train loss:  1.16036534e-02, bound:  3.15804571e-01\n",
      "Epoch: 13609 mean train loss:  1.16032027e-02, bound:  3.15804541e-01\n",
      "Epoch: 13610 mean train loss:  1.16026336e-02, bound:  3.15804482e-01\n",
      "Epoch: 13611 mean train loss:  1.16020897e-02, bound:  3.15804482e-01\n",
      "Epoch: 13612 mean train loss:  1.16016166e-02, bound:  3.15804452e-01\n",
      "Epoch: 13613 mean train loss:  1.16010522e-02, bound:  3.15804422e-01\n",
      "Epoch: 13614 mean train loss:  1.16005167e-02, bound:  3.15804362e-01\n",
      "Epoch: 13615 mean train loss:  1.15999058e-02, bound:  3.15804362e-01\n",
      "Epoch: 13616 mean train loss:  1.15993768e-02, bound:  3.15804332e-01\n",
      "Epoch: 13617 mean train loss:  1.15989251e-02, bound:  3.15804332e-01\n",
      "Epoch: 13618 mean train loss:  1.15983700e-02, bound:  3.15804273e-01\n",
      "Epoch: 13619 mean train loss:  1.15977814e-02, bound:  3.15804243e-01\n",
      "Epoch: 13620 mean train loss:  1.15973195e-02, bound:  3.15804243e-01\n",
      "Epoch: 13621 mean train loss:  1.15966666e-02, bound:  3.15804213e-01\n",
      "Epoch: 13622 mean train loss:  1.15963062e-02, bound:  3.15804183e-01\n",
      "Epoch: 13623 mean train loss:  1.15956319e-02, bound:  3.15804183e-01\n",
      "Epoch: 13624 mean train loss:  1.15950936e-02, bound:  3.15804124e-01\n",
      "Epoch: 13625 mean train loss:  1.15946662e-02, bound:  3.15804094e-01\n",
      "Epoch: 13626 mean train loss:  1.15940431e-02, bound:  3.15804094e-01\n",
      "Epoch: 13627 mean train loss:  1.15935002e-02, bound:  3.15804094e-01\n",
      "Epoch: 13628 mean train loss:  1.15930354e-02, bound:  3.15804005e-01\n",
      "Epoch: 13629 mean train loss:  1.15923081e-02, bound:  3.15804005e-01\n",
      "Epoch: 13630 mean train loss:  1.15919448e-02, bound:  3.15803975e-01\n",
      "Epoch: 13631 mean train loss:  1.15912864e-02, bound:  3.15803945e-01\n",
      "Epoch: 13632 mean train loss:  1.15906810e-02, bound:  3.15803915e-01\n",
      "Epoch: 13633 mean train loss:  1.15901995e-02, bound:  3.15803885e-01\n",
      "Epoch: 13634 mean train loss:  1.15897553e-02, bound:  3.15803885e-01\n",
      "Epoch: 13635 mean train loss:  1.15891239e-02, bound:  3.15803826e-01\n",
      "Epoch: 13636 mean train loss:  1.15886591e-02, bound:  3.15803796e-01\n",
      "Epoch: 13637 mean train loss:  1.15880622e-02, bound:  3.15803796e-01\n",
      "Epoch: 13638 mean train loss:  1.15875434e-02, bound:  3.15803766e-01\n",
      "Epoch: 13639 mean train loss:  1.15869585e-02, bound:  3.15803766e-01\n",
      "Epoch: 13640 mean train loss:  1.15863550e-02, bound:  3.15803707e-01\n",
      "Epoch: 13641 mean train loss:  1.15858810e-02, bound:  3.15803677e-01\n",
      "Epoch: 13642 mean train loss:  1.15853623e-02, bound:  3.15803677e-01\n",
      "Epoch: 13643 mean train loss:  1.15848584e-02, bound:  3.15803647e-01\n",
      "Epoch: 13644 mean train loss:  1.15842456e-02, bound:  3.15803647e-01\n",
      "Epoch: 13645 mean train loss:  1.15837399e-02, bound:  3.15803587e-01\n",
      "Epoch: 13646 mean train loss:  1.15832640e-02, bound:  3.15803558e-01\n",
      "Epoch: 13647 mean train loss:  1.15825990e-02, bound:  3.15803528e-01\n",
      "Epoch: 13648 mean train loss:  1.15821445e-02, bound:  3.15803528e-01\n",
      "Epoch: 13649 mean train loss:  1.15815671e-02, bound:  3.15803468e-01\n",
      "Epoch: 13650 mean train loss:  1.15810074e-02, bound:  3.15803438e-01\n",
      "Epoch: 13651 mean train loss:  1.15804588e-02, bound:  3.15803438e-01\n",
      "Epoch: 13652 mean train loss:  1.15798935e-02, bound:  3.15803409e-01\n",
      "Epoch: 13653 mean train loss:  1.15794279e-02, bound:  3.15803379e-01\n",
      "Epoch: 13654 mean train loss:  1.15789426e-02, bound:  3.15803379e-01\n",
      "Epoch: 13655 mean train loss:  1.15783326e-02, bound:  3.15803319e-01\n",
      "Epoch: 13656 mean train loss:  1.15777375e-02, bound:  3.15803260e-01\n",
      "Epoch: 13657 mean train loss:  1.15771824e-02, bound:  3.15803260e-01\n",
      "Epoch: 13658 mean train loss:  1.15766963e-02, bound:  3.15803260e-01\n",
      "Epoch: 13659 mean train loss:  1.15760658e-02, bound:  3.15803230e-01\n",
      "Epoch: 13660 mean train loss:  1.15755117e-02, bound:  3.15803230e-01\n",
      "Epoch: 13661 mean train loss:  1.15750274e-02, bound:  3.15803140e-01\n",
      "Epoch: 13662 mean train loss:  1.15744518e-02, bound:  3.15803140e-01\n",
      "Epoch: 13663 mean train loss:  1.15738939e-02, bound:  3.15803111e-01\n",
      "Epoch: 13664 mean train loss:  1.15733556e-02, bound:  3.15803111e-01\n",
      "Epoch: 13665 mean train loss:  1.15727913e-02, bound:  3.15803051e-01\n",
      "Epoch: 13666 mean train loss:  1.15722995e-02, bound:  3.15803051e-01\n",
      "Epoch: 13667 mean train loss:  1.15717240e-02, bound:  3.15803021e-01\n",
      "Epoch: 13668 mean train loss:  1.15711335e-02, bound:  3.15802991e-01\n",
      "Epoch: 13669 mean train loss:  1.15705887e-02, bound:  3.15802991e-01\n",
      "Epoch: 13670 mean train loss:  1.15701454e-02, bound:  3.15802932e-01\n",
      "Epoch: 13671 mean train loss:  1.15694916e-02, bound:  3.15802902e-01\n",
      "Epoch: 13672 mean train loss:  1.15689179e-02, bound:  3.15802872e-01\n",
      "Epoch: 13673 mean train loss:  1.15684578e-02, bound:  3.15802872e-01\n",
      "Epoch: 13674 mean train loss:  1.15679028e-02, bound:  3.15802813e-01\n",
      "Epoch: 13675 mean train loss:  1.15673328e-02, bound:  3.15802813e-01\n",
      "Epoch: 13676 mean train loss:  1.15668224e-02, bound:  3.15802783e-01\n",
      "Epoch: 13677 mean train loss:  1.15662189e-02, bound:  3.15802753e-01\n",
      "Epoch: 13678 mean train loss:  1.15657765e-02, bound:  3.15802723e-01\n",
      "Epoch: 13679 mean train loss:  1.15651553e-02, bound:  3.15802693e-01\n",
      "Epoch: 13680 mean train loss:  1.15645779e-02, bound:  3.15802664e-01\n",
      "Epoch: 13681 mean train loss:  1.15640787e-02, bound:  3.15802664e-01\n",
      "Epoch: 13682 mean train loss:  1.15635432e-02, bound:  3.15802664e-01\n",
      "Epoch: 13683 mean train loss:  1.15629453e-02, bound:  3.15802574e-01\n",
      "Epoch: 13684 mean train loss:  1.15623791e-02, bound:  3.15802574e-01\n",
      "Epoch: 13685 mean train loss:  1.15618594e-02, bound:  3.15802544e-01\n",
      "Epoch: 13686 mean train loss:  1.15612652e-02, bound:  3.15802544e-01\n",
      "Epoch: 13687 mean train loss:  1.15607809e-02, bound:  3.15802485e-01\n",
      "Epoch: 13688 mean train loss:  1.15601895e-02, bound:  3.15802455e-01\n",
      "Epoch: 13689 mean train loss:  1.15596391e-02, bound:  3.15802455e-01\n",
      "Epoch: 13690 mean train loss:  1.15591092e-02, bound:  3.15802395e-01\n",
      "Epoch: 13691 mean train loss:  1.15585309e-02, bound:  3.15802366e-01\n",
      "Epoch: 13692 mean train loss:  1.15580074e-02, bound:  3.15802366e-01\n",
      "Epoch: 13693 mean train loss:  1.15574244e-02, bound:  3.15802366e-01\n",
      "Epoch: 13694 mean train loss:  1.15569336e-02, bound:  3.15802306e-01\n",
      "Epoch: 13695 mean train loss:  1.15563329e-02, bound:  3.15802276e-01\n",
      "Epoch: 13696 mean train loss:  1.15557201e-02, bound:  3.15802246e-01\n",
      "Epoch: 13697 mean train loss:  1.15551446e-02, bound:  3.15802246e-01\n",
      "Epoch: 13698 mean train loss:  1.15546286e-02, bound:  3.15802217e-01\n",
      "Epoch: 13699 mean train loss:  1.15540596e-02, bound:  3.15802157e-01\n",
      "Epoch: 13700 mean train loss:  1.15536069e-02, bound:  3.15802127e-01\n",
      "Epoch: 13701 mean train loss:  1.15530537e-02, bound:  3.15802127e-01\n",
      "Epoch: 13702 mean train loss:  1.15525797e-02, bound:  3.15802097e-01\n",
      "Epoch: 13703 mean train loss:  1.15517909e-02, bound:  3.15802038e-01\n",
      "Epoch: 13704 mean train loss:  1.15512908e-02, bound:  3.15802038e-01\n",
      "Epoch: 13705 mean train loss:  1.15507608e-02, bound:  3.15802008e-01\n",
      "Epoch: 13706 mean train loss:  1.15503371e-02, bound:  3.15801978e-01\n",
      "Epoch: 13707 mean train loss:  1.15496395e-02, bound:  3.15801948e-01\n",
      "Epoch: 13708 mean train loss:  1.15490789e-02, bound:  3.15801948e-01\n",
      "Epoch: 13709 mean train loss:  1.15485722e-02, bound:  3.15801919e-01\n",
      "Epoch: 13710 mean train loss:  1.15481317e-02, bound:  3.15801889e-01\n",
      "Epoch: 13711 mean train loss:  1.15474192e-02, bound:  3.15801859e-01\n",
      "Epoch: 13712 mean train loss:  1.15468903e-02, bound:  3.15801829e-01\n",
      "Epoch: 13713 mean train loss:  1.15463249e-02, bound:  3.15801799e-01\n",
      "Epoch: 13714 mean train loss:  1.15457028e-02, bound:  3.15801799e-01\n",
      "Epoch: 13715 mean train loss:  1.15452316e-02, bound:  3.15801710e-01\n",
      "Epoch: 13716 mean train loss:  1.15446728e-02, bound:  3.15801710e-01\n",
      "Epoch: 13717 mean train loss:  1.15441121e-02, bound:  3.15801710e-01\n",
      "Epoch: 13718 mean train loss:  1.15435869e-02, bound:  3.15801680e-01\n",
      "Epoch: 13719 mean train loss:  1.15430504e-02, bound:  3.15801650e-01\n",
      "Epoch: 13720 mean train loss:  1.15424255e-02, bound:  3.15801650e-01\n",
      "Epoch: 13721 mean train loss:  1.15419179e-02, bound:  3.15801591e-01\n",
      "Epoch: 13722 mean train loss:  1.15413507e-02, bound:  3.15801561e-01\n",
      "Epoch: 13723 mean train loss:  1.15408786e-02, bound:  3.15801561e-01\n",
      "Epoch: 13724 mean train loss:  1.15402490e-02, bound:  3.15801531e-01\n",
      "Epoch: 13725 mean train loss:  1.15396455e-02, bound:  3.15801471e-01\n",
      "Epoch: 13726 mean train loss:  1.15391016e-02, bound:  3.15801442e-01\n",
      "Epoch: 13727 mean train loss:  1.15384366e-02, bound:  3.15801442e-01\n",
      "Epoch: 13728 mean train loss:  1.15379896e-02, bound:  3.15801382e-01\n",
      "Epoch: 13729 mean train loss:  1.15374764e-02, bound:  3.15801382e-01\n",
      "Epoch: 13730 mean train loss:  1.15368301e-02, bound:  3.15801352e-01\n",
      "Epoch: 13731 mean train loss:  1.15363048e-02, bound:  3.15801322e-01\n",
      "Epoch: 13732 mean train loss:  1.15357125e-02, bound:  3.15801263e-01\n",
      "Epoch: 13733 mean train loss:  1.15352059e-02, bound:  3.15801263e-01\n",
      "Epoch: 13734 mean train loss:  1.15346853e-02, bound:  3.15801233e-01\n",
      "Epoch: 13735 mean train loss:  1.15341442e-02, bound:  3.15801233e-01\n",
      "Epoch: 13736 mean train loss:  1.15335127e-02, bound:  3.15801203e-01\n",
      "Epoch: 13737 mean train loss:  1.15329977e-02, bound:  3.15801144e-01\n",
      "Epoch: 13738 mean train loss:  1.15324538e-02, bound:  3.15801144e-01\n",
      "Epoch: 13739 mean train loss:  1.15317693e-02, bound:  3.15801114e-01\n",
      "Epoch: 13740 mean train loss:  1.15312636e-02, bound:  3.15801054e-01\n",
      "Epoch: 13741 mean train loss:  1.15306769e-02, bound:  3.15801024e-01\n",
      "Epoch: 13742 mean train loss:  1.15302606e-02, bound:  3.15801024e-01\n",
      "Epoch: 13743 mean train loss:  1.15295816e-02, bound:  3.15800995e-01\n",
      "Epoch: 13744 mean train loss:  1.15290610e-02, bound:  3.15800995e-01\n",
      "Epoch: 13745 mean train loss:  1.15285451e-02, bound:  3.15800935e-01\n",
      "Epoch: 13746 mean train loss:  1.15279565e-02, bound:  3.15800935e-01\n",
      "Epoch: 13747 mean train loss:  1.15273315e-02, bound:  3.15800905e-01\n",
      "Epoch: 13748 mean train loss:  1.15267336e-02, bound:  3.15800875e-01\n",
      "Epoch: 13749 mean train loss:  1.15262251e-02, bound:  3.15800816e-01\n",
      "Epoch: 13750 mean train loss:  1.15256375e-02, bound:  3.15800816e-01\n",
      "Epoch: 13751 mean train loss:  1.15251224e-02, bound:  3.15800786e-01\n",
      "Epoch: 13752 mean train loss:  1.15245953e-02, bound:  3.15800726e-01\n",
      "Epoch: 13753 mean train loss:  1.15239592e-02, bound:  3.15800697e-01\n",
      "Epoch: 13754 mean train loss:  1.15233753e-02, bound:  3.15800697e-01\n",
      "Epoch: 13755 mean train loss:  1.15228640e-02, bound:  3.15800697e-01\n",
      "Epoch: 13756 mean train loss:  1.15222866e-02, bound:  3.15800667e-01\n",
      "Epoch: 13757 mean train loss:  1.15217138e-02, bound:  3.15800607e-01\n",
      "Epoch: 13758 mean train loss:  1.15212044e-02, bound:  3.15800577e-01\n",
      "Epoch: 13759 mean train loss:  1.15205655e-02, bound:  3.15800577e-01\n",
      "Epoch: 13760 mean train loss:  1.15200281e-02, bound:  3.15800548e-01\n",
      "Epoch: 13761 mean train loss:  1.15194712e-02, bound:  3.15800548e-01\n",
      "Epoch: 13762 mean train loss:  1.15189673e-02, bound:  3.15800458e-01\n",
      "Epoch: 13763 mean train loss:  1.15182940e-02, bound:  3.15800458e-01\n",
      "Epoch: 13764 mean train loss:  1.15177985e-02, bound:  3.15800428e-01\n",
      "Epoch: 13765 mean train loss:  1.15171773e-02, bound:  3.15800399e-01\n",
      "Epoch: 13766 mean train loss:  1.15167154e-02, bound:  3.15800369e-01\n",
      "Epoch: 13767 mean train loss:  1.15161110e-02, bound:  3.15800369e-01\n",
      "Epoch: 13768 mean train loss:  1.15154507e-02, bound:  3.15800309e-01\n",
      "Epoch: 13769 mean train loss:  1.15149263e-02, bound:  3.15800309e-01\n",
      "Epoch: 13770 mean train loss:  1.15144541e-02, bound:  3.15800250e-01\n",
      "Epoch: 13771 mean train loss:  1.15137557e-02, bound:  3.15800250e-01\n",
      "Epoch: 13772 mean train loss:  1.15131913e-02, bound:  3.15800220e-01\n",
      "Epoch: 13773 mean train loss:  1.15126707e-02, bound:  3.15800220e-01\n",
      "Epoch: 13774 mean train loss:  1.15121594e-02, bound:  3.15800160e-01\n",
      "Epoch: 13775 mean train loss:  1.15115093e-02, bound:  3.15800130e-01\n",
      "Epoch: 13776 mean train loss:  1.15108630e-02, bound:  3.15800130e-01\n",
      "Epoch: 13777 mean train loss:  1.15103936e-02, bound:  3.15800101e-01\n",
      "Epoch: 13778 mean train loss:  1.15098413e-02, bound:  3.15800041e-01\n",
      "Epoch: 13779 mean train loss:  1.15093132e-02, bound:  3.15800011e-01\n",
      "Epoch: 13780 mean train loss:  1.15087079e-02, bound:  3.15800011e-01\n",
      "Epoch: 13781 mean train loss:  1.15082096e-02, bound:  3.15799981e-01\n",
      "Epoch: 13782 mean train loss:  1.15076648e-02, bound:  3.15799952e-01\n",
      "Epoch: 13783 mean train loss:  1.15069589e-02, bound:  3.15799892e-01\n",
      "Epoch: 13784 mean train loss:  1.15065146e-02, bound:  3.15799892e-01\n",
      "Epoch: 13785 mean train loss:  1.15058515e-02, bound:  3.15799862e-01\n",
      "Epoch: 13786 mean train loss:  1.15052704e-02, bound:  3.15799832e-01\n",
      "Epoch: 13787 mean train loss:  1.15047013e-02, bound:  3.15799803e-01\n",
      "Epoch: 13788 mean train loss:  1.15041109e-02, bound:  3.15799803e-01\n",
      "Epoch: 13789 mean train loss:  1.15035716e-02, bound:  3.15799773e-01\n",
      "Epoch: 13790 mean train loss:  1.15030892e-02, bound:  3.15799713e-01\n",
      "Epoch: 13791 mean train loss:  1.15024587e-02, bound:  3.15799713e-01\n",
      "Epoch: 13792 mean train loss:  1.15019772e-02, bound:  3.15799683e-01\n",
      "Epoch: 13793 mean train loss:  1.15012527e-02, bound:  3.15799654e-01\n",
      "Epoch: 13794 mean train loss:  1.15008000e-02, bound:  3.15799594e-01\n",
      "Epoch: 13795 mean train loss:  1.15001583e-02, bound:  3.15799594e-01\n",
      "Epoch: 13796 mean train loss:  1.14995977e-02, bound:  3.15799564e-01\n",
      "Epoch: 13797 mean train loss:  1.14990305e-02, bound:  3.15799534e-01\n",
      "Epoch: 13798 mean train loss:  1.14985006e-02, bound:  3.15799505e-01\n",
      "Epoch: 13799 mean train loss:  1.14978338e-02, bound:  3.15799505e-01\n",
      "Epoch: 13800 mean train loss:  1.14972871e-02, bound:  3.15799445e-01\n",
      "Epoch: 13801 mean train loss:  1.14968056e-02, bound:  3.15799445e-01\n",
      "Epoch: 13802 mean train loss:  1.14961099e-02, bound:  3.15799385e-01\n",
      "Epoch: 13803 mean train loss:  1.14956116e-02, bound:  3.15799385e-01\n",
      "Epoch: 13804 mean train loss:  1.14950407e-02, bound:  3.15799356e-01\n",
      "Epoch: 13805 mean train loss:  1.14944307e-02, bound:  3.15799326e-01\n",
      "Epoch: 13806 mean train loss:  1.14937685e-02, bound:  3.15799266e-01\n",
      "Epoch: 13807 mean train loss:  1.14932777e-02, bound:  3.15799266e-01\n",
      "Epoch: 13808 mean train loss:  1.14927078e-02, bound:  3.15799236e-01\n",
      "Epoch: 13809 mean train loss:  1.14922496e-02, bound:  3.15799206e-01\n",
      "Epoch: 13810 mean train loss:  1.14915818e-02, bound:  3.15799147e-01\n",
      "Epoch: 13811 mean train loss:  1.14909858e-02, bound:  3.15799147e-01\n",
      "Epoch: 13812 mean train loss:  1.14904298e-02, bound:  3.15799117e-01\n",
      "Epoch: 13813 mean train loss:  1.14898216e-02, bound:  3.15799117e-01\n",
      "Epoch: 13814 mean train loss:  1.14893243e-02, bound:  3.15799117e-01\n",
      "Epoch: 13815 mean train loss:  1.14886621e-02, bound:  3.15799028e-01\n",
      "Epoch: 13816 mean train loss:  1.14880800e-02, bound:  3.15798998e-01\n",
      "Epoch: 13817 mean train loss:  1.14875091e-02, bound:  3.15798998e-01\n",
      "Epoch: 13818 mean train loss:  1.14869848e-02, bound:  3.15798998e-01\n",
      "Epoch: 13819 mean train loss:  1.14863254e-02, bound:  3.15798938e-01\n",
      "Epoch: 13820 mean train loss:  1.14858206e-02, bound:  3.15798879e-01\n",
      "Epoch: 13821 mean train loss:  1.14851529e-02, bound:  3.15798879e-01\n",
      "Epoch: 13822 mean train loss:  1.14847012e-02, bound:  3.15798879e-01\n",
      "Epoch: 13823 mean train loss:  1.14840884e-02, bound:  3.15798819e-01\n",
      "Epoch: 13824 mean train loss:  1.14834942e-02, bound:  3.15798789e-01\n",
      "Epoch: 13825 mean train loss:  1.14829419e-02, bound:  3.15798759e-01\n",
      "Epoch: 13826 mean train loss:  1.14822946e-02, bound:  3.15798730e-01\n",
      "Epoch: 13827 mean train loss:  1.14817861e-02, bound:  3.15798700e-01\n",
      "Epoch: 13828 mean train loss:  1.14811547e-02, bound:  3.15798700e-01\n",
      "Epoch: 13829 mean train loss:  1.14806509e-02, bound:  3.15798670e-01\n",
      "Epoch: 13830 mean train loss:  1.14799077e-02, bound:  3.15798610e-01\n",
      "Epoch: 13831 mean train loss:  1.14795091e-02, bound:  3.15798581e-01\n",
      "Epoch: 13832 mean train loss:  1.14787612e-02, bound:  3.15798581e-01\n",
      "Epoch: 13833 mean train loss:  1.14783254e-02, bound:  3.15798551e-01\n",
      "Epoch: 13834 mean train loss:  1.14777498e-02, bound:  3.15798551e-01\n",
      "Epoch: 13835 mean train loss:  1.14771230e-02, bound:  3.15798461e-01\n",
      "Epoch: 13836 mean train loss:  1.14765735e-02, bound:  3.15798461e-01\n",
      "Epoch: 13837 mean train loss:  1.14759756e-02, bound:  3.15798432e-01\n",
      "Epoch: 13838 mean train loss:  1.14753833e-02, bound:  3.15798402e-01\n",
      "Epoch: 13839 mean train loss:  1.14747798e-02, bound:  3.15798372e-01\n",
      "Epoch: 13840 mean train loss:  1.14742974e-02, bound:  3.15798342e-01\n",
      "Epoch: 13841 mean train loss:  1.14736017e-02, bound:  3.15798312e-01\n",
      "Epoch: 13842 mean train loss:  1.14730140e-02, bound:  3.15798283e-01\n",
      "Epoch: 13843 mean train loss:  1.14724999e-02, bound:  3.15798283e-01\n",
      "Epoch: 13844 mean train loss:  1.14719057e-02, bound:  3.15798253e-01\n",
      "Epoch: 13845 mean train loss:  1.14712454e-02, bound:  3.15798193e-01\n",
      "Epoch: 13846 mean train loss:  1.14707015e-02, bound:  3.15798163e-01\n",
      "Epoch: 13847 mean train loss:  1.14702312e-02, bound:  3.15798134e-01\n",
      "Epoch: 13848 mean train loss:  1.14695458e-02, bound:  3.15798134e-01\n",
      "Epoch: 13849 mean train loss:  1.14690149e-02, bound:  3.15798074e-01\n",
      "Epoch: 13850 mean train loss:  1.14683835e-02, bound:  3.15798074e-01\n",
      "Epoch: 13851 mean train loss:  1.14678992e-02, bound:  3.15798044e-01\n",
      "Epoch: 13852 mean train loss:  1.14671746e-02, bound:  3.15798014e-01\n",
      "Epoch: 13853 mean train loss:  1.14666773e-02, bound:  3.15797985e-01\n",
      "Epoch: 13854 mean train loss:  1.14660189e-02, bound:  3.15797955e-01\n",
      "Epoch: 13855 mean train loss:  1.14654424e-02, bound:  3.15797925e-01\n",
      "Epoch: 13856 mean train loss:  1.14648612e-02, bound:  3.15797895e-01\n",
      "Epoch: 13857 mean train loss:  1.14643667e-02, bound:  3.15797895e-01\n",
      "Epoch: 13858 mean train loss:  1.14637092e-02, bound:  3.15797865e-01\n",
      "Epoch: 13859 mean train loss:  1.14632361e-02, bound:  3.15797806e-01\n",
      "Epoch: 13860 mean train loss:  1.14625683e-02, bound:  3.15797776e-01\n",
      "Epoch: 13861 mean train loss:  1.14619387e-02, bound:  3.15797776e-01\n",
      "Epoch: 13862 mean train loss:  1.14614405e-02, bound:  3.15797716e-01\n",
      "Epoch: 13863 mean train loss:  1.14608463e-02, bound:  3.15797687e-01\n",
      "Epoch: 13864 mean train loss:  1.14602344e-02, bound:  3.15797687e-01\n",
      "Epoch: 13865 mean train loss:  1.14596989e-02, bound:  3.15797687e-01\n",
      "Epoch: 13866 mean train loss:  1.14590731e-02, bound:  3.15797597e-01\n",
      "Epoch: 13867 mean train loss:  1.14585189e-02, bound:  3.15797567e-01\n",
      "Epoch: 13868 mean train loss:  1.14578567e-02, bound:  3.15797567e-01\n",
      "Epoch: 13869 mean train loss:  1.14572812e-02, bound:  3.15797567e-01\n",
      "Epoch: 13870 mean train loss:  1.14566721e-02, bound:  3.15797508e-01\n",
      "Epoch: 13871 mean train loss:  1.14561375e-02, bound:  3.15797478e-01\n",
      "Epoch: 13872 mean train loss:  1.14555592e-02, bound:  3.15797448e-01\n",
      "Epoch: 13873 mean train loss:  1.14549482e-02, bound:  3.15797448e-01\n",
      "Epoch: 13874 mean train loss:  1.14543671e-02, bound:  3.15797389e-01\n",
      "Epoch: 13875 mean train loss:  1.14538111e-02, bound:  3.15797359e-01\n",
      "Epoch: 13876 mean train loss:  1.14532113e-02, bound:  3.15797359e-01\n",
      "Epoch: 13877 mean train loss:  1.14526525e-02, bound:  3.15797329e-01\n",
      "Epoch: 13878 mean train loss:  1.14520220e-02, bound:  3.15797269e-01\n",
      "Epoch: 13879 mean train loss:  1.14514688e-02, bound:  3.15797240e-01\n",
      "Epoch: 13880 mean train loss:  1.14509128e-02, bound:  3.15797240e-01\n",
      "Epoch: 13881 mean train loss:  1.14502739e-02, bound:  3.15797210e-01\n",
      "Epoch: 13882 mean train loss:  1.14496639e-02, bound:  3.15797150e-01\n",
      "Epoch: 13883 mean train loss:  1.14490250e-02, bound:  3.15797150e-01\n",
      "Epoch: 13884 mean train loss:  1.14485994e-02, bound:  3.15797120e-01\n",
      "Epoch: 13885 mean train loss:  1.14479074e-02, bound:  3.15797061e-01\n",
      "Epoch: 13886 mean train loss:  1.14473198e-02, bound:  3.15797031e-01\n",
      "Epoch: 13887 mean train loss:  1.14466706e-02, bound:  3.15797031e-01\n",
      "Epoch: 13888 mean train loss:  1.14461789e-02, bound:  3.15797001e-01\n",
      "Epoch: 13889 mean train loss:  1.14455624e-02, bound:  3.15796942e-01\n",
      "Epoch: 13890 mean train loss:  1.14449076e-02, bound:  3.15796942e-01\n",
      "Epoch: 13891 mean train loss:  1.14443712e-02, bound:  3.15796912e-01\n",
      "Epoch: 13892 mean train loss:  1.14438403e-02, bound:  3.15796882e-01\n",
      "Epoch: 13893 mean train loss:  1.14431847e-02, bound:  3.15796882e-01\n",
      "Epoch: 13894 mean train loss:  1.14425374e-02, bound:  3.15796822e-01\n",
      "Epoch: 13895 mean train loss:  1.14419973e-02, bound:  3.15796793e-01\n",
      "Epoch: 13896 mean train loss:  1.14414440e-02, bound:  3.15796763e-01\n",
      "Epoch: 13897 mean train loss:  1.14407875e-02, bound:  3.15796733e-01\n",
      "Epoch: 13898 mean train loss:  1.14402249e-02, bound:  3.15796703e-01\n",
      "Epoch: 13899 mean train loss:  1.14396121e-02, bound:  3.15796703e-01\n",
      "Epoch: 13900 mean train loss:  1.14390804e-02, bound:  3.15796673e-01\n",
      "Epoch: 13901 mean train loss:  1.14384163e-02, bound:  3.15796673e-01\n",
      "Epoch: 13902 mean train loss:  1.14378482e-02, bound:  3.15796584e-01\n",
      "Epoch: 13903 mean train loss:  1.14372326e-02, bound:  3.15796554e-01\n",
      "Epoch: 13904 mean train loss:  1.14366952e-02, bound:  3.15796554e-01\n",
      "Epoch: 13905 mean train loss:  1.14361010e-02, bound:  3.15796524e-01\n",
      "Epoch: 13906 mean train loss:  1.14354054e-02, bound:  3.15796465e-01\n",
      "Epoch: 13907 mean train loss:  1.14348307e-02, bound:  3.15796465e-01\n",
      "Epoch: 13908 mean train loss:  1.14342179e-02, bound:  3.15796435e-01\n",
      "Epoch: 13909 mean train loss:  1.14336787e-02, bound:  3.15796375e-01\n",
      "Epoch: 13910 mean train loss:  1.14330845e-02, bound:  3.15796375e-01\n",
      "Epoch: 13911 mean train loss:  1.14325080e-02, bound:  3.15796345e-01\n",
      "Epoch: 13912 mean train loss:  1.14319660e-02, bound:  3.15796345e-01\n",
      "Epoch: 13913 mean train loss:  1.14314118e-02, bound:  3.15796286e-01\n",
      "Epoch: 13914 mean train loss:  1.14307534e-02, bound:  3.15796256e-01\n",
      "Epoch: 13915 mean train loss:  1.14300894e-02, bound:  3.15796256e-01\n",
      "Epoch: 13916 mean train loss:  1.14295809e-02, bound:  3.15796167e-01\n",
      "Epoch: 13917 mean train loss:  1.14289643e-02, bound:  3.15796167e-01\n",
      "Epoch: 13918 mean train loss:  1.14283254e-02, bound:  3.15796137e-01\n",
      "Epoch: 13919 mean train loss:  1.14277555e-02, bound:  3.15796137e-01\n",
      "Epoch: 13920 mean train loss:  1.14271957e-02, bound:  3.15796077e-01\n",
      "Epoch: 13921 mean train loss:  1.14265913e-02, bound:  3.15796047e-01\n",
      "Epoch: 13922 mean train loss:  1.14259031e-02, bound:  3.15796018e-01\n",
      "Epoch: 13923 mean train loss:  1.14252921e-02, bound:  3.15796018e-01\n",
      "Epoch: 13924 mean train loss:  1.14247622e-02, bound:  3.15795958e-01\n",
      "Epoch: 13925 mean train loss:  1.14241922e-02, bound:  3.15795928e-01\n",
      "Epoch: 13926 mean train loss:  1.14236185e-02, bound:  3.15795898e-01\n",
      "Epoch: 13927 mean train loss:  1.14230337e-02, bound:  3.15795839e-01\n",
      "Epoch: 13928 mean train loss:  1.14223734e-02, bound:  3.15795839e-01\n",
      "Epoch: 13929 mean train loss:  1.14218211e-02, bound:  3.15795839e-01\n",
      "Epoch: 13930 mean train loss:  1.14211431e-02, bound:  3.15795809e-01\n",
      "Epoch: 13931 mean train loss:  1.14205824e-02, bound:  3.15795720e-01\n",
      "Epoch: 13932 mean train loss:  1.14200218e-02, bound:  3.15795720e-01\n",
      "Epoch: 13933 mean train loss:  1.14193801e-02, bound:  3.15795720e-01\n",
      "Epoch: 13934 mean train loss:  1.14188604e-02, bound:  3.15795690e-01\n",
      "Epoch: 13935 mean train loss:  1.14182048e-02, bound:  3.15795600e-01\n",
      "Epoch: 13936 mean train loss:  1.14175798e-02, bound:  3.15795600e-01\n",
      "Epoch: 13937 mean train loss:  1.14170145e-02, bound:  3.15795600e-01\n",
      "Epoch: 13938 mean train loss:  1.14164157e-02, bound:  3.15795571e-01\n",
      "Epoch: 13939 mean train loss:  1.14158038e-02, bound:  3.15795511e-01\n",
      "Epoch: 13940 mean train loss:  1.14152031e-02, bound:  3.15795511e-01\n",
      "Epoch: 13941 mean train loss:  1.14146248e-02, bound:  3.15795481e-01\n",
      "Epoch: 13942 mean train loss:  1.14141041e-02, bound:  3.15795451e-01\n",
      "Epoch: 13943 mean train loss:  1.14133479e-02, bound:  3.15795451e-01\n",
      "Epoch: 13944 mean train loss:  1.14128022e-02, bound:  3.15795392e-01\n",
      "Epoch: 13945 mean train loss:  1.14122247e-02, bound:  3.15795362e-01\n",
      "Epoch: 13946 mean train loss:  1.14115933e-02, bound:  3.15795332e-01\n",
      "Epoch: 13947 mean train loss:  1.14110028e-02, bound:  3.15795273e-01\n",
      "Epoch: 13948 mean train loss:  1.14103993e-02, bound:  3.15795273e-01\n",
      "Epoch: 13949 mean train loss:  1.14098294e-02, bound:  3.15795243e-01\n",
      "Epoch: 13950 mean train loss:  1.14092967e-02, bound:  3.15795243e-01\n",
      "Epoch: 13951 mean train loss:  1.14086606e-02, bound:  3.15795153e-01\n",
      "Epoch: 13952 mean train loss:  1.14080561e-02, bound:  3.15795153e-01\n",
      "Epoch: 13953 mean train loss:  1.14075029e-02, bound:  3.15795124e-01\n",
      "Epoch: 13954 mean train loss:  1.14066917e-02, bound:  3.15795124e-01\n",
      "Epoch: 13955 mean train loss:  1.14062112e-02, bound:  3.15795064e-01\n",
      "Epoch: 13956 mean train loss:  1.14055742e-02, bound:  3.15795034e-01\n",
      "Epoch: 13957 mean train loss:  1.14049278e-02, bound:  3.15795034e-01\n",
      "Epoch: 13958 mean train loss:  1.14043728e-02, bound:  3.15795004e-01\n",
      "Epoch: 13959 mean train loss:  1.14038819e-02, bound:  3.15794945e-01\n",
      "Epoch: 13960 mean train loss:  1.14032440e-02, bound:  3.15794945e-01\n",
      "Epoch: 13961 mean train loss:  1.14026377e-02, bound:  3.15794915e-01\n",
      "Epoch: 13962 mean train loss:  1.14019122e-02, bound:  3.15794855e-01\n",
      "Epoch: 13963 mean train loss:  1.14014149e-02, bound:  3.15794826e-01\n",
      "Epoch: 13964 mean train loss:  1.14007853e-02, bound:  3.15794826e-01\n",
      "Epoch: 13965 mean train loss:  1.14002079e-02, bound:  3.15794796e-01\n",
      "Epoch: 13966 mean train loss:  1.13995094e-02, bound:  3.15794736e-01\n",
      "Epoch: 13967 mean train loss:  1.13989320e-02, bound:  3.15794706e-01\n",
      "Epoch: 13968 mean train loss:  1.13983797e-02, bound:  3.15794706e-01\n",
      "Epoch: 13969 mean train loss:  1.13977827e-02, bound:  3.15794677e-01\n",
      "Epoch: 13970 mean train loss:  1.13971485e-02, bound:  3.15794617e-01\n",
      "Epoch: 13971 mean train loss:  1.13965552e-02, bound:  3.15794587e-01\n",
      "Epoch: 13972 mean train loss:  1.13959536e-02, bound:  3.15794587e-01\n",
      "Epoch: 13973 mean train loss:  1.13953380e-02, bound:  3.15794528e-01\n",
      "Epoch: 13974 mean train loss:  1.13947326e-02, bound:  3.15794498e-01\n",
      "Epoch: 13975 mean train loss:  1.13941887e-02, bound:  3.15794468e-01\n",
      "Epoch: 13976 mean train loss:  1.13935200e-02, bound:  3.15794468e-01\n",
      "Epoch: 13977 mean train loss:  1.13929268e-02, bound:  3.15794408e-01\n",
      "Epoch: 13978 mean train loss:  1.13922190e-02, bound:  3.15794408e-01\n",
      "Epoch: 13979 mean train loss:  1.13916369e-02, bound:  3.15794379e-01\n",
      "Epoch: 13980 mean train loss:  1.13911014e-02, bound:  3.15794349e-01\n",
      "Epoch: 13981 mean train loss:  1.13904374e-02, bound:  3.15794319e-01\n",
      "Epoch: 13982 mean train loss:  1.13898506e-02, bound:  3.15794289e-01\n",
      "Epoch: 13983 mean train loss:  1.13892956e-02, bound:  3.15794259e-01\n",
      "Epoch: 13984 mean train loss:  1.13887032e-02, bound:  3.15794230e-01\n",
      "Epoch: 13985 mean train loss:  1.13879750e-02, bound:  3.15794170e-01\n",
      "Epoch: 13986 mean train loss:  1.13873584e-02, bound:  3.15794140e-01\n",
      "Epoch: 13987 mean train loss:  1.13868257e-02, bound:  3.15794140e-01\n",
      "Epoch: 13988 mean train loss:  1.13862604e-02, bound:  3.15794110e-01\n",
      "Epoch: 13989 mean train loss:  1.13855787e-02, bound:  3.15794080e-01\n",
      "Epoch: 13990 mean train loss:  1.13849742e-02, bound:  3.15794021e-01\n",
      "Epoch: 13991 mean train loss:  1.13843605e-02, bound:  3.15794021e-01\n",
      "Epoch: 13992 mean train loss:  1.13837328e-02, bound:  3.15793991e-01\n",
      "Epoch: 13993 mean train loss:  1.13831405e-02, bound:  3.15793961e-01\n",
      "Epoch: 13994 mean train loss:  1.13825593e-02, bound:  3.15793931e-01\n",
      "Epoch: 13995 mean train loss:  1.13819567e-02, bound:  3.15793902e-01\n",
      "Epoch: 13996 mean train loss:  1.13812946e-02, bound:  3.15793842e-01\n",
      "Epoch: 13997 mean train loss:  1.13806380e-02, bound:  3.15793842e-01\n",
      "Epoch: 13998 mean train loss:  1.13800438e-02, bound:  3.15793812e-01\n",
      "Epoch: 13999 mean train loss:  1.13794943e-02, bound:  3.15793812e-01\n",
      "Epoch: 14000 mean train loss:  1.13788461e-02, bound:  3.15793723e-01\n",
      "Epoch: 14001 mean train loss:  1.13782771e-02, bound:  3.15793723e-01\n",
      "Epoch: 14002 mean train loss:  1.13777313e-02, bound:  3.15793693e-01\n",
      "Epoch: 14003 mean train loss:  1.13770673e-02, bound:  3.15793663e-01\n",
      "Epoch: 14004 mean train loss:  1.13763148e-02, bound:  3.15793604e-01\n",
      "Epoch: 14005 mean train loss:  1.13759004e-02, bound:  3.15793604e-01\n",
      "Epoch: 14006 mean train loss:  1.13752810e-02, bound:  3.15793574e-01\n",
      "Epoch: 14007 mean train loss:  1.13747045e-02, bound:  3.15793574e-01\n",
      "Epoch: 14008 mean train loss:  1.13740396e-02, bound:  3.15793514e-01\n",
      "Epoch: 14009 mean train loss:  1.13733243e-02, bound:  3.15793514e-01\n",
      "Epoch: 14010 mean train loss:  1.13727320e-02, bound:  3.15793455e-01\n",
      "Epoch: 14011 mean train loss:  1.13722160e-02, bound:  3.15793455e-01\n",
      "Epoch: 14012 mean train loss:  1.13715967e-02, bound:  3.15793395e-01\n",
      "Epoch: 14013 mean train loss:  1.13709196e-02, bound:  3.15793365e-01\n",
      "Epoch: 14014 mean train loss:  1.13703534e-02, bound:  3.15793335e-01\n",
      "Epoch: 14015 mean train loss:  1.13696726e-02, bound:  3.15793276e-01\n",
      "Epoch: 14016 mean train loss:  1.13690123e-02, bound:  3.15793276e-01\n",
      "Epoch: 14017 mean train loss:  1.13684079e-02, bound:  3.15793246e-01\n",
      "Epoch: 14018 mean train loss:  1.13678165e-02, bound:  3.15793246e-01\n",
      "Epoch: 14019 mean train loss:  1.13672707e-02, bound:  3.15793186e-01\n",
      "Epoch: 14020 mean train loss:  1.13666505e-02, bound:  3.15793157e-01\n",
      "Epoch: 14021 mean train loss:  1.13660237e-02, bound:  3.15793127e-01\n",
      "Epoch: 14022 mean train loss:  1.13653326e-02, bound:  3.15793127e-01\n",
      "Epoch: 14023 mean train loss:  1.13647813e-02, bound:  3.15793067e-01\n",
      "Epoch: 14024 mean train loss:  1.13640912e-02, bound:  3.15793037e-01\n",
      "Epoch: 14025 mean train loss:  1.13635054e-02, bound:  3.15793008e-01\n",
      "Epoch: 14026 mean train loss:  1.13630230e-02, bound:  3.15793008e-01\n",
      "Epoch: 14027 mean train loss:  1.13622937e-02, bound:  3.15792948e-01\n",
      "Epoch: 14028 mean train loss:  1.13616539e-02, bound:  3.15792918e-01\n",
      "Epoch: 14029 mean train loss:  1.13610225e-02, bound:  3.15792888e-01\n",
      "Epoch: 14030 mean train loss:  1.13605363e-02, bound:  3.15792859e-01\n",
      "Epoch: 14031 mean train loss:  1.13599235e-02, bound:  3.15792829e-01\n",
      "Epoch: 14032 mean train loss:  1.13591887e-02, bound:  3.15792799e-01\n",
      "Epoch: 14033 mean train loss:  1.13586271e-02, bound:  3.15792769e-01\n",
      "Epoch: 14034 mean train loss:  1.13580301e-02, bound:  3.15792710e-01\n",
      "Epoch: 14035 mean train loss:  1.13573773e-02, bound:  3.15792710e-01\n",
      "Epoch: 14036 mean train loss:  1.13567347e-02, bound:  3.15792680e-01\n",
      "Epoch: 14037 mean train loss:  1.13560064e-02, bound:  3.15792680e-01\n",
      "Epoch: 14038 mean train loss:  1.13555659e-02, bound:  3.15792620e-01\n",
      "Epoch: 14039 mean train loss:  1.13548385e-02, bound:  3.15792590e-01\n",
      "Epoch: 14040 mean train loss:  1.13542276e-02, bound:  3.15792590e-01\n",
      "Epoch: 14041 mean train loss:  1.13536520e-02, bound:  3.15792531e-01\n",
      "Epoch: 14042 mean train loss:  1.13530383e-02, bound:  3.15792471e-01\n",
      "Epoch: 14043 mean train loss:  1.13523779e-02, bound:  3.15792471e-01\n",
      "Epoch: 14044 mean train loss:  1.13517828e-02, bound:  3.15792471e-01\n",
      "Epoch: 14045 mean train loss:  1.13511709e-02, bound:  3.15792412e-01\n",
      "Epoch: 14046 mean train loss:  1.13505293e-02, bound:  3.15792382e-01\n",
      "Epoch: 14047 mean train loss:  1.13499491e-02, bound:  3.15792352e-01\n",
      "Epoch: 14048 mean train loss:  1.13492412e-02, bound:  3.15792352e-01\n",
      "Epoch: 14049 mean train loss:  1.13487141e-02, bound:  3.15792292e-01\n",
      "Epoch: 14050 mean train loss:  1.13480370e-02, bound:  3.15792263e-01\n",
      "Epoch: 14051 mean train loss:  1.13474438e-02, bound:  3.15792263e-01\n",
      "Epoch: 14052 mean train loss:  1.13468356e-02, bound:  3.15792233e-01\n",
      "Epoch: 14053 mean train loss:  1.13461828e-02, bound:  3.15792173e-01\n",
      "Epoch: 14054 mean train loss:  1.13455337e-02, bound:  3.15792143e-01\n",
      "Epoch: 14055 mean train loss:  1.13449730e-02, bound:  3.15792114e-01\n",
      "Epoch: 14056 mean train loss:  1.13443742e-02, bound:  3.15792084e-01\n",
      "Epoch: 14057 mean train loss:  1.13437036e-02, bound:  3.15792084e-01\n",
      "Epoch: 14058 mean train loss:  1.13431513e-02, bound:  3.15792024e-01\n",
      "Epoch: 14059 mean train loss:  1.13424258e-02, bound:  3.15791994e-01\n",
      "Epoch: 14060 mean train loss:  1.13418102e-02, bound:  3.15791965e-01\n",
      "Epoch: 14061 mean train loss:  1.13412542e-02, bound:  3.15791965e-01\n",
      "Epoch: 14062 mean train loss:  1.13405287e-02, bound:  3.15791905e-01\n",
      "Epoch: 14063 mean train loss:  1.13399457e-02, bound:  3.15791845e-01\n",
      "Epoch: 14064 mean train loss:  1.13394130e-02, bound:  3.15791845e-01\n",
      "Epoch: 14065 mean train loss:  1.13387303e-02, bound:  3.15791816e-01\n",
      "Epoch: 14066 mean train loss:  1.13380942e-02, bound:  3.15791816e-01\n",
      "Epoch: 14067 mean train loss:  1.13374060e-02, bound:  3.15791786e-01\n",
      "Epoch: 14068 mean train loss:  1.13367941e-02, bound:  3.15791726e-01\n",
      "Epoch: 14069 mean train loss:  1.13362353e-02, bound:  3.15791696e-01\n",
      "Epoch: 14070 mean train loss:  1.13355760e-02, bound:  3.15791696e-01\n",
      "Epoch: 14071 mean train loss:  1.13350190e-02, bound:  3.15791667e-01\n",
      "Epoch: 14072 mean train loss:  1.13344193e-02, bound:  3.15791607e-01\n",
      "Epoch: 14073 mean train loss:  1.13337506e-02, bound:  3.15791577e-01\n",
      "Epoch: 14074 mean train loss:  1.13331145e-02, bound:  3.15791577e-01\n",
      "Epoch: 14075 mean train loss:  1.13325361e-02, bound:  3.15791517e-01\n",
      "Epoch: 14076 mean train loss:  1.13318460e-02, bound:  3.15791488e-01\n",
      "Epoch: 14077 mean train loss:  1.13312183e-02, bound:  3.15791458e-01\n",
      "Epoch: 14078 mean train loss:  1.13305412e-02, bound:  3.15791458e-01\n",
      "Epoch: 14079 mean train loss:  1.13299126e-02, bound:  3.15791398e-01\n",
      "Epoch: 14080 mean train loss:  1.13293370e-02, bound:  3.15791368e-01\n",
      "Epoch: 14081 mean train loss:  1.13286069e-02, bound:  3.15791339e-01\n",
      "Epoch: 14082 mean train loss:  1.13281058e-02, bound:  3.15791339e-01\n",
      "Epoch: 14083 mean train loss:  1.13274716e-02, bound:  3.15791279e-01\n",
      "Epoch: 14084 mean train loss:  1.13268495e-02, bound:  3.15791249e-01\n",
      "Epoch: 14085 mean train loss:  1.13262273e-02, bound:  3.15791249e-01\n",
      "Epoch: 14086 mean train loss:  1.13255661e-02, bound:  3.15791160e-01\n",
      "Epoch: 14087 mean train loss:  1.13248965e-02, bound:  3.15791160e-01\n",
      "Epoch: 14088 mean train loss:  1.13244113e-02, bound:  3.15791160e-01\n",
      "Epoch: 14089 mean train loss:  1.13237016e-02, bound:  3.15791130e-01\n",
      "Epoch: 14090 mean train loss:  1.13230143e-02, bound:  3.15791041e-01\n",
      "Epoch: 14091 mean train loss:  1.13224201e-02, bound:  3.15791041e-01\n",
      "Epoch: 14092 mean train loss:  1.13217402e-02, bound:  3.15791011e-01\n",
      "Epoch: 14093 mean train loss:  1.13212457e-02, bound:  3.15790951e-01\n",
      "Epoch: 14094 mean train loss:  1.13205630e-02, bound:  3.15790951e-01\n",
      "Epoch: 14095 mean train loss:  1.13198888e-02, bound:  3.15790921e-01\n",
      "Epoch: 14096 mean train loss:  1.13192331e-02, bound:  3.15790862e-01\n",
      "Epoch: 14097 mean train loss:  1.13185868e-02, bound:  3.15790862e-01\n",
      "Epoch: 14098 mean train loss:  1.13179190e-02, bound:  3.15790832e-01\n",
      "Epoch: 14099 mean train loss:  1.13174021e-02, bound:  3.15790802e-01\n",
      "Epoch: 14100 mean train loss:  1.13167632e-02, bound:  3.15790743e-01\n",
      "Epoch: 14101 mean train loss:  1.13160526e-02, bound:  3.15790743e-01\n",
      "Epoch: 14102 mean train loss:  1.13154389e-02, bound:  3.15790713e-01\n",
      "Epoch: 14103 mean train loss:  1.13147972e-02, bound:  3.15790683e-01\n",
      "Epoch: 14104 mean train loss:  1.13141965e-02, bound:  3.15790683e-01\n",
      "Epoch: 14105 mean train loss:  1.13135716e-02, bound:  3.15790594e-01\n",
      "Epoch: 14106 mean train loss:  1.13130119e-02, bound:  3.15790594e-01\n",
      "Epoch: 14107 mean train loss:  1.13122361e-02, bound:  3.15790564e-01\n",
      "Epoch: 14108 mean train loss:  1.13117024e-02, bound:  3.15790534e-01\n",
      "Epoch: 14109 mean train loss:  1.13110952e-02, bound:  3.15790474e-01\n",
      "Epoch: 14110 mean train loss:  1.13104368e-02, bound:  3.15790445e-01\n",
      "Epoch: 14111 mean train loss:  1.13098659e-02, bound:  3.15790445e-01\n",
      "Epoch: 14112 mean train loss:  1.13091758e-02, bound:  3.15790415e-01\n",
      "Epoch: 14113 mean train loss:  1.13084875e-02, bound:  3.15790385e-01\n",
      "Epoch: 14114 mean train loss:  1.13078915e-02, bound:  3.15790355e-01\n",
      "Epoch: 14115 mean train loss:  1.13072703e-02, bound:  3.15790296e-01\n",
      "Epoch: 14116 mean train loss:  1.13066761e-02, bound:  3.15790296e-01\n",
      "Epoch: 14117 mean train loss:  1.13059608e-02, bound:  3.15790266e-01\n",
      "Epoch: 14118 mean train loss:  1.13052838e-02, bound:  3.15790236e-01\n",
      "Epoch: 14119 mean train loss:  1.13047184e-02, bound:  3.15790176e-01\n",
      "Epoch: 14120 mean train loss:  1.13040563e-02, bound:  3.15790147e-01\n",
      "Epoch: 14121 mean train loss:  1.13034416e-02, bound:  3.15790147e-01\n",
      "Epoch: 14122 mean train loss:  1.13028400e-02, bound:  3.15790117e-01\n",
      "Epoch: 14123 mean train loss:  1.13020716e-02, bound:  3.15790057e-01\n",
      "Epoch: 14124 mean train loss:  1.13016227e-02, bound:  3.15790027e-01\n",
      "Epoch: 14125 mean train loss:  1.13009298e-02, bound:  3.15790027e-01\n",
      "Epoch: 14126 mean train loss:  1.13001997e-02, bound:  3.15789998e-01\n",
      "Epoch: 14127 mean train loss:  1.12996642e-02, bound:  3.15789938e-01\n",
      "Epoch: 14128 mean train loss:  1.12990011e-02, bound:  3.15789908e-01\n",
      "Epoch: 14129 mean train loss:  1.12982905e-02, bound:  3.15789908e-01\n",
      "Epoch: 14130 mean train loss:  1.12977503e-02, bound:  3.15789849e-01\n",
      "Epoch: 14131 mean train loss:  1.12971794e-02, bound:  3.15789849e-01\n",
      "Epoch: 14132 mean train loss:  1.12964790e-02, bound:  3.15789819e-01\n",
      "Epoch: 14133 mean train loss:  1.12957619e-02, bound:  3.15789729e-01\n",
      "Epoch: 14134 mean train loss:  1.12952255e-02, bound:  3.15789729e-01\n",
      "Epoch: 14135 mean train loss:  1.12944860e-02, bound:  3.15789729e-01\n",
      "Epoch: 14136 mean train loss:  1.12938825e-02, bound:  3.15789700e-01\n",
      "Epoch: 14137 mean train loss:  1.12931319e-02, bound:  3.15789610e-01\n",
      "Epoch: 14138 mean train loss:  1.12927193e-02, bound:  3.15789610e-01\n",
      "Epoch: 14139 mean train loss:  1.12919882e-02, bound:  3.15789580e-01\n",
      "Epoch: 14140 mean train loss:  1.12912785e-02, bound:  3.15789580e-01\n",
      "Epoch: 14141 mean train loss:  1.12907514e-02, bound:  3.15789521e-01\n",
      "Epoch: 14142 mean train loss:  1.12900417e-02, bound:  3.15789491e-01\n",
      "Epoch: 14143 mean train loss:  1.12894615e-02, bound:  3.15789461e-01\n",
      "Epoch: 14144 mean train loss:  1.12887677e-02, bound:  3.15789402e-01\n",
      "Epoch: 14145 mean train loss:  1.12881595e-02, bound:  3.15789402e-01\n",
      "Epoch: 14146 mean train loss:  1.12875048e-02, bound:  3.15789372e-01\n",
      "Epoch: 14147 mean train loss:  1.12868259e-02, bound:  3.15789342e-01\n",
      "Epoch: 14148 mean train loss:  1.12862643e-02, bound:  3.15789282e-01\n",
      "Epoch: 14149 mean train loss:  1.12856003e-02, bound:  3.15789253e-01\n",
      "Epoch: 14150 mean train loss:  1.12849753e-02, bound:  3.15789253e-01\n",
      "Epoch: 14151 mean train loss:  1.12843076e-02, bound:  3.15789253e-01\n",
      "Epoch: 14152 mean train loss:  1.12836082e-02, bound:  3.15789163e-01\n",
      "Epoch: 14153 mean train loss:  1.12831127e-02, bound:  3.15789133e-01\n",
      "Epoch: 14154 mean train loss:  1.12823891e-02, bound:  3.15789133e-01\n",
      "Epoch: 14155 mean train loss:  1.12817530e-02, bound:  3.15789044e-01\n",
      "Epoch: 14156 mean train loss:  1.12811383e-02, bound:  3.15789044e-01\n",
      "Epoch: 14157 mean train loss:  1.12804864e-02, bound:  3.15789014e-01\n",
      "Epoch: 14158 mean train loss:  1.12797879e-02, bound:  3.15789014e-01\n",
      "Epoch: 14159 mean train loss:  1.12791453e-02, bound:  3.15788954e-01\n",
      "Epoch: 14160 mean train loss:  1.12785287e-02, bound:  3.15788954e-01\n",
      "Epoch: 14161 mean train loss:  1.12779681e-02, bound:  3.15788925e-01\n",
      "Epoch: 14162 mean train loss:  1.12772612e-02, bound:  3.15788865e-01\n",
      "Epoch: 14163 mean train loss:  1.12766037e-02, bound:  3.15788835e-01\n",
      "Epoch: 14164 mean train loss:  1.12758670e-02, bound:  3.15788805e-01\n",
      "Epoch: 14165 mean train loss:  1.12753678e-02, bound:  3.15788746e-01\n",
      "Epoch: 14166 mean train loss:  1.12747494e-02, bound:  3.15788746e-01\n",
      "Epoch: 14167 mean train loss:  1.12740090e-02, bound:  3.15788716e-01\n",
      "Epoch: 14168 mean train loss:  1.12733506e-02, bound:  3.15788686e-01\n",
      "Epoch: 14169 mean train loss:  1.12727173e-02, bound:  3.15788627e-01\n",
      "Epoch: 14170 mean train loss:  1.12721073e-02, bound:  3.15788597e-01\n",
      "Epoch: 14171 mean train loss:  1.12714591e-02, bound:  3.15788597e-01\n",
      "Epoch: 14172 mean train loss:  1.12708081e-02, bound:  3.15788567e-01\n",
      "Epoch: 14173 mean train loss:  1.12701189e-02, bound:  3.15788507e-01\n",
      "Epoch: 14174 mean train loss:  1.12695424e-02, bound:  3.15788478e-01\n",
      "Epoch: 14175 mean train loss:  1.12689473e-02, bound:  3.15788478e-01\n",
      "Epoch: 14176 mean train loss:  1.12682348e-02, bound:  3.15788418e-01\n",
      "Epoch: 14177 mean train loss:  1.12676285e-02, bound:  3.15788418e-01\n",
      "Epoch: 14178 mean train loss:  1.12669924e-02, bound:  3.15788388e-01\n",
      "Epoch: 14179 mean train loss:  1.12663116e-02, bound:  3.15788299e-01\n",
      "Epoch: 14180 mean train loss:  1.12656476e-02, bound:  3.15788299e-01\n",
      "Epoch: 14181 mean train loss:  1.12650963e-02, bound:  3.15788269e-01\n",
      "Epoch: 14182 mean train loss:  1.12643344e-02, bound:  3.15788269e-01\n",
      "Epoch: 14183 mean train loss:  1.12638120e-02, bound:  3.15788180e-01\n",
      "Epoch: 14184 mean train loss:  1.12631256e-02, bound:  3.15788180e-01\n",
      "Epoch: 14185 mean train loss:  1.12624690e-02, bound:  3.15788150e-01\n",
      "Epoch: 14186 mean train loss:  1.12618646e-02, bound:  3.15788120e-01\n",
      "Epoch: 14187 mean train loss:  1.12611735e-02, bound:  3.15788120e-01\n",
      "Epoch: 14188 mean train loss:  1.12605952e-02, bound:  3.15788060e-01\n",
      "Epoch: 14189 mean train loss:  1.12598343e-02, bound:  3.15788031e-01\n",
      "Epoch: 14190 mean train loss:  1.12591852e-02, bound:  3.15787971e-01\n",
      "Epoch: 14191 mean train loss:  1.12586394e-02, bound:  3.15787971e-01\n",
      "Epoch: 14192 mean train loss:  1.12578413e-02, bound:  3.15787941e-01\n",
      "Epoch: 14193 mean train loss:  1.12573337e-02, bound:  3.15787911e-01\n",
      "Epoch: 14194 mean train loss:  1.12565756e-02, bound:  3.15787852e-01\n",
      "Epoch: 14195 mean train loss:  1.12560056e-02, bound:  3.15787822e-01\n",
      "Epoch: 14196 mean train loss:  1.12553220e-02, bound:  3.15787822e-01\n",
      "Epoch: 14197 mean train loss:  1.12546775e-02, bound:  3.15787822e-01\n",
      "Epoch: 14198 mean train loss:  1.12540359e-02, bound:  3.15787733e-01\n",
      "Epoch: 14199 mean train loss:  1.12534231e-02, bound:  3.15787703e-01\n",
      "Epoch: 14200 mean train loss:  1.12527180e-02, bound:  3.15787703e-01\n",
      "Epoch: 14201 mean train loss:  1.12520941e-02, bound:  3.15787613e-01\n",
      "Epoch: 14202 mean train loss:  1.12513471e-02, bound:  3.15787584e-01\n",
      "Epoch: 14203 mean train loss:  1.12507297e-02, bound:  3.15787584e-01\n",
      "Epoch: 14204 mean train loss:  1.12500694e-02, bound:  3.15787584e-01\n",
      "Epoch: 14205 mean train loss:  1.12494500e-02, bound:  3.15787524e-01\n",
      "Epoch: 14206 mean train loss:  1.12487795e-02, bound:  3.15787494e-01\n",
      "Epoch: 14207 mean train loss:  1.12482235e-02, bound:  3.15787464e-01\n",
      "Epoch: 14208 mean train loss:  1.12474877e-02, bound:  3.15787464e-01\n",
      "Epoch: 14209 mean train loss:  1.12468917e-02, bound:  3.15787405e-01\n",
      "Epoch: 14210 mean train loss:  1.12461681e-02, bound:  3.15787375e-01\n",
      "Epoch: 14211 mean train loss:  1.12455646e-02, bound:  3.15787315e-01\n",
      "Epoch: 14212 mean train loss:  1.12448176e-02, bound:  3.15787286e-01\n",
      "Epoch: 14213 mean train loss:  1.12442765e-02, bound:  3.15787256e-01\n",
      "Epoch: 14214 mean train loss:  1.12436442e-02, bound:  3.15787256e-01\n",
      "Epoch: 14215 mean train loss:  1.12429662e-02, bound:  3.15787166e-01\n",
      "Epoch: 14216 mean train loss:  1.12422435e-02, bound:  3.15787166e-01\n",
      "Epoch: 14217 mean train loss:  1.12416074e-02, bound:  3.15787137e-01\n",
      "Epoch: 14218 mean train loss:  1.12409396e-02, bound:  3.15787077e-01\n",
      "Epoch: 14219 mean train loss:  1.12403352e-02, bound:  3.15787047e-01\n",
      "Epoch: 14220 mean train loss:  1.12396702e-02, bound:  3.15787017e-01\n",
      "Epoch: 14221 mean train loss:  1.12390472e-02, bound:  3.15787017e-01\n",
      "Epoch: 14222 mean train loss:  1.12383747e-02, bound:  3.15786988e-01\n",
      "Epoch: 14223 mean train loss:  1.12376977e-02, bound:  3.15786958e-01\n",
      "Epoch: 14224 mean train loss:  1.12370299e-02, bound:  3.15786898e-01\n",
      "Epoch: 14225 mean train loss:  1.12364246e-02, bound:  3.15786868e-01\n",
      "Epoch: 14226 mean train loss:  1.12356674e-02, bound:  3.15786868e-01\n",
      "Epoch: 14227 mean train loss:  1.12351049e-02, bound:  3.15786839e-01\n",
      "Epoch: 14228 mean train loss:  1.12344511e-02, bound:  3.15786779e-01\n",
      "Epoch: 14229 mean train loss:  1.12337936e-02, bound:  3.15786749e-01\n",
      "Epoch: 14230 mean train loss:  1.12330522e-02, bound:  3.15786719e-01\n",
      "Epoch: 14231 mean train loss:  1.12324711e-02, bound:  3.15786690e-01\n",
      "Epoch: 14232 mean train loss:  1.12317502e-02, bound:  3.15786660e-01\n",
      "Epoch: 14233 mean train loss:  1.12311766e-02, bound:  3.15786630e-01\n",
      "Epoch: 14234 mean train loss:  1.12304622e-02, bound:  3.15786600e-01\n",
      "Epoch: 14235 mean train loss:  1.12297414e-02, bound:  3.15786570e-01\n",
      "Epoch: 14236 mean train loss:  1.12291649e-02, bound:  3.15786511e-01\n",
      "Epoch: 14237 mean train loss:  1.12286182e-02, bound:  3.15786481e-01\n",
      "Epoch: 14238 mean train loss:  1.12277521e-02, bound:  3.15786481e-01\n",
      "Epoch: 14239 mean train loss:  1.12271430e-02, bound:  3.15786451e-01\n",
      "Epoch: 14240 mean train loss:  1.12266317e-02, bound:  3.15786391e-01\n",
      "Epoch: 14241 mean train loss:  1.12259071e-02, bound:  3.15786391e-01\n",
      "Epoch: 14242 mean train loss:  1.12251192e-02, bound:  3.15786362e-01\n",
      "Epoch: 14243 mean train loss:  1.12244589e-02, bound:  3.15786302e-01\n",
      "Epoch: 14244 mean train loss:  1.12239029e-02, bound:  3.15786272e-01\n",
      "Epoch: 14245 mean train loss:  1.12232212e-02, bound:  3.15786272e-01\n",
      "Epoch: 14246 mean train loss:  1.12225739e-02, bound:  3.15786183e-01\n",
      "Epoch: 14247 mean train loss:  1.12219406e-02, bound:  3.15786183e-01\n",
      "Epoch: 14248 mean train loss:  1.12212989e-02, bound:  3.15786153e-01\n",
      "Epoch: 14249 mean train loss:  1.12205399e-02, bound:  3.15786153e-01\n",
      "Epoch: 14250 mean train loss:  1.12199169e-02, bound:  3.15786093e-01\n",
      "Epoch: 14251 mean train loss:  1.12193478e-02, bound:  3.15786064e-01\n",
      "Epoch: 14252 mean train loss:  1.12186419e-02, bound:  3.15786034e-01\n",
      "Epoch: 14253 mean train loss:  1.12180458e-02, bound:  3.15785974e-01\n",
      "Epoch: 14254 mean train loss:  1.12171900e-02, bound:  3.15785974e-01\n",
      "Epoch: 14255 mean train loss:  1.12166628e-02, bound:  3.15785944e-01\n",
      "Epoch: 14256 mean train loss:  1.12160100e-02, bound:  3.15785915e-01\n",
      "Epoch: 14257 mean train loss:  1.12153245e-02, bound:  3.15785855e-01\n",
      "Epoch: 14258 mean train loss:  1.12145636e-02, bound:  3.15785825e-01\n",
      "Epoch: 14259 mean train loss:  1.12139992e-02, bound:  3.15785795e-01\n",
      "Epoch: 14260 mean train loss:  1.12131918e-02, bound:  3.15785736e-01\n",
      "Epoch: 14261 mean train loss:  1.12126600e-02, bound:  3.15785706e-01\n",
      "Epoch: 14262 mean train loss:  1.12120230e-02, bound:  3.15785706e-01\n",
      "Epoch: 14263 mean train loss:  1.12113245e-02, bound:  3.15785646e-01\n",
      "Epoch: 14264 mean train loss:  1.12106735e-02, bound:  3.15785617e-01\n",
      "Epoch: 14265 mean train loss:  1.12100020e-02, bound:  3.15785587e-01\n",
      "Epoch: 14266 mean train loss:  1.12093445e-02, bound:  3.15785587e-01\n",
      "Epoch: 14267 mean train loss:  1.12087335e-02, bound:  3.15785527e-01\n",
      "Epoch: 14268 mean train loss:  1.12080071e-02, bound:  3.15785527e-01\n",
      "Epoch: 14269 mean train loss:  1.12072676e-02, bound:  3.15785468e-01\n",
      "Epoch: 14270 mean train loss:  1.12067126e-02, bound:  3.15785468e-01\n",
      "Epoch: 14271 mean train loss:  1.12060513e-02, bound:  3.15785408e-01\n",
      "Epoch: 14272 mean train loss:  1.12052495e-02, bound:  3.15785348e-01\n",
      "Epoch: 14273 mean train loss:  1.12046702e-02, bound:  3.15785319e-01\n",
      "Epoch: 14274 mean train loss:  1.12040071e-02, bound:  3.15785319e-01\n",
      "Epoch: 14275 mean train loss:  1.12033384e-02, bound:  3.15785289e-01\n",
      "Epoch: 14276 mean train loss:  1.12026315e-02, bound:  3.15785259e-01\n",
      "Epoch: 14277 mean train loss:  1.12019386e-02, bound:  3.15785199e-01\n",
      "Epoch: 14278 mean train loss:  1.12013854e-02, bound:  3.15785170e-01\n",
      "Epoch: 14279 mean train loss:  1.12007102e-02, bound:  3.15785140e-01\n",
      "Epoch: 14280 mean train loss:  1.11999726e-02, bound:  3.15785140e-01\n",
      "Epoch: 14281 mean train loss:  1.11993719e-02, bound:  3.15785080e-01\n",
      "Epoch: 14282 mean train loss:  1.11986045e-02, bound:  3.15785050e-01\n",
      "Epoch: 14283 mean train loss:  1.11980438e-02, bound:  3.15785021e-01\n",
      "Epoch: 14284 mean train loss:  1.11972885e-02, bound:  3.15784961e-01\n",
      "Epoch: 14285 mean train loss:  1.11966459e-02, bound:  3.15784961e-01\n",
      "Epoch: 14286 mean train loss:  1.11959931e-02, bound:  3.15784961e-01\n",
      "Epoch: 14287 mean train loss:  1.11952750e-02, bound:  3.15784901e-01\n",
      "Epoch: 14288 mean train loss:  1.11946622e-02, bound:  3.15784842e-01\n",
      "Epoch: 14289 mean train loss:  1.11939302e-02, bound:  3.15784842e-01\n",
      "Epoch: 14290 mean train loss:  1.11932755e-02, bound:  3.15784782e-01\n",
      "Epoch: 14291 mean train loss:  1.11927493e-02, bound:  3.15784752e-01\n",
      "Epoch: 14292 mean train loss:  1.11919325e-02, bound:  3.15784723e-01\n",
      "Epoch: 14293 mean train loss:  1.11912983e-02, bound:  3.15784723e-01\n",
      "Epoch: 14294 mean train loss:  1.11905923e-02, bound:  3.15784663e-01\n",
      "Epoch: 14295 mean train loss:  1.11900242e-02, bound:  3.15784633e-01\n",
      "Epoch: 14296 mean train loss:  1.11892540e-02, bound:  3.15784603e-01\n",
      "Epoch: 14297 mean train loss:  1.11885788e-02, bound:  3.15784574e-01\n",
      "Epoch: 14298 mean train loss:  1.11879352e-02, bound:  3.15784544e-01\n",
      "Epoch: 14299 mean train loss:  1.11872843e-02, bound:  3.15784514e-01\n",
      "Epoch: 14300 mean train loss:  1.11866081e-02, bound:  3.15784484e-01\n",
      "Epoch: 14301 mean train loss:  1.11859189e-02, bound:  3.15784454e-01\n",
      "Epoch: 14302 mean train loss:  1.11852279e-02, bound:  3.15784395e-01\n",
      "Epoch: 14303 mean train loss:  1.11847110e-02, bound:  3.15784365e-01\n",
      "Epoch: 14304 mean train loss:  1.11839045e-02, bound:  3.15784365e-01\n",
      "Epoch: 14305 mean train loss:  1.11833420e-02, bound:  3.15784305e-01\n",
      "Epoch: 14306 mean train loss:  1.11825494e-02, bound:  3.15784276e-01\n",
      "Epoch: 14307 mean train loss:  1.11818956e-02, bound:  3.15784246e-01\n",
      "Epoch: 14308 mean train loss:  1.11812670e-02, bound:  3.15784186e-01\n",
      "Epoch: 14309 mean train loss:  1.11805666e-02, bound:  3.15784186e-01\n",
      "Epoch: 14310 mean train loss:  1.11798719e-02, bound:  3.15784156e-01\n",
      "Epoch: 14311 mean train loss:  1.11792739e-02, bound:  3.15784127e-01\n",
      "Epoch: 14312 mean train loss:  1.11785708e-02, bound:  3.15784067e-01\n",
      "Epoch: 14313 mean train loss:  1.11778630e-02, bound:  3.15784037e-01\n",
      "Epoch: 14314 mean train loss:  1.11771682e-02, bound:  3.15784037e-01\n",
      "Epoch: 14315 mean train loss:  1.11765061e-02, bound:  3.15783978e-01\n",
      "Epoch: 14316 mean train loss:  1.11757908e-02, bound:  3.15783918e-01\n",
      "Epoch: 14317 mean train loss:  1.11751826e-02, bound:  3.15783918e-01\n",
      "Epoch: 14318 mean train loss:  1.11744916e-02, bound:  3.15783858e-01\n",
      "Epoch: 14319 mean train loss:  1.11738676e-02, bound:  3.15783858e-01\n",
      "Epoch: 14320 mean train loss:  1.11731747e-02, bound:  3.15783828e-01\n",
      "Epoch: 14321 mean train loss:  1.11723831e-02, bound:  3.15783799e-01\n",
      "Epoch: 14322 mean train loss:  1.11717824e-02, bound:  3.15783739e-01\n",
      "Epoch: 14323 mean train loss:  1.11711277e-02, bound:  3.15783739e-01\n",
      "Epoch: 14324 mean train loss:  1.11704310e-02, bound:  3.15783709e-01\n",
      "Epoch: 14325 mean train loss:  1.11697121e-02, bound:  3.15783650e-01\n",
      "Epoch: 14326 mean train loss:  1.11690992e-02, bound:  3.15783620e-01\n",
      "Epoch: 14327 mean train loss:  1.11684892e-02, bound:  3.15783590e-01\n",
      "Epoch: 14328 mean train loss:  1.11677572e-02, bound:  3.15783530e-01\n",
      "Epoch: 14329 mean train loss:  1.11671025e-02, bound:  3.15783530e-01\n",
      "Epoch: 14330 mean train loss:  1.11663137e-02, bound:  3.15783501e-01\n",
      "Epoch: 14331 mean train loss:  1.11657679e-02, bound:  3.15783471e-01\n",
      "Epoch: 14332 mean train loss:  1.11650825e-02, bound:  3.15783411e-01\n",
      "Epoch: 14333 mean train loss:  1.11643476e-02, bound:  3.15783411e-01\n",
      "Epoch: 14334 mean train loss:  1.11637590e-02, bound:  3.15783352e-01\n",
      "Epoch: 14335 mean train loss:  1.11631230e-02, bound:  3.15783322e-01\n",
      "Epoch: 14336 mean train loss:  1.11623574e-02, bound:  3.15783292e-01\n",
      "Epoch: 14337 mean train loss:  1.11616300e-02, bound:  3.15783262e-01\n",
      "Epoch: 14338 mean train loss:  1.11608766e-02, bound:  3.15783262e-01\n",
      "Epoch: 14339 mean train loss:  1.11603634e-02, bound:  3.15783203e-01\n",
      "Epoch: 14340 mean train loss:  1.11596333e-02, bound:  3.15783173e-01\n",
      "Epoch: 14341 mean train loss:  1.11589143e-02, bound:  3.15783143e-01\n",
      "Epoch: 14342 mean train loss:  1.11582149e-02, bound:  3.15783083e-01\n",
      "Epoch: 14343 mean train loss:  1.11576533e-02, bound:  3.15783054e-01\n",
      "Epoch: 14344 mean train loss:  1.11568440e-02, bound:  3.15783024e-01\n",
      "Epoch: 14345 mean train loss:  1.11562237e-02, bound:  3.15783024e-01\n",
      "Epoch: 14346 mean train loss:  1.11555979e-02, bound:  3.15782964e-01\n",
      "Epoch: 14347 mean train loss:  1.11549245e-02, bound:  3.15782934e-01\n",
      "Epoch: 14348 mean train loss:  1.11541664e-02, bound:  3.15782905e-01\n",
      "Epoch: 14349 mean train loss:  1.11535303e-02, bound:  3.15782845e-01\n",
      "Epoch: 14350 mean train loss:  1.11528290e-02, bound:  3.15782815e-01\n",
      "Epoch: 14351 mean train loss:  1.11521576e-02, bound:  3.15782815e-01\n",
      "Epoch: 14352 mean train loss:  1.11514684e-02, bound:  3.15782785e-01\n",
      "Epoch: 14353 mean train loss:  1.11507326e-02, bound:  3.15782726e-01\n",
      "Epoch: 14354 mean train loss:  1.11500770e-02, bound:  3.15782726e-01\n",
      "Epoch: 14355 mean train loss:  1.11494977e-02, bound:  3.15782666e-01\n",
      "Epoch: 14356 mean train loss:  1.11487713e-02, bound:  3.15782607e-01\n",
      "Epoch: 14357 mean train loss:  1.11481333e-02, bound:  3.15782607e-01\n",
      "Epoch: 14358 mean train loss:  1.11473529e-02, bound:  3.15782577e-01\n",
      "Epoch: 14359 mean train loss:  1.11467084e-02, bound:  3.15782547e-01\n",
      "Epoch: 14360 mean train loss:  1.11459726e-02, bound:  3.15782487e-01\n",
      "Epoch: 14361 mean train loss:  1.11453487e-02, bound:  3.15782487e-01\n",
      "Epoch: 14362 mean train loss:  1.11446064e-02, bound:  3.15782428e-01\n",
      "Epoch: 14363 mean train loss:  1.11440215e-02, bound:  3.15782398e-01\n",
      "Epoch: 14364 mean train loss:  1.11432774e-02, bound:  3.15782398e-01\n",
      "Epoch: 14365 mean train loss:  1.11426217e-02, bound:  3.15782309e-01\n",
      "Epoch: 14366 mean train loss:  1.11419046e-02, bound:  3.15782309e-01\n",
      "Epoch: 14367 mean train loss:  1.11413039e-02, bound:  3.15782279e-01\n",
      "Epoch: 14368 mean train loss:  1.11405663e-02, bound:  3.15782249e-01\n",
      "Epoch: 14369 mean train loss:  1.11398296e-02, bound:  3.15782189e-01\n",
      "Epoch: 14370 mean train loss:  1.11391656e-02, bound:  3.15782160e-01\n",
      "Epoch: 14371 mean train loss:  1.11385453e-02, bound:  3.15782160e-01\n",
      "Epoch: 14372 mean train loss:  1.11377826e-02, bound:  3.15782100e-01\n",
      "Epoch: 14373 mean train loss:  1.11371083e-02, bound:  3.15782070e-01\n",
      "Epoch: 14374 mean train loss:  1.11363260e-02, bound:  3.15782040e-01\n",
      "Epoch: 14375 mean train loss:  1.11358427e-02, bound:  3.15781981e-01\n",
      "Epoch: 14376 mean train loss:  1.11351265e-02, bound:  3.15781981e-01\n",
      "Epoch: 14377 mean train loss:  1.11344000e-02, bound:  3.15781921e-01\n",
      "Epoch: 14378 mean train loss:  1.11336932e-02, bound:  3.15781921e-01\n",
      "Epoch: 14379 mean train loss:  1.11331046e-02, bound:  3.15781862e-01\n",
      "Epoch: 14380 mean train loss:  1.11323148e-02, bound:  3.15781862e-01\n",
      "Epoch: 14381 mean train loss:  1.11315679e-02, bound:  3.15781832e-01\n",
      "Epoch: 14382 mean train loss:  1.11309215e-02, bound:  3.15781802e-01\n",
      "Epoch: 14383 mean train loss:  1.11302454e-02, bound:  3.15781742e-01\n",
      "Epoch: 14384 mean train loss:  1.11295814e-02, bound:  3.15781713e-01\n",
      "Epoch: 14385 mean train loss:  1.11287637e-02, bound:  3.15781713e-01\n",
      "Epoch: 14386 mean train loss:  1.11282188e-02, bound:  3.15781623e-01\n",
      "Epoch: 14387 mean train loss:  1.11276302e-02, bound:  3.15781623e-01\n",
      "Epoch: 14388 mean train loss:  1.11267678e-02, bound:  3.15781593e-01\n",
      "Epoch: 14389 mean train loss:  1.11261345e-02, bound:  3.15781534e-01\n",
      "Epoch: 14390 mean train loss:  1.11253932e-02, bound:  3.15781504e-01\n",
      "Epoch: 14391 mean train loss:  1.11247795e-02, bound:  3.15781504e-01\n",
      "Epoch: 14392 mean train loss:  1.11241136e-02, bound:  3.15781474e-01\n",
      "Epoch: 14393 mean train loss:  1.11233722e-02, bound:  3.15781415e-01\n",
      "Epoch: 14394 mean train loss:  1.11226477e-02, bound:  3.15781385e-01\n",
      "Epoch: 14395 mean train loss:  1.11220228e-02, bound:  3.15781355e-01\n",
      "Epoch: 14396 mean train loss:  1.11212526e-02, bound:  3.15781295e-01\n",
      "Epoch: 14397 mean train loss:  1.11205401e-02, bound:  3.15781295e-01\n",
      "Epoch: 14398 mean train loss:  1.11198891e-02, bound:  3.15781265e-01\n",
      "Epoch: 14399 mean train loss:  1.11192716e-02, bound:  3.15781206e-01\n",
      "Epoch: 14400 mean train loss:  1.11185210e-02, bound:  3.15781176e-01\n",
      "Epoch: 14401 mean train loss:  1.11179249e-02, bound:  3.15781146e-01\n",
      "Epoch: 14402 mean train loss:  1.11171110e-02, bound:  3.15781087e-01\n",
      "Epoch: 14403 mean train loss:  1.11165568e-02, bound:  3.15781087e-01\n",
      "Epoch: 14404 mean train loss:  1.11158201e-02, bound:  3.15781057e-01\n",
      "Epoch: 14405 mean train loss:  1.11150369e-02, bound:  3.15781027e-01\n",
      "Epoch: 14406 mean train loss:  1.11143347e-02, bound:  3.15780967e-01\n",
      "Epoch: 14407 mean train loss:  1.11137405e-02, bound:  3.15780967e-01\n",
      "Epoch: 14408 mean train loss:  1.11128939e-02, bound:  3.15780908e-01\n",
      "Epoch: 14409 mean train loss:  1.11123109e-02, bound:  3.15780878e-01\n",
      "Epoch: 14410 mean train loss:  1.11115249e-02, bound:  3.15780848e-01\n",
      "Epoch: 14411 mean train loss:  1.11109894e-02, bound:  3.15780789e-01\n",
      "Epoch: 14412 mean train loss:  1.11101726e-02, bound:  3.15780789e-01\n",
      "Epoch: 14413 mean train loss:  1.11095551e-02, bound:  3.15780729e-01\n",
      "Epoch: 14414 mean train loss:  1.11087793e-02, bound:  3.15780699e-01\n",
      "Epoch: 14415 mean train loss:  1.11082634e-02, bound:  3.15780669e-01\n",
      "Epoch: 14416 mean train loss:  1.11074504e-02, bound:  3.15780640e-01\n",
      "Epoch: 14417 mean train loss:  1.11067090e-02, bound:  3.15780610e-01\n",
      "Epoch: 14418 mean train loss:  1.11060562e-02, bound:  3.15780580e-01\n",
      "Epoch: 14419 mean train loss:  1.11054275e-02, bound:  3.15780550e-01\n",
      "Epoch: 14420 mean train loss:  1.11047067e-02, bound:  3.15780491e-01\n",
      "Epoch: 14421 mean train loss:  1.11039411e-02, bound:  3.15780491e-01\n",
      "Epoch: 14422 mean train loss:  1.11033125e-02, bound:  3.15780461e-01\n",
      "Epoch: 14423 mean train loss:  1.11026186e-02, bound:  3.15780401e-01\n",
      "Epoch: 14424 mean train loss:  1.11018857e-02, bound:  3.15780401e-01\n",
      "Epoch: 14425 mean train loss:  1.11012412e-02, bound:  3.15780312e-01\n",
      "Epoch: 14426 mean train loss:  1.11005446e-02, bound:  3.15780282e-01\n",
      "Epoch: 14427 mean train loss:  1.10998470e-02, bound:  3.15780282e-01\n",
      "Epoch: 14428 mean train loss:  1.10990824e-02, bound:  3.15780252e-01\n",
      "Epoch: 14429 mean train loss:  1.10983653e-02, bound:  3.15780193e-01\n",
      "Epoch: 14430 mean train loss:  1.10976649e-02, bound:  3.15780163e-01\n",
      "Epoch: 14431 mean train loss:  1.10971015e-02, bound:  3.15780163e-01\n",
      "Epoch: 14432 mean train loss:  1.10963127e-02, bound:  3.15780133e-01\n",
      "Epoch: 14433 mean train loss:  1.10956011e-02, bound:  3.15780073e-01\n",
      "Epoch: 14434 mean train loss:  1.10948458e-02, bound:  3.15780044e-01\n",
      "Epoch: 14435 mean train loss:  1.10943327e-02, bound:  3.15780044e-01\n",
      "Epoch: 14436 mean train loss:  1.10934796e-02, bound:  3.15779984e-01\n",
      "Epoch: 14437 mean train loss:  1.10927513e-02, bound:  3.15779954e-01\n",
      "Epoch: 14438 mean train loss:  1.10920472e-02, bound:  3.15779924e-01\n",
      "Epoch: 14439 mean train loss:  1.10915219e-02, bound:  3.15779865e-01\n",
      "Epoch: 14440 mean train loss:  1.10907359e-02, bound:  3.15779835e-01\n",
      "Epoch: 14441 mean train loss:  1.10900374e-02, bound:  3.15779835e-01\n",
      "Epoch: 14442 mean train loss:  1.10892961e-02, bound:  3.15779746e-01\n",
      "Epoch: 14443 mean train loss:  1.10886600e-02, bound:  3.15779746e-01\n",
      "Epoch: 14444 mean train loss:  1.10879084e-02, bound:  3.15779716e-01\n",
      "Epoch: 14445 mean train loss:  1.10872136e-02, bound:  3.15779656e-01\n",
      "Epoch: 14446 mean train loss:  1.10864863e-02, bound:  3.15779626e-01\n",
      "Epoch: 14447 mean train loss:  1.10859163e-02, bound:  3.15779597e-01\n",
      "Epoch: 14448 mean train loss:  1.10851796e-02, bound:  3.15779537e-01\n",
      "Epoch: 14449 mean train loss:  1.10844662e-02, bound:  3.15779537e-01\n",
      "Epoch: 14450 mean train loss:  1.10836755e-02, bound:  3.15779507e-01\n",
      "Epoch: 14451 mean train loss:  1.10830721e-02, bound:  3.15779477e-01\n",
      "Epoch: 14452 mean train loss:  1.10823568e-02, bound:  3.15779418e-01\n",
      "Epoch: 14453 mean train loss:  1.10816294e-02, bound:  3.15779418e-01\n",
      "Epoch: 14454 mean train loss:  1.10809319e-02, bound:  3.15779358e-01\n",
      "Epoch: 14455 mean train loss:  1.10802846e-02, bound:  3.15779328e-01\n",
      "Epoch: 14456 mean train loss:  1.10795395e-02, bound:  3.15779299e-01\n",
      "Epoch: 14457 mean train loss:  1.10787936e-02, bound:  3.15779269e-01\n",
      "Epoch: 14458 mean train loss:  1.10780457e-02, bound:  3.15779269e-01\n",
      "Epoch: 14459 mean train loss:  1.10774823e-02, bound:  3.15779179e-01\n",
      "Epoch: 14460 mean train loss:  1.10766757e-02, bound:  3.15779150e-01\n",
      "Epoch: 14461 mean train loss:  1.10760657e-02, bound:  3.15779150e-01\n",
      "Epoch: 14462 mean train loss:  1.10752219e-02, bound:  3.15779090e-01\n",
      "Epoch: 14463 mean train loss:  1.10746725e-02, bound:  3.15779060e-01\n",
      "Epoch: 14464 mean train loss:  1.10739395e-02, bound:  3.15779030e-01\n",
      "Epoch: 14465 mean train loss:  1.10732280e-02, bound:  3.15778971e-01\n",
      "Epoch: 14466 mean train loss:  1.10725276e-02, bound:  3.15778971e-01\n",
      "Epoch: 14467 mean train loss:  1.10717919e-02, bound:  3.15778911e-01\n",
      "Epoch: 14468 mean train loss:  1.10710952e-02, bound:  3.15778881e-01\n",
      "Epoch: 14469 mean train loss:  1.10703856e-02, bound:  3.15778852e-01\n",
      "Epoch: 14470 mean train loss:  1.10697066e-02, bound:  3.15778852e-01\n",
      "Epoch: 14471 mean train loss:  1.10690007e-02, bound:  3.15778792e-01\n",
      "Epoch: 14472 mean train loss:  1.10682994e-02, bound:  3.15778732e-01\n",
      "Epoch: 14473 mean train loss:  1.10675795e-02, bound:  3.15778732e-01\n",
      "Epoch: 14474 mean train loss:  1.10668885e-02, bound:  3.15778702e-01\n",
      "Epoch: 14475 mean train loss:  1.10662170e-02, bound:  3.15778643e-01\n",
      "Epoch: 14476 mean train loss:  1.10655734e-02, bound:  3.15778613e-01\n",
      "Epoch: 14477 mean train loss:  1.10646673e-02, bound:  3.15778583e-01\n",
      "Epoch: 14478 mean train loss:  1.10640274e-02, bound:  3.15778553e-01\n",
      "Epoch: 14479 mean train loss:  1.10633923e-02, bound:  3.15778494e-01\n",
      "Epoch: 14480 mean train loss:  1.10627012e-02, bound:  3.15778494e-01\n",
      "Epoch: 14481 mean train loss:  1.10619804e-02, bound:  3.15778434e-01\n",
      "Epoch: 14482 mean train loss:  1.10611664e-02, bound:  3.15778404e-01\n",
      "Epoch: 14483 mean train loss:  1.10605564e-02, bound:  3.15778404e-01\n",
      "Epoch: 14484 mean train loss:  1.10598076e-02, bound:  3.15778315e-01\n",
      "Epoch: 14485 mean train loss:  1.10590868e-02, bound:  3.15778315e-01\n",
      "Epoch: 14486 mean train loss:  1.10584125e-02, bound:  3.15778285e-01\n",
      "Epoch: 14487 mean train loss:  1.10577494e-02, bound:  3.15778255e-01\n",
      "Epoch: 14488 mean train loss:  1.10569643e-02, bound:  3.15778196e-01\n",
      "Epoch: 14489 mean train loss:  1.10562705e-02, bound:  3.15778166e-01\n",
      "Epoch: 14490 mean train loss:  1.10556139e-02, bound:  3.15778136e-01\n",
      "Epoch: 14491 mean train loss:  1.10549731e-02, bound:  3.15778106e-01\n",
      "Epoch: 14492 mean train loss:  1.10542076e-02, bound:  3.15778077e-01\n",
      "Epoch: 14493 mean train loss:  1.10534085e-02, bound:  3.15778047e-01\n",
      "Epoch: 14494 mean train loss:  1.10527007e-02, bound:  3.15777987e-01\n",
      "Epoch: 14495 mean train loss:  1.10521223e-02, bound:  3.15777957e-01\n",
      "Epoch: 14496 mean train loss:  1.10512292e-02, bound:  3.15777928e-01\n",
      "Epoch: 14497 mean train loss:  1.10506630e-02, bound:  3.15777928e-01\n",
      "Epoch: 14498 mean train loss:  1.10498928e-02, bound:  3.15777838e-01\n",
      "Epoch: 14499 mean train loss:  1.10493163e-02, bound:  3.15777838e-01\n",
      "Epoch: 14500 mean train loss:  1.10485144e-02, bound:  3.15777808e-01\n",
      "Epoch: 14501 mean train loss:  1.10477824e-02, bound:  3.15777749e-01\n",
      "Epoch: 14502 mean train loss:  1.10469665e-02, bound:  3.15777719e-01\n",
      "Epoch: 14503 mean train loss:  1.10463835e-02, bound:  3.15777719e-01\n",
      "Epoch: 14504 mean train loss:  1.10456059e-02, bound:  3.15777630e-01\n",
      "Epoch: 14505 mean train loss:  1.10448860e-02, bound:  3.15777630e-01\n",
      "Epoch: 14506 mean train loss:  1.10441484e-02, bound:  3.15777600e-01\n",
      "Epoch: 14507 mean train loss:  1.10434350e-02, bound:  3.15777540e-01\n",
      "Epoch: 14508 mean train loss:  1.10428352e-02, bound:  3.15777540e-01\n",
      "Epoch: 14509 mean train loss:  1.10421097e-02, bound:  3.15777481e-01\n",
      "Epoch: 14510 mean train loss:  1.10413516e-02, bound:  3.15777421e-01\n",
      "Epoch: 14511 mean train loss:  1.10406717e-02, bound:  3.15777421e-01\n",
      "Epoch: 14512 mean train loss:  1.10400002e-02, bound:  3.15777391e-01\n",
      "Epoch: 14513 mean train loss:  1.10392086e-02, bound:  3.15777332e-01\n",
      "Epoch: 14514 mean train loss:  1.10384710e-02, bound:  3.15777302e-01\n",
      "Epoch: 14515 mean train loss:  1.10378154e-02, bound:  3.15777272e-01\n",
      "Epoch: 14516 mean train loss:  1.10371113e-02, bound:  3.15777272e-01\n",
      "Epoch: 14517 mean train loss:  1.10363392e-02, bound:  3.15777212e-01\n",
      "Epoch: 14518 mean train loss:  1.10356510e-02, bound:  3.15777183e-01\n",
      "Epoch: 14519 mean train loss:  1.10348798e-02, bound:  3.15777153e-01\n",
      "Epoch: 14520 mean train loss:  1.10342810e-02, bound:  3.15777153e-01\n",
      "Epoch: 14521 mean train loss:  1.10334437e-02, bound:  3.15777063e-01\n",
      "Epoch: 14522 mean train loss:  1.10328207e-02, bound:  3.15777034e-01\n",
      "Epoch: 14523 mean train loss:  1.10320793e-02, bound:  3.15777004e-01\n",
      "Epoch: 14524 mean train loss:  1.10313026e-02, bound:  3.15776974e-01\n",
      "Epoch: 14525 mean train loss:  1.10307196e-02, bound:  3.15776944e-01\n",
      "Epoch: 14526 mean train loss:  1.10299578e-02, bound:  3.15776914e-01\n",
      "Epoch: 14527 mean train loss:  1.10292071e-02, bound:  3.15776855e-01\n",
      "Epoch: 14528 mean train loss:  1.10284202e-02, bound:  3.15776825e-01\n",
      "Epoch: 14529 mean train loss:  1.10277329e-02, bound:  3.15776795e-01\n",
      "Epoch: 14530 mean train loss:  1.10269850e-02, bound:  3.15776765e-01\n",
      "Epoch: 14531 mean train loss:  1.10263079e-02, bound:  3.15776736e-01\n",
      "Epoch: 14532 mean train loss:  1.10257231e-02, bound:  3.15776706e-01\n",
      "Epoch: 14533 mean train loss:  1.10249827e-02, bound:  3.15776646e-01\n",
      "Epoch: 14534 mean train loss:  1.10241575e-02, bound:  3.15776616e-01\n",
      "Epoch: 14535 mean train loss:  1.10234637e-02, bound:  3.15776587e-01\n",
      "Epoch: 14536 mean train loss:  1.10227568e-02, bound:  3.15776557e-01\n",
      "Epoch: 14537 mean train loss:  1.10220611e-02, bound:  3.15776497e-01\n",
      "Epoch: 14538 mean train loss:  1.10214269e-02, bound:  3.15776497e-01\n",
      "Epoch: 14539 mean train loss:  1.10205999e-02, bound:  3.15776438e-01\n",
      "Epoch: 14540 mean train loss:  1.10199228e-02, bound:  3.15776408e-01\n",
      "Epoch: 14541 mean train loss:  1.10192243e-02, bound:  3.15776408e-01\n",
      "Epoch: 14542 mean train loss:  1.10183964e-02, bound:  3.15776318e-01\n",
      "Epoch: 14543 mean train loss:  1.10177333e-02, bound:  3.15776289e-01\n",
      "Epoch: 14544 mean train loss:  1.10170217e-02, bound:  3.15776289e-01\n",
      "Epoch: 14545 mean train loss:  1.10162478e-02, bound:  3.15776199e-01\n",
      "Epoch: 14546 mean train loss:  1.10155270e-02, bound:  3.15776199e-01\n",
      "Epoch: 14547 mean train loss:  1.10148415e-02, bound:  3.15776169e-01\n",
      "Epoch: 14548 mean train loss:  1.10141095e-02, bound:  3.15776139e-01\n",
      "Epoch: 14549 mean train loss:  1.10133626e-02, bound:  3.15776110e-01\n",
      "Epoch: 14550 mean train loss:  1.10126203e-02, bound:  3.15776080e-01\n",
      "Epoch: 14551 mean train loss:  1.10118883e-02, bound:  3.15775990e-01\n",
      "Epoch: 14552 mean train loss:  1.10112764e-02, bound:  3.15775990e-01\n",
      "Epoch: 14553 mean train loss:  1.10105556e-02, bound:  3.15775961e-01\n",
      "Epoch: 14554 mean train loss:  1.10097975e-02, bound:  3.15775931e-01\n",
      "Epoch: 14555 mean train loss:  1.10091874e-02, bound:  3.15775871e-01\n",
      "Epoch: 14556 mean train loss:  1.10084182e-02, bound:  3.15775841e-01\n",
      "Epoch: 14557 mean train loss:  1.10076461e-02, bound:  3.15775812e-01\n",
      "Epoch: 14558 mean train loss:  1.10069020e-02, bound:  3.15775812e-01\n",
      "Epoch: 14559 mean train loss:  1.10061821e-02, bound:  3.15775752e-01\n",
      "Epoch: 14560 mean train loss:  1.10054081e-02, bound:  3.15775722e-01\n",
      "Epoch: 14561 mean train loss:  1.10047748e-02, bound:  3.15775722e-01\n",
      "Epoch: 14562 mean train loss:  1.10039487e-02, bound:  3.15775633e-01\n",
      "Epoch: 14563 mean train loss:  1.10032698e-02, bound:  3.15775603e-01\n",
      "Epoch: 14564 mean train loss:  1.10025676e-02, bound:  3.15775603e-01\n",
      "Epoch: 14565 mean train loss:  1.10017750e-02, bound:  3.15775543e-01\n",
      "Epoch: 14566 mean train loss:  1.10011240e-02, bound:  3.15775484e-01\n",
      "Epoch: 14567 mean train loss:  1.10003231e-02, bound:  3.15775484e-01\n",
      "Epoch: 14568 mean train loss:  1.09997028e-02, bound:  3.15775424e-01\n",
      "Epoch: 14569 mean train loss:  1.09989047e-02, bound:  3.15775394e-01\n",
      "Epoch: 14570 mean train loss:  1.09982193e-02, bound:  3.15775394e-01\n",
      "Epoch: 14571 mean train loss:  1.09974928e-02, bound:  3.15775335e-01\n",
      "Epoch: 14572 mean train loss:  1.09967384e-02, bound:  3.15775305e-01\n",
      "Epoch: 14573 mean train loss:  1.09960036e-02, bound:  3.15775275e-01\n",
      "Epoch: 14574 mean train loss:  1.09953573e-02, bound:  3.15775216e-01\n",
      "Epoch: 14575 mean train loss:  1.09945918e-02, bound:  3.15775186e-01\n",
      "Epoch: 14576 mean train loss:  1.09938467e-02, bound:  3.15775156e-01\n",
      "Epoch: 14577 mean train loss:  1.09931221e-02, bound:  3.15775096e-01\n",
      "Epoch: 14578 mean train loss:  1.09924199e-02, bound:  3.15775067e-01\n",
      "Epoch: 14579 mean train loss:  1.09916171e-02, bound:  3.15775037e-01\n",
      "Epoch: 14580 mean train loss:  1.09910304e-02, bound:  3.15774977e-01\n",
      "Epoch: 14581 mean train loss:  1.09901987e-02, bound:  3.15774977e-01\n",
      "Epoch: 14582 mean train loss:  1.09895412e-02, bound:  3.15774918e-01\n",
      "Epoch: 14583 mean train loss:  1.09888036e-02, bound:  3.15774888e-01\n",
      "Epoch: 14584 mean train loss:  1.09880306e-02, bound:  3.15774888e-01\n",
      "Epoch: 14585 mean train loss:  1.09873032e-02, bound:  3.15774858e-01\n",
      "Epoch: 14586 mean train loss:  1.09866280e-02, bound:  3.15774798e-01\n",
      "Epoch: 14587 mean train loss:  1.09857181e-02, bound:  3.15774769e-01\n",
      "Epoch: 14588 mean train loss:  1.09851211e-02, bound:  3.15774739e-01\n",
      "Epoch: 14589 mean train loss:  1.09844198e-02, bound:  3.15774679e-01\n",
      "Epoch: 14590 mean train loss:  1.09837130e-02, bound:  3.15774649e-01\n",
      "Epoch: 14591 mean train loss:  1.09829558e-02, bound:  3.15774620e-01\n",
      "Epoch: 14592 mean train loss:  1.09821782e-02, bound:  3.15774590e-01\n",
      "Epoch: 14593 mean train loss:  1.09815206e-02, bound:  3.15774530e-01\n",
      "Epoch: 14594 mean train loss:  1.09806797e-02, bound:  3.15774500e-01\n",
      "Epoch: 14595 mean train loss:  1.09800044e-02, bound:  3.15774471e-01\n",
      "Epoch: 14596 mean train loss:  1.09792752e-02, bound:  3.15774471e-01\n",
      "Epoch: 14597 mean train loss:  1.09786298e-02, bound:  3.15774411e-01\n",
      "Epoch: 14598 mean train loss:  1.09778186e-02, bound:  3.15774381e-01\n",
      "Epoch: 14599 mean train loss:  1.09771946e-02, bound:  3.15774322e-01\n",
      "Epoch: 14600 mean train loss:  1.09763350e-02, bound:  3.15774292e-01\n",
      "Epoch: 14601 mean train loss:  1.09756989e-02, bound:  3.15774292e-01\n",
      "Epoch: 14602 mean train loss:  1.09748645e-02, bound:  3.15774202e-01\n",
      "Epoch: 14603 mean train loss:  1.09742424e-02, bound:  3.15774173e-01\n",
      "Epoch: 14604 mean train loss:  1.09734815e-02, bound:  3.15774173e-01\n",
      "Epoch: 14605 mean train loss:  1.09727466e-02, bound:  3.15774113e-01\n",
      "Epoch: 14606 mean train loss:  1.09720128e-02, bound:  3.15774053e-01\n",
      "Epoch: 14607 mean train loss:  1.09712025e-02, bound:  3.15774053e-01\n",
      "Epoch: 14608 mean train loss:  1.09705059e-02, bound:  3.15773994e-01\n",
      "Epoch: 14609 mean train loss:  1.09697627e-02, bound:  3.15773964e-01\n",
      "Epoch: 14610 mean train loss:  1.09691508e-02, bound:  3.15773964e-01\n",
      "Epoch: 14611 mean train loss:  1.09683881e-02, bound:  3.15773934e-01\n",
      "Epoch: 14612 mean train loss:  1.09676132e-02, bound:  3.15773875e-01\n",
      "Epoch: 14613 mean train loss:  1.09668802e-02, bound:  3.15773845e-01\n",
      "Epoch: 14614 mean train loss:  1.09661333e-02, bound:  3.15773815e-01\n",
      "Epoch: 14615 mean train loss:  1.09654255e-02, bound:  3.15773755e-01\n",
      "Epoch: 14616 mean train loss:  1.09645966e-02, bound:  3.15773726e-01\n",
      "Epoch: 14617 mean train loss:  1.09639391e-02, bound:  3.15773666e-01\n",
      "Epoch: 14618 mean train loss:  1.09631242e-02, bound:  3.15773636e-01\n",
      "Epoch: 14619 mean train loss:  1.09624667e-02, bound:  3.15773606e-01\n",
      "Epoch: 14620 mean train loss:  1.09616937e-02, bound:  3.15773547e-01\n",
      "Epoch: 14621 mean train loss:  1.09609542e-02, bound:  3.15773547e-01\n",
      "Epoch: 14622 mean train loss:  1.09602110e-02, bound:  3.15773517e-01\n",
      "Epoch: 14623 mean train loss:  1.09595088e-02, bound:  3.15773487e-01\n",
      "Epoch: 14624 mean train loss:  1.09587749e-02, bound:  3.15773427e-01\n",
      "Epoch: 14625 mean train loss:  1.09580187e-02, bound:  3.15773427e-01\n",
      "Epoch: 14626 mean train loss:  1.09573686e-02, bound:  3.15773368e-01\n",
      "Epoch: 14627 mean train loss:  1.09566329e-02, bound:  3.15773338e-01\n",
      "Epoch: 14628 mean train loss:  1.09558152e-02, bound:  3.15773308e-01\n",
      "Epoch: 14629 mean train loss:  1.09551754e-02, bound:  3.15773278e-01\n",
      "Epoch: 14630 mean train loss:  1.09543372e-02, bound:  3.15773219e-01\n",
      "Epoch: 14631 mean train loss:  1.09535987e-02, bound:  3.15773189e-01\n",
      "Epoch: 14632 mean train loss:  1.09528322e-02, bound:  3.15773159e-01\n",
      "Epoch: 14633 mean train loss:  1.09521458e-02, bound:  3.15773100e-01\n",
      "Epoch: 14634 mean train loss:  1.09514827e-02, bound:  3.15773070e-01\n",
      "Epoch: 14635 mean train loss:  1.09506967e-02, bound:  3.15773040e-01\n",
      "Epoch: 14636 mean train loss:  1.09499134e-02, bound:  3.15773010e-01\n",
      "Epoch: 14637 mean train loss:  1.09491935e-02, bound:  3.15772951e-01\n",
      "Epoch: 14638 mean train loss:  1.09485239e-02, bound:  3.15772921e-01\n",
      "Epoch: 14639 mean train loss:  1.09477257e-02, bound:  3.15772891e-01\n",
      "Epoch: 14640 mean train loss:  1.09470710e-02, bound:  3.15772861e-01\n",
      "Epoch: 14641 mean train loss:  1.09462943e-02, bound:  3.15772831e-01\n",
      "Epoch: 14642 mean train loss:  1.09454365e-02, bound:  3.15772772e-01\n",
      "Epoch: 14643 mean train loss:  1.09447446e-02, bound:  3.15772742e-01\n",
      "Epoch: 14644 mean train loss:  1.09440712e-02, bound:  3.15772712e-01\n",
      "Epoch: 14645 mean train loss:  1.09432843e-02, bound:  3.15772653e-01\n",
      "Epoch: 14646 mean train loss:  1.09425550e-02, bound:  3.15772623e-01\n",
      "Epoch: 14647 mean train loss:  1.09417811e-02, bound:  3.15772623e-01\n",
      "Epoch: 14648 mean train loss:  1.09410826e-02, bound:  3.15772563e-01\n",
      "Epoch: 14649 mean train loss:  1.09403580e-02, bound:  3.15772533e-01\n",
      "Epoch: 14650 mean train loss:  1.09395254e-02, bound:  3.15772504e-01\n",
      "Epoch: 14651 mean train loss:  1.09388400e-02, bound:  3.15772474e-01\n",
      "Epoch: 14652 mean train loss:  1.09380707e-02, bound:  3.15772414e-01\n",
      "Epoch: 14653 mean train loss:  1.09373778e-02, bound:  3.15772384e-01\n",
      "Epoch: 14654 mean train loss:  1.09365471e-02, bound:  3.15772325e-01\n",
      "Epoch: 14655 mean train loss:  1.09359091e-02, bound:  3.15772325e-01\n",
      "Epoch: 14656 mean train loss:  1.09351631e-02, bound:  3.15772295e-01\n",
      "Epoch: 14657 mean train loss:  1.09343948e-02, bound:  3.15772265e-01\n",
      "Epoch: 14658 mean train loss:  1.09337214e-02, bound:  3.15772206e-01\n",
      "Epoch: 14659 mean train loss:  1.09328916e-02, bound:  3.15772176e-01\n",
      "Epoch: 14660 mean train loss:  1.09321373e-02, bound:  3.15772146e-01\n",
      "Epoch: 14661 mean train loss:  1.09314527e-02, bound:  3.15772086e-01\n",
      "Epoch: 14662 mean train loss:  1.09306136e-02, bound:  3.15772057e-01\n",
      "Epoch: 14663 mean train loss:  1.09299058e-02, bound:  3.15772057e-01\n",
      "Epoch: 14664 mean train loss:  1.09292213e-02, bound:  3.15771997e-01\n",
      "Epoch: 14665 mean train loss:  1.09284418e-02, bound:  3.15771937e-01\n",
      "Epoch: 14666 mean train loss:  1.09277004e-02, bound:  3.15771937e-01\n",
      "Epoch: 14667 mean train loss:  1.09269023e-02, bound:  3.15771878e-01\n",
      "Epoch: 14668 mean train loss:  1.09262438e-02, bound:  3.15771818e-01\n",
      "Epoch: 14669 mean train loss:  1.09254215e-02, bound:  3.15771818e-01\n",
      "Epoch: 14670 mean train loss:  1.09247211e-02, bound:  3.15771759e-01\n",
      "Epoch: 14671 mean train loss:  1.09239891e-02, bound:  3.15771729e-01\n",
      "Epoch: 14672 mean train loss:  1.09232264e-02, bound:  3.15771729e-01\n",
      "Epoch: 14673 mean train loss:  1.09225037e-02, bound:  3.15771639e-01\n",
      "Epoch: 14674 mean train loss:  1.09217940e-02, bound:  3.15771639e-01\n",
      "Epoch: 14675 mean train loss:  1.09209688e-02, bound:  3.15771610e-01\n",
      "Epoch: 14676 mean train loss:  1.09202918e-02, bound:  3.15771550e-01\n",
      "Epoch: 14677 mean train loss:  1.09195383e-02, bound:  3.15771520e-01\n",
      "Epoch: 14678 mean train loss:  1.09186852e-02, bound:  3.15771490e-01\n",
      "Epoch: 14679 mean train loss:  1.09180054e-02, bound:  3.15771431e-01\n",
      "Epoch: 14680 mean train loss:  1.09172678e-02, bound:  3.15771431e-01\n",
      "Epoch: 14681 mean train loss:  1.09164892e-02, bound:  3.15771371e-01\n",
      "Epoch: 14682 mean train loss:  1.09157907e-02, bound:  3.15771312e-01\n",
      "Epoch: 14683 mean train loss:  1.09149599e-02, bound:  3.15771312e-01\n",
      "Epoch: 14684 mean train loss:  1.09142186e-02, bound:  3.15771252e-01\n",
      "Epoch: 14685 mean train loss:  1.09135173e-02, bound:  3.15771222e-01\n",
      "Epoch: 14686 mean train loss:  1.09127192e-02, bound:  3.15771192e-01\n",
      "Epoch: 14687 mean train loss:  1.09120626e-02, bound:  3.15771163e-01\n",
      "Epoch: 14688 mean train loss:  1.09112579e-02, bound:  3.15771103e-01\n",
      "Epoch: 14689 mean train loss:  1.09105883e-02, bound:  3.15771073e-01\n",
      "Epoch: 14690 mean train loss:  1.09097306e-02, bound:  3.15771013e-01\n",
      "Epoch: 14691 mean train loss:  1.09090451e-02, bound:  3.15770984e-01\n",
      "Epoch: 14692 mean train loss:  1.09083420e-02, bound:  3.15770954e-01\n",
      "Epoch: 14693 mean train loss:  1.09075177e-02, bound:  3.15770954e-01\n",
      "Epoch: 14694 mean train loss:  1.09067690e-02, bound:  3.15770894e-01\n",
      "Epoch: 14695 mean train loss:  1.09060146e-02, bound:  3.15770864e-01\n",
      "Epoch: 14696 mean train loss:  1.09052574e-02, bound:  3.15770805e-01\n",
      "Epoch: 14697 mean train loss:  1.09045682e-02, bound:  3.15770805e-01\n",
      "Epoch: 14698 mean train loss:  1.09037440e-02, bound:  3.15770745e-01\n",
      "Epoch: 14699 mean train loss:  1.09030679e-02, bound:  3.15770715e-01\n",
      "Epoch: 14700 mean train loss:  1.09022353e-02, bound:  3.15770656e-01\n",
      "Epoch: 14701 mean train loss:  1.09015787e-02, bound:  3.15770626e-01\n",
      "Epoch: 14702 mean train loss:  1.09008253e-02, bound:  3.15770626e-01\n",
      "Epoch: 14703 mean train loss:  1.09000131e-02, bound:  3.15770566e-01\n",
      "Epoch: 14704 mean train loss:  1.08992429e-02, bound:  3.15770507e-01\n",
      "Epoch: 14705 mean train loss:  1.08985966e-02, bound:  3.15770507e-01\n",
      "Epoch: 14706 mean train loss:  1.08977770e-02, bound:  3.15770447e-01\n",
      "Epoch: 14707 mean train loss:  1.08970180e-02, bound:  3.15770388e-01\n",
      "Epoch: 14708 mean train loss:  1.08961649e-02, bound:  3.15770388e-01\n",
      "Epoch: 14709 mean train loss:  1.08955884e-02, bound:  3.15770328e-01\n",
      "Epoch: 14710 mean train loss:  1.08947791e-02, bound:  3.15770298e-01\n",
      "Epoch: 14711 mean train loss:  1.08939977e-02, bound:  3.15770298e-01\n",
      "Epoch: 14712 mean train loss:  1.08932750e-02, bound:  3.15770209e-01\n",
      "Epoch: 14713 mean train loss:  1.08925542e-02, bound:  3.15770209e-01\n",
      "Epoch: 14714 mean train loss:  1.08917095e-02, bound:  3.15770179e-01\n",
      "Epoch: 14715 mean train loss:  1.08910529e-02, bound:  3.15770149e-01\n",
      "Epoch: 14716 mean train loss:  1.08902538e-02, bound:  3.15770090e-01\n",
      "Epoch: 14717 mean train loss:  1.08895153e-02, bound:  3.15770060e-01\n",
      "Epoch: 14718 mean train loss:  1.08886864e-02, bound:  3.15770000e-01\n",
      "Epoch: 14719 mean train loss:  1.08880745e-02, bound:  3.15770000e-01\n",
      "Epoch: 14720 mean train loss:  1.08872447e-02, bound:  3.15769941e-01\n",
      "Epoch: 14721 mean train loss:  1.08864019e-02, bound:  3.15769881e-01\n",
      "Epoch: 14722 mean train loss:  1.08857201e-02, bound:  3.15769881e-01\n",
      "Epoch: 14723 mean train loss:  1.08849593e-02, bound:  3.15769821e-01\n",
      "Epoch: 14724 mean train loss:  1.08842710e-02, bound:  3.15769821e-01\n",
      "Epoch: 14725 mean train loss:  1.08834691e-02, bound:  3.15769762e-01\n",
      "Epoch: 14726 mean train loss:  1.08827325e-02, bound:  3.15769732e-01\n",
      "Epoch: 14727 mean train loss:  1.08819650e-02, bound:  3.15769672e-01\n",
      "Epoch: 14728 mean train loss:  1.08811567e-02, bound:  3.15769643e-01\n",
      "Epoch: 14729 mean train loss:  1.08804069e-02, bound:  3.15769613e-01\n",
      "Epoch: 14730 mean train loss:  1.08797057e-02, bound:  3.15769553e-01\n",
      "Epoch: 14731 mean train loss:  1.08788805e-02, bound:  3.15769523e-01\n",
      "Epoch: 14732 mean train loss:  1.08781336e-02, bound:  3.15769494e-01\n",
      "Epoch: 14733 mean train loss:  1.08774761e-02, bound:  3.15769494e-01\n",
      "Epoch: 14734 mean train loss:  1.08766751e-02, bound:  3.15769434e-01\n",
      "Epoch: 14735 mean train loss:  1.08758789e-02, bound:  3.15769404e-01\n",
      "Epoch: 14736 mean train loss:  1.08750481e-02, bound:  3.15769345e-01\n",
      "Epoch: 14737 mean train loss:  1.08743673e-02, bound:  3.15769315e-01\n",
      "Epoch: 14738 mean train loss:  1.08736819e-02, bound:  3.15769285e-01\n",
      "Epoch: 14739 mean train loss:  1.08729182e-02, bound:  3.15769255e-01\n",
      "Epoch: 14740 mean train loss:  1.08720977e-02, bound:  3.15769196e-01\n",
      "Epoch: 14741 mean train loss:  1.08713340e-02, bound:  3.15769196e-01\n",
      "Epoch: 14742 mean train loss:  1.08705899e-02, bound:  3.15769166e-01\n",
      "Epoch: 14743 mean train loss:  1.08697815e-02, bound:  3.15769076e-01\n",
      "Epoch: 14744 mean train loss:  1.08691091e-02, bound:  3.15769076e-01\n",
      "Epoch: 14745 mean train loss:  1.08682895e-02, bound:  3.15769017e-01\n",
      "Epoch: 14746 mean train loss:  1.08675398e-02, bound:  3.15768957e-01\n",
      "Epoch: 14747 mean train loss:  1.08667240e-02, bound:  3.15768957e-01\n",
      "Epoch: 14748 mean train loss:  1.08660245e-02, bound:  3.15768898e-01\n",
      "Epoch: 14749 mean train loss:  1.08652832e-02, bound:  3.15768868e-01\n",
      "Epoch: 14750 mean train loss:  1.08645121e-02, bound:  3.15768838e-01\n",
      "Epoch: 14751 mean train loss:  1.08638098e-02, bound:  3.15768778e-01\n",
      "Epoch: 14752 mean train loss:  1.08629307e-02, bound:  3.15768749e-01\n",
      "Epoch: 14753 mean train loss:  1.08622536e-02, bound:  3.15768719e-01\n",
      "Epoch: 14754 mean train loss:  1.08614443e-02, bound:  3.15768659e-01\n",
      "Epoch: 14755 mean train loss:  1.08606219e-02, bound:  3.15768659e-01\n",
      "Epoch: 14756 mean train loss:  1.08599421e-02, bound:  3.15768600e-01\n",
      "Epoch: 14757 mean train loss:  1.08591923e-02, bound:  3.15768570e-01\n",
      "Epoch: 14758 mean train loss:  1.08584100e-02, bound:  3.15768510e-01\n",
      "Epoch: 14759 mean train loss:  1.08576845e-02, bound:  3.15768510e-01\n",
      "Epoch: 14760 mean train loss:  1.08568845e-02, bound:  3.15768480e-01\n",
      "Epoch: 14761 mean train loss:  1.08560873e-02, bound:  3.15768391e-01\n",
      "Epoch: 14762 mean train loss:  1.08553208e-02, bound:  3.15768391e-01\n",
      "Epoch: 14763 mean train loss:  1.08545786e-02, bound:  3.15768331e-01\n",
      "Epoch: 14764 mean train loss:  1.08538354e-02, bound:  3.15768301e-01\n",
      "Epoch: 14765 mean train loss:  1.08530195e-02, bound:  3.15768272e-01\n",
      "Epoch: 14766 mean train loss:  1.08523481e-02, bound:  3.15768212e-01\n",
      "Epoch: 14767 mean train loss:  1.08515238e-02, bound:  3.15768182e-01\n",
      "Epoch: 14768 mean train loss:  1.08508049e-02, bound:  3.15768182e-01\n",
      "Epoch: 14769 mean train loss:  1.08500142e-02, bound:  3.15768093e-01\n",
      "Epoch: 14770 mean train loss:  1.08492654e-02, bound:  3.15768063e-01\n",
      "Epoch: 14771 mean train loss:  1.08485464e-02, bound:  3.15768063e-01\n",
      "Epoch: 14772 mean train loss:  1.08476905e-02, bound:  3.15768003e-01\n",
      "Epoch: 14773 mean train loss:  1.08469790e-02, bound:  3.15767974e-01\n",
      "Epoch: 14774 mean train loss:  1.08462162e-02, bound:  3.15767944e-01\n",
      "Epoch: 14775 mean train loss:  1.08454330e-02, bound:  3.15767884e-01\n",
      "Epoch: 14776 mean train loss:  1.08446665e-02, bound:  3.15767854e-01\n",
      "Epoch: 14777 mean train loss:  1.08439131e-02, bound:  3.15767825e-01\n",
      "Epoch: 14778 mean train loss:  1.08432481e-02, bound:  3.15767765e-01\n",
      "Epoch: 14779 mean train loss:  1.08423959e-02, bound:  3.15767765e-01\n",
      "Epoch: 14780 mean train loss:  1.08415568e-02, bound:  3.15767735e-01\n",
      "Epoch: 14781 mean train loss:  1.08408714e-02, bound:  3.15767646e-01\n",
      "Epoch: 14782 mean train loss:  1.08400816e-02, bound:  3.15767646e-01\n",
      "Epoch: 14783 mean train loss:  1.08393375e-02, bound:  3.15767616e-01\n",
      "Epoch: 14784 mean train loss:  1.08385319e-02, bound:  3.15767527e-01\n",
      "Epoch: 14785 mean train loss:  1.08377542e-02, bound:  3.15767527e-01\n",
      "Epoch: 14786 mean train loss:  1.08370809e-02, bound:  3.15767497e-01\n",
      "Epoch: 14787 mean train loss:  1.08362734e-02, bound:  3.15767437e-01\n",
      "Epoch: 14788 mean train loss:  1.08354222e-02, bound:  3.15767407e-01\n",
      "Epoch: 14789 mean train loss:  1.08347209e-02, bound:  3.15767348e-01\n",
      "Epoch: 14790 mean train loss:  1.08340038e-02, bound:  3.15767348e-01\n",
      "Epoch: 14791 mean train loss:  1.08331693e-02, bound:  3.15767288e-01\n",
      "Epoch: 14792 mean train loss:  1.08324261e-02, bound:  3.15767258e-01\n",
      "Epoch: 14793 mean train loss:  1.08316159e-02, bound:  3.15767229e-01\n",
      "Epoch: 14794 mean train loss:  1.08308950e-02, bound:  3.15767199e-01\n",
      "Epoch: 14795 mean train loss:  1.08300997e-02, bound:  3.15767169e-01\n",
      "Epoch: 14796 mean train loss:  1.08293118e-02, bound:  3.15767109e-01\n",
      "Epoch: 14797 mean train loss:  1.08286310e-02, bound:  3.15767080e-01\n",
      "Epoch: 14798 mean train loss:  1.08278347e-02, bound:  3.15767050e-01\n",
      "Epoch: 14799 mean train loss:  1.08270245e-02, bound:  3.15766990e-01\n",
      "Epoch: 14800 mean train loss:  1.08262580e-02, bound:  3.15766960e-01\n",
      "Epoch: 14801 mean train loss:  1.08254468e-02, bound:  3.15766931e-01\n",
      "Epoch: 14802 mean train loss:  1.08246943e-02, bound:  3.15766901e-01\n",
      "Epoch: 14803 mean train loss:  1.08239427e-02, bound:  3.15766871e-01\n",
      "Epoch: 14804 mean train loss:  1.08231343e-02, bound:  3.15766841e-01\n",
      "Epoch: 14805 mean train loss:  1.08223651e-02, bound:  3.15766782e-01\n",
      "Epoch: 14806 mean train loss:  1.08216694e-02, bound:  3.15766752e-01\n",
      "Epoch: 14807 mean train loss:  1.08209047e-02, bound:  3.15766722e-01\n",
      "Epoch: 14808 mean train loss:  1.08201085e-02, bound:  3.15766662e-01\n",
      "Epoch: 14809 mean train loss:  1.08193681e-02, bound:  3.15766633e-01\n",
      "Epoch: 14810 mean train loss:  1.08184898e-02, bound:  3.15766603e-01\n",
      "Epoch: 14811 mean train loss:  1.08177820e-02, bound:  3.15766543e-01\n",
      "Epoch: 14812 mean train loss:  1.08169215e-02, bound:  3.15766513e-01\n",
      "Epoch: 14813 mean train loss:  1.08161159e-02, bound:  3.15766484e-01\n",
      "Epoch: 14814 mean train loss:  1.08153755e-02, bound:  3.15766424e-01\n",
      "Epoch: 14815 mean train loss:  1.08146323e-02, bound:  3.15766394e-01\n",
      "Epoch: 14816 mean train loss:  1.08139487e-02, bound:  3.15766335e-01\n",
      "Epoch: 14817 mean train loss:  1.08132046e-02, bound:  3.15766335e-01\n",
      "Epoch: 14818 mean train loss:  1.08124400e-02, bound:  3.15766305e-01\n",
      "Epoch: 14819 mean train loss:  1.08114947e-02, bound:  3.15766215e-01\n",
      "Epoch: 14820 mean train loss:  1.08107561e-02, bound:  3.15766215e-01\n",
      "Epoch: 14821 mean train loss:  1.08099971e-02, bound:  3.15766186e-01\n",
      "Epoch: 14822 mean train loss:  1.08092865e-02, bound:  3.15766156e-01\n",
      "Epoch: 14823 mean train loss:  1.08085200e-02, bound:  3.15766096e-01\n",
      "Epoch: 14824 mean train loss:  1.08077535e-02, bound:  3.15766066e-01\n",
      "Epoch: 14825 mean train loss:  1.08068176e-02, bound:  3.15766007e-01\n",
      "Epoch: 14826 mean train loss:  1.08061349e-02, bound:  3.15765977e-01\n",
      "Epoch: 14827 mean train loss:  1.08054578e-02, bound:  3.15765947e-01\n",
      "Epoch: 14828 mean train loss:  1.08046047e-02, bound:  3.15765887e-01\n",
      "Epoch: 14829 mean train loss:  1.08038103e-02, bound:  3.15765858e-01\n",
      "Epoch: 14830 mean train loss:  1.08031491e-02, bound:  3.15765828e-01\n",
      "Epoch: 14831 mean train loss:  1.08022233e-02, bound:  3.15765768e-01\n",
      "Epoch: 14832 mean train loss:  1.08014308e-02, bound:  3.15765738e-01\n",
      "Epoch: 14833 mean train loss:  1.08006438e-02, bound:  3.15765738e-01\n",
      "Epoch: 14834 mean train loss:  1.07998606e-02, bound:  3.15765649e-01\n",
      "Epoch: 14835 mean train loss:  1.07990783e-02, bound:  3.15765619e-01\n",
      "Epoch: 14836 mean train loss:  1.07984953e-02, bound:  3.15765619e-01\n",
      "Epoch: 14837 mean train loss:  1.07976599e-02, bound:  3.15765530e-01\n",
      "Epoch: 14838 mean train loss:  1.07968254e-02, bound:  3.15765530e-01\n",
      "Epoch: 14839 mean train loss:  1.07960319e-02, bound:  3.15765500e-01\n",
      "Epoch: 14840 mean train loss:  1.07953642e-02, bound:  3.15765440e-01\n",
      "Epoch: 14841 mean train loss:  1.07944999e-02, bound:  3.15765411e-01\n",
      "Epoch: 14842 mean train loss:  1.07937353e-02, bound:  3.15765351e-01\n",
      "Epoch: 14843 mean train loss:  1.07929893e-02, bound:  3.15765321e-01\n",
      "Epoch: 14844 mean train loss:  1.07921166e-02, bound:  3.15765291e-01\n",
      "Epoch: 14845 mean train loss:  1.07913837e-02, bound:  3.15765232e-01\n",
      "Epoch: 14846 mean train loss:  1.07905576e-02, bound:  3.15765202e-01\n",
      "Epoch: 14847 mean train loss:  1.07897865e-02, bound:  3.15765172e-01\n",
      "Epoch: 14848 mean train loss:  1.07890833e-02, bound:  3.15765113e-01\n",
      "Epoch: 14849 mean train loss:  1.07883383e-02, bound:  3.15765083e-01\n",
      "Epoch: 14850 mean train loss:  1.07875224e-02, bound:  3.15765083e-01\n",
      "Epoch: 14851 mean train loss:  1.07866582e-02, bound:  3.15765023e-01\n",
      "Epoch: 14852 mean train loss:  1.07859178e-02, bound:  3.15764993e-01\n",
      "Epoch: 14853 mean train loss:  1.07851289e-02, bound:  3.15764964e-01\n",
      "Epoch: 14854 mean train loss:  1.07843447e-02, bound:  3.15764904e-01\n",
      "Epoch: 14855 mean train loss:  1.07835671e-02, bound:  3.15764874e-01\n",
      "Epoch: 14856 mean train loss:  1.07827047e-02, bound:  3.15764844e-01\n",
      "Epoch: 14857 mean train loss:  1.07820360e-02, bound:  3.15764815e-01\n",
      "Epoch: 14858 mean train loss:  1.07812500e-02, bound:  3.15764755e-01\n",
      "Epoch: 14859 mean train loss:  1.07804313e-02, bound:  3.15764725e-01\n",
      "Epoch: 14860 mean train loss:  1.07796248e-02, bound:  3.15764666e-01\n",
      "Epoch: 14861 mean train loss:  1.07787950e-02, bound:  3.15764636e-01\n",
      "Epoch: 14862 mean train loss:  1.07781496e-02, bound:  3.15764606e-01\n",
      "Epoch: 14863 mean train loss:  1.07773552e-02, bound:  3.15764576e-01\n",
      "Epoch: 14864 mean train loss:  1.07765198e-02, bound:  3.15764517e-01\n",
      "Epoch: 14865 mean train loss:  1.07757272e-02, bound:  3.15764517e-01\n",
      "Epoch: 14866 mean train loss:  1.07749365e-02, bound:  3.15764457e-01\n",
      "Epoch: 14867 mean train loss:  1.07742352e-02, bound:  3.15764397e-01\n",
      "Epoch: 14868 mean train loss:  1.07734725e-02, bound:  3.15764397e-01\n",
      "Epoch: 14869 mean train loss:  1.07725710e-02, bound:  3.15764338e-01\n",
      "Epoch: 14870 mean train loss:  1.07718268e-02, bound:  3.15764308e-01\n",
      "Epoch: 14871 mean train loss:  1.07710594e-02, bound:  3.15764278e-01\n",
      "Epoch: 14872 mean train loss:  1.07702287e-02, bound:  3.15764219e-01\n",
      "Epoch: 14873 mean train loss:  1.07693514e-02, bound:  3.15764189e-01\n",
      "Epoch: 14874 mean train loss:  1.07686650e-02, bound:  3.15764159e-01\n",
      "Epoch: 14875 mean train loss:  1.07679144e-02, bound:  3.15764099e-01\n",
      "Epoch: 14876 mean train loss:  1.07671032e-02, bound:  3.15764070e-01\n",
      "Epoch: 14877 mean train loss:  1.07662883e-02, bound:  3.15764010e-01\n",
      "Epoch: 14878 mean train loss:  1.07655264e-02, bound:  3.15764010e-01\n",
      "Epoch: 14879 mean train loss:  1.07647525e-02, bound:  3.15763950e-01\n",
      "Epoch: 14880 mean train loss:  1.07639981e-02, bound:  3.15763891e-01\n",
      "Epoch: 14881 mean train loss:  1.07631227e-02, bound:  3.15763891e-01\n",
      "Epoch: 14882 mean train loss:  1.07624074e-02, bound:  3.15763831e-01\n",
      "Epoch: 14883 mean train loss:  1.07615190e-02, bound:  3.15763772e-01\n",
      "Epoch: 14884 mean train loss:  1.07608363e-02, bound:  3.15763772e-01\n",
      "Epoch: 14885 mean train loss:  1.07599553e-02, bound:  3.15763742e-01\n",
      "Epoch: 14886 mean train loss:  1.07592680e-02, bound:  3.15763682e-01\n",
      "Epoch: 14887 mean train loss:  1.07585276e-02, bound:  3.15763652e-01\n",
      "Epoch: 14888 mean train loss:  1.07577192e-02, bound:  3.15763623e-01\n",
      "Epoch: 14889 mean train loss:  1.07568437e-02, bound:  3.15763563e-01\n",
      "Epoch: 14890 mean train loss:  1.07560735e-02, bound:  3.15763533e-01\n",
      "Epoch: 14891 mean train loss:  1.07552204e-02, bound:  3.15763503e-01\n",
      "Epoch: 14892 mean train loss:  1.07545182e-02, bound:  3.15763444e-01\n",
      "Epoch: 14893 mean train loss:  1.07536186e-02, bound:  3.15763414e-01\n",
      "Epoch: 14894 mean train loss:  1.07529787e-02, bound:  3.15763384e-01\n",
      "Epoch: 14895 mean train loss:  1.07521117e-02, bound:  3.15763324e-01\n",
      "Epoch: 14896 mean train loss:  1.07513675e-02, bound:  3.15763295e-01\n",
      "Epoch: 14897 mean train loss:  1.07505787e-02, bound:  3.15763265e-01\n",
      "Epoch: 14898 mean train loss:  1.07497470e-02, bound:  3.15763205e-01\n",
      "Epoch: 14899 mean train loss:  1.07489424e-02, bound:  3.15763175e-01\n",
      "Epoch: 14900 mean train loss:  1.07482523e-02, bound:  3.15763175e-01\n",
      "Epoch: 14901 mean train loss:  1.07473079e-02, bound:  3.15763086e-01\n",
      "Epoch: 14902 mean train loss:  1.07465116e-02, bound:  3.15763086e-01\n",
      "Epoch: 14903 mean train loss:  1.07458271e-02, bound:  3.15763026e-01\n",
      "Epoch: 14904 mean train loss:  1.07450383e-02, bound:  3.15762967e-01\n",
      "Epoch: 14905 mean train loss:  1.07442141e-02, bound:  3.15762937e-01\n",
      "Epoch: 14906 mean train loss:  1.07433954e-02, bound:  3.15762907e-01\n",
      "Epoch: 14907 mean train loss:  1.07425163e-02, bound:  3.15762877e-01\n",
      "Epoch: 14908 mean train loss:  1.07418709e-02, bound:  3.15762818e-01\n",
      "Epoch: 14909 mean train loss:  1.07411109e-02, bound:  3.15762788e-01\n",
      "Epoch: 14910 mean train loss:  1.07402867e-02, bound:  3.15762758e-01\n",
      "Epoch: 14911 mean train loss:  1.07394550e-02, bound:  3.15762728e-01\n",
      "Epoch: 14912 mean train loss:  1.07386457e-02, bound:  3.15762669e-01\n",
      "Epoch: 14913 mean train loss:  1.07378885e-02, bound:  3.15762639e-01\n",
      "Epoch: 14914 mean train loss:  1.07371230e-02, bound:  3.15762609e-01\n",
      "Epoch: 14915 mean train loss:  1.07363891e-02, bound:  3.15762579e-01\n",
      "Epoch: 14916 mean train loss:  1.07356077e-02, bound:  3.15762520e-01\n",
      "Epoch: 14917 mean train loss:  1.07347220e-02, bound:  3.15762490e-01\n",
      "Epoch: 14918 mean train loss:  1.07339183e-02, bound:  3.15762460e-01\n",
      "Epoch: 14919 mean train loss:  1.07331146e-02, bound:  3.15762401e-01\n",
      "Epoch: 14920 mean train loss:  1.07323574e-02, bound:  3.15762341e-01\n",
      "Epoch: 14921 mean train loss:  1.07315825e-02, bound:  3.15762311e-01\n",
      "Epoch: 14922 mean train loss:  1.07307676e-02, bound:  3.15762311e-01\n",
      "Epoch: 14923 mean train loss:  1.07299704e-02, bound:  3.15762222e-01\n",
      "Epoch: 14924 mean train loss:  1.07291648e-02, bound:  3.15762192e-01\n",
      "Epoch: 14925 mean train loss:  1.07282940e-02, bound:  3.15762192e-01\n",
      "Epoch: 14926 mean train loss:  1.07275182e-02, bound:  3.15762103e-01\n",
      "Epoch: 14927 mean train loss:  1.07268086e-02, bound:  3.15762073e-01\n",
      "Epoch: 14928 mean train loss:  1.07258987e-02, bound:  3.15762073e-01\n",
      "Epoch: 14929 mean train loss:  1.07251229e-02, bound:  3.15762013e-01\n",
      "Epoch: 14930 mean train loss:  1.07244002e-02, bound:  3.15761954e-01\n",
      "Epoch: 14931 mean train loss:  1.07235843e-02, bound:  3.15761954e-01\n",
      "Epoch: 14932 mean train loss:  1.07228719e-02, bound:  3.15761894e-01\n",
      "Epoch: 14933 mean train loss:  1.07219722e-02, bound:  3.15761864e-01\n",
      "Epoch: 14934 mean train loss:  1.07211936e-02, bound:  3.15761834e-01\n",
      "Epoch: 14935 mean train loss:  1.07203359e-02, bound:  3.15761775e-01\n",
      "Epoch: 14936 mean train loss:  1.07197352e-02, bound:  3.15761745e-01\n",
      "Epoch: 14937 mean train loss:  1.07187964e-02, bound:  3.15761685e-01\n",
      "Epoch: 14938 mean train loss:  1.07180588e-02, bound:  3.15761656e-01\n",
      "Epoch: 14939 mean train loss:  1.07172085e-02, bound:  3.15761626e-01\n",
      "Epoch: 14940 mean train loss:  1.07165333e-02, bound:  3.15761566e-01\n",
      "Epoch: 14941 mean train loss:  1.07155209e-02, bound:  3.15761536e-01\n",
      "Epoch: 14942 mean train loss:  1.07148476e-02, bound:  3.15761507e-01\n",
      "Epoch: 14943 mean train loss:  1.07139386e-02, bound:  3.15761447e-01\n",
      "Epoch: 14944 mean train loss:  1.07132383e-02, bound:  3.15761447e-01\n",
      "Epoch: 14945 mean train loss:  1.07123945e-02, bound:  3.15761358e-01\n",
      "Epoch: 14946 mean train loss:  1.07115852e-02, bound:  3.15761328e-01\n",
      "Epoch: 14947 mean train loss:  1.07107908e-02, bound:  3.15761298e-01\n",
      "Epoch: 14948 mean train loss:  1.07100541e-02, bound:  3.15761238e-01\n",
      "Epoch: 14949 mean train loss:  1.07091833e-02, bound:  3.15761209e-01\n",
      "Epoch: 14950 mean train loss:  1.07083600e-02, bound:  3.15761209e-01\n",
      "Epoch: 14951 mean train loss:  1.07075674e-02, bound:  3.15761179e-01\n",
      "Epoch: 14952 mean train loss:  1.07068047e-02, bound:  3.15761089e-01\n",
      "Epoch: 14953 mean train loss:  1.07059646e-02, bound:  3.15761060e-01\n",
      "Epoch: 14954 mean train loss:  1.07052289e-02, bound:  3.15761030e-01\n",
      "Epoch: 14955 mean train loss:  1.07044168e-02, bound:  3.15760970e-01\n",
      "Epoch: 14956 mean train loss:  1.07036419e-02, bound:  3.15760940e-01\n",
      "Epoch: 14957 mean train loss:  1.07028075e-02, bound:  3.15760911e-01\n",
      "Epoch: 14958 mean train loss:  1.07019795e-02, bound:  3.15760881e-01\n",
      "Epoch: 14959 mean train loss:  1.07012074e-02, bound:  3.15760851e-01\n",
      "Epoch: 14960 mean train loss:  1.07004559e-02, bound:  3.15760791e-01\n",
      "Epoch: 14961 mean train loss:  1.06996531e-02, bound:  3.15760761e-01\n",
      "Epoch: 14962 mean train loss:  1.06988670e-02, bound:  3.15760732e-01\n",
      "Epoch: 14963 mean train loss:  1.06979804e-02, bound:  3.15760672e-01\n",
      "Epoch: 14964 mean train loss:  1.06971860e-02, bound:  3.15760642e-01\n",
      "Epoch: 14965 mean train loss:  1.06962873e-02, bound:  3.15760612e-01\n",
      "Epoch: 14966 mean train loss:  1.06955217e-02, bound:  3.15760553e-01\n",
      "Epoch: 14967 mean train loss:  1.06947469e-02, bound:  3.15760523e-01\n",
      "Epoch: 14968 mean train loss:  1.06940176e-02, bound:  3.15760463e-01\n",
      "Epoch: 14969 mean train loss:  1.06931794e-02, bound:  3.15760434e-01\n",
      "Epoch: 14970 mean train loss:  1.06924176e-02, bound:  3.15760404e-01\n",
      "Epoch: 14971 mean train loss:  1.06915301e-02, bound:  3.15760344e-01\n",
      "Epoch: 14972 mean train loss:  1.06908083e-02, bound:  3.15760314e-01\n",
      "Epoch: 14973 mean train loss:  1.06899394e-02, bound:  3.15760285e-01\n",
      "Epoch: 14974 mean train loss:  1.06891198e-02, bound:  3.15760225e-01\n",
      "Epoch: 14975 mean train loss:  1.06883245e-02, bound:  3.15760195e-01\n",
      "Epoch: 14976 mean train loss:  1.06876241e-02, bound:  3.15760165e-01\n",
      "Epoch: 14977 mean train loss:  1.06866667e-02, bound:  3.15760106e-01\n",
      "Epoch: 14978 mean train loss:  1.06858416e-02, bound:  3.15760076e-01\n",
      "Epoch: 14979 mean train loss:  1.06850583e-02, bound:  3.15760016e-01\n",
      "Epoch: 14980 mean train loss:  1.06842695e-02, bound:  3.15760016e-01\n",
      "Epoch: 14981 mean train loss:  1.06834937e-02, bound:  3.15759957e-01\n",
      "Epoch: 14982 mean train loss:  1.06826611e-02, bound:  3.15759957e-01\n",
      "Epoch: 14983 mean train loss:  1.06818415e-02, bound:  3.15759897e-01\n",
      "Epoch: 14984 mean train loss:  1.06810266e-02, bound:  3.15759838e-01\n",
      "Epoch: 14985 mean train loss:  1.06803244e-02, bound:  3.15759778e-01\n",
      "Epoch: 14986 mean train loss:  1.06794909e-02, bound:  3.15759778e-01\n",
      "Epoch: 14987 mean train loss:  1.06786797e-02, bound:  3.15759689e-01\n",
      "Epoch: 14988 mean train loss:  1.06779421e-02, bound:  3.15759659e-01\n",
      "Epoch: 14989 mean train loss:  1.06770145e-02, bound:  3.15759629e-01\n",
      "Epoch: 14990 mean train loss:  1.06762014e-02, bound:  3.15759569e-01\n",
      "Epoch: 14991 mean train loss:  1.06753213e-02, bound:  3.15759540e-01\n",
      "Epoch: 14992 mean train loss:  1.06746433e-02, bound:  3.15759510e-01\n",
      "Epoch: 14993 mean train loss:  1.06738312e-02, bound:  3.15759450e-01\n",
      "Epoch: 14994 mean train loss:  1.06729511e-02, bound:  3.15759420e-01\n",
      "Epoch: 14995 mean train loss:  1.06721884e-02, bound:  3.15759391e-01\n",
      "Epoch: 14996 mean train loss:  1.06714424e-02, bound:  3.15759331e-01\n",
      "Epoch: 14997 mean train loss:  1.06706349e-02, bound:  3.15759331e-01\n",
      "Epoch: 14998 mean train loss:  1.06698098e-02, bound:  3.15759271e-01\n",
      "Epoch: 14999 mean train loss:  1.06688766e-02, bound:  3.15759212e-01\n",
      "Epoch: 15000 mean train loss:  1.06681837e-02, bound:  3.15759212e-01\n",
      "Epoch: 15001 mean train loss:  1.06672952e-02, bound:  3.15759182e-01\n",
      "Epoch: 15002 mean train loss:  1.06665269e-02, bound:  3.15759093e-01\n",
      "Epoch: 15003 mean train loss:  1.06657464e-02, bound:  3.15759093e-01\n",
      "Epoch: 15004 mean train loss:  1.06650228e-02, bound:  3.15759033e-01\n",
      "Epoch: 15005 mean train loss:  1.06640644e-02, bound:  3.15759003e-01\n",
      "Epoch: 15006 mean train loss:  1.06632225e-02, bound:  3.15758973e-01\n",
      "Epoch: 15007 mean train loss:  1.06624318e-02, bound:  3.15758914e-01\n",
      "Epoch: 15008 mean train loss:  1.06617436e-02, bound:  3.15758884e-01\n",
      "Epoch: 15009 mean train loss:  1.06608504e-02, bound:  3.15758854e-01\n",
      "Epoch: 15010 mean train loss:  1.06600737e-02, bound:  3.15758795e-01\n",
      "Epoch: 15011 mean train loss:  1.06591349e-02, bound:  3.15758765e-01\n",
      "Epoch: 15012 mean train loss:  1.06584709e-02, bound:  3.15758675e-01\n",
      "Epoch: 15013 mean train loss:  1.06575098e-02, bound:  3.15758646e-01\n",
      "Epoch: 15014 mean train loss:  1.06567424e-02, bound:  3.15758616e-01\n",
      "Epoch: 15015 mean train loss:  1.06559768e-02, bound:  3.15758586e-01\n",
      "Epoch: 15016 mean train loss:  1.06552541e-02, bound:  3.15758556e-01\n",
      "Epoch: 15017 mean train loss:  1.06543917e-02, bound:  3.15758526e-01\n",
      "Epoch: 15018 mean train loss:  1.06534949e-02, bound:  3.15758467e-01\n",
      "Epoch: 15019 mean train loss:  1.06526408e-02, bound:  3.15758407e-01\n",
      "Epoch: 15020 mean train loss:  1.06519973e-02, bound:  3.15758407e-01\n",
      "Epoch: 15021 mean train loss:  1.06510734e-02, bound:  3.15758348e-01\n",
      "Epoch: 15022 mean train loss:  1.06502576e-02, bound:  3.15758318e-01\n",
      "Epoch: 15023 mean train loss:  1.06494902e-02, bound:  3.15758288e-01\n",
      "Epoch: 15024 mean train loss:  1.06487563e-02, bound:  3.15758228e-01\n",
      "Epoch: 15025 mean train loss:  1.06478697e-02, bound:  3.15758198e-01\n",
      "Epoch: 15026 mean train loss:  1.06470594e-02, bound:  3.15758169e-01\n",
      "Epoch: 15027 mean train loss:  1.06462305e-02, bound:  3.15758109e-01\n",
      "Epoch: 15028 mean train loss:  1.06455451e-02, bound:  3.15758079e-01\n",
      "Epoch: 15029 mean train loss:  1.06445737e-02, bound:  3.15758020e-01\n",
      "Epoch: 15030 mean train loss:  1.06438268e-02, bound:  3.15757990e-01\n",
      "Epoch: 15031 mean train loss:  1.06430044e-02, bound:  3.15757960e-01\n",
      "Epoch: 15032 mean train loss:  1.06422370e-02, bound:  3.15757900e-01\n",
      "Epoch: 15033 mean train loss:  1.06414016e-02, bound:  3.15757900e-01\n",
      "Epoch: 15034 mean train loss:  1.06405383e-02, bound:  3.15757841e-01\n",
      "Epoch: 15035 mean train loss:  1.06397234e-02, bound:  3.15757781e-01\n",
      "Epoch: 15036 mean train loss:  1.06389066e-02, bound:  3.15757781e-01\n",
      "Epoch: 15037 mean train loss:  1.06380312e-02, bound:  3.15757751e-01\n",
      "Epoch: 15038 mean train loss:  1.06373224e-02, bound:  3.15757662e-01\n",
      "Epoch: 15039 mean train loss:  1.06364610e-02, bound:  3.15757632e-01\n",
      "Epoch: 15040 mean train loss:  1.06356181e-02, bound:  3.15757573e-01\n",
      "Epoch: 15041 mean train loss:  1.06347660e-02, bound:  3.15757573e-01\n",
      "Epoch: 15042 mean train loss:  1.06339538e-02, bound:  3.15757543e-01\n",
      "Epoch: 15043 mean train loss:  1.06331324e-02, bound:  3.15757453e-01\n",
      "Epoch: 15044 mean train loss:  1.06323920e-02, bound:  3.15757424e-01\n",
      "Epoch: 15045 mean train loss:  1.06315147e-02, bound:  3.15757394e-01\n",
      "Epoch: 15046 mean train loss:  1.06307408e-02, bound:  3.15757334e-01\n",
      "Epoch: 15047 mean train loss:  1.06299007e-02, bound:  3.15757304e-01\n",
      "Epoch: 15048 mean train loss:  1.06291091e-02, bound:  3.15757275e-01\n",
      "Epoch: 15049 mean train loss:  1.06282793e-02, bound:  3.15757215e-01\n",
      "Epoch: 15050 mean train loss:  1.06274821e-02, bound:  3.15757185e-01\n",
      "Epoch: 15051 mean train loss:  1.06265834e-02, bound:  3.15757185e-01\n",
      "Epoch: 15052 mean train loss:  1.06257834e-02, bound:  3.15757096e-01\n",
      "Epoch: 15053 mean train loss:  1.06249312e-02, bound:  3.15757096e-01\n",
      "Epoch: 15054 mean train loss:  1.06241275e-02, bound:  3.15757066e-01\n",
      "Epoch: 15055 mean train loss:  1.06232977e-02, bound:  3.15756977e-01\n",
      "Epoch: 15056 mean train loss:  1.06225815e-02, bound:  3.15756977e-01\n",
      "Epoch: 15057 mean train loss:  1.06217293e-02, bound:  3.15756917e-01\n",
      "Epoch: 15058 mean train loss:  1.06208716e-02, bound:  3.15756857e-01\n",
      "Epoch: 15059 mean train loss:  1.06200250e-02, bound:  3.15756857e-01\n",
      "Epoch: 15060 mean train loss:  1.06192753e-02, bound:  3.15756798e-01\n",
      "Epoch: 15061 mean train loss:  1.06184110e-02, bound:  3.15756768e-01\n",
      "Epoch: 15062 mean train loss:  1.06175533e-02, bound:  3.15756679e-01\n",
      "Epoch: 15063 mean train loss:  1.06168091e-02, bound:  3.15756649e-01\n",
      "Epoch: 15064 mean train loss:  1.06159709e-02, bound:  3.15756619e-01\n",
      "Epoch: 15065 mean train loss:  1.06151216e-02, bound:  3.15756559e-01\n",
      "Epoch: 15066 mean train loss:  1.06142992e-02, bound:  3.15756530e-01\n",
      "Epoch: 15067 mean train loss:  1.06134843e-02, bound:  3.15756500e-01\n",
      "Epoch: 15068 mean train loss:  1.06126973e-02, bound:  3.15756470e-01\n",
      "Epoch: 15069 mean train loss:  1.06117642e-02, bound:  3.15756410e-01\n",
      "Epoch: 15070 mean train loss:  1.06110806e-02, bound:  3.15756381e-01\n",
      "Epoch: 15071 mean train loss:  1.06102070e-02, bound:  3.15756351e-01\n",
      "Epoch: 15072 mean train loss:  1.06093632e-02, bound:  3.15756291e-01\n",
      "Epoch: 15073 mean train loss:  1.06085362e-02, bound:  3.15756291e-01\n",
      "Epoch: 15074 mean train loss:  1.06077399e-02, bound:  3.15756232e-01\n",
      "Epoch: 15075 mean train loss:  1.06068514e-02, bound:  3.15756202e-01\n",
      "Epoch: 15076 mean train loss:  1.06060579e-02, bound:  3.15756172e-01\n",
      "Epoch: 15077 mean train loss:  1.06053380e-02, bound:  3.15756112e-01\n",
      "Epoch: 15078 mean train loss:  1.06043965e-02, bound:  3.15756083e-01\n",
      "Epoch: 15079 mean train loss:  1.06035322e-02, bound:  3.15756053e-01\n",
      "Epoch: 15080 mean train loss:  1.06027946e-02, bound:  3.15755963e-01\n",
      "Epoch: 15081 mean train loss:  1.06019527e-02, bound:  3.15755904e-01\n",
      "Epoch: 15082 mean train loss:  1.06011014e-02, bound:  3.15755874e-01\n",
      "Epoch: 15083 mean train loss:  1.06002735e-02, bound:  3.15755844e-01\n",
      "Epoch: 15084 mean train loss:  1.05995070e-02, bound:  3.15755785e-01\n",
      "Epoch: 15085 mean train loss:  1.05986400e-02, bound:  3.15755755e-01\n",
      "Epoch: 15086 mean train loss:  1.05978064e-02, bound:  3.15755725e-01\n",
      "Epoch: 15087 mean train loss:  1.05969589e-02, bound:  3.15755665e-01\n",
      "Epoch: 15088 mean train loss:  1.05961394e-02, bound:  3.15755665e-01\n",
      "Epoch: 15089 mean train loss:  1.05952900e-02, bound:  3.15755635e-01\n",
      "Epoch: 15090 mean train loss:  1.05945347e-02, bound:  3.15755546e-01\n",
      "Epoch: 15091 mean train loss:  1.05936565e-02, bound:  3.15755546e-01\n",
      "Epoch: 15092 mean train loss:  1.05928890e-02, bound:  3.15755516e-01\n",
      "Epoch: 15093 mean train loss:  1.05920713e-02, bound:  3.15755427e-01\n",
      "Epoch: 15094 mean train loss:  1.05912415e-02, bound:  3.15755427e-01\n",
      "Epoch: 15095 mean train loss:  1.05904052e-02, bound:  3.15755397e-01\n",
      "Epoch: 15096 mean train loss:  1.05895652e-02, bound:  3.15755337e-01\n",
      "Epoch: 15097 mean train loss:  1.05887009e-02, bound:  3.15755278e-01\n",
      "Epoch: 15098 mean train loss:  1.05878850e-02, bound:  3.15755218e-01\n",
      "Epoch: 15099 mean train loss:  1.05871344e-02, bound:  3.15755188e-01\n",
      "Epoch: 15100 mean train loss:  1.05861975e-02, bound:  3.15755129e-01\n",
      "Epoch: 15101 mean train loss:  1.05853919e-02, bound:  3.15755099e-01\n",
      "Epoch: 15102 mean train loss:  1.05845677e-02, bound:  3.15755069e-01\n",
      "Epoch: 15103 mean train loss:  1.05837267e-02, bound:  3.15755069e-01\n",
      "Epoch: 15104 mean train loss:  1.05829304e-02, bound:  3.15754980e-01\n",
      "Epoch: 15105 mean train loss:  1.05820922e-02, bound:  3.15754950e-01\n",
      "Epoch: 15106 mean train loss:  1.05813034e-02, bound:  3.15754920e-01\n",
      "Epoch: 15107 mean train loss:  1.05803376e-02, bound:  3.15754861e-01\n",
      "Epoch: 15108 mean train loss:  1.05795907e-02, bound:  3.15754831e-01\n",
      "Epoch: 15109 mean train loss:  1.05787516e-02, bound:  3.15754771e-01\n",
      "Epoch: 15110 mean train loss:  1.05778845e-02, bound:  3.15754741e-01\n",
      "Epoch: 15111 mean train loss:  1.05770798e-02, bound:  3.15754682e-01\n",
      "Epoch: 15112 mean train loss:  1.05762184e-02, bound:  3.15754652e-01\n",
      "Epoch: 15113 mean train loss:  1.05754212e-02, bound:  3.15754622e-01\n",
      "Epoch: 15114 mean train loss:  1.05745597e-02, bound:  3.15754563e-01\n",
      "Epoch: 15115 mean train loss:  1.05737923e-02, bound:  3.15754533e-01\n",
      "Epoch: 15116 mean train loss:  1.05728777e-02, bound:  3.15754503e-01\n",
      "Epoch: 15117 mean train loss:  1.05720777e-02, bound:  3.15754443e-01\n",
      "Epoch: 15118 mean train loss:  1.05712973e-02, bound:  3.15754414e-01\n",
      "Epoch: 15119 mean train loss:  1.05704162e-02, bound:  3.15754384e-01\n",
      "Epoch: 15120 mean train loss:  1.05695976e-02, bound:  3.15754324e-01\n",
      "Epoch: 15121 mean train loss:  1.05686486e-02, bound:  3.15754294e-01\n",
      "Epoch: 15122 mean train loss:  1.05680106e-02, bound:  3.15754235e-01\n",
      "Epoch: 15123 mean train loss:  1.05669955e-02, bound:  3.15754205e-01\n",
      "Epoch: 15124 mean train loss:  1.05662113e-02, bound:  3.15754175e-01\n",
      "Epoch: 15125 mean train loss:  1.05654672e-02, bound:  3.15754116e-01\n",
      "Epoch: 15126 mean train loss:  1.05646458e-02, bound:  3.15754086e-01\n",
      "Epoch: 15127 mean train loss:  1.05637386e-02, bound:  3.15753996e-01\n",
      "Epoch: 15128 mean train loss:  1.05628846e-02, bound:  3.15753996e-01\n",
      "Epoch: 15129 mean train loss:  1.05621116e-02, bound:  3.15753967e-01\n",
      "Epoch: 15130 mean train loss:  1.05612241e-02, bound:  3.15753907e-01\n",
      "Epoch: 15131 mean train loss:  1.05603710e-02, bound:  3.15753877e-01\n",
      "Epoch: 15132 mean train loss:  1.05595309e-02, bound:  3.15753847e-01\n",
      "Epoch: 15133 mean train loss:  1.05586676e-02, bound:  3.15753788e-01\n",
      "Epoch: 15134 mean train loss:  1.05579561e-02, bound:  3.15753758e-01\n",
      "Epoch: 15135 mean train loss:  1.05569754e-02, bound:  3.15753728e-01\n",
      "Epoch: 15136 mean train loss:  1.05562098e-02, bound:  3.15753669e-01\n",
      "Epoch: 15137 mean train loss:  1.05553614e-02, bound:  3.15753639e-01\n",
      "Epoch: 15138 mean train loss:  1.05545241e-02, bound:  3.15753549e-01\n",
      "Epoch: 15139 mean train loss:  1.05536506e-02, bound:  3.15753549e-01\n",
      "Epoch: 15140 mean train loss:  1.05527956e-02, bound:  3.15753520e-01\n",
      "Epoch: 15141 mean train loss:  1.05520338e-02, bound:  3.15753430e-01\n",
      "Epoch: 15142 mean train loss:  1.05510913e-02, bound:  3.15753430e-01\n",
      "Epoch: 15143 mean train loss:  1.05502550e-02, bound:  3.15753400e-01\n",
      "Epoch: 15144 mean train loss:  1.05495360e-02, bound:  3.15753341e-01\n",
      "Epoch: 15145 mean train loss:  1.05486885e-02, bound:  3.15753311e-01\n",
      "Epoch: 15146 mean train loss:  1.05477301e-02, bound:  3.15753222e-01\n",
      "Epoch: 15147 mean train loss:  1.05469525e-02, bound:  3.15753222e-01\n",
      "Epoch: 15148 mean train loss:  1.05461385e-02, bound:  3.15753162e-01\n",
      "Epoch: 15149 mean train loss:  1.05452873e-02, bound:  3.15753102e-01\n",
      "Epoch: 15150 mean train loss:  1.05444463e-02, bound:  3.15753073e-01\n",
      "Epoch: 15151 mean train loss:  1.05435429e-02, bound:  3.15753013e-01\n",
      "Epoch: 15152 mean train loss:  1.05428044e-02, bound:  3.15752983e-01\n",
      "Epoch: 15153 mean train loss:  1.05419476e-02, bound:  3.15752953e-01\n",
      "Epoch: 15154 mean train loss:  1.05410749e-02, bound:  3.15752894e-01\n",
      "Epoch: 15155 mean train loss:  1.05402516e-02, bound:  3.15752864e-01\n",
      "Epoch: 15156 mean train loss:  1.05394330e-02, bound:  3.15752834e-01\n",
      "Epoch: 15157 mean train loss:  1.05385548e-02, bound:  3.15752804e-01\n",
      "Epoch: 15158 mean train loss:  1.05376756e-02, bound:  3.15752774e-01\n",
      "Epoch: 15159 mean train loss:  1.05368569e-02, bound:  3.15752715e-01\n",
      "Epoch: 15160 mean train loss:  1.05360122e-02, bound:  3.15752685e-01\n",
      "Epoch: 15161 mean train loss:  1.05351703e-02, bound:  3.15752625e-01\n",
      "Epoch: 15162 mean train loss:  1.05343945e-02, bound:  3.15752566e-01\n",
      "Epoch: 15163 mean train loss:  1.05334921e-02, bound:  3.15752536e-01\n",
      "Epoch: 15164 mean train loss:  1.05326362e-02, bound:  3.15752506e-01\n",
      "Epoch: 15165 mean train loss:  1.05318027e-02, bound:  3.15752447e-01\n",
      "Epoch: 15166 mean train loss:  1.05310092e-02, bound:  3.15752417e-01\n",
      "Epoch: 15167 mean train loss:  1.05301803e-02, bound:  3.15752387e-01\n",
      "Epoch: 15168 mean train loss:  1.05292881e-02, bound:  3.15752327e-01\n",
      "Epoch: 15169 mean train loss:  1.05284033e-02, bound:  3.15752298e-01\n",
      "Epoch: 15170 mean train loss:  1.05275670e-02, bound:  3.15752238e-01\n",
      "Epoch: 15171 mean train loss:  1.05266785e-02, bound:  3.15752208e-01\n",
      "Epoch: 15172 mean train loss:  1.05258850e-02, bound:  3.15752208e-01\n",
      "Epoch: 15173 mean train loss:  1.05250981e-02, bound:  3.15752119e-01\n",
      "Epoch: 15174 mean train loss:  1.05242534e-02, bound:  3.15752089e-01\n",
      "Epoch: 15175 mean train loss:  1.05234208e-02, bound:  3.15752000e-01\n",
      "Epoch: 15176 mean train loss:  1.05225323e-02, bound:  3.15752000e-01\n",
      "Epoch: 15177 mean train loss:  1.05216978e-02, bound:  3.15751970e-01\n",
      "Epoch: 15178 mean train loss:  1.05208633e-02, bound:  3.15751910e-01\n",
      "Epoch: 15179 mean train loss:  1.05200354e-02, bound:  3.15751880e-01\n",
      "Epoch: 15180 mean train loss:  1.05191348e-02, bound:  3.15751851e-01\n",
      "Epoch: 15181 mean train loss:  1.05183180e-02, bound:  3.15751791e-01\n",
      "Epoch: 15182 mean train loss:  1.05174640e-02, bound:  3.15751761e-01\n",
      "Epoch: 15183 mean train loss:  1.05166547e-02, bound:  3.15751731e-01\n",
      "Epoch: 15184 mean train loss:  1.05157970e-02, bound:  3.15751672e-01\n",
      "Epoch: 15185 mean train loss:  1.05149643e-02, bound:  3.15751642e-01\n",
      "Epoch: 15186 mean train loss:  1.05140880e-02, bound:  3.15751582e-01\n",
      "Epoch: 15187 mean train loss:  1.05131753e-02, bound:  3.15751553e-01\n",
      "Epoch: 15188 mean train loss:  1.05123520e-02, bound:  3.15751463e-01\n",
      "Epoch: 15189 mean train loss:  1.05115147e-02, bound:  3.15751433e-01\n",
      "Epoch: 15190 mean train loss:  1.05106728e-02, bound:  3.15751404e-01\n",
      "Epoch: 15191 mean train loss:  1.05098682e-02, bound:  3.15751404e-01\n",
      "Epoch: 15192 mean train loss:  1.05090104e-02, bound:  3.15751344e-01\n",
      "Epoch: 15193 mean train loss:  1.05082160e-02, bound:  3.15751284e-01\n",
      "Epoch: 15194 mean train loss:  1.05073210e-02, bound:  3.15751225e-01\n",
      "Epoch: 15195 mean train loss:  1.05065228e-02, bound:  3.15751195e-01\n",
      "Epoch: 15196 mean train loss:  1.05056139e-02, bound:  3.15751165e-01\n",
      "Epoch: 15197 mean train loss:  1.05046965e-02, bound:  3.15751106e-01\n",
      "Epoch: 15198 mean train loss:  1.05038825e-02, bound:  3.15751076e-01\n",
      "Epoch: 15199 mean train loss:  1.05030388e-02, bound:  3.15751016e-01\n",
      "Epoch: 15200 mean train loss:  1.05022024e-02, bound:  3.15750986e-01\n",
      "Epoch: 15201 mean train loss:  1.05014024e-02, bound:  3.15750957e-01\n",
      "Epoch: 15202 mean train loss:  1.05004758e-02, bound:  3.15750897e-01\n",
      "Epoch: 15203 mean train loss:  1.04997046e-02, bound:  3.15750867e-01\n",
      "Epoch: 15204 mean train loss:  1.04988385e-02, bound:  3.15750808e-01\n",
      "Epoch: 15205 mean train loss:  1.04979053e-02, bound:  3.15750778e-01\n",
      "Epoch: 15206 mean train loss:  1.04971211e-02, bound:  3.15750748e-01\n",
      "Epoch: 15207 mean train loss:  1.04962457e-02, bound:  3.15750659e-01\n",
      "Epoch: 15208 mean train loss:  1.04954569e-02, bound:  3.15750659e-01\n",
      "Epoch: 15209 mean train loss:  1.04946075e-02, bound:  3.15750569e-01\n",
      "Epoch: 15210 mean train loss:  1.04937460e-02, bound:  3.15750569e-01\n",
      "Epoch: 15211 mean train loss:  1.04928361e-02, bound:  3.15750539e-01\n",
      "Epoch: 15212 mean train loss:  1.04920082e-02, bound:  3.15750480e-01\n",
      "Epoch: 15213 mean train loss:  1.04911486e-02, bound:  3.15750450e-01\n",
      "Epoch: 15214 mean train loss:  1.04903299e-02, bound:  3.15750390e-01\n",
      "Epoch: 15215 mean train loss:  1.04895113e-02, bound:  3.15750360e-01\n",
      "Epoch: 15216 mean train loss:  1.04885772e-02, bound:  3.15750301e-01\n",
      "Epoch: 15217 mean train loss:  1.04877735e-02, bound:  3.15750241e-01\n",
      "Epoch: 15218 mean train loss:  1.04869567e-02, bound:  3.15750241e-01\n",
      "Epoch: 15219 mean train loss:  1.04860915e-02, bound:  3.15750211e-01\n",
      "Epoch: 15220 mean train loss:  1.04851695e-02, bound:  3.15750182e-01\n",
      "Epoch: 15221 mean train loss:  1.04843527e-02, bound:  3.15750122e-01\n",
      "Epoch: 15222 mean train loss:  1.04834875e-02, bound:  3.15750092e-01\n",
      "Epoch: 15223 mean train loss:  1.04826493e-02, bound:  3.15750003e-01\n",
      "Epoch: 15224 mean train loss:  1.04817841e-02, bound:  3.15749973e-01\n",
      "Epoch: 15225 mean train loss:  1.04809972e-02, bound:  3.15749973e-01\n",
      "Epoch: 15226 mean train loss:  1.04800835e-02, bound:  3.15749884e-01\n",
      "Epoch: 15227 mean train loss:  1.04791978e-02, bound:  3.15749854e-01\n",
      "Epoch: 15228 mean train loss:  1.04783420e-02, bound:  3.15749854e-01\n",
      "Epoch: 15229 mean train loss:  1.04775652e-02, bound:  3.15749764e-01\n",
      "Epoch: 15230 mean train loss:  1.04766982e-02, bound:  3.15749735e-01\n",
      "Epoch: 15231 mean train loss:  1.04758609e-02, bound:  3.15749675e-01\n",
      "Epoch: 15232 mean train loss:  1.04750264e-02, bound:  3.15749645e-01\n",
      "Epoch: 15233 mean train loss:  1.04741156e-02, bound:  3.15749586e-01\n",
      "Epoch: 15234 mean train loss:  1.04733128e-02, bound:  3.15749556e-01\n",
      "Epoch: 15235 mean train loss:  1.04723973e-02, bound:  3.15749526e-01\n",
      "Epoch: 15236 mean train loss:  1.04715349e-02, bound:  3.15749437e-01\n",
      "Epoch: 15237 mean train loss:  1.04707116e-02, bound:  3.15749437e-01\n",
      "Epoch: 15238 mean train loss:  1.04698120e-02, bound:  3.15749407e-01\n",
      "Epoch: 15239 mean train loss:  1.04689710e-02, bound:  3.15749347e-01\n",
      "Epoch: 15240 mean train loss:  1.04681691e-02, bound:  3.15749317e-01\n",
      "Epoch: 15241 mean train loss:  1.04672685e-02, bound:  3.15749288e-01\n",
      "Epoch: 15242 mean train loss:  1.04663428e-02, bound:  3.15749228e-01\n",
      "Epoch: 15243 mean train loss:  1.04654608e-02, bound:  3.15749168e-01\n",
      "Epoch: 15244 mean train loss:  1.04646664e-02, bound:  3.15749139e-01\n",
      "Epoch: 15245 mean train loss:  1.04639046e-02, bound:  3.15749109e-01\n",
      "Epoch: 15246 mean train loss:  1.04629751e-02, bound:  3.15749079e-01\n",
      "Epoch: 15247 mean train loss:  1.04620131e-02, bound:  3.15748990e-01\n",
      "Epoch: 15248 mean train loss:  1.04612280e-02, bound:  3.15748960e-01\n",
      "Epoch: 15249 mean train loss:  1.04603460e-02, bound:  3.15748960e-01\n",
      "Epoch: 15250 mean train loss:  1.04595795e-02, bound:  3.15748870e-01\n",
      "Epoch: 15251 mean train loss:  1.04586417e-02, bound:  3.15748841e-01\n",
      "Epoch: 15252 mean train loss:  1.04578137e-02, bound:  3.15748811e-01\n",
      "Epoch: 15253 mean train loss:  1.04569886e-02, bound:  3.15748751e-01\n",
      "Epoch: 15254 mean train loss:  1.04560889e-02, bound:  3.15748721e-01\n",
      "Epoch: 15255 mean train loss:  1.04551921e-02, bound:  3.15748662e-01\n",
      "Epoch: 15256 mean train loss:  1.04543818e-02, bound:  3.15748632e-01\n",
      "Epoch: 15257 mean train loss:  1.04534356e-02, bound:  3.15748572e-01\n",
      "Epoch: 15258 mean train loss:  1.04526440e-02, bound:  3.15748543e-01\n",
      "Epoch: 15259 mean train loss:  1.04518021e-02, bound:  3.15748513e-01\n",
      "Epoch: 15260 mean train loss:  1.04509089e-02, bound:  3.15748453e-01\n",
      "Epoch: 15261 mean train loss:  1.04501275e-02, bound:  3.15748423e-01\n",
      "Epoch: 15262 mean train loss:  1.04491571e-02, bound:  3.15748394e-01\n",
      "Epoch: 15263 mean train loss:  1.04483496e-02, bound:  3.15748334e-01\n",
      "Epoch: 15264 mean train loss:  1.04474584e-02, bound:  3.15748274e-01\n",
      "Epoch: 15265 mean train loss:  1.04466081e-02, bound:  3.15748245e-01\n",
      "Epoch: 15266 mean train loss:  1.04458453e-02, bound:  3.15748215e-01\n",
      "Epoch: 15267 mean train loss:  1.04448795e-02, bound:  3.15748125e-01\n",
      "Epoch: 15268 mean train loss:  1.04440078e-02, bound:  3.15748125e-01\n",
      "Epoch: 15269 mean train loss:  1.04432404e-02, bound:  3.15748096e-01\n",
      "Epoch: 15270 mean train loss:  1.04422774e-02, bound:  3.15748006e-01\n",
      "Epoch: 15271 mean train loss:  1.04414420e-02, bound:  3.15748006e-01\n",
      "Epoch: 15272 mean train loss:  1.04405480e-02, bound:  3.15747976e-01\n",
      "Epoch: 15273 mean train loss:  1.04397824e-02, bound:  3.15747947e-01\n",
      "Epoch: 15274 mean train loss:  1.04388837e-02, bound:  3.15747857e-01\n",
      "Epoch: 15275 mean train loss:  1.04379822e-02, bound:  3.15747797e-01\n",
      "Epoch: 15276 mean train loss:  1.04372045e-02, bound:  3.15747768e-01\n",
      "Epoch: 15277 mean train loss:  1.04363291e-02, bound:  3.15747738e-01\n",
      "Epoch: 15278 mean train loss:  1.04353484e-02, bound:  3.15747678e-01\n",
      "Epoch: 15279 mean train loss:  1.04345577e-02, bound:  3.15747648e-01\n",
      "Epoch: 15280 mean train loss:  1.04336962e-02, bound:  3.15747648e-01\n",
      "Epoch: 15281 mean train loss:  1.04328394e-02, bound:  3.15747559e-01\n",
      "Epoch: 15282 mean train loss:  1.04319993e-02, bound:  3.15747529e-01\n",
      "Epoch: 15283 mean train loss:  1.04311332e-02, bound:  3.15747470e-01\n",
      "Epoch: 15284 mean train loss:  1.04302960e-02, bound:  3.15747440e-01\n",
      "Epoch: 15285 mean train loss:  1.04293460e-02, bound:  3.15747410e-01\n",
      "Epoch: 15286 mean train loss:  1.04285153e-02, bound:  3.15747350e-01\n",
      "Epoch: 15287 mean train loss:  1.04276501e-02, bound:  3.15747321e-01\n",
      "Epoch: 15288 mean train loss:  1.04267448e-02, bound:  3.15747291e-01\n",
      "Epoch: 15289 mean train loss:  1.04258964e-02, bound:  3.15747231e-01\n",
      "Epoch: 15290 mean train loss:  1.04250861e-02, bound:  3.15747172e-01\n",
      "Epoch: 15291 mean train loss:  1.04241949e-02, bound:  3.15747112e-01\n",
      "Epoch: 15292 mean train loss:  1.04233390e-02, bound:  3.15747112e-01\n",
      "Epoch: 15293 mean train loss:  1.04225455e-02, bound:  3.15747082e-01\n",
      "Epoch: 15294 mean train loss:  1.04216216e-02, bound:  3.15746993e-01\n",
      "Epoch: 15295 mean train loss:  1.04207285e-02, bound:  3.15746963e-01\n",
      "Epoch: 15296 mean train loss:  1.04198353e-02, bound:  3.15746963e-01\n",
      "Epoch: 15297 mean train loss:  1.04190651e-02, bound:  3.15746874e-01\n",
      "Epoch: 15298 mean train loss:  1.04181627e-02, bound:  3.15746844e-01\n",
      "Epoch: 15299 mean train loss:  1.04172090e-02, bound:  3.15746814e-01\n",
      "Epoch: 15300 mean train loss:  1.04164304e-02, bound:  3.15746754e-01\n",
      "Epoch: 15301 mean train loss:  1.04155876e-02, bound:  3.15746695e-01\n",
      "Epoch: 15302 mean train loss:  1.04147429e-02, bound:  3.15746665e-01\n",
      "Epoch: 15303 mean train loss:  1.04137957e-02, bound:  3.15746635e-01\n",
      "Epoch: 15304 mean train loss:  1.04129631e-02, bound:  3.15746605e-01\n",
      "Epoch: 15305 mean train loss:  1.04120914e-02, bound:  3.15746546e-01\n",
      "Epoch: 15306 mean train loss:  1.04111247e-02, bound:  3.15746456e-01\n",
      "Epoch: 15307 mean train loss:  1.04103489e-02, bound:  3.15746456e-01\n",
      "Epoch: 15308 mean train loss:  1.04094502e-02, bound:  3.15746397e-01\n",
      "Epoch: 15309 mean train loss:  1.04085896e-02, bound:  3.15746367e-01\n",
      "Epoch: 15310 mean train loss:  1.04076630e-02, bound:  3.15746307e-01\n",
      "Epoch: 15311 mean train loss:  1.04068713e-02, bound:  3.15746307e-01\n",
      "Epoch: 15312 mean train loss:  1.04059651e-02, bound:  3.15746248e-01\n",
      "Epoch: 15313 mean train loss:  1.04052024e-02, bound:  3.15746188e-01\n",
      "Epoch: 15314 mean train loss:  1.04042375e-02, bound:  3.15746129e-01\n",
      "Epoch: 15315 mean train loss:  1.04034366e-02, bound:  3.15746099e-01\n",
      "Epoch: 15316 mean train loss:  1.04024839e-02, bound:  3.15746069e-01\n",
      "Epoch: 15317 mean train loss:  1.04016513e-02, bound:  3.15746009e-01\n",
      "Epoch: 15318 mean train loss:  1.04007926e-02, bound:  3.15745980e-01\n",
      "Epoch: 15319 mean train loss:  1.03998808e-02, bound:  3.15745950e-01\n",
      "Epoch: 15320 mean train loss:  1.03990939e-02, bound:  3.15745890e-01\n",
      "Epoch: 15321 mean train loss:  1.03981681e-02, bound:  3.15745860e-01\n",
      "Epoch: 15322 mean train loss:  1.03973206e-02, bound:  3.15745801e-01\n",
      "Epoch: 15323 mean train loss:  1.03964601e-02, bound:  3.15745771e-01\n",
      "Epoch: 15324 mean train loss:  1.03955055e-02, bound:  3.15745682e-01\n",
      "Epoch: 15325 mean train loss:  1.03947604e-02, bound:  3.15745682e-01\n",
      "Epoch: 15326 mean train loss:  1.03938384e-02, bound:  3.15745622e-01\n",
      "Epoch: 15327 mean train loss:  1.03929378e-02, bound:  3.15745562e-01\n",
      "Epoch: 15328 mean train loss:  1.03920391e-02, bound:  3.15745562e-01\n",
      "Epoch: 15329 mean train loss:  1.03912689e-02, bound:  3.15745533e-01\n",
      "Epoch: 15330 mean train loss:  1.03903273e-02, bound:  3.15745473e-01\n",
      "Epoch: 15331 mean train loss:  1.03893410e-02, bound:  3.15745413e-01\n",
      "Epoch: 15332 mean train loss:  1.03885997e-02, bound:  3.15745413e-01\n",
      "Epoch: 15333 mean train loss:  1.03876768e-02, bound:  3.15745324e-01\n",
      "Epoch: 15334 mean train loss:  1.03868237e-02, bound:  3.15745294e-01\n",
      "Epoch: 15335 mean train loss:  1.03859836e-02, bound:  3.15745234e-01\n",
      "Epoch: 15336 mean train loss:  1.03850830e-02, bound:  3.15745205e-01\n",
      "Epoch: 15337 mean train loss:  1.03842160e-02, bound:  3.15745175e-01\n",
      "Epoch: 15338 mean train loss:  1.03833592e-02, bound:  3.15745115e-01\n",
      "Epoch: 15339 mean train loss:  1.03824260e-02, bound:  3.15745056e-01\n",
      "Epoch: 15340 mean train loss:  1.03816399e-02, bound:  3.15745026e-01\n",
      "Epoch: 15341 mean train loss:  1.03806676e-02, bound:  3.15744996e-01\n",
      "Epoch: 15342 mean train loss:  1.03798350e-02, bound:  3.15744966e-01\n",
      "Epoch: 15343 mean train loss:  1.03789214e-02, bound:  3.15744877e-01\n",
      "Epoch: 15344 mean train loss:  1.03781018e-02, bound:  3.15744877e-01\n",
      "Epoch: 15345 mean train loss:  1.03771286e-02, bound:  3.15744817e-01\n",
      "Epoch: 15346 mean train loss:  1.03763044e-02, bound:  3.15744758e-01\n",
      "Epoch: 15347 mean train loss:  1.03754057e-02, bound:  3.15744758e-01\n",
      "Epoch: 15348 mean train loss:  1.03746010e-02, bound:  3.15744668e-01\n",
      "Epoch: 15349 mean train loss:  1.03737386e-02, bound:  3.15744668e-01\n",
      "Epoch: 15350 mean train loss:  1.03727616e-02, bound:  3.15744579e-01\n",
      "Epoch: 15351 mean train loss:  1.03718517e-02, bound:  3.15744549e-01\n",
      "Epoch: 15352 mean train loss:  1.03710452e-02, bound:  3.15744519e-01\n",
      "Epoch: 15353 mean train loss:  1.03700766e-02, bound:  3.15744460e-01\n",
      "Epoch: 15354 mean train loss:  1.03692710e-02, bound:  3.15744430e-01\n",
      "Epoch: 15355 mean train loss:  1.03683285e-02, bound:  3.15744370e-01\n",
      "Epoch: 15356 mean train loss:  1.03675332e-02, bound:  3.15744340e-01\n",
      "Epoch: 15357 mean train loss:  1.03665693e-02, bound:  3.15744311e-01\n",
      "Epoch: 15358 mean train loss:  1.03658438e-02, bound:  3.15744251e-01\n",
      "Epoch: 15359 mean train loss:  1.03648724e-02, bound:  3.15744191e-01\n",
      "Epoch: 15360 mean train loss:  1.03639131e-02, bound:  3.15744132e-01\n",
      "Epoch: 15361 mean train loss:  1.03630647e-02, bound:  3.15744132e-01\n",
      "Epoch: 15362 mean train loss:  1.03622787e-02, bound:  3.15744072e-01\n",
      "Epoch: 15363 mean train loss:  1.03613045e-02, bound:  3.15744072e-01\n",
      "Epoch: 15364 mean train loss:  1.03606023e-02, bound:  3.15743983e-01\n",
      "Epoch: 15365 mean train loss:  1.03595732e-02, bound:  3.15743953e-01\n",
      "Epoch: 15366 mean train loss:  1.03587005e-02, bound:  3.15743893e-01\n",
      "Epoch: 15367 mean train loss:  1.03578782e-02, bound:  3.15743864e-01\n",
      "Epoch: 15368 mean train loss:  1.03569655e-02, bound:  3.15743804e-01\n",
      "Epoch: 15369 mean train loss:  1.03561077e-02, bound:  3.15743774e-01\n",
      "Epoch: 15370 mean train loss:  1.03551792e-02, bound:  3.15743744e-01\n",
      "Epoch: 15371 mean train loss:  1.03543205e-02, bound:  3.15743655e-01\n",
      "Epoch: 15372 mean train loss:  1.03534535e-02, bound:  3.15743625e-01\n",
      "Epoch: 15373 mean train loss:  1.03525249e-02, bound:  3.15743566e-01\n",
      "Epoch: 15374 mean train loss:  1.03516392e-02, bound:  3.15743566e-01\n",
      "Epoch: 15375 mean train loss:  1.03508122e-02, bound:  3.15743536e-01\n",
      "Epoch: 15376 mean train loss:  1.03499228e-02, bound:  3.15743446e-01\n",
      "Epoch: 15377 mean train loss:  1.03490036e-02, bound:  3.15743446e-01\n",
      "Epoch: 15378 mean train loss:  1.03481496e-02, bound:  3.15743327e-01\n",
      "Epoch: 15379 mean train loss:  1.03473095e-02, bound:  3.15743327e-01\n",
      "Epoch: 15380 mean train loss:  1.03463996e-02, bound:  3.15743297e-01\n",
      "Epoch: 15381 mean train loss:  1.03454720e-02, bound:  3.15743238e-01\n",
      "Epoch: 15382 mean train loss:  1.03446394e-02, bound:  3.15743208e-01\n",
      "Epoch: 15383 mean train loss:  1.03436820e-02, bound:  3.15743148e-01\n",
      "Epoch: 15384 mean train loss:  1.03428066e-02, bound:  3.15743119e-01\n",
      "Epoch: 15385 mean train loss:  1.03419404e-02, bound:  3.15743059e-01\n",
      "Epoch: 15386 mean train loss:  1.03410603e-02, bound:  3.15742999e-01\n",
      "Epoch: 15387 mean train loss:  1.03400955e-02, bound:  3.15742970e-01\n",
      "Epoch: 15388 mean train loss:  1.03393337e-02, bound:  3.15742970e-01\n",
      "Epoch: 15389 mean train loss:  1.03384154e-02, bound:  3.15742880e-01\n",
      "Epoch: 15390 mean train loss:  1.03375372e-02, bound:  3.15742850e-01\n",
      "Epoch: 15391 mean train loss:  1.03366319e-02, bound:  3.15742791e-01\n",
      "Epoch: 15392 mean train loss:  1.03358356e-02, bound:  3.15742761e-01\n",
      "Epoch: 15393 mean train loss:  1.03348782e-02, bound:  3.15742701e-01\n",
      "Epoch: 15394 mean train loss:  1.03339618e-02, bound:  3.15742671e-01\n",
      "Epoch: 15395 mean train loss:  1.03331478e-02, bound:  3.15742642e-01\n",
      "Epoch: 15396 mean train loss:  1.03322472e-02, bound:  3.15742582e-01\n",
      "Epoch: 15397 mean train loss:  1.03313634e-02, bound:  3.15742552e-01\n",
      "Epoch: 15398 mean train loss:  1.03303948e-02, bound:  3.15742493e-01\n",
      "Epoch: 15399 mean train loss:  1.03295501e-02, bound:  3.15742433e-01\n",
      "Epoch: 15400 mean train loss:  1.03286998e-02, bound:  3.15742403e-01\n",
      "Epoch: 15401 mean train loss:  1.03277918e-02, bound:  3.15742344e-01\n",
      "Epoch: 15402 mean train loss:  1.03269713e-02, bound:  3.15742314e-01\n",
      "Epoch: 15403 mean train loss:  1.03260633e-02, bound:  3.15742284e-01\n",
      "Epoch: 15404 mean train loss:  1.03252092e-02, bound:  3.15742224e-01\n",
      "Epoch: 15405 mean train loss:  1.03241941e-02, bound:  3.15742195e-01\n",
      "Epoch: 15406 mean train loss:  1.03233363e-02, bound:  3.15742135e-01\n",
      "Epoch: 15407 mean train loss:  1.03224991e-02, bound:  3.15742105e-01\n",
      "Epoch: 15408 mean train loss:  1.03217019e-02, bound:  3.15742016e-01\n",
      "Epoch: 15409 mean train loss:  1.03206066e-02, bound:  3.15742016e-01\n",
      "Epoch: 15410 mean train loss:  1.03197824e-02, bound:  3.15741986e-01\n",
      "Epoch: 15411 mean train loss:  1.03188874e-02, bound:  3.15741897e-01\n",
      "Epoch: 15412 mean train loss:  1.03180893e-02, bound:  3.15741867e-01\n",
      "Epoch: 15413 mean train loss:  1.03170713e-02, bound:  3.15741837e-01\n",
      "Epoch: 15414 mean train loss:  1.03161801e-02, bound:  3.15741777e-01\n",
      "Epoch: 15415 mean train loss:  1.03153540e-02, bound:  3.15741748e-01\n",
      "Epoch: 15416 mean train loss:  1.03144757e-02, bound:  3.15741688e-01\n",
      "Epoch: 15417 mean train loss:  1.03135603e-02, bound:  3.15741658e-01\n",
      "Epoch: 15418 mean train loss:  1.03127034e-02, bound:  3.15741628e-01\n",
      "Epoch: 15419 mean train loss:  1.03118289e-02, bound:  3.15741539e-01\n",
      "Epoch: 15420 mean train loss:  1.03109106e-02, bound:  3.15741539e-01\n",
      "Epoch: 15421 mean train loss:  1.03100277e-02, bound:  3.15741450e-01\n",
      "Epoch: 15422 mean train loss:  1.03091514e-02, bound:  3.15741420e-01\n",
      "Epoch: 15423 mean train loss:  1.03082601e-02, bound:  3.15741420e-01\n",
      "Epoch: 15424 mean train loss:  1.03074117e-02, bound:  3.15741330e-01\n",
      "Epoch: 15425 mean train loss:  1.03064636e-02, bound:  3.15741330e-01\n",
      "Epoch: 15426 mean train loss:  1.03055099e-02, bound:  3.15741241e-01\n",
      "Epoch: 15427 mean train loss:  1.03046829e-02, bound:  3.15741211e-01\n",
      "Epoch: 15428 mean train loss:  1.03037842e-02, bound:  3.15741181e-01\n",
      "Epoch: 15429 mean train loss:  1.03028594e-02, bound:  3.15741122e-01\n",
      "Epoch: 15430 mean train loss:  1.03019355e-02, bound:  3.15741092e-01\n",
      "Epoch: 15431 mean train loss:  1.03010433e-02, bound:  3.15741032e-01\n",
      "Epoch: 15432 mean train loss:  1.03002479e-02, bound:  3.15741003e-01\n",
      "Epoch: 15433 mean train loss:  1.02993511e-02, bound:  3.15740913e-01\n",
      "Epoch: 15434 mean train loss:  1.02984421e-02, bound:  3.15740883e-01\n",
      "Epoch: 15435 mean train loss:  1.02974549e-02, bound:  3.15740854e-01\n",
      "Epoch: 15436 mean train loss:  1.02966763e-02, bound:  3.15740794e-01\n",
      "Epoch: 15437 mean train loss:  1.02957897e-02, bound:  3.15740764e-01\n",
      "Epoch: 15438 mean train loss:  1.02948472e-02, bound:  3.15740705e-01\n",
      "Epoch: 15439 mean train loss:  1.02939457e-02, bound:  3.15740675e-01\n",
      "Epoch: 15440 mean train loss:  1.02930618e-02, bound:  3.15740615e-01\n",
      "Epoch: 15441 mean train loss:  1.02921315e-02, bound:  3.15740585e-01\n",
      "Epoch: 15442 mean train loss:  1.02912690e-02, bound:  3.15740556e-01\n",
      "Epoch: 15443 mean train loss:  1.02903862e-02, bound:  3.15740496e-01\n",
      "Epoch: 15444 mean train loss:  1.02895126e-02, bound:  3.15740436e-01\n",
      "Epoch: 15445 mean train loss:  1.02885328e-02, bound:  3.15740407e-01\n",
      "Epoch: 15446 mean train loss:  1.02876695e-02, bound:  3.15740347e-01\n",
      "Epoch: 15447 mean train loss:  1.02867745e-02, bound:  3.15740317e-01\n",
      "Epoch: 15448 mean train loss:  1.02859437e-02, bound:  3.15740287e-01\n",
      "Epoch: 15449 mean train loss:  1.02850031e-02, bound:  3.15740228e-01\n",
      "Epoch: 15450 mean train loss:  1.02840466e-02, bound:  3.15740198e-01\n",
      "Epoch: 15451 mean train loss:  1.02831861e-02, bound:  3.15740138e-01\n",
      "Epoch: 15452 mean train loss:  1.02823861e-02, bound:  3.15740108e-01\n",
      "Epoch: 15453 mean train loss:  1.02813737e-02, bound:  3.15740019e-01\n",
      "Epoch: 15454 mean train loss:  1.02804890e-02, bound:  3.15739989e-01\n",
      "Epoch: 15455 mean train loss:  1.02796145e-02, bound:  3.15739989e-01\n",
      "Epoch: 15456 mean train loss:  1.02787763e-02, bound:  3.15739900e-01\n",
      "Epoch: 15457 mean train loss:  1.02777528e-02, bound:  3.15739900e-01\n",
      "Epoch: 15458 mean train loss:  1.02769583e-02, bound:  3.15739810e-01\n",
      "Epoch: 15459 mean train loss:  1.02759907e-02, bound:  3.15739781e-01\n",
      "Epoch: 15460 mean train loss:  1.02751935e-02, bound:  3.15739751e-01\n",
      "Epoch: 15461 mean train loss:  1.02742249e-02, bound:  3.15739691e-01\n",
      "Epoch: 15462 mean train loss:  1.02734035e-02, bound:  3.15739661e-01\n",
      "Epoch: 15463 mean train loss:  1.02724861e-02, bound:  3.15739572e-01\n",
      "Epoch: 15464 mean train loss:  1.02715706e-02, bound:  3.15739542e-01\n",
      "Epoch: 15465 mean train loss:  1.02706458e-02, bound:  3.15739512e-01\n",
      "Epoch: 15466 mean train loss:  1.02696829e-02, bound:  3.15739453e-01\n",
      "Epoch: 15467 mean train loss:  1.02688186e-02, bound:  3.15739423e-01\n",
      "Epoch: 15468 mean train loss:  1.02679366e-02, bound:  3.15739363e-01\n",
      "Epoch: 15469 mean train loss:  1.02671077e-02, bound:  3.15739334e-01\n",
      "Epoch: 15470 mean train loss:  1.02661746e-02, bound:  3.15739304e-01\n",
      "Epoch: 15471 mean train loss:  1.02652730e-02, bound:  3.15739244e-01\n",
      "Epoch: 15472 mean train loss:  1.02644926e-02, bound:  3.15739185e-01\n",
      "Epoch: 15473 mean train loss:  1.02635296e-02, bound:  3.15739185e-01\n",
      "Epoch: 15474 mean train loss:  1.02626383e-02, bound:  3.15739125e-01\n",
      "Epoch: 15475 mean train loss:  1.02616204e-02, bound:  3.15739065e-01\n",
      "Epoch: 15476 mean train loss:  1.02607440e-02, bound:  3.15739006e-01\n",
      "Epoch: 15477 mean train loss:  1.02597903e-02, bound:  3.15738976e-01\n",
      "Epoch: 15478 mean train loss:  1.02589084e-02, bound:  3.15738916e-01\n",
      "Epoch: 15479 mean train loss:  1.02579882e-02, bound:  3.15738887e-01\n",
      "Epoch: 15480 mean train loss:  1.02572050e-02, bound:  3.15738857e-01\n",
      "Epoch: 15481 mean train loss:  1.02562392e-02, bound:  3.15738797e-01\n",
      "Epoch: 15482 mean train loss:  1.02553815e-02, bound:  3.15738767e-01\n",
      "Epoch: 15483 mean train loss:  1.02544418e-02, bound:  3.15738678e-01\n",
      "Epoch: 15484 mean train loss:  1.02536445e-02, bound:  3.15738678e-01\n",
      "Epoch: 15485 mean train loss:  1.02525894e-02, bound:  3.15738618e-01\n",
      "Epoch: 15486 mean train loss:  1.02517037e-02, bound:  3.15738559e-01\n",
      "Epoch: 15487 mean train loss:  1.02508161e-02, bound:  3.15738559e-01\n",
      "Epoch: 15488 mean train loss:  1.02499863e-02, bound:  3.15738499e-01\n",
      "Epoch: 15489 mean train loss:  1.02490680e-02, bound:  3.15738440e-01\n",
      "Epoch: 15490 mean train loss:  1.02482149e-02, bound:  3.15738380e-01\n",
      "Epoch: 15491 mean train loss:  1.02472315e-02, bound:  3.15738350e-01\n",
      "Epoch: 15492 mean train loss:  1.02463095e-02, bound:  3.15738320e-01\n",
      "Epoch: 15493 mean train loss:  1.02453977e-02, bound:  3.15738261e-01\n",
      "Epoch: 15494 mean train loss:  1.02445371e-02, bound:  3.15738201e-01\n",
      "Epoch: 15495 mean train loss:  1.02436431e-02, bound:  3.15738201e-01\n",
      "Epoch: 15496 mean train loss:  1.02426643e-02, bound:  3.15738112e-01\n",
      "Epoch: 15497 mean train loss:  1.02418577e-02, bound:  3.15738082e-01\n",
      "Epoch: 15498 mean train loss:  1.02409134e-02, bound:  3.15738022e-01\n",
      "Epoch: 15499 mean train loss:  1.02400640e-02, bound:  3.15737993e-01\n",
      "Epoch: 15500 mean train loss:  1.02391336e-02, bound:  3.15737963e-01\n",
      "Epoch: 15501 mean train loss:  1.02381902e-02, bound:  3.15737903e-01\n",
      "Epoch: 15502 mean train loss:  1.02373129e-02, bound:  3.15737873e-01\n",
      "Epoch: 15503 mean train loss:  1.02363490e-02, bound:  3.15737784e-01\n",
      "Epoch: 15504 mean train loss:  1.02355452e-02, bound:  3.15737754e-01\n",
      "Epoch: 15505 mean train loss:  1.02345170e-02, bound:  3.15737754e-01\n",
      "Epoch: 15506 mean train loss:  1.02336714e-02, bound:  3.15737665e-01\n",
      "Epoch: 15507 mean train loss:  1.02328295e-02, bound:  3.15737635e-01\n",
      "Epoch: 15508 mean train loss:  1.02318684e-02, bound:  3.15737575e-01\n",
      "Epoch: 15509 mean train loss:  1.02309464e-02, bound:  3.15737545e-01\n",
      "Epoch: 15510 mean train loss:  1.02300672e-02, bound:  3.15737516e-01\n",
      "Epoch: 15511 mean train loss:  1.02291135e-02, bound:  3.15737456e-01\n",
      "Epoch: 15512 mean train loss:  1.02282641e-02, bound:  3.15737426e-01\n",
      "Epoch: 15513 mean train loss:  1.02273831e-02, bound:  3.15737337e-01\n",
      "Epoch: 15514 mean train loss:  1.02264667e-02, bound:  3.15737307e-01\n",
      "Epoch: 15515 mean train loss:  1.02254916e-02, bound:  3.15737247e-01\n",
      "Epoch: 15516 mean train loss:  1.02245947e-02, bound:  3.15737218e-01\n",
      "Epoch: 15517 mean train loss:  1.02237519e-02, bound:  3.15737188e-01\n",
      "Epoch: 15518 mean train loss:  1.02227684e-02, bound:  3.15737128e-01\n",
      "Epoch: 15519 mean train loss:  1.02217756e-02, bound:  3.15737098e-01\n",
      "Epoch: 15520 mean train loss:  1.02210511e-02, bound:  3.15737069e-01\n",
      "Epoch: 15521 mean train loss:  1.02201104e-02, bound:  3.15737009e-01\n",
      "Epoch: 15522 mean train loss:  1.02191716e-02, bound:  3.15736949e-01\n",
      "Epoch: 15523 mean train loss:  1.02182152e-02, bound:  3.15736890e-01\n",
      "Epoch: 15524 mean train loss:  1.02174152e-02, bound:  3.15736860e-01\n",
      "Epoch: 15525 mean train loss:  1.02164457e-02, bound:  3.15736800e-01\n",
      "Epoch: 15526 mean train loss:  1.02155227e-02, bound:  3.15736771e-01\n",
      "Epoch: 15527 mean train loss:  1.02146445e-02, bound:  3.15736741e-01\n",
      "Epoch: 15528 mean train loss:  1.02136899e-02, bound:  3.15736681e-01\n",
      "Epoch: 15529 mean train loss:  1.02127949e-02, bound:  3.15736651e-01\n",
      "Epoch: 15530 mean train loss:  1.02119762e-02, bound:  3.15736592e-01\n",
      "Epoch: 15531 mean train loss:  1.02110067e-02, bound:  3.15736562e-01\n",
      "Epoch: 15532 mean train loss:  1.02099981e-02, bound:  3.15736502e-01\n",
      "Epoch: 15533 mean train loss:  1.02090975e-02, bound:  3.15736443e-01\n",
      "Epoch: 15534 mean train loss:  1.02081876e-02, bound:  3.15736413e-01\n",
      "Epoch: 15535 mean train loss:  1.02072749e-02, bound:  3.15736353e-01\n",
      "Epoch: 15536 mean train loss:  1.02064135e-02, bound:  3.15736324e-01\n",
      "Epoch: 15537 mean train loss:  1.02055836e-02, bound:  3.15736294e-01\n",
      "Epoch: 15538 mean train loss:  1.02045797e-02, bound:  3.15736234e-01\n",
      "Epoch: 15539 mean train loss:  1.02036810e-02, bound:  3.15736204e-01\n",
      "Epoch: 15540 mean train loss:  1.02027822e-02, bound:  3.15736115e-01\n",
      "Epoch: 15541 mean train loss:  1.02019180e-02, bound:  3.15736085e-01\n",
      "Epoch: 15542 mean train loss:  1.02009913e-02, bound:  3.15736055e-01\n",
      "Epoch: 15543 mean train loss:  1.02000870e-02, bound:  3.15735996e-01\n",
      "Epoch: 15544 mean train loss:  1.01990635e-02, bound:  3.15735966e-01\n",
      "Epoch: 15545 mean train loss:  1.01982057e-02, bound:  3.15735906e-01\n",
      "Epoch: 15546 mean train loss:  1.01972707e-02, bound:  3.15735877e-01\n",
      "Epoch: 15547 mean train loss:  1.01964558e-02, bound:  3.15735787e-01\n",
      "Epoch: 15548 mean train loss:  1.01954676e-02, bound:  3.15735757e-01\n",
      "Epoch: 15549 mean train loss:  1.01945763e-02, bound:  3.15735728e-01\n",
      "Epoch: 15550 mean train loss:  1.01936581e-02, bound:  3.15735668e-01\n",
      "Epoch: 15551 mean train loss:  1.01927044e-02, bound:  3.15735638e-01\n",
      "Epoch: 15552 mean train loss:  1.01917759e-02, bound:  3.15735579e-01\n",
      "Epoch: 15553 mean train loss:  1.01908585e-02, bound:  3.15735519e-01\n",
      "Epoch: 15554 mean train loss:  1.01899095e-02, bound:  3.15735519e-01\n",
      "Epoch: 15555 mean train loss:  1.01891374e-02, bound:  3.15735459e-01\n",
      "Epoch: 15556 mean train loss:  1.01880962e-02, bound:  3.15735430e-01\n",
      "Epoch: 15557 mean train loss:  1.01872599e-02, bound:  3.15735340e-01\n",
      "Epoch: 15558 mean train loss:  1.01863146e-02, bound:  3.15735310e-01\n",
      "Epoch: 15559 mean train loss:  1.01854037e-02, bound:  3.15735251e-01\n",
      "Epoch: 15560 mean train loss:  1.01844827e-02, bound:  3.15735221e-01\n",
      "Epoch: 15561 mean train loss:  1.01835402e-02, bound:  3.15735191e-01\n",
      "Epoch: 15562 mean train loss:  1.01826778e-02, bound:  3.15735132e-01\n",
      "Epoch: 15563 mean train loss:  1.01818051e-02, bound:  3.15735102e-01\n",
      "Epoch: 15564 mean train loss:  1.01808310e-02, bound:  3.15735072e-01\n",
      "Epoch: 15565 mean train loss:  1.01800067e-02, bound:  3.15735012e-01\n",
      "Epoch: 15566 mean train loss:  1.01789404e-02, bound:  3.15734953e-01\n",
      "Epoch: 15567 mean train loss:  1.01780165e-02, bound:  3.15734893e-01\n",
      "Epoch: 15568 mean train loss:  1.01771932e-02, bound:  3.15734863e-01\n",
      "Epoch: 15569 mean train loss:  1.01762302e-02, bound:  3.15734804e-01\n",
      "Epoch: 15570 mean train loss:  1.01752654e-02, bound:  3.15734774e-01\n",
      "Epoch: 15571 mean train loss:  1.01744011e-02, bound:  3.15734744e-01\n",
      "Epoch: 15572 mean train loss:  1.01735033e-02, bound:  3.15734684e-01\n",
      "Epoch: 15573 mean train loss:  1.01724910e-02, bound:  3.15734625e-01\n",
      "Epoch: 15574 mean train loss:  1.01717245e-02, bound:  3.15734595e-01\n",
      "Epoch: 15575 mean train loss:  1.01706656e-02, bound:  3.15734565e-01\n",
      "Epoch: 15576 mean train loss:  1.01699000e-02, bound:  3.15734506e-01\n",
      "Epoch: 15577 mean train loss:  1.01689603e-02, bound:  3.15734446e-01\n",
      "Epoch: 15578 mean train loss:  1.01679265e-02, bound:  3.15734416e-01\n",
      "Epoch: 15579 mean train loss:  1.01670902e-02, bound:  3.15734357e-01\n",
      "Epoch: 15580 mean train loss:  1.01661235e-02, bound:  3.15734327e-01\n",
      "Epoch: 15581 mean train loss:  1.01651922e-02, bound:  3.15734267e-01\n",
      "Epoch: 15582 mean train loss:  1.01642786e-02, bound:  3.15734208e-01\n",
      "Epoch: 15583 mean train loss:  1.01634059e-02, bound:  3.15734208e-01\n",
      "Epoch: 15584 mean train loss:  1.01624299e-02, bound:  3.15734118e-01\n",
      "Epoch: 15585 mean train loss:  1.01615461e-02, bound:  3.15734088e-01\n",
      "Epoch: 15586 mean train loss:  1.01605952e-02, bound:  3.15734029e-01\n",
      "Epoch: 15587 mean train loss:  1.01596871e-02, bound:  3.15733999e-01\n",
      "Epoch: 15588 mean train loss:  1.01587335e-02, bound:  3.15733910e-01\n",
      "Epoch: 15589 mean train loss:  1.01578068e-02, bound:  3.15733910e-01\n",
      "Epoch: 15590 mean train loss:  1.01569360e-02, bound:  3.15733880e-01\n",
      "Epoch: 15591 mean train loss:  1.01560242e-02, bound:  3.15733790e-01\n",
      "Epoch: 15592 mean train loss:  1.01551348e-02, bound:  3.15733761e-01\n",
      "Epoch: 15593 mean train loss:  1.01541765e-02, bound:  3.15733731e-01\n",
      "Epoch: 15594 mean train loss:  1.01532303e-02, bound:  3.15733671e-01\n",
      "Epoch: 15595 mean train loss:  1.01522952e-02, bound:  3.15733641e-01\n",
      "Epoch: 15596 mean train loss:  1.01514617e-02, bound:  3.15733582e-01\n",
      "Epoch: 15597 mean train loss:  1.01505229e-02, bound:  3.15733522e-01\n",
      "Epoch: 15598 mean train loss:  1.01495218e-02, bound:  3.15733463e-01\n",
      "Epoch: 15599 mean train loss:  1.01486659e-02, bound:  3.15733463e-01\n",
      "Epoch: 15600 mean train loss:  1.01476461e-02, bound:  3.15733403e-01\n",
      "Epoch: 15601 mean train loss:  1.01467613e-02, bound:  3.15733343e-01\n",
      "Epoch: 15602 mean train loss:  1.01459287e-02, bound:  3.15733314e-01\n",
      "Epoch: 15603 mean train loss:  1.01449704e-02, bound:  3.15733284e-01\n",
      "Epoch: 15604 mean train loss:  1.01440772e-02, bound:  3.15733224e-01\n",
      "Epoch: 15605 mean train loss:  1.01430826e-02, bound:  3.15733194e-01\n",
      "Epoch: 15606 mean train loss:  1.01422379e-02, bound:  3.15733105e-01\n",
      "Epoch: 15607 mean train loss:  1.01412321e-02, bound:  3.15733075e-01\n",
      "Epoch: 15608 mean train loss:  1.01402774e-02, bound:  3.15733016e-01\n",
      "Epoch: 15609 mean train loss:  1.01394132e-02, bound:  3.15732986e-01\n",
      "Epoch: 15610 mean train loss:  1.01385126e-02, bound:  3.15732956e-01\n",
      "Epoch: 15611 mean train loss:  1.01375114e-02, bound:  3.15732867e-01\n",
      "Epoch: 15612 mean train loss:  1.01366714e-02, bound:  3.15732867e-01\n",
      "Epoch: 15613 mean train loss:  1.01355752e-02, bound:  3.15732777e-01\n",
      "Epoch: 15614 mean train loss:  1.01346169e-02, bound:  3.15732747e-01\n",
      "Epoch: 15615 mean train loss:  1.01337330e-02, bound:  3.15732688e-01\n",
      "Epoch: 15616 mean train loss:  1.01328772e-02, bound:  3.15732658e-01\n",
      "Epoch: 15617 mean train loss:  1.01319216e-02, bound:  3.15732628e-01\n",
      "Epoch: 15618 mean train loss:  1.01310676e-02, bound:  3.15732569e-01\n",
      "Epoch: 15619 mean train loss:  1.01301121e-02, bound:  3.15732539e-01\n",
      "Epoch: 15620 mean train loss:  1.01292497e-02, bound:  3.15732449e-01\n",
      "Epoch: 15621 mean train loss:  1.01282820e-02, bound:  3.15732449e-01\n",
      "Epoch: 15622 mean train loss:  1.01273311e-02, bound:  3.15732360e-01\n",
      "Epoch: 15623 mean train loss:  1.01264827e-02, bound:  3.15732330e-01\n",
      "Epoch: 15624 mean train loss:  1.01254461e-02, bound:  3.15732300e-01\n",
      "Epoch: 15625 mean train loss:  1.01245642e-02, bound:  3.15732241e-01\n",
      "Epoch: 15626 mean train loss:  1.01236757e-02, bound:  3.15732181e-01\n",
      "Epoch: 15627 mean train loss:  1.01227295e-02, bound:  3.15732151e-01\n",
      "Epoch: 15628 mean train loss:  1.01217469e-02, bound:  3.15732092e-01\n",
      "Epoch: 15629 mean train loss:  1.01207942e-02, bound:  3.15732062e-01\n",
      "Epoch: 15630 mean train loss:  1.01198414e-02, bound:  3.15732002e-01\n",
      "Epoch: 15631 mean train loss:  1.01191066e-02, bound:  3.15731972e-01\n",
      "Epoch: 15632 mean train loss:  1.01180999e-02, bound:  3.15731913e-01\n",
      "Epoch: 15633 mean train loss:  1.01171080e-02, bound:  3.15731883e-01\n",
      "Epoch: 15634 mean train loss:  1.01162493e-02, bound:  3.15731853e-01\n",
      "Epoch: 15635 mean train loss:  1.01152854e-02, bound:  3.15731764e-01\n",
      "Epoch: 15636 mean train loss:  1.01143494e-02, bound:  3.15731734e-01\n",
      "Epoch: 15637 mean train loss:  1.01134861e-02, bound:  3.15731674e-01\n",
      "Epoch: 15638 mean train loss:  1.01124607e-02, bound:  3.15731645e-01\n",
      "Epoch: 15639 mean train loss:  1.01116393e-02, bound:  3.15731615e-01\n",
      "Epoch: 15640 mean train loss:  1.01106130e-02, bound:  3.15731555e-01\n",
      "Epoch: 15641 mean train loss:  1.01096937e-02, bound:  3.15731466e-01\n",
      "Epoch: 15642 mean train loss:  1.01087894e-02, bound:  3.15731436e-01\n",
      "Epoch: 15643 mean train loss:  1.01078125e-02, bound:  3.15731436e-01\n",
      "Epoch: 15644 mean train loss:  1.01069938e-02, bound:  3.15731347e-01\n",
      "Epoch: 15645 mean train loss:  1.01060374e-02, bound:  3.15731317e-01\n",
      "Epoch: 15646 mean train loss:  1.01050222e-02, bound:  3.15731227e-01\n",
      "Epoch: 15647 mean train loss:  1.01041049e-02, bound:  3.15731227e-01\n",
      "Epoch: 15648 mean train loss:  1.01032509e-02, bound:  3.15731198e-01\n",
      "Epoch: 15649 mean train loss:  1.01022227e-02, bound:  3.15731138e-01\n",
      "Epoch: 15650 mean train loss:  1.01012336e-02, bound:  3.15731108e-01\n",
      "Epoch: 15651 mean train loss:  1.01004047e-02, bound:  3.15731019e-01\n",
      "Epoch: 15652 mean train loss:  1.00993915e-02, bound:  3.15730989e-01\n",
      "Epoch: 15653 mean train loss:  1.00985086e-02, bound:  3.15730959e-01\n",
      "Epoch: 15654 mean train loss:  1.00975977e-02, bound:  3.15730900e-01\n",
      "Epoch: 15655 mean train loss:  1.00965798e-02, bound:  3.15730870e-01\n",
      "Epoch: 15656 mean train loss:  1.00957723e-02, bound:  3.15730810e-01\n",
      "Epoch: 15657 mean train loss:  1.00947497e-02, bound:  3.15730751e-01\n",
      "Epoch: 15658 mean train loss:  1.00937458e-02, bound:  3.15730751e-01\n",
      "Epoch: 15659 mean train loss:  1.00929672e-02, bound:  3.15730661e-01\n",
      "Epoch: 15660 mean train loss:  1.00920349e-02, bound:  3.15730631e-01\n",
      "Epoch: 15661 mean train loss:  1.00910300e-02, bound:  3.15730572e-01\n",
      "Epoch: 15662 mean train loss:  1.00901071e-02, bound:  3.15730542e-01\n",
      "Epoch: 15663 mean train loss:  1.00892251e-02, bound:  3.15730482e-01\n",
      "Epoch: 15664 mean train loss:  1.00881839e-02, bound:  3.15730453e-01\n",
      "Epoch: 15665 mean train loss:  1.00873131e-02, bound:  3.15730393e-01\n",
      "Epoch: 15666 mean train loss:  1.00863893e-02, bound:  3.15730333e-01\n",
      "Epoch: 15667 mean train loss:  1.00853546e-02, bound:  3.15730304e-01\n",
      "Epoch: 15668 mean train loss:  1.00845238e-02, bound:  3.15730244e-01\n",
      "Epoch: 15669 mean train loss:  1.00835515e-02, bound:  3.15730214e-01\n",
      "Epoch: 15670 mean train loss:  1.00826090e-02, bound:  3.15730125e-01\n",
      "Epoch: 15671 mean train loss:  1.00816200e-02, bound:  3.15730095e-01\n",
      "Epoch: 15672 mean train loss:  1.00808172e-02, bound:  3.15730065e-01\n",
      "Epoch: 15673 mean train loss:  1.00798402e-02, bound:  3.15730035e-01\n",
      "Epoch: 15674 mean train loss:  1.00789303e-02, bound:  3.15729946e-01\n",
      "Epoch: 15675 mean train loss:  1.00779654e-02, bound:  3.15729916e-01\n",
      "Epoch: 15676 mean train loss:  1.00770444e-02, bound:  3.15729886e-01\n",
      "Epoch: 15677 mean train loss:  1.00760423e-02, bound:  3.15729797e-01\n",
      "Epoch: 15678 mean train loss:  1.00751761e-02, bound:  3.15729797e-01\n",
      "Epoch: 15679 mean train loss:  1.00742085e-02, bound:  3.15729767e-01\n",
      "Epoch: 15680 mean train loss:  1.00733265e-02, bound:  3.15729678e-01\n",
      "Epoch: 15681 mean train loss:  1.00723282e-02, bound:  3.15729648e-01\n",
      "Epoch: 15682 mean train loss:  1.00714350e-02, bound:  3.15729618e-01\n",
      "Epoch: 15683 mean train loss:  1.00704841e-02, bound:  3.15729558e-01\n",
      "Epoch: 15684 mean train loss:  1.00696180e-02, bound:  3.15729469e-01\n",
      "Epoch: 15685 mean train loss:  1.00685563e-02, bound:  3.15729439e-01\n",
      "Epoch: 15686 mean train loss:  1.00675896e-02, bound:  3.15729409e-01\n",
      "Epoch: 15687 mean train loss:  1.00666648e-02, bound:  3.15729350e-01\n",
      "Epoch: 15688 mean train loss:  1.00657791e-02, bound:  3.15729320e-01\n",
      "Epoch: 15689 mean train loss:  1.00648711e-02, bound:  3.15729231e-01\n",
      "Epoch: 15690 mean train loss:  1.00639751e-02, bound:  3.15729201e-01\n",
      "Epoch: 15691 mean train loss:  1.00629590e-02, bound:  3.15729201e-01\n",
      "Epoch: 15692 mean train loss:  1.00619690e-02, bound:  3.15729111e-01\n",
      "Epoch: 15693 mean train loss:  1.00611085e-02, bound:  3.15729082e-01\n",
      "Epoch: 15694 mean train loss:  1.00601446e-02, bound:  3.15729022e-01\n",
      "Epoch: 15695 mean train loss:  1.00592030e-02, bound:  3.15728992e-01\n",
      "Epoch: 15696 mean train loss:  1.00582549e-02, bound:  3.15728962e-01\n",
      "Epoch: 15697 mean train loss:  1.00572919e-02, bound:  3.15728903e-01\n",
      "Epoch: 15698 mean train loss:  1.00564323e-02, bound:  3.15728843e-01\n",
      "Epoch: 15699 mean train loss:  1.00554479e-02, bound:  3.15728784e-01\n",
      "Epoch: 15700 mean train loss:  1.00544952e-02, bound:  3.15728754e-01\n",
      "Epoch: 15701 mean train loss:  1.00535415e-02, bound:  3.15728694e-01\n",
      "Epoch: 15702 mean train loss:  1.00526828e-02, bound:  3.15728664e-01\n",
      "Epoch: 15703 mean train loss:  1.00516612e-02, bound:  3.15728635e-01\n",
      "Epoch: 15704 mean train loss:  1.00507541e-02, bound:  3.15728545e-01\n",
      "Epoch: 15705 mean train loss:  1.00497585e-02, bound:  3.15728515e-01\n",
      "Epoch: 15706 mean train loss:  1.00488262e-02, bound:  3.15728456e-01\n",
      "Epoch: 15707 mean train loss:  1.00479415e-02, bound:  3.15728396e-01\n",
      "Epoch: 15708 mean train loss:  1.00469505e-02, bound:  3.15728366e-01\n",
      "Epoch: 15709 mean train loss:  1.00461002e-02, bound:  3.15728337e-01\n",
      "Epoch: 15710 mean train loss:  1.00451028e-02, bound:  3.15728307e-01\n",
      "Epoch: 15711 mean train loss:  1.00441417e-02, bound:  3.15728217e-01\n",
      "Epoch: 15712 mean train loss:  1.00432103e-02, bound:  3.15728188e-01\n",
      "Epoch: 15713 mean train loss:  1.00421626e-02, bound:  3.15728128e-01\n",
      "Epoch: 15714 mean train loss:  1.00413980e-02, bound:  3.15728098e-01\n",
      "Epoch: 15715 mean train loss:  1.00404555e-02, bound:  3.15728068e-01\n",
      "Epoch: 15716 mean train loss:  1.00393659e-02, bound:  3.15727979e-01\n",
      "Epoch: 15717 mean train loss:  1.00384727e-02, bound:  3.15727949e-01\n",
      "Epoch: 15718 mean train loss:  1.00375377e-02, bound:  3.15727890e-01\n",
      "Epoch: 15719 mean train loss:  1.00365970e-02, bound:  3.15727860e-01\n",
      "Epoch: 15720 mean train loss:  1.00357169e-02, bound:  3.15727770e-01\n",
      "Epoch: 15721 mean train loss:  1.00346478e-02, bound:  3.15727770e-01\n",
      "Epoch: 15722 mean train loss:  1.00337574e-02, bound:  3.15727681e-01\n",
      "Epoch: 15723 mean train loss:  1.00328485e-02, bound:  3.15727681e-01\n",
      "Epoch: 15724 mean train loss:  1.00318585e-02, bound:  3.15727651e-01\n",
      "Epoch: 15725 mean train loss:  1.00309066e-02, bound:  3.15727562e-01\n",
      "Epoch: 15726 mean train loss:  1.00300452e-02, bound:  3.15727532e-01\n",
      "Epoch: 15727 mean train loss:  1.00290766e-02, bound:  3.15727502e-01\n",
      "Epoch: 15728 mean train loss:  1.00281117e-02, bound:  3.15727413e-01\n",
      "Epoch: 15729 mean train loss:  1.00271711e-02, bound:  3.15727353e-01\n",
      "Epoch: 15730 mean train loss:  1.00262268e-02, bound:  3.15727323e-01\n",
      "Epoch: 15731 mean train loss:  1.00252228e-02, bound:  3.15727293e-01\n",
      "Epoch: 15732 mean train loss:  1.00243278e-02, bound:  3.15727234e-01\n",
      "Epoch: 15733 mean train loss:  1.00234151e-02, bound:  3.15727204e-01\n",
      "Epoch: 15734 mean train loss:  1.00224009e-02, bound:  3.15727115e-01\n",
      "Epoch: 15735 mean train loss:  1.00214416e-02, bound:  3.15727085e-01\n",
      "Epoch: 15736 mean train loss:  1.00205261e-02, bound:  3.15727085e-01\n",
      "Epoch: 15737 mean train loss:  1.00195734e-02, bound:  3.15727025e-01\n",
      "Epoch: 15738 mean train loss:  1.00186411e-02, bound:  3.15726966e-01\n",
      "Epoch: 15739 mean train loss:  1.00177629e-02, bound:  3.15726906e-01\n",
      "Epoch: 15740 mean train loss:  1.00167673e-02, bound:  3.15726876e-01\n",
      "Epoch: 15741 mean train loss:  1.00158183e-02, bound:  3.15726846e-01\n",
      "Epoch: 15742 mean train loss:  1.00148544e-02, bound:  3.15726757e-01\n",
      "Epoch: 15743 mean train loss:  1.00139277e-02, bound:  3.15726697e-01\n",
      "Epoch: 15744 mean train loss:  1.00129237e-02, bound:  3.15726668e-01\n",
      "Epoch: 15745 mean train loss:  1.00120185e-02, bound:  3.15726638e-01\n",
      "Epoch: 15746 mean train loss:  1.00111058e-02, bound:  3.15726578e-01\n",
      "Epoch: 15747 mean train loss:  1.00100972e-02, bound:  3.15726519e-01\n",
      "Epoch: 15748 mean train loss:  1.00092338e-02, bound:  3.15726459e-01\n",
      "Epoch: 15749 mean train loss:  1.00082597e-02, bound:  3.15726459e-01\n",
      "Epoch: 15750 mean train loss:  1.00073088e-02, bound:  3.15726399e-01\n",
      "Epoch: 15751 mean train loss:  1.00063803e-02, bound:  3.15726340e-01\n",
      "Epoch: 15752 mean train loss:  1.00054266e-02, bound:  3.15726280e-01\n",
      "Epoch: 15753 mean train loss:  1.00044953e-02, bound:  3.15726250e-01\n",
      "Epoch: 15754 mean train loss:  1.00034894e-02, bound:  3.15726221e-01\n",
      "Epoch: 15755 mean train loss:  1.00025330e-02, bound:  3.15726191e-01\n",
      "Epoch: 15756 mean train loss:  1.00016613e-02, bound:  3.15726101e-01\n",
      "Epoch: 15757 mean train loss:  1.00006768e-02, bound:  3.15726072e-01\n",
      "Epoch: 15758 mean train loss:  9.99977347e-03, bound:  3.15726012e-01\n",
      "Epoch: 15759 mean train loss:  9.99880396e-03, bound:  3.15725982e-01\n",
      "Epoch: 15760 mean train loss:  9.99786425e-03, bound:  3.15725893e-01\n",
      "Epoch: 15761 mean train loss:  9.99685097e-03, bound:  3.15725863e-01\n",
      "Epoch: 15762 mean train loss:  9.99597274e-03, bound:  3.15725833e-01\n",
      "Epoch: 15763 mean train loss:  9.99498088e-03, bound:  3.15725774e-01\n",
      "Epoch: 15764 mean train loss:  9.99392290e-03, bound:  3.15725684e-01\n",
      "Epoch: 15765 mean train loss:  9.99306608e-03, bound:  3.15725654e-01\n",
      "Epoch: 15766 mean train loss:  9.99211054e-03, bound:  3.15725654e-01\n",
      "Epoch: 15767 mean train loss:  9.99124069e-03, bound:  3.15725595e-01\n",
      "Epoch: 15768 mean train loss:  9.99019202e-03, bound:  3.15725535e-01\n",
      "Epoch: 15769 mean train loss:  9.98921040e-03, bound:  3.15725476e-01\n",
      "Epoch: 15770 mean train loss:  9.98838339e-03, bound:  3.15725446e-01\n",
      "Epoch: 15771 mean train loss:  9.98738501e-03, bound:  3.15725416e-01\n",
      "Epoch: 15772 mean train loss:  9.98642575e-03, bound:  3.15725356e-01\n",
      "Epoch: 15773 mean train loss:  9.98542923e-03, bound:  3.15725297e-01\n",
      "Epoch: 15774 mean train loss:  9.98450536e-03, bound:  3.15725237e-01\n",
      "Epoch: 15775 mean train loss:  9.98357311e-03, bound:  3.15725207e-01\n",
      "Epoch: 15776 mean train loss:  9.98258777e-03, bound:  3.15725178e-01\n",
      "Epoch: 15777 mean train loss:  9.98176448e-03, bound:  3.15725088e-01\n",
      "Epoch: 15778 mean train loss:  9.98069812e-03, bound:  3.15725058e-01\n",
      "Epoch: 15779 mean train loss:  9.97965410e-03, bound:  3.15725029e-01\n",
      "Epoch: 15780 mean train loss:  9.97871812e-03, bound:  3.15724969e-01\n",
      "Epoch: 15781 mean train loss:  9.97785944e-03, bound:  3.15724909e-01\n",
      "Epoch: 15782 mean train loss:  9.97689739e-03, bound:  3.15724850e-01\n",
      "Epoch: 15783 mean train loss:  9.97584220e-03, bound:  3.15724850e-01\n",
      "Epoch: 15784 mean train loss:  9.97505337e-03, bound:  3.15724790e-01\n",
      "Epoch: 15785 mean train loss:  9.97407641e-03, bound:  3.15724760e-01\n",
      "Epoch: 15786 mean train loss:  9.97301750e-03, bound:  3.15724671e-01\n",
      "Epoch: 15787 mean train loss:  9.97203588e-03, bound:  3.15724641e-01\n",
      "Epoch: 15788 mean train loss:  9.97113809e-03, bound:  3.15724581e-01\n",
      "Epoch: 15789 mean train loss:  9.97018814e-03, bound:  3.15724552e-01\n",
      "Epoch: 15790 mean train loss:  9.96924285e-03, bound:  3.15724462e-01\n",
      "Epoch: 15791 mean train loss:  9.96835157e-03, bound:  3.15724432e-01\n",
      "Epoch: 15792 mean train loss:  9.96731874e-03, bound:  3.15724403e-01\n",
      "Epoch: 15793 mean train loss:  9.96634271e-03, bound:  3.15724343e-01\n",
      "Epoch: 15794 mean train loss:  9.96544212e-03, bound:  3.15724283e-01\n",
      "Epoch: 15795 mean train loss:  9.96435806e-03, bound:  3.15724224e-01\n",
      "Epoch: 15796 mean train loss:  9.96343885e-03, bound:  3.15724224e-01\n",
      "Epoch: 15797 mean train loss:  9.96252615e-03, bound:  3.15724164e-01\n",
      "Epoch: 15798 mean train loss:  9.96158924e-03, bound:  3.15724105e-01\n",
      "Epoch: 15799 mean train loss:  9.96055733e-03, bound:  3.15724075e-01\n",
      "Epoch: 15800 mean train loss:  9.95960552e-03, bound:  3.15724015e-01\n",
      "Epoch: 15801 mean train loss:  9.95861273e-03, bound:  3.15723985e-01\n",
      "Epoch: 15802 mean train loss:  9.95777640e-03, bound:  3.15723926e-01\n",
      "Epoch: 15803 mean train loss:  9.95684322e-03, bound:  3.15723866e-01\n",
      "Epoch: 15804 mean train loss:  9.95584670e-03, bound:  3.15723836e-01\n",
      "Epoch: 15805 mean train loss:  9.95475519e-03, bound:  3.15723777e-01\n",
      "Epoch: 15806 mean train loss:  9.95389558e-03, bound:  3.15723717e-01\n",
      "Epoch: 15807 mean train loss:  9.95289348e-03, bound:  3.15723658e-01\n",
      "Epoch: 15808 mean train loss:  9.95202735e-03, bound:  3.15723628e-01\n",
      "Epoch: 15809 mean train loss:  9.95107926e-03, bound:  3.15723568e-01\n",
      "Epoch: 15810 mean train loss:  9.94994305e-03, bound:  3.15723538e-01\n",
      "Epoch: 15811 mean train loss:  9.94905084e-03, bound:  3.15723479e-01\n",
      "Epoch: 15812 mean train loss:  9.94806457e-03, bound:  3.15723419e-01\n",
      "Epoch: 15813 mean train loss:  9.94721707e-03, bound:  3.15723389e-01\n",
      "Epoch: 15814 mean train loss:  9.94623918e-03, bound:  3.15723300e-01\n",
      "Epoch: 15815 mean train loss:  9.94523149e-03, bound:  3.15723300e-01\n",
      "Epoch: 15816 mean train loss:  9.94422566e-03, bound:  3.15723240e-01\n",
      "Epoch: 15817 mean train loss:  9.94347129e-03, bound:  3.15723211e-01\n",
      "Epoch: 15818 mean train loss:  9.94232483e-03, bound:  3.15723121e-01\n",
      "Epoch: 15819 mean train loss:  9.94144194e-03, bound:  3.15723091e-01\n",
      "Epoch: 15820 mean train loss:  9.94044542e-03, bound:  3.15723062e-01\n",
      "Epoch: 15821 mean train loss:  9.93961375e-03, bound:  3.15723002e-01\n",
      "Epoch: 15822 mean train loss:  9.93852690e-03, bound:  3.15722913e-01\n",
      "Epoch: 15823 mean train loss:  9.93764680e-03, bound:  3.15722913e-01\n",
      "Epoch: 15824 mean train loss:  9.93659906e-03, bound:  3.15722853e-01\n",
      "Epoch: 15825 mean train loss:  9.93572455e-03, bound:  3.15722793e-01\n",
      "Epoch: 15826 mean train loss:  9.93465353e-03, bound:  3.15722764e-01\n",
      "Epoch: 15827 mean train loss:  9.93371848e-03, bound:  3.15722674e-01\n",
      "Epoch: 15828 mean train loss:  9.93279181e-03, bound:  3.15722674e-01\n",
      "Epoch: 15829 mean train loss:  9.93185211e-03, bound:  3.15722644e-01\n",
      "Epoch: 15830 mean train loss:  9.93076060e-03, bound:  3.15722555e-01\n",
      "Epoch: 15831 mean train loss:  9.92980786e-03, bound:  3.15722525e-01\n",
      "Epoch: 15832 mean train loss:  9.92890541e-03, bound:  3.15722466e-01\n",
      "Epoch: 15833 mean train loss:  9.92802903e-03, bound:  3.15722406e-01\n",
      "Epoch: 15834 mean train loss:  9.92690865e-03, bound:  3.15722406e-01\n",
      "Epoch: 15835 mean train loss:  9.92603041e-03, bound:  3.15722317e-01\n",
      "Epoch: 15836 mean train loss:  9.92506742e-03, bound:  3.15722287e-01\n",
      "Epoch: 15837 mean train loss:  9.92412679e-03, bound:  3.15722227e-01\n",
      "Epoch: 15838 mean train loss:  9.92317777e-03, bound:  3.15722167e-01\n",
      "Epoch: 15839 mean train loss:  9.92214400e-03, bound:  3.15722108e-01\n",
      "Epoch: 15840 mean train loss:  9.92117450e-03, bound:  3.15722108e-01\n",
      "Epoch: 15841 mean train loss:  9.92027391e-03, bound:  3.15722018e-01\n",
      "Epoch: 15842 mean train loss:  9.91918053e-03, bound:  3.15721989e-01\n",
      "Epoch: 15843 mean train loss:  9.91829857e-03, bound:  3.15721959e-01\n",
      "Epoch: 15844 mean train loss:  9.91739891e-03, bound:  3.15721869e-01\n",
      "Epoch: 15845 mean train loss:  9.91637167e-03, bound:  3.15721869e-01\n",
      "Epoch: 15846 mean train loss:  9.91528761e-03, bound:  3.15721780e-01\n",
      "Epoch: 15847 mean train loss:  9.91438515e-03, bound:  3.15721750e-01\n",
      "Epoch: 15848 mean train loss:  9.91342124e-03, bound:  3.15721691e-01\n",
      "Epoch: 15849 mean train loss:  9.91251972e-03, bound:  3.15721631e-01\n",
      "Epoch: 15850 mean train loss:  9.91152786e-03, bound:  3.15721571e-01\n",
      "Epoch: 15851 mean train loss:  9.91054624e-03, bound:  3.15721542e-01\n",
      "Epoch: 15852 mean train loss:  9.90959909e-03, bound:  3.15721512e-01\n",
      "Epoch: 15853 mean train loss:  9.90878791e-03, bound:  3.15721422e-01\n",
      "Epoch: 15854 mean train loss:  9.90764238e-03, bound:  3.15721393e-01\n",
      "Epoch: 15855 mean train loss:  9.90670733e-03, bound:  3.15721363e-01\n",
      "Epoch: 15856 mean train loss:  9.90569219e-03, bound:  3.15721303e-01\n",
      "Epoch: 15857 mean train loss:  9.90487821e-03, bound:  3.15721273e-01\n",
      "Epoch: 15858 mean train loss:  9.90380347e-03, bound:  3.15721214e-01\n",
      "Epoch: 15859 mean train loss:  9.90284700e-03, bound:  3.15721184e-01\n",
      "Epoch: 15860 mean train loss:  9.90188587e-03, bound:  3.15721124e-01\n",
      "Epoch: 15861 mean train loss:  9.90098622e-03, bound:  3.15721095e-01\n",
      "Epoch: 15862 mean train loss:  9.89993289e-03, bound:  3.15721005e-01\n",
      "Epoch: 15863 mean train loss:  9.89891868e-03, bound:  3.15720975e-01\n",
      "Epoch: 15864 mean train loss:  9.89791378e-03, bound:  3.15720916e-01\n",
      "Epoch: 15865 mean train loss:  9.89708491e-03, bound:  3.15720856e-01\n",
      "Epoch: 15866 mean train loss:  9.89602692e-03, bound:  3.15720797e-01\n",
      "Epoch: 15867 mean train loss:  9.89506580e-03, bound:  3.15720767e-01\n",
      "Epoch: 15868 mean train loss:  9.89409629e-03, bound:  3.15720737e-01\n",
      "Epoch: 15869 mean train loss:  9.89306159e-03, bound:  3.15720677e-01\n",
      "Epoch: 15870 mean train loss:  9.89213865e-03, bound:  3.15720648e-01\n",
      "Epoch: 15871 mean train loss:  9.89122782e-03, bound:  3.15720558e-01\n",
      "Epoch: 15872 mean train loss:  9.89021547e-03, bound:  3.15720558e-01\n",
      "Epoch: 15873 mean train loss:  9.88926552e-03, bound:  3.15720469e-01\n",
      "Epoch: 15874 mean train loss:  9.88829415e-03, bound:  3.15720439e-01\n",
      "Epoch: 15875 mean train loss:  9.88726411e-03, bound:  3.15720409e-01\n",
      "Epoch: 15876 mean train loss:  9.88627318e-03, bound:  3.15720350e-01\n",
      "Epoch: 15877 mean train loss:  9.88538004e-03, bound:  3.15720290e-01\n",
      "Epoch: 15878 mean train loss:  9.88439936e-03, bound:  3.15720290e-01\n",
      "Epoch: 15879 mean train loss:  9.88342054e-03, bound:  3.15720201e-01\n",
      "Epoch: 15880 mean train loss:  9.88249574e-03, bound:  3.15720141e-01\n",
      "Epoch: 15881 mean train loss:  9.88149177e-03, bound:  3.15720081e-01\n",
      "Epoch: 15882 mean train loss:  9.88053530e-03, bound:  3.15720081e-01\n",
      "Epoch: 15883 mean train loss:  9.87958536e-03, bound:  3.15719992e-01\n",
      "Epoch: 15884 mean train loss:  9.87859163e-03, bound:  3.15719962e-01\n",
      "Epoch: 15885 mean train loss:  9.87778883e-03, bound:  3.15719873e-01\n",
      "Epoch: 15886 mean train loss:  9.87663493e-03, bound:  3.15719873e-01\n",
      "Epoch: 15887 mean train loss:  9.87569150e-03, bound:  3.15719813e-01\n",
      "Epoch: 15888 mean train loss:  9.87467729e-03, bound:  3.15719754e-01\n",
      "Epoch: 15889 mean train loss:  9.87369195e-03, bound:  3.15719694e-01\n",
      "Epoch: 15890 mean train loss:  9.87276342e-03, bound:  3.15719664e-01\n",
      "Epoch: 15891 mean train loss:  9.87176411e-03, bound:  3.15719634e-01\n",
      "Epoch: 15892 mean train loss:  9.87078156e-03, bound:  3.15719545e-01\n",
      "Epoch: 15893 mean train loss:  9.86986514e-03, bound:  3.15719545e-01\n",
      "Epoch: 15894 mean train loss:  9.86887049e-03, bound:  3.15719455e-01\n",
      "Epoch: 15895 mean train loss:  9.86788142e-03, bound:  3.15719426e-01\n",
      "Epoch: 15896 mean train loss:  9.86696221e-03, bound:  3.15719366e-01\n",
      "Epoch: 15897 mean train loss:  9.86599643e-03, bound:  3.15719306e-01\n",
      "Epoch: 15898 mean train loss:  9.86487418e-03, bound:  3.15719277e-01\n",
      "Epoch: 15899 mean train loss:  9.86404158e-03, bound:  3.15719247e-01\n",
      "Epoch: 15900 mean train loss:  9.86300968e-03, bound:  3.15719187e-01\n",
      "Epoch: 15901 mean train loss:  9.86210816e-03, bound:  3.15719128e-01\n",
      "Epoch: 15902 mean train loss:  9.86100640e-03, bound:  3.15719098e-01\n",
      "Epoch: 15903 mean train loss:  9.86012258e-03, bound:  3.15719008e-01\n",
      "Epoch: 15904 mean train loss:  9.85908788e-03, bound:  3.15718979e-01\n",
      "Epoch: 15905 mean train loss:  9.85818263e-03, bound:  3.15718949e-01\n",
      "Epoch: 15906 mean train loss:  9.85711161e-03, bound:  3.15718889e-01\n",
      "Epoch: 15907 mean train loss:  9.85620730e-03, bound:  3.15718859e-01\n",
      "Epoch: 15908 mean train loss:  9.85520612e-03, bound:  3.15718770e-01\n",
      "Epoch: 15909 mean train loss:  9.85428132e-03, bound:  3.15718740e-01\n",
      "Epoch: 15910 mean train loss:  9.85325128e-03, bound:  3.15718681e-01\n",
      "Epoch: 15911 mean train loss:  9.85227339e-03, bound:  3.15718621e-01\n",
      "Epoch: 15912 mean train loss:  9.85129271e-03, bound:  3.15718561e-01\n",
      "Epoch: 15913 mean train loss:  9.85035766e-03, bound:  3.15718532e-01\n",
      "Epoch: 15914 mean train loss:  9.84937046e-03, bound:  3.15718532e-01\n",
      "Epoch: 15915 mean train loss:  9.84838605e-03, bound:  3.15718442e-01\n",
      "Epoch: 15916 mean train loss:  9.84739698e-03, bound:  3.15718412e-01\n",
      "Epoch: 15917 mean train loss:  9.84645449e-03, bound:  3.15718323e-01\n",
      "Epoch: 15918 mean train loss:  9.84543748e-03, bound:  3.15718323e-01\n",
      "Epoch: 15919 mean train loss:  9.84445121e-03, bound:  3.15718234e-01\n",
      "Epoch: 15920 mean train loss:  9.84349661e-03, bound:  3.15718204e-01\n",
      "Epoch: 15921 mean train loss:  9.84253921e-03, bound:  3.15718114e-01\n",
      "Epoch: 15922 mean train loss:  9.84153710e-03, bound:  3.15718114e-01\n",
      "Epoch: 15923 mean train loss:  9.84061044e-03, bound:  3.15718025e-01\n",
      "Epoch: 15924 mean train loss:  9.83949937e-03, bound:  3.15717995e-01\n",
      "Epoch: 15925 mean train loss:  9.83855501e-03, bound:  3.15717965e-01\n",
      "Epoch: 15926 mean train loss:  9.83760227e-03, bound:  3.15717906e-01\n",
      "Epoch: 15927 mean train loss:  9.83662810e-03, bound:  3.15717846e-01\n",
      "Epoch: 15928 mean train loss:  9.83563531e-03, bound:  3.15717846e-01\n",
      "Epoch: 15929 mean train loss:  9.83473565e-03, bound:  3.15717757e-01\n",
      "Epoch: 15930 mean train loss:  9.83366184e-03, bound:  3.15717727e-01\n",
      "Epoch: 15931 mean train loss:  9.83281247e-03, bound:  3.15717667e-01\n",
      "Epoch: 15932 mean train loss:  9.83174145e-03, bound:  3.15717638e-01\n",
      "Epoch: 15933 mean train loss:  9.83081758e-03, bound:  3.15717548e-01\n",
      "Epoch: 15934 mean train loss:  9.82980989e-03, bound:  3.15717518e-01\n",
      "Epoch: 15935 mean train loss:  9.82881524e-03, bound:  3.15717459e-01\n",
      "Epoch: 15936 mean train loss:  9.82779637e-03, bound:  3.15717429e-01\n",
      "Epoch: 15937 mean train loss:  9.82689671e-03, bound:  3.15717369e-01\n",
      "Epoch: 15938 mean train loss:  9.82586294e-03, bound:  3.15717310e-01\n",
      "Epoch: 15939 mean train loss:  9.82485153e-03, bound:  3.15717280e-01\n",
      "Epoch: 15940 mean train loss:  9.82388295e-03, bound:  3.15717191e-01\n",
      "Epoch: 15941 mean train loss:  9.82300751e-03, bound:  3.15717191e-01\n",
      "Epoch: 15942 mean train loss:  9.82197002e-03, bound:  3.15717101e-01\n",
      "Epoch: 15943 mean train loss:  9.82101634e-03, bound:  3.15717101e-01\n",
      "Epoch: 15944 mean train loss:  9.81998630e-03, bound:  3.15717012e-01\n",
      "Epoch: 15945 mean train loss:  9.81901400e-03, bound:  3.15716982e-01\n",
      "Epoch: 15946 mean train loss:  9.81796067e-03, bound:  3.15716892e-01\n",
      "Epoch: 15947 mean train loss:  9.81706660e-03, bound:  3.15716892e-01\n",
      "Epoch: 15948 mean train loss:  9.81608033e-03, bound:  3.15716833e-01\n",
      "Epoch: 15949 mean train loss:  9.81514249e-03, bound:  3.15716803e-01\n",
      "Epoch: 15950 mean train loss:  9.81410872e-03, bound:  3.15716743e-01\n",
      "Epoch: 15951 mean train loss:  9.81315132e-03, bound:  3.15716684e-01\n",
      "Epoch: 15952 mean train loss:  9.81209707e-03, bound:  3.15716624e-01\n",
      "Epoch: 15953 mean train loss:  9.81121697e-03, bound:  3.15716624e-01\n",
      "Epoch: 15954 mean train loss:  9.81011335e-03, bound:  3.15716535e-01\n",
      "Epoch: 15955 mean train loss:  9.80925001e-03, bound:  3.15716505e-01\n",
      "Epoch: 15956 mean train loss:  9.80822742e-03, bound:  3.15716445e-01\n",
      "Epoch: 15957 mean train loss:  9.80725233e-03, bound:  3.15716416e-01\n",
      "Epoch: 15958 mean train loss:  9.80628747e-03, bound:  3.15716326e-01\n",
      "Epoch: 15959 mean train loss:  9.80528072e-03, bound:  3.15716296e-01\n",
      "Epoch: 15960 mean train loss:  9.80429258e-03, bound:  3.15716237e-01\n",
      "Epoch: 15961 mean train loss:  9.80332214e-03, bound:  3.15716207e-01\n",
      "Epoch: 15962 mean train loss:  9.80229862e-03, bound:  3.15716177e-01\n",
      "Epoch: 15963 mean train loss:  9.80133470e-03, bound:  3.15716088e-01\n",
      "Epoch: 15964 mean train loss:  9.80030932e-03, bound:  3.15716058e-01\n",
      "Epoch: 15965 mean train loss:  9.79936682e-03, bound:  3.15715998e-01\n",
      "Epoch: 15966 mean train loss:  9.79842152e-03, bound:  3.15715969e-01\n",
      "Epoch: 15967 mean train loss:  9.79735050e-03, bound:  3.15715879e-01\n",
      "Epoch: 15968 mean train loss:  9.79642663e-03, bound:  3.15715879e-01\n",
      "Epoch: 15969 mean train loss:  9.79544595e-03, bound:  3.15715790e-01\n",
      "Epoch: 15970 mean train loss:  9.79439355e-03, bound:  3.15715760e-01\n",
      "Epoch: 15971 mean train loss:  9.79345664e-03, bound:  3.15715730e-01\n",
      "Epoch: 15972 mean train loss:  9.79253277e-03, bound:  3.15715671e-01\n",
      "Epoch: 15973 mean train loss:  9.79156978e-03, bound:  3.15715611e-01\n",
      "Epoch: 15974 mean train loss:  9.79050435e-03, bound:  3.15715551e-01\n",
      "Epoch: 15975 mean train loss:  9.78949293e-03, bound:  3.15715492e-01\n",
      "Epoch: 15976 mean train loss:  9.78855602e-03, bound:  3.15715462e-01\n",
      "Epoch: 15977 mean train loss:  9.78749525e-03, bound:  3.15715402e-01\n",
      "Epoch: 15978 mean train loss:  9.78653599e-03, bound:  3.15715402e-01\n",
      "Epoch: 15979 mean train loss:  9.78552457e-03, bound:  3.15715313e-01\n",
      "Epoch: 15980 mean train loss:  9.78462957e-03, bound:  3.15715283e-01\n",
      "Epoch: 15981 mean train loss:  9.78362095e-03, bound:  3.15715194e-01\n",
      "Epoch: 15982 mean train loss:  9.78266541e-03, bound:  3.15715164e-01\n",
      "Epoch: 15983 mean train loss:  9.78155527e-03, bound:  3.15715104e-01\n",
      "Epoch: 15984 mean train loss:  9.78055503e-03, bound:  3.15715104e-01\n",
      "Epoch: 15985 mean train loss:  9.77963675e-03, bound:  3.15714985e-01\n",
      "Epoch: 15986 mean train loss:  9.77869984e-03, bound:  3.15714985e-01\n",
      "Epoch: 15987 mean train loss:  9.77763906e-03, bound:  3.15714896e-01\n",
      "Epoch: 15988 mean train loss:  9.77674499e-03, bound:  3.15714866e-01\n",
      "Epoch: 15989 mean train loss:  9.77559574e-03, bound:  3.15714806e-01\n",
      "Epoch: 15990 mean train loss:  9.77466069e-03, bound:  3.15714777e-01\n",
      "Epoch: 15991 mean train loss:  9.77367163e-03, bound:  3.15714717e-01\n",
      "Epoch: 15992 mean train loss:  9.77266766e-03, bound:  3.15714657e-01\n",
      "Epoch: 15993 mean train loss:  9.77180619e-03, bound:  3.15714628e-01\n",
      "Epoch: 15994 mean train loss:  9.77073610e-03, bound:  3.15714568e-01\n",
      "Epoch: 15995 mean train loss:  9.76981688e-03, bound:  3.15714538e-01\n",
      "Epoch: 15996 mean train loss:  9.76878870e-03, bound:  3.15714449e-01\n",
      "Epoch: 15997 mean train loss:  9.76769999e-03, bound:  3.15714419e-01\n",
      "Epoch: 15998 mean train loss:  9.76679754e-03, bound:  3.15714389e-01\n",
      "Epoch: 15999 mean train loss:  9.76568926e-03, bound:  3.15714300e-01\n",
      "Epoch: 16000 mean train loss:  9.76472069e-03, bound:  3.15714240e-01\n",
      "Epoch: 16001 mean train loss:  9.76382196e-03, bound:  3.15714240e-01\n",
      "Epoch: 16002 mean train loss:  9.76273324e-03, bound:  3.15714180e-01\n",
      "Epoch: 16003 mean train loss:  9.76185780e-03, bound:  3.15714121e-01\n",
      "Epoch: 16004 mean train loss:  9.76091437e-03, bound:  3.15714061e-01\n",
      "Epoch: 16005 mean train loss:  9.75984987e-03, bound:  3.15714061e-01\n",
      "Epoch: 16006 mean train loss:  9.75888968e-03, bound:  3.15713972e-01\n",
      "Epoch: 16007 mean train loss:  9.75784194e-03, bound:  3.15713912e-01\n",
      "Epoch: 16008 mean train loss:  9.75693110e-03, bound:  3.15713882e-01\n",
      "Epoch: 16009 mean train loss:  9.75587219e-03, bound:  3.15713853e-01\n",
      "Epoch: 16010 mean train loss:  9.75486543e-03, bound:  3.15713763e-01\n",
      "Epoch: 16011 mean train loss:  9.75383818e-03, bound:  3.15713733e-01\n",
      "Epoch: 16012 mean train loss:  9.75284539e-03, bound:  3.15713674e-01\n",
      "Epoch: 16013 mean train loss:  9.75196995e-03, bound:  3.15713614e-01\n",
      "Epoch: 16014 mean train loss:  9.75097809e-03, bound:  3.15713555e-01\n",
      "Epoch: 16015 mean train loss:  9.74995270e-03, bound:  3.15713555e-01\n",
      "Epoch: 16016 mean train loss:  9.74889286e-03, bound:  3.15713435e-01\n",
      "Epoch: 16017 mean train loss:  9.74790473e-03, bound:  3.15713435e-01\n",
      "Epoch: 16018 mean train loss:  9.74694453e-03, bound:  3.15713406e-01\n",
      "Epoch: 16019 mean train loss:  9.74590145e-03, bound:  3.15713346e-01\n",
      "Epoch: 16020 mean train loss:  9.74494126e-03, bound:  3.15713257e-01\n",
      "Epoch: 16021 mean train loss:  9.74393077e-03, bound:  3.15713227e-01\n",
      "Epoch: 16022 mean train loss:  9.74297896e-03, bound:  3.15713197e-01\n",
      "Epoch: 16023 mean train loss:  9.74190515e-03, bound:  3.15713137e-01\n",
      "Epoch: 16024 mean train loss:  9.74094588e-03, bound:  3.15713078e-01\n",
      "Epoch: 16025 mean train loss:  9.73998662e-03, bound:  3.15713048e-01\n",
      "Epoch: 16026 mean train loss:  9.73892771e-03, bound:  3.15712988e-01\n",
      "Epoch: 16027 mean train loss:  9.73796099e-03, bound:  3.15712929e-01\n",
      "Epoch: 16028 mean train loss:  9.73704550e-03, bound:  3.15712869e-01\n",
      "Epoch: 16029 mean train loss:  9.73605085e-03, bound:  3.15712839e-01\n",
      "Epoch: 16030 mean train loss:  9.73502081e-03, bound:  3.15712750e-01\n",
      "Epoch: 16031 mean train loss:  9.73405503e-03, bound:  3.15712750e-01\n",
      "Epoch: 16032 mean train loss:  9.73303150e-03, bound:  3.15712690e-01\n",
      "Epoch: 16033 mean train loss:  9.73203499e-03, bound:  3.15712631e-01\n",
      "Epoch: 16034 mean train loss:  9.73103661e-03, bound:  3.15712571e-01\n",
      "Epoch: 16035 mean train loss:  9.73008480e-03, bound:  3.15712541e-01\n",
      "Epoch: 16036 mean train loss:  9.72907338e-03, bound:  3.15712512e-01\n",
      "Epoch: 16037 mean train loss:  9.72801447e-03, bound:  3.15712422e-01\n",
      "Epoch: 16038 mean train loss:  9.72704031e-03, bound:  3.15712392e-01\n",
      "Epoch: 16039 mean train loss:  9.72604286e-03, bound:  3.15712333e-01\n",
      "Epoch: 16040 mean train loss:  9.72504448e-03, bound:  3.15712273e-01\n",
      "Epoch: 16041 mean train loss:  9.72398277e-03, bound:  3.15712214e-01\n",
      "Epoch: 16042 mean train loss:  9.72302537e-03, bound:  3.15712184e-01\n",
      "Epoch: 16043 mean train loss:  9.72207077e-03, bound:  3.15712124e-01\n",
      "Epoch: 16044 mean train loss:  9.72108170e-03, bound:  3.15712065e-01\n",
      "Epoch: 16045 mean train loss:  9.72006377e-03, bound:  3.15712005e-01\n",
      "Epoch: 16046 mean train loss:  9.71898995e-03, bound:  3.15712005e-01\n",
      "Epoch: 16047 mean train loss:  9.71804745e-03, bound:  3.15711945e-01\n",
      "Epoch: 16048 mean train loss:  9.71709844e-03, bound:  3.15711886e-01\n",
      "Epoch: 16049 mean train loss:  9.71607119e-03, bound:  3.15711856e-01\n",
      "Epoch: 16050 mean train loss:  9.71506815e-03, bound:  3.15711766e-01\n",
      "Epoch: 16051 mean train loss:  9.71405953e-03, bound:  3.15711737e-01\n",
      "Epoch: 16052 mean train loss:  9.71310865e-03, bound:  3.15711677e-01\n",
      "Epoch: 16053 mean train loss:  9.71209817e-03, bound:  3.15711617e-01\n",
      "Epoch: 16054 mean train loss:  9.71110817e-03, bound:  3.15711558e-01\n",
      "Epoch: 16055 mean train loss:  9.71007254e-03, bound:  3.15711528e-01\n",
      "Epoch: 16056 mean train loss:  9.70907323e-03, bound:  3.15711498e-01\n",
      "Epoch: 16057 mean train loss:  9.70809627e-03, bound:  3.15711439e-01\n",
      "Epoch: 16058 mean train loss:  9.70711932e-03, bound:  3.15711379e-01\n",
      "Epoch: 16059 mean train loss:  9.70607530e-03, bound:  3.15711319e-01\n",
      "Epoch: 16060 mean train loss:  9.70508344e-03, bound:  3.15711319e-01\n",
      "Epoch: 16061 mean train loss:  9.70404129e-03, bound:  3.15711200e-01\n",
      "Epoch: 16062 mean train loss:  9.70310718e-03, bound:  3.15711200e-01\n",
      "Epoch: 16063 mean train loss:  9.70217306e-03, bound:  3.15711141e-01\n",
      "Epoch: 16064 mean train loss:  9.70113836e-03, bound:  3.15711081e-01\n",
      "Epoch: 16065 mean train loss:  9.70007200e-03, bound:  3.15711021e-01\n",
      "Epoch: 16066 mean train loss:  9.69914626e-03, bound:  3.15710992e-01\n",
      "Epoch: 16067 mean train loss:  9.69814043e-03, bound:  3.15710932e-01\n",
      "Epoch: 16068 mean train loss:  9.69704334e-03, bound:  3.15710872e-01\n",
      "Epoch: 16069 mean train loss:  9.69604030e-03, bound:  3.15710843e-01\n",
      "Epoch: 16070 mean train loss:  9.69509222e-03, bound:  3.15710783e-01\n",
      "Epoch: 16071 mean train loss:  9.69414599e-03, bound:  3.15710723e-01\n",
      "Epoch: 16072 mean train loss:  9.69314296e-03, bound:  3.15710694e-01\n",
      "Epoch: 16073 mean train loss:  9.69208777e-03, bound:  3.15710634e-01\n",
      "Epoch: 16074 mean train loss:  9.69109032e-03, bound:  3.15710574e-01\n",
      "Epoch: 16075 mean train loss:  9.69009474e-03, bound:  3.15710515e-01\n",
      "Epoch: 16076 mean train loss:  9.68906563e-03, bound:  3.15710515e-01\n",
      "Epoch: 16077 mean train loss:  9.68805514e-03, bound:  3.15710455e-01\n",
      "Epoch: 16078 mean train loss:  9.68703069e-03, bound:  3.15710396e-01\n",
      "Epoch: 16079 mean train loss:  9.68606770e-03, bound:  3.15710336e-01\n",
      "Epoch: 16080 mean train loss:  9.68499295e-03, bound:  3.15710276e-01\n",
      "Epoch: 16081 mean train loss:  9.68407374e-03, bound:  3.15710217e-01\n",
      "Epoch: 16082 mean train loss:  9.68318060e-03, bound:  3.15710187e-01\n",
      "Epoch: 16083 mean train loss:  9.68204718e-03, bound:  3.15710127e-01\n",
      "Epoch: 16084 mean train loss:  9.68108512e-03, bound:  3.15710068e-01\n",
      "Epoch: 16085 mean train loss:  9.68011841e-03, bound:  3.15710008e-01\n",
      "Epoch: 16086 mean train loss:  9.67898313e-03, bound:  3.15710008e-01\n",
      "Epoch: 16087 mean train loss:  9.67800431e-03, bound:  3.15709949e-01\n",
      "Epoch: 16088 mean train loss:  9.67708603e-03, bound:  3.15709889e-01\n",
      "Epoch: 16089 mean train loss:  9.67602059e-03, bound:  3.15709859e-01\n",
      "Epoch: 16090 mean train loss:  9.67494771e-03, bound:  3.15709770e-01\n",
      "Epoch: 16091 mean train loss:  9.67400707e-03, bound:  3.15709740e-01\n",
      "Epoch: 16092 mean train loss:  9.67306271e-03, bound:  3.15709680e-01\n",
      "Epoch: 16093 mean train loss:  9.67207458e-03, bound:  3.15709621e-01\n",
      "Epoch: 16094 mean train loss:  9.67102684e-03, bound:  3.15709561e-01\n",
      "Epoch: 16095 mean train loss:  9.67000425e-03, bound:  3.15709531e-01\n",
      "Epoch: 16096 mean train loss:  9.66903474e-03, bound:  3.15709502e-01\n",
      "Epoch: 16097 mean train loss:  9.66796745e-03, bound:  3.15709412e-01\n",
      "Epoch: 16098 mean train loss:  9.66697652e-03, bound:  3.15709382e-01\n",
      "Epoch: 16099 mean train loss:  9.66596603e-03, bound:  3.15709323e-01\n",
      "Epoch: 16100 mean train loss:  9.66502447e-03, bound:  3.15709293e-01\n",
      "Epoch: 16101 mean train loss:  9.66391992e-03, bound:  3.15709203e-01\n",
      "Epoch: 16102 mean train loss:  9.66297556e-03, bound:  3.15709174e-01\n",
      "Epoch: 16103 mean train loss:  9.66194086e-03, bound:  3.15709114e-01\n",
      "Epoch: 16104 mean train loss:  9.66093037e-03, bound:  3.15709084e-01\n",
      "Epoch: 16105 mean train loss:  9.66003537e-03, bound:  3.15709025e-01\n",
      "Epoch: 16106 mean train loss:  9.65896435e-03, bound:  3.15708995e-01\n",
      "Epoch: 16107 mean train loss:  9.65784770e-03, bound:  3.15708905e-01\n",
      "Epoch: 16108 mean train loss:  9.65696573e-03, bound:  3.15708876e-01\n",
      "Epoch: 16109 mean train loss:  9.65588912e-03, bound:  3.15708816e-01\n",
      "Epoch: 16110 mean train loss:  9.65493079e-03, bound:  3.15708756e-01\n",
      "Epoch: 16111 mean train loss:  9.65388678e-03, bound:  3.15708727e-01\n",
      "Epoch: 16112 mean train loss:  9.65292659e-03, bound:  3.15708667e-01\n",
      "Epoch: 16113 mean train loss:  9.65186954e-03, bound:  3.15708607e-01\n",
      "Epoch: 16114 mean train loss:  9.65090096e-03, bound:  3.15708578e-01\n",
      "Epoch: 16115 mean train loss:  9.64987185e-03, bound:  3.15708518e-01\n",
      "Epoch: 16116 mean train loss:  9.64875333e-03, bound:  3.15708458e-01\n",
      "Epoch: 16117 mean train loss:  9.64781176e-03, bound:  3.15708429e-01\n",
      "Epoch: 16118 mean train loss:  9.64677893e-03, bound:  3.15708339e-01\n",
      "Epoch: 16119 mean train loss:  9.64578148e-03, bound:  3.15708339e-01\n",
      "Epoch: 16120 mean train loss:  9.64474119e-03, bound:  3.15708280e-01\n",
      "Epoch: 16121 mean train loss:  9.64385364e-03, bound:  3.15708220e-01\n",
      "Epoch: 16122 mean train loss:  9.64281429e-03, bound:  3.15708190e-01\n",
      "Epoch: 16123 mean train loss:  9.64179449e-03, bound:  3.15708101e-01\n",
      "Epoch: 16124 mean train loss:  9.64078028e-03, bound:  3.15708071e-01\n",
      "Epoch: 16125 mean train loss:  9.63976420e-03, bound:  3.15708011e-01\n",
      "Epoch: 16126 mean train loss:  9.63875093e-03, bound:  3.15707952e-01\n",
      "Epoch: 16127 mean train loss:  9.63769387e-03, bound:  3.15707892e-01\n",
      "Epoch: 16128 mean train loss:  9.63679142e-03, bound:  3.15707862e-01\n",
      "Epoch: 16129 mean train loss:  9.63574927e-03, bound:  3.15707833e-01\n",
      "Epoch: 16130 mean train loss:  9.63473413e-03, bound:  3.15707743e-01\n",
      "Epoch: 16131 mean train loss:  9.63366590e-03, bound:  3.15707743e-01\n",
      "Epoch: 16132 mean train loss:  9.63283703e-03, bound:  3.15707654e-01\n",
      "Epoch: 16133 mean train loss:  9.63163469e-03, bound:  3.15707624e-01\n",
      "Epoch: 16134 mean train loss:  9.63067915e-03, bound:  3.15707535e-01\n",
      "Epoch: 16135 mean train loss:  9.62967984e-03, bound:  3.15707535e-01\n",
      "Epoch: 16136 mean train loss:  9.62866563e-03, bound:  3.15707445e-01\n",
      "Epoch: 16137 mean train loss:  9.62769892e-03, bound:  3.15707415e-01\n",
      "Epoch: 16138 mean train loss:  9.62667819e-03, bound:  3.15707386e-01\n",
      "Epoch: 16139 mean train loss:  9.62563697e-03, bound:  3.15707326e-01\n",
      "Epoch: 16140 mean train loss:  9.62470565e-03, bound:  3.15707237e-01\n",
      "Epoch: 16141 mean train loss:  9.62366816e-03, bound:  3.15707207e-01\n",
      "Epoch: 16142 mean train loss:  9.62256826e-03, bound:  3.15707177e-01\n",
      "Epoch: 16143 mean train loss:  9.62158106e-03, bound:  3.15707088e-01\n",
      "Epoch: 16144 mean train loss:  9.62064508e-03, bound:  3.15707058e-01\n",
      "Epoch: 16145 mean train loss:  9.61956196e-03, bound:  3.15706998e-01\n",
      "Epoch: 16146 mean train loss:  9.61862877e-03, bound:  3.15706939e-01\n",
      "Epoch: 16147 mean train loss:  9.61748976e-03, bound:  3.15706879e-01\n",
      "Epoch: 16148 mean train loss:  9.61652678e-03, bound:  3.15706849e-01\n",
      "Epoch: 16149 mean train loss:  9.61556844e-03, bound:  3.15706789e-01\n",
      "Epoch: 16150 mean train loss:  9.61448345e-03, bound:  3.15706760e-01\n",
      "Epoch: 16151 mean train loss:  9.61353537e-03, bound:  3.15706700e-01\n",
      "Epoch: 16152 mean train loss:  9.61248111e-03, bound:  3.15706640e-01\n",
      "Epoch: 16153 mean train loss:  9.61143989e-03, bound:  3.15706611e-01\n",
      "Epoch: 16154 mean train loss:  9.61036701e-03, bound:  3.15706521e-01\n",
      "Epoch: 16155 mean train loss:  9.60944127e-03, bound:  3.15706491e-01\n",
      "Epoch: 16156 mean train loss:  9.60841775e-03, bound:  3.15706432e-01\n",
      "Epoch: 16157 mean train loss:  9.60746408e-03, bound:  3.15706402e-01\n",
      "Epoch: 16158 mean train loss:  9.60631762e-03, bound:  3.15706342e-01\n",
      "Epoch: 16159 mean train loss:  9.60532390e-03, bound:  3.15706313e-01\n",
      "Epoch: 16160 mean train loss:  9.60431155e-03, bound:  3.15706223e-01\n",
      "Epoch: 16161 mean train loss:  9.60335135e-03, bound:  3.15706193e-01\n",
      "Epoch: 16162 mean train loss:  9.60232597e-03, bound:  3.15706104e-01\n",
      "Epoch: 16163 mean train loss:  9.60131735e-03, bound:  3.15706104e-01\n",
      "Epoch: 16164 mean train loss:  9.60033201e-03, bound:  3.15706074e-01\n",
      "Epoch: 16165 mean train loss:  9.59926751e-03, bound:  3.15705985e-01\n",
      "Epoch: 16166 mean train loss:  9.59826913e-03, bound:  3.15705895e-01\n",
      "Epoch: 16167 mean train loss:  9.59726609e-03, bound:  3.15705895e-01\n",
      "Epoch: 16168 mean train loss:  9.59626865e-03, bound:  3.15705836e-01\n",
      "Epoch: 16169 mean train loss:  9.59531777e-03, bound:  3.15705776e-01\n",
      "Epoch: 16170 mean train loss:  9.59421042e-03, bound:  3.15705746e-01\n",
      "Epoch: 16171 mean train loss:  9.59317759e-03, bound:  3.15705657e-01\n",
      "Epoch: 16172 mean train loss:  9.59224254e-03, bound:  3.15705627e-01\n",
      "Epoch: 16173 mean train loss:  9.59117617e-03, bound:  3.15705627e-01\n",
      "Epoch: 16174 mean train loss:  9.59017780e-03, bound:  3.15705508e-01\n",
      "Epoch: 16175 mean train loss:  9.58919339e-03, bound:  3.15705508e-01\n",
      "Epoch: 16176 mean train loss:  9.58805997e-03, bound:  3.15705419e-01\n",
      "Epoch: 16177 mean train loss:  9.58717056e-03, bound:  3.15705389e-01\n",
      "Epoch: 16178 mean train loss:  9.58609022e-03, bound:  3.15705329e-01\n",
      "Epoch: 16179 mean train loss:  9.58512910e-03, bound:  3.15705299e-01\n",
      "Epoch: 16180 mean train loss:  9.58408695e-03, bound:  3.15705210e-01\n",
      "Epoch: 16181 mean train loss:  9.58301779e-03, bound:  3.15705180e-01\n",
      "Epoch: 16182 mean train loss:  9.58194491e-03, bound:  3.15705121e-01\n",
      "Epoch: 16183 mean train loss:  9.58097447e-03, bound:  3.15705091e-01\n",
      "Epoch: 16184 mean train loss:  9.58002917e-03, bound:  3.15705001e-01\n",
      "Epoch: 16185 mean train loss:  9.57896467e-03, bound:  3.15704972e-01\n",
      "Epoch: 16186 mean train loss:  9.57781170e-03, bound:  3.15704942e-01\n",
      "Epoch: 16187 mean train loss:  9.57696233e-03, bound:  3.15704882e-01\n",
      "Epoch: 16188 mean train loss:  9.57591366e-03, bound:  3.15704823e-01\n",
      "Epoch: 16189 mean train loss:  9.57484823e-03, bound:  3.15704763e-01\n",
      "Epoch: 16190 mean train loss:  9.57385451e-03, bound:  3.15704703e-01\n",
      "Epoch: 16191 mean train loss:  9.57286544e-03, bound:  3.15704674e-01\n",
      "Epoch: 16192 mean train loss:  9.57183354e-03, bound:  3.15704614e-01\n",
      "Epoch: 16193 mean train loss:  9.57091246e-03, bound:  3.15704554e-01\n",
      "Epoch: 16194 mean train loss:  9.56981909e-03, bound:  3.15704525e-01\n",
      "Epoch: 16195 mean train loss:  9.56870336e-03, bound:  3.15704465e-01\n",
      "Epoch: 16196 mean train loss:  9.56773479e-03, bound:  3.15704405e-01\n",
      "Epoch: 16197 mean train loss:  9.56680160e-03, bound:  3.15704376e-01\n",
      "Epoch: 16198 mean train loss:  9.56561603e-03, bound:  3.15704316e-01\n",
      "Epoch: 16199 mean train loss:  9.56466887e-03, bound:  3.15704286e-01\n",
      "Epoch: 16200 mean train loss:  9.56364349e-03, bound:  3.15704197e-01\n",
      "Epoch: 16201 mean train loss:  9.56269447e-03, bound:  3.15704167e-01\n",
      "Epoch: 16202 mean train loss:  9.56160855e-03, bound:  3.15704077e-01\n",
      "Epoch: 16203 mean train loss:  9.56058037e-03, bound:  3.15704077e-01\n",
      "Epoch: 16204 mean train loss:  9.55960900e-03, bound:  3.15703988e-01\n",
      "Epoch: 16205 mean train loss:  9.55857243e-03, bound:  3.15703958e-01\n",
      "Epoch: 16206 mean train loss:  9.55752376e-03, bound:  3.15703899e-01\n",
      "Epoch: 16207 mean train loss:  9.55657009e-03, bound:  3.15703839e-01\n",
      "Epoch: 16208 mean train loss:  9.55551863e-03, bound:  3.15703779e-01\n",
      "Epoch: 16209 mean train loss:  9.55447741e-03, bound:  3.15703750e-01\n",
      "Epoch: 16210 mean train loss:  9.55339056e-03, bound:  3.15703660e-01\n",
      "Epoch: 16211 mean train loss:  9.55250766e-03, bound:  3.15703660e-01\n",
      "Epoch: 16212 mean train loss:  9.55135562e-03, bound:  3.15703601e-01\n",
      "Epoch: 16213 mean train loss:  9.55037307e-03, bound:  3.15703541e-01\n",
      "Epoch: 16214 mean train loss:  9.54937376e-03, bound:  3.15703511e-01\n",
      "Epoch: 16215 mean train loss:  9.54837073e-03, bound:  3.15703422e-01\n",
      "Epoch: 16216 mean train loss:  9.54734161e-03, bound:  3.15703392e-01\n",
      "Epoch: 16217 mean train loss:  9.54626873e-03, bound:  3.15703332e-01\n",
      "Epoch: 16218 mean train loss:  9.54522751e-03, bound:  3.15703273e-01\n",
      "Epoch: 16219 mean train loss:  9.54421703e-03, bound:  3.15703213e-01\n",
      "Epoch: 16220 mean train loss:  9.54329129e-03, bound:  3.15703183e-01\n",
      "Epoch: 16221 mean train loss:  9.54223424e-03, bound:  3.15703124e-01\n",
      "Epoch: 16222 mean train loss:  9.54119675e-03, bound:  3.15703094e-01\n",
      "Epoch: 16223 mean train loss:  9.54006892e-03, bound:  3.15703064e-01\n",
      "Epoch: 16224 mean train loss:  9.53910779e-03, bound:  3.15702975e-01\n",
      "Epoch: 16225 mean train loss:  9.53805633e-03, bound:  3.15702945e-01\n",
      "Epoch: 16226 mean train loss:  9.53703746e-03, bound:  3.15702856e-01\n",
      "Epoch: 16227 mean train loss:  9.53605957e-03, bound:  3.15702826e-01\n",
      "Epoch: 16228 mean train loss:  9.53503698e-03, bound:  3.15702766e-01\n",
      "Epoch: 16229 mean train loss:  9.53405444e-03, bound:  3.15702736e-01\n",
      "Epoch: 16230 mean train loss:  9.53295548e-03, bound:  3.15702677e-01\n",
      "Epoch: 16231 mean train loss:  9.53199621e-03, bound:  3.15702617e-01\n",
      "Epoch: 16232 mean train loss:  9.53095965e-03, bound:  3.15702558e-01\n",
      "Epoch: 16233 mean train loss:  9.52992775e-03, bound:  3.15702528e-01\n",
      "Epoch: 16234 mean train loss:  9.52885393e-03, bound:  3.15702498e-01\n",
      "Epoch: 16235 mean train loss:  9.52783879e-03, bound:  3.15702409e-01\n",
      "Epoch: 16236 mean train loss:  9.52678081e-03, bound:  3.15702349e-01\n",
      "Epoch: 16237 mean train loss:  9.52580851e-03, bound:  3.15702289e-01\n",
      "Epoch: 16238 mean train loss:  9.52468440e-03, bound:  3.15702260e-01\n",
      "Epoch: 16239 mean train loss:  9.52368136e-03, bound:  3.15702200e-01\n",
      "Epoch: 16240 mean train loss:  9.52266343e-03, bound:  3.15702170e-01\n",
      "Epoch: 16241 mean train loss:  9.52163246e-03, bound:  3.15702111e-01\n",
      "Epoch: 16242 mean train loss:  9.52059124e-03, bound:  3.15702081e-01\n",
      "Epoch: 16243 mean train loss:  9.51960403e-03, bound:  3.15701991e-01\n",
      "Epoch: 16244 mean train loss:  9.51857399e-03, bound:  3.15701962e-01\n",
      "Epoch: 16245 mean train loss:  9.51756723e-03, bound:  3.15701902e-01\n",
      "Epoch: 16246 mean train loss:  9.51649062e-03, bound:  3.15701842e-01\n",
      "Epoch: 16247 mean train loss:  9.51561052e-03, bound:  3.15701813e-01\n",
      "Epoch: 16248 mean train loss:  9.51446407e-03, bound:  3.15701753e-01\n",
      "Epoch: 16249 mean train loss:  9.51341726e-03, bound:  3.15701663e-01\n",
      "Epoch: 16250 mean train loss:  9.51241236e-03, bound:  3.15701634e-01\n",
      "Epoch: 16251 mean train loss:  9.51145310e-03, bound:  3.15701604e-01\n",
      "Epoch: 16252 mean train loss:  9.51032341e-03, bound:  3.15701544e-01\n",
      "Epoch: 16253 mean train loss:  9.50937998e-03, bound:  3.15701485e-01\n",
      "Epoch: 16254 mean train loss:  9.50829685e-03, bound:  3.15701425e-01\n",
      "Epoch: 16255 mean train loss:  9.50724725e-03, bound:  3.15701395e-01\n",
      "Epoch: 16256 mean train loss:  9.50634852e-03, bound:  3.15701336e-01\n",
      "Epoch: 16257 mean train loss:  9.50521976e-03, bound:  3.15701276e-01\n",
      "Epoch: 16258 mean train loss:  9.50426701e-03, bound:  3.15701216e-01\n",
      "Epoch: 16259 mean train loss:  9.50318947e-03, bound:  3.15701187e-01\n",
      "Epoch: 16260 mean train loss:  9.50204115e-03, bound:  3.15701157e-01\n",
      "Epoch: 16261 mean train loss:  9.50106978e-03, bound:  3.15701067e-01\n",
      "Epoch: 16262 mean train loss:  9.49998014e-03, bound:  3.15701067e-01\n",
      "Epoch: 16263 mean train loss:  9.49909817e-03, bound:  3.15700978e-01\n",
      "Epoch: 16264 mean train loss:  9.49800573e-03, bound:  3.15700948e-01\n",
      "Epoch: 16265 mean train loss:  9.49703529e-03, bound:  3.15700859e-01\n",
      "Epoch: 16266 mean train loss:  9.49589442e-03, bound:  3.15700829e-01\n",
      "Epoch: 16267 mean train loss:  9.49496403e-03, bound:  3.15700769e-01\n",
      "Epoch: 16268 mean train loss:  9.49387532e-03, bound:  3.15700710e-01\n",
      "Epoch: 16269 mean train loss:  9.49287601e-03, bound:  3.15700650e-01\n",
      "Epoch: 16270 mean train loss:  9.49172024e-03, bound:  3.15700620e-01\n",
      "Epoch: 16271 mean train loss:  9.49084014e-03, bound:  3.15700531e-01\n",
      "Epoch: 16272 mean train loss:  9.48980916e-03, bound:  3.15700531e-01\n",
      "Epoch: 16273 mean train loss:  9.48871300e-03, bound:  3.15700442e-01\n",
      "Epoch: 16274 mean train loss:  9.48769879e-03, bound:  3.15700412e-01\n",
      "Epoch: 16275 mean train loss:  9.48676188e-03, bound:  3.15700382e-01\n",
      "Epoch: 16276 mean train loss:  9.48568434e-03, bound:  3.15700293e-01\n",
      "Epoch: 16277 mean train loss:  9.48458072e-03, bound:  3.15700233e-01\n",
      "Epoch: 16278 mean train loss:  9.48356651e-03, bound:  3.15700203e-01\n",
      "Epoch: 16279 mean train loss:  9.48255509e-03, bound:  3.15700144e-01\n",
      "Epoch: 16280 mean train loss:  9.48153343e-03, bound:  3.15700084e-01\n",
      "Epoch: 16281 mean train loss:  9.48047545e-03, bound:  3.15700054e-01\n",
      "Epoch: 16282 mean train loss:  9.47950128e-03, bound:  3.15699965e-01\n",
      "Epoch: 16283 mean train loss:  9.47846659e-03, bound:  3.15699935e-01\n",
      "Epoch: 16284 mean train loss:  9.47733689e-03, bound:  3.15699905e-01\n",
      "Epoch: 16285 mean train loss:  9.47636552e-03, bound:  3.15699846e-01\n",
      "Epoch: 16286 mean train loss:  9.47532710e-03, bound:  3.15699786e-01\n",
      "Epoch: 16287 mean train loss:  9.47433803e-03, bound:  3.15699726e-01\n",
      "Epoch: 16288 mean train loss:  9.47324559e-03, bound:  3.15699697e-01\n",
      "Epoch: 16289 mean train loss:  9.47223417e-03, bound:  3.15699637e-01\n",
      "Epoch: 16290 mean train loss:  9.47125256e-03, bound:  3.15699607e-01\n",
      "Epoch: 16291 mean train loss:  9.47025698e-03, bound:  3.15699518e-01\n",
      "Epoch: 16292 mean train loss:  9.46907979e-03, bound:  3.15699488e-01\n",
      "Epoch: 16293 mean train loss:  9.46807768e-03, bound:  3.15699428e-01\n",
      "Epoch: 16294 mean train loss:  9.46699828e-03, bound:  3.15699369e-01\n",
      "Epoch: 16295 mean train loss:  9.46610607e-03, bound:  3.15699309e-01\n",
      "Epoch: 16296 mean train loss:  9.46501736e-03, bound:  3.15699279e-01\n",
      "Epoch: 16297 mean train loss:  9.46400873e-03, bound:  3.15699220e-01\n",
      "Epoch: 16298 mean train loss:  9.46296658e-03, bound:  3.15699160e-01\n",
      "Epoch: 16299 mean train loss:  9.46188159e-03, bound:  3.15699100e-01\n",
      "Epoch: 16300 mean train loss:  9.46086179e-03, bound:  3.15699071e-01\n",
      "Epoch: 16301 mean train loss:  9.45988949e-03, bound:  3.15698981e-01\n",
      "Epoch: 16302 mean train loss:  9.45874862e-03, bound:  3.15698981e-01\n",
      "Epoch: 16303 mean train loss:  9.45785083e-03, bound:  3.15698892e-01\n",
      "Epoch: 16304 mean train loss:  9.45676118e-03, bound:  3.15698862e-01\n",
      "Epoch: 16305 mean train loss:  9.45566595e-03, bound:  3.15698832e-01\n",
      "Epoch: 16306 mean train loss:  9.45460331e-03, bound:  3.15698743e-01\n",
      "Epoch: 16307 mean train loss:  9.45364870e-03, bound:  3.15698713e-01\n",
      "Epoch: 16308 mean train loss:  9.45252273e-03, bound:  3.15698653e-01\n",
      "Epoch: 16309 mean train loss:  9.45147593e-03, bound:  3.15698594e-01\n",
      "Epoch: 16310 mean train loss:  9.45045613e-03, bound:  3.15698534e-01\n",
      "Epoch: 16311 mean train loss:  9.44953971e-03, bound:  3.15698504e-01\n",
      "Epoch: 16312 mean train loss:  9.44841467e-03, bound:  3.15698415e-01\n",
      "Epoch: 16313 mean train loss:  9.44736041e-03, bound:  3.15698415e-01\n",
      "Epoch: 16314 mean train loss:  9.44634993e-03, bound:  3.15698326e-01\n",
      "Epoch: 16315 mean train loss:  9.44537204e-03, bound:  3.15698296e-01\n",
      "Epoch: 16316 mean train loss:  9.44428332e-03, bound:  3.15698236e-01\n",
      "Epoch: 16317 mean train loss:  9.44325980e-03, bound:  3.15698177e-01\n",
      "Epoch: 16318 mean train loss:  9.44217667e-03, bound:  3.15698117e-01\n",
      "Epoch: 16319 mean train loss:  9.44129564e-03, bound:  3.15698087e-01\n",
      "Epoch: 16320 mean train loss:  9.44005605e-03, bound:  3.15698057e-01\n",
      "Epoch: 16321 mean train loss:  9.43907723e-03, bound:  3.15697968e-01\n",
      "Epoch: 16322 mean train loss:  9.43799037e-03, bound:  3.15697938e-01\n",
      "Epoch: 16323 mean train loss:  9.43705905e-03, bound:  3.15697879e-01\n",
      "Epoch: 16324 mean train loss:  9.43603553e-03, bound:  3.15697849e-01\n",
      "Epoch: 16325 mean train loss:  9.43495147e-03, bound:  3.15697789e-01\n",
      "Epoch: 16326 mean train loss:  9.43384785e-03, bound:  3.15697730e-01\n",
      "Epoch: 16327 mean train loss:  9.43285134e-03, bound:  3.15697670e-01\n",
      "Epoch: 16328 mean train loss:  9.43188183e-03, bound:  3.15697610e-01\n",
      "Epoch: 16329 mean train loss:  9.43082757e-03, bound:  3.15697581e-01\n",
      "Epoch: 16330 mean train loss:  9.42962337e-03, bound:  3.15697521e-01\n",
      "Epoch: 16331 mean train loss:  9.42878705e-03, bound:  3.15697491e-01\n",
      "Epoch: 16332 mean train loss:  9.42765828e-03, bound:  3.15697402e-01\n",
      "Epoch: 16333 mean train loss:  9.42664780e-03, bound:  3.15697372e-01\n",
      "Epoch: 16334 mean train loss:  9.42562427e-03, bound:  3.15697312e-01\n",
      "Epoch: 16335 mean train loss:  9.42452159e-03, bound:  3.15697283e-01\n",
      "Epoch: 16336 mean train loss:  9.42349620e-03, bound:  3.15697193e-01\n",
      "Epoch: 16337 mean train loss:  9.42242704e-03, bound:  3.15697163e-01\n",
      "Epoch: 16338 mean train loss:  9.42151528e-03, bound:  3.15697074e-01\n",
      "Epoch: 16339 mean train loss:  9.42039862e-03, bound:  3.15697044e-01\n",
      "Epoch: 16340 mean train loss:  9.41932853e-03, bound:  3.15696985e-01\n",
      "Epoch: 16341 mean train loss:  9.41828638e-03, bound:  3.15696955e-01\n",
      "Epoch: 16342 mean train loss:  9.41720046e-03, bound:  3.15696865e-01\n",
      "Epoch: 16343 mean train loss:  9.41618253e-03, bound:  3.15696865e-01\n",
      "Epoch: 16344 mean train loss:  9.41517949e-03, bound:  3.15696746e-01\n",
      "Epoch: 16345 mean train loss:  9.41414759e-03, bound:  3.15696746e-01\n",
      "Epoch: 16346 mean train loss:  9.41306539e-03, bound:  3.15696657e-01\n",
      "Epoch: 16347 mean train loss:  9.41205677e-03, bound:  3.15696627e-01\n",
      "Epoch: 16348 mean train loss:  9.41094756e-03, bound:  3.15696597e-01\n",
      "Epoch: 16349 mean train loss:  9.40998271e-03, bound:  3.15696537e-01\n",
      "Epoch: 16350 mean train loss:  9.40900855e-03, bound:  3.15696448e-01\n",
      "Epoch: 16351 mean train loss:  9.40783042e-03, bound:  3.15696418e-01\n",
      "Epoch: 16352 mean train loss:  9.40687209e-03, bound:  3.15696388e-01\n",
      "Epoch: 16353 mean train loss:  9.40578803e-03, bound:  3.15696299e-01\n",
      "Epoch: 16354 mean train loss:  9.40480921e-03, bound:  3.15696269e-01\n",
      "Epoch: 16355 mean train loss:  9.40366182e-03, bound:  3.15696180e-01\n",
      "Epoch: 16356 mean train loss:  9.40278079e-03, bound:  3.15696180e-01\n",
      "Epoch: 16357 mean train loss:  9.40162037e-03, bound:  3.15696120e-01\n",
      "Epoch: 16358 mean train loss:  9.40066576e-03, bound:  3.15696061e-01\n",
      "Epoch: 16359 mean train loss:  9.39953793e-03, bound:  3.15696001e-01\n",
      "Epoch: 16360 mean train loss:  9.39843245e-03, bound:  3.15695971e-01\n",
      "Epoch: 16361 mean train loss:  9.39743966e-03, bound:  3.15695882e-01\n",
      "Epoch: 16362 mean train loss:  9.39645059e-03, bound:  3.15695852e-01\n",
      "Epoch: 16363 mean train loss:  9.39533394e-03, bound:  3.15695822e-01\n",
      "Epoch: 16364 mean train loss:  9.39425454e-03, bound:  3.15695733e-01\n",
      "Epoch: 16365 mean train loss:  9.39329434e-03, bound:  3.15695703e-01\n",
      "Epoch: 16366 mean train loss:  9.39228106e-03, bound:  3.15695643e-01\n",
      "Epoch: 16367 mean train loss:  9.39116720e-03, bound:  3.15695584e-01\n",
      "Epoch: 16368 mean train loss:  9.39018931e-03, bound:  3.15695554e-01\n",
      "Epoch: 16369 mean train loss:  9.38907266e-03, bound:  3.15695465e-01\n",
      "Epoch: 16370 mean train loss:  9.38805845e-03, bound:  3.15695435e-01\n",
      "Epoch: 16371 mean train loss:  9.38701443e-03, bound:  3.15695375e-01\n",
      "Epoch: 16372 mean train loss:  9.38588381e-03, bound:  3.15695316e-01\n",
      "Epoch: 16373 mean train loss:  9.38487891e-03, bound:  3.15695286e-01\n",
      "Epoch: 16374 mean train loss:  9.38396249e-03, bound:  3.15695256e-01\n",
      "Epoch: 16375 mean train loss:  9.38277878e-03, bound:  3.15695167e-01\n",
      "Epoch: 16376 mean train loss:  9.38177668e-03, bound:  3.15695137e-01\n",
      "Epoch: 16377 mean train loss:  9.38076992e-03, bound:  3.15695047e-01\n",
      "Epoch: 16378 mean train loss:  9.37967189e-03, bound:  3.15694988e-01\n",
      "Epoch: 16379 mean train loss:  9.37865209e-03, bound:  3.15694958e-01\n",
      "Epoch: 16380 mean train loss:  9.37761180e-03, bound:  3.15694928e-01\n",
      "Epoch: 16381 mean train loss:  9.37653612e-03, bound:  3.15694839e-01\n",
      "Epoch: 16382 mean train loss:  9.37556103e-03, bound:  3.15694809e-01\n",
      "Epoch: 16383 mean train loss:  9.37446393e-03, bound:  3.15694749e-01\n",
      "Epoch: 16384 mean train loss:  9.37340409e-03, bound:  3.15694720e-01\n",
      "Epoch: 16385 mean train loss:  9.37233213e-03, bound:  3.15694630e-01\n",
      "Epoch: 16386 mean train loss:  9.37134866e-03, bound:  3.15694600e-01\n",
      "Epoch: 16387 mean train loss:  9.37026646e-03, bound:  3.15694541e-01\n",
      "Epoch: 16388 mean train loss:  9.36929416e-03, bound:  3.15694511e-01\n",
      "Epoch: 16389 mean train loss:  9.36817750e-03, bound:  3.15694422e-01\n",
      "Epoch: 16390 mean train loss:  9.36714467e-03, bound:  3.15694392e-01\n",
      "Epoch: 16391 mean train loss:  9.36601777e-03, bound:  3.15694332e-01\n",
      "Epoch: 16392 mean train loss:  9.36499890e-03, bound:  3.15694273e-01\n",
      "Epoch: 16393 mean train loss:  9.36402939e-03, bound:  3.15694213e-01\n",
      "Epoch: 16394 mean train loss:  9.36292298e-03, bound:  3.15694153e-01\n",
      "Epoch: 16395 mean train loss:  9.36185289e-03, bound:  3.15694153e-01\n",
      "Epoch: 16396 mean train loss:  9.36079957e-03, bound:  3.15694064e-01\n",
      "Epoch: 16397 mean train loss:  9.35981702e-03, bound:  3.15694004e-01\n",
      "Epoch: 16398 mean train loss:  9.35874786e-03, bound:  3.15693974e-01\n",
      "Epoch: 16399 mean train loss:  9.35769361e-03, bound:  3.15693915e-01\n",
      "Epoch: 16400 mean train loss:  9.35666263e-03, bound:  3.15693855e-01\n",
      "Epoch: 16401 mean train loss:  9.35565401e-03, bound:  3.15693825e-01\n",
      "Epoch: 16402 mean train loss:  9.35450569e-03, bound:  3.15693736e-01\n",
      "Epoch: 16403 mean train loss:  9.35348775e-03, bound:  3.15693706e-01\n",
      "Epoch: 16404 mean train loss:  9.35243350e-03, bound:  3.15693647e-01\n",
      "Epoch: 16405 mean train loss:  9.35143419e-03, bound:  3.15693617e-01\n",
      "Epoch: 16406 mean train loss:  9.35032498e-03, bound:  3.15693527e-01\n",
      "Epoch: 16407 mean train loss:  9.34927352e-03, bound:  3.15693498e-01\n",
      "Epoch: 16408 mean train loss:  9.34831239e-03, bound:  3.15693438e-01\n",
      "Epoch: 16409 mean train loss:  9.34720039e-03, bound:  3.15693408e-01\n",
      "Epoch: 16410 mean train loss:  9.34616756e-03, bound:  3.15693319e-01\n",
      "Epoch: 16411 mean train loss:  9.34511051e-03, bound:  3.15693289e-01\n",
      "Epoch: 16412 mean train loss:  9.34413075e-03, bound:  3.15693200e-01\n",
      "Epoch: 16413 mean train loss:  9.34303086e-03, bound:  3.15693170e-01\n",
      "Epoch: 16414 mean train loss:  9.34203342e-03, bound:  3.15693110e-01\n",
      "Epoch: 16415 mean train loss:  9.34097171e-03, bound:  3.15693080e-01\n",
      "Epoch: 16416 mean train loss:  9.33988299e-03, bound:  3.15693021e-01\n",
      "Epoch: 16417 mean train loss:  9.33888648e-03, bound:  3.15692961e-01\n",
      "Epoch: 16418 mean train loss:  9.33776051e-03, bound:  3.15692931e-01\n",
      "Epoch: 16419 mean train loss:  9.33670253e-03, bound:  3.15692872e-01\n",
      "Epoch: 16420 mean train loss:  9.33561288e-03, bound:  3.15692812e-01\n",
      "Epoch: 16421 mean train loss:  9.33464244e-03, bound:  3.15692753e-01\n",
      "Epoch: 16422 mean train loss:  9.33350809e-03, bound:  3.15692723e-01\n",
      "Epoch: 16423 mean train loss:  9.33246594e-03, bound:  3.15692633e-01\n",
      "Epoch: 16424 mean train loss:  9.33135580e-03, bound:  3.15692604e-01\n",
      "Epoch: 16425 mean train loss:  9.33048129e-03, bound:  3.15692544e-01\n",
      "Epoch: 16426 mean train loss:  9.32939909e-03, bound:  3.15692514e-01\n",
      "Epoch: 16427 mean train loss:  9.32831224e-03, bound:  3.15692425e-01\n",
      "Epoch: 16428 mean train loss:  9.32721887e-03, bound:  3.15692395e-01\n",
      "Epoch: 16429 mean train loss:  9.32618883e-03, bound:  3.15692365e-01\n",
      "Epoch: 16430 mean train loss:  9.32511222e-03, bound:  3.15692306e-01\n",
      "Epoch: 16431 mean train loss:  9.32400301e-03, bound:  3.15692216e-01\n",
      "Epoch: 16432 mean train loss:  9.32299625e-03, bound:  3.15692186e-01\n",
      "Epoch: 16433 mean train loss:  9.32199508e-03, bound:  3.15692157e-01\n",
      "Epoch: 16434 mean train loss:  9.32088867e-03, bound:  3.15692067e-01\n",
      "Epoch: 16435 mean train loss:  9.31986049e-03, bound:  3.15692037e-01\n",
      "Epoch: 16436 mean train loss:  9.31881089e-03, bound:  3.15691978e-01\n",
      "Epoch: 16437 mean train loss:  9.31776222e-03, bound:  3.15691918e-01\n",
      "Epoch: 16438 mean train loss:  9.31676663e-03, bound:  3.15691859e-01\n",
      "Epoch: 16439 mean train loss:  9.31553263e-03, bound:  3.15691799e-01\n",
      "Epoch: 16440 mean train loss:  9.31455381e-03, bound:  3.15691769e-01\n",
      "Epoch: 16441 mean train loss:  9.31350142e-03, bound:  3.15691710e-01\n",
      "Epoch: 16442 mean train loss:  9.31251515e-03, bound:  3.15691650e-01\n",
      "Epoch: 16443 mean train loss:  9.31139477e-03, bound:  3.15691620e-01\n",
      "Epoch: 16444 mean train loss:  9.31036659e-03, bound:  3.15691531e-01\n",
      "Epoch: 16445 mean train loss:  9.30929556e-03, bound:  3.15691501e-01\n",
      "Epoch: 16446 mean train loss:  9.30831302e-03, bound:  3.15691441e-01\n",
      "Epoch: 16447 mean train loss:  9.30718519e-03, bound:  3.15691411e-01\n",
      "Epoch: 16448 mean train loss:  9.30616912e-03, bound:  3.15691322e-01\n",
      "Epoch: 16449 mean train loss:  9.30509530e-03, bound:  3.15691292e-01\n",
      "Epoch: 16450 mean train loss:  9.30403732e-03, bound:  3.15691233e-01\n",
      "Epoch: 16451 mean train loss:  9.30299237e-03, bound:  3.15691173e-01\n",
      "Epoch: 16452 mean train loss:  9.30190366e-03, bound:  3.15691113e-01\n",
      "Epoch: 16453 mean train loss:  9.30092856e-03, bound:  3.15691084e-01\n",
      "Epoch: 16454 mean train loss:  9.29986686e-03, bound:  3.15691024e-01\n",
      "Epoch: 16455 mean train loss:  9.29871202e-03, bound:  3.15690964e-01\n",
      "Epoch: 16456 mean train loss:  9.29770991e-03, bound:  3.15690905e-01\n",
      "Epoch: 16457 mean train loss:  9.29671060e-03, bound:  3.15690875e-01\n",
      "Epoch: 16458 mean train loss:  9.29563120e-03, bound:  3.15690815e-01\n",
      "Epoch: 16459 mean train loss:  9.29446984e-03, bound:  3.15690756e-01\n",
      "Epoch: 16460 mean train loss:  9.29346960e-03, bound:  3.15690696e-01\n",
      "Epoch: 16461 mean train loss:  9.29238554e-03, bound:  3.15690637e-01\n",
      "Epoch: 16462 mean train loss:  9.29142162e-03, bound:  3.15690607e-01\n",
      "Epoch: 16463 mean train loss:  9.29030403e-03, bound:  3.15690517e-01\n",
      "Epoch: 16464 mean train loss:  9.28926095e-03, bound:  3.15690488e-01\n",
      "Epoch: 16465 mean train loss:  9.28817969e-03, bound:  3.15690428e-01\n",
      "Epoch: 16466 mean train loss:  9.28712822e-03, bound:  3.15690368e-01\n",
      "Epoch: 16467 mean train loss:  9.28611401e-03, bound:  3.15690309e-01\n",
      "Epoch: 16468 mean train loss:  9.28500574e-03, bound:  3.15690279e-01\n",
      "Epoch: 16469 mean train loss:  9.28389747e-03, bound:  3.15690249e-01\n",
      "Epoch: 16470 mean train loss:  9.28288698e-03, bound:  3.15690160e-01\n",
      "Epoch: 16471 mean train loss:  9.28189140e-03, bound:  3.15690100e-01\n",
      "Epoch: 16472 mean train loss:  9.28083621e-03, bound:  3.15690070e-01\n",
      "Epoch: 16473 mean train loss:  9.27973446e-03, bound:  3.15690041e-01\n",
      "Epoch: 16474 mean train loss:  9.27855261e-03, bound:  3.15689951e-01\n",
      "Epoch: 16475 mean train loss:  9.27759986e-03, bound:  3.15689921e-01\n",
      "Epoch: 16476 mean train loss:  9.27653071e-03, bound:  3.15689862e-01\n",
      "Epoch: 16477 mean train loss:  9.27539449e-03, bound:  3.15689802e-01\n",
      "Epoch: 16478 mean train loss:  9.27441102e-03, bound:  3.15689743e-01\n",
      "Epoch: 16479 mean train loss:  9.27341264e-03, bound:  3.15689713e-01\n",
      "Epoch: 16480 mean train loss:  9.27224383e-03, bound:  3.15689653e-01\n",
      "Epoch: 16481 mean train loss:  9.27118491e-03, bound:  3.15689594e-01\n",
      "Epoch: 16482 mean train loss:  9.27011669e-03, bound:  3.15689534e-01\n",
      "Epoch: 16483 mean train loss:  9.26908012e-03, bound:  3.15689504e-01\n",
      "Epoch: 16484 mean train loss:  9.26800538e-03, bound:  3.15689474e-01\n",
      "Epoch: 16485 mean train loss:  9.26704425e-03, bound:  3.15689385e-01\n",
      "Epoch: 16486 mean train loss:  9.26594343e-03, bound:  3.15689325e-01\n",
      "Epoch: 16487 mean train loss:  9.26490128e-03, bound:  3.15689266e-01\n",
      "Epoch: 16488 mean train loss:  9.26381163e-03, bound:  3.15689236e-01\n",
      "Epoch: 16489 mean train loss:  9.26276762e-03, bound:  3.15689176e-01\n",
      "Epoch: 16490 mean train loss:  9.26168729e-03, bound:  3.15689147e-01\n",
      "Epoch: 16491 mean train loss:  9.26059857e-03, bound:  3.15689057e-01\n",
      "Epoch: 16492 mean train loss:  9.25959181e-03, bound:  3.15689027e-01\n",
      "Epoch: 16493 mean train loss:  9.25850961e-03, bound:  3.15688938e-01\n",
      "Epoch: 16494 mean train loss:  9.25747585e-03, bound:  3.15688938e-01\n",
      "Epoch: 16495 mean train loss:  9.25646164e-03, bound:  3.15688848e-01\n",
      "Epoch: 16496 mean train loss:  9.25535895e-03, bound:  3.15688819e-01\n",
      "Epoch: 16497 mean train loss:  9.25424229e-03, bound:  3.15688759e-01\n",
      "Epoch: 16498 mean train loss:  9.25326627e-03, bound:  3.15688729e-01\n",
      "Epoch: 16499 mean train loss:  9.25209560e-03, bound:  3.15688640e-01\n",
      "Epoch: 16500 mean train loss:  9.25119873e-03, bound:  3.15688610e-01\n",
      "Epoch: 16501 mean train loss:  9.24997590e-03, bound:  3.15688521e-01\n",
      "Epoch: 16502 mean train loss:  9.24897101e-03, bound:  3.15688491e-01\n",
      "Epoch: 16503 mean train loss:  9.24792234e-03, bound:  3.15688431e-01\n",
      "Epoch: 16504 mean train loss:  9.24689975e-03, bound:  3.15688372e-01\n",
      "Epoch: 16505 mean train loss:  9.24576540e-03, bound:  3.15688312e-01\n",
      "Epoch: 16506 mean train loss:  9.24460869e-03, bound:  3.15688252e-01\n",
      "Epoch: 16507 mean train loss:  9.24361497e-03, bound:  3.15688252e-01\n",
      "Epoch: 16508 mean train loss:  9.24256444e-03, bound:  3.15688163e-01\n",
      "Epoch: 16509 mean train loss:  9.24151856e-03, bound:  3.15688133e-01\n",
      "Epoch: 16510 mean train loss:  9.24046990e-03, bound:  3.15688074e-01\n",
      "Epoch: 16511 mean train loss:  9.23940632e-03, bound:  3.15687984e-01\n",
      "Epoch: 16512 mean train loss:  9.23838280e-03, bound:  3.15687954e-01\n",
      "Epoch: 16513 mean train loss:  9.23728384e-03, bound:  3.15687925e-01\n",
      "Epoch: 16514 mean train loss:  9.23623051e-03, bound:  3.15687835e-01\n",
      "Epoch: 16515 mean train loss:  9.23512876e-03, bound:  3.15687805e-01\n",
      "Epoch: 16516 mean train loss:  9.23411269e-03, bound:  3.15687746e-01\n",
      "Epoch: 16517 mean train loss:  9.23301280e-03, bound:  3.15687716e-01\n",
      "Epoch: 16518 mean train loss:  9.23194736e-03, bound:  3.15687627e-01\n",
      "Epoch: 16519 mean train loss:  9.23089683e-03, bound:  3.15687597e-01\n",
      "Epoch: 16520 mean train loss:  9.22983792e-03, bound:  3.15687507e-01\n",
      "Epoch: 16521 mean train loss:  9.22880135e-03, bound:  3.15687478e-01\n",
      "Epoch: 16522 mean train loss:  9.22768004e-03, bound:  3.15687418e-01\n",
      "Epoch: 16523 mean train loss:  9.22664441e-03, bound:  3.15687388e-01\n",
      "Epoch: 16524 mean train loss:  9.22560226e-03, bound:  3.15687299e-01\n",
      "Epoch: 16525 mean train loss:  9.22454149e-03, bound:  3.15687299e-01\n",
      "Epoch: 16526 mean train loss:  9.22347140e-03, bound:  3.15687209e-01\n",
      "Epoch: 16527 mean train loss:  9.22237337e-03, bound:  3.15687180e-01\n",
      "Epoch: 16528 mean train loss:  9.22135264e-03, bound:  3.15687120e-01\n",
      "Epoch: 16529 mean train loss:  9.22018383e-03, bound:  3.15687060e-01\n",
      "Epoch: 16530 mean train loss:  9.21919849e-03, bound:  3.15687031e-01\n",
      "Epoch: 16531 mean train loss:  9.21811070e-03, bound:  3.15686941e-01\n",
      "Epoch: 16532 mean train loss:  9.21705645e-03, bound:  3.15686911e-01\n",
      "Epoch: 16533 mean train loss:  9.21598263e-03, bound:  3.15686822e-01\n",
      "Epoch: 16534 mean train loss:  9.21492092e-03, bound:  3.15686822e-01\n",
      "Epoch: 16535 mean train loss:  9.21383034e-03, bound:  3.15686703e-01\n",
      "Epoch: 16536 mean train loss:  9.21274070e-03, bound:  3.15686703e-01\n",
      "Epoch: 16537 mean train loss:  9.21175629e-03, bound:  3.15686643e-01\n",
      "Epoch: 16538 mean train loss:  9.21063777e-03, bound:  3.15686584e-01\n",
      "Epoch: 16539 mean train loss:  9.20970086e-03, bound:  3.15686524e-01\n",
      "Epoch: 16540 mean train loss:  9.20851342e-03, bound:  3.15686464e-01\n",
      "Epoch: 16541 mean train loss:  9.20745265e-03, bound:  3.15686405e-01\n",
      "Epoch: 16542 mean train loss:  9.20633227e-03, bound:  3.15686375e-01\n",
      "Epoch: 16543 mean train loss:  9.20535717e-03, bound:  3.15686315e-01\n",
      "Epoch: 16544 mean train loss:  9.20420792e-03, bound:  3.15686256e-01\n",
      "Epoch: 16545 mean train loss:  9.20320489e-03, bound:  3.15686196e-01\n",
      "Epoch: 16546 mean train loss:  9.20224562e-03, bound:  3.15686136e-01\n",
      "Epoch: 16547 mean train loss:  9.20103025e-03, bound:  3.15686077e-01\n",
      "Epoch: 16548 mean train loss:  9.20007657e-03, bound:  3.15686047e-01\n",
      "Epoch: 16549 mean train loss:  9.19887424e-03, bound:  3.15686017e-01\n",
      "Epoch: 16550 mean train loss:  9.19782463e-03, bound:  3.15685958e-01\n",
      "Epoch: 16551 mean train loss:  9.19674337e-03, bound:  3.15685868e-01\n",
      "Epoch: 16552 mean train loss:  9.19574033e-03, bound:  3.15685838e-01\n",
      "Epoch: 16553 mean train loss:  9.19463020e-03, bound:  3.15685809e-01\n",
      "Epoch: 16554 mean train loss:  9.19359364e-03, bound:  3.15685719e-01\n",
      "Epoch: 16555 mean train loss:  9.19252262e-03, bound:  3.15685689e-01\n",
      "Epoch: 16556 mean train loss:  9.19147767e-03, bound:  3.15685600e-01\n",
      "Epoch: 16557 mean train loss:  9.19037033e-03, bound:  3.15685570e-01\n",
      "Epoch: 16558 mean train loss:  9.18929558e-03, bound:  3.15685511e-01\n",
      "Epoch: 16559 mean train loss:  9.18820594e-03, bound:  3.15685481e-01\n",
      "Epoch: 16560 mean train loss:  9.18721221e-03, bound:  3.15685391e-01\n",
      "Epoch: 16561 mean train loss:  9.18614306e-03, bound:  3.15685391e-01\n",
      "Epoch: 16562 mean train loss:  9.18504503e-03, bound:  3.15685272e-01\n",
      "Epoch: 16563 mean train loss:  9.18396935e-03, bound:  3.15685272e-01\n",
      "Epoch: 16564 mean train loss:  9.18295607e-03, bound:  3.15685183e-01\n",
      "Epoch: 16565 mean train loss:  9.18180589e-03, bound:  3.15685153e-01\n",
      "Epoch: 16566 mean train loss:  9.18078981e-03, bound:  3.15685093e-01\n",
      "Epoch: 16567 mean train loss:  9.17963590e-03, bound:  3.15685034e-01\n",
      "Epoch: 16568 mean train loss:  9.17859189e-03, bound:  3.15684974e-01\n",
      "Epoch: 16569 mean train loss:  9.17757489e-03, bound:  3.15684915e-01\n",
      "Epoch: 16570 mean train loss:  9.17640608e-03, bound:  3.15684915e-01\n",
      "Epoch: 16571 mean train loss:  9.17544309e-03, bound:  3.15684825e-01\n",
      "Epoch: 16572 mean train loss:  9.17436182e-03, bound:  3.15684766e-01\n",
      "Epoch: 16573 mean train loss:  9.17327311e-03, bound:  3.15684706e-01\n",
      "Epoch: 16574 mean train loss:  9.17217135e-03, bound:  3.15684676e-01\n",
      "Epoch: 16575 mean train loss:  9.17114038e-03, bound:  3.15684617e-01\n",
      "Epoch: 16576 mean train loss:  9.17012524e-03, bound:  3.15684587e-01\n",
      "Epoch: 16577 mean train loss:  9.16895177e-03, bound:  3.15684497e-01\n",
      "Epoch: 16578 mean train loss:  9.16789006e-03, bound:  3.15684468e-01\n",
      "Epoch: 16579 mean train loss:  9.16677807e-03, bound:  3.15684378e-01\n",
      "Epoch: 16580 mean train loss:  9.16573964e-03, bound:  3.15684348e-01\n",
      "Epoch: 16581 mean train loss:  9.16477200e-03, bound:  3.15684289e-01\n",
      "Epoch: 16582 mean train loss:  9.16369166e-03, bound:  3.15684259e-01\n",
      "Epoch: 16583 mean train loss:  9.16255172e-03, bound:  3.15684170e-01\n",
      "Epoch: 16584 mean train loss:  9.16149188e-03, bound:  3.15684140e-01\n",
      "Epoch: 16585 mean train loss:  9.16041154e-03, bound:  3.15684050e-01\n",
      "Epoch: 16586 mean train loss:  9.15935729e-03, bound:  3.15684050e-01\n",
      "Epoch: 16587 mean train loss:  9.15822294e-03, bound:  3.15683961e-01\n",
      "Epoch: 16588 mean train loss:  9.15712025e-03, bound:  3.15683931e-01\n",
      "Epoch: 16589 mean train loss:  9.15612653e-03, bound:  3.15683842e-01\n",
      "Epoch: 16590 mean train loss:  9.15499777e-03, bound:  3.15683812e-01\n",
      "Epoch: 16591 mean train loss:  9.15387925e-03, bound:  3.15683752e-01\n",
      "Epoch: 16592 mean train loss:  9.15286131e-03, bound:  3.15683693e-01\n",
      "Epoch: 16593 mean train loss:  9.15181357e-03, bound:  3.15683633e-01\n",
      "Epoch: 16594 mean train loss:  9.15078260e-03, bound:  3.15683573e-01\n",
      "Epoch: 16595 mean train loss:  9.14964732e-03, bound:  3.15683514e-01\n",
      "Epoch: 16596 mean train loss:  9.14860517e-03, bound:  3.15683484e-01\n",
      "Epoch: 16597 mean train loss:  9.14754905e-03, bound:  3.15683484e-01\n",
      "Epoch: 16598 mean train loss:  9.14642680e-03, bound:  3.15683365e-01\n",
      "Epoch: 16599 mean train loss:  9.14536510e-03, bound:  3.15683365e-01\n",
      "Epoch: 16600 mean train loss:  9.14424472e-03, bound:  3.15683246e-01\n",
      "Epoch: 16601 mean train loss:  9.14328266e-03, bound:  3.15683246e-01\n",
      "Epoch: 16602 mean train loss:  9.14218277e-03, bound:  3.15683186e-01\n",
      "Epoch: 16603 mean train loss:  9.14104562e-03, bound:  3.15683126e-01\n",
      "Epoch: 16604 mean train loss:  9.14008170e-03, bound:  3.15683067e-01\n",
      "Epoch: 16605 mean train loss:  9.13880952e-03, bound:  3.15683007e-01\n",
      "Epoch: 16606 mean train loss:  9.13790241e-03, bound:  3.15682948e-01\n",
      "Epoch: 16607 mean train loss:  9.13683139e-03, bound:  3.15682918e-01\n",
      "Epoch: 16608 mean train loss:  9.13566165e-03, bound:  3.15682828e-01\n",
      "Epoch: 16609 mean train loss:  9.13460925e-03, bound:  3.15682799e-01\n",
      "Epoch: 16610 mean train loss:  9.13357362e-03, bound:  3.15682739e-01\n",
      "Epoch: 16611 mean train loss:  9.13255662e-03, bound:  3.15682679e-01\n",
      "Epoch: 16612 mean train loss:  9.13139712e-03, bound:  3.15682650e-01\n",
      "Epoch: 16613 mean train loss:  9.13027395e-03, bound:  3.15682560e-01\n",
      "Epoch: 16614 mean train loss:  9.12927836e-03, bound:  3.15682530e-01\n",
      "Epoch: 16615 mean train loss:  9.12811700e-03, bound:  3.15682471e-01\n",
      "Epoch: 16616 mean train loss:  9.12711024e-03, bound:  3.15682411e-01\n",
      "Epoch: 16617 mean train loss:  9.12603363e-03, bound:  3.15682381e-01\n",
      "Epoch: 16618 mean train loss:  9.12490953e-03, bound:  3.15682292e-01\n",
      "Epoch: 16619 mean train loss:  9.12388414e-03, bound:  3.15682262e-01\n",
      "Epoch: 16620 mean train loss:  9.12273303e-03, bound:  3.15682173e-01\n",
      "Epoch: 16621 mean train loss:  9.12170950e-03, bound:  3.15682143e-01\n",
      "Epoch: 16622 mean train loss:  9.12065711e-03, bound:  3.15682113e-01\n",
      "Epoch: 16623 mean train loss:  9.11951624e-03, bound:  3.15682024e-01\n",
      "Epoch: 16624 mean train loss:  9.11839213e-03, bound:  3.15681964e-01\n",
      "Epoch: 16625 mean train loss:  9.11747478e-03, bound:  3.15681934e-01\n",
      "Epoch: 16626 mean train loss:  9.11623985e-03, bound:  3.15681845e-01\n",
      "Epoch: 16627 mean train loss:  9.11520701e-03, bound:  3.15681815e-01\n",
      "Epoch: 16628 mean train loss:  9.11424868e-03, bound:  3.15681726e-01\n",
      "Epoch: 16629 mean train loss:  9.11315624e-03, bound:  3.15681726e-01\n",
      "Epoch: 16630 mean train loss:  9.11196042e-03, bound:  3.15681696e-01\n",
      "Epoch: 16631 mean train loss:  9.11087543e-03, bound:  3.15681607e-01\n",
      "Epoch: 16632 mean train loss:  9.10987239e-03, bound:  3.15681577e-01\n",
      "Epoch: 16633 mean train loss:  9.10881441e-03, bound:  3.15681517e-01\n",
      "Epoch: 16634 mean train loss:  9.10767354e-03, bound:  3.15681458e-01\n",
      "Epoch: 16635 mean train loss:  9.10659228e-03, bound:  3.15681398e-01\n",
      "Epoch: 16636 mean train loss:  9.10553988e-03, bound:  3.15681338e-01\n",
      "Epoch: 16637 mean train loss:  9.10449959e-03, bound:  3.15681279e-01\n",
      "Epoch: 16638 mean train loss:  9.10334289e-03, bound:  3.15681249e-01\n",
      "Epoch: 16639 mean train loss:  9.10230447e-03, bound:  3.15681189e-01\n",
      "Epoch: 16640 mean train loss:  9.10123251e-03, bound:  3.15681130e-01\n",
      "Epoch: 16641 mean train loss:  9.10019781e-03, bound:  3.15681070e-01\n",
      "Epoch: 16642 mean train loss:  9.09905229e-03, bound:  3.15681010e-01\n",
      "Epoch: 16643 mean train loss:  9.09794960e-03, bound:  3.15681010e-01\n",
      "Epoch: 16644 mean train loss:  9.09694098e-03, bound:  3.15680921e-01\n",
      "Epoch: 16645 mean train loss:  9.09584109e-03, bound:  3.15680861e-01\n",
      "Epoch: 16646 mean train loss:  9.09465645e-03, bound:  3.15680832e-01\n",
      "Epoch: 16647 mean train loss:  9.09371022e-03, bound:  3.15680742e-01\n",
      "Epoch: 16648 mean train loss:  9.09251999e-03, bound:  3.15680712e-01\n",
      "Epoch: 16649 mean train loss:  9.09148157e-03, bound:  3.15680623e-01\n",
      "Epoch: 16650 mean train loss:  9.09038633e-03, bound:  3.15680593e-01\n",
      "Epoch: 16651 mean train loss:  9.08932276e-03, bound:  3.15680534e-01\n",
      "Epoch: 16652 mean train loss:  9.08826012e-03, bound:  3.15680504e-01\n",
      "Epoch: 16653 mean train loss:  9.08725802e-03, bound:  3.15680414e-01\n",
      "Epoch: 16654 mean train loss:  9.08600260e-03, bound:  3.15680385e-01\n",
      "Epoch: 16655 mean train loss:  9.08495393e-03, bound:  3.15680325e-01\n",
      "Epoch: 16656 mean train loss:  9.08398535e-03, bound:  3.15680265e-01\n",
      "Epoch: 16657 mean train loss:  9.08294506e-03, bound:  3.15680206e-01\n",
      "Epoch: 16658 mean train loss:  9.08172783e-03, bound:  3.15680176e-01\n",
      "Epoch: 16659 mean train loss:  9.08068754e-03, bound:  3.15680116e-01\n",
      "Epoch: 16660 mean train loss:  9.07951500e-03, bound:  3.15680027e-01\n",
      "Epoch: 16661 mean train loss:  9.07854177e-03, bound:  3.15679997e-01\n",
      "Epoch: 16662 mean train loss:  9.07740463e-03, bound:  3.15679938e-01\n",
      "Epoch: 16663 mean train loss:  9.07635409e-03, bound:  3.15679908e-01\n",
      "Epoch: 16664 mean train loss:  9.07527469e-03, bound:  3.15679818e-01\n",
      "Epoch: 16665 mean train loss:  9.07415338e-03, bound:  3.15679789e-01\n",
      "Epoch: 16666 mean train loss:  9.07316525e-03, bound:  3.15679729e-01\n",
      "Epoch: 16667 mean train loss:  9.07202624e-03, bound:  3.15679699e-01\n",
      "Epoch: 16668 mean train loss:  9.07091796e-03, bound:  3.15679610e-01\n",
      "Epoch: 16669 mean train loss:  9.06992238e-03, bound:  3.15679580e-01\n",
      "Epoch: 16670 mean train loss:  9.06876195e-03, bound:  3.15679491e-01\n",
      "Epoch: 16671 mean train loss:  9.06769652e-03, bound:  3.15679461e-01\n",
      "Epoch: 16672 mean train loss:  9.06667672e-03, bound:  3.15679401e-01\n",
      "Epoch: 16673 mean train loss:  9.06556752e-03, bound:  3.15679342e-01\n",
      "Epoch: 16674 mean train loss:  9.06441826e-03, bound:  3.15679282e-01\n",
      "Epoch: 16675 mean train loss:  9.06333979e-03, bound:  3.15679252e-01\n",
      "Epoch: 16676 mean train loss:  9.06226411e-03, bound:  3.15679163e-01\n",
      "Epoch: 16677 mean train loss:  9.06122662e-03, bound:  3.15679133e-01\n",
      "Epoch: 16678 mean train loss:  9.06014442e-03, bound:  3.15679073e-01\n",
      "Epoch: 16679 mean train loss:  9.05904826e-03, bound:  3.15679014e-01\n",
      "Epoch: 16680 mean train loss:  9.05788783e-03, bound:  3.15678954e-01\n",
      "Epoch: 16681 mean train loss:  9.05688200e-03, bound:  3.15678924e-01\n",
      "Epoch: 16682 mean train loss:  9.05572902e-03, bound:  3.15678895e-01\n",
      "Epoch: 16683 mean train loss:  9.05475114e-03, bound:  3.15678805e-01\n",
      "Epoch: 16684 mean train loss:  9.05365590e-03, bound:  3.15678775e-01\n",
      "Epoch: 16685 mean train loss:  9.05257463e-03, bound:  3.15678716e-01\n",
      "Epoch: 16686 mean train loss:  9.05150361e-03, bound:  3.15678686e-01\n",
      "Epoch: 16687 mean train loss:  9.05028731e-03, bound:  3.15678596e-01\n",
      "Epoch: 16688 mean train loss:  9.04921256e-03, bound:  3.15678567e-01\n",
      "Epoch: 16689 mean train loss:  9.04816948e-03, bound:  3.15678477e-01\n",
      "Epoch: 16690 mean train loss:  9.04708263e-03, bound:  3.15678418e-01\n",
      "Epoch: 16691 mean train loss:  9.04599205e-03, bound:  3.15678388e-01\n",
      "Epoch: 16692 mean train loss:  9.04497411e-03, bound:  3.15678328e-01\n",
      "Epoch: 16693 mean train loss:  9.04396828e-03, bound:  3.15678269e-01\n",
      "Epoch: 16694 mean train loss:  9.04283300e-03, bound:  3.15678239e-01\n",
      "Epoch: 16695 mean train loss:  9.04168840e-03, bound:  3.15678149e-01\n",
      "Epoch: 16696 mean train loss:  9.04060155e-03, bound:  3.15678120e-01\n",
      "Epoch: 16697 mean train loss:  9.03946906e-03, bound:  3.15678060e-01\n",
      "Epoch: 16698 mean train loss:  9.03846044e-03, bound:  3.15678000e-01\n",
      "Epoch: 16699 mean train loss:  9.03737545e-03, bound:  3.15677971e-01\n",
      "Epoch: 16700 mean train loss:  9.03620478e-03, bound:  3.15677881e-01\n",
      "Epoch: 16701 mean train loss:  9.03514307e-03, bound:  3.15677822e-01\n",
      "Epoch: 16702 mean train loss:  9.03411210e-03, bound:  3.15677792e-01\n",
      "Epoch: 16703 mean train loss:  9.03296098e-03, bound:  3.15677732e-01\n",
      "Epoch: 16704 mean train loss:  9.03188158e-03, bound:  3.15677673e-01\n",
      "Epoch: 16705 mean train loss:  9.03081894e-03, bound:  3.15677613e-01\n",
      "Epoch: 16706 mean train loss:  9.02965572e-03, bound:  3.15677583e-01\n",
      "Epoch: 16707 mean train loss:  9.02877841e-03, bound:  3.15677524e-01\n",
      "Epoch: 16708 mean train loss:  9.02757049e-03, bound:  3.15677464e-01\n",
      "Epoch: 16709 mean train loss:  9.02652927e-03, bound:  3.15677434e-01\n",
      "Epoch: 16710 mean train loss:  9.02536884e-03, bound:  3.15677345e-01\n",
      "Epoch: 16711 mean train loss:  9.02426615e-03, bound:  3.15677285e-01\n",
      "Epoch: 16712 mean train loss:  9.02326778e-03, bound:  3.15677255e-01\n",
      "Epoch: 16713 mean train loss:  9.02216509e-03, bound:  3.15677166e-01\n",
      "Epoch: 16714 mean train loss:  9.02105682e-03, bound:  3.15677136e-01\n",
      "Epoch: 16715 mean train loss:  9.01996996e-03, bound:  3.15677106e-01\n",
      "Epoch: 16716 mean train loss:  9.01891012e-03, bound:  3.15677017e-01\n",
      "Epoch: 16717 mean train loss:  9.01779812e-03, bound:  3.15676957e-01\n",
      "Epoch: 16718 mean train loss:  9.01659578e-03, bound:  3.15676928e-01\n",
      "Epoch: 16719 mean train loss:  9.01563279e-03, bound:  3.15676898e-01\n",
      "Epoch: 16720 mean train loss:  9.01454687e-03, bound:  3.15676838e-01\n",
      "Epoch: 16721 mean train loss:  9.01335478e-03, bound:  3.15676779e-01\n",
      "Epoch: 16722 mean train loss:  9.01228562e-03, bound:  3.15676719e-01\n",
      "Epoch: 16723 mean train loss:  9.01133660e-03, bound:  3.15676689e-01\n",
      "Epoch: 16724 mean train loss:  9.01008863e-03, bound:  3.15676600e-01\n",
      "Epoch: 16725 mean train loss:  9.00905021e-03, bound:  3.15676570e-01\n",
      "Epoch: 16726 mean train loss:  9.00804624e-03, bound:  3.15676481e-01\n",
      "Epoch: 16727 mean train loss:  9.00692586e-03, bound:  3.15676451e-01\n",
      "Epoch: 16728 mean train loss:  9.00577940e-03, bound:  3.15676391e-01\n",
      "Epoch: 16729 mean train loss:  9.00462270e-03, bound:  3.15676332e-01\n",
      "Epoch: 16730 mean train loss:  9.00357869e-03, bound:  3.15676272e-01\n",
      "Epoch: 16731 mean train loss:  9.00251977e-03, bound:  3.15676242e-01\n",
      "Epoch: 16732 mean train loss:  9.00135934e-03, bound:  3.15676153e-01\n",
      "Epoch: 16733 mean train loss:  9.00039822e-03, bound:  3.15676123e-01\n",
      "Epoch: 16734 mean train loss:  8.99919961e-03, bound:  3.15676063e-01\n",
      "Epoch: 16735 mean train loss:  8.99817422e-03, bound:  3.15676004e-01\n",
      "Epoch: 16736 mean train loss:  8.99710599e-03, bound:  3.15675944e-01\n",
      "Epoch: 16737 mean train loss:  8.99601914e-03, bound:  3.15675914e-01\n",
      "Epoch: 16738 mean train loss:  8.99493881e-03, bound:  3.15675825e-01\n",
      "Epoch: 16739 mean train loss:  8.99380073e-03, bound:  3.15675795e-01\n",
      "Epoch: 16740 mean train loss:  8.99270177e-03, bound:  3.15675735e-01\n",
      "Epoch: 16741 mean train loss:  8.99162516e-03, bound:  3.15675676e-01\n",
      "Epoch: 16742 mean train loss:  8.99050105e-03, bound:  3.15675616e-01\n",
      "Epoch: 16743 mean train loss:  8.98950547e-03, bound:  3.15675586e-01\n",
      "Epoch: 16744 mean train loss:  8.98841489e-03, bound:  3.15675497e-01\n",
      "Epoch: 16745 mean train loss:  8.98725726e-03, bound:  3.15675467e-01\n",
      "Epoch: 16746 mean train loss:  8.98625609e-03, bound:  3.15675408e-01\n",
      "Epoch: 16747 mean train loss:  8.98515247e-03, bound:  3.15675348e-01\n",
      "Epoch: 16748 mean train loss:  8.98399297e-03, bound:  3.15675318e-01\n",
      "Epoch: 16749 mean train loss:  8.98284093e-03, bound:  3.15675229e-01\n",
      "Epoch: 16750 mean train loss:  8.98184534e-03, bound:  3.15675229e-01\n",
      "Epoch: 16751 mean train loss:  8.98076128e-03, bound:  3.15675139e-01\n",
      "Epoch: 16752 mean train loss:  8.97961296e-03, bound:  3.15675110e-01\n",
      "Epoch: 16753 mean train loss:  8.97857081e-03, bound:  3.15675020e-01\n",
      "Epoch: 16754 mean train loss:  8.97744019e-03, bound:  3.15674990e-01\n",
      "Epoch: 16755 mean train loss:  8.97635613e-03, bound:  3.15674931e-01\n",
      "Epoch: 16756 mean train loss:  8.97528324e-03, bound:  3.15674841e-01\n",
      "Epoch: 16757 mean train loss:  8.97416286e-03, bound:  3.15674812e-01\n",
      "Epoch: 16758 mean train loss:  8.97311792e-03, bound:  3.15674722e-01\n",
      "Epoch: 16759 mean train loss:  8.97199102e-03, bound:  3.15674722e-01\n",
      "Epoch: 16760 mean train loss:  8.97092931e-03, bound:  3.15674663e-01\n",
      "Epoch: 16761 mean train loss:  8.96986015e-03, bound:  3.15674603e-01\n",
      "Epoch: 16762 mean train loss:  8.96869227e-03, bound:  3.15674573e-01\n",
      "Epoch: 16763 mean train loss:  8.96763895e-03, bound:  3.15674484e-01\n",
      "Epoch: 16764 mean train loss:  8.96649342e-03, bound:  3.15674454e-01\n",
      "Epoch: 16765 mean train loss:  8.96551646e-03, bound:  3.15674394e-01\n",
      "Epoch: 16766 mean train loss:  8.96442495e-03, bound:  3.15674335e-01\n",
      "Epoch: 16767 mean train loss:  8.96326918e-03, bound:  3.15674275e-01\n",
      "Epoch: 16768 mean train loss:  8.96212365e-03, bound:  3.15674216e-01\n",
      "Epoch: 16769 mean train loss:  8.96114763e-03, bound:  3.15674156e-01\n",
      "Epoch: 16770 mean train loss:  8.95996671e-03, bound:  3.15674096e-01\n",
      "Epoch: 16771 mean train loss:  8.95893481e-03, bound:  3.15674037e-01\n",
      "Epoch: 16772 mean train loss:  8.95779580e-03, bound:  3.15674007e-01\n",
      "Epoch: 16773 mean train loss:  8.95669032e-03, bound:  3.15673918e-01\n",
      "Epoch: 16774 mean train loss:  8.95570498e-03, bound:  3.15673918e-01\n",
      "Epoch: 16775 mean train loss:  8.95455666e-03, bound:  3.15673828e-01\n",
      "Epoch: 16776 mean train loss:  8.95352010e-03, bound:  3.15673798e-01\n",
      "Epoch: 16777 mean train loss:  8.95232335e-03, bound:  3.15673739e-01\n",
      "Epoch: 16778 mean train loss:  8.95121414e-03, bound:  3.15673679e-01\n",
      "Epoch: 16779 mean train loss:  8.95021204e-03, bound:  3.15673620e-01\n",
      "Epoch: 16780 mean train loss:  8.94898735e-03, bound:  3.15673590e-01\n",
      "Epoch: 16781 mean train loss:  8.94793589e-03, bound:  3.15673500e-01\n",
      "Epoch: 16782 mean train loss:  8.94678384e-03, bound:  3.15673470e-01\n",
      "Epoch: 16783 mean train loss:  8.94569792e-03, bound:  3.15673381e-01\n",
      "Epoch: 16784 mean train loss:  8.94468185e-03, bound:  3.15673351e-01\n",
      "Epoch: 16785 mean train loss:  8.94350279e-03, bound:  3.15673292e-01\n",
      "Epoch: 16786 mean train loss:  8.94241780e-03, bound:  3.15673232e-01\n",
      "Epoch: 16787 mean train loss:  8.94139055e-03, bound:  3.15673202e-01\n",
      "Epoch: 16788 mean train loss:  8.94026738e-03, bound:  3.15673143e-01\n",
      "Epoch: 16789 mean train loss:  8.93929135e-03, bound:  3.15673053e-01\n",
      "Epoch: 16790 mean train loss:  8.93811695e-03, bound:  3.15673023e-01\n",
      "Epoch: 16791 mean train loss:  8.93696491e-03, bound:  3.15672934e-01\n",
      "Epoch: 16792 mean train loss:  8.93595815e-03, bound:  3.15672904e-01\n",
      "Epoch: 16793 mean train loss:  8.93481262e-03, bound:  3.15672874e-01\n",
      "Epoch: 16794 mean train loss:  8.93375371e-03, bound:  3.15672785e-01\n",
      "Epoch: 16795 mean train loss:  8.93267710e-03, bound:  3.15672755e-01\n",
      "Epoch: 16796 mean train loss:  8.93153436e-03, bound:  3.15672696e-01\n",
      "Epoch: 16797 mean train loss:  8.93057883e-03, bound:  3.15672606e-01\n",
      "Epoch: 16798 mean train loss:  8.92949663e-03, bound:  3.15672576e-01\n",
      "Epoch: 16799 mean train loss:  8.92836973e-03, bound:  3.15672487e-01\n",
      "Epoch: 16800 mean train loss:  8.92736949e-03, bound:  3.15672487e-01\n",
      "Epoch: 16801 mean train loss:  8.92631151e-03, bound:  3.15672368e-01\n",
      "Epoch: 16802 mean train loss:  8.92517250e-03, bound:  3.15672368e-01\n",
      "Epoch: 16803 mean train loss:  8.92426167e-03, bound:  3.15672338e-01\n",
      "Epoch: 16804 mean train loss:  8.92318133e-03, bound:  3.15672249e-01\n",
      "Epoch: 16805 mean train loss:  8.92230216e-03, bound:  3.15672219e-01\n",
      "Epoch: 16806 mean train loss:  8.92148819e-03, bound:  3.15672159e-01\n",
      "Epoch: 16807 mean train loss:  8.92070588e-03, bound:  3.15672100e-01\n",
      "Epoch: 16808 mean train loss:  8.91985092e-03, bound:  3.15672040e-01\n",
      "Epoch: 16809 mean train loss:  8.91913753e-03, bound:  3.15671951e-01\n",
      "Epoch: 16810 mean train loss:  8.91848840e-03, bound:  3.15671921e-01\n",
      "Epoch: 16811 mean train loss:  8.91803205e-03, bound:  3.15671891e-01\n",
      "Epoch: 16812 mean train loss:  8.91752914e-03, bound:  3.15671831e-01\n",
      "Epoch: 16813 mean train loss:  8.91717244e-03, bound:  3.15671772e-01\n",
      "Epoch: 16814 mean train loss:  8.91685952e-03, bound:  3.15671772e-01\n",
      "Epoch: 16815 mean train loss:  8.91655684e-03, bound:  3.15671623e-01\n",
      "Epoch: 16816 mean train loss:  8.91630072e-03, bound:  3.15671623e-01\n",
      "Epoch: 16817 mean train loss:  8.91581830e-03, bound:  3.15671504e-01\n",
      "Epoch: 16818 mean train loss:  8.91500246e-03, bound:  3.15671533e-01\n",
      "Epoch: 16819 mean train loss:  8.91389791e-03, bound:  3.15671444e-01\n",
      "Epoch: 16820 mean train loss:  8.91220197e-03, bound:  3.15671444e-01\n",
      "Epoch: 16821 mean train loss:  8.91007576e-03, bound:  3.15671325e-01\n",
      "Epoch: 16822 mean train loss:  8.90761893e-03, bound:  3.15671325e-01\n",
      "Epoch: 16823 mean train loss:  8.90482031e-03, bound:  3.15671235e-01\n",
      "Epoch: 16824 mean train loss:  8.90241563e-03, bound:  3.15671206e-01\n",
      "Epoch: 16825 mean train loss:  8.90028477e-03, bound:  3.15671116e-01\n",
      "Epoch: 16826 mean train loss:  8.89879372e-03, bound:  3.15671086e-01\n",
      "Epoch: 16827 mean train loss:  8.89785588e-03, bound:  3.15671027e-01\n",
      "Epoch: 16828 mean train loss:  8.89710896e-03, bound:  3.15670967e-01\n",
      "Epoch: 16829 mean train loss:  8.89660604e-03, bound:  3.15670937e-01\n",
      "Epoch: 16830 mean train loss:  8.89592618e-03, bound:  3.15670818e-01\n",
      "Epoch: 16831 mean train loss:  8.89516715e-03, bound:  3.15670818e-01\n",
      "Epoch: 16832 mean train loss:  8.89413059e-03, bound:  3.15670758e-01\n",
      "Epoch: 16833 mean train loss:  8.89288168e-03, bound:  3.15670729e-01\n",
      "Epoch: 16834 mean train loss:  8.89141764e-03, bound:  3.15670669e-01\n",
      "Epoch: 16835 mean train loss:  8.88995454e-03, bound:  3.15670609e-01\n",
      "Epoch: 16836 mean train loss:  8.88858549e-03, bound:  3.15670550e-01\n",
      "Epoch: 16837 mean train loss:  8.88716243e-03, bound:  3.15670490e-01\n",
      "Epoch: 16838 mean train loss:  8.88596848e-03, bound:  3.15670460e-01\n",
      "Epoch: 16839 mean train loss:  8.88484903e-03, bound:  3.15670371e-01\n",
      "Epoch: 16840 mean train loss:  8.88380501e-03, bound:  3.15670341e-01\n",
      "Epoch: 16841 mean train loss:  8.88280664e-03, bound:  3.15670252e-01\n",
      "Epoch: 16842 mean train loss:  8.88180733e-03, bound:  3.15670222e-01\n",
      "Epoch: 16843 mean train loss:  8.88084527e-03, bound:  3.15670162e-01\n",
      "Epoch: 16844 mean train loss:  8.87991115e-03, bound:  3.15670133e-01\n",
      "Epoch: 16845 mean train loss:  8.87874048e-03, bound:  3.15670043e-01\n",
      "Epoch: 16846 mean train loss:  8.87772068e-03, bound:  3.15670013e-01\n",
      "Epoch: 16847 mean train loss:  8.87650717e-03, bound:  3.15669924e-01\n",
      "Epoch: 16848 mean train loss:  8.87535512e-03, bound:  3.15669894e-01\n",
      "Epoch: 16849 mean train loss:  8.87419004e-03, bound:  3.15669835e-01\n",
      "Epoch: 16850 mean train loss:  8.87307618e-03, bound:  3.15669805e-01\n",
      "Epoch: 16851 mean train loss:  8.87196511e-03, bound:  3.15669715e-01\n",
      "Epoch: 16852 mean train loss:  8.87089595e-03, bound:  3.15669686e-01\n",
      "Epoch: 16853 mean train loss:  8.86967778e-03, bound:  3.15669656e-01\n",
      "Epoch: 16854 mean train loss:  8.86872038e-03, bound:  3.15669596e-01\n",
      "Epoch: 16855 mean train loss:  8.86761118e-03, bound:  3.15669537e-01\n",
      "Epoch: 16856 mean train loss:  8.86667147e-03, bound:  3.15669477e-01\n",
      "Epoch: 16857 mean train loss:  8.86560138e-03, bound:  3.15669417e-01\n",
      "Epoch: 16858 mean train loss:  8.86459462e-03, bound:  3.15669358e-01\n",
      "Epoch: 16859 mean train loss:  8.86345096e-03, bound:  3.15669328e-01\n",
      "Epoch: 16860 mean train loss:  8.86248704e-03, bound:  3.15669239e-01\n",
      "Epoch: 16861 mean train loss:  8.86141974e-03, bound:  3.15669239e-01\n",
      "Epoch: 16862 mean train loss:  8.86043813e-03, bound:  3.15669149e-01\n",
      "Epoch: 16863 mean train loss:  8.85925628e-03, bound:  3.15669119e-01\n",
      "Epoch: 16864 mean train loss:  8.85824859e-03, bound:  3.15669030e-01\n",
      "Epoch: 16865 mean train loss:  8.85721203e-03, bound:  3.15669000e-01\n",
      "Epoch: 16866 mean train loss:  8.85626115e-03, bound:  3.15668911e-01\n",
      "Epoch: 16867 mean train loss:  8.85516685e-03, bound:  3.15668911e-01\n",
      "Epoch: 16868 mean train loss:  8.85419268e-03, bound:  3.15668821e-01\n",
      "Epoch: 16869 mean train loss:  8.85318406e-03, bound:  3.15668792e-01\n",
      "Epoch: 16870 mean train loss:  8.85208789e-03, bound:  3.15668702e-01\n",
      "Epoch: 16871 mean train loss:  8.85104761e-03, bound:  3.15668672e-01\n",
      "Epoch: 16872 mean train loss:  8.84999242e-03, bound:  3.15668613e-01\n",
      "Epoch: 16873 mean train loss:  8.84888973e-03, bound:  3.15668583e-01\n",
      "Epoch: 16874 mean train loss:  8.84793326e-03, bound:  3.15668523e-01\n",
      "Epoch: 16875 mean train loss:  8.84683803e-03, bound:  3.15668464e-01\n",
      "Epoch: 16876 mean train loss:  8.84587225e-03, bound:  3.15668374e-01\n",
      "Epoch: 16877 mean train loss:  8.84473138e-03, bound:  3.15668374e-01\n",
      "Epoch: 16878 mean train loss:  8.84363241e-03, bound:  3.15668315e-01\n",
      "Epoch: 16879 mean train loss:  8.84258375e-03, bound:  3.15668255e-01\n",
      "Epoch: 16880 mean train loss:  8.84153321e-03, bound:  3.15668225e-01\n",
      "Epoch: 16881 mean train loss:  8.84051900e-03, bound:  3.15668136e-01\n",
      "Epoch: 16882 mean train loss:  8.83932039e-03, bound:  3.15668106e-01\n",
      "Epoch: 16883 mean train loss:  8.83836206e-03, bound:  3.15668046e-01\n",
      "Epoch: 16884 mean train loss:  8.83733202e-03, bound:  3.15667987e-01\n",
      "Epoch: 16885 mean train loss:  8.83642025e-03, bound:  3.15667927e-01\n",
      "Epoch: 16886 mean train loss:  8.83535948e-03, bound:  3.15667868e-01\n",
      "Epoch: 16887 mean train loss:  8.83430243e-03, bound:  3.15667808e-01\n",
      "Epoch: 16888 mean train loss:  8.83342419e-03, bound:  3.15667778e-01\n",
      "Epoch: 16889 mean train loss:  8.83237273e-03, bound:  3.15667748e-01\n",
      "Epoch: 16890 mean train loss:  8.83138459e-03, bound:  3.15667659e-01\n",
      "Epoch: 16891 mean train loss:  8.83046258e-03, bound:  3.15667659e-01\n",
      "Epoch: 16892 mean train loss:  8.82954150e-03, bound:  3.15667540e-01\n",
      "Epoch: 16893 mean train loss:  8.82847328e-03, bound:  3.15667540e-01\n",
      "Epoch: 16894 mean train loss:  8.82750191e-03, bound:  3.15667421e-01\n",
      "Epoch: 16895 mean train loss:  8.82663578e-03, bound:  3.15667421e-01\n",
      "Epoch: 16896 mean train loss:  8.82586278e-03, bound:  3.15667331e-01\n",
      "Epoch: 16897 mean train loss:  8.82508699e-03, bound:  3.15667331e-01\n",
      "Epoch: 16898 mean train loss:  8.82428233e-03, bound:  3.15667242e-01\n",
      "Epoch: 16899 mean train loss:  8.82372819e-03, bound:  3.15667212e-01\n",
      "Epoch: 16900 mean train loss:  8.82294867e-03, bound:  3.15667123e-01\n",
      "Epoch: 16901 mean train loss:  8.82215612e-03, bound:  3.15667093e-01\n",
      "Epoch: 16902 mean train loss:  8.82116985e-03, bound:  3.15667003e-01\n",
      "Epoch: 16903 mean train loss:  8.82012490e-03, bound:  3.15667003e-01\n",
      "Epoch: 16904 mean train loss:  8.81884247e-03, bound:  3.15666884e-01\n",
      "Epoch: 16905 mean train loss:  8.81745480e-03, bound:  3.15666884e-01\n",
      "Epoch: 16906 mean train loss:  8.81583523e-03, bound:  3.15666795e-01\n",
      "Epoch: 16907 mean train loss:  8.81427713e-03, bound:  3.15666795e-01\n",
      "Epoch: 16908 mean train loss:  8.81257840e-03, bound:  3.15666676e-01\n",
      "Epoch: 16909 mean train loss:  8.81098211e-03, bound:  3.15666676e-01\n",
      "Epoch: 16910 mean train loss:  8.80939886e-03, bound:  3.15666586e-01\n",
      "Epoch: 16911 mean train loss:  8.80801212e-03, bound:  3.15666556e-01\n",
      "Epoch: 16912 mean train loss:  8.80664214e-03, bound:  3.15666497e-01\n",
      "Epoch: 16913 mean train loss:  8.80547520e-03, bound:  3.15666437e-01\n",
      "Epoch: 16914 mean train loss:  8.80432222e-03, bound:  3.15666407e-01\n",
      "Epoch: 16915 mean train loss:  8.80349148e-03, bound:  3.15666348e-01\n",
      "Epoch: 16916 mean train loss:  8.80252942e-03, bound:  3.15666288e-01\n",
      "Epoch: 16917 mean train loss:  8.80181137e-03, bound:  3.15666229e-01\n",
      "Epoch: 16918 mean train loss:  8.80085025e-03, bound:  3.15666199e-01\n",
      "Epoch: 16919 mean train loss:  8.79988540e-03, bound:  3.15666139e-01\n",
      "Epoch: 16920 mean train loss:  8.79903417e-03, bound:  3.15666109e-01\n",
      "Epoch: 16921 mean train loss:  8.79806466e-03, bound:  3.15666020e-01\n",
      "Epoch: 16922 mean train loss:  8.79694894e-03, bound:  3.15665990e-01\n",
      "Epoch: 16923 mean train loss:  8.79579224e-03, bound:  3.15665901e-01\n",
      "Epoch: 16924 mean train loss:  8.79464298e-03, bound:  3.15665871e-01\n",
      "Epoch: 16925 mean train loss:  8.79360083e-03, bound:  3.15665811e-01\n",
      "Epoch: 16926 mean train loss:  8.79231468e-03, bound:  3.15665781e-01\n",
      "Epoch: 16927 mean train loss:  8.79115518e-03, bound:  3.15665692e-01\n",
      "Epoch: 16928 mean train loss:  8.78995378e-03, bound:  3.15665662e-01\n",
      "Epoch: 16929 mean train loss:  8.78877100e-03, bound:  3.15665573e-01\n",
      "Epoch: 16930 mean train loss:  8.78769811e-03, bound:  3.15665573e-01\n",
      "Epoch: 16931 mean train loss:  8.78656097e-03, bound:  3.15665543e-01\n",
      "Epoch: 16932 mean train loss:  8.78532790e-03, bound:  3.15665454e-01\n",
      "Epoch: 16933 mean train loss:  8.78437608e-03, bound:  3.15665424e-01\n",
      "Epoch: 16934 mean train loss:  8.78327619e-03, bound:  3.15665364e-01\n",
      "Epoch: 16935 mean train loss:  8.78215209e-03, bound:  3.15665305e-01\n",
      "Epoch: 16936 mean train loss:  8.78108293e-03, bound:  3.15665245e-01\n",
      "Epoch: 16937 mean train loss:  8.78002867e-03, bound:  3.15665185e-01\n",
      "Epoch: 16938 mean train loss:  8.77903402e-03, bound:  3.15665126e-01\n",
      "Epoch: 16939 mean train loss:  8.77788756e-03, bound:  3.15665096e-01\n",
      "Epoch: 16940 mean train loss:  8.77677556e-03, bound:  3.15665036e-01\n",
      "Epoch: 16941 mean train loss:  8.77579395e-03, bound:  3.15664977e-01\n",
      "Epoch: 16942 mean train loss:  8.77477415e-03, bound:  3.15664977e-01\n",
      "Epoch: 16943 mean train loss:  8.77360441e-03, bound:  3.15664858e-01\n",
      "Epoch: 16944 mean train loss:  8.77253339e-03, bound:  3.15664858e-01\n",
      "Epoch: 16945 mean train loss:  8.77149310e-03, bound:  3.15664768e-01\n",
      "Epoch: 16946 mean train loss:  8.77042674e-03, bound:  3.15664709e-01\n",
      "Epoch: 16947 mean train loss:  8.76944978e-03, bound:  3.15664679e-01\n",
      "Epoch: 16948 mean train loss:  8.76829773e-03, bound:  3.15664649e-01\n",
      "Epoch: 16949 mean train loss:  8.76723696e-03, bound:  3.15664560e-01\n",
      "Epoch: 16950 mean train loss:  8.76620878e-03, bound:  3.15664530e-01\n",
      "Epoch: 16951 mean train loss:  8.76514986e-03, bound:  3.15664440e-01\n",
      "Epoch: 16952 mean train loss:  8.76409188e-03, bound:  3.15664440e-01\n",
      "Epoch: 16953 mean train loss:  8.76290072e-03, bound:  3.15664321e-01\n",
      "Epoch: 16954 mean train loss:  8.76194518e-03, bound:  3.15664321e-01\n",
      "Epoch: 16955 mean train loss:  8.76081549e-03, bound:  3.15664232e-01\n",
      "Epoch: 16956 mean train loss:  8.75986181e-03, bound:  3.15664232e-01\n",
      "Epoch: 16957 mean train loss:  8.75874888e-03, bound:  3.15664142e-01\n",
      "Epoch: 16958 mean train loss:  8.75768997e-03, bound:  3.15664113e-01\n",
      "Epoch: 16959 mean train loss:  8.75668693e-03, bound:  3.15664023e-01\n",
      "Epoch: 16960 mean train loss:  8.75562895e-03, bound:  3.15663993e-01\n",
      "Epoch: 16961 mean train loss:  8.75456166e-03, bound:  3.15663934e-01\n",
      "Epoch: 16962 mean train loss:  8.75352789e-03, bound:  3.15663874e-01\n",
      "Epoch: 16963 mean train loss:  8.75251368e-03, bound:  3.15663844e-01\n",
      "Epoch: 16964 mean train loss:  8.75163451e-03, bound:  3.15663785e-01\n",
      "Epoch: 16965 mean train loss:  8.75067059e-03, bound:  3.15663695e-01\n",
      "Epoch: 16966 mean train loss:  8.74971319e-03, bound:  3.15663695e-01\n",
      "Epoch: 16967 mean train loss:  8.74874555e-03, bound:  3.15663636e-01\n",
      "Epoch: 16968 mean train loss:  8.74810852e-03, bound:  3.15663576e-01\n",
      "Epoch: 16969 mean train loss:  8.74748733e-03, bound:  3.15663517e-01\n",
      "Epoch: 16970 mean train loss:  8.74701142e-03, bound:  3.15663517e-01\n",
      "Epoch: 16971 mean train loss:  8.74698441e-03, bound:  3.15663397e-01\n",
      "Epoch: 16972 mean train loss:  8.74734670e-03, bound:  3.15663397e-01\n",
      "Epoch: 16973 mean train loss:  8.74820538e-03, bound:  3.15663248e-01\n",
      "Epoch: 16974 mean train loss:  8.74965824e-03, bound:  3.15663308e-01\n",
      "Epoch: 16975 mean train loss:  8.75194184e-03, bound:  3.15663189e-01\n",
      "Epoch: 16976 mean train loss:  8.75480566e-03, bound:  3.15663189e-01\n",
      "Epoch: 16977 mean train loss:  8.75768438e-03, bound:  3.15663069e-01\n",
      "Epoch: 16978 mean train loss:  8.75913817e-03, bound:  3.15663129e-01\n",
      "Epoch: 16979 mean train loss:  8.75795074e-03, bound:  3.15662920e-01\n",
      "Epoch: 16980 mean train loss:  8.75292439e-03, bound:  3.15663010e-01\n",
      "Epoch: 16981 mean train loss:  8.74504726e-03, bound:  3.15662861e-01\n",
      "Epoch: 16982 mean train loss:  8.73690471e-03, bound:  3.15662891e-01\n",
      "Epoch: 16983 mean train loss:  8.73166323e-03, bound:  3.15662801e-01\n",
      "Epoch: 16984 mean train loss:  8.73088930e-03, bound:  3.15662742e-01\n",
      "Epoch: 16985 mean train loss:  8.73287674e-03, bound:  3.15662742e-01\n",
      "Epoch: 16986 mean train loss:  8.73551145e-03, bound:  3.15662622e-01\n",
      "Epoch: 16987 mean train loss:  8.73618759e-03, bound:  3.15662622e-01\n",
      "Epoch: 16988 mean train loss:  8.73397104e-03, bound:  3.15662503e-01\n",
      "Epoch: 16989 mean train loss:  8.72972701e-03, bound:  3.15662533e-01\n",
      "Epoch: 16990 mean train loss:  8.72552674e-03, bound:  3.15662444e-01\n",
      "Epoch: 16991 mean train loss:  8.72313604e-03, bound:  3.15662414e-01\n",
      "Epoch: 16992 mean train loss:  8.72275792e-03, bound:  3.15662354e-01\n",
      "Epoch: 16993 mean train loss:  8.72364175e-03, bound:  3.15662295e-01\n",
      "Epoch: 16994 mean train loss:  8.72399844e-03, bound:  3.15662265e-01\n",
      "Epoch: 16995 mean train loss:  8.72298982e-03, bound:  3.15662175e-01\n",
      "Epoch: 16996 mean train loss:  8.72067548e-03, bound:  3.15662146e-01\n",
      "Epoch: 16997 mean train loss:  8.71806592e-03, bound:  3.15662116e-01\n",
      "Epoch: 16998 mean train loss:  8.71600397e-03, bound:  3.15662056e-01\n",
      "Epoch: 16999 mean train loss:  8.71506799e-03, bound:  3.15661997e-01\n",
      "Epoch: 17000 mean train loss:  8.71483795e-03, bound:  3.15661907e-01\n",
      "Epoch: 17001 mean train loss:  8.71452037e-03, bound:  3.15661907e-01\n",
      "Epoch: 17002 mean train loss:  8.71367753e-03, bound:  3.15661818e-01\n",
      "Epoch: 17003 mean train loss:  8.71213991e-03, bound:  3.15661788e-01\n",
      "Epoch: 17004 mean train loss:  8.71036481e-03, bound:  3.15661758e-01\n",
      "Epoch: 17005 mean train loss:  8.70892406e-03, bound:  3.15661699e-01\n",
      "Epoch: 17006 mean train loss:  8.70788004e-03, bound:  3.15661639e-01\n",
      "Epoch: 17007 mean train loss:  8.70709866e-03, bound:  3.15661609e-01\n",
      "Epoch: 17008 mean train loss:  8.70646536e-03, bound:  3.15661579e-01\n",
      "Epoch: 17009 mean train loss:  8.70553125e-03, bound:  3.15661520e-01\n",
      "Epoch: 17010 mean train loss:  8.70440342e-03, bound:  3.15661460e-01\n",
      "Epoch: 17011 mean train loss:  8.70302133e-03, bound:  3.15661401e-01\n",
      "Epoch: 17012 mean train loss:  8.70176218e-03, bound:  3.15661341e-01\n",
      "Epoch: 17013 mean train loss:  8.70073866e-03, bound:  3.15661311e-01\n",
      "Epoch: 17014 mean train loss:  8.69977288e-03, bound:  3.15661281e-01\n",
      "Epoch: 17015 mean train loss:  8.69887229e-03, bound:  3.15661222e-01\n",
      "Epoch: 17016 mean train loss:  8.69806297e-03, bound:  3.15661132e-01\n",
      "Epoch: 17017 mean train loss:  8.69694445e-03, bound:  3.15661103e-01\n",
      "Epoch: 17018 mean train loss:  8.69577844e-03, bound:  3.15661013e-01\n",
      "Epoch: 17019 mean train loss:  8.69466364e-03, bound:  3.15660983e-01\n",
      "Epoch: 17020 mean train loss:  8.69359262e-03, bound:  3.15660983e-01\n",
      "Epoch: 17021 mean train loss:  8.69258586e-03, bound:  3.15660894e-01\n",
      "Epoch: 17022 mean train loss:  8.69159587e-03, bound:  3.15660864e-01\n",
      "Epoch: 17023 mean train loss:  8.69067479e-03, bound:  3.15660775e-01\n",
      "Epoch: 17024 mean train loss:  8.68971180e-03, bound:  3.15660775e-01\n",
      "Epoch: 17025 mean train loss:  8.68873578e-03, bound:  3.15660685e-01\n",
      "Epoch: 17026 mean train loss:  8.68754182e-03, bound:  3.15660655e-01\n",
      "Epoch: 17027 mean train loss:  8.68652482e-03, bound:  3.15660626e-01\n",
      "Epoch: 17028 mean train loss:  8.68545938e-03, bound:  3.15660566e-01\n",
      "Epoch: 17029 mean train loss:  8.68442096e-03, bound:  3.15660477e-01\n",
      "Epoch: 17030 mean train loss:  8.68339837e-03, bound:  3.15660447e-01\n",
      "Epoch: 17031 mean train loss:  8.68244935e-03, bound:  3.15660417e-01\n",
      "Epoch: 17032 mean train loss:  8.68142489e-03, bound:  3.15660328e-01\n",
      "Epoch: 17033 mean train loss:  8.68038181e-03, bound:  3.15660298e-01\n",
      "Epoch: 17034 mean train loss:  8.67930800e-03, bound:  3.15660298e-01\n",
      "Epoch: 17035 mean train loss:  8.67827609e-03, bound:  3.15660208e-01\n",
      "Epoch: 17036 mean train loss:  8.67723115e-03, bound:  3.15660179e-01\n",
      "Epoch: 17037 mean train loss:  8.67633894e-03, bound:  3.15660119e-01\n",
      "Epoch: 17038 mean train loss:  8.67531076e-03, bound:  3.15660089e-01\n",
      "Epoch: 17039 mean train loss:  8.67433101e-03, bound:  3.15660000e-01\n",
      "Epoch: 17040 mean train loss:  8.67321622e-03, bound:  3.15659970e-01\n",
      "Epoch: 17041 mean train loss:  8.67216568e-03, bound:  3.15659881e-01\n",
      "Epoch: 17042 mean train loss:  8.67128372e-03, bound:  3.15659851e-01\n",
      "Epoch: 17043 mean train loss:  8.67009349e-03, bound:  3.15659791e-01\n",
      "Epoch: 17044 mean train loss:  8.66911933e-03, bound:  3.15659761e-01\n",
      "Epoch: 17045 mean train loss:  8.66816565e-03, bound:  3.15659702e-01\n",
      "Epoch: 17046 mean train loss:  8.66709650e-03, bound:  3.15659642e-01\n",
      "Epoch: 17047 mean train loss:  8.66615586e-03, bound:  3.15659612e-01\n",
      "Epoch: 17048 mean train loss:  8.66509695e-03, bound:  3.15659553e-01\n",
      "Epoch: 17049 mean train loss:  8.66400264e-03, bound:  3.15659493e-01\n",
      "Epoch: 17050 mean train loss:  8.66305828e-03, bound:  3.15659463e-01\n",
      "Epoch: 17051 mean train loss:  8.66197422e-03, bound:  3.15659434e-01\n",
      "Epoch: 17052 mean train loss:  8.66098423e-03, bound:  3.15659344e-01\n",
      "Epoch: 17053 mean train loss:  8.65983684e-03, bound:  3.15659314e-01\n",
      "Epoch: 17054 mean train loss:  8.65899120e-03, bound:  3.15659225e-01\n",
      "Epoch: 17055 mean train loss:  8.65793135e-03, bound:  3.15659195e-01\n",
      "Epoch: 17056 mean train loss:  8.65687057e-03, bound:  3.15659165e-01\n",
      "Epoch: 17057 mean train loss:  8.65593739e-03, bound:  3.15659076e-01\n",
      "Epoch: 17058 mean train loss:  8.65481794e-03, bound:  3.15659076e-01\n",
      "Epoch: 17059 mean train loss:  8.65379162e-03, bound:  3.15658987e-01\n",
      "Epoch: 17060 mean train loss:  8.65279511e-03, bound:  3.15658957e-01\n",
      "Epoch: 17061 mean train loss:  8.65177810e-03, bound:  3.15658897e-01\n",
      "Epoch: 17062 mean train loss:  8.65077041e-03, bound:  3.15658867e-01\n",
      "Epoch: 17063 mean train loss:  8.64977390e-03, bound:  3.15658778e-01\n",
      "Epoch: 17064 mean train loss:  8.64876248e-03, bound:  3.15658748e-01\n",
      "Epoch: 17065 mean train loss:  8.64772312e-03, bound:  3.15658718e-01\n",
      "Epoch: 17066 mean train loss:  8.64670612e-03, bound:  3.15658659e-01\n",
      "Epoch: 17067 mean train loss:  8.64562765e-03, bound:  3.15658629e-01\n",
      "Epoch: 17068 mean train loss:  8.64459388e-03, bound:  3.15658540e-01\n",
      "Epoch: 17069 mean train loss:  8.64361227e-03, bound:  3.15658510e-01\n",
      "Epoch: 17070 mean train loss:  8.64253193e-03, bound:  3.15658450e-01\n",
      "Epoch: 17071 mean train loss:  8.64150468e-03, bound:  3.15658420e-01\n",
      "Epoch: 17072 mean train loss:  8.64057709e-03, bound:  3.15658331e-01\n",
      "Epoch: 17073 mean train loss:  8.63957778e-03, bound:  3.15658301e-01\n",
      "Epoch: 17074 mean train loss:  8.63851234e-03, bound:  3.15658242e-01\n",
      "Epoch: 17075 mean train loss:  8.63747951e-03, bound:  3.15658182e-01\n",
      "Epoch: 17076 mean train loss:  8.63635540e-03, bound:  3.15658122e-01\n",
      "Epoch: 17077 mean train loss:  8.63534864e-03, bound:  3.15658092e-01\n",
      "Epoch: 17078 mean train loss:  8.63444619e-03, bound:  3.15658063e-01\n",
      "Epoch: 17079 mean train loss:  8.63331649e-03, bound:  3.15658003e-01\n",
      "Epoch: 17080 mean train loss:  8.63233767e-03, bound:  3.15657943e-01\n",
      "Epoch: 17081 mean train loss:  8.63136351e-03, bound:  3.15657884e-01\n",
      "Epoch: 17082 mean train loss:  8.63035209e-03, bound:  3.15657854e-01\n",
      "Epoch: 17083 mean train loss:  8.62920471e-03, bound:  3.15657794e-01\n",
      "Epoch: 17084 mean train loss:  8.62828828e-03, bound:  3.15657765e-01\n",
      "Epoch: 17085 mean train loss:  8.62723403e-03, bound:  3.15657675e-01\n",
      "Epoch: 17086 mean train loss:  8.62617604e-03, bound:  3.15657645e-01\n",
      "Epoch: 17087 mean train loss:  8.62525776e-03, bound:  3.15657616e-01\n",
      "Epoch: 17088 mean train loss:  8.62416532e-03, bound:  3.15657526e-01\n",
      "Epoch: 17089 mean train loss:  8.62312876e-03, bound:  3.15657496e-01\n",
      "Epoch: 17090 mean train loss:  8.62215273e-03, bound:  3.15657437e-01\n",
      "Epoch: 17091 mean train loss:  8.62101093e-03, bound:  3.15657407e-01\n",
      "Epoch: 17092 mean train loss:  8.62005912e-03, bound:  3.15657318e-01\n",
      "Epoch: 17093 mean train loss:  8.61895550e-03, bound:  3.15657318e-01\n",
      "Epoch: 17094 mean train loss:  8.61803815e-03, bound:  3.15657228e-01\n",
      "Epoch: 17095 mean train loss:  8.61701462e-03, bound:  3.15657198e-01\n",
      "Epoch: 17096 mean train loss:  8.61601066e-03, bound:  3.15657139e-01\n",
      "Epoch: 17097 mean train loss:  8.61493032e-03, bound:  3.15657109e-01\n",
      "Epoch: 17098 mean train loss:  8.61387327e-03, bound:  3.15657049e-01\n",
      "Epoch: 17099 mean train loss:  8.61295126e-03, bound:  3.15656990e-01\n",
      "Epoch: 17100 mean train loss:  8.61181784e-03, bound:  3.15656930e-01\n",
      "Epoch: 17101 mean train loss:  8.61080643e-03, bound:  3.15656871e-01\n",
      "Epoch: 17102 mean train loss:  8.60986114e-03, bound:  3.15656841e-01\n",
      "Epoch: 17103 mean train loss:  8.60882550e-03, bound:  3.15656781e-01\n",
      "Epoch: 17104 mean train loss:  8.60784948e-03, bound:  3.15656751e-01\n",
      "Epoch: 17105 mean train loss:  8.60687718e-03, bound:  3.15656662e-01\n",
      "Epoch: 17106 mean train loss:  8.60589836e-03, bound:  3.15656632e-01\n",
      "Epoch: 17107 mean train loss:  8.60494748e-03, bound:  3.15656602e-01\n",
      "Epoch: 17108 mean train loss:  8.60396866e-03, bound:  3.15656513e-01\n",
      "Epoch: 17109 mean train loss:  8.60310718e-03, bound:  3.15656453e-01\n",
      "Epoch: 17110 mean train loss:  8.60217307e-03, bound:  3.15656453e-01\n",
      "Epoch: 17111 mean train loss:  8.60125478e-03, bound:  3.15656394e-01\n",
      "Epoch: 17112 mean train loss:  8.60041752e-03, bound:  3.15656334e-01\n",
      "Epoch: 17113 mean train loss:  8.59951787e-03, bound:  3.15656275e-01\n",
      "Epoch: 17114 mean train loss:  8.59866012e-03, bound:  3.15656215e-01\n",
      "Epoch: 17115 mean train loss:  8.59783497e-03, bound:  3.15656185e-01\n",
      "Epoch: 17116 mean train loss:  8.59712996e-03, bound:  3.15656126e-01\n",
      "Epoch: 17117 mean train loss:  8.59644357e-03, bound:  3.15656066e-01\n",
      "Epoch: 17118 mean train loss:  8.59578513e-03, bound:  3.15656066e-01\n",
      "Epoch: 17119 mean train loss:  8.59527104e-03, bound:  3.15655947e-01\n",
      "Epoch: 17120 mean train loss:  8.59487616e-03, bound:  3.15655947e-01\n",
      "Epoch: 17121 mean train loss:  8.59438162e-03, bound:  3.15655857e-01\n",
      "Epoch: 17122 mean train loss:  8.59402400e-03, bound:  3.15655857e-01\n",
      "Epoch: 17123 mean train loss:  8.59369058e-03, bound:  3.15655768e-01\n",
      "Epoch: 17124 mean train loss:  8.59321468e-03, bound:  3.15655768e-01\n",
      "Epoch: 17125 mean train loss:  8.59268662e-03, bound:  3.15655649e-01\n",
      "Epoch: 17126 mean train loss:  8.59174598e-03, bound:  3.15655649e-01\n",
      "Epoch: 17127 mean train loss:  8.59041326e-03, bound:  3.15655529e-01\n",
      "Epoch: 17128 mean train loss:  8.58871266e-03, bound:  3.15655529e-01\n",
      "Epoch: 17129 mean train loss:  8.58665910e-03, bound:  3.15655440e-01\n",
      "Epoch: 17130 mean train loss:  8.58451426e-03, bound:  3.15655440e-01\n",
      "Epoch: 17131 mean train loss:  8.58236104e-03, bound:  3.15655351e-01\n",
      "Epoch: 17132 mean train loss:  8.58041644e-03, bound:  3.15655351e-01\n",
      "Epoch: 17133 mean train loss:  8.57860968e-03, bound:  3.15655291e-01\n",
      "Epoch: 17134 mean train loss:  8.57719220e-03, bound:  3.15655202e-01\n",
      "Epoch: 17135 mean train loss:  8.57610535e-03, bound:  3.15655202e-01\n",
      "Epoch: 17136 mean train loss:  8.57537426e-03, bound:  3.15655112e-01\n",
      "Epoch: 17137 mean train loss:  8.57459661e-03, bound:  3.15655082e-01\n",
      "Epoch: 17138 mean train loss:  8.57398100e-03, bound:  3.15654993e-01\n",
      "Epoch: 17139 mean train loss:  8.57332163e-03, bound:  3.15654993e-01\n",
      "Epoch: 17140 mean train loss:  8.57254025e-03, bound:  3.15654904e-01\n",
      "Epoch: 17141 mean train loss:  8.57152976e-03, bound:  3.15654874e-01\n",
      "Epoch: 17142 mean train loss:  8.57037585e-03, bound:  3.15654814e-01\n",
      "Epoch: 17143 mean train loss:  8.56921729e-03, bound:  3.15654784e-01\n",
      "Epoch: 17144 mean train loss:  8.56795162e-03, bound:  3.15654725e-01\n",
      "Epoch: 17145 mean train loss:  8.56657512e-03, bound:  3.15654665e-01\n",
      "Epoch: 17146 mean train loss:  8.56526662e-03, bound:  3.15654635e-01\n",
      "Epoch: 17147 mean train loss:  8.56412388e-03, bound:  3.15654606e-01\n",
      "Epoch: 17148 mean train loss:  8.56299046e-03, bound:  3.15654546e-01\n",
      "Epoch: 17149 mean train loss:  8.56181886e-03, bound:  3.15654486e-01\n",
      "Epoch: 17150 mean train loss:  8.56092945e-03, bound:  3.15654427e-01\n",
      "Epoch: 17151 mean train loss:  8.56002141e-03, bound:  3.15654397e-01\n",
      "Epoch: 17152 mean train loss:  8.55913945e-03, bound:  3.15654367e-01\n",
      "Epoch: 17153 mean train loss:  8.55820719e-03, bound:  3.15654278e-01\n",
      "Epoch: 17154 mean train loss:  8.55736621e-03, bound:  3.15654218e-01\n",
      "Epoch: 17155 mean train loss:  8.55639298e-03, bound:  3.15654188e-01\n",
      "Epoch: 17156 mean train loss:  8.55550356e-03, bound:  3.15654159e-01\n",
      "Epoch: 17157 mean train loss:  8.55436828e-03, bound:  3.15654069e-01\n",
      "Epoch: 17158 mean train loss:  8.55331402e-03, bound:  3.15654069e-01\n",
      "Epoch: 17159 mean train loss:  8.55223369e-03, bound:  3.15653980e-01\n",
      "Epoch: 17160 mean train loss:  8.55118223e-03, bound:  3.15653950e-01\n",
      "Epoch: 17161 mean train loss:  8.55006650e-03, bound:  3.15653890e-01\n",
      "Epoch: 17162 mean train loss:  8.54892470e-03, bound:  3.15653861e-01\n",
      "Epoch: 17163 mean train loss:  8.54775682e-03, bound:  3.15653771e-01\n",
      "Epoch: 17164 mean train loss:  8.54678173e-03, bound:  3.15653741e-01\n",
      "Epoch: 17165 mean train loss:  8.54570325e-03, bound:  3.15653712e-01\n",
      "Epoch: 17166 mean train loss:  8.54475982e-03, bound:  3.15653652e-01\n",
      "Epoch: 17167 mean train loss:  8.54369346e-03, bound:  3.15653563e-01\n",
      "Epoch: 17168 mean train loss:  8.54268763e-03, bound:  3.15653533e-01\n",
      "Epoch: 17169 mean train loss:  8.54175538e-03, bound:  3.15653503e-01\n",
      "Epoch: 17170 mean train loss:  8.54078494e-03, bound:  3.15653443e-01\n",
      "Epoch: 17171 mean train loss:  8.53966735e-03, bound:  3.15653384e-01\n",
      "Epoch: 17172 mean train loss:  8.53867736e-03, bound:  3.15653324e-01\n",
      "Epoch: 17173 mean train loss:  8.53765104e-03, bound:  3.15653294e-01\n",
      "Epoch: 17174 mean train loss:  8.53673182e-03, bound:  3.15653235e-01\n",
      "Epoch: 17175 mean train loss:  8.53573252e-03, bound:  3.15653205e-01\n",
      "Epoch: 17176 mean train loss:  8.53468571e-03, bound:  3.15653145e-01\n",
      "Epoch: 17177 mean train loss:  8.53367243e-03, bound:  3.15653086e-01\n",
      "Epoch: 17178 mean train loss:  8.53270479e-03, bound:  3.15653056e-01\n",
      "Epoch: 17179 mean train loss:  8.53162073e-03, bound:  3.15652966e-01\n",
      "Epoch: 17180 mean train loss:  8.53064284e-03, bound:  3.15652966e-01\n",
      "Epoch: 17181 mean train loss:  8.52969475e-03, bound:  3.15652937e-01\n",
      "Epoch: 17182 mean train loss:  8.52864888e-03, bound:  3.15652847e-01\n",
      "Epoch: 17183 mean train loss:  8.52760207e-03, bound:  3.15652788e-01\n",
      "Epoch: 17184 mean train loss:  8.52672290e-03, bound:  3.15652728e-01\n",
      "Epoch: 17185 mean train loss:  8.52564070e-03, bound:  3.15652698e-01\n",
      "Epoch: 17186 mean train loss:  8.52470379e-03, bound:  3.15652639e-01\n",
      "Epoch: 17187 mean train loss:  8.52389447e-03, bound:  3.15652609e-01\n",
      "Epoch: 17188 mean train loss:  8.52289889e-03, bound:  3.15652549e-01\n",
      "Epoch: 17189 mean train loss:  8.52190889e-03, bound:  3.15652519e-01\n",
      "Epoch: 17190 mean train loss:  8.52114148e-03, bound:  3.15652430e-01\n",
      "Epoch: 17191 mean train loss:  8.52039549e-03, bound:  3.15652400e-01\n",
      "Epoch: 17192 mean train loss:  8.51964857e-03, bound:  3.15652341e-01\n",
      "Epoch: 17193 mean train loss:  8.51899572e-03, bound:  3.15652311e-01\n",
      "Epoch: 17194 mean train loss:  8.51845369e-03, bound:  3.15652251e-01\n",
      "Epoch: 17195 mean train loss:  8.51790886e-03, bound:  3.15652221e-01\n",
      "Epoch: 17196 mean train loss:  8.51768628e-03, bound:  3.15652162e-01\n",
      "Epoch: 17197 mean train loss:  8.51744041e-03, bound:  3.15652162e-01\n",
      "Epoch: 17198 mean train loss:  8.51757918e-03, bound:  3.15652043e-01\n",
      "Epoch: 17199 mean train loss:  8.51769932e-03, bound:  3.15652072e-01\n",
      "Epoch: 17200 mean train loss:  8.51815660e-03, bound:  3.15651953e-01\n",
      "Epoch: 17201 mean train loss:  8.51830188e-03, bound:  3.15651953e-01\n",
      "Epoch: 17202 mean train loss:  8.51829257e-03, bound:  3.15651834e-01\n",
      "Epoch: 17203 mean train loss:  8.51783156e-03, bound:  3.15651834e-01\n",
      "Epoch: 17204 mean train loss:  8.51664878e-03, bound:  3.15651715e-01\n",
      "Epoch: 17205 mean train loss:  8.51462968e-03, bound:  3.15651745e-01\n",
      "Epoch: 17206 mean train loss:  8.51174165e-03, bound:  3.15651625e-01\n",
      "Epoch: 17207 mean train loss:  8.50833207e-03, bound:  3.15651625e-01\n",
      "Epoch: 17208 mean train loss:  8.50510132e-03, bound:  3.15651536e-01\n",
      "Epoch: 17209 mean train loss:  8.50230642e-03, bound:  3.15651536e-01\n",
      "Epoch: 17210 mean train loss:  8.50050151e-03, bound:  3.15651476e-01\n",
      "Epoch: 17211 mean train loss:  8.49955529e-03, bound:  3.15651417e-01\n",
      "Epoch: 17212 mean train loss:  8.49916320e-03, bound:  3.15651387e-01\n",
      "Epoch: 17213 mean train loss:  8.49905331e-03, bound:  3.15651298e-01\n",
      "Epoch: 17214 mean train loss:  8.49888194e-03, bound:  3.15651298e-01\n",
      "Epoch: 17215 mean train loss:  8.49830080e-03, bound:  3.15651208e-01\n",
      "Epoch: 17216 mean train loss:  8.49734526e-03, bound:  3.15651208e-01\n",
      "Epoch: 17217 mean train loss:  8.49590171e-03, bound:  3.15651089e-01\n",
      "Epoch: 17218 mean train loss:  8.49426631e-03, bound:  3.15651089e-01\n",
      "Epoch: 17219 mean train loss:  8.49254988e-03, bound:  3.15651029e-01\n",
      "Epoch: 17220 mean train loss:  8.49095453e-03, bound:  3.15650970e-01\n",
      "Epoch: 17221 mean train loss:  8.48967582e-03, bound:  3.15650940e-01\n",
      "Epoch: 17222 mean train loss:  8.48851074e-03, bound:  3.15650851e-01\n",
      "Epoch: 17223 mean train loss:  8.48770235e-03, bound:  3.15650851e-01\n",
      "Epoch: 17224 mean train loss:  8.48703645e-03, bound:  3.15650821e-01\n",
      "Epoch: 17225 mean train loss:  8.48634541e-03, bound:  3.15650731e-01\n",
      "Epoch: 17226 mean train loss:  8.48557800e-03, bound:  3.15650672e-01\n",
      "Epoch: 17227 mean train loss:  8.48459266e-03, bound:  3.15650672e-01\n",
      "Epoch: 17228 mean train loss:  8.48367531e-03, bound:  3.15650612e-01\n",
      "Epoch: 17229 mean train loss:  8.48244876e-03, bound:  3.15650553e-01\n",
      "Epoch: 17230 mean train loss:  8.48116912e-03, bound:  3.15650493e-01\n",
      "Epoch: 17231 mean train loss:  8.47996213e-03, bound:  3.15650493e-01\n",
      "Epoch: 17232 mean train loss:  8.47874396e-03, bound:  3.15650403e-01\n",
      "Epoch: 17233 mean train loss:  8.47767852e-03, bound:  3.15650374e-01\n",
      "Epoch: 17234 mean train loss:  8.47667176e-03, bound:  3.15650314e-01\n",
      "Epoch: 17235 mean train loss:  8.47571064e-03, bound:  3.15650284e-01\n",
      "Epoch: 17236 mean train loss:  8.47484916e-03, bound:  3.15650225e-01\n",
      "Epoch: 17237 mean train loss:  8.47391225e-03, bound:  3.15650165e-01\n",
      "Epoch: 17238 mean train loss:  8.47298559e-03, bound:  3.15650135e-01\n",
      "Epoch: 17239 mean train loss:  8.47205985e-03, bound:  3.15650046e-01\n",
      "Epoch: 17240 mean train loss:  8.47105309e-03, bound:  3.15650016e-01\n",
      "Epoch: 17241 mean train loss:  8.47002678e-03, bound:  3.15649986e-01\n",
      "Epoch: 17242 mean train loss:  8.46892782e-03, bound:  3.15649956e-01\n",
      "Epoch: 17243 mean train loss:  8.46786797e-03, bound:  3.15649867e-01\n",
      "Epoch: 17244 mean train loss:  8.46680813e-03, bound:  3.15649837e-01\n",
      "Epoch: 17245 mean train loss:  8.46578646e-03, bound:  3.15649807e-01\n",
      "Epoch: 17246 mean train loss:  8.46475270e-03, bound:  3.15649748e-01\n",
      "Epoch: 17247 mean train loss:  8.46381672e-03, bound:  3.15649688e-01\n",
      "Epoch: 17248 mean train loss:  8.46276432e-03, bound:  3.15649629e-01\n",
      "Epoch: 17249 mean train loss:  8.46178364e-03, bound:  3.15649599e-01\n",
      "Epoch: 17250 mean train loss:  8.46077967e-03, bound:  3.15649539e-01\n",
      "Epoch: 17251 mean train loss:  8.45987163e-03, bound:  3.15649480e-01\n",
      "Epoch: 17252 mean train loss:  8.45884439e-03, bound:  3.15649420e-01\n",
      "Epoch: 17253 mean train loss:  8.45791120e-03, bound:  3.15649390e-01\n",
      "Epoch: 17254 mean train loss:  8.45689140e-03, bound:  3.15649360e-01\n",
      "Epoch: 17255 mean train loss:  8.45583901e-03, bound:  3.15649301e-01\n",
      "Epoch: 17256 mean train loss:  8.45489651e-03, bound:  3.15649271e-01\n",
      "Epoch: 17257 mean train loss:  8.45388975e-03, bound:  3.15649182e-01\n",
      "Epoch: 17258 mean train loss:  8.45287554e-03, bound:  3.15649182e-01\n",
      "Epoch: 17259 mean train loss:  8.45190231e-03, bound:  3.15649092e-01\n",
      "Epoch: 17260 mean train loss:  8.45093001e-03, bound:  3.15649062e-01\n",
      "Epoch: 17261 mean train loss:  8.44992045e-03, bound:  3.15649033e-01\n",
      "Epoch: 17262 mean train loss:  8.44896212e-03, bound:  3.15648973e-01\n",
      "Epoch: 17263 mean train loss:  8.44790414e-03, bound:  3.15648913e-01\n",
      "Epoch: 17264 mean train loss:  8.44694208e-03, bound:  3.15648854e-01\n",
      "Epoch: 17265 mean train loss:  8.44600983e-03, bound:  3.15648824e-01\n",
      "Epoch: 17266 mean train loss:  8.44493788e-03, bound:  3.15648764e-01\n",
      "Epoch: 17267 mean train loss:  8.44400004e-03, bound:  3.15648705e-01\n",
      "Epoch: 17268 mean train loss:  8.44297651e-03, bound:  3.15648705e-01\n",
      "Epoch: 17269 mean train loss:  8.44194740e-03, bound:  3.15648615e-01\n",
      "Epoch: 17270 mean train loss:  8.44100863e-03, bound:  3.15648586e-01\n",
      "Epoch: 17271 mean train loss:  8.44004657e-03, bound:  3.15648526e-01\n",
      "Epoch: 17272 mean train loss:  8.43898952e-03, bound:  3.15648496e-01\n",
      "Epoch: 17273 mean train loss:  8.43806285e-03, bound:  3.15648407e-01\n",
      "Epoch: 17274 mean train loss:  8.43706820e-03, bound:  3.15648377e-01\n",
      "Epoch: 17275 mean train loss:  8.43604840e-03, bound:  3.15648317e-01\n",
      "Epoch: 17276 mean train loss:  8.43508542e-03, bound:  3.15648288e-01\n",
      "Epoch: 17277 mean train loss:  8.43410753e-03, bound:  3.15648258e-01\n",
      "Epoch: 17278 mean train loss:  8.43305420e-03, bound:  3.15648168e-01\n",
      "Epoch: 17279 mean train loss:  8.43205769e-03, bound:  3.15648139e-01\n",
      "Epoch: 17280 mean train loss:  8.43106769e-03, bound:  3.15648079e-01\n",
      "Epoch: 17281 mean train loss:  8.43002368e-03, bound:  3.15648049e-01\n",
      "Epoch: 17282 mean train loss:  8.42904393e-03, bound:  3.15647960e-01\n",
      "Epoch: 17283 mean train loss:  8.42812564e-03, bound:  3.15647930e-01\n",
      "Epoch: 17284 mean train loss:  8.42707884e-03, bound:  3.15647870e-01\n",
      "Epoch: 17285 mean train loss:  8.42618570e-03, bound:  3.15647840e-01\n",
      "Epoch: 17286 mean train loss:  8.42514727e-03, bound:  3.15647751e-01\n",
      "Epoch: 17287 mean train loss:  8.42422619e-03, bound:  3.15647751e-01\n",
      "Epoch: 17288 mean train loss:  8.42327811e-03, bound:  3.15647691e-01\n",
      "Epoch: 17289 mean train loss:  8.42238776e-03, bound:  3.15647632e-01\n",
      "Epoch: 17290 mean train loss:  8.42140894e-03, bound:  3.15647602e-01\n",
      "Epoch: 17291 mean train loss:  8.42054654e-03, bound:  3.15647572e-01\n",
      "Epoch: 17292 mean train loss:  8.41973163e-03, bound:  3.15647483e-01\n",
      "Epoch: 17293 mean train loss:  8.41882825e-03, bound:  3.15647483e-01\n",
      "Epoch: 17294 mean train loss:  8.41819495e-03, bound:  3.15647393e-01\n",
      "Epoch: 17295 mean train loss:  8.41755606e-03, bound:  3.15647364e-01\n",
      "Epoch: 17296 mean train loss:  8.41703825e-03, bound:  3.15647274e-01\n",
      "Epoch: 17297 mean train loss:  8.41665082e-03, bound:  3.15647274e-01\n",
      "Epoch: 17298 mean train loss:  8.41668993e-03, bound:  3.15647185e-01\n",
      "Epoch: 17299 mean train loss:  8.41697771e-03, bound:  3.15647185e-01\n",
      "Epoch: 17300 mean train loss:  8.41788482e-03, bound:  3.15647066e-01\n",
      "Epoch: 17301 mean train loss:  8.41936842e-03, bound:  3.15647066e-01\n",
      "Epoch: 17302 mean train loss:  8.42131488e-03, bound:  3.15646976e-01\n",
      "Epoch: 17303 mean train loss:  8.42365436e-03, bound:  3.15647036e-01\n",
      "Epoch: 17304 mean train loss:  8.42545927e-03, bound:  3.15646857e-01\n",
      "Epoch: 17305 mean train loss:  8.42596870e-03, bound:  3.15646917e-01\n",
      "Epoch: 17306 mean train loss:  8.42395332e-03, bound:  3.15646738e-01\n",
      "Epoch: 17307 mean train loss:  8.41911417e-03, bound:  3.15646827e-01\n",
      "Epoch: 17308 mean train loss:  8.41244217e-03, bound:  3.15646708e-01\n",
      "Epoch: 17309 mean train loss:  8.40600673e-03, bound:  3.15646708e-01\n",
      "Epoch: 17310 mean train loss:  8.40189680e-03, bound:  3.15646619e-01\n",
      "Epoch: 17311 mean train loss:  8.40076618e-03, bound:  3.15646589e-01\n",
      "Epoch: 17312 mean train loss:  8.40197876e-03, bound:  3.15646589e-01\n",
      "Epoch: 17313 mean train loss:  8.40371847e-03, bound:  3.15646470e-01\n",
      "Epoch: 17314 mean train loss:  8.40450916e-03, bound:  3.15646470e-01\n",
      "Epoch: 17315 mean train loss:  8.40330776e-03, bound:  3.15646350e-01\n",
      "Epoch: 17316 mean train loss:  8.40039924e-03, bound:  3.15646380e-01\n",
      "Epoch: 17317 mean train loss:  8.39689188e-03, bound:  3.15646291e-01\n",
      "Epoch: 17318 mean train loss:  8.39416403e-03, bound:  3.15646291e-01\n",
      "Epoch: 17319 mean train loss:  8.39289464e-03, bound:  3.15646231e-01\n",
      "Epoch: 17320 mean train loss:  8.39285832e-03, bound:  3.15646172e-01\n",
      "Epoch: 17321 mean train loss:  8.39326438e-03, bound:  3.15646142e-01\n",
      "Epoch: 17322 mean train loss:  8.39309581e-03, bound:  3.15646052e-01\n",
      "Epoch: 17323 mean train loss:  8.39202572e-03, bound:  3.15646052e-01\n",
      "Epoch: 17324 mean train loss:  8.39010905e-03, bound:  3.15645963e-01\n",
      "Epoch: 17325 mean train loss:  8.38786643e-03, bound:  3.15645933e-01\n",
      "Epoch: 17326 mean train loss:  8.38616118e-03, bound:  3.15645903e-01\n",
      "Epoch: 17327 mean train loss:  8.38521309e-03, bound:  3.15645844e-01\n",
      "Epoch: 17328 mean train loss:  8.38480797e-03, bound:  3.15645844e-01\n",
      "Epoch: 17329 mean train loss:  8.38439539e-03, bound:  3.15645725e-01\n",
      "Epoch: 17330 mean train loss:  8.38368200e-03, bound:  3.15645725e-01\n",
      "Epoch: 17331 mean train loss:  8.38264730e-03, bound:  3.15645635e-01\n",
      "Epoch: 17332 mean train loss:  8.38126987e-03, bound:  3.15645635e-01\n",
      "Epoch: 17333 mean train loss:  8.37978628e-03, bound:  3.15645605e-01\n",
      "Epoch: 17334 mean train loss:  8.37857090e-03, bound:  3.15645546e-01\n",
      "Epoch: 17335 mean train loss:  8.37760326e-03, bound:  3.15645486e-01\n",
      "Epoch: 17336 mean train loss:  8.37682094e-03, bound:  3.15645427e-01\n",
      "Epoch: 17337 mean train loss:  8.37608706e-03, bound:  3.15645397e-01\n",
      "Epoch: 17338 mean train loss:  8.37523118e-03, bound:  3.15645337e-01\n",
      "Epoch: 17339 mean train loss:  8.37428309e-03, bound:  3.15645307e-01\n",
      "Epoch: 17340 mean train loss:  8.37318413e-03, bound:  3.15645248e-01\n",
      "Epoch: 17341 mean train loss:  8.37194640e-03, bound:  3.15645188e-01\n",
      "Epoch: 17342 mean train loss:  8.37089960e-03, bound:  3.15645158e-01\n",
      "Epoch: 17343 mean train loss:  8.36983230e-03, bound:  3.15645128e-01\n",
      "Epoch: 17344 mean train loss:  8.36895593e-03, bound:  3.15645039e-01\n",
      "Epoch: 17345 mean train loss:  8.36817827e-03, bound:  3.15645039e-01\n",
      "Epoch: 17346 mean train loss:  8.36715847e-03, bound:  3.15644950e-01\n",
      "Epoch: 17347 mean train loss:  8.36622156e-03, bound:  3.15644920e-01\n",
      "Epoch: 17348 mean train loss:  8.36525299e-03, bound:  3.15644890e-01\n",
      "Epoch: 17349 mean train loss:  8.36418010e-03, bound:  3.15644830e-01\n",
      "Epoch: 17350 mean train loss:  8.36323388e-03, bound:  3.15644801e-01\n",
      "Epoch: 17351 mean train loss:  8.36227089e-03, bound:  3.15644741e-01\n",
      "Epoch: 17352 mean train loss:  8.36126972e-03, bound:  3.15644681e-01\n",
      "Epoch: 17353 mean train loss:  8.36034026e-03, bound:  3.15644622e-01\n",
      "Epoch: 17354 mean train loss:  8.35944712e-03, bound:  3.15644592e-01\n",
      "Epoch: 17355 mean train loss:  8.35854746e-03, bound:  3.15644562e-01\n",
      "Epoch: 17356 mean train loss:  8.35752767e-03, bound:  3.15644503e-01\n",
      "Epoch: 17357 mean train loss:  8.35660100e-03, bound:  3.15644473e-01\n",
      "Epoch: 17358 mean train loss:  8.35555512e-03, bound:  3.15644413e-01\n",
      "Epoch: 17359 mean train loss:  8.35451204e-03, bound:  3.15644383e-01\n",
      "Epoch: 17360 mean train loss:  8.35361332e-03, bound:  3.15644294e-01\n",
      "Epoch: 17361 mean train loss:  8.35264008e-03, bound:  3.15644264e-01\n",
      "Epoch: 17362 mean train loss:  8.35176092e-03, bound:  3.15644234e-01\n",
      "Epoch: 17363 mean train loss:  8.35084636e-03, bound:  3.15644175e-01\n",
      "Epoch: 17364 mean train loss:  8.34986195e-03, bound:  3.15644115e-01\n",
      "Epoch: 17365 mean train loss:  8.34887754e-03, bound:  3.15644056e-01\n",
      "Epoch: 17366 mean train loss:  8.34797975e-03, bound:  3.15644026e-01\n",
      "Epoch: 17367 mean train loss:  8.34693015e-03, bound:  3.15644026e-01\n",
      "Epoch: 17368 mean train loss:  8.34598392e-03, bound:  3.15643936e-01\n",
      "Epoch: 17369 mean train loss:  8.34493432e-03, bound:  3.15643907e-01\n",
      "Epoch: 17370 mean train loss:  8.34405422e-03, bound:  3.15643817e-01\n",
      "Epoch: 17371 mean train loss:  8.34309589e-03, bound:  3.15643817e-01\n",
      "Epoch: 17372 mean train loss:  8.34212080e-03, bound:  3.15643758e-01\n",
      "Epoch: 17373 mean train loss:  8.34108423e-03, bound:  3.15643728e-01\n",
      "Epoch: 17374 mean train loss:  8.34018923e-03, bound:  3.15643638e-01\n",
      "Epoch: 17375 mean train loss:  8.33928678e-03, bound:  3.15643609e-01\n",
      "Epoch: 17376 mean train loss:  8.33838712e-03, bound:  3.15643579e-01\n",
      "Epoch: 17377 mean train loss:  8.33737757e-03, bound:  3.15643519e-01\n",
      "Epoch: 17378 mean train loss:  8.33638944e-03, bound:  3.15643489e-01\n",
      "Epoch: 17379 mean train loss:  8.33544787e-03, bound:  3.15643460e-01\n",
      "Epoch: 17380 mean train loss:  8.33451934e-03, bound:  3.15643370e-01\n",
      "Epoch: 17381 mean train loss:  8.33354611e-03, bound:  3.15643340e-01\n",
      "Epoch: 17382 mean train loss:  8.33259150e-03, bound:  3.15643311e-01\n",
      "Epoch: 17383 mean train loss:  8.33149254e-03, bound:  3.15643251e-01\n",
      "Epoch: 17384 mean train loss:  8.33054632e-03, bound:  3.15643191e-01\n",
      "Epoch: 17385 mean train loss:  8.32961220e-03, bound:  3.15643162e-01\n",
      "Epoch: 17386 mean train loss:  8.32872558e-03, bound:  3.15643102e-01\n",
      "Epoch: 17387 mean train loss:  8.32775608e-03, bound:  3.15643042e-01\n",
      "Epoch: 17388 mean train loss:  8.32681637e-03, bound:  3.15643042e-01\n",
      "Epoch: 17389 mean train loss:  8.32582917e-03, bound:  3.15642953e-01\n",
      "Epoch: 17390 mean train loss:  8.32487457e-03, bound:  3.15642923e-01\n",
      "Epoch: 17391 mean train loss:  8.32386874e-03, bound:  3.15642893e-01\n",
      "Epoch: 17392 mean train loss:  8.32302216e-03, bound:  3.15642834e-01\n",
      "Epoch: 17393 mean train loss:  8.32200982e-03, bound:  3.15642774e-01\n",
      "Epoch: 17394 mean train loss:  8.32109153e-03, bound:  3.15642715e-01\n",
      "Epoch: 17395 mean train loss:  8.32010712e-03, bound:  3.15642685e-01\n",
      "Epoch: 17396 mean train loss:  8.31915252e-03, bound:  3.15642625e-01\n",
      "Epoch: 17397 mean train loss:  8.31817649e-03, bound:  3.15642595e-01\n",
      "Epoch: 17398 mean train loss:  8.31711944e-03, bound:  3.15642506e-01\n",
      "Epoch: 17399 mean train loss:  8.31627101e-03, bound:  3.15642476e-01\n",
      "Epoch: 17400 mean train loss:  8.31524376e-03, bound:  3.15642476e-01\n",
      "Epoch: 17401 mean train loss:  8.31424631e-03, bound:  3.15642387e-01\n",
      "Epoch: 17402 mean train loss:  8.31331685e-03, bound:  3.15642357e-01\n",
      "Epoch: 17403 mean train loss:  8.31233244e-03, bound:  3.15642267e-01\n",
      "Epoch: 17404 mean train loss:  8.31135549e-03, bound:  3.15642238e-01\n",
      "Epoch: 17405 mean train loss:  8.31044093e-03, bound:  3.15642178e-01\n",
      "Epoch: 17406 mean train loss:  8.30947980e-03, bound:  3.15642148e-01\n",
      "Epoch: 17407 mean train loss:  8.30846094e-03, bound:  3.15642118e-01\n",
      "Epoch: 17408 mean train loss:  8.30754451e-03, bound:  3.15642059e-01\n",
      "Epoch: 17409 mean train loss:  8.30651727e-03, bound:  3.15642029e-01\n",
      "Epoch: 17410 mean train loss:  8.30567162e-03, bound:  3.15641999e-01\n",
      "Epoch: 17411 mean train loss:  8.30469839e-03, bound:  3.15641910e-01\n",
      "Epoch: 17412 mean train loss:  8.30371026e-03, bound:  3.15641910e-01\n",
      "Epoch: 17413 mean train loss:  8.30268301e-03, bound:  3.15641820e-01\n",
      "Epoch: 17414 mean train loss:  8.30172747e-03, bound:  3.15641791e-01\n",
      "Epoch: 17415 mean train loss:  8.30076635e-03, bound:  3.15641701e-01\n",
      "Epoch: 17416 mean train loss:  8.29980616e-03, bound:  3.15641671e-01\n",
      "Epoch: 17417 mean train loss:  8.29881523e-03, bound:  3.15641671e-01\n",
      "Epoch: 17418 mean train loss:  8.29783920e-03, bound:  3.15641612e-01\n",
      "Epoch: 17419 mean train loss:  8.29688460e-03, bound:  3.15641552e-01\n",
      "Epoch: 17420 mean train loss:  8.29596538e-03, bound:  3.15641493e-01\n",
      "Epoch: 17421 mean train loss:  8.29500612e-03, bound:  3.15641463e-01\n",
      "Epoch: 17422 mean train loss:  8.29403568e-03, bound:  3.15641373e-01\n",
      "Epoch: 17423 mean train loss:  8.29311553e-03, bound:  3.15641373e-01\n",
      "Epoch: 17424 mean train loss:  8.29212554e-03, bound:  3.15641344e-01\n",
      "Epoch: 17425 mean train loss:  8.29125009e-03, bound:  3.15641254e-01\n",
      "Epoch: 17426 mean train loss:  8.29032809e-03, bound:  3.15641224e-01\n",
      "Epoch: 17427 mean train loss:  8.28933343e-03, bound:  3.15641165e-01\n",
      "Epoch: 17428 mean train loss:  8.28845613e-03, bound:  3.15641135e-01\n",
      "Epoch: 17429 mean train loss:  8.28750152e-03, bound:  3.15641046e-01\n",
      "Epoch: 17430 mean train loss:  8.28650966e-03, bound:  3.15641046e-01\n",
      "Epoch: 17431 mean train loss:  8.28570034e-03, bound:  3.15640956e-01\n",
      "Epoch: 17432 mean train loss:  8.28472711e-03, bound:  3.15640926e-01\n",
      "Epoch: 17433 mean train loss:  8.28387495e-03, bound:  3.15640867e-01\n",
      "Epoch: 17434 mean train loss:  8.28304049e-03, bound:  3.15640867e-01\n",
      "Epoch: 17435 mean train loss:  8.28224607e-03, bound:  3.15640777e-01\n",
      "Epoch: 17436 mean train loss:  8.28129891e-03, bound:  3.15640748e-01\n",
      "Epoch: 17437 mean train loss:  8.28069821e-03, bound:  3.15640688e-01\n",
      "Epoch: 17438 mean train loss:  8.28014035e-03, bound:  3.15640658e-01\n",
      "Epoch: 17439 mean train loss:  8.27980880e-03, bound:  3.15640569e-01\n",
      "Epoch: 17440 mean train loss:  8.27970076e-03, bound:  3.15640569e-01\n",
      "Epoch: 17441 mean train loss:  8.27979390e-03, bound:  3.15640479e-01\n",
      "Epoch: 17442 mean train loss:  8.28019902e-03, bound:  3.15640509e-01\n",
      "Epoch: 17443 mean train loss:  8.28082673e-03, bound:  3.15640360e-01\n",
      "Epoch: 17444 mean train loss:  8.28164816e-03, bound:  3.15640390e-01\n",
      "Epoch: 17445 mean train loss:  8.28270428e-03, bound:  3.15640241e-01\n",
      "Epoch: 17446 mean train loss:  8.28366727e-03, bound:  3.15640330e-01\n",
      "Epoch: 17447 mean train loss:  8.28405563e-03, bound:  3.15640152e-01\n",
      "Epoch: 17448 mean train loss:  8.28326959e-03, bound:  3.15640241e-01\n",
      "Epoch: 17449 mean train loss:  8.28072242e-03, bound:  3.15640062e-01\n",
      "Epoch: 17450 mean train loss:  8.27682298e-03, bound:  3.15640122e-01\n",
      "Epoch: 17451 mean train loss:  8.27218685e-03, bound:  3.15640002e-01\n",
      "Epoch: 17452 mean train loss:  8.26775283e-03, bound:  3.15640002e-01\n",
      "Epoch: 17453 mean train loss:  8.26474000e-03, bound:  3.15639943e-01\n",
      "Epoch: 17454 mean train loss:  8.26344453e-03, bound:  3.15639913e-01\n",
      "Epoch: 17455 mean train loss:  8.26339237e-03, bound:  3.15639883e-01\n",
      "Epoch: 17456 mean train loss:  8.26402660e-03, bound:  3.15639794e-01\n",
      "Epoch: 17457 mean train loss:  8.26434977e-03, bound:  3.15639794e-01\n",
      "Epoch: 17458 mean train loss:  8.26401170e-03, bound:  3.15639704e-01\n",
      "Epoch: 17459 mean train loss:  8.26283079e-03, bound:  3.15639704e-01\n",
      "Epoch: 17460 mean train loss:  8.26081540e-03, bound:  3.15639585e-01\n",
      "Epoch: 17461 mean train loss:  8.25843960e-03, bound:  3.15639585e-01\n",
      "Epoch: 17462 mean train loss:  8.25637672e-03, bound:  3.15639555e-01\n",
      "Epoch: 17463 mean train loss:  8.25490151e-03, bound:  3.15639496e-01\n",
      "Epoch: 17464 mean train loss:  8.25417880e-03, bound:  3.15639466e-01\n",
      "Epoch: 17465 mean train loss:  8.25381838e-03, bound:  3.15639377e-01\n",
      "Epoch: 17466 mean train loss:  8.25347751e-03, bound:  3.15639377e-01\n",
      "Epoch: 17467 mean train loss:  8.25293735e-03, bound:  3.15639317e-01\n",
      "Epoch: 17468 mean train loss:  8.25203490e-03, bound:  3.15639287e-01\n",
      "Epoch: 17469 mean train loss:  8.25054757e-03, bound:  3.15639228e-01\n",
      "Epoch: 17470 mean train loss:  8.24913010e-03, bound:  3.15639228e-01\n",
      "Epoch: 17471 mean train loss:  8.24769307e-03, bound:  3.15639138e-01\n",
      "Epoch: 17472 mean train loss:  8.24651122e-03, bound:  3.15639108e-01\n",
      "Epoch: 17473 mean train loss:  8.24557990e-03, bound:  3.15639019e-01\n",
      "Epoch: 17474 mean train loss:  8.24477244e-03, bound:  3.15638989e-01\n",
      "Epoch: 17475 mean train loss:  8.24410189e-03, bound:  3.15638959e-01\n",
      "Epoch: 17476 mean train loss:  8.24338757e-03, bound:  3.15638900e-01\n",
      "Epoch: 17477 mean train loss:  8.24247859e-03, bound:  3.15638870e-01\n",
      "Epoch: 17478 mean train loss:  8.24144296e-03, bound:  3.15638810e-01\n",
      "Epoch: 17479 mean train loss:  8.24045390e-03, bound:  3.15638781e-01\n",
      "Epoch: 17480 mean train loss:  8.23934935e-03, bound:  3.15638691e-01\n",
      "Epoch: 17481 mean train loss:  8.23817402e-03, bound:  3.15638691e-01\n",
      "Epoch: 17482 mean train loss:  8.23716074e-03, bound:  3.15638632e-01\n",
      "Epoch: 17483 mean train loss:  8.23616982e-03, bound:  3.15638572e-01\n",
      "Epoch: 17484 mean train loss:  8.23525339e-03, bound:  3.15638542e-01\n",
      "Epoch: 17485 mean train loss:  8.23436771e-03, bound:  3.15638483e-01\n",
      "Epoch: 17486 mean train loss:  8.23349226e-03, bound:  3.15638453e-01\n",
      "Epoch: 17487 mean train loss:  8.23259912e-03, bound:  3.15638393e-01\n",
      "Epoch: 17488 mean train loss:  8.23174231e-03, bound:  3.15638363e-01\n",
      "Epoch: 17489 mean train loss:  8.23075417e-03, bound:  3.15638334e-01\n",
      "Epoch: 17490 mean train loss:  8.22970830e-03, bound:  3.15638274e-01\n",
      "Epoch: 17491 mean train loss:  8.22872389e-03, bound:  3.15638244e-01\n",
      "Epoch: 17492 mean train loss:  8.22776929e-03, bound:  3.15638155e-01\n",
      "Epoch: 17493 mean train loss:  8.22681189e-03, bound:  3.15638125e-01\n",
      "Epoch: 17494 mean train loss:  8.22593458e-03, bound:  3.15638095e-01\n",
      "Epoch: 17495 mean train loss:  8.22500139e-03, bound:  3.15638036e-01\n",
      "Epoch: 17496 mean train loss:  8.22409336e-03, bound:  3.15638006e-01\n",
      "Epoch: 17497 mean train loss:  8.22321512e-03, bound:  3.15637946e-01\n",
      "Epoch: 17498 mean train loss:  8.22225120e-03, bound:  3.15637887e-01\n",
      "Epoch: 17499 mean train loss:  8.22130125e-03, bound:  3.15637827e-01\n",
      "Epoch: 17500 mean train loss:  8.22036713e-03, bound:  3.15637827e-01\n",
      "Epoch: 17501 mean train loss:  8.21939856e-03, bound:  3.15637797e-01\n",
      "Epoch: 17502 mean train loss:  8.21848959e-03, bound:  3.15637708e-01\n",
      "Epoch: 17503 mean train loss:  8.21743719e-03, bound:  3.15637678e-01\n",
      "Epoch: 17504 mean train loss:  8.21647514e-03, bound:  3.15637589e-01\n",
      "Epoch: 17505 mean train loss:  8.21555685e-03, bound:  3.15637589e-01\n",
      "Epoch: 17506 mean train loss:  8.21458548e-03, bound:  3.15637559e-01\n",
      "Epoch: 17507 mean train loss:  8.21373519e-03, bound:  3.15637499e-01\n",
      "Epoch: 17508 mean train loss:  8.21275823e-03, bound:  3.15637439e-01\n",
      "Epoch: 17509 mean train loss:  8.21190607e-03, bound:  3.15637380e-01\n",
      "Epoch: 17510 mean train loss:  8.21093190e-03, bound:  3.15637350e-01\n",
      "Epoch: 17511 mean train loss:  8.20994936e-03, bound:  3.15637320e-01\n",
      "Epoch: 17512 mean train loss:  8.20902735e-03, bound:  3.15637261e-01\n",
      "Epoch: 17513 mean train loss:  8.20808206e-03, bound:  3.15637201e-01\n",
      "Epoch: 17514 mean train loss:  8.20714701e-03, bound:  3.15637141e-01\n",
      "Epoch: 17515 mean train loss:  8.20623990e-03, bound:  3.15637112e-01\n",
      "Epoch: 17516 mean train loss:  8.20526946e-03, bound:  3.15637112e-01\n",
      "Epoch: 17517 mean train loss:  8.20433348e-03, bound:  3.15637022e-01\n",
      "Epoch: 17518 mean train loss:  8.20334256e-03, bound:  3.15636992e-01\n",
      "Epoch: 17519 mean train loss:  8.20241682e-03, bound:  3.15636903e-01\n",
      "Epoch: 17520 mean train loss:  8.20154697e-03, bound:  3.15636903e-01\n",
      "Epoch: 17521 mean train loss:  8.20053276e-03, bound:  3.15636873e-01\n",
      "Epoch: 17522 mean train loss:  8.19957722e-03, bound:  3.15636814e-01\n",
      "Epoch: 17523 mean train loss:  8.19861796e-03, bound:  3.15636784e-01\n",
      "Epoch: 17524 mean train loss:  8.19773693e-03, bound:  3.15636694e-01\n",
      "Epoch: 17525 mean train loss:  8.19683634e-03, bound:  3.15636665e-01\n",
      "Epoch: 17526 mean train loss:  8.19582120e-03, bound:  3.15636605e-01\n",
      "Epoch: 17527 mean train loss:  8.19492061e-03, bound:  3.15636575e-01\n",
      "Epoch: 17528 mean train loss:  8.19400698e-03, bound:  3.15636545e-01\n",
      "Epoch: 17529 mean train loss:  8.19297042e-03, bound:  3.15636456e-01\n",
      "Epoch: 17530 mean train loss:  8.19203537e-03, bound:  3.15636426e-01\n",
      "Epoch: 17531 mean train loss:  8.19114503e-03, bound:  3.15636396e-01\n",
      "Epoch: 17532 mean train loss:  8.19015875e-03, bound:  3.15636337e-01\n",
      "Epoch: 17533 mean train loss:  8.18933081e-03, bound:  3.15636277e-01\n",
      "Epoch: 17534 mean train loss:  8.18835758e-03, bound:  3.15636247e-01\n",
      "Epoch: 17535 mean train loss:  8.18729121e-03, bound:  3.15636188e-01\n",
      "Epoch: 17536 mean train loss:  8.18644091e-03, bound:  3.15636158e-01\n",
      "Epoch: 17537 mean train loss:  8.18546303e-03, bound:  3.15636128e-01\n",
      "Epoch: 17538 mean train loss:  8.18458851e-03, bound:  3.15636098e-01\n",
      "Epoch: 17539 mean train loss:  8.18363763e-03, bound:  3.15636009e-01\n",
      "Epoch: 17540 mean train loss:  8.18274263e-03, bound:  3.15635979e-01\n",
      "Epoch: 17541 mean train loss:  8.18169117e-03, bound:  3.15635920e-01\n",
      "Epoch: 17542 mean train loss:  8.18070211e-03, bound:  3.15635890e-01\n",
      "Epoch: 17543 mean train loss:  8.17989744e-03, bound:  3.15635860e-01\n",
      "Epoch: 17544 mean train loss:  8.17889534e-03, bound:  3.15635771e-01\n",
      "Epoch: 17545 mean train loss:  8.17794818e-03, bound:  3.15635741e-01\n",
      "Epoch: 17546 mean train loss:  8.17709696e-03, bound:  3.15635681e-01\n",
      "Epoch: 17547 mean train loss:  8.17626808e-03, bound:  3.15635681e-01\n",
      "Epoch: 17548 mean train loss:  8.17534886e-03, bound:  3.15635592e-01\n",
      "Epoch: 17549 mean train loss:  8.17449111e-03, bound:  3.15635562e-01\n",
      "Epoch: 17550 mean train loss:  8.17368180e-03, bound:  3.15635562e-01\n",
      "Epoch: 17551 mean train loss:  8.17289762e-03, bound:  3.15635473e-01\n",
      "Epoch: 17552 mean train loss:  8.17207620e-03, bound:  3.15635443e-01\n",
      "Epoch: 17553 mean train loss:  8.17152392e-03, bound:  3.15635383e-01\n",
      "Epoch: 17554 mean train loss:  8.17090459e-03, bound:  3.15635353e-01\n",
      "Epoch: 17555 mean train loss:  8.17054696e-03, bound:  3.15635264e-01\n",
      "Epoch: 17556 mean train loss:  8.17017723e-03, bound:  3.15635264e-01\n",
      "Epoch: 17557 mean train loss:  8.17013998e-03, bound:  3.15635145e-01\n",
      "Epoch: 17558 mean train loss:  8.17030109e-03, bound:  3.15635145e-01\n",
      "Epoch: 17559 mean train loss:  8.17096233e-03, bound:  3.15635085e-01\n",
      "Epoch: 17560 mean train loss:  8.17196537e-03, bound:  3.15635115e-01\n",
      "Epoch: 17561 mean train loss:  8.17329064e-03, bound:  3.15634996e-01\n",
      "Epoch: 17562 mean train loss:  8.17463268e-03, bound:  3.15634996e-01\n",
      "Epoch: 17563 mean train loss:  8.17546807e-03, bound:  3.15634876e-01\n",
      "Epoch: 17564 mean train loss:  8.17545038e-03, bound:  3.15634906e-01\n",
      "Epoch: 17565 mean train loss:  8.17399006e-03, bound:  3.15634757e-01\n",
      "Epoch: 17566 mean train loss:  8.17079935e-03, bound:  3.15634817e-01\n",
      "Epoch: 17567 mean train loss:  8.16606823e-03, bound:  3.15634698e-01\n",
      "Epoch: 17568 mean train loss:  8.16090964e-03, bound:  3.15634698e-01\n",
      "Epoch: 17569 mean train loss:  8.15683976e-03, bound:  3.15634638e-01\n",
      "Epoch: 17570 mean train loss:  8.15464091e-03, bound:  3.15634578e-01\n",
      "Epoch: 17571 mean train loss:  8.15417338e-03, bound:  3.15634578e-01\n",
      "Epoch: 17572 mean train loss:  8.15492496e-03, bound:  3.15634489e-01\n",
      "Epoch: 17573 mean train loss:  8.15588608e-03, bound:  3.15634489e-01\n",
      "Epoch: 17574 mean train loss:  8.15605093e-03, bound:  3.15634370e-01\n",
      "Epoch: 17575 mean train loss:  8.15519132e-03, bound:  3.15634429e-01\n",
      "Epoch: 17576 mean train loss:  8.15299246e-03, bound:  3.15634310e-01\n",
      "Epoch: 17577 mean train loss:  8.15054215e-03, bound:  3.15634340e-01\n",
      "Epoch: 17578 mean train loss:  8.14801175e-03, bound:  3.15634221e-01\n",
      "Epoch: 17579 mean train loss:  8.14639963e-03, bound:  3.15634221e-01\n",
      "Epoch: 17580 mean train loss:  8.14557169e-03, bound:  3.15634161e-01\n",
      "Epoch: 17581 mean train loss:  8.14544782e-03, bound:  3.15634131e-01\n",
      "Epoch: 17582 mean train loss:  8.14530626e-03, bound:  3.15634102e-01\n",
      "Epoch: 17583 mean train loss:  8.14495422e-03, bound:  3.15634012e-01\n",
      "Epoch: 17584 mean train loss:  8.14400986e-03, bound:  3.15634012e-01\n",
      "Epoch: 17585 mean train loss:  8.14255979e-03, bound:  3.15633923e-01\n",
      "Epoch: 17586 mean train loss:  8.14097654e-03, bound:  3.15633893e-01\n",
      "Epoch: 17587 mean train loss:  8.13946221e-03, bound:  3.15633863e-01\n",
      "Epoch: 17588 mean train loss:  8.13809969e-03, bound:  3.15633804e-01\n",
      "Epoch: 17589 mean train loss:  8.13728850e-03, bound:  3.15633774e-01\n",
      "Epoch: 17590 mean train loss:  8.13664030e-03, bound:  3.15633714e-01\n",
      "Epoch: 17591 mean train loss:  8.13612528e-03, bound:  3.15633684e-01\n",
      "Epoch: 17592 mean train loss:  8.13538861e-03, bound:  3.15633625e-01\n",
      "Epoch: 17593 mean train loss:  8.13448615e-03, bound:  3.15633595e-01\n",
      "Epoch: 17594 mean train loss:  8.13335180e-03, bound:  3.15633565e-01\n",
      "Epoch: 17595 mean train loss:  8.13215133e-03, bound:  3.15633535e-01\n",
      "Epoch: 17596 mean train loss:  8.13096110e-03, bound:  3.15633446e-01\n",
      "Epoch: 17597 mean train loss:  8.13003350e-03, bound:  3.15633416e-01\n",
      "Epoch: 17598 mean train loss:  8.12899880e-03, bound:  3.15633357e-01\n",
      "Epoch: 17599 mean train loss:  8.12820438e-03, bound:  3.15633327e-01\n",
      "Epoch: 17600 mean train loss:  8.12743232e-03, bound:  3.15633297e-01\n",
      "Epoch: 17601 mean train loss:  8.12676828e-03, bound:  3.15633208e-01\n",
      "Epoch: 17602 mean train loss:  8.12580157e-03, bound:  3.15633208e-01\n",
      "Epoch: 17603 mean train loss:  8.12483858e-03, bound:  3.15633148e-01\n",
      "Epoch: 17604 mean train loss:  8.12374614e-03, bound:  3.15633118e-01\n",
      "Epoch: 17605 mean train loss:  8.12268723e-03, bound:  3.15633029e-01\n",
      "Epoch: 17606 mean train loss:  8.12176522e-03, bound:  3.15633029e-01\n",
      "Epoch: 17607 mean train loss:  8.12072959e-03, bound:  3.15632969e-01\n",
      "Epoch: 17608 mean train loss:  8.11993890e-03, bound:  3.15632910e-01\n",
      "Epoch: 17609 mean train loss:  8.11895542e-03, bound:  3.15632880e-01\n",
      "Epoch: 17610 mean train loss:  8.11813958e-03, bound:  3.15632850e-01\n",
      "Epoch: 17611 mean train loss:  8.11722968e-03, bound:  3.15632790e-01\n",
      "Epoch: 17612 mean train loss:  8.11640639e-03, bound:  3.15632761e-01\n",
      "Epoch: 17613 mean train loss:  8.11551604e-03, bound:  3.15632701e-01\n",
      "Epoch: 17614 mean train loss:  8.11467879e-03, bound:  3.15632641e-01\n",
      "Epoch: 17615 mean train loss:  8.11365433e-03, bound:  3.15632641e-01\n",
      "Epoch: 17616 mean train loss:  8.11269321e-03, bound:  3.15632582e-01\n",
      "Epoch: 17617 mean train loss:  8.11180472e-03, bound:  3.15632522e-01\n",
      "Epoch: 17618 mean train loss:  8.11070483e-03, bound:  3.15632463e-01\n",
      "Epoch: 17619 mean train loss:  8.10985547e-03, bound:  3.15632433e-01\n",
      "Epoch: 17620 mean train loss:  8.10893904e-03, bound:  3.15632433e-01\n",
      "Epoch: 17621 mean train loss:  8.10804032e-03, bound:  3.15632343e-01\n",
      "Epoch: 17622 mean train loss:  8.10716208e-03, bound:  3.15632313e-01\n",
      "Epoch: 17623 mean train loss:  8.10626056e-03, bound:  3.15632254e-01\n",
      "Epoch: 17624 mean train loss:  8.10541026e-03, bound:  3.15632224e-01\n",
      "Epoch: 17625 mean train loss:  8.10437649e-03, bound:  3.15632135e-01\n",
      "Epoch: 17626 mean train loss:  8.10350664e-03, bound:  3.15632105e-01\n",
      "Epoch: 17627 mean train loss:  8.10249243e-03, bound:  3.15632075e-01\n",
      "Epoch: 17628 mean train loss:  8.10160954e-03, bound:  3.15632015e-01\n",
      "Epoch: 17629 mean train loss:  8.10061675e-03, bound:  3.15632015e-01\n",
      "Epoch: 17630 mean train loss:  8.09982792e-03, bound:  3.15631926e-01\n",
      "Epoch: 17631 mean train loss:  8.09883699e-03, bound:  3.15631896e-01\n",
      "Epoch: 17632 mean train loss:  8.09796434e-03, bound:  3.15631866e-01\n",
      "Epoch: 17633 mean train loss:  8.09700321e-03, bound:  3.15631807e-01\n",
      "Epoch: 17634 mean train loss:  8.09616037e-03, bound:  3.15631777e-01\n",
      "Epoch: 17635 mean train loss:  8.09525605e-03, bound:  3.15631747e-01\n",
      "Epoch: 17636 mean train loss:  8.09419248e-03, bound:  3.15631658e-01\n",
      "Epoch: 17637 mean train loss:  8.09339713e-03, bound:  3.15631658e-01\n",
      "Epoch: 17638 mean train loss:  8.09246674e-03, bound:  3.15631598e-01\n",
      "Epoch: 17639 mean train loss:  8.09157174e-03, bound:  3.15631539e-01\n",
      "Epoch: 17640 mean train loss:  8.09069909e-03, bound:  3.15631509e-01\n",
      "Epoch: 17641 mean train loss:  8.08974355e-03, bound:  3.15631449e-01\n",
      "Epoch: 17642 mean train loss:  8.08876473e-03, bound:  3.15631419e-01\n",
      "Epoch: 17643 mean train loss:  8.08786787e-03, bound:  3.15631390e-01\n",
      "Epoch: 17644 mean train loss:  8.08694027e-03, bound:  3.15631330e-01\n",
      "Epoch: 17645 mean train loss:  8.08600616e-03, bound:  3.15631300e-01\n",
      "Epoch: 17646 mean train loss:  8.08507856e-03, bound:  3.15631241e-01\n",
      "Epoch: 17647 mean train loss:  8.08424316e-03, bound:  3.15631211e-01\n",
      "Epoch: 17648 mean train loss:  8.08325596e-03, bound:  3.15631151e-01\n",
      "Epoch: 17649 mean train loss:  8.08240473e-03, bound:  3.15631092e-01\n",
      "Epoch: 17650 mean train loss:  8.08148552e-03, bound:  3.15631062e-01\n",
      "Epoch: 17651 mean train loss:  8.08060262e-03, bound:  3.15631002e-01\n",
      "Epoch: 17652 mean train loss:  8.07965919e-03, bound:  3.15630972e-01\n",
      "Epoch: 17653 mean train loss:  8.07880331e-03, bound:  3.15630913e-01\n",
      "Epoch: 17654 mean train loss:  8.07784218e-03, bound:  3.15630883e-01\n",
      "Epoch: 17655 mean train loss:  8.07696674e-03, bound:  3.15630794e-01\n",
      "Epoch: 17656 mean train loss:  8.07599816e-03, bound:  3.15630794e-01\n",
      "Epoch: 17657 mean train loss:  8.07522610e-03, bound:  3.15630734e-01\n",
      "Epoch: 17658 mean train loss:  8.07418302e-03, bound:  3.15630674e-01\n",
      "Epoch: 17659 mean train loss:  8.07321444e-03, bound:  3.15630645e-01\n",
      "Epoch: 17660 mean train loss:  8.07239767e-03, bound:  3.15630615e-01\n",
      "Epoch: 17661 mean train loss:  8.07150174e-03, bound:  3.15630555e-01\n",
      "Epoch: 17662 mean train loss:  8.07050522e-03, bound:  3.15630525e-01\n",
      "Epoch: 17663 mean train loss:  8.06960743e-03, bound:  3.15630466e-01\n",
      "Epoch: 17664 mean train loss:  8.06867797e-03, bound:  3.15630436e-01\n",
      "Epoch: 17665 mean train loss:  8.06775875e-03, bound:  3.15630406e-01\n",
      "Epoch: 17666 mean train loss:  8.06680322e-03, bound:  3.15630347e-01\n",
      "Epoch: 17667 mean train loss:  8.06586631e-03, bound:  3.15630317e-01\n",
      "Epoch: 17668 mean train loss:  8.06496665e-03, bound:  3.15630227e-01\n",
      "Epoch: 17669 mean train loss:  8.06402974e-03, bound:  3.15630198e-01\n",
      "Epoch: 17670 mean train loss:  8.06315336e-03, bound:  3.15630138e-01\n",
      "Epoch: 17671 mean train loss:  8.06225557e-03, bound:  3.15630108e-01\n",
      "Epoch: 17672 mean train loss:  8.06135125e-03, bound:  3.15630078e-01\n",
      "Epoch: 17673 mean train loss:  8.06047488e-03, bound:  3.15630019e-01\n",
      "Epoch: 17674 mean train loss:  8.05963110e-03, bound:  3.15629989e-01\n",
      "Epoch: 17675 mean train loss:  8.05863738e-03, bound:  3.15629900e-01\n",
      "Epoch: 17676 mean train loss:  8.05790070e-03, bound:  3.15629900e-01\n",
      "Epoch: 17677 mean train loss:  8.05702992e-03, bound:  3.15629840e-01\n",
      "Epoch: 17678 mean train loss:  8.05623271e-03, bound:  3.15629780e-01\n",
      "Epoch: 17679 mean train loss:  8.05554166e-03, bound:  3.15629750e-01\n",
      "Epoch: 17680 mean train loss:  8.05488136e-03, bound:  3.15629750e-01\n",
      "Epoch: 17681 mean train loss:  8.05432443e-03, bound:  3.15629631e-01\n",
      "Epoch: 17682 mean train loss:  8.05388670e-03, bound:  3.15629661e-01\n",
      "Epoch: 17683 mean train loss:  8.05365015e-03, bound:  3.15629542e-01\n",
      "Epoch: 17684 mean train loss:  8.05377122e-03, bound:  3.15629542e-01\n",
      "Epoch: 17685 mean train loss:  8.05411022e-03, bound:  3.15629452e-01\n",
      "Epoch: 17686 mean train loss:  8.05497356e-03, bound:  3.15629452e-01\n",
      "Epoch: 17687 mean train loss:  8.05607345e-03, bound:  3.15629363e-01\n",
      "Epoch: 17688 mean train loss:  8.05755146e-03, bound:  3.15629363e-01\n",
      "Epoch: 17689 mean train loss:  8.05892050e-03, bound:  3.15629244e-01\n",
      "Epoch: 17690 mean train loss:  8.05981271e-03, bound:  3.15629303e-01\n",
      "Epoch: 17691 mean train loss:  8.05961061e-03, bound:  3.15629154e-01\n",
      "Epoch: 17692 mean train loss:  8.05775449e-03, bound:  3.15629214e-01\n",
      "Epoch: 17693 mean train loss:  8.05409346e-03, bound:  3.15629095e-01\n",
      "Epoch: 17694 mean train loss:  8.04916490e-03, bound:  3.15629095e-01\n",
      "Epoch: 17695 mean train loss:  8.04411434e-03, bound:  3.15629005e-01\n",
      "Epoch: 17696 mean train loss:  8.04034062e-03, bound:  3.15628976e-01\n",
      "Epoch: 17697 mean train loss:  8.03846121e-03, bound:  3.15628976e-01\n",
      "Epoch: 17698 mean train loss:  8.03854037e-03, bound:  3.15628886e-01\n",
      "Epoch: 17699 mean train loss:  8.03950801e-03, bound:  3.15628856e-01\n",
      "Epoch: 17700 mean train loss:  8.04053340e-03, bound:  3.15628797e-01\n",
      "Epoch: 17701 mean train loss:  8.04050080e-03, bound:  3.15628797e-01\n",
      "Epoch: 17702 mean train loss:  8.03938881e-03, bound:  3.15628707e-01\n",
      "Epoch: 17703 mean train loss:  8.03697668e-03, bound:  3.15628737e-01\n",
      "Epoch: 17704 mean train loss:  8.03435966e-03, bound:  3.15628648e-01\n",
      "Epoch: 17705 mean train loss:  8.03193077e-03, bound:  3.15628618e-01\n",
      "Epoch: 17706 mean train loss:  8.03042576e-03, bound:  3.15628558e-01\n",
      "Epoch: 17707 mean train loss:  8.02976731e-03, bound:  3.15628529e-01\n",
      "Epoch: 17708 mean train loss:  8.02950654e-03, bound:  3.15628499e-01\n",
      "Epoch: 17709 mean train loss:  8.02937243e-03, bound:  3.15628409e-01\n",
      "Epoch: 17710 mean train loss:  8.02868791e-03, bound:  3.15628409e-01\n",
      "Epoch: 17711 mean train loss:  8.02762713e-03, bound:  3.15628350e-01\n",
      "Epoch: 17712 mean train loss:  8.02612491e-03, bound:  3.15628320e-01\n",
      "Epoch: 17713 mean train loss:  8.02471023e-03, bound:  3.15628231e-01\n",
      "Epoch: 17714 mean train loss:  8.02347995e-03, bound:  3.15628231e-01\n",
      "Epoch: 17715 mean train loss:  8.02250300e-03, bound:  3.15628201e-01\n",
      "Epoch: 17716 mean train loss:  8.02175328e-03, bound:  3.15628171e-01\n",
      "Epoch: 17717 mean train loss:  8.02111439e-03, bound:  3.15628111e-01\n",
      "Epoch: 17718 mean train loss:  8.02048948e-03, bound:  3.15628052e-01\n",
      "Epoch: 17719 mean train loss:  8.01971368e-03, bound:  3.15628052e-01\n",
      "Epoch: 17720 mean train loss:  8.01866408e-03, bound:  3.15627962e-01\n",
      "Epoch: 17721 mean train loss:  8.01750459e-03, bound:  3.15627903e-01\n",
      "Epoch: 17722 mean train loss:  8.01650435e-03, bound:  3.15627873e-01\n",
      "Epoch: 17723 mean train loss:  8.01537465e-03, bound:  3.15627843e-01\n",
      "Epoch: 17724 mean train loss:  8.01440980e-03, bound:  3.15627784e-01\n",
      "Epoch: 17725 mean train loss:  8.01365450e-03, bound:  3.15627724e-01\n",
      "Epoch: 17726 mean train loss:  8.01283307e-03, bound:  3.15627724e-01\n",
      "Epoch: 17727 mean train loss:  8.01191386e-03, bound:  3.15627664e-01\n",
      "Epoch: 17728 mean train loss:  8.01104307e-03, bound:  3.15627635e-01\n",
      "Epoch: 17729 mean train loss:  8.01014341e-03, bound:  3.15627545e-01\n",
      "Epoch: 17730 mean train loss:  8.00927728e-03, bound:  3.15627545e-01\n",
      "Epoch: 17731 mean train loss:  8.00828449e-03, bound:  3.15627515e-01\n",
      "Epoch: 17732 mean train loss:  8.00732058e-03, bound:  3.15627426e-01\n",
      "Epoch: 17733 mean train loss:  8.00644606e-03, bound:  3.15627426e-01\n",
      "Epoch: 17734 mean train loss:  8.00547376e-03, bound:  3.15627337e-01\n",
      "Epoch: 17735 mean train loss:  8.00462440e-03, bound:  3.15627337e-01\n",
      "Epoch: 17736 mean train loss:  8.00387654e-03, bound:  3.15627277e-01\n",
      "Epoch: 17737 mean train loss:  8.00305046e-03, bound:  3.15627217e-01\n",
      "Epoch: 17738 mean train loss:  8.00213777e-03, bound:  3.15627187e-01\n",
      "Epoch: 17739 mean train loss:  8.00128095e-03, bound:  3.15627128e-01\n",
      "Epoch: 17740 mean train loss:  8.00032541e-03, bound:  3.15627098e-01\n",
      "Epoch: 17741 mean train loss:  7.99935590e-03, bound:  3.15627068e-01\n",
      "Epoch: 17742 mean train loss:  7.99843483e-03, bound:  3.15627009e-01\n",
      "Epoch: 17743 mean train loss:  7.99754914e-03, bound:  3.15626979e-01\n",
      "Epoch: 17744 mean train loss:  7.99655262e-03, bound:  3.15626949e-01\n",
      "Epoch: 17745 mean train loss:  7.99573027e-03, bound:  3.15626860e-01\n",
      "Epoch: 17746 mean train loss:  7.99487717e-03, bound:  3.15626830e-01\n",
      "Epoch: 17747 mean train loss:  7.99396448e-03, bound:  3.15626800e-01\n",
      "Epoch: 17748 mean train loss:  7.99310021e-03, bound:  3.15626740e-01\n",
      "Epoch: 17749 mean train loss:  7.99224060e-03, bound:  3.15626711e-01\n",
      "Epoch: 17750 mean train loss:  7.99132604e-03, bound:  3.15626681e-01\n",
      "Epoch: 17751 mean train loss:  7.99049623e-03, bound:  3.15626621e-01\n",
      "Epoch: 17752 mean train loss:  7.98956957e-03, bound:  3.15626562e-01\n",
      "Epoch: 17753 mean train loss:  7.98864011e-03, bound:  3.15626532e-01\n",
      "Epoch: 17754 mean train loss:  7.98771065e-03, bound:  3.15626502e-01\n",
      "Epoch: 17755 mean train loss:  7.98680354e-03, bound:  3.15626442e-01\n",
      "Epoch: 17756 mean train loss:  7.98586383e-03, bound:  3.15626413e-01\n",
      "Epoch: 17757 mean train loss:  7.98509177e-03, bound:  3.15626383e-01\n",
      "Epoch: 17758 mean train loss:  7.98416138e-03, bound:  3.15626293e-01\n",
      "Epoch: 17759 mean train loss:  7.98325147e-03, bound:  3.15626264e-01\n",
      "Epoch: 17760 mean train loss:  7.98241049e-03, bound:  3.15626234e-01\n",
      "Epoch: 17761 mean train loss:  7.98149500e-03, bound:  3.15626174e-01\n",
      "Epoch: 17762 mean train loss:  7.98048824e-03, bound:  3.15626144e-01\n",
      "Epoch: 17763 mean train loss:  7.97959324e-03, bound:  3.15626115e-01\n",
      "Epoch: 17764 mean train loss:  7.97875319e-03, bound:  3.15626085e-01\n",
      "Epoch: 17765 mean train loss:  7.97790289e-03, bound:  3.15625995e-01\n",
      "Epoch: 17766 mean train loss:  7.97701906e-03, bound:  3.15625966e-01\n",
      "Epoch: 17767 mean train loss:  7.97611196e-03, bound:  3.15625936e-01\n",
      "Epoch: 17768 mean train loss:  7.97517411e-03, bound:  3.15625876e-01\n",
      "Epoch: 17769 mean train loss:  7.97425117e-03, bound:  3.15625846e-01\n",
      "Epoch: 17770 mean train loss:  7.97339715e-03, bound:  3.15625817e-01\n",
      "Epoch: 17771 mean train loss:  7.97261391e-03, bound:  3.15625757e-01\n",
      "Epoch: 17772 mean train loss:  7.97163881e-03, bound:  3.15625727e-01\n",
      "Epoch: 17773 mean train loss:  7.97078386e-03, bound:  3.15625668e-01\n",
      "Epoch: 17774 mean train loss:  7.96986092e-03, bound:  3.15625638e-01\n",
      "Epoch: 17775 mean train loss:  7.96900317e-03, bound:  3.15625548e-01\n",
      "Epoch: 17776 mean train loss:  7.96813052e-03, bound:  3.15625519e-01\n",
      "Epoch: 17777 mean train loss:  7.96724204e-03, bound:  3.15625519e-01\n",
      "Epoch: 17778 mean train loss:  7.96629302e-03, bound:  3.15625429e-01\n",
      "Epoch: 17779 mean train loss:  7.96537381e-03, bound:  3.15625399e-01\n",
      "Epoch: 17780 mean train loss:  7.96451047e-03, bound:  3.15625370e-01\n",
      "Epoch: 17781 mean train loss:  7.96351023e-03, bound:  3.15625310e-01\n",
      "Epoch: 17782 mean train loss:  7.96279125e-03, bound:  3.15625280e-01\n",
      "Epoch: 17783 mean train loss:  7.96183106e-03, bound:  3.15625221e-01\n",
      "Epoch: 17784 mean train loss:  7.96091929e-03, bound:  3.15625191e-01\n",
      "Epoch: 17785 mean train loss:  7.96004292e-03, bound:  3.15625161e-01\n",
      "Epoch: 17786 mean train loss:  7.95914326e-03, bound:  3.15625101e-01\n",
      "Epoch: 17787 mean train loss:  7.95822497e-03, bound:  3.15625072e-01\n",
      "Epoch: 17788 mean train loss:  7.95732345e-03, bound:  3.15625042e-01\n",
      "Epoch: 17789 mean train loss:  7.95636792e-03, bound:  3.15624952e-01\n",
      "Epoch: 17790 mean train loss:  7.95554835e-03, bound:  3.15624893e-01\n",
      "Epoch: 17791 mean train loss:  7.95463659e-03, bound:  3.15624863e-01\n",
      "Epoch: 17792 mean train loss:  7.95369223e-03, bound:  3.15624833e-01\n",
      "Epoch: 17793 mean train loss:  7.95284472e-03, bound:  3.15624833e-01\n",
      "Epoch: 17794 mean train loss:  7.95194041e-03, bound:  3.15624744e-01\n",
      "Epoch: 17795 mean train loss:  7.95113202e-03, bound:  3.15624714e-01\n",
      "Epoch: 17796 mean train loss:  7.95014575e-03, bound:  3.15624624e-01\n",
      "Epoch: 17797 mean train loss:  7.94923119e-03, bound:  3.15624595e-01\n",
      "Epoch: 17798 mean train loss:  7.94830266e-03, bound:  3.15624565e-01\n",
      "Epoch: 17799 mean train loss:  7.94743001e-03, bound:  3.15624535e-01\n",
      "Epoch: 17800 mean train loss:  7.94659648e-03, bound:  3.15624505e-01\n",
      "Epoch: 17801 mean train loss:  7.94568192e-03, bound:  3.15624416e-01\n",
      "Epoch: 17802 mean train loss:  7.94479065e-03, bound:  3.15624386e-01\n",
      "Epoch: 17803 mean train loss:  7.94383883e-03, bound:  3.15624326e-01\n",
      "Epoch: 17804 mean train loss:  7.94291776e-03, bound:  3.15624297e-01\n",
      "Epoch: 17805 mean train loss:  7.94205070e-03, bound:  3.15624267e-01\n",
      "Epoch: 17806 mean train loss:  7.94123486e-03, bound:  3.15624207e-01\n",
      "Epoch: 17807 mean train loss:  7.94030633e-03, bound:  3.15624177e-01\n",
      "Epoch: 17808 mean train loss:  7.93941133e-03, bound:  3.15624118e-01\n",
      "Epoch: 17809 mean train loss:  7.93853216e-03, bound:  3.15624088e-01\n",
      "Epoch: 17810 mean train loss:  7.93766789e-03, bound:  3.15624028e-01\n",
      "Epoch: 17811 mean train loss:  7.93687161e-03, bound:  3.15623999e-01\n",
      "Epoch: 17812 mean train loss:  7.93594774e-03, bound:  3.15623969e-01\n",
      "Epoch: 17813 mean train loss:  7.93512538e-03, bound:  3.15623909e-01\n",
      "Epoch: 17814 mean train loss:  7.93441385e-03, bound:  3.15623850e-01\n",
      "Epoch: 17815 mean train loss:  7.93364365e-03, bound:  3.15623820e-01\n",
      "Epoch: 17816 mean train loss:  7.93302618e-03, bound:  3.15623760e-01\n",
      "Epoch: 17817 mean train loss:  7.93249067e-03, bound:  3.15623730e-01\n",
      "Epoch: 17818 mean train loss:  7.93209486e-03, bound:  3.15623701e-01\n",
      "Epoch: 17819 mean train loss:  7.93194957e-03, bound:  3.15623671e-01\n",
      "Epoch: 17820 mean train loss:  7.93237053e-03, bound:  3.15623581e-01\n",
      "Epoch: 17821 mean train loss:  7.93336611e-03, bound:  3.15623581e-01\n",
      "Epoch: 17822 mean train loss:  7.93516263e-03, bound:  3.15623492e-01\n",
      "Epoch: 17823 mean train loss:  7.93812890e-03, bound:  3.15623492e-01\n",
      "Epoch: 17824 mean train loss:  7.94210564e-03, bound:  3.15623403e-01\n",
      "Epoch: 17825 mean train loss:  7.94672966e-03, bound:  3.15623432e-01\n",
      "Epoch: 17826 mean train loss:  7.95084983e-03, bound:  3.15623283e-01\n",
      "Epoch: 17827 mean train loss:  7.95193855e-03, bound:  3.15623403e-01\n",
      "Epoch: 17828 mean train loss:  7.94780254e-03, bound:  3.15623164e-01\n",
      "Epoch: 17829 mean train loss:  7.93841016e-03, bound:  3.15623283e-01\n",
      "Epoch: 17830 mean train loss:  7.92731438e-03, bound:  3.15623164e-01\n",
      "Epoch: 17831 mean train loss:  7.91996717e-03, bound:  3.15623164e-01\n",
      "Epoch: 17832 mean train loss:  7.91870896e-03, bound:  3.15623105e-01\n",
      "Epoch: 17833 mean train loss:  7.92217813e-03, bound:  3.15623045e-01\n",
      "Epoch: 17834 mean train loss:  7.92637374e-03, bound:  3.15623075e-01\n",
      "Epoch: 17835 mean train loss:  7.92740285e-03, bound:  3.15622926e-01\n",
      "Epoch: 17836 mean train loss:  7.92404544e-03, bound:  3.15622985e-01\n",
      "Epoch: 17837 mean train loss:  7.91818276e-03, bound:  3.15622866e-01\n",
      "Epoch: 17838 mean train loss:  7.91351032e-03, bound:  3.15622866e-01\n",
      "Epoch: 17839 mean train loss:  7.91234430e-03, bound:  3.15622836e-01\n",
      "Epoch: 17840 mean train loss:  7.91382883e-03, bound:  3.15622747e-01\n",
      "Epoch: 17841 mean train loss:  7.91540928e-03, bound:  3.15622747e-01\n",
      "Epoch: 17842 mean train loss:  7.91503210e-03, bound:  3.15622628e-01\n",
      "Epoch: 17843 mean train loss:  7.91227911e-03, bound:  3.15622658e-01\n",
      "Epoch: 17844 mean train loss:  7.90906604e-03, bound:  3.15622598e-01\n",
      "Epoch: 17845 mean train loss:  7.90701900e-03, bound:  3.15622598e-01\n",
      "Epoch: 17846 mean train loss:  7.90658314e-03, bound:  3.15622538e-01\n",
      "Epoch: 17847 mean train loss:  7.90701620e-03, bound:  3.15622479e-01\n",
      "Epoch: 17848 mean train loss:  7.90702924e-03, bound:  3.15622479e-01\n",
      "Epoch: 17849 mean train loss:  7.90602993e-03, bound:  3.15622360e-01\n",
      "Epoch: 17850 mean train loss:  7.90411141e-03, bound:  3.15622360e-01\n",
      "Epoch: 17851 mean train loss:  7.90214725e-03, bound:  3.15622300e-01\n",
      "Epoch: 17852 mean train loss:  7.90096819e-03, bound:  3.15622270e-01\n",
      "Epoch: 17853 mean train loss:  7.90072791e-03, bound:  3.15622270e-01\n",
      "Epoch: 17854 mean train loss:  7.90039450e-03, bound:  3.15622181e-01\n",
      "Epoch: 17855 mean train loss:  7.89979193e-03, bound:  3.15622181e-01\n",
      "Epoch: 17856 mean train loss:  7.89858028e-03, bound:  3.15622091e-01\n",
      "Epoch: 17857 mean train loss:  7.89718609e-03, bound:  3.15622061e-01\n",
      "Epoch: 17858 mean train loss:  7.89600145e-03, bound:  3.15622032e-01\n",
      "Epoch: 17859 mean train loss:  7.89512228e-03, bound:  3.15621972e-01\n",
      "Epoch: 17860 mean train loss:  7.89456442e-03, bound:  3.15621942e-01\n",
      "Epoch: 17861 mean train loss:  7.89386686e-03, bound:  3.15621883e-01\n",
      "Epoch: 17862 mean train loss:  7.89298583e-03, bound:  3.15621853e-01\n",
      "Epoch: 17863 mean train loss:  7.89200049e-03, bound:  3.15621823e-01\n",
      "Epoch: 17864 mean train loss:  7.89082143e-03, bound:  3.15621763e-01\n",
      "Epoch: 17865 mean train loss:  7.88993016e-03, bound:  3.15621734e-01\n",
      "Epoch: 17866 mean train loss:  7.88914692e-03, bound:  3.15621704e-01\n",
      "Epoch: 17867 mean train loss:  7.88836181e-03, bound:  3.15621644e-01\n",
      "Epoch: 17868 mean train loss:  7.88771827e-03, bound:  3.15621614e-01\n",
      "Epoch: 17869 mean train loss:  7.88681209e-03, bound:  3.15621585e-01\n",
      "Epoch: 17870 mean train loss:  7.88575504e-03, bound:  3.15621525e-01\n",
      "Epoch: 17871 mean train loss:  7.88484421e-03, bound:  3.15621495e-01\n",
      "Epoch: 17872 mean train loss:  7.88400788e-03, bound:  3.15621465e-01\n",
      "Epoch: 17873 mean train loss:  7.88319856e-03, bound:  3.15621406e-01\n",
      "Epoch: 17874 mean train loss:  7.88245630e-03, bound:  3.15621376e-01\n",
      "Epoch: 17875 mean train loss:  7.88154919e-03, bound:  3.15621316e-01\n",
      "Epoch: 17876 mean train loss:  7.88073335e-03, bound:  3.15621287e-01\n",
      "Epoch: 17877 mean train loss:  7.87984580e-03, bound:  3.15621257e-01\n",
      "Epoch: 17878 mean train loss:  7.87890051e-03, bound:  3.15621197e-01\n",
      "Epoch: 17879 mean train loss:  7.87798595e-03, bound:  3.15621167e-01\n",
      "Epoch: 17880 mean train loss:  7.87710864e-03, bound:  3.15621138e-01\n",
      "Epoch: 17881 mean train loss:  7.87630398e-03, bound:  3.15621078e-01\n",
      "Epoch: 17882 mean train loss:  7.87547510e-03, bound:  3.15621048e-01\n",
      "Epoch: 17883 mean train loss:  7.87461735e-03, bound:  3.15620989e-01\n",
      "Epoch: 17884 mean train loss:  7.87378475e-03, bound:  3.15620929e-01\n",
      "Epoch: 17885 mean train loss:  7.87292793e-03, bound:  3.15620929e-01\n",
      "Epoch: 17886 mean train loss:  7.87208974e-03, bound:  3.15620869e-01\n",
      "Epoch: 17887 mean train loss:  7.87122454e-03, bound:  3.15620840e-01\n",
      "Epoch: 17888 mean train loss:  7.87035562e-03, bound:  3.15620810e-01\n",
      "Epoch: 17889 mean train loss:  7.86954258e-03, bound:  3.15620750e-01\n",
      "Epoch: 17890 mean train loss:  7.86866620e-03, bound:  3.15620720e-01\n",
      "Epoch: 17891 mean train loss:  7.86776375e-03, bound:  3.15620631e-01\n",
      "Epoch: 17892 mean train loss:  7.86692183e-03, bound:  3.15620631e-01\n",
      "Epoch: 17893 mean train loss:  7.86611158e-03, bound:  3.15620601e-01\n",
      "Epoch: 17894 mean train loss:  7.86520634e-03, bound:  3.15620512e-01\n",
      "Epoch: 17895 mean train loss:  7.86434487e-03, bound:  3.15620482e-01\n",
      "Epoch: 17896 mean train loss:  7.86356069e-03, bound:  3.15620422e-01\n",
      "Epoch: 17897 mean train loss:  7.86262937e-03, bound:  3.15620422e-01\n",
      "Epoch: 17898 mean train loss:  7.86186941e-03, bound:  3.15620363e-01\n",
      "Epoch: 17899 mean train loss:  7.86102470e-03, bound:  3.15620363e-01\n",
      "Epoch: 17900 mean train loss:  7.86016695e-03, bound:  3.15620303e-01\n",
      "Epoch: 17901 mean train loss:  7.85924867e-03, bound:  3.15620244e-01\n",
      "Epoch: 17902 mean train loss:  7.85844680e-03, bound:  3.15620184e-01\n",
      "Epoch: 17903 mean train loss:  7.85752013e-03, bound:  3.15620184e-01\n",
      "Epoch: 17904 mean train loss:  7.85663351e-03, bound:  3.15620154e-01\n",
      "Epoch: 17905 mean train loss:  7.85579998e-03, bound:  3.15620065e-01\n",
      "Epoch: 17906 mean train loss:  7.85507169e-03, bound:  3.15620035e-01\n",
      "Epoch: 17907 mean train loss:  7.85411801e-03, bound:  3.15620035e-01\n",
      "Epoch: 17908 mean train loss:  7.85323791e-03, bound:  3.15619946e-01\n",
      "Epoch: 17909 mean train loss:  7.85236899e-03, bound:  3.15619916e-01\n",
      "Epoch: 17910 mean train loss:  7.85159133e-03, bound:  3.15619856e-01\n",
      "Epoch: 17911 mean train loss:  7.85077643e-03, bound:  3.15619826e-01\n",
      "Epoch: 17912 mean train loss:  7.84992799e-03, bound:  3.15619797e-01\n",
      "Epoch: 17913 mean train loss:  7.84907490e-03, bound:  3.15619737e-01\n",
      "Epoch: 17914 mean train loss:  7.84814358e-03, bound:  3.15619707e-01\n",
      "Epoch: 17915 mean train loss:  7.84725975e-03, bound:  3.15619677e-01\n",
      "Epoch: 17916 mean train loss:  7.84646720e-03, bound:  3.15619618e-01\n",
      "Epoch: 17917 mean train loss:  7.84557778e-03, bound:  3.15619618e-01\n",
      "Epoch: 17918 mean train loss:  7.84474798e-03, bound:  3.15619498e-01\n",
      "Epoch: 17919 mean train loss:  7.84384273e-03, bound:  3.15619498e-01\n",
      "Epoch: 17920 mean train loss:  7.84301385e-03, bound:  3.15619469e-01\n",
      "Epoch: 17921 mean train loss:  7.84226693e-03, bound:  3.15619409e-01\n",
      "Epoch: 17922 mean train loss:  7.84135237e-03, bound:  3.15619379e-01\n",
      "Epoch: 17923 mean train loss:  7.84053933e-03, bound:  3.15619320e-01\n",
      "Epoch: 17924 mean train loss:  7.83967506e-03, bound:  3.15619290e-01\n",
      "Epoch: 17925 mean train loss:  7.83879030e-03, bound:  3.15619260e-01\n",
      "Epoch: 17926 mean train loss:  7.83789158e-03, bound:  3.15619200e-01\n",
      "Epoch: 17927 mean train loss:  7.83701707e-03, bound:  3.15619171e-01\n",
      "Epoch: 17928 mean train loss:  7.83618074e-03, bound:  3.15619141e-01\n",
      "Epoch: 17929 mean train loss:  7.83528574e-03, bound:  3.15619081e-01\n",
      "Epoch: 17930 mean train loss:  7.83450063e-03, bound:  3.15619022e-01\n",
      "Epoch: 17931 mean train loss:  7.83359818e-03, bound:  3.15618962e-01\n",
      "Epoch: 17932 mean train loss:  7.83274975e-03, bound:  3.15618932e-01\n",
      "Epoch: 17933 mean train loss:  7.83192553e-03, bound:  3.15618932e-01\n",
      "Epoch: 17934 mean train loss:  7.83102959e-03, bound:  3.15618873e-01\n",
      "Epoch: 17935 mean train loss:  7.83016160e-03, bound:  3.15618813e-01\n",
      "Epoch: 17936 mean train loss:  7.82927405e-03, bound:  3.15618813e-01\n",
      "Epoch: 17937 mean train loss:  7.82849826e-03, bound:  3.15618753e-01\n",
      "Epoch: 17938 mean train loss:  7.82761443e-03, bound:  3.15618694e-01\n",
      "Epoch: 17939 mean train loss:  7.82680977e-03, bound:  3.15618634e-01\n",
      "Epoch: 17940 mean train loss:  7.82585889e-03, bound:  3.15618604e-01\n",
      "Epoch: 17941 mean train loss:  7.82503095e-03, bound:  3.15618575e-01\n",
      "Epoch: 17942 mean train loss:  7.82417972e-03, bound:  3.15618515e-01\n",
      "Epoch: 17943 mean train loss:  7.82324467e-03, bound:  3.15618485e-01\n",
      "Epoch: 17944 mean train loss:  7.82248471e-03, bound:  3.15618455e-01\n",
      "Epoch: 17945 mean train loss:  7.82168377e-03, bound:  3.15618396e-01\n",
      "Epoch: 17946 mean train loss:  7.82073289e-03, bound:  3.15618366e-01\n",
      "Epoch: 17947 mean train loss:  7.81982020e-03, bound:  3.15618336e-01\n",
      "Epoch: 17948 mean train loss:  7.81898573e-03, bound:  3.15618306e-01\n",
      "Epoch: 17949 mean train loss:  7.81816058e-03, bound:  3.15618247e-01\n",
      "Epoch: 17950 mean train loss:  7.81727117e-03, bound:  3.15618187e-01\n",
      "Epoch: 17951 mean train loss:  7.81639479e-03, bound:  3.15618128e-01\n",
      "Epoch: 17952 mean train loss:  7.81556591e-03, bound:  3.15618068e-01\n",
      "Epoch: 17953 mean train loss:  7.81476870e-03, bound:  3.15618068e-01\n",
      "Epoch: 17954 mean train loss:  7.81385042e-03, bound:  3.15618038e-01\n",
      "Epoch: 17955 mean train loss:  7.81298801e-03, bound:  3.15618008e-01\n",
      "Epoch: 17956 mean train loss:  7.81216240e-03, bound:  3.15617949e-01\n",
      "Epoch: 17957 mean train loss:  7.81129673e-03, bound:  3.15617919e-01\n",
      "Epoch: 17958 mean train loss:  7.81050930e-03, bound:  3.15617830e-01\n",
      "Epoch: 17959 mean train loss:  7.80962454e-03, bound:  3.15617830e-01\n",
      "Epoch: 17960 mean train loss:  7.80871371e-03, bound:  3.15617740e-01\n",
      "Epoch: 17961 mean train loss:  7.80777959e-03, bound:  3.15617710e-01\n",
      "Epoch: 17962 mean train loss:  7.80697819e-03, bound:  3.15617681e-01\n",
      "Epoch: 17963 mean train loss:  7.80608691e-03, bound:  3.15617621e-01\n",
      "Epoch: 17964 mean train loss:  7.80530833e-03, bound:  3.15617591e-01\n",
      "Epoch: 17965 mean train loss:  7.80445756e-03, bound:  3.15617561e-01\n",
      "Epoch: 17966 mean train loss:  7.80356070e-03, bound:  3.15617502e-01\n",
      "Epoch: 17967 mean train loss:  7.80260609e-03, bound:  3.15617502e-01\n",
      "Epoch: 17968 mean train loss:  7.80180562e-03, bound:  3.15617442e-01\n",
      "Epoch: 17969 mean train loss:  7.80092878e-03, bound:  3.15617383e-01\n",
      "Epoch: 17970 mean train loss:  7.80012645e-03, bound:  3.15617383e-01\n",
      "Epoch: 17971 mean train loss:  7.79924449e-03, bound:  3.15617263e-01\n",
      "Epoch: 17972 mean train loss:  7.79835228e-03, bound:  3.15617263e-01\n",
      "Epoch: 17973 mean train loss:  7.79754715e-03, bound:  3.15617234e-01\n",
      "Epoch: 17974 mean train loss:  7.79669732e-03, bound:  3.15617174e-01\n",
      "Epoch: 17975 mean train loss:  7.79579952e-03, bound:  3.15617144e-01\n",
      "Epoch: 17976 mean train loss:  7.79497344e-03, bound:  3.15617085e-01\n",
      "Epoch: 17977 mean train loss:  7.79411895e-03, bound:  3.15617055e-01\n",
      "Epoch: 17978 mean train loss:  7.79327517e-03, bound:  3.15617025e-01\n",
      "Epoch: 17979 mean train loss:  7.79232336e-03, bound:  3.15616965e-01\n",
      "Epoch: 17980 mean train loss:  7.79151404e-03, bound:  3.15616935e-01\n",
      "Epoch: 17981 mean train loss:  7.79073453e-03, bound:  3.15616876e-01\n",
      "Epoch: 17982 mean train loss:  7.78983673e-03, bound:  3.15616816e-01\n",
      "Epoch: 17983 mean train loss:  7.78900413e-03, bound:  3.15616786e-01\n",
      "Epoch: 17984 mean train loss:  7.78817246e-03, bound:  3.15616757e-01\n",
      "Epoch: 17985 mean train loss:  7.78732449e-03, bound:  3.15616697e-01\n",
      "Epoch: 17986 mean train loss:  7.78637920e-03, bound:  3.15616667e-01\n",
      "Epoch: 17987 mean train loss:  7.78561877e-03, bound:  3.15616637e-01\n",
      "Epoch: 17988 mean train loss:  7.78470933e-03, bound:  3.15616578e-01\n",
      "Epoch: 17989 mean train loss:  7.78386090e-03, bound:  3.15616518e-01\n",
      "Epoch: 17990 mean train loss:  7.78305158e-03, bound:  3.15616518e-01\n",
      "Epoch: 17991 mean train loss:  7.78226089e-03, bound:  3.15616459e-01\n",
      "Epoch: 17992 mean train loss:  7.78141199e-03, bound:  3.15616429e-01\n",
      "Epoch: 17993 mean train loss:  7.78081780e-03, bound:  3.15616369e-01\n",
      "Epoch: 17994 mean train loss:  7.78002013e-03, bound:  3.15616339e-01\n",
      "Epoch: 17995 mean train loss:  7.77925178e-03, bound:  3.15616280e-01\n",
      "Epoch: 17996 mean train loss:  7.77873443e-03, bound:  3.15616250e-01\n",
      "Epoch: 17997 mean train loss:  7.77821895e-03, bound:  3.15616161e-01\n",
      "Epoch: 17998 mean train loss:  7.77783943e-03, bound:  3.15616161e-01\n",
      "Epoch: 17999 mean train loss:  7.77771836e-03, bound:  3.15616101e-01\n",
      "Epoch: 18000 mean train loss:  7.77782500e-03, bound:  3.15616101e-01\n",
      "Epoch: 18001 mean train loss:  7.77806249e-03, bound:  3.15616012e-01\n",
      "Epoch: 18002 mean train loss:  7.77882989e-03, bound:  3.15616012e-01\n",
      "Epoch: 18003 mean train loss:  7.77974864e-03, bound:  3.15615922e-01\n",
      "Epoch: 18004 mean train loss:  7.78100220e-03, bound:  3.15615952e-01\n",
      "Epoch: 18005 mean train loss:  7.78232701e-03, bound:  3.15615833e-01\n",
      "Epoch: 18006 mean train loss:  7.78322946e-03, bound:  3.15615833e-01\n",
      "Epoch: 18007 mean train loss:  7.78312935e-03, bound:  3.15615714e-01\n",
      "Epoch: 18008 mean train loss:  7.78159639e-03, bound:  3.15615803e-01\n",
      "Epoch: 18009 mean train loss:  7.77842896e-03, bound:  3.15615684e-01\n",
      "Epoch: 18010 mean train loss:  7.77372159e-03, bound:  3.15615714e-01\n",
      "Epoch: 18011 mean train loss:  7.76906079e-03, bound:  3.15615594e-01\n",
      "Epoch: 18012 mean train loss:  7.76519859e-03, bound:  3.15615594e-01\n",
      "Epoch: 18013 mean train loss:  7.76313292e-03, bound:  3.15615505e-01\n",
      "Epoch: 18014 mean train loss:  7.76283955e-03, bound:  3.15615475e-01\n",
      "Epoch: 18015 mean train loss:  7.76354084e-03, bound:  3.15615475e-01\n",
      "Epoch: 18016 mean train loss:  7.76431011e-03, bound:  3.15615356e-01\n",
      "Epoch: 18017 mean train loss:  7.76436599e-03, bound:  3.15615386e-01\n",
      "Epoch: 18018 mean train loss:  7.76357856e-03, bound:  3.15615267e-01\n",
      "Epoch: 18019 mean train loss:  7.76168285e-03, bound:  3.15615326e-01\n",
      "Epoch: 18020 mean train loss:  7.75936851e-03, bound:  3.15615267e-01\n",
      "Epoch: 18021 mean train loss:  7.75720458e-03, bound:  3.15615237e-01\n",
      "Epoch: 18022 mean train loss:  7.75565859e-03, bound:  3.15615147e-01\n",
      "Epoch: 18023 mean train loss:  7.75491958e-03, bound:  3.15615147e-01\n",
      "Epoch: 18024 mean train loss:  7.75460619e-03, bound:  3.15615118e-01\n",
      "Epoch: 18025 mean train loss:  7.75438221e-03, bound:  3.15615028e-01\n",
      "Epoch: 18026 mean train loss:  7.75399199e-03, bound:  3.15615028e-01\n",
      "Epoch: 18027 mean train loss:  7.75317755e-03, bound:  3.15614939e-01\n",
      "Epoch: 18028 mean train loss:  7.75202783e-03, bound:  3.15614939e-01\n",
      "Epoch: 18029 mean train loss:  7.75075937e-03, bound:  3.15614849e-01\n",
      "Epoch: 18030 mean train loss:  7.74939219e-03, bound:  3.15614849e-01\n",
      "Epoch: 18031 mean train loss:  7.74815446e-03, bound:  3.15614790e-01\n",
      "Epoch: 18032 mean train loss:  7.74722500e-03, bound:  3.15614730e-01\n",
      "Epoch: 18033 mean train loss:  7.74657726e-03, bound:  3.15614730e-01\n",
      "Epoch: 18034 mean train loss:  7.74594210e-03, bound:  3.15614671e-01\n",
      "Epoch: 18035 mean train loss:  7.74538377e-03, bound:  3.15614671e-01\n",
      "Epoch: 18036 mean train loss:  7.74453348e-03, bound:  3.15614581e-01\n",
      "Epoch: 18037 mean train loss:  7.74363568e-03, bound:  3.15614581e-01\n",
      "Epoch: 18038 mean train loss:  7.74265733e-03, bound:  3.15614522e-01\n",
      "Epoch: 18039 mean train loss:  7.74159096e-03, bound:  3.15614462e-01\n",
      "Epoch: 18040 mean train loss:  7.74067407e-03, bound:  3.15614432e-01\n",
      "Epoch: 18041 mean train loss:  7.73970596e-03, bound:  3.15614402e-01\n",
      "Epoch: 18042 mean train loss:  7.73885055e-03, bound:  3.15614372e-01\n",
      "Epoch: 18043 mean train loss:  7.73807568e-03, bound:  3.15614343e-01\n",
      "Epoch: 18044 mean train loss:  7.73740886e-03, bound:  3.15614283e-01\n",
      "Epoch: 18045 mean train loss:  7.73666613e-03, bound:  3.15614253e-01\n",
      "Epoch: 18046 mean train loss:  7.73580885e-03, bound:  3.15614223e-01\n",
      "Epoch: 18047 mean train loss:  7.73494411e-03, bound:  3.15614134e-01\n",
      "Epoch: 18048 mean train loss:  7.73403374e-03, bound:  3.15614134e-01\n",
      "Epoch: 18049 mean train loss:  7.73315271e-03, bound:  3.15614045e-01\n",
      "Epoch: 18050 mean train loss:  7.73237506e-03, bound:  3.15614015e-01\n",
      "Epoch: 18051 mean train loss:  7.73140928e-03, bound:  3.15613985e-01\n",
      "Epoch: 18052 mean train loss:  7.73052452e-03, bound:  3.15613925e-01\n",
      "Epoch: 18053 mean train loss:  7.72961415e-03, bound:  3.15613896e-01\n",
      "Epoch: 18054 mean train loss:  7.72895385e-03, bound:  3.15613866e-01\n",
      "Epoch: 18055 mean train loss:  7.72803323e-03, bound:  3.15613836e-01\n",
      "Epoch: 18056 mean train loss:  7.72720622e-03, bound:  3.15613806e-01\n",
      "Epoch: 18057 mean train loss:  7.72632752e-03, bound:  3.15613717e-01\n",
      "Epoch: 18058 mean train loss:  7.72557687e-03, bound:  3.15613717e-01\n",
      "Epoch: 18059 mean train loss:  7.72482762e-03, bound:  3.15613687e-01\n",
      "Epoch: 18060 mean train loss:  7.72402808e-03, bound:  3.15613598e-01\n",
      "Epoch: 18061 mean train loss:  7.72317173e-03, bound:  3.15613598e-01\n",
      "Epoch: 18062 mean train loss:  7.72228232e-03, bound:  3.15613538e-01\n",
      "Epoch: 18063 mean train loss:  7.72137241e-03, bound:  3.15613508e-01\n",
      "Epoch: 18064 mean train loss:  7.72049418e-03, bound:  3.15613449e-01\n",
      "Epoch: 18065 mean train loss:  7.71961501e-03, bound:  3.15613389e-01\n",
      "Epoch: 18066 mean train loss:  7.71876285e-03, bound:  3.15613359e-01\n",
      "Epoch: 18067 mean train loss:  7.71796424e-03, bound:  3.15613329e-01\n",
      "Epoch: 18068 mean train loss:  7.71709764e-03, bound:  3.15613329e-01\n",
      "Epoch: 18069 mean train loss:  7.71634234e-03, bound:  3.15613240e-01\n",
      "Epoch: 18070 mean train loss:  7.71544734e-03, bound:  3.15613210e-01\n",
      "Epoch: 18071 mean train loss:  7.71454303e-03, bound:  3.15613151e-01\n",
      "Epoch: 18072 mean train loss:  7.71373091e-03, bound:  3.15613121e-01\n",
      "Epoch: 18073 mean train loss:  7.71292299e-03, bound:  3.15613121e-01\n",
      "Epoch: 18074 mean train loss:  7.71202147e-03, bound:  3.15613031e-01\n",
      "Epoch: 18075 mean train loss:  7.71121494e-03, bound:  3.15613002e-01\n",
      "Epoch: 18076 mean train loss:  7.71038327e-03, bound:  3.15613002e-01\n",
      "Epoch: 18077 mean train loss:  7.70952320e-03, bound:  3.15612912e-01\n",
      "Epoch: 18078 mean train loss:  7.70861423e-03, bound:  3.15612882e-01\n",
      "Epoch: 18079 mean train loss:  7.70786405e-03, bound:  3.15612823e-01\n",
      "Epoch: 18080 mean train loss:  7.70702632e-03, bound:  3.15612823e-01\n",
      "Epoch: 18081 mean train loss:  7.70619465e-03, bound:  3.15612763e-01\n",
      "Epoch: 18082 mean train loss:  7.70535739e-03, bound:  3.15612704e-01\n",
      "Epoch: 18083 mean train loss:  7.70452479e-03, bound:  3.15612674e-01\n",
      "Epoch: 18084 mean train loss:  7.70364469e-03, bound:  3.15612614e-01\n",
      "Epoch: 18085 mean train loss:  7.70283211e-03, bound:  3.15612584e-01\n",
      "Epoch: 18086 mean train loss:  7.70194083e-03, bound:  3.15612555e-01\n",
      "Epoch: 18087 mean train loss:  7.70114129e-03, bound:  3.15612495e-01\n",
      "Epoch: 18088 mean train loss:  7.70028122e-03, bound:  3.15612465e-01\n",
      "Epoch: 18089 mean train loss:  7.69948633e-03, bound:  3.15612435e-01\n",
      "Epoch: 18090 mean train loss:  7.69857178e-03, bound:  3.15612376e-01\n",
      "Epoch: 18091 mean train loss:  7.69777410e-03, bound:  3.15612346e-01\n",
      "Epoch: 18092 mean train loss:  7.69694336e-03, bound:  3.15612316e-01\n",
      "Epoch: 18093 mean train loss:  7.69609725e-03, bound:  3.15612257e-01\n",
      "Epoch: 18094 mean train loss:  7.69537548e-03, bound:  3.15612197e-01\n",
      "Epoch: 18095 mean train loss:  7.69442553e-03, bound:  3.15612167e-01\n",
      "Epoch: 18096 mean train loss:  7.69356918e-03, bound:  3.15612137e-01\n",
      "Epoch: 18097 mean train loss:  7.69271189e-03, bound:  3.15612078e-01\n",
      "Epoch: 18098 mean train loss:  7.69193424e-03, bound:  3.15612048e-01\n",
      "Epoch: 18099 mean train loss:  7.69103644e-03, bound:  3.15612018e-01\n",
      "Epoch: 18100 mean train loss:  7.69027648e-03, bound:  3.15611988e-01\n",
      "Epoch: 18101 mean train loss:  7.68946251e-03, bound:  3.15611929e-01\n",
      "Epoch: 18102 mean train loss:  7.68860197e-03, bound:  3.15611899e-01\n",
      "Epoch: 18103 mean train loss:  7.68776750e-03, bound:  3.15611839e-01\n",
      "Epoch: 18104 mean train loss:  7.68697960e-03, bound:  3.15611809e-01\n",
      "Epoch: 18105 mean train loss:  7.68613536e-03, bound:  3.15611780e-01\n",
      "Epoch: 18106 mean train loss:  7.68527063e-03, bound:  3.15611720e-01\n",
      "Epoch: 18107 mean train loss:  7.68445665e-03, bound:  3.15611690e-01\n",
      "Epoch: 18108 mean train loss:  7.68362870e-03, bound:  3.15611660e-01\n",
      "Epoch: 18109 mean train loss:  7.68277608e-03, bound:  3.15611601e-01\n",
      "Epoch: 18110 mean train loss:  7.68204778e-03, bound:  3.15611571e-01\n",
      "Epoch: 18111 mean train loss:  7.68123195e-03, bound:  3.15611482e-01\n",
      "Epoch: 18112 mean train loss:  7.68049527e-03, bound:  3.15611482e-01\n",
      "Epoch: 18113 mean train loss:  7.67979398e-03, bound:  3.15611422e-01\n",
      "Epoch: 18114 mean train loss:  7.67912390e-03, bound:  3.15611422e-01\n",
      "Epoch: 18115 mean train loss:  7.67858280e-03, bound:  3.15611333e-01\n",
      "Epoch: 18116 mean train loss:  7.67817395e-03, bound:  3.15611333e-01\n",
      "Epoch: 18117 mean train loss:  7.67790666e-03, bound:  3.15611273e-01\n",
      "Epoch: 18118 mean train loss:  7.67787872e-03, bound:  3.15611243e-01\n",
      "Epoch: 18119 mean train loss:  7.67803285e-03, bound:  3.15611184e-01\n",
      "Epoch: 18120 mean train loss:  7.67855439e-03, bound:  3.15611184e-01\n",
      "Epoch: 18121 mean train loss:  7.67924357e-03, bound:  3.15611094e-01\n",
      "Epoch: 18122 mean train loss:  7.68056698e-03, bound:  3.15611094e-01\n",
      "Epoch: 18123 mean train loss:  7.68201007e-03, bound:  3.15611005e-01\n",
      "Epoch: 18124 mean train loss:  7.68350763e-03, bound:  3.15611005e-01\n",
      "Epoch: 18125 mean train loss:  7.68426294e-03, bound:  3.15610886e-01\n",
      "Epoch: 18126 mean train loss:  7.68373860e-03, bound:  3.15610915e-01\n",
      "Epoch: 18127 mean train loss:  7.68161332e-03, bound:  3.15610796e-01\n",
      "Epoch: 18128 mean train loss:  7.67762586e-03, bound:  3.15610886e-01\n",
      "Epoch: 18129 mean train loss:  7.67264375e-03, bound:  3.15610766e-01\n",
      "Epoch: 18130 mean train loss:  7.66797597e-03, bound:  3.15610766e-01\n",
      "Epoch: 18131 mean train loss:  7.66477222e-03, bound:  3.15610677e-01\n",
      "Epoch: 18132 mean train loss:  7.66342320e-03, bound:  3.15610647e-01\n",
      "Epoch: 18133 mean train loss:  7.66379619e-03, bound:  3.15610647e-01\n",
      "Epoch: 18134 mean train loss:  7.66486162e-03, bound:  3.15610588e-01\n",
      "Epoch: 18135 mean train loss:  7.66576594e-03, bound:  3.15610588e-01\n",
      "Epoch: 18136 mean train loss:  7.66571891e-03, bound:  3.15610468e-01\n",
      "Epoch: 18137 mean train loss:  7.66451238e-03, bound:  3.15610528e-01\n",
      "Epoch: 18138 mean train loss:  7.66221806e-03, bound:  3.15610379e-01\n",
      "Epoch: 18139 mean train loss:  7.65954936e-03, bound:  3.15610379e-01\n",
      "Epoch: 18140 mean train loss:  7.65727926e-03, bound:  3.15610349e-01\n",
      "Epoch: 18141 mean train loss:  7.65602989e-03, bound:  3.15610319e-01\n",
      "Epoch: 18142 mean train loss:  7.65574723e-03, bound:  3.15610260e-01\n",
      "Epoch: 18143 mean train loss:  7.65579287e-03, bound:  3.15610230e-01\n",
      "Epoch: 18144 mean train loss:  7.65587250e-03, bound:  3.15610200e-01\n",
      "Epoch: 18145 mean train loss:  7.65531650e-03, bound:  3.15610141e-01\n",
      "Epoch: 18146 mean train loss:  7.65422219e-03, bound:  3.15610141e-01\n",
      "Epoch: 18147 mean train loss:  7.65258493e-03, bound:  3.15610081e-01\n",
      "Epoch: 18148 mean train loss:  7.65091041e-03, bound:  3.15610081e-01\n",
      "Epoch: 18149 mean train loss:  7.64957489e-03, bound:  3.15609962e-01\n",
      "Epoch: 18150 mean train loss:  7.64877396e-03, bound:  3.15609932e-01\n",
      "Epoch: 18151 mean train loss:  7.64827570e-03, bound:  3.15609932e-01\n",
      "Epoch: 18152 mean train loss:  7.64781795e-03, bound:  3.15609872e-01\n",
      "Epoch: 18153 mean train loss:  7.64736906e-03, bound:  3.15609872e-01\n",
      "Epoch: 18154 mean train loss:  7.64668081e-03, bound:  3.15609783e-01\n",
      "Epoch: 18155 mean train loss:  7.64562795e-03, bound:  3.15609783e-01\n",
      "Epoch: 18156 mean train loss:  7.64444238e-03, bound:  3.15609694e-01\n",
      "Epoch: 18157 mean train loss:  7.64329964e-03, bound:  3.15609664e-01\n",
      "Epoch: 18158 mean train loss:  7.64215877e-03, bound:  3.15609664e-01\n",
      "Epoch: 18159 mean train loss:  7.64142117e-03, bound:  3.15609574e-01\n",
      "Epoch: 18160 mean train loss:  7.64070218e-03, bound:  3.15609574e-01\n",
      "Epoch: 18161 mean train loss:  7.64005585e-03, bound:  3.15609515e-01\n",
      "Epoch: 18162 mean train loss:  7.63934013e-03, bound:  3.15609455e-01\n",
      "Epoch: 18163 mean train loss:  7.63856666e-03, bound:  3.15609425e-01\n",
      "Epoch: 18164 mean train loss:  7.63782114e-03, bound:  3.15609396e-01\n",
      "Epoch: 18165 mean train loss:  7.63678318e-03, bound:  3.15609336e-01\n",
      "Epoch: 18166 mean train loss:  7.63592031e-03, bound:  3.15609336e-01\n",
      "Epoch: 18167 mean train loss:  7.63494195e-03, bound:  3.15609276e-01\n",
      "Epoch: 18168 mean train loss:  7.63406977e-03, bound:  3.15609246e-01\n",
      "Epoch: 18169 mean train loss:  7.63326185e-03, bound:  3.15609217e-01\n",
      "Epoch: 18170 mean train loss:  7.63248838e-03, bound:  3.15609157e-01\n",
      "Epoch: 18171 mean train loss:  7.63170747e-03, bound:  3.15609127e-01\n",
      "Epoch: 18172 mean train loss:  7.63101084e-03, bound:  3.15609068e-01\n",
      "Epoch: 18173 mean train loss:  7.63023924e-03, bound:  3.15609068e-01\n",
      "Epoch: 18174 mean train loss:  7.62945181e-03, bound:  3.15609008e-01\n",
      "Epoch: 18175 mean train loss:  7.62850279e-03, bound:  3.15608978e-01\n",
      "Epoch: 18176 mean train loss:  7.62765994e-03, bound:  3.15608889e-01\n",
      "Epoch: 18177 mean train loss:  7.62682501e-03, bound:  3.15608889e-01\n",
      "Epoch: 18178 mean train loss:  7.62596494e-03, bound:  3.15608799e-01\n",
      "Epoch: 18179 mean train loss:  7.62505271e-03, bound:  3.15608770e-01\n",
      "Epoch: 18180 mean train loss:  7.62421917e-03, bound:  3.15608770e-01\n",
      "Epoch: 18181 mean train loss:  7.62351928e-03, bound:  3.15608680e-01\n",
      "Epoch: 18182 mean train loss:  7.62279145e-03, bound:  3.15608680e-01\n",
      "Epoch: 18183 mean train loss:  7.62203801e-03, bound:  3.15608650e-01\n",
      "Epoch: 18184 mean train loss:  7.62118027e-03, bound:  3.15608591e-01\n",
      "Epoch: 18185 mean train loss:  7.62031088e-03, bound:  3.15608531e-01\n",
      "Epoch: 18186 mean train loss:  7.61947595e-03, bound:  3.15608531e-01\n",
      "Epoch: 18187 mean train loss:  7.61864148e-03, bound:  3.15608472e-01\n",
      "Epoch: 18188 mean train loss:  7.61771342e-03, bound:  3.15608442e-01\n",
      "Epoch: 18189 mean train loss:  7.61694135e-03, bound:  3.15608412e-01\n",
      "Epoch: 18190 mean train loss:  7.61607662e-03, bound:  3.15608352e-01\n",
      "Epoch: 18191 mean train loss:  7.61521189e-03, bound:  3.15608323e-01\n",
      "Epoch: 18192 mean train loss:  7.61447381e-03, bound:  3.15608293e-01\n",
      "Epoch: 18193 mean train loss:  7.61378836e-03, bound:  3.15608233e-01\n",
      "Epoch: 18194 mean train loss:  7.61294924e-03, bound:  3.15608203e-01\n",
      "Epoch: 18195 mean train loss:  7.61210965e-03, bound:  3.15608174e-01\n",
      "Epoch: 18196 mean train loss:  7.61127472e-03, bound:  3.15608114e-01\n",
      "Epoch: 18197 mean train loss:  7.61049800e-03, bound:  3.15608084e-01\n",
      "Epoch: 18198 mean train loss:  7.60967657e-03, bound:  3.15608025e-01\n",
      "Epoch: 18199 mean train loss:  7.60888867e-03, bound:  3.15607995e-01\n",
      "Epoch: 18200 mean train loss:  7.60793453e-03, bound:  3.15607965e-01\n",
      "Epoch: 18201 mean train loss:  7.60710938e-03, bound:  3.15607905e-01\n",
      "Epoch: 18202 mean train loss:  7.60631450e-03, bound:  3.15607846e-01\n",
      "Epoch: 18203 mean train loss:  7.60551915e-03, bound:  3.15607846e-01\n",
      "Epoch: 18204 mean train loss:  7.60466279e-03, bound:  3.15607786e-01\n",
      "Epoch: 18205 mean train loss:  7.60376966e-03, bound:  3.15607756e-01\n",
      "Epoch: 18206 mean train loss:  7.60306651e-03, bound:  3.15607697e-01\n",
      "Epoch: 18207 mean train loss:  7.60215567e-03, bound:  3.15607667e-01\n",
      "Epoch: 18208 mean train loss:  7.60141481e-03, bound:  3.15607637e-01\n",
      "Epoch: 18209 mean train loss:  7.60055380e-03, bound:  3.15607578e-01\n",
      "Epoch: 18210 mean train loss:  7.59967091e-03, bound:  3.15607548e-01\n",
      "Epoch: 18211 mean train loss:  7.59894354e-03, bound:  3.15607518e-01\n",
      "Epoch: 18212 mean train loss:  7.59815797e-03, bound:  3.15607458e-01\n",
      "Epoch: 18213 mean train loss:  7.59737147e-03, bound:  3.15607429e-01\n",
      "Epoch: 18214 mean train loss:  7.59653747e-03, bound:  3.15607399e-01\n",
      "Epoch: 18215 mean train loss:  7.59571698e-03, bound:  3.15607339e-01\n",
      "Epoch: 18216 mean train loss:  7.59498123e-03, bound:  3.15607339e-01\n",
      "Epoch: 18217 mean train loss:  7.59420404e-03, bound:  3.15607280e-01\n",
      "Epoch: 18218 mean train loss:  7.59338588e-03, bound:  3.15607220e-01\n",
      "Epoch: 18219 mean train loss:  7.59264175e-03, bound:  3.15607220e-01\n",
      "Epoch: 18220 mean train loss:  7.59176211e-03, bound:  3.15607190e-01\n",
      "Epoch: 18221 mean train loss:  7.59101380e-03, bound:  3.15607101e-01\n",
      "Epoch: 18222 mean train loss:  7.59023381e-03, bound:  3.15607101e-01\n",
      "Epoch: 18223 mean train loss:  7.58945150e-03, bound:  3.15607011e-01\n",
      "Epoch: 18224 mean train loss:  7.58860772e-03, bound:  3.15607011e-01\n",
      "Epoch: 18225 mean train loss:  7.58781331e-03, bound:  3.15606952e-01\n",
      "Epoch: 18226 mean train loss:  7.58701470e-03, bound:  3.15606922e-01\n",
      "Epoch: 18227 mean train loss:  7.58612575e-03, bound:  3.15606862e-01\n",
      "Epoch: 18228 mean train loss:  7.58522330e-03, bound:  3.15606862e-01\n",
      "Epoch: 18229 mean train loss:  7.58451410e-03, bound:  3.15606773e-01\n",
      "Epoch: 18230 mean train loss:  7.58383749e-03, bound:  3.15606773e-01\n",
      "Epoch: 18231 mean train loss:  7.58297695e-03, bound:  3.15606713e-01\n",
      "Epoch: 18232 mean train loss:  7.58223608e-03, bound:  3.15606683e-01\n",
      "Epoch: 18233 mean train loss:  7.58158322e-03, bound:  3.15606624e-01\n",
      "Epoch: 18234 mean train loss:  7.58086052e-03, bound:  3.15606624e-01\n",
      "Epoch: 18235 mean train loss:  7.58020114e-03, bound:  3.15606534e-01\n",
      "Epoch: 18236 mean train loss:  7.57954968e-03, bound:  3.15606534e-01\n",
      "Epoch: 18237 mean train loss:  7.57901231e-03, bound:  3.15606445e-01\n",
      "Epoch: 18238 mean train loss:  7.57844560e-03, bound:  3.15606415e-01\n",
      "Epoch: 18239 mean train loss:  7.57793430e-03, bound:  3.15606385e-01\n",
      "Epoch: 18240 mean train loss:  7.57742161e-03, bound:  3.15606356e-01\n",
      "Epoch: 18241 mean train loss:  7.57722789e-03, bound:  3.15606296e-01\n",
      "Epoch: 18242 mean train loss:  7.57702487e-03, bound:  3.15606296e-01\n",
      "Epoch: 18243 mean train loss:  7.57685723e-03, bound:  3.15606207e-01\n",
      "Epoch: 18244 mean train loss:  7.57681997e-03, bound:  3.15606207e-01\n",
      "Epoch: 18245 mean train loss:  7.57676316e-03, bound:  3.15606117e-01\n",
      "Epoch: 18246 mean train loss:  7.57651916e-03, bound:  3.15606117e-01\n",
      "Epoch: 18247 mean train loss:  7.57619832e-03, bound:  3.15606058e-01\n",
      "Epoch: 18248 mean train loss:  7.57551938e-03, bound:  3.15606087e-01\n",
      "Epoch: 18249 mean train loss:  7.57446792e-03, bound:  3.15605968e-01\n",
      "Epoch: 18250 mean train loss:  7.57303555e-03, bound:  3.15605968e-01\n",
      "Epoch: 18251 mean train loss:  7.57118594e-03, bound:  3.15605879e-01\n",
      "Epoch: 18252 mean train loss:  7.56903971e-03, bound:  3.15605909e-01\n",
      "Epoch: 18253 mean train loss:  7.56705878e-03, bound:  3.15605789e-01\n",
      "Epoch: 18254 mean train loss:  7.56517472e-03, bound:  3.15605789e-01\n",
      "Epoch: 18255 mean train loss:  7.56349554e-03, bound:  3.15605730e-01\n",
      "Epoch: 18256 mean train loss:  7.56222615e-03, bound:  3.15605730e-01\n",
      "Epoch: 18257 mean train loss:  7.56138796e-03, bound:  3.15605670e-01\n",
      "Epoch: 18258 mean train loss:  7.56068667e-03, bound:  3.15605640e-01\n",
      "Epoch: 18259 mean train loss:  7.56015722e-03, bound:  3.15605611e-01\n",
      "Epoch: 18260 mean train loss:  7.55975582e-03, bound:  3.15605551e-01\n",
      "Epoch: 18261 mean train loss:  7.55918445e-03, bound:  3.15605521e-01\n",
      "Epoch: 18262 mean train loss:  7.55856512e-03, bound:  3.15605462e-01\n",
      "Epoch: 18263 mean train loss:  7.55774742e-03, bound:  3.15605462e-01\n",
      "Epoch: 18264 mean train loss:  7.55691668e-03, bound:  3.15605402e-01\n",
      "Epoch: 18265 mean train loss:  7.55588431e-03, bound:  3.15605402e-01\n",
      "Epoch: 18266 mean train loss:  7.55486172e-03, bound:  3.15605313e-01\n",
      "Epoch: 18267 mean train loss:  7.55375111e-03, bound:  3.15605283e-01\n",
      "Epoch: 18268 mean train loss:  7.55275181e-03, bound:  3.15605223e-01\n",
      "Epoch: 18269 mean train loss:  7.55184703e-03, bound:  3.15605193e-01\n",
      "Epoch: 18270 mean train loss:  7.55081652e-03, bound:  3.15605164e-01\n",
      "Epoch: 18271 mean train loss:  7.55006354e-03, bound:  3.15605104e-01\n",
      "Epoch: 18272 mean train loss:  7.54930452e-03, bound:  3.15605104e-01\n",
      "Epoch: 18273 mean train loss:  7.54856318e-03, bound:  3.15605074e-01\n",
      "Epoch: 18274 mean train loss:  7.54800485e-03, bound:  3.15604985e-01\n",
      "Epoch: 18275 mean train loss:  7.54723884e-03, bound:  3.15604985e-01\n",
      "Epoch: 18276 mean train loss:  7.54651567e-03, bound:  3.15604955e-01\n",
      "Epoch: 18277 mean train loss:  7.54571613e-03, bound:  3.15604866e-01\n",
      "Epoch: 18278 mean train loss:  7.54489517e-03, bound:  3.15604866e-01\n",
      "Epoch: 18279 mean train loss:  7.54409702e-03, bound:  3.15604776e-01\n",
      "Epoch: 18280 mean train loss:  7.54330959e-03, bound:  3.15604776e-01\n",
      "Epoch: 18281 mean train loss:  7.54248584e-03, bound:  3.15604687e-01\n",
      "Epoch: 18282 mean train loss:  7.54155638e-03, bound:  3.15604687e-01\n",
      "Epoch: 18283 mean train loss:  7.54071819e-03, bound:  3.15604627e-01\n",
      "Epoch: 18284 mean train loss:  7.53976405e-03, bound:  3.15604597e-01\n",
      "Epoch: 18285 mean train loss:  7.53901899e-03, bound:  3.15604568e-01\n",
      "Epoch: 18286 mean train loss:  7.53812585e-03, bound:  3.15604538e-01\n",
      "Epoch: 18287 mean train loss:  7.53724203e-03, bound:  3.15604478e-01\n",
      "Epoch: 18288 mean train loss:  7.53640337e-03, bound:  3.15604478e-01\n",
      "Epoch: 18289 mean train loss:  7.53561407e-03, bound:  3.15604389e-01\n",
      "Epoch: 18290 mean train loss:  7.53484201e-03, bound:  3.15604389e-01\n",
      "Epoch: 18291 mean train loss:  7.53394095e-03, bound:  3.15604299e-01\n",
      "Epoch: 18292 mean train loss:  7.53300684e-03, bound:  3.15604299e-01\n",
      "Epoch: 18293 mean train loss:  7.53234234e-03, bound:  3.15604270e-01\n",
      "Epoch: 18294 mean train loss:  7.53152603e-03, bound:  3.15604210e-01\n",
      "Epoch: 18295 mean train loss:  7.53066735e-03, bound:  3.15604180e-01\n",
      "Epoch: 18296 mean train loss:  7.52987154e-03, bound:  3.15604150e-01\n",
      "Epoch: 18297 mean train loss:  7.52909109e-03, bound:  3.15604091e-01\n",
      "Epoch: 18298 mean train loss:  7.52831250e-03, bound:  3.15604091e-01\n",
      "Epoch: 18299 mean train loss:  7.52746174e-03, bound:  3.15604001e-01\n",
      "Epoch: 18300 mean train loss:  7.52668688e-03, bound:  3.15603971e-01\n",
      "Epoch: 18301 mean train loss:  7.52585893e-03, bound:  3.15603942e-01\n",
      "Epoch: 18302 mean train loss:  7.52506964e-03, bound:  3.15603882e-01\n",
      "Epoch: 18303 mean train loss:  7.52421888e-03, bound:  3.15603852e-01\n",
      "Epoch: 18304 mean train loss:  7.52343424e-03, bound:  3.15603822e-01\n",
      "Epoch: 18305 mean train loss:  7.52264261e-03, bound:  3.15603763e-01\n",
      "Epoch: 18306 mean train loss:  7.52184214e-03, bound:  3.15603733e-01\n",
      "Epoch: 18307 mean train loss:  7.52107473e-03, bound:  3.15603703e-01\n",
      "Epoch: 18308 mean train loss:  7.52025843e-03, bound:  3.15603673e-01\n",
      "Epoch: 18309 mean train loss:  7.51950499e-03, bound:  3.15603614e-01\n",
      "Epoch: 18310 mean train loss:  7.51870032e-03, bound:  3.15603554e-01\n",
      "Epoch: 18311 mean train loss:  7.51797622e-03, bound:  3.15603554e-01\n",
      "Epoch: 18312 mean train loss:  7.51732802e-03, bound:  3.15603495e-01\n",
      "Epoch: 18313 mean train loss:  7.51671847e-03, bound:  3.15603435e-01\n",
      "Epoch: 18314 mean train loss:  7.51603721e-03, bound:  3.15603405e-01\n",
      "Epoch: 18315 mean train loss:  7.51549518e-03, bound:  3.15603405e-01\n",
      "Epoch: 18316 mean train loss:  7.51524139e-03, bound:  3.15603316e-01\n",
      "Epoch: 18317 mean train loss:  7.51495128e-03, bound:  3.15603316e-01\n",
      "Epoch: 18318 mean train loss:  7.51481345e-03, bound:  3.15603226e-01\n",
      "Epoch: 18319 mean train loss:  7.51502160e-03, bound:  3.15603226e-01\n",
      "Epoch: 18320 mean train loss:  7.51563953e-03, bound:  3.15603167e-01\n",
      "Epoch: 18321 mean train loss:  7.51666818e-03, bound:  3.15603167e-01\n",
      "Epoch: 18322 mean train loss:  7.51808472e-03, bound:  3.15603048e-01\n",
      "Epoch: 18323 mean train loss:  7.51973735e-03, bound:  3.15603107e-01\n",
      "Epoch: 18324 mean train loss:  7.52123585e-03, bound:  3.15602958e-01\n",
      "Epoch: 18325 mean train loss:  7.52200233e-03, bound:  3.15603048e-01\n",
      "Epoch: 18326 mean train loss:  7.52137462e-03, bound:  3.15602869e-01\n",
      "Epoch: 18327 mean train loss:  7.51905376e-03, bound:  3.15602958e-01\n",
      "Epoch: 18328 mean train loss:  7.51491077e-03, bound:  3.15602839e-01\n",
      "Epoch: 18329 mean train loss:  7.50982109e-03, bound:  3.15602869e-01\n",
      "Epoch: 18330 mean train loss:  7.50518870e-03, bound:  3.15602750e-01\n",
      "Epoch: 18331 mean train loss:  7.50216097e-03, bound:  3.15602750e-01\n",
      "Epoch: 18332 mean train loss:  7.50105968e-03, bound:  3.15602720e-01\n",
      "Epoch: 18333 mean train loss:  7.50151137e-03, bound:  3.15602660e-01\n",
      "Epoch: 18334 mean train loss:  7.50239799e-03, bound:  3.15602660e-01\n",
      "Epoch: 18335 mean train loss:  7.50301173e-03, bound:  3.15602601e-01\n",
      "Epoch: 18336 mean train loss:  7.50257494e-03, bound:  3.15602601e-01\n",
      "Epoch: 18337 mean train loss:  7.50110764e-03, bound:  3.15602511e-01\n",
      "Epoch: 18338 mean train loss:  7.49885803e-03, bound:  3.15602511e-01\n",
      "Epoch: 18339 mean train loss:  7.49657815e-03, bound:  3.15602422e-01\n",
      "Epoch: 18340 mean train loss:  7.49486638e-03, bound:  3.15602422e-01\n",
      "Epoch: 18341 mean train loss:  7.49387499e-03, bound:  3.15602392e-01\n",
      "Epoch: 18342 mean train loss:  7.49359326e-03, bound:  3.15602332e-01\n",
      "Epoch: 18343 mean train loss:  7.49359000e-03, bound:  3.15602303e-01\n",
      "Epoch: 18344 mean train loss:  7.49344099e-03, bound:  3.15602273e-01\n",
      "Epoch: 18345 mean train loss:  7.49286683e-03, bound:  3.15602273e-01\n",
      "Epoch: 18346 mean train loss:  7.49173528e-03, bound:  3.15602183e-01\n",
      "Epoch: 18347 mean train loss:  7.49029173e-03, bound:  3.15602154e-01\n",
      "Epoch: 18348 mean train loss:  7.48877553e-03, bound:  3.15602094e-01\n",
      "Epoch: 18349 mean train loss:  7.48766400e-03, bound:  3.15602064e-01\n",
      "Epoch: 18350 mean train loss:  7.48685608e-03, bound:  3.15602034e-01\n",
      "Epoch: 18351 mean train loss:  7.48631125e-03, bound:  3.15601975e-01\n",
      "Epoch: 18352 mean train loss:  7.48574687e-03, bound:  3.15601975e-01\n",
      "Epoch: 18353 mean train loss:  7.48529658e-03, bound:  3.15601915e-01\n",
      "Epoch: 18354 mean train loss:  7.48464745e-03, bound:  3.15601885e-01\n",
      "Epoch: 18355 mean train loss:  7.48372125e-03, bound:  3.15601856e-01\n",
      "Epoch: 18356 mean train loss:  7.48270052e-03, bound:  3.15601826e-01\n",
      "Epoch: 18357 mean train loss:  7.48161413e-03, bound:  3.15601736e-01\n",
      "Epoch: 18358 mean train loss:  7.48057198e-03, bound:  3.15601736e-01\n",
      "Epoch: 18359 mean train loss:  7.47975754e-03, bound:  3.15601707e-01\n",
      "Epoch: 18360 mean train loss:  7.47914053e-03, bound:  3.15601617e-01\n",
      "Epoch: 18361 mean train loss:  7.47835869e-03, bound:  3.15601617e-01\n",
      "Epoch: 18362 mean train loss:  7.47772865e-03, bound:  3.15601557e-01\n",
      "Epoch: 18363 mean train loss:  7.47692632e-03, bound:  3.15601528e-01\n",
      "Epoch: 18364 mean train loss:  7.47619569e-03, bound:  3.15601498e-01\n",
      "Epoch: 18365 mean train loss:  7.47531559e-03, bound:  3.15601468e-01\n",
      "Epoch: 18366 mean train loss:  7.47437915e-03, bound:  3.15601408e-01\n",
      "Epoch: 18367 mean train loss:  7.47354096e-03, bound:  3.15601408e-01\n",
      "Epoch: 18368 mean train loss:  7.47279683e-03, bound:  3.15601319e-01\n",
      "Epoch: 18369 mean train loss:  7.47187063e-03, bound:  3.15601319e-01\n",
      "Epoch: 18370 mean train loss:  7.47114653e-03, bound:  3.15601289e-01\n",
      "Epoch: 18371 mean train loss:  7.47036748e-03, bound:  3.15601200e-01\n",
      "Epoch: 18372 mean train loss:  7.46965315e-03, bound:  3.15601200e-01\n",
      "Epoch: 18373 mean train loss:  7.46892625e-03, bound:  3.15601170e-01\n",
      "Epoch: 18374 mean train loss:  7.46815745e-03, bound:  3.15601140e-01\n",
      "Epoch: 18375 mean train loss:  7.46733416e-03, bound:  3.15601051e-01\n",
      "Epoch: 18376 mean train loss:  7.46659376e-03, bound:  3.15601051e-01\n",
      "Epoch: 18377 mean train loss:  7.46582542e-03, bound:  3.15601021e-01\n",
      "Epoch: 18378 mean train loss:  7.46493507e-03, bound:  3.15600961e-01\n",
      "Epoch: 18379 mean train loss:  7.46415276e-03, bound:  3.15600932e-01\n",
      "Epoch: 18380 mean train loss:  7.46327499e-03, bound:  3.15600872e-01\n",
      "Epoch: 18381 mean train loss:  7.46249966e-03, bound:  3.15600842e-01\n",
      "Epoch: 18382 mean train loss:  7.46167079e-03, bound:  3.15600842e-01\n",
      "Epoch: 18383 mean train loss:  7.46090664e-03, bound:  3.15600753e-01\n",
      "Epoch: 18384 mean train loss:  7.46010384e-03, bound:  3.15600723e-01\n",
      "Epoch: 18385 mean train loss:  7.45941559e-03, bound:  3.15600693e-01\n",
      "Epoch: 18386 mean train loss:  7.45862396e-03, bound:  3.15600634e-01\n",
      "Epoch: 18387 mean train loss:  7.45782210e-03, bound:  3.15600604e-01\n",
      "Epoch: 18388 mean train loss:  7.45717110e-03, bound:  3.15600604e-01\n",
      "Epoch: 18389 mean train loss:  7.45627237e-03, bound:  3.15600544e-01\n",
      "Epoch: 18390 mean train loss:  7.45547609e-03, bound:  3.15600485e-01\n",
      "Epoch: 18391 mean train loss:  7.45467423e-03, bound:  3.15600485e-01\n",
      "Epoch: 18392 mean train loss:  7.45391147e-03, bound:  3.15600425e-01\n",
      "Epoch: 18393 mean train loss:  7.45311752e-03, bound:  3.15600395e-01\n",
      "Epoch: 18394 mean train loss:  7.45232357e-03, bound:  3.15600306e-01\n",
      "Epoch: 18395 mean train loss:  7.45158317e-03, bound:  3.15600306e-01\n",
      "Epoch: 18396 mean train loss:  7.45066348e-03, bound:  3.15600276e-01\n",
      "Epoch: 18397 mean train loss:  7.44991191e-03, bound:  3.15600216e-01\n",
      "Epoch: 18398 mean train loss:  7.44915009e-03, bound:  3.15600187e-01\n",
      "Epoch: 18399 mean train loss:  7.44831981e-03, bound:  3.15600157e-01\n",
      "Epoch: 18400 mean train loss:  7.44754123e-03, bound:  3.15600097e-01\n",
      "Epoch: 18401 mean train loss:  7.44680502e-03, bound:  3.15600067e-01\n",
      "Epoch: 18402 mean train loss:  7.44603947e-03, bound:  3.15600038e-01\n",
      "Epoch: 18403 mean train loss:  7.44516402e-03, bound:  3.15600038e-01\n",
      "Epoch: 18404 mean train loss:  7.44444877e-03, bound:  3.15599948e-01\n",
      "Epoch: 18405 mean train loss:  7.44372793e-03, bound:  3.15599918e-01\n",
      "Epoch: 18406 mean train loss:  7.44286599e-03, bound:  3.15599859e-01\n",
      "Epoch: 18407 mean train loss:  7.44202035e-03, bound:  3.15599829e-01\n",
      "Epoch: 18408 mean train loss:  7.44130695e-03, bound:  3.15599799e-01\n",
      "Epoch: 18409 mean train loss:  7.44061312e-03, bound:  3.15599740e-01\n",
      "Epoch: 18410 mean train loss:  7.43968505e-03, bound:  3.15599710e-01\n",
      "Epoch: 18411 mean train loss:  7.43902521e-03, bound:  3.15599710e-01\n",
      "Epoch: 18412 mean train loss:  7.43820658e-03, bound:  3.15599620e-01\n",
      "Epoch: 18413 mean train loss:  7.43743218e-03, bound:  3.15599620e-01\n",
      "Epoch: 18414 mean train loss:  7.43668107e-03, bound:  3.15599531e-01\n",
      "Epoch: 18415 mean train loss:  7.43599422e-03, bound:  3.15599531e-01\n",
      "Epoch: 18416 mean train loss:  7.43514718e-03, bound:  3.15599501e-01\n",
      "Epoch: 18417 mean train loss:  7.43451621e-03, bound:  3.15599471e-01\n",
      "Epoch: 18418 mean train loss:  7.43372040e-03, bound:  3.15599382e-01\n",
      "Epoch: 18419 mean train loss:  7.43298000e-03, bound:  3.15599382e-01\n",
      "Epoch: 18420 mean train loss:  7.43223308e-03, bound:  3.15599293e-01\n",
      "Epoch: 18421 mean train loss:  7.43144378e-03, bound:  3.15599293e-01\n",
      "Epoch: 18422 mean train loss:  7.43073970e-03, bound:  3.15599233e-01\n",
      "Epoch: 18423 mean train loss:  7.42998486e-03, bound:  3.15599233e-01\n",
      "Epoch: 18424 mean train loss:  7.42929941e-03, bound:  3.15599173e-01\n",
      "Epoch: 18425 mean train loss:  7.42858369e-03, bound:  3.15599173e-01\n",
      "Epoch: 18426 mean train loss:  7.42788799e-03, bound:  3.15599084e-01\n",
      "Epoch: 18427 mean train loss:  7.42717925e-03, bound:  3.15599084e-01\n",
      "Epoch: 18428 mean train loss:  7.42648961e-03, bound:  3.15598994e-01\n",
      "Epoch: 18429 mean train loss:  7.42582465e-03, bound:  3.15598994e-01\n",
      "Epoch: 18430 mean train loss:  7.42507819e-03, bound:  3.15598935e-01\n",
      "Epoch: 18431 mean train loss:  7.42446631e-03, bound:  3.15598935e-01\n",
      "Epoch: 18432 mean train loss:  7.42394058e-03, bound:  3.15598845e-01\n",
      "Epoch: 18433 mean train loss:  7.42334733e-03, bound:  3.15598845e-01\n",
      "Epoch: 18434 mean train loss:  7.42289983e-03, bound:  3.15598786e-01\n",
      "Epoch: 18435 mean train loss:  7.42244860e-03, bound:  3.15598786e-01\n",
      "Epoch: 18436 mean train loss:  7.42215430e-03, bound:  3.15598696e-01\n",
      "Epoch: 18437 mean train loss:  7.42193358e-03, bound:  3.15598696e-01\n",
      "Epoch: 18438 mean train loss:  7.42183905e-03, bound:  3.15598607e-01\n",
      "Epoch: 18439 mean train loss:  7.42172543e-03, bound:  3.15598607e-01\n",
      "Epoch: 18440 mean train loss:  7.42167188e-03, bound:  3.15598518e-01\n",
      "Epoch: 18441 mean train loss:  7.42148561e-03, bound:  3.15598518e-01\n",
      "Epoch: 18442 mean train loss:  7.42109772e-03, bound:  3.15598458e-01\n",
      "Epoch: 18443 mean train loss:  7.42057897e-03, bound:  3.15598488e-01\n",
      "Epoch: 18444 mean train loss:  7.41958944e-03, bound:  3.15598369e-01\n",
      "Epoch: 18445 mean train loss:  7.41832983e-03, bound:  3.15598398e-01\n",
      "Epoch: 18446 mean train loss:  7.41664506e-03, bound:  3.15598309e-01\n",
      "Epoch: 18447 mean train loss:  7.41475075e-03, bound:  3.15598309e-01\n",
      "Epoch: 18448 mean train loss:  7.41270138e-03, bound:  3.15598249e-01\n",
      "Epoch: 18449 mean train loss:  7.41070556e-03, bound:  3.15598249e-01\n",
      "Epoch: 18450 mean train loss:  7.40901940e-03, bound:  3.15598190e-01\n",
      "Epoch: 18451 mean train loss:  7.40773696e-03, bound:  3.15598160e-01\n",
      "Epoch: 18452 mean train loss:  7.40672974e-03, bound:  3.15598130e-01\n",
      "Epoch: 18453 mean train loss:  7.40608945e-03, bound:  3.15598071e-01\n",
      "Epoch: 18454 mean train loss:  7.40554463e-03, bound:  3.15598041e-01\n",
      "Epoch: 18455 mean train loss:  7.40520889e-03, bound:  3.15597951e-01\n",
      "Epoch: 18456 mean train loss:  7.40475440e-03, bound:  3.15597951e-01\n",
      "Epoch: 18457 mean train loss:  7.40435021e-03, bound:  3.15597862e-01\n",
      "Epoch: 18458 mean train loss:  7.40373181e-03, bound:  3.15597862e-01\n",
      "Epoch: 18459 mean train loss:  7.40290107e-03, bound:  3.15597832e-01\n",
      "Epoch: 18460 mean train loss:  7.40199629e-03, bound:  3.15597802e-01\n",
      "Epoch: 18461 mean train loss:  7.40101421e-03, bound:  3.15597743e-01\n",
      "Epoch: 18462 mean train loss:  7.39999814e-03, bound:  3.15597743e-01\n",
      "Epoch: 18463 mean train loss:  7.39889033e-03, bound:  3.15597683e-01\n",
      "Epoch: 18464 mean train loss:  7.39781512e-03, bound:  3.15597683e-01\n",
      "Epoch: 18465 mean train loss:  7.39684841e-03, bound:  3.15597624e-01\n",
      "Epoch: 18466 mean train loss:  7.39591941e-03, bound:  3.15597564e-01\n",
      "Epoch: 18467 mean train loss:  7.39509612e-03, bound:  3.15597534e-01\n",
      "Epoch: 18468 mean train loss:  7.39428122e-03, bound:  3.15597504e-01\n",
      "Epoch: 18469 mean train loss:  7.39362091e-03, bound:  3.15597475e-01\n",
      "Epoch: 18470 mean train loss:  7.39300484e-03, bound:  3.15597415e-01\n",
      "Epoch: 18471 mean train loss:  7.39232078e-03, bound:  3.15597385e-01\n",
      "Epoch: 18472 mean train loss:  7.39166047e-03, bound:  3.15597355e-01\n",
      "Epoch: 18473 mean train loss:  7.39087304e-03, bound:  3.15597296e-01\n",
      "Epoch: 18474 mean train loss:  7.39019364e-03, bound:  3.15597266e-01\n",
      "Epoch: 18475 mean train loss:  7.38938991e-03, bound:  3.15597236e-01\n",
      "Epoch: 18476 mean train loss:  7.38855125e-03, bound:  3.15597177e-01\n",
      "Epoch: 18477 mean train loss:  7.38771399e-03, bound:  3.15597177e-01\n",
      "Epoch: 18478 mean train loss:  7.38701690e-03, bound:  3.15597117e-01\n",
      "Epoch: 18479 mean train loss:  7.38616753e-03, bound:  3.15597057e-01\n",
      "Epoch: 18480 mean train loss:  7.38525065e-03, bound:  3.15597057e-01\n",
      "Epoch: 18481 mean train loss:  7.38443946e-03, bound:  3.15596998e-01\n",
      "Epoch: 18482 mean train loss:  7.38361711e-03, bound:  3.15596938e-01\n",
      "Epoch: 18483 mean train loss:  7.38280639e-03, bound:  3.15596938e-01\n",
      "Epoch: 18484 mean train loss:  7.38202222e-03, bound:  3.15596908e-01\n",
      "Epoch: 18485 mean train loss:  7.38118822e-03, bound:  3.15596849e-01\n",
      "Epoch: 18486 mean train loss:  7.38040544e-03, bound:  3.15596819e-01\n",
      "Epoch: 18487 mean train loss:  7.37968739e-03, bound:  3.15596759e-01\n",
      "Epoch: 18488 mean train loss:  7.37882312e-03, bound:  3.15596730e-01\n",
      "Epoch: 18489 mean train loss:  7.37808505e-03, bound:  3.15596700e-01\n",
      "Epoch: 18490 mean train loss:  7.37725338e-03, bound:  3.15596670e-01\n",
      "Epoch: 18491 mean train loss:  7.37659447e-03, bound:  3.15596610e-01\n",
      "Epoch: 18492 mean train loss:  7.37573393e-03, bound:  3.15596581e-01\n",
      "Epoch: 18493 mean train loss:  7.37501122e-03, bound:  3.15596551e-01\n",
      "Epoch: 18494 mean train loss:  7.37413857e-03, bound:  3.15596491e-01\n",
      "Epoch: 18495 mean train loss:  7.37339258e-03, bound:  3.15596461e-01\n",
      "Epoch: 18496 mean train loss:  7.37269130e-03, bound:  3.15596402e-01\n",
      "Epoch: 18497 mean train loss:  7.37186242e-03, bound:  3.15596372e-01\n",
      "Epoch: 18498 mean train loss:  7.37107079e-03, bound:  3.15596372e-01\n",
      "Epoch: 18499 mean train loss:  7.37038255e-03, bound:  3.15596312e-01\n",
      "Epoch: 18500 mean train loss:  7.36956624e-03, bound:  3.15596253e-01\n",
      "Epoch: 18501 mean train loss:  7.36877182e-03, bound:  3.15596253e-01\n",
      "Epoch: 18502 mean train loss:  7.36800442e-03, bound:  3.15596193e-01\n",
      "Epoch: 18503 mean train loss:  7.36727007e-03, bound:  3.15596133e-01\n",
      "Epoch: 18504 mean train loss:  7.36646727e-03, bound:  3.15596133e-01\n",
      "Epoch: 18505 mean train loss:  7.36572221e-03, bound:  3.15596074e-01\n",
      "Epoch: 18506 mean train loss:  7.36496923e-03, bound:  3.15596044e-01\n",
      "Epoch: 18507 mean train loss:  7.36417994e-03, bound:  3.15596014e-01\n",
      "Epoch: 18508 mean train loss:  7.36355828e-03, bound:  3.15595955e-01\n",
      "Epoch: 18509 mean train loss:  7.36281415e-03, bound:  3.15595955e-01\n",
      "Epoch: 18510 mean train loss:  7.36213662e-03, bound:  3.15595895e-01\n",
      "Epoch: 18511 mean train loss:  7.36165559e-03, bound:  3.15595835e-01\n",
      "Epoch: 18512 mean train loss:  7.36097526e-03, bound:  3.15595806e-01\n",
      "Epoch: 18513 mean train loss:  7.36057106e-03, bound:  3.15595806e-01\n",
      "Epoch: 18514 mean train loss:  7.36011518e-03, bound:  3.15595716e-01\n",
      "Epoch: 18515 mean train loss:  7.35989492e-03, bound:  3.15595716e-01\n",
      "Epoch: 18516 mean train loss:  7.35980505e-03, bound:  3.15595627e-01\n",
      "Epoch: 18517 mean train loss:  7.35996570e-03, bound:  3.15595627e-01\n",
      "Epoch: 18518 mean train loss:  7.36040808e-03, bound:  3.15595567e-01\n",
      "Epoch: 18519 mean train loss:  7.36111170e-03, bound:  3.15595567e-01\n",
      "Epoch: 18520 mean train loss:  7.36203510e-03, bound:  3.15595448e-01\n",
      "Epoch: 18521 mean train loss:  7.36317830e-03, bound:  3.15595508e-01\n",
      "Epoch: 18522 mean train loss:  7.36420229e-03, bound:  3.15595388e-01\n",
      "Epoch: 18523 mean train loss:  7.36489519e-03, bound:  3.15595448e-01\n",
      "Epoch: 18524 mean train loss:  7.36468146e-03, bound:  3.15595329e-01\n",
      "Epoch: 18525 mean train loss:  7.36314338e-03, bound:  3.15595359e-01\n",
      "Epoch: 18526 mean train loss:  7.36014964e-03, bound:  3.15595239e-01\n",
      "Epoch: 18527 mean train loss:  7.35619618e-03, bound:  3.15595269e-01\n",
      "Epoch: 18528 mean train loss:  7.35200522e-03, bound:  3.15595180e-01\n",
      "Epoch: 18529 mean train loss:  7.34859379e-03, bound:  3.15595180e-01\n",
      "Epoch: 18530 mean train loss:  7.34651694e-03, bound:  3.15595150e-01\n",
      "Epoch: 18531 mean train loss:  7.34580215e-03, bound:  3.15595120e-01\n",
      "Epoch: 18532 mean train loss:  7.34614814e-03, bound:  3.15595061e-01\n",
      "Epoch: 18533 mean train loss:  7.34688155e-03, bound:  3.15595001e-01\n",
      "Epoch: 18534 mean train loss:  7.34720193e-03, bound:  3.15595031e-01\n",
      "Epoch: 18535 mean train loss:  7.34682567e-03, bound:  3.15594941e-01\n",
      "Epoch: 18536 mean train loss:  7.34548969e-03, bound:  3.15594941e-01\n",
      "Epoch: 18537 mean train loss:  7.34346732e-03, bound:  3.15594852e-01\n",
      "Epoch: 18538 mean train loss:  7.34139187e-03, bound:  3.15594852e-01\n",
      "Epoch: 18539 mean train loss:  7.33974110e-03, bound:  3.15594822e-01\n",
      "Epoch: 18540 mean train loss:  7.33875344e-03, bound:  3.15594792e-01\n",
      "Epoch: 18541 mean train loss:  7.33835110e-03, bound:  3.15594733e-01\n",
      "Epoch: 18542 mean train loss:  7.33800186e-03, bound:  3.15594703e-01\n",
      "Epoch: 18543 mean train loss:  7.33768661e-03, bound:  3.15594673e-01\n",
      "Epoch: 18544 mean train loss:  7.33727682e-03, bound:  3.15594614e-01\n",
      "Epoch: 18545 mean train loss:  7.33656064e-03, bound:  3.15594614e-01\n",
      "Epoch: 18546 mean train loss:  7.33549614e-03, bound:  3.15594524e-01\n",
      "Epoch: 18547 mean train loss:  7.33433897e-03, bound:  3.15594524e-01\n",
      "Epoch: 18548 mean train loss:  7.33319018e-03, bound:  3.15594494e-01\n",
      "Epoch: 18549 mean train loss:  7.33212382e-03, bound:  3.15594465e-01\n",
      "Epoch: 18550 mean train loss:  7.33131776e-03, bound:  3.15594405e-01\n",
      "Epoch: 18551 mean train loss:  7.33053498e-03, bound:  3.15594375e-01\n",
      "Epoch: 18552 mean train loss:  7.32987979e-03, bound:  3.15594345e-01\n",
      "Epoch: 18553 mean train loss:  7.32938666e-03, bound:  3.15594316e-01\n",
      "Epoch: 18554 mean train loss:  7.32875476e-03, bound:  3.15594256e-01\n",
      "Epoch: 18555 mean train loss:  7.32804788e-03, bound:  3.15594226e-01\n",
      "Epoch: 18556 mean train loss:  7.32726185e-03, bound:  3.15594226e-01\n",
      "Epoch: 18557 mean train loss:  7.32639572e-03, bound:  3.15594137e-01\n",
      "Epoch: 18558 mean train loss:  7.32544018e-03, bound:  3.15594137e-01\n",
      "Epoch: 18559 mean train loss:  7.32455216e-03, bound:  3.15594077e-01\n",
      "Epoch: 18560 mean train loss:  7.32370187e-03, bound:  3.15594018e-01\n",
      "Epoch: 18561 mean train loss:  7.32292514e-03, bound:  3.15594018e-01\n",
      "Epoch: 18562 mean train loss:  7.32225413e-03, bound:  3.15593958e-01\n",
      "Epoch: 18563 mean train loss:  7.32160453e-03, bound:  3.15593898e-01\n",
      "Epoch: 18564 mean train loss:  7.32088275e-03, bound:  3.15593898e-01\n",
      "Epoch: 18565 mean train loss:  7.32016284e-03, bound:  3.15593839e-01\n",
      "Epoch: 18566 mean train loss:  7.31944852e-03, bound:  3.15593809e-01\n",
      "Epoch: 18567 mean train loss:  7.31868483e-03, bound:  3.15593809e-01\n",
      "Epoch: 18568 mean train loss:  7.31784850e-03, bound:  3.15593719e-01\n",
      "Epoch: 18569 mean train loss:  7.31696095e-03, bound:  3.15593690e-01\n",
      "Epoch: 18570 mean train loss:  7.31623499e-03, bound:  3.15593690e-01\n",
      "Epoch: 18571 mean train loss:  7.31540332e-03, bound:  3.15593600e-01\n",
      "Epoch: 18572 mean train loss:  7.31459539e-03, bound:  3.15593600e-01\n",
      "Epoch: 18573 mean train loss:  7.31384335e-03, bound:  3.15593541e-01\n",
      "Epoch: 18574 mean train loss:  7.31315417e-03, bound:  3.15593541e-01\n",
      "Epoch: 18575 mean train loss:  7.31237652e-03, bound:  3.15593481e-01\n",
      "Epoch: 18576 mean train loss:  7.31165474e-03, bound:  3.15593451e-01\n",
      "Epoch: 18577 mean train loss:  7.31089106e-03, bound:  3.15593421e-01\n",
      "Epoch: 18578 mean train loss:  7.31018931e-03, bound:  3.15593362e-01\n",
      "Epoch: 18579 mean train loss:  7.30944378e-03, bound:  3.15593332e-01\n",
      "Epoch: 18580 mean train loss:  7.30862981e-03, bound:  3.15593272e-01\n",
      "Epoch: 18581 mean train loss:  7.30789546e-03, bound:  3.15593243e-01\n",
      "Epoch: 18582 mean train loss:  7.30712945e-03, bound:  3.15593243e-01\n",
      "Epoch: 18583 mean train loss:  7.30632711e-03, bound:  3.15593153e-01\n",
      "Epoch: 18584 mean train loss:  7.30557181e-03, bound:  3.15593123e-01\n",
      "Epoch: 18585 mean train loss:  7.30488496e-03, bound:  3.15593123e-01\n",
      "Epoch: 18586 mean train loss:  7.30403792e-03, bound:  3.15593094e-01\n",
      "Epoch: 18587 mean train loss:  7.30332779e-03, bound:  3.15593034e-01\n",
      "Epoch: 18588 mean train loss:  7.30249798e-03, bound:  3.15593004e-01\n",
      "Epoch: 18589 mean train loss:  7.30179623e-03, bound:  3.15592945e-01\n",
      "Epoch: 18590 mean train loss:  7.30102370e-03, bound:  3.15592915e-01\n",
      "Epoch: 18591 mean train loss:  7.30028143e-03, bound:  3.15592885e-01\n",
      "Epoch: 18592 mean train loss:  7.29947211e-03, bound:  3.15592825e-01\n",
      "Epoch: 18593 mean train loss:  7.29875080e-03, bound:  3.15592796e-01\n",
      "Epoch: 18594 mean train loss:  7.29801413e-03, bound:  3.15592766e-01\n",
      "Epoch: 18595 mean train loss:  7.29730725e-03, bound:  3.15592706e-01\n",
      "Epoch: 18596 mean train loss:  7.29662227e-03, bound:  3.15592676e-01\n",
      "Epoch: 18597 mean train loss:  7.29583995e-03, bound:  3.15592676e-01\n",
      "Epoch: 18598 mean train loss:  7.29501387e-03, bound:  3.15592587e-01\n",
      "Epoch: 18599 mean train loss:  7.29434425e-03, bound:  3.15592587e-01\n",
      "Epoch: 18600 mean train loss:  7.29366578e-03, bound:  3.15592557e-01\n",
      "Epoch: 18601 mean train loss:  7.29305064e-03, bound:  3.15592498e-01\n",
      "Epoch: 18602 mean train loss:  7.29232561e-03, bound:  3.15592468e-01\n",
      "Epoch: 18603 mean train loss:  7.29159219e-03, bound:  3.15592438e-01\n",
      "Epoch: 18604 mean train loss:  7.29077216e-03, bound:  3.15592378e-01\n",
      "Epoch: 18605 mean train loss:  7.29007646e-03, bound:  3.15592378e-01\n",
      "Epoch: 18606 mean train loss:  7.28920475e-03, bound:  3.15592319e-01\n",
      "Epoch: 18607 mean train loss:  7.28842104e-03, bound:  3.15592319e-01\n",
      "Epoch: 18608 mean train loss:  7.28764338e-03, bound:  3.15592259e-01\n",
      "Epoch: 18609 mean train loss:  7.28688808e-03, bound:  3.15592200e-01\n",
      "Epoch: 18610 mean train loss:  7.28611788e-03, bound:  3.15592170e-01\n",
      "Epoch: 18611 mean train loss:  7.28533743e-03, bound:  3.15592140e-01\n",
      "Epoch: 18612 mean train loss:  7.28459936e-03, bound:  3.15592080e-01\n",
      "Epoch: 18613 mean train loss:  7.28389528e-03, bound:  3.15592080e-01\n",
      "Epoch: 18614 mean train loss:  7.28321401e-03, bound:  3.15592021e-01\n",
      "Epoch: 18615 mean train loss:  7.28243543e-03, bound:  3.15591991e-01\n",
      "Epoch: 18616 mean train loss:  7.28173321e-03, bound:  3.15591931e-01\n",
      "Epoch: 18617 mean train loss:  7.28102261e-03, bound:  3.15591902e-01\n",
      "Epoch: 18618 mean train loss:  7.28033669e-03, bound:  3.15591872e-01\n",
      "Epoch: 18619 mean train loss:  7.27974111e-03, bound:  3.15591842e-01\n",
      "Epoch: 18620 mean train loss:  7.27914274e-03, bound:  3.15591782e-01\n",
      "Epoch: 18621 mean train loss:  7.27858441e-03, bound:  3.15591782e-01\n",
      "Epoch: 18622 mean train loss:  7.27808103e-03, bound:  3.15591693e-01\n",
      "Epoch: 18623 mean train loss:  7.27766287e-03, bound:  3.15591693e-01\n",
      "Epoch: 18624 mean train loss:  7.27719255e-03, bound:  3.15591633e-01\n",
      "Epoch: 18625 mean train loss:  7.27682374e-03, bound:  3.15591604e-01\n",
      "Epoch: 18626 mean train loss:  7.27651641e-03, bound:  3.15591574e-01\n",
      "Epoch: 18627 mean train loss:  7.27625797e-03, bound:  3.15591574e-01\n",
      "Epoch: 18628 mean train loss:  7.27611873e-03, bound:  3.15591455e-01\n",
      "Epoch: 18629 mean train loss:  7.27605540e-03, bound:  3.15591484e-01\n",
      "Epoch: 18630 mean train loss:  7.27617601e-03, bound:  3.15591395e-01\n",
      "Epoch: 18631 mean train loss:  7.27609405e-03, bound:  3.15591425e-01\n",
      "Epoch: 18632 mean train loss:  7.27582909e-03, bound:  3.15591305e-01\n",
      "Epoch: 18633 mean train loss:  7.27543794e-03, bound:  3.15591335e-01\n",
      "Epoch: 18634 mean train loss:  7.27461139e-03, bound:  3.15591246e-01\n",
      "Epoch: 18635 mean train loss:  7.27339601e-03, bound:  3.15591276e-01\n",
      "Epoch: 18636 mean train loss:  7.27193616e-03, bound:  3.15591156e-01\n",
      "Epoch: 18637 mean train loss:  7.27005582e-03, bound:  3.15591186e-01\n",
      "Epoch: 18638 mean train loss:  7.26799294e-03, bound:  3.15591127e-01\n",
      "Epoch: 18639 mean train loss:  7.26595335e-03, bound:  3.15591127e-01\n",
      "Epoch: 18640 mean train loss:  7.26407068e-03, bound:  3.15591037e-01\n",
      "Epoch: 18641 mean train loss:  7.26262620e-03, bound:  3.15591037e-01\n",
      "Epoch: 18642 mean train loss:  7.26170372e-03, bound:  3.15590978e-01\n",
      "Epoch: 18643 mean train loss:  7.26104155e-03, bound:  3.15590918e-01\n",
      "Epoch: 18644 mean train loss:  7.26071047e-03, bound:  3.15590918e-01\n",
      "Epoch: 18645 mean train loss:  7.26041989e-03, bound:  3.15590888e-01\n",
      "Epoch: 18646 mean train loss:  7.26005621e-03, bound:  3.15590858e-01\n",
      "Epoch: 18647 mean train loss:  7.25971395e-03, bound:  3.15590769e-01\n",
      "Epoch: 18648 mean train loss:  7.25909090e-03, bound:  3.15590769e-01\n",
      "Epoch: 18649 mean train loss:  7.25828251e-03, bound:  3.15590709e-01\n",
      "Epoch: 18650 mean train loss:  7.25725293e-03, bound:  3.15590709e-01\n",
      "Epoch: 18651 mean train loss:  7.25618098e-03, bound:  3.15590650e-01\n",
      "Epoch: 18652 mean train loss:  7.25507317e-03, bound:  3.15590650e-01\n",
      "Epoch: 18653 mean train loss:  7.25397514e-03, bound:  3.15590560e-01\n",
      "Epoch: 18654 mean train loss:  7.25299632e-03, bound:  3.15590560e-01\n",
      "Epoch: 18655 mean train loss:  7.25206360e-03, bound:  3.15590471e-01\n",
      "Epoch: 18656 mean train loss:  7.25128036e-03, bound:  3.15590441e-01\n",
      "Epoch: 18657 mean train loss:  7.25049200e-03, bound:  3.15590441e-01\n",
      "Epoch: 18658 mean train loss:  7.24984659e-03, bound:  3.15590352e-01\n",
      "Epoch: 18659 mean train loss:  7.24917185e-03, bound:  3.15590352e-01\n",
      "Epoch: 18660 mean train loss:  7.24852504e-03, bound:  3.15590322e-01\n",
      "Epoch: 18661 mean train loss:  7.24794716e-03, bound:  3.15590322e-01\n",
      "Epoch: 18662 mean train loss:  7.24733062e-03, bound:  3.15590262e-01\n",
      "Epoch: 18663 mean train loss:  7.24662188e-03, bound:  3.15590203e-01\n",
      "Epoch: 18664 mean train loss:  7.24579021e-03, bound:  3.15590143e-01\n",
      "Epoch: 18665 mean train loss:  7.24512385e-03, bound:  3.15590143e-01\n",
      "Epoch: 18666 mean train loss:  7.24439463e-03, bound:  3.15590084e-01\n",
      "Epoch: 18667 mean train loss:  7.24348007e-03, bound:  3.15590084e-01\n",
      "Epoch: 18668 mean train loss:  7.24271825e-03, bound:  3.15590024e-01\n",
      "Epoch: 18669 mean train loss:  7.24187400e-03, bound:  3.15589994e-01\n",
      "Epoch: 18670 mean train loss:  7.24105164e-03, bound:  3.15589935e-01\n",
      "Epoch: 18671 mean train loss:  7.24026887e-03, bound:  3.15589905e-01\n",
      "Epoch: 18672 mean train loss:  7.23942323e-03, bound:  3.15589875e-01\n",
      "Epoch: 18673 mean train loss:  7.23856222e-03, bound:  3.15589875e-01\n",
      "Epoch: 18674 mean train loss:  7.23786419e-03, bound:  3.15589786e-01\n",
      "Epoch: 18675 mean train loss:  7.23703019e-03, bound:  3.15589756e-01\n",
      "Epoch: 18676 mean train loss:  7.23632658e-03, bound:  3.15589756e-01\n",
      "Epoch: 18677 mean train loss:  7.23556057e-03, bound:  3.15589666e-01\n",
      "Epoch: 18678 mean train loss:  7.23473448e-03, bound:  3.15589637e-01\n",
      "Epoch: 18679 mean train loss:  7.23409560e-03, bound:  3.15589637e-01\n",
      "Epoch: 18680 mean train loss:  7.23334914e-03, bound:  3.15589577e-01\n",
      "Epoch: 18681 mean train loss:  7.23259803e-03, bound:  3.15589547e-01\n",
      "Epoch: 18682 mean train loss:  7.23188277e-03, bound:  3.15589488e-01\n",
      "Epoch: 18683 mean train loss:  7.23109860e-03, bound:  3.15589458e-01\n",
      "Epoch: 18684 mean train loss:  7.23050674e-03, bound:  3.15589428e-01\n",
      "Epoch: 18685 mean train loss:  7.22966762e-03, bound:  3.15589368e-01\n",
      "Epoch: 18686 mean train loss:  7.22895237e-03, bound:  3.15589339e-01\n",
      "Epoch: 18687 mean train loss:  7.22818775e-03, bound:  3.15589309e-01\n",
      "Epoch: 18688 mean train loss:  7.22757960e-03, bound:  3.15589309e-01\n",
      "Epoch: 18689 mean train loss:  7.22686155e-03, bound:  3.15589219e-01\n",
      "Epoch: 18690 mean train loss:  7.22612627e-03, bound:  3.15589219e-01\n",
      "Epoch: 18691 mean train loss:  7.22535513e-03, bound:  3.15589160e-01\n",
      "Epoch: 18692 mean train loss:  7.22475117e-03, bound:  3.15589130e-01\n",
      "Epoch: 18693 mean train loss:  7.22407037e-03, bound:  3.15589100e-01\n",
      "Epoch: 18694 mean train loss:  7.22348411e-03, bound:  3.15589070e-01\n",
      "Epoch: 18695 mean train loss:  7.22273998e-03, bound:  3.15589011e-01\n",
      "Epoch: 18696 mean train loss:  7.22208526e-03, bound:  3.15589011e-01\n",
      "Epoch: 18697 mean train loss:  7.22154602e-03, bound:  3.15588921e-01\n",
      "Epoch: 18698 mean train loss:  7.22099887e-03, bound:  3.15588921e-01\n",
      "Epoch: 18699 mean train loss:  7.22048618e-03, bound:  3.15588892e-01\n",
      "Epoch: 18700 mean train loss:  7.21998699e-03, bound:  3.15588862e-01\n",
      "Epoch: 18701 mean train loss:  7.21970294e-03, bound:  3.15588802e-01\n",
      "Epoch: 18702 mean train loss:  7.21955020e-03, bound:  3.15588802e-01\n",
      "Epoch: 18703 mean train loss:  7.21942866e-03, bound:  3.15588713e-01\n",
      "Epoch: 18704 mean train loss:  7.21951667e-03, bound:  3.15588742e-01\n",
      "Epoch: 18705 mean train loss:  7.21984636e-03, bound:  3.15588623e-01\n",
      "Epoch: 18706 mean train loss:  7.22037861e-03, bound:  3.15588653e-01\n",
      "Epoch: 18707 mean train loss:  7.22105196e-03, bound:  3.15588534e-01\n",
      "Epoch: 18708 mean train loss:  7.22170388e-03, bound:  3.15588593e-01\n",
      "Epoch: 18709 mean train loss:  7.22205406e-03, bound:  3.15588474e-01\n",
      "Epoch: 18710 mean train loss:  7.22198002e-03, bound:  3.15588534e-01\n",
      "Epoch: 18711 mean train loss:  7.22110970e-03, bound:  3.15588415e-01\n",
      "Epoch: 18712 mean train loss:  7.21913297e-03, bound:  3.15588444e-01\n",
      "Epoch: 18713 mean train loss:  7.21622445e-03, bound:  3.15588325e-01\n",
      "Epoch: 18714 mean train loss:  7.21278926e-03, bound:  3.15588355e-01\n",
      "Epoch: 18715 mean train loss:  7.20966933e-03, bound:  3.15588295e-01\n",
      "Epoch: 18716 mean train loss:  7.20727537e-03, bound:  3.15588236e-01\n",
      "Epoch: 18717 mean train loss:  7.20585138e-03, bound:  3.15588206e-01\n",
      "Epoch: 18718 mean train loss:  7.20525952e-03, bound:  3.15588206e-01\n",
      "Epoch: 18719 mean train loss:  7.20531307e-03, bound:  3.15588176e-01\n",
      "Epoch: 18720 mean train loss:  7.20539037e-03, bound:  3.15588117e-01\n",
      "Epoch: 18721 mean train loss:  7.20542623e-03, bound:  3.15588117e-01\n",
      "Epoch: 18722 mean train loss:  7.20497081e-03, bound:  3.15588027e-01\n",
      "Epoch: 18723 mean train loss:  7.20413541e-03, bound:  3.15588027e-01\n",
      "Epoch: 18724 mean train loss:  7.20293121e-03, bound:  3.15587968e-01\n",
      "Epoch: 18725 mean train loss:  7.20143598e-03, bound:  3.15587968e-01\n",
      "Epoch: 18726 mean train loss:  7.19997892e-03, bound:  3.15587878e-01\n",
      "Epoch: 18727 mean train loss:  7.19869137e-03, bound:  3.15587878e-01\n",
      "Epoch: 18728 mean train loss:  7.19784619e-03, bound:  3.15587848e-01\n",
      "Epoch: 18729 mean train loss:  7.19717005e-03, bound:  3.15587789e-01\n",
      "Epoch: 18730 mean train loss:  7.19659217e-03, bound:  3.15587759e-01\n",
      "Epoch: 18731 mean train loss:  7.19617261e-03, bound:  3.15587729e-01\n",
      "Epoch: 18732 mean train loss:  7.19565200e-03, bound:  3.15587729e-01\n",
      "Epoch: 18733 mean train loss:  7.19505269e-03, bound:  3.15587640e-01\n",
      "Epoch: 18734 mean train loss:  7.19416374e-03, bound:  3.15587640e-01\n",
      "Epoch: 18735 mean train loss:  7.19327712e-03, bound:  3.15587550e-01\n",
      "Epoch: 18736 mean train loss:  7.19240867e-03, bound:  3.15587550e-01\n",
      "Epoch: 18737 mean train loss:  7.19140470e-03, bound:  3.15587521e-01\n",
      "Epoch: 18738 mean train loss:  7.19059817e-03, bound:  3.15587461e-01\n",
      "Epoch: 18739 mean train loss:  7.18975672e-03, bound:  3.15587431e-01\n",
      "Epoch: 18740 mean train loss:  7.18910294e-03, bound:  3.15587401e-01\n",
      "Epoch: 18741 mean train loss:  7.18838489e-03, bound:  3.15587401e-01\n",
      "Epoch: 18742 mean train loss:  7.18773017e-03, bound:  3.15587312e-01\n",
      "Epoch: 18743 mean train loss:  7.18704564e-03, bound:  3.15587312e-01\n",
      "Epoch: 18744 mean train loss:  7.18634808e-03, bound:  3.15587252e-01\n",
      "Epoch: 18745 mean train loss:  7.18570687e-03, bound:  3.15587223e-01\n",
      "Epoch: 18746 mean train loss:  7.18494924e-03, bound:  3.15587193e-01\n",
      "Epoch: 18747 mean train loss:  7.18409102e-03, bound:  3.15587133e-01\n",
      "Epoch: 18748 mean train loss:  7.18338881e-03, bound:  3.15587103e-01\n",
      "Epoch: 18749 mean train loss:  7.18254317e-03, bound:  3.15587074e-01\n",
      "Epoch: 18750 mean train loss:  7.18179671e-03, bound:  3.15587014e-01\n",
      "Epoch: 18751 mean train loss:  7.18107168e-03, bound:  3.15586984e-01\n",
      "Epoch: 18752 mean train loss:  7.18033174e-03, bound:  3.15586984e-01\n",
      "Epoch: 18753 mean train loss:  7.17956899e-03, bound:  3.15586954e-01\n",
      "Epoch: 18754 mean train loss:  7.17884814e-03, bound:  3.15586895e-01\n",
      "Epoch: 18755 mean train loss:  7.17816129e-03, bound:  3.15586865e-01\n",
      "Epoch: 18756 mean train loss:  7.17741949e-03, bound:  3.15586835e-01\n",
      "Epoch: 18757 mean train loss:  7.17667164e-03, bound:  3.15586776e-01\n",
      "Epoch: 18758 mean train loss:  7.17585348e-03, bound:  3.15586776e-01\n",
      "Epoch: 18759 mean train loss:  7.17516197e-03, bound:  3.15586716e-01\n",
      "Epoch: 18760 mean train loss:  7.17443554e-03, bound:  3.15586686e-01\n",
      "Epoch: 18761 mean train loss:  7.17370491e-03, bound:  3.15586656e-01\n",
      "Epoch: 18762 mean train loss:  7.17299245e-03, bound:  3.15586627e-01\n",
      "Epoch: 18763 mean train loss:  7.17221480e-03, bound:  3.15586567e-01\n",
      "Epoch: 18764 mean train loss:  7.17154378e-03, bound:  3.15586537e-01\n",
      "Epoch: 18765 mean train loss:  7.17076473e-03, bound:  3.15586507e-01\n",
      "Epoch: 18766 mean train loss:  7.17011094e-03, bound:  3.15586448e-01\n",
      "Epoch: 18767 mean train loss:  7.16933468e-03, bound:  3.15586418e-01\n",
      "Epoch: 18768 mean train loss:  7.16858078e-03, bound:  3.15586388e-01\n",
      "Epoch: 18769 mean train loss:  7.16783106e-03, bound:  3.15586358e-01\n",
      "Epoch: 18770 mean train loss:  7.16709998e-03, bound:  3.15586329e-01\n",
      "Epoch: 18771 mean train loss:  7.16632837e-03, bound:  3.15586299e-01\n",
      "Epoch: 18772 mean train loss:  7.16568995e-03, bound:  3.15586269e-01\n",
      "Epoch: 18773 mean train loss:  7.16494815e-03, bound:  3.15586209e-01\n",
      "Epoch: 18774 mean train loss:  7.16419797e-03, bound:  3.15586209e-01\n",
      "Epoch: 18775 mean train loss:  7.16343522e-03, bound:  3.15586120e-01\n",
      "Epoch: 18776 mean train loss:  7.16273533e-03, bound:  3.15586090e-01\n",
      "Epoch: 18777 mean train loss:  7.16206199e-03, bound:  3.15586090e-01\n",
      "Epoch: 18778 mean train loss:  7.16135092e-03, bound:  3.15586001e-01\n",
      "Epoch: 18779 mean train loss:  7.16056256e-03, bound:  3.15585971e-01\n",
      "Epoch: 18780 mean train loss:  7.15984264e-03, bound:  3.15585971e-01\n",
      "Epoch: 18781 mean train loss:  7.15910736e-03, bound:  3.15585911e-01\n",
      "Epoch: 18782 mean train loss:  7.15842145e-03, bound:  3.15585881e-01\n",
      "Epoch: 18783 mean train loss:  7.15773273e-03, bound:  3.15585852e-01\n",
      "Epoch: 18784 mean train loss:  7.15694763e-03, bound:  3.15585822e-01\n",
      "Epoch: 18785 mean train loss:  7.15634180e-03, bound:  3.15585762e-01\n",
      "Epoch: 18786 mean train loss:  7.15565402e-03, bound:  3.15585732e-01\n",
      "Epoch: 18787 mean train loss:  7.15498859e-03, bound:  3.15585673e-01\n",
      "Epoch: 18788 mean train loss:  7.15430081e-03, bound:  3.15585673e-01\n",
      "Epoch: 18789 mean train loss:  7.15362467e-03, bound:  3.15585613e-01\n",
      "Epoch: 18790 mean train loss:  7.15308543e-03, bound:  3.15585613e-01\n",
      "Epoch: 18791 mean train loss:  7.15249777e-03, bound:  3.15585554e-01\n",
      "Epoch: 18792 mean train loss:  7.15200696e-03, bound:  3.15585524e-01\n",
      "Epoch: 18793 mean train loss:  7.15159206e-03, bound:  3.15585494e-01\n",
      "Epoch: 18794 mean train loss:  7.15118367e-03, bound:  3.15585494e-01\n",
      "Epoch: 18795 mean train loss:  7.15100905e-03, bound:  3.15585405e-01\n",
      "Epoch: 18796 mean train loss:  7.15092663e-03, bound:  3.15585405e-01\n",
      "Epoch: 18797 mean train loss:  7.15096388e-03, bound:  3.15585285e-01\n",
      "Epoch: 18798 mean train loss:  7.15128565e-03, bound:  3.15585345e-01\n",
      "Epoch: 18799 mean train loss:  7.15178624e-03, bound:  3.15585226e-01\n",
      "Epoch: 18800 mean train loss:  7.15250708e-03, bound:  3.15585285e-01\n",
      "Epoch: 18801 mean train loss:  7.15318508e-03, bound:  3.15585166e-01\n",
      "Epoch: 18802 mean train loss:  7.15373131e-03, bound:  3.15585196e-01\n",
      "Epoch: 18803 mean train loss:  7.15387892e-03, bound:  3.15585077e-01\n",
      "Epoch: 18804 mean train loss:  7.15324376e-03, bound:  3.15585107e-01\n",
      "Epoch: 18805 mean train loss:  7.15150870e-03, bound:  3.15584987e-01\n",
      "Epoch: 18806 mean train loss:  7.14891776e-03, bound:  3.15585077e-01\n",
      "Epoch: 18807 mean train loss:  7.14564789e-03, bound:  3.15584958e-01\n",
      "Epoch: 18808 mean train loss:  7.14246836e-03, bound:  3.15584958e-01\n",
      "Epoch: 18809 mean train loss:  7.13973166e-03, bound:  3.15584898e-01\n",
      "Epoch: 18810 mean train loss:  7.13807810e-03, bound:  3.15584868e-01\n",
      "Epoch: 18811 mean train loss:  7.13736750e-03, bound:  3.15584838e-01\n",
      "Epoch: 18812 mean train loss:  7.13748392e-03, bound:  3.15584779e-01\n",
      "Epoch: 18813 mean train loss:  7.13782432e-03, bound:  3.15584779e-01\n",
      "Epoch: 18814 mean train loss:  7.13807624e-03, bound:  3.15584719e-01\n",
      "Epoch: 18815 mean train loss:  7.13793980e-03, bound:  3.15584719e-01\n",
      "Epoch: 18816 mean train loss:  7.13704992e-03, bound:  3.15584660e-01\n",
      "Epoch: 18817 mean train loss:  7.13580241e-03, bound:  3.15584660e-01\n",
      "Epoch: 18818 mean train loss:  7.13414839e-03, bound:  3.15584540e-01\n",
      "Epoch: 18819 mean train loss:  7.13254977e-03, bound:  3.15584540e-01\n",
      "Epoch: 18820 mean train loss:  7.13108806e-03, bound:  3.15584540e-01\n",
      "Epoch: 18821 mean train loss:  7.13006407e-03, bound:  3.15584511e-01\n",
      "Epoch: 18822 mean train loss:  7.12943869e-03, bound:  3.15584451e-01\n",
      "Epoch: 18823 mean train loss:  7.12902891e-03, bound:  3.15584421e-01\n",
      "Epoch: 18824 mean train loss:  7.12864334e-03, bound:  3.15584391e-01\n",
      "Epoch: 18825 mean train loss:  7.12822797e-03, bound:  3.15584332e-01\n",
      "Epoch: 18826 mean train loss:  7.12759933e-03, bound:  3.15584302e-01\n",
      "Epoch: 18827 mean train loss:  7.12689105e-03, bound:  3.15584272e-01\n",
      "Epoch: 18828 mean train loss:  7.12594995e-03, bound:  3.15584272e-01\n",
      "Epoch: 18829 mean train loss:  7.12497067e-03, bound:  3.15584183e-01\n",
      "Epoch: 18830 mean train loss:  7.12401606e-03, bound:  3.15584183e-01\n",
      "Epoch: 18831 mean train loss:  7.12301210e-03, bound:  3.15584123e-01\n",
      "Epoch: 18832 mean train loss:  7.12219300e-03, bound:  3.15584093e-01\n",
      "Epoch: 18833 mean train loss:  7.12144421e-03, bound:  3.15584064e-01\n",
      "Epoch: 18834 mean train loss:  7.12084537e-03, bound:  3.15584034e-01\n",
      "Epoch: 18835 mean train loss:  7.12018786e-03, bound:  3.15583974e-01\n",
      "Epoch: 18836 mean train loss:  7.11957691e-03, bound:  3.15583974e-01\n",
      "Epoch: 18837 mean train loss:  7.11902883e-03, bound:  3.15583944e-01\n",
      "Epoch: 18838 mean train loss:  7.11830892e-03, bound:  3.15583855e-01\n",
      "Epoch: 18839 mean train loss:  7.11760065e-03, bound:  3.15583855e-01\n",
      "Epoch: 18840 mean train loss:  7.11676572e-03, bound:  3.15583795e-01\n",
      "Epoch: 18841 mean train loss:  7.11597502e-03, bound:  3.15583766e-01\n",
      "Epoch: 18842 mean train loss:  7.11527001e-03, bound:  3.15583736e-01\n",
      "Epoch: 18843 mean train loss:  7.11451005e-03, bound:  3.15583736e-01\n",
      "Epoch: 18844 mean train loss:  7.11365137e-03, bound:  3.15583646e-01\n",
      "Epoch: 18845 mean train loss:  7.11289793e-03, bound:  3.15583646e-01\n",
      "Epoch: 18846 mean train loss:  7.11216172e-03, bound:  3.15583616e-01\n",
      "Epoch: 18847 mean train loss:  7.11139804e-03, bound:  3.15583557e-01\n",
      "Epoch: 18848 mean train loss:  7.11081456e-03, bound:  3.15583527e-01\n",
      "Epoch: 18849 mean train loss:  7.11000524e-03, bound:  3.15583497e-01\n",
      "Epoch: 18850 mean train loss:  7.10937381e-03, bound:  3.15583438e-01\n",
      "Epoch: 18851 mean train loss:  7.10861152e-03, bound:  3.15583438e-01\n",
      "Epoch: 18852 mean train loss:  7.10793072e-03, bound:  3.15583408e-01\n",
      "Epoch: 18853 mean train loss:  7.10717216e-03, bound:  3.15583318e-01\n",
      "Epoch: 18854 mean train loss:  7.10640894e-03, bound:  3.15583318e-01\n",
      "Epoch: 18855 mean train loss:  7.10571278e-03, bound:  3.15583289e-01\n",
      "Epoch: 18856 mean train loss:  7.10505061e-03, bound:  3.15583259e-01\n",
      "Epoch: 18857 mean train loss:  7.10431905e-03, bound:  3.15583229e-01\n",
      "Epoch: 18858 mean train loss:  7.10353162e-03, bound:  3.15583169e-01\n",
      "Epoch: 18859 mean train loss:  7.10286666e-03, bound:  3.15583140e-01\n",
      "Epoch: 18860 mean train loss:  7.10213417e-03, bound:  3.15583110e-01\n",
      "Epoch: 18861 mean train loss:  7.10142870e-03, bound:  3.15583050e-01\n",
      "Epoch: 18862 mean train loss:  7.10065663e-03, bound:  3.15583050e-01\n",
      "Epoch: 18863 mean train loss:  7.09994603e-03, bound:  3.15582991e-01\n",
      "Epoch: 18864 mean train loss:  7.09927967e-03, bound:  3.15582961e-01\n",
      "Epoch: 18865 mean train loss:  7.09855696e-03, bound:  3.15582961e-01\n",
      "Epoch: 18866 mean train loss:  7.09784310e-03, bound:  3.15582871e-01\n",
      "Epoch: 18867 mean train loss:  7.09706778e-03, bound:  3.15582842e-01\n",
      "Epoch: 18868 mean train loss:  7.09636929e-03, bound:  3.15582842e-01\n",
      "Epoch: 18869 mean train loss:  7.09563401e-03, bound:  3.15582752e-01\n",
      "Epoch: 18870 mean train loss:  7.09496345e-03, bound:  3.15582722e-01\n",
      "Epoch: 18871 mean train loss:  7.09426031e-03, bound:  3.15582722e-01\n",
      "Epoch: 18872 mean train loss:  7.09341560e-03, bound:  3.15582663e-01\n",
      "Epoch: 18873 mean train loss:  7.09274085e-03, bound:  3.15582633e-01\n",
      "Epoch: 18874 mean train loss:  7.09201489e-03, bound:  3.15582603e-01\n",
      "Epoch: 18875 mean train loss:  7.09139230e-03, bound:  3.15582544e-01\n",
      "Epoch: 18876 mean train loss:  7.09062116e-03, bound:  3.15582544e-01\n",
      "Epoch: 18877 mean train loss:  7.08995108e-03, bound:  3.15582514e-01\n",
      "Epoch: 18878 mean train loss:  7.08922651e-03, bound:  3.15582484e-01\n",
      "Epoch: 18879 mean train loss:  7.08861370e-03, bound:  3.15582424e-01\n",
      "Epoch: 18880 mean train loss:  7.08777132e-03, bound:  3.15582395e-01\n",
      "Epoch: 18881 mean train loss:  7.08718970e-03, bound:  3.15582365e-01\n",
      "Epoch: 18882 mean train loss:  7.08652847e-03, bound:  3.15582305e-01\n",
      "Epoch: 18883 mean train loss:  7.08588492e-03, bound:  3.15582305e-01\n",
      "Epoch: 18884 mean train loss:  7.08516687e-03, bound:  3.15582216e-01\n",
      "Epoch: 18885 mean train loss:  7.08455360e-03, bound:  3.15582216e-01\n",
      "Epoch: 18886 mean train loss:  7.08381645e-03, bound:  3.15582156e-01\n",
      "Epoch: 18887 mean train loss:  7.08335172e-03, bound:  3.15582156e-01\n",
      "Epoch: 18888 mean train loss:  7.08276965e-03, bound:  3.15582097e-01\n",
      "Epoch: 18889 mean train loss:  7.08215358e-03, bound:  3.15582067e-01\n",
      "Epoch: 18890 mean train loss:  7.08170189e-03, bound:  3.15582037e-01\n",
      "Epoch: 18891 mean train loss:  7.08129769e-03, bound:  3.15582037e-01\n",
      "Epoch: 18892 mean train loss:  7.08095543e-03, bound:  3.15581948e-01\n",
      "Epoch: 18893 mean train loss:  7.08071422e-03, bound:  3.15581948e-01\n",
      "Epoch: 18894 mean train loss:  7.08067184e-03, bound:  3.15581858e-01\n",
      "Epoch: 18895 mean train loss:  7.08070071e-03, bound:  3.15581858e-01\n",
      "Epoch: 18896 mean train loss:  7.08097545e-03, bound:  3.15581799e-01\n",
      "Epoch: 18897 mean train loss:  7.08138291e-03, bound:  3.15581828e-01\n",
      "Epoch: 18898 mean train loss:  7.08203763e-03, bound:  3.15581739e-01\n",
      "Epoch: 18899 mean train loss:  7.08272634e-03, bound:  3.15581739e-01\n",
      "Epoch: 18900 mean train loss:  7.08325254e-03, bound:  3.15581620e-01\n",
      "Epoch: 18901 mean train loss:  7.08345976e-03, bound:  3.15581709e-01\n",
      "Epoch: 18902 mean train loss:  7.08280504e-03, bound:  3.15581590e-01\n",
      "Epoch: 18903 mean train loss:  7.08121294e-03, bound:  3.15581620e-01\n",
      "Epoch: 18904 mean train loss:  7.07854051e-03, bound:  3.15581501e-01\n",
      "Epoch: 18905 mean train loss:  7.07540708e-03, bound:  3.15581501e-01\n",
      "Epoch: 18906 mean train loss:  7.07219774e-03, bound:  3.15581441e-01\n",
      "Epoch: 18907 mean train loss:  7.06954580e-03, bound:  3.15581441e-01\n",
      "Epoch: 18908 mean train loss:  7.06785964e-03, bound:  3.15581411e-01\n",
      "Epoch: 18909 mean train loss:  7.06716068e-03, bound:  3.15581381e-01\n",
      "Epoch: 18910 mean train loss:  7.06714531e-03, bound:  3.15581322e-01\n",
      "Epoch: 18911 mean train loss:  7.06738187e-03, bound:  3.15581292e-01\n",
      "Epoch: 18912 mean train loss:  7.06743775e-03, bound:  3.15581292e-01\n",
      "Epoch: 18913 mean train loss:  7.06725568e-03, bound:  3.15581203e-01\n",
      "Epoch: 18914 mean train loss:  7.06641562e-03, bound:  3.15581203e-01\n",
      "Epoch: 18915 mean train loss:  7.06514949e-03, bound:  3.15581173e-01\n",
      "Epoch: 18916 mean train loss:  7.06358301e-03, bound:  3.15581143e-01\n",
      "Epoch: 18917 mean train loss:  7.06224190e-03, bound:  3.15581083e-01\n",
      "Epoch: 18918 mean train loss:  7.06096459e-03, bound:  3.15581053e-01\n",
      "Epoch: 18919 mean train loss:  7.05999509e-03, bound:  3.15581024e-01\n",
      "Epoch: 18920 mean train loss:  7.05942325e-03, bound:  3.15580994e-01\n",
      "Epoch: 18921 mean train loss:  7.05909310e-03, bound:  3.15580964e-01\n",
      "Epoch: 18922 mean train loss:  7.05875736e-03, bound:  3.15580904e-01\n",
      "Epoch: 18923 mean train loss:  7.05837132e-03, bound:  3.15580904e-01\n",
      "Epoch: 18924 mean train loss:  7.05790659e-03, bound:  3.15580845e-01\n",
      "Epoch: 18925 mean train loss:  7.05721974e-03, bound:  3.15580845e-01\n",
      "Epoch: 18926 mean train loss:  7.05618085e-03, bound:  3.15580785e-01\n",
      "Epoch: 18927 mean train loss:  7.05520436e-03, bound:  3.15580755e-01\n",
      "Epoch: 18928 mean train loss:  7.05419853e-03, bound:  3.15580726e-01\n",
      "Epoch: 18929 mean train loss:  7.05325278e-03, bound:  3.15580696e-01\n",
      "Epoch: 18930 mean train loss:  7.05227861e-03, bound:  3.15580636e-01\n",
      "Epoch: 18931 mean train loss:  7.05160061e-03, bound:  3.15580606e-01\n",
      "Epoch: 18932 mean train loss:  7.05093751e-03, bound:  3.15580577e-01\n",
      "Epoch: 18933 mean train loss:  7.05036009e-03, bound:  3.15580517e-01\n",
      "Epoch: 18934 mean train loss:  7.04988232e-03, bound:  3.15580517e-01\n",
      "Epoch: 18935 mean train loss:  7.04928441e-03, bound:  3.15580457e-01\n",
      "Epoch: 18936 mean train loss:  7.04861246e-03, bound:  3.15580457e-01\n",
      "Epoch: 18937 mean train loss:  7.04795867e-03, bound:  3.15580398e-01\n",
      "Epoch: 18938 mean train loss:  7.04719592e-03, bound:  3.15580368e-01\n",
      "Epoch: 18939 mean train loss:  7.04644853e-03, bound:  3.15580308e-01\n",
      "Epoch: 18940 mean train loss:  7.04559172e-03, bound:  3.15580308e-01\n",
      "Epoch: 18941 mean train loss:  7.04479776e-03, bound:  3.15580249e-01\n",
      "Epoch: 18942 mean train loss:  7.04397541e-03, bound:  3.15580189e-01\n",
      "Epoch: 18943 mean train loss:  7.04319216e-03, bound:  3.15580189e-01\n",
      "Epoch: 18944 mean train loss:  7.04248715e-03, bound:  3.15580159e-01\n",
      "Epoch: 18945 mean train loss:  7.04176351e-03, bound:  3.15580070e-01\n",
      "Epoch: 18946 mean train loss:  7.04108551e-03, bound:  3.15580070e-01\n",
      "Epoch: 18947 mean train loss:  7.04038423e-03, bound:  3.15580070e-01\n",
      "Epoch: 18948 mean train loss:  7.03969691e-03, bound:  3.15580040e-01\n",
      "Epoch: 18949 mean train loss:  7.03902962e-03, bound:  3.15579981e-01\n",
      "Epoch: 18950 mean train loss:  7.03825755e-03, bound:  3.15579951e-01\n",
      "Epoch: 18951 mean train loss:  7.03760516e-03, bound:  3.15579921e-01\n",
      "Epoch: 18952 mean train loss:  7.03693321e-03, bound:  3.15579861e-01\n",
      "Epoch: 18953 mean train loss:  7.03618675e-03, bound:  3.15579832e-01\n",
      "Epoch: 18954 mean train loss:  7.03547243e-03, bound:  3.15579802e-01\n",
      "Epoch: 18955 mean train loss:  7.03472272e-03, bound:  3.15579802e-01\n",
      "Epoch: 18956 mean train loss:  7.03411829e-03, bound:  3.15579712e-01\n",
      "Epoch: 18957 mean train loss:  7.03330105e-03, bound:  3.15579712e-01\n",
      "Epoch: 18958 mean train loss:  7.03257881e-03, bound:  3.15579683e-01\n",
      "Epoch: 18959 mean train loss:  7.03192037e-03, bound:  3.15579623e-01\n",
      "Epoch: 18960 mean train loss:  7.03116320e-03, bound:  3.15579593e-01\n",
      "Epoch: 18961 mean train loss:  7.03048287e-03, bound:  3.15579593e-01\n",
      "Epoch: 18962 mean train loss:  7.02979509e-03, bound:  3.15579504e-01\n",
      "Epoch: 18963 mean train loss:  7.02910451e-03, bound:  3.15579474e-01\n",
      "Epoch: 18964 mean train loss:  7.02837156e-03, bound:  3.15579474e-01\n",
      "Epoch: 18965 mean train loss:  7.02777132e-03, bound:  3.15579385e-01\n",
      "Epoch: 18966 mean train loss:  7.02703232e-03, bound:  3.15579385e-01\n",
      "Epoch: 18967 mean train loss:  7.02632777e-03, bound:  3.15579355e-01\n",
      "Epoch: 18968 mean train loss:  7.02562043e-03, bound:  3.15579355e-01\n",
      "Epoch: 18969 mean train loss:  7.02481437e-03, bound:  3.15579295e-01\n",
      "Epoch: 18970 mean train loss:  7.02422764e-03, bound:  3.15579265e-01\n",
      "Epoch: 18971 mean train loss:  7.02347653e-03, bound:  3.15579206e-01\n",
      "Epoch: 18972 mean train loss:  7.02283066e-03, bound:  3.15579176e-01\n",
      "Epoch: 18973 mean train loss:  7.02216150e-03, bound:  3.15579146e-01\n",
      "Epoch: 18974 mean train loss:  7.02149188e-03, bound:  3.15579087e-01\n",
      "Epoch: 18975 mean train loss:  7.02071469e-03, bound:  3.15579057e-01\n",
      "Epoch: 18976 mean train loss:  7.02006789e-03, bound:  3.15579027e-01\n",
      "Epoch: 18977 mean train loss:  7.01946486e-03, bound:  3.15578967e-01\n",
      "Epoch: 18978 mean train loss:  7.01869978e-03, bound:  3.15578967e-01\n",
      "Epoch: 18979 mean train loss:  7.01802783e-03, bound:  3.15578908e-01\n",
      "Epoch: 18980 mean train loss:  7.01731537e-03, bound:  3.15578908e-01\n",
      "Epoch: 18981 mean train loss:  7.01670581e-03, bound:  3.15578848e-01\n",
      "Epoch: 18982 mean train loss:  7.01604970e-03, bound:  3.15578818e-01\n",
      "Epoch: 18983 mean train loss:  7.01542012e-03, bound:  3.15578759e-01\n",
      "Epoch: 18984 mean train loss:  7.01485900e-03, bound:  3.15578759e-01\n",
      "Epoch: 18985 mean train loss:  7.01437239e-03, bound:  3.15578729e-01\n",
      "Epoch: 18986 mean train loss:  7.01387785e-03, bound:  3.15578699e-01\n",
      "Epoch: 18987 mean train loss:  7.01355608e-03, bound:  3.15578640e-01\n",
      "Epoch: 18988 mean train loss:  7.01324455e-03, bound:  3.15578640e-01\n",
      "Epoch: 18989 mean train loss:  7.01298332e-03, bound:  3.15578580e-01\n",
      "Epoch: 18990 mean train loss:  7.01283803e-03, bound:  3.15578580e-01\n",
      "Epoch: 18991 mean train loss:  7.01285899e-03, bound:  3.15578490e-01\n",
      "Epoch: 18992 mean train loss:  7.01294700e-03, bound:  3.15578490e-01\n",
      "Epoch: 18993 mean train loss:  7.01331627e-03, bound:  3.15578401e-01\n",
      "Epoch: 18994 mean train loss:  7.01371534e-03, bound:  3.15578461e-01\n",
      "Epoch: 18995 mean train loss:  7.01419683e-03, bound:  3.15578371e-01\n",
      "Epoch: 18996 mean train loss:  7.01450417e-03, bound:  3.15578371e-01\n",
      "Epoch: 18997 mean train loss:  7.01459730e-03, bound:  3.15578252e-01\n",
      "Epoch: 18998 mean train loss:  7.01411068e-03, bound:  3.15578312e-01\n",
      "Epoch: 18999 mean train loss:  7.01296702e-03, bound:  3.15578192e-01\n",
      "Epoch: 19000 mean train loss:  7.01114768e-03, bound:  3.15578252e-01\n",
      "Epoch: 19001 mean train loss:  7.00862659e-03, bound:  3.15578133e-01\n",
      "Epoch: 19002 mean train loss:  7.00579863e-03, bound:  3.15578133e-01\n",
      "Epoch: 19003 mean train loss:  7.00316671e-03, bound:  3.15578073e-01\n",
      "Epoch: 19004 mean train loss:  7.00108241e-03, bound:  3.15578043e-01\n",
      "Epoch: 19005 mean train loss:  6.99972967e-03, bound:  3.15578043e-01\n",
      "Epoch: 19006 mean train loss:  6.99899578e-03, bound:  3.15577954e-01\n",
      "Epoch: 19007 mean train loss:  6.99883187e-03, bound:  3.15577954e-01\n",
      "Epoch: 19008 mean train loss:  6.99883513e-03, bound:  3.15577924e-01\n",
      "Epoch: 19009 mean train loss:  6.99882070e-03, bound:  3.15577924e-01\n",
      "Epoch: 19010 mean train loss:  6.99841324e-03, bound:  3.15577835e-01\n",
      "Epoch: 19011 mean train loss:  6.99777063e-03, bound:  3.15577835e-01\n",
      "Epoch: 19012 mean train loss:  6.99680718e-03, bound:  3.15577805e-01\n",
      "Epoch: 19013 mean train loss:  6.99564349e-03, bound:  3.15577805e-01\n",
      "Epoch: 19014 mean train loss:  6.99434988e-03, bound:  3.15577716e-01\n",
      "Epoch: 19015 mean train loss:  6.99312240e-03, bound:  3.15577716e-01\n",
      "Epoch: 19016 mean train loss:  6.99216919e-03, bound:  3.15577626e-01\n",
      "Epoch: 19017 mean train loss:  6.99135615e-03, bound:  3.15577626e-01\n",
      "Epoch: 19018 mean train loss:  6.99076522e-03, bound:  3.15577596e-01\n",
      "Epoch: 19019 mean train loss:  6.99032750e-03, bound:  3.15577567e-01\n",
      "Epoch: 19020 mean train loss:  6.98988512e-03, bound:  3.15577507e-01\n",
      "Epoch: 19021 mean train loss:  6.98923785e-03, bound:  3.15577477e-01\n",
      "Epoch: 19022 mean train loss:  6.98867906e-03, bound:  3.15577477e-01\n",
      "Epoch: 19023 mean train loss:  6.98794145e-03, bound:  3.15577388e-01\n",
      "Epoch: 19024 mean train loss:  6.98721316e-03, bound:  3.15577388e-01\n",
      "Epoch: 19025 mean train loss:  6.98632421e-03, bound:  3.15577358e-01\n",
      "Epoch: 19026 mean train loss:  6.98550325e-03, bound:  3.15577358e-01\n",
      "Epoch: 19027 mean train loss:  6.98458496e-03, bound:  3.15577269e-01\n",
      "Epoch: 19028 mean train loss:  6.98388554e-03, bound:  3.15577239e-01\n",
      "Epoch: 19029 mean train loss:  6.98308554e-03, bound:  3.15577239e-01\n",
      "Epoch: 19030 mean train loss:  6.98235445e-03, bound:  3.15577179e-01\n",
      "Epoch: 19031 mean train loss:  6.98164478e-03, bound:  3.15577149e-01\n",
      "Epoch: 19032 mean train loss:  6.98108599e-03, bound:  3.15577120e-01\n",
      "Epoch: 19033 mean train loss:  6.98039122e-03, bound:  3.15577060e-01\n",
      "Epoch: 19034 mean train loss:  6.97980402e-03, bound:  3.15577030e-01\n",
      "Epoch: 19035 mean train loss:  6.97912043e-03, bound:  3.15577030e-01\n",
      "Epoch: 19036 mean train loss:  6.97842706e-03, bound:  3.15577000e-01\n",
      "Epoch: 19037 mean train loss:  6.97771460e-03, bound:  3.15576941e-01\n",
      "Epoch: 19038 mean train loss:  6.97693322e-03, bound:  3.15576911e-01\n",
      "Epoch: 19039 mean train loss:  6.97625475e-03, bound:  3.15576851e-01\n",
      "Epoch: 19040 mean train loss:  6.97544683e-03, bound:  3.15576822e-01\n",
      "Epoch: 19041 mean train loss:  6.97485544e-03, bound:  3.15576792e-01\n",
      "Epoch: 19042 mean train loss:  6.97412295e-03, bound:  3.15576732e-01\n",
      "Epoch: 19043 mean train loss:  6.97342120e-03, bound:  3.15576702e-01\n",
      "Epoch: 19044 mean train loss:  6.97279442e-03, bound:  3.15576673e-01\n",
      "Epoch: 19045 mean train loss:  6.97204191e-03, bound:  3.15576673e-01\n",
      "Epoch: 19046 mean train loss:  6.97130337e-03, bound:  3.15576613e-01\n",
      "Epoch: 19047 mean train loss:  6.97071850e-03, bound:  3.15576583e-01\n",
      "Epoch: 19048 mean train loss:  6.97000697e-03, bound:  3.15576553e-01\n",
      "Epoch: 19049 mean train loss:  6.96925027e-03, bound:  3.15576524e-01\n",
      "Epoch: 19050 mean train loss:  6.96866680e-03, bound:  3.15576494e-01\n",
      "Epoch: 19051 mean train loss:  6.96800370e-03, bound:  3.15576434e-01\n",
      "Epoch: 19052 mean train loss:  6.96734758e-03, bound:  3.15576404e-01\n",
      "Epoch: 19053 mean train loss:  6.96659507e-03, bound:  3.15576375e-01\n",
      "Epoch: 19054 mean train loss:  6.96587656e-03, bound:  3.15576375e-01\n",
      "Epoch: 19055 mean train loss:  6.96525257e-03, bound:  3.15576285e-01\n",
      "Epoch: 19056 mean train loss:  6.96454104e-03, bound:  3.15576285e-01\n",
      "Epoch: 19057 mean train loss:  6.96390634e-03, bound:  3.15576255e-01\n",
      "Epoch: 19058 mean train loss:  6.96317572e-03, bound:  3.15576226e-01\n",
      "Epoch: 19059 mean train loss:  6.96241530e-03, bound:  3.15576166e-01\n",
      "Epoch: 19060 mean train loss:  6.96177548e-03, bound:  3.15576136e-01\n",
      "Epoch: 19061 mean train loss:  6.96104905e-03, bound:  3.15576106e-01\n",
      "Epoch: 19062 mean train loss:  6.96032867e-03, bound:  3.15576077e-01\n",
      "Epoch: 19063 mean train loss:  6.95966929e-03, bound:  3.15576017e-01\n",
      "Epoch: 19064 mean train loss:  6.95892470e-03, bound:  3.15576017e-01\n",
      "Epoch: 19065 mean train loss:  6.95830863e-03, bound:  3.15575927e-01\n",
      "Epoch: 19066 mean train loss:  6.95768418e-03, bound:  3.15575927e-01\n",
      "Epoch: 19067 mean train loss:  6.95692934e-03, bound:  3.15575927e-01\n",
      "Epoch: 19068 mean train loss:  6.95621781e-03, bound:  3.15575868e-01\n",
      "Epoch: 19069 mean train loss:  6.95561944e-03, bound:  3.15575808e-01\n",
      "Epoch: 19070 mean train loss:  6.95487158e-03, bound:  3.15575808e-01\n",
      "Epoch: 19071 mean train loss:  6.95416378e-03, bound:  3.15575719e-01\n",
      "Epoch: 19072 mean train loss:  6.95348019e-03, bound:  3.15575719e-01\n",
      "Epoch: 19073 mean train loss:  6.95283385e-03, bound:  3.15575689e-01\n",
      "Epoch: 19074 mean train loss:  6.95216842e-03, bound:  3.15575689e-01\n",
      "Epoch: 19075 mean train loss:  6.95149833e-03, bound:  3.15575600e-01\n",
      "Epoch: 19076 mean train loss:  6.95086783e-03, bound:  3.15575600e-01\n",
      "Epoch: 19077 mean train loss:  6.95018191e-03, bound:  3.15575570e-01\n",
      "Epoch: 19078 mean train loss:  6.94960961e-03, bound:  3.15575540e-01\n",
      "Epoch: 19079 mean train loss:  6.94906292e-03, bound:  3.15575451e-01\n",
      "Epoch: 19080 mean train loss:  6.94856467e-03, bound:  3.15575451e-01\n",
      "Epoch: 19081 mean train loss:  6.94812695e-03, bound:  3.15575421e-01\n",
      "Epoch: 19082 mean train loss:  6.94772368e-03, bound:  3.15575421e-01\n",
      "Epoch: 19083 mean train loss:  6.94756722e-03, bound:  3.15575331e-01\n",
      "Epoch: 19084 mean train loss:  6.94770692e-03, bound:  3.15575331e-01\n",
      "Epoch: 19085 mean train loss:  6.94787083e-03, bound:  3.15575242e-01\n",
      "Epoch: 19086 mean train loss:  6.94843242e-03, bound:  3.15575272e-01\n",
      "Epoch: 19087 mean train loss:  6.94940658e-03, bound:  3.15575153e-01\n",
      "Epoch: 19088 mean train loss:  6.95086364e-03, bound:  3.15575242e-01\n",
      "Epoch: 19089 mean train loss:  6.95259310e-03, bound:  3.15575123e-01\n",
      "Epoch: 19090 mean train loss:  6.95442129e-03, bound:  3.15575123e-01\n",
      "Epoch: 19091 mean train loss:  6.95581036e-03, bound:  3.15575004e-01\n",
      "Epoch: 19092 mean train loss:  6.95610419e-03, bound:  3.15575123e-01\n",
      "Epoch: 19093 mean train loss:  6.95446460e-03, bound:  3.15574944e-01\n",
      "Epoch: 19094 mean train loss:  6.95082825e-03, bound:  3.15575004e-01\n",
      "Epoch: 19095 mean train loss:  6.94572367e-03, bound:  3.15574914e-01\n",
      "Epoch: 19096 mean train loss:  6.94069779e-03, bound:  3.15574914e-01\n",
      "Epoch: 19097 mean train loss:  6.93697715e-03, bound:  3.15574884e-01\n",
      "Epoch: 19098 mean train loss:  6.93546981e-03, bound:  3.15574825e-01\n",
      "Epoch: 19099 mean train loss:  6.93588005e-03, bound:  3.15574825e-01\n",
      "Epoch: 19100 mean train loss:  6.93727983e-03, bound:  3.15574765e-01\n",
      "Epoch: 19101 mean train loss:  6.93864003e-03, bound:  3.15574765e-01\n",
      "Epoch: 19102 mean train loss:  6.93886122e-03, bound:  3.15574676e-01\n",
      "Epoch: 19103 mean train loss:  6.93772640e-03, bound:  3.15574706e-01\n",
      "Epoch: 19104 mean train loss:  6.93536643e-03, bound:  3.15574646e-01\n",
      "Epoch: 19105 mean train loss:  6.93273870e-03, bound:  3.15574646e-01\n",
      "Epoch: 19106 mean train loss:  6.93057664e-03, bound:  3.15574586e-01\n",
      "Epoch: 19107 mean train loss:  6.92939386e-03, bound:  3.15574557e-01\n",
      "Epoch: 19108 mean train loss:  6.92922575e-03, bound:  3.15574497e-01\n",
      "Epoch: 19109 mean train loss:  6.92953682e-03, bound:  3.15574467e-01\n",
      "Epoch: 19110 mean train loss:  6.92975149e-03, bound:  3.15574467e-01\n",
      "Epoch: 19111 mean train loss:  6.92929793e-03, bound:  3.15574378e-01\n",
      "Epoch: 19112 mean train loss:  6.92826975e-03, bound:  3.15574378e-01\n",
      "Epoch: 19113 mean train loss:  6.92679826e-03, bound:  3.15574318e-01\n",
      "Epoch: 19114 mean train loss:  6.92526391e-03, bound:  3.15574318e-01\n",
      "Epoch: 19115 mean train loss:  6.92407275e-03, bound:  3.15574318e-01\n",
      "Epoch: 19116 mean train loss:  6.92341430e-03, bound:  3.15574259e-01\n",
      "Epoch: 19117 mean train loss:  6.92309998e-03, bound:  3.15574229e-01\n",
      "Epoch: 19118 mean train loss:  6.92295469e-03, bound:  3.15574169e-01\n",
      "Epoch: 19119 mean train loss:  6.92248018e-03, bound:  3.15574169e-01\n",
      "Epoch: 19120 mean train loss:  6.92182546e-03, bound:  3.15574110e-01\n",
      "Epoch: 19121 mean train loss:  6.92089926e-03, bound:  3.15574110e-01\n",
      "Epoch: 19122 mean train loss:  6.91992557e-03, bound:  3.15574020e-01\n",
      "Epoch: 19123 mean train loss:  6.91889646e-03, bound:  3.15574020e-01\n",
      "Epoch: 19124 mean train loss:  6.91812159e-03, bound:  3.15573990e-01\n",
      "Epoch: 19125 mean train loss:  6.91738166e-03, bound:  3.15573931e-01\n",
      "Epoch: 19126 mean train loss:  6.91692717e-03, bound:  3.15573901e-01\n",
      "Epoch: 19127 mean train loss:  6.91625709e-03, bound:  3.15573871e-01\n",
      "Epoch: 19128 mean train loss:  6.91574207e-03, bound:  3.15573871e-01\n",
      "Epoch: 19129 mean train loss:  6.91511808e-03, bound:  3.15573812e-01\n",
      "Epoch: 19130 mean train loss:  6.91443123e-03, bound:  3.15573812e-01\n",
      "Epoch: 19131 mean train loss:  6.91369036e-03, bound:  3.15573752e-01\n",
      "Epoch: 19132 mean train loss:  6.91288337e-03, bound:  3.15573692e-01\n",
      "Epoch: 19133 mean train loss:  6.91207731e-03, bound:  3.15573692e-01\n",
      "Epoch: 19134 mean train loss:  6.91134017e-03, bound:  3.15573663e-01\n",
      "Epoch: 19135 mean train loss:  6.91072783e-03, bound:  3.15573603e-01\n",
      "Epoch: 19136 mean train loss:  6.91000139e-03, bound:  3.15573573e-01\n",
      "Epoch: 19137 mean train loss:  6.90945797e-03, bound:  3.15573573e-01\n",
      "Epoch: 19138 mean train loss:  6.90883864e-03, bound:  3.15573484e-01\n",
      "Epoch: 19139 mean train loss:  6.90809963e-03, bound:  3.15573484e-01\n",
      "Epoch: 19140 mean train loss:  6.90749800e-03, bound:  3.15573454e-01\n",
      "Epoch: 19141 mean train loss:  6.90678740e-03, bound:  3.15573424e-01\n",
      "Epoch: 19142 mean train loss:  6.90611266e-03, bound:  3.15573364e-01\n",
      "Epoch: 19143 mean train loss:  6.90541929e-03, bound:  3.15573335e-01\n",
      "Epoch: 19144 mean train loss:  6.90471288e-03, bound:  3.15573305e-01\n",
      "Epoch: 19145 mean train loss:  6.90411543e-03, bound:  3.15573275e-01\n",
      "Epoch: 19146 mean train loss:  6.90333638e-03, bound:  3.15573245e-01\n",
      "Epoch: 19147 mean train loss:  6.90274686e-03, bound:  3.15573215e-01\n",
      "Epoch: 19148 mean train loss:  6.90212473e-03, bound:  3.15573186e-01\n",
      "Epoch: 19149 mean train loss:  6.90146582e-03, bound:  3.15573156e-01\n",
      "Epoch: 19150 mean train loss:  6.90076407e-03, bound:  3.15573126e-01\n",
      "Epoch: 19151 mean train loss:  6.90003624e-03, bound:  3.15573096e-01\n",
      "Epoch: 19152 mean train loss:  6.89940946e-03, bound:  3.15573066e-01\n",
      "Epoch: 19153 mean train loss:  6.89873472e-03, bound:  3.15573007e-01\n",
      "Epoch: 19154 mean train loss:  6.89805159e-03, bound:  3.15573007e-01\n",
      "Epoch: 19155 mean train loss:  6.89729489e-03, bound:  3.15572917e-01\n",
      "Epoch: 19156 mean train loss:  6.89663365e-03, bound:  3.15572917e-01\n",
      "Epoch: 19157 mean train loss:  6.89587416e-03, bound:  3.15572888e-01\n",
      "Epoch: 19158 mean train loss:  6.89528044e-03, bound:  3.15572858e-01\n",
      "Epoch: 19159 mean train loss:  6.89458614e-03, bound:  3.15572798e-01\n",
      "Epoch: 19160 mean train loss:  6.89392351e-03, bound:  3.15572768e-01\n",
      "Epoch: 19161 mean train loss:  6.89328276e-03, bound:  3.15572768e-01\n",
      "Epoch: 19162 mean train loss:  6.89261360e-03, bound:  3.15572709e-01\n",
      "Epoch: 19163 mean train loss:  6.89195236e-03, bound:  3.15572679e-01\n",
      "Epoch: 19164 mean train loss:  6.89121243e-03, bound:  3.15572649e-01\n",
      "Epoch: 19165 mean train loss:  6.89064711e-03, bound:  3.15572590e-01\n",
      "Epoch: 19166 mean train loss:  6.88992580e-03, bound:  3.15572560e-01\n",
      "Epoch: 19167 mean train loss:  6.88931299e-03, bound:  3.15572560e-01\n",
      "Epoch: 19168 mean train loss:  6.88851392e-03, bound:  3.15572470e-01\n",
      "Epoch: 19169 mean train loss:  6.88790577e-03, bound:  3.15572470e-01\n",
      "Epoch: 19170 mean train loss:  6.88717607e-03, bound:  3.15572470e-01\n",
      "Epoch: 19171 mean train loss:  6.88661216e-03, bound:  3.15572441e-01\n",
      "Epoch: 19172 mean train loss:  6.88581681e-03, bound:  3.15572351e-01\n",
      "Epoch: 19173 mean train loss:  6.88515045e-03, bound:  3.15572351e-01\n",
      "Epoch: 19174 mean train loss:  6.88454043e-03, bound:  3.15572321e-01\n",
      "Epoch: 19175 mean train loss:  6.88390154e-03, bound:  3.15572262e-01\n",
      "Epoch: 19176 mean train loss:  6.88314578e-03, bound:  3.15572262e-01\n",
      "Epoch: 19177 mean train loss:  6.88248873e-03, bound:  3.15572202e-01\n",
      "Epoch: 19178 mean train loss:  6.88180141e-03, bound:  3.15572202e-01\n",
      "Epoch: 19179 mean train loss:  6.88115181e-03, bound:  3.15572143e-01\n",
      "Epoch: 19180 mean train loss:  6.88045425e-03, bound:  3.15572113e-01\n",
      "Epoch: 19181 mean train loss:  6.87981024e-03, bound:  3.15572083e-01\n",
      "Epoch: 19182 mean train loss:  6.87914481e-03, bound:  3.15572023e-01\n",
      "Epoch: 19183 mean train loss:  6.87851012e-03, bound:  3.15571994e-01\n",
      "Epoch: 19184 mean train loss:  6.87775528e-03, bound:  3.15571994e-01\n",
      "Epoch: 19185 mean train loss:  6.87710615e-03, bound:  3.15571964e-01\n",
      "Epoch: 19186 mean train loss:  6.87639741e-03, bound:  3.15571904e-01\n",
      "Epoch: 19187 mean train loss:  6.87576737e-03, bound:  3.15571874e-01\n",
      "Epoch: 19188 mean train loss:  6.87504839e-03, bound:  3.15571874e-01\n",
      "Epoch: 19189 mean train loss:  6.87439600e-03, bound:  3.15571785e-01\n",
      "Epoch: 19190 mean train loss:  6.87379064e-03, bound:  3.15571755e-01\n",
      "Epoch: 19191 mean train loss:  6.87314849e-03, bound:  3.15571755e-01\n",
      "Epoch: 19192 mean train loss:  6.87237550e-03, bound:  3.15571696e-01\n",
      "Epoch: 19193 mean train loss:  6.87176222e-03, bound:  3.15571666e-01\n",
      "Epoch: 19194 mean train loss:  6.87106187e-03, bound:  3.15571636e-01\n",
      "Epoch: 19195 mean train loss:  6.87046442e-03, bound:  3.15571576e-01\n",
      "Epoch: 19196 mean train loss:  6.86975848e-03, bound:  3.15571576e-01\n",
      "Epoch: 19197 mean train loss:  6.86906464e-03, bound:  3.15571547e-01\n",
      "Epoch: 19198 mean train loss:  6.86837034e-03, bound:  3.15571487e-01\n",
      "Epoch: 19199 mean train loss:  6.86771376e-03, bound:  3.15571457e-01\n",
      "Epoch: 19200 mean train loss:  6.86705299e-03, bound:  3.15571427e-01\n",
      "Epoch: 19201 mean train loss:  6.86640013e-03, bound:  3.15571427e-01\n",
      "Epoch: 19202 mean train loss:  6.86580781e-03, bound:  3.15571338e-01\n",
      "Epoch: 19203 mean train loss:  6.86512701e-03, bound:  3.15571338e-01\n",
      "Epoch: 19204 mean train loss:  6.86452258e-03, bound:  3.15571308e-01\n",
      "Epoch: 19205 mean train loss:  6.86392887e-03, bound:  3.15571249e-01\n",
      "Epoch: 19206 mean train loss:  6.86336029e-03, bound:  3.15571219e-01\n",
      "Epoch: 19207 mean train loss:  6.86290534e-03, bound:  3.15571219e-01\n",
      "Epoch: 19208 mean train loss:  6.86250720e-03, bound:  3.15571129e-01\n",
      "Epoch: 19209 mean train loss:  6.86226087e-03, bound:  3.15571129e-01\n",
      "Epoch: 19210 mean train loss:  6.86207972e-03, bound:  3.15571070e-01\n",
      "Epoch: 19211 mean train loss:  6.86207041e-03, bound:  3.15571100e-01\n",
      "Epoch: 19212 mean train loss:  6.86231209e-03, bound:  3.15571010e-01\n",
      "Epoch: 19213 mean train loss:  6.86281361e-03, bound:  3.15571010e-01\n",
      "Epoch: 19214 mean train loss:  6.86370907e-03, bound:  3.15570951e-01\n",
      "Epoch: 19215 mean train loss:  6.86495937e-03, bound:  3.15570951e-01\n",
      "Epoch: 19216 mean train loss:  6.86645973e-03, bound:  3.15570861e-01\n",
      "Epoch: 19217 mean train loss:  6.86802343e-03, bound:  3.15570891e-01\n",
      "Epoch: 19218 mean train loss:  6.86923880e-03, bound:  3.15570772e-01\n",
      "Epoch: 19219 mean train loss:  6.86951308e-03, bound:  3.15570861e-01\n",
      "Epoch: 19220 mean train loss:  6.86833356e-03, bound:  3.15570712e-01\n",
      "Epoch: 19221 mean train loss:  6.86532911e-03, bound:  3.15570772e-01\n",
      "Epoch: 19222 mean train loss:  6.86114095e-03, bound:  3.15570652e-01\n",
      "Epoch: 19223 mean train loss:  6.85636979e-03, bound:  3.15570682e-01\n",
      "Epoch: 19224 mean train loss:  6.85248012e-03, bound:  3.15570652e-01\n",
      "Epoch: 19225 mean train loss:  6.85038231e-03, bound:  3.15570593e-01\n",
      "Epoch: 19226 mean train loss:  6.85003027e-03, bound:  3.15570593e-01\n",
      "Epoch: 19227 mean train loss:  6.85089547e-03, bound:  3.15570533e-01\n",
      "Epoch: 19228 mean train loss:  6.85202330e-03, bound:  3.15570533e-01\n",
      "Epoch: 19229 mean train loss:  6.85243215e-03, bound:  3.15570444e-01\n",
      "Epoch: 19230 mean train loss:  6.85183331e-03, bound:  3.15570474e-01\n",
      "Epoch: 19231 mean train loss:  6.85004098e-03, bound:  3.15570384e-01\n",
      "Epoch: 19232 mean train loss:  6.84779836e-03, bound:  3.15570414e-01\n",
      "Epoch: 19233 mean train loss:  6.84586307e-03, bound:  3.15570325e-01\n",
      "Epoch: 19234 mean train loss:  6.84445770e-03, bound:  3.15570325e-01\n",
      "Epoch: 19235 mean train loss:  6.84381090e-03, bound:  3.15570295e-01\n",
      "Epoch: 19236 mean train loss:  6.84382813e-03, bound:  3.15570235e-01\n",
      "Epoch: 19237 mean train loss:  6.84379274e-03, bound:  3.15570235e-01\n",
      "Epoch: 19238 mean train loss:  6.84366608e-03, bound:  3.15570176e-01\n",
      "Epoch: 19239 mean train loss:  6.84302114e-03, bound:  3.15570176e-01\n",
      "Epoch: 19240 mean train loss:  6.84190402e-03, bound:  3.15570086e-01\n",
      "Epoch: 19241 mean train loss:  6.84072031e-03, bound:  3.15570086e-01\n",
      "Epoch: 19242 mean train loss:  6.83949934e-03, bound:  3.15570056e-01\n",
      "Epoch: 19243 mean train loss:  6.83862064e-03, bound:  3.15570027e-01\n",
      "Epoch: 19244 mean train loss:  6.83787744e-03, bound:  3.15569967e-01\n",
      "Epoch: 19245 mean train loss:  6.83736755e-03, bound:  3.15569907e-01\n",
      "Epoch: 19246 mean train loss:  6.83705043e-03, bound:  3.15569907e-01\n",
      "Epoch: 19247 mean train loss:  6.83647813e-03, bound:  3.15569878e-01\n",
      "Epoch: 19248 mean train loss:  6.83586905e-03, bound:  3.15569878e-01\n",
      "Epoch: 19249 mean train loss:  6.83512772e-03, bound:  3.15569848e-01\n",
      "Epoch: 19250 mean train loss:  6.83417125e-03, bound:  3.15569788e-01\n",
      "Epoch: 19251 mean train loss:  6.83335168e-03, bound:  3.15569758e-01\n",
      "Epoch: 19252 mean train loss:  6.83265040e-03, bound:  3.15569758e-01\n",
      "Epoch: 19253 mean train loss:  6.83198357e-03, bound:  3.15569729e-01\n",
      "Epoch: 19254 mean train loss:  6.83138706e-03, bound:  3.15569669e-01\n",
      "Epoch: 19255 mean train loss:  6.83080452e-03, bound:  3.15569639e-01\n",
      "Epoch: 19256 mean train loss:  6.83026761e-03, bound:  3.15569609e-01\n",
      "Epoch: 19257 mean train loss:  6.82958681e-03, bound:  3.15569550e-01\n",
      "Epoch: 19258 mean train loss:  6.82897819e-03, bound:  3.15569520e-01\n",
      "Epoch: 19259 mean train loss:  6.82827923e-03, bound:  3.15569520e-01\n",
      "Epoch: 19260 mean train loss:  6.82752160e-03, bound:  3.15569460e-01\n",
      "Epoch: 19261 mean train loss:  6.82681194e-03, bound:  3.15569431e-01\n",
      "Epoch: 19262 mean train loss:  6.82601612e-03, bound:  3.15569401e-01\n",
      "Epoch: 19263 mean train loss:  6.82543358e-03, bound:  3.15569341e-01\n",
      "Epoch: 19264 mean train loss:  6.82475790e-03, bound:  3.15569311e-01\n",
      "Epoch: 19265 mean train loss:  6.82419678e-03, bound:  3.15569282e-01\n",
      "Epoch: 19266 mean train loss:  6.82353042e-03, bound:  3.15569282e-01\n",
      "Epoch: 19267 mean train loss:  6.82292646e-03, bound:  3.15569222e-01\n",
      "Epoch: 19268 mean train loss:  6.82227779e-03, bound:  3.15569222e-01\n",
      "Epoch: 19269 mean train loss:  6.82165427e-03, bound:  3.15569192e-01\n",
      "Epoch: 19270 mean train loss:  6.82102516e-03, bound:  3.15569103e-01\n",
      "Epoch: 19271 mean train loss:  6.82029361e-03, bound:  3.15569103e-01\n",
      "Epoch: 19272 mean train loss:  6.81961840e-03, bound:  3.15569073e-01\n",
      "Epoch: 19273 mean train loss:  6.81887334e-03, bound:  3.15569013e-01\n",
      "Epoch: 19274 mean train loss:  6.81825215e-03, bound:  3.15568984e-01\n",
      "Epoch: 19275 mean train loss:  6.81756018e-03, bound:  3.15568954e-01\n",
      "Epoch: 19276 mean train loss:  6.81691198e-03, bound:  3.15568954e-01\n",
      "Epoch: 19277 mean train loss:  6.81625819e-03, bound:  3.15568894e-01\n",
      "Epoch: 19278 mean train loss:  6.81559136e-03, bound:  3.15568864e-01\n",
      "Epoch: 19279 mean train loss:  6.81496039e-03, bound:  3.15568835e-01\n",
      "Epoch: 19280 mean train loss:  6.81426516e-03, bound:  3.15568835e-01\n",
      "Epoch: 19281 mean train loss:  6.81368774e-03, bound:  3.15568775e-01\n",
      "Epoch: 19282 mean train loss:  6.81299344e-03, bound:  3.15568745e-01\n",
      "Epoch: 19283 mean train loss:  6.81230100e-03, bound:  3.15568715e-01\n",
      "Epoch: 19284 mean train loss:  6.81173615e-03, bound:  3.15568656e-01\n",
      "Epoch: 19285 mean train loss:  6.81101950e-03, bound:  3.15568626e-01\n",
      "Epoch: 19286 mean train loss:  6.81037037e-03, bound:  3.15568626e-01\n",
      "Epoch: 19287 mean train loss:  6.80973101e-03, bound:  3.15568566e-01\n",
      "Epoch: 19288 mean train loss:  6.80906232e-03, bound:  3.15568537e-01\n",
      "Epoch: 19289 mean train loss:  6.80839689e-03, bound:  3.15568507e-01\n",
      "Epoch: 19290 mean train loss:  6.80773612e-03, bound:  3.15568507e-01\n",
      "Epoch: 19291 mean train loss:  6.80708606e-03, bound:  3.15568417e-01\n",
      "Epoch: 19292 mean train loss:  6.80641644e-03, bound:  3.15568417e-01\n",
      "Epoch: 19293 mean train loss:  6.80578919e-03, bound:  3.15568388e-01\n",
      "Epoch: 19294 mean train loss:  6.80515822e-03, bound:  3.15568358e-01\n",
      "Epoch: 19295 mean train loss:  6.80456962e-03, bound:  3.15568328e-01\n",
      "Epoch: 19296 mean train loss:  6.80388696e-03, bound:  3.15568298e-01\n",
      "Epoch: 19297 mean train loss:  6.80332072e-03, bound:  3.15568238e-01\n",
      "Epoch: 19298 mean train loss:  6.80267531e-03, bound:  3.15568209e-01\n",
      "Epoch: 19299 mean train loss:  6.80200150e-03, bound:  3.15568179e-01\n",
      "Epoch: 19300 mean train loss:  6.80138823e-03, bound:  3.15568119e-01\n",
      "Epoch: 19301 mean train loss:  6.80072512e-03, bound:  3.15568119e-01\n",
      "Epoch: 19302 mean train loss:  6.79999404e-03, bound:  3.15568089e-01\n",
      "Epoch: 19303 mean train loss:  6.79941662e-03, bound:  3.15568060e-01\n",
      "Epoch: 19304 mean train loss:  6.79867901e-03, bound:  3.15568000e-01\n",
      "Epoch: 19305 mean train loss:  6.79799728e-03, bound:  3.15567970e-01\n",
      "Epoch: 19306 mean train loss:  6.79742359e-03, bound:  3.15567970e-01\n",
      "Epoch: 19307 mean train loss:  6.79681357e-03, bound:  3.15567911e-01\n",
      "Epoch: 19308 mean train loss:  6.79621287e-03, bound:  3.15567911e-01\n",
      "Epoch: 19309 mean train loss:  6.79546269e-03, bound:  3.15567851e-01\n",
      "Epoch: 19310 mean train loss:  6.79494254e-03, bound:  3.15567821e-01\n",
      "Epoch: 19311 mean train loss:  6.79417793e-03, bound:  3.15567791e-01\n",
      "Epoch: 19312 mean train loss:  6.79358700e-03, bound:  3.15567762e-01\n",
      "Epoch: 19313 mean train loss:  6.79291086e-03, bound:  3.15567702e-01\n",
      "Epoch: 19314 mean train loss:  6.79234043e-03, bound:  3.15567702e-01\n",
      "Epoch: 19315 mean train loss:  6.79163076e-03, bound:  3.15567642e-01\n",
      "Epoch: 19316 mean train loss:  6.79111760e-03, bound:  3.15567642e-01\n",
      "Epoch: 19317 mean train loss:  6.79043354e-03, bound:  3.15567553e-01\n",
      "Epoch: 19318 mean train loss:  6.78983843e-03, bound:  3.15567553e-01\n",
      "Epoch: 19319 mean train loss:  6.78921817e-03, bound:  3.15567523e-01\n",
      "Epoch: 19320 mean train loss:  6.78866403e-03, bound:  3.15567523e-01\n",
      "Epoch: 19321 mean train loss:  6.78814389e-03, bound:  3.15567434e-01\n",
      "Epoch: 19322 mean train loss:  6.78765215e-03, bound:  3.15567434e-01\n",
      "Epoch: 19323 mean train loss:  6.78721117e-03, bound:  3.15567404e-01\n",
      "Epoch: 19324 mean train loss:  6.78667612e-03, bound:  3.15567374e-01\n",
      "Epoch: 19325 mean train loss:  6.78637577e-03, bound:  3.15567315e-01\n",
      "Epoch: 19326 mean train loss:  6.78610709e-03, bound:  3.15567285e-01\n",
      "Epoch: 19327 mean train loss:  6.78590126e-03, bound:  3.15567255e-01\n",
      "Epoch: 19328 mean train loss:  6.78576110e-03, bound:  3.15567255e-01\n",
      "Epoch: 19329 mean train loss:  6.78557577e-03, bound:  3.15567166e-01\n",
      "Epoch: 19330 mean train loss:  6.78550638e-03, bound:  3.15567195e-01\n",
      "Epoch: 19331 mean train loss:  6.78545795e-03, bound:  3.15567076e-01\n",
      "Epoch: 19332 mean train loss:  6.78534433e-03, bound:  3.15567106e-01\n",
      "Epoch: 19333 mean train loss:  6.78513013e-03, bound:  3.15566987e-01\n",
      "Epoch: 19334 mean train loss:  6.78490195e-03, bound:  3.15567076e-01\n",
      "Epoch: 19335 mean train loss:  6.78424118e-03, bound:  3.15566987e-01\n",
      "Epoch: 19336 mean train loss:  6.78333268e-03, bound:  3.15566987e-01\n",
      "Epoch: 19337 mean train loss:  6.78216806e-03, bound:  3.15566868e-01\n",
      "Epoch: 19338 mean train loss:  6.78078644e-03, bound:  3.15566927e-01\n",
      "Epoch: 19339 mean train loss:  6.77927770e-03, bound:  3.15566838e-01\n",
      "Epoch: 19340 mean train loss:  6.77769724e-03, bound:  3.15566838e-01\n",
      "Epoch: 19341 mean train loss:  6.77606696e-03, bound:  3.15566778e-01\n",
      "Epoch: 19342 mean train loss:  6.77460013e-03, bound:  3.15566778e-01\n",
      "Epoch: 19343 mean train loss:  6.77338988e-03, bound:  3.15566748e-01\n",
      "Epoch: 19344 mean train loss:  6.77237101e-03, bound:  3.15566719e-01\n",
      "Epoch: 19345 mean train loss:  6.77165808e-03, bound:  3.15566659e-01\n",
      "Epoch: 19346 mean train loss:  6.77110208e-03, bound:  3.15566629e-01\n",
      "Epoch: 19347 mean train loss:  6.77056145e-03, bound:  3.15566599e-01\n",
      "Epoch: 19348 mean train loss:  6.77017542e-03, bound:  3.15566540e-01\n",
      "Epoch: 19349 mean train loss:  6.76972046e-03, bound:  3.15566540e-01\n",
      "Epoch: 19350 mean train loss:  6.76922919e-03, bound:  3.15566510e-01\n",
      "Epoch: 19351 mean train loss:  6.76879287e-03, bound:  3.15566510e-01\n",
      "Epoch: 19352 mean train loss:  6.76822662e-03, bound:  3.15566421e-01\n",
      "Epoch: 19353 mean train loss:  6.76755002e-03, bound:  3.15566421e-01\n",
      "Epoch: 19354 mean train loss:  6.76686596e-03, bound:  3.15566391e-01\n",
      "Epoch: 19355 mean train loss:  6.76607993e-03, bound:  3.15566391e-01\n",
      "Epoch: 19356 mean train loss:  6.76529249e-03, bound:  3.15566301e-01\n",
      "Epoch: 19357 mean train loss:  6.76454930e-03, bound:  3.15566272e-01\n",
      "Epoch: 19358 mean train loss:  6.76370738e-03, bound:  3.15566272e-01\n",
      "Epoch: 19359 mean train loss:  6.76287385e-03, bound:  3.15566212e-01\n",
      "Epoch: 19360 mean train loss:  6.76209992e-03, bound:  3.15566182e-01\n",
      "Epoch: 19361 mean train loss:  6.76133065e-03, bound:  3.15566152e-01\n",
      "Epoch: 19362 mean train loss:  6.76065497e-03, bound:  3.15566093e-01\n",
      "Epoch: 19363 mean train loss:  6.76004216e-03, bound:  3.15566093e-01\n",
      "Epoch: 19364 mean train loss:  6.75940793e-03, bound:  3.15566063e-01\n",
      "Epoch: 19365 mean train loss:  6.75872201e-03, bound:  3.15566003e-01\n",
      "Epoch: 19366 mean train loss:  6.75810501e-03, bound:  3.15565974e-01\n",
      "Epoch: 19367 mean train loss:  6.75750803e-03, bound:  3.15565974e-01\n",
      "Epoch: 19368 mean train loss:  6.75690453e-03, bound:  3.15565944e-01\n",
      "Epoch: 19369 mean train loss:  6.75631221e-03, bound:  3.15565854e-01\n",
      "Epoch: 19370 mean train loss:  6.75565097e-03, bound:  3.15565854e-01\n",
      "Epoch: 19371 mean train loss:  6.75506191e-03, bound:  3.15565825e-01\n",
      "Epoch: 19372 mean train loss:  6.75444258e-03, bound:  3.15565825e-01\n",
      "Epoch: 19373 mean train loss:  6.75368495e-03, bound:  3.15565735e-01\n",
      "Epoch: 19374 mean train loss:  6.75312895e-03, bound:  3.15565735e-01\n",
      "Epoch: 19375 mean train loss:  6.75245514e-03, bound:  3.15565675e-01\n",
      "Epoch: 19376 mean train loss:  6.75179763e-03, bound:  3.15565646e-01\n",
      "Epoch: 19377 mean train loss:  6.75114710e-03, bound:  3.15565616e-01\n",
      "Epoch: 19378 mean train loss:  6.75045000e-03, bound:  3.15565586e-01\n",
      "Epoch: 19379 mean train loss:  6.74976734e-03, bound:  3.15565556e-01\n",
      "Epoch: 19380 mean train loss:  6.74914755e-03, bound:  3.15565526e-01\n",
      "Epoch: 19381 mean train loss:  6.74847513e-03, bound:  3.15565497e-01\n",
      "Epoch: 19382 mean train loss:  6.74781762e-03, bound:  3.15565467e-01\n",
      "Epoch: 19383 mean train loss:  6.74722111e-03, bound:  3.15565407e-01\n",
      "Epoch: 19384 mean train loss:  6.74651936e-03, bound:  3.15565407e-01\n",
      "Epoch: 19385 mean train loss:  6.74598012e-03, bound:  3.15565377e-01\n",
      "Epoch: 19386 mean train loss:  6.74533844e-03, bound:  3.15565348e-01\n",
      "Epoch: 19387 mean train loss:  6.74472330e-03, bound:  3.15565288e-01\n",
      "Epoch: 19388 mean train loss:  6.74410723e-03, bound:  3.15565258e-01\n",
      "Epoch: 19389 mean train loss:  6.74347160e-03, bound:  3.15565228e-01\n",
      "Epoch: 19390 mean train loss:  6.74273120e-03, bound:  3.15565199e-01\n",
      "Epoch: 19391 mean train loss:  6.74222270e-03, bound:  3.15565169e-01\n",
      "Epoch: 19392 mean train loss:  6.74163364e-03, bound:  3.15565169e-01\n",
      "Epoch: 19393 mean train loss:  6.74099661e-03, bound:  3.15565079e-01\n",
      "Epoch: 19394 mean train loss:  6.74050162e-03, bound:  3.15565050e-01\n",
      "Epoch: 19395 mean train loss:  6.74004294e-03, bound:  3.15565050e-01\n",
      "Epoch: 19396 mean train loss:  6.73963781e-03, bound:  3.15565020e-01\n",
      "Epoch: 19397 mean train loss:  6.73933420e-03, bound:  3.15564960e-01\n",
      "Epoch: 19398 mean train loss:  6.73919218e-03, bound:  3.15564960e-01\n",
      "Epoch: 19399 mean train loss:  6.73911627e-03, bound:  3.15564871e-01\n",
      "Epoch: 19400 mean train loss:  6.73933979e-03, bound:  3.15564871e-01\n",
      "Epoch: 19401 mean train loss:  6.73978636e-03, bound:  3.15564841e-01\n",
      "Epoch: 19402 mean train loss:  6.74029253e-03, bound:  3.15564841e-01\n",
      "Epoch: 19403 mean train loss:  6.74134307e-03, bound:  3.15564752e-01\n",
      "Epoch: 19404 mean train loss:  6.74250163e-03, bound:  3.15564752e-01\n",
      "Epoch: 19405 mean train loss:  6.74367230e-03, bound:  3.15564662e-01\n",
      "Epoch: 19406 mean train loss:  6.74445508e-03, bound:  3.15564722e-01\n",
      "Epoch: 19407 mean train loss:  6.74438383e-03, bound:  3.15564603e-01\n",
      "Epoch: 19408 mean train loss:  6.74337614e-03, bound:  3.15564662e-01\n",
      "Epoch: 19409 mean train loss:  6.74097659e-03, bound:  3.15564543e-01\n",
      "Epoch: 19410 mean train loss:  6.73733326e-03, bound:  3.15564603e-01\n",
      "Epoch: 19411 mean train loss:  6.73353113e-03, bound:  3.15564483e-01\n",
      "Epoch: 19412 mean train loss:  6.73015881e-03, bound:  3.15564513e-01\n",
      "Epoch: 19413 mean train loss:  6.72788592e-03, bound:  3.15564424e-01\n",
      "Epoch: 19414 mean train loss:  6.72708638e-03, bound:  3.15564424e-01\n",
      "Epoch: 19415 mean train loss:  6.72725681e-03, bound:  3.15564424e-01\n",
      "Epoch: 19416 mean train loss:  6.72804471e-03, bound:  3.15564364e-01\n",
      "Epoch: 19417 mean train loss:  6.72870036e-03, bound:  3.15564364e-01\n",
      "Epoch: 19418 mean train loss:  6.72885729e-03, bound:  3.15564275e-01\n",
      "Epoch: 19419 mean train loss:  6.72827428e-03, bound:  3.15564305e-01\n",
      "Epoch: 19420 mean train loss:  6.72686193e-03, bound:  3.15564185e-01\n",
      "Epoch: 19421 mean train loss:  6.72497414e-03, bound:  3.15564245e-01\n",
      "Epoch: 19422 mean train loss:  6.72298716e-03, bound:  3.15564185e-01\n",
      "Epoch: 19423 mean train loss:  6.72151893e-03, bound:  3.15564156e-01\n",
      "Epoch: 19424 mean train loss:  6.72070123e-03, bound:  3.15564096e-01\n",
      "Epoch: 19425 mean train loss:  6.72034780e-03, bound:  3.15564066e-01\n",
      "Epoch: 19426 mean train loss:  6.72027003e-03, bound:  3.15564066e-01\n",
      "Epoch: 19427 mean train loss:  6.72012614e-03, bound:  3.15564036e-01\n",
      "Epoch: 19428 mean train loss:  6.71967259e-03, bound:  3.15563977e-01\n",
      "Epoch: 19429 mean train loss:  6.71898434e-03, bound:  3.15563947e-01\n",
      "Epoch: 19430 mean train loss:  6.71806745e-03, bound:  3.15563947e-01\n",
      "Epoch: 19431 mean train loss:  6.71698293e-03, bound:  3.15563858e-01\n",
      "Epoch: 19432 mean train loss:  6.71601342e-03, bound:  3.15563858e-01\n",
      "Epoch: 19433 mean train loss:  6.71512494e-03, bound:  3.15563828e-01\n",
      "Epoch: 19434 mean train loss:  6.71437057e-03, bound:  3.15563798e-01\n",
      "Epoch: 19435 mean train loss:  6.71389187e-03, bound:  3.15563738e-01\n",
      "Epoch: 19436 mean train loss:  6.71347464e-03, bound:  3.15563738e-01\n",
      "Epoch: 19437 mean train loss:  6.71306020e-03, bound:  3.15563738e-01\n",
      "Epoch: 19438 mean train loss:  6.71245251e-03, bound:  3.15563619e-01\n",
      "Epoch: 19439 mean train loss:  6.71188813e-03, bound:  3.15563619e-01\n",
      "Epoch: 19440 mean train loss:  6.71117194e-03, bound:  3.15563589e-01\n",
      "Epoch: 19441 mean train loss:  6.71034586e-03, bound:  3.15563589e-01\n",
      "Epoch: 19442 mean train loss:  6.70955377e-03, bound:  3.15563530e-01\n",
      "Epoch: 19443 mean train loss:  6.70877006e-03, bound:  3.15563500e-01\n",
      "Epoch: 19444 mean train loss:  6.70812605e-03, bound:  3.15563500e-01\n",
      "Epoch: 19445 mean train loss:  6.70749974e-03, bound:  3.15563470e-01\n",
      "Epoch: 19446 mean train loss:  6.70687528e-03, bound:  3.15563411e-01\n",
      "Epoch: 19447 mean train loss:  6.70632115e-03, bound:  3.15563381e-01\n",
      "Epoch: 19448 mean train loss:  6.70576002e-03, bound:  3.15563351e-01\n",
      "Epoch: 19449 mean train loss:  6.70519006e-03, bound:  3.15563321e-01\n",
      "Epoch: 19450 mean train loss:  6.70450833e-03, bound:  3.15563291e-01\n",
      "Epoch: 19451 mean train loss:  6.70387410e-03, bound:  3.15563262e-01\n",
      "Epoch: 19452 mean train loss:  6.70314394e-03, bound:  3.15563232e-01\n",
      "Epoch: 19453 mean train loss:  6.70246407e-03, bound:  3.15563202e-01\n",
      "Epoch: 19454 mean train loss:  6.70194020e-03, bound:  3.15563172e-01\n",
      "Epoch: 19455 mean train loss:  6.70121610e-03, bound:  3.15563142e-01\n",
      "Epoch: 19456 mean train loss:  6.70049340e-03, bound:  3.15563083e-01\n",
      "Epoch: 19457 mean train loss:  6.69991551e-03, bound:  3.15563053e-01\n",
      "Epoch: 19458 mean train loss:  6.69926312e-03, bound:  3.15563053e-01\n",
      "Epoch: 19459 mean train loss:  6.69867080e-03, bound:  3.15563023e-01\n",
      "Epoch: 19460 mean train loss:  6.69803191e-03, bound:  3.15562963e-01\n",
      "Epoch: 19461 mean train loss:  6.69736415e-03, bound:  3.15562934e-01\n",
      "Epoch: 19462 mean train loss:  6.69678720e-03, bound:  3.15562904e-01\n",
      "Epoch: 19463 mean train loss:  6.69620605e-03, bound:  3.15562874e-01\n",
      "Epoch: 19464 mean train loss:  6.69555832e-03, bound:  3.15562844e-01\n",
      "Epoch: 19465 mean train loss:  6.69483608e-03, bound:  3.15562844e-01\n",
      "Epoch: 19466 mean train loss:  6.69423444e-03, bound:  3.15562785e-01\n",
      "Epoch: 19467 mean train loss:  6.69362582e-03, bound:  3.15562725e-01\n",
      "Epoch: 19468 mean train loss:  6.69300649e-03, bound:  3.15562725e-01\n",
      "Epoch: 19469 mean train loss:  6.69237319e-03, bound:  3.15562725e-01\n",
      "Epoch: 19470 mean train loss:  6.69162255e-03, bound:  3.15562665e-01\n",
      "Epoch: 19471 mean train loss:  6.69104420e-03, bound:  3.15562606e-01\n",
      "Epoch: 19472 mean train loss:  6.69041742e-03, bound:  3.15562606e-01\n",
      "Epoch: 19473 mean train loss:  6.68975897e-03, bound:  3.15562546e-01\n",
      "Epoch: 19474 mean train loss:  6.68920856e-03, bound:  3.15562516e-01\n",
      "Epoch: 19475 mean train loss:  6.68854313e-03, bound:  3.15562487e-01\n",
      "Epoch: 19476 mean train loss:  6.68788888e-03, bound:  3.15562487e-01\n",
      "Epoch: 19477 mean train loss:  6.68721320e-03, bound:  3.15562427e-01\n",
      "Epoch: 19478 mean train loss:  6.68657664e-03, bound:  3.15562397e-01\n",
      "Epoch: 19479 mean train loss:  6.68598386e-03, bound:  3.15562367e-01\n",
      "Epoch: 19480 mean train loss:  6.68536406e-03, bound:  3.15562338e-01\n",
      "Epoch: 19481 mean train loss:  6.68471865e-03, bound:  3.15562308e-01\n",
      "Epoch: 19482 mean train loss:  6.68408489e-03, bound:  3.15562278e-01\n",
      "Epoch: 19483 mean train loss:  6.68341713e-03, bound:  3.15562248e-01\n",
      "Epoch: 19484 mean train loss:  6.68281969e-03, bound:  3.15562218e-01\n",
      "Epoch: 19485 mean train loss:  6.68208394e-03, bound:  3.15562159e-01\n",
      "Epoch: 19486 mean train loss:  6.68148743e-03, bound:  3.15562129e-01\n",
      "Epoch: 19487 mean train loss:  6.68097381e-03, bound:  3.15562129e-01\n",
      "Epoch: 19488 mean train loss:  6.68025203e-03, bound:  3.15562069e-01\n",
      "Epoch: 19489 mean train loss:  6.67967135e-03, bound:  3.15562040e-01\n",
      "Epoch: 19490 mean train loss:  6.67898497e-03, bound:  3.15562010e-01\n",
      "Epoch: 19491 mean train loss:  6.67832559e-03, bound:  3.15561950e-01\n",
      "Epoch: 19492 mean train loss:  6.67769602e-03, bound:  3.15561920e-01\n",
      "Epoch: 19493 mean train loss:  6.67711999e-03, bound:  3.15561920e-01\n",
      "Epoch: 19494 mean train loss:  6.67645596e-03, bound:  3.15561920e-01\n",
      "Epoch: 19495 mean train loss:  6.67583337e-03, bound:  3.15561861e-01\n",
      "Epoch: 19496 mean train loss:  6.67519495e-03, bound:  3.15561801e-01\n",
      "Epoch: 19497 mean train loss:  6.67462125e-03, bound:  3.15561801e-01\n",
      "Epoch: 19498 mean train loss:  6.67394185e-03, bound:  3.15561801e-01\n",
      "Epoch: 19499 mean train loss:  6.67336769e-03, bound:  3.15561712e-01\n",
      "Epoch: 19500 mean train loss:  6.67278562e-03, bound:  3.15561712e-01\n",
      "Epoch: 19501 mean train loss:  6.67216629e-03, bound:  3.15561682e-01\n",
      "Epoch: 19502 mean train loss:  6.67161262e-03, bound:  3.15561622e-01\n",
      "Epoch: 19503 mean train loss:  6.67104358e-03, bound:  3.15561593e-01\n",
      "Epoch: 19504 mean train loss:  6.67046010e-03, bound:  3.15561593e-01\n",
      "Epoch: 19505 mean train loss:  6.66986313e-03, bound:  3.15561503e-01\n",
      "Epoch: 19506 mean train loss:  6.66936720e-03, bound:  3.15561503e-01\n",
      "Epoch: 19507 mean train loss:  6.66899374e-03, bound:  3.15561473e-01\n",
      "Epoch: 19508 mean train loss:  6.66865986e-03, bound:  3.15561473e-01\n",
      "Epoch: 19509 mean train loss:  6.66848524e-03, bound:  3.15561384e-01\n",
      "Epoch: 19510 mean train loss:  6.66849967e-03, bound:  3.15561384e-01\n",
      "Epoch: 19511 mean train loss:  6.66858861e-03, bound:  3.15561295e-01\n",
      "Epoch: 19512 mean train loss:  6.66899839e-03, bound:  3.15561295e-01\n",
      "Epoch: 19513 mean train loss:  6.66972948e-03, bound:  3.15561235e-01\n",
      "Epoch: 19514 mean train loss:  6.67091506e-03, bound:  3.15561295e-01\n",
      "Epoch: 19515 mean train loss:  6.67255698e-03, bound:  3.15561175e-01\n",
      "Epoch: 19516 mean train loss:  6.67457888e-03, bound:  3.15561175e-01\n",
      "Epoch: 19517 mean train loss:  6.67665014e-03, bound:  3.15561116e-01\n",
      "Epoch: 19518 mean train loss:  6.67821476e-03, bound:  3.15561175e-01\n",
      "Epoch: 19519 mean train loss:  6.67816028e-03, bound:  3.15561026e-01\n",
      "Epoch: 19520 mean train loss:  6.67592045e-03, bound:  3.15561116e-01\n",
      "Epoch: 19521 mean train loss:  6.67145755e-03, bound:  3.15560967e-01\n",
      "Epoch: 19522 mean train loss:  6.66597579e-03, bound:  3.15561026e-01\n",
      "Epoch: 19523 mean train loss:  6.66096993e-03, bound:  3.15560937e-01\n",
      "Epoch: 19524 mean train loss:  6.65782671e-03, bound:  3.15560937e-01\n",
      "Epoch: 19525 mean train loss:  6.65711658e-03, bound:  3.15560907e-01\n",
      "Epoch: 19526 mean train loss:  6.65822206e-03, bound:  3.15560848e-01\n",
      "Epoch: 19527 mean train loss:  6.65987376e-03, bound:  3.15560848e-01\n",
      "Epoch: 19528 mean train loss:  6.66089542e-03, bound:  3.15560788e-01\n",
      "Epoch: 19529 mean train loss:  6.66040787e-03, bound:  3.15560818e-01\n",
      "Epoch: 19530 mean train loss:  6.65846514e-03, bound:  3.15560728e-01\n",
      "Epoch: 19531 mean train loss:  6.65580109e-03, bound:  3.15560728e-01\n",
      "Epoch: 19532 mean train loss:  6.65350677e-03, bound:  3.15560669e-01\n",
      "Epoch: 19533 mean train loss:  6.65207906e-03, bound:  3.15560669e-01\n",
      "Epoch: 19534 mean train loss:  6.65169349e-03, bound:  3.15560669e-01\n",
      "Epoch: 19535 mean train loss:  6.65197801e-03, bound:  3.15560579e-01\n",
      "Epoch: 19536 mean train loss:  6.65234122e-03, bound:  3.15560579e-01\n",
      "Epoch: 19537 mean train loss:  6.65207813e-03, bound:  3.15560490e-01\n",
      "Epoch: 19538 mean train loss:  6.65105972e-03, bound:  3.15560520e-01\n",
      "Epoch: 19539 mean train loss:  6.64967066e-03, bound:  3.15560490e-01\n",
      "Epoch: 19540 mean train loss:  6.64821640e-03, bound:  3.15560460e-01\n",
      "Epoch: 19541 mean train loss:  6.64711976e-03, bound:  3.15560430e-01\n",
      "Epoch: 19542 mean train loss:  6.64656842e-03, bound:  3.15560371e-01\n",
      "Epoch: 19543 mean train loss:  6.64622150e-03, bound:  3.15560371e-01\n",
      "Epoch: 19544 mean train loss:  6.64604595e-03, bound:  3.15560341e-01\n",
      "Epoch: 19545 mean train loss:  6.64574653e-03, bound:  3.15560341e-01\n",
      "Epoch: 19546 mean train loss:  6.64509088e-03, bound:  3.15560251e-01\n",
      "Epoch: 19547 mean train loss:  6.64421171e-03, bound:  3.15560251e-01\n",
      "Epoch: 19548 mean train loss:  6.64325245e-03, bound:  3.15560192e-01\n",
      "Epoch: 19549 mean train loss:  6.64230809e-03, bound:  3.15560162e-01\n",
      "Epoch: 19550 mean train loss:  6.64160866e-03, bound:  3.15560132e-01\n",
      "Epoch: 19551 mean train loss:  6.64106850e-03, bound:  3.15560102e-01\n",
      "Epoch: 19552 mean train loss:  6.64056698e-03, bound:  3.15560073e-01\n",
      "Epoch: 19553 mean train loss:  6.64017629e-03, bound:  3.15560043e-01\n",
      "Epoch: 19554 mean train loss:  6.63959887e-03, bound:  3.15560013e-01\n",
      "Epoch: 19555 mean train loss:  6.63901540e-03, bound:  3.15559983e-01\n",
      "Epoch: 19556 mean train loss:  6.63828943e-03, bound:  3.15559983e-01\n",
      "Epoch: 19557 mean train loss:  6.63744612e-03, bound:  3.15559924e-01\n",
      "Epoch: 19558 mean train loss:  6.63675694e-03, bound:  3.15559894e-01\n",
      "Epoch: 19559 mean train loss:  6.63603749e-03, bound:  3.15559864e-01\n",
      "Epoch: 19560 mean train loss:  6.63552945e-03, bound:  3.15559804e-01\n",
      "Epoch: 19561 mean train loss:  6.63503399e-03, bound:  3.15559804e-01\n",
      "Epoch: 19562 mean train loss:  6.63436297e-03, bound:  3.15559775e-01\n",
      "Epoch: 19563 mean train loss:  6.63383538e-03, bound:  3.15559745e-01\n",
      "Epoch: 19564 mean train loss:  6.63321977e-03, bound:  3.15559685e-01\n",
      "Epoch: 19565 mean train loss:  6.63254363e-03, bound:  3.15559685e-01\n",
      "Epoch: 19566 mean train loss:  6.63176971e-03, bound:  3.15559626e-01\n",
      "Epoch: 19567 mean train loss:  6.63121603e-03, bound:  3.15559596e-01\n",
      "Epoch: 19568 mean train loss:  6.63055945e-03, bound:  3.15559566e-01\n",
      "Epoch: 19569 mean train loss:  6.62993873e-03, bound:  3.15559566e-01\n",
      "Epoch: 19570 mean train loss:  6.62934082e-03, bound:  3.15559506e-01\n",
      "Epoch: 19571 mean train loss:  6.62877923e-03, bound:  3.15559477e-01\n",
      "Epoch: 19572 mean train loss:  6.62816362e-03, bound:  3.15559477e-01\n",
      "Epoch: 19573 mean train loss:  6.62755547e-03, bound:  3.15559447e-01\n",
      "Epoch: 19574 mean train loss:  6.62700087e-03, bound:  3.15559387e-01\n",
      "Epoch: 19575 mean train loss:  6.62632380e-03, bound:  3.15559357e-01\n",
      "Epoch: 19576 mean train loss:  6.62566582e-03, bound:  3.15559328e-01\n",
      "Epoch: 19577 mean train loss:  6.62503671e-03, bound:  3.15559268e-01\n",
      "Epoch: 19578 mean train loss:  6.62444904e-03, bound:  3.15559268e-01\n",
      "Epoch: 19579 mean train loss:  6.62382646e-03, bound:  3.15559238e-01\n",
      "Epoch: 19580 mean train loss:  6.62321085e-03, bound:  3.15559208e-01\n",
      "Epoch: 19581 mean train loss:  6.62259012e-03, bound:  3.15559179e-01\n",
      "Epoch: 19582 mean train loss:  6.62189722e-03, bound:  3.15559149e-01\n",
      "Epoch: 19583 mean train loss:  6.62132213e-03, bound:  3.15559119e-01\n",
      "Epoch: 19584 mean train loss:  6.62074005e-03, bound:  3.15559089e-01\n",
      "Epoch: 19585 mean train loss:  6.62013190e-03, bound:  3.15559059e-01\n",
      "Epoch: 19586 mean train loss:  6.61948323e-03, bound:  3.15559000e-01\n",
      "Epoch: 19587 mean train loss:  6.61889603e-03, bound:  3.15558940e-01\n",
      "Epoch: 19588 mean train loss:  6.61828276e-03, bound:  3.15558940e-01\n",
      "Epoch: 19589 mean train loss:  6.61768578e-03, bound:  3.15558940e-01\n",
      "Epoch: 19590 mean train loss:  6.61706505e-03, bound:  3.15558881e-01\n",
      "Epoch: 19591 mean train loss:  6.61642617e-03, bound:  3.15558881e-01\n",
      "Epoch: 19592 mean train loss:  6.61585899e-03, bound:  3.15558821e-01\n",
      "Epoch: 19593 mean train loss:  6.61520613e-03, bound:  3.15558821e-01\n",
      "Epoch: 19594 mean train loss:  6.61457097e-03, bound:  3.15558791e-01\n",
      "Epoch: 19595 mean train loss:  6.61395630e-03, bound:  3.15558761e-01\n",
      "Epoch: 19596 mean train loss:  6.61333278e-03, bound:  3.15558702e-01\n",
      "Epoch: 19597 mean train loss:  6.61270740e-03, bound:  3.15558672e-01\n",
      "Epoch: 19598 mean train loss:  6.61208993e-03, bound:  3.15558672e-01\n",
      "Epoch: 19599 mean train loss:  6.61144312e-03, bound:  3.15558612e-01\n",
      "Epoch: 19600 mean train loss:  6.61085732e-03, bound:  3.15558583e-01\n",
      "Epoch: 19601 mean train loss:  6.61029201e-03, bound:  3.15558553e-01\n",
      "Epoch: 19602 mean train loss:  6.60967687e-03, bound:  3.15558493e-01\n",
      "Epoch: 19603 mean train loss:  6.60902495e-03, bound:  3.15558493e-01\n",
      "Epoch: 19604 mean train loss:  6.60847686e-03, bound:  3.15558463e-01\n",
      "Epoch: 19605 mean train loss:  6.60778536e-03, bound:  3.15558434e-01\n",
      "Epoch: 19606 mean train loss:  6.60723634e-03, bound:  3.15558374e-01\n",
      "Epoch: 19607 mean train loss:  6.60662726e-03, bound:  3.15558344e-01\n",
      "Epoch: 19608 mean train loss:  6.60596183e-03, bound:  3.15558344e-01\n",
      "Epoch: 19609 mean train loss:  6.60533644e-03, bound:  3.15558314e-01\n",
      "Epoch: 19610 mean train loss:  6.60469895e-03, bound:  3.15558255e-01\n",
      "Epoch: 19611 mean train loss:  6.60403678e-03, bound:  3.15558255e-01\n",
      "Epoch: 19612 mean train loss:  6.60343701e-03, bound:  3.15558225e-01\n",
      "Epoch: 19613 mean train loss:  6.60282746e-03, bound:  3.15558136e-01\n",
      "Epoch: 19614 mean train loss:  6.60220021e-03, bound:  3.15558136e-01\n",
      "Epoch: 19615 mean train loss:  6.60152407e-03, bound:  3.15558136e-01\n",
      "Epoch: 19616 mean train loss:  6.60097925e-03, bound:  3.15558106e-01\n",
      "Epoch: 19617 mean train loss:  6.60038926e-03, bound:  3.15558046e-01\n",
      "Epoch: 19618 mean train loss:  6.59976667e-03, bound:  3.15558016e-01\n",
      "Epoch: 19619 mean train loss:  6.59918180e-03, bound:  3.15557986e-01\n",
      "Epoch: 19620 mean train loss:  6.59851637e-03, bound:  3.15557986e-01\n",
      "Epoch: 19621 mean train loss:  6.59783930e-03, bound:  3.15557927e-01\n",
      "Epoch: 19622 mean train loss:  6.59727352e-03, bound:  3.15557897e-01\n",
      "Epoch: 19623 mean train loss:  6.59671705e-03, bound:  3.15557837e-01\n",
      "Epoch: 19624 mean train loss:  6.59601903e-03, bound:  3.15557837e-01\n",
      "Epoch: 19625 mean train loss:  6.59544067e-03, bound:  3.15557808e-01\n",
      "Epoch: 19626 mean train loss:  6.59481576e-03, bound:  3.15557748e-01\n",
      "Epoch: 19627 mean train loss:  6.59422344e-03, bound:  3.15557748e-01\n",
      "Epoch: 19628 mean train loss:  6.59359246e-03, bound:  3.15557718e-01\n",
      "Epoch: 19629 mean train loss:  6.59306953e-03, bound:  3.15557659e-01\n",
      "Epoch: 19630 mean train loss:  6.59237057e-03, bound:  3.15557659e-01\n",
      "Epoch: 19631 mean train loss:  6.59179175e-03, bound:  3.15557599e-01\n",
      "Epoch: 19632 mean train loss:  6.59121061e-03, bound:  3.15557569e-01\n",
      "Epoch: 19633 mean train loss:  6.59067789e-03, bound:  3.15557539e-01\n",
      "Epoch: 19634 mean train loss:  6.59000920e-03, bound:  3.15557539e-01\n",
      "Epoch: 19635 mean train loss:  6.58948300e-03, bound:  3.15557450e-01\n",
      "Epoch: 19636 mean train loss:  6.58888277e-03, bound:  3.15557450e-01\n",
      "Epoch: 19637 mean train loss:  6.58847764e-03, bound:  3.15557420e-01\n",
      "Epoch: 19638 mean train loss:  6.58804551e-03, bound:  3.15557390e-01\n",
      "Epoch: 19639 mean train loss:  6.58760127e-03, bound:  3.15557361e-01\n",
      "Epoch: 19640 mean train loss:  6.58732140e-03, bound:  3.15557361e-01\n",
      "Epoch: 19641 mean train loss:  6.58724131e-03, bound:  3.15557271e-01\n",
      "Epoch: 19642 mean train loss:  6.58731721e-03, bound:  3.15557301e-01\n",
      "Epoch: 19643 mean train loss:  6.58768509e-03, bound:  3.15557212e-01\n",
      "Epoch: 19644 mean train loss:  6.58839429e-03, bound:  3.15557241e-01\n",
      "Epoch: 19645 mean train loss:  6.58952910e-03, bound:  3.15557122e-01\n",
      "Epoch: 19646 mean train loss:  6.59104390e-03, bound:  3.15557152e-01\n",
      "Epoch: 19647 mean train loss:  6.59297407e-03, bound:  3.15557033e-01\n",
      "Epoch: 19648 mean train loss:  6.59524929e-03, bound:  3.15557122e-01\n",
      "Epoch: 19649 mean train loss:  6.59715151e-03, bound:  3.15557003e-01\n",
      "Epoch: 19650 mean train loss:  6.59790821e-03, bound:  3.15557033e-01\n",
      "Epoch: 19651 mean train loss:  6.59676455e-03, bound:  3.15556914e-01\n",
      "Epoch: 19652 mean train loss:  6.59316173e-03, bound:  3.15557003e-01\n",
      "Epoch: 19653 mean train loss:  6.58777775e-03, bound:  3.15556884e-01\n",
      "Epoch: 19654 mean train loss:  6.58222102e-03, bound:  3.15556914e-01\n",
      "Epoch: 19655 mean train loss:  6.57808036e-03, bound:  3.15556854e-01\n",
      "Epoch: 19656 mean train loss:  6.57638954e-03, bound:  3.15556824e-01\n",
      "Epoch: 19657 mean train loss:  6.57697860e-03, bound:  3.15556824e-01\n",
      "Epoch: 19658 mean train loss:  6.57860842e-03, bound:  3.15556765e-01\n",
      "Epoch: 19659 mean train loss:  6.58009062e-03, bound:  3.15556765e-01\n",
      "Epoch: 19660 mean train loss:  6.58026990e-03, bound:  3.15556675e-01\n",
      "Epoch: 19661 mean train loss:  6.57886500e-03, bound:  3.15556705e-01\n",
      "Epoch: 19662 mean train loss:  6.57623494e-03, bound:  3.15556645e-01\n",
      "Epoch: 19663 mean train loss:  6.57354342e-03, bound:  3.15556645e-01\n",
      "Epoch: 19664 mean train loss:  6.57169940e-03, bound:  3.15556586e-01\n",
      "Epoch: 19665 mean train loss:  6.57109590e-03, bound:  3.15556556e-01\n",
      "Epoch: 19666 mean train loss:  6.57151965e-03, bound:  3.15556556e-01\n",
      "Epoch: 19667 mean train loss:  6.57201000e-03, bound:  3.15556467e-01\n",
      "Epoch: 19668 mean train loss:  6.57209195e-03, bound:  3.15556526e-01\n",
      "Epoch: 19669 mean train loss:  6.57121558e-03, bound:  3.15556437e-01\n",
      "Epoch: 19670 mean train loss:  6.56986376e-03, bound:  3.15556437e-01\n",
      "Epoch: 19671 mean train loss:  6.56819297e-03, bound:  3.15556377e-01\n",
      "Epoch: 19672 mean train loss:  6.56698225e-03, bound:  3.15556377e-01\n",
      "Epoch: 19673 mean train loss:  6.56630704e-03, bound:  3.15556318e-01\n",
      "Epoch: 19674 mean train loss:  6.56595873e-03, bound:  3.15556318e-01\n",
      "Epoch: 19675 mean train loss:  6.56573009e-03, bound:  3.15556258e-01\n",
      "Epoch: 19676 mean train loss:  6.56537665e-03, bound:  3.15556228e-01\n",
      "Epoch: 19677 mean train loss:  6.56482950e-03, bound:  3.15556228e-01\n",
      "Epoch: 19678 mean train loss:  6.56399736e-03, bound:  3.15556198e-01\n",
      "Epoch: 19679 mean train loss:  6.56308793e-03, bound:  3.15556139e-01\n",
      "Epoch: 19680 mean train loss:  6.56226138e-03, bound:  3.15556109e-01\n",
      "Epoch: 19681 mean train loss:  6.56148838e-03, bound:  3.15556109e-01\n",
      "Epoch: 19682 mean train loss:  6.56081317e-03, bound:  3.15556079e-01\n",
      "Epoch: 19683 mean train loss:  6.56034425e-03, bound:  3.15556020e-01\n",
      "Epoch: 19684 mean train loss:  6.55991770e-03, bound:  3.15556020e-01\n",
      "Epoch: 19685 mean train loss:  6.55948883e-03, bound:  3.15555960e-01\n",
      "Epoch: 19686 mean train loss:  6.55886764e-03, bound:  3.15555900e-01\n",
      "Epoch: 19687 mean train loss:  6.55818311e-03, bound:  3.15555900e-01\n",
      "Epoch: 19688 mean train loss:  6.55739428e-03, bound:  3.15555871e-01\n",
      "Epoch: 19689 mean train loss:  6.55674236e-03, bound:  3.15555811e-01\n",
      "Epoch: 19690 mean train loss:  6.55607227e-03, bound:  3.15555811e-01\n",
      "Epoch: 19691 mean train loss:  6.55552931e-03, bound:  3.15555811e-01\n",
      "Epoch: 19692 mean train loss:  6.55506831e-03, bound:  3.15555751e-01\n",
      "Epoch: 19693 mean train loss:  6.55452767e-03, bound:  3.15555751e-01\n",
      "Epoch: 19694 mean train loss:  6.55397726e-03, bound:  3.15555692e-01\n",
      "Epoch: 19695 mean train loss:  6.55331928e-03, bound:  3.15555662e-01\n",
      "Epoch: 19696 mean train loss:  6.55269856e-03, bound:  3.15555632e-01\n",
      "Epoch: 19697 mean train loss:  6.55200006e-03, bound:  3.15555632e-01\n",
      "Epoch: 19698 mean train loss:  6.55138958e-03, bound:  3.15555573e-01\n",
      "Epoch: 19699 mean train loss:  6.55075815e-03, bound:  3.15555543e-01\n",
      "Epoch: 19700 mean train loss:  6.55011740e-03, bound:  3.15555513e-01\n",
      "Epoch: 19701 mean train loss:  6.54956419e-03, bound:  3.15555453e-01\n",
      "Epoch: 19702 mean train loss:  6.54898398e-03, bound:  3.15555453e-01\n",
      "Epoch: 19703 mean train loss:  6.54840469e-03, bound:  3.15555423e-01\n",
      "Epoch: 19704 mean train loss:  6.54779281e-03, bound:  3.15555423e-01\n",
      "Epoch: 19705 mean train loss:  6.54730387e-03, bound:  3.15555334e-01\n",
      "Epoch: 19706 mean train loss:  6.54657278e-03, bound:  3.15555334e-01\n",
      "Epoch: 19707 mean train loss:  6.54595206e-03, bound:  3.15555304e-01\n",
      "Epoch: 19708 mean train loss:  6.54539606e-03, bound:  3.15555304e-01\n",
      "Epoch: 19709 mean train loss:  6.54472364e-03, bound:  3.15555215e-01\n",
      "Epoch: 19710 mean train loss:  6.54414948e-03, bound:  3.15555215e-01\n",
      "Epoch: 19711 mean train loss:  6.54361164e-03, bound:  3.15555185e-01\n",
      "Epoch: 19712 mean train loss:  6.54301839e-03, bound:  3.15555185e-01\n",
      "Epoch: 19713 mean train loss:  6.54240744e-03, bound:  3.15555125e-01\n",
      "Epoch: 19714 mean train loss:  6.54175133e-03, bound:  3.15555096e-01\n",
      "Epoch: 19715 mean train loss:  6.54120604e-03, bound:  3.15555066e-01\n",
      "Epoch: 19716 mean train loss:  6.54058950e-03, bound:  3.15555006e-01\n",
      "Epoch: 19717 mean train loss:  6.53996691e-03, bound:  3.15555006e-01\n",
      "Epoch: 19718 mean train loss:  6.53940812e-03, bound:  3.15554976e-01\n",
      "Epoch: 19719 mean train loss:  6.53873896e-03, bound:  3.15554917e-01\n",
      "Epoch: 19720 mean train loss:  6.53816294e-03, bound:  3.15554887e-01\n",
      "Epoch: 19721 mean train loss:  6.53759204e-03, bound:  3.15554857e-01\n",
      "Epoch: 19722 mean train loss:  6.53701322e-03, bound:  3.15554857e-01\n",
      "Epoch: 19723 mean train loss:  6.53637946e-03, bound:  3.15554798e-01\n",
      "Epoch: 19724 mean train loss:  6.53575594e-03, bound:  3.15554768e-01\n",
      "Epoch: 19725 mean train loss:  6.53521018e-03, bound:  3.15554768e-01\n",
      "Epoch: 19726 mean train loss:  6.53463649e-03, bound:  3.15554708e-01\n",
      "Epoch: 19727 mean train loss:  6.53398968e-03, bound:  3.15554678e-01\n",
      "Epoch: 19728 mean train loss:  6.53344579e-03, bound:  3.15554649e-01\n",
      "Epoch: 19729 mean train loss:  6.53277757e-03, bound:  3.15554619e-01\n",
      "Epoch: 19730 mean train loss:  6.53215451e-03, bound:  3.15554589e-01\n",
      "Epoch: 19731 mean train loss:  6.53156918e-03, bound:  3.15554589e-01\n",
      "Epoch: 19732 mean train loss:  6.53102668e-03, bound:  3.15554559e-01\n",
      "Epoch: 19733 mean train loss:  6.53038267e-03, bound:  3.15554500e-01\n",
      "Epoch: 19734 mean train loss:  6.52979733e-03, bound:  3.15554500e-01\n",
      "Epoch: 19735 mean train loss:  6.52914448e-03, bound:  3.15554440e-01\n",
      "Epoch: 19736 mean train loss:  6.52858661e-03, bound:  3.15554380e-01\n",
      "Epoch: 19737 mean train loss:  6.52798964e-03, bound:  3.15554380e-01\n",
      "Epoch: 19738 mean train loss:  6.52737031e-03, bound:  3.15554351e-01\n",
      "Epoch: 19739 mean train loss:  6.52679754e-03, bound:  3.15554321e-01\n",
      "Epoch: 19740 mean train loss:  6.52621919e-03, bound:  3.15554291e-01\n",
      "Epoch: 19741 mean train loss:  6.52551092e-03, bound:  3.15554261e-01\n",
      "Epoch: 19742 mean train loss:  6.52501779e-03, bound:  3.15554202e-01\n",
      "Epoch: 19743 mean train loss:  6.52439287e-03, bound:  3.15554202e-01\n",
      "Epoch: 19744 mean train loss:  6.52376004e-03, bound:  3.15554172e-01\n",
      "Epoch: 19745 mean train loss:  6.52319239e-03, bound:  3.15554112e-01\n",
      "Epoch: 19746 mean train loss:  6.52258703e-03, bound:  3.15554112e-01\n",
      "Epoch: 19747 mean train loss:  6.52196817e-03, bound:  3.15554082e-01\n",
      "Epoch: 19748 mean train loss:  6.52138237e-03, bound:  3.15554053e-01\n",
      "Epoch: 19749 mean train loss:  6.52080541e-03, bound:  3.15553993e-01\n",
      "Epoch: 19750 mean train loss:  6.52018096e-03, bound:  3.15553993e-01\n",
      "Epoch: 19751 mean train loss:  6.51961286e-03, bound:  3.15553993e-01\n",
      "Epoch: 19752 mean train loss:  6.51900517e-03, bound:  3.15553904e-01\n",
      "Epoch: 19753 mean train loss:  6.51842216e-03, bound:  3.15553874e-01\n",
      "Epoch: 19754 mean train loss:  6.51779491e-03, bound:  3.15553874e-01\n",
      "Epoch: 19755 mean train loss:  6.51720772e-03, bound:  3.15553784e-01\n",
      "Epoch: 19756 mean train loss:  6.51663123e-03, bound:  3.15553784e-01\n",
      "Epoch: 19757 mean train loss:  6.51603658e-03, bound:  3.15553784e-01\n",
      "Epoch: 19758 mean train loss:  6.51553040e-03, bound:  3.15553755e-01\n",
      "Epoch: 19759 mean train loss:  6.51500188e-03, bound:  3.15553665e-01\n",
      "Epoch: 19760 mean train loss:  6.51439698e-03, bound:  3.15553665e-01\n",
      "Epoch: 19761 mean train loss:  6.51379628e-03, bound:  3.15553665e-01\n",
      "Epoch: 19762 mean train loss:  6.51324447e-03, bound:  3.15553606e-01\n",
      "Epoch: 19763 mean train loss:  6.51273318e-03, bound:  3.15553576e-01\n",
      "Epoch: 19764 mean train loss:  6.51215576e-03, bound:  3.15553546e-01\n",
      "Epoch: 19765 mean train loss:  6.51167287e-03, bound:  3.15553516e-01\n",
      "Epoch: 19766 mean train loss:  6.51121233e-03, bound:  3.15553457e-01\n",
      "Epoch: 19767 mean train loss:  6.51076576e-03, bound:  3.15553457e-01\n",
      "Epoch: 19768 mean train loss:  6.51031500e-03, bound:  3.15553427e-01\n",
      "Epoch: 19769 mean train loss:  6.51000487e-03, bound:  3.15553397e-01\n",
      "Epoch: 19770 mean train loss:  6.50981860e-03, bound:  3.15553337e-01\n",
      "Epoch: 19771 mean train loss:  6.50977343e-03, bound:  3.15553337e-01\n",
      "Epoch: 19772 mean train loss:  6.50977390e-03, bound:  3.15553278e-01\n",
      "Epoch: 19773 mean train loss:  6.51013711e-03, bound:  3.15553308e-01\n",
      "Epoch: 19774 mean train loss:  6.51080906e-03, bound:  3.15553218e-01\n",
      "Epoch: 19775 mean train loss:  6.51170313e-03, bound:  3.15553218e-01\n",
      "Epoch: 19776 mean train loss:  6.51292503e-03, bound:  3.15553129e-01\n",
      "Epoch: 19777 mean train loss:  6.51423028e-03, bound:  3.15553188e-01\n",
      "Epoch: 19778 mean train loss:  6.51550665e-03, bound:  3.15553069e-01\n",
      "Epoch: 19779 mean train loss:  6.51626522e-03, bound:  3.15553099e-01\n",
      "Epoch: 19780 mean train loss:  6.51606964e-03, bound:  3.15553010e-01\n",
      "Epoch: 19781 mean train loss:  6.51447382e-03, bound:  3.15553069e-01\n",
      "Epoch: 19782 mean train loss:  6.51149126e-03, bound:  3.15552980e-01\n",
      "Epoch: 19783 mean train loss:  6.50743395e-03, bound:  3.15553010e-01\n",
      "Epoch: 19784 mean train loss:  6.50338037e-03, bound:  3.15552950e-01\n",
      "Epoch: 19785 mean train loss:  6.50023203e-03, bound:  3.15552950e-01\n",
      "Epoch: 19786 mean train loss:  6.49863668e-03, bound:  3.15552890e-01\n",
      "Epoch: 19787 mean train loss:  6.49853377e-03, bound:  3.15552860e-01\n",
      "Epoch: 19788 mean train loss:  6.49923459e-03, bound:  3.15552860e-01\n",
      "Epoch: 19789 mean train loss:  6.50019618e-03, bound:  3.15552771e-01\n",
      "Epoch: 19790 mean train loss:  6.50056917e-03, bound:  3.15552771e-01\n",
      "Epoch: 19791 mean train loss:  6.50005229e-03, bound:  3.15552682e-01\n",
      "Epoch: 19792 mean train loss:  6.49858452e-03, bound:  3.15552741e-01\n",
      "Epoch: 19793 mean train loss:  6.49682200e-03, bound:  3.15552652e-01\n",
      "Epoch: 19794 mean train loss:  6.49493281e-03, bound:  3.15552652e-01\n",
      "Epoch: 19795 mean train loss:  6.49359310e-03, bound:  3.15552622e-01\n",
      "Epoch: 19796 mean train loss:  6.49281405e-03, bound:  3.15552562e-01\n",
      "Epoch: 19797 mean train loss:  6.49252301e-03, bound:  3.15552562e-01\n",
      "Epoch: 19798 mean train loss:  6.49245735e-03, bound:  3.15552503e-01\n",
      "Epoch: 19799 mean train loss:  6.49234094e-03, bound:  3.15552533e-01\n",
      "Epoch: 19800 mean train loss:  6.49202801e-03, bound:  3.15552443e-01\n",
      "Epoch: 19801 mean train loss:  6.49131183e-03, bound:  3.15552443e-01\n",
      "Epoch: 19802 mean train loss:  6.49048947e-03, bound:  3.15552384e-01\n",
      "Epoch: 19803 mean train loss:  6.48942124e-03, bound:  3.15552384e-01\n",
      "Epoch: 19804 mean train loss:  6.48839306e-03, bound:  3.15552354e-01\n",
      "Epoch: 19805 mean train loss:  6.48759864e-03, bound:  3.15552324e-01\n",
      "Epoch: 19806 mean train loss:  6.48692017e-03, bound:  3.15552324e-01\n",
      "Epoch: 19807 mean train loss:  6.48648478e-03, bound:  3.15552264e-01\n",
      "Epoch: 19808 mean train loss:  6.48595998e-03, bound:  3.15552235e-01\n",
      "Epoch: 19809 mean train loss:  6.48564519e-03, bound:  3.15552205e-01\n",
      "Epoch: 19810 mean train loss:  6.48523355e-03, bound:  3.15552205e-01\n",
      "Epoch: 19811 mean train loss:  6.48463937e-03, bound:  3.15552115e-01\n",
      "Epoch: 19812 mean train loss:  6.48395112e-03, bound:  3.15552115e-01\n",
      "Epoch: 19813 mean train loss:  6.48320792e-03, bound:  3.15552086e-01\n",
      "Epoch: 19814 mean train loss:  6.48248009e-03, bound:  3.15552056e-01\n",
      "Epoch: 19815 mean train loss:  6.48180442e-03, bound:  3.15552056e-01\n",
      "Epoch: 19816 mean train loss:  6.48106635e-03, bound:  3.15551996e-01\n",
      "Epoch: 19817 mean train loss:  6.48054434e-03, bound:  3.15551966e-01\n",
      "Epoch: 19818 mean train loss:  6.48004748e-03, bound:  3.15551937e-01\n",
      "Epoch: 19819 mean train loss:  6.47945888e-03, bound:  3.15551937e-01\n",
      "Epoch: 19820 mean train loss:  6.47906540e-03, bound:  3.15551877e-01\n",
      "Epoch: 19821 mean train loss:  6.47849962e-03, bound:  3.15551847e-01\n",
      "Epoch: 19822 mean train loss:  6.47786865e-03, bound:  3.15551817e-01\n",
      "Epoch: 19823 mean train loss:  6.47735968e-03, bound:  3.15551788e-01\n",
      "Epoch: 19824 mean train loss:  6.47664024e-03, bound:  3.15551758e-01\n",
      "Epoch: 19825 mean train loss:  6.47599669e-03, bound:  3.15551758e-01\n",
      "Epoch: 19826 mean train loss:  6.47527864e-03, bound:  3.15551698e-01\n",
      "Epoch: 19827 mean train loss:  6.47470308e-03, bound:  3.15551639e-01\n",
      "Epoch: 19828 mean train loss:  6.47410657e-03, bound:  3.15551639e-01\n",
      "Epoch: 19829 mean train loss:  6.47349702e-03, bound:  3.15551639e-01\n",
      "Epoch: 19830 mean train loss:  6.47300575e-03, bound:  3.15551549e-01\n",
      "Epoch: 19831 mean train loss:  6.47239061e-03, bound:  3.15551549e-01\n",
      "Epoch: 19832 mean train loss:  6.47181598e-03, bound:  3.15551519e-01\n",
      "Epoch: 19833 mean train loss:  6.47127070e-03, bound:  3.15551490e-01\n",
      "Epoch: 19834 mean train loss:  6.47065369e-03, bound:  3.15551460e-01\n",
      "Epoch: 19835 mean train loss:  6.47010142e-03, bound:  3.15551430e-01\n",
      "Epoch: 19836 mean train loss:  6.46945974e-03, bound:  3.15551400e-01\n",
      "Epoch: 19837 mean train loss:  6.46891305e-03, bound:  3.15551341e-01\n",
      "Epoch: 19838 mean train loss:  6.46825042e-03, bound:  3.15551341e-01\n",
      "Epoch: 19839 mean train loss:  6.46770280e-03, bound:  3.15551311e-01\n",
      "Epoch: 19840 mean train loss:  6.46709185e-03, bound:  3.15551281e-01\n",
      "Epoch: 19841 mean train loss:  6.46650931e-03, bound:  3.15551251e-01\n",
      "Epoch: 19842 mean train loss:  6.46591606e-03, bound:  3.15551251e-01\n",
      "Epoch: 19843 mean train loss:  6.46528462e-03, bound:  3.15551192e-01\n",
      "Epoch: 19844 mean train loss:  6.46471884e-03, bound:  3.15551162e-01\n",
      "Epoch: 19845 mean train loss:  6.46414375e-03, bound:  3.15551102e-01\n",
      "Epoch: 19846 mean train loss:  6.46347459e-03, bound:  3.15551072e-01\n",
      "Epoch: 19847 mean train loss:  6.46295212e-03, bound:  3.15551072e-01\n",
      "Epoch: 19848 mean train loss:  6.46238541e-03, bound:  3.15551043e-01\n",
      "Epoch: 19849 mean train loss:  6.46182336e-03, bound:  3.15550983e-01\n",
      "Epoch: 19850 mean train loss:  6.46117469e-03, bound:  3.15550953e-01\n",
      "Epoch: 19851 mean train loss:  6.46067457e-03, bound:  3.15550953e-01\n",
      "Epoch: 19852 mean train loss:  6.46008691e-03, bound:  3.15550953e-01\n",
      "Epoch: 19853 mean train loss:  6.45947875e-03, bound:  3.15550894e-01\n",
      "Epoch: 19854 mean train loss:  6.45888317e-03, bound:  3.15550834e-01\n",
      "Epoch: 19855 mean train loss:  6.45829225e-03, bound:  3.15550834e-01\n",
      "Epoch: 19856 mean train loss:  6.45768223e-03, bound:  3.15550834e-01\n",
      "Epoch: 19857 mean train loss:  6.45709969e-03, bound:  3.15550774e-01\n",
      "Epoch: 19858 mean train loss:  6.45650225e-03, bound:  3.15550715e-01\n",
      "Epoch: 19859 mean train loss:  6.45592948e-03, bound:  3.15550715e-01\n",
      "Epoch: 19860 mean train loss:  6.45531015e-03, bound:  3.15550655e-01\n",
      "Epoch: 19861 mean train loss:  6.45475974e-03, bound:  3.15550655e-01\n",
      "Epoch: 19862 mean train loss:  6.45421399e-03, bound:  3.15550625e-01\n",
      "Epoch: 19863 mean train loss:  6.45359000e-03, bound:  3.15550596e-01\n",
      "Epoch: 19864 mean train loss:  6.45299070e-03, bound:  3.15550596e-01\n",
      "Epoch: 19865 mean train loss:  6.45236438e-03, bound:  3.15550536e-01\n",
      "Epoch: 19866 mean train loss:  6.45179860e-03, bound:  3.15550506e-01\n",
      "Epoch: 19867 mean train loss:  6.45124260e-03, bound:  3.15550476e-01\n",
      "Epoch: 19868 mean train loss:  6.45066472e-03, bound:  3.15550417e-01\n",
      "Epoch: 19869 mean train loss:  6.45004818e-03, bound:  3.15550387e-01\n",
      "Epoch: 19870 mean train loss:  6.44953270e-03, bound:  3.15550387e-01\n",
      "Epoch: 19871 mean train loss:  6.44893385e-03, bound:  3.15550327e-01\n",
      "Epoch: 19872 mean train loss:  6.44835038e-03, bound:  3.15550297e-01\n",
      "Epoch: 19873 mean train loss:  6.44769333e-03, bound:  3.15550297e-01\n",
      "Epoch: 19874 mean train loss:  6.44707726e-03, bound:  3.15550268e-01\n",
      "Epoch: 19875 mean train loss:  6.44654548e-03, bound:  3.15550208e-01\n",
      "Epoch: 19876 mean train loss:  6.44593360e-03, bound:  3.15550208e-01\n",
      "Epoch: 19877 mean train loss:  6.44538039e-03, bound:  3.15550148e-01\n",
      "Epoch: 19878 mean train loss:  6.44488400e-03, bound:  3.15550148e-01\n",
      "Epoch: 19879 mean train loss:  6.44431822e-03, bound:  3.15550148e-01\n",
      "Epoch: 19880 mean train loss:  6.44380506e-03, bound:  3.15550089e-01\n",
      "Epoch: 19881 mean train loss:  6.44323044e-03, bound:  3.15550029e-01\n",
      "Epoch: 19882 mean train loss:  6.44277642e-03, bound:  3.15550029e-01\n",
      "Epoch: 19883 mean train loss:  6.44229725e-03, bound:  3.15549970e-01\n",
      "Epoch: 19884 mean train loss:  6.44186931e-03, bound:  3.15549940e-01\n",
      "Epoch: 19885 mean train loss:  6.44156709e-03, bound:  3.15549940e-01\n",
      "Epoch: 19886 mean train loss:  6.44142833e-03, bound:  3.15549880e-01\n",
      "Epoch: 19887 mean train loss:  6.44140691e-03, bound:  3.15549880e-01\n",
      "Epoch: 19888 mean train loss:  6.44160062e-03, bound:  3.15549821e-01\n",
      "Epoch: 19889 mean train loss:  6.44216733e-03, bound:  3.15549821e-01\n",
      "Epoch: 19890 mean train loss:  6.44325465e-03, bound:  3.15549731e-01\n",
      "Epoch: 19891 mean train loss:  6.44489285e-03, bound:  3.15549761e-01\n",
      "Epoch: 19892 mean train loss:  6.44690916e-03, bound:  3.15549642e-01\n",
      "Epoch: 19893 mean train loss:  6.44929474e-03, bound:  3.15549731e-01\n",
      "Epoch: 19894 mean train loss:  6.45175949e-03, bound:  3.15549612e-01\n",
      "Epoch: 19895 mean train loss:  6.45346520e-03, bound:  3.15549701e-01\n",
      "Epoch: 19896 mean train loss:  6.45328406e-03, bound:  3.15549523e-01\n",
      "Epoch: 19897 mean train loss:  6.45078439e-03, bound:  3.15549612e-01\n",
      "Epoch: 19898 mean train loss:  6.44565793e-03, bound:  3.15549523e-01\n",
      "Epoch: 19899 mean train loss:  6.43941155e-03, bound:  3.15549523e-01\n",
      "Epoch: 19900 mean train loss:  6.43420871e-03, bound:  3.15549463e-01\n",
      "Epoch: 19901 mean train loss:  6.43145712e-03, bound:  3.15549463e-01\n",
      "Epoch: 19902 mean train loss:  6.43140217e-03, bound:  3.15549403e-01\n",
      "Epoch: 19903 mean train loss:  6.43311115e-03, bound:  3.15549374e-01\n",
      "Epoch: 19904 mean train loss:  6.43484900e-03, bound:  3.15549403e-01\n",
      "Epoch: 19905 mean train loss:  6.43550185e-03, bound:  3.15549314e-01\n",
      "Epoch: 19906 mean train loss:  6.43433165e-03, bound:  3.15549314e-01\n",
      "Epoch: 19907 mean train loss:  6.43191999e-03, bound:  3.15549284e-01\n",
      "Epoch: 19908 mean train loss:  6.42914139e-03, bound:  3.15549284e-01\n",
      "Epoch: 19909 mean train loss:  6.42703474e-03, bound:  3.15549195e-01\n",
      "Epoch: 19910 mean train loss:  6.42630877e-03, bound:  3.15549195e-01\n",
      "Epoch: 19911 mean train loss:  6.42655417e-03, bound:  3.15549165e-01\n",
      "Epoch: 19912 mean train loss:  6.42711157e-03, bound:  3.15549105e-01\n",
      "Epoch: 19913 mean train loss:  6.42723870e-03, bound:  3.15549105e-01\n",
      "Epoch: 19914 mean train loss:  6.42660446e-03, bound:  3.15549076e-01\n",
      "Epoch: 19915 mean train loss:  6.42538210e-03, bound:  3.15549076e-01\n",
      "Epoch: 19916 mean train loss:  6.42368803e-03, bound:  3.15549016e-01\n",
      "Epoch: 19917 mean train loss:  6.42254530e-03, bound:  3.15549016e-01\n",
      "Epoch: 19918 mean train loss:  6.42166240e-03, bound:  3.15548956e-01\n",
      "Epoch: 19919 mean train loss:  6.42134761e-03, bound:  3.15548927e-01\n",
      "Epoch: 19920 mean train loss:  6.42134715e-03, bound:  3.15548897e-01\n",
      "Epoch: 19921 mean train loss:  6.42103050e-03, bound:  3.15548867e-01\n",
      "Epoch: 19922 mean train loss:  6.42054202e-03, bound:  3.15548867e-01\n",
      "Epoch: 19923 mean train loss:  6.41963072e-03, bound:  3.15548807e-01\n",
      "Epoch: 19924 mean train loss:  6.41875016e-03, bound:  3.15548807e-01\n",
      "Epoch: 19925 mean train loss:  6.41785795e-03, bound:  3.15548748e-01\n",
      "Epoch: 19926 mean train loss:  6.41722418e-03, bound:  3.15548718e-01\n",
      "Epoch: 19927 mean train loss:  6.41664676e-03, bound:  3.15548718e-01\n",
      "Epoch: 19928 mean train loss:  6.41629286e-03, bound:  3.15548658e-01\n",
      "Epoch: 19929 mean train loss:  6.41587377e-03, bound:  3.15548658e-01\n",
      "Epoch: 19930 mean train loss:  6.41536107e-03, bound:  3.15548599e-01\n",
      "Epoch: 19931 mean train loss:  6.41479157e-03, bound:  3.15548599e-01\n",
      "Epoch: 19932 mean train loss:  6.41409587e-03, bound:  3.15548539e-01\n",
      "Epoch: 19933 mean train loss:  6.41333638e-03, bound:  3.15548539e-01\n",
      "Epoch: 19934 mean train loss:  6.41267886e-03, bound:  3.15548480e-01\n",
      "Epoch: 19935 mean train loss:  6.41202554e-03, bound:  3.15548480e-01\n",
      "Epoch: 19936 mean train loss:  6.41155569e-03, bound:  3.15548450e-01\n",
      "Epoch: 19937 mean train loss:  6.41100388e-03, bound:  3.15548420e-01\n",
      "Epoch: 19938 mean train loss:  6.41047116e-03, bound:  3.15548390e-01\n",
      "Epoch: 19939 mean train loss:  6.40992727e-03, bound:  3.15548360e-01\n",
      "Epoch: 19940 mean train loss:  6.40938897e-03, bound:  3.15548301e-01\n",
      "Epoch: 19941 mean train loss:  6.40874729e-03, bound:  3.15548301e-01\n",
      "Epoch: 19942 mean train loss:  6.40810374e-03, bound:  3.15548271e-01\n",
      "Epoch: 19943 mean train loss:  6.40752958e-03, bound:  3.15548241e-01\n",
      "Epoch: 19944 mean train loss:  6.40697731e-03, bound:  3.15548182e-01\n",
      "Epoch: 19945 mean train loss:  6.40645437e-03, bound:  3.15548182e-01\n",
      "Epoch: 19946 mean train loss:  6.40583225e-03, bound:  3.15548152e-01\n",
      "Epoch: 19947 mean train loss:  6.40530698e-03, bound:  3.15548122e-01\n",
      "Epoch: 19948 mean train loss:  6.40475098e-03, bound:  3.15548092e-01\n",
      "Epoch: 19949 mean train loss:  6.40419172e-03, bound:  3.15548062e-01\n",
      "Epoch: 19950 mean train loss:  6.40366459e-03, bound:  3.15548033e-01\n",
      "Epoch: 19951 mean train loss:  6.40300428e-03, bound:  3.15547973e-01\n",
      "Epoch: 19952 mean train loss:  6.40243758e-03, bound:  3.15547973e-01\n",
      "Epoch: 19953 mean train loss:  6.40191603e-03, bound:  3.15547973e-01\n",
      "Epoch: 19954 mean train loss:  6.40131347e-03, bound:  3.15547913e-01\n",
      "Epoch: 19955 mean train loss:  6.40071929e-03, bound:  3.15547913e-01\n",
      "Epoch: 19956 mean train loss:  6.40021265e-03, bound:  3.15547854e-01\n",
      "Epoch: 19957 mean train loss:  6.39959751e-03, bound:  3.15547824e-01\n",
      "Epoch: 19958 mean train loss:  6.39895815e-03, bound:  3.15547794e-01\n",
      "Epoch: 19959 mean train loss:  6.39843056e-03, bound:  3.15547794e-01\n",
      "Epoch: 19960 mean train loss:  6.39789551e-03, bound:  3.15547734e-01\n",
      "Epoch: 19961 mean train loss:  6.39733300e-03, bound:  3.15547734e-01\n",
      "Epoch: 19962 mean train loss:  6.39676908e-03, bound:  3.15547705e-01\n",
      "Epoch: 19963 mean train loss:  6.39619166e-03, bound:  3.15547675e-01\n",
      "Epoch: 19964 mean train loss:  6.39566360e-03, bound:  3.15547615e-01\n",
      "Epoch: 19965 mean train loss:  6.39505358e-03, bound:  3.15547585e-01\n",
      "Epoch: 19966 mean train loss:  6.39444403e-03, bound:  3.15547585e-01\n",
      "Epoch: 19967 mean train loss:  6.39388198e-03, bound:  3.15547526e-01\n",
      "Epoch: 19968 mean train loss:  6.39331760e-03, bound:  3.15547496e-01\n",
      "Epoch: 19969 mean train loss:  6.39282307e-03, bound:  3.15547466e-01\n",
      "Epoch: 19970 mean train loss:  6.39221957e-03, bound:  3.15547466e-01\n",
      "Epoch: 19971 mean train loss:  6.39167568e-03, bound:  3.15547407e-01\n",
      "Epoch: 19972 mean train loss:  6.39109686e-03, bound:  3.15547377e-01\n",
      "Epoch: 19973 mean train loss:  6.39049662e-03, bound:  3.15547347e-01\n",
      "Epoch: 19974 mean train loss:  6.38993690e-03, bound:  3.15547347e-01\n",
      "Epoch: 19975 mean train loss:  6.38933014e-03, bound:  3.15547287e-01\n",
      "Epoch: 19976 mean train loss:  6.38877461e-03, bound:  3.15547287e-01\n",
      "Epoch: 19977 mean train loss:  6.38815435e-03, bound:  3.15547258e-01\n",
      "Epoch: 19978 mean train loss:  6.38761558e-03, bound:  3.15547198e-01\n",
      "Epoch: 19979 mean train loss:  6.38707168e-03, bound:  3.15547168e-01\n",
      "Epoch: 19980 mean train loss:  6.38647005e-03, bound:  3.15547168e-01\n",
      "Epoch: 19981 mean train loss:  6.38592243e-03, bound:  3.15547138e-01\n",
      "Epoch: 19982 mean train loss:  6.38530264e-03, bound:  3.15547079e-01\n",
      "Epoch: 19983 mean train loss:  6.38478808e-03, bound:  3.15547049e-01\n",
      "Epoch: 19984 mean train loss:  6.38419762e-03, bound:  3.15547049e-01\n",
      "Epoch: 19985 mean train loss:  6.38367701e-03, bound:  3.15547019e-01\n",
      "Epoch: 19986 mean train loss:  6.38305536e-03, bound:  3.15546960e-01\n",
      "Epoch: 19987 mean train loss:  6.38253661e-03, bound:  3.15546960e-01\n",
      "Epoch: 19988 mean train loss:  6.38196152e-03, bound:  3.15546930e-01\n",
      "Epoch: 19989 mean train loss:  6.38141157e-03, bound:  3.15546900e-01\n",
      "Epoch: 19990 mean train loss:  6.38081972e-03, bound:  3.15546900e-01\n",
      "Epoch: 19991 mean train loss:  6.38025627e-03, bound:  3.15546840e-01\n",
      "Epoch: 19992 mean train loss:  6.37967093e-03, bound:  3.15546811e-01\n",
      "Epoch: 19993 mean train loss:  6.37911540e-03, bound:  3.15546781e-01\n",
      "Epoch: 19994 mean train loss:  6.37855707e-03, bound:  3.15546751e-01\n",
      "Epoch: 19995 mean train loss:  6.37801317e-03, bound:  3.15546721e-01\n",
      "Epoch: 19996 mean train loss:  6.37734495e-03, bound:  3.15546691e-01\n",
      "Epoch: 19997 mean train loss:  6.37677824e-03, bound:  3.15546662e-01\n",
      "Epoch: 19998 mean train loss:  6.37620501e-03, bound:  3.15546602e-01\n",
      "Epoch: 19999 mean train loss:  6.37563039e-03, bound:  3.15546602e-01\n",
      "Epoch: 20000 mean train loss:  6.37512607e-03, bound:  3.15546572e-01\n",
      "Epoch: 20001 mean train loss:  6.37451373e-03, bound:  3.15546572e-01\n",
      "Epoch: 20002 mean train loss:  6.37398753e-03, bound:  3.15546513e-01\n",
      "Epoch: 20003 mean train loss:  6.37340639e-03, bound:  3.15546483e-01\n",
      "Epoch: 20004 mean train loss:  6.37281360e-03, bound:  3.15546453e-01\n",
      "Epoch: 20005 mean train loss:  6.37228228e-03, bound:  3.15546423e-01\n",
      "Epoch: 20006 mean train loss:  6.37168903e-03, bound:  3.15546364e-01\n",
      "Epoch: 20007 mean train loss:  6.37119589e-03, bound:  3.15546364e-01\n",
      "Epoch: 20008 mean train loss:  6.37058960e-03, bound:  3.15546334e-01\n",
      "Epoch: 20009 mean train loss:  6.36998750e-03, bound:  3.15546304e-01\n",
      "Epoch: 20010 mean train loss:  6.36947481e-03, bound:  3.15546274e-01\n",
      "Epoch: 20011 mean train loss:  6.36890018e-03, bound:  3.15546274e-01\n",
      "Epoch: 20012 mean train loss:  6.36839541e-03, bound:  3.15546215e-01\n",
      "Epoch: 20013 mean train loss:  6.36775559e-03, bound:  3.15546155e-01\n",
      "Epoch: 20014 mean train loss:  6.36726618e-03, bound:  3.15546155e-01\n",
      "Epoch: 20015 mean train loss:  6.36671484e-03, bound:  3.15546155e-01\n",
      "Epoch: 20016 mean train loss:  6.36618678e-03, bound:  3.15546095e-01\n",
      "Epoch: 20017 mean train loss:  6.36568759e-03, bound:  3.15546066e-01\n",
      "Epoch: 20018 mean train loss:  6.36518607e-03, bound:  3.15546036e-01\n",
      "Epoch: 20019 mean train loss:  6.36475533e-03, bound:  3.15546036e-01\n",
      "Epoch: 20020 mean train loss:  6.36436418e-03, bound:  3.15545946e-01\n",
      "Epoch: 20021 mean train loss:  6.36399956e-03, bound:  3.15545946e-01\n",
      "Epoch: 20022 mean train loss:  6.36372156e-03, bound:  3.15545917e-01\n",
      "Epoch: 20023 mean train loss:  6.36355858e-03, bound:  3.15545917e-01\n",
      "Epoch: 20024 mean train loss:  6.36339700e-03, bound:  3.15545827e-01\n",
      "Epoch: 20025 mean train loss:  6.36354648e-03, bound:  3.15545857e-01\n",
      "Epoch: 20026 mean train loss:  6.36371365e-03, bound:  3.15545768e-01\n",
      "Epoch: 20027 mean train loss:  6.36425009e-03, bound:  3.15545797e-01\n",
      "Epoch: 20028 mean train loss:  6.36511203e-03, bound:  3.15545708e-01\n",
      "Epoch: 20029 mean train loss:  6.36642333e-03, bound:  3.15545738e-01\n",
      "Epoch: 20030 mean train loss:  6.36802567e-03, bound:  3.15545619e-01\n",
      "Epoch: 20031 mean train loss:  6.36977982e-03, bound:  3.15545708e-01\n",
      "Epoch: 20032 mean train loss:  6.37126435e-03, bound:  3.15545589e-01\n",
      "Epoch: 20033 mean train loss:  6.37198472e-03, bound:  3.15545619e-01\n",
      "Epoch: 20034 mean train loss:  6.37118518e-03, bound:  3.15545499e-01\n",
      "Epoch: 20035 mean train loss:  6.36860169e-03, bound:  3.15545589e-01\n",
      "Epoch: 20036 mean train loss:  6.36440562e-03, bound:  3.15545470e-01\n",
      "Epoch: 20037 mean train loss:  6.35959301e-03, bound:  3.15545499e-01\n",
      "Epoch: 20038 mean train loss:  6.35553058e-03, bound:  3.15545470e-01\n",
      "Epoch: 20039 mean train loss:  6.35309471e-03, bound:  3.15545470e-01\n",
      "Epoch: 20040 mean train loss:  6.35262486e-03, bound:  3.15545440e-01\n",
      "Epoch: 20041 mean train loss:  6.35350263e-03, bound:  3.15545350e-01\n",
      "Epoch: 20042 mean train loss:  6.35477621e-03, bound:  3.15545350e-01\n",
      "Epoch: 20043 mean train loss:  6.35569124e-03, bound:  3.15545291e-01\n",
      "Epoch: 20044 mean train loss:  6.35545142e-03, bound:  3.15545321e-01\n",
      "Epoch: 20045 mean train loss:  6.35398226e-03, bound:  3.15545231e-01\n",
      "Epoch: 20046 mean train loss:  6.35191426e-03, bound:  3.15545261e-01\n",
      "Epoch: 20047 mean train loss:  6.34965673e-03, bound:  3.15545171e-01\n",
      "Epoch: 20048 mean train loss:  6.34803297e-03, bound:  3.15545171e-01\n",
      "Epoch: 20049 mean train loss:  6.34743785e-03, bound:  3.15545142e-01\n",
      "Epoch: 20050 mean train loss:  6.34749513e-03, bound:  3.15545112e-01\n",
      "Epoch: 20051 mean train loss:  6.34778989e-03, bound:  3.15545112e-01\n",
      "Epoch: 20052 mean train loss:  6.34789653e-03, bound:  3.15545052e-01\n",
      "Epoch: 20053 mean train loss:  6.34744437e-03, bound:  3.15545052e-01\n",
      "Epoch: 20054 mean train loss:  6.34659687e-03, bound:  3.15545022e-01\n",
      "Epoch: 20055 mean train loss:  6.34522457e-03, bound:  3.15545022e-01\n",
      "Epoch: 20056 mean train loss:  6.34399243e-03, bound:  3.15544933e-01\n",
      "Epoch: 20057 mean train loss:  6.34298055e-03, bound:  3.15544933e-01\n",
      "Epoch: 20058 mean train loss:  6.34240173e-03, bound:  3.15544903e-01\n",
      "Epoch: 20059 mean train loss:  6.34207483e-03, bound:  3.15544844e-01\n",
      "Epoch: 20060 mean train loss:  6.34184107e-03, bound:  3.15544814e-01\n",
      "Epoch: 20061 mean train loss:  6.34150952e-03, bound:  3.15544784e-01\n",
      "Epoch: 20062 mean train loss:  6.34095958e-03, bound:  3.15544784e-01\n",
      "Epoch: 20063 mean train loss:  6.34033140e-03, bound:  3.15544724e-01\n",
      "Epoch: 20064 mean train loss:  6.33948483e-03, bound:  3.15544724e-01\n",
      "Epoch: 20065 mean train loss:  6.33880263e-03, bound:  3.15544695e-01\n",
      "Epoch: 20066 mean train loss:  6.33803569e-03, bound:  3.15544665e-01\n",
      "Epoch: 20067 mean train loss:  6.33741543e-03, bound:  3.15544665e-01\n",
      "Epoch: 20068 mean train loss:  6.33692509e-03, bound:  3.15544605e-01\n",
      "Epoch: 20069 mean train loss:  6.33642916e-03, bound:  3.15544575e-01\n",
      "Epoch: 20070 mean train loss:  6.33603986e-03, bound:  3.15544546e-01\n",
      "Epoch: 20071 mean train loss:  6.33552764e-03, bound:  3.15544546e-01\n",
      "Epoch: 20072 mean train loss:  6.33495767e-03, bound:  3.15544486e-01\n",
      "Epoch: 20073 mean train loss:  6.33435789e-03, bound:  3.15544456e-01\n",
      "Epoch: 20074 mean train loss:  6.33373670e-03, bound:  3.15544456e-01\n",
      "Epoch: 20075 mean train loss:  6.33311551e-03, bound:  3.15544456e-01\n",
      "Epoch: 20076 mean train loss:  6.33244356e-03, bound:  3.15544367e-01\n",
      "Epoch: 20077 mean train loss:  6.33191690e-03, bound:  3.15544337e-01\n",
      "Epoch: 20078 mean train loss:  6.33125845e-03, bound:  3.15544337e-01\n",
      "Epoch: 20079 mean train loss:  6.33071968e-03, bound:  3.15544248e-01\n",
      "Epoch: 20080 mean train loss:  6.33023167e-03, bound:  3.15544248e-01\n",
      "Epoch: 20081 mean train loss:  6.32969011e-03, bound:  3.15544248e-01\n",
      "Epoch: 20082 mean train loss:  6.32914389e-03, bound:  3.15544218e-01\n",
      "Epoch: 20083 mean train loss:  6.32854365e-03, bound:  3.15544158e-01\n",
      "Epoch: 20084 mean train loss:  6.32801047e-03, bound:  3.15544158e-01\n",
      "Epoch: 20085 mean train loss:  6.32746657e-03, bound:  3.15544128e-01\n",
      "Epoch: 20086 mean train loss:  6.32688031e-03, bound:  3.15544069e-01\n",
      "Epoch: 20087 mean train loss:  6.32633874e-03, bound:  3.15544069e-01\n",
      "Epoch: 20088 mean train loss:  6.32578554e-03, bound:  3.15544039e-01\n",
      "Epoch: 20089 mean train loss:  6.32517971e-03, bound:  3.15544009e-01\n",
      "Epoch: 20090 mean train loss:  6.32465305e-03, bound:  3.15543950e-01\n",
      "Epoch: 20091 mean train loss:  6.32414967e-03, bound:  3.15543950e-01\n",
      "Epoch: 20092 mean train loss:  6.32358901e-03, bound:  3.15543920e-01\n",
      "Epoch: 20093 mean train loss:  6.32305024e-03, bound:  3.15543890e-01\n",
      "Epoch: 20094 mean train loss:  6.32247422e-03, bound:  3.15543830e-01\n",
      "Epoch: 20095 mean train loss:  6.32194802e-03, bound:  3.15543830e-01\n",
      "Epoch: 20096 mean train loss:  6.32135943e-03, bound:  3.15543801e-01\n",
      "Epoch: 20097 mean train loss:  6.32082671e-03, bound:  3.15543771e-01\n",
      "Epoch: 20098 mean train loss:  6.32023672e-03, bound:  3.15543711e-01\n",
      "Epoch: 20099 mean train loss:  6.31969795e-03, bound:  3.15543711e-01\n",
      "Epoch: 20100 mean train loss:  6.31916476e-03, bound:  3.15543681e-01\n",
      "Epoch: 20101 mean train loss:  6.31850306e-03, bound:  3.15543652e-01\n",
      "Epoch: 20102 mean train loss:  6.31803088e-03, bound:  3.15543622e-01\n",
      "Epoch: 20103 mean train loss:  6.31747395e-03, bound:  3.15543622e-01\n",
      "Epoch: 20104 mean train loss:  6.31685229e-03, bound:  3.15543562e-01\n",
      "Epoch: 20105 mean train loss:  6.31629303e-03, bound:  3.15543532e-01\n",
      "Epoch: 20106 mean train loss:  6.31580502e-03, bound:  3.15543532e-01\n",
      "Epoch: 20107 mean train loss:  6.31521456e-03, bound:  3.15543503e-01\n",
      "Epoch: 20108 mean train loss:  6.31466648e-03, bound:  3.15543473e-01\n",
      "Epoch: 20109 mean train loss:  6.31411374e-03, bound:  3.15543413e-01\n",
      "Epoch: 20110 mean train loss:  6.31358381e-03, bound:  3.15543413e-01\n",
      "Epoch: 20111 mean train loss:  6.31304365e-03, bound:  3.15543383e-01\n",
      "Epoch: 20112 mean train loss:  6.31249323e-03, bound:  3.15543354e-01\n",
      "Epoch: 20113 mean train loss:  6.31200429e-03, bound:  3.15543324e-01\n",
      "Epoch: 20114 mean train loss:  6.31145341e-03, bound:  3.15543324e-01\n",
      "Epoch: 20115 mean train loss:  6.31090999e-03, bound:  3.15543264e-01\n",
      "Epoch: 20116 mean train loss:  6.31023524e-03, bound:  3.15543234e-01\n",
      "Epoch: 20117 mean train loss:  6.30972488e-03, bound:  3.15543205e-01\n",
      "Epoch: 20118 mean train loss:  6.30918238e-03, bound:  3.15543145e-01\n",
      "Epoch: 20119 mean train loss:  6.30864408e-03, bound:  3.15543145e-01\n",
      "Epoch: 20120 mean train loss:  6.30799215e-03, bound:  3.15543115e-01\n",
      "Epoch: 20121 mean train loss:  6.30749064e-03, bound:  3.15543085e-01\n",
      "Epoch: 20122 mean train loss:  6.30689645e-03, bound:  3.15543056e-01\n",
      "Epoch: 20123 mean train loss:  6.30639214e-03, bound:  3.15543026e-01\n",
      "Epoch: 20124 mean train loss:  6.30577514e-03, bound:  3.15542996e-01\n",
      "Epoch: 20125 mean train loss:  6.30527455e-03, bound:  3.15542966e-01\n",
      "Epoch: 20126 mean train loss:  6.30469387e-03, bound:  3.15542907e-01\n",
      "Epoch: 20127 mean train loss:  6.30416395e-03, bound:  3.15542907e-01\n",
      "Epoch: 20128 mean train loss:  6.30363915e-03, bound:  3.15542907e-01\n",
      "Epoch: 20129 mean train loss:  6.30305102e-03, bound:  3.15542817e-01\n",
      "Epoch: 20130 mean train loss:  6.30246056e-03, bound:  3.15542817e-01\n",
      "Epoch: 20131 mean train loss:  6.30195765e-03, bound:  3.15542817e-01\n",
      "Epoch: 20132 mean train loss:  6.30143704e-03, bound:  3.15542787e-01\n",
      "Epoch: 20133 mean train loss:  6.30084518e-03, bound:  3.15542698e-01\n",
      "Epoch: 20134 mean train loss:  6.30031759e-03, bound:  3.15542698e-01\n",
      "Epoch: 20135 mean train loss:  6.29972713e-03, bound:  3.15542668e-01\n",
      "Epoch: 20136 mean train loss:  6.29915763e-03, bound:  3.15542668e-01\n",
      "Epoch: 20137 mean train loss:  6.29860256e-03, bound:  3.15542608e-01\n",
      "Epoch: 20138 mean train loss:  6.29809126e-03, bound:  3.15542579e-01\n",
      "Epoch: 20139 mean train loss:  6.29753713e-03, bound:  3.15542549e-01\n",
      "Epoch: 20140 mean train loss:  6.29705936e-03, bound:  3.15542549e-01\n",
      "Epoch: 20141 mean train loss:  6.29653083e-03, bound:  3.15542519e-01\n",
      "Epoch: 20142 mean train loss:  6.29596366e-03, bound:  3.15542489e-01\n",
      "Epoch: 20143 mean train loss:  6.29540067e-03, bound:  3.15542430e-01\n",
      "Epoch: 20144 mean train loss:  6.29489357e-03, bound:  3.15542430e-01\n",
      "Epoch: 20145 mean train loss:  6.29452942e-03, bound:  3.15542370e-01\n",
      "Epoch: 20146 mean train loss:  6.29410427e-03, bound:  3.15542340e-01\n",
      "Epoch: 20147 mean train loss:  6.29380904e-03, bound:  3.15542310e-01\n",
      "Epoch: 20148 mean train loss:  6.29362045e-03, bound:  3.15542310e-01\n",
      "Epoch: 20149 mean train loss:  6.29343418e-03, bound:  3.15542251e-01\n",
      "Epoch: 20150 mean train loss:  6.29345886e-03, bound:  3.15542251e-01\n",
      "Epoch: 20151 mean train loss:  6.29376899e-03, bound:  3.15542221e-01\n",
      "Epoch: 20152 mean train loss:  6.29433105e-03, bound:  3.15542221e-01\n",
      "Epoch: 20153 mean train loss:  6.29533269e-03, bound:  3.15542102e-01\n",
      "Epoch: 20154 mean train loss:  6.29674597e-03, bound:  3.15542132e-01\n",
      "Epoch: 20155 mean train loss:  6.29852572e-03, bound:  3.15542102e-01\n",
      "Epoch: 20156 mean train loss:  6.30053785e-03, bound:  3.15542102e-01\n",
      "Epoch: 20157 mean train loss:  6.30248711e-03, bound:  3.15541983e-01\n",
      "Epoch: 20158 mean train loss:  6.30343566e-03, bound:  3.15542042e-01\n",
      "Epoch: 20159 mean train loss:  6.30282424e-03, bound:  3.15541923e-01\n",
      "Epoch: 20160 mean train loss:  6.30006893e-03, bound:  3.15541983e-01\n",
      "Epoch: 20161 mean train loss:  6.29564375e-03, bound:  3.15541893e-01\n",
      "Epoch: 20162 mean train loss:  6.29044184e-03, bound:  3.15541923e-01\n",
      "Epoch: 20163 mean train loss:  6.28607720e-03, bound:  3.15541863e-01\n",
      "Epoch: 20164 mean train loss:  6.28369953e-03, bound:  3.15541863e-01\n",
      "Epoch: 20165 mean train loss:  6.28339499e-03, bound:  3.15541863e-01\n",
      "Epoch: 20166 mean train loss:  6.28451305e-03, bound:  3.15541774e-01\n",
      "Epoch: 20167 mean train loss:  6.28601573e-03, bound:  3.15541804e-01\n",
      "Epoch: 20168 mean train loss:  6.28665136e-03, bound:  3.15541685e-01\n",
      "Epoch: 20169 mean train loss:  6.28615590e-03, bound:  3.15541744e-01\n",
      "Epoch: 20170 mean train loss:  6.28437195e-03, bound:  3.15541655e-01\n",
      "Epoch: 20171 mean train loss:  6.28198311e-03, bound:  3.15541685e-01\n",
      "Epoch: 20172 mean train loss:  6.27993839e-03, bound:  3.15541595e-01\n",
      "Epoch: 20173 mean train loss:  6.27871836e-03, bound:  3.15541595e-01\n",
      "Epoch: 20174 mean train loss:  6.27841707e-03, bound:  3.15541565e-01\n",
      "Epoch: 20175 mean train loss:  6.27871137e-03, bound:  3.15541536e-01\n",
      "Epoch: 20176 mean train loss:  6.27898239e-03, bound:  3.15541536e-01\n",
      "Epoch: 20177 mean train loss:  6.27893955e-03, bound:  3.15541476e-01\n",
      "Epoch: 20178 mean train loss:  6.27822941e-03, bound:  3.15541476e-01\n",
      "Epoch: 20179 mean train loss:  6.27700891e-03, bound:  3.15541416e-01\n",
      "Epoch: 20180 mean train loss:  6.27571670e-03, bound:  3.15541416e-01\n",
      "Epoch: 20181 mean train loss:  6.27454137e-03, bound:  3.15541416e-01\n",
      "Epoch: 20182 mean train loss:  6.27384894e-03, bound:  3.15541357e-01\n",
      "Epoch: 20183 mean train loss:  6.27349038e-03, bound:  3.15541357e-01\n",
      "Epoch: 20184 mean train loss:  6.27327804e-03, bound:  3.15541267e-01\n",
      "Epoch: 20185 mean train loss:  6.27307640e-03, bound:  3.15541267e-01\n",
      "Epoch: 20186 mean train loss:  6.27278630e-03, bound:  3.15541238e-01\n",
      "Epoch: 20187 mean train loss:  6.27210224e-03, bound:  3.15541238e-01\n",
      "Epoch: 20188 mean train loss:  6.27127942e-03, bound:  3.15541178e-01\n",
      "Epoch: 20189 mean train loss:  6.27044262e-03, bound:  3.15541148e-01\n",
      "Epoch: 20190 mean train loss:  6.26964308e-03, bound:  3.15541118e-01\n",
      "Epoch: 20191 mean train loss:  6.26903353e-03, bound:  3.15541118e-01\n",
      "Epoch: 20192 mean train loss:  6.26853248e-03, bound:  3.15541089e-01\n",
      "Epoch: 20193 mean train loss:  6.26814179e-03, bound:  3.15541029e-01\n",
      "Epoch: 20194 mean train loss:  6.26780558e-03, bound:  3.15540999e-01\n",
      "Epoch: 20195 mean train loss:  6.26729010e-03, bound:  3.15540969e-01\n",
      "Epoch: 20196 mean train loss:  6.26676017e-03, bound:  3.15540969e-01\n",
      "Epoch: 20197 mean train loss:  6.26612222e-03, bound:  3.15540910e-01\n",
      "Epoch: 20198 mean train loss:  6.26548845e-03, bound:  3.15540910e-01\n",
      "Epoch: 20199 mean train loss:  6.26478298e-03, bound:  3.15540880e-01\n",
      "Epoch: 20200 mean train loss:  6.26421208e-03, bound:  3.15540850e-01\n",
      "Epoch: 20201 mean train loss:  6.26364071e-03, bound:  3.15540820e-01\n",
      "Epoch: 20202 mean train loss:  6.26320858e-03, bound:  3.15540761e-01\n",
      "Epoch: 20203 mean train loss:  6.26270799e-03, bound:  3.15540761e-01\n",
      "Epoch: 20204 mean train loss:  6.26222230e-03, bound:  3.15540731e-01\n",
      "Epoch: 20205 mean train loss:  6.26167189e-03, bound:  3.15540701e-01\n",
      "Epoch: 20206 mean train loss:  6.26116199e-03, bound:  3.15540671e-01\n",
      "Epoch: 20207 mean train loss:  6.26050308e-03, bound:  3.15540671e-01\n",
      "Epoch: 20208 mean train loss:  6.25998667e-03, bound:  3.15540612e-01\n",
      "Epoch: 20209 mean train loss:  6.25933986e-03, bound:  3.15540582e-01\n",
      "Epoch: 20210 mean train loss:  6.25880109e-03, bound:  3.15540582e-01\n",
      "Epoch: 20211 mean train loss:  6.25826698e-03, bound:  3.15540552e-01\n",
      "Epoch: 20212 mean train loss:  6.25765882e-03, bound:  3.15540463e-01\n",
      "Epoch: 20213 mean train loss:  6.25712238e-03, bound:  3.15540463e-01\n",
      "Epoch: 20214 mean train loss:  6.25670468e-03, bound:  3.15540463e-01\n",
      "Epoch: 20215 mean train loss:  6.25615800e-03, bound:  3.15540433e-01\n",
      "Epoch: 20216 mean train loss:  6.25563832e-03, bound:  3.15540403e-01\n",
      "Epoch: 20217 mean train loss:  6.25506649e-03, bound:  3.15540373e-01\n",
      "Epoch: 20218 mean train loss:  6.25455379e-03, bound:  3.15540344e-01\n",
      "Epoch: 20219 mean train loss:  6.25400431e-03, bound:  3.15540314e-01\n",
      "Epoch: 20220 mean train loss:  6.25337847e-03, bound:  3.15540284e-01\n",
      "Epoch: 20221 mean train loss:  6.25290256e-03, bound:  3.15540284e-01\n",
      "Epoch: 20222 mean train loss:  6.25229301e-03, bound:  3.15540224e-01\n",
      "Epoch: 20223 mean train loss:  6.25168532e-03, bound:  3.15540195e-01\n",
      "Epoch: 20224 mean train loss:  6.25126669e-03, bound:  3.15540165e-01\n",
      "Epoch: 20225 mean train loss:  6.25065714e-03, bound:  3.15540165e-01\n",
      "Epoch: 20226 mean train loss:  6.25014910e-03, bound:  3.15540105e-01\n",
      "Epoch: 20227 mean train loss:  6.24961825e-03, bound:  3.15540075e-01\n",
      "Epoch: 20228 mean train loss:  6.24910463e-03, bound:  3.15540075e-01\n",
      "Epoch: 20229 mean train loss:  6.24857843e-03, bound:  3.15540016e-01\n",
      "Epoch: 20230 mean train loss:  6.24804338e-03, bound:  3.15539986e-01\n",
      "Epoch: 20231 mean train loss:  6.24749996e-03, bound:  3.15539986e-01\n",
      "Epoch: 20232 mean train loss:  6.24690298e-03, bound:  3.15539956e-01\n",
      "Epoch: 20233 mean train loss:  6.24634419e-03, bound:  3.15539896e-01\n",
      "Epoch: 20234 mean train loss:  6.24577329e-03, bound:  3.15539867e-01\n",
      "Epoch: 20235 mean train loss:  6.24520611e-03, bound:  3.15539867e-01\n",
      "Epoch: 20236 mean train loss:  6.24468364e-03, bound:  3.15539867e-01\n",
      "Epoch: 20237 mean train loss:  6.24419330e-03, bound:  3.15539807e-01\n",
      "Epoch: 20238 mean train loss:  6.24366244e-03, bound:  3.15539747e-01\n",
      "Epoch: 20239 mean train loss:  6.24311715e-03, bound:  3.15539747e-01\n",
      "Epoch: 20240 mean train loss:  6.24257000e-03, bound:  3.15539747e-01\n",
      "Epoch: 20241 mean train loss:  6.24203496e-03, bound:  3.15539688e-01\n",
      "Epoch: 20242 mean train loss:  6.24145661e-03, bound:  3.15539658e-01\n",
      "Epoch: 20243 mean train loss:  6.24093832e-03, bound:  3.15539658e-01\n",
      "Epoch: 20244 mean train loss:  6.24040654e-03, bound:  3.15539628e-01\n",
      "Epoch: 20245 mean train loss:  6.23983145e-03, bound:  3.15539569e-01\n",
      "Epoch: 20246 mean train loss:  6.23930525e-03, bound:  3.15539569e-01\n",
      "Epoch: 20247 mean train loss:  6.23876974e-03, bound:  3.15539539e-01\n",
      "Epoch: 20248 mean train loss:  6.23823795e-03, bound:  3.15539509e-01\n",
      "Epoch: 20249 mean train loss:  6.23771502e-03, bound:  3.15539449e-01\n",
      "Epoch: 20250 mean train loss:  6.23713434e-03, bound:  3.15539449e-01\n",
      "Epoch: 20251 mean train loss:  6.23657834e-03, bound:  3.15539420e-01\n",
      "Epoch: 20252 mean train loss:  6.23600278e-03, bound:  3.15539390e-01\n",
      "Epoch: 20253 mean train loss:  6.23554457e-03, bound:  3.15539330e-01\n",
      "Epoch: 20254 mean train loss:  6.23503467e-03, bound:  3.15539330e-01\n",
      "Epoch: 20255 mean train loss:  6.23446750e-03, bound:  3.15539300e-01\n",
      "Epoch: 20256 mean train loss:  6.23397576e-03, bound:  3.15539241e-01\n",
      "Epoch: 20257 mean train loss:  6.23341929e-03, bound:  3.15539211e-01\n",
      "Epoch: 20258 mean train loss:  6.23291172e-03, bound:  3.15539211e-01\n",
      "Epoch: 20259 mean train loss:  6.23245072e-03, bound:  3.15539181e-01\n",
      "Epoch: 20260 mean train loss:  6.23193104e-03, bound:  3.15539181e-01\n",
      "Epoch: 20261 mean train loss:  6.23133872e-03, bound:  3.15539122e-01\n",
      "Epoch: 20262 mean train loss:  6.23092614e-03, bound:  3.15539122e-01\n",
      "Epoch: 20263 mean train loss:  6.23045024e-03, bound:  3.15539062e-01\n",
      "Epoch: 20264 mean train loss:  6.23003859e-03, bound:  3.15539062e-01\n",
      "Epoch: 20265 mean train loss:  6.22963067e-03, bound:  3.15539002e-01\n",
      "Epoch: 20266 mean train loss:  6.22928003e-03, bound:  3.15539002e-01\n",
      "Epoch: 20267 mean train loss:  6.22896943e-03, bound:  3.15538943e-01\n",
      "Epoch: 20268 mean train loss:  6.22889819e-03, bound:  3.15538943e-01\n",
      "Epoch: 20269 mean train loss:  6.22878596e-03, bound:  3.15538883e-01\n",
      "Epoch: 20270 mean train loss:  6.22877199e-03, bound:  3.15538883e-01\n",
      "Epoch: 20271 mean train loss:  6.22896198e-03, bound:  3.15538794e-01\n",
      "Epoch: 20272 mean train loss:  6.22923905e-03, bound:  3.15538853e-01\n",
      "Epoch: 20273 mean train loss:  6.22970378e-03, bound:  3.15538764e-01\n",
      "Epoch: 20274 mean train loss:  6.23019924e-03, bound:  3.15538764e-01\n",
      "Epoch: 20275 mean train loss:  6.23097643e-03, bound:  3.15538675e-01\n",
      "Epoch: 20276 mean train loss:  6.23156410e-03, bound:  3.15538734e-01\n",
      "Epoch: 20277 mean train loss:  6.23198273e-03, bound:  3.15538645e-01\n",
      "Epoch: 20278 mean train loss:  6.23197202e-03, bound:  3.15538645e-01\n",
      "Epoch: 20279 mean train loss:  6.23134244e-03, bound:  3.15538555e-01\n",
      "Epoch: 20280 mean train loss:  6.22973638e-03, bound:  3.15538645e-01\n",
      "Epoch: 20281 mean train loss:  6.22738432e-03, bound:  3.15538555e-01\n",
      "Epoch: 20282 mean train loss:  6.22471003e-03, bound:  3.15538555e-01\n",
      "Epoch: 20283 mean train loss:  6.22206181e-03, bound:  3.15538496e-01\n",
      "Epoch: 20284 mean train loss:  6.21974748e-03, bound:  3.15538496e-01\n",
      "Epoch: 20285 mean train loss:  6.21833187e-03, bound:  3.15538436e-01\n",
      "Epoch: 20286 mean train loss:  6.21772790e-03, bound:  3.15538436e-01\n",
      "Epoch: 20287 mean train loss:  6.21763850e-03, bound:  3.15538406e-01\n",
      "Epoch: 20288 mean train loss:  6.21789182e-03, bound:  3.15538347e-01\n",
      "Epoch: 20289 mean train loss:  6.21818006e-03, bound:  3.15538347e-01\n",
      "Epoch: 20290 mean train loss:  6.21828577e-03, bound:  3.15538317e-01\n",
      "Epoch: 20291 mean train loss:  6.21796399e-03, bound:  3.15538317e-01\n",
      "Epoch: 20292 mean train loss:  6.21726364e-03, bound:  3.15538228e-01\n",
      "Epoch: 20293 mean train loss:  6.21618703e-03, bound:  3.15538228e-01\n",
      "Epoch: 20294 mean train loss:  6.21497119e-03, bound:  3.15538198e-01\n",
      "Epoch: 20295 mean train loss:  6.21369015e-03, bound:  3.15538198e-01\n",
      "Epoch: 20296 mean train loss:  6.21264661e-03, bound:  3.15538168e-01\n",
      "Epoch: 20297 mean train loss:  6.21191971e-03, bound:  3.15538168e-01\n",
      "Epoch: 20298 mean train loss:  6.21132739e-03, bound:  3.15538079e-01\n",
      "Epoch: 20299 mean train loss:  6.21094834e-03, bound:  3.15538049e-01\n",
      "Epoch: 20300 mean train loss:  6.21067081e-03, bound:  3.15538049e-01\n",
      "Epoch: 20301 mean train loss:  6.21038396e-03, bound:  3.15538019e-01\n",
      "Epoch: 20302 mean train loss:  6.21000677e-03, bound:  3.15537989e-01\n",
      "Epoch: 20303 mean train loss:  6.20950107e-03, bound:  3.15537959e-01\n",
      "Epoch: 20304 mean train loss:  6.20888360e-03, bound:  3.15537930e-01\n",
      "Epoch: 20305 mean train loss:  6.20815251e-03, bound:  3.15537900e-01\n",
      "Epoch: 20306 mean train loss:  6.20739302e-03, bound:  3.15537870e-01\n",
      "Epoch: 20307 mean train loss:  6.20661583e-03, bound:  3.15537840e-01\n",
      "Epoch: 20308 mean train loss:  6.20602863e-03, bound:  3.15537840e-01\n",
      "Epoch: 20309 mean train loss:  6.20551687e-03, bound:  3.15537781e-01\n",
      "Epoch: 20310 mean train loss:  6.20497763e-03, bound:  3.15537751e-01\n",
      "Epoch: 20311 mean train loss:  6.20449893e-03, bound:  3.15537751e-01\n",
      "Epoch: 20312 mean train loss:  6.20409474e-03, bound:  3.15537721e-01\n",
      "Epoch: 20313 mean train loss:  6.20360859e-03, bound:  3.15537661e-01\n",
      "Epoch: 20314 mean train loss:  6.20317226e-03, bound:  3.15537632e-01\n",
      "Epoch: 20315 mean train loss:  6.20257528e-03, bound:  3.15537632e-01\n",
      "Epoch: 20316 mean train loss:  6.20207237e-03, bound:  3.15537602e-01\n",
      "Epoch: 20317 mean train loss:  6.20148191e-03, bound:  3.15537602e-01\n",
      "Epoch: 20318 mean train loss:  6.20080717e-03, bound:  3.15537542e-01\n",
      "Epoch: 20319 mean train loss:  6.20026048e-03, bound:  3.15537512e-01\n",
      "Epoch: 20320 mean train loss:  6.19969750e-03, bound:  3.15537483e-01\n",
      "Epoch: 20321 mean train loss:  6.19912054e-03, bound:  3.15537453e-01\n",
      "Epoch: 20322 mean train loss:  6.19854871e-03, bound:  3.15537423e-01\n",
      "Epoch: 20323 mean train loss:  6.19799085e-03, bound:  3.15537393e-01\n",
      "Epoch: 20324 mean train loss:  6.19750051e-03, bound:  3.15537393e-01\n",
      "Epoch: 20325 mean train loss:  6.19704509e-03, bound:  3.15537333e-01\n",
      "Epoch: 20326 mean train loss:  6.19652728e-03, bound:  3.15537333e-01\n",
      "Epoch: 20327 mean train loss:  6.19597500e-03, bound:  3.15537304e-01\n",
      "Epoch: 20328 mean train loss:  6.19548652e-03, bound:  3.15537304e-01\n",
      "Epoch: 20329 mean train loss:  6.19489001e-03, bound:  3.15537214e-01\n",
      "Epoch: 20330 mean train loss:  6.19432330e-03, bound:  3.15537214e-01\n",
      "Epoch: 20331 mean train loss:  6.19392376e-03, bound:  3.15537184e-01\n",
      "Epoch: 20332 mean train loss:  6.19340874e-03, bound:  3.15537155e-01\n",
      "Epoch: 20333 mean train loss:  6.19275402e-03, bound:  3.15537095e-01\n",
      "Epoch: 20334 mean train loss:  6.19231816e-03, bound:  3.15537095e-01\n",
      "Epoch: 20335 mean train loss:  6.19173935e-03, bound:  3.15537065e-01\n",
      "Epoch: 20336 mean train loss:  6.19121268e-03, bound:  3.15537065e-01\n",
      "Epoch: 20337 mean train loss:  6.19066413e-03, bound:  3.15537035e-01\n",
      "Epoch: 20338 mean train loss:  6.19017286e-03, bound:  3.15536976e-01\n",
      "Epoch: 20339 mean train loss:  6.18959125e-03, bound:  3.15536946e-01\n",
      "Epoch: 20340 mean train loss:  6.18906971e-03, bound:  3.15536946e-01\n",
      "Epoch: 20341 mean train loss:  6.18848391e-03, bound:  3.15536886e-01\n",
      "Epoch: 20342 mean train loss:  6.18796144e-03, bound:  3.15536857e-01\n",
      "Epoch: 20343 mean train loss:  6.18743664e-03, bound:  3.15536827e-01\n",
      "Epoch: 20344 mean train loss:  6.18690951e-03, bound:  3.15536827e-01\n",
      "Epoch: 20345 mean train loss:  6.18643966e-03, bound:  3.15536767e-01\n",
      "Epoch: 20346 mean train loss:  6.18580310e-03, bound:  3.15536767e-01\n",
      "Epoch: 20347 mean train loss:  6.18527550e-03, bound:  3.15536737e-01\n",
      "Epoch: 20348 mean train loss:  6.18483266e-03, bound:  3.15536737e-01\n",
      "Epoch: 20349 mean train loss:  6.18431531e-03, bound:  3.15536648e-01\n",
      "Epoch: 20350 mean train loss:  6.18374115e-03, bound:  3.15536648e-01\n",
      "Epoch: 20351 mean train loss:  6.18321309e-03, bound:  3.15536618e-01\n",
      "Epoch: 20352 mean train loss:  6.18276931e-03, bound:  3.15536618e-01\n",
      "Epoch: 20353 mean train loss:  6.18222170e-03, bound:  3.15536529e-01\n",
      "Epoch: 20354 mean train loss:  6.18171273e-03, bound:  3.15536529e-01\n",
      "Epoch: 20355 mean train loss:  6.18124986e-03, bound:  3.15536499e-01\n",
      "Epoch: 20356 mean train loss:  6.18069433e-03, bound:  3.15536499e-01\n",
      "Epoch: 20357 mean train loss:  6.18034601e-03, bound:  3.15536439e-01\n",
      "Epoch: 20358 mean train loss:  6.17989991e-03, bound:  3.15536439e-01\n",
      "Epoch: 20359 mean train loss:  6.17953436e-03, bound:  3.15536380e-01\n",
      "Epoch: 20360 mean train loss:  6.17917022e-03, bound:  3.15536380e-01\n",
      "Epoch: 20361 mean train loss:  6.17883215e-03, bound:  3.15536320e-01\n",
      "Epoch: 20362 mean train loss:  6.17867149e-03, bound:  3.15536320e-01\n",
      "Epoch: 20363 mean train loss:  6.17859326e-03, bound:  3.15536261e-01\n",
      "Epoch: 20364 mean train loss:  6.17876975e-03, bound:  3.15536261e-01\n",
      "Epoch: 20365 mean train loss:  6.17898675e-03, bound:  3.15536201e-01\n",
      "Epoch: 20366 mean train loss:  6.17943658e-03, bound:  3.15536201e-01\n",
      "Epoch: 20367 mean train loss:  6.18000003e-03, bound:  3.15536171e-01\n",
      "Epoch: 20368 mean train loss:  6.18076324e-03, bound:  3.15536201e-01\n",
      "Epoch: 20369 mean train loss:  6.18164521e-03, bound:  3.15536082e-01\n",
      "Epoch: 20370 mean train loss:  6.18259190e-03, bound:  3.15536082e-01\n",
      "Epoch: 20371 mean train loss:  6.18346827e-03, bound:  3.15535992e-01\n",
      "Epoch: 20372 mean train loss:  6.18368434e-03, bound:  3.15536082e-01\n",
      "Epoch: 20373 mean train loss:  6.18298026e-03, bound:  3.15535963e-01\n",
      "Epoch: 20374 mean train loss:  6.18114369e-03, bound:  3.15535992e-01\n",
      "Epoch: 20375 mean train loss:  6.17828919e-03, bound:  3.15535933e-01\n",
      "Epoch: 20376 mean train loss:  6.17502118e-03, bound:  3.15535963e-01\n",
      "Epoch: 20377 mean train loss:  6.17184956e-03, bound:  3.15535933e-01\n",
      "Epoch: 20378 mean train loss:  6.16953336e-03, bound:  3.15535873e-01\n",
      "Epoch: 20379 mean train loss:  6.16828538e-03, bound:  3.15535843e-01\n",
      "Epoch: 20380 mean train loss:  6.16799016e-03, bound:  3.15535814e-01\n",
      "Epoch: 20381 mean train loss:  6.16846327e-03, bound:  3.15535814e-01\n",
      "Epoch: 20382 mean train loss:  6.16906118e-03, bound:  3.15535754e-01\n",
      "Epoch: 20383 mean train loss:  6.16944628e-03, bound:  3.15535754e-01\n",
      "Epoch: 20384 mean train loss:  6.16930891e-03, bound:  3.15535694e-01\n",
      "Epoch: 20385 mean train loss:  6.16853498e-03, bound:  3.15535694e-01\n",
      "Epoch: 20386 mean train loss:  6.16725162e-03, bound:  3.15535635e-01\n",
      "Epoch: 20387 mean train loss:  6.16573170e-03, bound:  3.15535635e-01\n",
      "Epoch: 20388 mean train loss:  6.16433332e-03, bound:  3.15535605e-01\n",
      "Epoch: 20389 mean train loss:  6.16320688e-03, bound:  3.15535575e-01\n",
      "Epoch: 20390 mean train loss:  6.16255682e-03, bound:  3.15535575e-01\n",
      "Epoch: 20391 mean train loss:  6.16225228e-03, bound:  3.15535516e-01\n",
      "Epoch: 20392 mean train loss:  6.16198219e-03, bound:  3.15535516e-01\n",
      "Epoch: 20393 mean train loss:  6.16171397e-03, bound:  3.15535486e-01\n",
      "Epoch: 20394 mean train loss:  6.16138848e-03, bound:  3.15535486e-01\n",
      "Epoch: 20395 mean train loss:  6.16089953e-03, bound:  3.15535396e-01\n",
      "Epoch: 20396 mean train loss:  6.16028719e-03, bound:  3.15535396e-01\n",
      "Epoch: 20397 mean train loss:  6.15955424e-03, bound:  3.15535367e-01\n",
      "Epoch: 20398 mean train loss:  6.15884643e-03, bound:  3.15535367e-01\n",
      "Epoch: 20399 mean train loss:  6.15810277e-03, bound:  3.15535277e-01\n",
      "Epoch: 20400 mean train loss:  6.15744805e-03, bound:  3.15535277e-01\n",
      "Epoch: 20401 mean train loss:  6.15683245e-03, bound:  3.15535277e-01\n",
      "Epoch: 20402 mean train loss:  6.15636585e-03, bound:  3.15535218e-01\n",
      "Epoch: 20403 mean train loss:  6.15599100e-03, bound:  3.15535188e-01\n",
      "Epoch: 20404 mean train loss:  6.15553092e-03, bound:  3.15535158e-01\n",
      "Epoch: 20405 mean train loss:  6.15511648e-03, bound:  3.15535158e-01\n",
      "Epoch: 20406 mean train loss:  6.15461729e-03, bound:  3.15535098e-01\n",
      "Epoch: 20407 mean train loss:  6.15404267e-03, bound:  3.15535098e-01\n",
      "Epoch: 20408 mean train loss:  6.15343917e-03, bound:  3.15535069e-01\n",
      "Epoch: 20409 mean train loss:  6.15286827e-03, bound:  3.15535039e-01\n",
      "Epoch: 20410 mean train loss:  6.15228619e-03, bound:  3.15534979e-01\n",
      "Epoch: 20411 mean train loss:  6.15168316e-03, bound:  3.15534979e-01\n",
      "Epoch: 20412 mean train loss:  6.15110714e-03, bound:  3.15534949e-01\n",
      "Epoch: 20413 mean train loss:  6.15058606e-03, bound:  3.15534920e-01\n",
      "Epoch: 20414 mean train loss:  6.15010597e-03, bound:  3.15534890e-01\n",
      "Epoch: 20415 mean train loss:  6.14964496e-03, bound:  3.15534890e-01\n",
      "Epoch: 20416 mean train loss:  6.14916161e-03, bound:  3.15534830e-01\n",
      "Epoch: 20417 mean train loss:  6.14869175e-03, bound:  3.15534800e-01\n",
      "Epoch: 20418 mean train loss:  6.14815764e-03, bound:  3.15534800e-01\n",
      "Epoch: 20419 mean train loss:  6.14754669e-03, bound:  3.15534770e-01\n",
      "Epoch: 20420 mean train loss:  6.14699908e-03, bound:  3.15534711e-01\n",
      "Epoch: 20421 mean train loss:  6.14646683e-03, bound:  3.15534711e-01\n",
      "Epoch: 20422 mean train loss:  6.14592666e-03, bound:  3.15534681e-01\n",
      "Epoch: 20423 mean train loss:  6.14541303e-03, bound:  3.15534651e-01\n",
      "Epoch: 20424 mean train loss:  6.14490500e-03, bound:  3.15534621e-01\n",
      "Epoch: 20425 mean train loss:  6.14432106e-03, bound:  3.15534621e-01\n",
      "Epoch: 20426 mean train loss:  6.14384236e-03, bound:  3.15534592e-01\n",
      "Epoch: 20427 mean train loss:  6.14329334e-03, bound:  3.15534532e-01\n",
      "Epoch: 20428 mean train loss:  6.14278065e-03, bound:  3.15534502e-01\n",
      "Epoch: 20429 mean train loss:  6.14231033e-03, bound:  3.15534502e-01\n",
      "Epoch: 20430 mean train loss:  6.14182884e-03, bound:  3.15534472e-01\n",
      "Epoch: 20431 mean train loss:  6.14122255e-03, bound:  3.15534413e-01\n",
      "Epoch: 20432 mean train loss:  6.14077784e-03, bound:  3.15534413e-01\n",
      "Epoch: 20433 mean train loss:  6.14025677e-03, bound:  3.15534383e-01\n",
      "Epoch: 20434 mean train loss:  6.13968726e-03, bound:  3.15534353e-01\n",
      "Epoch: 20435 mean train loss:  6.13921089e-03, bound:  3.15534353e-01\n",
      "Epoch: 20436 mean train loss:  6.13865443e-03, bound:  3.15534294e-01\n",
      "Epoch: 20437 mean train loss:  6.13815105e-03, bound:  3.15534264e-01\n",
      "Epoch: 20438 mean train loss:  6.13761134e-03, bound:  3.15534234e-01\n",
      "Epoch: 20439 mean train loss:  6.13708282e-03, bound:  3.15534234e-01\n",
      "Epoch: 20440 mean train loss:  6.13656593e-03, bound:  3.15534174e-01\n",
      "Epoch: 20441 mean train loss:  6.13602018e-03, bound:  3.15534145e-01\n",
      "Epoch: 20442 mean train loss:  6.13555033e-03, bound:  3.15534145e-01\n",
      "Epoch: 20443 mean train loss:  6.13496033e-03, bound:  3.15534085e-01\n",
      "Epoch: 20444 mean train loss:  6.13451051e-03, bound:  3.15534085e-01\n",
      "Epoch: 20445 mean train loss:  6.13388466e-03, bound:  3.15534055e-01\n",
      "Epoch: 20446 mean train loss:  6.13335287e-03, bound:  3.15534025e-01\n",
      "Epoch: 20447 mean train loss:  6.13288861e-03, bound:  3.15533966e-01\n",
      "Epoch: 20448 mean train loss:  6.13236986e-03, bound:  3.15533966e-01\n",
      "Epoch: 20449 mean train loss:  6.13183901e-03, bound:  3.15533936e-01\n",
      "Epoch: 20450 mean train loss:  6.13132445e-03, bound:  3.15533936e-01\n",
      "Epoch: 20451 mean train loss:  6.13081083e-03, bound:  3.15533847e-01\n",
      "Epoch: 20452 mean train loss:  6.13025809e-03, bound:  3.15533847e-01\n",
      "Epoch: 20453 mean train loss:  6.12968765e-03, bound:  3.15533817e-01\n",
      "Epoch: 20454 mean train loss:  6.12919964e-03, bound:  3.15533817e-01\n",
      "Epoch: 20455 mean train loss:  6.12874003e-03, bound:  3.15533787e-01\n",
      "Epoch: 20456 mean train loss:  6.12817751e-03, bound:  3.15533727e-01\n",
      "Epoch: 20457 mean train loss:  6.12763828e-03, bound:  3.15533727e-01\n",
      "Epoch: 20458 mean train loss:  6.12713955e-03, bound:  3.15533698e-01\n",
      "Epoch: 20459 mean train loss:  6.12650998e-03, bound:  3.15533638e-01\n",
      "Epoch: 20460 mean train loss:  6.12601032e-03, bound:  3.15533638e-01\n",
      "Epoch: 20461 mean train loss:  6.12554792e-03, bound:  3.15533608e-01\n",
      "Epoch: 20462 mean train loss:  6.12508180e-03, bound:  3.15533578e-01\n",
      "Epoch: 20463 mean train loss:  6.12444850e-03, bound:  3.15533578e-01\n",
      "Epoch: 20464 mean train loss:  6.12394186e-03, bound:  3.15533519e-01\n",
      "Epoch: 20465 mean train loss:  6.12350134e-03, bound:  3.15533489e-01\n",
      "Epoch: 20466 mean train loss:  6.12290530e-03, bound:  3.15533459e-01\n",
      "Epoch: 20467 mean train loss:  6.12251181e-03, bound:  3.15533459e-01\n",
      "Epoch: 20468 mean train loss:  6.12194370e-03, bound:  3.15533400e-01\n",
      "Epoch: 20469 mean train loss:  6.12148456e-03, bound:  3.15533370e-01\n",
      "Epoch: 20470 mean train loss:  6.12099655e-03, bound:  3.15533370e-01\n",
      "Epoch: 20471 mean train loss:  6.12065429e-03, bound:  3.15533370e-01\n",
      "Epoch: 20472 mean train loss:  6.12034695e-03, bound:  3.15533280e-01\n",
      "Epoch: 20473 mean train loss:  6.12023845e-03, bound:  3.15533280e-01\n",
      "Epoch: 20474 mean train loss:  6.12027850e-03, bound:  3.15533251e-01\n",
      "Epoch: 20475 mean train loss:  6.12060446e-03, bound:  3.15533251e-01\n",
      "Epoch: 20476 mean train loss:  6.12120517e-03, bound:  3.15533161e-01\n",
      "Epoch: 20477 mean train loss:  6.12244103e-03, bound:  3.15533161e-01\n",
      "Epoch: 20478 mean train loss:  6.12431299e-03, bound:  3.15533131e-01\n",
      "Epoch: 20479 mean train loss:  6.12724293e-03, bound:  3.15533161e-01\n",
      "Epoch: 20480 mean train loss:  6.13102457e-03, bound:  3.15533042e-01\n",
      "Epoch: 20481 mean train loss:  6.13555918e-03, bound:  3.15533072e-01\n",
      "Epoch: 20482 mean train loss:  6.13966212e-03, bound:  3.15532953e-01\n",
      "Epoch: 20483 mean train loss:  6.14139065e-03, bound:  3.15533072e-01\n",
      "Epoch: 20484 mean train loss:  6.13876339e-03, bound:  3.15532923e-01\n",
      "Epoch: 20485 mean train loss:  6.13168441e-03, bound:  3.15533012e-01\n",
      "Epoch: 20486 mean train loss:  6.12224033e-03, bound:  3.15532863e-01\n",
      "Epoch: 20487 mean train loss:  6.11456623e-03, bound:  3.15532923e-01\n",
      "Epoch: 20488 mean train loss:  6.11151708e-03, bound:  3.15532863e-01\n",
      "Epoch: 20489 mean train loss:  6.11310732e-03, bound:  3.15532833e-01\n",
      "Epoch: 20490 mean train loss:  6.11692248e-03, bound:  3.15532833e-01\n",
      "Epoch: 20491 mean train loss:  6.11969968e-03, bound:  3.15532744e-01\n",
      "Epoch: 20492 mean train loss:  6.11936068e-03, bound:  3.15532833e-01\n",
      "Epoch: 20493 mean train loss:  6.11582724e-03, bound:  3.15532714e-01\n",
      "Epoch: 20494 mean train loss:  6.11121301e-03, bound:  3.15532744e-01\n",
      "Epoch: 20495 mean train loss:  6.10824581e-03, bound:  3.15532684e-01\n",
      "Epoch: 20496 mean train loss:  6.10803068e-03, bound:  3.15532684e-01\n",
      "Epoch: 20497 mean train loss:  6.10967912e-03, bound:  3.15532684e-01\n",
      "Epoch: 20498 mean train loss:  6.11112500e-03, bound:  3.15532565e-01\n",
      "Epoch: 20499 mean train loss:  6.11096201e-03, bound:  3.15532625e-01\n",
      "Epoch: 20500 mean train loss:  6.10893080e-03, bound:  3.15532565e-01\n",
      "Epoch: 20501 mean train loss:  6.10641902e-03, bound:  3.15532565e-01\n",
      "Epoch: 20502 mean train loss:  6.10463833e-03, bound:  3.15532506e-01\n",
      "Epoch: 20503 mean train loss:  6.10424811e-03, bound:  3.15532476e-01\n",
      "Epoch: 20504 mean train loss:  6.10484509e-03, bound:  3.15532476e-01\n",
      "Epoch: 20505 mean train loss:  6.10537315e-03, bound:  3.15532416e-01\n",
      "Epoch: 20506 mean train loss:  6.10495219e-03, bound:  3.15532446e-01\n",
      "Epoch: 20507 mean train loss:  6.10358827e-03, bound:  3.15532386e-01\n",
      "Epoch: 20508 mean train loss:  6.10210653e-03, bound:  3.15532386e-01\n",
      "Epoch: 20509 mean train loss:  6.10108534e-03, bound:  3.15532357e-01\n",
      "Epoch: 20510 mean train loss:  6.10079942e-03, bound:  3.15532297e-01\n",
      "Epoch: 20511 mean train loss:  6.10076077e-03, bound:  3.15532297e-01\n",
      "Epoch: 20512 mean train loss:  6.10073097e-03, bound:  3.15532237e-01\n",
      "Epoch: 20513 mean train loss:  6.10020710e-03, bound:  3.15532237e-01\n",
      "Epoch: 20514 mean train loss:  6.09939126e-03, bound:  3.15532178e-01\n",
      "Epoch: 20515 mean train loss:  6.09835703e-03, bound:  3.15532178e-01\n",
      "Epoch: 20516 mean train loss:  6.09761849e-03, bound:  3.15532148e-01\n",
      "Epoch: 20517 mean train loss:  6.09719614e-03, bound:  3.15532118e-01\n",
      "Epoch: 20518 mean train loss:  6.09697308e-03, bound:  3.15532118e-01\n",
      "Epoch: 20519 mean train loss:  6.09666528e-03, bound:  3.15532058e-01\n",
      "Epoch: 20520 mean train loss:  6.09621545e-03, bound:  3.15532058e-01\n",
      "Epoch: 20521 mean train loss:  6.09554397e-03, bound:  3.15531999e-01\n",
      "Epoch: 20522 mean train loss:  6.09480916e-03, bound:  3.15531999e-01\n",
      "Epoch: 20523 mean train loss:  6.09408366e-03, bound:  3.15531939e-01\n",
      "Epoch: 20524 mean train loss:  6.09363476e-03, bound:  3.15531939e-01\n",
      "Epoch: 20525 mean train loss:  6.09331718e-03, bound:  3.15531909e-01\n",
      "Epoch: 20526 mean train loss:  6.09295676e-03, bound:  3.15531880e-01\n",
      "Epoch: 20527 mean train loss:  6.09247759e-03, bound:  3.15531880e-01\n",
      "Epoch: 20528 mean train loss:  6.09194813e-03, bound:  3.15531850e-01\n",
      "Epoch: 20529 mean train loss:  6.09135674e-03, bound:  3.15531820e-01\n",
      "Epoch: 20530 mean train loss:  6.09071087e-03, bound:  3.15531790e-01\n",
      "Epoch: 20531 mean train loss:  6.09011576e-03, bound:  3.15531760e-01\n",
      "Epoch: 20532 mean train loss:  6.08969945e-03, bound:  3.15531731e-01\n",
      "Epoch: 20533 mean train loss:  6.08924218e-03, bound:  3.15531701e-01\n",
      "Epoch: 20534 mean train loss:  6.08873367e-03, bound:  3.15531701e-01\n",
      "Epoch: 20535 mean train loss:  6.08824752e-03, bound:  3.15531611e-01\n",
      "Epoch: 20536 mean train loss:  6.08781492e-03, bound:  3.15531611e-01\n",
      "Epoch: 20537 mean train loss:  6.08725473e-03, bound:  3.15531611e-01\n",
      "Epoch: 20538 mean train loss:  6.08662283e-03, bound:  3.15531582e-01\n",
      "Epoch: 20539 mean train loss:  6.08616835e-03, bound:  3.15531552e-01\n",
      "Epoch: 20540 mean train loss:  6.08562492e-03, bound:  3.15531522e-01\n",
      "Epoch: 20541 mean train loss:  6.08525053e-03, bound:  3.15531492e-01\n",
      "Epoch: 20542 mean train loss:  6.08469686e-03, bound:  3.15531462e-01\n",
      "Epoch: 20543 mean train loss:  6.08422328e-03, bound:  3.15531462e-01\n",
      "Epoch: 20544 mean train loss:  6.08372875e-03, bound:  3.15531433e-01\n",
      "Epoch: 20545 mean train loss:  6.08326402e-03, bound:  3.15531373e-01\n",
      "Epoch: 20546 mean train loss:  6.08271314e-03, bound:  3.15531343e-01\n",
      "Epoch: 20547 mean train loss:  6.08221535e-03, bound:  3.15531343e-01\n",
      "Epoch: 20548 mean train loss:  6.08165516e-03, bound:  3.15531284e-01\n",
      "Epoch: 20549 mean train loss:  6.08114805e-03, bound:  3.15531284e-01\n",
      "Epoch: 20550 mean train loss:  6.08066097e-03, bound:  3.15531254e-01\n",
      "Epoch: 20551 mean train loss:  6.08018599e-03, bound:  3.15531224e-01\n",
      "Epoch: 20552 mean train loss:  6.07976038e-03, bound:  3.15531224e-01\n",
      "Epoch: 20553 mean train loss:  6.07917365e-03, bound:  3.15531164e-01\n",
      "Epoch: 20554 mean train loss:  6.07872615e-03, bound:  3.15531135e-01\n",
      "Epoch: 20555 mean train loss:  6.07817993e-03, bound:  3.15531135e-01\n",
      "Epoch: 20556 mean train loss:  6.07769936e-03, bound:  3.15531135e-01\n",
      "Epoch: 20557 mean train loss:  6.07718062e-03, bound:  3.15531045e-01\n",
      "Epoch: 20558 mean train loss:  6.07670983e-03, bound:  3.15531015e-01\n",
      "Epoch: 20559 mean train loss:  6.07623579e-03, bound:  3.15531015e-01\n",
      "Epoch: 20560 mean train loss:  6.07563183e-03, bound:  3.15531015e-01\n",
      "Epoch: 20561 mean train loss:  6.07517827e-03, bound:  3.15530926e-01\n",
      "Epoch: 20562 mean train loss:  6.07466977e-03, bound:  3.15530926e-01\n",
      "Epoch: 20563 mean train loss:  6.07418641e-03, bound:  3.15530926e-01\n",
      "Epoch: 20564 mean train loss:  6.07366720e-03, bound:  3.15530896e-01\n",
      "Epoch: 20565 mean train loss:  6.07319130e-03, bound:  3.15530837e-01\n",
      "Epoch: 20566 mean train loss:  6.07273588e-03, bound:  3.15530837e-01\n",
      "Epoch: 20567 mean train loss:  6.07223483e-03, bound:  3.15530837e-01\n",
      "Epoch: 20568 mean train loss:  6.07171608e-03, bound:  3.15530777e-01\n",
      "Epoch: 20569 mean train loss:  6.07116241e-03, bound:  3.15530777e-01\n",
      "Epoch: 20570 mean train loss:  6.07070187e-03, bound:  3.15530717e-01\n",
      "Epoch: 20571 mean train loss:  6.07020501e-03, bound:  3.15530717e-01\n",
      "Epoch: 20572 mean train loss:  6.06969977e-03, bound:  3.15530658e-01\n",
      "Epoch: 20573 mean train loss:  6.06915541e-03, bound:  3.15530658e-01\n",
      "Epoch: 20574 mean train loss:  6.06863108e-03, bound:  3.15530598e-01\n",
      "Epoch: 20575 mean train loss:  6.06820220e-03, bound:  3.15530598e-01\n",
      "Epoch: 20576 mean train loss:  6.06773468e-03, bound:  3.15530568e-01\n",
      "Epoch: 20577 mean train loss:  6.06721593e-03, bound:  3.15530568e-01\n",
      "Epoch: 20578 mean train loss:  6.06670836e-03, bound:  3.15530509e-01\n",
      "Epoch: 20579 mean train loss:  6.06618682e-03, bound:  3.15530479e-01\n",
      "Epoch: 20580 mean train loss:  6.06567506e-03, bound:  3.15530449e-01\n",
      "Epoch: 20581 mean train loss:  6.06520660e-03, bound:  3.15530449e-01\n",
      "Epoch: 20582 mean train loss:  6.06469810e-03, bound:  3.15530449e-01\n",
      "Epoch: 20583 mean train loss:  6.06421335e-03, bound:  3.15530360e-01\n",
      "Epoch: 20584 mean train loss:  6.06364058e-03, bound:  3.15530360e-01\n",
      "Epoch: 20585 mean train loss:  6.06321450e-03, bound:  3.15530330e-01\n",
      "Epoch: 20586 mean train loss:  6.06266782e-03, bound:  3.15530330e-01\n",
      "Epoch: 20587 mean train loss:  6.06216537e-03, bound:  3.15530270e-01\n",
      "Epoch: 20588 mean train loss:  6.06170483e-03, bound:  3.15530241e-01\n",
      "Epoch: 20589 mean train loss:  6.06120285e-03, bound:  3.15530211e-01\n",
      "Epoch: 20590 mean train loss:  6.06067525e-03, bound:  3.15530211e-01\n",
      "Epoch: 20591 mean train loss:  6.06016442e-03, bound:  3.15530151e-01\n",
      "Epoch: 20592 mean train loss:  6.05963869e-03, bound:  3.15530121e-01\n",
      "Epoch: 20593 mean train loss:  6.05916977e-03, bound:  3.15530121e-01\n",
      "Epoch: 20594 mean train loss:  6.05865894e-03, bound:  3.15530092e-01\n",
      "Epoch: 20595 mean train loss:  6.05818117e-03, bound:  3.15530032e-01\n",
      "Epoch: 20596 mean train loss:  6.05763681e-03, bound:  3.15530032e-01\n",
      "Epoch: 20597 mean train loss:  6.05715392e-03, bound:  3.15530002e-01\n",
      "Epoch: 20598 mean train loss:  6.05673483e-03, bound:  3.15530002e-01\n",
      "Epoch: 20599 mean train loss:  6.05620164e-03, bound:  3.15529943e-01\n",
      "Epoch: 20600 mean train loss:  6.05564658e-03, bound:  3.15529913e-01\n",
      "Epoch: 20601 mean train loss:  6.05514878e-03, bound:  3.15529913e-01\n",
      "Epoch: 20602 mean train loss:  6.05458952e-03, bound:  3.15529883e-01\n",
      "Epoch: 20603 mean train loss:  6.05416158e-03, bound:  3.15529823e-01\n",
      "Epoch: 20604 mean train loss:  6.05362700e-03, bound:  3.15529794e-01\n",
      "Epoch: 20605 mean train loss:  6.05311524e-03, bound:  3.15529794e-01\n",
      "Epoch: 20606 mean train loss:  6.05262863e-03, bound:  3.15529764e-01\n",
      "Epoch: 20607 mean train loss:  6.05215225e-03, bound:  3.15529764e-01\n",
      "Epoch: 20608 mean train loss:  6.05165446e-03, bound:  3.15529704e-01\n",
      "Epoch: 20609 mean train loss:  6.05116365e-03, bound:  3.15529674e-01\n",
      "Epoch: 20610 mean train loss:  6.05067331e-03, bound:  3.15529674e-01\n",
      "Epoch: 20611 mean train loss:  6.05016667e-03, bound:  3.15529615e-01\n",
      "Epoch: 20612 mean train loss:  6.04962138e-03, bound:  3.15529585e-01\n",
      "Epoch: 20613 mean train loss:  6.04909332e-03, bound:  3.15529585e-01\n",
      "Epoch: 20614 mean train loss:  6.04863744e-03, bound:  3.15529555e-01\n",
      "Epoch: 20615 mean train loss:  6.04810473e-03, bound:  3.15529525e-01\n",
      "Epoch: 20616 mean train loss:  6.04762370e-03, bound:  3.15529495e-01\n",
      "Epoch: 20617 mean train loss:  6.04709797e-03, bound:  3.15529466e-01\n",
      "Epoch: 20618 mean train loss:  6.04661601e-03, bound:  3.15529436e-01\n",
      "Epoch: 20619 mean train loss:  6.04611030e-03, bound:  3.15529406e-01\n",
      "Epoch: 20620 mean train loss:  6.04564277e-03, bound:  3.15529406e-01\n",
      "Epoch: 20621 mean train loss:  6.04517898e-03, bound:  3.15529346e-01\n",
      "Epoch: 20622 mean train loss:  6.04470400e-03, bound:  3.15529346e-01\n",
      "Epoch: 20623 mean train loss:  6.04420528e-03, bound:  3.15529317e-01\n",
      "Epoch: 20624 mean train loss:  6.04367349e-03, bound:  3.15529287e-01\n",
      "Epoch: 20625 mean train loss:  6.04324369e-03, bound:  3.15529227e-01\n",
      "Epoch: 20626 mean train loss:  6.04278548e-03, bound:  3.15529227e-01\n",
      "Epoch: 20627 mean train loss:  6.04227884e-03, bound:  3.15529197e-01\n",
      "Epoch: 20628 mean train loss:  6.04180200e-03, bound:  3.15529197e-01\n",
      "Epoch: 20629 mean train loss:  6.04129862e-03, bound:  3.15529138e-01\n",
      "Epoch: 20630 mean train loss:  6.04087394e-03, bound:  3.15529108e-01\n",
      "Epoch: 20631 mean train loss:  6.04043202e-03, bound:  3.15529078e-01\n",
      "Epoch: 20632 mean train loss:  6.04004692e-03, bound:  3.15529078e-01\n",
      "Epoch: 20633 mean train loss:  6.03965623e-03, bound:  3.15529019e-01\n",
      "Epoch: 20634 mean train loss:  6.03925111e-03, bound:  3.15529019e-01\n",
      "Epoch: 20635 mean train loss:  6.03893958e-03, bound:  3.15528989e-01\n",
      "Epoch: 20636 mean train loss:  6.03871886e-03, bound:  3.15529019e-01\n",
      "Epoch: 20637 mean train loss:  6.03859732e-03, bound:  3.15528899e-01\n",
      "Epoch: 20638 mean train loss:  6.03855029e-03, bound:  3.15528899e-01\n",
      "Epoch: 20639 mean train loss:  6.03870070e-03, bound:  3.15528870e-01\n",
      "Epoch: 20640 mean train loss:  6.03908161e-03, bound:  3.15528899e-01\n",
      "Epoch: 20641 mean train loss:  6.03972655e-03, bound:  3.15528780e-01\n",
      "Epoch: 20642 mean train loss:  6.04061782e-03, bound:  3.15528810e-01\n",
      "Epoch: 20643 mean train loss:  6.04163203e-03, bound:  3.15528750e-01\n",
      "Epoch: 20644 mean train loss:  6.04288932e-03, bound:  3.15528780e-01\n",
      "Epoch: 20645 mean train loss:  6.04395662e-03, bound:  3.15528691e-01\n",
      "Epoch: 20646 mean train loss:  6.04470586e-03, bound:  3.15528721e-01\n",
      "Epoch: 20647 mean train loss:  6.04457734e-03, bound:  3.15528601e-01\n",
      "Epoch: 20648 mean train loss:  6.04321714e-03, bound:  3.15528691e-01\n",
      "Epoch: 20649 mean train loss:  6.04060059e-03, bound:  3.15528572e-01\n",
      "Epoch: 20650 mean train loss:  6.03717566e-03, bound:  3.15528631e-01\n",
      "Epoch: 20651 mean train loss:  6.03371207e-03, bound:  3.15528542e-01\n",
      "Epoch: 20652 mean train loss:  6.03086175e-03, bound:  3.15528542e-01\n",
      "Epoch: 20653 mean train loss:  6.02909643e-03, bound:  3.15528482e-01\n",
      "Epoch: 20654 mean train loss:  6.02868665e-03, bound:  3.15528482e-01\n",
      "Epoch: 20655 mean train loss:  6.02907594e-03, bound:  3.15528482e-01\n",
      "Epoch: 20656 mean train loss:  6.02990016e-03, bound:  3.15528423e-01\n",
      "Epoch: 20657 mean train loss:  6.03054045e-03, bound:  3.15528423e-01\n",
      "Epoch: 20658 mean train loss:  6.03046874e-03, bound:  3.15528363e-01\n",
      "Epoch: 20659 mean train loss:  6.02961145e-03, bound:  3.15528363e-01\n",
      "Epoch: 20660 mean train loss:  6.02816138e-03, bound:  3.15528303e-01\n",
      "Epoch: 20661 mean train loss:  6.02652133e-03, bound:  3.15528333e-01\n",
      "Epoch: 20662 mean train loss:  6.02513133e-03, bound:  3.15528244e-01\n",
      "Epoch: 20663 mean train loss:  6.02420932e-03, bound:  3.15528244e-01\n",
      "Epoch: 20664 mean train loss:  6.02361327e-03, bound:  3.15528244e-01\n",
      "Epoch: 20665 mean train loss:  6.02346333e-03, bound:  3.15528214e-01\n",
      "Epoch: 20666 mean train loss:  6.02347543e-03, bound:  3.15528214e-01\n",
      "Epoch: 20667 mean train loss:  6.02346007e-03, bound:  3.15528125e-01\n",
      "Epoch: 20668 mean train loss:  6.02319557e-03, bound:  3.15528125e-01\n",
      "Epoch: 20669 mean train loss:  6.02251664e-03, bound:  3.15528095e-01\n",
      "Epoch: 20670 mean train loss:  6.02178555e-03, bound:  3.15528095e-01\n",
      "Epoch: 20671 mean train loss:  6.02083327e-03, bound:  3.15528035e-01\n",
      "Epoch: 20672 mean train loss:  6.01996854e-03, bound:  3.15528035e-01\n",
      "Epoch: 20673 mean train loss:  6.01928774e-03, bound:  3.15527976e-01\n",
      "Epoch: 20674 mean train loss:  6.01869402e-03, bound:  3.15527976e-01\n",
      "Epoch: 20675 mean train loss:  6.01835269e-03, bound:  3.15527976e-01\n",
      "Epoch: 20676 mean train loss:  6.01809239e-03, bound:  3.15527916e-01\n",
      "Epoch: 20677 mean train loss:  6.01774547e-03, bound:  3.15527886e-01\n",
      "Epoch: 20678 mean train loss:  6.01745909e-03, bound:  3.15527856e-01\n",
      "Epoch: 20679 mean train loss:  6.01699576e-03, bound:  3.15527856e-01\n",
      "Epoch: 20680 mean train loss:  6.01650821e-03, bound:  3.15527797e-01\n",
      "Epoch: 20681 mean train loss:  6.01588003e-03, bound:  3.15527797e-01\n",
      "Epoch: 20682 mean train loss:  6.01522997e-03, bound:  3.15527767e-01\n",
      "Epoch: 20683 mean train loss:  6.01455150e-03, bound:  3.15527767e-01\n",
      "Epoch: 20684 mean train loss:  6.01397781e-03, bound:  3.15527678e-01\n",
      "Epoch: 20685 mean train loss:  6.01338921e-03, bound:  3.15527678e-01\n",
      "Epoch: 20686 mean train loss:  6.01280760e-03, bound:  3.15527648e-01\n",
      "Epoch: 20687 mean train loss:  6.01229398e-03, bound:  3.15527648e-01\n",
      "Epoch: 20688 mean train loss:  6.01185393e-03, bound:  3.15527558e-01\n",
      "Epoch: 20689 mean train loss:  6.01143204e-03, bound:  3.15527558e-01\n",
      "Epoch: 20690 mean train loss:  6.01093611e-03, bound:  3.15527529e-01\n",
      "Epoch: 20691 mean train loss:  6.01064879e-03, bound:  3.15527529e-01\n",
      "Epoch: 20692 mean train loss:  6.01015333e-03, bound:  3.15527469e-01\n",
      "Epoch: 20693 mean train loss:  6.00964110e-03, bound:  3.15527469e-01\n",
      "Epoch: 20694 mean train loss:  6.00911677e-03, bound:  3.15527469e-01\n",
      "Epoch: 20695 mean train loss:  6.00861618e-03, bound:  3.15527409e-01\n",
      "Epoch: 20696 mean train loss:  6.00804575e-03, bound:  3.15527350e-01\n",
      "Epoch: 20697 mean train loss:  6.00747578e-03, bound:  3.15527350e-01\n",
      "Epoch: 20698 mean train loss:  6.00696076e-03, bound:  3.15527350e-01\n",
      "Epoch: 20699 mean train loss:  6.00633537e-03, bound:  3.15527290e-01\n",
      "Epoch: 20700 mean train loss:  6.00596797e-03, bound:  3.15527260e-01\n",
      "Epoch: 20701 mean train loss:  6.00545015e-03, bound:  3.15527260e-01\n",
      "Epoch: 20702 mean train loss:  6.00495888e-03, bound:  3.15527231e-01\n",
      "Epoch: 20703 mean train loss:  6.00452069e-03, bound:  3.15527201e-01\n",
      "Epoch: 20704 mean train loss:  6.00402942e-03, bound:  3.15527201e-01\n",
      "Epoch: 20705 mean train loss:  6.00359775e-03, bound:  3.15527141e-01\n",
      "Epoch: 20706 mean train loss:  6.00310881e-03, bound:  3.15527111e-01\n",
      "Epoch: 20707 mean train loss:  6.00267854e-03, bound:  3.15527111e-01\n",
      "Epoch: 20708 mean train loss:  6.00212486e-03, bound:  3.15527081e-01\n",
      "Epoch: 20709 mean train loss:  6.00160565e-03, bound:  3.15527081e-01\n",
      "Epoch: 20710 mean train loss:  6.00113533e-03, bound:  3.15526992e-01\n",
      "Epoch: 20711 mean train loss:  6.00070599e-03, bound:  3.15526992e-01\n",
      "Epoch: 20712 mean train loss:  6.00015419e-03, bound:  3.15526962e-01\n",
      "Epoch: 20713 mean train loss:  5.99959353e-03, bound:  3.15526962e-01\n",
      "Epoch: 20714 mean train loss:  5.99909434e-03, bound:  3.15526873e-01\n",
      "Epoch: 20715 mean train loss:  5.99859189e-03, bound:  3.15526873e-01\n",
      "Epoch: 20716 mean train loss:  5.99810947e-03, bound:  3.15526873e-01\n",
      "Epoch: 20717 mean train loss:  5.99758234e-03, bound:  3.15526843e-01\n",
      "Epoch: 20718 mean train loss:  5.99707337e-03, bound:  3.15526783e-01\n",
      "Epoch: 20719 mean train loss:  5.99660398e-03, bound:  3.15526783e-01\n",
      "Epoch: 20720 mean train loss:  5.99605497e-03, bound:  3.15526783e-01\n",
      "Epoch: 20721 mean train loss:  5.99566149e-03, bound:  3.15526724e-01\n",
      "Epoch: 20722 mean train loss:  5.99505519e-03, bound:  3.15526694e-01\n",
      "Epoch: 20723 mean train loss:  5.99462911e-03, bound:  3.15526664e-01\n",
      "Epoch: 20724 mean train loss:  5.99408895e-03, bound:  3.15526664e-01\n",
      "Epoch: 20725 mean train loss:  5.99364145e-03, bound:  3.15526605e-01\n",
      "Epoch: 20726 mean train loss:  5.99315530e-03, bound:  3.15526605e-01\n",
      "Epoch: 20727 mean train loss:  5.99275623e-03, bound:  3.15526545e-01\n",
      "Epoch: 20728 mean train loss:  5.99221420e-03, bound:  3.15526545e-01\n",
      "Epoch: 20729 mean train loss:  5.99183515e-03, bound:  3.15526515e-01\n",
      "Epoch: 20730 mean train loss:  5.99132618e-03, bound:  3.15526485e-01\n",
      "Epoch: 20731 mean train loss:  5.99091826e-03, bound:  3.15526456e-01\n",
      "Epoch: 20732 mean train loss:  5.99048426e-03, bound:  3.15526456e-01\n",
      "Epoch: 20733 mean train loss:  5.99004794e-03, bound:  3.15526426e-01\n",
      "Epoch: 20734 mean train loss:  5.98964002e-03, bound:  3.15526366e-01\n",
      "Epoch: 20735 mean train loss:  5.98926842e-03, bound:  3.15526336e-01\n",
      "Epoch: 20736 mean train loss:  5.98896947e-03, bound:  3.15526336e-01\n",
      "Epoch: 20737 mean train loss:  5.98869193e-03, bound:  3.15526307e-01\n",
      "Epoch: 20738 mean train loss:  5.98843209e-03, bound:  3.15526307e-01\n",
      "Epoch: 20739 mean train loss:  5.98826539e-03, bound:  3.15526247e-01\n",
      "Epoch: 20740 mean train loss:  5.98811125e-03, bound:  3.15526247e-01\n",
      "Epoch: 20741 mean train loss:  5.98801067e-03, bound:  3.15526187e-01\n",
      "Epoch: 20742 mean train loss:  5.98808378e-03, bound:  3.15526187e-01\n",
      "Epoch: 20743 mean train loss:  5.98815037e-03, bound:  3.15526128e-01\n",
      "Epoch: 20744 mean train loss:  5.98832406e-03, bound:  3.15526158e-01\n",
      "Epoch: 20745 mean train loss:  5.98856993e-03, bound:  3.15526068e-01\n",
      "Epoch: 20746 mean train loss:  5.98879997e-03, bound:  3.15526098e-01\n",
      "Epoch: 20747 mean train loss:  5.98904397e-03, bound:  3.15526009e-01\n",
      "Epoch: 20748 mean train loss:  5.98926190e-03, bound:  3.15526068e-01\n",
      "Epoch: 20749 mean train loss:  5.98915201e-03, bound:  3.15525949e-01\n",
      "Epoch: 20750 mean train loss:  5.98874316e-03, bound:  3.15525979e-01\n",
      "Epoch: 20751 mean train loss:  5.98798925e-03, bound:  3.15525919e-01\n",
      "Epoch: 20752 mean train loss:  5.98665420e-03, bound:  3.15525949e-01\n",
      "Epoch: 20753 mean train loss:  5.98507281e-03, bound:  3.15525860e-01\n",
      "Epoch: 20754 mean train loss:  5.98319899e-03, bound:  3.15525860e-01\n",
      "Epoch: 20755 mean train loss:  5.98125439e-03, bound:  3.15525830e-01\n",
      "Epoch: 20756 mean train loss:  5.97957801e-03, bound:  3.15525860e-01\n",
      "Epoch: 20757 mean train loss:  5.97821595e-03, bound:  3.15525740e-01\n",
      "Epoch: 20758 mean train loss:  5.97741688e-03, bound:  3.15525740e-01\n",
      "Epoch: 20759 mean train loss:  5.97700290e-03, bound:  3.15525740e-01\n",
      "Epoch: 20760 mean train loss:  5.97686879e-03, bound:  3.15525681e-01\n",
      "Epoch: 20761 mean train loss:  5.97682036e-03, bound:  3.15525681e-01\n",
      "Epoch: 20762 mean train loss:  5.97685017e-03, bound:  3.15525651e-01\n",
      "Epoch: 20763 mean train loss:  5.97664854e-03, bound:  3.15525651e-01\n",
      "Epoch: 20764 mean train loss:  5.97624900e-03, bound:  3.15525621e-01\n",
      "Epoch: 20765 mean train loss:  5.97566366e-03, bound:  3.15525621e-01\n",
      "Epoch: 20766 mean train loss:  5.97492419e-03, bound:  3.15525532e-01\n",
      "Epoch: 20767 mean train loss:  5.97407203e-03, bound:  3.15525532e-01\n",
      "Epoch: 20768 mean train loss:  5.97320031e-03, bound:  3.15525502e-01\n",
      "Epoch: 20769 mean train loss:  5.97244781e-03, bound:  3.15525502e-01\n",
      "Epoch: 20770 mean train loss:  5.97174233e-03, bound:  3.15525442e-01\n",
      "Epoch: 20771 mean train loss:  5.97114721e-03, bound:  3.15525413e-01\n",
      "Epoch: 20772 mean train loss:  5.97059214e-03, bound:  3.15525413e-01\n",
      "Epoch: 20773 mean train loss:  5.97021542e-03, bound:  3.15525383e-01\n",
      "Epoch: 20774 mean train loss:  5.96986525e-03, bound:  3.15525323e-01\n",
      "Epoch: 20775 mean train loss:  5.96958818e-03, bound:  3.15525293e-01\n",
      "Epoch: 20776 mean train loss:  5.96922403e-03, bound:  3.15525293e-01\n",
      "Epoch: 20777 mean train loss:  5.96881518e-03, bound:  3.15525234e-01\n",
      "Epoch: 20778 mean train loss:  5.96840028e-03, bound:  3.15525234e-01\n",
      "Epoch: 20779 mean train loss:  5.96799841e-03, bound:  3.15525204e-01\n",
      "Epoch: 20780 mean train loss:  5.96747128e-03, bound:  3.15525204e-01\n",
      "Epoch: 20781 mean train loss:  5.96680539e-03, bound:  3.15525115e-01\n",
      "Epoch: 20782 mean train loss:  5.96629735e-03, bound:  3.15525115e-01\n",
      "Epoch: 20783 mean train loss:  5.96565427e-03, bound:  3.15525115e-01\n",
      "Epoch: 20784 mean train loss:  5.96504286e-03, bound:  3.15525085e-01\n",
      "Epoch: 20785 mean train loss:  5.96441049e-03, bound:  3.15525055e-01\n",
      "Epoch: 20786 mean train loss:  5.96386986e-03, bound:  3.15525055e-01\n",
      "Epoch: 20787 mean train loss:  5.96336136e-03, bound:  3.15524995e-01\n",
      "Epoch: 20788 mean train loss:  5.96288778e-03, bound:  3.15524995e-01\n",
      "Epoch: 20789 mean train loss:  5.96242771e-03, bound:  3.15524966e-01\n",
      "Epoch: 20790 mean train loss:  5.96204400e-03, bound:  3.15524936e-01\n",
      "Epoch: 20791 mean train loss:  5.96157461e-03, bound:  3.15524936e-01\n",
      "Epoch: 20792 mean train loss:  5.96120581e-03, bound:  3.15524876e-01\n",
      "Epoch: 20793 mean train loss:  5.96082164e-03, bound:  3.15524846e-01\n",
      "Epoch: 20794 mean train loss:  5.96034620e-03, bound:  3.15524787e-01\n",
      "Epoch: 20795 mean train loss:  5.95994852e-03, bound:  3.15524787e-01\n",
      "Epoch: 20796 mean train loss:  5.95954666e-03, bound:  3.15524757e-01\n",
      "Epoch: 20797 mean train loss:  5.95905259e-03, bound:  3.15524757e-01\n",
      "Epoch: 20798 mean train loss:  5.95863024e-03, bound:  3.15524727e-01\n",
      "Epoch: 20799 mean train loss:  5.95817901e-03, bound:  3.15524727e-01\n",
      "Epoch: 20800 mean train loss:  5.95761836e-03, bound:  3.15524638e-01\n",
      "Epoch: 20801 mean train loss:  5.95702324e-03, bound:  3.15524638e-01\n",
      "Epoch: 20802 mean train loss:  5.95653336e-03, bound:  3.15524608e-01\n",
      "Epoch: 20803 mean train loss:  5.95596526e-03, bound:  3.15524608e-01\n",
      "Epoch: 20804 mean train loss:  5.95556945e-03, bound:  3.15524548e-01\n",
      "Epoch: 20805 mean train loss:  5.95504139e-03, bound:  3.15524518e-01\n",
      "Epoch: 20806 mean train loss:  5.95457060e-03, bound:  3.15524489e-01\n",
      "Epoch: 20807 mean train loss:  5.95415523e-03, bound:  3.15524489e-01\n",
      "Epoch: 20808 mean train loss:  5.95364021e-03, bound:  3.15524429e-01\n",
      "Epoch: 20809 mean train loss:  5.95315173e-03, bound:  3.15524429e-01\n",
      "Epoch: 20810 mean train loss:  5.95275685e-03, bound:  3.15524399e-01\n",
      "Epoch: 20811 mean train loss:  5.95233263e-03, bound:  3.15524399e-01\n",
      "Epoch: 20812 mean train loss:  5.95191959e-03, bound:  3.15524310e-01\n",
      "Epoch: 20813 mean train loss:  5.95155871e-03, bound:  3.15524310e-01\n",
      "Epoch: 20814 mean train loss:  5.95115870e-03, bound:  3.15524310e-01\n",
      "Epoch: 20815 mean train loss:  5.95077546e-03, bound:  3.15524310e-01\n",
      "Epoch: 20816 mean train loss:  5.95039967e-03, bound:  3.15524280e-01\n",
      "Epoch: 20817 mean train loss:  5.94999781e-03, bound:  3.15524220e-01\n",
      "Epoch: 20818 mean train loss:  5.94963040e-03, bound:  3.15524191e-01\n",
      "Epoch: 20819 mean train loss:  5.94918244e-03, bound:  3.15524191e-01\n",
      "Epoch: 20820 mean train loss:  5.94882434e-03, bound:  3.15524161e-01\n",
      "Epoch: 20821 mean train loss:  5.94853889e-03, bound:  3.15524161e-01\n",
      "Epoch: 20822 mean train loss:  5.94823062e-03, bound:  3.15524071e-01\n",
      "Epoch: 20823 mean train loss:  5.94810909e-03, bound:  3.15524101e-01\n",
      "Epoch: 20824 mean train loss:  5.94796333e-03, bound:  3.15524012e-01\n",
      "Epoch: 20825 mean train loss:  5.94787765e-03, bound:  3.15524012e-01\n",
      "Epoch: 20826 mean train loss:  5.94790652e-03, bound:  3.15523952e-01\n",
      "Epoch: 20827 mean train loss:  5.94790094e-03, bound:  3.15523982e-01\n",
      "Epoch: 20828 mean train loss:  5.94791397e-03, bound:  3.15523922e-01\n",
      "Epoch: 20829 mean train loss:  5.94793586e-03, bound:  3.15523922e-01\n",
      "Epoch: 20830 mean train loss:  5.94784226e-03, bound:  3.15523863e-01\n",
      "Epoch: 20831 mean train loss:  5.94757358e-03, bound:  3.15523863e-01\n",
      "Epoch: 20832 mean train loss:  5.94710046e-03, bound:  3.15523803e-01\n",
      "Epoch: 20833 mean train loss:  5.94636751e-03, bound:  3.15523833e-01\n",
      "Epoch: 20834 mean train loss:  5.94535377e-03, bound:  3.15523744e-01\n",
      "Epoch: 20835 mean train loss:  5.94403455e-03, bound:  3.15523773e-01\n",
      "Epoch: 20836 mean train loss:  5.94251929e-03, bound:  3.15523714e-01\n",
      "Epoch: 20837 mean train loss:  5.94113208e-03, bound:  3.15523714e-01\n",
      "Epoch: 20838 mean train loss:  5.93985757e-03, bound:  3.15523684e-01\n",
      "Epoch: 20839 mean train loss:  5.93872136e-03, bound:  3.15523624e-01\n",
      "Epoch: 20840 mean train loss:  5.93786780e-03, bound:  3.15523624e-01\n",
      "Epoch: 20841 mean train loss:  5.93718840e-03, bound:  3.15523624e-01\n",
      "Epoch: 20842 mean train loss:  5.93677722e-03, bound:  3.15523565e-01\n",
      "Epoch: 20843 mean train loss:  5.93642751e-03, bound:  3.15523535e-01\n",
      "Epoch: 20844 mean train loss:  5.93620911e-03, bound:  3.15523505e-01\n",
      "Epoch: 20845 mean train loss:  5.93597163e-03, bound:  3.15523505e-01\n",
      "Epoch: 20846 mean train loss:  5.93575696e-03, bound:  3.15523505e-01\n",
      "Epoch: 20847 mean train loss:  5.93544869e-03, bound:  3.15523446e-01\n",
      "Epoch: 20848 mean train loss:  5.93527639e-03, bound:  3.15523446e-01\n",
      "Epoch: 20849 mean train loss:  5.93479117e-03, bound:  3.15523386e-01\n",
      "Epoch: 20850 mean train loss:  5.93427522e-03, bound:  3.15523386e-01\n",
      "Epoch: 20851 mean train loss:  5.93371596e-03, bound:  3.15523326e-01\n",
      "Epoch: 20852 mean train loss:  5.93304681e-03, bound:  3.15523326e-01\n",
      "Epoch: 20853 mean train loss:  5.93231851e-03, bound:  3.15523297e-01\n",
      "Epoch: 20854 mean train loss:  5.93168940e-03, bound:  3.15523297e-01\n",
      "Epoch: 20855 mean train loss:  5.93093736e-03, bound:  3.15523207e-01\n",
      "Epoch: 20856 mean train loss:  5.93029661e-03, bound:  3.15523207e-01\n",
      "Epoch: 20857 mean train loss:  5.92966657e-03, bound:  3.15523177e-01\n",
      "Epoch: 20858 mean train loss:  5.92903327e-03, bound:  3.15523177e-01\n",
      "Epoch: 20859 mean train loss:  5.92864072e-03, bound:  3.15523148e-01\n",
      "Epoch: 20860 mean train loss:  5.92807727e-03, bound:  3.15523118e-01\n",
      "Epoch: 20861 mean train loss:  5.92763722e-03, bound:  3.15523118e-01\n",
      "Epoch: 20862 mean train loss:  5.92726097e-03, bound:  3.15523058e-01\n",
      "Epoch: 20863 mean train loss:  5.92678366e-03, bound:  3.15523028e-01\n",
      "Epoch: 20864 mean train loss:  5.92627330e-03, bound:  3.15522999e-01\n",
      "Epoch: 20865 mean train loss:  5.92583837e-03, bound:  3.15522999e-01\n",
      "Epoch: 20866 mean train loss:  5.92545327e-03, bound:  3.15522939e-01\n",
      "Epoch: 20867 mean train loss:  5.92505699e-03, bound:  3.15522939e-01\n",
      "Epoch: 20868 mean train loss:  5.92463883e-03, bound:  3.15522909e-01\n",
      "Epoch: 20869 mean train loss:  5.92422532e-03, bound:  3.15522879e-01\n",
      "Epoch: 20870 mean train loss:  5.92383882e-03, bound:  3.15522850e-01\n",
      "Epoch: 20871 mean train loss:  5.92336338e-03, bound:  3.15522850e-01\n",
      "Epoch: 20872 mean train loss:  5.92303649e-03, bound:  3.15522820e-01\n",
      "Epoch: 20873 mean train loss:  5.92256943e-03, bound:  3.15522820e-01\n",
      "Epoch: 20874 mean train loss:  5.92218898e-03, bound:  3.15522730e-01\n",
      "Epoch: 20875 mean train loss:  5.92181133e-03, bound:  3.15522730e-01\n",
      "Epoch: 20876 mean train loss:  5.92141878e-03, bound:  3.15522701e-01\n",
      "Epoch: 20877 mean train loss:  5.92100900e-03, bound:  3.15522701e-01\n",
      "Epoch: 20878 mean train loss:  5.92050375e-03, bound:  3.15522641e-01\n",
      "Epoch: 20879 mean train loss:  5.92007069e-03, bound:  3.15522641e-01\n",
      "Epoch: 20880 mean train loss:  5.91963623e-03, bound:  3.15522611e-01\n",
      "Epoch: 20881 mean train loss:  5.91927441e-03, bound:  3.15522581e-01\n",
      "Epoch: 20882 mean train loss:  5.91886463e-03, bound:  3.15522522e-01\n",
      "Epoch: 20883 mean train loss:  5.91857266e-03, bound:  3.15522552e-01\n",
      "Epoch: 20884 mean train loss:  5.91824716e-03, bound:  3.15522492e-01\n",
      "Epoch: 20885 mean train loss:  5.91780758e-03, bound:  3.15522492e-01\n",
      "Epoch: 20886 mean train loss:  5.91757614e-03, bound:  3.15522432e-01\n",
      "Epoch: 20887 mean train loss:  5.91719989e-03, bound:  3.15522432e-01\n",
      "Epoch: 20888 mean train loss:  5.91684505e-03, bound:  3.15522373e-01\n",
      "Epoch: 20889 mean train loss:  5.91656612e-03, bound:  3.15522403e-01\n",
      "Epoch: 20890 mean train loss:  5.91615681e-03, bound:  3.15522313e-01\n",
      "Epoch: 20891 mean train loss:  5.91581687e-03, bound:  3.15522313e-01\n",
      "Epoch: 20892 mean train loss:  5.91537217e-03, bound:  3.15522254e-01\n",
      "Epoch: 20893 mean train loss:  5.91502571e-03, bound:  3.15522283e-01\n",
      "Epoch: 20894 mean train loss:  5.91456098e-03, bound:  3.15522194e-01\n",
      "Epoch: 20895 mean train loss:  5.91403525e-03, bound:  3.15522254e-01\n",
      "Epoch: 20896 mean train loss:  5.91352582e-03, bound:  3.15522164e-01\n",
      "Epoch: 20897 mean train loss:  5.91285480e-03, bound:  3.15522194e-01\n",
      "Epoch: 20898 mean train loss:  5.91221452e-03, bound:  3.15522075e-01\n",
      "Epoch: 20899 mean train loss:  5.91159519e-03, bound:  3.15522134e-01\n",
      "Epoch: 20900 mean train loss:  5.91085758e-03, bound:  3.15522075e-01\n",
      "Epoch: 20901 mean train loss:  5.91019168e-03, bound:  3.15522075e-01\n",
      "Epoch: 20902 mean train loss:  5.90958726e-03, bound:  3.15522045e-01\n",
      "Epoch: 20903 mean train loss:  5.90896467e-03, bound:  3.15522045e-01\n",
      "Epoch: 20904 mean train loss:  5.90827782e-03, bound:  3.15521955e-01\n",
      "Epoch: 20905 mean train loss:  5.90761891e-03, bound:  3.15521955e-01\n",
      "Epoch: 20906 mean train loss:  5.90692833e-03, bound:  3.15521926e-01\n",
      "Epoch: 20907 mean train loss:  5.90621680e-03, bound:  3.15521926e-01\n",
      "Epoch: 20908 mean train loss:  5.90567943e-03, bound:  3.15521866e-01\n",
      "Epoch: 20909 mean train loss:  5.90499723e-03, bound:  3.15521866e-01\n",
      "Epoch: 20910 mean train loss:  5.90441981e-03, bound:  3.15521836e-01\n",
      "Epoch: 20911 mean train loss:  5.90390060e-03, bound:  3.15521806e-01\n",
      "Epoch: 20912 mean train loss:  5.90342283e-03, bound:  3.15521806e-01\n",
      "Epoch: 20913 mean train loss:  5.90279000e-03, bound:  3.15521747e-01\n",
      "Epoch: 20914 mean train loss:  5.90233551e-03, bound:  3.15521717e-01\n",
      "Epoch: 20915 mean train loss:  5.90192853e-03, bound:  3.15521687e-01\n",
      "Epoch: 20916 mean train loss:  5.90145495e-03, bound:  3.15521687e-01\n",
      "Epoch: 20917 mean train loss:  5.90100884e-03, bound:  3.15521628e-01\n",
      "Epoch: 20918 mean train loss:  5.90054644e-03, bound:  3.15521628e-01\n",
      "Epoch: 20919 mean train loss:  5.90014132e-03, bound:  3.15521598e-01\n",
      "Epoch: 20920 mean train loss:  5.89975109e-03, bound:  3.15521598e-01\n",
      "Epoch: 20921 mean train loss:  5.89935714e-03, bound:  3.15521508e-01\n",
      "Epoch: 20922 mean train loss:  5.89898322e-03, bound:  3.15521508e-01\n",
      "Epoch: 20923 mean train loss:  5.89872245e-03, bound:  3.15521479e-01\n",
      "Epoch: 20924 mean train loss:  5.89830521e-03, bound:  3.15521479e-01\n",
      "Epoch: 20925 mean train loss:  5.89783583e-03, bound:  3.15521479e-01\n",
      "Epoch: 20926 mean train loss:  5.89748006e-03, bound:  3.15521419e-01\n",
      "Epoch: 20927 mean train loss:  5.89717692e-03, bound:  3.15521389e-01\n",
      "Epoch: 20928 mean train loss:  5.89681696e-03, bound:  3.15521389e-01\n",
      "Epoch: 20929 mean train loss:  5.89645421e-03, bound:  3.15521359e-01\n",
      "Epoch: 20930 mean train loss:  5.89625072e-03, bound:  3.15521359e-01\n",
      "Epoch: 20931 mean train loss:  5.89607749e-03, bound:  3.15521270e-01\n",
      "Epoch: 20932 mean train loss:  5.89591870e-03, bound:  3.15521300e-01\n",
      "Epoch: 20933 mean train loss:  5.89571055e-03, bound:  3.15521210e-01\n",
      "Epoch: 20934 mean train loss:  5.89557784e-03, bound:  3.15521240e-01\n",
      "Epoch: 20935 mean train loss:  5.89554524e-03, bound:  3.15521181e-01\n",
      "Epoch: 20936 mean train loss:  5.89552009e-03, bound:  3.15521181e-01\n",
      "Epoch: 20937 mean train loss:  5.89546375e-03, bound:  3.15521091e-01\n",
      "Epoch: 20938 mean train loss:  5.89554058e-03, bound:  3.15521121e-01\n",
      "Epoch: 20939 mean train loss:  5.89547120e-03, bound:  3.15521061e-01\n",
      "Epoch: 20940 mean train loss:  5.89534314e-03, bound:  3.15521061e-01\n",
      "Epoch: 20941 mean train loss:  5.89512475e-03, bound:  3.15521002e-01\n",
      "Epoch: 20942 mean train loss:  5.89474943e-03, bound:  3.15521032e-01\n",
      "Epoch: 20943 mean train loss:  5.89403836e-03, bound:  3.15520942e-01\n",
      "Epoch: 20944 mean train loss:  5.89311123e-03, bound:  3.15520972e-01\n",
      "Epoch: 20945 mean train loss:  5.89195546e-03, bound:  3.15520912e-01\n",
      "Epoch: 20946 mean train loss:  5.89056360e-03, bound:  3.15520942e-01\n",
      "Epoch: 20947 mean train loss:  5.88913122e-03, bound:  3.15520853e-01\n",
      "Epoch: 20948 mean train loss:  5.88773331e-03, bound:  3.15520853e-01\n",
      "Epoch: 20949 mean train loss:  5.88652166e-03, bound:  3.15520823e-01\n",
      "Epoch: 20950 mean train loss:  5.88564761e-03, bound:  3.15520823e-01\n",
      "Epoch: 20951 mean train loss:  5.88484015e-03, bound:  3.15520763e-01\n",
      "Epoch: 20952 mean train loss:  5.88438893e-03, bound:  3.15520763e-01\n",
      "Epoch: 20953 mean train loss:  5.88417705e-03, bound:  3.15520734e-01\n",
      "Epoch: 20954 mean train loss:  5.88393258e-03, bound:  3.15520704e-01\n",
      "Epoch: 20955 mean train loss:  5.88368857e-03, bound:  3.15520704e-01\n",
      "Epoch: 20956 mean train loss:  5.88348694e-03, bound:  3.15520644e-01\n",
      "Epoch: 20957 mean train loss:  5.88321313e-03, bound:  3.15520674e-01\n",
      "Epoch: 20958 mean train loss:  5.88296028e-03, bound:  3.15520614e-01\n",
      "Epoch: 20959 mean train loss:  5.88260172e-03, bound:  3.15520614e-01\n",
      "Epoch: 20960 mean train loss:  5.88218495e-03, bound:  3.15520525e-01\n",
      "Epoch: 20961 mean train loss:  5.88164805e-03, bound:  3.15520555e-01\n",
      "Epoch: 20962 mean train loss:  5.88097982e-03, bound:  3.15520495e-01\n",
      "Epoch: 20963 mean train loss:  5.88034419e-03, bound:  3.15520495e-01\n",
      "Epoch: 20964 mean train loss:  5.87959541e-03, bound:  3.15520465e-01\n",
      "Epoch: 20965 mean train loss:  5.87885687e-03, bound:  3.15520465e-01\n",
      "Epoch: 20966 mean train loss:  5.87814720e-03, bound:  3.15520376e-01\n",
      "Epoch: 20967 mean train loss:  5.87757956e-03, bound:  3.15520376e-01\n",
      "Epoch: 20968 mean train loss:  5.87698910e-03, bound:  3.15520376e-01\n",
      "Epoch: 20969 mean train loss:  5.87640470e-03, bound:  3.15520346e-01\n",
      "Epoch: 20970 mean train loss:  5.87596418e-03, bound:  3.15520287e-01\n",
      "Epoch: 20971 mean train loss:  5.87555626e-03, bound:  3.15520257e-01\n",
      "Epoch: 20972 mean train loss:  5.87509759e-03, bound:  3.15520257e-01\n",
      "Epoch: 20973 mean train loss:  5.87463984e-03, bound:  3.15520227e-01\n",
      "Epoch: 20974 mean train loss:  5.87426359e-03, bound:  3.15520197e-01\n",
      "Epoch: 20975 mean train loss:  5.87383285e-03, bound:  3.15520167e-01\n",
      "Epoch: 20976 mean train loss:  5.87339466e-03, bound:  3.15520138e-01\n",
      "Epoch: 20977 mean train loss:  5.87302260e-03, bound:  3.15520138e-01\n",
      "Epoch: 20978 mean train loss:  5.87251410e-03, bound:  3.15520138e-01\n",
      "Epoch: 20979 mean train loss:  5.87213179e-03, bound:  3.15520048e-01\n",
      "Epoch: 20980 mean train loss:  5.87164052e-03, bound:  3.15520048e-01\n",
      "Epoch: 20981 mean train loss:  5.87122003e-03, bound:  3.15520018e-01\n",
      "Epoch: 20982 mean train loss:  5.87080233e-03, bound:  3.15520018e-01\n",
      "Epoch: 20983 mean train loss:  5.87030314e-03, bound:  3.15519959e-01\n",
      "Epoch: 20984 mean train loss:  5.86979743e-03, bound:  3.15519959e-01\n",
      "Epoch: 20985 mean train loss:  5.86928986e-03, bound:  3.15519929e-01\n",
      "Epoch: 20986 mean train loss:  5.86887728e-03, bound:  3.15519899e-01\n",
      "Epoch: 20987 mean train loss:  5.86833153e-03, bound:  3.15519840e-01\n",
      "Epoch: 20988 mean train loss:  5.86788170e-03, bound:  3.15519840e-01\n",
      "Epoch: 20989 mean train loss:  5.86736714e-03, bound:  3.15519810e-01\n",
      "Epoch: 20990 mean train loss:  5.86688332e-03, bound:  3.15519810e-01\n",
      "Epoch: 20991 mean train loss:  5.86636923e-03, bound:  3.15519780e-01\n",
      "Epoch: 20992 mean train loss:  5.86588308e-03, bound:  3.15519750e-01\n",
      "Epoch: 20993 mean train loss:  5.86541789e-03, bound:  3.15519720e-01\n",
      "Epoch: 20994 mean train loss:  5.86499041e-03, bound:  3.15519691e-01\n",
      "Epoch: 20995 mean train loss:  5.86458482e-03, bound:  3.15519631e-01\n",
      "Epoch: 20996 mean train loss:  5.86411031e-03, bound:  3.15519631e-01\n",
      "Epoch: 20997 mean train loss:  5.86369121e-03, bound:  3.15519631e-01\n",
      "Epoch: 20998 mean train loss:  5.86330378e-03, bound:  3.15519601e-01\n",
      "Epoch: 20999 mean train loss:  5.86295454e-03, bound:  3.15519571e-01\n",
      "Epoch: 21000 mean train loss:  5.86261693e-03, bound:  3.15519571e-01\n",
      "Epoch: 21001 mean train loss:  5.86222159e-03, bound:  3.15519512e-01\n",
      "Epoch: 21002 mean train loss:  5.86196361e-03, bound:  3.15519482e-01\n",
      "Epoch: 21003 mean train loss:  5.86179830e-03, bound:  3.15519452e-01\n",
      "Epoch: 21004 mean train loss:  5.86158177e-03, bound:  3.15519452e-01\n",
      "Epoch: 21005 mean train loss:  5.86143276e-03, bound:  3.15519392e-01\n",
      "Epoch: 21006 mean train loss:  5.86155290e-03, bound:  3.15519392e-01\n",
      "Epoch: 21007 mean train loss:  5.86172426e-03, bound:  3.15519363e-01\n",
      "Epoch: 21008 mean train loss:  5.86211588e-03, bound:  3.15519363e-01\n",
      "Epoch: 21009 mean train loss:  5.86270075e-03, bound:  3.15519273e-01\n",
      "Epoch: 21010 mean train loss:  5.86336153e-03, bound:  3.15519273e-01\n",
      "Epoch: 21011 mean train loss:  5.86408889e-03, bound:  3.15519243e-01\n",
      "Epoch: 21012 mean train loss:  5.86495548e-03, bound:  3.15519243e-01\n",
      "Epoch: 21013 mean train loss:  5.86560182e-03, bound:  3.15519154e-01\n",
      "Epoch: 21014 mean train loss:  5.86577877e-03, bound:  3.15519243e-01\n",
      "Epoch: 21015 mean train loss:  5.86551381e-03, bound:  3.15519124e-01\n",
      "Epoch: 21016 mean train loss:  5.86431986e-03, bound:  3.15519154e-01\n",
      "Epoch: 21017 mean train loss:  5.86220948e-03, bound:  3.15519065e-01\n",
      "Epoch: 21018 mean train loss:  5.85946720e-03, bound:  3.15519124e-01\n",
      "Epoch: 21019 mean train loss:  5.85656427e-03, bound:  3.15519035e-01\n",
      "Epoch: 21020 mean train loss:  5.85408974e-03, bound:  3.15519035e-01\n",
      "Epoch: 21021 mean train loss:  5.85247856e-03, bound:  3.15519005e-01\n",
      "Epoch: 21022 mean train loss:  5.85171347e-03, bound:  3.15519005e-01\n",
      "Epoch: 21023 mean train loss:  5.85167017e-03, bound:  3.15518945e-01\n",
      "Epoch: 21024 mean train loss:  5.85202221e-03, bound:  3.15518945e-01\n",
      "Epoch: 21025 mean train loss:  5.85231371e-03, bound:  3.15518945e-01\n",
      "Epoch: 21026 mean train loss:  5.85244410e-03, bound:  3.15518886e-01\n",
      "Epoch: 21027 mean train loss:  5.85223222e-03, bound:  3.15518886e-01\n",
      "Epoch: 21028 mean train loss:  5.85143128e-03, bound:  3.15518826e-01\n",
      "Epoch: 21029 mean train loss:  5.85039146e-03, bound:  3.15518826e-01\n",
      "Epoch: 21030 mean train loss:  5.84921194e-03, bound:  3.15518796e-01\n",
      "Epoch: 21031 mean train loss:  5.84817305e-03, bound:  3.15518796e-01\n",
      "Epoch: 21032 mean train loss:  5.84726036e-03, bound:  3.15518737e-01\n",
      "Epoch: 21033 mean train loss:  5.84667875e-03, bound:  3.15518707e-01\n",
      "Epoch: 21034 mean train loss:  5.84636582e-03, bound:  3.15518707e-01\n",
      "Epoch: 21035 mean train loss:  5.84612275e-03, bound:  3.15518677e-01\n",
      "Epoch: 21036 mean train loss:  5.84597047e-03, bound:  3.15518677e-01\n",
      "Epoch: 21037 mean train loss:  5.84569713e-03, bound:  3.15518618e-01\n",
      "Epoch: 21038 mean train loss:  5.84531669e-03, bound:  3.15518588e-01\n",
      "Epoch: 21039 mean train loss:  5.84483379e-03, bound:  3.15518558e-01\n",
      "Epoch: 21040 mean train loss:  5.84422518e-03, bound:  3.15518558e-01\n",
      "Epoch: 21041 mean train loss:  5.84354019e-03, bound:  3.15518498e-01\n",
      "Epoch: 21042 mean train loss:  5.84292598e-03, bound:  3.15518498e-01\n",
      "Epoch: 21043 mean train loss:  5.84228151e-03, bound:  3.15518469e-01\n",
      "Epoch: 21044 mean train loss:  5.84174693e-03, bound:  3.15518439e-01\n",
      "Epoch: 21045 mean train loss:  5.84114995e-03, bound:  3.15518409e-01\n",
      "Epoch: 21046 mean train loss:  5.84076392e-03, bound:  3.15518379e-01\n",
      "Epoch: 21047 mean train loss:  5.84035134e-03, bound:  3.15518379e-01\n",
      "Epoch: 21048 mean train loss:  5.83987450e-03, bound:  3.15518320e-01\n",
      "Epoch: 21049 mean train loss:  5.83952852e-03, bound:  3.15518320e-01\n",
      "Epoch: 21050 mean train loss:  5.83914109e-03, bound:  3.15518260e-01\n",
      "Epoch: 21051 mean train loss:  5.83864935e-03, bound:  3.15518260e-01\n",
      "Epoch: 21052 mean train loss:  5.83816599e-03, bound:  3.15518230e-01\n",
      "Epoch: 21053 mean train loss:  5.83771104e-03, bound:  3.15518230e-01\n",
      "Epoch: 21054 mean train loss:  5.83715737e-03, bound:  3.15518171e-01\n",
      "Epoch: 21055 mean train loss:  5.83668007e-03, bound:  3.15518171e-01\n",
      "Epoch: 21056 mean train loss:  5.83616365e-03, bound:  3.15518141e-01\n",
      "Epoch: 21057 mean train loss:  5.83567098e-03, bound:  3.15518141e-01\n",
      "Epoch: 21058 mean train loss:  5.83528215e-03, bound:  3.15518111e-01\n",
      "Epoch: 21059 mean train loss:  5.83479740e-03, bound:  3.15518051e-01\n",
      "Epoch: 21060 mean train loss:  5.83434245e-03, bound:  3.15518022e-01\n",
      "Epoch: 21061 mean train loss:  5.83393732e-03, bound:  3.15518022e-01\n",
      "Epoch: 21062 mean train loss:  5.83351543e-03, bound:  3.15517992e-01\n",
      "Epoch: 21063 mean train loss:  5.83304837e-03, bound:  3.15517962e-01\n",
      "Epoch: 21064 mean train loss:  5.83263207e-03, bound:  3.15517932e-01\n",
      "Epoch: 21065 mean train loss:  5.83218830e-03, bound:  3.15517902e-01\n",
      "Epoch: 21066 mean train loss:  5.83177153e-03, bound:  3.15517902e-01\n",
      "Epoch: 21067 mean train loss:  5.83131844e-03, bound:  3.15517843e-01\n",
      "Epoch: 21068 mean train loss:  5.83093055e-03, bound:  3.15517813e-01\n",
      "Epoch: 21069 mean train loss:  5.83054218e-03, bound:  3.15517813e-01\n",
      "Epoch: 21070 mean train loss:  5.83014451e-03, bound:  3.15517813e-01\n",
      "Epoch: 21071 mean train loss:  5.82973380e-03, bound:  3.15517753e-01\n",
      "Epoch: 21072 mean train loss:  5.82922855e-03, bound:  3.15517753e-01\n",
      "Epoch: 21073 mean train loss:  5.82876336e-03, bound:  3.15517694e-01\n",
      "Epoch: 21074 mean train loss:  5.82825812e-03, bound:  3.15517694e-01\n",
      "Epoch: 21075 mean train loss:  5.82788372e-03, bound:  3.15517664e-01\n",
      "Epoch: 21076 mean train loss:  5.82736451e-03, bound:  3.15517604e-01\n",
      "Epoch: 21077 mean train loss:  5.82690770e-03, bound:  3.15517604e-01\n",
      "Epoch: 21078 mean train loss:  5.82646113e-03, bound:  3.15517604e-01\n",
      "Epoch: 21079 mean train loss:  5.82595542e-03, bound:  3.15517575e-01\n",
      "Epoch: 21080 mean train loss:  5.82549674e-03, bound:  3.15517545e-01\n",
      "Epoch: 21081 mean train loss:  5.82511118e-03, bound:  3.15517515e-01\n",
      "Epoch: 21082 mean train loss:  5.82462130e-03, bound:  3.15517485e-01\n",
      "Epoch: 21083 mean train loss:  5.82420081e-03, bound:  3.15517455e-01\n",
      "Epoch: 21084 mean train loss:  5.82376355e-03, bound:  3.15517455e-01\n",
      "Epoch: 21085 mean train loss:  5.82328485e-03, bound:  3.15517426e-01\n",
      "Epoch: 21086 mean train loss:  5.82289463e-03, bound:  3.15517396e-01\n",
      "Epoch: 21087 mean train loss:  5.82248205e-03, bound:  3.15517366e-01\n",
      "Epoch: 21088 mean train loss:  5.82207413e-03, bound:  3.15517336e-01\n",
      "Epoch: 21089 mean train loss:  5.82163455e-03, bound:  3.15517336e-01\n",
      "Epoch: 21090 mean train loss:  5.82129089e-03, bound:  3.15517306e-01\n",
      "Epoch: 21091 mean train loss:  5.82088344e-03, bound:  3.15517247e-01\n",
      "Epoch: 21092 mean train loss:  5.82048949e-03, bound:  3.15517247e-01\n",
      "Epoch: 21093 mean train loss:  5.82010439e-03, bound:  3.15517217e-01\n",
      "Epoch: 21094 mean train loss:  5.81969833e-03, bound:  3.15517217e-01\n",
      "Epoch: 21095 mean train loss:  5.81926946e-03, bound:  3.15517157e-01\n",
      "Epoch: 21096 mean train loss:  5.81892254e-03, bound:  3.15517157e-01\n",
      "Epoch: 21097 mean train loss:  5.81870507e-03, bound:  3.15517128e-01\n",
      "Epoch: 21098 mean train loss:  5.81841171e-03, bound:  3.15517128e-01\n",
      "Epoch: 21099 mean train loss:  5.81815327e-03, bound:  3.15517038e-01\n",
      "Epoch: 21100 mean train loss:  5.81791345e-03, bound:  3.15517038e-01\n",
      "Epoch: 21101 mean train loss:  5.81769738e-03, bound:  3.15517008e-01\n",
      "Epoch: 21102 mean train loss:  5.81753859e-03, bound:  3.15517008e-01\n",
      "Epoch: 21103 mean train loss:  5.81746968e-03, bound:  3.15516949e-01\n",
      "Epoch: 21104 mean train loss:  5.81732672e-03, bound:  3.15516949e-01\n",
      "Epoch: 21105 mean train loss:  5.81720099e-03, bound:  3.15516889e-01\n",
      "Epoch: 21106 mean train loss:  5.81699563e-03, bound:  3.15516919e-01\n",
      "Epoch: 21107 mean train loss:  5.81685314e-03, bound:  3.15516829e-01\n",
      "Epoch: 21108 mean train loss:  5.81647595e-03, bound:  3.15516889e-01\n",
      "Epoch: 21109 mean train loss:  5.81600470e-03, bound:  3.15516800e-01\n",
      "Epoch: 21110 mean train loss:  5.81542123e-03, bound:  3.15516800e-01\n",
      "Epoch: 21111 mean train loss:  5.81468642e-03, bound:  3.15516710e-01\n",
      "Epoch: 21112 mean train loss:  5.81386313e-03, bound:  3.15516770e-01\n",
      "Epoch: 21113 mean train loss:  5.81291225e-03, bound:  3.15516710e-01\n",
      "Epoch: 21114 mean train loss:  5.81183331e-03, bound:  3.15516710e-01\n",
      "Epoch: 21115 mean train loss:  5.81079768e-03, bound:  3.15516651e-01\n",
      "Epoch: 21116 mean train loss:  5.80981933e-03, bound:  3.15516651e-01\n",
      "Epoch: 21117 mean train loss:  5.80890849e-03, bound:  3.15516591e-01\n",
      "Epoch: 21118 mean train loss:  5.80817834e-03, bound:  3.15516591e-01\n",
      "Epoch: 21119 mean train loss:  5.80743141e-03, bound:  3.15516561e-01\n",
      "Epoch: 21120 mean train loss:  5.80690103e-03, bound:  3.15516561e-01\n",
      "Epoch: 21121 mean train loss:  5.80639951e-03, bound:  3.15516531e-01\n",
      "Epoch: 21122 mean train loss:  5.80595760e-03, bound:  3.15516472e-01\n",
      "Epoch: 21123 mean train loss:  5.80549100e-03, bound:  3.15516472e-01\n",
      "Epoch: 21124 mean train loss:  5.80513431e-03, bound:  3.15516442e-01\n",
      "Epoch: 21125 mean train loss:  5.80475107e-03, bound:  3.15516442e-01\n",
      "Epoch: 21126 mean train loss:  5.80433942e-03, bound:  3.15516382e-01\n",
      "Epoch: 21127 mean train loss:  5.80400834e-03, bound:  3.15516353e-01\n",
      "Epoch: 21128 mean train loss:  5.80361905e-03, bound:  3.15516323e-01\n",
      "Epoch: 21129 mean train loss:  5.80330752e-03, bound:  3.15516323e-01\n",
      "Epoch: 21130 mean train loss:  5.80298854e-03, bound:  3.15516323e-01\n",
      "Epoch: 21131 mean train loss:  5.80267422e-03, bound:  3.15516323e-01\n",
      "Epoch: 21132 mean train loss:  5.80237666e-03, bound:  3.15516233e-01\n",
      "Epoch: 21133 mean train loss:  5.80196735e-03, bound:  3.15516204e-01\n",
      "Epoch: 21134 mean train loss:  5.80158830e-03, bound:  3.15516204e-01\n",
      "Epoch: 21135 mean train loss:  5.80123067e-03, bound:  3.15516204e-01\n",
      "Epoch: 21136 mean train loss:  5.80072822e-03, bound:  3.15516144e-01\n",
      "Epoch: 21137 mean train loss:  5.80028864e-03, bound:  3.15516144e-01\n",
      "Epoch: 21138 mean train loss:  5.79978013e-03, bound:  3.15516084e-01\n",
      "Epoch: 21139 mean train loss:  5.79926791e-03, bound:  3.15516084e-01\n",
      "Epoch: 21140 mean train loss:  5.79881622e-03, bound:  3.15516025e-01\n",
      "Epoch: 21141 mean train loss:  5.79831563e-03, bound:  3.15516025e-01\n",
      "Epoch: 21142 mean train loss:  5.79784205e-03, bound:  3.15515995e-01\n",
      "Epoch: 21143 mean train loss:  5.79736382e-03, bound:  3.15515995e-01\n",
      "Epoch: 21144 mean train loss:  5.79692144e-03, bound:  3.15515935e-01\n",
      "Epoch: 21145 mean train loss:  5.79645252e-03, bound:  3.15515906e-01\n",
      "Epoch: 21146 mean train loss:  5.79595659e-03, bound:  3.15515876e-01\n",
      "Epoch: 21147 mean train loss:  5.79547323e-03, bound:  3.15515876e-01\n",
      "Epoch: 21148 mean train loss:  5.79497823e-03, bound:  3.15515816e-01\n",
      "Epoch: 21149 mean train loss:  5.79448929e-03, bound:  3.15515816e-01\n",
      "Epoch: 21150 mean train loss:  5.79406321e-03, bound:  3.15515786e-01\n",
      "Epoch: 21151 mean train loss:  5.79361757e-03, bound:  3.15515786e-01\n",
      "Epoch: 21152 mean train loss:  5.79310767e-03, bound:  3.15515757e-01\n",
      "Epoch: 21153 mean train loss:  5.79261687e-03, bound:  3.15515697e-01\n",
      "Epoch: 21154 mean train loss:  5.79216797e-03, bound:  3.15515697e-01\n",
      "Epoch: 21155 mean train loss:  5.79160685e-03, bound:  3.15515697e-01\n",
      "Epoch: 21156 mean train loss:  5.79108438e-03, bound:  3.15515637e-01\n",
      "Epoch: 21157 mean train loss:  5.79058751e-03, bound:  3.15515637e-01\n",
      "Epoch: 21158 mean train loss:  5.79008367e-03, bound:  3.15515578e-01\n",
      "Epoch: 21159 mean train loss:  5.78963757e-03, bound:  3.15515578e-01\n",
      "Epoch: 21160 mean train loss:  5.78907598e-03, bound:  3.15515518e-01\n",
      "Epoch: 21161 mean train loss:  5.78869181e-03, bound:  3.15515518e-01\n",
      "Epoch: 21162 mean train loss:  5.78826712e-03, bound:  3.15515488e-01\n",
      "Epoch: 21163 mean train loss:  5.78779960e-03, bound:  3.15515488e-01\n",
      "Epoch: 21164 mean train loss:  5.78741729e-03, bound:  3.15515459e-01\n",
      "Epoch: 21165 mean train loss:  5.78706432e-03, bound:  3.15515429e-01\n",
      "Epoch: 21166 mean train loss:  5.78664616e-03, bound:  3.15515369e-01\n",
      "Epoch: 21167 mean train loss:  5.78638492e-03, bound:  3.15515369e-01\n",
      "Epoch: 21168 mean train loss:  5.78613486e-03, bound:  3.15515339e-01\n",
      "Epoch: 21169 mean train loss:  5.78584010e-03, bound:  3.15515339e-01\n",
      "Epoch: 21170 mean train loss:  5.78569667e-03, bound:  3.15515310e-01\n",
      "Epoch: 21171 mean train loss:  5.78556675e-03, bound:  3.15515310e-01\n",
      "Epoch: 21172 mean train loss:  5.78543311e-03, bound:  3.15515220e-01\n",
      "Epoch: 21173 mean train loss:  5.78554813e-03, bound:  3.15515220e-01\n",
      "Epoch: 21174 mean train loss:  5.78565849e-03, bound:  3.15515190e-01\n",
      "Epoch: 21175 mean train loss:  5.78591553e-03, bound:  3.15515190e-01\n",
      "Epoch: 21176 mean train loss:  5.78634534e-03, bound:  3.15515101e-01\n",
      "Epoch: 21177 mean train loss:  5.78688318e-03, bound:  3.15515161e-01\n",
      "Epoch: 21178 mean train loss:  5.78743592e-03, bound:  3.15515071e-01\n",
      "Epoch: 21179 mean train loss:  5.78796724e-03, bound:  3.15515101e-01\n",
      "Epoch: 21180 mean train loss:  5.78824105e-03, bound:  3.15515012e-01\n",
      "Epoch: 21181 mean train loss:  5.78819122e-03, bound:  3.15515071e-01\n",
      "Epoch: 21182 mean train loss:  5.78759145e-03, bound:  3.15514982e-01\n",
      "Epoch: 21183 mean train loss:  5.78631833e-03, bound:  3.15515012e-01\n",
      "Epoch: 21184 mean train loss:  5.78440959e-03, bound:  3.15514922e-01\n",
      "Epoch: 21185 mean train loss:  5.78207150e-03, bound:  3.15514952e-01\n",
      "Epoch: 21186 mean train loss:  5.77984564e-03, bound:  3.15514892e-01\n",
      "Epoch: 21187 mean train loss:  5.77786239e-03, bound:  3.15514892e-01\n",
      "Epoch: 21188 mean train loss:  5.77637181e-03, bound:  3.15514833e-01\n",
      "Epoch: 21189 mean train loss:  5.77568682e-03, bound:  3.15514833e-01\n",
      "Epoch: 21190 mean train loss:  5.77539532e-03, bound:  3.15514833e-01\n",
      "Epoch: 21191 mean train loss:  5.77552989e-03, bound:  3.15514773e-01\n",
      "Epoch: 21192 mean train loss:  5.77572547e-03, bound:  3.15514773e-01\n",
      "Epoch: 21193 mean train loss:  5.77586284e-03, bound:  3.15514714e-01\n",
      "Epoch: 21194 mean train loss:  5.77563746e-03, bound:  3.15514743e-01\n",
      "Epoch: 21195 mean train loss:  5.77513641e-03, bound:  3.15514654e-01\n",
      "Epoch: 21196 mean train loss:  5.77440672e-03, bound:  3.15514654e-01\n",
      "Epoch: 21197 mean train loss:  5.77343395e-03, bound:  3.15514654e-01\n",
      "Epoch: 21198 mean train loss:  5.77247469e-03, bound:  3.15514654e-01\n",
      "Epoch: 21199 mean train loss:  5.77157410e-03, bound:  3.15514565e-01\n",
      "Epoch: 21200 mean train loss:  5.77087235e-03, bound:  3.15514565e-01\n",
      "Epoch: 21201 mean train loss:  5.77029353e-03, bound:  3.15514535e-01\n",
      "Epoch: 21202 mean train loss:  5.77001879e-03, bound:  3.15514535e-01\n",
      "Epoch: 21203 mean train loss:  5.76979993e-03, bound:  3.15514535e-01\n",
      "Epoch: 21204 mean train loss:  5.76970074e-03, bound:  3.15514445e-01\n",
      "Epoch: 21205 mean train loss:  5.76948095e-03, bound:  3.15514445e-01\n",
      "Epoch: 21206 mean train loss:  5.76915964e-03, bound:  3.15514416e-01\n",
      "Epoch: 21207 mean train loss:  5.76875778e-03, bound:  3.15514416e-01\n",
      "Epoch: 21208 mean train loss:  5.76819526e-03, bound:  3.15514356e-01\n",
      "Epoch: 21209 mean train loss:  5.76755730e-03, bound:  3.15514356e-01\n",
      "Epoch: 21210 mean train loss:  5.76690957e-03, bound:  3.15514326e-01\n",
      "Epoch: 21211 mean train loss:  5.76622691e-03, bound:  3.15514296e-01\n",
      "Epoch: 21212 mean train loss:  5.76567557e-03, bound:  3.15514237e-01\n",
      "Epoch: 21213 mean train loss:  5.76512469e-03, bound:  3.15514237e-01\n",
      "Epoch: 21214 mean train loss:  5.76460082e-03, bound:  3.15514207e-01\n",
      "Epoch: 21215 mean train loss:  5.76404342e-03, bound:  3.15514207e-01\n",
      "Epoch: 21216 mean train loss:  5.76365134e-03, bound:  3.15514177e-01\n",
      "Epoch: 21217 mean train loss:  5.76324575e-03, bound:  3.15514147e-01\n",
      "Epoch: 21218 mean train loss:  5.76283876e-03, bound:  3.15514147e-01\n",
      "Epoch: 21219 mean train loss:  5.76248346e-03, bound:  3.15514088e-01\n",
      "Epoch: 21220 mean train loss:  5.76209510e-03, bound:  3.15514088e-01\n",
      "Epoch: 21221 mean train loss:  5.76165412e-03, bound:  3.15514088e-01\n",
      "Epoch: 21222 mean train loss:  5.76127181e-03, bound:  3.15514028e-01\n",
      "Epoch: 21223 mean train loss:  5.76077076e-03, bound:  3.15513968e-01\n",
      "Epoch: 21224 mean train loss:  5.76024177e-03, bound:  3.15513968e-01\n",
      "Epoch: 21225 mean train loss:  5.75981243e-03, bound:  3.15513968e-01\n",
      "Epoch: 21226 mean train loss:  5.75932302e-03, bound:  3.15513909e-01\n",
      "Epoch: 21227 mean train loss:  5.75882848e-03, bound:  3.15513909e-01\n",
      "Epoch: 21228 mean train loss:  5.75836655e-03, bound:  3.15513879e-01\n",
      "Epoch: 21229 mean train loss:  5.75788226e-03, bound:  3.15513879e-01\n",
      "Epoch: 21230 mean train loss:  5.75747341e-03, bound:  3.15513849e-01\n",
      "Epoch: 21231 mean train loss:  5.75701054e-03, bound:  3.15513790e-01\n",
      "Epoch: 21232 mean train loss:  5.75660821e-03, bound:  3.15513790e-01\n",
      "Epoch: 21233 mean train loss:  5.75622031e-03, bound:  3.15513760e-01\n",
      "Epoch: 21234 mean train loss:  5.75582543e-03, bound:  3.15513730e-01\n",
      "Epoch: 21235 mean train loss:  5.75539470e-03, bound:  3.15513730e-01\n",
      "Epoch: 21236 mean train loss:  5.75496256e-03, bound:  3.15513670e-01\n",
      "Epoch: 21237 mean train loss:  5.75453555e-03, bound:  3.15513670e-01\n",
      "Epoch: 21238 mean train loss:  5.75412950e-03, bound:  3.15513641e-01\n",
      "Epoch: 21239 mean train loss:  5.75367454e-03, bound:  3.15513611e-01\n",
      "Epoch: 21240 mean train loss:  5.75325871e-03, bound:  3.15513551e-01\n",
      "Epoch: 21241 mean train loss:  5.75280003e-03, bound:  3.15513551e-01\n",
      "Epoch: 21242 mean train loss:  5.75235207e-03, bound:  3.15513521e-01\n",
      "Epoch: 21243 mean train loss:  5.75198652e-03, bound:  3.15513521e-01\n",
      "Epoch: 21244 mean train loss:  5.75147429e-03, bound:  3.15513462e-01\n",
      "Epoch: 21245 mean train loss:  5.75097417e-03, bound:  3.15513462e-01\n",
      "Epoch: 21246 mean train loss:  5.75054949e-03, bound:  3.15513432e-01\n",
      "Epoch: 21247 mean train loss:  5.75010618e-03, bound:  3.15513432e-01\n",
      "Epoch: 21248 mean train loss:  5.74972248e-03, bound:  3.15513343e-01\n",
      "Epoch: 21249 mean train loss:  5.74925402e-03, bound:  3.15513343e-01\n",
      "Epoch: 21250 mean train loss:  5.74887265e-03, bound:  3.15513343e-01\n",
      "Epoch: 21251 mean train loss:  5.74850431e-03, bound:  3.15513343e-01\n",
      "Epoch: 21252 mean train loss:  5.74812107e-03, bound:  3.15513283e-01\n",
      "Epoch: 21253 mean train loss:  5.74771222e-03, bound:  3.15513283e-01\n",
      "Epoch: 21254 mean train loss:  5.74734434e-03, bound:  3.15513223e-01\n",
      "Epoch: 21255 mean train loss:  5.74706169e-03, bound:  3.15513223e-01\n",
      "Epoch: 21256 mean train loss:  5.74680883e-03, bound:  3.15513194e-01\n",
      "Epoch: 21257 mean train loss:  5.74654015e-03, bound:  3.15513194e-01\n",
      "Epoch: 21258 mean train loss:  5.74638508e-03, bound:  3.15513134e-01\n",
      "Epoch: 21259 mean train loss:  5.74636972e-03, bound:  3.15513134e-01\n",
      "Epoch: 21260 mean train loss:  5.74634923e-03, bound:  3.15513074e-01\n",
      "Epoch: 21261 mean train loss:  5.74648520e-03, bound:  3.15513074e-01\n",
      "Epoch: 21262 mean train loss:  5.74669335e-03, bound:  3.15513015e-01\n",
      "Epoch: 21263 mean train loss:  5.74712548e-03, bound:  3.15513074e-01\n",
      "Epoch: 21264 mean train loss:  5.74765867e-03, bound:  3.15512985e-01\n",
      "Epoch: 21265 mean train loss:  5.74824819e-03, bound:  3.15512985e-01\n",
      "Epoch: 21266 mean train loss:  5.74881071e-03, bound:  3.15512896e-01\n",
      "Epoch: 21267 mean train loss:  5.74934250e-03, bound:  3.15512955e-01\n",
      "Epoch: 21268 mean train loss:  5.74955810e-03, bound:  3.15512866e-01\n",
      "Epoch: 21269 mean train loss:  5.74934762e-03, bound:  3.15512896e-01\n",
      "Epoch: 21270 mean train loss:  5.74856997e-03, bound:  3.15512836e-01\n",
      "Epoch: 21271 mean train loss:  5.74685959e-03, bound:  3.15512836e-01\n",
      "Epoch: 21272 mean train loss:  5.74459694e-03, bound:  3.15512776e-01\n",
      "Epoch: 21273 mean train loss:  5.74216060e-03, bound:  3.15512776e-01\n",
      "Epoch: 21274 mean train loss:  5.73989516e-03, bound:  3.15512747e-01\n",
      "Epoch: 21275 mean train loss:  5.73815219e-03, bound:  3.15512747e-01\n",
      "Epoch: 21276 mean train loss:  5.73709374e-03, bound:  3.15512717e-01\n",
      "Epoch: 21277 mean train loss:  5.73665928e-03, bound:  3.15512687e-01\n",
      "Epoch: 21278 mean train loss:  5.73673239e-03, bound:  3.15512657e-01\n",
      "Epoch: 21279 mean train loss:  5.73698618e-03, bound:  3.15512657e-01\n",
      "Epoch: 21280 mean train loss:  5.73724788e-03, bound:  3.15512657e-01\n",
      "Epoch: 21281 mean train loss:  5.73730841e-03, bound:  3.15512538e-01\n",
      "Epoch: 21282 mean train loss:  5.73693216e-03, bound:  3.15512568e-01\n",
      "Epoch: 21283 mean train loss:  5.73629700e-03, bound:  3.15512538e-01\n",
      "Epoch: 21284 mean train loss:  5.73537173e-03, bound:  3.15512538e-01\n",
      "Epoch: 21285 mean train loss:  5.73436869e-03, bound:  3.15512508e-01\n",
      "Epoch: 21286 mean train loss:  5.73330326e-03, bound:  3.15512508e-01\n",
      "Epoch: 21287 mean train loss:  5.73254097e-03, bound:  3.15512419e-01\n",
      "Epoch: 21288 mean train loss:  5.73187834e-03, bound:  3.15512419e-01\n",
      "Epoch: 21289 mean train loss:  5.73136518e-03, bound:  3.15512419e-01\n",
      "Epoch: 21290 mean train loss:  5.73107693e-03, bound:  3.15512389e-01\n",
      "Epoch: 21291 mean train loss:  5.73081151e-03, bound:  3.15512389e-01\n",
      "Epoch: 21292 mean train loss:  5.73050091e-03, bound:  3.15512329e-01\n",
      "Epoch: 21293 mean train loss:  5.73027181e-03, bound:  3.15512300e-01\n",
      "Epoch: 21294 mean train loss:  5.72992070e-03, bound:  3.15512270e-01\n",
      "Epoch: 21295 mean train loss:  5.72944107e-03, bound:  3.15512270e-01\n",
      "Epoch: 21296 mean train loss:  5.72897773e-03, bound:  3.15512210e-01\n",
      "Epoch: 21297 mean train loss:  5.72842825e-03, bound:  3.15512210e-01\n",
      "Epoch: 21298 mean train loss:  5.72782569e-03, bound:  3.15512180e-01\n",
      "Epoch: 21299 mean train loss:  5.72715607e-03, bound:  3.15512151e-01\n",
      "Epoch: 21300 mean train loss:  5.72662940e-03, bound:  3.15512121e-01\n",
      "Epoch: 21301 mean train loss:  5.72612276e-03, bound:  3.15512091e-01\n",
      "Epoch: 21302 mean train loss:  5.72568271e-03, bound:  3.15512091e-01\n",
      "Epoch: 21303 mean train loss:  5.72526501e-03, bound:  3.15512061e-01\n",
      "Epoch: 21304 mean train loss:  5.72491763e-03, bound:  3.15512031e-01\n",
      "Epoch: 21305 mean train loss:  5.72442869e-03, bound:  3.15512002e-01\n",
      "Epoch: 21306 mean train loss:  5.72397606e-03, bound:  3.15511972e-01\n",
      "Epoch: 21307 mean train loss:  5.72356069e-03, bound:  3.15511942e-01\n",
      "Epoch: 21308 mean train loss:  5.72319655e-03, bound:  3.15511942e-01\n",
      "Epoch: 21309 mean train loss:  5.72272856e-03, bound:  3.15511912e-01\n",
      "Epoch: 21310 mean train loss:  5.72231179e-03, bound:  3.15511882e-01\n",
      "Epoch: 21311 mean train loss:  5.72180003e-03, bound:  3.15511853e-01\n",
      "Epoch: 21312 mean train loss:  5.72135579e-03, bound:  3.15511853e-01\n",
      "Epoch: 21313 mean train loss:  5.72099490e-03, bound:  3.15511823e-01\n",
      "Epoch: 21314 mean train loss:  5.72049711e-03, bound:  3.15511793e-01\n",
      "Epoch: 21315 mean train loss:  5.72011620e-03, bound:  3.15511763e-01\n",
      "Epoch: 21316 mean train loss:  5.71962213e-03, bound:  3.15511733e-01\n",
      "Epoch: 21317 mean train loss:  5.71918581e-03, bound:  3.15511733e-01\n",
      "Epoch: 21318 mean train loss:  5.71872480e-03, bound:  3.15511703e-01\n",
      "Epoch: 21319 mean train loss:  5.71826333e-03, bound:  3.15511674e-01\n",
      "Epoch: 21320 mean train loss:  5.71781769e-03, bound:  3.15511644e-01\n",
      "Epoch: 21321 mean train loss:  5.71739255e-03, bound:  3.15511644e-01\n",
      "Epoch: 21322 mean train loss:  5.71699999e-03, bound:  3.15511614e-01\n",
      "Epoch: 21323 mean train loss:  5.71649615e-03, bound:  3.15511554e-01\n",
      "Epoch: 21324 mean train loss:  5.71609335e-03, bound:  3.15511525e-01\n",
      "Epoch: 21325 mean train loss:  5.71566960e-03, bound:  3.15511525e-01\n",
      "Epoch: 21326 mean train loss:  5.71527425e-03, bound:  3.15511495e-01\n",
      "Epoch: 21327 mean train loss:  5.71473734e-03, bound:  3.15511435e-01\n",
      "Epoch: 21328 mean train loss:  5.71432197e-03, bound:  3.15511435e-01\n",
      "Epoch: 21329 mean train loss:  5.71394060e-03, bound:  3.15511405e-01\n",
      "Epoch: 21330 mean train loss:  5.71348984e-03, bound:  3.15511405e-01\n",
      "Epoch: 21331 mean train loss:  5.71306515e-03, bound:  3.15511376e-01\n",
      "Epoch: 21332 mean train loss:  5.71259065e-03, bound:  3.15511346e-01\n",
      "Epoch: 21333 mean train loss:  5.71213197e-03, bound:  3.15511316e-01\n",
      "Epoch: 21334 mean train loss:  5.71170636e-03, bound:  3.15511286e-01\n",
      "Epoch: 21335 mean train loss:  5.71123697e-03, bound:  3.15511286e-01\n",
      "Epoch: 21336 mean train loss:  5.71080763e-03, bound:  3.15511256e-01\n",
      "Epoch: 21337 mean train loss:  5.71041554e-03, bound:  3.15511227e-01\n",
      "Epoch: 21338 mean train loss:  5.71003184e-03, bound:  3.15511197e-01\n",
      "Epoch: 21339 mean train loss:  5.70954615e-03, bound:  3.15511167e-01\n",
      "Epoch: 21340 mean train loss:  5.70915407e-03, bound:  3.15511107e-01\n",
      "Epoch: 21341 mean train loss:  5.70875034e-03, bound:  3.15511107e-01\n",
      "Epoch: 21342 mean train loss:  5.70835499e-03, bound:  3.15511107e-01\n",
      "Epoch: 21343 mean train loss:  5.70794847e-03, bound:  3.15511078e-01\n",
      "Epoch: 21344 mean train loss:  5.70757873e-03, bound:  3.15511048e-01\n",
      "Epoch: 21345 mean train loss:  5.70722343e-03, bound:  3.15511078e-01\n",
      "Epoch: 21346 mean train loss:  5.70684858e-03, bound:  3.15510988e-01\n",
      "Epoch: 21347 mean train loss:  5.70655474e-03, bound:  3.15510988e-01\n",
      "Epoch: 21348 mean train loss:  5.70632098e-03, bound:  3.15510958e-01\n",
      "Epoch: 21349 mean train loss:  5.70620643e-03, bound:  3.15510958e-01\n",
      "Epoch: 21350 mean train loss:  5.70621574e-03, bound:  3.15510929e-01\n",
      "Epoch: 21351 mean train loss:  5.70628187e-03, bound:  3.15510929e-01\n",
      "Epoch: 21352 mean train loss:  5.70662320e-03, bound:  3.15510839e-01\n",
      "Epoch: 21353 mean train loss:  5.70700970e-03, bound:  3.15510869e-01\n",
      "Epoch: 21354 mean train loss:  5.70784416e-03, bound:  3.15510809e-01\n",
      "Epoch: 21355 mean train loss:  5.70900878e-03, bound:  3.15510839e-01\n",
      "Epoch: 21356 mean train loss:  5.71051054e-03, bound:  3.15510720e-01\n",
      "Epoch: 21357 mean train loss:  5.71244024e-03, bound:  3.15510809e-01\n",
      "Epoch: 21358 mean train loss:  5.71452687e-03, bound:  3.15510660e-01\n",
      "Epoch: 21359 mean train loss:  5.71628008e-03, bound:  3.15510750e-01\n",
      "Epoch: 21360 mean train loss:  5.71727520e-03, bound:  3.15510631e-01\n",
      "Epoch: 21361 mean train loss:  5.71654318e-03, bound:  3.15510720e-01\n",
      "Epoch: 21362 mean train loss:  5.71368309e-03, bound:  3.15510601e-01\n",
      "Epoch: 21363 mean train loss:  5.70899900e-03, bound:  3.15510631e-01\n",
      "Epoch: 21364 mean train loss:  5.70381433e-03, bound:  3.15510541e-01\n",
      "Epoch: 21365 mean train loss:  5.69965225e-03, bound:  3.15510541e-01\n",
      "Epoch: 21366 mean train loss:  5.69784222e-03, bound:  3.15510511e-01\n",
      "Epoch: 21367 mean train loss:  5.69803687e-03, bound:  3.15510511e-01\n",
      "Epoch: 21368 mean train loss:  5.69966016e-03, bound:  3.15510511e-01\n",
      "Epoch: 21369 mean train loss:  5.70134493e-03, bound:  3.15510422e-01\n",
      "Epoch: 21370 mean train loss:  5.70205785e-03, bound:  3.15510482e-01\n",
      "Epoch: 21371 mean train loss:  5.70119778e-03, bound:  3.15510392e-01\n",
      "Epoch: 21372 mean train loss:  5.69926202e-03, bound:  3.15510422e-01\n",
      "Epoch: 21373 mean train loss:  5.69685455e-03, bound:  3.15510392e-01\n",
      "Epoch: 21374 mean train loss:  5.69492532e-03, bound:  3.15510392e-01\n",
      "Epoch: 21375 mean train loss:  5.69395907e-03, bound:  3.15510303e-01\n",
      "Epoch: 21376 mean train loss:  5.69388131e-03, bound:  3.15510303e-01\n",
      "Epoch: 21377 mean train loss:  5.69418725e-03, bound:  3.15510303e-01\n",
      "Epoch: 21378 mean train loss:  5.69452625e-03, bound:  3.15510273e-01\n",
      "Epoch: 21379 mean train loss:  5.69430878e-03, bound:  3.15510273e-01\n",
      "Epoch: 21380 mean train loss:  5.69353765e-03, bound:  3.15510184e-01\n",
      "Epoch: 21381 mean train loss:  5.69252856e-03, bound:  3.15510184e-01\n",
      "Epoch: 21382 mean train loss:  5.69146872e-03, bound:  3.15510154e-01\n",
      "Epoch: 21383 mean train loss:  5.69070037e-03, bound:  3.15510154e-01\n",
      "Epoch: 21384 mean train loss:  5.69017045e-03, bound:  3.15510094e-01\n",
      "Epoch: 21385 mean train loss:  5.68989431e-03, bound:  3.15510094e-01\n",
      "Epoch: 21386 mean train loss:  5.68986079e-03, bound:  3.15510094e-01\n",
      "Epoch: 21387 mean train loss:  5.68973320e-03, bound:  3.15510035e-01\n",
      "Epoch: 21388 mean train loss:  5.68938255e-03, bound:  3.15510035e-01\n",
      "Epoch: 21389 mean train loss:  5.68882935e-03, bound:  3.15509975e-01\n",
      "Epoch: 21390 mean train loss:  5.68816299e-03, bound:  3.15509975e-01\n",
      "Epoch: 21391 mean train loss:  5.68742026e-03, bound:  3.15509945e-01\n",
      "Epoch: 21392 mean train loss:  5.68684889e-03, bound:  3.15509945e-01\n",
      "Epoch: 21393 mean train loss:  5.68636321e-03, bound:  3.15509886e-01\n",
      "Epoch: 21394 mean train loss:  5.68595249e-03, bound:  3.15509856e-01\n",
      "Epoch: 21395 mean train loss:  5.68572525e-03, bound:  3.15509856e-01\n",
      "Epoch: 21396 mean train loss:  5.68533689e-03, bound:  3.15509826e-01\n",
      "Epoch: 21397 mean train loss:  5.68502536e-03, bound:  3.15509826e-01\n",
      "Epoch: 21398 mean train loss:  5.68460301e-03, bound:  3.15509766e-01\n",
      "Epoch: 21399 mean train loss:  5.68407355e-03, bound:  3.15509737e-01\n",
      "Epoch: 21400 mean train loss:  5.68356318e-03, bound:  3.15509737e-01\n",
      "Epoch: 21401 mean train loss:  5.68305887e-03, bound:  3.15509707e-01\n",
      "Epoch: 21402 mean train loss:  5.68254571e-03, bound:  3.15509707e-01\n",
      "Epoch: 21403 mean train loss:  5.68213640e-03, bound:  3.15509647e-01\n",
      "Epoch: 21404 mean train loss:  5.68174152e-03, bound:  3.15509617e-01\n",
      "Epoch: 21405 mean train loss:  5.68141276e-03, bound:  3.15509617e-01\n",
      "Epoch: 21406 mean train loss:  5.68097970e-03, bound:  3.15509588e-01\n",
      "Epoch: 21407 mean train loss:  5.68069192e-03, bound:  3.15509588e-01\n",
      "Epoch: 21408 mean train loss:  5.68027608e-03, bound:  3.15509588e-01\n",
      "Epoch: 21409 mean train loss:  5.67983137e-03, bound:  3.15509498e-01\n",
      "Epoch: 21410 mean train loss:  5.67937363e-03, bound:  3.15509498e-01\n",
      "Epoch: 21411 mean train loss:  5.67883672e-03, bound:  3.15509468e-01\n",
      "Epoch: 21412 mean train loss:  5.67833614e-03, bound:  3.15509439e-01\n",
      "Epoch: 21413 mean train loss:  5.67790912e-03, bound:  3.15509409e-01\n",
      "Epoch: 21414 mean train loss:  5.67743648e-03, bound:  3.15509409e-01\n",
      "Epoch: 21415 mean train loss:  5.67709003e-03, bound:  3.15509409e-01\n",
      "Epoch: 21416 mean train loss:  5.67666534e-03, bound:  3.15509349e-01\n",
      "Epoch: 21417 mean train loss:  5.67628955e-03, bound:  3.15509319e-01\n",
      "Epoch: 21418 mean train loss:  5.67582855e-03, bound:  3.15509290e-01\n",
      "Epoch: 21419 mean train loss:  5.67546161e-03, bound:  3.15509290e-01\n",
      "Epoch: 21420 mean train loss:  5.67500247e-03, bound:  3.15509260e-01\n",
      "Epoch: 21421 mean train loss:  5.67469792e-03, bound:  3.15509230e-01\n",
      "Epoch: 21422 mean train loss:  5.67417592e-03, bound:  3.15509200e-01\n",
      "Epoch: 21423 mean train loss:  5.67367533e-03, bound:  3.15509170e-01\n",
      "Epoch: 21424 mean train loss:  5.67324506e-03, bound:  3.15509170e-01\n",
      "Epoch: 21425 mean train loss:  5.67280408e-03, bound:  3.15509140e-01\n",
      "Epoch: 21426 mean train loss:  5.67235006e-03, bound:  3.15509081e-01\n",
      "Epoch: 21427 mean train loss:  5.67194773e-03, bound:  3.15509081e-01\n",
      "Epoch: 21428 mean train loss:  5.67156356e-03, bound:  3.15509051e-01\n",
      "Epoch: 21429 mean train loss:  5.67111466e-03, bound:  3.15509051e-01\n",
      "Epoch: 21430 mean train loss:  5.67072909e-03, bound:  3.15509021e-01\n",
      "Epoch: 21431 mean train loss:  5.67037240e-03, bound:  3.15508991e-01\n",
      "Epoch: 21432 mean train loss:  5.66991139e-03, bound:  3.15508991e-01\n",
      "Epoch: 21433 mean train loss:  5.66948811e-03, bound:  3.15508932e-01\n",
      "Epoch: 21434 mean train loss:  5.66908810e-03, bound:  3.15508932e-01\n",
      "Epoch: 21435 mean train loss:  5.66861918e-03, bound:  3.15508902e-01\n",
      "Epoch: 21436 mean train loss:  5.66824432e-03, bound:  3.15508872e-01\n",
      "Epoch: 21437 mean train loss:  5.66774793e-03, bound:  3.15508842e-01\n",
      "Epoch: 21438 mean train loss:  5.66727901e-03, bound:  3.15508842e-01\n",
      "Epoch: 21439 mean train loss:  5.66684688e-03, bound:  3.15508783e-01\n",
      "Epoch: 21440 mean train loss:  5.66647528e-03, bound:  3.15508783e-01\n",
      "Epoch: 21441 mean train loss:  5.66601427e-03, bound:  3.15508753e-01\n",
      "Epoch: 21442 mean train loss:  5.66564174e-03, bound:  3.15508723e-01\n",
      "Epoch: 21443 mean train loss:  5.66512113e-03, bound:  3.15508723e-01\n",
      "Epoch: 21444 mean train loss:  5.66473324e-03, bound:  3.15508693e-01\n",
      "Epoch: 21445 mean train loss:  5.66432718e-03, bound:  3.15508693e-01\n",
      "Epoch: 21446 mean train loss:  5.66382846e-03, bound:  3.15508634e-01\n",
      "Epoch: 21447 mean train loss:  5.66344149e-03, bound:  3.15508604e-01\n",
      "Epoch: 21448 mean train loss:  5.66301588e-03, bound:  3.15508604e-01\n",
      "Epoch: 21449 mean train loss:  5.66254323e-03, bound:  3.15508574e-01\n",
      "Epoch: 21450 mean train loss:  5.66218700e-03, bound:  3.15508544e-01\n",
      "Epoch: 21451 mean train loss:  5.66174416e-03, bound:  3.15508515e-01\n",
      "Epoch: 21452 mean train loss:  5.66130504e-03, bound:  3.15508485e-01\n",
      "Epoch: 21453 mean train loss:  5.66091435e-03, bound:  3.15508485e-01\n",
      "Epoch: 21454 mean train loss:  5.66044543e-03, bound:  3.15508455e-01\n",
      "Epoch: 21455 mean train loss:  5.66001143e-03, bound:  3.15508395e-01\n",
      "Epoch: 21456 mean train loss:  5.65962493e-03, bound:  3.15508366e-01\n",
      "Epoch: 21457 mean train loss:  5.65922773e-03, bound:  3.15508366e-01\n",
      "Epoch: 21458 mean train loss:  5.65872109e-03, bound:  3.15508366e-01\n",
      "Epoch: 21459 mean train loss:  5.65831549e-03, bound:  3.15508336e-01\n",
      "Epoch: 21460 mean train loss:  5.65787638e-03, bound:  3.15508276e-01\n",
      "Epoch: 21461 mean train loss:  5.65741537e-03, bound:  3.15508276e-01\n",
      "Epoch: 21462 mean train loss:  5.65705774e-03, bound:  3.15508276e-01\n",
      "Epoch: 21463 mean train loss:  5.65665402e-03, bound:  3.15508187e-01\n",
      "Epoch: 21464 mean train loss:  5.65615296e-03, bound:  3.15508187e-01\n",
      "Epoch: 21465 mean train loss:  5.65567985e-03, bound:  3.15508187e-01\n",
      "Epoch: 21466 mean train loss:  5.65530825e-03, bound:  3.15508157e-01\n",
      "Epoch: 21467 mean train loss:  5.65488357e-03, bound:  3.15508157e-01\n",
      "Epoch: 21468 mean train loss:  5.65443235e-03, bound:  3.15508068e-01\n",
      "Epoch: 21469 mean train loss:  5.65401232e-03, bound:  3.15508068e-01\n",
      "Epoch: 21470 mean train loss:  5.65359648e-03, bound:  3.15508068e-01\n",
      "Epoch: 21471 mean train loss:  5.65322395e-03, bound:  3.15508038e-01\n",
      "Epoch: 21472 mean train loss:  5.65278018e-03, bound:  3.15508038e-01\n",
      "Epoch: 21473 mean train loss:  5.65230986e-03, bound:  3.15507948e-01\n",
      "Epoch: 21474 mean train loss:  5.65192662e-03, bound:  3.15507948e-01\n",
      "Epoch: 21475 mean train loss:  5.65154757e-03, bound:  3.15507948e-01\n",
      "Epoch: 21476 mean train loss:  5.65110333e-03, bound:  3.15507919e-01\n",
      "Epoch: 21477 mean train loss:  5.65081416e-03, bound:  3.15507889e-01\n",
      "Epoch: 21478 mean train loss:  5.65050822e-03, bound:  3.15507889e-01\n",
      "Epoch: 21479 mean train loss:  5.65013010e-03, bound:  3.15507829e-01\n",
      "Epoch: 21480 mean train loss:  5.64988563e-03, bound:  3.15507829e-01\n",
      "Epoch: 21481 mean train loss:  5.64976363e-03, bound:  3.15507799e-01\n",
      "Epoch: 21482 mean train loss:  5.64970216e-03, bound:  3.15507799e-01\n",
      "Epoch: 21483 mean train loss:  5.64992102e-03, bound:  3.15507740e-01\n",
      "Epoch: 21484 mean train loss:  5.65038342e-03, bound:  3.15507740e-01\n",
      "Epoch: 21485 mean train loss:  5.65119041e-03, bound:  3.15507680e-01\n",
      "Epoch: 21486 mean train loss:  5.65264979e-03, bound:  3.15507710e-01\n",
      "Epoch: 21487 mean train loss:  5.65462746e-03, bound:  3.15507621e-01\n",
      "Epoch: 21488 mean train loss:  5.65746706e-03, bound:  3.15507680e-01\n",
      "Epoch: 21489 mean train loss:  5.66091947e-03, bound:  3.15507591e-01\n",
      "Epoch: 21490 mean train loss:  5.66465082e-03, bound:  3.15507621e-01\n",
      "Epoch: 21491 mean train loss:  5.66760078e-03, bound:  3.15507501e-01\n",
      "Epoch: 21492 mean train loss:  5.66820335e-03, bound:  3.15507591e-01\n",
      "Epoch: 21493 mean train loss:  5.66512300e-03, bound:  3.15507472e-01\n",
      "Epoch: 21494 mean train loss:  5.65854972e-03, bound:  3.15507561e-01\n",
      "Epoch: 21495 mean train loss:  5.65059436e-03, bound:  3.15507472e-01\n",
      "Epoch: 21496 mean train loss:  5.64448396e-03, bound:  3.15507472e-01\n",
      "Epoch: 21497 mean train loss:  5.64210908e-03, bound:  3.15507472e-01\n",
      "Epoch: 21498 mean train loss:  5.64368116e-03, bound:  3.15507382e-01\n",
      "Epoch: 21499 mean train loss:  5.64692868e-03, bound:  3.15507382e-01\n",
      "Epoch: 21500 mean train loss:  5.64945536e-03, bound:  3.15507352e-01\n",
      "Epoch: 21501 mean train loss:  5.64934732e-03, bound:  3.15507352e-01\n",
      "Epoch: 21502 mean train loss:  5.64664975e-03, bound:  3.15507293e-01\n",
      "Epoch: 21503 mean train loss:  5.64274518e-03, bound:  3.15507352e-01\n",
      "Epoch: 21504 mean train loss:  5.63977892e-03, bound:  3.15507263e-01\n",
      "Epoch: 21505 mean train loss:  5.63897938e-03, bound:  3.15507263e-01\n",
      "Epoch: 21506 mean train loss:  5.64002758e-03, bound:  3.15507263e-01\n",
      "Epoch: 21507 mean train loss:  5.64148976e-03, bound:  3.15507174e-01\n",
      "Epoch: 21508 mean train loss:  5.64195216e-03, bound:  3.15507233e-01\n",
      "Epoch: 21509 mean train loss:  5.64094260e-03, bound:  3.15507144e-01\n",
      "Epoch: 21510 mean train loss:  5.63888671e-03, bound:  3.15507174e-01\n",
      "Epoch: 21511 mean train loss:  5.63699845e-03, bound:  3.15507114e-01\n",
      "Epoch: 21512 mean train loss:  5.63603034e-03, bound:  3.15507114e-01\n",
      "Epoch: 21513 mean train loss:  5.63609973e-03, bound:  3.15507114e-01\n",
      "Epoch: 21514 mean train loss:  5.63650532e-03, bound:  3.15507025e-01\n",
      "Epoch: 21515 mean train loss:  5.63663431e-03, bound:  3.15507054e-01\n",
      "Epoch: 21516 mean train loss:  5.63609600e-03, bound:  3.15506965e-01\n",
      "Epoch: 21517 mean train loss:  5.63496025e-03, bound:  3.15506965e-01\n",
      "Epoch: 21518 mean train loss:  5.63387852e-03, bound:  3.15506935e-01\n",
      "Epoch: 21519 mean train loss:  5.63320005e-03, bound:  3.15506935e-01\n",
      "Epoch: 21520 mean train loss:  5.63290389e-03, bound:  3.15506905e-01\n",
      "Epoch: 21521 mean train loss:  5.63289737e-03, bound:  3.15506846e-01\n",
      "Epoch: 21522 mean train loss:  5.63284662e-03, bound:  3.15506846e-01\n",
      "Epoch: 21523 mean train loss:  5.63239306e-03, bound:  3.15506816e-01\n",
      "Epoch: 21524 mean train loss:  5.63182263e-03, bound:  3.15506816e-01\n",
      "Epoch: 21525 mean train loss:  5.63106779e-03, bound:  3.15506786e-01\n",
      "Epoch: 21526 mean train loss:  5.63031808e-03, bound:  3.15506786e-01\n",
      "Epoch: 21527 mean train loss:  5.62989991e-03, bound:  3.15506786e-01\n",
      "Epoch: 21528 mean train loss:  5.62957115e-03, bound:  3.15506697e-01\n",
      "Epoch: 21529 mean train loss:  5.62937185e-03, bound:  3.15506697e-01\n",
      "Epoch: 21530 mean train loss:  5.62903332e-03, bound:  3.15506667e-01\n",
      "Epoch: 21531 mean train loss:  5.62856719e-03, bound:  3.15506667e-01\n",
      "Epoch: 21532 mean train loss:  5.62806241e-03, bound:  3.15506667e-01\n",
      "Epoch: 21533 mean train loss:  5.62751992e-03, bound:  3.15506607e-01\n",
      "Epoch: 21534 mean train loss:  5.62712410e-03, bound:  3.15506607e-01\n",
      "Epoch: 21535 mean train loss:  5.62668685e-03, bound:  3.15506548e-01\n",
      "Epoch: 21536 mean train loss:  5.62642701e-03, bound:  3.15506548e-01\n",
      "Epoch: 21537 mean train loss:  5.62601397e-03, bound:  3.15506488e-01\n",
      "Epoch: 21538 mean train loss:  5.62562654e-03, bound:  3.15506488e-01\n",
      "Epoch: 21539 mean train loss:  5.62516134e-03, bound:  3.15506458e-01\n",
      "Epoch: 21540 mean train loss:  5.62470173e-03, bound:  3.15506458e-01\n",
      "Epoch: 21541 mean train loss:  5.62424399e-03, bound:  3.15506399e-01\n",
      "Epoch: 21542 mean train loss:  5.62373828e-03, bound:  3.15506399e-01\n",
      "Epoch: 21543 mean train loss:  5.62343700e-03, bound:  3.15506369e-01\n",
      "Epoch: 21544 mean train loss:  5.62299043e-03, bound:  3.15506369e-01\n",
      "Epoch: 21545 mean train loss:  5.62257692e-03, bound:  3.15506339e-01\n",
      "Epoch: 21546 mean train loss:  5.62220486e-03, bound:  3.15506279e-01\n",
      "Epoch: 21547 mean train loss:  5.62177924e-03, bound:  3.15506279e-01\n",
      "Epoch: 21548 mean train loss:  5.62134571e-03, bound:  3.15506250e-01\n",
      "Epoch: 21549 mean train loss:  5.62096713e-03, bound:  3.15506250e-01\n",
      "Epoch: 21550 mean train loss:  5.62061230e-03, bound:  3.15506220e-01\n",
      "Epoch: 21551 mean train loss:  5.62014105e-03, bound:  3.15506160e-01\n",
      "Epoch: 21552 mean train loss:  5.61972940e-03, bound:  3.15506160e-01\n",
      "Epoch: 21553 mean train loss:  5.61928423e-03, bound:  3.15506130e-01\n",
      "Epoch: 21554 mean train loss:  5.61888982e-03, bound:  3.15506130e-01\n",
      "Epoch: 21555 mean train loss:  5.61848283e-03, bound:  3.15506101e-01\n",
      "Epoch: 21556 mean train loss:  5.61809959e-03, bound:  3.15506041e-01\n",
      "Epoch: 21557 mean train loss:  5.61767817e-03, bound:  3.15506041e-01\n",
      "Epoch: 21558 mean train loss:  5.61722647e-03, bound:  3.15506041e-01\n",
      "Epoch: 21559 mean train loss:  5.61686978e-03, bound:  3.15506011e-01\n",
      "Epoch: 21560 mean train loss:  5.61645953e-03, bound:  3.15505981e-01\n",
      "Epoch: 21561 mean train loss:  5.61600272e-03, bound:  3.15505922e-01\n",
      "Epoch: 21562 mean train loss:  5.61561994e-03, bound:  3.15505922e-01\n",
      "Epoch: 21563 mean train loss:  5.61524229e-03, bound:  3.15505922e-01\n",
      "Epoch: 21564 mean train loss:  5.61484369e-03, bound:  3.15505922e-01\n",
      "Epoch: 21565 mean train loss:  5.61440364e-03, bound:  3.15505862e-01\n",
      "Epoch: 21566 mean train loss:  5.61404228e-03, bound:  3.15505832e-01\n",
      "Epoch: 21567 mean train loss:  5.61355613e-03, bound:  3.15505832e-01\n",
      "Epoch: 21568 mean train loss:  5.61320456e-03, bound:  3.15505803e-01\n",
      "Epoch: 21569 mean train loss:  5.61272539e-03, bound:  3.15505803e-01\n",
      "Epoch: 21570 mean train loss:  5.61235053e-03, bound:  3.15505743e-01\n",
      "Epoch: 21571 mean train loss:  5.61192958e-03, bound:  3.15505713e-01\n",
      "Epoch: 21572 mean train loss:  5.61152538e-03, bound:  3.15505713e-01\n",
      "Epoch: 21573 mean train loss:  5.61106391e-03, bound:  3.15505683e-01\n",
      "Epoch: 21574 mean train loss:  5.61063411e-03, bound:  3.15505654e-01\n",
      "Epoch: 21575 mean train loss:  5.61031234e-03, bound:  3.15505654e-01\n",
      "Epoch: 21576 mean train loss:  5.60991140e-03, bound:  3.15505594e-01\n",
      "Epoch: 21577 mean train loss:  5.60942106e-03, bound:  3.15505594e-01\n",
      "Epoch: 21578 mean train loss:  5.60905226e-03, bound:  3.15505564e-01\n",
      "Epoch: 21579 mean train loss:  5.60868066e-03, bound:  3.15505534e-01\n",
      "Epoch: 21580 mean train loss:  5.60819171e-03, bound:  3.15505534e-01\n",
      "Epoch: 21581 mean train loss:  5.60785830e-03, bound:  3.15505475e-01\n",
      "Epoch: 21582 mean train loss:  5.60744619e-03, bound:  3.15505475e-01\n",
      "Epoch: 21583 mean train loss:  5.60699543e-03, bound:  3.15505445e-01\n",
      "Epoch: 21584 mean train loss:  5.60666295e-03, bound:  3.15505445e-01\n",
      "Epoch: 21585 mean train loss:  5.60620427e-03, bound:  3.15505385e-01\n",
      "Epoch: 21586 mean train loss:  5.60580986e-03, bound:  3.15505356e-01\n",
      "Epoch: 21587 mean train loss:  5.60539262e-03, bound:  3.15505356e-01\n",
      "Epoch: 21588 mean train loss:  5.60504058e-03, bound:  3.15505356e-01\n",
      "Epoch: 21589 mean train loss:  5.60457585e-03, bound:  3.15505266e-01\n",
      "Epoch: 21590 mean train loss:  5.60419075e-03, bound:  3.15505266e-01\n",
      "Epoch: 21591 mean train loss:  5.60372788e-03, bound:  3.15505236e-01\n",
      "Epoch: 21592 mean train loss:  5.60329808e-03, bound:  3.15505236e-01\n",
      "Epoch: 21593 mean train loss:  5.60296699e-03, bound:  3.15505236e-01\n",
      "Epoch: 21594 mean train loss:  5.60247526e-03, bound:  3.15505147e-01\n",
      "Epoch: 21595 mean train loss:  5.60205802e-03, bound:  3.15505147e-01\n",
      "Epoch: 21596 mean train loss:  5.60163567e-03, bound:  3.15505117e-01\n",
      "Epoch: 21597 mean train loss:  5.60119515e-03, bound:  3.15505117e-01\n",
      "Epoch: 21598 mean train loss:  5.60078770e-03, bound:  3.15505117e-01\n",
      "Epoch: 21599 mean train loss:  5.60041144e-03, bound:  3.15505058e-01\n",
      "Epoch: 21600 mean train loss:  5.59994252e-03, bound:  3.15505058e-01\n",
      "Epoch: 21601 mean train loss:  5.59956068e-03, bound:  3.15505028e-01\n",
      "Epoch: 21602 mean train loss:  5.59914578e-03, bound:  3.15504998e-01\n",
      "Epoch: 21603 mean train loss:  5.59874112e-03, bound:  3.15504998e-01\n",
      "Epoch: 21604 mean train loss:  5.59834205e-03, bound:  3.15504938e-01\n",
      "Epoch: 21605 mean train loss:  5.59787545e-03, bound:  3.15504909e-01\n",
      "Epoch: 21606 mean train loss:  5.59752714e-03, bound:  3.15504909e-01\n",
      "Epoch: 21607 mean train loss:  5.59708895e-03, bound:  3.15504879e-01\n",
      "Epoch: 21608 mean train loss:  5.59666427e-03, bound:  3.15504879e-01\n",
      "Epoch: 21609 mean train loss:  5.59624191e-03, bound:  3.15504819e-01\n",
      "Epoch: 21610 mean train loss:  5.59581956e-03, bound:  3.15504819e-01\n",
      "Epoch: 21611 mean train loss:  5.59539534e-03, bound:  3.15504789e-01\n",
      "Epoch: 21612 mean train loss:  5.59503678e-03, bound:  3.15504760e-01\n",
      "Epoch: 21613 mean train loss:  5.59462141e-03, bound:  3.15504760e-01\n",
      "Epoch: 21614 mean train loss:  5.59421396e-03, bound:  3.15504700e-01\n",
      "Epoch: 21615 mean train loss:  5.59382746e-03, bound:  3.15504670e-01\n",
      "Epoch: 21616 mean train loss:  5.59343258e-03, bound:  3.15504670e-01\n",
      "Epoch: 21617 mean train loss:  5.59297297e-03, bound:  3.15504640e-01\n",
      "Epoch: 21618 mean train loss:  5.59257809e-03, bound:  3.15504640e-01\n",
      "Epoch: 21619 mean train loss:  5.59213571e-03, bound:  3.15504581e-01\n",
      "Epoch: 21620 mean train loss:  5.59173105e-03, bound:  3.15504551e-01\n",
      "Epoch: 21621 mean train loss:  5.59132593e-03, bound:  3.15504551e-01\n",
      "Epoch: 21622 mean train loss:  5.59091149e-03, bound:  3.15504491e-01\n",
      "Epoch: 21623 mean train loss:  5.59049565e-03, bound:  3.15504491e-01\n",
      "Epoch: 21624 mean train loss:  5.59009612e-03, bound:  3.15504491e-01\n",
      "Epoch: 21625 mean train loss:  5.58966910e-03, bound:  3.15504432e-01\n",
      "Epoch: 21626 mean train loss:  5.58922719e-03, bound:  3.15504432e-01\n",
      "Epoch: 21627 mean train loss:  5.58882998e-03, bound:  3.15504432e-01\n",
      "Epoch: 21628 mean train loss:  5.58845047e-03, bound:  3.15504372e-01\n",
      "Epoch: 21629 mean train loss:  5.58798807e-03, bound:  3.15504372e-01\n",
      "Epoch: 21630 mean train loss:  5.58760297e-03, bound:  3.15504313e-01\n",
      "Epoch: 21631 mean train loss:  5.58720157e-03, bound:  3.15504313e-01\n",
      "Epoch: 21632 mean train loss:  5.58674894e-03, bound:  3.15504313e-01\n",
      "Epoch: 21633 mean train loss:  5.58635779e-03, bound:  3.15504253e-01\n",
      "Epoch: 21634 mean train loss:  5.58599224e-03, bound:  3.15504223e-01\n",
      "Epoch: 21635 mean train loss:  5.58565930e-03, bound:  3.15504223e-01\n",
      "Epoch: 21636 mean train loss:  5.58532495e-03, bound:  3.15504164e-01\n",
      "Epoch: 21637 mean train loss:  5.58497570e-03, bound:  3.15504164e-01\n",
      "Epoch: 21638 mean train loss:  5.58466651e-03, bound:  3.15504134e-01\n",
      "Epoch: 21639 mean train loss:  5.58442017e-03, bound:  3.15504134e-01\n",
      "Epoch: 21640 mean train loss:  5.58432797e-03, bound:  3.15504104e-01\n",
      "Epoch: 21641 mean train loss:  5.58417011e-03, bound:  3.15504104e-01\n",
      "Epoch: 21642 mean train loss:  5.58430655e-03, bound:  3.15504014e-01\n",
      "Epoch: 21643 mean train loss:  5.58460644e-03, bound:  3.15504044e-01\n",
      "Epoch: 21644 mean train loss:  5.58531983e-03, bound:  3.15503985e-01\n",
      "Epoch: 21645 mean train loss:  5.58636524e-03, bound:  3.15504014e-01\n",
      "Epoch: 21646 mean train loss:  5.58788469e-03, bound:  3.15503925e-01\n",
      "Epoch: 21647 mean train loss:  5.59021113e-03, bound:  3.15503985e-01\n",
      "Epoch: 21648 mean train loss:  5.59303537e-03, bound:  3.15503865e-01\n",
      "Epoch: 21649 mean train loss:  5.59619488e-03, bound:  3.15503895e-01\n",
      "Epoch: 21650 mean train loss:  5.59891528e-03, bound:  3.15503806e-01\n",
      "Epoch: 21651 mean train loss:  5.60012180e-03, bound:  3.15503895e-01\n",
      "Epoch: 21652 mean train loss:  5.59869176e-03, bound:  3.15503806e-01\n",
      "Epoch: 21653 mean train loss:  5.59421675e-03, bound:  3.15503865e-01\n",
      "Epoch: 21654 mean train loss:  5.58764953e-03, bound:  3.15503746e-01\n",
      "Epoch: 21655 mean train loss:  5.58130210e-03, bound:  3.15503776e-01\n",
      "Epoch: 21656 mean train loss:  5.57731558e-03, bound:  3.15503687e-01\n",
      "Epoch: 21657 mean train loss:  5.57682896e-03, bound:  3.15503687e-01\n",
      "Epoch: 21658 mean train loss:  5.57874842e-03, bound:  3.15503687e-01\n",
      "Epoch: 21659 mean train loss:  5.58127090e-03, bound:  3.15503627e-01\n",
      "Epoch: 21660 mean train loss:  5.58266183e-03, bound:  3.15503687e-01\n",
      "Epoch: 21661 mean train loss:  5.58204250e-03, bound:  3.15503597e-01\n",
      "Epoch: 21662 mean train loss:  5.57956286e-03, bound:  3.15503627e-01\n",
      "Epoch: 21663 mean train loss:  5.57644293e-03, bound:  3.15503567e-01\n",
      "Epoch: 21664 mean train loss:  5.57416910e-03, bound:  3.15503567e-01\n",
      "Epoch: 21665 mean train loss:  5.57331089e-03, bound:  3.15503567e-01\n",
      "Epoch: 21666 mean train loss:  5.57388272e-03, bound:  3.15503538e-01\n",
      "Epoch: 21667 mean train loss:  5.57490624e-03, bound:  3.15503538e-01\n",
      "Epoch: 21668 mean train loss:  5.57547435e-03, bound:  3.15503448e-01\n",
      "Epoch: 21669 mean train loss:  5.57494210e-03, bound:  3.15503448e-01\n",
      "Epoch: 21670 mean train loss:  5.57351531e-03, bound:  3.15503418e-01\n",
      "Epoch: 21671 mean train loss:  5.57180494e-03, bound:  3.15503418e-01\n",
      "Epoch: 21672 mean train loss:  5.57062076e-03, bound:  3.15503359e-01\n",
      "Epoch: 21673 mean train loss:  5.57013648e-03, bound:  3.15503359e-01\n",
      "Epoch: 21674 mean train loss:  5.57027850e-03, bound:  3.15503329e-01\n",
      "Epoch: 21675 mean train loss:  5.57060866e-03, bound:  3.15503299e-01\n",
      "Epoch: 21676 mean train loss:  5.57053275e-03, bound:  3.15503299e-01\n",
      "Epoch: 21677 mean train loss:  5.56995673e-03, bound:  3.15503240e-01\n",
      "Epoch: 21678 mean train loss:  5.56897279e-03, bound:  3.15503269e-01\n",
      "Epoch: 21679 mean train loss:  5.56809455e-03, bound:  3.15503210e-01\n",
      "Epoch: 21680 mean train loss:  5.56733180e-03, bound:  3.15503210e-01\n",
      "Epoch: 21681 mean train loss:  5.56699838e-03, bound:  3.15503180e-01\n",
      "Epoch: 21682 mean train loss:  5.56684472e-03, bound:  3.15503120e-01\n",
      "Epoch: 21683 mean train loss:  5.56661375e-03, bound:  3.15503120e-01\n",
      "Epoch: 21684 mean train loss:  5.56633575e-03, bound:  3.15503120e-01\n",
      "Epoch: 21685 mean train loss:  5.56583889e-03, bound:  3.15503120e-01\n",
      "Epoch: 21686 mean train loss:  5.56525262e-03, bound:  3.15503061e-01\n",
      "Epoch: 21687 mean train loss:  5.56459231e-03, bound:  3.15503031e-01\n",
      "Epoch: 21688 mean train loss:  5.56417461e-03, bound:  3.15503001e-01\n",
      "Epoch: 21689 mean train loss:  5.56376902e-03, bound:  3.15503001e-01\n",
      "Epoch: 21690 mean train loss:  5.56344073e-03, bound:  3.15503001e-01\n",
      "Epoch: 21691 mean train loss:  5.56312641e-03, bound:  3.15502942e-01\n",
      "Epoch: 21692 mean train loss:  5.56276785e-03, bound:  3.15502912e-01\n",
      "Epoch: 21693 mean train loss:  5.56227751e-03, bound:  3.15502882e-01\n",
      "Epoch: 21694 mean train loss:  5.56183700e-03, bound:  3.15502882e-01\n",
      "Epoch: 21695 mean train loss:  5.56145096e-03, bound:  3.15502852e-01\n",
      "Epoch: 21696 mean train loss:  5.56099182e-03, bound:  3.15502822e-01\n",
      "Epoch: 21697 mean train loss:  5.56057319e-03, bound:  3.15502793e-01\n",
      "Epoch: 21698 mean train loss:  5.56025840e-03, bound:  3.15502793e-01\n",
      "Epoch: 21699 mean train loss:  5.55984303e-03, bound:  3.15502763e-01\n",
      "Epoch: 21700 mean train loss:  5.55942534e-03, bound:  3.15502763e-01\n",
      "Epoch: 21701 mean train loss:  5.55902719e-03, bound:  3.15502703e-01\n",
      "Epoch: 21702 mean train loss:  5.55859786e-03, bound:  3.15502673e-01\n",
      "Epoch: 21703 mean train loss:  5.55820018e-03, bound:  3.15502673e-01\n",
      "Epoch: 21704 mean train loss:  5.55777317e-03, bound:  3.15502673e-01\n",
      "Epoch: 21705 mean train loss:  5.55737130e-03, bound:  3.15502644e-01\n",
      "Epoch: 21706 mean train loss:  5.55699272e-03, bound:  3.15502584e-01\n",
      "Epoch: 21707 mean train loss:  5.55658201e-03, bound:  3.15502584e-01\n",
      "Epoch: 21708 mean train loss:  5.55625418e-03, bound:  3.15502554e-01\n",
      "Epoch: 21709 mean train loss:  5.55586256e-03, bound:  3.15502524e-01\n",
      "Epoch: 21710 mean train loss:  5.55543648e-03, bound:  3.15502524e-01\n",
      "Epoch: 21711 mean train loss:  5.55500062e-03, bound:  3.15502465e-01\n",
      "Epoch: 21712 mean train loss:  5.55458572e-03, bound:  3.15502465e-01\n",
      "Epoch: 21713 mean train loss:  5.55420527e-03, bound:  3.15502435e-01\n",
      "Epoch: 21714 mean train loss:  5.55378385e-03, bound:  3.15502435e-01\n",
      "Epoch: 21715 mean train loss:  5.55344624e-03, bound:  3.15502405e-01\n",
      "Epoch: 21716 mean train loss:  5.55301178e-03, bound:  3.15502346e-01\n",
      "Epoch: 21717 mean train loss:  5.55259874e-03, bound:  3.15502346e-01\n",
      "Epoch: 21718 mean train loss:  5.55220386e-03, bound:  3.15502316e-01\n",
      "Epoch: 21719 mean train loss:  5.55177964e-03, bound:  3.15502316e-01\n",
      "Epoch: 21720 mean train loss:  5.55137824e-03, bound:  3.15502286e-01\n",
      "Epoch: 21721 mean train loss:  5.55104017e-03, bound:  3.15502286e-01\n",
      "Epoch: 21722 mean train loss:  5.55063225e-03, bound:  3.15502256e-01\n",
      "Epoch: 21723 mean train loss:  5.55018848e-03, bound:  3.15502197e-01\n",
      "Epoch: 21724 mean train loss:  5.54978102e-03, bound:  3.15502197e-01\n",
      "Epoch: 21725 mean train loss:  5.54942805e-03, bound:  3.15502197e-01\n",
      "Epoch: 21726 mean train loss:  5.54899173e-03, bound:  3.15502167e-01\n",
      "Epoch: 21727 mean train loss:  5.54861454e-03, bound:  3.15502107e-01\n",
      "Epoch: 21728 mean train loss:  5.54822013e-03, bound:  3.15502107e-01\n",
      "Epoch: 21729 mean train loss:  5.54780196e-03, bound:  3.15502077e-01\n",
      "Epoch: 21730 mean train loss:  5.54744527e-03, bound:  3.15502077e-01\n",
      "Epoch: 21731 mean train loss:  5.54705178e-03, bound:  3.15502018e-01\n",
      "Epoch: 21732 mean train loss:  5.54665830e-03, bound:  3.15502018e-01\n",
      "Epoch: 21733 mean train loss:  5.54624014e-03, bound:  3.15501988e-01\n",
      "Epoch: 21734 mean train loss:  5.54578099e-03, bound:  3.15501988e-01\n",
      "Epoch: 21735 mean train loss:  5.54542476e-03, bound:  3.15501958e-01\n",
      "Epoch: 21736 mean train loss:  5.54501824e-03, bound:  3.15501899e-01\n",
      "Epoch: 21737 mean train loss:  5.54462802e-03, bound:  3.15501899e-01\n",
      "Epoch: 21738 mean train loss:  5.54426573e-03, bound:  3.15501869e-01\n",
      "Epoch: 21739 mean train loss:  5.54381078e-03, bound:  3.15501869e-01\n",
      "Epoch: 21740 mean train loss:  5.54345362e-03, bound:  3.15501839e-01\n",
      "Epoch: 21741 mean train loss:  5.54302521e-03, bound:  3.15501779e-01\n",
      "Epoch: 21742 mean train loss:  5.54261450e-03, bound:  3.15501779e-01\n",
      "Epoch: 21743 mean train loss:  5.54220984e-03, bound:  3.15501750e-01\n",
      "Epoch: 21744 mean train loss:  5.54181123e-03, bound:  3.15501750e-01\n",
      "Epoch: 21745 mean train loss:  5.54142799e-03, bound:  3.15501690e-01\n",
      "Epoch: 21746 mean train loss:  5.54102287e-03, bound:  3.15501690e-01\n",
      "Epoch: 21747 mean train loss:  5.54058421e-03, bound:  3.15501690e-01\n",
      "Epoch: 21748 mean train loss:  5.54018607e-03, bound:  3.15501630e-01\n",
      "Epoch: 21749 mean train loss:  5.53983683e-03, bound:  3.15501630e-01\n",
      "Epoch: 21750 mean train loss:  5.53936977e-03, bound:  3.15501571e-01\n",
      "Epoch: 21751 mean train loss:  5.53902192e-03, bound:  3.15501571e-01\n",
      "Epoch: 21752 mean train loss:  5.53859212e-03, bound:  3.15501571e-01\n",
      "Epoch: 21753 mean train loss:  5.53819025e-03, bound:  3.15501541e-01\n",
      "Epoch: 21754 mean train loss:  5.53774228e-03, bound:  3.15501511e-01\n",
      "Epoch: 21755 mean train loss:  5.53741073e-03, bound:  3.15501451e-01\n",
      "Epoch: 21756 mean train loss:  5.53700328e-03, bound:  3.15501451e-01\n",
      "Epoch: 21757 mean train loss:  5.53660467e-03, bound:  3.15501422e-01\n",
      "Epoch: 21758 mean train loss:  5.53617952e-03, bound:  3.15501422e-01\n",
      "Epoch: 21759 mean train loss:  5.53582842e-03, bound:  3.15501392e-01\n",
      "Epoch: 21760 mean train loss:  5.53539256e-03, bound:  3.15501392e-01\n",
      "Epoch: 21761 mean train loss:  5.53502608e-03, bound:  3.15501332e-01\n",
      "Epoch: 21762 mean train loss:  5.53459348e-03, bound:  3.15501332e-01\n",
      "Epoch: 21763 mean train loss:  5.53413061e-03, bound:  3.15501302e-01\n",
      "Epoch: 21764 mean train loss:  5.53376367e-03, bound:  3.15501302e-01\n",
      "Epoch: 21765 mean train loss:  5.53342700e-03, bound:  3.15501243e-01\n",
      "Epoch: 21766 mean train loss:  5.53296460e-03, bound:  3.15501213e-01\n",
      "Epoch: 21767 mean train loss:  5.53257857e-03, bound:  3.15501183e-01\n",
      "Epoch: 21768 mean train loss:  5.53216785e-03, bound:  3.15501183e-01\n",
      "Epoch: 21769 mean train loss:  5.53180184e-03, bound:  3.15501183e-01\n",
      "Epoch: 21770 mean train loss:  5.53136365e-03, bound:  3.15501124e-01\n",
      "Epoch: 21771 mean train loss:  5.53107215e-03, bound:  3.15501094e-01\n",
      "Epoch: 21772 mean train loss:  5.53064840e-03, bound:  3.15501094e-01\n",
      "Epoch: 21773 mean train loss:  5.53021114e-03, bound:  3.15501064e-01\n",
      "Epoch: 21774 mean train loss:  5.52985724e-03, bound:  3.15501064e-01\n",
      "Epoch: 21775 mean train loss:  5.52954990e-03, bound:  3.15501004e-01\n",
      "Epoch: 21776 mean train loss:  5.52918296e-03, bound:  3.15501004e-01\n",
      "Epoch: 21777 mean train loss:  5.52886631e-03, bound:  3.15500975e-01\n",
      "Epoch: 21778 mean train loss:  5.52855618e-03, bound:  3.15500945e-01\n",
      "Epoch: 21779 mean train loss:  5.52832615e-03, bound:  3.15500885e-01\n",
      "Epoch: 21780 mean train loss:  5.52814687e-03, bound:  3.15500885e-01\n",
      "Epoch: 21781 mean train loss:  5.52794896e-03, bound:  3.15500855e-01\n",
      "Epoch: 21782 mean train loss:  5.52792056e-03, bound:  3.15500855e-01\n",
      "Epoch: 21783 mean train loss:  5.52801462e-03, bound:  3.15500826e-01\n",
      "Epoch: 21784 mean train loss:  5.52828982e-03, bound:  3.15500826e-01\n",
      "Epoch: 21785 mean train loss:  5.52873779e-03, bound:  3.15500766e-01\n",
      "Epoch: 21786 mean train loss:  5.52953733e-03, bound:  3.15500766e-01\n",
      "Epoch: 21787 mean train loss:  5.53070521e-03, bound:  3.15500736e-01\n",
      "Epoch: 21788 mean train loss:  5.53220045e-03, bound:  3.15500766e-01\n",
      "Epoch: 21789 mean train loss:  5.53410314e-03, bound:  3.15500647e-01\n",
      "Epoch: 21790 mean train loss:  5.53599885e-03, bound:  3.15500736e-01\n",
      "Epoch: 21791 mean train loss:  5.53759234e-03, bound:  3.15500617e-01\n",
      "Epoch: 21792 mean train loss:  5.53820189e-03, bound:  3.15500647e-01\n",
      "Epoch: 21793 mean train loss:  5.53741073e-03, bound:  3.15500557e-01\n",
      "Epoch: 21794 mean train loss:  5.53471828e-03, bound:  3.15500617e-01\n",
      "Epoch: 21795 mean train loss:  5.53046865e-03, bound:  3.15500528e-01\n",
      "Epoch: 21796 mean train loss:  5.52577851e-03, bound:  3.15500557e-01\n",
      "Epoch: 21797 mean train loss:  5.52201644e-03, bound:  3.15500498e-01\n",
      "Epoch: 21798 mean train loss:  5.52022317e-03, bound:  3.15500468e-01\n",
      "Epoch: 21799 mean train loss:  5.52029349e-03, bound:  3.15500468e-01\n",
      "Epoch: 21800 mean train loss:  5.52149350e-03, bound:  3.15500438e-01\n",
      "Epoch: 21801 mean train loss:  5.52302320e-03, bound:  3.15500438e-01\n",
      "Epoch: 21802 mean train loss:  5.52367093e-03, bound:  3.15500349e-01\n",
      "Epoch: 21803 mean train loss:  5.52316895e-03, bound:  3.15500408e-01\n",
      "Epoch: 21804 mean train loss:  5.52163133e-03, bound:  3.15500349e-01\n",
      "Epoch: 21805 mean train loss:  5.51953493e-03, bound:  3.15500349e-01\n",
      "Epoch: 21806 mean train loss:  5.51774306e-03, bound:  3.15500319e-01\n",
      "Epoch: 21807 mean train loss:  5.51660731e-03, bound:  3.15500289e-01\n",
      "Epoch: 21808 mean train loss:  5.51645411e-03, bound:  3.15500289e-01\n",
      "Epoch: 21809 mean train loss:  5.51678240e-03, bound:  3.15500230e-01\n",
      "Epoch: 21810 mean train loss:  5.51711582e-03, bound:  3.15500259e-01\n",
      "Epoch: 21811 mean train loss:  5.51708788e-03, bound:  3.15500200e-01\n",
      "Epoch: 21812 mean train loss:  5.51663106e-03, bound:  3.15500200e-01\n",
      "Epoch: 21813 mean train loss:  5.51570486e-03, bound:  3.15500170e-01\n",
      "Epoch: 21814 mean train loss:  5.51466737e-03, bound:  3.15500170e-01\n",
      "Epoch: 21815 mean train loss:  5.51374862e-03, bound:  3.15500110e-01\n",
      "Epoch: 21816 mean train loss:  5.51311346e-03, bound:  3.15500081e-01\n",
      "Epoch: 21817 mean train loss:  5.51281543e-03, bound:  3.15500081e-01\n",
      "Epoch: 21818 mean train loss:  5.51263895e-03, bound:  3.15500051e-01\n",
      "Epoch: 21819 mean train loss:  5.51257934e-03, bound:  3.15500021e-01\n",
      "Epoch: 21820 mean train loss:  5.51232230e-03, bound:  3.15499991e-01\n",
      "Epoch: 21821 mean train loss:  5.51190600e-03, bound:  3.15499961e-01\n",
      "Epoch: 21822 mean train loss:  5.51143801e-03, bound:  3.15499961e-01\n",
      "Epoch: 21823 mean train loss:  5.51081216e-03, bound:  3.15499961e-01\n",
      "Epoch: 21824 mean train loss:  5.51015930e-03, bound:  3.15499902e-01\n",
      "Epoch: 21825 mean train loss:  5.50959352e-03, bound:  3.15499872e-01\n",
      "Epoch: 21826 mean train loss:  5.50919771e-03, bound:  3.15499872e-01\n",
      "Epoch: 21827 mean train loss:  5.50893974e-03, bound:  3.15499842e-01\n",
      "Epoch: 21828 mean train loss:  5.50864451e-03, bound:  3.15499842e-01\n",
      "Epoch: 21829 mean train loss:  5.50827337e-03, bound:  3.15499783e-01\n",
      "Epoch: 21830 mean train loss:  5.50794275e-03, bound:  3.15499783e-01\n",
      "Epoch: 21831 mean train loss:  5.50751667e-03, bound:  3.15499753e-01\n",
      "Epoch: 21832 mean train loss:  5.50708687e-03, bound:  3.15499723e-01\n",
      "Epoch: 21833 mean train loss:  5.50660910e-03, bound:  3.15499693e-01\n",
      "Epoch: 21834 mean train loss:  5.50619420e-03, bound:  3.15499663e-01\n",
      "Epoch: 21835 mean train loss:  5.50576160e-03, bound:  3.15499634e-01\n",
      "Epoch: 21836 mean train loss:  5.50532341e-03, bound:  3.15499634e-01\n",
      "Epoch: 21837 mean train loss:  5.50493645e-03, bound:  3.15499604e-01\n",
      "Epoch: 21838 mean train loss:  5.50455321e-03, bound:  3.15499604e-01\n",
      "Epoch: 21839 mean train loss:  5.50417742e-03, bound:  3.15499544e-01\n",
      "Epoch: 21840 mean train loss:  5.50381420e-03, bound:  3.15499544e-01\n",
      "Epoch: 21841 mean train loss:  5.50344819e-03, bound:  3.15499514e-01\n",
      "Epoch: 21842 mean train loss:  5.50300302e-03, bound:  3.15499485e-01\n",
      "Epoch: 21843 mean train loss:  5.50264865e-03, bound:  3.15499485e-01\n",
      "Epoch: 21844 mean train loss:  5.50214807e-03, bound:  3.15499455e-01\n",
      "Epoch: 21845 mean train loss:  5.50182164e-03, bound:  3.15499425e-01\n",
      "Epoch: 21846 mean train loss:  5.50143002e-03, bound:  3.15499395e-01\n",
      "Epoch: 21847 mean train loss:  5.50102955e-03, bound:  3.15499395e-01\n",
      "Epoch: 21848 mean train loss:  5.50060440e-03, bound:  3.15499365e-01\n",
      "Epoch: 21849 mean train loss:  5.50026586e-03, bound:  3.15499336e-01\n",
      "Epoch: 21850 mean train loss:  5.49988588e-03, bound:  3.15499336e-01\n",
      "Epoch: 21851 mean train loss:  5.49953431e-03, bound:  3.15499276e-01\n",
      "Epoch: 21852 mean train loss:  5.49906539e-03, bound:  3.15499276e-01\n",
      "Epoch: 21853 mean train loss:  5.49867982e-03, bound:  3.15499216e-01\n",
      "Epoch: 21854 mean train loss:  5.49823930e-03, bound:  3.15499216e-01\n",
      "Epoch: 21855 mean train loss:  5.49794454e-03, bound:  3.15499216e-01\n",
      "Epoch: 21856 mean train loss:  5.49750915e-03, bound:  3.15499187e-01\n",
      "Epoch: 21857 mean train loss:  5.49706677e-03, bound:  3.15499187e-01\n",
      "Epoch: 21858 mean train loss:  5.49668074e-03, bound:  3.15499157e-01\n",
      "Epoch: 21859 mean train loss:  5.49631193e-03, bound:  3.15499097e-01\n",
      "Epoch: 21860 mean train loss:  5.49593288e-03, bound:  3.15499097e-01\n",
      "Epoch: 21861 mean train loss:  5.49559295e-03, bound:  3.15499067e-01\n",
      "Epoch: 21862 mean train loss:  5.49517572e-03, bound:  3.15499067e-01\n",
      "Epoch: 21863 mean train loss:  5.49480272e-03, bound:  3.15499038e-01\n",
      "Epoch: 21864 mean train loss:  5.49437245e-03, bound:  3.15498978e-01\n",
      "Epoch: 21865 mean train loss:  5.49396314e-03, bound:  3.15498978e-01\n",
      "Epoch: 21866 mean train loss:  5.49359526e-03, bound:  3.15498948e-01\n",
      "Epoch: 21867 mean train loss:  5.49320085e-03, bound:  3.15498948e-01\n",
      "Epoch: 21868 mean train loss:  5.49281761e-03, bound:  3.15498918e-01\n",
      "Epoch: 21869 mean train loss:  5.49244788e-03, bound:  3.15498918e-01\n",
      "Epoch: 21870 mean train loss:  5.49203251e-03, bound:  3.15498859e-01\n",
      "Epoch: 21871 mean train loss:  5.49159246e-03, bound:  3.15498829e-01\n",
      "Epoch: 21872 mean train loss:  5.49123716e-03, bound:  3.15498829e-01\n",
      "Epoch: 21873 mean train loss:  5.49081666e-03, bound:  3.15498769e-01\n",
      "Epoch: 21874 mean train loss:  5.49045112e-03, bound:  3.15498769e-01\n",
      "Epoch: 21875 mean train loss:  5.49006462e-03, bound:  3.15498739e-01\n",
      "Epoch: 21876 mean train loss:  5.48965205e-03, bound:  3.15498710e-01\n",
      "Epoch: 21877 mean train loss:  5.48931211e-03, bound:  3.15498710e-01\n",
      "Epoch: 21878 mean train loss:  5.48891444e-03, bound:  3.15498650e-01\n",
      "Epoch: 21879 mean train loss:  5.48849767e-03, bound:  3.15498650e-01\n",
      "Epoch: 21880 mean train loss:  5.48820337e-03, bound:  3.15498620e-01\n",
      "Epoch: 21881 mean train loss:  5.48775215e-03, bound:  3.15498620e-01\n",
      "Epoch: 21882 mean train loss:  5.48738660e-03, bound:  3.15498620e-01\n",
      "Epoch: 21883 mean train loss:  5.48697822e-03, bound:  3.15498590e-01\n",
      "Epoch: 21884 mean train loss:  5.48661873e-03, bound:  3.15498531e-01\n",
      "Epoch: 21885 mean train loss:  5.48627321e-03, bound:  3.15498531e-01\n",
      "Epoch: 21886 mean train loss:  5.48590859e-03, bound:  3.15498501e-01\n",
      "Epoch: 21887 mean train loss:  5.48560638e-03, bound:  3.15498501e-01\n",
      "Epoch: 21888 mean train loss:  5.48532233e-03, bound:  3.15498412e-01\n",
      "Epoch: 21889 mean train loss:  5.48498379e-03, bound:  3.15498412e-01\n",
      "Epoch: 21890 mean train loss:  5.48479706e-03, bound:  3.15498412e-01\n",
      "Epoch: 21891 mean train loss:  5.48450183e-03, bound:  3.15498412e-01\n",
      "Epoch: 21892 mean train loss:  5.48420707e-03, bound:  3.15498382e-01\n",
      "Epoch: 21893 mean train loss:  5.48410229e-03, bound:  3.15498322e-01\n",
      "Epoch: 21894 mean train loss:  5.48402872e-03, bound:  3.15498292e-01\n",
      "Epoch: 21895 mean train loss:  5.48400078e-03, bound:  3.15498292e-01\n",
      "Epoch: 21896 mean train loss:  5.48407529e-03, bound:  3.15498263e-01\n",
      "Epoch: 21897 mean train loss:  5.48418704e-03, bound:  3.15498263e-01\n",
      "Epoch: 21898 mean train loss:  5.48451161e-03, bound:  3.15498203e-01\n",
      "Epoch: 21899 mean train loss:  5.48491534e-03, bound:  3.15498203e-01\n",
      "Epoch: 21900 mean train loss:  5.48534142e-03, bound:  3.15498143e-01\n",
      "Epoch: 21901 mean train loss:  5.48594492e-03, bound:  3.15498173e-01\n",
      "Epoch: 21902 mean train loss:  5.48653491e-03, bound:  3.15498084e-01\n",
      "Epoch: 21903 mean train loss:  5.48710162e-03, bound:  3.15498143e-01\n",
      "Epoch: 21904 mean train loss:  5.48748532e-03, bound:  3.15498024e-01\n",
      "Epoch: 21905 mean train loss:  5.48745645e-03, bound:  3.15498084e-01\n",
      "Epoch: 21906 mean train loss:  5.48672443e-03, bound:  3.15498024e-01\n",
      "Epoch: 21907 mean train loss:  5.48527436e-03, bound:  3.15498054e-01\n",
      "Epoch: 21908 mean train loss:  5.48322545e-03, bound:  3.15497965e-01\n",
      "Epoch: 21909 mean train loss:  5.48088085e-03, bound:  3.15498024e-01\n",
      "Epoch: 21910 mean train loss:  5.47865126e-03, bound:  3.15497935e-01\n",
      "Epoch: 21911 mean train loss:  5.47680398e-03, bound:  3.15497935e-01\n",
      "Epoch: 21912 mean train loss:  5.47563098e-03, bound:  3.15497935e-01\n",
      "Epoch: 21913 mean train loss:  5.47525426e-03, bound:  3.15497935e-01\n",
      "Epoch: 21914 mean train loss:  5.47517557e-03, bound:  3.15497845e-01\n",
      "Epoch: 21915 mean train loss:  5.47539722e-03, bound:  3.15497816e-01\n",
      "Epoch: 21916 mean train loss:  5.47554297e-03, bound:  3.15497845e-01\n",
      "Epoch: 21917 mean train loss:  5.47554204e-03, bound:  3.15497816e-01\n",
      "Epoch: 21918 mean train loss:  5.47532365e-03, bound:  3.15497816e-01\n",
      "Epoch: 21919 mean train loss:  5.47480956e-03, bound:  3.15497726e-01\n",
      "Epoch: 21920 mean train loss:  5.47405798e-03, bound:  3.15497726e-01\n",
      "Epoch: 21921 mean train loss:  5.47310431e-03, bound:  3.15497726e-01\n",
      "Epoch: 21922 mean train loss:  5.47226425e-03, bound:  3.15497726e-01\n",
      "Epoch: 21923 mean train loss:  5.47156064e-03, bound:  3.15497637e-01\n",
      "Epoch: 21924 mean train loss:  5.47098182e-03, bound:  3.15497637e-01\n",
      "Epoch: 21925 mean train loss:  5.47051104e-03, bound:  3.15497637e-01\n",
      "Epoch: 21926 mean train loss:  5.47027262e-03, bound:  3.15497607e-01\n",
      "Epoch: 21927 mean train loss:  5.47003001e-03, bound:  3.15497577e-01\n",
      "Epoch: 21928 mean train loss:  5.46982465e-03, bound:  3.15497547e-01\n",
      "Epoch: 21929 mean train loss:  5.46949310e-03, bound:  3.15497518e-01\n",
      "Epoch: 21930 mean train loss:  5.46921557e-03, bound:  3.15497488e-01\n",
      "Epoch: 21931 mean train loss:  5.46878157e-03, bound:  3.15497488e-01\n",
      "Epoch: 21932 mean train loss:  5.46833966e-03, bound:  3.15497428e-01\n",
      "Epoch: 21933 mean train loss:  5.46792895e-03, bound:  3.15497428e-01\n",
      "Epoch: 21934 mean train loss:  5.46737853e-03, bound:  3.15497398e-01\n",
      "Epoch: 21935 mean train loss:  5.46693942e-03, bound:  3.15497398e-01\n",
      "Epoch: 21936 mean train loss:  5.46649797e-03, bound:  3.15497369e-01\n",
      "Epoch: 21937 mean train loss:  5.46596665e-03, bound:  3.15497369e-01\n",
      "Epoch: 21938 mean train loss:  5.46558993e-03, bound:  3.15497309e-01\n",
      "Epoch: 21939 mean train loss:  5.46517456e-03, bound:  3.15497279e-01\n",
      "Epoch: 21940 mean train loss:  5.46477642e-03, bound:  3.15497279e-01\n",
      "Epoch: 21941 mean train loss:  5.46438014e-03, bound:  3.15497249e-01\n",
      "Epoch: 21942 mean train loss:  5.46401041e-03, bound:  3.15497249e-01\n",
      "Epoch: 21943 mean train loss:  5.46366442e-03, bound:  3.15497190e-01\n",
      "Epoch: 21944 mean train loss:  5.46324346e-03, bound:  3.15497160e-01\n",
      "Epoch: 21945 mean train loss:  5.46288444e-03, bound:  3.15497160e-01\n",
      "Epoch: 21946 mean train loss:  5.46252169e-03, bound:  3.15497130e-01\n",
      "Epoch: 21947 mean train loss:  5.46215009e-03, bound:  3.15497130e-01\n",
      "Epoch: 21948 mean train loss:  5.46178548e-03, bound:  3.15497100e-01\n",
      "Epoch: 21949 mean train loss:  5.46144554e-03, bound:  3.15497041e-01\n",
      "Epoch: 21950 mean train loss:  5.46106836e-03, bound:  3.15497041e-01\n",
      "Epoch: 21951 mean train loss:  5.46061294e-03, bound:  3.15497011e-01\n",
      "Epoch: 21952 mean train loss:  5.46025392e-03, bound:  3.15497011e-01\n",
      "Epoch: 21953 mean train loss:  5.45986695e-03, bound:  3.15496981e-01\n",
      "Epoch: 21954 mean train loss:  5.45945391e-03, bound:  3.15496981e-01\n",
      "Epoch: 21955 mean train loss:  5.45913400e-03, bound:  3.15496951e-01\n",
      "Epoch: 21956 mean train loss:  5.45867439e-03, bound:  3.15496951e-01\n",
      "Epoch: 21957 mean train loss:  5.45832189e-03, bound:  3.15496922e-01\n",
      "Epoch: 21958 mean train loss:  5.45786787e-03, bound:  3.15496862e-01\n",
      "Epoch: 21959 mean train loss:  5.45750791e-03, bound:  3.15496832e-01\n",
      "Epoch: 21960 mean train loss:  5.45704225e-03, bound:  3.15496832e-01\n",
      "Epoch: 21961 mean train loss:  5.45667158e-03, bound:  3.15496802e-01\n",
      "Epoch: 21962 mean train loss:  5.45623060e-03, bound:  3.15496802e-01\n",
      "Epoch: 21963 mean train loss:  5.45592373e-03, bound:  3.15496743e-01\n",
      "Epoch: 21964 mean train loss:  5.45549067e-03, bound:  3.15496743e-01\n",
      "Epoch: 21965 mean train loss:  5.45510789e-03, bound:  3.15496713e-01\n",
      "Epoch: 21966 mean train loss:  5.45475259e-03, bound:  3.15496713e-01\n",
      "Epoch: 21967 mean train loss:  5.45426738e-03, bound:  3.15496683e-01\n",
      "Epoch: 21968 mean train loss:  5.45392698e-03, bound:  3.15496624e-01\n",
      "Epoch: 21969 mean train loss:  5.45356050e-03, bound:  3.15496624e-01\n",
      "Epoch: 21970 mean train loss:  5.45315584e-03, bound:  3.15496594e-01\n",
      "Epoch: 21971 mean train loss:  5.45273954e-03, bound:  3.15496594e-01\n",
      "Epoch: 21972 mean train loss:  5.45238005e-03, bound:  3.15496564e-01\n",
      "Epoch: 21973 mean train loss:  5.45198331e-03, bound:  3.15496504e-01\n",
      "Epoch: 21974 mean train loss:  5.45154605e-03, bound:  3.15496504e-01\n",
      "Epoch: 21975 mean train loss:  5.45110833e-03, bound:  3.15496475e-01\n",
      "Epoch: 21976 mean train loss:  5.45085687e-03, bound:  3.15496475e-01\n",
      "Epoch: 21977 mean train loss:  5.45037305e-03, bound:  3.15496445e-01\n",
      "Epoch: 21978 mean train loss:  5.45006292e-03, bound:  3.15496385e-01\n",
      "Epoch: 21979 mean train loss:  5.44968341e-03, bound:  3.15496385e-01\n",
      "Epoch: 21980 mean train loss:  5.44932950e-03, bound:  3.15496385e-01\n",
      "Epoch: 21981 mean train loss:  5.44891413e-03, bound:  3.15496385e-01\n",
      "Epoch: 21982 mean train loss:  5.44846430e-03, bound:  3.15496296e-01\n",
      "Epoch: 21983 mean train loss:  5.44812577e-03, bound:  3.15496296e-01\n",
      "Epoch: 21984 mean train loss:  5.44770528e-03, bound:  3.15496296e-01\n",
      "Epoch: 21985 mean train loss:  5.44740912e-03, bound:  3.15496266e-01\n",
      "Epoch: 21986 mean train loss:  5.44696487e-03, bound:  3.15496266e-01\n",
      "Epoch: 21987 mean train loss:  5.44660073e-03, bound:  3.15496176e-01\n",
      "Epoch: 21988 mean train loss:  5.44624217e-03, bound:  3.15496176e-01\n",
      "Epoch: 21989 mean train loss:  5.44585008e-03, bound:  3.15496176e-01\n",
      "Epoch: 21990 mean train loss:  5.44551061e-03, bound:  3.15496147e-01\n",
      "Epoch: 21991 mean train loss:  5.44513995e-03, bound:  3.15496087e-01\n",
      "Epoch: 21992 mean train loss:  5.44479908e-03, bound:  3.15496087e-01\n",
      "Epoch: 21993 mean train loss:  5.44447778e-03, bound:  3.15496057e-01\n",
      "Epoch: 21994 mean train loss:  5.44425519e-03, bound:  3.15496057e-01\n",
      "Epoch: 21995 mean train loss:  5.44408383e-03, bound:  3.15496027e-01\n",
      "Epoch: 21996 mean train loss:  5.44392131e-03, bound:  3.15496027e-01\n",
      "Epoch: 21997 mean train loss:  5.44404564e-03, bound:  3.15495998e-01\n",
      "Epoch: 21998 mean train loss:  5.44429058e-03, bound:  3.15495998e-01\n",
      "Epoch: 21999 mean train loss:  5.44506591e-03, bound:  3.15495908e-01\n",
      "Epoch: 22000 mean train loss:  5.44630922e-03, bound:  3.15495938e-01\n",
      "Epoch: 22001 mean train loss:  5.44852298e-03, bound:  3.15495849e-01\n",
      "Epoch: 22002 mean train loss:  5.45171183e-03, bound:  3.15495908e-01\n",
      "Epoch: 22003 mean train loss:  5.45641407e-03, bound:  3.15495819e-01\n",
      "Epoch: 22004 mean train loss:  5.46207419e-03, bound:  3.15495908e-01\n",
      "Epoch: 22005 mean train loss:  5.46787120e-03, bound:  3.15495729e-01\n",
      "Epoch: 22006 mean train loss:  5.47155086e-03, bound:  3.15495849e-01\n",
      "Epoch: 22007 mean train loss:  5.47022186e-03, bound:  3.15495700e-01\n",
      "Epoch: 22008 mean train loss:  5.46257105e-03, bound:  3.15495819e-01\n",
      "Epoch: 22009 mean train loss:  5.45085082e-03, bound:  3.15495700e-01\n",
      "Epoch: 22010 mean train loss:  5.44081675e-03, bound:  3.15495700e-01\n",
      "Epoch: 22011 mean train loss:  5.43726655e-03, bound:  3.15495700e-01\n",
      "Epoch: 22012 mean train loss:  5.44043630e-03, bound:  3.15495610e-01\n",
      "Epoch: 22013 mean train loss:  5.44610852e-03, bound:  3.15495700e-01\n",
      "Epoch: 22014 mean train loss:  5.44924149e-03, bound:  3.15495580e-01\n",
      "Epoch: 22015 mean train loss:  5.44732716e-03, bound:  3.15495610e-01\n",
      "Epoch: 22016 mean train loss:  5.44170663e-03, bound:  3.15495580e-01\n",
      "Epoch: 22017 mean train loss:  5.43641998e-03, bound:  3.15495580e-01\n",
      "Epoch: 22018 mean train loss:  5.43475384e-03, bound:  3.15495580e-01\n",
      "Epoch: 22019 mean train loss:  5.43667842e-03, bound:  3.15495491e-01\n",
      "Epoch: 22020 mean train loss:  5.43952594e-03, bound:  3.15495491e-01\n",
      "Epoch: 22021 mean train loss:  5.44027379e-03, bound:  3.15495461e-01\n",
      "Epoch: 22022 mean train loss:  5.43815549e-03, bound:  3.15495491e-01\n",
      "Epoch: 22023 mean train loss:  5.43476641e-03, bound:  3.15495461e-01\n",
      "Epoch: 22024 mean train loss:  5.43254055e-03, bound:  3.15495461e-01\n",
      "Epoch: 22025 mean train loss:  5.43267094e-03, bound:  3.15495402e-01\n",
      "Epoch: 22026 mean train loss:  5.43402880e-03, bound:  3.15495372e-01\n",
      "Epoch: 22027 mean train loss:  5.43469237e-03, bound:  3.15495372e-01\n",
      "Epoch: 22028 mean train loss:  5.43380948e-03, bound:  3.15495342e-01\n",
      "Epoch: 22029 mean train loss:  5.43208420e-03, bound:  3.15495342e-01\n",
      "Epoch: 22030 mean train loss:  5.43055544e-03, bound:  3.15495282e-01\n",
      "Epoch: 22031 mean train loss:  5.43013448e-03, bound:  3.15495253e-01\n",
      "Epoch: 22032 mean train loss:  5.43048372e-03, bound:  3.15495253e-01\n",
      "Epoch: 22033 mean train loss:  5.43086790e-03, bound:  3.15495223e-01\n",
      "Epoch: 22034 mean train loss:  5.43058570e-03, bound:  3.15495223e-01\n",
      "Epoch: 22035 mean train loss:  5.42955566e-03, bound:  3.15495163e-01\n",
      "Epoch: 22036 mean train loss:  5.42841852e-03, bound:  3.15495163e-01\n",
      "Epoch: 22037 mean train loss:  5.42777590e-03, bound:  3.15495133e-01\n",
      "Epoch: 22038 mean train loss:  5.42762643e-03, bound:  3.15495074e-01\n",
      "Epoch: 22039 mean train loss:  5.42781828e-03, bound:  3.15495074e-01\n",
      "Epoch: 22040 mean train loss:  5.42768463e-03, bound:  3.15495044e-01\n",
      "Epoch: 22041 mean train loss:  5.42714074e-03, bound:  3.15495044e-01\n",
      "Epoch: 22042 mean train loss:  5.42625738e-03, bound:  3.15495014e-01\n",
      "Epoch: 22043 mean train loss:  5.42563153e-03, bound:  3.15495014e-01\n",
      "Epoch: 22044 mean train loss:  5.42533631e-03, bound:  3.15494955e-01\n",
      "Epoch: 22045 mean train loss:  5.42518310e-03, bound:  3.15494925e-01\n",
      "Epoch: 22046 mean train loss:  5.42499358e-03, bound:  3.15494925e-01\n",
      "Epoch: 22047 mean train loss:  5.42460708e-03, bound:  3.15494895e-01\n",
      "Epoch: 22048 mean train loss:  5.42407250e-03, bound:  3.15494895e-01\n",
      "Epoch: 22049 mean train loss:  5.42352302e-03, bound:  3.15494895e-01\n",
      "Epoch: 22050 mean train loss:  5.42303082e-03, bound:  3.15494895e-01\n",
      "Epoch: 22051 mean train loss:  5.42273838e-03, bound:  3.15494835e-01\n",
      "Epoch: 22052 mean train loss:  5.42250974e-03, bound:  3.15494776e-01\n",
      "Epoch: 22053 mean train loss:  5.42218378e-03, bound:  3.15494776e-01\n",
      "Epoch: 22054 mean train loss:  5.42179728e-03, bound:  3.15494776e-01\n",
      "Epoch: 22055 mean train loss:  5.42138331e-03, bound:  3.15494776e-01\n",
      "Epoch: 22056 mean train loss:  5.42090181e-03, bound:  3.15494716e-01\n",
      "Epoch: 22057 mean train loss:  5.42045524e-03, bound:  3.15494716e-01\n",
      "Epoch: 22058 mean train loss:  5.42013254e-03, bound:  3.15494657e-01\n",
      "Epoch: 22059 mean train loss:  5.41981263e-03, bound:  3.15494657e-01\n",
      "Epoch: 22060 mean train loss:  5.41942753e-03, bound:  3.15494627e-01\n",
      "Epoch: 22061 mean train loss:  5.41910995e-03, bound:  3.15494597e-01\n",
      "Epoch: 22062 mean train loss:  5.41871740e-03, bound:  3.15494597e-01\n",
      "Epoch: 22063 mean train loss:  5.41829551e-03, bound:  3.15494567e-01\n",
      "Epoch: 22064 mean train loss:  5.41794160e-03, bound:  3.15494567e-01\n",
      "Epoch: 22065 mean train loss:  5.41753136e-03, bound:  3.15494508e-01\n",
      "Epoch: 22066 mean train loss:  5.41728269e-03, bound:  3.15494508e-01\n",
      "Epoch: 22067 mean train loss:  5.41683054e-03, bound:  3.15494478e-01\n",
      "Epoch: 22068 mean train loss:  5.41649619e-03, bound:  3.15494478e-01\n",
      "Epoch: 22069 mean train loss:  5.41616837e-03, bound:  3.15494448e-01\n",
      "Epoch: 22070 mean train loss:  5.41577488e-03, bound:  3.15494388e-01\n",
      "Epoch: 22071 mean train loss:  5.41536417e-03, bound:  3.15494388e-01\n",
      "Epoch: 22072 mean train loss:  5.41497208e-03, bound:  3.15494359e-01\n",
      "Epoch: 22073 mean train loss:  5.41462936e-03, bound:  3.15494359e-01\n",
      "Epoch: 22074 mean train loss:  5.41430386e-03, bound:  3.15494329e-01\n",
      "Epoch: 22075 mean train loss:  5.41390106e-03, bound:  3.15494329e-01\n",
      "Epoch: 22076 mean train loss:  5.41357929e-03, bound:  3.15494269e-01\n",
      "Epoch: 22077 mean train loss:  5.41310804e-03, bound:  3.15494269e-01\n",
      "Epoch: 22078 mean train loss:  5.41281328e-03, bound:  3.15494239e-01\n",
      "Epoch: 22079 mean train loss:  5.41244494e-03, bound:  3.15494239e-01\n",
      "Epoch: 22080 mean train loss:  5.41210640e-03, bound:  3.15494210e-01\n",
      "Epoch: 22081 mean train loss:  5.41173154e-03, bound:  3.15494150e-01\n",
      "Epoch: 22082 mean train loss:  5.41135110e-03, bound:  3.15494150e-01\n",
      "Epoch: 22083 mean train loss:  5.41100139e-03, bound:  3.15494150e-01\n",
      "Epoch: 22084 mean train loss:  5.41061722e-03, bound:  3.15494120e-01\n",
      "Epoch: 22085 mean train loss:  5.41023677e-03, bound:  3.15494090e-01\n",
      "Epoch: 22086 mean train loss:  5.40991686e-03, bound:  3.15494090e-01\n",
      "Epoch: 22087 mean train loss:  5.40952245e-03, bound:  3.15494061e-01\n",
      "Epoch: 22088 mean train loss:  5.40909497e-03, bound:  3.15494031e-01\n",
      "Epoch: 22089 mean train loss:  5.40882256e-03, bound:  3.15494031e-01\n",
      "Epoch: 22090 mean train loss:  5.40837739e-03, bound:  3.15493971e-01\n",
      "Epoch: 22091 mean train loss:  5.40804863e-03, bound:  3.15493971e-01\n",
      "Epoch: 22092 mean train loss:  5.40768029e-03, bound:  3.15493941e-01\n",
      "Epoch: 22093 mean train loss:  5.40732965e-03, bound:  3.15493941e-01\n",
      "Epoch: 22094 mean train loss:  5.40693663e-03, bound:  3.15493912e-01\n",
      "Epoch: 22095 mean train loss:  5.40660135e-03, bound:  3.15493852e-01\n",
      "Epoch: 22096 mean train loss:  5.40621625e-03, bound:  3.15493852e-01\n",
      "Epoch: 22097 mean train loss:  5.40581532e-03, bound:  3.15493822e-01\n",
      "Epoch: 22098 mean train loss:  5.40550565e-03, bound:  3.15493792e-01\n",
      "Epoch: 22099 mean train loss:  5.40511962e-03, bound:  3.15493792e-01\n",
      "Epoch: 22100 mean train loss:  5.40472288e-03, bound:  3.15493762e-01\n",
      "Epoch: 22101 mean train loss:  5.40435826e-03, bound:  3.15493762e-01\n",
      "Epoch: 22102 mean train loss:  5.40395128e-03, bound:  3.15493703e-01\n",
      "Epoch: 22103 mean train loss:  5.40359365e-03, bound:  3.15493703e-01\n",
      "Epoch: 22104 mean train loss:  5.40326303e-03, bound:  3.15493673e-01\n",
      "Epoch: 22105 mean train loss:  5.40289516e-03, bound:  3.15493673e-01\n",
      "Epoch: 22106 mean train loss:  5.40251378e-03, bound:  3.15493643e-01\n",
      "Epoch: 22107 mean train loss:  5.40215056e-03, bound:  3.15493643e-01\n",
      "Epoch: 22108 mean train loss:  5.40179806e-03, bound:  3.15493584e-01\n",
      "Epoch: 22109 mean train loss:  5.40145254e-03, bound:  3.15493554e-01\n",
      "Epoch: 22110 mean train loss:  5.40105253e-03, bound:  3.15493554e-01\n",
      "Epoch: 22111 mean train loss:  5.40065998e-03, bound:  3.15493524e-01\n",
      "Epoch: 22112 mean train loss:  5.40030422e-03, bound:  3.15493524e-01\n",
      "Epoch: 22113 mean train loss:  5.39994054e-03, bound:  3.15493464e-01\n",
      "Epoch: 22114 mean train loss:  5.39957918e-03, bound:  3.15493464e-01\n",
      "Epoch: 22115 mean train loss:  5.39918523e-03, bound:  3.15493464e-01\n",
      "Epoch: 22116 mean train loss:  5.39876893e-03, bound:  3.15493435e-01\n",
      "Epoch: 22117 mean train loss:  5.39845042e-03, bound:  3.15493375e-01\n",
      "Epoch: 22118 mean train loss:  5.39810583e-03, bound:  3.15493345e-01\n",
      "Epoch: 22119 mean train loss:  5.39772492e-03, bound:  3.15493345e-01\n",
      "Epoch: 22120 mean train loss:  5.39733237e-03, bound:  3.15493345e-01\n",
      "Epoch: 22121 mean train loss:  5.39694121e-03, bound:  3.15493345e-01\n",
      "Epoch: 22122 mean train loss:  5.39658731e-03, bound:  3.15493286e-01\n",
      "Epoch: 22123 mean train loss:  5.39618125e-03, bound:  3.15493256e-01\n",
      "Epoch: 22124 mean train loss:  5.39588323e-03, bound:  3.15493256e-01\n",
      "Epoch: 22125 mean train loss:  5.39546041e-03, bound:  3.15493226e-01\n",
      "Epoch: 22126 mean train loss:  5.39518381e-03, bound:  3.15493226e-01\n",
      "Epoch: 22127 mean train loss:  5.39470650e-03, bound:  3.15493166e-01\n",
      "Epoch: 22128 mean train loss:  5.39431954e-03, bound:  3.15493166e-01\n",
      "Epoch: 22129 mean train loss:  5.39400475e-03, bound:  3.15493137e-01\n",
      "Epoch: 22130 mean train loss:  5.39359683e-03, bound:  3.15493107e-01\n",
      "Epoch: 22131 mean train loss:  5.39327506e-03, bound:  3.15493107e-01\n",
      "Epoch: 22132 mean train loss:  5.39287552e-03, bound:  3.15493047e-01\n",
      "Epoch: 22133 mean train loss:  5.39251138e-03, bound:  3.15493047e-01\n",
      "Epoch: 22134 mean train loss:  5.39212814e-03, bound:  3.15493017e-01\n",
      "Epoch: 22135 mean train loss:  5.39180962e-03, bound:  3.15492988e-01\n",
      "Epoch: 22136 mean train loss:  5.39137283e-03, bound:  3.15492988e-01\n",
      "Epoch: 22137 mean train loss:  5.39103290e-03, bound:  3.15492928e-01\n",
      "Epoch: 22138 mean train loss:  5.39064640e-03, bound:  3.15492928e-01\n",
      "Epoch: 22139 mean train loss:  5.39026875e-03, bound:  3.15492898e-01\n",
      "Epoch: 22140 mean train loss:  5.38995396e-03, bound:  3.15492898e-01\n",
      "Epoch: 22141 mean train loss:  5.38956374e-03, bound:  3.15492868e-01\n",
      "Epoch: 22142 mean train loss:  5.38918097e-03, bound:  3.15492809e-01\n",
      "Epoch: 22143 mean train loss:  5.38883358e-03, bound:  3.15492809e-01\n",
      "Epoch: 22144 mean train loss:  5.38847316e-03, bound:  3.15492779e-01\n",
      "Epoch: 22145 mean train loss:  5.38804894e-03, bound:  3.15492779e-01\n",
      "Epoch: 22146 mean train loss:  5.38767548e-03, bound:  3.15492749e-01\n",
      "Epoch: 22147 mean train loss:  5.38726011e-03, bound:  3.15492749e-01\n",
      "Epoch: 22148 mean train loss:  5.38688852e-03, bound:  3.15492690e-01\n",
      "Epoch: 22149 mean train loss:  5.38657792e-03, bound:  3.15492690e-01\n",
      "Epoch: 22150 mean train loss:  5.38621284e-03, bound:  3.15492660e-01\n",
      "Epoch: 22151 mean train loss:  5.38583891e-03, bound:  3.15492660e-01\n",
      "Epoch: 22152 mean train loss:  5.38549805e-03, bound:  3.15492660e-01\n",
      "Epoch: 22153 mean train loss:  5.38508175e-03, bound:  3.15492600e-01\n",
      "Epoch: 22154 mean train loss:  5.38465707e-03, bound:  3.15492570e-01\n",
      "Epoch: 22155 mean train loss:  5.38437674e-03, bound:  3.15492541e-01\n",
      "Epoch: 22156 mean train loss:  5.38396090e-03, bound:  3.15492541e-01\n",
      "Epoch: 22157 mean train loss:  5.38358698e-03, bound:  3.15492541e-01\n",
      "Epoch: 22158 mean train loss:  5.38326055e-03, bound:  3.15492481e-01\n",
      "Epoch: 22159 mean train loss:  5.38282096e-03, bound:  3.15492481e-01\n",
      "Epoch: 22160 mean train loss:  5.38249547e-03, bound:  3.15492421e-01\n",
      "Epoch: 22161 mean train loss:  5.38210291e-03, bound:  3.15492421e-01\n",
      "Epoch: 22162 mean train loss:  5.38180070e-03, bound:  3.15492421e-01\n",
      "Epoch: 22163 mean train loss:  5.38139138e-03, bound:  3.15492362e-01\n",
      "Epoch: 22164 mean train loss:  5.38099743e-03, bound:  3.15492362e-01\n",
      "Epoch: 22165 mean train loss:  5.38058951e-03, bound:  3.15492332e-01\n",
      "Epoch: 22166 mean train loss:  5.38027240e-03, bound:  3.15492332e-01\n",
      "Epoch: 22167 mean train loss:  5.37985191e-03, bound:  3.15492302e-01\n",
      "Epoch: 22168 mean train loss:  5.37945516e-03, bound:  3.15492243e-01\n",
      "Epoch: 22169 mean train loss:  5.37908589e-03, bound:  3.15492243e-01\n",
      "Epoch: 22170 mean train loss:  5.37873898e-03, bound:  3.15492243e-01\n",
      "Epoch: 22171 mean train loss:  5.37838880e-03, bound:  3.15492213e-01\n",
      "Epoch: 22172 mean train loss:  5.37800277e-03, bound:  3.15492153e-01\n",
      "Epoch: 22173 mean train loss:  5.37773268e-03, bound:  3.15492123e-01\n",
      "Epoch: 22174 mean train loss:  5.37737552e-03, bound:  3.15492123e-01\n",
      "Epoch: 22175 mean train loss:  5.37715759e-03, bound:  3.15492094e-01\n",
      "Epoch: 22176 mean train loss:  5.37685724e-03, bound:  3.15492094e-01\n",
      "Epoch: 22177 mean train loss:  5.37665421e-03, bound:  3.15492034e-01\n",
      "Epoch: 22178 mean train loss:  5.37655130e-03, bound:  3.15492034e-01\n",
      "Epoch: 22179 mean train loss:  5.37646050e-03, bound:  3.15492034e-01\n",
      "Epoch: 22180 mean train loss:  5.37641300e-03, bound:  3.15492034e-01\n",
      "Epoch: 22181 mean train loss:  5.37652802e-03, bound:  3.15491974e-01\n",
      "Epoch: 22182 mean train loss:  5.37681160e-03, bound:  3.15491974e-01\n",
      "Epoch: 22183 mean train loss:  5.37722418e-03, bound:  3.15491915e-01\n",
      "Epoch: 22184 mean train loss:  5.37794130e-03, bound:  3.15491915e-01\n",
      "Epoch: 22185 mean train loss:  5.37883071e-03, bound:  3.15491855e-01\n",
      "Epoch: 22186 mean train loss:  5.38005913e-03, bound:  3.15491915e-01\n",
      "Epoch: 22187 mean train loss:  5.38141094e-03, bound:  3.15491796e-01\n",
      "Epoch: 22188 mean train loss:  5.38280467e-03, bound:  3.15491855e-01\n",
      "Epoch: 22189 mean train loss:  5.38404100e-03, bound:  3.15491736e-01\n",
      "Epoch: 22190 mean train loss:  5.38459327e-03, bound:  3.15491855e-01\n",
      "Epoch: 22191 mean train loss:  5.38415695e-03, bound:  3.15491706e-01\n",
      "Epoch: 22192 mean train loss:  5.38235437e-03, bound:  3.15491796e-01\n",
      "Epoch: 22193 mean train loss:  5.37921907e-03, bound:  3.15491676e-01\n",
      "Epoch: 22194 mean train loss:  5.37532615e-03, bound:  3.15491706e-01\n",
      "Epoch: 22195 mean train loss:  5.37182810e-03, bound:  3.15491676e-01\n",
      "Epoch: 22196 mean train loss:  5.36953192e-03, bound:  3.15491676e-01\n",
      "Epoch: 22197 mean train loss:  5.36874309e-03, bound:  3.15491647e-01\n",
      "Epoch: 22198 mean train loss:  5.36906719e-03, bound:  3.15491647e-01\n",
      "Epoch: 22199 mean train loss:  5.37011400e-03, bound:  3.15491647e-01\n",
      "Epoch: 22200 mean train loss:  5.37106628e-03, bound:  3.15491557e-01\n",
      "Epoch: 22201 mean train loss:  5.37151564e-03, bound:  3.15491557e-01\n",
      "Epoch: 22202 mean train loss:  5.37130469e-03, bound:  3.15491527e-01\n",
      "Epoch: 22203 mean train loss:  5.37019782e-03, bound:  3.15491527e-01\n",
      "Epoch: 22204 mean train loss:  5.36865462e-03, bound:  3.15491468e-01\n",
      "Epoch: 22205 mean train loss:  5.36696473e-03, bound:  3.15491468e-01\n",
      "Epoch: 22206 mean train loss:  5.36576798e-03, bound:  3.15491438e-01\n",
      "Epoch: 22207 mean train loss:  5.36502292e-03, bound:  3.15491408e-01\n",
      "Epoch: 22208 mean train loss:  5.36492746e-03, bound:  3.15491408e-01\n",
      "Epoch: 22209 mean train loss:  5.36503689e-03, bound:  3.15491349e-01\n",
      "Epoch: 22210 mean train loss:  5.36521524e-03, bound:  3.15491349e-01\n",
      "Epoch: 22211 mean train loss:  5.36525622e-03, bound:  3.15491319e-01\n",
      "Epoch: 22212 mean train loss:  5.36488788e-03, bound:  3.15491319e-01\n",
      "Epoch: 22213 mean train loss:  5.36420615e-03, bound:  3.15491289e-01\n",
      "Epoch: 22214 mean train loss:  5.36331069e-03, bound:  3.15491259e-01\n",
      "Epoch: 22215 mean train loss:  5.36247157e-03, bound:  3.15491229e-01\n",
      "Epoch: 22216 mean train loss:  5.36186667e-03, bound:  3.15491229e-01\n",
      "Epoch: 22217 mean train loss:  5.36152441e-03, bound:  3.15491229e-01\n",
      "Epoch: 22218 mean train loss:  5.36119193e-03, bound:  3.15491170e-01\n",
      "Epoch: 22219 mean train loss:  5.36100613e-03, bound:  3.15491170e-01\n",
      "Epoch: 22220 mean train loss:  5.36081335e-03, bound:  3.15491110e-01\n",
      "Epoch: 22221 mean train loss:  5.36061311e-03, bound:  3.15491110e-01\n",
      "Epoch: 22222 mean train loss:  5.36026806e-03, bound:  3.15491110e-01\n",
      "Epoch: 22223 mean train loss:  5.35986386e-03, bound:  3.15491080e-01\n",
      "Epoch: 22224 mean train loss:  5.35936188e-03, bound:  3.15491080e-01\n",
      "Epoch: 22225 mean train loss:  5.35880867e-03, bound:  3.15491021e-01\n",
      "Epoch: 22226 mean train loss:  5.35828294e-03, bound:  3.15490991e-01\n",
      "Epoch: 22227 mean train loss:  5.35786990e-03, bound:  3.15490991e-01\n",
      "Epoch: 22228 mean train loss:  5.35744615e-03, bound:  3.15490991e-01\n",
      "Epoch: 22229 mean train loss:  5.35711320e-03, bound:  3.15490931e-01\n",
      "Epoch: 22230 mean train loss:  5.35677746e-03, bound:  3.15490901e-01\n",
      "Epoch: 22231 mean train loss:  5.35645010e-03, bound:  3.15490901e-01\n",
      "Epoch: 22232 mean train loss:  5.35606313e-03, bound:  3.15490872e-01\n",
      "Epoch: 22233 mean train loss:  5.35578374e-03, bound:  3.15490842e-01\n",
      "Epoch: 22234 mean train loss:  5.35542751e-03, bound:  3.15490812e-01\n",
      "Epoch: 22235 mean train loss:  5.35493623e-03, bound:  3.15490812e-01\n",
      "Epoch: 22236 mean train loss:  5.35460562e-03, bound:  3.15490782e-01\n",
      "Epoch: 22237 mean train loss:  5.35425777e-03, bound:  3.15490782e-01\n",
      "Epoch: 22238 mean train loss:  5.35384892e-03, bound:  3.15490752e-01\n",
      "Epoch: 22239 mean train loss:  5.35349036e-03, bound:  3.15490723e-01\n",
      "Epoch: 22240 mean train loss:  5.35313832e-03, bound:  3.15490723e-01\n",
      "Epoch: 22241 mean train loss:  5.35273133e-03, bound:  3.15490693e-01\n",
      "Epoch: 22242 mean train loss:  5.35233645e-03, bound:  3.15490663e-01\n",
      "Epoch: 22243 mean train loss:  5.35208965e-03, bound:  3.15490633e-01\n",
      "Epoch: 22244 mean train loss:  5.35167241e-03, bound:  3.15490603e-01\n",
      "Epoch: 22245 mean train loss:  5.35136461e-03, bound:  3.15490574e-01\n",
      "Epoch: 22246 mean train loss:  5.35094691e-03, bound:  3.15490544e-01\n",
      "Epoch: 22247 mean train loss:  5.35062607e-03, bound:  3.15490544e-01\n",
      "Epoch: 22248 mean train loss:  5.35030011e-03, bound:  3.15490514e-01\n",
      "Epoch: 22249 mean train loss:  5.34987869e-03, bound:  3.15490514e-01\n",
      "Epoch: 22250 mean train loss:  5.34951175e-03, bound:  3.15490484e-01\n",
      "Epoch: 22251 mean train loss:  5.34913410e-03, bound:  3.15490454e-01\n",
      "Epoch: 22252 mean train loss:  5.34874713e-03, bound:  3.15490425e-01\n",
      "Epoch: 22253 mean train loss:  5.34834061e-03, bound:  3.15490425e-01\n",
      "Epoch: 22254 mean train loss:  5.34797879e-03, bound:  3.15490425e-01\n",
      "Epoch: 22255 mean train loss:  5.34767238e-03, bound:  3.15490395e-01\n",
      "Epoch: 22256 mean train loss:  5.34723839e-03, bound:  3.15490335e-01\n",
      "Epoch: 22257 mean train loss:  5.34692593e-03, bound:  3.15490305e-01\n",
      "Epoch: 22258 mean train loss:  5.34651708e-03, bound:  3.15490305e-01\n",
      "Epoch: 22259 mean train loss:  5.34619158e-03, bound:  3.15490305e-01\n",
      "Epoch: 22260 mean train loss:  5.34584979e-03, bound:  3.15490276e-01\n",
      "Epoch: 22261 mean train loss:  5.34547633e-03, bound:  3.15490246e-01\n",
      "Epoch: 22262 mean train loss:  5.34510380e-03, bound:  3.15490216e-01\n",
      "Epoch: 22263 mean train loss:  5.34472009e-03, bound:  3.15490186e-01\n",
      "Epoch: 22264 mean train loss:  5.34439646e-03, bound:  3.15490186e-01\n",
      "Epoch: 22265 mean train loss:  5.34401229e-03, bound:  3.15490127e-01\n",
      "Epoch: 22266 mean train loss:  5.34360856e-03, bound:  3.15490127e-01\n",
      "Epoch: 22267 mean train loss:  5.34325885e-03, bound:  3.15490097e-01\n",
      "Epoch: 22268 mean train loss:  5.34288678e-03, bound:  3.15490097e-01\n",
      "Epoch: 22269 mean train loss:  5.34253428e-03, bound:  3.15490067e-01\n",
      "Epoch: 22270 mean train loss:  5.34218457e-03, bound:  3.15490007e-01\n",
      "Epoch: 22271 mean train loss:  5.34180319e-03, bound:  3.15490007e-01\n",
      "Epoch: 22272 mean train loss:  5.34144323e-03, bound:  3.15490007e-01\n",
      "Epoch: 22273 mean train loss:  5.34105580e-03, bound:  3.15489978e-01\n",
      "Epoch: 22274 mean train loss:  5.34065859e-03, bound:  3.15489978e-01\n",
      "Epoch: 22275 mean train loss:  5.34031820e-03, bound:  3.15489888e-01\n",
      "Epoch: 22276 mean train loss:  5.33993775e-03, bound:  3.15489888e-01\n",
      "Epoch: 22277 mean train loss:  5.33964438e-03, bound:  3.15489888e-01\n",
      "Epoch: 22278 mean train loss:  5.33917174e-03, bound:  3.15489858e-01\n",
      "Epoch: 22279 mean train loss:  5.33885136e-03, bound:  3.15489858e-01\n",
      "Epoch: 22280 mean train loss:  5.33847883e-03, bound:  3.15489829e-01\n",
      "Epoch: 22281 mean train loss:  5.33815008e-03, bound:  3.15489799e-01\n",
      "Epoch: 22282 mean train loss:  5.33779152e-03, bound:  3.15489769e-01\n",
      "Epoch: 22283 mean train loss:  5.33747720e-03, bound:  3.15489739e-01\n",
      "Epoch: 22284 mean train loss:  5.33708138e-03, bound:  3.15489739e-01\n",
      "Epoch: 22285 mean train loss:  5.33671491e-03, bound:  3.15489680e-01\n",
      "Epoch: 22286 mean train loss:  5.33634424e-03, bound:  3.15489680e-01\n",
      "Epoch: 22287 mean train loss:  5.33601549e-03, bound:  3.15489650e-01\n",
      "Epoch: 22288 mean train loss:  5.33568254e-03, bound:  3.15489650e-01\n",
      "Epoch: 22289 mean train loss:  5.33538545e-03, bound:  3.15489620e-01\n",
      "Epoch: 22290 mean train loss:  5.33516333e-03, bound:  3.15489620e-01\n",
      "Epoch: 22291 mean train loss:  5.33501757e-03, bound:  3.15489560e-01\n",
      "Epoch: 22292 mean train loss:  5.33488300e-03, bound:  3.15489560e-01\n",
      "Epoch: 22293 mean train loss:  5.33492258e-03, bound:  3.15489531e-01\n",
      "Epoch: 22294 mean train loss:  5.33518428e-03, bound:  3.15489531e-01\n",
      "Epoch: 22295 mean train loss:  5.33558009e-03, bound:  3.15489501e-01\n",
      "Epoch: 22296 mean train loss:  5.33623155e-03, bound:  3.15489501e-01\n",
      "Epoch: 22297 mean train loss:  5.33732912e-03, bound:  3.15489411e-01\n",
      "Epoch: 22298 mean train loss:  5.33893006e-03, bound:  3.15489441e-01\n",
      "Epoch: 22299 mean train loss:  5.34101203e-03, bound:  3.15489382e-01\n",
      "Epoch: 22300 mean train loss:  5.34342043e-03, bound:  3.15489411e-01\n",
      "Epoch: 22301 mean train loss:  5.34606166e-03, bound:  3.15489322e-01\n",
      "Epoch: 22302 mean train loss:  5.34813991e-03, bound:  3.15489411e-01\n",
      "Epoch: 22303 mean train loss:  5.34914900e-03, bound:  3.15489292e-01\n",
      "Epoch: 22304 mean train loss:  5.34792012e-03, bound:  3.15489322e-01\n",
      "Epoch: 22305 mean train loss:  5.34408260e-03, bound:  3.15489233e-01\n",
      "Epoch: 22306 mean train loss:  5.33845881e-03, bound:  3.15489292e-01\n",
      "Epoch: 22307 mean train loss:  5.33279683e-03, bound:  3.15489203e-01\n",
      "Epoch: 22308 mean train loss:  5.32904547e-03, bound:  3.15489233e-01\n",
      "Epoch: 22309 mean train loss:  5.32804197e-03, bound:  3.15489173e-01\n",
      "Epoch: 22310 mean train loss:  5.32933185e-03, bound:  3.15489173e-01\n",
      "Epoch: 22311 mean train loss:  5.33163454e-03, bound:  3.15489173e-01\n",
      "Epoch: 22312 mean train loss:  5.33323642e-03, bound:  3.15489113e-01\n",
      "Epoch: 22313 mean train loss:  5.33317355e-03, bound:  3.15489113e-01\n",
      "Epoch: 22314 mean train loss:  5.33135189e-03, bound:  3.15489084e-01\n",
      "Epoch: 22315 mean train loss:  5.32868970e-03, bound:  3.15489084e-01\n",
      "Epoch: 22316 mean train loss:  5.32633532e-03, bound:  3.15489054e-01\n",
      "Epoch: 22317 mean train loss:  5.32513577e-03, bound:  3.15488994e-01\n",
      "Epoch: 22318 mean train loss:  5.32523077e-03, bound:  3.15488994e-01\n",
      "Epoch: 22319 mean train loss:  5.32605918e-03, bound:  3.15488994e-01\n",
      "Epoch: 22320 mean train loss:  5.32685500e-03, bound:  3.15488994e-01\n",
      "Epoch: 22321 mean train loss:  5.32684755e-03, bound:  3.15488935e-01\n",
      "Epoch: 22322 mean train loss:  5.32599725e-03, bound:  3.15488935e-01\n",
      "Epoch: 22323 mean train loss:  5.32449130e-03, bound:  3.15488875e-01\n",
      "Epoch: 22324 mean train loss:  5.32321725e-03, bound:  3.15488875e-01\n",
      "Epoch: 22325 mean train loss:  5.32225892e-03, bound:  3.15488875e-01\n",
      "Epoch: 22326 mean train loss:  5.32210944e-03, bound:  3.15488845e-01\n",
      "Epoch: 22327 mean train loss:  5.32222632e-03, bound:  3.15488845e-01\n",
      "Epoch: 22328 mean train loss:  5.32233762e-03, bound:  3.15488756e-01\n",
      "Epoch: 22329 mean train loss:  5.32229338e-03, bound:  3.15488756e-01\n",
      "Epoch: 22330 mean train loss:  5.32183936e-03, bound:  3.15488756e-01\n",
      "Epoch: 22331 mean train loss:  5.32095646e-03, bound:  3.15488756e-01\n",
      "Epoch: 22332 mean train loss:  5.32026496e-03, bound:  3.15488726e-01\n",
      "Epoch: 22333 mean train loss:  5.31953480e-03, bound:  3.15488666e-01\n",
      "Epoch: 22334 mean train loss:  5.31914085e-03, bound:  3.15488666e-01\n",
      "Epoch: 22335 mean train loss:  5.31898020e-03, bound:  3.15488636e-01\n",
      "Epoch: 22336 mean train loss:  5.31890290e-03, bound:  3.15488636e-01\n",
      "Epoch: 22337 mean train loss:  5.31877857e-03, bound:  3.15488607e-01\n",
      "Epoch: 22338 mean train loss:  5.31854248e-03, bound:  3.15488607e-01\n",
      "Epoch: 22339 mean train loss:  5.31805772e-03, bound:  3.15488547e-01\n",
      "Epoch: 22340 mean train loss:  5.31749567e-03, bound:  3.15488547e-01\n",
      "Epoch: 22341 mean train loss:  5.31687262e-03, bound:  3.15488517e-01\n",
      "Epoch: 22342 mean train loss:  5.31631429e-03, bound:  3.15488487e-01\n",
      "Epoch: 22343 mean train loss:  5.31591801e-03, bound:  3.15488458e-01\n",
      "Epoch: 22344 mean train loss:  5.31566562e-03, bound:  3.15488428e-01\n",
      "Epoch: 22345 mean train loss:  5.31544536e-03, bound:  3.15488428e-01\n",
      "Epoch: 22346 mean train loss:  5.31515805e-03, bound:  3.15488398e-01\n",
      "Epoch: 22347 mean train loss:  5.31484652e-03, bound:  3.15488398e-01\n",
      "Epoch: 22348 mean train loss:  5.31444326e-03, bound:  3.15488368e-01\n",
      "Epoch: 22349 mean train loss:  5.31399297e-03, bound:  3.15488338e-01\n",
      "Epoch: 22350 mean train loss:  5.31353196e-03, bound:  3.15488309e-01\n",
      "Epoch: 22351 mean train loss:  5.31315105e-03, bound:  3.15488309e-01\n",
      "Epoch: 22352 mean train loss:  5.31279016e-03, bound:  3.15488279e-01\n",
      "Epoch: 22353 mean train loss:  5.31247258e-03, bound:  3.15488279e-01\n",
      "Epoch: 22354 mean train loss:  5.31213265e-03, bound:  3.15488279e-01\n",
      "Epoch: 22355 mean train loss:  5.31190261e-03, bound:  3.15488189e-01\n",
      "Epoch: 22356 mean train loss:  5.31151704e-03, bound:  3.15488189e-01\n",
      "Epoch: 22357 mean train loss:  5.31118503e-03, bound:  3.15488189e-01\n",
      "Epoch: 22358 mean train loss:  5.31080319e-03, bound:  3.15488160e-01\n",
      "Epoch: 22359 mean train loss:  5.31039946e-03, bound:  3.15488160e-01\n",
      "Epoch: 22360 mean train loss:  5.31002553e-03, bound:  3.15488070e-01\n",
      "Epoch: 22361 mean train loss:  5.30965114e-03, bound:  3.15488070e-01\n",
      "Epoch: 22362 mean train loss:  5.30920876e-03, bound:  3.15488070e-01\n",
      "Epoch: 22363 mean train loss:  5.30894892e-03, bound:  3.15488070e-01\n",
      "Epoch: 22364 mean train loss:  5.30848140e-03, bound:  3.15488011e-01\n",
      "Epoch: 22365 mean train loss:  5.30821877e-03, bound:  3.15487981e-01\n",
      "Epoch: 22366 mean train loss:  5.30785974e-03, bound:  3.15487981e-01\n",
      "Epoch: 22367 mean train loss:  5.30750770e-03, bound:  3.15487981e-01\n",
      "Epoch: 22368 mean train loss:  5.30715892e-03, bound:  3.15487951e-01\n",
      "Epoch: 22369 mean train loss:  5.30683761e-03, bound:  3.15487921e-01\n",
      "Epoch: 22370 mean train loss:  5.30650793e-03, bound:  3.15487891e-01\n",
      "Epoch: 22371 mean train loss:  5.30613074e-03, bound:  3.15487862e-01\n",
      "Epoch: 22372 mean train loss:  5.30577544e-03, bound:  3.15487862e-01\n",
      "Epoch: 22373 mean train loss:  5.30534470e-03, bound:  3.15487832e-01\n",
      "Epoch: 22374 mean train loss:  5.30499499e-03, bound:  3.15487802e-01\n",
      "Epoch: 22375 mean train loss:  5.30467881e-03, bound:  3.15487772e-01\n",
      "Epoch: 22376 mean train loss:  5.30429836e-03, bound:  3.15487742e-01\n",
      "Epoch: 22377 mean train loss:  5.30401664e-03, bound:  3.15487742e-01\n",
      "Epoch: 22378 mean train loss:  5.30357333e-03, bound:  3.15487713e-01\n",
      "Epoch: 22379 mean train loss:  5.30323666e-03, bound:  3.15487713e-01\n",
      "Epoch: 22380 mean train loss:  5.30287391e-03, bound:  3.15487653e-01\n",
      "Epoch: 22381 mean train loss:  5.30258054e-03, bound:  3.15487653e-01\n",
      "Epoch: 22382 mean train loss:  5.30221267e-03, bound:  3.15487623e-01\n",
      "Epoch: 22383 mean train loss:  5.30177820e-03, bound:  3.15487623e-01\n",
      "Epoch: 22384 mean train loss:  5.30148763e-03, bound:  3.15487593e-01\n",
      "Epoch: 22385 mean train loss:  5.30108158e-03, bound:  3.15487564e-01\n",
      "Epoch: 22386 mean train loss:  5.30073931e-03, bound:  3.15487564e-01\n",
      "Epoch: 22387 mean train loss:  5.30040730e-03, bound:  3.15487504e-01\n",
      "Epoch: 22388 mean train loss:  5.30005898e-03, bound:  3.15487504e-01\n",
      "Epoch: 22389 mean train loss:  5.29969903e-03, bound:  3.15487474e-01\n",
      "Epoch: 22390 mean train loss:  5.29937446e-03, bound:  3.15487474e-01\n",
      "Epoch: 22391 mean train loss:  5.29903965e-03, bound:  3.15487444e-01\n",
      "Epoch: 22392 mean train loss:  5.29862056e-03, bound:  3.15487444e-01\n",
      "Epoch: 22393 mean train loss:  5.29827084e-03, bound:  3.15487385e-01\n",
      "Epoch: 22394 mean train loss:  5.29792719e-03, bound:  3.15487385e-01\n",
      "Epoch: 22395 mean train loss:  5.29757980e-03, bound:  3.15487355e-01\n",
      "Epoch: 22396 mean train loss:  5.29716071e-03, bound:  3.15487325e-01\n",
      "Epoch: 22397 mean train loss:  5.29680913e-03, bound:  3.15487325e-01\n",
      "Epoch: 22398 mean train loss:  5.29646175e-03, bound:  3.15487295e-01\n",
      "Epoch: 22399 mean train loss:  5.29613299e-03, bound:  3.15487266e-01\n",
      "Epoch: 22400 mean train loss:  5.29574882e-03, bound:  3.15487266e-01\n",
      "Epoch: 22401 mean train loss:  5.29542845e-03, bound:  3.15487266e-01\n",
      "Epoch: 22402 mean train loss:  5.29502751e-03, bound:  3.15487206e-01\n",
      "Epoch: 22403 mean train loss:  5.29469922e-03, bound:  3.15487176e-01\n",
      "Epoch: 22404 mean train loss:  5.29440120e-03, bound:  3.15487176e-01\n",
      "Epoch: 22405 mean train loss:  5.29399840e-03, bound:  3.15487146e-01\n",
      "Epoch: 22406 mean train loss:  5.29360306e-03, bound:  3.15487146e-01\n",
      "Epoch: 22407 mean train loss:  5.29325008e-03, bound:  3.15487087e-01\n",
      "Epoch: 22408 mean train loss:  5.29288128e-03, bound:  3.15487087e-01\n",
      "Epoch: 22409 mean train loss:  5.29253716e-03, bound:  3.15487057e-01\n",
      "Epoch: 22410 mean train loss:  5.29215438e-03, bound:  3.15487027e-01\n",
      "Epoch: 22411 mean train loss:  5.29188570e-03, bound:  3.15487027e-01\n",
      "Epoch: 22412 mean train loss:  5.29147685e-03, bound:  3.15486968e-01\n",
      "Epoch: 22413 mean train loss:  5.29112946e-03, bound:  3.15486968e-01\n",
      "Epoch: 22414 mean train loss:  5.29075507e-03, bound:  3.15486938e-01\n",
      "Epoch: 22415 mean train loss:  5.29044773e-03, bound:  3.15486938e-01\n",
      "Epoch: 22416 mean train loss:  5.29002352e-03, bound:  3.15486908e-01\n",
      "Epoch: 22417 mean train loss:  5.28968172e-03, bound:  3.15486848e-01\n",
      "Epoch: 22418 mean train loss:  5.28935622e-03, bound:  3.15486848e-01\n",
      "Epoch: 22419 mean train loss:  5.28899441e-03, bound:  3.15486819e-01\n",
      "Epoch: 22420 mean train loss:  5.28868660e-03, bound:  3.15486819e-01\n",
      "Epoch: 22421 mean train loss:  5.28838951e-03, bound:  3.15486759e-01\n",
      "Epoch: 22422 mean train loss:  5.28796948e-03, bound:  3.15486759e-01\n",
      "Epoch: 22423 mean train loss:  5.28757041e-03, bound:  3.15486759e-01\n",
      "Epoch: 22424 mean train loss:  5.28720161e-03, bound:  3.15486729e-01\n",
      "Epoch: 22425 mean train loss:  5.28693898e-03, bound:  3.15486729e-01\n",
      "Epoch: 22426 mean train loss:  5.28660696e-03, bound:  3.15486699e-01\n",
      "Epoch: 22427 mean train loss:  5.28626004e-03, bound:  3.15486640e-01\n",
      "Epoch: 22428 mean train loss:  5.28594106e-03, bound:  3.15486640e-01\n",
      "Epoch: 22429 mean train loss:  5.28565887e-03, bound:  3.15486610e-01\n",
      "Epoch: 22430 mean train loss:  5.28541859e-03, bound:  3.15486610e-01\n",
      "Epoch: 22431 mean train loss:  5.28522953e-03, bound:  3.15486521e-01\n",
      "Epoch: 22432 mean train loss:  5.28500788e-03, bound:  3.15486521e-01\n",
      "Epoch: 22433 mean train loss:  5.28494734e-03, bound:  3.15486521e-01\n",
      "Epoch: 22434 mean train loss:  5.28501114e-03, bound:  3.15486521e-01\n",
      "Epoch: 22435 mean train loss:  5.28526260e-03, bound:  3.15486491e-01\n",
      "Epoch: 22436 mean train loss:  5.28578041e-03, bound:  3.15486491e-01\n",
      "Epoch: 22437 mean train loss:  5.28684817e-03, bound:  3.15486401e-01\n",
      "Epoch: 22438 mean train loss:  5.28835785e-03, bound:  3.15486461e-01\n",
      "Epoch: 22439 mean train loss:  5.29061770e-03, bound:  3.15486372e-01\n",
      "Epoch: 22440 mean train loss:  5.29350247e-03, bound:  3.15486401e-01\n",
      "Epoch: 22441 mean train loss:  5.29688457e-03, bound:  3.15486312e-01\n",
      "Epoch: 22442 mean train loss:  5.29999379e-03, bound:  3.15486401e-01\n",
      "Epoch: 22443 mean train loss:  5.30196261e-03, bound:  3.15486252e-01\n",
      "Epoch: 22444 mean train loss:  5.30156400e-03, bound:  3.15486372e-01\n",
      "Epoch: 22445 mean train loss:  5.29793184e-03, bound:  3.15486193e-01\n",
      "Epoch: 22446 mean train loss:  5.29164542e-03, bound:  3.15486282e-01\n",
      "Epoch: 22447 mean train loss:  5.28488774e-03, bound:  3.15486193e-01\n",
      "Epoch: 22448 mean train loss:  5.27997455e-03, bound:  3.15486252e-01\n",
      "Epoch: 22449 mean train loss:  5.27841225e-03, bound:  3.15486193e-01\n",
      "Epoch: 22450 mean train loss:  5.27979853e-03, bound:  3.15486163e-01\n",
      "Epoch: 22451 mean train loss:  5.28260646e-03, bound:  3.15486163e-01\n",
      "Epoch: 22452 mean train loss:  5.28492825e-03, bound:  3.15486073e-01\n",
      "Epoch: 22453 mean train loss:  5.28517924e-03, bound:  3.15486133e-01\n",
      "Epoch: 22454 mean train loss:  5.28308144e-03, bound:  3.15486044e-01\n",
      "Epoch: 22455 mean train loss:  5.27972076e-03, bound:  3.15486073e-01\n",
      "Epoch: 22456 mean train loss:  5.27685974e-03, bound:  3.15486044e-01\n",
      "Epoch: 22457 mean train loss:  5.27560245e-03, bound:  3.15486044e-01\n",
      "Epoch: 22458 mean train loss:  5.27597871e-03, bound:  3.15486044e-01\n",
      "Epoch: 22459 mean train loss:  5.27722808e-03, bound:  3.15485954e-01\n",
      "Epoch: 22460 mean train loss:  5.27815986e-03, bound:  3.15485954e-01\n",
      "Epoch: 22461 mean train loss:  5.27797779e-03, bound:  3.15485924e-01\n",
      "Epoch: 22462 mean train loss:  5.27672656e-03, bound:  3.15485924e-01\n",
      "Epoch: 22463 mean train loss:  5.27490955e-03, bound:  3.15485924e-01\n",
      "Epoch: 22464 mean train loss:  5.27354656e-03, bound:  3.15485924e-01\n",
      "Epoch: 22465 mean train loss:  5.27286250e-03, bound:  3.15485865e-01\n",
      "Epoch: 22466 mean train loss:  5.27292630e-03, bound:  3.15485835e-01\n",
      "Epoch: 22467 mean train loss:  5.27330767e-03, bound:  3.15485835e-01\n",
      "Epoch: 22468 mean train loss:  5.27348369e-03, bound:  3.15485805e-01\n",
      "Epoch: 22469 mean train loss:  5.27305156e-03, bound:  3.15485805e-01\n",
      "Epoch: 22470 mean train loss:  5.27224690e-03, bound:  3.15485746e-01\n",
      "Epoch: 22471 mean train loss:  5.27130347e-03, bound:  3.15485746e-01\n",
      "Epoch: 22472 mean train loss:  5.27054491e-03, bound:  3.15485716e-01\n",
      "Epoch: 22473 mean train loss:  5.27013885e-03, bound:  3.15485716e-01\n",
      "Epoch: 22474 mean train loss:  5.27001871e-03, bound:  3.15485686e-01\n",
      "Epoch: 22475 mean train loss:  5.26997540e-03, bound:  3.15485626e-01\n",
      "Epoch: 22476 mean train loss:  5.26982220e-03, bound:  3.15485626e-01\n",
      "Epoch: 22477 mean train loss:  5.26942499e-03, bound:  3.15485597e-01\n",
      "Epoch: 22478 mean train loss:  5.26887784e-03, bound:  3.15485597e-01\n",
      "Epoch: 22479 mean train loss:  5.26832137e-03, bound:  3.15485537e-01\n",
      "Epoch: 22480 mean train loss:  5.26777096e-03, bound:  3.15485537e-01\n",
      "Epoch: 22481 mean train loss:  5.26739843e-03, bound:  3.15485507e-01\n",
      "Epoch: 22482 mean train loss:  5.26718143e-03, bound:  3.15485507e-01\n",
      "Epoch: 22483 mean train loss:  5.26691182e-03, bound:  3.15485507e-01\n",
      "Epoch: 22484 mean train loss:  5.26660541e-03, bound:  3.15485477e-01\n",
      "Epoch: 22485 mean train loss:  5.26629062e-03, bound:  3.15485418e-01\n",
      "Epoch: 22486 mean train loss:  5.26587293e-03, bound:  3.15485388e-01\n",
      "Epoch: 22487 mean train loss:  5.26547339e-03, bound:  3.15485388e-01\n",
      "Epoch: 22488 mean train loss:  5.26503148e-03, bound:  3.15485388e-01\n",
      "Epoch: 22489 mean train loss:  5.26465476e-03, bound:  3.15485358e-01\n",
      "Epoch: 22490 mean train loss:  5.26428130e-03, bound:  3.15485358e-01\n",
      "Epoch: 22491 mean train loss:  5.26402518e-03, bound:  3.15485358e-01\n",
      "Epoch: 22492 mean train loss:  5.26374346e-03, bound:  3.15485269e-01\n",
      "Epoch: 22493 mean train loss:  5.26345335e-03, bound:  3.15485269e-01\n",
      "Epoch: 22494 mean train loss:  5.26307011e-03, bound:  3.15485239e-01\n",
      "Epoch: 22495 mean train loss:  5.26269013e-03, bound:  3.15485239e-01\n",
      "Epoch: 22496 mean train loss:  5.26231853e-03, bound:  3.15485239e-01\n",
      "Epoch: 22497 mean train loss:  5.26194321e-03, bound:  3.15485150e-01\n",
      "Epoch: 22498 mean train loss:  5.26157906e-03, bound:  3.15485150e-01\n",
      "Epoch: 22499 mean train loss:  5.26122097e-03, bound:  3.15485120e-01\n",
      "Epoch: 22500 mean train loss:  5.26095275e-03, bound:  3.15485120e-01\n",
      "Epoch: 22501 mean train loss:  5.26059559e-03, bound:  3.15485120e-01\n",
      "Epoch: 22502 mean train loss:  5.26029756e-03, bound:  3.15485090e-01\n",
      "Epoch: 22503 mean train loss:  5.25993854e-03, bound:  3.15485060e-01\n",
      "Epoch: 22504 mean train loss:  5.25954133e-03, bound:  3.15485060e-01\n",
      "Epoch: 22505 mean train loss:  5.25920326e-03, bound:  3.15485030e-01\n",
      "Epoch: 22506 mean train loss:  5.25885681e-03, bound:  3.15485030e-01\n",
      "Epoch: 22507 mean train loss:  5.25850197e-03, bound:  3.15484971e-01\n",
      "Epoch: 22508 mean train loss:  5.25817694e-03, bound:  3.15484941e-01\n",
      "Epoch: 22509 mean train loss:  5.25782583e-03, bound:  3.15484941e-01\n",
      "Epoch: 22510 mean train loss:  5.25747845e-03, bound:  3.15484911e-01\n",
      "Epoch: 22511 mean train loss:  5.25717763e-03, bound:  3.15484911e-01\n",
      "Epoch: 22512 mean train loss:  5.25682280e-03, bound:  3.15484852e-01\n",
      "Epoch: 22513 mean train loss:  5.25645865e-03, bound:  3.15484852e-01\n",
      "Epoch: 22514 mean train loss:  5.25616854e-03, bound:  3.15484822e-01\n",
      "Epoch: 22515 mean train loss:  5.25576482e-03, bound:  3.15484822e-01\n",
      "Epoch: 22516 mean train loss:  5.25541464e-03, bound:  3.15484792e-01\n",
      "Epoch: 22517 mean train loss:  5.25505096e-03, bound:  3.15484792e-01\n",
      "Epoch: 22518 mean train loss:  5.25468541e-03, bound:  3.15484732e-01\n",
      "Epoch: 22519 mean train loss:  5.25436364e-03, bound:  3.15484703e-01\n",
      "Epoch: 22520 mean train loss:  5.25404559e-03, bound:  3.15484703e-01\n",
      "Epoch: 22521 mean train loss:  5.25374012e-03, bound:  3.15484703e-01\n",
      "Epoch: 22522 mean train loss:  5.25335129e-03, bound:  3.15484673e-01\n",
      "Epoch: 22523 mean train loss:  5.25301509e-03, bound:  3.15484613e-01\n",
      "Epoch: 22524 mean train loss:  5.25263464e-03, bound:  3.15484613e-01\n",
      "Epoch: 22525 mean train loss:  5.25230821e-03, bound:  3.15484583e-01\n",
      "Epoch: 22526 mean train loss:  5.25200786e-03, bound:  3.15484583e-01\n",
      "Epoch: 22527 mean train loss:  5.25159901e-03, bound:  3.15484554e-01\n",
      "Epoch: 22528 mean train loss:  5.25123207e-03, bound:  3.15484524e-01\n",
      "Epoch: 22529 mean train loss:  5.25088934e-03, bound:  3.15484524e-01\n",
      "Epoch: 22530 mean train loss:  5.25061646e-03, bound:  3.15484494e-01\n",
      "Epoch: 22531 mean train loss:  5.25027886e-03, bound:  3.15484494e-01\n",
      "Epoch: 22532 mean train loss:  5.24993381e-03, bound:  3.15484434e-01\n",
      "Epoch: 22533 mean train loss:  5.24960691e-03, bound:  3.15484434e-01\n",
      "Epoch: 22534 mean train loss:  5.24924509e-03, bound:  3.15484405e-01\n",
      "Epoch: 22535 mean train loss:  5.24884928e-03, bound:  3.15484375e-01\n",
      "Epoch: 22536 mean train loss:  5.24859363e-03, bound:  3.15484375e-01\n",
      "Epoch: 22537 mean train loss:  5.24820294e-03, bound:  3.15484345e-01\n",
      "Epoch: 22538 mean train loss:  5.24793938e-03, bound:  3.15484285e-01\n",
      "Epoch: 22539 mean train loss:  5.24749188e-03, bound:  3.15484285e-01\n",
      "Epoch: 22540 mean train loss:  5.24717104e-03, bound:  3.15484285e-01\n",
      "Epoch: 22541 mean train loss:  5.24677709e-03, bound:  3.15484256e-01\n",
      "Epoch: 22542 mean train loss:  5.24642505e-03, bound:  3.15484256e-01\n",
      "Epoch: 22543 mean train loss:  5.24609350e-03, bound:  3.15484226e-01\n",
      "Epoch: 22544 mean train loss:  5.24577266e-03, bound:  3.15484196e-01\n",
      "Epoch: 22545 mean train loss:  5.24542155e-03, bound:  3.15484166e-01\n",
      "Epoch: 22546 mean train loss:  5.24505414e-03, bound:  3.15484136e-01\n",
      "Epoch: 22547 mean train loss:  5.24471374e-03, bound:  3.15484136e-01\n",
      "Epoch: 22548 mean train loss:  5.24440641e-03, bound:  3.15484107e-01\n",
      "Epoch: 22549 mean train loss:  5.24400594e-03, bound:  3.15484107e-01\n",
      "Epoch: 22550 mean train loss:  5.24369953e-03, bound:  3.15484047e-01\n",
      "Epoch: 22551 mean train loss:  5.24327159e-03, bound:  3.15484047e-01\n",
      "Epoch: 22552 mean train loss:  5.24298800e-03, bound:  3.15484017e-01\n",
      "Epoch: 22553 mean train loss:  5.24257729e-03, bound:  3.15484017e-01\n",
      "Epoch: 22554 mean train loss:  5.24231885e-03, bound:  3.15483987e-01\n",
      "Epoch: 22555 mean train loss:  5.24189835e-03, bound:  3.15483958e-01\n",
      "Epoch: 22556 mean train loss:  5.24156122e-03, bound:  3.15483928e-01\n",
      "Epoch: 22557 mean train loss:  5.24122640e-03, bound:  3.15483928e-01\n",
      "Epoch: 22558 mean train loss:  5.24091674e-03, bound:  3.15483898e-01\n",
      "Epoch: 22559 mean train loss:  5.24058193e-03, bound:  3.15483838e-01\n",
      "Epoch: 22560 mean train loss:  5.24021639e-03, bound:  3.15483838e-01\n",
      "Epoch: 22561 mean train loss:  5.23983734e-03, bound:  3.15483809e-01\n",
      "Epoch: 22562 mean train loss:  5.23944991e-03, bound:  3.15483809e-01\n",
      "Epoch: 22563 mean train loss:  5.23917703e-03, bound:  3.15483809e-01\n",
      "Epoch: 22564 mean train loss:  5.23884268e-03, bound:  3.15483779e-01\n",
      "Epoch: 22565 mean train loss:  5.23843989e-03, bound:  3.15483719e-01\n",
      "Epoch: 22566 mean train loss:  5.23809809e-03, bound:  3.15483719e-01\n",
      "Epoch: 22567 mean train loss:  5.23771858e-03, bound:  3.15483689e-01\n",
      "Epoch: 22568 mean train loss:  5.23735350e-03, bound:  3.15483689e-01\n",
      "Epoch: 22569 mean train loss:  5.23706432e-03, bound:  3.15483689e-01\n",
      "Epoch: 22570 mean train loss:  5.23675373e-03, bound:  3.15483600e-01\n",
      "Epoch: 22571 mean train loss:  5.23631042e-03, bound:  3.15483600e-01\n",
      "Epoch: 22572 mean train loss:  5.23596257e-03, bound:  3.15483600e-01\n",
      "Epoch: 22573 mean train loss:  5.23564266e-03, bound:  3.15483570e-01\n",
      "Epoch: 22574 mean train loss:  5.23531251e-03, bound:  3.15483570e-01\n",
      "Epoch: 22575 mean train loss:  5.23499958e-03, bound:  3.15483510e-01\n",
      "Epoch: 22576 mean train loss:  5.23461588e-03, bound:  3.15483510e-01\n",
      "Epoch: 22577 mean train loss:  5.23430528e-03, bound:  3.15483481e-01\n",
      "Epoch: 22578 mean train loss:  5.23394672e-03, bound:  3.15483481e-01\n",
      "Epoch: 22579 mean train loss:  5.23363706e-03, bound:  3.15483451e-01\n",
      "Epoch: 22580 mean train loss:  5.23332693e-03, bound:  3.15483451e-01\n",
      "Epoch: 22581 mean train loss:  5.23296837e-03, bound:  3.15483391e-01\n",
      "Epoch: 22582 mean train loss:  5.23262843e-03, bound:  3.15483361e-01\n",
      "Epoch: 22583 mean train loss:  5.23231411e-03, bound:  3.15483361e-01\n",
      "Epoch: 22584 mean train loss:  5.23205334e-03, bound:  3.15483332e-01\n",
      "Epoch: 22585 mean train loss:  5.23177208e-03, bound:  3.15483272e-01\n",
      "Epoch: 22586 mean train loss:  5.23146428e-03, bound:  3.15483272e-01\n",
      "Epoch: 22587 mean train loss:  5.23121003e-03, bound:  3.15483272e-01\n",
      "Epoch: 22588 mean train loss:  5.23110991e-03, bound:  3.15483242e-01\n",
      "Epoch: 22589 mean train loss:  5.23100747e-03, bound:  3.15483212e-01\n",
      "Epoch: 22590 mean train loss:  5.23101911e-03, bound:  3.15483212e-01\n",
      "Epoch: 22591 mean train loss:  5.23121189e-03, bound:  3.15483153e-01\n",
      "Epoch: 22592 mean train loss:  5.23163751e-03, bound:  3.15483153e-01\n",
      "Epoch: 22593 mean train loss:  5.23248129e-03, bound:  3.15483123e-01\n",
      "Epoch: 22594 mean train loss:  5.23392390e-03, bound:  3.15483123e-01\n",
      "Epoch: 22595 mean train loss:  5.23602916e-03, bound:  3.15483034e-01\n",
      "Epoch: 22596 mean train loss:  5.23907552e-03, bound:  3.15483123e-01\n",
      "Epoch: 22597 mean train loss:  5.24313189e-03, bound:  3.15483004e-01\n",
      "Epoch: 22598 mean train loss:  5.24779363e-03, bound:  3.15483063e-01\n",
      "Epoch: 22599 mean train loss:  5.25210425e-03, bound:  3.15482944e-01\n",
      "Epoch: 22600 mean train loss:  5.25422301e-03, bound:  3.15483034e-01\n",
      "Epoch: 22601 mean train loss:  5.25202649e-03, bound:  3.15482914e-01\n",
      "Epoch: 22602 mean train loss:  5.24504110e-03, bound:  3.15483004e-01\n",
      "Epoch: 22603 mean train loss:  5.23547502e-03, bound:  3.15482885e-01\n",
      "Epoch: 22604 mean train loss:  5.22759184e-03, bound:  3.15482944e-01\n",
      "Epoch: 22605 mean train loss:  5.22455294e-03, bound:  3.15482885e-01\n",
      "Epoch: 22606 mean train loss:  5.22658136e-03, bound:  3.15482885e-01\n",
      "Epoch: 22607 mean train loss:  5.23083424e-03, bound:  3.15482885e-01\n",
      "Epoch: 22608 mean train loss:  5.23371948e-03, bound:  3.15482795e-01\n",
      "Epoch: 22609 mean train loss:  5.23317559e-03, bound:  3.15482885e-01\n",
      "Epoch: 22610 mean train loss:  5.22928778e-03, bound:  3.15482795e-01\n",
      "Epoch: 22611 mean train loss:  5.22488216e-03, bound:  3.15482795e-01\n",
      "Epoch: 22612 mean train loss:  5.22231031e-03, bound:  3.15482765e-01\n",
      "Epoch: 22613 mean train loss:  5.22258319e-03, bound:  3.15482765e-01\n",
      "Epoch: 22614 mean train loss:  5.22452081e-03, bound:  3.15482765e-01\n",
      "Epoch: 22615 mean train loss:  5.22594200e-03, bound:  3.15482676e-01\n",
      "Epoch: 22616 mean train loss:  5.22564072e-03, bound:  3.15482706e-01\n",
      "Epoch: 22617 mean train loss:  5.22367470e-03, bound:  3.15482616e-01\n",
      "Epoch: 22618 mean train loss:  5.22135897e-03, bound:  3.15482676e-01\n",
      "Epoch: 22619 mean train loss:  5.21999924e-03, bound:  3.15482616e-01\n",
      "Epoch: 22620 mean train loss:  5.21996431e-03, bound:  3.15482587e-01\n",
      "Epoch: 22621 mean train loss:  5.22071728e-03, bound:  3.15482587e-01\n",
      "Epoch: 22622 mean train loss:  5.22120437e-03, bound:  3.15482557e-01\n",
      "Epoch: 22623 mean train loss:  5.22089051e-03, bound:  3.15482557e-01\n",
      "Epoch: 22624 mean train loss:  5.21971518e-03, bound:  3.15482497e-01\n",
      "Epoch: 22625 mean train loss:  5.21845417e-03, bound:  3.15482497e-01\n",
      "Epoch: 22626 mean train loss:  5.21763274e-03, bound:  3.15482467e-01\n",
      "Epoch: 22627 mean train loss:  5.21755544e-03, bound:  3.15482438e-01\n",
      "Epoch: 22628 mean train loss:  5.21773566e-03, bound:  3.15482438e-01\n",
      "Epoch: 22629 mean train loss:  5.21773472e-03, bound:  3.15482378e-01\n",
      "Epoch: 22630 mean train loss:  5.21749444e-03, bound:  3.15482378e-01\n",
      "Epoch: 22631 mean train loss:  5.21669770e-03, bound:  3.15482348e-01\n",
      "Epoch: 22632 mean train loss:  5.21597126e-03, bound:  3.15482348e-01\n",
      "Epoch: 22633 mean train loss:  5.21538127e-03, bound:  3.15482318e-01\n",
      "Epoch: 22634 mean train loss:  5.21510467e-03, bound:  3.15482318e-01\n",
      "Epoch: 22635 mean train loss:  5.21495333e-03, bound:  3.15482259e-01\n",
      "Epoch: 22636 mean train loss:  5.21489419e-03, bound:  3.15482259e-01\n",
      "Epoch: 22637 mean train loss:  5.21462085e-03, bound:  3.15482259e-01\n",
      "Epoch: 22638 mean train loss:  5.21415845e-03, bound:  3.15482199e-01\n",
      "Epoch: 22639 mean train loss:  5.21359546e-03, bound:  3.15482199e-01\n",
      "Epoch: 22640 mean train loss:  5.21308137e-03, bound:  3.15482169e-01\n",
      "Epoch: 22641 mean train loss:  5.21272281e-03, bound:  3.15482169e-01\n",
      "Epoch: 22642 mean train loss:  5.21250069e-03, bound:  3.15482140e-01\n",
      "Epoch: 22643 mean train loss:  5.21225994e-03, bound:  3.15482140e-01\n",
      "Epoch: 22644 mean train loss:  5.21198707e-03, bound:  3.15482110e-01\n",
      "Epoch: 22645 mean train loss:  5.21165272e-03, bound:  3.15482080e-01\n",
      "Epoch: 22646 mean train loss:  5.21123689e-03, bound:  3.15482050e-01\n",
      "Epoch: 22647 mean train loss:  5.21078939e-03, bound:  3.15482020e-01\n",
      "Epoch: 22648 mean train loss:  5.21035492e-03, bound:  3.15482020e-01\n",
      "Epoch: 22649 mean train loss:  5.21008903e-03, bound:  3.15482020e-01\n",
      "Epoch: 22650 mean train loss:  5.20985806e-03, bound:  3.15481991e-01\n",
      "Epoch: 22651 mean train loss:  5.20953909e-03, bound:  3.15481991e-01\n",
      "Epoch: 22652 mean train loss:  5.20931091e-03, bound:  3.15481931e-01\n",
      "Epoch: 22653 mean train loss:  5.20890625e-03, bound:  3.15481901e-01\n",
      "Epoch: 22654 mean train loss:  5.20849694e-03, bound:  3.15481901e-01\n",
      "Epoch: 22655 mean train loss:  5.20809693e-03, bound:  3.15481871e-01\n",
      "Epoch: 22656 mean train loss:  5.20776026e-03, bound:  3.15481871e-01\n",
      "Epoch: 22657 mean train loss:  5.20741614e-03, bound:  3.15481812e-01\n",
      "Epoch: 22658 mean train loss:  5.20711485e-03, bound:  3.15481812e-01\n",
      "Epoch: 22659 mean train loss:  5.20682335e-03, bound:  3.15481782e-01\n",
      "Epoch: 22660 mean train loss:  5.20653976e-03, bound:  3.15481782e-01\n",
      "Epoch: 22661 mean train loss:  5.20613510e-03, bound:  3.15481752e-01\n",
      "Epoch: 22662 mean train loss:  5.20577189e-03, bound:  3.15481752e-01\n",
      "Epoch: 22663 mean train loss:  5.20542543e-03, bound:  3.15481693e-01\n",
      "Epoch: 22664 mean train loss:  5.20512648e-03, bound:  3.15481693e-01\n",
      "Epoch: 22665 mean train loss:  5.20482752e-03, bound:  3.15481693e-01\n",
      "Epoch: 22666 mean train loss:  5.20454440e-03, bound:  3.15481663e-01\n",
      "Epoch: 22667 mean train loss:  5.20421052e-03, bound:  3.15481633e-01\n",
      "Epoch: 22668 mean train loss:  5.20380028e-03, bound:  3.15481573e-01\n",
      "Epoch: 22669 mean train loss:  5.20347897e-03, bound:  3.15481573e-01\n",
      "Epoch: 22670 mean train loss:  5.20314183e-03, bound:  3.15481573e-01\n",
      "Epoch: 22671 mean train loss:  5.20276278e-03, bound:  3.15481573e-01\n",
      "Epoch: 22672 mean train loss:  5.20248339e-03, bound:  3.15481514e-01\n",
      "Epoch: 22673 mean train loss:  5.20214997e-03, bound:  3.15481484e-01\n",
      "Epoch: 22674 mean train loss:  5.20178769e-03, bound:  3.15481454e-01\n",
      "Epoch: 22675 mean train loss:  5.20146918e-03, bound:  3.15481454e-01\n",
      "Epoch: 22676 mean train loss:  5.20114740e-03, bound:  3.15481454e-01\n",
      "Epoch: 22677 mean train loss:  5.20079443e-03, bound:  3.15481424e-01\n",
      "Epoch: 22678 mean train loss:  5.20051131e-03, bound:  3.15481395e-01\n",
      "Epoch: 22679 mean train loss:  5.20014996e-03, bound:  3.15481365e-01\n",
      "Epoch: 22680 mean train loss:  5.19980164e-03, bound:  3.15481365e-01\n",
      "Epoch: 22681 mean train loss:  5.19945566e-03, bound:  3.15481365e-01\n",
      "Epoch: 22682 mean train loss:  5.19917905e-03, bound:  3.15481335e-01\n",
      "Epoch: 22683 mean train loss:  5.19877067e-03, bound:  3.15481305e-01\n",
      "Epoch: 22684 mean train loss:  5.19852759e-03, bound:  3.15481275e-01\n",
      "Epoch: 22685 mean train loss:  5.19820210e-03, bound:  3.15481275e-01\n",
      "Epoch: 22686 mean train loss:  5.19779325e-03, bound:  3.15481246e-01\n",
      "Epoch: 22687 mean train loss:  5.19751897e-03, bound:  3.15481216e-01\n",
      "Epoch: 22688 mean train loss:  5.19717764e-03, bound:  3.15481216e-01\n",
      "Epoch: 22689 mean train loss:  5.19686146e-03, bound:  3.15481156e-01\n",
      "Epoch: 22690 mean train loss:  5.19652618e-03, bound:  3.15481156e-01\n",
      "Epoch: 22691 mean train loss:  5.19618392e-03, bound:  3.15481126e-01\n",
      "Epoch: 22692 mean train loss:  5.19579183e-03, bound:  3.15481097e-01\n",
      "Epoch: 22693 mean train loss:  5.19553572e-03, bound:  3.15481097e-01\n",
      "Epoch: 22694 mean train loss:  5.19516040e-03, bound:  3.15481067e-01\n",
      "Epoch: 22695 mean train loss:  5.19479485e-03, bound:  3.15481067e-01\n",
      "Epoch: 22696 mean train loss:  5.19450661e-03, bound:  3.15481037e-01\n",
      "Epoch: 22697 mean train loss:  5.19418577e-03, bound:  3.15481007e-01\n",
      "Epoch: 22698 mean train loss:  5.19380579e-03, bound:  3.15481007e-01\n",
      "Epoch: 22699 mean train loss:  5.19346679e-03, bound:  3.15480977e-01\n",
      "Epoch: 22700 mean train loss:  5.19321067e-03, bound:  3.15480918e-01\n",
      "Epoch: 22701 mean train loss:  5.19278552e-03, bound:  3.15480918e-01\n",
      "Epoch: 22702 mean train loss:  5.19252894e-03, bound:  3.15480888e-01\n",
      "Epoch: 22703 mean train loss:  5.19216200e-03, bound:  3.15480888e-01\n",
      "Epoch: 22704 mean train loss:  5.19179692e-03, bound:  3.15480858e-01\n",
      "Epoch: 22705 mean train loss:  5.19146770e-03, bound:  3.15480858e-01\n",
      "Epoch: 22706 mean train loss:  5.19109750e-03, bound:  3.15480798e-01\n",
      "Epoch: 22707 mean train loss:  5.19078271e-03, bound:  3.15480798e-01\n",
      "Epoch: 22708 mean train loss:  5.19045489e-03, bound:  3.15480769e-01\n",
      "Epoch: 22709 mean train loss:  5.19011449e-03, bound:  3.15480769e-01\n",
      "Epoch: 22710 mean train loss:  5.18979272e-03, bound:  3.15480769e-01\n",
      "Epoch: 22711 mean train loss:  5.18949889e-03, bound:  3.15480709e-01\n",
      "Epoch: 22712 mean train loss:  5.18907281e-03, bound:  3.15480709e-01\n",
      "Epoch: 22713 mean train loss:  5.18876687e-03, bound:  3.15480649e-01\n",
      "Epoch: 22714 mean train loss:  5.18847443e-03, bound:  3.15480649e-01\n",
      "Epoch: 22715 mean train loss:  5.18812006e-03, bound:  3.15480649e-01\n",
      "Epoch: 22716 mean train loss:  5.18783322e-03, bound:  3.15480620e-01\n",
      "Epoch: 22717 mean train loss:  5.18749328e-03, bound:  3.15480590e-01\n",
      "Epoch: 22718 mean train loss:  5.18716406e-03, bound:  3.15480560e-01\n",
      "Epoch: 22719 mean train loss:  5.18685626e-03, bound:  3.15480560e-01\n",
      "Epoch: 22720 mean train loss:  5.18651446e-03, bound:  3.15480530e-01\n",
      "Epoch: 22721 mean train loss:  5.18615963e-03, bound:  3.15480471e-01\n",
      "Epoch: 22722 mean train loss:  5.18580945e-03, bound:  3.15480471e-01\n",
      "Epoch: 22723 mean train loss:  5.18545695e-03, bound:  3.15480441e-01\n",
      "Epoch: 22724 mean train loss:  5.18512586e-03, bound:  3.15480441e-01\n",
      "Epoch: 22725 mean train loss:  5.18478639e-03, bound:  3.15480441e-01\n",
      "Epoch: 22726 mean train loss:  5.18445345e-03, bound:  3.15480411e-01\n",
      "Epoch: 22727 mean train loss:  5.18405018e-03, bound:  3.15480351e-01\n",
      "Epoch: 22728 mean train loss:  5.18377405e-03, bound:  3.15480351e-01\n",
      "Epoch: 22729 mean train loss:  5.18349418e-03, bound:  3.15480322e-01\n",
      "Epoch: 22730 mean train loss:  5.18313283e-03, bound:  3.15480322e-01\n",
      "Epoch: 22731 mean train loss:  5.18278126e-03, bound:  3.15480322e-01\n",
      "Epoch: 22732 mean train loss:  5.18249208e-03, bound:  3.15480292e-01\n",
      "Epoch: 22733 mean train loss:  5.18217823e-03, bound:  3.15480232e-01\n",
      "Epoch: 22734 mean train loss:  5.18179592e-03, bound:  3.15480232e-01\n",
      "Epoch: 22735 mean train loss:  5.18152025e-03, bound:  3.15480232e-01\n",
      "Epoch: 22736 mean train loss:  5.18117659e-03, bound:  3.15480202e-01\n",
      "Epoch: 22737 mean train loss:  5.18089067e-03, bound:  3.15480173e-01\n",
      "Epoch: 22738 mean train loss:  5.18052978e-03, bound:  3.15480143e-01\n",
      "Epoch: 22739 mean train loss:  5.18020010e-03, bound:  3.15480143e-01\n",
      "Epoch: 22740 mean train loss:  5.17983967e-03, bound:  3.15480083e-01\n",
      "Epoch: 22741 mean train loss:  5.17959055e-03, bound:  3.15480083e-01\n",
      "Epoch: 22742 mean train loss:  5.17920777e-03, bound:  3.15480024e-01\n",
      "Epoch: 22743 mean train loss:  5.17896796e-03, bound:  3.15480024e-01\n",
      "Epoch: 22744 mean train loss:  5.17858844e-03, bound:  3.15480024e-01\n",
      "Epoch: 22745 mean train loss:  5.17823314e-03, bound:  3.15479994e-01\n",
      "Epoch: 22746 mean train loss:  5.17798401e-03, bound:  3.15479964e-01\n",
      "Epoch: 22747 mean train loss:  5.17762359e-03, bound:  3.15479964e-01\n",
      "Epoch: 22748 mean train loss:  5.17730787e-03, bound:  3.15479964e-01\n",
      "Epoch: 22749 mean train loss:  5.17704105e-03, bound:  3.15479904e-01\n",
      "Epoch: 22750 mean train loss:  5.17670810e-03, bound:  3.15479875e-01\n",
      "Epoch: 22751 mean train loss:  5.17640822e-03, bound:  3.15479875e-01\n",
      "Epoch: 22752 mean train loss:  5.17622754e-03, bound:  3.15479845e-01\n",
      "Epoch: 22753 mean train loss:  5.17598167e-03, bound:  3.15479845e-01\n",
      "Epoch: 22754 mean train loss:  5.17583964e-03, bound:  3.15479785e-01\n",
      "Epoch: 22755 mean train loss:  5.17574046e-03, bound:  3.15479785e-01\n",
      "Epoch: 22756 mean train loss:  5.17564593e-03, bound:  3.15479755e-01\n",
      "Epoch: 22757 mean train loss:  5.17568085e-03, bound:  3.15479755e-01\n",
      "Epoch: 22758 mean train loss:  5.17582847e-03, bound:  3.15479696e-01\n",
      "Epoch: 22759 mean train loss:  5.17622009e-03, bound:  3.15479755e-01\n",
      "Epoch: 22760 mean train loss:  5.17676026e-03, bound:  3.15479666e-01\n",
      "Epoch: 22761 mean train loss:  5.17745782e-03, bound:  3.15479666e-01\n",
      "Epoch: 22762 mean train loss:  5.17831836e-03, bound:  3.15479636e-01\n",
      "Epoch: 22763 mean train loss:  5.17928321e-03, bound:  3.15479636e-01\n",
      "Epoch: 22764 mean train loss:  5.18037379e-03, bound:  3.15479547e-01\n",
      "Epoch: 22765 mean train loss:  5.18143503e-03, bound:  3.15479577e-01\n",
      "Epoch: 22766 mean train loss:  5.18211443e-03, bound:  3.15479517e-01\n",
      "Epoch: 22767 mean train loss:  5.18211117e-03, bound:  3.15479547e-01\n",
      "Epoch: 22768 mean train loss:  5.18119475e-03, bound:  3.15479457e-01\n",
      "Epoch: 22769 mean train loss:  5.17932512e-03, bound:  3.15479517e-01\n",
      "Epoch: 22770 mean train loss:  5.17663872e-03, bound:  3.15479457e-01\n",
      "Epoch: 22771 mean train loss:  5.17361145e-03, bound:  3.15479457e-01\n",
      "Epoch: 22772 mean train loss:  5.17092459e-03, bound:  3.15479457e-01\n",
      "Epoch: 22773 mean train loss:  5.16903214e-03, bound:  3.15479428e-01\n",
      "Epoch: 22774 mean train loss:  5.16826799e-03, bound:  3.15479398e-01\n",
      "Epoch: 22775 mean train loss:  5.16844727e-03, bound:  3.15479398e-01\n",
      "Epoch: 22776 mean train loss:  5.16903866e-03, bound:  3.15479398e-01\n",
      "Epoch: 22777 mean train loss:  5.16986428e-03, bound:  3.15479338e-01\n",
      "Epoch: 22778 mean train loss:  5.17028105e-03, bound:  3.15479338e-01\n",
      "Epoch: 22779 mean train loss:  5.17031597e-03, bound:  3.15479279e-01\n",
      "Epoch: 22780 mean train loss:  5.16978884e-03, bound:  3.15479279e-01\n",
      "Epoch: 22781 mean train loss:  5.16865263e-03, bound:  3.15479219e-01\n",
      "Epoch: 22782 mean train loss:  5.16743632e-03, bound:  3.15479219e-01\n",
      "Epoch: 22783 mean train loss:  5.16623119e-03, bound:  3.15479219e-01\n",
      "Epoch: 22784 mean train loss:  5.16522350e-03, bound:  3.15479219e-01\n",
      "Epoch: 22785 mean train loss:  5.16474759e-03, bound:  3.15479189e-01\n",
      "Epoch: 22786 mean train loss:  5.16448915e-03, bound:  3.15479130e-01\n",
      "Epoch: 22787 mean train loss:  5.16451290e-03, bound:  3.15479130e-01\n",
      "Epoch: 22788 mean train loss:  5.16463257e-03, bound:  3.15479100e-01\n",
      "Epoch: 22789 mean train loss:  5.16461860e-03, bound:  3.15479100e-01\n",
      "Epoch: 22790 mean train loss:  5.16438670e-03, bound:  3.15479070e-01\n",
      "Epoch: 22791 mean train loss:  5.16399601e-03, bound:  3.15479070e-01\n",
      "Epoch: 22792 mean train loss:  5.16347541e-03, bound:  3.15479010e-01\n",
      "Epoch: 22793 mean train loss:  5.16278343e-03, bound:  3.15479010e-01\n",
      "Epoch: 22794 mean train loss:  5.16211661e-03, bound:  3.15478981e-01\n",
      "Epoch: 22795 mean train loss:  5.16154990e-03, bound:  3.15478981e-01\n",
      "Epoch: 22796 mean train loss:  5.16117970e-03, bound:  3.15478951e-01\n",
      "Epoch: 22797 mean train loss:  5.16075548e-03, bound:  3.15478951e-01\n",
      "Epoch: 22798 mean train loss:  5.16052311e-03, bound:  3.15478891e-01\n",
      "Epoch: 22799 mean train loss:  5.16036712e-03, bound:  3.15478891e-01\n",
      "Epoch: 22800 mean train loss:  5.16009564e-03, bound:  3.15478861e-01\n",
      "Epoch: 22801 mean train loss:  5.15987398e-03, bound:  3.15478832e-01\n",
      "Epoch: 22802 mean train loss:  5.15950983e-03, bound:  3.15478832e-01\n",
      "Epoch: 22803 mean train loss:  5.15915267e-03, bound:  3.15478772e-01\n",
      "Epoch: 22804 mean train loss:  5.15868468e-03, bound:  3.15478772e-01\n",
      "Epoch: 22805 mean train loss:  5.15829446e-03, bound:  3.15478742e-01\n",
      "Epoch: 22806 mean train loss:  5.15786838e-03, bound:  3.15478742e-01\n",
      "Epoch: 22807 mean train loss:  5.15751634e-03, bound:  3.15478712e-01\n",
      "Epoch: 22808 mean train loss:  5.15715173e-03, bound:  3.15478683e-01\n",
      "Epoch: 22809 mean train loss:  5.15681040e-03, bound:  3.15478653e-01\n",
      "Epoch: 22810 mean train loss:  5.15653519e-03, bound:  3.15478653e-01\n",
      "Epoch: 22811 mean train loss:  5.15616033e-03, bound:  3.15478623e-01\n",
      "Epoch: 22812 mean train loss:  5.15591074e-03, bound:  3.15478623e-01\n",
      "Epoch: 22813 mean train loss:  5.15559176e-03, bound:  3.15478593e-01\n",
      "Epoch: 22814 mean train loss:  5.15526393e-03, bound:  3.15478534e-01\n",
      "Epoch: 22815 mean train loss:  5.15493006e-03, bound:  3.15478534e-01\n",
      "Epoch: 22816 mean train loss:  5.15455380e-03, bound:  3.15478534e-01\n",
      "Epoch: 22817 mean train loss:  5.15427766e-03, bound:  3.15478504e-01\n",
      "Epoch: 22818 mean train loss:  5.15394425e-03, bound:  3.15478474e-01\n",
      "Epoch: 22819 mean train loss:  5.15357172e-03, bound:  3.15478444e-01\n",
      "Epoch: 22820 mean train loss:  5.15322946e-03, bound:  3.15478414e-01\n",
      "Epoch: 22821 mean train loss:  5.15284156e-03, bound:  3.15478414e-01\n",
      "Epoch: 22822 mean train loss:  5.15261805e-03, bound:  3.15478414e-01\n",
      "Epoch: 22823 mean train loss:  5.15223481e-03, bound:  3.15478384e-01\n",
      "Epoch: 22824 mean train loss:  5.15189022e-03, bound:  3.15478355e-01\n",
      "Epoch: 22825 mean train loss:  5.15150512e-03, bound:  3.15478325e-01\n",
      "Epoch: 22826 mean train loss:  5.15126251e-03, bound:  3.15478295e-01\n",
      "Epoch: 22827 mean train loss:  5.15093468e-03, bound:  3.15478295e-01\n",
      "Epoch: 22828 mean train loss:  5.15055703e-03, bound:  3.15478265e-01\n",
      "Epoch: 22829 mean train loss:  5.15022129e-03, bound:  3.15478235e-01\n",
      "Epoch: 22830 mean train loss:  5.14991349e-03, bound:  3.15478206e-01\n",
      "Epoch: 22831 mean train loss:  5.14959497e-03, bound:  3.15478206e-01\n",
      "Epoch: 22832 mean train loss:  5.14927693e-03, bound:  3.15478206e-01\n",
      "Epoch: 22833 mean train loss:  5.14888810e-03, bound:  3.15478176e-01\n",
      "Epoch: 22834 mean train loss:  5.14865993e-03, bound:  3.15478146e-01\n",
      "Epoch: 22835 mean train loss:  5.14830556e-03, bound:  3.15478116e-01\n",
      "Epoch: 22836 mean train loss:  5.14792977e-03, bound:  3.15478086e-01\n",
      "Epoch: 22837 mean train loss:  5.14754653e-03, bound:  3.15478086e-01\n",
      "Epoch: 22838 mean train loss:  5.14727505e-03, bound:  3.15478086e-01\n",
      "Epoch: 22839 mean train loss:  5.14706271e-03, bound:  3.15477997e-01\n",
      "Epoch: 22840 mean train loss:  5.14661707e-03, bound:  3.15477997e-01\n",
      "Epoch: 22841 mean train loss:  5.14627527e-03, bound:  3.15477997e-01\n",
      "Epoch: 22842 mean train loss:  5.14597446e-03, bound:  3.15477967e-01\n",
      "Epoch: 22843 mean train loss:  5.14569087e-03, bound:  3.15477967e-01\n",
      "Epoch: 22844 mean train loss:  5.14530484e-03, bound:  3.15477937e-01\n",
      "Epoch: 22845 mean train loss:  5.14497003e-03, bound:  3.15477908e-01\n",
      "Epoch: 22846 mean train loss:  5.14462451e-03, bound:  3.15477908e-01\n",
      "Epoch: 22847 mean train loss:  5.14431205e-03, bound:  3.15477878e-01\n",
      "Epoch: 22848 mean train loss:  5.14401123e-03, bound:  3.15477848e-01\n",
      "Epoch: 22849 mean train loss:  5.14368154e-03, bound:  3.15477848e-01\n",
      "Epoch: 22850 mean train loss:  5.14333881e-03, bound:  3.15477818e-01\n",
      "Epoch: 22851 mean train loss:  5.14301565e-03, bound:  3.15477788e-01\n",
      "Epoch: 22852 mean train loss:  5.14268689e-03, bound:  3.15477759e-01\n",
      "Epoch: 22853 mean train loss:  5.14231250e-03, bound:  3.15477729e-01\n",
      "Epoch: 22854 mean train loss:  5.14198421e-03, bound:  3.15477729e-01\n",
      "Epoch: 22855 mean train loss:  5.14165172e-03, bound:  3.15477729e-01\n",
      "Epoch: 22856 mean train loss:  5.14134998e-03, bound:  3.15477699e-01\n",
      "Epoch: 22857 mean train loss:  5.14097326e-03, bound:  3.15477669e-01\n",
      "Epoch: 22858 mean train loss:  5.14070783e-03, bound:  3.15477639e-01\n",
      "Epoch: 22859 mean train loss:  5.14042471e-03, bound:  3.15477639e-01\n",
      "Epoch: 22860 mean train loss:  5.14014624e-03, bound:  3.15477610e-01\n",
      "Epoch: 22861 mean train loss:  5.13990223e-03, bound:  3.15477610e-01\n",
      "Epoch: 22862 mean train loss:  5.13979746e-03, bound:  3.15477550e-01\n",
      "Epoch: 22863 mean train loss:  5.13962051e-03, bound:  3.15477550e-01\n",
      "Epoch: 22864 mean train loss:  5.13962423e-03, bound:  3.15477520e-01\n",
      "Epoch: 22865 mean train loss:  5.13964938e-03, bound:  3.15477520e-01\n",
      "Epoch: 22866 mean train loss:  5.13995253e-03, bound:  3.15477431e-01\n",
      "Epoch: 22867 mean train loss:  5.14064962e-03, bound:  3.15477490e-01\n",
      "Epoch: 22868 mean train loss:  5.14167547e-03, bound:  3.15477401e-01\n",
      "Epoch: 22869 mean train loss:  5.14342869e-03, bound:  3.15477431e-01\n",
      "Epoch: 22870 mean train loss:  5.14590926e-03, bound:  3.15477371e-01\n",
      "Epoch: 22871 mean train loss:  5.14934864e-03, bound:  3.15477401e-01\n",
      "Epoch: 22872 mean train loss:  5.15344879e-03, bound:  3.15477312e-01\n",
      "Epoch: 22873 mean train loss:  5.15776966e-03, bound:  3.15477401e-01\n",
      "Epoch: 22874 mean train loss:  5.16080158e-03, bound:  3.15477252e-01\n",
      "Epoch: 22875 mean train loss:  5.16097574e-03, bound:  3.15477371e-01\n",
      "Epoch: 22876 mean train loss:  5.15663764e-03, bound:  3.15477222e-01\n",
      "Epoch: 22877 mean train loss:  5.14866551e-03, bound:  3.15477282e-01\n",
      "Epoch: 22878 mean train loss:  5.14001865e-03, bound:  3.15477192e-01\n",
      "Epoch: 22879 mean train loss:  5.13456156e-03, bound:  3.15477222e-01\n",
      "Epoch: 22880 mean train loss:  5.13385609e-03, bound:  3.15477192e-01\n",
      "Epoch: 22881 mean train loss:  5.13694761e-03, bound:  3.15477163e-01\n",
      "Epoch: 22882 mean train loss:  5.14067663e-03, bound:  3.15477192e-01\n",
      "Epoch: 22883 mean train loss:  5.14235115e-03, bound:  3.15477103e-01\n",
      "Epoch: 22884 mean train loss:  5.14087221e-03, bound:  3.15477163e-01\n",
      "Epoch: 22885 mean train loss:  5.13696158e-03, bound:  3.15477103e-01\n",
      "Epoch: 22886 mean train loss:  5.13314269e-03, bound:  3.15477103e-01\n",
      "Epoch: 22887 mean train loss:  5.13125258e-03, bound:  3.15477043e-01\n",
      "Epoch: 22888 mean train loss:  5.13184257e-03, bound:  3.15477043e-01\n",
      "Epoch: 22889 mean train loss:  5.13360929e-03, bound:  3.15477043e-01\n",
      "Epoch: 22890 mean train loss:  5.13485773e-03, bound:  3.15476984e-01\n",
      "Epoch: 22891 mean train loss:  5.13426680e-03, bound:  3.15476984e-01\n",
      "Epoch: 22892 mean train loss:  5.13236551e-03, bound:  3.15476954e-01\n",
      "Epoch: 22893 mean train loss:  5.13025699e-03, bound:  3.15476984e-01\n",
      "Epoch: 22894 mean train loss:  5.12899971e-03, bound:  3.15476954e-01\n",
      "Epoch: 22895 mean train loss:  5.12910774e-03, bound:  3.15476924e-01\n",
      "Epoch: 22896 mean train loss:  5.12985280e-03, bound:  3.15476924e-01\n",
      "Epoch: 22897 mean train loss:  5.13041904e-03, bound:  3.15476865e-01\n",
      "Epoch: 22898 mean train loss:  5.13005117e-03, bound:  3.15476865e-01\n",
      "Epoch: 22899 mean train loss:  5.12901228e-03, bound:  3.15476835e-01\n",
      "Epoch: 22900 mean train loss:  5.12768561e-03, bound:  3.15476835e-01\n",
      "Epoch: 22901 mean train loss:  5.12684463e-03, bound:  3.15476775e-01\n",
      "Epoch: 22902 mean train loss:  5.12671284e-03, bound:  3.15476775e-01\n",
      "Epoch: 22903 mean train loss:  5.12679433e-03, bound:  3.15476745e-01\n",
      "Epoch: 22904 mean train loss:  5.12692938e-03, bound:  3.15476716e-01\n",
      "Epoch: 22905 mean train loss:  5.12677198e-03, bound:  3.15476745e-01\n",
      "Epoch: 22906 mean train loss:  5.12608141e-03, bound:  3.15476716e-01\n",
      "Epoch: 22907 mean train loss:  5.12527209e-03, bound:  3.15476656e-01\n",
      "Epoch: 22908 mean train loss:  5.12470538e-03, bound:  3.15476656e-01\n",
      "Epoch: 22909 mean train loss:  5.12434449e-03, bound:  3.15476626e-01\n",
      "Epoch: 22910 mean train loss:  5.12432307e-03, bound:  3.15476626e-01\n",
      "Epoch: 22911 mean train loss:  5.12427837e-03, bound:  3.15476596e-01\n",
      "Epoch: 22912 mean train loss:  5.12405438e-03, bound:  3.15476596e-01\n",
      "Epoch: 22913 mean train loss:  5.12357662e-03, bound:  3.15476537e-01\n",
      "Epoch: 22914 mean train loss:  5.12304809e-03, bound:  3.15476537e-01\n",
      "Epoch: 22915 mean train loss:  5.12244646e-03, bound:  3.15476507e-01\n",
      "Epoch: 22916 mean train loss:  5.12204878e-03, bound:  3.15476477e-01\n",
      "Epoch: 22917 mean train loss:  5.12189278e-03, bound:  3.15476477e-01\n",
      "Epoch: 22918 mean train loss:  5.12161804e-03, bound:  3.15476418e-01\n",
      "Epoch: 22919 mean train loss:  5.12142945e-03, bound:  3.15476418e-01\n",
      "Epoch: 22920 mean train loss:  5.12104714e-03, bound:  3.15476388e-01\n",
      "Epoch: 22921 mean train loss:  5.12062153e-03, bound:  3.15476388e-01\n",
      "Epoch: 22922 mean train loss:  5.12026763e-03, bound:  3.15476388e-01\n",
      "Epoch: 22923 mean train loss:  5.11988625e-03, bound:  3.15476328e-01\n",
      "Epoch: 22924 mean train loss:  5.11958962e-03, bound:  3.15476298e-01\n",
      "Epoch: 22925 mean train loss:  5.11939730e-03, bound:  3.15476298e-01\n",
      "Epoch: 22926 mean train loss:  5.11912350e-03, bound:  3.15476298e-01\n",
      "Epoch: 22927 mean train loss:  5.11879520e-03, bound:  3.15476269e-01\n",
      "Epoch: 22928 mean train loss:  5.11839706e-03, bound:  3.15476269e-01\n",
      "Epoch: 22929 mean train loss:  5.11800172e-03, bound:  3.15476209e-01\n",
      "Epoch: 22930 mean train loss:  5.11766644e-03, bound:  3.15476179e-01\n",
      "Epoch: 22931 mean train loss:  5.11734653e-03, bound:  3.15476179e-01\n",
      "Epoch: 22932 mean train loss:  5.11709601e-03, bound:  3.15476179e-01\n",
      "Epoch: 22933 mean train loss:  5.11674117e-03, bound:  3.15476149e-01\n",
      "Epoch: 22934 mean train loss:  5.11646969e-03, bound:  3.15476149e-01\n",
      "Epoch: 22935 mean train loss:  5.11614513e-03, bound:  3.15476090e-01\n",
      "Epoch: 22936 mean train loss:  5.11583034e-03, bound:  3.15476060e-01\n",
      "Epoch: 22937 mean train loss:  5.11552533e-03, bound:  3.15476060e-01\n",
      "Epoch: 22938 mean train loss:  5.11520775e-03, bound:  3.15476030e-01\n",
      "Epoch: 22939 mean train loss:  5.11483802e-03, bound:  3.15476030e-01\n",
      "Epoch: 22940 mean train loss:  5.11456607e-03, bound:  3.15475971e-01\n",
      "Epoch: 22941 mean train loss:  5.11423917e-03, bound:  3.15475971e-01\n",
      "Epoch: 22942 mean train loss:  5.11391787e-03, bound:  3.15475971e-01\n",
      "Epoch: 22943 mean train loss:  5.11354301e-03, bound:  3.15475941e-01\n",
      "Epoch: 22944 mean train loss:  5.11328690e-03, bound:  3.15475911e-01\n",
      "Epoch: 22945 mean train loss:  5.11292368e-03, bound:  3.15475881e-01\n",
      "Epoch: 22946 mean train loss:  5.11268154e-03, bound:  3.15475881e-01\n",
      "Epoch: 22947 mean train loss:  5.11234347e-03, bound:  3.15475851e-01\n",
      "Epoch: 22948 mean train loss:  5.11204544e-03, bound:  3.15475851e-01\n",
      "Epoch: 22949 mean train loss:  5.11165475e-03, bound:  3.15475821e-01\n",
      "Epoch: 22950 mean train loss:  5.11136232e-03, bound:  3.15475792e-01\n",
      "Epoch: 22951 mean train loss:  5.11110248e-03, bound:  3.15475792e-01\n",
      "Epoch: 22952 mean train loss:  5.11075277e-03, bound:  3.15475762e-01\n",
      "Epoch: 22953 mean train loss:  5.11039747e-03, bound:  3.15475732e-01\n",
      "Epoch: 22954 mean train loss:  5.11003751e-03, bound:  3.15475732e-01\n",
      "Epoch: 22955 mean train loss:  5.10977395e-03, bound:  3.15475702e-01\n",
      "Epoch: 22956 mean train loss:  5.10941399e-03, bound:  3.15475702e-01\n",
      "Epoch: 22957 mean train loss:  5.10910759e-03, bound:  3.15475643e-01\n",
      "Epoch: 22958 mean train loss:  5.10877557e-03, bound:  3.15475613e-01\n",
      "Epoch: 22959 mean train loss:  5.10848593e-03, bound:  3.15475613e-01\n",
      "Epoch: 22960 mean train loss:  5.10817952e-03, bound:  3.15475583e-01\n",
      "Epoch: 22961 mean train loss:  5.10788057e-03, bound:  3.15475583e-01\n",
      "Epoch: 22962 mean train loss:  5.10756671e-03, bound:  3.15475583e-01\n",
      "Epoch: 22963 mean train loss:  5.10725006e-03, bound:  3.15475523e-01\n",
      "Epoch: 22964 mean train loss:  5.10692364e-03, bound:  3.15475494e-01\n",
      "Epoch: 22965 mean train loss:  5.10661257e-03, bound:  3.15475494e-01\n",
      "Epoch: 22966 mean train loss:  5.10630058e-03, bound:  3.15475494e-01\n",
      "Epoch: 22967 mean train loss:  5.10596344e-03, bound:  3.15475464e-01\n",
      "Epoch: 22968 mean train loss:  5.10569941e-03, bound:  3.15475434e-01\n",
      "Epoch: 22969 mean train loss:  5.10537671e-03, bound:  3.15475404e-01\n",
      "Epoch: 22970 mean train loss:  5.10502513e-03, bound:  3.15475404e-01\n",
      "Epoch: 22971 mean train loss:  5.10466425e-03, bound:  3.15475374e-01\n",
      "Epoch: 22972 mean train loss:  5.10439463e-03, bound:  3.15475374e-01\n",
      "Epoch: 22973 mean train loss:  5.10405609e-03, bound:  3.15475315e-01\n",
      "Epoch: 22974 mean train loss:  5.10374503e-03, bound:  3.15475285e-01\n",
      "Epoch: 22975 mean train loss:  5.10345167e-03, bound:  3.15475285e-01\n",
      "Epoch: 22976 mean train loss:  5.10309311e-03, bound:  3.15475285e-01\n",
      "Epoch: 22977 mean train loss:  5.10282395e-03, bound:  3.15475255e-01\n",
      "Epoch: 22978 mean train loss:  5.10247657e-03, bound:  3.15475225e-01\n",
      "Epoch: 22979 mean train loss:  5.10214688e-03, bound:  3.15475196e-01\n",
      "Epoch: 22980 mean train loss:  5.10187354e-03, bound:  3.15475166e-01\n",
      "Epoch: 22981 mean train loss:  5.10153966e-03, bound:  3.15475166e-01\n",
      "Epoch: 22982 mean train loss:  5.10120392e-03, bound:  3.15475136e-01\n",
      "Epoch: 22983 mean train loss:  5.10085886e-03, bound:  3.15475136e-01\n",
      "Epoch: 22984 mean train loss:  5.10052964e-03, bound:  3.15475076e-01\n",
      "Epoch: 22985 mean train loss:  5.10021253e-03, bound:  3.15475076e-01\n",
      "Epoch: 22986 mean train loss:  5.09990798e-03, bound:  3.15475047e-01\n",
      "Epoch: 22987 mean train loss:  5.09956479e-03, bound:  3.15475047e-01\n",
      "Epoch: 22988 mean train loss:  5.09926770e-03, bound:  3.15475017e-01\n",
      "Epoch: 22989 mean train loss:  5.09889564e-03, bound:  3.15475017e-01\n",
      "Epoch: 22990 mean train loss:  5.09862788e-03, bound:  3.15474957e-01\n",
      "Epoch: 22991 mean train loss:  5.09828655e-03, bound:  3.15474957e-01\n",
      "Epoch: 22992 mean train loss:  5.09794103e-03, bound:  3.15474927e-01\n",
      "Epoch: 22993 mean train loss:  5.09771379e-03, bound:  3.15474898e-01\n",
      "Epoch: 22994 mean train loss:  5.09737385e-03, bound:  3.15474898e-01\n",
      "Epoch: 22995 mean train loss:  5.09701157e-03, bound:  3.15474868e-01\n",
      "Epoch: 22996 mean train loss:  5.09670703e-03, bound:  3.15474868e-01\n",
      "Epoch: 22997 mean train loss:  5.09640574e-03, bound:  3.15474838e-01\n",
      "Epoch: 22998 mean train loss:  5.09605417e-03, bound:  3.15474808e-01\n",
      "Epoch: 22999 mean train loss:  5.09570539e-03, bound:  3.15474778e-01\n",
      "Epoch: 23000 mean train loss:  5.09538362e-03, bound:  3.15474778e-01\n",
      "Epoch: 23001 mean train loss:  5.09510702e-03, bound:  3.15474749e-01\n",
      "Epoch: 23002 mean train loss:  5.09478152e-03, bound:  3.15474719e-01\n",
      "Epoch: 23003 mean train loss:  5.09447046e-03, bound:  3.15474719e-01\n",
      "Epoch: 23004 mean train loss:  5.09416219e-03, bound:  3.15474689e-01\n",
      "Epoch: 23005 mean train loss:  5.09381760e-03, bound:  3.15474629e-01\n",
      "Epoch: 23006 mean train loss:  5.09346835e-03, bound:  3.15474629e-01\n",
      "Epoch: 23007 mean train loss:  5.09313820e-03, bound:  3.15474629e-01\n",
      "Epoch: 23008 mean train loss:  5.09284716e-03, bound:  3.15474600e-01\n",
      "Epoch: 23009 mean train loss:  5.09252120e-03, bound:  3.15474600e-01\n",
      "Epoch: 23010 mean train loss:  5.09218639e-03, bound:  3.15474570e-01\n",
      "Epoch: 23011 mean train loss:  5.09195123e-03, bound:  3.15474570e-01\n",
      "Epoch: 23012 mean train loss:  5.09163085e-03, bound:  3.15474510e-01\n",
      "Epoch: 23013 mean train loss:  5.09138172e-03, bound:  3.15474480e-01\n",
      "Epoch: 23014 mean train loss:  5.09103108e-03, bound:  3.15474480e-01\n",
      "Epoch: 23015 mean train loss:  5.09080896e-03, bound:  3.15474451e-01\n",
      "Epoch: 23016 mean train loss:  5.09062177e-03, bound:  3.15474451e-01\n",
      "Epoch: 23017 mean train loss:  5.09038288e-03, bound:  3.15474391e-01\n",
      "Epoch: 23018 mean train loss:  5.09016495e-03, bound:  3.15474391e-01\n",
      "Epoch: 23019 mean train loss:  5.09002106e-03, bound:  3.15474361e-01\n",
      "Epoch: 23020 mean train loss:  5.08990791e-03, bound:  3.15474361e-01\n",
      "Epoch: 23021 mean train loss:  5.08992234e-03, bound:  3.15474331e-01\n",
      "Epoch: 23022 mean train loss:  5.08989207e-03, bound:  3.15474331e-01\n",
      "Epoch: 23023 mean train loss:  5.09011187e-03, bound:  3.15474272e-01\n",
      "Epoch: 23024 mean train loss:  5.09045506e-03, bound:  3.15474272e-01\n",
      "Epoch: 23025 mean train loss:  5.09099802e-03, bound:  3.15474242e-01\n",
      "Epoch: 23026 mean train loss:  5.09184226e-03, bound:  3.15474242e-01\n",
      "Epoch: 23027 mean train loss:  5.09283505e-03, bound:  3.15474182e-01\n",
      "Epoch: 23028 mean train loss:  5.09403367e-03, bound:  3.15474182e-01\n",
      "Epoch: 23029 mean train loss:  5.09527978e-03, bound:  3.15474153e-01\n",
      "Epoch: 23030 mean train loss:  5.09631820e-03, bound:  3.15474182e-01\n",
      "Epoch: 23031 mean train loss:  5.09679876e-03, bound:  3.15474063e-01\n",
      "Epoch: 23032 mean train loss:  5.09666093e-03, bound:  3.15474153e-01\n",
      "Epoch: 23033 mean train loss:  5.09552564e-03, bound:  3.15474063e-01\n",
      "Epoch: 23034 mean train loss:  5.09351678e-03, bound:  3.15474123e-01\n",
      "Epoch: 23035 mean train loss:  5.09073958e-03, bound:  3.15474033e-01\n",
      "Epoch: 23036 mean train loss:  5.08775748e-03, bound:  3.15474063e-01\n",
      "Epoch: 23037 mean train loss:  5.08526247e-03, bound:  3.15474004e-01\n",
      "Epoch: 23038 mean train loss:  5.08356048e-03, bound:  3.15474033e-01\n",
      "Epoch: 23039 mean train loss:  5.08296210e-03, bound:  3.15473944e-01\n",
      "Epoch: 23040 mean train loss:  5.08308271e-03, bound:  3.15473944e-01\n",
      "Epoch: 23041 mean train loss:  5.08369133e-03, bound:  3.15473944e-01\n",
      "Epoch: 23042 mean train loss:  5.08444663e-03, bound:  3.15473914e-01\n",
      "Epoch: 23043 mean train loss:  5.08484617e-03, bound:  3.15473944e-01\n",
      "Epoch: 23044 mean train loss:  5.08464826e-03, bound:  3.15473855e-01\n",
      "Epoch: 23045 mean train loss:  5.08386921e-03, bound:  3.15473884e-01\n",
      "Epoch: 23046 mean train loss:  5.08276466e-03, bound:  3.15473825e-01\n",
      "Epoch: 23047 mean train loss:  5.08152321e-03, bound:  3.15473825e-01\n",
      "Epoch: 23048 mean train loss:  5.08048246e-03, bound:  3.15473795e-01\n",
      "Epoch: 23049 mean train loss:  5.07983891e-03, bound:  3.15473795e-01\n",
      "Epoch: 23050 mean train loss:  5.07951900e-03, bound:  3.15473735e-01\n",
      "Epoch: 23051 mean train loss:  5.07942634e-03, bound:  3.15473735e-01\n",
      "Epoch: 23052 mean train loss:  5.07946452e-03, bound:  3.15473735e-01\n",
      "Epoch: 23053 mean train loss:  5.07948315e-03, bound:  3.15473676e-01\n",
      "Epoch: 23054 mean train loss:  5.07940445e-03, bound:  3.15473706e-01\n",
      "Epoch: 23055 mean train loss:  5.07910550e-03, bound:  3.15473676e-01\n",
      "Epoch: 23056 mean train loss:  5.07867243e-03, bound:  3.15473676e-01\n",
      "Epoch: 23057 mean train loss:  5.07812249e-03, bound:  3.15473586e-01\n",
      "Epoch: 23058 mean train loss:  5.07747103e-03, bound:  3.15473586e-01\n",
      "Epoch: 23059 mean train loss:  5.07684471e-03, bound:  3.15473557e-01\n",
      "Epoch: 23060 mean train loss:  5.07633528e-03, bound:  3.15473557e-01\n",
      "Epoch: 23061 mean train loss:  5.07604750e-03, bound:  3.15473557e-01\n",
      "Epoch: 23062 mean train loss:  5.07585471e-03, bound:  3.15473497e-01\n",
      "Epoch: 23063 mean train loss:  5.07567031e-03, bound:  3.15473497e-01\n",
      "Epoch: 23064 mean train loss:  5.07553667e-03, bound:  3.15473467e-01\n",
      "Epoch: 23065 mean train loss:  5.07533224e-03, bound:  3.15473467e-01\n",
      "Epoch: 23066 mean train loss:  5.07513527e-03, bound:  3.15473408e-01\n",
      "Epoch: 23067 mean train loss:  5.07481443e-03, bound:  3.15473378e-01\n",
      "Epoch: 23068 mean train loss:  5.07442933e-03, bound:  3.15473378e-01\n",
      "Epoch: 23069 mean train loss:  5.07398043e-03, bound:  3.15473348e-01\n",
      "Epoch: 23070 mean train loss:  5.07353200e-03, bound:  3.15473348e-01\n",
      "Epoch: 23071 mean train loss:  5.07306494e-03, bound:  3.15473348e-01\n",
      "Epoch: 23072 mean train loss:  5.07261511e-03, bound:  3.15473288e-01\n",
      "Epoch: 23073 mean train loss:  5.07225562e-03, bound:  3.15473258e-01\n",
      "Epoch: 23074 mean train loss:  5.07201627e-03, bound:  3.15473258e-01\n",
      "Epoch: 23075 mean train loss:  5.07169077e-03, bound:  3.15473258e-01\n",
      "Epoch: 23076 mean train loss:  5.07144677e-03, bound:  3.15473229e-01\n",
      "Epoch: 23077 mean train loss:  5.07111428e-03, bound:  3.15473229e-01\n",
      "Epoch: 23078 mean train loss:  5.07082790e-03, bound:  3.15473229e-01\n",
      "Epoch: 23079 mean train loss:  5.07067423e-03, bound:  3.15473169e-01\n",
      "Epoch: 23080 mean train loss:  5.07039949e-03, bound:  3.15473139e-01\n",
      "Epoch: 23081 mean train loss:  5.07011451e-03, bound:  3.15473139e-01\n",
      "Epoch: 23082 mean train loss:  5.06975828e-03, bound:  3.15473109e-01\n",
      "Epoch: 23083 mean train loss:  5.06942067e-03, bound:  3.15473109e-01\n",
      "Epoch: 23084 mean train loss:  5.06902812e-03, bound:  3.15473050e-01\n",
      "Epoch: 23085 mean train loss:  5.06867561e-03, bound:  3.15473050e-01\n",
      "Epoch: 23086 mean train loss:  5.06823044e-03, bound:  3.15473020e-01\n",
      "Epoch: 23087 mean train loss:  5.06792869e-03, bound:  3.15473020e-01\n",
      "Epoch: 23088 mean train loss:  5.06759854e-03, bound:  3.15472960e-01\n",
      "Epoch: 23089 mean train loss:  5.06727397e-03, bound:  3.15472960e-01\n",
      "Epoch: 23090 mean train loss:  5.06692939e-03, bound:  3.15472931e-01\n",
      "Epoch: 23091 mean train loss:  5.06659877e-03, bound:  3.15472931e-01\n",
      "Epoch: 23092 mean train loss:  5.06634964e-03, bound:  3.15472901e-01\n",
      "Epoch: 23093 mean train loss:  5.06604137e-03, bound:  3.15472841e-01\n",
      "Epoch: 23094 mean train loss:  5.06576383e-03, bound:  3.15472841e-01\n",
      "Epoch: 23095 mean train loss:  5.06540341e-03, bound:  3.15472811e-01\n",
      "Epoch: 23096 mean train loss:  5.06519899e-03, bound:  3.15472811e-01\n",
      "Epoch: 23097 mean train loss:  5.06496802e-03, bound:  3.15472782e-01\n",
      "Epoch: 23098 mean train loss:  5.06467186e-03, bound:  3.15472782e-01\n",
      "Epoch: 23099 mean train loss:  5.06431609e-03, bound:  3.15472782e-01\n",
      "Epoch: 23100 mean train loss:  5.06410841e-03, bound:  3.15472722e-01\n",
      "Epoch: 23101 mean train loss:  5.06379781e-03, bound:  3.15472692e-01\n",
      "Epoch: 23102 mean train loss:  5.06355846e-03, bound:  3.15472692e-01\n",
      "Epoch: 23103 mean train loss:  5.06327627e-03, bound:  3.15472662e-01\n",
      "Epoch: 23104 mean train loss:  5.06296707e-03, bound:  3.15472662e-01\n",
      "Epoch: 23105 mean train loss:  5.06270397e-03, bound:  3.15472662e-01\n",
      "Epoch: 23106 mean train loss:  5.06236404e-03, bound:  3.15472603e-01\n",
      "Epoch: 23107 mean train loss:  5.06206462e-03, bound:  3.15472603e-01\n",
      "Epoch: 23108 mean train loss:  5.06177405e-03, bound:  3.15472543e-01\n",
      "Epoch: 23109 mean train loss:  5.06145647e-03, bound:  3.15472543e-01\n",
      "Epoch: 23110 mean train loss:  5.06122503e-03, bound:  3.15472513e-01\n",
      "Epoch: 23111 mean train loss:  5.06100757e-03, bound:  3.15472513e-01\n",
      "Epoch: 23112 mean train loss:  5.06077753e-03, bound:  3.15472484e-01\n",
      "Epoch: 23113 mean train loss:  5.06057637e-03, bound:  3.15472484e-01\n",
      "Epoch: 23114 mean train loss:  5.06037939e-03, bound:  3.15472424e-01\n",
      "Epoch: 23115 mean train loss:  5.06021827e-03, bound:  3.15472424e-01\n",
      "Epoch: 23116 mean train loss:  5.06008463e-03, bound:  3.15472394e-01\n",
      "Epoch: 23117 mean train loss:  5.05999895e-03, bound:  3.15472394e-01\n",
      "Epoch: 23118 mean train loss:  5.05993096e-03, bound:  3.15472335e-01\n",
      "Epoch: 23119 mean train loss:  5.05991932e-03, bound:  3.15472364e-01\n",
      "Epoch: 23120 mean train loss:  5.06004691e-03, bound:  3.15472305e-01\n",
      "Epoch: 23121 mean train loss:  5.06015122e-03, bound:  3.15472335e-01\n",
      "Epoch: 23122 mean train loss:  5.06030815e-03, bound:  3.15472275e-01\n",
      "Epoch: 23123 mean train loss:  5.06063458e-03, bound:  3.15472275e-01\n",
      "Epoch: 23124 mean train loss:  5.06094890e-03, bound:  3.15472245e-01\n",
      "Epoch: 23125 mean train loss:  5.06124040e-03, bound:  3.15472245e-01\n",
      "Epoch: 23126 mean train loss:  5.06145461e-03, bound:  3.15472156e-01\n",
      "Epoch: 23127 mean train loss:  5.06148161e-03, bound:  3.15472215e-01\n",
      "Epoch: 23128 mean train loss:  5.06141130e-03, bound:  3.15472126e-01\n",
      "Epoch: 23129 mean train loss:  5.06099919e-03, bound:  3.15472156e-01\n",
      "Epoch: 23130 mean train loss:  5.06017124e-03, bound:  3.15472096e-01\n",
      "Epoch: 23131 mean train loss:  5.05900476e-03, bound:  3.15472126e-01\n",
      "Epoch: 23132 mean train loss:  5.05765807e-03, bound:  3.15472037e-01\n",
      "Epoch: 23133 mean train loss:  5.05613955e-03, bound:  3.15472096e-01\n",
      "Epoch: 23134 mean train loss:  5.05469972e-03, bound:  3.15472007e-01\n",
      "Epoch: 23135 mean train loss:  5.05356397e-03, bound:  3.15472037e-01\n",
      "Epoch: 23136 mean train loss:  5.05267549e-03, bound:  3.15472007e-01\n",
      "Epoch: 23137 mean train loss:  5.05215023e-03, bound:  3.15471977e-01\n",
      "Epoch: 23138 mean train loss:  5.05187968e-03, bound:  3.15471977e-01\n",
      "Epoch: 23139 mean train loss:  5.05174929e-03, bound:  3.15471917e-01\n",
      "Epoch: 23140 mean train loss:  5.05176606e-03, bound:  3.15471917e-01\n",
      "Epoch: 23141 mean train loss:  5.05177584e-03, bound:  3.15471917e-01\n",
      "Epoch: 23142 mean train loss:  5.05176419e-03, bound:  3.15471917e-01\n",
      "Epoch: 23143 mean train loss:  5.05168224e-03, bound:  3.15471858e-01\n",
      "Epoch: 23144 mean train loss:  5.05148713e-03, bound:  3.15471858e-01\n",
      "Epoch: 23145 mean train loss:  5.05115231e-03, bound:  3.15471828e-01\n",
      "Epoch: 23146 mean train loss:  5.05065173e-03, bound:  3.15471828e-01\n",
      "Epoch: 23147 mean train loss:  5.05013112e-03, bound:  3.15471798e-01\n",
      "Epoch: 23148 mean train loss:  5.04939165e-03, bound:  3.15471798e-01\n",
      "Epoch: 23149 mean train loss:  5.04881795e-03, bound:  3.15471709e-01\n",
      "Epoch: 23150 mean train loss:  5.04833600e-03, bound:  3.15471709e-01\n",
      "Epoch: 23151 mean train loss:  5.04788337e-03, bound:  3.15471709e-01\n",
      "Epoch: 23152 mean train loss:  5.04749361e-03, bound:  3.15471679e-01\n",
      "Epoch: 23153 mean train loss:  5.04718488e-03, bound:  3.15471679e-01\n",
      "Epoch: 23154 mean train loss:  5.04686264e-03, bound:  3.15471619e-01\n",
      "Epoch: 23155 mean train loss:  5.04663354e-03, bound:  3.15471619e-01\n",
      "Epoch: 23156 mean train loss:  5.04636252e-03, bound:  3.15471590e-01\n",
      "Epoch: 23157 mean train loss:  5.04600583e-03, bound:  3.15471590e-01\n",
      "Epoch: 23158 mean train loss:  5.04577067e-03, bound:  3.15471560e-01\n",
      "Epoch: 23159 mean train loss:  5.04551735e-03, bound:  3.15471560e-01\n",
      "Epoch: 23160 mean train loss:  5.04526310e-03, bound:  3.15471530e-01\n",
      "Epoch: 23161 mean train loss:  5.04498603e-03, bound:  3.15471530e-01\n",
      "Epoch: 23162 mean train loss:  5.04469126e-03, bound:  3.15471470e-01\n",
      "Epoch: 23163 mean train loss:  5.04451152e-03, bound:  3.15471470e-01\n",
      "Epoch: 23164 mean train loss:  5.04426798e-03, bound:  3.15471441e-01\n",
      "Epoch: 23165 mean train loss:  5.04400302e-03, bound:  3.15471441e-01\n",
      "Epoch: 23166 mean train loss:  5.04368963e-03, bound:  3.15471381e-01\n",
      "Epoch: 23167 mean train loss:  5.04333433e-03, bound:  3.15471381e-01\n",
      "Epoch: 23168 mean train loss:  5.04303072e-03, bound:  3.15471351e-01\n",
      "Epoch: 23169 mean train loss:  5.04264934e-03, bound:  3.15471321e-01\n",
      "Epoch: 23170 mean train loss:  5.04227309e-03, bound:  3.15471262e-01\n",
      "Epoch: 23171 mean train loss:  5.04186749e-03, bound:  3.15471262e-01\n",
      "Epoch: 23172 mean train loss:  5.04151918e-03, bound:  3.15471262e-01\n",
      "Epoch: 23173 mean train loss:  5.04109077e-03, bound:  3.15471232e-01\n",
      "Epoch: 23174 mean train loss:  5.04078623e-03, bound:  3.15471232e-01\n",
      "Epoch: 23175 mean train loss:  5.04039833e-03, bound:  3.15471232e-01\n",
      "Epoch: 23176 mean train loss:  5.04002813e-03, bound:  3.15471143e-01\n",
      "Epoch: 23177 mean train loss:  5.03967609e-03, bound:  3.15471143e-01\n",
      "Epoch: 23178 mean train loss:  5.03941393e-03, bound:  3.15471143e-01\n",
      "Epoch: 23179 mean train loss:  5.03898552e-03, bound:  3.15471113e-01\n",
      "Epoch: 23180 mean train loss:  5.03873732e-03, bound:  3.15471113e-01\n",
      "Epoch: 23181 mean train loss:  5.03847562e-03, bound:  3.15471113e-01\n",
      "Epoch: 23182 mean train loss:  5.03817573e-03, bound:  3.15471083e-01\n",
      "Epoch: 23183 mean train loss:  5.03785675e-03, bound:  3.15471023e-01\n",
      "Epoch: 23184 mean train loss:  5.03757456e-03, bound:  3.15471023e-01\n",
      "Epoch: 23185 mean train loss:  5.03726909e-03, bound:  3.15470994e-01\n",
      "Epoch: 23186 mean train loss:  5.03697852e-03, bound:  3.15470994e-01\n",
      "Epoch: 23187 mean train loss:  5.03664650e-03, bound:  3.15470994e-01\n",
      "Epoch: 23188 mean train loss:  5.03638340e-03, bound:  3.15470934e-01\n",
      "Epoch: 23189 mean train loss:  5.03610680e-03, bound:  3.15470904e-01\n",
      "Epoch: 23190 mean train loss:  5.03579015e-03, bound:  3.15470904e-01\n",
      "Epoch: 23191 mean train loss:  5.03562437e-03, bound:  3.15470874e-01\n",
      "Epoch: 23192 mean train loss:  5.03539294e-03, bound:  3.15470874e-01\n",
      "Epoch: 23193 mean train loss:  5.03528118e-03, bound:  3.15470815e-01\n",
      "Epoch: 23194 mean train loss:  5.03523741e-03, bound:  3.15470815e-01\n",
      "Epoch: 23195 mean train loss:  5.03511494e-03, bound:  3.15470785e-01\n",
      "Epoch: 23196 mean train loss:  5.03514800e-03, bound:  3.15470785e-01\n",
      "Epoch: 23197 mean train loss:  5.03533939e-03, bound:  3.15470755e-01\n",
      "Epoch: 23198 mean train loss:  5.03568584e-03, bound:  3.15470755e-01\n",
      "Epoch: 23199 mean train loss:  5.03628049e-03, bound:  3.15470695e-01\n",
      "Epoch: 23200 mean train loss:  5.03714662e-03, bound:  3.15470695e-01\n",
      "Epoch: 23201 mean train loss:  5.03828144e-03, bound:  3.15470636e-01\n",
      "Epoch: 23202 mean train loss:  5.03976271e-03, bound:  3.15470695e-01\n",
      "Epoch: 23203 mean train loss:  5.04135946e-03, bound:  3.15470576e-01\n",
      "Epoch: 23204 mean train loss:  5.04318764e-03, bound:  3.15470666e-01\n",
      "Epoch: 23205 mean train loss:  5.04478440e-03, bound:  3.15470546e-01\n",
      "Epoch: 23206 mean train loss:  5.04571758e-03, bound:  3.15470636e-01\n",
      "Epoch: 23207 mean train loss:  5.04530780e-03, bound:  3.15470517e-01\n",
      "Epoch: 23208 mean train loss:  5.04309777e-03, bound:  3.15470576e-01\n",
      "Epoch: 23209 mean train loss:  5.03943022e-03, bound:  3.15470517e-01\n",
      "Epoch: 23210 mean train loss:  5.03520481e-03, bound:  3.15470546e-01\n",
      "Epoch: 23211 mean train loss:  5.03143622e-03, bound:  3.15470457e-01\n",
      "Epoch: 23212 mean train loss:  5.02914516e-03, bound:  3.15470457e-01\n",
      "Epoch: 23213 mean train loss:  5.02859289e-03, bound:  3.15470457e-01\n",
      "Epoch: 23214 mean train loss:  5.02938638e-03, bound:  3.15470427e-01\n",
      "Epoch: 23215 mean train loss:  5.03071677e-03, bound:  3.15470427e-01\n",
      "Epoch: 23216 mean train loss:  5.03198942e-03, bound:  3.15470368e-01\n",
      "Epoch: 23217 mean train loss:  5.03234053e-03, bound:  3.15470427e-01\n",
      "Epoch: 23218 mean train loss:  5.03156381e-03, bound:  3.15470308e-01\n",
      "Epoch: 23219 mean train loss:  5.03002293e-03, bound:  3.15470368e-01\n",
      "Epoch: 23220 mean train loss:  5.02817053e-03, bound:  3.15470308e-01\n",
      "Epoch: 23221 mean train loss:  5.02664875e-03, bound:  3.15470308e-01\n",
      "Epoch: 23222 mean train loss:  5.02583617e-03, bound:  3.15470308e-01\n",
      "Epoch: 23223 mean train loss:  5.02569508e-03, bound:  3.15470248e-01\n",
      "Epoch: 23224 mean train loss:  5.02601033e-03, bound:  3.15470248e-01\n",
      "Epoch: 23225 mean train loss:  5.02633117e-03, bound:  3.15470189e-01\n",
      "Epoch: 23226 mean train loss:  5.02642151e-03, bound:  3.15470219e-01\n",
      "Epoch: 23227 mean train loss:  5.02622360e-03, bound:  3.15470189e-01\n",
      "Epoch: 23228 mean train loss:  5.02553303e-03, bound:  3.15470189e-01\n",
      "Epoch: 23229 mean train loss:  5.02466829e-03, bound:  3.15470129e-01\n",
      "Epoch: 23230 mean train loss:  5.02387900e-03, bound:  3.15470129e-01\n",
      "Epoch: 23231 mean train loss:  5.02313161e-03, bound:  3.15470099e-01\n",
      "Epoch: 23232 mean train loss:  5.02273627e-03, bound:  3.15470099e-01\n",
      "Epoch: 23233 mean train loss:  5.02255745e-03, bound:  3.15470040e-01\n",
      "Epoch: 23234 mean train loss:  5.02249878e-03, bound:  3.15470010e-01\n",
      "Epoch: 23235 mean train loss:  5.02242520e-03, bound:  3.15470010e-01\n",
      "Epoch: 23236 mean train loss:  5.02227526e-03, bound:  3.15469980e-01\n",
      "Epoch: 23237 mean train loss:  5.02196886e-03, bound:  3.15469980e-01\n",
      "Epoch: 23238 mean train loss:  5.02151949e-03, bound:  3.15469921e-01\n",
      "Epoch: 23239 mean train loss:  5.02108317e-03, bound:  3.15469921e-01\n",
      "Epoch: 23240 mean train loss:  5.02061751e-03, bound:  3.15469891e-01\n",
      "Epoch: 23241 mean train loss:  5.02009178e-03, bound:  3.15469891e-01\n",
      "Epoch: 23242 mean train loss:  5.01975743e-03, bound:  3.15469861e-01\n",
      "Epoch: 23243 mean train loss:  5.01944078e-03, bound:  3.15469861e-01\n",
      "Epoch: 23244 mean train loss:  5.01924194e-03, bound:  3.15469861e-01\n",
      "Epoch: 23245 mean train loss:  5.01893321e-03, bound:  3.15469801e-01\n",
      "Epoch: 23246 mean train loss:  5.01877163e-03, bound:  3.15469772e-01\n",
      "Epoch: 23247 mean train loss:  5.01845637e-03, bound:  3.15469772e-01\n",
      "Epoch: 23248 mean train loss:  5.01812994e-03, bound:  3.15469772e-01\n",
      "Epoch: 23249 mean train loss:  5.01779048e-03, bound:  3.15469742e-01\n",
      "Epoch: 23250 mean train loss:  5.01749059e-03, bound:  3.15469682e-01\n",
      "Epoch: 23251 mean train loss:  5.01704263e-03, bound:  3.15469682e-01\n",
      "Epoch: 23252 mean train loss:  5.01670130e-03, bound:  3.15469682e-01\n",
      "Epoch: 23253 mean train loss:  5.01631433e-03, bound:  3.15469682e-01\n",
      "Epoch: 23254 mean train loss:  5.01606008e-03, bound:  3.15469623e-01\n",
      "Epoch: 23255 mean train loss:  5.01582026e-03, bound:  3.15469623e-01\n",
      "Epoch: 23256 mean train loss:  5.01552876e-03, bound:  3.15469593e-01\n",
      "Epoch: 23257 mean train loss:  5.01529919e-03, bound:  3.15469593e-01\n",
      "Epoch: 23258 mean train loss:  5.01500769e-03, bound:  3.15469563e-01\n",
      "Epoch: 23259 mean train loss:  5.01474366e-03, bound:  3.15469563e-01\n",
      "Epoch: 23260 mean train loss:  5.01440885e-03, bound:  3.15469503e-01\n",
      "Epoch: 23261 mean train loss:  5.01410151e-03, bound:  3.15469503e-01\n",
      "Epoch: 23262 mean train loss:  5.01380675e-03, bound:  3.15469474e-01\n",
      "Epoch: 23263 mean train loss:  5.01344586e-03, bound:  3.15469474e-01\n",
      "Epoch: 23264 mean train loss:  5.01313433e-03, bound:  3.15469444e-01\n",
      "Epoch: 23265 mean train loss:  5.01274085e-03, bound:  3.15469414e-01\n",
      "Epoch: 23266 mean train loss:  5.01241162e-03, bound:  3.15469414e-01\n",
      "Epoch: 23267 mean train loss:  5.01215085e-03, bound:  3.15469414e-01\n",
      "Epoch: 23268 mean train loss:  5.01188915e-03, bound:  3.15469354e-01\n",
      "Epoch: 23269 mean train loss:  5.01161395e-03, bound:  3.15469325e-01\n",
      "Epoch: 23270 mean train loss:  5.01129543e-03, bound:  3.15469325e-01\n",
      "Epoch: 23271 mean train loss:  5.01108076e-03, bound:  3.15469295e-01\n",
      "Epoch: 23272 mean train loss:  5.01079205e-03, bound:  3.15469295e-01\n",
      "Epoch: 23273 mean train loss:  5.01055410e-03, bound:  3.15469235e-01\n",
      "Epoch: 23274 mean train loss:  5.01025701e-03, bound:  3.15469235e-01\n",
      "Epoch: 23275 mean train loss:  5.00989240e-03, bound:  3.15469205e-01\n",
      "Epoch: 23276 mean train loss:  5.00960601e-03, bound:  3.15469205e-01\n",
      "Epoch: 23277 mean train loss:  5.00931265e-03, bound:  3.15469176e-01\n",
      "Epoch: 23278 mean train loss:  5.00902440e-03, bound:  3.15469176e-01\n",
      "Epoch: 23279 mean train loss:  5.00870869e-03, bound:  3.15469116e-01\n",
      "Epoch: 23280 mean train loss:  5.00835525e-03, bound:  3.15469116e-01\n",
      "Epoch: 23281 mean train loss:  5.00807865e-03, bound:  3.15469086e-01\n",
      "Epoch: 23282 mean train loss:  5.00768237e-03, bound:  3.15469086e-01\n",
      "Epoch: 23283 mean train loss:  5.00733266e-03, bound:  3.15469056e-01\n",
      "Epoch: 23284 mean train loss:  5.00703370e-03, bound:  3.15469056e-01\n",
      "Epoch: 23285 mean train loss:  5.00676408e-03, bound:  3.15468997e-01\n",
      "Epoch: 23286 mean train loss:  5.00636641e-03, bound:  3.15468997e-01\n",
      "Epoch: 23287 mean train loss:  5.00607351e-03, bound:  3.15468997e-01\n",
      "Epoch: 23288 mean train loss:  5.00575639e-03, bound:  3.15468967e-01\n",
      "Epoch: 23289 mean train loss:  5.00548957e-03, bound:  3.15468937e-01\n",
      "Epoch: 23290 mean train loss:  5.00515010e-03, bound:  3.15468907e-01\n",
      "Epoch: 23291 mean train loss:  5.00481622e-03, bound:  3.15468878e-01\n",
      "Epoch: 23292 mean train loss:  5.00446977e-03, bound:  3.15468878e-01\n",
      "Epoch: 23293 mean train loss:  5.00416011e-03, bound:  3.15468878e-01\n",
      "Epoch: 23294 mean train loss:  5.00393286e-03, bound:  3.15468818e-01\n",
      "Epoch: 23295 mean train loss:  5.00360643e-03, bound:  3.15468818e-01\n",
      "Epoch: 23296 mean train loss:  5.00328001e-03, bound:  3.15468758e-01\n",
      "Epoch: 23297 mean train loss:  5.00303833e-03, bound:  3.15468758e-01\n",
      "Epoch: 23298 mean train loss:  5.00275008e-03, bound:  3.15468758e-01\n",
      "Epoch: 23299 mean train loss:  5.00241248e-03, bound:  3.15468758e-01\n",
      "Epoch: 23300 mean train loss:  5.00213541e-03, bound:  3.15468699e-01\n",
      "Epoch: 23301 mean train loss:  5.00182761e-03, bound:  3.15468669e-01\n",
      "Epoch: 23302 mean train loss:  5.00148721e-03, bound:  3.15468669e-01\n",
      "Epoch: 23303 mean train loss:  5.00125904e-03, bound:  3.15468639e-01\n",
      "Epoch: 23304 mean train loss:  5.00102248e-03, bound:  3.15468639e-01\n",
      "Epoch: 23305 mean train loss:  5.00073610e-03, bound:  3.15468580e-01\n",
      "Epoch: 23306 mean train loss:  5.00041433e-03, bound:  3.15468580e-01\n",
      "Epoch: 23307 mean train loss:  5.00023225e-03, bound:  3.15468580e-01\n",
      "Epoch: 23308 mean train loss:  4.99997800e-03, bound:  3.15468520e-01\n",
      "Epoch: 23309 mean train loss:  4.99972608e-03, bound:  3.15468520e-01\n",
      "Epoch: 23310 mean train loss:  4.99957101e-03, bound:  3.15468520e-01\n",
      "Epoch: 23311 mean train loss:  4.99947835e-03, bound:  3.15468490e-01\n",
      "Epoch: 23312 mean train loss:  4.99943458e-03, bound:  3.15468460e-01\n",
      "Epoch: 23313 mean train loss:  4.99948533e-03, bound:  3.15468460e-01\n",
      "Epoch: 23314 mean train loss:  4.99971770e-03, bound:  3.15468401e-01\n",
      "Epoch: 23315 mean train loss:  5.00004971e-03, bound:  3.15468431e-01\n",
      "Epoch: 23316 mean train loss:  5.00063365e-03, bound:  3.15468341e-01\n",
      "Epoch: 23317 mean train loss:  5.00143599e-03, bound:  3.15468341e-01\n",
      "Epoch: 23318 mean train loss:  5.00264158e-03, bound:  3.15468311e-01\n",
      "Epoch: 23319 mean train loss:  5.00416709e-03, bound:  3.15468341e-01\n",
      "Epoch: 23320 mean train loss:  5.00588398e-03, bound:  3.15468282e-01\n",
      "Epoch: 23321 mean train loss:  5.00754267e-03, bound:  3.15468311e-01\n",
      "Epoch: 23322 mean train loss:  5.00876317e-03, bound:  3.15468192e-01\n",
      "Epoch: 23323 mean train loss:  5.00917295e-03, bound:  3.15468282e-01\n",
      "Epoch: 23324 mean train loss:  5.00845583e-03, bound:  3.15468192e-01\n",
      "Epoch: 23325 mean train loss:  5.00606792e-03, bound:  3.15468222e-01\n",
      "Epoch: 23326 mean train loss:  5.00255451e-03, bound:  3.15468192e-01\n",
      "Epoch: 23327 mean train loss:  4.99857776e-03, bound:  3.15468192e-01\n",
      "Epoch: 23328 mean train loss:  4.99535305e-03, bound:  3.15468132e-01\n",
      "Epoch: 23329 mean train loss:  4.99342522e-03, bound:  3.15468132e-01\n",
      "Epoch: 23330 mean train loss:  4.99307225e-03, bound:  3.15468103e-01\n",
      "Epoch: 23331 mean train loss:  4.99384105e-03, bound:  3.15468073e-01\n",
      "Epoch: 23332 mean train loss:  4.99499356e-03, bound:  3.15468073e-01\n",
      "Epoch: 23333 mean train loss:  4.99583036e-03, bound:  3.15468073e-01\n",
      "Epoch: 23334 mean train loss:  4.99594351e-03, bound:  3.15468073e-01\n",
      "Epoch: 23335 mean train loss:  4.99524223e-03, bound:  3.15467983e-01\n",
      "Epoch: 23336 mean train loss:  4.99387039e-03, bound:  3.15468013e-01\n",
      "Epoch: 23337 mean train loss:  4.99230390e-03, bound:  3.15467954e-01\n",
      "Epoch: 23338 mean train loss:  4.99105267e-03, bound:  3.15467954e-01\n",
      "Epoch: 23339 mean train loss:  4.99034626e-03, bound:  3.15467954e-01\n",
      "Epoch: 23340 mean train loss:  4.99010086e-03, bound:  3.15467894e-01\n",
      "Epoch: 23341 mean train loss:  4.99019679e-03, bound:  3.15467894e-01\n",
      "Epoch: 23342 mean train loss:  4.99039283e-03, bound:  3.15467864e-01\n",
      "Epoch: 23343 mean train loss:  4.99048503e-03, bound:  3.15467864e-01\n",
      "Epoch: 23344 mean train loss:  4.99023916e-03, bound:  3.15467834e-01\n",
      "Epoch: 23345 mean train loss:  4.98969201e-03, bound:  3.15467864e-01\n",
      "Epoch: 23346 mean train loss:  4.98902611e-03, bound:  3.15467775e-01\n",
      "Epoch: 23347 mean train loss:  4.98834439e-03, bound:  3.15467775e-01\n",
      "Epoch: 23348 mean train loss:  4.98779444e-03, bound:  3.15467745e-01\n",
      "Epoch: 23349 mean train loss:  4.98732785e-03, bound:  3.15467745e-01\n",
      "Epoch: 23350 mean train loss:  4.98697301e-03, bound:  3.15467745e-01\n",
      "Epoch: 23351 mean train loss:  4.98686964e-03, bound:  3.15467715e-01\n",
      "Epoch: 23352 mean train loss:  4.98673692e-03, bound:  3.15467656e-01\n",
      "Epoch: 23353 mean train loss:  4.98652318e-03, bound:  3.15467656e-01\n",
      "Epoch: 23354 mean train loss:  4.98629455e-03, bound:  3.15467656e-01\n",
      "Epoch: 23355 mean train loss:  4.98592202e-03, bound:  3.15467626e-01\n",
      "Epoch: 23356 mean train loss:  4.98552574e-03, bound:  3.15467626e-01\n",
      "Epoch: 23357 mean train loss:  4.98512294e-03, bound:  3.15467536e-01\n",
      "Epoch: 23358 mean train loss:  4.98466566e-03, bound:  3.15467536e-01\n",
      "Epoch: 23359 mean train loss:  4.98430105e-03, bound:  3.15467536e-01\n",
      "Epoch: 23360 mean train loss:  4.98400023e-03, bound:  3.15467507e-01\n",
      "Epoch: 23361 mean train loss:  4.98381117e-03, bound:  3.15467507e-01\n",
      "Epoch: 23362 mean train loss:  4.98354109e-03, bound:  3.15467447e-01\n",
      "Epoch: 23363 mean train loss:  4.98327147e-03, bound:  3.15467447e-01\n",
      "Epoch: 23364 mean train loss:  4.98300185e-03, bound:  3.15467447e-01\n",
      "Epoch: 23365 mean train loss:  4.98267962e-03, bound:  3.15467447e-01\n",
      "Epoch: 23366 mean train loss:  4.98236064e-03, bound:  3.15467387e-01\n",
      "Epoch: 23367 mean train loss:  4.98199416e-03, bound:  3.15467328e-01\n",
      "Epoch: 23368 mean train loss:  4.98168403e-03, bound:  3.15467328e-01\n",
      "Epoch: 23369 mean train loss:  4.98134270e-03, bound:  3.15467328e-01\n",
      "Epoch: 23370 mean train loss:  4.98103118e-03, bound:  3.15467328e-01\n",
      "Epoch: 23371 mean train loss:  4.98078810e-03, bound:  3.15467298e-01\n",
      "Epoch: 23372 mean train loss:  4.98046633e-03, bound:  3.15467268e-01\n",
      "Epoch: 23373 mean train loss:  4.98018507e-03, bound:  3.15467268e-01\n",
      "Epoch: 23374 mean train loss:  4.97992290e-03, bound:  3.15467268e-01\n",
      "Epoch: 23375 mean train loss:  4.97959042e-03, bound:  3.15467209e-01\n",
      "Epoch: 23376 mean train loss:  4.97938180e-03, bound:  3.15467209e-01\n",
      "Epoch: 23377 mean train loss:  4.97911172e-03, bound:  3.15467179e-01\n",
      "Epoch: 23378 mean train loss:  4.97876760e-03, bound:  3.15467179e-01\n",
      "Epoch: 23379 mean train loss:  4.97845467e-03, bound:  3.15467149e-01\n",
      "Epoch: 23380 mean train loss:  4.97808540e-03, bound:  3.15467149e-01\n",
      "Epoch: 23381 mean train loss:  4.97783814e-03, bound:  3.15467089e-01\n",
      "Epoch: 23382 mean train loss:  4.97754430e-03, bound:  3.15467089e-01\n",
      "Epoch: 23383 mean train loss:  4.97711776e-03, bound:  3.15467060e-01\n",
      "Epoch: 23384 mean train loss:  4.97688446e-03, bound:  3.15467060e-01\n",
      "Epoch: 23385 mean train loss:  4.97656036e-03, bound:  3.15467000e-01\n",
      "Epoch: 23386 mean train loss:  4.97624604e-03, bound:  3.15467000e-01\n",
      "Epoch: 23387 mean train loss:  4.97593777e-03, bound:  3.15466970e-01\n",
      "Epoch: 23388 mean train loss:  4.97565325e-03, bound:  3.15466940e-01\n",
      "Epoch: 23389 mean train loss:  4.97535663e-03, bound:  3.15466940e-01\n",
      "Epoch: 23390 mean train loss:  4.97501204e-03, bound:  3.15466940e-01\n",
      "Epoch: 23391 mean train loss:  4.97477408e-03, bound:  3.15466881e-01\n",
      "Epoch: 23392 mean train loss:  4.97450586e-03, bound:  3.15466881e-01\n",
      "Epoch: 23393 mean train loss:  4.97427210e-03, bound:  3.15466851e-01\n",
      "Epoch: 23394 mean train loss:  4.97389212e-03, bound:  3.15466851e-01\n",
      "Epoch: 23395 mean train loss:  4.97358758e-03, bound:  3.15466821e-01\n",
      "Epoch: 23396 mean train loss:  4.97331051e-03, bound:  3.15466821e-01\n",
      "Epoch: 23397 mean train loss:  4.97300085e-03, bound:  3.15466762e-01\n",
      "Epoch: 23398 mean train loss:  4.97273589e-03, bound:  3.15466762e-01\n",
      "Epoch: 23399 mean train loss:  4.97244578e-03, bound:  3.15466762e-01\n",
      "Epoch: 23400 mean train loss:  4.97211237e-03, bound:  3.15466732e-01\n",
      "Epoch: 23401 mean train loss:  4.97181062e-03, bound:  3.15466702e-01\n",
      "Epoch: 23402 mean train loss:  4.97152610e-03, bound:  3.15466702e-01\n",
      "Epoch: 23403 mean train loss:  4.97116521e-03, bound:  3.15466642e-01\n",
      "Epoch: 23404 mean train loss:  4.97090584e-03, bound:  3.15466642e-01\n",
      "Epoch: 23405 mean train loss:  4.97057941e-03, bound:  3.15466613e-01\n",
      "Epoch: 23406 mean train loss:  4.97028185e-03, bound:  3.15466613e-01\n",
      "Epoch: 23407 mean train loss:  4.97002807e-03, bound:  3.15466613e-01\n",
      "Epoch: 23408 mean train loss:  4.96966578e-03, bound:  3.15466523e-01\n",
      "Epoch: 23409 mean train loss:  4.96938452e-03, bound:  3.15466523e-01\n",
      "Epoch: 23410 mean train loss:  4.96907998e-03, bound:  3.15466523e-01\n",
      "Epoch: 23411 mean train loss:  4.96875262e-03, bound:  3.15466493e-01\n",
      "Epoch: 23412 mean train loss:  4.96842060e-03, bound:  3.15466493e-01\n",
      "Epoch: 23413 mean train loss:  4.96814167e-03, bound:  3.15466434e-01\n",
      "Epoch: 23414 mean train loss:  4.96785762e-03, bound:  3.15466434e-01\n",
      "Epoch: 23415 mean train loss:  4.96760663e-03, bound:  3.15466404e-01\n",
      "Epoch: 23416 mean train loss:  4.96737333e-03, bound:  3.15466404e-01\n",
      "Epoch: 23417 mean train loss:  4.96706739e-03, bound:  3.15466374e-01\n",
      "Epoch: 23418 mean train loss:  4.96688299e-03, bound:  3.15466374e-01\n",
      "Epoch: 23419 mean train loss:  4.96665481e-03, bound:  3.15466315e-01\n",
      "Epoch: 23420 mean train loss:  4.96647693e-03, bound:  3.15466315e-01\n",
      "Epoch: 23421 mean train loss:  4.96638939e-03, bound:  3.15466285e-01\n",
      "Epoch: 23422 mean train loss:  4.96630371e-03, bound:  3.15466285e-01\n",
      "Epoch: 23423 mean train loss:  4.96638846e-03, bound:  3.15466225e-01\n",
      "Epoch: 23424 mean train loss:  4.96657565e-03, bound:  3.15466225e-01\n",
      "Epoch: 23425 mean train loss:  4.96687042e-03, bound:  3.15466195e-01\n",
      "Epoch: 23426 mean train loss:  4.96733049e-03, bound:  3.15466195e-01\n",
      "Epoch: 23427 mean train loss:  4.96820407e-03, bound:  3.15466166e-01\n",
      "Epoch: 23428 mean train loss:  4.96941386e-03, bound:  3.15466166e-01\n",
      "Epoch: 23429 mean train loss:  4.97118384e-03, bound:  3.15466106e-01\n",
      "Epoch: 23430 mean train loss:  4.97350004e-03, bound:  3.15466136e-01\n",
      "Epoch: 23431 mean train loss:  4.97603742e-03, bound:  3.15466046e-01\n",
      "Epoch: 23432 mean train loss:  4.97839320e-03, bound:  3.15466136e-01\n",
      "Epoch: 23433 mean train loss:  4.98001976e-03, bound:  3.15466017e-01\n",
      "Epoch: 23434 mean train loss:  4.97993175e-03, bound:  3.15466076e-01\n",
      "Epoch: 23435 mean train loss:  4.97778645e-03, bound:  3.15465957e-01\n",
      "Epoch: 23436 mean train loss:  4.97353775e-03, bound:  3.15466046e-01\n",
      "Epoch: 23437 mean train loss:  4.96822968e-03, bound:  3.15465957e-01\n",
      "Epoch: 23438 mean train loss:  4.96349437e-03, bound:  3.15465987e-01\n",
      "Epoch: 23439 mean train loss:  4.96063195e-03, bound:  3.15465927e-01\n",
      "Epoch: 23440 mean train loss:  4.96030785e-03, bound:  3.15465927e-01\n",
      "Epoch: 23441 mean train loss:  4.96175792e-03, bound:  3.15465927e-01\n",
      "Epoch: 23442 mean train loss:  4.96370578e-03, bound:  3.15465838e-01\n",
      "Epoch: 23443 mean train loss:  4.96501010e-03, bound:  3.15465927e-01\n",
      "Epoch: 23444 mean train loss:  4.96489694e-03, bound:  3.15465838e-01\n",
      "Epoch: 23445 mean train loss:  4.96340543e-03, bound:  3.15465838e-01\n",
      "Epoch: 23446 mean train loss:  4.96114558e-03, bound:  3.15465808e-01\n",
      "Epoch: 23447 mean train loss:  4.95904312e-03, bound:  3.15465838e-01\n",
      "Epoch: 23448 mean train loss:  4.95777419e-03, bound:  3.15465778e-01\n",
      "Epoch: 23449 mean train loss:  4.95772250e-03, bound:  3.15465719e-01\n",
      "Epoch: 23450 mean train loss:  4.95819282e-03, bound:  3.15465719e-01\n",
      "Epoch: 23451 mean train loss:  4.95889736e-03, bound:  3.15465719e-01\n",
      "Epoch: 23452 mean train loss:  4.95893648e-03, bound:  3.15465719e-01\n",
      "Epoch: 23453 mean train loss:  4.95841121e-03, bound:  3.15465659e-01\n",
      "Epoch: 23454 mean train loss:  4.95739980e-03, bound:  3.15465659e-01\n",
      "Epoch: 23455 mean train loss:  4.95632971e-03, bound:  3.15465629e-01\n",
      "Epoch: 23456 mean train loss:  4.95556509e-03, bound:  3.15465629e-01\n",
      "Epoch: 23457 mean train loss:  4.95514320e-03, bound:  3.15465629e-01\n",
      "Epoch: 23458 mean train loss:  4.95520234e-03, bound:  3.15465599e-01\n",
      "Epoch: 23459 mean train loss:  4.95529175e-03, bound:  3.15465569e-01\n",
      "Epoch: 23460 mean train loss:  4.95524146e-03, bound:  3.15465510e-01\n",
      "Epoch: 23461 mean train loss:  4.95498301e-03, bound:  3.15465510e-01\n",
      "Epoch: 23462 mean train loss:  4.95454110e-03, bound:  3.15465510e-01\n",
      "Epoch: 23463 mean train loss:  4.95385146e-03, bound:  3.15465510e-01\n",
      "Epoch: 23464 mean train loss:  4.95331967e-03, bound:  3.15465450e-01\n",
      "Epoch: 23465 mean train loss:  4.95285261e-03, bound:  3.15465450e-01\n",
      "Epoch: 23466 mean train loss:  4.95253736e-03, bound:  3.15465420e-01\n",
      "Epoch: 23467 mean train loss:  4.95239720e-03, bound:  3.15465391e-01\n",
      "Epoch: 23468 mean train loss:  4.95225331e-03, bound:  3.15465391e-01\n",
      "Epoch: 23469 mean train loss:  4.95205121e-03, bound:  3.15465361e-01\n",
      "Epoch: 23470 mean train loss:  4.95178625e-03, bound:  3.15465331e-01\n",
      "Epoch: 23471 mean train loss:  4.95135272e-03, bound:  3.15465331e-01\n",
      "Epoch: 23472 mean train loss:  4.95099183e-03, bound:  3.15465301e-01\n",
      "Epoch: 23473 mean train loss:  4.95056761e-03, bound:  3.15465271e-01\n",
      "Epoch: 23474 mean train loss:  4.95021837e-03, bound:  3.15465271e-01\n",
      "Epoch: 23475 mean train loss:  4.94993525e-03, bound:  3.15465242e-01\n",
      "Epoch: 23476 mean train loss:  4.94966935e-03, bound:  3.15465242e-01\n",
      "Epoch: 23477 mean train loss:  4.94948821e-03, bound:  3.15465212e-01\n",
      "Epoch: 23478 mean train loss:  4.94917016e-03, bound:  3.15465182e-01\n",
      "Epoch: 23479 mean train loss:  4.94889263e-03, bound:  3.15465152e-01\n",
      "Epoch: 23480 mean train loss:  4.94862907e-03, bound:  3.15465152e-01\n",
      "Epoch: 23481 mean train loss:  4.94826352e-03, bound:  3.15465122e-01\n",
      "Epoch: 23482 mean train loss:  4.94797854e-03, bound:  3.15465122e-01\n",
      "Epoch: 23483 mean train loss:  4.94766375e-03, bound:  3.15465093e-01\n",
      "Epoch: 23484 mean train loss:  4.94731870e-03, bound:  3.15465063e-01\n",
      "Epoch: 23485 mean train loss:  4.94702114e-03, bound:  3.15465033e-01\n",
      "Epoch: 23486 mean train loss:  4.94672870e-03, bound:  3.15465033e-01\n",
      "Epoch: 23487 mean train loss:  4.94643580e-03, bound:  3.15465033e-01\n",
      "Epoch: 23488 mean train loss:  4.94612847e-03, bound:  3.15465003e-01\n",
      "Epoch: 23489 mean train loss:  4.94595477e-03, bound:  3.15464973e-01\n",
      "Epoch: 23490 mean train loss:  4.94557852e-03, bound:  3.15464944e-01\n",
      "Epoch: 23491 mean train loss:  4.94530238e-03, bound:  3.15464944e-01\n",
      "Epoch: 23492 mean train loss:  4.94507980e-03, bound:  3.15464914e-01\n",
      "Epoch: 23493 mean train loss:  4.94471099e-03, bound:  3.15464914e-01\n",
      "Epoch: 23494 mean train loss:  4.94443346e-03, bound:  3.15464854e-01\n",
      "Epoch: 23495 mean train loss:  4.94410610e-03, bound:  3.15464854e-01\n",
      "Epoch: 23496 mean train loss:  4.94379923e-03, bound:  3.15464824e-01\n",
      "Epoch: 23497 mean train loss:  4.94350074e-03, bound:  3.15464824e-01\n",
      "Epoch: 23498 mean train loss:  4.94320551e-03, bound:  3.15464824e-01\n",
      "Epoch: 23499 mean train loss:  4.94288420e-03, bound:  3.15464795e-01\n",
      "Epoch: 23500 mean train loss:  4.94264159e-03, bound:  3.15464735e-01\n",
      "Epoch: 23501 mean train loss:  4.94234543e-03, bound:  3.15464735e-01\n",
      "Epoch: 23502 mean train loss:  4.94204555e-03, bound:  3.15464705e-01\n",
      "Epoch: 23503 mean train loss:  4.94173402e-03, bound:  3.15464705e-01\n",
      "Epoch: 23504 mean train loss:  4.94147325e-03, bound:  3.15464705e-01\n",
      "Epoch: 23505 mean train loss:  4.94115474e-03, bound:  3.15464675e-01\n",
      "Epoch: 23506 mean train loss:  4.94090421e-03, bound:  3.15464616e-01\n",
      "Epoch: 23507 mean train loss:  4.94062994e-03, bound:  3.15464616e-01\n",
      "Epoch: 23508 mean train loss:  4.94032400e-03, bound:  3.15464616e-01\n",
      "Epoch: 23509 mean train loss:  4.94003389e-03, bound:  3.15464586e-01\n",
      "Epoch: 23510 mean train loss:  4.93975356e-03, bound:  3.15464526e-01\n",
      "Epoch: 23511 mean train loss:  4.93939780e-03, bound:  3.15464526e-01\n",
      "Epoch: 23512 mean train loss:  4.93921014e-03, bound:  3.15464526e-01\n",
      "Epoch: 23513 mean train loss:  4.93886322e-03, bound:  3.15464497e-01\n",
      "Epoch: 23514 mean train loss:  4.93854983e-03, bound:  3.15464467e-01\n",
      "Epoch: 23515 mean train loss:  4.93827276e-03, bound:  3.15464467e-01\n",
      "Epoch: 23516 mean train loss:  4.93794354e-03, bound:  3.15464407e-01\n",
      "Epoch: 23517 mean train loss:  4.93763713e-03, bound:  3.15464407e-01\n",
      "Epoch: 23518 mean train loss:  4.93737822e-03, bound:  3.15464407e-01\n",
      "Epoch: 23519 mean train loss:  4.93710488e-03, bound:  3.15464407e-01\n",
      "Epoch: 23520 mean train loss:  4.93683852e-03, bound:  3.15464377e-01\n",
      "Epoch: 23521 mean train loss:  4.93649347e-03, bound:  3.15464348e-01\n",
      "Epoch: 23522 mean train loss:  4.93616797e-03, bound:  3.15464288e-01\n",
      "Epoch: 23523 mean train loss:  4.93595563e-03, bound:  3.15464288e-01\n",
      "Epoch: 23524 mean train loss:  4.93560173e-03, bound:  3.15464288e-01\n",
      "Epoch: 23525 mean train loss:  4.93536051e-03, bound:  3.15464258e-01\n",
      "Epoch: 23526 mean train loss:  4.93506575e-03, bound:  3.15464258e-01\n",
      "Epoch: 23527 mean train loss:  4.93475236e-03, bound:  3.15464228e-01\n",
      "Epoch: 23528 mean train loss:  4.93444875e-03, bound:  3.15464228e-01\n",
      "Epoch: 23529 mean train loss:  4.93414374e-03, bound:  3.15464169e-01\n",
      "Epoch: 23530 mean train loss:  4.93380800e-03, bound:  3.15464169e-01\n",
      "Epoch: 23531 mean train loss:  4.93356120e-03, bound:  3.15464139e-01\n",
      "Epoch: 23532 mean train loss:  4.93324501e-03, bound:  3.15464139e-01\n",
      "Epoch: 23533 mean train loss:  4.93295165e-03, bound:  3.15464079e-01\n",
      "Epoch: 23534 mean train loss:  4.93270066e-03, bound:  3.15464079e-01\n",
      "Epoch: 23535 mean train loss:  4.93243150e-03, bound:  3.15464050e-01\n",
      "Epoch: 23536 mean train loss:  4.93210368e-03, bound:  3.15464020e-01\n",
      "Epoch: 23537 mean train loss:  4.93182661e-03, bound:  3.15464020e-01\n",
      "Epoch: 23538 mean train loss:  4.93146013e-03, bound:  3.15464020e-01\n",
      "Epoch: 23539 mean train loss:  4.93116770e-03, bound:  3.15463960e-01\n",
      "Epoch: 23540 mean train loss:  4.93089948e-03, bound:  3.15463960e-01\n",
      "Epoch: 23541 mean train loss:  4.93063685e-03, bound:  3.15463930e-01\n",
      "Epoch: 23542 mean train loss:  4.93031740e-03, bound:  3.15463901e-01\n",
      "Epoch: 23543 mean train loss:  4.93001333e-03, bound:  3.15463901e-01\n",
      "Epoch: 23544 mean train loss:  4.92970087e-03, bound:  3.15463901e-01\n",
      "Epoch: 23545 mean train loss:  4.92941868e-03, bound:  3.15463841e-01\n",
      "Epoch: 23546 mean train loss:  4.92911320e-03, bound:  3.15463841e-01\n",
      "Epoch: 23547 mean train loss:  4.92882822e-03, bound:  3.15463811e-01\n",
      "Epoch: 23548 mean train loss:  4.92855581e-03, bound:  3.15463811e-01\n",
      "Epoch: 23549 mean train loss:  4.92824661e-03, bound:  3.15463781e-01\n",
      "Epoch: 23550 mean train loss:  4.92802495e-03, bound:  3.15463781e-01\n",
      "Epoch: 23551 mean train loss:  4.92775720e-03, bound:  3.15463722e-01\n",
      "Epoch: 23552 mean train loss:  4.92753508e-03, bound:  3.15463722e-01\n",
      "Epoch: 23553 mean train loss:  4.92730457e-03, bound:  3.15463692e-01\n",
      "Epoch: 23554 mean train loss:  4.92706802e-03, bound:  3.15463692e-01\n",
      "Epoch: 23555 mean train loss:  4.92693530e-03, bound:  3.15463632e-01\n",
      "Epoch: 23556 mean train loss:  4.92687058e-03, bound:  3.15463692e-01\n",
      "Epoch: 23557 mean train loss:  4.92691668e-03, bound:  3.15463603e-01\n",
      "Epoch: 23558 mean train loss:  4.92710946e-03, bound:  3.15463603e-01\n",
      "Epoch: 23559 mean train loss:  4.92759189e-03, bound:  3.15463573e-01\n",
      "Epoch: 23560 mean train loss:  4.92851064e-03, bound:  3.15463603e-01\n",
      "Epoch: 23561 mean train loss:  4.92986711e-03, bound:  3.15463483e-01\n",
      "Epoch: 23562 mean train loss:  4.93191183e-03, bound:  3.15463573e-01\n",
      "Epoch: 23563 mean train loss:  4.93458379e-03, bound:  3.15463454e-01\n",
      "Epoch: 23564 mean train loss:  4.93812561e-03, bound:  3.15463483e-01\n",
      "Epoch: 23565 mean train loss:  4.94214194e-03, bound:  3.15463394e-01\n",
      "Epoch: 23566 mean train loss:  4.94582718e-03, bound:  3.15463483e-01\n",
      "Epoch: 23567 mean train loss:  4.94772149e-03, bound:  3.15463394e-01\n",
      "Epoch: 23568 mean train loss:  4.94635664e-03, bound:  3.15463454e-01\n",
      "Epoch: 23569 mean train loss:  4.94108116e-03, bound:  3.15463334e-01\n",
      "Epoch: 23570 mean train loss:  4.93325340e-03, bound:  3.15463394e-01\n",
      "Epoch: 23571 mean train loss:  4.92587266e-03, bound:  3.15463334e-01\n",
      "Epoch: 23572 mean train loss:  4.92177904e-03, bound:  3.15463364e-01\n",
      "Epoch: 23573 mean train loss:  4.92201326e-03, bound:  3.15463334e-01\n",
      "Epoch: 23574 mean train loss:  4.92505031e-03, bound:  3.15463275e-01\n",
      "Epoch: 23575 mean train loss:  4.92820749e-03, bound:  3.15463334e-01\n",
      "Epoch: 23576 mean train loss:  4.92928876e-03, bound:  3.15463215e-01\n",
      "Epoch: 23577 mean train loss:  4.92772227e-03, bound:  3.15463275e-01\n",
      "Epoch: 23578 mean train loss:  4.92410688e-03, bound:  3.15463215e-01\n",
      "Epoch: 23579 mean train loss:  4.92079277e-03, bound:  3.15463215e-01\n",
      "Epoch: 23580 mean train loss:  4.91922768e-03, bound:  3.15463185e-01\n",
      "Epoch: 23581 mean train loss:  4.91980184e-03, bound:  3.15463156e-01\n",
      "Epoch: 23582 mean train loss:  4.92131105e-03, bound:  3.15463156e-01\n",
      "Epoch: 23583 mean train loss:  4.92237275e-03, bound:  3.15463126e-01\n",
      "Epoch: 23584 mean train loss:  4.92211524e-03, bound:  3.15463126e-01\n",
      "Epoch: 23585 mean train loss:  4.92067821e-03, bound:  3.15463126e-01\n",
      "Epoch: 23586 mean train loss:  4.91883559e-03, bound:  3.15463126e-01\n",
      "Epoch: 23587 mean train loss:  4.91749914e-03, bound:  3.15463036e-01\n",
      "Epoch: 23588 mean train loss:  4.91708191e-03, bound:  3.15463036e-01\n",
      "Epoch: 23589 mean train loss:  4.91758855e-03, bound:  3.15463036e-01\n",
      "Epoch: 23590 mean train loss:  4.91808355e-03, bound:  3.15463006e-01\n",
      "Epoch: 23591 mean train loss:  4.91814595e-03, bound:  3.15463006e-01\n",
      "Epoch: 23592 mean train loss:  4.91747446e-03, bound:  3.15462947e-01\n",
      "Epoch: 23593 mean train loss:  4.91642021e-03, bound:  3.15462947e-01\n",
      "Epoch: 23594 mean train loss:  4.91546653e-03, bound:  3.15462917e-01\n",
      "Epoch: 23595 mean train loss:  4.91508795e-03, bound:  3.15462887e-01\n",
      "Epoch: 23596 mean train loss:  4.91505023e-03, bound:  3.15462887e-01\n",
      "Epoch: 23597 mean train loss:  4.91517223e-03, bound:  3.15462887e-01\n",
      "Epoch: 23598 mean train loss:  4.91514802e-03, bound:  3.15462887e-01\n",
      "Epoch: 23599 mean train loss:  4.91482439e-03, bound:  3.15462828e-01\n",
      "Epoch: 23600 mean train loss:  4.91427816e-03, bound:  3.15462828e-01\n",
      "Epoch: 23601 mean train loss:  4.91357641e-03, bound:  3.15462798e-01\n",
      "Epoch: 23602 mean train loss:  4.91306093e-03, bound:  3.15462798e-01\n",
      "Epoch: 23603 mean train loss:  4.91281087e-03, bound:  3.15462738e-01\n",
      "Epoch: 23604 mean train loss:  4.91265673e-03, bound:  3.15462738e-01\n",
      "Epoch: 23605 mean train loss:  4.91256081e-03, bound:  3.15462708e-01\n",
      "Epoch: 23606 mean train loss:  4.91240481e-03, bound:  3.15462708e-01\n",
      "Epoch: 23607 mean train loss:  4.91198385e-03, bound:  3.15462679e-01\n",
      "Epoch: 23608 mean train loss:  4.91160899e-03, bound:  3.15462619e-01\n",
      "Epoch: 23609 mean train loss:  4.91118571e-03, bound:  3.15462619e-01\n",
      "Epoch: 23610 mean train loss:  4.91081504e-03, bound:  3.15462589e-01\n",
      "Epoch: 23611 mean train loss:  4.91055008e-03, bound:  3.15462589e-01\n",
      "Epoch: 23612 mean train loss:  4.91028745e-03, bound:  3.15462589e-01\n",
      "Epoch: 23613 mean train loss:  4.91002807e-03, bound:  3.15462559e-01\n",
      "Epoch: 23614 mean train loss:  4.90985299e-03, bound:  3.15462559e-01\n",
      "Epoch: 23615 mean train loss:  4.90954844e-03, bound:  3.15462500e-01\n",
      "Epoch: 23616 mean train loss:  4.90918895e-03, bound:  3.15462500e-01\n",
      "Epoch: 23617 mean train loss:  4.90886811e-03, bound:  3.15462470e-01\n",
      "Epoch: 23618 mean train loss:  4.90855891e-03, bound:  3.15462470e-01\n",
      "Epoch: 23619 mean train loss:  4.90826648e-03, bound:  3.15462440e-01\n",
      "Epoch: 23620 mean train loss:  4.90797684e-03, bound:  3.15462410e-01\n",
      "Epoch: 23621 mean train loss:  4.90778638e-03, bound:  3.15462381e-01\n",
      "Epoch: 23622 mean train loss:  4.90744133e-03, bound:  3.15462381e-01\n",
      "Epoch: 23623 mean train loss:  4.90720756e-03, bound:  3.15462351e-01\n",
      "Epoch: 23624 mean train loss:  4.90688113e-03, bound:  3.15462351e-01\n",
      "Epoch: 23625 mean train loss:  4.90661524e-03, bound:  3.15462321e-01\n",
      "Epoch: 23626 mean train loss:  4.90634190e-03, bound:  3.15462291e-01\n",
      "Epoch: 23627 mean train loss:  4.90601175e-03, bound:  3.15462261e-01\n",
      "Epoch: 23628 mean train loss:  4.90574678e-03, bound:  3.15462261e-01\n",
      "Epoch: 23629 mean train loss:  4.90546599e-03, bound:  3.15462232e-01\n",
      "Epoch: 23630 mean train loss:  4.90519218e-03, bound:  3.15462232e-01\n",
      "Epoch: 23631 mean train loss:  4.90488531e-03, bound:  3.15462202e-01\n",
      "Epoch: 23632 mean train loss:  4.90459846e-03, bound:  3.15462202e-01\n",
      "Epoch: 23633 mean train loss:  4.90436843e-03, bound:  3.15462172e-01\n",
      "Epoch: 23634 mean train loss:  4.90404060e-03, bound:  3.15462172e-01\n",
      "Epoch: 23635 mean train loss:  4.90380824e-03, bound:  3.15462142e-01\n",
      "Epoch: 23636 mean train loss:  4.90346877e-03, bound:  3.15462142e-01\n",
      "Epoch: 23637 mean train loss:  4.90324711e-03, bound:  3.15462083e-01\n",
      "Epoch: 23638 mean train loss:  4.90292162e-03, bound:  3.15462083e-01\n",
      "Epoch: 23639 mean train loss:  4.90267714e-03, bound:  3.15462053e-01\n",
      "Epoch: 23640 mean train loss:  4.90237633e-03, bound:  3.15462053e-01\n",
      "Epoch: 23641 mean train loss:  4.90208436e-03, bound:  3.15462023e-01\n",
      "Epoch: 23642 mean train loss:  4.90179891e-03, bound:  3.15462023e-01\n",
      "Epoch: 23643 mean train loss:  4.90157679e-03, bound:  3.15461993e-01\n",
      "Epoch: 23644 mean train loss:  4.90118563e-03, bound:  3.15461993e-01\n",
      "Epoch: 23645 mean train loss:  4.90095466e-03, bound:  3.15461934e-01\n",
      "Epoch: 23646 mean train loss:  4.90063382e-03, bound:  3.15461934e-01\n",
      "Epoch: 23647 mean train loss:  4.90036281e-03, bound:  3.15461904e-01\n",
      "Epoch: 23648 mean train loss:  4.90010856e-03, bound:  3.15461904e-01\n",
      "Epoch: 23649 mean train loss:  4.89979191e-03, bound:  3.15461874e-01\n",
      "Epoch: 23650 mean train loss:  4.89948411e-03, bound:  3.15461874e-01\n",
      "Epoch: 23651 mean train loss:  4.89922287e-03, bound:  3.15461814e-01\n",
      "Epoch: 23652 mean train loss:  4.89899656e-03, bound:  3.15461814e-01\n",
      "Epoch: 23653 mean train loss:  4.89871856e-03, bound:  3.15461785e-01\n",
      "Epoch: 23654 mean train loss:  4.89840657e-03, bound:  3.15461785e-01\n",
      "Epoch: 23655 mean train loss:  4.89810621e-03, bound:  3.15461755e-01\n",
      "Epoch: 23656 mean train loss:  4.89775930e-03, bound:  3.15461725e-01\n",
      "Epoch: 23657 mean train loss:  4.89755999e-03, bound:  3.15461695e-01\n",
      "Epoch: 23658 mean train loss:  4.89724195e-03, bound:  3.15461665e-01\n",
      "Epoch: 23659 mean train loss:  4.89698816e-03, bound:  3.15461665e-01\n",
      "Epoch: 23660 mean train loss:  4.89662541e-03, bound:  3.15461665e-01\n",
      "Epoch: 23661 mean train loss:  4.89641633e-03, bound:  3.15461636e-01\n",
      "Epoch: 23662 mean train loss:  4.89608804e-03, bound:  3.15461606e-01\n",
      "Epoch: 23663 mean train loss:  4.89580818e-03, bound:  3.15461576e-01\n",
      "Epoch: 23664 mean train loss:  4.89558186e-03, bound:  3.15461576e-01\n",
      "Epoch: 23665 mean train loss:  4.89528943e-03, bound:  3.15461546e-01\n",
      "Epoch: 23666 mean train loss:  4.89497464e-03, bound:  3.15461546e-01\n",
      "Epoch: 23667 mean train loss:  4.89473995e-03, bound:  3.15461487e-01\n",
      "Epoch: 23668 mean train loss:  4.89441631e-03, bound:  3.15461457e-01\n",
      "Epoch: 23669 mean train loss:  4.89415089e-03, bound:  3.15461457e-01\n",
      "Epoch: 23670 mean train loss:  4.89383377e-03, bound:  3.15461457e-01\n",
      "Epoch: 23671 mean train loss:  4.89351060e-03, bound:  3.15461457e-01\n",
      "Epoch: 23672 mean train loss:  4.89329081e-03, bound:  3.15461367e-01\n",
      "Epoch: 23673 mean train loss:  4.89303144e-03, bound:  3.15461367e-01\n",
      "Epoch: 23674 mean train loss:  4.89270687e-03, bound:  3.15461367e-01\n",
      "Epoch: 23675 mean train loss:  4.89241071e-03, bound:  3.15461367e-01\n",
      "Epoch: 23676 mean train loss:  4.89217322e-03, bound:  3.15461338e-01\n",
      "Epoch: 23677 mean train loss:  4.89188125e-03, bound:  3.15461338e-01\n",
      "Epoch: 23678 mean train loss:  4.89157205e-03, bound:  3.15461248e-01\n",
      "Epoch: 23679 mean train loss:  4.89129731e-03, bound:  3.15461248e-01\n",
      "Epoch: 23680 mean train loss:  4.89102863e-03, bound:  3.15461248e-01\n",
      "Epoch: 23681 mean train loss:  4.89071151e-03, bound:  3.15461248e-01\n",
      "Epoch: 23682 mean train loss:  4.89044143e-03, bound:  3.15461218e-01\n",
      "Epoch: 23683 mean train loss:  4.89009824e-03, bound:  3.15461218e-01\n",
      "Epoch: 23684 mean train loss:  4.88984631e-03, bound:  3.15461159e-01\n",
      "Epoch: 23685 mean train loss:  4.88955341e-03, bound:  3.15461129e-01\n",
      "Epoch: 23686 mean train loss:  4.88924095e-03, bound:  3.15461129e-01\n",
      "Epoch: 23687 mean train loss:  4.88890382e-03, bound:  3.15461129e-01\n",
      "Epoch: 23688 mean train loss:  4.88863699e-03, bound:  3.15461099e-01\n",
      "Epoch: 23689 mean train loss:  4.88840928e-03, bound:  3.15461099e-01\n",
      "Epoch: 23690 mean train loss:  4.88808425e-03, bound:  3.15461040e-01\n",
      "Epoch: 23691 mean train loss:  4.88782628e-03, bound:  3.15461040e-01\n",
      "Epoch: 23692 mean train loss:  4.88752266e-03, bound:  3.15461010e-01\n",
      "Epoch: 23693 mean train loss:  4.88723209e-03, bound:  3.15461010e-01\n",
      "Epoch: 23694 mean train loss:  4.88698157e-03, bound:  3.15460980e-01\n",
      "Epoch: 23695 mean train loss:  4.88667563e-03, bound:  3.15460920e-01\n",
      "Epoch: 23696 mean train loss:  4.88631893e-03, bound:  3.15460920e-01\n",
      "Epoch: 23697 mean train loss:  4.88602184e-03, bound:  3.15460891e-01\n",
      "Epoch: 23698 mean train loss:  4.88586864e-03, bound:  3.15460891e-01\n",
      "Epoch: 23699 mean train loss:  4.88556130e-03, bound:  3.15460891e-01\n",
      "Epoch: 23700 mean train loss:  4.88521485e-03, bound:  3.15460891e-01\n",
      "Epoch: 23701 mean train loss:  4.88496432e-03, bound:  3.15460801e-01\n",
      "Epoch: 23702 mean train loss:  4.88468027e-03, bound:  3.15460801e-01\n",
      "Epoch: 23703 mean train loss:  4.88440506e-03, bound:  3.15460771e-01\n",
      "Epoch: 23704 mean train loss:  4.88414383e-03, bound:  3.15460771e-01\n",
      "Epoch: 23705 mean train loss:  4.88382298e-03, bound:  3.15460771e-01\n",
      "Epoch: 23706 mean train loss:  4.88357805e-03, bound:  3.15460771e-01\n",
      "Epoch: 23707 mean train loss:  4.88329446e-03, bound:  3.15460682e-01\n",
      "Epoch: 23708 mean train loss:  4.88301460e-03, bound:  3.15460682e-01\n",
      "Epoch: 23709 mean train loss:  4.88278922e-03, bound:  3.15460652e-01\n",
      "Epoch: 23710 mean train loss:  4.88262856e-03, bound:  3.15460652e-01\n",
      "Epoch: 23711 mean train loss:  4.88254288e-03, bound:  3.15460652e-01\n",
      "Epoch: 23712 mean train loss:  4.88240551e-03, bound:  3.15460593e-01\n",
      "Epoch: 23713 mean train loss:  4.88243531e-03, bound:  3.15460593e-01\n",
      "Epoch: 23714 mean train loss:  4.88259317e-03, bound:  3.15460563e-01\n",
      "Epoch: 23715 mean train loss:  4.88305651e-03, bound:  3.15460593e-01\n",
      "Epoch: 23716 mean train loss:  4.88382112e-03, bound:  3.15460533e-01\n",
      "Epoch: 23717 mean train loss:  4.88500297e-03, bound:  3.15460563e-01\n",
      "Epoch: 23718 mean train loss:  4.88682510e-03, bound:  3.15460473e-01\n",
      "Epoch: 23719 mean train loss:  4.88925492e-03, bound:  3.15460533e-01\n",
      "Epoch: 23720 mean train loss:  4.89252387e-03, bound:  3.15460443e-01\n",
      "Epoch: 23721 mean train loss:  4.89622960e-03, bound:  3.15460473e-01\n",
      "Epoch: 23722 mean train loss:  4.89980588e-03, bound:  3.15460354e-01\n",
      "Epoch: 23723 mean train loss:  4.90195490e-03, bound:  3.15460443e-01\n",
      "Epoch: 23724 mean train loss:  4.90105385e-03, bound:  3.15460354e-01\n",
      "Epoch: 23725 mean train loss:  4.89672413e-03, bound:  3.15460414e-01\n",
      "Epoch: 23726 mean train loss:  4.88976017e-03, bound:  3.15460324e-01\n",
      "Epoch: 23727 mean train loss:  4.88261972e-03, bound:  3.15460354e-01\n",
      "Epoch: 23728 mean train loss:  4.87797568e-03, bound:  3.15460324e-01\n",
      "Epoch: 23729 mean train loss:  4.87711327e-03, bound:  3.15460294e-01\n",
      "Epoch: 23730 mean train loss:  4.87932749e-03, bound:  3.15460265e-01\n",
      "Epoch: 23731 mean train loss:  4.88229189e-03, bound:  3.15460235e-01\n",
      "Epoch: 23732 mean train loss:  4.88408562e-03, bound:  3.15460265e-01\n",
      "Epoch: 23733 mean train loss:  4.88341926e-03, bound:  3.15460205e-01\n",
      "Epoch: 23734 mean train loss:  4.88078455e-03, bound:  3.15460235e-01\n",
      "Epoch: 23735 mean train loss:  4.87751514e-03, bound:  3.15460145e-01\n",
      "Epoch: 23736 mean train loss:  4.87523107e-03, bound:  3.15460145e-01\n",
      "Epoch: 23737 mean train loss:  4.87489300e-03, bound:  3.15460145e-01\n",
      "Epoch: 23738 mean train loss:  4.87593561e-03, bound:  3.15460116e-01\n",
      "Epoch: 23739 mean train loss:  4.87716729e-03, bound:  3.15460116e-01\n",
      "Epoch: 23740 mean train loss:  4.87764925e-03, bound:  3.15460086e-01\n",
      "Epoch: 23741 mean train loss:  4.87694936e-03, bound:  3.15460086e-01\n",
      "Epoch: 23742 mean train loss:  4.87537682e-03, bound:  3.15460026e-01\n",
      "Epoch: 23743 mean train loss:  4.87377774e-03, bound:  3.15460026e-01\n",
      "Epoch: 23744 mean train loss:  4.87286178e-03, bound:  3.15459996e-01\n",
      "Epoch: 23745 mean train loss:  4.87268670e-03, bound:  3.15459996e-01\n",
      "Epoch: 23746 mean train loss:  4.87318914e-03, bound:  3.15459996e-01\n",
      "Epoch: 23747 mean train loss:  4.87358449e-03, bound:  3.15459967e-01\n",
      "Epoch: 23748 mean train loss:  4.87351837e-03, bound:  3.15459967e-01\n",
      "Epoch: 23749 mean train loss:  4.87284549e-03, bound:  3.15459907e-01\n",
      "Epoch: 23750 mean train loss:  4.87193326e-03, bound:  3.15459907e-01\n",
      "Epoch: 23751 mean train loss:  4.87103686e-03, bound:  3.15459877e-01\n",
      "Epoch: 23752 mean train loss:  4.87062521e-03, bound:  3.15459847e-01\n",
      "Epoch: 23753 mean train loss:  4.87051299e-03, bound:  3.15459847e-01\n",
      "Epoch: 23754 mean train loss:  4.87062894e-03, bound:  3.15459818e-01\n",
      "Epoch: 23755 mean train loss:  4.87061171e-03, bound:  3.15459818e-01\n",
      "Epoch: 23756 mean train loss:  4.87040123e-03, bound:  3.15459788e-01\n",
      "Epoch: 23757 mean train loss:  4.86988155e-03, bound:  3.15459788e-01\n",
      "Epoch: 23758 mean train loss:  4.86929296e-03, bound:  3.15459728e-01\n",
      "Epoch: 23759 mean train loss:  4.86879982e-03, bound:  3.15459728e-01\n",
      "Epoch: 23760 mean train loss:  4.86845337e-03, bound:  3.15459698e-01\n",
      "Epoch: 23761 mean train loss:  4.86827968e-03, bound:  3.15459698e-01\n",
      "Epoch: 23762 mean train loss:  4.86807153e-03, bound:  3.15459669e-01\n",
      "Epoch: 23763 mean train loss:  4.86800587e-03, bound:  3.15459639e-01\n",
      "Epoch: 23764 mean train loss:  4.86769387e-03, bound:  3.15459639e-01\n",
      "Epoch: 23765 mean train loss:  4.86729434e-03, bound:  3.15459639e-01\n",
      "Epoch: 23766 mean train loss:  4.86689480e-03, bound:  3.15459579e-01\n",
      "Epoch: 23767 mean train loss:  4.86660469e-03, bound:  3.15459579e-01\n",
      "Epoch: 23768 mean train loss:  4.86618141e-03, bound:  3.15459549e-01\n",
      "Epoch: 23769 mean train loss:  4.86591877e-03, bound:  3.15459549e-01\n",
      "Epoch: 23770 mean train loss:  4.86569805e-03, bound:  3.15459520e-01\n",
      "Epoch: 23771 mean train loss:  4.86548059e-03, bound:  3.15459520e-01\n",
      "Epoch: 23772 mean train loss:  4.86531435e-03, bound:  3.15459460e-01\n",
      "Epoch: 23773 mean train loss:  4.86498093e-03, bound:  3.15459460e-01\n",
      "Epoch: 23774 mean train loss:  4.86462703e-03, bound:  3.15459430e-01\n",
      "Epoch: 23775 mean train loss:  4.86428663e-03, bound:  3.15459430e-01\n",
      "Epoch: 23776 mean train loss:  4.86405287e-03, bound:  3.15459400e-01\n",
      "Epoch: 23777 mean train loss:  4.86370362e-03, bound:  3.15459400e-01\n",
      "Epoch: 23778 mean train loss:  4.86345123e-03, bound:  3.15459341e-01\n",
      "Epoch: 23779 mean train loss:  4.86325053e-03, bound:  3.15459341e-01\n",
      "Epoch: 23780 mean train loss:  4.86299116e-03, bound:  3.15459341e-01\n",
      "Epoch: 23781 mean train loss:  4.86277742e-03, bound:  3.15459311e-01\n",
      "Epoch: 23782 mean train loss:  4.86242259e-03, bound:  3.15459281e-01\n",
      "Epoch: 23783 mean train loss:  4.86213434e-03, bound:  3.15459281e-01\n",
      "Epoch: 23784 mean train loss:  4.86183912e-03, bound:  3.15459222e-01\n",
      "Epoch: 23785 mean train loss:  4.86154947e-03, bound:  3.15459222e-01\n",
      "Epoch: 23786 mean train loss:  4.86128125e-03, bound:  3.15459222e-01\n",
      "Epoch: 23787 mean train loss:  4.86100418e-03, bound:  3.15459222e-01\n",
      "Epoch: 23788 mean train loss:  4.86068754e-03, bound:  3.15459162e-01\n",
      "Epoch: 23789 mean train loss:  4.86039231e-03, bound:  3.15459162e-01\n",
      "Epoch: 23790 mean train loss:  4.86011431e-03, bound:  3.15459132e-01\n",
      "Epoch: 23791 mean train loss:  4.85985307e-03, bound:  3.15459132e-01\n",
      "Epoch: 23792 mean train loss:  4.85953409e-03, bound:  3.15459102e-01\n",
      "Epoch: 23793 mean train loss:  4.85936739e-03, bound:  3.15459102e-01\n",
      "Epoch: 23794 mean train loss:  4.85903304e-03, bound:  3.15459043e-01\n",
      "Epoch: 23795 mean train loss:  4.85876855e-03, bound:  3.15459043e-01\n",
      "Epoch: 23796 mean train loss:  4.85844724e-03, bound:  3.15459013e-01\n",
      "Epoch: 23797 mean train loss:  4.85815480e-03, bound:  3.15459013e-01\n",
      "Epoch: 23798 mean train loss:  4.85792942e-03, bound:  3.15458983e-01\n",
      "Epoch: 23799 mean train loss:  4.85764211e-03, bound:  3.15458983e-01\n",
      "Epoch: 23800 mean train loss:  4.85743023e-03, bound:  3.15458924e-01\n",
      "Epoch: 23801 mean train loss:  4.85712895e-03, bound:  3.15458924e-01\n",
      "Epoch: 23802 mean train loss:  4.85683838e-03, bound:  3.15458894e-01\n",
      "Epoch: 23803 mean train loss:  4.85654874e-03, bound:  3.15458894e-01\n",
      "Epoch: 23804 mean train loss:  4.85627959e-03, bound:  3.15458864e-01\n",
      "Epoch: 23805 mean train loss:  4.85596526e-03, bound:  3.15458864e-01\n",
      "Epoch: 23806 mean train loss:  4.85572172e-03, bound:  3.15458834e-01\n",
      "Epoch: 23807 mean train loss:  4.85543720e-03, bound:  3.15458834e-01\n",
      "Epoch: 23808 mean train loss:  4.85524582e-03, bound:  3.15458775e-01\n",
      "Epoch: 23809 mean train loss:  4.85491008e-03, bound:  3.15458775e-01\n",
      "Epoch: 23810 mean train loss:  4.85467073e-03, bound:  3.15458745e-01\n",
      "Epoch: 23811 mean train loss:  4.85430192e-03, bound:  3.15458745e-01\n",
      "Epoch: 23812 mean train loss:  4.85404441e-03, bound:  3.15458715e-01\n",
      "Epoch: 23813 mean train loss:  4.85377666e-03, bound:  3.15458715e-01\n",
      "Epoch: 23814 mean train loss:  4.85352241e-03, bound:  3.15458655e-01\n",
      "Epoch: 23815 mean train loss:  4.85320063e-03, bound:  3.15458655e-01\n",
      "Epoch: 23816 mean train loss:  4.85294266e-03, bound:  3.15458626e-01\n",
      "Epoch: 23817 mean train loss:  4.85273218e-03, bound:  3.15458626e-01\n",
      "Epoch: 23818 mean train loss:  4.85236803e-03, bound:  3.15458626e-01\n",
      "Epoch: 23819 mean train loss:  4.85212915e-03, bound:  3.15458566e-01\n",
      "Epoch: 23820 mean train loss:  4.85183252e-03, bound:  3.15458536e-01\n",
      "Epoch: 23821 mean train loss:  4.85153263e-03, bound:  3.15458536e-01\n",
      "Epoch: 23822 mean train loss:  4.85125417e-03, bound:  3.15458536e-01\n",
      "Epoch: 23823 mean train loss:  4.85099573e-03, bound:  3.15458536e-01\n",
      "Epoch: 23824 mean train loss:  4.85072145e-03, bound:  3.15458447e-01\n",
      "Epoch: 23825 mean train loss:  4.85040713e-03, bound:  3.15458447e-01\n",
      "Epoch: 23826 mean train loss:  4.85019246e-03, bound:  3.15458417e-01\n",
      "Epoch: 23827 mean train loss:  4.84986324e-03, bound:  3.15458417e-01\n",
      "Epoch: 23828 mean train loss:  4.84958338e-03, bound:  3.15458417e-01\n",
      "Epoch: 23829 mean train loss:  4.84928908e-03, bound:  3.15458417e-01\n",
      "Epoch: 23830 mean train loss:  4.84902598e-03, bound:  3.15458357e-01\n",
      "Epoch: 23831 mean train loss:  4.84876381e-03, bound:  3.15458357e-01\n",
      "Epoch: 23832 mean train loss:  4.84849233e-03, bound:  3.15458328e-01\n",
      "Epoch: 23833 mean train loss:  4.84814774e-03, bound:  3.15458298e-01\n",
      "Epoch: 23834 mean train loss:  4.84793214e-03, bound:  3.15458298e-01\n",
      "Epoch: 23835 mean train loss:  4.84769559e-03, bound:  3.15458298e-01\n",
      "Epoch: 23836 mean train loss:  4.84735798e-03, bound:  3.15458238e-01\n",
      "Epoch: 23837 mean train loss:  4.84707719e-03, bound:  3.15458238e-01\n",
      "Epoch: 23838 mean train loss:  4.84684762e-03, bound:  3.15458208e-01\n",
      "Epoch: 23839 mean train loss:  4.84647602e-03, bound:  3.15458208e-01\n",
      "Epoch: 23840 mean train loss:  4.84623574e-03, bound:  3.15458179e-01\n",
      "Epoch: 23841 mean train loss:  4.84593492e-03, bound:  3.15458179e-01\n",
      "Epoch: 23842 mean train loss:  4.84571652e-03, bound:  3.15458119e-01\n",
      "Epoch: 23843 mean train loss:  4.84537100e-03, bound:  3.15458089e-01\n",
      "Epoch: 23844 mean train loss:  4.84509394e-03, bound:  3.15458089e-01\n",
      "Epoch: 23845 mean train loss:  4.84479498e-03, bound:  3.15458089e-01\n",
      "Epoch: 23846 mean train loss:  4.84458730e-03, bound:  3.15458059e-01\n",
      "Epoch: 23847 mean train loss:  4.84428182e-03, bound:  3.15458000e-01\n",
      "Epoch: 23848 mean train loss:  4.84405085e-03, bound:  3.15458000e-01\n",
      "Epoch: 23849 mean train loss:  4.84380219e-03, bound:  3.15457970e-01\n",
      "Epoch: 23850 mean train loss:  4.84358286e-03, bound:  3.15457970e-01\n",
      "Epoch: 23851 mean train loss:  4.84340033e-03, bound:  3.15457940e-01\n",
      "Epoch: 23852 mean train loss:  4.84315865e-03, bound:  3.15457940e-01\n",
      "Epoch: 23853 mean train loss:  4.84302919e-03, bound:  3.15457880e-01\n",
      "Epoch: 23854 mean train loss:  4.84285690e-03, bound:  3.15457880e-01\n",
      "Epoch: 23855 mean train loss:  4.84277168e-03, bound:  3.15457851e-01\n",
      "Epoch: 23856 mean train loss:  4.84277494e-03, bound:  3.15457851e-01\n",
      "Epoch: 23857 mean train loss:  4.84278146e-03, bound:  3.15457821e-01\n",
      "Epoch: 23858 mean train loss:  4.84305155e-03, bound:  3.15457821e-01\n",
      "Epoch: 23859 mean train loss:  4.84338589e-03, bound:  3.15457761e-01\n",
      "Epoch: 23860 mean train loss:  4.84404759e-03, bound:  3.15457761e-01\n",
      "Epoch: 23861 mean train loss:  4.84496122e-03, bound:  3.15457731e-01\n",
      "Epoch: 23862 mean train loss:  4.84612351e-03, bound:  3.15457761e-01\n",
      "Epoch: 23863 mean train loss:  4.84766485e-03, bound:  3.15457672e-01\n",
      "Epoch: 23864 mean train loss:  4.84927790e-03, bound:  3.15457731e-01\n",
      "Epoch: 23865 mean train loss:  4.85072937e-03, bound:  3.15457612e-01\n",
      "Epoch: 23866 mean train loss:  4.85169888e-03, bound:  3.15457731e-01\n",
      "Epoch: 23867 mean train loss:  4.85176872e-03, bound:  3.15457612e-01\n",
      "Epoch: 23868 mean train loss:  4.85058501e-03, bound:  3.15457642e-01\n",
      "Epoch: 23869 mean train loss:  4.84816078e-03, bound:  3.15457612e-01\n",
      "Epoch: 23870 mean train loss:  4.84492304e-03, bound:  3.15457612e-01\n",
      "Epoch: 23871 mean train loss:  4.84155305e-03, bound:  3.15457553e-01\n",
      "Epoch: 23872 mean train loss:  4.83875256e-03, bound:  3.15457553e-01\n",
      "Epoch: 23873 mean train loss:  4.83715069e-03, bound:  3.15457493e-01\n",
      "Epoch: 23874 mean train loss:  4.83684195e-03, bound:  3.15457493e-01\n",
      "Epoch: 23875 mean train loss:  4.83745709e-03, bound:  3.15457493e-01\n",
      "Epoch: 23876 mean train loss:  4.83845128e-03, bound:  3.15457493e-01\n",
      "Epoch: 23877 mean train loss:  4.83928993e-03, bound:  3.15457493e-01\n",
      "Epoch: 23878 mean train loss:  4.83947201e-03, bound:  3.15457433e-01\n",
      "Epoch: 23879 mean train loss:  4.83896490e-03, bound:  3.15457433e-01\n",
      "Epoch: 23880 mean train loss:  4.83794045e-03, bound:  3.15457404e-01\n",
      "Epoch: 23881 mean train loss:  4.83656116e-03, bound:  3.15457404e-01\n",
      "Epoch: 23882 mean train loss:  4.83532250e-03, bound:  3.15457374e-01\n",
      "Epoch: 23883 mean train loss:  4.83444845e-03, bound:  3.15457314e-01\n",
      "Epoch: 23884 mean train loss:  4.83403215e-03, bound:  3.15457314e-01\n",
      "Epoch: 23885 mean train loss:  4.83390922e-03, bound:  3.15457284e-01\n",
      "Epoch: 23886 mean train loss:  4.83406894e-03, bound:  3.15457284e-01\n",
      "Epoch: 23887 mean train loss:  4.83420724e-03, bound:  3.15457284e-01\n",
      "Epoch: 23888 mean train loss:  4.83427988e-03, bound:  3.15457225e-01\n",
      "Epoch: 23889 mean train loss:  4.83410805e-03, bound:  3.15457195e-01\n",
      "Epoch: 23890 mean train loss:  4.83369408e-03, bound:  3.15457195e-01\n",
      "Epoch: 23891 mean train loss:  4.83313343e-03, bound:  3.15457165e-01\n",
      "Epoch: 23892 mean train loss:  4.83249547e-03, bound:  3.15457165e-01\n",
      "Epoch: 23893 mean train loss:  4.83187847e-03, bound:  3.15457165e-01\n",
      "Epoch: 23894 mean train loss:  4.83136997e-03, bound:  3.15457106e-01\n",
      "Epoch: 23895 mean train loss:  4.83101606e-03, bound:  3.15457106e-01\n",
      "Epoch: 23896 mean train loss:  4.83074412e-03, bound:  3.15457106e-01\n",
      "Epoch: 23897 mean train loss:  4.83057275e-03, bound:  3.15457076e-01\n",
      "Epoch: 23898 mean train loss:  4.83037345e-03, bound:  3.15457046e-01\n",
      "Epoch: 23899 mean train loss:  4.83014900e-03, bound:  3.15457046e-01\n",
      "Epoch: 23900 mean train loss:  4.82990593e-03, bound:  3.15456986e-01\n",
      "Epoch: 23901 mean train loss:  4.82961349e-03, bound:  3.15456986e-01\n",
      "Epoch: 23902 mean train loss:  4.82928054e-03, bound:  3.15456986e-01\n",
      "Epoch: 23903 mean train loss:  4.82897786e-03, bound:  3.15456986e-01\n",
      "Epoch: 23904 mean train loss:  4.82860301e-03, bound:  3.15456927e-01\n",
      "Epoch: 23905 mean train loss:  4.82830871e-03, bound:  3.15456927e-01\n",
      "Epoch: 23906 mean train loss:  4.82801255e-03, bound:  3.15456927e-01\n",
      "Epoch: 23907 mean train loss:  4.82770894e-03, bound:  3.15456867e-01\n",
      "Epoch: 23908 mean train loss:  4.82743746e-03, bound:  3.15456867e-01\n",
      "Epoch: 23909 mean train loss:  4.82716039e-03, bound:  3.15456837e-01\n",
      "Epoch: 23910 mean train loss:  4.82686283e-03, bound:  3.15456837e-01\n",
      "Epoch: 23911 mean train loss:  4.82663745e-03, bound:  3.15456778e-01\n",
      "Epoch: 23912 mean train loss:  4.82640741e-03, bound:  3.15456778e-01\n",
      "Epoch: 23913 mean train loss:  4.82618436e-03, bound:  3.15456748e-01\n",
      "Epoch: 23914 mean train loss:  4.82585654e-03, bound:  3.15456748e-01\n",
      "Epoch: 23915 mean train loss:  4.82559530e-03, bound:  3.15456718e-01\n",
      "Epoch: 23916 mean train loss:  4.82524512e-03, bound:  3.15456718e-01\n",
      "Epoch: 23917 mean train loss:  4.82493872e-03, bound:  3.15456659e-01\n",
      "Epoch: 23918 mean train loss:  4.82472265e-03, bound:  3.15456659e-01\n",
      "Epoch: 23919 mean train loss:  4.82446095e-03, bound:  3.15456629e-01\n",
      "Epoch: 23920 mean train loss:  4.82419413e-03, bound:  3.15456629e-01\n",
      "Epoch: 23921 mean train loss:  4.82387329e-03, bound:  3.15456599e-01\n",
      "Epoch: 23922 mean train loss:  4.82359808e-03, bound:  3.15456599e-01\n",
      "Epoch: 23923 mean train loss:  4.82328096e-03, bound:  3.15456539e-01\n",
      "Epoch: 23924 mean train loss:  4.82308585e-03, bound:  3.15456539e-01\n",
      "Epoch: 23925 mean train loss:  4.82278131e-03, bound:  3.15456510e-01\n",
      "Epoch: 23926 mean train loss:  4.82248981e-03, bound:  3.15456510e-01\n",
      "Epoch: 23927 mean train loss:  4.82225604e-03, bound:  3.15456480e-01\n",
      "Epoch: 23928 mean train loss:  4.82197246e-03, bound:  3.15456480e-01\n",
      "Epoch: 23929 mean train loss:  4.82171541e-03, bound:  3.15456420e-01\n",
      "Epoch: 23930 mean train loss:  4.82140062e-03, bound:  3.15456420e-01\n",
      "Epoch: 23931 mean train loss:  4.82113753e-03, bound:  3.15456390e-01\n",
      "Epoch: 23932 mean train loss:  4.82077431e-03, bound:  3.15456390e-01\n",
      "Epoch: 23933 mean train loss:  4.82054893e-03, bound:  3.15456390e-01\n",
      "Epoch: 23934 mean train loss:  4.82030446e-03, bound:  3.15456361e-01\n",
      "Epoch: 23935 mean train loss:  4.82002180e-03, bound:  3.15456331e-01\n",
      "Epoch: 23936 mean train loss:  4.81979502e-03, bound:  3.15456301e-01\n",
      "Epoch: 23937 mean train loss:  4.81951796e-03, bound:  3.15456301e-01\n",
      "Epoch: 23938 mean train loss:  4.81921947e-03, bound:  3.15456301e-01\n",
      "Epoch: 23939 mean train loss:  4.81895311e-03, bound:  3.15456271e-01\n",
      "Epoch: 23940 mean train loss:  4.81869932e-03, bound:  3.15456241e-01\n",
      "Epoch: 23941 mean train loss:  4.81847068e-03, bound:  3.15456182e-01\n",
      "Epoch: 23942 mean train loss:  4.81820107e-03, bound:  3.15456182e-01\n",
      "Epoch: 23943 mean train loss:  4.81789419e-03, bound:  3.15456182e-01\n",
      "Epoch: 23944 mean train loss:  4.81763156e-03, bound:  3.15456182e-01\n",
      "Epoch: 23945 mean train loss:  4.81738895e-03, bound:  3.15456152e-01\n",
      "Epoch: 23946 mean train loss:  4.81715240e-03, bound:  3.15456152e-01\n",
      "Epoch: 23947 mean train loss:  4.81686555e-03, bound:  3.15456063e-01\n",
      "Epoch: 23948 mean train loss:  4.81664157e-03, bound:  3.15456063e-01\n",
      "Epoch: 23949 mean train loss:  4.81645158e-03, bound:  3.15456063e-01\n",
      "Epoch: 23950 mean train loss:  4.81622340e-03, bound:  3.15456063e-01\n",
      "Epoch: 23951 mean train loss:  4.81610699e-03, bound:  3.15456003e-01\n",
      "Epoch: 23952 mean train loss:  4.81591420e-03, bound:  3.15456003e-01\n",
      "Epoch: 23953 mean train loss:  4.81580338e-03, bound:  3.15455973e-01\n",
      "Epoch: 23954 mean train loss:  4.81575215e-03, bound:  3.15455973e-01\n",
      "Epoch: 23955 mean train loss:  4.81574377e-03, bound:  3.15455943e-01\n",
      "Epoch: 23956 mean train loss:  4.81587555e-03, bound:  3.15455943e-01\n",
      "Epoch: 23957 mean train loss:  4.81609395e-03, bound:  3.15455884e-01\n",
      "Epoch: 23958 mean train loss:  4.81651118e-03, bound:  3.15455884e-01\n",
      "Epoch: 23959 mean train loss:  4.81693121e-03, bound:  3.15455854e-01\n",
      "Epoch: 23960 mean train loss:  4.81759245e-03, bound:  3.15455884e-01\n",
      "Epoch: 23961 mean train loss:  4.81836544e-03, bound:  3.15455794e-01\n",
      "Epoch: 23962 mean train loss:  4.81933728e-03, bound:  3.15455854e-01\n",
      "Epoch: 23963 mean train loss:  4.82037943e-03, bound:  3.15455735e-01\n",
      "Epoch: 23964 mean train loss:  4.82135639e-03, bound:  3.15455794e-01\n",
      "Epoch: 23965 mean train loss:  4.82202042e-03, bound:  3.15455735e-01\n",
      "Epoch: 23966 mean train loss:  4.82218713e-03, bound:  3.15455765e-01\n",
      "Epoch: 23967 mean train loss:  4.82149003e-03, bound:  3.15455675e-01\n",
      "Epoch: 23968 mean train loss:  4.82000317e-03, bound:  3.15455735e-01\n",
      "Epoch: 23969 mean train loss:  4.81776753e-03, bound:  3.15455675e-01\n",
      "Epoch: 23970 mean train loss:  4.81514400e-03, bound:  3.15455675e-01\n",
      "Epoch: 23971 mean train loss:  4.81261406e-03, bound:  3.15455616e-01\n",
      "Epoch: 23972 mean train loss:  4.81068250e-03, bound:  3.15455645e-01\n",
      "Epoch: 23973 mean train loss:  4.80969390e-03, bound:  3.15455616e-01\n",
      "Epoch: 23974 mean train loss:  4.80936933e-03, bound:  3.15455586e-01\n",
      "Epoch: 23975 mean train loss:  4.80968365e-03, bound:  3.15455586e-01\n",
      "Epoch: 23976 mean train loss:  4.81018377e-03, bound:  3.15455526e-01\n",
      "Epoch: 23977 mean train loss:  4.81064525e-03, bound:  3.15455526e-01\n",
      "Epoch: 23978 mean train loss:  4.81077936e-03, bound:  3.15455496e-01\n",
      "Epoch: 23979 mean train loss:  4.81056841e-03, bound:  3.15455496e-01\n",
      "Epoch: 23980 mean train loss:  4.80998354e-03, bound:  3.15455496e-01\n",
      "Epoch: 23981 mean train loss:  4.80908295e-03, bound:  3.15455496e-01\n",
      "Epoch: 23982 mean train loss:  4.80817212e-03, bound:  3.15455437e-01\n",
      "Epoch: 23983 mean train loss:  4.80734976e-03, bound:  3.15455407e-01\n",
      "Epoch: 23984 mean train loss:  4.80668899e-03, bound:  3.15455377e-01\n",
      "Epoch: 23985 mean train loss:  4.80629830e-03, bound:  3.15455377e-01\n",
      "Epoch: 23986 mean train loss:  4.80613159e-03, bound:  3.15455377e-01\n",
      "Epoch: 23987 mean train loss:  4.80609667e-03, bound:  3.15455347e-01\n",
      "Epoch: 23988 mean train loss:  4.80617816e-03, bound:  3.15455347e-01\n",
      "Epoch: 23989 mean train loss:  4.80609015e-03, bound:  3.15455258e-01\n",
      "Epoch: 23990 mean train loss:  4.80603054e-03, bound:  3.15455288e-01\n",
      "Epoch: 23991 mean train loss:  4.80578281e-03, bound:  3.15455258e-01\n",
      "Epoch: 23992 mean train loss:  4.80541820e-03, bound:  3.15455258e-01\n",
      "Epoch: 23993 mean train loss:  4.80488595e-03, bound:  3.15455198e-01\n",
      "Epoch: 23994 mean train loss:  4.80434624e-03, bound:  3.15455198e-01\n",
      "Epoch: 23995 mean train loss:  4.80385823e-03, bound:  3.15455168e-01\n",
      "Epoch: 23996 mean train loss:  4.80343448e-03, bound:  3.15455168e-01\n",
      "Epoch: 23997 mean train loss:  4.80306707e-03, bound:  3.15455168e-01\n",
      "Epoch: 23998 mean train loss:  4.80278302e-03, bound:  3.15455139e-01\n",
      "Epoch: 23999 mean train loss:  4.80262237e-03, bound:  3.15455079e-01\n",
      "Epoch: 24000 mean train loss:  4.80239326e-03, bound:  3.15455079e-01\n",
      "Epoch: 24001 mean train loss:  4.80224472e-03, bound:  3.15455079e-01\n",
      "Epoch: 24002 mean train loss:  4.80211386e-03, bound:  3.15455049e-01\n",
      "Epoch: 24003 mean train loss:  4.80186753e-03, bound:  3.15455049e-01\n",
      "Epoch: 24004 mean train loss:  4.80158953e-03, bound:  3.15454960e-01\n",
      "Epoch: 24005 mean train loss:  4.80127754e-03, bound:  3.15454960e-01\n",
      "Epoch: 24006 mean train loss:  4.80092457e-03, bound:  3.15454960e-01\n",
      "Epoch: 24007 mean train loss:  4.80060792e-03, bound:  3.15454960e-01\n",
      "Epoch: 24008 mean train loss:  4.80023073e-03, bound:  3.15454930e-01\n",
      "Epoch: 24009 mean train loss:  4.79989545e-03, bound:  3.15454930e-01\n",
      "Epoch: 24010 mean train loss:  4.79960674e-03, bound:  3.15454900e-01\n",
      "Epoch: 24011 mean train loss:  4.79928311e-03, bound:  3.15454870e-01\n",
      "Epoch: 24012 mean train loss:  4.79904655e-03, bound:  3.15454870e-01\n",
      "Epoch: 24013 mean train loss:  4.79877461e-03, bound:  3.15454841e-01\n",
      "Epoch: 24014 mean train loss:  4.79844259e-03, bound:  3.15454811e-01\n",
      "Epoch: 24015 mean train loss:  4.79823118e-03, bound:  3.15454811e-01\n",
      "Epoch: 24016 mean train loss:  4.79797274e-03, bound:  3.15454781e-01\n",
      "Epoch: 24017 mean train loss:  4.79767565e-03, bound:  3.15454781e-01\n",
      "Epoch: 24018 mean train loss:  4.79748100e-03, bound:  3.15454751e-01\n",
      "Epoch: 24019 mean train loss:  4.79719136e-03, bound:  3.15454721e-01\n",
      "Epoch: 24020 mean train loss:  4.79690218e-03, bound:  3.15454692e-01\n",
      "Epoch: 24021 mean train loss:  4.79657575e-03, bound:  3.15454692e-01\n",
      "Epoch: 24022 mean train loss:  4.79628658e-03, bound:  3.15454692e-01\n",
      "Epoch: 24023 mean train loss:  4.79601137e-03, bound:  3.15454632e-01\n",
      "Epoch: 24024 mean train loss:  4.79573151e-03, bound:  3.15454632e-01\n",
      "Epoch: 24025 mean train loss:  4.79549775e-03, bound:  3.15454602e-01\n",
      "Epoch: 24026 mean train loss:  4.79515223e-03, bound:  3.15454602e-01\n",
      "Epoch: 24027 mean train loss:  4.79495386e-03, bound:  3.15454572e-01\n",
      "Epoch: 24028 mean train loss:  4.79464093e-03, bound:  3.15454572e-01\n",
      "Epoch: 24029 mean train loss:  4.79440112e-03, bound:  3.15454513e-01\n",
      "Epoch: 24030 mean train loss:  4.79414081e-03, bound:  3.15454513e-01\n",
      "Epoch: 24031 mean train loss:  4.79389122e-03, bound:  3.15454483e-01\n",
      "Epoch: 24032 mean train loss:  4.79372405e-03, bound:  3.15454483e-01\n",
      "Epoch: 24033 mean train loss:  4.79346421e-03, bound:  3.15454483e-01\n",
      "Epoch: 24034 mean train loss:  4.79331613e-03, bound:  3.15454453e-01\n",
      "Epoch: 24035 mean train loss:  4.79310611e-03, bound:  3.15454394e-01\n",
      "Epoch: 24036 mean train loss:  4.79289005e-03, bound:  3.15454394e-01\n",
      "Epoch: 24037 mean train loss:  4.79276758e-03, bound:  3.15454394e-01\n",
      "Epoch: 24038 mean train loss:  4.79265722e-03, bound:  3.15454364e-01\n",
      "Epoch: 24039 mean train loss:  4.79255058e-03, bound:  3.15454364e-01\n",
      "Epoch: 24040 mean train loss:  4.79262508e-03, bound:  3.15454304e-01\n",
      "Epoch: 24041 mean train loss:  4.79264744e-03, bound:  3.15454274e-01\n",
      "Epoch: 24042 mean train loss:  4.79282113e-03, bound:  3.15454245e-01\n",
      "Epoch: 24043 mean train loss:  4.79302369e-03, bound:  3.15454245e-01\n",
      "Epoch: 24044 mean train loss:  4.79323370e-03, bound:  3.15454245e-01\n",
      "Epoch: 24045 mean train loss:  4.79355920e-03, bound:  3.15454245e-01\n",
      "Epoch: 24046 mean train loss:  4.79406584e-03, bound:  3.15454155e-01\n",
      "Epoch: 24047 mean train loss:  4.79476014e-03, bound:  3.15454185e-01\n",
      "Epoch: 24048 mean train loss:  4.79541533e-03, bound:  3.15454125e-01\n",
      "Epoch: 24049 mean train loss:  4.79626423e-03, bound:  3.15454185e-01\n",
      "Epoch: 24050 mean train loss:  4.79690079e-03, bound:  3.15454066e-01\n",
      "Epoch: 24051 mean train loss:  4.79715923e-03, bound:  3.15454155e-01\n",
      "Epoch: 24052 mean train loss:  4.79702838e-03, bound:  3.15454066e-01\n",
      "Epoch: 24053 mean train loss:  4.79631359e-03, bound:  3.15454125e-01\n",
      "Epoch: 24054 mean train loss:  4.79488540e-03, bound:  3.15454006e-01\n",
      "Epoch: 24055 mean train loss:  4.79303114e-03, bound:  3.15454066e-01\n",
      "Epoch: 24056 mean train loss:  4.79096686e-03, bound:  3.15454006e-01\n",
      "Epoch: 24057 mean train loss:  4.78897197e-03, bound:  3.15454036e-01\n",
      "Epoch: 24058 mean train loss:  4.78744134e-03, bound:  3.15453947e-01\n",
      "Epoch: 24059 mean train loss:  4.78647929e-03, bound:  3.15453947e-01\n",
      "Epoch: 24060 mean train loss:  4.78600757e-03, bound:  3.15453947e-01\n",
      "Epoch: 24061 mean train loss:  4.78604203e-03, bound:  3.15453947e-01\n",
      "Epoch: 24062 mean train loss:  4.78637172e-03, bound:  3.15453917e-01\n",
      "Epoch: 24063 mean train loss:  4.78678895e-03, bound:  3.15453857e-01\n",
      "Epoch: 24064 mean train loss:  4.78722248e-03, bound:  3.15453857e-01\n",
      "Epoch: 24065 mean train loss:  4.78735985e-03, bound:  3.15453827e-01\n",
      "Epoch: 24066 mean train loss:  4.78705112e-03, bound:  3.15453827e-01\n",
      "Epoch: 24067 mean train loss:  4.78647929e-03, bound:  3.15453798e-01\n",
      "Epoch: 24068 mean train loss:  4.78568301e-03, bound:  3.15453798e-01\n",
      "Epoch: 24069 mean train loss:  4.78484528e-03, bound:  3.15453738e-01\n",
      "Epoch: 24070 mean train loss:  4.78405738e-03, bound:  3.15453738e-01\n",
      "Epoch: 24071 mean train loss:  4.78336215e-03, bound:  3.15453708e-01\n",
      "Epoch: 24072 mean train loss:  4.78285691e-03, bound:  3.15453708e-01\n",
      "Epoch: 24073 mean train loss:  4.78248624e-03, bound:  3.15453708e-01\n",
      "Epoch: 24074 mean train loss:  4.78229299e-03, bound:  3.15453678e-01\n",
      "Epoch: 24075 mean train loss:  4.78229905e-03, bound:  3.15453678e-01\n",
      "Epoch: 24076 mean train loss:  4.78230324e-03, bound:  3.15453619e-01\n",
      "Epoch: 24077 mean train loss:  4.78221849e-03, bound:  3.15453619e-01\n",
      "Epoch: 24078 mean train loss:  4.78215003e-03, bound:  3.15453589e-01\n",
      "Epoch: 24079 mean train loss:  4.78192000e-03, bound:  3.15453589e-01\n",
      "Epoch: 24080 mean train loss:  4.78160148e-03, bound:  3.15453559e-01\n",
      "Epoch: 24081 mean train loss:  4.78119450e-03, bound:  3.15453559e-01\n",
      "Epoch: 24082 mean train loss:  4.78071487e-03, bound:  3.15453500e-01\n",
      "Epoch: 24083 mean train loss:  4.78027016e-03, bound:  3.15453500e-01\n",
      "Epoch: 24084 mean train loss:  4.77974676e-03, bound:  3.15453470e-01\n",
      "Epoch: 24085 mean train loss:  4.77940310e-03, bound:  3.15453440e-01\n",
      "Epoch: 24086 mean train loss:  4.77905339e-03, bound:  3.15453440e-01\n",
      "Epoch: 24087 mean train loss:  4.77877911e-03, bound:  3.15453440e-01\n",
      "Epoch: 24088 mean train loss:  4.77853324e-03, bound:  3.15453380e-01\n",
      "Epoch: 24089 mean train loss:  4.77840053e-03, bound:  3.15453380e-01\n",
      "Epoch: 24090 mean train loss:  4.77818772e-03, bound:  3.15453351e-01\n",
      "Epoch: 24091 mean train loss:  4.77789622e-03, bound:  3.15453351e-01\n",
      "Epoch: 24092 mean train loss:  4.77770204e-03, bound:  3.15453351e-01\n",
      "Epoch: 24093 mean train loss:  4.77741612e-03, bound:  3.15453291e-01\n",
      "Epoch: 24094 mean train loss:  4.77712229e-03, bound:  3.15453261e-01\n",
      "Epoch: 24095 mean train loss:  4.77683498e-03, bound:  3.15453261e-01\n",
      "Epoch: 24096 mean train loss:  4.77652205e-03, bound:  3.15453261e-01\n",
      "Epoch: 24097 mean train loss:  4.77619609e-03, bound:  3.15453231e-01\n",
      "Epoch: 24098 mean train loss:  4.77588922e-03, bound:  3.15453231e-01\n",
      "Epoch: 24099 mean train loss:  4.77559818e-03, bound:  3.15453172e-01\n",
      "Epoch: 24100 mean train loss:  4.77538304e-03, bound:  3.15453142e-01\n",
      "Epoch: 24101 mean train loss:  4.77499468e-03, bound:  3.15453142e-01\n",
      "Epoch: 24102 mean train loss:  4.77476744e-03, bound:  3.15453142e-01\n",
      "Epoch: 24103 mean train loss:  4.77453414e-03, bound:  3.15453112e-01\n",
      "Epoch: 24104 mean train loss:  4.77418350e-03, bound:  3.15453112e-01\n",
      "Epoch: 24105 mean train loss:  4.77397488e-03, bound:  3.15453053e-01\n",
      "Epoch: 24106 mean train loss:  4.77369316e-03, bound:  3.15453023e-01\n",
      "Epoch: 24107 mean train loss:  4.77338955e-03, bound:  3.15453023e-01\n",
      "Epoch: 24108 mean train loss:  4.77315020e-03, bound:  3.15453023e-01\n",
      "Epoch: 24109 mean train loss:  4.77293786e-03, bound:  3.15452993e-01\n",
      "Epoch: 24110 mean train loss:  4.77262493e-03, bound:  3.15452963e-01\n",
      "Epoch: 24111 mean train loss:  4.77237627e-03, bound:  3.15452933e-01\n",
      "Epoch: 24112 mean train loss:  4.77213180e-03, bound:  3.15452933e-01\n",
      "Epoch: 24113 mean train loss:  4.77183564e-03, bound:  3.15452933e-01\n",
      "Epoch: 24114 mean train loss:  4.77159768e-03, bound:  3.15452904e-01\n",
      "Epoch: 24115 mean train loss:  4.77123260e-03, bound:  3.15452874e-01\n",
      "Epoch: 24116 mean train loss:  4.77102771e-03, bound:  3.15452844e-01\n",
      "Epoch: 24117 mean train loss:  4.77076694e-03, bound:  3.15452814e-01\n",
      "Epoch: 24118 mean train loss:  4.77045635e-03, bound:  3.15452814e-01\n",
      "Epoch: 24119 mean train loss:  4.77020117e-03, bound:  3.15452814e-01\n",
      "Epoch: 24120 mean train loss:  4.76998324e-03, bound:  3.15452784e-01\n",
      "Epoch: 24121 mean train loss:  4.76972992e-03, bound:  3.15452754e-01\n",
      "Epoch: 24122 mean train loss:  4.76938905e-03, bound:  3.15452725e-01\n",
      "Epoch: 24123 mean train loss:  4.76911711e-03, bound:  3.15452725e-01\n",
      "Epoch: 24124 mean train loss:  4.76888008e-03, bound:  3.15452695e-01\n",
      "Epoch: 24125 mean train loss:  4.76865750e-03, bound:  3.15452695e-01\n",
      "Epoch: 24126 mean train loss:  4.76839906e-03, bound:  3.15452665e-01\n",
      "Epoch: 24127 mean train loss:  4.76814201e-03, bound:  3.15452665e-01\n",
      "Epoch: 24128 mean train loss:  4.76788403e-03, bound:  3.15452665e-01\n",
      "Epoch: 24129 mean train loss:  4.76768613e-03, bound:  3.15452665e-01\n",
      "Epoch: 24130 mean train loss:  4.76755854e-03, bound:  3.15452576e-01\n",
      "Epoch: 24131 mean train loss:  4.76736343e-03, bound:  3.15452576e-01\n",
      "Epoch: 24132 mean train loss:  4.76729544e-03, bound:  3.15452546e-01\n",
      "Epoch: 24133 mean train loss:  4.76724887e-03, bound:  3.15452546e-01\n",
      "Epoch: 24134 mean train loss:  4.76733316e-03, bound:  3.15452516e-01\n",
      "Epoch: 24135 mean train loss:  4.76750173e-03, bound:  3.15452516e-01\n",
      "Epoch: 24136 mean train loss:  4.76777367e-03, bound:  3.15452456e-01\n",
      "Epoch: 24137 mean train loss:  4.76834131e-03, bound:  3.15452456e-01\n",
      "Epoch: 24138 mean train loss:  4.76917485e-03, bound:  3.15452397e-01\n",
      "Epoch: 24139 mean train loss:  4.77039954e-03, bound:  3.15452427e-01\n",
      "Epoch: 24140 mean train loss:  4.77215415e-03, bound:  3.15452367e-01\n",
      "Epoch: 24141 mean train loss:  4.77440702e-03, bound:  3.15452427e-01\n",
      "Epoch: 24142 mean train loss:  4.77705803e-03, bound:  3.15452337e-01\n",
      "Epoch: 24143 mean train loss:  4.77976631e-03, bound:  3.15452397e-01\n",
      "Epoch: 24144 mean train loss:  4.78183012e-03, bound:  3.15452278e-01\n",
      "Epoch: 24145 mean train loss:  4.78214305e-03, bound:  3.15452367e-01\n",
      "Epoch: 24146 mean train loss:  4.78017470e-03, bound:  3.15452248e-01\n",
      "Epoch: 24147 mean train loss:  4.77597723e-03, bound:  3.15452337e-01\n",
      "Epoch: 24148 mean train loss:  4.77041490e-03, bound:  3.15452248e-01\n",
      "Epoch: 24149 mean train loss:  4.76530846e-03, bound:  3.15452248e-01\n",
      "Epoch: 24150 mean train loss:  4.76228073e-03, bound:  3.15452248e-01\n",
      "Epoch: 24151 mean train loss:  4.76187002e-03, bound:  3.15452218e-01\n",
      "Epoch: 24152 mean train loss:  4.76343418e-03, bound:  3.15452218e-01\n",
      "Epoch: 24153 mean train loss:  4.76584723e-03, bound:  3.15452129e-01\n",
      "Epoch: 24154 mean train loss:  4.76747612e-03, bound:  3.15452188e-01\n",
      "Epoch: 24155 mean train loss:  4.76735225e-03, bound:  3.15452129e-01\n",
      "Epoch: 24156 mean train loss:  4.76570567e-03, bound:  3.15452129e-01\n",
      "Epoch: 24157 mean train loss:  4.76309750e-03, bound:  3.15452099e-01\n",
      "Epoch: 24158 mean train loss:  4.76079667e-03, bound:  3.15452099e-01\n",
      "Epoch: 24159 mean train loss:  4.75956313e-03, bound:  3.15452039e-01\n",
      "Epoch: 24160 mean train loss:  4.75960411e-03, bound:  3.15452009e-01\n",
      "Epoch: 24161 mean train loss:  4.76043765e-03, bound:  3.15452039e-01\n",
      "Epoch: 24162 mean train loss:  4.76121344e-03, bound:  3.15452009e-01\n",
      "Epoch: 24163 mean train loss:  4.76142205e-03, bound:  3.15452009e-01\n",
      "Epoch: 24164 mean train loss:  4.76081064e-03, bound:  3.15451950e-01\n",
      "Epoch: 24165 mean train loss:  4.75963065e-03, bound:  3.15451980e-01\n",
      "Epoch: 24166 mean train loss:  4.75840922e-03, bound:  3.15451920e-01\n",
      "Epoch: 24167 mean train loss:  4.75758873e-03, bound:  3.15451920e-01\n",
      "Epoch: 24168 mean train loss:  4.75726509e-03, bound:  3.15451890e-01\n",
      "Epoch: 24169 mean train loss:  4.75723390e-03, bound:  3.15451890e-01\n",
      "Epoch: 24170 mean train loss:  4.75746859e-03, bound:  3.15451890e-01\n",
      "Epoch: 24171 mean train loss:  4.75749141e-03, bound:  3.15451831e-01\n",
      "Epoch: 24172 mean train loss:  4.75719525e-03, bound:  3.15451831e-01\n",
      "Epoch: 24173 mean train loss:  4.75669373e-03, bound:  3.15451771e-01\n",
      "Epoch: 24174 mean train loss:  4.75600222e-03, bound:  3.15451771e-01\n",
      "Epoch: 24175 mean train loss:  4.75550629e-03, bound:  3.15451771e-01\n",
      "Epoch: 24176 mean train loss:  4.75516077e-03, bound:  3.15451711e-01\n",
      "Epoch: 24177 mean train loss:  4.75494144e-03, bound:  3.15451711e-01\n",
      "Epoch: 24178 mean train loss:  4.75488557e-03, bound:  3.15451682e-01\n",
      "Epoch: 24179 mean train loss:  4.75484179e-03, bound:  3.15451711e-01\n",
      "Epoch: 24180 mean train loss:  4.75470116e-03, bound:  3.15451652e-01\n",
      "Epoch: 24181 mean train loss:  4.75438870e-03, bound:  3.15451652e-01\n",
      "Epoch: 24182 mean train loss:  4.75390349e-03, bound:  3.15451592e-01\n",
      "Epoch: 24183 mean train loss:  4.75342292e-03, bound:  3.15451592e-01\n",
      "Epoch: 24184 mean train loss:  4.75310301e-03, bound:  3.15451562e-01\n",
      "Epoch: 24185 mean train loss:  4.75279754e-03, bound:  3.15451562e-01\n",
      "Epoch: 24186 mean train loss:  4.75255679e-03, bound:  3.15451562e-01\n",
      "Epoch: 24187 mean train loss:  4.75243200e-03, bound:  3.15451533e-01\n",
      "Epoch: 24188 mean train loss:  4.75222571e-03, bound:  3.15451533e-01\n",
      "Epoch: 24189 mean train loss:  4.75195516e-03, bound:  3.15451473e-01\n",
      "Epoch: 24190 mean train loss:  4.75171488e-03, bound:  3.15451473e-01\n",
      "Epoch: 24191 mean train loss:  4.75137867e-03, bound:  3.15451443e-01\n",
      "Epoch: 24192 mean train loss:  4.75108484e-03, bound:  3.15451443e-01\n",
      "Epoch: 24193 mean train loss:  4.75071231e-03, bound:  3.15451443e-01\n",
      "Epoch: 24194 mean train loss:  4.75044455e-03, bound:  3.15451413e-01\n",
      "Epoch: 24195 mean train loss:  4.75022430e-03, bound:  3.15451384e-01\n",
      "Epoch: 24196 mean train loss:  4.74998029e-03, bound:  3.15451354e-01\n",
      "Epoch: 24197 mean train loss:  4.74976283e-03, bound:  3.15451354e-01\n",
      "Epoch: 24198 mean train loss:  4.74953186e-03, bound:  3.15451324e-01\n",
      "Epoch: 24199 mean train loss:  4.74927388e-03, bound:  3.15451324e-01\n",
      "Epoch: 24200 mean train loss:  4.74904897e-03, bound:  3.15451264e-01\n",
      "Epoch: 24201 mean train loss:  4.74873465e-03, bound:  3.15451264e-01\n",
      "Epoch: 24202 mean train loss:  4.74844687e-03, bound:  3.15451264e-01\n",
      "Epoch: 24203 mean train loss:  4.74812742e-03, bound:  3.15451235e-01\n",
      "Epoch: 24204 mean train loss:  4.74782335e-03, bound:  3.15451205e-01\n",
      "Epoch: 24205 mean train loss:  4.74757468e-03, bound:  3.15451205e-01\n",
      "Epoch: 24206 mean train loss:  4.74730320e-03, bound:  3.15451205e-01\n",
      "Epoch: 24207 mean train loss:  4.74704150e-03, bound:  3.15451145e-01\n",
      "Epoch: 24208 mean train loss:  4.74677421e-03, bound:  3.15451145e-01\n",
      "Epoch: 24209 mean train loss:  4.74654092e-03, bound:  3.15451115e-01\n",
      "Epoch: 24210 mean train loss:  4.74635651e-03, bound:  3.15451115e-01\n",
      "Epoch: 24211 mean train loss:  4.74605337e-03, bound:  3.15451086e-01\n",
      "Epoch: 24212 mean train loss:  4.74584475e-03, bound:  3.15451086e-01\n",
      "Epoch: 24213 mean train loss:  4.74558072e-03, bound:  3.15451026e-01\n",
      "Epoch: 24214 mean train loss:  4.74525476e-03, bound:  3.15451026e-01\n",
      "Epoch: 24215 mean train loss:  4.74503124e-03, bound:  3.15450996e-01\n",
      "Epoch: 24216 mean train loss:  4.74470016e-03, bound:  3.15450996e-01\n",
      "Epoch: 24217 mean train loss:  4.74439934e-03, bound:  3.15450966e-01\n",
      "Epoch: 24218 mean train loss:  4.74418141e-03, bound:  3.15450966e-01\n",
      "Epoch: 24219 mean train loss:  4.74390015e-03, bound:  3.15450907e-01\n",
      "Epoch: 24220 mean train loss:  4.74367710e-03, bound:  3.15450907e-01\n",
      "Epoch: 24221 mean train loss:  4.74341214e-03, bound:  3.15450907e-01\n",
      "Epoch: 24222 mean train loss:  4.74314159e-03, bound:  3.15450877e-01\n",
      "Epoch: 24223 mean train loss:  4.74288315e-03, bound:  3.15450877e-01\n",
      "Epoch: 24224 mean train loss:  4.74258279e-03, bound:  3.15450817e-01\n",
      "Epoch: 24225 mean train loss:  4.74235229e-03, bound:  3.15450788e-01\n",
      "Epoch: 24226 mean train loss:  4.74209385e-03, bound:  3.15450788e-01\n",
      "Epoch: 24227 mean train loss:  4.74186707e-03, bound:  3.15450788e-01\n",
      "Epoch: 24228 mean train loss:  4.74163657e-03, bound:  3.15450758e-01\n",
      "Epoch: 24229 mean train loss:  4.74132504e-03, bound:  3.15450758e-01\n",
      "Epoch: 24230 mean train loss:  4.74101864e-03, bound:  3.15450698e-01\n",
      "Epoch: 24231 mean train loss:  4.74087661e-03, bound:  3.15450698e-01\n",
      "Epoch: 24232 mean train loss:  4.74057579e-03, bound:  3.15450698e-01\n",
      "Epoch: 24233 mean train loss:  4.74029873e-03, bound:  3.15450639e-01\n",
      "Epoch: 24234 mean train loss:  4.74004867e-03, bound:  3.15450639e-01\n",
      "Epoch: 24235 mean train loss:  4.73977812e-03, bound:  3.15450639e-01\n",
      "Epoch: 24236 mean train loss:  4.73949220e-03, bound:  3.15450579e-01\n",
      "Epoch: 24237 mean train loss:  4.73923143e-03, bound:  3.15450579e-01\n",
      "Epoch: 24238 mean train loss:  4.73897019e-03, bound:  3.15450579e-01\n",
      "Epoch: 24239 mean train loss:  4.73867124e-03, bound:  3.15450549e-01\n",
      "Epoch: 24240 mean train loss:  4.73840255e-03, bound:  3.15450519e-01\n",
      "Epoch: 24241 mean train loss:  4.73827310e-03, bound:  3.15450519e-01\n",
      "Epoch: 24242 mean train loss:  4.73796111e-03, bound:  3.15450519e-01\n",
      "Epoch: 24243 mean train loss:  4.73765284e-03, bound:  3.15450460e-01\n",
      "Epoch: 24244 mean train loss:  4.73736646e-03, bound:  3.15450460e-01\n",
      "Epoch: 24245 mean train loss:  4.73716436e-03, bound:  3.15450430e-01\n",
      "Epoch: 24246 mean train loss:  4.73692082e-03, bound:  3.15450430e-01\n",
      "Epoch: 24247 mean train loss:  4.73667961e-03, bound:  3.15450430e-01\n",
      "Epoch: 24248 mean train loss:  4.73641139e-03, bound:  3.15450370e-01\n",
      "Epoch: 24249 mean train loss:  4.73615946e-03, bound:  3.15450341e-01\n",
      "Epoch: 24250 mean train loss:  4.73590242e-03, bound:  3.15450341e-01\n",
      "Epoch: 24251 mean train loss:  4.73558158e-03, bound:  3.15450311e-01\n",
      "Epoch: 24252 mean train loss:  4.73539019e-03, bound:  3.15450311e-01\n",
      "Epoch: 24253 mean train loss:  4.73503675e-03, bound:  3.15450311e-01\n",
      "Epoch: 24254 mean train loss:  4.73480020e-03, bound:  3.15450251e-01\n",
      "Epoch: 24255 mean train loss:  4.73459205e-03, bound:  3.15450221e-01\n",
      "Epoch: 24256 mean train loss:  4.73432243e-03, bound:  3.15450221e-01\n",
      "Epoch: 24257 mean train loss:  4.73403605e-03, bound:  3.15450191e-01\n",
      "Epoch: 24258 mean train loss:  4.73381253e-03, bound:  3.15450191e-01\n",
      "Epoch: 24259 mean train loss:  4.73357458e-03, bound:  3.15450191e-01\n",
      "Epoch: 24260 mean train loss:  4.73331660e-03, bound:  3.15450132e-01\n",
      "Epoch: 24261 mean train loss:  4.73316666e-03, bound:  3.15450132e-01\n",
      "Epoch: 24262 mean train loss:  4.73292917e-03, bound:  3.15450102e-01\n",
      "Epoch: 24263 mean train loss:  4.73282347e-03, bound:  3.15450102e-01\n",
      "Epoch: 24264 mean train loss:  4.73268097e-03, bound:  3.15450072e-01\n",
      "Epoch: 24265 mean train loss:  4.73261252e-03, bound:  3.15450072e-01\n",
      "Epoch: 24266 mean train loss:  4.73256549e-03, bound:  3.15450013e-01\n",
      "Epoch: 24267 mean train loss:  4.73262230e-03, bound:  3.15450013e-01\n",
      "Epoch: 24268 mean train loss:  4.73276386e-03, bound:  3.15450013e-01\n",
      "Epoch: 24269 mean train loss:  4.73304139e-03, bound:  3.15449983e-01\n",
      "Epoch: 24270 mean train loss:  4.73348191e-03, bound:  3.15449923e-01\n",
      "Epoch: 24271 mean train loss:  4.73423721e-03, bound:  3.15449923e-01\n",
      "Epoch: 24272 mean train loss:  4.73528495e-03, bound:  3.15449893e-01\n",
      "Epoch: 24273 mean train loss:  4.73667122e-03, bound:  3.15449893e-01\n",
      "Epoch: 24274 mean train loss:  4.73838532e-03, bound:  3.15449864e-01\n",
      "Epoch: 24275 mean train loss:  4.74033644e-03, bound:  3.15449893e-01\n",
      "Epoch: 24276 mean train loss:  4.74209292e-03, bound:  3.15449774e-01\n",
      "Epoch: 24277 mean train loss:  4.74334741e-03, bound:  3.15449864e-01\n",
      "Epoch: 24278 mean train loss:  4.74363519e-03, bound:  3.15449774e-01\n",
      "Epoch: 24279 mean train loss:  4.74226661e-03, bound:  3.15449804e-01\n",
      "Epoch: 24280 mean train loss:  4.73941304e-03, bound:  3.15449744e-01\n",
      "Epoch: 24281 mean train loss:  4.73527238e-03, bound:  3.15449774e-01\n",
      "Epoch: 24282 mean train loss:  4.73124860e-03, bound:  3.15449744e-01\n",
      "Epoch: 24283 mean train loss:  4.72820085e-03, bound:  3.15449685e-01\n",
      "Epoch: 24284 mean train loss:  4.72683366e-03, bound:  3.15449685e-01\n",
      "Epoch: 24285 mean train loss:  4.72703809e-03, bound:  3.15449655e-01\n",
      "Epoch: 24286 mean train loss:  4.72818781e-03, bound:  3.15449655e-01\n",
      "Epoch: 24287 mean train loss:  4.72948654e-03, bound:  3.15449625e-01\n",
      "Epoch: 24288 mean train loss:  4.73032612e-03, bound:  3.15449655e-01\n",
      "Epoch: 24289 mean train loss:  4.73017711e-03, bound:  3.15449625e-01\n",
      "Epoch: 24290 mean train loss:  4.72908933e-03, bound:  3.15449625e-01\n",
      "Epoch: 24291 mean train loss:  4.72748419e-03, bound:  3.15449566e-01\n",
      "Epoch: 24292 mean train loss:  4.72586555e-03, bound:  3.15449536e-01\n",
      "Epoch: 24293 mean train loss:  4.72469814e-03, bound:  3.15449536e-01\n",
      "Epoch: 24294 mean train loss:  4.72434098e-03, bound:  3.15449506e-01\n",
      "Epoch: 24295 mean train loss:  4.72428324e-03, bound:  3.15449506e-01\n",
      "Epoch: 24296 mean train loss:  4.72466974e-03, bound:  3.15449446e-01\n",
      "Epoch: 24297 mean train loss:  4.72496217e-03, bound:  3.15449476e-01\n",
      "Epoch: 24298 mean train loss:  4.72495938e-03, bound:  3.15449417e-01\n",
      "Epoch: 24299 mean train loss:  4.72455705e-03, bound:  3.15449446e-01\n",
      "Epoch: 24300 mean train loss:  4.72397590e-03, bound:  3.15449387e-01\n",
      "Epoch: 24301 mean train loss:  4.72321548e-03, bound:  3.15449387e-01\n",
      "Epoch: 24302 mean train loss:  4.72246669e-03, bound:  3.15449357e-01\n",
      "Epoch: 24303 mean train loss:  4.72201966e-03, bound:  3.15449327e-01\n",
      "Epoch: 24304 mean train loss:  4.72175470e-03, bound:  3.15449327e-01\n",
      "Epoch: 24305 mean train loss:  4.72167740e-03, bound:  3.15449297e-01\n",
      "Epoch: 24306 mean train loss:  4.72165179e-03, bound:  3.15449297e-01\n",
      "Epoch: 24307 mean train loss:  4.72154561e-03, bound:  3.15449268e-01\n",
      "Epoch: 24308 mean train loss:  4.72137146e-03, bound:  3.15449238e-01\n",
      "Epoch: 24309 mean train loss:  4.72103851e-03, bound:  3.15449208e-01\n",
      "Epoch: 24310 mean train loss:  4.72060358e-03, bound:  3.15449208e-01\n",
      "Epoch: 24311 mean train loss:  4.72014165e-03, bound:  3.15449178e-01\n",
      "Epoch: 24312 mean train loss:  4.71978169e-03, bound:  3.15449178e-01\n",
      "Epoch: 24313 mean train loss:  4.71941940e-03, bound:  3.15449178e-01\n",
      "Epoch: 24314 mean train loss:  4.71918844e-03, bound:  3.15449119e-01\n",
      "Epoch: 24315 mean train loss:  4.71902872e-03, bound:  3.15449119e-01\n",
      "Epoch: 24316 mean train loss:  4.71879961e-03, bound:  3.15449089e-01\n",
      "Epoch: 24317 mean train loss:  4.71862080e-03, bound:  3.15449089e-01\n",
      "Epoch: 24318 mean train loss:  4.71834326e-03, bound:  3.15449059e-01\n",
      "Epoch: 24319 mean train loss:  4.71810345e-03, bound:  3.15449059e-01\n",
      "Epoch: 24320 mean train loss:  4.71774396e-03, bound:  3.15448999e-01\n",
      "Epoch: 24321 mean train loss:  4.71741101e-03, bound:  3.15448999e-01\n",
      "Epoch: 24322 mean train loss:  4.71714186e-03, bound:  3.15448970e-01\n",
      "Epoch: 24323 mean train loss:  4.71687084e-03, bound:  3.15448970e-01\n",
      "Epoch: 24324 mean train loss:  4.71662357e-03, bound:  3.15448970e-01\n",
      "Epoch: 24325 mean train loss:  4.71639168e-03, bound:  3.15448910e-01\n",
      "Epoch: 24326 mean train loss:  4.71619889e-03, bound:  3.15448910e-01\n",
      "Epoch: 24327 mean train loss:  4.71599679e-03, bound:  3.15448880e-01\n",
      "Epoch: 24328 mean train loss:  4.71572764e-03, bound:  3.15448880e-01\n",
      "Epoch: 24329 mean train loss:  4.71547432e-03, bound:  3.15448850e-01\n",
      "Epoch: 24330 mean train loss:  4.71520284e-03, bound:  3.15448880e-01\n",
      "Epoch: 24331 mean train loss:  4.71488666e-03, bound:  3.15448821e-01\n",
      "Epoch: 24332 mean train loss:  4.71459050e-03, bound:  3.15448791e-01\n",
      "Epoch: 24333 mean train loss:  4.71430598e-03, bound:  3.15448761e-01\n",
      "Epoch: 24334 mean train loss:  4.71400097e-03, bound:  3.15448761e-01\n",
      "Epoch: 24335 mean train loss:  4.71374951e-03, bound:  3.15448761e-01\n",
      "Epoch: 24336 mean train loss:  4.71350225e-03, bound:  3.15448731e-01\n",
      "Epoch: 24337 mean train loss:  4.71328758e-03, bound:  3.15448672e-01\n",
      "Epoch: 24338 mean train loss:  4.71305009e-03, bound:  3.15448672e-01\n",
      "Epoch: 24339 mean train loss:  4.71287081e-03, bound:  3.15448672e-01\n",
      "Epoch: 24340 mean train loss:  4.71259607e-03, bound:  3.15448642e-01\n",
      "Epoch: 24341 mean train loss:  4.71238606e-03, bound:  3.15448642e-01\n",
      "Epoch: 24342 mean train loss:  4.71211784e-03, bound:  3.15448612e-01\n",
      "Epoch: 24343 mean train loss:  4.71182540e-03, bound:  3.15448552e-01\n",
      "Epoch: 24344 mean train loss:  4.71159955e-03, bound:  3.15448552e-01\n",
      "Epoch: 24345 mean train loss:  4.71128104e-03, bound:  3.15448552e-01\n",
      "Epoch: 24346 mean train loss:  4.71100025e-03, bound:  3.15448523e-01\n",
      "Epoch: 24347 mean train loss:  4.71076090e-03, bound:  3.15448523e-01\n",
      "Epoch: 24348 mean train loss:  4.71046241e-03, bound:  3.15448493e-01\n",
      "Epoch: 24349 mean train loss:  4.71018953e-03, bound:  3.15448463e-01\n",
      "Epoch: 24350 mean train loss:  4.70987381e-03, bound:  3.15448463e-01\n",
      "Epoch: 24351 mean train loss:  4.70964238e-03, bound:  3.15448433e-01\n",
      "Epoch: 24352 mean train loss:  4.70940582e-03, bound:  3.15448433e-01\n",
      "Epoch: 24353 mean train loss:  4.70914226e-03, bound:  3.15448403e-01\n",
      "Epoch: 24354 mean train loss:  4.70885262e-03, bound:  3.15448403e-01\n",
      "Epoch: 24355 mean train loss:  4.70855366e-03, bound:  3.15448374e-01\n",
      "Epoch: 24356 mean train loss:  4.70829150e-03, bound:  3.15448374e-01\n",
      "Epoch: 24357 mean train loss:  4.70802886e-03, bound:  3.15448314e-01\n",
      "Epoch: 24358 mean train loss:  4.70782071e-03, bound:  3.15448314e-01\n",
      "Epoch: 24359 mean train loss:  4.70751664e-03, bound:  3.15448284e-01\n",
      "Epoch: 24360 mean train loss:  4.70727077e-03, bound:  3.15448284e-01\n",
      "Epoch: 24361 mean train loss:  4.70703188e-03, bound:  3.15448225e-01\n",
      "Epoch: 24362 mean train loss:  4.70675528e-03, bound:  3.15448225e-01\n",
      "Epoch: 24363 mean train loss:  4.70654713e-03, bound:  3.15448195e-01\n",
      "Epoch: 24364 mean train loss:  4.70624771e-03, bound:  3.15448195e-01\n",
      "Epoch: 24365 mean train loss:  4.70601302e-03, bound:  3.15448195e-01\n",
      "Epoch: 24366 mean train loss:  4.70571732e-03, bound:  3.15448165e-01\n",
      "Epoch: 24367 mean train loss:  4.70541744e-03, bound:  3.15448105e-01\n",
      "Epoch: 24368 mean train loss:  4.70522512e-03, bound:  3.15448105e-01\n",
      "Epoch: 24369 mean train loss:  4.70494898e-03, bound:  3.15448076e-01\n",
      "Epoch: 24370 mean train loss:  4.70464630e-03, bound:  3.15448076e-01\n",
      "Epoch: 24371 mean train loss:  4.70444281e-03, bound:  3.15448076e-01\n",
      "Epoch: 24372 mean train loss:  4.70417459e-03, bound:  3.15448076e-01\n",
      "Epoch: 24373 mean train loss:  4.70392127e-03, bound:  3.15447986e-01\n",
      "Epoch: 24374 mean train loss:  4.70361020e-03, bound:  3.15447986e-01\n",
      "Epoch: 24375 mean train loss:  4.70335642e-03, bound:  3.15447956e-01\n",
      "Epoch: 24376 mean train loss:  4.70315851e-03, bound:  3.15447956e-01\n",
      "Epoch: 24377 mean train loss:  4.70291544e-03, bound:  3.15447956e-01\n",
      "Epoch: 24378 mean train loss:  4.70261043e-03, bound:  3.15447956e-01\n",
      "Epoch: 24379 mean train loss:  4.70240274e-03, bound:  3.15447897e-01\n",
      "Epoch: 24380 mean train loss:  4.70212614e-03, bound:  3.15447897e-01\n",
      "Epoch: 24381 mean train loss:  4.70190076e-03, bound:  3.15447867e-01\n",
      "Epoch: 24382 mean train loss:  4.70167631e-03, bound:  3.15447837e-01\n",
      "Epoch: 24383 mean train loss:  4.70142765e-03, bound:  3.15447837e-01\n",
      "Epoch: 24384 mean train loss:  4.70130146e-03, bound:  3.15447837e-01\n",
      "Epoch: 24385 mean train loss:  4.70117293e-03, bound:  3.15447778e-01\n",
      "Epoch: 24386 mean train loss:  4.70113382e-03, bound:  3.15447778e-01\n",
      "Epoch: 24387 mean train loss:  4.70099598e-03, bound:  3.15447748e-01\n",
      "Epoch: 24388 mean train loss:  4.70117200e-03, bound:  3.15447748e-01\n",
      "Epoch: 24389 mean train loss:  4.70142346e-03, bound:  3.15447718e-01\n",
      "Epoch: 24390 mean train loss:  4.70189843e-03, bound:  3.15447718e-01\n",
      "Epoch: 24391 mean train loss:  4.70282417e-03, bound:  3.15447658e-01\n",
      "Epoch: 24392 mean train loss:  4.70418530e-03, bound:  3.15447658e-01\n",
      "Epoch: 24393 mean train loss:  4.70630638e-03, bound:  3.15447628e-01\n",
      "Epoch: 24394 mean train loss:  4.70920280e-03, bound:  3.15447628e-01\n",
      "Epoch: 24395 mean train loss:  4.71311575e-03, bound:  3.15447539e-01\n",
      "Epoch: 24396 mean train loss:  4.71759494e-03, bound:  3.15447628e-01\n",
      "Epoch: 24397 mean train loss:  4.72187297e-03, bound:  3.15447509e-01\n",
      "Epoch: 24398 mean train loss:  4.72421292e-03, bound:  3.15447599e-01\n",
      "Epoch: 24399 mean train loss:  4.72272560e-03, bound:  3.15447479e-01\n",
      "Epoch: 24400 mean train loss:  4.71674278e-03, bound:  3.15447539e-01\n",
      "Epoch: 24401 mean train loss:  4.70792456e-03, bound:  3.15447479e-01\n",
      "Epoch: 24402 mean train loss:  4.69995616e-03, bound:  3.15447479e-01\n",
      "Epoch: 24403 mean train loss:  4.69618663e-03, bound:  3.15447479e-01\n",
      "Epoch: 24404 mean train loss:  4.69724229e-03, bound:  3.15447420e-01\n",
      "Epoch: 24405 mean train loss:  4.70105279e-03, bound:  3.15447479e-01\n",
      "Epoch: 24406 mean train loss:  4.70456760e-03, bound:  3.15447390e-01\n",
      "Epoch: 24407 mean train loss:  4.70527261e-03, bound:  3.15447420e-01\n",
      "Epoch: 24408 mean train loss:  4.70277853e-03, bound:  3.15447330e-01\n",
      "Epoch: 24409 mean train loss:  4.69852146e-03, bound:  3.15447390e-01\n",
      "Epoch: 24410 mean train loss:  4.69515799e-03, bound:  3.15447330e-01\n",
      "Epoch: 24411 mean train loss:  4.69424203e-03, bound:  3.15447301e-01\n",
      "Epoch: 24412 mean train loss:  4.69566230e-03, bound:  3.15447301e-01\n",
      "Epoch: 24413 mean train loss:  4.69749980e-03, bound:  3.15447271e-01\n",
      "Epoch: 24414 mean train loss:  4.69815917e-03, bound:  3.15447271e-01\n",
      "Epoch: 24415 mean train loss:  4.69710212e-03, bound:  3.15447271e-01\n",
      "Epoch: 24416 mean train loss:  4.69503785e-03, bound:  3.15447271e-01\n",
      "Epoch: 24417 mean train loss:  4.69314167e-03, bound:  3.15447211e-01\n",
      "Epoch: 24418 mean train loss:  4.69242688e-03, bound:  3.15447211e-01\n",
      "Epoch: 24419 mean train loss:  4.69282828e-03, bound:  3.15447211e-01\n",
      "Epoch: 24420 mean train loss:  4.69351606e-03, bound:  3.15447152e-01\n",
      "Epoch: 24421 mean train loss:  4.69384296e-03, bound:  3.15447152e-01\n",
      "Epoch: 24422 mean train loss:  4.69336053e-03, bound:  3.15447092e-01\n",
      "Epoch: 24423 mean train loss:  4.69230721e-03, bound:  3.15447152e-01\n",
      "Epoch: 24424 mean train loss:  4.69129719e-03, bound:  3.15447092e-01\n",
      "Epoch: 24425 mean train loss:  4.69070440e-03, bound:  3.15447062e-01\n",
      "Epoch: 24426 mean train loss:  4.69068531e-03, bound:  3.15447062e-01\n",
      "Epoch: 24427 mean train loss:  4.69081337e-03, bound:  3.15447003e-01\n",
      "Epoch: 24428 mean train loss:  4.69088601e-03, bound:  3.15447003e-01\n",
      "Epoch: 24429 mean train loss:  4.69074026e-03, bound:  3.15446973e-01\n",
      "Epoch: 24430 mean train loss:  4.69009066e-03, bound:  3.15446973e-01\n",
      "Epoch: 24431 mean train loss:  4.68946295e-03, bound:  3.15446943e-01\n",
      "Epoch: 24432 mean train loss:  4.68903547e-03, bound:  3.15446943e-01\n",
      "Epoch: 24433 mean train loss:  4.68872581e-03, bound:  3.15446943e-01\n",
      "Epoch: 24434 mean train loss:  4.68866620e-03, bound:  3.15446883e-01\n",
      "Epoch: 24435 mean train loss:  4.68861731e-03, bound:  3.15446883e-01\n",
      "Epoch: 24436 mean train loss:  4.68841800e-03, bound:  3.15446854e-01\n",
      "Epoch: 24437 mean train loss:  4.68809297e-03, bound:  3.15446854e-01\n",
      "Epoch: 24438 mean train loss:  4.68763523e-03, bound:  3.15446824e-01\n",
      "Epoch: 24439 mean train loss:  4.68723848e-03, bound:  3.15446824e-01\n",
      "Epoch: 24440 mean train loss:  4.68698749e-03, bound:  3.15446764e-01\n",
      "Epoch: 24441 mean train loss:  4.68681101e-03, bound:  3.15446734e-01\n",
      "Epoch: 24442 mean train loss:  4.68665361e-03, bound:  3.15446734e-01\n",
      "Epoch: 24443 mean train loss:  4.68645804e-03, bound:  3.15446734e-01\n",
      "Epoch: 24444 mean train loss:  4.68627829e-03, bound:  3.15446705e-01\n",
      "Epoch: 24445 mean train loss:  4.68586665e-03, bound:  3.15446705e-01\n",
      "Epoch: 24446 mean train loss:  4.68555233e-03, bound:  3.15446645e-01\n",
      "Epoch: 24447 mean train loss:  4.68525896e-03, bound:  3.15446645e-01\n",
      "Epoch: 24448 mean train loss:  4.68499260e-03, bound:  3.15446645e-01\n",
      "Epoch: 24449 mean train loss:  4.68475604e-03, bound:  3.15446645e-01\n",
      "Epoch: 24450 mean train loss:  4.68461681e-03, bound:  3.15446585e-01\n",
      "Epoch: 24451 mean train loss:  4.68438771e-03, bound:  3.15446585e-01\n",
      "Epoch: 24452 mean train loss:  4.68412787e-03, bound:  3.15446526e-01\n",
      "Epoch: 24453 mean train loss:  4.68383962e-03, bound:  3.15446526e-01\n",
      "Epoch: 24454 mean train loss:  4.68354113e-03, bound:  3.15446526e-01\n",
      "Epoch: 24455 mean train loss:  4.68326313e-03, bound:  3.15446526e-01\n",
      "Epoch: 24456 mean train loss:  4.68302239e-03, bound:  3.15446496e-01\n",
      "Epoch: 24457 mean train loss:  4.68277792e-03, bound:  3.15446466e-01\n",
      "Epoch: 24458 mean train loss:  4.68257768e-03, bound:  3.15446436e-01\n",
      "Epoch: 24459 mean train loss:  4.68232809e-03, bound:  3.15446436e-01\n",
      "Epoch: 24460 mean train loss:  4.68208268e-03, bound:  3.15446436e-01\n",
      "Epoch: 24461 mean train loss:  4.68183542e-03, bound:  3.15446407e-01\n",
      "Epoch: 24462 mean train loss:  4.68156673e-03, bound:  3.15446407e-01\n",
      "Epoch: 24463 mean train loss:  4.68130596e-03, bound:  3.15446377e-01\n",
      "Epoch: 24464 mean train loss:  4.68101958e-03, bound:  3.15446377e-01\n",
      "Epoch: 24465 mean train loss:  4.68074577e-03, bound:  3.15446317e-01\n",
      "Epoch: 24466 mean train loss:  4.68057487e-03, bound:  3.15446317e-01\n",
      "Epoch: 24467 mean train loss:  4.68034204e-03, bound:  3.15446287e-01\n",
      "Epoch: 24468 mean train loss:  4.68008639e-03, bound:  3.15446258e-01\n",
      "Epoch: 24469 mean train loss:  4.67985962e-03, bound:  3.15446258e-01\n",
      "Epoch: 24470 mean train loss:  4.67960630e-03, bound:  3.15446258e-01\n",
      "Epoch: 24471 mean train loss:  4.67931619e-03, bound:  3.15446198e-01\n",
      "Epoch: 24472 mean train loss:  4.67911782e-03, bound:  3.15446198e-01\n",
      "Epoch: 24473 mean train loss:  4.67884215e-03, bound:  3.15446168e-01\n",
      "Epoch: 24474 mean train loss:  4.67854412e-03, bound:  3.15446168e-01\n",
      "Epoch: 24475 mean train loss:  4.67831548e-03, bound:  3.15446138e-01\n",
      "Epoch: 24476 mean train loss:  4.67802119e-03, bound:  3.15446138e-01\n",
      "Epoch: 24477 mean train loss:  4.67778277e-03, bound:  3.15446079e-01\n",
      "Epoch: 24478 mean train loss:  4.67755366e-03, bound:  3.15446079e-01\n",
      "Epoch: 24479 mean train loss:  4.67730081e-03, bound:  3.15446049e-01\n",
      "Epoch: 24480 mean train loss:  4.67704330e-03, bound:  3.15446049e-01\n",
      "Epoch: 24481 mean train loss:  4.67675645e-03, bound:  3.15446049e-01\n",
      "Epoch: 24482 mean train loss:  4.67653899e-03, bound:  3.15446019e-01\n",
      "Epoch: 24483 mean train loss:  4.67625679e-03, bound:  3.15445989e-01\n",
      "Epoch: 24484 mean train loss:  4.67603933e-03, bound:  3.15445960e-01\n",
      "Epoch: 24485 mean train loss:  4.67582466e-03, bound:  3.15445960e-01\n",
      "Epoch: 24486 mean train loss:  4.67550522e-03, bound:  3.15445960e-01\n",
      "Epoch: 24487 mean train loss:  4.67530824e-03, bound:  3.15445900e-01\n",
      "Epoch: 24488 mean train loss:  4.67503304e-03, bound:  3.15445900e-01\n",
      "Epoch: 24489 mean train loss:  4.67482349e-03, bound:  3.15445900e-01\n",
      "Epoch: 24490 mean train loss:  4.67449985e-03, bound:  3.15445840e-01\n",
      "Epoch: 24491 mean train loss:  4.67428192e-03, bound:  3.15445840e-01\n",
      "Epoch: 24492 mean train loss:  4.67405468e-03, bound:  3.15445840e-01\n",
      "Epoch: 24493 mean train loss:  4.67374036e-03, bound:  3.15445840e-01\n",
      "Epoch: 24494 mean train loss:  4.67349915e-03, bound:  3.15445781e-01\n",
      "Epoch: 24495 mean train loss:  4.67327749e-03, bound:  3.15445781e-01\n",
      "Epoch: 24496 mean train loss:  4.67302790e-03, bound:  3.15445721e-01\n",
      "Epoch: 24497 mean train loss:  4.67275502e-03, bound:  3.15445721e-01\n",
      "Epoch: 24498 mean train loss:  4.67254641e-03, bound:  3.15445721e-01\n",
      "Epoch: 24499 mean train loss:  4.67229029e-03, bound:  3.15445721e-01\n",
      "Epoch: 24500 mean train loss:  4.67204303e-03, bound:  3.15445662e-01\n",
      "Epoch: 24501 mean train loss:  4.67181439e-03, bound:  3.15445662e-01\n",
      "Epoch: 24502 mean train loss:  4.67154710e-03, bound:  3.15445632e-01\n",
      "Epoch: 24503 mean train loss:  4.67125513e-03, bound:  3.15445632e-01\n",
      "Epoch: 24504 mean train loss:  4.67099575e-03, bound:  3.15445602e-01\n",
      "Epoch: 24505 mean train loss:  4.67077550e-03, bound:  3.15445602e-01\n",
      "Epoch: 24506 mean train loss:  4.67044534e-03, bound:  3.15445542e-01\n",
      "Epoch: 24507 mean train loss:  4.67025675e-03, bound:  3.15445542e-01\n",
      "Epoch: 24508 mean train loss:  4.66999924e-03, bound:  3.15445513e-01\n",
      "Epoch: 24509 mean train loss:  4.66978457e-03, bound:  3.15445513e-01\n",
      "Epoch: 24510 mean train loss:  4.66951448e-03, bound:  3.15445483e-01\n",
      "Epoch: 24511 mean train loss:  4.66921646e-03, bound:  3.15445483e-01\n",
      "Epoch: 24512 mean train loss:  4.66902275e-03, bound:  3.15445423e-01\n",
      "Epoch: 24513 mean train loss:  4.66877641e-03, bound:  3.15445423e-01\n",
      "Epoch: 24514 mean train loss:  4.66851797e-03, bound:  3.15445393e-01\n",
      "Epoch: 24515 mean train loss:  4.66824882e-03, bound:  3.15445393e-01\n",
      "Epoch: 24516 mean train loss:  4.66802809e-03, bound:  3.15445364e-01\n",
      "Epoch: 24517 mean train loss:  4.66777710e-03, bound:  3.15445364e-01\n",
      "Epoch: 24518 mean train loss:  4.66749305e-03, bound:  3.15445334e-01\n",
      "Epoch: 24519 mean train loss:  4.66724066e-03, bound:  3.15445334e-01\n",
      "Epoch: 24520 mean train loss:  4.66700178e-03, bound:  3.15445304e-01\n",
      "Epoch: 24521 mean train loss:  4.66673635e-03, bound:  3.15445274e-01\n",
      "Epoch: 24522 mean train loss:  4.66647325e-03, bound:  3.15445274e-01\n",
      "Epoch: 24523 mean train loss:  4.66620130e-03, bound:  3.15445244e-01\n",
      "Epoch: 24524 mean train loss:  4.66596615e-03, bound:  3.15445185e-01\n",
      "Epoch: 24525 mean train loss:  4.66572586e-03, bound:  3.15445185e-01\n",
      "Epoch: 24526 mean train loss:  4.66547860e-03, bound:  3.15445185e-01\n",
      "Epoch: 24527 mean train loss:  4.66520386e-03, bound:  3.15445155e-01\n",
      "Epoch: 24528 mean train loss:  4.66492074e-03, bound:  3.15445125e-01\n",
      "Epoch: 24529 mean train loss:  4.66468278e-03, bound:  3.15445125e-01\n",
      "Epoch: 24530 mean train loss:  4.66443039e-03, bound:  3.15445065e-01\n",
      "Epoch: 24531 mean train loss:  4.66418127e-03, bound:  3.15445065e-01\n",
      "Epoch: 24532 mean train loss:  4.66391258e-03, bound:  3.15445036e-01\n",
      "Epoch: 24533 mean train loss:  4.66371328e-03, bound:  3.15445036e-01\n",
      "Epoch: 24534 mean train loss:  4.66347858e-03, bound:  3.15445036e-01\n",
      "Epoch: 24535 mean train loss:  4.66322759e-03, bound:  3.15445036e-01\n",
      "Epoch: 24536 mean train loss:  4.66296775e-03, bound:  3.15445006e-01\n",
      "Epoch: 24537 mean train loss:  4.66271862e-03, bound:  3.15444976e-01\n",
      "Epoch: 24538 mean train loss:  4.66246810e-03, bound:  3.15444916e-01\n",
      "Epoch: 24539 mean train loss:  4.66226554e-03, bound:  3.15444916e-01\n",
      "Epoch: 24540 mean train loss:  4.66199312e-03, bound:  3.15444916e-01\n",
      "Epoch: 24541 mean train loss:  4.66180639e-03, bound:  3.15444916e-01\n",
      "Epoch: 24542 mean train loss:  4.66153864e-03, bound:  3.15444857e-01\n",
      "Epoch: 24543 mean train loss:  4.66135144e-03, bound:  3.15444857e-01\n",
      "Epoch: 24544 mean train loss:  4.66120569e-03, bound:  3.15444827e-01\n",
      "Epoch: 24545 mean train loss:  4.66100732e-03, bound:  3.15444797e-01\n",
      "Epoch: 24546 mean train loss:  4.66085086e-03, bound:  3.15444797e-01\n",
      "Epoch: 24547 mean train loss:  4.66079870e-03, bound:  3.15444797e-01\n",
      "Epoch: 24548 mean train loss:  4.66072792e-03, bound:  3.15444797e-01\n",
      "Epoch: 24549 mean train loss:  4.66090441e-03, bound:  3.15444738e-01\n",
      "Epoch: 24550 mean train loss:  4.66108089e-03, bound:  3.15444738e-01\n",
      "Epoch: 24551 mean train loss:  4.66156937e-03, bound:  3.15444708e-01\n",
      "Epoch: 24552 mean train loss:  4.66226554e-03, bound:  3.15444708e-01\n",
      "Epoch: 24553 mean train loss:  4.66324203e-03, bound:  3.15444618e-01\n",
      "Epoch: 24554 mean train loss:  4.66453936e-03, bound:  3.15444678e-01\n",
      "Epoch: 24555 mean train loss:  4.66627907e-03, bound:  3.15444618e-01\n",
      "Epoch: 24556 mean train loss:  4.66858037e-03, bound:  3.15444618e-01\n",
      "Epoch: 24557 mean train loss:  4.67108516e-03, bound:  3.15444589e-01\n",
      "Epoch: 24558 mean train loss:  4.67349868e-03, bound:  3.15444618e-01\n",
      "Epoch: 24559 mean train loss:  4.67507262e-03, bound:  3.15444499e-01\n",
      "Epoch: 24560 mean train loss:  4.67484863e-03, bound:  3.15444589e-01\n",
      "Epoch: 24561 mean train loss:  4.67241881e-03, bound:  3.15444499e-01\n",
      "Epoch: 24562 mean train loss:  4.66794241e-03, bound:  3.15444559e-01\n",
      "Epoch: 24563 mean train loss:  4.66255751e-03, bound:  3.15444469e-01\n",
      "Epoch: 24564 mean train loss:  4.65803966e-03, bound:  3.15444469e-01\n",
      "Epoch: 24565 mean train loss:  4.65573510e-03, bound:  3.15444440e-01\n",
      "Epoch: 24566 mean train loss:  4.65577329e-03, bound:  3.15444440e-01\n",
      "Epoch: 24567 mean train loss:  4.65728110e-03, bound:  3.15444440e-01\n",
      "Epoch: 24568 mean train loss:  4.65917168e-03, bound:  3.15444410e-01\n",
      "Epoch: 24569 mean train loss:  4.66026459e-03, bound:  3.15444410e-01\n",
      "Epoch: 24570 mean train loss:  4.66000428e-03, bound:  3.15444350e-01\n",
      "Epoch: 24571 mean train loss:  4.65854583e-03, bound:  3.15444380e-01\n",
      "Epoch: 24572 mean train loss:  4.65642102e-03, bound:  3.15444291e-01\n",
      "Epoch: 24573 mean train loss:  4.65450808e-03, bound:  3.15444291e-01\n",
      "Epoch: 24574 mean train loss:  4.65345150e-03, bound:  3.15444291e-01\n",
      "Epoch: 24575 mean train loss:  4.65335092e-03, bound:  3.15444291e-01\n",
      "Epoch: 24576 mean train loss:  4.65390040e-03, bound:  3.15444291e-01\n",
      "Epoch: 24577 mean train loss:  4.65454347e-03, bound:  3.15444231e-01\n",
      "Epoch: 24578 mean train loss:  4.65475861e-03, bound:  3.15444231e-01\n",
      "Epoch: 24579 mean train loss:  4.65442101e-03, bound:  3.15444231e-01\n",
      "Epoch: 24580 mean train loss:  4.65352321e-03, bound:  3.15444231e-01\n",
      "Epoch: 24581 mean train loss:  4.65248711e-03, bound:  3.15444171e-01\n",
      "Epoch: 24582 mean train loss:  4.65162378e-03, bound:  3.15444171e-01\n",
      "Epoch: 24583 mean train loss:  4.65114927e-03, bound:  3.15444142e-01\n",
      "Epoch: 24584 mean train loss:  4.65104496e-03, bound:  3.15444142e-01\n",
      "Epoch: 24585 mean train loss:  4.65115346e-03, bound:  3.15444112e-01\n",
      "Epoch: 24586 mean train loss:  4.65116743e-03, bound:  3.15444052e-01\n",
      "Epoch: 24587 mean train loss:  4.65101097e-03, bound:  3.15444052e-01\n",
      "Epoch: 24588 mean train loss:  4.65064216e-03, bound:  3.15444022e-01\n",
      "Epoch: 24589 mean train loss:  4.65019885e-03, bound:  3.15444022e-01\n",
      "Epoch: 24590 mean train loss:  4.64966055e-03, bound:  3.15444022e-01\n",
      "Epoch: 24591 mean train loss:  4.64926613e-03, bound:  3.15443963e-01\n",
      "Epoch: 24592 mean train loss:  4.64896858e-03, bound:  3.15443963e-01\n",
      "Epoch: 24593 mean train loss:  4.64876369e-03, bound:  3.15443933e-01\n",
      "Epoch: 24594 mean train loss:  4.64865472e-03, bound:  3.15443933e-01\n",
      "Epoch: 24595 mean train loss:  4.64854343e-03, bound:  3.15443903e-01\n",
      "Epoch: 24596 mean train loss:  4.64836089e-03, bound:  3.15443903e-01\n",
      "Epoch: 24597 mean train loss:  4.64807125e-03, bound:  3.15443844e-01\n",
      "Epoch: 24598 mean train loss:  4.64769732e-03, bound:  3.15443844e-01\n",
      "Epoch: 24599 mean train loss:  4.64732107e-03, bound:  3.15443814e-01\n",
      "Epoch: 24600 mean train loss:  4.64700116e-03, bound:  3.15443814e-01\n",
      "Epoch: 24601 mean train loss:  4.64671711e-03, bound:  3.15443814e-01\n",
      "Epoch: 24602 mean train loss:  4.64654807e-03, bound:  3.15443784e-01\n",
      "Epoch: 24603 mean train loss:  4.64634085e-03, bound:  3.15443724e-01\n",
      "Epoch: 24604 mean train loss:  4.64615179e-03, bound:  3.15443724e-01\n",
      "Epoch: 24605 mean train loss:  4.64593107e-03, bound:  3.15443724e-01\n",
      "Epoch: 24606 mean train loss:  4.64574713e-03, bound:  3.15443695e-01\n",
      "Epoch: 24607 mean train loss:  4.64551290e-03, bound:  3.15443695e-01\n",
      "Epoch: 24608 mean train loss:  4.64521674e-03, bound:  3.15443665e-01\n",
      "Epoch: 24609 mean train loss:  4.64487588e-03, bound:  3.15443665e-01\n",
      "Epoch: 24610 mean train loss:  4.64463839e-03, bound:  3.15443605e-01\n",
      "Epoch: 24611 mean train loss:  4.64429101e-03, bound:  3.15443605e-01\n",
      "Epoch: 24612 mean train loss:  4.64403862e-03, bound:  3.15443605e-01\n",
      "Epoch: 24613 mean train loss:  4.64377366e-03, bound:  3.15443575e-01\n",
      "Epoch: 24614 mean train loss:  4.64362046e-03, bound:  3.15443546e-01\n",
      "Epoch: 24615 mean train loss:  4.64338670e-03, bound:  3.15443516e-01\n",
      "Epoch: 24616 mean train loss:  4.64307889e-03, bound:  3.15443486e-01\n",
      "Epoch: 24617 mean train loss:  4.64290148e-03, bound:  3.15443486e-01\n",
      "Epoch: 24618 mean train loss:  4.64262953e-03, bound:  3.15443486e-01\n",
      "Epoch: 24619 mean train loss:  4.64234874e-03, bound:  3.15443486e-01\n",
      "Epoch: 24620 mean train loss:  4.64210752e-03, bound:  3.15443456e-01\n",
      "Epoch: 24621 mean train loss:  4.64184675e-03, bound:  3.15443456e-01\n",
      "Epoch: 24622 mean train loss:  4.64155944e-03, bound:  3.15443397e-01\n",
      "Epoch: 24623 mean train loss:  4.64131823e-03, bound:  3.15443397e-01\n",
      "Epoch: 24624 mean train loss:  4.64107096e-03, bound:  3.15443367e-01\n",
      "Epoch: 24625 mean train loss:  4.64081997e-03, bound:  3.15443367e-01\n",
      "Epoch: 24626 mean train loss:  4.64061927e-03, bound:  3.15443337e-01\n",
      "Epoch: 24627 mean train loss:  4.64036642e-03, bound:  3.15443337e-01\n",
      "Epoch: 24628 mean train loss:  4.64013359e-03, bound:  3.15443277e-01\n",
      "Epoch: 24629 mean train loss:  4.63986769e-03, bound:  3.15443277e-01\n",
      "Epoch: 24630 mean train loss:  4.63957945e-03, bound:  3.15443248e-01\n",
      "Epoch: 24631 mean train loss:  4.63934336e-03, bound:  3.15443248e-01\n",
      "Epoch: 24632 mean train loss:  4.63907840e-03, bound:  3.15443218e-01\n",
      "Epoch: 24633 mean train loss:  4.63880366e-03, bound:  3.15443218e-01\n",
      "Epoch: 24634 mean train loss:  4.63861460e-03, bound:  3.15443218e-01\n",
      "Epoch: 24635 mean train loss:  4.63832496e-03, bound:  3.15443158e-01\n",
      "Epoch: 24636 mean train loss:  4.63810423e-03, bound:  3.15443158e-01\n",
      "Epoch: 24637 mean train loss:  4.63780202e-03, bound:  3.15443128e-01\n",
      "Epoch: 24638 mean train loss:  4.63762647e-03, bound:  3.15443099e-01\n",
      "Epoch: 24639 mean train loss:  4.63734753e-03, bound:  3.15443099e-01\n",
      "Epoch: 24640 mean train loss:  4.63710772e-03, bound:  3.15443039e-01\n",
      "Epoch: 24641 mean train loss:  4.63684415e-03, bound:  3.15443039e-01\n",
      "Epoch: 24642 mean train loss:  4.63656383e-03, bound:  3.15443039e-01\n",
      "Epoch: 24643 mean train loss:  4.63638688e-03, bound:  3.15443009e-01\n",
      "Epoch: 24644 mean train loss:  4.63615777e-03, bound:  3.15442979e-01\n",
      "Epoch: 24645 mean train loss:  4.63588210e-03, bound:  3.15442979e-01\n",
      "Epoch: 24646 mean train loss:  4.63566789e-03, bound:  3.15442950e-01\n",
      "Epoch: 24647 mean train loss:  4.63537360e-03, bound:  3.15442920e-01\n",
      "Epoch: 24648 mean train loss:  4.63514356e-03, bound:  3.15442920e-01\n",
      "Epoch: 24649 mean train loss:  4.63490048e-03, bound:  3.15442890e-01\n",
      "Epoch: 24650 mean train loss:  4.63464390e-03, bound:  3.15442890e-01\n",
      "Epoch: 24651 mean train loss:  4.63444134e-03, bound:  3.15442860e-01\n",
      "Epoch: 24652 mean train loss:  4.63421503e-03, bound:  3.15442830e-01\n",
      "Epoch: 24653 mean train loss:  4.63387743e-03, bound:  3.15442801e-01\n",
      "Epoch: 24654 mean train loss:  4.63379547e-03, bound:  3.15442801e-01\n",
      "Epoch: 24655 mean train loss:  4.63348813e-03, bound:  3.15442801e-01\n",
      "Epoch: 24656 mean train loss:  4.63329535e-03, bound:  3.15442771e-01\n",
      "Epoch: 24657 mean train loss:  4.63298988e-03, bound:  3.15442771e-01\n",
      "Epoch: 24658 mean train loss:  4.63276217e-03, bound:  3.15442711e-01\n",
      "Epoch: 24659 mean train loss:  4.63248882e-03, bound:  3.15442681e-01\n",
      "Epoch: 24660 mean train loss:  4.63226438e-03, bound:  3.15442681e-01\n",
      "Epoch: 24661 mean train loss:  4.63201758e-03, bound:  3.15442681e-01\n",
      "Epoch: 24662 mean train loss:  4.63181594e-03, bound:  3.15442681e-01\n",
      "Epoch: 24663 mean train loss:  4.63154167e-03, bound:  3.15442622e-01\n",
      "Epoch: 24664 mean train loss:  4.63137496e-03, bound:  3.15442592e-01\n",
      "Epoch: 24665 mean train loss:  4.63115936e-03, bound:  3.15442562e-01\n",
      "Epoch: 24666 mean train loss:  4.63096099e-03, bound:  3.15442562e-01\n",
      "Epoch: 24667 mean train loss:  4.63079708e-03, bound:  3.15442562e-01\n",
      "Epoch: 24668 mean train loss:  4.63063922e-03, bound:  3.15442532e-01\n",
      "Epoch: 24669 mean train loss:  4.63056052e-03, bound:  3.15442502e-01\n",
      "Epoch: 24670 mean train loss:  4.63045947e-03, bound:  3.15442473e-01\n",
      "Epoch: 24671 mean train loss:  4.63049440e-03, bound:  3.15442473e-01\n",
      "Epoch: 24672 mean train loss:  4.63050650e-03, bound:  3.15442473e-01\n",
      "Epoch: 24673 mean train loss:  4.63073654e-03, bound:  3.15442443e-01\n",
      "Epoch: 24674 mean train loss:  4.63108951e-03, bound:  3.15442383e-01\n",
      "Epoch: 24675 mean train loss:  4.63170558e-03, bound:  3.15442443e-01\n",
      "Epoch: 24676 mean train loss:  4.63256845e-03, bound:  3.15442353e-01\n",
      "Epoch: 24677 mean train loss:  4.63364134e-03, bound:  3.15442383e-01\n",
      "Epoch: 24678 mean train loss:  4.63492516e-03, bound:  3.15442324e-01\n",
      "Epoch: 24679 mean train loss:  4.63631330e-03, bound:  3.15442353e-01\n",
      "Epoch: 24680 mean train loss:  4.63776197e-03, bound:  3.15442264e-01\n",
      "Epoch: 24681 mean train loss:  4.63901693e-03, bound:  3.15442324e-01\n",
      "Epoch: 24682 mean train loss:  4.63977223e-03, bound:  3.15442234e-01\n",
      "Epoch: 24683 mean train loss:  4.63952729e-03, bound:  3.15442264e-01\n",
      "Epoch: 24684 mean train loss:  4.63805767e-03, bound:  3.15442204e-01\n",
      "Epoch: 24685 mean train loss:  4.63538710e-03, bound:  3.15442264e-01\n",
      "Epoch: 24686 mean train loss:  4.63201245e-03, bound:  3.15442175e-01\n",
      "Epoch: 24687 mean train loss:  4.62870765e-03, bound:  3.15442204e-01\n",
      "Epoch: 24688 mean train loss:  4.62622242e-03, bound:  3.15442175e-01\n",
      "Epoch: 24689 mean train loss:  4.62496514e-03, bound:  3.15442175e-01\n",
      "Epoch: 24690 mean train loss:  4.62492602e-03, bound:  3.15442115e-01\n",
      "Epoch: 24691 mean train loss:  4.62563802e-03, bound:  3.15442115e-01\n",
      "Epoch: 24692 mean train loss:  4.62654699e-03, bound:  3.15442115e-01\n",
      "Epoch: 24693 mean train loss:  4.62725759e-03, bound:  3.15442085e-01\n",
      "Epoch: 24694 mean train loss:  4.62737121e-03, bound:  3.15442085e-01\n",
      "Epoch: 24695 mean train loss:  4.62683244e-03, bound:  3.15442026e-01\n",
      "Epoch: 24696 mean train loss:  4.62577445e-03, bound:  3.15442055e-01\n",
      "Epoch: 24697 mean train loss:  4.62449761e-03, bound:  3.15441996e-01\n",
      "Epoch: 24698 mean train loss:  4.62332880e-03, bound:  3.15441996e-01\n",
      "Epoch: 24699 mean train loss:  4.62260144e-03, bound:  3.15441996e-01\n",
      "Epoch: 24700 mean train loss:  4.62223729e-03, bound:  3.15441936e-01\n",
      "Epoch: 24701 mean train loss:  4.62224195e-03, bound:  3.15441936e-01\n",
      "Epoch: 24702 mean train loss:  4.62236116e-03, bound:  3.15441906e-01\n",
      "Epoch: 24703 mean train loss:  4.62249061e-03, bound:  3.15441906e-01\n",
      "Epoch: 24704 mean train loss:  4.62247152e-03, bound:  3.15441877e-01\n",
      "Epoch: 24705 mean train loss:  4.62215673e-03, bound:  3.15441877e-01\n",
      "Epoch: 24706 mean train loss:  4.62169666e-03, bound:  3.15441847e-01\n",
      "Epoch: 24707 mean train loss:  4.62110015e-03, bound:  3.15441817e-01\n",
      "Epoch: 24708 mean train loss:  4.62062284e-03, bound:  3.15441787e-01\n",
      "Epoch: 24709 mean train loss:  4.62016603e-03, bound:  3.15441787e-01\n",
      "Epoch: 24710 mean train loss:  4.61979862e-03, bound:  3.15441787e-01\n",
      "Epoch: 24711 mean train loss:  4.61954856e-03, bound:  3.15441757e-01\n",
      "Epoch: 24712 mean train loss:  4.61939536e-03, bound:  3.15441757e-01\n",
      "Epoch: 24713 mean train loss:  4.61926823e-03, bound:  3.15441698e-01\n",
      "Epoch: 24714 mean train loss:  4.61902888e-03, bound:  3.15441698e-01\n",
      "Epoch: 24715 mean train loss:  4.61881375e-03, bound:  3.15441668e-01\n",
      "Epoch: 24716 mean train loss:  4.61858278e-03, bound:  3.15441668e-01\n",
      "Epoch: 24717 mean train loss:  4.61829733e-03, bound:  3.15441638e-01\n",
      "Epoch: 24718 mean train loss:  4.61800350e-03, bound:  3.15441638e-01\n",
      "Epoch: 24719 mean train loss:  4.61774459e-03, bound:  3.15441579e-01\n",
      "Epoch: 24720 mean train loss:  4.61745774e-03, bound:  3.15441579e-01\n",
      "Epoch: 24721 mean train loss:  4.61715134e-03, bound:  3.15441549e-01\n",
      "Epoch: 24722 mean train loss:  4.61688405e-03, bound:  3.15441549e-01\n",
      "Epoch: 24723 mean train loss:  4.61671455e-03, bound:  3.15441519e-01\n",
      "Epoch: 24724 mean train loss:  4.61645704e-03, bound:  3.15441519e-01\n",
      "Epoch: 24725 mean train loss:  4.61617857e-03, bound:  3.15441459e-01\n",
      "Epoch: 24726 mean train loss:  4.61598672e-03, bound:  3.15441459e-01\n",
      "Epoch: 24727 mean train loss:  4.61576181e-03, bound:  3.15441459e-01\n",
      "Epoch: 24728 mean train loss:  4.61553456e-03, bound:  3.15441430e-01\n",
      "Epoch: 24729 mean train loss:  4.61529987e-03, bound:  3.15441370e-01\n",
      "Epoch: 24730 mean train loss:  4.61504515e-03, bound:  3.15441370e-01\n",
      "Epoch: 24731 mean train loss:  4.61480860e-03, bound:  3.15441370e-01\n",
      "Epoch: 24732 mean train loss:  4.61458741e-03, bound:  3.15441370e-01\n",
      "Epoch: 24733 mean train loss:  4.61431593e-03, bound:  3.15441340e-01\n",
      "Epoch: 24734 mean train loss:  4.61410312e-03, bound:  3.15441310e-01\n",
      "Epoch: 24735 mean train loss:  4.61386377e-03, bound:  3.15441310e-01\n",
      "Epoch: 24736 mean train loss:  4.61361511e-03, bound:  3.15441251e-01\n",
      "Epoch: 24737 mean train loss:  4.61337576e-03, bound:  3.15441251e-01\n",
      "Epoch: 24738 mean train loss:  4.61308705e-03, bound:  3.15441251e-01\n",
      "Epoch: 24739 mean train loss:  4.61277366e-03, bound:  3.15441251e-01\n",
      "Epoch: 24740 mean train loss:  4.61252127e-03, bound:  3.15441221e-01\n",
      "Epoch: 24741 mean train loss:  4.61226376e-03, bound:  3.15441221e-01\n",
      "Epoch: 24742 mean train loss:  4.61202208e-03, bound:  3.15441132e-01\n",
      "Epoch: 24743 mean train loss:  4.61175246e-03, bound:  3.15441132e-01\n",
      "Epoch: 24744 mean train loss:  4.61148843e-03, bound:  3.15441132e-01\n",
      "Epoch: 24745 mean train loss:  4.61124256e-03, bound:  3.15441132e-01\n",
      "Epoch: 24746 mean train loss:  4.61104279e-03, bound:  3.15441102e-01\n",
      "Epoch: 24747 mean train loss:  4.61080577e-03, bound:  3.15441102e-01\n",
      "Epoch: 24748 mean train loss:  4.61051427e-03, bound:  3.15441072e-01\n",
      "Epoch: 24749 mean train loss:  4.61027399e-03, bound:  3.15441072e-01\n",
      "Epoch: 24750 mean train loss:  4.61004861e-03, bound:  3.15441012e-01\n",
      "Epoch: 24751 mean train loss:  4.60983021e-03, bound:  3.15441012e-01\n",
      "Epoch: 24752 mean train loss:  4.60954802e-03, bound:  3.15440983e-01\n",
      "Epoch: 24753 mean train loss:  4.60937573e-03, bound:  3.15440983e-01\n",
      "Epoch: 24754 mean train loss:  4.60904371e-03, bound:  3.15440983e-01\n",
      "Epoch: 24755 mean train loss:  4.60875547e-03, bound:  3.15440923e-01\n",
      "Epoch: 24756 mean train loss:  4.60858038e-03, bound:  3.15440893e-01\n",
      "Epoch: 24757 mean train loss:  4.60832706e-03, bound:  3.15440893e-01\n",
      "Epoch: 24758 mean train loss:  4.60809236e-03, bound:  3.15440863e-01\n",
      "Epoch: 24759 mean train loss:  4.60786279e-03, bound:  3.15440863e-01\n",
      "Epoch: 24760 mean train loss:  4.60759597e-03, bound:  3.15440804e-01\n",
      "Epoch: 24761 mean train loss:  4.60737478e-03, bound:  3.15440804e-01\n",
      "Epoch: 24762 mean train loss:  4.60714428e-03, bound:  3.15440774e-01\n",
      "Epoch: 24763 mean train loss:  4.60685603e-03, bound:  3.15440774e-01\n",
      "Epoch: 24764 mean train loss:  4.60666232e-03, bound:  3.15440744e-01\n",
      "Epoch: 24765 mean train loss:  4.60640993e-03, bound:  3.15440744e-01\n",
      "Epoch: 24766 mean train loss:  4.60615521e-03, bound:  3.15440685e-01\n",
      "Epoch: 24767 mean train loss:  4.60589537e-03, bound:  3.15440685e-01\n",
      "Epoch: 24768 mean train loss:  4.60562250e-03, bound:  3.15440655e-01\n",
      "Epoch: 24769 mean train loss:  4.60535055e-03, bound:  3.15440655e-01\n",
      "Epoch: 24770 mean train loss:  4.60509025e-03, bound:  3.15440655e-01\n",
      "Epoch: 24771 mean train loss:  4.60482948e-03, bound:  3.15440655e-01\n",
      "Epoch: 24772 mean train loss:  4.60462784e-03, bound:  3.15440565e-01\n",
      "Epoch: 24773 mean train loss:  4.60436754e-03, bound:  3.15440565e-01\n",
      "Epoch: 24774 mean train loss:  4.60408395e-03, bound:  3.15440565e-01\n",
      "Epoch: 24775 mean train loss:  4.60390188e-03, bound:  3.15440565e-01\n",
      "Epoch: 24776 mean train loss:  4.60360665e-03, bound:  3.15440565e-01\n",
      "Epoch: 24777 mean train loss:  4.60342923e-03, bound:  3.15440536e-01\n",
      "Epoch: 24778 mean train loss:  4.60317265e-03, bound:  3.15440476e-01\n",
      "Epoch: 24779 mean train loss:  4.60302038e-03, bound:  3.15440446e-01\n",
      "Epoch: 24780 mean train loss:  4.60285507e-03, bound:  3.15440446e-01\n",
      "Epoch: 24781 mean train loss:  4.60273772e-03, bound:  3.15440446e-01\n",
      "Epoch: 24782 mean train loss:  4.60257940e-03, bound:  3.15440446e-01\n",
      "Epoch: 24783 mean train loss:  4.60255984e-03, bound:  3.15440416e-01\n",
      "Epoch: 24784 mean train loss:  4.60267253e-03, bound:  3.15440416e-01\n",
      "Epoch: 24785 mean train loss:  4.60295379e-03, bound:  3.15440327e-01\n",
      "Epoch: 24786 mean train loss:  4.60349070e-03, bound:  3.15440327e-01\n",
      "Epoch: 24787 mean train loss:  4.60440479e-03, bound:  3.15440297e-01\n",
      "Epoch: 24788 mean train loss:  4.60596103e-03, bound:  3.15440327e-01\n",
      "Epoch: 24789 mean train loss:  4.60819341e-03, bound:  3.15440238e-01\n",
      "Epoch: 24790 mean train loss:  4.61140880e-03, bound:  3.15440297e-01\n",
      "Epoch: 24791 mean train loss:  4.61564912e-03, bound:  3.15440208e-01\n",
      "Epoch: 24792 mean train loss:  4.62061306e-03, bound:  3.15440238e-01\n",
      "Epoch: 24793 mean train loss:  4.62534698e-03, bound:  3.15440178e-01\n",
      "Epoch: 24794 mean train loss:  4.62816749e-03, bound:  3.15440238e-01\n",
      "Epoch: 24795 mean train loss:  4.62691393e-03, bound:  3.15440118e-01\n",
      "Epoch: 24796 mean train loss:  4.62058512e-03, bound:  3.15440208e-01\n",
      "Epoch: 24797 mean train loss:  4.61089937e-03, bound:  3.15440118e-01\n",
      "Epoch: 24798 mean train loss:  4.60211234e-03, bound:  3.15440178e-01\n",
      "Epoch: 24799 mean train loss:  4.59808717e-03, bound:  3.15440118e-01\n",
      "Epoch: 24800 mean train loss:  4.59942315e-03, bound:  3.15440089e-01\n",
      "Epoch: 24801 mean train loss:  4.60381620e-03, bound:  3.15440118e-01\n",
      "Epoch: 24802 mean train loss:  4.60761972e-03, bound:  3.15440029e-01\n",
      "Epoch: 24803 mean train loss:  4.60804161e-03, bound:  3.15440089e-01\n",
      "Epoch: 24804 mean train loss:  4.60482296e-03, bound:  3.15439999e-01\n",
      "Epoch: 24805 mean train loss:  4.59996145e-03, bound:  3.15440029e-01\n",
      "Epoch: 24806 mean train loss:  4.59671253e-03, bound:  3.15439999e-01\n",
      "Epoch: 24807 mean train loss:  4.59652394e-03, bound:  3.15439969e-01\n",
      "Epoch: 24808 mean train loss:  4.59852442e-03, bound:  3.15439969e-01\n",
      "Epoch: 24809 mean train loss:  4.60056635e-03, bound:  3.15439969e-01\n",
      "Epoch: 24810 mean train loss:  4.60078241e-03, bound:  3.15439969e-01\n",
      "Epoch: 24811 mean train loss:  4.59904037e-03, bound:  3.15439880e-01\n",
      "Epoch: 24812 mean train loss:  4.59650159e-03, bound:  3.15439880e-01\n",
      "Epoch: 24813 mean train loss:  4.59480938e-03, bound:  3.15439880e-01\n",
      "Epoch: 24814 mean train loss:  4.59474837e-03, bound:  3.15439850e-01\n",
      "Epoch: 24815 mean train loss:  4.59576538e-03, bound:  3.15439850e-01\n",
      "Epoch: 24816 mean train loss:  4.59667947e-03, bound:  3.15439850e-01\n",
      "Epoch: 24817 mean train loss:  4.59658075e-03, bound:  3.15439850e-01\n",
      "Epoch: 24818 mean train loss:  4.59535467e-03, bound:  3.15439761e-01\n",
      "Epoch: 24819 mean train loss:  4.59390879e-03, bound:  3.15439761e-01\n",
      "Epoch: 24820 mean train loss:  4.59307991e-03, bound:  3.15439731e-01\n",
      "Epoch: 24821 mean train loss:  4.59307432e-03, bound:  3.15439731e-01\n",
      "Epoch: 24822 mean train loss:  4.59338631e-03, bound:  3.15439731e-01\n",
      "Epoch: 24823 mean train loss:  4.59363544e-03, bound:  3.15439671e-01\n",
      "Epoch: 24824 mean train loss:  4.59331553e-03, bound:  3.15439671e-01\n",
      "Epoch: 24825 mean train loss:  4.59264219e-03, bound:  3.15439641e-01\n",
      "Epoch: 24826 mean train loss:  4.59191436e-03, bound:  3.15439641e-01\n",
      "Epoch: 24827 mean train loss:  4.59145289e-03, bound:  3.15439641e-01\n",
      "Epoch: 24828 mean train loss:  4.59135743e-03, bound:  3.15439582e-01\n",
      "Epoch: 24829 mean train loss:  4.59139608e-03, bound:  3.15439582e-01\n",
      "Epoch: 24830 mean train loss:  4.59142262e-03, bound:  3.15439552e-01\n",
      "Epoch: 24831 mean train loss:  4.59120655e-03, bound:  3.15439552e-01\n",
      "Epoch: 24832 mean train loss:  4.59069293e-03, bound:  3.15439522e-01\n",
      "Epoch: 24833 mean train loss:  4.59021050e-03, bound:  3.15439522e-01\n",
      "Epoch: 24834 mean train loss:  4.58978442e-03, bound:  3.15439463e-01\n",
      "Epoch: 24835 mean train loss:  4.58960468e-03, bound:  3.15439463e-01\n",
      "Epoch: 24836 mean train loss:  4.58952365e-03, bound:  3.15439433e-01\n",
      "Epoch: 24837 mean train loss:  4.58935229e-03, bound:  3.15439433e-01\n",
      "Epoch: 24838 mean train loss:  4.58910828e-03, bound:  3.15439433e-01\n",
      "Epoch: 24839 mean train loss:  4.58886381e-03, bound:  3.15439403e-01\n",
      "Epoch: 24840 mean train loss:  4.58852807e-03, bound:  3.15439403e-01\n",
      "Epoch: 24841 mean train loss:  4.58814064e-03, bound:  3.15439343e-01\n",
      "Epoch: 24842 mean train loss:  4.58796229e-03, bound:  3.15439314e-01\n",
      "Epoch: 24843 mean train loss:  4.58773552e-03, bound:  3.15439314e-01\n",
      "Epoch: 24844 mean train loss:  4.58763959e-03, bound:  3.15439314e-01\n",
      "Epoch: 24845 mean train loss:  4.58743982e-03, bound:  3.15439284e-01\n",
      "Epoch: 24846 mean train loss:  4.58711339e-03, bound:  3.15439284e-01\n",
      "Epoch: 24847 mean train loss:  4.58686752e-03, bound:  3.15439224e-01\n",
      "Epoch: 24848 mean train loss:  4.58658580e-03, bound:  3.15439224e-01\n",
      "Epoch: 24849 mean train loss:  4.58631618e-03, bound:  3.15439194e-01\n",
      "Epoch: 24850 mean train loss:  4.58607683e-03, bound:  3.15439194e-01\n",
      "Epoch: 24851 mean train loss:  4.58586914e-03, bound:  3.15439165e-01\n",
      "Epoch: 24852 mean train loss:  4.58563212e-03, bound:  3.15439165e-01\n",
      "Epoch: 24853 mean train loss:  4.58545750e-03, bound:  3.15439135e-01\n",
      "Epoch: 24854 mean train loss:  4.58519580e-03, bound:  3.15439135e-01\n",
      "Epoch: 24855 mean train loss:  4.58492897e-03, bound:  3.15439105e-01\n",
      "Epoch: 24856 mean train loss:  4.58462350e-03, bound:  3.15439075e-01\n",
      "Epoch: 24857 mean train loss:  4.58441628e-03, bound:  3.15439075e-01\n",
      "Epoch: 24858 mean train loss:  4.58420534e-03, bound:  3.15439045e-01\n",
      "Epoch: 24859 mean train loss:  4.58401861e-03, bound:  3.15439045e-01\n",
      "Epoch: 24860 mean train loss:  4.58376342e-03, bound:  3.15439016e-01\n",
      "Epoch: 24861 mean train loss:  4.58356552e-03, bound:  3.15439016e-01\n",
      "Epoch: 24862 mean train loss:  4.58331080e-03, bound:  3.15438986e-01\n",
      "Epoch: 24863 mean train loss:  4.58303187e-03, bound:  3.15438986e-01\n",
      "Epoch: 24864 mean train loss:  4.58281627e-03, bound:  3.15438926e-01\n",
      "Epoch: 24865 mean train loss:  4.58253501e-03, bound:  3.15438926e-01\n",
      "Epoch: 24866 mean train loss:  4.58224211e-03, bound:  3.15438926e-01\n",
      "Epoch: 24867 mean train loss:  4.58206283e-03, bound:  3.15438896e-01\n",
      "Epoch: 24868 mean train loss:  4.58182674e-03, bound:  3.15438867e-01\n",
      "Epoch: 24869 mean train loss:  4.58158692e-03, bound:  3.15438867e-01\n",
      "Epoch: 24870 mean train loss:  4.58142161e-03, bound:  3.15438867e-01\n",
      "Epoch: 24871 mean train loss:  4.58117342e-03, bound:  3.15438837e-01\n",
      "Epoch: 24872 mean train loss:  4.58088331e-03, bound:  3.15438807e-01\n",
      "Epoch: 24873 mean train loss:  4.58068633e-03, bound:  3.15438777e-01\n",
      "Epoch: 24874 mean train loss:  4.58040554e-03, bound:  3.15438747e-01\n",
      "Epoch: 24875 mean train loss:  4.58014850e-03, bound:  3.15438747e-01\n",
      "Epoch: 24876 mean train loss:  4.57990495e-03, bound:  3.15438747e-01\n",
      "Epoch: 24877 mean train loss:  4.57971683e-03, bound:  3.15438718e-01\n",
      "Epoch: 24878 mean train loss:  4.57942020e-03, bound:  3.15438718e-01\n",
      "Epoch: 24879 mean train loss:  4.57928330e-03, bound:  3.15438658e-01\n",
      "Epoch: 24880 mean train loss:  4.57898853e-03, bound:  3.15438658e-01\n",
      "Epoch: 24881 mean train loss:  4.57874453e-03, bound:  3.15438628e-01\n",
      "Epoch: 24882 mean train loss:  4.57852986e-03, bound:  3.15438628e-01\n",
      "Epoch: 24883 mean train loss:  4.57829144e-03, bound:  3.15438598e-01\n",
      "Epoch: 24884 mean train loss:  4.57802648e-03, bound:  3.15438569e-01\n",
      "Epoch: 24885 mean train loss:  4.57776478e-03, bound:  3.15438569e-01\n",
      "Epoch: 24886 mean train loss:  4.57756873e-03, bound:  3.15438539e-01\n",
      "Epoch: 24887 mean train loss:  4.57738200e-03, bound:  3.15438509e-01\n",
      "Epoch: 24888 mean train loss:  4.57705511e-03, bound:  3.15438509e-01\n",
      "Epoch: 24889 mean train loss:  4.57688794e-03, bound:  3.15438509e-01\n",
      "Epoch: 24890 mean train loss:  4.57665836e-03, bound:  3.15438479e-01\n",
      "Epoch: 24891 mean train loss:  4.57636500e-03, bound:  3.15438449e-01\n",
      "Epoch: 24892 mean train loss:  4.57614567e-03, bound:  3.15438420e-01\n",
      "Epoch: 24893 mean train loss:  4.57589887e-03, bound:  3.15438420e-01\n",
      "Epoch: 24894 mean train loss:  4.57569119e-03, bound:  3.15438420e-01\n",
      "Epoch: 24895 mean train loss:  4.57543507e-03, bound:  3.15438420e-01\n",
      "Epoch: 24896 mean train loss:  4.57517104e-03, bound:  3.15438330e-01\n",
      "Epoch: 24897 mean train loss:  4.57493728e-03, bound:  3.15438330e-01\n",
      "Epoch: 24898 mean train loss:  4.57474869e-03, bound:  3.15438330e-01\n",
      "Epoch: 24899 mean train loss:  4.57452191e-03, bound:  3.15438330e-01\n",
      "Epoch: 24900 mean train loss:  4.57426254e-03, bound:  3.15438300e-01\n",
      "Epoch: 24901 mean train loss:  4.57399106e-03, bound:  3.15438300e-01\n",
      "Epoch: 24902 mean train loss:  4.57377266e-03, bound:  3.15438211e-01\n",
      "Epoch: 24903 mean train loss:  4.57348675e-03, bound:  3.15438211e-01\n",
      "Epoch: 24904 mean train loss:  4.57326276e-03, bound:  3.15438211e-01\n",
      "Epoch: 24905 mean train loss:  4.57303599e-03, bound:  3.15438211e-01\n",
      "Epoch: 24906 mean train loss:  4.57283389e-03, bound:  3.15438181e-01\n",
      "Epoch: 24907 mean train loss:  4.57256334e-03, bound:  3.15438181e-01\n",
      "Epoch: 24908 mean train loss:  4.57232445e-03, bound:  3.15438151e-01\n",
      "Epoch: 24909 mean train loss:  4.57204599e-03, bound:  3.15438092e-01\n",
      "Epoch: 24910 mean train loss:  4.57189512e-03, bound:  3.15438092e-01\n",
      "Epoch: 24911 mean train loss:  4.57159849e-03, bound:  3.15438092e-01\n",
      "Epoch: 24912 mean train loss:  4.57138568e-03, bound:  3.15438062e-01\n",
      "Epoch: 24913 mean train loss:  4.57116682e-03, bound:  3.15438062e-01\n",
      "Epoch: 24914 mean train loss:  4.57086042e-03, bound:  3.15438032e-01\n",
      "Epoch: 24915 mean train loss:  4.57065599e-03, bound:  3.15438032e-01\n",
      "Epoch: 24916 mean train loss:  4.57040360e-03, bound:  3.15437973e-01\n",
      "Epoch: 24917 mean train loss:  4.57019825e-03, bound:  3.15437973e-01\n",
      "Epoch: 24918 mean train loss:  4.56994120e-03, bound:  3.15437943e-01\n",
      "Epoch: 24919 mean train loss:  4.56970790e-03, bound:  3.15437943e-01\n",
      "Epoch: 24920 mean train loss:  4.56946529e-03, bound:  3.15437883e-01\n",
      "Epoch: 24921 mean train loss:  4.56913095e-03, bound:  3.15437883e-01\n",
      "Epoch: 24922 mean train loss:  4.56897030e-03, bound:  3.15437883e-01\n",
      "Epoch: 24923 mean train loss:  4.56874864e-03, bound:  3.15437853e-01\n",
      "Epoch: 24924 mean train loss:  4.56850929e-03, bound:  3.15437853e-01\n",
      "Epoch: 24925 mean train loss:  4.56821127e-03, bound:  3.15437824e-01\n",
      "Epoch: 24926 mean train loss:  4.56801243e-03, bound:  3.15437824e-01\n",
      "Epoch: 24927 mean train loss:  4.56775213e-03, bound:  3.15437764e-01\n",
      "Epoch: 24928 mean train loss:  4.56754025e-03, bound:  3.15437764e-01\n",
      "Epoch: 24929 mean train loss:  4.56729298e-03, bound:  3.15437734e-01\n",
      "Epoch: 24930 mean train loss:  4.56710020e-03, bound:  3.15437734e-01\n",
      "Epoch: 24931 mean train loss:  4.56679892e-03, bound:  3.15437734e-01\n",
      "Epoch: 24932 mean train loss:  4.56658099e-03, bound:  3.15437704e-01\n",
      "Epoch: 24933 mean train loss:  4.56635933e-03, bound:  3.15437704e-01\n",
      "Epoch: 24934 mean train loss:  4.56609065e-03, bound:  3.15437645e-01\n",
      "Epoch: 24935 mean train loss:  4.56587505e-03, bound:  3.15437615e-01\n",
      "Epoch: 24936 mean train loss:  4.56565339e-03, bound:  3.15437615e-01\n",
      "Epoch: 24937 mean train loss:  4.56536980e-03, bound:  3.15437615e-01\n",
      "Epoch: 24938 mean train loss:  4.56521381e-03, bound:  3.15437615e-01\n",
      "Epoch: 24939 mean train loss:  4.56494419e-03, bound:  3.15437526e-01\n",
      "Epoch: 24940 mean train loss:  4.56476910e-03, bound:  3.15437526e-01\n",
      "Epoch: 24941 mean train loss:  4.56462521e-03, bound:  3.15437496e-01\n",
      "Epoch: 24942 mean train loss:  4.56448970e-03, bound:  3.15437496e-01\n",
      "Epoch: 24943 mean train loss:  4.56443802e-03, bound:  3.15437496e-01\n",
      "Epoch: 24944 mean train loss:  4.56436677e-03, bound:  3.15437496e-01\n",
      "Epoch: 24945 mean train loss:  4.56441753e-03, bound:  3.15437436e-01\n",
      "Epoch: 24946 mean train loss:  4.56446270e-03, bound:  3.15437436e-01\n",
      "Epoch: 24947 mean train loss:  4.56455210e-03, bound:  3.15437406e-01\n",
      "Epoch: 24948 mean train loss:  4.56473185e-03, bound:  3.15437406e-01\n",
      "Epoch: 24949 mean train loss:  4.56498098e-03, bound:  3.15437376e-01\n",
      "Epoch: 24950 mean train loss:  4.56541637e-03, bound:  3.15437376e-01\n",
      "Epoch: 24951 mean train loss:  4.56592534e-03, bound:  3.15437317e-01\n",
      "Epoch: 24952 mean train loss:  4.56672162e-03, bound:  3.15437317e-01\n",
      "Epoch: 24953 mean train loss:  4.56763618e-03, bound:  3.15437287e-01\n",
      "Epoch: 24954 mean train loss:  4.56865318e-03, bound:  3.15437317e-01\n",
      "Epoch: 24955 mean train loss:  4.56968509e-03, bound:  3.15437257e-01\n",
      "Epoch: 24956 mean train loss:  4.57059452e-03, bound:  3.15437287e-01\n",
      "Epoch: 24957 mean train loss:  4.57121478e-03, bound:  3.15437198e-01\n",
      "Epoch: 24958 mean train loss:  4.57125483e-03, bound:  3.15437257e-01\n",
      "Epoch: 24959 mean train loss:  4.57044411e-03, bound:  3.15437168e-01\n",
      "Epoch: 24960 mean train loss:  4.56867274e-03, bound:  3.15437198e-01\n",
      "Epoch: 24961 mean train loss:  4.56632627e-03, bound:  3.15437138e-01\n",
      "Epoch: 24962 mean train loss:  4.56365617e-03, bound:  3.15437168e-01\n",
      "Epoch: 24963 mean train loss:  4.56128409e-03, bound:  3.15437078e-01\n",
      "Epoch: 24964 mean train loss:  4.55952436e-03, bound:  3.15437078e-01\n",
      "Epoch: 24965 mean train loss:  4.55864519e-03, bound:  3.15437078e-01\n",
      "Epoch: 24966 mean train loss:  4.55868337e-03, bound:  3.15437078e-01\n",
      "Epoch: 24967 mean train loss:  4.55924962e-03, bound:  3.15437078e-01\n",
      "Epoch: 24968 mean train loss:  4.56003239e-03, bound:  3.15436989e-01\n",
      "Epoch: 24969 mean train loss:  4.56073834e-03, bound:  3.15437049e-01\n",
      "Epoch: 24970 mean train loss:  4.56096465e-03, bound:  3.15436989e-01\n",
      "Epoch: 24971 mean train loss:  4.56065778e-03, bound:  3.15436989e-01\n",
      "Epoch: 24972 mean train loss:  4.55982937e-03, bound:  3.15436929e-01\n",
      "Epoch: 24973 mean train loss:  4.55876952e-03, bound:  3.15436959e-01\n",
      "Epoch: 24974 mean train loss:  4.55768593e-03, bound:  3.15436929e-01\n",
      "Epoch: 24975 mean train loss:  4.55674343e-03, bound:  3.15436929e-01\n",
      "Epoch: 24976 mean train loss:  4.55612363e-03, bound:  3.15436929e-01\n",
      "Epoch: 24977 mean train loss:  4.55585355e-03, bound:  3.15436870e-01\n",
      "Epoch: 24978 mean train loss:  4.55581956e-03, bound:  3.15436840e-01\n",
      "Epoch: 24979 mean train loss:  4.55600815e-03, bound:  3.15436810e-01\n",
      "Epoch: 24980 mean train loss:  4.55619860e-03, bound:  3.15436840e-01\n",
      "Epoch: 24981 mean train loss:  4.55631409e-03, bound:  3.15436810e-01\n",
      "Epoch: 24982 mean train loss:  4.55617206e-03, bound:  3.15436810e-01\n",
      "Epoch: 24983 mean train loss:  4.55583073e-03, bound:  3.15436751e-01\n",
      "Epoch: 24984 mean train loss:  4.55525611e-03, bound:  3.15436751e-01\n",
      "Epoch: 24985 mean train loss:  4.55460604e-03, bound:  3.15436721e-01\n",
      "Epoch: 24986 mean train loss:  4.55403980e-03, bound:  3.15436691e-01\n",
      "Epoch: 24987 mean train loss:  4.55354946e-03, bound:  3.15436691e-01\n",
      "Epoch: 24988 mean train loss:  4.55326727e-03, bound:  3.15436691e-01\n",
      "Epoch: 24989 mean train loss:  4.55307169e-03, bound:  3.15436631e-01\n",
      "Epoch: 24990 mean train loss:  4.55298182e-03, bound:  3.15436631e-01\n",
      "Epoch: 24991 mean train loss:  4.55296179e-03, bound:  3.15436631e-01\n",
      "Epoch: 24992 mean train loss:  4.55289846e-03, bound:  3.15436602e-01\n",
      "Epoch: 24993 mean train loss:  4.55270149e-03, bound:  3.15436602e-01\n",
      "Epoch: 24994 mean train loss:  4.55247844e-03, bound:  3.15436542e-01\n",
      "Epoch: 24995 mean train loss:  4.55218786e-03, bound:  3.15436512e-01\n",
      "Epoch: 24996 mean train loss:  4.55174083e-03, bound:  3.15436512e-01\n",
      "Epoch: 24997 mean train loss:  4.55138087e-03, bound:  3.15436512e-01\n",
      "Epoch: 24998 mean train loss:  4.55100415e-03, bound:  3.15436482e-01\n",
      "Epoch: 24999 mean train loss:  4.55064699e-03, bound:  3.15436482e-01\n",
      "Epoch: 25000 mean train loss:  4.55040159e-03, bound:  3.15436423e-01\n",
      "Epoch: 25001 mean train loss:  4.55013569e-03, bound:  3.15436393e-01\n",
      "Epoch: 25002 mean train loss:  4.54998901e-03, bound:  3.15436393e-01\n",
      "Epoch: 25003 mean train loss:  4.54975571e-03, bound:  3.15436393e-01\n",
      "Epoch: 25004 mean train loss:  4.54960112e-03, bound:  3.15436393e-01\n",
      "Epoch: 25005 mean train loss:  4.54937294e-03, bound:  3.15436363e-01\n",
      "Epoch: 25006 mean train loss:  4.54916107e-03, bound:  3.15436363e-01\n",
      "Epoch: 25007 mean train loss:  4.54892544e-03, bound:  3.15436304e-01\n",
      "Epoch: 25008 mean train loss:  4.54864418e-03, bound:  3.15436274e-01\n",
      "Epoch: 25009 mean train loss:  4.54840157e-03, bound:  3.15436274e-01\n",
      "Epoch: 25010 mean train loss:  4.54814592e-03, bound:  3.15436274e-01\n",
      "Epoch: 25011 mean train loss:  4.54785256e-03, bound:  3.15436244e-01\n",
      "Epoch: 25012 mean train loss:  4.54770820e-03, bound:  3.15436244e-01\n",
      "Epoch: 25013 mean train loss:  4.54747584e-03, bound:  3.15436184e-01\n",
      "Epoch: 25014 mean train loss:  4.54726536e-03, bound:  3.15436184e-01\n",
      "Epoch: 25015 mean train loss:  4.54698410e-03, bound:  3.15436184e-01\n",
      "Epoch: 25016 mean train loss:  4.54677641e-03, bound:  3.15436155e-01\n",
      "Epoch: 25017 mean train loss:  4.54657245e-03, bound:  3.15436125e-01\n",
      "Epoch: 25018 mean train loss:  4.54634801e-03, bound:  3.15436095e-01\n",
      "Epoch: 25019 mean train loss:  4.54606535e-03, bound:  3.15436065e-01\n",
      "Epoch: 25020 mean train loss:  4.54579107e-03, bound:  3.15436065e-01\n",
      "Epoch: 25021 mean train loss:  4.54556197e-03, bound:  3.15436065e-01\n",
      "Epoch: 25022 mean train loss:  4.54531470e-03, bound:  3.15436035e-01\n",
      "Epoch: 25023 mean train loss:  4.54501715e-03, bound:  3.15436006e-01\n",
      "Epoch: 25024 mean train loss:  4.54477035e-03, bound:  3.15435976e-01\n",
      "Epoch: 25025 mean train loss:  4.54461947e-03, bound:  3.15435976e-01\n",
      "Epoch: 25026 mean train loss:  4.54435265e-03, bound:  3.15435946e-01\n",
      "Epoch: 25027 mean train loss:  4.54408396e-03, bound:  3.15435946e-01\n",
      "Epoch: 25028 mean train loss:  4.54388699e-03, bound:  3.15435946e-01\n",
      "Epoch: 25029 mean train loss:  4.54368070e-03, bound:  3.15435886e-01\n",
      "Epoch: 25030 mean train loss:  4.54347162e-03, bound:  3.15435886e-01\n",
      "Epoch: 25031 mean train loss:  4.54327604e-03, bound:  3.15435886e-01\n",
      "Epoch: 25032 mean train loss:  4.54300223e-03, bound:  3.15435857e-01\n",
      "Epoch: 25033 mean train loss:  4.54285461e-03, bound:  3.15435827e-01\n",
      "Epoch: 25034 mean train loss:  4.54258313e-03, bound:  3.15435827e-01\n",
      "Epoch: 25035 mean train loss:  4.54244623e-03, bound:  3.15435797e-01\n",
      "Epoch: 25036 mean train loss:  4.54227393e-03, bound:  3.15435797e-01\n",
      "Epoch: 25037 mean train loss:  4.54213284e-03, bound:  3.15435737e-01\n",
      "Epoch: 25038 mean train loss:  4.54204110e-03, bound:  3.15435737e-01\n",
      "Epoch: 25039 mean train loss:  4.54207975e-03, bound:  3.15435708e-01\n",
      "Epoch: 25040 mean train loss:  4.54208581e-03, bound:  3.15435708e-01\n",
      "Epoch: 25041 mean train loss:  4.54223715e-03, bound:  3.15435678e-01\n",
      "Epoch: 25042 mean train loss:  4.54236474e-03, bound:  3.15435678e-01\n",
      "Epoch: 25043 mean train loss:  4.54252539e-03, bound:  3.15435618e-01\n",
      "Epoch: 25044 mean train loss:  4.54282342e-03, bound:  3.15435618e-01\n",
      "Epoch: 25045 mean train loss:  4.54319362e-03, bound:  3.15435588e-01\n",
      "Epoch: 25046 mean train loss:  4.54371655e-03, bound:  3.15435588e-01\n",
      "Epoch: 25047 mean train loss:  4.54428466e-03, bound:  3.15435559e-01\n",
      "Epoch: 25048 mean train loss:  4.54502832e-03, bound:  3.15435588e-01\n",
      "Epoch: 25049 mean train loss:  4.54581529e-03, bound:  3.15435499e-01\n",
      "Epoch: 25050 mean train loss:  4.54658829e-03, bound:  3.15435559e-01\n",
      "Epoch: 25051 mean train loss:  4.54697153e-03, bound:  3.15435499e-01\n",
      "Epoch: 25052 mean train loss:  4.54710145e-03, bound:  3.15435499e-01\n",
      "Epoch: 25053 mean train loss:  4.54657897e-03, bound:  3.15435439e-01\n",
      "Epoch: 25054 mean train loss:  4.54540364e-03, bound:  3.15435499e-01\n",
      "Epoch: 25055 mean train loss:  4.54366906e-03, bound:  3.15435380e-01\n",
      "Epoch: 25056 mean train loss:  4.54160199e-03, bound:  3.15435439e-01\n",
      "Epoch: 25057 mean train loss:  4.53954050e-03, bound:  3.15435380e-01\n",
      "Epoch: 25058 mean train loss:  4.53788368e-03, bound:  3.15435380e-01\n",
      "Epoch: 25059 mean train loss:  4.53672232e-03, bound:  3.15435380e-01\n",
      "Epoch: 25060 mean train loss:  4.53622220e-03, bound:  3.15435350e-01\n",
      "Epoch: 25061 mean train loss:  4.53625293e-03, bound:  3.15435320e-01\n",
      "Epoch: 25062 mean train loss:  4.53664130e-03, bound:  3.15435261e-01\n",
      "Epoch: 25063 mean train loss:  4.53712139e-03, bound:  3.15435290e-01\n",
      "Epoch: 25064 mean train loss:  4.53756936e-03, bound:  3.15435261e-01\n",
      "Epoch: 25065 mean train loss:  4.53781011e-03, bound:  3.15435261e-01\n",
      "Epoch: 25066 mean train loss:  4.53761220e-03, bound:  3.15435201e-01\n",
      "Epoch: 25067 mean train loss:  4.53720568e-03, bound:  3.15435261e-01\n",
      "Epoch: 25068 mean train loss:  4.53647133e-03, bound:  3.15435171e-01\n",
      "Epoch: 25069 mean train loss:  4.53564571e-03, bound:  3.15435171e-01\n",
      "Epoch: 25070 mean train loss:  4.53480007e-03, bound:  3.15435141e-01\n",
      "Epoch: 25071 mean train loss:  4.53401683e-03, bound:  3.15435141e-01\n",
      "Epoch: 25072 mean train loss:  4.53345012e-03, bound:  3.15435141e-01\n",
      "Epoch: 25073 mean train loss:  4.53317072e-03, bound:  3.15435112e-01\n",
      "Epoch: 25074 mean train loss:  4.53304406e-03, bound:  3.15435082e-01\n",
      "Epoch: 25075 mean train loss:  4.53306967e-03, bound:  3.15435052e-01\n",
      "Epoch: 25076 mean train loss:  4.53312555e-03, bound:  3.15435052e-01\n",
      "Epoch: 25077 mean train loss:  4.53308923e-03, bound:  3.15435022e-01\n",
      "Epoch: 25078 mean train loss:  4.53303289e-03, bound:  3.15435022e-01\n",
      "Epoch: 25079 mean train loss:  4.53286944e-03, bound:  3.15434963e-01\n",
      "Epoch: 25080 mean train loss:  4.53260029e-03, bound:  3.15434963e-01\n",
      "Epoch: 25081 mean train loss:  4.53229202e-03, bound:  3.15434933e-01\n",
      "Epoch: 25082 mean train loss:  4.53186035e-03, bound:  3.15434933e-01\n",
      "Epoch: 25083 mean train loss:  4.53134486e-03, bound:  3.15434903e-01\n",
      "Epoch: 25084 mean train loss:  4.53090947e-03, bound:  3.15434903e-01\n",
      "Epoch: 25085 mean train loss:  4.53050015e-03, bound:  3.15434843e-01\n",
      "Epoch: 25086 mean train loss:  4.53016488e-03, bound:  3.15434843e-01\n",
      "Epoch: 25087 mean train loss:  4.52986406e-03, bound:  3.15434813e-01\n",
      "Epoch: 25088 mean train loss:  4.52969922e-03, bound:  3.15434813e-01\n",
      "Epoch: 25089 mean train loss:  4.52952133e-03, bound:  3.15434813e-01\n",
      "Epoch: 25090 mean train loss:  4.52931412e-03, bound:  3.15434784e-01\n",
      "Epoch: 25091 mean train loss:  4.52911016e-03, bound:  3.15434784e-01\n",
      "Epoch: 25092 mean train loss:  4.52901702e-03, bound:  3.15434724e-01\n",
      "Epoch: 25093 mean train loss:  4.52885078e-03, bound:  3.15434724e-01\n",
      "Epoch: 25094 mean train loss:  4.52868408e-03, bound:  3.15434694e-01\n",
      "Epoch: 25095 mean train loss:  4.52843355e-03, bound:  3.15434694e-01\n",
      "Epoch: 25096 mean train loss:  4.52817604e-03, bound:  3.15434694e-01\n",
      "Epoch: 25097 mean train loss:  4.52801213e-03, bound:  3.15434694e-01\n",
      "Epoch: 25098 mean train loss:  4.52777091e-03, bound:  3.15434605e-01\n",
      "Epoch: 25099 mean train loss:  4.52753296e-03, bound:  3.15434605e-01\n",
      "Epoch: 25100 mean train loss:  4.52720048e-03, bound:  3.15434575e-01\n",
      "Epoch: 25101 mean train loss:  4.52694343e-03, bound:  3.15434575e-01\n",
      "Epoch: 25102 mean train loss:  4.52673715e-03, bound:  3.15434575e-01\n",
      "Epoch: 25103 mean train loss:  4.52641631e-03, bound:  3.15434575e-01\n",
      "Epoch: 25104 mean train loss:  4.52612201e-03, bound:  3.15434515e-01\n",
      "Epoch: 25105 mean train loss:  4.52586077e-03, bound:  3.15434515e-01\n",
      "Epoch: 25106 mean train loss:  4.52557206e-03, bound:  3.15434456e-01\n",
      "Epoch: 25107 mean train loss:  4.52526752e-03, bound:  3.15434456e-01\n",
      "Epoch: 25108 mean train loss:  4.52498393e-03, bound:  3.15434456e-01\n",
      "Epoch: 25109 mean train loss:  4.52479906e-03, bound:  3.15434456e-01\n",
      "Epoch: 25110 mean train loss:  4.52452106e-03, bound:  3.15434396e-01\n",
      "Epoch: 25111 mean train loss:  4.52428730e-03, bound:  3.15434396e-01\n",
      "Epoch: 25112 mean train loss:  4.52408753e-03, bound:  3.15434366e-01\n",
      "Epoch: 25113 mean train loss:  4.52379789e-03, bound:  3.15434366e-01\n",
      "Epoch: 25114 mean train loss:  4.52361628e-03, bound:  3.15434337e-01\n",
      "Epoch: 25115 mean train loss:  4.52338811e-03, bound:  3.15434337e-01\n",
      "Epoch: 25116 mean train loss:  4.52313060e-03, bound:  3.15434337e-01\n",
      "Epoch: 25117 mean train loss:  4.52292152e-03, bound:  3.15434277e-01\n",
      "Epoch: 25118 mean train loss:  4.52269753e-03, bound:  3.15434277e-01\n",
      "Epoch: 25119 mean train loss:  4.52239020e-03, bound:  3.15434247e-01\n",
      "Epoch: 25120 mean train loss:  4.52224631e-03, bound:  3.15434247e-01\n",
      "Epoch: 25121 mean train loss:  4.52207122e-03, bound:  3.15434217e-01\n",
      "Epoch: 25122 mean train loss:  4.52182814e-03, bound:  3.15434217e-01\n",
      "Epoch: 25123 mean train loss:  4.52162651e-03, bound:  3.15434158e-01\n",
      "Epoch: 25124 mean train loss:  4.52148030e-03, bound:  3.15434158e-01\n",
      "Epoch: 25125 mean train loss:  4.52132616e-03, bound:  3.15434128e-01\n",
      "Epoch: 25126 mean train loss:  4.52119391e-03, bound:  3.15434128e-01\n",
      "Epoch: 25127 mean train loss:  4.52110125e-03, bound:  3.15434098e-01\n",
      "Epoch: 25128 mean train loss:  4.52099461e-03, bound:  3.15434098e-01\n",
      "Epoch: 25129 mean train loss:  4.52099880e-03, bound:  3.15434039e-01\n",
      "Epoch: 25130 mean train loss:  4.52105794e-03, bound:  3.15434039e-01\n",
      "Epoch: 25131 mean train loss:  4.52124560e-03, bound:  3.15434009e-01\n",
      "Epoch: 25132 mean train loss:  4.52152500e-03, bound:  3.15434009e-01\n",
      "Epoch: 25133 mean train loss:  4.52198461e-03, bound:  3.15433949e-01\n",
      "Epoch: 25134 mean train loss:  4.52263746e-03, bound:  3.15433949e-01\n",
      "Epoch: 25135 mean train loss:  4.52340255e-03, bound:  3.15433949e-01\n",
      "Epoch: 25136 mean train loss:  4.52436740e-03, bound:  3.15433949e-01\n",
      "Epoch: 25137 mean train loss:  4.52557113e-03, bound:  3.15433890e-01\n",
      "Epoch: 25138 mean train loss:  4.52687265e-03, bound:  3.15433919e-01\n",
      "Epoch: 25139 mean train loss:  4.52817325e-03, bound:  3.15433830e-01\n",
      "Epoch: 25140 mean train loss:  4.52914787e-03, bound:  3.15433890e-01\n",
      "Epoch: 25141 mean train loss:  4.52958420e-03, bound:  3.15433830e-01\n",
      "Epoch: 25142 mean train loss:  4.52911574e-03, bound:  3.15433830e-01\n",
      "Epoch: 25143 mean train loss:  4.52738767e-03, bound:  3.15433770e-01\n",
      "Epoch: 25144 mean train loss:  4.52480000e-03, bound:  3.15433830e-01\n",
      "Epoch: 25145 mean train loss:  4.52172011e-03, bound:  3.15433770e-01\n",
      "Epoch: 25146 mean train loss:  4.51883161e-03, bound:  3.15433770e-01\n",
      "Epoch: 25147 mean train loss:  4.51670773e-03, bound:  3.15433770e-01\n",
      "Epoch: 25148 mean train loss:  4.51568281e-03, bound:  3.15433711e-01\n",
      "Epoch: 25149 mean train loss:  4.51564789e-03, bound:  3.15433711e-01\n",
      "Epoch: 25150 mean train loss:  4.51641530e-03, bound:  3.15433651e-01\n",
      "Epoch: 25151 mean train loss:  4.51755617e-03, bound:  3.15433681e-01\n",
      "Epoch: 25152 mean train loss:  4.51836828e-03, bound:  3.15433651e-01\n",
      "Epoch: 25153 mean train loss:  4.51845862e-03, bound:  3.15433651e-01\n",
      "Epoch: 25154 mean train loss:  4.51790076e-03, bound:  3.15433592e-01\n",
      "Epoch: 25155 mean train loss:  4.51673754e-03, bound:  3.15433651e-01\n",
      "Epoch: 25156 mean train loss:  4.51532146e-03, bound:  3.15433562e-01\n",
      "Epoch: 25157 mean train loss:  4.51409444e-03, bound:  3.15433562e-01\n",
      "Epoch: 25158 mean train loss:  4.51333728e-03, bound:  3.15433562e-01\n",
      "Epoch: 25159 mean train loss:  4.51315753e-03, bound:  3.15433502e-01\n",
      "Epoch: 25160 mean train loss:  4.51335683e-03, bound:  3.15433502e-01\n",
      "Epoch: 25161 mean train loss:  4.51366557e-03, bound:  3.15433472e-01\n",
      "Epoch: 25162 mean train loss:  4.51383786e-03, bound:  3.15433472e-01\n",
      "Epoch: 25163 mean train loss:  4.51381458e-03, bound:  3.15433443e-01\n",
      "Epoch: 25164 mean train loss:  4.51357616e-03, bound:  3.15433443e-01\n",
      "Epoch: 25165 mean train loss:  4.51300247e-03, bound:  3.15433443e-01\n",
      "Epoch: 25166 mean train loss:  4.51230630e-03, bound:  3.15433383e-01\n",
      "Epoch: 25167 mean train loss:  4.51164180e-03, bound:  3.15433353e-01\n",
      "Epoch: 25168 mean train loss:  4.51112352e-03, bound:  3.15433353e-01\n",
      "Epoch: 25169 mean train loss:  4.51080548e-03, bound:  3.15433353e-01\n",
      "Epoch: 25170 mean train loss:  4.51063504e-03, bound:  3.15433323e-01\n",
      "Epoch: 25171 mean train loss:  4.51053772e-03, bound:  3.15433323e-01\n",
      "Epoch: 25172 mean train loss:  4.51050093e-03, bound:  3.15433264e-01\n",
      "Epoch: 25173 mean train loss:  4.51037008e-03, bound:  3.15433264e-01\n",
      "Epoch: 25174 mean train loss:  4.51014750e-03, bound:  3.15433264e-01\n",
      "Epoch: 25175 mean train loss:  4.50995285e-03, bound:  3.15433264e-01\n",
      "Epoch: 25176 mean train loss:  4.50955750e-03, bound:  3.15433204e-01\n",
      "Epoch: 25177 mean train loss:  4.50923201e-03, bound:  3.15433204e-01\n",
      "Epoch: 25178 mean train loss:  4.50885994e-03, bound:  3.15433145e-01\n",
      "Epoch: 25179 mean train loss:  4.50849021e-03, bound:  3.15433145e-01\n",
      "Epoch: 25180 mean train loss:  4.50824434e-03, bound:  3.15433145e-01\n",
      "Epoch: 25181 mean train loss:  4.50805202e-03, bound:  3.15433145e-01\n",
      "Epoch: 25182 mean train loss:  4.50784108e-03, bound:  3.15433085e-01\n",
      "Epoch: 25183 mean train loss:  4.50770091e-03, bound:  3.15433085e-01\n",
      "Epoch: 25184 mean train loss:  4.50748671e-03, bound:  3.15433055e-01\n",
      "Epoch: 25185 mean train loss:  4.50740289e-03, bound:  3.15433025e-01\n",
      "Epoch: 25186 mean train loss:  4.50710021e-03, bound:  3.15433025e-01\n",
      "Epoch: 25187 mean train loss:  4.50684270e-03, bound:  3.15433025e-01\n",
      "Epoch: 25188 mean train loss:  4.50659683e-03, bound:  3.15433025e-01\n",
      "Epoch: 25189 mean train loss:  4.50632256e-03, bound:  3.15432996e-01\n",
      "Epoch: 25190 mean train loss:  4.50606039e-03, bound:  3.15432936e-01\n",
      "Epoch: 25191 mean train loss:  4.50581545e-03, bound:  3.15432936e-01\n",
      "Epoch: 25192 mean train loss:  4.50557144e-03, bound:  3.15432906e-01\n",
      "Epoch: 25193 mean train loss:  4.50528553e-03, bound:  3.15432906e-01\n",
      "Epoch: 25194 mean train loss:  4.50506201e-03, bound:  3.15432876e-01\n",
      "Epoch: 25195 mean train loss:  4.50480031e-03, bound:  3.15432876e-01\n",
      "Epoch: 25196 mean train loss:  4.50457027e-03, bound:  3.15432876e-01\n",
      "Epoch: 25197 mean train loss:  4.50433791e-03, bound:  3.15432817e-01\n",
      "Epoch: 25198 mean train loss:  4.50410554e-03, bound:  3.15432817e-01\n",
      "Epoch: 25199 mean train loss:  4.50389087e-03, bound:  3.15432787e-01\n",
      "Epoch: 25200 mean train loss:  4.50362591e-03, bound:  3.15432757e-01\n",
      "Epoch: 25201 mean train loss:  4.50337026e-03, bound:  3.15432757e-01\n",
      "Epoch: 25202 mean train loss:  4.50318120e-03, bound:  3.15432757e-01\n",
      "Epoch: 25203 mean train loss:  4.50297957e-03, bound:  3.15432698e-01\n",
      "Epoch: 25204 mean train loss:  4.50270902e-03, bound:  3.15432698e-01\n",
      "Epoch: 25205 mean train loss:  4.50251065e-03, bound:  3.15432668e-01\n",
      "Epoch: 25206 mean train loss:  4.50233556e-03, bound:  3.15432668e-01\n",
      "Epoch: 25207 mean train loss:  4.50204872e-03, bound:  3.15432638e-01\n",
      "Epoch: 25208 mean train loss:  4.50184802e-03, bound:  3.15432638e-01\n",
      "Epoch: 25209 mean train loss:  4.50162450e-03, bound:  3.15432608e-01\n",
      "Epoch: 25210 mean train loss:  4.50137490e-03, bound:  3.15432578e-01\n",
      "Epoch: 25211 mean train loss:  4.50117793e-03, bound:  3.15432578e-01\n",
      "Epoch: 25212 mean train loss:  4.50095022e-03, bound:  3.15432549e-01\n",
      "Epoch: 25213 mean train loss:  4.50073462e-03, bound:  3.15432549e-01\n",
      "Epoch: 25214 mean train loss:  4.50053811e-03, bound:  3.15432519e-01\n",
      "Epoch: 25215 mean train loss:  4.50027967e-03, bound:  3.15432519e-01\n",
      "Epoch: 25216 mean train loss:  4.50003473e-03, bound:  3.15432459e-01\n",
      "Epoch: 25217 mean train loss:  4.49979305e-03, bound:  3.15432459e-01\n",
      "Epoch: 25218 mean train loss:  4.49959375e-03, bound:  3.15432429e-01\n",
      "Epoch: 25219 mean train loss:  4.49930225e-03, bound:  3.15432429e-01\n",
      "Epoch: 25220 mean train loss:  4.49909689e-03, bound:  3.15432400e-01\n",
      "Epoch: 25221 mean train loss:  4.49882308e-03, bound:  3.15432400e-01\n",
      "Epoch: 25222 mean train loss:  4.49861400e-03, bound:  3.15432370e-01\n",
      "Epoch: 25223 mean train loss:  4.49833134e-03, bound:  3.15432340e-01\n",
      "Epoch: 25224 mean train loss:  4.49810689e-03, bound:  3.15432340e-01\n",
      "Epoch: 25225 mean train loss:  4.49785264e-03, bound:  3.15432340e-01\n",
      "Epoch: 25226 mean train loss:  4.49761050e-03, bound:  3.15432310e-01\n",
      "Epoch: 25227 mean train loss:  4.49734041e-03, bound:  3.15432310e-01\n",
      "Epoch: 25228 mean train loss:  4.49713692e-03, bound:  3.15432250e-01\n",
      "Epoch: 25229 mean train loss:  4.49687988e-03, bound:  3.15432221e-01\n",
      "Epoch: 25230 mean train loss:  4.49665729e-03, bound:  3.15432221e-01\n",
      "Epoch: 25231 mean train loss:  4.49649477e-03, bound:  3.15432221e-01\n",
      "Epoch: 25232 mean train loss:  4.49623074e-03, bound:  3.15432191e-01\n",
      "Epoch: 25233 mean train loss:  4.49600583e-03, bound:  3.15432161e-01\n",
      "Epoch: 25234 mean train loss:  4.49571898e-03, bound:  3.15432131e-01\n",
      "Epoch: 25235 mean train loss:  4.49555926e-03, bound:  3.15432131e-01\n",
      "Epoch: 25236 mean train loss:  4.49532596e-03, bound:  3.15432101e-01\n",
      "Epoch: 25237 mean train loss:  4.49515646e-03, bound:  3.15432101e-01\n",
      "Epoch: 25238 mean train loss:  4.49501863e-03, bound:  3.15432072e-01\n",
      "Epoch: 25239 mean train loss:  4.49482305e-03, bound:  3.15432042e-01\n",
      "Epoch: 25240 mean train loss:  4.49482119e-03, bound:  3.15432012e-01\n",
      "Epoch: 25241 mean train loss:  4.49479930e-03, bound:  3.15432012e-01\n",
      "Epoch: 25242 mean train loss:  4.49498044e-03, bound:  3.15432012e-01\n",
      "Epoch: 25243 mean train loss:  4.49539442e-03, bound:  3.15431952e-01\n",
      "Epoch: 25244 mean train loss:  4.49608732e-03, bound:  3.15431982e-01\n",
      "Epoch: 25245 mean train loss:  4.49728500e-03, bound:  3.15431923e-01\n",
      "Epoch: 25246 mean train loss:  4.49907873e-03, bound:  3.15431923e-01\n",
      "Epoch: 25247 mean train loss:  4.50167153e-03, bound:  3.15431893e-01\n",
      "Epoch: 25248 mean train loss:  4.50511742e-03, bound:  3.15431923e-01\n",
      "Epoch: 25249 mean train loss:  4.50940337e-03, bound:  3.15431833e-01\n",
      "Epoch: 25250 mean train loss:  4.51414427e-03, bound:  3.15431893e-01\n",
      "Epoch: 25251 mean train loss:  4.51807864e-03, bound:  3.15431774e-01\n",
      "Epoch: 25252 mean train loss:  4.51950356e-03, bound:  3.15431863e-01\n",
      "Epoch: 25253 mean train loss:  4.51662438e-03, bound:  3.15431744e-01\n",
      "Epoch: 25254 mean train loss:  4.50938893e-03, bound:  3.15431833e-01\n",
      "Epoch: 25255 mean train loss:  4.50007804e-03, bound:  3.15431744e-01\n",
      "Epoch: 25256 mean train loss:  4.49280767e-03, bound:  3.15431774e-01\n",
      "Epoch: 25257 mean train loss:  4.49045515e-03, bound:  3.15431744e-01\n",
      "Epoch: 25258 mean train loss:  4.49272664e-03, bound:  3.15431714e-01\n",
      "Epoch: 25259 mean train loss:  4.49705031e-03, bound:  3.15431744e-01\n",
      "Epoch: 25260 mean train loss:  4.50002681e-03, bound:  3.15431654e-01\n",
      "Epoch: 25261 mean train loss:  4.49967571e-03, bound:  3.15431714e-01\n",
      "Epoch: 25262 mean train loss:  4.49616974e-03, bound:  3.15431625e-01\n",
      "Epoch: 25263 mean train loss:  4.49175108e-03, bound:  3.15431654e-01\n",
      "Epoch: 25264 mean train loss:  4.48906003e-03, bound:  3.15431625e-01\n",
      "Epoch: 25265 mean train loss:  4.48912429e-03, bound:  3.15431595e-01\n",
      "Epoch: 25266 mean train loss:  4.49089101e-03, bound:  3.15431595e-01\n",
      "Epoch: 25267 mean train loss:  4.49259486e-03, bound:  3.15431535e-01\n",
      "Epoch: 25268 mean train loss:  4.49269312e-03, bound:  3.15431595e-01\n",
      "Epoch: 25269 mean train loss:  4.49111219e-03, bound:  3.15431535e-01\n",
      "Epoch: 25270 mean train loss:  4.48894547e-03, bound:  3.15431535e-01\n",
      "Epoch: 25271 mean train loss:  4.48743906e-03, bound:  3.15431535e-01\n",
      "Epoch: 25272 mean train loss:  4.48729796e-03, bound:  3.15431476e-01\n",
      "Epoch: 25273 mean train loss:  4.48810030e-03, bound:  3.15431476e-01\n",
      "Epoch: 25274 mean train loss:  4.48876899e-03, bound:  3.15431416e-01\n",
      "Epoch: 25275 mean train loss:  4.48878063e-03, bound:  3.15431416e-01\n",
      "Epoch: 25276 mean train loss:  4.48791683e-03, bound:  3.15431416e-01\n",
      "Epoch: 25277 mean train loss:  4.48674848e-03, bound:  3.15431416e-01\n",
      "Epoch: 25278 mean train loss:  4.48587956e-03, bound:  3.15431356e-01\n",
      "Epoch: 25279 mean train loss:  4.48558060e-03, bound:  3.15431356e-01\n",
      "Epoch: 25280 mean train loss:  4.48581902e-03, bound:  3.15431327e-01\n",
      "Epoch: 25281 mean train loss:  4.48597735e-03, bound:  3.15431327e-01\n",
      "Epoch: 25282 mean train loss:  4.48592426e-03, bound:  3.15431327e-01\n",
      "Epoch: 25283 mean train loss:  4.48546698e-03, bound:  3.15431297e-01\n",
      "Epoch: 25284 mean train loss:  4.48484952e-03, bound:  3.15431237e-01\n",
      "Epoch: 25285 mean train loss:  4.48430330e-03, bound:  3.15431237e-01\n",
      "Epoch: 25286 mean train loss:  4.48401272e-03, bound:  3.15431207e-01\n",
      "Epoch: 25287 mean train loss:  4.48401831e-03, bound:  3.15431207e-01\n",
      "Epoch: 25288 mean train loss:  4.48407466e-03, bound:  3.15431207e-01\n",
      "Epoch: 25289 mean train loss:  4.48397873e-03, bound:  3.15431178e-01\n",
      "Epoch: 25290 mean train loss:  4.48372820e-03, bound:  3.15431118e-01\n",
      "Epoch: 25291 mean train loss:  4.48329467e-03, bound:  3.15431118e-01\n",
      "Epoch: 25292 mean train loss:  4.48287837e-03, bound:  3.15431118e-01\n",
      "Epoch: 25293 mean train loss:  4.48251329e-03, bound:  3.15431088e-01\n",
      "Epoch: 25294 mean train loss:  4.48232796e-03, bound:  3.15431088e-01\n",
      "Epoch: 25295 mean train loss:  4.48221108e-03, bound:  3.15431058e-01\n",
      "Epoch: 25296 mean train loss:  4.48216777e-03, bound:  3.15431058e-01\n",
      "Epoch: 25297 mean train loss:  4.48199548e-03, bound:  3.15430999e-01\n",
      "Epoch: 25298 mean train loss:  4.48168628e-03, bound:  3.15430999e-01\n",
      "Epoch: 25299 mean train loss:  4.48133610e-03, bound:  3.15430969e-01\n",
      "Epoch: 25300 mean train loss:  4.48099012e-03, bound:  3.15430969e-01\n",
      "Epoch: 25301 mean train loss:  4.48070234e-03, bound:  3.15430969e-01\n",
      "Epoch: 25302 mean train loss:  4.48048115e-03, bound:  3.15430909e-01\n",
      "Epoch: 25303 mean train loss:  4.48035169e-03, bound:  3.15430909e-01\n",
      "Epoch: 25304 mean train loss:  4.48027439e-03, bound:  3.15430909e-01\n",
      "Epoch: 25305 mean train loss:  4.48008953e-03, bound:  3.15430909e-01\n",
      "Epoch: 25306 mean train loss:  4.47981432e-03, bound:  3.15430850e-01\n",
      "Epoch: 25307 mean train loss:  4.47950559e-03, bound:  3.15430850e-01\n",
      "Epoch: 25308 mean train loss:  4.47919732e-03, bound:  3.15430790e-01\n",
      "Epoch: 25309 mean train loss:  4.47891140e-03, bound:  3.15430790e-01\n",
      "Epoch: 25310 mean train loss:  4.47867159e-03, bound:  3.15430790e-01\n",
      "Epoch: 25311 mean train loss:  4.47853608e-03, bound:  3.15430790e-01\n",
      "Epoch: 25312 mean train loss:  4.47832048e-03, bound:  3.15430760e-01\n",
      "Epoch: 25313 mean train loss:  4.47811186e-03, bound:  3.15430760e-01\n",
      "Epoch: 25314 mean train loss:  4.47787344e-03, bound:  3.15430731e-01\n",
      "Epoch: 25315 mean train loss:  4.47769230e-03, bound:  3.15430731e-01\n",
      "Epoch: 25316 mean train loss:  4.47745435e-03, bound:  3.15430731e-01\n",
      "Epoch: 25317 mean train loss:  4.47718846e-03, bound:  3.15430671e-01\n",
      "Epoch: 25318 mean train loss:  4.47688950e-03, bound:  3.15430671e-01\n",
      "Epoch: 25319 mean train loss:  4.47674142e-03, bound:  3.15430641e-01\n",
      "Epoch: 25320 mean train loss:  4.47648158e-03, bound:  3.15430611e-01\n",
      "Epoch: 25321 mean train loss:  4.47631488e-03, bound:  3.15430611e-01\n",
      "Epoch: 25322 mean train loss:  4.47607879e-03, bound:  3.15430552e-01\n",
      "Epoch: 25323 mean train loss:  4.47582407e-03, bound:  3.15430552e-01\n",
      "Epoch: 25324 mean train loss:  4.47560567e-03, bound:  3.15430552e-01\n",
      "Epoch: 25325 mean train loss:  4.47538774e-03, bound:  3.15430522e-01\n",
      "Epoch: 25326 mean train loss:  4.47514514e-03, bound:  3.15430522e-01\n",
      "Epoch: 25327 mean train loss:  4.47484059e-03, bound:  3.15430522e-01\n",
      "Epoch: 25328 mean train loss:  4.47468879e-03, bound:  3.15430462e-01\n",
      "Epoch: 25329 mean train loss:  4.47444525e-03, bound:  3.15430433e-01\n",
      "Epoch: 25330 mean train loss:  4.47429111e-03, bound:  3.15430433e-01\n",
      "Epoch: 25331 mean train loss:  4.47403267e-03, bound:  3.15430403e-01\n",
      "Epoch: 25332 mean train loss:  4.47380636e-03, bound:  3.15430403e-01\n",
      "Epoch: 25333 mean train loss:  4.47357213e-03, bound:  3.15430403e-01\n",
      "Epoch: 25334 mean train loss:  4.47338633e-03, bound:  3.15430343e-01\n",
      "Epoch: 25335 mean train loss:  4.47312091e-03, bound:  3.15430343e-01\n",
      "Epoch: 25336 mean train loss:  4.47289180e-03, bound:  3.15430313e-01\n",
      "Epoch: 25337 mean train loss:  4.47269809e-03, bound:  3.15430313e-01\n",
      "Epoch: 25338 mean train loss:  4.47241170e-03, bound:  3.15430284e-01\n",
      "Epoch: 25339 mean train loss:  4.47219796e-03, bound:  3.15430284e-01\n",
      "Epoch: 25340 mean train loss:  4.47200797e-03, bound:  3.15430284e-01\n",
      "Epoch: 25341 mean train loss:  4.47168853e-03, bound:  3.15430224e-01\n",
      "Epoch: 25342 mean train loss:  4.47156187e-03, bound:  3.15430194e-01\n",
      "Epoch: 25343 mean train loss:  4.47133509e-03, bound:  3.15430194e-01\n",
      "Epoch: 25344 mean train loss:  4.47104033e-03, bound:  3.15430164e-01\n",
      "Epoch: 25345 mean train loss:  4.47085826e-03, bound:  3.15430164e-01\n",
      "Epoch: 25346 mean train loss:  4.47066640e-03, bound:  3.15430164e-01\n",
      "Epoch: 25347 mean train loss:  4.47037676e-03, bound:  3.15430105e-01\n",
      "Epoch: 25348 mean train loss:  4.47017606e-03, bound:  3.15430105e-01\n",
      "Epoch: 25349 mean train loss:  4.46997024e-03, bound:  3.15430105e-01\n",
      "Epoch: 25350 mean train loss:  4.46970295e-03, bound:  3.15430075e-01\n",
      "Epoch: 25351 mean train loss:  4.46955347e-03, bound:  3.15430075e-01\n",
      "Epoch: 25352 mean train loss:  4.46927734e-03, bound:  3.15430015e-01\n",
      "Epoch: 25353 mean train loss:  4.46904404e-03, bound:  3.15429986e-01\n",
      "Epoch: 25354 mean train loss:  4.46882565e-03, bound:  3.15429986e-01\n",
      "Epoch: 25355 mean train loss:  4.46860166e-03, bound:  3.15429986e-01\n",
      "Epoch: 25356 mean train loss:  4.46836045e-03, bound:  3.15429986e-01\n",
      "Epoch: 25357 mean train loss:  4.46814438e-03, bound:  3.15429956e-01\n",
      "Epoch: 25358 mean train loss:  4.46793716e-03, bound:  3.15429956e-01\n",
      "Epoch: 25359 mean train loss:  4.46773507e-03, bound:  3.15429896e-01\n",
      "Epoch: 25360 mean train loss:  4.46746685e-03, bound:  3.15429866e-01\n",
      "Epoch: 25361 mean train loss:  4.46725823e-03, bound:  3.15429866e-01\n",
      "Epoch: 25362 mean train loss:  4.46702819e-03, bound:  3.15429866e-01\n",
      "Epoch: 25363 mean train loss:  4.46686801e-03, bound:  3.15429837e-01\n",
      "Epoch: 25364 mean train loss:  4.46662912e-03, bound:  3.15429837e-01\n",
      "Epoch: 25365 mean train loss:  4.46637627e-03, bound:  3.15429777e-01\n",
      "Epoch: 25366 mean train loss:  4.46618535e-03, bound:  3.15429777e-01\n",
      "Epoch: 25367 mean train loss:  4.46590036e-03, bound:  3.15429777e-01\n",
      "Epoch: 25368 mean train loss:  4.46568988e-03, bound:  3.15429747e-01\n",
      "Epoch: 25369 mean train loss:  4.46546543e-03, bound:  3.15429717e-01\n",
      "Epoch: 25370 mean train loss:  4.46524285e-03, bound:  3.15429717e-01\n",
      "Epoch: 25371 mean train loss:  4.46502352e-03, bound:  3.15429717e-01\n",
      "Epoch: 25372 mean train loss:  4.46482049e-03, bound:  3.15429658e-01\n",
      "Epoch: 25373 mean train loss:  4.46450617e-03, bound:  3.15429658e-01\n",
      "Epoch: 25374 mean train loss:  4.46431851e-03, bound:  3.15429658e-01\n",
      "Epoch: 25375 mean train loss:  4.46406193e-03, bound:  3.15429598e-01\n",
      "Epoch: 25376 mean train loss:  4.46383143e-03, bound:  3.15429598e-01\n",
      "Epoch: 25377 mean train loss:  4.46361024e-03, bound:  3.15429568e-01\n",
      "Epoch: 25378 mean train loss:  4.46337508e-03, bound:  3.15429538e-01\n",
      "Epoch: 25379 mean train loss:  4.46315622e-03, bound:  3.15429538e-01\n",
      "Epoch: 25380 mean train loss:  4.46296483e-03, bound:  3.15429538e-01\n",
      "Epoch: 25381 mean train loss:  4.46272874e-03, bound:  3.15429509e-01\n",
      "Epoch: 25382 mean train loss:  4.46246425e-03, bound:  3.15429479e-01\n",
      "Epoch: 25383 mean train loss:  4.46226867e-03, bound:  3.15429479e-01\n",
      "Epoch: 25384 mean train loss:  4.46199486e-03, bound:  3.15429449e-01\n",
      "Epoch: 25385 mean train loss:  4.46181931e-03, bound:  3.15429419e-01\n",
      "Epoch: 25386 mean train loss:  4.46159998e-03, bound:  3.15429419e-01\n",
      "Epoch: 25387 mean train loss:  4.46132664e-03, bound:  3.15429389e-01\n",
      "Epoch: 25388 mean train loss:  4.46110731e-03, bound:  3.15429389e-01\n",
      "Epoch: 25389 mean train loss:  4.46083117e-03, bound:  3.15429389e-01\n",
      "Epoch: 25390 mean train loss:  4.46063979e-03, bound:  3.15429330e-01\n",
      "Epoch: 25391 mean train loss:  4.46041068e-03, bound:  3.15429300e-01\n",
      "Epoch: 25392 mean train loss:  4.46017552e-03, bound:  3.15429300e-01\n",
      "Epoch: 25393 mean train loss:  4.46000556e-03, bound:  3.15429300e-01\n",
      "Epoch: 25394 mean train loss:  4.45975596e-03, bound:  3.15429270e-01\n",
      "Epoch: 25395 mean train loss:  4.45951475e-03, bound:  3.15429270e-01\n",
      "Epoch: 25396 mean train loss:  4.45932616e-03, bound:  3.15429211e-01\n",
      "Epoch: 25397 mean train loss:  4.45906073e-03, bound:  3.15429211e-01\n",
      "Epoch: 25398 mean train loss:  4.45888611e-03, bound:  3.15429181e-01\n",
      "Epoch: 25399 mean train loss:  4.45862906e-03, bound:  3.15429181e-01\n",
      "Epoch: 25400 mean train loss:  4.45843069e-03, bound:  3.15429151e-01\n",
      "Epoch: 25401 mean train loss:  4.45825746e-03, bound:  3.15429151e-01\n",
      "Epoch: 25402 mean train loss:  4.45810426e-03, bound:  3.15429121e-01\n",
      "Epoch: 25403 mean train loss:  4.45795059e-03, bound:  3.15429091e-01\n",
      "Epoch: 25404 mean train loss:  4.45776433e-03, bound:  3.15429091e-01\n",
      "Epoch: 25405 mean train loss:  4.45766561e-03, bound:  3.15429091e-01\n",
      "Epoch: 25406 mean train loss:  4.45758784e-03, bound:  3.15429062e-01\n",
      "Epoch: 25407 mean train loss:  4.45762323e-03, bound:  3.15429002e-01\n",
      "Epoch: 25408 mean train loss:  4.45774058e-03, bound:  3.15429002e-01\n",
      "Epoch: 25409 mean train loss:  4.45801252e-03, bound:  3.15428972e-01\n",
      "Epoch: 25410 mean train loss:  4.45845025e-03, bound:  3.15428972e-01\n",
      "Epoch: 25411 mean train loss:  4.45923209e-03, bound:  3.15428942e-01\n",
      "Epoch: 25412 mean train loss:  4.46044980e-03, bound:  3.15428972e-01\n",
      "Epoch: 25413 mean train loss:  4.46226448e-03, bound:  3.15428883e-01\n",
      "Epoch: 25414 mean train loss:  4.46481816e-03, bound:  3.15428913e-01\n",
      "Epoch: 25415 mean train loss:  4.46806895e-03, bound:  3.15428853e-01\n",
      "Epoch: 25416 mean train loss:  4.47180076e-03, bound:  3.15428883e-01\n",
      "Epoch: 25417 mean train loss:  4.47531277e-03, bound:  3.15428764e-01\n",
      "Epoch: 25418 mean train loss:  4.47753631e-03, bound:  3.15428853e-01\n",
      "Epoch: 25419 mean train loss:  4.47710976e-03, bound:  3.15428764e-01\n",
      "Epoch: 25420 mean train loss:  4.47307900e-03, bound:  3.15428823e-01\n",
      "Epoch: 25421 mean train loss:  4.46639536e-03, bound:  3.15428734e-01\n",
      "Epoch: 25422 mean train loss:  4.45912220e-03, bound:  3.15428764e-01\n",
      "Epoch: 25423 mean train loss:  4.45422018e-03, bound:  3.15428734e-01\n",
      "Epoch: 25424 mean train loss:  4.45308164e-03, bound:  3.15428734e-01\n",
      "Epoch: 25425 mean train loss:  4.45501041e-03, bound:  3.15428734e-01\n",
      "Epoch: 25426 mean train loss:  4.45823278e-03, bound:  3.15428674e-01\n",
      "Epoch: 25427 mean train loss:  4.46044654e-03, bound:  3.15428704e-01\n",
      "Epoch: 25428 mean train loss:  4.46031149e-03, bound:  3.15428644e-01\n",
      "Epoch: 25429 mean train loss:  4.45788447e-03, bound:  3.15428674e-01\n",
      "Epoch: 25430 mean train loss:  4.45444137e-03, bound:  3.15428615e-01\n",
      "Epoch: 25431 mean train loss:  4.45196358e-03, bound:  3.15428615e-01\n",
      "Epoch: 25432 mean train loss:  4.45123203e-03, bound:  3.15428585e-01\n",
      "Epoch: 25433 mean train loss:  4.45227232e-03, bound:  3.15428585e-01\n",
      "Epoch: 25434 mean train loss:  4.45369678e-03, bound:  3.15428585e-01\n",
      "Epoch: 25435 mean train loss:  4.45449166e-03, bound:  3.15428525e-01\n",
      "Epoch: 25436 mean train loss:  4.45401855e-03, bound:  3.15428555e-01\n",
      "Epoch: 25437 mean train loss:  4.45258152e-03, bound:  3.15428495e-01\n",
      "Epoch: 25438 mean train loss:  4.45093494e-03, bound:  3.15428495e-01\n",
      "Epoch: 25439 mean train loss:  4.44972189e-03, bound:  3.15428495e-01\n",
      "Epoch: 25440 mean train loss:  4.44955286e-03, bound:  3.15428466e-01\n",
      "Epoch: 25441 mean train loss:  4.44998778e-03, bound:  3.15428436e-01\n",
      "Epoch: 25442 mean train loss:  4.45057964e-03, bound:  3.15428406e-01\n",
      "Epoch: 25443 mean train loss:  4.45073424e-03, bound:  3.15428406e-01\n",
      "Epoch: 25444 mean train loss:  4.45025740e-03, bound:  3.15428376e-01\n",
      "Epoch: 25445 mean train loss:  4.44942620e-03, bound:  3.15428376e-01\n",
      "Epoch: 25446 mean train loss:  4.44850838e-03, bound:  3.15428346e-01\n",
      "Epoch: 25447 mean train loss:  4.44794307e-03, bound:  3.15428317e-01\n",
      "Epoch: 25448 mean train loss:  4.44772327e-03, bound:  3.15428287e-01\n",
      "Epoch: 25449 mean train loss:  4.44793468e-03, bound:  3.15428287e-01\n",
      "Epoch: 25450 mean train loss:  4.44810931e-03, bound:  3.15428287e-01\n",
      "Epoch: 25451 mean train loss:  4.44803899e-03, bound:  3.15428257e-01\n",
      "Epoch: 25452 mean train loss:  4.44772374e-03, bound:  3.15428257e-01\n",
      "Epoch: 25453 mean train loss:  4.44713747e-03, bound:  3.15428197e-01\n",
      "Epoch: 25454 mean train loss:  4.44657262e-03, bound:  3.15428197e-01\n",
      "Epoch: 25455 mean train loss:  4.44620475e-03, bound:  3.15428168e-01\n",
      "Epoch: 25456 mean train loss:  4.44598868e-03, bound:  3.15428168e-01\n",
      "Epoch: 25457 mean train loss:  4.44592489e-03, bound:  3.15428168e-01\n",
      "Epoch: 25458 mean train loss:  4.44581266e-03, bound:  3.15428108e-01\n",
      "Epoch: 25459 mean train loss:  4.44573583e-03, bound:  3.15428078e-01\n",
      "Epoch: 25460 mean train loss:  4.44540568e-03, bound:  3.15428048e-01\n",
      "Epoch: 25461 mean train loss:  4.44511930e-03, bound:  3.15428048e-01\n",
      "Epoch: 25462 mean train loss:  4.44474444e-03, bound:  3.15428048e-01\n",
      "Epoch: 25463 mean train loss:  4.44441708e-03, bound:  3.15428048e-01\n",
      "Epoch: 25464 mean train loss:  4.44420753e-03, bound:  3.15428019e-01\n",
      "Epoch: 25465 mean train loss:  4.44406178e-03, bound:  3.15427989e-01\n",
      "Epoch: 25466 mean train loss:  4.44385782e-03, bound:  3.15427959e-01\n",
      "Epoch: 25467 mean train loss:  4.44373209e-03, bound:  3.15427959e-01\n",
      "Epoch: 25468 mean train loss:  4.44352580e-03, bound:  3.15427929e-01\n",
      "Epoch: 25469 mean train loss:  4.44327574e-03, bound:  3.15427929e-01\n",
      "Epoch: 25470 mean train loss:  4.44291672e-03, bound:  3.15427929e-01\n",
      "Epoch: 25471 mean train loss:  4.44272440e-03, bound:  3.15427870e-01\n",
      "Epoch: 25472 mean train loss:  4.44245664e-03, bound:  3.15427870e-01\n",
      "Epoch: 25473 mean train loss:  4.44221590e-03, bound:  3.15427870e-01\n",
      "Epoch: 25474 mean train loss:  4.44199936e-03, bound:  3.15427840e-01\n",
      "Epoch: 25475 mean train loss:  4.44182521e-03, bound:  3.15427840e-01\n",
      "Epoch: 25476 mean train loss:  4.44163848e-03, bound:  3.15427810e-01\n",
      "Epoch: 25477 mean train loss:  4.44138050e-03, bound:  3.15427810e-01\n",
      "Epoch: 25478 mean train loss:  4.44123475e-03, bound:  3.15427750e-01\n",
      "Epoch: 25479 mean train loss:  4.44095628e-03, bound:  3.15427750e-01\n",
      "Epoch: 25480 mean train loss:  4.44076583e-03, bound:  3.15427721e-01\n",
      "Epoch: 25481 mean train loss:  4.44056280e-03, bound:  3.15427721e-01\n",
      "Epoch: 25482 mean train loss:  4.44027083e-03, bound:  3.15427691e-01\n",
      "Epoch: 25483 mean train loss:  4.44002403e-03, bound:  3.15427631e-01\n",
      "Epoch: 25484 mean train loss:  4.43978235e-03, bound:  3.15427631e-01\n",
      "Epoch: 25485 mean train loss:  4.43966733e-03, bound:  3.15427631e-01\n",
      "Epoch: 25486 mean train loss:  4.43948107e-03, bound:  3.15427631e-01\n",
      "Epoch: 25487 mean train loss:  4.43921657e-03, bound:  3.15427601e-01\n",
      "Epoch: 25488 mean train loss:  4.43901122e-03, bound:  3.15427572e-01\n",
      "Epoch: 25489 mean train loss:  4.43872670e-03, bound:  3.15427572e-01\n",
      "Epoch: 25490 mean train loss:  4.43853857e-03, bound:  3.15427572e-01\n",
      "Epoch: 25491 mean train loss:  4.43828106e-03, bound:  3.15427542e-01\n",
      "Epoch: 25492 mean train loss:  4.43806732e-03, bound:  3.15427512e-01\n",
      "Epoch: 25493 mean train loss:  4.43782518e-03, bound:  3.15427482e-01\n",
      "Epoch: 25494 mean train loss:  4.43760306e-03, bound:  3.15427482e-01\n",
      "Epoch: 25495 mean train loss:  4.43733856e-03, bound:  3.15427482e-01\n",
      "Epoch: 25496 mean train loss:  4.43717418e-03, bound:  3.15427452e-01\n",
      "Epoch: 25497 mean train loss:  4.43690922e-03, bound:  3.15427423e-01\n",
      "Epoch: 25498 mean train loss:  4.43669641e-03, bound:  3.15427393e-01\n",
      "Epoch: 25499 mean train loss:  4.43647895e-03, bound:  3.15427363e-01\n",
      "Epoch: 25500 mean train loss:  4.43624565e-03, bound:  3.15427363e-01\n",
      "Epoch: 25501 mean train loss:  4.43603378e-03, bound:  3.15427363e-01\n",
      "Epoch: 25502 mean train loss:  4.43583028e-03, bound:  3.15427303e-01\n",
      "Epoch: 25503 mean train loss:  4.43560211e-03, bound:  3.15427303e-01\n",
      "Epoch: 25504 mean train loss:  4.43542376e-03, bound:  3.15427303e-01\n",
      "Epoch: 25505 mean train loss:  4.43513971e-03, bound:  3.15427244e-01\n",
      "Epoch: 25506 mean train loss:  4.43492038e-03, bound:  3.15427244e-01\n",
      "Epoch: 25507 mean train loss:  4.43470757e-03, bound:  3.15427244e-01\n",
      "Epoch: 25508 mean train loss:  4.43448033e-03, bound:  3.15427184e-01\n",
      "Epoch: 25509 mean train loss:  4.43426985e-03, bound:  3.15427184e-01\n",
      "Epoch: 25510 mean train loss:  4.43400443e-03, bound:  3.15427184e-01\n",
      "Epoch: 25511 mean train loss:  4.43381397e-03, bound:  3.15427154e-01\n",
      "Epoch: 25512 mean train loss:  4.43362677e-03, bound:  3.15427154e-01\n",
      "Epoch: 25513 mean train loss:  4.43340139e-03, bound:  3.15427154e-01\n",
      "Epoch: 25514 mean train loss:  4.43317927e-03, bound:  3.15427125e-01\n",
      "Epoch: 25515 mean train loss:  4.43296647e-03, bound:  3.15427065e-01\n",
      "Epoch: 25516 mean train loss:  4.43271082e-03, bound:  3.15427065e-01\n",
      "Epoch: 25517 mean train loss:  4.43253433e-03, bound:  3.15427065e-01\n",
      "Epoch: 25518 mean train loss:  4.43228893e-03, bound:  3.15427035e-01\n",
      "Epoch: 25519 mean train loss:  4.43205470e-03, bound:  3.15427035e-01\n",
      "Epoch: 25520 mean train loss:  4.43178555e-03, bound:  3.15426975e-01\n",
      "Epoch: 25521 mean train loss:  4.43160534e-03, bound:  3.15426975e-01\n",
      "Epoch: 25522 mean train loss:  4.43140883e-03, bound:  3.15426946e-01\n",
      "Epoch: 25523 mean train loss:  4.43115830e-03, bound:  3.15426946e-01\n",
      "Epoch: 25524 mean train loss:  4.43097437e-03, bound:  3.15426916e-01\n",
      "Epoch: 25525 mean train loss:  4.43073455e-03, bound:  3.15426916e-01\n",
      "Epoch: 25526 mean train loss:  4.43057250e-03, bound:  3.15426916e-01\n",
      "Epoch: 25527 mean train loss:  4.43018088e-03, bound:  3.15426856e-01\n",
      "Epoch: 25528 mean train loss:  4.43001604e-03, bound:  3.15426856e-01\n",
      "Epoch: 25529 mean train loss:  4.42979252e-03, bound:  3.15426856e-01\n",
      "Epoch: 25530 mean train loss:  4.42958996e-03, bound:  3.15426826e-01\n",
      "Epoch: 25531 mean train loss:  4.42933291e-03, bound:  3.15426797e-01\n",
      "Epoch: 25532 mean train loss:  4.42913827e-03, bound:  3.15426797e-01\n",
      "Epoch: 25533 mean train loss:  4.42896783e-03, bound:  3.15426737e-01\n",
      "Epoch: 25534 mean train loss:  4.42867354e-03, bound:  3.15426737e-01\n",
      "Epoch: 25535 mean train loss:  4.42848727e-03, bound:  3.15426737e-01\n",
      "Epoch: 25536 mean train loss:  4.42821300e-03, bound:  3.15426707e-01\n",
      "Epoch: 25537 mean train loss:  4.42803884e-03, bound:  3.15426677e-01\n",
      "Epoch: 25538 mean train loss:  4.42783628e-03, bound:  3.15426677e-01\n",
      "Epoch: 25539 mean train loss:  4.42760065e-03, bound:  3.15426677e-01\n",
      "Epoch: 25540 mean train loss:  4.42734640e-03, bound:  3.15426618e-01\n",
      "Epoch: 25541 mean train loss:  4.42717178e-03, bound:  3.15426618e-01\n",
      "Epoch: 25542 mean train loss:  4.42688214e-03, bound:  3.15426618e-01\n",
      "Epoch: 25543 mean train loss:  4.42667957e-03, bound:  3.15426588e-01\n",
      "Epoch: 25544 mean train loss:  4.42645978e-03, bound:  3.15426528e-01\n",
      "Epoch: 25545 mean train loss:  4.42624092e-03, bound:  3.15426528e-01\n",
      "Epoch: 25546 mean train loss:  4.42599691e-03, bound:  3.15426528e-01\n",
      "Epoch: 25547 mean train loss:  4.42583859e-03, bound:  3.15426499e-01\n",
      "Epoch: 25548 mean train loss:  4.42556478e-03, bound:  3.15426499e-01\n",
      "Epoch: 25549 mean train loss:  4.42533102e-03, bound:  3.15426469e-01\n",
      "Epoch: 25550 mean train loss:  4.42507537e-03, bound:  3.15426469e-01\n",
      "Epoch: 25551 mean train loss:  4.42482810e-03, bound:  3.15426409e-01\n",
      "Epoch: 25552 mean train loss:  4.42464417e-03, bound:  3.15426409e-01\n",
      "Epoch: 25553 mean train loss:  4.42448445e-03, bound:  3.15426379e-01\n",
      "Epoch: 25554 mean train loss:  4.42418316e-03, bound:  3.15426379e-01\n",
      "Epoch: 25555 mean train loss:  4.42402810e-03, bound:  3.15426350e-01\n",
      "Epoch: 25556 mean train loss:  4.42380458e-03, bound:  3.15426350e-01\n",
      "Epoch: 25557 mean train loss:  4.42355918e-03, bound:  3.15426350e-01\n",
      "Epoch: 25558 mean train loss:  4.42334078e-03, bound:  3.15426290e-01\n",
      "Epoch: 25559 mean train loss:  4.42323647e-03, bound:  3.15426290e-01\n",
      "Epoch: 25560 mean train loss:  4.42302087e-03, bound:  3.15426260e-01\n",
      "Epoch: 25561 mean train loss:  4.42287931e-03, bound:  3.15426230e-01\n",
      "Epoch: 25562 mean train loss:  4.42282064e-03, bound:  3.15426230e-01\n",
      "Epoch: 25563 mean train loss:  4.42272611e-03, bound:  3.15426230e-01\n",
      "Epoch: 25564 mean train loss:  4.42274287e-03, bound:  3.15426171e-01\n",
      "Epoch: 25565 mean train loss:  4.42295894e-03, bound:  3.15426171e-01\n",
      "Epoch: 25566 mean train loss:  4.42327000e-03, bound:  3.15426141e-01\n",
      "Epoch: 25567 mean train loss:  4.42395499e-03, bound:  3.15426141e-01\n",
      "Epoch: 25568 mean train loss:  4.42490354e-03, bound:  3.15426081e-01\n",
      "Epoch: 25569 mean train loss:  4.42665536e-03, bound:  3.15426141e-01\n",
      "Epoch: 25570 mean train loss:  4.42917645e-03, bound:  3.15426052e-01\n",
      "Epoch: 25571 mean train loss:  4.43283096e-03, bound:  3.15426081e-01\n",
      "Epoch: 25572 mean train loss:  4.43773437e-03, bound:  3.15426022e-01\n",
      "Epoch: 25573 mean train loss:  4.44346899e-03, bound:  3.15426052e-01\n",
      "Epoch: 25574 mean train loss:  4.44859127e-03, bound:  3.15425932e-01\n",
      "Epoch: 25575 mean train loss:  4.45087114e-03, bound:  3.15426052e-01\n",
      "Epoch: 25576 mean train loss:  4.44785226e-03, bound:  3.15425932e-01\n",
      "Epoch: 25577 mean train loss:  4.43951553e-03, bound:  3.15426022e-01\n",
      "Epoch: 25578 mean train loss:  4.42872429e-03, bound:  3.15425932e-01\n",
      "Epoch: 25579 mean train loss:  4.42061527e-03, bound:  3.15425932e-01\n",
      "Epoch: 25580 mean train loss:  4.41853143e-03, bound:  3.15425932e-01\n",
      "Epoch: 25581 mean train loss:  4.42166720e-03, bound:  3.15425903e-01\n",
      "Epoch: 25582 mean train loss:  4.42669541e-03, bound:  3.15425932e-01\n",
      "Epoch: 25583 mean train loss:  4.42966353e-03, bound:  3.15425813e-01\n",
      "Epoch: 25584 mean train loss:  4.42828191e-03, bound:  3.15425903e-01\n",
      "Epoch: 25585 mean train loss:  4.42365231e-03, bound:  3.15425813e-01\n",
      "Epoch: 25586 mean train loss:  4.41887882e-03, bound:  3.15425813e-01\n",
      "Epoch: 25587 mean train loss:  4.41685040e-03, bound:  3.15425813e-01\n",
      "Epoch: 25588 mean train loss:  4.41800850e-03, bound:  3.15425783e-01\n",
      "Epoch: 25589 mean train loss:  4.42052307e-03, bound:  3.15425783e-01\n",
      "Epoch: 25590 mean train loss:  4.42199176e-03, bound:  3.15425724e-01\n",
      "Epoch: 25591 mean train loss:  4.42117639e-03, bound:  3.15425724e-01\n",
      "Epoch: 25592 mean train loss:  4.41883644e-03, bound:  3.15425694e-01\n",
      "Epoch: 25593 mean train loss:  4.41634282e-03, bound:  3.15425694e-01\n",
      "Epoch: 25594 mean train loss:  4.41544084e-03, bound:  3.15425664e-01\n",
      "Epoch: 25595 mean train loss:  4.41618357e-03, bound:  3.15425664e-01\n",
      "Epoch: 25596 mean train loss:  4.41747392e-03, bound:  3.15425664e-01\n",
      "Epoch: 25597 mean train loss:  4.41788929e-03, bound:  3.15425605e-01\n",
      "Epoch: 25598 mean train loss:  4.41702176e-03, bound:  3.15425634e-01\n",
      "Epoch: 25599 mean train loss:  4.41550417e-03, bound:  3.15425605e-01\n",
      "Epoch: 25600 mean train loss:  4.41426877e-03, bound:  3.15425605e-01\n",
      "Epoch: 25601 mean train loss:  4.41408204e-03, bound:  3.15425545e-01\n",
      "Epoch: 25602 mean train loss:  4.41448949e-03, bound:  3.15425515e-01\n",
      "Epoch: 25603 mean train loss:  4.41487227e-03, bound:  3.15425545e-01\n",
      "Epoch: 25604 mean train loss:  4.41479823e-03, bound:  3.15425485e-01\n",
      "Epoch: 25605 mean train loss:  4.41418448e-03, bound:  3.15425485e-01\n",
      "Epoch: 25606 mean train loss:  4.41328622e-03, bound:  3.15425456e-01\n",
      "Epoch: 25607 mean train loss:  4.41273069e-03, bound:  3.15425426e-01\n",
      "Epoch: 25608 mean train loss:  4.41252440e-03, bound:  3.15425426e-01\n",
      "Epoch: 25609 mean train loss:  4.41264734e-03, bound:  3.15425396e-01\n",
      "Epoch: 25610 mean train loss:  4.41276329e-03, bound:  3.15425396e-01\n",
      "Epoch: 25611 mean train loss:  4.41261986e-03, bound:  3.15425366e-01\n",
      "Epoch: 25612 mean train loss:  4.41206899e-03, bound:  3.15425366e-01\n",
      "Epoch: 25613 mean train loss:  4.41162474e-03, bound:  3.15425336e-01\n",
      "Epoch: 25614 mean train loss:  4.41122148e-03, bound:  3.15425336e-01\n",
      "Epoch: 25615 mean train loss:  4.41097189e-03, bound:  3.15425277e-01\n",
      "Epoch: 25616 mean train loss:  4.41089924e-03, bound:  3.15425277e-01\n",
      "Epoch: 25617 mean train loss:  4.41088621e-03, bound:  3.15425247e-01\n",
      "Epoch: 25618 mean train loss:  4.41066269e-03, bound:  3.15425247e-01\n",
      "Epoch: 25619 mean train loss:  4.41038422e-03, bound:  3.15425247e-01\n",
      "Epoch: 25620 mean train loss:  4.41003917e-03, bound:  3.15425217e-01\n",
      "Epoch: 25621 mean train loss:  4.40968061e-03, bound:  3.15425217e-01\n",
      "Epoch: 25622 mean train loss:  4.40952787e-03, bound:  3.15425158e-01\n",
      "Epoch: 25623 mean train loss:  4.40928387e-03, bound:  3.15425128e-01\n",
      "Epoch: 25624 mean train loss:  4.40917257e-03, bound:  3.15425128e-01\n",
      "Epoch: 25625 mean train loss:  4.40897746e-03, bound:  3.15425128e-01\n",
      "Epoch: 25626 mean train loss:  4.40870272e-03, bound:  3.15425128e-01\n",
      "Epoch: 25627 mean train loss:  4.40850807e-03, bound:  3.15425098e-01\n",
      "Epoch: 25628 mean train loss:  4.40820539e-03, bound:  3.15425068e-01\n",
      "Epoch: 25629 mean train loss:  4.40796604e-03, bound:  3.15425038e-01\n",
      "Epoch: 25630 mean train loss:  4.40781051e-03, bound:  3.15425009e-01\n",
      "Epoch: 25631 mean train loss:  4.40759212e-03, bound:  3.15425009e-01\n",
      "Epoch: 25632 mean train loss:  4.40742401e-03, bound:  3.15425009e-01\n",
      "Epoch: 25633 mean train loss:  4.40720702e-03, bound:  3.15425009e-01\n",
      "Epoch: 25634 mean train loss:  4.40698070e-03, bound:  3.15424949e-01\n",
      "Epoch: 25635 mean train loss:  4.40670876e-03, bound:  3.15424949e-01\n",
      "Epoch: 25636 mean train loss:  4.40650200e-03, bound:  3.15424919e-01\n",
      "Epoch: 25637 mean train loss:  4.40627057e-03, bound:  3.15424919e-01\n",
      "Epoch: 25638 mean train loss:  4.40604333e-03, bound:  3.15424919e-01\n",
      "Epoch: 25639 mean train loss:  4.40589478e-03, bound:  3.15424889e-01\n",
      "Epoch: 25640 mean train loss:  4.40567639e-03, bound:  3.15424860e-01\n",
      "Epoch: 25641 mean train loss:  4.40547476e-03, bound:  3.15424830e-01\n",
      "Epoch: 25642 mean train loss:  4.40522330e-03, bound:  3.15424800e-01\n",
      "Epoch: 25643 mean train loss:  4.40506963e-03, bound:  3.15424800e-01\n",
      "Epoch: 25644 mean train loss:  4.40480374e-03, bound:  3.15424800e-01\n",
      "Epoch: 25645 mean train loss:  4.40454436e-03, bound:  3.15424770e-01\n",
      "Epoch: 25646 mean train loss:  4.40436881e-03, bound:  3.15424770e-01\n",
      "Epoch: 25647 mean train loss:  4.40418394e-03, bound:  3.15424711e-01\n",
      "Epoch: 25648 mean train loss:  4.40392736e-03, bound:  3.15424711e-01\n",
      "Epoch: 25649 mean train loss:  4.40374110e-03, bound:  3.15424681e-01\n",
      "Epoch: 25650 mean train loss:  4.40353807e-03, bound:  3.15424681e-01\n",
      "Epoch: 25651 mean train loss:  4.40331921e-03, bound:  3.15424651e-01\n",
      "Epoch: 25652 mean train loss:  4.40306077e-03, bound:  3.15424651e-01\n",
      "Epoch: 25653 mean train loss:  4.40284330e-03, bound:  3.15424621e-01\n",
      "Epoch: 25654 mean train loss:  4.40264307e-03, bound:  3.15424621e-01\n",
      "Epoch: 25655 mean train loss:  4.40246984e-03, bound:  3.15424591e-01\n",
      "Epoch: 25656 mean train loss:  4.40227473e-03, bound:  3.15424562e-01\n",
      "Epoch: 25657 mean train loss:  4.40200977e-03, bound:  3.15424562e-01\n",
      "Epoch: 25658 mean train loss:  4.40183049e-03, bound:  3.15424532e-01\n",
      "Epoch: 25659 mean train loss:  4.40161722e-03, bound:  3.15424532e-01\n",
      "Epoch: 25660 mean train loss:  4.40137414e-03, bound:  3.15424532e-01\n",
      "Epoch: 25661 mean train loss:  4.40109847e-03, bound:  3.15424472e-01\n",
      "Epoch: 25662 mean train loss:  4.40091221e-03, bound:  3.15424442e-01\n",
      "Epoch: 25663 mean train loss:  4.40067844e-03, bound:  3.15424442e-01\n",
      "Epoch: 25664 mean train loss:  4.40050289e-03, bound:  3.15424442e-01\n",
      "Epoch: 25665 mean train loss:  4.40026168e-03, bound:  3.15424412e-01\n",
      "Epoch: 25666 mean train loss:  4.40001069e-03, bound:  3.15424412e-01\n",
      "Epoch: 25667 mean train loss:  4.39988403e-03, bound:  3.15424383e-01\n",
      "Epoch: 25668 mean train loss:  4.39960929e-03, bound:  3.15424353e-01\n",
      "Epoch: 25669 mean train loss:  4.39943234e-03, bound:  3.15424353e-01\n",
      "Epoch: 25670 mean train loss:  4.39920742e-03, bound:  3.15424323e-01\n",
      "Epoch: 25671 mean train loss:  4.39898111e-03, bound:  3.15424323e-01\n",
      "Epoch: 25672 mean train loss:  4.39874548e-03, bound:  3.15424263e-01\n",
      "Epoch: 25673 mean train loss:  4.39849077e-03, bound:  3.15424263e-01\n",
      "Epoch: 25674 mean train loss:  4.39828867e-03, bound:  3.15424263e-01\n",
      "Epoch: 25675 mean train loss:  4.39813221e-03, bound:  3.15424234e-01\n",
      "Epoch: 25676 mean train loss:  4.39787377e-03, bound:  3.15424234e-01\n",
      "Epoch: 25677 mean train loss:  4.39766049e-03, bound:  3.15424204e-01\n",
      "Epoch: 25678 mean train loss:  4.39751195e-03, bound:  3.15424144e-01\n",
      "Epoch: 25679 mean train loss:  4.39723814e-03, bound:  3.15424144e-01\n",
      "Epoch: 25680 mean train loss:  4.39703697e-03, bound:  3.15424144e-01\n",
      "Epoch: 25681 mean train loss:  4.39678133e-03, bound:  3.15424114e-01\n",
      "Epoch: 25682 mean train loss:  4.39660484e-03, bound:  3.15424114e-01\n",
      "Epoch: 25683 mean train loss:  4.39638179e-03, bound:  3.15424114e-01\n",
      "Epoch: 25684 mean train loss:  4.39612474e-03, bound:  3.15424085e-01\n",
      "Epoch: 25685 mean train loss:  4.39595059e-03, bound:  3.15424025e-01\n",
      "Epoch: 25686 mean train loss:  4.39571543e-03, bound:  3.15424025e-01\n",
      "Epoch: 25687 mean train loss:  4.39550541e-03, bound:  3.15423995e-01\n",
      "Epoch: 25688 mean train loss:  4.39529028e-03, bound:  3.15423995e-01\n",
      "Epoch: 25689 mean train loss:  4.39514406e-03, bound:  3.15423995e-01\n",
      "Epoch: 25690 mean train loss:  4.39488655e-03, bound:  3.15423995e-01\n",
      "Epoch: 25691 mean train loss:  4.39464999e-03, bound:  3.15423936e-01\n",
      "Epoch: 25692 mean train loss:  4.39441623e-03, bound:  3.15423936e-01\n",
      "Epoch: 25693 mean train loss:  4.39419877e-03, bound:  3.15423906e-01\n",
      "Epoch: 25694 mean train loss:  4.39394871e-03, bound:  3.15423906e-01\n",
      "Epoch: 25695 mean train loss:  4.39379271e-03, bound:  3.15423876e-01\n",
      "Epoch: 25696 mean train loss:  4.39355848e-03, bound:  3.15423816e-01\n",
      "Epoch: 25697 mean train loss:  4.39330423e-03, bound:  3.15423816e-01\n",
      "Epoch: 25698 mean train loss:  4.39311890e-03, bound:  3.15423816e-01\n",
      "Epoch: 25699 mean train loss:  4.39289538e-03, bound:  3.15423816e-01\n",
      "Epoch: 25700 mean train loss:  4.39264113e-03, bound:  3.15423787e-01\n",
      "Epoch: 25701 mean train loss:  4.39244602e-03, bound:  3.15423757e-01\n",
      "Epoch: 25702 mean train loss:  4.39224113e-03, bound:  3.15423757e-01\n",
      "Epoch: 25703 mean train loss:  4.39201202e-03, bound:  3.15423697e-01\n",
      "Epoch: 25704 mean train loss:  4.39173216e-03, bound:  3.15423697e-01\n",
      "Epoch: 25705 mean train loss:  4.39156964e-03, bound:  3.15423697e-01\n",
      "Epoch: 25706 mean train loss:  4.39136242e-03, bound:  3.15423667e-01\n",
      "Epoch: 25707 mean train loss:  4.39112028e-03, bound:  3.15423667e-01\n",
      "Epoch: 25708 mean train loss:  4.39089723e-03, bound:  3.15423638e-01\n",
      "Epoch: 25709 mean train loss:  4.39070957e-03, bound:  3.15423578e-01\n",
      "Epoch: 25710 mean train loss:  4.39046556e-03, bound:  3.15423578e-01\n",
      "Epoch: 25711 mean train loss:  4.39023878e-03, bound:  3.15423578e-01\n",
      "Epoch: 25712 mean train loss:  4.39000688e-03, bound:  3.15423548e-01\n",
      "Epoch: 25713 mean train loss:  4.38983086e-03, bound:  3.15423548e-01\n",
      "Epoch: 25714 mean train loss:  4.38960968e-03, bound:  3.15423518e-01\n",
      "Epoch: 25715 mean train loss:  4.38937964e-03, bound:  3.15423518e-01\n",
      "Epoch: 25716 mean train loss:  4.38922318e-03, bound:  3.15423459e-01\n",
      "Epoch: 25717 mean train loss:  4.38897591e-03, bound:  3.15423459e-01\n",
      "Epoch: 25718 mean train loss:  4.38881060e-03, bound:  3.15423459e-01\n",
      "Epoch: 25719 mean train loss:  4.38859547e-03, bound:  3.15423429e-01\n",
      "Epoch: 25720 mean train loss:  4.38833842e-03, bound:  3.15423429e-01\n",
      "Epoch: 25721 mean train loss:  4.38816566e-03, bound:  3.15423429e-01\n",
      "Epoch: 25722 mean train loss:  4.38790582e-03, bound:  3.15423369e-01\n",
      "Epoch: 25723 mean train loss:  4.38777776e-03, bound:  3.15423369e-01\n",
      "Epoch: 25724 mean train loss:  4.38748067e-03, bound:  3.15423340e-01\n",
      "Epoch: 25725 mean train loss:  4.38734144e-03, bound:  3.15423310e-01\n",
      "Epoch: 25726 mean train loss:  4.38712165e-03, bound:  3.15423310e-01\n",
      "Epoch: 25727 mean train loss:  4.38688742e-03, bound:  3.15423310e-01\n",
      "Epoch: 25728 mean train loss:  4.38669324e-03, bound:  3.15423250e-01\n",
      "Epoch: 25729 mean train loss:  4.38652141e-03, bound:  3.15423250e-01\n",
      "Epoch: 25730 mean train loss:  4.38637519e-03, bound:  3.15423250e-01\n",
      "Epoch: 25731 mean train loss:  4.38622804e-03, bound:  3.15423191e-01\n",
      "Epoch: 25732 mean train loss:  4.38612560e-03, bound:  3.15423191e-01\n",
      "Epoch: 25733 mean train loss:  4.38608974e-03, bound:  3.15423191e-01\n",
      "Epoch: 25734 mean train loss:  4.38604970e-03, bound:  3.15423191e-01\n",
      "Epoch: 25735 mean train loss:  4.38619731e-03, bound:  3.15423101e-01\n",
      "Epoch: 25736 mean train loss:  4.38643899e-03, bound:  3.15423131e-01\n",
      "Epoch: 25737 mean train loss:  4.38690744e-03, bound:  3.15423101e-01\n",
      "Epoch: 25738 mean train loss:  4.38760500e-03, bound:  3.15423101e-01\n",
      "Epoch: 25739 mean train loss:  4.38857637e-03, bound:  3.15423012e-01\n",
      "Epoch: 25740 mean train loss:  4.38998407e-03, bound:  3.15423071e-01\n",
      "Epoch: 25741 mean train loss:  4.39194683e-03, bound:  3.15422982e-01\n",
      "Epoch: 25742 mean train loss:  4.39430727e-03, bound:  3.15423012e-01\n",
      "Epoch: 25743 mean train loss:  4.39694896e-03, bound:  3.15422982e-01\n",
      "Epoch: 25744 mean train loss:  4.39941511e-03, bound:  3.15423012e-01\n",
      "Epoch: 25745 mean train loss:  4.40094713e-03, bound:  3.15422893e-01\n",
      "Epoch: 25746 mean train loss:  4.40060580e-03, bound:  3.15422982e-01\n",
      "Epoch: 25747 mean train loss:  4.39798087e-03, bound:  3.15422893e-01\n",
      "Epoch: 25748 mean train loss:  4.39346069e-03, bound:  3.15422922e-01\n",
      "Epoch: 25749 mean train loss:  4.38821409e-03, bound:  3.15422893e-01\n",
      "Epoch: 25750 mean train loss:  4.38385922e-03, bound:  3.15422893e-01\n",
      "Epoch: 25751 mean train loss:  4.38157283e-03, bound:  3.15422863e-01\n",
      "Epoch: 25752 mean train loss:  4.38169204e-03, bound:  3.15422863e-01\n",
      "Epoch: 25753 mean train loss:  4.38338658e-03, bound:  3.15422863e-01\n",
      "Epoch: 25754 mean train loss:  4.38553700e-03, bound:  3.15422773e-01\n",
      "Epoch: 25755 mean train loss:  4.38689766e-03, bound:  3.15422803e-01\n",
      "Epoch: 25756 mean train loss:  4.38672863e-03, bound:  3.15422773e-01\n",
      "Epoch: 25757 mean train loss:  4.38516354e-03, bound:  3.15422773e-01\n",
      "Epoch: 25758 mean train loss:  4.38293954e-03, bound:  3.15422744e-01\n",
      "Epoch: 25759 mean train loss:  4.38083848e-03, bound:  3.15422744e-01\n",
      "Epoch: 25760 mean train loss:  4.37963521e-03, bound:  3.15422684e-01\n",
      "Epoch: 25761 mean train loss:  4.37954906e-03, bound:  3.15422684e-01\n",
      "Epoch: 25762 mean train loss:  4.38006083e-03, bound:  3.15422684e-01\n",
      "Epoch: 25763 mean train loss:  4.38072532e-03, bound:  3.15422654e-01\n",
      "Epoch: 25764 mean train loss:  4.38104104e-03, bound:  3.15422684e-01\n",
      "Epoch: 25765 mean train loss:  4.38077701e-03, bound:  3.15422624e-01\n",
      "Epoch: 25766 mean train loss:  4.37998306e-03, bound:  3.15422624e-01\n",
      "Epoch: 25767 mean train loss:  4.37895255e-03, bound:  3.15422565e-01\n",
      "Epoch: 25768 mean train loss:  4.37815115e-03, bound:  3.15422565e-01\n",
      "Epoch: 25769 mean train loss:  4.37767385e-03, bound:  3.15422565e-01\n",
      "Epoch: 25770 mean train loss:  4.37755045e-03, bound:  3.15422505e-01\n",
      "Epoch: 25771 mean train loss:  4.37767152e-03, bound:  3.15422505e-01\n",
      "Epoch: 25772 mean train loss:  4.37779631e-03, bound:  3.15422475e-01\n",
      "Epoch: 25773 mean train loss:  4.37771156e-03, bound:  3.15422475e-01\n",
      "Epoch: 25774 mean train loss:  4.37740423e-03, bound:  3.15422446e-01\n",
      "Epoch: 25775 mean train loss:  4.37695812e-03, bound:  3.15422446e-01\n",
      "Epoch: 25776 mean train loss:  4.37643798e-03, bound:  3.15422416e-01\n",
      "Epoch: 25777 mean train loss:  4.37604962e-03, bound:  3.15422416e-01\n",
      "Epoch: 25778 mean train loss:  4.37576696e-03, bound:  3.15422416e-01\n",
      "Epoch: 25779 mean train loss:  4.37560072e-03, bound:  3.15422356e-01\n",
      "Epoch: 25780 mean train loss:  4.37555043e-03, bound:  3.15422356e-01\n",
      "Epoch: 25781 mean train loss:  4.37541865e-03, bound:  3.15422326e-01\n",
      "Epoch: 25782 mean train loss:  4.37528547e-03, bound:  3.15422326e-01\n",
      "Epoch: 25783 mean train loss:  4.37501585e-03, bound:  3.15422297e-01\n",
      "Epoch: 25784 mean train loss:  4.37468197e-03, bound:  3.15422297e-01\n",
      "Epoch: 25785 mean train loss:  4.37439978e-03, bound:  3.15422237e-01\n",
      "Epoch: 25786 mean train loss:  4.37404588e-03, bound:  3.15422237e-01\n",
      "Epoch: 25787 mean train loss:  4.37387731e-03, bound:  3.15422207e-01\n",
      "Epoch: 25788 mean train loss:  4.37365891e-03, bound:  3.15422207e-01\n",
      "Epoch: 25789 mean train loss:  4.37344005e-03, bound:  3.15422177e-01\n",
      "Epoch: 25790 mean train loss:  4.37323609e-03, bound:  3.15422177e-01\n",
      "Epoch: 25791 mean train loss:  4.37313505e-03, bound:  3.15422118e-01\n",
      "Epoch: 25792 mean train loss:  4.37294366e-03, bound:  3.15422118e-01\n",
      "Epoch: 25793 mean train loss:  4.37268987e-03, bound:  3.15422088e-01\n",
      "Epoch: 25794 mean train loss:  4.37242398e-03, bound:  3.15422088e-01\n",
      "Epoch: 25795 mean train loss:  4.37220093e-03, bound:  3.15422058e-01\n",
      "Epoch: 25796 mean train loss:  4.37198346e-03, bound:  3.15422058e-01\n",
      "Epoch: 25797 mean train loss:  4.37177997e-03, bound:  3.15422028e-01\n",
      "Epoch: 25798 mean train loss:  4.37153270e-03, bound:  3.15421999e-01\n",
      "Epoch: 25799 mean train loss:  4.37124772e-03, bound:  3.15421999e-01\n",
      "Epoch: 25800 mean train loss:  4.37107123e-03, bound:  3.15421999e-01\n",
      "Epoch: 25801 mean train loss:  4.37085098e-03, bound:  3.15421999e-01\n",
      "Epoch: 25802 mean train loss:  4.37066890e-03, bound:  3.15421939e-01\n",
      "Epoch: 25803 mean train loss:  4.37040208e-03, bound:  3.15421939e-01\n",
      "Epoch: 25804 mean train loss:  4.37023165e-03, bound:  3.15421879e-01\n",
      "Epoch: 25805 mean train loss:  4.37001837e-03, bound:  3.15421879e-01\n",
      "Epoch: 25806 mean train loss:  4.36978554e-03, bound:  3.15421879e-01\n",
      "Epoch: 25807 mean train loss:  4.36959043e-03, bound:  3.15421879e-01\n",
      "Epoch: 25808 mean train loss:  4.36935946e-03, bound:  3.15421849e-01\n",
      "Epoch: 25809 mean train loss:  4.36913641e-03, bound:  3.15421820e-01\n",
      "Epoch: 25810 mean train loss:  4.36893804e-03, bound:  3.15421760e-01\n",
      "Epoch: 25811 mean train loss:  4.36868472e-03, bound:  3.15421760e-01\n",
      "Epoch: 25812 mean train loss:  4.36848775e-03, bound:  3.15421760e-01\n",
      "Epoch: 25813 mean train loss:  4.36833175e-03, bound:  3.15421760e-01\n",
      "Epoch: 25814 mean train loss:  4.36804164e-03, bound:  3.15421730e-01\n",
      "Epoch: 25815 mean train loss:  4.36780415e-03, bound:  3.15421700e-01\n",
      "Epoch: 25816 mean train loss:  4.36767843e-03, bound:  3.15421671e-01\n",
      "Epoch: 25817 mean train loss:  4.36739437e-03, bound:  3.15421671e-01\n",
      "Epoch: 25818 mean train loss:  4.36725374e-03, bound:  3.15421641e-01\n",
      "Epoch: 25819 mean train loss:  4.36701486e-03, bound:  3.15421641e-01\n",
      "Epoch: 25820 mean train loss:  4.36678808e-03, bound:  3.15421641e-01\n",
      "Epoch: 25821 mean train loss:  4.36658133e-03, bound:  3.15421611e-01\n",
      "Epoch: 25822 mean train loss:  4.36639925e-03, bound:  3.15421581e-01\n",
      "Epoch: 25823 mean train loss:  4.36611054e-03, bound:  3.15421551e-01\n",
      "Epoch: 25824 mean train loss:  4.36590472e-03, bound:  3.15421551e-01\n",
      "Epoch: 25825 mean train loss:  4.36571008e-03, bound:  3.15421522e-01\n",
      "Epoch: 25826 mean train loss:  4.36551962e-03, bound:  3.15421522e-01\n",
      "Epoch: 25827 mean train loss:  4.36525978e-03, bound:  3.15421462e-01\n",
      "Epoch: 25828 mean train loss:  4.36504977e-03, bound:  3.15421462e-01\n",
      "Epoch: 25829 mean train loss:  4.36481414e-03, bound:  3.15421462e-01\n",
      "Epoch: 25830 mean train loss:  4.36462788e-03, bound:  3.15421432e-01\n",
      "Epoch: 25831 mean train loss:  4.36440809e-03, bound:  3.15421432e-01\n",
      "Epoch: 25832 mean train loss:  4.36419155e-03, bound:  3.15421402e-01\n",
      "Epoch: 25833 mean train loss:  4.36393730e-03, bound:  3.15421373e-01\n",
      "Epoch: 25834 mean train loss:  4.36368259e-03, bound:  3.15421373e-01\n",
      "Epoch: 25835 mean train loss:  4.36353683e-03, bound:  3.15421343e-01\n",
      "Epoch: 25836 mean train loss:  4.36328351e-03, bound:  3.15421313e-01\n",
      "Epoch: 25837 mean train loss:  4.36308654e-03, bound:  3.15421313e-01\n",
      "Epoch: 25838 mean train loss:  4.36289748e-03, bound:  3.15421283e-01\n",
      "Epoch: 25839 mean train loss:  4.36264183e-03, bound:  3.15421283e-01\n",
      "Epoch: 25840 mean train loss:  4.36245278e-03, bound:  3.15421224e-01\n",
      "Epoch: 25841 mean train loss:  4.36220458e-03, bound:  3.15421224e-01\n",
      "Epoch: 25842 mean train loss:  4.36201785e-03, bound:  3.15421194e-01\n",
      "Epoch: 25843 mean train loss:  4.36182274e-03, bound:  3.15421194e-01\n",
      "Epoch: 25844 mean train loss:  4.36161412e-03, bound:  3.15421164e-01\n",
      "Epoch: 25845 mean train loss:  4.36138501e-03, bound:  3.15421164e-01\n",
      "Epoch: 25846 mean train loss:  4.36116103e-03, bound:  3.15421134e-01\n",
      "Epoch: 25847 mean train loss:  4.36091237e-03, bound:  3.15421104e-01\n",
      "Epoch: 25848 mean train loss:  4.36074520e-03, bound:  3.15421075e-01\n",
      "Epoch: 25849 mean train loss:  4.36051935e-03, bound:  3.15421075e-01\n",
      "Epoch: 25850 mean train loss:  4.36025672e-03, bound:  3.15421075e-01\n",
      "Epoch: 25851 mean train loss:  4.36002389e-03, bound:  3.15421075e-01\n",
      "Epoch: 25852 mean train loss:  4.35985578e-03, bound:  3.15421015e-01\n",
      "Epoch: 25853 mean train loss:  4.35964717e-03, bound:  3.15421015e-01\n",
      "Epoch: 25854 mean train loss:  4.35946230e-03, bound:  3.15420955e-01\n",
      "Epoch: 25855 mean train loss:  4.35916567e-03, bound:  3.15420955e-01\n",
      "Epoch: 25856 mean train loss:  4.35902365e-03, bound:  3.15420955e-01\n",
      "Epoch: 25857 mean train loss:  4.35884669e-03, bound:  3.15420955e-01\n",
      "Epoch: 25858 mean train loss:  4.35867440e-03, bound:  3.15420896e-01\n",
      "Epoch: 25859 mean train loss:  4.35847463e-03, bound:  3.15420896e-01\n",
      "Epoch: 25860 mean train loss:  4.35828278e-03, bound:  3.15420866e-01\n",
      "Epoch: 25861 mean train loss:  4.35816171e-03, bound:  3.15420866e-01\n",
      "Epoch: 25862 mean train loss:  4.35800478e-03, bound:  3.15420836e-01\n",
      "Epoch: 25863 mean train loss:  4.35791397e-03, bound:  3.15420836e-01\n",
      "Epoch: 25864 mean train loss:  4.35788557e-03, bound:  3.15420777e-01\n",
      "Epoch: 25865 mean train loss:  4.35796939e-03, bound:  3.15420777e-01\n",
      "Epoch: 25866 mean train loss:  4.35816078e-03, bound:  3.15420747e-01\n",
      "Epoch: 25867 mean train loss:  4.35846951e-03, bound:  3.15420747e-01\n",
      "Epoch: 25868 mean train loss:  4.35918476e-03, bound:  3.15420717e-01\n",
      "Epoch: 25869 mean train loss:  4.36027441e-03, bound:  3.15420717e-01\n",
      "Epoch: 25870 mean train loss:  4.36192006e-03, bound:  3.15420657e-01\n",
      "Epoch: 25871 mean train loss:  4.36424837e-03, bound:  3.15420717e-01\n",
      "Epoch: 25872 mean train loss:  4.36745770e-03, bound:  3.15420628e-01\n",
      "Epoch: 25873 mean train loss:  4.37147124e-03, bound:  3.15420657e-01\n",
      "Epoch: 25874 mean train loss:  4.37581772e-03, bound:  3.15420538e-01\n",
      "Epoch: 25875 mean train loss:  4.37963288e-03, bound:  3.15420657e-01\n",
      "Epoch: 25876 mean train loss:  4.38109087e-03, bound:  3.15420538e-01\n",
      "Epoch: 25877 mean train loss:  4.37873974e-03, bound:  3.15420628e-01\n",
      "Epoch: 25878 mean train loss:  4.37230058e-03, bound:  3.15420538e-01\n",
      "Epoch: 25879 mean train loss:  4.36377106e-03, bound:  3.15420598e-01\n",
      "Epoch: 25880 mean train loss:  4.35657799e-03, bound:  3.15420508e-01\n",
      "Epoch: 25881 mean train loss:  4.35356144e-03, bound:  3.15420508e-01\n",
      "Epoch: 25882 mean train loss:  4.35502641e-03, bound:  3.15420508e-01\n",
      "Epoch: 25883 mean train loss:  4.35892632e-03, bound:  3.15420449e-01\n",
      "Epoch: 25884 mean train loss:  4.36220923e-03, bound:  3.15420508e-01\n",
      "Epoch: 25885 mean train loss:  4.36265534e-03, bound:  3.15420419e-01\n",
      "Epoch: 25886 mean train loss:  4.35991026e-03, bound:  3.15420479e-01\n",
      "Epoch: 25887 mean train loss:  4.35590791e-03, bound:  3.15420389e-01\n",
      "Epoch: 25888 mean train loss:  4.35273349e-03, bound:  3.15420389e-01\n",
      "Epoch: 25889 mean train loss:  4.35200613e-03, bound:  3.15420389e-01\n",
      "Epoch: 25890 mean train loss:  4.35332721e-03, bound:  3.15420330e-01\n",
      "Epoch: 25891 mean train loss:  4.35513398e-03, bound:  3.15420330e-01\n",
      "Epoch: 25892 mean train loss:  4.35597356e-03, bound:  3.15420330e-01\n",
      "Epoch: 25893 mean train loss:  4.35512094e-03, bound:  3.15420330e-01\n",
      "Epoch: 25894 mean train loss:  4.35320940e-03, bound:  3.15420270e-01\n",
      "Epoch: 25895 mean train loss:  4.35139984e-03, bound:  3.15420270e-01\n",
      "Epoch: 25896 mean train loss:  4.35042568e-03, bound:  3.15420270e-01\n",
      "Epoch: 25897 mean train loss:  4.35072230e-03, bound:  3.15420270e-01\n",
      "Epoch: 25898 mean train loss:  4.35153954e-03, bound:  3.15420270e-01\n",
      "Epoch: 25899 mean train loss:  4.35204897e-03, bound:  3.15420210e-01\n",
      "Epoch: 25900 mean train loss:  4.35173325e-03, bound:  3.15420210e-01\n",
      "Epoch: 25901 mean train loss:  4.35081730e-03, bound:  3.15420181e-01\n",
      "Epoch: 25902 mean train loss:  4.34973603e-03, bound:  3.15420181e-01\n",
      "Epoch: 25903 mean train loss:  4.34899796e-03, bound:  3.15420151e-01\n",
      "Epoch: 25904 mean train loss:  4.34889738e-03, bound:  3.15420091e-01\n",
      "Epoch: 25905 mean train loss:  4.34918469e-03, bound:  3.15420091e-01\n",
      "Epoch: 25906 mean train loss:  4.34949668e-03, bound:  3.15420061e-01\n",
      "Epoch: 25907 mean train loss:  4.34931880e-03, bound:  3.15420061e-01\n",
      "Epoch: 25908 mean train loss:  4.34889365e-03, bound:  3.15420061e-01\n",
      "Epoch: 25909 mean train loss:  4.34815791e-03, bound:  3.15420032e-01\n",
      "Epoch: 25910 mean train loss:  4.34763310e-03, bound:  3.15419972e-01\n",
      "Epoch: 25911 mean train loss:  4.34732251e-03, bound:  3.15419972e-01\n",
      "Epoch: 25912 mean train loss:  4.34733881e-03, bound:  3.15419972e-01\n",
      "Epoch: 25913 mean train loss:  4.34752181e-03, bound:  3.15419942e-01\n",
      "Epoch: 25914 mean train loss:  4.34744917e-03, bound:  3.15419942e-01\n",
      "Epoch: 25915 mean train loss:  4.34718234e-03, bound:  3.15419883e-01\n",
      "Epoch: 25916 mean train loss:  4.34676697e-03, bound:  3.15419883e-01\n",
      "Epoch: 25917 mean train loss:  4.34625102e-03, bound:  3.15419883e-01\n",
      "Epoch: 25918 mean train loss:  4.34589945e-03, bound:  3.15419853e-01\n",
      "Epoch: 25919 mean train loss:  4.34567826e-03, bound:  3.15419823e-01\n",
      "Epoch: 25920 mean train loss:  4.34558140e-03, bound:  3.15419823e-01\n",
      "Epoch: 25921 mean train loss:  4.34558559e-03, bound:  3.15419823e-01\n",
      "Epoch: 25922 mean train loss:  4.34551295e-03, bound:  3.15419763e-01\n",
      "Epoch: 25923 mean train loss:  4.34523541e-03, bound:  3.15419763e-01\n",
      "Epoch: 25924 mean train loss:  4.34489734e-03, bound:  3.15419763e-01\n",
      "Epoch: 25925 mean train loss:  4.34460910e-03, bound:  3.15419763e-01\n",
      "Epoch: 25926 mean train loss:  4.34427662e-03, bound:  3.15419704e-01\n",
      "Epoch: 25927 mean train loss:  4.34403773e-03, bound:  3.15419704e-01\n",
      "Epoch: 25928 mean train loss:  4.34379745e-03, bound:  3.15419644e-01\n",
      "Epoch: 25929 mean train loss:  4.34366427e-03, bound:  3.15419644e-01\n",
      "Epoch: 25930 mean train loss:  4.34347754e-03, bound:  3.15419644e-01\n",
      "Epoch: 25931 mean train loss:  4.34332667e-03, bound:  3.15419644e-01\n",
      "Epoch: 25932 mean train loss:  4.34309291e-03, bound:  3.15419644e-01\n",
      "Epoch: 25933 mean train loss:  4.34279349e-03, bound:  3.15419585e-01\n",
      "Epoch: 25934 mean train loss:  4.34261048e-03, bound:  3.15419585e-01\n",
      "Epoch: 25935 mean train loss:  4.34239767e-03, bound:  3.15419525e-01\n",
      "Epoch: 25936 mean train loss:  4.34215087e-03, bound:  3.15419525e-01\n",
      "Epoch: 25937 mean train loss:  4.34197346e-03, bound:  3.15419525e-01\n",
      "Epoch: 25938 mean train loss:  4.34176670e-03, bound:  3.15419525e-01\n",
      "Epoch: 25939 mean train loss:  4.34153667e-03, bound:  3.15419495e-01\n",
      "Epoch: 25940 mean train loss:  4.34133783e-03, bound:  3.15419495e-01\n",
      "Epoch: 25941 mean train loss:  4.34111664e-03, bound:  3.15419436e-01\n",
      "Epoch: 25942 mean train loss:  4.34093876e-03, bound:  3.15419436e-01\n",
      "Epoch: 25943 mean train loss:  4.34071058e-03, bound:  3.15419406e-01\n",
      "Epoch: 25944 mean train loss:  4.34049871e-03, bound:  3.15419406e-01\n",
      "Epoch: 25945 mean train loss:  4.34026262e-03, bound:  3.15419376e-01\n",
      "Epoch: 25946 mean train loss:  4.34003212e-03, bound:  3.15419376e-01\n",
      "Epoch: 25947 mean train loss:  4.33984585e-03, bound:  3.15419376e-01\n",
      "Epoch: 25948 mean train loss:  4.33962652e-03, bound:  3.15419316e-01\n",
      "Epoch: 25949 mean train loss:  4.33942629e-03, bound:  3.15419316e-01\n",
      "Epoch: 25950 mean train loss:  4.33918415e-03, bound:  3.15419286e-01\n",
      "Epoch: 25951 mean train loss:  4.33900161e-03, bound:  3.15419286e-01\n",
      "Epoch: 25952 mean train loss:  4.33883537e-03, bound:  3.15419257e-01\n",
      "Epoch: 25953 mean train loss:  4.33858950e-03, bound:  3.15419257e-01\n",
      "Epoch: 25954 mean train loss:  4.33839718e-03, bound:  3.15419257e-01\n",
      "Epoch: 25955 mean train loss:  4.33819927e-03, bound:  3.15419197e-01\n",
      "Epoch: 25956 mean train loss:  4.33798647e-03, bound:  3.15419197e-01\n",
      "Epoch: 25957 mean train loss:  4.33778437e-03, bound:  3.15419167e-01\n",
      "Epoch: 25958 mean train loss:  4.33756085e-03, bound:  3.15419137e-01\n",
      "Epoch: 25959 mean train loss:  4.33734059e-03, bound:  3.15419137e-01\n",
      "Epoch: 25960 mean train loss:  4.33712825e-03, bound:  3.15419137e-01\n",
      "Epoch: 25961 mean train loss:  4.33690520e-03, bound:  3.15419078e-01\n",
      "Epoch: 25962 mean train loss:  4.33669193e-03, bound:  3.15419078e-01\n",
      "Epoch: 25963 mean train loss:  4.33648424e-03, bound:  3.15419048e-01\n",
      "Epoch: 25964 mean train loss:  4.33627237e-03, bound:  3.15419048e-01\n",
      "Epoch: 25965 mean train loss:  4.33610007e-03, bound:  3.15419018e-01\n",
      "Epoch: 25966 mean train loss:  4.33587935e-03, bound:  3.15419018e-01\n",
      "Epoch: 25967 mean train loss:  4.33567353e-03, bound:  3.15418959e-01\n",
      "Epoch: 25968 mean train loss:  4.33543883e-03, bound:  3.15418959e-01\n",
      "Epoch: 25969 mean train loss:  4.33522742e-03, bound:  3.15418929e-01\n",
      "Epoch: 25970 mean train loss:  4.33505001e-03, bound:  3.15418929e-01\n",
      "Epoch: 25971 mean train loss:  4.33482369e-03, bound:  3.15418929e-01\n",
      "Epoch: 25972 mean train loss:  4.33467468e-03, bound:  3.15418929e-01\n",
      "Epoch: 25973 mean train loss:  4.33443580e-03, bound:  3.15418929e-01\n",
      "Epoch: 25974 mean train loss:  4.33421042e-03, bound:  3.15418839e-01\n",
      "Epoch: 25975 mean train loss:  4.33400972e-03, bound:  3.15418839e-01\n",
      "Epoch: 25976 mean train loss:  4.33377596e-03, bound:  3.15418839e-01\n",
      "Epoch: 25977 mean train loss:  4.33357060e-03, bound:  3.15418839e-01\n",
      "Epoch: 25978 mean train loss:  4.33334941e-03, bound:  3.15418810e-01\n",
      "Epoch: 25979 mean train loss:  4.33313381e-03, bound:  3.15418810e-01\n",
      "Epoch: 25980 mean train loss:  4.33292566e-03, bound:  3.15418720e-01\n",
      "Epoch: 25981 mean train loss:  4.33264440e-03, bound:  3.15418720e-01\n",
      "Epoch: 25982 mean train loss:  4.33252100e-03, bound:  3.15418720e-01\n",
      "Epoch: 25983 mean train loss:  4.33232961e-03, bound:  3.15418720e-01\n",
      "Epoch: 25984 mean train loss:  4.33206139e-03, bound:  3.15418690e-01\n",
      "Epoch: 25985 mean train loss:  4.33185045e-03, bound:  3.15418690e-01\n",
      "Epoch: 25986 mean train loss:  4.33159806e-03, bound:  3.15418601e-01\n",
      "Epoch: 25987 mean train loss:  4.33143834e-03, bound:  3.15418601e-01\n",
      "Epoch: 25988 mean train loss:  4.33126278e-03, bound:  3.15418601e-01\n",
      "Epoch: 25989 mean train loss:  4.33099782e-03, bound:  3.15418601e-01\n",
      "Epoch: 25990 mean train loss:  4.33081295e-03, bound:  3.15418541e-01\n",
      "Epoch: 25991 mean train loss:  4.33058292e-03, bound:  3.15418541e-01\n",
      "Epoch: 25992 mean train loss:  4.33039293e-03, bound:  3.15418512e-01\n",
      "Epoch: 25993 mean train loss:  4.33015171e-03, bound:  3.15418512e-01\n",
      "Epoch: 25994 mean train loss:  4.32998547e-03, bound:  3.15418512e-01\n",
      "Epoch: 25995 mean train loss:  4.32969397e-03, bound:  3.15418482e-01\n",
      "Epoch: 25996 mean train loss:  4.32949793e-03, bound:  3.15418452e-01\n",
      "Epoch: 25997 mean train loss:  4.32928139e-03, bound:  3.15418422e-01\n",
      "Epoch: 25998 mean train loss:  4.32909233e-03, bound:  3.15418422e-01\n",
      "Epoch: 25999 mean train loss:  4.32886276e-03, bound:  3.15418392e-01\n",
      "Epoch: 26000 mean train loss:  4.32870584e-03, bound:  3.15418392e-01\n",
      "Epoch: 26001 mean train loss:  4.32848791e-03, bound:  3.15418363e-01\n",
      "Epoch: 26002 mean train loss:  4.32833051e-03, bound:  3.15418363e-01\n",
      "Epoch: 26003 mean train loss:  4.32820944e-03, bound:  3.15418333e-01\n",
      "Epoch: 26004 mean train loss:  4.32796450e-03, bound:  3.15418333e-01\n",
      "Epoch: 26005 mean train loss:  4.32787137e-03, bound:  3.15418303e-01\n",
      "Epoch: 26006 mean train loss:  4.32774937e-03, bound:  3.15418273e-01\n",
      "Epoch: 26007 mean train loss:  4.32763994e-03, bound:  3.15418273e-01\n",
      "Epoch: 26008 mean train loss:  4.32757474e-03, bound:  3.15418243e-01\n",
      "Epoch: 26009 mean train loss:  4.32754494e-03, bound:  3.15418243e-01\n",
      "Epoch: 26010 mean train loss:  4.32746625e-03, bound:  3.15418243e-01\n",
      "Epoch: 26011 mean train loss:  4.32746485e-03, bound:  3.15418184e-01\n",
      "Epoch: 26012 mean train loss:  4.32753051e-03, bound:  3.15418154e-01\n",
      "Epoch: 26013 mean train loss:  4.32764925e-03, bound:  3.15418154e-01\n",
      "Epoch: 26014 mean train loss:  4.32785926e-03, bound:  3.15418124e-01\n",
      "Epoch: 26015 mean train loss:  4.32807580e-03, bound:  3.15418124e-01\n",
      "Epoch: 26016 mean train loss:  4.32834169e-03, bound:  3.15418065e-01\n",
      "Epoch: 26017 mean train loss:  4.32876730e-03, bound:  3.15418094e-01\n",
      "Epoch: 26018 mean train loss:  4.32917848e-03, bound:  3.15418035e-01\n",
      "Epoch: 26019 mean train loss:  4.32970934e-03, bound:  3.15418035e-01\n",
      "Epoch: 26020 mean train loss:  4.33029793e-03, bound:  3.15418005e-01\n",
      "Epoch: 26021 mean train loss:  4.33081388e-03, bound:  3.15418035e-01\n",
      "Epoch: 26022 mean train loss:  4.33108723e-03, bound:  3.15417975e-01\n",
      "Epoch: 26023 mean train loss:  4.33110958e-03, bound:  3.15418035e-01\n",
      "Epoch: 26024 mean train loss:  4.33072215e-03, bound:  3.15417945e-01\n",
      "Epoch: 26025 mean train loss:  4.32987930e-03, bound:  3.15417945e-01\n",
      "Epoch: 26026 mean train loss:  4.32859501e-03, bound:  3.15417916e-01\n",
      "Epoch: 26027 mean train loss:  4.32710769e-03, bound:  3.15417945e-01\n",
      "Epoch: 26028 mean train loss:  4.32559941e-03, bound:  3.15417856e-01\n",
      "Epoch: 26029 mean train loss:  4.32411442e-03, bound:  3.15417886e-01\n",
      "Epoch: 26030 mean train loss:  4.32298332e-03, bound:  3.15417826e-01\n",
      "Epoch: 26031 mean train loss:  4.32224665e-03, bound:  3.15417826e-01\n",
      "Epoch: 26032 mean train loss:  4.32183314e-03, bound:  3.15417826e-01\n",
      "Epoch: 26033 mean train loss:  4.32180054e-03, bound:  3.15417796e-01\n",
      "Epoch: 26034 mean train loss:  4.32195794e-03, bound:  3.15417796e-01\n",
      "Epoch: 26035 mean train loss:  4.32210555e-03, bound:  3.15417767e-01\n",
      "Epoch: 26036 mean train loss:  4.32232674e-03, bound:  3.15417737e-01\n",
      "Epoch: 26037 mean train loss:  4.32233745e-03, bound:  3.15417707e-01\n",
      "Epoch: 26038 mean train loss:  4.32223501e-03, bound:  3.15417707e-01\n",
      "Epoch: 26039 mean train loss:  4.32196120e-03, bound:  3.15417677e-01\n",
      "Epoch: 26040 mean train loss:  4.32158215e-03, bound:  3.15417677e-01\n",
      "Epoch: 26041 mean train loss:  4.32111323e-03, bound:  3.15417618e-01\n",
      "Epoch: 26042 mean train loss:  4.32068855e-03, bound:  3.15417618e-01\n",
      "Epoch: 26043 mean train loss:  4.32019448e-03, bound:  3.15417618e-01\n",
      "Epoch: 26044 mean train loss:  4.31979680e-03, bound:  3.15417588e-01\n",
      "Epoch: 26045 mean train loss:  4.31936979e-03, bound:  3.15417588e-01\n",
      "Epoch: 26046 mean train loss:  4.31906991e-03, bound:  3.15417558e-01\n",
      "Epoch: 26047 mean train loss:  4.31876583e-03, bound:  3.15417558e-01\n",
      "Epoch: 26048 mean train loss:  4.31849295e-03, bound:  3.15417498e-01\n",
      "Epoch: 26049 mean train loss:  4.31828573e-03, bound:  3.15417498e-01\n",
      "Epoch: 26050 mean train loss:  4.31809528e-03, bound:  3.15417469e-01\n",
      "Epoch: 26051 mean train loss:  4.31796303e-03, bound:  3.15417469e-01\n",
      "Epoch: 26052 mean train loss:  4.31770552e-03, bound:  3.15417439e-01\n",
      "Epoch: 26053 mean train loss:  4.31760401e-03, bound:  3.15417439e-01\n",
      "Epoch: 26054 mean train loss:  4.31741215e-03, bound:  3.15417409e-01\n",
      "Epoch: 26055 mean train loss:  4.31722077e-03, bound:  3.15417409e-01\n",
      "Epoch: 26056 mean train loss:  4.31695161e-03, bound:  3.15417349e-01\n",
      "Epoch: 26057 mean train loss:  4.31679143e-03, bound:  3.15417349e-01\n",
      "Epoch: 26058 mean train loss:  4.31652181e-03, bound:  3.15417320e-01\n",
      "Epoch: 26059 mean train loss:  4.31635743e-03, bound:  3.15417320e-01\n",
      "Epoch: 26060 mean train loss:  4.31613671e-03, bound:  3.15417290e-01\n",
      "Epoch: 26061 mean train loss:  4.31589456e-03, bound:  3.15417290e-01\n",
      "Epoch: 26062 mean train loss:  4.31568269e-03, bound:  3.15417260e-01\n",
      "Epoch: 26063 mean train loss:  4.31547547e-03, bound:  3.15417260e-01\n",
      "Epoch: 26064 mean train loss:  4.31520026e-03, bound:  3.15417230e-01\n",
      "Epoch: 26065 mean train loss:  4.31500887e-03, bound:  3.15417230e-01\n",
      "Epoch: 26066 mean train loss:  4.31482261e-03, bound:  3.15417171e-01\n",
      "Epoch: 26067 mean train loss:  4.31460328e-03, bound:  3.15417171e-01\n",
      "Epoch: 26068 mean train loss:  4.31435741e-03, bound:  3.15417171e-01\n",
      "Epoch: 26069 mean train loss:  4.31421353e-03, bound:  3.15417141e-01\n",
      "Epoch: 26070 mean train loss:  4.31396347e-03, bound:  3.15417141e-01\n",
      "Epoch: 26071 mean train loss:  4.31369804e-03, bound:  3.15417111e-01\n",
      "Epoch: 26072 mean train loss:  4.31347312e-03, bound:  3.15417051e-01\n",
      "Epoch: 26073 mean train loss:  4.31330875e-03, bound:  3.15417051e-01\n",
      "Epoch: 26074 mean train loss:  4.31306986e-03, bound:  3.15417022e-01\n",
      "Epoch: 26075 mean train loss:  4.31286404e-03, bound:  3.15417022e-01\n",
      "Epoch: 26076 mean train loss:  4.31262283e-03, bound:  3.15417022e-01\n",
      "Epoch: 26077 mean train loss:  4.31241561e-03, bound:  3.15416992e-01\n",
      "Epoch: 26078 mean train loss:  4.31232573e-03, bound:  3.15416992e-01\n",
      "Epoch: 26079 mean train loss:  4.31208732e-03, bound:  3.15416932e-01\n",
      "Epoch: 26080 mean train loss:  4.31200862e-03, bound:  3.15416902e-01\n",
      "Epoch: 26081 mean train loss:  4.31190804e-03, bound:  3.15416902e-01\n",
      "Epoch: 26082 mean train loss:  4.31187404e-03, bound:  3.15416902e-01\n",
      "Epoch: 26083 mean train loss:  4.31191875e-03, bound:  3.15416873e-01\n",
      "Epoch: 26084 mean train loss:  4.31195507e-03, bound:  3.15416843e-01\n",
      "Epoch: 26085 mean train loss:  4.31208452e-03, bound:  3.15416873e-01\n",
      "Epoch: 26086 mean train loss:  4.31232294e-03, bound:  3.15416813e-01\n",
      "Epoch: 26087 mean train loss:  4.31258511e-03, bound:  3.15416813e-01\n",
      "Epoch: 26088 mean train loss:  4.31304006e-03, bound:  3.15416783e-01\n",
      "Epoch: 26089 mean train loss:  4.31373157e-03, bound:  3.15416783e-01\n",
      "Epoch: 26090 mean train loss:  4.31476254e-03, bound:  3.15416723e-01\n",
      "Epoch: 26091 mean train loss:  4.31611901e-03, bound:  3.15416723e-01\n",
      "Epoch: 26092 mean train loss:  4.31773392e-03, bound:  3.15416694e-01\n",
      "Epoch: 26093 mean train loss:  4.31969063e-03, bound:  3.15416694e-01\n",
      "Epoch: 26094 mean train loss:  4.32170229e-03, bound:  3.15416664e-01\n",
      "Epoch: 26095 mean train loss:  4.32335725e-03, bound:  3.15416694e-01\n",
      "Epoch: 26096 mean train loss:  4.32398776e-03, bound:  3.15416604e-01\n",
      "Epoch: 26097 mean train loss:  4.32310440e-03, bound:  3.15416694e-01\n",
      "Epoch: 26098 mean train loss:  4.32066387e-03, bound:  3.15416604e-01\n",
      "Epoch: 26099 mean train loss:  4.31679841e-03, bound:  3.15416604e-01\n",
      "Epoch: 26100 mean train loss:  4.31256415e-03, bound:  3.15416574e-01\n",
      "Epoch: 26101 mean train loss:  4.30911733e-03, bound:  3.15416604e-01\n",
      "Epoch: 26102 mean train loss:  4.30724025e-03, bound:  3.15416574e-01\n",
      "Epoch: 26103 mean train loss:  4.30709450e-03, bound:  3.15416485e-01\n",
      "Epoch: 26104 mean train loss:  4.30811942e-03, bound:  3.15416485e-01\n",
      "Epoch: 26105 mean train loss:  4.30968031e-03, bound:  3.15416485e-01\n",
      "Epoch: 26106 mean train loss:  4.31098044e-03, bound:  3.15416485e-01\n",
      "Epoch: 26107 mean train loss:  4.31139348e-03, bound:  3.15416455e-01\n",
      "Epoch: 26108 mean train loss:  4.31067077e-03, bound:  3.15416485e-01\n",
      "Epoch: 26109 mean train loss:  4.30903677e-03, bound:  3.15416366e-01\n",
      "Epoch: 26110 mean train loss:  4.30723140e-03, bound:  3.15416425e-01\n",
      "Epoch: 26111 mean train loss:  4.30568494e-03, bound:  3.15416366e-01\n",
      "Epoch: 26112 mean train loss:  4.30502929e-03, bound:  3.15416366e-01\n",
      "Epoch: 26113 mean train loss:  4.30508284e-03, bound:  3.15416366e-01\n",
      "Epoch: 26114 mean train loss:  4.30554198e-03, bound:  3.15416336e-01\n",
      "Epoch: 26115 mean train loss:  4.30604490e-03, bound:  3.15416336e-01\n",
      "Epoch: 26116 mean train loss:  4.30621346e-03, bound:  3.15416276e-01\n",
      "Epoch: 26117 mean train loss:  4.30588238e-03, bound:  3.15416276e-01\n",
      "Epoch: 26118 mean train loss:  4.30521742e-03, bound:  3.15416247e-01\n",
      "Epoch: 26119 mean train loss:  4.30439785e-03, bound:  3.15416247e-01\n",
      "Epoch: 26120 mean train loss:  4.30363370e-03, bound:  3.15416217e-01\n",
      "Epoch: 26121 mean train loss:  4.30324348e-03, bound:  3.15416217e-01\n",
      "Epoch: 26122 mean train loss:  4.30302974e-03, bound:  3.15416217e-01\n",
      "Epoch: 26123 mean train loss:  4.30304185e-03, bound:  3.15416157e-01\n",
      "Epoch: 26124 mean train loss:  4.30313218e-03, bound:  3.15416157e-01\n",
      "Epoch: 26125 mean train loss:  4.30310704e-03, bound:  3.15416127e-01\n",
      "Epoch: 26126 mean train loss:  4.30289702e-03, bound:  3.15416127e-01\n",
      "Epoch: 26127 mean train loss:  4.30255849e-03, bound:  3.15416098e-01\n",
      "Epoch: 26128 mean train loss:  4.30224091e-03, bound:  3.15416098e-01\n",
      "Epoch: 26129 mean train loss:  4.30174684e-03, bound:  3.15416038e-01\n",
      "Epoch: 26130 mean train loss:  4.30142460e-03, bound:  3.15416038e-01\n",
      "Epoch: 26131 mean train loss:  4.30109492e-03, bound:  3.15416008e-01\n",
      "Epoch: 26132 mean train loss:  4.30093100e-03, bound:  3.15416008e-01\n",
      "Epoch: 26133 mean train loss:  4.30077780e-03, bound:  3.15416008e-01\n",
      "Epoch: 26134 mean train loss:  4.30067070e-03, bound:  3.15416008e-01\n",
      "Epoch: 26135 mean train loss:  4.30053193e-03, bound:  3.15415978e-01\n",
      "Epoch: 26136 mean train loss:  4.30037454e-03, bound:  3.15415919e-01\n",
      "Epoch: 26137 mean train loss:  4.30015987e-03, bound:  3.15415919e-01\n",
      "Epoch: 26138 mean train loss:  4.29990841e-03, bound:  3.15415889e-01\n",
      "Epoch: 26139 mean train loss:  4.29967372e-03, bound:  3.15415889e-01\n",
      "Epoch: 26140 mean train loss:  4.29941760e-03, bound:  3.15415889e-01\n",
      "Epoch: 26141 mean train loss:  4.29909443e-03, bound:  3.15415829e-01\n",
      "Epoch: 26142 mean train loss:  4.29887092e-03, bound:  3.15415800e-01\n",
      "Epoch: 26143 mean train loss:  4.29864787e-03, bound:  3.15415800e-01\n",
      "Epoch: 26144 mean train loss:  4.29846346e-03, bound:  3.15415800e-01\n",
      "Epoch: 26145 mean train loss:  4.29836893e-03, bound:  3.15415800e-01\n",
      "Epoch: 26146 mean train loss:  4.29816032e-03, bound:  3.15415770e-01\n",
      "Epoch: 26147 mean train loss:  4.29796288e-03, bound:  3.15415770e-01\n",
      "Epoch: 26148 mean train loss:  4.29786183e-03, bound:  3.15415710e-01\n",
      "Epoch: 26149 mean train loss:  4.29762201e-03, bound:  3.15415680e-01\n",
      "Epoch: 26150 mean train loss:  4.29737195e-03, bound:  3.15415680e-01\n",
      "Epoch: 26151 mean train loss:  4.29712003e-03, bound:  3.15415680e-01\n",
      "Epoch: 26152 mean train loss:  4.29689419e-03, bound:  3.15415651e-01\n",
      "Epoch: 26153 mean train loss:  4.29666275e-03, bound:  3.15415651e-01\n",
      "Epoch: 26154 mean train loss:  4.29642759e-03, bound:  3.15415591e-01\n",
      "Epoch: 26155 mean train loss:  4.29619662e-03, bound:  3.15415591e-01\n",
      "Epoch: 26156 mean train loss:  4.29598615e-03, bound:  3.15415591e-01\n",
      "Epoch: 26157 mean train loss:  4.29575937e-03, bound:  3.15415561e-01\n",
      "Epoch: 26158 mean train loss:  4.29553632e-03, bound:  3.15415502e-01\n",
      "Epoch: 26159 mean train loss:  4.29532491e-03, bound:  3.15415502e-01\n",
      "Epoch: 26160 mean train loss:  4.29514749e-03, bound:  3.15415472e-01\n",
      "Epoch: 26161 mean train loss:  4.29490022e-03, bound:  3.15415472e-01\n",
      "Epoch: 26162 mean train loss:  4.29468928e-03, bound:  3.15415472e-01\n",
      "Epoch: 26163 mean train loss:  4.29448765e-03, bound:  3.15415442e-01\n",
      "Epoch: 26164 mean train loss:  4.29431954e-03, bound:  3.15415442e-01\n",
      "Epoch: 26165 mean train loss:  4.29407414e-03, bound:  3.15415382e-01\n",
      "Epoch: 26166 mean train loss:  4.29384410e-03, bound:  3.15415382e-01\n",
      "Epoch: 26167 mean train loss:  4.29366855e-03, bound:  3.15415353e-01\n",
      "Epoch: 26168 mean train loss:  4.29347623e-03, bound:  3.15415353e-01\n",
      "Epoch: 26169 mean train loss:  4.29322943e-03, bound:  3.15415323e-01\n",
      "Epoch: 26170 mean train loss:  4.29298729e-03, bound:  3.15415323e-01\n",
      "Epoch: 26171 mean train loss:  4.29277914e-03, bound:  3.15415263e-01\n",
      "Epoch: 26172 mean train loss:  4.29263199e-03, bound:  3.15415263e-01\n",
      "Epoch: 26173 mean train loss:  4.29242337e-03, bound:  3.15415233e-01\n",
      "Epoch: 26174 mean train loss:  4.29217145e-03, bound:  3.15415233e-01\n",
      "Epoch: 26175 mean train loss:  4.29200986e-03, bound:  3.15415204e-01\n",
      "Epoch: 26176 mean train loss:  4.29174490e-03, bound:  3.15415204e-01\n",
      "Epoch: 26177 mean train loss:  4.29157773e-03, bound:  3.15415204e-01\n",
      "Epoch: 26178 mean train loss:  4.29135421e-03, bound:  3.15415204e-01\n",
      "Epoch: 26179 mean train loss:  4.29113349e-03, bound:  3.15415144e-01\n",
      "Epoch: 26180 mean train loss:  4.29092767e-03, bound:  3.15415114e-01\n",
      "Epoch: 26181 mean train loss:  4.29072790e-03, bound:  3.15415114e-01\n",
      "Epoch: 26182 mean train loss:  4.29052254e-03, bound:  3.15415084e-01\n",
      "Epoch: 26183 mean train loss:  4.29030275e-03, bound:  3.15415084e-01\n",
      "Epoch: 26184 mean train loss:  4.29009739e-03, bound:  3.15415055e-01\n",
      "Epoch: 26185 mean train loss:  4.28984733e-03, bound:  3.15415025e-01\n",
      "Epoch: 26186 mean train loss:  4.28968854e-03, bound:  3.15415025e-01\n",
      "Epoch: 26187 mean train loss:  4.28947108e-03, bound:  3.15415025e-01\n",
      "Epoch: 26188 mean train loss:  4.28927131e-03, bound:  3.15414965e-01\n",
      "Epoch: 26189 mean train loss:  4.28902265e-03, bound:  3.15414965e-01\n",
      "Epoch: 26190 mean train loss:  4.28885687e-03, bound:  3.15414935e-01\n",
      "Epoch: 26191 mean train loss:  4.28857887e-03, bound:  3.15414935e-01\n",
      "Epoch: 26192 mean train loss:  4.28838888e-03, bound:  3.15414906e-01\n",
      "Epoch: 26193 mean train loss:  4.28822637e-03, bound:  3.15414906e-01\n",
      "Epoch: 26194 mean train loss:  4.28799959e-03, bound:  3.15414876e-01\n",
      "Epoch: 26195 mean train loss:  4.28773789e-03, bound:  3.15414876e-01\n",
      "Epoch: 26196 mean train loss:  4.28755162e-03, bound:  3.15414816e-01\n",
      "Epoch: 26197 mean train loss:  4.28733276e-03, bound:  3.15414816e-01\n",
      "Epoch: 26198 mean train loss:  4.28715814e-03, bound:  3.15414786e-01\n",
      "Epoch: 26199 mean train loss:  4.28696210e-03, bound:  3.15414786e-01\n",
      "Epoch: 26200 mean train loss:  4.28675348e-03, bound:  3.15414786e-01\n",
      "Epoch: 26201 mean train loss:  4.28654300e-03, bound:  3.15414757e-01\n",
      "Epoch: 26202 mean train loss:  4.28631110e-03, bound:  3.15414757e-01\n",
      "Epoch: 26203 mean train loss:  4.28610854e-03, bound:  3.15414697e-01\n",
      "Epoch: 26204 mean train loss:  4.28589387e-03, bound:  3.15414697e-01\n",
      "Epoch: 26205 mean train loss:  4.28566756e-03, bound:  3.15414667e-01\n",
      "Epoch: 26206 mean train loss:  4.28544218e-03, bound:  3.15414667e-01\n",
      "Epoch: 26207 mean train loss:  4.28529363e-03, bound:  3.15414637e-01\n",
      "Epoch: 26208 mean train loss:  4.28501563e-03, bound:  3.15414637e-01\n",
      "Epoch: 26209 mean train loss:  4.28476650e-03, bound:  3.15414578e-01\n",
      "Epoch: 26210 mean train loss:  4.28465055e-03, bound:  3.15414578e-01\n",
      "Epoch: 26211 mean train loss:  4.28444147e-03, bound:  3.15414548e-01\n",
      "Epoch: 26212 mean train loss:  4.28419048e-03, bound:  3.15414548e-01\n",
      "Epoch: 26213 mean train loss:  4.28406661e-03, bound:  3.15414518e-01\n",
      "Epoch: 26214 mean train loss:  4.28378023e-03, bound:  3.15414518e-01\n",
      "Epoch: 26215 mean train loss:  4.28362563e-03, bound:  3.15414488e-01\n",
      "Epoch: 26216 mean train loss:  4.28357348e-03, bound:  3.15414459e-01\n",
      "Epoch: 26217 mean train loss:  4.28342540e-03, bound:  3.15414459e-01\n",
      "Epoch: 26218 mean train loss:  4.28339280e-03, bound:  3.15414429e-01\n",
      "Epoch: 26219 mean train loss:  4.28346219e-03, bound:  3.15414429e-01\n",
      "Epoch: 26220 mean train loss:  4.28383518e-03, bound:  3.15414399e-01\n",
      "Epoch: 26221 mean train loss:  4.28451411e-03, bound:  3.15414399e-01\n",
      "Epoch: 26222 mean train loss:  4.28578863e-03, bound:  3.15414369e-01\n",
      "Epoch: 26223 mean train loss:  4.28802939e-03, bound:  3.15414369e-01\n",
      "Epoch: 26224 mean train loss:  4.29177238e-03, bound:  3.15414280e-01\n",
      "Epoch: 26225 mean train loss:  4.29758755e-03, bound:  3.15414339e-01\n",
      "Epoch: 26226 mean train loss:  4.30609146e-03, bound:  3.15414250e-01\n",
      "Epoch: 26227 mean train loss:  4.31670528e-03, bound:  3.15414339e-01\n",
      "Epoch: 26228 mean train loss:  4.32675472e-03, bound:  3.15414190e-01\n",
      "Epoch: 26229 mean train loss:  4.33069281e-03, bound:  3.15414310e-01\n",
      "Epoch: 26230 mean train loss:  4.32283711e-03, bound:  3.15414131e-01\n",
      "Epoch: 26231 mean train loss:  4.30445326e-03, bound:  3.15414250e-01\n",
      "Epoch: 26232 mean train loss:  4.28633997e-03, bound:  3.15414190e-01\n",
      "Epoch: 26233 mean train loss:  4.27983748e-03, bound:  3.15414220e-01\n",
      "Epoch: 26234 mean train loss:  4.28621517e-03, bound:  3.15414220e-01\n",
      "Epoch: 26235 mean train loss:  4.29657195e-03, bound:  3.15414131e-01\n",
      "Epoch: 26236 mean train loss:  4.30015149e-03, bound:  3.15414220e-01\n",
      "Epoch: 26237 mean train loss:  4.29368624e-03, bound:  3.15414101e-01\n",
      "Epoch: 26238 mean train loss:  4.28332109e-03, bound:  3.15414131e-01\n",
      "Epoch: 26239 mean train loss:  4.27861372e-03, bound:  3.15414101e-01\n",
      "Epoch: 26240 mean train loss:  4.28218208e-03, bound:  3.15414071e-01\n",
      "Epoch: 26241 mean train loss:  4.28802241e-03, bound:  3.15414101e-01\n",
      "Epoch: 26242 mean train loss:  4.28893138e-03, bound:  3.15414071e-01\n",
      "Epoch: 26243 mean train loss:  4.28401586e-03, bound:  3.15414071e-01\n",
      "Epoch: 26244 mean train loss:  4.27861465e-03, bound:  3.15414011e-01\n",
      "Epoch: 26245 mean train loss:  4.27791942e-03, bound:  3.15414011e-01\n",
      "Epoch: 26246 mean train loss:  4.28103190e-03, bound:  3.15414011e-01\n",
      "Epoch: 26247 mean train loss:  4.28346964e-03, bound:  3.15413982e-01\n",
      "Epoch: 26248 mean train loss:  4.28219885e-03, bound:  3.15413982e-01\n",
      "Epoch: 26249 mean train loss:  4.27864259e-03, bound:  3.15413952e-01\n",
      "Epoch: 26250 mean train loss:  4.27651359e-03, bound:  3.15413952e-01\n",
      "Epoch: 26251 mean train loss:  4.27740207e-03, bound:  3.15413922e-01\n",
      "Epoch: 26252 mean train loss:  4.27948544e-03, bound:  3.15413892e-01\n",
      "Epoch: 26253 mean train loss:  4.27977508e-03, bound:  3.15413892e-01\n",
      "Epoch: 26254 mean train loss:  4.27789753e-03, bound:  3.15413862e-01\n",
      "Epoch: 26255 mean train loss:  4.27589100e-03, bound:  3.15413833e-01\n",
      "Epoch: 26256 mean train loss:  4.27549658e-03, bound:  3.15413833e-01\n",
      "Epoch: 26257 mean train loss:  4.27647959e-03, bound:  3.15413773e-01\n",
      "Epoch: 26258 mean train loss:  4.27720509e-03, bound:  3.15413773e-01\n",
      "Epoch: 26259 mean train loss:  4.27658902e-03, bound:  3.15413773e-01\n",
      "Epoch: 26260 mean train loss:  4.27523814e-03, bound:  3.15413773e-01\n",
      "Epoch: 26261 mean train loss:  4.27437061e-03, bound:  3.15413684e-01\n",
      "Epoch: 26262 mean train loss:  4.27456666e-03, bound:  3.15413684e-01\n",
      "Epoch: 26263 mean train loss:  4.27503139e-03, bound:  3.15413684e-01\n",
      "Epoch: 26264 mean train loss:  4.27491870e-03, bound:  3.15413654e-01\n",
      "Epoch: 26265 mean train loss:  4.27423138e-03, bound:  3.15413654e-01\n",
      "Epoch: 26266 mean train loss:  4.27343836e-03, bound:  3.15413654e-01\n",
      "Epoch: 26267 mean train loss:  4.27326886e-03, bound:  3.15413564e-01\n",
      "Epoch: 26268 mean train loss:  4.27342625e-03, bound:  3.15413564e-01\n",
      "Epoch: 26269 mean train loss:  4.27349983e-03, bound:  3.15413564e-01\n",
      "Epoch: 26270 mean train loss:  4.27320274e-03, bound:  3.15413564e-01\n",
      "Epoch: 26271 mean train loss:  4.27254988e-03, bound:  3.15413564e-01\n",
      "Epoch: 26272 mean train loss:  4.27226257e-03, bound:  3.15413535e-01\n",
      "Epoch: 26273 mean train loss:  4.27217642e-03, bound:  3.15413535e-01\n",
      "Epoch: 26274 mean train loss:  4.27220901e-03, bound:  3.15413475e-01\n",
      "Epoch: 26275 mean train loss:  4.27208422e-03, bound:  3.15413475e-01\n",
      "Epoch: 26276 mean train loss:  4.27164929e-03, bound:  3.15413475e-01\n",
      "Epoch: 26277 mean train loss:  4.27128654e-03, bound:  3.15413445e-01\n",
      "Epoch: 26278 mean train loss:  4.27106582e-03, bound:  3.15413415e-01\n",
      "Epoch: 26279 mean train loss:  4.27102344e-03, bound:  3.15413415e-01\n",
      "Epoch: 26280 mean train loss:  4.27097268e-03, bound:  3.15413415e-01\n",
      "Epoch: 26281 mean train loss:  4.27072216e-03, bound:  3.15413356e-01\n",
      "Epoch: 26282 mean train loss:  4.27043671e-03, bound:  3.15413356e-01\n",
      "Epoch: 26283 mean train loss:  4.27011261e-03, bound:  3.15413326e-01\n",
      "Epoch: 26284 mean train loss:  4.26986860e-03, bound:  3.15413326e-01\n",
      "Epoch: 26285 mean train loss:  4.26979084e-03, bound:  3.15413296e-01\n",
      "Epoch: 26286 mean train loss:  4.26963856e-03, bound:  3.15413296e-01\n",
      "Epoch: 26287 mean train loss:  4.26935498e-03, bound:  3.15413296e-01\n",
      "Epoch: 26288 mean train loss:  4.26919246e-03, bound:  3.15413237e-01\n",
      "Epoch: 26289 mean train loss:  4.26894287e-03, bound:  3.15413237e-01\n",
      "Epoch: 26290 mean train loss:  4.26875614e-03, bound:  3.15413207e-01\n",
      "Epoch: 26291 mean train loss:  4.26859362e-03, bound:  3.15413207e-01\n",
      "Epoch: 26292 mean train loss:  4.26842831e-03, bound:  3.15413177e-01\n",
      "Epoch: 26293 mean train loss:  4.26822715e-03, bound:  3.15413177e-01\n",
      "Epoch: 26294 mean train loss:  4.26798547e-03, bound:  3.15413117e-01\n",
      "Epoch: 26295 mean train loss:  4.26777964e-03, bound:  3.15413117e-01\n",
      "Epoch: 26296 mean train loss:  4.26756917e-03, bound:  3.15413117e-01\n",
      "Epoch: 26297 mean train loss:  4.26742295e-03, bound:  3.15413088e-01\n",
      "Epoch: 26298 mean train loss:  4.26723063e-03, bound:  3.15413088e-01\n",
      "Epoch: 26299 mean train loss:  4.26701410e-03, bound:  3.15413058e-01\n",
      "Epoch: 26300 mean train loss:  4.26680455e-03, bound:  3.15413058e-01\n",
      "Epoch: 26301 mean train loss:  4.26659361e-03, bound:  3.15412998e-01\n",
      "Epoch: 26302 mean train loss:  4.26637847e-03, bound:  3.15412998e-01\n",
      "Epoch: 26303 mean train loss:  4.26621363e-03, bound:  3.15412968e-01\n",
      "Epoch: 26304 mean train loss:  4.26601293e-03, bound:  3.15412968e-01\n",
      "Epoch: 26305 mean train loss:  4.26576100e-03, bound:  3.15412968e-01\n",
      "Epoch: 26306 mean train loss:  4.26557427e-03, bound:  3.15412968e-01\n",
      "Epoch: 26307 mean train loss:  4.26540291e-03, bound:  3.15412939e-01\n",
      "Epoch: 26308 mean train loss:  4.26519150e-03, bound:  3.15412879e-01\n",
      "Epoch: 26309 mean train loss:  4.26505646e-03, bound:  3.15412879e-01\n",
      "Epoch: 26310 mean train loss:  4.26479615e-03, bound:  3.15412849e-01\n",
      "Epoch: 26311 mean train loss:  4.26462898e-03, bound:  3.15412849e-01\n",
      "Epoch: 26312 mean train loss:  4.26443340e-03, bound:  3.15412849e-01\n",
      "Epoch: 26313 mean train loss:  4.26424341e-03, bound:  3.15412849e-01\n",
      "Epoch: 26314 mean train loss:  4.26406087e-03, bound:  3.15412790e-01\n",
      "Epoch: 26315 mean train loss:  4.26387088e-03, bound:  3.15412790e-01\n",
      "Epoch: 26316 mean train loss:  4.26363619e-03, bound:  3.15412730e-01\n",
      "Epoch: 26317 mean train loss:  4.26340150e-03, bound:  3.15412730e-01\n",
      "Epoch: 26318 mean train loss:  4.26330324e-03, bound:  3.15412730e-01\n",
      "Epoch: 26319 mean train loss:  4.26302897e-03, bound:  3.15412730e-01\n",
      "Epoch: 26320 mean train loss:  4.26285667e-03, bound:  3.15412730e-01\n",
      "Epoch: 26321 mean train loss:  4.26264759e-03, bound:  3.15412670e-01\n",
      "Epoch: 26322 mean train loss:  4.26244317e-03, bound:  3.15412670e-01\n",
      "Epoch: 26323 mean train loss:  4.26224899e-03, bound:  3.15412641e-01\n",
      "Epoch: 26324 mean train loss:  4.26202593e-03, bound:  3.15412641e-01\n",
      "Epoch: 26325 mean train loss:  4.26182849e-03, bound:  3.15412611e-01\n",
      "Epoch: 26326 mean train loss:  4.26164223e-03, bound:  3.15412611e-01\n",
      "Epoch: 26327 mean train loss:  4.26138798e-03, bound:  3.15412551e-01\n",
      "Epoch: 26328 mean train loss:  4.26123617e-03, bound:  3.15412551e-01\n",
      "Epoch: 26329 mean train loss:  4.26110532e-03, bound:  3.15412551e-01\n",
      "Epoch: 26330 mean train loss:  4.26086551e-03, bound:  3.15412521e-01\n",
      "Epoch: 26331 mean train loss:  4.26067458e-03, bound:  3.15412521e-01\n",
      "Epoch: 26332 mean train loss:  4.26044781e-03, bound:  3.15412492e-01\n",
      "Epoch: 26333 mean train loss:  4.26028110e-03, bound:  3.15412492e-01\n",
      "Epoch: 26334 mean train loss:  4.26010881e-03, bound:  3.15412432e-01\n",
      "Epoch: 26335 mean train loss:  4.25988669e-03, bound:  3.15412432e-01\n",
      "Epoch: 26336 mean train loss:  4.25967295e-03, bound:  3.15412402e-01\n",
      "Epoch: 26337 mean train loss:  4.25945874e-03, bound:  3.15412402e-01\n",
      "Epoch: 26338 mean train loss:  4.25925711e-03, bound:  3.15412402e-01\n",
      "Epoch: 26339 mean train loss:  4.25908435e-03, bound:  3.15412343e-01\n",
      "Epoch: 26340 mean train loss:  4.25887015e-03, bound:  3.15412313e-01\n",
      "Epoch: 26341 mean train loss:  4.25870484e-03, bound:  3.15412313e-01\n",
      "Epoch: 26342 mean train loss:  4.25846921e-03, bound:  3.15412313e-01\n",
      "Epoch: 26343 mean train loss:  4.25827969e-03, bound:  3.15412283e-01\n",
      "Epoch: 26344 mean train loss:  4.25809389e-03, bound:  3.15412283e-01\n",
      "Epoch: 26345 mean train loss:  4.25792299e-03, bound:  3.15412223e-01\n",
      "Epoch: 26346 mean train loss:  4.25769761e-03, bound:  3.15412223e-01\n",
      "Epoch: 26347 mean train loss:  4.25748294e-03, bound:  3.15412194e-01\n",
      "Epoch: 26348 mean train loss:  4.25728736e-03, bound:  3.15412194e-01\n",
      "Epoch: 26349 mean train loss:  4.25710669e-03, bound:  3.15412194e-01\n",
      "Epoch: 26350 mean train loss:  4.25686734e-03, bound:  3.15412164e-01\n",
      "Epoch: 26351 mean train loss:  4.25667176e-03, bound:  3.15412164e-01\n",
      "Epoch: 26352 mean train loss:  4.25651763e-03, bound:  3.15412164e-01\n",
      "Epoch: 26353 mean train loss:  4.25627641e-03, bound:  3.15412164e-01\n",
      "Epoch: 26354 mean train loss:  4.25607990e-03, bound:  3.15412104e-01\n",
      "Epoch: 26355 mean train loss:  4.25587967e-03, bound:  3.15412104e-01\n",
      "Epoch: 26356 mean train loss:  4.25568596e-03, bound:  3.15412045e-01\n",
      "Epoch: 26357 mean train loss:  4.25548851e-03, bound:  3.15412045e-01\n",
      "Epoch: 26358 mean train loss:  4.25527571e-03, bound:  3.15412045e-01\n",
      "Epoch: 26359 mean train loss:  4.25509736e-03, bound:  3.15412045e-01\n",
      "Epoch: 26360 mean train loss:  4.25491342e-03, bound:  3.15411985e-01\n",
      "Epoch: 26361 mean train loss:  4.25468665e-03, bound:  3.15411985e-01\n",
      "Epoch: 26362 mean train loss:  4.25450597e-03, bound:  3.15411985e-01\n",
      "Epoch: 26363 mean train loss:  4.25430341e-03, bound:  3.15411925e-01\n",
      "Epoch: 26364 mean train loss:  4.25409526e-03, bound:  3.15411925e-01\n",
      "Epoch: 26365 mean train loss:  4.25393879e-03, bound:  3.15411896e-01\n",
      "Epoch: 26366 mean train loss:  4.25367663e-03, bound:  3.15411896e-01\n",
      "Epoch: 26367 mean train loss:  4.25352389e-03, bound:  3.15411866e-01\n",
      "Epoch: 26368 mean train loss:  4.25326778e-03, bound:  3.15411866e-01\n",
      "Epoch: 26369 mean train loss:  4.25309688e-03, bound:  3.15411836e-01\n",
      "Epoch: 26370 mean train loss:  4.25289199e-03, bound:  3.15411836e-01\n",
      "Epoch: 26371 mean train loss:  4.25267825e-03, bound:  3.15411776e-01\n",
      "Epoch: 26372 mean train loss:  4.25250130e-03, bound:  3.15411776e-01\n",
      "Epoch: 26373 mean train loss:  4.25227359e-03, bound:  3.15411747e-01\n",
      "Epoch: 26374 mean train loss:  4.25208081e-03, bound:  3.15411747e-01\n",
      "Epoch: 26375 mean train loss:  4.25187312e-03, bound:  3.15411717e-01\n",
      "Epoch: 26376 mean train loss:  4.25167149e-03, bound:  3.15411717e-01\n",
      "Epoch: 26377 mean train loss:  4.25146939e-03, bound:  3.15411657e-01\n",
      "Epoch: 26378 mean train loss:  4.25125612e-03, bound:  3.15411657e-01\n",
      "Epoch: 26379 mean train loss:  4.25109500e-03, bound:  3.15411657e-01\n",
      "Epoch: 26380 mean train loss:  4.25085006e-03, bound:  3.15411627e-01\n",
      "Epoch: 26381 mean train loss:  4.25069919e-03, bound:  3.15411627e-01\n",
      "Epoch: 26382 mean train loss:  4.25049942e-03, bound:  3.15411597e-01\n",
      "Epoch: 26383 mean train loss:  4.25026566e-03, bound:  3.15411597e-01\n",
      "Epoch: 26384 mean train loss:  4.25009895e-03, bound:  3.15411538e-01\n",
      "Epoch: 26385 mean train loss:  4.24988614e-03, bound:  3.15411538e-01\n",
      "Epoch: 26386 mean train loss:  4.24965983e-03, bound:  3.15411538e-01\n",
      "Epoch: 26387 mean train loss:  4.24948568e-03, bound:  3.15411538e-01\n",
      "Epoch: 26388 mean train loss:  4.24921839e-03, bound:  3.15411508e-01\n",
      "Epoch: 26389 mean train loss:  4.24906053e-03, bound:  3.15411478e-01\n",
      "Epoch: 26390 mean train loss:  4.24883142e-03, bound:  3.15411419e-01\n",
      "Epoch: 26391 mean train loss:  4.24866471e-03, bound:  3.15411419e-01\n",
      "Epoch: 26392 mean train loss:  4.24844772e-03, bound:  3.15411419e-01\n",
      "Epoch: 26393 mean train loss:  4.24827402e-03, bound:  3.15411419e-01\n",
      "Epoch: 26394 mean train loss:  4.24803747e-03, bound:  3.15411359e-01\n",
      "Epoch: 26395 mean train loss:  4.24786052e-03, bound:  3.15411359e-01\n",
      "Epoch: 26396 mean train loss:  4.24766727e-03, bound:  3.15411359e-01\n",
      "Epoch: 26397 mean train loss:  4.24747262e-03, bound:  3.15411299e-01\n",
      "Epoch: 26398 mean train loss:  4.24726168e-03, bound:  3.15411299e-01\n",
      "Epoch: 26399 mean train loss:  4.24702233e-03, bound:  3.15411299e-01\n",
      "Epoch: 26400 mean train loss:  4.24688216e-03, bound:  3.15411299e-01\n",
      "Epoch: 26401 mean train loss:  4.24665073e-03, bound:  3.15411240e-01\n",
      "Epoch: 26402 mean train loss:  4.24642209e-03, bound:  3.15411240e-01\n",
      "Epoch: 26403 mean train loss:  4.24623210e-03, bound:  3.15411210e-01\n",
      "Epoch: 26404 mean train loss:  4.24603466e-03, bound:  3.15411210e-01\n",
      "Epoch: 26405 mean train loss:  4.24580462e-03, bound:  3.15411180e-01\n",
      "Epoch: 26406 mean train loss:  4.24561370e-03, bound:  3.15411180e-01\n",
      "Epoch: 26407 mean train loss:  4.24539112e-03, bound:  3.15411150e-01\n",
      "Epoch: 26408 mean train loss:  4.24521603e-03, bound:  3.15411150e-01\n",
      "Epoch: 26409 mean train loss:  4.24502790e-03, bound:  3.15411091e-01\n",
      "Epoch: 26410 mean train loss:  4.24478576e-03, bound:  3.15411091e-01\n",
      "Epoch: 26411 mean train loss:  4.24462371e-03, bound:  3.15411091e-01\n",
      "Epoch: 26412 mean train loss:  4.24440717e-03, bound:  3.15411061e-01\n",
      "Epoch: 26413 mean train loss:  4.24423907e-03, bound:  3.15411061e-01\n",
      "Epoch: 26414 mean train loss:  4.24398528e-03, bound:  3.15411031e-01\n",
      "Epoch: 26415 mean train loss:  4.24376968e-03, bound:  3.15411001e-01\n",
      "Epoch: 26416 mean train loss:  4.24359180e-03, bound:  3.15410972e-01\n",
      "Epoch: 26417 mean train loss:  4.24338086e-03, bound:  3.15410972e-01\n",
      "Epoch: 26418 mean train loss:  4.24313685e-03, bound:  3.15410942e-01\n",
      "Epoch: 26419 mean train loss:  4.24297014e-03, bound:  3.15410942e-01\n",
      "Epoch: 26420 mean train loss:  4.24280297e-03, bound:  3.15410912e-01\n",
      "Epoch: 26421 mean train loss:  4.24261019e-03, bound:  3.15410912e-01\n",
      "Epoch: 26422 mean train loss:  4.24237875e-03, bound:  3.15410912e-01\n",
      "Epoch: 26423 mean train loss:  4.24227538e-03, bound:  3.15410852e-01\n",
      "Epoch: 26424 mean train loss:  4.24205838e-03, bound:  3.15410852e-01\n",
      "Epoch: 26425 mean train loss:  4.24193498e-03, bound:  3.15410823e-01\n",
      "Epoch: 26426 mean train loss:  4.24172776e-03, bound:  3.15410823e-01\n",
      "Epoch: 26427 mean train loss:  4.24159504e-03, bound:  3.15410793e-01\n",
      "Epoch: 26428 mean train loss:  4.24135150e-03, bound:  3.15410793e-01\n",
      "Epoch: 26429 mean train loss:  4.24121507e-03, bound:  3.15410733e-01\n",
      "Epoch: 26430 mean train loss:  4.24103439e-03, bound:  3.15410733e-01\n",
      "Epoch: 26431 mean train loss:  4.24092403e-03, bound:  3.15410733e-01\n",
      "Epoch: 26432 mean train loss:  4.24077781e-03, bound:  3.15410733e-01\n",
      "Epoch: 26433 mean train loss:  4.24069166e-03, bound:  3.15410674e-01\n",
      "Epoch: 26434 mean train loss:  4.24059248e-03, bound:  3.15410674e-01\n",
      "Epoch: 26435 mean train loss:  4.24047885e-03, bound:  3.15410614e-01\n",
      "Epoch: 26436 mean train loss:  4.24041087e-03, bound:  3.15410614e-01\n",
      "Epoch: 26437 mean train loss:  4.24042763e-03, bound:  3.15410614e-01\n",
      "Epoch: 26438 mean train loss:  4.24045604e-03, bound:  3.15410614e-01\n",
      "Epoch: 26439 mean train loss:  4.24054312e-03, bound:  3.15410554e-01\n",
      "Epoch: 26440 mean train loss:  4.24072472e-03, bound:  3.15410554e-01\n",
      "Epoch: 26441 mean train loss:  4.24089003e-03, bound:  3.15410495e-01\n",
      "Epoch: 26442 mean train loss:  4.24115406e-03, bound:  3.15410495e-01\n",
      "Epoch: 26443 mean train loss:  4.24147490e-03, bound:  3.15410495e-01\n",
      "Epoch: 26444 mean train loss:  4.24198294e-03, bound:  3.15410495e-01\n",
      "Epoch: 26445 mean train loss:  4.24256315e-03, bound:  3.15410435e-01\n",
      "Epoch: 26446 mean train loss:  4.24320344e-03, bound:  3.15410435e-01\n",
      "Epoch: 26447 mean train loss:  4.24378598e-03, bound:  3.15410405e-01\n",
      "Epoch: 26448 mean train loss:  4.24429961e-03, bound:  3.15410405e-01\n",
      "Epoch: 26449 mean train loss:  4.24471078e-03, bound:  3.15410376e-01\n",
      "Epoch: 26450 mean train loss:  4.24477737e-03, bound:  3.15410405e-01\n",
      "Epoch: 26451 mean train loss:  4.24434803e-03, bound:  3.15410316e-01\n",
      "Epoch: 26452 mean train loss:  4.24340460e-03, bound:  3.15410376e-01\n",
      "Epoch: 26453 mean train loss:  4.24193684e-03, bound:  3.15410286e-01\n",
      "Epoch: 26454 mean train loss:  4.24030097e-03, bound:  3.15410316e-01\n",
      "Epoch: 26455 mean train loss:  4.23848256e-03, bound:  3.15410286e-01\n",
      "Epoch: 26456 mean train loss:  4.23689140e-03, bound:  3.15410286e-01\n",
      "Epoch: 26457 mean train loss:  4.23573190e-03, bound:  3.15410256e-01\n",
      "Epoch: 26458 mean train loss:  4.23506927e-03, bound:  3.15410256e-01\n",
      "Epoch: 26459 mean train loss:  4.23486251e-03, bound:  3.15410256e-01\n",
      "Epoch: 26460 mean train loss:  4.23495844e-03, bound:  3.15410197e-01\n",
      "Epoch: 26461 mean train loss:  4.23526112e-03, bound:  3.15410197e-01\n",
      "Epoch: 26462 mean train loss:  4.23557777e-03, bound:  3.15410167e-01\n",
      "Epoch: 26463 mean train loss:  4.23577847e-03, bound:  3.15410167e-01\n",
      "Epoch: 26464 mean train loss:  4.23579942e-03, bound:  3.15410137e-01\n",
      "Epoch: 26465 mean train loss:  4.23556101e-03, bound:  3.15410137e-01\n",
      "Epoch: 26466 mean train loss:  4.23517916e-03, bound:  3.15410078e-01\n",
      "Epoch: 26467 mean train loss:  4.23459988e-03, bound:  3.15410078e-01\n",
      "Epoch: 26468 mean train loss:  4.23396751e-03, bound:  3.15410048e-01\n",
      "Epoch: 26469 mean train loss:  4.23339661e-03, bound:  3.15410048e-01\n",
      "Epoch: 26470 mean train loss:  4.23288811e-03, bound:  3.15410018e-01\n",
      "Epoch: 26471 mean train loss:  4.23245318e-03, bound:  3.15410018e-01\n",
      "Epoch: 26472 mean train loss:  4.23218356e-03, bound:  3.15409958e-01\n",
      "Epoch: 26473 mean train loss:  4.23203036e-03, bound:  3.15409958e-01\n",
      "Epoch: 26474 mean train loss:  4.23205411e-03, bound:  3.15409929e-01\n",
      "Epoch: 26475 mean train loss:  4.23210068e-03, bound:  3.15409929e-01\n",
      "Epoch: 26476 mean train loss:  4.23215982e-03, bound:  3.15409899e-01\n",
      "Epoch: 26477 mean train loss:  4.23219847e-03, bound:  3.15409899e-01\n",
      "Epoch: 26478 mean train loss:  4.23213793e-03, bound:  3.15409869e-01\n",
      "Epoch: 26479 mean train loss:  4.23195865e-03, bound:  3.15409839e-01\n",
      "Epoch: 26480 mean train loss:  4.23171651e-03, bound:  3.15409839e-01\n",
      "Epoch: 26481 mean train loss:  4.23137937e-03, bound:  3.15409809e-01\n",
      "Epoch: 26482 mean train loss:  4.23098309e-03, bound:  3.15409809e-01\n",
      "Epoch: 26483 mean train loss:  4.23054397e-03, bound:  3.15409809e-01\n",
      "Epoch: 26484 mean train loss:  4.23015840e-03, bound:  3.15409780e-01\n",
      "Epoch: 26485 mean train loss:  4.22973605e-03, bound:  3.15409750e-01\n",
      "Epoch: 26486 mean train loss:  4.22943709e-03, bound:  3.15409690e-01\n",
      "Epoch: 26487 mean train loss:  4.22918424e-03, bound:  3.15409690e-01\n",
      "Epoch: 26488 mean train loss:  4.22895467e-03, bound:  3.15409690e-01\n",
      "Epoch: 26489 mean train loss:  4.22874093e-03, bound:  3.15409690e-01\n",
      "Epoch: 26490 mean train loss:  4.22853744e-03, bound:  3.15409631e-01\n",
      "Epoch: 26491 mean train loss:  4.22834884e-03, bound:  3.15409631e-01\n",
      "Epoch: 26492 mean train loss:  4.22815140e-03, bound:  3.15409631e-01\n",
      "Epoch: 26493 mean train loss:  4.22799680e-03, bound:  3.15409601e-01\n",
      "Epoch: 26494 mean train loss:  4.22778586e-03, bound:  3.15409601e-01\n",
      "Epoch: 26495 mean train loss:  4.22765734e-03, bound:  3.15409571e-01\n",
      "Epoch: 26496 mean train loss:  4.22748225e-03, bound:  3.15409511e-01\n",
      "Epoch: 26497 mean train loss:  4.22727969e-03, bound:  3.15409511e-01\n",
      "Epoch: 26498 mean train loss:  4.22711810e-03, bound:  3.15409482e-01\n",
      "Epoch: 26499 mean train loss:  4.22699237e-03, bound:  3.15409482e-01\n",
      "Epoch: 26500 mean train loss:  4.22675349e-03, bound:  3.15409482e-01\n",
      "Epoch: 26501 mean train loss:  4.22649272e-03, bound:  3.15409452e-01\n",
      "Epoch: 26502 mean train loss:  4.22627153e-03, bound:  3.15409452e-01\n",
      "Epoch: 26503 mean train loss:  4.22607921e-03, bound:  3.15409392e-01\n",
      "Epoch: 26504 mean train loss:  4.22584824e-03, bound:  3.15409392e-01\n",
      "Epoch: 26505 mean train loss:  4.22570668e-03, bound:  3.15409392e-01\n",
      "Epoch: 26506 mean train loss:  4.22544358e-03, bound:  3.15409362e-01\n",
      "Epoch: 26507 mean train loss:  4.22527129e-03, bound:  3.15409333e-01\n",
      "Epoch: 26508 mean train loss:  4.22505243e-03, bound:  3.15409333e-01\n",
      "Epoch: 26509 mean train loss:  4.22490388e-03, bound:  3.15409303e-01\n",
      "Epoch: 26510 mean train loss:  4.22466174e-03, bound:  3.15409303e-01\n",
      "Epoch: 26511 mean train loss:  4.22447547e-03, bound:  3.15409273e-01\n",
      "Epoch: 26512 mean train loss:  4.22425941e-03, bound:  3.15409243e-01\n",
      "Epoch: 26513 mean train loss:  4.22414113e-03, bound:  3.15409243e-01\n",
      "Epoch: 26514 mean train loss:  4.22394834e-03, bound:  3.15409184e-01\n",
      "Epoch: 26515 mean train loss:  4.22379421e-03, bound:  3.15409184e-01\n",
      "Epoch: 26516 mean train loss:  4.22369596e-03, bound:  3.15409184e-01\n",
      "Epoch: 26517 mean train loss:  4.22360562e-03, bound:  3.15409184e-01\n",
      "Epoch: 26518 mean train loss:  4.22353018e-03, bound:  3.15409124e-01\n",
      "Epoch: 26519 mean train loss:  4.22344403e-03, bound:  3.15409124e-01\n",
      "Epoch: 26520 mean train loss:  4.22346592e-03, bound:  3.15409064e-01\n",
      "Epoch: 26521 mean train loss:  4.22344729e-03, bound:  3.15409064e-01\n",
      "Epoch: 26522 mean train loss:  4.22364986e-03, bound:  3.15409064e-01\n",
      "Epoch: 26523 mean train loss:  4.22381982e-03, bound:  3.15409064e-01\n",
      "Epoch: 26524 mean train loss:  4.22426313e-03, bound:  3.15409005e-01\n",
      "Epoch: 26525 mean train loss:  4.22487641e-03, bound:  3.15409005e-01\n",
      "Epoch: 26526 mean train loss:  4.22578026e-03, bound:  3.15409005e-01\n",
      "Epoch: 26527 mean train loss:  4.22694487e-03, bound:  3.15409005e-01\n",
      "Epoch: 26528 mean train loss:  4.22844151e-03, bound:  3.15408945e-01\n",
      "Epoch: 26529 mean train loss:  4.23026318e-03, bound:  3.15409005e-01\n",
      "Epoch: 26530 mean train loss:  4.23217053e-03, bound:  3.15408915e-01\n",
      "Epoch: 26531 mean train loss:  4.23394004e-03, bound:  3.15408945e-01\n",
      "Epoch: 26532 mean train loss:  4.23495006e-03, bound:  3.15408885e-01\n",
      "Epoch: 26533 mean train loss:  4.23472049e-03, bound:  3.15408915e-01\n",
      "Epoch: 26534 mean train loss:  4.23295563e-03, bound:  3.15408826e-01\n",
      "Epoch: 26535 mean train loss:  4.22975095e-03, bound:  3.15408885e-01\n",
      "Epoch: 26536 mean train loss:  4.22569830e-03, bound:  3.15408796e-01\n",
      "Epoch: 26537 mean train loss:  4.22205450e-03, bound:  3.15408826e-01\n",
      "Epoch: 26538 mean train loss:  4.21952643e-03, bound:  3.15408796e-01\n",
      "Epoch: 26539 mean train loss:  4.21859929e-03, bound:  3.15408796e-01\n",
      "Epoch: 26540 mean train loss:  4.21894388e-03, bound:  3.15408736e-01\n",
      "Epoch: 26541 mean train loss:  4.22000466e-03, bound:  3.15408707e-01\n",
      "Epoch: 26542 mean train loss:  4.22107708e-03, bound:  3.15408736e-01\n",
      "Epoch: 26543 mean train loss:  4.22165496e-03, bound:  3.15408677e-01\n",
      "Epoch: 26544 mean train loss:  4.22148639e-03, bound:  3.15408707e-01\n",
      "Epoch: 26545 mean train loss:  4.22057882e-03, bound:  3.15408677e-01\n",
      "Epoch: 26546 mean train loss:  4.21914971e-03, bound:  3.15408677e-01\n",
      "Epoch: 26547 mean train loss:  4.21787659e-03, bound:  3.15408617e-01\n",
      "Epoch: 26548 mean train loss:  4.21688426e-03, bound:  3.15408617e-01\n",
      "Epoch: 26549 mean train loss:  4.21665981e-03, bound:  3.15408587e-01\n",
      "Epoch: 26550 mean train loss:  4.21680836e-03, bound:  3.15408587e-01\n",
      "Epoch: 26551 mean train loss:  4.21731500e-03, bound:  3.15408587e-01\n",
      "Epoch: 26552 mean train loss:  4.21767356e-03, bound:  3.15408558e-01\n",
      "Epoch: 26553 mean train loss:  4.21772012e-03, bound:  3.15408558e-01\n",
      "Epoch: 26554 mean train loss:  4.21739696e-03, bound:  3.15408498e-01\n",
      "Epoch: 26555 mean train loss:  4.21681907e-03, bound:  3.15408498e-01\n",
      "Epoch: 26556 mean train loss:  4.21604188e-03, bound:  3.15408468e-01\n",
      "Epoch: 26557 mean train loss:  4.21539135e-03, bound:  3.15408468e-01\n",
      "Epoch: 26558 mean train loss:  4.21492988e-03, bound:  3.15408438e-01\n",
      "Epoch: 26559 mean train loss:  4.21467144e-03, bound:  3.15408379e-01\n",
      "Epoch: 26560 mean train loss:  4.21454245e-03, bound:  3.15408379e-01\n",
      "Epoch: 26561 mean train loss:  4.21452383e-03, bound:  3.15408379e-01\n",
      "Epoch: 26562 mean train loss:  4.21445770e-03, bound:  3.15408379e-01\n",
      "Epoch: 26563 mean train loss:  4.21433710e-03, bound:  3.15408349e-01\n",
      "Epoch: 26564 mean train loss:  4.21412149e-03, bound:  3.15408349e-01\n",
      "Epoch: 26565 mean train loss:  4.21380578e-03, bound:  3.15408260e-01\n",
      "Epoch: 26566 mean train loss:  4.21353569e-03, bound:  3.15408260e-01\n",
      "Epoch: 26567 mean train loss:  4.21322184e-03, bound:  3.15408260e-01\n",
      "Epoch: 26568 mean train loss:  4.21290472e-03, bound:  3.15408260e-01\n",
      "Epoch: 26569 mean train loss:  4.21265466e-03, bound:  3.15408260e-01\n",
      "Epoch: 26570 mean train loss:  4.21243161e-03, bound:  3.15408230e-01\n",
      "Epoch: 26571 mean train loss:  4.21225280e-03, bound:  3.15408170e-01\n",
      "Epoch: 26572 mean train loss:  4.21207165e-03, bound:  3.15408170e-01\n",
      "Epoch: 26573 mean train loss:  4.21186164e-03, bound:  3.15408170e-01\n",
      "Epoch: 26574 mean train loss:  4.21169540e-03, bound:  3.15408140e-01\n",
      "Epoch: 26575 mean train loss:  4.21154778e-03, bound:  3.15408140e-01\n",
      "Epoch: 26576 mean train loss:  4.21137782e-03, bound:  3.15408111e-01\n",
      "Epoch: 26577 mean train loss:  4.21116035e-03, bound:  3.15408111e-01\n",
      "Epoch: 26578 mean train loss:  4.21094988e-03, bound:  3.15408051e-01\n",
      "Epoch: 26579 mean train loss:  4.21074498e-03, bound:  3.15408051e-01\n",
      "Epoch: 26580 mean train loss:  4.21050331e-03, bound:  3.15408021e-01\n",
      "Epoch: 26581 mean train loss:  4.21029888e-03, bound:  3.15408021e-01\n",
      "Epoch: 26582 mean train loss:  4.21008980e-03, bound:  3.15407991e-01\n",
      "Epoch: 26583 mean train loss:  4.20986256e-03, bound:  3.15407991e-01\n",
      "Epoch: 26584 mean train loss:  4.20961482e-03, bound:  3.15407932e-01\n",
      "Epoch: 26585 mean train loss:  4.20949934e-03, bound:  3.15407932e-01\n",
      "Epoch: 26586 mean train loss:  4.20927629e-03, bound:  3.15407932e-01\n",
      "Epoch: 26587 mean train loss:  4.20903135e-03, bound:  3.15407902e-01\n",
      "Epoch: 26588 mean train loss:  4.20888001e-03, bound:  3.15407872e-01\n",
      "Epoch: 26589 mean train loss:  4.20865510e-03, bound:  3.15407872e-01\n",
      "Epoch: 26590 mean train loss:  4.20848001e-03, bound:  3.15407872e-01\n",
      "Epoch: 26591 mean train loss:  4.20831330e-03, bound:  3.15407813e-01\n",
      "Epoch: 26592 mean train loss:  4.20810282e-03, bound:  3.15407813e-01\n",
      "Epoch: 26593 mean train loss:  4.20786906e-03, bound:  3.15407783e-01\n",
      "Epoch: 26594 mean train loss:  4.20766464e-03, bound:  3.15407783e-01\n",
      "Epoch: 26595 mean train loss:  4.20743786e-03, bound:  3.15407753e-01\n",
      "Epoch: 26596 mean train loss:  4.20721527e-03, bound:  3.15407753e-01\n",
      "Epoch: 26597 mean train loss:  4.20704810e-03, bound:  3.15407723e-01\n",
      "Epoch: 26598 mean train loss:  4.20683436e-03, bound:  3.15407693e-01\n",
      "Epoch: 26599 mean train loss:  4.20660991e-03, bound:  3.15407693e-01\n",
      "Epoch: 26600 mean train loss:  4.20645811e-03, bound:  3.15407664e-01\n",
      "Epoch: 26601 mean train loss:  4.20625787e-03, bound:  3.15407664e-01\n",
      "Epoch: 26602 mean train loss:  4.20602923e-03, bound:  3.15407664e-01\n",
      "Epoch: 26603 mean train loss:  4.20589373e-03, bound:  3.15407664e-01\n",
      "Epoch: 26604 mean train loss:  4.20565763e-03, bound:  3.15407574e-01\n",
      "Epoch: 26605 mean train loss:  4.20551607e-03, bound:  3.15407574e-01\n",
      "Epoch: 26606 mean train loss:  4.20525251e-03, bound:  3.15407574e-01\n",
      "Epoch: 26607 mean train loss:  4.20508720e-03, bound:  3.15407574e-01\n",
      "Epoch: 26608 mean train loss:  4.20496752e-03, bound:  3.15407544e-01\n",
      "Epoch: 26609 mean train loss:  4.20475658e-03, bound:  3.15407544e-01\n",
      "Epoch: 26610 mean train loss:  4.20459453e-03, bound:  3.15407455e-01\n",
      "Epoch: 26611 mean train loss:  4.20441851e-03, bound:  3.15407455e-01\n",
      "Epoch: 26612 mean train loss:  4.20416519e-03, bound:  3.15407455e-01\n",
      "Epoch: 26613 mean train loss:  4.20402316e-03, bound:  3.15407455e-01\n",
      "Epoch: 26614 mean train loss:  4.20377310e-03, bound:  3.15407395e-01\n",
      "Epoch: 26615 mean train loss:  4.20353282e-03, bound:  3.15407395e-01\n",
      "Epoch: 26616 mean train loss:  4.20335867e-03, bound:  3.15407366e-01\n",
      "Epoch: 26617 mean train loss:  4.20315145e-03, bound:  3.15407366e-01\n",
      "Epoch: 26618 mean train loss:  4.20303410e-03, bound:  3.15407336e-01\n",
      "Epoch: 26619 mean train loss:  4.20287251e-03, bound:  3.15407336e-01\n",
      "Epoch: 26620 mean train loss:  4.20274492e-03, bound:  3.15407276e-01\n",
      "Epoch: 26621 mean train loss:  4.20252793e-03, bound:  3.15407276e-01\n",
      "Epoch: 26622 mean train loss:  4.20242688e-03, bound:  3.15407246e-01\n",
      "Epoch: 26623 mean train loss:  4.20230441e-03, bound:  3.15407246e-01\n",
      "Epoch: 26624 mean train loss:  4.20219870e-03, bound:  3.15407217e-01\n",
      "Epoch: 26625 mean train loss:  4.20214143e-03, bound:  3.15407217e-01\n",
      "Epoch: 26626 mean train loss:  4.20224620e-03, bound:  3.15407157e-01\n",
      "Epoch: 26627 mean train loss:  4.20240965e-03, bound:  3.15407157e-01\n",
      "Epoch: 26628 mean train loss:  4.20265039e-03, bound:  3.15407127e-01\n",
      "Epoch: 26629 mean train loss:  4.20304574e-03, bound:  3.15407127e-01\n",
      "Epoch: 26630 mean train loss:  4.20357520e-03, bound:  3.15407097e-01\n",
      "Epoch: 26631 mean train loss:  4.20439988e-03, bound:  3.15407127e-01\n",
      "Epoch: 26632 mean train loss:  4.20543458e-03, bound:  3.15407068e-01\n",
      "Epoch: 26633 mean train loss:  4.20678779e-03, bound:  3.15407097e-01\n",
      "Epoch: 26634 mean train loss:  4.20823833e-03, bound:  3.15407008e-01\n",
      "Epoch: 26635 mean train loss:  4.20993473e-03, bound:  3.15407068e-01\n",
      "Epoch: 26636 mean train loss:  4.21147794e-03, bound:  3.15406978e-01\n",
      "Epoch: 26637 mean train loss:  4.21274221e-03, bound:  3.15407008e-01\n",
      "Epoch: 26638 mean train loss:  4.21297224e-03, bound:  3.15406948e-01\n",
      "Epoch: 26639 mean train loss:  4.21196315e-03, bound:  3.15406978e-01\n",
      "Epoch: 26640 mean train loss:  4.20950027e-03, bound:  3.15406919e-01\n",
      "Epoch: 26641 mean train loss:  4.20601619e-03, bound:  3.15406948e-01\n",
      "Epoch: 26642 mean train loss:  4.20238124e-03, bound:  3.15406889e-01\n",
      "Epoch: 26643 mean train loss:  4.19941777e-03, bound:  3.15406889e-01\n",
      "Epoch: 26644 mean train loss:  4.19777073e-03, bound:  3.15406889e-01\n",
      "Epoch: 26645 mean train loss:  4.19752114e-03, bound:  3.15406859e-01\n",
      "Epoch: 26646 mean train loss:  4.19825222e-03, bound:  3.15406859e-01\n",
      "Epoch: 26647 mean train loss:  4.19940380e-03, bound:  3.15406829e-01\n",
      "Epoch: 26648 mean train loss:  4.20031836e-03, bound:  3.15406829e-01\n",
      "Epoch: 26649 mean train loss:  4.20063036e-03, bound:  3.15406770e-01\n",
      "Epoch: 26650 mean train loss:  4.20026714e-03, bound:  3.15406770e-01\n",
      "Epoch: 26651 mean train loss:  4.19917516e-03, bound:  3.15406770e-01\n",
      "Epoch: 26652 mean train loss:  4.19789599e-03, bound:  3.15406770e-01\n",
      "Epoch: 26653 mean train loss:  4.19665826e-03, bound:  3.15406710e-01\n",
      "Epoch: 26654 mean train loss:  4.19580797e-03, bound:  3.15406710e-01\n",
      "Epoch: 26655 mean train loss:  4.19545174e-03, bound:  3.15406710e-01\n",
      "Epoch: 26656 mean train loss:  4.19544987e-03, bound:  3.15406680e-01\n",
      "Epoch: 26657 mean train loss:  4.19566734e-03, bound:  3.15406680e-01\n",
      "Epoch: 26658 mean train loss:  4.19590855e-03, bound:  3.15406650e-01\n",
      "Epoch: 26659 mean train loss:  4.19590576e-03, bound:  3.15406650e-01\n",
      "Epoch: 26660 mean train loss:  4.19569993e-03, bound:  3.15406591e-01\n",
      "Epoch: 26661 mean train loss:  4.19526035e-03, bound:  3.15406591e-01\n",
      "Epoch: 26662 mean train loss:  4.19467501e-03, bound:  3.15406561e-01\n",
      "Epoch: 26663 mean train loss:  4.19416232e-03, bound:  3.15406561e-01\n",
      "Epoch: 26664 mean train loss:  4.19372786e-03, bound:  3.15406531e-01\n",
      "Epoch: 26665 mean train loss:  4.19348292e-03, bound:  3.15406531e-01\n",
      "Epoch: 26666 mean train loss:  4.19343077e-03, bound:  3.15406471e-01\n",
      "Epoch: 26667 mean train loss:  4.19342332e-03, bound:  3.15406471e-01\n",
      "Epoch: 26668 mean train loss:  4.19341773e-03, bound:  3.15406442e-01\n",
      "Epoch: 26669 mean train loss:  4.19335114e-03, bound:  3.15406442e-01\n",
      "Epoch: 26670 mean train loss:  4.19307360e-03, bound:  3.15406442e-01\n",
      "Epoch: 26671 mean train loss:  4.19274298e-03, bound:  3.15406412e-01\n",
      "Epoch: 26672 mean train loss:  4.19240026e-03, bound:  3.15406382e-01\n",
      "Epoch: 26673 mean train loss:  4.19206591e-03, bound:  3.15406352e-01\n",
      "Epoch: 26674 mean train loss:  4.19178326e-03, bound:  3.15406352e-01\n",
      "Epoch: 26675 mean train loss:  4.19151783e-03, bound:  3.15406322e-01\n",
      "Epoch: 26676 mean train loss:  4.19130968e-03, bound:  3.15406293e-01\n",
      "Epoch: 26677 mean train loss:  4.19110898e-03, bound:  3.15406293e-01\n",
      "Epoch: 26678 mean train loss:  4.19089198e-03, bound:  3.15406293e-01\n",
      "Epoch: 26679 mean train loss:  4.19076206e-03, bound:  3.15406233e-01\n",
      "Epoch: 26680 mean train loss:  4.19056695e-03, bound:  3.15406203e-01\n",
      "Epoch: 26681 mean train loss:  4.19037184e-03, bound:  3.15406203e-01\n",
      "Epoch: 26682 mean train loss:  4.19019209e-03, bound:  3.15406203e-01\n",
      "Epoch: 26683 mean train loss:  4.19003470e-03, bound:  3.15406173e-01\n",
      "Epoch: 26684 mean train loss:  4.18979349e-03, bound:  3.15406144e-01\n",
      "Epoch: 26685 mean train loss:  4.18955600e-03, bound:  3.15406144e-01\n",
      "Epoch: 26686 mean train loss:  4.18942189e-03, bound:  3.15406144e-01\n",
      "Epoch: 26687 mean train loss:  4.18918394e-03, bound:  3.15406084e-01\n",
      "Epoch: 26688 mean train loss:  4.18892130e-03, bound:  3.15406084e-01\n",
      "Epoch: 26689 mean train loss:  4.18871967e-03, bound:  3.15406084e-01\n",
      "Epoch: 26690 mean train loss:  4.18855762e-03, bound:  3.15406024e-01\n",
      "Epoch: 26691 mean train loss:  4.18833736e-03, bound:  3.15406024e-01\n",
      "Epoch: 26692 mean train loss:  4.18812549e-03, bound:  3.15406024e-01\n",
      "Epoch: 26693 mean train loss:  4.18787403e-03, bound:  3.15406024e-01\n",
      "Epoch: 26694 mean train loss:  4.18772548e-03, bound:  3.15405995e-01\n",
      "Epoch: 26695 mean train loss:  4.18753643e-03, bound:  3.15405995e-01\n",
      "Epoch: 26696 mean train loss:  4.18735249e-03, bound:  3.15405965e-01\n",
      "Epoch: 26697 mean train loss:  4.18717414e-03, bound:  3.15405965e-01\n",
      "Epoch: 26698 mean train loss:  4.18701861e-03, bound:  3.15405905e-01\n",
      "Epoch: 26699 mean train loss:  4.18677134e-03, bound:  3.15405905e-01\n",
      "Epoch: 26700 mean train loss:  4.18664468e-03, bound:  3.15405875e-01\n",
      "Epoch: 26701 mean train loss:  4.18640301e-03, bound:  3.15405875e-01\n",
      "Epoch: 26702 mean train loss:  4.18623025e-03, bound:  3.15405846e-01\n",
      "Epoch: 26703 mean train loss:  4.18604724e-03, bound:  3.15405846e-01\n",
      "Epoch: 26704 mean train loss:  4.18582279e-03, bound:  3.15405786e-01\n",
      "Epoch: 26705 mean train loss:  4.18562209e-03, bound:  3.15405786e-01\n",
      "Epoch: 26706 mean train loss:  4.18549078e-03, bound:  3.15405756e-01\n",
      "Epoch: 26707 mean train loss:  4.18526540e-03, bound:  3.15405756e-01\n",
      "Epoch: 26708 mean train loss:  4.18512058e-03, bound:  3.15405726e-01\n",
      "Epoch: 26709 mean train loss:  4.18481650e-03, bound:  3.15405726e-01\n",
      "Epoch: 26710 mean train loss:  4.18467773e-03, bound:  3.15405667e-01\n",
      "Epoch: 26711 mean train loss:  4.18435549e-03, bound:  3.15405667e-01\n",
      "Epoch: 26712 mean train loss:  4.18416783e-03, bound:  3.15405667e-01\n",
      "Epoch: 26713 mean train loss:  4.18393780e-03, bound:  3.15405637e-01\n",
      "Epoch: 26714 mean train loss:  4.18373803e-03, bound:  3.15405637e-01\n",
      "Epoch: 26715 mean train loss:  4.18352522e-03, bound:  3.15405577e-01\n",
      "Epoch: 26716 mean train loss:  4.18338599e-03, bound:  3.15405577e-01\n",
      "Epoch: 26717 mean train loss:  4.18312522e-03, bound:  3.15405548e-01\n",
      "Epoch: 26718 mean train loss:  4.18298692e-03, bound:  3.15405548e-01\n",
      "Epoch: 26719 mean train loss:  4.18274524e-03, bound:  3.15405518e-01\n",
      "Epoch: 26720 mean train loss:  4.18252265e-03, bound:  3.15405518e-01\n",
      "Epoch: 26721 mean train loss:  4.18235315e-03, bound:  3.15405458e-01\n",
      "Epoch: 26722 mean train loss:  4.18216363e-03, bound:  3.15405458e-01\n",
      "Epoch: 26723 mean train loss:  4.18195734e-03, bound:  3.15405428e-01\n",
      "Epoch: 26724 mean train loss:  4.18173661e-03, bound:  3.15405428e-01\n",
      "Epoch: 26725 mean train loss:  4.18152614e-03, bound:  3.15405428e-01\n",
      "Epoch: 26726 mean train loss:  4.18134360e-03, bound:  3.15405428e-01\n",
      "Epoch: 26727 mean train loss:  4.18117037e-03, bound:  3.15405399e-01\n",
      "Epoch: 26728 mean train loss:  4.18095011e-03, bound:  3.15405339e-01\n",
      "Epoch: 26729 mean train loss:  4.18078667e-03, bound:  3.15405339e-01\n",
      "Epoch: 26730 mean train loss:  4.18057619e-03, bound:  3.15405339e-01\n",
      "Epoch: 26731 mean train loss:  4.18040529e-03, bound:  3.15405309e-01\n",
      "Epoch: 26732 mean train loss:  4.18030890e-03, bound:  3.15405309e-01\n",
      "Epoch: 26733 mean train loss:  4.18023765e-03, bound:  3.15405279e-01\n",
      "Epoch: 26734 mean train loss:  4.18021437e-03, bound:  3.15405220e-01\n",
      "Epoch: 26735 mean train loss:  4.18030191e-03, bound:  3.15405220e-01\n",
      "Epoch: 26736 mean train loss:  4.18052776e-03, bound:  3.15405220e-01\n",
      "Epoch: 26737 mean train loss:  4.18089796e-03, bound:  3.15405220e-01\n",
      "Epoch: 26738 mean train loss:  4.18146374e-03, bound:  3.15405190e-01\n",
      "Epoch: 26739 mean train loss:  4.18244721e-03, bound:  3.15405101e-01\n",
      "Epoch: 26740 mean train loss:  4.18384606e-03, bound:  3.15405190e-01\n",
      "Epoch: 26741 mean train loss:  4.18574270e-03, bound:  3.15405101e-01\n",
      "Epoch: 26742 mean train loss:  4.18845378e-03, bound:  3.15405101e-01\n",
      "Epoch: 26743 mean train loss:  4.19206405e-03, bound:  3.15405071e-01\n",
      "Epoch: 26744 mean train loss:  4.19603288e-03, bound:  3.15405101e-01\n",
      "Epoch: 26745 mean train loss:  4.19976329e-03, bound:  3.15405011e-01\n",
      "Epoch: 26746 mean train loss:  4.20216983e-03, bound:  3.15405101e-01\n",
      "Epoch: 26747 mean train loss:  4.20158543e-03, bound:  3.15404981e-01\n",
      "Epoch: 26748 mean train loss:  4.19726269e-03, bound:  3.15405071e-01\n",
      "Epoch: 26749 mean train loss:  4.18987731e-03, bound:  3.15404981e-01\n",
      "Epoch: 26750 mean train loss:  4.18216363e-03, bound:  3.15405011e-01\n",
      "Epoch: 26751 mean train loss:  4.17720340e-03, bound:  3.15404952e-01\n",
      "Epoch: 26752 mean train loss:  4.17636009e-03, bound:  3.15404952e-01\n",
      "Epoch: 26753 mean train loss:  4.17883880e-03, bound:  3.15404952e-01\n",
      "Epoch: 26754 mean train loss:  4.18212404e-03, bound:  3.15404892e-01\n",
      "Epoch: 26755 mean train loss:  4.18410311e-03, bound:  3.15404952e-01\n",
      "Epoch: 26756 mean train loss:  4.18349588e-03, bound:  3.15404862e-01\n",
      "Epoch: 26757 mean train loss:  4.18076804e-03, bound:  3.15404892e-01\n",
      "Epoch: 26758 mean train loss:  4.17740270e-03, bound:  3.15404832e-01\n",
      "Epoch: 26759 mean train loss:  4.17512562e-03, bound:  3.15404832e-01\n",
      "Epoch: 26760 mean train loss:  4.17474331e-03, bound:  3.15404832e-01\n",
      "Epoch: 26761 mean train loss:  4.17589536e-03, bound:  3.15404773e-01\n",
      "Epoch: 26762 mean train loss:  4.17738501e-03, bound:  3.15404773e-01\n",
      "Epoch: 26763 mean train loss:  4.17802250e-03, bound:  3.15404743e-01\n",
      "Epoch: 26764 mean train loss:  4.17739293e-03, bound:  3.15404743e-01\n",
      "Epoch: 26765 mean train loss:  4.17588418e-03, bound:  3.15404743e-01\n",
      "Epoch: 26766 mean train loss:  4.17423807e-03, bound:  3.15404743e-01\n",
      "Epoch: 26767 mean train loss:  4.17330023e-03, bound:  3.15404654e-01\n",
      "Epoch: 26768 mean train loss:  4.17325087e-03, bound:  3.15404654e-01\n",
      "Epoch: 26769 mean train loss:  4.17382922e-03, bound:  3.15404654e-01\n",
      "Epoch: 26770 mean train loss:  4.17428697e-03, bound:  3.15404624e-01\n",
      "Epoch: 26771 mean train loss:  4.17422457e-03, bound:  3.15404624e-01\n",
      "Epoch: 26772 mean train loss:  4.17380454e-03, bound:  3.15404624e-01\n",
      "Epoch: 26773 mean train loss:  4.17291326e-03, bound:  3.15404624e-01\n",
      "Epoch: 26774 mean train loss:  4.17218078e-03, bound:  3.15404534e-01\n",
      "Epoch: 26775 mean train loss:  4.17174492e-03, bound:  3.15404534e-01\n",
      "Epoch: 26776 mean train loss:  4.17167973e-03, bound:  3.15404534e-01\n",
      "Epoch: 26777 mean train loss:  4.17188602e-03, bound:  3.15404534e-01\n",
      "Epoch: 26778 mean train loss:  4.17192653e-03, bound:  3.15404505e-01\n",
      "Epoch: 26779 mean train loss:  4.17178636e-03, bound:  3.15404505e-01\n",
      "Epoch: 26780 mean train loss:  4.17143153e-03, bound:  3.15404445e-01\n",
      "Epoch: 26781 mean train loss:  4.17091092e-03, bound:  3.15404445e-01\n",
      "Epoch: 26782 mean train loss:  4.17044386e-03, bound:  3.15404415e-01\n",
      "Epoch: 26783 mean train loss:  4.17022500e-03, bound:  3.15404415e-01\n",
      "Epoch: 26784 mean train loss:  4.17010952e-03, bound:  3.15404385e-01\n",
      "Epoch: 26785 mean train loss:  4.17006901e-03, bound:  3.15404385e-01\n",
      "Epoch: 26786 mean train loss:  4.16998612e-03, bound:  3.15404326e-01\n",
      "Epoch: 26787 mean train loss:  4.16989485e-03, bound:  3.15404326e-01\n",
      "Epoch: 26788 mean train loss:  4.16956842e-03, bound:  3.15404296e-01\n",
      "Epoch: 26789 mean train loss:  4.16924199e-03, bound:  3.15404296e-01\n",
      "Epoch: 26790 mean train loss:  4.16893279e-03, bound:  3.15404236e-01\n",
      "Epoch: 26791 mean train loss:  4.16866224e-03, bound:  3.15404236e-01\n",
      "Epoch: 26792 mean train loss:  4.16848529e-03, bound:  3.15404207e-01\n",
      "Epoch: 26793 mean train loss:  4.16839635e-03, bound:  3.15404207e-01\n",
      "Epoch: 26794 mean train loss:  4.16823337e-03, bound:  3.15404207e-01\n",
      "Epoch: 26795 mean train loss:  4.16810019e-03, bound:  3.15404177e-01\n",
      "Epoch: 26796 mean train loss:  4.16788645e-03, bound:  3.15404177e-01\n",
      "Epoch: 26797 mean train loss:  4.16765269e-03, bound:  3.15404117e-01\n",
      "Epoch: 26798 mean train loss:  4.16737981e-03, bound:  3.15404117e-01\n",
      "Epoch: 26799 mean train loss:  4.16713208e-03, bound:  3.15404087e-01\n",
      "Epoch: 26800 mean train loss:  4.16693743e-03, bound:  3.15404087e-01\n",
      "Epoch: 26801 mean train loss:  4.16677725e-03, bound:  3.15404087e-01\n",
      "Epoch: 26802 mean train loss:  4.16662730e-03, bound:  3.15404058e-01\n",
      "Epoch: 26803 mean train loss:  4.16639168e-03, bound:  3.15403998e-01\n",
      "Epoch: 26804 mean train loss:  4.16624825e-03, bound:  3.15403998e-01\n",
      "Epoch: 26805 mean train loss:  4.16599680e-03, bound:  3.15403998e-01\n",
      "Epoch: 26806 mean train loss:  4.16582264e-03, bound:  3.15403968e-01\n",
      "Epoch: 26807 mean train loss:  4.16560145e-03, bound:  3.15403968e-01\n",
      "Epoch: 26808 mean train loss:  4.16536024e-03, bound:  3.15403938e-01\n",
      "Epoch: 26809 mean train loss:  4.16518888e-03, bound:  3.15403938e-01\n",
      "Epoch: 26810 mean train loss:  4.16501379e-03, bound:  3.15403938e-01\n",
      "Epoch: 26811 mean train loss:  4.16477257e-03, bound:  3.15403938e-01\n",
      "Epoch: 26812 mean train loss:  4.16463148e-03, bound:  3.15403849e-01\n",
      "Epoch: 26813 mean train loss:  4.16442705e-03, bound:  3.15403849e-01\n",
      "Epoch: 26814 mean train loss:  4.16421890e-03, bound:  3.15403819e-01\n",
      "Epoch: 26815 mean train loss:  4.16404661e-03, bound:  3.15403819e-01\n",
      "Epoch: 26816 mean train loss:  4.16384824e-03, bound:  3.15403819e-01\n",
      "Epoch: 26817 mean train loss:  4.16363217e-03, bound:  3.15403789e-01\n",
      "Epoch: 26818 mean train loss:  4.16350085e-03, bound:  3.15403789e-01\n",
      "Epoch: 26819 mean train loss:  4.16323775e-03, bound:  3.15403759e-01\n",
      "Epoch: 26820 mean train loss:  4.16307664e-03, bound:  3.15403759e-01\n",
      "Epoch: 26821 mean train loss:  4.16289177e-03, bound:  3.15403700e-01\n",
      "Epoch: 26822 mean train loss:  4.16264217e-03, bound:  3.15403700e-01\n",
      "Epoch: 26823 mean train loss:  4.16248105e-03, bound:  3.15403670e-01\n",
      "Epoch: 26824 mean train loss:  4.16225614e-03, bound:  3.15403670e-01\n",
      "Epoch: 26825 mean train loss:  4.16209921e-03, bound:  3.15403640e-01\n",
      "Epoch: 26826 mean train loss:  4.16185707e-03, bound:  3.15403640e-01\n",
      "Epoch: 26827 mean train loss:  4.16168151e-03, bound:  3.15403610e-01\n",
      "Epoch: 26828 mean train loss:  4.16151760e-03, bound:  3.15403610e-01\n",
      "Epoch: 26829 mean train loss:  4.16128570e-03, bound:  3.15403581e-01\n",
      "Epoch: 26830 mean train loss:  4.16115532e-03, bound:  3.15403551e-01\n",
      "Epoch: 26831 mean train loss:  4.16088616e-03, bound:  3.15403521e-01\n",
      "Epoch: 26832 mean train loss:  4.16075485e-03, bound:  3.15403521e-01\n",
      "Epoch: 26833 mean train loss:  4.16049641e-03, bound:  3.15403521e-01\n",
      "Epoch: 26834 mean train loss:  4.16030409e-03, bound:  3.15403491e-01\n",
      "Epoch: 26835 mean train loss:  4.16014204e-03, bound:  3.15403491e-01\n",
      "Epoch: 26836 mean train loss:  4.15996090e-03, bound:  3.15403432e-01\n",
      "Epoch: 26837 mean train loss:  4.15973412e-03, bound:  3.15403432e-01\n",
      "Epoch: 26838 mean train loss:  4.15959302e-03, bound:  3.15403402e-01\n",
      "Epoch: 26839 mean train loss:  4.15934855e-03, bound:  3.15403402e-01\n",
      "Epoch: 26840 mean train loss:  4.15915903e-03, bound:  3.15403372e-01\n",
      "Epoch: 26841 mean train loss:  4.15894995e-03, bound:  3.15403372e-01\n",
      "Epoch: 26842 mean train loss:  4.15876089e-03, bound:  3.15403312e-01\n",
      "Epoch: 26843 mean train loss:  4.15855600e-03, bound:  3.15403312e-01\n",
      "Epoch: 26844 mean train loss:  4.15839115e-03, bound:  3.15403283e-01\n",
      "Epoch: 26845 mean train loss:  4.15818160e-03, bound:  3.15403283e-01\n",
      "Epoch: 26846 mean train loss:  4.15799767e-03, bound:  3.15403253e-01\n",
      "Epoch: 26847 mean train loss:  4.15779697e-03, bound:  3.15403253e-01\n",
      "Epoch: 26848 mean train loss:  4.15752269e-03, bound:  3.15403253e-01\n",
      "Epoch: 26849 mean train loss:  4.15737834e-03, bound:  3.15403193e-01\n",
      "Epoch: 26850 mean train loss:  4.15722188e-03, bound:  3.15403193e-01\n",
      "Epoch: 26851 mean train loss:  4.15696949e-03, bound:  3.15403193e-01\n",
      "Epoch: 26852 mean train loss:  4.15682606e-03, bound:  3.15403193e-01\n",
      "Epoch: 26853 mean train loss:  4.15661652e-03, bound:  3.15403134e-01\n",
      "Epoch: 26854 mean train loss:  4.15642746e-03, bound:  3.15403104e-01\n",
      "Epoch: 26855 mean train loss:  4.15624864e-03, bound:  3.15403104e-01\n",
      "Epoch: 26856 mean train loss:  4.15606610e-03, bound:  3.15403104e-01\n",
      "Epoch: 26857 mean train loss:  4.15586401e-03, bound:  3.15403074e-01\n",
      "Epoch: 26858 mean train loss:  4.15564608e-03, bound:  3.15403074e-01\n",
      "Epoch: 26859 mean train loss:  4.15551756e-03, bound:  3.15403044e-01\n",
      "Epoch: 26860 mean train loss:  4.15534386e-03, bound:  3.15402985e-01\n",
      "Epoch: 26861 mean train loss:  4.15516924e-03, bound:  3.15402985e-01\n",
      "Epoch: 26862 mean train loss:  4.15500021e-03, bound:  3.15402985e-01\n",
      "Epoch: 26863 mean train loss:  4.15482651e-03, bound:  3.15402955e-01\n",
      "Epoch: 26864 mean train loss:  4.15466260e-03, bound:  3.15402955e-01\n",
      "Epoch: 26865 mean train loss:  4.15454805e-03, bound:  3.15402955e-01\n",
      "Epoch: 26866 mean train loss:  4.15440509e-03, bound:  3.15402865e-01\n",
      "Epoch: 26867 mean train loss:  4.15429100e-03, bound:  3.15402865e-01\n",
      "Epoch: 26868 mean train loss:  4.15414711e-03, bound:  3.15402865e-01\n",
      "Epoch: 26869 mean train loss:  4.15410567e-03, bound:  3.15402865e-01\n",
      "Epoch: 26870 mean train loss:  4.15411685e-03, bound:  3.15402836e-01\n",
      "Epoch: 26871 mean train loss:  4.15406376e-03, bound:  3.15402836e-01\n",
      "Epoch: 26872 mean train loss:  4.15413734e-03, bound:  3.15402806e-01\n",
      "Epoch: 26873 mean train loss:  4.15425329e-03, bound:  3.15402806e-01\n",
      "Epoch: 26874 mean train loss:  4.15455643e-03, bound:  3.15402746e-01\n",
      "Epoch: 26875 mean train loss:  4.15489543e-03, bound:  3.15402746e-01\n",
      "Epoch: 26876 mean train loss:  4.15532896e-03, bound:  3.15402716e-01\n",
      "Epoch: 26877 mean train loss:  4.15604096e-03, bound:  3.15402716e-01\n",
      "Epoch: 26878 mean train loss:  4.15672967e-03, bound:  3.15402687e-01\n",
      "Epoch: 26879 mean train loss:  4.15763725e-03, bound:  3.15402716e-01\n",
      "Epoch: 26880 mean train loss:  4.15851176e-03, bound:  3.15402627e-01\n",
      "Epoch: 26881 mean train loss:  4.15939325e-03, bound:  3.15402687e-01\n",
      "Epoch: 26882 mean train loss:  4.16012201e-03, bound:  3.15402597e-01\n",
      "Epoch: 26883 mean train loss:  4.16057417e-03, bound:  3.15402627e-01\n",
      "Epoch: 26884 mean train loss:  4.16033855e-03, bound:  3.15402538e-01\n",
      "Epoch: 26885 mean train loss:  4.15946171e-03, bound:  3.15402597e-01\n",
      "Epoch: 26886 mean train loss:  4.15788265e-03, bound:  3.15402538e-01\n",
      "Epoch: 26887 mean train loss:  4.15587053e-03, bound:  3.15402538e-01\n",
      "Epoch: 26888 mean train loss:  4.15370660e-03, bound:  3.15402508e-01\n",
      "Epoch: 26889 mean train loss:  4.15165862e-03, bound:  3.15402508e-01\n",
      "Epoch: 26890 mean train loss:  4.15013125e-03, bound:  3.15402508e-01\n",
      "Epoch: 26891 mean train loss:  4.14925907e-03, bound:  3.15402508e-01\n",
      "Epoch: 26892 mean train loss:  4.14893078e-03, bound:  3.15402418e-01\n",
      "Epoch: 26893 mean train loss:  4.14903183e-03, bound:  3.15402418e-01\n",
      "Epoch: 26894 mean train loss:  4.14945418e-03, bound:  3.15402418e-01\n",
      "Epoch: 26895 mean train loss:  4.14989842e-03, bound:  3.15402389e-01\n",
      "Epoch: 26896 mean train loss:  4.15018480e-03, bound:  3.15402389e-01\n",
      "Epoch: 26897 mean train loss:  4.15017782e-03, bound:  3.15402389e-01\n",
      "Epoch: 26898 mean train loss:  4.14987700e-03, bound:  3.15402389e-01\n",
      "Epoch: 26899 mean train loss:  4.14930843e-03, bound:  3.15402299e-01\n",
      "Epoch: 26900 mean train loss:  4.14862065e-03, bound:  3.15402299e-01\n",
      "Epoch: 26901 mean train loss:  4.14784066e-03, bound:  3.15402299e-01\n",
      "Epoch: 26902 mean train loss:  4.14732005e-03, bound:  3.15402299e-01\n",
      "Epoch: 26903 mean train loss:  4.14685952e-03, bound:  3.15402269e-01\n",
      "Epoch: 26904 mean train loss:  4.14663367e-03, bound:  3.15402240e-01\n",
      "Epoch: 26905 mean train loss:  4.14651493e-03, bound:  3.15402210e-01\n",
      "Epoch: 26906 mean train loss:  4.14653029e-03, bound:  3.15402180e-01\n",
      "Epoch: 26907 mean train loss:  4.14658943e-03, bound:  3.15402180e-01\n",
      "Epoch: 26908 mean train loss:  4.14668582e-03, bound:  3.15402150e-01\n",
      "Epoch: 26909 mean train loss:  4.14665136e-03, bound:  3.15402150e-01\n",
      "Epoch: 26910 mean train loss:  4.14664112e-03, bound:  3.15402091e-01\n",
      "Epoch: 26911 mean train loss:  4.14641341e-03, bound:  3.15402091e-01\n",
      "Epoch: 26912 mean train loss:  4.14613308e-03, bound:  3.15402091e-01\n",
      "Epoch: 26913 mean train loss:  4.14572144e-03, bound:  3.15402091e-01\n",
      "Epoch: 26914 mean train loss:  4.14532237e-03, bound:  3.15402031e-01\n",
      "Epoch: 26915 mean train loss:  4.14488837e-03, bound:  3.15402031e-01\n",
      "Epoch: 26916 mean train loss:  4.14448744e-03, bound:  3.15402031e-01\n",
      "Epoch: 26917 mean train loss:  4.14417265e-03, bound:  3.15401971e-01\n",
      "Epoch: 26918 mean train loss:  4.14387649e-03, bound:  3.15401971e-01\n",
      "Epoch: 26919 mean train loss:  4.14367346e-03, bound:  3.15401942e-01\n",
      "Epoch: 26920 mean train loss:  4.14347602e-03, bound:  3.15401942e-01\n",
      "Epoch: 26921 mean train loss:  4.14325856e-03, bound:  3.15401912e-01\n",
      "Epoch: 26922 mean train loss:  4.14317334e-03, bound:  3.15401912e-01\n",
      "Epoch: 26923 mean train loss:  4.14302852e-03, bound:  3.15401912e-01\n",
      "Epoch: 26924 mean train loss:  4.14279010e-03, bound:  3.15401852e-01\n",
      "Epoch: 26925 mean train loss:  4.14263410e-03, bound:  3.15401852e-01\n",
      "Epoch: 26926 mean train loss:  4.14245250e-03, bound:  3.15401822e-01\n",
      "Epoch: 26927 mean train loss:  4.14230581e-03, bound:  3.15401822e-01\n",
      "Epoch: 26928 mean train loss:  4.14214097e-03, bound:  3.15401822e-01\n",
      "Epoch: 26929 mean train loss:  4.14199103e-03, bound:  3.15401793e-01\n",
      "Epoch: 26930 mean train loss:  4.14177310e-03, bound:  3.15401733e-01\n",
      "Epoch: 26931 mean train loss:  4.14154492e-03, bound:  3.15401703e-01\n",
      "Epoch: 26932 mean train loss:  4.14130744e-03, bound:  3.15401733e-01\n",
      "Epoch: 26933 mean train loss:  4.14111558e-03, bound:  3.15401703e-01\n",
      "Epoch: 26934 mean train loss:  4.14092699e-03, bound:  3.15401703e-01\n",
      "Epoch: 26935 mean train loss:  4.14067740e-03, bound:  3.15401703e-01\n",
      "Epoch: 26936 mean train loss:  4.14042594e-03, bound:  3.15401614e-01\n",
      "Epoch: 26937 mean train loss:  4.14025271e-03, bound:  3.15401614e-01\n",
      "Epoch: 26938 mean train loss:  4.14000312e-03, bound:  3.15401584e-01\n",
      "Epoch: 26939 mean train loss:  4.13978426e-03, bound:  3.15401584e-01\n",
      "Epoch: 26940 mean train loss:  4.13959287e-03, bound:  3.15401584e-01\n",
      "Epoch: 26941 mean train loss:  4.13939450e-03, bound:  3.15401584e-01\n",
      "Epoch: 26942 mean train loss:  4.13920544e-03, bound:  3.15401524e-01\n",
      "Epoch: 26943 mean train loss:  4.13904618e-03, bound:  3.15401524e-01\n",
      "Epoch: 26944 mean train loss:  4.13875841e-03, bound:  3.15401524e-01\n",
      "Epoch: 26945 mean train loss:  4.13863733e-03, bound:  3.15401465e-01\n",
      "Epoch: 26946 mean train loss:  4.13841242e-03, bound:  3.15401465e-01\n",
      "Epoch: 26947 mean train loss:  4.13819263e-03, bound:  3.15401465e-01\n",
      "Epoch: 26948 mean train loss:  4.13806876e-03, bound:  3.15401465e-01\n",
      "Epoch: 26949 mean train loss:  4.13782196e-03, bound:  3.15401405e-01\n",
      "Epoch: 26950 mean train loss:  4.13762778e-03, bound:  3.15401405e-01\n",
      "Epoch: 26951 mean train loss:  4.13743779e-03, bound:  3.15401375e-01\n",
      "Epoch: 26952 mean train loss:  4.13722498e-03, bound:  3.15401375e-01\n",
      "Epoch: 26953 mean train loss:  4.13702847e-03, bound:  3.15401345e-01\n",
      "Epoch: 26954 mean train loss:  4.13678400e-03, bound:  3.15401345e-01\n",
      "Epoch: 26955 mean train loss:  4.13672626e-03, bound:  3.15401286e-01\n",
      "Epoch: 26956 mean train loss:  4.13653022e-03, bound:  3.15401286e-01\n",
      "Epoch: 26957 mean train loss:  4.13638353e-03, bound:  3.15401256e-01\n",
      "Epoch: 26958 mean train loss:  4.13624942e-03, bound:  3.15401256e-01\n",
      "Epoch: 26959 mean train loss:  4.13612137e-03, bound:  3.15401196e-01\n",
      "Epoch: 26960 mean train loss:  4.13601566e-03, bound:  3.15401196e-01\n",
      "Epoch: 26961 mean train loss:  4.13605245e-03, bound:  3.15401167e-01\n",
      "Epoch: 26962 mean train loss:  4.13605804e-03, bound:  3.15401167e-01\n",
      "Epoch: 26963 mean train loss:  4.13618702e-03, bound:  3.15401167e-01\n",
      "Epoch: 26964 mean train loss:  4.13643848e-03, bound:  3.15401167e-01\n",
      "Epoch: 26965 mean train loss:  4.13682032e-03, bound:  3.15401137e-01\n",
      "Epoch: 26966 mean train loss:  4.13750065e-03, bound:  3.15401137e-01\n",
      "Epoch: 26967 mean train loss:  4.13834676e-03, bound:  3.15401047e-01\n",
      "Epoch: 26968 mean train loss:  4.13971394e-03, bound:  3.15401077e-01\n",
      "Epoch: 26969 mean train loss:  4.14165435e-03, bound:  3.15401047e-01\n",
      "Epoch: 26970 mean train loss:  4.14430164e-03, bound:  3.15401047e-01\n",
      "Epoch: 26971 mean train loss:  4.14744159e-03, bound:  3.15400958e-01\n",
      "Epoch: 26972 mean train loss:  4.15081298e-03, bound:  3.15401047e-01\n",
      "Epoch: 26973 mean train loss:  4.15365957e-03, bound:  3.15400958e-01\n",
      "Epoch: 26974 mean train loss:  4.15498903e-03, bound:  3.15401018e-01\n",
      "Epoch: 26975 mean train loss:  4.15358413e-03, bound:  3.15400898e-01\n",
      "Epoch: 26976 mean train loss:  4.14897921e-03, bound:  3.15400958e-01\n",
      "Epoch: 26977 mean train loss:  4.14236169e-03, bound:  3.15400898e-01\n",
      "Epoch: 26978 mean train loss:  4.13618609e-03, bound:  3.15400928e-01\n",
      "Epoch: 26979 mean train loss:  4.13243053e-03, bound:  3.15400898e-01\n",
      "Epoch: 26980 mean train loss:  4.13209200e-03, bound:  3.15400898e-01\n",
      "Epoch: 26981 mean train loss:  4.13426245e-03, bound:  3.15400898e-01\n",
      "Epoch: 26982 mean train loss:  4.13722871e-03, bound:  3.15400839e-01\n",
      "Epoch: 26983 mean train loss:  4.13911417e-03, bound:  3.15400898e-01\n",
      "Epoch: 26984 mean train loss:  4.13876446e-03, bound:  3.15400779e-01\n",
      "Epoch: 26985 mean train loss:  4.13644081e-03, bound:  3.15400809e-01\n",
      "Epoch: 26986 mean train loss:  4.13336745e-03, bound:  3.15400749e-01\n",
      "Epoch: 26987 mean train loss:  4.13099816e-03, bound:  3.15400749e-01\n",
      "Epoch: 26988 mean train loss:  4.13026521e-03, bound:  3.15400749e-01\n",
      "Epoch: 26989 mean train loss:  4.13102098e-03, bound:  3.15400720e-01\n",
      "Epoch: 26990 mean train loss:  4.13226429e-03, bound:  3.15400720e-01\n",
      "Epoch: 26991 mean train loss:  4.13311087e-03, bound:  3.15400660e-01\n",
      "Epoch: 26992 mean train loss:  4.13292181e-03, bound:  3.15400660e-01\n",
      "Epoch: 26993 mean train loss:  4.13179863e-03, bound:  3.15400630e-01\n",
      "Epoch: 26994 mean train loss:  4.13028756e-03, bound:  3.15400630e-01\n",
      "Epoch: 26995 mean train loss:  4.12921142e-03, bound:  3.15400600e-01\n",
      "Epoch: 26996 mean train loss:  4.12878254e-03, bound:  3.15400600e-01\n",
      "Epoch: 26997 mean train loss:  4.12893901e-03, bound:  3.15400600e-01\n",
      "Epoch: 26998 mean train loss:  4.12934693e-03, bound:  3.15400571e-01\n",
      "Epoch: 26999 mean train loss:  4.12960770e-03, bound:  3.15400571e-01\n",
      "Epoch: 27000 mean train loss:  4.12955508e-03, bound:  3.15400511e-01\n",
      "Epoch: 27001 mean train loss:  4.12897998e-03, bound:  3.15400511e-01\n",
      "Epoch: 27002 mean train loss:  4.12822701e-03, bound:  3.15400481e-01\n",
      "Epoch: 27003 mean train loss:  4.12763702e-03, bound:  3.15400481e-01\n",
      "Epoch: 27004 mean train loss:  4.12722025e-03, bound:  3.15400451e-01\n",
      "Epoch: 27005 mean train loss:  4.12716810e-03, bound:  3.15400451e-01\n",
      "Epoch: 27006 mean train loss:  4.12722025e-03, bound:  3.15400451e-01\n",
      "Epoch: 27007 mean train loss:  4.12731292e-03, bound:  3.15400362e-01\n",
      "Epoch: 27008 mean train loss:  4.12715971e-03, bound:  3.15400362e-01\n",
      "Epoch: 27009 mean train loss:  4.12687287e-03, bound:  3.15400362e-01\n",
      "Epoch: 27010 mean train loss:  4.12641931e-03, bound:  3.15400332e-01\n",
      "Epoch: 27011 mean train loss:  4.12600162e-03, bound:  3.15400332e-01\n",
      "Epoch: 27012 mean train loss:  4.12568916e-03, bound:  3.15400273e-01\n",
      "Epoch: 27013 mean train loss:  4.12557181e-03, bound:  3.15400273e-01\n",
      "Epoch: 27014 mean train loss:  4.12550522e-03, bound:  3.15400273e-01\n",
      "Epoch: 27015 mean train loss:  4.12547402e-03, bound:  3.15400273e-01\n",
      "Epoch: 27016 mean train loss:  4.12533153e-03, bound:  3.15400213e-01\n",
      "Epoch: 27017 mean train loss:  4.12512803e-03, bound:  3.15400213e-01\n",
      "Epoch: 27018 mean train loss:  4.12488403e-03, bound:  3.15400153e-01\n",
      "Epoch: 27019 mean train loss:  4.12455061e-03, bound:  3.15400153e-01\n",
      "Epoch: 27020 mean train loss:  4.12429450e-03, bound:  3.15400153e-01\n",
      "Epoch: 27021 mean train loss:  4.12401836e-03, bound:  3.15400153e-01\n",
      "Epoch: 27022 mean train loss:  4.12378274e-03, bound:  3.15400153e-01\n",
      "Epoch: 27023 mean train loss:  4.12370125e-03, bound:  3.15400094e-01\n",
      "Epoch: 27024 mean train loss:  4.12347633e-03, bound:  3.15400094e-01\n",
      "Epoch: 27025 mean train loss:  4.12334269e-03, bound:  3.15400034e-01\n",
      "Epoch: 27026 mean train loss:  4.12316946e-03, bound:  3.15400034e-01\n",
      "Epoch: 27027 mean train loss:  4.12296597e-03, bound:  3.15400034e-01\n",
      "Epoch: 27028 mean train loss:  4.12275037e-03, bound:  3.15400034e-01\n",
      "Epoch: 27029 mean train loss:  4.12253989e-03, bound:  3.15399975e-01\n",
      "Epoch: 27030 mean train loss:  4.12230752e-03, bound:  3.15399975e-01\n",
      "Epoch: 27031 mean train loss:  4.12209472e-03, bound:  3.15399945e-01\n",
      "Epoch: 27032 mean train loss:  4.12192289e-03, bound:  3.15399945e-01\n",
      "Epoch: 27033 mean train loss:  4.12184373e-03, bound:  3.15399945e-01\n",
      "Epoch: 27034 mean train loss:  4.12165327e-03, bound:  3.15399915e-01\n",
      "Epoch: 27035 mean train loss:  4.12150705e-03, bound:  3.15399915e-01\n",
      "Epoch: 27036 mean train loss:  4.12130728e-03, bound:  3.15399885e-01\n",
      "Epoch: 27037 mean train loss:  4.12105769e-03, bound:  3.15399885e-01\n",
      "Epoch: 27038 mean train loss:  4.12084674e-03, bound:  3.15399826e-01\n",
      "Epoch: 27039 mean train loss:  4.12062835e-03, bound:  3.15399826e-01\n",
      "Epoch: 27040 mean train loss:  4.12035640e-03, bound:  3.15399796e-01\n",
      "Epoch: 27041 mean train loss:  4.12020879e-03, bound:  3.15399796e-01\n",
      "Epoch: 27042 mean train loss:  4.11997177e-03, bound:  3.15399796e-01\n",
      "Epoch: 27043 mean train loss:  4.11980180e-03, bound:  3.15399766e-01\n",
      "Epoch: 27044 mean train loss:  4.11960110e-03, bound:  3.15399706e-01\n",
      "Epoch: 27045 mean train loss:  4.11948794e-03, bound:  3.15399706e-01\n",
      "Epoch: 27046 mean train loss:  4.11927886e-03, bound:  3.15399706e-01\n",
      "Epoch: 27047 mean train loss:  4.11910657e-03, bound:  3.15399677e-01\n",
      "Epoch: 27048 mean train loss:  4.11891192e-03, bound:  3.15399677e-01\n",
      "Epoch: 27049 mean train loss:  4.11866093e-03, bound:  3.15399647e-01\n",
      "Epoch: 27050 mean train loss:  4.11854545e-03, bound:  3.15399647e-01\n",
      "Epoch: 27051 mean train loss:  4.11832891e-03, bound:  3.15399587e-01\n",
      "Epoch: 27052 mean train loss:  4.11810633e-03, bound:  3.15399587e-01\n",
      "Epoch: 27053 mean train loss:  4.11793729e-03, bound:  3.15399557e-01\n",
      "Epoch: 27054 mean train loss:  4.11767419e-03, bound:  3.15399557e-01\n",
      "Epoch: 27055 mean train loss:  4.11752472e-03, bound:  3.15399528e-01\n",
      "Epoch: 27056 mean train loss:  4.11733659e-03, bound:  3.15399528e-01\n",
      "Epoch: 27057 mean train loss:  4.11713310e-03, bound:  3.15399468e-01\n",
      "Epoch: 27058 mean train loss:  4.11693146e-03, bound:  3.15399468e-01\n",
      "Epoch: 27059 mean train loss:  4.11677314e-03, bound:  3.15399468e-01\n",
      "Epoch: 27060 mean train loss:  4.11652913e-03, bound:  3.15399468e-01\n",
      "Epoch: 27061 mean train loss:  4.11639409e-03, bound:  3.15399468e-01\n",
      "Epoch: 27062 mean train loss:  4.11617197e-03, bound:  3.15399379e-01\n",
      "Epoch: 27063 mean train loss:  4.11596987e-03, bound:  3.15399379e-01\n",
      "Epoch: 27064 mean train loss:  4.11577756e-03, bound:  3.15399349e-01\n",
      "Epoch: 27065 mean train loss:  4.11555590e-03, bound:  3.15399349e-01\n",
      "Epoch: 27066 mean train loss:  4.11540177e-03, bound:  3.15399349e-01\n",
      "Epoch: 27067 mean train loss:  4.11522668e-03, bound:  3.15399349e-01\n",
      "Epoch: 27068 mean train loss:  4.11502225e-03, bound:  3.15399289e-01\n",
      "Epoch: 27069 mean train loss:  4.11484716e-03, bound:  3.15399289e-01\n",
      "Epoch: 27070 mean train loss:  4.11462830e-03, bound:  3.15399230e-01\n",
      "Epoch: 27071 mean train loss:  4.11446253e-03, bound:  3.15399230e-01\n",
      "Epoch: 27072 mean train loss:  4.11423808e-03, bound:  3.15399230e-01\n",
      "Epoch: 27073 mean train loss:  4.11411049e-03, bound:  3.15399230e-01\n",
      "Epoch: 27074 mean train loss:  4.11389815e-03, bound:  3.15399200e-01\n",
      "Epoch: 27075 mean train loss:  4.11371281e-03, bound:  3.15399170e-01\n",
      "Epoch: 27076 mean train loss:  4.11352795e-03, bound:  3.15399140e-01\n",
      "Epoch: 27077 mean train loss:  4.11327928e-03, bound:  3.15399140e-01\n",
      "Epoch: 27078 mean train loss:  4.11307579e-03, bound:  3.15399110e-01\n",
      "Epoch: 27079 mean train loss:  4.11292352e-03, bound:  3.15399110e-01\n",
      "Epoch: 27080 mean train loss:  4.11274843e-03, bound:  3.15399110e-01\n",
      "Epoch: 27081 mean train loss:  4.11256868e-03, bound:  3.15399051e-01\n",
      "Epoch: 27082 mean train loss:  4.11236798e-03, bound:  3.15399051e-01\n",
      "Epoch: 27083 mean train loss:  4.11220966e-03, bound:  3.15399021e-01\n",
      "Epoch: 27084 mean train loss:  4.11202572e-03, bound:  3.15399021e-01\n",
      "Epoch: 27085 mean train loss:  4.11187066e-03, bound:  3.15398991e-01\n",
      "Epoch: 27086 mean train loss:  4.11168998e-03, bound:  3.15398991e-01\n",
      "Epoch: 27087 mean train loss:  4.11142968e-03, bound:  3.15398932e-01\n",
      "Epoch: 27088 mean train loss:  4.11133002e-03, bound:  3.15398932e-01\n",
      "Epoch: 27089 mean train loss:  4.11111489e-03, bound:  3.15398902e-01\n",
      "Epoch: 27090 mean train loss:  4.11096867e-03, bound:  3.15398902e-01\n",
      "Epoch: 27091 mean train loss:  4.11078986e-03, bound:  3.15398872e-01\n",
      "Epoch: 27092 mean train loss:  4.11052210e-03, bound:  3.15398872e-01\n",
      "Epoch: 27093 mean train loss:  4.11034236e-03, bound:  3.15398812e-01\n",
      "Epoch: 27094 mean train loss:  4.11017193e-03, bound:  3.15398812e-01\n",
      "Epoch: 27095 mean train loss:  4.11003502e-03, bound:  3.15398812e-01\n",
      "Epoch: 27096 mean train loss:  4.10989393e-03, bound:  3.15398782e-01\n",
      "Epoch: 27097 mean train loss:  4.10984829e-03, bound:  3.15398753e-01\n",
      "Epoch: 27098 mean train loss:  4.10984177e-03, bound:  3.15398753e-01\n",
      "Epoch: 27099 mean train loss:  4.10977937e-03, bound:  3.15398753e-01\n",
      "Epoch: 27100 mean train loss:  4.10984317e-03, bound:  3.15398753e-01\n",
      "Epoch: 27101 mean train loss:  4.11001593e-03, bound:  3.15398693e-01\n",
      "Epoch: 27102 mean train loss:  4.11034701e-03, bound:  3.15398693e-01\n",
      "Epoch: 27103 mean train loss:  4.11084807e-03, bound:  3.15398663e-01\n",
      "Epoch: 27104 mean train loss:  4.11166716e-03, bound:  3.15398663e-01\n",
      "Epoch: 27105 mean train loss:  4.11279360e-03, bound:  3.15398604e-01\n",
      "Epoch: 27106 mean train loss:  4.11443599e-03, bound:  3.15398663e-01\n",
      "Epoch: 27107 mean train loss:  4.11652820e-03, bound:  3.15398544e-01\n",
      "Epoch: 27108 mean train loss:  4.11922298e-03, bound:  3.15398604e-01\n",
      "Epoch: 27109 mean train loss:  4.12219577e-03, bound:  3.15398544e-01\n",
      "Epoch: 27110 mean train loss:  4.12488915e-03, bound:  3.15398544e-01\n",
      "Epoch: 27111 mean train loss:  4.12650872e-03, bound:  3.15398484e-01\n",
      "Epoch: 27112 mean train loss:  4.12615668e-03, bound:  3.15398544e-01\n",
      "Epoch: 27113 mean train loss:  4.12319833e-03, bound:  3.15398455e-01\n",
      "Epoch: 27114 mean train loss:  4.11817105e-03, bound:  3.15398484e-01\n",
      "Epoch: 27115 mean train loss:  4.11244761e-03, bound:  3.15398425e-01\n",
      "Epoch: 27116 mean train loss:  4.10776958e-03, bound:  3.15398455e-01\n",
      "Epoch: 27117 mean train loss:  4.10561915e-03, bound:  3.15398425e-01\n",
      "Epoch: 27118 mean train loss:  4.10603778e-03, bound:  3.15398425e-01\n",
      "Epoch: 27119 mean train loss:  4.10805922e-03, bound:  3.15398425e-01\n",
      "Epoch: 27120 mean train loss:  4.11026692e-03, bound:  3.15398365e-01\n",
      "Epoch: 27121 mean train loss:  4.11135657e-03, bound:  3.15398365e-01\n",
      "Epoch: 27122 mean train loss:  4.11074283e-03, bound:  3.15398335e-01\n",
      "Epoch: 27123 mean train loss:  4.10870416e-03, bound:  3.15398365e-01\n",
      "Epoch: 27124 mean train loss:  4.10627294e-03, bound:  3.15398335e-01\n",
      "Epoch: 27125 mean train loss:  4.10446851e-03, bound:  3.15398306e-01\n",
      "Epoch: 27126 mean train loss:  4.10386315e-03, bound:  3.15398306e-01\n",
      "Epoch: 27127 mean train loss:  4.10434790e-03, bound:  3.15398246e-01\n",
      "Epoch: 27128 mean train loss:  4.10522660e-03, bound:  3.15398246e-01\n",
      "Epoch: 27129 mean train loss:  4.10589809e-03, bound:  3.15398216e-01\n",
      "Epoch: 27130 mean train loss:  4.10596421e-03, bound:  3.15398216e-01\n",
      "Epoch: 27131 mean train loss:  4.10521915e-03, bound:  3.15398186e-01\n",
      "Epoch: 27132 mean train loss:  4.10409039e-03, bound:  3.15398186e-01\n",
      "Epoch: 27133 mean train loss:  4.10303473e-03, bound:  3.15398127e-01\n",
      "Epoch: 27134 mean train loss:  4.10233159e-03, bound:  3.15398127e-01\n",
      "Epoch: 27135 mean train loss:  4.10228223e-03, bound:  3.15398127e-01\n",
      "Epoch: 27136 mean train loss:  4.10246756e-03, bound:  3.15398097e-01\n",
      "Epoch: 27137 mean train loss:  4.10272880e-03, bound:  3.15398097e-01\n",
      "Epoch: 27138 mean train loss:  4.10283031e-03, bound:  3.15398037e-01\n",
      "Epoch: 27139 mean train loss:  4.10257140e-03, bound:  3.15398037e-01\n",
      "Epoch: 27140 mean train loss:  4.10201401e-03, bound:  3.15398008e-01\n",
      "Epoch: 27141 mean train loss:  4.10141470e-03, bound:  3.15398008e-01\n",
      "Epoch: 27142 mean train loss:  4.10099793e-03, bound:  3.15397978e-01\n",
      "Epoch: 27143 mean train loss:  4.10065148e-03, bound:  3.15397978e-01\n",
      "Epoch: 27144 mean train loss:  4.10053879e-03, bound:  3.15397918e-01\n",
      "Epoch: 27145 mean train loss:  4.10048198e-03, bound:  3.15397918e-01\n",
      "Epoch: 27146 mean train loss:  4.10047313e-03, bound:  3.15397918e-01\n",
      "Epoch: 27147 mean train loss:  4.10034414e-03, bound:  3.15397888e-01\n",
      "Epoch: 27148 mean train loss:  4.10010479e-03, bound:  3.15397888e-01\n",
      "Epoch: 27149 mean train loss:  4.09985473e-03, bound:  3.15397859e-01\n",
      "Epoch: 27150 mean train loss:  4.09952132e-03, bound:  3.15397859e-01\n",
      "Epoch: 27151 mean train loss:  4.09918977e-03, bound:  3.15397799e-01\n",
      "Epoch: 27152 mean train loss:  4.09897510e-03, bound:  3.15397799e-01\n",
      "Epoch: 27153 mean train loss:  4.09880793e-03, bound:  3.15397799e-01\n",
      "Epoch: 27154 mean train loss:  4.09864401e-03, bound:  3.15397739e-01\n",
      "Epoch: 27155 mean train loss:  4.09859791e-03, bound:  3.15397739e-01\n",
      "Epoch: 27156 mean train loss:  4.09845868e-03, bound:  3.15397739e-01\n",
      "Epoch: 27157 mean train loss:  4.09827894e-03, bound:  3.15397739e-01\n",
      "Epoch: 27158 mean train loss:  4.09806985e-03, bound:  3.15397680e-01\n",
      "Epoch: 27159 mean train loss:  4.09782492e-03, bound:  3.15397680e-01\n",
      "Epoch: 27160 mean train loss:  4.09756554e-03, bound:  3.15397680e-01\n",
      "Epoch: 27161 mean train loss:  4.09736950e-03, bound:  3.15397650e-01\n",
      "Epoch: 27162 mean train loss:  4.09710547e-03, bound:  3.15397650e-01\n",
      "Epoch: 27163 mean train loss:  4.09689639e-03, bound:  3.15397590e-01\n",
      "Epoch: 27164 mean train loss:  4.09672596e-03, bound:  3.15397590e-01\n",
      "Epoch: 27165 mean train loss:  4.09657834e-03, bound:  3.15397561e-01\n",
      "Epoch: 27166 mean train loss:  4.09643259e-03, bound:  3.15397561e-01\n",
      "Epoch: 27167 mean train loss:  4.09633480e-03, bound:  3.15397531e-01\n",
      "Epoch: 27168 mean train loss:  4.09611128e-03, bound:  3.15397531e-01\n",
      "Epoch: 27169 mean train loss:  4.09589708e-03, bound:  3.15397471e-01\n",
      "Epoch: 27170 mean train loss:  4.09567310e-03, bound:  3.15397471e-01\n",
      "Epoch: 27171 mean train loss:  4.09551105e-03, bound:  3.15397441e-01\n",
      "Epoch: 27172 mean train loss:  4.09518881e-03, bound:  3.15397441e-01\n",
      "Epoch: 27173 mean train loss:  4.09508590e-03, bound:  3.15397412e-01\n",
      "Epoch: 27174 mean train loss:  4.09484189e-03, bound:  3.15397412e-01\n",
      "Epoch: 27175 mean train loss:  4.09464026e-03, bound:  3.15397412e-01\n",
      "Epoch: 27176 mean train loss:  4.09444096e-03, bound:  3.15397352e-01\n",
      "Epoch: 27177 mean train loss:  4.09429055e-03, bound:  3.15397352e-01\n",
      "Epoch: 27178 mean train loss:  4.09408519e-03, bound:  3.15397322e-01\n",
      "Epoch: 27179 mean train loss:  4.09400137e-03, bound:  3.15397322e-01\n",
      "Epoch: 27180 mean train loss:  4.09373036e-03, bound:  3.15397292e-01\n",
      "Epoch: 27181 mean train loss:  4.09360230e-03, bound:  3.15397292e-01\n",
      "Epoch: 27182 mean train loss:  4.09335503e-03, bound:  3.15397292e-01\n",
      "Epoch: 27183 mean train loss:  4.09312872e-03, bound:  3.15397233e-01\n",
      "Epoch: 27184 mean train loss:  4.09294944e-03, bound:  3.15397233e-01\n",
      "Epoch: 27185 mean train loss:  4.09277761e-03, bound:  3.15397233e-01\n",
      "Epoch: 27186 mean train loss:  4.09255177e-03, bound:  3.15397203e-01\n",
      "Epoch: 27187 mean train loss:  4.09235992e-03, bound:  3.15397173e-01\n",
      "Epoch: 27188 mean train loss:  4.09215363e-03, bound:  3.15397173e-01\n",
      "Epoch: 27189 mean train loss:  4.09198226e-03, bound:  3.15397114e-01\n",
      "Epoch: 27190 mean train loss:  4.09178017e-03, bound:  3.15397114e-01\n",
      "Epoch: 27191 mean train loss:  4.09165677e-03, bound:  3.15397114e-01\n",
      "Epoch: 27192 mean train loss:  4.09148820e-03, bound:  3.15397114e-01\n",
      "Epoch: 27193 mean train loss:  4.09133965e-03, bound:  3.15397084e-01\n",
      "Epoch: 27194 mean train loss:  4.09116177e-03, bound:  3.15397084e-01\n",
      "Epoch: 27195 mean train loss:  4.09101462e-03, bound:  3.15396994e-01\n",
      "Epoch: 27196 mean train loss:  4.09089774e-03, bound:  3.15396994e-01\n",
      "Epoch: 27197 mean train loss:  4.09075897e-03, bound:  3.15396994e-01\n",
      "Epoch: 27198 mean train loss:  4.09052335e-03, bound:  3.15396994e-01\n",
      "Epoch: 27199 mean train loss:  4.09034127e-03, bound:  3.15396965e-01\n",
      "Epoch: 27200 mean train loss:  4.09012614e-03, bound:  3.15396965e-01\n",
      "Epoch: 27201 mean train loss:  4.08998271e-03, bound:  3.15396965e-01\n",
      "Epoch: 27202 mean train loss:  4.08980018e-03, bound:  3.15396905e-01\n",
      "Epoch: 27203 mean train loss:  4.08964464e-03, bound:  3.15396875e-01\n",
      "Epoch: 27204 mean train loss:  4.08951240e-03, bound:  3.15396875e-01\n",
      "Epoch: 27205 mean train loss:  4.08929121e-03, bound:  3.15396845e-01\n",
      "Epoch: 27206 mean train loss:  4.08906024e-03, bound:  3.15396845e-01\n",
      "Epoch: 27207 mean train loss:  4.08882368e-03, bound:  3.15396786e-01\n",
      "Epoch: 27208 mean train loss:  4.08862485e-03, bound:  3.15396786e-01\n",
      "Epoch: 27209 mean train loss:  4.08842089e-03, bound:  3.15396786e-01\n",
      "Epoch: 27210 mean train loss:  4.08822205e-03, bound:  3.15396756e-01\n",
      "Epoch: 27211 mean train loss:  4.08799108e-03, bound:  3.15396756e-01\n",
      "Epoch: 27212 mean train loss:  4.08781739e-03, bound:  3.15396756e-01\n",
      "Epoch: 27213 mean train loss:  4.08763997e-03, bound:  3.15396726e-01\n",
      "Epoch: 27214 mean train loss:  4.08755476e-03, bound:  3.15396726e-01\n",
      "Epoch: 27215 mean train loss:  4.08735638e-03, bound:  3.15396667e-01\n",
      "Epoch: 27216 mean train loss:  4.08726931e-03, bound:  3.15396667e-01\n",
      "Epoch: 27217 mean train loss:  4.08712728e-03, bound:  3.15396637e-01\n",
      "Epoch: 27218 mean train loss:  4.08710074e-03, bound:  3.15396637e-01\n",
      "Epoch: 27219 mean train loss:  4.08703694e-03, bound:  3.15396607e-01\n",
      "Epoch: 27220 mean train loss:  4.08699270e-03, bound:  3.15396607e-01\n",
      "Epoch: 27221 mean train loss:  4.08697734e-03, bound:  3.15396547e-01\n",
      "Epoch: 27222 mean train loss:  4.08700667e-03, bound:  3.15396547e-01\n",
      "Epoch: 27223 mean train loss:  4.08726186e-03, bound:  3.15396518e-01\n",
      "Epoch: 27224 mean train loss:  4.08751238e-03, bound:  3.15396518e-01\n",
      "Epoch: 27225 mean train loss:  4.08789981e-03, bound:  3.15396488e-01\n",
      "Epoch: 27226 mean train loss:  4.08843718e-03, bound:  3.15396488e-01\n",
      "Epoch: 27227 mean train loss:  4.08906396e-03, bound:  3.15396428e-01\n",
      "Epoch: 27228 mean train loss:  4.08981647e-03, bound:  3.15396458e-01\n",
      "Epoch: 27229 mean train loss:  4.09057876e-03, bound:  3.15396398e-01\n",
      "Epoch: 27230 mean train loss:  4.09135967e-03, bound:  3.15396428e-01\n",
      "Epoch: 27231 mean train loss:  4.09203721e-03, bound:  3.15396398e-01\n",
      "Epoch: 27232 mean train loss:  4.09255736e-03, bound:  3.15396398e-01\n",
      "Epoch: 27233 mean train loss:  4.09260485e-03, bound:  3.15396339e-01\n",
      "Epoch: 27234 mean train loss:  4.09207353e-03, bound:  3.15396398e-01\n",
      "Epoch: 27235 mean train loss:  4.09092614e-03, bound:  3.15396309e-01\n",
      "Epoch: 27236 mean train loss:  4.08932799e-03, bound:  3.15396309e-01\n",
      "Epoch: 27237 mean train loss:  4.08739783e-03, bound:  3.15396279e-01\n",
      "Epoch: 27238 mean train loss:  4.08537406e-03, bound:  3.15396309e-01\n",
      "Epoch: 27239 mean train loss:  4.08366555e-03, bound:  3.15396279e-01\n",
      "Epoch: 27240 mean train loss:  4.08254284e-03, bound:  3.15396279e-01\n",
      "Epoch: 27241 mean train loss:  4.08200687e-03, bound:  3.15396190e-01\n",
      "Epoch: 27242 mean train loss:  4.08204272e-03, bound:  3.15396190e-01\n",
      "Epoch: 27243 mean train loss:  4.08237940e-03, bound:  3.15396190e-01\n",
      "Epoch: 27244 mean train loss:  4.08295076e-03, bound:  3.15396160e-01\n",
      "Epoch: 27245 mean train loss:  4.08343552e-03, bound:  3.15396190e-01\n",
      "Epoch: 27246 mean train loss:  4.08372423e-03, bound:  3.15396130e-01\n",
      "Epoch: 27247 mean train loss:  4.08372097e-03, bound:  3.15396130e-01\n",
      "Epoch: 27248 mean train loss:  4.08332003e-03, bound:  3.15396100e-01\n",
      "Epoch: 27249 mean train loss:  4.08271840e-03, bound:  3.15396100e-01\n",
      "Epoch: 27250 mean train loss:  4.08192258e-03, bound:  3.15396070e-01\n",
      "Epoch: 27251 mean train loss:  4.08114539e-03, bound:  3.15396070e-01\n",
      "Epoch: 27252 mean train loss:  4.08045715e-03, bound:  3.15396011e-01\n",
      "Epoch: 27253 mean train loss:  4.07988392e-03, bound:  3.15396011e-01\n",
      "Epoch: 27254 mean train loss:  4.07955237e-03, bound:  3.15395981e-01\n",
      "Epoch: 27255 mean train loss:  4.07945225e-03, bound:  3.15395981e-01\n",
      "Epoch: 27256 mean train loss:  4.07938380e-03, bound:  3.15395951e-01\n",
      "Epoch: 27257 mean train loss:  4.07939777e-03, bound:  3.15395892e-01\n",
      "Epoch: 27258 mean train loss:  4.07942059e-03, bound:  3.15395921e-01\n",
      "Epoch: 27259 mean train loss:  4.07936238e-03, bound:  3.15395892e-01\n",
      "Epoch: 27260 mean train loss:  4.07922314e-03, bound:  3.15395892e-01\n",
      "Epoch: 27261 mean train loss:  4.07905085e-03, bound:  3.15395832e-01\n",
      "Epoch: 27262 mean train loss:  4.07876819e-03, bound:  3.15395862e-01\n",
      "Epoch: 27263 mean train loss:  4.07842081e-03, bound:  3.15395832e-01\n",
      "Epoch: 27264 mean train loss:  4.07809811e-03, bound:  3.15395772e-01\n",
      "Epoch: 27265 mean train loss:  4.07777261e-03, bound:  3.15395772e-01\n",
      "Epoch: 27266 mean train loss:  4.07745084e-03, bound:  3.15395772e-01\n",
      "Epoch: 27267 mean train loss:  4.07715188e-03, bound:  3.15395743e-01\n",
      "Epoch: 27268 mean train loss:  4.07697354e-03, bound:  3.15395743e-01\n",
      "Epoch: 27269 mean train loss:  4.07672534e-03, bound:  3.15395713e-01\n",
      "Epoch: 27270 mean train loss:  4.07654932e-03, bound:  3.15395683e-01\n",
      "Epoch: 27271 mean train loss:  4.07634210e-03, bound:  3.15395683e-01\n",
      "Epoch: 27272 mean train loss:  4.07625549e-03, bound:  3.15395653e-01\n",
      "Epoch: 27273 mean train loss:  4.07606037e-03, bound:  3.15395623e-01\n",
      "Epoch: 27274 mean train loss:  4.07585828e-03, bound:  3.15395623e-01\n",
      "Epoch: 27275 mean train loss:  4.07572789e-03, bound:  3.15395594e-01\n",
      "Epoch: 27276 mean train loss:  4.07555327e-03, bound:  3.15395594e-01\n",
      "Epoch: 27277 mean train loss:  4.07538004e-03, bound:  3.15395564e-01\n",
      "Epoch: 27278 mean train loss:  4.07516304e-03, bound:  3.15395564e-01\n",
      "Epoch: 27279 mean train loss:  4.07500798e-03, bound:  3.15395504e-01\n",
      "Epoch: 27280 mean train loss:  4.07472579e-03, bound:  3.15395504e-01\n",
      "Epoch: 27281 mean train loss:  4.07448132e-03, bound:  3.15395504e-01\n",
      "Epoch: 27282 mean train loss:  4.07432159e-03, bound:  3.15395504e-01\n",
      "Epoch: 27283 mean train loss:  4.07411950e-03, bound:  3.15395445e-01\n",
      "Epoch: 27284 mean train loss:  4.07391088e-03, bound:  3.15395445e-01\n",
      "Epoch: 27285 mean train loss:  4.07370413e-03, bound:  3.15395415e-01\n",
      "Epoch: 27286 mean train loss:  4.07350063e-03, bound:  3.15395415e-01\n",
      "Epoch: 27287 mean train loss:  4.07334883e-03, bound:  3.15395415e-01\n",
      "Epoch: 27288 mean train loss:  4.07316675e-03, bound:  3.15395385e-01\n",
      "Epoch: 27289 mean train loss:  4.07298608e-03, bound:  3.15395355e-01\n",
      "Epoch: 27290 mean train loss:  4.07269923e-03, bound:  3.15395325e-01\n",
      "Epoch: 27291 mean train loss:  4.07254137e-03, bound:  3.15395325e-01\n",
      "Epoch: 27292 mean train loss:  4.07236582e-03, bound:  3.15395296e-01\n",
      "Epoch: 27293 mean train loss:  4.07218747e-03, bound:  3.15395296e-01\n",
      "Epoch: 27294 mean train loss:  4.07195790e-03, bound:  3.15395266e-01\n",
      "Epoch: 27295 mean train loss:  4.07182984e-03, bound:  3.15395266e-01\n",
      "Epoch: 27296 mean train loss:  4.07164218e-03, bound:  3.15395206e-01\n",
      "Epoch: 27297 mean train loss:  4.07143217e-03, bound:  3.15395206e-01\n",
      "Epoch: 27298 mean train loss:  4.07126592e-03, bound:  3.15395176e-01\n",
      "Epoch: 27299 mean train loss:  4.07110015e-03, bound:  3.15395176e-01\n",
      "Epoch: 27300 mean train loss:  4.07094136e-03, bound:  3.15395176e-01\n",
      "Epoch: 27301 mean train loss:  4.07073973e-03, bound:  3.15395147e-01\n",
      "Epoch: 27302 mean train loss:  4.07056371e-03, bound:  3.15395087e-01\n",
      "Epoch: 27303 mean train loss:  4.07033646e-03, bound:  3.15395087e-01\n",
      "Epoch: 27304 mean train loss:  4.07013530e-03, bound:  3.15395087e-01\n",
      "Epoch: 27305 mean train loss:  4.07001004e-03, bound:  3.15395057e-01\n",
      "Epoch: 27306 mean train loss:  4.06996207e-03, bound:  3.15395027e-01\n",
      "Epoch: 27307 mean train loss:  4.06984473e-03, bound:  3.15395027e-01\n",
      "Epoch: 27308 mean train loss:  4.06980189e-03, bound:  3.15394998e-01\n",
      "Epoch: 27309 mean train loss:  4.06980189e-03, bound:  3.15394998e-01\n",
      "Epoch: 27310 mean train loss:  4.06990014e-03, bound:  3.15394968e-01\n",
      "Epoch: 27311 mean train loss:  4.07015672e-03, bound:  3.15394968e-01\n",
      "Epoch: 27312 mean train loss:  4.07050457e-03, bound:  3.15394938e-01\n",
      "Epoch: 27313 mean train loss:  4.07119980e-03, bound:  3.15394938e-01\n",
      "Epoch: 27314 mean train loss:  4.07219119e-03, bound:  3.15394878e-01\n",
      "Epoch: 27315 mean train loss:  4.07354021e-03, bound:  3.15394908e-01\n",
      "Epoch: 27316 mean train loss:  4.07553697e-03, bound:  3.15394849e-01\n",
      "Epoch: 27317 mean train loss:  4.07817727e-03, bound:  3.15394878e-01\n",
      "Epoch: 27318 mean train loss:  4.08150628e-03, bound:  3.15394819e-01\n",
      "Epoch: 27319 mean train loss:  4.08500526e-03, bound:  3.15394849e-01\n",
      "Epoch: 27320 mean train loss:  4.08805907e-03, bound:  3.15394759e-01\n",
      "Epoch: 27321 mean train loss:  4.08955943e-03, bound:  3.15394819e-01\n",
      "Epoch: 27322 mean train loss:  4.08837665e-03, bound:  3.15394729e-01\n",
      "Epoch: 27323 mean train loss:  4.08408465e-03, bound:  3.15394759e-01\n",
      "Epoch: 27324 mean train loss:  4.07756679e-03, bound:  3.15394729e-01\n",
      "Epoch: 27325 mean train loss:  4.07101214e-03, bound:  3.15394759e-01\n",
      "Epoch: 27326 mean train loss:  4.06674296e-03, bound:  3.15394729e-01\n",
      "Epoch: 27327 mean train loss:  4.06585308e-03, bound:  3.15394700e-01\n",
      "Epoch: 27328 mean train loss:  4.06786427e-03, bound:  3.15394700e-01\n",
      "Epoch: 27329 mean train loss:  4.07089759e-03, bound:  3.15394640e-01\n",
      "Epoch: 27330 mean train loss:  4.07290598e-03, bound:  3.15394700e-01\n",
      "Epoch: 27331 mean train loss:  4.07269178e-03, bound:  3.15394610e-01\n",
      "Epoch: 27332 mean train loss:  4.07048734e-03, bound:  3.15394640e-01\n",
      "Epoch: 27333 mean train loss:  4.06742981e-03, bound:  3.15394580e-01\n",
      "Epoch: 27334 mean train loss:  4.06501908e-03, bound:  3.15394610e-01\n",
      "Epoch: 27335 mean train loss:  4.06427681e-03, bound:  3.15394580e-01\n",
      "Epoch: 27336 mean train loss:  4.06495761e-03, bound:  3.15394521e-01\n",
      "Epoch: 27337 mean train loss:  4.06620093e-03, bound:  3.15394521e-01\n",
      "Epoch: 27338 mean train loss:  4.06707451e-03, bound:  3.15394491e-01\n",
      "Epoch: 27339 mean train loss:  4.06694086e-03, bound:  3.15394491e-01\n",
      "Epoch: 27340 mean train loss:  4.06588102e-03, bound:  3.15394491e-01\n",
      "Epoch: 27341 mean train loss:  4.06441139e-03, bound:  3.15394491e-01\n",
      "Epoch: 27342 mean train loss:  4.06325748e-03, bound:  3.15394461e-01\n",
      "Epoch: 27343 mean train loss:  4.06275783e-03, bound:  3.15394402e-01\n",
      "Epoch: 27344 mean train loss:  4.06298600e-03, bound:  3.15394402e-01\n",
      "Epoch: 27345 mean train loss:  4.06356668e-03, bound:  3.15394372e-01\n",
      "Epoch: 27346 mean train loss:  4.06390801e-03, bound:  3.15394372e-01\n",
      "Epoch: 27347 mean train loss:  4.06391313e-03, bound:  3.15394312e-01\n",
      "Epoch: 27348 mean train loss:  4.06330265e-03, bound:  3.15394312e-01\n",
      "Epoch: 27349 mean train loss:  4.06246260e-03, bound:  3.15394282e-01\n",
      "Epoch: 27350 mean train loss:  4.06173803e-03, bound:  3.15394282e-01\n",
      "Epoch: 27351 mean train loss:  4.06131847e-03, bound:  3.15394253e-01\n",
      "Epoch: 27352 mean train loss:  4.06113826e-03, bound:  3.15394253e-01\n",
      "Epoch: 27353 mean train loss:  4.06121323e-03, bound:  3.15394253e-01\n",
      "Epoch: 27354 mean train loss:  4.06127051e-03, bound:  3.15394193e-01\n",
      "Epoch: 27355 mean train loss:  4.06121789e-03, bound:  3.15394193e-01\n",
      "Epoch: 27356 mean train loss:  4.06096410e-03, bound:  3.15394163e-01\n",
      "Epoch: 27357 mean train loss:  4.06056829e-03, bound:  3.15394163e-01\n",
      "Epoch: 27358 mean train loss:  4.06022137e-03, bound:  3.15394163e-01\n",
      "Epoch: 27359 mean train loss:  4.05979669e-03, bound:  3.15394163e-01\n",
      "Epoch: 27360 mean train loss:  4.05965745e-03, bound:  3.15394074e-01\n",
      "Epoch: 27361 mean train loss:  4.05956013e-03, bound:  3.15394074e-01\n",
      "Epoch: 27362 mean train loss:  4.05951915e-03, bound:  3.15394074e-01\n",
      "Epoch: 27363 mean train loss:  4.05943766e-03, bound:  3.15394074e-01\n",
      "Epoch: 27364 mean train loss:  4.05923836e-03, bound:  3.15394044e-01\n",
      "Epoch: 27365 mean train loss:  4.05901670e-03, bound:  3.15394044e-01\n",
      "Epoch: 27366 mean train loss:  4.05871216e-03, bound:  3.15394014e-01\n",
      "Epoch: 27367 mean train loss:  4.05842904e-03, bound:  3.15393955e-01\n",
      "Epoch: 27368 mean train loss:  4.05817665e-03, bound:  3.15393955e-01\n",
      "Epoch: 27369 mean train loss:  4.05801972e-03, bound:  3.15393955e-01\n",
      "Epoch: 27370 mean train loss:  4.05785162e-03, bound:  3.15393925e-01\n",
      "Epoch: 27371 mean train loss:  4.05773008e-03, bound:  3.15393925e-01\n",
      "Epoch: 27372 mean train loss:  4.05758619e-03, bound:  3.15393865e-01\n",
      "Epoch: 27373 mean train loss:  4.05736454e-03, bound:  3.15393835e-01\n",
      "Epoch: 27374 mean train loss:  4.05719597e-03, bound:  3.15393835e-01\n",
      "Epoch: 27375 mean train loss:  4.05699108e-03, bound:  3.15393835e-01\n",
      "Epoch: 27376 mean train loss:  4.05675499e-03, bound:  3.15393835e-01\n",
      "Epoch: 27377 mean train loss:  4.05653799e-03, bound:  3.15393806e-01\n",
      "Epoch: 27378 mean train loss:  4.05632425e-03, bound:  3.15393806e-01\n",
      "Epoch: 27379 mean train loss:  4.05615475e-03, bound:  3.15393746e-01\n",
      "Epoch: 27380 mean train loss:  4.05594753e-03, bound:  3.15393746e-01\n",
      "Epoch: 27381 mean train loss:  4.05576965e-03, bound:  3.15393716e-01\n",
      "Epoch: 27382 mean train loss:  4.05559596e-03, bound:  3.15393716e-01\n",
      "Epoch: 27383 mean train loss:  4.05542413e-03, bound:  3.15393716e-01\n",
      "Epoch: 27384 mean train loss:  4.05521644e-03, bound:  3.15393686e-01\n",
      "Epoch: 27385 mean train loss:  4.05503437e-03, bound:  3.15393686e-01\n",
      "Epoch: 27386 mean train loss:  4.05489095e-03, bound:  3.15393627e-01\n",
      "Epoch: 27387 mean train loss:  4.05468512e-03, bound:  3.15393627e-01\n",
      "Epoch: 27388 mean train loss:  4.05450305e-03, bound:  3.15393597e-01\n",
      "Epoch: 27389 mean train loss:  4.05433215e-03, bound:  3.15393597e-01\n",
      "Epoch: 27390 mean train loss:  4.05413145e-03, bound:  3.15393567e-01\n",
      "Epoch: 27391 mean train loss:  4.05390793e-03, bound:  3.15393567e-01\n",
      "Epoch: 27392 mean train loss:  4.05378919e-03, bound:  3.15393507e-01\n",
      "Epoch: 27393 mean train loss:  4.05357359e-03, bound:  3.15393507e-01\n",
      "Epoch: 27394 mean train loss:  4.05342411e-03, bound:  3.15393478e-01\n",
      "Epoch: 27395 mean train loss:  4.05320851e-03, bound:  3.15393478e-01\n",
      "Epoch: 27396 mean train loss:  4.05303715e-03, bound:  3.15393478e-01\n",
      "Epoch: 27397 mean train loss:  4.05280804e-03, bound:  3.15393478e-01\n",
      "Epoch: 27398 mean train loss:  4.05259896e-03, bound:  3.15393418e-01\n",
      "Epoch: 27399 mean train loss:  4.05242061e-03, bound:  3.15393388e-01\n",
      "Epoch: 27400 mean train loss:  4.05224552e-03, bound:  3.15393388e-01\n",
      "Epoch: 27401 mean train loss:  4.05209651e-03, bound:  3.15393358e-01\n",
      "Epoch: 27402 mean train loss:  4.05184971e-03, bound:  3.15393358e-01\n",
      "Epoch: 27403 mean train loss:  4.05167928e-03, bound:  3.15393358e-01\n",
      "Epoch: 27404 mean train loss:  4.05145437e-03, bound:  3.15393358e-01\n",
      "Epoch: 27405 mean train loss:  4.05128999e-03, bound:  3.15393269e-01\n",
      "Epoch: 27406 mean train loss:  4.05112840e-03, bound:  3.15393269e-01\n",
      "Epoch: 27407 mean train loss:  4.05097753e-03, bound:  3.15393269e-01\n",
      "Epoch: 27408 mean train loss:  4.05077310e-03, bound:  3.15393269e-01\n",
      "Epoch: 27409 mean train loss:  4.05058824e-03, bound:  3.15393239e-01\n",
      "Epoch: 27410 mean train loss:  4.05040151e-03, bound:  3.15393239e-01\n",
      "Epoch: 27411 mean train loss:  4.05023480e-03, bound:  3.15393180e-01\n",
      "Epoch: 27412 mean train loss:  4.04999591e-03, bound:  3.15393180e-01\n",
      "Epoch: 27413 mean train loss:  4.04982409e-03, bound:  3.15393150e-01\n",
      "Epoch: 27414 mean train loss:  4.04964248e-03, bound:  3.15393150e-01\n",
      "Epoch: 27415 mean train loss:  4.04944224e-03, bound:  3.15393120e-01\n",
      "Epoch: 27416 mean train loss:  4.04926203e-03, bound:  3.15393120e-01\n",
      "Epoch: 27417 mean train loss:  4.04906087e-03, bound:  3.15393120e-01\n",
      "Epoch: 27418 mean train loss:  4.04892582e-03, bound:  3.15393060e-01\n",
      "Epoch: 27419 mean train loss:  4.04874235e-03, bound:  3.15393060e-01\n",
      "Epoch: 27420 mean train loss:  4.04849043e-03, bound:  3.15393031e-01\n",
      "Epoch: 27421 mean train loss:  4.04829392e-03, bound:  3.15393031e-01\n",
      "Epoch: 27422 mean train loss:  4.04813653e-03, bound:  3.15392971e-01\n",
      "Epoch: 27423 mean train loss:  4.04795771e-03, bound:  3.15392971e-01\n",
      "Epoch: 27424 mean train loss:  4.04771790e-03, bound:  3.15392941e-01\n",
      "Epoch: 27425 mean train loss:  4.04757075e-03, bound:  3.15392941e-01\n",
      "Epoch: 27426 mean train loss:  4.04737890e-03, bound:  3.15392941e-01\n",
      "Epoch: 27427 mean train loss:  4.04719263e-03, bound:  3.15392911e-01\n",
      "Epoch: 27428 mean train loss:  4.04700730e-03, bound:  3.15392882e-01\n",
      "Epoch: 27429 mean train loss:  4.04683407e-03, bound:  3.15392852e-01\n",
      "Epoch: 27430 mean train loss:  4.04662732e-03, bound:  3.15392852e-01\n",
      "Epoch: 27431 mean train loss:  4.04648576e-03, bound:  3.15392822e-01\n",
      "Epoch: 27432 mean train loss:  4.04630322e-03, bound:  3.15392822e-01\n",
      "Epoch: 27433 mean train loss:  4.04614350e-03, bound:  3.15392792e-01\n",
      "Epoch: 27434 mean train loss:  4.04592883e-03, bound:  3.15392792e-01\n",
      "Epoch: 27435 mean train loss:  4.04582545e-03, bound:  3.15392733e-01\n",
      "Epoch: 27436 mean train loss:  4.04569833e-03, bound:  3.15392733e-01\n",
      "Epoch: 27437 mean train loss:  4.04557958e-03, bound:  3.15392703e-01\n",
      "Epoch: 27438 mean train loss:  4.04551346e-03, bound:  3.15392703e-01\n",
      "Epoch: 27439 mean train loss:  4.04546317e-03, bound:  3.15392673e-01\n",
      "Epoch: 27440 mean train loss:  4.04547155e-03, bound:  3.15392673e-01\n",
      "Epoch: 27441 mean train loss:  4.04557213e-03, bound:  3.15392673e-01\n",
      "Epoch: 27442 mean train loss:  4.04571788e-03, bound:  3.15392673e-01\n",
      "Epoch: 27443 mean train loss:  4.04592231e-03, bound:  3.15392584e-01\n",
      "Epoch: 27444 mean train loss:  4.04624315e-03, bound:  3.15392584e-01\n",
      "Epoch: 27445 mean train loss:  4.04670369e-03, bound:  3.15392554e-01\n",
      "Epoch: 27446 mean train loss:  4.04739752e-03, bound:  3.15392584e-01\n",
      "Epoch: 27447 mean train loss:  4.04821010e-03, bound:  3.15392524e-01\n",
      "Epoch: 27448 mean train loss:  4.04932862e-03, bound:  3.15392554e-01\n",
      "Epoch: 27449 mean train loss:  4.05068463e-03, bound:  3.15392494e-01\n",
      "Epoch: 27450 mean train loss:  4.05222317e-03, bound:  3.15392524e-01\n",
      "Epoch: 27451 mean train loss:  4.05388279e-03, bound:  3.15392435e-01\n",
      "Epoch: 27452 mean train loss:  4.05524904e-03, bound:  3.15392494e-01\n",
      "Epoch: 27453 mean train loss:  4.05603927e-03, bound:  3.15392405e-01\n",
      "Epoch: 27454 mean train loss:  4.05572308e-03, bound:  3.15392435e-01\n",
      "Epoch: 27455 mean train loss:  4.05412819e-03, bound:  3.15392375e-01\n",
      "Epoch: 27456 mean train loss:  4.05133190e-03, bound:  3.15392435e-01\n",
      "Epoch: 27457 mean train loss:  4.04781662e-03, bound:  3.15392375e-01\n",
      "Epoch: 27458 mean train loss:  4.04449832e-03, bound:  3.15392375e-01\n",
      "Epoch: 27459 mean train loss:  4.04215464e-03, bound:  3.15392345e-01\n",
      "Epoch: 27460 mean train loss:  4.04106220e-03, bound:  3.15392345e-01\n",
      "Epoch: 27461 mean train loss:  4.04106034e-03, bound:  3.15392345e-01\n",
      "Epoch: 27462 mean train loss:  4.04198095e-03, bound:  3.15392286e-01\n",
      "Epoch: 27463 mean train loss:  4.04310925e-03, bound:  3.15392286e-01\n",
      "Epoch: 27464 mean train loss:  4.04394837e-03, bound:  3.15392256e-01\n",
      "Epoch: 27465 mean train loss:  4.04413557e-03, bound:  3.15392256e-01\n",
      "Epoch: 27466 mean train loss:  4.04360285e-03, bound:  3.15392226e-01\n",
      "Epoch: 27467 mean train loss:  4.04251833e-03, bound:  3.15392226e-01\n",
      "Epoch: 27468 mean train loss:  4.04131040e-03, bound:  3.15392166e-01\n",
      "Epoch: 27469 mean train loss:  4.04007733e-03, bound:  3.15392166e-01\n",
      "Epoch: 27470 mean train loss:  4.03926102e-03, bound:  3.15392137e-01\n",
      "Epoch: 27471 mean train loss:  4.03896673e-03, bound:  3.15392137e-01\n",
      "Epoch: 27472 mean train loss:  4.03909711e-03, bound:  3.15392137e-01\n",
      "Epoch: 27473 mean train loss:  4.03941143e-03, bound:  3.15392107e-01\n",
      "Epoch: 27474 mean train loss:  4.03972249e-03, bound:  3.15392107e-01\n",
      "Epoch: 27475 mean train loss:  4.03979560e-03, bound:  3.15392047e-01\n",
      "Epoch: 27476 mean train loss:  4.03961400e-03, bound:  3.15392047e-01\n",
      "Epoch: 27477 mean train loss:  4.03919769e-03, bound:  3.15392017e-01\n",
      "Epoch: 27478 mean train loss:  4.03858628e-03, bound:  3.15392017e-01\n",
      "Epoch: 27479 mean train loss:  4.03805729e-03, bound:  3.15391988e-01\n",
      "Epoch: 27480 mean train loss:  4.03755344e-03, bound:  3.15391988e-01\n",
      "Epoch: 27481 mean train loss:  4.03716136e-03, bound:  3.15391928e-01\n",
      "Epoch: 27482 mean train loss:  4.03695740e-03, bound:  3.15391928e-01\n",
      "Epoch: 27483 mean train loss:  4.03686427e-03, bound:  3.15391898e-01\n",
      "Epoch: 27484 mean train loss:  4.03680420e-03, bound:  3.15391898e-01\n",
      "Epoch: 27485 mean train loss:  4.03681537e-03, bound:  3.15391898e-01\n",
      "Epoch: 27486 mean train loss:  4.03674319e-03, bound:  3.15391868e-01\n",
      "Epoch: 27487 mean train loss:  4.03664680e-03, bound:  3.15391839e-01\n",
      "Epoch: 27488 mean train loss:  4.03651223e-03, bound:  3.15391839e-01\n",
      "Epoch: 27489 mean train loss:  4.03627707e-03, bound:  3.15391839e-01\n",
      "Epoch: 27490 mean train loss:  4.03591199e-03, bound:  3.15391809e-01\n",
      "Epoch: 27491 mean train loss:  4.03555483e-03, bound:  3.15391779e-01\n",
      "Epoch: 27492 mean train loss:  4.03526053e-03, bound:  3.15391749e-01\n",
      "Epoch: 27493 mean train loss:  4.03494807e-03, bound:  3.15391749e-01\n",
      "Epoch: 27494 mean train loss:  4.03475063e-03, bound:  3.15391719e-01\n",
      "Epoch: 27495 mean train loss:  4.03450755e-03, bound:  3.15391690e-01\n",
      "Epoch: 27496 mean train loss:  4.03439905e-03, bound:  3.15391690e-01\n",
      "Epoch: 27497 mean train loss:  4.03427659e-03, bound:  3.15391690e-01\n",
      "Epoch: 27498 mean train loss:  4.03412431e-03, bound:  3.15391600e-01\n",
      "Epoch: 27499 mean train loss:  4.03401954e-03, bound:  3.15391600e-01\n",
      "Epoch: 27500 mean train loss:  4.03376855e-03, bound:  3.15391600e-01\n",
      "Epoch: 27501 mean train loss:  4.03362187e-03, bound:  3.15391600e-01\n",
      "Epoch: 27502 mean train loss:  4.03339323e-03, bound:  3.15391570e-01\n",
      "Epoch: 27503 mean train loss:  4.03314829e-03, bound:  3.15391570e-01\n",
      "Epoch: 27504 mean train loss:  4.03286377e-03, bound:  3.15391541e-01\n",
      "Epoch: 27505 mean train loss:  4.03273804e-03, bound:  3.15391541e-01\n",
      "Epoch: 27506 mean train loss:  4.03255178e-03, bound:  3.15391541e-01\n",
      "Epoch: 27507 mean train loss:  4.03227611e-03, bound:  3.15391481e-01\n",
      "Epoch: 27508 mean train loss:  4.03210428e-03, bound:  3.15391451e-01\n",
      "Epoch: 27509 mean train loss:  4.03190171e-03, bound:  3.15391451e-01\n",
      "Epoch: 27510 mean train loss:  4.03178250e-03, bound:  3.15391451e-01\n",
      "Epoch: 27511 mean train loss:  4.03161021e-03, bound:  3.15391451e-01\n",
      "Epoch: 27512 mean train loss:  4.03141044e-03, bound:  3.15391421e-01\n",
      "Epoch: 27513 mean train loss:  4.03127493e-03, bound:  3.15391392e-01\n",
      "Epoch: 27514 mean train loss:  4.03105468e-03, bound:  3.15391362e-01\n",
      "Epoch: 27515 mean train loss:  4.03090613e-03, bound:  3.15391332e-01\n",
      "Epoch: 27516 mean train loss:  4.03066399e-03, bound:  3.15391332e-01\n",
      "Epoch: 27517 mean train loss:  4.03048797e-03, bound:  3.15391332e-01\n",
      "Epoch: 27518 mean train loss:  4.03030543e-03, bound:  3.15391302e-01\n",
      "Epoch: 27519 mean train loss:  4.03015641e-03, bound:  3.15391302e-01\n",
      "Epoch: 27520 mean train loss:  4.02994221e-03, bound:  3.15391243e-01\n",
      "Epoch: 27521 mean train loss:  4.02975176e-03, bound:  3.15391243e-01\n",
      "Epoch: 27522 mean train loss:  4.02961997e-03, bound:  3.15391243e-01\n",
      "Epoch: 27523 mean train loss:  4.02936991e-03, bound:  3.15391213e-01\n",
      "Epoch: 27524 mean train loss:  4.02921345e-03, bound:  3.15391153e-01\n",
      "Epoch: 27525 mean train loss:  4.02904255e-03, bound:  3.15391153e-01\n",
      "Epoch: 27526 mean train loss:  4.02883021e-03, bound:  3.15391153e-01\n",
      "Epoch: 27527 mean train loss:  4.02865699e-03, bound:  3.15391123e-01\n",
      "Epoch: 27528 mean train loss:  4.02841950e-03, bound:  3.15391123e-01\n",
      "Epoch: 27529 mean train loss:  4.02823836e-03, bound:  3.15391123e-01\n",
      "Epoch: 27530 mean train loss:  4.02803393e-03, bound:  3.15391034e-01\n",
      "Epoch: 27531 mean train loss:  4.02789982e-03, bound:  3.15391034e-01\n",
      "Epoch: 27532 mean train loss:  4.02774382e-03, bound:  3.15391034e-01\n",
      "Epoch: 27533 mean train loss:  4.02753148e-03, bound:  3.15391034e-01\n",
      "Epoch: 27534 mean train loss:  4.02733451e-03, bound:  3.15391004e-01\n",
      "Epoch: 27535 mean train loss:  4.02718177e-03, bound:  3.15391004e-01\n",
      "Epoch: 27536 mean train loss:  4.02696338e-03, bound:  3.15391004e-01\n",
      "Epoch: 27537 mean train loss:  4.02678177e-03, bound:  3.15390915e-01\n",
      "Epoch: 27538 mean train loss:  4.02662857e-03, bound:  3.15390915e-01\n",
      "Epoch: 27539 mean train loss:  4.02648468e-03, bound:  3.15390915e-01\n",
      "Epoch: 27540 mean train loss:  4.02631285e-03, bound:  3.15390915e-01\n",
      "Epoch: 27541 mean train loss:  4.02614428e-03, bound:  3.15390885e-01\n",
      "Epoch: 27542 mean train loss:  4.02605394e-03, bound:  3.15390885e-01\n",
      "Epoch: 27543 mean train loss:  4.02595708e-03, bound:  3.15390855e-01\n",
      "Epoch: 27544 mean train loss:  4.02584719e-03, bound:  3.15390825e-01\n",
      "Epoch: 27545 mean train loss:  4.02574381e-03, bound:  3.15390825e-01\n",
      "Epoch: 27546 mean train loss:  4.02564835e-03, bound:  3.15390795e-01\n",
      "Epoch: 27547 mean train loss:  4.02572704e-03, bound:  3.15390795e-01\n",
      "Epoch: 27548 mean train loss:  4.02579550e-03, bound:  3.15390766e-01\n",
      "Epoch: 27549 mean train loss:  4.02608095e-03, bound:  3.15390766e-01\n",
      "Epoch: 27550 mean train loss:  4.02655220e-03, bound:  3.15390706e-01\n",
      "Epoch: 27551 mean train loss:  4.02736291e-03, bound:  3.15390706e-01\n",
      "Epoch: 27552 mean train loss:  4.02840599e-03, bound:  3.15390646e-01\n",
      "Epoch: 27553 mean train loss:  4.02985234e-03, bound:  3.15390676e-01\n",
      "Epoch: 27554 mean train loss:  4.03181370e-03, bound:  3.15390587e-01\n",
      "Epoch: 27555 mean train loss:  4.03427565e-03, bound:  3.15390646e-01\n",
      "Epoch: 27556 mean train loss:  4.03708965e-03, bound:  3.15390587e-01\n",
      "Epoch: 27557 mean train loss:  4.03990783e-03, bound:  3.15390646e-01\n",
      "Epoch: 27558 mean train loss:  4.04199120e-03, bound:  3.15390557e-01\n",
      "Epoch: 27559 mean train loss:  4.04248061e-03, bound:  3.15390587e-01\n",
      "Epoch: 27560 mean train loss:  4.04050294e-03, bound:  3.15390527e-01\n",
      "Epoch: 27561 mean train loss:  4.03610151e-03, bound:  3.15390557e-01\n",
      "Epoch: 27562 mean train loss:  4.03048983e-03, bound:  3.15390468e-01\n",
      "Epoch: 27563 mean train loss:  4.02532285e-03, bound:  3.15390527e-01\n",
      "Epoch: 27564 mean train loss:  4.02225042e-03, bound:  3.15390468e-01\n",
      "Epoch: 27565 mean train loss:  4.02167393e-03, bound:  3.15390468e-01\n",
      "Epoch: 27566 mean train loss:  4.02314868e-03, bound:  3.15390468e-01\n",
      "Epoch: 27567 mean train loss:  4.02546069e-03, bound:  3.15390438e-01\n",
      "Epoch: 27568 mean train loss:  4.02719714e-03, bound:  3.15390438e-01\n",
      "Epoch: 27569 mean train loss:  4.02748678e-03, bound:  3.15390348e-01\n",
      "Epoch: 27570 mean train loss:  4.02615406e-03, bound:  3.15390438e-01\n",
      "Epoch: 27571 mean train loss:  4.02376382e-03, bound:  3.15390348e-01\n",
      "Epoch: 27572 mean train loss:  4.02154587e-03, bound:  3.15390348e-01\n",
      "Epoch: 27573 mean train loss:  4.02023084e-03, bound:  3.15390319e-01\n",
      "Epoch: 27574 mean train loss:  4.02006414e-03, bound:  3.15390319e-01\n",
      "Epoch: 27575 mean train loss:  4.02073003e-03, bound:  3.15390319e-01\n",
      "Epoch: 27576 mean train loss:  4.02163388e-03, bound:  3.15390259e-01\n",
      "Epoch: 27577 mean train loss:  4.02206928e-03, bound:  3.15390319e-01\n",
      "Epoch: 27578 mean train loss:  4.02185833e-03, bound:  3.15390259e-01\n",
      "Epoch: 27579 mean train loss:  4.02096892e-03, bound:  3.15390259e-01\n",
      "Epoch: 27580 mean train loss:  4.01987974e-03, bound:  3.15390199e-01\n",
      "Epoch: 27581 mean train loss:  4.01892699e-03, bound:  3.15390199e-01\n",
      "Epoch: 27582 mean train loss:  4.01846040e-03, bound:  3.15390199e-01\n",
      "Epoch: 27583 mean train loss:  4.01844271e-03, bound:  3.15390199e-01\n",
      "Epoch: 27584 mean train loss:  4.01862944e-03, bound:  3.15390140e-01\n",
      "Epoch: 27585 mean train loss:  4.01888881e-03, bound:  3.15390140e-01\n",
      "Epoch: 27586 mean train loss:  4.01887763e-03, bound:  3.15390140e-01\n",
      "Epoch: 27587 mean train loss:  4.01862012e-03, bound:  3.15390080e-01\n",
      "Epoch: 27588 mean train loss:  4.01818892e-03, bound:  3.15390080e-01\n",
      "Epoch: 27589 mean train loss:  4.01766738e-03, bound:  3.15390080e-01\n",
      "Epoch: 27590 mean train loss:  4.01716772e-03, bound:  3.15390021e-01\n",
      "Epoch: 27591 mean train loss:  4.01684921e-03, bound:  3.15390021e-01\n",
      "Epoch: 27592 mean train loss:  4.01663035e-03, bound:  3.15389991e-01\n",
      "Epoch: 27593 mean train loss:  4.01654607e-03, bound:  3.15389991e-01\n",
      "Epoch: 27594 mean train loss:  4.01646690e-03, bound:  3.15389961e-01\n",
      "Epoch: 27595 mean train loss:  4.01634537e-03, bound:  3.15389961e-01\n",
      "Epoch: 27596 mean train loss:  4.01621405e-03, bound:  3.15389901e-01\n",
      "Epoch: 27597 mean train loss:  4.01599193e-03, bound:  3.15389901e-01\n",
      "Epoch: 27598 mean train loss:  4.01572790e-03, bound:  3.15389901e-01\n",
      "Epoch: 27599 mean train loss:  4.01550019e-03, bound:  3.15389872e-01\n",
      "Epoch: 27600 mean train loss:  4.01519565e-03, bound:  3.15389812e-01\n",
      "Epoch: 27601 mean train loss:  4.01495025e-03, bound:  3.15389812e-01\n",
      "Epoch: 27602 mean train loss:  4.01483057e-03, bound:  3.15389782e-01\n",
      "Epoch: 27603 mean train loss:  4.01468761e-03, bound:  3.15389782e-01\n",
      "Epoch: 27604 mean train loss:  4.01457679e-03, bound:  3.15389782e-01\n",
      "Epoch: 27605 mean train loss:  4.01444873e-03, bound:  3.15389752e-01\n",
      "Epoch: 27606 mean train loss:  4.01428668e-03, bound:  3.15389752e-01\n",
      "Epoch: 27607 mean train loss:  4.01414884e-03, bound:  3.15389693e-01\n",
      "Epoch: 27608 mean train loss:  4.01396630e-03, bound:  3.15389693e-01\n",
      "Epoch: 27609 mean train loss:  4.01368691e-03, bound:  3.15389693e-01\n",
      "Epoch: 27610 mean train loss:  4.01336653e-03, bound:  3.15389663e-01\n",
      "Epoch: 27611 mean train loss:  4.01316630e-03, bound:  3.15389663e-01\n",
      "Epoch: 27612 mean train loss:  4.01297398e-03, bound:  3.15389633e-01\n",
      "Epoch: 27613 mean train loss:  4.01280029e-03, bound:  3.15389633e-01\n",
      "Epoch: 27614 mean train loss:  4.01262892e-03, bound:  3.15389633e-01\n",
      "Epoch: 27615 mean train loss:  4.01244825e-03, bound:  3.15389633e-01\n",
      "Epoch: 27616 mean train loss:  4.01232718e-03, bound:  3.15389574e-01\n",
      "Epoch: 27617 mean train loss:  4.01212648e-03, bound:  3.15389574e-01\n",
      "Epoch: 27618 mean train loss:  4.01192112e-03, bound:  3.15389514e-01\n",
      "Epoch: 27619 mean train loss:  4.01177676e-03, bound:  3.15389514e-01\n",
      "Epoch: 27620 mean train loss:  4.01151692e-03, bound:  3.15389514e-01\n",
      "Epoch: 27621 mean train loss:  4.01136884e-03, bound:  3.15389514e-01\n",
      "Epoch: 27622 mean train loss:  4.01121192e-03, bound:  3.15389454e-01\n",
      "Epoch: 27623 mean train loss:  4.01103403e-03, bound:  3.15389454e-01\n",
      "Epoch: 27624 mean train loss:  4.01081238e-03, bound:  3.15389395e-01\n",
      "Epoch: 27625 mean train loss:  4.01057489e-03, bound:  3.15389395e-01\n",
      "Epoch: 27626 mean train loss:  4.01045661e-03, bound:  3.15389395e-01\n",
      "Epoch: 27627 mean train loss:  4.01022052e-03, bound:  3.15389365e-01\n",
      "Epoch: 27628 mean train loss:  4.01006034e-03, bound:  3.15389365e-01\n",
      "Epoch: 27629 mean train loss:  4.00984474e-03, bound:  3.15389335e-01\n",
      "Epoch: 27630 mean train loss:  4.00970643e-03, bound:  3.15389335e-01\n",
      "Epoch: 27631 mean train loss:  4.00953228e-03, bound:  3.15389305e-01\n",
      "Epoch: 27632 mean train loss:  4.00931947e-03, bound:  3.15389305e-01\n",
      "Epoch: 27633 mean train loss:  4.00914811e-03, bound:  3.15389246e-01\n",
      "Epoch: 27634 mean train loss:  4.00896277e-03, bound:  3.15389246e-01\n",
      "Epoch: 27635 mean train loss:  4.00876813e-03, bound:  3.15389216e-01\n",
      "Epoch: 27636 mean train loss:  4.00861120e-03, bound:  3.15389216e-01\n",
      "Epoch: 27637 mean train loss:  4.00844403e-03, bound:  3.15389216e-01\n",
      "Epoch: 27638 mean train loss:  4.00823820e-03, bound:  3.15389186e-01\n",
      "Epoch: 27639 mean train loss:  4.00808034e-03, bound:  3.15389186e-01\n",
      "Epoch: 27640 mean train loss:  4.00784239e-03, bound:  3.15389127e-01\n",
      "Epoch: 27641 mean train loss:  4.00768640e-03, bound:  3.15389127e-01\n",
      "Epoch: 27642 mean train loss:  4.00752202e-03, bound:  3.15389097e-01\n",
      "Epoch: 27643 mean train loss:  4.00731387e-03, bound:  3.15389097e-01\n",
      "Epoch: 27644 mean train loss:  4.00713691e-03, bound:  3.15389067e-01\n",
      "Epoch: 27645 mean train loss:  4.00697067e-03, bound:  3.15389067e-01\n",
      "Epoch: 27646 mean train loss:  4.00676019e-03, bound:  3.15389007e-01\n",
      "Epoch: 27647 mean train loss:  4.00655763e-03, bound:  3.15389007e-01\n",
      "Epoch: 27648 mean train loss:  4.00639884e-03, bound:  3.15388978e-01\n",
      "Epoch: 27649 mean train loss:  4.00620280e-03, bound:  3.15388978e-01\n",
      "Epoch: 27650 mean train loss:  4.00603143e-03, bound:  3.15388948e-01\n",
      "Epoch: 27651 mean train loss:  4.00585355e-03, bound:  3.15388948e-01\n",
      "Epoch: 27652 mean train loss:  4.00563888e-03, bound:  3.15388888e-01\n",
      "Epoch: 27653 mean train loss:  4.00547683e-03, bound:  3.15388888e-01\n",
      "Epoch: 27654 mean train loss:  4.00534272e-03, bound:  3.15388888e-01\n",
      "Epoch: 27655 mean train loss:  4.00510337e-03, bound:  3.15388888e-01\n",
      "Epoch: 27656 mean train loss:  4.00491897e-03, bound:  3.15388858e-01\n",
      "Epoch: 27657 mean train loss:  4.00475599e-03, bound:  3.15388829e-01\n",
      "Epoch: 27658 mean train loss:  4.00461536e-03, bound:  3.15388799e-01\n",
      "Epoch: 27659 mean train loss:  4.00438113e-03, bound:  3.15388769e-01\n",
      "Epoch: 27660 mean train loss:  4.00417531e-03, bound:  3.15388769e-01\n",
      "Epoch: 27661 mean train loss:  4.00408171e-03, bound:  3.15388769e-01\n",
      "Epoch: 27662 mean train loss:  4.00386052e-03, bound:  3.15388769e-01\n",
      "Epoch: 27663 mean train loss:  4.00372595e-03, bound:  3.15388709e-01\n",
      "Epoch: 27664 mean train loss:  4.00350941e-03, bound:  3.15388709e-01\n",
      "Epoch: 27665 mean train loss:  4.00337856e-03, bound:  3.15388680e-01\n",
      "Epoch: 27666 mean train loss:  4.00324818e-03, bound:  3.15388680e-01\n",
      "Epoch: 27667 mean train loss:  4.00307495e-03, bound:  3.15388650e-01\n",
      "Epoch: 27668 mean train loss:  4.00304422e-03, bound:  3.15388650e-01\n",
      "Epoch: 27669 mean train loss:  4.00290452e-03, bound:  3.15388650e-01\n",
      "Epoch: 27670 mean train loss:  4.00290731e-03, bound:  3.15388620e-01\n",
      "Epoch: 27671 mean train loss:  4.00296506e-03, bound:  3.15388620e-01\n",
      "Epoch: 27672 mean train loss:  4.00312105e-03, bound:  3.15388560e-01\n",
      "Epoch: 27673 mean train loss:  4.00339765e-03, bound:  3.15388560e-01\n",
      "Epoch: 27674 mean train loss:  4.00386378e-03, bound:  3.15388530e-01\n",
      "Epoch: 27675 mean train loss:  4.00453014e-03, bound:  3.15388530e-01\n",
      "Epoch: 27676 mean train loss:  4.00560908e-03, bound:  3.15388501e-01\n",
      "Epoch: 27677 mean train loss:  4.00711875e-03, bound:  3.15388501e-01\n",
      "Epoch: 27678 mean train loss:  4.00908943e-03, bound:  3.15388411e-01\n",
      "Epoch: 27679 mean train loss:  4.01153462e-03, bound:  3.15388501e-01\n",
      "Epoch: 27680 mean train loss:  4.01450461e-03, bound:  3.15388381e-01\n",
      "Epoch: 27681 mean train loss:  4.01764084e-03, bound:  3.15388441e-01\n",
      "Epoch: 27682 mean train loss:  4.02017124e-03, bound:  3.15388352e-01\n",
      "Epoch: 27683 mean train loss:  4.02094983e-03, bound:  3.15388411e-01\n",
      "Epoch: 27684 mean train loss:  4.01928043e-03, bound:  3.15388322e-01\n",
      "Epoch: 27685 mean train loss:  4.01484827e-03, bound:  3.15388381e-01\n",
      "Epoch: 27686 mean train loss:  4.00870526e-03, bound:  3.15388322e-01\n",
      "Epoch: 27687 mean train loss:  4.00301442e-03, bound:  3.15388322e-01\n",
      "Epoch: 27688 mean train loss:  3.99955502e-03, bound:  3.15388292e-01\n",
      "Epoch: 27689 mean train loss:  3.99901764e-03, bound:  3.15388292e-01\n",
      "Epoch: 27690 mean train loss:  4.00068751e-03, bound:  3.15388292e-01\n",
      "Epoch: 27691 mean train loss:  4.00325563e-03, bound:  3.15388262e-01\n",
      "Epoch: 27692 mean train loss:  4.00511269e-03, bound:  3.15388262e-01\n",
      "Epoch: 27693 mean train loss:  4.00535204e-03, bound:  3.15388203e-01\n",
      "Epoch: 27694 mean train loss:  4.00379440e-03, bound:  3.15388203e-01\n",
      "Epoch: 27695 mean train loss:  4.00119089e-03, bound:  3.15388203e-01\n",
      "Epoch: 27696 mean train loss:  3.99878575e-03, bound:  3.15388203e-01\n",
      "Epoch: 27697 mean train loss:  3.99743579e-03, bound:  3.15388203e-01\n",
      "Epoch: 27698 mean train loss:  3.99748981e-03, bound:  3.15388143e-01\n",
      "Epoch: 27699 mean train loss:  3.99833685e-03, bound:  3.15388143e-01\n",
      "Epoch: 27700 mean train loss:  3.99912754e-03, bound:  3.15388083e-01\n",
      "Epoch: 27701 mean train loss:  3.99954524e-03, bound:  3.15388083e-01\n",
      "Epoch: 27702 mean train loss:  3.99906980e-03, bound:  3.15388083e-01\n",
      "Epoch: 27703 mean train loss:  3.99813289e-03, bound:  3.15388083e-01\n",
      "Epoch: 27704 mean train loss:  3.99712985e-03, bound:  3.15387994e-01\n",
      "Epoch: 27705 mean train loss:  3.99622647e-03, bound:  3.15387994e-01\n",
      "Epoch: 27706 mean train loss:  3.99579527e-03, bound:  3.15387964e-01\n",
      "Epoch: 27707 mean train loss:  3.99581157e-03, bound:  3.15387964e-01\n",
      "Epoch: 27708 mean train loss:  3.99598526e-03, bound:  3.15387964e-01\n",
      "Epoch: 27709 mean train loss:  3.99613986e-03, bound:  3.15387934e-01\n",
      "Epoch: 27710 mean train loss:  3.99603648e-03, bound:  3.15387964e-01\n",
      "Epoch: 27711 mean train loss:  3.99571657e-03, bound:  3.15387905e-01\n",
      "Epoch: 27712 mean train loss:  3.99519689e-03, bound:  3.15387905e-01\n",
      "Epoch: 27713 mean train loss:  3.99471028e-03, bound:  3.15387875e-01\n",
      "Epoch: 27714 mean train loss:  3.99444113e-03, bound:  3.15387845e-01\n",
      "Epoch: 27715 mean train loss:  3.99421854e-03, bound:  3.15387845e-01\n",
      "Epoch: 27716 mean train loss:  3.99417523e-03, bound:  3.15387815e-01\n",
      "Epoch: 27717 mean train loss:  3.99411842e-03, bound:  3.15387785e-01\n",
      "Epoch: 27718 mean train loss:  3.99402622e-03, bound:  3.15387756e-01\n",
      "Epoch: 27719 mean train loss:  3.99389397e-03, bound:  3.15387756e-01\n",
      "Epoch: 27720 mean train loss:  3.99369514e-03, bound:  3.15387726e-01\n",
      "Epoch: 27721 mean train loss:  3.99333378e-03, bound:  3.15387726e-01\n",
      "Epoch: 27722 mean train loss:  3.99307115e-03, bound:  3.15387696e-01\n",
      "Epoch: 27723 mean train loss:  3.99279315e-03, bound:  3.15387666e-01\n",
      "Epoch: 27724 mean train loss:  3.99261760e-03, bound:  3.15387666e-01\n",
      "Epoch: 27725 mean train loss:  3.99241876e-03, bound:  3.15387666e-01\n",
      "Epoch: 27726 mean train loss:  3.99230560e-03, bound:  3.15387636e-01\n",
      "Epoch: 27727 mean train loss:  3.99215426e-03, bound:  3.15387607e-01\n",
      "Epoch: 27728 mean train loss:  3.99202947e-03, bound:  3.15387607e-01\n",
      "Epoch: 27729 mean train loss:  3.99185251e-03, bound:  3.15387547e-01\n",
      "Epoch: 27730 mean train loss:  3.99166066e-03, bound:  3.15387547e-01\n",
      "Epoch: 27731 mean train loss:  3.99144180e-03, bound:  3.15387547e-01\n",
      "Epoch: 27732 mean train loss:  3.99120897e-03, bound:  3.15387517e-01\n",
      "Epoch: 27733 mean train loss:  3.99099523e-03, bound:  3.15387517e-01\n",
      "Epoch: 27734 mean train loss:  3.99076752e-03, bound:  3.15387487e-01\n",
      "Epoch: 27735 mean train loss:  3.99057334e-03, bound:  3.15387487e-01\n",
      "Epoch: 27736 mean train loss:  3.99046065e-03, bound:  3.15387428e-01\n",
      "Epoch: 27737 mean train loss:  3.99030000e-03, bound:  3.15387428e-01\n",
      "Epoch: 27738 mean train loss:  3.99016263e-03, bound:  3.15387398e-01\n",
      "Epoch: 27739 mean train loss:  3.98992980e-03, bound:  3.15387398e-01\n",
      "Epoch: 27740 mean train loss:  3.98979243e-03, bound:  3.15387398e-01\n",
      "Epoch: 27741 mean train loss:  3.98957869e-03, bound:  3.15387398e-01\n",
      "Epoch: 27742 mean train loss:  3.98936309e-03, bound:  3.15387368e-01\n",
      "Epoch: 27743 mean train loss:  3.98917357e-03, bound:  3.15387338e-01\n",
      "Epoch: 27744 mean train loss:  3.98897240e-03, bound:  3.15387309e-01\n",
      "Epoch: 27745 mean train loss:  3.98882804e-03, bound:  3.15387279e-01\n",
      "Epoch: 27746 mean train loss:  3.98862036e-03, bound:  3.15387279e-01\n",
      "Epoch: 27747 mean train loss:  3.98841500e-03, bound:  3.15387279e-01\n",
      "Epoch: 27748 mean train loss:  3.98825062e-03, bound:  3.15387279e-01\n",
      "Epoch: 27749 mean train loss:  3.98809137e-03, bound:  3.15387249e-01\n",
      "Epoch: 27750 mean train loss:  3.98791954e-03, bound:  3.15387219e-01\n",
      "Epoch: 27751 mean train loss:  3.98775982e-03, bound:  3.15387189e-01\n",
      "Epoch: 27752 mean train loss:  3.98749718e-03, bound:  3.15387160e-01\n",
      "Epoch: 27753 mean train loss:  3.98738123e-03, bound:  3.15387160e-01\n",
      "Epoch: 27754 mean train loss:  3.98722803e-03, bound:  3.15387160e-01\n",
      "Epoch: 27755 mean train loss:  3.98699287e-03, bound:  3.15387100e-01\n",
      "Epoch: 27756 mean train loss:  3.98687599e-03, bound:  3.15387100e-01\n",
      "Epoch: 27757 mean train loss:  3.98672326e-03, bound:  3.15387100e-01\n",
      "Epoch: 27758 mean train loss:  3.98642570e-03, bound:  3.15387070e-01\n",
      "Epoch: 27759 mean train loss:  3.98625946e-03, bound:  3.15387070e-01\n",
      "Epoch: 27760 mean train loss:  3.98613280e-03, bound:  3.15387040e-01\n",
      "Epoch: 27761 mean train loss:  3.98591766e-03, bound:  3.15387040e-01\n",
      "Epoch: 27762 mean train loss:  3.98572953e-03, bound:  3.15386981e-01\n",
      "Epoch: 27763 mean train loss:  3.98555258e-03, bound:  3.15386981e-01\n",
      "Epoch: 27764 mean train loss:  3.98537377e-03, bound:  3.15386951e-01\n",
      "Epoch: 27765 mean train loss:  3.98515770e-03, bound:  3.15386951e-01\n",
      "Epoch: 27766 mean train loss:  3.98502452e-03, bound:  3.15386951e-01\n",
      "Epoch: 27767 mean train loss:  3.98483546e-03, bound:  3.15386921e-01\n",
      "Epoch: 27768 mean train loss:  3.98465293e-03, bound:  3.15386921e-01\n",
      "Epoch: 27769 mean train loss:  3.98442755e-03, bound:  3.15386862e-01\n",
      "Epoch: 27770 mean train loss:  3.98423476e-03, bound:  3.15386862e-01\n",
      "Epoch: 27771 mean train loss:  3.98407504e-03, bound:  3.15386832e-01\n",
      "Epoch: 27772 mean train loss:  3.98385618e-03, bound:  3.15386832e-01\n",
      "Epoch: 27773 mean train loss:  3.98372067e-03, bound:  3.15386802e-01\n",
      "Epoch: 27774 mean train loss:  3.98355117e-03, bound:  3.15386802e-01\n",
      "Epoch: 27775 mean train loss:  3.98339611e-03, bound:  3.15386742e-01\n",
      "Epoch: 27776 mean train loss:  3.98317911e-03, bound:  3.15386742e-01\n",
      "Epoch: 27777 mean train loss:  3.98304872e-03, bound:  3.15386713e-01\n",
      "Epoch: 27778 mean train loss:  3.98286432e-03, bound:  3.15386713e-01\n",
      "Epoch: 27779 mean train loss:  3.98273254e-03, bound:  3.15386713e-01\n",
      "Epoch: 27780 mean train loss:  3.98257794e-03, bound:  3.15386653e-01\n",
      "Epoch: 27781 mean train loss:  3.98246804e-03, bound:  3.15386653e-01\n",
      "Epoch: 27782 mean train loss:  3.98230646e-03, bound:  3.15386653e-01\n",
      "Epoch: 27783 mean train loss:  3.98214767e-03, bound:  3.15386653e-01\n",
      "Epoch: 27784 mean train loss:  3.98206897e-03, bound:  3.15386593e-01\n",
      "Epoch: 27785 mean train loss:  3.98196746e-03, bound:  3.15386593e-01\n",
      "Epoch: 27786 mean train loss:  3.98195162e-03, bound:  3.15386534e-01\n",
      "Epoch: 27787 mean train loss:  3.98185663e-03, bound:  3.15386534e-01\n",
      "Epoch: 27788 mean train loss:  3.98179283e-03, bound:  3.15386534e-01\n",
      "Epoch: 27789 mean train loss:  3.98177188e-03, bound:  3.15386534e-01\n",
      "Epoch: 27790 mean train loss:  3.98179004e-03, bound:  3.15386474e-01\n",
      "Epoch: 27791 mean train loss:  3.98171507e-03, bound:  3.15386474e-01\n",
      "Epoch: 27792 mean train loss:  3.98190273e-03, bound:  3.15386474e-01\n",
      "Epoch: 27793 mean train loss:  3.98211554e-03, bound:  3.15386474e-01\n",
      "Epoch: 27794 mean train loss:  3.98240099e-03, bound:  3.15386415e-01\n",
      "Epoch: 27795 mean train loss:  3.98282614e-03, bound:  3.15386415e-01\n",
      "Epoch: 27796 mean train loss:  3.98326200e-03, bound:  3.15386385e-01\n",
      "Epoch: 27797 mean train loss:  3.98396421e-03, bound:  3.15386415e-01\n",
      "Epoch: 27798 mean train loss:  3.98475397e-03, bound:  3.15386385e-01\n",
      "Epoch: 27799 mean train loss:  3.98549996e-03, bound:  3.15386385e-01\n",
      "Epoch: 27800 mean train loss:  3.98622360e-03, bound:  3.15386295e-01\n",
      "Epoch: 27801 mean train loss:  3.98686575e-03, bound:  3.15386355e-01\n",
      "Epoch: 27802 mean train loss:  3.98723315e-03, bound:  3.15386266e-01\n",
      "Epoch: 27803 mean train loss:  3.98718333e-03, bound:  3.15386295e-01\n",
      "Epoch: 27804 mean train loss:  3.98654724e-03, bound:  3.15386266e-01\n",
      "Epoch: 27805 mean train loss:  3.98526527e-03, bound:  3.15386266e-01\n",
      "Epoch: 27806 mean train loss:  3.98344081e-03, bound:  3.15386206e-01\n",
      "Epoch: 27807 mean train loss:  3.98148084e-03, bound:  3.15386206e-01\n",
      "Epoch: 27808 mean train loss:  3.97960562e-03, bound:  3.15386176e-01\n",
      "Epoch: 27809 mean train loss:  3.97803495e-03, bound:  3.15386176e-01\n",
      "Epoch: 27810 mean train loss:  3.97716137e-03, bound:  3.15386146e-01\n",
      "Epoch: 27811 mean train loss:  3.97683261e-03, bound:  3.15386146e-01\n",
      "Epoch: 27812 mean train loss:  3.97685776e-03, bound:  3.15386146e-01\n",
      "Epoch: 27813 mean train loss:  3.97712132e-03, bound:  3.15386087e-01\n",
      "Epoch: 27814 mean train loss:  3.97750596e-03, bound:  3.15386087e-01\n",
      "Epoch: 27815 mean train loss:  3.97787336e-03, bound:  3.15386057e-01\n",
      "Epoch: 27816 mean train loss:  3.97809129e-03, bound:  3.15386057e-01\n",
      "Epoch: 27817 mean train loss:  3.97799769e-03, bound:  3.15386027e-01\n",
      "Epoch: 27818 mean train loss:  3.97769082e-03, bound:  3.15386027e-01\n",
      "Epoch: 27819 mean train loss:  3.97717720e-03, bound:  3.15385967e-01\n",
      "Epoch: 27820 mean train loss:  3.97654902e-03, bound:  3.15385967e-01\n",
      "Epoch: 27821 mean train loss:  3.97589989e-03, bound:  3.15385967e-01\n",
      "Epoch: 27822 mean train loss:  3.97525122e-03, bound:  3.15385967e-01\n",
      "Epoch: 27823 mean train loss:  3.97476647e-03, bound:  3.15385967e-01\n",
      "Epoch: 27824 mean train loss:  3.97446705e-03, bound:  3.15385908e-01\n",
      "Epoch: 27825 mean train loss:  3.97426961e-03, bound:  3.15385908e-01\n",
      "Epoch: 27826 mean train loss:  3.97421187e-03, bound:  3.15385848e-01\n",
      "Epoch: 27827 mean train loss:  3.97427054e-03, bound:  3.15385848e-01\n",
      "Epoch: 27828 mean train loss:  3.97427101e-03, bound:  3.15385848e-01\n",
      "Epoch: 27829 mean train loss:  3.97434412e-03, bound:  3.15385848e-01\n",
      "Epoch: 27830 mean train loss:  3.97432921e-03, bound:  3.15385789e-01\n",
      "Epoch: 27831 mean train loss:  3.97422584e-03, bound:  3.15385789e-01\n",
      "Epoch: 27832 mean train loss:  3.97409499e-03, bound:  3.15385729e-01\n",
      "Epoch: 27833 mean train loss:  3.97381932e-03, bound:  3.15385729e-01\n",
      "Epoch: 27834 mean train loss:  3.97352129e-03, bound:  3.15385729e-01\n",
      "Epoch: 27835 mean train loss:  3.97315342e-03, bound:  3.15385729e-01\n",
      "Epoch: 27836 mean train loss:  3.97281582e-03, bound:  3.15385699e-01\n",
      "Epoch: 27837 mean train loss:  3.97249032e-03, bound:  3.15385699e-01\n",
      "Epoch: 27838 mean train loss:  3.97216482e-03, bound:  3.15385640e-01\n",
      "Epoch: 27839 mean train loss:  3.97192361e-03, bound:  3.15385640e-01\n",
      "Epoch: 27840 mean train loss:  3.97164095e-03, bound:  3.15385640e-01\n",
      "Epoch: 27841 mean train loss:  3.97142442e-03, bound:  3.15385610e-01\n",
      "Epoch: 27842 mean train loss:  3.97120649e-03, bound:  3.15385610e-01\n",
      "Epoch: 27843 mean train loss:  3.97101976e-03, bound:  3.15385580e-01\n",
      "Epoch: 27844 mean train loss:  3.97092104e-03, bound:  3.15385580e-01\n",
      "Epoch: 27845 mean train loss:  3.97069426e-03, bound:  3.15385520e-01\n",
      "Epoch: 27846 mean train loss:  3.97053687e-03, bound:  3.15385520e-01\n",
      "Epoch: 27847 mean train loss:  3.97033477e-03, bound:  3.15385491e-01\n",
      "Epoch: 27848 mean train loss:  3.97017552e-03, bound:  3.15385491e-01\n",
      "Epoch: 27849 mean train loss:  3.96996364e-03, bound:  3.15385461e-01\n",
      "Epoch: 27850 mean train loss:  3.96982627e-03, bound:  3.15385461e-01\n",
      "Epoch: 27851 mean train loss:  3.96968704e-03, bound:  3.15385401e-01\n",
      "Epoch: 27852 mean train loss:  3.96952732e-03, bound:  3.15385401e-01\n",
      "Epoch: 27853 mean train loss:  3.96944536e-03, bound:  3.15385401e-01\n",
      "Epoch: 27854 mean train loss:  3.96929123e-03, bound:  3.15385371e-01\n",
      "Epoch: 27855 mean train loss:  3.96910543e-03, bound:  3.15385342e-01\n",
      "Epoch: 27856 mean train loss:  3.96901229e-03, bound:  3.15385342e-01\n",
      "Epoch: 27857 mean train loss:  3.96886328e-03, bound:  3.15385342e-01\n",
      "Epoch: 27858 mean train loss:  3.96873197e-03, bound:  3.15385282e-01\n",
      "Epoch: 27859 mean train loss:  3.96862719e-03, bound:  3.15385282e-01\n",
      "Epoch: 27860 mean train loss:  3.96855967e-03, bound:  3.15385252e-01\n",
      "Epoch: 27861 mean train loss:  3.96849960e-03, bound:  3.15385252e-01\n",
      "Epoch: 27862 mean train loss:  3.96842742e-03, bound:  3.15385252e-01\n",
      "Epoch: 27863 mean train loss:  3.96828493e-03, bound:  3.15385193e-01\n",
      "Epoch: 27864 mean train loss:  3.96814151e-03, bound:  3.15385222e-01\n",
      "Epoch: 27865 mean train loss:  3.96806980e-03, bound:  3.15385163e-01\n",
      "Epoch: 27866 mean train loss:  3.96795804e-03, bound:  3.15385163e-01\n",
      "Epoch: 27867 mean train loss:  3.96786025e-03, bound:  3.15385133e-01\n",
      "Epoch: 27868 mean train loss:  3.96779971e-03, bound:  3.15385133e-01\n",
      "Epoch: 27869 mean train loss:  3.96781834e-03, bound:  3.15385103e-01\n",
      "Epoch: 27870 mean train loss:  3.96783650e-03, bound:  3.15385103e-01\n",
      "Epoch: 27871 mean train loss:  3.96794733e-03, bound:  3.15385044e-01\n",
      "Epoch: 27872 mean train loss:  3.96810891e-03, bound:  3.15385044e-01\n",
      "Epoch: 27873 mean train loss:  3.96833429e-03, bound:  3.15385044e-01\n",
      "Epoch: 27874 mean train loss:  3.96852382e-03, bound:  3.15385044e-01\n",
      "Epoch: 27875 mean train loss:  3.96875758e-03, bound:  3.15385014e-01\n",
      "Epoch: 27876 mean train loss:  3.96906212e-03, bound:  3.15385014e-01\n",
      "Epoch: 27877 mean train loss:  3.96936107e-03, bound:  3.15384924e-01\n",
      "Epoch: 27878 mean train loss:  3.96981370e-03, bound:  3.15385014e-01\n",
      "Epoch: 27879 mean train loss:  3.97030916e-03, bound:  3.15384924e-01\n",
      "Epoch: 27880 mean train loss:  3.97075573e-03, bound:  3.15384924e-01\n",
      "Epoch: 27881 mean train loss:  3.97113850e-03, bound:  3.15384865e-01\n",
      "Epoch: 27882 mean train loss:  3.97136854e-03, bound:  3.15384924e-01\n",
      "Epoch: 27883 mean train loss:  3.97112127e-03, bound:  3.15384835e-01\n",
      "Epoch: 27884 mean train loss:  3.97051545e-03, bound:  3.15384865e-01\n",
      "Epoch: 27885 mean train loss:  3.96945234e-03, bound:  3.15384805e-01\n",
      "Epoch: 27886 mean train loss:  3.96817038e-03, bound:  3.15384835e-01\n",
      "Epoch: 27887 mean train loss:  3.96667328e-03, bound:  3.15384775e-01\n",
      "Epoch: 27888 mean train loss:  3.96524183e-03, bound:  3.15384805e-01\n",
      "Epoch: 27889 mean train loss:  3.96395940e-03, bound:  3.15384746e-01\n",
      "Epoch: 27890 mean train loss:  3.96302296e-03, bound:  3.15384746e-01\n",
      "Epoch: 27891 mean train loss:  3.96245252e-03, bound:  3.15384716e-01\n",
      "Epoch: 27892 mean train loss:  3.96214752e-03, bound:  3.15384716e-01\n",
      "Epoch: 27893 mean train loss:  3.96208791e-03, bound:  3.15384716e-01\n",
      "Epoch: 27894 mean train loss:  3.96214379e-03, bound:  3.15384686e-01\n",
      "Epoch: 27895 mean train loss:  3.96221550e-03, bound:  3.15384686e-01\n",
      "Epoch: 27896 mean train loss:  3.96226952e-03, bound:  3.15384626e-01\n",
      "Epoch: 27897 mean train loss:  3.96227837e-03, bound:  3.15384626e-01\n",
      "Epoch: 27898 mean train loss:  3.96210141e-03, bound:  3.15384597e-01\n",
      "Epoch: 27899 mean train loss:  3.96187138e-03, bound:  3.15384597e-01\n",
      "Epoch: 27900 mean train loss:  3.96152958e-03, bound:  3.15384567e-01\n",
      "Epoch: 27901 mean train loss:  3.96123435e-03, bound:  3.15384567e-01\n",
      "Epoch: 27902 mean train loss:  3.96084087e-03, bound:  3.15384537e-01\n",
      "Epoch: 27903 mean train loss:  3.96048557e-03, bound:  3.15384537e-01\n",
      "Epoch: 27904 mean train loss:  3.96018615e-03, bound:  3.15384477e-01\n",
      "Epoch: 27905 mean train loss:  3.95988906e-03, bound:  3.15384477e-01\n",
      "Epoch: 27906 mean train loss:  3.95967066e-03, bound:  3.15384448e-01\n",
      "Epoch: 27907 mean train loss:  3.95944575e-03, bound:  3.15384448e-01\n",
      "Epoch: 27908 mean train loss:  3.95933539e-03, bound:  3.15384418e-01\n",
      "Epoch: 27909 mean train loss:  3.95920407e-03, bound:  3.15384418e-01\n",
      "Epoch: 27910 mean train loss:  3.95908253e-03, bound:  3.15384358e-01\n",
      "Epoch: 27911 mean train loss:  3.95895680e-03, bound:  3.15384358e-01\n",
      "Epoch: 27912 mean train loss:  3.95881664e-03, bound:  3.15384328e-01\n",
      "Epoch: 27913 mean train loss:  3.95862013e-03, bound:  3.15384328e-01\n",
      "Epoch: 27914 mean train loss:  3.95847857e-03, bound:  3.15384328e-01\n",
      "Epoch: 27915 mean train loss:  3.95824481e-03, bound:  3.15384299e-01\n",
      "Epoch: 27916 mean train loss:  3.95808509e-03, bound:  3.15384299e-01\n",
      "Epoch: 27917 mean train loss:  3.95790767e-03, bound:  3.15384239e-01\n",
      "Epoch: 27918 mean train loss:  3.95769207e-03, bound:  3.15384239e-01\n",
      "Epoch: 27919 mean train loss:  3.95748112e-03, bound:  3.15384239e-01\n",
      "Epoch: 27920 mean train loss:  3.95727903e-03, bound:  3.15384239e-01\n",
      "Epoch: 27921 mean train loss:  3.95707507e-03, bound:  3.15384209e-01\n",
      "Epoch: 27922 mean train loss:  3.95686598e-03, bound:  3.15384179e-01\n",
      "Epoch: 27923 mean train loss:  3.95665737e-03, bound:  3.15384120e-01\n",
      "Epoch: 27924 mean train loss:  3.95646505e-03, bound:  3.15384120e-01\n",
      "Epoch: 27925 mean train loss:  3.95621918e-03, bound:  3.15384120e-01\n",
      "Epoch: 27926 mean train loss:  3.95606831e-03, bound:  3.15384120e-01\n",
      "Epoch: 27927 mean train loss:  3.95587645e-03, bound:  3.15384090e-01\n",
      "Epoch: 27928 mean train loss:  3.95574700e-03, bound:  3.15384060e-01\n",
      "Epoch: 27929 mean train loss:  3.95549182e-03, bound:  3.15384030e-01\n",
      "Epoch: 27930 mean train loss:  3.95534839e-03, bound:  3.15384030e-01\n",
      "Epoch: 27931 mean train loss:  3.95519380e-03, bound:  3.15384030e-01\n",
      "Epoch: 27932 mean train loss:  3.95501731e-03, bound:  3.15384001e-01\n",
      "Epoch: 27933 mean train loss:  3.95489624e-03, bound:  3.15384001e-01\n",
      "Epoch: 27934 mean train loss:  3.95479100e-03, bound:  3.15383941e-01\n",
      "Epoch: 27935 mean train loss:  3.95473279e-03, bound:  3.15383941e-01\n",
      "Epoch: 27936 mean train loss:  3.95469880e-03, bound:  3.15383911e-01\n",
      "Epoch: 27937 mean train loss:  3.95466806e-03, bound:  3.15383911e-01\n",
      "Epoch: 27938 mean train loss:  3.95476539e-03, bound:  3.15383881e-01\n",
      "Epoch: 27939 mean train loss:  3.95492930e-03, bound:  3.15383911e-01\n",
      "Epoch: 27940 mean train loss:  3.95525061e-03, bound:  3.15383822e-01\n",
      "Epoch: 27941 mean train loss:  3.95568134e-03, bound:  3.15383822e-01\n",
      "Epoch: 27942 mean train loss:  3.95627366e-03, bound:  3.15383792e-01\n",
      "Epoch: 27943 mean train loss:  3.95723525e-03, bound:  3.15383822e-01\n",
      "Epoch: 27944 mean train loss:  3.95839987e-03, bound:  3.15383792e-01\n",
      "Epoch: 27945 mean train loss:  3.96003248e-03, bound:  3.15383792e-01\n",
      "Epoch: 27946 mean train loss:  3.96218384e-03, bound:  3.15383703e-01\n",
      "Epoch: 27947 mean train loss:  3.96463787e-03, bound:  3.15383762e-01\n",
      "Epoch: 27948 mean train loss:  3.96708585e-03, bound:  3.15383673e-01\n",
      "Epoch: 27949 mean train loss:  3.96893173e-03, bound:  3.15383703e-01\n",
      "Epoch: 27950 mean train loss:  3.96945002e-03, bound:  3.15383673e-01\n",
      "Epoch: 27951 mean train loss:  3.96814710e-03, bound:  3.15383703e-01\n",
      "Epoch: 27952 mean train loss:  3.96473706e-03, bound:  3.15383613e-01\n",
      "Epoch: 27953 mean train loss:  3.95997893e-03, bound:  3.15383673e-01\n",
      "Epoch: 27954 mean train loss:  3.95524735e-03, bound:  3.15383613e-01\n",
      "Epoch: 27955 mean train loss:  3.95189039e-03, bound:  3.15383643e-01\n",
      "Epoch: 27956 mean train loss:  3.95063497e-03, bound:  3.15383613e-01\n",
      "Epoch: 27957 mean train loss:  3.95127153e-03, bound:  3.15383554e-01\n",
      "Epoch: 27958 mean train loss:  3.95302661e-03, bound:  3.15383583e-01\n",
      "Epoch: 27959 mean train loss:  3.95469414e-03, bound:  3.15383554e-01\n",
      "Epoch: 27960 mean train loss:  3.95553466e-03, bound:  3.15383554e-01\n",
      "Epoch: 27961 mean train loss:  3.95513093e-03, bound:  3.15383524e-01\n",
      "Epoch: 27962 mean train loss:  3.95366456e-03, bound:  3.15383524e-01\n",
      "Epoch: 27963 mean train loss:  3.95163195e-03, bound:  3.15383464e-01\n",
      "Epoch: 27964 mean train loss:  3.95006919e-03, bound:  3.15383494e-01\n",
      "Epoch: 27965 mean train loss:  3.94914765e-03, bound:  3.15383464e-01\n",
      "Epoch: 27966 mean train loss:  3.94910108e-03, bound:  3.15383434e-01\n",
      "Epoch: 27967 mean train loss:  3.94965056e-03, bound:  3.15383434e-01\n",
      "Epoch: 27968 mean train loss:  3.95034673e-03, bound:  3.15383375e-01\n",
      "Epoch: 27969 mean train loss:  3.95069877e-03, bound:  3.15383375e-01\n",
      "Epoch: 27970 mean train loss:  3.95051530e-03, bound:  3.15383345e-01\n",
      "Epoch: 27971 mean train loss:  3.94983171e-03, bound:  3.15383345e-01\n",
      "Epoch: 27972 mean train loss:  3.94888781e-03, bound:  3.15383315e-01\n",
      "Epoch: 27973 mean train loss:  3.94807057e-03, bound:  3.15383315e-01\n",
      "Epoch: 27974 mean train loss:  3.94753832e-03, bound:  3.15383255e-01\n",
      "Epoch: 27975 mean train loss:  3.94734927e-03, bound:  3.15383255e-01\n",
      "Epoch: 27976 mean train loss:  3.94742889e-03, bound:  3.15383255e-01\n",
      "Epoch: 27977 mean train loss:  3.94750759e-03, bound:  3.15383226e-01\n",
      "Epoch: 27978 mean train loss:  3.94751877e-03, bound:  3.15383226e-01\n",
      "Epoch: 27979 mean train loss:  3.94747453e-03, bound:  3.15383196e-01\n",
      "Epoch: 27980 mean train loss:  3.94722540e-03, bound:  3.15383196e-01\n",
      "Epoch: 27981 mean train loss:  3.94685566e-03, bound:  3.15383136e-01\n",
      "Epoch: 27982 mean train loss:  3.94638116e-03, bound:  3.15383136e-01\n",
      "Epoch: 27983 mean train loss:  3.94599652e-03, bound:  3.15383106e-01\n",
      "Epoch: 27984 mean train loss:  3.94574553e-03, bound:  3.15383106e-01\n",
      "Epoch: 27985 mean train loss:  3.94565566e-03, bound:  3.15383106e-01\n",
      "Epoch: 27986 mean train loss:  3.94558813e-03, bound:  3.15383077e-01\n",
      "Epoch: 27987 mean train loss:  3.94557556e-03, bound:  3.15383047e-01\n",
      "Epoch: 27988 mean train loss:  3.94555833e-03, bound:  3.15383017e-01\n",
      "Epoch: 27989 mean train loss:  3.94541537e-03, bound:  3.15383017e-01\n",
      "Epoch: 27990 mean train loss:  3.94517183e-03, bound:  3.15382987e-01\n",
      "Epoch: 27991 mean train loss:  3.94487241e-03, bound:  3.15382987e-01\n",
      "Epoch: 27992 mean train loss:  3.94460605e-03, bound:  3.15382928e-01\n",
      "Epoch: 27993 mean train loss:  3.94425262e-03, bound:  3.15382928e-01\n",
      "Epoch: 27994 mean train loss:  3.94403329e-03, bound:  3.15382928e-01\n",
      "Epoch: 27995 mean train loss:  3.94383958e-03, bound:  3.15382898e-01\n",
      "Epoch: 27996 mean train loss:  3.94368777e-03, bound:  3.15382898e-01\n",
      "Epoch: 27997 mean train loss:  3.94355878e-03, bound:  3.15382868e-01\n",
      "Epoch: 27998 mean train loss:  3.94346192e-03, bound:  3.15382868e-01\n",
      "Epoch: 27999 mean train loss:  3.94331152e-03, bound:  3.15382808e-01\n",
      "Epoch: 28000 mean train loss:  3.94316018e-03, bound:  3.15382808e-01\n",
      "Epoch: 28001 mean train loss:  3.94301629e-03, bound:  3.15382808e-01\n",
      "Epoch: 28002 mean train loss:  3.94277880e-03, bound:  3.15382808e-01\n",
      "Epoch: 28003 mean train loss:  3.94260045e-03, bound:  3.15382779e-01\n",
      "Epoch: 28004 mean train loss:  3.94233689e-03, bound:  3.15382779e-01\n",
      "Epoch: 28005 mean train loss:  3.94214643e-03, bound:  3.15382689e-01\n",
      "Epoch: 28006 mean train loss:  3.94189684e-03, bound:  3.15382689e-01\n",
      "Epoch: 28007 mean train loss:  3.94170452e-03, bound:  3.15382689e-01\n",
      "Epoch: 28008 mean train loss:  3.94152245e-03, bound:  3.15382689e-01\n",
      "Epoch: 28009 mean train loss:  3.94134736e-03, bound:  3.15382689e-01\n",
      "Epoch: 28010 mean train loss:  3.94121464e-03, bound:  3.15382659e-01\n",
      "Epoch: 28011 mean train loss:  3.94103117e-03, bound:  3.15382659e-01\n",
      "Epoch: 28012 mean train loss:  3.94092780e-03, bound:  3.15382570e-01\n",
      "Epoch: 28013 mean train loss:  3.94076668e-03, bound:  3.15382570e-01\n",
      "Epoch: 28014 mean train loss:  3.94057576e-03, bound:  3.15382570e-01\n",
      "Epoch: 28015 mean train loss:  3.94037319e-03, bound:  3.15382570e-01\n",
      "Epoch: 28016 mean train loss:  3.94022930e-03, bound:  3.15382540e-01\n",
      "Epoch: 28017 mean train loss:  3.94000160e-03, bound:  3.15382540e-01\n",
      "Epoch: 28018 mean train loss:  3.93984467e-03, bound:  3.15382481e-01\n",
      "Epoch: 28019 mean train loss:  3.93963745e-03, bound:  3.15382481e-01\n",
      "Epoch: 28020 mean train loss:  3.93949775e-03, bound:  3.15382481e-01\n",
      "Epoch: 28021 mean train loss:  3.93924676e-03, bound:  3.15382451e-01\n",
      "Epoch: 28022 mean train loss:  3.93911963e-03, bound:  3.15382451e-01\n",
      "Epoch: 28023 mean train loss:  3.93886352e-03, bound:  3.15382421e-01\n",
      "Epoch: 28024 mean train loss:  3.93871404e-03, bound:  3.15382421e-01\n",
      "Epoch: 28025 mean train loss:  3.93856270e-03, bound:  3.15382361e-01\n",
      "Epoch: 28026 mean train loss:  3.93832615e-03, bound:  3.15382361e-01\n",
      "Epoch: 28027 mean train loss:  3.93817807e-03, bound:  3.15382332e-01\n",
      "Epoch: 28028 mean train loss:  3.93798482e-03, bound:  3.15382332e-01\n",
      "Epoch: 28029 mean train loss:  3.93780507e-03, bound:  3.15382302e-01\n",
      "Epoch: 28030 mean train loss:  3.93759040e-03, bound:  3.15382302e-01\n",
      "Epoch: 28031 mean train loss:  3.93742556e-03, bound:  3.15382302e-01\n",
      "Epoch: 28032 mean train loss:  3.93729610e-03, bound:  3.15382242e-01\n",
      "Epoch: 28033 mean train loss:  3.93708935e-03, bound:  3.15382242e-01\n",
      "Epoch: 28034 mean train loss:  3.93691054e-03, bound:  3.15382212e-01\n",
      "Epoch: 28035 mean train loss:  3.93676013e-03, bound:  3.15382212e-01\n",
      "Epoch: 28036 mean train loss:  3.93654825e-03, bound:  3.15382183e-01\n",
      "Epoch: 28037 mean train loss:  3.93636199e-03, bound:  3.15382183e-01\n",
      "Epoch: 28038 mean train loss:  3.93619435e-03, bound:  3.15382123e-01\n",
      "Epoch: 28039 mean train loss:  3.93604301e-03, bound:  3.15382123e-01\n",
      "Epoch: 28040 mean train loss:  3.93582974e-03, bound:  3.15382123e-01\n",
      "Epoch: 28041 mean train loss:  3.93567095e-03, bound:  3.15382093e-01\n",
      "Epoch: 28042 mean train loss:  3.93549958e-03, bound:  3.15382093e-01\n",
      "Epoch: 28043 mean train loss:  3.93530726e-03, bound:  3.15382093e-01\n",
      "Epoch: 28044 mean train loss:  3.93517129e-03, bound:  3.15382034e-01\n",
      "Epoch: 28045 mean train loss:  3.93494871e-03, bound:  3.15382004e-01\n",
      "Epoch: 28046 mean train loss:  3.93480994e-03, bound:  3.15382004e-01\n",
      "Epoch: 28047 mean train loss:  3.93459620e-03, bound:  3.15382004e-01\n",
      "Epoch: 28048 mean train loss:  3.93442810e-03, bound:  3.15382004e-01\n",
      "Epoch: 28049 mean train loss:  3.93427024e-03, bound:  3.15381974e-01\n",
      "Epoch: 28050 mean train loss:  3.93414032e-03, bound:  3.15381974e-01\n",
      "Epoch: 28051 mean train loss:  3.93393403e-03, bound:  3.15381885e-01\n",
      "Epoch: 28052 mean train loss:  3.93376173e-03, bound:  3.15381885e-01\n",
      "Epoch: 28053 mean train loss:  3.93364253e-03, bound:  3.15381885e-01\n",
      "Epoch: 28054 mean train loss:  3.93341621e-03, bound:  3.15381885e-01\n",
      "Epoch: 28055 mean train loss:  3.93336220e-03, bound:  3.15381855e-01\n",
      "Epoch: 28056 mean train loss:  3.93327652e-03, bound:  3.15381855e-01\n",
      "Epoch: 28057 mean train loss:  3.93317314e-03, bound:  3.15381795e-01\n",
      "Epoch: 28058 mean train loss:  3.93314147e-03, bound:  3.15381795e-01\n",
      "Epoch: 28059 mean train loss:  3.93316383e-03, bound:  3.15381795e-01\n",
      "Epoch: 28060 mean train loss:  3.93329468e-03, bound:  3.15381795e-01\n",
      "Epoch: 28061 mean train loss:  3.93358758e-03, bound:  3.15381736e-01\n",
      "Epoch: 28062 mean train loss:  3.93407512e-03, bound:  3.15381736e-01\n",
      "Epoch: 28063 mean train loss:  3.93485231e-03, bound:  3.15381676e-01\n",
      "Epoch: 28064 mean train loss:  3.93606164e-03, bound:  3.15381736e-01\n",
      "Epoch: 28065 mean train loss:  3.93777899e-03, bound:  3.15381646e-01\n",
      "Epoch: 28066 mean train loss:  3.94010404e-03, bound:  3.15381676e-01\n",
      "Epoch: 28067 mean train loss:  3.94314667e-03, bound:  3.15381587e-01\n",
      "Epoch: 28068 mean train loss:  3.94679699e-03, bound:  3.15381676e-01\n",
      "Epoch: 28069 mean train loss:  3.95060889e-03, bound:  3.15381557e-01\n",
      "Epoch: 28070 mean train loss:  3.95371486e-03, bound:  3.15381646e-01\n",
      "Epoch: 28071 mean train loss:  3.95467458e-03, bound:  3.15381527e-01\n",
      "Epoch: 28072 mean train loss:  3.95233976e-03, bound:  3.15381587e-01\n",
      "Epoch: 28073 mean train loss:  3.94662563e-03, bound:  3.15381527e-01\n",
      "Epoch: 28074 mean train loss:  3.93907214e-03, bound:  3.15381557e-01\n",
      "Epoch: 28075 mean train loss:  3.93262412e-03, bound:  3.15381527e-01\n",
      "Epoch: 28076 mean train loss:  3.92951025e-03, bound:  3.15381527e-01\n",
      "Epoch: 28077 mean train loss:  3.93015286e-03, bound:  3.15381467e-01\n",
      "Epoch: 28078 mean train loss:  3.93300783e-03, bound:  3.15381467e-01\n",
      "Epoch: 28079 mean train loss:  3.93588236e-03, bound:  3.15381467e-01\n",
      "Epoch: 28080 mean train loss:  3.93722160e-03, bound:  3.15381408e-01\n",
      "Epoch: 28081 mean train loss:  3.93613474e-03, bound:  3.15381438e-01\n",
      "Epoch: 28082 mean train loss:  3.93324252e-03, bound:  3.15381408e-01\n",
      "Epoch: 28083 mean train loss:  3.93016171e-03, bound:  3.15381408e-01\n",
      "Epoch: 28084 mean train loss:  3.92825017e-03, bound:  3.15381408e-01\n",
      "Epoch: 28085 mean train loss:  3.92809324e-03, bound:  3.15381348e-01\n",
      "Epoch: 28086 mean train loss:  3.92915867e-03, bound:  3.15381318e-01\n",
      "Epoch: 28087 mean train loss:  3.93048953e-03, bound:  3.15381289e-01\n",
      "Epoch: 28088 mean train loss:  3.93099058e-03, bound:  3.15381318e-01\n",
      "Epoch: 28089 mean train loss:  3.93046904e-03, bound:  3.15381289e-01\n",
      "Epoch: 28090 mean train loss:  3.92910559e-03, bound:  3.15381289e-01\n",
      "Epoch: 28091 mean train loss:  3.92761454e-03, bound:  3.15381229e-01\n",
      "Epoch: 28092 mean train loss:  3.92670091e-03, bound:  3.15381229e-01\n",
      "Epoch: 28093 mean train loss:  3.92668229e-03, bound:  3.15381199e-01\n",
      "Epoch: 28094 mean train loss:  3.92713817e-03, bound:  3.15381199e-01\n",
      "Epoch: 28095 mean train loss:  3.92768160e-03, bound:  3.15381199e-01\n",
      "Epoch: 28096 mean train loss:  3.92775005e-03, bound:  3.15381140e-01\n",
      "Epoch: 28097 mean train loss:  3.92737240e-03, bound:  3.15381140e-01\n",
      "Epoch: 28098 mean train loss:  3.92654445e-03, bound:  3.15381110e-01\n",
      "Epoch: 28099 mean train loss:  3.92582314e-03, bound:  3.15381110e-01\n",
      "Epoch: 28100 mean train loss:  3.92535608e-03, bound:  3.15381110e-01\n",
      "Epoch: 28101 mean train loss:  3.92519031e-03, bound:  3.15381080e-01\n",
      "Epoch: 28102 mean train loss:  3.92532954e-03, bound:  3.15381080e-01\n",
      "Epoch: 28103 mean train loss:  3.92550556e-03, bound:  3.15381020e-01\n",
      "Epoch: 28104 mean train loss:  3.92541569e-03, bound:  3.15381020e-01\n",
      "Epoch: 28105 mean train loss:  3.92513396e-03, bound:  3.15380991e-01\n",
      "Epoch: 28106 mean train loss:  3.92479403e-03, bound:  3.15380991e-01\n",
      "Epoch: 28107 mean train loss:  3.92431812e-03, bound:  3.15380961e-01\n",
      "Epoch: 28108 mean train loss:  3.92401218e-03, bound:  3.15380961e-01\n",
      "Epoch: 28109 mean train loss:  3.92374257e-03, bound:  3.15380901e-01\n",
      "Epoch: 28110 mean train loss:  3.92361870e-03, bound:  3.15380901e-01\n",
      "Epoch: 28111 mean train loss:  3.92360613e-03, bound:  3.15380901e-01\n",
      "Epoch: 28112 mean train loss:  3.92358936e-03, bound:  3.15380871e-01\n",
      "Epoch: 28113 mean train loss:  3.92339472e-03, bound:  3.15380871e-01\n",
      "Epoch: 28114 mean train loss:  3.92313069e-03, bound:  3.15380841e-01\n",
      "Epoch: 28115 mean train loss:  3.92286899e-03, bound:  3.15380841e-01\n",
      "Epoch: 28116 mean train loss:  3.92260868e-03, bound:  3.15380782e-01\n",
      "Epoch: 28117 mean train loss:  3.92237166e-03, bound:  3.15380782e-01\n",
      "Epoch: 28118 mean train loss:  3.92223010e-03, bound:  3.15380752e-01\n",
      "Epoch: 28119 mean train loss:  3.92202148e-03, bound:  3.15380752e-01\n",
      "Epoch: 28120 mean train loss:  3.92189948e-03, bound:  3.15380722e-01\n",
      "Epoch: 28121 mean train loss:  3.92175978e-03, bound:  3.15380722e-01\n",
      "Epoch: 28122 mean train loss:  3.92162474e-03, bound:  3.15380722e-01\n",
      "Epoch: 28123 mean train loss:  3.92144080e-03, bound:  3.15380663e-01\n",
      "Epoch: 28124 mean train loss:  3.92121729e-03, bound:  3.15380663e-01\n",
      "Epoch: 28125 mean train loss:  3.92101007e-03, bound:  3.15380633e-01\n",
      "Epoch: 28126 mean train loss:  3.92081821e-03, bound:  3.15380633e-01\n",
      "Epoch: 28127 mean train loss:  3.92060541e-03, bound:  3.15380603e-01\n",
      "Epoch: 28128 mean train loss:  3.92048806e-03, bound:  3.15380603e-01\n",
      "Epoch: 28129 mean train loss:  3.92033393e-03, bound:  3.15380573e-01\n",
      "Epoch: 28130 mean train loss:  3.92013835e-03, bound:  3.15380573e-01\n",
      "Epoch: 28131 mean train loss:  3.91998654e-03, bound:  3.15380543e-01\n",
      "Epoch: 28132 mean train loss:  3.91978910e-03, bound:  3.15380543e-01\n",
      "Epoch: 28133 mean train loss:  3.91962565e-03, bound:  3.15380514e-01\n",
      "Epoch: 28134 mean train loss:  3.91946221e-03, bound:  3.15380484e-01\n",
      "Epoch: 28135 mean train loss:  3.91923822e-03, bound:  3.15380484e-01\n",
      "Epoch: 28136 mean train loss:  3.91904311e-03, bound:  3.15380454e-01\n",
      "Epoch: 28137 mean train loss:  3.91887594e-03, bound:  3.15380454e-01\n",
      "Epoch: 28138 mean train loss:  3.91876511e-03, bound:  3.15380424e-01\n",
      "Epoch: 28139 mean train loss:  3.91856395e-03, bound:  3.15380424e-01\n",
      "Epoch: 28140 mean train loss:  3.91838094e-03, bound:  3.15380394e-01\n",
      "Epoch: 28141 mean train loss:  3.91823100e-03, bound:  3.15380365e-01\n",
      "Epoch: 28142 mean train loss:  3.91807733e-03, bound:  3.15380365e-01\n",
      "Epoch: 28143 mean train loss:  3.91788129e-03, bound:  3.15380335e-01\n",
      "Epoch: 28144 mean train loss:  3.91770620e-03, bound:  3.15380335e-01\n",
      "Epoch: 28145 mean train loss:  3.91751714e-03, bound:  3.15380305e-01\n",
      "Epoch: 28146 mean train loss:  3.91732017e-03, bound:  3.15380305e-01\n",
      "Epoch: 28147 mean train loss:  3.91720328e-03, bound:  3.15380275e-01\n",
      "Epoch: 28148 mean train loss:  3.91702540e-03, bound:  3.15380275e-01\n",
      "Epoch: 28149 mean train loss:  3.91685544e-03, bound:  3.15380216e-01\n",
      "Epoch: 28150 mean train loss:  3.91665613e-03, bound:  3.15380216e-01\n",
      "Epoch: 28151 mean train loss:  3.91649734e-03, bound:  3.15380186e-01\n",
      "Epoch: 28152 mean train loss:  3.91632644e-03, bound:  3.15380186e-01\n",
      "Epoch: 28153 mean train loss:  3.91616113e-03, bound:  3.15380186e-01\n",
      "Epoch: 28154 mean train loss:  3.91596416e-03, bound:  3.15380156e-01\n",
      "Epoch: 28155 mean train loss:  3.91578814e-03, bound:  3.15380156e-01\n",
      "Epoch: 28156 mean train loss:  3.91563494e-03, bound:  3.15380096e-01\n",
      "Epoch: 28157 mean train loss:  3.91544402e-03, bound:  3.15380096e-01\n",
      "Epoch: 28158 mean train loss:  3.91531037e-03, bound:  3.15380067e-01\n",
      "Epoch: 28159 mean train loss:  3.91504355e-03, bound:  3.15380067e-01\n",
      "Epoch: 28160 mean train loss:  3.91490245e-03, bound:  3.15380067e-01\n",
      "Epoch: 28161 mean train loss:  3.91469756e-03, bound:  3.15380037e-01\n",
      "Epoch: 28162 mean train loss:  3.91454156e-03, bound:  3.15380007e-01\n",
      "Epoch: 28163 mean train loss:  3.91433761e-03, bound:  3.15379977e-01\n",
      "Epoch: 28164 mean train loss:  3.91424680e-03, bound:  3.15379947e-01\n",
      "Epoch: 28165 mean train loss:  3.91402328e-03, bound:  3.15379947e-01\n",
      "Epoch: 28166 mean train loss:  3.91387055e-03, bound:  3.15379947e-01\n",
      "Epoch: 28167 mean train loss:  3.91365495e-03, bound:  3.15379918e-01\n",
      "Epoch: 28168 mean train loss:  3.91354272e-03, bound:  3.15379918e-01\n",
      "Epoch: 28169 mean train loss:  3.91335180e-03, bound:  3.15379858e-01\n",
      "Epoch: 28170 mean train loss:  3.91318789e-03, bound:  3.15379858e-01\n",
      "Epoch: 28171 mean train loss:  3.91301513e-03, bound:  3.15379858e-01\n",
      "Epoch: 28172 mean train loss:  3.91286565e-03, bound:  3.15379858e-01\n",
      "Epoch: 28173 mean train loss:  3.91270872e-03, bound:  3.15379828e-01\n",
      "Epoch: 28174 mean train loss:  3.91255505e-03, bound:  3.15379769e-01\n",
      "Epoch: 28175 mean train loss:  3.91246565e-03, bound:  3.15379769e-01\n",
      "Epoch: 28176 mean train loss:  3.91224585e-03, bound:  3.15379769e-01\n",
      "Epoch: 28177 mean train loss:  3.91210802e-03, bound:  3.15379769e-01\n",
      "Epoch: 28178 mean train loss:  3.91190266e-03, bound:  3.15379739e-01\n",
      "Epoch: 28179 mean train loss:  3.91174760e-03, bound:  3.15379739e-01\n",
      "Epoch: 28180 mean train loss:  3.91153991e-03, bound:  3.15379649e-01\n",
      "Epoch: 28181 mean train loss:  3.91143141e-03, bound:  3.15379649e-01\n",
      "Epoch: 28182 mean train loss:  3.91124142e-03, bound:  3.15379649e-01\n",
      "Epoch: 28183 mean train loss:  3.91109055e-03, bound:  3.15379649e-01\n",
      "Epoch: 28184 mean train loss:  3.91087402e-03, bound:  3.15379620e-01\n",
      "Epoch: 28185 mean train loss:  3.91072826e-03, bound:  3.15379620e-01\n",
      "Epoch: 28186 mean train loss:  3.91059089e-03, bound:  3.15379590e-01\n",
      "Epoch: 28187 mean train loss:  3.91046144e-03, bound:  3.15379560e-01\n",
      "Epoch: 28188 mean train loss:  3.91023234e-03, bound:  3.15379560e-01\n",
      "Epoch: 28189 mean train loss:  3.91002884e-03, bound:  3.15379530e-01\n",
      "Epoch: 28190 mean train loss:  3.90995899e-03, bound:  3.15379500e-01\n",
      "Epoch: 28191 mean train loss:  3.90976435e-03, bound:  3.15379500e-01\n",
      "Epoch: 28192 mean train loss:  3.90958926e-03, bound:  3.15379471e-01\n",
      "Epoch: 28193 mean train loss:  3.90941510e-03, bound:  3.15379471e-01\n",
      "Epoch: 28194 mean train loss:  3.90930241e-03, bound:  3.15379471e-01\n",
      "Epoch: 28195 mean train loss:  3.90918180e-03, bound:  3.15379471e-01\n",
      "Epoch: 28196 mean train loss:  3.90912825e-03, bound:  3.15379411e-01\n",
      "Epoch: 28197 mean train loss:  3.90913291e-03, bound:  3.15379411e-01\n",
      "Epoch: 28198 mean train loss:  3.90915480e-03, bound:  3.15379381e-01\n",
      "Epoch: 28199 mean train loss:  3.90927587e-03, bound:  3.15379381e-01\n",
      "Epoch: 28200 mean train loss:  3.90947564e-03, bound:  3.15379322e-01\n",
      "Epoch: 28201 mean train loss:  3.90983885e-03, bound:  3.15379322e-01\n",
      "Epoch: 28202 mean train loss:  3.91033338e-03, bound:  3.15379292e-01\n",
      "Epoch: 28203 mean train loss:  3.91100999e-03, bound:  3.15379292e-01\n",
      "Epoch: 28204 mean train loss:  3.91217368e-03, bound:  3.15379262e-01\n",
      "Epoch: 28205 mean train loss:  3.91370896e-03, bound:  3.15379292e-01\n",
      "Epoch: 28206 mean train loss:  3.91552458e-03, bound:  3.15379202e-01\n",
      "Epoch: 28207 mean train loss:  3.91777605e-03, bound:  3.15379262e-01\n",
      "Epoch: 28208 mean train loss:  3.92027479e-03, bound:  3.15379173e-01\n",
      "Epoch: 28209 mean train loss:  3.92242568e-03, bound:  3.15379202e-01\n",
      "Epoch: 28210 mean train loss:  3.92369740e-03, bound:  3.15379143e-01\n",
      "Epoch: 28211 mean train loss:  3.92324384e-03, bound:  3.15379202e-01\n",
      "Epoch: 28212 mean train loss:  3.92088993e-03, bound:  3.15379083e-01\n",
      "Epoch: 28213 mean train loss:  3.91681213e-03, bound:  3.15379173e-01\n",
      "Epoch: 28214 mean train loss:  3.91197391e-03, bound:  3.15379083e-01\n",
      "Epoch: 28215 mean train loss:  3.90785467e-03, bound:  3.15379083e-01\n",
      "Epoch: 28216 mean train loss:  3.90540645e-03, bound:  3.15379083e-01\n",
      "Epoch: 28217 mean train loss:  3.90502904e-03, bound:  3.15379053e-01\n",
      "Epoch: 28218 mean train loss:  3.90624721e-03, bound:  3.15379053e-01\n",
      "Epoch: 28219 mean train loss:  3.90795525e-03, bound:  3.15379053e-01\n",
      "Epoch: 28220 mean train loss:  3.90921440e-03, bound:  3.15379053e-01\n",
      "Epoch: 28221 mean train loss:  3.90952779e-03, bound:  3.15378964e-01\n",
      "Epoch: 28222 mean train loss:  3.90868261e-03, bound:  3.15378964e-01\n",
      "Epoch: 28223 mean train loss:  3.90706072e-03, bound:  3.15378934e-01\n",
      "Epoch: 28224 mean train loss:  3.90521064e-03, bound:  3.15378934e-01\n",
      "Epoch: 28225 mean train loss:  3.90383415e-03, bound:  3.15378934e-01\n",
      "Epoch: 28226 mean train loss:  3.90334008e-03, bound:  3.15378934e-01\n",
      "Epoch: 28227 mean train loss:  3.90365580e-03, bound:  3.15378934e-01\n",
      "Epoch: 28228 mean train loss:  3.90419457e-03, bound:  3.15378875e-01\n",
      "Epoch: 28229 mean train loss:  3.90460272e-03, bound:  3.15378875e-01\n",
      "Epoch: 28230 mean train loss:  3.90471076e-03, bound:  3.15378845e-01\n",
      "Epoch: 28231 mean train loss:  3.90442112e-03, bound:  3.15378845e-01\n",
      "Epoch: 28232 mean train loss:  3.90367047e-03, bound:  3.15378815e-01\n",
      "Epoch: 28233 mean train loss:  3.90285230e-03, bound:  3.15378815e-01\n",
      "Epoch: 28234 mean train loss:  3.90220340e-03, bound:  3.15378755e-01\n",
      "Epoch: 28235 mean train loss:  3.90182948e-03, bound:  3.15378755e-01\n",
      "Epoch: 28236 mean train loss:  3.90181295e-03, bound:  3.15378755e-01\n",
      "Epoch: 28237 mean train loss:  3.90185323e-03, bound:  3.15378726e-01\n",
      "Epoch: 28238 mean train loss:  3.90197337e-03, bound:  3.15378726e-01\n",
      "Epoch: 28239 mean train loss:  3.90196918e-03, bound:  3.15378696e-01\n",
      "Epoch: 28240 mean train loss:  3.90179828e-03, bound:  3.15378696e-01\n",
      "Epoch: 28241 mean train loss:  3.90146440e-03, bound:  3.15378636e-01\n",
      "Epoch: 28242 mean train loss:  3.90106463e-03, bound:  3.15378636e-01\n",
      "Epoch: 28243 mean train loss:  3.90068046e-03, bound:  3.15378606e-01\n",
      "Epoch: 28244 mean train loss:  3.90038523e-03, bound:  3.15378606e-01\n",
      "Epoch: 28245 mean train loss:  3.90009000e-03, bound:  3.15378577e-01\n",
      "Epoch: 28246 mean train loss:  3.89995333e-03, bound:  3.15378517e-01\n",
      "Epoch: 28247 mean train loss:  3.89993819e-03, bound:  3.15378517e-01\n",
      "Epoch: 28248 mean train loss:  3.89983971e-03, bound:  3.15378517e-01\n",
      "Epoch: 28249 mean train loss:  3.89981386e-03, bound:  3.15378517e-01\n",
      "Epoch: 28250 mean train loss:  3.89964110e-03, bound:  3.15378487e-01\n",
      "Epoch: 28251 mean train loss:  3.89938755e-03, bound:  3.15378487e-01\n",
      "Epoch: 28252 mean train loss:  3.89918312e-03, bound:  3.15378428e-01\n",
      "Epoch: 28253 mean train loss:  3.89888650e-03, bound:  3.15378428e-01\n",
      "Epoch: 28254 mean train loss:  3.89862107e-03, bound:  3.15378398e-01\n",
      "Epoch: 28255 mean train loss:  3.89835960e-03, bound:  3.15378398e-01\n",
      "Epoch: 28256 mean train loss:  3.89822712e-03, bound:  3.15378368e-01\n",
      "Epoch: 28257 mean train loss:  3.89809185e-03, bound:  3.15378368e-01\n",
      "Epoch: 28258 mean train loss:  3.89802083e-03, bound:  3.15378368e-01\n",
      "Epoch: 28259 mean train loss:  3.89790209e-03, bound:  3.15378368e-01\n",
      "Epoch: 28260 mean train loss:  3.89783573e-03, bound:  3.15378368e-01\n",
      "Epoch: 28261 mean train loss:  3.89774633e-03, bound:  3.15378278e-01\n",
      "Epoch: 28262 mean train loss:  3.89751419e-03, bound:  3.15378278e-01\n",
      "Epoch: 28263 mean train loss:  3.89731582e-03, bound:  3.15378249e-01\n",
      "Epoch: 28264 mean train loss:  3.89704574e-03, bound:  3.15378249e-01\n",
      "Epoch: 28265 mean train loss:  3.89679591e-03, bound:  3.15378249e-01\n",
      "Epoch: 28266 mean train loss:  3.89655656e-03, bound:  3.15378249e-01\n",
      "Epoch: 28267 mean train loss:  3.89632233e-03, bound:  3.15378189e-01\n",
      "Epoch: 28268 mean train loss:  3.89618659e-03, bound:  3.15378189e-01\n",
      "Epoch: 28269 mean train loss:  3.89599544e-03, bound:  3.15378189e-01\n",
      "Epoch: 28270 mean train loss:  3.89582710e-03, bound:  3.15378129e-01\n",
      "Epoch: 28271 mean train loss:  3.89567111e-03, bound:  3.15378129e-01\n",
      "Epoch: 28272 mean train loss:  3.89551814e-03, bound:  3.15378129e-01\n",
      "Epoch: 28273 mean train loss:  3.89538519e-03, bound:  3.15378129e-01\n",
      "Epoch: 28274 mean train loss:  3.89525667e-03, bound:  3.15378070e-01\n",
      "Epoch: 28275 mean train loss:  3.89516540e-03, bound:  3.15378070e-01\n",
      "Epoch: 28276 mean train loss:  3.89498984e-03, bound:  3.15378040e-01\n",
      "Epoch: 28277 mean train loss:  3.89482081e-03, bound:  3.15378040e-01\n",
      "Epoch: 28278 mean train loss:  3.89465201e-03, bound:  3.15378010e-01\n",
      "Epoch: 28279 mean train loss:  3.89442500e-03, bound:  3.15377980e-01\n",
      "Epoch: 28280 mean train loss:  3.89421429e-03, bound:  3.15377951e-01\n",
      "Epoch: 28281 mean train loss:  3.89398634e-03, bound:  3.15377951e-01\n",
      "Epoch: 28282 mean train loss:  3.89382592e-03, bound:  3.15377951e-01\n",
      "Epoch: 28283 mean train loss:  3.89359239e-03, bound:  3.15377921e-01\n",
      "Epoch: 28284 mean train loss:  3.89339635e-03, bound:  3.15377921e-01\n",
      "Epoch: 28285 mean train loss:  3.89322033e-03, bound:  3.15377861e-01\n",
      "Epoch: 28286 mean train loss:  3.89304780e-03, bound:  3.15377861e-01\n",
      "Epoch: 28287 mean train loss:  3.89286410e-03, bound:  3.15377831e-01\n",
      "Epoch: 28288 mean train loss:  3.89269018e-03, bound:  3.15377831e-01\n",
      "Epoch: 28289 mean train loss:  3.89254955e-03, bound:  3.15377831e-01\n",
      "Epoch: 28290 mean train loss:  3.89235280e-03, bound:  3.15377802e-01\n",
      "Epoch: 28291 mean train loss:  3.89223732e-03, bound:  3.15377802e-01\n",
      "Epoch: 28292 mean train loss:  3.89203010e-03, bound:  3.15377742e-01\n",
      "Epoch: 28293 mean train loss:  3.89183848e-03, bound:  3.15377742e-01\n",
      "Epoch: 28294 mean train loss:  3.89166875e-03, bound:  3.15377712e-01\n",
      "Epoch: 28295 mean train loss:  3.89153673e-03, bound:  3.15377712e-01\n",
      "Epoch: 28296 mean train loss:  3.89142265e-03, bound:  3.15377682e-01\n",
      "Epoch: 28297 mean train loss:  3.89121263e-03, bound:  3.15377682e-01\n",
      "Epoch: 28298 mean train loss:  3.89111950e-03, bound:  3.15377682e-01\n",
      "Epoch: 28299 mean train loss:  3.89095815e-03, bound:  3.15377623e-01\n",
      "Epoch: 28300 mean train loss:  3.89078748e-03, bound:  3.15377623e-01\n",
      "Epoch: 28301 mean train loss:  3.89064685e-03, bound:  3.15377623e-01\n",
      "Epoch: 28302 mean train loss:  3.89051251e-03, bound:  3.15377623e-01\n",
      "Epoch: 28303 mean train loss:  3.89048224e-03, bound:  3.15377563e-01\n",
      "Epoch: 28304 mean train loss:  3.89038771e-03, bound:  3.15377563e-01\n",
      "Epoch: 28305 mean train loss:  3.89037444e-03, bound:  3.15377504e-01\n",
      "Epoch: 28306 mean train loss:  3.89037770e-03, bound:  3.15377504e-01\n",
      "Epoch: 28307 mean train loss:  3.89046757e-03, bound:  3.15377504e-01\n",
      "Epoch: 28308 mean train loss:  3.89060704e-03, bound:  3.15377504e-01\n",
      "Epoch: 28309 mean train loss:  3.89086641e-03, bound:  3.15377474e-01\n",
      "Epoch: 28310 mean train loss:  3.89116257e-03, bound:  3.15377444e-01\n",
      "Epoch: 28311 mean train loss:  3.89167247e-03, bound:  3.15377414e-01\n",
      "Epoch: 28312 mean train loss:  3.89225525e-03, bound:  3.15377444e-01\n",
      "Epoch: 28313 mean train loss:  3.89292766e-03, bound:  3.15377384e-01\n",
      "Epoch: 28314 mean train loss:  3.89374653e-03, bound:  3.15377384e-01\n",
      "Epoch: 28315 mean train loss:  3.89475795e-03, bound:  3.15377325e-01\n",
      "Epoch: 28316 mean train loss:  3.89577751e-03, bound:  3.15377384e-01\n",
      "Epoch: 28317 mean train loss:  3.89677333e-03, bound:  3.15377295e-01\n",
      "Epoch: 28318 mean train loss:  3.89742991e-03, bound:  3.15377325e-01\n",
      "Epoch: 28319 mean train loss:  3.89754726e-03, bound:  3.15377265e-01\n",
      "Epoch: 28320 mean train loss:  3.89709976e-03, bound:  3.15377295e-01\n",
      "Epoch: 28321 mean train loss:  3.89591558e-03, bound:  3.15377235e-01\n",
      "Epoch: 28322 mean train loss:  3.89409228e-03, bound:  3.15377265e-01\n",
      "Epoch: 28323 mean train loss:  3.89174907e-03, bound:  3.15377235e-01\n",
      "Epoch: 28324 mean train loss:  3.88942822e-03, bound:  3.15377235e-01\n",
      "Epoch: 28325 mean train loss:  3.88757885e-03, bound:  3.15377176e-01\n",
      "Epoch: 28326 mean train loss:  3.88634694e-03, bound:  3.15377235e-01\n",
      "Epoch: 28327 mean train loss:  3.88593250e-03, bound:  3.15377176e-01\n",
      "Epoch: 28328 mean train loss:  3.88607685e-03, bound:  3.15377146e-01\n",
      "Epoch: 28329 mean train loss:  3.88655951e-03, bound:  3.15377146e-01\n",
      "Epoch: 28330 mean train loss:  3.88709968e-03, bound:  3.15377116e-01\n",
      "Epoch: 28331 mean train loss:  3.88751877e-03, bound:  3.15377116e-01\n",
      "Epoch: 28332 mean train loss:  3.88762774e-03, bound:  3.15377057e-01\n",
      "Epoch: 28333 mean train loss:  3.88740632e-03, bound:  3.15377116e-01\n",
      "Epoch: 28334 mean train loss:  3.88690806e-03, bound:  3.15377027e-01\n",
      "Epoch: 28335 mean train loss:  3.88614042e-03, bound:  3.15377027e-01\n",
      "Epoch: 28336 mean train loss:  3.88541841e-03, bound:  3.15376997e-01\n",
      "Epoch: 28337 mean train loss:  3.88470781e-03, bound:  3.15377027e-01\n",
      "Epoch: 28338 mean train loss:  3.88420862e-03, bound:  3.15376997e-01\n",
      "Epoch: 28339 mean train loss:  3.88388359e-03, bound:  3.15376937e-01\n",
      "Epoch: 28340 mean train loss:  3.88369965e-03, bound:  3.15376937e-01\n",
      "Epoch: 28341 mean train loss:  3.88366124e-03, bound:  3.15376937e-01\n",
      "Epoch: 28342 mean train loss:  3.88368638e-03, bound:  3.15376937e-01\n",
      "Epoch: 28343 mean train loss:  3.88373318e-03, bound:  3.15376878e-01\n",
      "Epoch: 28344 mean train loss:  3.88367497e-03, bound:  3.15376878e-01\n",
      "Epoch: 28345 mean train loss:  3.88357230e-03, bound:  3.15376818e-01\n",
      "Epoch: 28346 mean train loss:  3.88339511e-03, bound:  3.15376818e-01\n",
      "Epoch: 28347 mean train loss:  3.88313574e-03, bound:  3.15376818e-01\n",
      "Epoch: 28348 mean train loss:  3.88284773e-03, bound:  3.15376818e-01\n",
      "Epoch: 28349 mean train loss:  3.88254575e-03, bound:  3.15376818e-01\n",
      "Epoch: 28350 mean train loss:  3.88218858e-03, bound:  3.15376759e-01\n",
      "Epoch: 28351 mean train loss:  3.88195738e-03, bound:  3.15376759e-01\n",
      "Epoch: 28352 mean train loss:  3.88173154e-03, bound:  3.15376699e-01\n",
      "Epoch: 28353 mean train loss:  3.88149358e-03, bound:  3.15376699e-01\n",
      "Epoch: 28354 mean train loss:  3.88126425e-03, bound:  3.15376699e-01\n",
      "Epoch: 28355 mean train loss:  3.88107542e-03, bound:  3.15376699e-01\n",
      "Epoch: 28356 mean train loss:  3.88100208e-03, bound:  3.15376669e-01\n",
      "Epoch: 28357 mean train loss:  3.88084748e-03, bound:  3.15376669e-01\n",
      "Epoch: 28358 mean train loss:  3.88070033e-03, bound:  3.15376610e-01\n",
      "Epoch: 28359 mean train loss:  3.88057739e-03, bound:  3.15376610e-01\n",
      "Epoch: 28360 mean train loss:  3.88037297e-03, bound:  3.15376580e-01\n",
      "Epoch: 28361 mean train loss:  3.88022908e-03, bound:  3.15376580e-01\n",
      "Epoch: 28362 mean train loss:  3.88006354e-03, bound:  3.15376550e-01\n",
      "Epoch: 28363 mean train loss:  3.87989939e-03, bound:  3.15376550e-01\n",
      "Epoch: 28364 mean train loss:  3.87978065e-03, bound:  3.15376520e-01\n",
      "Epoch: 28365 mean train loss:  3.87960556e-03, bound:  3.15376490e-01\n",
      "Epoch: 28366 mean train loss:  3.87941976e-03, bound:  3.15376461e-01\n",
      "Epoch: 28367 mean train loss:  3.87921394e-03, bound:  3.15376461e-01\n",
      "Epoch: 28368 mean train loss:  3.87909845e-03, bound:  3.15376461e-01\n",
      "Epoch: 28369 mean train loss:  3.87892779e-03, bound:  3.15376431e-01\n",
      "Epoch: 28370 mean train loss:  3.87880136e-03, bound:  3.15376401e-01\n",
      "Epoch: 28371 mean train loss:  3.87858669e-03, bound:  3.15376401e-01\n",
      "Epoch: 28372 mean train loss:  3.87844746e-03, bound:  3.15376371e-01\n",
      "Epoch: 28373 mean train loss:  3.87825025e-03, bound:  3.15376371e-01\n",
      "Epoch: 28374 mean train loss:  3.87806236e-03, bound:  3.15376341e-01\n",
      "Epoch: 28375 mean train loss:  3.87785770e-03, bound:  3.15376341e-01\n",
      "Epoch: 28376 mean train loss:  3.87763418e-03, bound:  3.15376312e-01\n",
      "Epoch: 28377 mean train loss:  3.87746305e-03, bound:  3.15376312e-01\n",
      "Epoch: 28378 mean train loss:  3.87727912e-03, bound:  3.15376252e-01\n",
      "Epoch: 28379 mean train loss:  3.87708936e-03, bound:  3.15376252e-01\n",
      "Epoch: 28380 mean train loss:  3.87686794e-03, bound:  3.15376252e-01\n",
      "Epoch: 28381 mean train loss:  3.87671520e-03, bound:  3.15376222e-01\n",
      "Epoch: 28382 mean train loss:  3.87653918e-03, bound:  3.15376222e-01\n",
      "Epoch: 28383 mean train loss:  3.87636502e-03, bound:  3.15376192e-01\n",
      "Epoch: 28384 mean train loss:  3.87614220e-03, bound:  3.15376192e-01\n",
      "Epoch: 28385 mean train loss:  3.87609215e-03, bound:  3.15376133e-01\n",
      "Epoch: 28386 mean train loss:  3.87589959e-03, bound:  3.15376133e-01\n",
      "Epoch: 28387 mean train loss:  3.87569470e-03, bound:  3.15376133e-01\n",
      "Epoch: 28388 mean train loss:  3.87552963e-03, bound:  3.15376133e-01\n",
      "Epoch: 28389 mean train loss:  3.87542159e-03, bound:  3.15376133e-01\n",
      "Epoch: 28390 mean train loss:  3.87527491e-03, bound:  3.15376043e-01\n",
      "Epoch: 28391 mean train loss:  3.87513544e-03, bound:  3.15376043e-01\n",
      "Epoch: 28392 mean train loss:  3.87503323e-03, bound:  3.15376014e-01\n",
      "Epoch: 28393 mean train loss:  3.87502182e-03, bound:  3.15376014e-01\n",
      "Epoch: 28394 mean train loss:  3.87508213e-03, bound:  3.15376014e-01\n",
      "Epoch: 28395 mean train loss:  3.87518038e-03, bound:  3.15376014e-01\n",
      "Epoch: 28396 mean train loss:  3.87549214e-03, bound:  3.15375954e-01\n",
      "Epoch: 28397 mean train loss:  3.87591473e-03, bound:  3.15375954e-01\n",
      "Epoch: 28398 mean train loss:  3.87660484e-03, bound:  3.15375894e-01\n",
      "Epoch: 28399 mean train loss:  3.87751660e-03, bound:  3.15375894e-01\n",
      "Epoch: 28400 mean train loss:  3.87876970e-03, bound:  3.15375894e-01\n",
      "Epoch: 28401 mean train loss:  3.88050964e-03, bound:  3.15375894e-01\n",
      "Epoch: 28402 mean train loss:  3.88286472e-03, bound:  3.15375805e-01\n",
      "Epoch: 28403 mean train loss:  3.88559001e-03, bound:  3.15375894e-01\n",
      "Epoch: 28404 mean train loss:  3.88845545e-03, bound:  3.15375775e-01\n",
      "Epoch: 28405 mean train loss:  3.89096187e-03, bound:  3.15375835e-01\n",
      "Epoch: 28406 mean train loss:  3.89236188e-03, bound:  3.15375775e-01\n",
      "Epoch: 28407 mean train loss:  3.89186270e-03, bound:  3.15375835e-01\n",
      "Epoch: 28408 mean train loss:  3.88881797e-03, bound:  3.15375745e-01\n",
      "Epoch: 28409 mean train loss:  3.88375833e-03, bound:  3.15375805e-01\n",
      "Epoch: 28410 mean train loss:  3.87793034e-03, bound:  3.15375715e-01\n",
      "Epoch: 28411 mean train loss:  3.87337175e-03, bound:  3.15375715e-01\n",
      "Epoch: 28412 mean train loss:  3.87134706e-03, bound:  3.15375715e-01\n",
      "Epoch: 28413 mean train loss:  3.87192890e-03, bound:  3.15375686e-01\n",
      "Epoch: 28414 mean train loss:  3.87413730e-03, bound:  3.15375686e-01\n",
      "Epoch: 28415 mean train loss:  3.87642835e-03, bound:  3.15375656e-01\n",
      "Epoch: 28416 mean train loss:  3.87751218e-03, bound:  3.15375686e-01\n",
      "Epoch: 28417 mean train loss:  3.87689588e-03, bound:  3.15375596e-01\n",
      "Epoch: 28418 mean train loss:  3.87482368e-03, bound:  3.15375656e-01\n",
      "Epoch: 28419 mean train loss:  3.87230306e-03, bound:  3.15375596e-01\n",
      "Epoch: 28420 mean train loss:  3.87043646e-03, bound:  3.15375566e-01\n",
      "Epoch: 28421 mean train loss:  3.86977172e-03, bound:  3.15375566e-01\n",
      "Epoch: 28422 mean train loss:  3.87025904e-03, bound:  3.15375537e-01\n",
      "Epoch: 28423 mean train loss:  3.87132773e-03, bound:  3.15375537e-01\n",
      "Epoch: 28424 mean train loss:  3.87204671e-03, bound:  3.15375477e-01\n",
      "Epoch: 28425 mean train loss:  3.87217267e-03, bound:  3.15375477e-01\n",
      "Epoch: 28426 mean train loss:  3.87148885e-03, bound:  3.15375447e-01\n",
      "Epoch: 28427 mean train loss:  3.87028023e-03, bound:  3.15375447e-01\n",
      "Epoch: 28428 mean train loss:  3.86916706e-03, bound:  3.15375447e-01\n",
      "Epoch: 28429 mean train loss:  3.86849139e-03, bound:  3.15375417e-01\n",
      "Epoch: 28430 mean train loss:  3.86838731e-03, bound:  3.15375417e-01\n",
      "Epoch: 28431 mean train loss:  3.86853586e-03, bound:  3.15375358e-01\n",
      "Epoch: 28432 mean train loss:  3.86877917e-03, bound:  3.15375358e-01\n",
      "Epoch: 28433 mean train loss:  3.86890839e-03, bound:  3.15375328e-01\n",
      "Epoch: 28434 mean train loss:  3.86874797e-03, bound:  3.15375328e-01\n",
      "Epoch: 28435 mean train loss:  3.86825507e-03, bound:  3.15375298e-01\n",
      "Epoch: 28436 mean train loss:  3.86778149e-03, bound:  3.15375298e-01\n",
      "Epoch: 28437 mean train loss:  3.86728044e-03, bound:  3.15375268e-01\n",
      "Epoch: 28438 mean train loss:  3.86692514e-03, bound:  3.15375268e-01\n",
      "Epoch: 28439 mean train loss:  3.86679778e-03, bound:  3.15375239e-01\n",
      "Epoch: 28440 mean train loss:  3.86675633e-03, bound:  3.15375209e-01\n",
      "Epoch: 28441 mean train loss:  3.86673631e-03, bound:  3.15375209e-01\n",
      "Epoch: 28442 mean train loss:  3.86665016e-03, bound:  3.15375209e-01\n",
      "Epoch: 28443 mean train loss:  3.86643829e-03, bound:  3.15375209e-01\n",
      "Epoch: 28444 mean train loss:  3.86625575e-03, bound:  3.15375149e-01\n",
      "Epoch: 28445 mean train loss:  3.86595773e-03, bound:  3.15375149e-01\n",
      "Epoch: 28446 mean train loss:  3.86571116e-03, bound:  3.15375090e-01\n",
      "Epoch: 28447 mean train loss:  3.86544713e-03, bound:  3.15375090e-01\n",
      "Epoch: 28448 mean train loss:  3.86526925e-03, bound:  3.15375090e-01\n",
      "Epoch: 28449 mean train loss:  3.86517006e-03, bound:  3.15375090e-01\n",
      "Epoch: 28450 mean train loss:  3.86496843e-03, bound:  3.15375030e-01\n",
      "Epoch: 28451 mean train loss:  3.86484270e-03, bound:  3.15375030e-01\n",
      "Epoch: 28452 mean train loss:  3.86473001e-03, bound:  3.15375030e-01\n",
      "Epoch: 28453 mean train loss:  3.86461080e-03, bound:  3.15375000e-01\n",
      "Epoch: 28454 mean train loss:  3.86436679e-03, bound:  3.15375000e-01\n",
      "Epoch: 28455 mean train loss:  3.86419729e-03, bound:  3.15374970e-01\n",
      "Epoch: 28456 mean train loss:  3.86398355e-03, bound:  3.15374970e-01\n",
      "Epoch: 28457 mean train loss:  3.86382965e-03, bound:  3.15374911e-01\n",
      "Epoch: 28458 mean train loss:  3.86360218e-03, bound:  3.15374911e-01\n",
      "Epoch: 28459 mean train loss:  3.86342010e-03, bound:  3.15374881e-01\n",
      "Epoch: 28460 mean train loss:  3.86329275e-03, bound:  3.15374881e-01\n",
      "Epoch: 28461 mean train loss:  3.86313279e-03, bound:  3.15374881e-01\n",
      "Epoch: 28462 mean train loss:  3.86300753e-03, bound:  3.15374851e-01\n",
      "Epoch: 28463 mean train loss:  3.86290345e-03, bound:  3.15374851e-01\n",
      "Epoch: 28464 mean train loss:  3.86274629e-03, bound:  3.15374792e-01\n",
      "Epoch: 28465 mean train loss:  3.86269856e-03, bound:  3.15374792e-01\n",
      "Epoch: 28466 mean train loss:  3.86255374e-03, bound:  3.15374762e-01\n",
      "Epoch: 28467 mean train loss:  3.86230787e-03, bound:  3.15374762e-01\n",
      "Epoch: 28468 mean train loss:  3.86213139e-03, bound:  3.15374762e-01\n",
      "Epoch: 28469 mean train loss:  3.86184244e-03, bound:  3.15374702e-01\n",
      "Epoch: 28470 mean train loss:  3.86165641e-03, bound:  3.15374702e-01\n",
      "Epoch: 28471 mean train loss:  3.86144361e-03, bound:  3.15374702e-01\n",
      "Epoch: 28472 mean train loss:  3.86125012e-03, bound:  3.15374702e-01\n",
      "Epoch: 28473 mean train loss:  3.86110251e-03, bound:  3.15374643e-01\n",
      "Epoch: 28474 mean train loss:  3.86089995e-03, bound:  3.15374643e-01\n",
      "Epoch: 28475 mean train loss:  3.86072579e-03, bound:  3.15374583e-01\n",
      "Epoch: 28476 mean train loss:  3.86055023e-03, bound:  3.15374583e-01\n",
      "Epoch: 28477 mean train loss:  3.86040891e-03, bound:  3.15374583e-01\n",
      "Epoch: 28478 mean train loss:  3.86024569e-03, bound:  3.15374583e-01\n",
      "Epoch: 28479 mean train loss:  3.86007014e-03, bound:  3.15374583e-01\n",
      "Epoch: 28480 mean train loss:  3.85991321e-03, bound:  3.15374523e-01\n",
      "Epoch: 28481 mean train loss:  3.85972997e-03, bound:  3.15374523e-01\n",
      "Epoch: 28482 mean train loss:  3.85956350e-03, bound:  3.15374464e-01\n",
      "Epoch: 28483 mean train loss:  3.85936350e-03, bound:  3.15374464e-01\n",
      "Epoch: 28484 mean train loss:  3.85921542e-03, bound:  3.15374464e-01\n",
      "Epoch: 28485 mean train loss:  3.85899725e-03, bound:  3.15374464e-01\n",
      "Epoch: 28486 mean train loss:  3.85882775e-03, bound:  3.15374404e-01\n",
      "Epoch: 28487 mean train loss:  3.85869015e-03, bound:  3.15374404e-01\n",
      "Epoch: 28488 mean train loss:  3.85849108e-03, bound:  3.15374404e-01\n",
      "Epoch: 28489 mean train loss:  3.85833066e-03, bound:  3.15374404e-01\n",
      "Epoch: 28490 mean train loss:  3.85817303e-03, bound:  3.15374404e-01\n",
      "Epoch: 28491 mean train loss:  3.85800051e-03, bound:  3.15374345e-01\n",
      "Epoch: 28492 mean train loss:  3.85785382e-03, bound:  3.15374345e-01\n",
      "Epoch: 28493 mean train loss:  3.85772740e-03, bound:  3.15374315e-01\n",
      "Epoch: 28494 mean train loss:  3.85758234e-03, bound:  3.15374315e-01\n",
      "Epoch: 28495 mean train loss:  3.85745172e-03, bound:  3.15374315e-01\n",
      "Epoch: 28496 mean train loss:  3.85727105e-03, bound:  3.15374255e-01\n",
      "Epoch: 28497 mean train loss:  3.85712343e-03, bound:  3.15374255e-01\n",
      "Epoch: 28498 mean train loss:  3.85698327e-03, bound:  3.15374225e-01\n",
      "Epoch: 28499 mean train loss:  3.85684706e-03, bound:  3.15374225e-01\n",
      "Epoch: 28500 mean train loss:  3.85668292e-03, bound:  3.15374196e-01\n",
      "Epoch: 28501 mean train loss:  3.85650177e-03, bound:  3.15374196e-01\n",
      "Epoch: 28502 mean train loss:  3.85635183e-03, bound:  3.15374136e-01\n",
      "Epoch: 28503 mean train loss:  3.85628827e-03, bound:  3.15374136e-01\n",
      "Epoch: 28504 mean train loss:  3.85617395e-03, bound:  3.15374106e-01\n",
      "Epoch: 28505 mean train loss:  3.85596580e-03, bound:  3.15374106e-01\n",
      "Epoch: 28506 mean train loss:  3.85587197e-03, bound:  3.15374076e-01\n",
      "Epoch: 28507 mean train loss:  3.85570177e-03, bound:  3.15374076e-01\n",
      "Epoch: 28508 mean train loss:  3.85556789e-03, bound:  3.15374076e-01\n",
      "Epoch: 28509 mean train loss:  3.85541120e-03, bound:  3.15374076e-01\n",
      "Epoch: 28510 mean train loss:  3.85538139e-03, bound:  3.15374017e-01\n",
      "Epoch: 28511 mean train loss:  3.85539141e-03, bound:  3.15374017e-01\n",
      "Epoch: 28512 mean train loss:  3.85534880e-03, bound:  3.15373987e-01\n",
      "Epoch: 28513 mean train loss:  3.85535695e-03, bound:  3.15373987e-01\n",
      "Epoch: 28514 mean train loss:  3.85535602e-03, bound:  3.15373957e-01\n",
      "Epoch: 28515 mean train loss:  3.85533040e-03, bound:  3.15373957e-01\n",
      "Epoch: 28516 mean train loss:  3.85539839e-03, bound:  3.15373898e-01\n",
      "Epoch: 28517 mean train loss:  3.85552202e-03, bound:  3.15373898e-01\n",
      "Epoch: 28518 mean train loss:  3.85579909e-03, bound:  3.15373868e-01\n",
      "Epoch: 28519 mean train loss:  3.85606894e-03, bound:  3.15373898e-01\n",
      "Epoch: 28520 mean train loss:  3.85657209e-03, bound:  3.15373838e-01\n",
      "Epoch: 28521 mean train loss:  3.85714136e-03, bound:  3.15373838e-01\n",
      "Epoch: 28522 mean train loss:  3.85795301e-03, bound:  3.15373778e-01\n",
      "Epoch: 28523 mean train loss:  3.85875697e-03, bound:  3.15373808e-01\n",
      "Epoch: 28524 mean train loss:  3.85963009e-03, bound:  3.15373778e-01\n",
      "Epoch: 28525 mean train loss:  3.86039889e-03, bound:  3.15373778e-01\n",
      "Epoch: 28526 mean train loss:  3.86099215e-03, bound:  3.15373749e-01\n",
      "Epoch: 28527 mean train loss:  3.86113278e-03, bound:  3.15373778e-01\n",
      "Epoch: 28528 mean train loss:  3.86071648e-03, bound:  3.15373659e-01\n",
      "Epoch: 28529 mean train loss:  3.85963777e-03, bound:  3.15373749e-01\n",
      "Epoch: 28530 mean train loss:  3.85795021e-03, bound:  3.15373659e-01\n",
      "Epoch: 28531 mean train loss:  3.85602098e-03, bound:  3.15373659e-01\n",
      "Epoch: 28532 mean train loss:  3.85408197e-03, bound:  3.15373659e-01\n",
      "Epoch: 28533 mean train loss:  3.85233294e-03, bound:  3.15373659e-01\n",
      "Epoch: 28534 mean train loss:  3.85117857e-03, bound:  3.15373629e-01\n",
      "Epoch: 28535 mean train loss:  3.85058345e-03, bound:  3.15373629e-01\n",
      "Epoch: 28536 mean train loss:  3.85043630e-03, bound:  3.15373629e-01\n",
      "Epoch: 28537 mean train loss:  3.85068660e-03, bound:  3.15373570e-01\n",
      "Epoch: 28538 mean train loss:  3.85115296e-03, bound:  3.15373570e-01\n",
      "Epoch: 28539 mean train loss:  3.85151501e-03, bound:  3.15373540e-01\n",
      "Epoch: 28540 mean train loss:  3.85176344e-03, bound:  3.15373540e-01\n",
      "Epoch: 28541 mean train loss:  3.85180255e-03, bound:  3.15373510e-01\n",
      "Epoch: 28542 mean train loss:  3.85153759e-03, bound:  3.15373510e-01\n",
      "Epoch: 28543 mean train loss:  3.85108171e-03, bound:  3.15373451e-01\n",
      "Epoch: 28544 mean train loss:  3.85050010e-03, bound:  3.15373451e-01\n",
      "Epoch: 28545 mean train loss:  3.84983700e-03, bound:  3.15373421e-01\n",
      "Epoch: 28546 mean train loss:  3.84928240e-03, bound:  3.15373421e-01\n",
      "Epoch: 28547 mean train loss:  3.84876109e-03, bound:  3.15373421e-01\n",
      "Epoch: 28548 mean train loss:  3.84839019e-03, bound:  3.15373361e-01\n",
      "Epoch: 28549 mean train loss:  3.84818297e-03, bound:  3.15373361e-01\n",
      "Epoch: 28550 mean train loss:  3.84803838e-03, bound:  3.15373331e-01\n",
      "Epoch: 28551 mean train loss:  3.84796411e-03, bound:  3.15373331e-01\n",
      "Epoch: 28552 mean train loss:  3.84803792e-03, bound:  3.15373302e-01\n",
      "Epoch: 28553 mean train loss:  3.84799810e-03, bound:  3.15373302e-01\n",
      "Epoch: 28554 mean train loss:  3.84792662e-03, bound:  3.15373272e-01\n",
      "Epoch: 28555 mean train loss:  3.84783442e-03, bound:  3.15373272e-01\n",
      "Epoch: 28556 mean train loss:  3.84769053e-03, bound:  3.15373212e-01\n",
      "Epoch: 28557 mean train loss:  3.84747726e-03, bound:  3.15373212e-01\n",
      "Epoch: 28558 mean train loss:  3.84722557e-03, bound:  3.15373212e-01\n",
      "Epoch: 28559 mean train loss:  3.84693919e-03, bound:  3.15373182e-01\n",
      "Epoch: 28560 mean train loss:  3.84665933e-03, bound:  3.15373182e-01\n",
      "Epoch: 28561 mean train loss:  3.84642230e-03, bound:  3.15373152e-01\n",
      "Epoch: 28562 mean train loss:  3.84617434e-03, bound:  3.15373152e-01\n",
      "Epoch: 28563 mean train loss:  3.84590449e-03, bound:  3.15373093e-01\n",
      "Epoch: 28564 mean train loss:  3.84572055e-03, bound:  3.15373093e-01\n",
      "Epoch: 28565 mean train loss:  3.84553568e-03, bound:  3.15373093e-01\n",
      "Epoch: 28566 mean train loss:  3.84533079e-03, bound:  3.15373063e-01\n",
      "Epoch: 28567 mean train loss:  3.84515361e-03, bound:  3.15373063e-01\n",
      "Epoch: 28568 mean train loss:  3.84496851e-03, bound:  3.15373063e-01\n",
      "Epoch: 28569 mean train loss:  3.84481577e-03, bound:  3.15373063e-01\n",
      "Epoch: 28570 mean train loss:  3.84465232e-03, bound:  3.15372974e-01\n",
      "Epoch: 28571 mean train loss:  3.84455267e-03, bound:  3.15372974e-01\n",
      "Epoch: 28572 mean train loss:  3.84439598e-03, bound:  3.15372974e-01\n",
      "Epoch: 28573 mean train loss:  3.84425675e-03, bound:  3.15372974e-01\n",
      "Epoch: 28574 mean train loss:  3.84410820e-03, bound:  3.15372944e-01\n",
      "Epoch: 28575 mean train loss:  3.84402159e-03, bound:  3.15372914e-01\n",
      "Epoch: 28576 mean train loss:  3.84390005e-03, bound:  3.15372914e-01\n",
      "Epoch: 28577 mean train loss:  3.84376873e-03, bound:  3.15372854e-01\n",
      "Epoch: 28578 mean train loss:  3.84363858e-03, bound:  3.15372854e-01\n",
      "Epoch: 28579 mean train loss:  3.84346931e-03, bound:  3.15372854e-01\n",
      "Epoch: 28580 mean train loss:  3.84331006e-03, bound:  3.15372854e-01\n",
      "Epoch: 28581 mean train loss:  3.84316267e-03, bound:  3.15372795e-01\n",
      "Epoch: 28582 mean train loss:  3.84301040e-03, bound:  3.15372795e-01\n",
      "Epoch: 28583 mean train loss:  3.84289236e-03, bound:  3.15372765e-01\n",
      "Epoch: 28584 mean train loss:  3.84274172e-03, bound:  3.15372765e-01\n",
      "Epoch: 28585 mean train loss:  3.84263415e-03, bound:  3.15372735e-01\n",
      "Epoch: 28586 mean train loss:  3.84250376e-03, bound:  3.15372735e-01\n",
      "Epoch: 28587 mean train loss:  3.84246255e-03, bound:  3.15372705e-01\n",
      "Epoch: 28588 mean train loss:  3.84234893e-03, bound:  3.15372705e-01\n",
      "Epoch: 28589 mean train loss:  3.84229841e-03, bound:  3.15372676e-01\n",
      "Epoch: 28590 mean train loss:  3.84223019e-03, bound:  3.15372646e-01\n",
      "Epoch: 28591 mean train loss:  3.84212565e-03, bound:  3.15372646e-01\n",
      "Epoch: 28592 mean train loss:  3.84212658e-03, bound:  3.15372646e-01\n",
      "Epoch: 28593 mean train loss:  3.84207699e-03, bound:  3.15372586e-01\n",
      "Epoch: 28594 mean train loss:  3.84218711e-03, bound:  3.15372616e-01\n",
      "Epoch: 28595 mean train loss:  3.84234148e-03, bound:  3.15372556e-01\n",
      "Epoch: 28596 mean train loss:  3.84260970e-03, bound:  3.15372556e-01\n",
      "Epoch: 28597 mean train loss:  3.84295988e-03, bound:  3.15372527e-01\n",
      "Epoch: 28598 mean train loss:  3.84345069e-03, bound:  3.15372527e-01\n",
      "Epoch: 28599 mean train loss:  3.84406280e-03, bound:  3.15372497e-01\n",
      "Epoch: 28600 mean train loss:  3.84480716e-03, bound:  3.15372497e-01\n",
      "Epoch: 28601 mean train loss:  3.84566584e-03, bound:  3.15372437e-01\n",
      "Epoch: 28602 mean train loss:  3.84666305e-03, bound:  3.15372437e-01\n",
      "Epoch: 28603 mean train loss:  3.84776574e-03, bound:  3.15372407e-01\n",
      "Epoch: 28604 mean train loss:  3.84868076e-03, bound:  3.15372437e-01\n",
      "Epoch: 28605 mean train loss:  3.84912128e-03, bound:  3.15372378e-01\n",
      "Epoch: 28606 mean train loss:  3.84893198e-03, bound:  3.15372407e-01\n",
      "Epoch: 28607 mean train loss:  3.84798297e-03, bound:  3.15372348e-01\n",
      "Epoch: 28608 mean train loss:  3.84634268e-03, bound:  3.15372378e-01\n",
      "Epoch: 28609 mean train loss:  3.84410354e-03, bound:  3.15372318e-01\n",
      "Epoch: 28610 mean train loss:  3.84170003e-03, bound:  3.15372348e-01\n",
      "Epoch: 28611 mean train loss:  3.83961154e-03, bound:  3.15372288e-01\n",
      "Epoch: 28612 mean train loss:  3.83814075e-03, bound:  3.15372288e-01\n",
      "Epoch: 28613 mean train loss:  3.83739267e-03, bound:  3.15372288e-01\n",
      "Epoch: 28614 mean train loss:  3.83741246e-03, bound:  3.15372258e-01\n",
      "Epoch: 28615 mean train loss:  3.83784529e-03, bound:  3.15372258e-01\n",
      "Epoch: 28616 mean train loss:  3.83842736e-03, bound:  3.15372229e-01\n",
      "Epoch: 28617 mean train loss:  3.83894728e-03, bound:  3.15372229e-01\n",
      "Epoch: 28618 mean train loss:  3.83921340e-03, bound:  3.15372169e-01\n",
      "Epoch: 28619 mean train loss:  3.83901899e-03, bound:  3.15372169e-01\n",
      "Epoch: 28620 mean train loss:  3.83849815e-03, bound:  3.15372169e-01\n",
      "Epoch: 28621 mean train loss:  3.83775705e-03, bound:  3.15372169e-01\n",
      "Epoch: 28622 mean train loss:  3.83696938e-03, bound:  3.15372109e-01\n",
      "Epoch: 28623 mean train loss:  3.83624178e-03, bound:  3.15372109e-01\n",
      "Epoch: 28624 mean train loss:  3.83570744e-03, bound:  3.15372109e-01\n",
      "Epoch: 28625 mean train loss:  3.83540150e-03, bound:  3.15372080e-01\n",
      "Epoch: 28626 mean train loss:  3.83527903e-03, bound:  3.15372080e-01\n",
      "Epoch: 28627 mean train loss:  3.83533421e-03, bound:  3.15372050e-01\n",
      "Epoch: 28628 mean train loss:  3.83542362e-03, bound:  3.15372050e-01\n",
      "Epoch: 28629 mean train loss:  3.83544713e-03, bound:  3.15371990e-01\n",
      "Epoch: 28630 mean train loss:  3.83544154e-03, bound:  3.15371990e-01\n",
      "Epoch: 28631 mean train loss:  3.83533072e-03, bound:  3.15371960e-01\n",
      "Epoch: 28632 mean train loss:  3.83508857e-03, bound:  3.15371960e-01\n",
      "Epoch: 28633 mean train loss:  3.83480499e-03, bound:  3.15371931e-01\n",
      "Epoch: 28634 mean train loss:  3.83450580e-03, bound:  3.15371931e-01\n",
      "Epoch: 28635 mean train loss:  3.83408065e-03, bound:  3.15371871e-01\n",
      "Epoch: 28636 mean train loss:  3.83376586e-03, bound:  3.15371871e-01\n",
      "Epoch: 28637 mean train loss:  3.83347739e-03, bound:  3.15371871e-01\n",
      "Epoch: 28638 mean train loss:  3.83324758e-03, bound:  3.15371841e-01\n",
      "Epoch: 28639 mean train loss:  3.83312022e-03, bound:  3.15371841e-01\n",
      "Epoch: 28640 mean train loss:  3.83303454e-03, bound:  3.15371811e-01\n",
      "Epoch: 28641 mean train loss:  3.83292418e-03, bound:  3.15371811e-01\n",
      "Epoch: 28642 mean train loss:  3.83286201e-03, bound:  3.15371752e-01\n",
      "Epoch: 28643 mean train loss:  3.83277796e-03, bound:  3.15371752e-01\n",
      "Epoch: 28644 mean train loss:  3.83261917e-03, bound:  3.15371722e-01\n",
      "Epoch: 28645 mean train loss:  3.83247295e-03, bound:  3.15371722e-01\n",
      "Epoch: 28646 mean train loss:  3.83230229e-03, bound:  3.15371692e-01\n",
      "Epoch: 28647 mean train loss:  3.83209391e-03, bound:  3.15371692e-01\n",
      "Epoch: 28648 mean train loss:  3.83198890e-03, bound:  3.15371662e-01\n",
      "Epoch: 28649 mean train loss:  3.83175770e-03, bound:  3.15371633e-01\n",
      "Epoch: 28650 mean train loss:  3.83159146e-03, bound:  3.15371633e-01\n",
      "Epoch: 28651 mean train loss:  3.83136352e-03, bound:  3.15371603e-01\n",
      "Epoch: 28652 mean train loss:  3.83119518e-03, bound:  3.15371603e-01\n",
      "Epoch: 28653 mean train loss:  3.83094768e-03, bound:  3.15371573e-01\n",
      "Epoch: 28654 mean train loss:  3.83077702e-03, bound:  3.15371543e-01\n",
      "Epoch: 28655 mean train loss:  3.83051508e-03, bound:  3.15371543e-01\n",
      "Epoch: 28656 mean train loss:  3.83039005e-03, bound:  3.15371543e-01\n",
      "Epoch: 28657 mean train loss:  3.83014930e-03, bound:  3.15371513e-01\n",
      "Epoch: 28658 mean train loss:  3.82994907e-03, bound:  3.15371484e-01\n",
      "Epoch: 28659 mean train loss:  3.82980355e-03, bound:  3.15371484e-01\n",
      "Epoch: 28660 mean train loss:  3.82961030e-03, bound:  3.15371424e-01\n",
      "Epoch: 28661 mean train loss:  3.82941402e-03, bound:  3.15371424e-01\n",
      "Epoch: 28662 mean train loss:  3.82927153e-03, bound:  3.15371424e-01\n",
      "Epoch: 28663 mean train loss:  3.82914953e-03, bound:  3.15371424e-01\n",
      "Epoch: 28664 mean train loss:  3.82890739e-03, bound:  3.15371394e-01\n",
      "Epoch: 28665 mean train loss:  3.82875674e-03, bound:  3.15371394e-01\n",
      "Epoch: 28666 mean train loss:  3.82859702e-03, bound:  3.15371364e-01\n",
      "Epoch: 28667 mean train loss:  3.82838841e-03, bound:  3.15371305e-01\n",
      "Epoch: 28668 mean train loss:  3.82823171e-03, bound:  3.15371305e-01\n",
      "Epoch: 28669 mean train loss:  3.82799702e-03, bound:  3.15371305e-01\n",
      "Epoch: 28670 mean train loss:  3.82792950e-03, bound:  3.15371305e-01\n",
      "Epoch: 28671 mean train loss:  3.82775697e-03, bound:  3.15371275e-01\n",
      "Epoch: 28672 mean train loss:  3.82763334e-03, bound:  3.15371245e-01\n",
      "Epoch: 28673 mean train loss:  3.82744242e-03, bound:  3.15371245e-01\n",
      "Epoch: 28674 mean train loss:  3.82728805e-03, bound:  3.15371245e-01\n",
      "Epoch: 28675 mean train loss:  3.82722053e-03, bound:  3.15371186e-01\n",
      "Epoch: 28676 mean train loss:  3.82702542e-03, bound:  3.15371186e-01\n",
      "Epoch: 28677 mean train loss:  3.82695650e-03, bound:  3.15371156e-01\n",
      "Epoch: 28678 mean train loss:  3.82682029e-03, bound:  3.15371156e-01\n",
      "Epoch: 28679 mean train loss:  3.82674066e-03, bound:  3.15371156e-01\n",
      "Epoch: 28680 mean train loss:  3.82665219e-03, bound:  3.15371126e-01\n",
      "Epoch: 28681 mean train loss:  3.82661540e-03, bound:  3.15371126e-01\n",
      "Epoch: 28682 mean train loss:  3.82651249e-03, bound:  3.15371066e-01\n",
      "Epoch: 28683 mean train loss:  3.82654020e-03, bound:  3.15371066e-01\n",
      "Epoch: 28684 mean train loss:  3.82645428e-03, bound:  3.15371037e-01\n",
      "Epoch: 28685 mean train loss:  3.82657908e-03, bound:  3.15371037e-01\n",
      "Epoch: 28686 mean train loss:  3.82683170e-03, bound:  3.15370977e-01\n",
      "Epoch: 28687 mean train loss:  3.82712530e-03, bound:  3.15371037e-01\n",
      "Epoch: 28688 mean train loss:  3.82775231e-03, bound:  3.15370977e-01\n",
      "Epoch: 28689 mean train loss:  3.82847898e-03, bound:  3.15370977e-01\n",
      "Epoch: 28690 mean train loss:  3.82955861e-03, bound:  3.15370917e-01\n",
      "Epoch: 28691 mean train loss:  3.83106410e-03, bound:  3.15370947e-01\n",
      "Epoch: 28692 mean train loss:  3.83312325e-03, bound:  3.15370917e-01\n",
      "Epoch: 28693 mean train loss:  3.83572071e-03, bound:  3.15370917e-01\n",
      "Epoch: 28694 mean train loss:  3.83879058e-03, bound:  3.15370858e-01\n",
      "Epoch: 28695 mean train loss:  3.84169049e-03, bound:  3.15370917e-01\n",
      "Epoch: 28696 mean train loss:  3.84371192e-03, bound:  3.15370828e-01\n",
      "Epoch: 28697 mean train loss:  3.84389167e-03, bound:  3.15370858e-01\n",
      "Epoch: 28698 mean train loss:  3.84156452e-03, bound:  3.15370798e-01\n",
      "Epoch: 28699 mean train loss:  3.83670488e-03, bound:  3.15370828e-01\n",
      "Epoch: 28700 mean train loss:  3.83072323e-03, bound:  3.15370798e-01\n",
      "Epoch: 28701 mean train loss:  3.82557861e-03, bound:  3.15370798e-01\n",
      "Epoch: 28702 mean train loss:  3.82278417e-03, bound:  3.15370739e-01\n",
      "Epoch: 28703 mean train loss:  3.82277276e-03, bound:  3.15370739e-01\n",
      "Epoch: 28704 mean train loss:  3.82477487e-03, bound:  3.15370739e-01\n",
      "Epoch: 28705 mean train loss:  3.82720726e-03, bound:  3.15370709e-01\n",
      "Epoch: 28706 mean train loss:  3.82876117e-03, bound:  3.15370739e-01\n",
      "Epoch: 28707 mean train loss:  3.82857327e-03, bound:  3.15370679e-01\n",
      "Epoch: 28708 mean train loss:  3.82681284e-03, bound:  3.15370709e-01\n",
      "Epoch: 28709 mean train loss:  3.82428872e-03, bound:  3.15370619e-01\n",
      "Epoch: 28710 mean train loss:  3.82211199e-03, bound:  3.15370619e-01\n",
      "Epoch: 28711 mean train loss:  3.82110244e-03, bound:  3.15370619e-01\n",
      "Epoch: 28712 mean train loss:  3.82129406e-03, bound:  3.15370619e-01\n",
      "Epoch: 28713 mean train loss:  3.82223655e-03, bound:  3.15370619e-01\n",
      "Epoch: 28714 mean train loss:  3.82308546e-03, bound:  3.15370530e-01\n",
      "Epoch: 28715 mean train loss:  3.82334366e-03, bound:  3.15370589e-01\n",
      "Epoch: 28716 mean train loss:  3.82269477e-03, bound:  3.15370500e-01\n",
      "Epoch: 28717 mean train loss:  3.82160651e-03, bound:  3.15370500e-01\n",
      "Epoch: 28718 mean train loss:  3.82058485e-03, bound:  3.15370500e-01\n",
      "Epoch: 28719 mean train loss:  3.81987472e-03, bound:  3.15370500e-01\n",
      "Epoch: 28720 mean train loss:  3.81966191e-03, bound:  3.15370470e-01\n",
      "Epoch: 28721 mean train loss:  3.81980743e-03, bound:  3.15370470e-01\n",
      "Epoch: 28722 mean train loss:  3.82011849e-03, bound:  3.15370470e-01\n",
      "Epoch: 28723 mean train loss:  3.82034504e-03, bound:  3.15370411e-01\n",
      "Epoch: 28724 mean train loss:  3.82019510e-03, bound:  3.15370411e-01\n",
      "Epoch: 28725 mean train loss:  3.81986029e-03, bound:  3.15370381e-01\n",
      "Epoch: 28726 mean train loss:  3.81925120e-03, bound:  3.15370381e-01\n",
      "Epoch: 28727 mean train loss:  3.81874805e-03, bound:  3.15370351e-01\n",
      "Epoch: 28728 mean train loss:  3.81835201e-03, bound:  3.15370351e-01\n",
      "Epoch: 28729 mean train loss:  3.81814432e-03, bound:  3.15370291e-01\n",
      "Epoch: 28730 mean train loss:  3.81814176e-03, bound:  3.15370291e-01\n",
      "Epoch: 28731 mean train loss:  3.81809403e-03, bound:  3.15370262e-01\n",
      "Epoch: 28732 mean train loss:  3.81807983e-03, bound:  3.15370232e-01\n",
      "Epoch: 28733 mean train loss:  3.81788681e-03, bound:  3.15370232e-01\n",
      "Epoch: 28734 mean train loss:  3.81765654e-03, bound:  3.15370232e-01\n",
      "Epoch: 28735 mean train loss:  3.81743093e-03, bound:  3.15370232e-01\n",
      "Epoch: 28736 mean train loss:  3.81707819e-03, bound:  3.15370172e-01\n",
      "Epoch: 28737 mean train loss:  3.81687563e-03, bound:  3.15370172e-01\n",
      "Epoch: 28738 mean train loss:  3.81671824e-03, bound:  3.15370142e-01\n",
      "Epoch: 28739 mean train loss:  3.81658878e-03, bound:  3.15370142e-01\n",
      "Epoch: 28740 mean train loss:  3.81642673e-03, bound:  3.15370142e-01\n",
      "Epoch: 28741 mean train loss:  3.81633290e-03, bound:  3.15370142e-01\n",
      "Epoch: 28742 mean train loss:  3.81615665e-03, bound:  3.15370053e-01\n",
      "Epoch: 28743 mean train loss:  3.81593010e-03, bound:  3.15370053e-01\n",
      "Epoch: 28744 mean train loss:  3.81576596e-03, bound:  3.15370053e-01\n",
      "Epoch: 28745 mean train loss:  3.81562370e-03, bound:  3.15370023e-01\n",
      "Epoch: 28746 mean train loss:  3.81546142e-03, bound:  3.15370023e-01\n",
      "Epoch: 28747 mean train loss:  3.81525909e-03, bound:  3.15370023e-01\n",
      "Epoch: 28748 mean train loss:  3.81507282e-03, bound:  3.15370023e-01\n",
      "Epoch: 28749 mean train loss:  3.81494383e-03, bound:  3.15369934e-01\n",
      "Epoch: 28750 mean train loss:  3.81473801e-03, bound:  3.15369934e-01\n",
      "Epoch: 28751 mean train loss:  3.81467841e-03, bound:  3.15369934e-01\n",
      "Epoch: 28752 mean train loss:  3.81452241e-03, bound:  3.15369934e-01\n",
      "Epoch: 28753 mean train loss:  3.81435943e-03, bound:  3.15369904e-01\n",
      "Epoch: 28754 mean train loss:  3.81415547e-03, bound:  3.15369904e-01\n",
      "Epoch: 28755 mean train loss:  3.81402462e-03, bound:  3.15369904e-01\n",
      "Epoch: 28756 mean train loss:  3.81382066e-03, bound:  3.15369844e-01\n",
      "Epoch: 28757 mean train loss:  3.81359598e-03, bound:  3.15369815e-01\n",
      "Epoch: 28758 mean train loss:  3.81344231e-03, bound:  3.15369815e-01\n",
      "Epoch: 28759 mean train loss:  3.81323812e-03, bound:  3.15369815e-01\n",
      "Epoch: 28760 mean train loss:  3.81308352e-03, bound:  3.15369785e-01\n",
      "Epoch: 28761 mean train loss:  3.81290703e-03, bound:  3.15369785e-01\n",
      "Epoch: 28762 mean train loss:  3.81274126e-03, bound:  3.15369725e-01\n",
      "Epoch: 28763 mean train loss:  3.81259597e-03, bound:  3.15369725e-01\n",
      "Epoch: 28764 mean train loss:  3.81248840e-03, bound:  3.15369725e-01\n",
      "Epoch: 28765 mean train loss:  3.81229771e-03, bound:  3.15369695e-01\n",
      "Epoch: 28766 mean train loss:  3.81213333e-03, bound:  3.15369695e-01\n",
      "Epoch: 28767 mean train loss:  3.81195731e-03, bound:  3.15369666e-01\n",
      "Epoch: 28768 mean train loss:  3.81175010e-03, bound:  3.15369636e-01\n",
      "Epoch: 28769 mean train loss:  3.81164229e-03, bound:  3.15369606e-01\n",
      "Epoch: 28770 mean train loss:  3.81149258e-03, bound:  3.15369606e-01\n",
      "Epoch: 28771 mean train loss:  3.81127885e-03, bound:  3.15369606e-01\n",
      "Epoch: 28772 mean train loss:  3.81117593e-03, bound:  3.15369576e-01\n",
      "Epoch: 28773 mean train loss:  3.81098338e-03, bound:  3.15369576e-01\n",
      "Epoch: 28774 mean train loss:  3.81085975e-03, bound:  3.15369517e-01\n",
      "Epoch: 28775 mean train loss:  3.81064392e-03, bound:  3.15369517e-01\n",
      "Epoch: 28776 mean train loss:  3.81050445e-03, bound:  3.15369517e-01\n",
      "Epoch: 28777 mean train loss:  3.81032727e-03, bound:  3.15369487e-01\n",
      "Epoch: 28778 mean train loss:  3.81022692e-03, bound:  3.15369487e-01\n",
      "Epoch: 28779 mean train loss:  3.80998757e-03, bound:  3.15369457e-01\n",
      "Epoch: 28780 mean train loss:  3.80983995e-03, bound:  3.15369457e-01\n",
      "Epoch: 28781 mean train loss:  3.80965183e-03, bound:  3.15369457e-01\n",
      "Epoch: 28782 mean train loss:  3.80951702e-03, bound:  3.15369397e-01\n",
      "Epoch: 28783 mean train loss:  3.80928791e-03, bound:  3.15369397e-01\n",
      "Epoch: 28784 mean train loss:  3.80920735e-03, bound:  3.15369368e-01\n",
      "Epoch: 28785 mean train loss:  3.80901271e-03, bound:  3.15369368e-01\n",
      "Epoch: 28786 mean train loss:  3.80886602e-03, bound:  3.15369338e-01\n",
      "Epoch: 28787 mean train loss:  3.80867161e-03, bound:  3.15369338e-01\n",
      "Epoch: 28788 mean train loss:  3.80852027e-03, bound:  3.15369338e-01\n",
      "Epoch: 28789 mean train loss:  3.80831934e-03, bound:  3.15369338e-01\n",
      "Epoch: 28790 mean train loss:  3.80821037e-03, bound:  3.15369278e-01\n",
      "Epoch: 28791 mean train loss:  3.80798336e-03, bound:  3.15369248e-01\n",
      "Epoch: 28792 mean train loss:  3.80783994e-03, bound:  3.15369248e-01\n",
      "Epoch: 28793 mean train loss:  3.80766112e-03, bound:  3.15369219e-01\n",
      "Epoch: 28794 mean train loss:  3.80753260e-03, bound:  3.15369219e-01\n",
      "Epoch: 28795 mean train loss:  3.80734657e-03, bound:  3.15369189e-01\n",
      "Epoch: 28796 mean train loss:  3.80718592e-03, bound:  3.15369189e-01\n",
      "Epoch: 28797 mean train loss:  3.80700361e-03, bound:  3.15369159e-01\n",
      "Epoch: 28798 mean train loss:  3.80680873e-03, bound:  3.15369159e-01\n",
      "Epoch: 28799 mean train loss:  3.80667392e-03, bound:  3.15369099e-01\n",
      "Epoch: 28800 mean train loss:  3.80652770e-03, bound:  3.15369099e-01\n",
      "Epoch: 28801 mean train loss:  3.80632048e-03, bound:  3.15369070e-01\n",
      "Epoch: 28802 mean train loss:  3.80614703e-03, bound:  3.15369070e-01\n",
      "Epoch: 28803 mean train loss:  3.80604435e-03, bound:  3.15369040e-01\n",
      "Epoch: 28804 mean train loss:  3.80585971e-03, bound:  3.15369040e-01\n",
      "Epoch: 28805 mean train loss:  3.80566716e-03, bound:  3.15369040e-01\n",
      "Epoch: 28806 mean train loss:  3.80552234e-03, bound:  3.15369010e-01\n",
      "Epoch: 28807 mean train loss:  3.80535424e-03, bound:  3.15369010e-01\n",
      "Epoch: 28808 mean train loss:  3.80517147e-03, bound:  3.15368950e-01\n",
      "Epoch: 28809 mean train loss:  3.80503689e-03, bound:  3.15368950e-01\n",
      "Epoch: 28810 mean train loss:  3.80498520e-03, bound:  3.15368921e-01\n",
      "Epoch: 28811 mean train loss:  3.80479754e-03, bound:  3.15368921e-01\n",
      "Epoch: 28812 mean train loss:  3.80468811e-03, bound:  3.15368921e-01\n",
      "Epoch: 28813 mean train loss:  3.80455959e-03, bound:  3.15368891e-01\n",
      "Epoch: 28814 mean train loss:  3.80450557e-03, bound:  3.15368891e-01\n",
      "Epoch: 28815 mean train loss:  3.80439335e-03, bound:  3.15368831e-01\n",
      "Epoch: 28816 mean train loss:  3.80442850e-03, bound:  3.15368831e-01\n",
      "Epoch: 28817 mean train loss:  3.80450883e-03, bound:  3.15368801e-01\n",
      "Epoch: 28818 mean train loss:  3.80477007e-03, bound:  3.15368801e-01\n",
      "Epoch: 28819 mean train loss:  3.80512723e-03, bound:  3.15368772e-01\n",
      "Epoch: 28820 mean train loss:  3.80584318e-03, bound:  3.15368801e-01\n",
      "Epoch: 28821 mean train loss:  3.80694726e-03, bound:  3.15368712e-01\n",
      "Epoch: 28822 mean train loss:  3.80857056e-03, bound:  3.15368742e-01\n",
      "Epoch: 28823 mean train loss:  3.81096941e-03, bound:  3.15368682e-01\n",
      "Epoch: 28824 mean train loss:  3.81440809e-03, bound:  3.15368712e-01\n",
      "Epoch: 28825 mean train loss:  3.81888566e-03, bound:  3.15368652e-01\n",
      "Epoch: 28826 mean train loss:  3.82419606e-03, bound:  3.15368712e-01\n",
      "Epoch: 28827 mean train loss:  3.82927340e-03, bound:  3.15368593e-01\n",
      "Epoch: 28828 mean train loss:  3.83218774e-03, bound:  3.15368682e-01\n",
      "Epoch: 28829 mean train loss:  3.83080333e-03, bound:  3.15368593e-01\n",
      "Epoch: 28830 mean train loss:  3.82420677e-03, bound:  3.15368652e-01\n",
      "Epoch: 28831 mean train loss:  3.81437526e-03, bound:  3.15368563e-01\n",
      "Epoch: 28832 mean train loss:  3.80544947e-03, bound:  3.15368593e-01\n",
      "Epoch: 28833 mean train loss:  3.80118657e-03, bound:  3.15368563e-01\n",
      "Epoch: 28834 mean train loss:  3.80256469e-03, bound:  3.15368563e-01\n",
      "Epoch: 28835 mean train loss:  3.80728510e-03, bound:  3.15368563e-01\n",
      "Epoch: 28836 mean train loss:  3.81160178e-03, bound:  3.15368503e-01\n",
      "Epoch: 28837 mean train loss:  3.81250260e-03, bound:  3.15368563e-01\n",
      "Epoch: 28838 mean train loss:  3.80934752e-03, bound:  3.15368474e-01\n",
      "Epoch: 28839 mean train loss:  3.80434073e-03, bound:  3.15368503e-01\n",
      "Epoch: 28840 mean train loss:  3.80063010e-03, bound:  3.15368474e-01\n",
      "Epoch: 28841 mean train loss:  3.80001427e-03, bound:  3.15368474e-01\n",
      "Epoch: 28842 mean train loss:  3.80210765e-03, bound:  3.15368474e-01\n",
      "Epoch: 28843 mean train loss:  3.80457775e-03, bound:  3.15368384e-01\n",
      "Epoch: 28844 mean train loss:  3.80534795e-03, bound:  3.15368414e-01\n",
      "Epoch: 28845 mean train loss:  3.80378682e-03, bound:  3.15368384e-01\n",
      "Epoch: 28846 mean train loss:  3.80109763e-03, bound:  3.15368384e-01\n",
      "Epoch: 28847 mean train loss:  3.79911950e-03, bound:  3.15368354e-01\n",
      "Epoch: 28848 mean train loss:  3.79884243e-03, bound:  3.15368354e-01\n",
      "Epoch: 28849 mean train loss:  3.79982241e-03, bound:  3.15368325e-01\n",
      "Epoch: 28850 mean train loss:  3.80112790e-03, bound:  3.15368265e-01\n",
      "Epoch: 28851 mean train loss:  3.80140403e-03, bound:  3.15368265e-01\n",
      "Epoch: 28852 mean train loss:  3.80054512e-03, bound:  3.15368265e-01\n",
      "Epoch: 28853 mean train loss:  3.79905198e-03, bound:  3.15368265e-01\n",
      "Epoch: 28854 mean train loss:  3.79788596e-03, bound:  3.15368235e-01\n",
      "Epoch: 28855 mean train loss:  3.79769737e-03, bound:  3.15368235e-01\n",
      "Epoch: 28856 mean train loss:  3.79826874e-03, bound:  3.15368205e-01\n",
      "Epoch: 28857 mean train loss:  3.79892648e-03, bound:  3.15368205e-01\n",
      "Epoch: 28858 mean train loss:  3.79902381e-03, bound:  3.15368176e-01\n",
      "Epoch: 28859 mean train loss:  3.79846571e-03, bound:  3.15368146e-01\n",
      "Epoch: 28860 mean train loss:  3.79752042e-03, bound:  3.15368146e-01\n",
      "Epoch: 28861 mean train loss:  3.79673974e-03, bound:  3.15368116e-01\n",
      "Epoch: 28862 mean train loss:  3.79654788e-03, bound:  3.15368086e-01\n",
      "Epoch: 28863 mean train loss:  3.79671040e-03, bound:  3.15368086e-01\n",
      "Epoch: 28864 mean train loss:  3.79702356e-03, bound:  3.15368086e-01\n",
      "Epoch: 28865 mean train loss:  3.79701750e-03, bound:  3.15368056e-01\n",
      "Epoch: 28866 mean train loss:  3.79672018e-03, bound:  3.15368026e-01\n",
      "Epoch: 28867 mean train loss:  3.79614159e-03, bound:  3.15367997e-01\n",
      "Epoch: 28868 mean train loss:  3.79567733e-03, bound:  3.15367997e-01\n",
      "Epoch: 28869 mean train loss:  3.79542680e-03, bound:  3.15367967e-01\n",
      "Epoch: 28870 mean train loss:  3.79535672e-03, bound:  3.15367967e-01\n",
      "Epoch: 28871 mean train loss:  3.79534764e-03, bound:  3.15367907e-01\n",
      "Epoch: 28872 mean train loss:  3.79534787e-03, bound:  3.15367907e-01\n",
      "Epoch: 28873 mean train loss:  3.79517884e-03, bound:  3.15367877e-01\n",
      "Epoch: 28874 mean train loss:  3.79491760e-03, bound:  3.15367877e-01\n",
      "Epoch: 28875 mean train loss:  3.79457395e-03, bound:  3.15367877e-01\n",
      "Epoch: 28876 mean train loss:  3.79430084e-03, bound:  3.15367848e-01\n",
      "Epoch: 28877 mean train loss:  3.79412435e-03, bound:  3.15367848e-01\n",
      "Epoch: 28878 mean train loss:  3.79401748e-03, bound:  3.15367788e-01\n",
      "Epoch: 28879 mean train loss:  3.79390526e-03, bound:  3.15367788e-01\n",
      "Epoch: 28880 mean train loss:  3.79376556e-03, bound:  3.15367788e-01\n",
      "Epoch: 28881 mean train loss:  3.79358022e-03, bound:  3.15367788e-01\n",
      "Epoch: 28882 mean train loss:  3.79337417e-03, bound:  3.15367788e-01\n",
      "Epoch: 28883 mean train loss:  3.79322609e-03, bound:  3.15367699e-01\n",
      "Epoch: 28884 mean train loss:  3.79303726e-03, bound:  3.15367699e-01\n",
      "Epoch: 28885 mean train loss:  3.79286357e-03, bound:  3.15367699e-01\n",
      "Epoch: 28886 mean train loss:  3.79274529e-03, bound:  3.15367669e-01\n",
      "Epoch: 28887 mean train loss:  3.79256788e-03, bound:  3.15367669e-01\n",
      "Epoch: 28888 mean train loss:  3.79243703e-03, bound:  3.15367669e-01\n",
      "Epoch: 28889 mean train loss:  3.79228522e-03, bound:  3.15367669e-01\n",
      "Epoch: 28890 mean train loss:  3.79210012e-03, bound:  3.15367639e-01\n",
      "Epoch: 28891 mean train loss:  3.79198743e-03, bound:  3.15367579e-01\n",
      "Epoch: 28892 mean train loss:  3.79174342e-03, bound:  3.15367579e-01\n",
      "Epoch: 28893 mean train loss:  3.79157998e-03, bound:  3.15367579e-01\n",
      "Epoch: 28894 mean train loss:  3.79137904e-03, bound:  3.15367579e-01\n",
      "Epoch: 28895 mean train loss:  3.79125774e-03, bound:  3.15367550e-01\n",
      "Epoch: 28896 mean train loss:  3.79112456e-03, bound:  3.15367550e-01\n",
      "Epoch: 28897 mean train loss:  3.79095995e-03, bound:  3.15367520e-01\n",
      "Epoch: 28898 mean train loss:  3.79078789e-03, bound:  3.15367490e-01\n",
      "Epoch: 28899 mean train loss:  3.79064167e-03, bound:  3.15367490e-01\n",
      "Epoch: 28900 mean train loss:  3.79049475e-03, bound:  3.15367460e-01\n",
      "Epoch: 28901 mean train loss:  3.79033480e-03, bound:  3.15367460e-01\n",
      "Epoch: 28902 mean train loss:  3.79018299e-03, bound:  3.15367430e-01\n",
      "Epoch: 28903 mean train loss:  3.78998742e-03, bound:  3.15367430e-01\n",
      "Epoch: 28904 mean train loss:  3.78988218e-03, bound:  3.15367401e-01\n",
      "Epoch: 28905 mean train loss:  3.78969056e-03, bound:  3.15367401e-01\n",
      "Epoch: 28906 mean train loss:  3.78951780e-03, bound:  3.15367371e-01\n",
      "Epoch: 28907 mean train loss:  3.78934667e-03, bound:  3.15367341e-01\n",
      "Epoch: 28908 mean train loss:  3.78920184e-03, bound:  3.15367341e-01\n",
      "Epoch: 28909 mean train loss:  3.78904911e-03, bound:  3.15367311e-01\n",
      "Epoch: 28910 mean train loss:  3.78887611e-03, bound:  3.15367311e-01\n",
      "Epoch: 28911 mean train loss:  3.78869288e-03, bound:  3.15367252e-01\n",
      "Epoch: 28912 mean train loss:  3.78854689e-03, bound:  3.15367252e-01\n",
      "Epoch: 28913 mean train loss:  3.78835038e-03, bound:  3.15367252e-01\n",
      "Epoch: 28914 mean train loss:  3.78822279e-03, bound:  3.15367222e-01\n",
      "Epoch: 28915 mean train loss:  3.78807285e-03, bound:  3.15367222e-01\n",
      "Epoch: 28916 mean train loss:  3.78792384e-03, bound:  3.15367192e-01\n",
      "Epoch: 28917 mean train loss:  3.78776691e-03, bound:  3.15367192e-01\n",
      "Epoch: 28918 mean train loss:  3.78761953e-03, bound:  3.15367132e-01\n",
      "Epoch: 28919 mean train loss:  3.78744770e-03, bound:  3.15367132e-01\n",
      "Epoch: 28920 mean train loss:  3.78733664e-03, bound:  3.15367132e-01\n",
      "Epoch: 28921 mean train loss:  3.78709775e-03, bound:  3.15367103e-01\n",
      "Epoch: 28922 mean train loss:  3.78692080e-03, bound:  3.15367103e-01\n",
      "Epoch: 28923 mean train loss:  3.78677878e-03, bound:  3.15367103e-01\n",
      "Epoch: 28924 mean train loss:  3.78663838e-03, bound:  3.15367103e-01\n",
      "Epoch: 28925 mean train loss:  3.78653081e-03, bound:  3.15367013e-01\n",
      "Epoch: 28926 mean train loss:  3.78634105e-03, bound:  3.15367013e-01\n",
      "Epoch: 28927 mean train loss:  3.78614757e-03, bound:  3.15367013e-01\n",
      "Epoch: 28928 mean train loss:  3.78597807e-03, bound:  3.15366983e-01\n",
      "Epoch: 28929 mean train loss:  3.78586724e-03, bound:  3.15366983e-01\n",
      "Epoch: 28930 mean train loss:  3.78566887e-03, bound:  3.15366983e-01\n",
      "Epoch: 28931 mean train loss:  3.78551846e-03, bound:  3.15366983e-01\n",
      "Epoch: 28932 mean train loss:  3.78535246e-03, bound:  3.15366894e-01\n",
      "Epoch: 28933 mean train loss:  3.78518994e-03, bound:  3.15366894e-01\n",
      "Epoch: 28934 mean train loss:  3.78505350e-03, bound:  3.15366894e-01\n",
      "Epoch: 28935 mean train loss:  3.78489215e-03, bound:  3.15366864e-01\n",
      "Epoch: 28936 mean train loss:  3.78470961e-03, bound:  3.15366864e-01\n",
      "Epoch: 28937 mean train loss:  3.78457876e-03, bound:  3.15366864e-01\n",
      "Epoch: 28938 mean train loss:  3.78440041e-03, bound:  3.15366864e-01\n",
      "Epoch: 28939 mean train loss:  3.78422835e-03, bound:  3.15366805e-01\n",
      "Epoch: 28940 mean train loss:  3.78410262e-03, bound:  3.15366805e-01\n",
      "Epoch: 28941 mean train loss:  3.78392800e-03, bound:  3.15366775e-01\n",
      "Epoch: 28942 mean train loss:  3.78375058e-03, bound:  3.15366775e-01\n",
      "Epoch: 28943 mean train loss:  3.78359039e-03, bound:  3.15366745e-01\n",
      "Epoch: 28944 mean train loss:  3.78345023e-03, bound:  3.15366745e-01\n",
      "Epoch: 28945 mean train loss:  3.78324254e-03, bound:  3.15366745e-01\n",
      "Epoch: 28946 mean train loss:  3.78305814e-03, bound:  3.15366685e-01\n",
      "Epoch: 28947 mean train loss:  3.78292846e-03, bound:  3.15366685e-01\n",
      "Epoch: 28948 mean train loss:  3.78275127e-03, bound:  3.15366685e-01\n",
      "Epoch: 28949 mean train loss:  3.78256594e-03, bound:  3.15366656e-01\n",
      "Epoch: 28950 mean train loss:  3.78242531e-03, bound:  3.15366626e-01\n",
      "Epoch: 28951 mean train loss:  3.78234289e-03, bound:  3.15366626e-01\n",
      "Epoch: 28952 mean train loss:  3.78212309e-03, bound:  3.15366626e-01\n",
      "Epoch: 28953 mean train loss:  3.78202763e-03, bound:  3.15366566e-01\n",
      "Epoch: 28954 mean train loss:  3.78182810e-03, bound:  3.15366566e-01\n",
      "Epoch: 28955 mean train loss:  3.78172263e-03, bound:  3.15366536e-01\n",
      "Epoch: 28956 mean train loss:  3.78151308e-03, bound:  3.15366536e-01\n",
      "Epoch: 28957 mean train loss:  3.78137780e-03, bound:  3.15366536e-01\n",
      "Epoch: 28958 mean train loss:  3.78123275e-03, bound:  3.15366507e-01\n",
      "Epoch: 28959 mean train loss:  3.78104416e-03, bound:  3.15366447e-01\n",
      "Epoch: 28960 mean train loss:  3.78090609e-03, bound:  3.15366447e-01\n",
      "Epoch: 28961 mean train loss:  3.78075126e-03, bound:  3.15366447e-01\n",
      "Epoch: 28962 mean train loss:  3.78054986e-03, bound:  3.15366417e-01\n",
      "Epoch: 28963 mean train loss:  3.78034730e-03, bound:  3.15366417e-01\n",
      "Epoch: 28964 mean train loss:  3.78020946e-03, bound:  3.15366358e-01\n",
      "Epoch: 28965 mean train loss:  3.78002552e-03, bound:  3.15366358e-01\n",
      "Epoch: 28966 mean train loss:  3.77986720e-03, bound:  3.15366358e-01\n",
      "Epoch: 28967 mean train loss:  3.77971469e-03, bound:  3.15366328e-01\n",
      "Epoch: 28968 mean train loss:  3.77953378e-03, bound:  3.15366328e-01\n",
      "Epoch: 28969 mean train loss:  3.77937336e-03, bound:  3.15366298e-01\n",
      "Epoch: 28970 mean train loss:  3.77924647e-03, bound:  3.15366298e-01\n",
      "Epoch: 28971 mean train loss:  3.77909443e-03, bound:  3.15366298e-01\n",
      "Epoch: 28972 mean train loss:  3.77894053e-03, bound:  3.15366298e-01\n",
      "Epoch: 28973 mean train loss:  3.77876335e-03, bound:  3.15366238e-01\n",
      "Epoch: 28974 mean train loss:  3.77864484e-03, bound:  3.15366238e-01\n",
      "Epoch: 28975 mean train loss:  3.77857545e-03, bound:  3.15366209e-01\n",
      "Epoch: 28976 mean train loss:  3.77834192e-03, bound:  3.15366179e-01\n",
      "Epoch: 28977 mean train loss:  3.77824903e-03, bound:  3.15366179e-01\n",
      "Epoch: 28978 mean train loss:  3.77818919e-03, bound:  3.15366179e-01\n",
      "Epoch: 28979 mean train loss:  3.77802271e-03, bound:  3.15366179e-01\n",
      "Epoch: 28980 mean train loss:  3.77795403e-03, bound:  3.15366119e-01\n",
      "Epoch: 28981 mean train loss:  3.77792236e-03, bound:  3.15366119e-01\n",
      "Epoch: 28982 mean train loss:  3.77798965e-03, bound:  3.15366060e-01\n",
      "Epoch: 28983 mean train loss:  3.77805997e-03, bound:  3.15366060e-01\n",
      "Epoch: 28984 mean train loss:  3.77825531e-03, bound:  3.15366060e-01\n",
      "Epoch: 28985 mean train loss:  3.77863576e-03, bound:  3.15366060e-01\n",
      "Epoch: 28986 mean train loss:  3.77920386e-03, bound:  3.15366000e-01\n",
      "Epoch: 28987 mean train loss:  3.78006021e-03, bound:  3.15366060e-01\n",
      "Epoch: 28988 mean train loss:  3.78133310e-03, bound:  3.15365970e-01\n",
      "Epoch: 28989 mean train loss:  3.78299993e-03, bound:  3.15366000e-01\n",
      "Epoch: 28990 mean train loss:  3.78527306e-03, bound:  3.15365940e-01\n",
      "Epoch: 28991 mean train loss:  3.78811569e-03, bound:  3.15365970e-01\n",
      "Epoch: 28992 mean train loss:  3.79157695e-03, bound:  3.15365881e-01\n",
      "Epoch: 28993 mean train loss:  3.79493576e-03, bound:  3.15365970e-01\n",
      "Epoch: 28994 mean train loss:  3.79738817e-03, bound:  3.15365851e-01\n",
      "Epoch: 28995 mean train loss:  3.79784009e-03, bound:  3.15365911e-01\n",
      "Epoch: 28996 mean train loss:  3.79514624e-03, bound:  3.15365851e-01\n",
      "Epoch: 28997 mean train loss:  3.78961558e-03, bound:  3.15365881e-01\n",
      "Epoch: 28998 mean train loss:  3.78285628e-03, bound:  3.15365851e-01\n",
      "Epoch: 28999 mean train loss:  3.77723388e-03, bound:  3.15365851e-01\n",
      "Epoch: 29000 mean train loss:  3.77447437e-03, bound:  3.15365791e-01\n",
      "Epoch: 29001 mean train loss:  3.77494842e-03, bound:  3.15365791e-01\n",
      "Epoch: 29002 mean train loss:  3.77746532e-03, bound:  3.15365791e-01\n",
      "Epoch: 29003 mean train loss:  3.78018990e-03, bound:  3.15365762e-01\n",
      "Epoch: 29004 mean train loss:  3.78145813e-03, bound:  3.15365762e-01\n",
      "Epoch: 29005 mean train loss:  3.78066674e-03, bound:  3.15365732e-01\n",
      "Epoch: 29006 mean train loss:  3.77824879e-03, bound:  3.15365762e-01\n",
      "Epoch: 29007 mean train loss:  3.77540127e-03, bound:  3.15365672e-01\n",
      "Epoch: 29008 mean train loss:  3.77348787e-03, bound:  3.15365672e-01\n",
      "Epoch: 29009 mean train loss:  3.77300731e-03, bound:  3.15365672e-01\n",
      "Epoch: 29010 mean train loss:  3.77373933e-03, bound:  3.15365642e-01\n",
      "Epoch: 29011 mean train loss:  3.77496704e-03, bound:  3.15365642e-01\n",
      "Epoch: 29012 mean train loss:  3.77557660e-03, bound:  3.15365613e-01\n",
      "Epoch: 29013 mean train loss:  3.77533701e-03, bound:  3.15365613e-01\n",
      "Epoch: 29014 mean train loss:  3.77428206e-03, bound:  3.15365553e-01\n",
      "Epoch: 29015 mean train loss:  3.77306039e-03, bound:  3.15365553e-01\n",
      "Epoch: 29016 mean train loss:  3.77208134e-03, bound:  3.15365553e-01\n",
      "Epoch: 29017 mean train loss:  3.77166551e-03, bound:  3.15365553e-01\n",
      "Epoch: 29018 mean train loss:  3.77181428e-03, bound:  3.15365553e-01\n",
      "Epoch: 29019 mean train loss:  3.77210462e-03, bound:  3.15365493e-01\n",
      "Epoch: 29020 mean train loss:  3.77238914e-03, bound:  3.15365493e-01\n",
      "Epoch: 29021 mean train loss:  3.77232279e-03, bound:  3.15365434e-01\n",
      "Epoch: 29022 mean train loss:  3.77190998e-03, bound:  3.15365434e-01\n",
      "Epoch: 29023 mean train loss:  3.77136027e-03, bound:  3.15365434e-01\n",
      "Epoch: 29024 mean train loss:  3.77083290e-03, bound:  3.15365434e-01\n",
      "Epoch: 29025 mean train loss:  3.77045269e-03, bound:  3.15365374e-01\n",
      "Epoch: 29026 mean train loss:  3.77029716e-03, bound:  3.15365374e-01\n",
      "Epoch: 29027 mean train loss:  3.77034792e-03, bound:  3.15365374e-01\n",
      "Epoch: 29028 mean train loss:  3.77032720e-03, bound:  3.15365344e-01\n",
      "Epoch: 29029 mean train loss:  3.77032417e-03, bound:  3.15365344e-01\n",
      "Epoch: 29030 mean train loss:  3.77018866e-03, bound:  3.15365314e-01\n",
      "Epoch: 29031 mean train loss:  3.76990344e-03, bound:  3.15365314e-01\n",
      "Epoch: 29032 mean train loss:  3.76952463e-03, bound:  3.15365285e-01\n",
      "Epoch: 29033 mean train loss:  3.76925780e-03, bound:  3.15365285e-01\n",
      "Epoch: 29034 mean train loss:  3.76904244e-03, bound:  3.15365225e-01\n",
      "Epoch: 29035 mean train loss:  3.76888248e-03, bound:  3.15365225e-01\n",
      "Epoch: 29036 mean train loss:  3.76886851e-03, bound:  3.15365195e-01\n",
      "Epoch: 29037 mean train loss:  3.76880402e-03, bound:  3.15365195e-01\n",
      "Epoch: 29038 mean train loss:  3.76871997e-03, bound:  3.15365195e-01\n",
      "Epoch: 29039 mean train loss:  3.76859959e-03, bound:  3.15365165e-01\n",
      "Epoch: 29040 mean train loss:  3.76842730e-03, bound:  3.15365165e-01\n",
      "Epoch: 29041 mean train loss:  3.76816792e-03, bound:  3.15365106e-01\n",
      "Epoch: 29042 mean train loss:  3.76788038e-03, bound:  3.15365106e-01\n",
      "Epoch: 29043 mean train loss:  3.76762636e-03, bound:  3.15365076e-01\n",
      "Epoch: 29044 mean train loss:  3.76748270e-03, bound:  3.15365076e-01\n",
      "Epoch: 29045 mean train loss:  3.76733742e-03, bound:  3.15365046e-01\n",
      "Epoch: 29046 mean train loss:  3.76722636e-03, bound:  3.15365046e-01\n",
      "Epoch: 29047 mean train loss:  3.76712740e-03, bound:  3.15365046e-01\n",
      "Epoch: 29048 mean train loss:  3.76702356e-03, bound:  3.15364987e-01\n",
      "Epoch: 29049 mean train loss:  3.76690296e-03, bound:  3.15364987e-01\n",
      "Epoch: 29050 mean train loss:  3.76673369e-03, bound:  3.15364957e-01\n",
      "Epoch: 29051 mean train loss:  3.76647362e-03, bound:  3.15364957e-01\n",
      "Epoch: 29052 mean train loss:  3.76629829e-03, bound:  3.15364957e-01\n",
      "Epoch: 29053 mean train loss:  3.76604265e-03, bound:  3.15364927e-01\n",
      "Epoch: 29054 mean train loss:  3.76589876e-03, bound:  3.15364927e-01\n",
      "Epoch: 29055 mean train loss:  3.76574975e-03, bound:  3.15364897e-01\n",
      "Epoch: 29056 mean train loss:  3.76558769e-03, bound:  3.15364867e-01\n",
      "Epoch: 29057 mean train loss:  3.76539817e-03, bound:  3.15364867e-01\n",
      "Epoch: 29058 mean train loss:  3.76526918e-03, bound:  3.15364867e-01\n",
      "Epoch: 29059 mean train loss:  3.76514532e-03, bound:  3.15364867e-01\n",
      "Epoch: 29060 mean train loss:  3.76496091e-03, bound:  3.15364808e-01\n",
      "Epoch: 29061 mean train loss:  3.76481540e-03, bound:  3.15364808e-01\n",
      "Epoch: 29062 mean train loss:  3.76463891e-03, bound:  3.15364778e-01\n",
      "Epoch: 29063 mean train loss:  3.76448152e-03, bound:  3.15364748e-01\n",
      "Epoch: 29064 mean train loss:  3.76425078e-03, bound:  3.15364748e-01\n",
      "Epoch: 29065 mean train loss:  3.76414508e-03, bound:  3.15364748e-01\n",
      "Epoch: 29066 mean train loss:  3.76396673e-03, bound:  3.15364748e-01\n",
      "Epoch: 29067 mean train loss:  3.76383727e-03, bound:  3.15364689e-01\n",
      "Epoch: 29068 mean train loss:  3.76366009e-03, bound:  3.15364689e-01\n",
      "Epoch: 29069 mean train loss:  3.76355695e-03, bound:  3.15364629e-01\n",
      "Epoch: 29070 mean train loss:  3.76336393e-03, bound:  3.15364629e-01\n",
      "Epoch: 29071 mean train loss:  3.76321399e-03, bound:  3.15364629e-01\n",
      "Epoch: 29072 mean train loss:  3.76300863e-03, bound:  3.15364629e-01\n",
      "Epoch: 29073 mean train loss:  3.76290199e-03, bound:  3.15364599e-01\n",
      "Epoch: 29074 mean train loss:  3.76272807e-03, bound:  3.15364569e-01\n",
      "Epoch: 29075 mean train loss:  3.76256090e-03, bound:  3.15364569e-01\n",
      "Epoch: 29076 mean train loss:  3.76240001e-03, bound:  3.15364540e-01\n",
      "Epoch: 29077 mean train loss:  3.76226800e-03, bound:  3.15364540e-01\n",
      "Epoch: 29078 mean train loss:  3.76208313e-03, bound:  3.15364510e-01\n",
      "Epoch: 29079 mean train loss:  3.76190781e-03, bound:  3.15364510e-01\n",
      "Epoch: 29080 mean train loss:  3.76174902e-03, bound:  3.15364510e-01\n",
      "Epoch: 29081 mean train loss:  3.76158208e-03, bound:  3.15364450e-01\n",
      "Epoch: 29082 mean train loss:  3.76139255e-03, bound:  3.15364450e-01\n",
      "Epoch: 29083 mean train loss:  3.76126682e-03, bound:  3.15364420e-01\n",
      "Epoch: 29084 mean train loss:  3.76109383e-03, bound:  3.15364420e-01\n",
      "Epoch: 29085 mean train loss:  3.76095180e-03, bound:  3.15364391e-01\n",
      "Epoch: 29086 mean train loss:  3.76080582e-03, bound:  3.15364391e-01\n",
      "Epoch: 29087 mean train loss:  3.76062584e-03, bound:  3.15364391e-01\n",
      "Epoch: 29088 mean train loss:  3.76050104e-03, bound:  3.15364331e-01\n",
      "Epoch: 29089 mean train loss:  3.76031315e-03, bound:  3.15364331e-01\n",
      "Epoch: 29090 mean train loss:  3.76018323e-03, bound:  3.15364301e-01\n",
      "Epoch: 29091 mean train loss:  3.76000302e-03, bound:  3.15364301e-01\n",
      "Epoch: 29092 mean train loss:  3.75983980e-03, bound:  3.15364271e-01\n",
      "Epoch: 29093 mean train loss:  3.75965773e-03, bound:  3.15364271e-01\n",
      "Epoch: 29094 mean train loss:  3.75952804e-03, bound:  3.15364242e-01\n",
      "Epoch: 29095 mean train loss:  3.75935598e-03, bound:  3.15364242e-01\n",
      "Epoch: 29096 mean train loss:  3.75921465e-03, bound:  3.15364242e-01\n",
      "Epoch: 29097 mean train loss:  3.75905237e-03, bound:  3.15364182e-01\n",
      "Epoch: 29098 mean train loss:  3.75888706e-03, bound:  3.15364182e-01\n",
      "Epoch: 29099 mean train loss:  3.75871686e-03, bound:  3.15364152e-01\n",
      "Epoch: 29100 mean train loss:  3.75857903e-03, bound:  3.15364152e-01\n",
      "Epoch: 29101 mean train loss:  3.75838508e-03, bound:  3.15364152e-01\n",
      "Epoch: 29102 mean train loss:  3.75826680e-03, bound:  3.15364122e-01\n",
      "Epoch: 29103 mean train loss:  3.75813572e-03, bound:  3.15364093e-01\n",
      "Epoch: 29104 mean train loss:  3.75794386e-03, bound:  3.15364063e-01\n",
      "Epoch: 29105 mean train loss:  3.75785702e-03, bound:  3.15364063e-01\n",
      "Epoch: 29106 mean train loss:  3.75774107e-03, bound:  3.15364033e-01\n",
      "Epoch: 29107 mean train loss:  3.75759299e-03, bound:  3.15364033e-01\n",
      "Epoch: 29108 mean train loss:  3.75753152e-03, bound:  3.15364033e-01\n",
      "Epoch: 29109 mean train loss:  3.75737762e-03, bound:  3.15363973e-01\n",
      "Epoch: 29110 mean train loss:  3.75729054e-03, bound:  3.15363973e-01\n",
      "Epoch: 29111 mean train loss:  3.75718530e-03, bound:  3.15363944e-01\n",
      "Epoch: 29112 mean train loss:  3.75717343e-03, bound:  3.15363944e-01\n",
      "Epoch: 29113 mean train loss:  3.75711732e-03, bound:  3.15363944e-01\n",
      "Epoch: 29114 mean train loss:  3.75717413e-03, bound:  3.15363944e-01\n",
      "Epoch: 29115 mean train loss:  3.75723979e-03, bound:  3.15363884e-01\n",
      "Epoch: 29116 mean train loss:  3.75748891e-03, bound:  3.15363884e-01\n",
      "Epoch: 29117 mean train loss:  3.75783700e-03, bound:  3.15363824e-01\n",
      "Epoch: 29118 mean train loss:  3.75845772e-03, bound:  3.15363884e-01\n",
      "Epoch: 29119 mean train loss:  3.75930173e-03, bound:  3.15363824e-01\n",
      "Epoch: 29120 mean train loss:  3.76046263e-03, bound:  3.15363824e-01\n",
      "Epoch: 29121 mean train loss:  3.76206613e-03, bound:  3.15363795e-01\n",
      "Epoch: 29122 mean train loss:  3.76403821e-03, bound:  3.15363824e-01\n",
      "Epoch: 29123 mean train loss:  3.76643124e-03, bound:  3.15363735e-01\n",
      "Epoch: 29124 mean train loss:  3.76906199e-03, bound:  3.15363795e-01\n",
      "Epoch: 29125 mean train loss:  3.77163221e-03, bound:  3.15363705e-01\n",
      "Epoch: 29126 mean train loss:  3.77325621e-03, bound:  3.15363765e-01\n",
      "Epoch: 29127 mean train loss:  3.77325015e-03, bound:  3.15363675e-01\n",
      "Epoch: 29128 mean train loss:  3.77080427e-03, bound:  3.15363735e-01\n",
      "Epoch: 29129 mean train loss:  3.76627641e-03, bound:  3.15363646e-01\n",
      "Epoch: 29130 mean train loss:  3.76097090e-03, bound:  3.15363705e-01\n",
      "Epoch: 29131 mean train loss:  3.75630171e-03, bound:  3.15363616e-01\n",
      "Epoch: 29132 mean train loss:  3.75375641e-03, bound:  3.15363646e-01\n",
      "Epoch: 29133 mean train loss:  3.75353871e-03, bound:  3.15363616e-01\n",
      "Epoch: 29134 mean train loss:  3.75506491e-03, bound:  3.15363616e-01\n",
      "Epoch: 29135 mean train loss:  3.75719741e-03, bound:  3.15363616e-01\n",
      "Epoch: 29136 mean train loss:  3.75875062e-03, bound:  3.15363526e-01\n",
      "Epoch: 29137 mean train loss:  3.75901302e-03, bound:  3.15363616e-01\n",
      "Epoch: 29138 mean train loss:  3.75781744e-03, bound:  3.15363526e-01\n",
      "Epoch: 29139 mean train loss:  3.75576736e-03, bound:  3.15363526e-01\n",
      "Epoch: 29140 mean train loss:  3.75368097e-03, bound:  3.15363497e-01\n",
      "Epoch: 29141 mean train loss:  3.75232100e-03, bound:  3.15363497e-01\n",
      "Epoch: 29142 mean train loss:  3.75194056e-03, bound:  3.15363497e-01\n",
      "Epoch: 29143 mean train loss:  3.75243649e-03, bound:  3.15363467e-01\n",
      "Epoch: 29144 mean train loss:  3.75326676e-03, bound:  3.15363467e-01\n",
      "Epoch: 29145 mean train loss:  3.75386095e-03, bound:  3.15363407e-01\n",
      "Epoch: 29146 mean train loss:  3.75392870e-03, bound:  3.15363407e-01\n",
      "Epoch: 29147 mean train loss:  3.75332008e-03, bound:  3.15363377e-01\n",
      "Epoch: 29148 mean train loss:  3.75245558e-03, bound:  3.15363377e-01\n",
      "Epoch: 29149 mean train loss:  3.75146652e-03, bound:  3.15363377e-01\n",
      "Epoch: 29150 mean train loss:  3.75082507e-03, bound:  3.15363348e-01\n",
      "Epoch: 29151 mean train loss:  3.75054916e-03, bound:  3.15363348e-01\n",
      "Epoch: 29152 mean train loss:  3.75053543e-03, bound:  3.15363288e-01\n",
      "Epoch: 29153 mean train loss:  3.75076872e-03, bound:  3.15363288e-01\n",
      "Epoch: 29154 mean train loss:  3.75087955e-03, bound:  3.15363258e-01\n",
      "Epoch: 29155 mean train loss:  3.75087140e-03, bound:  3.15363258e-01\n",
      "Epoch: 29156 mean train loss:  3.75065790e-03, bound:  3.15363228e-01\n",
      "Epoch: 29157 mean train loss:  3.75030958e-03, bound:  3.15363228e-01\n",
      "Epoch: 29158 mean train loss:  3.74980457e-03, bound:  3.15363228e-01\n",
      "Epoch: 29159 mean train loss:  3.74938571e-03, bound:  3.15363199e-01\n",
      "Epoch: 29160 mean train loss:  3.74911330e-03, bound:  3.15363199e-01\n",
      "Epoch: 29161 mean train loss:  3.74895916e-03, bound:  3.15363139e-01\n",
      "Epoch: 29162 mean train loss:  3.74892703e-03, bound:  3.15363139e-01\n",
      "Epoch: 29163 mean train loss:  3.74884438e-03, bound:  3.15363139e-01\n",
      "Epoch: 29164 mean train loss:  3.74875404e-03, bound:  3.15363139e-01\n",
      "Epoch: 29165 mean train loss:  3.74856475e-03, bound:  3.15363079e-01\n",
      "Epoch: 29166 mean train loss:  3.74838081e-03, bound:  3.15363079e-01\n",
      "Epoch: 29167 mean train loss:  3.74814332e-03, bound:  3.15363079e-01\n",
      "Epoch: 29168 mean train loss:  3.74794309e-03, bound:  3.15363020e-01\n",
      "Epoch: 29169 mean train loss:  3.74773168e-03, bound:  3.15363020e-01\n",
      "Epoch: 29170 mean train loss:  3.74756986e-03, bound:  3.15363020e-01\n",
      "Epoch: 29171 mean train loss:  3.74741317e-03, bound:  3.15363020e-01\n",
      "Epoch: 29172 mean train loss:  3.74728232e-03, bound:  3.15363020e-01\n",
      "Epoch: 29173 mean train loss:  3.74717824e-03, bound:  3.15362960e-01\n",
      "Epoch: 29174 mean train loss:  3.74707789e-03, bound:  3.15362960e-01\n",
      "Epoch: 29175 mean train loss:  3.74697242e-03, bound:  3.15362930e-01\n",
      "Epoch: 29176 mean train loss:  3.74675635e-03, bound:  3.15362930e-01\n",
      "Epoch: 29177 mean train loss:  3.74659896e-03, bound:  3.15362930e-01\n",
      "Epoch: 29178 mean train loss:  3.74639523e-03, bound:  3.15362900e-01\n",
      "Epoch: 29179 mean train loss:  3.74615565e-03, bound:  3.15362841e-01\n",
      "Epoch: 29180 mean train loss:  3.74605577e-03, bound:  3.15362841e-01\n",
      "Epoch: 29181 mean train loss:  3.74586764e-03, bound:  3.15362811e-01\n",
      "Epoch: 29182 mean train loss:  3.74571653e-03, bound:  3.15362811e-01\n",
      "Epoch: 29183 mean train loss:  3.74553492e-03, bound:  3.15362811e-01\n",
      "Epoch: 29184 mean train loss:  3.74541082e-03, bound:  3.15362781e-01\n",
      "Epoch: 29185 mean train loss:  3.74527322e-03, bound:  3.15362781e-01\n",
      "Epoch: 29186 mean train loss:  3.74516007e-03, bound:  3.15362781e-01\n",
      "Epoch: 29187 mean train loss:  3.74502223e-03, bound:  3.15362722e-01\n",
      "Epoch: 29188 mean train loss:  3.74487764e-03, bound:  3.15362722e-01\n",
      "Epoch: 29189 mean train loss:  3.74466763e-03, bound:  3.15362692e-01\n",
      "Epoch: 29190 mean train loss:  3.74452048e-03, bound:  3.15362692e-01\n",
      "Epoch: 29191 mean train loss:  3.74439918e-03, bound:  3.15362632e-01\n",
      "Epoch: 29192 mean train loss:  3.74420593e-03, bound:  3.15362632e-01\n",
      "Epoch: 29193 mean train loss:  3.74399684e-03, bound:  3.15362632e-01\n",
      "Epoch: 29194 mean train loss:  3.74382641e-03, bound:  3.15362602e-01\n",
      "Epoch: 29195 mean train loss:  3.74369556e-03, bound:  3.15362602e-01\n",
      "Epoch: 29196 mean train loss:  3.74349835e-03, bound:  3.15362573e-01\n",
      "Epoch: 29197 mean train loss:  3.74332489e-03, bound:  3.15362573e-01\n",
      "Epoch: 29198 mean train loss:  3.74317588e-03, bound:  3.15362513e-01\n",
      "Epoch: 29199 mean train loss:  3.74302454e-03, bound:  3.15362513e-01\n",
      "Epoch: 29200 mean train loss:  3.74285295e-03, bound:  3.15362513e-01\n",
      "Epoch: 29201 mean train loss:  3.74274515e-03, bound:  3.15362513e-01\n",
      "Epoch: 29202 mean train loss:  3.74258170e-03, bound:  3.15362513e-01\n",
      "Epoch: 29203 mean train loss:  3.74239241e-03, bound:  3.15362453e-01\n",
      "Epoch: 29204 mean train loss:  3.74224340e-03, bound:  3.15362453e-01\n",
      "Epoch: 29205 mean train loss:  3.74212652e-03, bound:  3.15362453e-01\n",
      "Epoch: 29206 mean train loss:  3.74197355e-03, bound:  3.15362394e-01\n",
      "Epoch: 29207 mean train loss:  3.74181056e-03, bound:  3.15362394e-01\n",
      "Epoch: 29208 mean train loss:  3.74170835e-03, bound:  3.15362394e-01\n",
      "Epoch: 29209 mean train loss:  3.74151114e-03, bound:  3.15362394e-01\n",
      "Epoch: 29210 mean train loss:  3.74137051e-03, bound:  3.15362334e-01\n",
      "Epoch: 29211 mean train loss:  3.74126923e-03, bound:  3.15362334e-01\n",
      "Epoch: 29212 mean train loss:  3.74121103e-03, bound:  3.15362334e-01\n",
      "Epoch: 29213 mean train loss:  3.74107133e-03, bound:  3.15362334e-01\n",
      "Epoch: 29214 mean train loss:  3.74099589e-03, bound:  3.15362275e-01\n",
      "Epoch: 29215 mean train loss:  3.74081614e-03, bound:  3.15362275e-01\n",
      "Epoch: 29216 mean train loss:  3.74078006e-03, bound:  3.15362245e-01\n",
      "Epoch: 29217 mean train loss:  3.74066364e-03, bound:  3.15362245e-01\n",
      "Epoch: 29218 mean train loss:  3.74048925e-03, bound:  3.15362245e-01\n",
      "Epoch: 29219 mean train loss:  3.74037190e-03, bound:  3.15362185e-01\n",
      "Epoch: 29220 mean train loss:  3.74029297e-03, bound:  3.15362185e-01\n",
      "Epoch: 29221 mean train loss:  3.74011300e-03, bound:  3.15362155e-01\n",
      "Epoch: 29222 mean train loss:  3.74003849e-03, bound:  3.15362155e-01\n",
      "Epoch: 29223 mean train loss:  3.73995840e-03, bound:  3.15362155e-01\n",
      "Epoch: 29224 mean train loss:  3.73979402e-03, bound:  3.15362126e-01\n",
      "Epoch: 29225 mean train loss:  3.73983360e-03, bound:  3.15362126e-01\n",
      "Epoch: 29226 mean train loss:  3.73982126e-03, bound:  3.15362066e-01\n",
      "Epoch: 29227 mean train loss:  3.73982289e-03, bound:  3.15362066e-01\n",
      "Epoch: 29228 mean train loss:  3.73983337e-03, bound:  3.15362036e-01\n",
      "Epoch: 29229 mean train loss:  3.73983919e-03, bound:  3.15362036e-01\n",
      "Epoch: 29230 mean train loss:  3.73994326e-03, bound:  3.15362006e-01\n",
      "Epoch: 29231 mean train loss:  3.74000613e-03, bound:  3.15362036e-01\n",
      "Epoch: 29232 mean train loss:  3.74032115e-03, bound:  3.15362006e-01\n",
      "Epoch: 29233 mean train loss:  3.74058238e-03, bound:  3.15362006e-01\n",
      "Epoch: 29234 mean train loss:  3.74086155e-03, bound:  3.15361947e-01\n",
      "Epoch: 29235 mean train loss:  3.74123803e-03, bound:  3.15361947e-01\n",
      "Epoch: 29236 mean train loss:  3.74169834e-03, bound:  3.15361917e-01\n",
      "Epoch: 29237 mean train loss:  3.74213653e-03, bound:  3.15361917e-01\n",
      "Epoch: 29238 mean train loss:  3.74241848e-03, bound:  3.15361887e-01\n",
      "Epoch: 29239 mean train loss:  3.74276680e-03, bound:  3.15361887e-01\n",
      "Epoch: 29240 mean train loss:  3.74302012e-03, bound:  3.15361828e-01\n",
      "Epoch: 29241 mean train loss:  3.74297635e-03, bound:  3.15361887e-01\n",
      "Epoch: 29242 mean train loss:  3.74265597e-03, bound:  3.15361798e-01\n",
      "Epoch: 29243 mean train loss:  3.74209858e-03, bound:  3.15361828e-01\n",
      "Epoch: 29244 mean train loss:  3.74132651e-03, bound:  3.15361798e-01\n",
      "Epoch 29246: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 29245 mean train loss:  3.74023942e-03, bound:  3.15361798e-01\n",
      "Epoch: 29246 mean train loss:  3.73902963e-03, bound:  3.15361768e-01\n",
      "Epoch: 29247 mean train loss:  3.73785943e-03, bound:  3.15361768e-01\n",
      "Epoch: 29248 mean train loss:  3.73600260e-03, bound:  3.15361768e-01\n",
      "Epoch: 29249 mean train loss:  3.73568549e-03, bound:  3.15361768e-01\n",
      "Epoch: 29250 mean train loss:  3.73691181e-03, bound:  3.15361768e-01\n",
      "Epoch: 29251 mean train loss:  3.73755558e-03, bound:  3.15361768e-01\n",
      "Epoch: 29252 mean train loss:  3.73677164e-03, bound:  3.15361768e-01\n",
      "Epoch: 29253 mean train loss:  3.73571762e-03, bound:  3.15361768e-01\n",
      "Epoch: 29254 mean train loss:  3.73561564e-03, bound:  3.15361768e-01\n",
      "Epoch: 29255 mean train loss:  3.73632414e-03, bound:  3.15361708e-01\n",
      "Epoch: 29256 mean train loss:  3.73666477e-03, bound:  3.15361708e-01\n",
      "Epoch: 29257 mean train loss:  3.73615674e-03, bound:  3.15361708e-01\n",
      "Epoch: 29258 mean train loss:  3.73551482e-03, bound:  3.15361738e-01\n",
      "Epoch: 29259 mean train loss:  3.73549992e-03, bound:  3.15361768e-01\n",
      "Epoch: 29260 mean train loss:  3.73596605e-03, bound:  3.15361768e-01\n",
      "Epoch: 29261 mean train loss:  3.73605359e-03, bound:  3.15361768e-01\n",
      "Epoch: 29262 mean train loss:  3.73574137e-03, bound:  3.15361708e-01\n",
      "Epoch: 29263 mean train loss:  3.73540400e-03, bound:  3.15361708e-01\n",
      "Epoch: 29264 mean train loss:  3.73540795e-03, bound:  3.15361708e-01\n",
      "Epoch: 29265 mean train loss:  3.73564940e-03, bound:  3.15361708e-01\n",
      "Epoch: 29266 mean train loss:  3.73572134e-03, bound:  3.15361708e-01\n",
      "Epoch: 29267 mean train loss:  3.73550016e-03, bound:  3.15361708e-01\n",
      "Epoch: 29268 mean train loss:  3.73530970e-03, bound:  3.15361708e-01\n",
      "Epoch: 29269 mean train loss:  3.73533298e-03, bound:  3.15361708e-01\n",
      "Epoch: 29270 mean train loss:  3.73544591e-03, bound:  3.15361708e-01\n",
      "Epoch: 29271 mean train loss:  3.73549340e-03, bound:  3.15361708e-01\n",
      "Epoch: 29272 mean train loss:  3.73530947e-03, bound:  3.15361708e-01\n",
      "Epoch: 29273 mean train loss:  3.73524777e-03, bound:  3.15361708e-01\n",
      "Epoch: 29274 mean train loss:  3.73524195e-03, bound:  3.15361708e-01\n",
      "Epoch: 29275 mean train loss:  3.73529340e-03, bound:  3.15361708e-01\n",
      "Epoch: 29276 mean train loss:  3.73532204e-03, bound:  3.15361708e-01\n",
      "Epoch: 29277 mean train loss:  3.73524963e-03, bound:  3.15361708e-01\n",
      "Epoch: 29278 mean train loss:  3.73512134e-03, bound:  3.15361708e-01\n",
      "Epoch: 29279 mean train loss:  3.73512600e-03, bound:  3.15361708e-01\n",
      "Epoch: 29280 mean train loss:  3.73519887e-03, bound:  3.15361708e-01\n",
      "Epoch: 29281 mean train loss:  3.73516232e-03, bound:  3.15361708e-01\n",
      "Epoch: 29282 mean train loss:  3.73511878e-03, bound:  3.15361708e-01\n",
      "Epoch: 29283 mean train loss:  3.73504893e-03, bound:  3.15361708e-01\n",
      "Epoch: 29284 mean train loss:  3.73504707e-03, bound:  3.15361708e-01\n",
      "Epoch: 29285 mean train loss:  3.73509130e-03, bound:  3.15361708e-01\n",
      "Epoch: 29286 mean train loss:  3.73506104e-03, bound:  3.15361708e-01\n",
      "Epoch: 29287 mean train loss:  3.73496558e-03, bound:  3.15361708e-01\n",
      "Epoch: 29288 mean train loss:  3.73500329e-03, bound:  3.15361708e-01\n",
      "Epoch: 29289 mean train loss:  3.73495836e-03, bound:  3.15361708e-01\n",
      "Epoch: 29290 mean train loss:  3.73498024e-03, bound:  3.15361708e-01\n",
      "Epoch: 29291 mean train loss:  3.73498211e-03, bound:  3.15361708e-01\n",
      "Epoch: 29292 mean train loss:  3.73487640e-03, bound:  3.15361708e-01\n",
      "Epoch: 29293 mean train loss:  3.73488129e-03, bound:  3.15361708e-01\n",
      "Epoch: 29294 mean train loss:  3.73486895e-03, bound:  3.15361708e-01\n",
      "Epoch: 29295 mean train loss:  3.73485638e-03, bound:  3.15361708e-01\n",
      "Epoch: 29296 mean train loss:  3.73484124e-03, bound:  3.15361708e-01\n",
      "Epoch: 29297 mean train loss:  3.73481424e-03, bound:  3.15361708e-01\n",
      "Epoch: 29298 mean train loss:  3.73476185e-03, bound:  3.15361708e-01\n",
      "Epoch: 29299 mean train loss:  3.73477209e-03, bound:  3.15361679e-01\n",
      "Epoch: 29300 mean train loss:  3.73480516e-03, bound:  3.15361679e-01\n",
      "Epoch: 29301 mean train loss:  3.73476930e-03, bound:  3.15361679e-01\n",
      "Epoch: 29302 mean train loss:  3.73475393e-03, bound:  3.15361679e-01\n",
      "Epoch: 29303 mean train loss:  3.73473926e-03, bound:  3.15361708e-01\n",
      "Epoch: 29304 mean train loss:  3.73472343e-03, bound:  3.15361679e-01\n",
      "Epoch: 29305 mean train loss:  3.73472110e-03, bound:  3.15361708e-01\n",
      "Epoch: 29306 mean train loss:  3.73469945e-03, bound:  3.15361679e-01\n",
      "Epoch: 29307 mean train loss:  3.73464660e-03, bound:  3.15361679e-01\n",
      "Epoch: 29308 mean train loss:  3.73468013e-03, bound:  3.15361679e-01\n",
      "Epoch: 29309 mean train loss:  3.73468129e-03, bound:  3.15361679e-01\n",
      "Epoch: 29310 mean train loss:  3.73466592e-03, bound:  3.15361679e-01\n",
      "Epoch: 29311 mean train loss:  3.73459957e-03, bound:  3.15361679e-01\n",
      "Epoch: 29312 mean train loss:  3.73458420e-03, bound:  3.15361679e-01\n",
      "Epoch: 29313 mean train loss:  3.73459002e-03, bound:  3.15361679e-01\n",
      "Epoch: 29314 mean train loss:  3.73458955e-03, bound:  3.15361679e-01\n",
      "Epoch: 29315 mean train loss:  3.73455207e-03, bound:  3.15361679e-01\n",
      "Epoch: 29316 mean train loss:  3.73451086e-03, bound:  3.15361679e-01\n",
      "Epoch: 29317 mean train loss:  3.73449340e-03, bound:  3.15361679e-01\n",
      "Epoch: 29318 mean train loss:  3.73444520e-03, bound:  3.15361679e-01\n",
      "Epoch: 29319 mean train loss:  3.73448362e-03, bound:  3.15361679e-01\n",
      "Epoch: 29320 mean train loss:  3.73447710e-03, bound:  3.15361679e-01\n",
      "Epoch: 29321 mean train loss:  3.73441051e-03, bound:  3.15361679e-01\n",
      "Epoch: 29322 mean train loss:  3.73441121e-03, bound:  3.15361679e-01\n",
      "Epoch: 29323 mean train loss:  3.73439770e-03, bound:  3.15361679e-01\n",
      "Epoch: 29324 mean train loss:  3.73441400e-03, bound:  3.15361679e-01\n",
      "Epoch: 29325 mean train loss:  3.73438466e-03, bound:  3.15361619e-01\n",
      "Epoch: 29326 mean train loss:  3.73437954e-03, bound:  3.15361619e-01\n",
      "Epoch: 29327 mean train loss:  3.73438746e-03, bound:  3.15361619e-01\n",
      "Epoch: 29328 mean train loss:  3.73438257e-03, bound:  3.15361619e-01\n",
      "Epoch: 29329 mean train loss:  3.73435440e-03, bound:  3.15361589e-01\n",
      "Epoch: 29330 mean train loss:  3.73431318e-03, bound:  3.15361619e-01\n",
      "Epoch: 29331 mean train loss:  3.73428944e-03, bound:  3.15361619e-01\n",
      "Epoch: 29332 mean train loss:  3.73435067e-03, bound:  3.15361619e-01\n",
      "Epoch: 29333 mean train loss:  3.73433204e-03, bound:  3.15361589e-01\n",
      "Epoch: 29334 mean train loss:  3.73426150e-03, bound:  3.15361589e-01\n",
      "Epoch: 29335 mean train loss:  3.73426266e-03, bound:  3.15361589e-01\n",
      "Epoch: 29336 mean train loss:  3.73421633e-03, bound:  3.15361589e-01\n",
      "Epoch: 29337 mean train loss:  3.73423565e-03, bound:  3.15361589e-01\n",
      "Epoch: 29338 mean train loss:  3.73417046e-03, bound:  3.15361589e-01\n",
      "Epoch: 29339 mean train loss:  3.73416580e-03, bound:  3.15361589e-01\n",
      "Epoch: 29340 mean train loss:  3.73418350e-03, bound:  3.15361589e-01\n",
      "Epoch: 29341 mean train loss:  3.73418140e-03, bound:  3.15361589e-01\n",
      "Epoch: 29342 mean train loss:  3.73409176e-03, bound:  3.15361589e-01\n",
      "Epoch: 29343 mean train loss:  3.73409106e-03, bound:  3.15361589e-01\n",
      "Epoch: 29344 mean train loss:  3.73407896e-03, bound:  3.15361589e-01\n",
      "Epoch: 29345 mean train loss:  3.73406662e-03, bound:  3.15361589e-01\n",
      "Epoch: 29346 mean train loss:  3.73404683e-03, bound:  3.15361589e-01\n",
      "Epoch: 29347 mean train loss:  3.73406662e-03, bound:  3.15361589e-01\n",
      "Epoch: 29348 mean train loss:  3.73403123e-03, bound:  3.15361589e-01\n",
      "Epoch: 29349 mean train loss:  3.73398350e-03, bound:  3.15361589e-01\n",
      "Epoch: 29350 mean train loss:  3.73400352e-03, bound:  3.15361589e-01\n",
      "Epoch: 29351 mean train loss:  3.73395393e-03, bound:  3.15361589e-01\n",
      "Epoch: 29352 mean train loss:  3.73397092e-03, bound:  3.15361589e-01\n",
      "Epoch: 29353 mean train loss:  3.73394089e-03, bound:  3.15361589e-01\n",
      "Epoch: 29354 mean train loss:  3.73393740e-03, bound:  3.15361589e-01\n",
      "Epoch: 29355 mean train loss:  3.73387034e-03, bound:  3.15361589e-01\n",
      "Epoch: 29356 mean train loss:  3.73389851e-03, bound:  3.15361589e-01\n",
      "Epoch: 29357 mean train loss:  3.73385591e-03, bound:  3.15361589e-01\n",
      "Epoch: 29358 mean train loss:  3.73387360e-03, bound:  3.15361589e-01\n",
      "Epoch: 29359 mean train loss:  3.73387500e-03, bound:  3.15361589e-01\n",
      "Epoch: 29360 mean train loss:  3.73383285e-03, bound:  3.15361589e-01\n",
      "Epoch: 29361 mean train loss:  3.73380445e-03, bound:  3.15361589e-01\n",
      "Epoch: 29362 mean train loss:  3.73384240e-03, bound:  3.15361589e-01\n",
      "Epoch: 29363 mean train loss:  3.73372040e-03, bound:  3.15361589e-01\n",
      "Epoch: 29364 mean train loss:  3.73373437e-03, bound:  3.15361589e-01\n",
      "Epoch: 29365 mean train loss:  3.73370200e-03, bound:  3.15361589e-01\n",
      "Epoch: 29366 mean train loss:  3.73370526e-03, bound:  3.15361589e-01\n",
      "Epoch: 29367 mean train loss:  3.73370899e-03, bound:  3.15361589e-01\n",
      "Epoch: 29368 mean train loss:  3.73366778e-03, bound:  3.15361589e-01\n",
      "Epoch: 29369 mean train loss:  3.73367523e-03, bound:  3.15361559e-01\n",
      "Epoch: 29370 mean train loss:  3.73369060e-03, bound:  3.15361559e-01\n",
      "Epoch: 29371 mean train loss:  3.73363029e-03, bound:  3.15361559e-01\n",
      "Epoch: 29372 mean train loss:  3.73370154e-03, bound:  3.15361559e-01\n",
      "Epoch: 29373 mean train loss:  3.73363984e-03, bound:  3.15361559e-01\n",
      "Epoch: 29374 mean train loss:  3.73362890e-03, bound:  3.15361559e-01\n",
      "Epoch: 29375 mean train loss:  3.73360398e-03, bound:  3.15361559e-01\n",
      "Epoch: 29376 mean train loss:  3.73354275e-03, bound:  3.15361559e-01\n",
      "Epoch: 29377 mean train loss:  3.73357465e-03, bound:  3.15361559e-01\n",
      "Epoch: 29378 mean train loss:  3.73351132e-03, bound:  3.15361559e-01\n",
      "Epoch: 29379 mean train loss:  3.73353227e-03, bound:  3.15361559e-01\n",
      "Epoch: 29380 mean train loss:  3.73353763e-03, bound:  3.15361559e-01\n",
      "Epoch: 29381 mean train loss:  3.73346289e-03, bound:  3.15361559e-01\n",
      "Epoch: 29382 mean train loss:  3.73345194e-03, bound:  3.15361559e-01\n",
      "Epoch: 29383 mean train loss:  3.73347756e-03, bound:  3.15361559e-01\n",
      "Epoch: 29384 mean train loss:  3.73344030e-03, bound:  3.15361559e-01\n",
      "Epoch: 29385 mean train loss:  3.73337697e-03, bound:  3.15361559e-01\n",
      "Epoch: 29386 mean train loss:  3.73336743e-03, bound:  3.15361559e-01\n",
      "Epoch: 29387 mean train loss:  3.73337814e-03, bound:  3.15361559e-01\n",
      "Epoch: 29388 mean train loss:  3.73334484e-03, bound:  3.15361500e-01\n",
      "Epoch: 29389 mean train loss:  3.73330084e-03, bound:  3.15361500e-01\n",
      "Epoch: 29390 mean train loss:  3.73328920e-03, bound:  3.15361500e-01\n",
      "Epoch: 29391 mean train loss:  3.73333041e-03, bound:  3.15361500e-01\n",
      "Epoch: 29392 mean train loss:  3.73326801e-03, bound:  3.15361500e-01\n",
      "Epoch: 29393 mean train loss:  3.73329571e-03, bound:  3.15361500e-01\n",
      "Epoch: 29394 mean train loss:  3.73326358e-03, bound:  3.15361470e-01\n",
      "Epoch: 29395 mean train loss:  3.73325567e-03, bound:  3.15361500e-01\n",
      "Epoch: 29396 mean train loss:  3.73323052e-03, bound:  3.15361470e-01\n",
      "Epoch: 29397 mean train loss:  3.73323099e-03, bound:  3.15361470e-01\n",
      "Epoch: 29398 mean train loss:  3.73320840e-03, bound:  3.15361470e-01\n",
      "Epoch: 29399 mean train loss:  3.73320654e-03, bound:  3.15361470e-01\n",
      "Epoch: 29400 mean train loss:  3.73317348e-03, bound:  3.15361470e-01\n",
      "Epoch: 29401 mean train loss:  3.73312435e-03, bound:  3.15361470e-01\n",
      "Epoch: 29402 mean train loss:  3.73311015e-03, bound:  3.15361470e-01\n",
      "Epoch: 29403 mean train loss:  3.73310526e-03, bound:  3.15361470e-01\n",
      "Epoch: 29404 mean train loss:  3.73311993e-03, bound:  3.15361470e-01\n",
      "Epoch: 29405 mean train loss:  3.73305334e-03, bound:  3.15361470e-01\n",
      "Epoch: 29406 mean train loss:  3.73308687e-03, bound:  3.15361470e-01\n",
      "Epoch: 29407 mean train loss:  3.73307639e-03, bound:  3.15361470e-01\n",
      "Epoch: 29408 mean train loss:  3.73301725e-03, bound:  3.15361470e-01\n",
      "Epoch: 29409 mean train loss:  3.73299932e-03, bound:  3.15361470e-01\n",
      "Epoch: 29410 mean train loss:  3.73300817e-03, bound:  3.15361470e-01\n",
      "Epoch: 29411 mean train loss:  3.73300351e-03, bound:  3.15361470e-01\n",
      "Epoch: 29412 mean train loss:  3.73301446e-03, bound:  3.15361470e-01\n",
      "Epoch: 29413 mean train loss:  3.73298395e-03, bound:  3.15361470e-01\n",
      "Epoch: 29414 mean train loss:  3.73291480e-03, bound:  3.15361470e-01\n",
      "Epoch: 29415 mean train loss:  3.73291364e-03, bound:  3.15361470e-01\n",
      "Epoch: 29416 mean train loss:  3.73289897e-03, bound:  3.15361470e-01\n",
      "Epoch: 29417 mean train loss:  3.73285357e-03, bound:  3.15361470e-01\n",
      "Epoch: 29418 mean train loss:  3.73285823e-03, bound:  3.15361470e-01\n",
      "Epoch: 29419 mean train loss:  3.73287057e-03, bound:  3.15361470e-01\n",
      "Epoch: 29420 mean train loss:  3.73284915e-03, bound:  3.15361470e-01\n",
      "Epoch: 29421 mean train loss:  3.73280095e-03, bound:  3.15361470e-01\n",
      "Epoch: 29422 mean train loss:  3.73277161e-03, bound:  3.15361470e-01\n",
      "Epoch: 29423 mean train loss:  3.73276649e-03, bound:  3.15361470e-01\n",
      "Epoch: 29424 mean train loss:  3.73279583e-03, bound:  3.15361470e-01\n",
      "Epoch: 29425 mean train loss:  3.73276463e-03, bound:  3.15361470e-01\n",
      "Epoch: 29426 mean train loss:  3.73275043e-03, bound:  3.15361470e-01\n",
      "Epoch: 29427 mean train loss:  3.73272831e-03, bound:  3.15361470e-01\n",
      "Epoch: 29428 mean train loss:  3.73271247e-03, bound:  3.15361440e-01\n",
      "Epoch: 29429 mean train loss:  3.73271923e-03, bound:  3.15361470e-01\n",
      "Epoch: 29430 mean train loss:  3.73268337e-03, bound:  3.15361440e-01\n",
      "Epoch: 29431 mean train loss:  3.73264938e-03, bound:  3.15361440e-01\n",
      "Epoch: 29432 mean train loss:  3.73259210e-03, bound:  3.15361440e-01\n",
      "Epoch: 29433 mean train loss:  3.73259024e-03, bound:  3.15361440e-01\n",
      "Epoch: 29434 mean train loss:  3.73255461e-03, bound:  3.15361440e-01\n",
      "Epoch: 29435 mean train loss:  3.73255671e-03, bound:  3.15361440e-01\n",
      "Epoch: 29436 mean train loss:  3.73255764e-03, bound:  3.15361440e-01\n",
      "Epoch: 29437 mean train loss:  3.73254926e-03, bound:  3.15361440e-01\n",
      "Epoch: 29438 mean train loss:  3.73254041e-03, bound:  3.15361440e-01\n",
      "Epoch: 29439 mean train loss:  3.73253296e-03, bound:  3.15361440e-01\n",
      "Epoch: 29440 mean train loss:  3.73249990e-03, bound:  3.15361440e-01\n",
      "Epoch: 29441 mean train loss:  3.73248756e-03, bound:  3.15361440e-01\n",
      "Epoch: 29442 mean train loss:  3.73248290e-03, bound:  3.15361440e-01\n",
      "Epoch: 29443 mean train loss:  3.73242795e-03, bound:  3.15361440e-01\n",
      "Epoch: 29444 mean train loss:  3.73242795e-03, bound:  3.15361440e-01\n",
      "Epoch: 29445 mean train loss:  3.73243913e-03, bound:  3.15361440e-01\n",
      "Epoch: 29446 mean train loss:  3.73237138e-03, bound:  3.15361440e-01\n",
      "Epoch: 29447 mean train loss:  3.73238139e-03, bound:  3.15361440e-01\n",
      "Epoch: 29448 mean train loss:  3.73236253e-03, bound:  3.15361440e-01\n",
      "Epoch: 29449 mean train loss:  3.73233343e-03, bound:  3.15361440e-01\n",
      "Epoch: 29450 mean train loss:  3.73230409e-03, bound:  3.15361440e-01\n",
      "Epoch: 29451 mean train loss:  3.73229408e-03, bound:  3.15361381e-01\n",
      "Epoch: 29452 mean train loss:  3.73226777e-03, bound:  3.15361381e-01\n",
      "Epoch: 29453 mean train loss:  3.73222632e-03, bound:  3.15361381e-01\n",
      "Epoch: 29454 mean train loss:  3.73225310e-03, bound:  3.15361381e-01\n",
      "Epoch: 29455 mean train loss:  3.73229501e-03, bound:  3.15361381e-01\n",
      "Epoch: 29456 mean train loss:  3.73223331e-03, bound:  3.15361381e-01\n",
      "Epoch: 29457 mean train loss:  3.73215927e-03, bound:  3.15361381e-01\n",
      "Epoch: 29458 mean train loss:  3.73221911e-03, bound:  3.15361381e-01\n",
      "Epoch: 29459 mean train loss:  3.73219163e-03, bound:  3.15361381e-01\n",
      "Epoch: 29460 mean train loss:  3.73217207e-03, bound:  3.15361381e-01\n",
      "Epoch: 29461 mean train loss:  3.73216812e-03, bound:  3.15361381e-01\n",
      "Epoch: 29462 mean train loss:  3.73211317e-03, bound:  3.15361381e-01\n",
      "Epoch: 29463 mean train loss:  3.73208360e-03, bound:  3.15361381e-01\n",
      "Epoch: 29464 mean train loss:  3.73209896e-03, bound:  3.15361381e-01\n",
      "Epoch: 29465 mean train loss:  3.73204239e-03, bound:  3.15361381e-01\n",
      "Epoch: 29466 mean train loss:  3.73202865e-03, bound:  3.15361381e-01\n",
      "Epoch: 29467 mean train loss:  3.73200630e-03, bound:  3.15361381e-01\n",
      "Epoch: 29468 mean train loss:  3.73198069e-03, bound:  3.15361381e-01\n",
      "Epoch: 29469 mean train loss:  3.73198045e-03, bound:  3.15361381e-01\n",
      "Epoch: 29470 mean train loss:  3.73196439e-03, bound:  3.15361381e-01\n",
      "Epoch: 29471 mean train loss:  3.73193063e-03, bound:  3.15361381e-01\n",
      "Epoch: 29472 mean train loss:  3.73194087e-03, bound:  3.15361351e-01\n",
      "Epoch: 29473 mean train loss:  3.73190176e-03, bound:  3.15361351e-01\n",
      "Epoch: 29474 mean train loss:  3.73191596e-03, bound:  3.15361351e-01\n",
      "Epoch: 29475 mean train loss:  3.73191223e-03, bound:  3.15361351e-01\n",
      "Epoch: 29476 mean train loss:  3.73189338e-03, bound:  3.15361351e-01\n",
      "Epoch: 29477 mean train loss:  3.73186334e-03, bound:  3.15361351e-01\n",
      "Epoch: 29478 mean train loss:  3.73188406e-03, bound:  3.15361351e-01\n",
      "Epoch: 29479 mean train loss:  3.73182585e-03, bound:  3.15361351e-01\n",
      "Epoch: 29480 mean train loss:  3.73179256e-03, bound:  3.15361351e-01\n",
      "Epoch: 29481 mean train loss:  3.73179209e-03, bound:  3.15361351e-01\n",
      "Epoch: 29482 mean train loss:  3.73177323e-03, bound:  3.15361351e-01\n",
      "Epoch: 29483 mean train loss:  3.73175577e-03, bound:  3.15361351e-01\n",
      "Epoch: 29484 mean train loss:  3.73172574e-03, bound:  3.15361351e-01\n",
      "Epoch: 29485 mean train loss:  3.73172364e-03, bound:  3.15361351e-01\n",
      "Epoch: 29486 mean train loss:  3.73165682e-03, bound:  3.15361351e-01\n",
      "Epoch: 29487 mean train loss:  3.73165682e-03, bound:  3.15361351e-01\n",
      "Epoch: 29488 mean train loss:  3.73166776e-03, bound:  3.15361351e-01\n",
      "Epoch: 29489 mean train loss:  3.73164518e-03, bound:  3.15361351e-01\n",
      "Epoch: 29490 mean train loss:  3.73161584e-03, bound:  3.15361351e-01\n",
      "Epoch: 29491 mean train loss:  3.73156019e-03, bound:  3.15361321e-01\n",
      "Epoch: 29492 mean train loss:  3.73156019e-03, bound:  3.15361321e-01\n",
      "Epoch: 29493 mean train loss:  3.73157347e-03, bound:  3.15361321e-01\n",
      "Epoch: 29494 mean train loss:  3.73156532e-03, bound:  3.15361291e-01\n",
      "Epoch: 29495 mean train loss:  3.73152248e-03, bound:  3.15361291e-01\n",
      "Epoch: 29496 mean train loss:  3.73152317e-03, bound:  3.15361291e-01\n",
      "Epoch: 29497 mean train loss:  3.73147870e-03, bound:  3.15361291e-01\n",
      "Epoch: 29498 mean train loss:  3.73144727e-03, bound:  3.15361291e-01\n",
      "Epoch: 29499 mean train loss:  3.73145379e-03, bound:  3.15361291e-01\n",
      "Epoch: 29500 mean train loss:  3.73146916e-03, bound:  3.15361291e-01\n",
      "Epoch: 29501 mean train loss:  3.73146753e-03, bound:  3.15361291e-01\n",
      "Epoch: 29502 mean train loss:  3.73147265e-03, bound:  3.15361291e-01\n",
      "Epoch: 29503 mean train loss:  3.73142911e-03, bound:  3.15361291e-01\n",
      "Epoch: 29504 mean train loss:  3.73141142e-03, bound:  3.15361291e-01\n",
      "Epoch: 29505 mean train loss:  3.73135577e-03, bound:  3.15361291e-01\n",
      "Epoch: 29506 mean train loss:  3.73132806e-03, bound:  3.15361291e-01\n",
      "Epoch: 29507 mean train loss:  3.73128103e-03, bound:  3.15361291e-01\n",
      "Epoch: 29508 mean train loss:  3.73132504e-03, bound:  3.15361291e-01\n",
      "Epoch: 29509 mean train loss:  3.73126892e-03, bound:  3.15361291e-01\n",
      "Epoch: 29510 mean train loss:  3.73128685e-03, bound:  3.15361291e-01\n",
      "Epoch: 29511 mean train loss:  3.73126240e-03, bound:  3.15361261e-01\n",
      "Epoch: 29512 mean train loss:  3.73125356e-03, bound:  3.15361261e-01\n",
      "Epoch: 29513 mean train loss:  3.73122725e-03, bound:  3.15361261e-01\n",
      "Epoch: 29514 mean train loss:  3.73125076e-03, bound:  3.15361261e-01\n",
      "Epoch: 29515 mean train loss:  3.73119372e-03, bound:  3.15361261e-01\n",
      "Epoch: 29516 mean train loss:  3.73115693e-03, bound:  3.15361261e-01\n",
      "Epoch: 29517 mean train loss:  3.73119465e-03, bound:  3.15361261e-01\n",
      "Epoch: 29518 mean train loss:  3.73115856e-03, bound:  3.15361261e-01\n",
      "Epoch: 29519 mean train loss:  3.73113295e-03, bound:  3.15361261e-01\n",
      "Epoch: 29520 mean train loss:  3.73108708e-03, bound:  3.15361261e-01\n",
      "Epoch: 29521 mean train loss:  3.73108871e-03, bound:  3.15361261e-01\n",
      "Epoch: 29522 mean train loss:  3.73107893e-03, bound:  3.15361261e-01\n",
      "Epoch: 29523 mean train loss:  3.73107661e-03, bound:  3.15361261e-01\n",
      "Epoch: 29524 mean train loss:  3.73101281e-03, bound:  3.15361261e-01\n",
      "Epoch: 29525 mean train loss:  3.73099488e-03, bound:  3.15361261e-01\n",
      "Epoch: 29526 mean train loss:  3.73100326e-03, bound:  3.15361261e-01\n",
      "Epoch: 29527 mean train loss:  3.73102515e-03, bound:  3.15361261e-01\n",
      "Epoch: 29528 mean train loss:  3.73097206e-03, bound:  3.15361261e-01\n",
      "Epoch: 29529 mean train loss:  3.73093574e-03, bound:  3.15361261e-01\n",
      "Epoch: 29530 mean train loss:  3.73091688e-03, bound:  3.15361261e-01\n",
      "Epoch: 29531 mean train loss:  3.73096881e-03, bound:  3.15361261e-01\n",
      "Epoch: 29532 mean train loss:  3.73087847e-03, bound:  3.15361232e-01\n",
      "Epoch: 29533 mean train loss:  3.73091968e-03, bound:  3.15361261e-01\n",
      "Epoch: 29534 mean train loss:  3.73084494e-03, bound:  3.15361232e-01\n",
      "Epoch: 29535 mean train loss:  3.73084308e-03, bound:  3.15361232e-01\n",
      "Epoch: 29536 mean train loss:  3.73083330e-03, bound:  3.15361232e-01\n",
      "Epoch: 29537 mean train loss:  3.73086007e-03, bound:  3.15361232e-01\n",
      "Epoch: 29538 mean train loss:  3.73077230e-03, bound:  3.15361232e-01\n",
      "Epoch: 29539 mean train loss:  3.73080280e-03, bound:  3.15361232e-01\n",
      "Epoch: 29540 mean train loss:  3.73074156e-03, bound:  3.15361232e-01\n",
      "Epoch: 29541 mean train loss:  3.73075600e-03, bound:  3.15361232e-01\n",
      "Epoch: 29542 mean train loss:  3.73069802e-03, bound:  3.15361232e-01\n",
      "Epoch: 29543 mean train loss:  3.73071479e-03, bound:  3.15361232e-01\n",
      "Epoch: 29544 mean train loss:  3.73075227e-03, bound:  3.15361232e-01\n",
      "Epoch: 29545 mean train loss:  3.73066054e-03, bound:  3.15361232e-01\n",
      "Epoch: 29546 mean train loss:  3.73064540e-03, bound:  3.15361232e-01\n",
      "Epoch: 29547 mean train loss:  3.73067381e-03, bound:  3.15361202e-01\n",
      "Epoch: 29548 mean train loss:  3.73063539e-03, bound:  3.15361202e-01\n",
      "Epoch: 29549 mean train loss:  3.73061234e-03, bound:  3.15361202e-01\n",
      "Epoch: 29550 mean train loss:  3.73059325e-03, bound:  3.15361202e-01\n",
      "Epoch: 29551 mean train loss:  3.73060466e-03, bound:  3.15361202e-01\n",
      "Epoch: 29552 mean train loss:  3.73055646e-03, bound:  3.15361202e-01\n",
      "Epoch: 29553 mean train loss:  3.73051548e-03, bound:  3.15361202e-01\n",
      "Epoch: 29554 mean train loss:  3.73052340e-03, bound:  3.15361202e-01\n",
      "Epoch: 29555 mean train loss:  3.73051269e-03, bound:  3.15361202e-01\n",
      "Epoch: 29556 mean train loss:  3.73044331e-03, bound:  3.15361202e-01\n",
      "Epoch: 29557 mean train loss:  3.73050291e-03, bound:  3.15361202e-01\n",
      "Epoch: 29558 mean train loss:  3.73045471e-03, bound:  3.15361202e-01\n",
      "Epoch: 29559 mean train loss:  3.73042515e-03, bound:  3.15361202e-01\n",
      "Epoch: 29560 mean train loss:  3.73038836e-03, bound:  3.15361202e-01\n",
      "Epoch: 29561 mean train loss:  3.73040023e-03, bound:  3.15361202e-01\n",
      "Epoch: 29562 mean train loss:  3.73037392e-03, bound:  3.15361202e-01\n",
      "Epoch: 29563 mean train loss:  3.73041048e-03, bound:  3.15361172e-01\n",
      "Epoch: 29564 mean train loss:  3.73031432e-03, bound:  3.15361172e-01\n",
      "Epoch: 29565 mean train loss:  3.73030128e-03, bound:  3.15361172e-01\n",
      "Epoch: 29566 mean train loss:  3.73031921e-03, bound:  3.15361172e-01\n",
      "Epoch: 29567 mean train loss:  3.73029569e-03, bound:  3.15361142e-01\n",
      "Epoch: 29568 mean train loss:  3.73024843e-03, bound:  3.15361142e-01\n",
      "Epoch: 29569 mean train loss:  3.73029592e-03, bound:  3.15361142e-01\n",
      "Epoch: 29570 mean train loss:  3.73025425e-03, bound:  3.15361142e-01\n",
      "Epoch: 29571 mean train loss:  3.73020256e-03, bound:  3.15361142e-01\n",
      "Epoch: 29572 mean train loss:  3.73023213e-03, bound:  3.15361142e-01\n",
      "Epoch: 29573 mean train loss:  3.73015855e-03, bound:  3.15361142e-01\n",
      "Epoch: 29574 mean train loss:  3.73015180e-03, bound:  3.15361142e-01\n",
      "Epoch: 29575 mean train loss:  3.73013644e-03, bound:  3.15361142e-01\n",
      "Epoch: 29576 mean train loss:  3.73009848e-03, bound:  3.15361142e-01\n",
      "Epoch: 29577 mean train loss:  3.73011269e-03, bound:  3.15361142e-01\n",
      "Epoch: 29578 mean train loss:  3.73006496e-03, bound:  3.15361142e-01\n",
      "Epoch: 29579 mean train loss:  3.73006682e-03, bound:  3.15361142e-01\n",
      "Epoch: 29580 mean train loss:  3.73007171e-03, bound:  3.15361142e-01\n",
      "Epoch: 29581 mean train loss:  3.73002328e-03, bound:  3.15361142e-01\n",
      "Epoch: 29582 mean train loss:  3.73002514e-03, bound:  3.15361142e-01\n",
      "Epoch: 29583 mean train loss:  3.72998440e-03, bound:  3.15361142e-01\n",
      "Epoch: 29584 mean train loss:  3.72996996e-03, bound:  3.15361142e-01\n",
      "Epoch: 29585 mean train loss:  3.72994854e-03, bound:  3.15361142e-01\n",
      "Epoch: 29586 mean train loss:  3.72994249e-03, bound:  3.15361112e-01\n",
      "Epoch: 29587 mean train loss:  3.72990430e-03, bound:  3.15361142e-01\n",
      "Epoch: 29588 mean train loss:  3.72989872e-03, bound:  3.15361142e-01\n",
      "Epoch: 29589 mean train loss:  3.72989173e-03, bound:  3.15361142e-01\n",
      "Epoch: 29590 mean train loss:  3.72987054e-03, bound:  3.15361112e-01\n",
      "Epoch: 29591 mean train loss:  3.72985518e-03, bound:  3.15361112e-01\n",
      "Epoch: 29592 mean train loss:  3.72984959e-03, bound:  3.15361112e-01\n",
      "Epoch: 29593 mean train loss:  3.72981373e-03, bound:  3.15361112e-01\n",
      "Epoch: 29594 mean train loss:  3.72979511e-03, bound:  3.15361112e-01\n",
      "Epoch: 29595 mean train loss:  3.72976600e-03, bound:  3.15361112e-01\n",
      "Epoch: 29596 mean train loss:  3.72976833e-03, bound:  3.15361112e-01\n",
      "Epoch: 29597 mean train loss:  3.72971664e-03, bound:  3.15361112e-01\n",
      "Epoch: 29598 mean train loss:  3.72969452e-03, bound:  3.15361112e-01\n",
      "Epoch: 29599 mean train loss:  3.72970663e-03, bound:  3.15361112e-01\n",
      "Epoch: 29600 mean train loss:  3.72971059e-03, bound:  3.15361112e-01\n",
      "Epoch: 29601 mean train loss:  3.72964703e-03, bound:  3.15361112e-01\n",
      "Epoch: 29602 mean train loss:  3.72973643e-03, bound:  3.15361112e-01\n",
      "Epoch: 29603 mean train loss:  3.72965983e-03, bound:  3.15361112e-01\n",
      "Epoch: 29604 mean train loss:  3.72956949e-03, bound:  3.15361112e-01\n",
      "Epoch: 29605 mean train loss:  3.72962467e-03, bound:  3.15361112e-01\n",
      "Epoch: 29606 mean train loss:  3.72957089e-03, bound:  3.15361083e-01\n",
      "Epoch: 29607 mean train loss:  3.72951804e-03, bound:  3.15361083e-01\n",
      "Epoch: 29608 mean train loss:  3.72954295e-03, bound:  3.15361083e-01\n",
      "Epoch: 29609 mean train loss:  3.72951804e-03, bound:  3.15361083e-01\n",
      "Epoch: 29610 mean train loss:  3.72952921e-03, bound:  3.15361083e-01\n",
      "Epoch: 29611 mean train loss:  3.72949475e-03, bound:  3.15361083e-01\n",
      "Epoch: 29612 mean train loss:  3.72947264e-03, bound:  3.15361083e-01\n",
      "Epoch: 29613 mean train loss:  3.72944516e-03, bound:  3.15361083e-01\n",
      "Epoch: 29614 mean train loss:  3.72947007e-03, bound:  3.15361083e-01\n",
      "Epoch: 29615 mean train loss:  3.72946076e-03, bound:  3.15361083e-01\n",
      "Epoch: 29616 mean train loss:  3.72940535e-03, bound:  3.15361083e-01\n",
      "Epoch: 29617 mean train loss:  3.72934458e-03, bound:  3.15361083e-01\n",
      "Epoch: 29618 mean train loss:  3.72936483e-03, bound:  3.15361083e-01\n",
      "Epoch: 29619 mean train loss:  3.72934807e-03, bound:  3.15361083e-01\n",
      "Epoch: 29620 mean train loss:  3.72933177e-03, bound:  3.15361083e-01\n",
      "Epoch: 29621 mean train loss:  3.72927752e-03, bound:  3.15361083e-01\n",
      "Epoch: 29622 mean train loss:  3.72927403e-03, bound:  3.15361083e-01\n",
      "Epoch: 29623 mean train loss:  3.72927240e-03, bound:  3.15361053e-01\n",
      "Epoch: 29624 mean train loss:  3.72926821e-03, bound:  3.15361023e-01\n",
      "Epoch: 29625 mean train loss:  3.72925610e-03, bound:  3.15361023e-01\n",
      "Epoch: 29626 mean train loss:  3.72919347e-03, bound:  3.15361023e-01\n",
      "Epoch: 29627 mean train loss:  3.72919347e-03, bound:  3.15361023e-01\n",
      "Epoch: 29628 mean train loss:  3.72916390e-03, bound:  3.15361023e-01\n",
      "Epoch: 29629 mean train loss:  3.72915296e-03, bound:  3.15361023e-01\n",
      "Epoch: 29630 mean train loss:  3.72912595e-03, bound:  3.15361023e-01\n",
      "Epoch: 29631 mean train loss:  3.72909941e-03, bound:  3.15361023e-01\n",
      "Epoch: 29632 mean train loss:  3.72906984e-03, bound:  3.15361023e-01\n",
      "Epoch: 29633 mean train loss:  3.72910174e-03, bound:  3.15361023e-01\n",
      "Epoch: 29634 mean train loss:  3.72908125e-03, bound:  3.15361023e-01\n",
      "Epoch: 29635 mean train loss:  3.72906541e-03, bound:  3.15361023e-01\n",
      "Epoch: 29636 mean train loss:  3.72902141e-03, bound:  3.15361023e-01\n",
      "Epoch: 29637 mean train loss:  3.72900185e-03, bound:  3.15361023e-01\n",
      "Epoch: 29638 mean train loss:  3.72894853e-03, bound:  3.15361023e-01\n",
      "Epoch: 29639 mean train loss:  3.72901047e-03, bound:  3.15361023e-01\n",
      "Epoch: 29640 mean train loss:  3.72895901e-03, bound:  3.15361023e-01\n",
      "Epoch: 29641 mean train loss:  3.72892176e-03, bound:  3.15361023e-01\n",
      "Epoch: 29642 mean train loss:  3.72891477e-03, bound:  3.15361023e-01\n",
      "Epoch: 29643 mean train loss:  3.72887799e-03, bound:  3.15361023e-01\n",
      "Epoch: 29644 mean train loss:  3.72890569e-03, bound:  3.15360993e-01\n",
      "Epoch: 29645 mean train loss:  3.72884795e-03, bound:  3.15361023e-01\n",
      "Epoch: 29646 mean train loss:  3.72884329e-03, bound:  3.15360993e-01\n",
      "Epoch: 29647 mean train loss:  3.72882257e-03, bound:  3.15360993e-01\n",
      "Epoch: 29648 mean train loss:  3.72880604e-03, bound:  3.15360993e-01\n",
      "Epoch: 29649 mean train loss:  3.72878439e-03, bound:  3.15360993e-01\n",
      "Epoch: 29650 mean train loss:  3.72874388e-03, bound:  3.15360993e-01\n",
      "Epoch: 29651 mean train loss:  3.72874783e-03, bound:  3.15360993e-01\n",
      "Epoch: 29652 mean train loss:  3.72866821e-03, bound:  3.15360993e-01\n",
      "Epoch: 29653 mean train loss:  3.72872315e-03, bound:  3.15360993e-01\n",
      "Epoch: 29654 mean train loss:  3.72873759e-03, bound:  3.15360993e-01\n",
      "Epoch: 29655 mean train loss:  3.72866588e-03, bound:  3.15360993e-01\n",
      "Epoch: 29656 mean train loss:  3.72869428e-03, bound:  3.15360993e-01\n",
      "Epoch: 29657 mean train loss:  3.72862560e-03, bound:  3.15360993e-01\n",
      "Epoch: 29658 mean train loss:  3.72861442e-03, bound:  3.15360993e-01\n",
      "Epoch: 29659 mean train loss:  3.72860651e-03, bound:  3.15360993e-01\n",
      "Epoch: 29660 mean train loss:  3.72862653e-03, bound:  3.15360993e-01\n",
      "Epoch: 29661 mean train loss:  3.72857321e-03, bound:  3.15360993e-01\n",
      "Epoch: 29662 mean train loss:  3.72853782e-03, bound:  3.15360993e-01\n",
      "Epoch: 29663 mean train loss:  3.72854248e-03, bound:  3.15360993e-01\n",
      "Epoch: 29664 mean train loss:  3.72850127e-03, bound:  3.15360993e-01\n",
      "Epoch: 29665 mean train loss:  3.72848101e-03, bound:  3.15360993e-01\n",
      "Epoch: 29666 mean train loss:  3.72845423e-03, bound:  3.15360993e-01\n",
      "Epoch: 29667 mean train loss:  3.72843049e-03, bound:  3.15360993e-01\n",
      "Epoch: 29668 mean train loss:  3.72840255e-03, bound:  3.15360993e-01\n",
      "Epoch: 29669 mean train loss:  3.72842257e-03, bound:  3.15360993e-01\n",
      "Epoch: 29670 mean train loss:  3.72840208e-03, bound:  3.15360993e-01\n",
      "Epoch: 29671 mean train loss:  3.72837414e-03, bound:  3.15360993e-01\n",
      "Epoch: 29672 mean train loss:  3.72837158e-03, bound:  3.15360993e-01\n",
      "Epoch: 29673 mean train loss:  3.72832455e-03, bound:  3.15360993e-01\n",
      "Epoch: 29674 mean train loss:  3.72832920e-03, bound:  3.15360993e-01\n",
      "Epoch: 29675 mean train loss:  3.72829870e-03, bound:  3.15360993e-01\n",
      "Epoch: 29676 mean train loss:  3.72827752e-03, bound:  3.15360993e-01\n",
      "Epoch: 29677 mean train loss:  3.72826844e-03, bound:  3.15360993e-01\n",
      "Epoch: 29678 mean train loss:  3.72825284e-03, bound:  3.15360993e-01\n",
      "Epoch: 29679 mean train loss:  3.72823165e-03, bound:  3.15360993e-01\n",
      "Epoch: 29680 mean train loss:  3.72822117e-03, bound:  3.15360993e-01\n",
      "Epoch: 29681 mean train loss:  3.72818438e-03, bound:  3.15360904e-01\n",
      "Epoch: 29682 mean train loss:  3.72818462e-03, bound:  3.15360904e-01\n",
      "Epoch: 29683 mean train loss:  3.72812152e-03, bound:  3.15360904e-01\n",
      "Epoch: 29684 mean train loss:  3.72815761e-03, bound:  3.15360904e-01\n",
      "Epoch: 29685 mean train loss:  3.72810080e-03, bound:  3.15360904e-01\n",
      "Epoch: 29686 mean train loss:  3.72812641e-03, bound:  3.15360904e-01\n",
      "Epoch: 29687 mean train loss:  3.72807402e-03, bound:  3.15360904e-01\n",
      "Epoch: 29688 mean train loss:  3.72805237e-03, bound:  3.15360904e-01\n",
      "Epoch: 29689 mean train loss:  3.72804166e-03, bound:  3.15360904e-01\n",
      "Epoch: 29690 mean train loss:  3.72803863e-03, bound:  3.15360904e-01\n",
      "Epoch: 29691 mean train loss:  3.72797111e-03, bound:  3.15360904e-01\n",
      "Epoch: 29692 mean train loss:  3.72801535e-03, bound:  3.15360904e-01\n",
      "Epoch: 29693 mean train loss:  3.72794969e-03, bound:  3.15360904e-01\n",
      "Epoch: 29694 mean train loss:  3.72795667e-03, bound:  3.15360904e-01\n",
      "Epoch: 29695 mean train loss:  3.72794084e-03, bound:  3.15360904e-01\n",
      "Epoch: 29696 mean train loss:  3.72788636e-03, bound:  3.15360904e-01\n",
      "Epoch: 29697 mean train loss:  3.72786052e-03, bound:  3.15360904e-01\n",
      "Epoch: 29698 mean train loss:  3.72789241e-03, bound:  3.15360904e-01\n",
      "Epoch: 29699 mean train loss:  3.72786797e-03, bound:  3.15360904e-01\n",
      "Epoch: 29700 mean train loss:  3.72781581e-03, bound:  3.15360904e-01\n",
      "Epoch: 29701 mean train loss:  3.72776762e-03, bound:  3.15360904e-01\n",
      "Epoch: 29702 mean train loss:  3.72777390e-03, bound:  3.15360904e-01\n",
      "Epoch: 29703 mean train loss:  3.72778182e-03, bound:  3.15360904e-01\n",
      "Epoch: 29704 mean train loss:  3.72776459e-03, bound:  3.15360904e-01\n",
      "Epoch: 29705 mean train loss:  3.72772547e-03, bound:  3.15360904e-01\n",
      "Epoch: 29706 mean train loss:  3.72770475e-03, bound:  3.15360904e-01\n",
      "Epoch: 29707 mean train loss:  3.72769055e-03, bound:  3.15360904e-01\n",
      "Epoch: 29708 mean train loss:  3.72767565e-03, bound:  3.15360904e-01\n",
      "Epoch: 29709 mean train loss:  3.72767984e-03, bound:  3.15360904e-01\n",
      "Epoch: 29710 mean train loss:  3.72764561e-03, bound:  3.15360904e-01\n",
      "Epoch: 29711 mean train loss:  3.72763840e-03, bound:  3.15360904e-01\n",
      "Epoch: 29712 mean train loss:  3.72764235e-03, bound:  3.15360904e-01\n",
      "Epoch: 29713 mean train loss:  3.72760277e-03, bound:  3.15360904e-01\n",
      "Epoch: 29714 mean train loss:  3.72754852e-03, bound:  3.15360904e-01\n",
      "Epoch: 29715 mean train loss:  3.72755551e-03, bound:  3.15360904e-01\n",
      "Epoch: 29716 mean train loss:  3.72749497e-03, bound:  3.15360904e-01\n",
      "Epoch: 29717 mean train loss:  3.72745097e-03, bound:  3.15360904e-01\n",
      "Epoch: 29718 mean train loss:  3.72746121e-03, bound:  3.15360874e-01\n",
      "Epoch: 29719 mean train loss:  3.72747425e-03, bound:  3.15360874e-01\n",
      "Epoch: 29720 mean train loss:  3.72745818e-03, bound:  3.15360874e-01\n",
      "Epoch: 29721 mean train loss:  3.72741092e-03, bound:  3.15360874e-01\n",
      "Epoch: 29722 mean train loss:  3.72740114e-03, bound:  3.15360874e-01\n",
      "Epoch: 29723 mean train loss:  3.72738205e-03, bound:  3.15360874e-01\n",
      "Epoch: 29724 mean train loss:  3.72740184e-03, bound:  3.15360874e-01\n",
      "Epoch: 29725 mean train loss:  3.72736412e-03, bound:  3.15360874e-01\n",
      "Epoch: 29726 mean train loss:  3.72733502e-03, bound:  3.15360874e-01\n",
      "Epoch: 29727 mean train loss:  3.72732664e-03, bound:  3.15360874e-01\n",
      "Epoch: 29728 mean train loss:  3.72732989e-03, bound:  3.15360874e-01\n",
      "Epoch: 29729 mean train loss:  3.72727728e-03, bound:  3.15360874e-01\n",
      "Epoch: 29730 mean train loss:  3.72729008e-03, bound:  3.15360874e-01\n",
      "Epoch: 29731 mean train loss:  3.72727774e-03, bound:  3.15360874e-01\n",
      "Epoch: 29732 mean train loss:  3.72722629e-03, bound:  3.15360874e-01\n",
      "Epoch: 29733 mean train loss:  3.72721674e-03, bound:  3.15360874e-01\n",
      "Epoch: 29734 mean train loss:  3.72719765e-03, bound:  3.15360874e-01\n",
      "Epoch: 29735 mean train loss:  3.72716482e-03, bound:  3.15360874e-01\n",
      "Epoch: 29736 mean train loss:  3.72713222e-03, bound:  3.15360814e-01\n",
      "Epoch: 29737 mean train loss:  3.72713944e-03, bound:  3.15360814e-01\n",
      "Epoch: 29738 mean train loss:  3.72712617e-03, bound:  3.15360785e-01\n",
      "Epoch: 29739 mean train loss:  3.72706656e-03, bound:  3.15360785e-01\n",
      "Epoch: 29740 mean train loss:  3.72703630e-03, bound:  3.15360785e-01\n",
      "Epoch: 29741 mean train loss:  3.72702722e-03, bound:  3.15360785e-01\n",
      "Epoch: 29742 mean train loss:  3.72701301e-03, bound:  3.15360785e-01\n",
      "Epoch: 29743 mean train loss:  3.72699182e-03, bound:  3.15360785e-01\n",
      "Epoch: 29744 mean train loss:  3.72699415e-03, bound:  3.15360785e-01\n",
      "Epoch: 29745 mean train loss:  3.72700440e-03, bound:  3.15360785e-01\n",
      "Epoch: 29746 mean train loss:  3.72698600e-03, bound:  3.15360785e-01\n",
      "Epoch: 29747 mean train loss:  3.72691895e-03, bound:  3.15360785e-01\n",
      "Epoch: 29748 mean train loss:  3.72694200e-03, bound:  3.15360785e-01\n",
      "Epoch: 29749 mean train loss:  3.72687867e-03, bound:  3.15360785e-01\n",
      "Epoch: 29750 mean train loss:  3.72690172e-03, bound:  3.15360785e-01\n",
      "Epoch: 29751 mean train loss:  3.72681883e-03, bound:  3.15360785e-01\n",
      "Epoch: 29752 mean train loss:  3.72682326e-03, bound:  3.15360785e-01\n",
      "Epoch: 29753 mean train loss:  3.72686191e-03, bound:  3.15360785e-01\n",
      "Epoch: 29754 mean train loss:  3.72679345e-03, bound:  3.15360785e-01\n",
      "Epoch: 29755 mean train loss:  3.72679508e-03, bound:  3.15360785e-01\n",
      "Epoch: 29756 mean train loss:  3.72677110e-03, bound:  3.15360785e-01\n",
      "Epoch: 29757 mean train loss:  3.72674805e-03, bound:  3.15360785e-01\n",
      "Epoch: 29758 mean train loss:  3.72670102e-03, bound:  3.15360785e-01\n",
      "Epoch: 29759 mean train loss:  3.72673199e-03, bound:  3.15360785e-01\n",
      "Epoch: 29760 mean train loss:  3.72667960e-03, bound:  3.15360785e-01\n",
      "Epoch: 29761 mean train loss:  3.72669497e-03, bound:  3.15360785e-01\n",
      "Epoch: 29762 mean train loss:  3.72661883e-03, bound:  3.15360785e-01\n",
      "Epoch: 29763 mean train loss:  3.72662139e-03, bound:  3.15360785e-01\n",
      "Epoch: 29764 mean train loss:  3.72663490e-03, bound:  3.15360785e-01\n",
      "Epoch: 29765 mean train loss:  3.72660509e-03, bound:  3.15360785e-01\n",
      "Epoch: 29766 mean train loss:  3.72657296e-03, bound:  3.15360785e-01\n",
      "Epoch: 29767 mean train loss:  3.72656854e-03, bound:  3.15360785e-01\n",
      "Epoch: 29768 mean train loss:  3.72655550e-03, bound:  3.15360785e-01\n",
      "Epoch: 29769 mean train loss:  3.72654269e-03, bound:  3.15360785e-01\n",
      "Epoch: 29770 mean train loss:  3.72645282e-03, bound:  3.15360785e-01\n",
      "Epoch: 29771 mean train loss:  3.72647494e-03, bound:  3.15360755e-01\n",
      "Epoch: 29772 mean train loss:  3.72643583e-03, bound:  3.15360725e-01\n",
      "Epoch: 29773 mean train loss:  3.72638414e-03, bound:  3.15360755e-01\n",
      "Epoch: 29774 mean train loss:  3.72642814e-03, bound:  3.15360725e-01\n",
      "Epoch: 29775 mean train loss:  3.72639089e-03, bound:  3.15360725e-01\n",
      "Epoch: 29776 mean train loss:  3.72640183e-03, bound:  3.15360725e-01\n",
      "Epoch: 29777 mean train loss:  3.72636854e-03, bound:  3.15360725e-01\n",
      "Epoch: 29778 mean train loss:  3.72636621e-03, bound:  3.15360725e-01\n",
      "Epoch: 29779 mean train loss:  3.72630334e-03, bound:  3.15360725e-01\n",
      "Epoch: 29780 mean train loss:  3.72631289e-03, bound:  3.15360725e-01\n",
      "Epoch: 29781 mean train loss:  3.72629683e-03, bound:  3.15360725e-01\n",
      "Epoch: 29782 mean train loss:  3.72626376e-03, bound:  3.15360725e-01\n",
      "Epoch: 29783 mean train loss:  3.72621464e-03, bound:  3.15360725e-01\n",
      "Epoch: 29784 mean train loss:  3.72620509e-03, bound:  3.15360725e-01\n",
      "Epoch: 29785 mean train loss:  3.72617575e-03, bound:  3.15360725e-01\n",
      "Epoch: 29786 mean train loss:  3.72613687e-03, bound:  3.15360725e-01\n",
      "Epoch: 29787 mean train loss:  3.72615247e-03, bound:  3.15360695e-01\n",
      "Epoch: 29788 mean train loss:  3.72613803e-03, bound:  3.15360695e-01\n",
      "Epoch: 29789 mean train loss:  3.72611545e-03, bound:  3.15360695e-01\n",
      "Epoch: 29790 mean train loss:  3.72610427e-03, bound:  3.15360695e-01\n",
      "Epoch: 29791 mean train loss:  3.72604933e-03, bound:  3.15360695e-01\n",
      "Epoch: 29792 mean train loss:  3.72609054e-03, bound:  3.15360695e-01\n",
      "Epoch: 29793 mean train loss:  3.72604840e-03, bound:  3.15360695e-01\n",
      "Epoch: 29794 mean train loss:  3.72598879e-03, bound:  3.15360695e-01\n",
      "Epoch: 29795 mean train loss:  3.72595806e-03, bound:  3.15360695e-01\n",
      "Epoch: 29796 mean train loss:  3.72597878e-03, bound:  3.15360695e-01\n",
      "Epoch: 29797 mean train loss:  3.72595061e-03, bound:  3.15360695e-01\n",
      "Epoch: 29798 mean train loss:  3.72592057e-03, bound:  3.15360695e-01\n",
      "Epoch: 29799 mean train loss:  3.72589799e-03, bound:  3.15360695e-01\n",
      "Epoch: 29800 mean train loss:  3.72591033e-03, bound:  3.15360695e-01\n",
      "Epoch: 29801 mean train loss:  3.72588728e-03, bound:  3.15360695e-01\n",
      "Epoch: 29802 mean train loss:  3.72585398e-03, bound:  3.15360695e-01\n",
      "Epoch: 29803 mean train loss:  3.72588728e-03, bound:  3.15360695e-01\n",
      "Epoch: 29804 mean train loss:  3.72583745e-03, bound:  3.15360695e-01\n",
      "Epoch: 29805 mean train loss:  3.72580043e-03, bound:  3.15360695e-01\n",
      "Epoch: 29806 mean train loss:  3.72576737e-03, bound:  3.15360695e-01\n",
      "Epoch: 29807 mean train loss:  3.72583466e-03, bound:  3.15360695e-01\n",
      "Epoch: 29808 mean train loss:  3.72575875e-03, bound:  3.15360665e-01\n",
      "Epoch: 29809 mean train loss:  3.72571032e-03, bound:  3.15360665e-01\n",
      "Epoch: 29810 mean train loss:  3.72569170e-03, bound:  3.15360665e-01\n",
      "Epoch: 29811 mean train loss:  3.72568122e-03, bound:  3.15360665e-01\n",
      "Epoch: 29812 mean train loss:  3.72563396e-03, bound:  3.15360665e-01\n",
      "Epoch: 29813 mean train loss:  3.72566283e-03, bound:  3.15360665e-01\n",
      "Epoch: 29814 mean train loss:  3.72559321e-03, bound:  3.15360665e-01\n",
      "Epoch: 29815 mean train loss:  3.72561556e-03, bound:  3.15360665e-01\n",
      "Epoch: 29816 mean train loss:  3.72557761e-03, bound:  3.15360665e-01\n",
      "Epoch: 29817 mean train loss:  3.72556434e-03, bound:  3.15360665e-01\n",
      "Epoch: 29818 mean train loss:  3.72555875e-03, bound:  3.15360665e-01\n",
      "Epoch: 29819 mean train loss:  3.72552173e-03, bound:  3.15360665e-01\n",
      "Epoch: 29820 mean train loss:  3.72546026e-03, bound:  3.15360665e-01\n",
      "Epoch: 29821 mean train loss:  3.72546795e-03, bound:  3.15360665e-01\n",
      "Epoch: 29822 mean train loss:  3.72546515e-03, bound:  3.15360665e-01\n",
      "Epoch: 29823 mean train loss:  3.72545980e-03, bound:  3.15360665e-01\n",
      "Epoch: 29824 mean train loss:  3.72544024e-03, bound:  3.15360606e-01\n",
      "Epoch: 29825 mean train loss:  3.72545421e-03, bound:  3.15360606e-01\n",
      "Epoch: 29826 mean train loss:  3.72540508e-03, bound:  3.15360606e-01\n",
      "Epoch: 29827 mean train loss:  3.72538995e-03, bound:  3.15360606e-01\n",
      "Epoch: 29828 mean train loss:  3.72534664e-03, bound:  3.15360606e-01\n",
      "Epoch: 29829 mean train loss:  3.72533710e-03, bound:  3.15360606e-01\n",
      "Epoch: 29830 mean train loss:  3.72532033e-03, bound:  3.15360606e-01\n",
      "Epoch: 29831 mean train loss:  3.72528355e-03, bound:  3.15360606e-01\n",
      "Epoch: 29832 mean train loss:  3.72528494e-03, bound:  3.15360606e-01\n",
      "Epoch: 29833 mean train loss:  3.72530753e-03, bound:  3.15360606e-01\n",
      "Epoch: 29834 mean train loss:  3.72524839e-03, bound:  3.15360606e-01\n",
      "Epoch: 29835 mean train loss:  3.72519344e-03, bound:  3.15360606e-01\n",
      "Epoch: 29836 mean train loss:  3.72521160e-03, bound:  3.15360606e-01\n",
      "Epoch: 29837 mean train loss:  3.72519391e-03, bound:  3.15360606e-01\n",
      "Epoch: 29838 mean train loss:  3.72519018e-03, bound:  3.15360606e-01\n",
      "Epoch: 29839 mean train loss:  3.72513290e-03, bound:  3.15360606e-01\n",
      "Epoch: 29840 mean train loss:  3.72511288e-03, bound:  3.15360606e-01\n",
      "Epoch: 29841 mean train loss:  3.72513779e-03, bound:  3.15360606e-01\n",
      "Epoch: 29842 mean train loss:  3.72509193e-03, bound:  3.15360576e-01\n",
      "Epoch: 29843 mean train loss:  3.72510334e-03, bound:  3.15360576e-01\n",
      "Epoch: 29844 mean train loss:  3.72498878e-03, bound:  3.15360576e-01\n",
      "Epoch: 29845 mean train loss:  3.72501509e-03, bound:  3.15360576e-01\n",
      "Epoch: 29846 mean train loss:  3.72502836e-03, bound:  3.15360576e-01\n",
      "Epoch: 29847 mean train loss:  3.72497737e-03, bound:  3.15360576e-01\n",
      "Epoch: 29848 mean train loss:  3.72495851e-03, bound:  3.15360576e-01\n",
      "Epoch: 29849 mean train loss:  3.72492406e-03, bound:  3.15360576e-01\n",
      "Epoch: 29850 mean train loss:  3.72494035e-03, bound:  3.15360576e-01\n",
      "Epoch: 29851 mean train loss:  3.72488168e-03, bound:  3.15360576e-01\n",
      "Epoch: 29852 mean train loss:  3.72488773e-03, bound:  3.15360576e-01\n",
      "Epoch: 29853 mean train loss:  3.72488052e-03, bound:  3.15360576e-01\n",
      "Epoch: 29854 mean train loss:  3.72487563e-03, bound:  3.15360576e-01\n",
      "Epoch: 29855 mean train loss:  3.72480578e-03, bound:  3.15360576e-01\n",
      "Epoch: 29856 mean train loss:  3.72482720e-03, bound:  3.15360576e-01\n",
      "Epoch: 29857 mean train loss:  3.72473802e-03, bound:  3.15360576e-01\n",
      "Epoch: 29858 mean train loss:  3.72475525e-03, bound:  3.15360576e-01\n",
      "Epoch: 29859 mean train loss:  3.72469123e-03, bound:  3.15360576e-01\n",
      "Epoch: 29860 mean train loss:  3.72472499e-03, bound:  3.15360546e-01\n",
      "Epoch: 29861 mean train loss:  3.72467353e-03, bound:  3.15360546e-01\n",
      "Epoch: 29862 mean train loss:  3.72467889e-03, bound:  3.15360546e-01\n",
      "Epoch: 29863 mean train loss:  3.72466864e-03, bound:  3.15360576e-01\n",
      "Epoch: 29864 mean train loss:  3.72466026e-03, bound:  3.15360546e-01\n",
      "Epoch: 29865 mean train loss:  3.72459763e-03, bound:  3.15360546e-01\n",
      "Epoch: 29866 mean train loss:  3.72460531e-03, bound:  3.15360546e-01\n",
      "Epoch: 29867 mean train loss:  3.72458622e-03, bound:  3.15360546e-01\n",
      "Epoch: 29868 mean train loss:  3.72457923e-03, bound:  3.15360546e-01\n",
      "Epoch: 29869 mean train loss:  3.72452778e-03, bound:  3.15360546e-01\n",
      "Epoch: 29870 mean train loss:  3.72452638e-03, bound:  3.15360546e-01\n",
      "Epoch: 29871 mean train loss:  3.72449355e-03, bound:  3.15360546e-01\n",
      "Epoch: 29872 mean train loss:  3.72446817e-03, bound:  3.15360546e-01\n",
      "Epoch: 29873 mean train loss:  3.72443232e-03, bound:  3.15360546e-01\n",
      "Epoch: 29874 mean train loss:  3.72444699e-03, bound:  3.15360546e-01\n",
      "Epoch: 29875 mean train loss:  3.72442137e-03, bound:  3.15360546e-01\n",
      "Epoch: 29876 mean train loss:  3.72437108e-03, bound:  3.15360546e-01\n",
      "Epoch: 29877 mean train loss:  3.72435758e-03, bound:  3.15360546e-01\n",
      "Epoch: 29878 mean train loss:  3.72434082e-03, bound:  3.15360546e-01\n",
      "Epoch: 29879 mean train loss:  3.72433662e-03, bound:  3.15360546e-01\n",
      "Epoch: 29880 mean train loss:  3.72430496e-03, bound:  3.15360516e-01\n",
      "Epoch: 29881 mean train loss:  3.72431637e-03, bound:  3.15360516e-01\n",
      "Epoch: 29882 mean train loss:  3.72429425e-03, bound:  3.15360487e-01\n",
      "Epoch: 29883 mean train loss:  3.72426165e-03, bound:  3.15360487e-01\n",
      "Epoch: 29884 mean train loss:  3.72421555e-03, bound:  3.15360487e-01\n",
      "Epoch: 29885 mean train loss:  3.72423371e-03, bound:  3.15360487e-01\n",
      "Epoch: 29886 mean train loss:  3.72417178e-03, bound:  3.15360487e-01\n",
      "Epoch: 29887 mean train loss:  3.72419646e-03, bound:  3.15360487e-01\n",
      "Epoch: 29888 mean train loss:  3.72417970e-03, bound:  3.15360487e-01\n",
      "Epoch: 29889 mean train loss:  3.72413686e-03, bound:  3.15360487e-01\n",
      "Epoch: 29890 mean train loss:  3.72410193e-03, bound:  3.15360487e-01\n",
      "Epoch: 29891 mean train loss:  3.72409634e-03, bound:  3.15360487e-01\n",
      "Epoch: 29892 mean train loss:  3.72405141e-03, bound:  3.15360487e-01\n",
      "Epoch: 29893 mean train loss:  3.72407632e-03, bound:  3.15360487e-01\n",
      "Epoch: 29894 mean train loss:  3.72402021e-03, bound:  3.15360487e-01\n",
      "Epoch: 29895 mean train loss:  3.72405909e-03, bound:  3.15360487e-01\n",
      "Epoch: 29896 mean train loss:  3.72395595e-03, bound:  3.15360487e-01\n",
      "Epoch: 29897 mean train loss:  3.72395827e-03, bound:  3.15360487e-01\n",
      "Epoch: 29898 mean train loss:  3.72393779e-03, bound:  3.15360487e-01\n",
      "Epoch: 29899 mean train loss:  3.72391078e-03, bound:  3.15360457e-01\n",
      "Epoch: 29900 mean train loss:  3.72389541e-03, bound:  3.15360457e-01\n",
      "Epoch: 29901 mean train loss:  3.72388237e-03, bound:  3.15360457e-01\n",
      "Epoch: 29902 mean train loss:  3.72380228e-03, bound:  3.15360457e-01\n",
      "Epoch: 29903 mean train loss:  3.72384861e-03, bound:  3.15360457e-01\n",
      "Epoch: 29904 mean train loss:  3.72378575e-03, bound:  3.15360457e-01\n",
      "Epoch: 29905 mean train loss:  3.72380996e-03, bound:  3.15360457e-01\n",
      "Epoch: 29906 mean train loss:  3.72373359e-03, bound:  3.15360457e-01\n",
      "Epoch: 29907 mean train loss:  3.72373941e-03, bound:  3.15360457e-01\n",
      "Epoch: 29908 mean train loss:  3.72370426e-03, bound:  3.15360457e-01\n",
      "Epoch: 29909 mean train loss:  3.72370658e-03, bound:  3.15360457e-01\n",
      "Epoch: 29910 mean train loss:  3.72370356e-03, bound:  3.15360457e-01\n",
      "Epoch: 29911 mean train loss:  3.72367562e-03, bound:  3.15360457e-01\n",
      "Epoch: 29912 mean train loss:  3.72365653e-03, bound:  3.15360457e-01\n",
      "Epoch: 29913 mean train loss:  3.72359273e-03, bound:  3.15360457e-01\n",
      "Epoch: 29914 mean train loss:  3.72361508e-03, bound:  3.15360457e-01\n",
      "Epoch: 29915 mean train loss:  3.72359529e-03, bound:  3.15360457e-01\n",
      "Epoch: 29916 mean train loss:  3.72357387e-03, bound:  3.15360457e-01\n",
      "Epoch: 29917 mean train loss:  3.72358365e-03, bound:  3.15360457e-01\n",
      "Epoch: 29918 mean train loss:  3.72352125e-03, bound:  3.15360427e-01\n",
      "Epoch: 29919 mean train loss:  3.72356223e-03, bound:  3.15360457e-01\n",
      "Epoch: 29920 mean train loss:  3.72349820e-03, bound:  3.15360427e-01\n",
      "Epoch: 29921 mean train loss:  3.72350262e-03, bound:  3.15360427e-01\n",
      "Epoch: 29922 mean train loss:  3.72342812e-03, bound:  3.15360427e-01\n",
      "Epoch: 29923 mean train loss:  3.72340227e-03, bound:  3.15360427e-01\n",
      "Epoch: 29924 mean train loss:  3.72340530e-03, bound:  3.15360427e-01\n",
      "Epoch: 29925 mean train loss:  3.72341508e-03, bound:  3.15360427e-01\n",
      "Epoch: 29926 mean train loss:  3.72335501e-03, bound:  3.15360427e-01\n",
      "Epoch: 29927 mean train loss:  3.72330146e-03, bound:  3.15360427e-01\n",
      "Epoch: 29928 mean train loss:  3.72333173e-03, bound:  3.15360427e-01\n",
      "Epoch: 29929 mean train loss:  3.72331357e-03, bound:  3.15360427e-01\n",
      "Epoch: 29930 mean train loss:  3.72326956e-03, bound:  3.15360427e-01\n",
      "Epoch: 29931 mean train loss:  3.72323487e-03, bound:  3.15360427e-01\n",
      "Epoch: 29932 mean train loss:  3.72321811e-03, bound:  3.15360427e-01\n",
      "Epoch: 29933 mean train loss:  3.72319762e-03, bound:  3.15360427e-01\n",
      "Epoch: 29934 mean train loss:  3.72316292e-03, bound:  3.15360427e-01\n",
      "Epoch: 29935 mean train loss:  3.72316432e-03, bound:  3.15360397e-01\n",
      "Epoch: 29936 mean train loss:  3.72314709e-03, bound:  3.15360367e-01\n",
      "Epoch: 29937 mean train loss:  3.72310588e-03, bound:  3.15360367e-01\n",
      "Epoch: 29938 mean train loss:  3.72310937e-03, bound:  3.15360367e-01\n",
      "Epoch: 29939 mean train loss:  3.72312590e-03, bound:  3.15360367e-01\n",
      "Epoch: 29940 mean train loss:  3.72307491e-03, bound:  3.15360367e-01\n",
      "Epoch: 29941 mean train loss:  3.72309354e-03, bound:  3.15360367e-01\n",
      "Epoch: 29942 mean train loss:  3.72303673e-03, bound:  3.15360367e-01\n",
      "Epoch: 29943 mean train loss:  3.72298039e-03, bound:  3.15360367e-01\n",
      "Epoch: 29944 mean train loss:  3.72299994e-03, bound:  3.15360367e-01\n",
      "Epoch: 29945 mean train loss:  3.72295198e-03, bound:  3.15360367e-01\n",
      "Epoch: 29946 mean train loss:  3.72296618e-03, bound:  3.15360367e-01\n",
      "Epoch: 29947 mean train loss:  3.72295780e-03, bound:  3.15360367e-01\n",
      "Epoch: 29948 mean train loss:  3.72294965e-03, bound:  3.15360367e-01\n",
      "Epoch: 29949 mean train loss:  3.72288027e-03, bound:  3.15360367e-01\n",
      "Epoch: 29950 mean train loss:  3.72288935e-03, bound:  3.15360367e-01\n",
      "Epoch: 29951 mean train loss:  3.72281717e-03, bound:  3.15360367e-01\n",
      "Epoch: 29952 mean train loss:  3.72278551e-03, bound:  3.15360337e-01\n",
      "Epoch: 29953 mean train loss:  3.72282253e-03, bound:  3.15360337e-01\n",
      "Epoch: 29954 mean train loss:  3.72281391e-03, bound:  3.15360337e-01\n",
      "Epoch: 29955 mean train loss:  3.72274686e-03, bound:  3.15360337e-01\n",
      "Epoch: 29956 mean train loss:  3.72278737e-03, bound:  3.15360337e-01\n",
      "Epoch: 29957 mean train loss:  3.72270611e-03, bound:  3.15360337e-01\n",
      "Epoch: 29958 mean train loss:  3.72269587e-03, bound:  3.15360337e-01\n",
      "Epoch: 29959 mean train loss:  3.72268655e-03, bound:  3.15360337e-01\n",
      "Epoch: 29960 mean train loss:  3.72267957e-03, bound:  3.15360337e-01\n",
      "Epoch: 29961 mean train loss:  3.72265233e-03, bound:  3.15360337e-01\n",
      "Epoch: 29962 mean train loss:  3.72264162e-03, bound:  3.15360337e-01\n",
      "Epoch: 29963 mean train loss:  3.72259947e-03, bound:  3.15360337e-01\n",
      "Epoch: 29964 mean train loss:  3.72258131e-03, bound:  3.15360337e-01\n",
      "Epoch: 29965 mean train loss:  3.72251705e-03, bound:  3.15360337e-01\n",
      "Epoch: 29966 mean train loss:  3.72251822e-03, bound:  3.15360337e-01\n",
      "Epoch: 29967 mean train loss:  3.72250983e-03, bound:  3.15360337e-01\n",
      "Epoch: 29968 mean train loss:  3.72246047e-03, bound:  3.15360308e-01\n",
      "Epoch: 29969 mean train loss:  3.72246630e-03, bound:  3.15360308e-01\n",
      "Epoch: 29970 mean train loss:  3.72243812e-03, bound:  3.15360308e-01\n",
      "Epoch: 29971 mean train loss:  3.72244068e-03, bound:  3.15360308e-01\n",
      "Epoch: 29972 mean train loss:  3.72242532e-03, bound:  3.15360308e-01\n",
      "Epoch: 29973 mean train loss:  3.72245186e-03, bound:  3.15360308e-01\n",
      "Epoch: 29974 mean train loss:  3.72237456e-03, bound:  3.15360308e-01\n",
      "Epoch: 29975 mean train loss:  3.72234802e-03, bound:  3.15360308e-01\n",
      "Epoch: 29976 mean train loss:  3.72234592e-03, bound:  3.15360308e-01\n",
      "Epoch: 29977 mean train loss:  3.72225419e-03, bound:  3.15360308e-01\n",
      "Epoch: 29978 mean train loss:  3.72228026e-03, bound:  3.15360308e-01\n",
      "Epoch: 29979 mean train loss:  3.72223160e-03, bound:  3.15360308e-01\n",
      "Epoch: 29980 mean train loss:  3.72221577e-03, bound:  3.15360308e-01\n",
      "Epoch: 29981 mean train loss:  3.72223719e-03, bound:  3.15360308e-01\n",
      "Epoch: 29982 mean train loss:  3.72220553e-03, bound:  3.15360308e-01\n",
      "Epoch: 29983 mean train loss:  3.72217433e-03, bound:  3.15360308e-01\n",
      "Epoch: 29984 mean train loss:  3.72216268e-03, bound:  3.15360308e-01\n",
      "Epoch: 29985 mean train loss:  3.72215593e-03, bound:  3.15360278e-01\n",
      "Epoch: 29986 mean train loss:  3.72212171e-03, bound:  3.15360278e-01\n",
      "Epoch: 29987 mean train loss:  3.72207863e-03, bound:  3.15360278e-01\n",
      "Epoch: 29988 mean train loss:  3.72211053e-03, bound:  3.15360278e-01\n",
      "Epoch: 29989 mean train loss:  3.72204045e-03, bound:  3.15360278e-01\n",
      "Epoch: 29990 mean train loss:  3.72200529e-03, bound:  3.15360278e-01\n",
      "Epoch: 29991 mean train loss:  3.72204348e-03, bound:  3.15360278e-01\n",
      "Epoch: 29992 mean train loss:  3.72199318e-03, bound:  3.15360278e-01\n",
      "Epoch: 29993 mean train loss:  3.72195221e-03, bound:  3.15360278e-01\n",
      "Epoch: 29994 mean train loss:  3.72192799e-03, bound:  3.15360278e-01\n",
      "Epoch: 29995 mean train loss:  3.72191728e-03, bound:  3.15360278e-01\n",
      "Epoch: 29996 mean train loss:  3.72190797e-03, bound:  3.15360278e-01\n",
      "Epoch: 29997 mean train loss:  3.72188771e-03, bound:  3.15360278e-01\n",
      "Epoch: 29998 mean train loss:  3.72187956e-03, bound:  3.15360278e-01\n",
      "Epoch: 29999 mean train loss:  3.72186140e-03, bound:  3.15360248e-01\n",
      "Epoch: 30000 mean train loss:  3.72183486e-03, bound:  3.15360248e-01\n",
      "Epoch: 30001 mean train loss:  3.72180622e-03, bound:  3.15360218e-01\n",
      "Epoch: 30002 mean train loss:  3.72178876e-03, bound:  3.15360218e-01\n",
      "Epoch: 30003 mean train loss:  3.72173544e-03, bound:  3.15360218e-01\n",
      "Epoch: 30004 mean train loss:  3.72172543e-03, bound:  3.15360218e-01\n",
      "Epoch: 30005 mean train loss:  3.72170773e-03, bound:  3.15360218e-01\n",
      "Epoch: 30006 mean train loss:  3.72169260e-03, bound:  3.15360218e-01\n",
      "Epoch: 30007 mean train loss:  3.72168142e-03, bound:  3.15360218e-01\n",
      "Epoch: 30008 mean train loss:  3.72163393e-03, bound:  3.15360218e-01\n",
      "Epoch: 30009 mean train loss:  3.72162275e-03, bound:  3.15360218e-01\n",
      "Epoch: 30010 mean train loss:  3.72156338e-03, bound:  3.15360218e-01\n",
      "Epoch: 30011 mean train loss:  3.72156966e-03, bound:  3.15360218e-01\n",
      "Epoch: 30012 mean train loss:  3.72151425e-03, bound:  3.15360218e-01\n",
      "Epoch: 30013 mean train loss:  3.72153451e-03, bound:  3.15360218e-01\n",
      "Epoch: 30014 mean train loss:  3.72149795e-03, bound:  3.15360218e-01\n",
      "Epoch: 30015 mean train loss:  3.72149446e-03, bound:  3.15360218e-01\n",
      "Epoch: 30016 mean train loss:  3.72145628e-03, bound:  3.15360218e-01\n",
      "Epoch: 30017 mean train loss:  3.72147374e-03, bound:  3.15360188e-01\n",
      "Epoch: 30018 mean train loss:  3.72142112e-03, bound:  3.15360188e-01\n",
      "Epoch: 30019 mean train loss:  3.72143555e-03, bound:  3.15360218e-01\n",
      "Epoch: 30020 mean train loss:  3.72140156e-03, bound:  3.15360188e-01\n",
      "Epoch: 30021 mean train loss:  3.72139062e-03, bound:  3.15360188e-01\n",
      "Epoch: 30022 mean train loss:  3.72135406e-03, bound:  3.15360188e-01\n",
      "Epoch: 30023 mean train loss:  3.72131052e-03, bound:  3.15360188e-01\n",
      "Epoch: 30024 mean train loss:  3.72133590e-03, bound:  3.15360188e-01\n",
      "Epoch: 30025 mean train loss:  3.72130377e-03, bound:  3.15360188e-01\n",
      "Epoch: 30026 mean train loss:  3.72122810e-03, bound:  3.15360188e-01\n",
      "Epoch: 30027 mean train loss:  3.72121809e-03, bound:  3.15360188e-01\n",
      "Epoch: 30028 mean train loss:  3.72124789e-03, bound:  3.15360188e-01\n",
      "Epoch: 30029 mean train loss:  3.72117362e-03, bound:  3.15360188e-01\n",
      "Epoch: 30030 mean train loss:  3.72119504e-03, bound:  3.15360188e-01\n",
      "Epoch: 30031 mean train loss:  3.72115709e-03, bound:  3.15360188e-01\n",
      "Epoch: 30032 mean train loss:  3.72109236e-03, bound:  3.15360188e-01\n",
      "Epoch: 30033 mean train loss:  3.72110843e-03, bound:  3.15360188e-01\n",
      "Epoch: 30034 mean train loss:  3.72107117e-03, bound:  3.15360159e-01\n",
      "Epoch: 30035 mean train loss:  3.72105767e-03, bound:  3.15360159e-01\n",
      "Epoch: 30036 mean train loss:  3.72103136e-03, bound:  3.15360159e-01\n",
      "Epoch: 30037 mean train loss:  3.72099318e-03, bound:  3.15360159e-01\n",
      "Epoch: 30038 mean train loss:  3.72100389e-03, bound:  3.15360159e-01\n",
      "Epoch: 30039 mean train loss:  3.72100575e-03, bound:  3.15360159e-01\n",
      "Epoch: 30040 mean train loss:  3.72095848e-03, bound:  3.15360159e-01\n",
      "Epoch: 30041 mean train loss:  3.72095802e-03, bound:  3.15360159e-01\n",
      "Epoch: 30042 mean train loss:  3.72089795e-03, bound:  3.15360159e-01\n",
      "Epoch: 30043 mean train loss:  3.72091145e-03, bound:  3.15360159e-01\n",
      "Epoch: 30044 mean train loss:  3.72084300e-03, bound:  3.15360159e-01\n",
      "Epoch: 30045 mean train loss:  3.72088957e-03, bound:  3.15360159e-01\n",
      "Epoch: 30046 mean train loss:  3.72081995e-03, bound:  3.15360159e-01\n",
      "Epoch: 30047 mean train loss:  3.72079574e-03, bound:  3.15360159e-01\n",
      "Epoch: 30048 mean train loss:  3.72079294e-03, bound:  3.15360159e-01\n",
      "Epoch: 30049 mean train loss:  3.72078270e-03, bound:  3.15360129e-01\n",
      "Epoch: 30050 mean train loss:  3.72072449e-03, bound:  3.15360099e-01\n",
      "Epoch: 30051 mean train loss:  3.72074335e-03, bound:  3.15360129e-01\n",
      "Epoch: 30052 mean train loss:  3.72069585e-03, bound:  3.15360099e-01\n",
      "Epoch: 30053 mean train loss:  3.72066791e-03, bound:  3.15360099e-01\n",
      "Epoch: 30054 mean train loss:  3.72067792e-03, bound:  3.15360099e-01\n",
      "Epoch: 30055 mean train loss:  3.72066419e-03, bound:  3.15360099e-01\n",
      "Epoch: 30056 mean train loss:  3.72061273e-03, bound:  3.15360099e-01\n",
      "Epoch: 30057 mean train loss:  3.72057967e-03, bound:  3.15360099e-01\n",
      "Epoch: 30058 mean train loss:  3.72061483e-03, bound:  3.15360099e-01\n",
      "Epoch: 30059 mean train loss:  3.72055406e-03, bound:  3.15360099e-01\n",
      "Epoch: 30060 mean train loss:  3.72053660e-03, bound:  3.15360099e-01\n",
      "Epoch: 30061 mean train loss:  3.72051657e-03, bound:  3.15360099e-01\n",
      "Epoch: 30062 mean train loss:  3.72045650e-03, bound:  3.15360099e-01\n",
      "Epoch: 30063 mean train loss:  3.72046372e-03, bound:  3.15360099e-01\n",
      "Epoch: 30064 mean train loss:  3.72039457e-03, bound:  3.15360069e-01\n",
      "Epoch: 30065 mean train loss:  3.72042996e-03, bound:  3.15360069e-01\n",
      "Epoch: 30066 mean train loss:  3.72038689e-03, bound:  3.15360069e-01\n",
      "Epoch: 30067 mean train loss:  3.72038456e-03, bound:  3.15360069e-01\n",
      "Epoch: 30068 mean train loss:  3.72039038e-03, bound:  3.15360069e-01\n",
      "Epoch: 30069 mean train loss:  3.72036383e-03, bound:  3.15360069e-01\n",
      "Epoch: 30070 mean train loss:  3.72033543e-03, bound:  3.15360069e-01\n",
      "Epoch: 30071 mean train loss:  3.72029399e-03, bound:  3.15360069e-01\n",
      "Epoch: 30072 mean train loss:  3.72025510e-03, bound:  3.15360069e-01\n",
      "Epoch: 30073 mean train loss:  3.72025464e-03, bound:  3.15360069e-01\n",
      "Epoch: 30074 mean train loss:  3.72021343e-03, bound:  3.15360069e-01\n",
      "Epoch: 30075 mean train loss:  3.72021901e-03, bound:  3.15360069e-01\n",
      "Epoch: 30076 mean train loss:  3.72020924e-03, bound:  3.15360069e-01\n",
      "Epoch: 30077 mean train loss:  3.72015405e-03, bound:  3.15360069e-01\n",
      "Epoch: 30078 mean train loss:  3.72013543e-03, bound:  3.15360069e-01\n",
      "Epoch: 30079 mean train loss:  3.72012705e-03, bound:  3.15360069e-01\n",
      "Epoch: 30080 mean train loss:  3.72010749e-03, bound:  3.15360069e-01\n",
      "Epoch: 30081 mean train loss:  3.72008514e-03, bound:  3.15360069e-01\n",
      "Epoch: 30082 mean train loss:  3.72001203e-03, bound:  3.15360039e-01\n",
      "Epoch: 30083 mean train loss:  3.72003415e-03, bound:  3.15360039e-01\n",
      "Epoch: 30084 mean train loss:  3.71999084e-03, bound:  3.15360039e-01\n",
      "Epoch: 30085 mean train loss:  3.71999107e-03, bound:  3.15360039e-01\n",
      "Epoch: 30086 mean train loss:  3.71994032e-03, bound:  3.15360039e-01\n",
      "Epoch: 30087 mean train loss:  3.71996267e-03, bound:  3.15360039e-01\n",
      "Epoch: 30088 mean train loss:  3.71993566e-03, bound:  3.15360039e-01\n",
      "Epoch: 30089 mean train loss:  3.71988188e-03, bound:  3.15360039e-01\n",
      "Epoch: 30090 mean train loss:  3.71989748e-03, bound:  3.15360039e-01\n",
      "Epoch: 30091 mean train loss:  3.71984206e-03, bound:  3.15360039e-01\n",
      "Epoch: 30092 mean train loss:  3.71984439e-03, bound:  3.15360039e-01\n",
      "Epoch: 30093 mean train loss:  3.71980644e-03, bound:  3.15360039e-01\n",
      "Epoch: 30094 mean train loss:  3.71979782e-03, bound:  3.15360039e-01\n",
      "Epoch: 30095 mean train loss:  3.71972146e-03, bound:  3.15360039e-01\n",
      "Epoch: 30096 mean train loss:  3.71974357e-03, bound:  3.15360039e-01\n",
      "Epoch: 30097 mean train loss:  3.71973170e-03, bound:  3.15360039e-01\n",
      "Epoch: 30098 mean train loss:  3.71971773e-03, bound:  3.15360039e-01\n",
      "Epoch: 30099 mean train loss:  3.71968769e-03, bound:  3.15360010e-01\n",
      "Epoch: 30100 mean train loss:  3.71966558e-03, bound:  3.15360010e-01\n",
      "Epoch: 30101 mean train loss:  3.71966138e-03, bound:  3.15360010e-01\n",
      "Epoch: 30102 mean train loss:  3.71961482e-03, bound:  3.15360010e-01\n",
      "Epoch: 30103 mean train loss:  3.71958897e-03, bound:  3.15360010e-01\n",
      "Epoch: 30104 mean train loss:  3.71956150e-03, bound:  3.15360010e-01\n",
      "Epoch: 30105 mean train loss:  3.71953147e-03, bound:  3.15360010e-01\n",
      "Epoch: 30106 mean train loss:  3.71949188e-03, bound:  3.15360010e-01\n",
      "Epoch: 30107 mean train loss:  3.71945649e-03, bound:  3.15360010e-01\n",
      "Epoch: 30108 mean train loss:  3.71952495e-03, bound:  3.15360010e-01\n",
      "Epoch: 30109 mean train loss:  3.71948839e-03, bound:  3.15360010e-01\n",
      "Epoch: 30110 mean train loss:  3.71946418e-03, bound:  3.15360010e-01\n",
      "Epoch: 30111 mean train loss:  3.71941016e-03, bound:  3.15360010e-01\n",
      "Epoch: 30112 mean train loss:  3.71935312e-03, bound:  3.15360010e-01\n",
      "Epoch: 30113 mean train loss:  3.71933100e-03, bound:  3.15360010e-01\n",
      "Epoch: 30114 mean train loss:  3.71931447e-03, bound:  3.15360010e-01\n",
      "Epoch: 30115 mean train loss:  3.71934823e-03, bound:  3.15359980e-01\n",
      "Epoch: 30116 mean train loss:  3.71931354e-03, bound:  3.15359980e-01\n",
      "Epoch: 30117 mean train loss:  3.71928653e-03, bound:  3.15359980e-01\n",
      "Epoch: 30118 mean train loss:  3.71925114e-03, bound:  3.15359980e-01\n",
      "Epoch: 30119 mean train loss:  3.71920201e-03, bound:  3.15359980e-01\n",
      "Epoch: 30120 mean train loss:  3.71921342e-03, bound:  3.15359980e-01\n",
      "Epoch: 30121 mean train loss:  3.71916592e-03, bound:  3.15359980e-01\n",
      "Epoch: 30122 mean train loss:  3.71915312e-03, bound:  3.15359980e-01\n",
      "Epoch: 30123 mean train loss:  3.71911121e-03, bound:  3.15359980e-01\n",
      "Epoch: 30124 mean train loss:  3.71912983e-03, bound:  3.15359980e-01\n",
      "Epoch: 30125 mean train loss:  3.71908816e-03, bound:  3.15359980e-01\n",
      "Epoch: 30126 mean train loss:  3.71904857e-03, bound:  3.15359980e-01\n",
      "Epoch: 30127 mean train loss:  3.71905812e-03, bound:  3.15359980e-01\n",
      "Epoch: 30128 mean train loss:  3.71906045e-03, bound:  3.15359980e-01\n",
      "Epoch: 30129 mean train loss:  3.71898431e-03, bound:  3.15359980e-01\n",
      "Epoch: 30130 mean train loss:  3.71902180e-03, bound:  3.15359980e-01\n",
      "Epoch: 30131 mean train loss:  3.71898850e-03, bound:  3.15359950e-01\n",
      "Epoch: 30132 mean train loss:  3.71892820e-03, bound:  3.15359950e-01\n",
      "Epoch: 30133 mean train loss:  3.71892634e-03, bound:  3.15359920e-01\n",
      "Epoch: 30134 mean train loss:  3.71887395e-03, bound:  3.15359920e-01\n",
      "Epoch: 30135 mean train loss:  3.71887651e-03, bound:  3.15359920e-01\n",
      "Epoch: 30136 mean train loss:  3.71887325e-03, bound:  3.15359920e-01\n",
      "Epoch: 30137 mean train loss:  3.71886580e-03, bound:  3.15359920e-01\n",
      "Epoch: 30138 mean train loss:  3.71879293e-03, bound:  3.15359920e-01\n",
      "Epoch: 30139 mean train loss:  3.71876615e-03, bound:  3.15359920e-01\n",
      "Epoch: 30140 mean train loss:  3.71878105e-03, bound:  3.15359920e-01\n",
      "Epoch: 30141 mean train loss:  3.71876382e-03, bound:  3.15359920e-01\n",
      "Epoch: 30142 mean train loss:  3.71870259e-03, bound:  3.15359920e-01\n",
      "Epoch: 30143 mean train loss:  3.71868955e-03, bound:  3.15359920e-01\n",
      "Epoch: 30144 mean train loss:  3.71871260e-03, bound:  3.15359920e-01\n",
      "Epoch: 30145 mean train loss:  3.71869677e-03, bound:  3.15359920e-01\n",
      "Epoch: 30146 mean train loss:  3.71861900e-03, bound:  3.15359890e-01\n",
      "Epoch: 30147 mean train loss:  3.71858710e-03, bound:  3.15359890e-01\n",
      "Epoch: 30148 mean train loss:  3.71862110e-03, bound:  3.15359890e-01\n",
      "Epoch: 30149 mean train loss:  3.71857663e-03, bound:  3.15359890e-01\n",
      "Epoch: 30150 mean train loss:  3.71854729e-03, bound:  3.15359890e-01\n",
      "Epoch: 30151 mean train loss:  3.71847302e-03, bound:  3.15359890e-01\n",
      "Epoch: 30152 mean train loss:  3.71852517e-03, bound:  3.15359890e-01\n",
      "Epoch: 30153 mean train loss:  3.71849211e-03, bound:  3.15359890e-01\n",
      "Epoch: 30154 mean train loss:  3.71844205e-03, bound:  3.15359890e-01\n",
      "Epoch: 30155 mean train loss:  3.71843646e-03, bound:  3.15359890e-01\n",
      "Epoch: 30156 mean train loss:  3.71838780e-03, bound:  3.15359890e-01\n",
      "Epoch: 30157 mean train loss:  3.71838361e-03, bound:  3.15359890e-01\n",
      "Epoch: 30158 mean train loss:  3.71837080e-03, bound:  3.15359890e-01\n",
      "Epoch: 30159 mean train loss:  3.71832005e-03, bound:  3.15359890e-01\n",
      "Epoch: 30160 mean train loss:  3.71827884e-03, bound:  3.15359890e-01\n",
      "Epoch: 30161 mean train loss:  3.71828605e-03, bound:  3.15359890e-01\n",
      "Epoch: 30162 mean train loss:  3.71826673e-03, bound:  3.15359890e-01\n",
      "Epoch: 30163 mean train loss:  3.71823693e-03, bound:  3.15359890e-01\n",
      "Epoch: 30164 mean train loss:  3.71825579e-03, bound:  3.15359861e-01\n",
      "Epoch: 30165 mean train loss:  3.71822622e-03, bound:  3.15359861e-01\n",
      "Epoch: 30166 mean train loss:  3.71815171e-03, bound:  3.15359861e-01\n",
      "Epoch: 30167 mean train loss:  3.71813122e-03, bound:  3.15359861e-01\n",
      "Epoch: 30168 mean train loss:  3.71815683e-03, bound:  3.15359861e-01\n",
      "Epoch: 30169 mean train loss:  3.71815125e-03, bound:  3.15359861e-01\n",
      "Epoch: 30170 mean train loss:  3.71806417e-03, bound:  3.15359861e-01\n",
      "Epoch: 30171 mean train loss:  3.71807627e-03, bound:  3.15359861e-01\n",
      "Epoch: 30172 mean train loss:  3.71804135e-03, bound:  3.15359861e-01\n",
      "Epoch: 30173 mean train loss:  3.71798244e-03, bound:  3.15359861e-01\n",
      "Epoch: 30174 mean train loss:  3.71795124e-03, bound:  3.15359861e-01\n",
      "Epoch: 30175 mean train loss:  3.71800433e-03, bound:  3.15359861e-01\n",
      "Epoch: 30176 mean train loss:  3.71795357e-03, bound:  3.15359861e-01\n",
      "Epoch: 30177 mean train loss:  3.71791469e-03, bound:  3.15359801e-01\n",
      "Epoch: 30178 mean train loss:  3.71792936e-03, bound:  3.15359801e-01\n",
      "Epoch: 30179 mean train loss:  3.71787837e-03, bound:  3.15359801e-01\n",
      "Epoch: 30180 mean train loss:  3.71785928e-03, bound:  3.15359801e-01\n",
      "Epoch: 30181 mean train loss:  3.71782715e-03, bound:  3.15359801e-01\n",
      "Epoch: 30182 mean train loss:  3.71782761e-03, bound:  3.15359801e-01\n",
      "Epoch: 30183 mean train loss:  3.71776894e-03, bound:  3.15359801e-01\n",
      "Epoch: 30184 mean train loss:  3.71776801e-03, bound:  3.15359801e-01\n",
      "Epoch: 30185 mean train loss:  3.71777033e-03, bound:  3.15359801e-01\n",
      "Epoch: 30186 mean train loss:  3.71772307e-03, bound:  3.15359801e-01\n",
      "Epoch: 30187 mean train loss:  3.71769303e-03, bound:  3.15359801e-01\n",
      "Epoch: 30188 mean train loss:  3.71769001e-03, bound:  3.15359801e-01\n",
      "Epoch: 30189 mean train loss:  3.71761410e-03, bound:  3.15359801e-01\n",
      "Epoch: 30190 mean train loss:  3.71759455e-03, bound:  3.15359801e-01\n",
      "Epoch: 30191 mean train loss:  3.71758523e-03, bound:  3.15359801e-01\n",
      "Epoch: 30192 mean train loss:  3.71761248e-03, bound:  3.15359801e-01\n",
      "Epoch: 30193 mean train loss:  3.71757406e-03, bound:  3.15359771e-01\n",
      "Epoch: 30194 mean train loss:  3.71754798e-03, bound:  3.15359801e-01\n",
      "Epoch: 30195 mean train loss:  3.71753005e-03, bound:  3.15359801e-01\n",
      "Epoch: 30196 mean train loss:  3.71747604e-03, bound:  3.15359771e-01\n",
      "Epoch: 30197 mean train loss:  3.71744740e-03, bound:  3.15359771e-01\n",
      "Epoch: 30198 mean train loss:  3.71750444e-03, bound:  3.15359771e-01\n",
      "Epoch: 30199 mean train loss:  3.71739059e-03, bound:  3.15359771e-01\n",
      "Epoch: 30200 mean train loss:  3.71738244e-03, bound:  3.15359771e-01\n",
      "Epoch: 30201 mean train loss:  3.71739035e-03, bound:  3.15359771e-01\n",
      "Epoch: 30202 mean train loss:  3.71733098e-03, bound:  3.15359771e-01\n",
      "Epoch: 30203 mean train loss:  3.71732470e-03, bound:  3.15359771e-01\n",
      "Epoch: 30204 mean train loss:  3.71734286e-03, bound:  3.15359771e-01\n",
      "Epoch: 30205 mean train loss:  3.71727603e-03, bound:  3.15359771e-01\n",
      "Epoch: 30206 mean train loss:  3.71723203e-03, bound:  3.15359771e-01\n",
      "Epoch: 30207 mean train loss:  3.71723715e-03, bound:  3.15359771e-01\n",
      "Epoch: 30208 mean train loss:  3.71720642e-03, bound:  3.15359741e-01\n",
      "Epoch: 30209 mean train loss:  3.71719175e-03, bound:  3.15359741e-01\n",
      "Epoch: 30210 mean train loss:  3.71715892e-03, bound:  3.15359741e-01\n",
      "Epoch: 30211 mean train loss:  3.71716707e-03, bound:  3.15359741e-01\n",
      "Epoch: 30212 mean train loss:  3.71713773e-03, bound:  3.15359741e-01\n",
      "Epoch: 30213 mean train loss:  3.71711096e-03, bound:  3.15359741e-01\n",
      "Epoch: 30214 mean train loss:  3.71703762e-03, bound:  3.15359741e-01\n",
      "Epoch: 30215 mean train loss:  3.71703622e-03, bound:  3.15359741e-01\n",
      "Epoch: 30216 mean train loss:  3.71698523e-03, bound:  3.15359741e-01\n",
      "Epoch: 30217 mean train loss:  3.71701200e-03, bound:  3.15359741e-01\n",
      "Epoch: 30218 mean train loss:  3.71699920e-03, bound:  3.15359741e-01\n",
      "Epoch: 30219 mean train loss:  3.71697033e-03, bound:  3.15359741e-01\n",
      "Epoch: 30220 mean train loss:  3.71690467e-03, bound:  3.15359741e-01\n",
      "Epoch: 30221 mean train loss:  3.71690816e-03, bound:  3.15359741e-01\n",
      "Epoch: 30222 mean train loss:  3.71692097e-03, bound:  3.15359741e-01\n",
      "Epoch: 30223 mean train loss:  3.71690444e-03, bound:  3.15359741e-01\n",
      "Epoch: 30224 mean train loss:  3.71680339e-03, bound:  3.15359741e-01\n",
      "Epoch: 30225 mean train loss:  3.71686136e-03, bound:  3.15359682e-01\n",
      "Epoch: 30226 mean train loss:  3.71676730e-03, bound:  3.15359682e-01\n",
      "Epoch: 30227 mean train loss:  3.71677196e-03, bound:  3.15359682e-01\n",
      "Epoch: 30228 mean train loss:  3.71670560e-03, bound:  3.15359682e-01\n",
      "Epoch: 30229 mean train loss:  3.71674658e-03, bound:  3.15359682e-01\n",
      "Epoch: 30230 mean train loss:  3.71671305e-03, bound:  3.15359682e-01\n",
      "Epoch: 30231 mean train loss:  3.71667673e-03, bound:  3.15359682e-01\n",
      "Epoch: 30232 mean train loss:  3.71666090e-03, bound:  3.15359682e-01\n",
      "Epoch: 30233 mean train loss:  3.71666835e-03, bound:  3.15359682e-01\n",
      "Epoch: 30234 mean train loss:  3.71661386e-03, bound:  3.15359682e-01\n",
      "Epoch: 30235 mean train loss:  3.71656776e-03, bound:  3.15359682e-01\n",
      "Epoch: 30236 mean train loss:  3.71656707e-03, bound:  3.15359682e-01\n",
      "Epoch: 30237 mean train loss:  3.71655310e-03, bound:  3.15359682e-01\n",
      "Epoch: 30238 mean train loss:  3.71652935e-03, bound:  3.15359682e-01\n",
      "Epoch: 30239 mean train loss:  3.71652399e-03, bound:  3.15359682e-01\n",
      "Epoch: 30240 mean train loss:  3.71645507e-03, bound:  3.15359682e-01\n",
      "Epoch: 30241 mean train loss:  3.71643691e-03, bound:  3.15359652e-01\n",
      "Epoch: 30242 mean train loss:  3.71645251e-03, bound:  3.15359652e-01\n",
      "Epoch: 30243 mean train loss:  3.71641060e-03, bound:  3.15359652e-01\n",
      "Epoch: 30244 mean train loss:  3.71638336e-03, bound:  3.15359652e-01\n",
      "Epoch: 30245 mean train loss:  3.71633004e-03, bound:  3.15359652e-01\n",
      "Epoch: 30246 mean train loss:  3.71639687e-03, bound:  3.15359652e-01\n",
      "Epoch: 30247 mean train loss:  3.71632166e-03, bound:  3.15359652e-01\n",
      "Epoch: 30248 mean train loss:  3.71630397e-03, bound:  3.15359652e-01\n",
      "Epoch: 30249 mean train loss:  3.71623039e-03, bound:  3.15359652e-01\n",
      "Epoch: 30250 mean train loss:  3.71619617e-03, bound:  3.15359652e-01\n",
      "Epoch: 30251 mean train loss:  3.71622341e-03, bound:  3.15359652e-01\n",
      "Epoch: 30252 mean train loss:  3.71615752e-03, bound:  3.15359652e-01\n",
      "Epoch: 30253 mean train loss:  3.71615868e-03, bound:  3.15359622e-01\n",
      "Epoch: 30254 mean train loss:  3.71612515e-03, bound:  3.15359622e-01\n",
      "Epoch: 30255 mean train loss:  3.71609558e-03, bound:  3.15359622e-01\n",
      "Epoch: 30256 mean train loss:  3.71609232e-03, bound:  3.15359622e-01\n",
      "Epoch: 30257 mean train loss:  3.71606997e-03, bound:  3.15359622e-01\n",
      "Epoch: 30258 mean train loss:  3.71605647e-03, bound:  3.15359622e-01\n",
      "Epoch: 30259 mean train loss:  3.71603644e-03, bound:  3.15359622e-01\n",
      "Epoch: 30260 mean train loss:  3.71604273e-03, bound:  3.15359622e-01\n",
      "Epoch: 30261 mean train loss:  3.71596939e-03, bound:  3.15359622e-01\n",
      "Epoch: 30262 mean train loss:  3.71594285e-03, bound:  3.15359622e-01\n",
      "Epoch: 30263 mean train loss:  3.71590350e-03, bound:  3.15359622e-01\n",
      "Epoch: 30264 mean train loss:  3.71593353e-03, bound:  3.15359622e-01\n",
      "Epoch: 30265 mean train loss:  3.71587579e-03, bound:  3.15359592e-01\n",
      "Epoch: 30266 mean train loss:  3.71585856e-03, bound:  3.15359592e-01\n",
      "Epoch: 30267 mean train loss:  3.71583668e-03, bound:  3.15359563e-01\n",
      "Epoch: 30268 mean train loss:  3.71582201e-03, bound:  3.15359563e-01\n",
      "Epoch: 30269 mean train loss:  3.71582061e-03, bound:  3.15359563e-01\n",
      "Epoch: 30270 mean train loss:  3.71576147e-03, bound:  3.15359563e-01\n",
      "Epoch: 30271 mean train loss:  3.71572981e-03, bound:  3.15359563e-01\n",
      "Epoch: 30272 mean train loss:  3.71576636e-03, bound:  3.15359563e-01\n",
      "Epoch: 30273 mean train loss:  3.71570536e-03, bound:  3.15359563e-01\n",
      "Epoch: 30274 mean train loss:  3.71568883e-03, bound:  3.15359563e-01\n",
      "Epoch: 30275 mean train loss:  3.71568743e-03, bound:  3.15359563e-01\n",
      "Epoch: 30276 mean train loss:  3.71562177e-03, bound:  3.15359563e-01\n",
      "Epoch: 30277 mean train loss:  3.71560548e-03, bound:  3.15359563e-01\n",
      "Epoch: 30278 mean train loss:  3.71556450e-03, bound:  3.15359563e-01\n",
      "Epoch: 30279 mean train loss:  3.71553283e-03, bound:  3.15359563e-01\n",
      "Epoch: 30280 mean train loss:  3.71550885e-03, bound:  3.15359563e-01\n",
      "Epoch: 30281 mean train loss:  3.71553260e-03, bound:  3.15359563e-01\n",
      "Epoch: 30282 mean train loss:  3.71545018e-03, bound:  3.15359533e-01\n",
      "Epoch: 30283 mean train loss:  3.71546042e-03, bound:  3.15359533e-01\n",
      "Epoch: 30284 mean train loss:  3.71541828e-03, bound:  3.15359533e-01\n",
      "Epoch: 30285 mean train loss:  3.71537521e-03, bound:  3.15359533e-01\n",
      "Epoch: 30286 mean train loss:  3.71535635e-03, bound:  3.15359533e-01\n",
      "Epoch: 30287 mean train loss:  3.71539779e-03, bound:  3.15359533e-01\n",
      "Epoch: 30288 mean train loss:  3.71535262e-03, bound:  3.15359533e-01\n",
      "Epoch: 30289 mean train loss:  3.71529371e-03, bound:  3.15359533e-01\n",
      "Epoch: 30290 mean train loss:  3.71524156e-03, bound:  3.15359533e-01\n",
      "Epoch: 30291 mean train loss:  3.71527532e-03, bound:  3.15359533e-01\n",
      "Epoch: 30292 mean train loss:  3.71523807e-03, bound:  3.15359533e-01\n",
      "Epoch: 30293 mean train loss:  3.71522200e-03, bound:  3.15359533e-01\n",
      "Epoch: 30294 mean train loss:  3.71514703e-03, bound:  3.15359533e-01\n",
      "Epoch: 30295 mean train loss:  3.71514214e-03, bound:  3.15359503e-01\n",
      "Epoch: 30296 mean train loss:  3.71512235e-03, bound:  3.15359503e-01\n",
      "Epoch: 30297 mean train loss:  3.71512375e-03, bound:  3.15359503e-01\n",
      "Epoch: 30298 mean train loss:  3.71508044e-03, bound:  3.15359503e-01\n",
      "Epoch: 30299 mean train loss:  3.71504598e-03, bound:  3.15359503e-01\n",
      "Epoch: 30300 mean train loss:  3.71504948e-03, bound:  3.15359503e-01\n",
      "Epoch: 30301 mean train loss:  3.71501781e-03, bound:  3.15359503e-01\n",
      "Epoch: 30302 mean train loss:  3.71496892e-03, bound:  3.15359503e-01\n",
      "Epoch: 30303 mean train loss:  3.71496938e-03, bound:  3.15359503e-01\n",
      "Epoch: 30304 mean train loss:  3.71491862e-03, bound:  3.15359503e-01\n",
      "Epoch: 30305 mean train loss:  3.71494843e-03, bound:  3.15359503e-01\n",
      "Epoch: 30306 mean train loss:  3.71490815e-03, bound:  3.15359503e-01\n",
      "Epoch: 30307 mean train loss:  3.71491257e-03, bound:  3.15359503e-01\n",
      "Epoch: 30308 mean train loss:  3.71482968e-03, bound:  3.15359503e-01\n",
      "Epoch: 30309 mean train loss:  3.71479872e-03, bound:  3.15359503e-01\n",
      "Epoch: 30310 mean train loss:  3.71480081e-03, bound:  3.15359473e-01\n",
      "Epoch: 30311 mean train loss:  3.71478149e-03, bound:  3.15359473e-01\n",
      "Epoch: 30312 mean train loss:  3.71477939e-03, bound:  3.15359473e-01\n",
      "Epoch: 30313 mean train loss:  3.71476193e-03, bound:  3.15359473e-01\n",
      "Epoch: 30314 mean train loss:  3.71475774e-03, bound:  3.15359473e-01\n",
      "Epoch: 30315 mean train loss:  3.71465692e-03, bound:  3.15359473e-01\n",
      "Epoch: 30316 mean train loss:  3.71469860e-03, bound:  3.15359473e-01\n",
      "Epoch: 30317 mean train loss:  3.71463667e-03, bound:  3.15359473e-01\n",
      "Epoch: 30318 mean train loss:  3.71459406e-03, bound:  3.15359473e-01\n",
      "Epoch: 30319 mean train loss:  3.71457287e-03, bound:  3.15359473e-01\n",
      "Epoch: 30320 mean train loss:  3.71455960e-03, bound:  3.15359473e-01\n",
      "Epoch: 30321 mean train loss:  3.71453119e-03, bound:  3.15359473e-01\n",
      "Epoch: 30322 mean train loss:  3.71451979e-03, bound:  3.15359473e-01\n",
      "Epoch: 30323 mean train loss:  3.71450954e-03, bound:  3.15359473e-01\n",
      "Epoch: 30324 mean train loss:  3.71445925e-03, bound:  3.15359473e-01\n",
      "Epoch: 30325 mean train loss:  3.71442828e-03, bound:  3.15359443e-01\n",
      "Epoch: 30326 mean train loss:  3.71438777e-03, bound:  3.15359414e-01\n",
      "Epoch: 30327 mean train loss:  3.71440011e-03, bound:  3.15359414e-01\n",
      "Epoch: 30328 mean train loss:  3.71438125e-03, bound:  3.15359414e-01\n",
      "Epoch: 30329 mean train loss:  3.71433189e-03, bound:  3.15359414e-01\n",
      "Epoch: 30330 mean train loss:  3.71430512e-03, bound:  3.15359414e-01\n",
      "Epoch: 30331 mean train loss:  3.71427997e-03, bound:  3.15359414e-01\n",
      "Epoch: 30332 mean train loss:  3.71428719e-03, bound:  3.15359414e-01\n",
      "Epoch: 30333 mean train loss:  3.71425878e-03, bound:  3.15359414e-01\n",
      "Epoch: 30334 mean train loss:  3.71424551e-03, bound:  3.15359414e-01\n",
      "Epoch: 30335 mean train loss:  3.71422386e-03, bound:  3.15359414e-01\n",
      "Epoch: 30336 mean train loss:  3.71416030e-03, bound:  3.15359414e-01\n",
      "Epoch: 30337 mean train loss:  3.71412537e-03, bound:  3.15359414e-01\n",
      "Epoch: 30338 mean train loss:  3.71409138e-03, bound:  3.15359354e-01\n",
      "Epoch: 30339 mean train loss:  3.71409836e-03, bound:  3.15359414e-01\n",
      "Epoch: 30340 mean train loss:  3.71407950e-03, bound:  3.15359354e-01\n",
      "Epoch: 30341 mean train loss:  3.71408369e-03, bound:  3.15359354e-01\n",
      "Epoch: 30342 mean train loss:  3.71402642e-03, bound:  3.15359354e-01\n",
      "Epoch: 30343 mean train loss:  3.71398078e-03, bound:  3.15359354e-01\n",
      "Epoch: 30344 mean train loss:  3.71398055e-03, bound:  3.15359354e-01\n",
      "Epoch: 30345 mean train loss:  3.71397473e-03, bound:  3.15359354e-01\n",
      "Epoch: 30346 mean train loss:  3.71395075e-03, bound:  3.15359354e-01\n",
      "Epoch: 30347 mean train loss:  3.71392234e-03, bound:  3.15359354e-01\n",
      "Epoch: 30348 mean train loss:  3.71387461e-03, bound:  3.15359354e-01\n",
      "Epoch: 30349 mean train loss:  3.71387927e-03, bound:  3.15359354e-01\n",
      "Epoch: 30350 mean train loss:  3.71383037e-03, bound:  3.15359354e-01\n",
      "Epoch: 30351 mean train loss:  3.71379615e-03, bound:  3.15359354e-01\n",
      "Epoch: 30352 mean train loss:  3.71378311e-03, bound:  3.15359354e-01\n",
      "Epoch: 30353 mean train loss:  3.71377333e-03, bound:  3.15359354e-01\n",
      "Epoch: 30354 mean train loss:  3.71376262e-03, bound:  3.15359354e-01\n",
      "Epoch: 30355 mean train loss:  3.71373631e-03, bound:  3.15359354e-01\n",
      "Epoch: 30356 mean train loss:  3.71370860e-03, bound:  3.15359354e-01\n",
      "Epoch: 30357 mean train loss:  3.71366017e-03, bound:  3.15359354e-01\n",
      "Epoch: 30358 mean train loss:  3.71364411e-03, bound:  3.15359354e-01\n",
      "Epoch: 30359 mean train loss:  3.71368206e-03, bound:  3.15359354e-01\n",
      "Epoch: 30360 mean train loss:  3.71357566e-03, bound:  3.15359354e-01\n",
      "Epoch: 30361 mean train loss:  3.71355377e-03, bound:  3.15359354e-01\n",
      "Epoch: 30362 mean train loss:  3.71358451e-03, bound:  3.15359354e-01\n",
      "Epoch: 30363 mean train loss:  3.71352420e-03, bound:  3.15359354e-01\n",
      "Epoch: 30364 mean train loss:  3.71346879e-03, bound:  3.15359354e-01\n",
      "Epoch: 30365 mean train loss:  3.71344830e-03, bound:  3.15359354e-01\n",
      "Epoch: 30366 mean train loss:  3.71346856e-03, bound:  3.15359324e-01\n",
      "Epoch: 30367 mean train loss:  3.71340546e-03, bound:  3.15359354e-01\n",
      "Epoch: 30368 mean train loss:  3.71336448e-03, bound:  3.15359324e-01\n",
      "Epoch: 30369 mean train loss:  3.71337775e-03, bound:  3.15359324e-01\n",
      "Epoch: 30370 mean train loss:  3.71334562e-03, bound:  3.15359324e-01\n",
      "Epoch: 30371 mean train loss:  3.71336122e-03, bound:  3.15359324e-01\n",
      "Epoch: 30372 mean train loss:  3.71325016e-03, bound:  3.15359324e-01\n",
      "Epoch: 30373 mean train loss:  3.71324411e-03, bound:  3.15359324e-01\n",
      "Epoch: 30374 mean train loss:  3.71323666e-03, bound:  3.15359324e-01\n",
      "Epoch: 30375 mean train loss:  3.71320802e-03, bound:  3.15359324e-01\n",
      "Epoch: 30376 mean train loss:  3.71317286e-03, bound:  3.15359324e-01\n",
      "Epoch: 30377 mean train loss:  3.71319591e-03, bound:  3.15359324e-01\n",
      "Epoch: 30378 mean train loss:  3.71312699e-03, bound:  3.15359324e-01\n",
      "Epoch: 30379 mean train loss:  3.71312839e-03, bound:  3.15359324e-01\n",
      "Epoch: 30380 mean train loss:  3.71308695e-03, bound:  3.15359324e-01\n",
      "Epoch: 30381 mean train loss:  3.71308136e-03, bound:  3.15359324e-01\n",
      "Epoch: 30382 mean train loss:  3.71305295e-03, bound:  3.15359294e-01\n",
      "Epoch: 30383 mean train loss:  3.71300266e-03, bound:  3.15359235e-01\n",
      "Epoch: 30384 mean train loss:  3.71296215e-03, bound:  3.15359235e-01\n",
      "Epoch: 30385 mean train loss:  3.71296378e-03, bound:  3.15359235e-01\n",
      "Epoch: 30386 mean train loss:  3.71293118e-03, bound:  3.15359235e-01\n",
      "Epoch: 30387 mean train loss:  3.71285970e-03, bound:  3.15359235e-01\n",
      "Epoch: 30388 mean train loss:  3.71288904e-03, bound:  3.15359235e-01\n",
      "Epoch: 30389 mean train loss:  3.71285621e-03, bound:  3.15359235e-01\n",
      "Epoch: 30390 mean train loss:  3.71286273e-03, bound:  3.15359235e-01\n",
      "Epoch: 30391 mean train loss:  3.71282292e-03, bound:  3.15359235e-01\n",
      "Epoch: 30392 mean train loss:  3.71279893e-03, bound:  3.15359235e-01\n",
      "Epoch: 30393 mean train loss:  3.71277705e-03, bound:  3.15359235e-01\n",
      "Epoch: 30394 mean train loss:  3.71273328e-03, bound:  3.15359235e-01\n",
      "Epoch: 30395 mean train loss:  3.71269905e-03, bound:  3.15359235e-01\n",
      "Epoch: 30396 mean train loss:  3.71264666e-03, bound:  3.15359235e-01\n",
      "Epoch: 30397 mean train loss:  3.71267693e-03, bound:  3.15359235e-01\n",
      "Epoch: 30398 mean train loss:  3.71260336e-03, bound:  3.15359235e-01\n",
      "Epoch: 30399 mean train loss:  3.71263828e-03, bound:  3.15359235e-01\n",
      "Epoch: 30400 mean train loss:  3.71259404e-03, bound:  3.15359235e-01\n",
      "Epoch: 30401 mean train loss:  3.71251232e-03, bound:  3.15359235e-01\n",
      "Epoch: 30402 mean train loss:  3.71251558e-03, bound:  3.15359235e-01\n",
      "Epoch: 30403 mean train loss:  3.71247716e-03, bound:  3.15359235e-01\n",
      "Epoch: 30404 mean train loss:  3.71251092e-03, bound:  3.15359235e-01\n",
      "Epoch: 30405 mean train loss:  3.71248950e-03, bound:  3.15359235e-01\n",
      "Epoch: 30406 mean train loss:  3.71242804e-03, bound:  3.15359235e-01\n",
      "Epoch: 30407 mean train loss:  3.71242082e-03, bound:  3.15359235e-01\n",
      "Epoch: 30408 mean train loss:  3.71242245e-03, bound:  3.15359235e-01\n",
      "Epoch: 30409 mean train loss:  3.71233700e-03, bound:  3.15359205e-01\n",
      "Epoch: 30410 mean train loss:  3.71236703e-03, bound:  3.15359205e-01\n",
      "Epoch: 30411 mean train loss:  3.71232023e-03, bound:  3.15359205e-01\n",
      "Epoch: 30412 mean train loss:  3.71226133e-03, bound:  3.15359205e-01\n",
      "Epoch: 30413 mean train loss:  3.71223944e-03, bound:  3.15359205e-01\n",
      "Epoch: 30414 mean train loss:  3.71220778e-03, bound:  3.15359205e-01\n",
      "Epoch: 30415 mean train loss:  3.71224759e-03, bound:  3.15359205e-01\n",
      "Epoch: 30416 mean train loss:  3.71221406e-03, bound:  3.15359205e-01\n",
      "Epoch: 30417 mean train loss:  3.71217239e-03, bound:  3.15359205e-01\n",
      "Epoch: 30418 mean train loss:  3.71214072e-03, bound:  3.15359205e-01\n",
      "Epoch: 30419 mean train loss:  3.71208671e-03, bound:  3.15359205e-01\n",
      "Epoch: 30420 mean train loss:  3.71211465e-03, bound:  3.15359205e-01\n",
      "Epoch: 30421 mean train loss:  3.71206109e-03, bound:  3.15359205e-01\n",
      "Epoch: 30422 mean train loss:  3.71205783e-03, bound:  3.15359205e-01\n",
      "Epoch: 30423 mean train loss:  3.71200754e-03, bound:  3.15359175e-01\n",
      "Epoch: 30424 mean train loss:  3.71200871e-03, bound:  3.15359175e-01\n",
      "Epoch: 30425 mean train loss:  3.71195003e-03, bound:  3.15359175e-01\n",
      "Epoch: 30426 mean train loss:  3.71193117e-03, bound:  3.15359175e-01\n",
      "Epoch: 30427 mean train loss:  3.71192512e-03, bound:  3.15359175e-01\n",
      "Epoch: 30428 mean train loss:  3.71188042e-03, bound:  3.15359175e-01\n",
      "Epoch: 30429 mean train loss:  3.71182477e-03, bound:  3.15359175e-01\n",
      "Epoch: 30430 mean train loss:  3.71181499e-03, bound:  3.15359175e-01\n",
      "Epoch: 30431 mean train loss:  3.71178356e-03, bound:  3.15359175e-01\n",
      "Epoch: 30432 mean train loss:  3.71175003e-03, bound:  3.15359175e-01\n",
      "Epoch: 30433 mean train loss:  3.71176726e-03, bound:  3.15359175e-01\n",
      "Epoch: 30434 mean train loss:  3.71175725e-03, bound:  3.15359175e-01\n",
      "Epoch: 30435 mean train loss:  3.71165876e-03, bound:  3.15359175e-01\n",
      "Epoch: 30436 mean train loss:  3.71169555e-03, bound:  3.15359175e-01\n",
      "Epoch: 30437 mean train loss:  3.71161872e-03, bound:  3.15359175e-01\n",
      "Epoch: 30438 mean train loss:  3.71161941e-03, bound:  3.15359175e-01\n",
      "Epoch: 30439 mean train loss:  3.71161615e-03, bound:  3.15359116e-01\n",
      "Epoch: 30440 mean train loss:  3.71155841e-03, bound:  3.15359116e-01\n",
      "Epoch: 30441 mean train loss:  3.71153001e-03, bound:  3.15359116e-01\n",
      "Epoch: 30442 mean train loss:  3.71153490e-03, bound:  3.15359116e-01\n",
      "Epoch: 30443 mean train loss:  3.71148135e-03, bound:  3.15359116e-01\n",
      "Epoch: 30444 mean train loss:  3.71146528e-03, bound:  3.15359116e-01\n",
      "Epoch: 30445 mean train loss:  3.71142733e-03, bound:  3.15359116e-01\n",
      "Epoch: 30446 mean train loss:  3.71142197e-03, bound:  3.15359116e-01\n",
      "Epoch: 30447 mean train loss:  3.71141848e-03, bound:  3.15359116e-01\n",
      "Epoch: 30448 mean train loss:  3.71138426e-03, bound:  3.15359116e-01\n",
      "Epoch: 30449 mean train loss:  3.71136237e-03, bound:  3.15359086e-01\n",
      "Epoch: 30450 mean train loss:  3.71133164e-03, bound:  3.15359086e-01\n",
      "Epoch: 30451 mean train loss:  3.71126691e-03, bound:  3.15359086e-01\n",
      "Epoch: 30452 mean train loss:  3.71127389e-03, bound:  3.15359086e-01\n",
      "Epoch: 30453 mean train loss:  3.71126388e-03, bound:  3.15359086e-01\n",
      "Epoch: 30454 mean train loss:  3.71122826e-03, bound:  3.15359086e-01\n",
      "Epoch: 30455 mean train loss:  3.71116097e-03, bound:  3.15359086e-01\n",
      "Epoch: 30456 mean train loss:  3.71119124e-03, bound:  3.15359086e-01\n",
      "Epoch: 30457 mean train loss:  3.71113606e-03, bound:  3.15359086e-01\n",
      "Epoch: 30458 mean train loss:  3.71111277e-03, bound:  3.15359086e-01\n",
      "Epoch: 30459 mean train loss:  3.71105736e-03, bound:  3.15359086e-01\n",
      "Epoch: 30460 mean train loss:  3.71102290e-03, bound:  3.15359086e-01\n",
      "Epoch: 30461 mean train loss:  3.71103175e-03, bound:  3.15359086e-01\n",
      "Epoch: 30462 mean train loss:  3.71097401e-03, bound:  3.15359086e-01\n",
      "Epoch: 30463 mean train loss:  3.71098588e-03, bound:  3.15359086e-01\n",
      "Epoch: 30464 mean train loss:  3.71093350e-03, bound:  3.15359056e-01\n",
      "Epoch: 30465 mean train loss:  3.71093256e-03, bound:  3.15359056e-01\n",
      "Epoch: 30466 mean train loss:  3.71093373e-03, bound:  3.15359056e-01\n",
      "Epoch: 30467 mean train loss:  3.71090975e-03, bound:  3.15359056e-01\n",
      "Epoch: 30468 mean train loss:  3.71088739e-03, bound:  3.15359056e-01\n",
      "Epoch: 30469 mean train loss:  3.71080451e-03, bound:  3.15359056e-01\n",
      "Epoch: 30470 mean train loss:  3.71079776e-03, bound:  3.15359056e-01\n",
      "Epoch: 30471 mean train loss:  3.71075328e-03, bound:  3.15359056e-01\n",
      "Epoch: 30472 mean train loss:  3.71075701e-03, bound:  3.15359056e-01\n",
      "Epoch: 30473 mean train loss:  3.71075305e-03, bound:  3.15359056e-01\n",
      "Epoch: 30474 mean train loss:  3.71073955e-03, bound:  3.15359056e-01\n",
      "Epoch: 30475 mean train loss:  3.71066062e-03, bound:  3.15359026e-01\n",
      "Epoch: 30476 mean train loss:  3.71063990e-03, bound:  3.15358996e-01\n",
      "Epoch: 30477 mean train loss:  3.71065480e-03, bound:  3.15358996e-01\n",
      "Epoch: 30478 mean train loss:  3.71061219e-03, bound:  3.15358996e-01\n",
      "Epoch: 30479 mean train loss:  3.71058797e-03, bound:  3.15358996e-01\n",
      "Epoch: 30480 mean train loss:  3.71054094e-03, bound:  3.15358996e-01\n",
      "Epoch: 30481 mean train loss:  3.71050043e-03, bound:  3.15358996e-01\n",
      "Epoch: 30482 mean train loss:  3.71045456e-03, bound:  3.15358996e-01\n",
      "Epoch: 30483 mean train loss:  3.71048064e-03, bound:  3.15358996e-01\n",
      "Epoch: 30484 mean train loss:  3.71043780e-03, bound:  3.15358996e-01\n",
      "Epoch: 30485 mean train loss:  3.71038378e-03, bound:  3.15358996e-01\n",
      "Epoch: 30486 mean train loss:  3.71036748e-03, bound:  3.15358996e-01\n",
      "Epoch: 30487 mean train loss:  3.71037237e-03, bound:  3.15358996e-01\n",
      "Epoch: 30488 mean train loss:  3.71034257e-03, bound:  3.15358996e-01\n",
      "Epoch: 30489 mean train loss:  3.71031836e-03, bound:  3.15358996e-01\n",
      "Epoch: 30490 mean train loss:  3.71033326e-03, bound:  3.15358967e-01\n",
      "Epoch: 30491 mean train loss:  3.71025875e-03, bound:  3.15358967e-01\n",
      "Epoch: 30492 mean train loss:  3.71019403e-03, bound:  3.15358967e-01\n",
      "Epoch: 30493 mean train loss:  3.71015095e-03, bound:  3.15358967e-01\n",
      "Epoch: 30494 mean train loss:  3.71017237e-03, bound:  3.15358967e-01\n",
      "Epoch: 30495 mean train loss:  3.71013093e-03, bound:  3.15358967e-01\n",
      "Epoch: 30496 mean train loss:  3.71010811e-03, bound:  3.15358967e-01\n",
      "Epoch: 30497 mean train loss:  3.71008320e-03, bound:  3.15358967e-01\n",
      "Epoch: 30498 mean train loss:  3.71007388e-03, bound:  3.15358967e-01\n",
      "Epoch: 30499 mean train loss:  3.71004175e-03, bound:  3.15358967e-01\n",
      "Epoch: 30500 mean train loss:  3.71002615e-03, bound:  3.15358967e-01\n",
      "Epoch: 30501 mean train loss:  3.70999007e-03, bound:  3.15358967e-01\n",
      "Epoch: 30502 mean train loss:  3.70997679e-03, bound:  3.15358907e-01\n",
      "Epoch: 30503 mean train loss:  3.70991789e-03, bound:  3.15358907e-01\n",
      "Epoch: 30504 mean train loss:  3.70991300e-03, bound:  3.15358907e-01\n",
      "Epoch: 30505 mean train loss:  3.70986969e-03, bound:  3.15358907e-01\n",
      "Epoch: 30506 mean train loss:  3.70986806e-03, bound:  3.15358907e-01\n",
      "Epoch: 30507 mean train loss:  3.70978750e-03, bound:  3.15358907e-01\n",
      "Epoch: 30508 mean train loss:  3.70979588e-03, bound:  3.15358907e-01\n",
      "Epoch: 30509 mean train loss:  3.70982895e-03, bound:  3.15358907e-01\n",
      "Epoch: 30510 mean train loss:  3.70978052e-03, bound:  3.15358907e-01\n",
      "Epoch: 30511 mean train loss:  3.70977051e-03, bound:  3.15358907e-01\n",
      "Epoch: 30512 mean train loss:  3.70973465e-03, bound:  3.15358907e-01\n",
      "Epoch: 30513 mean train loss:  3.70965782e-03, bound:  3.15358907e-01\n",
      "Epoch: 30514 mean train loss:  3.70962801e-03, bound:  3.15358907e-01\n",
      "Epoch: 30515 mean train loss:  3.70960217e-03, bound:  3.15358907e-01\n",
      "Epoch: 30516 mean train loss:  3.70961917e-03, bound:  3.15358877e-01\n",
      "Epoch: 30517 mean train loss:  3.70954070e-03, bound:  3.15358877e-01\n",
      "Epoch: 30518 mean train loss:  3.70951346e-03, bound:  3.15358877e-01\n",
      "Epoch: 30519 mean train loss:  3.70954070e-03, bound:  3.15358877e-01\n",
      "Epoch: 30520 mean train loss:  3.70951602e-03, bound:  3.15358877e-01\n",
      "Epoch: 30521 mean train loss:  3.70941660e-03, bound:  3.15358877e-01\n",
      "Epoch: 30522 mean train loss:  3.70943244e-03, bound:  3.15358877e-01\n",
      "Epoch: 30523 mean train loss:  3.70940613e-03, bound:  3.15358877e-01\n",
      "Epoch: 30524 mean train loss:  3.70940100e-03, bound:  3.15358877e-01\n",
      "Epoch: 30525 mean train loss:  3.70933488e-03, bound:  3.15358877e-01\n",
      "Epoch: 30526 mean train loss:  3.70930787e-03, bound:  3.15358877e-01\n",
      "Epoch: 30527 mean train loss:  3.70931253e-03, bound:  3.15358877e-01\n",
      "Epoch: 30528 mean train loss:  3.70930298e-03, bound:  3.15358847e-01\n",
      "Epoch: 30529 mean train loss:  3.70922987e-03, bound:  3.15358847e-01\n",
      "Epoch: 30530 mean train loss:  3.70923430e-03, bound:  3.15358847e-01\n",
      "Epoch: 30531 mean train loss:  3.70921218e-03, bound:  3.15358847e-01\n",
      "Epoch: 30532 mean train loss:  3.70916212e-03, bound:  3.15358847e-01\n",
      "Epoch: 30533 mean train loss:  3.70914047e-03, bound:  3.15358847e-01\n",
      "Epoch: 30534 mean train loss:  3.70910508e-03, bound:  3.15358847e-01\n",
      "Epoch: 30535 mean train loss:  3.70908529e-03, bound:  3.15358847e-01\n",
      "Epoch: 30536 mean train loss:  3.70911788e-03, bound:  3.15358847e-01\n",
      "Epoch: 30537 mean train loss:  3.70905246e-03, bound:  3.15358847e-01\n",
      "Epoch: 30538 mean train loss:  3.70902754e-03, bound:  3.15358847e-01\n",
      "Epoch: 30539 mean train loss:  3.70896491e-03, bound:  3.15358847e-01\n",
      "Epoch: 30540 mean train loss:  3.70895769e-03, bound:  3.15358847e-01\n",
      "Epoch: 30541 mean train loss:  3.70892812e-03, bound:  3.15358847e-01\n",
      "Epoch: 30542 mean train loss:  3.70894419e-03, bound:  3.15358847e-01\n",
      "Epoch: 30543 mean train loss:  3.70887737e-03, bound:  3.15358788e-01\n",
      "Epoch: 30544 mean train loss:  3.70890205e-03, bound:  3.15358788e-01\n",
      "Epoch: 30545 mean train loss:  3.70885758e-03, bound:  3.15358788e-01\n",
      "Epoch: 30546 mean train loss:  3.70880077e-03, bound:  3.15358788e-01\n",
      "Epoch: 30547 mean train loss:  3.70875979e-03, bound:  3.15358788e-01\n",
      "Epoch: 30548 mean train loss:  3.70879052e-03, bound:  3.15358788e-01\n",
      "Epoch: 30549 mean train loss:  3.70873441e-03, bound:  3.15358788e-01\n",
      "Epoch: 30550 mean train loss:  3.70869203e-03, bound:  3.15358788e-01\n",
      "Epoch: 30551 mean train loss:  3.70867690e-03, bound:  3.15358788e-01\n",
      "Epoch: 30552 mean train loss:  3.70865082e-03, bound:  3.15358788e-01\n",
      "Epoch: 30553 mean train loss:  3.70862894e-03, bound:  3.15358788e-01\n",
      "Epoch: 30554 mean train loss:  3.70860193e-03, bound:  3.15358758e-01\n",
      "Epoch: 30555 mean train loss:  3.70856933e-03, bound:  3.15358758e-01\n",
      "Epoch: 30556 mean train loss:  3.70858144e-03, bound:  3.15358758e-01\n",
      "Epoch: 30557 mean train loss:  3.70847830e-03, bound:  3.15358758e-01\n",
      "Epoch: 30558 mean train loss:  3.70846665e-03, bound:  3.15358758e-01\n",
      "Epoch: 30559 mean train loss:  3.70839192e-03, bound:  3.15358758e-01\n",
      "Epoch: 30560 mean train loss:  3.70846665e-03, bound:  3.15358758e-01\n",
      "Epoch: 30561 mean train loss:  3.70839960e-03, bound:  3.15358758e-01\n",
      "Epoch: 30562 mean train loss:  3.70834139e-03, bound:  3.15358758e-01\n",
      "Epoch: 30563 mean train loss:  3.70840216e-03, bound:  3.15358758e-01\n",
      "Epoch: 30564 mean train loss:  3.70837306e-03, bound:  3.15358758e-01\n",
      "Epoch: 30565 mean train loss:  3.70827084e-03, bound:  3.15358758e-01\n",
      "Epoch: 30566 mean train loss:  3.70828179e-03, bound:  3.15358758e-01\n",
      "Epoch: 30567 mean train loss:  3.70822125e-03, bound:  3.15358758e-01\n",
      "Epoch: 30568 mean train loss:  3.70820053e-03, bound:  3.15358728e-01\n",
      "Epoch: 30569 mean train loss:  3.70813604e-03, bound:  3.15358728e-01\n",
      "Epoch: 30570 mean train loss:  3.70817818e-03, bound:  3.15358728e-01\n",
      "Epoch: 30571 mean train loss:  3.70813510e-03, bound:  3.15358728e-01\n",
      "Epoch: 30572 mean train loss:  3.70816118e-03, bound:  3.15358728e-01\n",
      "Epoch: 30573 mean train loss:  3.70811066e-03, bound:  3.15358728e-01\n",
      "Epoch: 30574 mean train loss:  3.70804919e-03, bound:  3.15358728e-01\n",
      "Epoch: 30575 mean train loss:  3.70804570e-03, bound:  3.15358728e-01\n",
      "Epoch: 30576 mean train loss:  3.70801450e-03, bound:  3.15358728e-01\n",
      "Epoch: 30577 mean train loss:  3.70798865e-03, bound:  3.15358728e-01\n",
      "Epoch: 30578 mean train loss:  3.70793650e-03, bound:  3.15358728e-01\n",
      "Epoch: 30579 mean train loss:  3.70789552e-03, bound:  3.15358728e-01\n",
      "Epoch: 30580 mean train loss:  3.70792323e-03, bound:  3.15358728e-01\n",
      "Epoch: 30581 mean train loss:  3.70784639e-03, bound:  3.15358728e-01\n",
      "Epoch: 30582 mean train loss:  3.70783638e-03, bound:  3.15358728e-01\n",
      "Epoch: 30583 mean train loss:  3.70787876e-03, bound:  3.15358669e-01\n",
      "Epoch: 30584 mean train loss:  3.70781496e-03, bound:  3.15358669e-01\n",
      "Epoch: 30585 mean train loss:  3.70782404e-03, bound:  3.15358669e-01\n",
      "Epoch: 30586 mean train loss:  3.70775349e-03, bound:  3.15358669e-01\n",
      "Epoch: 30587 mean train loss:  3.70770460e-03, bound:  3.15358669e-01\n",
      "Epoch: 30588 mean train loss:  3.70771275e-03, bound:  3.15358669e-01\n",
      "Epoch: 30589 mean train loss:  3.70767014e-03, bound:  3.15358669e-01\n",
      "Epoch: 30590 mean train loss:  3.70759494e-03, bound:  3.15358669e-01\n",
      "Epoch: 30591 mean train loss:  3.70758050e-03, bound:  3.15358669e-01\n",
      "Epoch: 30592 mean train loss:  3.70755699e-03, bound:  3.15358669e-01\n",
      "Epoch: 30593 mean train loss:  3.70750600e-03, bound:  3.15358669e-01\n",
      "Epoch: 30594 mean train loss:  3.70752066e-03, bound:  3.15358669e-01\n",
      "Epoch: 30595 mean train loss:  3.70752160e-03, bound:  3.15358669e-01\n",
      "Epoch: 30596 mean train loss:  3.70747223e-03, bound:  3.15358669e-01\n",
      "Epoch: 30597 mean train loss:  3.70744150e-03, bound:  3.15358669e-01\n",
      "Epoch: 30598 mean train loss:  3.70743196e-03, bound:  3.15358669e-01\n",
      "Epoch: 30599 mean train loss:  3.70739913e-03, bound:  3.15358669e-01\n",
      "Epoch: 30600 mean train loss:  3.70739959e-03, bound:  3.15358669e-01\n",
      "Epoch: 30601 mean train loss:  3.70731344e-03, bound:  3.15358669e-01\n",
      "Epoch: 30602 mean train loss:  3.70729621e-03, bound:  3.15358669e-01\n",
      "Epoch: 30603 mean train loss:  3.70725663e-03, bound:  3.15358669e-01\n",
      "Epoch: 30604 mean train loss:  3.70722543e-03, bound:  3.15358669e-01\n",
      "Epoch: 30605 mean train loss:  3.70724523e-03, bound:  3.15358669e-01\n",
      "Epoch: 30606 mean train loss:  3.70717188e-03, bound:  3.15358669e-01\n",
      "Epoch: 30607 mean train loss:  3.70716001e-03, bound:  3.15358669e-01\n",
      "Epoch: 30608 mean train loss:  3.70717631e-03, bound:  3.15358639e-01\n",
      "Epoch: 30609 mean train loss:  3.70710343e-03, bound:  3.15358639e-01\n",
      "Epoch: 30610 mean train loss:  3.70704988e-03, bound:  3.15358639e-01\n",
      "Epoch: 30611 mean train loss:  3.70705943e-03, bound:  3.15358639e-01\n",
      "Epoch: 30612 mean train loss:  3.70706129e-03, bound:  3.15358639e-01\n",
      "Epoch: 30613 mean train loss:  3.70704220e-03, bound:  3.15358639e-01\n",
      "Epoch: 30614 mean train loss:  3.70696909e-03, bound:  3.15358639e-01\n",
      "Epoch: 30615 mean train loss:  3.70694348e-03, bound:  3.15358639e-01\n",
      "Epoch: 30616 mean train loss:  3.70691135e-03, bound:  3.15358639e-01\n",
      "Epoch: 30617 mean train loss:  3.70689761e-03, bound:  3.15358639e-01\n",
      "Epoch: 30618 mean train loss:  3.70687735e-03, bound:  3.15358639e-01\n",
      "Epoch: 30619 mean train loss:  3.70679377e-03, bound:  3.15358639e-01\n",
      "Epoch: 30620 mean train loss:  3.70684429e-03, bound:  3.15358639e-01\n",
      "Epoch: 30621 mean train loss:  3.70679027e-03, bound:  3.15358639e-01\n",
      "Epoch: 30622 mean train loss:  3.70673370e-03, bound:  3.15358639e-01\n",
      "Epoch: 30623 mean train loss:  3.70672555e-03, bound:  3.15358639e-01\n",
      "Epoch: 30624 mean train loss:  3.70670529e-03, bound:  3.15358609e-01\n",
      "Epoch: 30625 mean train loss:  3.70667852e-03, bound:  3.15358549e-01\n",
      "Epoch: 30626 mean train loss:  3.70668015e-03, bound:  3.15358609e-01\n",
      "Epoch: 30627 mean train loss:  3.70660634e-03, bound:  3.15358549e-01\n",
      "Epoch: 30628 mean train loss:  3.70659959e-03, bound:  3.15358549e-01\n",
      "Epoch: 30629 mean train loss:  3.70655209e-03, bound:  3.15358549e-01\n",
      "Epoch: 30630 mean train loss:  3.70658725e-03, bound:  3.15358549e-01\n",
      "Epoch: 30631 mean train loss:  3.70651018e-03, bound:  3.15358549e-01\n",
      "Epoch: 30632 mean train loss:  3.70653602e-03, bound:  3.15358549e-01\n",
      "Epoch: 30633 mean train loss:  3.70647735e-03, bound:  3.15358549e-01\n",
      "Epoch: 30634 mean train loss:  3.70645360e-03, bound:  3.15358549e-01\n",
      "Epoch: 30635 mean train loss:  3.70641425e-03, bound:  3.15358549e-01\n",
      "Epoch: 30636 mean train loss:  3.70640866e-03, bound:  3.15358549e-01\n",
      "Epoch: 30637 mean train loss:  3.70634394e-03, bound:  3.15358549e-01\n",
      "Epoch: 30638 mean train loss:  3.70634790e-03, bound:  3.15358549e-01\n",
      "Epoch: 30639 mean train loss:  3.70629481e-03, bound:  3.15358549e-01\n",
      "Epoch: 30640 mean train loss:  3.70626594e-03, bound:  3.15358549e-01\n",
      "Epoch: 30641 mean train loss:  3.70623334e-03, bound:  3.15358549e-01\n",
      "Epoch: 30642 mean train loss:  3.70620191e-03, bound:  3.15358549e-01\n",
      "Epoch: 30643 mean train loss:  3.70612391e-03, bound:  3.15358549e-01\n",
      "Epoch: 30644 mean train loss:  3.70617094e-03, bound:  3.15358549e-01\n",
      "Epoch: 30645 mean train loss:  3.70613392e-03, bound:  3.15358549e-01\n",
      "Epoch: 30646 mean train loss:  3.70607642e-03, bound:  3.15358549e-01\n",
      "Epoch: 30647 mean train loss:  3.70610831e-03, bound:  3.15358549e-01\n",
      "Epoch: 30648 mean train loss:  3.70604056e-03, bound:  3.15358520e-01\n",
      "Epoch: 30649 mean train loss:  3.70601099e-03, bound:  3.15358520e-01\n",
      "Epoch: 30650 mean train loss:  3.70600820e-03, bound:  3.15358520e-01\n",
      "Epoch: 30651 mean train loss:  3.70600284e-03, bound:  3.15358520e-01\n",
      "Epoch: 30652 mean train loss:  3.70593951e-03, bound:  3.15358520e-01\n",
      "Epoch: 30653 mean train loss:  3.70591739e-03, bound:  3.15358520e-01\n",
      "Epoch: 30654 mean train loss:  3.70593905e-03, bound:  3.15358520e-01\n",
      "Epoch: 30655 mean train loss:  3.70586291e-03, bound:  3.15358520e-01\n",
      "Epoch: 30656 mean train loss:  3.70582193e-03, bound:  3.15358520e-01\n",
      "Epoch: 30657 mean train loss:  3.70574347e-03, bound:  3.15358520e-01\n",
      "Epoch: 30658 mean train loss:  3.70579190e-03, bound:  3.15358520e-01\n",
      "Epoch: 30659 mean train loss:  3.70576093e-03, bound:  3.15358520e-01\n",
      "Epoch: 30660 mean train loss:  3.70574649e-03, bound:  3.15358520e-01\n",
      "Epoch: 30661 mean train loss:  3.70565359e-03, bound:  3.15358430e-01\n",
      "Epoch: 30662 mean train loss:  3.70560377e-03, bound:  3.15358430e-01\n",
      "Epoch: 30663 mean train loss:  3.70564335e-03, bound:  3.15358430e-01\n",
      "Epoch: 30664 mean train loss:  3.70560284e-03, bound:  3.15358430e-01\n",
      "Epoch: 30665 mean train loss:  3.70556605e-03, bound:  3.15358430e-01\n",
      "Epoch: 30666 mean train loss:  3.70555930e-03, bound:  3.15358430e-01\n",
      "Epoch: 30667 mean train loss:  3.70553136e-03, bound:  3.15358430e-01\n",
      "Epoch: 30668 mean train loss:  3.70545941e-03, bound:  3.15358430e-01\n",
      "Epoch: 30669 mean train loss:  3.70544498e-03, bound:  3.15358430e-01\n",
      "Epoch: 30670 mean train loss:  3.70542984e-03, bound:  3.15358430e-01\n",
      "Epoch: 30671 mean train loss:  3.70543730e-03, bound:  3.15358430e-01\n",
      "Epoch: 30672 mean train loss:  3.70536442e-03, bound:  3.15358430e-01\n",
      "Epoch: 30673 mean train loss:  3.70530668e-03, bound:  3.15358430e-01\n",
      "Epoch: 30674 mean train loss:  3.70532647e-03, bound:  3.15358430e-01\n",
      "Epoch: 30675 mean train loss:  3.70532135e-03, bound:  3.15358430e-01\n",
      "Epoch: 30676 mean train loss:  3.70524847e-03, bound:  3.15358430e-01\n",
      "Epoch: 30677 mean train loss:  3.70522868e-03, bound:  3.15358430e-01\n",
      "Epoch: 30678 mean train loss:  3.70521052e-03, bound:  3.15358430e-01\n",
      "Epoch: 30679 mean train loss:  3.70521005e-03, bound:  3.15358430e-01\n",
      "Epoch: 30680 mean train loss:  3.70515650e-03, bound:  3.15358430e-01\n",
      "Epoch: 30681 mean train loss:  3.70515161e-03, bound:  3.15358430e-01\n",
      "Epoch: 30682 mean train loss:  3.70505801e-03, bound:  3.15358430e-01\n",
      "Epoch: 30683 mean train loss:  3.70507571e-03, bound:  3.15358430e-01\n",
      "Epoch: 30684 mean train loss:  3.70502588e-03, bound:  3.15358430e-01\n",
      "Epoch: 30685 mean train loss:  3.70500144e-03, bound:  3.15358430e-01\n",
      "Epoch: 30686 mean train loss:  3.70496395e-03, bound:  3.15358400e-01\n",
      "Epoch: 30687 mean train loss:  3.70497489e-03, bound:  3.15358400e-01\n",
      "Epoch: 30688 mean train loss:  3.70493461e-03, bound:  3.15358400e-01\n",
      "Epoch: 30689 mean train loss:  3.70487059e-03, bound:  3.15358400e-01\n",
      "Epoch: 30690 mean train loss:  3.70486965e-03, bound:  3.15358400e-01\n",
      "Epoch: 30691 mean train loss:  3.70484637e-03, bound:  3.15358400e-01\n",
      "Epoch: 30692 mean train loss:  3.70481168e-03, bound:  3.15358400e-01\n",
      "Epoch: 30693 mean train loss:  3.70477349e-03, bound:  3.15358400e-01\n",
      "Epoch: 30694 mean train loss:  3.70477093e-03, bound:  3.15358400e-01\n",
      "Epoch: 30695 mean train loss:  3.70473973e-03, bound:  3.15358400e-01\n",
      "Epoch: 30696 mean train loss:  3.70468199e-03, bound:  3.15358400e-01\n",
      "Epoch: 30697 mean train loss:  3.70467175e-03, bound:  3.15358400e-01\n",
      "Epoch: 30698 mean train loss:  3.70464660e-03, bound:  3.15358400e-01\n",
      "Epoch: 30699 mean train loss:  3.70463356e-03, bound:  3.15358400e-01\n",
      "Epoch: 30700 mean train loss:  3.70459701e-03, bound:  3.15358400e-01\n",
      "Epoch: 30701 mean train loss:  3.70460353e-03, bound:  3.15358341e-01\n",
      "Epoch: 30702 mean train loss:  3.70453112e-03, bound:  3.15358341e-01\n",
      "Epoch: 30703 mean train loss:  3.70451366e-03, bound:  3.15358341e-01\n",
      "Epoch: 30704 mean train loss:  3.70446919e-03, bound:  3.15358341e-01\n",
      "Epoch: 30705 mean train loss:  3.70447198e-03, bound:  3.15358341e-01\n",
      "Epoch: 30706 mean train loss:  3.70444707e-03, bound:  3.15358341e-01\n",
      "Epoch: 30707 mean train loss:  3.70439980e-03, bound:  3.15358341e-01\n",
      "Epoch: 30708 mean train loss:  3.70437303e-03, bound:  3.15358341e-01\n",
      "Epoch: 30709 mean train loss:  3.70434765e-03, bound:  3.15358341e-01\n",
      "Epoch: 30710 mean train loss:  3.70426476e-03, bound:  3.15358341e-01\n",
      "Epoch: 30711 mean train loss:  3.70428036e-03, bound:  3.15358341e-01\n",
      "Epoch: 30712 mean train loss:  3.70427524e-03, bound:  3.15358341e-01\n",
      "Epoch: 30713 mean train loss:  3.70423635e-03, bound:  3.15358341e-01\n",
      "Epoch: 30714 mean train loss:  3.70418839e-03, bound:  3.15358311e-01\n",
      "Epoch: 30715 mean train loss:  3.70412692e-03, bound:  3.15358311e-01\n",
      "Epoch: 30716 mean train loss:  3.70418793e-03, bound:  3.15358311e-01\n",
      "Epoch: 30717 mean train loss:  3.70412786e-03, bound:  3.15358311e-01\n",
      "Epoch: 30718 mean train loss:  3.70413321e-03, bound:  3.15358311e-01\n",
      "Epoch: 30719 mean train loss:  3.70402168e-03, bound:  3.15358311e-01\n",
      "Epoch: 30720 mean train loss:  3.70401517e-03, bound:  3.15358311e-01\n",
      "Epoch: 30721 mean train loss:  3.70399421e-03, bound:  3.15358311e-01\n",
      "Epoch: 30722 mean train loss:  3.70398397e-03, bound:  3.15358311e-01\n",
      "Epoch: 30723 mean train loss:  3.70393274e-03, bound:  3.15358311e-01\n",
      "Epoch: 30724 mean train loss:  3.70390690e-03, bound:  3.15358311e-01\n",
      "Epoch: 30725 mean train loss:  3.70391854e-03, bound:  3.15358281e-01\n",
      "Epoch: 30726 mean train loss:  3.70384241e-03, bound:  3.15358281e-01\n",
      "Epoch: 30727 mean train loss:  3.70386406e-03, bound:  3.15358281e-01\n",
      "Epoch: 30728 mean train loss:  3.70380725e-03, bound:  3.15358281e-01\n",
      "Epoch: 30729 mean train loss:  3.70373507e-03, bound:  3.15358281e-01\n",
      "Epoch: 30730 mean train loss:  3.70379025e-03, bound:  3.15358281e-01\n",
      "Epoch: 30731 mean train loss:  3.70373856e-03, bound:  3.15358281e-01\n",
      "Epoch: 30732 mean train loss:  3.70364217e-03, bound:  3.15358281e-01\n",
      "Epoch: 30733 mean train loss:  3.70365265e-03, bound:  3.15358281e-01\n",
      "Epoch: 30734 mean train loss:  3.70362378e-03, bound:  3.15358281e-01\n",
      "Epoch: 30735 mean train loss:  3.70359537e-03, bound:  3.15358281e-01\n",
      "Epoch: 30736 mean train loss:  3.70358606e-03, bound:  3.15358281e-01\n",
      "Epoch: 30737 mean train loss:  3.70353507e-03, bound:  3.15358281e-01\n",
      "Epoch: 30738 mean train loss:  3.70350177e-03, bound:  3.15358281e-01\n",
      "Epoch: 30739 mean train loss:  3.70352203e-03, bound:  3.15358281e-01\n",
      "Epoch: 30740 mean train loss:  3.70345265e-03, bound:  3.15358281e-01\n",
      "Epoch: 30741 mean train loss:  3.70339816e-03, bound:  3.15358222e-01\n",
      "Epoch: 30742 mean train loss:  3.70340142e-03, bound:  3.15358222e-01\n",
      "Epoch: 30743 mean train loss:  3.70336184e-03, bound:  3.15358222e-01\n",
      "Epoch: 30744 mean train loss:  3.70333856e-03, bound:  3.15358222e-01\n",
      "Epoch: 30745 mean train loss:  3.70331528e-03, bound:  3.15358222e-01\n",
      "Epoch: 30746 mean train loss:  3.70329968e-03, bound:  3.15358222e-01\n",
      "Epoch: 30747 mean train loss:  3.70323169e-03, bound:  3.15358222e-01\n",
      "Epoch: 30748 mean train loss:  3.70326196e-03, bound:  3.15358222e-01\n",
      "Epoch: 30749 mean train loss:  3.70320957e-03, bound:  3.15358222e-01\n",
      "Epoch: 30750 mean train loss:  3.70317162e-03, bound:  3.15358222e-01\n",
      "Epoch: 30751 mean train loss:  3.70306917e-03, bound:  3.15358222e-01\n",
      "Epoch: 30752 mean train loss:  3.70313250e-03, bound:  3.15358222e-01\n",
      "Epoch: 30753 mean train loss:  3.70312901e-03, bound:  3.15358222e-01\n",
      "Epoch: 30754 mean train loss:  3.70305008e-03, bound:  3.15358222e-01\n",
      "Epoch: 30755 mean train loss:  3.70300096e-03, bound:  3.15358222e-01\n",
      "Epoch: 30756 mean train loss:  3.70300305e-03, bound:  3.15358192e-01\n",
      "Epoch: 30757 mean train loss:  3.70296673e-03, bound:  3.15358192e-01\n",
      "Epoch: 30758 mean train loss:  3.70291364e-03, bound:  3.15358192e-01\n",
      "Epoch: 30759 mean train loss:  3.70291783e-03, bound:  3.15358192e-01\n",
      "Epoch: 30760 mean train loss:  3.70289967e-03, bound:  3.15358192e-01\n",
      "Epoch: 30761 mean train loss:  3.70283355e-03, bound:  3.15358192e-01\n",
      "Epoch: 30762 mean train loss:  3.70284170e-03, bound:  3.15358192e-01\n",
      "Epoch: 30763 mean train loss:  3.70274275e-03, bound:  3.15358192e-01\n",
      "Epoch: 30764 mean train loss:  3.70280701e-03, bound:  3.15358192e-01\n",
      "Epoch: 30765 mean train loss:  3.70273134e-03, bound:  3.15358162e-01\n",
      "Epoch: 30766 mean train loss:  3.70270363e-03, bound:  3.15358162e-01\n",
      "Epoch: 30767 mean train loss:  3.70266545e-03, bound:  3.15358162e-01\n",
      "Epoch: 30768 mean train loss:  3.70263937e-03, bound:  3.15358162e-01\n",
      "Epoch: 30769 mean train loss:  3.70262540e-03, bound:  3.15358162e-01\n",
      "Epoch: 30770 mean train loss:  3.70258815e-03, bound:  3.15358162e-01\n",
      "Epoch: 30771 mean train loss:  3.70258768e-03, bound:  3.15358162e-01\n",
      "Epoch: 30772 mean train loss:  3.70258256e-03, bound:  3.15358162e-01\n",
      "Epoch: 30773 mean train loss:  3.70251620e-03, bound:  3.15358162e-01\n",
      "Epoch: 30774 mean train loss:  3.70246568e-03, bound:  3.15358162e-01\n",
      "Epoch: 30775 mean train loss:  3.70247127e-03, bound:  3.15358162e-01\n",
      "Epoch: 30776 mean train loss:  3.70248710e-03, bound:  3.15358162e-01\n",
      "Epoch: 30777 mean train loss:  3.70239746e-03, bound:  3.15358162e-01\n",
      "Epoch: 30778 mean train loss:  3.70237581e-03, bound:  3.15358162e-01\n",
      "Epoch: 30779 mean train loss:  3.70231271e-03, bound:  3.15358162e-01\n",
      "Epoch: 30780 mean train loss:  3.70234414e-03, bound:  3.15358162e-01\n",
      "Epoch: 30781 mean train loss:  3.70226940e-03, bound:  3.15358162e-01\n",
      "Epoch: 30782 mean train loss:  3.70226474e-03, bound:  3.15358162e-01\n",
      "Epoch: 30783 mean train loss:  3.70225753e-03, bound:  3.15358102e-01\n",
      "Epoch: 30784 mean train loss:  3.70219117e-03, bound:  3.15358102e-01\n",
      "Epoch: 30785 mean train loss:  3.70212062e-03, bound:  3.15358102e-01\n",
      "Epoch: 30786 mean train loss:  3.70212668e-03, bound:  3.15358102e-01\n",
      "Epoch: 30787 mean train loss:  3.70211271e-03, bound:  3.15358102e-01\n",
      "Epoch: 30788 mean train loss:  3.70210293e-03, bound:  3.15358102e-01\n",
      "Epoch: 30789 mean train loss:  3.70205729e-03, bound:  3.15358102e-01\n",
      "Epoch: 30790 mean train loss:  3.70200328e-03, bound:  3.15358102e-01\n",
      "Epoch: 30791 mean train loss:  3.70198791e-03, bound:  3.15358102e-01\n",
      "Epoch: 30792 mean train loss:  3.70197254e-03, bound:  3.15358102e-01\n",
      "Epoch: 30793 mean train loss:  3.70192830e-03, bound:  3.15358073e-01\n",
      "Epoch: 30794 mean train loss:  3.70188849e-03, bound:  3.15358073e-01\n",
      "Epoch: 30795 mean train loss:  3.70186265e-03, bound:  3.15358073e-01\n",
      "Epoch: 30796 mean train loss:  3.70180956e-03, bound:  3.15358073e-01\n",
      "Epoch: 30797 mean train loss:  3.70179559e-03, bound:  3.15358073e-01\n",
      "Epoch: 30798 mean train loss:  3.70174041e-03, bound:  3.15358073e-01\n",
      "Epoch: 30799 mean train loss:  3.70177696e-03, bound:  3.15358073e-01\n",
      "Epoch: 30800 mean train loss:  3.70173389e-03, bound:  3.15358073e-01\n",
      "Epoch: 30801 mean train loss:  3.70165845e-03, bound:  3.15358073e-01\n",
      "Epoch: 30802 mean train loss:  3.70169128e-03, bound:  3.15358073e-01\n",
      "Epoch: 30803 mean train loss:  3.70164774e-03, bound:  3.15358073e-01\n",
      "Epoch: 30804 mean train loss:  3.70162888e-03, bound:  3.15358073e-01\n",
      "Epoch: 30805 mean train loss:  3.70156858e-03, bound:  3.15358073e-01\n",
      "Epoch: 30806 mean train loss:  3.70157138e-03, bound:  3.15358073e-01\n",
      "Epoch: 30807 mean train loss:  3.70150339e-03, bound:  3.15358073e-01\n",
      "Epoch: 30808 mean train loss:  3.70150898e-03, bound:  3.15358073e-01\n",
      "Epoch: 30809 mean train loss:  3.70147754e-03, bound:  3.15358073e-01\n",
      "Epoch: 30810 mean train loss:  3.70145985e-03, bound:  3.15358073e-01\n",
      "Epoch: 30811 mean train loss:  3.70143470e-03, bound:  3.15358073e-01\n",
      "Epoch: 30812 mean train loss:  3.70139326e-03, bound:  3.15358073e-01\n",
      "Epoch: 30813 mean train loss:  3.70135414e-03, bound:  3.15358073e-01\n",
      "Epoch: 30814 mean train loss:  3.70134111e-03, bound:  3.15358073e-01\n",
      "Epoch: 30815 mean train loss:  3.70126031e-03, bound:  3.15358073e-01\n",
      "Epoch: 30816 mean train loss:  3.70127126e-03, bound:  3.15358073e-01\n",
      "Epoch: 30817 mean train loss:  3.70125263e-03, bound:  3.15357983e-01\n",
      "Epoch: 30818 mean train loss:  3.70121584e-03, bound:  3.15357983e-01\n",
      "Epoch: 30819 mean train loss:  3.70114972e-03, bound:  3.15357983e-01\n",
      "Epoch: 30820 mean train loss:  3.70116415e-03, bound:  3.15357983e-01\n",
      "Epoch: 30821 mean train loss:  3.70110525e-03, bound:  3.15357983e-01\n",
      "Epoch: 30822 mean train loss:  3.70106124e-03, bound:  3.15357983e-01\n",
      "Epoch: 30823 mean train loss:  3.70103726e-03, bound:  3.15357983e-01\n",
      "Epoch: 30824 mean train loss:  3.70102702e-03, bound:  3.15357983e-01\n",
      "Epoch: 30825 mean train loss:  3.70100955e-03, bound:  3.15357983e-01\n",
      "Epoch: 30826 mean train loss:  3.70093342e-03, bound:  3.15357983e-01\n",
      "Epoch: 30827 mean train loss:  3.70093738e-03, bound:  3.15357953e-01\n",
      "Epoch: 30828 mean train loss:  3.70092969e-03, bound:  3.15357953e-01\n",
      "Epoch: 30829 mean train loss:  3.70089826e-03, bound:  3.15357953e-01\n",
      "Epoch: 30830 mean train loss:  3.70085239e-03, bound:  3.15357953e-01\n",
      "Epoch: 30831 mean train loss:  3.70078813e-03, bound:  3.15357953e-01\n",
      "Epoch: 30832 mean train loss:  3.70078254e-03, bound:  3.15357953e-01\n",
      "Epoch: 30833 mean train loss:  3.70074692e-03, bound:  3.15357953e-01\n",
      "Epoch: 30834 mean train loss:  3.70072038e-03, bound:  3.15357953e-01\n",
      "Epoch: 30835 mean train loss:  3.70070548e-03, bound:  3.15357953e-01\n",
      "Epoch: 30836 mean train loss:  3.70066846e-03, bound:  3.15357953e-01\n",
      "Epoch: 30837 mean train loss:  3.70065845e-03, bound:  3.15357953e-01\n",
      "Epoch: 30838 mean train loss:  3.70063935e-03, bound:  3.15357953e-01\n",
      "Epoch: 30839 mean train loss:  3.70057975e-03, bound:  3.15357953e-01\n",
      "Epoch: 30840 mean train loss:  3.70056788e-03, bound:  3.15357953e-01\n",
      "Epoch: 30841 mean train loss:  3.70054762e-03, bound:  3.15357953e-01\n",
      "Epoch: 30842 mean train loss:  3.70048219e-03, bound:  3.15357953e-01\n",
      "Epoch: 30843 mean train loss:  3.70045239e-03, bound:  3.15357953e-01\n",
      "Epoch: 30844 mean train loss:  3.70043796e-03, bound:  3.15357953e-01\n",
      "Epoch: 30845 mean train loss:  3.70036182e-03, bound:  3.15357953e-01\n",
      "Epoch: 30846 mean train loss:  3.70039884e-03, bound:  3.15357953e-01\n",
      "Epoch: 30847 mean train loss:  3.70038184e-03, bound:  3.15357953e-01\n",
      "Epoch: 30848 mean train loss:  3.70032224e-03, bound:  3.15357953e-01\n",
      "Epoch: 30849 mean train loss:  3.70025262e-03, bound:  3.15357953e-01\n",
      "Epoch: 30850 mean train loss:  3.70024471e-03, bound:  3.15357953e-01\n",
      "Epoch: 30851 mean train loss:  3.70021909e-03, bound:  3.15357953e-01\n",
      "Epoch: 30852 mean train loss:  3.70017975e-03, bound:  3.15357894e-01\n",
      "Epoch: 30853 mean train loss:  3.70018138e-03, bound:  3.15357894e-01\n",
      "Epoch: 30854 mean train loss:  3.70014040e-03, bound:  3.15357864e-01\n",
      "Epoch: 30855 mean train loss:  3.70012480e-03, bound:  3.15357864e-01\n",
      "Epoch: 30856 mean train loss:  3.70008894e-03, bound:  3.15357864e-01\n",
      "Epoch: 30857 mean train loss:  3.70007870e-03, bound:  3.15357864e-01\n",
      "Epoch: 30858 mean train loss:  3.70003143e-03, bound:  3.15357864e-01\n",
      "Epoch: 30859 mean train loss:  3.69997392e-03, bound:  3.15357864e-01\n",
      "Epoch: 30860 mean train loss:  3.69998394e-03, bound:  3.15357864e-01\n",
      "Epoch: 30861 mean train loss:  3.69996927e-03, bound:  3.15357864e-01\n",
      "Epoch: 30862 mean train loss:  3.69988452e-03, bound:  3.15357864e-01\n",
      "Epoch: 30863 mean train loss:  3.69982887e-03, bound:  3.15357864e-01\n",
      "Epoch: 30864 mean train loss:  3.69991525e-03, bound:  3.15357834e-01\n",
      "Epoch: 30865 mean train loss:  3.69979860e-03, bound:  3.15357864e-01\n",
      "Epoch: 30866 mean train loss:  3.69980652e-03, bound:  3.15357834e-01\n",
      "Epoch: 30867 mean train loss:  3.69976368e-03, bound:  3.15357834e-01\n",
      "Epoch: 30868 mean train loss:  3.69974622e-03, bound:  3.15357834e-01\n",
      "Epoch: 30869 mean train loss:  3.69972596e-03, bound:  3.15357834e-01\n",
      "Epoch: 30870 mean train loss:  3.69967683e-03, bound:  3.15357834e-01\n",
      "Epoch: 30871 mean train loss:  3.69962212e-03, bound:  3.15357834e-01\n",
      "Epoch: 30872 mean train loss:  3.69962258e-03, bound:  3.15357834e-01\n",
      "Epoch: 30873 mean train loss:  3.69956251e-03, bound:  3.15357834e-01\n",
      "Epoch: 30874 mean train loss:  3.69952410e-03, bound:  3.15357834e-01\n",
      "Epoch: 30875 mean train loss:  3.69954761e-03, bound:  3.15357834e-01\n",
      "Epoch: 30876 mean train loss:  3.69951990e-03, bound:  3.15357834e-01\n",
      "Epoch: 30877 mean train loss:  3.69946496e-03, bound:  3.15357834e-01\n",
      "Epoch: 30878 mean train loss:  3.69941816e-03, bound:  3.15357834e-01\n",
      "Epoch: 30879 mean train loss:  3.69938742e-03, bound:  3.15357834e-01\n",
      "Epoch: 30880 mean train loss:  3.69936903e-03, bound:  3.15357834e-01\n",
      "Epoch: 30881 mean train loss:  3.69935413e-03, bound:  3.15357834e-01\n",
      "Epoch: 30882 mean train loss:  3.69931967e-03, bound:  3.15357834e-01\n",
      "Epoch: 30883 mean train loss:  3.69925029e-03, bound:  3.15357834e-01\n",
      "Epoch: 30884 mean train loss:  3.69929057e-03, bound:  3.15357834e-01\n",
      "Epoch: 30885 mean train loss:  3.69924074e-03, bound:  3.15357834e-01\n",
      "Epoch: 30886 mean train loss:  3.69920465e-03, bound:  3.15357834e-01\n",
      "Epoch: 30887 mean train loss:  3.69919674e-03, bound:  3.15357834e-01\n",
      "Epoch: 30888 mean train loss:  3.69914994e-03, bound:  3.15357774e-01\n",
      "Epoch: 30889 mean train loss:  3.69910151e-03, bound:  3.15357834e-01\n",
      "Epoch: 30890 mean train loss:  3.69904144e-03, bound:  3.15357745e-01\n",
      "Epoch: 30891 mean train loss:  3.69904819e-03, bound:  3.15357745e-01\n",
      "Epoch: 30892 mean train loss:  3.69899767e-03, bound:  3.15357745e-01\n",
      "Epoch: 30893 mean train loss:  3.69896414e-03, bound:  3.15357745e-01\n",
      "Epoch: 30894 mean train loss:  3.69895832e-03, bound:  3.15357745e-01\n",
      "Epoch: 30895 mean train loss:  3.69890011e-03, bound:  3.15357745e-01\n",
      "Epoch: 30896 mean train loss:  3.69889289e-03, bound:  3.15357745e-01\n",
      "Epoch: 30897 mean train loss:  3.69884330e-03, bound:  3.15357745e-01\n",
      "Epoch: 30898 mean train loss:  3.69881024e-03, bound:  3.15357745e-01\n",
      "Epoch: 30899 mean train loss:  3.69879277e-03, bound:  3.15357745e-01\n",
      "Epoch: 30900 mean train loss:  3.69876949e-03, bound:  3.15357745e-01\n",
      "Epoch: 30901 mean train loss:  3.69875901e-03, bound:  3.15357745e-01\n",
      "Epoch: 30902 mean train loss:  3.69873922e-03, bound:  3.15357745e-01\n",
      "Epoch: 30903 mean train loss:  3.69866355e-03, bound:  3.15357745e-01\n",
      "Epoch: 30904 mean train loss:  3.69867077e-03, bound:  3.15357745e-01\n",
      "Epoch: 30905 mean train loss:  3.69861163e-03, bound:  3.15357745e-01\n",
      "Epoch: 30906 mean train loss:  3.69860325e-03, bound:  3.15357745e-01\n",
      "Epoch: 30907 mean train loss:  3.69856739e-03, bound:  3.15357745e-01\n",
      "Epoch: 30908 mean train loss:  3.69854295e-03, bound:  3.15357745e-01\n",
      "Epoch: 30909 mean train loss:  3.69852711e-03, bound:  3.15357745e-01\n",
      "Epoch: 30910 mean train loss:  3.69846239e-03, bound:  3.15357745e-01\n",
      "Epoch: 30911 mean train loss:  3.69846937e-03, bound:  3.15357745e-01\n",
      "Epoch: 30912 mean train loss:  3.69842793e-03, bound:  3.15357745e-01\n",
      "Epoch: 30913 mean train loss:  3.69837182e-03, bound:  3.15357715e-01\n",
      "Epoch: 30914 mean train loss:  3.69831966e-03, bound:  3.15357745e-01\n",
      "Epoch: 30915 mean train loss:  3.69830965e-03, bound:  3.15357715e-01\n",
      "Epoch: 30916 mean train loss:  3.69829265e-03, bound:  3.15357715e-01\n",
      "Epoch: 30917 mean train loss:  3.69821466e-03, bound:  3.15357715e-01\n",
      "Epoch: 30918 mean train loss:  3.69822560e-03, bound:  3.15357715e-01\n",
      "Epoch: 30919 mean train loss:  3.69822467e-03, bound:  3.15357715e-01\n",
      "Epoch: 30920 mean train loss:  3.69818043e-03, bound:  3.15357715e-01\n",
      "Epoch: 30921 mean train loss:  3.69811710e-03, bound:  3.15357715e-01\n",
      "Epoch: 30922 mean train loss:  3.69813759e-03, bound:  3.15357715e-01\n",
      "Epoch: 30923 mean train loss:  3.69804492e-03, bound:  3.15357715e-01\n",
      "Epoch: 30924 mean train loss:  3.69805447e-03, bound:  3.15357655e-01\n",
      "Epoch: 30925 mean train loss:  3.69803887e-03, bound:  3.15357655e-01\n",
      "Epoch: 30926 mean train loss:  3.69798997e-03, bound:  3.15357655e-01\n",
      "Epoch: 30927 mean train loss:  3.69796040e-03, bound:  3.15357655e-01\n",
      "Epoch: 30928 mean train loss:  3.69790150e-03, bound:  3.15357655e-01\n",
      "Epoch: 30929 mean train loss:  3.69788148e-03, bound:  3.15357655e-01\n",
      "Epoch: 30930 mean train loss:  3.69785586e-03, bound:  3.15357655e-01\n",
      "Epoch: 30931 mean train loss:  3.69780720e-03, bound:  3.15357655e-01\n",
      "Epoch: 30932 mean train loss:  3.69783910e-03, bound:  3.15357655e-01\n",
      "Epoch: 30933 mean train loss:  3.69776087e-03, bound:  3.15357655e-01\n",
      "Epoch: 30934 mean train loss:  3.69770150e-03, bound:  3.15357655e-01\n",
      "Epoch: 30935 mean train loss:  3.69765027e-03, bound:  3.15357655e-01\n",
      "Epoch: 30936 mean train loss:  3.69767589e-03, bound:  3.15357655e-01\n",
      "Epoch: 30937 mean train loss:  3.69767309e-03, bound:  3.15357625e-01\n",
      "Epoch: 30938 mean train loss:  3.69762816e-03, bound:  3.15357625e-01\n",
      "Epoch: 30939 mean train loss:  3.69760324e-03, bound:  3.15357625e-01\n",
      "Epoch: 30940 mean train loss:  3.69752804e-03, bound:  3.15357625e-01\n",
      "Epoch: 30941 mean train loss:  3.69749311e-03, bound:  3.15357625e-01\n",
      "Epoch: 30942 mean train loss:  3.69753176e-03, bound:  3.15357625e-01\n",
      "Epoch: 30943 mean train loss:  3.69746936e-03, bound:  3.15357625e-01\n",
      "Epoch: 30944 mean train loss:  3.69743258e-03, bound:  3.15357625e-01\n",
      "Epoch: 30945 mean train loss:  3.69739183e-03, bound:  3.15357625e-01\n",
      "Epoch: 30946 mean train loss:  3.69735342e-03, bound:  3.15357596e-01\n",
      "Epoch: 30947 mean train loss:  3.69731570e-03, bound:  3.15357625e-01\n",
      "Epoch: 30948 mean train loss:  3.69734969e-03, bound:  3.15357596e-01\n",
      "Epoch: 30949 mean train loss:  3.69727192e-03, bound:  3.15357596e-01\n",
      "Epoch: 30950 mean train loss:  3.69724818e-03, bound:  3.15357596e-01\n",
      "Epoch: 30951 mean train loss:  3.69717088e-03, bound:  3.15357596e-01\n",
      "Epoch: 30952 mean train loss:  3.69719439e-03, bound:  3.15357596e-01\n",
      "Epoch: 30953 mean train loss:  3.69716482e-03, bound:  3.15357596e-01\n",
      "Epoch: 30954 mean train loss:  3.69710824e-03, bound:  3.15357596e-01\n",
      "Epoch: 30955 mean train loss:  3.69708287e-03, bound:  3.15357596e-01\n",
      "Epoch: 30956 mean train loss:  3.69711057e-03, bound:  3.15357596e-01\n",
      "Epoch: 30957 mean train loss:  3.69701488e-03, bound:  3.15357596e-01\n",
      "Epoch: 30958 mean train loss:  3.69697553e-03, bound:  3.15357596e-01\n",
      "Epoch: 30959 mean train loss:  3.69701721e-03, bound:  3.15357596e-01\n",
      "Epoch: 30960 mean train loss:  3.69695062e-03, bound:  3.15357596e-01\n",
      "Epoch: 30961 mean train loss:  3.69693758e-03, bound:  3.15357536e-01\n",
      "Epoch: 30962 mean train loss:  3.69690196e-03, bound:  3.15357536e-01\n",
      "Epoch: 30963 mean train loss:  3.69681930e-03, bound:  3.15357536e-01\n",
      "Epoch: 30964 mean train loss:  3.69684608e-03, bound:  3.15357536e-01\n",
      "Epoch: 30965 mean train loss:  3.69676901e-03, bound:  3.15357536e-01\n",
      "Epoch: 30966 mean train loss:  3.69674480e-03, bound:  3.15357536e-01\n",
      "Epoch: 30967 mean train loss:  3.69672291e-03, bound:  3.15357536e-01\n",
      "Epoch: 30968 mean train loss:  3.69673432e-03, bound:  3.15357536e-01\n",
      "Epoch: 30969 mean train loss:  3.69665981e-03, bound:  3.15357536e-01\n",
      "Epoch: 30970 mean train loss:  3.69664887e-03, bound:  3.15357536e-01\n",
      "Epoch: 30971 mean train loss:  3.69659765e-03, bound:  3.15357536e-01\n",
      "Epoch: 30972 mean train loss:  3.69660114e-03, bound:  3.15357506e-01\n",
      "Epoch: 30973 mean train loss:  3.69651965e-03, bound:  3.15357536e-01\n",
      "Epoch: 30974 mean train loss:  3.69652477e-03, bound:  3.15357506e-01\n",
      "Epoch: 30975 mean train loss:  3.69650102e-03, bound:  3.15357506e-01\n",
      "Epoch: 30976 mean train loss:  3.69644118e-03, bound:  3.15357506e-01\n",
      "Epoch: 30977 mean train loss:  3.69642512e-03, bound:  3.15357506e-01\n",
      "Epoch: 30978 mean train loss:  3.69633082e-03, bound:  3.15357506e-01\n",
      "Epoch: 30979 mean train loss:  3.69633827e-03, bound:  3.15357506e-01\n",
      "Epoch: 30980 mean train loss:  3.69630544e-03, bound:  3.15357506e-01\n",
      "Epoch: 30981 mean train loss:  3.69632104e-03, bound:  3.15357506e-01\n",
      "Epoch: 30982 mean train loss:  3.69632058e-03, bound:  3.15357506e-01\n",
      "Epoch: 30983 mean train loss:  3.69620766e-03, bound:  3.15357476e-01\n",
      "Epoch: 30984 mean train loss:  3.69620859e-03, bound:  3.15357447e-01\n",
      "Epoch: 30985 mean train loss:  3.69617087e-03, bound:  3.15357476e-01\n",
      "Epoch: 30986 mean train loss:  3.69610079e-03, bound:  3.15357447e-01\n",
      "Epoch: 30987 mean train loss:  3.69607774e-03, bound:  3.15357447e-01\n",
      "Epoch: 30988 mean train loss:  3.69608379e-03, bound:  3.15357447e-01\n",
      "Epoch: 30989 mean train loss:  3.69598833e-03, bound:  3.15357447e-01\n",
      "Epoch: 30990 mean train loss:  3.69599555e-03, bound:  3.15357447e-01\n",
      "Epoch: 30991 mean train loss:  3.69597739e-03, bound:  3.15357447e-01\n",
      "Epoch: 30992 mean train loss:  3.69595666e-03, bound:  3.15357447e-01\n",
      "Epoch: 30993 mean train loss:  3.69591266e-03, bound:  3.15357447e-01\n",
      "Epoch: 30994 mean train loss:  3.69593594e-03, bound:  3.15357447e-01\n",
      "Epoch: 30995 mean train loss:  3.69587098e-03, bound:  3.15357447e-01\n",
      "Epoch: 30996 mean train loss:  3.69584141e-03, bound:  3.15357447e-01\n",
      "Epoch: 30997 mean train loss:  3.69581976e-03, bound:  3.15357417e-01\n",
      "Epoch: 30998 mean train loss:  3.69572802e-03, bound:  3.15357417e-01\n",
      "Epoch: 30999 mean train loss:  3.69574060e-03, bound:  3.15357417e-01\n",
      "Epoch: 31000 mean train loss:  3.69570591e-03, bound:  3.15357417e-01\n",
      "Epoch: 31001 mean train loss:  3.69569962e-03, bound:  3.15357417e-01\n",
      "Epoch: 31002 mean train loss:  3.69562069e-03, bound:  3.15357417e-01\n",
      "Epoch: 31003 mean train loss:  3.69561696e-03, bound:  3.15357417e-01\n",
      "Epoch: 31004 mean train loss:  3.69559741e-03, bound:  3.15357417e-01\n",
      "Epoch: 31005 mean train loss:  3.69557389e-03, bound:  3.15357417e-01\n",
      "Epoch: 31006 mean train loss:  3.69553920e-03, bound:  3.15357417e-01\n",
      "Epoch: 31007 mean train loss:  3.69548262e-03, bound:  3.15357417e-01\n",
      "Epoch: 31008 mean train loss:  3.69542721e-03, bound:  3.15357387e-01\n",
      "Epoch: 31009 mean train loss:  3.69540439e-03, bound:  3.15357387e-01\n",
      "Epoch: 31010 mean train loss:  3.69540835e-03, bound:  3.15357387e-01\n",
      "Epoch: 31011 mean train loss:  3.69534339e-03, bound:  3.15357387e-01\n",
      "Epoch: 31012 mean train loss:  3.69529729e-03, bound:  3.15357387e-01\n",
      "Epoch: 31013 mean train loss:  3.69526539e-03, bound:  3.15357387e-01\n",
      "Epoch: 31014 mean train loss:  3.69526586e-03, bound:  3.15357387e-01\n",
      "Epoch: 31015 mean train loss:  3.69518157e-03, bound:  3.15357387e-01\n",
      "Epoch: 31016 mean train loss:  3.69518390e-03, bound:  3.15357387e-01\n",
      "Epoch: 31017 mean train loss:  3.69515433e-03, bound:  3.15357387e-01\n",
      "Epoch: 31018 mean train loss:  3.69515410e-03, bound:  3.15357387e-01\n",
      "Epoch: 31019 mean train loss:  3.69509356e-03, bound:  3.15357387e-01\n",
      "Epoch: 31020 mean train loss:  3.69512546e-03, bound:  3.15357327e-01\n",
      "Epoch: 31021 mean train loss:  3.69506679e-03, bound:  3.15357327e-01\n",
      "Epoch: 31022 mean train loss:  3.69497179e-03, bound:  3.15357327e-01\n",
      "Epoch: 31023 mean train loss:  3.69500020e-03, bound:  3.15357327e-01\n",
      "Epoch: 31024 mean train loss:  3.69492243e-03, bound:  3.15357327e-01\n",
      "Epoch: 31025 mean train loss:  3.69489589e-03, bound:  3.15357327e-01\n",
      "Epoch: 31026 mean train loss:  3.69489659e-03, bound:  3.15357327e-01\n",
      "Epoch: 31027 mean train loss:  3.69479530e-03, bound:  3.15357327e-01\n",
      "Epoch: 31028 mean train loss:  3.69483768e-03, bound:  3.15357327e-01\n",
      "Epoch: 31029 mean train loss:  3.69475037e-03, bound:  3.15357327e-01\n",
      "Epoch: 31030 mean train loss:  3.69471894e-03, bound:  3.15357298e-01\n",
      "Epoch: 31031 mean train loss:  3.69472289e-03, bound:  3.15357298e-01\n",
      "Epoch: 31032 mean train loss:  3.69466888e-03, bound:  3.15357298e-01\n",
      "Epoch: 31033 mean train loss:  3.69464303e-03, bound:  3.15357298e-01\n",
      "Epoch: 31034 mean train loss:  3.69464420e-03, bound:  3.15357298e-01\n",
      "Epoch: 31035 mean train loss:  3.69458087e-03, bound:  3.15357298e-01\n",
      "Epoch: 31036 mean train loss:  3.69458948e-03, bound:  3.15357298e-01\n",
      "Epoch: 31037 mean train loss:  3.69449309e-03, bound:  3.15357298e-01\n",
      "Epoch: 31038 mean train loss:  3.69450613e-03, bound:  3.15357298e-01\n",
      "Epoch: 31039 mean train loss:  3.69444373e-03, bound:  3.15357298e-01\n",
      "Epoch: 31040 mean train loss:  3.69445258e-03, bound:  3.15357298e-01\n",
      "Epoch: 31041 mean train loss:  3.69436038e-03, bound:  3.15357268e-01\n",
      "Epoch: 31042 mean train loss:  3.69433570e-03, bound:  3.15357268e-01\n",
      "Epoch: 31043 mean train loss:  3.69435805e-03, bound:  3.15357268e-01\n",
      "Epoch: 31044 mean train loss:  3.69431498e-03, bound:  3.15357268e-01\n",
      "Epoch: 31045 mean train loss:  3.69425677e-03, bound:  3.15357268e-01\n",
      "Epoch: 31046 mean train loss:  3.69422790e-03, bound:  3.15357268e-01\n",
      "Epoch: 31047 mean train loss:  3.69419972e-03, bound:  3.15357268e-01\n",
      "Epoch: 31048 mean train loss:  3.69417574e-03, bound:  3.15357268e-01\n",
      "Epoch: 31049 mean train loss:  3.69414804e-03, bound:  3.15357268e-01\n",
      "Epoch: 31050 mean train loss:  3.69413407e-03, bound:  3.15357268e-01\n",
      "Epoch: 31051 mean train loss:  3.69408238e-03, bound:  3.15357268e-01\n",
      "Epoch: 31052 mean train loss:  3.69408331e-03, bound:  3.15357268e-01\n",
      "Epoch: 31053 mean train loss:  3.69403604e-03, bound:  3.15357268e-01\n",
      "Epoch: 31054 mean train loss:  3.69401695e-03, bound:  3.15357268e-01\n",
      "Epoch: 31055 mean train loss:  3.69395572e-03, bound:  3.15357268e-01\n",
      "Epoch: 31056 mean train loss:  3.69396573e-03, bound:  3.15357268e-01\n",
      "Epoch: 31057 mean train loss:  3.69390845e-03, bound:  3.15357268e-01\n",
      "Epoch: 31058 mean train loss:  3.69385490e-03, bound:  3.15357268e-01\n",
      "Epoch: 31059 mean train loss:  3.69382696e-03, bound:  3.15357268e-01\n",
      "Epoch: 31060 mean train loss:  3.69378831e-03, bound:  3.15357268e-01\n",
      "Epoch: 31061 mean train loss:  3.69376200e-03, bound:  3.15357268e-01\n",
      "Epoch: 31062 mean train loss:  3.69378040e-03, bound:  3.15357208e-01\n",
      "Epoch: 31063 mean train loss:  3.69370333e-03, bound:  3.15357178e-01\n",
      "Epoch: 31064 mean train loss:  3.69365024e-03, bound:  3.15357178e-01\n",
      "Epoch: 31065 mean train loss:  3.69361672e-03, bound:  3.15357178e-01\n",
      "Epoch: 31066 mean train loss:  3.69362743e-03, bound:  3.15357178e-01\n",
      "Epoch: 31067 mean train loss:  3.69358552e-03, bound:  3.15357178e-01\n",
      "Epoch: 31068 mean train loss:  3.69362789e-03, bound:  3.15357178e-01\n",
      "Epoch: 31069 mean train loss:  3.69350147e-03, bound:  3.15357178e-01\n",
      "Epoch: 31070 mean train loss:  3.69348656e-03, bound:  3.15357178e-01\n",
      "Epoch: 31071 mean train loss:  3.69347143e-03, bound:  3.15357178e-01\n",
      "Epoch: 31072 mean train loss:  3.69338994e-03, bound:  3.15357178e-01\n",
      "Epoch: 31073 mean train loss:  3.69337504e-03, bound:  3.15357178e-01\n",
      "Epoch: 31074 mean train loss:  3.69337830e-03, bound:  3.15357178e-01\n",
      "Epoch: 31075 mean train loss:  3.69331473e-03, bound:  3.15357149e-01\n",
      "Epoch: 31076 mean train loss:  3.69332288e-03, bound:  3.15357149e-01\n",
      "Epoch: 31077 mean train loss:  3.69327702e-03, bound:  3.15357149e-01\n",
      "Epoch: 31078 mean train loss:  3.69322929e-03, bound:  3.15357149e-01\n",
      "Epoch: 31079 mean train loss:  3.69319669e-03, bound:  3.15357149e-01\n",
      "Epoch: 31080 mean train loss:  3.69319343e-03, bound:  3.15357149e-01\n",
      "Epoch: 31081 mean train loss:  3.69310495e-03, bound:  3.15357149e-01\n",
      "Epoch: 31082 mean train loss:  3.69308027e-03, bound:  3.15357149e-01\n",
      "Epoch: 31083 mean train loss:  3.69305280e-03, bound:  3.15357149e-01\n",
      "Epoch: 31084 mean train loss:  3.69302742e-03, bound:  3.15357149e-01\n",
      "Epoch: 31085 mean train loss:  3.69296432e-03, bound:  3.15357149e-01\n",
      "Epoch: 31086 mean train loss:  3.69296921e-03, bound:  3.15357149e-01\n",
      "Epoch: 31087 mean train loss:  3.69293685e-03, bound:  3.15357149e-01\n",
      "Epoch: 31088 mean train loss:  3.69291590e-03, bound:  3.15357149e-01\n",
      "Epoch: 31089 mean train loss:  3.69289285e-03, bound:  3.15357149e-01\n",
      "Epoch: 31090 mean train loss:  3.69285350e-03, bound:  3.15357149e-01\n",
      "Epoch: 31091 mean train loss:  3.69278411e-03, bound:  3.15357149e-01\n",
      "Epoch: 31092 mean train loss:  3.69278900e-03, bound:  3.15357149e-01\n",
      "Epoch: 31093 mean train loss:  3.69277643e-03, bound:  3.15357149e-01\n",
      "Epoch: 31094 mean train loss:  3.69267189e-03, bound:  3.15357149e-01\n",
      "Epoch: 31095 mean train loss:  3.69270355e-03, bound:  3.15357089e-01\n",
      "Epoch: 31096 mean train loss:  3.69266653e-03, bound:  3.15357089e-01\n",
      "Epoch: 31097 mean train loss:  3.69263790e-03, bound:  3.15357089e-01\n",
      "Epoch: 31098 mean train loss:  3.69261950e-03, bound:  3.15357089e-01\n",
      "Epoch: 31099 mean train loss:  3.69257852e-03, bound:  3.15357089e-01\n",
      "Epoch: 31100 mean train loss:  3.69250518e-03, bound:  3.15357089e-01\n",
      "Epoch: 31101 mean train loss:  3.69249005e-03, bound:  3.15357089e-01\n",
      "Epoch: 31102 mean train loss:  3.69243650e-03, bound:  3.15357089e-01\n",
      "Epoch: 31103 mean train loss:  3.69241205e-03, bound:  3.15357089e-01\n",
      "Epoch: 31104 mean train loss:  3.69239133e-03, bound:  3.15357089e-01\n",
      "Epoch: 31105 mean train loss:  3.69235594e-03, bound:  3.15357089e-01\n",
      "Epoch: 31106 mean train loss:  3.69234569e-03, bound:  3.15357059e-01\n",
      "Epoch: 31107 mean train loss:  3.69229750e-03, bound:  3.15357029e-01\n",
      "Epoch: 31108 mean train loss:  3.69229820e-03, bound:  3.15357029e-01\n",
      "Epoch: 31109 mean train loss:  3.69221112e-03, bound:  3.15357029e-01\n",
      "Epoch: 31110 mean train loss:  3.69219203e-03, bound:  3.15357029e-01\n",
      "Epoch: 31111 mean train loss:  3.69216036e-03, bound:  3.15357029e-01\n",
      "Epoch: 31112 mean train loss:  3.69214313e-03, bound:  3.15357029e-01\n",
      "Epoch: 31113 mean train loss:  3.69206560e-03, bound:  3.15357029e-01\n",
      "Epoch: 31114 mean train loss:  3.69212124e-03, bound:  3.15357029e-01\n",
      "Epoch: 31115 mean train loss:  3.69204138e-03, bound:  3.15357029e-01\n",
      "Epoch: 31116 mean train loss:  3.69204371e-03, bound:  3.15357000e-01\n",
      "Epoch: 31117 mean train loss:  3.69199528e-03, bound:  3.15357029e-01\n",
      "Epoch: 31118 mean train loss:  3.69195268e-03, bound:  3.15357000e-01\n",
      "Epoch: 31119 mean train loss:  3.69192287e-03, bound:  3.15357000e-01\n",
      "Epoch: 31120 mean train loss:  3.69186536e-03, bound:  3.15357000e-01\n",
      "Epoch: 31121 mean train loss:  3.69187444e-03, bound:  3.15357000e-01\n",
      "Epoch: 31122 mean train loss:  3.69180669e-03, bound:  3.15357000e-01\n",
      "Epoch: 31123 mean train loss:  3.69177223e-03, bound:  3.15357000e-01\n",
      "Epoch: 31124 mean train loss:  3.69176804e-03, bound:  3.15357000e-01\n",
      "Epoch: 31125 mean train loss:  3.69169167e-03, bound:  3.15357000e-01\n",
      "Epoch: 31126 mean train loss:  3.69170099e-03, bound:  3.15356970e-01\n",
      "Epoch: 31127 mean train loss:  3.69164674e-03, bound:  3.15356970e-01\n",
      "Epoch: 31128 mean train loss:  3.69164231e-03, bound:  3.15356970e-01\n",
      "Epoch: 31129 mean train loss:  3.69156315e-03, bound:  3.15356970e-01\n",
      "Epoch: 31130 mean train loss:  3.69152473e-03, bound:  3.15356970e-01\n",
      "Epoch: 31131 mean train loss:  3.69149912e-03, bound:  3.15356970e-01\n",
      "Epoch: 31132 mean train loss:  3.69155756e-03, bound:  3.15356970e-01\n",
      "Epoch: 31133 mean train loss:  3.69144138e-03, bound:  3.15356970e-01\n",
      "Epoch: 31134 mean train loss:  3.69145046e-03, bound:  3.15356970e-01\n",
      "Epoch: 31135 mean train loss:  3.69137293e-03, bound:  3.15356970e-01\n",
      "Epoch: 31136 mean train loss:  3.69138387e-03, bound:  3.15356970e-01\n",
      "Epoch: 31137 mean train loss:  3.69133428e-03, bound:  3.15356970e-01\n",
      "Epoch: 31138 mean train loss:  3.69131495e-03, bound:  3.15356940e-01\n",
      "Epoch: 31139 mean train loss:  3.69124347e-03, bound:  3.15356940e-01\n",
      "Epoch: 31140 mean train loss:  3.69124184e-03, bound:  3.15356940e-01\n",
      "Epoch: 31141 mean train loss:  3.69117409e-03, bound:  3.15356940e-01\n",
      "Epoch: 31142 mean train loss:  3.69114662e-03, bound:  3.15356940e-01\n",
      "Epoch: 31143 mean train loss:  3.69111262e-03, bound:  3.15356940e-01\n",
      "Epoch: 31144 mean train loss:  3.69108049e-03, bound:  3.15356940e-01\n",
      "Epoch: 31145 mean train loss:  3.69103998e-03, bound:  3.15356940e-01\n",
      "Epoch: 31146 mean train loss:  3.69099458e-03, bound:  3.15356940e-01\n",
      "Epoch: 31147 mean train loss:  3.69100599e-03, bound:  3.15356940e-01\n",
      "Epoch: 31148 mean train loss:  3.69098061e-03, bound:  3.15356880e-01\n",
      "Epoch: 31149 mean train loss:  3.69089143e-03, bound:  3.15356880e-01\n",
      "Epoch: 31150 mean train loss:  3.69089609e-03, bound:  3.15356880e-01\n",
      "Epoch: 31151 mean train loss:  3.69088096e-03, bound:  3.15356880e-01\n",
      "Epoch: 31152 mean train loss:  3.69086210e-03, bound:  3.15356880e-01\n",
      "Epoch: 31153 mean train loss:  3.69079760e-03, bound:  3.15356880e-01\n",
      "Epoch: 31154 mean train loss:  3.69080226e-03, bound:  3.15356880e-01\n",
      "Epoch: 31155 mean train loss:  3.69070936e-03, bound:  3.15356880e-01\n",
      "Epoch: 31156 mean train loss:  3.69070494e-03, bound:  3.15356851e-01\n",
      "Epoch: 31157 mean train loss:  3.69065139e-03, bound:  3.15356851e-01\n",
      "Epoch: 31158 mean train loss:  3.69063998e-03, bound:  3.15356851e-01\n",
      "Epoch: 31159 mean train loss:  3.69062112e-03, bound:  3.15356851e-01\n",
      "Epoch: 31160 mean train loss:  3.69058433e-03, bound:  3.15356851e-01\n",
      "Epoch: 31161 mean train loss:  3.69051937e-03, bound:  3.15356851e-01\n",
      "Epoch: 31162 mean train loss:  3.69049353e-03, bound:  3.15356851e-01\n",
      "Epoch: 31163 mean train loss:  3.69041669e-03, bound:  3.15356851e-01\n",
      "Epoch: 31164 mean train loss:  3.69047024e-03, bound:  3.15356851e-01\n",
      "Epoch: 31165 mean train loss:  3.69036803e-03, bound:  3.15356851e-01\n",
      "Epoch: 31166 mean train loss:  3.69036524e-03, bound:  3.15356851e-01\n",
      "Epoch: 31167 mean train loss:  3.69034871e-03, bound:  3.15356851e-01\n",
      "Epoch: 31168 mean train loss:  3.69033357e-03, bound:  3.15356851e-01\n",
      "Epoch: 31169 mean train loss:  3.69025068e-03, bound:  3.15356851e-01\n",
      "Epoch: 31170 mean train loss:  3.69027024e-03, bound:  3.15356821e-01\n",
      "Epoch: 31171 mean train loss:  3.69018549e-03, bound:  3.15356821e-01\n",
      "Epoch: 31172 mean train loss:  3.69017920e-03, bound:  3.15356821e-01\n",
      "Epoch: 31173 mean train loss:  3.69011238e-03, bound:  3.15356821e-01\n",
      "Epoch: 31174 mean train loss:  3.69007979e-03, bound:  3.15356821e-01\n",
      "Epoch: 31175 mean train loss:  3.69003601e-03, bound:  3.15356821e-01\n",
      "Epoch: 31176 mean train loss:  3.69006279e-03, bound:  3.15356821e-01\n",
      "Epoch: 31177 mean train loss:  3.69000342e-03, bound:  3.15356821e-01\n",
      "Epoch: 31178 mean train loss:  3.68999713e-03, bound:  3.15356821e-01\n",
      "Epoch: 31179 mean train loss:  3.68991587e-03, bound:  3.15356791e-01\n",
      "Epoch: 31180 mean train loss:  3.68993054e-03, bound:  3.15356761e-01\n",
      "Epoch: 31181 mean train loss:  3.68991541e-03, bound:  3.15356761e-01\n",
      "Epoch: 31182 mean train loss:  3.68985394e-03, bound:  3.15356761e-01\n",
      "Epoch: 31183 mean train loss:  3.68979201e-03, bound:  3.15356761e-01\n",
      "Epoch: 31184 mean train loss:  3.68978223e-03, bound:  3.15356761e-01\n",
      "Epoch: 31185 mean train loss:  3.68971401e-03, bound:  3.15356761e-01\n",
      "Epoch: 31186 mean train loss:  3.68966698e-03, bound:  3.15356761e-01\n",
      "Epoch: 31187 mean train loss:  3.68967024e-03, bound:  3.15356761e-01\n",
      "Epoch: 31188 mean train loss:  3.68958712e-03, bound:  3.15356761e-01\n",
      "Epoch: 31189 mean train loss:  3.68962623e-03, bound:  3.15356761e-01\n",
      "Epoch: 31190 mean train loss:  3.68954241e-03, bound:  3.15356731e-01\n",
      "Epoch: 31191 mean train loss:  3.68952332e-03, bound:  3.15356731e-01\n",
      "Epoch: 31192 mean train loss:  3.68952448e-03, bound:  3.15356731e-01\n",
      "Epoch: 31193 mean train loss:  3.68949235e-03, bound:  3.15356731e-01\n",
      "Epoch: 31194 mean train loss:  3.68942996e-03, bound:  3.15356731e-01\n",
      "Epoch: 31195 mean train loss:  3.68941016e-03, bound:  3.15356731e-01\n",
      "Epoch: 31196 mean train loss:  3.68937431e-03, bound:  3.15356731e-01\n",
      "Epoch: 31197 mean train loss:  3.68931610e-03, bound:  3.15356731e-01\n",
      "Epoch: 31198 mean train loss:  3.68928793e-03, bound:  3.15356731e-01\n",
      "Epoch: 31199 mean train loss:  3.68929841e-03, bound:  3.15356731e-01\n",
      "Epoch: 31200 mean train loss:  3.68926534e-03, bound:  3.15356731e-01\n",
      "Epoch: 31201 mean train loss:  3.68917733e-03, bound:  3.15356702e-01\n",
      "Epoch: 31202 mean train loss:  3.68917733e-03, bound:  3.15356702e-01\n",
      "Epoch: 31203 mean train loss:  3.68913403e-03, bound:  3.15356702e-01\n",
      "Epoch: 31204 mean train loss:  3.68912681e-03, bound:  3.15356702e-01\n",
      "Epoch: 31205 mean train loss:  3.68908909e-03, bound:  3.15356702e-01\n",
      "Epoch: 31206 mean train loss:  3.68897757e-03, bound:  3.15356702e-01\n",
      "Epoch: 31207 mean train loss:  3.68900737e-03, bound:  3.15356702e-01\n",
      "Epoch: 31208 mean train loss:  3.68897058e-03, bound:  3.15356702e-01\n",
      "Epoch: 31209 mean train loss:  3.68895568e-03, bound:  3.15356702e-01\n",
      "Epoch: 31210 mean train loss:  3.68887652e-03, bound:  3.15356702e-01\n",
      "Epoch: 31211 mean train loss:  3.68887489e-03, bound:  3.15356642e-01\n",
      "Epoch: 31212 mean train loss:  3.68886883e-03, bound:  3.15356642e-01\n",
      "Epoch: 31213 mean train loss:  3.68882972e-03, bound:  3.15356642e-01\n",
      "Epoch: 31214 mean train loss:  3.68882180e-03, bound:  3.15356642e-01\n",
      "Epoch: 31215 mean train loss:  3.68877640e-03, bound:  3.15356642e-01\n",
      "Epoch: 31216 mean train loss:  3.68871796e-03, bound:  3.15356642e-01\n",
      "Epoch: 31217 mean train loss:  3.68867349e-03, bound:  3.15356642e-01\n",
      "Epoch: 31218 mean train loss:  3.68866324e-03, bound:  3.15356642e-01\n",
      "Epoch: 31219 mean train loss:  3.68858641e-03, bound:  3.15356642e-01\n",
      "Epoch: 31220 mean train loss:  3.68854566e-03, bound:  3.15356642e-01\n",
      "Epoch: 31221 mean train loss:  3.68858804e-03, bound:  3.15356612e-01\n",
      "Epoch: 31222 mean train loss:  3.68848094e-03, bound:  3.15356612e-01\n",
      "Epoch: 31223 mean train loss:  3.68847488e-03, bound:  3.15356612e-01\n",
      "Epoch: 31224 mean train loss:  3.68843647e-03, bound:  3.15356612e-01\n",
      "Epoch: 31225 mean train loss:  3.68838874e-03, bound:  3.15356612e-01\n",
      "Epoch: 31226 mean train loss:  3.68838338e-03, bound:  3.15356612e-01\n",
      "Epoch: 31227 mean train loss:  3.68830864e-03, bound:  3.15356612e-01\n",
      "Epoch: 31228 mean train loss:  3.68832191e-03, bound:  3.15356612e-01\n",
      "Epoch: 31229 mean train loss:  3.68823833e-03, bound:  3.15356612e-01\n",
      "Epoch: 31230 mean train loss:  3.68824066e-03, bound:  3.15356582e-01\n",
      "Epoch: 31231 mean train loss:  3.68822063e-03, bound:  3.15356582e-01\n",
      "Epoch: 31232 mean train loss:  3.68816056e-03, bound:  3.15356582e-01\n",
      "Epoch: 31233 mean train loss:  3.68812378e-03, bound:  3.15356582e-01\n",
      "Epoch: 31234 mean train loss:  3.68807837e-03, bound:  3.15356582e-01\n",
      "Epoch: 31235 mean train loss:  3.68802273e-03, bound:  3.15356582e-01\n",
      "Epoch: 31236 mean train loss:  3.68805253e-03, bound:  3.15356582e-01\n",
      "Epoch: 31237 mean train loss:  3.68802203e-03, bound:  3.15356582e-01\n",
      "Epoch: 31238 mean train loss:  3.68794636e-03, bound:  3.15356582e-01\n",
      "Epoch: 31239 mean train loss:  3.68793588e-03, bound:  3.15356582e-01\n",
      "Epoch: 31240 mean train loss:  3.68790817e-03, bound:  3.15356582e-01\n",
      "Epoch: 31241 mean train loss:  3.68783833e-03, bound:  3.15356553e-01\n",
      "Epoch: 31242 mean train loss:  3.68781015e-03, bound:  3.15356523e-01\n",
      "Epoch: 31243 mean train loss:  3.68778454e-03, bound:  3.15356523e-01\n",
      "Epoch: 31244 mean train loss:  3.68776987e-03, bound:  3.15356523e-01\n",
      "Epoch: 31245 mean train loss:  3.68772284e-03, bound:  3.15356523e-01\n",
      "Epoch: 31246 mean train loss:  3.68767208e-03, bound:  3.15356523e-01\n",
      "Epoch: 31247 mean train loss:  3.68768047e-03, bound:  3.15356523e-01\n",
      "Epoch: 31248 mean train loss:  3.68764019e-03, bound:  3.15356523e-01\n",
      "Epoch: 31249 mean train loss:  3.68757709e-03, bound:  3.15356523e-01\n",
      "Epoch: 31250 mean train loss:  3.68754193e-03, bound:  3.15356523e-01\n",
      "Epoch: 31251 mean train loss:  3.68752517e-03, bound:  3.15356493e-01\n",
      "Epoch: 31252 mean train loss:  3.68746230e-03, bound:  3.15356493e-01\n",
      "Epoch: 31253 mean train loss:  3.68742738e-03, bound:  3.15356493e-01\n",
      "Epoch: 31254 mean train loss:  3.68739688e-03, bound:  3.15356493e-01\n",
      "Epoch: 31255 mean train loss:  3.68740875e-03, bound:  3.15356493e-01\n",
      "Epoch: 31256 mean train loss:  3.68735567e-03, bound:  3.15356493e-01\n",
      "Epoch: 31257 mean train loss:  3.68729723e-03, bound:  3.15356493e-01\n",
      "Epoch: 31258 mean train loss:  3.68722342e-03, bound:  3.15356493e-01\n",
      "Epoch: 31259 mean train loss:  3.68722831e-03, bound:  3.15356493e-01\n",
      "Epoch: 31260 mean train loss:  3.68723436e-03, bound:  3.15356493e-01\n",
      "Epoch: 31261 mean train loss:  3.68714775e-03, bound:  3.15356463e-01\n",
      "Epoch: 31262 mean train loss:  3.68716754e-03, bound:  3.15356463e-01\n",
      "Epoch: 31263 mean train loss:  3.68711492e-03, bound:  3.15356463e-01\n",
      "Epoch: 31264 mean train loss:  3.68702738e-03, bound:  3.15356463e-01\n",
      "Epoch: 31265 mean train loss:  3.68700293e-03, bound:  3.15356463e-01\n",
      "Epoch: 31266 mean train loss:  3.68698640e-03, bound:  3.15356463e-01\n",
      "Epoch: 31267 mean train loss:  3.68698314e-03, bound:  3.15356463e-01\n",
      "Epoch: 31268 mean train loss:  3.68694961e-03, bound:  3.15356463e-01\n",
      "Epoch: 31269 mean train loss:  3.68683902e-03, bound:  3.15356463e-01\n",
      "Epoch: 31270 mean train loss:  3.68683273e-03, bound:  3.15356433e-01\n",
      "Epoch: 31271 mean train loss:  3.68689303e-03, bound:  3.15356433e-01\n",
      "Epoch: 31272 mean train loss:  3.68679664e-03, bound:  3.15356433e-01\n",
      "Epoch: 31273 mean train loss:  3.68674053e-03, bound:  3.15356404e-01\n",
      "Epoch: 31274 mean train loss:  3.68673098e-03, bound:  3.15356404e-01\n",
      "Epoch: 31275 mean train loss:  3.68664018e-03, bound:  3.15356404e-01\n",
      "Epoch: 31276 mean train loss:  3.68667417e-03, bound:  3.15356404e-01\n",
      "Epoch: 31277 mean train loss:  3.68663808e-03, bound:  3.15356404e-01\n",
      "Epoch: 31278 mean train loss:  3.68656265e-03, bound:  3.15356404e-01\n",
      "Epoch: 31279 mean train loss:  3.68653517e-03, bound:  3.15356404e-01\n",
      "Epoch: 31280 mean train loss:  3.68653540e-03, bound:  3.15356404e-01\n",
      "Epoch: 31281 mean train loss:  3.68650444e-03, bound:  3.15356404e-01\n",
      "Epoch: 31282 mean train loss:  3.68645485e-03, bound:  3.15356404e-01\n",
      "Epoch: 31283 mean train loss:  3.68642295e-03, bound:  3.15356404e-01\n",
      "Epoch: 31284 mean train loss:  3.68637568e-03, bound:  3.15356404e-01\n",
      "Epoch: 31285 mean train loss:  3.68636986e-03, bound:  3.15356404e-01\n",
      "Epoch: 31286 mean train loss:  3.68629838e-03, bound:  3.15356404e-01\n",
      "Epoch: 31287 mean train loss:  3.68626066e-03, bound:  3.15356404e-01\n",
      "Epoch: 31288 mean train loss:  3.68624623e-03, bound:  3.15356404e-01\n",
      "Epoch: 31289 mean train loss:  3.68622527e-03, bound:  3.15356404e-01\n",
      "Epoch: 31290 mean train loss:  3.68618849e-03, bound:  3.15356404e-01\n",
      "Epoch: 31291 mean train loss:  3.68613540e-03, bound:  3.15356344e-01\n",
      "Epoch: 31292 mean train loss:  3.68611026e-03, bound:  3.15356344e-01\n",
      "Epoch: 31293 mean train loss:  3.68602970e-03, bound:  3.15356374e-01\n",
      "Epoch: 31294 mean train loss:  3.68600921e-03, bound:  3.15356344e-01\n",
      "Epoch: 31295 mean train loss:  3.68597638e-03, bound:  3.15356344e-01\n",
      "Epoch: 31296 mean train loss:  3.68599943e-03, bound:  3.15356344e-01\n",
      "Epoch: 31297 mean train loss:  3.68594308e-03, bound:  3.15356344e-01\n",
      "Epoch: 31298 mean train loss:  3.68589116e-03, bound:  3.15356344e-01\n",
      "Epoch: 31299 mean train loss:  3.68588697e-03, bound:  3.15356344e-01\n",
      "Epoch: 31300 mean train loss:  3.68584343e-03, bound:  3.15356344e-01\n",
      "Epoch: 31301 mean train loss:  3.68577614e-03, bound:  3.15356344e-01\n",
      "Epoch: 31302 mean train loss:  3.68574471e-03, bound:  3.15356314e-01\n",
      "Epoch: 31303 mean train loss:  3.68574983e-03, bound:  3.15356314e-01\n",
      "Epoch: 31304 mean train loss:  3.68572515e-03, bound:  3.15356314e-01\n",
      "Epoch: 31305 mean train loss:  3.68564343e-03, bound:  3.15356314e-01\n",
      "Epoch: 31306 mean train loss:  3.68553586e-03, bound:  3.15356314e-01\n",
      "Epoch: 31307 mean train loss:  3.68559896e-03, bound:  3.15356314e-01\n",
      "Epoch: 31308 mean train loss:  3.68553866e-03, bound:  3.15356314e-01\n",
      "Epoch: 31309 mean train loss:  3.68548115e-03, bound:  3.15356314e-01\n",
      "Epoch: 31310 mean train loss:  3.68549302e-03, bound:  3.15356284e-01\n",
      "Epoch: 31311 mean train loss:  3.68541735e-03, bound:  3.15356284e-01\n",
      "Epoch: 31312 mean train loss:  3.68539314e-03, bound:  3.15356284e-01\n",
      "Epoch: 31313 mean train loss:  3.68535239e-03, bound:  3.15356284e-01\n",
      "Epoch: 31314 mean train loss:  3.68535891e-03, bound:  3.15356284e-01\n",
      "Epoch: 31315 mean train loss:  3.68529651e-03, bound:  3.15356284e-01\n",
      "Epoch: 31316 mean train loss:  3.68528068e-03, bound:  3.15356284e-01\n",
      "Epoch: 31317 mean train loss:  3.68523179e-03, bound:  3.15356284e-01\n",
      "Epoch: 31318 mean train loss:  3.68519151e-03, bound:  3.15356284e-01\n",
      "Epoch: 31319 mean train loss:  3.68512585e-03, bound:  3.15356284e-01\n",
      "Epoch: 31320 mean train loss:  3.68516357e-03, bound:  3.15356284e-01\n",
      "Epoch: 31321 mean train loss:  3.68504645e-03, bound:  3.15356225e-01\n",
      "Epoch: 31322 mean train loss:  3.68504110e-03, bound:  3.15356225e-01\n",
      "Epoch: 31323 mean train loss:  3.68501525e-03, bound:  3.15356225e-01\n",
      "Epoch: 31324 mean train loss:  3.68499756e-03, bound:  3.15356225e-01\n",
      "Epoch: 31325 mean train loss:  3.68492980e-03, bound:  3.15356225e-01\n",
      "Epoch: 31326 mean train loss:  3.68494168e-03, bound:  3.15356225e-01\n",
      "Epoch: 31327 mean train loss:  3.68487136e-03, bound:  3.15356225e-01\n",
      "Epoch: 31328 mean train loss:  3.68485297e-03, bound:  3.15356225e-01\n",
      "Epoch: 31329 mean train loss:  3.68478778e-03, bound:  3.15356225e-01\n",
      "Epoch: 31330 mean train loss:  3.68477334e-03, bound:  3.15356225e-01\n",
      "Epoch: 31331 mean train loss:  3.68473609e-03, bound:  3.15356225e-01\n",
      "Epoch: 31332 mean train loss:  3.68470885e-03, bound:  3.15356225e-01\n",
      "Epoch: 31333 mean train loss:  3.68467020e-03, bound:  3.15356195e-01\n",
      "Epoch: 31334 mean train loss:  3.68459499e-03, bound:  3.15356195e-01\n",
      "Epoch: 31335 mean train loss:  3.68463178e-03, bound:  3.15356195e-01\n",
      "Epoch: 31336 mean train loss:  3.68452328e-03, bound:  3.15356195e-01\n",
      "Epoch: 31337 mean train loss:  3.68450605e-03, bound:  3.15356195e-01\n",
      "Epoch: 31338 mean train loss:  3.68452398e-03, bound:  3.15356195e-01\n",
      "Epoch: 31339 mean train loss:  3.68448603e-03, bound:  3.15356195e-01\n",
      "Epoch: 31340 mean train loss:  3.68443131e-03, bound:  3.15356195e-01\n",
      "Epoch: 31341 mean train loss:  3.68437800e-03, bound:  3.15356195e-01\n",
      "Epoch: 31342 mean train loss:  3.68430768e-03, bound:  3.15356165e-01\n",
      "Epoch: 31343 mean train loss:  3.68429883e-03, bound:  3.15356165e-01\n",
      "Epoch: 31344 mean train loss:  3.68433190e-03, bound:  3.15356165e-01\n",
      "Epoch: 31345 mean train loss:  3.68422712e-03, bound:  3.15356165e-01\n",
      "Epoch: 31346 mean train loss:  3.68421711e-03, bound:  3.15356165e-01\n",
      "Epoch: 31347 mean train loss:  3.68417054e-03, bound:  3.15356165e-01\n",
      "Epoch: 31348 mean train loss:  3.68413120e-03, bound:  3.15356165e-01\n",
      "Epoch: 31349 mean train loss:  3.68407741e-03, bound:  3.15356165e-01\n",
      "Epoch: 31350 mean train loss:  3.68404738e-03, bound:  3.15356165e-01\n",
      "Epoch: 31351 mean train loss:  3.68400454e-03, bound:  3.15356165e-01\n",
      "Epoch: 31352 mean train loss:  3.68400034e-03, bound:  3.15356135e-01\n",
      "Epoch: 31353 mean train loss:  3.68397264e-03, bound:  3.15356135e-01\n",
      "Epoch: 31354 mean train loss:  3.68391094e-03, bound:  3.15356135e-01\n",
      "Epoch: 31355 mean train loss:  3.68387927e-03, bound:  3.15356135e-01\n",
      "Epoch: 31356 mean train loss:  3.68384388e-03, bound:  3.15356135e-01\n",
      "Epoch: 31357 mean train loss:  3.68381245e-03, bound:  3.15356135e-01\n",
      "Epoch: 31358 mean train loss:  3.68379406e-03, bound:  3.15356135e-01\n",
      "Epoch: 31359 mean train loss:  3.68373212e-03, bound:  3.15356135e-01\n",
      "Epoch: 31360 mean train loss:  3.68372141e-03, bound:  3.15356135e-01\n",
      "Epoch: 31361 mean train loss:  3.68361617e-03, bound:  3.15356076e-01\n",
      "Epoch: 31362 mean train loss:  3.68364225e-03, bound:  3.15356076e-01\n",
      "Epoch: 31363 mean train loss:  3.68356961e-03, bound:  3.15356076e-01\n",
      "Epoch: 31364 mean train loss:  3.68355284e-03, bound:  3.15356076e-01\n",
      "Epoch: 31365 mean train loss:  3.68353585e-03, bound:  3.15356076e-01\n",
      "Epoch: 31366 mean train loss:  3.68346833e-03, bound:  3.15356076e-01\n",
      "Epoch: 31367 mean train loss:  3.68346786e-03, bound:  3.15356076e-01\n",
      "Epoch: 31368 mean train loss:  3.68340476e-03, bound:  3.15356076e-01\n",
      "Epoch: 31369 mean train loss:  3.68334143e-03, bound:  3.15356076e-01\n",
      "Epoch: 31370 mean train loss:  3.68336425e-03, bound:  3.15356076e-01\n",
      "Epoch: 31371 mean train loss:  3.68328695e-03, bound:  3.15356076e-01\n",
      "Epoch: 31372 mean train loss:  3.68328486e-03, bound:  3.15356046e-01\n",
      "Epoch: 31373 mean train loss:  3.68324900e-03, bound:  3.15356046e-01\n",
      "Epoch: 31374 mean train loss:  3.68316751e-03, bound:  3.15356046e-01\n",
      "Epoch: 31375 mean train loss:  3.68320616e-03, bound:  3.15356046e-01\n",
      "Epoch: 31376 mean train loss:  3.68317612e-03, bound:  3.15356046e-01\n",
      "Epoch: 31377 mean train loss:  3.68311466e-03, bound:  3.15356046e-01\n",
      "Epoch: 31378 mean train loss:  3.68305645e-03, bound:  3.15356046e-01\n",
      "Epoch: 31379 mean train loss:  3.68303247e-03, bound:  3.15356046e-01\n",
      "Epoch: 31380 mean train loss:  3.68297938e-03, bound:  3.15356046e-01\n",
      "Epoch: 31381 mean train loss:  3.68295657e-03, bound:  3.15356046e-01\n",
      "Epoch: 31382 mean train loss:  3.68293235e-03, bound:  3.15356016e-01\n",
      "Epoch: 31383 mean train loss:  3.68286902e-03, bound:  3.15356016e-01\n",
      "Epoch: 31384 mean train loss:  3.68284108e-03, bound:  3.15356016e-01\n",
      "Epoch: 31385 mean train loss:  3.68280592e-03, bound:  3.15356016e-01\n",
      "Epoch: 31386 mean train loss:  3.68278194e-03, bound:  3.15356016e-01\n",
      "Epoch: 31387 mean train loss:  3.68276634e-03, bound:  3.15356016e-01\n",
      "Epoch: 31388 mean train loss:  3.68267950e-03, bound:  3.15356016e-01\n",
      "Epoch: 31389 mean train loss:  3.68263992e-03, bound:  3.15356016e-01\n",
      "Epoch: 31390 mean train loss:  3.68262874e-03, bound:  3.15356016e-01\n",
      "Epoch: 31391 mean train loss:  3.68259242e-03, bound:  3.15356016e-01\n",
      "Epoch: 31392 mean train loss:  3.68255959e-03, bound:  3.15355986e-01\n",
      "Epoch: 31393 mean train loss:  3.68255214e-03, bound:  3.15355957e-01\n",
      "Epoch: 31394 mean train loss:  3.68250837e-03, bound:  3.15355957e-01\n",
      "Epoch: 31395 mean train loss:  3.68247484e-03, bound:  3.15355957e-01\n",
      "Epoch: 31396 mean train loss:  3.68240033e-03, bound:  3.15355957e-01\n",
      "Epoch: 31397 mean train loss:  3.68239474e-03, bound:  3.15355957e-01\n",
      "Epoch: 31398 mean train loss:  3.68233654e-03, bound:  3.15355957e-01\n",
      "Epoch: 31399 mean train loss:  3.68230464e-03, bound:  3.15355957e-01\n",
      "Epoch: 31400 mean train loss:  3.68224294e-03, bound:  3.15355927e-01\n",
      "Epoch: 31401 mean train loss:  3.68226040e-03, bound:  3.15355927e-01\n",
      "Epoch: 31402 mean train loss:  3.68218566e-03, bound:  3.15355927e-01\n",
      "Epoch: 31403 mean train loss:  3.68215912e-03, bound:  3.15355927e-01\n",
      "Epoch: 31404 mean train loss:  3.68213700e-03, bound:  3.15355927e-01\n",
      "Epoch: 31405 mean train loss:  3.68210115e-03, bound:  3.15355927e-01\n",
      "Epoch: 31406 mean train loss:  3.68204364e-03, bound:  3.15355927e-01\n",
      "Epoch: 31407 mean train loss:  3.68200033e-03, bound:  3.15355927e-01\n",
      "Epoch: 31408 mean train loss:  3.68202804e-03, bound:  3.15355927e-01\n",
      "Epoch: 31409 mean train loss:  3.68196983e-03, bound:  3.15355927e-01\n",
      "Epoch: 31410 mean train loss:  3.68191930e-03, bound:  3.15355927e-01\n",
      "Epoch: 31411 mean train loss:  3.68188904e-03, bound:  3.15355927e-01\n",
      "Epoch: 31412 mean train loss:  3.68183595e-03, bound:  3.15355897e-01\n",
      "Epoch: 31413 mean train loss:  3.68179241e-03, bound:  3.15355897e-01\n",
      "Epoch: 31414 mean train loss:  3.68178147e-03, bound:  3.15355897e-01\n",
      "Epoch: 31415 mean train loss:  3.68177239e-03, bound:  3.15355897e-01\n",
      "Epoch: 31416 mean train loss:  3.68167763e-03, bound:  3.15355897e-01\n",
      "Epoch: 31417 mean train loss:  3.68166645e-03, bound:  3.15355897e-01\n",
      "Epoch: 31418 mean train loss:  3.68161313e-03, bound:  3.15355897e-01\n",
      "Epoch: 31419 mean train loss:  3.68160009e-03, bound:  3.15355867e-01\n",
      "Epoch: 31420 mean train loss:  3.68158217e-03, bound:  3.15355867e-01\n",
      "Epoch: 31421 mean train loss:  3.68152722e-03, bound:  3.15355867e-01\n",
      "Epoch: 31422 mean train loss:  3.68150813e-03, bound:  3.15355867e-01\n",
      "Epoch: 31423 mean train loss:  3.68142547e-03, bound:  3.15355837e-01\n",
      "Epoch: 31424 mean train loss:  3.68140498e-03, bound:  3.15355837e-01\n",
      "Epoch: 31425 mean train loss:  3.68135166e-03, bound:  3.15355837e-01\n",
      "Epoch: 31426 mean train loss:  3.68132838e-03, bound:  3.15355837e-01\n",
      "Epoch: 31427 mean train loss:  3.68131930e-03, bound:  3.15355837e-01\n",
      "Epoch: 31428 mean train loss:  3.68126715e-03, bound:  3.15355837e-01\n",
      "Epoch: 31429 mean train loss:  3.68125550e-03, bound:  3.15355837e-01\n",
      "Epoch: 31430 mean train loss:  3.68119148e-03, bound:  3.15355837e-01\n",
      "Epoch: 31431 mean train loss:  3.68116843e-03, bound:  3.15355837e-01\n",
      "Epoch: 31432 mean train loss:  3.68111464e-03, bound:  3.15355808e-01\n",
      "Epoch: 31433 mean train loss:  3.68108414e-03, bound:  3.15355808e-01\n",
      "Epoch: 31434 mean train loss:  3.68102710e-03, bound:  3.15355808e-01\n",
      "Epoch: 31435 mean train loss:  3.68099497e-03, bound:  3.15355808e-01\n",
      "Epoch: 31436 mean train loss:  3.68101313e-03, bound:  3.15355808e-01\n",
      "Epoch: 31437 mean train loss:  3.68095445e-03, bound:  3.15355808e-01\n",
      "Epoch: 31438 mean train loss:  3.68090160e-03, bound:  3.15355808e-01\n",
      "Epoch: 31439 mean train loss:  3.68089322e-03, bound:  3.15355808e-01\n",
      "Epoch: 31440 mean train loss:  3.68082337e-03, bound:  3.15355808e-01\n",
      "Epoch: 31441 mean train loss:  3.68080940e-03, bound:  3.15355808e-01\n",
      "Epoch: 31442 mean train loss:  3.68074980e-03, bound:  3.15355778e-01\n",
      "Epoch: 31443 mean train loss:  3.68070719e-03, bound:  3.15355778e-01\n",
      "Epoch: 31444 mean train loss:  3.68069857e-03, bound:  3.15355778e-01\n",
      "Epoch: 31445 mean train loss:  3.68060917e-03, bound:  3.15355778e-01\n",
      "Epoch: 31446 mean train loss:  3.68061475e-03, bound:  3.15355778e-01\n",
      "Epoch: 31447 mean train loss:  3.68056539e-03, bound:  3.15355778e-01\n",
      "Epoch: 31448 mean train loss:  3.68057517e-03, bound:  3.15355778e-01\n",
      "Epoch: 31449 mean train loss:  3.68048414e-03, bound:  3.15355778e-01\n",
      "Epoch: 31450 mean train loss:  3.68047459e-03, bound:  3.15355778e-01\n",
      "Epoch: 31451 mean train loss:  3.68044712e-03, bound:  3.15355748e-01\n",
      "Epoch: 31452 mean train loss:  3.68036865e-03, bound:  3.15355748e-01\n",
      "Epoch: 31453 mean train loss:  3.68037727e-03, bound:  3.15355718e-01\n",
      "Epoch: 31454 mean train loss:  3.68029997e-03, bound:  3.15355718e-01\n",
      "Epoch: 31455 mean train loss:  3.68026318e-03, bound:  3.15355718e-01\n",
      "Epoch: 31456 mean train loss:  3.68025270e-03, bound:  3.15355718e-01\n",
      "Epoch: 31457 mean train loss:  3.68019962e-03, bound:  3.15355718e-01\n",
      "Epoch: 31458 mean train loss:  3.68013186e-03, bound:  3.15355718e-01\n",
      "Epoch: 31459 mean train loss:  3.68014793e-03, bound:  3.15355718e-01\n",
      "Epoch: 31460 mean train loss:  3.68008134e-03, bound:  3.15355718e-01\n",
      "Epoch: 31461 mean train loss:  3.68009461e-03, bound:  3.15355718e-01\n",
      "Epoch: 31462 mean train loss:  3.68002360e-03, bound:  3.15355718e-01\n",
      "Epoch: 31463 mean train loss:  3.67999822e-03, bound:  3.15355718e-01\n",
      "Epoch: 31464 mean train loss:  3.67993466e-03, bound:  3.15355718e-01\n",
      "Epoch: 31465 mean train loss:  3.67989042e-03, bound:  3.15355718e-01\n",
      "Epoch: 31466 mean train loss:  3.67983826e-03, bound:  3.15355718e-01\n",
      "Epoch: 31467 mean train loss:  3.67984688e-03, bound:  3.15355718e-01\n",
      "Epoch: 31468 mean train loss:  3.67982453e-03, bound:  3.15355718e-01\n",
      "Epoch: 31469 mean train loss:  3.67977773e-03, bound:  3.15355718e-01\n",
      "Epoch: 31470 mean train loss:  3.67976469e-03, bound:  3.15355688e-01\n",
      "Epoch: 31471 mean train loss:  3.67967505e-03, bound:  3.15355629e-01\n",
      "Epoch: 31472 mean train loss:  3.67966969e-03, bound:  3.15355629e-01\n",
      "Epoch: 31473 mean train loss:  3.67965130e-03, bound:  3.15355629e-01\n",
      "Epoch: 31474 mean train loss:  3.67955910e-03, bound:  3.15355629e-01\n",
      "Epoch: 31475 mean train loss:  3.67959589e-03, bound:  3.15355629e-01\n",
      "Epoch: 31476 mean train loss:  3.67951789e-03, bound:  3.15355629e-01\n",
      "Epoch: 31477 mean train loss:  3.67945479e-03, bound:  3.15355629e-01\n",
      "Epoch: 31478 mean train loss:  3.67938098e-03, bound:  3.15355629e-01\n",
      "Epoch: 31479 mean train loss:  3.67940660e-03, bound:  3.15355629e-01\n",
      "Epoch: 31480 mean train loss:  3.67938401e-03, bound:  3.15355599e-01\n",
      "Epoch: 31481 mean train loss:  3.67933046e-03, bound:  3.15355599e-01\n",
      "Epoch: 31482 mean train loss:  3.67930508e-03, bound:  3.15355599e-01\n",
      "Epoch: 31483 mean train loss:  3.67924222e-03, bound:  3.15355599e-01\n",
      "Epoch: 31484 mean train loss:  3.67923826e-03, bound:  3.15355599e-01\n",
      "Epoch: 31485 mean train loss:  3.67917726e-03, bound:  3.15355599e-01\n",
      "Epoch: 31486 mean train loss:  3.67914094e-03, bound:  3.15355599e-01\n",
      "Epoch: 31487 mean train loss:  3.67908413e-03, bound:  3.15355599e-01\n",
      "Epoch: 31488 mean train loss:  3.67902825e-03, bound:  3.15355599e-01\n",
      "Epoch: 31489 mean train loss:  3.67904198e-03, bound:  3.15355599e-01\n",
      "Epoch: 31490 mean train loss:  3.67896841e-03, bound:  3.15355599e-01\n",
      "Epoch: 31491 mean train loss:  3.67893977e-03, bound:  3.15355599e-01\n",
      "Epoch: 31492 mean train loss:  3.67891439e-03, bound:  3.15355599e-01\n",
      "Epoch: 31493 mean train loss:  3.67884175e-03, bound:  3.15355599e-01\n",
      "Epoch: 31494 mean train loss:  3.67881474e-03, bound:  3.15355599e-01\n",
      "Epoch: 31495 mean train loss:  3.67883849e-03, bound:  3.15355599e-01\n",
      "Epoch: 31496 mean train loss:  3.67875863e-03, bound:  3.15355599e-01\n",
      "Epoch: 31497 mean train loss:  3.67868529e-03, bound:  3.15355569e-01\n",
      "Epoch: 31498 mean train loss:  3.67868110e-03, bound:  3.15355510e-01\n",
      "Epoch: 31499 mean train loss:  3.67864268e-03, bound:  3.15355569e-01\n",
      "Epoch: 31500 mean train loss:  3.67866969e-03, bound:  3.15355510e-01\n",
      "Epoch: 31501 mean train loss:  3.67860729e-03, bound:  3.15355510e-01\n",
      "Epoch: 31502 mean train loss:  3.67851206e-03, bound:  3.15355510e-01\n",
      "Epoch: 31503 mean train loss:  3.67852394e-03, bound:  3.15355510e-01\n",
      "Epoch: 31504 mean train loss:  3.67848482e-03, bound:  3.15355510e-01\n",
      "Epoch: 31505 mean train loss:  3.67842382e-03, bound:  3.15355510e-01\n",
      "Epoch: 31506 mean train loss:  3.67843639e-03, bound:  3.15355510e-01\n",
      "Epoch: 31507 mean train loss:  3.67834466e-03, bound:  3.15355510e-01\n",
      "Epoch: 31508 mean train loss:  3.67830927e-03, bound:  3.15355510e-01\n",
      "Epoch: 31509 mean train loss:  3.67824291e-03, bound:  3.15355510e-01\n",
      "Epoch: 31510 mean train loss:  3.67820472e-03, bound:  3.15355510e-01\n",
      "Epoch: 31511 mean train loss:  3.67818866e-03, bound:  3.15355510e-01\n",
      "Epoch: 31512 mean train loss:  3.67821031e-03, bound:  3.15355510e-01\n",
      "Epoch: 31513 mean train loss:  3.67815257e-03, bound:  3.15355510e-01\n",
      "Epoch: 31514 mean train loss:  3.67810577e-03, bound:  3.15355510e-01\n",
      "Epoch: 31515 mean train loss:  3.67809343e-03, bound:  3.15355510e-01\n",
      "Epoch: 31516 mean train loss:  3.67803639e-03, bound:  3.15355510e-01\n",
      "Epoch: 31517 mean train loss:  3.67800216e-03, bound:  3.15355480e-01\n",
      "Epoch: 31518 mean train loss:  3.67792719e-03, bound:  3.15355480e-01\n",
      "Epoch: 31519 mean train loss:  3.67792230e-03, bound:  3.15355480e-01\n",
      "Epoch: 31520 mean train loss:  3.67788528e-03, bound:  3.15355480e-01\n",
      "Epoch: 31521 mean train loss:  3.67777771e-03, bound:  3.15355480e-01\n",
      "Epoch: 31522 mean train loss:  3.67781427e-03, bound:  3.15355480e-01\n",
      "Epoch: 31523 mean train loss:  3.67776002e-03, bound:  3.15355480e-01\n",
      "Epoch: 31524 mean train loss:  3.67771694e-03, bound:  3.15355480e-01\n",
      "Epoch: 31525 mean train loss:  3.67765827e-03, bound:  3.15355480e-01\n",
      "Epoch: 31526 mean train loss:  3.67764616e-03, bound:  3.15355450e-01\n",
      "Epoch: 31527 mean train loss:  3.67758353e-03, bound:  3.15355420e-01\n",
      "Epoch: 31528 mean train loss:  3.67760262e-03, bound:  3.15355420e-01\n",
      "Epoch: 31529 mean train loss:  3.67757771e-03, bound:  3.15355420e-01\n",
      "Epoch: 31530 mean train loss:  3.67748621e-03, bound:  3.15355420e-01\n",
      "Epoch: 31531 mean train loss:  3.67747503e-03, bound:  3.15355420e-01\n",
      "Epoch: 31532 mean train loss:  3.67744220e-03, bound:  3.15355420e-01\n",
      "Epoch: 31533 mean train loss:  3.67740123e-03, bound:  3.15355420e-01\n",
      "Epoch: 31534 mean train loss:  3.67735163e-03, bound:  3.15355420e-01\n",
      "Epoch: 31535 mean train loss:  3.67732090e-03, bound:  3.15355420e-01\n",
      "Epoch: 31536 mean train loss:  3.67729133e-03, bound:  3.15355420e-01\n",
      "Epoch: 31537 mean train loss:  3.67722940e-03, bound:  3.15355390e-01\n",
      "Epoch: 31538 mean train loss:  3.67721682e-03, bound:  3.15355390e-01\n",
      "Epoch: 31539 mean train loss:  3.67711461e-03, bound:  3.15355390e-01\n",
      "Epoch: 31540 mean train loss:  3.67709808e-03, bound:  3.15355390e-01\n",
      "Epoch: 31541 mean train loss:  3.67707084e-03, bound:  3.15355390e-01\n",
      "Epoch: 31542 mean train loss:  3.67706083e-03, bound:  3.15355390e-01\n",
      "Epoch: 31543 mean train loss:  3.67701217e-03, bound:  3.15355390e-01\n",
      "Epoch: 31544 mean train loss:  3.67700262e-03, bound:  3.15355361e-01\n",
      "Epoch: 31545 mean train loss:  3.67696420e-03, bound:  3.15355361e-01\n",
      "Epoch: 31546 mean train loss:  3.67693254e-03, bound:  3.15355361e-01\n",
      "Epoch: 31547 mean train loss:  3.67686618e-03, bound:  3.15355361e-01\n",
      "Epoch: 31548 mean train loss:  3.67683684e-03, bound:  3.15355361e-01\n",
      "Epoch: 31549 mean train loss:  3.67681193e-03, bound:  3.15355361e-01\n",
      "Epoch: 31550 mean train loss:  3.67674092e-03, bound:  3.15355361e-01\n",
      "Epoch: 31551 mean train loss:  3.67675046e-03, bound:  3.15355361e-01\n",
      "Epoch: 31552 mean train loss:  3.67669179e-03, bound:  3.15355361e-01\n",
      "Epoch: 31553 mean train loss:  3.67664383e-03, bound:  3.15355361e-01\n",
      "Epoch: 31554 mean train loss:  3.67660332e-03, bound:  3.15355331e-01\n",
      "Epoch: 31555 mean train loss:  3.67658352e-03, bound:  3.15355301e-01\n",
      "Epoch: 31556 mean train loss:  3.67652904e-03, bound:  3.15355301e-01\n",
      "Epoch: 31557 mean train loss:  3.67647898e-03, bound:  3.15355301e-01\n",
      "Epoch: 31558 mean train loss:  3.67646222e-03, bound:  3.15355301e-01\n",
      "Epoch: 31559 mean train loss:  3.67640029e-03, bound:  3.15355301e-01\n",
      "Epoch: 31560 mean train loss:  3.67634208e-03, bound:  3.15355301e-01\n",
      "Epoch: 31561 mean train loss:  3.67630250e-03, bound:  3.15355301e-01\n",
      "Epoch: 31562 mean train loss:  3.67628783e-03, bound:  3.15355301e-01\n",
      "Epoch: 31563 mean train loss:  3.67626897e-03, bound:  3.15355301e-01\n",
      "Epoch: 31564 mean train loss:  3.67620448e-03, bound:  3.15355301e-01\n",
      "Epoch: 31565 mean train loss:  3.67619144e-03, bound:  3.15355301e-01\n",
      "Epoch: 31566 mean train loss:  3.67610063e-03, bound:  3.15355271e-01\n",
      "Epoch: 31567 mean train loss:  3.67608760e-03, bound:  3.15355271e-01\n",
      "Epoch: 31568 mean train loss:  3.67608946e-03, bound:  3.15355271e-01\n",
      "Epoch: 31569 mean train loss:  3.67598259e-03, bound:  3.15355271e-01\n",
      "Epoch: 31570 mean train loss:  3.67597491e-03, bound:  3.15355271e-01\n",
      "Epoch: 31571 mean train loss:  3.67594766e-03, bound:  3.15355271e-01\n",
      "Epoch: 31572 mean train loss:  3.67590552e-03, bound:  3.15355241e-01\n",
      "Epoch: 31573 mean train loss:  3.67585267e-03, bound:  3.15355241e-01\n",
      "Epoch: 31574 mean train loss:  3.67579050e-03, bound:  3.15355241e-01\n",
      "Epoch: 31575 mean train loss:  3.67578398e-03, bound:  3.15355241e-01\n",
      "Epoch: 31576 mean train loss:  3.67575744e-03, bound:  3.15355241e-01\n",
      "Epoch: 31577 mean train loss:  3.67571460e-03, bound:  3.15355241e-01\n",
      "Epoch: 31578 mean train loss:  3.67570156e-03, bound:  3.15355241e-01\n",
      "Epoch: 31579 mean train loss:  3.67563963e-03, bound:  3.15355241e-01\n",
      "Epoch: 31580 mean train loss:  3.67561844e-03, bound:  3.15355241e-01\n",
      "Epoch: 31581 mean train loss:  3.67554440e-03, bound:  3.15355182e-01\n",
      "Epoch: 31582 mean train loss:  3.67551576e-03, bound:  3.15355182e-01\n",
      "Epoch: 31583 mean train loss:  3.67550994e-03, bound:  3.15355182e-01\n",
      "Epoch: 31584 mean train loss:  3.67550668e-03, bound:  3.15355182e-01\n",
      "Epoch: 31585 mean train loss:  3.67541518e-03, bound:  3.15355182e-01\n",
      "Epoch: 31586 mean train loss:  3.67536978e-03, bound:  3.15355182e-01\n",
      "Epoch: 31587 mean train loss:  3.67531320e-03, bound:  3.15355182e-01\n",
      "Epoch: 31588 mean train loss:  3.67533020e-03, bound:  3.15355182e-01\n",
      "Epoch: 31589 mean train loss:  3.67528526e-03, bound:  3.15355182e-01\n",
      "Epoch: 31590 mean train loss:  3.67519795e-03, bound:  3.15355182e-01\n",
      "Epoch: 31591 mean train loss:  3.67516815e-03, bound:  3.15355182e-01\n",
      "Epoch: 31592 mean train loss:  3.67520633e-03, bound:  3.15355182e-01\n",
      "Epoch: 31593 mean train loss:  3.67510458e-03, bound:  3.15355152e-01\n",
      "Epoch: 31594 mean train loss:  3.67504149e-03, bound:  3.15355152e-01\n",
      "Epoch: 31595 mean train loss:  3.67501727e-03, bound:  3.15355152e-01\n",
      "Epoch: 31596 mean train loss:  3.67502635e-03, bound:  3.15355152e-01\n",
      "Epoch: 31597 mean train loss:  3.67498258e-03, bound:  3.15355152e-01\n",
      "Epoch: 31598 mean train loss:  3.67492996e-03, bound:  3.15355152e-01\n",
      "Epoch: 31599 mean train loss:  3.67486081e-03, bound:  3.15355152e-01\n",
      "Epoch: 31600 mean train loss:  3.67483310e-03, bound:  3.15355122e-01\n",
      "Epoch: 31601 mean train loss:  3.67477979e-03, bound:  3.15355122e-01\n",
      "Epoch: 31602 mean train loss:  3.67473974e-03, bound:  3.15355122e-01\n",
      "Epoch: 31603 mean train loss:  3.67471483e-03, bound:  3.15355122e-01\n",
      "Epoch: 31604 mean train loss:  3.67470342e-03, bound:  3.15355122e-01\n",
      "Epoch: 31605 mean train loss:  3.67463403e-03, bound:  3.15355122e-01\n",
      "Epoch: 31606 mean train loss:  3.67463985e-03, bound:  3.15355122e-01\n",
      "Epoch: 31607 mean train loss:  3.67455999e-03, bound:  3.15355122e-01\n",
      "Epoch: 31608 mean train loss:  3.67450458e-03, bound:  3.15355122e-01\n",
      "Epoch: 31609 mean train loss:  3.67444544e-03, bound:  3.15355062e-01\n",
      "Epoch: 31610 mean train loss:  3.67444288e-03, bound:  3.15355062e-01\n",
      "Epoch: 31611 mean train loss:  3.67443706e-03, bound:  3.15355062e-01\n",
      "Epoch: 31612 mean train loss:  3.67437094e-03, bound:  3.15355062e-01\n",
      "Epoch: 31613 mean train loss:  3.67433089e-03, bound:  3.15355062e-01\n",
      "Epoch: 31614 mean train loss:  3.67431063e-03, bound:  3.15355062e-01\n",
      "Epoch: 31615 mean train loss:  3.67425452e-03, bound:  3.15355062e-01\n",
      "Epoch: 31616 mean train loss:  3.67426523e-03, bound:  3.15355062e-01\n",
      "Epoch: 31617 mean train loss:  3.67416604e-03, bound:  3.15355062e-01\n",
      "Epoch: 31618 mean train loss:  3.67412553e-03, bound:  3.15355033e-01\n",
      "Epoch: 31619 mean train loss:  3.67410714e-03, bound:  3.15355033e-01\n",
      "Epoch: 31620 mean train loss:  3.67407617e-03, bound:  3.15355033e-01\n",
      "Epoch: 31621 mean train loss:  3.67406337e-03, bound:  3.15355033e-01\n",
      "Epoch: 31622 mean train loss:  3.67402728e-03, bound:  3.15355033e-01\n",
      "Epoch: 31623 mean train loss:  3.67397070e-03, bound:  3.15355033e-01\n",
      "Epoch: 31624 mean train loss:  3.67392949e-03, bound:  3.15355033e-01\n",
      "Epoch: 31625 mean train loss:  3.67387757e-03, bound:  3.15355033e-01\n",
      "Epoch: 31626 mean train loss:  3.67386150e-03, bound:  3.15355033e-01\n",
      "Epoch: 31627 mean train loss:  3.67379352e-03, bound:  3.15355033e-01\n",
      "Epoch: 31628 mean train loss:  3.67376138e-03, bound:  3.15355033e-01\n",
      "Epoch: 31629 mean train loss:  3.67371528e-03, bound:  3.15355033e-01\n",
      "Epoch: 31630 mean train loss:  3.67365638e-03, bound:  3.15355033e-01\n",
      "Epoch: 31631 mean train loss:  3.67359072e-03, bound:  3.15355033e-01\n",
      "Epoch: 31632 mean train loss:  3.67361633e-03, bound:  3.15355033e-01\n",
      "Epoch: 31633 mean train loss:  3.67354951e-03, bound:  3.15355033e-01\n",
      "Epoch: 31634 mean train loss:  3.67354532e-03, bound:  3.15355033e-01\n",
      "Epoch: 31635 mean train loss:  3.67347733e-03, bound:  3.15355003e-01\n",
      "Epoch: 31636 mean train loss:  3.67343985e-03, bound:  3.15354943e-01\n",
      "Epoch: 31637 mean train loss:  3.67344054e-03, bound:  3.15354943e-01\n",
      "Epoch: 31638 mean train loss:  3.67339049e-03, bound:  3.15354943e-01\n",
      "Epoch: 31639 mean train loss:  3.67335859e-03, bound:  3.15354943e-01\n",
      "Epoch: 31640 mean train loss:  3.67334276e-03, bound:  3.15354943e-01\n",
      "Epoch: 31641 mean train loss:  3.67331435e-03, bound:  3.15354943e-01\n",
      "Epoch: 31642 mean train loss:  3.67320492e-03, bound:  3.15354943e-01\n",
      "Epoch: 31643 mean train loss:  3.67318443e-03, bound:  3.15354943e-01\n",
      "Epoch: 31644 mean train loss:  3.67312366e-03, bound:  3.15354943e-01\n",
      "Epoch: 31645 mean train loss:  3.67314834e-03, bound:  3.15354943e-01\n",
      "Epoch: 31646 mean train loss:  3.67310224e-03, bound:  3.15354913e-01\n",
      "Epoch: 31647 mean train loss:  3.67303286e-03, bound:  3.15354913e-01\n",
      "Epoch: 31648 mean train loss:  3.67300166e-03, bound:  3.15354913e-01\n",
      "Epoch: 31649 mean train loss:  3.67292459e-03, bound:  3.15354913e-01\n",
      "Epoch: 31650 mean train loss:  3.67289293e-03, bound:  3.15354913e-01\n",
      "Epoch: 31651 mean train loss:  3.67289479e-03, bound:  3.15354913e-01\n",
      "Epoch: 31652 mean train loss:  3.67286126e-03, bound:  3.15354913e-01\n",
      "Epoch: 31653 mean train loss:  3.67280864e-03, bound:  3.15354913e-01\n",
      "Epoch: 31654 mean train loss:  3.67276184e-03, bound:  3.15354913e-01\n",
      "Epoch: 31655 mean train loss:  3.67272599e-03, bound:  3.15354913e-01\n",
      "Epoch: 31656 mean train loss:  3.67270107e-03, bound:  3.15354913e-01\n",
      "Epoch: 31657 mean train loss:  3.67270084e-03, bound:  3.15354913e-01\n",
      "Epoch: 31658 mean train loss:  3.67257698e-03, bound:  3.15354913e-01\n",
      "Epoch: 31659 mean train loss:  3.67259001e-03, bound:  3.15354913e-01\n",
      "Epoch: 31660 mean train loss:  3.67255206e-03, bound:  3.15354913e-01\n",
      "Epoch: 31661 mean train loss:  3.67247220e-03, bound:  3.15354913e-01\n",
      "Epoch: 31662 mean train loss:  3.67247663e-03, bound:  3.15354884e-01\n",
      "Epoch: 31663 mean train loss:  3.67241912e-03, bound:  3.15354824e-01\n",
      "Epoch: 31664 mean train loss:  3.67237302e-03, bound:  3.15354824e-01\n",
      "Epoch: 31665 mean train loss:  3.67231760e-03, bound:  3.15354824e-01\n",
      "Epoch: 31666 mean train loss:  3.67226498e-03, bound:  3.15354824e-01\n",
      "Epoch: 31667 mean train loss:  3.67227243e-03, bound:  3.15354824e-01\n",
      "Epoch: 31668 mean train loss:  3.67224379e-03, bound:  3.15354824e-01\n",
      "Epoch: 31669 mean train loss:  3.67213273e-03, bound:  3.15354794e-01\n",
      "Epoch: 31670 mean train loss:  3.67215578e-03, bound:  3.15354794e-01\n",
      "Epoch: 31671 mean train loss:  3.67211434e-03, bound:  3.15354794e-01\n",
      "Epoch: 31672 mean train loss:  3.67208058e-03, bound:  3.15354794e-01\n",
      "Epoch: 31673 mean train loss:  3.67202098e-03, bound:  3.15354794e-01\n",
      "Epoch: 31674 mean train loss:  3.67197208e-03, bound:  3.15354794e-01\n",
      "Epoch: 31675 mean train loss:  3.67193297e-03, bound:  3.15354794e-01\n",
      "Epoch: 31676 mean train loss:  3.67190992e-03, bound:  3.15354794e-01\n",
      "Epoch: 31677 mean train loss:  3.67187476e-03, bound:  3.15354794e-01\n",
      "Epoch: 31678 mean train loss:  3.67181655e-03, bound:  3.15354794e-01\n",
      "Epoch: 31679 mean train loss:  3.67179397e-03, bound:  3.15354794e-01\n",
      "Epoch: 31680 mean train loss:  3.67176952e-03, bound:  3.15354794e-01\n",
      "Epoch: 31681 mean train loss:  3.67168989e-03, bound:  3.15354794e-01\n",
      "Epoch: 31682 mean train loss:  3.67163471e-03, bound:  3.15354794e-01\n",
      "Epoch: 31683 mean train loss:  3.67163494e-03, bound:  3.15354794e-01\n",
      "Epoch: 31684 mean train loss:  3.67160654e-03, bound:  3.15354794e-01\n",
      "Epoch: 31685 mean train loss:  3.67154623e-03, bound:  3.15354794e-01\n",
      "Epoch: 31686 mean train loss:  3.67151457e-03, bound:  3.15354794e-01\n",
      "Epoch: 31687 mean train loss:  3.67145939e-03, bound:  3.15354764e-01\n",
      "Epoch: 31688 mean train loss:  3.67144821e-03, bound:  3.15354764e-01\n",
      "Epoch: 31689 mean train loss:  3.67137883e-03, bound:  3.15354735e-01\n",
      "Epoch: 31690 mean train loss:  3.67134111e-03, bound:  3.15354735e-01\n",
      "Epoch: 31691 mean train loss:  3.67127522e-03, bound:  3.15354735e-01\n",
      "Epoch: 31692 mean train loss:  3.67127219e-03, bound:  3.15354735e-01\n",
      "Epoch: 31693 mean train loss:  3.67123564e-03, bound:  3.15354735e-01\n",
      "Epoch: 31694 mean train loss:  3.67119489e-03, bound:  3.15354735e-01\n",
      "Epoch: 31695 mean train loss:  3.67120607e-03, bound:  3.15354705e-01\n",
      "Epoch: 31696 mean train loss:  3.67112458e-03, bound:  3.15354705e-01\n",
      "Epoch: 31697 mean train loss:  3.67110060e-03, bound:  3.15354705e-01\n",
      "Epoch: 31698 mean train loss:  3.67102609e-03, bound:  3.15354705e-01\n",
      "Epoch: 31699 mean train loss:  3.67097580e-03, bound:  3.15354705e-01\n",
      "Epoch: 31700 mean train loss:  3.67090642e-03, bound:  3.15354705e-01\n",
      "Epoch: 31701 mean train loss:  3.67093203e-03, bound:  3.15354705e-01\n",
      "Epoch: 31702 mean train loss:  3.67087056e-03, bound:  3.15354705e-01\n",
      "Epoch: 31703 mean train loss:  3.67082260e-03, bound:  3.15354705e-01\n",
      "Epoch: 31704 mean train loss:  3.67078208e-03, bound:  3.15354705e-01\n",
      "Epoch: 31705 mean train loss:  3.67071014e-03, bound:  3.15354675e-01\n",
      "Epoch: 31706 mean train loss:  3.67073249e-03, bound:  3.15354675e-01\n",
      "Epoch: 31707 mean train loss:  3.67065961e-03, bound:  3.15354675e-01\n",
      "Epoch: 31708 mean train loss:  3.67064658e-03, bound:  3.15354675e-01\n",
      "Epoch: 31709 mean train loss:  3.67053947e-03, bound:  3.15354675e-01\n",
      "Epoch: 31710 mean train loss:  3.67055531e-03, bound:  3.15354675e-01\n",
      "Epoch: 31711 mean train loss:  3.67056066e-03, bound:  3.15354675e-01\n",
      "Epoch: 31712 mean train loss:  3.67048755e-03, bound:  3.15354615e-01\n",
      "Epoch: 31713 mean train loss:  3.67040629e-03, bound:  3.15354615e-01\n",
      "Epoch: 31714 mean train loss:  3.67039139e-03, bound:  3.15354615e-01\n",
      "Epoch: 31715 mean train loss:  3.67038441e-03, bound:  3.15354615e-01\n",
      "Epoch: 31716 mean train loss:  3.67033971e-03, bound:  3.15354615e-01\n",
      "Epoch: 31717 mean train loss:  3.67030269e-03, bound:  3.15354615e-01\n",
      "Epoch: 31718 mean train loss:  3.67021048e-03, bound:  3.15354615e-01\n",
      "Epoch: 31719 mean train loss:  3.67016299e-03, bound:  3.15354615e-01\n",
      "Epoch: 31720 mean train loss:  3.67018534e-03, bound:  3.15354615e-01\n",
      "Epoch: 31721 mean train loss:  3.67013807e-03, bound:  3.15354586e-01\n",
      "Epoch: 31722 mean train loss:  3.67007544e-03, bound:  3.15354586e-01\n",
      "Epoch: 31723 mean train loss:  3.67001374e-03, bound:  3.15354586e-01\n",
      "Epoch: 31724 mean train loss:  3.66998301e-03, bound:  3.15354586e-01\n",
      "Epoch: 31725 mean train loss:  3.66992899e-03, bound:  3.15354586e-01\n",
      "Epoch: 31726 mean train loss:  3.66988196e-03, bound:  3.15354586e-01\n",
      "Epoch: 31727 mean train loss:  3.66990245e-03, bound:  3.15354586e-01\n",
      "Epoch: 31728 mean train loss:  3.66985006e-03, bound:  3.15354586e-01\n",
      "Epoch: 31729 mean train loss:  3.66978813e-03, bound:  3.15354586e-01\n",
      "Epoch: 31730 mean train loss:  3.66975227e-03, bound:  3.15354586e-01\n",
      "Epoch: 31731 mean train loss:  3.66969616e-03, bound:  3.15354556e-01\n",
      "Epoch: 31732 mean train loss:  3.66969150e-03, bound:  3.15354556e-01\n",
      "Epoch: 31733 mean train loss:  3.66966729e-03, bound:  3.15354556e-01\n",
      "Epoch: 31734 mean train loss:  3.66960536e-03, bound:  3.15354556e-01\n",
      "Epoch: 31735 mean train loss:  3.66956042e-03, bound:  3.15354556e-01\n",
      "Epoch: 31736 mean train loss:  3.66955623e-03, bound:  3.15354556e-01\n",
      "Epoch: 31737 mean train loss:  3.66947544e-03, bound:  3.15354556e-01\n",
      "Epoch: 31738 mean train loss:  3.66940675e-03, bound:  3.15354556e-01\n",
      "Epoch: 31739 mean train loss:  3.66939907e-03, bound:  3.15354496e-01\n",
      "Epoch: 31740 mean train loss:  3.66935763e-03, bound:  3.15354496e-01\n",
      "Epoch: 31741 mean train loss:  3.66935064e-03, bound:  3.15354496e-01\n",
      "Epoch: 31742 mean train loss:  3.66928382e-03, bound:  3.15354496e-01\n",
      "Epoch: 31743 mean train loss:  3.66919907e-03, bound:  3.15354496e-01\n",
      "Epoch: 31744 mean train loss:  3.66918230e-03, bound:  3.15354496e-01\n",
      "Epoch: 31745 mean train loss:  3.66915623e-03, bound:  3.15354496e-01\n",
      "Epoch: 31746 mean train loss:  3.66909080e-03, bound:  3.15354496e-01\n",
      "Epoch: 31747 mean train loss:  3.66906216e-03, bound:  3.15354496e-01\n",
      "Epoch: 31748 mean train loss:  3.66903376e-03, bound:  3.15354496e-01\n",
      "Epoch: 31749 mean train loss:  3.66897415e-03, bound:  3.15354466e-01\n",
      "Epoch: 31750 mean train loss:  3.66894784e-03, bound:  3.15354496e-01\n",
      "Epoch: 31751 mean train loss:  3.66892386e-03, bound:  3.15354466e-01\n",
      "Epoch: 31752 mean train loss:  3.66888708e-03, bound:  3.15354466e-01\n",
      "Epoch: 31753 mean train loss:  3.66879907e-03, bound:  3.15354466e-01\n",
      "Epoch: 31754 mean train loss:  3.66877695e-03, bound:  3.15354466e-01\n",
      "Epoch: 31755 mean train loss:  3.66869569e-03, bound:  3.15354466e-01\n",
      "Epoch: 31756 mean train loss:  3.66870058e-03, bound:  3.15354466e-01\n",
      "Epoch: 31757 mean train loss:  3.66863329e-03, bound:  3.15354437e-01\n",
      "Epoch: 31758 mean train loss:  3.66866565e-03, bound:  3.15354437e-01\n",
      "Epoch: 31759 mean train loss:  3.66861001e-03, bound:  3.15354437e-01\n",
      "Epoch: 31760 mean train loss:  3.66853061e-03, bound:  3.15354437e-01\n",
      "Epoch: 31761 mean train loss:  3.66850826e-03, bound:  3.15354437e-01\n",
      "Epoch: 31762 mean train loss:  3.66844912e-03, bound:  3.15354437e-01\n",
      "Epoch: 31763 mean train loss:  3.66843329e-03, bound:  3.15354437e-01\n",
      "Epoch: 31764 mean train loss:  3.66842467e-03, bound:  3.15354437e-01\n",
      "Epoch: 31765 mean train loss:  3.66833759e-03, bound:  3.15354437e-01\n",
      "Epoch: 31766 mean train loss:  3.66830709e-03, bound:  3.15354377e-01\n",
      "Epoch: 31767 mean train loss:  3.66829406e-03, bound:  3.15354377e-01\n",
      "Epoch: 31768 mean train loss:  3.66822048e-03, bound:  3.15354377e-01\n",
      "Epoch: 31769 mean train loss:  3.66815715e-03, bound:  3.15354377e-01\n",
      "Epoch: 31770 mean train loss:  3.66816623e-03, bound:  3.15354377e-01\n",
      "Epoch: 31771 mean train loss:  3.66810476e-03, bound:  3.15354377e-01\n",
      "Epoch: 31772 mean train loss:  3.66805331e-03, bound:  3.15354377e-01\n",
      "Epoch: 31773 mean train loss:  3.66804027e-03, bound:  3.15354377e-01\n",
      "Epoch: 31774 mean train loss:  3.66796111e-03, bound:  3.15354377e-01\n",
      "Epoch: 31775 mean train loss:  3.66796250e-03, bound:  3.15354377e-01\n",
      "Epoch: 31776 mean train loss:  3.66789591e-03, bound:  3.15354347e-01\n",
      "Epoch: 31777 mean train loss:  3.66788288e-03, bound:  3.15354347e-01\n",
      "Epoch: 31778 mean train loss:  3.66777973e-03, bound:  3.15354347e-01\n",
      "Epoch: 31779 mean train loss:  3.66778462e-03, bound:  3.15354347e-01\n",
      "Epoch: 31780 mean train loss:  3.66770499e-03, bound:  3.15354347e-01\n",
      "Epoch: 31781 mean train loss:  3.66766052e-03, bound:  3.15354347e-01\n",
      "Epoch: 31782 mean train loss:  3.66764050e-03, bound:  3.15354347e-01\n",
      "Epoch: 31783 mean train loss:  3.66763468e-03, bound:  3.15354347e-01\n",
      "Epoch: 31784 mean train loss:  3.66758602e-03, bound:  3.15354317e-01\n",
      "Epoch: 31785 mean train loss:  3.66750592e-03, bound:  3.15354317e-01\n",
      "Epoch: 31786 mean train loss:  3.66745261e-03, bound:  3.15354317e-01\n",
      "Epoch: 31787 mean train loss:  3.66744399e-03, bound:  3.15354317e-01\n",
      "Epoch: 31788 mean train loss:  3.66745633e-03, bound:  3.15354317e-01\n",
      "Epoch: 31789 mean train loss:  3.66739137e-03, bound:  3.15354317e-01\n",
      "Epoch: 31790 mean train loss:  3.66733666e-03, bound:  3.15354317e-01\n",
      "Epoch: 31791 mean train loss:  3.66731966e-03, bound:  3.15354317e-01\n",
      "Epoch: 31792 mean train loss:  3.66723631e-03, bound:  3.15354317e-01\n",
      "Epoch: 31793 mean train loss:  3.66721372e-03, bound:  3.15354317e-01\n",
      "Epoch: 31794 mean train loss:  3.66714387e-03, bound:  3.15354258e-01\n",
      "Epoch: 31795 mean train loss:  3.66708823e-03, bound:  3.15354258e-01\n",
      "Epoch: 31796 mean train loss:  3.66709568e-03, bound:  3.15354258e-01\n",
      "Epoch: 31797 mean train loss:  3.66702070e-03, bound:  3.15354258e-01\n",
      "Epoch: 31798 mean train loss:  3.66701116e-03, bound:  3.15354258e-01\n",
      "Epoch: 31799 mean train loss:  3.66698368e-03, bound:  3.15354258e-01\n",
      "Epoch: 31800 mean train loss:  3.66691872e-03, bound:  3.15354258e-01\n",
      "Epoch: 31801 mean train loss:  3.66690662e-03, bound:  3.15354228e-01\n",
      "Epoch: 31802 mean train loss:  3.66683700e-03, bound:  3.15354228e-01\n",
      "Epoch: 31803 mean train loss:  3.66679602e-03, bound:  3.15354228e-01\n",
      "Epoch: 31804 mean train loss:  3.66677600e-03, bound:  3.15354228e-01\n",
      "Epoch: 31805 mean train loss:  3.66671151e-03, bound:  3.15354228e-01\n",
      "Epoch: 31806 mean train loss:  3.66665004e-03, bound:  3.15354228e-01\n",
      "Epoch: 31807 mean train loss:  3.66662559e-03, bound:  3.15354228e-01\n",
      "Epoch: 31808 mean train loss:  3.66659067e-03, bound:  3.15354228e-01\n",
      "Epoch: 31809 mean train loss:  3.66655830e-03, bound:  3.15354228e-01\n",
      "Epoch: 31810 mean train loss:  3.66651453e-03, bound:  3.15354228e-01\n",
      "Epoch: 31811 mean train loss:  3.66642815e-03, bound:  3.15354168e-01\n",
      "Epoch: 31812 mean train loss:  3.66644841e-03, bound:  3.15354168e-01\n",
      "Epoch: 31813 mean train loss:  3.66637320e-03, bound:  3.15354168e-01\n",
      "Epoch: 31814 mean train loss:  3.66631732e-03, bound:  3.15354168e-01\n",
      "Epoch: 31815 mean train loss:  3.66629334e-03, bound:  3.15354168e-01\n",
      "Epoch: 31816 mean train loss:  3.66627448e-03, bound:  3.15354168e-01\n",
      "Epoch: 31817 mean train loss:  3.66621045e-03, bound:  3.15354168e-01\n",
      "Epoch: 31818 mean train loss:  3.66614410e-03, bound:  3.15354139e-01\n",
      "Epoch: 31819 mean train loss:  3.66610521e-03, bound:  3.15354168e-01\n",
      "Epoch: 31820 mean train loss:  3.66610801e-03, bound:  3.15354139e-01\n",
      "Epoch: 31821 mean train loss:  3.66610033e-03, bound:  3.15354139e-01\n",
      "Epoch: 31822 mean train loss:  3.66603979e-03, bound:  3.15354139e-01\n",
      "Epoch: 31823 mean train loss:  3.66593990e-03, bound:  3.15354139e-01\n",
      "Epoch: 31824 mean train loss:  3.66589869e-03, bound:  3.15354139e-01\n",
      "Epoch: 31825 mean train loss:  3.66587378e-03, bound:  3.15354139e-01\n",
      "Epoch: 31826 mean train loss:  3.66584584e-03, bound:  3.15354139e-01\n",
      "Epoch: 31827 mean train loss:  3.66579718e-03, bound:  3.15354139e-01\n",
      "Epoch: 31828 mean train loss:  3.66575876e-03, bound:  3.15354109e-01\n",
      "Epoch: 31829 mean train loss:  3.66575131e-03, bound:  3.15354109e-01\n",
      "Epoch: 31830 mean train loss:  3.66565608e-03, bound:  3.15354109e-01\n",
      "Epoch: 31831 mean train loss:  3.66560230e-03, bound:  3.15354109e-01\n",
      "Epoch: 31832 mean train loss:  3.66556016e-03, bound:  3.15354109e-01\n",
      "Epoch: 31833 mean train loss:  3.66556807e-03, bound:  3.15354109e-01\n",
      "Epoch: 31834 mean train loss:  3.66552756e-03, bound:  3.15354109e-01\n",
      "Epoch: 31835 mean train loss:  3.66545934e-03, bound:  3.15354109e-01\n",
      "Epoch: 31836 mean train loss:  3.66545841e-03, bound:  3.15354109e-01\n",
      "Epoch: 31837 mean train loss:  3.66537273e-03, bound:  3.15354109e-01\n",
      "Epoch: 31838 mean train loss:  3.66535806e-03, bound:  3.15354109e-01\n",
      "Epoch: 31839 mean train loss:  3.66528868e-03, bound:  3.15354109e-01\n",
      "Epoch: 31840 mean train loss:  3.66526050e-03, bound:  3.15354109e-01\n",
      "Epoch: 31841 mean train loss:  3.66520439e-03, bound:  3.15354109e-01\n",
      "Epoch: 31842 mean train loss:  3.66519764e-03, bound:  3.15354109e-01\n",
      "Epoch: 31843 mean train loss:  3.66512104e-03, bound:  3.15354109e-01\n",
      "Epoch: 31844 mean train loss:  3.66512057e-03, bound:  3.15354049e-01\n",
      "Epoch: 31845 mean train loss:  3.66506819e-03, bound:  3.15354049e-01\n",
      "Epoch: 31846 mean train loss:  3.66500835e-03, bound:  3.15354049e-01\n",
      "Epoch: 31847 mean train loss:  3.66499461e-03, bound:  3.15354049e-01\n",
      "Epoch: 31848 mean train loss:  3.66492895e-03, bound:  3.15354049e-01\n",
      "Epoch: 31849 mean train loss:  3.66489030e-03, bound:  3.15354049e-01\n",
      "Epoch: 31850 mean train loss:  3.66483675e-03, bound:  3.15354049e-01\n",
      "Epoch: 31851 mean train loss:  3.66476574e-03, bound:  3.15354019e-01\n",
      "Epoch: 31852 mean train loss:  3.66473920e-03, bound:  3.15354019e-01\n",
      "Epoch: 31853 mean train loss:  3.66476248e-03, bound:  3.15353990e-01\n",
      "Epoch: 31854 mean train loss:  3.66467307e-03, bound:  3.15353990e-01\n",
      "Epoch: 31855 mean train loss:  3.66464700e-03, bound:  3.15353990e-01\n",
      "Epoch: 31856 mean train loss:  3.66460648e-03, bound:  3.15353990e-01\n",
      "Epoch: 31857 mean train loss:  3.66458227e-03, bound:  3.15353990e-01\n",
      "Epoch: 31858 mean train loss:  3.66452872e-03, bound:  3.15353990e-01\n",
      "Epoch: 31859 mean train loss:  3.66446865e-03, bound:  3.15353990e-01\n",
      "Epoch: 31860 mean train loss:  3.66445538e-03, bound:  3.15353990e-01\n",
      "Epoch: 31861 mean train loss:  3.66433337e-03, bound:  3.15353990e-01\n",
      "Epoch: 31862 mean train loss:  3.66434106e-03, bound:  3.15353990e-01\n",
      "Epoch: 31863 mean train loss:  3.66431405e-03, bound:  3.15353990e-01\n",
      "Epoch: 31864 mean train loss:  3.66426236e-03, bound:  3.15353990e-01\n",
      "Epoch: 31865 mean train loss:  3.66424536e-03, bound:  3.15353990e-01\n",
      "Epoch: 31866 mean train loss:  3.66418250e-03, bound:  3.15353990e-01\n",
      "Epoch: 31867 mean train loss:  3.66410520e-03, bound:  3.15353990e-01\n",
      "Epoch: 31868 mean train loss:  3.66410916e-03, bound:  3.15353930e-01\n",
      "Epoch: 31869 mean train loss:  3.66404117e-03, bound:  3.15353930e-01\n",
      "Epoch: 31870 mean train loss:  3.66401067e-03, bound:  3.15353930e-01\n",
      "Epoch: 31871 mean train loss:  3.66400299e-03, bound:  3.15353930e-01\n",
      "Epoch: 31872 mean train loss:  3.66393453e-03, bound:  3.15353930e-01\n",
      "Epoch: 31873 mean train loss:  3.66390962e-03, bound:  3.15353930e-01\n",
      "Epoch: 31874 mean train loss:  3.66386794e-03, bound:  3.15353930e-01\n",
      "Epoch: 31875 mean train loss:  3.66382278e-03, bound:  3.15353930e-01\n",
      "Epoch: 31876 mean train loss:  3.66378366e-03, bound:  3.15353900e-01\n",
      "Epoch: 31877 mean train loss:  3.66373081e-03, bound:  3.15353870e-01\n",
      "Epoch: 31878 mean train loss:  3.66366585e-03, bound:  3.15353870e-01\n",
      "Epoch: 31879 mean train loss:  3.66362091e-03, bound:  3.15353870e-01\n",
      "Epoch: 31880 mean train loss:  3.66364233e-03, bound:  3.15353870e-01\n",
      "Epoch: 31881 mean train loss:  3.66356689e-03, bound:  3.15353870e-01\n",
      "Epoch: 31882 mean train loss:  3.66348657e-03, bound:  3.15353870e-01\n",
      "Epoch: 31883 mean train loss:  3.66345164e-03, bound:  3.15353870e-01\n",
      "Epoch: 31884 mean train loss:  3.66343767e-03, bound:  3.15353870e-01\n",
      "Epoch: 31885 mean train loss:  3.66336270e-03, bound:  3.15353870e-01\n",
      "Epoch: 31886 mean train loss:  3.66332545e-03, bound:  3.15353870e-01\n",
      "Epoch: 31887 mean train loss:  3.66329262e-03, bound:  3.15353870e-01\n",
      "Epoch: 31888 mean train loss:  3.66328447e-03, bound:  3.15353870e-01\n",
      "Epoch: 31889 mean train loss:  3.66320717e-03, bound:  3.15353870e-01\n",
      "Epoch: 31890 mean train loss:  3.66313243e-03, bound:  3.15353870e-01\n",
      "Epoch: 31891 mean train loss:  3.66319250e-03, bound:  3.15353870e-01\n",
      "Epoch: 31892 mean train loss:  3.66309588e-03, bound:  3.15353870e-01\n",
      "Epoch: 31893 mean train loss:  3.66303534e-03, bound:  3.15353811e-01\n",
      "Epoch: 31894 mean train loss:  3.66301765e-03, bound:  3.15353811e-01\n",
      "Epoch: 31895 mean train loss:  3.66300112e-03, bound:  3.15353811e-01\n",
      "Epoch: 31896 mean train loss:  3.66293918e-03, bound:  3.15353811e-01\n",
      "Epoch: 31897 mean train loss:  3.66287678e-03, bound:  3.15353811e-01\n",
      "Epoch: 31898 mean train loss:  3.66284233e-03, bound:  3.15353811e-01\n",
      "Epoch: 31899 mean train loss:  3.66278458e-03, bound:  3.15353811e-01\n",
      "Epoch: 31900 mean train loss:  3.66278132e-03, bound:  3.15353781e-01\n",
      "Epoch: 31901 mean train loss:  3.66270170e-03, bound:  3.15353781e-01\n",
      "Epoch: 31902 mean train loss:  3.66268354e-03, bound:  3.15353781e-01\n",
      "Epoch: 31903 mean train loss:  3.66263138e-03, bound:  3.15353781e-01\n",
      "Epoch: 31904 mean train loss:  3.66258412e-03, bound:  3.15353781e-01\n",
      "Epoch: 31905 mean train loss:  3.66253243e-03, bound:  3.15353781e-01\n",
      "Epoch: 31906 mean train loss:  3.66253057e-03, bound:  3.15353781e-01\n",
      "Epoch: 31907 mean train loss:  3.66242812e-03, bound:  3.15353781e-01\n",
      "Epoch: 31908 mean train loss:  3.66242882e-03, bound:  3.15353781e-01\n",
      "Epoch: 31909 mean train loss:  3.66235757e-03, bound:  3.15353781e-01\n",
      "Epoch: 31910 mean train loss:  3.66230565e-03, bound:  3.15353781e-01\n",
      "Epoch: 31911 mean train loss:  3.66227725e-03, bound:  3.15353721e-01\n",
      "Epoch: 31912 mean train loss:  3.66220833e-03, bound:  3.15353721e-01\n",
      "Epoch: 31913 mean train loss:  3.66219226e-03, bound:  3.15353721e-01\n",
      "Epoch: 31914 mean train loss:  3.66216735e-03, bound:  3.15353721e-01\n",
      "Epoch: 31915 mean train loss:  3.66207608e-03, bound:  3.15353721e-01\n",
      "Epoch: 31916 mean train loss:  3.66207841e-03, bound:  3.15353692e-01\n",
      "Epoch: 31917 mean train loss:  3.66201857e-03, bound:  3.15353721e-01\n",
      "Epoch: 31918 mean train loss:  3.66195920e-03, bound:  3.15353692e-01\n",
      "Epoch: 31919 mean train loss:  3.66195664e-03, bound:  3.15353692e-01\n",
      "Epoch: 31920 mean train loss:  3.66191240e-03, bound:  3.15353692e-01\n",
      "Epoch: 31921 mean train loss:  3.66182509e-03, bound:  3.15353692e-01\n",
      "Epoch: 31922 mean train loss:  3.66179482e-03, bound:  3.15353692e-01\n",
      "Epoch: 31923 mean train loss:  3.66176735e-03, bound:  3.15353692e-01\n",
      "Epoch: 31924 mean train loss:  3.66169261e-03, bound:  3.15353692e-01\n",
      "Epoch: 31925 mean train loss:  3.66165582e-03, bound:  3.15353692e-01\n",
      "Epoch: 31926 mean train loss:  3.66158923e-03, bound:  3.15353692e-01\n",
      "Epoch: 31927 mean train loss:  3.66163696e-03, bound:  3.15353662e-01\n",
      "Epoch: 31928 mean train loss:  3.66157573e-03, bound:  3.15353662e-01\n",
      "Epoch: 31929 mean train loss:  3.66151193e-03, bound:  3.15353662e-01\n",
      "Epoch: 31930 mean train loss:  3.66142206e-03, bound:  3.15353662e-01\n",
      "Epoch: 31931 mean train loss:  3.66139621e-03, bound:  3.15353662e-01\n",
      "Epoch: 31932 mean train loss:  3.66138970e-03, bound:  3.15353662e-01\n",
      "Epoch: 31933 mean train loss:  3.66137060e-03, bound:  3.15353662e-01\n",
      "Epoch: 31934 mean train loss:  3.66125349e-03, bound:  3.15353602e-01\n",
      "Epoch: 31935 mean train loss:  3.66124464e-03, bound:  3.15353602e-01\n",
      "Epoch: 31936 mean train loss:  3.66122485e-03, bound:  3.15353602e-01\n",
      "Epoch: 31937 mean train loss:  3.66120110e-03, bound:  3.15353602e-01\n",
      "Epoch: 31938 mean train loss:  3.66111076e-03, bound:  3.15353602e-01\n",
      "Epoch: 31939 mean train loss:  3.66108282e-03, bound:  3.15353602e-01\n",
      "Epoch: 31940 mean train loss:  3.66105884e-03, bound:  3.15353602e-01\n",
      "Epoch: 31941 mean train loss:  3.66098830e-03, bound:  3.15353602e-01\n",
      "Epoch: 31942 mean train loss:  3.66103905e-03, bound:  3.15353572e-01\n",
      "Epoch: 31943 mean train loss:  3.66089516e-03, bound:  3.15353572e-01\n",
      "Epoch: 31944 mean train loss:  3.66091006e-03, bound:  3.15353572e-01\n",
      "Epoch: 31945 mean train loss:  3.66082368e-03, bound:  3.15353572e-01\n",
      "Epoch: 31946 mean train loss:  3.66079155e-03, bound:  3.15353572e-01\n",
      "Epoch: 31947 mean train loss:  3.66071914e-03, bound:  3.15353572e-01\n",
      "Epoch: 31948 mean train loss:  3.66070447e-03, bound:  3.15353572e-01\n",
      "Epoch: 31949 mean train loss:  3.66064906e-03, bound:  3.15353572e-01\n",
      "Epoch: 31950 mean train loss:  3.66060459e-03, bound:  3.15353543e-01\n",
      "Epoch: 31951 mean train loss:  3.66055151e-03, bound:  3.15353543e-01\n",
      "Epoch: 31952 mean train loss:  3.66052892e-03, bound:  3.15353543e-01\n",
      "Epoch: 31953 mean train loss:  3.66045791e-03, bound:  3.15353543e-01\n",
      "Epoch: 31954 mean train loss:  3.66044859e-03, bound:  3.15353543e-01\n",
      "Epoch: 31955 mean train loss:  3.66041437e-03, bound:  3.15353543e-01\n",
      "Epoch: 31956 mean train loss:  3.66041437e-03, bound:  3.15353543e-01\n",
      "Epoch: 31957 mean train loss:  3.66033590e-03, bound:  3.15353543e-01\n",
      "Epoch: 31958 mean train loss:  3.66027188e-03, bound:  3.15353483e-01\n",
      "Epoch: 31959 mean train loss:  3.66023066e-03, bound:  3.15353483e-01\n",
      "Epoch: 31960 mean train loss:  3.66017967e-03, bound:  3.15353483e-01\n",
      "Epoch: 31961 mean train loss:  3.66012007e-03, bound:  3.15353483e-01\n",
      "Epoch: 31962 mean train loss:  3.66012147e-03, bound:  3.15353483e-01\n",
      "Epoch: 31963 mean train loss:  3.66006815e-03, bound:  3.15353483e-01\n",
      "Epoch: 31964 mean train loss:  3.66004347e-03, bound:  3.15353483e-01\n",
      "Epoch: 31965 mean train loss:  3.65995662e-03, bound:  3.15353483e-01\n",
      "Epoch: 31966 mean train loss:  3.65986698e-03, bound:  3.15353483e-01\n",
      "Epoch: 31967 mean train loss:  3.65991145e-03, bound:  3.15353483e-01\n",
      "Epoch: 31968 mean train loss:  3.65983578e-03, bound:  3.15353483e-01\n",
      "Epoch: 31969 mean train loss:  3.65973637e-03, bound:  3.15353483e-01\n",
      "Epoch: 31970 mean train loss:  3.65974545e-03, bound:  3.15353483e-01\n",
      "Epoch: 31971 mean train loss:  3.65971006e-03, bound:  3.15353483e-01\n",
      "Epoch: 31972 mean train loss:  3.65966326e-03, bound:  3.15353483e-01\n",
      "Epoch: 31973 mean train loss:  3.65960458e-03, bound:  3.15353483e-01\n",
      "Epoch: 31974 mean train loss:  3.65958596e-03, bound:  3.15353453e-01\n",
      "Epoch: 31975 mean train loss:  3.65949539e-03, bound:  3.15353423e-01\n",
      "Epoch: 31976 mean train loss:  3.65949329e-03, bound:  3.15353423e-01\n",
      "Epoch: 31977 mean train loss:  3.65939410e-03, bound:  3.15353423e-01\n",
      "Epoch: 31978 mean train loss:  3.65937687e-03, bound:  3.15353423e-01\n",
      "Epoch: 31979 mean train loss:  3.65933194e-03, bound:  3.15353423e-01\n",
      "Epoch: 31980 mean train loss:  3.65932658e-03, bound:  3.15353423e-01\n",
      "Epoch: 31981 mean train loss:  3.65925953e-03, bound:  3.15353423e-01\n",
      "Epoch: 31982 mean train loss:  3.65921669e-03, bound:  3.15353423e-01\n",
      "Epoch: 31983 mean train loss:  3.65914218e-03, bound:  3.15353364e-01\n",
      "Epoch: 31984 mean train loss:  3.65913752e-03, bound:  3.15353364e-01\n",
      "Epoch: 31985 mean train loss:  3.65907606e-03, bound:  3.15353364e-01\n",
      "Epoch: 31986 mean train loss:  3.65904998e-03, bound:  3.15353364e-01\n",
      "Epoch: 31987 mean train loss:  3.65901436e-03, bound:  3.15353364e-01\n",
      "Epoch: 31988 mean train loss:  3.65898293e-03, bound:  3.15353364e-01\n",
      "Epoch: 31989 mean train loss:  3.65887908e-03, bound:  3.15353364e-01\n",
      "Epoch: 31990 mean train loss:  3.65888583e-03, bound:  3.15353364e-01\n",
      "Epoch: 31991 mean train loss:  3.65883997e-03, bound:  3.15353364e-01\n",
      "Epoch: 31992 mean train loss:  3.65875149e-03, bound:  3.15353364e-01\n",
      "Epoch: 31993 mean train loss:  3.65871657e-03, bound:  3.15353364e-01\n",
      "Epoch: 31994 mean train loss:  3.65872006e-03, bound:  3.15353364e-01\n",
      "Epoch: 31995 mean train loss:  3.65866162e-03, bound:  3.15353364e-01\n",
      "Epoch: 31996 mean train loss:  3.65861063e-03, bound:  3.15353364e-01\n",
      "Epoch: 31997 mean train loss:  3.65854218e-03, bound:  3.15353364e-01\n",
      "Epoch: 31998 mean train loss:  3.65852029e-03, bound:  3.15353334e-01\n",
      "Epoch: 31999 mean train loss:  3.65847000e-03, bound:  3.15353304e-01\n",
      "Epoch: 32000 mean train loss:  3.65843484e-03, bound:  3.15353304e-01\n",
      "Epoch: 32001 mean train loss:  3.65840807e-03, bound:  3.15353304e-01\n",
      "Epoch: 32002 mean train loss:  3.65833309e-03, bound:  3.15353304e-01\n",
      "Epoch: 32003 mean train loss:  3.65829119e-03, bound:  3.15353304e-01\n",
      "Epoch: 32004 mean train loss:  3.65827186e-03, bound:  3.15353304e-01\n",
      "Epoch: 32005 mean train loss:  3.65822832e-03, bound:  3.15353304e-01\n",
      "Epoch: 32006 mean train loss:  3.65816336e-03, bound:  3.15353274e-01\n",
      "Epoch: 32007 mean train loss:  3.65813333e-03, bound:  3.15353274e-01\n",
      "Epoch: 32008 mean train loss:  3.65809118e-03, bound:  3.15353274e-01\n",
      "Epoch: 32009 mean train loss:  3.65803181e-03, bound:  3.15353274e-01\n",
      "Epoch: 32010 mean train loss:  3.65795381e-03, bound:  3.15353274e-01\n",
      "Epoch: 32011 mean train loss:  3.65796918e-03, bound:  3.15353274e-01\n",
      "Epoch: 32012 mean train loss:  3.65792890e-03, bound:  3.15353274e-01\n",
      "Epoch: 32013 mean train loss:  3.65787535e-03, bound:  3.15353245e-01\n",
      "Epoch: 32014 mean train loss:  3.65779828e-03, bound:  3.15353245e-01\n",
      "Epoch: 32015 mean train loss:  3.65778687e-03, bound:  3.15353245e-01\n",
      "Epoch: 32016 mean train loss:  3.65774450e-03, bound:  3.15353245e-01\n",
      "Epoch: 32017 mean train loss:  3.65769910e-03, bound:  3.15353245e-01\n",
      "Epoch: 32018 mean train loss:  3.65763460e-03, bound:  3.15353245e-01\n",
      "Epoch: 32019 mean train loss:  3.65757779e-03, bound:  3.15353245e-01\n",
      "Epoch: 32020 mean train loss:  3.65754985e-03, bound:  3.15353245e-01\n",
      "Epoch: 32021 mean train loss:  3.65753309e-03, bound:  3.15353245e-01\n",
      "Epoch: 32022 mean train loss:  3.65746184e-03, bound:  3.15353215e-01\n",
      "Epoch: 32023 mean train loss:  3.65742925e-03, bound:  3.15353185e-01\n",
      "Epoch: 32024 mean train loss:  3.65737639e-03, bound:  3.15353185e-01\n",
      "Epoch: 32025 mean train loss:  3.65730003e-03, bound:  3.15353185e-01\n",
      "Epoch: 32026 mean train loss:  3.65728885e-03, bound:  3.15353185e-01\n",
      "Epoch: 32027 mean train loss:  3.65727139e-03, bound:  3.15353185e-01\n",
      "Epoch: 32028 mean train loss:  3.65720503e-03, bound:  3.15353185e-01\n",
      "Epoch: 32029 mean train loss:  3.65713635e-03, bound:  3.15353185e-01\n",
      "Epoch: 32030 mean train loss:  3.65708652e-03, bound:  3.15353185e-01\n",
      "Epoch: 32031 mean train loss:  3.65708605e-03, bound:  3.15353155e-01\n",
      "Epoch: 32032 mean train loss:  3.65702482e-03, bound:  3.15353155e-01\n",
      "Epoch: 32033 mean train loss:  3.65697360e-03, bound:  3.15353155e-01\n",
      "Epoch: 32034 mean train loss:  3.65691632e-03, bound:  3.15353155e-01\n",
      "Epoch: 32035 mean train loss:  3.65688372e-03, bound:  3.15353155e-01\n",
      "Epoch: 32036 mean train loss:  3.65685835e-03, bound:  3.15353155e-01\n",
      "Epoch: 32037 mean train loss:  3.65679222e-03, bound:  3.15353125e-01\n",
      "Epoch: 32038 mean train loss:  3.65674100e-03, bound:  3.15353125e-01\n",
      "Epoch: 32039 mean train loss:  3.65672144e-03, bound:  3.15353125e-01\n",
      "Epoch: 32040 mean train loss:  3.65668465e-03, bound:  3.15353125e-01\n",
      "Epoch: 32041 mean train loss:  3.65663040e-03, bound:  3.15353125e-01\n",
      "Epoch: 32042 mean train loss:  3.65656498e-03, bound:  3.15353125e-01\n",
      "Epoch: 32043 mean train loss:  3.65649816e-03, bound:  3.15353125e-01\n",
      "Epoch: 32044 mean train loss:  3.65649769e-03, bound:  3.15353125e-01\n",
      "Epoch: 32045 mean train loss:  3.65647627e-03, bound:  3.15353125e-01\n",
      "Epoch: 32046 mean train loss:  3.65640409e-03, bound:  3.15353096e-01\n",
      "Epoch: 32047 mean train loss:  3.65636055e-03, bound:  3.15353096e-01\n",
      "Epoch: 32048 mean train loss:  3.65634961e-03, bound:  3.15353096e-01\n",
      "Epoch: 32049 mean train loss:  3.65627045e-03, bound:  3.15353096e-01\n",
      "Epoch: 32050 mean train loss:  3.65626207e-03, bound:  3.15353096e-01\n",
      "Epoch: 32051 mean train loss:  3.65617126e-03, bound:  3.15353096e-01\n",
      "Epoch: 32052 mean train loss:  3.65610188e-03, bound:  3.15353096e-01\n",
      "Epoch: 32053 mean train loss:  3.65609652e-03, bound:  3.15353036e-01\n",
      "Epoch: 32054 mean train loss:  3.65606882e-03, bound:  3.15353096e-01\n",
      "Epoch: 32055 mean train loss:  3.65602318e-03, bound:  3.15353036e-01\n",
      "Epoch: 32056 mean train loss:  3.65596078e-03, bound:  3.15353036e-01\n",
      "Epoch: 32057 mean train loss:  3.65593424e-03, bound:  3.15353036e-01\n",
      "Epoch: 32058 mean train loss:  3.65591072e-03, bound:  3.15353036e-01\n",
      "Epoch: 32059 mean train loss:  3.65582691e-03, bound:  3.15353036e-01\n",
      "Epoch: 32060 mean train loss:  3.65579268e-03, bound:  3.15353036e-01\n",
      "Epoch: 32061 mean train loss:  3.65572469e-03, bound:  3.15353006e-01\n",
      "Epoch: 32062 mean train loss:  3.65569629e-03, bound:  3.15353006e-01\n",
      "Epoch: 32063 mean train loss:  3.65570607e-03, bound:  3.15353006e-01\n",
      "Epoch: 32064 mean train loss:  3.65564530e-03, bound:  3.15353006e-01\n",
      "Epoch: 32065 mean train loss:  3.65558709e-03, bound:  3.15353006e-01\n",
      "Epoch: 32066 mean train loss:  3.65551631e-03, bound:  3.15353006e-01\n",
      "Epoch: 32067 mean train loss:  3.65548534e-03, bound:  3.15353006e-01\n",
      "Epoch: 32068 mean train loss:  3.65544274e-03, bound:  3.15353006e-01\n",
      "Epoch: 32069 mean train loss:  3.65534378e-03, bound:  3.15352976e-01\n",
      "Epoch: 32070 mean train loss:  3.65536334e-03, bound:  3.15352976e-01\n",
      "Epoch: 32071 mean train loss:  3.65530327e-03, bound:  3.15352976e-01\n",
      "Epoch: 32072 mean train loss:  3.65525694e-03, bound:  3.15352976e-01\n",
      "Epoch: 32073 mean train loss:  3.65521177e-03, bound:  3.15352976e-01\n",
      "Epoch: 32074 mean train loss:  3.65514890e-03, bound:  3.15352976e-01\n",
      "Epoch: 32075 mean train loss:  3.65508627e-03, bound:  3.15352976e-01\n",
      "Epoch: 32076 mean train loss:  3.65512399e-03, bound:  3.15352976e-01\n",
      "Epoch: 32077 mean train loss:  3.65498429e-03, bound:  3.15352917e-01\n",
      "Epoch: 32078 mean train loss:  3.65490164e-03, bound:  3.15352917e-01\n",
      "Epoch: 32079 mean train loss:  3.65491072e-03, bound:  3.15352917e-01\n",
      "Epoch: 32080 mean train loss:  3.65491514e-03, bound:  3.15352917e-01\n",
      "Epoch: 32081 mean train loss:  3.65480734e-03, bound:  3.15352917e-01\n",
      "Epoch: 32082 mean train loss:  3.65480687e-03, bound:  3.15352917e-01\n",
      "Epoch: 32083 mean train loss:  3.65475938e-03, bound:  3.15352917e-01\n",
      "Epoch: 32084 mean train loss:  3.65473656e-03, bound:  3.15352887e-01\n",
      "Epoch: 32085 mean train loss:  3.65465577e-03, bound:  3.15352887e-01\n",
      "Epoch: 32086 mean train loss:  3.65461386e-03, bound:  3.15352887e-01\n",
      "Epoch: 32087 mean train loss:  3.65459383e-03, bound:  3.15352887e-01\n",
      "Epoch: 32088 mean train loss:  3.65455286e-03, bound:  3.15352887e-01\n",
      "Epoch: 32089 mean train loss:  3.65447765e-03, bound:  3.15352887e-01\n",
      "Epoch: 32090 mean train loss:  3.65445483e-03, bound:  3.15352887e-01\n",
      "Epoch: 32091 mean train loss:  3.65439383e-03, bound:  3.15352857e-01\n",
      "Epoch: 32092 mean train loss:  3.65434377e-03, bound:  3.15352857e-01\n",
      "Epoch: 32093 mean train loss:  3.65432375e-03, bound:  3.15352857e-01\n",
      "Epoch: 32094 mean train loss:  3.65430489e-03, bound:  3.15352857e-01\n",
      "Epoch: 32095 mean train loss:  3.65422829e-03, bound:  3.15352857e-01\n",
      "Epoch: 32096 mean train loss:  3.65419569e-03, bound:  3.15352857e-01\n",
      "Epoch: 32097 mean train loss:  3.65413935e-03, bound:  3.15352857e-01\n",
      "Epoch: 32098 mean train loss:  3.65403434e-03, bound:  3.15352857e-01\n",
      "Epoch: 32099 mean train loss:  3.65406810e-03, bound:  3.15352827e-01\n",
      "Epoch: 32100 mean train loss:  3.65402643e-03, bound:  3.15352798e-01\n",
      "Epoch: 32101 mean train loss:  3.65396868e-03, bound:  3.15352798e-01\n",
      "Epoch: 32102 mean train loss:  3.65390326e-03, bound:  3.15352798e-01\n",
      "Epoch: 32103 mean train loss:  3.65385786e-03, bound:  3.15352798e-01\n",
      "Epoch: 32104 mean train loss:  3.65383108e-03, bound:  3.15352798e-01\n",
      "Epoch: 32105 mean train loss:  3.65379499e-03, bound:  3.15352798e-01\n",
      "Epoch: 32106 mean train loss:  3.65371793e-03, bound:  3.15352798e-01\n",
      "Epoch: 32107 mean train loss:  3.65362712e-03, bound:  3.15352798e-01\n",
      "Epoch: 32108 mean train loss:  3.65362549e-03, bound:  3.15352768e-01\n",
      "Epoch: 32109 mean train loss:  3.65360524e-03, bound:  3.15352768e-01\n",
      "Epoch: 32110 mean train loss:  3.65356426e-03, bound:  3.15352768e-01\n",
      "Epoch: 32111 mean train loss:  3.65348556e-03, bound:  3.15352768e-01\n",
      "Epoch: 32112 mean train loss:  3.65345762e-03, bound:  3.15352768e-01\n",
      "Epoch: 32113 mean train loss:  3.65342596e-03, bound:  3.15352768e-01\n",
      "Epoch: 32114 mean train loss:  3.65334237e-03, bound:  3.15352768e-01\n",
      "Epoch: 32115 mean train loss:  3.65331350e-03, bound:  3.15352768e-01\n",
      "Epoch: 32116 mean train loss:  3.65329557e-03, bound:  3.15352738e-01\n",
      "Epoch: 32117 mean train loss:  3.65320407e-03, bound:  3.15352738e-01\n",
      "Epoch: 32118 mean train loss:  3.65319522e-03, bound:  3.15352738e-01\n",
      "Epoch: 32119 mean train loss:  3.65315774e-03, bound:  3.15352738e-01\n",
      "Epoch: 32120 mean train loss:  3.65313515e-03, bound:  3.15352738e-01\n",
      "Epoch: 32121 mean train loss:  3.65302828e-03, bound:  3.15352738e-01\n",
      "Epoch: 32122 mean train loss:  3.65296286e-03, bound:  3.15352708e-01\n",
      "Epoch: 32123 mean train loss:  3.65297846e-03, bound:  3.15352678e-01\n",
      "Epoch: 32124 mean train loss:  3.65292607e-03, bound:  3.15352708e-01\n",
      "Epoch: 32125 mean train loss:  3.65289464e-03, bound:  3.15352678e-01\n",
      "Epoch: 32126 mean train loss:  3.65278218e-03, bound:  3.15352678e-01\n",
      "Epoch: 32127 mean train loss:  3.65280989e-03, bound:  3.15352678e-01\n",
      "Epoch: 32128 mean train loss:  3.65269859e-03, bound:  3.15352678e-01\n",
      "Epoch: 32129 mean train loss:  3.65269766e-03, bound:  3.15352678e-01\n",
      "Epoch: 32130 mean train loss:  3.65262409e-03, bound:  3.15352678e-01\n",
      "Epoch: 32131 mean train loss:  3.65260360e-03, bound:  3.15352678e-01\n",
      "Epoch: 32132 mean train loss:  3.65253841e-03, bound:  3.15352678e-01\n",
      "Epoch: 32133 mean train loss:  3.65253864e-03, bound:  3.15352678e-01\n",
      "Epoch: 32134 mean train loss:  3.65246693e-03, bound:  3.15352678e-01\n",
      "Epoch: 32135 mean train loss:  3.65244155e-03, bound:  3.15352678e-01\n",
      "Epoch: 32136 mean train loss:  3.65238846e-03, bound:  3.15352678e-01\n",
      "Epoch: 32137 mean train loss:  3.65231815e-03, bound:  3.15352678e-01\n",
      "Epoch: 32138 mean train loss:  3.65225901e-03, bound:  3.15352619e-01\n",
      "Epoch: 32139 mean train loss:  3.65223386e-03, bound:  3.15352619e-01\n",
      "Epoch: 32140 mean train loss:  3.65218427e-03, bound:  3.15352619e-01\n",
      "Epoch: 32141 mean train loss:  3.65210953e-03, bound:  3.15352619e-01\n",
      "Epoch: 32142 mean train loss:  3.65210325e-03, bound:  3.15352619e-01\n",
      "Epoch: 32143 mean train loss:  3.65203340e-03, bound:  3.15352619e-01\n",
      "Epoch: 32144 mean train loss:  3.65196285e-03, bound:  3.15352619e-01\n",
      "Epoch: 32145 mean train loss:  3.65195982e-03, bound:  3.15352619e-01\n",
      "Epoch: 32146 mean train loss:  3.65190534e-03, bound:  3.15352589e-01\n",
      "Epoch: 32147 mean train loss:  3.65187065e-03, bound:  3.15352559e-01\n",
      "Epoch: 32148 mean train loss:  3.65182245e-03, bound:  3.15352559e-01\n",
      "Epoch: 32149 mean train loss:  3.65181803e-03, bound:  3.15352559e-01\n",
      "Epoch: 32150 mean train loss:  3.65176983e-03, bound:  3.15352559e-01\n",
      "Epoch: 32151 mean train loss:  3.65168508e-03, bound:  3.15352559e-01\n",
      "Epoch: 32152 mean train loss:  3.65166483e-03, bound:  3.15352559e-01\n",
      "Epoch: 32153 mean train loss:  3.65160592e-03, bound:  3.15352559e-01\n",
      "Epoch: 32154 mean train loss:  3.65155167e-03, bound:  3.15352559e-01\n",
      "Epoch: 32155 mean train loss:  3.65154166e-03, bound:  3.15352559e-01\n",
      "Epoch: 32156 mean train loss:  3.65149369e-03, bound:  3.15352559e-01\n",
      "Epoch: 32157 mean train loss:  3.65139451e-03, bound:  3.15352559e-01\n",
      "Epoch: 32158 mean train loss:  3.65137984e-03, bound:  3.15352559e-01\n",
      "Epoch: 32159 mean train loss:  3.65133537e-03, bound:  3.15352559e-01\n",
      "Epoch: 32160 mean train loss:  3.65128298e-03, bound:  3.15352559e-01\n",
      "Epoch: 32161 mean train loss:  3.65122058e-03, bound:  3.15352559e-01\n",
      "Epoch: 32162 mean train loss:  3.65116657e-03, bound:  3.15352529e-01\n",
      "Epoch: 32163 mean train loss:  3.65114049e-03, bound:  3.15352529e-01\n",
      "Epoch: 32164 mean train loss:  3.65109742e-03, bound:  3.15352529e-01\n",
      "Epoch: 32165 mean train loss:  3.65102454e-03, bound:  3.15352529e-01\n",
      "Epoch: 32166 mean train loss:  3.65101988e-03, bound:  3.15352529e-01\n",
      "Epoch: 32167 mean train loss:  3.65097076e-03, bound:  3.15352529e-01\n",
      "Epoch: 32168 mean train loss:  3.65089928e-03, bound:  3.15352529e-01\n",
      "Epoch: 32169 mean train loss:  3.65083967e-03, bound:  3.15352470e-01\n",
      "Epoch: 32170 mean train loss:  3.65082221e-03, bound:  3.15352470e-01\n",
      "Epoch: 32171 mean train loss:  3.65081173e-03, bound:  3.15352470e-01\n",
      "Epoch: 32172 mean train loss:  3.65074375e-03, bound:  3.15352470e-01\n",
      "Epoch: 32173 mean train loss:  3.65068787e-03, bound:  3.15352470e-01\n",
      "Epoch: 32174 mean train loss:  3.65066552e-03, bound:  3.15352470e-01\n",
      "Epoch: 32175 mean train loss:  3.65056959e-03, bound:  3.15352470e-01\n",
      "Epoch: 32176 mean train loss:  3.65051976e-03, bound:  3.15352470e-01\n",
      "Epoch: 32177 mean train loss:  3.65047553e-03, bound:  3.15352440e-01\n",
      "Epoch: 32178 mean train loss:  3.65044363e-03, bound:  3.15352440e-01\n",
      "Epoch: 32179 mean train loss:  3.65043571e-03, bound:  3.15352440e-01\n",
      "Epoch: 32180 mean train loss:  3.65035771e-03, bound:  3.15352440e-01\n",
      "Epoch: 32181 mean train loss:  3.65032116e-03, bound:  3.15352440e-01\n",
      "Epoch: 32182 mean train loss:  3.65028763e-03, bound:  3.15352440e-01\n",
      "Epoch: 32183 mean train loss:  3.65020474e-03, bound:  3.15352410e-01\n",
      "Epoch: 32184 mean train loss:  3.65014304e-03, bound:  3.15352410e-01\n",
      "Epoch: 32185 mean train loss:  3.65013583e-03, bound:  3.15352410e-01\n",
      "Epoch: 32186 mean train loss:  3.65013164e-03, bound:  3.15352410e-01\n",
      "Epoch: 32187 mean train loss:  3.65004246e-03, bound:  3.15352410e-01\n",
      "Epoch: 32188 mean train loss:  3.64997517e-03, bound:  3.15352410e-01\n",
      "Epoch: 32189 mean train loss:  3.64994095e-03, bound:  3.15352410e-01\n",
      "Epoch: 32190 mean train loss:  3.64991045e-03, bound:  3.15352410e-01\n",
      "Epoch: 32191 mean train loss:  3.64980940e-03, bound:  3.15352350e-01\n",
      "Epoch: 32192 mean train loss:  3.64978658e-03, bound:  3.15352350e-01\n",
      "Epoch: 32193 mean train loss:  3.64977517e-03, bound:  3.15352350e-01\n",
      "Epoch: 32194 mean train loss:  3.64972744e-03, bound:  3.15352350e-01\n",
      "Epoch: 32195 mean train loss:  3.64964944e-03, bound:  3.15352350e-01\n",
      "Epoch: 32196 mean train loss:  3.64961103e-03, bound:  3.15352350e-01\n",
      "Epoch: 32197 mean train loss:  3.64958937e-03, bound:  3.15352350e-01\n",
      "Epoch: 32198 mean train loss:  3.64954257e-03, bound:  3.15352350e-01\n",
      "Epoch: 32199 mean train loss:  3.64946388e-03, bound:  3.15352321e-01\n",
      "Epoch: 32200 mean train loss:  3.64941987e-03, bound:  3.15352321e-01\n",
      "Epoch: 32201 mean train loss:  3.64942569e-03, bound:  3.15352321e-01\n",
      "Epoch: 32202 mean train loss:  3.64933256e-03, bound:  3.15352321e-01\n",
      "Epoch: 32203 mean train loss:  3.64930090e-03, bound:  3.15352321e-01\n",
      "Epoch: 32204 mean train loss:  3.64927133e-03, bound:  3.15352321e-01\n",
      "Epoch: 32205 mean train loss:  3.64914746e-03, bound:  3.15352321e-01\n",
      "Epoch: 32206 mean train loss:  3.64915328e-03, bound:  3.15352321e-01\n",
      "Epoch: 32207 mean train loss:  3.64910229e-03, bound:  3.15352291e-01\n",
      "Epoch: 32208 mean train loss:  3.64910578e-03, bound:  3.15352261e-01\n",
      "Epoch: 32209 mean train loss:  3.64901824e-03, bound:  3.15352261e-01\n",
      "Epoch: 32210 mean train loss:  3.64897470e-03, bound:  3.15352261e-01\n",
      "Epoch: 32211 mean train loss:  3.64888832e-03, bound:  3.15352261e-01\n",
      "Epoch: 32212 mean train loss:  3.64888040e-03, bound:  3.15352261e-01\n",
      "Epoch: 32213 mean train loss:  3.64885456e-03, bound:  3.15352261e-01\n",
      "Epoch: 32214 mean train loss:  3.64879449e-03, bound:  3.15352261e-01\n",
      "Epoch: 32215 mean train loss:  3.64872208e-03, bound:  3.15352231e-01\n",
      "Epoch: 32216 mean train loss:  3.64868785e-03, bound:  3.15352231e-01\n",
      "Epoch: 32217 mean train loss:  3.64863966e-03, bound:  3.15352231e-01\n",
      "Epoch: 32218 mean train loss:  3.64861288e-03, bound:  3.15352231e-01\n",
      "Epoch: 32219 mean train loss:  3.64854187e-03, bound:  3.15352231e-01\n",
      "Epoch: 32220 mean train loss:  3.64851858e-03, bound:  3.15352231e-01\n",
      "Epoch: 32221 mean train loss:  3.64843733e-03, bound:  3.15352231e-01\n",
      "Epoch: 32222 mean train loss:  3.64840333e-03, bound:  3.15352201e-01\n",
      "Epoch: 32223 mean train loss:  3.64835979e-03, bound:  3.15352201e-01\n",
      "Epoch: 32224 mean train loss:  3.64830880e-03, bound:  3.15352201e-01\n",
      "Epoch: 32225 mean train loss:  3.64827411e-03, bound:  3.15352201e-01\n",
      "Epoch: 32226 mean train loss:  3.64824315e-03, bound:  3.15352201e-01\n",
      "Epoch: 32227 mean train loss:  3.64821637e-03, bound:  3.15352201e-01\n",
      "Epoch: 32228 mean train loss:  3.64813185e-03, bound:  3.15352172e-01\n",
      "Epoch: 32229 mean train loss:  3.64810531e-03, bound:  3.15352172e-01\n",
      "Epoch: 32230 mean train loss:  3.64805502e-03, bound:  3.15352172e-01\n",
      "Epoch: 32231 mean train loss:  3.64797097e-03, bound:  3.15352172e-01\n",
      "Epoch: 32232 mean train loss:  3.64795956e-03, bound:  3.15352172e-01\n",
      "Epoch: 32233 mean train loss:  3.64790414e-03, bound:  3.15352172e-01\n",
      "Epoch: 32234 mean train loss:  3.64788319e-03, bound:  3.15352172e-01\n",
      "Epoch: 32235 mean train loss:  3.64782009e-03, bound:  3.15352172e-01\n",
      "Epoch: 32236 mean train loss:  3.64777958e-03, bound:  3.15352142e-01\n",
      "Epoch: 32237 mean train loss:  3.64770577e-03, bound:  3.15352112e-01\n",
      "Epoch: 32238 mean train loss:  3.64762568e-03, bound:  3.15352112e-01\n",
      "Epoch: 32239 mean train loss:  3.64759704e-03, bound:  3.15352112e-01\n",
      "Epoch: 32240 mean train loss:  3.64757958e-03, bound:  3.15352112e-01\n",
      "Epoch: 32241 mean train loss:  3.64753790e-03, bound:  3.15352112e-01\n",
      "Epoch: 32242 mean train loss:  3.64745781e-03, bound:  3.15352112e-01\n",
      "Epoch: 32243 mean train loss:  3.64741520e-03, bound:  3.15352112e-01\n",
      "Epoch: 32244 mean train loss:  3.64743709e-03, bound:  3.15352082e-01\n",
      "Epoch: 32245 mean train loss:  3.64734838e-03, bound:  3.15352082e-01\n",
      "Epoch: 32246 mean train loss:  3.64732346e-03, bound:  3.15352082e-01\n",
      "Epoch: 32247 mean train loss:  3.64726526e-03, bound:  3.15352082e-01\n",
      "Epoch: 32248 mean train loss:  3.64714558e-03, bound:  3.15352082e-01\n",
      "Epoch: 32249 mean train loss:  3.64717795e-03, bound:  3.15352082e-01\n",
      "Epoch: 32250 mean train loss:  3.64712276e-03, bound:  3.15352082e-01\n",
      "Epoch: 32251 mean train loss:  3.64706106e-03, bound:  3.15352052e-01\n",
      "Epoch: 32252 mean train loss:  3.64702498e-03, bound:  3.15352052e-01\n",
      "Epoch: 32253 mean train loss:  3.64695955e-03, bound:  3.15352052e-01\n",
      "Epoch: 32254 mean train loss:  3.64691881e-03, bound:  3.15352052e-01\n",
      "Epoch: 32255 mean train loss:  3.64682125e-03, bound:  3.15352052e-01\n",
      "Epoch: 32256 mean train loss:  3.64684244e-03, bound:  3.15352052e-01\n",
      "Epoch: 32257 mean train loss:  3.64678260e-03, bound:  3.15352023e-01\n",
      "Epoch: 32258 mean train loss:  3.64674884e-03, bound:  3.15351993e-01\n",
      "Epoch: 32259 mean train loss:  3.64665850e-03, bound:  3.15351993e-01\n",
      "Epoch: 32260 mean train loss:  3.64665058e-03, bound:  3.15351993e-01\n",
      "Epoch: 32261 mean train loss:  3.64655745e-03, bound:  3.15351993e-01\n",
      "Epoch: 32262 mean train loss:  3.64656164e-03, bound:  3.15351993e-01\n",
      "Epoch: 32263 mean train loss:  3.64653138e-03, bound:  3.15351993e-01\n",
      "Epoch: 32264 mean train loss:  3.64646269e-03, bound:  3.15351993e-01\n",
      "Epoch: 32265 mean train loss:  3.64641054e-03, bound:  3.15351993e-01\n",
      "Epoch: 32266 mean train loss:  3.64637328e-03, bound:  3.15351993e-01\n",
      "Epoch: 32267 mean train loss:  3.64632672e-03, bound:  3.15351963e-01\n",
      "Epoch: 32268 mean train loss:  3.64624872e-03, bound:  3.15351963e-01\n",
      "Epoch: 32269 mean train loss:  3.64622264e-03, bound:  3.15351963e-01\n",
      "Epoch: 32270 mean train loss:  3.64614115e-03, bound:  3.15351963e-01\n",
      "Epoch: 32271 mean train loss:  3.64613230e-03, bound:  3.15351963e-01\n",
      "Epoch: 32272 mean train loss:  3.64604825e-03, bound:  3.15351963e-01\n",
      "Epoch: 32273 mean train loss:  3.64602753e-03, bound:  3.15351933e-01\n",
      "Epoch: 32274 mean train loss:  3.64598958e-03, bound:  3.15351933e-01\n",
      "Epoch: 32275 mean train loss:  3.64593044e-03, bound:  3.15351933e-01\n",
      "Epoch: 32276 mean train loss:  3.64585966e-03, bound:  3.15351933e-01\n",
      "Epoch: 32277 mean train loss:  3.64584266e-03, bound:  3.15351933e-01\n",
      "Epoch: 32278 mean train loss:  3.64576862e-03, bound:  3.15351933e-01\n",
      "Epoch: 32279 mean train loss:  3.64574557e-03, bound:  3.15351903e-01\n",
      "Epoch: 32280 mean train loss:  3.64566804e-03, bound:  3.15351903e-01\n",
      "Epoch: 32281 mean train loss:  3.64564406e-03, bound:  3.15351874e-01\n",
      "Epoch: 32282 mean train loss:  3.64559120e-03, bound:  3.15351874e-01\n",
      "Epoch: 32283 mean train loss:  3.64556396e-03, bound:  3.15351874e-01\n",
      "Epoch: 32284 mean train loss:  3.64552555e-03, bound:  3.15351874e-01\n",
      "Epoch: 32285 mean train loss:  3.64543661e-03, bound:  3.15351874e-01\n",
      "Epoch: 32286 mean train loss:  3.64536303e-03, bound:  3.15351874e-01\n",
      "Epoch: 32287 mean train loss:  3.64537351e-03, bound:  3.15351874e-01\n",
      "Epoch: 32288 mean train loss:  3.64527712e-03, bound:  3.15351874e-01\n",
      "Epoch: 32289 mean train loss:  3.64527386e-03, bound:  3.15351874e-01\n",
      "Epoch: 32290 mean train loss:  3.64521449e-03, bound:  3.15351874e-01\n",
      "Epoch: 32291 mean train loss:  3.64520471e-03, bound:  3.15351874e-01\n",
      "Epoch: 32292 mean train loss:  3.64515046e-03, bound:  3.15351874e-01\n",
      "Epoch: 32293 mean train loss:  3.64509714e-03, bound:  3.15351874e-01\n",
      "Epoch: 32294 mean train loss:  3.64505104e-03, bound:  3.15351814e-01\n",
      "Epoch: 32295 mean train loss:  3.64498189e-03, bound:  3.15351844e-01\n",
      "Epoch: 32296 mean train loss:  3.64493974e-03, bound:  3.15351814e-01\n",
      "Epoch: 32297 mean train loss:  3.64484498e-03, bound:  3.15351814e-01\n",
      "Epoch: 32298 mean train loss:  3.64484871e-03, bound:  3.15351814e-01\n",
      "Epoch: 32299 mean train loss:  3.64479912e-03, bound:  3.15351814e-01\n",
      "Epoch: 32300 mean train loss:  3.64474580e-03, bound:  3.15351784e-01\n",
      "Epoch: 32301 mean train loss:  3.64471041e-03, bound:  3.15351784e-01\n",
      "Epoch: 32302 mean train loss:  3.64461425e-03, bound:  3.15351754e-01\n",
      "Epoch: 32303 mean train loss:  3.64457979e-03, bound:  3.15351754e-01\n",
      "Epoch: 32304 mean train loss:  3.64452600e-03, bound:  3.15351754e-01\n",
      "Epoch: 32305 mean train loss:  3.64449108e-03, bound:  3.15351754e-01\n",
      "Epoch: 32306 mean train loss:  3.64448084e-03, bound:  3.15351754e-01\n",
      "Epoch: 32307 mean train loss:  3.64438840e-03, bound:  3.15351754e-01\n",
      "Epoch: 32308 mean train loss:  3.64434184e-03, bound:  3.15351754e-01\n",
      "Epoch: 32309 mean train loss:  3.64428363e-03, bound:  3.15351754e-01\n",
      "Epoch: 32310 mean train loss:  3.64423357e-03, bound:  3.15351754e-01\n",
      "Epoch: 32311 mean train loss:  3.64420190e-03, bound:  3.15351754e-01\n",
      "Epoch: 32312 mean train loss:  3.64417396e-03, bound:  3.15351754e-01\n",
      "Epoch: 32313 mean train loss:  3.64412693e-03, bound:  3.15351754e-01\n",
      "Epoch: 32314 mean train loss:  3.64404707e-03, bound:  3.15351754e-01\n",
      "Epoch: 32315 mean train loss:  3.64401168e-03, bound:  3.15351725e-01\n",
      "Epoch: 32316 mean train loss:  3.64400609e-03, bound:  3.15351725e-01\n",
      "Epoch: 32317 mean train loss:  3.64390807e-03, bound:  3.15351695e-01\n",
      "Epoch: 32318 mean train loss:  3.64387664e-03, bound:  3.15351695e-01\n",
      "Epoch: 32319 mean train loss:  3.64385475e-03, bound:  3.15351695e-01\n",
      "Epoch: 32320 mean train loss:  3.64376325e-03, bound:  3.15351695e-01\n",
      "Epoch: 32321 mean train loss:  3.64373578e-03, bound:  3.15351695e-01\n",
      "Epoch: 32322 mean train loss:  3.64372111e-03, bound:  3.15351695e-01\n",
      "Epoch: 32323 mean train loss:  3.64365778e-03, bound:  3.15351665e-01\n",
      "Epoch: 32324 mean train loss:  3.64361401e-03, bound:  3.15351635e-01\n",
      "Epoch: 32325 mean train loss:  3.64355021e-03, bound:  3.15351635e-01\n",
      "Epoch: 32326 mean train loss:  3.64348991e-03, bound:  3.15351635e-01\n",
      "Epoch: 32327 mean train loss:  3.64343263e-03, bound:  3.15351635e-01\n",
      "Epoch: 32328 mean train loss:  3.64339189e-03, bound:  3.15351635e-01\n",
      "Epoch: 32329 mean train loss:  3.64336488e-03, bound:  3.15351635e-01\n",
      "Epoch: 32330 mean train loss:  3.64328572e-03, bound:  3.15351635e-01\n",
      "Epoch: 32331 mean train loss:  3.64327943e-03, bound:  3.15351635e-01\n",
      "Epoch: 32332 mean train loss:  3.64319817e-03, bound:  3.15351635e-01\n",
      "Epoch: 32333 mean train loss:  3.64315673e-03, bound:  3.15351635e-01\n",
      "Epoch: 32334 mean train loss:  3.64309363e-03, bound:  3.15351635e-01\n",
      "Epoch: 32335 mean train loss:  3.64304078e-03, bound:  3.15351635e-01\n",
      "Epoch: 32336 mean train loss:  3.64302006e-03, bound:  3.15351635e-01\n",
      "Epoch: 32337 mean train loss:  3.64299095e-03, bound:  3.15351635e-01\n",
      "Epoch: 32338 mean train loss:  3.64292273e-03, bound:  3.15351576e-01\n",
      "Epoch: 32339 mean train loss:  3.64284543e-03, bound:  3.15351576e-01\n",
      "Epoch: 32340 mean train loss:  3.64282890e-03, bound:  3.15351576e-01\n",
      "Epoch: 32341 mean train loss:  3.64275370e-03, bound:  3.15351576e-01\n",
      "Epoch: 32342 mean train loss:  3.64267593e-03, bound:  3.15351576e-01\n",
      "Epoch: 32343 mean train loss:  3.64267710e-03, bound:  3.15351576e-01\n",
      "Epoch: 32344 mean train loss:  3.64266755e-03, bound:  3.15351576e-01\n",
      "Epoch: 32345 mean train loss:  3.64257069e-03, bound:  3.15351576e-01\n",
      "Epoch: 32346 mean train loss:  3.64251062e-03, bound:  3.15351546e-01\n",
      "Epoch: 32347 mean train loss:  3.64251714e-03, bound:  3.15351546e-01\n",
      "Epoch: 32348 mean train loss:  3.64245288e-03, bound:  3.15351546e-01\n",
      "Epoch: 32349 mean train loss:  3.64236650e-03, bound:  3.15351546e-01\n",
      "Epoch: 32350 mean train loss:  3.64233600e-03, bound:  3.15351546e-01\n",
      "Epoch: 32351 mean train loss:  3.64224426e-03, bound:  3.15351546e-01\n",
      "Epoch: 32352 mean train loss:  3.64224915e-03, bound:  3.15351516e-01\n",
      "Epoch: 32353 mean train loss:  3.64217744e-03, bound:  3.15351516e-01\n",
      "Epoch: 32354 mean train loss:  3.64215742e-03, bound:  3.15351516e-01\n",
      "Epoch: 32355 mean train loss:  3.64206778e-03, bound:  3.15351516e-01\n",
      "Epoch: 32356 mean train loss:  3.64205916e-03, bound:  3.15351516e-01\n",
      "Epoch: 32357 mean train loss:  3.64202610e-03, bound:  3.15351516e-01\n",
      "Epoch: 32358 mean train loss:  3.64198023e-03, bound:  3.15351516e-01\n",
      "Epoch: 32359 mean train loss:  3.64184403e-03, bound:  3.15351456e-01\n",
      "Epoch: 32360 mean train loss:  3.64186964e-03, bound:  3.15351456e-01\n",
      "Epoch: 32361 mean train loss:  3.64179257e-03, bound:  3.15351456e-01\n",
      "Epoch: 32362 mean train loss:  3.64171783e-03, bound:  3.15351456e-01\n",
      "Epoch: 32363 mean train loss:  3.64169129e-03, bound:  3.15351456e-01\n",
      "Epoch: 32364 mean train loss:  3.64162982e-03, bound:  3.15351456e-01\n",
      "Epoch: 32365 mean train loss:  3.64161027e-03, bound:  3.15351456e-01\n",
      "Epoch: 32366 mean train loss:  3.64153669e-03, bound:  3.15351456e-01\n",
      "Epoch: 32367 mean train loss:  3.64151341e-03, bound:  3.15351427e-01\n",
      "Epoch: 32368 mean train loss:  3.64145660e-03, bound:  3.15351427e-01\n",
      "Epoch: 32369 mean train loss:  3.64147918e-03, bound:  3.15351427e-01\n",
      "Epoch: 32370 mean train loss:  3.64141003e-03, bound:  3.15351427e-01\n",
      "Epoch: 32371 mean train loss:  3.64129711e-03, bound:  3.15351427e-01\n",
      "Epoch: 32372 mean train loss:  3.64125543e-03, bound:  3.15351427e-01\n",
      "Epoch: 32373 mean train loss:  3.64123890e-03, bound:  3.15351427e-01\n",
      "Epoch: 32374 mean train loss:  3.64115532e-03, bound:  3.15351427e-01\n",
      "Epoch: 32375 mean train loss:  3.64111294e-03, bound:  3.15351397e-01\n",
      "Epoch: 32376 mean train loss:  3.64106498e-03, bound:  3.15351397e-01\n",
      "Epoch: 32377 mean train loss:  3.64100630e-03, bound:  3.15351397e-01\n",
      "Epoch: 32378 mean train loss:  3.64094437e-03, bound:  3.15351337e-01\n",
      "Epoch: 32379 mean train loss:  3.64092365e-03, bound:  3.15351397e-01\n",
      "Epoch: 32380 mean train loss:  3.64086521e-03, bound:  3.15351337e-01\n",
      "Epoch: 32381 mean train loss:  3.64080560e-03, bound:  3.15351337e-01\n",
      "Epoch: 32382 mean train loss:  3.64078302e-03, bound:  3.15351337e-01\n",
      "Epoch: 32383 mean train loss:  3.64073459e-03, bound:  3.15351337e-01\n",
      "Epoch: 32384 mean train loss:  3.64071713e-03, bound:  3.15351337e-01\n",
      "Epoch: 32385 mean train loss:  3.64059070e-03, bound:  3.15351337e-01\n",
      "Epoch: 32386 mean train loss:  3.64054297e-03, bound:  3.15351337e-01\n",
      "Epoch: 32387 mean train loss:  3.64053133e-03, bound:  3.15351307e-01\n",
      "Epoch: 32388 mean train loss:  3.64047335e-03, bound:  3.15351307e-01\n",
      "Epoch: 32389 mean train loss:  3.64042283e-03, bound:  3.15351307e-01\n",
      "Epoch: 32390 mean train loss:  3.64039396e-03, bound:  3.15351307e-01\n",
      "Epoch: 32391 mean train loss:  3.64028616e-03, bound:  3.15351307e-01\n",
      "Epoch: 32392 mean train loss:  3.64031061e-03, bound:  3.15351307e-01\n",
      "Epoch: 32393 mean train loss:  3.64019442e-03, bound:  3.15351307e-01\n",
      "Epoch: 32394 mean train loss:  3.64018045e-03, bound:  3.15351307e-01\n",
      "Epoch: 32395 mean train loss:  3.64014483e-03, bound:  3.15351307e-01\n",
      "Epoch: 32396 mean train loss:  3.64013109e-03, bound:  3.15351278e-01\n",
      "Epoch: 32397 mean train loss:  3.64002003e-03, bound:  3.15351278e-01\n",
      "Epoch: 32398 mean train loss:  3.64000024e-03, bound:  3.15351278e-01\n",
      "Epoch: 32399 mean train loss:  3.63992807e-03, bound:  3.15351278e-01\n",
      "Epoch: 32400 mean train loss:  3.63989291e-03, bound:  3.15351278e-01\n",
      "Epoch: 32401 mean train loss:  3.63986520e-03, bound:  3.15351278e-01\n",
      "Epoch: 32402 mean train loss:  3.63981351e-03, bound:  3.15351218e-01\n",
      "Epoch: 32403 mean train loss:  3.63973458e-03, bound:  3.15351218e-01\n",
      "Epoch: 32404 mean train loss:  3.63971153e-03, bound:  3.15351218e-01\n",
      "Epoch: 32405 mean train loss:  3.63963982e-03, bound:  3.15351218e-01\n",
      "Epoch: 32406 mean train loss:  3.63961747e-03, bound:  3.15351218e-01\n",
      "Epoch: 32407 mean train loss:  3.63954064e-03, bound:  3.15351218e-01\n",
      "Epoch: 32408 mean train loss:  3.63948382e-03, bound:  3.15351218e-01\n",
      "Epoch: 32409 mean train loss:  3.63947893e-03, bound:  3.15351218e-01\n",
      "Epoch: 32410 mean train loss:  3.63941141e-03, bound:  3.15351218e-01\n",
      "Epoch: 32411 mean train loss:  3.63935065e-03, bound:  3.15351188e-01\n",
      "Epoch: 32412 mean train loss:  3.63931572e-03, bound:  3.15351188e-01\n",
      "Epoch: 32413 mean train loss:  3.63926194e-03, bound:  3.15351188e-01\n",
      "Epoch: 32414 mean train loss:  3.63922841e-03, bound:  3.15351188e-01\n",
      "Epoch: 32415 mean train loss:  3.63915553e-03, bound:  3.15351188e-01\n",
      "Epoch: 32416 mean train loss:  3.63910873e-03, bound:  3.15351188e-01\n",
      "Epoch: 32417 mean train loss:  3.63906426e-03, bound:  3.15351188e-01\n",
      "Epoch: 32418 mean train loss:  3.63901607e-03, bound:  3.15351158e-01\n",
      "Epoch: 32419 mean train loss:  3.63895786e-03, bound:  3.15351158e-01\n",
      "Epoch: 32420 mean train loss:  3.63890617e-03, bound:  3.15351158e-01\n",
      "Epoch: 32421 mean train loss:  3.63885867e-03, bound:  3.15351158e-01\n",
      "Epoch: 32422 mean train loss:  3.63881001e-03, bound:  3.15351158e-01\n",
      "Epoch: 32423 mean train loss:  3.63873132e-03, bound:  3.15351158e-01\n",
      "Epoch: 32424 mean train loss:  3.63868033e-03, bound:  3.15351129e-01\n",
      "Epoch: 32425 mean train loss:  3.63864913e-03, bound:  3.15351129e-01\n",
      "Epoch: 32426 mean train loss:  3.63862514e-03, bound:  3.15351129e-01\n",
      "Epoch: 32427 mean train loss:  3.63854389e-03, bound:  3.15351129e-01\n",
      "Epoch: 32428 mean train loss:  3.63848521e-03, bound:  3.15351099e-01\n",
      "Epoch: 32429 mean train loss:  3.63850407e-03, bound:  3.15351099e-01\n",
      "Epoch: 32430 mean train loss:  3.63842142e-03, bound:  3.15351099e-01\n",
      "Epoch: 32431 mean train loss:  3.63831711e-03, bound:  3.15351069e-01\n",
      "Epoch: 32432 mean train loss:  3.63834412e-03, bound:  3.15351069e-01\n",
      "Epoch: 32433 mean train loss:  3.63828940e-03, bound:  3.15351069e-01\n",
      "Epoch: 32434 mean train loss:  3.63817858e-03, bound:  3.15351069e-01\n",
      "Epoch: 32435 mean train loss:  3.63817136e-03, bound:  3.15351069e-01\n",
      "Epoch: 32436 mean train loss:  3.63809499e-03, bound:  3.15351069e-01\n",
      "Epoch: 32437 mean train loss:  3.63808824e-03, bound:  3.15351069e-01\n",
      "Epoch: 32438 mean train loss:  3.63802188e-03, bound:  3.15351069e-01\n",
      "Epoch: 32439 mean train loss:  3.63796344e-03, bound:  3.15351069e-01\n",
      "Epoch: 32440 mean train loss:  3.63792083e-03, bound:  3.15351069e-01\n",
      "Epoch: 32441 mean train loss:  3.63788544e-03, bound:  3.15351069e-01\n",
      "Epoch: 32442 mean train loss:  3.63776833e-03, bound:  3.15351069e-01\n",
      "Epoch: 32443 mean train loss:  3.63774598e-03, bound:  3.15351039e-01\n",
      "Epoch: 32444 mean train loss:  3.63770896e-03, bound:  3.15351009e-01\n",
      "Epoch: 32445 mean train loss:  3.63767263e-03, bound:  3.15351009e-01\n",
      "Epoch: 32446 mean train loss:  3.63756320e-03, bound:  3.15351009e-01\n",
      "Epoch: 32447 mean train loss:  3.63755412e-03, bound:  3.15351009e-01\n",
      "Epoch: 32448 mean train loss:  3.63749638e-03, bound:  3.15351009e-01\n",
      "Epoch: 32449 mean train loss:  3.63745796e-03, bound:  3.15351009e-01\n",
      "Epoch: 32450 mean train loss:  3.63739277e-03, bound:  3.15351009e-01\n",
      "Epoch: 32451 mean train loss:  3.63734365e-03, bound:  3.15350980e-01\n",
      "Epoch: 32452 mean train loss:  3.63731990e-03, bound:  3.15350950e-01\n",
      "Epoch: 32453 mean train loss:  3.63724632e-03, bound:  3.15350950e-01\n",
      "Epoch: 32454 mean train loss:  3.63723189e-03, bound:  3.15350950e-01\n",
      "Epoch: 32455 mean train loss:  3.63715133e-03, bound:  3.15350950e-01\n",
      "Epoch: 32456 mean train loss:  3.63711594e-03, bound:  3.15350950e-01\n",
      "Epoch: 32457 mean train loss:  3.63702513e-03, bound:  3.15350950e-01\n",
      "Epoch: 32458 mean train loss:  3.63699114e-03, bound:  3.15350950e-01\n",
      "Epoch: 32459 mean train loss:  3.63694085e-03, bound:  3.15350950e-01\n",
      "Epoch: 32460 mean train loss:  3.63691477e-03, bound:  3.15350950e-01\n",
      "Epoch: 32461 mean train loss:  3.63683468e-03, bound:  3.15350950e-01\n",
      "Epoch: 32462 mean train loss:  3.63683514e-03, bound:  3.15350950e-01\n",
      "Epoch: 32463 mean train loss:  3.63674364e-03, bound:  3.15350950e-01\n",
      "Epoch: 32464 mean train loss:  3.63670010e-03, bound:  3.15350950e-01\n",
      "Epoch: 32465 mean train loss:  3.63668031e-03, bound:  3.15350890e-01\n",
      "Epoch: 32466 mean train loss:  3.63657111e-03, bound:  3.15350890e-01\n",
      "Epoch: 32467 mean train loss:  3.63653852e-03, bound:  3.15350890e-01\n",
      "Epoch: 32468 mean train loss:  3.63650871e-03, bound:  3.15350890e-01\n",
      "Epoch: 32469 mean train loss:  3.63646029e-03, bound:  3.15350890e-01\n",
      "Epoch: 32470 mean train loss:  3.63637414e-03, bound:  3.15350890e-01\n",
      "Epoch: 32471 mean train loss:  3.63636296e-03, bound:  3.15350890e-01\n",
      "Epoch: 32472 mean train loss:  3.63629521e-03, bound:  3.15350860e-01\n",
      "Epoch: 32473 mean train loss:  3.63626168e-03, bound:  3.15350860e-01\n",
      "Epoch: 32474 mean train loss:  3.63619626e-03, bound:  3.15350860e-01\n",
      "Epoch: 32475 mean train loss:  3.63618438e-03, bound:  3.15350860e-01\n",
      "Epoch: 32476 mean train loss:  3.63608426e-03, bound:  3.15350860e-01\n",
      "Epoch: 32477 mean train loss:  3.63608589e-03, bound:  3.15350860e-01\n",
      "Epoch: 32478 mean train loss:  3.63598904e-03, bound:  3.15350860e-01\n",
      "Epoch: 32479 mean train loss:  3.63593502e-03, bound:  3.15350860e-01\n",
      "Epoch: 32480 mean train loss:  3.63592198e-03, bound:  3.15350831e-01\n",
      "Epoch: 32481 mean train loss:  3.63587984e-03, bound:  3.15350831e-01\n",
      "Epoch: 32482 mean train loss:  3.63581697e-03, bound:  3.15350831e-01\n",
      "Epoch: 32483 mean train loss:  3.63573269e-03, bound:  3.15350831e-01\n",
      "Epoch: 32484 mean train loss:  3.63567541e-03, bound:  3.15350831e-01\n",
      "Epoch: 32485 mean train loss:  3.63563676e-03, bound:  3.15350771e-01\n",
      "Epoch: 32486 mean train loss:  3.63558601e-03, bound:  3.15350771e-01\n",
      "Epoch: 32487 mean train loss:  3.63554549e-03, bound:  3.15350771e-01\n",
      "Epoch: 32488 mean train loss:  3.63550452e-03, bound:  3.15350771e-01\n",
      "Epoch: 32489 mean train loss:  3.63546354e-03, bound:  3.15350771e-01\n",
      "Epoch: 32490 mean train loss:  3.63541651e-03, bound:  3.15350771e-01\n",
      "Epoch: 32491 mean train loss:  3.63535527e-03, bound:  3.15350741e-01\n",
      "Epoch: 32492 mean train loss:  3.63529567e-03, bound:  3.15350741e-01\n",
      "Epoch: 32493 mean train loss:  3.63522233e-03, bound:  3.15350741e-01\n",
      "Epoch: 32494 mean train loss:  3.63520090e-03, bound:  3.15350741e-01\n",
      "Epoch: 32495 mean train loss:  3.63509310e-03, bound:  3.15350741e-01\n",
      "Epoch: 32496 mean train loss:  3.63506959e-03, bound:  3.15350741e-01\n",
      "Epoch: 32497 mean train loss:  3.63506703e-03, bound:  3.15350741e-01\n",
      "Epoch: 32498 mean train loss:  3.63504724e-03, bound:  3.15350741e-01\n",
      "Epoch: 32499 mean train loss:  3.63493827e-03, bound:  3.15350741e-01\n",
      "Epoch: 32500 mean train loss:  3.63488775e-03, bound:  3.15350711e-01\n",
      "Epoch: 32501 mean train loss:  3.63483070e-03, bound:  3.15350711e-01\n",
      "Epoch: 32502 mean train loss:  3.63481534e-03, bound:  3.15350711e-01\n",
      "Epoch: 32503 mean train loss:  3.63478367e-03, bound:  3.15350711e-01\n",
      "Epoch: 32504 mean train loss:  3.63468868e-03, bound:  3.15350711e-01\n",
      "Epoch: 32505 mean train loss:  3.63464979e-03, bound:  3.15350652e-01\n",
      "Epoch: 32506 mean train loss:  3.63459019e-03, bound:  3.15350652e-01\n",
      "Epoch: 32507 mean train loss:  3.63454851e-03, bound:  3.15350652e-01\n",
      "Epoch: 32508 mean train loss:  3.63446446e-03, bound:  3.15350652e-01\n",
      "Epoch: 32509 mean train loss:  3.63444677e-03, bound:  3.15350652e-01\n",
      "Epoch: 32510 mean train loss:  3.63437785e-03, bound:  3.15350652e-01\n",
      "Epoch: 32511 mean train loss:  3.63435410e-03, bound:  3.15350652e-01\n",
      "Epoch: 32512 mean train loss:  3.63427657e-03, bound:  3.15350652e-01\n",
      "Epoch: 32513 mean train loss:  3.63422651e-03, bound:  3.15350622e-01\n",
      "Epoch: 32514 mean train loss:  3.63415410e-03, bound:  3.15350622e-01\n",
      "Epoch: 32515 mean train loss:  3.63411871e-03, bound:  3.15350622e-01\n",
      "Epoch: 32516 mean train loss:  3.63410730e-03, bound:  3.15350622e-01\n",
      "Epoch: 32517 mean train loss:  3.63405468e-03, bound:  3.15350622e-01\n",
      "Epoch: 32518 mean train loss:  3.63396760e-03, bound:  3.15350622e-01\n",
      "Epoch: 32519 mean train loss:  3.63394874e-03, bound:  3.15350622e-01\n",
      "Epoch: 32520 mean train loss:  3.63387866e-03, bound:  3.15350622e-01\n",
      "Epoch: 32521 mean train loss:  3.63385864e-03, bound:  3.15350592e-01\n",
      "Epoch: 32522 mean train loss:  3.63380834e-03, bound:  3.15350592e-01\n",
      "Epoch: 32523 mean train loss:  3.63373943e-03, bound:  3.15350592e-01\n",
      "Epoch: 32524 mean train loss:  3.63366306e-03, bound:  3.15350592e-01\n",
      "Epoch: 32525 mean train loss:  3.63366329e-03, bound:  3.15350533e-01\n",
      "Epoch: 32526 mean train loss:  3.63360229e-03, bound:  3.15350533e-01\n",
      "Epoch: 32527 mean train loss:  3.63353291e-03, bound:  3.15350533e-01\n",
      "Epoch: 32528 mean train loss:  3.63348681e-03, bound:  3.15350533e-01\n",
      "Epoch: 32529 mean train loss:  3.63343349e-03, bound:  3.15350533e-01\n",
      "Epoch: 32530 mean train loss:  3.63336620e-03, bound:  3.15350533e-01\n",
      "Epoch: 32531 mean train loss:  3.63332895e-03, bound:  3.15350533e-01\n",
      "Epoch: 32532 mean train loss:  3.63329542e-03, bound:  3.15350533e-01\n",
      "Epoch: 32533 mean train loss:  3.63323675e-03, bound:  3.15350533e-01\n",
      "Epoch: 32534 mean train loss:  3.63321952e-03, bound:  3.15350533e-01\n",
      "Epoch: 32535 mean train loss:  3.63314827e-03, bound:  3.15350503e-01\n",
      "Epoch: 32536 mean train loss:  3.63311078e-03, bound:  3.15350503e-01\n",
      "Epoch: 32537 mean train loss:  3.63296899e-03, bound:  3.15350503e-01\n",
      "Epoch: 32538 mean train loss:  3.63298226e-03, bound:  3.15350503e-01\n",
      "Epoch: 32539 mean train loss:  3.63295339e-03, bound:  3.15350503e-01\n",
      "Epoch: 32540 mean train loss:  3.63291963e-03, bound:  3.15350503e-01\n",
      "Epoch: 32541 mean train loss:  3.63279879e-03, bound:  3.15350503e-01\n",
      "Epoch: 32542 mean train loss:  3.63278319e-03, bound:  3.15350443e-01\n",
      "Epoch: 32543 mean train loss:  3.63274477e-03, bound:  3.15350443e-01\n",
      "Epoch: 32544 mean train loss:  3.63266701e-03, bound:  3.15350443e-01\n",
      "Epoch: 32545 mean train loss:  3.63262696e-03, bound:  3.15350443e-01\n",
      "Epoch: 32546 mean train loss:  3.63253569e-03, bound:  3.15350443e-01\n",
      "Epoch: 32547 mean train loss:  3.63250193e-03, bound:  3.15350443e-01\n",
      "Epoch: 32548 mean train loss:  3.63238435e-03, bound:  3.15350443e-01\n",
      "Epoch: 32549 mean train loss:  3.63240065e-03, bound:  3.15350443e-01\n",
      "Epoch: 32550 mean train loss:  3.63237783e-03, bound:  3.15350443e-01\n",
      "Epoch: 32551 mean train loss:  3.63232777e-03, bound:  3.15350443e-01\n",
      "Epoch: 32552 mean train loss:  3.63223045e-03, bound:  3.15350443e-01\n",
      "Epoch: 32553 mean train loss:  3.63219227e-03, bound:  3.15350413e-01\n",
      "Epoch: 32554 mean train loss:  3.63213220e-03, bound:  3.15350384e-01\n",
      "Epoch: 32555 mean train loss:  3.63207865e-03, bound:  3.15350384e-01\n",
      "Epoch: 32556 mean train loss:  3.63207818e-03, bound:  3.15350384e-01\n",
      "Epoch: 32557 mean train loss:  3.63202184e-03, bound:  3.15350384e-01\n",
      "Epoch: 32558 mean train loss:  3.63194244e-03, bound:  3.15350384e-01\n",
      "Epoch: 32559 mean train loss:  3.63191543e-03, bound:  3.15350384e-01\n",
      "Epoch: 32560 mean train loss:  3.63183487e-03, bound:  3.15350384e-01\n",
      "Epoch: 32561 mean train loss:  3.63180321e-03, bound:  3.15350324e-01\n",
      "Epoch: 32562 mean train loss:  3.63173336e-03, bound:  3.15350324e-01\n",
      "Epoch: 32563 mean train loss:  3.63166351e-03, bound:  3.15350324e-01\n",
      "Epoch: 32564 mean train loss:  3.63161135e-03, bound:  3.15350324e-01\n",
      "Epoch: 32565 mean train loss:  3.63157992e-03, bound:  3.15350324e-01\n",
      "Epoch: 32566 mean train loss:  3.63155873e-03, bound:  3.15350324e-01\n",
      "Epoch: 32567 mean train loss:  3.63149261e-03, bound:  3.15350324e-01\n",
      "Epoch: 32568 mean train loss:  3.63142206e-03, bound:  3.15350324e-01\n",
      "Epoch: 32569 mean train loss:  3.63141834e-03, bound:  3.15350324e-01\n",
      "Epoch: 32570 mean train loss:  3.63129610e-03, bound:  3.15350324e-01\n",
      "Epoch: 32571 mean train loss:  3.63130029e-03, bound:  3.15350324e-01\n",
      "Epoch: 32572 mean train loss:  3.63125093e-03, bound:  3.15350324e-01\n",
      "Epoch: 32573 mean train loss:  3.63117945e-03, bound:  3.15350324e-01\n",
      "Epoch: 32574 mean train loss:  3.63115384e-03, bound:  3.15350294e-01\n",
      "Epoch: 32575 mean train loss:  3.63107678e-03, bound:  3.15350294e-01\n",
      "Epoch: 32576 mean train loss:  3.63100995e-03, bound:  3.15350264e-01\n",
      "Epoch: 32577 mean train loss:  3.63098853e-03, bound:  3.15350264e-01\n",
      "Epoch: 32578 mean train loss:  3.63094476e-03, bound:  3.15350264e-01\n",
      "Epoch: 32579 mean train loss:  3.63089889e-03, bound:  3.15350264e-01\n",
      "Epoch: 32580 mean train loss:  3.63083207e-03, bound:  3.15350264e-01\n",
      "Epoch: 32581 mean train loss:  3.63077573e-03, bound:  3.15350264e-01\n",
      "Epoch: 32582 mean train loss:  3.63074220e-03, bound:  3.15350205e-01\n",
      "Epoch: 32583 mean train loss:  3.63068865e-03, bound:  3.15350205e-01\n",
      "Epoch: 32584 mean train loss:  3.63058411e-03, bound:  3.15350205e-01\n",
      "Epoch: 32585 mean train loss:  3.63053405e-03, bound:  3.15350205e-01\n",
      "Epoch: 32586 mean train loss:  3.63052962e-03, bound:  3.15350205e-01\n",
      "Epoch: 32587 mean train loss:  3.63042788e-03, bound:  3.15350205e-01\n",
      "Epoch: 32588 mean train loss:  3.63037572e-03, bound:  3.15350205e-01\n",
      "Epoch: 32589 mean train loss:  3.63036268e-03, bound:  3.15350205e-01\n",
      "Epoch: 32590 mean train loss:  3.63030611e-03, bound:  3.15350205e-01\n",
      "Epoch: 32591 mean train loss:  3.63028399e-03, bound:  3.15350205e-01\n",
      "Epoch: 32592 mean train loss:  3.63020948e-03, bound:  3.15350205e-01\n",
      "Epoch: 32593 mean train loss:  3.63012729e-03, bound:  3.15350175e-01\n",
      "Epoch: 32594 mean train loss:  3.63012147e-03, bound:  3.15350175e-01\n",
      "Epoch: 32595 mean train loss:  3.63002415e-03, bound:  3.15350175e-01\n",
      "Epoch: 32596 mean train loss:  3.63001600e-03, bound:  3.15350175e-01\n",
      "Epoch: 32597 mean train loss:  3.62995872e-03, bound:  3.15350175e-01\n",
      "Epoch: 32598 mean train loss:  3.62988072e-03, bound:  3.15350175e-01\n",
      "Epoch: 32599 mean train loss:  3.62984766e-03, bound:  3.15350175e-01\n",
      "Epoch: 32600 mean train loss:  3.62977060e-03, bound:  3.15350175e-01\n",
      "Epoch: 32601 mean train loss:  3.62975243e-03, bound:  3.15350145e-01\n",
      "Epoch: 32602 mean train loss:  3.62964952e-03, bound:  3.15350145e-01\n",
      "Epoch: 32603 mean train loss:  3.62961576e-03, bound:  3.15350145e-01\n",
      "Epoch: 32604 mean train loss:  3.62958782e-03, bound:  3.15350145e-01\n",
      "Epoch: 32605 mean train loss:  3.62949539e-03, bound:  3.15350145e-01\n",
      "Epoch: 32606 mean train loss:  3.62945744e-03, bound:  3.15350145e-01\n",
      "Epoch: 32607 mean train loss:  3.62939481e-03, bound:  3.15350145e-01\n",
      "Epoch: 32608 mean train loss:  3.62937315e-03, bound:  3.15350085e-01\n",
      "Epoch: 32609 mean train loss:  3.62931984e-03, bound:  3.15350085e-01\n",
      "Epoch: 32610 mean train loss:  3.62924160e-03, bound:  3.15350085e-01\n",
      "Epoch: 32611 mean train loss:  3.62921692e-03, bound:  3.15350085e-01\n",
      "Epoch: 32612 mean train loss:  3.62912496e-03, bound:  3.15350085e-01\n",
      "Epoch: 32613 mean train loss:  3.62909166e-03, bound:  3.15350085e-01\n",
      "Epoch: 32614 mean train loss:  3.62904090e-03, bound:  3.15350085e-01\n",
      "Epoch: 32615 mean train loss:  3.62899178e-03, bound:  3.15350056e-01\n",
      "Epoch: 32616 mean train loss:  3.62894218e-03, bound:  3.15350056e-01\n",
      "Epoch: 32617 mean train loss:  3.62888305e-03, bound:  3.15350056e-01\n",
      "Epoch: 32618 mean train loss:  3.62887350e-03, bound:  3.15350056e-01\n",
      "Epoch: 32619 mean train loss:  3.62881692e-03, bound:  3.15350056e-01\n",
      "Epoch: 32620 mean train loss:  3.62877245e-03, bound:  3.15350056e-01\n",
      "Epoch: 32621 mean train loss:  3.62871401e-03, bound:  3.15350056e-01\n",
      "Epoch: 32622 mean train loss:  3.62864952e-03, bound:  3.15350056e-01\n",
      "Epoch: 32623 mean train loss:  3.62859014e-03, bound:  3.15349996e-01\n",
      "Epoch: 32624 mean train loss:  3.62853520e-03, bound:  3.15349996e-01\n",
      "Epoch: 32625 mean train loss:  3.62846907e-03, bound:  3.15349996e-01\n",
      "Epoch: 32626 mean train loss:  3.62842646e-03, bound:  3.15349996e-01\n",
      "Epoch: 32627 mean train loss:  3.62842460e-03, bound:  3.15349996e-01\n",
      "Epoch: 32628 mean train loss:  3.62833706e-03, bound:  3.15349966e-01\n",
      "Epoch: 32629 mean train loss:  3.62830958e-03, bound:  3.15349966e-01\n",
      "Epoch: 32630 mean train loss:  3.62822739e-03, bound:  3.15349966e-01\n",
      "Epoch: 32631 mean train loss:  3.62818525e-03, bound:  3.15349966e-01\n",
      "Epoch: 32632 mean train loss:  3.62812425e-03, bound:  3.15349966e-01\n",
      "Epoch: 32633 mean train loss:  3.62805976e-03, bound:  3.15349966e-01\n",
      "Epoch: 32634 mean train loss:  3.62801296e-03, bound:  3.15349936e-01\n",
      "Epoch: 32635 mean train loss:  3.62798083e-03, bound:  3.15349936e-01\n",
      "Epoch: 32636 mean train loss:  3.62793892e-03, bound:  3.15349936e-01\n",
      "Epoch: 32637 mean train loss:  3.62783694e-03, bound:  3.15349936e-01\n",
      "Epoch: 32638 mean train loss:  3.62779689e-03, bound:  3.15349936e-01\n",
      "Epoch: 32639 mean train loss:  3.62777105e-03, bound:  3.15349936e-01\n",
      "Epoch: 32640 mean train loss:  3.62768257e-03, bound:  3.15349936e-01\n",
      "Epoch: 32641 mean train loss:  3.62764532e-03, bound:  3.15349936e-01\n",
      "Epoch: 32642 mean train loss:  3.62765160e-03, bound:  3.15349877e-01\n",
      "Epoch: 32643 mean train loss:  3.62756150e-03, bound:  3.15349877e-01\n",
      "Epoch: 32644 mean train loss:  3.62743880e-03, bound:  3.15349877e-01\n",
      "Epoch: 32645 mean train loss:  3.62742785e-03, bound:  3.15349877e-01\n",
      "Epoch: 32646 mean train loss:  3.62738175e-03, bound:  3.15349877e-01\n",
      "Epoch: 32647 mean train loss:  3.62727977e-03, bound:  3.15349877e-01\n",
      "Epoch: 32648 mean train loss:  3.62723647e-03, bound:  3.15349877e-01\n",
      "Epoch: 32649 mean train loss:  3.62724718e-03, bound:  3.15349847e-01\n",
      "Epoch: 32650 mean train loss:  3.62716615e-03, bound:  3.15349847e-01\n",
      "Epoch: 32651 mean train loss:  3.62715451e-03, bound:  3.15349847e-01\n",
      "Epoch: 32652 mean train loss:  3.62706766e-03, bound:  3.15349847e-01\n",
      "Epoch: 32653 mean train loss:  3.62701993e-03, bound:  3.15349847e-01\n",
      "Epoch: 32654 mean train loss:  3.62696871e-03, bound:  3.15349817e-01\n",
      "Epoch: 32655 mean train loss:  3.62692541e-03, bound:  3.15349817e-01\n",
      "Epoch: 32656 mean train loss:  3.62685812e-03, bound:  3.15349817e-01\n",
      "Epoch: 32657 mean train loss:  3.62677500e-03, bound:  3.15349817e-01\n",
      "Epoch: 32658 mean train loss:  3.62675358e-03, bound:  3.15349817e-01\n",
      "Epoch: 32659 mean train loss:  3.62674915e-03, bound:  3.15349817e-01\n",
      "Epoch: 32660 mean train loss:  3.62662249e-03, bound:  3.15349817e-01\n",
      "Epoch: 32661 mean train loss:  3.62661225e-03, bound:  3.15349817e-01\n",
      "Epoch: 32662 mean train loss:  3.62659944e-03, bound:  3.15349758e-01\n",
      "Epoch: 32663 mean train loss:  3.62651027e-03, bound:  3.15349758e-01\n",
      "Epoch: 32664 mean train loss:  3.62641527e-03, bound:  3.15349758e-01\n",
      "Epoch: 32665 mean train loss:  3.62638757e-03, bound:  3.15349758e-01\n",
      "Epoch: 32666 mean train loss:  3.62632377e-03, bound:  3.15349758e-01\n",
      "Epoch: 32667 mean train loss:  3.62631306e-03, bound:  3.15349758e-01\n",
      "Epoch: 32668 mean train loss:  3.62622738e-03, bound:  3.15349758e-01\n",
      "Epoch: 32669 mean train loss:  3.62616265e-03, bound:  3.15349758e-01\n",
      "Epoch: 32670 mean train loss:  3.62609862e-03, bound:  3.15349728e-01\n",
      "Epoch: 32671 mean train loss:  3.62605229e-03, bound:  3.15349728e-01\n",
      "Epoch: 32672 mean train loss:  3.62602132e-03, bound:  3.15349728e-01\n",
      "Epoch: 32673 mean train loss:  3.62598384e-03, bound:  3.15349728e-01\n",
      "Epoch: 32674 mean train loss:  3.62587511e-03, bound:  3.15349698e-01\n",
      "Epoch: 32675 mean train loss:  3.62582272e-03, bound:  3.15349698e-01\n",
      "Epoch: 32676 mean train loss:  3.62579338e-03, bound:  3.15349698e-01\n",
      "Epoch: 32677 mean train loss:  3.62577639e-03, bound:  3.15349698e-01\n",
      "Epoch: 32678 mean train loss:  3.62571701e-03, bound:  3.15349698e-01\n",
      "Epoch: 32679 mean train loss:  3.62565275e-03, bound:  3.15349698e-01\n",
      "Epoch: 32680 mean train loss:  3.62559035e-03, bound:  3.15349698e-01\n",
      "Epoch: 32681 mean train loss:  3.62556521e-03, bound:  3.15349698e-01\n",
      "Epoch: 32682 mean train loss:  3.62547417e-03, bound:  3.15349638e-01\n",
      "Epoch: 32683 mean train loss:  3.62545741e-03, bound:  3.15349638e-01\n",
      "Epoch: 32684 mean train loss:  3.62535426e-03, bound:  3.15349638e-01\n",
      "Epoch: 32685 mean train loss:  3.62531538e-03, bound:  3.15349638e-01\n",
      "Epoch: 32686 mean train loss:  3.62523086e-03, bound:  3.15349638e-01\n",
      "Epoch: 32687 mean train loss:  3.62517824e-03, bound:  3.15349638e-01\n",
      "Epoch: 32688 mean train loss:  3.62516148e-03, bound:  3.15349638e-01\n",
      "Epoch: 32689 mean train loss:  3.62508954e-03, bound:  3.15349638e-01\n",
      "Epoch: 32690 mean train loss:  3.62508558e-03, bound:  3.15349638e-01\n",
      "Epoch: 32691 mean train loss:  3.62497056e-03, bound:  3.15349638e-01\n",
      "Epoch: 32692 mean train loss:  3.62492376e-03, bound:  3.15349609e-01\n",
      "Epoch: 32693 mean train loss:  3.62489722e-03, bound:  3.15349609e-01\n",
      "Epoch: 32694 mean train loss:  3.62485368e-03, bound:  3.15349579e-01\n",
      "Epoch: 32695 mean train loss:  3.62474960e-03, bound:  3.15349579e-01\n",
      "Epoch: 32696 mean train loss:  3.62474099e-03, bound:  3.15349579e-01\n",
      "Epoch: 32697 mean train loss:  3.62469279e-03, bound:  3.15349579e-01\n",
      "Epoch: 32698 mean train loss:  3.62462667e-03, bound:  3.15349579e-01\n",
      "Epoch: 32699 mean train loss:  3.62456287e-03, bound:  3.15349579e-01\n",
      "Epoch: 32700 mean train loss:  3.62450932e-03, bound:  3.15349579e-01\n",
      "Epoch: 32701 mean train loss:  3.62445158e-03, bound:  3.15349519e-01\n",
      "Epoch: 32702 mean train loss:  3.62442946e-03, bound:  3.15349519e-01\n",
      "Epoch: 32703 mean train loss:  3.62438243e-03, bound:  3.15349519e-01\n",
      "Epoch: 32704 mean train loss:  3.62427230e-03, bound:  3.15349519e-01\n",
      "Epoch: 32705 mean train loss:  3.62426438e-03, bound:  3.15349519e-01\n",
      "Epoch: 32706 mean train loss:  3.62419547e-03, bound:  3.15349519e-01\n",
      "Epoch: 32707 mean train loss:  3.62415402e-03, bound:  3.15349519e-01\n",
      "Epoch: 32708 mean train loss:  3.62413260e-03, bound:  3.15349519e-01\n",
      "Epoch: 32709 mean train loss:  3.62401456e-03, bound:  3.15349519e-01\n",
      "Epoch: 32710 mean train loss:  3.62394401e-03, bound:  3.15349519e-01\n",
      "Epoch: 32711 mean train loss:  3.62394610e-03, bound:  3.15349519e-01\n",
      "Epoch: 32712 mean train loss:  3.62387113e-03, bound:  3.15349519e-01\n",
      "Epoch: 32713 mean train loss:  3.62386159e-03, bound:  3.15349489e-01\n",
      "Epoch: 32714 mean train loss:  3.62380059e-03, bound:  3.15349489e-01\n",
      "Epoch: 32715 mean train loss:  3.62374191e-03, bound:  3.15349489e-01\n",
      "Epoch: 32716 mean train loss:  3.62364459e-03, bound:  3.15349489e-01\n",
      "Epoch: 32717 mean train loss:  3.62360803e-03, bound:  3.15349489e-01\n",
      "Epoch: 32718 mean train loss:  3.62360617e-03, bound:  3.15349489e-01\n",
      "Epoch: 32719 mean train loss:  3.62355285e-03, bound:  3.15349400e-01\n",
      "Epoch: 32720 mean train loss:  3.62345437e-03, bound:  3.15349400e-01\n",
      "Epoch: 32721 mean train loss:  3.62342480e-03, bound:  3.15349400e-01\n",
      "Epoch: 32722 mean train loss:  3.62333143e-03, bound:  3.15349400e-01\n",
      "Epoch: 32723 mean train loss:  3.62328463e-03, bound:  3.15349400e-01\n",
      "Epoch: 32724 mean train loss:  3.62320617e-03, bound:  3.15349400e-01\n",
      "Epoch: 32725 mean train loss:  3.62318731e-03, bound:  3.15349400e-01\n",
      "Epoch: 32726 mean train loss:  3.62313469e-03, bound:  3.15349400e-01\n",
      "Epoch: 32727 mean train loss:  3.62309976e-03, bound:  3.15349400e-01\n",
      "Epoch: 32728 mean train loss:  3.62301921e-03, bound:  3.15349400e-01\n",
      "Epoch: 32729 mean train loss:  3.62293818e-03, bound:  3.15349400e-01\n",
      "Epoch: 32730 mean train loss:  3.62293446e-03, bound:  3.15349400e-01\n",
      "Epoch: 32731 mean train loss:  3.62285366e-03, bound:  3.15349400e-01\n",
      "Epoch: 32732 mean train loss:  3.62282200e-03, bound:  3.15349370e-01\n",
      "Epoch: 32733 mean train loss:  3.62272142e-03, bound:  3.15349370e-01\n",
      "Epoch: 32734 mean train loss:  3.62270698e-03, bound:  3.15349370e-01\n",
      "Epoch: 32735 mean train loss:  3.62265995e-03, bound:  3.15349370e-01\n",
      "Epoch: 32736 mean train loss:  3.62260244e-03, bound:  3.15349370e-01\n",
      "Epoch: 32737 mean train loss:  3.62255890e-03, bound:  3.15349370e-01\n",
      "Epoch: 32738 mean train loss:  3.62244574e-03, bound:  3.15349370e-01\n",
      "Epoch: 32739 mean train loss:  3.62244062e-03, bound:  3.15349370e-01\n",
      "Epoch: 32740 mean train loss:  3.62234353e-03, bound:  3.15349311e-01\n",
      "Epoch: 32741 mean train loss:  3.62230628e-03, bound:  3.15349311e-01\n",
      "Epoch: 32742 mean train loss:  3.62229114e-03, bound:  3.15349311e-01\n",
      "Epoch: 32743 mean train loss:  3.62219498e-03, bound:  3.15349311e-01\n",
      "Epoch: 32744 mean train loss:  3.62217659e-03, bound:  3.15349311e-01\n",
      "Epoch: 32745 mean train loss:  3.62211606e-03, bound:  3.15349281e-01\n",
      "Epoch: 32746 mean train loss:  3.62205133e-03, bound:  3.15349281e-01\n",
      "Epoch: 32747 mean train loss:  3.62201990e-03, bound:  3.15349281e-01\n",
      "Epoch: 32748 mean train loss:  3.62196844e-03, bound:  3.15349281e-01\n",
      "Epoch: 32749 mean train loss:  3.62188648e-03, bound:  3.15349281e-01\n",
      "Epoch: 32750 mean train loss:  3.62187135e-03, bound:  3.15349281e-01\n",
      "Epoch: 32751 mean train loss:  3.62180243e-03, bound:  3.15349281e-01\n",
      "Epoch: 32752 mean train loss:  3.62174679e-03, bound:  3.15349251e-01\n",
      "Epoch: 32753 mean train loss:  3.62165854e-03, bound:  3.15349251e-01\n",
      "Epoch: 32754 mean train loss:  3.62163340e-03, bound:  3.15349251e-01\n",
      "Epoch: 32755 mean train loss:  3.62157961e-03, bound:  3.15349251e-01\n",
      "Epoch: 32756 mean train loss:  3.62152257e-03, bound:  3.15349251e-01\n",
      "Epoch: 32757 mean train loss:  3.62148834e-03, bound:  3.15349251e-01\n",
      "Epoch: 32758 mean train loss:  3.62140033e-03, bound:  3.15349251e-01\n",
      "Epoch: 32759 mean train loss:  3.62132280e-03, bound:  3.15349251e-01\n",
      "Epoch: 32760 mean train loss:  3.62132885e-03, bound:  3.15349191e-01\n",
      "Epoch: 32761 mean train loss:  3.62125132e-03, bound:  3.15349191e-01\n",
      "Epoch: 32762 mean train loss:  3.62121430e-03, bound:  3.15349191e-01\n",
      "Epoch: 32763 mean train loss:  3.62116168e-03, bound:  3.15349191e-01\n",
      "Epoch: 32764 mean train loss:  3.62113351e-03, bound:  3.15349191e-01\n",
      "Epoch: 32765 mean train loss:  3.62097682e-03, bound:  3.15349191e-01\n",
      "Epoch: 32766 mean train loss:  3.62094375e-03, bound:  3.15349162e-01\n",
      "Epoch: 32767 mean train loss:  3.62090301e-03, bound:  3.15349162e-01\n",
      "Epoch: 32768 mean train loss:  3.62083851e-03, bound:  3.15349162e-01\n",
      "Epoch: 32769 mean train loss:  3.62077681e-03, bound:  3.15349162e-01\n",
      "Epoch: 32770 mean train loss:  3.62074515e-03, bound:  3.15349162e-01\n",
      "Epoch: 32771 mean train loss:  3.62068368e-03, bound:  3.15349132e-01\n",
      "Epoch: 32772 mean train loss:  3.62066412e-03, bound:  3.15349132e-01\n",
      "Epoch: 32773 mean train loss:  3.62059707e-03, bound:  3.15349132e-01\n",
      "Epoch: 32774 mean train loss:  3.62054072e-03, bound:  3.15349132e-01\n",
      "Epoch: 32775 mean train loss:  3.62053909e-03, bound:  3.15349132e-01\n",
      "Epoch: 32776 mean train loss:  3.62040638e-03, bound:  3.15349132e-01\n",
      "Epoch: 32777 mean train loss:  3.62038170e-03, bound:  3.15349132e-01\n",
      "Epoch: 32778 mean train loss:  3.62030556e-03, bound:  3.15349072e-01\n",
      "Epoch: 32779 mean train loss:  3.62028531e-03, bound:  3.15349072e-01\n",
      "Epoch: 32780 mean train loss:  3.62022081e-03, bound:  3.15349072e-01\n",
      "Epoch: 32781 mean train loss:  3.62017634e-03, bound:  3.15349072e-01\n",
      "Epoch: 32782 mean train loss:  3.62011278e-03, bound:  3.15349072e-01\n",
      "Epoch: 32783 mean train loss:  3.62007366e-03, bound:  3.15349072e-01\n",
      "Epoch: 32784 mean train loss:  3.62000288e-03, bound:  3.15349072e-01\n",
      "Epoch: 32785 mean train loss:  3.61993816e-03, bound:  3.15349072e-01\n",
      "Epoch: 32786 mean train loss:  3.61989136e-03, bound:  3.15349042e-01\n",
      "Epoch: 32787 mean train loss:  3.61985713e-03, bound:  3.15349042e-01\n",
      "Epoch: 32788 mean train loss:  3.61975352e-03, bound:  3.15349042e-01\n",
      "Epoch: 32789 mean train loss:  3.61972186e-03, bound:  3.15349042e-01\n",
      "Epoch: 32790 mean train loss:  3.61962523e-03, bound:  3.15349013e-01\n",
      "Epoch: 32791 mean train loss:  3.61964060e-03, bound:  3.15349013e-01\n",
      "Epoch: 32792 mean train loss:  3.61955096e-03, bound:  3.15349013e-01\n",
      "Epoch: 32793 mean train loss:  3.61948437e-03, bound:  3.15349013e-01\n",
      "Epoch: 32794 mean train loss:  3.61947063e-03, bound:  3.15349013e-01\n",
      "Epoch: 32795 mean train loss:  3.61938495e-03, bound:  3.15349013e-01\n",
      "Epoch: 32796 mean train loss:  3.61931953e-03, bound:  3.15349013e-01\n",
      "Epoch: 32797 mean train loss:  3.61926598e-03, bound:  3.15348983e-01\n",
      "Epoch: 32798 mean train loss:  3.61919845e-03, bound:  3.15348953e-01\n",
      "Epoch: 32799 mean train loss:  3.61918379e-03, bound:  3.15348953e-01\n",
      "Epoch: 32800 mean train loss:  3.61913536e-03, bound:  3.15348953e-01\n",
      "Epoch: 32801 mean train loss:  3.61902453e-03, bound:  3.15348953e-01\n",
      "Epoch: 32802 mean train loss:  3.61898053e-03, bound:  3.15348953e-01\n",
      "Epoch: 32803 mean train loss:  3.61895864e-03, bound:  3.15348953e-01\n",
      "Epoch: 32804 mean train loss:  3.61892232e-03, bound:  3.15348923e-01\n",
      "Epoch: 32805 mean train loss:  3.61882779e-03, bound:  3.15348923e-01\n",
      "Epoch: 32806 mean train loss:  3.61879682e-03, bound:  3.15348923e-01\n",
      "Epoch: 32807 mean train loss:  3.61876702e-03, bound:  3.15348923e-01\n",
      "Epoch: 32808 mean train loss:  3.61865014e-03, bound:  3.15348923e-01\n",
      "Epoch: 32809 mean train loss:  3.61860567e-03, bound:  3.15348923e-01\n",
      "Epoch: 32810 mean train loss:  3.61860520e-03, bound:  3.15348923e-01\n",
      "Epoch: 32811 mean train loss:  3.61851207e-03, bound:  3.15348923e-01\n",
      "Epoch: 32812 mean train loss:  3.61846457e-03, bound:  3.15348923e-01\n",
      "Epoch: 32813 mean train loss:  3.61836446e-03, bound:  3.15348923e-01\n",
      "Epoch: 32814 mean train loss:  3.61833139e-03, bound:  3.15348923e-01\n",
      "Epoch: 32815 mean train loss:  3.61830345e-03, bound:  3.15348923e-01\n",
      "Epoch: 32816 mean train loss:  3.61824501e-03, bound:  3.15348864e-01\n",
      "Epoch: 32817 mean train loss:  3.61822359e-03, bound:  3.15348834e-01\n",
      "Epoch: 32818 mean train loss:  3.61809367e-03, bound:  3.15348834e-01\n",
      "Epoch: 32819 mean train loss:  3.61811975e-03, bound:  3.15348834e-01\n",
      "Epoch: 32820 mean train loss:  3.61802801e-03, bound:  3.15348834e-01\n",
      "Epoch: 32821 mean train loss:  3.61794117e-03, bound:  3.15348834e-01\n",
      "Epoch: 32822 mean train loss:  3.61792697e-03, bound:  3.15348834e-01\n",
      "Epoch: 32823 mean train loss:  3.61788040e-03, bound:  3.15348834e-01\n",
      "Epoch: 32824 mean train loss:  3.61781754e-03, bound:  3.15348834e-01\n",
      "Epoch: 32825 mean train loss:  3.61772440e-03, bound:  3.15348834e-01\n",
      "Epoch: 32826 mean train loss:  3.61769367e-03, bound:  3.15348834e-01\n",
      "Epoch: 32827 mean train loss:  3.61763942e-03, bound:  3.15348834e-01\n",
      "Epoch: 32828 mean train loss:  3.61760939e-03, bound:  3.15348804e-01\n",
      "Epoch: 32829 mean train loss:  3.61751160e-03, bound:  3.15348804e-01\n",
      "Epoch: 32830 mean train loss:  3.61745548e-03, bound:  3.15348804e-01\n",
      "Epoch: 32831 mean train loss:  3.61741008e-03, bound:  3.15348804e-01\n",
      "Epoch: 32832 mean train loss:  3.61735607e-03, bound:  3.15348804e-01\n",
      "Epoch: 32833 mean train loss:  3.61730740e-03, bound:  3.15348804e-01\n",
      "Epoch: 32834 mean train loss:  3.61721963e-03, bound:  3.15348804e-01\n",
      "Epoch: 32835 mean train loss:  3.61723546e-03, bound:  3.15348744e-01\n",
      "Epoch: 32836 mean train loss:  3.61712743e-03, bound:  3.15348715e-01\n",
      "Epoch: 32837 mean train loss:  3.61708109e-03, bound:  3.15348715e-01\n",
      "Epoch: 32838 mean train loss:  3.61702638e-03, bound:  3.15348715e-01\n",
      "Epoch: 32839 mean train loss:  3.61691858e-03, bound:  3.15348715e-01\n",
      "Epoch: 32840 mean train loss:  3.61692579e-03, bound:  3.15348715e-01\n",
      "Epoch: 32841 mean train loss:  3.61686852e-03, bound:  3.15348715e-01\n",
      "Epoch: 32842 mean train loss:  3.61682312e-03, bound:  3.15348715e-01\n",
      "Epoch: 32843 mean train loss:  3.61675560e-03, bound:  3.15348715e-01\n",
      "Epoch: 32844 mean train loss:  3.61669483e-03, bound:  3.15348715e-01\n",
      "Epoch: 32845 mean train loss:  3.61663965e-03, bound:  3.15348715e-01\n",
      "Epoch: 32846 mean train loss:  3.61656002e-03, bound:  3.15348715e-01\n",
      "Epoch: 32847 mean train loss:  3.61650717e-03, bound:  3.15348685e-01\n",
      "Epoch: 32848 mean train loss:  3.61644919e-03, bound:  3.15348685e-01\n",
      "Epoch: 32849 mean train loss:  3.61641706e-03, bound:  3.15348685e-01\n",
      "Epoch: 32850 mean train loss:  3.61639215e-03, bound:  3.15348685e-01\n",
      "Epoch: 32851 mean train loss:  3.61629762e-03, bound:  3.15348685e-01\n",
      "Epoch: 32852 mean train loss:  3.61621939e-03, bound:  3.15348685e-01\n",
      "Epoch: 32853 mean train loss:  3.61618353e-03, bound:  3.15348625e-01\n",
      "Epoch: 32854 mean train loss:  3.61610972e-03, bound:  3.15348625e-01\n",
      "Epoch: 32855 mean train loss:  3.61602474e-03, bound:  3.15348595e-01\n",
      "Epoch: 32856 mean train loss:  3.61603545e-03, bound:  3.15348595e-01\n",
      "Epoch: 32857 mean train loss:  3.61597585e-03, bound:  3.15348595e-01\n",
      "Epoch: 32858 mean train loss:  3.61592346e-03, bound:  3.15348595e-01\n",
      "Epoch: 32859 mean train loss:  3.61584988e-03, bound:  3.15348595e-01\n",
      "Epoch: 32860 mean train loss:  3.61580122e-03, bound:  3.15348595e-01\n",
      "Epoch: 32861 mean train loss:  3.61574884e-03, bound:  3.15348595e-01\n",
      "Epoch: 32862 mean train loss:  3.61569668e-03, bound:  3.15348595e-01\n",
      "Epoch: 32863 mean train loss:  3.61562660e-03, bound:  3.15348595e-01\n",
      "Epoch: 32864 mean train loss:  3.61559098e-03, bound:  3.15348595e-01\n",
      "Epoch: 32865 mean train loss:  3.61558655e-03, bound:  3.15348595e-01\n",
      "Epoch: 32866 mean train loss:  3.61545617e-03, bound:  3.15348566e-01\n",
      "Epoch: 32867 mean train loss:  3.61539167e-03, bound:  3.15348536e-01\n",
      "Epoch: 32868 mean train loss:  3.61534860e-03, bound:  3.15348536e-01\n",
      "Epoch: 32869 mean train loss:  3.61529761e-03, bound:  3.15348536e-01\n",
      "Epoch: 32870 mean train loss:  3.61521379e-03, bound:  3.15348536e-01\n",
      "Epoch: 32871 mean train loss:  3.61517840e-03, bound:  3.15348536e-01\n",
      "Epoch: 32872 mean train loss:  3.61517677e-03, bound:  3.15348506e-01\n",
      "Epoch: 32873 mean train loss:  3.61508247e-03, bound:  3.15348506e-01\n",
      "Epoch: 32874 mean train loss:  3.61504313e-03, bound:  3.15348506e-01\n",
      "Epoch: 32875 mean train loss:  3.61499237e-03, bound:  3.15348506e-01\n",
      "Epoch: 32876 mean train loss:  3.61495023e-03, bound:  3.15348506e-01\n",
      "Epoch: 32877 mean train loss:  3.61486711e-03, bound:  3.15348506e-01\n",
      "Epoch: 32878 mean train loss:  3.61481588e-03, bound:  3.15348506e-01\n",
      "Epoch: 32879 mean train loss:  3.61476396e-03, bound:  3.15348476e-01\n",
      "Epoch: 32880 mean train loss:  3.61471763e-03, bound:  3.15348476e-01\n",
      "Epoch: 32881 mean train loss:  3.61463381e-03, bound:  3.15348476e-01\n",
      "Epoch: 32882 mean train loss:  3.61461402e-03, bound:  3.15348476e-01\n",
      "Epoch: 32883 mean train loss:  3.61450855e-03, bound:  3.15348476e-01\n",
      "Epoch: 32884 mean train loss:  3.61450133e-03, bound:  3.15348446e-01\n",
      "Epoch: 32885 mean train loss:  3.61443078e-03, bound:  3.15348446e-01\n",
      "Epoch: 32886 mean train loss:  3.61438259e-03, bound:  3.15348417e-01\n",
      "Epoch: 32887 mean train loss:  3.61427781e-03, bound:  3.15348417e-01\n",
      "Epoch: 32888 mean train loss:  3.61420470e-03, bound:  3.15348417e-01\n",
      "Epoch: 32889 mean train loss:  3.61418258e-03, bound:  3.15348417e-01\n",
      "Epoch: 32890 mean train loss:  3.61415767e-03, bound:  3.15348417e-01\n",
      "Epoch: 32891 mean train loss:  3.61408782e-03, bound:  3.15348387e-01\n",
      "Epoch: 32892 mean train loss:  3.61399422e-03, bound:  3.15348387e-01\n",
      "Epoch: 32893 mean train loss:  3.61394626e-03, bound:  3.15348387e-01\n",
      "Epoch: 32894 mean train loss:  3.61388666e-03, bound:  3.15348387e-01\n",
      "Epoch: 32895 mean train loss:  3.61381867e-03, bound:  3.15348387e-01\n",
      "Epoch: 32896 mean train loss:  3.61381704e-03, bound:  3.15348387e-01\n",
      "Epoch: 32897 mean train loss:  3.61374090e-03, bound:  3.15348387e-01\n",
      "Epoch: 32898 mean train loss:  3.61368293e-03, bound:  3.15348357e-01\n",
      "Epoch: 32899 mean train loss:  3.61361052e-03, bound:  3.15348357e-01\n",
      "Epoch: 32900 mean train loss:  3.61357699e-03, bound:  3.15348357e-01\n",
      "Epoch: 32901 mean train loss:  3.61356745e-03, bound:  3.15348357e-01\n",
      "Epoch: 32902 mean train loss:  3.61346314e-03, bound:  3.15348357e-01\n",
      "Epoch: 32903 mean train loss:  3.61342076e-03, bound:  3.15348327e-01\n",
      "Epoch: 32904 mean train loss:  3.61332810e-03, bound:  3.15348327e-01\n",
      "Epoch: 32905 mean train loss:  3.61333345e-03, bound:  3.15348297e-01\n",
      "Epoch: 32906 mean train loss:  3.61326966e-03, bound:  3.15348297e-01\n",
      "Epoch: 32907 mean train loss:  3.61313391e-03, bound:  3.15348297e-01\n",
      "Epoch: 32908 mean train loss:  3.61312088e-03, bound:  3.15348297e-01\n",
      "Epoch: 32909 mean train loss:  3.61302728e-03, bound:  3.15348297e-01\n",
      "Epoch: 32910 mean train loss:  3.61300074e-03, bound:  3.15348297e-01\n",
      "Epoch: 32911 mean train loss:  3.61297745e-03, bound:  3.15348297e-01\n",
      "Epoch: 32912 mean train loss:  3.61286988e-03, bound:  3.15348268e-01\n",
      "Epoch: 32913 mean train loss:  3.61283636e-03, bound:  3.15348268e-01\n",
      "Epoch: 32914 mean train loss:  3.61275999e-03, bound:  3.15348268e-01\n",
      "Epoch: 32915 mean train loss:  3.61270038e-03, bound:  3.15348268e-01\n",
      "Epoch: 32916 mean train loss:  3.61265242e-03, bound:  3.15348268e-01\n",
      "Epoch: 32917 mean train loss:  3.61260236e-03, bound:  3.15348238e-01\n",
      "Epoch: 32918 mean train loss:  3.61256325e-03, bound:  3.15348238e-01\n",
      "Epoch: 32919 mean train loss:  3.61249177e-03, bound:  3.15348238e-01\n",
      "Epoch: 32920 mean train loss:  3.61244311e-03, bound:  3.15348238e-01\n",
      "Epoch: 32921 mean train loss:  3.61239002e-03, bound:  3.15348238e-01\n",
      "Epoch: 32922 mean train loss:  3.61236581e-03, bound:  3.15348238e-01\n",
      "Epoch: 32923 mean train loss:  3.61228338e-03, bound:  3.15348238e-01\n",
      "Epoch: 32924 mean train loss:  3.61227687e-03, bound:  3.15348238e-01\n",
      "Epoch: 32925 mean train loss:  3.61212413e-03, bound:  3.15348238e-01\n",
      "Epoch: 32926 mean train loss:  3.61207919e-03, bound:  3.15348238e-01\n",
      "Epoch: 32927 mean train loss:  3.61202122e-03, bound:  3.15348238e-01\n",
      "Epoch: 32928 mean train loss:  3.61201470e-03, bound:  3.15348178e-01\n",
      "Epoch: 32929 mean train loss:  3.61197279e-03, bound:  3.15348178e-01\n",
      "Epoch: 32930 mean train loss:  3.61187081e-03, bound:  3.15348148e-01\n",
      "Epoch: 32931 mean train loss:  3.61180631e-03, bound:  3.15348148e-01\n",
      "Epoch: 32932 mean train loss:  3.61176906e-03, bound:  3.15348148e-01\n",
      "Epoch: 32933 mean train loss:  3.61171947e-03, bound:  3.15348148e-01\n",
      "Epoch: 32934 mean train loss:  3.61167756e-03, bound:  3.15348148e-01\n",
      "Epoch: 32935 mean train loss:  3.61156953e-03, bound:  3.15348148e-01\n",
      "Epoch: 32936 mean train loss:  3.61152412e-03, bound:  3.15348119e-01\n",
      "Epoch: 32937 mean train loss:  3.61150014e-03, bound:  3.15348119e-01\n",
      "Epoch: 32938 mean train loss:  3.61141586e-03, bound:  3.15348119e-01\n",
      "Epoch: 32939 mean train loss:  3.61134741e-03, bound:  3.15348119e-01\n",
      "Epoch: 32940 mean train loss:  3.61126685e-03, bound:  3.15348119e-01\n",
      "Epoch: 32941 mean train loss:  3.61125125e-03, bound:  3.15348119e-01\n",
      "Epoch: 32942 mean train loss:  3.61119863e-03, bound:  3.15348119e-01\n",
      "Epoch: 32943 mean train loss:  3.61112063e-03, bound:  3.15348089e-01\n",
      "Epoch: 32944 mean train loss:  3.61108663e-03, bound:  3.15348089e-01\n",
      "Epoch: 32945 mean train loss:  3.61098628e-03, bound:  3.15348089e-01\n",
      "Epoch: 32946 mean train loss:  3.61098256e-03, bound:  3.15348089e-01\n",
      "Epoch: 32947 mean train loss:  3.61088687e-03, bound:  3.15348059e-01\n",
      "Epoch: 32948 mean train loss:  3.61086219e-03, bound:  3.15348059e-01\n",
      "Epoch: 32949 mean train loss:  3.61080002e-03, bound:  3.15348059e-01\n",
      "Epoch: 32950 mean train loss:  3.61073972e-03, bound:  3.15348029e-01\n",
      "Epoch: 32951 mean train loss:  3.61073366e-03, bound:  3.15348029e-01\n",
      "Epoch: 32952 mean train loss:  3.61063238e-03, bound:  3.15348029e-01\n",
      "Epoch: 32953 mean train loss:  3.61057697e-03, bound:  3.15348029e-01\n",
      "Epoch: 32954 mean train loss:  3.61053413e-03, bound:  3.15348029e-01\n",
      "Epoch: 32955 mean train loss:  3.61043192e-03, bound:  3.15347999e-01\n",
      "Epoch: 32956 mean train loss:  3.61041212e-03, bound:  3.15347999e-01\n",
      "Epoch: 32957 mean train loss:  3.61034111e-03, bound:  3.15347999e-01\n",
      "Epoch: 32958 mean train loss:  3.61033925e-03, bound:  3.15347999e-01\n",
      "Epoch: 32959 mean train loss:  3.61023447e-03, bound:  3.15347999e-01\n",
      "Epoch: 32960 mean train loss:  3.61016812e-03, bound:  3.15347999e-01\n",
      "Epoch: 32961 mean train loss:  3.61010432e-03, bound:  3.15347999e-01\n",
      "Epoch: 32962 mean train loss:  3.61005915e-03, bound:  3.15347970e-01\n",
      "Epoch: 32963 mean train loss:  3.61004146e-03, bound:  3.15347970e-01\n",
      "Epoch: 32964 mean train loss:  3.60991643e-03, bound:  3.15347970e-01\n",
      "Epoch: 32965 mean train loss:  3.60989687e-03, bound:  3.15347970e-01\n",
      "Epoch: 32966 mean train loss:  3.60981072e-03, bound:  3.15347940e-01\n",
      "Epoch: 32967 mean train loss:  3.60979000e-03, bound:  3.15347940e-01\n",
      "Epoch: 32968 mean train loss:  3.60972201e-03, bound:  3.15347910e-01\n",
      "Epoch: 32969 mean train loss:  3.60972504e-03, bound:  3.15347910e-01\n",
      "Epoch: 32970 mean train loss:  3.60962260e-03, bound:  3.15347910e-01\n",
      "Epoch: 32971 mean train loss:  3.60956509e-03, bound:  3.15347910e-01\n",
      "Epoch: 32972 mean train loss:  3.60950851e-03, bound:  3.15347910e-01\n",
      "Epoch: 32973 mean train loss:  3.60941468e-03, bound:  3.15347910e-01\n",
      "Epoch: 32974 mean train loss:  3.60933412e-03, bound:  3.15347910e-01\n",
      "Epoch: 32975 mean train loss:  3.60935344e-03, bound:  3.15347910e-01\n",
      "Epoch: 32976 mean train loss:  3.60924774e-03, bound:  3.15347910e-01\n",
      "Epoch: 32977 mean train loss:  3.60920257e-03, bound:  3.15347910e-01\n",
      "Epoch: 32978 mean train loss:  3.60918348e-03, bound:  3.15347850e-01\n",
      "Epoch: 32979 mean train loss:  3.60902911e-03, bound:  3.15347850e-01\n",
      "Epoch: 32980 mean train loss:  3.60906892e-03, bound:  3.15347850e-01\n",
      "Epoch: 32981 mean train loss:  3.60897626e-03, bound:  3.15347850e-01\n",
      "Epoch: 32982 mean train loss:  3.60889151e-03, bound:  3.15347850e-01\n",
      "Epoch: 32983 mean train loss:  3.60887451e-03, bound:  3.15347850e-01\n",
      "Epoch: 32984 mean train loss:  3.60883004e-03, bound:  3.15347821e-01\n",
      "Epoch: 32985 mean train loss:  3.60875856e-03, bound:  3.15347821e-01\n",
      "Epoch: 32986 mean train loss:  3.60869337e-03, bound:  3.15347821e-01\n",
      "Epoch: 32987 mean train loss:  3.60864168e-03, bound:  3.15347821e-01\n",
      "Epoch: 32988 mean train loss:  3.60856205e-03, bound:  3.15347821e-01\n",
      "Epoch: 32989 mean train loss:  3.60851362e-03, bound:  3.15347821e-01\n",
      "Epoch: 32990 mean train loss:  3.60844354e-03, bound:  3.15347821e-01\n",
      "Epoch: 32991 mean train loss:  3.60840699e-03, bound:  3.15347791e-01\n",
      "Epoch: 32992 mean train loss:  3.60835274e-03, bound:  3.15347791e-01\n",
      "Epoch: 32993 mean train loss:  3.60827241e-03, bound:  3.15347791e-01\n",
      "Epoch: 32994 mean train loss:  3.60824028e-03, bound:  3.15347791e-01\n",
      "Epoch: 32995 mean train loss:  3.60816624e-03, bound:  3.15347791e-01\n",
      "Epoch: 32996 mean train loss:  3.60812224e-03, bound:  3.15347791e-01\n",
      "Epoch: 32997 mean train loss:  3.60804936e-03, bound:  3.15347731e-01\n",
      "Epoch: 32998 mean train loss:  3.60800023e-03, bound:  3.15347731e-01\n",
      "Epoch: 32999 mean train loss:  3.60794249e-03, bound:  3.15347731e-01\n",
      "Epoch: 33000 mean train loss:  3.60787916e-03, bound:  3.15347731e-01\n",
      "Epoch: 33001 mean train loss:  3.60777345e-03, bound:  3.15347731e-01\n",
      "Epoch: 33002 mean train loss:  3.60774458e-03, bound:  3.15347701e-01\n",
      "Epoch: 33003 mean train loss:  3.60770128e-03, bound:  3.15347701e-01\n",
      "Epoch: 33004 mean train loss:  3.60769359e-03, bound:  3.15347701e-01\n",
      "Epoch: 33005 mean train loss:  3.60757904e-03, bound:  3.15347701e-01\n",
      "Epoch: 33006 mean train loss:  3.60752316e-03, bound:  3.15347701e-01\n",
      "Epoch: 33007 mean train loss:  3.60746612e-03, bound:  3.15347701e-01\n",
      "Epoch: 33008 mean train loss:  3.60741396e-03, bound:  3.15347701e-01\n",
      "Epoch: 33009 mean train loss:  3.60735692e-03, bound:  3.15347701e-01\n",
      "Epoch: 33010 mean train loss:  3.60728102e-03, bound:  3.15347701e-01\n",
      "Epoch: 33011 mean train loss:  3.60724423e-03, bound:  3.15347672e-01\n",
      "Epoch: 33012 mean train loss:  3.60716181e-03, bound:  3.15347672e-01\n",
      "Epoch: 33013 mean train loss:  3.60713922e-03, bound:  3.15347672e-01\n",
      "Epoch: 33014 mean train loss:  3.60710407e-03, bound:  3.15347612e-01\n",
      "Epoch: 33015 mean train loss:  3.60702607e-03, bound:  3.15347612e-01\n",
      "Epoch: 33016 mean train loss:  3.60697275e-03, bound:  3.15347612e-01\n",
      "Epoch: 33017 mean train loss:  3.60683724e-03, bound:  3.15347612e-01\n",
      "Epoch: 33018 mean train loss:  3.60683864e-03, bound:  3.15347612e-01\n",
      "Epoch: 33019 mean train loss:  3.60679673e-03, bound:  3.15347612e-01\n",
      "Epoch: 33020 mean train loss:  3.60672129e-03, bound:  3.15347612e-01\n",
      "Epoch: 33021 mean train loss:  3.60665191e-03, bound:  3.15347612e-01\n",
      "Epoch: 33022 mean train loss:  3.60662257e-03, bound:  3.15347582e-01\n",
      "Epoch: 33023 mean train loss:  3.60656413e-03, bound:  3.15347582e-01\n",
      "Epoch: 33024 mean train loss:  3.60650430e-03, bound:  3.15347582e-01\n",
      "Epoch: 33025 mean train loss:  3.60645843e-03, bound:  3.15347582e-01\n",
      "Epoch: 33026 mean train loss:  3.60636692e-03, bound:  3.15347582e-01\n",
      "Epoch: 33027 mean train loss:  3.60634015e-03, bound:  3.15347582e-01\n",
      "Epoch: 33028 mean train loss:  3.60627938e-03, bound:  3.15347582e-01\n",
      "Epoch: 33029 mean train loss:  3.60620534e-03, bound:  3.15347552e-01\n",
      "Epoch: 33030 mean train loss:  3.60614271e-03, bound:  3.15347552e-01\n",
      "Epoch: 33031 mean train loss:  3.60612595e-03, bound:  3.15347552e-01\n",
      "Epoch: 33032 mean train loss:  3.60603747e-03, bound:  3.15347552e-01\n",
      "Epoch: 33033 mean train loss:  3.60596459e-03, bound:  3.15347552e-01\n",
      "Epoch: 33034 mean train loss:  3.60590103e-03, bound:  3.15347522e-01\n",
      "Epoch: 33035 mean train loss:  3.60586192e-03, bound:  3.15347493e-01\n",
      "Epoch: 33036 mean train loss:  3.60577484e-03, bound:  3.15347493e-01\n",
      "Epoch: 33037 mean train loss:  3.60576692e-03, bound:  3.15347493e-01\n",
      "Epoch: 33038 mean train loss:  3.60567751e-03, bound:  3.15347493e-01\n",
      "Epoch: 33039 mean train loss:  3.60564189e-03, bound:  3.15347493e-01\n",
      "Epoch: 33040 mean train loss:  3.60554107e-03, bound:  3.15347463e-01\n",
      "Epoch: 33041 mean train loss:  3.60552059e-03, bound:  3.15347463e-01\n",
      "Epoch: 33042 mean train loss:  3.60539649e-03, bound:  3.15347463e-01\n",
      "Epoch: 33043 mean train loss:  3.60538880e-03, bound:  3.15347463e-01\n",
      "Epoch: 33044 mean train loss:  3.60531639e-03, bound:  3.15347463e-01\n",
      "Epoch: 33045 mean train loss:  3.60529264e-03, bound:  3.15347463e-01\n",
      "Epoch: 33046 mean train loss:  3.60521465e-03, bound:  3.15347433e-01\n",
      "Epoch: 33047 mean train loss:  3.60512664e-03, bound:  3.15347433e-01\n",
      "Epoch: 33048 mean train loss:  3.60510149e-03, bound:  3.15347433e-01\n",
      "Epoch: 33049 mean train loss:  3.60502745e-03, bound:  3.15347433e-01\n",
      "Epoch: 33050 mean train loss:  3.60500300e-03, bound:  3.15347403e-01\n",
      "Epoch: 33051 mean train loss:  3.60492035e-03, bound:  3.15347403e-01\n",
      "Epoch: 33052 mean train loss:  3.60491336e-03, bound:  3.15347403e-01\n",
      "Epoch: 33053 mean train loss:  3.60480906e-03, bound:  3.15347403e-01\n",
      "Epoch: 33054 mean train loss:  3.60478740e-03, bound:  3.15347403e-01\n",
      "Epoch: 33055 mean train loss:  3.60467541e-03, bound:  3.15347403e-01\n",
      "Epoch: 33056 mean train loss:  3.60464235e-03, bound:  3.15347403e-01\n",
      "Epoch: 33057 mean train loss:  3.60459439e-03, bound:  3.15347373e-01\n",
      "Epoch: 33058 mean train loss:  3.60446447e-03, bound:  3.15347373e-01\n",
      "Epoch: 33059 mean train loss:  3.60445096e-03, bound:  3.15347344e-01\n",
      "Epoch: 33060 mean train loss:  3.60439788e-03, bound:  3.15347344e-01\n",
      "Epoch: 33061 mean train loss:  3.60433618e-03, bound:  3.15347344e-01\n",
      "Epoch: 33062 mean train loss:  3.60426772e-03, bound:  3.15347344e-01\n",
      "Epoch: 33063 mean train loss:  3.60419671e-03, bound:  3.15347344e-01\n",
      "Epoch: 33064 mean train loss:  3.60417948e-03, bound:  3.15347344e-01\n",
      "Epoch: 33065 mean train loss:  3.60412220e-03, bound:  3.15347314e-01\n",
      "Epoch: 33066 mean train loss:  3.60402116e-03, bound:  3.15347314e-01\n",
      "Epoch: 33067 mean train loss:  3.60394199e-03, bound:  3.15347314e-01\n",
      "Epoch: 33068 mean train loss:  3.60391825e-03, bound:  3.15347314e-01\n",
      "Epoch: 33069 mean train loss:  3.60385887e-03, bound:  3.15347314e-01\n",
      "Epoch: 33070 mean train loss:  3.60381999e-03, bound:  3.15347284e-01\n",
      "Epoch: 33071 mean train loss:  3.60371871e-03, bound:  3.15347284e-01\n",
      "Epoch: 33072 mean train loss:  3.60374665e-03, bound:  3.15347284e-01\n",
      "Epoch: 33073 mean train loss:  3.60364001e-03, bound:  3.15347284e-01\n",
      "Epoch: 33074 mean train loss:  3.60360183e-03, bound:  3.15347284e-01\n",
      "Epoch: 33075 mean train loss:  3.60356248e-03, bound:  3.15347284e-01\n",
      "Epoch: 33076 mean train loss:  3.60344537e-03, bound:  3.15347284e-01\n",
      "Epoch: 33077 mean train loss:  3.60341556e-03, bound:  3.15347254e-01\n",
      "Epoch: 33078 mean train loss:  3.60332616e-03, bound:  3.15347254e-01\n",
      "Epoch: 33079 mean train loss:  3.60329868e-03, bound:  3.15347254e-01\n",
      "Epoch: 33080 mean train loss:  3.60319647e-03, bound:  3.15347254e-01\n",
      "Epoch: 33081 mean train loss:  3.60318134e-03, bound:  3.15347224e-01\n",
      "Epoch: 33082 mean train loss:  3.60306399e-03, bound:  3.15347224e-01\n",
      "Epoch: 33083 mean train loss:  3.60304862e-03, bound:  3.15347224e-01\n",
      "Epoch: 33084 mean train loss:  3.60300113e-03, bound:  3.15347195e-01\n",
      "Epoch: 33085 mean train loss:  3.60291172e-03, bound:  3.15347195e-01\n",
      "Epoch: 33086 mean train loss:  3.60288867e-03, bound:  3.15347195e-01\n",
      "Epoch: 33087 mean train loss:  3.60279321e-03, bound:  3.15347195e-01\n",
      "Epoch: 33088 mean train loss:  3.60275269e-03, bound:  3.15347195e-01\n",
      "Epoch: 33089 mean train loss:  3.60273104e-03, bound:  3.15347195e-01\n",
      "Epoch: 33090 mean train loss:  3.60264536e-03, bound:  3.15347165e-01\n",
      "Epoch: 33091 mean train loss:  3.60258669e-03, bound:  3.15347165e-01\n",
      "Epoch: 33092 mean train loss:  3.60250543e-03, bound:  3.15347165e-01\n",
      "Epoch: 33093 mean train loss:  3.60248191e-03, bound:  3.15347135e-01\n",
      "Epoch: 33094 mean train loss:  3.60238389e-03, bound:  3.15347135e-01\n",
      "Epoch: 33095 mean train loss:  3.60234245e-03, bound:  3.15347135e-01\n",
      "Epoch: 33096 mean train loss:  3.60231008e-03, bound:  3.15347135e-01\n",
      "Epoch: 33097 mean train loss:  3.60227912e-03, bound:  3.15347135e-01\n",
      "Epoch: 33098 mean train loss:  3.60209378e-03, bound:  3.15347135e-01\n",
      "Epoch: 33099 mean train loss:  3.60209774e-03, bound:  3.15347135e-01\n",
      "Epoch: 33100 mean train loss:  3.60201835e-03, bound:  3.15347105e-01\n",
      "Epoch: 33101 mean train loss:  3.60197504e-03, bound:  3.15347105e-01\n",
      "Epoch: 33102 mean train loss:  3.60191520e-03, bound:  3.15347105e-01\n",
      "Epoch: 33103 mean train loss:  3.60188470e-03, bound:  3.15347105e-01\n",
      "Epoch: 33104 mean train loss:  3.60178715e-03, bound:  3.15347105e-01\n",
      "Epoch: 33105 mean train loss:  3.60174617e-03, bound:  3.15347046e-01\n",
      "Epoch: 33106 mean train loss:  3.60171334e-03, bound:  3.15347046e-01\n",
      "Epoch: 33107 mean train loss:  3.60166468e-03, bound:  3.15347046e-01\n",
      "Epoch: 33108 mean train loss:  3.60158808e-03, bound:  3.15347046e-01\n",
      "Epoch: 33109 mean train loss:  3.60154640e-03, bound:  3.15347046e-01\n",
      "Epoch: 33110 mean train loss:  3.60145816e-03, bound:  3.15347046e-01\n",
      "Epoch: 33111 mean train loss:  3.60143674e-03, bound:  3.15347046e-01\n",
      "Epoch: 33112 mean train loss:  3.60129611e-03, bound:  3.15347016e-01\n",
      "Epoch: 33113 mean train loss:  3.60126304e-03, bound:  3.15347016e-01\n",
      "Epoch: 33114 mean train loss:  3.60121531e-03, bound:  3.15347016e-01\n",
      "Epoch: 33115 mean train loss:  3.60116037e-03, bound:  3.15347016e-01\n",
      "Epoch: 33116 mean train loss:  3.60106817e-03, bound:  3.15347016e-01\n",
      "Epoch: 33117 mean train loss:  3.60104977e-03, bound:  3.15347016e-01\n",
      "Epoch: 33118 mean train loss:  3.60099575e-03, bound:  3.15347016e-01\n",
      "Epoch: 33119 mean train loss:  3.60091589e-03, bound:  3.15346986e-01\n",
      "Epoch: 33120 mean train loss:  3.60085606e-03, bound:  3.15346986e-01\n",
      "Epoch: 33121 mean train loss:  3.60078784e-03, bound:  3.15346986e-01\n",
      "Epoch: 33122 mean train loss:  3.60073405e-03, bound:  3.15346926e-01\n",
      "Epoch: 33123 mean train loss:  3.60066444e-03, bound:  3.15346926e-01\n",
      "Epoch: 33124 mean train loss:  3.60062951e-03, bound:  3.15346926e-01\n",
      "Epoch: 33125 mean train loss:  3.60058085e-03, bound:  3.15346926e-01\n",
      "Epoch: 33126 mean train loss:  3.60050285e-03, bound:  3.15346926e-01\n",
      "Epoch: 33127 mean train loss:  3.60044139e-03, bound:  3.15346926e-01\n",
      "Epoch: 33128 mean train loss:  3.60039552e-03, bound:  3.15346926e-01\n",
      "Epoch: 33129 mean train loss:  3.60034220e-03, bound:  3.15346897e-01\n",
      "Epoch: 33130 mean train loss:  3.60026443e-03, bound:  3.15346897e-01\n",
      "Epoch: 33131 mean train loss:  3.60018015e-03, bound:  3.15346897e-01\n",
      "Epoch: 33132 mean train loss:  3.60012054e-03, bound:  3.15346897e-01\n",
      "Epoch: 33133 mean train loss:  3.60007910e-03, bound:  3.15346897e-01\n",
      "Epoch: 33134 mean train loss:  3.60005163e-03, bound:  3.15346897e-01\n",
      "Epoch: 33135 mean train loss:  3.59995547e-03, bound:  3.15346897e-01\n",
      "Epoch: 33136 mean train loss:  3.59987793e-03, bound:  3.15346897e-01\n",
      "Epoch: 33137 mean train loss:  3.59983067e-03, bound:  3.15346867e-01\n",
      "Epoch: 33138 mean train loss:  3.59978620e-03, bound:  3.15346867e-01\n",
      "Epoch: 33139 mean train loss:  3.59969679e-03, bound:  3.15346867e-01\n",
      "Epoch: 33140 mean train loss:  3.59967561e-03, bound:  3.15346867e-01\n",
      "Epoch: 33141 mean train loss:  3.59959202e-03, bound:  3.15346837e-01\n",
      "Epoch: 33142 mean train loss:  3.59955989e-03, bound:  3.15346807e-01\n",
      "Epoch: 33143 mean train loss:  3.59947630e-03, bound:  3.15346807e-01\n",
      "Epoch: 33144 mean train loss:  3.59939947e-03, bound:  3.15346807e-01\n",
      "Epoch: 33145 mean train loss:  3.59937432e-03, bound:  3.15346807e-01\n",
      "Epoch: 33146 mean train loss:  3.59929074e-03, bound:  3.15346777e-01\n",
      "Epoch: 33147 mean train loss:  3.59922322e-03, bound:  3.15346777e-01\n",
      "Epoch: 33148 mean train loss:  3.59919062e-03, bound:  3.15346777e-01\n",
      "Epoch: 33149 mean train loss:  3.59911728e-03, bound:  3.15346777e-01\n",
      "Epoch: 33150 mean train loss:  3.59905791e-03, bound:  3.15346777e-01\n",
      "Epoch: 33151 mean train loss:  3.59907025e-03, bound:  3.15346777e-01\n",
      "Epoch: 33152 mean train loss:  3.59895173e-03, bound:  3.15346777e-01\n",
      "Epoch: 33153 mean train loss:  3.59888584e-03, bound:  3.15346777e-01\n",
      "Epoch: 33154 mean train loss:  3.59882368e-03, bound:  3.15346748e-01\n",
      "Epoch: 33155 mean train loss:  3.59875732e-03, bound:  3.15346748e-01\n",
      "Epoch: 33156 mean train loss:  3.59864905e-03, bound:  3.15346748e-01\n",
      "Epoch: 33157 mean train loss:  3.59865371e-03, bound:  3.15346748e-01\n",
      "Epoch: 33158 mean train loss:  3.59857967e-03, bound:  3.15346748e-01\n",
      "Epoch: 33159 mean train loss:  3.59851005e-03, bound:  3.15346688e-01\n",
      "Epoch: 33160 mean train loss:  3.59846745e-03, bound:  3.15346688e-01\n",
      "Epoch: 33161 mean train loss:  3.59840901e-03, bound:  3.15346688e-01\n",
      "Epoch: 33162 mean train loss:  3.59834149e-03, bound:  3.15346688e-01\n",
      "Epoch: 33163 mean train loss:  3.59828025e-03, bound:  3.15346688e-01\n",
      "Epoch: 33164 mean train loss:  3.59821622e-03, bound:  3.15346688e-01\n",
      "Epoch: 33165 mean train loss:  3.59812356e-03, bound:  3.15346688e-01\n",
      "Epoch: 33166 mean train loss:  3.59811774e-03, bound:  3.15346658e-01\n",
      "Epoch: 33167 mean train loss:  3.59806791e-03, bound:  3.15346658e-01\n",
      "Epoch: 33168 mean train loss:  3.59799736e-03, bound:  3.15346658e-01\n",
      "Epoch: 33169 mean train loss:  3.59793263e-03, bound:  3.15346658e-01\n",
      "Epoch: 33170 mean train loss:  3.59787210e-03, bound:  3.15346658e-01\n",
      "Epoch: 33171 mean train loss:  3.59781762e-03, bound:  3.15346658e-01\n",
      "Epoch: 33172 mean train loss:  3.59772961e-03, bound:  3.15346599e-01\n",
      "Epoch: 33173 mean train loss:  3.59770074e-03, bound:  3.15346599e-01\n",
      "Epoch: 33174 mean train loss:  3.59762227e-03, bound:  3.15346599e-01\n",
      "Epoch: 33175 mean train loss:  3.59756546e-03, bound:  3.15346599e-01\n",
      "Epoch: 33176 mean train loss:  3.59753380e-03, bound:  3.15346599e-01\n",
      "Epoch: 33177 mean train loss:  3.59741645e-03, bound:  3.15346599e-01\n",
      "Epoch: 33178 mean train loss:  3.59738967e-03, bound:  3.15346569e-01\n",
      "Epoch: 33179 mean train loss:  3.59732984e-03, bound:  3.15346569e-01\n",
      "Epoch: 33180 mean train loss:  3.59725603e-03, bound:  3.15346569e-01\n",
      "Epoch: 33181 mean train loss:  3.59718944e-03, bound:  3.15346569e-01\n",
      "Epoch: 33182 mean train loss:  3.59710469e-03, bound:  3.15346569e-01\n",
      "Epoch: 33183 mean train loss:  3.59704113e-03, bound:  3.15346569e-01\n",
      "Epoch: 33184 mean train loss:  3.59702716e-03, bound:  3.15346569e-01\n",
      "Epoch: 33185 mean train loss:  3.59695894e-03, bound:  3.15346569e-01\n",
      "Epoch: 33186 mean train loss:  3.59689514e-03, bound:  3.15346569e-01\n",
      "Epoch: 33187 mean train loss:  3.59682320e-03, bound:  3.15346569e-01\n",
      "Epoch: 33188 mean train loss:  3.59683647e-03, bound:  3.15346539e-01\n",
      "Epoch: 33189 mean train loss:  3.59674869e-03, bound:  3.15346479e-01\n",
      "Epoch: 33190 mean train loss:  3.59668769e-03, bound:  3.15346479e-01\n",
      "Epoch: 33191 mean train loss:  3.59660271e-03, bound:  3.15346479e-01\n",
      "Epoch: 33192 mean train loss:  3.59658292e-03, bound:  3.15346479e-01\n",
      "Epoch: 33193 mean train loss:  3.59648117e-03, bound:  3.15346479e-01\n",
      "Epoch: 33194 mean train loss:  3.59646347e-03, bound:  3.15346479e-01\n",
      "Epoch: 33195 mean train loss:  3.59635986e-03, bound:  3.15346479e-01\n",
      "Epoch: 33196 mean train loss:  3.59630561e-03, bound:  3.15346479e-01\n",
      "Epoch: 33197 mean train loss:  3.59618827e-03, bound:  3.15346479e-01\n",
      "Epoch: 33198 mean train loss:  3.59618338e-03, bound:  3.15346479e-01\n",
      "Epoch: 33199 mean train loss:  3.59614659e-03, bound:  3.15346479e-01\n",
      "Epoch: 33200 mean train loss:  3.59605416e-03, bound:  3.15346450e-01\n",
      "Epoch: 33201 mean train loss:  3.59599432e-03, bound:  3.15346450e-01\n",
      "Epoch: 33202 mean train loss:  3.59592517e-03, bound:  3.15346450e-01\n",
      "Epoch: 33203 mean train loss:  3.59590468e-03, bound:  3.15346450e-01\n",
      "Epoch: 33204 mean train loss:  3.59580782e-03, bound:  3.15346450e-01\n",
      "Epoch: 33205 mean train loss:  3.59576289e-03, bound:  3.15346420e-01\n",
      "Epoch: 33206 mean train loss:  3.59567301e-03, bound:  3.15346420e-01\n",
      "Epoch: 33207 mean train loss:  3.59568023e-03, bound:  3.15346420e-01\n",
      "Epoch: 33208 mean train loss:  3.59559525e-03, bound:  3.15346420e-01\n",
      "Epoch: 33209 mean train loss:  3.59552167e-03, bound:  3.15346360e-01\n",
      "Epoch: 33210 mean train loss:  3.59545508e-03, bound:  3.15346360e-01\n",
      "Epoch: 33211 mean train loss:  3.59536079e-03, bound:  3.15346360e-01\n",
      "Epoch: 33212 mean train loss:  3.59533518e-03, bound:  3.15346360e-01\n",
      "Epoch: 33213 mean train loss:  3.59527138e-03, bound:  3.15346360e-01\n",
      "Epoch: 33214 mean train loss:  3.59521480e-03, bound:  3.15346360e-01\n",
      "Epoch: 33215 mean train loss:  3.59513517e-03, bound:  3.15346360e-01\n",
      "Epoch: 33216 mean train loss:  3.59509676e-03, bound:  3.15346360e-01\n",
      "Epoch: 33217 mean train loss:  3.59498383e-03, bound:  3.15346330e-01\n",
      "Epoch: 33218 mean train loss:  3.59497429e-03, bound:  3.15346330e-01\n",
      "Epoch: 33219 mean train loss:  3.59491096e-03, bound:  3.15346330e-01\n",
      "Epoch: 33220 mean train loss:  3.59483459e-03, bound:  3.15346330e-01\n",
      "Epoch: 33221 mean train loss:  3.59476614e-03, bound:  3.15346330e-01\n",
      "Epoch: 33222 mean train loss:  3.59470863e-03, bound:  3.15346330e-01\n",
      "Epoch: 33223 mean train loss:  3.59465997e-03, bound:  3.15346301e-01\n",
      "Epoch: 33224 mean train loss:  3.59460525e-03, bound:  3.15346301e-01\n",
      "Epoch: 33225 mean train loss:  3.59454053e-03, bound:  3.15346301e-01\n",
      "Epoch: 33226 mean train loss:  3.59451142e-03, bound:  3.15346301e-01\n",
      "Epoch: 33227 mean train loss:  3.59439850e-03, bound:  3.15346301e-01\n",
      "Epoch: 33228 mean train loss:  3.59429838e-03, bound:  3.15346301e-01\n",
      "Epoch: 33229 mean train loss:  3.59431724e-03, bound:  3.15346271e-01\n",
      "Epoch: 33230 mean train loss:  3.59420804e-03, bound:  3.15346271e-01\n",
      "Epoch: 33231 mean train loss:  3.59414704e-03, bound:  3.15346241e-01\n",
      "Epoch: 33232 mean train loss:  3.59414541e-03, bound:  3.15346241e-01\n",
      "Epoch: 33233 mean train loss:  3.59408883e-03, bound:  3.15346241e-01\n",
      "Epoch: 33234 mean train loss:  3.59398313e-03, bound:  3.15346211e-01\n",
      "Epoch: 33235 mean train loss:  3.59393936e-03, bound:  3.15346211e-01\n",
      "Epoch: 33236 mean train loss:  3.59391258e-03, bound:  3.15346211e-01\n",
      "Epoch: 33237 mean train loss:  3.59381898e-03, bound:  3.15346211e-01\n",
      "Epoch: 33238 mean train loss:  3.59379128e-03, bound:  3.15346211e-01\n",
      "Epoch: 33239 mean train loss:  3.59366112e-03, bound:  3.15346211e-01\n",
      "Epoch: 33240 mean train loss:  3.59363249e-03, bound:  3.15346152e-01\n",
      "Epoch: 33241 mean train loss:  3.59357428e-03, bound:  3.15346152e-01\n",
      "Epoch: 33242 mean train loss:  3.59348604e-03, bound:  3.15346152e-01\n",
      "Epoch: 33243 mean train loss:  3.59344319e-03, bound:  3.15346152e-01\n",
      "Epoch: 33244 mean train loss:  3.59332701e-03, bound:  3.15346152e-01\n",
      "Epoch: 33245 mean train loss:  3.59334168e-03, bound:  3.15346152e-01\n",
      "Epoch: 33246 mean train loss:  3.59321455e-03, bound:  3.15346152e-01\n",
      "Epoch: 33247 mean train loss:  3.59324296e-03, bound:  3.15346152e-01\n",
      "Epoch: 33248 mean train loss:  3.59311420e-03, bound:  3.15346152e-01\n",
      "Epoch: 33249 mean train loss:  3.59306205e-03, bound:  3.15346122e-01\n",
      "Epoch: 33250 mean train loss:  3.59300105e-03, bound:  3.15346122e-01\n",
      "Epoch: 33251 mean train loss:  3.59292515e-03, bound:  3.15346122e-01\n",
      "Epoch: 33252 mean train loss:  3.59290978e-03, bound:  3.15346092e-01\n",
      "Epoch: 33253 mean train loss:  3.59284878e-03, bound:  3.15346092e-01\n",
      "Epoch: 33254 mean train loss:  3.59272561e-03, bound:  3.15346092e-01\n",
      "Epoch: 33255 mean train loss:  3.59271769e-03, bound:  3.15346092e-01\n",
      "Epoch: 33256 mean train loss:  3.59267718e-03, bound:  3.15346092e-01\n",
      "Epoch: 33257 mean train loss:  3.59257846e-03, bound:  3.15346092e-01\n",
      "Epoch: 33258 mean train loss:  3.59255448e-03, bound:  3.15346092e-01\n",
      "Epoch: 33259 mean train loss:  3.59248207e-03, bound:  3.15346092e-01\n",
      "Epoch: 33260 mean train loss:  3.59237753e-03, bound:  3.15346032e-01\n",
      "Epoch: 33261 mean train loss:  3.59237776e-03, bound:  3.15346032e-01\n",
      "Epoch: 33262 mean train loss:  3.59227811e-03, bound:  3.15346032e-01\n",
      "Epoch: 33263 mean train loss:  3.59222339e-03, bound:  3.15346032e-01\n",
      "Epoch: 33264 mean train loss:  3.59215727e-03, bound:  3.15346032e-01\n",
      "Epoch: 33265 mean train loss:  3.59210745e-03, bound:  3.15346003e-01\n",
      "Epoch: 33266 mean train loss:  3.59201618e-03, bound:  3.15346032e-01\n",
      "Epoch: 33267 mean train loss:  3.59197636e-03, bound:  3.15346003e-01\n",
      "Epoch: 33268 mean train loss:  3.59192071e-03, bound:  3.15346003e-01\n",
      "Epoch: 33269 mean train loss:  3.59188928e-03, bound:  3.15345973e-01\n",
      "Epoch: 33270 mean train loss:  3.59173841e-03, bound:  3.15345973e-01\n",
      "Epoch: 33271 mean train loss:  3.59173282e-03, bound:  3.15345973e-01\n",
      "Epoch: 33272 mean train loss:  3.59171303e-03, bound:  3.15345973e-01\n",
      "Epoch: 33273 mean train loss:  3.59158753e-03, bound:  3.15345973e-01\n",
      "Epoch: 33274 mean train loss:  3.59154749e-03, bound:  3.15345973e-01\n",
      "Epoch: 33275 mean train loss:  3.59146460e-03, bound:  3.15345973e-01\n",
      "Epoch: 33276 mean train loss:  3.59140709e-03, bound:  3.15345973e-01\n",
      "Epoch: 33277 mean train loss:  3.59134329e-03, bound:  3.15345973e-01\n",
      "Epoch: 33278 mean train loss:  3.59130325e-03, bound:  3.15345913e-01\n",
      "Epoch: 33279 mean train loss:  3.59125296e-03, bound:  3.15345913e-01\n",
      "Epoch: 33280 mean train loss:  3.59115656e-03, bound:  3.15345913e-01\n",
      "Epoch: 33281 mean train loss:  3.59110604e-03, bound:  3.15345913e-01\n",
      "Epoch: 33282 mean train loss:  3.59104690e-03, bound:  3.15345913e-01\n",
      "Epoch: 33283 mean train loss:  3.59101174e-03, bound:  3.15345883e-01\n",
      "Epoch: 33284 mean train loss:  3.59091908e-03, bound:  3.15345883e-01\n",
      "Epoch: 33285 mean train loss:  3.59083829e-03, bound:  3.15345883e-01\n",
      "Epoch: 33286 mean train loss:  3.59080592e-03, bound:  3.15345883e-01\n",
      "Epoch: 33287 mean train loss:  3.59077053e-03, bound:  3.15345883e-01\n",
      "Epoch: 33288 mean train loss:  3.59069905e-03, bound:  3.15345883e-01\n",
      "Epoch: 33289 mean train loss:  3.59064294e-03, bound:  3.15345883e-01\n",
      "Epoch: 33290 mean train loss:  3.59057169e-03, bound:  3.15345883e-01\n",
      "Epoch: 33291 mean train loss:  3.59054073e-03, bound:  3.15345883e-01\n",
      "Epoch: 33292 mean train loss:  3.59038543e-03, bound:  3.15345854e-01\n",
      "Epoch: 33293 mean train loss:  3.59038496e-03, bound:  3.15345854e-01\n",
      "Epoch: 33294 mean train loss:  3.59030417e-03, bound:  3.15345794e-01\n",
      "Epoch: 33295 mean train loss:  3.59021546e-03, bound:  3.15345794e-01\n",
      "Epoch: 33296 mean train loss:  3.59019917e-03, bound:  3.15345794e-01\n",
      "Epoch: 33297 mean train loss:  3.59012745e-03, bound:  3.15345794e-01\n",
      "Epoch: 33298 mean train loss:  3.59003874e-03, bound:  3.15345794e-01\n",
      "Epoch: 33299 mean train loss:  3.58999474e-03, bound:  3.15345794e-01\n",
      "Epoch: 33300 mean train loss:  3.58995632e-03, bound:  3.15345764e-01\n",
      "Epoch: 33301 mean train loss:  3.58984387e-03, bound:  3.15345794e-01\n",
      "Epoch: 33302 mean train loss:  3.58979986e-03, bound:  3.15345764e-01\n",
      "Epoch: 33303 mean train loss:  3.58974887e-03, bound:  3.15345764e-01\n",
      "Epoch: 33304 mean train loss:  3.58970091e-03, bound:  3.15345764e-01\n",
      "Epoch: 33305 mean train loss:  3.58961965e-03, bound:  3.15345764e-01\n",
      "Epoch: 33306 mean train loss:  3.58953513e-03, bound:  3.15345764e-01\n",
      "Epoch: 33307 mean train loss:  3.58953536e-03, bound:  3.15345764e-01\n",
      "Epoch: 33308 mean train loss:  3.58946482e-03, bound:  3.15345764e-01\n",
      "Epoch: 33309 mean train loss:  3.58935958e-03, bound:  3.15345764e-01\n",
      "Epoch: 33310 mean train loss:  3.58931324e-03, bound:  3.15345705e-01\n",
      "Epoch: 33311 mean train loss:  3.58927064e-03, bound:  3.15345705e-01\n",
      "Epoch: 33312 mean train loss:  3.58918915e-03, bound:  3.15345675e-01\n",
      "Epoch: 33313 mean train loss:  3.58911627e-03, bound:  3.15345675e-01\n",
      "Epoch: 33314 mean train loss:  3.58908833e-03, bound:  3.15345675e-01\n",
      "Epoch: 33315 mean train loss:  3.58900591e-03, bound:  3.15345675e-01\n",
      "Epoch: 33316 mean train loss:  3.58893396e-03, bound:  3.15345675e-01\n",
      "Epoch: 33317 mean train loss:  3.58888204e-03, bound:  3.15345675e-01\n",
      "Epoch: 33318 mean train loss:  3.58883291e-03, bound:  3.15345675e-01\n",
      "Epoch: 33319 mean train loss:  3.58876656e-03, bound:  3.15345675e-01\n",
      "Epoch: 33320 mean train loss:  3.58873210e-03, bound:  3.15345645e-01\n",
      "Epoch: 33321 mean train loss:  3.58863827e-03, bound:  3.15345645e-01\n",
      "Epoch: 33322 mean train loss:  3.58859450e-03, bound:  3.15345645e-01\n",
      "Epoch: 33323 mean train loss:  3.58850695e-03, bound:  3.15345645e-01\n",
      "Epoch: 33324 mean train loss:  3.58847389e-03, bound:  3.15345645e-01\n",
      "Epoch: 33325 mean train loss:  3.58840730e-03, bound:  3.15345645e-01\n",
      "Epoch: 33326 mean train loss:  3.58833745e-03, bound:  3.15345585e-01\n",
      "Epoch: 33327 mean train loss:  3.58825852e-03, bound:  3.15345585e-01\n",
      "Epoch: 33328 mean train loss:  3.58817331e-03, bound:  3.15345585e-01\n",
      "Epoch: 33329 mean train loss:  3.58812138e-03, bound:  3.15345585e-01\n",
      "Epoch: 33330 mean train loss:  3.58812069e-03, bound:  3.15345585e-01\n",
      "Epoch: 33331 mean train loss:  3.58800776e-03, bound:  3.15345585e-01\n",
      "Epoch: 33332 mean train loss:  3.58795957e-03, bound:  3.15345585e-01\n",
      "Epoch: 33333 mean train loss:  3.58785782e-03, bound:  3.15345585e-01\n",
      "Epoch: 33334 mean train loss:  3.58785014e-03, bound:  3.15345556e-01\n",
      "Epoch: 33335 mean train loss:  3.58774303e-03, bound:  3.15345556e-01\n",
      "Epoch: 33336 mean train loss:  3.58770834e-03, bound:  3.15345556e-01\n",
      "Epoch: 33337 mean train loss:  3.58760334e-03, bound:  3.15345556e-01\n",
      "Epoch: 33338 mean train loss:  3.58756888e-03, bound:  3.15345526e-01\n",
      "Epoch: 33339 mean train loss:  3.58751998e-03, bound:  3.15345526e-01\n",
      "Epoch: 33340 mean train loss:  3.58743500e-03, bound:  3.15345526e-01\n",
      "Epoch: 33341 mean train loss:  3.58741242e-03, bound:  3.15345526e-01\n",
      "Epoch: 33342 mean train loss:  3.58735095e-03, bound:  3.15345526e-01\n",
      "Epoch: 33343 mean train loss:  3.58722522e-03, bound:  3.15345526e-01\n",
      "Epoch: 33344 mean train loss:  3.58721847e-03, bound:  3.15345466e-01\n",
      "Epoch: 33345 mean train loss:  3.58716142e-03, bound:  3.15345466e-01\n",
      "Epoch: 33346 mean train loss:  3.58705129e-03, bound:  3.15345466e-01\n",
      "Epoch: 33347 mean train loss:  3.58700380e-03, bound:  3.15345466e-01\n",
      "Epoch: 33348 mean train loss:  3.58693185e-03, bound:  3.15345466e-01\n",
      "Epoch: 33349 mean train loss:  3.58683639e-03, bound:  3.15345466e-01\n",
      "Epoch: 33350 mean train loss:  3.58686573e-03, bound:  3.15345466e-01\n",
      "Epoch: 33351 mean train loss:  3.58678191e-03, bound:  3.15345436e-01\n",
      "Epoch: 33352 mean train loss:  3.58669204e-03, bound:  3.15345436e-01\n",
      "Epoch: 33353 mean train loss:  3.58663802e-03, bound:  3.15345436e-01\n",
      "Epoch: 33354 mean train loss:  3.58658913e-03, bound:  3.15345436e-01\n",
      "Epoch: 33355 mean train loss:  3.58648924e-03, bound:  3.15345407e-01\n",
      "Epoch: 33356 mean train loss:  3.58645129e-03, bound:  3.15345407e-01\n",
      "Epoch: 33357 mean train loss:  3.58635350e-03, bound:  3.15345407e-01\n",
      "Epoch: 33358 mean train loss:  3.58628994e-03, bound:  3.15345407e-01\n",
      "Epoch: 33359 mean train loss:  3.58622335e-03, bound:  3.15345407e-01\n",
      "Epoch: 33360 mean train loss:  3.58616677e-03, bound:  3.15345347e-01\n",
      "Epoch: 33361 mean train loss:  3.58613767e-03, bound:  3.15345347e-01\n",
      "Epoch: 33362 mean train loss:  3.58607341e-03, bound:  3.15345347e-01\n",
      "Epoch: 33363 mean train loss:  3.58598167e-03, bound:  3.15345347e-01\n",
      "Epoch: 33364 mean train loss:  3.58591322e-03, bound:  3.15345347e-01\n",
      "Epoch: 33365 mean train loss:  3.58588505e-03, bound:  3.15345347e-01\n",
      "Epoch: 33366 mean train loss:  3.58584989e-03, bound:  3.15345347e-01\n",
      "Epoch: 33367 mean train loss:  3.58577934e-03, bound:  3.15345317e-01\n",
      "Epoch: 33368 mean train loss:  3.58568388e-03, bound:  3.15345317e-01\n",
      "Epoch: 33369 mean train loss:  3.58569971e-03, bound:  3.15345317e-01\n",
      "Epoch: 33370 mean train loss:  3.58554046e-03, bound:  3.15345317e-01\n",
      "Epoch: 33371 mean train loss:  3.58552299e-03, bound:  3.15345317e-01\n",
      "Epoch: 33372 mean train loss:  3.58545175e-03, bound:  3.15345287e-01\n",
      "Epoch: 33373 mean train loss:  3.58540961e-03, bound:  3.15345287e-01\n",
      "Epoch: 33374 mean train loss:  3.58535303e-03, bound:  3.15345258e-01\n",
      "Epoch: 33375 mean train loss:  3.58523335e-03, bound:  3.15345258e-01\n",
      "Epoch: 33376 mean train loss:  3.58520332e-03, bound:  3.15345258e-01\n",
      "Epoch: 33377 mean train loss:  3.58512206e-03, bound:  3.15345258e-01\n",
      "Epoch: 33378 mean train loss:  3.58505244e-03, bound:  3.15345258e-01\n",
      "Epoch: 33379 mean train loss:  3.58499563e-03, bound:  3.15345228e-01\n",
      "Epoch: 33380 mean train loss:  3.58491438e-03, bound:  3.15345228e-01\n",
      "Epoch: 33381 mean train loss:  3.58491438e-03, bound:  3.15345228e-01\n",
      "Epoch: 33382 mean train loss:  3.58477607e-03, bound:  3.15345228e-01\n",
      "Epoch: 33383 mean train loss:  3.58476141e-03, bound:  3.15345228e-01\n",
      "Epoch: 33384 mean train loss:  3.58466827e-03, bound:  3.15345228e-01\n",
      "Epoch: 33385 mean train loss:  3.58457607e-03, bound:  3.15345198e-01\n",
      "Epoch: 33386 mean train loss:  3.58454906e-03, bound:  3.15345198e-01\n",
      "Epoch: 33387 mean train loss:  3.58448061e-03, bound:  3.15345198e-01\n",
      "Epoch: 33388 mean train loss:  3.58440750e-03, bound:  3.15345198e-01\n",
      "Epoch: 33389 mean train loss:  3.58437491e-03, bound:  3.15345198e-01\n",
      "Epoch: 33390 mean train loss:  3.58432555e-03, bound:  3.15345138e-01\n",
      "Epoch: 33391 mean train loss:  3.58422543e-03, bound:  3.15345138e-01\n",
      "Epoch: 33392 mean train loss:  3.58418981e-03, bound:  3.15345138e-01\n",
      "Epoch: 33393 mean train loss:  3.58413649e-03, bound:  3.15345138e-01\n",
      "Epoch: 33394 mean train loss:  3.58403521e-03, bound:  3.15345138e-01\n",
      "Epoch: 33395 mean train loss:  3.58398352e-03, bound:  3.15345138e-01\n",
      "Epoch: 33396 mean train loss:  3.58393020e-03, bound:  3.15345109e-01\n",
      "Epoch: 33397 mean train loss:  3.58387199e-03, bound:  3.15345109e-01\n",
      "Epoch: 33398 mean train loss:  3.58377141e-03, bound:  3.15345109e-01\n",
      "Epoch: 33399 mean train loss:  3.58378165e-03, bound:  3.15345109e-01\n",
      "Epoch: 33400 mean train loss:  3.58367898e-03, bound:  3.15345109e-01\n",
      "Epoch: 33401 mean train loss:  3.58361332e-03, bound:  3.15345109e-01\n",
      "Epoch: 33402 mean train loss:  3.58351297e-03, bound:  3.15345079e-01\n",
      "Epoch: 33403 mean train loss:  3.58354324e-03, bound:  3.15345079e-01\n",
      "Epoch: 33404 mean train loss:  3.58339283e-03, bound:  3.15345079e-01\n",
      "Epoch: 33405 mean train loss:  3.58340889e-03, bound:  3.15345079e-01\n",
      "Epoch: 33406 mean train loss:  3.58331506e-03, bound:  3.15345079e-01\n",
      "Epoch: 33407 mean train loss:  3.58325266e-03, bound:  3.15345079e-01\n",
      "Epoch: 33408 mean train loss:  3.58317583e-03, bound:  3.15345079e-01\n",
      "Epoch: 33409 mean train loss:  3.58309853e-03, bound:  3.15345079e-01\n",
      "Epoch: 33410 mean train loss:  3.58305243e-03, bound:  3.15345079e-01\n",
      "Epoch: 33411 mean train loss:  3.58299725e-03, bound:  3.15345019e-01\n",
      "Epoch: 33412 mean train loss:  3.58291995e-03, bound:  3.15345019e-01\n",
      "Epoch: 33413 mean train loss:  3.58283659e-03, bound:  3.15344989e-01\n",
      "Epoch: 33414 mean train loss:  3.58278072e-03, bound:  3.15344989e-01\n",
      "Epoch: 33415 mean train loss:  3.58272367e-03, bound:  3.15344989e-01\n",
      "Epoch: 33416 mean train loss:  3.58267082e-03, bound:  3.15344989e-01\n",
      "Epoch: 33417 mean train loss:  3.58258956e-03, bound:  3.15344989e-01\n",
      "Epoch: 33418 mean train loss:  3.58253601e-03, bound:  3.15344989e-01\n",
      "Epoch: 33419 mean train loss:  3.58248316e-03, bound:  3.15344959e-01\n",
      "Epoch: 33420 mean train loss:  3.58240516e-03, bound:  3.15344959e-01\n",
      "Epoch: 33421 mean train loss:  3.58241261e-03, bound:  3.15344959e-01\n",
      "Epoch: 33422 mean train loss:  3.58230178e-03, bound:  3.15344959e-01\n",
      "Epoch: 33423 mean train loss:  3.58219212e-03, bound:  3.15344959e-01\n",
      "Epoch: 33424 mean train loss:  3.58219352e-03, bound:  3.15344959e-01\n",
      "Epoch: 33425 mean train loss:  3.58211016e-03, bound:  3.15344959e-01\n",
      "Epoch: 33426 mean train loss:  3.58204218e-03, bound:  3.15344959e-01\n",
      "Epoch: 33427 mean train loss:  3.58198048e-03, bound:  3.15344959e-01\n",
      "Epoch: 33428 mean train loss:  3.58189852e-03, bound:  3.15344900e-01\n",
      "Epoch: 33429 mean train loss:  3.58185777e-03, bound:  3.15344900e-01\n",
      "Epoch: 33430 mean train loss:  3.58173857e-03, bound:  3.15344900e-01\n",
      "Epoch: 33431 mean train loss:  3.58172180e-03, bound:  3.15344870e-01\n",
      "Epoch: 33432 mean train loss:  3.58163822e-03, bound:  3.15344870e-01\n",
      "Epoch: 33433 mean train loss:  3.58156557e-03, bound:  3.15344870e-01\n",
      "Epoch: 33434 mean train loss:  3.58149968e-03, bound:  3.15344870e-01\n",
      "Epoch: 33435 mean train loss:  3.58145032e-03, bound:  3.15344870e-01\n",
      "Epoch: 33436 mean train loss:  3.58135765e-03, bound:  3.15344870e-01\n",
      "Epoch: 33437 mean train loss:  3.58135067e-03, bound:  3.15344870e-01\n",
      "Epoch: 33438 mean train loss:  3.58124310e-03, bound:  3.15344870e-01\n",
      "Epoch: 33439 mean train loss:  3.58119095e-03, bound:  3.15344840e-01\n",
      "Epoch: 33440 mean train loss:  3.58110457e-03, bound:  3.15344840e-01\n",
      "Epoch: 33441 mean train loss:  3.58108478e-03, bound:  3.15344840e-01\n",
      "Epoch: 33442 mean train loss:  3.58103728e-03, bound:  3.15344781e-01\n",
      "Epoch: 33443 mean train loss:  3.58096696e-03, bound:  3.15344781e-01\n",
      "Epoch: 33444 mean train loss:  3.58089595e-03, bound:  3.15344781e-01\n",
      "Epoch: 33445 mean train loss:  3.58082284e-03, bound:  3.15344781e-01\n",
      "Epoch: 33446 mean train loss:  3.58076813e-03, bound:  3.15344781e-01\n",
      "Epoch: 33447 mean train loss:  3.58075695e-03, bound:  3.15344781e-01\n",
      "Epoch: 33448 mean train loss:  3.58059607e-03, bound:  3.15344781e-01\n",
      "Epoch: 33449 mean train loss:  3.58055346e-03, bound:  3.15344751e-01\n",
      "Epoch: 33450 mean train loss:  3.58048920e-03, bound:  3.15344751e-01\n",
      "Epoch: 33451 mean train loss:  3.58040538e-03, bound:  3.15344751e-01\n",
      "Epoch: 33452 mean train loss:  3.58040840e-03, bound:  3.15344751e-01\n",
      "Epoch: 33453 mean train loss:  3.58032249e-03, bound:  3.15344751e-01\n",
      "Epoch: 33454 mean train loss:  3.58024589e-03, bound:  3.15344721e-01\n",
      "Epoch: 33455 mean train loss:  3.58018209e-03, bound:  3.15344721e-01\n",
      "Epoch: 33456 mean train loss:  3.58011806e-03, bound:  3.15344691e-01\n",
      "Epoch: 33457 mean train loss:  3.58006964e-03, bound:  3.15344691e-01\n",
      "Epoch: 33458 mean train loss:  3.57998488e-03, bound:  3.15344691e-01\n",
      "Epoch: 33459 mean train loss:  3.57990712e-03, bound:  3.15344661e-01\n",
      "Epoch: 33460 mean train loss:  3.57984798e-03, bound:  3.15344661e-01\n",
      "Epoch: 33461 mean train loss:  3.57980304e-03, bound:  3.15344661e-01\n",
      "Epoch: 33462 mean train loss:  3.57973948e-03, bound:  3.15344661e-01\n",
      "Epoch: 33463 mean train loss:  3.57965869e-03, bound:  3.15344661e-01\n",
      "Epoch: 33464 mean train loss:  3.57958605e-03, bound:  3.15344661e-01\n",
      "Epoch: 33465 mean train loss:  3.57954530e-03, bound:  3.15344661e-01\n",
      "Epoch: 33466 mean train loss:  3.57944542e-03, bound:  3.15344661e-01\n",
      "Epoch: 33467 mean train loss:  3.57943168e-03, bound:  3.15344661e-01\n",
      "Epoch: 33468 mean train loss:  3.57933505e-03, bound:  3.15344632e-01\n",
      "Epoch: 33469 mean train loss:  3.57927871e-03, bound:  3.15344661e-01\n",
      "Epoch: 33470 mean train loss:  3.57922749e-03, bound:  3.15344632e-01\n",
      "Epoch: 33471 mean train loss:  3.57915950e-03, bound:  3.15344632e-01\n",
      "Epoch: 33472 mean train loss:  3.57908290e-03, bound:  3.15344632e-01\n",
      "Epoch: 33473 mean train loss:  3.57903983e-03, bound:  3.15344632e-01\n",
      "Epoch: 33474 mean train loss:  3.57898325e-03, bound:  3.15344632e-01\n",
      "Epoch: 33475 mean train loss:  3.57888150e-03, bound:  3.15344542e-01\n",
      "Epoch: 33476 mean train loss:  3.57885077e-03, bound:  3.15344542e-01\n",
      "Epoch: 33477 mean train loss:  3.57880304e-03, bound:  3.15344542e-01\n",
      "Epoch: 33478 mean train loss:  3.57871130e-03, bound:  3.15344542e-01\n",
      "Epoch: 33479 mean train loss:  3.57870129e-03, bound:  3.15344542e-01\n",
      "Epoch: 33480 mean train loss:  3.57855367e-03, bound:  3.15344542e-01\n",
      "Epoch: 33481 mean train loss:  3.57850990e-03, bound:  3.15344542e-01\n",
      "Epoch: 33482 mean train loss:  3.57839605e-03, bound:  3.15344542e-01\n",
      "Epoch: 33483 mean train loss:  3.57837300e-03, bound:  3.15344542e-01\n",
      "Epoch: 33484 mean train loss:  3.57831293e-03, bound:  3.15344542e-01\n",
      "Epoch: 33485 mean train loss:  3.57824564e-03, bound:  3.15344542e-01\n",
      "Epoch: 33486 mean train loss:  3.57818557e-03, bound:  3.15344512e-01\n",
      "Epoch: 33487 mean train loss:  3.57816252e-03, bound:  3.15344512e-01\n",
      "Epoch: 33488 mean train loss:  3.57803935e-03, bound:  3.15344512e-01\n",
      "Epoch: 33489 mean train loss:  3.57799721e-03, bound:  3.15344512e-01\n",
      "Epoch: 33490 mean train loss:  3.57794482e-03, bound:  3.15344512e-01\n",
      "Epoch: 33491 mean train loss:  3.57790221e-03, bound:  3.15344453e-01\n",
      "Epoch: 33492 mean train loss:  3.57781723e-03, bound:  3.15344453e-01\n",
      "Epoch: 33493 mean train loss:  3.57772294e-03, bound:  3.15344453e-01\n",
      "Epoch: 33494 mean train loss:  3.57762701e-03, bound:  3.15344453e-01\n",
      "Epoch: 33495 mean train loss:  3.57765239e-03, bound:  3.15344453e-01\n",
      "Epoch: 33496 mean train loss:  3.57752270e-03, bound:  3.15344423e-01\n",
      "Epoch: 33497 mean train loss:  3.57750384e-03, bound:  3.15344423e-01\n",
      "Epoch: 33498 mean train loss:  3.57741467e-03, bound:  3.15344423e-01\n",
      "Epoch: 33499 mean train loss:  3.57735460e-03, bound:  3.15344423e-01\n",
      "Epoch: 33500 mean train loss:  3.57725122e-03, bound:  3.15344423e-01\n",
      "Epoch: 33501 mean train loss:  3.57721164e-03, bound:  3.15344423e-01\n",
      "Epoch: 33502 mean train loss:  3.57714202e-03, bound:  3.15344393e-01\n",
      "Epoch: 33503 mean train loss:  3.57709825e-03, bound:  3.15344393e-01\n",
      "Epoch: 33504 mean train loss:  3.57699161e-03, bound:  3.15344393e-01\n",
      "Epoch: 33505 mean train loss:  3.57695762e-03, bound:  3.15344393e-01\n",
      "Epoch: 33506 mean train loss:  3.57692735e-03, bound:  3.15344393e-01\n",
      "Epoch: 33507 mean train loss:  3.57684982e-03, bound:  3.15344334e-01\n",
      "Epoch: 33508 mean train loss:  3.57674458e-03, bound:  3.15344334e-01\n",
      "Epoch: 33509 mean train loss:  3.57669056e-03, bound:  3.15344334e-01\n",
      "Epoch: 33510 mean train loss:  3.57661443e-03, bound:  3.15344334e-01\n",
      "Epoch: 33511 mean train loss:  3.57656111e-03, bound:  3.15344334e-01\n",
      "Epoch: 33512 mean train loss:  3.57651943e-03, bound:  3.15344304e-01\n",
      "Epoch: 33513 mean train loss:  3.57645540e-03, bound:  3.15344304e-01\n",
      "Epoch: 33514 mean train loss:  3.57635994e-03, bound:  3.15344304e-01\n",
      "Epoch: 33515 mean train loss:  3.57632036e-03, bound:  3.15344304e-01\n",
      "Epoch: 33516 mean train loss:  3.57622257e-03, bound:  3.15344304e-01\n",
      "Epoch: 33517 mean train loss:  3.57621582e-03, bound:  3.15344304e-01\n",
      "Epoch: 33518 mean train loss:  3.57612246e-03, bound:  3.15344274e-01\n",
      "Epoch: 33519 mean train loss:  3.57603095e-03, bound:  3.15344274e-01\n",
      "Epoch: 33520 mean train loss:  3.57599114e-03, bound:  3.15344244e-01\n",
      "Epoch: 33521 mean train loss:  3.57598090e-03, bound:  3.15344244e-01\n",
      "Epoch: 33522 mean train loss:  3.57584911e-03, bound:  3.15344244e-01\n",
      "Epoch: 33523 mean train loss:  3.57580837e-03, bound:  3.15344244e-01\n",
      "Epoch: 33524 mean train loss:  3.57570895e-03, bound:  3.15344244e-01\n",
      "Epoch: 33525 mean train loss:  3.57565586e-03, bound:  3.15344244e-01\n",
      "Epoch: 33526 mean train loss:  3.57557391e-03, bound:  3.15344214e-01\n",
      "Epoch: 33527 mean train loss:  3.57552082e-03, bound:  3.15344214e-01\n",
      "Epoch: 33528 mean train loss:  3.57545330e-03, bound:  3.15344214e-01\n",
      "Epoch: 33529 mean train loss:  3.57536552e-03, bound:  3.15344214e-01\n",
      "Epoch: 33530 mean train loss:  3.57533386e-03, bound:  3.15344214e-01\n",
      "Epoch: 33531 mean train loss:  3.57528124e-03, bound:  3.15344185e-01\n",
      "Epoch: 33532 mean train loss:  3.57520836e-03, bound:  3.15344185e-01\n",
      "Epoch: 33533 mean train loss:  3.57513945e-03, bound:  3.15344185e-01\n",
      "Epoch: 33534 mean train loss:  3.57509777e-03, bound:  3.15344155e-01\n",
      "Epoch: 33535 mean train loss:  3.57502745e-03, bound:  3.15344155e-01\n",
      "Epoch: 33536 mean train loss:  3.57492780e-03, bound:  3.15344155e-01\n",
      "Epoch: 33537 mean train loss:  3.57486517e-03, bound:  3.15344155e-01\n",
      "Epoch: 33538 mean train loss:  3.57484003e-03, bound:  3.15344155e-01\n",
      "Epoch: 33539 mean train loss:  3.57476249e-03, bound:  3.15344155e-01\n",
      "Epoch: 33540 mean train loss:  3.57467774e-03, bound:  3.15344125e-01\n",
      "Epoch: 33541 mean train loss:  3.57461954e-03, bound:  3.15344095e-01\n",
      "Epoch: 33542 mean train loss:  3.57456412e-03, bound:  3.15344095e-01\n",
      "Epoch: 33543 mean train loss:  3.57447099e-03, bound:  3.15344095e-01\n",
      "Epoch: 33544 mean train loss:  3.57439392e-03, bound:  3.15344095e-01\n",
      "Epoch: 33545 mean train loss:  3.57437739e-03, bound:  3.15344095e-01\n",
      "Epoch: 33546 mean train loss:  3.57422512e-03, bound:  3.15344095e-01\n",
      "Epoch: 33547 mean train loss:  3.57424770e-03, bound:  3.15344065e-01\n",
      "Epoch: 33548 mean train loss:  3.57415015e-03, bound:  3.15344065e-01\n",
      "Epoch: 33549 mean train loss:  3.57411429e-03, bound:  3.15344065e-01\n",
      "Epoch: 33550 mean train loss:  3.57399858e-03, bound:  3.15344036e-01\n",
      "Epoch: 33551 mean train loss:  3.57398205e-03, bound:  3.15344036e-01\n",
      "Epoch: 33552 mean train loss:  3.57386447e-03, bound:  3.15344036e-01\n",
      "Epoch: 33553 mean train loss:  3.57383420e-03, bound:  3.15344036e-01\n",
      "Epoch: 33554 mean train loss:  3.57376412e-03, bound:  3.15344036e-01\n",
      "Epoch: 33555 mean train loss:  3.57370637e-03, bound:  3.15344036e-01\n",
      "Epoch: 33556 mean train loss:  3.57363676e-03, bound:  3.15344006e-01\n",
      "Epoch: 33557 mean train loss:  3.57358390e-03, bound:  3.15344006e-01\n",
      "Epoch: 33558 mean train loss:  3.57351336e-03, bound:  3.15344006e-01\n",
      "Epoch: 33559 mean train loss:  3.57346376e-03, bound:  3.15343976e-01\n",
      "Epoch: 33560 mean train loss:  3.57339554e-03, bound:  3.15343976e-01\n",
      "Epoch: 33561 mean train loss:  3.57333501e-03, bound:  3.15343976e-01\n",
      "Epoch: 33562 mean train loss:  3.57326190e-03, bound:  3.15343976e-01\n",
      "Epoch: 33563 mean train loss:  3.57322698e-03, bound:  3.15343946e-01\n",
      "Epoch: 33564 mean train loss:  3.57310893e-03, bound:  3.15343946e-01\n",
      "Epoch: 33565 mean train loss:  3.57307307e-03, bound:  3.15343946e-01\n",
      "Epoch: 33566 mean train loss:  3.57299973e-03, bound:  3.15343946e-01\n",
      "Epoch: 33567 mean train loss:  3.57294246e-03, bound:  3.15343946e-01\n",
      "Epoch: 33568 mean train loss:  3.57284746e-03, bound:  3.15343946e-01\n",
      "Epoch: 33569 mean train loss:  3.57276388e-03, bound:  3.15343946e-01\n",
      "Epoch: 33570 mean train loss:  3.57269985e-03, bound:  3.15343946e-01\n",
      "Epoch: 33571 mean train loss:  3.57267330e-03, bound:  3.15343946e-01\n",
      "Epoch: 33572 mean train loss:  3.57260718e-03, bound:  3.15343887e-01\n",
      "Epoch: 33573 mean train loss:  3.57250380e-03, bound:  3.15343857e-01\n",
      "Epoch: 33574 mean train loss:  3.57244420e-03, bound:  3.15343857e-01\n",
      "Epoch: 33575 mean train loss:  3.57242906e-03, bound:  3.15343857e-01\n",
      "Epoch: 33576 mean train loss:  3.57234827e-03, bound:  3.15343857e-01\n",
      "Epoch: 33577 mean train loss:  3.57228890e-03, bound:  3.15343857e-01\n",
      "Epoch: 33578 mean train loss:  3.57221393e-03, bound:  3.15343857e-01\n",
      "Epoch: 33579 mean train loss:  3.57211870e-03, bound:  3.15343857e-01\n",
      "Epoch: 33580 mean train loss:  3.57203581e-03, bound:  3.15343857e-01\n",
      "Epoch: 33581 mean train loss:  3.57200415e-03, bound:  3.15343857e-01\n",
      "Epoch: 33582 mean train loss:  3.57196899e-03, bound:  3.15343857e-01\n",
      "Epoch: 33583 mean train loss:  3.57188052e-03, bound:  3.15343857e-01\n",
      "Epoch: 33584 mean train loss:  3.57180275e-03, bound:  3.15343827e-01\n",
      "Epoch: 33585 mean train loss:  3.57169495e-03, bound:  3.15343797e-01\n",
      "Epoch: 33586 mean train loss:  3.57169169e-03, bound:  3.15343797e-01\n",
      "Epoch: 33587 mean train loss:  3.57161695e-03, bound:  3.15343797e-01\n",
      "Epoch: 33588 mean train loss:  3.57156759e-03, bound:  3.15343767e-01\n",
      "Epoch: 33589 mean train loss:  3.57147958e-03, bound:  3.15343767e-01\n",
      "Epoch: 33590 mean train loss:  3.57144792e-03, bound:  3.15343767e-01\n",
      "Epoch: 33591 mean train loss:  3.57135921e-03, bound:  3.15343738e-01\n",
      "Epoch: 33592 mean train loss:  3.57129565e-03, bound:  3.15343738e-01\n",
      "Epoch: 33593 mean train loss:  3.57121462e-03, bound:  3.15343738e-01\n",
      "Epoch: 33594 mean train loss:  3.57116642e-03, bound:  3.15343738e-01\n",
      "Epoch: 33595 mean train loss:  3.57113895e-03, bound:  3.15343738e-01\n",
      "Epoch: 33596 mean train loss:  3.57100065e-03, bound:  3.15343738e-01\n",
      "Epoch: 33597 mean train loss:  3.57097178e-03, bound:  3.15343738e-01\n",
      "Epoch: 33598 mean train loss:  3.57090170e-03, bound:  3.15343738e-01\n",
      "Epoch: 33599 mean train loss:  3.57082696e-03, bound:  3.15343738e-01\n",
      "Epoch: 33600 mean train loss:  3.57076875e-03, bound:  3.15343708e-01\n",
      "Epoch: 33601 mean train loss:  3.57072521e-03, bound:  3.15343678e-01\n",
      "Epoch: 33602 mean train loss:  3.57062928e-03, bound:  3.15343678e-01\n",
      "Epoch: 33603 mean train loss:  3.57055967e-03, bound:  3.15343648e-01\n",
      "Epoch: 33604 mean train loss:  3.57050030e-03, bound:  3.15343648e-01\n",
      "Epoch: 33605 mean train loss:  3.57041159e-03, bound:  3.15343648e-01\n",
      "Epoch: 33606 mean train loss:  3.57036223e-03, bound:  3.15343648e-01\n",
      "Epoch: 33607 mean train loss:  3.57027864e-03, bound:  3.15343648e-01\n",
      "Epoch: 33608 mean train loss:  3.57021322e-03, bound:  3.15343648e-01\n",
      "Epoch: 33609 mean train loss:  3.57018481e-03, bound:  3.15343648e-01\n",
      "Epoch: 33610 mean train loss:  3.57009261e-03, bound:  3.15343648e-01\n",
      "Epoch: 33611 mean train loss:  3.57008073e-03, bound:  3.15343648e-01\n",
      "Epoch: 33612 mean train loss:  3.56998038e-03, bound:  3.15343618e-01\n",
      "Epoch: 33613 mean train loss:  3.56990402e-03, bound:  3.15343618e-01\n",
      "Epoch: 33614 mean train loss:  3.56984185e-03, bound:  3.15343618e-01\n",
      "Epoch: 33615 mean train loss:  3.56976734e-03, bound:  3.15343589e-01\n",
      "Epoch: 33616 mean train loss:  3.56971612e-03, bound:  3.15343589e-01\n",
      "Epoch: 33617 mean train loss:  3.56964278e-03, bound:  3.15343589e-01\n",
      "Epoch: 33618 mean train loss:  3.56957316e-03, bound:  3.15343559e-01\n",
      "Epoch: 33619 mean train loss:  3.56952078e-03, bound:  3.15343559e-01\n",
      "Epoch: 33620 mean train loss:  3.56943789e-03, bound:  3.15343559e-01\n",
      "Epoch: 33621 mean train loss:  3.56937270e-03, bound:  3.15343529e-01\n",
      "Epoch: 33622 mean train loss:  3.56929516e-03, bound:  3.15343529e-01\n",
      "Epoch: 33623 mean train loss:  3.56921135e-03, bound:  3.15343529e-01\n",
      "Epoch: 33624 mean train loss:  3.56913568e-03, bound:  3.15343529e-01\n",
      "Epoch: 33625 mean train loss:  3.56911845e-03, bound:  3.15343529e-01\n",
      "Epoch: 33626 mean train loss:  3.56903253e-03, bound:  3.15343529e-01\n",
      "Epoch: 33627 mean train loss:  3.56901111e-03, bound:  3.15343499e-01\n",
      "Epoch: 33628 mean train loss:  3.56886536e-03, bound:  3.15343499e-01\n",
      "Epoch: 33629 mean train loss:  3.56887281e-03, bound:  3.15343499e-01\n",
      "Epoch: 33630 mean train loss:  3.56875244e-03, bound:  3.15343499e-01\n",
      "Epoch: 33631 mean train loss:  3.56872147e-03, bound:  3.15343499e-01\n",
      "Epoch: 33632 mean train loss:  3.56861181e-03, bound:  3.15343440e-01\n",
      "Epoch: 33633 mean train loss:  3.56858969e-03, bound:  3.15343440e-01\n",
      "Epoch: 33634 mean train loss:  3.56845837e-03, bound:  3.15343440e-01\n",
      "Epoch: 33635 mean train loss:  3.56848235e-03, bound:  3.15343440e-01\n",
      "Epoch: 33636 mean train loss:  3.56840715e-03, bound:  3.15343440e-01\n",
      "Epoch: 33637 mean train loss:  3.56830307e-03, bound:  3.15343440e-01\n",
      "Epoch: 33638 mean train loss:  3.56823020e-03, bound:  3.15343440e-01\n",
      "Epoch: 33639 mean train loss:  3.56820365e-03, bound:  3.15343410e-01\n",
      "Epoch: 33640 mean train loss:  3.56808957e-03, bound:  3.15343410e-01\n",
      "Epoch: 33641 mean train loss:  3.56802391e-03, bound:  3.15343410e-01\n",
      "Epoch: 33642 mean train loss:  3.56794568e-03, bound:  3.15343410e-01\n",
      "Epoch: 33643 mean train loss:  3.56788514e-03, bound:  3.15343410e-01\n",
      "Epoch: 33644 mean train loss:  3.56784160e-03, bound:  3.15343380e-01\n",
      "Epoch: 33645 mean train loss:  3.56772612e-03, bound:  3.15343380e-01\n",
      "Epoch: 33646 mean train loss:  3.56769352e-03, bound:  3.15343380e-01\n",
      "Epoch: 33647 mean train loss:  3.56764393e-03, bound:  3.15343320e-01\n",
      "Epoch: 33648 mean train loss:  3.56758526e-03, bound:  3.15343320e-01\n",
      "Epoch: 33649 mean train loss:  3.56756174e-03, bound:  3.15343320e-01\n",
      "Epoch: 33650 mean train loss:  3.56745603e-03, bound:  3.15343320e-01\n",
      "Epoch: 33651 mean train loss:  3.56740458e-03, bound:  3.15343320e-01\n",
      "Epoch: 33652 mean train loss:  3.56728747e-03, bound:  3.15343320e-01\n",
      "Epoch: 33653 mean train loss:  3.56725045e-03, bound:  3.15343320e-01\n",
      "Epoch: 33654 mean train loss:  3.56712472e-03, bound:  3.15343291e-01\n",
      "Epoch: 33655 mean train loss:  3.56706581e-03, bound:  3.15343291e-01\n",
      "Epoch: 33656 mean train loss:  3.56706302e-03, bound:  3.15343291e-01\n",
      "Epoch: 33657 mean train loss:  3.56699573e-03, bound:  3.15343291e-01\n",
      "Epoch: 33658 mean train loss:  3.56690888e-03, bound:  3.15343291e-01\n",
      "Epoch: 33659 mean train loss:  3.56689468e-03, bound:  3.15343291e-01\n",
      "Epoch: 33660 mean train loss:  3.56680481e-03, bound:  3.15343291e-01\n",
      "Epoch: 33661 mean train loss:  3.56672122e-03, bound:  3.15343261e-01\n",
      "Epoch: 33662 mean train loss:  3.56667582e-03, bound:  3.15343261e-01\n",
      "Epoch: 33663 mean train loss:  3.56656010e-03, bound:  3.15343231e-01\n",
      "Epoch: 33664 mean train loss:  3.56656732e-03, bound:  3.15343231e-01\n",
      "Epoch: 33665 mean train loss:  3.56646022e-03, bound:  3.15343231e-01\n",
      "Epoch: 33666 mean train loss:  3.56637826e-03, bound:  3.15343231e-01\n",
      "Epoch: 33667 mean train loss:  3.56628094e-03, bound:  3.15343231e-01\n",
      "Epoch: 33668 mean train loss:  3.56627721e-03, bound:  3.15343231e-01\n",
      "Epoch: 33669 mean train loss:  3.56618827e-03, bound:  3.15343201e-01\n",
      "Epoch: 33670 mean train loss:  3.56610911e-03, bound:  3.15343201e-01\n",
      "Epoch: 33671 mean train loss:  3.56607907e-03, bound:  3.15343171e-01\n",
      "Epoch: 33672 mean train loss:  3.56597197e-03, bound:  3.15343171e-01\n",
      "Epoch: 33673 mean train loss:  3.56592378e-03, bound:  3.15343171e-01\n",
      "Epoch: 33674 mean train loss:  3.56582389e-03, bound:  3.15343171e-01\n",
      "Epoch: 33675 mean train loss:  3.56578804e-03, bound:  3.15343171e-01\n",
      "Epoch: 33676 mean train loss:  3.56569770e-03, bound:  3.15343171e-01\n",
      "Epoch: 33677 mean train loss:  3.56564415e-03, bound:  3.15343142e-01\n",
      "Epoch: 33678 mean train loss:  3.56557383e-03, bound:  3.15343142e-01\n",
      "Epoch: 33679 mean train loss:  3.56551656e-03, bound:  3.15343142e-01\n",
      "Epoch: 33680 mean train loss:  3.56543786e-03, bound:  3.15343142e-01\n",
      "Epoch: 33681 mean train loss:  3.56537849e-03, bound:  3.15343142e-01\n",
      "Epoch: 33682 mean train loss:  3.56531260e-03, bound:  3.15343112e-01\n",
      "Epoch: 33683 mean train loss:  3.56526533e-03, bound:  3.15343082e-01\n",
      "Epoch: 33684 mean train loss:  3.56515357e-03, bound:  3.15343082e-01\n",
      "Epoch: 33685 mean train loss:  3.56513844e-03, bound:  3.15343082e-01\n",
      "Epoch: 33686 mean train loss:  3.56507208e-03, bound:  3.15343082e-01\n",
      "Epoch: 33687 mean train loss:  3.56497662e-03, bound:  3.15343052e-01\n",
      "Epoch: 33688 mean train loss:  3.56489513e-03, bound:  3.15343052e-01\n",
      "Epoch: 33689 mean train loss:  3.56485113e-03, bound:  3.15343052e-01\n",
      "Epoch: 33690 mean train loss:  3.56476824e-03, bound:  3.15343052e-01\n",
      "Epoch: 33691 mean train loss:  3.56472447e-03, bound:  3.15343052e-01\n",
      "Epoch: 33692 mean train loss:  3.56462644e-03, bound:  3.15343052e-01\n",
      "Epoch: 33693 mean train loss:  3.56455566e-03, bound:  3.15343052e-01\n",
      "Epoch: 33694 mean train loss:  3.56452726e-03, bound:  3.15343052e-01\n",
      "Epoch: 33695 mean train loss:  3.56447999e-03, bound:  3.15343022e-01\n",
      "Epoch: 33696 mean train loss:  3.56437685e-03, bound:  3.15343022e-01\n",
      "Epoch: 33697 mean train loss:  3.56431678e-03, bound:  3.15342993e-01\n",
      "Epoch: 33698 mean train loss:  3.56422411e-03, bound:  3.15342993e-01\n",
      "Epoch: 33699 mean train loss:  3.56416684e-03, bound:  3.15342993e-01\n",
      "Epoch: 33700 mean train loss:  3.56408418e-03, bound:  3.15342963e-01\n",
      "Epoch: 33701 mean train loss:  3.56402947e-03, bound:  3.15342963e-01\n",
      "Epoch: 33702 mean train loss:  3.56398849e-03, bound:  3.15342963e-01\n",
      "Epoch: 33703 mean train loss:  3.56387626e-03, bound:  3.15342963e-01\n",
      "Epoch: 33704 mean train loss:  3.56381270e-03, bound:  3.15342963e-01\n",
      "Epoch: 33705 mean train loss:  3.56377591e-03, bound:  3.15342933e-01\n",
      "Epoch: 33706 mean train loss:  3.56368627e-03, bound:  3.15342963e-01\n",
      "Epoch: 33707 mean train loss:  3.56359011e-03, bound:  3.15342933e-01\n",
      "Epoch: 33708 mean train loss:  3.56358336e-03, bound:  3.15342933e-01\n",
      "Epoch: 33709 mean train loss:  3.56348138e-03, bound:  3.15342933e-01\n",
      "Epoch: 33710 mean train loss:  3.56342364e-03, bound:  3.15342933e-01\n",
      "Epoch: 33711 mean train loss:  3.56337684e-03, bound:  3.15342873e-01\n",
      "Epoch: 33712 mean train loss:  3.56329791e-03, bound:  3.15342873e-01\n",
      "Epoch: 33713 mean train loss:  3.56324203e-03, bound:  3.15342873e-01\n",
      "Epoch: 33714 mean train loss:  3.56317265e-03, bound:  3.15342873e-01\n",
      "Epoch: 33715 mean train loss:  3.56312306e-03, bound:  3.15342873e-01\n",
      "Epoch: 33716 mean train loss:  3.56299826e-03, bound:  3.15342844e-01\n",
      "Epoch: 33717 mean train loss:  3.56298918e-03, bound:  3.15342844e-01\n",
      "Epoch: 33718 mean train loss:  3.56282759e-03, bound:  3.15342844e-01\n",
      "Epoch: 33719 mean train loss:  3.56282713e-03, bound:  3.15342844e-01\n",
      "Epoch: 33720 mean train loss:  3.56275029e-03, bound:  3.15342844e-01\n",
      "Epoch: 33721 mean train loss:  3.56271048e-03, bound:  3.15342844e-01\n",
      "Epoch: 33722 mean train loss:  3.56260617e-03, bound:  3.15342844e-01\n",
      "Epoch: 33723 mean train loss:  3.56258475e-03, bound:  3.15342844e-01\n",
      "Epoch: 33724 mean train loss:  3.56245670e-03, bound:  3.15342844e-01\n",
      "Epoch: 33725 mean train loss:  3.56244948e-03, bound:  3.15342814e-01\n",
      "Epoch: 33726 mean train loss:  3.56234075e-03, bound:  3.15342814e-01\n",
      "Epoch: 33727 mean train loss:  3.56228929e-03, bound:  3.15342754e-01\n",
      "Epoch: 33728 mean train loss:  3.56219127e-03, bound:  3.15342754e-01\n",
      "Epoch: 33729 mean train loss:  3.56215099e-03, bound:  3.15342754e-01\n",
      "Epoch: 33730 mean train loss:  3.56206158e-03, bound:  3.15342754e-01\n",
      "Epoch: 33731 mean train loss:  3.56203550e-03, bound:  3.15342754e-01\n",
      "Epoch: 33732 mean train loss:  3.56197031e-03, bound:  3.15342724e-01\n",
      "Epoch: 33733 mean train loss:  3.56191536e-03, bound:  3.15342724e-01\n",
      "Epoch: 33734 mean train loss:  3.56183923e-03, bound:  3.15342724e-01\n",
      "Epoch: 33735 mean train loss:  3.56175890e-03, bound:  3.15342724e-01\n",
      "Epoch: 33736 mean train loss:  3.56168975e-03, bound:  3.15342724e-01\n",
      "Epoch: 33737 mean train loss:  3.56161408e-03, bound:  3.15342724e-01\n",
      "Epoch: 33738 mean train loss:  3.56152700e-03, bound:  3.15342724e-01\n",
      "Epoch: 33739 mean train loss:  3.56152258e-03, bound:  3.15342724e-01\n",
      "Epoch: 33740 mean train loss:  3.56142689e-03, bound:  3.15342695e-01\n",
      "Epoch: 33741 mean train loss:  3.56134656e-03, bound:  3.15342695e-01\n",
      "Epoch: 33742 mean train loss:  3.56124085e-03, bound:  3.15342665e-01\n",
      "Epoch: 33743 mean train loss:  3.56119615e-03, bound:  3.15342665e-01\n",
      "Epoch: 33744 mean train loss:  3.56110209e-03, bound:  3.15342665e-01\n",
      "Epoch: 33745 mean train loss:  3.56108765e-03, bound:  3.15342635e-01\n",
      "Epoch: 33746 mean train loss:  3.56099941e-03, bound:  3.15342635e-01\n",
      "Epoch: 33747 mean train loss:  3.56096285e-03, bound:  3.15342635e-01\n",
      "Epoch: 33748 mean train loss:  3.56087205e-03, bound:  3.15342635e-01\n",
      "Epoch: 33749 mean train loss:  3.56081920e-03, bound:  3.15342635e-01\n",
      "Epoch: 33750 mean train loss:  3.56075028e-03, bound:  3.15342635e-01\n",
      "Epoch: 33751 mean train loss:  3.56065156e-03, bound:  3.15342605e-01\n",
      "Epoch: 33752 mean train loss:  3.56057240e-03, bound:  3.15342605e-01\n",
      "Epoch: 33753 mean train loss:  3.56057147e-03, bound:  3.15342605e-01\n",
      "Epoch: 33754 mean train loss:  3.56044457e-03, bound:  3.15342605e-01\n",
      "Epoch: 33755 mean train loss:  3.56038706e-03, bound:  3.15342605e-01\n",
      "Epoch: 33756 mean train loss:  3.56029347e-03, bound:  3.15342575e-01\n",
      "Epoch: 33757 mean train loss:  3.56025784e-03, bound:  3.15342575e-01\n",
      "Epoch: 33758 mean train loss:  3.56020429e-03, bound:  3.15342546e-01\n",
      "Epoch: 33759 mean train loss:  3.56013095e-03, bound:  3.15342546e-01\n",
      "Epoch: 33760 mean train loss:  3.56003013e-03, bound:  3.15342546e-01\n",
      "Epoch: 33761 mean train loss:  3.56000685e-03, bound:  3.15342516e-01\n",
      "Epoch: 33762 mean train loss:  3.55990697e-03, bound:  3.15342516e-01\n",
      "Epoch: 33763 mean train loss:  3.55984340e-03, bound:  3.15342516e-01\n",
      "Epoch: 33764 mean train loss:  3.55975376e-03, bound:  3.15342516e-01\n",
      "Epoch: 33765 mean train loss:  3.55974259e-03, bound:  3.15342516e-01\n",
      "Epoch: 33766 mean train loss:  3.55963316e-03, bound:  3.15342486e-01\n",
      "Epoch: 33767 mean train loss:  3.55958170e-03, bound:  3.15342486e-01\n",
      "Epoch: 33768 mean train loss:  3.55951046e-03, bound:  3.15342486e-01\n",
      "Epoch: 33769 mean train loss:  3.55947041e-03, bound:  3.15342486e-01\n",
      "Epoch: 33770 mean train loss:  3.55935888e-03, bound:  3.15342486e-01\n",
      "Epoch: 33771 mean train loss:  3.55930720e-03, bound:  3.15342486e-01\n",
      "Epoch: 33772 mean train loss:  3.55922082e-03, bound:  3.15342456e-01\n",
      "Epoch: 33773 mean train loss:  3.55917076e-03, bound:  3.15342456e-01\n",
      "Epoch: 33774 mean train loss:  3.55912605e-03, bound:  3.15342426e-01\n",
      "Epoch: 33775 mean train loss:  3.55901592e-03, bound:  3.15342426e-01\n",
      "Epoch: 33776 mean train loss:  3.55899846e-03, bound:  3.15342426e-01\n",
      "Epoch: 33777 mean train loss:  3.55891651e-03, bound:  3.15342426e-01\n",
      "Epoch: 33778 mean train loss:  3.55880079e-03, bound:  3.15342426e-01\n",
      "Epoch: 33779 mean train loss:  3.55873210e-03, bound:  3.15342396e-01\n",
      "Epoch: 33780 mean train loss:  3.55869462e-03, bound:  3.15342396e-01\n",
      "Epoch: 33781 mean train loss:  3.55865736e-03, bound:  3.15342396e-01\n",
      "Epoch: 33782 mean train loss:  3.55855236e-03, bound:  3.15342367e-01\n",
      "Epoch: 33783 mean train loss:  3.55852395e-03, bound:  3.15342367e-01\n",
      "Epoch: 33784 mean train loss:  3.55842174e-03, bound:  3.15342367e-01\n",
      "Epoch: 33785 mean train loss:  3.55835236e-03, bound:  3.15342367e-01\n",
      "Epoch: 33786 mean train loss:  3.55824875e-03, bound:  3.15342367e-01\n",
      "Epoch: 33787 mean train loss:  3.55823757e-03, bound:  3.15342367e-01\n",
      "Epoch: 33788 mean train loss:  3.55814025e-03, bound:  3.15342307e-01\n",
      "Epoch: 33789 mean train loss:  3.55806062e-03, bound:  3.15342307e-01\n",
      "Epoch: 33790 mean train loss:  3.55801755e-03, bound:  3.15342307e-01\n",
      "Epoch: 33791 mean train loss:  3.55797377e-03, bound:  3.15342307e-01\n",
      "Epoch: 33792 mean train loss:  3.55790136e-03, bound:  3.15342307e-01\n",
      "Epoch: 33793 mean train loss:  3.55780334e-03, bound:  3.15342307e-01\n",
      "Epoch: 33794 mean train loss:  3.55772045e-03, bound:  3.15342307e-01\n",
      "Epoch: 33795 mean train loss:  3.55766900e-03, bound:  3.15342277e-01\n",
      "Epoch: 33796 mean train loss:  3.55758960e-03, bound:  3.15342277e-01\n",
      "Epoch: 33797 mean train loss:  3.55750672e-03, bound:  3.15342277e-01\n",
      "Epoch: 33798 mean train loss:  3.55747412e-03, bound:  3.15342277e-01\n",
      "Epoch: 33799 mean train loss:  3.55737587e-03, bound:  3.15342277e-01\n",
      "Epoch: 33800 mean train loss:  3.55728460e-03, bound:  3.15342247e-01\n",
      "Epoch: 33801 mean train loss:  3.55722755e-03, bound:  3.15342247e-01\n",
      "Epoch: 33802 mean train loss:  3.55716865e-03, bound:  3.15342247e-01\n",
      "Epoch: 33803 mean train loss:  3.55711509e-03, bound:  3.15342247e-01\n",
      "Epoch: 33804 mean train loss:  3.55705805e-03, bound:  3.15342188e-01\n",
      "Epoch: 33805 mean train loss:  3.55699030e-03, bound:  3.15342188e-01\n",
      "Epoch: 33806 mean train loss:  3.55689367e-03, bound:  3.15342188e-01\n",
      "Epoch: 33807 mean train loss:  3.55687900e-03, bound:  3.15342188e-01\n",
      "Epoch: 33808 mean train loss:  3.55673768e-03, bound:  3.15342188e-01\n",
      "Epoch: 33809 mean train loss:  3.55672906e-03, bound:  3.15342188e-01\n",
      "Epoch: 33810 mean train loss:  3.55662219e-03, bound:  3.15342158e-01\n",
      "Epoch: 33811 mean train loss:  3.55657656e-03, bound:  3.15342158e-01\n",
      "Epoch: 33812 mean train loss:  3.55650089e-03, bound:  3.15342158e-01\n",
      "Epoch: 33813 mean train loss:  3.55636491e-03, bound:  3.15342158e-01\n",
      "Epoch: 33814 mean train loss:  3.55637819e-03, bound:  3.15342128e-01\n",
      "Epoch: 33815 mean train loss:  3.55634908e-03, bound:  3.15342128e-01\n",
      "Epoch: 33816 mean train loss:  3.55619518e-03, bound:  3.15342128e-01\n",
      "Epoch: 33817 mean train loss:  3.55617399e-03, bound:  3.15342128e-01\n",
      "Epoch: 33818 mean train loss:  3.55610461e-03, bound:  3.15342128e-01\n",
      "Epoch: 33819 mean train loss:  3.55602102e-03, bound:  3.15342128e-01\n",
      "Epoch: 33820 mean train loss:  3.55599704e-03, bound:  3.15342128e-01\n",
      "Epoch: 33821 mean train loss:  3.55588784e-03, bound:  3.15342069e-01\n",
      "Epoch: 33822 mean train loss:  3.55584710e-03, bound:  3.15342069e-01\n",
      "Epoch: 33823 mean train loss:  3.55579564e-03, bound:  3.15342069e-01\n",
      "Epoch: 33824 mean train loss:  3.55568738e-03, bound:  3.15342069e-01\n",
      "Epoch: 33825 mean train loss:  3.55560402e-03, bound:  3.15342069e-01\n",
      "Epoch: 33826 mean train loss:  3.55555397e-03, bound:  3.15342069e-01\n",
      "Epoch: 33827 mean train loss:  3.55544570e-03, bound:  3.15342039e-01\n",
      "Epoch: 33828 mean train loss:  3.55541054e-03, bound:  3.15342039e-01\n",
      "Epoch: 33829 mean train loss:  3.55535420e-03, bound:  3.15342039e-01\n",
      "Epoch: 33830 mean train loss:  3.55529925e-03, bound:  3.15342039e-01\n",
      "Epoch: 33831 mean train loss:  3.55520472e-03, bound:  3.15342039e-01\n",
      "Epoch: 33832 mean train loss:  3.55512300e-03, bound:  3.15342039e-01\n",
      "Epoch: 33833 mean train loss:  3.55512323e-03, bound:  3.15342039e-01\n",
      "Epoch: 33834 mean train loss:  3.55494930e-03, bound:  3.15342009e-01\n",
      "Epoch: 33835 mean train loss:  3.55494441e-03, bound:  3.15342009e-01\n",
      "Epoch: 33836 mean train loss:  3.55486316e-03, bound:  3.15342009e-01\n",
      "Epoch: 33837 mean train loss:  3.55476886e-03, bound:  3.15342009e-01\n",
      "Epoch: 33838 mean train loss:  3.55470437e-03, bound:  3.15341979e-01\n",
      "Epoch: 33839 mean train loss:  3.55464290e-03, bound:  3.15341949e-01\n",
      "Epoch: 33840 mean train loss:  3.55456676e-03, bound:  3.15341949e-01\n",
      "Epoch: 33841 mean train loss:  3.55451042e-03, bound:  3.15341949e-01\n",
      "Epoch: 33842 mean train loss:  3.55440518e-03, bound:  3.15341949e-01\n",
      "Epoch: 33843 mean train loss:  3.55437445e-03, bound:  3.15341920e-01\n",
      "Epoch: 33844 mean train loss:  3.55430902e-03, bound:  3.15341920e-01\n",
      "Epoch: 33845 mean train loss:  3.55422590e-03, bound:  3.15341920e-01\n",
      "Epoch: 33846 mean train loss:  3.55414464e-03, bound:  3.15341920e-01\n",
      "Epoch: 33847 mean train loss:  3.55409575e-03, bound:  3.15341920e-01\n",
      "Epoch: 33848 mean train loss:  3.55399982e-03, bound:  3.15341920e-01\n",
      "Epoch: 33849 mean train loss:  3.55394813e-03, bound:  3.15341860e-01\n",
      "Epoch: 33850 mean train loss:  3.55387386e-03, bound:  3.15341860e-01\n",
      "Epoch: 33851 mean train loss:  3.55386501e-03, bound:  3.15341860e-01\n",
      "Epoch: 33852 mean train loss:  3.55374953e-03, bound:  3.15341860e-01\n",
      "Epoch: 33853 mean train loss:  3.55368992e-03, bound:  3.15341860e-01\n",
      "Epoch: 33854 mean train loss:  3.55358748e-03, bound:  3.15341860e-01\n",
      "Epoch: 33855 mean train loss:  3.55357677e-03, bound:  3.15341830e-01\n",
      "Epoch: 33856 mean train loss:  3.55345989e-03, bound:  3.15341830e-01\n",
      "Epoch: 33857 mean train loss:  3.55339912e-03, bound:  3.15341830e-01\n",
      "Epoch: 33858 mean train loss:  3.55332461e-03, bound:  3.15341800e-01\n",
      "Epoch: 33859 mean train loss:  3.55324359e-03, bound:  3.15341800e-01\n",
      "Epoch: 33860 mean train loss:  3.55318049e-03, bound:  3.15341800e-01\n",
      "Epoch: 33861 mean train loss:  3.55314580e-03, bound:  3.15341800e-01\n",
      "Epoch: 33862 mean train loss:  3.55307991e-03, bound:  3.15341800e-01\n",
      "Epoch: 33863 mean train loss:  3.55303986e-03, bound:  3.15341800e-01\n",
      "Epoch: 33864 mean train loss:  3.55294859e-03, bound:  3.15341800e-01\n",
      "Epoch: 33865 mean train loss:  3.55285313e-03, bound:  3.15341741e-01\n",
      "Epoch: 33866 mean train loss:  3.55278747e-03, bound:  3.15341741e-01\n",
      "Epoch: 33867 mean train loss:  3.55270738e-03, bound:  3.15341741e-01\n",
      "Epoch: 33868 mean train loss:  3.55267082e-03, bound:  3.15341741e-01\n",
      "Epoch: 33869 mean train loss:  3.55261657e-03, bound:  3.15341741e-01\n",
      "Epoch: 33870 mean train loss:  3.55253718e-03, bound:  3.15341711e-01\n",
      "Epoch: 33871 mean train loss:  3.55250668e-03, bound:  3.15341711e-01\n",
      "Epoch: 33872 mean train loss:  3.55235161e-03, bound:  3.15341711e-01\n",
      "Epoch: 33873 mean train loss:  3.55233578e-03, bound:  3.15341711e-01\n",
      "Epoch: 33874 mean train loss:  3.55225848e-03, bound:  3.15341711e-01\n",
      "Epoch: 33875 mean train loss:  3.55220120e-03, bound:  3.15341681e-01\n",
      "Epoch: 33876 mean train loss:  3.55211273e-03, bound:  3.15341681e-01\n",
      "Epoch: 33877 mean train loss:  3.55202029e-03, bound:  3.15341681e-01\n",
      "Epoch: 33878 mean train loss:  3.55196279e-03, bound:  3.15341681e-01\n",
      "Epoch: 33879 mean train loss:  3.55193485e-03, bound:  3.15341681e-01\n",
      "Epoch: 33880 mean train loss:  3.55182309e-03, bound:  3.15341622e-01\n",
      "Epoch: 33881 mean train loss:  3.55178793e-03, bound:  3.15341622e-01\n",
      "Epoch: 33882 mean train loss:  3.55168572e-03, bound:  3.15341622e-01\n",
      "Epoch: 33883 mean train loss:  3.55162378e-03, bound:  3.15341622e-01\n",
      "Epoch: 33884 mean train loss:  3.55154788e-03, bound:  3.15341622e-01\n",
      "Epoch: 33885 mean train loss:  3.55150853e-03, bound:  3.15341622e-01\n",
      "Epoch: 33886 mean train loss:  3.55141913e-03, bound:  3.15341622e-01\n",
      "Epoch: 33887 mean train loss:  3.55135719e-03, bound:  3.15341622e-01\n",
      "Epoch: 33888 mean train loss:  3.55124823e-03, bound:  3.15341592e-01\n",
      "Epoch: 33889 mean train loss:  3.55121703e-03, bound:  3.15341592e-01\n",
      "Epoch: 33890 mean train loss:  3.55116418e-03, bound:  3.15341592e-01\n",
      "Epoch: 33891 mean train loss:  3.55108082e-03, bound:  3.15341592e-01\n",
      "Epoch: 33892 mean train loss:  3.55099770e-03, bound:  3.15341592e-01\n",
      "Epoch: 33893 mean train loss:  3.55092855e-03, bound:  3.15341592e-01\n",
      "Epoch: 33894 mean train loss:  3.55089875e-03, bound:  3.15341562e-01\n",
      "Epoch: 33895 mean train loss:  3.55079141e-03, bound:  3.15341562e-01\n",
      "Epoch: 33896 mean train loss:  3.55070480e-03, bound:  3.15341502e-01\n",
      "Epoch: 33897 mean train loss:  3.55065893e-03, bound:  3.15341502e-01\n",
      "Epoch: 33898 mean train loss:  3.55060236e-03, bound:  3.15341502e-01\n",
      "Epoch: 33899 mean train loss:  3.55050433e-03, bound:  3.15341502e-01\n",
      "Epoch: 33900 mean train loss:  3.55046452e-03, bound:  3.15341502e-01\n",
      "Epoch: 33901 mean train loss:  3.55038443e-03, bound:  3.15341502e-01\n",
      "Epoch: 33902 mean train loss:  3.55026894e-03, bound:  3.15341502e-01\n",
      "Epoch: 33903 mean train loss:  3.55022703e-03, bound:  3.15341502e-01\n",
      "Epoch: 33904 mean train loss:  3.55016883e-03, bound:  3.15341502e-01\n",
      "Epoch: 33905 mean train loss:  3.55010689e-03, bound:  3.15341473e-01\n",
      "Epoch: 33906 mean train loss:  3.54997092e-03, bound:  3.15341473e-01\n",
      "Epoch: 33907 mean train loss:  3.54999327e-03, bound:  3.15341473e-01\n",
      "Epoch: 33908 mean train loss:  3.54989595e-03, bound:  3.15341473e-01\n",
      "Epoch: 33909 mean train loss:  3.54986195e-03, bound:  3.15341473e-01\n",
      "Epoch: 33910 mean train loss:  3.54979606e-03, bound:  3.15341413e-01\n",
      "Epoch: 33911 mean train loss:  3.54969921e-03, bound:  3.15341383e-01\n",
      "Epoch: 33912 mean train loss:  3.54957068e-03, bound:  3.15341383e-01\n",
      "Epoch: 33913 mean train loss:  3.54958163e-03, bound:  3.15341383e-01\n",
      "Epoch: 33914 mean train loss:  3.54942889e-03, bound:  3.15341383e-01\n",
      "Epoch: 33915 mean train loss:  3.54941376e-03, bound:  3.15341383e-01\n",
      "Epoch: 33916 mean train loss:  3.54934856e-03, bound:  3.15341383e-01\n",
      "Epoch: 33917 mean train loss:  3.54921562e-03, bound:  3.15341383e-01\n",
      "Epoch: 33918 mean train loss:  3.54915205e-03, bound:  3.15341383e-01\n",
      "Epoch: 33919 mean train loss:  3.54915019e-03, bound:  3.15341383e-01\n",
      "Epoch: 33920 mean train loss:  3.54909524e-03, bound:  3.15341353e-01\n",
      "Epoch: 33921 mean train loss:  3.54900211e-03, bound:  3.15341353e-01\n",
      "Epoch: 33922 mean train loss:  3.54895857e-03, bound:  3.15341353e-01\n",
      "Epoch: 33923 mean train loss:  3.54884053e-03, bound:  3.15341353e-01\n",
      "Epoch: 33924 mean train loss:  3.54876835e-03, bound:  3.15341353e-01\n",
      "Epoch: 33925 mean train loss:  3.54869757e-03, bound:  3.15341353e-01\n",
      "Epoch: 33926 mean train loss:  3.54863820e-03, bound:  3.15341294e-01\n",
      "Epoch: 33927 mean train loss:  3.54858395e-03, bound:  3.15341294e-01\n",
      "Epoch: 33928 mean train loss:  3.54853249e-03, bound:  3.15341294e-01\n",
      "Epoch: 33929 mean train loss:  3.54843680e-03, bound:  3.15341294e-01\n",
      "Epoch: 33930 mean train loss:  3.54838138e-03, bound:  3.15341294e-01\n",
      "Epoch: 33931 mean train loss:  3.54833691e-03, bound:  3.15341294e-01\n",
      "Epoch: 33932 mean train loss:  3.54822073e-03, bound:  3.15341264e-01\n",
      "Epoch: 33933 mean train loss:  3.54811782e-03, bound:  3.15341264e-01\n",
      "Epoch: 33934 mean train loss:  3.54809058e-03, bound:  3.15341264e-01\n",
      "Epoch: 33935 mean train loss:  3.54801677e-03, bound:  3.15341234e-01\n",
      "Epoch: 33936 mean train loss:  3.54797021e-03, bound:  3.15341234e-01\n",
      "Epoch: 33937 mean train loss:  3.54786939e-03, bound:  3.15341234e-01\n",
      "Epoch: 33938 mean train loss:  3.54782748e-03, bound:  3.15341234e-01\n",
      "Epoch: 33939 mean train loss:  3.54774320e-03, bound:  3.15341234e-01\n",
      "Epoch: 33940 mean train loss:  3.54766473e-03, bound:  3.15341234e-01\n",
      "Epoch: 33941 mean train loss:  3.54759139e-03, bound:  3.15341234e-01\n",
      "Epoch: 33942 mean train loss:  3.54753574e-03, bound:  3.15341175e-01\n",
      "Epoch: 33943 mean train loss:  3.54741816e-03, bound:  3.15341175e-01\n",
      "Epoch: 33944 mean train loss:  3.54737951e-03, bound:  3.15341175e-01\n",
      "Epoch: 33945 mean train loss:  3.54736578e-03, bound:  3.15341175e-01\n",
      "Epoch: 33946 mean train loss:  3.54734622e-03, bound:  3.15341175e-01\n",
      "Epoch: 33947 mean train loss:  3.54722841e-03, bound:  3.15341145e-01\n",
      "Epoch: 33948 mean train loss:  3.54718883e-03, bound:  3.15341145e-01\n",
      "Epoch: 33949 mean train loss:  3.54716694e-03, bound:  3.15341145e-01\n",
      "Epoch: 33950 mean train loss:  3.54703795e-03, bound:  3.15341145e-01\n",
      "Epoch: 33951 mean train loss:  3.54692712e-03, bound:  3.15341115e-01\n",
      "Epoch: 33952 mean train loss:  3.54685332e-03, bound:  3.15341115e-01\n",
      "Epoch: 33953 mean train loss:  3.54675320e-03, bound:  3.15341115e-01\n",
      "Epoch: 33954 mean train loss:  3.54674319e-03, bound:  3.15341115e-01\n",
      "Epoch: 33955 mean train loss:  3.54667287e-03, bound:  3.15341115e-01\n",
      "Epoch: 33956 mean train loss:  3.54654645e-03, bound:  3.15341115e-01\n",
      "Epoch: 33957 mean train loss:  3.54651990e-03, bound:  3.15341115e-01\n",
      "Epoch: 33958 mean train loss:  3.54643865e-03, bound:  3.15341115e-01\n",
      "Epoch: 33959 mean train loss:  3.54637462e-03, bound:  3.15341055e-01\n",
      "Epoch: 33960 mean train loss:  3.54629382e-03, bound:  3.15341055e-01\n",
      "Epoch: 33961 mean train loss:  3.54619650e-03, bound:  3.15341055e-01\n",
      "Epoch: 33962 mean train loss:  3.54618835e-03, bound:  3.15341055e-01\n",
      "Epoch: 33963 mean train loss:  3.54609406e-03, bound:  3.15341026e-01\n",
      "Epoch: 33964 mean train loss:  3.54601885e-03, bound:  3.15341026e-01\n",
      "Epoch: 33965 mean train loss:  3.54592851e-03, bound:  3.15341026e-01\n",
      "Epoch: 33966 mean train loss:  3.54588917e-03, bound:  3.15341026e-01\n",
      "Epoch: 33967 mean train loss:  3.54578206e-03, bound:  3.15341026e-01\n",
      "Epoch: 33968 mean train loss:  3.54573037e-03, bound:  3.15341026e-01\n",
      "Epoch: 33969 mean train loss:  3.54568451e-03, bound:  3.15341026e-01\n",
      "Epoch: 33970 mean train loss:  3.54556111e-03, bound:  3.15341026e-01\n",
      "Epoch: 33971 mean train loss:  3.54555575e-03, bound:  3.15340966e-01\n",
      "Epoch: 33972 mean train loss:  3.54545540e-03, bound:  3.15340966e-01\n",
      "Epoch: 33973 mean train loss:  3.54542118e-03, bound:  3.15340966e-01\n",
      "Epoch: 33974 mean train loss:  3.54533968e-03, bound:  3.15340936e-01\n",
      "Epoch: 33975 mean train loss:  3.54529521e-03, bound:  3.15340936e-01\n",
      "Epoch: 33976 mean train loss:  3.54519533e-03, bound:  3.15340936e-01\n",
      "Epoch: 33977 mean train loss:  3.54512013e-03, bound:  3.15340936e-01\n",
      "Epoch: 33978 mean train loss:  3.54505703e-03, bound:  3.15340936e-01\n",
      "Epoch: 33979 mean train loss:  3.54499556e-03, bound:  3.15340906e-01\n",
      "Epoch: 33980 mean train loss:  3.54489475e-03, bound:  3.15340906e-01\n",
      "Epoch: 33981 mean train loss:  3.54486308e-03, bound:  3.15340906e-01\n",
      "Epoch: 33982 mean train loss:  3.54482234e-03, bound:  3.15340906e-01\n",
      "Epoch: 33983 mean train loss:  3.54471896e-03, bound:  3.15340906e-01\n",
      "Epoch: 33984 mean train loss:  3.54462094e-03, bound:  3.15340906e-01\n",
      "Epoch: 33985 mean train loss:  3.54458368e-03, bound:  3.15340906e-01\n",
      "Epoch: 33986 mean train loss:  3.54451942e-03, bound:  3.15340906e-01\n",
      "Epoch: 33987 mean train loss:  3.54444399e-03, bound:  3.15340906e-01\n",
      "Epoch: 33988 mean train loss:  3.54437530e-03, bound:  3.15340817e-01\n",
      "Epoch: 33989 mean train loss:  3.54431383e-03, bound:  3.15340817e-01\n",
      "Epoch: 33990 mean train loss:  3.54422303e-03, bound:  3.15340817e-01\n",
      "Epoch: 33991 mean train loss:  3.54415108e-03, bound:  3.15340817e-01\n",
      "Epoch: 33992 mean train loss:  3.54405446e-03, bound:  3.15340817e-01\n",
      "Epoch: 33993 mean train loss:  3.54401185e-03, bound:  3.15340817e-01\n",
      "Epoch: 33994 mean train loss:  3.54397413e-03, bound:  3.15340787e-01\n",
      "Epoch: 33995 mean train loss:  3.54385632e-03, bound:  3.15340787e-01\n",
      "Epoch: 33996 mean train loss:  3.54379299e-03, bound:  3.15340787e-01\n",
      "Epoch: 33997 mean train loss:  3.54376785e-03, bound:  3.15340787e-01\n",
      "Epoch: 33998 mean train loss:  3.54364654e-03, bound:  3.15340787e-01\n",
      "Epoch: 33999 mean train loss:  3.54359113e-03, bound:  3.15340787e-01\n",
      "Epoch: 34000 mean train loss:  3.54352477e-03, bound:  3.15340787e-01\n",
      "Epoch: 34001 mean train loss:  3.54344468e-03, bound:  3.15340787e-01\n",
      "Epoch: 34002 mean train loss:  3.54339601e-03, bound:  3.15340787e-01\n",
      "Epoch: 34003 mean train loss:  3.54330940e-03, bound:  3.15340728e-01\n",
      "Epoch: 34004 mean train loss:  3.54334526e-03, bound:  3.15340728e-01\n",
      "Epoch: 34005 mean train loss:  3.54321976e-03, bound:  3.15340698e-01\n",
      "Epoch: 34006 mean train loss:  3.54316598e-03, bound:  3.15340698e-01\n",
      "Epoch: 34007 mean train loss:  3.54310824e-03, bound:  3.15340698e-01\n",
      "Epoch: 34008 mean train loss:  3.54303280e-03, bound:  3.15340698e-01\n",
      "Epoch: 34009 mean train loss:  3.54295620e-03, bound:  3.15340698e-01\n",
      "Epoch: 34010 mean train loss:  3.54288355e-03, bound:  3.15340698e-01\n",
      "Epoch: 34011 mean train loss:  3.54277971e-03, bound:  3.15340698e-01\n",
      "Epoch: 34012 mean train loss:  3.54273617e-03, bound:  3.15340698e-01\n",
      "Epoch: 34013 mean train loss:  3.54263978e-03, bound:  3.15340698e-01\n",
      "Epoch: 34014 mean train loss:  3.54257529e-03, bound:  3.15340668e-01\n",
      "Epoch: 34015 mean train loss:  3.54256481e-03, bound:  3.15340668e-01\n",
      "Epoch: 34016 mean train loss:  3.54250777e-03, bound:  3.15340668e-01\n",
      "Epoch: 34017 mean train loss:  3.54246749e-03, bound:  3.15340668e-01\n",
      "Epoch: 34018 mean train loss:  3.54236783e-03, bound:  3.15340668e-01\n",
      "Epoch: 34019 mean train loss:  3.54229100e-03, bound:  3.15340608e-01\n",
      "Epoch: 34020 mean train loss:  3.54218087e-03, bound:  3.15340608e-01\n",
      "Epoch: 34021 mean train loss:  3.54208425e-03, bound:  3.15340608e-01\n",
      "Epoch: 34022 mean train loss:  3.54202976e-03, bound:  3.15340608e-01\n",
      "Epoch: 34023 mean train loss:  3.54198017e-03, bound:  3.15340608e-01\n",
      "Epoch: 34024 mean train loss:  3.54191731e-03, bound:  3.15340608e-01\n",
      "Epoch: 34025 mean train loss:  3.54179903e-03, bound:  3.15340608e-01\n",
      "Epoch: 34026 mean train loss:  3.54176597e-03, bound:  3.15340579e-01\n",
      "Epoch: 34027 mean train loss:  3.54167703e-03, bound:  3.15340579e-01\n",
      "Epoch: 34028 mean train loss:  3.54162324e-03, bound:  3.15340579e-01\n",
      "Epoch: 34029 mean train loss:  3.54151847e-03, bound:  3.15340579e-01\n",
      "Epoch: 34030 mean train loss:  3.54150566e-03, bound:  3.15340549e-01\n",
      "Epoch: 34031 mean train loss:  3.54142929e-03, bound:  3.15340549e-01\n",
      "Epoch: 34032 mean train loss:  3.54141206e-03, bound:  3.15340549e-01\n",
      "Epoch: 34033 mean train loss:  3.54130985e-03, bound:  3.15340549e-01\n",
      "Epoch: 34034 mean train loss:  3.54125095e-03, bound:  3.15340549e-01\n",
      "Epoch: 34035 mean train loss:  3.54119507e-03, bound:  3.15340489e-01\n",
      "Epoch: 34036 mean train loss:  3.54110752e-03, bound:  3.15340489e-01\n",
      "Epoch: 34037 mean train loss:  3.54100228e-03, bound:  3.15340489e-01\n",
      "Epoch: 34038 mean train loss:  3.54095153e-03, bound:  3.15340489e-01\n",
      "Epoch: 34039 mean train loss:  3.54080671e-03, bound:  3.15340489e-01\n",
      "Epoch: 34040 mean train loss:  3.54080740e-03, bound:  3.15340489e-01\n",
      "Epoch: 34041 mean train loss:  3.54077667e-03, bound:  3.15340489e-01\n",
      "Epoch: 34042 mean train loss:  3.54074198e-03, bound:  3.15340489e-01\n",
      "Epoch: 34043 mean train loss:  3.54062999e-03, bound:  3.15340459e-01\n",
      "Epoch: 34044 mean train loss:  3.54049727e-03, bound:  3.15340459e-01\n",
      "Epoch: 34045 mean train loss:  3.54042579e-03, bound:  3.15340459e-01\n",
      "Epoch: 34046 mean train loss:  3.54039157e-03, bound:  3.15340430e-01\n",
      "Epoch: 34047 mean train loss:  3.54032405e-03, bound:  3.15340400e-01\n",
      "Epoch: 34048 mean train loss:  3.54023115e-03, bound:  3.15340400e-01\n",
      "Epoch: 34049 mean train loss:  3.54014616e-03, bound:  3.15340370e-01\n",
      "Epoch: 34050 mean train loss:  3.54010100e-03, bound:  3.15340370e-01\n",
      "Epoch: 34051 mean train loss:  3.54005839e-03, bound:  3.15340370e-01\n",
      "Epoch: 34052 mean train loss:  3.53999343e-03, bound:  3.15340370e-01\n",
      "Epoch: 34053 mean train loss:  3.53991892e-03, bound:  3.15340370e-01\n",
      "Epoch: 34054 mean train loss:  3.53984325e-03, bound:  3.15340370e-01\n",
      "Epoch: 34055 mean train loss:  3.53979925e-03, bound:  3.15340370e-01\n",
      "Epoch: 34056 mean train loss:  3.53971124e-03, bound:  3.15340370e-01\n",
      "Epoch: 34057 mean train loss:  3.53963091e-03, bound:  3.15340370e-01\n",
      "Epoch: 34058 mean train loss:  3.53960437e-03, bound:  3.15340370e-01\n",
      "Epoch: 34059 mean train loss:  3.53956106e-03, bound:  3.15340340e-01\n",
      "Epoch: 34060 mean train loss:  3.53948562e-03, bound:  3.15340340e-01\n",
      "Epoch: 34061 mean train loss:  3.53938481e-03, bound:  3.15340340e-01\n",
      "Epoch: 34062 mean train loss:  3.53931868e-03, bound:  3.15340340e-01\n",
      "Epoch: 34063 mean train loss:  3.53924464e-03, bound:  3.15340340e-01\n",
      "Epoch: 34064 mean train loss:  3.53913847e-03, bound:  3.15340281e-01\n",
      "Epoch: 34065 mean train loss:  3.53907794e-03, bound:  3.15340281e-01\n",
      "Epoch: 34066 mean train loss:  3.53906373e-03, bound:  3.15340281e-01\n",
      "Epoch: 34067 mean train loss:  3.53894825e-03, bound:  3.15340281e-01\n",
      "Epoch: 34068 mean train loss:  3.53895430e-03, bound:  3.15340281e-01\n",
      "Epoch: 34069 mean train loss:  3.53886932e-03, bound:  3.15340281e-01\n",
      "Epoch: 34070 mean train loss:  3.53872450e-03, bound:  3.15340251e-01\n",
      "Epoch: 34071 mean train loss:  3.53866862e-03, bound:  3.15340251e-01\n",
      "Epoch: 34072 mean train loss:  3.53862415e-03, bound:  3.15340251e-01\n",
      "Epoch: 34073 mean train loss:  3.53854313e-03, bound:  3.15340251e-01\n",
      "Epoch: 34074 mean train loss:  3.53851216e-03, bound:  3.15340251e-01\n",
      "Epoch: 34075 mean train loss:  3.53843649e-03, bound:  3.15340221e-01\n",
      "Epoch: 34076 mean train loss:  3.53836641e-03, bound:  3.15340221e-01\n",
      "Epoch: 34077 mean train loss:  3.53828748e-03, bound:  3.15340221e-01\n",
      "Epoch: 34078 mean train loss:  3.53822624e-03, bound:  3.15340221e-01\n",
      "Epoch: 34079 mean train loss:  3.53813870e-03, bound:  3.15340221e-01\n",
      "Epoch: 34080 mean train loss:  3.53805535e-03, bound:  3.15340221e-01\n",
      "Epoch: 34081 mean train loss:  3.53801111e-03, bound:  3.15340221e-01\n",
      "Epoch: 34082 mean train loss:  3.53790144e-03, bound:  3.15340161e-01\n",
      "Epoch: 34083 mean train loss:  3.53784976e-03, bound:  3.15340161e-01\n",
      "Epoch: 34084 mean train loss:  3.53779341e-03, bound:  3.15340161e-01\n",
      "Epoch: 34085 mean train loss:  3.53772915e-03, bound:  3.15340132e-01\n",
      "Epoch: 34086 mean train loss:  3.53767327e-03, bound:  3.15340132e-01\n",
      "Epoch: 34087 mean train loss:  3.53760342e-03, bound:  3.15340132e-01\n",
      "Epoch: 34088 mean train loss:  3.53751471e-03, bound:  3.15340132e-01\n",
      "Epoch: 34089 mean train loss:  3.53748095e-03, bound:  3.15340132e-01\n",
      "Epoch: 34090 mean train loss:  3.53736291e-03, bound:  3.15340102e-01\n",
      "Epoch: 34091 mean train loss:  3.53731192e-03, bound:  3.15340102e-01\n",
      "Epoch: 34092 mean train loss:  3.53727723e-03, bound:  3.15340102e-01\n",
      "Epoch: 34093 mean train loss:  3.53719876e-03, bound:  3.15340102e-01\n",
      "Epoch: 34094 mean train loss:  3.53715010e-03, bound:  3.15340102e-01\n",
      "Epoch: 34095 mean train loss:  3.53703788e-03, bound:  3.15340102e-01\n",
      "Epoch: 34096 mean train loss:  3.53702903e-03, bound:  3.15340102e-01\n",
      "Epoch: 34097 mean train loss:  3.53690260e-03, bound:  3.15340102e-01\n",
      "Epoch: 34098 mean train loss:  3.53682833e-03, bound:  3.15340042e-01\n",
      "Epoch: 34099 mean train loss:  3.53677198e-03, bound:  3.15340042e-01\n",
      "Epoch: 34100 mean train loss:  3.53671797e-03, bound:  3.15340042e-01\n",
      "Epoch: 34101 mean train loss:  3.53661366e-03, bound:  3.15340042e-01\n",
      "Epoch: 34102 mean train loss:  3.53655708e-03, bound:  3.15340042e-01\n",
      "Epoch: 34103 mean train loss:  3.53651331e-03, bound:  3.15340012e-01\n",
      "Epoch: 34104 mean train loss:  3.53644416e-03, bound:  3.15340012e-01\n",
      "Epoch: 34105 mean train loss:  3.53638921e-03, bound:  3.15340012e-01\n",
      "Epoch: 34106 mean train loss:  3.53630655e-03, bound:  3.15339983e-01\n",
      "Epoch: 34107 mean train loss:  3.53624299e-03, bound:  3.15339983e-01\n",
      "Epoch: 34108 mean train loss:  3.53613426e-03, bound:  3.15339983e-01\n",
      "Epoch: 34109 mean train loss:  3.53609212e-03, bound:  3.15339983e-01\n",
      "Epoch: 34110 mean train loss:  3.53604765e-03, bound:  3.15339983e-01\n",
      "Epoch: 34111 mean train loss:  3.53592285e-03, bound:  3.15339983e-01\n",
      "Epoch: 34112 mean train loss:  3.53589375e-03, bound:  3.15339983e-01\n",
      "Epoch: 34113 mean train loss:  3.53581645e-03, bound:  3.15339953e-01\n",
      "Epoch: 34114 mean train loss:  3.53575265e-03, bound:  3.15339923e-01\n",
      "Epoch: 34115 mean train loss:  3.53568955e-03, bound:  3.15339923e-01\n",
      "Epoch: 34116 mean train loss:  3.53563367e-03, bound:  3.15339923e-01\n",
      "Epoch: 34117 mean train loss:  3.53557221e-03, bound:  3.15339923e-01\n",
      "Epoch: 34118 mean train loss:  3.53551330e-03, bound:  3.15339923e-01\n",
      "Epoch: 34119 mean train loss:  3.53543321e-03, bound:  3.15339893e-01\n",
      "Epoch: 34120 mean train loss:  3.53533472e-03, bound:  3.15339893e-01\n",
      "Epoch: 34121 mean train loss:  3.53533681e-03, bound:  3.15339893e-01\n",
      "Epoch: 34122 mean train loss:  3.53519386e-03, bound:  3.15339893e-01\n",
      "Epoch: 34123 mean train loss:  3.53518175e-03, bound:  3.15339893e-01\n",
      "Epoch: 34124 mean train loss:  3.53507232e-03, bound:  3.15339863e-01\n",
      "Epoch: 34125 mean train loss:  3.53503949e-03, bound:  3.15339863e-01\n",
      "Epoch: 34126 mean train loss:  3.53493844e-03, bound:  3.15339863e-01\n",
      "Epoch: 34127 mean train loss:  3.53489257e-03, bound:  3.15339833e-01\n",
      "Epoch: 34128 mean train loss:  3.53473076e-03, bound:  3.15339833e-01\n",
      "Epoch: 34129 mean train loss:  3.53472703e-03, bound:  3.15339833e-01\n",
      "Epoch: 34130 mean train loss:  3.53463832e-03, bound:  3.15339804e-01\n",
      "Epoch: 34131 mean train loss:  3.53457686e-03, bound:  3.15339804e-01\n",
      "Epoch: 34132 mean train loss:  3.53454705e-03, bound:  3.15339804e-01\n",
      "Epoch: 34133 mean train loss:  3.53445578e-03, bound:  3.15339804e-01\n",
      "Epoch: 34134 mean train loss:  3.53439990e-03, bound:  3.15339804e-01\n",
      "Epoch: 34135 mean train loss:  3.53430863e-03, bound:  3.15339804e-01\n",
      "Epoch: 34136 mean train loss:  3.53426137e-03, bound:  3.15339804e-01\n",
      "Epoch: 34137 mean train loss:  3.53417918e-03, bound:  3.15339804e-01\n",
      "Epoch: 34138 mean train loss:  3.53410421e-03, bound:  3.15339774e-01\n",
      "Epoch: 34139 mean train loss:  3.53403785e-03, bound:  3.15339774e-01\n",
      "Epoch: 34140 mean train loss:  3.53396474e-03, bound:  3.15339744e-01\n",
      "Epoch: 34141 mean train loss:  3.53389350e-03, bound:  3.15339744e-01\n",
      "Epoch: 34142 mean train loss:  3.53380153e-03, bound:  3.15339744e-01\n",
      "Epoch: 34143 mean train loss:  3.53377638e-03, bound:  3.15339714e-01\n",
      "Epoch: 34144 mean train loss:  3.53367324e-03, bound:  3.15339714e-01\n",
      "Epoch: 34145 mean train loss:  3.53361643e-03, bound:  3.15339684e-01\n",
      "Epoch: 34146 mean train loss:  3.53358989e-03, bound:  3.15339684e-01\n",
      "Epoch: 34147 mean train loss:  3.53349140e-03, bound:  3.15339684e-01\n",
      "Epoch: 34148 mean train loss:  3.53350537e-03, bound:  3.15339684e-01\n",
      "Epoch: 34149 mean train loss:  3.53339431e-03, bound:  3.15339684e-01\n",
      "Epoch: 34150 mean train loss:  3.53332190e-03, bound:  3.15339684e-01\n",
      "Epoch: 34151 mean train loss:  3.53323040e-03, bound:  3.15339684e-01\n",
      "Epoch: 34152 mean train loss:  3.53315566e-03, bound:  3.15339684e-01\n",
      "Epoch: 34153 mean train loss:  3.53308790e-03, bound:  3.15339655e-01\n",
      "Epoch: 34154 mean train loss:  3.53303086e-03, bound:  3.15339655e-01\n",
      "Epoch: 34155 mean train loss:  3.53295403e-03, bound:  3.15339655e-01\n",
      "Epoch: 34156 mean train loss:  3.53289931e-03, bound:  3.15339655e-01\n",
      "Epoch: 34157 mean train loss:  3.53283365e-03, bound:  3.15339655e-01\n",
      "Epoch: 34158 mean train loss:  3.53274448e-03, bound:  3.15339655e-01\n",
      "Epoch: 34159 mean train loss:  3.53268581e-03, bound:  3.15339595e-01\n",
      "Epoch: 34160 mean train loss:  3.53261409e-03, bound:  3.15339565e-01\n",
      "Epoch: 34161 mean train loss:  3.53256450e-03, bound:  3.15339565e-01\n",
      "Epoch: 34162 mean train loss:  3.53249116e-03, bound:  3.15339565e-01\n",
      "Epoch: 34163 mean train loss:  3.53241852e-03, bound:  3.15339565e-01\n",
      "Epoch: 34164 mean train loss:  3.53237754e-03, bound:  3.15339565e-01\n",
      "Epoch: 34165 mean train loss:  3.53223761e-03, bound:  3.15339565e-01\n",
      "Epoch: 34166 mean train loss:  3.53222620e-03, bound:  3.15339565e-01\n",
      "Epoch: 34167 mean train loss:  3.53213190e-03, bound:  3.15339565e-01\n",
      "Epoch: 34168 mean train loss:  3.53202736e-03, bound:  3.15339565e-01\n",
      "Epoch: 34169 mean train loss:  3.53201525e-03, bound:  3.15339535e-01\n",
      "Epoch: 34170 mean train loss:  3.53189977e-03, bound:  3.15339535e-01\n",
      "Epoch: 34171 mean train loss:  3.53183295e-03, bound:  3.15339535e-01\n",
      "Epoch: 34172 mean train loss:  3.53175122e-03, bound:  3.15339506e-01\n",
      "Epoch: 34173 mean train loss:  3.53168626e-03, bound:  3.15339506e-01\n",
      "Epoch: 34174 mean train loss:  3.53162526e-03, bound:  3.15339506e-01\n",
      "Epoch: 34175 mean train loss:  3.53159173e-03, bound:  3.15339476e-01\n",
      "Epoch: 34176 mean train loss:  3.53149814e-03, bound:  3.15339476e-01\n",
      "Epoch: 34177 mean train loss:  3.53144924e-03, bound:  3.15339476e-01\n",
      "Epoch: 34178 mean train loss:  3.53138056e-03, bound:  3.15339476e-01\n",
      "Epoch: 34179 mean train loss:  3.53131443e-03, bound:  3.15339476e-01\n",
      "Epoch: 34180 mean train loss:  3.53124971e-03, bound:  3.15339446e-01\n",
      "Epoch: 34181 mean train loss:  3.53117730e-03, bound:  3.15339446e-01\n",
      "Epoch: 34182 mean train loss:  3.53110582e-03, bound:  3.15339446e-01\n",
      "Epoch: 34183 mean train loss:  3.53101641e-03, bound:  3.15339446e-01\n",
      "Epoch: 34184 mean train loss:  3.53093096e-03, bound:  3.15339446e-01\n",
      "Epoch: 34185 mean train loss:  3.53087648e-03, bound:  3.15339416e-01\n",
      "Epoch: 34186 mean train loss:  3.53080244e-03, bound:  3.15339416e-01\n",
      "Epoch: 34187 mean train loss:  3.53069417e-03, bound:  3.15339416e-01\n",
      "Epoch: 34188 mean train loss:  3.53069487e-03, bound:  3.15339416e-01\n",
      "Epoch: 34189 mean train loss:  3.53065459e-03, bound:  3.15339386e-01\n",
      "Epoch: 34190 mean train loss:  3.53055145e-03, bound:  3.15339386e-01\n",
      "Epoch: 34191 mean train loss:  3.53046739e-03, bound:  3.15339386e-01\n",
      "Epoch: 34192 mean train loss:  3.53039661e-03, bound:  3.15339386e-01\n",
      "Epoch: 34193 mean train loss:  3.53032118e-03, bound:  3.15339357e-01\n",
      "Epoch: 34194 mean train loss:  3.53027484e-03, bound:  3.15339357e-01\n",
      "Epoch: 34195 mean train loss:  3.53020732e-03, bound:  3.15339357e-01\n",
      "Epoch: 34196 mean train loss:  3.53013887e-03, bound:  3.15339357e-01\n",
      "Epoch: 34197 mean train loss:  3.53006390e-03, bound:  3.15339357e-01\n",
      "Epoch: 34198 mean train loss:  3.53005785e-03, bound:  3.15339327e-01\n",
      "Epoch: 34199 mean train loss:  3.52995237e-03, bound:  3.15339327e-01\n",
      "Epoch: 34200 mean train loss:  3.52986925e-03, bound:  3.15339327e-01\n",
      "Epoch: 34201 mean train loss:  3.52979987e-03, bound:  3.15339297e-01\n",
      "Epoch: 34202 mean train loss:  3.52975912e-03, bound:  3.15339297e-01\n",
      "Epoch: 34203 mean train loss:  3.52966529e-03, bound:  3.15339267e-01\n",
      "Epoch: 34204 mean train loss:  3.52960057e-03, bound:  3.15339267e-01\n",
      "Epoch: 34205 mean train loss:  3.52949160e-03, bound:  3.15339267e-01\n",
      "Epoch: 34206 mean train loss:  3.52945761e-03, bound:  3.15339267e-01\n",
      "Epoch: 34207 mean train loss:  3.52936913e-03, bound:  3.15339267e-01\n",
      "Epoch: 34208 mean train loss:  3.52929952e-03, bound:  3.15339237e-01\n",
      "Epoch: 34209 mean train loss:  3.52927903e-03, bound:  3.15339237e-01\n",
      "Epoch: 34210 mean train loss:  3.52917425e-03, bound:  3.15339237e-01\n",
      "Epoch: 34211 mean train loss:  3.52915050e-03, bound:  3.15339237e-01\n",
      "Epoch: 34212 mean train loss:  3.52907460e-03, bound:  3.15339237e-01\n",
      "Epoch: 34213 mean train loss:  3.52901779e-03, bound:  3.15339237e-01\n",
      "Epoch: 34214 mean train loss:  3.52894934e-03, bound:  3.15339208e-01\n",
      "Epoch: 34215 mean train loss:  3.52891535e-03, bound:  3.15339208e-01\n",
      "Epoch: 34216 mean train loss:  3.52887670e-03, bound:  3.15339178e-01\n",
      "Epoch: 34217 mean train loss:  3.52877588e-03, bound:  3.15339178e-01\n",
      "Epoch: 34218 mean train loss:  3.52871581e-03, bound:  3.15339178e-01\n",
      "Epoch: 34219 mean train loss:  3.52859567e-03, bound:  3.15339178e-01\n",
      "Epoch: 34220 mean train loss:  3.52853700e-03, bound:  3.15339178e-01\n",
      "Epoch: 34221 mean train loss:  3.52839613e-03, bound:  3.15339178e-01\n",
      "Epoch: 34222 mean train loss:  3.52840312e-03, bound:  3.15339148e-01\n",
      "Epoch: 34223 mean train loss:  3.52827483e-03, bound:  3.15339148e-01\n",
      "Epoch: 34224 mean train loss:  3.52824666e-03, bound:  3.15339118e-01\n",
      "Epoch: 34225 mean train loss:  3.52815539e-03, bound:  3.15339118e-01\n",
      "Epoch: 34226 mean train loss:  3.52810579e-03, bound:  3.15339118e-01\n",
      "Epoch: 34227 mean train loss:  3.52800079e-03, bound:  3.15339118e-01\n",
      "Epoch: 34228 mean train loss:  3.52796237e-03, bound:  3.15339118e-01\n",
      "Epoch: 34229 mean train loss:  3.52792884e-03, bound:  3.15339118e-01\n",
      "Epoch: 34230 mean train loss:  3.52787902e-03, bound:  3.15339118e-01\n",
      "Epoch: 34231 mean train loss:  3.52777448e-03, bound:  3.15339088e-01\n",
      "Epoch: 34232 mean train loss:  3.52764060e-03, bound:  3.15339088e-01\n",
      "Epoch: 34233 mean train loss:  3.52759939e-03, bound:  3.15339088e-01\n",
      "Epoch: 34234 mean train loss:  3.52753862e-03, bound:  3.15339088e-01\n",
      "Epoch: 34235 mean train loss:  3.52749065e-03, bound:  3.15339029e-01\n",
      "Epoch: 34236 mean train loss:  3.52738076e-03, bound:  3.15339029e-01\n",
      "Epoch: 34237 mean train loss:  3.52731207e-03, bound:  3.15339029e-01\n",
      "Epoch: 34238 mean train loss:  3.52726365e-03, bound:  3.15339029e-01\n",
      "Epoch: 34239 mean train loss:  3.52722011e-03, bound:  3.15339029e-01\n",
      "Epoch: 34240 mean train loss:  3.52714350e-03, bound:  3.15339029e-01\n",
      "Epoch: 34241 mean train loss:  3.52701382e-03, bound:  3.15338999e-01\n",
      "Epoch: 34242 mean train loss:  3.52707249e-03, bound:  3.15338999e-01\n",
      "Epoch: 34243 mean train loss:  3.52698681e-03, bound:  3.15338999e-01\n",
      "Epoch: 34244 mean train loss:  3.52696446e-03, bound:  3.15338969e-01\n",
      "Epoch: 34245 mean train loss:  3.52685642e-03, bound:  3.15338999e-01\n",
      "Epoch: 34246 mean train loss:  3.52681754e-03, bound:  3.15338969e-01\n",
      "Epoch: 34247 mean train loss:  3.52671300e-03, bound:  3.15338969e-01\n",
      "Epoch: 34248 mean train loss:  3.52661498e-03, bound:  3.15338969e-01\n",
      "Epoch: 34249 mean train loss:  3.52654466e-03, bound:  3.15338969e-01\n",
      "Epoch: 34250 mean train loss:  3.52644548e-03, bound:  3.15338969e-01\n",
      "Epoch: 34251 mean train loss:  3.52636422e-03, bound:  3.15338939e-01\n",
      "Epoch: 34252 mean train loss:  3.52631626e-03, bound:  3.15338939e-01\n",
      "Epoch: 34253 mean train loss:  3.52625316e-03, bound:  3.15338910e-01\n",
      "Epoch: 34254 mean train loss:  3.52616515e-03, bound:  3.15338910e-01\n",
      "Epoch: 34255 mean train loss:  3.52607621e-03, bound:  3.15338910e-01\n",
      "Epoch: 34256 mean train loss:  3.52608599e-03, bound:  3.15338910e-01\n",
      "Epoch: 34257 mean train loss:  3.52600799e-03, bound:  3.15338880e-01\n",
      "Epoch: 34258 mean train loss:  3.52596305e-03, bound:  3.15338880e-01\n",
      "Epoch: 34259 mean train loss:  3.52590252e-03, bound:  3.15338880e-01\n",
      "Epoch: 34260 mean train loss:  3.52576817e-03, bound:  3.15338880e-01\n",
      "Epoch: 34261 mean train loss:  3.52574489e-03, bound:  3.15338880e-01\n",
      "Epoch: 34262 mean train loss:  3.52566456e-03, bound:  3.15338880e-01\n",
      "Epoch: 34263 mean train loss:  3.52556189e-03, bound:  3.15338880e-01\n",
      "Epoch: 34264 mean train loss:  3.52545548e-03, bound:  3.15338850e-01\n",
      "Epoch: 34265 mean train loss:  3.52543662e-03, bound:  3.15338850e-01\n",
      "Epoch: 34266 mean train loss:  3.52535234e-03, bound:  3.15338850e-01\n",
      "Epoch: 34267 mean train loss:  3.52531788e-03, bound:  3.15338850e-01\n",
      "Epoch: 34268 mean train loss:  3.52527876e-03, bound:  3.15338820e-01\n",
      "Epoch: 34269 mean train loss:  3.52519401e-03, bound:  3.15338820e-01\n",
      "Epoch: 34270 mean train loss:  3.52508691e-03, bound:  3.15338790e-01\n",
      "Epoch: 34271 mean train loss:  3.52503592e-03, bound:  3.15338790e-01\n",
      "Epoch: 34272 mean train loss:  3.52499285e-03, bound:  3.15338761e-01\n",
      "Epoch: 34273 mean train loss:  3.52492160e-03, bound:  3.15338761e-01\n",
      "Epoch: 34274 mean train loss:  3.52483941e-03, bound:  3.15338761e-01\n",
      "Epoch: 34275 mean train loss:  3.52476421e-03, bound:  3.15338761e-01\n",
      "Epoch: 34276 mean train loss:  3.52473254e-03, bound:  3.15338761e-01\n",
      "Epoch: 34277 mean train loss:  3.52464826e-03, bound:  3.15338761e-01\n",
      "Epoch: 34278 mean train loss:  3.52452579e-03, bound:  3.15338761e-01\n",
      "Epoch: 34279 mean train loss:  3.52446386e-03, bound:  3.15338761e-01\n",
      "Epoch: 34280 mean train loss:  3.52441426e-03, bound:  3.15338731e-01\n",
      "Epoch: 34281 mean train loss:  3.52433324e-03, bound:  3.15338731e-01\n",
      "Epoch: 34282 mean train loss:  3.52422451e-03, bound:  3.15338731e-01\n",
      "Epoch: 34283 mean train loss:  3.52421869e-03, bound:  3.15338701e-01\n",
      "Epoch: 34284 mean train loss:  3.52413743e-03, bound:  3.15338701e-01\n",
      "Epoch: 34285 mean train loss:  3.52410297e-03, bound:  3.15338671e-01\n",
      "Epoch: 34286 mean train loss:  3.52404057e-03, bound:  3.15338701e-01\n",
      "Epoch: 34287 mean train loss:  3.52395722e-03, bound:  3.15338671e-01\n",
      "Epoch: 34288 mean train loss:  3.52389086e-03, bound:  3.15338671e-01\n",
      "Epoch: 34289 mean train loss:  3.52374371e-03, bound:  3.15338671e-01\n",
      "Epoch: 34290 mean train loss:  3.52372788e-03, bound:  3.15338671e-01\n",
      "Epoch: 34291 mean train loss:  3.52364918e-03, bound:  3.15338671e-01\n",
      "Epoch: 34292 mean train loss:  3.52358306e-03, bound:  3.15338671e-01\n",
      "Epoch: 34293 mean train loss:  3.52353090e-03, bound:  3.15338671e-01\n",
      "Epoch: 34294 mean train loss:  3.52345640e-03, bound:  3.15338641e-01\n",
      "Epoch: 34295 mean train loss:  3.52340424e-03, bound:  3.15338641e-01\n",
      "Epoch: 34296 mean train loss:  3.52330855e-03, bound:  3.15338641e-01\n",
      "Epoch: 34297 mean train loss:  3.52330622e-03, bound:  3.15338641e-01\n",
      "Epoch: 34298 mean train loss:  3.52316815e-03, bound:  3.15338612e-01\n",
      "Epoch: 34299 mean train loss:  3.52312042e-03, bound:  3.15338582e-01\n",
      "Epoch: 34300 mean train loss:  3.52308899e-03, bound:  3.15338582e-01\n",
      "Epoch: 34301 mean train loss:  3.52297118e-03, bound:  3.15338582e-01\n",
      "Epoch: 34302 mean train loss:  3.52291553e-03, bound:  3.15338552e-01\n",
      "Epoch: 34303 mean train loss:  3.52288294e-03, bound:  3.15338552e-01\n",
      "Epoch: 34304 mean train loss:  3.52279446e-03, bound:  3.15338552e-01\n",
      "Epoch: 34305 mean train loss:  3.52271157e-03, bound:  3.15338552e-01\n",
      "Epoch: 34306 mean train loss:  3.52261798e-03, bound:  3.15338552e-01\n",
      "Epoch: 34307 mean train loss:  3.52259260e-03, bound:  3.15338552e-01\n",
      "Epoch: 34308 mean train loss:  3.52250552e-03, bound:  3.15338552e-01\n",
      "Epoch: 34309 mean train loss:  3.52243148e-03, bound:  3.15338552e-01\n",
      "Epoch: 34310 mean train loss:  3.52236954e-03, bound:  3.15338522e-01\n",
      "Epoch: 34311 mean train loss:  3.52228433e-03, bound:  3.15338522e-01\n",
      "Epoch: 34312 mean train loss:  3.52227571e-03, bound:  3.15338463e-01\n",
      "Epoch: 34313 mean train loss:  3.52216628e-03, bound:  3.15338463e-01\n",
      "Epoch: 34314 mean train loss:  3.52209457e-03, bound:  3.15338463e-01\n",
      "Epoch: 34315 mean train loss:  3.52199865e-03, bound:  3.15338463e-01\n",
      "Epoch: 34316 mean train loss:  3.52196419e-03, bound:  3.15338463e-01\n",
      "Epoch: 34317 mean train loss:  3.52191296e-03, bound:  3.15338463e-01\n",
      "Epoch: 34318 mean train loss:  3.52181192e-03, bound:  3.15338463e-01\n",
      "Epoch: 34319 mean train loss:  3.52176116e-03, bound:  3.15338463e-01\n",
      "Epoch: 34320 mean train loss:  3.52172903e-03, bound:  3.15338433e-01\n",
      "Epoch: 34321 mean train loss:  3.52159841e-03, bound:  3.15338433e-01\n",
      "Epoch: 34322 mean train loss:  3.52157396e-03, bound:  3.15338433e-01\n",
      "Epoch: 34323 mean train loss:  3.52146919e-03, bound:  3.15338433e-01\n",
      "Epoch: 34324 mean train loss:  3.52140656e-03, bound:  3.15338433e-01\n",
      "Epoch: 34325 mean train loss:  3.52136418e-03, bound:  3.15338433e-01\n",
      "Epoch: 34326 mean train loss:  3.52131366e-03, bound:  3.15338403e-01\n",
      "Epoch: 34327 mean train loss:  3.52119701e-03, bound:  3.15338403e-01\n",
      "Epoch: 34328 mean train loss:  3.52115440e-03, bound:  3.15338373e-01\n",
      "Epoch: 34329 mean train loss:  3.52109782e-03, bound:  3.15338373e-01\n",
      "Epoch: 34330 mean train loss:  3.52102099e-03, bound:  3.15338373e-01\n",
      "Epoch: 34331 mean train loss:  3.52098863e-03, bound:  3.15338373e-01\n",
      "Epoch: 34332 mean train loss:  3.52088315e-03, bound:  3.15338373e-01\n",
      "Epoch: 34333 mean train loss:  3.52080259e-03, bound:  3.15338343e-01\n",
      "Epoch: 34334 mean train loss:  3.52074346e-03, bound:  3.15338343e-01\n",
      "Epoch: 34335 mean train loss:  3.52067780e-03, bound:  3.15338343e-01\n",
      "Epoch: 34336 mean train loss:  3.52059514e-03, bound:  3.15338343e-01\n",
      "Epoch: 34337 mean train loss:  3.52053926e-03, bound:  3.15338314e-01\n",
      "Epoch: 34338 mean train loss:  3.52046150e-03, bound:  3.15338314e-01\n",
      "Epoch: 34339 mean train loss:  3.52038955e-03, bound:  3.15338314e-01\n",
      "Epoch: 34340 mean train loss:  3.52034182e-03, bound:  3.15338314e-01\n",
      "Epoch: 34341 mean train loss:  3.52023402e-03, bound:  3.15338314e-01\n",
      "Epoch: 34342 mean train loss:  3.52018536e-03, bound:  3.15338284e-01\n",
      "Epoch: 34343 mean train loss:  3.52012296e-03, bound:  3.15338284e-01\n",
      "Epoch: 34344 mean train loss:  3.52006545e-03, bound:  3.15338284e-01\n",
      "Epoch: 34345 mean train loss:  3.51997628e-03, bound:  3.15338284e-01\n",
      "Epoch: 34346 mean train loss:  3.51992017e-03, bound:  3.15338284e-01\n",
      "Epoch: 34347 mean train loss:  3.51986359e-03, bound:  3.15338254e-01\n",
      "Epoch: 34348 mean train loss:  3.51979211e-03, bound:  3.15338254e-01\n",
      "Epoch: 34349 mean train loss:  3.51970689e-03, bound:  3.15338254e-01\n",
      "Epoch: 34350 mean train loss:  3.51969898e-03, bound:  3.15338254e-01\n",
      "Epoch: 34351 mean train loss:  3.51957511e-03, bound:  3.15338224e-01\n",
      "Epoch: 34352 mean train loss:  3.51949595e-03, bound:  3.15338224e-01\n",
      "Epoch: 34353 mean train loss:  3.51946894e-03, bound:  3.15338194e-01\n",
      "Epoch: 34354 mean train loss:  3.51939350e-03, bound:  3.15338194e-01\n",
      "Epoch: 34355 mean train loss:  3.51930875e-03, bound:  3.15338194e-01\n",
      "Epoch: 34356 mean train loss:  3.51925706e-03, bound:  3.15338194e-01\n",
      "Epoch: 34357 mean train loss:  3.51919024e-03, bound:  3.15338194e-01\n",
      "Epoch: 34358 mean train loss:  3.51914251e-03, bound:  3.15338165e-01\n",
      "Epoch: 34359 mean train loss:  3.51908128e-03, bound:  3.15338165e-01\n",
      "Epoch: 34360 mean train loss:  3.51904938e-03, bound:  3.15338165e-01\n",
      "Epoch: 34361 mean train loss:  3.51894530e-03, bound:  3.15338165e-01\n",
      "Epoch: 34362 mean train loss:  3.51886777e-03, bound:  3.15338165e-01\n",
      "Epoch: 34363 mean train loss:  3.51881958e-03, bound:  3.15338135e-01\n",
      "Epoch: 34364 mean train loss:  3.51866730e-03, bound:  3.15338135e-01\n",
      "Epoch: 34365 mean train loss:  3.51868710e-03, bound:  3.15338135e-01\n",
      "Epoch: 34366 mean train loss:  3.51861794e-03, bound:  3.15338135e-01\n",
      "Epoch: 34367 mean train loss:  3.51850875e-03, bound:  3.15338105e-01\n",
      "Epoch: 34368 mean train loss:  3.51843983e-03, bound:  3.15338105e-01\n",
      "Epoch: 34369 mean train loss:  3.51840421e-03, bound:  3.15338075e-01\n",
      "Epoch: 34370 mean train loss:  3.51828127e-03, bound:  3.15338075e-01\n",
      "Epoch: 34371 mean train loss:  3.51822167e-03, bound:  3.15338075e-01\n",
      "Epoch: 34372 mean train loss:  3.51815764e-03, bound:  3.15338075e-01\n",
      "Epoch: 34373 mean train loss:  3.51812365e-03, bound:  3.15338075e-01\n",
      "Epoch: 34374 mean train loss:  3.51802865e-03, bound:  3.15338075e-01\n",
      "Epoch: 34375 mean train loss:  3.51799908e-03, bound:  3.15338075e-01\n",
      "Epoch: 34376 mean train loss:  3.51783703e-03, bound:  3.15338075e-01\n",
      "Epoch: 34377 mean train loss:  3.51786474e-03, bound:  3.15338016e-01\n",
      "Epoch: 34378 mean train loss:  3.51779955e-03, bound:  3.15338016e-01\n",
      "Epoch: 34379 mean train loss:  3.51768220e-03, bound:  3.15338016e-01\n",
      "Epoch: 34380 mean train loss:  3.51766194e-03, bound:  3.15338016e-01\n",
      "Epoch: 34381 mean train loss:  3.51753109e-03, bound:  3.15338016e-01\n",
      "Epoch: 34382 mean train loss:  3.51748592e-03, bound:  3.15337986e-01\n",
      "Epoch: 34383 mean train loss:  3.51741840e-03, bound:  3.15337986e-01\n",
      "Epoch: 34384 mean train loss:  3.51739279e-03, bound:  3.15337986e-01\n",
      "Epoch: 34385 mean train loss:  3.51725682e-03, bound:  3.15337986e-01\n",
      "Epoch: 34386 mean train loss:  3.51719325e-03, bound:  3.15337986e-01\n",
      "Epoch: 34387 mean train loss:  3.51713714e-03, bound:  3.15337986e-01\n",
      "Epoch: 34388 mean train loss:  3.51711991e-03, bound:  3.15337986e-01\n",
      "Epoch: 34389 mean train loss:  3.51700233e-03, bound:  3.15337986e-01\n",
      "Epoch: 34390 mean train loss:  3.51694599e-03, bound:  3.15337956e-01\n",
      "Epoch: 34391 mean train loss:  3.51689477e-03, bound:  3.15337956e-01\n",
      "Epoch: 34392 mean train loss:  3.51678883e-03, bound:  3.15337956e-01\n",
      "Epoch: 34393 mean train loss:  3.51676834e-03, bound:  3.15337956e-01\n",
      "Epoch: 34394 mean train loss:  3.51671292e-03, bound:  3.15337896e-01\n",
      "Epoch: 34395 mean train loss:  3.51661537e-03, bound:  3.15337896e-01\n",
      "Epoch: 34396 mean train loss:  3.51654366e-03, bound:  3.15337896e-01\n",
      "Epoch: 34397 mean train loss:  3.51653970e-03, bound:  3.15337896e-01\n",
      "Epoch: 34398 mean train loss:  3.51644144e-03, bound:  3.15337896e-01\n",
      "Epoch: 34399 mean train loss:  3.51636787e-03, bound:  3.15337867e-01\n",
      "Epoch: 34400 mean train loss:  3.51627870e-03, bound:  3.15337867e-01\n",
      "Epoch: 34401 mean train loss:  3.51622771e-03, bound:  3.15337867e-01\n",
      "Epoch: 34402 mean train loss:  3.51616018e-03, bound:  3.15337867e-01\n",
      "Epoch: 34403 mean train loss:  3.51608638e-03, bound:  3.15337867e-01\n",
      "Epoch: 34404 mean train loss:  3.51602491e-03, bound:  3.15337867e-01\n",
      "Epoch: 34405 mean train loss:  3.51590523e-03, bound:  3.15337867e-01\n",
      "Epoch: 34406 mean train loss:  3.51589196e-03, bound:  3.15337837e-01\n",
      "Epoch: 34407 mean train loss:  3.51583632e-03, bound:  3.15337837e-01\n",
      "Epoch: 34408 mean train loss:  3.51574202e-03, bound:  3.15337837e-01\n",
      "Epoch: 34409 mean train loss:  3.51568055e-03, bound:  3.15337837e-01\n",
      "Epoch: 34410 mean train loss:  3.51562002e-03, bound:  3.15337777e-01\n",
      "Epoch: 34411 mean train loss:  3.51549848e-03, bound:  3.15337777e-01\n",
      "Epoch: 34412 mean train loss:  3.51549895e-03, bound:  3.15337777e-01\n",
      "Epoch: 34413 mean train loss:  3.51540581e-03, bound:  3.15337777e-01\n",
      "Epoch: 34414 mean train loss:  3.51532293e-03, bound:  3.15337777e-01\n",
      "Epoch: 34415 mean train loss:  3.51526192e-03, bound:  3.15337747e-01\n",
      "Epoch: 34416 mean train loss:  3.51516437e-03, bound:  3.15337747e-01\n",
      "Epoch: 34417 mean train loss:  3.51510197e-03, bound:  3.15337747e-01\n",
      "Epoch: 34418 mean train loss:  3.51502304e-03, bound:  3.15337747e-01\n",
      "Epoch: 34419 mean train loss:  3.51498183e-03, bound:  3.15337747e-01\n",
      "Epoch: 34420 mean train loss:  3.51492944e-03, bound:  3.15337747e-01\n",
      "Epoch: 34421 mean train loss:  3.51486285e-03, bound:  3.15337747e-01\n",
      "Epoch: 34422 mean train loss:  3.51478416e-03, bound:  3.15337747e-01\n",
      "Epoch: 34423 mean train loss:  3.51467985e-03, bound:  3.15337718e-01\n",
      "Epoch: 34424 mean train loss:  3.51463840e-03, bound:  3.15337718e-01\n",
      "Epoch: 34425 mean train loss:  3.51458066e-03, bound:  3.15337718e-01\n",
      "Epoch: 34426 mean train loss:  3.51454713e-03, bound:  3.15337688e-01\n",
      "Epoch: 34427 mean train loss:  3.51440674e-03, bound:  3.15337688e-01\n",
      "Epoch: 34428 mean train loss:  3.51434853e-03, bound:  3.15337658e-01\n",
      "Epoch: 34429 mean train loss:  3.51428171e-03, bound:  3.15337658e-01\n",
      "Epoch: 34430 mean train loss:  3.51422769e-03, bound:  3.15337658e-01\n",
      "Epoch: 34431 mean train loss:  3.51418578e-03, bound:  3.15337658e-01\n",
      "Epoch: 34432 mean train loss:  3.51407356e-03, bound:  3.15337628e-01\n",
      "Epoch: 34433 mean train loss:  3.51402070e-03, bound:  3.15337628e-01\n",
      "Epoch: 34434 mean train loss:  3.51399835e-03, bound:  3.15337628e-01\n",
      "Epoch: 34435 mean train loss:  3.51389474e-03, bound:  3.15337628e-01\n",
      "Epoch: 34436 mean train loss:  3.51383211e-03, bound:  3.15337628e-01\n",
      "Epoch: 34437 mean train loss:  3.51375458e-03, bound:  3.15337628e-01\n",
      "Epoch: 34438 mean train loss:  3.51369823e-03, bound:  3.15337628e-01\n",
      "Epoch: 34439 mean train loss:  3.51360906e-03, bound:  3.15337569e-01\n",
      "Epoch: 34440 mean train loss:  3.51355691e-03, bound:  3.15337569e-01\n",
      "Epoch: 34441 mean train loss:  3.51351011e-03, bound:  3.15337569e-01\n",
      "Epoch: 34442 mean train loss:  3.51345027e-03, bound:  3.15337569e-01\n",
      "Epoch: 34443 mean train loss:  3.51337390e-03, bound:  3.15337569e-01\n",
      "Epoch: 34444 mean train loss:  3.51333828e-03, bound:  3.15337569e-01\n",
      "Epoch: 34445 mean train loss:  3.51324468e-03, bound:  3.15337569e-01\n",
      "Epoch: 34446 mean train loss:  3.51323001e-03, bound:  3.15337539e-01\n",
      "Epoch: 34447 mean train loss:  3.51317856e-03, bound:  3.15337569e-01\n",
      "Epoch: 34448 mean train loss:  3.51310871e-03, bound:  3.15337539e-01\n",
      "Epoch: 34449 mean train loss:  3.51301348e-03, bound:  3.15337539e-01\n",
      "Epoch: 34450 mean train loss:  3.51293641e-03, bound:  3.15337509e-01\n",
      "Epoch: 34451 mean train loss:  3.51284584e-03, bound:  3.15337509e-01\n",
      "Epoch: 34452 mean train loss:  3.51275946e-03, bound:  3.15337509e-01\n",
      "Epoch: 34453 mean train loss:  3.51270894e-03, bound:  3.15337509e-01\n",
      "Epoch: 34454 mean train loss:  3.51262209e-03, bound:  3.15337509e-01\n",
      "Epoch: 34455 mean train loss:  3.51258647e-03, bound:  3.15337449e-01\n",
      "Epoch: 34456 mean train loss:  3.51246214e-03, bound:  3.15337449e-01\n",
      "Epoch: 34457 mean train loss:  3.51234293e-03, bound:  3.15337449e-01\n",
      "Epoch: 34458 mean train loss:  3.51234712e-03, bound:  3.15337449e-01\n",
      "Epoch: 34459 mean train loss:  3.51228006e-03, bound:  3.15337449e-01\n",
      "Epoch: 34460 mean train loss:  3.51215806e-03, bound:  3.15337449e-01\n",
      "Epoch: 34461 mean train loss:  3.51217692e-03, bound:  3.15337449e-01\n",
      "Epoch: 34462 mean train loss:  3.51203769e-03, bound:  3.15337449e-01\n",
      "Epoch: 34463 mean train loss:  3.51196504e-03, bound:  3.15337420e-01\n",
      "Epoch: 34464 mean train loss:  3.51194269e-03, bound:  3.15337420e-01\n",
      "Epoch: 34465 mean train loss:  3.51187610e-03, bound:  3.15337420e-01\n",
      "Epoch: 34466 mean train loss:  3.51176481e-03, bound:  3.15337420e-01\n",
      "Epoch: 34467 mean train loss:  3.51176108e-03, bound:  3.15337420e-01\n",
      "Epoch: 34468 mean train loss:  3.51165771e-03, bound:  3.15337420e-01\n",
      "Epoch: 34469 mean train loss:  3.51156690e-03, bound:  3.15337390e-01\n",
      "Epoch: 34470 mean train loss:  3.51152895e-03, bound:  3.15337390e-01\n",
      "Epoch: 34471 mean train loss:  3.51147214e-03, bound:  3.15337390e-01\n",
      "Epoch: 34472 mean train loss:  3.51140113e-03, bound:  3.15337330e-01\n",
      "Epoch: 34473 mean train loss:  3.51134478e-03, bound:  3.15337330e-01\n",
      "Epoch: 34474 mean train loss:  3.51128308e-03, bound:  3.15337330e-01\n",
      "Epoch: 34475 mean train loss:  3.51125863e-03, bound:  3.15337330e-01\n",
      "Epoch: 34476 mean train loss:  3.51115735e-03, bound:  3.15337330e-01\n",
      "Epoch: 34477 mean train loss:  3.51114757e-03, bound:  3.15337330e-01\n",
      "Epoch: 34478 mean train loss:  3.51105444e-03, bound:  3.15337330e-01\n",
      "Epoch: 34479 mean train loss:  3.51100834e-03, bound:  3.15337300e-01\n",
      "Epoch: 34480 mean train loss:  3.51093453e-03, bound:  3.15337300e-01\n",
      "Epoch: 34481 mean train loss:  3.51083768e-03, bound:  3.15337300e-01\n",
      "Epoch: 34482 mean train loss:  3.51074222e-03, bound:  3.15337300e-01\n",
      "Epoch: 34483 mean train loss:  3.51072266e-03, bound:  3.15337300e-01\n",
      "Epoch: 34484 mean train loss:  3.51063092e-03, bound:  3.15337300e-01\n",
      "Epoch: 34485 mean train loss:  3.51054640e-03, bound:  3.15337270e-01\n",
      "Epoch: 34486 mean train loss:  3.51043558e-03, bound:  3.15337270e-01\n",
      "Epoch: 34487 mean train loss:  3.51038459e-03, bound:  3.15337270e-01\n",
      "Epoch: 34488 mean train loss:  3.51028098e-03, bound:  3.15337270e-01\n",
      "Epoch: 34489 mean train loss:  3.51028470e-03, bound:  3.15337270e-01\n",
      "Epoch: 34490 mean train loss:  3.51012684e-03, bound:  3.15337211e-01\n",
      "Epoch: 34491 mean train loss:  3.51011474e-03, bound:  3.15337211e-01\n",
      "Epoch: 34492 mean train loss:  3.51002207e-03, bound:  3.15337211e-01\n",
      "Epoch: 34493 mean train loss:  3.50995455e-03, bound:  3.15337211e-01\n",
      "Epoch: 34494 mean train loss:  3.50988796e-03, bound:  3.15337211e-01\n",
      "Epoch: 34495 mean train loss:  3.50982626e-03, bound:  3.15337211e-01\n",
      "Epoch: 34496 mean train loss:  3.50978412e-03, bound:  3.15337181e-01\n",
      "Epoch: 34497 mean train loss:  3.50971846e-03, bound:  3.15337181e-01\n",
      "Epoch: 34498 mean train loss:  3.50964209e-03, bound:  3.15337181e-01\n",
      "Epoch: 34499 mean train loss:  3.50961322e-03, bound:  3.15337181e-01\n",
      "Epoch: 34500 mean train loss:  3.50950775e-03, bound:  3.15337121e-01\n",
      "Epoch: 34501 mean train loss:  3.50943394e-03, bound:  3.15337181e-01\n",
      "Epoch: 34502 mean train loss:  3.50936106e-03, bound:  3.15337121e-01\n",
      "Epoch: 34503 mean train loss:  3.50928749e-03, bound:  3.15337121e-01\n",
      "Epoch: 34504 mean train loss:  3.50925140e-03, bound:  3.15337121e-01\n",
      "Epoch: 34505 mean train loss:  3.50912986e-03, bound:  3.15337121e-01\n",
      "Epoch: 34506 mean train loss:  3.50910076e-03, bound:  3.15337092e-01\n",
      "Epoch: 34507 mean train loss:  3.50903557e-03, bound:  3.15337092e-01\n",
      "Epoch: 34508 mean train loss:  3.50895966e-03, bound:  3.15337092e-01\n",
      "Epoch: 34509 mean train loss:  3.50890378e-03, bound:  3.15337092e-01\n",
      "Epoch: 34510 mean train loss:  3.50883440e-03, bound:  3.15337092e-01\n",
      "Epoch: 34511 mean train loss:  3.50879086e-03, bound:  3.15337062e-01\n",
      "Epoch: 34512 mean train loss:  3.50871333e-03, bound:  3.15337062e-01\n",
      "Epoch: 34513 mean train loss:  3.50860716e-03, bound:  3.15337062e-01\n",
      "Epoch: 34514 mean train loss:  3.50857875e-03, bound:  3.15337062e-01\n",
      "Epoch: 34515 mean train loss:  3.50851752e-03, bound:  3.15337062e-01\n",
      "Epoch: 34516 mean train loss:  3.50844208e-03, bound:  3.15337062e-01\n",
      "Epoch: 34517 mean train loss:  3.50836734e-03, bound:  3.15337062e-01\n",
      "Epoch: 34518 mean train loss:  3.50829889e-03, bound:  3.15337002e-01\n",
      "Epoch: 34519 mean train loss:  3.50820739e-03, bound:  3.15337002e-01\n",
      "Epoch: 34520 mean train loss:  3.50819412e-03, bound:  3.15337002e-01\n",
      "Epoch: 34521 mean train loss:  3.50812636e-03, bound:  3.15337002e-01\n",
      "Epoch: 34522 mean train loss:  3.50803137e-03, bound:  3.15337002e-01\n",
      "Epoch: 34523 mean train loss:  3.50794708e-03, bound:  3.15337002e-01\n",
      "Epoch: 34524 mean train loss:  3.50793218e-03, bound:  3.15336972e-01\n",
      "Epoch: 34525 mean train loss:  3.50784347e-03, bound:  3.15336972e-01\n",
      "Epoch: 34526 mean train loss:  3.50774312e-03, bound:  3.15336972e-01\n",
      "Epoch: 34527 mean train loss:  3.50769283e-03, bound:  3.15336972e-01\n",
      "Epoch: 34528 mean train loss:  3.50763532e-03, bound:  3.15336972e-01\n",
      "Epoch: 34529 mean train loss:  3.50760878e-03, bound:  3.15336943e-01\n",
      "Epoch: 34530 mean train loss:  3.50747630e-03, bound:  3.15336943e-01\n",
      "Epoch: 34531 mean train loss:  3.50744720e-03, bound:  3.15336943e-01\n",
      "Epoch: 34532 mean train loss:  3.50740110e-03, bound:  3.15336943e-01\n",
      "Epoch: 34533 mean train loss:  3.50728934e-03, bound:  3.15336943e-01\n",
      "Epoch: 34534 mean train loss:  3.50718549e-03, bound:  3.15336943e-01\n",
      "Epoch: 34535 mean train loss:  3.50718130e-03, bound:  3.15336943e-01\n",
      "Epoch: 34536 mean train loss:  3.50707583e-03, bound:  3.15336943e-01\n",
      "Epoch: 34537 mean train loss:  3.50702880e-03, bound:  3.15336883e-01\n",
      "Epoch: 34538 mean train loss:  3.50693474e-03, bound:  3.15336883e-01\n",
      "Epoch: 34539 mean train loss:  3.50686046e-03, bound:  3.15336883e-01\n",
      "Epoch: 34540 mean train loss:  3.50679806e-03, bound:  3.15336853e-01\n",
      "Epoch: 34541 mean train loss:  3.50674265e-03, bound:  3.15336853e-01\n",
      "Epoch: 34542 mean train loss:  3.50666186e-03, bound:  3.15336853e-01\n",
      "Epoch: 34543 mean train loss:  3.50656244e-03, bound:  3.15336853e-01\n",
      "Epoch: 34544 mean train loss:  3.50658898e-03, bound:  3.15336853e-01\n",
      "Epoch: 34545 mean train loss:  3.50645790e-03, bound:  3.15336823e-01\n",
      "Epoch: 34546 mean train loss:  3.50643112e-03, bound:  3.15336823e-01\n",
      "Epoch: 34547 mean train loss:  3.50635196e-03, bound:  3.15336823e-01\n",
      "Epoch: 34548 mean train loss:  3.50626465e-03, bound:  3.15336823e-01\n",
      "Epoch: 34549 mean train loss:  3.50619946e-03, bound:  3.15336823e-01\n",
      "Epoch: 34550 mean train loss:  3.50617757e-03, bound:  3.15336823e-01\n",
      "Epoch: 34551 mean train loss:  3.50613892e-03, bound:  3.15336764e-01\n",
      "Epoch: 34552 mean train loss:  3.50606954e-03, bound:  3.15336764e-01\n",
      "Epoch: 34553 mean train loss:  3.50599946e-03, bound:  3.15336764e-01\n",
      "Epoch: 34554 mean train loss:  3.50592728e-03, bound:  3.15336764e-01\n",
      "Epoch: 34555 mean train loss:  3.50591959e-03, bound:  3.15336764e-01\n",
      "Epoch: 34556 mean train loss:  3.50590679e-03, bound:  3.15336734e-01\n",
      "Epoch: 34557 mean train loss:  3.50588071e-03, bound:  3.15336734e-01\n",
      "Epoch: 34558 mean train loss:  3.50575009e-03, bound:  3.15336734e-01\n",
      "Epoch: 34559 mean train loss:  3.50564020e-03, bound:  3.15336734e-01\n",
      "Epoch: 34560 mean train loss:  3.50556662e-03, bound:  3.15336734e-01\n",
      "Epoch: 34561 mean train loss:  3.50545999e-03, bound:  3.15336734e-01\n",
      "Epoch: 34562 mean train loss:  3.50535382e-03, bound:  3.15336734e-01\n",
      "Epoch: 34563 mean train loss:  3.50523787e-03, bound:  3.15336734e-01\n",
      "Epoch: 34564 mean train loss:  3.50523763e-03, bound:  3.15336734e-01\n",
      "Epoch: 34565 mean train loss:  3.50512611e-03, bound:  3.15336734e-01\n",
      "Epoch: 34566 mean train loss:  3.50509677e-03, bound:  3.15336734e-01\n",
      "Epoch: 34567 mean train loss:  3.50500224e-03, bound:  3.15336645e-01\n",
      "Epoch: 34568 mean train loss:  3.50494776e-03, bound:  3.15336645e-01\n",
      "Epoch: 34569 mean train loss:  3.50486650e-03, bound:  3.15336645e-01\n",
      "Epoch: 34570 mean train loss:  3.50479363e-03, bound:  3.15336645e-01\n",
      "Epoch: 34571 mean train loss:  3.50474496e-03, bound:  3.15336645e-01\n",
      "Epoch: 34572 mean train loss:  3.50471027e-03, bound:  3.15336645e-01\n",
      "Epoch: 34573 mean train loss:  3.50461737e-03, bound:  3.15336645e-01\n",
      "Epoch: 34574 mean train loss:  3.50454333e-03, bound:  3.15336645e-01\n",
      "Epoch: 34575 mean train loss:  3.50445998e-03, bound:  3.15336645e-01\n",
      "Epoch: 34576 mean train loss:  3.50438338e-03, bound:  3.15336645e-01\n",
      "Epoch: 34577 mean train loss:  3.50433425e-03, bound:  3.15336615e-01\n",
      "Epoch: 34578 mean train loss:  3.50428885e-03, bound:  3.15336615e-01\n",
      "Epoch: 34579 mean train loss:  3.50423623e-03, bound:  3.15336615e-01\n",
      "Epoch: 34580 mean train loss:  3.50413308e-03, bound:  3.15336615e-01\n",
      "Epoch: 34581 mean train loss:  3.50412121e-03, bound:  3.15336615e-01\n",
      "Epoch: 34582 mean train loss:  3.50401970e-03, bound:  3.15336615e-01\n",
      "Epoch: 34583 mean train loss:  3.50393448e-03, bound:  3.15336525e-01\n",
      "Epoch: 34584 mean train loss:  3.50388489e-03, bound:  3.15336525e-01\n",
      "Epoch: 34585 mean train loss:  3.50380992e-03, bound:  3.15336525e-01\n",
      "Epoch: 34586 mean train loss:  3.50379595e-03, bound:  3.15336525e-01\n",
      "Epoch: 34587 mean train loss:  3.50368791e-03, bound:  3.15336525e-01\n",
      "Epoch: 34588 mean train loss:  3.50364787e-03, bound:  3.15336525e-01\n",
      "Epoch: 34589 mean train loss:  3.50360223e-03, bound:  3.15336525e-01\n",
      "Epoch: 34590 mean train loss:  3.50350491e-03, bound:  3.15336525e-01\n",
      "Epoch: 34591 mean train loss:  3.50350700e-03, bound:  3.15336525e-01\n",
      "Epoch: 34592 mean train loss:  3.50338290e-03, bound:  3.15336525e-01\n",
      "Epoch: 34593 mean train loss:  3.50331794e-03, bound:  3.15336496e-01\n",
      "Epoch: 34594 mean train loss:  3.50321410e-03, bound:  3.15336496e-01\n",
      "Epoch: 34595 mean train loss:  3.50320153e-03, bound:  3.15336496e-01\n",
      "Epoch: 34596 mean train loss:  3.50306532e-03, bound:  3.15336496e-01\n",
      "Epoch: 34597 mean train loss:  3.50302062e-03, bound:  3.15336496e-01\n",
      "Epoch: 34598 mean train loss:  3.50293610e-03, bound:  3.15336496e-01\n",
      "Epoch: 34599 mean train loss:  3.50288325e-03, bound:  3.15336436e-01\n",
      "Epoch: 34600 mean train loss:  3.50280432e-03, bound:  3.15336406e-01\n",
      "Epoch: 34601 mean train loss:  3.50271002e-03, bound:  3.15336406e-01\n",
      "Epoch: 34602 mean train loss:  3.50268465e-03, bound:  3.15336406e-01\n",
      "Epoch: 34603 mean train loss:  3.50265112e-03, bound:  3.15336406e-01\n",
      "Epoch: 34604 mean train loss:  3.50255123e-03, bound:  3.15336406e-01\n",
      "Epoch: 34605 mean train loss:  3.50250117e-03, bound:  3.15336406e-01\n",
      "Epoch: 34606 mean train loss:  3.50236590e-03, bound:  3.15336406e-01\n",
      "Epoch: 34607 mean train loss:  3.50235333e-03, bound:  3.15336406e-01\n",
      "Epoch: 34608 mean train loss:  3.50227533e-03, bound:  3.15336406e-01\n",
      "Epoch: 34609 mean train loss:  3.50222760e-03, bound:  3.15336406e-01\n",
      "Epoch: 34610 mean train loss:  3.50213354e-03, bound:  3.15336406e-01\n",
      "Epoch: 34611 mean train loss:  3.50209512e-03, bound:  3.15336376e-01\n",
      "Epoch: 34612 mean train loss:  3.50198802e-03, bound:  3.15336376e-01\n",
      "Epoch: 34613 mean train loss:  3.50197498e-03, bound:  3.15336376e-01\n",
      "Epoch: 34614 mean train loss:  3.50192864e-03, bound:  3.15336376e-01\n",
      "Epoch: 34615 mean train loss:  3.50181153e-03, bound:  3.15336376e-01\n",
      "Epoch: 34616 mean train loss:  3.50171723e-03, bound:  3.15336317e-01\n",
      "Epoch: 34617 mean train loss:  3.50168068e-03, bound:  3.15336317e-01\n",
      "Epoch: 34618 mean train loss:  3.50163132e-03, bound:  3.15336317e-01\n",
      "Epoch: 34619 mean train loss:  3.50159826e-03, bound:  3.15336317e-01\n",
      "Epoch: 34620 mean train loss:  3.50144226e-03, bound:  3.15336317e-01\n",
      "Epoch: 34621 mean train loss:  3.50142713e-03, bound:  3.15336317e-01\n",
      "Epoch: 34622 mean train loss:  3.50133399e-03, bound:  3.15336287e-01\n",
      "Epoch: 34623 mean train loss:  3.50133772e-03, bound:  3.15336287e-01\n",
      "Epoch: 34624 mean train loss:  3.50120571e-03, bound:  3.15336287e-01\n",
      "Epoch: 34625 mean train loss:  3.50117963e-03, bound:  3.15336287e-01\n",
      "Epoch: 34626 mean train loss:  3.50106158e-03, bound:  3.15336257e-01\n",
      "Epoch: 34627 mean train loss:  3.50105157e-03, bound:  3.15336257e-01\n",
      "Epoch: 34628 mean train loss:  3.50093376e-03, bound:  3.15336257e-01\n",
      "Epoch: 34629 mean train loss:  3.50089488e-03, bound:  3.15336257e-01\n",
      "Epoch: 34630 mean train loss:  3.50082968e-03, bound:  3.15336257e-01\n",
      "Epoch: 34631 mean train loss:  3.50078428e-03, bound:  3.15336257e-01\n",
      "Epoch: 34632 mean train loss:  3.50067648e-03, bound:  3.15336257e-01\n",
      "Epoch: 34633 mean train loss:  3.50068766e-03, bound:  3.15336198e-01\n",
      "Epoch: 34634 mean train loss:  3.50056705e-03, bound:  3.15336198e-01\n",
      "Epoch: 34635 mean train loss:  3.50051746e-03, bound:  3.15336198e-01\n",
      "Epoch: 34636 mean train loss:  3.50046111e-03, bound:  3.15336198e-01\n",
      "Epoch: 34637 mean train loss:  3.50038847e-03, bound:  3.15336198e-01\n",
      "Epoch: 34638 mean train loss:  3.50030069e-03, bound:  3.15336168e-01\n",
      "Epoch: 34639 mean train loss:  3.50024248e-03, bound:  3.15336168e-01\n",
      "Epoch: 34640 mean train loss:  3.50012700e-03, bound:  3.15336168e-01\n",
      "Epoch: 34641 mean train loss:  3.50011722e-03, bound:  3.15336168e-01\n",
      "Epoch: 34642 mean train loss:  3.50004970e-03, bound:  3.15336138e-01\n",
      "Epoch: 34643 mean train loss:  3.49998032e-03, bound:  3.15336138e-01\n",
      "Epoch: 34644 mean train loss:  3.49986483e-03, bound:  3.15336138e-01\n",
      "Epoch: 34645 mean train loss:  3.49986344e-03, bound:  3.15336138e-01\n",
      "Epoch: 34646 mean train loss:  3.49976844e-03, bound:  3.15336138e-01\n",
      "Epoch: 34647 mean train loss:  3.49967345e-03, bound:  3.15336108e-01\n",
      "Epoch: 34648 mean train loss:  3.49969650e-03, bound:  3.15336108e-01\n",
      "Epoch: 34649 mean train loss:  3.49957752e-03, bound:  3.15336108e-01\n",
      "Epoch: 34650 mean train loss:  3.49951419e-03, bound:  3.15336078e-01\n",
      "Epoch: 34651 mean train loss:  3.49941058e-03, bound:  3.15336078e-01\n",
      "Epoch: 34652 mean train loss:  3.49933235e-03, bound:  3.15336078e-01\n",
      "Epoch: 34653 mean train loss:  3.49931186e-03, bound:  3.15336078e-01\n",
      "Epoch: 34654 mean train loss:  3.49920942e-03, bound:  3.15336078e-01\n",
      "Epoch: 34655 mean train loss:  3.49917123e-03, bound:  3.15336049e-01\n",
      "Epoch: 34656 mean train loss:  3.49913957e-03, bound:  3.15336049e-01\n",
      "Epoch: 34657 mean train loss:  3.49904178e-03, bound:  3.15336049e-01\n",
      "Epoch: 34658 mean train loss:  3.49894399e-03, bound:  3.15336049e-01\n",
      "Epoch: 34659 mean train loss:  3.49890534e-03, bound:  3.15336049e-01\n",
      "Epoch: 34660 mean train loss:  3.49885202e-03, bound:  3.15336049e-01\n",
      "Epoch: 34661 mean train loss:  3.49876727e-03, bound:  3.15336049e-01\n",
      "Epoch: 34662 mean train loss:  3.49869509e-03, bound:  3.15336049e-01\n",
      "Epoch: 34663 mean train loss:  3.49859148e-03, bound:  3.15336049e-01\n",
      "Epoch: 34664 mean train loss:  3.49859148e-03, bound:  3.15336049e-01\n",
      "Epoch: 34665 mean train loss:  3.49852280e-03, bound:  3.15336049e-01\n",
      "Epoch: 34666 mean train loss:  3.49843572e-03, bound:  3.15335989e-01\n",
      "Epoch: 34667 mean train loss:  3.49836005e-03, bound:  3.15335959e-01\n",
      "Epoch: 34668 mean train loss:  3.49827646e-03, bound:  3.15335959e-01\n",
      "Epoch: 34669 mean train loss:  3.49823828e-03, bound:  3.15335959e-01\n",
      "Epoch: 34670 mean train loss:  3.49820964e-03, bound:  3.15335959e-01\n",
      "Epoch: 34671 mean train loss:  3.49811977e-03, bound:  3.15335929e-01\n",
      "Epoch: 34672 mean train loss:  3.49808205e-03, bound:  3.15335929e-01\n",
      "Epoch: 34673 mean train loss:  3.49798892e-03, bound:  3.15335929e-01\n",
      "Epoch: 34674 mean train loss:  3.49793537e-03, bound:  3.15335929e-01\n",
      "Epoch: 34675 mean train loss:  3.49782011e-03, bound:  3.15335929e-01\n",
      "Epoch: 34676 mean train loss:  3.49772465e-03, bound:  3.15335929e-01\n",
      "Epoch: 34677 mean train loss:  3.49770952e-03, bound:  3.15335929e-01\n",
      "Epoch: 34678 mean train loss:  3.49761685e-03, bound:  3.15335929e-01\n",
      "Epoch: 34679 mean train loss:  3.49750579e-03, bound:  3.15335929e-01\n",
      "Epoch: 34680 mean train loss:  3.49752698e-03, bound:  3.15335870e-01\n",
      "Epoch: 34681 mean train loss:  3.49744875e-03, bound:  3.15335840e-01\n",
      "Epoch: 34682 mean train loss:  3.49741406e-03, bound:  3.15335840e-01\n",
      "Epoch: 34683 mean train loss:  3.49732791e-03, bound:  3.15335840e-01\n",
      "Epoch: 34684 mean train loss:  3.49726714e-03, bound:  3.15335840e-01\n",
      "Epoch: 34685 mean train loss:  3.49715981e-03, bound:  3.15335840e-01\n",
      "Epoch: 34686 mean train loss:  3.49707250e-03, bound:  3.15335840e-01\n",
      "Epoch: 34687 mean train loss:  3.49704642e-03, bound:  3.15335840e-01\n",
      "Epoch: 34688 mean train loss:  3.49695981e-03, bound:  3.15335840e-01\n",
      "Epoch: 34689 mean train loss:  3.49689112e-03, bound:  3.15335840e-01\n",
      "Epoch: 34690 mean train loss:  3.49679333e-03, bound:  3.15335840e-01\n",
      "Epoch: 34691 mean train loss:  3.49677051e-03, bound:  3.15335840e-01\n",
      "Epoch: 34692 mean train loss:  3.49668157e-03, bound:  3.15335810e-01\n",
      "Epoch: 34693 mean train loss:  3.49661172e-03, bound:  3.15335810e-01\n",
      "Epoch: 34694 mean train loss:  3.49656772e-03, bound:  3.15335810e-01\n",
      "Epoch: 34695 mean train loss:  3.49649135e-03, bound:  3.15335810e-01\n",
      "Epoch: 34696 mean train loss:  3.49637703e-03, bound:  3.15335810e-01\n",
      "Epoch: 34697 mean train loss:  3.49639729e-03, bound:  3.15335810e-01\n",
      "Epoch: 34698 mean train loss:  3.49631486e-03, bound:  3.15335810e-01\n",
      "Epoch: 34699 mean train loss:  3.49624548e-03, bound:  3.15335751e-01\n",
      "Epoch: 34700 mean train loss:  3.49618052e-03, bound:  3.15335751e-01\n",
      "Epoch: 34701 mean train loss:  3.49615281e-03, bound:  3.15335751e-01\n",
      "Epoch: 34702 mean train loss:  3.49605526e-03, bound:  3.15335751e-01\n",
      "Epoch: 34703 mean train loss:  3.49597004e-03, bound:  3.15335721e-01\n",
      "Epoch: 34704 mean train loss:  3.49591300e-03, bound:  3.15335721e-01\n",
      "Epoch: 34705 mean train loss:  3.49581405e-03, bound:  3.15335721e-01\n",
      "Epoch: 34706 mean train loss:  3.49577237e-03, bound:  3.15335721e-01\n",
      "Epoch: 34707 mean train loss:  3.49576306e-03, bound:  3.15335721e-01\n",
      "Epoch: 34708 mean train loss:  3.49565828e-03, bound:  3.15335721e-01\n",
      "Epoch: 34709 mean train loss:  3.49557749e-03, bound:  3.15335691e-01\n",
      "Epoch: 34710 mean train loss:  3.49552580e-03, bound:  3.15335661e-01\n",
      "Epoch: 34711 mean train loss:  3.49541102e-03, bound:  3.15335661e-01\n",
      "Epoch: 34712 mean train loss:  3.49534163e-03, bound:  3.15335661e-01\n",
      "Epoch: 34713 mean train loss:  3.49525083e-03, bound:  3.15335661e-01\n",
      "Epoch: 34714 mean train loss:  3.49527132e-03, bound:  3.15335631e-01\n",
      "Epoch: 34715 mean train loss:  3.49514070e-03, bound:  3.15335631e-01\n",
      "Epoch: 34716 mean train loss:  3.49510671e-03, bound:  3.15335631e-01\n",
      "Epoch: 34717 mean train loss:  3.49501357e-03, bound:  3.15335631e-01\n",
      "Epoch: 34718 mean train loss:  3.49496864e-03, bound:  3.15335631e-01\n",
      "Epoch: 34719 mean train loss:  3.49489739e-03, bound:  3.15335631e-01\n",
      "Epoch: 34720 mean train loss:  3.49482894e-03, bound:  3.15335631e-01\n",
      "Epoch: 34721 mean train loss:  3.49476375e-03, bound:  3.15335602e-01\n",
      "Epoch: 34722 mean train loss:  3.49471928e-03, bound:  3.15335602e-01\n",
      "Epoch: 34723 mean train loss:  3.49461776e-03, bound:  3.15335602e-01\n",
      "Epoch: 34724 mean train loss:  3.49460123e-03, bound:  3.15335602e-01\n",
      "Epoch: 34725 mean train loss:  3.49448598e-03, bound:  3.15335602e-01\n",
      "Epoch: 34726 mean train loss:  3.49440519e-03, bound:  3.15335542e-01\n",
      "Epoch: 34727 mean train loss:  3.49437073e-03, bound:  3.15335542e-01\n",
      "Epoch: 34728 mean train loss:  3.49429669e-03, bound:  3.15335542e-01\n",
      "Epoch: 34729 mean train loss:  3.49424779e-03, bound:  3.15335542e-01\n",
      "Epoch: 34730 mean train loss:  3.49419168e-03, bound:  3.15335542e-01\n",
      "Epoch: 34731 mean train loss:  3.49413836e-03, bound:  3.15335542e-01\n",
      "Epoch: 34732 mean train loss:  3.49409902e-03, bound:  3.15335512e-01\n",
      "Epoch: 34733 mean train loss:  3.49403662e-03, bound:  3.15335512e-01\n",
      "Epoch: 34734 mean train loss:  3.49395210e-03, bound:  3.15335512e-01\n",
      "Epoch: 34735 mean train loss:  3.49390926e-03, bound:  3.15335512e-01\n",
      "Epoch: 34736 mean train loss:  3.49380518e-03, bound:  3.15335482e-01\n",
      "Epoch: 34737 mean train loss:  3.49377072e-03, bound:  3.15335482e-01\n",
      "Epoch: 34738 mean train loss:  3.49369785e-03, bound:  3.15335482e-01\n",
      "Epoch: 34739 mean train loss:  3.49361938e-03, bound:  3.15335482e-01\n",
      "Epoch: 34740 mean train loss:  3.49356583e-03, bound:  3.15335482e-01\n",
      "Epoch: 34741 mean train loss:  3.49345943e-03, bound:  3.15335482e-01\n",
      "Epoch: 34742 mean train loss:  3.49342637e-03, bound:  3.15335423e-01\n",
      "Epoch: 34743 mean train loss:  3.49333230e-03, bound:  3.15335423e-01\n",
      "Epoch: 34744 mean train loss:  3.49326781e-03, bound:  3.15335423e-01\n",
      "Epoch: 34745 mean train loss:  3.49319726e-03, bound:  3.15335423e-01\n",
      "Epoch: 34746 mean train loss:  3.49311833e-03, bound:  3.15335423e-01\n",
      "Epoch: 34747 mean train loss:  3.49301239e-03, bound:  3.15335423e-01\n",
      "Epoch: 34748 mean train loss:  3.49293835e-03, bound:  3.15335423e-01\n",
      "Epoch: 34749 mean train loss:  3.49293975e-03, bound:  3.15335393e-01\n",
      "Epoch: 34750 mean train loss:  3.49287060e-03, bound:  3.15335393e-01\n",
      "Epoch: 34751 mean train loss:  3.49281891e-03, bound:  3.15335393e-01\n",
      "Epoch: 34752 mean train loss:  3.49273183e-03, bound:  3.15335393e-01\n",
      "Epoch: 34753 mean train loss:  3.49266641e-03, bound:  3.15335363e-01\n",
      "Epoch: 34754 mean train loss:  3.49257863e-03, bound:  3.15335363e-01\n",
      "Epoch: 34755 mean train loss:  3.49249947e-03, bound:  3.15335363e-01\n",
      "Epoch: 34756 mean train loss:  3.49244801e-03, bound:  3.15335363e-01\n",
      "Epoch: 34757 mean train loss:  3.49235022e-03, bound:  3.15335363e-01\n",
      "Epoch: 34758 mean train loss:  3.49234184e-03, bound:  3.15335304e-01\n",
      "Epoch: 34759 mean train loss:  3.49226408e-03, bound:  3.15335304e-01\n",
      "Epoch: 34760 mean train loss:  3.49216629e-03, bound:  3.15335304e-01\n",
      "Epoch: 34761 mean train loss:  3.49208177e-03, bound:  3.15335304e-01\n",
      "Epoch: 34762 mean train loss:  3.49201821e-03, bound:  3.15335304e-01\n",
      "Epoch: 34763 mean train loss:  3.49197816e-03, bound:  3.15335304e-01\n",
      "Epoch: 34764 mean train loss:  3.49192973e-03, bound:  3.15335304e-01\n",
      "Epoch: 34765 mean train loss:  3.49183194e-03, bound:  3.15335274e-01\n",
      "Epoch: 34766 mean train loss:  3.49178235e-03, bound:  3.15335274e-01\n",
      "Epoch: 34767 mean train loss:  3.49174696e-03, bound:  3.15335274e-01\n",
      "Epoch: 34768 mean train loss:  3.49168153e-03, bound:  3.15335274e-01\n",
      "Epoch: 34769 mean train loss:  3.49159515e-03, bound:  3.15335274e-01\n",
      "Epoch: 34770 mean train loss:  3.49146035e-03, bound:  3.15335244e-01\n",
      "Epoch: 34771 mean train loss:  3.49148829e-03, bound:  3.15335244e-01\n",
      "Epoch: 34772 mean train loss:  3.49140889e-03, bound:  3.15335244e-01\n",
      "Epoch: 34773 mean train loss:  3.49130225e-03, bound:  3.15335244e-01\n",
      "Epoch: 34774 mean train loss:  3.49125802e-03, bound:  3.15335244e-01\n",
      "Epoch: 34775 mean train loss:  3.49119608e-03, bound:  3.15335214e-01\n",
      "Epoch: 34776 mean train loss:  3.49111250e-03, bound:  3.15335214e-01\n",
      "Epoch: 34777 mean train loss:  3.49104428e-03, bound:  3.15335214e-01\n",
      "Epoch: 34778 mean train loss:  3.49099841e-03, bound:  3.15335214e-01\n",
      "Epoch: 34779 mean train loss:  3.49093834e-03, bound:  3.15335214e-01\n",
      "Epoch: 34780 mean train loss:  3.49085801e-03, bound:  3.15335184e-01\n",
      "Epoch: 34781 mean train loss:  3.49075813e-03, bound:  3.15335155e-01\n",
      "Epoch: 34782 mean train loss:  3.49069783e-03, bound:  3.15335155e-01\n",
      "Epoch: 34783 mean train loss:  3.49062961e-03, bound:  3.15335155e-01\n",
      "Epoch: 34784 mean train loss:  3.49057489e-03, bound:  3.15335155e-01\n",
      "Epoch: 34785 mean train loss:  3.49048944e-03, bound:  3.15335155e-01\n",
      "Epoch: 34786 mean train loss:  3.49044078e-03, bound:  3.15335155e-01\n",
      "Epoch: 34787 mean train loss:  3.49038350e-03, bound:  3.15335125e-01\n",
      "Epoch: 34788 mean train loss:  3.49031389e-03, bound:  3.15335125e-01\n",
      "Epoch: 34789 mean train loss:  3.49023473e-03, bound:  3.15335125e-01\n",
      "Epoch: 34790 mean train loss:  3.49022308e-03, bound:  3.15335125e-01\n",
      "Epoch: 34791 mean train loss:  3.49013298e-03, bound:  3.15335125e-01\n",
      "Epoch: 34792 mean train loss:  3.49007268e-03, bound:  3.15335125e-01\n",
      "Epoch: 34793 mean train loss:  3.48997233e-03, bound:  3.15335095e-01\n",
      "Epoch: 34794 mean train loss:  3.48994625e-03, bound:  3.15335095e-01\n",
      "Epoch: 34795 mean train loss:  3.48987337e-03, bound:  3.15335095e-01\n",
      "Epoch: 34796 mean train loss:  3.48975742e-03, bound:  3.15335095e-01\n",
      "Epoch: 34797 mean train loss:  3.48974904e-03, bound:  3.15335065e-01\n",
      "Epoch: 34798 mean train loss:  3.48967384e-03, bound:  3.15335065e-01\n",
      "Epoch: 34799 mean train loss:  3.48963519e-03, bound:  3.15335065e-01\n",
      "Epoch: 34800 mean train loss:  3.48957558e-03, bound:  3.15335065e-01\n",
      "Epoch: 34801 mean train loss:  3.48946149e-03, bound:  3.15335035e-01\n",
      "Epoch: 34802 mean train loss:  3.48940911e-03, bound:  3.15335035e-01\n",
      "Epoch: 34803 mean train loss:  3.48931621e-03, bound:  3.15335035e-01\n",
      "Epoch: 34804 mean train loss:  3.48925148e-03, bound:  3.15335035e-01\n",
      "Epoch: 34805 mean train loss:  3.48919421e-03, bound:  3.15335035e-01\n",
      "Epoch: 34806 mean train loss:  3.48911411e-03, bound:  3.15335035e-01\n",
      "Epoch: 34807 mean train loss:  3.48907849e-03, bound:  3.15335006e-01\n",
      "Epoch: 34808 mean train loss:  3.48899141e-03, bound:  3.15335006e-01\n",
      "Epoch: 34809 mean train loss:  3.48894135e-03, bound:  3.15334976e-01\n",
      "Epoch: 34810 mean train loss:  3.48888151e-03, bound:  3.15334976e-01\n",
      "Epoch: 34811 mean train loss:  3.48882284e-03, bound:  3.15334976e-01\n",
      "Epoch: 34812 mean train loss:  3.48876440e-03, bound:  3.15334976e-01\n",
      "Epoch: 34813 mean train loss:  3.48877395e-03, bound:  3.15334946e-01\n",
      "Epoch: 34814 mean train loss:  3.48869106e-03, bound:  3.15334946e-01\n",
      "Epoch: 34815 mean train loss:  3.48857790e-03, bound:  3.15334946e-01\n",
      "Epoch: 34816 mean train loss:  3.48853413e-03, bound:  3.15334946e-01\n",
      "Epoch: 34817 mean train loss:  3.48845846e-03, bound:  3.15334946e-01\n",
      "Epoch: 34818 mean train loss:  3.48840072e-03, bound:  3.15334946e-01\n",
      "Epoch: 34819 mean train loss:  3.48834880e-03, bound:  3.15334916e-01\n",
      "Epoch: 34820 mean train loss:  3.48828104e-03, bound:  3.15334916e-01\n",
      "Epoch: 34821 mean train loss:  3.48818838e-03, bound:  3.15334916e-01\n",
      "Epoch: 34822 mean train loss:  3.48814833e-03, bound:  3.15334916e-01\n",
      "Epoch: 34823 mean train loss:  3.48808733e-03, bound:  3.15334916e-01\n",
      "Epoch: 34824 mean train loss:  3.48799629e-03, bound:  3.15334916e-01\n",
      "Epoch: 34825 mean train loss:  3.48791457e-03, bound:  3.15334886e-01\n",
      "Epoch: 34826 mean train loss:  3.48784029e-03, bound:  3.15334857e-01\n",
      "Epoch: 34827 mean train loss:  3.48778069e-03, bound:  3.15334857e-01\n",
      "Epoch: 34828 mean train loss:  3.48773273e-03, bound:  3.15334857e-01\n",
      "Epoch: 34829 mean train loss:  3.48756625e-03, bound:  3.15334827e-01\n",
      "Epoch: 34830 mean train loss:  3.48755624e-03, bound:  3.15334827e-01\n",
      "Epoch: 34831 mean train loss:  3.48748500e-03, bound:  3.15334827e-01\n",
      "Epoch: 34832 mean train loss:  3.48740909e-03, bound:  3.15334827e-01\n",
      "Epoch: 34833 mean train loss:  3.48740024e-03, bound:  3.15334827e-01\n",
      "Epoch: 34834 mean train loss:  3.48732946e-03, bound:  3.15334827e-01\n",
      "Epoch: 34835 mean train loss:  3.48724937e-03, bound:  3.15334827e-01\n",
      "Epoch: 34836 mean train loss:  3.48714460e-03, bound:  3.15334797e-01\n",
      "Epoch: 34837 mean train loss:  3.48710641e-03, bound:  3.15334797e-01\n",
      "Epoch: 34838 mean train loss:  3.48702422e-03, bound:  3.15334797e-01\n",
      "Epoch: 34839 mean train loss:  3.48698790e-03, bound:  3.15334797e-01\n",
      "Epoch: 34840 mean train loss:  3.48688685e-03, bound:  3.15334797e-01\n",
      "Epoch: 34841 mean train loss:  3.48682492e-03, bound:  3.15334797e-01\n",
      "Epoch: 34842 mean train loss:  3.48672806e-03, bound:  3.15334737e-01\n",
      "Epoch: 34843 mean train loss:  3.48675507e-03, bound:  3.15334737e-01\n",
      "Epoch: 34844 mean train loss:  3.48665100e-03, bound:  3.15334737e-01\n",
      "Epoch: 34845 mean train loss:  3.48657975e-03, bound:  3.15334737e-01\n",
      "Epoch: 34846 mean train loss:  3.48648825e-03, bound:  3.15334737e-01\n",
      "Epoch: 34847 mean train loss:  3.48645449e-03, bound:  3.15334707e-01\n",
      "Epoch: 34848 mean train loss:  3.48634482e-03, bound:  3.15334707e-01\n",
      "Epoch: 34849 mean train loss:  3.48637369e-03, bound:  3.15334707e-01\n",
      "Epoch: 34850 mean train loss:  3.48625053e-03, bound:  3.15334707e-01\n",
      "Epoch: 34851 mean train loss:  3.48615600e-03, bound:  3.15334707e-01\n",
      "Epoch: 34852 mean train loss:  3.48611828e-03, bound:  3.15334707e-01\n",
      "Epoch: 34853 mean train loss:  3.48604470e-03, bound:  3.15334678e-01\n",
      "Epoch: 34854 mean train loss:  3.48595111e-03, bound:  3.15334707e-01\n",
      "Epoch: 34855 mean train loss:  3.48592387e-03, bound:  3.15334678e-01\n",
      "Epoch: 34856 mean train loss:  3.48584889e-03, bound:  3.15334678e-01\n",
      "Epoch: 34857 mean train loss:  3.48581444e-03, bound:  3.15334648e-01\n",
      "Epoch: 34858 mean train loss:  3.48570780e-03, bound:  3.15334618e-01\n",
      "Epoch: 34859 mean train loss:  3.48570058e-03, bound:  3.15334618e-01\n",
      "Epoch: 34860 mean train loss:  3.48571129e-03, bound:  3.15334618e-01\n",
      "Epoch: 34861 mean train loss:  3.48558370e-03, bound:  3.15334618e-01\n",
      "Epoch: 34862 mean train loss:  3.48553108e-03, bound:  3.15334618e-01\n",
      "Epoch: 34863 mean train loss:  3.48542980e-03, bound:  3.15334618e-01\n",
      "Epoch: 34864 mean train loss:  3.48536647e-03, bound:  3.15334588e-01\n",
      "Epoch: 34865 mean train loss:  3.48531455e-03, bound:  3.15334588e-01\n",
      "Epoch: 34866 mean train loss:  3.48518649e-03, bound:  3.15334588e-01\n",
      "Epoch: 34867 mean train loss:  3.48507450e-03, bound:  3.15334588e-01\n",
      "Epoch: 34868 mean train loss:  3.48507869e-03, bound:  3.15334588e-01\n",
      "Epoch: 34869 mean train loss:  3.48501489e-03, bound:  3.15334588e-01\n",
      "Epoch: 34870 mean train loss:  3.48498882e-03, bound:  3.15334558e-01\n",
      "Epoch: 34871 mean train loss:  3.48493620e-03, bound:  3.15334588e-01\n",
      "Epoch: 34872 mean train loss:  3.48489522e-03, bound:  3.15334558e-01\n",
      "Epoch: 34873 mean train loss:  3.48481536e-03, bound:  3.15334558e-01\n",
      "Epoch: 34874 mean train loss:  3.48472176e-03, bound:  3.15334529e-01\n",
      "Epoch: 34875 mean train loss:  3.48463398e-03, bound:  3.15334529e-01\n",
      "Epoch: 34876 mean train loss:  3.48459044e-03, bound:  3.15334499e-01\n",
      "Epoch: 34877 mean train loss:  3.48450127e-03, bound:  3.15334499e-01\n",
      "Epoch: 34878 mean train loss:  3.48441955e-03, bound:  3.15334499e-01\n",
      "Epoch: 34879 mean train loss:  3.48441279e-03, bound:  3.15334499e-01\n",
      "Epoch: 34880 mean train loss:  3.48431827e-03, bound:  3.15334499e-01\n",
      "Epoch: 34881 mean train loss:  3.48424679e-03, bound:  3.15334499e-01\n",
      "Epoch: 34882 mean train loss:  3.48417088e-03, bound:  3.15334469e-01\n",
      "Epoch: 34883 mean train loss:  3.48409824e-03, bound:  3.15334469e-01\n",
      "Epoch: 34884 mean train loss:  3.48401815e-03, bound:  3.15334469e-01\n",
      "Epoch: 34885 mean train loss:  3.48389708e-03, bound:  3.15334469e-01\n",
      "Epoch: 34886 mean train loss:  3.48385959e-03, bound:  3.15334469e-01\n",
      "Epoch: 34887 mean train loss:  3.48382699e-03, bound:  3.15334439e-01\n",
      "Epoch: 34888 mean train loss:  3.48378695e-03, bound:  3.15334439e-01\n",
      "Epoch: 34889 mean train loss:  3.48373456e-03, bound:  3.15334439e-01\n",
      "Epoch: 34890 mean train loss:  3.48365214e-03, bound:  3.15334439e-01\n",
      "Epoch: 34891 mean train loss:  3.48359998e-03, bound:  3.15334439e-01\n",
      "Epoch: 34892 mean train loss:  3.48352757e-03, bound:  3.15334409e-01\n",
      "Epoch: 34893 mean train loss:  3.48342559e-03, bound:  3.15334409e-01\n",
      "Epoch: 34894 mean train loss:  3.48336995e-03, bound:  3.15334409e-01\n",
      "Epoch: 34895 mean train loss:  3.48325470e-03, bound:  3.15334409e-01\n",
      "Epoch: 34896 mean train loss:  3.48325516e-03, bound:  3.15334409e-01\n",
      "Epoch: 34897 mean train loss:  3.48316086e-03, bound:  3.15334409e-01\n",
      "Epoch: 34898 mean train loss:  3.48307588e-03, bound:  3.15334380e-01\n",
      "Epoch: 34899 mean train loss:  3.48302070e-03, bound:  3.15334380e-01\n",
      "Epoch: 34900 mean train loss:  3.48294899e-03, bound:  3.15334380e-01\n",
      "Epoch: 34901 mean train loss:  3.48289940e-03, bound:  3.15334380e-01\n",
      "Epoch: 34902 mean train loss:  3.48285143e-03, bound:  3.15334380e-01\n",
      "Epoch: 34903 mean train loss:  3.48280906e-03, bound:  3.15334320e-01\n",
      "Epoch: 34904 mean train loss:  3.48274736e-03, bound:  3.15334320e-01\n",
      "Epoch: 34905 mean train loss:  3.48266005e-03, bound:  3.15334320e-01\n",
      "Epoch: 34906 mean train loss:  3.48255644e-03, bound:  3.15334320e-01\n",
      "Epoch: 34907 mean train loss:  3.48247052e-03, bound:  3.15334320e-01\n",
      "Epoch: 34908 mean train loss:  3.48246843e-03, bound:  3.15334320e-01\n",
      "Epoch: 34909 mean train loss:  3.48242139e-03, bound:  3.15334320e-01\n",
      "Epoch: 34910 mean train loss:  3.48231778e-03, bound:  3.15334290e-01\n",
      "Epoch: 34911 mean train loss:  3.48222558e-03, bound:  3.15334290e-01\n",
      "Epoch: 34912 mean train loss:  3.48217157e-03, bound:  3.15334290e-01\n",
      "Epoch: 34913 mean train loss:  3.48213222e-03, bound:  3.15334290e-01\n",
      "Epoch: 34914 mean train loss:  3.48209450e-03, bound:  3.15334260e-01\n",
      "Epoch: 34915 mean train loss:  3.48200649e-03, bound:  3.15334260e-01\n",
      "Epoch: 34916 mean train loss:  3.48195899e-03, bound:  3.15334260e-01\n",
      "Epoch: 34917 mean train loss:  3.48184002e-03, bound:  3.15334260e-01\n",
      "Epoch: 34918 mean train loss:  3.48182884e-03, bound:  3.15334260e-01\n",
      "Epoch: 34919 mean train loss:  3.48174339e-03, bound:  3.15334260e-01\n",
      "Epoch: 34920 mean train loss:  3.48168169e-03, bound:  3.15334231e-01\n",
      "Epoch: 34921 mean train loss:  3.48167680e-03, bound:  3.15334171e-01\n",
      "Epoch: 34922 mean train loss:  3.48156481e-03, bound:  3.15334171e-01\n",
      "Epoch: 34923 mean train loss:  3.48152057e-03, bound:  3.15334171e-01\n",
      "Epoch: 34924 mean train loss:  3.48138600e-03, bound:  3.15334171e-01\n",
      "Epoch: 34925 mean train loss:  3.48137692e-03, bound:  3.15334171e-01\n",
      "Epoch: 34926 mean train loss:  3.48134013e-03, bound:  3.15334171e-01\n",
      "Epoch: 34927 mean train loss:  3.48122930e-03, bound:  3.15334171e-01\n",
      "Epoch: 34928 mean train loss:  3.48118786e-03, bound:  3.15334171e-01\n",
      "Epoch: 34929 mean train loss:  3.48115037e-03, bound:  3.15334171e-01\n",
      "Epoch: 34930 mean train loss:  3.48103815e-03, bound:  3.15334141e-01\n",
      "Epoch: 34931 mean train loss:  3.48093896e-03, bound:  3.15334141e-01\n",
      "Epoch: 34932 mean train loss:  3.48088727e-03, bound:  3.15334141e-01\n",
      "Epoch: 34933 mean train loss:  3.48076480e-03, bound:  3.15334141e-01\n",
      "Epoch: 34934 mean train loss:  3.48075759e-03, bound:  3.15334141e-01\n",
      "Epoch: 34935 mean train loss:  3.48073686e-03, bound:  3.15334141e-01\n",
      "Epoch: 34936 mean train loss:  3.48068145e-03, bound:  3.15334111e-01\n",
      "Epoch: 34937 mean train loss:  3.48059065e-03, bound:  3.15334111e-01\n",
      "Epoch: 34938 mean train loss:  3.48053570e-03, bound:  3.15334111e-01\n",
      "Epoch: 34939 mean train loss:  3.48047353e-03, bound:  3.15334111e-01\n",
      "Epoch: 34940 mean train loss:  3.48038133e-03, bound:  3.15334111e-01\n",
      "Epoch: 34941 mean train loss:  3.48031637e-03, bound:  3.15334111e-01\n",
      "Epoch: 34942 mean train loss:  3.48027400e-03, bound:  3.15334082e-01\n",
      "Epoch: 34943 mean train loss:  3.48014524e-03, bound:  3.15334052e-01\n",
      "Epoch: 34944 mean train loss:  3.48013965e-03, bound:  3.15334052e-01\n",
      "Epoch: 34945 mean train loss:  3.48008354e-03, bound:  3.15334052e-01\n",
      "Epoch: 34946 mean train loss:  3.48001625e-03, bound:  3.15334022e-01\n",
      "Epoch: 34947 mean train loss:  3.47996270e-03, bound:  3.15334022e-01\n",
      "Epoch: 34948 mean train loss:  3.47988494e-03, bound:  3.15334022e-01\n",
      "Epoch: 34949 mean train loss:  3.47978924e-03, bound:  3.15334022e-01\n",
      "Epoch: 34950 mean train loss:  3.47977411e-03, bound:  3.15334022e-01\n",
      "Epoch: 34951 mean train loss:  3.47964652e-03, bound:  3.15334022e-01\n",
      "Epoch: 34952 mean train loss:  3.47960042e-03, bound:  3.15334022e-01\n",
      "Epoch: 34953 mean train loss:  3.47953080e-03, bound:  3.15334022e-01\n",
      "Epoch: 34954 mean train loss:  3.47946445e-03, bound:  3.15333992e-01\n",
      "Epoch: 34955 mean train loss:  3.47940228e-03, bound:  3.15333992e-01\n",
      "Epoch: 34956 mean train loss:  3.47937969e-03, bound:  3.15333992e-01\n",
      "Epoch: 34957 mean train loss:  3.47931334e-03, bound:  3.15333992e-01\n",
      "Epoch: 34958 mean train loss:  3.47925094e-03, bound:  3.15333992e-01\n",
      "Epoch: 34959 mean train loss:  3.47911986e-03, bound:  3.15333933e-01\n",
      "Epoch: 34960 mean train loss:  3.47911753e-03, bound:  3.15333933e-01\n",
      "Epoch: 34961 mean train loss:  3.47903860e-03, bound:  3.15333933e-01\n",
      "Epoch: 34962 mean train loss:  3.47895571e-03, bound:  3.15333933e-01\n",
      "Epoch: 34963 mean train loss:  3.47892661e-03, bound:  3.15333903e-01\n",
      "Epoch: 34964 mean train loss:  3.47878225e-03, bound:  3.15333903e-01\n",
      "Epoch: 34965 mean train loss:  3.47876293e-03, bound:  3.15333903e-01\n",
      "Epoch: 34966 mean train loss:  3.47868376e-03, bound:  3.15333903e-01\n",
      "Epoch: 34967 mean train loss:  3.47865256e-03, bound:  3.15333903e-01\n",
      "Epoch: 34968 mean train loss:  3.47856176e-03, bound:  3.15333903e-01\n",
      "Epoch: 34969 mean train loss:  3.47852218e-03, bound:  3.15333903e-01\n",
      "Epoch: 34970 mean train loss:  3.47842160e-03, bound:  3.15333903e-01\n",
      "Epoch: 34971 mean train loss:  3.47835640e-03, bound:  3.15333873e-01\n",
      "Epoch: 34972 mean train loss:  3.47827259e-03, bound:  3.15333873e-01\n",
      "Epoch: 34973 mean train loss:  3.47827957e-03, bound:  3.15333873e-01\n",
      "Epoch: 34974 mean train loss:  3.47816036e-03, bound:  3.15333873e-01\n",
      "Epoch: 34975 mean train loss:  3.47812963e-03, bound:  3.15333843e-01\n",
      "Epoch: 34976 mean train loss:  3.47806583e-03, bound:  3.15333813e-01\n",
      "Epoch: 34977 mean train loss:  3.47798457e-03, bound:  3.15333813e-01\n",
      "Epoch: 34978 mean train loss:  3.47790588e-03, bound:  3.15333813e-01\n",
      "Epoch: 34979 mean train loss:  3.47782928e-03, bound:  3.15333813e-01\n",
      "Epoch: 34980 mean train loss:  3.47776175e-03, bound:  3.15333813e-01\n",
      "Epoch: 34981 mean train loss:  3.47770029e-03, bound:  3.15333784e-01\n",
      "Epoch: 34982 mean train loss:  3.47765954e-03, bound:  3.15333784e-01\n",
      "Epoch: 34983 mean train loss:  3.47763277e-03, bound:  3.15333784e-01\n",
      "Epoch: 34984 mean train loss:  3.47753498e-03, bound:  3.15333784e-01\n",
      "Epoch: 34985 mean train loss:  3.47745488e-03, bound:  3.15333784e-01\n",
      "Epoch: 34986 mean train loss:  3.47737526e-03, bound:  3.15333784e-01\n",
      "Epoch: 34987 mean train loss:  3.47730564e-03, bound:  3.15333784e-01\n",
      "Epoch: 34988 mean train loss:  3.47727886e-03, bound:  3.15333724e-01\n",
      "Epoch: 34989 mean train loss:  3.47722252e-03, bound:  3.15333724e-01\n",
      "Epoch: 34990 mean train loss:  3.47713125e-03, bound:  3.15333724e-01\n",
      "Epoch: 34991 mean train loss:  3.47710797e-03, bound:  3.15333724e-01\n",
      "Epoch: 34992 mean train loss:  3.47699248e-03, bound:  3.15333724e-01\n",
      "Epoch: 34993 mean train loss:  3.47688608e-03, bound:  3.15333694e-01\n",
      "Epoch: 34994 mean train loss:  3.47687840e-03, bound:  3.15333694e-01\n",
      "Epoch: 34995 mean train loss:  3.47681320e-03, bound:  3.15333694e-01\n",
      "Epoch: 34996 mean train loss:  3.47668026e-03, bound:  3.15333694e-01\n",
      "Epoch: 34997 mean train loss:  3.47670563e-03, bound:  3.15333694e-01\n",
      "Epoch: 34998 mean train loss:  3.47659737e-03, bound:  3.15333694e-01\n",
      "Epoch: 34999 mean train loss:  3.47656757e-03, bound:  3.15333694e-01\n",
      "Epoch: 35000 mean train loss:  3.47645627e-03, bound:  3.15333694e-01\n",
      "Epoch: 35001 mean train loss:  3.47642903e-03, bound:  3.15333694e-01\n",
      "Epoch: 35002 mean train loss:  3.47636500e-03, bound:  3.15333694e-01\n",
      "Epoch: 35003 mean train loss:  3.47627024e-03, bound:  3.15333694e-01\n",
      "Epoch: 35004 mean train loss:  3.47620365e-03, bound:  3.15333605e-01\n",
      "Epoch: 35005 mean train loss:  3.47617106e-03, bound:  3.15333605e-01\n",
      "Epoch: 35006 mean train loss:  3.47609119e-03, bound:  3.15333605e-01\n",
      "Epoch: 35007 mean train loss:  3.47601320e-03, bound:  3.15333605e-01\n",
      "Epoch: 35008 mean train loss:  3.47594731e-03, bound:  3.15333605e-01\n",
      "Epoch: 35009 mean train loss:  3.47591424e-03, bound:  3.15333605e-01\n",
      "Epoch: 35010 mean train loss:  3.47580016e-03, bound:  3.15333605e-01\n",
      "Epoch: 35011 mean train loss:  3.47576500e-03, bound:  3.15333605e-01\n",
      "Epoch: 35012 mean train loss:  3.47571075e-03, bound:  3.15333605e-01\n",
      "Epoch: 35013 mean train loss:  3.47570376e-03, bound:  3.15333605e-01\n",
      "Epoch: 35014 mean train loss:  3.47558036e-03, bound:  3.15333605e-01\n",
      "Epoch: 35015 mean train loss:  3.47552542e-03, bound:  3.15333575e-01\n",
      "Epoch: 35016 mean train loss:  3.47540854e-03, bound:  3.15333575e-01\n",
      "Epoch: 35017 mean train loss:  3.47541808e-03, bound:  3.15333575e-01\n",
      "Epoch: 35018 mean train loss:  3.47533333e-03, bound:  3.15333575e-01\n",
      "Epoch: 35019 mean train loss:  3.47527815e-03, bound:  3.15333575e-01\n",
      "Epoch: 35020 mean train loss:  3.47513892e-03, bound:  3.15333545e-01\n",
      "Epoch: 35021 mean train loss:  3.47509072e-03, bound:  3.15333486e-01\n",
      "Epoch: 35022 mean train loss:  3.47503321e-03, bound:  3.15333486e-01\n",
      "Epoch: 35023 mean train loss:  3.47496103e-03, bound:  3.15333486e-01\n",
      "Epoch: 35024 mean train loss:  3.47491680e-03, bound:  3.15333486e-01\n",
      "Epoch: 35025 mean train loss:  3.47483857e-03, bound:  3.15333486e-01\n",
      "Epoch: 35026 mean train loss:  3.47478059e-03, bound:  3.15333486e-01\n",
      "Epoch: 35027 mean train loss:  3.47471493e-03, bound:  3.15333486e-01\n",
      "Epoch: 35028 mean train loss:  3.47465603e-03, bound:  3.15333486e-01\n",
      "Epoch: 35029 mean train loss:  3.47455894e-03, bound:  3.15333486e-01\n",
      "Epoch: 35030 mean train loss:  3.47446091e-03, bound:  3.15333486e-01\n",
      "Epoch: 35031 mean train loss:  3.47445090e-03, bound:  3.15333456e-01\n",
      "Epoch: 35032 mean train loss:  3.47435079e-03, bound:  3.15333456e-01\n",
      "Epoch: 35033 mean train loss:  3.47434124e-03, bound:  3.15333456e-01\n",
      "Epoch: 35034 mean train loss:  3.47426697e-03, bound:  3.15333456e-01\n",
      "Epoch: 35035 mean train loss:  3.47419269e-03, bound:  3.15333456e-01\n",
      "Epoch: 35036 mean train loss:  3.47408210e-03, bound:  3.15333456e-01\n",
      "Epoch: 35037 mean train loss:  3.47407837e-03, bound:  3.15333396e-01\n",
      "Epoch: 35038 mean train loss:  3.47403740e-03, bound:  3.15333396e-01\n",
      "Epoch: 35039 mean train loss:  3.47398221e-03, bound:  3.15333396e-01\n",
      "Epoch: 35040 mean train loss:  3.47384787e-03, bound:  3.15333396e-01\n",
      "Epoch: 35041 mean train loss:  3.47384159e-03, bound:  3.15333396e-01\n",
      "Epoch: 35042 mean train loss:  3.47374217e-03, bound:  3.15333396e-01\n",
      "Epoch: 35043 mean train loss:  3.47364182e-03, bound:  3.15333366e-01\n",
      "Epoch: 35044 mean train loss:  3.47356941e-03, bound:  3.15333366e-01\n",
      "Epoch: 35045 mean train loss:  3.47353984e-03, bound:  3.15333366e-01\n",
      "Epoch: 35046 mean train loss:  3.47347255e-03, bound:  3.15333366e-01\n",
      "Epoch: 35047 mean train loss:  3.47343017e-03, bound:  3.15333366e-01\n",
      "Epoch: 35048 mean train loss:  3.47335008e-03, bound:  3.15333337e-01\n",
      "Epoch: 35049 mean train loss:  3.47327441e-03, bound:  3.15333337e-01\n",
      "Epoch: 35050 mean train loss:  3.47322505e-03, bound:  3.15333337e-01\n",
      "Epoch: 35051 mean train loss:  3.47312633e-03, bound:  3.15333337e-01\n",
      "Epoch: 35052 mean train loss:  3.47307534e-03, bound:  3.15333337e-01\n",
      "Epoch: 35053 mean train loss:  3.47303622e-03, bound:  3.15333337e-01\n",
      "Epoch: 35054 mean train loss:  3.47295427e-03, bound:  3.15333277e-01\n",
      "Epoch: 35055 mean train loss:  3.47287371e-03, bound:  3.15333277e-01\n",
      "Epoch: 35056 mean train loss:  3.47281620e-03, bound:  3.15333277e-01\n",
      "Epoch: 35057 mean train loss:  3.47275869e-03, bound:  3.15333277e-01\n",
      "Epoch: 35058 mean train loss:  3.47270910e-03, bound:  3.15333277e-01\n",
      "Epoch: 35059 mean train loss:  3.47264460e-03, bound:  3.15333277e-01\n",
      "Epoch: 35060 mean train loss:  3.47254751e-03, bound:  3.15333247e-01\n",
      "Epoch: 35061 mean train loss:  3.47249350e-03, bound:  3.15333247e-01\n",
      "Epoch: 35062 mean train loss:  3.47247883e-03, bound:  3.15333247e-01\n",
      "Epoch: 35063 mean train loss:  3.47236241e-03, bound:  3.15333247e-01\n",
      "Epoch: 35064 mean train loss:  3.47231119e-03, bound:  3.15333247e-01\n",
      "Epoch: 35065 mean train loss:  3.47223901e-03, bound:  3.15333247e-01\n",
      "Epoch: 35066 mean train loss:  3.47217079e-03, bound:  3.15333217e-01\n",
      "Epoch: 35067 mean train loss:  3.47214844e-03, bound:  3.15333217e-01\n",
      "Epoch: 35068 mean train loss:  3.47206998e-03, bound:  3.15333217e-01\n",
      "Epoch: 35069 mean train loss:  3.47197475e-03, bound:  3.15333217e-01\n",
      "Epoch: 35070 mean train loss:  3.47192702e-03, bound:  3.15333217e-01\n",
      "Epoch: 35071 mean train loss:  3.47182574e-03, bound:  3.15333158e-01\n",
      "Epoch: 35072 mean train loss:  3.47182900e-03, bound:  3.15333158e-01\n",
      "Epoch: 35073 mean train loss:  3.47167230e-03, bound:  3.15333158e-01\n",
      "Epoch: 35074 mean train loss:  3.47165880e-03, bound:  3.15333158e-01\n",
      "Epoch: 35075 mean train loss:  3.47157917e-03, bound:  3.15333158e-01\n",
      "Epoch: 35076 mean train loss:  3.47154890e-03, bound:  3.15333158e-01\n",
      "Epoch: 35077 mean train loss:  3.47144273e-03, bound:  3.15333128e-01\n",
      "Epoch: 35078 mean train loss:  3.47140245e-03, bound:  3.15333128e-01\n",
      "Epoch: 35079 mean train loss:  3.47133679e-03, bound:  3.15333128e-01\n",
      "Epoch: 35080 mean train loss:  3.47121293e-03, bound:  3.15333128e-01\n",
      "Epoch: 35081 mean train loss:  3.47119570e-03, bound:  3.15333128e-01\n",
      "Epoch: 35082 mean train loss:  3.47111328e-03, bound:  3.15333098e-01\n",
      "Epoch: 35083 mean train loss:  3.47105507e-03, bound:  3.15333098e-01\n",
      "Epoch: 35084 mean train loss:  3.47096217e-03, bound:  3.15333098e-01\n",
      "Epoch: 35085 mean train loss:  3.47094121e-03, bound:  3.15333098e-01\n",
      "Epoch: 35086 mean train loss:  3.47090047e-03, bound:  3.15333098e-01\n",
      "Epoch: 35087 mean train loss:  3.47082247e-03, bound:  3.15333098e-01\n",
      "Epoch: 35088 mean train loss:  3.47076007e-03, bound:  3.15333039e-01\n",
      "Epoch: 35089 mean train loss:  3.47064412e-03, bound:  3.15333039e-01\n",
      "Epoch: 35090 mean train loss:  3.47059383e-03, bound:  3.15333039e-01\n",
      "Epoch: 35091 mean train loss:  3.47057451e-03, bound:  3.15333039e-01\n",
      "Epoch: 35092 mean train loss:  3.47050326e-03, bound:  3.15333039e-01\n",
      "Epoch: 35093 mean train loss:  3.47044389e-03, bound:  3.15333009e-01\n",
      "Epoch: 35094 mean train loss:  3.47037753e-03, bound:  3.15333009e-01\n",
      "Epoch: 35095 mean train loss:  3.47028743e-03, bound:  3.15333009e-01\n",
      "Epoch: 35096 mean train loss:  3.47024202e-03, bound:  3.15333009e-01\n",
      "Epoch: 35097 mean train loss:  3.47014843e-03, bound:  3.15333009e-01\n",
      "Epoch: 35098 mean train loss:  3.47006833e-03, bound:  3.15333009e-01\n",
      "Epoch: 35099 mean train loss:  3.47002433e-03, bound:  3.15333009e-01\n",
      "Epoch: 35100 mean train loss:  3.46994423e-03, bound:  3.15333009e-01\n",
      "Epoch: 35101 mean train loss:  3.46994004e-03, bound:  3.15333009e-01\n",
      "Epoch: 35102 mean train loss:  3.46985203e-03, bound:  3.15333009e-01\n",
      "Epoch: 35103 mean train loss:  3.46984831e-03, bound:  3.15333009e-01\n",
      "Epoch: 35104 mean train loss:  3.46975564e-03, bound:  3.15332979e-01\n",
      "Epoch: 35105 mean train loss:  3.46979033e-03, bound:  3.15332919e-01\n",
      "Epoch: 35106 mean train loss:  3.46968137e-03, bound:  3.15332919e-01\n",
      "Epoch: 35107 mean train loss:  3.46962409e-03, bound:  3.15332919e-01\n",
      "Epoch: 35108 mean train loss:  3.46956099e-03, bound:  3.15332919e-01\n",
      "Epoch: 35109 mean train loss:  3.46941431e-03, bound:  3.15332919e-01\n",
      "Epoch: 35110 mean train loss:  3.46935634e-03, bound:  3.15332919e-01\n",
      "Epoch: 35111 mean train loss:  3.46921664e-03, bound:  3.15332890e-01\n",
      "Epoch: 35112 mean train loss:  3.46916821e-03, bound:  3.15332890e-01\n",
      "Epoch: 35113 mean train loss:  3.46912374e-03, bound:  3.15332890e-01\n",
      "Epoch: 35114 mean train loss:  3.46906064e-03, bound:  3.15332890e-01\n",
      "Epoch: 35115 mean train loss:  3.46898986e-03, bound:  3.15332890e-01\n",
      "Epoch: 35116 mean train loss:  3.46892071e-03, bound:  3.15332890e-01\n",
      "Epoch: 35117 mean train loss:  3.46888369e-03, bound:  3.15332890e-01\n",
      "Epoch: 35118 mean train loss:  3.46882455e-03, bound:  3.15332890e-01\n",
      "Epoch: 35119 mean train loss:  3.46872443e-03, bound:  3.15332890e-01\n",
      "Epoch: 35120 mean train loss:  3.46868834e-03, bound:  3.15332890e-01\n",
      "Epoch: 35121 mean train loss:  3.46861454e-03, bound:  3.15332860e-01\n",
      "Epoch: 35122 mean train loss:  3.46850255e-03, bound:  3.15332800e-01\n",
      "Epoch: 35123 mean train loss:  3.46844015e-03, bound:  3.15332800e-01\n",
      "Epoch: 35124 mean train loss:  3.46838520e-03, bound:  3.15332800e-01\n",
      "Epoch: 35125 mean train loss:  3.46836913e-03, bound:  3.15332800e-01\n",
      "Epoch: 35126 mean train loss:  3.46829300e-03, bound:  3.15332800e-01\n",
      "Epoch: 35127 mean train loss:  3.46825970e-03, bound:  3.15332800e-01\n",
      "Epoch: 35128 mean train loss:  3.46815260e-03, bound:  3.15332800e-01\n",
      "Epoch: 35129 mean train loss:  3.46806785e-03, bound:  3.15332770e-01\n",
      "Epoch: 35130 mean train loss:  3.46801151e-03, bound:  3.15332770e-01\n",
      "Epoch: 35131 mean train loss:  3.46792326e-03, bound:  3.15332770e-01\n",
      "Epoch: 35132 mean train loss:  3.46788880e-03, bound:  3.15332770e-01\n",
      "Epoch: 35133 mean train loss:  3.46779800e-03, bound:  3.15332770e-01\n",
      "Epoch: 35134 mean train loss:  3.46777239e-03, bound:  3.15332770e-01\n",
      "Epoch: 35135 mean train loss:  3.46769672e-03, bound:  3.15332770e-01\n",
      "Epoch: 35136 mean train loss:  3.46763665e-03, bound:  3.15332770e-01\n",
      "Epoch: 35137 mean train loss:  3.46753723e-03, bound:  3.15332770e-01\n",
      "Epoch: 35138 mean train loss:  3.46743106e-03, bound:  3.15332770e-01\n",
      "Epoch: 35139 mean train loss:  3.46745364e-03, bound:  3.15332711e-01\n",
      "Epoch: 35140 mean train loss:  3.46736470e-03, bound:  3.15332711e-01\n",
      "Epoch: 35141 mean train loss:  3.46727623e-03, bound:  3.15332711e-01\n",
      "Epoch: 35142 mean train loss:  3.46723408e-03, bound:  3.15332711e-01\n",
      "Epoch: 35143 mean train loss:  3.46717634e-03, bound:  3.15332711e-01\n",
      "Epoch: 35144 mean train loss:  3.46711255e-03, bound:  3.15332681e-01\n",
      "Epoch: 35145 mean train loss:  3.46701615e-03, bound:  3.15332681e-01\n",
      "Epoch: 35146 mean train loss:  3.46700475e-03, bound:  3.15332681e-01\n",
      "Epoch: 35147 mean train loss:  3.46694607e-03, bound:  3.15332681e-01\n",
      "Epoch: 35148 mean train loss:  3.46687157e-03, bound:  3.15332681e-01\n",
      "Epoch: 35149 mean train loss:  3.46677471e-03, bound:  3.15332651e-01\n",
      "Epoch: 35150 mean train loss:  3.46672744e-03, bound:  3.15332651e-01\n",
      "Epoch: 35151 mean train loss:  3.46664479e-03, bound:  3.15332651e-01\n",
      "Epoch: 35152 mean train loss:  3.46661778e-03, bound:  3.15332651e-01\n",
      "Epoch: 35153 mean train loss:  3.46652605e-03, bound:  3.15332651e-01\n",
      "Epoch: 35154 mean train loss:  3.46646551e-03, bound:  3.15332592e-01\n",
      "Epoch: 35155 mean train loss:  3.46640754e-03, bound:  3.15332592e-01\n",
      "Epoch: 35156 mean train loss:  3.46635561e-03, bound:  3.15332592e-01\n",
      "Epoch: 35157 mean train loss:  3.46628414e-03, bound:  3.15332592e-01\n",
      "Epoch: 35158 mean train loss:  3.46621173e-03, bound:  3.15332592e-01\n",
      "Epoch: 35159 mean train loss:  3.46613722e-03, bound:  3.15332592e-01\n",
      "Epoch: 35160 mean train loss:  3.46607319e-03, bound:  3.15332592e-01\n",
      "Epoch: 35161 mean train loss:  3.46598285e-03, bound:  3.15332592e-01\n",
      "Epoch: 35162 mean train loss:  3.46594607e-03, bound:  3.15332592e-01\n",
      "Epoch: 35163 mean train loss:  3.46585317e-03, bound:  3.15332562e-01\n",
      "Epoch: 35164 mean train loss:  3.46583896e-03, bound:  3.15332562e-01\n",
      "Epoch: 35165 mean train loss:  3.46573838e-03, bound:  3.15332562e-01\n",
      "Epoch: 35166 mean train loss:  3.46566830e-03, bound:  3.15332532e-01\n",
      "Epoch: 35167 mean train loss:  3.46566411e-03, bound:  3.15332532e-01\n",
      "Epoch: 35168 mean train loss:  3.46557004e-03, bound:  3.15332532e-01\n",
      "Epoch: 35169 mean train loss:  3.46547202e-03, bound:  3.15332532e-01\n",
      "Epoch: 35170 mean train loss:  3.46545991e-03, bound:  3.15332532e-01\n",
      "Epoch: 35171 mean train loss:  3.46539961e-03, bound:  3.15332532e-01\n",
      "Epoch: 35172 mean train loss:  3.46531346e-03, bound:  3.15332532e-01\n",
      "Epoch: 35173 mean train loss:  3.46520334e-03, bound:  3.15332472e-01\n",
      "Epoch: 35174 mean train loss:  3.46516352e-03, bound:  3.15332472e-01\n",
      "Epoch: 35175 mean train loss:  3.46510671e-03, bound:  3.15332472e-01\n",
      "Epoch: 35176 mean train loss:  3.46506550e-03, bound:  3.15332472e-01\n",
      "Epoch: 35177 mean train loss:  3.46496631e-03, bound:  3.15332472e-01\n",
      "Epoch: 35178 mean train loss:  3.46494070e-03, bound:  3.15332472e-01\n",
      "Epoch: 35179 mean train loss:  3.46486014e-03, bound:  3.15332443e-01\n",
      "Epoch: 35180 mean train loss:  3.46480822e-03, bound:  3.15332443e-01\n",
      "Epoch: 35181 mean train loss:  3.46471625e-03, bound:  3.15332443e-01\n",
      "Epoch: 35182 mean train loss:  3.46466061e-03, bound:  3.15332443e-01\n",
      "Epoch: 35183 mean train loss:  3.46460007e-03, bound:  3.15332443e-01\n",
      "Epoch: 35184 mean train loss:  3.46454279e-03, bound:  3.15332413e-01\n",
      "Epoch: 35185 mean train loss:  3.46446154e-03, bound:  3.15332413e-01\n",
      "Epoch: 35186 mean train loss:  3.46440519e-03, bound:  3.15332413e-01\n",
      "Epoch: 35187 mean train loss:  3.46431532e-03, bound:  3.15332413e-01\n",
      "Epoch: 35188 mean train loss:  3.46429157e-03, bound:  3.15332413e-01\n",
      "Epoch: 35189 mean train loss:  3.46417958e-03, bound:  3.15332353e-01\n",
      "Epoch: 35190 mean train loss:  3.46415490e-03, bound:  3.15332353e-01\n",
      "Epoch: 35191 mean train loss:  3.46411346e-03, bound:  3.15332353e-01\n",
      "Epoch: 35192 mean train loss:  3.46406107e-03, bound:  3.15332353e-01\n",
      "Epoch: 35193 mean train loss:  3.46399494e-03, bound:  3.15332353e-01\n",
      "Epoch: 35194 mean train loss:  3.46398796e-03, bound:  3.15332353e-01\n",
      "Epoch: 35195 mean train loss:  3.46385012e-03, bound:  3.15332353e-01\n",
      "Epoch: 35196 mean train loss:  3.46387457e-03, bound:  3.15332323e-01\n",
      "Epoch: 35197 mean train loss:  3.46382428e-03, bound:  3.15332323e-01\n",
      "Epoch: 35198 mean train loss:  3.46373604e-03, bound:  3.15332323e-01\n",
      "Epoch: 35199 mean train loss:  3.46363592e-03, bound:  3.15332323e-01\n",
      "Epoch: 35200 mean train loss:  3.46354279e-03, bound:  3.15332323e-01\n",
      "Epoch: 35201 mean train loss:  3.46347690e-03, bound:  3.15332323e-01\n",
      "Epoch: 35202 mean train loss:  3.46336770e-03, bound:  3.15332264e-01\n",
      "Epoch: 35203 mean train loss:  3.46330344e-03, bound:  3.15332264e-01\n",
      "Epoch: 35204 mean train loss:  3.46320448e-03, bound:  3.15332264e-01\n",
      "Epoch: 35205 mean train loss:  3.46317445e-03, bound:  3.15332264e-01\n",
      "Epoch: 35206 mean train loss:  3.46312486e-03, bound:  3.15332264e-01\n",
      "Epoch: 35207 mean train loss:  3.46305384e-03, bound:  3.15332234e-01\n",
      "Epoch: 35208 mean train loss:  3.46300681e-03, bound:  3.15332234e-01\n",
      "Epoch: 35209 mean train loss:  3.46291857e-03, bound:  3.15332234e-01\n",
      "Epoch: 35210 mean train loss:  3.46284872e-03, bound:  3.15332234e-01\n",
      "Epoch: 35211 mean train loss:  3.46275396e-03, bound:  3.15332234e-01\n",
      "Epoch: 35212 mean train loss:  3.46270273e-03, bound:  3.15332234e-01\n",
      "Epoch: 35213 mean train loss:  3.46264592e-03, bound:  3.15332234e-01\n",
      "Epoch: 35214 mean train loss:  3.46259284e-03, bound:  3.15332204e-01\n",
      "Epoch: 35215 mean train loss:  3.46251857e-03, bound:  3.15332204e-01\n",
      "Epoch: 35216 mean train loss:  3.46246315e-03, bound:  3.15332204e-01\n",
      "Epoch: 35217 mean train loss:  3.46236257e-03, bound:  3.15332204e-01\n",
      "Epoch: 35218 mean train loss:  3.46234231e-03, bound:  3.15332204e-01\n",
      "Epoch: 35219 mean train loss:  3.46222590e-03, bound:  3.15332204e-01\n",
      "Epoch: 35220 mean train loss:  3.46222240e-03, bound:  3.15332204e-01\n",
      "Epoch: 35221 mean train loss:  3.46209947e-03, bound:  3.15332204e-01\n",
      "Epoch: 35222 mean train loss:  3.46210832e-03, bound:  3.15332204e-01\n",
      "Epoch: 35223 mean train loss:  3.46197328e-03, bound:  3.15332115e-01\n",
      "Epoch: 35224 mean train loss:  3.46196326e-03, bound:  3.15332115e-01\n",
      "Epoch: 35225 mean train loss:  3.46184336e-03, bound:  3.15332115e-01\n",
      "Epoch: 35226 mean train loss:  3.46180052e-03, bound:  3.15332115e-01\n",
      "Epoch: 35227 mean train loss:  3.46173998e-03, bound:  3.15332115e-01\n",
      "Epoch: 35228 mean train loss:  3.46171274e-03, bound:  3.15332115e-01\n",
      "Epoch: 35229 mean train loss:  3.46162962e-03, bound:  3.15332115e-01\n",
      "Epoch: 35230 mean train loss:  3.46155325e-03, bound:  3.15332085e-01\n",
      "Epoch: 35231 mean train loss:  3.46152880e-03, bound:  3.15332085e-01\n",
      "Epoch: 35232 mean train loss:  3.46147874e-03, bound:  3.15332085e-01\n",
      "Epoch: 35233 mean train loss:  3.46132205e-03, bound:  3.15332085e-01\n",
      "Epoch: 35234 mean train loss:  3.46131157e-03, bound:  3.15332085e-01\n",
      "Epoch: 35235 mean train loss:  3.46122449e-03, bound:  3.15332085e-01\n",
      "Epoch: 35236 mean train loss:  3.46120237e-03, bound:  3.15332085e-01\n",
      "Epoch: 35237 mean train loss:  3.46110691e-03, bound:  3.15332085e-01\n",
      "Epoch: 35238 mean train loss:  3.46107315e-03, bound:  3.15332085e-01\n",
      "Epoch: 35239 mean train loss:  3.46101355e-03, bound:  3.15332085e-01\n",
      "Epoch: 35240 mean train loss:  3.46091227e-03, bound:  3.15332025e-01\n",
      "Epoch: 35241 mean train loss:  3.46085546e-03, bound:  3.15332025e-01\n",
      "Epoch: 35242 mean train loss:  3.46079655e-03, bound:  3.15332025e-01\n",
      "Epoch: 35243 mean train loss:  3.46073252e-03, bound:  3.15332025e-01\n",
      "Epoch: 35244 mean train loss:  3.46067711e-03, bound:  3.15332025e-01\n",
      "Epoch: 35245 mean train loss:  3.46059445e-03, bound:  3.15331995e-01\n",
      "Epoch: 35246 mean train loss:  3.46054859e-03, bound:  3.15331995e-01\n",
      "Epoch: 35247 mean train loss:  3.46045080e-03, bound:  3.15331966e-01\n",
      "Epoch: 35248 mean train loss:  3.46038048e-03, bound:  3.15331966e-01\n",
      "Epoch: 35249 mean train loss:  3.46036139e-03, bound:  3.15331966e-01\n",
      "Epoch: 35250 mean train loss:  3.46028828e-03, bound:  3.15331966e-01\n",
      "Epoch: 35251 mean train loss:  3.46018816e-03, bound:  3.15331966e-01\n",
      "Epoch: 35252 mean train loss:  3.46013927e-03, bound:  3.15331966e-01\n",
      "Epoch: 35253 mean train loss:  3.46010947e-03, bound:  3.15331966e-01\n",
      "Epoch: 35254 mean train loss:  3.46005941e-03, bound:  3.15331966e-01\n",
      "Epoch: 35255 mean train loss:  3.45994369e-03, bound:  3.15331966e-01\n",
      "Epoch: 35256 mean train loss:  3.45996371e-03, bound:  3.15331906e-01\n",
      "Epoch: 35257 mean train loss:  3.45987221e-03, bound:  3.15331906e-01\n",
      "Epoch: 35258 mean train loss:  3.45979910e-03, bound:  3.15331906e-01\n",
      "Epoch: 35259 mean train loss:  3.45973484e-03, bound:  3.15331906e-01\n",
      "Epoch: 35260 mean train loss:  3.45969666e-03, bound:  3.15331906e-01\n",
      "Epoch: 35261 mean train loss:  3.45957093e-03, bound:  3.15331906e-01\n",
      "Epoch: 35262 mean train loss:  3.45951528e-03, bound:  3.15331876e-01\n",
      "Epoch: 35263 mean train loss:  3.45948711e-03, bound:  3.15331876e-01\n",
      "Epoch: 35264 mean train loss:  3.45939002e-03, bound:  3.15331876e-01\n",
      "Epoch: 35265 mean train loss:  3.45928455e-03, bound:  3.15331876e-01\n",
      "Epoch: 35266 mean train loss:  3.45921796e-03, bound:  3.15331876e-01\n",
      "Epoch: 35267 mean train loss:  3.45917675e-03, bound:  3.15331876e-01\n",
      "Epoch: 35268 mean train loss:  3.45908524e-03, bound:  3.15331876e-01\n",
      "Epoch: 35269 mean train loss:  3.45906056e-03, bound:  3.15331817e-01\n",
      "Epoch: 35270 mean train loss:  3.45900073e-03, bound:  3.15331817e-01\n",
      "Epoch: 35271 mean train loss:  3.45894764e-03, bound:  3.15331817e-01\n",
      "Epoch: 35272 mean train loss:  3.45889293e-03, bound:  3.15331817e-01\n",
      "Epoch: 35273 mean train loss:  3.45887034e-03, bound:  3.15331817e-01\n",
      "Epoch: 35274 mean train loss:  3.45873204e-03, bound:  3.15331787e-01\n",
      "Epoch: 35275 mean train loss:  3.45875975e-03, bound:  3.15331787e-01\n",
      "Epoch: 35276 mean train loss:  3.45862727e-03, bound:  3.15331787e-01\n",
      "Epoch: 35277 mean train loss:  3.45856301e-03, bound:  3.15331787e-01\n",
      "Epoch: 35278 mean train loss:  3.45844682e-03, bound:  3.15331787e-01\n",
      "Epoch: 35279 mean train loss:  3.45841120e-03, bound:  3.15331787e-01\n",
      "Epoch: 35280 mean train loss:  3.45842983e-03, bound:  3.15331787e-01\n",
      "Epoch: 35281 mean train loss:  3.45827918e-03, bound:  3.15331757e-01\n",
      "Epoch: 35282 mean train loss:  3.45823169e-03, bound:  3.15331757e-01\n",
      "Epoch: 35283 mean train loss:  3.45817651e-03, bound:  3.15331757e-01\n",
      "Epoch: 35284 mean train loss:  3.45813017e-03, bound:  3.15331757e-01\n",
      "Epoch: 35285 mean train loss:  3.45804333e-03, bound:  3.15331757e-01\n",
      "Epoch: 35286 mean train loss:  3.45797441e-03, bound:  3.15331757e-01\n",
      "Epoch: 35287 mean train loss:  3.45790014e-03, bound:  3.15331757e-01\n",
      "Epoch: 35288 mean train loss:  3.45782586e-03, bound:  3.15331697e-01\n",
      "Epoch: 35289 mean train loss:  3.45778791e-03, bound:  3.15331697e-01\n",
      "Epoch: 35290 mean train loss:  3.45770107e-03, bound:  3.15331697e-01\n",
      "Epoch: 35291 mean train loss:  3.45768733e-03, bound:  3.15331697e-01\n",
      "Epoch: 35292 mean train loss:  3.45759001e-03, bound:  3.15331697e-01\n",
      "Epoch: 35293 mean train loss:  3.45753273e-03, bound:  3.15331668e-01\n",
      "Epoch: 35294 mean train loss:  3.45744332e-03, bound:  3.15331668e-01\n",
      "Epoch: 35295 mean train loss:  3.45745031e-03, bound:  3.15331668e-01\n",
      "Epoch: 35296 mean train loss:  3.45734367e-03, bound:  3.15331668e-01\n",
      "Epoch: 35297 mean train loss:  3.45726614e-03, bound:  3.15331668e-01\n",
      "Epoch: 35298 mean train loss:  3.45722260e-03, bound:  3.15331638e-01\n",
      "Epoch: 35299 mean train loss:  3.45711247e-03, bound:  3.15331638e-01\n",
      "Epoch: 35300 mean train loss:  3.45704425e-03, bound:  3.15331638e-01\n",
      "Epoch: 35301 mean train loss:  3.45699233e-03, bound:  3.15331638e-01\n",
      "Epoch: 35302 mean train loss:  3.45696509e-03, bound:  3.15331638e-01\n",
      "Epoch: 35303 mean train loss:  3.45688453e-03, bound:  3.15331638e-01\n",
      "Epoch: 35304 mean train loss:  3.45684821e-03, bound:  3.15331638e-01\n",
      "Epoch: 35305 mean train loss:  3.45674460e-03, bound:  3.15331578e-01\n",
      "Epoch: 35306 mean train loss:  3.45665216e-03, bound:  3.15331578e-01\n",
      "Epoch: 35307 mean train loss:  3.45663331e-03, bound:  3.15331578e-01\n",
      "Epoch: 35308 mean train loss:  3.45654809e-03, bound:  3.15331578e-01\n",
      "Epoch: 35309 mean train loss:  3.45651642e-03, bound:  3.15331548e-01\n",
      "Epoch: 35310 mean train loss:  3.45642911e-03, bound:  3.15331548e-01\n",
      "Epoch: 35311 mean train loss:  3.45633971e-03, bound:  3.15331548e-01\n",
      "Epoch: 35312 mean train loss:  3.45630385e-03, bound:  3.15331548e-01\n",
      "Epoch: 35313 mean train loss:  3.45626567e-03, bound:  3.15331548e-01\n",
      "Epoch: 35314 mean train loss:  3.45619209e-03, bound:  3.15331548e-01\n",
      "Epoch: 35315 mean train loss:  3.45609128e-03, bound:  3.15331548e-01\n",
      "Epoch: 35316 mean train loss:  3.45601118e-03, bound:  3.15331519e-01\n",
      "Epoch: 35317 mean train loss:  3.45599581e-03, bound:  3.15331519e-01\n",
      "Epoch: 35318 mean train loss:  3.45591572e-03, bound:  3.15331519e-01\n",
      "Epoch: 35319 mean train loss:  3.45586124e-03, bound:  3.15331519e-01\n",
      "Epoch: 35320 mean train loss:  3.45581211e-03, bound:  3.15331519e-01\n",
      "Epoch: 35321 mean train loss:  3.45575251e-03, bound:  3.15331459e-01\n",
      "Epoch: 35322 mean train loss:  3.45565053e-03, bound:  3.15331459e-01\n",
      "Epoch: 35323 mean train loss:  3.45562096e-03, bound:  3.15331459e-01\n",
      "Epoch: 35324 mean train loss:  3.45547707e-03, bound:  3.15331459e-01\n",
      "Epoch: 35325 mean train loss:  3.45546170e-03, bound:  3.15331459e-01\n",
      "Epoch: 35326 mean train loss:  3.45541281e-03, bound:  3.15331459e-01\n",
      "Epoch: 35327 mean train loss:  3.45533900e-03, bound:  3.15331459e-01\n",
      "Epoch: 35328 mean train loss:  3.45526240e-03, bound:  3.15331459e-01\n",
      "Epoch: 35329 mean train loss:  3.45519604e-03, bound:  3.15331459e-01\n",
      "Epoch: 35330 mean train loss:  3.45513085e-03, bound:  3.15331459e-01\n",
      "Epoch: 35331 mean train loss:  3.45506030e-03, bound:  3.15331459e-01\n",
      "Epoch: 35332 mean train loss:  3.45504074e-03, bound:  3.15331399e-01\n",
      "Epoch: 35333 mean train loss:  3.45488009e-03, bound:  3.15331399e-01\n",
      "Epoch: 35334 mean train loss:  3.45492619e-03, bound:  3.15331399e-01\n",
      "Epoch: 35335 mean train loss:  3.45484447e-03, bound:  3.15331399e-01\n",
      "Epoch: 35336 mean train loss:  3.45475390e-03, bound:  3.15331399e-01\n",
      "Epoch: 35337 mean train loss:  3.45466495e-03, bound:  3.15331399e-01\n",
      "Epoch: 35338 mean train loss:  3.45465681e-03, bound:  3.15331370e-01\n",
      "Epoch: 35339 mean train loss:  3.45456041e-03, bound:  3.15331370e-01\n",
      "Epoch: 35340 mean train loss:  3.45451524e-03, bound:  3.15331370e-01\n",
      "Epoch: 35341 mean train loss:  3.45443236e-03, bound:  3.15331370e-01\n",
      "Epoch: 35342 mean train loss:  3.45436088e-03, bound:  3.15331370e-01\n",
      "Epoch: 35343 mean train loss:  3.45428754e-03, bound:  3.15331370e-01\n",
      "Epoch: 35344 mean train loss:  3.45426030e-03, bound:  3.15331340e-01\n",
      "Epoch: 35345 mean train loss:  3.45413922e-03, bound:  3.15331340e-01\n",
      "Epoch: 35346 mean train loss:  3.45410663e-03, bound:  3.15331340e-01\n",
      "Epoch: 35347 mean train loss:  3.45402863e-03, bound:  3.15331340e-01\n",
      "Epoch: 35348 mean train loss:  3.45395296e-03, bound:  3.15331340e-01\n",
      "Epoch: 35349 mean train loss:  3.45395342e-03, bound:  3.15331310e-01\n",
      "Epoch: 35350 mean train loss:  3.45386728e-03, bound:  3.15331310e-01\n",
      "Epoch: 35351 mean train loss:  3.45382513e-03, bound:  3.15331280e-01\n",
      "Epoch: 35352 mean train loss:  3.45373014e-03, bound:  3.15331280e-01\n",
      "Epoch: 35353 mean train loss:  3.45366402e-03, bound:  3.15331280e-01\n",
      "Epoch: 35354 mean train loss:  3.45360534e-03, bound:  3.15331280e-01\n",
      "Epoch: 35355 mean train loss:  3.45355715e-03, bound:  3.15331250e-01\n",
      "Epoch: 35356 mean train loss:  3.45346960e-03, bound:  3.15331250e-01\n",
      "Epoch: 35357 mean train loss:  3.45343421e-03, bound:  3.15331250e-01\n",
      "Epoch: 35358 mean train loss:  3.45333316e-03, bound:  3.15331250e-01\n",
      "Epoch: 35359 mean train loss:  3.45326494e-03, bound:  3.15331250e-01\n",
      "Epoch: 35360 mean train loss:  3.45320906e-03, bound:  3.15331221e-01\n",
      "Epoch: 35361 mean train loss:  3.45314760e-03, bound:  3.15331221e-01\n",
      "Epoch: 35362 mean train loss:  3.45310080e-03, bound:  3.15331221e-01\n",
      "Epoch: 35363 mean train loss:  3.45303281e-03, bound:  3.15331221e-01\n",
      "Epoch: 35364 mean train loss:  3.45299905e-03, bound:  3.15331221e-01\n",
      "Epoch: 35365 mean train loss:  3.45294178e-03, bound:  3.15331191e-01\n",
      "Epoch: 35366 mean train loss:  3.45288427e-03, bound:  3.15331221e-01\n",
      "Epoch: 35367 mean train loss:  3.45285027e-03, bound:  3.15331161e-01\n",
      "Epoch: 35368 mean train loss:  3.45277856e-03, bound:  3.15331161e-01\n",
      "Epoch: 35369 mean train loss:  3.45268403e-03, bound:  3.15331161e-01\n",
      "Epoch: 35370 mean train loss:  3.45261372e-03, bound:  3.15331161e-01\n",
      "Epoch: 35371 mean train loss:  3.45251523e-03, bound:  3.15331161e-01\n",
      "Epoch: 35372 mean train loss:  3.45246308e-03, bound:  3.15331131e-01\n",
      "Epoch: 35373 mean train loss:  3.45235132e-03, bound:  3.15331131e-01\n",
      "Epoch: 35374 mean train loss:  3.45230917e-03, bound:  3.15331131e-01\n",
      "Epoch: 35375 mean train loss:  3.45226680e-03, bound:  3.15331131e-01\n",
      "Epoch: 35376 mean train loss:  3.45221744e-03, bound:  3.15331131e-01\n",
      "Epoch: 35377 mean train loss:  3.45223094e-03, bound:  3.15331131e-01\n",
      "Epoch: 35378 mean train loss:  3.45217809e-03, bound:  3.15331101e-01\n",
      "Epoch: 35379 mean train loss:  3.45213804e-03, bound:  3.15331101e-01\n",
      "Epoch: 35380 mean train loss:  3.45198624e-03, bound:  3.15331101e-01\n",
      "Epoch: 35381 mean train loss:  3.45191755e-03, bound:  3.15331101e-01\n",
      "Epoch: 35382 mean train loss:  3.45181022e-03, bound:  3.15331101e-01\n",
      "Epoch: 35383 mean train loss:  3.45172756e-03, bound:  3.15331101e-01\n",
      "Epoch: 35384 mean train loss:  3.45169101e-03, bound:  3.15331072e-01\n",
      "Epoch: 35385 mean train loss:  3.45162768e-03, bound:  3.15331072e-01\n",
      "Epoch: 35386 mean train loss:  3.45159019e-03, bound:  3.15331072e-01\n",
      "Epoch: 35387 mean train loss:  3.45152454e-03, bound:  3.15331072e-01\n",
      "Epoch: 35388 mean train loss:  3.45149357e-03, bound:  3.15331072e-01\n",
      "Epoch: 35389 mean train loss:  3.45137366e-03, bound:  3.15331012e-01\n",
      "Epoch: 35390 mean train loss:  3.45140276e-03, bound:  3.15331012e-01\n",
      "Epoch: 35391 mean train loss:  3.45124491e-03, bound:  3.15331012e-01\n",
      "Epoch: 35392 mean train loss:  3.45119205e-03, bound:  3.15331012e-01\n",
      "Epoch: 35393 mean train loss:  3.45109892e-03, bound:  3.15331012e-01\n",
      "Epoch: 35394 mean train loss:  3.45105678e-03, bound:  3.15330982e-01\n",
      "Epoch: 35395 mean train loss:  3.45101301e-03, bound:  3.15330982e-01\n",
      "Epoch: 35396 mean train loss:  3.45093268e-03, bound:  3.15330982e-01\n",
      "Epoch: 35397 mean train loss:  3.45088658e-03, bound:  3.15330982e-01\n",
      "Epoch: 35398 mean train loss:  3.45082255e-03, bound:  3.15330982e-01\n",
      "Epoch: 35399 mean train loss:  3.45071685e-03, bound:  3.15330982e-01\n",
      "Epoch: 35400 mean train loss:  3.45067168e-03, bound:  3.15330982e-01\n",
      "Epoch: 35401 mean train loss:  3.45060020e-03, bound:  3.15330952e-01\n",
      "Epoch: 35402 mean train loss:  3.45052243e-03, bound:  3.15330952e-01\n",
      "Epoch: 35403 mean train loss:  3.45053873e-03, bound:  3.15330952e-01\n",
      "Epoch: 35404 mean train loss:  3.45037808e-03, bound:  3.15330952e-01\n",
      "Epoch: 35405 mean train loss:  3.45034082e-03, bound:  3.15330952e-01\n",
      "Epoch: 35406 mean train loss:  3.45029170e-03, bound:  3.15330952e-01\n",
      "Epoch: 35407 mean train loss:  3.45022534e-03, bound:  3.15330893e-01\n",
      "Epoch: 35408 mean train loss:  3.45021370e-03, bound:  3.15330893e-01\n",
      "Epoch: 35409 mean train loss:  3.45007540e-03, bound:  3.15330893e-01\n",
      "Epoch: 35410 mean train loss:  3.44999880e-03, bound:  3.15330893e-01\n",
      "Epoch: 35411 mean train loss:  3.44998087e-03, bound:  3.15330893e-01\n",
      "Epoch: 35412 mean train loss:  3.44987283e-03, bound:  3.15330863e-01\n",
      "Epoch: 35413 mean train loss:  3.44981370e-03, bound:  3.15330863e-01\n",
      "Epoch: 35414 mean train loss:  3.44977225e-03, bound:  3.15330863e-01\n",
      "Epoch: 35415 mean train loss:  3.44970124e-03, bound:  3.15330863e-01\n",
      "Epoch: 35416 mean train loss:  3.44961043e-03, bound:  3.15330863e-01\n",
      "Epoch: 35417 mean train loss:  3.44962697e-03, bound:  3.15330863e-01\n",
      "Epoch: 35418 mean train loss:  3.44953756e-03, bound:  3.15330863e-01\n",
      "Epoch: 35419 mean train loss:  3.44941299e-03, bound:  3.15330833e-01\n",
      "Epoch: 35420 mean train loss:  3.44941253e-03, bound:  3.15330833e-01\n",
      "Epoch: 35421 mean train loss:  3.44930519e-03, bound:  3.15330833e-01\n",
      "Epoch: 35422 mean train loss:  3.44929448e-03, bound:  3.15330833e-01\n",
      "Epoch: 35423 mean train loss:  3.44923628e-03, bound:  3.15330833e-01\n",
      "Epoch: 35424 mean train loss:  3.44916526e-03, bound:  3.15330774e-01\n",
      "Epoch: 35425 mean train loss:  3.44910822e-03, bound:  3.15330774e-01\n",
      "Epoch: 35426 mean train loss:  3.44900903e-03, bound:  3.15330774e-01\n",
      "Epoch: 35427 mean train loss:  3.44895432e-03, bound:  3.15330774e-01\n",
      "Epoch: 35428 mean train loss:  3.44889727e-03, bound:  3.15330774e-01\n",
      "Epoch: 35429 mean train loss:  3.44880228e-03, bound:  3.15330774e-01\n",
      "Epoch: 35430 mean train loss:  3.44875688e-03, bound:  3.15330774e-01\n",
      "Epoch: 35431 mean train loss:  3.44872824e-03, bound:  3.15330774e-01\n",
      "Epoch: 35432 mean train loss:  3.44863581e-03, bound:  3.15330774e-01\n",
      "Epoch: 35433 mean train loss:  3.44853872e-03, bound:  3.15330774e-01\n",
      "Epoch: 35434 mean train loss:  3.44855245e-03, bound:  3.15330774e-01\n",
      "Epoch: 35435 mean train loss:  3.44848167e-03, bound:  3.15330774e-01\n",
      "Epoch: 35436 mean train loss:  3.44841275e-03, bound:  3.15330714e-01\n",
      "Epoch: 35437 mean train loss:  3.44833219e-03, bound:  3.15330714e-01\n",
      "Epoch: 35438 mean train loss:  3.44828423e-03, bound:  3.15330714e-01\n",
      "Epoch: 35439 mean train loss:  3.44822626e-03, bound:  3.15330714e-01\n",
      "Epoch: 35440 mean train loss:  3.44813918e-03, bound:  3.15330714e-01\n",
      "Epoch: 35441 mean train loss:  3.44806956e-03, bound:  3.15330714e-01\n",
      "Epoch: 35442 mean train loss:  3.44798784e-03, bound:  3.15330654e-01\n",
      "Epoch: 35443 mean train loss:  3.44797457e-03, bound:  3.15330654e-01\n",
      "Epoch: 35444 mean train loss:  3.44789587e-03, bound:  3.15330654e-01\n",
      "Epoch: 35445 mean train loss:  3.44779901e-03, bound:  3.15330654e-01\n",
      "Epoch: 35446 mean train loss:  3.44773754e-03, bound:  3.15330654e-01\n",
      "Epoch: 35447 mean train loss:  3.44770215e-03, bound:  3.15330654e-01\n",
      "Epoch: 35448 mean train loss:  3.44762928e-03, bound:  3.15330654e-01\n",
      "Epoch: 35449 mean train loss:  3.44756665e-03, bound:  3.15330654e-01\n",
      "Epoch: 35450 mean train loss:  3.44749005e-03, bound:  3.15330654e-01\n",
      "Epoch: 35451 mean train loss:  3.44739738e-03, bound:  3.15330654e-01\n",
      "Epoch: 35452 mean train loss:  3.44737805e-03, bound:  3.15330654e-01\n",
      "Epoch: 35453 mean train loss:  3.44731682e-03, bound:  3.15330595e-01\n",
      "Epoch: 35454 mean train loss:  3.44720250e-03, bound:  3.15330595e-01\n",
      "Epoch: 35455 mean train loss:  3.44714406e-03, bound:  3.15330595e-01\n",
      "Epoch: 35456 mean train loss:  3.44710122e-03, bound:  3.15330595e-01\n",
      "Epoch: 35457 mean train loss:  3.44707072e-03, bound:  3.15330595e-01\n",
      "Epoch: 35458 mean train loss:  3.44697922e-03, bound:  3.15330595e-01\n",
      "Epoch: 35459 mean train loss:  3.44696874e-03, bound:  3.15330535e-01\n",
      "Epoch: 35460 mean train loss:  3.44684627e-03, bound:  3.15330535e-01\n",
      "Epoch: 35461 mean train loss:  3.44678992e-03, bound:  3.15330535e-01\n",
      "Epoch: 35462 mean train loss:  3.44675547e-03, bound:  3.15330535e-01\n",
      "Epoch: 35463 mean train loss:  3.44664720e-03, bound:  3.15330535e-01\n",
      "Epoch: 35464 mean train loss:  3.44662298e-03, bound:  3.15330535e-01\n",
      "Epoch: 35465 mean train loss:  3.44653823e-03, bound:  3.15330535e-01\n",
      "Epoch: 35466 mean train loss:  3.44650052e-03, bound:  3.15330535e-01\n",
      "Epoch: 35467 mean train loss:  3.44645604e-03, bound:  3.15330535e-01\n",
      "Epoch: 35468 mean train loss:  3.44634708e-03, bound:  3.15330535e-01\n",
      "Epoch: 35469 mean train loss:  3.44631984e-03, bound:  3.15330535e-01\n",
      "Epoch: 35470 mean train loss:  3.44618410e-03, bound:  3.15330505e-01\n",
      "Epoch: 35471 mean train loss:  3.44617502e-03, bound:  3.15330476e-01\n",
      "Epoch: 35472 mean train loss:  3.44608515e-03, bound:  3.15330476e-01\n",
      "Epoch: 35473 mean train loss:  3.44604114e-03, bound:  3.15330476e-01\n",
      "Epoch: 35474 mean train loss:  3.44595779e-03, bound:  3.15330476e-01\n",
      "Epoch: 35475 mean train loss:  3.44590610e-03, bound:  3.15330446e-01\n",
      "Epoch: 35476 mean train loss:  3.44581902e-03, bound:  3.15330446e-01\n",
      "Epoch: 35477 mean train loss:  3.44581250e-03, bound:  3.15330446e-01\n",
      "Epoch: 35478 mean train loss:  3.44567350e-03, bound:  3.15330446e-01\n",
      "Epoch: 35479 mean train loss:  3.44566163e-03, bound:  3.15330446e-01\n",
      "Epoch: 35480 mean train loss:  3.44561459e-03, bound:  3.15330446e-01\n",
      "Epoch: 35481 mean train loss:  3.44552193e-03, bound:  3.15330416e-01\n",
      "Epoch: 35482 mean train loss:  3.44544533e-03, bound:  3.15330416e-01\n",
      "Epoch: 35483 mean train loss:  3.44541366e-03, bound:  3.15330416e-01\n",
      "Epoch: 35484 mean train loss:  3.44533706e-03, bound:  3.15330416e-01\n",
      "Epoch: 35485 mean train loss:  3.44530912e-03, bound:  3.15330416e-01\n",
      "Epoch: 35486 mean train loss:  3.44523322e-03, bound:  3.15330416e-01\n",
      "Epoch: 35487 mean train loss:  3.44519434e-03, bound:  3.15330356e-01\n",
      "Epoch: 35488 mean train loss:  3.44509212e-03, bound:  3.15330356e-01\n",
      "Epoch: 35489 mean train loss:  3.44500807e-03, bound:  3.15330356e-01\n",
      "Epoch: 35490 mean train loss:  3.44493403e-03, bound:  3.15330356e-01\n",
      "Epoch: 35491 mean train loss:  3.44489818e-03, bound:  3.15330356e-01\n",
      "Epoch: 35492 mean train loss:  3.44483741e-03, bound:  3.15330356e-01\n",
      "Epoch: 35493 mean train loss:  3.44473985e-03, bound:  3.15330327e-01\n",
      "Epoch: 35494 mean train loss:  3.44467326e-03, bound:  3.15330327e-01\n",
      "Epoch: 35495 mean train loss:  3.44464439e-03, bound:  3.15330327e-01\n",
      "Epoch: 35496 mean train loss:  3.44456220e-03, bound:  3.15330327e-01\n",
      "Epoch: 35497 mean train loss:  3.44453682e-03, bound:  3.15330327e-01\n",
      "Epoch: 35498 mean train loss:  3.44446790e-03, bound:  3.15330297e-01\n",
      "Epoch: 35499 mean train loss:  3.44439386e-03, bound:  3.15330297e-01\n",
      "Epoch: 35500 mean train loss:  3.44430725e-03, bound:  3.15330297e-01\n",
      "Epoch: 35501 mean train loss:  3.44426138e-03, bound:  3.15330297e-01\n",
      "Epoch: 35502 mean train loss:  3.44416080e-03, bound:  3.15330297e-01\n",
      "Epoch: 35503 mean train loss:  3.44414264e-03, bound:  3.15330297e-01\n",
      "Epoch: 35504 mean train loss:  3.44405067e-03, bound:  3.15330237e-01\n",
      "Epoch: 35505 mean train loss:  3.44398874e-03, bound:  3.15330237e-01\n",
      "Epoch: 35506 mean train loss:  3.44392727e-03, bound:  3.15330237e-01\n",
      "Epoch: 35507 mean train loss:  3.44386394e-03, bound:  3.15330237e-01\n",
      "Epoch: 35508 mean train loss:  3.44381062e-03, bound:  3.15330237e-01\n",
      "Epoch: 35509 mean train loss:  3.44369351e-03, bound:  3.15330237e-01\n",
      "Epoch: 35510 mean train loss:  3.44367931e-03, bound:  3.15330237e-01\n",
      "Epoch: 35511 mean train loss:  3.44362040e-03, bound:  3.15330207e-01\n",
      "Epoch: 35512 mean train loss:  3.44354566e-03, bound:  3.15330207e-01\n",
      "Epoch: 35513 mean train loss:  3.44346697e-03, bound:  3.15330207e-01\n",
      "Epoch: 35514 mean train loss:  3.44339409e-03, bound:  3.15330207e-01\n",
      "Epoch: 35515 mean train loss:  3.44335707e-03, bound:  3.15330207e-01\n",
      "Epoch: 35516 mean train loss:  3.44328792e-03, bound:  3.15330178e-01\n",
      "Epoch: 35517 mean train loss:  3.44320969e-03, bound:  3.15330178e-01\n",
      "Epoch: 35518 mean train loss:  3.44312354e-03, bound:  3.15330178e-01\n",
      "Epoch: 35519 mean train loss:  3.44307884e-03, bound:  3.15330178e-01\n",
      "Epoch: 35520 mean train loss:  3.44301481e-03, bound:  3.15330178e-01\n",
      "Epoch: 35521 mean train loss:  3.44292959e-03, bound:  3.15330148e-01\n",
      "Epoch: 35522 mean train loss:  3.44289467e-03, bound:  3.15330148e-01\n",
      "Epoch: 35523 mean train loss:  3.44284903e-03, bound:  3.15330148e-01\n",
      "Epoch: 35524 mean train loss:  3.44278011e-03, bound:  3.15330148e-01\n",
      "Epoch: 35525 mean train loss:  3.44269979e-03, bound:  3.15330148e-01\n",
      "Epoch: 35526 mean train loss:  3.44264205e-03, bound:  3.15330148e-01\n",
      "Epoch: 35527 mean train loss:  3.44258570e-03, bound:  3.15330088e-01\n",
      "Epoch: 35528 mean train loss:  3.44250281e-03, bound:  3.15330088e-01\n",
      "Epoch: 35529 mean train loss:  3.44242505e-03, bound:  3.15330088e-01\n",
      "Epoch: 35530 mean train loss:  3.44237499e-03, bound:  3.15330088e-01\n",
      "Epoch: 35531 mean train loss:  3.44227976e-03, bound:  3.15330088e-01\n",
      "Epoch: 35532 mean train loss:  3.44221457e-03, bound:  3.15330088e-01\n",
      "Epoch: 35533 mean train loss:  3.44220875e-03, bound:  3.15330058e-01\n",
      "Epoch: 35534 mean train loss:  3.44211864e-03, bound:  3.15330058e-01\n",
      "Epoch: 35535 mean train loss:  3.44206207e-03, bound:  3.15330058e-01\n",
      "Epoch: 35536 mean train loss:  3.44197103e-03, bound:  3.15330058e-01\n",
      "Epoch: 35537 mean train loss:  3.44195752e-03, bound:  3.15330058e-01\n",
      "Epoch: 35538 mean train loss:  3.44190374e-03, bound:  3.15330058e-01\n",
      "Epoch: 35539 mean train loss:  3.44185275e-03, bound:  3.15330029e-01\n",
      "Epoch: 35540 mean train loss:  3.44173284e-03, bound:  3.15330029e-01\n",
      "Epoch: 35541 mean train loss:  3.44170001e-03, bound:  3.15330029e-01\n",
      "Epoch: 35542 mean train loss:  3.44167789e-03, bound:  3.15330029e-01\n",
      "Epoch: 35543 mean train loss:  3.44166206e-03, bound:  3.15330029e-01\n",
      "Epoch: 35544 mean train loss:  3.44163273e-03, bound:  3.15329999e-01\n",
      "Epoch: 35545 mean train loss:  3.44160688e-03, bound:  3.15329969e-01\n",
      "Epoch: 35546 mean train loss:  3.44162597e-03, bound:  3.15329969e-01\n",
      "Epoch: 35547 mean train loss:  3.44150350e-03, bound:  3.15329969e-01\n",
      "Epoch: 35548 mean train loss:  3.44145368e-03, bound:  3.15329969e-01\n",
      "Epoch: 35549 mean train loss:  3.44133051e-03, bound:  3.15329969e-01\n",
      "Epoch: 35550 mean train loss:  3.44118662e-03, bound:  3.15329969e-01\n",
      "Epoch: 35551 mean train loss:  3.44108464e-03, bound:  3.15329969e-01\n",
      "Epoch: 35552 mean train loss:  3.44099454e-03, bound:  3.15329969e-01\n",
      "Epoch: 35553 mean train loss:  3.44092213e-03, bound:  3.15329969e-01\n",
      "Epoch: 35554 mean train loss:  3.44088161e-03, bound:  3.15329969e-01\n",
      "Epoch: 35555 mean train loss:  3.44081339e-03, bound:  3.15329969e-01\n",
      "Epoch: 35556 mean train loss:  3.44089209e-03, bound:  3.15329880e-01\n",
      "Epoch: 35557 mean train loss:  3.44073726e-03, bound:  3.15329880e-01\n",
      "Epoch: 35558 mean train loss:  3.44065484e-03, bound:  3.15329880e-01\n",
      "Epoch: 35559 mean train loss:  3.44059104e-03, bound:  3.15329880e-01\n",
      "Epoch: 35560 mean train loss:  3.44050373e-03, bound:  3.15329880e-01\n",
      "Epoch: 35561 mean train loss:  3.44042410e-03, bound:  3.15329880e-01\n",
      "Epoch: 35562 mean train loss:  3.44038103e-03, bound:  3.15329850e-01\n",
      "Epoch: 35563 mean train loss:  3.44028743e-03, bound:  3.15329850e-01\n",
      "Epoch: 35564 mean train loss:  3.44023923e-03, bound:  3.15329850e-01\n",
      "Epoch: 35565 mean train loss:  3.44017777e-03, bound:  3.15329850e-01\n",
      "Epoch: 35566 mean train loss:  3.44013376e-03, bound:  3.15329850e-01\n",
      "Epoch: 35567 mean train loss:  3.44009139e-03, bound:  3.15329850e-01\n",
      "Epoch: 35568 mean train loss:  3.43996612e-03, bound:  3.15329850e-01\n",
      "Epoch: 35569 mean train loss:  3.43994680e-03, bound:  3.15329850e-01\n",
      "Epoch: 35570 mean train loss:  3.43985157e-03, bound:  3.15329850e-01\n",
      "Epoch: 35571 mean train loss:  3.43984296e-03, bound:  3.15329850e-01\n",
      "Epoch: 35572 mean train loss:  3.43970698e-03, bound:  3.15329850e-01\n",
      "Epoch: 35573 mean train loss:  3.43968463e-03, bound:  3.15329820e-01\n",
      "Epoch: 35574 mean train loss:  3.43965786e-03, bound:  3.15329790e-01\n",
      "Epoch: 35575 mean train loss:  3.43955657e-03, bound:  3.15329790e-01\n",
      "Epoch: 35576 mean train loss:  3.43946298e-03, bound:  3.15329790e-01\n",
      "Epoch: 35577 mean train loss:  3.43942549e-03, bound:  3.15329790e-01\n",
      "Epoch: 35578 mean train loss:  3.43933422e-03, bound:  3.15329790e-01\n",
      "Epoch: 35579 mean train loss:  3.43929091e-03, bound:  3.15329790e-01\n",
      "Epoch: 35580 mean train loss:  3.43930628e-03, bound:  3.15329731e-01\n",
      "Epoch: 35581 mean train loss:  3.43923154e-03, bound:  3.15329731e-01\n",
      "Epoch: 35582 mean train loss:  3.43912141e-03, bound:  3.15329731e-01\n",
      "Epoch: 35583 mean train loss:  3.43907112e-03, bound:  3.15329731e-01\n",
      "Epoch: 35584 mean train loss:  3.43901059e-03, bound:  3.15329731e-01\n",
      "Epoch: 35585 mean train loss:  3.43894889e-03, bound:  3.15329731e-01\n",
      "Epoch: 35586 mean train loss:  3.43883364e-03, bound:  3.15329731e-01\n",
      "Epoch: 35587 mean train loss:  3.43879778e-03, bound:  3.15329731e-01\n",
      "Epoch: 35588 mean train loss:  3.43870767e-03, bound:  3.15329731e-01\n",
      "Epoch: 35589 mean train loss:  3.43869301e-03, bound:  3.15329731e-01\n",
      "Epoch: 35590 mean train loss:  3.43863643e-03, bound:  3.15329731e-01\n",
      "Epoch: 35591 mean train loss:  3.43854516e-03, bound:  3.15329671e-01\n",
      "Epoch: 35592 mean train loss:  3.43848369e-03, bound:  3.15329671e-01\n",
      "Epoch: 35593 mean train loss:  3.43846879e-03, bound:  3.15329671e-01\n",
      "Epoch: 35594 mean train loss:  3.43836867e-03, bound:  3.15329671e-01\n",
      "Epoch: 35595 mean train loss:  3.43830418e-03, bound:  3.15329671e-01\n",
      "Epoch: 35596 mean train loss:  3.43824225e-03, bound:  3.15329671e-01\n",
      "Epoch: 35597 mean train loss:  3.43814003e-03, bound:  3.15329641e-01\n",
      "Epoch: 35598 mean train loss:  3.43811396e-03, bound:  3.15329641e-01\n",
      "Epoch: 35599 mean train loss:  3.43805971e-03, bound:  3.15329641e-01\n",
      "Epoch: 35600 mean train loss:  3.43800336e-03, bound:  3.15329641e-01\n",
      "Epoch: 35601 mean train loss:  3.43792397e-03, bound:  3.15329641e-01\n",
      "Epoch: 35602 mean train loss:  3.43784154e-03, bound:  3.15329611e-01\n",
      "Epoch: 35603 mean train loss:  3.43780452e-03, bound:  3.15329611e-01\n",
      "Epoch: 35604 mean train loss:  3.43775470e-03, bound:  3.15329611e-01\n",
      "Epoch: 35605 mean train loss:  3.43768834e-03, bound:  3.15329611e-01\n",
      "Epoch: 35606 mean train loss:  3.43761034e-03, bound:  3.15329611e-01\n",
      "Epoch: 35607 mean train loss:  3.43755167e-03, bound:  3.15329611e-01\n",
      "Epoch: 35608 mean train loss:  3.43748578e-03, bound:  3.15329552e-01\n",
      "Epoch: 35609 mean train loss:  3.43742501e-03, bound:  3.15329552e-01\n",
      "Epoch: 35610 mean train loss:  3.43738752e-03, bound:  3.15329552e-01\n",
      "Epoch: 35611 mean train loss:  3.43728717e-03, bound:  3.15329552e-01\n",
      "Epoch: 35612 mean train loss:  3.43726389e-03, bound:  3.15329552e-01\n",
      "Epoch: 35613 mean train loss:  3.43720615e-03, bound:  3.15329552e-01\n",
      "Epoch: 35614 mean train loss:  3.43711441e-03, bound:  3.15329552e-01\n",
      "Epoch: 35615 mean train loss:  3.43707041e-03, bound:  3.15329522e-01\n",
      "Epoch: 35616 mean train loss:  3.43700428e-03, bound:  3.15329522e-01\n",
      "Epoch: 35617 mean train loss:  3.43690580e-03, bound:  3.15329522e-01\n",
      "Epoch: 35618 mean train loss:  3.43685714e-03, bound:  3.15329522e-01\n",
      "Epoch: 35619 mean train loss:  3.43676331e-03, bound:  3.15329492e-01\n",
      "Epoch: 35620 mean train loss:  3.43674305e-03, bound:  3.15329492e-01\n",
      "Epoch: 35621 mean train loss:  3.43664782e-03, bound:  3.15329492e-01\n",
      "Epoch: 35622 mean train loss:  3.43663897e-03, bound:  3.15329492e-01\n",
      "Epoch: 35623 mean train loss:  3.43654607e-03, bound:  3.15329492e-01\n",
      "Epoch: 35624 mean train loss:  3.43644968e-03, bound:  3.15329492e-01\n",
      "Epoch: 35625 mean train loss:  3.43644270e-03, bound:  3.15329492e-01\n",
      "Epoch: 35626 mean train loss:  3.43635515e-03, bound:  3.15329432e-01\n",
      "Epoch: 35627 mean train loss:  3.43631813e-03, bound:  3.15329432e-01\n",
      "Epoch: 35628 mean train loss:  3.43624456e-03, bound:  3.15329432e-01\n",
      "Epoch: 35629 mean train loss:  3.43619776e-03, bound:  3.15329432e-01\n",
      "Epoch: 35630 mean train loss:  3.43609625e-03, bound:  3.15329432e-01\n",
      "Epoch: 35631 mean train loss:  3.43609578e-03, bound:  3.15329403e-01\n",
      "Epoch: 35632 mean train loss:  3.43598565e-03, bound:  3.15329403e-01\n",
      "Epoch: 35633 mean train loss:  3.43595655e-03, bound:  3.15329403e-01\n",
      "Epoch: 35634 mean train loss:  3.43584316e-03, bound:  3.15329403e-01\n",
      "Epoch: 35635 mean train loss:  3.43578984e-03, bound:  3.15329403e-01\n",
      "Epoch: 35636 mean train loss:  3.43573256e-03, bound:  3.15329403e-01\n",
      "Epoch: 35637 mean train loss:  3.43561941e-03, bound:  3.15329403e-01\n",
      "Epoch: 35638 mean train loss:  3.43557703e-03, bound:  3.15329373e-01\n",
      "Epoch: 35639 mean train loss:  3.43552534e-03, bound:  3.15329373e-01\n",
      "Epoch: 35640 mean train loss:  3.43547342e-03, bound:  3.15329373e-01\n",
      "Epoch: 35641 mean train loss:  3.43543594e-03, bound:  3.15329373e-01\n",
      "Epoch: 35642 mean train loss:  3.43535980e-03, bound:  3.15329313e-01\n",
      "Epoch: 35643 mean train loss:  3.43528553e-03, bound:  3.15329313e-01\n",
      "Epoch: 35644 mean train loss:  3.43520218e-03, bound:  3.15329313e-01\n",
      "Epoch: 35645 mean train loss:  3.43514117e-03, bound:  3.15329313e-01\n",
      "Epoch: 35646 mean train loss:  3.43516096e-03, bound:  3.15329313e-01\n",
      "Epoch: 35647 mean train loss:  3.43502057e-03, bound:  3.15329313e-01\n",
      "Epoch: 35648 mean train loss:  3.43495817e-03, bound:  3.15329313e-01\n",
      "Epoch: 35649 mean train loss:  3.43490229e-03, bound:  3.15329313e-01\n",
      "Epoch: 35650 mean train loss:  3.43483174e-03, bound:  3.15329283e-01\n",
      "Epoch: 35651 mean train loss:  3.43478448e-03, bound:  3.15329283e-01\n",
      "Epoch: 35652 mean train loss:  3.43472115e-03, bound:  3.15329283e-01\n",
      "Epoch: 35653 mean train loss:  3.43469228e-03, bound:  3.15329283e-01\n",
      "Epoch: 35654 mean train loss:  3.43459146e-03, bound:  3.15329283e-01\n",
      "Epoch: 35655 mean train loss:  3.43454769e-03, bound:  3.15329254e-01\n",
      "Epoch: 35656 mean train loss:  3.43446038e-03, bound:  3.15329254e-01\n",
      "Epoch: 35657 mean train loss:  3.43438867e-03, bound:  3.15329254e-01\n",
      "Epoch: 35658 mean train loss:  3.43432650e-03, bound:  3.15329254e-01\n",
      "Epoch: 35659 mean train loss:  3.43429763e-03, bound:  3.15329254e-01\n",
      "Epoch: 35660 mean train loss:  3.43420589e-03, bound:  3.15329254e-01\n",
      "Epoch: 35661 mean train loss:  3.43411556e-03, bound:  3.15329254e-01\n",
      "Epoch: 35662 mean train loss:  3.43410810e-03, bound:  3.15329194e-01\n",
      "Epoch: 35663 mean train loss:  3.43400775e-03, bound:  3.15329194e-01\n",
      "Epoch: 35664 mean train loss:  3.43397702e-03, bound:  3.15329194e-01\n",
      "Epoch: 35665 mean train loss:  3.43389832e-03, bound:  3.15329194e-01\n",
      "Epoch: 35666 mean train loss:  3.43382917e-03, bound:  3.15329194e-01\n",
      "Epoch: 35667 mean train loss:  3.43376840e-03, bound:  3.15329164e-01\n",
      "Epoch: 35668 mean train loss:  3.43368389e-03, bound:  3.15329164e-01\n",
      "Epoch: 35669 mean train loss:  3.43366666e-03, bound:  3.15329164e-01\n",
      "Epoch: 35670 mean train loss:  3.43361450e-03, bound:  3.15329164e-01\n",
      "Epoch: 35671 mean train loss:  3.43356002e-03, bound:  3.15329164e-01\n",
      "Epoch: 35672 mean train loss:  3.43344524e-03, bound:  3.15329164e-01\n",
      "Epoch: 35673 mean train loss:  3.43341241e-03, bound:  3.15329134e-01\n",
      "Epoch: 35674 mean train loss:  3.43335164e-03, bound:  3.15329134e-01\n",
      "Epoch: 35675 mean train loss:  3.43330530e-03, bound:  3.15329134e-01\n",
      "Epoch: 35676 mean train loss:  3.43324849e-03, bound:  3.15329134e-01\n",
      "Epoch: 35677 mean train loss:  3.43313138e-03, bound:  3.15329134e-01\n",
      "Epoch: 35678 mean train loss:  3.43311043e-03, bound:  3.15329134e-01\n",
      "Epoch: 35679 mean train loss:  3.43301147e-03, bound:  3.15329105e-01\n",
      "Epoch: 35680 mean train loss:  3.43294372e-03, bound:  3.15329105e-01\n",
      "Epoch: 35681 mean train loss:  3.43288784e-03, bound:  3.15329105e-01\n",
      "Epoch: 35682 mean train loss:  3.43281147e-03, bound:  3.15329105e-01\n",
      "Epoch: 35683 mean train loss:  3.43277305e-03, bound:  3.15329105e-01\n",
      "Epoch: 35684 mean train loss:  3.43272998e-03, bound:  3.15329075e-01\n",
      "Epoch: 35685 mean train loss:  3.43264895e-03, bound:  3.15329045e-01\n",
      "Epoch: 35686 mean train loss:  3.43258283e-03, bound:  3.15329045e-01\n",
      "Epoch: 35687 mean train loss:  3.43253417e-03, bound:  3.15329045e-01\n",
      "Epoch: 35688 mean train loss:  3.43250157e-03, bound:  3.15329045e-01\n",
      "Epoch: 35689 mean train loss:  3.43243522e-03, bound:  3.15329045e-01\n",
      "Epoch: 35690 mean train loss:  3.43236653e-03, bound:  3.15329045e-01\n",
      "Epoch: 35691 mean train loss:  3.43230832e-03, bound:  3.15329045e-01\n",
      "Epoch: 35692 mean train loss:  3.43224360e-03, bound:  3.15329045e-01\n",
      "Epoch: 35693 mean train loss:  3.43219889e-03, bound:  3.15329045e-01\n",
      "Epoch: 35694 mean train loss:  3.43206432e-03, bound:  3.15329045e-01\n",
      "Epoch: 35695 mean train loss:  3.43200890e-03, bound:  3.15329045e-01\n",
      "Epoch: 35696 mean train loss:  3.43194790e-03, bound:  3.15328985e-01\n",
      "Epoch: 35697 mean train loss:  3.43186478e-03, bound:  3.15328985e-01\n",
      "Epoch: 35698 mean train loss:  3.43184336e-03, bound:  3.15328985e-01\n",
      "Epoch: 35699 mean train loss:  3.43176746e-03, bound:  3.15328985e-01\n",
      "Epoch: 35700 mean train loss:  3.43170715e-03, bound:  3.15328985e-01\n",
      "Epoch: 35701 mean train loss:  3.43162520e-03, bound:  3.15328985e-01\n",
      "Epoch: 35702 mean train loss:  3.43157793e-03, bound:  3.15328926e-01\n",
      "Epoch: 35703 mean train loss:  3.43151856e-03, bound:  3.15328926e-01\n",
      "Epoch: 35704 mean train loss:  3.43143172e-03, bound:  3.15328926e-01\n",
      "Epoch: 35705 mean train loss:  3.43134627e-03, bound:  3.15328926e-01\n",
      "Epoch: 35706 mean train loss:  3.43131693e-03, bound:  3.15328926e-01\n",
      "Epoch: 35707 mean train loss:  3.43124592e-03, bound:  3.15328926e-01\n",
      "Epoch: 35708 mean train loss:  3.43115907e-03, bound:  3.15328926e-01\n",
      "Epoch: 35709 mean train loss:  3.43115372e-03, bound:  3.15328926e-01\n",
      "Epoch: 35710 mean train loss:  3.43106803e-03, bound:  3.15328926e-01\n",
      "Epoch: 35711 mean train loss:  3.43101844e-03, bound:  3.15328926e-01\n",
      "Epoch: 35712 mean train loss:  3.43093951e-03, bound:  3.15328926e-01\n",
      "Epoch: 35713 mean train loss:  3.43090435e-03, bound:  3.15328866e-01\n",
      "Epoch: 35714 mean train loss:  3.43083567e-03, bound:  3.15328866e-01\n",
      "Epoch: 35715 mean train loss:  3.43080261e-03, bound:  3.15328866e-01\n",
      "Epoch: 35716 mean train loss:  3.43067921e-03, bound:  3.15328866e-01\n",
      "Epoch: 35717 mean train loss:  3.43062729e-03, bound:  3.15328866e-01\n",
      "Epoch: 35718 mean train loss:  3.43066244e-03, bound:  3.15328866e-01\n",
      "Epoch: 35719 mean train loss:  3.43059120e-03, bound:  3.15328836e-01\n",
      "Epoch: 35720 mean train loss:  3.43051390e-03, bound:  3.15328836e-01\n",
      "Epoch: 35721 mean train loss:  3.43046989e-03, bound:  3.15328836e-01\n",
      "Epoch: 35722 mean train loss:  3.43041541e-03, bound:  3.15328836e-01\n",
      "Epoch: 35723 mean train loss:  3.43030668e-03, bound:  3.15328836e-01\n",
      "Epoch: 35724 mean train loss:  3.43021704e-03, bound:  3.15328836e-01\n",
      "Epoch: 35725 mean train loss:  3.43017350e-03, bound:  3.15328807e-01\n",
      "Epoch: 35726 mean train loss:  3.43007874e-03, bound:  3.15328807e-01\n",
      "Epoch: 35727 mean train loss:  3.43002193e-03, bound:  3.15328807e-01\n",
      "Epoch: 35728 mean train loss:  3.42994346e-03, bound:  3.15328807e-01\n",
      "Epoch: 35729 mean train loss:  3.42983752e-03, bound:  3.15328747e-01\n",
      "Epoch: 35730 mean train loss:  3.42982425e-03, bound:  3.15328807e-01\n",
      "Epoch: 35731 mean train loss:  3.42970085e-03, bound:  3.15328747e-01\n",
      "Epoch: 35732 mean train loss:  3.42967617e-03, bound:  3.15328747e-01\n",
      "Epoch: 35733 mean train loss:  3.42960446e-03, bound:  3.15328747e-01\n",
      "Epoch: 35734 mean train loss:  3.42957582e-03, bound:  3.15328747e-01\n",
      "Epoch: 35735 mean train loss:  3.42950970e-03, bound:  3.15328747e-01\n",
      "Epoch: 35736 mean train loss:  3.42947990e-03, bound:  3.15328717e-01\n",
      "Epoch: 35737 mean train loss:  3.42940469e-03, bound:  3.15328717e-01\n",
      "Epoch: 35738 mean train loss:  3.42929061e-03, bound:  3.15328717e-01\n",
      "Epoch: 35739 mean train loss:  3.42926453e-03, bound:  3.15328717e-01\n",
      "Epoch: 35740 mean train loss:  3.42920050e-03, bound:  3.15328717e-01\n",
      "Epoch: 35741 mean train loss:  3.42916162e-03, bound:  3.15328717e-01\n",
      "Epoch: 35742 mean train loss:  3.42905731e-03, bound:  3.15328687e-01\n",
      "Epoch: 35743 mean train loss:  3.42905638e-03, bound:  3.15328687e-01\n",
      "Epoch: 35744 mean train loss:  3.42895277e-03, bound:  3.15328687e-01\n",
      "Epoch: 35745 mean train loss:  3.42891342e-03, bound:  3.15328687e-01\n",
      "Epoch: 35746 mean train loss:  3.42882704e-03, bound:  3.15328687e-01\n",
      "Epoch: 35747 mean train loss:  3.42879328e-03, bound:  3.15328628e-01\n",
      "Epoch: 35748 mean train loss:  3.42867803e-03, bound:  3.15328628e-01\n",
      "Epoch: 35749 mean train loss:  3.42865498e-03, bound:  3.15328628e-01\n",
      "Epoch: 35750 mean train loss:  3.42859002e-03, bound:  3.15328628e-01\n",
      "Epoch: 35751 mean train loss:  3.42851365e-03, bound:  3.15328628e-01\n",
      "Epoch: 35752 mean train loss:  3.42849549e-03, bound:  3.15328628e-01\n",
      "Epoch: 35753 mean train loss:  3.42834834e-03, bound:  3.15328628e-01\n",
      "Epoch: 35754 mean train loss:  3.42833181e-03, bound:  3.15328598e-01\n",
      "Epoch: 35755 mean train loss:  3.42825917e-03, bound:  3.15328598e-01\n",
      "Epoch: 35756 mean train loss:  3.42822145e-03, bound:  3.15328598e-01\n",
      "Epoch: 35757 mean train loss:  3.42813437e-03, bound:  3.15328598e-01\n",
      "Epoch: 35758 mean train loss:  3.42811574e-03, bound:  3.15328598e-01\n",
      "Epoch: 35759 mean train loss:  3.42803099e-03, bound:  3.15328598e-01\n",
      "Epoch: 35760 mean train loss:  3.42800235e-03, bound:  3.15328568e-01\n",
      "Epoch: 35761 mean train loss:  3.42792459e-03, bound:  3.15328568e-01\n",
      "Epoch: 35762 mean train loss:  3.42781330e-03, bound:  3.15328568e-01\n",
      "Epoch: 35763 mean train loss:  3.42780631e-03, bound:  3.15328568e-01\n",
      "Epoch: 35764 mean train loss:  3.42769246e-03, bound:  3.15328568e-01\n",
      "Epoch: 35765 mean train loss:  3.42761050e-03, bound:  3.15328509e-01\n",
      "Epoch: 35766 mean train loss:  3.42761166e-03, bound:  3.15328509e-01\n",
      "Epoch: 35767 mean train loss:  3.42752365e-03, bound:  3.15328509e-01\n",
      "Epoch: 35768 mean train loss:  3.42742377e-03, bound:  3.15328509e-01\n",
      "Epoch: 35769 mean train loss:  3.42739630e-03, bound:  3.15328509e-01\n",
      "Epoch: 35770 mean train loss:  3.42730270e-03, bound:  3.15328509e-01\n",
      "Epoch: 35771 mean train loss:  3.42730782e-03, bound:  3.15328479e-01\n",
      "Epoch: 35772 mean train loss:  3.42722097e-03, bound:  3.15328479e-01\n",
      "Epoch: 35773 mean train loss:  3.42714507e-03, bound:  3.15328479e-01\n",
      "Epoch: 35774 mean train loss:  3.42707755e-03, bound:  3.15328479e-01\n",
      "Epoch: 35775 mean train loss:  3.42700724e-03, bound:  3.15328479e-01\n",
      "Epoch: 35776 mean train loss:  3.42692644e-03, bound:  3.15328419e-01\n",
      "Epoch: 35777 mean train loss:  3.42693250e-03, bound:  3.15328419e-01\n",
      "Epoch: 35778 mean train loss:  3.42683285e-03, bound:  3.15328419e-01\n",
      "Epoch: 35779 mean train loss:  3.42679583e-03, bound:  3.15328419e-01\n",
      "Epoch: 35780 mean train loss:  3.42667685e-03, bound:  3.15328419e-01\n",
      "Epoch: 35781 mean train loss:  3.42659140e-03, bound:  3.15328419e-01\n",
      "Epoch: 35782 mean train loss:  3.42658139e-03, bound:  3.15328419e-01\n",
      "Epoch: 35783 mean train loss:  3.42651224e-03, bound:  3.15328419e-01\n",
      "Epoch: 35784 mean train loss:  3.42646963e-03, bound:  3.15328419e-01\n",
      "Epoch: 35785 mean train loss:  3.42636649e-03, bound:  3.15328419e-01\n",
      "Epoch: 35786 mean train loss:  3.42631852e-03, bound:  3.15328419e-01\n",
      "Epoch: 35787 mean train loss:  3.42623936e-03, bound:  3.15328419e-01\n",
      "Epoch: 35788 mean train loss:  3.42618162e-03, bound:  3.15328389e-01\n",
      "Epoch: 35789 mean train loss:  3.42610781e-03, bound:  3.15328360e-01\n",
      "Epoch: 35790 mean train loss:  3.42608406e-03, bound:  3.15328360e-01\n",
      "Epoch: 35791 mean train loss:  3.42599559e-03, bound:  3.15328360e-01\n",
      "Epoch: 35792 mean train loss:  3.42598744e-03, bound:  3.15328360e-01\n",
      "Epoch: 35793 mean train loss:  3.42589593e-03, bound:  3.15328360e-01\n",
      "Epoch: 35794 mean train loss:  3.42581910e-03, bound:  3.15328360e-01\n",
      "Epoch: 35795 mean train loss:  3.42577370e-03, bound:  3.15328300e-01\n",
      "Epoch: 35796 mean train loss:  3.42565356e-03, bound:  3.15328300e-01\n",
      "Epoch: 35797 mean train loss:  3.42564890e-03, bound:  3.15328300e-01\n",
      "Epoch: 35798 mean train loss:  3.42555507e-03, bound:  3.15328300e-01\n",
      "Epoch: 35799 mean train loss:  3.42554715e-03, bound:  3.15328300e-01\n",
      "Epoch: 35800 mean train loss:  3.42548382e-03, bound:  3.15328300e-01\n",
      "Epoch: 35801 mean train loss:  3.42540280e-03, bound:  3.15328300e-01\n",
      "Epoch: 35802 mean train loss:  3.42531526e-03, bound:  3.15328300e-01\n",
      "Epoch: 35803 mean train loss:  3.42527032e-03, bound:  3.15328300e-01\n",
      "Epoch: 35804 mean train loss:  3.42522701e-03, bound:  3.15328300e-01\n",
      "Epoch: 35805 mean train loss:  3.42518138e-03, bound:  3.15328300e-01\n",
      "Epoch: 35806 mean train loss:  3.42509709e-03, bound:  3.15328270e-01\n",
      "Epoch: 35807 mean train loss:  3.42501677e-03, bound:  3.15328240e-01\n",
      "Epoch: 35808 mean train loss:  3.42500792e-03, bound:  3.15328240e-01\n",
      "Epoch: 35809 mean train loss:  3.42487125e-03, bound:  3.15328240e-01\n",
      "Epoch: 35810 mean train loss:  3.42480536e-03, bound:  3.15328240e-01\n",
      "Epoch: 35811 mean train loss:  3.42476577e-03, bound:  3.15328240e-01\n",
      "Epoch: 35812 mean train loss:  3.42468661e-03, bound:  3.15328240e-01\n",
      "Epoch: 35813 mean train loss:  3.42463120e-03, bound:  3.15328240e-01\n",
      "Epoch: 35814 mean train loss:  3.42460047e-03, bound:  3.15328240e-01\n",
      "Epoch: 35815 mean train loss:  3.42454691e-03, bound:  3.15328240e-01\n",
      "Epoch: 35816 mean train loss:  3.42444959e-03, bound:  3.15328240e-01\n",
      "Epoch: 35817 mean train loss:  3.42435972e-03, bound:  3.15328240e-01\n",
      "Epoch: 35818 mean train loss:  3.42432898e-03, bound:  3.15328181e-01\n",
      "Epoch: 35819 mean train loss:  3.42432177e-03, bound:  3.15328181e-01\n",
      "Epoch: 35820 mean train loss:  3.42419697e-03, bound:  3.15328181e-01\n",
      "Epoch: 35821 mean train loss:  3.42415785e-03, bound:  3.15328181e-01\n",
      "Epoch: 35822 mean train loss:  3.42412409e-03, bound:  3.15328181e-01\n",
      "Epoch: 35823 mean train loss:  3.42401280e-03, bound:  3.15328151e-01\n",
      "Epoch: 35824 mean train loss:  3.42397788e-03, bound:  3.15328151e-01\n",
      "Epoch: 35825 mean train loss:  3.42394877e-03, bound:  3.15328151e-01\n",
      "Epoch: 35826 mean train loss:  3.42389219e-03, bound:  3.15328151e-01\n",
      "Epoch: 35827 mean train loss:  3.42383864e-03, bound:  3.15328151e-01\n",
      "Epoch: 35828 mean train loss:  3.42380442e-03, bound:  3.15328151e-01\n",
      "Epoch: 35829 mean train loss:  3.42369406e-03, bound:  3.15328151e-01\n",
      "Epoch: 35830 mean train loss:  3.42366914e-03, bound:  3.15328121e-01\n",
      "Epoch: 35831 mean train loss:  3.42356064e-03, bound:  3.15328121e-01\n",
      "Epoch: 35832 mean train loss:  3.42347682e-03, bound:  3.15328121e-01\n",
      "Epoch: 35833 mean train loss:  3.42341070e-03, bound:  3.15328121e-01\n",
      "Epoch: 35834 mean train loss:  3.42332525e-03, bound:  3.15328121e-01\n",
      "Epoch: 35835 mean train loss:  3.42328823e-03, bound:  3.15328062e-01\n",
      "Epoch: 35836 mean train loss:  3.42321675e-03, bound:  3.15328062e-01\n",
      "Epoch: 35837 mean train loss:  3.42317414e-03, bound:  3.15328062e-01\n",
      "Epoch: 35838 mean train loss:  3.42313107e-03, bound:  3.15328062e-01\n",
      "Epoch: 35839 mean train loss:  3.42303445e-03, bound:  3.15328062e-01\n",
      "Epoch: 35840 mean train loss:  3.42296739e-03, bound:  3.15328062e-01\n",
      "Epoch: 35841 mean train loss:  3.42291920e-03, bound:  3.15328032e-01\n",
      "Epoch: 35842 mean train loss:  3.42286192e-03, bound:  3.15328032e-01\n",
      "Epoch: 35843 mean train loss:  3.42283910e-03, bound:  3.15328032e-01\n",
      "Epoch: 35844 mean train loss:  3.42274574e-03, bound:  3.15328032e-01\n",
      "Epoch: 35845 mean train loss:  3.42266215e-03, bound:  3.15328032e-01\n",
      "Epoch: 35846 mean train loss:  3.42256902e-03, bound:  3.15328032e-01\n",
      "Epoch: 35847 mean train loss:  3.42250848e-03, bound:  3.15328032e-01\n",
      "Epoch: 35848 mean train loss:  3.42245330e-03, bound:  3.15327972e-01\n",
      "Epoch: 35849 mean train loss:  3.42239346e-03, bound:  3.15327972e-01\n",
      "Epoch: 35850 mean train loss:  3.42234573e-03, bound:  3.15327972e-01\n",
      "Epoch: 35851 mean train loss:  3.42230895e-03, bound:  3.15327972e-01\n",
      "Epoch: 35852 mean train loss:  3.42225470e-03, bound:  3.15327972e-01\n",
      "Epoch: 35853 mean train loss:  3.42217507e-03, bound:  3.15327972e-01\n",
      "Epoch: 35854 mean train loss:  3.42213223e-03, bound:  3.15327942e-01\n",
      "Epoch: 35855 mean train loss:  3.42202163e-03, bound:  3.15327942e-01\n",
      "Epoch: 35856 mean train loss:  3.42194736e-03, bound:  3.15327942e-01\n",
      "Epoch: 35857 mean train loss:  3.42191639e-03, bound:  3.15327942e-01\n",
      "Epoch: 35858 mean train loss:  3.42182489e-03, bound:  3.15327942e-01\n",
      "Epoch: 35859 mean train loss:  3.42176994e-03, bound:  3.15327942e-01\n",
      "Epoch: 35860 mean train loss:  3.42174270e-03, bound:  3.15327913e-01\n",
      "Epoch: 35861 mean train loss:  3.42163607e-03, bound:  3.15327913e-01\n",
      "Epoch: 35862 mean train loss:  3.42162349e-03, bound:  3.15327913e-01\n",
      "Epoch: 35863 mean train loss:  3.42158182e-03, bound:  3.15327913e-01\n",
      "Epoch: 35864 mean train loss:  3.42151779e-03, bound:  3.15327913e-01\n",
      "Epoch: 35865 mean train loss:  3.42143606e-03, bound:  3.15327913e-01\n",
      "Epoch: 35866 mean train loss:  3.42135713e-03, bound:  3.15327853e-01\n",
      "Epoch: 35867 mean train loss:  3.42130824e-03, bound:  3.15327853e-01\n",
      "Epoch: 35868 mean train loss:  3.42120975e-03, bound:  3.15327853e-01\n",
      "Epoch: 35869 mean train loss:  3.42121813e-03, bound:  3.15327853e-01\n",
      "Epoch: 35870 mean train loss:  3.42104048e-03, bound:  3.15327853e-01\n",
      "Epoch: 35871 mean train loss:  3.42105189e-03, bound:  3.15327853e-01\n",
      "Epoch: 35872 mean train loss:  3.42098367e-03, bound:  3.15327823e-01\n",
      "Epoch: 35873 mean train loss:  3.42095643e-03, bound:  3.15327823e-01\n",
      "Epoch: 35874 mean train loss:  3.42086679e-03, bound:  3.15327823e-01\n",
      "Epoch: 35875 mean train loss:  3.42082721e-03, bound:  3.15327823e-01\n",
      "Epoch: 35876 mean train loss:  3.42076342e-03, bound:  3.15327823e-01\n",
      "Epoch: 35877 mean train loss:  3.42067378e-03, bound:  3.15327823e-01\n",
      "Epoch: 35878 mean train loss:  3.42063140e-03, bound:  3.15327823e-01\n",
      "Epoch: 35879 mean train loss:  3.42054595e-03, bound:  3.15327793e-01\n",
      "Epoch: 35880 mean train loss:  3.42048984e-03, bound:  3.15327793e-01\n",
      "Epoch: 35881 mean train loss:  3.42041510e-03, bound:  3.15327793e-01\n",
      "Epoch: 35882 mean train loss:  3.42036062e-03, bound:  3.15327793e-01\n",
      "Epoch: 35883 mean train loss:  3.42028099e-03, bound:  3.15327734e-01\n",
      "Epoch: 35884 mean train loss:  3.42027214e-03, bound:  3.15327734e-01\n",
      "Epoch: 35885 mean train loss:  3.42014758e-03, bound:  3.15327734e-01\n",
      "Epoch: 35886 mean train loss:  3.42011708e-03, bound:  3.15327734e-01\n",
      "Epoch: 35887 mean train loss:  3.42011102e-03, bound:  3.15327734e-01\n",
      "Epoch: 35888 mean train loss:  3.41997575e-03, bound:  3.15327734e-01\n",
      "Epoch: 35889 mean train loss:  3.41995014e-03, bound:  3.15327704e-01\n",
      "Epoch: 35890 mean train loss:  3.41983093e-03, bound:  3.15327704e-01\n",
      "Epoch: 35891 mean train loss:  3.41979810e-03, bound:  3.15327704e-01\n",
      "Epoch: 35892 mean train loss:  3.41968983e-03, bound:  3.15327704e-01\n",
      "Epoch: 35893 mean train loss:  3.41968774e-03, bound:  3.15327704e-01\n",
      "Epoch: 35894 mean train loss:  3.41959344e-03, bound:  3.15327704e-01\n",
      "Epoch: 35895 mean train loss:  3.41955037e-03, bound:  3.15327674e-01\n",
      "Epoch: 35896 mean train loss:  3.41947377e-03, bound:  3.15327674e-01\n",
      "Epoch: 35897 mean train loss:  3.41947121e-03, bound:  3.15327674e-01\n",
      "Epoch: 35898 mean train loss:  3.41939228e-03, bound:  3.15327674e-01\n",
      "Epoch: 35899 mean train loss:  3.41933547e-03, bound:  3.15327674e-01\n",
      "Epoch: 35900 mean train loss:  3.41927819e-03, bound:  3.15327674e-01\n",
      "Epoch: 35901 mean train loss:  3.41921113e-03, bound:  3.15327674e-01\n",
      "Epoch: 35902 mean train loss:  3.41917621e-03, bound:  3.15327615e-01\n",
      "Epoch: 35903 mean train loss:  3.41904536e-03, bound:  3.15327615e-01\n",
      "Epoch: 35904 mean train loss:  3.41903721e-03, bound:  3.15327615e-01\n",
      "Epoch: 35905 mean train loss:  3.41896131e-03, bound:  3.15327615e-01\n",
      "Epoch: 35906 mean train loss:  3.41891241e-03, bound:  3.15327615e-01\n",
      "Epoch: 35907 mean train loss:  3.41881416e-03, bound:  3.15327615e-01\n",
      "Epoch: 35908 mean train loss:  3.41879018e-03, bound:  3.15327615e-01\n",
      "Epoch: 35909 mean train loss:  3.41865770e-03, bound:  3.15327615e-01\n",
      "Epoch: 35910 mean train loss:  3.41864373e-03, bound:  3.15327615e-01\n",
      "Epoch: 35911 mean train loss:  3.41859856e-03, bound:  3.15327615e-01\n",
      "Epoch: 35912 mean train loss:  3.41851753e-03, bound:  3.15327615e-01\n",
      "Epoch: 35913 mean train loss:  3.41848098e-03, bound:  3.15327585e-01\n",
      "Epoch: 35914 mean train loss:  3.41840624e-03, bound:  3.15327555e-01\n",
      "Epoch: 35915 mean train loss:  3.41835758e-03, bound:  3.15327555e-01\n",
      "Epoch: 35916 mean train loss:  3.41829867e-03, bound:  3.15327555e-01\n",
      "Epoch: 35917 mean train loss:  3.41822999e-03, bound:  3.15327555e-01\n",
      "Epoch: 35918 mean train loss:  3.41812312e-03, bound:  3.15327555e-01\n",
      "Epoch: 35919 mean train loss:  3.41807865e-03, bound:  3.15327555e-01\n",
      "Epoch: 35920 mean train loss:  3.41802766e-03, bound:  3.15327495e-01\n",
      "Epoch: 35921 mean train loss:  3.41795664e-03, bound:  3.15327495e-01\n",
      "Epoch: 35922 mean train loss:  3.41792149e-03, bound:  3.15327495e-01\n",
      "Epoch: 35923 mean train loss:  3.41787981e-03, bound:  3.15327495e-01\n",
      "Epoch: 35924 mean train loss:  3.41775292e-03, bound:  3.15327495e-01\n",
      "Epoch: 35925 mean train loss:  3.41772800e-03, bound:  3.15327495e-01\n",
      "Epoch: 35926 mean train loss:  3.41760763e-03, bound:  3.15327495e-01\n",
      "Epoch: 35927 mean train loss:  3.41755687e-03, bound:  3.15327495e-01\n",
      "Epoch: 35928 mean train loss:  3.41751752e-03, bound:  3.15327495e-01\n",
      "Epoch: 35929 mean train loss:  3.41742998e-03, bound:  3.15327495e-01\n",
      "Epoch: 35930 mean train loss:  3.41737852e-03, bound:  3.15327466e-01\n",
      "Epoch: 35931 mean train loss:  3.41732684e-03, bound:  3.15327466e-01\n",
      "Epoch: 35932 mean train loss:  3.41727189e-03, bound:  3.15327466e-01\n",
      "Epoch: 35933 mean train loss:  3.41719459e-03, bound:  3.15327466e-01\n",
      "Epoch: 35934 mean train loss:  3.41713591e-03, bound:  3.15327466e-01\n",
      "Epoch: 35935 mean train loss:  3.41710588e-03, bound:  3.15327466e-01\n",
      "Epoch: 35936 mean train loss:  3.41706420e-03, bound:  3.15327466e-01\n",
      "Epoch: 35937 mean train loss:  3.41696176e-03, bound:  3.15327406e-01\n",
      "Epoch: 35938 mean train loss:  3.41694127e-03, bound:  3.15327406e-01\n",
      "Epoch: 35939 mean train loss:  3.41684092e-03, bound:  3.15327406e-01\n",
      "Epoch: 35940 mean train loss:  3.41675803e-03, bound:  3.15327406e-01\n",
      "Epoch: 35941 mean train loss:  3.41667072e-03, bound:  3.15327406e-01\n",
      "Epoch: 35942 mean train loss:  3.41665745e-03, bound:  3.15327376e-01\n",
      "Epoch: 35943 mean train loss:  3.41658294e-03, bound:  3.15327376e-01\n",
      "Epoch: 35944 mean train loss:  3.41651030e-03, bound:  3.15327376e-01\n",
      "Epoch: 35945 mean train loss:  3.41644115e-03, bound:  3.15327376e-01\n",
      "Epoch: 35946 mean train loss:  3.41642043e-03, bound:  3.15327376e-01\n",
      "Epoch: 35947 mean train loss:  3.41633102e-03, bound:  3.15327376e-01\n",
      "Epoch: 35948 mean train loss:  3.41628259e-03, bound:  3.15327346e-01\n",
      "Epoch: 35949 mean train loss:  3.41624534e-03, bound:  3.15327346e-01\n",
      "Epoch: 35950 mean train loss:  3.41612380e-03, bound:  3.15327346e-01\n",
      "Epoch: 35951 mean train loss:  3.41610913e-03, bound:  3.15327346e-01\n",
      "Epoch: 35952 mean train loss:  3.41600669e-03, bound:  3.15327346e-01\n",
      "Epoch: 35953 mean train loss:  3.41601064e-03, bound:  3.15327346e-01\n",
      "Epoch: 35954 mean train loss:  3.41588422e-03, bound:  3.15327287e-01\n",
      "Epoch: 35955 mean train loss:  3.41583323e-03, bound:  3.15327287e-01\n",
      "Epoch: 35956 mean train loss:  3.41578084e-03, bound:  3.15327287e-01\n",
      "Epoch: 35957 mean train loss:  3.41573008e-03, bound:  3.15327287e-01\n",
      "Epoch: 35958 mean train loss:  3.41573101e-03, bound:  3.15327287e-01\n",
      "Epoch: 35959 mean train loss:  3.41567397e-03, bound:  3.15327257e-01\n",
      "Epoch: 35960 mean train loss:  3.41560785e-03, bound:  3.15327257e-01\n",
      "Epoch: 35961 mean train loss:  3.41553288e-03, bound:  3.15327257e-01\n",
      "Epoch: 35962 mean train loss:  3.41545860e-03, bound:  3.15327257e-01\n",
      "Epoch: 35963 mean train loss:  3.41539900e-03, bound:  3.15327257e-01\n",
      "Epoch: 35964 mean train loss:  3.41529911e-03, bound:  3.15327257e-01\n",
      "Epoch: 35965 mean train loss:  3.41525860e-03, bound:  3.15327227e-01\n",
      "Epoch: 35966 mean train loss:  3.41515802e-03, bound:  3.15327227e-01\n",
      "Epoch: 35967 mean train loss:  3.41510191e-03, bound:  3.15327227e-01\n",
      "Epoch: 35968 mean train loss:  3.41502251e-03, bound:  3.15327227e-01\n",
      "Epoch: 35969 mean train loss:  3.41498130e-03, bound:  3.15327227e-01\n",
      "Epoch: 35970 mean train loss:  3.41491890e-03, bound:  3.15327227e-01\n",
      "Epoch: 35971 mean train loss:  3.41484090e-03, bound:  3.15327168e-01\n",
      "Epoch: 35972 mean train loss:  3.41476360e-03, bound:  3.15327227e-01\n",
      "Epoch: 35973 mean train loss:  3.41471517e-03, bound:  3.15327168e-01\n",
      "Epoch: 35974 mean train loss:  3.41465441e-03, bound:  3.15327168e-01\n",
      "Epoch: 35975 mean train loss:  3.41461948e-03, bound:  3.15327168e-01\n",
      "Epoch: 35976 mean train loss:  3.41456244e-03, bound:  3.15327168e-01\n",
      "Epoch: 35977 mean train loss:  3.41448211e-03, bound:  3.15327168e-01\n",
      "Epoch: 35978 mean train loss:  3.41439177e-03, bound:  3.15327138e-01\n",
      "Epoch: 35979 mean train loss:  3.41437338e-03, bound:  3.15327138e-01\n",
      "Epoch: 35980 mean train loss:  3.41427233e-03, bound:  3.15327138e-01\n",
      "Epoch: 35981 mean train loss:  3.41424067e-03, bound:  3.15327138e-01\n",
      "Epoch: 35982 mean train loss:  3.41414381e-03, bound:  3.15327138e-01\n",
      "Epoch: 35983 mean train loss:  3.41409631e-03, bound:  3.15327108e-01\n",
      "Epoch: 35984 mean train loss:  3.41402134e-03, bound:  3.15327108e-01\n",
      "Epoch: 35985 mean train loss:  3.41399596e-03, bound:  3.15327108e-01\n",
      "Epoch: 35986 mean train loss:  3.41389980e-03, bound:  3.15327108e-01\n",
      "Epoch: 35987 mean train loss:  3.41386162e-03, bound:  3.15327108e-01\n",
      "Epoch: 35988 mean train loss:  3.41379317e-03, bound:  3.15327108e-01\n",
      "Epoch: 35989 mean train loss:  3.41373356e-03, bound:  3.15327108e-01\n",
      "Epoch: 35990 mean train loss:  3.41363321e-03, bound:  3.15327048e-01\n",
      "Epoch: 35991 mean train loss:  3.41365184e-03, bound:  3.15327048e-01\n",
      "Epoch: 35992 mean train loss:  3.41356103e-03, bound:  3.15327048e-01\n",
      "Epoch: 35993 mean train loss:  3.41346744e-03, bound:  3.15327048e-01\n",
      "Epoch: 35994 mean train loss:  3.41340504e-03, bound:  3.15327048e-01\n",
      "Epoch: 35995 mean train loss:  3.41335405e-03, bound:  3.15327018e-01\n",
      "Epoch: 35996 mean train loss:  3.41325789e-03, bound:  3.15327018e-01\n",
      "Epoch: 35997 mean train loss:  3.41320666e-03, bound:  3.15327018e-01\n",
      "Epoch: 35998 mean train loss:  3.41316382e-03, bound:  3.15327018e-01\n",
      "Epoch: 35999 mean train loss:  3.41311772e-03, bound:  3.15327018e-01\n",
      "Epoch: 36000 mean train loss:  3.41306371e-03, bound:  3.15327018e-01\n",
      "Epoch: 36001 mean train loss:  3.41296196e-03, bound:  3.15327018e-01\n",
      "Epoch: 36002 mean train loss:  3.41292890e-03, bound:  3.15326989e-01\n",
      "Epoch: 36003 mean train loss:  3.41283367e-03, bound:  3.15326989e-01\n",
      "Epoch: 36004 mean train loss:  3.41279339e-03, bound:  3.15326989e-01\n",
      "Epoch: 36005 mean train loss:  3.41270887e-03, bound:  3.15326989e-01\n",
      "Epoch: 36006 mean train loss:  3.41264810e-03, bound:  3.15326959e-01\n",
      "Epoch: 36007 mean train loss:  3.41260177e-03, bound:  3.15326959e-01\n",
      "Epoch: 36008 mean train loss:  3.41256522e-03, bound:  3.15326929e-01\n",
      "Epoch: 36009 mean train loss:  3.41250049e-03, bound:  3.15326929e-01\n",
      "Epoch: 36010 mean train loss:  3.41247674e-03, bound:  3.15326929e-01\n",
      "Epoch: 36011 mean train loss:  3.41240247e-03, bound:  3.15326929e-01\n",
      "Epoch: 36012 mean train loss:  3.41232703e-03, bound:  3.15326929e-01\n",
      "Epoch: 36013 mean train loss:  3.41224065e-03, bound:  3.15326929e-01\n",
      "Epoch: 36014 mean train loss:  3.41219362e-03, bound:  3.15326899e-01\n",
      "Epoch: 36015 mean train loss:  3.41217127e-03, bound:  3.15326899e-01\n",
      "Epoch: 36016 mean train loss:  3.41206277e-03, bound:  3.15326899e-01\n",
      "Epoch: 36017 mean train loss:  3.41202179e-03, bound:  3.15326899e-01\n",
      "Epoch: 36018 mean train loss:  3.41193378e-03, bound:  3.15326899e-01\n",
      "Epoch: 36019 mean train loss:  3.41188931e-03, bound:  3.15326869e-01\n",
      "Epoch: 36020 mean train loss:  3.41185206e-03, bound:  3.15326869e-01\n",
      "Epoch: 36021 mean train loss:  3.41177476e-03, bound:  3.15326869e-01\n",
      "Epoch: 36022 mean train loss:  3.41171608e-03, bound:  3.15326869e-01\n",
      "Epoch: 36023 mean train loss:  3.41168512e-03, bound:  3.15326869e-01\n",
      "Epoch: 36024 mean train loss:  3.41161480e-03, bound:  3.15326840e-01\n",
      "Epoch: 36025 mean train loss:  3.41160269e-03, bound:  3.15326810e-01\n",
      "Epoch: 36026 mean train loss:  3.41149606e-03, bound:  3.15326840e-01\n",
      "Epoch: 36027 mean train loss:  3.41146626e-03, bound:  3.15326810e-01\n",
      "Epoch: 36028 mean train loss:  3.41131911e-03, bound:  3.15326810e-01\n",
      "Epoch: 36029 mean train loss:  3.41122784e-03, bound:  3.15326810e-01\n",
      "Epoch: 36030 mean train loss:  3.41120083e-03, bound:  3.15326810e-01\n",
      "Epoch: 36031 mean train loss:  3.41111235e-03, bound:  3.15326810e-01\n",
      "Epoch: 36032 mean train loss:  3.41103273e-03, bound:  3.15326810e-01\n",
      "Epoch: 36033 mean train loss:  3.41093983e-03, bound:  3.15326810e-01\n",
      "Epoch: 36034 mean train loss:  3.41096148e-03, bound:  3.15326810e-01\n",
      "Epoch: 36035 mean train loss:  3.41088092e-03, bound:  3.15326810e-01\n",
      "Epoch: 36036 mean train loss:  3.41080292e-03, bound:  3.15326780e-01\n",
      "Epoch: 36037 mean train loss:  3.41073866e-03, bound:  3.15326780e-01\n",
      "Epoch: 36038 mean train loss:  3.41069000e-03, bound:  3.15326780e-01\n",
      "Epoch: 36039 mean train loss:  3.41062970e-03, bound:  3.15326780e-01\n",
      "Epoch: 36040 mean train loss:  3.41055682e-03, bound:  3.15326780e-01\n",
      "Epoch: 36041 mean train loss:  3.41049233e-03, bound:  3.15326780e-01\n",
      "Epoch: 36042 mean train loss:  3.41043784e-03, bound:  3.15326780e-01\n",
      "Epoch: 36043 mean train loss:  3.41041596e-03, bound:  3.15326691e-01\n",
      "Epoch: 36044 mean train loss:  3.41028301e-03, bound:  3.15326691e-01\n",
      "Epoch: 36045 mean train loss:  3.41029209e-03, bound:  3.15326691e-01\n",
      "Epoch: 36046 mean train loss:  3.41017847e-03, bound:  3.15326691e-01\n",
      "Epoch: 36047 mean train loss:  3.41013144e-03, bound:  3.15326691e-01\n",
      "Epoch: 36048 mean train loss:  3.41004808e-03, bound:  3.15326691e-01\n",
      "Epoch: 36049 mean train loss:  3.41000455e-03, bound:  3.15326691e-01\n",
      "Epoch: 36050 mean train loss:  3.40995402e-03, bound:  3.15326691e-01\n",
      "Epoch: 36051 mean train loss:  3.40990396e-03, bound:  3.15326691e-01\n",
      "Epoch: 36052 mean train loss:  3.40983900e-03, bound:  3.15326691e-01\n",
      "Epoch: 36053 mean train loss:  3.40977730e-03, bound:  3.15326691e-01\n",
      "Epoch: 36054 mean train loss:  3.40969395e-03, bound:  3.15326691e-01\n",
      "Epoch: 36055 mean train loss:  3.40967625e-03, bound:  3.15326661e-01\n",
      "Epoch: 36056 mean train loss:  3.40953749e-03, bound:  3.15326661e-01\n",
      "Epoch: 36057 mean train loss:  3.40953283e-03, bound:  3.15326661e-01\n",
      "Epoch: 36058 mean train loss:  3.40940803e-03, bound:  3.15326661e-01\n",
      "Epoch: 36059 mean train loss:  3.40936100e-03, bound:  3.15326661e-01\n",
      "Epoch: 36060 mean train loss:  3.40933376e-03, bound:  3.15326601e-01\n",
      "Epoch: 36061 mean train loss:  3.40928673e-03, bound:  3.15326571e-01\n",
      "Epoch: 36062 mean train loss:  3.40923830e-03, bound:  3.15326571e-01\n",
      "Epoch: 36063 mean train loss:  3.40915029e-03, bound:  3.15326571e-01\n",
      "Epoch: 36064 mean train loss:  3.40908486e-03, bound:  3.15326571e-01\n",
      "Epoch: 36065 mean train loss:  3.40900873e-03, bound:  3.15326571e-01\n",
      "Epoch: 36066 mean train loss:  3.40897823e-03, bound:  3.15326571e-01\n",
      "Epoch: 36067 mean train loss:  3.40892724e-03, bound:  3.15326571e-01\n",
      "Epoch: 36068 mean train loss:  3.40886903e-03, bound:  3.15326571e-01\n",
      "Epoch: 36069 mean train loss:  3.40878498e-03, bound:  3.15326571e-01\n",
      "Epoch: 36070 mean train loss:  3.40872258e-03, bound:  3.15326571e-01\n",
      "Epoch: 36071 mean train loss:  3.40863084e-03, bound:  3.15326542e-01\n",
      "Epoch: 36072 mean train loss:  3.40857380e-03, bound:  3.15326542e-01\n",
      "Epoch: 36073 mean train loss:  3.40850605e-03, bound:  3.15326512e-01\n",
      "Epoch: 36074 mean train loss:  3.40849999e-03, bound:  3.15326512e-01\n",
      "Epoch: 36075 mean train loss:  3.40842223e-03, bound:  3.15326512e-01\n",
      "Epoch: 36076 mean train loss:  3.40831955e-03, bound:  3.15326512e-01\n",
      "Epoch: 36077 mean train loss:  3.40829208e-03, bound:  3.15326512e-01\n",
      "Epoch: 36078 mean train loss:  3.40825203e-03, bound:  3.15326482e-01\n",
      "Epoch: 36079 mean train loss:  3.40817310e-03, bound:  3.15326482e-01\n",
      "Epoch: 36080 mean train loss:  3.40813142e-03, bound:  3.15326482e-01\n",
      "Epoch: 36081 mean train loss:  3.40806157e-03, bound:  3.15326482e-01\n",
      "Epoch: 36082 mean train loss:  3.40802060e-03, bound:  3.15326482e-01\n",
      "Epoch: 36083 mean train loss:  3.40796472e-03, bound:  3.15326482e-01\n",
      "Epoch: 36084 mean train loss:  3.40789882e-03, bound:  3.15326482e-01\n",
      "Epoch: 36085 mean train loss:  3.40786926e-03, bound:  3.15326452e-01\n",
      "Epoch: 36086 mean train loss:  3.40780709e-03, bound:  3.15326452e-01\n",
      "Epoch: 36087 mean train loss:  3.40769184e-03, bound:  3.15326452e-01\n",
      "Epoch: 36088 mean train loss:  3.40768741e-03, bound:  3.15326422e-01\n",
      "Epoch: 36089 mean train loss:  3.40757705e-03, bound:  3.15326452e-01\n",
      "Epoch: 36090 mean train loss:  3.40747624e-03, bound:  3.15326422e-01\n",
      "Epoch: 36091 mean train loss:  3.40744178e-03, bound:  3.15326393e-01\n",
      "Epoch: 36092 mean train loss:  3.40737402e-03, bound:  3.15326393e-01\n",
      "Epoch: 36093 mean train loss:  3.40732886e-03, bound:  3.15326393e-01\n",
      "Epoch: 36094 mean train loss:  3.40723363e-03, bound:  3.15326393e-01\n",
      "Epoch: 36095 mean train loss:  3.40722734e-03, bound:  3.15326393e-01\n",
      "Epoch: 36096 mean train loss:  3.40713561e-03, bound:  3.15326393e-01\n",
      "Epoch: 36097 mean train loss:  3.40707856e-03, bound:  3.15326363e-01\n",
      "Epoch: 36098 mean train loss:  3.40704108e-03, bound:  3.15326363e-01\n",
      "Epoch: 36099 mean train loss:  3.40703200e-03, bound:  3.15326363e-01\n",
      "Epoch: 36100 mean train loss:  3.40696843e-03, bound:  3.15326363e-01\n",
      "Epoch: 36101 mean train loss:  3.40690115e-03, bound:  3.15326363e-01\n",
      "Epoch: 36102 mean train loss:  3.40684876e-03, bound:  3.15326333e-01\n",
      "Epoch: 36103 mean train loss:  3.40671092e-03, bound:  3.15326333e-01\n",
      "Epoch: 36104 mean train loss:  3.40664177e-03, bound:  3.15326333e-01\n",
      "Epoch: 36105 mean train loss:  3.40657122e-03, bound:  3.15326333e-01\n",
      "Epoch: 36106 mean train loss:  3.40649649e-03, bound:  3.15326333e-01\n",
      "Epoch: 36107 mean train loss:  3.40644433e-03, bound:  3.15326303e-01\n",
      "Epoch: 36108 mean train loss:  3.40641895e-03, bound:  3.15326303e-01\n",
      "Epoch: 36109 mean train loss:  3.40632000e-03, bound:  3.15326303e-01\n",
      "Epoch: 36110 mean train loss:  3.40627320e-03, bound:  3.15326303e-01\n",
      "Epoch: 36111 mean train loss:  3.40620801e-03, bound:  3.15326303e-01\n",
      "Epoch: 36112 mean train loss:  3.40617821e-03, bound:  3.15326303e-01\n",
      "Epoch: 36113 mean train loss:  3.40607972e-03, bound:  3.15326303e-01\n",
      "Epoch: 36114 mean train loss:  3.40603385e-03, bound:  3.15326244e-01\n",
      "Epoch: 36115 mean train loss:  3.40596493e-03, bound:  3.15326244e-01\n",
      "Epoch: 36116 mean train loss:  3.40584503e-03, bound:  3.15326244e-01\n",
      "Epoch: 36117 mean train loss:  3.40582337e-03, bound:  3.15326244e-01\n",
      "Epoch: 36118 mean train loss:  3.40577657e-03, bound:  3.15326244e-01\n",
      "Epoch: 36119 mean train loss:  3.40575678e-03, bound:  3.15326244e-01\n",
      "Epoch: 36120 mean train loss:  3.40564782e-03, bound:  3.15326244e-01\n",
      "Epoch: 36121 mean train loss:  3.40562104e-03, bound:  3.15326214e-01\n",
      "Epoch: 36122 mean train loss:  3.40554607e-03, bound:  3.15326214e-01\n",
      "Epoch: 36123 mean train loss:  3.40545294e-03, bound:  3.15326214e-01\n",
      "Epoch: 36124 mean train loss:  3.40543874e-03, bound:  3.15326214e-01\n",
      "Epoch: 36125 mean train loss:  3.40535981e-03, bound:  3.15326214e-01\n",
      "Epoch: 36126 mean train loss:  3.40526924e-03, bound:  3.15326214e-01\n",
      "Epoch: 36127 mean train loss:  3.40524013e-03, bound:  3.15326184e-01\n",
      "Epoch: 36128 mean train loss:  3.40515771e-03, bound:  3.15326184e-01\n",
      "Epoch: 36129 mean train loss:  3.40516167e-03, bound:  3.15326184e-01\n",
      "Epoch: 36130 mean train loss:  3.40502220e-03, bound:  3.15326184e-01\n",
      "Epoch: 36131 mean train loss:  3.40493349e-03, bound:  3.15326184e-01\n",
      "Epoch: 36132 mean train loss:  3.40488274e-03, bound:  3.15326154e-01\n",
      "Epoch: 36133 mean train loss:  3.40488437e-03, bound:  3.15326124e-01\n",
      "Epoch: 36134 mean train loss:  3.40478751e-03, bound:  3.15326124e-01\n",
      "Epoch: 36135 mean train loss:  3.40472767e-03, bound:  3.15326124e-01\n",
      "Epoch: 36136 mean train loss:  3.40468553e-03, bound:  3.15326124e-01\n",
      "Epoch: 36137 mean train loss:  3.40458215e-03, bound:  3.15326124e-01\n",
      "Epoch: 36138 mean train loss:  3.40453954e-03, bound:  3.15326124e-01\n",
      "Epoch: 36139 mean train loss:  3.40449554e-03, bound:  3.15326124e-01\n",
      "Epoch: 36140 mean train loss:  3.40442080e-03, bound:  3.15326095e-01\n",
      "Epoch: 36141 mean train loss:  3.40437004e-03, bound:  3.15326095e-01\n",
      "Epoch: 36142 mean train loss:  3.40432441e-03, bound:  3.15326095e-01\n",
      "Epoch: 36143 mean train loss:  3.40424944e-03, bound:  3.15326095e-01\n",
      "Epoch: 36144 mean train loss:  3.40414722e-03, bound:  3.15326095e-01\n",
      "Epoch: 36145 mean train loss:  3.40411812e-03, bound:  3.15326065e-01\n",
      "Epoch: 36146 mean train loss:  3.40400450e-03, bound:  3.15326065e-01\n",
      "Epoch: 36147 mean train loss:  3.40397656e-03, bound:  3.15326065e-01\n",
      "Epoch: 36148 mean train loss:  3.40395817e-03, bound:  3.15326065e-01\n",
      "Epoch: 36149 mean train loss:  3.40388506e-03, bound:  3.15326065e-01\n",
      "Epoch: 36150 mean train loss:  3.40382592e-03, bound:  3.15326035e-01\n",
      "Epoch: 36151 mean train loss:  3.40376399e-03, bound:  3.15326005e-01\n",
      "Epoch: 36152 mean train loss:  3.40371509e-03, bound:  3.15326005e-01\n",
      "Epoch: 36153 mean train loss:  3.40363220e-03, bound:  3.15326005e-01\n",
      "Epoch: 36154 mean train loss:  3.40356957e-03, bound:  3.15326005e-01\n",
      "Epoch: 36155 mean train loss:  3.40350438e-03, bound:  3.15326005e-01\n",
      "Epoch: 36156 mean train loss:  3.40344617e-03, bound:  3.15326005e-01\n",
      "Epoch: 36157 mean train loss:  3.40341683e-03, bound:  3.15326005e-01\n",
      "Epoch: 36158 mean train loss:  3.40330927e-03, bound:  3.15326005e-01\n",
      "Epoch: 36159 mean train loss:  3.40327783e-03, bound:  3.15326005e-01\n",
      "Epoch: 36160 mean train loss:  3.40319425e-03, bound:  3.15326005e-01\n",
      "Epoch: 36161 mean train loss:  3.40316002e-03, bound:  3.15325975e-01\n",
      "Epoch: 36162 mean train loss:  3.40310973e-03, bound:  3.15326005e-01\n",
      "Epoch: 36163 mean train loss:  3.40301311e-03, bound:  3.15325946e-01\n",
      "Epoch: 36164 mean train loss:  3.40294233e-03, bound:  3.15325946e-01\n",
      "Epoch: 36165 mean train loss:  3.40290414e-03, bound:  3.15325946e-01\n",
      "Epoch: 36166 mean train loss:  3.40286084e-03, bound:  3.15325946e-01\n",
      "Epoch: 36167 mean train loss:  3.40276817e-03, bound:  3.15325946e-01\n",
      "Epoch: 36168 mean train loss:  3.40271322e-03, bound:  3.15325886e-01\n",
      "Epoch: 36169 mean train loss:  3.40263569e-03, bound:  3.15325886e-01\n",
      "Epoch: 36170 mean train loss:  3.40260356e-03, bound:  3.15325886e-01\n",
      "Epoch: 36171 mean train loss:  3.40253953e-03, bound:  3.15325886e-01\n",
      "Epoch: 36172 mean train loss:  3.40247201e-03, bound:  3.15325886e-01\n",
      "Epoch: 36173 mean train loss:  3.40239517e-03, bound:  3.15325886e-01\n",
      "Epoch: 36174 mean train loss:  3.40240169e-03, bound:  3.15325886e-01\n",
      "Epoch: 36175 mean train loss:  3.40227527e-03, bound:  3.15325886e-01\n",
      "Epoch: 36176 mean train loss:  3.40221054e-03, bound:  3.15325886e-01\n",
      "Epoch: 36177 mean train loss:  3.40221846e-03, bound:  3.15325886e-01\n",
      "Epoch: 36178 mean train loss:  3.40212509e-03, bound:  3.15325886e-01\n",
      "Epoch: 36179 mean train loss:  3.40208202e-03, bound:  3.15325856e-01\n",
      "Epoch: 36180 mean train loss:  3.40199424e-03, bound:  3.15325826e-01\n",
      "Epoch: 36181 mean train loss:  3.40192951e-03, bound:  3.15325826e-01\n",
      "Epoch: 36182 mean train loss:  3.40188411e-03, bound:  3.15325826e-01\n",
      "Epoch: 36183 mean train loss:  3.40182194e-03, bound:  3.15325826e-01\n",
      "Epoch: 36184 mean train loss:  3.40178586e-03, bound:  3.15325797e-01\n",
      "Epoch: 36185 mean train loss:  3.40169994e-03, bound:  3.15325826e-01\n",
      "Epoch: 36186 mean train loss:  3.40158562e-03, bound:  3.15325797e-01\n",
      "Epoch: 36187 mean train loss:  3.40156234e-03, bound:  3.15325797e-01\n",
      "Epoch: 36188 mean train loss:  3.40150180e-03, bound:  3.15325797e-01\n",
      "Epoch: 36189 mean train loss:  3.40145058e-03, bound:  3.15325797e-01\n",
      "Epoch: 36190 mean train loss:  3.40136862e-03, bound:  3.15325797e-01\n",
      "Epoch: 36191 mean train loss:  3.40130879e-03, bound:  3.15325797e-01\n",
      "Epoch: 36192 mean train loss:  3.40123475e-03, bound:  3.15325797e-01\n",
      "Epoch: 36193 mean train loss:  3.40118422e-03, bound:  3.15325767e-01\n",
      "Epoch: 36194 mean train loss:  3.40114953e-03, bound:  3.15325767e-01\n",
      "Epoch: 36195 mean train loss:  3.40109388e-03, bound:  3.15325767e-01\n",
      "Epoch: 36196 mean train loss:  3.40101495e-03, bound:  3.15325767e-01\n",
      "Epoch: 36197 mean train loss:  3.40095186e-03, bound:  3.15325767e-01\n",
      "Epoch: 36198 mean train loss:  3.40094022e-03, bound:  3.15325737e-01\n",
      "Epoch: 36199 mean train loss:  3.40087153e-03, bound:  3.15325707e-01\n",
      "Epoch: 36200 mean train loss:  3.40074254e-03, bound:  3.15325707e-01\n",
      "Epoch: 36201 mean train loss:  3.40069504e-03, bound:  3.15325707e-01\n",
      "Epoch: 36202 mean train loss:  3.40073602e-03, bound:  3.15325707e-01\n",
      "Epoch: 36203 mean train loss:  3.40061169e-03, bound:  3.15325707e-01\n",
      "Epoch: 36204 mean train loss:  3.40054650e-03, bound:  3.15325677e-01\n",
      "Epoch: 36205 mean train loss:  3.40045569e-03, bound:  3.15325677e-01\n",
      "Epoch: 36206 mean train loss:  3.40040727e-03, bound:  3.15325677e-01\n",
      "Epoch: 36207 mean train loss:  3.40033765e-03, bound:  3.15325677e-01\n",
      "Epoch: 36208 mean train loss:  3.40029434e-03, bound:  3.15325677e-01\n",
      "Epoch: 36209 mean train loss:  3.40022985e-03, bound:  3.15325677e-01\n",
      "Epoch: 36210 mean train loss:  3.40014743e-03, bound:  3.15325677e-01\n",
      "Epoch: 36211 mean train loss:  3.40006663e-03, bound:  3.15325648e-01\n",
      "Epoch: 36212 mean train loss:  3.40003287e-03, bound:  3.15325648e-01\n",
      "Epoch: 36213 mean train loss:  3.40000284e-03, bound:  3.15325648e-01\n",
      "Epoch: 36214 mean train loss:  3.39988386e-03, bound:  3.15325648e-01\n",
      "Epoch: 36215 mean train loss:  3.39982635e-03, bound:  3.15325648e-01\n",
      "Epoch: 36216 mean train loss:  3.39980517e-03, bound:  3.15325648e-01\n",
      "Epoch: 36217 mean train loss:  3.39973206e-03, bound:  3.15325588e-01\n",
      "Epoch: 36218 mean train loss:  3.39969597e-03, bound:  3.15325588e-01\n",
      "Epoch: 36219 mean train loss:  3.39957909e-03, bound:  3.15325588e-01\n",
      "Epoch: 36220 mean train loss:  3.39952880e-03, bound:  3.15325588e-01\n",
      "Epoch: 36221 mean train loss:  3.39951529e-03, bound:  3.15325588e-01\n",
      "Epoch: 36222 mean train loss:  3.39943822e-03, bound:  3.15325588e-01\n",
      "Epoch: 36223 mean train loss:  3.39931366e-03, bound:  3.15325558e-01\n",
      "Epoch: 36224 mean train loss:  3.39929154e-03, bound:  3.15325558e-01\n",
      "Epoch: 36225 mean train loss:  3.39926453e-03, bound:  3.15325558e-01\n",
      "Epoch: 36226 mean train loss:  3.39915534e-03, bound:  3.15325558e-01\n",
      "Epoch: 36227 mean train loss:  3.39905266e-03, bound:  3.15325558e-01\n",
      "Epoch: 36228 mean train loss:  3.39901960e-03, bound:  3.15325528e-01\n",
      "Epoch: 36229 mean train loss:  3.39903496e-03, bound:  3.15325528e-01\n",
      "Epoch: 36230 mean train loss:  3.39894672e-03, bound:  3.15325528e-01\n",
      "Epoch: 36231 mean train loss:  3.39887664e-03, bound:  3.15325528e-01\n",
      "Epoch: 36232 mean train loss:  3.39876232e-03, bound:  3.15325528e-01\n",
      "Epoch: 36233 mean train loss:  3.39878467e-03, bound:  3.15325499e-01\n",
      "Epoch: 36234 mean train loss:  3.39869456e-03, bound:  3.15325469e-01\n",
      "Epoch: 36235 mean train loss:  3.39863589e-03, bound:  3.15325469e-01\n",
      "Epoch: 36236 mean train loss:  3.39861214e-03, bound:  3.15325469e-01\n",
      "Epoch: 36237 mean train loss:  3.39856255e-03, bound:  3.15325469e-01\n",
      "Epoch: 36238 mean train loss:  3.39849596e-03, bound:  3.15325469e-01\n",
      "Epoch: 36239 mean train loss:  3.39843868e-03, bound:  3.15325469e-01\n",
      "Epoch: 36240 mean train loss:  3.39834136e-03, bound:  3.15325439e-01\n",
      "Epoch: 36241 mean train loss:  3.39829060e-03, bound:  3.15325439e-01\n",
      "Epoch: 36242 mean train loss:  3.39819025e-03, bound:  3.15325439e-01\n",
      "Epoch: 36243 mean train loss:  3.39810667e-03, bound:  3.15325439e-01\n",
      "Epoch: 36244 mean train loss:  3.39803821e-03, bound:  3.15325439e-01\n",
      "Epoch: 36245 mean train loss:  3.39798327e-03, bound:  3.15325439e-01\n",
      "Epoch: 36246 mean train loss:  3.39801703e-03, bound:  3.15325439e-01\n",
      "Epoch: 36247 mean train loss:  3.39788920e-03, bound:  3.15325409e-01\n",
      "Epoch: 36248 mean train loss:  3.39784287e-03, bound:  3.15325409e-01\n",
      "Epoch: 36249 mean train loss:  3.39780981e-03, bound:  3.15325409e-01\n",
      "Epoch: 36250 mean train loss:  3.39771109e-03, bound:  3.15325409e-01\n",
      "Epoch: 36251 mean train loss:  3.39763542e-03, bound:  3.15325409e-01\n",
      "Epoch: 36252 mean train loss:  3.39762354e-03, bound:  3.15325409e-01\n",
      "Epoch: 36253 mean train loss:  3.39754159e-03, bound:  3.15325379e-01\n",
      "Epoch: 36254 mean train loss:  3.39745684e-03, bound:  3.15325379e-01\n",
      "Epoch: 36255 mean train loss:  3.39742773e-03, bound:  3.15325379e-01\n",
      "Epoch: 36256 mean train loss:  3.39731784e-03, bound:  3.15325379e-01\n",
      "Epoch: 36257 mean train loss:  3.39728198e-03, bound:  3.15325379e-01\n",
      "Epoch: 36258 mean train loss:  3.39721330e-03, bound:  3.15325350e-01\n",
      "Epoch: 36259 mean train loss:  3.39714647e-03, bound:  3.15325320e-01\n",
      "Epoch: 36260 mean train loss:  3.39706987e-03, bound:  3.15325320e-01\n",
      "Epoch: 36261 mean train loss:  3.39703634e-03, bound:  3.15325320e-01\n",
      "Epoch: 36262 mean train loss:  3.39694927e-03, bound:  3.15325320e-01\n",
      "Epoch: 36263 mean train loss:  3.39691597e-03, bound:  3.15325320e-01\n",
      "Epoch: 36264 mean train loss:  3.39684379e-03, bound:  3.15325320e-01\n",
      "Epoch: 36265 mean train loss:  3.39674018e-03, bound:  3.15325320e-01\n",
      "Epoch: 36266 mean train loss:  3.39674577e-03, bound:  3.15325290e-01\n",
      "Epoch: 36267 mean train loss:  3.39666172e-03, bound:  3.15325290e-01\n",
      "Epoch: 36268 mean train loss:  3.39658652e-03, bound:  3.15325290e-01\n",
      "Epoch: 36269 mean train loss:  3.39656719e-03, bound:  3.15325290e-01\n",
      "Epoch: 36270 mean train loss:  3.39647988e-03, bound:  3.15325290e-01\n",
      "Epoch: 36271 mean train loss:  3.39640328e-03, bound:  3.15325260e-01\n",
      "Epoch: 36272 mean train loss:  3.39636602e-03, bound:  3.15325260e-01\n",
      "Epoch: 36273 mean train loss:  3.39629827e-03, bound:  3.15325260e-01\n",
      "Epoch: 36274 mean train loss:  3.39624682e-03, bound:  3.15325260e-01\n",
      "Epoch: 36275 mean train loss:  3.39615555e-03, bound:  3.15325260e-01\n",
      "Epoch: 36276 mean train loss:  3.39612225e-03, bound:  3.15325230e-01\n",
      "Epoch: 36277 mean train loss:  3.39606171e-03, bound:  3.15325201e-01\n",
      "Epoch: 36278 mean train loss:  3.39605729e-03, bound:  3.15325201e-01\n",
      "Epoch: 36279 mean train loss:  3.39594460e-03, bound:  3.15325201e-01\n",
      "Epoch: 36280 mean train loss:  3.39590060e-03, bound:  3.15325201e-01\n",
      "Epoch: 36281 mean train loss:  3.39583331e-03, bound:  3.15325201e-01\n",
      "Epoch: 36282 mean train loss:  3.39575997e-03, bound:  3.15325201e-01\n",
      "Epoch: 36283 mean train loss:  3.39576672e-03, bound:  3.15325201e-01\n",
      "Epoch: 36284 mean train loss:  3.39568988e-03, bound:  3.15325201e-01\n",
      "Epoch: 36285 mean train loss:  3.39561957e-03, bound:  3.15325201e-01\n",
      "Epoch: 36286 mean train loss:  3.39554716e-03, bound:  3.15325171e-01\n",
      "Epoch: 36287 mean train loss:  3.39553878e-03, bound:  3.15325201e-01\n",
      "Epoch: 36288 mean train loss:  3.39542446e-03, bound:  3.15325141e-01\n",
      "Epoch: 36289 mean train loss:  3.39534134e-03, bound:  3.15325141e-01\n",
      "Epoch: 36290 mean train loss:  3.39528965e-03, bound:  3.15325141e-01\n",
      "Epoch: 36291 mean train loss:  3.39519652e-03, bound:  3.15325141e-01\n",
      "Epoch: 36292 mean train loss:  3.39516113e-03, bound:  3.15325141e-01\n",
      "Epoch: 36293 mean train loss:  3.39510292e-03, bound:  3.15325141e-01\n",
      "Epoch: 36294 mean train loss:  3.39497370e-03, bound:  3.15325111e-01\n",
      "Epoch: 36295 mean train loss:  3.39498976e-03, bound:  3.15325111e-01\n",
      "Epoch: 36296 mean train loss:  3.39487963e-03, bound:  3.15325111e-01\n",
      "Epoch: 36297 mean train loss:  3.39479418e-03, bound:  3.15325111e-01\n",
      "Epoch: 36298 mean train loss:  3.39478068e-03, bound:  3.15325111e-01\n",
      "Epoch: 36299 mean train loss:  3.39469127e-03, bound:  3.15325081e-01\n",
      "Epoch: 36300 mean train loss:  3.39466683e-03, bound:  3.15325081e-01\n",
      "Epoch: 36301 mean train loss:  3.39465030e-03, bound:  3.15325081e-01\n",
      "Epoch: 36302 mean train loss:  3.39458068e-03, bound:  3.15325081e-01\n",
      "Epoch: 36303 mean train loss:  3.39453272e-03, bound:  3.15325081e-01\n",
      "Epoch: 36304 mean train loss:  3.39447614e-03, bound:  3.15325081e-01\n",
      "Epoch: 36305 mean train loss:  3.39439837e-03, bound:  3.15325081e-01\n",
      "Epoch: 36306 mean train loss:  3.39429709e-03, bound:  3.15325081e-01\n",
      "Epoch: 36307 mean train loss:  3.39426775e-03, bound:  3.15325022e-01\n",
      "Epoch: 36308 mean train loss:  3.39413015e-03, bound:  3.15325022e-01\n",
      "Epoch: 36309 mean train loss:  3.39411222e-03, bound:  3.15325022e-01\n",
      "Epoch: 36310 mean train loss:  3.39401467e-03, bound:  3.15325022e-01\n",
      "Epoch: 36311 mean train loss:  3.39402631e-03, bound:  3.15324992e-01\n",
      "Epoch: 36312 mean train loss:  3.39395367e-03, bound:  3.15324992e-01\n",
      "Epoch: 36313 mean train loss:  3.39387986e-03, bound:  3.15324992e-01\n",
      "Epoch: 36314 mean train loss:  3.39383283e-03, bound:  3.15324992e-01\n",
      "Epoch: 36315 mean train loss:  3.39375669e-03, bound:  3.15324992e-01\n",
      "Epoch: 36316 mean train loss:  3.39378347e-03, bound:  3.15324992e-01\n",
      "Epoch: 36317 mean train loss:  3.39362118e-03, bound:  3.15324992e-01\n",
      "Epoch: 36318 mean train loss:  3.39356880e-03, bound:  3.15324962e-01\n",
      "Epoch: 36319 mean train loss:  3.39351106e-03, bound:  3.15324962e-01\n",
      "Epoch: 36320 mean train loss:  3.39343864e-03, bound:  3.15324962e-01\n",
      "Epoch: 36321 mean train loss:  3.39333387e-03, bound:  3.15324962e-01\n",
      "Epoch: 36322 mean train loss:  3.39327310e-03, bound:  3.15324962e-01\n",
      "Epoch: 36323 mean train loss:  3.39324865e-03, bound:  3.15324903e-01\n",
      "Epoch: 36324 mean train loss:  3.39321862e-03, bound:  3.15324903e-01\n",
      "Epoch: 36325 mean train loss:  3.39314039e-03, bound:  3.15324903e-01\n",
      "Epoch: 36326 mean train loss:  3.39313946e-03, bound:  3.15324903e-01\n",
      "Epoch: 36327 mean train loss:  3.39303166e-03, bound:  3.15324903e-01\n",
      "Epoch: 36328 mean train loss:  3.39298276e-03, bound:  3.15324903e-01\n",
      "Epoch: 36329 mean train loss:  3.39289638e-03, bound:  3.15324903e-01\n",
      "Epoch: 36330 mean train loss:  3.39283329e-03, bound:  3.15324873e-01\n",
      "Epoch: 36331 mean train loss:  3.39284469e-03, bound:  3.15324873e-01\n",
      "Epoch: 36332 mean train loss:  3.39272292e-03, bound:  3.15324873e-01\n",
      "Epoch: 36333 mean train loss:  3.39266658e-03, bound:  3.15324873e-01\n",
      "Epoch: 36334 mean train loss:  3.39260697e-03, bound:  3.15324873e-01\n",
      "Epoch: 36335 mean train loss:  3.39256390e-03, bound:  3.15324873e-01\n",
      "Epoch: 36336 mean train loss:  3.39248334e-03, bound:  3.15324873e-01\n",
      "Epoch: 36337 mean train loss:  3.39242001e-03, bound:  3.15324843e-01\n",
      "Epoch: 36338 mean train loss:  3.39234807e-03, bound:  3.15324843e-01\n",
      "Epoch: 36339 mean train loss:  3.39229219e-03, bound:  3.15324843e-01\n",
      "Epoch: 36340 mean train loss:  3.39223677e-03, bound:  3.15324843e-01\n",
      "Epoch: 36341 mean train loss:  3.39218718e-03, bound:  3.15324843e-01\n",
      "Epoch: 36342 mean train loss:  3.39211966e-03, bound:  3.15324813e-01\n",
      "Epoch: 36343 mean train loss:  3.39206425e-03, bound:  3.15324783e-01\n",
      "Epoch: 36344 mean train loss:  3.39202210e-03, bound:  3.15324783e-01\n",
      "Epoch: 36345 mean train loss:  3.39191710e-03, bound:  3.15324783e-01\n",
      "Epoch: 36346 mean train loss:  3.39190289e-03, bound:  3.15324783e-01\n",
      "Epoch: 36347 mean train loss:  3.39187495e-03, bound:  3.15324754e-01\n",
      "Epoch: 36348 mean train loss:  3.39177740e-03, bound:  3.15324783e-01\n",
      "Epoch: 36349 mean train loss:  3.39169893e-03, bound:  3.15324754e-01\n",
      "Epoch: 36350 mean train loss:  3.39164981e-03, bound:  3.15324754e-01\n",
      "Epoch: 36351 mean train loss:  3.39154317e-03, bound:  3.15324754e-01\n",
      "Epoch: 36352 mean train loss:  3.39150080e-03, bound:  3.15324754e-01\n",
      "Epoch: 36353 mean train loss:  3.39140440e-03, bound:  3.15324754e-01\n",
      "Epoch: 36354 mean train loss:  3.39138787e-03, bound:  3.15324754e-01\n",
      "Epoch: 36355 mean train loss:  3.39132687e-03, bound:  3.15324724e-01\n",
      "Epoch: 36356 mean train loss:  3.39128333e-03, bound:  3.15324724e-01\n",
      "Epoch: 36357 mean train loss:  3.39117576e-03, bound:  3.15324724e-01\n",
      "Epoch: 36358 mean train loss:  3.39114084e-03, bound:  3.15324724e-01\n",
      "Epoch: 36359 mean train loss:  3.39107052e-03, bound:  3.15324724e-01\n",
      "Epoch: 36360 mean train loss:  3.39099113e-03, bound:  3.15324724e-01\n",
      "Epoch: 36361 mean train loss:  3.39098182e-03, bound:  3.15324664e-01\n",
      "Epoch: 36362 mean train loss:  3.39087984e-03, bound:  3.15324664e-01\n",
      "Epoch: 36363 mean train loss:  3.39080906e-03, bound:  3.15324664e-01\n",
      "Epoch: 36364 mean train loss:  3.39078414e-03, bound:  3.15324664e-01\n",
      "Epoch: 36365 mean train loss:  3.39070614e-03, bound:  3.15324664e-01\n",
      "Epoch: 36366 mean train loss:  3.39066982e-03, bound:  3.15324664e-01\n",
      "Epoch: 36367 mean train loss:  3.39060044e-03, bound:  3.15324634e-01\n",
      "Epoch: 36368 mean train loss:  3.39053408e-03, bound:  3.15324634e-01\n",
      "Epoch: 36369 mean train loss:  3.39051173e-03, bound:  3.15324634e-01\n",
      "Epoch: 36370 mean train loss:  3.39042256e-03, bound:  3.15324634e-01\n",
      "Epoch: 36371 mean train loss:  3.39034735e-03, bound:  3.15324634e-01\n",
      "Epoch: 36372 mean train loss:  3.39031918e-03, bound:  3.15324634e-01\n",
      "Epoch: 36373 mean train loss:  3.39025003e-03, bound:  3.15324575e-01\n",
      "Epoch: 36374 mean train loss:  3.39017413e-03, bound:  3.15324575e-01\n",
      "Epoch: 36375 mean train loss:  3.39012453e-03, bound:  3.15324575e-01\n",
      "Epoch: 36376 mean train loss:  3.39005748e-03, bound:  3.15324575e-01\n",
      "Epoch: 36377 mean train loss:  3.38995364e-03, bound:  3.15324575e-01\n",
      "Epoch: 36378 mean train loss:  3.38992802e-03, bound:  3.15324575e-01\n",
      "Epoch: 36379 mean train loss:  3.38982837e-03, bound:  3.15324575e-01\n",
      "Epoch: 36380 mean train loss:  3.38976085e-03, bound:  3.15324575e-01\n",
      "Epoch: 36381 mean train loss:  3.38973734e-03, bound:  3.15324575e-01\n",
      "Epoch: 36382 mean train loss:  3.38973664e-03, bound:  3.15324575e-01\n",
      "Epoch: 36383 mean train loss:  3.38968844e-03, bound:  3.15324575e-01\n",
      "Epoch: 36384 mean train loss:  3.38964490e-03, bound:  3.15324545e-01\n",
      "Epoch: 36385 mean train loss:  3.38957622e-03, bound:  3.15324515e-01\n",
      "Epoch: 36386 mean train loss:  3.38955806e-03, bound:  3.15324515e-01\n",
      "Epoch: 36387 mean train loss:  3.38947121e-03, bound:  3.15324515e-01\n",
      "Epoch: 36388 mean train loss:  3.38939275e-03, bound:  3.15324515e-01\n",
      "Epoch: 36389 mean train loss:  3.38931661e-03, bound:  3.15324515e-01\n",
      "Epoch: 36390 mean train loss:  3.38921766e-03, bound:  3.15324455e-01\n",
      "Epoch: 36391 mean train loss:  3.38918646e-03, bound:  3.15324455e-01\n",
      "Epoch: 36392 mean train loss:  3.38905933e-03, bound:  3.15324455e-01\n",
      "Epoch: 36393 mean train loss:  3.38900881e-03, bound:  3.15324455e-01\n",
      "Epoch: 36394 mean train loss:  3.38894338e-03, bound:  3.15324455e-01\n",
      "Epoch: 36395 mean train loss:  3.38890450e-03, bound:  3.15324455e-01\n",
      "Epoch: 36396 mean train loss:  3.38887516e-03, bound:  3.15324455e-01\n",
      "Epoch: 36397 mean train loss:  3.38884955e-03, bound:  3.15324455e-01\n",
      "Epoch: 36398 mean train loss:  3.38882371e-03, bound:  3.15324455e-01\n",
      "Epoch: 36399 mean train loss:  3.38878389e-03, bound:  3.15324455e-01\n",
      "Epoch: 36400 mean train loss:  3.38875572e-03, bound:  3.15324455e-01\n",
      "Epoch: 36401 mean train loss:  3.38868657e-03, bound:  3.15324455e-01\n",
      "Epoch: 36402 mean train loss:  3.38857737e-03, bound:  3.15324455e-01\n",
      "Epoch: 36403 mean train loss:  3.38850962e-03, bound:  3.15324426e-01\n",
      "Epoch: 36404 mean train loss:  3.38840298e-03, bound:  3.15324426e-01\n",
      "Epoch: 36405 mean train loss:  3.38833034e-03, bound:  3.15324426e-01\n",
      "Epoch: 36406 mean train loss:  3.38824443e-03, bound:  3.15324426e-01\n",
      "Epoch: 36407 mean train loss:  3.38819087e-03, bound:  3.15324426e-01\n",
      "Epoch: 36408 mean train loss:  3.38819763e-03, bound:  3.15324396e-01\n",
      "Epoch: 36409 mean train loss:  3.38814058e-03, bound:  3.15324426e-01\n",
      "Epoch: 36410 mean train loss:  3.38808075e-03, bound:  3.15324336e-01\n",
      "Epoch: 36411 mean train loss:  3.38802743e-03, bound:  3.15324336e-01\n",
      "Epoch: 36412 mean train loss:  3.38790333e-03, bound:  3.15324336e-01\n",
      "Epoch: 36413 mean train loss:  3.38781415e-03, bound:  3.15324336e-01\n",
      "Epoch: 36414 mean train loss:  3.38773872e-03, bound:  3.15324336e-01\n",
      "Epoch: 36415 mean train loss:  3.38770682e-03, bound:  3.15324336e-01\n",
      "Epoch: 36416 mean train loss:  3.38766491e-03, bound:  3.15324336e-01\n",
      "Epoch: 36417 mean train loss:  3.38760647e-03, bound:  3.15324336e-01\n",
      "Epoch: 36418 mean train loss:  3.38757131e-03, bound:  3.15324336e-01\n",
      "Epoch: 36419 mean train loss:  3.38750333e-03, bound:  3.15324336e-01\n",
      "Epoch: 36420 mean train loss:  3.38745024e-03, bound:  3.15324336e-01\n",
      "Epoch: 36421 mean train loss:  3.38730216e-03, bound:  3.15324336e-01\n",
      "Epoch: 36422 mean train loss:  3.38728912e-03, bound:  3.15324306e-01\n",
      "Epoch: 36423 mean train loss:  3.38720577e-03, bound:  3.15324306e-01\n",
      "Epoch: 36424 mean train loss:  3.38723511e-03, bound:  3.15324306e-01\n",
      "Epoch: 36425 mean train loss:  3.38714500e-03, bound:  3.15324306e-01\n",
      "Epoch: 36426 mean train loss:  3.38705396e-03, bound:  3.15324306e-01\n",
      "Epoch: 36427 mean train loss:  3.38700623e-03, bound:  3.15324277e-01\n",
      "Epoch: 36428 mean train loss:  3.38688469e-03, bound:  3.15324277e-01\n",
      "Epoch: 36429 mean train loss:  3.38686933e-03, bound:  3.15324277e-01\n",
      "Epoch: 36430 mean train loss:  3.38681368e-03, bound:  3.15324277e-01\n",
      "Epoch: 36431 mean train loss:  3.38680670e-03, bound:  3.15324277e-01\n",
      "Epoch: 36432 mean train loss:  3.38676525e-03, bound:  3.15324277e-01\n",
      "Epoch: 36433 mean train loss:  3.38666351e-03, bound:  3.15324217e-01\n",
      "Epoch: 36434 mean train loss:  3.38657014e-03, bound:  3.15324217e-01\n",
      "Epoch: 36435 mean train loss:  3.38650122e-03, bound:  3.15324217e-01\n",
      "Epoch: 36436 mean train loss:  3.38644069e-03, bound:  3.15324217e-01\n",
      "Epoch: 36437 mean train loss:  3.38636595e-03, bound:  3.15324217e-01\n",
      "Epoch: 36438 mean train loss:  3.38632124e-03, bound:  3.15324217e-01\n",
      "Epoch: 36439 mean train loss:  3.38627561e-03, bound:  3.15324187e-01\n",
      "Epoch: 36440 mean train loss:  3.38624045e-03, bound:  3.15324187e-01\n",
      "Epoch: 36441 mean train loss:  3.38614988e-03, bound:  3.15324187e-01\n",
      "Epoch: 36442 mean train loss:  3.38612893e-03, bound:  3.15324187e-01\n",
      "Epoch: 36443 mean train loss:  3.38607910e-03, bound:  3.15324187e-01\n",
      "Epoch: 36444 mean train loss:  3.38599971e-03, bound:  3.15324187e-01\n",
      "Epoch: 36445 mean train loss:  3.38595756e-03, bound:  3.15324187e-01\n",
      "Epoch: 36446 mean train loss:  3.38586862e-03, bound:  3.15324157e-01\n",
      "Epoch: 36447 mean train loss:  3.38582881e-03, bound:  3.15324157e-01\n",
      "Epoch: 36448 mean train loss:  3.38580017e-03, bound:  3.15324157e-01\n",
      "Epoch: 36449 mean train loss:  3.38568306e-03, bound:  3.15324157e-01\n",
      "Epoch: 36450 mean train loss:  3.38562601e-03, bound:  3.15324157e-01\n",
      "Epoch: 36451 mean train loss:  3.38553078e-03, bound:  3.15324128e-01\n",
      "Epoch: 36452 mean train loss:  3.38549446e-03, bound:  3.15324128e-01\n",
      "Epoch: 36453 mean train loss:  3.38542834e-03, bound:  3.15324098e-01\n",
      "Epoch: 36454 mean train loss:  3.38538014e-03, bound:  3.15324098e-01\n",
      "Epoch: 36455 mean train loss:  3.38530261e-03, bound:  3.15324098e-01\n",
      "Epoch: 36456 mean train loss:  3.38526978e-03, bound:  3.15324098e-01\n",
      "Epoch: 36457 mean train loss:  3.38521530e-03, bound:  3.15324098e-01\n",
      "Epoch: 36458 mean train loss:  3.38512170e-03, bound:  3.15324068e-01\n",
      "Epoch: 36459 mean train loss:  3.38510936e-03, bound:  3.15324068e-01\n",
      "Epoch: 36460 mean train loss:  3.38502438e-03, bound:  3.15324068e-01\n",
      "Epoch: 36461 mean train loss:  3.38500110e-03, bound:  3.15324068e-01\n",
      "Epoch: 36462 mean train loss:  3.38490261e-03, bound:  3.15324068e-01\n",
      "Epoch: 36463 mean train loss:  3.38485069e-03, bound:  3.15324068e-01\n",
      "Epoch: 36464 mean train loss:  3.38481390e-03, bound:  3.15324068e-01\n",
      "Epoch: 36465 mean train loss:  3.38476314e-03, bound:  3.15324008e-01\n",
      "Epoch: 36466 mean train loss:  3.38470400e-03, bound:  3.15324008e-01\n",
      "Epoch: 36467 mean train loss:  3.38463834e-03, bound:  3.15324008e-01\n",
      "Epoch: 36468 mean train loss:  3.38459294e-03, bound:  3.15324008e-01\n",
      "Epoch: 36469 mean train loss:  3.38448305e-03, bound:  3.15324008e-01\n",
      "Epoch: 36470 mean train loss:  3.38446465e-03, bound:  3.15324008e-01\n",
      "Epoch: 36471 mean train loss:  3.38438386e-03, bound:  3.15323979e-01\n",
      "Epoch: 36472 mean train loss:  3.38431192e-03, bound:  3.15323979e-01\n",
      "Epoch: 36473 mean train loss:  3.38427769e-03, bound:  3.15323979e-01\n",
      "Epoch: 36474 mean train loss:  3.38423182e-03, bound:  3.15323979e-01\n",
      "Epoch: 36475 mean train loss:  3.38414824e-03, bound:  3.15323979e-01\n",
      "Epoch: 36476 mean train loss:  3.38409375e-03, bound:  3.15323979e-01\n",
      "Epoch: 36477 mean train loss:  3.38401808e-03, bound:  3.15323949e-01\n",
      "Epoch: 36478 mean train loss:  3.38394381e-03, bound:  3.15323949e-01\n",
      "Epoch: 36479 mean train loss:  3.38391121e-03, bound:  3.15323949e-01\n",
      "Epoch: 36480 mean train loss:  3.38385999e-03, bound:  3.15323949e-01\n",
      "Epoch: 36481 mean train loss:  3.38378339e-03, bound:  3.15323949e-01\n",
      "Epoch: 36482 mean train loss:  3.38372053e-03, bound:  3.15323949e-01\n",
      "Epoch: 36483 mean train loss:  3.38368118e-03, bound:  3.15323949e-01\n",
      "Epoch: 36484 mean train loss:  3.38358362e-03, bound:  3.15323889e-01\n",
      "Epoch: 36485 mean train loss:  3.38356383e-03, bound:  3.15323889e-01\n",
      "Epoch: 36486 mean train loss:  3.38352099e-03, bound:  3.15323889e-01\n",
      "Epoch: 36487 mean train loss:  3.38345114e-03, bound:  3.15323889e-01\n",
      "Epoch: 36488 mean train loss:  3.38334939e-03, bound:  3.15323889e-01\n",
      "Epoch: 36489 mean train loss:  3.38332984e-03, bound:  3.15323859e-01\n",
      "Epoch: 36490 mean train loss:  3.38323903e-03, bound:  3.15323859e-01\n",
      "Epoch: 36491 mean train loss:  3.38318408e-03, bound:  3.15323859e-01\n",
      "Epoch: 36492 mean train loss:  3.38315521e-03, bound:  3.15323859e-01\n",
      "Epoch: 36493 mean train loss:  3.38307628e-03, bound:  3.15323859e-01\n",
      "Epoch: 36494 mean train loss:  3.38300620e-03, bound:  3.15323859e-01\n",
      "Epoch: 36495 mean train loss:  3.38293845e-03, bound:  3.15323859e-01\n",
      "Epoch: 36496 mean train loss:  3.38291982e-03, bound:  3.15323859e-01\n",
      "Epoch: 36497 mean train loss:  3.38281784e-03, bound:  3.15323859e-01\n",
      "Epoch: 36498 mean train loss:  3.38279735e-03, bound:  3.15323859e-01\n",
      "Epoch: 36499 mean train loss:  3.38269793e-03, bound:  3.15323859e-01\n",
      "Epoch: 36500 mean train loss:  3.38261598e-03, bound:  3.15323859e-01\n",
      "Epoch: 36501 mean train loss:  3.38260690e-03, bound:  3.15323830e-01\n",
      "Epoch: 36502 mean train loss:  3.38255055e-03, bound:  3.15323770e-01\n",
      "Epoch: 36503 mean train loss:  3.38244741e-03, bound:  3.15323770e-01\n",
      "Epoch: 36504 mean train loss:  3.38240154e-03, bound:  3.15323770e-01\n",
      "Epoch: 36505 mean train loss:  3.38233332e-03, bound:  3.15323770e-01\n",
      "Epoch: 36506 mean train loss:  3.38232913e-03, bound:  3.15323770e-01\n",
      "Epoch: 36507 mean train loss:  3.38222040e-03, bound:  3.15323770e-01\n",
      "Epoch: 36508 mean train loss:  3.38217174e-03, bound:  3.15323770e-01\n",
      "Epoch: 36509 mean train loss:  3.38209071e-03, bound:  3.15323770e-01\n",
      "Epoch: 36510 mean train loss:  3.38203972e-03, bound:  3.15323770e-01\n",
      "Epoch: 36511 mean train loss:  3.38198384e-03, bound:  3.15323770e-01\n",
      "Epoch: 36512 mean train loss:  3.38192843e-03, bound:  3.15323740e-01\n",
      "Epoch: 36513 mean train loss:  3.38189211e-03, bound:  3.15323740e-01\n",
      "Epoch: 36514 mean train loss:  3.38181574e-03, bound:  3.15323740e-01\n",
      "Epoch: 36515 mean train loss:  3.38177616e-03, bound:  3.15323740e-01\n",
      "Epoch: 36516 mean train loss:  3.38171935e-03, bound:  3.15323740e-01\n",
      "Epoch: 36517 mean train loss:  3.38161807e-03, bound:  3.15323740e-01\n",
      "Epoch: 36518 mean train loss:  3.38157360e-03, bound:  3.15323740e-01\n",
      "Epoch: 36519 mean train loss:  3.38151562e-03, bound:  3.15323651e-01\n",
      "Epoch: 36520 mean train loss:  3.38145299e-03, bound:  3.15323710e-01\n",
      "Epoch: 36521 mean train loss:  3.38143157e-03, bound:  3.15323651e-01\n",
      "Epoch: 36522 mean train loss:  3.38132191e-03, bound:  3.15323651e-01\n",
      "Epoch: 36523 mean train loss:  3.38129699e-03, bound:  3.15323651e-01\n",
      "Epoch: 36524 mean train loss:  3.38125345e-03, bound:  3.15323651e-01\n",
      "Epoch: 36525 mean train loss:  3.38115450e-03, bound:  3.15323651e-01\n",
      "Epoch: 36526 mean train loss:  3.38111562e-03, bound:  3.15323651e-01\n",
      "Epoch: 36527 mean train loss:  3.38104041e-03, bound:  3.15323651e-01\n",
      "Epoch: 36528 mean train loss:  3.38101690e-03, bound:  3.15323651e-01\n",
      "Epoch: 36529 mean train loss:  3.38091655e-03, bound:  3.15323651e-01\n",
      "Epoch: 36530 mean train loss:  3.38084344e-03, bound:  3.15323651e-01\n",
      "Epoch: 36531 mean train loss:  3.38075333e-03, bound:  3.15323621e-01\n",
      "Epoch: 36532 mean train loss:  3.38068441e-03, bound:  3.15323621e-01\n",
      "Epoch: 36533 mean train loss:  3.38067650e-03, bound:  3.15323621e-01\n",
      "Epoch: 36534 mean train loss:  3.38059361e-03, bound:  3.15323621e-01\n",
      "Epoch: 36535 mean train loss:  3.38055287e-03, bound:  3.15323621e-01\n",
      "Epoch: 36536 mean train loss:  3.38054518e-03, bound:  3.15323621e-01\n",
      "Epoch: 36537 mean train loss:  3.38042877e-03, bound:  3.15323561e-01\n",
      "Epoch: 36538 mean train loss:  3.38040991e-03, bound:  3.15323561e-01\n",
      "Epoch: 36539 mean train loss:  3.38031189e-03, bound:  3.15323532e-01\n",
      "Epoch: 36540 mean train loss:  3.38022038e-03, bound:  3.15323532e-01\n",
      "Epoch: 36541 mean train loss:  3.38018872e-03, bound:  3.15323532e-01\n",
      "Epoch: 36542 mean train loss:  3.38011933e-03, bound:  3.15323532e-01\n",
      "Epoch: 36543 mean train loss:  3.38008953e-03, bound:  3.15323532e-01\n",
      "Epoch: 36544 mean train loss:  3.38002504e-03, bound:  3.15323532e-01\n",
      "Epoch: 36545 mean train loss:  3.37995752e-03, bound:  3.15323532e-01\n",
      "Epoch: 36546 mean train loss:  3.37991468e-03, bound:  3.15323532e-01\n",
      "Epoch: 36547 mean train loss:  3.37987067e-03, bound:  3.15323532e-01\n",
      "Epoch: 36548 mean train loss:  3.37982085e-03, bound:  3.15323532e-01\n",
      "Epoch: 36549 mean train loss:  3.37971118e-03, bound:  3.15323532e-01\n",
      "Epoch: 36550 mean train loss:  3.37965484e-03, bound:  3.15323502e-01\n",
      "Epoch: 36551 mean train loss:  3.37958871e-03, bound:  3.15323502e-01\n",
      "Epoch: 36552 mean train loss:  3.37951374e-03, bound:  3.15323502e-01\n",
      "Epoch: 36553 mean train loss:  3.37946950e-03, bound:  3.15323502e-01\n",
      "Epoch: 36554 mean train loss:  3.37941549e-03, bound:  3.15323502e-01\n",
      "Epoch: 36555 mean train loss:  3.37936822e-03, bound:  3.15323502e-01\n",
      "Epoch: 36556 mean train loss:  3.37928929e-03, bound:  3.15323502e-01\n",
      "Epoch: 36557 mean train loss:  3.37924040e-03, bound:  3.15323442e-01\n",
      "Epoch: 36558 mean train loss:  3.37919546e-03, bound:  3.15323442e-01\n",
      "Epoch: 36559 mean train loss:  3.37913237e-03, bound:  3.15323442e-01\n",
      "Epoch: 36560 mean train loss:  3.37903923e-03, bound:  3.15323442e-01\n",
      "Epoch: 36561 mean train loss:  3.37904249e-03, bound:  3.15323442e-01\n",
      "Epoch: 36562 mean train loss:  3.37897986e-03, bound:  3.15323442e-01\n",
      "Epoch: 36563 mean train loss:  3.37890023e-03, bound:  3.15323412e-01\n",
      "Epoch: 36564 mean train loss:  3.37883411e-03, bound:  3.15323412e-01\n",
      "Epoch: 36565 mean train loss:  3.37880198e-03, bound:  3.15323412e-01\n",
      "Epoch: 36566 mean train loss:  3.37878242e-03, bound:  3.15323412e-01\n",
      "Epoch: 36567 mean train loss:  3.37871956e-03, bound:  3.15323383e-01\n",
      "Epoch: 36568 mean train loss:  3.37861548e-03, bound:  3.15323383e-01\n",
      "Epoch: 36569 mean train loss:  3.37856496e-03, bound:  3.15323383e-01\n",
      "Epoch: 36570 mean train loss:  3.37848836e-03, bound:  3.15323383e-01\n",
      "Epoch: 36571 mean train loss:  3.37840221e-03, bound:  3.15323383e-01\n",
      "Epoch: 36572 mean train loss:  3.37832770e-03, bound:  3.15323383e-01\n",
      "Epoch: 36573 mean train loss:  3.37829092e-03, bound:  3.15323383e-01\n",
      "Epoch: 36574 mean train loss:  3.37820733e-03, bound:  3.15323383e-01\n",
      "Epoch: 36575 mean train loss:  3.37817706e-03, bound:  3.15323323e-01\n",
      "Epoch: 36576 mean train loss:  3.37810628e-03, bound:  3.15323323e-01\n",
      "Epoch: 36577 mean train loss:  3.37805762e-03, bound:  3.15323323e-01\n",
      "Epoch: 36578 mean train loss:  3.37798009e-03, bound:  3.15323323e-01\n",
      "Epoch: 36579 mean train loss:  3.37789487e-03, bound:  3.15323323e-01\n",
      "Epoch: 36580 mean train loss:  3.37787601e-03, bound:  3.15323323e-01\n",
      "Epoch: 36581 mean train loss:  3.37776565e-03, bound:  3.15323293e-01\n",
      "Epoch: 36582 mean train loss:  3.37773608e-03, bound:  3.15323293e-01\n",
      "Epoch: 36583 mean train loss:  3.37770744e-03, bound:  3.15323293e-01\n",
      "Epoch: 36584 mean train loss:  3.37763270e-03, bound:  3.15323293e-01\n",
      "Epoch: 36585 mean train loss:  3.37759824e-03, bound:  3.15323263e-01\n",
      "Epoch: 36586 mean train loss:  3.37749766e-03, bound:  3.15323263e-01\n",
      "Epoch: 36587 mean train loss:  3.37746018e-03, bound:  3.15323263e-01\n",
      "Epoch: 36588 mean train loss:  3.37739545e-03, bound:  3.15323263e-01\n",
      "Epoch: 36589 mean train loss:  3.37734306e-03, bound:  3.15323263e-01\n",
      "Epoch: 36590 mean train loss:  3.37729324e-03, bound:  3.15323263e-01\n",
      "Epoch: 36591 mean train loss:  3.37725203e-03, bound:  3.15323263e-01\n",
      "Epoch: 36592 mean train loss:  3.37713491e-03, bound:  3.15323263e-01\n",
      "Epoch: 36593 mean train loss:  3.37706902e-03, bound:  3.15323204e-01\n",
      "Epoch: 36594 mean train loss:  3.37702688e-03, bound:  3.15323204e-01\n",
      "Epoch: 36595 mean train loss:  3.37694562e-03, bound:  3.15323204e-01\n",
      "Epoch: 36596 mean train loss:  3.37688415e-03, bound:  3.15323204e-01\n",
      "Epoch: 36597 mean train loss:  3.37680732e-03, bound:  3.15323204e-01\n",
      "Epoch: 36598 mean train loss:  3.37682012e-03, bound:  3.15323204e-01\n",
      "Epoch: 36599 mean train loss:  3.37669603e-03, bound:  3.15323204e-01\n",
      "Epoch: 36600 mean train loss:  3.37666785e-03, bound:  3.15323174e-01\n",
      "Epoch: 36601 mean train loss:  3.37657938e-03, bound:  3.15323174e-01\n",
      "Epoch: 36602 mean train loss:  3.37651838e-03, bound:  3.15323174e-01\n",
      "Epoch: 36603 mean train loss:  3.37649696e-03, bound:  3.15323174e-01\n",
      "Epoch: 36604 mean train loss:  3.37641197e-03, bound:  3.15323174e-01\n",
      "Epoch: 36605 mean train loss:  3.37636797e-03, bound:  3.15323174e-01\n",
      "Epoch: 36606 mean train loss:  3.37633328e-03, bound:  3.15323174e-01\n",
      "Epoch: 36607 mean train loss:  3.37625062e-03, bound:  3.15323174e-01\n",
      "Epoch: 36608 mean train loss:  3.37618310e-03, bound:  3.15323174e-01\n",
      "Epoch: 36609 mean train loss:  3.37611604e-03, bound:  3.15323174e-01\n",
      "Epoch: 36610 mean train loss:  3.37610138e-03, bound:  3.15323114e-01\n",
      "Epoch: 36611 mean train loss:  3.37599451e-03, bound:  3.15323114e-01\n",
      "Epoch: 36612 mean train loss:  3.37594212e-03, bound:  3.15323085e-01\n",
      "Epoch: 36613 mean train loss:  3.37589555e-03, bound:  3.15323085e-01\n",
      "Epoch: 36614 mean train loss:  3.37584456e-03, bound:  3.15323085e-01\n",
      "Epoch: 36615 mean train loss:  3.37575912e-03, bound:  3.15323085e-01\n",
      "Epoch: 36616 mean train loss:  3.37566924e-03, bound:  3.15323085e-01\n",
      "Epoch: 36617 mean train loss:  3.37565714e-03, bound:  3.15323055e-01\n",
      "Epoch: 36618 mean train loss:  3.37556936e-03, bound:  3.15323055e-01\n",
      "Epoch: 36619 mean train loss:  3.37557215e-03, bound:  3.15323055e-01\n",
      "Epoch: 36620 mean train loss:  3.37547995e-03, bound:  3.15323055e-01\n",
      "Epoch: 36621 mean train loss:  3.37541802e-03, bound:  3.15323055e-01\n",
      "Epoch: 36622 mean train loss:  3.37534840e-03, bound:  3.15323055e-01\n",
      "Epoch: 36623 mean train loss:  3.37527180e-03, bound:  3.15323055e-01\n",
      "Epoch: 36624 mean train loss:  3.37522780e-03, bound:  3.15323055e-01\n",
      "Epoch: 36625 mean train loss:  3.37519357e-03, bound:  3.15323055e-01\n",
      "Epoch: 36626 mean train loss:  3.37512279e-03, bound:  3.15323055e-01\n",
      "Epoch: 36627 mean train loss:  3.37506458e-03, bound:  3.15323055e-01\n",
      "Epoch: 36628 mean train loss:  3.37497401e-03, bound:  3.15322995e-01\n",
      "Epoch: 36629 mean train loss:  3.37494398e-03, bound:  3.15322995e-01\n",
      "Epoch: 36630 mean train loss:  3.37485364e-03, bound:  3.15322995e-01\n",
      "Epoch: 36631 mean train loss:  3.37483874e-03, bound:  3.15322965e-01\n",
      "Epoch: 36632 mean train loss:  3.37471720e-03, bound:  3.15322965e-01\n",
      "Epoch: 36633 mean train loss:  3.37469205e-03, bound:  3.15322965e-01\n",
      "Epoch: 36634 mean train loss:  3.37465713e-03, bound:  3.15322965e-01\n",
      "Epoch: 36635 mean train loss:  3.37455980e-03, bound:  3.15322965e-01\n",
      "Epoch: 36636 mean train loss:  3.37450206e-03, bound:  3.15322936e-01\n",
      "Epoch: 36637 mean train loss:  3.37449112e-03, bound:  3.15322936e-01\n",
      "Epoch: 36638 mean train loss:  3.37438611e-03, bound:  3.15322936e-01\n",
      "Epoch: 36639 mean train loss:  3.37437447e-03, bound:  3.15322936e-01\n",
      "Epoch: 36640 mean train loss:  3.37429019e-03, bound:  3.15322936e-01\n",
      "Epoch: 36641 mean train loss:  3.37420520e-03, bound:  3.15322936e-01\n",
      "Epoch: 36642 mean train loss:  3.37415328e-03, bound:  3.15322936e-01\n",
      "Epoch: 36643 mean train loss:  3.37408716e-03, bound:  3.15322936e-01\n",
      "Epoch: 36644 mean train loss:  3.37402523e-03, bound:  3.15322936e-01\n",
      "Epoch: 36645 mean train loss:  3.37399030e-03, bound:  3.15322876e-01\n",
      "Epoch: 36646 mean train loss:  3.37396096e-03, bound:  3.15322876e-01\n",
      "Epoch: 36647 mean train loss:  3.37385200e-03, bound:  3.15322876e-01\n",
      "Epoch: 36648 mean train loss:  3.37378914e-03, bound:  3.15322876e-01\n",
      "Epoch: 36649 mean train loss:  3.37376725e-03, bound:  3.15322876e-01\n",
      "Epoch: 36650 mean train loss:  3.37369088e-03, bound:  3.15322876e-01\n",
      "Epoch: 36651 mean train loss:  3.37367225e-03, bound:  3.15322846e-01\n",
      "Epoch: 36652 mean train loss:  3.37361987e-03, bound:  3.15322846e-01\n",
      "Epoch: 36653 mean train loss:  3.37358960e-03, bound:  3.15322846e-01\n",
      "Epoch: 36654 mean train loss:  3.37357260e-03, bound:  3.15322846e-01\n",
      "Epoch: 36655 mean train loss:  3.37353465e-03, bound:  3.15322846e-01\n",
      "Epoch: 36656 mean train loss:  3.37355770e-03, bound:  3.15322846e-01\n",
      "Epoch: 36657 mean train loss:  3.37352953e-03, bound:  3.15322816e-01\n",
      "Epoch: 36658 mean train loss:  3.37343989e-03, bound:  3.15322816e-01\n",
      "Epoch: 36659 mean train loss:  3.37335537e-03, bound:  3.15322816e-01\n",
      "Epoch: 36660 mean train loss:  3.37319169e-03, bound:  3.15322816e-01\n",
      "Epoch: 36661 mean train loss:  3.37305292e-03, bound:  3.15322816e-01\n",
      "Epoch: 36662 mean train loss:  3.37296748e-03, bound:  3.15322816e-01\n",
      "Epoch: 36663 mean train loss:  3.37291998e-03, bound:  3.15322757e-01\n",
      "Epoch: 36664 mean train loss:  3.37287365e-03, bound:  3.15322757e-01\n",
      "Epoch: 36665 mean train loss:  3.37291206e-03, bound:  3.15322757e-01\n",
      "Epoch: 36666 mean train loss:  3.37285665e-03, bound:  3.15322757e-01\n",
      "Epoch: 36667 mean train loss:  3.37276934e-03, bound:  3.15322757e-01\n",
      "Epoch: 36668 mean train loss:  3.37260868e-03, bound:  3.15322757e-01\n",
      "Epoch: 36669 mean train loss:  3.37253022e-03, bound:  3.15322757e-01\n",
      "Epoch: 36670 mean train loss:  3.37248947e-03, bound:  3.15322757e-01\n",
      "Epoch: 36671 mean train loss:  3.37241800e-03, bound:  3.15322757e-01\n",
      "Epoch: 36672 mean train loss:  3.37240729e-03, bound:  3.15322727e-01\n",
      "Epoch: 36673 mean train loss:  3.37233441e-03, bound:  3.15322727e-01\n",
      "Epoch: 36674 mean train loss:  3.37227155e-03, bound:  3.15322727e-01\n",
      "Epoch: 36675 mean train loss:  3.37218586e-03, bound:  3.15322727e-01\n",
      "Epoch: 36676 mean train loss:  3.37214977e-03, bound:  3.15322727e-01\n",
      "Epoch: 36677 mean train loss:  3.37201799e-03, bound:  3.15322697e-01\n",
      "Epoch: 36678 mean train loss:  3.37205525e-03, bound:  3.15322697e-01\n",
      "Epoch: 36679 mean train loss:  3.37198516e-03, bound:  3.15322667e-01\n",
      "Epoch: 36680 mean train loss:  3.37194698e-03, bound:  3.15322667e-01\n",
      "Epoch: 36681 mean train loss:  3.37184174e-03, bound:  3.15322667e-01\n",
      "Epoch: 36682 mean train loss:  3.37176211e-03, bound:  3.15322667e-01\n",
      "Epoch: 36683 mean train loss:  3.37175978e-03, bound:  3.15322667e-01\n",
      "Epoch: 36684 mean train loss:  3.37165222e-03, bound:  3.15322638e-01\n",
      "Epoch: 36685 mean train loss:  3.37161520e-03, bound:  3.15322638e-01\n",
      "Epoch: 36686 mean train loss:  3.37156816e-03, bound:  3.15322638e-01\n",
      "Epoch: 36687 mean train loss:  3.37149156e-03, bound:  3.15322638e-01\n",
      "Epoch: 36688 mean train loss:  3.37144034e-03, bound:  3.15322638e-01\n",
      "Epoch: 36689 mean train loss:  3.37137631e-03, bound:  3.15322638e-01\n",
      "Epoch: 36690 mean train loss:  3.37135000e-03, bound:  3.15322608e-01\n",
      "Epoch: 36691 mean train loss:  3.37125990e-03, bound:  3.15322638e-01\n",
      "Epoch: 36692 mean train loss:  3.37121799e-03, bound:  3.15322608e-01\n",
      "Epoch: 36693 mean train loss:  3.37112066e-03, bound:  3.15322608e-01\n",
      "Epoch: 36694 mean train loss:  3.37110320e-03, bound:  3.15322608e-01\n",
      "Epoch: 36695 mean train loss:  3.37101263e-03, bound:  3.15322578e-01\n",
      "Epoch: 36696 mean train loss:  3.37095582e-03, bound:  3.15322578e-01\n",
      "Epoch: 36697 mean train loss:  3.37086036e-03, bound:  3.15322578e-01\n",
      "Epoch: 36698 mean train loss:  3.37086688e-03, bound:  3.15322548e-01\n",
      "Epoch: 36699 mean train loss:  3.37082008e-03, bound:  3.15322548e-01\n",
      "Epoch: 36700 mean train loss:  3.37075279e-03, bound:  3.15322518e-01\n",
      "Epoch: 36701 mean train loss:  3.37066874e-03, bound:  3.15322518e-01\n",
      "Epoch: 36702 mean train loss:  3.37063312e-03, bound:  3.15322518e-01\n",
      "Epoch: 36703 mean train loss:  3.37054371e-03, bound:  3.15322518e-01\n",
      "Epoch: 36704 mean train loss:  3.37045779e-03, bound:  3.15322518e-01\n",
      "Epoch: 36705 mean train loss:  3.37043172e-03, bound:  3.15322518e-01\n",
      "Epoch: 36706 mean train loss:  3.37038259e-03, bound:  3.15322518e-01\n",
      "Epoch: 36707 mean train loss:  3.37032322e-03, bound:  3.15322518e-01\n",
      "Epoch: 36708 mean train loss:  3.37027921e-03, bound:  3.15322518e-01\n",
      "Epoch: 36709 mean train loss:  3.37022217e-03, bound:  3.15322518e-01\n",
      "Epoch: 36710 mean train loss:  3.37015954e-03, bound:  3.15322489e-01\n",
      "Epoch: 36711 mean train loss:  3.37006641e-03, bound:  3.15322489e-01\n",
      "Epoch: 36712 mean train loss:  3.37003218e-03, bound:  3.15322489e-01\n",
      "Epoch: 36713 mean train loss:  3.36997700e-03, bound:  3.15322489e-01\n",
      "Epoch: 36714 mean train loss:  3.36994114e-03, bound:  3.15322489e-01\n",
      "Epoch: 36715 mean train loss:  3.36984359e-03, bound:  3.15322429e-01\n",
      "Epoch: 36716 mean train loss:  3.36981635e-03, bound:  3.15322429e-01\n",
      "Epoch: 36717 mean train loss:  3.36972345e-03, bound:  3.15322429e-01\n",
      "Epoch: 36718 mean train loss:  3.36964265e-03, bound:  3.15322429e-01\n",
      "Epoch: 36719 mean train loss:  3.36962123e-03, bound:  3.15322429e-01\n",
      "Epoch: 36720 mean train loss:  3.36955325e-03, bound:  3.15322429e-01\n",
      "Epoch: 36721 mean train loss:  3.36954487e-03, bound:  3.15322429e-01\n",
      "Epoch: 36722 mean train loss:  3.36942310e-03, bound:  3.15322429e-01\n",
      "Epoch: 36723 mean train loss:  3.36936861e-03, bound:  3.15322399e-01\n",
      "Epoch: 36724 mean train loss:  3.36935930e-03, bound:  3.15322399e-01\n",
      "Epoch: 36725 mean train loss:  3.36923380e-03, bound:  3.15322399e-01\n",
      "Epoch: 36726 mean train loss:  3.36921355e-03, bound:  3.15322399e-01\n",
      "Epoch: 36727 mean train loss:  3.36913485e-03, bound:  3.15322399e-01\n",
      "Epoch: 36728 mean train loss:  3.36909783e-03, bound:  3.15322399e-01\n",
      "Epoch: 36729 mean train loss:  3.36902565e-03, bound:  3.15322369e-01\n",
      "Epoch: 36730 mean train loss:  3.36899282e-03, bound:  3.15322369e-01\n",
      "Epoch: 36731 mean train loss:  3.36893671e-03, bound:  3.15322369e-01\n",
      "Epoch: 36732 mean train loss:  3.36886640e-03, bound:  3.15322369e-01\n",
      "Epoch: 36733 mean train loss:  3.36879632e-03, bound:  3.15322369e-01\n",
      "Epoch: 36734 mean train loss:  3.36870831e-03, bound:  3.15322369e-01\n",
      "Epoch: 36735 mean train loss:  3.36868130e-03, bound:  3.15322369e-01\n",
      "Epoch: 36736 mean train loss:  3.36862030e-03, bound:  3.15322369e-01\n",
      "Epoch: 36737 mean train loss:  3.36856768e-03, bound:  3.15322369e-01\n",
      "Epoch: 36738 mean train loss:  3.36850411e-03, bound:  3.15322369e-01\n",
      "Epoch: 36739 mean train loss:  3.36841773e-03, bound:  3.15322310e-01\n",
      "Epoch: 36740 mean train loss:  3.36835673e-03, bound:  3.15322310e-01\n",
      "Epoch: 36741 mean train loss:  3.36834532e-03, bound:  3.15322280e-01\n",
      "Epoch: 36742 mean train loss:  3.36822495e-03, bound:  3.15322280e-01\n",
      "Epoch: 36743 mean train loss:  3.36819235e-03, bound:  3.15322280e-01\n",
      "Epoch: 36744 mean train loss:  3.36817629e-03, bound:  3.15322280e-01\n",
      "Epoch: 36745 mean train loss:  3.36809433e-03, bound:  3.15322280e-01\n",
      "Epoch: 36746 mean train loss:  3.36798537e-03, bound:  3.15322280e-01\n",
      "Epoch: 36747 mean train loss:  3.36795743e-03, bound:  3.15322250e-01\n",
      "Epoch: 36748 mean train loss:  3.36786662e-03, bound:  3.15322250e-01\n",
      "Epoch: 36749 mean train loss:  3.36783100e-03, bound:  3.15322250e-01\n",
      "Epoch: 36750 mean train loss:  3.36782774e-03, bound:  3.15322250e-01\n",
      "Epoch: 36751 mean train loss:  3.36774043e-03, bound:  3.15322250e-01\n",
      "Epoch: 36752 mean train loss:  3.36766755e-03, bound:  3.15322250e-01\n",
      "Epoch: 36753 mean train loss:  3.36760865e-03, bound:  3.15322250e-01\n",
      "Epoch: 36754 mean train loss:  3.36756581e-03, bound:  3.15322250e-01\n",
      "Epoch: 36755 mean train loss:  3.36749409e-03, bound:  3.15322250e-01\n",
      "Epoch: 36756 mean train loss:  3.36744543e-03, bound:  3.15322191e-01\n",
      "Epoch: 36757 mean train loss:  3.36740143e-03, bound:  3.15322191e-01\n",
      "Epoch: 36758 mean train loss:  3.36732622e-03, bound:  3.15322191e-01\n",
      "Epoch: 36759 mean train loss:  3.36722331e-03, bound:  3.15322191e-01\n",
      "Epoch: 36760 mean train loss:  3.36718210e-03, bound:  3.15322191e-01\n",
      "Epoch: 36761 mean train loss:  3.36712715e-03, bound:  3.15322191e-01\n",
      "Epoch: 36762 mean train loss:  3.36705055e-03, bound:  3.15322161e-01\n",
      "Epoch: 36763 mean train loss:  3.36704962e-03, bound:  3.15322161e-01\n",
      "Epoch: 36764 mean train loss:  3.36697302e-03, bound:  3.15322161e-01\n",
      "Epoch: 36765 mean train loss:  3.36686894e-03, bound:  3.15322131e-01\n",
      "Epoch: 36766 mean train loss:  3.36684054e-03, bound:  3.15322131e-01\n",
      "Epoch: 36767 mean train loss:  3.36676347e-03, bound:  3.15322131e-01\n",
      "Epoch: 36768 mean train loss:  3.36674624e-03, bound:  3.15322131e-01\n",
      "Epoch: 36769 mean train loss:  3.36663960e-03, bound:  3.15322131e-01\n",
      "Epoch: 36770 mean train loss:  3.36661213e-03, bound:  3.15322131e-01\n",
      "Epoch: 36771 mean train loss:  3.36655043e-03, bound:  3.15322101e-01\n",
      "Epoch: 36772 mean train loss:  3.36644356e-03, bound:  3.15322101e-01\n",
      "Epoch: 36773 mean train loss:  3.36643541e-03, bound:  3.15322101e-01\n",
      "Epoch: 36774 mean train loss:  3.36633902e-03, bound:  3.15322071e-01\n",
      "Epoch: 36775 mean train loss:  3.36634042e-03, bound:  3.15322071e-01\n",
      "Epoch: 36776 mean train loss:  3.36625567e-03, bound:  3.15322071e-01\n",
      "Epoch: 36777 mean train loss:  3.36622400e-03, bound:  3.15322071e-01\n",
      "Epoch: 36778 mean train loss:  3.36615881e-03, bound:  3.15322071e-01\n",
      "Epoch: 36779 mean train loss:  3.36614554e-03, bound:  3.15322071e-01\n",
      "Epoch: 36780 mean train loss:  3.36610340e-03, bound:  3.15322071e-01\n",
      "Epoch: 36781 mean train loss:  3.36604752e-03, bound:  3.15322042e-01\n",
      "Epoch: 36782 mean train loss:  3.36598605e-03, bound:  3.15322042e-01\n",
      "Epoch: 36783 mean train loss:  3.36593436e-03, bound:  3.15322042e-01\n",
      "Epoch: 36784 mean train loss:  3.36583471e-03, bound:  3.15322042e-01\n",
      "Epoch: 36785 mean train loss:  3.36574577e-03, bound:  3.15322042e-01\n",
      "Epoch: 36786 mean train loss:  3.36568733e-03, bound:  3.15322042e-01\n",
      "Epoch: 36787 mean train loss:  3.36559978e-03, bound:  3.15322012e-01\n",
      "Epoch: 36788 mean train loss:  3.36557138e-03, bound:  3.15322012e-01\n",
      "Epoch: 36789 mean train loss:  3.36550688e-03, bound:  3.15321982e-01\n",
      "Epoch: 36790 mean train loss:  3.36541072e-03, bound:  3.15321982e-01\n",
      "Epoch: 36791 mean train loss:  3.36539745e-03, bound:  3.15321982e-01\n",
      "Epoch: 36792 mean train loss:  3.36536160e-03, bound:  3.15321982e-01\n",
      "Epoch: 36793 mean train loss:  3.36527964e-03, bound:  3.15321952e-01\n",
      "Epoch: 36794 mean train loss:  3.36519792e-03, bound:  3.15321952e-01\n",
      "Epoch: 36795 mean train loss:  3.36514856e-03, bound:  3.15321952e-01\n",
      "Epoch: 36796 mean train loss:  3.36507009e-03, bound:  3.15321952e-01\n",
      "Epoch: 36797 mean train loss:  3.36499163e-03, bound:  3.15321952e-01\n",
      "Epoch: 36798 mean train loss:  3.36490758e-03, bound:  3.15321952e-01\n",
      "Epoch: 36799 mean train loss:  3.36492131e-03, bound:  3.15321952e-01\n",
      "Epoch: 36800 mean train loss:  3.36483843e-03, bound:  3.15321952e-01\n",
      "Epoch: 36801 mean train loss:  3.36480676e-03, bound:  3.15321922e-01\n",
      "Epoch: 36802 mean train loss:  3.36477882e-03, bound:  3.15321922e-01\n",
      "Epoch: 36803 mean train loss:  3.36467288e-03, bound:  3.15321922e-01\n",
      "Epoch: 36804 mean train loss:  3.36458674e-03, bound:  3.15321892e-01\n",
      "Epoch: 36805 mean train loss:  3.36455461e-03, bound:  3.15321892e-01\n",
      "Epoch: 36806 mean train loss:  3.36445682e-03, bound:  3.15321892e-01\n",
      "Epoch: 36807 mean train loss:  3.36436764e-03, bound:  3.15321892e-01\n",
      "Epoch: 36808 mean train loss:  3.36435018e-03, bound:  3.15321863e-01\n",
      "Epoch: 36809 mean train loss:  3.36431013e-03, bound:  3.15321863e-01\n",
      "Epoch: 36810 mean train loss:  3.36426287e-03, bound:  3.15321833e-01\n",
      "Epoch: 36811 mean train loss:  3.36418138e-03, bound:  3.15321833e-01\n",
      "Epoch: 36812 mean train loss:  3.36414739e-03, bound:  3.15321833e-01\n",
      "Epoch: 36813 mean train loss:  3.36405216e-03, bound:  3.15321833e-01\n",
      "Epoch: 36814 mean train loss:  3.36401933e-03, bound:  3.15321833e-01\n",
      "Epoch: 36815 mean train loss:  3.36395809e-03, bound:  3.15321833e-01\n",
      "Epoch: 36816 mean train loss:  3.36390967e-03, bound:  3.15321833e-01\n",
      "Epoch: 36817 mean train loss:  3.36379465e-03, bound:  3.15321833e-01\n",
      "Epoch: 36818 mean train loss:  3.36376391e-03, bound:  3.15321833e-01\n",
      "Epoch: 36819 mean train loss:  3.36369872e-03, bound:  3.15321833e-01\n",
      "Epoch: 36820 mean train loss:  3.36369011e-03, bound:  3.15321803e-01\n",
      "Epoch: 36821 mean train loss:  3.36360373e-03, bound:  3.15321803e-01\n",
      "Epoch: 36822 mean train loss:  3.36358766e-03, bound:  3.15321803e-01\n",
      "Epoch: 36823 mean train loss:  3.36350640e-03, bound:  3.15321803e-01\n",
      "Epoch: 36824 mean train loss:  3.36343911e-03, bound:  3.15321803e-01\n",
      "Epoch: 36825 mean train loss:  3.36334249e-03, bound:  3.15321803e-01\n",
      "Epoch: 36826 mean train loss:  3.36330291e-03, bound:  3.15321803e-01\n",
      "Epoch: 36827 mean train loss:  3.36321048e-03, bound:  3.15321743e-01\n",
      "Epoch: 36828 mean train loss:  3.36316274e-03, bound:  3.15321714e-01\n",
      "Epoch: 36829 mean train loss:  3.36313737e-03, bound:  3.15321743e-01\n",
      "Epoch: 36830 mean train loss:  3.36306775e-03, bound:  3.15321714e-01\n",
      "Epoch: 36831 mean train loss:  3.36305075e-03, bound:  3.15321714e-01\n",
      "Epoch: 36832 mean train loss:  3.36297299e-03, bound:  3.15321714e-01\n",
      "Epoch: 36833 mean train loss:  3.36290547e-03, bound:  3.15321714e-01\n",
      "Epoch: 36834 mean train loss:  3.36280209e-03, bound:  3.15321714e-01\n",
      "Epoch: 36835 mean train loss:  3.36279534e-03, bound:  3.15321714e-01\n",
      "Epoch: 36836 mean train loss:  3.36272549e-03, bound:  3.15321714e-01\n",
      "Epoch: 36837 mean train loss:  3.36267543e-03, bound:  3.15321714e-01\n",
      "Epoch: 36838 mean train loss:  3.36257461e-03, bound:  3.15321714e-01\n",
      "Epoch: 36839 mean train loss:  3.36257741e-03, bound:  3.15321684e-01\n",
      "Epoch: 36840 mean train loss:  3.36248591e-03, bound:  3.15321684e-01\n",
      "Epoch: 36841 mean train loss:  3.36241093e-03, bound:  3.15321684e-01\n",
      "Epoch: 36842 mean train loss:  3.36239394e-03, bound:  3.15321654e-01\n",
      "Epoch: 36843 mean train loss:  3.36229848e-03, bound:  3.15321654e-01\n",
      "Epoch: 36844 mean train loss:  3.36226239e-03, bound:  3.15321654e-01\n",
      "Epoch: 36845 mean train loss:  3.36216181e-03, bound:  3.15321624e-01\n",
      "Epoch: 36846 mean train loss:  3.36209731e-03, bound:  3.15321624e-01\n",
      "Epoch: 36847 mean train loss:  3.36203189e-03, bound:  3.15321624e-01\n",
      "Epoch: 36848 mean train loss:  3.36204283e-03, bound:  3.15321624e-01\n",
      "Epoch: 36849 mean train loss:  3.36197647e-03, bound:  3.15321624e-01\n",
      "Epoch: 36850 mean train loss:  3.36190104e-03, bound:  3.15321624e-01\n",
      "Epoch: 36851 mean train loss:  3.36187030e-03, bound:  3.15321594e-01\n",
      "Epoch: 36852 mean train loss:  3.36185121e-03, bound:  3.15321594e-01\n",
      "Epoch: 36853 mean train loss:  3.36171756e-03, bound:  3.15321594e-01\n",
      "Epoch: 36854 mean train loss:  3.36166355e-03, bound:  3.15321594e-01\n",
      "Epoch: 36855 mean train loss:  3.36158951e-03, bound:  3.15321594e-01\n",
      "Epoch: 36856 mean train loss:  3.36154713e-03, bound:  3.15321594e-01\n",
      "Epoch: 36857 mean train loss:  3.36145260e-03, bound:  3.15321594e-01\n",
      "Epoch: 36858 mean train loss:  3.36144655e-03, bound:  3.15321565e-01\n",
      "Epoch: 36859 mean train loss:  3.36136133e-03, bound:  3.15321565e-01\n",
      "Epoch: 36860 mean train loss:  3.36131081e-03, bound:  3.15321565e-01\n",
      "Epoch: 36861 mean train loss:  3.36123630e-03, bound:  3.15321535e-01\n",
      "Epoch: 36862 mean train loss:  3.36122583e-03, bound:  3.15321535e-01\n",
      "Epoch: 36863 mean train loss:  3.36114457e-03, bound:  3.15321535e-01\n",
      "Epoch: 36864 mean train loss:  3.36106331e-03, bound:  3.15321505e-01\n",
      "Epoch: 36865 mean train loss:  3.36101372e-03, bound:  3.15321505e-01\n",
      "Epoch: 36866 mean train loss:  3.36094596e-03, bound:  3.15321505e-01\n",
      "Epoch: 36867 mean train loss:  3.36088357e-03, bound:  3.15321505e-01\n",
      "Epoch: 36868 mean train loss:  3.36084422e-03, bound:  3.15321505e-01\n",
      "Epoch: 36869 mean train loss:  3.36080394e-03, bound:  3.15321505e-01\n",
      "Epoch: 36870 mean train loss:  3.36071174e-03, bound:  3.15321505e-01\n",
      "Epoch: 36871 mean train loss:  3.36067239e-03, bound:  3.15321505e-01\n",
      "Epoch: 36872 mean train loss:  3.36056645e-03, bound:  3.15321475e-01\n",
      "Epoch: 36873 mean train loss:  3.36054200e-03, bound:  3.15321475e-01\n",
      "Epoch: 36874 mean train loss:  3.36050731e-03, bound:  3.15321475e-01\n",
      "Epoch: 36875 mean train loss:  3.36037809e-03, bound:  3.15321475e-01\n",
      "Epoch: 36876 mean train loss:  3.36037227e-03, bound:  3.15321445e-01\n",
      "Epoch: 36877 mean train loss:  3.36032547e-03, bound:  3.15321445e-01\n",
      "Epoch: 36878 mean train loss:  3.36022000e-03, bound:  3.15321445e-01\n",
      "Epoch: 36879 mean train loss:  3.36019602e-03, bound:  3.15321416e-01\n",
      "Epoch: 36880 mean train loss:  3.36010009e-03, bound:  3.15321416e-01\n",
      "Epoch: 36881 mean train loss:  3.36006959e-03, bound:  3.15321416e-01\n",
      "Epoch: 36882 mean train loss:  3.35998298e-03, bound:  3.15321416e-01\n",
      "Epoch: 36883 mean train loss:  3.35995154e-03, bound:  3.15321416e-01\n",
      "Epoch: 36884 mean train loss:  3.35992454e-03, bound:  3.15321416e-01\n",
      "Epoch: 36885 mean train loss:  3.35985399e-03, bound:  3.15321416e-01\n",
      "Epoch: 36886 mean train loss:  3.35977739e-03, bound:  3.15321386e-01\n",
      "Epoch: 36887 mean train loss:  3.35971452e-03, bound:  3.15321386e-01\n",
      "Epoch: 36888 mean train loss:  3.35965818e-03, bound:  3.15321386e-01\n",
      "Epoch: 36889 mean train loss:  3.35963187e-03, bound:  3.15321386e-01\n",
      "Epoch: 36890 mean train loss:  3.35960137e-03, bound:  3.15321386e-01\n",
      "Epoch: 36891 mean train loss:  3.35956784e-03, bound:  3.15321356e-01\n",
      "Epoch: 36892 mean train loss:  3.35942535e-03, bound:  3.15321356e-01\n",
      "Epoch: 36893 mean train loss:  3.35936877e-03, bound:  3.15321356e-01\n",
      "Epoch: 36894 mean train loss:  3.35929892e-03, bound:  3.15321356e-01\n",
      "Epoch: 36895 mean train loss:  3.35921883e-03, bound:  3.15321326e-01\n",
      "Epoch: 36896 mean train loss:  3.35920928e-03, bound:  3.15321326e-01\n",
      "Epoch: 36897 mean train loss:  3.35909845e-03, bound:  3.15321326e-01\n",
      "Epoch: 36898 mean train loss:  3.35905352e-03, bound:  3.15321326e-01\n",
      "Epoch: 36899 mean train loss:  3.35900439e-03, bound:  3.15321326e-01\n",
      "Epoch: 36900 mean train loss:  3.35893361e-03, bound:  3.15321326e-01\n",
      "Epoch: 36901 mean train loss:  3.35889612e-03, bound:  3.15321326e-01\n",
      "Epoch: 36902 mean train loss:  3.35884630e-03, bound:  3.15321296e-01\n",
      "Epoch: 36903 mean train loss:  3.35871149e-03, bound:  3.15321296e-01\n",
      "Epoch: 36904 mean train loss:  3.35874315e-03, bound:  3.15321296e-01\n",
      "Epoch: 36905 mean train loss:  3.35862767e-03, bound:  3.15321267e-01\n",
      "Epoch: 36906 mean train loss:  3.35857435e-03, bound:  3.15321267e-01\n",
      "Epoch: 36907 mean train loss:  3.35852033e-03, bound:  3.15321267e-01\n",
      "Epoch: 36908 mean train loss:  3.35845468e-03, bound:  3.15321267e-01\n",
      "Epoch: 36909 mean train loss:  3.35841160e-03, bound:  3.15321267e-01\n",
      "Epoch: 36910 mean train loss:  3.35835805e-03, bound:  3.15321237e-01\n",
      "Epoch: 36911 mean train loss:  3.35832639e-03, bound:  3.15321237e-01\n",
      "Epoch: 36912 mean train loss:  3.35821346e-03, bound:  3.15321237e-01\n",
      "Epoch: 36913 mean train loss:  3.35820811e-03, bound:  3.15321237e-01\n",
      "Epoch: 36914 mean train loss:  3.35812452e-03, bound:  3.15321237e-01\n",
      "Epoch: 36915 mean train loss:  3.35810264e-03, bound:  3.15321237e-01\n",
      "Epoch: 36916 mean train loss:  3.35805234e-03, bound:  3.15321207e-01\n",
      "Epoch: 36917 mean train loss:  3.35796201e-03, bound:  3.15321207e-01\n",
      "Epoch: 36918 mean train loss:  3.35794943e-03, bound:  3.15321207e-01\n",
      "Epoch: 36919 mean train loss:  3.35788447e-03, bound:  3.15321177e-01\n",
      "Epoch: 36920 mean train loss:  3.35785886e-03, bound:  3.15321177e-01\n",
      "Epoch: 36921 mean train loss:  3.35780438e-03, bound:  3.15321177e-01\n",
      "Epoch: 36922 mean train loss:  3.35768284e-03, bound:  3.15321177e-01\n",
      "Epoch: 36923 mean train loss:  3.35762626e-03, bound:  3.15321147e-01\n",
      "Epoch: 36924 mean train loss:  3.35757667e-03, bound:  3.15321147e-01\n",
      "Epoch: 36925 mean train loss:  3.35747446e-03, bound:  3.15321147e-01\n",
      "Epoch: 36926 mean train loss:  3.35743651e-03, bound:  3.15321147e-01\n",
      "Epoch: 36927 mean train loss:  3.35730216e-03, bound:  3.15321147e-01\n",
      "Epoch: 36928 mean train loss:  3.35727027e-03, bound:  3.15321118e-01\n",
      "Epoch: 36929 mean train loss:  3.35727516e-03, bound:  3.15321118e-01\n",
      "Epoch: 36930 mean train loss:  3.35716037e-03, bound:  3.15321118e-01\n",
      "Epoch: 36931 mean train loss:  3.35715432e-03, bound:  3.15321118e-01\n",
      "Epoch: 36932 mean train loss:  3.35708563e-03, bound:  3.15321118e-01\n",
      "Epoch: 36933 mean train loss:  3.35699553e-03, bound:  3.15321118e-01\n",
      "Epoch: 36934 mean train loss:  3.35694663e-03, bound:  3.15321088e-01\n",
      "Epoch: 36935 mean train loss:  3.35689867e-03, bound:  3.15321088e-01\n",
      "Epoch: 36936 mean train loss:  3.35683371e-03, bound:  3.15321088e-01\n",
      "Epoch: 36937 mean train loss:  3.35674407e-03, bound:  3.15321088e-01\n",
      "Epoch: 36938 mean train loss:  3.35672661e-03, bound:  3.15321088e-01\n",
      "Epoch: 36939 mean train loss:  3.35665490e-03, bound:  3.15321058e-01\n",
      "Epoch: 36940 mean train loss:  3.35658155e-03, bound:  3.15321058e-01\n",
      "Epoch: 36941 mean train loss:  3.35654337e-03, bound:  3.15321028e-01\n",
      "Epoch: 36942 mean train loss:  3.35646491e-03, bound:  3.15321028e-01\n",
      "Epoch: 36943 mean train loss:  3.35638202e-03, bound:  3.15321028e-01\n",
      "Epoch: 36944 mean train loss:  3.35635268e-03, bound:  3.15321028e-01\n",
      "Epoch: 36945 mean train loss:  3.35633475e-03, bound:  3.15321028e-01\n",
      "Epoch: 36946 mean train loss:  3.35624209e-03, bound:  3.15321028e-01\n",
      "Epoch: 36947 mean train loss:  3.35618760e-03, bound:  3.15321028e-01\n",
      "Epoch: 36948 mean train loss:  3.35610867e-03, bound:  3.15321028e-01\n",
      "Epoch: 36949 mean train loss:  3.35605396e-03, bound:  3.15321028e-01\n",
      "Epoch: 36950 mean train loss:  3.35598947e-03, bound:  3.15320998e-01\n",
      "Epoch: 36951 mean train loss:  3.35592031e-03, bound:  3.15320998e-01\n",
      "Epoch: 36952 mean train loss:  3.35588260e-03, bound:  3.15320998e-01\n",
      "Epoch: 36953 mean train loss:  3.35583324e-03, bound:  3.15320998e-01\n",
      "Epoch: 36954 mean train loss:  3.35580856e-03, bound:  3.15320969e-01\n",
      "Epoch: 36955 mean train loss:  3.35567934e-03, bound:  3.15320939e-01\n",
      "Epoch: 36956 mean train loss:  3.35568236e-03, bound:  3.15320939e-01\n",
      "Epoch: 36957 mean train loss:  3.35558830e-03, bound:  3.15320939e-01\n",
      "Epoch: 36958 mean train loss:  3.35554406e-03, bound:  3.15320939e-01\n",
      "Epoch: 36959 mean train loss:  3.35550471e-03, bound:  3.15320909e-01\n",
      "Epoch: 36960 mean train loss:  3.35541158e-03, bound:  3.15320909e-01\n",
      "Epoch: 36961 mean train loss:  3.35536292e-03, bound:  3.15320909e-01\n",
      "Epoch: 36962 mean train loss:  3.35531612e-03, bound:  3.15320909e-01\n",
      "Epoch: 36963 mean train loss:  3.35525838e-03, bound:  3.15320909e-01\n",
      "Epoch: 36964 mean train loss:  3.35520296e-03, bound:  3.15320909e-01\n",
      "Epoch: 36965 mean train loss:  3.35518573e-03, bound:  3.15320909e-01\n",
      "Epoch: 36966 mean train loss:  3.35515477e-03, bound:  3.15320909e-01\n",
      "Epoch: 36967 mean train loss:  3.35504347e-03, bound:  3.15320909e-01\n",
      "Epoch: 36968 mean train loss:  3.35495197e-03, bound:  3.15320909e-01\n",
      "Epoch: 36969 mean train loss:  3.35493847e-03, bound:  3.15320879e-01\n",
      "Epoch: 36970 mean train loss:  3.35481996e-03, bound:  3.15320879e-01\n",
      "Epoch: 36971 mean train loss:  3.35476478e-03, bound:  3.15320879e-01\n",
      "Epoch: 36972 mean train loss:  3.35469074e-03, bound:  3.15320849e-01\n",
      "Epoch: 36973 mean train loss:  3.35466769e-03, bound:  3.15320849e-01\n",
      "Epoch: 36974 mean train loss:  3.35461646e-03, bound:  3.15320849e-01\n",
      "Epoch: 36975 mean train loss:  3.35457688e-03, bound:  3.15320849e-01\n",
      "Epoch: 36976 mean train loss:  3.35450959e-03, bound:  3.15320820e-01\n",
      "Epoch: 36977 mean train loss:  3.35444813e-03, bound:  3.15320820e-01\n",
      "Epoch: 36978 mean train loss:  3.35437292e-03, bound:  3.15320820e-01\n",
      "Epoch: 36979 mean train loss:  3.35434289e-03, bound:  3.15320820e-01\n",
      "Epoch: 36980 mean train loss:  3.35421343e-03, bound:  3.15320820e-01\n",
      "Epoch: 36981 mean train loss:  3.35418968e-03, bound:  3.15320820e-01\n",
      "Epoch: 36982 mean train loss:  3.35413916e-03, bound:  3.15320820e-01\n",
      "Epoch: 36983 mean train loss:  3.35404836e-03, bound:  3.15320820e-01\n",
      "Epoch: 36984 mean train loss:  3.35399644e-03, bound:  3.15320790e-01\n",
      "Epoch: 36985 mean train loss:  3.35394451e-03, bound:  3.15320790e-01\n",
      "Epoch: 36986 mean train loss:  3.35389702e-03, bound:  3.15320790e-01\n",
      "Epoch: 36987 mean train loss:  3.35388514e-03, bound:  3.15320760e-01\n",
      "Epoch: 36988 mean train loss:  3.35376919e-03, bound:  3.15320760e-01\n",
      "Epoch: 36989 mean train loss:  3.35374451e-03, bound:  3.15320760e-01\n",
      "Epoch: 36990 mean train loss:  3.35370773e-03, bound:  3.15320760e-01\n",
      "Epoch: 36991 mean train loss:  3.35360505e-03, bound:  3.15320730e-01\n",
      "Epoch: 36992 mean train loss:  3.35352891e-03, bound:  3.15320730e-01\n",
      "Epoch: 36993 mean train loss:  3.35348910e-03, bound:  3.15320730e-01\n",
      "Epoch: 36994 mean train loss:  3.35340016e-03, bound:  3.15320700e-01\n",
      "Epoch: 36995 mean train loss:  3.35338106e-03, bound:  3.15320700e-01\n",
      "Epoch: 36996 mean train loss:  3.35332588e-03, bound:  3.15320700e-01\n",
      "Epoch: 36997 mean train loss:  3.35324393e-03, bound:  3.15320700e-01\n",
      "Epoch: 36998 mean train loss:  3.35317547e-03, bound:  3.15320700e-01\n",
      "Epoch: 36999 mean train loss:  3.35312216e-03, bound:  3.15320700e-01\n",
      "Epoch: 37000 mean train loss:  3.35304998e-03, bound:  3.15320700e-01\n",
      "Epoch: 37001 mean train loss:  3.35300039e-03, bound:  3.15320700e-01\n",
      "Epoch: 37002 mean train loss:  3.35296639e-03, bound:  3.15320671e-01\n",
      "Epoch: 37003 mean train loss:  3.35287512e-03, bound:  3.15320671e-01\n",
      "Epoch: 37004 mean train loss:  3.35284811e-03, bound:  3.15320671e-01\n",
      "Epoch: 37005 mean train loss:  3.35279247e-03, bound:  3.15320671e-01\n",
      "Epoch: 37006 mean train loss:  3.35270679e-03, bound:  3.15320611e-01\n",
      "Epoch: 37007 mean train loss:  3.35265021e-03, bound:  3.15320611e-01\n",
      "Epoch: 37008 mean train loss:  3.35262832e-03, bound:  3.15320611e-01\n",
      "Epoch: 37009 mean train loss:  3.35258641e-03, bound:  3.15320611e-01\n",
      "Epoch: 37010 mean train loss:  3.35251377e-03, bound:  3.15320611e-01\n",
      "Epoch: 37011 mean train loss:  3.35246534e-03, bound:  3.15320611e-01\n",
      "Epoch: 37012 mean train loss:  3.35241063e-03, bound:  3.15320611e-01\n",
      "Epoch: 37013 mean train loss:  3.35235032e-03, bound:  3.15320611e-01\n",
      "Epoch: 37014 mean train loss:  3.35230515e-03, bound:  3.15320611e-01\n",
      "Epoch: 37015 mean train loss:  3.35227000e-03, bound:  3.15320581e-01\n",
      "Epoch: 37016 mean train loss:  3.35218711e-03, bound:  3.15320581e-01\n",
      "Epoch: 37017 mean train loss:  3.35207232e-03, bound:  3.15320581e-01\n",
      "Epoch: 37018 mean train loss:  3.35201272e-03, bound:  3.15320581e-01\n",
      "Epoch: 37019 mean train loss:  3.35195148e-03, bound:  3.15320581e-01\n",
      "Epoch: 37020 mean train loss:  3.35190468e-03, bound:  3.15320581e-01\n",
      "Epoch: 37021 mean train loss:  3.35187162e-03, bound:  3.15320551e-01\n",
      "Epoch: 37022 mean train loss:  3.35183274e-03, bound:  3.15320551e-01\n",
      "Epoch: 37023 mean train loss:  3.35173821e-03, bound:  3.15320551e-01\n",
      "Epoch: 37024 mean train loss:  3.35171982e-03, bound:  3.15320551e-01\n",
      "Epoch: 37025 mean train loss:  3.35169095e-03, bound:  3.15320551e-01\n",
      "Epoch: 37026 mean train loss:  3.35160526e-03, bound:  3.15320522e-01\n",
      "Epoch: 37027 mean train loss:  3.35154589e-03, bound:  3.15320522e-01\n",
      "Epoch: 37028 mean train loss:  3.35150561e-03, bound:  3.15320522e-01\n",
      "Epoch: 37029 mean train loss:  3.35140084e-03, bound:  3.15320522e-01\n",
      "Epoch: 37030 mean train loss:  3.35130817e-03, bound:  3.15320522e-01\n",
      "Epoch: 37031 mean train loss:  3.35123879e-03, bound:  3.15320492e-01\n",
      "Epoch: 37032 mean train loss:  3.35117965e-03, bound:  3.15320492e-01\n",
      "Epoch: 37033 mean train loss:  3.35113890e-03, bound:  3.15320462e-01\n",
      "Epoch: 37034 mean train loss:  3.35110864e-03, bound:  3.15320462e-01\n",
      "Epoch: 37035 mean train loss:  3.35103134e-03, bound:  3.15320462e-01\n",
      "Epoch: 37036 mean train loss:  3.35095427e-03, bound:  3.15320462e-01\n",
      "Epoch: 37037 mean train loss:  3.35096591e-03, bound:  3.15320462e-01\n",
      "Epoch: 37038 mean train loss:  3.35086230e-03, bound:  3.15320462e-01\n",
      "Epoch: 37039 mean train loss:  3.35082202e-03, bound:  3.15320462e-01\n",
      "Epoch: 37040 mean train loss:  3.35073774e-03, bound:  3.15320432e-01\n",
      "Epoch: 37041 mean train loss:  3.35066556e-03, bound:  3.15320432e-01\n",
      "Epoch: 37042 mean train loss:  3.35059455e-03, bound:  3.15320432e-01\n",
      "Epoch: 37043 mean train loss:  3.35058453e-03, bound:  3.15320432e-01\n",
      "Epoch: 37044 mean train loss:  3.35051329e-03, bound:  3.15320402e-01\n",
      "Epoch: 37045 mean train loss:  3.35043506e-03, bound:  3.15320402e-01\n",
      "Epoch: 37046 mean train loss:  3.35039571e-03, bound:  3.15320402e-01\n",
      "Epoch: 37047 mean train loss:  3.35029117e-03, bound:  3.15320402e-01\n",
      "Epoch: 37048 mean train loss:  3.35022970e-03, bound:  3.15320402e-01\n",
      "Epoch: 37049 mean train loss:  3.35019687e-03, bound:  3.15320402e-01\n",
      "Epoch: 37050 mean train loss:  3.35015496e-03, bound:  3.15320373e-01\n",
      "Epoch: 37051 mean train loss:  3.35007533e-03, bound:  3.15320373e-01\n",
      "Epoch: 37052 mean train loss:  3.35008255e-03, bound:  3.15320343e-01\n",
      "Epoch: 37053 mean train loss:  3.35001922e-03, bound:  3.15320343e-01\n",
      "Epoch: 37054 mean train loss:  3.34994122e-03, bound:  3.15320343e-01\n",
      "Epoch: 37055 mean train loss:  3.34984972e-03, bound:  3.15320343e-01\n",
      "Epoch: 37056 mean train loss:  3.34983855e-03, bound:  3.15320343e-01\n",
      "Epoch: 37057 mean train loss:  3.34974704e-03, bound:  3.15320343e-01\n",
      "Epoch: 37058 mean train loss:  3.34973796e-03, bound:  3.15320313e-01\n",
      "Epoch: 37059 mean train loss:  3.34964786e-03, bound:  3.15320313e-01\n",
      "Epoch: 37060 mean train loss:  3.34957684e-03, bound:  3.15320313e-01\n",
      "Epoch: 37061 mean train loss:  3.34951468e-03, bound:  3.15320313e-01\n",
      "Epoch: 37062 mean train loss:  3.34952376e-03, bound:  3.15320313e-01\n",
      "Epoch: 37063 mean train loss:  3.34941340e-03, bound:  3.15320313e-01\n",
      "Epoch: 37064 mean train loss:  3.34936171e-03, bound:  3.15320283e-01\n",
      "Epoch: 37065 mean train loss:  3.34929908e-03, bound:  3.15320283e-01\n",
      "Epoch: 37066 mean train loss:  3.34921479e-03, bound:  3.15320283e-01\n",
      "Epoch: 37067 mean train loss:  3.34917475e-03, bound:  3.15320283e-01\n",
      "Epoch: 37068 mean train loss:  3.34911747e-03, bound:  3.15320253e-01\n",
      "Epoch: 37069 mean train loss:  3.34905204e-03, bound:  3.15320253e-01\n",
      "Epoch: 37070 mean train loss:  3.34898615e-03, bound:  3.15320224e-01\n",
      "Epoch: 37071 mean train loss:  3.34894704e-03, bound:  3.15320224e-01\n",
      "Epoch: 37072 mean train loss:  3.34890024e-03, bound:  3.15320224e-01\n",
      "Epoch: 37073 mean train loss:  3.34883714e-03, bound:  3.15320224e-01\n",
      "Epoch: 37074 mean train loss:  3.34877195e-03, bound:  3.15320224e-01\n",
      "Epoch: 37075 mean train loss:  3.34871258e-03, bound:  3.15320224e-01\n",
      "Epoch: 37076 mean train loss:  3.34864785e-03, bound:  3.15320224e-01\n",
      "Epoch: 37077 mean train loss:  3.34859453e-03, bound:  3.15320224e-01\n",
      "Epoch: 37078 mean train loss:  3.34853563e-03, bound:  3.15320224e-01\n",
      "Epoch: 37079 mean train loss:  3.34844762e-03, bound:  3.15320224e-01\n",
      "Epoch: 37080 mean train loss:  3.34840501e-03, bound:  3.15320224e-01\n",
      "Epoch: 37081 mean train loss:  3.34833306e-03, bound:  3.15320164e-01\n",
      "Epoch: 37082 mean train loss:  3.34829371e-03, bound:  3.15320164e-01\n",
      "Epoch: 37083 mean train loss:  3.34828487e-03, bound:  3.15320164e-01\n",
      "Epoch: 37084 mean train loss:  3.34812771e-03, bound:  3.15320164e-01\n",
      "Epoch: 37085 mean train loss:  3.34811769e-03, bound:  3.15320164e-01\n",
      "Epoch: 37086 mean train loss:  3.34805017e-03, bound:  3.15320164e-01\n",
      "Epoch: 37087 mean train loss:  3.34797101e-03, bound:  3.15320134e-01\n",
      "Epoch: 37088 mean train loss:  3.34792398e-03, bound:  3.15320134e-01\n",
      "Epoch: 37089 mean train loss:  3.34784994e-03, bound:  3.15320134e-01\n",
      "Epoch: 37090 mean train loss:  3.34779569e-03, bound:  3.15320134e-01\n",
      "Epoch: 37091 mean train loss:  3.34772281e-03, bound:  3.15320134e-01\n",
      "Epoch: 37092 mean train loss:  3.34768067e-03, bound:  3.15320134e-01\n",
      "Epoch: 37093 mean train loss:  3.34763736e-03, bound:  3.15320134e-01\n",
      "Epoch: 37094 mean train loss:  3.34756565e-03, bound:  3.15320134e-01\n",
      "Epoch: 37095 mean train loss:  3.34750838e-03, bound:  3.15320104e-01\n",
      "Epoch: 37096 mean train loss:  3.34744132e-03, bound:  3.15320104e-01\n",
      "Epoch: 37097 mean train loss:  3.34737799e-03, bound:  3.15320104e-01\n",
      "Epoch: 37098 mean train loss:  3.34733329e-03, bound:  3.15320104e-01\n",
      "Epoch: 37099 mean train loss:  3.34727066e-03, bound:  3.15320045e-01\n",
      "Epoch: 37100 mean train loss:  3.34720523e-03, bound:  3.15320045e-01\n",
      "Epoch: 37101 mean train loss:  3.34717776e-03, bound:  3.15320045e-01\n",
      "Epoch: 37102 mean train loss:  3.34712467e-03, bound:  3.15320045e-01\n",
      "Epoch: 37103 mean train loss:  3.34705086e-03, bound:  3.15320045e-01\n",
      "Epoch: 37104 mean train loss:  3.34698404e-03, bound:  3.15320045e-01\n",
      "Epoch: 37105 mean train loss:  3.34689487e-03, bound:  3.15320015e-01\n",
      "Epoch: 37106 mean train loss:  3.34690069e-03, bound:  3.15320015e-01\n",
      "Epoch: 37107 mean train loss:  3.34682548e-03, bound:  3.15320015e-01\n",
      "Epoch: 37108 mean train loss:  3.34679871e-03, bound:  3.15320015e-01\n",
      "Epoch: 37109 mean train loss:  3.34668742e-03, bound:  3.15320015e-01\n",
      "Epoch: 37110 mean train loss:  3.34665505e-03, bound:  3.15320015e-01\n",
      "Epoch: 37111 mean train loss:  3.34659452e-03, bound:  3.15320015e-01\n",
      "Epoch: 37112 mean train loss:  3.34656658e-03, bound:  3.15320015e-01\n",
      "Epoch: 37113 mean train loss:  3.34651070e-03, bound:  3.15320015e-01\n",
      "Epoch: 37114 mean train loss:  3.34643992e-03, bound:  3.15319985e-01\n",
      "Epoch: 37115 mean train loss:  3.34638264e-03, bound:  3.15319985e-01\n",
      "Epoch: 37116 mean train loss:  3.34628741e-03, bound:  3.15319985e-01\n",
      "Epoch: 37117 mean train loss:  3.34625272e-03, bound:  3.15319985e-01\n",
      "Epoch: 37118 mean train loss:  3.34619684e-03, bound:  3.15319926e-01\n",
      "Epoch: 37119 mean train loss:  3.34611768e-03, bound:  3.15319926e-01\n",
      "Epoch: 37120 mean train loss:  3.34602571e-03, bound:  3.15319926e-01\n",
      "Epoch: 37121 mean train loss:  3.34601407e-03, bound:  3.15319926e-01\n",
      "Epoch: 37122 mean train loss:  3.34593398e-03, bound:  3.15319926e-01\n",
      "Epoch: 37123 mean train loss:  3.34586948e-03, bound:  3.15319926e-01\n",
      "Epoch: 37124 mean train loss:  3.34581058e-03, bound:  3.15319926e-01\n",
      "Epoch: 37125 mean train loss:  3.34570138e-03, bound:  3.15319896e-01\n",
      "Epoch: 37126 mean train loss:  3.34571581e-03, bound:  3.15319896e-01\n",
      "Epoch: 37127 mean train loss:  3.34565109e-03, bound:  3.15319896e-01\n",
      "Epoch: 37128 mean train loss:  3.34557262e-03, bound:  3.15319896e-01\n",
      "Epoch: 37129 mean train loss:  3.34555213e-03, bound:  3.15319896e-01\n",
      "Epoch: 37130 mean train loss:  3.34545644e-03, bound:  3.15319896e-01\n",
      "Epoch: 37131 mean train loss:  3.34544154e-03, bound:  3.15319896e-01\n",
      "Epoch: 37132 mean train loss:  3.34534817e-03, bound:  3.15319896e-01\n",
      "Epoch: 37133 mean train loss:  3.34533374e-03, bound:  3.15319866e-01\n",
      "Epoch: 37134 mean train loss:  3.34521919e-03, bound:  3.15319866e-01\n",
      "Epoch: 37135 mean train loss:  3.34515423e-03, bound:  3.15319866e-01\n",
      "Epoch: 37136 mean train loss:  3.34510952e-03, bound:  3.15319836e-01\n",
      "Epoch: 37137 mean train loss:  3.34508345e-03, bound:  3.15319836e-01\n",
      "Epoch: 37138 mean train loss:  3.34502989e-03, bound:  3.15319836e-01\n",
      "Epoch: 37139 mean train loss:  3.34494538e-03, bound:  3.15319806e-01\n",
      "Epoch: 37140 mean train loss:  3.34487623e-03, bound:  3.15319806e-01\n",
      "Epoch: 37141 mean train loss:  3.34480614e-03, bound:  3.15319806e-01\n",
      "Epoch: 37142 mean train loss:  3.34480428e-03, bound:  3.15319806e-01\n",
      "Epoch: 37143 mean train loss:  3.34473210e-03, bound:  3.15319806e-01\n",
      "Epoch: 37144 mean train loss:  3.34472116e-03, bound:  3.15319777e-01\n",
      "Epoch: 37145 mean train loss:  3.34463199e-03, bound:  3.15319777e-01\n",
      "Epoch: 37146 mean train loss:  3.34461709e-03, bound:  3.15319777e-01\n",
      "Epoch: 37147 mean train loss:  3.34456260e-03, bound:  3.15319777e-01\n",
      "Epoch: 37148 mean train loss:  3.34452186e-03, bound:  3.15319777e-01\n",
      "Epoch: 37149 mean train loss:  3.34444991e-03, bound:  3.15319777e-01\n",
      "Epoch: 37150 mean train loss:  3.34433396e-03, bound:  3.15319777e-01\n",
      "Epoch: 37151 mean train loss:  3.34424479e-03, bound:  3.15319717e-01\n",
      "Epoch: 37152 mean train loss:  3.34420404e-03, bound:  3.15319717e-01\n",
      "Epoch: 37153 mean train loss:  3.34411394e-03, bound:  3.15319717e-01\n",
      "Epoch: 37154 mean train loss:  3.34407040e-03, bound:  3.15319717e-01\n",
      "Epoch: 37155 mean train loss:  3.34403827e-03, bound:  3.15319717e-01\n",
      "Epoch: 37156 mean train loss:  3.34399333e-03, bound:  3.15319717e-01\n",
      "Epoch: 37157 mean train loss:  3.34390998e-03, bound:  3.15319717e-01\n",
      "Epoch: 37158 mean train loss:  3.34380148e-03, bound:  3.15319687e-01\n",
      "Epoch: 37159 mean train loss:  3.34380474e-03, bound:  3.15319687e-01\n",
      "Epoch: 37160 mean train loss:  3.34374490e-03, bound:  3.15319687e-01\n",
      "Epoch: 37161 mean train loss:  3.34368693e-03, bound:  3.15319687e-01\n",
      "Epoch: 37162 mean train loss:  3.34362988e-03, bound:  3.15319687e-01\n",
      "Epoch: 37163 mean train loss:  3.34358891e-03, bound:  3.15319687e-01\n",
      "Epoch: 37164 mean train loss:  3.34358192e-03, bound:  3.15319657e-01\n",
      "Epoch: 37165 mean train loss:  3.34349647e-03, bound:  3.15319657e-01\n",
      "Epoch: 37166 mean train loss:  3.34340404e-03, bound:  3.15319657e-01\n",
      "Epoch: 37167 mean train loss:  3.34337237e-03, bound:  3.15319657e-01\n",
      "Epoch: 37168 mean train loss:  3.34329833e-03, bound:  3.15319657e-01\n",
      "Epoch: 37169 mean train loss:  3.34321428e-03, bound:  3.15319657e-01\n",
      "Epoch: 37170 mean train loss:  3.34315561e-03, bound:  3.15319598e-01\n",
      "Epoch: 37171 mean train loss:  3.34310951e-03, bound:  3.15319598e-01\n",
      "Epoch: 37172 mean train loss:  3.34302359e-03, bound:  3.15319598e-01\n",
      "Epoch: 37173 mean train loss:  3.34292953e-03, bound:  3.15319598e-01\n",
      "Epoch: 37174 mean train loss:  3.34291114e-03, bound:  3.15319598e-01\n",
      "Epoch: 37175 mean train loss:  3.34283453e-03, bound:  3.15319598e-01\n",
      "Epoch: 37176 mean train loss:  3.34281265e-03, bound:  3.15319598e-01\n",
      "Epoch: 37177 mean train loss:  3.34273232e-03, bound:  3.15319598e-01\n",
      "Epoch: 37178 mean train loss:  3.34271486e-03, bound:  3.15319598e-01\n",
      "Epoch: 37179 mean train loss:  3.34260939e-03, bound:  3.15319568e-01\n",
      "Epoch: 37180 mean train loss:  3.34257050e-03, bound:  3.15319568e-01\n",
      "Epoch: 37181 mean train loss:  3.34247574e-03, bound:  3.15319568e-01\n",
      "Epoch: 37182 mean train loss:  3.34246783e-03, bound:  3.15319568e-01\n",
      "Epoch: 37183 mean train loss:  3.34238377e-03, bound:  3.15319568e-01\n",
      "Epoch: 37184 mean train loss:  3.34233977e-03, bound:  3.15319568e-01\n",
      "Epoch: 37185 mean train loss:  3.34227295e-03, bound:  3.15319568e-01\n",
      "Epoch: 37186 mean train loss:  3.34220473e-03, bound:  3.15319538e-01\n",
      "Epoch: 37187 mean train loss:  3.34215374e-03, bound:  3.15319479e-01\n",
      "Epoch: 37188 mean train loss:  3.34210182e-03, bound:  3.15319479e-01\n",
      "Epoch: 37189 mean train loss:  3.34206177e-03, bound:  3.15319479e-01\n",
      "Epoch: 37190 mean train loss:  3.34197143e-03, bound:  3.15319479e-01\n",
      "Epoch: 37191 mean train loss:  3.34195211e-03, bound:  3.15319479e-01\n",
      "Epoch: 37192 mean train loss:  3.34190624e-03, bound:  3.15319479e-01\n",
      "Epoch: 37193 mean train loss:  3.34178936e-03, bound:  3.15319479e-01\n",
      "Epoch: 37194 mean train loss:  3.34176747e-03, bound:  3.15319479e-01\n",
      "Epoch: 37195 mean train loss:  3.34171928e-03, bound:  3.15319479e-01\n",
      "Epoch: 37196 mean train loss:  3.34166642e-03, bound:  3.15319479e-01\n",
      "Epoch: 37197 mean train loss:  3.34156840e-03, bound:  3.15319449e-01\n",
      "Epoch: 37198 mean train loss:  3.34157422e-03, bound:  3.15319449e-01\n",
      "Epoch: 37199 mean train loss:  3.34144849e-03, bound:  3.15319449e-01\n",
      "Epoch: 37200 mean train loss:  3.34136910e-03, bound:  3.15319449e-01\n",
      "Epoch: 37201 mean train loss:  3.34135769e-03, bound:  3.15319449e-01\n",
      "Epoch: 37202 mean train loss:  3.34130484e-03, bound:  3.15319449e-01\n",
      "Epoch: 37203 mean train loss:  3.34124011e-03, bound:  3.15319449e-01\n",
      "Epoch: 37204 mean train loss:  3.34122032e-03, bound:  3.15319419e-01\n",
      "Epoch: 37205 mean train loss:  3.34112369e-03, bound:  3.15319419e-01\n",
      "Epoch: 37206 mean train loss:  3.34106362e-03, bound:  3.15319419e-01\n",
      "Epoch: 37207 mean train loss:  3.34100588e-03, bound:  3.15319419e-01\n",
      "Epoch: 37208 mean train loss:  3.34092882e-03, bound:  3.15319419e-01\n",
      "Epoch: 37209 mean train loss:  3.34090507e-03, bound:  3.15319419e-01\n",
      "Epoch: 37210 mean train loss:  3.34081915e-03, bound:  3.15319419e-01\n",
      "Epoch: 37211 mean train loss:  3.34073394e-03, bound:  3.15319359e-01\n",
      "Epoch: 37212 mean train loss:  3.34071135e-03, bound:  3.15319359e-01\n",
      "Epoch: 37213 mean train loss:  3.34065920e-03, bound:  3.15319359e-01\n",
      "Epoch: 37214 mean train loss:  3.34058260e-03, bound:  3.15319359e-01\n",
      "Epoch: 37215 mean train loss:  3.34057980e-03, bound:  3.15319359e-01\n",
      "Epoch: 37216 mean train loss:  3.34048760e-03, bound:  3.15319359e-01\n",
      "Epoch: 37217 mean train loss:  3.34045338e-03, bound:  3.15319330e-01\n",
      "Epoch: 37218 mean train loss:  3.34037212e-03, bound:  3.15319330e-01\n",
      "Epoch: 37219 mean train loss:  3.34031601e-03, bound:  3.15319330e-01\n",
      "Epoch: 37220 mean train loss:  3.34024965e-03, bound:  3.15319330e-01\n",
      "Epoch: 37221 mean train loss:  3.34018818e-03, bound:  3.15319330e-01\n",
      "Epoch: 37222 mean train loss:  3.34014581e-03, bound:  3.15319330e-01\n",
      "Epoch: 37223 mean train loss:  3.34004243e-03, bound:  3.15319300e-01\n",
      "Epoch: 37224 mean train loss:  3.34000657e-03, bound:  3.15319300e-01\n",
      "Epoch: 37225 mean train loss:  3.33988247e-03, bound:  3.15319300e-01\n",
      "Epoch: 37226 mean train loss:  3.33989877e-03, bound:  3.15319300e-01\n",
      "Epoch: 37227 mean train loss:  3.33981798e-03, bound:  3.15319300e-01\n",
      "Epoch: 37228 mean train loss:  3.33976932e-03, bound:  3.15319300e-01\n",
      "Epoch: 37229 mean train loss:  3.33966943e-03, bound:  3.15319240e-01\n",
      "Epoch: 37230 mean train loss:  3.33969691e-03, bound:  3.15319240e-01\n",
      "Epoch: 37231 mean train loss:  3.33961681e-03, bound:  3.15319240e-01\n",
      "Epoch: 37232 mean train loss:  3.33952950e-03, bound:  3.15319240e-01\n",
      "Epoch: 37233 mean train loss:  3.33950878e-03, bound:  3.15319240e-01\n",
      "Epoch: 37234 mean train loss:  3.33939213e-03, bound:  3.15319240e-01\n",
      "Epoch: 37235 mean train loss:  3.33938934e-03, bound:  3.15319240e-01\n",
      "Epoch: 37236 mean train loss:  3.33929434e-03, bound:  3.15319210e-01\n",
      "Epoch: 37237 mean train loss:  3.33924498e-03, bound:  3.15319210e-01\n",
      "Epoch: 37238 mean train loss:  3.33920633e-03, bound:  3.15319210e-01\n",
      "Epoch: 37239 mean train loss:  3.33908177e-03, bound:  3.15319210e-01\n",
      "Epoch: 37240 mean train loss:  3.33907851e-03, bound:  3.15319210e-01\n",
      "Epoch: 37241 mean train loss:  3.33900144e-03, bound:  3.15319210e-01\n",
      "Epoch: 37242 mean train loss:  3.33893625e-03, bound:  3.15319210e-01\n",
      "Epoch: 37243 mean train loss:  3.33891739e-03, bound:  3.15319210e-01\n",
      "Epoch: 37244 mean train loss:  3.33882682e-03, bound:  3.15319151e-01\n",
      "Epoch: 37245 mean train loss:  3.33873881e-03, bound:  3.15319151e-01\n",
      "Epoch: 37246 mean train loss:  3.33869550e-03, bound:  3.15319151e-01\n",
      "Epoch: 37247 mean train loss:  3.33863194e-03, bound:  3.15319151e-01\n",
      "Epoch: 37248 mean train loss:  3.33858375e-03, bound:  3.15319151e-01\n",
      "Epoch: 37249 mean train loss:  3.33856419e-03, bound:  3.15319121e-01\n",
      "Epoch: 37250 mean train loss:  3.33850179e-03, bound:  3.15319121e-01\n",
      "Epoch: 37251 mean train loss:  3.33842519e-03, bound:  3.15319121e-01\n",
      "Epoch: 37252 mean train loss:  3.33839329e-03, bound:  3.15319121e-01\n",
      "Epoch: 37253 mean train loss:  3.33830784e-03, bound:  3.15319121e-01\n",
      "Epoch: 37254 mean train loss:  3.33828968e-03, bound:  3.15319091e-01\n",
      "Epoch: 37255 mean train loss:  3.33818863e-03, bound:  3.15319091e-01\n",
      "Epoch: 37256 mean train loss:  3.33814416e-03, bound:  3.15319091e-01\n",
      "Epoch: 37257 mean train loss:  3.33808595e-03, bound:  3.15319091e-01\n",
      "Epoch: 37258 mean train loss:  3.33802844e-03, bound:  3.15319091e-01\n",
      "Epoch: 37259 mean train loss:  3.33798584e-03, bound:  3.15319091e-01\n",
      "Epoch: 37260 mean train loss:  3.33786756e-03, bound:  3.15319091e-01\n",
      "Epoch: 37261 mean train loss:  3.33786616e-03, bound:  3.15319091e-01\n",
      "Epoch: 37262 mean train loss:  3.33777722e-03, bound:  3.15319091e-01\n",
      "Epoch: 37263 mean train loss:  3.33773228e-03, bound:  3.15319031e-01\n",
      "Epoch: 37264 mean train loss:  3.33767920e-03, bound:  3.15319031e-01\n",
      "Epoch: 37265 mean train loss:  3.33759701e-03, bound:  3.15319031e-01\n",
      "Epoch: 37266 mean train loss:  3.33758024e-03, bound:  3.15319031e-01\n",
      "Epoch: 37267 mean train loss:  3.33751133e-03, bound:  3.15319031e-01\n",
      "Epoch: 37268 mean train loss:  3.33747757e-03, bound:  3.15319031e-01\n",
      "Epoch: 37269 mean train loss:  3.33738094e-03, bound:  3.15319002e-01\n",
      "Epoch: 37270 mean train loss:  3.33728478e-03, bound:  3.15319002e-01\n",
      "Epoch: 37271 mean train loss:  3.33727803e-03, bound:  3.15319002e-01\n",
      "Epoch: 37272 mean train loss:  3.33722844e-03, bound:  3.15319002e-01\n",
      "Epoch: 37273 mean train loss:  3.33718932e-03, bound:  3.15319002e-01\n",
      "Epoch: 37274 mean train loss:  3.33712786e-03, bound:  3.15318972e-01\n",
      "Epoch: 37275 mean train loss:  3.33710108e-03, bound:  3.15318972e-01\n",
      "Epoch: 37276 mean train loss:  3.33710248e-03, bound:  3.15318972e-01\n",
      "Epoch: 37277 mean train loss:  3.33707780e-03, bound:  3.15318972e-01\n",
      "Epoch: 37278 mean train loss:  3.33711365e-03, bound:  3.15318972e-01\n",
      "Epoch: 37279 mean train loss:  3.33709060e-03, bound:  3.15318972e-01\n",
      "Epoch: 37280 mean train loss:  3.33703123e-03, bound:  3.15318972e-01\n",
      "Epoch: 37281 mean train loss:  3.33688967e-03, bound:  3.15318912e-01\n",
      "Epoch: 37282 mean train loss:  3.33677465e-03, bound:  3.15318912e-01\n",
      "Epoch: 37283 mean train loss:  3.33662308e-03, bound:  3.15318912e-01\n",
      "Epoch: 37284 mean train loss:  3.33654182e-03, bound:  3.15318912e-01\n",
      "Epoch: 37285 mean train loss:  3.33645591e-03, bound:  3.15318912e-01\n",
      "Epoch: 37286 mean train loss:  3.33645078e-03, bound:  3.15318912e-01\n",
      "Epoch: 37287 mean train loss:  3.33644729e-03, bound:  3.15318882e-01\n",
      "Epoch: 37288 mean train loss:  3.33642913e-03, bound:  3.15318882e-01\n",
      "Epoch: 37289 mean train loss:  3.33631155e-03, bound:  3.15318882e-01\n",
      "Epoch: 37290 mean train loss:  3.33616883e-03, bound:  3.15318882e-01\n",
      "Epoch: 37291 mean train loss:  3.33610945e-03, bound:  3.15318882e-01\n",
      "Epoch: 37292 mean train loss:  3.33603052e-03, bound:  3.15318882e-01\n",
      "Epoch: 37293 mean train loss:  3.33604263e-03, bound:  3.15318882e-01\n",
      "Epoch: 37294 mean train loss:  3.33598000e-03, bound:  3.15318882e-01\n",
      "Epoch: 37295 mean train loss:  3.33592948e-03, bound:  3.15318882e-01\n",
      "Epoch: 37296 mean train loss:  3.33587662e-03, bound:  3.15318882e-01\n",
      "Epoch: 37297 mean train loss:  3.33575066e-03, bound:  3.15318882e-01\n",
      "Epoch: 37298 mean train loss:  3.33570177e-03, bound:  3.15318853e-01\n",
      "Epoch: 37299 mean train loss:  3.33568593e-03, bound:  3.15318793e-01\n",
      "Epoch: 37300 mean train loss:  3.33563285e-03, bound:  3.15318793e-01\n",
      "Epoch: 37301 mean train loss:  3.33560095e-03, bound:  3.15318793e-01\n",
      "Epoch: 37302 mean train loss:  3.33553436e-03, bound:  3.15318793e-01\n",
      "Epoch: 37303 mean train loss:  3.33543494e-03, bound:  3.15318793e-01\n",
      "Epoch: 37304 mean train loss:  3.33537185e-03, bound:  3.15318793e-01\n",
      "Epoch: 37305 mean train loss:  3.33533483e-03, bound:  3.15318793e-01\n",
      "Epoch: 37306 mean train loss:  3.33529199e-03, bound:  3.15318793e-01\n",
      "Epoch: 37307 mean train loss:  3.33520630e-03, bound:  3.15318793e-01\n",
      "Epoch: 37308 mean train loss:  3.33515904e-03, bound:  3.15318793e-01\n",
      "Epoch: 37309 mean train loss:  3.33510921e-03, bound:  3.15318793e-01\n",
      "Epoch: 37310 mean train loss:  3.33501678e-03, bound:  3.15318793e-01\n",
      "Epoch: 37311 mean train loss:  3.33501538e-03, bound:  3.15318763e-01\n",
      "Epoch: 37312 mean train loss:  3.33488942e-03, bound:  3.15318763e-01\n",
      "Epoch: 37313 mean train loss:  3.33485357e-03, bound:  3.15318763e-01\n",
      "Epoch: 37314 mean train loss:  3.33478348e-03, bound:  3.15318763e-01\n",
      "Epoch: 37315 mean train loss:  3.33476742e-03, bound:  3.15318763e-01\n",
      "Epoch: 37316 mean train loss:  3.33469710e-03, bound:  3.15318763e-01\n",
      "Epoch: 37317 mean train loss:  3.33460281e-03, bound:  3.15318704e-01\n",
      "Epoch: 37318 mean train loss:  3.33457720e-03, bound:  3.15318704e-01\n",
      "Epoch: 37319 mean train loss:  3.33447149e-03, bound:  3.15318704e-01\n",
      "Epoch: 37320 mean train loss:  3.33450013e-03, bound:  3.15318674e-01\n",
      "Epoch: 37321 mean train loss:  3.33442958e-03, bound:  3.15318674e-01\n",
      "Epoch: 37322 mean train loss:  3.33436485e-03, bound:  3.15318674e-01\n",
      "Epoch: 37323 mean train loss:  3.33430571e-03, bound:  3.15318674e-01\n",
      "Epoch: 37324 mean train loss:  3.33420606e-03, bound:  3.15318674e-01\n",
      "Epoch: 37325 mean train loss:  3.33422422e-03, bound:  3.15318674e-01\n",
      "Epoch: 37326 mean train loss:  3.33413645e-03, bound:  3.15318674e-01\n",
      "Epoch: 37327 mean train loss:  3.33404285e-03, bound:  3.15318674e-01\n",
      "Epoch: 37328 mean train loss:  3.33405915e-03, bound:  3.15318674e-01\n",
      "Epoch: 37329 mean train loss:  3.33400443e-03, bound:  3.15318674e-01\n",
      "Epoch: 37330 mean train loss:  3.33391270e-03, bound:  3.15318674e-01\n",
      "Epoch: 37331 mean train loss:  3.33383353e-03, bound:  3.15318644e-01\n",
      "Epoch: 37332 mean train loss:  3.33374180e-03, bound:  3.15318644e-01\n",
      "Epoch: 37333 mean train loss:  3.33372783e-03, bound:  3.15318644e-01\n",
      "Epoch: 37334 mean train loss:  3.33369686e-03, bound:  3.15318644e-01\n",
      "Epoch: 37335 mean train loss:  3.33360536e-03, bound:  3.15318584e-01\n",
      "Epoch: 37336 mean train loss:  3.33357393e-03, bound:  3.15318584e-01\n",
      "Epoch: 37337 mean train loss:  3.33353691e-03, bound:  3.15318584e-01\n",
      "Epoch: 37338 mean train loss:  3.33346357e-03, bound:  3.15318555e-01\n",
      "Epoch: 37339 mean train loss:  3.33342515e-03, bound:  3.15318555e-01\n",
      "Epoch: 37340 mean train loss:  3.33332131e-03, bound:  3.15318555e-01\n",
      "Epoch: 37341 mean train loss:  3.33327288e-03, bound:  3.15318555e-01\n",
      "Epoch: 37342 mean train loss:  3.33326822e-03, bound:  3.15318555e-01\n",
      "Epoch: 37343 mean train loss:  3.33316461e-03, bound:  3.15318555e-01\n",
      "Epoch: 37344 mean train loss:  3.33310640e-03, bound:  3.15318555e-01\n",
      "Epoch: 37345 mean train loss:  3.33303725e-03, bound:  3.15318555e-01\n",
      "Epoch: 37346 mean train loss:  3.33302049e-03, bound:  3.15318555e-01\n",
      "Epoch: 37347 mean train loss:  3.33295739e-03, bound:  3.15318555e-01\n",
      "Epoch: 37348 mean train loss:  3.33285006e-03, bound:  3.15318525e-01\n",
      "Epoch: 37349 mean train loss:  3.33282957e-03, bound:  3.15318525e-01\n",
      "Epoch: 37350 mean train loss:  3.33277206e-03, bound:  3.15318525e-01\n",
      "Epoch: 37351 mean train loss:  3.33272829e-03, bound:  3.15318525e-01\n",
      "Epoch: 37352 mean train loss:  3.33265564e-03, bound:  3.15318525e-01\n",
      "Epoch: 37353 mean train loss:  3.33263236e-03, bound:  3.15318525e-01\n",
      "Epoch: 37354 mean train loss:  3.33255529e-03, bound:  3.15318465e-01\n",
      "Epoch: 37355 mean train loss:  3.33249639e-03, bound:  3.15318465e-01\n",
      "Epoch: 37356 mean train loss:  3.33243841e-03, bound:  3.15318465e-01\n",
      "Epoch: 37357 mean train loss:  3.33238440e-03, bound:  3.15318465e-01\n",
      "Epoch: 37358 mean train loss:  3.33235716e-03, bound:  3.15318465e-01\n",
      "Epoch: 37359 mean train loss:  3.33228032e-03, bound:  3.15318465e-01\n",
      "Epoch: 37360 mean train loss:  3.33217951e-03, bound:  3.15318465e-01\n",
      "Epoch: 37361 mean train loss:  3.33215506e-03, bound:  3.15318465e-01\n",
      "Epoch: 37362 mean train loss:  3.33207916e-03, bound:  3.15318465e-01\n",
      "Epoch: 37363 mean train loss:  3.33202258e-03, bound:  3.15318465e-01\n",
      "Epoch: 37364 mean train loss:  3.33197555e-03, bound:  3.15318435e-01\n",
      "Epoch: 37365 mean train loss:  3.33189499e-03, bound:  3.15318435e-01\n",
      "Epoch: 37366 mean train loss:  3.33180907e-03, bound:  3.15318435e-01\n",
      "Epoch: 37367 mean train loss:  3.33177089e-03, bound:  3.15318435e-01\n",
      "Epoch: 37368 mean train loss:  3.33175808e-03, bound:  3.15318406e-01\n",
      "Epoch: 37369 mean train loss:  3.33173503e-03, bound:  3.15318406e-01\n",
      "Epoch: 37370 mean train loss:  3.33162467e-03, bound:  3.15318406e-01\n",
      "Epoch: 37371 mean train loss:  3.33163189e-03, bound:  3.15318406e-01\n",
      "Epoch: 37372 mean train loss:  3.33146472e-03, bound:  3.15318406e-01\n",
      "Epoch: 37373 mean train loss:  3.33143887e-03, bound:  3.15318406e-01\n",
      "Epoch: 37374 mean train loss:  3.33137810e-03, bound:  3.15318406e-01\n",
      "Epoch: 37375 mean train loss:  3.33135133e-03, bound:  3.15318406e-01\n",
      "Epoch: 37376 mean train loss:  3.33130592e-03, bound:  3.15318346e-01\n",
      "Epoch: 37377 mean train loss:  3.33123049e-03, bound:  3.15318406e-01\n",
      "Epoch: 37378 mean train loss:  3.33113992e-03, bound:  3.15318346e-01\n",
      "Epoch: 37379 mean train loss:  3.33113573e-03, bound:  3.15318346e-01\n",
      "Epoch: 37380 mean train loss:  3.33105447e-03, bound:  3.15318346e-01\n",
      "Epoch: 37381 mean train loss:  3.33099230e-03, bound:  3.15318346e-01\n",
      "Epoch: 37382 mean train loss:  3.33095249e-03, bound:  3.15318346e-01\n",
      "Epoch: 37383 mean train loss:  3.33090383e-03, bound:  3.15318316e-01\n",
      "Epoch: 37384 mean train loss:  3.33080534e-03, bound:  3.15318316e-01\n",
      "Epoch: 37385 mean train loss:  3.33079253e-03, bound:  3.15318316e-01\n",
      "Epoch: 37386 mean train loss:  3.33072385e-03, bound:  3.15318286e-01\n",
      "Epoch: 37387 mean train loss:  3.33064981e-03, bound:  3.15318286e-01\n",
      "Epoch: 37388 mean train loss:  3.33060534e-03, bound:  3.15318286e-01\n",
      "Epoch: 37389 mean train loss:  3.33053642e-03, bound:  3.15318286e-01\n",
      "Epoch: 37390 mean train loss:  3.33055365e-03, bound:  3.15318286e-01\n",
      "Epoch: 37391 mean train loss:  3.33046960e-03, bound:  3.15318286e-01\n",
      "Epoch: 37392 mean train loss:  3.33037484e-03, bound:  3.15318286e-01\n",
      "Epoch: 37393 mean train loss:  3.33035784e-03, bound:  3.15318286e-01\n",
      "Epoch: 37394 mean train loss:  3.33024655e-03, bound:  3.15318257e-01\n",
      "Epoch: 37395 mean train loss:  3.33019788e-03, bound:  3.15318257e-01\n",
      "Epoch: 37396 mean train loss:  3.33016459e-03, bound:  3.15318227e-01\n",
      "Epoch: 37397 mean train loss:  3.33009893e-03, bound:  3.15318227e-01\n",
      "Epoch: 37398 mean train loss:  3.33007448e-03, bound:  3.15318227e-01\n",
      "Epoch: 37399 mean train loss:  3.32999509e-03, bound:  3.15318227e-01\n",
      "Epoch: 37400 mean train loss:  3.32993106e-03, bound:  3.15318227e-01\n",
      "Epoch: 37401 mean train loss:  3.32983886e-03, bound:  3.15318197e-01\n",
      "Epoch: 37402 mean train loss:  3.32978880e-03, bound:  3.15318197e-01\n",
      "Epoch: 37403 mean train loss:  3.32974619e-03, bound:  3.15318197e-01\n",
      "Epoch: 37404 mean train loss:  3.32969031e-03, bound:  3.15318197e-01\n",
      "Epoch: 37405 mean train loss:  3.32960533e-03, bound:  3.15318197e-01\n",
      "Epoch: 37406 mean train loss:  3.32957064e-03, bound:  3.15318197e-01\n",
      "Epoch: 37407 mean train loss:  3.32954549e-03, bound:  3.15318197e-01\n",
      "Epoch: 37408 mean train loss:  3.32951406e-03, bound:  3.15318197e-01\n",
      "Epoch: 37409 mean train loss:  3.32944514e-03, bound:  3.15318197e-01\n",
      "Epoch: 37410 mean train loss:  3.32938368e-03, bound:  3.15318137e-01\n",
      "Epoch: 37411 mean train loss:  3.32931522e-03, bound:  3.15318137e-01\n",
      "Epoch: 37412 mean train loss:  3.32923210e-03, bound:  3.15318137e-01\n",
      "Epoch: 37413 mean train loss:  3.32916901e-03, bound:  3.15318108e-01\n",
      "Epoch: 37414 mean train loss:  3.32908682e-03, bound:  3.15318137e-01\n",
      "Epoch: 37415 mean train loss:  3.32906400e-03, bound:  3.15318108e-01\n",
      "Epoch: 37416 mean train loss:  3.32903676e-03, bound:  3.15318108e-01\n",
      "Epoch: 37417 mean train loss:  3.32894525e-03, bound:  3.15318108e-01\n",
      "Epoch: 37418 mean train loss:  3.32891312e-03, bound:  3.15318108e-01\n",
      "Epoch: 37419 mean train loss:  3.32888076e-03, bound:  3.15318108e-01\n",
      "Epoch: 37420 mean train loss:  3.32880602e-03, bound:  3.15318108e-01\n",
      "Epoch: 37421 mean train loss:  3.32875084e-03, bound:  3.15318078e-01\n",
      "Epoch: 37422 mean train loss:  3.32866795e-03, bound:  3.15318078e-01\n",
      "Epoch: 37423 mean train loss:  3.32861743e-03, bound:  3.15318078e-01\n",
      "Epoch: 37424 mean train loss:  3.32854595e-03, bound:  3.15318078e-01\n",
      "Epoch: 37425 mean train loss:  3.32846795e-03, bound:  3.15318078e-01\n",
      "Epoch: 37426 mean train loss:  3.32846167e-03, bound:  3.15318078e-01\n",
      "Epoch: 37427 mean train loss:  3.32841882e-03, bound:  3.15318078e-01\n",
      "Epoch: 37428 mean train loss:  3.32831941e-03, bound:  3.15318078e-01\n",
      "Epoch: 37429 mean train loss:  3.32828425e-03, bound:  3.15318078e-01\n",
      "Epoch: 37430 mean train loss:  3.32819438e-03, bound:  3.15318018e-01\n",
      "Epoch: 37431 mean train loss:  3.32814734e-03, bound:  3.15318018e-01\n",
      "Epoch: 37432 mean train loss:  3.32811917e-03, bound:  3.15318018e-01\n",
      "Epoch: 37433 mean train loss:  3.32802883e-03, bound:  3.15318018e-01\n",
      "Epoch: 37434 mean train loss:  3.32796527e-03, bound:  3.15317988e-01\n",
      "Epoch: 37435 mean train loss:  3.32794897e-03, bound:  3.15317988e-01\n",
      "Epoch: 37436 mean train loss:  3.32785049e-03, bound:  3.15317988e-01\n",
      "Epoch: 37437 mean train loss:  3.32781696e-03, bound:  3.15317988e-01\n",
      "Epoch: 37438 mean train loss:  3.32769379e-03, bound:  3.15317988e-01\n",
      "Epoch: 37439 mean train loss:  3.32770962e-03, bound:  3.15317988e-01\n",
      "Epoch: 37440 mean train loss:  3.32764606e-03, bound:  3.15317988e-01\n",
      "Epoch: 37441 mean train loss:  3.32759786e-03, bound:  3.15317988e-01\n",
      "Epoch: 37442 mean train loss:  3.32751474e-03, bound:  3.15317959e-01\n",
      "Epoch: 37443 mean train loss:  3.32748098e-03, bound:  3.15317959e-01\n",
      "Epoch: 37444 mean train loss:  3.32741719e-03, bound:  3.15317959e-01\n",
      "Epoch: 37445 mean train loss:  3.32734338e-03, bound:  3.15317959e-01\n",
      "Epoch: 37446 mean train loss:  3.32734291e-03, bound:  3.15317959e-01\n",
      "Epoch: 37447 mean train loss:  3.32726282e-03, bound:  3.15317959e-01\n",
      "Epoch: 37448 mean train loss:  3.32719134e-03, bound:  3.15317899e-01\n",
      "Epoch: 37449 mean train loss:  3.32715991e-03, bound:  3.15317899e-01\n",
      "Epoch: 37450 mean train loss:  3.32713779e-03, bound:  3.15317899e-01\n",
      "Epoch: 37451 mean train loss:  3.32702254e-03, bound:  3.15317899e-01\n",
      "Epoch: 37452 mean train loss:  3.32697714e-03, bound:  3.15317899e-01\n",
      "Epoch: 37453 mean train loss:  3.32692615e-03, bound:  3.15317899e-01\n",
      "Epoch: 37454 mean train loss:  3.32685281e-03, bound:  3.15317899e-01\n",
      "Epoch: 37455 mean train loss:  3.32681485e-03, bound:  3.15317899e-01\n",
      "Epoch: 37456 mean train loss:  3.32681159e-03, bound:  3.15317869e-01\n",
      "Epoch: 37457 mean train loss:  3.32673918e-03, bound:  3.15317869e-01\n",
      "Epoch: 37458 mean train loss:  3.32664815e-03, bound:  3.15317869e-01\n",
      "Epoch: 37459 mean train loss:  3.32658249e-03, bound:  3.15317869e-01\n",
      "Epoch: 37460 mean train loss:  3.32646933e-03, bound:  3.15317869e-01\n",
      "Epoch: 37461 mean train loss:  3.32644931e-03, bound:  3.15317839e-01\n",
      "Epoch: 37462 mean train loss:  3.32635059e-03, bound:  3.15317839e-01\n",
      "Epoch: 37463 mean train loss:  3.32633220e-03, bound:  3.15317839e-01\n",
      "Epoch: 37464 mean train loss:  3.32625606e-03, bound:  3.15317839e-01\n",
      "Epoch: 37465 mean train loss:  3.32622789e-03, bound:  3.15317839e-01\n",
      "Epoch: 37466 mean train loss:  3.32612917e-03, bound:  3.15317839e-01\n",
      "Epoch: 37467 mean train loss:  3.32613965e-03, bound:  3.15317780e-01\n",
      "Epoch: 37468 mean train loss:  3.32605746e-03, bound:  3.15317780e-01\n",
      "Epoch: 37469 mean train loss:  3.32601345e-03, bound:  3.15317780e-01\n",
      "Epoch: 37470 mean train loss:  3.32592474e-03, bound:  3.15317780e-01\n",
      "Epoch: 37471 mean train loss:  3.32588959e-03, bound:  3.15317780e-01\n",
      "Epoch: 37472 mean train loss:  3.32580251e-03, bound:  3.15317780e-01\n",
      "Epoch: 37473 mean train loss:  3.32576479e-03, bound:  3.15317780e-01\n",
      "Epoch: 37474 mean train loss:  3.32567887e-03, bound:  3.15317780e-01\n",
      "Epoch: 37475 mean train loss:  3.32564395e-03, bound:  3.15317780e-01\n",
      "Epoch: 37476 mean train loss:  3.32559575e-03, bound:  3.15317750e-01\n",
      "Epoch: 37477 mean train loss:  3.32554942e-03, bound:  3.15317750e-01\n",
      "Epoch: 37478 mean train loss:  3.32551450e-03, bound:  3.15317750e-01\n",
      "Epoch: 37479 mean train loss:  3.32542835e-03, bound:  3.15317720e-01\n",
      "Epoch: 37480 mean train loss:  3.32536269e-03, bound:  3.15317720e-01\n",
      "Epoch: 37481 mean train loss:  3.32531054e-03, bound:  3.15317720e-01\n",
      "Epoch: 37482 mean train loss:  3.32523300e-03, bound:  3.15317720e-01\n",
      "Epoch: 37483 mean train loss:  3.32519272e-03, bound:  3.15317690e-01\n",
      "Epoch: 37484 mean train loss:  3.32509750e-03, bound:  3.15317690e-01\n",
      "Epoch: 37485 mean train loss:  3.32505023e-03, bound:  3.15317690e-01\n",
      "Epoch: 37486 mean train loss:  3.32504814e-03, bound:  3.15317661e-01\n",
      "Epoch: 37487 mean train loss:  3.32495058e-03, bound:  3.15317661e-01\n",
      "Epoch: 37488 mean train loss:  3.32489237e-03, bound:  3.15317661e-01\n",
      "Epoch: 37489 mean train loss:  3.32482834e-03, bound:  3.15317661e-01\n",
      "Epoch: 37490 mean train loss:  3.32476851e-03, bound:  3.15317661e-01\n",
      "Epoch: 37491 mean train loss:  3.32474220e-03, bound:  3.15317661e-01\n",
      "Epoch: 37492 mean train loss:  3.32466792e-03, bound:  3.15317661e-01\n",
      "Epoch: 37493 mean train loss:  3.32461298e-03, bound:  3.15317661e-01\n",
      "Epoch: 37494 mean train loss:  3.32457223e-03, bound:  3.15317661e-01\n",
      "Epoch: 37495 mean train loss:  3.32447072e-03, bound:  3.15317661e-01\n",
      "Epoch: 37496 mean train loss:  3.32448306e-03, bound:  3.15317631e-01\n",
      "Epoch: 37497 mean train loss:  3.32437968e-03, bound:  3.15317631e-01\n",
      "Epoch: 37498 mean train loss:  3.32434825e-03, bound:  3.15317601e-01\n",
      "Epoch: 37499 mean train loss:  3.32425511e-03, bound:  3.15317601e-01\n",
      "Epoch: 37500 mean train loss:  3.32425465e-03, bound:  3.15317601e-01\n",
      "Epoch: 37501 mean train loss:  3.32412869e-03, bound:  3.15317601e-01\n",
      "Epoch: 37502 mean train loss:  3.32408771e-03, bound:  3.15317571e-01\n",
      "Epoch: 37503 mean train loss:  3.32404254e-03, bound:  3.15317571e-01\n",
      "Epoch: 37504 mean train loss:  3.32399854e-03, bound:  3.15317571e-01\n",
      "Epoch: 37505 mean train loss:  3.32392240e-03, bound:  3.15317571e-01\n",
      "Epoch: 37506 mean train loss:  3.32386908e-03, bound:  3.15317571e-01\n",
      "Epoch: 37507 mean train loss:  3.32377432e-03, bound:  3.15317571e-01\n",
      "Epoch: 37508 mean train loss:  3.32375406e-03, bound:  3.15317541e-01\n",
      "Epoch: 37509 mean train loss:  3.32370400e-03, bound:  3.15317541e-01\n",
      "Epoch: 37510 mean train loss:  3.32364370e-03, bound:  3.15317541e-01\n",
      "Epoch: 37511 mean train loss:  3.32359551e-03, bound:  3.15317541e-01\n",
      "Epoch: 37512 mean train loss:  3.32353986e-03, bound:  3.15317541e-01\n",
      "Epoch: 37513 mean train loss:  3.32345022e-03, bound:  3.15317541e-01\n",
      "Epoch: 37514 mean train loss:  3.32342624e-03, bound:  3.15317541e-01\n",
      "Epoch: 37515 mean train loss:  3.32339481e-03, bound:  3.15317512e-01\n",
      "Epoch: 37516 mean train loss:  3.32326652e-03, bound:  3.15317512e-01\n",
      "Epoch: 37517 mean train loss:  3.32325837e-03, bound:  3.15317512e-01\n",
      "Epoch: 37518 mean train loss:  3.32317012e-03, bound:  3.15317512e-01\n",
      "Epoch: 37519 mean train loss:  3.32309701e-03, bound:  3.15317512e-01\n",
      "Epoch: 37520 mean train loss:  3.32310586e-03, bound:  3.15317512e-01\n",
      "Epoch: 37521 mean train loss:  3.32300761e-03, bound:  3.15317452e-01\n",
      "Epoch: 37522 mean train loss:  3.32297338e-03, bound:  3.15317452e-01\n",
      "Epoch: 37523 mean train loss:  3.32289259e-03, bound:  3.15317452e-01\n",
      "Epoch: 37524 mean train loss:  3.32283764e-03, bound:  3.15317452e-01\n",
      "Epoch: 37525 mean train loss:  3.32285976e-03, bound:  3.15317452e-01\n",
      "Epoch: 37526 mean train loss:  3.32273007e-03, bound:  3.15317452e-01\n",
      "Epoch: 37527 mean train loss:  3.32267303e-03, bound:  3.15317452e-01\n",
      "Epoch: 37528 mean train loss:  3.32259061e-03, bound:  3.15317422e-01\n",
      "Epoch: 37529 mean train loss:  3.32254870e-03, bound:  3.15317422e-01\n",
      "Epoch: 37530 mean train loss:  3.32249515e-03, bound:  3.15317422e-01\n",
      "Epoch: 37531 mean train loss:  3.32244346e-03, bound:  3.15317422e-01\n",
      "Epoch: 37532 mean train loss:  3.32236802e-03, bound:  3.15317422e-01\n",
      "Epoch: 37533 mean train loss:  3.32230795e-03, bound:  3.15317422e-01\n",
      "Epoch: 37534 mean train loss:  3.32228048e-03, bound:  3.15317392e-01\n",
      "Epoch: 37535 mean train loss:  3.32219736e-03, bound:  3.15317392e-01\n",
      "Epoch: 37536 mean train loss:  3.32213729e-03, bound:  3.15317392e-01\n",
      "Epoch: 37537 mean train loss:  3.32211051e-03, bound:  3.15317392e-01\n",
      "Epoch: 37538 mean train loss:  3.32203461e-03, bound:  3.15317392e-01\n",
      "Epoch: 37539 mean train loss:  3.32201808e-03, bound:  3.15317392e-01\n",
      "Epoch: 37540 mean train loss:  3.32194078e-03, bound:  3.15317392e-01\n",
      "Epoch: 37541 mean train loss:  3.32188304e-03, bound:  3.15317392e-01\n",
      "Epoch: 37542 mean train loss:  3.32183740e-03, bound:  3.15317333e-01\n",
      "Epoch: 37543 mean train loss:  3.32180993e-03, bound:  3.15317333e-01\n",
      "Epoch: 37544 mean train loss:  3.32176359e-03, bound:  3.15317333e-01\n",
      "Epoch: 37545 mean train loss:  3.32171563e-03, bound:  3.15317333e-01\n",
      "Epoch: 37546 mean train loss:  3.32164648e-03, bound:  3.15317303e-01\n",
      "Epoch: 37547 mean train loss:  3.32158967e-03, bound:  3.15317303e-01\n",
      "Epoch: 37548 mean train loss:  3.32150795e-03, bound:  3.15317303e-01\n",
      "Epoch: 37549 mean train loss:  3.32150212e-03, bound:  3.15317303e-01\n",
      "Epoch: 37550 mean train loss:  3.32140853e-03, bound:  3.15317303e-01\n",
      "Epoch: 37551 mean train loss:  3.32134822e-03, bound:  3.15317273e-01\n",
      "Epoch: 37552 mean train loss:  3.32127092e-03, bound:  3.15317273e-01\n",
      "Epoch: 37553 mean train loss:  3.32118012e-03, bound:  3.15317273e-01\n",
      "Epoch: 37554 mean train loss:  3.32114310e-03, bound:  3.15317273e-01\n",
      "Epoch: 37555 mean train loss:  3.32106976e-03, bound:  3.15317273e-01\n",
      "Epoch: 37556 mean train loss:  3.32103204e-03, bound:  3.15317273e-01\n",
      "Epoch: 37557 mean train loss:  3.32100829e-03, bound:  3.15317273e-01\n",
      "Epoch: 37558 mean train loss:  3.32088862e-03, bound:  3.15317243e-01\n",
      "Epoch: 37559 mean train loss:  3.32090142e-03, bound:  3.15317243e-01\n",
      "Epoch: 37560 mean train loss:  3.32081714e-03, bound:  3.15317243e-01\n",
      "Epoch: 37561 mean train loss:  3.32073751e-03, bound:  3.15317214e-01\n",
      "Epoch: 37562 mean train loss:  3.32071120e-03, bound:  3.15317214e-01\n",
      "Epoch: 37563 mean train loss:  3.32061993e-03, bound:  3.15317214e-01\n",
      "Epoch: 37564 mean train loss:  3.32053564e-03, bound:  3.15317214e-01\n",
      "Epoch: 37565 mean train loss:  3.32054752e-03, bound:  3.15317214e-01\n",
      "Epoch: 37566 mean train loss:  3.32042645e-03, bound:  3.15317184e-01\n",
      "Epoch: 37567 mean train loss:  3.32037802e-03, bound:  3.15317184e-01\n",
      "Epoch: 37568 mean train loss:  3.32029141e-03, bound:  3.15317184e-01\n",
      "Epoch: 37569 mean train loss:  3.32024856e-03, bound:  3.15317184e-01\n",
      "Epoch: 37570 mean train loss:  3.32018384e-03, bound:  3.15317184e-01\n",
      "Epoch: 37571 mean train loss:  3.32017452e-03, bound:  3.15317154e-01\n",
      "Epoch: 37572 mean train loss:  3.32008209e-03, bound:  3.15317154e-01\n",
      "Epoch: 37573 mean train loss:  3.32002039e-03, bound:  3.15317154e-01\n",
      "Epoch: 37574 mean train loss:  3.32001667e-03, bound:  3.15317154e-01\n",
      "Epoch: 37575 mean train loss:  3.31995822e-03, bound:  3.15317154e-01\n",
      "Epoch: 37576 mean train loss:  3.31987324e-03, bound:  3.15317154e-01\n",
      "Epoch: 37577 mean train loss:  3.31983436e-03, bound:  3.15317154e-01\n",
      "Epoch: 37578 mean train loss:  3.31979571e-03, bound:  3.15317124e-01\n",
      "Epoch: 37579 mean train loss:  3.31976358e-03, bound:  3.15317124e-01\n",
      "Epoch: 37580 mean train loss:  3.31970444e-03, bound:  3.15317094e-01\n",
      "Epoch: 37581 mean train loss:  3.31964204e-03, bound:  3.15317094e-01\n",
      "Epoch: 37582 mean train loss:  3.31961317e-03, bound:  3.15317094e-01\n",
      "Epoch: 37583 mean train loss:  3.31949489e-03, bound:  3.15317094e-01\n",
      "Epoch: 37584 mean train loss:  3.31947557e-03, bound:  3.15317094e-01\n",
      "Epoch: 37585 mean train loss:  3.31936940e-03, bound:  3.15317094e-01\n",
      "Epoch: 37586 mean train loss:  3.31929722e-03, bound:  3.15317065e-01\n",
      "Epoch: 37587 mean train loss:  3.31925973e-03, bound:  3.15317065e-01\n",
      "Epoch: 37588 mean train loss:  3.31917568e-03, bound:  3.15317065e-01\n",
      "Epoch: 37589 mean train loss:  3.31916311e-03, bound:  3.15317065e-01\n",
      "Epoch: 37590 mean train loss:  3.31910094e-03, bound:  3.15317065e-01\n",
      "Epoch: 37591 mean train loss:  3.31903738e-03, bound:  3.15317065e-01\n",
      "Epoch: 37592 mean train loss:  3.31894984e-03, bound:  3.15317065e-01\n",
      "Epoch: 37593 mean train loss:  3.31890560e-03, bound:  3.15317035e-01\n",
      "Epoch: 37594 mean train loss:  3.31883784e-03, bound:  3.15317035e-01\n",
      "Epoch: 37595 mean train loss:  3.31880338e-03, bound:  3.15317035e-01\n",
      "Epoch: 37596 mean train loss:  3.31870606e-03, bound:  3.15317035e-01\n",
      "Epoch: 37597 mean train loss:  3.31865950e-03, bound:  3.15317005e-01\n",
      "Epoch: 37598 mean train loss:  3.31865228e-03, bound:  3.15317005e-01\n",
      "Epoch: 37599 mean train loss:  3.31858546e-03, bound:  3.15316975e-01\n",
      "Epoch: 37600 mean train loss:  3.31848185e-03, bound:  3.15316975e-01\n",
      "Epoch: 37601 mean train loss:  3.31849349e-03, bound:  3.15316975e-01\n",
      "Epoch: 37602 mean train loss:  3.31841130e-03, bound:  3.15316975e-01\n",
      "Epoch: 37603 mean train loss:  3.31834797e-03, bound:  3.15316975e-01\n",
      "Epoch: 37604 mean train loss:  3.31829675e-03, bound:  3.15316975e-01\n",
      "Epoch: 37605 mean train loss:  3.31819919e-03, bound:  3.15316975e-01\n",
      "Epoch: 37606 mean train loss:  3.31813190e-03, bound:  3.15316975e-01\n",
      "Epoch: 37607 mean train loss:  3.31810699e-03, bound:  3.15316975e-01\n",
      "Epoch: 37608 mean train loss:  3.31805251e-03, bound:  3.15316975e-01\n",
      "Epoch: 37609 mean train loss:  3.31802177e-03, bound:  3.15316945e-01\n",
      "Epoch: 37610 mean train loss:  3.31794750e-03, bound:  3.15316945e-01\n",
      "Epoch: 37611 mean train loss:  3.31793400e-03, bound:  3.15316916e-01\n",
      "Epoch: 37612 mean train loss:  3.31787299e-03, bound:  3.15316916e-01\n",
      "Epoch: 37613 mean train loss:  3.31781060e-03, bound:  3.15316916e-01\n",
      "Epoch: 37614 mean train loss:  3.31777101e-03, bound:  3.15316916e-01\n",
      "Epoch: 37615 mean train loss:  3.31770605e-03, bound:  3.15316916e-01\n",
      "Epoch: 37616 mean train loss:  3.31758801e-03, bound:  3.15316886e-01\n",
      "Epoch: 37617 mean train loss:  3.31752468e-03, bound:  3.15316886e-01\n",
      "Epoch: 37618 mean train loss:  3.31748743e-03, bound:  3.15316856e-01\n",
      "Epoch: 37619 mean train loss:  3.31742386e-03, bound:  3.15316856e-01\n",
      "Epoch: 37620 mean train loss:  3.31737334e-03, bound:  3.15316856e-01\n",
      "Epoch: 37621 mean train loss:  3.31732049e-03, bound:  3.15316856e-01\n",
      "Epoch: 37622 mean train loss:  3.31724179e-03, bound:  3.15316856e-01\n",
      "Epoch: 37623 mean train loss:  3.31723387e-03, bound:  3.15316856e-01\n",
      "Epoch: 37624 mean train loss:  3.31713096e-03, bound:  3.15316856e-01\n",
      "Epoch: 37625 mean train loss:  3.31705855e-03, bound:  3.15316856e-01\n",
      "Epoch: 37626 mean train loss:  3.31702596e-03, bound:  3.15316856e-01\n",
      "Epoch: 37627 mean train loss:  3.31695471e-03, bound:  3.15316856e-01\n",
      "Epoch: 37628 mean train loss:  3.31694074e-03, bound:  3.15316826e-01\n",
      "Epoch: 37629 mean train loss:  3.31682782e-03, bound:  3.15316796e-01\n",
      "Epoch: 37630 mean train loss:  3.31678614e-03, bound:  3.15316796e-01\n",
      "Epoch: 37631 mean train loss:  3.31671978e-03, bound:  3.15316796e-01\n",
      "Epoch: 37632 mean train loss:  3.31666833e-03, bound:  3.15316796e-01\n",
      "Epoch: 37633 mean train loss:  3.31661594e-03, bound:  3.15316767e-01\n",
      "Epoch: 37634 mean train loss:  3.31656192e-03, bound:  3.15316767e-01\n",
      "Epoch: 37635 mean train loss:  3.31646763e-03, bound:  3.15316767e-01\n",
      "Epoch: 37636 mean train loss:  3.31648602e-03, bound:  3.15316767e-01\n",
      "Epoch: 37637 mean train loss:  3.31639941e-03, bound:  3.15316737e-01\n",
      "Epoch: 37638 mean train loss:  3.31632677e-03, bound:  3.15316737e-01\n",
      "Epoch: 37639 mean train loss:  3.31628928e-03, bound:  3.15316737e-01\n",
      "Epoch: 37640 mean train loss:  3.31621966e-03, bound:  3.15316737e-01\n",
      "Epoch: 37641 mean train loss:  3.31614446e-03, bound:  3.15316737e-01\n",
      "Epoch: 37642 mean train loss:  3.31617822e-03, bound:  3.15316737e-01\n",
      "Epoch: 37643 mean train loss:  3.31606972e-03, bound:  3.15316737e-01\n",
      "Epoch: 37644 mean train loss:  3.31601547e-03, bound:  3.15316737e-01\n",
      "Epoch: 37645 mean train loss:  3.31595098e-03, bound:  3.15316737e-01\n",
      "Epoch: 37646 mean train loss:  3.31586692e-03, bound:  3.15316737e-01\n",
      "Epoch: 37647 mean train loss:  3.31584155e-03, bound:  3.15316707e-01\n",
      "Epoch: 37648 mean train loss:  3.31573677e-03, bound:  3.15316707e-01\n",
      "Epoch: 37649 mean train loss:  3.31572653e-03, bound:  3.15316677e-01\n",
      "Epoch: 37650 mean train loss:  3.31568159e-03, bound:  3.15316677e-01\n",
      "Epoch: 37651 mean train loss:  3.31562129e-03, bound:  3.15316677e-01\n",
      "Epoch: 37652 mean train loss:  3.31552164e-03, bound:  3.15316677e-01\n",
      "Epoch: 37653 mean train loss:  3.31548275e-03, bound:  3.15316647e-01\n",
      "Epoch: 37654 mean train loss:  3.31545505e-03, bound:  3.15316647e-01\n",
      "Epoch: 37655 mean train loss:  3.31533700e-03, bound:  3.15316647e-01\n",
      "Epoch: 37656 mean train loss:  3.31531558e-03, bound:  3.15316647e-01\n",
      "Epoch: 37657 mean train loss:  3.31525039e-03, bound:  3.15316647e-01\n",
      "Epoch: 37658 mean train loss:  3.31518613e-03, bound:  3.15316647e-01\n",
      "Epoch: 37659 mean train loss:  3.31512466e-03, bound:  3.15316617e-01\n",
      "Epoch: 37660 mean train loss:  3.31507111e-03, bound:  3.15316617e-01\n",
      "Epoch: 37661 mean train loss:  3.31505761e-03, bound:  3.15316617e-01\n",
      "Epoch: 37662 mean train loss:  3.31494282e-03, bound:  3.15316617e-01\n",
      "Epoch: 37663 mean train loss:  3.31491255e-03, bound:  3.15316617e-01\n",
      "Epoch: 37664 mean train loss:  3.31482640e-03, bound:  3.15316617e-01\n",
      "Epoch: 37665 mean train loss:  3.31481034e-03, bound:  3.15316588e-01\n",
      "Epoch: 37666 mean train loss:  3.31477122e-03, bound:  3.15316558e-01\n",
      "Epoch: 37667 mean train loss:  3.31469462e-03, bound:  3.15316558e-01\n",
      "Epoch: 37668 mean train loss:  3.31463874e-03, bound:  3.15316588e-01\n",
      "Epoch: 37669 mean train loss:  3.31457099e-03, bound:  3.15316558e-01\n",
      "Epoch: 37670 mean train loss:  3.31450207e-03, bound:  3.15316558e-01\n",
      "Epoch: 37671 mean train loss:  3.31448764e-03, bound:  3.15316558e-01\n",
      "Epoch: 37672 mean train loss:  3.31439381e-03, bound:  3.15316558e-01\n",
      "Epoch: 37673 mean train loss:  3.31434445e-03, bound:  3.15316528e-01\n",
      "Epoch: 37674 mean train loss:  3.31427809e-03, bound:  3.15316528e-01\n",
      "Epoch: 37675 mean train loss:  3.31420498e-03, bound:  3.15316528e-01\n",
      "Epoch: 37676 mean train loss:  3.31415422e-03, bound:  3.15316528e-01\n",
      "Epoch: 37677 mean train loss:  3.31414863e-03, bound:  3.15316528e-01\n",
      "Epoch: 37678 mean train loss:  3.31406947e-03, bound:  3.15316528e-01\n",
      "Epoch: 37679 mean train loss:  3.31399054e-03, bound:  3.15316498e-01\n",
      "Epoch: 37680 mean train loss:  3.31399566e-03, bound:  3.15316498e-01\n",
      "Epoch: 37681 mean train loss:  3.31401569e-03, bound:  3.15316498e-01\n",
      "Epoch: 37682 mean train loss:  3.31396307e-03, bound:  3.15316498e-01\n",
      "Epoch: 37683 mean train loss:  3.31392395e-03, bound:  3.15316468e-01\n",
      "Epoch: 37684 mean train loss:  3.31385923e-03, bound:  3.15316498e-01\n",
      "Epoch: 37685 mean train loss:  3.31378053e-03, bound:  3.15316468e-01\n",
      "Epoch: 37686 mean train loss:  3.31370975e-03, bound:  3.15316468e-01\n",
      "Epoch: 37687 mean train loss:  3.31360218e-03, bound:  3.15316468e-01\n",
      "Epoch: 37688 mean train loss:  3.31352232e-03, bound:  3.15316468e-01\n",
      "Epoch: 37689 mean train loss:  3.31341359e-03, bound:  3.15316468e-01\n",
      "Epoch: 37690 mean train loss:  3.31337447e-03, bound:  3.15316439e-01\n",
      "Epoch: 37691 mean train loss:  3.31338006e-03, bound:  3.15316409e-01\n",
      "Epoch: 37692 mean train loss:  3.31330858e-03, bound:  3.15316409e-01\n",
      "Epoch: 37693 mean train loss:  3.31330649e-03, bound:  3.15316409e-01\n",
      "Epoch: 37694 mean train loss:  3.31323175e-03, bound:  3.15316409e-01\n",
      "Epoch: 37695 mean train loss:  3.31318914e-03, bound:  3.15316409e-01\n",
      "Epoch: 37696 mean train loss:  3.31310532e-03, bound:  3.15316409e-01\n",
      "Epoch: 37697 mean train loss:  3.31295701e-03, bound:  3.15316409e-01\n",
      "Epoch: 37698 mean train loss:  3.31297098e-03, bound:  3.15316379e-01\n",
      "Epoch: 37699 mean train loss:  3.31292022e-03, bound:  3.15316379e-01\n",
      "Epoch: 37700 mean train loss:  3.31280893e-03, bound:  3.15316379e-01\n",
      "Epoch: 37701 mean train loss:  3.31275491e-03, bound:  3.15316379e-01\n",
      "Epoch: 37702 mean train loss:  3.31269274e-03, bound:  3.15316379e-01\n",
      "Epoch: 37703 mean train loss:  3.31263850e-03, bound:  3.15316379e-01\n",
      "Epoch: 37704 mean train loss:  3.31257260e-03, bound:  3.15316349e-01\n",
      "Epoch: 37705 mean train loss:  3.31252161e-03, bound:  3.15316349e-01\n",
      "Epoch: 37706 mean train loss:  3.31248227e-03, bound:  3.15316349e-01\n",
      "Epoch: 37707 mean train loss:  3.31240031e-03, bound:  3.15316349e-01\n",
      "Epoch: 37708 mean train loss:  3.31237866e-03, bound:  3.15316349e-01\n",
      "Epoch: 37709 mean train loss:  3.31227272e-03, bound:  3.15316319e-01\n",
      "Epoch: 37710 mean train loss:  3.31229833e-03, bound:  3.15316290e-01\n",
      "Epoch: 37711 mean train loss:  3.31222569e-03, bound:  3.15316319e-01\n",
      "Epoch: 37712 mean train loss:  3.31215723e-03, bound:  3.15316290e-01\n",
      "Epoch: 37713 mean train loss:  3.31211858e-03, bound:  3.15316290e-01\n",
      "Epoch: 37714 mean train loss:  3.31203337e-03, bound:  3.15316290e-01\n",
      "Epoch: 37715 mean train loss:  3.31194396e-03, bound:  3.15316290e-01\n",
      "Epoch: 37716 mean train loss:  3.31193977e-03, bound:  3.15316290e-01\n",
      "Epoch: 37717 mean train loss:  3.31185409e-03, bound:  3.15316290e-01\n",
      "Epoch: 37718 mean train loss:  3.31181451e-03, bound:  3.15316260e-01\n",
      "Epoch: 37719 mean train loss:  3.31177190e-03, bound:  3.15316260e-01\n",
      "Epoch: 37720 mean train loss:  3.31169926e-03, bound:  3.15316260e-01\n",
      "Epoch: 37721 mean train loss:  3.31164990e-03, bound:  3.15316260e-01\n",
      "Epoch: 37722 mean train loss:  3.31157539e-03, bound:  3.15316260e-01\n",
      "Epoch: 37723 mean train loss:  3.31149832e-03, bound:  3.15316260e-01\n",
      "Epoch: 37724 mean train loss:  3.31148715e-03, bound:  3.15316230e-01\n",
      "Epoch: 37725 mean train loss:  3.31140426e-03, bound:  3.15316230e-01\n",
      "Epoch: 37726 mean train loss:  3.31135839e-03, bound:  3.15316230e-01\n",
      "Epoch: 37727 mean train loss:  3.31135350e-03, bound:  3.15316230e-01\n",
      "Epoch: 37728 mean train loss:  3.31126060e-03, bound:  3.15316170e-01\n",
      "Epoch: 37729 mean train loss:  3.31117562e-03, bound:  3.15316200e-01\n",
      "Epoch: 37730 mean train loss:  3.31113162e-03, bound:  3.15316200e-01\n",
      "Epoch: 37731 mean train loss:  3.31110321e-03, bound:  3.15316170e-01\n",
      "Epoch: 37732 mean train loss:  3.31102521e-03, bound:  3.15316170e-01\n",
      "Epoch: 37733 mean train loss:  3.31095001e-03, bound:  3.15316170e-01\n",
      "Epoch: 37734 mean train loss:  3.31088318e-03, bound:  3.15316170e-01\n",
      "Epoch: 37735 mean train loss:  3.31086968e-03, bound:  3.15316170e-01\n",
      "Epoch: 37736 mean train loss:  3.31081520e-03, bound:  3.15316170e-01\n",
      "Epoch: 37737 mean train loss:  3.31070344e-03, bound:  3.15316170e-01\n",
      "Epoch: 37738 mean train loss:  3.31067573e-03, bound:  3.15316170e-01\n",
      "Epoch: 37739 mean train loss:  3.31059191e-03, bound:  3.15316170e-01\n",
      "Epoch: 37740 mean train loss:  3.31058865e-03, bound:  3.15316170e-01\n",
      "Epoch: 37741 mean train loss:  3.31050297e-03, bound:  3.15316141e-01\n",
      "Epoch: 37742 mean train loss:  3.31050390e-03, bound:  3.15316141e-01\n",
      "Epoch: 37743 mean train loss:  3.31036909e-03, bound:  3.15316111e-01\n",
      "Epoch: 37744 mean train loss:  3.31030576e-03, bound:  3.15316111e-01\n",
      "Epoch: 37745 mean train loss:  3.31026269e-03, bound:  3.15316111e-01\n",
      "Epoch: 37746 mean train loss:  3.31017654e-03, bound:  3.15316111e-01\n",
      "Epoch: 37747 mean train loss:  3.31013696e-03, bound:  3.15316111e-01\n",
      "Epoch: 37748 mean train loss:  3.31010786e-03, bound:  3.15316111e-01\n",
      "Epoch: 37749 mean train loss:  3.31006409e-03, bound:  3.15316081e-01\n",
      "Epoch: 37750 mean train loss:  3.31000285e-03, bound:  3.15316081e-01\n",
      "Epoch: 37751 mean train loss:  3.30991042e-03, bound:  3.15316051e-01\n",
      "Epoch: 37752 mean train loss:  3.30990367e-03, bound:  3.15316051e-01\n",
      "Epoch: 37753 mean train loss:  3.30989878e-03, bound:  3.15316051e-01\n",
      "Epoch: 37754 mean train loss:  3.30976862e-03, bound:  3.15316051e-01\n",
      "Epoch: 37755 mean train loss:  3.30967712e-03, bound:  3.15316051e-01\n",
      "Epoch: 37756 mean train loss:  3.30961635e-03, bound:  3.15316051e-01\n",
      "Epoch: 37757 mean train loss:  3.30963405e-03, bound:  3.15316051e-01\n",
      "Epoch: 37758 mean train loss:  3.30954790e-03, bound:  3.15316051e-01\n",
      "Epoch: 37759 mean train loss:  3.30948853e-03, bound:  3.15316051e-01\n",
      "Epoch: 37760 mean train loss:  3.30938213e-03, bound:  3.15316051e-01\n",
      "Epoch: 37761 mean train loss:  3.30936327e-03, bound:  3.15316021e-01\n",
      "Epoch: 37762 mean train loss:  3.30932462e-03, bound:  3.15316021e-01\n",
      "Epoch: 37763 mean train loss:  3.30926199e-03, bound:  3.15315992e-01\n",
      "Epoch: 37764 mean train loss:  3.30919912e-03, bound:  3.15315992e-01\n",
      "Epoch: 37765 mean train loss:  3.30909411e-03, bound:  3.15315992e-01\n",
      "Epoch: 37766 mean train loss:  3.30912694e-03, bound:  3.15315992e-01\n",
      "Epoch: 37767 mean train loss:  3.30904615e-03, bound:  3.15315962e-01\n",
      "Epoch: 37768 mean train loss:  3.30896396e-03, bound:  3.15315962e-01\n",
      "Epoch: 37769 mean train loss:  3.30890645e-03, bound:  3.15315962e-01\n",
      "Epoch: 37770 mean train loss:  3.30891204e-03, bound:  3.15315932e-01\n",
      "Epoch: 37771 mean train loss:  3.30882706e-03, bound:  3.15315932e-01\n",
      "Epoch: 37772 mean train loss:  3.30880051e-03, bound:  3.15315932e-01\n",
      "Epoch: 37773 mean train loss:  3.30873113e-03, bound:  3.15315932e-01\n",
      "Epoch: 37774 mean train loss:  3.30869318e-03, bound:  3.15315932e-01\n",
      "Epoch: 37775 mean train loss:  3.30860796e-03, bound:  3.15315932e-01\n",
      "Epoch: 37776 mean train loss:  3.30855232e-03, bound:  3.15315932e-01\n",
      "Epoch: 37777 mean train loss:  3.30843055e-03, bound:  3.15315932e-01\n",
      "Epoch: 37778 mean train loss:  3.30841960e-03, bound:  3.15315932e-01\n",
      "Epoch: 37779 mean train loss:  3.30837560e-03, bound:  3.15315932e-01\n",
      "Epoch: 37780 mean train loss:  3.30826547e-03, bound:  3.15315872e-01\n",
      "Epoch: 37781 mean train loss:  3.30826011e-03, bound:  3.15315872e-01\n",
      "Epoch: 37782 mean train loss:  3.30819353e-03, bound:  3.15315872e-01\n",
      "Epoch: 37783 mean train loss:  3.30811483e-03, bound:  3.15315872e-01\n",
      "Epoch: 37784 mean train loss:  3.30805872e-03, bound:  3.15315872e-01\n",
      "Epoch: 37785 mean train loss:  3.30803776e-03, bound:  3.15315872e-01\n",
      "Epoch: 37786 mean train loss:  3.30793322e-03, bound:  3.15315843e-01\n",
      "Epoch: 37787 mean train loss:  3.30789597e-03, bound:  3.15315843e-01\n",
      "Epoch: 37788 mean train loss:  3.30778980e-03, bound:  3.15315843e-01\n",
      "Epoch: 37789 mean train loss:  3.30779026e-03, bound:  3.15315843e-01\n",
      "Epoch: 37790 mean train loss:  3.30772251e-03, bound:  3.15315843e-01\n",
      "Epoch: 37791 mean train loss:  3.30769923e-03, bound:  3.15315843e-01\n",
      "Epoch: 37792 mean train loss:  3.30766966e-03, bound:  3.15315843e-01\n",
      "Epoch: 37793 mean train loss:  3.30758095e-03, bound:  3.15315843e-01\n",
      "Epoch: 37794 mean train loss:  3.30757117e-03, bound:  3.15315813e-01\n",
      "Epoch: 37795 mean train loss:  3.30750272e-03, bound:  3.15315813e-01\n",
      "Epoch: 37796 mean train loss:  3.30745894e-03, bound:  3.15315813e-01\n",
      "Epoch: 37797 mean train loss:  3.30745289e-03, bound:  3.15315813e-01\n",
      "Epoch: 37798 mean train loss:  3.30738677e-03, bound:  3.15315813e-01\n",
      "Epoch: 37799 mean train loss:  3.30731226e-03, bound:  3.15315813e-01\n",
      "Epoch: 37800 mean train loss:  3.30719468e-03, bound:  3.15315753e-01\n",
      "Epoch: 37801 mean train loss:  3.30706802e-03, bound:  3.15315753e-01\n",
      "Epoch: 37802 mean train loss:  3.30703473e-03, bound:  3.15315753e-01\n",
      "Epoch: 37803 mean train loss:  3.30700609e-03, bound:  3.15315753e-01\n",
      "Epoch: 37804 mean train loss:  3.30693414e-03, bound:  3.15315753e-01\n",
      "Epoch: 37805 mean train loss:  3.30689643e-03, bound:  3.15315753e-01\n",
      "Epoch: 37806 mean train loss:  3.30681261e-03, bound:  3.15315723e-01\n",
      "Epoch: 37807 mean train loss:  3.30681354e-03, bound:  3.15315723e-01\n",
      "Epoch: 37808 mean train loss:  3.30673507e-03, bound:  3.15315723e-01\n",
      "Epoch: 37809 mean train loss:  3.30671319e-03, bound:  3.15315723e-01\n",
      "Epoch: 37810 mean train loss:  3.30659794e-03, bound:  3.15315723e-01\n",
      "Epoch: 37811 mean train loss:  3.30652832e-03, bound:  3.15315723e-01\n",
      "Epoch: 37812 mean train loss:  3.30651621e-03, bound:  3.15315723e-01\n",
      "Epoch: 37813 mean train loss:  3.30642518e-03, bound:  3.15315694e-01\n",
      "Epoch: 37814 mean train loss:  3.30635323e-03, bound:  3.15315694e-01\n",
      "Epoch: 37815 mean train loss:  3.30629689e-03, bound:  3.15315694e-01\n",
      "Epoch: 37816 mean train loss:  3.30626220e-03, bound:  3.15315694e-01\n",
      "Epoch: 37817 mean train loss:  3.30627104e-03, bound:  3.15315694e-01\n",
      "Epoch: 37818 mean train loss:  3.30611994e-03, bound:  3.15315694e-01\n",
      "Epoch: 37819 mean train loss:  3.30614392e-03, bound:  3.15315634e-01\n",
      "Epoch: 37820 mean train loss:  3.30604147e-03, bound:  3.15315634e-01\n",
      "Epoch: 37821 mean train loss:  3.30606452e-03, bound:  3.15315634e-01\n",
      "Epoch: 37822 mean train loss:  3.30593972e-03, bound:  3.15315634e-01\n",
      "Epoch: 37823 mean train loss:  3.30588501e-03, bound:  3.15315634e-01\n",
      "Epoch: 37824 mean train loss:  3.30587639e-03, bound:  3.15315634e-01\n",
      "Epoch: 37825 mean train loss:  3.30580352e-03, bound:  3.15315604e-01\n",
      "Epoch: 37826 mean train loss:  3.30567965e-03, bound:  3.15315604e-01\n",
      "Epoch: 37827 mean train loss:  3.30565963e-03, bound:  3.15315604e-01\n",
      "Epoch: 37828 mean train loss:  3.30561004e-03, bound:  3.15315604e-01\n",
      "Epoch: 37829 mean train loss:  3.30559304e-03, bound:  3.15315604e-01\n",
      "Epoch: 37830 mean train loss:  3.30544799e-03, bound:  3.15315604e-01\n",
      "Epoch: 37831 mean train loss:  3.30542191e-03, bound:  3.15315604e-01\n",
      "Epoch: 37832 mean train loss:  3.30539513e-03, bound:  3.15315574e-01\n",
      "Epoch: 37833 mean train loss:  3.30534321e-03, bound:  3.15315574e-01\n",
      "Epoch: 37834 mean train loss:  3.30525916e-03, bound:  3.15315574e-01\n",
      "Epoch: 37835 mean train loss:  3.30518698e-03, bound:  3.15315574e-01\n",
      "Epoch: 37836 mean train loss:  3.30512552e-03, bound:  3.15315545e-01\n",
      "Epoch: 37837 mean train loss:  3.30510992e-03, bound:  3.15315545e-01\n",
      "Epoch: 37838 mean train loss:  3.30499955e-03, bound:  3.15315545e-01\n",
      "Epoch: 37839 mean train loss:  3.30496859e-03, bound:  3.15315545e-01\n",
      "Epoch: 37840 mean train loss:  3.30490642e-03, bound:  3.15315545e-01\n",
      "Epoch: 37841 mean train loss:  3.30487941e-03, bound:  3.15315545e-01\n",
      "Epoch: 37842 mean train loss:  3.30480095e-03, bound:  3.15315545e-01\n",
      "Epoch: 37843 mean train loss:  3.30474903e-03, bound:  3.15315485e-01\n",
      "Epoch: 37844 mean train loss:  3.30472086e-03, bound:  3.15315515e-01\n",
      "Epoch: 37845 mean train loss:  3.30462796e-03, bound:  3.15315485e-01\n",
      "Epoch: 37846 mean train loss:  3.30457487e-03, bound:  3.15315485e-01\n",
      "Epoch: 37847 mean train loss:  3.30451201e-03, bound:  3.15315485e-01\n",
      "Epoch: 37848 mean train loss:  3.30446661e-03, bound:  3.15315485e-01\n",
      "Epoch: 37849 mean train loss:  3.30445240e-03, bound:  3.15315485e-01\n",
      "Epoch: 37850 mean train loss:  3.30436253e-03, bound:  3.15315485e-01\n",
      "Epoch: 37851 mean train loss:  3.30436416e-03, bound:  3.15315485e-01\n",
      "Epoch: 37852 mean train loss:  3.30426591e-03, bound:  3.15315455e-01\n",
      "Epoch: 37853 mean train loss:  3.30416788e-03, bound:  3.15315455e-01\n",
      "Epoch: 37854 mean train loss:  3.30408243e-03, bound:  3.15315455e-01\n",
      "Epoch: 37855 mean train loss:  3.30408127e-03, bound:  3.15315455e-01\n",
      "Epoch: 37856 mean train loss:  3.30401096e-03, bound:  3.15315455e-01\n",
      "Epoch: 37857 mean train loss:  3.30393668e-03, bound:  3.15315425e-01\n",
      "Epoch: 37858 mean train loss:  3.30393203e-03, bound:  3.15315425e-01\n",
      "Epoch: 37859 mean train loss:  3.30388499e-03, bound:  3.15315425e-01\n",
      "Epoch: 37860 mean train loss:  3.30378348e-03, bound:  3.15315425e-01\n",
      "Epoch: 37861 mean train loss:  3.30369570e-03, bound:  3.15315425e-01\n",
      "Epoch: 37862 mean train loss:  3.30371363e-03, bound:  3.15315425e-01\n",
      "Epoch: 37863 mean train loss:  3.30358651e-03, bound:  3.15315366e-01\n",
      "Epoch: 37864 mean train loss:  3.30354390e-03, bound:  3.15315366e-01\n",
      "Epoch: 37865 mean train loss:  3.30352667e-03, bound:  3.15315366e-01\n",
      "Epoch: 37866 mean train loss:  3.30342539e-03, bound:  3.15315366e-01\n",
      "Epoch: 37867 mean train loss:  3.30341002e-03, bound:  3.15315366e-01\n",
      "Epoch: 37868 mean train loss:  3.30334343e-03, bound:  3.15315366e-01\n",
      "Epoch: 37869 mean train loss:  3.30328057e-03, bound:  3.15315366e-01\n",
      "Epoch: 37870 mean train loss:  3.30322073e-03, bound:  3.15315306e-01\n",
      "Epoch: 37871 mean train loss:  3.30317067e-03, bound:  3.15315306e-01\n",
      "Epoch: 37872 mean train loss:  3.30313668e-03, bound:  3.15315306e-01\n",
      "Epoch: 37873 mean train loss:  3.30304983e-03, bound:  3.15315306e-01\n",
      "Epoch: 37874 mean train loss:  3.30303493e-03, bound:  3.15315306e-01\n",
      "Epoch: 37875 mean train loss:  3.30293318e-03, bound:  3.15315306e-01\n",
      "Epoch: 37876 mean train loss:  3.30286357e-03, bound:  3.15315306e-01\n",
      "Epoch: 37877 mean train loss:  3.30284098e-03, bound:  3.15315306e-01\n",
      "Epoch: 37878 mean train loss:  3.30280559e-03, bound:  3.15315306e-01\n",
      "Epoch: 37879 mean train loss:  3.30274506e-03, bound:  3.15315306e-01\n",
      "Epoch: 37880 mean train loss:  3.30267008e-03, bound:  3.15315306e-01\n",
      "Epoch: 37881 mean train loss:  3.30262375e-03, bound:  3.15315306e-01\n",
      "Epoch: 37882 mean train loss:  3.30256345e-03, bound:  3.15315247e-01\n",
      "Epoch: 37883 mean train loss:  3.30250920e-03, bound:  3.15315247e-01\n",
      "Epoch: 37884 mean train loss:  3.30248522e-03, bound:  3.15315247e-01\n",
      "Epoch: 37885 mean train loss:  3.30239022e-03, bound:  3.15315247e-01\n",
      "Epoch: 37886 mean train loss:  3.30231478e-03, bound:  3.15315247e-01\n",
      "Epoch: 37887 mean train loss:  3.30225518e-03, bound:  3.15315247e-01\n",
      "Epoch: 37888 mean train loss:  3.30222887e-03, bound:  3.15315247e-01\n",
      "Epoch: 37889 mean train loss:  3.30217904e-03, bound:  3.15315247e-01\n",
      "Epoch: 37890 mean train loss:  3.30211292e-03, bound:  3.15315247e-01\n",
      "Epoch: 37891 mean train loss:  3.30203958e-03, bound:  3.15315247e-01\n",
      "Epoch: 37892 mean train loss:  3.30197858e-03, bound:  3.15315247e-01\n",
      "Epoch: 37893 mean train loss:  3.30190058e-03, bound:  3.15315187e-01\n",
      "Epoch: 37894 mean train loss:  3.30186775e-03, bound:  3.15315187e-01\n",
      "Epoch: 37895 mean train loss:  3.30180535e-03, bound:  3.15315187e-01\n",
      "Epoch: 37896 mean train loss:  3.30172502e-03, bound:  3.15315187e-01\n",
      "Epoch: 37897 mean train loss:  3.30171431e-03, bound:  3.15315187e-01\n",
      "Epoch: 37898 mean train loss:  3.30167077e-03, bound:  3.15315187e-01\n",
      "Epoch: 37899 mean train loss:  3.30160349e-03, bound:  3.15315187e-01\n",
      "Epoch: 37900 mean train loss:  3.30153573e-03, bound:  3.15315187e-01\n",
      "Epoch: 37901 mean train loss:  3.30144237e-03, bound:  3.15315157e-01\n",
      "Epoch: 37902 mean train loss:  3.30143352e-03, bound:  3.15315157e-01\n",
      "Epoch: 37903 mean train loss:  3.30134016e-03, bound:  3.15315157e-01\n",
      "Epoch: 37904 mean train loss:  3.30134947e-03, bound:  3.15315157e-01\n",
      "Epoch: 37905 mean train loss:  3.30124097e-03, bound:  3.15315157e-01\n",
      "Epoch: 37906 mean train loss:  3.30119347e-03, bound:  3.15315157e-01\n",
      "Epoch: 37907 mean train loss:  3.30116996e-03, bound:  3.15315157e-01\n",
      "Epoch: 37908 mean train loss:  3.30106914e-03, bound:  3.15315127e-01\n",
      "Epoch: 37909 mean train loss:  3.30105075e-03, bound:  3.15315127e-01\n",
      "Epoch: 37910 mean train loss:  3.30103189e-03, bound:  3.15315127e-01\n",
      "Epoch: 37911 mean train loss:  3.30097158e-03, bound:  3.15315127e-01\n",
      "Epoch: 37912 mean train loss:  3.30091850e-03, bound:  3.15315127e-01\n",
      "Epoch: 37913 mean train loss:  3.30084027e-03, bound:  3.15315068e-01\n",
      "Epoch: 37914 mean train loss:  3.30079999e-03, bound:  3.15315068e-01\n",
      "Epoch: 37915 mean train loss:  3.30077903e-03, bound:  3.15315068e-01\n",
      "Epoch: 37916 mean train loss:  3.30068520e-03, bound:  3.15315068e-01\n",
      "Epoch: 37917 mean train loss:  3.30060115e-03, bound:  3.15315068e-01\n",
      "Epoch: 37918 mean train loss:  3.30054224e-03, bound:  3.15315068e-01\n",
      "Epoch: 37919 mean train loss:  3.30049335e-03, bound:  3.15315038e-01\n",
      "Epoch: 37920 mean train loss:  3.30041070e-03, bound:  3.15315038e-01\n",
      "Epoch: 37921 mean train loss:  3.30037531e-03, bound:  3.15315038e-01\n",
      "Epoch: 37922 mean train loss:  3.30027030e-03, bound:  3.15315038e-01\n",
      "Epoch: 37923 mean train loss:  3.30030848e-03, bound:  3.15315038e-01\n",
      "Epoch: 37924 mean train loss:  3.30023607e-03, bound:  3.15315038e-01\n",
      "Epoch: 37925 mean train loss:  3.30016902e-03, bound:  3.15315038e-01\n",
      "Epoch: 37926 mean train loss:  3.30012548e-03, bound:  3.15315008e-01\n",
      "Epoch: 37927 mean train loss:  3.30009754e-03, bound:  3.15315008e-01\n",
      "Epoch: 37928 mean train loss:  3.30003002e-03, bound:  3.15315008e-01\n",
      "Epoch: 37929 mean train loss:  3.29994271e-03, bound:  3.15315008e-01\n",
      "Epoch: 37930 mean train loss:  3.29987775e-03, bound:  3.15315008e-01\n",
      "Epoch: 37931 mean train loss:  3.29982839e-03, bound:  3.15315008e-01\n",
      "Epoch: 37932 mean train loss:  3.29975272e-03, bound:  3.15314949e-01\n",
      "Epoch: 37933 mean train loss:  3.29967123e-03, bound:  3.15314949e-01\n",
      "Epoch: 37934 mean train loss:  3.29962792e-03, bound:  3.15314949e-01\n",
      "Epoch: 37935 mean train loss:  3.29958904e-03, bound:  3.15314949e-01\n",
      "Epoch: 37936 mean train loss:  3.29955458e-03, bound:  3.15314949e-01\n",
      "Epoch: 37937 mean train loss:  3.29945982e-03, bound:  3.15314949e-01\n",
      "Epoch: 37938 mean train loss:  3.29944072e-03, bound:  3.15314949e-01\n",
      "Epoch: 37939 mean train loss:  3.29938252e-03, bound:  3.15314919e-01\n",
      "Epoch: 37940 mean train loss:  3.29927402e-03, bound:  3.15314919e-01\n",
      "Epoch: 37941 mean train loss:  3.29922186e-03, bound:  3.15314919e-01\n",
      "Epoch: 37942 mean train loss:  3.29913967e-03, bound:  3.15314919e-01\n",
      "Epoch: 37943 mean train loss:  3.29917925e-03, bound:  3.15314919e-01\n",
      "Epoch: 37944 mean train loss:  3.29907471e-03, bound:  3.15314919e-01\n",
      "Epoch: 37945 mean train loss:  3.29904002e-03, bound:  3.15314919e-01\n",
      "Epoch: 37946 mean train loss:  3.29896249e-03, bound:  3.15314889e-01\n",
      "Epoch: 37947 mean train loss:  3.29885678e-03, bound:  3.15314889e-01\n",
      "Epoch: 37948 mean train loss:  3.29884957e-03, bound:  3.15314889e-01\n",
      "Epoch: 37949 mean train loss:  3.29879392e-03, bound:  3.15314889e-01\n",
      "Epoch: 37950 mean train loss:  3.29874200e-03, bound:  3.15314859e-01\n",
      "Epoch: 37951 mean train loss:  3.29869683e-03, bound:  3.15314889e-01\n",
      "Epoch: 37952 mean train loss:  3.29864514e-03, bound:  3.15314829e-01\n",
      "Epoch: 37953 mean train loss:  3.29853781e-03, bound:  3.15314829e-01\n",
      "Epoch: 37954 mean train loss:  3.29849939e-03, bound:  3.15314829e-01\n",
      "Epoch: 37955 mean train loss:  3.29845864e-03, bound:  3.15314829e-01\n",
      "Epoch: 37956 mean train loss:  3.29839555e-03, bound:  3.15314829e-01\n",
      "Epoch: 37957 mean train loss:  3.29831569e-03, bound:  3.15314829e-01\n",
      "Epoch: 37958 mean train loss:  3.29829729e-03, bound:  3.15314800e-01\n",
      "Epoch: 37959 mean train loss:  3.29822232e-03, bound:  3.15314800e-01\n",
      "Epoch: 37960 mean train loss:  3.29813757e-03, bound:  3.15314800e-01\n",
      "Epoch: 37961 mean train loss:  3.29810404e-03, bound:  3.15314800e-01\n",
      "Epoch: 37962 mean train loss:  3.29806912e-03, bound:  3.15314800e-01\n",
      "Epoch: 37963 mean train loss:  3.29799205e-03, bound:  3.15314800e-01\n",
      "Epoch: 37964 mean train loss:  3.29794083e-03, bound:  3.15314800e-01\n",
      "Epoch: 37965 mean train loss:  3.29787773e-03, bound:  3.15314740e-01\n",
      "Epoch: 37966 mean train loss:  3.29784374e-03, bound:  3.15314740e-01\n",
      "Epoch: 37967 mean train loss:  3.29778646e-03, bound:  3.15314740e-01\n",
      "Epoch: 37968 mean train loss:  3.29772034e-03, bound:  3.15314740e-01\n",
      "Epoch: 37969 mean train loss:  3.29770008e-03, bound:  3.15314740e-01\n",
      "Epoch: 37970 mean train loss:  3.29763233e-03, bound:  3.15314740e-01\n",
      "Epoch: 37971 mean train loss:  3.29757552e-03, bound:  3.15314740e-01\n",
      "Epoch: 37972 mean train loss:  3.29752383e-03, bound:  3.15314740e-01\n",
      "Epoch: 37973 mean train loss:  3.29746190e-03, bound:  3.15314740e-01\n",
      "Epoch: 37974 mean train loss:  3.29737877e-03, bound:  3.15314740e-01\n",
      "Epoch: 37975 mean train loss:  3.29732616e-03, bound:  3.15314740e-01\n",
      "Epoch: 37976 mean train loss:  3.29729146e-03, bound:  3.15314710e-01\n",
      "Epoch: 37977 mean train loss:  3.29722255e-03, bound:  3.15314710e-01\n",
      "Epoch: 37978 mean train loss:  3.29718436e-03, bound:  3.15314680e-01\n",
      "Epoch: 37979 mean train loss:  3.29707889e-03, bound:  3.15314680e-01\n",
      "Epoch: 37980 mean train loss:  3.29706981e-03, bound:  3.15314680e-01\n",
      "Epoch: 37981 mean train loss:  3.29699228e-03, bound:  3.15314680e-01\n",
      "Epoch: 37982 mean train loss:  3.29693384e-03, bound:  3.15314680e-01\n",
      "Epoch: 37983 mean train loss:  3.29689123e-03, bound:  3.15314680e-01\n",
      "Epoch: 37984 mean train loss:  3.29679414e-03, bound:  3.15314621e-01\n",
      "Epoch: 37985 mean train loss:  3.29677761e-03, bound:  3.15314621e-01\n",
      "Epoch: 37986 mean train loss:  3.29667446e-03, bound:  3.15314621e-01\n",
      "Epoch: 37987 mean train loss:  3.29667400e-03, bound:  3.15314621e-01\n",
      "Epoch: 37988 mean train loss:  3.29665514e-03, bound:  3.15314621e-01\n",
      "Epoch: 37989 mean train loss:  3.29655851e-03, bound:  3.15314621e-01\n",
      "Epoch: 37990 mean train loss:  3.29652568e-03, bound:  3.15314621e-01\n",
      "Epoch: 37991 mean train loss:  3.29643837e-03, bound:  3.15314621e-01\n",
      "Epoch: 37992 mean train loss:  3.29639413e-03, bound:  3.15314621e-01\n",
      "Epoch: 37993 mean train loss:  3.29627749e-03, bound:  3.15314621e-01\n",
      "Epoch: 37994 mean train loss:  3.29628191e-03, bound:  3.15314621e-01\n",
      "Epoch: 37995 mean train loss:  3.29618482e-03, bound:  3.15314621e-01\n",
      "Epoch: 37996 mean train loss:  3.29613709e-03, bound:  3.15314591e-01\n",
      "Epoch: 37997 mean train loss:  3.29612172e-03, bound:  3.15314591e-01\n",
      "Epoch: 37998 mean train loss:  3.29604745e-03, bound:  3.15314591e-01\n",
      "Epoch: 37999 mean train loss:  3.29604140e-03, bound:  3.15314591e-01\n",
      "Epoch: 38000 mean train loss:  3.29593685e-03, bound:  3.15314591e-01\n",
      "Epoch: 38001 mean train loss:  3.29587120e-03, bound:  3.15314561e-01\n",
      "Epoch: 38002 mean train loss:  3.29578342e-03, bound:  3.15314502e-01\n",
      "Epoch: 38003 mean train loss:  3.29574570e-03, bound:  3.15314502e-01\n",
      "Epoch: 38004 mean train loss:  3.29573918e-03, bound:  3.15314502e-01\n",
      "Epoch: 38005 mean train loss:  3.29567096e-03, bound:  3.15314502e-01\n",
      "Epoch: 38006 mean train loss:  3.29558155e-03, bound:  3.15314502e-01\n",
      "Epoch: 38007 mean train loss:  3.29555175e-03, bound:  3.15314502e-01\n",
      "Epoch: 38008 mean train loss:  3.29553126e-03, bound:  3.15314502e-01\n",
      "Epoch: 38009 mean train loss:  3.29545303e-03, bound:  3.15314502e-01\n",
      "Epoch: 38010 mean train loss:  3.29542346e-03, bound:  3.15314502e-01\n",
      "Epoch: 38011 mean train loss:  3.29535152e-03, bound:  3.15314502e-01\n",
      "Epoch: 38012 mean train loss:  3.29523627e-03, bound:  3.15314502e-01\n",
      "Epoch: 38013 mean train loss:  3.29523673e-03, bound:  3.15314502e-01\n",
      "Epoch: 38014 mean train loss:  3.29514430e-03, bound:  3.15314502e-01\n",
      "Epoch: 38015 mean train loss:  3.29514267e-03, bound:  3.15314472e-01\n",
      "Epoch: 38016 mean train loss:  3.29502416e-03, bound:  3.15314472e-01\n",
      "Epoch: 38017 mean train loss:  3.29499319e-03, bound:  3.15314472e-01\n",
      "Epoch: 38018 mean train loss:  3.29489633e-03, bound:  3.15314472e-01\n",
      "Epoch: 38019 mean train loss:  3.29484628e-03, bound:  3.15314472e-01\n",
      "Epoch: 38020 mean train loss:  3.29478970e-03, bound:  3.15314472e-01\n",
      "Epoch: 38021 mean train loss:  3.29477550e-03, bound:  3.15314442e-01\n",
      "Epoch: 38022 mean train loss:  3.29466769e-03, bound:  3.15314442e-01\n",
      "Epoch: 38023 mean train loss:  3.29462788e-03, bound:  3.15314442e-01\n",
      "Epoch: 38024 mean train loss:  3.29456991e-03, bound:  3.15314442e-01\n",
      "Epoch: 38025 mean train loss:  3.29452171e-03, bound:  3.15314442e-01\n",
      "Epoch: 38026 mean train loss:  3.29446071e-03, bound:  3.15314442e-01\n",
      "Epoch: 38027 mean train loss:  3.29439365e-03, bound:  3.15314382e-01\n",
      "Epoch: 38028 mean train loss:  3.29435105e-03, bound:  3.15314382e-01\n",
      "Epoch: 38029 mean train loss:  3.29429540e-03, bound:  3.15314382e-01\n",
      "Epoch: 38030 mean train loss:  3.29427444e-03, bound:  3.15314382e-01\n",
      "Epoch: 38031 mean train loss:  3.29417130e-03, bound:  3.15314382e-01\n",
      "Epoch: 38032 mean train loss:  3.29415989e-03, bound:  3.15314382e-01\n",
      "Epoch: 38033 mean train loss:  3.29413218e-03, bound:  3.15314382e-01\n",
      "Epoch: 38034 mean train loss:  3.29405768e-03, bound:  3.15314353e-01\n",
      "Epoch: 38035 mean train loss:  3.29408329e-03, bound:  3.15314353e-01\n",
      "Epoch: 38036 mean train loss:  3.29398713e-03, bound:  3.15314353e-01\n",
      "Epoch: 38037 mean train loss:  3.29398364e-03, bound:  3.15314353e-01\n",
      "Epoch: 38038 mean train loss:  3.29387700e-03, bound:  3.15314353e-01\n",
      "Epoch: 38039 mean train loss:  3.29386559e-03, bound:  3.15314353e-01\n",
      "Epoch: 38040 mean train loss:  3.29375546e-03, bound:  3.15314353e-01\n",
      "Epoch: 38041 mean train loss:  3.29372054e-03, bound:  3.15314293e-01\n",
      "Epoch: 38042 mean train loss:  3.29363719e-03, bound:  3.15314293e-01\n",
      "Epoch: 38043 mean train loss:  3.29357875e-03, bound:  3.15314293e-01\n",
      "Epoch: 38044 mean train loss:  3.29347001e-03, bound:  3.15314293e-01\n",
      "Epoch: 38045 mean train loss:  3.29343136e-03, bound:  3.15314293e-01\n",
      "Epoch: 38046 mean train loss:  3.29335360e-03, bound:  3.15314293e-01\n",
      "Epoch: 38047 mean train loss:  3.29328747e-03, bound:  3.15314263e-01\n",
      "Epoch: 38048 mean train loss:  3.29327281e-03, bound:  3.15314263e-01\n",
      "Epoch: 38049 mean train loss:  3.29320203e-03, bound:  3.15314263e-01\n",
      "Epoch: 38050 mean train loss:  3.29317921e-03, bound:  3.15314263e-01\n",
      "Epoch: 38051 mean train loss:  3.29307793e-03, bound:  3.15314263e-01\n",
      "Epoch: 38052 mean train loss:  3.29302554e-03, bound:  3.15314263e-01\n",
      "Epoch: 38053 mean train loss:  3.29296920e-03, bound:  3.15314233e-01\n",
      "Epoch: 38054 mean train loss:  3.29294964e-03, bound:  3.15314233e-01\n",
      "Epoch: 38055 mean train loss:  3.29285092e-03, bound:  3.15314233e-01\n",
      "Epoch: 38056 mean train loss:  3.29280505e-03, bound:  3.15314233e-01\n",
      "Epoch: 38057 mean train loss:  3.29270610e-03, bound:  3.15314233e-01\n",
      "Epoch: 38058 mean train loss:  3.29264929e-03, bound:  3.15314233e-01\n",
      "Epoch: 38059 mean train loss:  3.29262228e-03, bound:  3.15314233e-01\n",
      "Epoch: 38060 mean train loss:  3.29255313e-03, bound:  3.15314174e-01\n",
      "Epoch: 38061 mean train loss:  3.29248817e-03, bound:  3.15314174e-01\n",
      "Epoch: 38062 mean train loss:  3.29241366e-03, bound:  3.15314174e-01\n",
      "Epoch: 38063 mean train loss:  3.29240528e-03, bound:  3.15314174e-01\n",
      "Epoch: 38064 mean train loss:  3.29234474e-03, bound:  3.15314174e-01\n",
      "Epoch: 38065 mean train loss:  3.29226954e-03, bound:  3.15314174e-01\n",
      "Epoch: 38066 mean train loss:  3.29223438e-03, bound:  3.15314174e-01\n",
      "Epoch: 38067 mean train loss:  3.29219503e-03, bound:  3.15314144e-01\n",
      "Epoch: 38068 mean train loss:  3.29211890e-03, bound:  3.15314144e-01\n",
      "Epoch: 38069 mean train loss:  3.29208374e-03, bound:  3.15314144e-01\n",
      "Epoch: 38070 mean train loss:  3.29200062e-03, bound:  3.15314144e-01\n",
      "Epoch: 38071 mean train loss:  3.29193869e-03, bound:  3.15314144e-01\n",
      "Epoch: 38072 mean train loss:  3.29191424e-03, bound:  3.15314114e-01\n",
      "Epoch: 38073 mean train loss:  3.29184369e-03, bound:  3.15314114e-01\n",
      "Epoch: 38074 mean train loss:  3.29178222e-03, bound:  3.15314114e-01\n",
      "Epoch: 38075 mean train loss:  3.29173775e-03, bound:  3.15314114e-01\n",
      "Epoch: 38076 mean train loss:  3.29168630e-03, bound:  3.15314114e-01\n",
      "Epoch: 38077 mean train loss:  3.29159899e-03, bound:  3.15314114e-01\n",
      "Epoch: 38078 mean train loss:  3.29157407e-03, bound:  3.15314054e-01\n",
      "Epoch: 38079 mean train loss:  3.29150679e-03, bound:  3.15314054e-01\n",
      "Epoch: 38080 mean train loss:  3.29145556e-03, bound:  3.15314054e-01\n",
      "Epoch: 38081 mean train loss:  3.29141552e-03, bound:  3.15314054e-01\n",
      "Epoch: 38082 mean train loss:  3.29134450e-03, bound:  3.15314054e-01\n",
      "Epoch: 38083 mean train loss:  3.29129351e-03, bound:  3.15314054e-01\n",
      "Epoch: 38084 mean train loss:  3.29124997e-03, bound:  3.15314054e-01\n",
      "Epoch: 38085 mean train loss:  3.29116802e-03, bound:  3.15314054e-01\n",
      "Epoch: 38086 mean train loss:  3.29109095e-03, bound:  3.15314054e-01\n",
      "Epoch: 38087 mean train loss:  3.29104951e-03, bound:  3.15314025e-01\n",
      "Epoch: 38088 mean train loss:  3.29102599e-03, bound:  3.15314025e-01\n",
      "Epoch: 38089 mean train loss:  3.29095172e-03, bound:  3.15314025e-01\n",
      "Epoch: 38090 mean train loss:  3.29089747e-03, bound:  3.15314025e-01\n",
      "Epoch: 38091 mean train loss:  3.29086045e-03, bound:  3.15313995e-01\n",
      "Epoch: 38092 mean train loss:  3.29076755e-03, bound:  3.15313995e-01\n",
      "Epoch: 38093 mean train loss:  3.29073565e-03, bound:  3.15313995e-01\n",
      "Epoch: 38094 mean train loss:  3.29065137e-03, bound:  3.15313995e-01\n",
      "Epoch: 38095 mean train loss:  3.29063111e-03, bound:  3.15313995e-01\n",
      "Epoch: 38096 mean train loss:  3.29057453e-03, bound:  3.15313995e-01\n",
      "Epoch: 38097 mean train loss:  3.29051004e-03, bound:  3.15313995e-01\n",
      "Epoch: 38098 mean train loss:  3.29048163e-03, bound:  3.15313935e-01\n",
      "Epoch: 38099 mean train loss:  3.29036918e-03, bound:  3.15313935e-01\n",
      "Epoch: 38100 mean train loss:  3.29032633e-03, bound:  3.15313935e-01\n",
      "Epoch: 38101 mean train loss:  3.29030119e-03, bound:  3.15313935e-01\n",
      "Epoch: 38102 mean train loss:  3.29024671e-03, bound:  3.15313935e-01\n",
      "Epoch: 38103 mean train loss:  3.29023506e-03, bound:  3.15313935e-01\n",
      "Epoch: 38104 mean train loss:  3.29015055e-03, bound:  3.15313935e-01\n",
      "Epoch: 38105 mean train loss:  3.29010771e-03, bound:  3.15313935e-01\n",
      "Epoch: 38106 mean train loss:  3.29008163e-03, bound:  3.15313935e-01\n",
      "Epoch: 38107 mean train loss:  3.29001946e-03, bound:  3.15313935e-01\n",
      "Epoch: 38108 mean train loss:  3.28998012e-03, bound:  3.15313935e-01\n",
      "Epoch: 38109 mean train loss:  3.28987581e-03, bound:  3.15313935e-01\n",
      "Epoch: 38110 mean train loss:  3.28977057e-03, bound:  3.15313905e-01\n",
      "Epoch: 38111 mean train loss:  3.28977336e-03, bound:  3.15313905e-01\n",
      "Epoch: 38112 mean train loss:  3.28969397e-03, bound:  3.15313905e-01\n",
      "Epoch: 38113 mean train loss:  3.28956731e-03, bound:  3.15313905e-01\n",
      "Epoch: 38114 mean train loss:  3.28953750e-03, bound:  3.15313905e-01\n",
      "Epoch: 38115 mean train loss:  3.28947580e-03, bound:  3.15313905e-01\n",
      "Epoch: 38116 mean train loss:  3.28941969e-03, bound:  3.15313846e-01\n",
      "Epoch: 38117 mean train loss:  3.28935753e-03, bound:  3.15313816e-01\n",
      "Epoch: 38118 mean train loss:  3.28932866e-03, bound:  3.15313816e-01\n",
      "Epoch: 38119 mean train loss:  3.28930980e-03, bound:  3.15313816e-01\n",
      "Epoch: 38120 mean train loss:  3.28925555e-03, bound:  3.15313816e-01\n",
      "Epoch: 38121 mean train loss:  3.28915613e-03, bound:  3.15313816e-01\n",
      "Epoch: 38122 mean train loss:  3.28908814e-03, bound:  3.15313816e-01\n",
      "Epoch: 38123 mean train loss:  3.28903808e-03, bound:  3.15313816e-01\n",
      "Epoch: 38124 mean train loss:  3.28900455e-03, bound:  3.15313816e-01\n",
      "Epoch: 38125 mean train loss:  3.28890700e-03, bound:  3.15313816e-01\n",
      "Epoch: 38126 mean train loss:  3.28887580e-03, bound:  3.15313816e-01\n",
      "Epoch: 38127 mean train loss:  3.28884157e-03, bound:  3.15313816e-01\n",
      "Epoch: 38128 mean train loss:  3.28879943e-03, bound:  3.15313816e-01\n",
      "Epoch: 38129 mean train loss:  3.28874821e-03, bound:  3.15313786e-01\n",
      "Epoch: 38130 mean train loss:  3.28867510e-03, bound:  3.15313786e-01\n",
      "Epoch: 38131 mean train loss:  3.28864623e-03, bound:  3.15313786e-01\n",
      "Epoch: 38132 mean train loss:  3.28856730e-03, bound:  3.15313786e-01\n",
      "Epoch: 38133 mean train loss:  3.28849745e-03, bound:  3.15313786e-01\n",
      "Epoch: 38134 mean train loss:  3.28849303e-03, bound:  3.15313786e-01\n",
      "Epoch: 38135 mean train loss:  3.28837079e-03, bound:  3.15313786e-01\n",
      "Epoch: 38136 mean train loss:  3.28834867e-03, bound:  3.15313697e-01\n",
      "Epoch: 38137 mean train loss:  3.28827603e-03, bound:  3.15313697e-01\n",
      "Epoch: 38138 mean train loss:  3.28821270e-03, bound:  3.15313697e-01\n",
      "Epoch: 38139 mean train loss:  3.28816776e-03, bound:  3.15313697e-01\n",
      "Epoch: 38140 mean train loss:  3.28809605e-03, bound:  3.15313697e-01\n",
      "Epoch: 38141 mean train loss:  3.28806415e-03, bound:  3.15313697e-01\n",
      "Epoch: 38142 mean train loss:  3.28800967e-03, bound:  3.15313697e-01\n",
      "Epoch: 38143 mean train loss:  3.28794168e-03, bound:  3.15313697e-01\n",
      "Epoch: 38144 mean train loss:  3.28787277e-03, bound:  3.15313697e-01\n",
      "Epoch: 38145 mean train loss:  3.28779989e-03, bound:  3.15313697e-01\n",
      "Epoch: 38146 mean train loss:  3.28780734e-03, bound:  3.15313697e-01\n",
      "Epoch: 38147 mean train loss:  3.28772212e-03, bound:  3.15313697e-01\n",
      "Epoch: 38148 mean train loss:  3.28764669e-03, bound:  3.15313697e-01\n",
      "Epoch: 38149 mean train loss:  3.28765460e-03, bound:  3.15313667e-01\n",
      "Epoch: 38150 mean train loss:  3.28757474e-03, bound:  3.15313667e-01\n",
      "Epoch: 38151 mean train loss:  3.28754587e-03, bound:  3.15313667e-01\n",
      "Epoch: 38152 mean train loss:  3.28746904e-03, bound:  3.15313667e-01\n",
      "Epoch: 38153 mean train loss:  3.28741269e-03, bound:  3.15313667e-01\n",
      "Epoch: 38154 mean train loss:  3.28734657e-03, bound:  3.15313667e-01\n",
      "Epoch: 38155 mean train loss:  3.28727695e-03, bound:  3.15313607e-01\n",
      "Epoch: 38156 mean train loss:  3.28721944e-03, bound:  3.15313607e-01\n",
      "Epoch: 38157 mean train loss:  3.28713143e-03, bound:  3.15313607e-01\n",
      "Epoch: 38158 mean train loss:  3.28710442e-03, bound:  3.15313607e-01\n",
      "Epoch: 38159 mean train loss:  3.28701176e-03, bound:  3.15313607e-01\n",
      "Epoch: 38160 mean train loss:  3.28697916e-03, bound:  3.15313607e-01\n",
      "Epoch: 38161 mean train loss:  3.28691979e-03, bound:  3.15313607e-01\n",
      "Epoch: 38162 mean train loss:  3.28687718e-03, bound:  3.15313578e-01\n",
      "Epoch: 38163 mean train loss:  3.28680640e-03, bound:  3.15313578e-01\n",
      "Epoch: 38164 mean train loss:  3.28677939e-03, bound:  3.15313578e-01\n",
      "Epoch: 38165 mean train loss:  3.28678312e-03, bound:  3.15313578e-01\n",
      "Epoch: 38166 mean train loss:  3.28670605e-03, bound:  3.15313578e-01\n",
      "Epoch: 38167 mean train loss:  3.28663923e-03, bound:  3.15313578e-01\n",
      "Epoch: 38168 mean train loss:  3.28660104e-03, bound:  3.15313548e-01\n",
      "Epoch: 38169 mean train loss:  3.28651536e-03, bound:  3.15313548e-01\n",
      "Epoch: 38170 mean train loss:  3.28643271e-03, bound:  3.15313548e-01\n",
      "Epoch: 38171 mean train loss:  3.28638335e-03, bound:  3.15313548e-01\n",
      "Epoch: 38172 mean train loss:  3.28633818e-03, bound:  3.15313548e-01\n",
      "Epoch: 38173 mean train loss:  3.28624574e-03, bound:  3.15313548e-01\n",
      "Epoch: 38174 mean train loss:  3.28616519e-03, bound:  3.15313488e-01\n",
      "Epoch: 38175 mean train loss:  3.28612095e-03, bound:  3.15313488e-01\n",
      "Epoch: 38176 mean train loss:  3.28612421e-03, bound:  3.15313488e-01\n",
      "Epoch: 38177 mean train loss:  3.28607252e-03, bound:  3.15313488e-01\n",
      "Epoch: 38178 mean train loss:  3.28601641e-03, bound:  3.15313488e-01\n",
      "Epoch: 38179 mean train loss:  3.28597170e-03, bound:  3.15313488e-01\n",
      "Epoch: 38180 mean train loss:  3.28589324e-03, bound:  3.15313488e-01\n",
      "Epoch: 38181 mean train loss:  3.28582083e-03, bound:  3.15313488e-01\n",
      "Epoch: 38182 mean train loss:  3.28579056e-03, bound:  3.15313458e-01\n",
      "Epoch: 38183 mean train loss:  3.28567065e-03, bound:  3.15313458e-01\n",
      "Epoch: 38184 mean train loss:  3.28562642e-03, bound:  3.15313458e-01\n",
      "Epoch: 38185 mean train loss:  3.28559102e-03, bound:  3.15313458e-01\n",
      "Epoch: 38186 mean train loss:  3.28555400e-03, bound:  3.15313429e-01\n",
      "Epoch: 38187 mean train loss:  3.28549696e-03, bound:  3.15313429e-01\n",
      "Epoch: 38188 mean train loss:  3.28542385e-03, bound:  3.15313429e-01\n",
      "Epoch: 38189 mean train loss:  3.28536495e-03, bound:  3.15313429e-01\n",
      "Epoch: 38190 mean train loss:  3.28534679e-03, bound:  3.15313429e-01\n",
      "Epoch: 38191 mean train loss:  3.28524946e-03, bound:  3.15313429e-01\n",
      "Epoch: 38192 mean train loss:  3.28525016e-03, bound:  3.15313429e-01\n",
      "Epoch: 38193 mean train loss:  3.28515004e-03, bound:  3.15313429e-01\n",
      "Epoch: 38194 mean train loss:  3.28512047e-03, bound:  3.15313369e-01\n",
      "Epoch: 38195 mean train loss:  3.28505505e-03, bound:  3.15313369e-01\n",
      "Epoch: 38196 mean train loss:  3.28499591e-03, bound:  3.15313369e-01\n",
      "Epoch: 38197 mean train loss:  3.28497402e-03, bound:  3.15313369e-01\n",
      "Epoch: 38198 mean train loss:  3.28484015e-03, bound:  3.15313369e-01\n",
      "Epoch: 38199 mean train loss:  3.28481174e-03, bound:  3.15313339e-01\n",
      "Epoch: 38200 mean train loss:  3.28478054e-03, bound:  3.15313339e-01\n",
      "Epoch: 38201 mean train loss:  3.28468787e-03, bound:  3.15313339e-01\n",
      "Epoch: 38202 mean train loss:  3.28462757e-03, bound:  3.15313339e-01\n",
      "Epoch: 38203 mean train loss:  3.28458869e-03, bound:  3.15313339e-01\n",
      "Epoch: 38204 mean train loss:  3.28456168e-03, bound:  3.15313339e-01\n",
      "Epoch: 38205 mean train loss:  3.28456587e-03, bound:  3.15313339e-01\n",
      "Epoch: 38206 mean train loss:  3.28446738e-03, bound:  3.15313309e-01\n",
      "Epoch: 38207 mean train loss:  3.28441756e-03, bound:  3.15313309e-01\n",
      "Epoch: 38208 mean train loss:  3.28440173e-03, bound:  3.15313309e-01\n",
      "Epoch: 38209 mean train loss:  3.28432303e-03, bound:  3.15313309e-01\n",
      "Epoch: 38210 mean train loss:  3.28427833e-03, bound:  3.15313309e-01\n",
      "Epoch: 38211 mean train loss:  3.28419823e-03, bound:  3.15313309e-01\n",
      "Epoch: 38212 mean train loss:  3.28412442e-03, bound:  3.15313309e-01\n",
      "Epoch: 38213 mean train loss:  3.28407693e-03, bound:  3.15313250e-01\n",
      "Epoch: 38214 mean train loss:  3.28399893e-03, bound:  3.15313250e-01\n",
      "Epoch: 38215 mean train loss:  3.28391558e-03, bound:  3.15313250e-01\n",
      "Epoch: 38216 mean train loss:  3.28390021e-03, bound:  3.15313250e-01\n",
      "Epoch: 38217 mean train loss:  3.28378426e-03, bound:  3.15313250e-01\n",
      "Epoch: 38218 mean train loss:  3.28379194e-03, bound:  3.15313250e-01\n",
      "Epoch: 38219 mean train loss:  3.28371860e-03, bound:  3.15313220e-01\n",
      "Epoch: 38220 mean train loss:  3.28365946e-03, bound:  3.15313220e-01\n",
      "Epoch: 38221 mean train loss:  3.28358239e-03, bound:  3.15313220e-01\n",
      "Epoch: 38222 mean train loss:  3.28350719e-03, bound:  3.15313220e-01\n",
      "Epoch: 38223 mean train loss:  3.28349625e-03, bound:  3.15313220e-01\n",
      "Epoch: 38224 mean train loss:  3.28343152e-03, bound:  3.15313220e-01\n",
      "Epoch: 38225 mean train loss:  3.28338053e-03, bound:  3.15313220e-01\n",
      "Epoch: 38226 mean train loss:  3.28331743e-03, bound:  3.15313220e-01\n",
      "Epoch: 38227 mean train loss:  3.28327669e-03, bound:  3.15313220e-01\n",
      "Epoch: 38228 mean train loss:  3.28322290e-03, bound:  3.15313220e-01\n",
      "Epoch: 38229 mean train loss:  3.28314584e-03, bound:  3.15313220e-01\n",
      "Epoch: 38230 mean train loss:  3.28312954e-03, bound:  3.15313220e-01\n",
      "Epoch: 38231 mean train loss:  3.28301592e-03, bound:  3.15313160e-01\n",
      "Epoch: 38232 mean train loss:  3.28300684e-03, bound:  3.15313131e-01\n",
      "Epoch: 38233 mean train loss:  3.28290393e-03, bound:  3.15313131e-01\n",
      "Epoch: 38234 mean train loss:  3.28288553e-03, bound:  3.15313131e-01\n",
      "Epoch: 38235 mean train loss:  3.28288041e-03, bound:  3.15313131e-01\n",
      "Epoch: 38236 mean train loss:  3.28273023e-03, bound:  3.15313131e-01\n",
      "Epoch: 38237 mean train loss:  3.28270090e-03, bound:  3.15313131e-01\n",
      "Epoch: 38238 mean train loss:  3.28266248e-03, bound:  3.15313131e-01\n",
      "Epoch: 38239 mean train loss:  3.28262104e-03, bound:  3.15313101e-01\n",
      "Epoch: 38240 mean train loss:  3.28253442e-03, bound:  3.15313101e-01\n",
      "Epoch: 38241 mean train loss:  3.28251882e-03, bound:  3.15313101e-01\n",
      "Epoch: 38242 mean train loss:  3.28248320e-03, bound:  3.15313101e-01\n",
      "Epoch: 38243 mean train loss:  3.28234606e-03, bound:  3.15313101e-01\n",
      "Epoch: 38244 mean train loss:  3.28233442e-03, bound:  3.15313101e-01\n",
      "Epoch: 38245 mean train loss:  3.28227808e-03, bound:  3.15313101e-01\n",
      "Epoch: 38246 mean train loss:  3.28221708e-03, bound:  3.15313101e-01\n",
      "Epoch: 38247 mean train loss:  3.28217074e-03, bound:  3.15313101e-01\n",
      "Epoch: 38248 mean train loss:  3.28209717e-03, bound:  3.15313101e-01\n",
      "Epoch: 38249 mean train loss:  3.28206457e-03, bound:  3.15313101e-01\n",
      "Epoch: 38250 mean train loss:  3.28195142e-03, bound:  3.15313041e-01\n",
      "Epoch: 38251 mean train loss:  3.28197237e-03, bound:  3.15313101e-01\n",
      "Epoch: 38252 mean train loss:  3.28187249e-03, bound:  3.15313011e-01\n",
      "Epoch: 38253 mean train loss:  3.28180683e-03, bound:  3.15313011e-01\n",
      "Epoch: 38254 mean train loss:  3.28178750e-03, bound:  3.15313011e-01\n",
      "Epoch: 38255 mean train loss:  3.28168203e-03, bound:  3.15313011e-01\n",
      "Epoch: 38256 mean train loss:  3.28165921e-03, bound:  3.15313011e-01\n",
      "Epoch: 38257 mean train loss:  3.28159658e-03, bound:  3.15313011e-01\n",
      "Epoch: 38258 mean train loss:  3.28159309e-03, bound:  3.15313011e-01\n",
      "Epoch: 38259 mean train loss:  3.28149507e-03, bound:  3.15313011e-01\n",
      "Epoch: 38260 mean train loss:  3.28141660e-03, bound:  3.15313011e-01\n",
      "Epoch: 38261 mean train loss:  3.28139984e-03, bound:  3.15313011e-01\n",
      "Epoch: 38262 mean train loss:  3.28137656e-03, bound:  3.15313011e-01\n",
      "Epoch: 38263 mean train loss:  3.28129204e-03, bound:  3.15312982e-01\n",
      "Epoch: 38264 mean train loss:  3.28125525e-03, bound:  3.15312982e-01\n",
      "Epoch: 38265 mean train loss:  3.28119425e-03, bound:  3.15312982e-01\n",
      "Epoch: 38266 mean train loss:  3.28114768e-03, bound:  3.15312982e-01\n",
      "Epoch: 38267 mean train loss:  3.28109600e-03, bound:  3.15312982e-01\n",
      "Epoch: 38268 mean train loss:  3.28105129e-03, bound:  3.15312922e-01\n",
      "Epoch: 38269 mean train loss:  3.28102894e-03, bound:  3.15312922e-01\n",
      "Epoch: 38270 mean train loss:  3.28095653e-03, bound:  3.15312922e-01\n",
      "Epoch: 38271 mean train loss:  3.28086643e-03, bound:  3.15312922e-01\n",
      "Epoch: 38272 mean train loss:  3.28078889e-03, bound:  3.15312922e-01\n",
      "Epoch: 38273 mean train loss:  3.28073208e-03, bound:  3.15312922e-01\n",
      "Epoch: 38274 mean train loss:  3.28064058e-03, bound:  3.15312922e-01\n",
      "Epoch: 38275 mean train loss:  3.28058703e-03, bound:  3.15312922e-01\n",
      "Epoch: 38276 mean train loss:  3.28055397e-03, bound:  3.15312892e-01\n",
      "Epoch: 38277 mean train loss:  3.28051834e-03, bound:  3.15312892e-01\n",
      "Epoch: 38278 mean train loss:  3.28048505e-03, bound:  3.15312892e-01\n",
      "Epoch: 38279 mean train loss:  3.28040658e-03, bound:  3.15312892e-01\n",
      "Epoch: 38280 mean train loss:  3.28036118e-03, bound:  3.15312892e-01\n",
      "Epoch: 38281 mean train loss:  3.28025571e-03, bound:  3.15312892e-01\n",
      "Epoch: 38282 mean train loss:  3.28022102e-03, bound:  3.15312833e-01\n",
      "Epoch: 38283 mean train loss:  3.28018703e-03, bound:  3.15312833e-01\n",
      "Epoch: 38284 mean train loss:  3.28010693e-03, bound:  3.15312833e-01\n",
      "Epoch: 38285 mean train loss:  3.28003708e-03, bound:  3.15312833e-01\n",
      "Epoch: 38286 mean train loss:  3.27998679e-03, bound:  3.15312833e-01\n",
      "Epoch: 38287 mean train loss:  3.27996560e-03, bound:  3.15312833e-01\n",
      "Epoch: 38288 mean train loss:  3.27993114e-03, bound:  3.15312803e-01\n",
      "Epoch: 38289 mean train loss:  3.27984616e-03, bound:  3.15312803e-01\n",
      "Epoch: 38290 mean train loss:  3.27977887e-03, bound:  3.15312803e-01\n",
      "Epoch: 38291 mean train loss:  3.27971857e-03, bound:  3.15312803e-01\n",
      "Epoch: 38292 mean train loss:  3.27967783e-03, bound:  3.15312803e-01\n",
      "Epoch: 38293 mean train loss:  3.27965175e-03, bound:  3.15312803e-01\n",
      "Epoch: 38294 mean train loss:  3.27957235e-03, bound:  3.15312803e-01\n",
      "Epoch: 38295 mean train loss:  3.27950087e-03, bound:  3.15312773e-01\n",
      "Epoch: 38296 mean train loss:  3.27941682e-03, bound:  3.15312773e-01\n",
      "Epoch: 38297 mean train loss:  3.27937095e-03, bound:  3.15312773e-01\n",
      "Epoch: 38298 mean train loss:  3.27932648e-03, bound:  3.15312773e-01\n",
      "Epoch: 38299 mean train loss:  3.27924779e-03, bound:  3.15312773e-01\n",
      "Epoch: 38300 mean train loss:  3.27922753e-03, bound:  3.15312773e-01\n",
      "Epoch: 38301 mean train loss:  3.27918399e-03, bound:  3.15312743e-01\n",
      "Epoch: 38302 mean train loss:  3.27911484e-03, bound:  3.15312713e-01\n",
      "Epoch: 38303 mean train loss:  3.27907014e-03, bound:  3.15312713e-01\n",
      "Epoch: 38304 mean train loss:  3.27900425e-03, bound:  3.15312713e-01\n",
      "Epoch: 38305 mean train loss:  3.27896746e-03, bound:  3.15312713e-01\n",
      "Epoch: 38306 mean train loss:  3.27893137e-03, bound:  3.15312713e-01\n",
      "Epoch: 38307 mean train loss:  3.27882357e-03, bound:  3.15312713e-01\n",
      "Epoch: 38308 mean train loss:  3.27880564e-03, bound:  3.15312684e-01\n",
      "Epoch: 38309 mean train loss:  3.27873277e-03, bound:  3.15312684e-01\n",
      "Epoch: 38310 mean train loss:  3.27863777e-03, bound:  3.15312684e-01\n",
      "Epoch: 38311 mean train loss:  3.27857654e-03, bound:  3.15312684e-01\n",
      "Epoch: 38312 mean train loss:  3.27853207e-03, bound:  3.15312684e-01\n",
      "Epoch: 38313 mean train loss:  3.27849318e-03, bound:  3.15312684e-01\n",
      "Epoch: 38314 mean train loss:  3.27844778e-03, bound:  3.15312654e-01\n",
      "Epoch: 38315 mean train loss:  3.27838166e-03, bound:  3.15312654e-01\n",
      "Epoch: 38316 mean train loss:  3.27833439e-03, bound:  3.15312654e-01\n",
      "Epoch: 38317 mean train loss:  3.27825709e-03, bound:  3.15312654e-01\n",
      "Epoch: 38318 mean train loss:  3.27823567e-03, bound:  3.15312654e-01\n",
      "Epoch: 38319 mean train loss:  3.27817537e-03, bound:  3.15312594e-01\n",
      "Epoch: 38320 mean train loss:  3.27808759e-03, bound:  3.15312624e-01\n",
      "Epoch: 38321 mean train loss:  3.27807502e-03, bound:  3.15312594e-01\n",
      "Epoch: 38322 mean train loss:  3.27800470e-03, bound:  3.15312594e-01\n",
      "Epoch: 38323 mean train loss:  3.27798561e-03, bound:  3.15312594e-01\n",
      "Epoch: 38324 mean train loss:  3.27792182e-03, bound:  3.15312594e-01\n",
      "Epoch: 38325 mean train loss:  3.27787222e-03, bound:  3.15312594e-01\n",
      "Epoch: 38326 mean train loss:  3.27783963e-03, bound:  3.15312594e-01\n",
      "Epoch: 38327 mean train loss:  3.27780331e-03, bound:  3.15312564e-01\n",
      "Epoch: 38328 mean train loss:  3.27773881e-03, bound:  3.15312564e-01\n",
      "Epoch: 38329 mean train loss:  3.27771390e-03, bound:  3.15312564e-01\n",
      "Epoch: 38330 mean train loss:  3.27762426e-03, bound:  3.15312564e-01\n",
      "Epoch: 38331 mean train loss:  3.27760563e-03, bound:  3.15312564e-01\n",
      "Epoch: 38332 mean train loss:  3.27757746e-03, bound:  3.15312564e-01\n",
      "Epoch: 38333 mean train loss:  3.27744638e-03, bound:  3.15312535e-01\n",
      "Epoch: 38334 mean train loss:  3.27740447e-03, bound:  3.15312535e-01\n",
      "Epoch: 38335 mean train loss:  3.27735045e-03, bound:  3.15312535e-01\n",
      "Epoch: 38336 mean train loss:  3.27724870e-03, bound:  3.15312535e-01\n",
      "Epoch: 38337 mean train loss:  3.27717583e-03, bound:  3.15312535e-01\n",
      "Epoch: 38338 mean train loss:  3.27711040e-03, bound:  3.15312535e-01\n",
      "Epoch: 38339 mean train loss:  3.27712600e-03, bound:  3.15312535e-01\n",
      "Epoch: 38340 mean train loss:  3.27702099e-03, bound:  3.15312535e-01\n",
      "Epoch: 38341 mean train loss:  3.27695673e-03, bound:  3.15312535e-01\n",
      "Epoch: 38342 mean train loss:  3.27690295e-03, bound:  3.15312535e-01\n",
      "Epoch: 38343 mean train loss:  3.27684428e-03, bound:  3.15312535e-01\n",
      "Epoch: 38344 mean train loss:  3.27681936e-03, bound:  3.15312535e-01\n",
      "Epoch: 38345 mean train loss:  3.27671366e-03, bound:  3.15312535e-01\n",
      "Epoch: 38346 mean train loss:  3.27668153e-03, bound:  3.15312445e-01\n",
      "Epoch: 38347 mean train loss:  3.27661098e-03, bound:  3.15312445e-01\n",
      "Epoch: 38348 mean train loss:  3.27659119e-03, bound:  3.15312445e-01\n",
      "Epoch: 38349 mean train loss:  3.27648688e-03, bound:  3.15312445e-01\n",
      "Epoch: 38350 mean train loss:  3.27642774e-03, bound:  3.15312445e-01\n",
      "Epoch: 38351 mean train loss:  3.27640842e-03, bound:  3.15312445e-01\n",
      "Epoch: 38352 mean train loss:  3.27631435e-03, bound:  3.15312415e-01\n",
      "Epoch: 38353 mean train loss:  3.27625521e-03, bound:  3.15312415e-01\n",
      "Epoch: 38354 mean train loss:  3.27626173e-03, bound:  3.15312415e-01\n",
      "Epoch: 38355 mean train loss:  3.27617535e-03, bound:  3.15312415e-01\n",
      "Epoch: 38356 mean train loss:  3.27613973e-03, bound:  3.15312415e-01\n",
      "Epoch: 38357 mean train loss:  3.27605545e-03, bound:  3.15312415e-01\n",
      "Epoch: 38358 mean train loss:  3.27599281e-03, bound:  3.15312386e-01\n",
      "Epoch: 38359 mean train loss:  3.27597023e-03, bound:  3.15312386e-01\n",
      "Epoch: 38360 mean train loss:  3.27590131e-03, bound:  3.15312386e-01\n",
      "Epoch: 38361 mean train loss:  3.27583612e-03, bound:  3.15312386e-01\n",
      "Epoch: 38362 mean train loss:  3.27580725e-03, bound:  3.15312386e-01\n",
      "Epoch: 38363 mean train loss:  3.27572064e-03, bound:  3.15312386e-01\n",
      "Epoch: 38364 mean train loss:  3.27570364e-03, bound:  3.15312386e-01\n",
      "Epoch: 38365 mean train loss:  3.27567011e-03, bound:  3.15312326e-01\n",
      "Epoch: 38366 mean train loss:  3.27559677e-03, bound:  3.15312326e-01\n",
      "Epoch: 38367 mean train loss:  3.27558746e-03, bound:  3.15312326e-01\n",
      "Epoch: 38368 mean train loss:  3.27557535e-03, bound:  3.15312326e-01\n",
      "Epoch: 38369 mean train loss:  3.27549689e-03, bound:  3.15312326e-01\n",
      "Epoch: 38370 mean train loss:  3.27545917e-03, bound:  3.15312326e-01\n",
      "Epoch: 38371 mean train loss:  3.27535649e-03, bound:  3.15312326e-01\n",
      "Epoch: 38372 mean train loss:  3.27532971e-03, bound:  3.15312326e-01\n",
      "Epoch: 38373 mean train loss:  3.27520818e-03, bound:  3.15312296e-01\n",
      "Epoch: 38374 mean train loss:  3.27514461e-03, bound:  3.15312296e-01\n",
      "Epoch: 38375 mean train loss:  3.27504473e-03, bound:  3.15312296e-01\n",
      "Epoch: 38376 mean train loss:  3.27501958e-03, bound:  3.15312296e-01\n",
      "Epoch: 38377 mean train loss:  3.27496114e-03, bound:  3.15312296e-01\n",
      "Epoch: 38378 mean train loss:  3.27493157e-03, bound:  3.15312266e-01\n",
      "Epoch: 38379 mean train loss:  3.27482657e-03, bound:  3.15312266e-01\n",
      "Epoch: 38380 mean train loss:  3.27479676e-03, bound:  3.15312266e-01\n",
      "Epoch: 38381 mean train loss:  3.27479281e-03, bound:  3.15312266e-01\n",
      "Epoch: 38382 mean train loss:  3.27468710e-03, bound:  3.15312266e-01\n",
      "Epoch: 38383 mean train loss:  3.27466684e-03, bound:  3.15312266e-01\n",
      "Epoch: 38384 mean train loss:  3.27453739e-03, bound:  3.15312237e-01\n",
      "Epoch: 38385 mean train loss:  3.27452854e-03, bound:  3.15312237e-01\n",
      "Epoch: 38386 mean train loss:  3.27445148e-03, bound:  3.15312237e-01\n",
      "Epoch: 38387 mean train loss:  3.27443518e-03, bound:  3.15312237e-01\n",
      "Epoch: 38388 mean train loss:  3.27436998e-03, bound:  3.15312237e-01\n",
      "Epoch: 38389 mean train loss:  3.27430363e-03, bound:  3.15312237e-01\n",
      "Epoch: 38390 mean train loss:  3.27422912e-03, bound:  3.15312237e-01\n",
      "Epoch: 38391 mean train loss:  3.27420817e-03, bound:  3.15312207e-01\n",
      "Epoch: 38392 mean train loss:  3.27413809e-03, bound:  3.15312207e-01\n",
      "Epoch: 38393 mean train loss:  3.27407825e-03, bound:  3.15312207e-01\n",
      "Epoch: 38394 mean train loss:  3.27406335e-03, bound:  3.15312207e-01\n",
      "Epoch: 38395 mean train loss:  3.27398907e-03, bound:  3.15312207e-01\n",
      "Epoch: 38396 mean train loss:  3.27390921e-03, bound:  3.15312207e-01\n",
      "Epoch: 38397 mean train loss:  3.27388686e-03, bound:  3.15312177e-01\n",
      "Epoch: 38398 mean train loss:  3.27383191e-03, bound:  3.15312147e-01\n",
      "Epoch: 38399 mean train loss:  3.27375648e-03, bound:  3.15312147e-01\n",
      "Epoch: 38400 mean train loss:  3.27370339e-03, bound:  3.15312147e-01\n",
      "Epoch: 38401 mean train loss:  3.27367079e-03, bound:  3.15312147e-01\n",
      "Epoch: 38402 mean train loss:  3.27359233e-03, bound:  3.15312147e-01\n",
      "Epoch: 38403 mean train loss:  3.27352039e-03, bound:  3.15312147e-01\n",
      "Epoch: 38404 mean train loss:  3.27346567e-03, bound:  3.15312117e-01\n",
      "Epoch: 38405 mean train loss:  3.27343051e-03, bound:  3.15312117e-01\n",
      "Epoch: 38406 mean train loss:  3.27340537e-03, bound:  3.15312117e-01\n",
      "Epoch: 38407 mean train loss:  3.27333482e-03, bound:  3.15312117e-01\n",
      "Epoch: 38408 mean train loss:  3.27329012e-03, bound:  3.15312117e-01\n",
      "Epoch: 38409 mean train loss:  3.27323563e-03, bound:  3.15312117e-01\n",
      "Epoch: 38410 mean train loss:  3.27315624e-03, bound:  3.15312117e-01\n",
      "Epoch: 38411 mean train loss:  3.27309128e-03, bound:  3.15312088e-01\n",
      "Epoch: 38412 mean train loss:  3.27307126e-03, bound:  3.15312088e-01\n",
      "Epoch: 38413 mean train loss:  3.27294669e-03, bound:  3.15312088e-01\n",
      "Epoch: 38414 mean train loss:  3.27295135e-03, bound:  3.15312088e-01\n",
      "Epoch: 38415 mean train loss:  3.27288173e-03, bound:  3.15312088e-01\n",
      "Epoch: 38416 mean train loss:  3.27280979e-03, bound:  3.15312058e-01\n",
      "Epoch: 38417 mean train loss:  3.27274716e-03, bound:  3.15312028e-01\n",
      "Epoch: 38418 mean train loss:  3.27273272e-03, bound:  3.15312028e-01\n",
      "Epoch: 38419 mean train loss:  3.27261747e-03, bound:  3.15312028e-01\n",
      "Epoch: 38420 mean train loss:  3.27258464e-03, bound:  3.15312028e-01\n",
      "Epoch: 38421 mean train loss:  3.27255065e-03, bound:  3.15312028e-01\n",
      "Epoch: 38422 mean train loss:  3.27250478e-03, bound:  3.15312028e-01\n",
      "Epoch: 38423 mean train loss:  3.27245868e-03, bound:  3.15311998e-01\n",
      "Epoch: 38424 mean train loss:  3.27236089e-03, bound:  3.15311998e-01\n",
      "Epoch: 38425 mean train loss:  3.27230548e-03, bound:  3.15311998e-01\n",
      "Epoch: 38426 mean train loss:  3.27228801e-03, bound:  3.15311998e-01\n",
      "Epoch: 38427 mean train loss:  3.27220070e-03, bound:  3.15311998e-01\n",
      "Epoch: 38428 mean train loss:  3.27215903e-03, bound:  3.15311998e-01\n",
      "Epoch: 38429 mean train loss:  3.27207497e-03, bound:  3.15311998e-01\n",
      "Epoch: 38430 mean train loss:  3.27200349e-03, bound:  3.15311968e-01\n",
      "Epoch: 38431 mean train loss:  3.27201490e-03, bound:  3.15311968e-01\n",
      "Epoch: 38432 mean train loss:  3.27193062e-03, bound:  3.15311968e-01\n",
      "Epoch: 38433 mean train loss:  3.27189662e-03, bound:  3.15311968e-01\n",
      "Epoch: 38434 mean train loss:  3.27183655e-03, bound:  3.15311968e-01\n",
      "Epoch: 38435 mean train loss:  3.27176694e-03, bound:  3.15311939e-01\n",
      "Epoch: 38436 mean train loss:  3.27174552e-03, bound:  3.15311939e-01\n",
      "Epoch: 38437 mean train loss:  3.27167148e-03, bound:  3.15311909e-01\n",
      "Epoch: 38438 mean train loss:  3.27162142e-03, bound:  3.15311909e-01\n",
      "Epoch: 38439 mean train loss:  3.27155320e-03, bound:  3.15311909e-01\n",
      "Epoch: 38440 mean train loss:  3.27150780e-03, bound:  3.15311909e-01\n",
      "Epoch: 38441 mean train loss:  3.27148451e-03, bound:  3.15311879e-01\n",
      "Epoch: 38442 mean train loss:  3.27140186e-03, bound:  3.15311879e-01\n",
      "Epoch: 38443 mean train loss:  3.27135483e-03, bound:  3.15311879e-01\n",
      "Epoch: 38444 mean train loss:  3.27131385e-03, bound:  3.15311879e-01\n",
      "Epoch: 38445 mean train loss:  3.27125634e-03, bound:  3.15311879e-01\n",
      "Epoch: 38446 mean train loss:  3.27117625e-03, bound:  3.15311879e-01\n",
      "Epoch: 38447 mean train loss:  3.27111571e-03, bound:  3.15311879e-01\n",
      "Epoch: 38448 mean train loss:  3.27105471e-03, bound:  3.15311879e-01\n",
      "Epoch: 38449 mean train loss:  3.27105564e-03, bound:  3.15311849e-01\n",
      "Epoch: 38450 mean train loss:  3.27100581e-03, bound:  3.15311849e-01\n",
      "Epoch: 38451 mean train loss:  3.27094994e-03, bound:  3.15311849e-01\n",
      "Epoch: 38452 mean train loss:  3.27092269e-03, bound:  3.15311849e-01\n",
      "Epoch: 38453 mean train loss:  3.27087636e-03, bound:  3.15311849e-01\n",
      "Epoch: 38454 mean train loss:  3.27084959e-03, bound:  3.15311819e-01\n",
      "Epoch: 38455 mean train loss:  3.27083236e-03, bound:  3.15311790e-01\n",
      "Epoch: 38456 mean train loss:  3.27078439e-03, bound:  3.15311790e-01\n",
      "Epoch: 38457 mean train loss:  3.27071920e-03, bound:  3.15311790e-01\n",
      "Epoch: 38458 mean train loss:  3.27067147e-03, bound:  3.15311790e-01\n",
      "Epoch: 38459 mean train loss:  3.27062071e-03, bound:  3.15311790e-01\n",
      "Epoch: 38460 mean train loss:  3.27054970e-03, bound:  3.15311790e-01\n",
      "Epoch: 38461 mean train loss:  3.27033876e-03, bound:  3.15311790e-01\n",
      "Epoch: 38462 mean train loss:  3.27032479e-03, bound:  3.15311760e-01\n",
      "Epoch: 38463 mean train loss:  3.27029335e-03, bound:  3.15311760e-01\n",
      "Epoch: 38464 mean train loss:  3.27018462e-03, bound:  3.15311760e-01\n",
      "Epoch: 38465 mean train loss:  3.27013852e-03, bound:  3.15311760e-01\n",
      "Epoch: 38466 mean train loss:  3.27016599e-03, bound:  3.15311760e-01\n",
      "Epoch: 38467 mean train loss:  3.27010453e-03, bound:  3.15311730e-01\n",
      "Epoch: 38468 mean train loss:  3.27006634e-03, bound:  3.15311730e-01\n",
      "Epoch: 38469 mean train loss:  3.26998136e-03, bound:  3.15311730e-01\n",
      "Epoch: 38470 mean train loss:  3.26991454e-03, bound:  3.15311730e-01\n",
      "Epoch: 38471 mean train loss:  3.26984026e-03, bound:  3.15311730e-01\n",
      "Epoch: 38472 mean train loss:  3.26977065e-03, bound:  3.15311730e-01\n",
      "Epoch: 38473 mean train loss:  3.26968241e-03, bound:  3.15311730e-01\n",
      "Epoch: 38474 mean train loss:  3.26966844e-03, bound:  3.15311670e-01\n",
      "Epoch: 38475 mean train loss:  3.26965004e-03, bound:  3.15311670e-01\n",
      "Epoch: 38476 mean train loss:  3.26957344e-03, bound:  3.15311670e-01\n",
      "Epoch: 38477 mean train loss:  3.26954876e-03, bound:  3.15311670e-01\n",
      "Epoch: 38478 mean train loss:  3.26947263e-03, bound:  3.15311670e-01\n",
      "Epoch: 38479 mean train loss:  3.26937623e-03, bound:  3.15311670e-01\n",
      "Epoch: 38480 mean train loss:  3.26936925e-03, bound:  3.15311670e-01\n",
      "Epoch: 38481 mean train loss:  3.26923979e-03, bound:  3.15311641e-01\n",
      "Epoch: 38482 mean train loss:  3.26923886e-03, bound:  3.15311641e-01\n",
      "Epoch: 38483 mean train loss:  3.26914969e-03, bound:  3.15311641e-01\n",
      "Epoch: 38484 mean train loss:  3.26909707e-03, bound:  3.15311641e-01\n",
      "Epoch: 38485 mean train loss:  3.26906354e-03, bound:  3.15311641e-01\n",
      "Epoch: 38486 mean train loss:  3.26905586e-03, bound:  3.15311641e-01\n",
      "Epoch: 38487 mean train loss:  3.26898159e-03, bound:  3.15311641e-01\n",
      "Epoch: 38488 mean train loss:  3.26886796e-03, bound:  3.15311611e-01\n",
      "Epoch: 38489 mean train loss:  3.26883863e-03, bound:  3.15311611e-01\n",
      "Epoch: 38490 mean train loss:  3.26876342e-03, bound:  3.15311611e-01\n",
      "Epoch: 38491 mean train loss:  3.26876435e-03, bound:  3.15311611e-01\n",
      "Epoch: 38492 mean train loss:  3.26868915e-03, bound:  3.15311611e-01\n",
      "Epoch: 38493 mean train loss:  3.26864701e-03, bound:  3.15311581e-01\n",
      "Epoch: 38494 mean train loss:  3.26861674e-03, bound:  3.15311581e-01\n",
      "Epoch: 38495 mean train loss:  3.26854945e-03, bound:  3.15311581e-01\n",
      "Epoch: 38496 mean train loss:  3.26847588e-03, bound:  3.15311581e-01\n",
      "Epoch: 38497 mean train loss:  3.26839439e-03, bound:  3.15311581e-01\n",
      "Epoch: 38498 mean train loss:  3.26837203e-03, bound:  3.15311581e-01\n",
      "Epoch: 38499 mean train loss:  3.26832058e-03, bound:  3.15311551e-01\n",
      "Epoch: 38500 mean train loss:  3.26822163e-03, bound:  3.15311551e-01\n",
      "Epoch: 38501 mean train loss:  3.26821674e-03, bound:  3.15311551e-01\n",
      "Epoch: 38502 mean train loss:  3.26817925e-03, bound:  3.15311551e-01\n",
      "Epoch: 38503 mean train loss:  3.26810405e-03, bound:  3.15311551e-01\n",
      "Epoch: 38504 mean train loss:  3.26798717e-03, bound:  3.15311551e-01\n",
      "Epoch: 38505 mean train loss:  3.26798926e-03, bound:  3.15311521e-01\n",
      "Epoch: 38506 mean train loss:  3.26790707e-03, bound:  3.15311521e-01\n",
      "Epoch: 38507 mean train loss:  3.26782255e-03, bound:  3.15311521e-01\n",
      "Epoch: 38508 mean train loss:  3.26779671e-03, bound:  3.15311491e-01\n",
      "Epoch: 38509 mean train loss:  3.26775969e-03, bound:  3.15311491e-01\n",
      "Epoch: 38510 mean train loss:  3.26776667e-03, bound:  3.15311491e-01\n",
      "Epoch: 38511 mean train loss:  3.26765492e-03, bound:  3.15311491e-01\n",
      "Epoch: 38512 mean train loss:  3.26761906e-03, bound:  3.15311491e-01\n",
      "Epoch: 38513 mean train loss:  3.26751382e-03, bound:  3.15311491e-01\n",
      "Epoch: 38514 mean train loss:  3.26748658e-03, bound:  3.15311462e-01\n",
      "Epoch: 38515 mean train loss:  3.26747657e-03, bound:  3.15311462e-01\n",
      "Epoch: 38516 mean train loss:  3.26739764e-03, bound:  3.15311462e-01\n",
      "Epoch: 38517 mean train loss:  3.26736690e-03, bound:  3.15311462e-01\n",
      "Epoch: 38518 mean train loss:  3.26735945e-03, bound:  3.15311462e-01\n",
      "Epoch: 38519 mean train loss:  3.26723908e-03, bound:  3.15311462e-01\n",
      "Epoch: 38520 mean train loss:  3.26715456e-03, bound:  3.15311432e-01\n",
      "Epoch: 38521 mean train loss:  3.26708890e-03, bound:  3.15311432e-01\n",
      "Epoch: 38522 mean train loss:  3.26707028e-03, bound:  3.15311432e-01\n",
      "Epoch: 38523 mean train loss:  3.26703140e-03, bound:  3.15311432e-01\n",
      "Epoch: 38524 mean train loss:  3.26697924e-03, bound:  3.15311432e-01\n",
      "Epoch: 38525 mean train loss:  3.26691568e-03, bound:  3.15311432e-01\n",
      "Epoch: 38526 mean train loss:  3.26683116e-03, bound:  3.15311432e-01\n",
      "Epoch: 38527 mean train loss:  3.26679437e-03, bound:  3.15311402e-01\n",
      "Epoch: 38528 mean train loss:  3.26676411e-03, bound:  3.15311402e-01\n",
      "Epoch: 38529 mean train loss:  3.26669356e-03, bound:  3.15311402e-01\n",
      "Epoch: 38530 mean train loss:  3.26661882e-03, bound:  3.15311402e-01\n",
      "Epoch: 38531 mean train loss:  3.26659391e-03, bound:  3.15311402e-01\n",
      "Epoch: 38532 mean train loss:  3.26653197e-03, bound:  3.15311402e-01\n",
      "Epoch: 38533 mean train loss:  3.26650497e-03, bound:  3.15311342e-01\n",
      "Epoch: 38534 mean train loss:  3.26639903e-03, bound:  3.15311342e-01\n",
      "Epoch: 38535 mean train loss:  3.26636061e-03, bound:  3.15311342e-01\n",
      "Epoch: 38536 mean train loss:  3.26629984e-03, bound:  3.15311342e-01\n",
      "Epoch: 38537 mean train loss:  3.26627051e-03, bound:  3.15311342e-01\n",
      "Epoch: 38538 mean train loss:  3.26624163e-03, bound:  3.15311342e-01\n",
      "Epoch: 38539 mean train loss:  3.26615060e-03, bound:  3.15311342e-01\n",
      "Epoch: 38540 mean train loss:  3.26609728e-03, bound:  3.15311313e-01\n",
      "Epoch: 38541 mean train loss:  3.26601067e-03, bound:  3.15311313e-01\n",
      "Epoch: 38542 mean train loss:  3.26596177e-03, bound:  3.15311313e-01\n",
      "Epoch: 38543 mean train loss:  3.26593313e-03, bound:  3.15311313e-01\n",
      "Epoch: 38544 mean train loss:  3.26593150e-03, bound:  3.15311313e-01\n",
      "Epoch: 38545 mean train loss:  3.26579716e-03, bound:  3.15311283e-01\n",
      "Epoch: 38546 mean train loss:  3.26573825e-03, bound:  3.15311313e-01\n",
      "Epoch: 38547 mean train loss:  3.26574128e-03, bound:  3.15311283e-01\n",
      "Epoch: 38548 mean train loss:  3.26564023e-03, bound:  3.15311283e-01\n",
      "Epoch: 38549 mean train loss:  3.26558715e-03, bound:  3.15311283e-01\n",
      "Epoch: 38550 mean train loss:  3.26556340e-03, bound:  3.15311283e-01\n",
      "Epoch: 38551 mean train loss:  3.26550868e-03, bound:  3.15311283e-01\n",
      "Epoch: 38552 mean train loss:  3.26544582e-03, bound:  3.15311253e-01\n",
      "Epoch: 38553 mean train loss:  3.26538202e-03, bound:  3.15311223e-01\n",
      "Epoch: 38554 mean train loss:  3.26528959e-03, bound:  3.15311223e-01\n",
      "Epoch: 38555 mean train loss:  3.26528680e-03, bound:  3.15311223e-01\n",
      "Epoch: 38556 mean train loss:  3.26521625e-03, bound:  3.15311223e-01\n",
      "Epoch: 38557 mean train loss:  3.26519273e-03, bound:  3.15311223e-01\n",
      "Epoch: 38558 mean train loss:  3.26512149e-03, bound:  3.15311193e-01\n",
      "Epoch: 38559 mean train loss:  3.26505420e-03, bound:  3.15311193e-01\n",
      "Epoch: 38560 mean train loss:  3.26499483e-03, bound:  3.15311193e-01\n",
      "Epoch: 38561 mean train loss:  3.26492032e-03, bound:  3.15311193e-01\n",
      "Epoch: 38562 mean train loss:  3.26492451e-03, bound:  3.15311193e-01\n",
      "Epoch: 38563 mean train loss:  3.26481368e-03, bound:  3.15311193e-01\n",
      "Epoch: 38564 mean train loss:  3.26479599e-03, bound:  3.15311193e-01\n",
      "Epoch: 38565 mean train loss:  3.26472917e-03, bound:  3.15311193e-01\n",
      "Epoch: 38566 mean train loss:  3.26466747e-03, bound:  3.15311164e-01\n",
      "Epoch: 38567 mean train loss:  3.26462975e-03, bound:  3.15311164e-01\n",
      "Epoch: 38568 mean train loss:  3.26454011e-03, bound:  3.15311164e-01\n",
      "Epoch: 38569 mean train loss:  3.26457061e-03, bound:  3.15311164e-01\n",
      "Epoch: 38570 mean train loss:  3.26446304e-03, bound:  3.15311134e-01\n",
      "Epoch: 38571 mean train loss:  3.26438574e-03, bound:  3.15311164e-01\n",
      "Epoch: 38572 mean train loss:  3.26433242e-03, bound:  3.15311104e-01\n",
      "Epoch: 38573 mean train loss:  3.26429517e-03, bound:  3.15311104e-01\n",
      "Epoch: 38574 mean train loss:  3.26427491e-03, bound:  3.15311104e-01\n",
      "Epoch: 38575 mean train loss:  3.26417829e-03, bound:  3.15311104e-01\n",
      "Epoch: 38576 mean train loss:  3.26416036e-03, bound:  3.15311104e-01\n",
      "Epoch: 38577 mean train loss:  3.26408562e-03, bound:  3.15311104e-01\n",
      "Epoch: 38578 mean train loss:  3.26405908e-03, bound:  3.15311074e-01\n",
      "Epoch: 38579 mean train loss:  3.26397340e-03, bound:  3.15311074e-01\n",
      "Epoch: 38580 mean train loss:  3.26393568e-03, bound:  3.15311074e-01\n",
      "Epoch: 38581 mean train loss:  3.26383812e-03, bound:  3.15311074e-01\n",
      "Epoch: 38582 mean train loss:  3.26378038e-03, bound:  3.15311074e-01\n",
      "Epoch: 38583 mean train loss:  3.26379412e-03, bound:  3.15311074e-01\n",
      "Epoch: 38584 mean train loss:  3.26370704e-03, bound:  3.15311074e-01\n",
      "Epoch: 38585 mean train loss:  3.26367165e-03, bound:  3.15311044e-01\n",
      "Epoch: 38586 mean train loss:  3.26364697e-03, bound:  3.15311044e-01\n",
      "Epoch: 38587 mean train loss:  3.26355756e-03, bound:  3.15311044e-01\n",
      "Epoch: 38588 mean train loss:  3.26348166e-03, bound:  3.15311044e-01\n",
      "Epoch: 38589 mean train loss:  3.26347444e-03, bound:  3.15311044e-01\n",
      "Epoch: 38590 mean train loss:  3.26340552e-03, bound:  3.15311044e-01\n",
      "Epoch: 38591 mean train loss:  3.26330983e-03, bound:  3.15310985e-01\n",
      "Epoch: 38592 mean train loss:  3.26331914e-03, bound:  3.15310985e-01\n",
      "Epoch: 38593 mean train loss:  3.26322042e-03, bound:  3.15310985e-01\n",
      "Epoch: 38594 mean train loss:  3.26315709e-03, bound:  3.15310985e-01\n",
      "Epoch: 38595 mean train loss:  3.26309819e-03, bound:  3.15310985e-01\n",
      "Epoch: 38596 mean train loss:  3.26307514e-03, bound:  3.15310985e-01\n",
      "Epoch: 38597 mean train loss:  3.26298201e-03, bound:  3.15310955e-01\n",
      "Epoch: 38598 mean train loss:  3.26294196e-03, bound:  3.15310955e-01\n",
      "Epoch: 38599 mean train loss:  3.26286745e-03, bound:  3.15310955e-01\n",
      "Epoch: 38600 mean train loss:  3.26283788e-03, bound:  3.15310955e-01\n",
      "Epoch: 38601 mean train loss:  3.26274941e-03, bound:  3.15310955e-01\n",
      "Epoch: 38602 mean train loss:  3.26273078e-03, bound:  3.15310955e-01\n",
      "Epoch: 38603 mean train loss:  3.26267583e-03, bound:  3.15310955e-01\n",
      "Epoch: 38604 mean train loss:  3.26264068e-03, bound:  3.15310955e-01\n",
      "Epoch: 38605 mean train loss:  3.26256291e-03, bound:  3.15310895e-01\n",
      "Epoch: 38606 mean train loss:  3.26252449e-03, bound:  3.15310895e-01\n",
      "Epoch: 38607 mean train loss:  3.26249097e-03, bound:  3.15310895e-01\n",
      "Epoch: 38608 mean train loss:  3.26239900e-03, bound:  3.15310895e-01\n",
      "Epoch: 38609 mean train loss:  3.26235662e-03, bound:  3.15310895e-01\n",
      "Epoch: 38610 mean train loss:  3.26226093e-03, bound:  3.15310866e-01\n",
      "Epoch: 38611 mean train loss:  3.26223834e-03, bound:  3.15310866e-01\n",
      "Epoch: 38612 mean train loss:  3.26214894e-03, bound:  3.15310866e-01\n",
      "Epoch: 38613 mean train loss:  3.26212007e-03, bound:  3.15310866e-01\n",
      "Epoch: 38614 mean train loss:  3.26205720e-03, bound:  3.15310866e-01\n",
      "Epoch: 38615 mean train loss:  3.26201250e-03, bound:  3.15310866e-01\n",
      "Epoch: 38616 mean train loss:  3.26197105e-03, bound:  3.15310866e-01\n",
      "Epoch: 38617 mean train loss:  3.26191494e-03, bound:  3.15310866e-01\n",
      "Epoch: 38618 mean train loss:  3.26188491e-03, bound:  3.15310866e-01\n",
      "Epoch: 38619 mean train loss:  3.26175615e-03, bound:  3.15310866e-01\n",
      "Epoch: 38620 mean train loss:  3.26176849e-03, bound:  3.15310866e-01\n",
      "Epoch: 38621 mean train loss:  3.26167280e-03, bound:  3.15310866e-01\n",
      "Epoch: 38622 mean train loss:  3.26160062e-03, bound:  3.15310836e-01\n",
      "Epoch: 38623 mean train loss:  3.26159503e-03, bound:  3.15310776e-01\n",
      "Epoch: 38624 mean train loss:  3.26150167e-03, bound:  3.15310776e-01\n",
      "Epoch: 38625 mean train loss:  3.26151028e-03, bound:  3.15310776e-01\n",
      "Epoch: 38626 mean train loss:  3.26144858e-03, bound:  3.15310776e-01\n",
      "Epoch: 38627 mean train loss:  3.26143601e-03, bound:  3.15310776e-01\n",
      "Epoch: 38628 mean train loss:  3.26137431e-03, bound:  3.15310776e-01\n",
      "Epoch: 38629 mean train loss:  3.26129375e-03, bound:  3.15310776e-01\n",
      "Epoch: 38630 mean train loss:  3.26124090e-03, bound:  3.15310776e-01\n",
      "Epoch: 38631 mean train loss:  3.26122832e-03, bound:  3.15310776e-01\n",
      "Epoch: 38632 mean train loss:  3.26115405e-03, bound:  3.15310776e-01\n",
      "Epoch: 38633 mean train loss:  3.26105626e-03, bound:  3.15310776e-01\n",
      "Epoch: 38634 mean train loss:  3.26097547e-03, bound:  3.15310776e-01\n",
      "Epoch: 38635 mean train loss:  3.26093053e-03, bound:  3.15310776e-01\n",
      "Epoch: 38636 mean train loss:  3.26093147e-03, bound:  3.15310746e-01\n",
      "Epoch: 38637 mean train loss:  3.26083251e-03, bound:  3.15310746e-01\n",
      "Epoch: 38638 mean train loss:  3.26075824e-03, bound:  3.15310746e-01\n",
      "Epoch: 38639 mean train loss:  3.26065859e-03, bound:  3.15310746e-01\n",
      "Epoch: 38640 mean train loss:  3.26066650e-03, bound:  3.15310746e-01\n",
      "Epoch: 38641 mean train loss:  3.26062599e-03, bound:  3.15310746e-01\n",
      "Epoch: 38642 mean train loss:  3.26054869e-03, bound:  3.15310746e-01\n",
      "Epoch: 38643 mean train loss:  3.26048816e-03, bound:  3.15310657e-01\n",
      "Epoch: 38644 mean train loss:  3.26045766e-03, bound:  3.15310657e-01\n",
      "Epoch: 38645 mean train loss:  3.26040993e-03, bound:  3.15310657e-01\n",
      "Epoch: 38646 mean train loss:  3.26039130e-03, bound:  3.15310657e-01\n",
      "Epoch: 38647 mean train loss:  3.26033775e-03, bound:  3.15310657e-01\n",
      "Epoch: 38648 mean train loss:  3.26033123e-03, bound:  3.15310657e-01\n",
      "Epoch: 38649 mean train loss:  3.26026906e-03, bound:  3.15310657e-01\n",
      "Epoch: 38650 mean train loss:  3.26025928e-03, bound:  3.15310657e-01\n",
      "Epoch: 38651 mean train loss:  3.26016196e-03, bound:  3.15310657e-01\n",
      "Epoch: 38652 mean train loss:  3.26010352e-03, bound:  3.15310657e-01\n",
      "Epoch: 38653 mean train loss:  3.26003274e-03, bound:  3.15310657e-01\n",
      "Epoch: 38654 mean train loss:  3.25994054e-03, bound:  3.15310657e-01\n",
      "Epoch: 38655 mean train loss:  3.25986138e-03, bound:  3.15310627e-01\n",
      "Epoch: 38656 mean train loss:  3.25983879e-03, bound:  3.15310627e-01\n",
      "Epoch: 38657 mean train loss:  3.25973518e-03, bound:  3.15310627e-01\n",
      "Epoch: 38658 mean train loss:  3.25971562e-03, bound:  3.15310627e-01\n",
      "Epoch: 38659 mean train loss:  3.25967604e-03, bound:  3.15310627e-01\n",
      "Epoch: 38660 mean train loss:  3.25965858e-03, bound:  3.15310627e-01\n",
      "Epoch: 38661 mean train loss:  3.25959222e-03, bound:  3.15310597e-01\n",
      "Epoch: 38662 mean train loss:  3.25951190e-03, bound:  3.15310568e-01\n",
      "Epoch: 38663 mean train loss:  3.25943320e-03, bound:  3.15310568e-01\n",
      "Epoch: 38664 mean train loss:  3.25938594e-03, bound:  3.15310568e-01\n",
      "Epoch: 38665 mean train loss:  3.25928512e-03, bound:  3.15310568e-01\n",
      "Epoch: 38666 mean train loss:  3.25927744e-03, bound:  3.15310568e-01\n",
      "Epoch: 38667 mean train loss:  3.25920759e-03, bound:  3.15310538e-01\n",
      "Epoch: 38668 mean train loss:  3.25917173e-03, bound:  3.15310568e-01\n",
      "Epoch: 38669 mean train loss:  3.25915054e-03, bound:  3.15310538e-01\n",
      "Epoch: 38670 mean train loss:  3.25911352e-03, bound:  3.15310538e-01\n",
      "Epoch: 38671 mean train loss:  3.25902528e-03, bound:  3.15310538e-01\n",
      "Epoch: 38672 mean train loss:  3.25895310e-03, bound:  3.15310538e-01\n",
      "Epoch: 38673 mean train loss:  3.25884810e-03, bound:  3.15310508e-01\n",
      "Epoch: 38674 mean train loss:  3.25881573e-03, bound:  3.15310538e-01\n",
      "Epoch: 38675 mean train loss:  3.25877476e-03, bound:  3.15310508e-01\n",
      "Epoch: 38676 mean train loss:  3.25874030e-03, bound:  3.15310508e-01\n",
      "Epoch: 38677 mean train loss:  3.25869396e-03, bound:  3.15310508e-01\n",
      "Epoch: 38678 mean train loss:  3.25861922e-03, bound:  3.15310508e-01\n",
      "Epoch: 38679 mean train loss:  3.25862295e-03, bound:  3.15310508e-01\n",
      "Epoch: 38680 mean train loss:  3.25851841e-03, bound:  3.15310508e-01\n",
      "Epoch: 38681 mean train loss:  3.25845252e-03, bound:  3.15310478e-01\n",
      "Epoch: 38682 mean train loss:  3.25838919e-03, bound:  3.15310478e-01\n",
      "Epoch: 38683 mean train loss:  3.25832935e-03, bound:  3.15310478e-01\n",
      "Epoch: 38684 mean train loss:  3.25831212e-03, bound:  3.15310478e-01\n",
      "Epoch: 38685 mean train loss:  3.25827091e-03, bound:  3.15310478e-01\n",
      "Epoch: 38686 mean train loss:  3.25820711e-03, bound:  3.15310478e-01\n",
      "Epoch: 38687 mean train loss:  3.25812656e-03, bound:  3.15310478e-01\n",
      "Epoch: 38688 mean train loss:  3.25805531e-03, bound:  3.15310448e-01\n",
      "Epoch: 38689 mean train loss:  3.25801410e-03, bound:  3.15310419e-01\n",
      "Epoch: 38690 mean train loss:  3.25797009e-03, bound:  3.15310419e-01\n",
      "Epoch: 38691 mean train loss:  3.25786625e-03, bound:  3.15310419e-01\n",
      "Epoch: 38692 mean train loss:  3.25786206e-03, bound:  3.15310419e-01\n",
      "Epoch: 38693 mean train loss:  3.25779547e-03, bound:  3.15310389e-01\n",
      "Epoch: 38694 mean train loss:  3.25774122e-03, bound:  3.15310389e-01\n",
      "Epoch: 38695 mean train loss:  3.25771095e-03, bound:  3.15310389e-01\n",
      "Epoch: 38696 mean train loss:  3.25762923e-03, bound:  3.15310389e-01\n",
      "Epoch: 38697 mean train loss:  3.25759733e-03, bound:  3.15310389e-01\n",
      "Epoch: 38698 mean train loss:  3.25749046e-03, bound:  3.15310389e-01\n",
      "Epoch: 38699 mean train loss:  3.25748837e-03, bound:  3.15310389e-01\n",
      "Epoch: 38700 mean train loss:  3.25744879e-03, bound:  3.15310389e-01\n",
      "Epoch: 38701 mean train loss:  3.25736753e-03, bound:  3.15310389e-01\n",
      "Epoch: 38702 mean train loss:  3.25733447e-03, bound:  3.15310329e-01\n",
      "Epoch: 38703 mean train loss:  3.25727998e-03, bound:  3.15310329e-01\n",
      "Epoch: 38704 mean train loss:  3.25724622e-03, bound:  3.15310329e-01\n",
      "Epoch: 38705 mean train loss:  3.25716683e-03, bound:  3.15310329e-01\n",
      "Epoch: 38706 mean train loss:  3.25713283e-03, bound:  3.15310329e-01\n",
      "Epoch: 38707 mean train loss:  3.25703784e-03, bound:  3.15310329e-01\n",
      "Epoch: 38708 mean train loss:  3.25700780e-03, bound:  3.15310299e-01\n",
      "Epoch: 38709 mean train loss:  3.25693982e-03, bound:  3.15310299e-01\n",
      "Epoch: 38710 mean train loss:  3.25687509e-03, bound:  3.15310299e-01\n",
      "Epoch: 38711 mean train loss:  3.25681991e-03, bound:  3.15310299e-01\n",
      "Epoch: 38712 mean train loss:  3.25680524e-03, bound:  3.15310299e-01\n",
      "Epoch: 38713 mean train loss:  3.25675681e-03, bound:  3.15310270e-01\n",
      "Epoch: 38714 mean train loss:  3.25667066e-03, bound:  3.15310270e-01\n",
      "Epoch: 38715 mean train loss:  3.25663970e-03, bound:  3.15310270e-01\n",
      "Epoch: 38716 mean train loss:  3.25659732e-03, bound:  3.15310270e-01\n",
      "Epoch: 38717 mean train loss:  3.25654098e-03, bound:  3.15310270e-01\n",
      "Epoch: 38718 mean train loss:  3.25646461e-03, bound:  3.15310270e-01\n",
      "Epoch: 38719 mean train loss:  3.25642549e-03, bound:  3.15310270e-01\n",
      "Epoch: 38720 mean train loss:  3.25633702e-03, bound:  3.15310270e-01\n",
      "Epoch: 38721 mean train loss:  3.25631676e-03, bound:  3.15310210e-01\n",
      "Epoch: 38722 mean train loss:  3.25623341e-03, bound:  3.15310210e-01\n",
      "Epoch: 38723 mean train loss:  3.25617171e-03, bound:  3.15310210e-01\n",
      "Epoch: 38724 mean train loss:  3.25615774e-03, bound:  3.15310210e-01\n",
      "Epoch: 38725 mean train loss:  3.25604505e-03, bound:  3.15310210e-01\n",
      "Epoch: 38726 mean train loss:  3.25600826e-03, bound:  3.15310210e-01\n",
      "Epoch: 38727 mean train loss:  3.25597776e-03, bound:  3.15310210e-01\n",
      "Epoch: 38728 mean train loss:  3.25594586e-03, bound:  3.15310180e-01\n",
      "Epoch: 38729 mean train loss:  3.25585925e-03, bound:  3.15310180e-01\n",
      "Epoch: 38730 mean train loss:  3.25580570e-03, bound:  3.15310180e-01\n",
      "Epoch: 38731 mean train loss:  3.25569371e-03, bound:  3.15310180e-01\n",
      "Epoch: 38732 mean train loss:  3.25572374e-03, bound:  3.15310180e-01\n",
      "Epoch: 38733 mean train loss:  3.25565203e-03, bound:  3.15310180e-01\n",
      "Epoch: 38734 mean train loss:  3.25559475e-03, bound:  3.15310180e-01\n",
      "Epoch: 38735 mean train loss:  3.25553562e-03, bound:  3.15310180e-01\n",
      "Epoch: 38736 mean train loss:  3.25546786e-03, bound:  3.15310180e-01\n",
      "Epoch: 38737 mean train loss:  3.25540593e-03, bound:  3.15310180e-01\n",
      "Epoch: 38738 mean train loss:  3.25537333e-03, bound:  3.15310180e-01\n",
      "Epoch: 38739 mean train loss:  3.25531350e-03, bound:  3.15310180e-01\n",
      "Epoch: 38740 mean train loss:  3.25528951e-03, bound:  3.15310150e-01\n",
      "Epoch: 38741 mean train loss:  3.25521664e-03, bound:  3.15310091e-01\n",
      "Epoch: 38742 mean train loss:  3.25513654e-03, bound:  3.15310091e-01\n",
      "Epoch: 38743 mean train loss:  3.25510744e-03, bound:  3.15310091e-01\n",
      "Epoch: 38744 mean train loss:  3.25504504e-03, bound:  3.15310091e-01\n",
      "Epoch: 38745 mean train loss:  3.25495657e-03, bound:  3.15310091e-01\n",
      "Epoch: 38746 mean train loss:  3.25492234e-03, bound:  3.15310091e-01\n",
      "Epoch: 38747 mean train loss:  3.25484201e-03, bound:  3.15310091e-01\n",
      "Epoch: 38748 mean train loss:  3.25483852e-03, bound:  3.15310061e-01\n",
      "Epoch: 38749 mean train loss:  3.25477659e-03, bound:  3.15310061e-01\n",
      "Epoch: 38750 mean train loss:  3.25475656e-03, bound:  3.15310061e-01\n",
      "Epoch: 38751 mean train loss:  3.25465272e-03, bound:  3.15310061e-01\n",
      "Epoch: 38752 mean train loss:  3.25459661e-03, bound:  3.15310061e-01\n",
      "Epoch: 38753 mean train loss:  3.25456238e-03, bound:  3.15310061e-01\n",
      "Epoch: 38754 mean train loss:  3.25457798e-03, bound:  3.15310061e-01\n",
      "Epoch: 38755 mean train loss:  3.25447065e-03, bound:  3.15310061e-01\n",
      "Epoch: 38756 mean train loss:  3.25441197e-03, bound:  3.15310061e-01\n",
      "Epoch: 38757 mean train loss:  3.25432234e-03, bound:  3.15310061e-01\n",
      "Epoch: 38758 mean train loss:  3.25428555e-03, bound:  3.15310061e-01\n",
      "Epoch: 38759 mean train loss:  3.25422175e-03, bound:  3.15310031e-01\n",
      "Epoch: 38760 mean train loss:  3.25414422e-03, bound:  3.15309972e-01\n",
      "Epoch: 38761 mean train loss:  3.25415889e-03, bound:  3.15309972e-01\n",
      "Epoch: 38762 mean train loss:  3.25408904e-03, bound:  3.15309972e-01\n",
      "Epoch: 38763 mean train loss:  3.25400475e-03, bound:  3.15309972e-01\n",
      "Epoch: 38764 mean train loss:  3.25391698e-03, bound:  3.15309972e-01\n",
      "Epoch: 38765 mean train loss:  3.25395400e-03, bound:  3.15309972e-01\n",
      "Epoch: 38766 mean train loss:  3.25388974e-03, bound:  3.15309972e-01\n",
      "Epoch: 38767 mean train loss:  3.25378566e-03, bound:  3.15309972e-01\n",
      "Epoch: 38768 mean train loss:  3.25375935e-03, bound:  3.15309972e-01\n",
      "Epoch: 38769 mean train loss:  3.25366925e-03, bound:  3.15309972e-01\n",
      "Epoch: 38770 mean train loss:  3.25364922e-03, bound:  3.15309972e-01\n",
      "Epoch: 38771 mean train loss:  3.25358752e-03, bound:  3.15309972e-01\n",
      "Epoch: 38772 mean train loss:  3.25353607e-03, bound:  3.15309942e-01\n",
      "Epoch: 38773 mean train loss:  3.25349323e-03, bound:  3.15309942e-01\n",
      "Epoch: 38774 mean train loss:  3.25345830e-03, bound:  3.15309942e-01\n",
      "Epoch: 38775 mean train loss:  3.25335097e-03, bound:  3.15309942e-01\n",
      "Epoch: 38776 mean train loss:  3.25337239e-03, bound:  3.15309942e-01\n",
      "Epoch: 38777 mean train loss:  3.25326552e-03, bound:  3.15309942e-01\n",
      "Epoch: 38778 mean train loss:  3.25323455e-03, bound:  3.15309942e-01\n",
      "Epoch: 38779 mean train loss:  3.25316750e-03, bound:  3.15309882e-01\n",
      "Epoch: 38780 mean train loss:  3.25303013e-03, bound:  3.15309882e-01\n",
      "Epoch: 38781 mean train loss:  3.25307227e-03, bound:  3.15309882e-01\n",
      "Epoch: 38782 mean train loss:  3.25297867e-03, bound:  3.15309882e-01\n",
      "Epoch: 38783 mean train loss:  3.25295632e-03, bound:  3.15309882e-01\n",
      "Epoch: 38784 mean train loss:  3.25286062e-03, bound:  3.15309852e-01\n",
      "Epoch: 38785 mean train loss:  3.25281359e-03, bound:  3.15309852e-01\n",
      "Epoch: 38786 mean train loss:  3.25273280e-03, bound:  3.15309852e-01\n",
      "Epoch: 38787 mean train loss:  3.25272372e-03, bound:  3.15309852e-01\n",
      "Epoch: 38788 mean train loss:  3.25268507e-03, bound:  3.15309852e-01\n",
      "Epoch: 38789 mean train loss:  3.25263757e-03, bound:  3.15309852e-01\n",
      "Epoch: 38790 mean train loss:  3.25257611e-03, bound:  3.15309852e-01\n",
      "Epoch: 38791 mean train loss:  3.25244572e-03, bound:  3.15309852e-01\n",
      "Epoch: 38792 mean train loss:  3.25241243e-03, bound:  3.15309852e-01\n",
      "Epoch: 38793 mean train loss:  3.25239100e-03, bound:  3.15309823e-01\n",
      "Epoch: 38794 mean train loss:  3.25234281e-03, bound:  3.15309823e-01\n",
      "Epoch: 38795 mean train loss:  3.25232395e-03, bound:  3.15309823e-01\n",
      "Epoch: 38796 mean train loss:  3.25223408e-03, bound:  3.15309823e-01\n",
      "Epoch: 38797 mean train loss:  3.25218728e-03, bound:  3.15309823e-01\n",
      "Epoch: 38798 mean train loss:  3.25212511e-03, bound:  3.15309763e-01\n",
      "Epoch: 38799 mean train loss:  3.25209368e-03, bound:  3.15309763e-01\n",
      "Epoch: 38800 mean train loss:  3.25208623e-03, bound:  3.15309763e-01\n",
      "Epoch: 38801 mean train loss:  3.25199985e-03, bound:  3.15309763e-01\n",
      "Epoch: 38802 mean train loss:  3.25199170e-03, bound:  3.15309763e-01\n",
      "Epoch: 38803 mean train loss:  3.25197168e-03, bound:  3.15309763e-01\n",
      "Epoch: 38804 mean train loss:  3.25189531e-03, bound:  3.15309763e-01\n",
      "Epoch: 38805 mean train loss:  3.25187971e-03, bound:  3.15309733e-01\n",
      "Epoch: 38806 mean train loss:  3.25184548e-03, bound:  3.15309733e-01\n",
      "Epoch: 38807 mean train loss:  3.25176935e-03, bound:  3.15309733e-01\n",
      "Epoch: 38808 mean train loss:  3.25173070e-03, bound:  3.15309733e-01\n",
      "Epoch: 38809 mean train loss:  3.25165363e-03, bound:  3.15309703e-01\n",
      "Epoch: 38810 mean train loss:  3.25157796e-03, bound:  3.15309703e-01\n",
      "Epoch: 38811 mean train loss:  3.25148180e-03, bound:  3.15309703e-01\n",
      "Epoch: 38812 mean train loss:  3.25140776e-03, bound:  3.15309703e-01\n",
      "Epoch: 38813 mean train loss:  3.25129787e-03, bound:  3.15309703e-01\n",
      "Epoch: 38814 mean train loss:  3.25127086e-03, bound:  3.15309703e-01\n",
      "Epoch: 38815 mean train loss:  3.25120636e-03, bound:  3.15309703e-01\n",
      "Epoch: 38816 mean train loss:  3.25113628e-03, bound:  3.15309644e-01\n",
      "Epoch: 38817 mean train loss:  3.25111533e-03, bound:  3.15309644e-01\n",
      "Epoch: 38818 mean train loss:  3.25108063e-03, bound:  3.15309644e-01\n",
      "Epoch: 38819 mean train loss:  3.25099682e-03, bound:  3.15309644e-01\n",
      "Epoch: 38820 mean train loss:  3.25093721e-03, bound:  3.15309644e-01\n",
      "Epoch: 38821 mean train loss:  3.25092697e-03, bound:  3.15309644e-01\n",
      "Epoch: 38822 mean train loss:  3.25085362e-03, bound:  3.15309644e-01\n",
      "Epoch: 38823 mean train loss:  3.25077330e-03, bound:  3.15309614e-01\n",
      "Epoch: 38824 mean train loss:  3.25070973e-03, bound:  3.15309644e-01\n",
      "Epoch: 38825 mean train loss:  3.25069274e-03, bound:  3.15309614e-01\n",
      "Epoch: 38826 mean train loss:  3.25060636e-03, bound:  3.15309614e-01\n",
      "Epoch: 38827 mean train loss:  3.25060845e-03, bound:  3.15309614e-01\n",
      "Epoch: 38828 mean train loss:  3.25057865e-03, bound:  3.15309614e-01\n",
      "Epoch: 38829 mean train loss:  3.25044524e-03, bound:  3.15309584e-01\n",
      "Epoch: 38830 mean train loss:  3.25042242e-03, bound:  3.15309584e-01\n",
      "Epoch: 38831 mean train loss:  3.25034303e-03, bound:  3.15309584e-01\n",
      "Epoch: 38832 mean train loss:  3.25030996e-03, bound:  3.15309584e-01\n",
      "Epoch: 38833 mean train loss:  3.25023453e-03, bound:  3.15309584e-01\n",
      "Epoch: 38834 mean train loss:  3.25018028e-03, bound:  3.15309584e-01\n",
      "Epoch: 38835 mean train loss:  3.25012743e-03, bound:  3.15309584e-01\n",
      "Epoch: 38836 mean train loss:  3.25008156e-03, bound:  3.15309584e-01\n",
      "Epoch: 38837 mean train loss:  3.24998633e-03, bound:  3.15309525e-01\n",
      "Epoch: 38838 mean train loss:  3.24996375e-03, bound:  3.15309525e-01\n",
      "Epoch: 38839 mean train loss:  3.24989553e-03, bound:  3.15309525e-01\n",
      "Epoch: 38840 mean train loss:  3.24984360e-03, bound:  3.15309525e-01\n",
      "Epoch: 38841 mean train loss:  3.24982870e-03, bound:  3.15309525e-01\n",
      "Epoch: 38842 mean train loss:  3.24972603e-03, bound:  3.15309525e-01\n",
      "Epoch: 38843 mean train loss:  3.24969064e-03, bound:  3.15309525e-01\n",
      "Epoch: 38844 mean train loss:  3.24964523e-03, bound:  3.15309495e-01\n",
      "Epoch: 38845 mean train loss:  3.24960868e-03, bound:  3.15309495e-01\n",
      "Epoch: 38846 mean train loss:  3.24954581e-03, bound:  3.15309495e-01\n",
      "Epoch: 38847 mean train loss:  3.24949948e-03, bound:  3.15309495e-01\n",
      "Epoch: 38848 mean train loss:  3.24942358e-03, bound:  3.15309495e-01\n",
      "Epoch: 38849 mean train loss:  3.24940751e-03, bound:  3.15309435e-01\n",
      "Epoch: 38850 mean train loss:  3.24933301e-03, bound:  3.15309435e-01\n",
      "Epoch: 38851 mean train loss:  3.24929412e-03, bound:  3.15309435e-01\n",
      "Epoch: 38852 mean train loss:  3.24922707e-03, bound:  3.15309435e-01\n",
      "Epoch: 38853 mean train loss:  3.24914907e-03, bound:  3.15309435e-01\n",
      "Epoch: 38854 mean train loss:  3.24910739e-03, bound:  3.15309435e-01\n",
      "Epoch: 38855 mean train loss:  3.24905152e-03, bound:  3.15309435e-01\n",
      "Epoch: 38856 mean train loss:  3.24901007e-03, bound:  3.15309435e-01\n",
      "Epoch: 38857 mean train loss:  3.24899051e-03, bound:  3.15309405e-01\n",
      "Epoch: 38858 mean train loss:  3.24891065e-03, bound:  3.15309405e-01\n",
      "Epoch: 38859 mean train loss:  3.24880984e-03, bound:  3.15309405e-01\n",
      "Epoch: 38860 mean train loss:  3.24880239e-03, bound:  3.15309405e-01\n",
      "Epoch: 38861 mean train loss:  3.24872113e-03, bound:  3.15309405e-01\n",
      "Epoch: 38862 mean train loss:  3.24867177e-03, bound:  3.15309405e-01\n",
      "Epoch: 38863 mean train loss:  3.24865710e-03, bound:  3.15309376e-01\n",
      "Epoch: 38864 mean train loss:  3.24860169e-03, bound:  3.15309376e-01\n",
      "Epoch: 38865 mean train loss:  3.24854208e-03, bound:  3.15309376e-01\n",
      "Epoch: 38866 mean train loss:  3.24846059e-03, bound:  3.15309376e-01\n",
      "Epoch: 38867 mean train loss:  3.24838515e-03, bound:  3.15309376e-01\n",
      "Epoch: 38868 mean train loss:  3.24837514e-03, bound:  3.15309376e-01\n",
      "Epoch: 38869 mean train loss:  3.24832415e-03, bound:  3.15309376e-01\n",
      "Epoch: 38870 mean train loss:  3.24822916e-03, bound:  3.15309376e-01\n",
      "Epoch: 38871 mean train loss:  3.24820960e-03, bound:  3.15309376e-01\n",
      "Epoch: 38872 mean train loss:  3.24814371e-03, bound:  3.15309376e-01\n",
      "Epoch: 38873 mean train loss:  3.24809528e-03, bound:  3.15309376e-01\n",
      "Epoch: 38874 mean train loss:  3.24801891e-03, bound:  3.15309376e-01\n",
      "Epoch: 38875 mean train loss:  3.24792927e-03, bound:  3.15309376e-01\n",
      "Epoch: 38876 mean train loss:  3.24797188e-03, bound:  3.15309286e-01\n",
      "Epoch: 38877 mean train loss:  3.24789854e-03, bound:  3.15309286e-01\n",
      "Epoch: 38878 mean train loss:  3.24782613e-03, bound:  3.15309286e-01\n",
      "Epoch: 38879 mean train loss:  3.24777793e-03, bound:  3.15309286e-01\n",
      "Epoch: 38880 mean train loss:  3.24770948e-03, bound:  3.15309286e-01\n",
      "Epoch: 38881 mean train loss:  3.24766128e-03, bound:  3.15309286e-01\n",
      "Epoch: 38882 mean train loss:  3.24761472e-03, bound:  3.15309286e-01\n",
      "Epoch: 38883 mean train loss:  3.24757746e-03, bound:  3.15309256e-01\n",
      "Epoch: 38884 mean train loss:  3.24749551e-03, bound:  3.15309256e-01\n",
      "Epoch: 38885 mean train loss:  3.24744033e-03, bound:  3.15309256e-01\n",
      "Epoch: 38886 mean train loss:  3.24739027e-03, bound:  3.15309256e-01\n",
      "Epoch: 38887 mean train loss:  3.24736279e-03, bound:  3.15309256e-01\n",
      "Epoch: 38888 mean train loss:  3.24727269e-03, bound:  3.15309256e-01\n",
      "Epoch: 38889 mean train loss:  3.24725755e-03, bound:  3.15309256e-01\n",
      "Epoch: 38890 mean train loss:  3.24719050e-03, bound:  3.15309256e-01\n",
      "Epoch: 38891 mean train loss:  3.24711436e-03, bound:  3.15309256e-01\n",
      "Epoch: 38892 mean train loss:  3.24707921e-03, bound:  3.15309256e-01\n",
      "Epoch: 38893 mean train loss:  3.24699772e-03, bound:  3.15309256e-01\n",
      "Epoch: 38894 mean train loss:  3.24694905e-03, bound:  3.15309256e-01\n",
      "Epoch: 38895 mean train loss:  3.24692740e-03, bound:  3.15309197e-01\n",
      "Epoch: 38896 mean train loss:  3.24683823e-03, bound:  3.15309197e-01\n",
      "Epoch: 38897 mean train loss:  3.24679632e-03, bound:  3.15309197e-01\n",
      "Epoch: 38898 mean train loss:  3.24672880e-03, bound:  3.15309197e-01\n",
      "Epoch: 38899 mean train loss:  3.24669667e-03, bound:  3.15309197e-01\n",
      "Epoch: 38900 mean train loss:  3.24662006e-03, bound:  3.15309197e-01\n",
      "Epoch: 38901 mean train loss:  3.24657117e-03, bound:  3.15309167e-01\n",
      "Epoch: 38902 mean train loss:  3.24653182e-03, bound:  3.15309137e-01\n",
      "Epoch: 38903 mean train loss:  3.24644637e-03, bound:  3.15309137e-01\n",
      "Epoch: 38904 mean train loss:  3.24639864e-03, bound:  3.15309137e-01\n",
      "Epoch: 38905 mean train loss:  3.24637955e-03, bound:  3.15309137e-01\n",
      "Epoch: 38906 mean train loss:  3.24626849e-03, bound:  3.15309137e-01\n",
      "Epoch: 38907 mean train loss:  3.24624847e-03, bound:  3.15309137e-01\n",
      "Epoch: 38908 mean train loss:  3.24619166e-03, bound:  3.15309137e-01\n",
      "Epoch: 38909 mean train loss:  3.24615859e-03, bound:  3.15309137e-01\n",
      "Epoch: 38910 mean train loss:  3.24610434e-03, bound:  3.15309137e-01\n",
      "Epoch: 38911 mean train loss:  3.24602565e-03, bound:  3.15309137e-01\n",
      "Epoch: 38912 mean train loss:  3.24599235e-03, bound:  3.15309137e-01\n",
      "Epoch: 38913 mean train loss:  3.24597023e-03, bound:  3.15309137e-01\n",
      "Epoch: 38914 mean train loss:  3.24585894e-03, bound:  3.15309137e-01\n",
      "Epoch: 38915 mean train loss:  3.24581377e-03, bound:  3.15309078e-01\n",
      "Epoch: 38916 mean train loss:  3.24576977e-03, bound:  3.15309078e-01\n",
      "Epoch: 38917 mean train loss:  3.24573438e-03, bound:  3.15309078e-01\n",
      "Epoch: 38918 mean train loss:  3.24567314e-03, bound:  3.15309078e-01\n",
      "Epoch: 38919 mean train loss:  3.24562727e-03, bound:  3.15309078e-01\n",
      "Epoch: 38920 mean train loss:  3.24557628e-03, bound:  3.15309048e-01\n",
      "Epoch: 38921 mean train loss:  3.24553857e-03, bound:  3.15309048e-01\n",
      "Epoch: 38922 mean train loss:  3.24542774e-03, bound:  3.15309048e-01\n",
      "Epoch: 38923 mean train loss:  3.24539281e-03, bound:  3.15309048e-01\n",
      "Epoch: 38924 mean train loss:  3.24537442e-03, bound:  3.15309048e-01\n",
      "Epoch: 38925 mean train loss:  3.24526778e-03, bound:  3.15309048e-01\n",
      "Epoch: 38926 mean train loss:  3.24524404e-03, bound:  3.15309048e-01\n",
      "Epoch: 38927 mean train loss:  3.24521912e-03, bound:  3.15309018e-01\n",
      "Epoch: 38928 mean train loss:  3.24514019e-03, bound:  3.15309018e-01\n",
      "Epoch: 38929 mean train loss:  3.24507966e-03, bound:  3.15309018e-01\n",
      "Epoch: 38930 mean train loss:  3.24500375e-03, bound:  3.15309018e-01\n",
      "Epoch: 38931 mean train loss:  3.24496697e-03, bound:  3.15309018e-01\n",
      "Epoch: 38932 mean train loss:  3.24490131e-03, bound:  3.15309018e-01\n",
      "Epoch: 38933 mean train loss:  3.24483868e-03, bound:  3.15308958e-01\n",
      "Epoch: 38934 mean train loss:  3.24479816e-03, bound:  3.15308958e-01\n",
      "Epoch: 38935 mean train loss:  3.24472319e-03, bound:  3.15308958e-01\n",
      "Epoch: 38936 mean train loss:  3.24471202e-03, bound:  3.15308958e-01\n",
      "Epoch: 38937 mean train loss:  3.24463332e-03, bound:  3.15308958e-01\n",
      "Epoch: 38938 mean train loss:  3.24462214e-03, bound:  3.15308958e-01\n",
      "Epoch: 38939 mean train loss:  3.24456254e-03, bound:  3.15308958e-01\n",
      "Epoch: 38940 mean train loss:  3.24454578e-03, bound:  3.15308928e-01\n",
      "Epoch: 38941 mean train loss:  3.24454834e-03, bound:  3.15308928e-01\n",
      "Epoch: 38942 mean train loss:  3.24450177e-03, bound:  3.15308928e-01\n",
      "Epoch: 38943 mean train loss:  3.24452878e-03, bound:  3.15308928e-01\n",
      "Epoch: 38944 mean train loss:  3.24447872e-03, bound:  3.15308928e-01\n",
      "Epoch: 38945 mean train loss:  3.24436184e-03, bound:  3.15308928e-01\n",
      "Epoch: 38946 mean train loss:  3.24430899e-03, bound:  3.15308928e-01\n",
      "Epoch: 38947 mean train loss:  3.24418163e-03, bound:  3.15308869e-01\n",
      "Epoch: 38948 mean train loss:  3.24406102e-03, bound:  3.15308869e-01\n",
      "Epoch: 38949 mean train loss:  3.24397278e-03, bound:  3.15308869e-01\n",
      "Epoch: 38950 mean train loss:  3.24394926e-03, bound:  3.15308869e-01\n",
      "Epoch: 38951 mean train loss:  3.24392389e-03, bound:  3.15308869e-01\n",
      "Epoch: 38952 mean train loss:  3.24387033e-03, bound:  3.15308839e-01\n",
      "Epoch: 38953 mean train loss:  3.24384798e-03, bound:  3.15308839e-01\n",
      "Epoch: 38954 mean train loss:  3.24378419e-03, bound:  3.15308839e-01\n",
      "Epoch: 38955 mean train loss:  3.24367732e-03, bound:  3.15308839e-01\n",
      "Epoch: 38956 mean train loss:  3.24361166e-03, bound:  3.15308839e-01\n",
      "Epoch: 38957 mean train loss:  3.24354833e-03, bound:  3.15308839e-01\n",
      "Epoch: 38958 mean train loss:  3.24350107e-03, bound:  3.15308839e-01\n",
      "Epoch: 38959 mean train loss:  3.24351504e-03, bound:  3.15308839e-01\n",
      "Epoch: 38960 mean train loss:  3.24340817e-03, bound:  3.15308839e-01\n",
      "Epoch: 38961 mean train loss:  3.24337184e-03, bound:  3.15308809e-01\n",
      "Epoch: 38962 mean train loss:  3.24333599e-03, bound:  3.15308809e-01\n",
      "Epoch: 38963 mean train loss:  3.24328849e-03, bound:  3.15308809e-01\n",
      "Epoch: 38964 mean train loss:  3.24321142e-03, bound:  3.15308809e-01\n",
      "Epoch: 38965 mean train loss:  3.24316742e-03, bound:  3.15308809e-01\n",
      "Epoch: 38966 mean train loss:  3.24310013e-03, bound:  3.15308809e-01\n",
      "Epoch: 38967 mean train loss:  3.24305566e-03, bound:  3.15308750e-01\n",
      "Epoch: 38968 mean train loss:  3.24299606e-03, bound:  3.15308750e-01\n",
      "Epoch: 38969 mean train loss:  3.24299256e-03, bound:  3.15308750e-01\n",
      "Epoch: 38970 mean train loss:  3.24290246e-03, bound:  3.15308750e-01\n",
      "Epoch: 38971 mean train loss:  3.24280979e-03, bound:  3.15308750e-01\n",
      "Epoch: 38972 mean train loss:  3.24273831e-03, bound:  3.15308720e-01\n",
      "Epoch: 38973 mean train loss:  3.24273086e-03, bound:  3.15308720e-01\n",
      "Epoch: 38974 mean train loss:  3.24264821e-03, bound:  3.15308720e-01\n",
      "Epoch: 38975 mean train loss:  3.24262888e-03, bound:  3.15308720e-01\n",
      "Epoch: 38976 mean train loss:  3.24253016e-03, bound:  3.15308720e-01\n",
      "Epoch: 38977 mean train loss:  3.24252946e-03, bound:  3.15308720e-01\n",
      "Epoch: 38978 mean train loss:  3.24246264e-03, bound:  3.15308720e-01\n",
      "Epoch: 38979 mean train loss:  3.24240350e-03, bound:  3.15308720e-01\n",
      "Epoch: 38980 mean train loss:  3.24238767e-03, bound:  3.15308690e-01\n",
      "Epoch: 38981 mean train loss:  3.24228988e-03, bound:  3.15308690e-01\n",
      "Epoch: 38982 mean train loss:  3.24224774e-03, bound:  3.15308690e-01\n",
      "Epoch: 38983 mean train loss:  3.24218534e-03, bound:  3.15308690e-01\n",
      "Epoch: 38984 mean train loss:  3.24214622e-03, bound:  3.15308690e-01\n",
      "Epoch: 38985 mean train loss:  3.24209360e-03, bound:  3.15308690e-01\n",
      "Epoch: 38986 mean train loss:  3.24205472e-03, bound:  3.15308630e-01\n",
      "Epoch: 38987 mean train loss:  3.24198045e-03, bound:  3.15308630e-01\n",
      "Epoch: 38988 mean train loss:  3.24192224e-03, bound:  3.15308630e-01\n",
      "Epoch: 38989 mean train loss:  3.24188406e-03, bound:  3.15308630e-01\n",
      "Epoch: 38990 mean train loss:  3.24179861e-03, bound:  3.15308630e-01\n",
      "Epoch: 38991 mean train loss:  3.24176741e-03, bound:  3.15308630e-01\n",
      "Epoch: 38992 mean train loss:  3.24171106e-03, bound:  3.15308630e-01\n",
      "Epoch: 38993 mean train loss:  3.24168988e-03, bound:  3.15308630e-01\n",
      "Epoch: 38994 mean train loss:  3.24159395e-03, bound:  3.15308601e-01\n",
      "Epoch: 38995 mean train loss:  3.24158324e-03, bound:  3.15308601e-01\n",
      "Epoch: 38996 mean train loss:  3.24150501e-03, bound:  3.15308601e-01\n",
      "Epoch: 38997 mean train loss:  3.24147008e-03, bound:  3.15308601e-01\n",
      "Epoch: 38998 mean train loss:  3.24141467e-03, bound:  3.15308601e-01\n",
      "Epoch: 38999 mean train loss:  3.24128172e-03, bound:  3.15308571e-01\n",
      "Epoch: 39000 mean train loss:  3.24128149e-03, bound:  3.15308601e-01\n",
      "Epoch: 39001 mean train loss:  3.24125239e-03, bound:  3.15308571e-01\n",
      "Epoch: 39002 mean train loss:  3.24117858e-03, bound:  3.15308571e-01\n",
      "Epoch: 39003 mean train loss:  3.24110081e-03, bound:  3.15308571e-01\n",
      "Epoch: 39004 mean train loss:  3.24103935e-03, bound:  3.15308571e-01\n",
      "Epoch: 39005 mean train loss:  3.24099394e-03, bound:  3.15308571e-01\n",
      "Epoch: 39006 mean train loss:  3.24091129e-03, bound:  3.15308571e-01\n",
      "Epoch: 39007 mean train loss:  3.24087916e-03, bound:  3.15308571e-01\n",
      "Epoch: 39008 mean train loss:  3.24084400e-03, bound:  3.15308571e-01\n",
      "Epoch: 39009 mean train loss:  3.24080326e-03, bound:  3.15308571e-01\n",
      "Epoch: 39010 mean train loss:  3.24072782e-03, bound:  3.15308571e-01\n",
      "Epoch: 39011 mean train loss:  3.24073527e-03, bound:  3.15308571e-01\n",
      "Epoch: 39012 mean train loss:  3.24066239e-03, bound:  3.15308511e-01\n",
      "Epoch: 39013 mean train loss:  3.24056181e-03, bound:  3.15308511e-01\n",
      "Epoch: 39014 mean train loss:  3.24054901e-03, bound:  3.15308511e-01\n",
      "Epoch: 39015 mean train loss:  3.24048894e-03, bound:  3.15308511e-01\n",
      "Epoch: 39016 mean train loss:  3.24042863e-03, bound:  3.15308511e-01\n",
      "Epoch: 39017 mean train loss:  3.24038323e-03, bound:  3.15308511e-01\n",
      "Epoch: 39018 mean train loss:  3.24031827e-03, bound:  3.15308481e-01\n",
      "Epoch: 39019 mean train loss:  3.24030383e-03, bound:  3.15308452e-01\n",
      "Epoch: 39020 mean train loss:  3.24021513e-03, bound:  3.15308452e-01\n",
      "Epoch: 39021 mean train loss:  3.24017298e-03, bound:  3.15308452e-01\n",
      "Epoch: 39022 mean train loss:  3.24012828e-03, bound:  3.15308452e-01\n",
      "Epoch: 39023 mean train loss:  3.24005587e-03, bound:  3.15308452e-01\n",
      "Epoch: 39024 mean train loss:  3.24001699e-03, bound:  3.15308452e-01\n",
      "Epoch: 39025 mean train loss:  3.23996390e-03, bound:  3.15308422e-01\n",
      "Epoch: 39026 mean train loss:  3.23991966e-03, bound:  3.15308422e-01\n",
      "Epoch: 39027 mean train loss:  3.23987869e-03, bound:  3.15308422e-01\n",
      "Epoch: 39028 mean train loss:  3.23982933e-03, bound:  3.15308422e-01\n",
      "Epoch: 39029 mean train loss:  3.23977275e-03, bound:  3.15308422e-01\n",
      "Epoch: 39030 mean train loss:  3.23970569e-03, bound:  3.15308422e-01\n",
      "Epoch: 39031 mean train loss:  3.23967868e-03, bound:  3.15308392e-01\n",
      "Epoch: 39032 mean train loss:  3.23968171e-03, bound:  3.15308392e-01\n",
      "Epoch: 39033 mean train loss:  3.23961838e-03, bound:  3.15308392e-01\n",
      "Epoch: 39034 mean train loss:  3.23948567e-03, bound:  3.15308392e-01\n",
      "Epoch: 39035 mean train loss:  3.23941698e-03, bound:  3.15308392e-01\n",
      "Epoch: 39036 mean train loss:  3.23935365e-03, bound:  3.15308392e-01\n",
      "Epoch: 39037 mean train loss:  3.23929708e-03, bound:  3.15308362e-01\n",
      "Epoch: 39038 mean train loss:  3.23929102e-03, bound:  3.15308362e-01\n",
      "Epoch: 39039 mean train loss:  3.23923002e-03, bound:  3.15308362e-01\n",
      "Epoch: 39040 mean train loss:  3.23914271e-03, bound:  3.15308332e-01\n",
      "Epoch: 39041 mean train loss:  3.23911826e-03, bound:  3.15308332e-01\n",
      "Epoch: 39042 mean train loss:  3.23906774e-03, bound:  3.15308332e-01\n",
      "Epoch: 39043 mean train loss:  3.23903724e-03, bound:  3.15308332e-01\n",
      "Epoch: 39044 mean train loss:  3.23895505e-03, bound:  3.15308332e-01\n",
      "Epoch: 39045 mean train loss:  3.23891477e-03, bound:  3.15308332e-01\n",
      "Epoch: 39046 mean train loss:  3.23886238e-03, bound:  3.15308332e-01\n",
      "Epoch: 39047 mean train loss:  3.23881791e-03, bound:  3.15308303e-01\n",
      "Epoch: 39048 mean train loss:  3.23875714e-03, bound:  3.15308303e-01\n",
      "Epoch: 39049 mean train loss:  3.23868147e-03, bound:  3.15308303e-01\n",
      "Epoch: 39050 mean train loss:  3.23862233e-03, bound:  3.15308303e-01\n",
      "Epoch: 39051 mean train loss:  3.23855435e-03, bound:  3.15308303e-01\n",
      "Epoch: 39052 mean train loss:  3.23856366e-03, bound:  3.15308273e-01\n",
      "Epoch: 39053 mean train loss:  3.23847961e-03, bound:  3.15308273e-01\n",
      "Epoch: 39054 mean train loss:  3.23841465e-03, bound:  3.15308273e-01\n",
      "Epoch: 39055 mean train loss:  3.23836552e-03, bound:  3.15308273e-01\n",
      "Epoch: 39056 mean train loss:  3.23830638e-03, bound:  3.15308273e-01\n",
      "Epoch: 39057 mean train loss:  3.23823490e-03, bound:  3.15308273e-01\n",
      "Epoch: 39058 mean train loss:  3.23819229e-03, bound:  3.15308243e-01\n",
      "Epoch: 39059 mean train loss:  3.23817343e-03, bound:  3.15308243e-01\n",
      "Epoch: 39060 mean train loss:  3.23803909e-03, bound:  3.15308243e-01\n",
      "Epoch: 39061 mean train loss:  3.23804584e-03, bound:  3.15308243e-01\n",
      "Epoch: 39062 mean train loss:  3.23803490e-03, bound:  3.15308243e-01\n",
      "Epoch: 39063 mean train loss:  3.23794037e-03, bound:  3.15308243e-01\n",
      "Epoch: 39064 mean train loss:  3.23790591e-03, bound:  3.15308243e-01\n",
      "Epoch: 39065 mean train loss:  3.23789311e-03, bound:  3.15308243e-01\n",
      "Epoch: 39066 mean train loss:  3.23779229e-03, bound:  3.15308183e-01\n",
      "Epoch: 39067 mean train loss:  3.23771709e-03, bound:  3.15308183e-01\n",
      "Epoch: 39068 mean train loss:  3.23768170e-03, bound:  3.15308183e-01\n",
      "Epoch: 39069 mean train loss:  3.23761534e-03, bound:  3.15308183e-01\n",
      "Epoch: 39070 mean train loss:  3.23761045e-03, bound:  3.15308183e-01\n",
      "Epoch: 39071 mean train loss:  3.23752686e-03, bound:  3.15308154e-01\n",
      "Epoch: 39072 mean train loss:  3.23744118e-03, bound:  3.15308154e-01\n",
      "Epoch: 39073 mean train loss:  3.23740812e-03, bound:  3.15308154e-01\n",
      "Epoch: 39074 mean train loss:  3.23734130e-03, bound:  3.15308154e-01\n",
      "Epoch: 39075 mean train loss:  3.23728612e-03, bound:  3.15308154e-01\n",
      "Epoch: 39076 mean train loss:  3.23726935e-03, bound:  3.15308154e-01\n",
      "Epoch: 39077 mean train loss:  3.23719974e-03, bound:  3.15308154e-01\n",
      "Epoch: 39078 mean train loss:  3.23714130e-03, bound:  3.15308154e-01\n",
      "Epoch: 39079 mean train loss:  3.23709217e-03, bound:  3.15308124e-01\n",
      "Epoch: 39080 mean train loss:  3.23698181e-03, bound:  3.15308124e-01\n",
      "Epoch: 39081 mean train loss:  3.23701301e-03, bound:  3.15308124e-01\n",
      "Epoch: 39082 mean train loss:  3.23688961e-03, bound:  3.15308124e-01\n",
      "Epoch: 39083 mean train loss:  3.23683885e-03, bound:  3.15308124e-01\n",
      "Epoch: 39084 mean train loss:  3.23684537e-03, bound:  3.15308124e-01\n",
      "Epoch: 39085 mean train loss:  3.23670870e-03, bound:  3.15308064e-01\n",
      "Epoch: 39086 mean train loss:  3.23670800e-03, bound:  3.15308064e-01\n",
      "Epoch: 39087 mean train loss:  3.23668192e-03, bound:  3.15308064e-01\n",
      "Epoch: 39088 mean train loss:  3.23660672e-03, bound:  3.15308064e-01\n",
      "Epoch: 39089 mean train loss:  3.23658390e-03, bound:  3.15308064e-01\n",
      "Epoch: 39090 mean train loss:  3.23651545e-03, bound:  3.15308034e-01\n",
      "Epoch: 39091 mean train loss:  3.23644863e-03, bound:  3.15308034e-01\n",
      "Epoch: 39092 mean train loss:  3.23638809e-03, bound:  3.15308034e-01\n",
      "Epoch: 39093 mean train loss:  3.23636271e-03, bound:  3.15308034e-01\n",
      "Epoch: 39094 mean train loss:  3.23631614e-03, bound:  3.15308034e-01\n",
      "Epoch: 39095 mean train loss:  3.23621999e-03, bound:  3.15308034e-01\n",
      "Epoch: 39096 mean train loss:  3.23619507e-03, bound:  3.15308034e-01\n",
      "Epoch: 39097 mean train loss:  3.23610008e-03, bound:  3.15308034e-01\n",
      "Epoch: 39098 mean train loss:  3.23604979e-03, bound:  3.15308005e-01\n",
      "Epoch: 39099 mean train loss:  3.23601323e-03, bound:  3.15308005e-01\n",
      "Epoch: 39100 mean train loss:  3.23597342e-03, bound:  3.15308005e-01\n",
      "Epoch: 39101 mean train loss:  3.23588657e-03, bound:  3.15308005e-01\n",
      "Epoch: 39102 mean train loss:  3.23587726e-03, bound:  3.15307975e-01\n",
      "Epoch: 39103 mean train loss:  3.23584699e-03, bound:  3.15307975e-01\n",
      "Epoch: 39104 mean train loss:  3.23572080e-03, bound:  3.15307945e-01\n",
      "Epoch: 39105 mean train loss:  3.23570264e-03, bound:  3.15307945e-01\n",
      "Epoch: 39106 mean train loss:  3.23568983e-03, bound:  3.15307945e-01\n",
      "Epoch: 39107 mean train loss:  3.23561276e-03, bound:  3.15307945e-01\n",
      "Epoch: 39108 mean train loss:  3.23555549e-03, bound:  3.15307945e-01\n",
      "Epoch: 39109 mean train loss:  3.23548354e-03, bound:  3.15307945e-01\n",
      "Epoch: 39110 mean train loss:  3.23546841e-03, bound:  3.15307945e-01\n",
      "Epoch: 39111 mean train loss:  3.23542021e-03, bound:  3.15307945e-01\n",
      "Epoch: 39112 mean train loss:  3.23537202e-03, bound:  3.15307945e-01\n",
      "Epoch: 39113 mean train loss:  3.23530869e-03, bound:  3.15307945e-01\n",
      "Epoch: 39114 mean train loss:  3.23524815e-03, bound:  3.15307945e-01\n",
      "Epoch: 39115 mean train loss:  3.23518785e-03, bound:  3.15307945e-01\n",
      "Epoch: 39116 mean train loss:  3.23517225e-03, bound:  3.15307945e-01\n",
      "Epoch: 39117 mean train loss:  3.23512522e-03, bound:  3.15307915e-01\n",
      "Epoch: 39118 mean train loss:  3.23504955e-03, bound:  3.15307915e-01\n",
      "Epoch: 39119 mean train loss:  3.23505024e-03, bound:  3.15307885e-01\n",
      "Epoch: 39120 mean train loss:  3.23499250e-03, bound:  3.15307885e-01\n",
      "Epoch: 39121 mean train loss:  3.23488121e-03, bound:  3.15307885e-01\n",
      "Epoch: 39122 mean train loss:  3.23479297e-03, bound:  3.15307856e-01\n",
      "Epoch: 39123 mean train loss:  3.23471264e-03, bound:  3.15307826e-01\n",
      "Epoch: 39124 mean train loss:  3.23464721e-03, bound:  3.15307826e-01\n",
      "Epoch: 39125 mean train loss:  3.23463115e-03, bound:  3.15307826e-01\n",
      "Epoch: 39126 mean train loss:  3.23460624e-03, bound:  3.15307826e-01\n",
      "Epoch: 39127 mean train loss:  3.23464163e-03, bound:  3.15307826e-01\n",
      "Epoch: 39128 mean train loss:  3.23455199e-03, bound:  3.15307826e-01\n",
      "Epoch: 39129 mean train loss:  3.23451031e-03, bound:  3.15307826e-01\n",
      "Epoch: 39130 mean train loss:  3.23441927e-03, bound:  3.15307826e-01\n",
      "Epoch: 39131 mean train loss:  3.23436805e-03, bound:  3.15307826e-01\n",
      "Epoch: 39132 mean train loss:  3.23426048e-03, bound:  3.15307826e-01\n",
      "Epoch: 39133 mean train loss:  3.23418528e-03, bound:  3.15307826e-01\n",
      "Epoch: 39134 mean train loss:  3.23413149e-03, bound:  3.15307796e-01\n",
      "Epoch: 39135 mean train loss:  3.23412311e-03, bound:  3.15307796e-01\n",
      "Epoch: 39136 mean train loss:  3.23409890e-03, bound:  3.15307826e-01\n",
      "Epoch: 39137 mean train loss:  3.23404302e-03, bound:  3.15307766e-01\n",
      "Epoch: 39138 mean train loss:  3.23398248e-03, bound:  3.15307766e-01\n",
      "Epoch: 39139 mean train loss:  3.23393103e-03, bound:  3.15307736e-01\n",
      "Epoch: 39140 mean train loss:  3.23382649e-03, bound:  3.15307766e-01\n",
      "Epoch: 39141 mean train loss:  3.23377154e-03, bound:  3.15307766e-01\n",
      "Epoch: 39142 mean train loss:  3.23372078e-03, bound:  3.15307736e-01\n",
      "Epoch: 39143 mean train loss:  3.23368167e-03, bound:  3.15307736e-01\n",
      "Epoch: 39144 mean train loss:  3.23366211e-03, bound:  3.15307736e-01\n",
      "Epoch: 39145 mean train loss:  3.23353964e-03, bound:  3.15307736e-01\n",
      "Epoch: 39146 mean train loss:  3.23350937e-03, bound:  3.15307736e-01\n",
      "Epoch: 39147 mean train loss:  3.23347957e-03, bound:  3.15307736e-01\n",
      "Epoch: 39148 mean train loss:  3.23340273e-03, bound:  3.15307707e-01\n",
      "Epoch: 39149 mean train loss:  3.23337037e-03, bound:  3.15307707e-01\n",
      "Epoch: 39150 mean train loss:  3.23332357e-03, bound:  3.15307707e-01\n",
      "Epoch: 39151 mean train loss:  3.23326117e-03, bound:  3.15307707e-01\n",
      "Epoch: 39152 mean train loss:  3.23325861e-03, bound:  3.15307707e-01\n",
      "Epoch: 39153 mean train loss:  3.23318574e-03, bound:  3.15307707e-01\n",
      "Epoch: 39154 mean train loss:  3.23319528e-03, bound:  3.15307707e-01\n",
      "Epoch: 39155 mean train loss:  3.23304813e-03, bound:  3.15307647e-01\n",
      "Epoch: 39156 mean train loss:  3.23295966e-03, bound:  3.15307647e-01\n",
      "Epoch: 39157 mean train loss:  3.23295011e-03, bound:  3.15307647e-01\n",
      "Epoch: 39158 mean train loss:  3.23289447e-03, bound:  3.15307647e-01\n",
      "Epoch: 39159 mean train loss:  3.23281134e-03, bound:  3.15307647e-01\n",
      "Epoch: 39160 mean train loss:  3.23277083e-03, bound:  3.15307647e-01\n",
      "Epoch: 39161 mean train loss:  3.23272962e-03, bound:  3.15307617e-01\n",
      "Epoch: 39162 mean train loss:  3.23261670e-03, bound:  3.15307617e-01\n",
      "Epoch: 39163 mean train loss:  3.23259365e-03, bound:  3.15307617e-01\n",
      "Epoch: 39164 mean train loss:  3.23258620e-03, bound:  3.15307617e-01\n",
      "Epoch: 39165 mean train loss:  3.23251681e-03, bound:  3.15307617e-01\n",
      "Epoch: 39166 mean train loss:  3.23244953e-03, bound:  3.15307617e-01\n",
      "Epoch: 39167 mean train loss:  3.23239900e-03, bound:  3.15307587e-01\n",
      "Epoch: 39168 mean train loss:  3.23234755e-03, bound:  3.15307587e-01\n",
      "Epoch: 39169 mean train loss:  3.23229469e-03, bound:  3.15307587e-01\n",
      "Epoch: 39170 mean train loss:  3.23224301e-03, bound:  3.15307587e-01\n",
      "Epoch: 39171 mean train loss:  3.23215872e-03, bound:  3.15307587e-01\n",
      "Epoch: 39172 mean train loss:  3.23214359e-03, bound:  3.15307587e-01\n",
      "Epoch: 39173 mean train loss:  3.23208724e-03, bound:  3.15307587e-01\n",
      "Epoch: 39174 mean train loss:  3.23202321e-03, bound:  3.15307558e-01\n",
      "Epoch: 39175 mean train loss:  3.23199574e-03, bound:  3.15307558e-01\n",
      "Epoch: 39176 mean train loss:  3.23194358e-03, bound:  3.15307558e-01\n",
      "Epoch: 39177 mean train loss:  3.23191076e-03, bound:  3.15307558e-01\n",
      "Epoch: 39178 mean train loss:  3.23188514e-03, bound:  3.15307558e-01\n",
      "Epoch: 39179 mean train loss:  3.23176454e-03, bound:  3.15307558e-01\n",
      "Epoch: 39180 mean train loss:  3.23173753e-03, bound:  3.15307558e-01\n",
      "Epoch: 39181 mean train loss:  3.23164836e-03, bound:  3.15307558e-01\n",
      "Epoch: 39182 mean train loss:  3.23163928e-03, bound:  3.15307558e-01\n",
      "Epoch: 39183 mean train loss:  3.23157781e-03, bound:  3.15307498e-01\n",
      "Epoch: 39184 mean train loss:  3.23150493e-03, bound:  3.15307498e-01\n",
      "Epoch: 39185 mean train loss:  3.23146954e-03, bound:  3.15307498e-01\n",
      "Epoch: 39186 mean train loss:  3.23138572e-03, bound:  3.15307498e-01\n",
      "Epoch: 39187 mean train loss:  3.23135057e-03, bound:  3.15307498e-01\n",
      "Epoch: 39188 mean train loss:  3.23130563e-03, bound:  3.15307468e-01\n",
      "Epoch: 39189 mean train loss:  3.23123112e-03, bound:  3.15307468e-01\n",
      "Epoch: 39190 mean train loss:  3.23117920e-03, bound:  3.15307468e-01\n",
      "Epoch: 39191 mean train loss:  3.23110889e-03, bound:  3.15307468e-01\n",
      "Epoch: 39192 mean train loss:  3.23110609e-03, bound:  3.15307468e-01\n",
      "Epoch: 39193 mean train loss:  3.23107583e-03, bound:  3.15307468e-01\n",
      "Epoch: 39194 mean train loss:  3.23098828e-03, bound:  3.15307438e-01\n",
      "Epoch: 39195 mean train loss:  3.23092192e-03, bound:  3.15307438e-01\n",
      "Epoch: 39196 mean train loss:  3.23088560e-03, bound:  3.15307438e-01\n",
      "Epoch: 39197 mean train loss:  3.23081668e-03, bound:  3.15307438e-01\n",
      "Epoch: 39198 mean train loss:  3.23078944e-03, bound:  3.15307409e-01\n",
      "Epoch: 39199 mean train loss:  3.23073589e-03, bound:  3.15307409e-01\n",
      "Epoch: 39200 mean train loss:  3.23066209e-03, bound:  3.15307409e-01\n",
      "Epoch: 39201 mean train loss:  3.23060597e-03, bound:  3.15307379e-01\n",
      "Epoch: 39202 mean train loss:  3.23058059e-03, bound:  3.15307379e-01\n",
      "Epoch: 39203 mean train loss:  3.23052355e-03, bound:  3.15307379e-01\n",
      "Epoch: 39204 mean train loss:  3.23047861e-03, bound:  3.15307379e-01\n",
      "Epoch: 39205 mean train loss:  3.23041272e-03, bound:  3.15307379e-01\n",
      "Epoch: 39206 mean train loss:  3.23037151e-03, bound:  3.15307349e-01\n",
      "Epoch: 39207 mean train loss:  3.23030911e-03, bound:  3.15307349e-01\n",
      "Epoch: 39208 mean train loss:  3.23027908e-03, bound:  3.15307349e-01\n",
      "Epoch: 39209 mean train loss:  3.23020574e-03, bound:  3.15307349e-01\n",
      "Epoch: 39210 mean train loss:  3.23017291e-03, bound:  3.15307349e-01\n",
      "Epoch: 39211 mean train loss:  3.23009281e-03, bound:  3.15307349e-01\n",
      "Epoch: 39212 mean train loss:  3.23006907e-03, bound:  3.15307319e-01\n",
      "Epoch: 39213 mean train loss:  3.22997198e-03, bound:  3.15307319e-01\n",
      "Epoch: 39214 mean train loss:  3.22992192e-03, bound:  3.15307319e-01\n",
      "Epoch: 39215 mean train loss:  3.22990119e-03, bound:  3.15307319e-01\n",
      "Epoch: 39216 mean train loss:  3.22982692e-03, bound:  3.15307319e-01\n",
      "Epoch: 39217 mean train loss:  3.22980178e-03, bound:  3.15307319e-01\n",
      "Epoch: 39218 mean train loss:  3.22972378e-03, bound:  3.15307319e-01\n",
      "Epoch: 39219 mean train loss:  3.22969980e-03, bound:  3.15307289e-01\n",
      "Epoch: 39220 mean train loss:  3.22957989e-03, bound:  3.15307289e-01\n",
      "Epoch: 39221 mean train loss:  3.22955986e-03, bound:  3.15307260e-01\n",
      "Epoch: 39222 mean train loss:  3.22948815e-03, bound:  3.15307260e-01\n",
      "Epoch: 39223 mean train loss:  3.22944485e-03, bound:  3.15307260e-01\n",
      "Epoch: 39224 mean train loss:  3.22941225e-03, bound:  3.15307260e-01\n",
      "Epoch: 39225 mean train loss:  3.22929258e-03, bound:  3.15307260e-01\n",
      "Epoch: 39226 mean train loss:  3.22924973e-03, bound:  3.15307260e-01\n",
      "Epoch: 39227 mean train loss:  3.22921132e-03, bound:  3.15307260e-01\n",
      "Epoch: 39228 mean train loss:  3.22919292e-03, bound:  3.15307230e-01\n",
      "Epoch: 39229 mean train loss:  3.22914054e-03, bound:  3.15307230e-01\n",
      "Epoch: 39230 mean train loss:  3.22908093e-03, bound:  3.15307230e-01\n",
      "Epoch: 39231 mean train loss:  3.22901784e-03, bound:  3.15307230e-01\n",
      "Epoch: 39232 mean train loss:  3.22901434e-03, bound:  3.15307230e-01\n",
      "Epoch: 39233 mean train loss:  3.22891911e-03, bound:  3.15307230e-01\n",
      "Epoch: 39234 mean train loss:  3.22892144e-03, bound:  3.15307200e-01\n",
      "Epoch: 39235 mean train loss:  3.22884484e-03, bound:  3.15307200e-01\n",
      "Epoch: 39236 mean train loss:  3.22876777e-03, bound:  3.15307200e-01\n",
      "Epoch: 39237 mean train loss:  3.22871283e-03, bound:  3.15307200e-01\n",
      "Epoch: 39238 mean train loss:  3.22869979e-03, bound:  3.15307200e-01\n",
      "Epoch: 39239 mean train loss:  3.22859711e-03, bound:  3.15307170e-01\n",
      "Epoch: 39240 mean train loss:  3.22855497e-03, bound:  3.15307170e-01\n",
      "Epoch: 39241 mean train loss:  3.22847138e-03, bound:  3.15307140e-01\n",
      "Epoch: 39242 mean train loss:  3.22841387e-03, bound:  3.15307140e-01\n",
      "Epoch: 39243 mean train loss:  3.22840549e-03, bound:  3.15307140e-01\n",
      "Epoch: 39244 mean train loss:  3.22829792e-03, bound:  3.15307140e-01\n",
      "Epoch: 39245 mean train loss:  3.22830607e-03, bound:  3.15307140e-01\n",
      "Epoch: 39246 mean train loss:  3.22821410e-03, bound:  3.15307140e-01\n",
      "Epoch: 39247 mean train loss:  3.22814798e-03, bound:  3.15307140e-01\n",
      "Epoch: 39248 mean train loss:  3.22808046e-03, bound:  3.15307140e-01\n",
      "Epoch: 39249 mean train loss:  3.22809699e-03, bound:  3.15307140e-01\n",
      "Epoch: 39250 mean train loss:  3.22800735e-03, bound:  3.15307140e-01\n",
      "Epoch: 39251 mean train loss:  3.22793098e-03, bound:  3.15307140e-01\n",
      "Epoch: 39252 mean train loss:  3.22790584e-03, bound:  3.15307111e-01\n",
      "Epoch: 39253 mean train loss:  3.22785904e-03, bound:  3.15307081e-01\n",
      "Epoch: 39254 mean train loss:  3.22781364e-03, bound:  3.15307081e-01\n",
      "Epoch: 39255 mean train loss:  3.22772586e-03, bound:  3.15307081e-01\n",
      "Epoch: 39256 mean train loss:  3.22769466e-03, bound:  3.15307081e-01\n",
      "Epoch: 39257 mean train loss:  3.22762667e-03, bound:  3.15307051e-01\n",
      "Epoch: 39258 mean train loss:  3.22759454e-03, bound:  3.15307051e-01\n",
      "Epoch: 39259 mean train loss:  3.22750630e-03, bound:  3.15307051e-01\n",
      "Epoch: 39260 mean train loss:  3.22749652e-03, bound:  3.15307021e-01\n",
      "Epoch: 39261 mean train loss:  3.22742644e-03, bound:  3.15307021e-01\n",
      "Epoch: 39262 mean train loss:  3.22734076e-03, bound:  3.15307021e-01\n",
      "Epoch: 39263 mean train loss:  3.22734891e-03, bound:  3.15307021e-01\n",
      "Epoch: 39264 mean train loss:  3.22727649e-03, bound:  3.15307021e-01\n",
      "Epoch: 39265 mean train loss:  3.22725484e-03, bound:  3.15307021e-01\n",
      "Epoch: 39266 mean train loss:  3.22717614e-03, bound:  3.15307021e-01\n",
      "Epoch: 39267 mean train loss:  3.22710583e-03, bound:  3.15307021e-01\n",
      "Epoch: 39268 mean train loss:  3.22705018e-03, bound:  3.15307021e-01\n",
      "Epoch: 39269 mean train loss:  3.22699640e-03, bound:  3.15307021e-01\n",
      "Epoch: 39270 mean train loss:  3.22693959e-03, bound:  3.15307021e-01\n",
      "Epoch: 39271 mean train loss:  3.22691561e-03, bound:  3.15307021e-01\n",
      "Epoch: 39272 mean train loss:  3.22683575e-03, bound:  3.15306991e-01\n",
      "Epoch: 39273 mean train loss:  3.22682783e-03, bound:  3.15306991e-01\n",
      "Epoch: 39274 mean train loss:  3.22678988e-03, bound:  3.15306932e-01\n",
      "Epoch: 39275 mean train loss:  3.22675612e-03, bound:  3.15306932e-01\n",
      "Epoch: 39276 mean train loss:  3.22675961e-03, bound:  3.15306932e-01\n",
      "Epoch: 39277 mean train loss:  3.22671351e-03, bound:  3.15306932e-01\n",
      "Epoch: 39278 mean train loss:  3.22673493e-03, bound:  3.15306902e-01\n",
      "Epoch: 39279 mean train loss:  3.22666671e-03, bound:  3.15306932e-01\n",
      "Epoch: 39280 mean train loss:  3.22663295e-03, bound:  3.15306902e-01\n",
      "Epoch: 39281 mean train loss:  3.22651910e-03, bound:  3.15306902e-01\n",
      "Epoch: 39282 mean train loss:  3.22641828e-03, bound:  3.15306902e-01\n",
      "Epoch: 39283 mean train loss:  3.22633074e-03, bound:  3.15306902e-01\n",
      "Epoch: 39284 mean train loss:  3.22621828e-03, bound:  3.15306902e-01\n",
      "Epoch: 39285 mean train loss:  3.22617195e-03, bound:  3.15306902e-01\n",
      "Epoch: 39286 mean train loss:  3.22616659e-03, bound:  3.15306902e-01\n",
      "Epoch: 39287 mean train loss:  3.22614750e-03, bound:  3.15306902e-01\n",
      "Epoch: 39288 mean train loss:  3.22607975e-03, bound:  3.15306902e-01\n",
      "Epoch: 39289 mean train loss:  3.22599057e-03, bound:  3.15306902e-01\n",
      "Epoch: 39290 mean train loss:  3.22596030e-03, bound:  3.15306902e-01\n",
      "Epoch: 39291 mean train loss:  3.22588347e-03, bound:  3.15306872e-01\n",
      "Epoch: 39292 mean train loss:  3.22581222e-03, bound:  3.15306872e-01\n",
      "Epoch: 39293 mean train loss:  3.22572282e-03, bound:  3.15306842e-01\n",
      "Epoch: 39294 mean train loss:  3.22570372e-03, bound:  3.15306842e-01\n",
      "Epoch: 39295 mean train loss:  3.22562386e-03, bound:  3.15306842e-01\n",
      "Epoch: 39296 mean train loss:  3.22562549e-03, bound:  3.15306813e-01\n",
      "Epoch: 39297 mean train loss:  3.22559988e-03, bound:  3.15306842e-01\n",
      "Epoch: 39298 mean train loss:  3.22547066e-03, bound:  3.15306813e-01\n",
      "Epoch: 39299 mean train loss:  3.22543294e-03, bound:  3.15306813e-01\n",
      "Epoch: 39300 mean train loss:  3.22533119e-03, bound:  3.15306813e-01\n",
      "Epoch: 39301 mean train loss:  3.22535075e-03, bound:  3.15306813e-01\n",
      "Epoch: 39302 mean train loss:  3.22530139e-03, bound:  3.15306813e-01\n",
      "Epoch: 39303 mean train loss:  3.22524412e-03, bound:  3.15306813e-01\n",
      "Epoch: 39304 mean train loss:  3.22516984e-03, bound:  3.15306783e-01\n",
      "Epoch: 39305 mean train loss:  3.22510698e-03, bound:  3.15306783e-01\n",
      "Epoch: 39306 mean train loss:  3.22508509e-03, bound:  3.15306783e-01\n",
      "Epoch: 39307 mean train loss:  3.22498218e-03, bound:  3.15306783e-01\n",
      "Epoch: 39308 mean train loss:  3.22495820e-03, bound:  3.15306783e-01\n",
      "Epoch: 39309 mean train loss:  3.22489650e-03, bound:  3.15306783e-01\n",
      "Epoch: 39310 mean train loss:  3.22484784e-03, bound:  3.15306783e-01\n",
      "Epoch: 39311 mean train loss:  3.22480919e-03, bound:  3.15306753e-01\n",
      "Epoch: 39312 mean train loss:  3.22474539e-03, bound:  3.15306723e-01\n",
      "Epoch: 39313 mean train loss:  3.22472211e-03, bound:  3.15306723e-01\n",
      "Epoch: 39314 mean train loss:  3.22461128e-03, bound:  3.15306723e-01\n",
      "Epoch: 39315 mean train loss:  3.22460779e-03, bound:  3.15306723e-01\n",
      "Epoch: 39316 mean train loss:  3.22452863e-03, bound:  3.15306723e-01\n",
      "Epoch: 39317 mean train loss:  3.22448369e-03, bound:  3.15306723e-01\n",
      "Epoch: 39318 mean train loss:  3.22442106e-03, bound:  3.15306723e-01\n",
      "Epoch: 39319 mean train loss:  3.22439056e-03, bound:  3.15306723e-01\n",
      "Epoch: 39320 mean train loss:  3.22428811e-03, bound:  3.15306693e-01\n",
      "Epoch: 39321 mean train loss:  3.22426064e-03, bound:  3.15306693e-01\n",
      "Epoch: 39322 mean train loss:  3.22419149e-03, bound:  3.15306693e-01\n",
      "Epoch: 39323 mean train loss:  3.22417915e-03, bound:  3.15306693e-01\n",
      "Epoch: 39324 mean train loss:  3.22407647e-03, bound:  3.15306664e-01\n",
      "Epoch: 39325 mean train loss:  3.22400755e-03, bound:  3.15306664e-01\n",
      "Epoch: 39326 mean train loss:  3.22403386e-03, bound:  3.15306664e-01\n",
      "Epoch: 39327 mean train loss:  3.22397449e-03, bound:  3.15306664e-01\n",
      "Epoch: 39328 mean train loss:  3.22392327e-03, bound:  3.15306664e-01\n",
      "Epoch: 39329 mean train loss:  3.22384527e-03, bound:  3.15306664e-01\n",
      "Epoch: 39330 mean train loss:  3.22379847e-03, bound:  3.15306664e-01\n",
      "Epoch: 39331 mean train loss:  3.22376727e-03, bound:  3.15306604e-01\n",
      "Epoch: 39332 mean train loss:  3.22368159e-03, bound:  3.15306604e-01\n",
      "Epoch: 39333 mean train loss:  3.22360825e-03, bound:  3.15306604e-01\n",
      "Epoch: 39334 mean train loss:  3.22357495e-03, bound:  3.15306604e-01\n",
      "Epoch: 39335 mean train loss:  3.22353863e-03, bound:  3.15306604e-01\n",
      "Epoch: 39336 mean train loss:  3.22353444e-03, bound:  3.15306604e-01\n",
      "Epoch: 39337 mean train loss:  3.22342268e-03, bound:  3.15306604e-01\n",
      "Epoch: 39338 mean train loss:  3.22334655e-03, bound:  3.15306574e-01\n",
      "Epoch: 39339 mean train loss:  3.22335935e-03, bound:  3.15306574e-01\n",
      "Epoch: 39340 mean train loss:  3.22320103e-03, bound:  3.15306574e-01\n",
      "Epoch: 39341 mean train loss:  3.22321686e-03, bound:  3.15306574e-01\n",
      "Epoch: 39342 mean train loss:  3.22318729e-03, bound:  3.15306574e-01\n",
      "Epoch: 39343 mean train loss:  3.22309160e-03, bound:  3.15306574e-01\n",
      "Epoch: 39344 mean train loss:  3.22312885e-03, bound:  3.15306574e-01\n",
      "Epoch: 39345 mean train loss:  3.22302082e-03, bound:  3.15306574e-01\n",
      "Epoch: 39346 mean train loss:  3.22298612e-03, bound:  3.15306544e-01\n",
      "Epoch: 39347 mean train loss:  3.22294980e-03, bound:  3.15306544e-01\n",
      "Epoch: 39348 mean train loss:  3.22285714e-03, bound:  3.15306544e-01\n",
      "Epoch: 39349 mean train loss:  3.22280056e-03, bound:  3.15306544e-01\n",
      "Epoch: 39350 mean train loss:  3.22275003e-03, bound:  3.15306515e-01\n",
      "Epoch: 39351 mean train loss:  3.22268065e-03, bound:  3.15306515e-01\n",
      "Epoch: 39352 mean train loss:  3.22265644e-03, bound:  3.15306515e-01\n",
      "Epoch: 39353 mean train loss:  3.22259450e-03, bound:  3.15306515e-01\n",
      "Epoch: 39354 mean train loss:  3.22256074e-03, bound:  3.15306515e-01\n",
      "Epoch: 39355 mean train loss:  3.22247250e-03, bound:  3.15306515e-01\n",
      "Epoch: 39356 mean train loss:  3.22244433e-03, bound:  3.15306515e-01\n",
      "Epoch: 39357 mean train loss:  3.22232815e-03, bound:  3.15306485e-01\n",
      "Epoch: 39358 mean train loss:  3.22233397e-03, bound:  3.15306485e-01\n",
      "Epoch: 39359 mean train loss:  3.22227133e-03, bound:  3.15306485e-01\n",
      "Epoch: 39360 mean train loss:  3.22224572e-03, bound:  3.15306455e-01\n",
      "Epoch: 39361 mean train loss:  3.22212046e-03, bound:  3.15306455e-01\n",
      "Epoch: 39362 mean train loss:  3.22210812e-03, bound:  3.15306455e-01\n",
      "Epoch: 39363 mean train loss:  3.22202523e-03, bound:  3.15306455e-01\n",
      "Epoch: 39364 mean train loss:  3.22198472e-03, bound:  3.15306455e-01\n",
      "Epoch: 39365 mean train loss:  3.22194933e-03, bound:  3.15306455e-01\n",
      "Epoch: 39366 mean train loss:  3.22190020e-03, bound:  3.15306425e-01\n",
      "Epoch: 39367 mean train loss:  3.22184595e-03, bound:  3.15306425e-01\n",
      "Epoch: 39368 mean train loss:  3.22180614e-03, bound:  3.15306425e-01\n",
      "Epoch: 39369 mean train loss:  3.22176027e-03, bound:  3.15306425e-01\n",
      "Epoch: 39370 mean train loss:  3.22164735e-03, bound:  3.15306425e-01\n",
      "Epoch: 39371 mean train loss:  3.22164735e-03, bound:  3.15306425e-01\n",
      "Epoch: 39372 mean train loss:  3.22154723e-03, bound:  3.15306365e-01\n",
      "Epoch: 39373 mean train loss:  3.22152278e-03, bound:  3.15306365e-01\n",
      "Epoch: 39374 mean train loss:  3.22147273e-03, bound:  3.15306365e-01\n",
      "Epoch: 39375 mean train loss:  3.22143687e-03, bound:  3.15306365e-01\n",
      "Epoch: 39376 mean train loss:  3.22135352e-03, bound:  3.15306365e-01\n",
      "Epoch: 39377 mean train loss:  3.22134700e-03, bound:  3.15306365e-01\n",
      "Epoch: 39378 mean train loss:  3.22125037e-03, bound:  3.15306365e-01\n",
      "Epoch: 39379 mean train loss:  3.22120730e-03, bound:  3.15306365e-01\n",
      "Epoch: 39380 mean train loss:  3.22117656e-03, bound:  3.15306336e-01\n",
      "Epoch: 39381 mean train loss:  3.22113023e-03, bound:  3.15306336e-01\n",
      "Epoch: 39382 mean train loss:  3.22107365e-03, bound:  3.15306336e-01\n",
      "Epoch: 39383 mean train loss:  3.22105014e-03, bound:  3.15306336e-01\n",
      "Epoch: 39384 mean train loss:  3.22096422e-03, bound:  3.15306336e-01\n",
      "Epoch: 39385 mean train loss:  3.22094443e-03, bound:  3.15306336e-01\n",
      "Epoch: 39386 mean train loss:  3.22085829e-03, bound:  3.15306336e-01\n",
      "Epoch: 39387 mean train loss:  3.22079146e-03, bound:  3.15306336e-01\n",
      "Epoch: 39388 mean train loss:  3.22076818e-03, bound:  3.15306306e-01\n",
      "Epoch: 39389 mean train loss:  3.22072697e-03, bound:  3.15306306e-01\n",
      "Epoch: 39390 mean train loss:  3.22066061e-03, bound:  3.15306306e-01\n",
      "Epoch: 39391 mean train loss:  3.22060427e-03, bound:  3.15306306e-01\n",
      "Epoch: 39392 mean train loss:  3.22055025e-03, bound:  3.15306276e-01\n",
      "Epoch: 39393 mean train loss:  3.22055584e-03, bound:  3.15306276e-01\n",
      "Epoch: 39394 mean train loss:  3.22050205e-03, bound:  3.15306276e-01\n",
      "Epoch: 39395 mean train loss:  3.22041777e-03, bound:  3.15306246e-01\n",
      "Epoch: 39396 mean train loss:  3.22036678e-03, bound:  3.15306246e-01\n",
      "Epoch: 39397 mean train loss:  3.22029390e-03, bound:  3.15306246e-01\n",
      "Epoch: 39398 mean train loss:  3.22022988e-03, bound:  3.15306216e-01\n",
      "Epoch: 39399 mean train loss:  3.22016771e-03, bound:  3.15306246e-01\n",
      "Epoch: 39400 mean train loss:  3.22009856e-03, bound:  3.15306216e-01\n",
      "Epoch: 39401 mean train loss:  3.22003732e-03, bound:  3.15306216e-01\n",
      "Epoch: 39402 mean train loss:  3.21999821e-03, bound:  3.15306216e-01\n",
      "Epoch: 39403 mean train loss:  3.21997050e-03, bound:  3.15306216e-01\n",
      "Epoch: 39404 mean train loss:  3.21993954e-03, bound:  3.15306216e-01\n",
      "Epoch: 39405 mean train loss:  3.21986480e-03, bound:  3.15306216e-01\n",
      "Epoch: 39406 mean train loss:  3.21983057e-03, bound:  3.15306216e-01\n",
      "Epoch: 39407 mean train loss:  3.21980426e-03, bound:  3.15306187e-01\n",
      "Epoch: 39408 mean train loss:  3.21974535e-03, bound:  3.15306216e-01\n",
      "Epoch: 39409 mean train loss:  3.21972300e-03, bound:  3.15306187e-01\n",
      "Epoch: 39410 mean train loss:  3.21964384e-03, bound:  3.15306187e-01\n",
      "Epoch: 39411 mean train loss:  3.21959169e-03, bound:  3.15306187e-01\n",
      "Epoch: 39412 mean train loss:  3.21948715e-03, bound:  3.15306187e-01\n",
      "Epoch: 39413 mean train loss:  3.21946596e-03, bound:  3.15306157e-01\n",
      "Epoch: 39414 mean train loss:  3.21936468e-03, bound:  3.15306127e-01\n",
      "Epoch: 39415 mean train loss:  3.21932836e-03, bound:  3.15306127e-01\n",
      "Epoch: 39416 mean train loss:  3.21927760e-03, bound:  3.15306127e-01\n",
      "Epoch: 39417 mean train loss:  3.21923126e-03, bound:  3.15306127e-01\n",
      "Epoch: 39418 mean train loss:  3.21918214e-03, bound:  3.15306097e-01\n",
      "Epoch: 39419 mean train loss:  3.21912346e-03, bound:  3.15306097e-01\n",
      "Epoch: 39420 mean train loss:  3.21911299e-03, bound:  3.15306097e-01\n",
      "Epoch: 39421 mean train loss:  3.21902381e-03, bound:  3.15306097e-01\n",
      "Epoch: 39422 mean train loss:  3.21897725e-03, bound:  3.15306097e-01\n",
      "Epoch: 39423 mean train loss:  3.21891950e-03, bound:  3.15306097e-01\n",
      "Epoch: 39424 mean train loss:  3.21887690e-03, bound:  3.15306097e-01\n",
      "Epoch: 39425 mean train loss:  3.21882055e-03, bound:  3.15306097e-01\n",
      "Epoch: 39426 mean train loss:  3.21876537e-03, bound:  3.15306097e-01\n",
      "Epoch: 39427 mean train loss:  3.21870600e-03, bound:  3.15306097e-01\n",
      "Epoch: 39428 mean train loss:  3.21865571e-03, bound:  3.15306067e-01\n",
      "Epoch: 39429 mean train loss:  3.21862753e-03, bound:  3.15306067e-01\n",
      "Epoch: 39430 mean train loss:  3.21856560e-03, bound:  3.15306067e-01\n",
      "Epoch: 39431 mean train loss:  3.21847340e-03, bound:  3.15306067e-01\n",
      "Epoch: 39432 mean train loss:  3.21843778e-03, bound:  3.15306038e-01\n",
      "Epoch: 39433 mean train loss:  3.21836746e-03, bound:  3.15306008e-01\n",
      "Epoch: 39434 mean train loss:  3.21829156e-03, bound:  3.15306008e-01\n",
      "Epoch: 39435 mean train loss:  3.21830646e-03, bound:  3.15306008e-01\n",
      "Epoch: 39436 mean train loss:  3.21823289e-03, bound:  3.15306008e-01\n",
      "Epoch: 39437 mean train loss:  3.21818609e-03, bound:  3.15306008e-01\n",
      "Epoch: 39438 mean train loss:  3.21812090e-03, bound:  3.15306008e-01\n",
      "Epoch: 39439 mean train loss:  3.21804336e-03, bound:  3.15306008e-01\n",
      "Epoch: 39440 mean train loss:  3.21803708e-03, bound:  3.15306008e-01\n",
      "Epoch: 39441 mean train loss:  3.21795349e-03, bound:  3.15305978e-01\n",
      "Epoch: 39442 mean train loss:  3.21795791e-03, bound:  3.15305978e-01\n",
      "Epoch: 39443 mean train loss:  3.21787759e-03, bound:  3.15305978e-01\n",
      "Epoch: 39444 mean train loss:  3.21776746e-03, bound:  3.15305978e-01\n",
      "Epoch: 39445 mean train loss:  3.21779586e-03, bound:  3.15305978e-01\n",
      "Epoch: 39446 mean train loss:  3.21766920e-03, bound:  3.15305978e-01\n",
      "Epoch: 39447 mean train loss:  3.21763731e-03, bound:  3.15305918e-01\n",
      "Epoch: 39448 mean train loss:  3.21759819e-03, bound:  3.15305918e-01\n",
      "Epoch: 39449 mean train loss:  3.21755814e-03, bound:  3.15305918e-01\n",
      "Epoch: 39450 mean train loss:  3.21752951e-03, bound:  3.15305918e-01\n",
      "Epoch: 39451 mean train loss:  3.21748550e-03, bound:  3.15305918e-01\n",
      "Epoch: 39452 mean train loss:  3.21737025e-03, bound:  3.15305918e-01\n",
      "Epoch: 39453 mean train loss:  3.21729411e-03, bound:  3.15305918e-01\n",
      "Epoch: 39454 mean train loss:  3.21733812e-03, bound:  3.15305918e-01\n",
      "Epoch: 39455 mean train loss:  3.21721355e-03, bound:  3.15305918e-01\n",
      "Epoch: 39456 mean train loss:  3.21723404e-03, bound:  3.15305918e-01\n",
      "Epoch: 39457 mean train loss:  3.21712741e-03, bound:  3.15305918e-01\n",
      "Epoch: 39458 mean train loss:  3.21711577e-03, bound:  3.15305889e-01\n",
      "Epoch: 39459 mean train loss:  3.21708457e-03, bound:  3.15305889e-01\n",
      "Epoch: 39460 mean train loss:  3.21695814e-03, bound:  3.15305889e-01\n",
      "Epoch: 39461 mean train loss:  3.21695255e-03, bound:  3.15305889e-01\n",
      "Epoch: 39462 mean train loss:  3.21689551e-03, bound:  3.15305889e-01\n",
      "Epoch: 39463 mean train loss:  3.21681006e-03, bound:  3.15305889e-01\n",
      "Epoch: 39464 mean train loss:  3.21676745e-03, bound:  3.15305859e-01\n",
      "Epoch: 39465 mean train loss:  3.21672251e-03, bound:  3.15305859e-01\n",
      "Epoch: 39466 mean train loss:  3.21660726e-03, bound:  3.15305799e-01\n",
      "Epoch: 39467 mean train loss:  3.21656931e-03, bound:  3.15305799e-01\n",
      "Epoch: 39468 mean train loss:  3.21655930e-03, bound:  3.15305799e-01\n",
      "Epoch: 39469 mean train loss:  3.21647059e-03, bound:  3.15305799e-01\n",
      "Epoch: 39470 mean train loss:  3.21643637e-03, bound:  3.15305799e-01\n",
      "Epoch: 39471 mean train loss:  3.21638002e-03, bound:  3.15305799e-01\n",
      "Epoch: 39472 mean train loss:  3.21635138e-03, bound:  3.15305799e-01\n",
      "Epoch: 39473 mean train loss:  3.21631273e-03, bound:  3.15305799e-01\n",
      "Epoch: 39474 mean train loss:  3.21622286e-03, bound:  3.15305799e-01\n",
      "Epoch: 39475 mean train loss:  3.21611739e-03, bound:  3.15305799e-01\n",
      "Epoch: 39476 mean train loss:  3.21616093e-03, bound:  3.15305769e-01\n",
      "Epoch: 39477 mean train loss:  3.21606430e-03, bound:  3.15305769e-01\n",
      "Epoch: 39478 mean train loss:  3.21600446e-03, bound:  3.15305769e-01\n",
      "Epoch: 39479 mean train loss:  3.21590737e-03, bound:  3.15305769e-01\n",
      "Epoch: 39480 mean train loss:  3.21589364e-03, bound:  3.15305769e-01\n",
      "Epoch: 39481 mean train loss:  3.21582262e-03, bound:  3.15305769e-01\n",
      "Epoch: 39482 mean train loss:  3.21576977e-03, bound:  3.15305769e-01\n",
      "Epoch: 39483 mean train loss:  3.21574998e-03, bound:  3.15305740e-01\n",
      "Epoch: 39484 mean train loss:  3.21573694e-03, bound:  3.15305740e-01\n",
      "Epoch: 39485 mean train loss:  3.21565289e-03, bound:  3.15305740e-01\n",
      "Epoch: 39486 mean train loss:  3.21557256e-03, bound:  3.15305680e-01\n",
      "Epoch: 39487 mean train loss:  3.21555068e-03, bound:  3.15305680e-01\n",
      "Epoch: 39488 mean train loss:  3.21549526e-03, bound:  3.15305680e-01\n",
      "Epoch: 39489 mean train loss:  3.21543682e-03, bound:  3.15305680e-01\n",
      "Epoch: 39490 mean train loss:  3.21538839e-03, bound:  3.15305680e-01\n",
      "Epoch: 39491 mean train loss:  3.21529619e-03, bound:  3.15305680e-01\n",
      "Epoch: 39492 mean train loss:  3.21526895e-03, bound:  3.15305680e-01\n",
      "Epoch: 39493 mean train loss:  3.21522704e-03, bound:  3.15305680e-01\n",
      "Epoch: 39494 mean train loss:  3.21511342e-03, bound:  3.15305680e-01\n",
      "Epoch: 39495 mean train loss:  3.21512157e-03, bound:  3.15305680e-01\n",
      "Epoch: 39496 mean train loss:  3.21505009e-03, bound:  3.15305680e-01\n",
      "Epoch: 39497 mean train loss:  3.21501144e-03, bound:  3.15305650e-01\n",
      "Epoch: 39498 mean train loss:  3.21502192e-03, bound:  3.15305650e-01\n",
      "Epoch: 39499 mean train loss:  3.21490294e-03, bound:  3.15305650e-01\n",
      "Epoch: 39500 mean train loss:  3.21481447e-03, bound:  3.15305650e-01\n",
      "Epoch: 39501 mean train loss:  3.21480795e-03, bound:  3.15305650e-01\n",
      "Epoch: 39502 mean train loss:  3.21475160e-03, bound:  3.15305620e-01\n",
      "Epoch: 39503 mean train loss:  3.21470946e-03, bound:  3.15305620e-01\n",
      "Epoch: 39504 mean train loss:  3.21466685e-03, bound:  3.15305620e-01\n",
      "Epoch: 39505 mean train loss:  3.21462820e-03, bound:  3.15305620e-01\n",
      "Epoch: 39506 mean train loss:  3.21455998e-03, bound:  3.15305620e-01\n",
      "Epoch: 39507 mean train loss:  3.21449805e-03, bound:  3.15305620e-01\n",
      "Epoch: 39508 mean train loss:  3.21444939e-03, bound:  3.15305620e-01\n",
      "Epoch: 39509 mean train loss:  3.21441493e-03, bound:  3.15305591e-01\n",
      "Epoch: 39510 mean train loss:  3.21429246e-03, bound:  3.15305591e-01\n",
      "Epoch: 39511 mean train loss:  3.21428454e-03, bound:  3.15305561e-01\n",
      "Epoch: 39512 mean train loss:  3.21423751e-03, bound:  3.15305561e-01\n",
      "Epoch: 39513 mean train loss:  3.21420142e-03, bound:  3.15305561e-01\n",
      "Epoch: 39514 mean train loss:  3.21418839e-03, bound:  3.15305561e-01\n",
      "Epoch: 39515 mean train loss:  3.21415183e-03, bound:  3.15305531e-01\n",
      "Epoch: 39516 mean train loss:  3.21407430e-03, bound:  3.15305531e-01\n",
      "Epoch: 39517 mean train loss:  3.21402890e-03, bound:  3.15305531e-01\n",
      "Epoch: 39518 mean train loss:  3.21398932e-03, bound:  3.15305531e-01\n",
      "Epoch: 39519 mean train loss:  3.21390759e-03, bound:  3.15305531e-01\n",
      "Epoch: 39520 mean train loss:  3.21386755e-03, bound:  3.15305531e-01\n",
      "Epoch: 39521 mean train loss:  3.21381073e-03, bound:  3.15305471e-01\n",
      "Epoch: 39522 mean train loss:  3.21374252e-03, bound:  3.15305471e-01\n",
      "Epoch: 39523 mean train loss:  3.21367220e-03, bound:  3.15305471e-01\n",
      "Epoch: 39524 mean train loss:  3.21359094e-03, bound:  3.15305471e-01\n",
      "Epoch: 39525 mean train loss:  3.21353250e-03, bound:  3.15305471e-01\n",
      "Epoch: 39526 mean train loss:  3.21347616e-03, bound:  3.15305471e-01\n",
      "Epoch: 39527 mean train loss:  3.21344263e-03, bound:  3.15305471e-01\n",
      "Epoch: 39528 mean train loss:  3.21334414e-03, bound:  3.15305471e-01\n",
      "Epoch: 39529 mean train loss:  3.21334065e-03, bound:  3.15305471e-01\n",
      "Epoch: 39530 mean train loss:  3.21332295e-03, bound:  3.15305471e-01\n",
      "Epoch: 39531 mean train loss:  3.21321306e-03, bound:  3.15305442e-01\n",
      "Epoch: 39532 mean train loss:  3.21316323e-03, bound:  3.15305442e-01\n",
      "Epoch: 39533 mean train loss:  3.21309105e-03, bound:  3.15305442e-01\n",
      "Epoch: 39534 mean train loss:  3.21302353e-03, bound:  3.15305442e-01\n",
      "Epoch: 39535 mean train loss:  3.21304658e-03, bound:  3.15305412e-01\n",
      "Epoch: 39536 mean train loss:  3.21298046e-03, bound:  3.15305412e-01\n",
      "Epoch: 39537 mean train loss:  3.21293320e-03, bound:  3.15305412e-01\n",
      "Epoch: 39538 mean train loss:  3.21290572e-03, bound:  3.15305412e-01\n",
      "Epoch: 39539 mean train loss:  3.21281864e-03, bound:  3.15305412e-01\n",
      "Epoch: 39540 mean train loss:  3.21275624e-03, bound:  3.15305412e-01\n",
      "Epoch: 39541 mean train loss:  3.21273552e-03, bound:  3.15305412e-01\n",
      "Epoch: 39542 mean train loss:  3.21262097e-03, bound:  3.15305412e-01\n",
      "Epoch: 39543 mean train loss:  3.21257301e-03, bound:  3.15305412e-01\n",
      "Epoch: 39544 mean train loss:  3.21252737e-03, bound:  3.15305352e-01\n",
      "Epoch: 39545 mean train loss:  3.21248174e-03, bound:  3.15305352e-01\n",
      "Epoch: 39546 mean train loss:  3.21246753e-03, bound:  3.15305352e-01\n",
      "Epoch: 39547 mean train loss:  3.21243494e-03, bound:  3.15305352e-01\n",
      "Epoch: 39548 mean train loss:  3.21232015e-03, bound:  3.15305352e-01\n",
      "Epoch: 39549 mean train loss:  3.21227126e-03, bound:  3.15305352e-01\n",
      "Epoch: 39550 mean train loss:  3.21222073e-03, bound:  3.15305352e-01\n",
      "Epoch: 39551 mean train loss:  3.21218651e-03, bound:  3.15305322e-01\n",
      "Epoch: 39552 mean train loss:  3.21212970e-03, bound:  3.15305322e-01\n",
      "Epoch: 39553 mean train loss:  3.21204541e-03, bound:  3.15305293e-01\n",
      "Epoch: 39554 mean train loss:  3.21201957e-03, bound:  3.15305293e-01\n",
      "Epoch: 39555 mean train loss:  3.21190502e-03, bound:  3.15305293e-01\n",
      "Epoch: 39556 mean train loss:  3.21192644e-03, bound:  3.15305293e-01\n",
      "Epoch: 39557 mean train loss:  3.21187265e-03, bound:  3.15305293e-01\n",
      "Epoch: 39558 mean train loss:  3.21181328e-03, bound:  3.15305293e-01\n",
      "Epoch: 39559 mean train loss:  3.21177719e-03, bound:  3.15305293e-01\n",
      "Epoch: 39560 mean train loss:  3.21168662e-03, bound:  3.15305293e-01\n",
      "Epoch: 39561 mean train loss:  3.21167614e-03, bound:  3.15305293e-01\n",
      "Epoch: 39562 mean train loss:  3.21161747e-03, bound:  3.15305293e-01\n",
      "Epoch: 39563 mean train loss:  3.21154133e-03, bound:  3.15305233e-01\n",
      "Epoch: 39564 mean train loss:  3.21145868e-03, bound:  3.15305233e-01\n",
      "Epoch: 39565 mean train loss:  3.21146706e-03, bound:  3.15305233e-01\n",
      "Epoch: 39566 mean train loss:  3.21141491e-03, bound:  3.15305233e-01\n",
      "Epoch: 39567 mean train loss:  3.21131549e-03, bound:  3.15305233e-01\n",
      "Epoch: 39568 mean train loss:  3.21125239e-03, bound:  3.15305233e-01\n",
      "Epoch: 39569 mean train loss:  3.21126333e-03, bound:  3.15305203e-01\n",
      "Epoch: 39570 mean train loss:  3.21115227e-03, bound:  3.15305203e-01\n",
      "Epoch: 39571 mean train loss:  3.21110408e-03, bound:  3.15305203e-01\n",
      "Epoch: 39572 mean train loss:  3.21103935e-03, bound:  3.15305203e-01\n",
      "Epoch: 39573 mean train loss:  3.21097509e-03, bound:  3.15305203e-01\n",
      "Epoch: 39574 mean train loss:  3.21095996e-03, bound:  3.15305203e-01\n",
      "Epoch: 39575 mean train loss:  3.21090873e-03, bound:  3.15305203e-01\n",
      "Epoch: 39576 mean train loss:  3.21084238e-03, bound:  3.15305203e-01\n",
      "Epoch: 39577 mean train loss:  3.21084308e-03, bound:  3.15305203e-01\n",
      "Epoch: 39578 mean train loss:  3.21077625e-03, bound:  3.15305203e-01\n",
      "Epoch: 39579 mean train loss:  3.21069825e-03, bound:  3.15305203e-01\n",
      "Epoch: 39580 mean train loss:  3.21064051e-03, bound:  3.15305173e-01\n",
      "Epoch: 39581 mean train loss:  3.21058603e-03, bound:  3.15305173e-01\n",
      "Epoch: 39582 mean train loss:  3.21054505e-03, bound:  3.15305173e-01\n",
      "Epoch: 39583 mean train loss:  3.21048428e-03, bound:  3.15305114e-01\n",
      "Epoch: 39584 mean train loss:  3.21047381e-03, bound:  3.15305114e-01\n",
      "Epoch: 39585 mean train loss:  3.21036391e-03, bound:  3.15305114e-01\n",
      "Epoch: 39586 mean train loss:  3.21032270e-03, bound:  3.15305114e-01\n",
      "Epoch: 39587 mean train loss:  3.21029895e-03, bound:  3.15305114e-01\n",
      "Epoch: 39588 mean train loss:  3.21021932e-03, bound:  3.15305114e-01\n",
      "Epoch: 39589 mean train loss:  3.21019930e-03, bound:  3.15305084e-01\n",
      "Epoch: 39590 mean train loss:  3.21013108e-03, bound:  3.15305084e-01\n",
      "Epoch: 39591 mean train loss:  3.21003725e-03, bound:  3.15305084e-01\n",
      "Epoch: 39592 mean train loss:  3.21004121e-03, bound:  3.15305084e-01\n",
      "Epoch: 39593 mean train loss:  3.20991315e-03, bound:  3.15305084e-01\n",
      "Epoch: 39594 mean train loss:  3.20992945e-03, bound:  3.15305084e-01\n",
      "Epoch: 39595 mean train loss:  3.20985913e-03, bound:  3.15305084e-01\n",
      "Epoch: 39596 mean train loss:  3.20979557e-03, bound:  3.15305084e-01\n",
      "Epoch: 39597 mean train loss:  3.20972176e-03, bound:  3.15305084e-01\n",
      "Epoch: 39598 mean train loss:  3.20970383e-03, bound:  3.15305084e-01\n",
      "Epoch: 39599 mean train loss:  3.20969429e-03, bound:  3.15305084e-01\n",
      "Epoch: 39600 mean train loss:  3.20960325e-03, bound:  3.15305054e-01\n",
      "Epoch: 39601 mean train loss:  3.20950150e-03, bound:  3.15305054e-01\n",
      "Epoch: 39602 mean train loss:  3.20948940e-03, bound:  3.15304995e-01\n",
      "Epoch: 39603 mean train loss:  3.20941396e-03, bound:  3.15304995e-01\n",
      "Epoch: 39604 mean train loss:  3.20937182e-03, bound:  3.15304995e-01\n",
      "Epoch: 39605 mean train loss:  3.20932223e-03, bound:  3.15304995e-01\n",
      "Epoch: 39606 mean train loss:  3.20927054e-03, bound:  3.15304995e-01\n",
      "Epoch: 39607 mean train loss:  3.20925261e-03, bound:  3.15304995e-01\n",
      "Epoch: 39608 mean train loss:  3.20918299e-03, bound:  3.15304995e-01\n",
      "Epoch: 39609 mean train loss:  3.20916320e-03, bound:  3.15304995e-01\n",
      "Epoch: 39610 mean train loss:  3.20909824e-03, bound:  3.15304995e-01\n",
      "Epoch: 39611 mean train loss:  3.20907310e-03, bound:  3.15304995e-01\n",
      "Epoch: 39612 mean train loss:  3.20903654e-03, bound:  3.15304995e-01\n",
      "Epoch: 39613 mean train loss:  3.20903119e-03, bound:  3.15304965e-01\n",
      "Epoch: 39614 mean train loss:  3.20899091e-03, bound:  3.15304965e-01\n",
      "Epoch: 39615 mean train loss:  3.20900930e-03, bound:  3.15304965e-01\n",
      "Epoch: 39616 mean train loss:  3.20898113e-03, bound:  3.15304965e-01\n",
      "Epoch: 39617 mean train loss:  3.20886984e-03, bound:  3.15304965e-01\n",
      "Epoch: 39618 mean train loss:  3.20880138e-03, bound:  3.15304965e-01\n",
      "Epoch: 39619 mean train loss:  3.20868869e-03, bound:  3.15304905e-01\n",
      "Epoch: 39620 mean train loss:  3.20855249e-03, bound:  3.15304965e-01\n",
      "Epoch: 39621 mean train loss:  3.20852431e-03, bound:  3.15304905e-01\n",
      "Epoch: 39622 mean train loss:  3.20843188e-03, bound:  3.15304905e-01\n",
      "Epoch: 39623 mean train loss:  3.20838951e-03, bound:  3.15304905e-01\n",
      "Epoch: 39624 mean train loss:  3.20841139e-03, bound:  3.15304905e-01\n",
      "Epoch: 39625 mean train loss:  3.20837321e-03, bound:  3.15304905e-01\n",
      "Epoch: 39626 mean train loss:  3.20834108e-03, bound:  3.15304905e-01\n",
      "Epoch: 39627 mean train loss:  3.20828753e-03, bound:  3.15304875e-01\n",
      "Epoch: 39628 mean train loss:  3.20817158e-03, bound:  3.15304875e-01\n",
      "Epoch: 39629 mean train loss:  3.20813269e-03, bound:  3.15304875e-01\n",
      "Epoch: 39630 mean train loss:  3.20804189e-03, bound:  3.15304875e-01\n",
      "Epoch: 39631 mean train loss:  3.20801116e-03, bound:  3.15304875e-01\n",
      "Epoch: 39632 mean train loss:  3.20797460e-03, bound:  3.15304846e-01\n",
      "Epoch: 39633 mean train loss:  3.20798019e-03, bound:  3.15304846e-01\n",
      "Epoch: 39634 mean train loss:  3.20789777e-03, bound:  3.15304846e-01\n",
      "Epoch: 39635 mean train loss:  3.20779276e-03, bound:  3.15304846e-01\n",
      "Epoch: 39636 mean train loss:  3.20776040e-03, bound:  3.15304846e-01\n",
      "Epoch: 39637 mean train loss:  3.20767239e-03, bound:  3.15304786e-01\n",
      "Epoch: 39638 mean train loss:  3.20761302e-03, bound:  3.15304786e-01\n",
      "Epoch: 39639 mean train loss:  3.20759485e-03, bound:  3.15304786e-01\n",
      "Epoch: 39640 mean train loss:  3.20756086e-03, bound:  3.15304786e-01\n",
      "Epoch: 39641 mean train loss:  3.20754782e-03, bound:  3.15304786e-01\n",
      "Epoch: 39642 mean train loss:  3.20748333e-03, bound:  3.15304786e-01\n",
      "Epoch: 39643 mean train loss:  3.20740766e-03, bound:  3.15304786e-01\n",
      "Epoch: 39644 mean train loss:  3.20732803e-03, bound:  3.15304786e-01\n",
      "Epoch: 39645 mean train loss:  3.20724677e-03, bound:  3.15304786e-01\n",
      "Epoch: 39646 mean train loss:  3.20720556e-03, bound:  3.15304786e-01\n",
      "Epoch: 39647 mean train loss:  3.20715038e-03, bound:  3.15304786e-01\n",
      "Epoch: 39648 mean train loss:  3.20711196e-03, bound:  3.15304786e-01\n",
      "Epoch: 39649 mean train loss:  3.20710195e-03, bound:  3.15304756e-01\n",
      "Epoch: 39650 mean train loss:  3.20700160e-03, bound:  3.15304756e-01\n",
      "Epoch: 39651 mean train loss:  3.20701627e-03, bound:  3.15304726e-01\n",
      "Epoch: 39652 mean train loss:  3.20690894e-03, bound:  3.15304726e-01\n",
      "Epoch: 39653 mean train loss:  3.20687331e-03, bound:  3.15304726e-01\n",
      "Epoch: 39654 mean train loss:  3.20681790e-03, bound:  3.15304726e-01\n",
      "Epoch: 39655 mean train loss:  3.20672756e-03, bound:  3.15304726e-01\n",
      "Epoch: 39656 mean train loss:  3.20671732e-03, bound:  3.15304726e-01\n",
      "Epoch: 39657 mean train loss:  3.20663373e-03, bound:  3.15304726e-01\n",
      "Epoch: 39658 mean train loss:  3.20661231e-03, bound:  3.15304726e-01\n",
      "Epoch: 39659 mean train loss:  3.20655759e-03, bound:  3.15304726e-01\n",
      "Epoch: 39660 mean train loss:  3.20650148e-03, bound:  3.15304667e-01\n",
      "Epoch: 39661 mean train loss:  3.20646795e-03, bound:  3.15304667e-01\n",
      "Epoch: 39662 mean train loss:  3.20637645e-03, bound:  3.15304667e-01\n",
      "Epoch: 39663 mean train loss:  3.20633035e-03, bound:  3.15304667e-01\n",
      "Epoch: 39664 mean train loss:  3.20628402e-03, bound:  3.15304667e-01\n",
      "Epoch: 39665 mean train loss:  3.20620742e-03, bound:  3.15304667e-01\n",
      "Epoch: 39666 mean train loss:  3.20620253e-03, bound:  3.15304667e-01\n",
      "Epoch: 39667 mean train loss:  3.20612080e-03, bound:  3.15304637e-01\n",
      "Epoch: 39668 mean train loss:  3.20606423e-03, bound:  3.15304637e-01\n",
      "Epoch: 39669 mean train loss:  3.20603279e-03, bound:  3.15304637e-01\n",
      "Epoch: 39670 mean train loss:  3.20600858e-03, bound:  3.15304637e-01\n",
      "Epoch: 39671 mean train loss:  3.20593966e-03, bound:  3.15304637e-01\n",
      "Epoch: 39672 mean train loss:  3.20585119e-03, bound:  3.15304637e-01\n",
      "Epoch: 39673 mean train loss:  3.20584932e-03, bound:  3.15304637e-01\n",
      "Epoch: 39674 mean train loss:  3.20579158e-03, bound:  3.15304607e-01\n",
      "Epoch: 39675 mean train loss:  3.20571079e-03, bound:  3.15304607e-01\n",
      "Epoch: 39676 mean train loss:  3.20566981e-03, bound:  3.15304607e-01\n",
      "Epoch: 39677 mean train loss:  3.20559577e-03, bound:  3.15304607e-01\n",
      "Epoch: 39678 mean train loss:  3.20563023e-03, bound:  3.15304607e-01\n",
      "Epoch: 39679 mean train loss:  3.20557202e-03, bound:  3.15304607e-01\n",
      "Epoch: 39680 mean train loss:  3.20539996e-03, bound:  3.15304607e-01\n",
      "Epoch: 39681 mean train loss:  3.20543256e-03, bound:  3.15304548e-01\n",
      "Epoch: 39682 mean train loss:  3.20532010e-03, bound:  3.15304607e-01\n",
      "Epoch: 39683 mean train loss:  3.20527214e-03, bound:  3.15304548e-01\n",
      "Epoch: 39684 mean train loss:  3.20524420e-03, bound:  3.15304548e-01\n",
      "Epoch: 39685 mean train loss:  3.20517924e-03, bound:  3.15304548e-01\n",
      "Epoch: 39686 mean train loss:  3.20517691e-03, bound:  3.15304548e-01\n",
      "Epoch: 39687 mean train loss:  3.20510939e-03, bound:  3.15304548e-01\n",
      "Epoch: 39688 mean train loss:  3.20501323e-03, bound:  3.15304518e-01\n",
      "Epoch: 39689 mean train loss:  3.20500089e-03, bound:  3.15304518e-01\n",
      "Epoch: 39690 mean train loss:  3.20496177e-03, bound:  3.15304518e-01\n",
      "Epoch: 39691 mean train loss:  3.20488750e-03, bound:  3.15304518e-01\n",
      "Epoch: 39692 mean train loss:  3.20479251e-03, bound:  3.15304518e-01\n",
      "Epoch: 39693 mean train loss:  3.20481509e-03, bound:  3.15304518e-01\n",
      "Epoch: 39694 mean train loss:  3.20475851e-03, bound:  3.15304518e-01\n",
      "Epoch: 39695 mean train loss:  3.20468820e-03, bound:  3.15304518e-01\n",
      "Epoch: 39696 mean train loss:  3.20465048e-03, bound:  3.15304518e-01\n",
      "Epoch: 39697 mean train loss:  3.20457947e-03, bound:  3.15304518e-01\n",
      "Epoch: 39698 mean train loss:  3.20453290e-03, bound:  3.15304458e-01\n",
      "Epoch: 39699 mean train loss:  3.20454361e-03, bound:  3.15304458e-01\n",
      "Epoch: 39700 mean train loss:  3.20446538e-03, bound:  3.15304458e-01\n",
      "Epoch: 39701 mean train loss:  3.20439064e-03, bound:  3.15304428e-01\n",
      "Epoch: 39702 mean train loss:  3.20437690e-03, bound:  3.15304428e-01\n",
      "Epoch: 39703 mean train loss:  3.20425630e-03, bound:  3.15304428e-01\n",
      "Epoch: 39704 mean train loss:  3.20421043e-03, bound:  3.15304428e-01\n",
      "Epoch: 39705 mean train loss:  3.20421671e-03, bound:  3.15304428e-01\n",
      "Epoch: 39706 mean train loss:  3.20410961e-03, bound:  3.15304428e-01\n",
      "Epoch: 39707 mean train loss:  3.20407213e-03, bound:  3.15304428e-01\n",
      "Epoch: 39708 mean train loss:  3.20407446e-03, bound:  3.15304399e-01\n",
      "Epoch: 39709 mean train loss:  3.20398039e-03, bound:  3.15304399e-01\n",
      "Epoch: 39710 mean train loss:  3.20395129e-03, bound:  3.15304399e-01\n",
      "Epoch: 39711 mean train loss:  3.20385885e-03, bound:  3.15304399e-01\n",
      "Epoch: 39712 mean train loss:  3.20382579e-03, bound:  3.15304399e-01\n",
      "Epoch: 39713 mean train loss:  3.20373918e-03, bound:  3.15304399e-01\n",
      "Epoch: 39714 mean train loss:  3.20374756e-03, bound:  3.15304399e-01\n",
      "Epoch: 39715 mean train loss:  3.20364907e-03, bound:  3.15304399e-01\n",
      "Epoch: 39716 mean train loss:  3.20365024e-03, bound:  3.15304399e-01\n",
      "Epoch: 39717 mean train loss:  3.20359296e-03, bound:  3.15304399e-01\n",
      "Epoch: 39718 mean train loss:  3.20350472e-03, bound:  3.15304339e-01\n",
      "Epoch: 39719 mean train loss:  3.20345559e-03, bound:  3.15304339e-01\n",
      "Epoch: 39720 mean train loss:  3.20339575e-03, bound:  3.15304339e-01\n",
      "Epoch: 39721 mean train loss:  3.20340064e-03, bound:  3.15304309e-01\n",
      "Epoch: 39722 mean train loss:  3.20334127e-03, bound:  3.15304309e-01\n",
      "Epoch: 39723 mean train loss:  3.20325093e-03, bound:  3.15304309e-01\n",
      "Epoch: 39724 mean train loss:  3.20323487e-03, bound:  3.15304309e-01\n",
      "Epoch: 39725 mean train loss:  3.20320320e-03, bound:  3.15304309e-01\n",
      "Epoch: 39726 mean train loss:  3.20312381e-03, bound:  3.15304309e-01\n",
      "Epoch: 39727 mean train loss:  3.20305815e-03, bound:  3.15304309e-01\n",
      "Epoch: 39728 mean train loss:  3.20299948e-03, bound:  3.15304279e-01\n",
      "Epoch: 39729 mean train loss:  3.20292474e-03, bound:  3.15304279e-01\n",
      "Epoch: 39730 mean train loss:  3.20290145e-03, bound:  3.15304279e-01\n",
      "Epoch: 39731 mean train loss:  3.20282811e-03, bound:  3.15304279e-01\n",
      "Epoch: 39732 mean train loss:  3.20281927e-03, bound:  3.15304279e-01\n",
      "Epoch: 39733 mean train loss:  3.20277782e-03, bound:  3.15304279e-01\n",
      "Epoch: 39734 mean train loss:  3.20267351e-03, bound:  3.15304279e-01\n",
      "Epoch: 39735 mean train loss:  3.20261344e-03, bound:  3.15304279e-01\n",
      "Epoch: 39736 mean train loss:  3.20262671e-03, bound:  3.15304279e-01\n",
      "Epoch: 39737 mean train loss:  3.20250797e-03, bound:  3.15304279e-01\n",
      "Epoch: 39738 mean train loss:  3.20248283e-03, bound:  3.15304279e-01\n",
      "Epoch: 39739 mean train loss:  3.20243975e-03, bound:  3.15304279e-01\n",
      "Epoch: 39740 mean train loss:  3.20236920e-03, bound:  3.15304220e-01\n",
      "Epoch: 39741 mean train loss:  3.20234010e-03, bound:  3.15304220e-01\n",
      "Epoch: 39742 mean train loss:  3.20229633e-03, bound:  3.15304220e-01\n",
      "Epoch: 39743 mean train loss:  3.20217572e-03, bound:  3.15304220e-01\n",
      "Epoch: 39744 mean train loss:  3.20215151e-03, bound:  3.15304190e-01\n",
      "Epoch: 39745 mean train loss:  3.20215221e-03, bound:  3.15304190e-01\n",
      "Epoch: 39746 mean train loss:  3.20204371e-03, bound:  3.15304190e-01\n",
      "Epoch: 39747 mean train loss:  3.20198922e-03, bound:  3.15304190e-01\n",
      "Epoch: 39748 mean train loss:  3.20196198e-03, bound:  3.15304190e-01\n",
      "Epoch: 39749 mean train loss:  3.20193172e-03, bound:  3.15304190e-01\n",
      "Epoch: 39750 mean train loss:  3.20183951e-03, bound:  3.15304190e-01\n",
      "Epoch: 39751 mean train loss:  3.20182857e-03, bound:  3.15304190e-01\n",
      "Epoch: 39752 mean train loss:  3.20171984e-03, bound:  3.15304190e-01\n",
      "Epoch: 39753 mean train loss:  3.20167048e-03, bound:  3.15304160e-01\n",
      "Epoch: 39754 mean train loss:  3.20167025e-03, bound:  3.15304160e-01\n",
      "Epoch: 39755 mean train loss:  3.20158852e-03, bound:  3.15304160e-01\n",
      "Epoch: 39756 mean train loss:  3.20157013e-03, bound:  3.15304160e-01\n",
      "Epoch: 39757 mean train loss:  3.20146023e-03, bound:  3.15304101e-01\n",
      "Epoch: 39758 mean train loss:  3.20144091e-03, bound:  3.15304101e-01\n",
      "Epoch: 39759 mean train loss:  3.20143229e-03, bound:  3.15304101e-01\n",
      "Epoch: 39760 mean train loss:  3.20133171e-03, bound:  3.15304101e-01\n",
      "Epoch: 39761 mean train loss:  3.20128654e-03, bound:  3.15304101e-01\n",
      "Epoch: 39762 mean train loss:  3.20124649e-03, bound:  3.15304101e-01\n",
      "Epoch: 39763 mean train loss:  3.20117129e-03, bound:  3.15304101e-01\n",
      "Epoch: 39764 mean train loss:  3.20111471e-03, bound:  3.15304101e-01\n",
      "Epoch: 39765 mean train loss:  3.20107862e-03, bound:  3.15304071e-01\n",
      "Epoch: 39766 mean train loss:  3.20102135e-03, bound:  3.15304071e-01\n",
      "Epoch: 39767 mean train loss:  3.20095336e-03, bound:  3.15304071e-01\n",
      "Epoch: 39768 mean train loss:  3.20091308e-03, bound:  3.15304071e-01\n",
      "Epoch: 39769 mean train loss:  3.20090167e-03, bound:  3.15304071e-01\n",
      "Epoch: 39770 mean train loss:  3.20082298e-03, bound:  3.15304041e-01\n",
      "Epoch: 39771 mean train loss:  3.20078572e-03, bound:  3.15304071e-01\n",
      "Epoch: 39772 mean train loss:  3.20070959e-03, bound:  3.15304041e-01\n",
      "Epoch: 39773 mean train loss:  3.20061436e-03, bound:  3.15304011e-01\n",
      "Epoch: 39774 mean train loss:  3.20064882e-03, bound:  3.15304011e-01\n",
      "Epoch: 39775 mean train loss:  3.20057059e-03, bound:  3.15304011e-01\n",
      "Epoch: 39776 mean train loss:  3.20055848e-03, bound:  3.15304011e-01\n",
      "Epoch: 39777 mean train loss:  3.20048071e-03, bound:  3.15303981e-01\n",
      "Epoch: 39778 mean train loss:  3.20037594e-03, bound:  3.15303981e-01\n",
      "Epoch: 39779 mean train loss:  3.20036081e-03, bound:  3.15303981e-01\n",
      "Epoch: 39780 mean train loss:  3.20029608e-03, bound:  3.15303981e-01\n",
      "Epoch: 39781 mean train loss:  3.20022320e-03, bound:  3.15303981e-01\n",
      "Epoch: 39782 mean train loss:  3.20014334e-03, bound:  3.15303981e-01\n",
      "Epoch: 39783 mean train loss:  3.20012891e-03, bound:  3.15303981e-01\n",
      "Epoch: 39784 mean train loss:  3.20008327e-03, bound:  3.15303981e-01\n",
      "Epoch: 39785 mean train loss:  3.20005347e-03, bound:  3.15303981e-01\n",
      "Epoch: 39786 mean train loss:  3.19999037e-03, bound:  3.15303981e-01\n",
      "Epoch: 39787 mean train loss:  3.19999689e-03, bound:  3.15303952e-01\n",
      "Epoch: 39788 mean train loss:  3.19988327e-03, bound:  3.15303952e-01\n",
      "Epoch: 39789 mean train loss:  3.19987000e-03, bound:  3.15303952e-01\n",
      "Epoch: 39790 mean train loss:  3.19982646e-03, bound:  3.15303952e-01\n",
      "Epoch: 39791 mean train loss:  3.19979526e-03, bound:  3.15303952e-01\n",
      "Epoch: 39792 mean train loss:  3.19966744e-03, bound:  3.15303952e-01\n",
      "Epoch: 39793 mean train loss:  3.19966651e-03, bound:  3.15303892e-01\n",
      "Epoch: 39794 mean train loss:  3.19963950e-03, bound:  3.15303892e-01\n",
      "Epoch: 39795 mean train loss:  3.19954031e-03, bound:  3.15303862e-01\n",
      "Epoch: 39796 mean train loss:  3.19949095e-03, bound:  3.15303892e-01\n",
      "Epoch: 39797 mean train loss:  3.19942087e-03, bound:  3.15303862e-01\n",
      "Epoch: 39798 mean train loss:  3.19938106e-03, bound:  3.15303862e-01\n",
      "Epoch: 39799 mean train loss:  3.19930026e-03, bound:  3.15303862e-01\n",
      "Epoch: 39800 mean train loss:  3.19925533e-03, bound:  3.15303862e-01\n",
      "Epoch: 39801 mean train loss:  3.19919619e-03, bound:  3.15303862e-01\n",
      "Epoch: 39802 mean train loss:  3.19912331e-03, bound:  3.15303862e-01\n",
      "Epoch: 39803 mean train loss:  3.19913751e-03, bound:  3.15303862e-01\n",
      "Epoch: 39804 mean train loss:  3.19907465e-03, bound:  3.15303862e-01\n",
      "Epoch: 39805 mean train loss:  3.19904997e-03, bound:  3.15303862e-01\n",
      "Epoch: 39806 mean train loss:  3.19901365e-03, bound:  3.15303832e-01\n",
      "Epoch: 39807 mean train loss:  3.19894636e-03, bound:  3.15303832e-01\n",
      "Epoch: 39808 mean train loss:  3.19892936e-03, bound:  3.15303832e-01\n",
      "Epoch: 39809 mean train loss:  3.19891120e-03, bound:  3.15303832e-01\n",
      "Epoch: 39810 mean train loss:  3.19887022e-03, bound:  3.15303832e-01\n",
      "Epoch: 39811 mean train loss:  3.19886184e-03, bound:  3.15303832e-01\n",
      "Epoch: 39812 mean train loss:  3.19878804e-03, bound:  3.15303832e-01\n",
      "Epoch: 39813 mean train loss:  3.19871912e-03, bound:  3.15303773e-01\n",
      "Epoch: 39814 mean train loss:  3.19865975e-03, bound:  3.15303773e-01\n",
      "Epoch: 39815 mean train loss:  3.19857453e-03, bound:  3.15303773e-01\n",
      "Epoch: 39816 mean train loss:  3.19845276e-03, bound:  3.15303773e-01\n",
      "Epoch: 39817 mean train loss:  3.19839921e-03, bound:  3.15303773e-01\n",
      "Epoch: 39818 mean train loss:  3.19835264e-03, bound:  3.15303773e-01\n",
      "Epoch: 39819 mean train loss:  3.19827860e-03, bound:  3.15303743e-01\n",
      "Epoch: 39820 mean train loss:  3.19822249e-03, bound:  3.15303743e-01\n",
      "Epoch: 39821 mean train loss:  3.19812330e-03, bound:  3.15303743e-01\n",
      "Epoch: 39822 mean train loss:  3.19811958e-03, bound:  3.15303743e-01\n",
      "Epoch: 39823 mean train loss:  3.19813890e-03, bound:  3.15303743e-01\n",
      "Epoch: 39824 mean train loss:  3.19799408e-03, bound:  3.15303743e-01\n",
      "Epoch: 39825 mean train loss:  3.19798104e-03, bound:  3.15303743e-01\n",
      "Epoch: 39826 mean train loss:  3.19791818e-03, bound:  3.15303713e-01\n",
      "Epoch: 39827 mean train loss:  3.19786230e-03, bound:  3.15303713e-01\n",
      "Epoch: 39828 mean train loss:  3.19781667e-03, bound:  3.15303713e-01\n",
      "Epoch: 39829 mean train loss:  3.19775916e-03, bound:  3.15303713e-01\n",
      "Epoch: 39830 mean train loss:  3.19770374e-03, bound:  3.15303713e-01\n",
      "Epoch: 39831 mean train loss:  3.19766579e-03, bound:  3.15303713e-01\n",
      "Epoch: 39832 mean train loss:  3.19761899e-03, bound:  3.15303713e-01\n",
      "Epoch: 39833 mean train loss:  3.19753960e-03, bound:  3.15303653e-01\n",
      "Epoch: 39834 mean train loss:  3.19750793e-03, bound:  3.15303653e-01\n",
      "Epoch: 39835 mean train loss:  3.19743622e-03, bound:  3.15303653e-01\n",
      "Epoch: 39836 mean train loss:  3.19741084e-03, bound:  3.15303653e-01\n",
      "Epoch: 39837 mean train loss:  3.19735589e-03, bound:  3.15303653e-01\n",
      "Epoch: 39838 mean train loss:  3.19729326e-03, bound:  3.15303653e-01\n",
      "Epoch: 39839 mean train loss:  3.19724926e-03, bound:  3.15303624e-01\n",
      "Epoch: 39840 mean train loss:  3.19720292e-03, bound:  3.15303624e-01\n",
      "Epoch: 39841 mean train loss:  3.19713587e-03, bound:  3.15303624e-01\n",
      "Epoch: 39842 mean train loss:  3.19708302e-03, bound:  3.15303624e-01\n",
      "Epoch: 39843 mean train loss:  3.19703319e-03, bound:  3.15303624e-01\n",
      "Epoch: 39844 mean train loss:  3.19703203e-03, bound:  3.15303624e-01\n",
      "Epoch: 39845 mean train loss:  3.19693959e-03, bound:  3.15303624e-01\n",
      "Epoch: 39846 mean train loss:  3.19688255e-03, bound:  3.15303594e-01\n",
      "Epoch: 39847 mean train loss:  3.19682620e-03, bound:  3.15303594e-01\n",
      "Epoch: 39848 mean train loss:  3.19675240e-03, bound:  3.15303594e-01\n",
      "Epoch: 39849 mean train loss:  3.19672772e-03, bound:  3.15303594e-01\n",
      "Epoch: 39850 mean train loss:  3.19669466e-03, bound:  3.15303594e-01\n",
      "Epoch: 39851 mean train loss:  3.19661829e-03, bound:  3.15303594e-01\n",
      "Epoch: 39852 mean train loss:  3.19657731e-03, bound:  3.15303594e-01\n",
      "Epoch: 39853 mean train loss:  3.19651910e-03, bound:  3.15303594e-01\n",
      "Epoch: 39854 mean train loss:  3.19648930e-03, bound:  3.15303594e-01\n",
      "Epoch: 39855 mean train loss:  3.19644343e-03, bound:  3.15303594e-01\n",
      "Epoch: 39856 mean train loss:  3.19638872e-03, bound:  3.15303534e-01\n",
      "Epoch: 39857 mean train loss:  3.19629302e-03, bound:  3.15303534e-01\n",
      "Epoch: 39858 mean train loss:  3.19625181e-03, bound:  3.15303534e-01\n",
      "Epoch: 39859 mean train loss:  3.19622760e-03, bound:  3.15303534e-01\n",
      "Epoch: 39860 mean train loss:  3.19615589e-03, bound:  3.15303534e-01\n",
      "Epoch: 39861 mean train loss:  3.19607439e-03, bound:  3.15303534e-01\n",
      "Epoch: 39862 mean train loss:  3.19607579e-03, bound:  3.15303534e-01\n",
      "Epoch: 39863 mean train loss:  3.19604017e-03, bound:  3.15303504e-01\n",
      "Epoch: 39864 mean train loss:  3.19594843e-03, bound:  3.15303504e-01\n",
      "Epoch: 39865 mean train loss:  3.19590606e-03, bound:  3.15303504e-01\n",
      "Epoch: 39866 mean train loss:  3.19588091e-03, bound:  3.15303504e-01\n",
      "Epoch: 39867 mean train loss:  3.19583551e-03, bound:  3.15303475e-01\n",
      "Epoch: 39868 mean train loss:  3.19579151e-03, bound:  3.15303475e-01\n",
      "Epoch: 39869 mean train loss:  3.19570513e-03, bound:  3.15303475e-01\n",
      "Epoch: 39870 mean train loss:  3.19563760e-03, bound:  3.15303475e-01\n",
      "Epoch: 39871 mean train loss:  3.19562317e-03, bound:  3.15303475e-01\n",
      "Epoch: 39872 mean train loss:  3.19554866e-03, bound:  3.15303475e-01\n",
      "Epoch: 39873 mean train loss:  3.19548743e-03, bound:  3.15303445e-01\n",
      "Epoch: 39874 mean train loss:  3.19546834e-03, bound:  3.15303445e-01\n",
      "Epoch: 39875 mean train loss:  3.19541153e-03, bound:  3.15303445e-01\n",
      "Epoch: 39876 mean train loss:  3.19537986e-03, bound:  3.15303415e-01\n",
      "Epoch: 39877 mean train loss:  3.19532701e-03, bound:  3.15303415e-01\n",
      "Epoch: 39878 mean train loss:  3.19531141e-03, bound:  3.15303415e-01\n",
      "Epoch: 39879 mean train loss:  3.19520524e-03, bound:  3.15303415e-01\n",
      "Epoch: 39880 mean train loss:  3.19515099e-03, bound:  3.15303415e-01\n",
      "Epoch: 39881 mean train loss:  3.19510512e-03, bound:  3.15303415e-01\n",
      "Epoch: 39882 mean train loss:  3.19506926e-03, bound:  3.15303415e-01\n",
      "Epoch: 39883 mean train loss:  3.19500477e-03, bound:  3.15303385e-01\n",
      "Epoch: 39884 mean train loss:  3.19492631e-03, bound:  3.15303385e-01\n",
      "Epoch: 39885 mean train loss:  3.19493003e-03, bound:  3.15303385e-01\n",
      "Epoch: 39886 mean train loss:  3.19481618e-03, bound:  3.15303385e-01\n",
      "Epoch: 39887 mean train loss:  3.19476170e-03, bound:  3.15303385e-01\n",
      "Epoch: 39888 mean train loss:  3.19470908e-03, bound:  3.15303385e-01\n",
      "Epoch: 39889 mean train loss:  3.19470209e-03, bound:  3.15303385e-01\n",
      "Epoch: 39890 mean train loss:  3.19458125e-03, bound:  3.15303355e-01\n",
      "Epoch: 39891 mean train loss:  3.19461012e-03, bound:  3.15303355e-01\n",
      "Epoch: 39892 mean train loss:  3.19450605e-03, bound:  3.15303326e-01\n",
      "Epoch: 39893 mean train loss:  3.19451350e-03, bound:  3.15303326e-01\n",
      "Epoch: 39894 mean train loss:  3.19440151e-03, bound:  3.15303326e-01\n",
      "Epoch: 39895 mean train loss:  3.19435541e-03, bound:  3.15303326e-01\n",
      "Epoch: 39896 mean train loss:  3.19433818e-03, bound:  3.15303326e-01\n",
      "Epoch: 39897 mean train loss:  3.19424411e-03, bound:  3.15303296e-01\n",
      "Epoch: 39898 mean train loss:  3.19420989e-03, bound:  3.15303296e-01\n",
      "Epoch: 39899 mean train loss:  3.19412723e-03, bound:  3.15303296e-01\n",
      "Epoch: 39900 mean train loss:  3.19413515e-03, bound:  3.15303296e-01\n",
      "Epoch: 39901 mean train loss:  3.19405226e-03, bound:  3.15303296e-01\n",
      "Epoch: 39902 mean train loss:  3.19398614e-03, bound:  3.15303266e-01\n",
      "Epoch: 39903 mean train loss:  3.19394353e-03, bound:  3.15303266e-01\n",
      "Epoch: 39904 mean train loss:  3.19391140e-03, bound:  3.15303266e-01\n",
      "Epoch: 39905 mean train loss:  3.19384853e-03, bound:  3.15303266e-01\n",
      "Epoch: 39906 mean train loss:  3.19380057e-03, bound:  3.15303266e-01\n",
      "Epoch: 39907 mean train loss:  3.19372048e-03, bound:  3.15303266e-01\n",
      "Epoch: 39908 mean train loss:  3.19372071e-03, bound:  3.15303266e-01\n",
      "Epoch: 39909 mean train loss:  3.19368485e-03, bound:  3.15303266e-01\n",
      "Epoch: 39910 mean train loss:  3.19354958e-03, bound:  3.15303266e-01\n",
      "Epoch: 39911 mean train loss:  3.19355703e-03, bound:  3.15303266e-01\n",
      "Epoch: 39912 mean train loss:  3.19351116e-03, bound:  3.15303266e-01\n",
      "Epoch: 39913 mean train loss:  3.19345435e-03, bound:  3.15303206e-01\n",
      "Epoch: 39914 mean train loss:  3.19340848e-03, bound:  3.15303206e-01\n",
      "Epoch: 39915 mean train loss:  3.19335074e-03, bound:  3.15303206e-01\n",
      "Epoch: 39916 mean train loss:  3.19333002e-03, bound:  3.15303177e-01\n",
      "Epoch: 39917 mean train loss:  3.19327950e-03, bound:  3.15303177e-01\n",
      "Epoch: 39918 mean train loss:  3.19322781e-03, bound:  3.15303177e-01\n",
      "Epoch: 39919 mean train loss:  3.19320802e-03, bound:  3.15303177e-01\n",
      "Epoch: 39920 mean train loss:  3.19312909e-03, bound:  3.15303177e-01\n",
      "Epoch: 39921 mean train loss:  3.19309393e-03, bound:  3.15303177e-01\n",
      "Epoch: 39922 mean train loss:  3.19303479e-03, bound:  3.15303177e-01\n",
      "Epoch: 39923 mean train loss:  3.19296541e-03, bound:  3.15303177e-01\n",
      "Epoch: 39924 mean train loss:  3.19288974e-03, bound:  3.15303177e-01\n",
      "Epoch: 39925 mean train loss:  3.19282152e-03, bound:  3.15303147e-01\n",
      "Epoch: 39926 mean train loss:  3.19275283e-03, bound:  3.15303147e-01\n",
      "Epoch: 39927 mean train loss:  3.19272000e-03, bound:  3.15303147e-01\n",
      "Epoch: 39928 mean train loss:  3.19267157e-03, bound:  3.15303147e-01\n",
      "Epoch: 39929 mean train loss:  3.19266622e-03, bound:  3.15303147e-01\n",
      "Epoch: 39930 mean train loss:  3.19268066e-03, bound:  3.15303147e-01\n",
      "Epoch: 39931 mean train loss:  3.19258845e-03, bound:  3.15303087e-01\n",
      "Epoch: 39932 mean train loss:  3.19253840e-03, bound:  3.15303147e-01\n",
      "Epoch: 39933 mean train loss:  3.19250487e-03, bound:  3.15303087e-01\n",
      "Epoch: 39934 mean train loss:  3.19236307e-03, bound:  3.15303087e-01\n",
      "Epoch: 39935 mean train loss:  3.19230673e-03, bound:  3.15303057e-01\n",
      "Epoch: 39936 mean train loss:  3.19225620e-03, bound:  3.15303057e-01\n",
      "Epoch: 39937 mean train loss:  3.19222198e-03, bound:  3.15303057e-01\n",
      "Epoch: 39938 mean train loss:  3.19216284e-03, bound:  3.15303057e-01\n",
      "Epoch: 39939 mean train loss:  3.19212885e-03, bound:  3.15303057e-01\n",
      "Epoch: 39940 mean train loss:  3.19210906e-03, bound:  3.15303057e-01\n",
      "Epoch: 39941 mean train loss:  3.19207064e-03, bound:  3.15303057e-01\n",
      "Epoch: 39942 mean train loss:  3.19196144e-03, bound:  3.15303057e-01\n",
      "Epoch: 39943 mean train loss:  3.19190603e-03, bound:  3.15303057e-01\n",
      "Epoch: 39944 mean train loss:  3.19184735e-03, bound:  3.15303057e-01\n",
      "Epoch: 39945 mean train loss:  3.19178356e-03, bound:  3.15303057e-01\n",
      "Epoch: 39946 mean train loss:  3.19174933e-03, bound:  3.15303028e-01\n",
      "Epoch: 39947 mean train loss:  3.19168158e-03, bound:  3.15303028e-01\n",
      "Epoch: 39948 mean train loss:  3.19164526e-03, bound:  3.15302998e-01\n",
      "Epoch: 39949 mean train loss:  3.19159892e-03, bound:  3.15302998e-01\n",
      "Epoch: 39950 mean train loss:  3.19154607e-03, bound:  3.15302998e-01\n",
      "Epoch: 39951 mean train loss:  3.19149019e-03, bound:  3.15302998e-01\n",
      "Epoch: 39952 mean train loss:  3.19144106e-03, bound:  3.15302968e-01\n",
      "Epoch: 39953 mean train loss:  3.19135375e-03, bound:  3.15302968e-01\n",
      "Epoch: 39954 mean train loss:  3.19133257e-03, bound:  3.15302968e-01\n",
      "Epoch: 39955 mean train loss:  3.19131766e-03, bound:  3.15302938e-01\n",
      "Epoch: 39956 mean train loss:  3.19120497e-03, bound:  3.15302938e-01\n",
      "Epoch: 39957 mean train loss:  3.19117820e-03, bound:  3.15302938e-01\n",
      "Epoch: 39958 mean train loss:  3.19114258e-03, bound:  3.15302938e-01\n",
      "Epoch: 39959 mean train loss:  3.19106644e-03, bound:  3.15302938e-01\n",
      "Epoch: 39960 mean train loss:  3.19104386e-03, bound:  3.15302938e-01\n",
      "Epoch: 39961 mean train loss:  3.19098518e-03, bound:  3.15302938e-01\n",
      "Epoch: 39962 mean train loss:  3.19093862e-03, bound:  3.15302938e-01\n",
      "Epoch: 39963 mean train loss:  3.19088809e-03, bound:  3.15302938e-01\n",
      "Epoch: 39964 mean train loss:  3.19084572e-03, bound:  3.15302938e-01\n",
      "Epoch: 39965 mean train loss:  3.19075491e-03, bound:  3.15302908e-01\n",
      "Epoch: 39966 mean train loss:  3.19071836e-03, bound:  3.15302879e-01\n",
      "Epoch: 39967 mean train loss:  3.19068413e-03, bound:  3.15302908e-01\n",
      "Epoch: 39968 mean train loss:  3.19059449e-03, bound:  3.15302879e-01\n",
      "Epoch: 39969 mean train loss:  3.19056190e-03, bound:  3.15302879e-01\n",
      "Epoch: 39970 mean train loss:  3.19052255e-03, bound:  3.15302879e-01\n",
      "Epoch: 39971 mean train loss:  3.19048110e-03, bound:  3.15302849e-01\n",
      "Epoch: 39972 mean train loss:  3.19042639e-03, bound:  3.15302849e-01\n",
      "Epoch: 39973 mean train loss:  3.19037796e-03, bound:  3.15302849e-01\n",
      "Epoch: 39974 mean train loss:  3.19030811e-03, bound:  3.15302849e-01\n",
      "Epoch: 39975 mean train loss:  3.19027575e-03, bound:  3.15302849e-01\n",
      "Epoch: 39976 mean train loss:  3.19020241e-03, bound:  3.15302849e-01\n",
      "Epoch: 39977 mean train loss:  3.19016981e-03, bound:  3.15302849e-01\n",
      "Epoch: 39978 mean train loss:  3.19009367e-03, bound:  3.15302849e-01\n",
      "Epoch: 39979 mean train loss:  3.19005200e-03, bound:  3.15302849e-01\n",
      "Epoch: 39980 mean train loss:  3.19001940e-03, bound:  3.15302819e-01\n",
      "Epoch: 39981 mean train loss:  3.18994373e-03, bound:  3.15302819e-01\n",
      "Epoch: 39982 mean train loss:  3.18991742e-03, bound:  3.15302819e-01\n",
      "Epoch: 39983 mean train loss:  3.18986829e-03, bound:  3.15302819e-01\n",
      "Epoch: 39984 mean train loss:  3.18977889e-03, bound:  3.15302819e-01\n",
      "Epoch: 39985 mean train loss:  3.18974513e-03, bound:  3.15302789e-01\n",
      "Epoch: 39986 mean train loss:  3.18969693e-03, bound:  3.15302789e-01\n",
      "Epoch: 39987 mean train loss:  3.18964431e-03, bound:  3.15302789e-01\n",
      "Epoch: 39988 mean train loss:  3.18957097e-03, bound:  3.15302759e-01\n",
      "Epoch: 39989 mean train loss:  3.18955281e-03, bound:  3.15302759e-01\n",
      "Epoch: 39990 mean train loss:  3.18950554e-03, bound:  3.15302759e-01\n",
      "Epoch: 39991 mean train loss:  3.18944524e-03, bound:  3.15302759e-01\n",
      "Epoch: 39992 mean train loss:  3.18938121e-03, bound:  3.15302759e-01\n",
      "Epoch: 39993 mean train loss:  3.18933721e-03, bound:  3.15302759e-01\n",
      "Epoch: 39994 mean train loss:  3.18929856e-03, bound:  3.15302730e-01\n",
      "Epoch: 39995 mean train loss:  3.18927295e-03, bound:  3.15302730e-01\n",
      "Epoch: 39996 mean train loss:  3.18916002e-03, bound:  3.15302730e-01\n",
      "Epoch: 39997 mean train loss:  3.18913418e-03, bound:  3.15302730e-01\n",
      "Epoch: 39998 mean train loss:  3.18910182e-03, bound:  3.15302730e-01\n",
      "Epoch: 39999 mean train loss:  3.18900892e-03, bound:  3.15302730e-01\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = train_dual(net_dual, loaders, args, A_interp_inv, H1, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NN_library/training_data/square/MNPINN_dual_{total_params}', np.vstack([losses_train, losses_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "U2 = net_dual(grid_data).detach().cpu()\n",
    "error_1, _ = PDE_loss_dual(grid_data, net_dual, A_interp_inv, H1)\n",
    "error_2, _ = PDE_loss_dual(grid_data, net_dual, A_inv, H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1b10c62aed0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAEjwAAAUvCAYAAAAbHPvqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAC4jAAAuIwF4pT92AAEAAElEQVR4nOzdebgtZ1kg+q/qnAyQSBLIQBhMMCESQkRmEuC5cBEQuu9DlGhQUUyLNHhVaMArfbuvgPbt21wBH1u7FVEMDSrTBcRuARUJzRCRDEwhSAYCxhBDJkhCprOr7h97Ve1V36qvhjXstc8+v9/z7Genqt7vfd/vq1q11mZ5yqwsyzIAAAAAAAAAAAAAAAAAAAAAAACsUL7uBgAAAAAAAAAAAAAAAAAAAAAAgN3PA48AAAAAAAAAAAAAAAAAAAAAAICV88AjAAAAAAAAAAAAAAAAAAAAAABg5TzwCAAAAAAAAAAAAAAAAAAAAAAAWDkPPAIAAAAAAAAAAAAAAAAAAAAAAFbOA48AAAAAAAAAAAAAAAAAAAAAAICV88AjAAAAAAAAAAAAAAAAAAAAAABg5TzwCAAAAAAAAAAAAAAAAAAAAAAAWDkPPAIAAAAAAAAAAAAAAAAAAAAAAFbOA48AAAAAAAAAAAAAAAAAAAAAAICV88AjAAAAAAAAAAAAAAAAAAAAAABg5TzwCAAAAAAAAAAAAAAAAAAAAAAAWDkPPAIAAAAAAAAAAAAAAAAAAAAAAFbOA48AAAAAAAAAAAAAAAAAAAAAAICV88AjAAAAAAAAAAAAAAAAAAAAAABg5TzwCAAAAAAAAAAAAAAAAAAAAAAAWDkPPAIAAAAAAAAAAAAAAAAAAAAAAFbOA48AAAAAAAAAAAAAAAAAAAAAAICV88AjAAAAAAAAAAAAAAAAAAAAAABg5TzwCAAAAAAAAAAAAAAAAAAAAAAAWDkPPAIAAAAAAAAAAAAAAAAAAAAAAFbOA48AAAAAAAAAAAAAAAAAAAAAAICV88AjAAAAAAAAAAAAAAAAAAAAAABg5TzwCAAAAAAAAAAAAAAAAAAAAAAAWDkPPAIAAAAAAAAAAAAAAAAAAAAAAFbOA48AAAAAAAAAAAAAAAAAAAAAAICV88AjAAAAAAAAAAAAAAAAAAAAAABg5TzwCAAAAAAAAAAAAAAAAAAAAAAAWDkPPAIAAAAAAAAAAAAAAAAAAAAAAFZu77obAAAAAAAAAAAAAAB2v6uvvjpcfPHF4etf/3q4/fbbwyGHHBLud7/7hZNOOik88pGPDEceeeS6WwQAAAAAAABWzAOPAAAAAAAAAAAAAGAHuOqqq8JnP/vZcOGFF4bPfvaz4eKLLw633nprffyEE04IV1999foanMOdd94Zfv/3fz/8wR/8QbjsssuScVmWhVNPPTU85znPCf/pP/2nsGfPnm3sEgAAAAAAANguWVmW5bqbAAAAAAAAAAAAAIAD0fnnnx/+n//n/wkXXnhhuOmmmzpj97cHHp1//vnhhS98YfjGN74xatwdd9wRDj300BV1BQAAAAAAAKzT3nU3AAAAAAAAAAAAAAAHqs997nPhr/7qr9bdxtL90R/9UfjX//pfh42Njcb+4447Ljz84Q8Pxx13XLjnnnvC9ddfH77whS+Eb3/722vqFAAAAAAAANhOHngEAAAAAAAAAAAAADvMIYccEh70oAeFK6+8ct2tjPbe9743/PzP/3woy7Le98xnPjO87nWvC094whNClmUzYy6++OLwgQ98ILz1rW/dzlYBAAAAAACAbeaBRwAAAAAAAAAAAACwRgcddFA47bTTwmMf+9jwuMc9Ljz2sY8Np59+evjUpz4Vnva0p627vVGuvfba8KIXvajxsKPf+q3fCi9/+cs7xz360Y8Oj370o8Ov/dqvhb17/Z84AwAAAAAAwG7l20AAAAAAAAAAAAAAWJMXvvCF4SUveUk49NBD193KUrzkJS8J3/72t+vtX/u1X+t92NE0DzsCAAAAAACA3c03ggAAAAAAAAAAAACwJkcdddS6W1iaCy64IPzFX/xFvX3qqaeGf/fv/t0aOwIAAAAAAAB2Gg88AgAAAAAAAAAAAIAD0G233RY+9alPhWuvvTZ861vfCnv27AnHHntsOPXUU8OjH/3okOf5qHxvfvObG9uvetWrwsEHH7zMlgEAAAAAAID9nAceAQAAAAAAAAAAAMAB5CMf+Uh4/etfHz75yU+Ge+65pzXm6KOPDj/3cz8XfvVXfzUcddRRvTlvvfXW8O53v7vePuyww8KP//iPL61nAAAAAAAAYHcY9/92BQAAAAAAAAAAAADYL91www3hGc94RvjhH/7h8LGPfSz5sKMq9vWvf3146EMfGv7n//yfvbn/7u/+Ltxxxx319hOf+MRw+OGHL6VvAAAAAAAAYPfYu+4GAAAAAAAAAAAAAIDVuuKKK8KznvWscNVVVzX2f8/3fE94zGMeE4477riwsbERrr766nDxxReHoihCCCHceOON4RnPeEb44Ac/GJ71rGcl8//93/99Y/uMM86o//szn/lMePvb3x7+5//8n+Gaa64Jd9xxRzj66KPDiSeeGJ7+9KeHH/uxHwunnXbaEmcLAAAAAAAA7FQeeAQAAAAAAAAAAAAAu9h3v/vd8CM/8iONhx19//d/f/i//+//O5x11llhz549jfhvfvOb4TWveU14y1veEkII4e677w4veMELwuc+97nwwAc+sLXGhRde2Ng+9dRTww033BD+9//9fw/vfve7Z+KvueaacM0114RPfvKT4dd//dfDOeecE37nd34nHH300YtOFwAAAAAAANjB8nU3AAAAAAAAAAAAAACszq/8yq+EL33pS/X2s5/97HDJJZeE5z3veTMPOwohhOOPPz78wR/8QXjjG99Y77vhhhvC//V//V/JGt/85jcb2wcffHA444wzWh92FCvLMrzzne8Mj3/848NXvvKVIVMCAAAAAAAA9lNZWZblupsAAAAAAAAAAAAAAJrOP//88LSnPa3ePuGEE8LVV189Kse1114bHvKQh4S77747hBDCiSeeGC699NJw73vfe9D4f/Ev/kX4y7/8yxDC5kOMvv71r4f73//+M3EPe9jDwj/8wz/U2w960IPCNddcE0II4aCDDgrnnntueO5zn1v3ctlll4V3vOMd4X/8j//RyHPSSSeFiy66KBxxxBGj5gkAAAAAAADsH/J1NwAAAAAAAAAAAAAArMbv//7v1w87CiGE17zmNYMfdhRCCK985Svr/7777rvDhz/84da4W265pbFdPezoAQ94QLjooovCm9/85vCc5zwnnHrqqeGRj3xkeP7znx/++3//7+HP/uzPwkEHHVSPu/LKKxs1AQAAAAAAgN3FA48AAAAAAAAAAAAAYJf667/+6/q/9+zZE84+++xR45/85CeHvXv31tuf+MQnWuOKopjZt3fv3vDBD34wnH766cn8z3/+88Mb3vCGxr7/9t/+W/jGN74xqk8AAAAAAABg/7C3PwQAAAAAAAAAAAAA2N/ceeed4aKLLqq3H/zgB4cbbrgh3HDDDaPyHHnkkfWYK6+8sjXm8MMPD9/61rca+376p386POYxj+nN/0u/9Evhd3/3d8Pll18eQgjhnnvuCe9617vCr/zKr4zqEwAAAAAAANj5PPAIAAAAAAAAAAAAAHah6667Ltxzzz319tVXXx0e8pCHLJTzpptuat1/+OGHz+z7mZ/5mUE5sywLL3jBC8JrXvOaet/555/vgUcAAAAAAACwC+XrbgAAAAAAAAAAAAAAWL4bb7xx6TlvvfXW1v1HHnlkYzvP8/D4xz9+cN4nPvGJje3LLrtsdG8AAAAAAADAzueBRwAAAAAAAAAAAACwC919991Lz1mWZev+U045pbF9n/vcJ9z73vcenPcBD3hAY3sVD2sCAAAAAAAA1s8DjwAAAAAAAAAAAABgFzr66KMb28985jNDWZYL/Vx99dWttU477bTG9iGHHDKq1zj+zjvvHDUeAAAAAAAA2D944BEAAAAAAAAAAAAA7ELHHXdcY/urX/3qymr9wA/8QGP7lltuGTU+jr/f/e63YEcAAAAAAADATuSBRwAAAAAAAAAAAACwC93nPvcJp512Wr199dVXh8svv3wltZ785CeH+9znPvX2XXfdFa688srB47/0pS81th/0oActrTcAAAAAAABg5/DAIwAAAAAAAAAAAADYpZ71rGc1tt/ylrespM4hhxwS/uW//JeNfR/+8IcHj49jn/KUpyylLwAAAAAAAGBn8cAjAAAAAAAAAAAAANilXvrSl4a9e/fW27/zO78TLr300pXUOvfccxvbv/u7vxvuvvvu3nFXXnlleP/739/YFz88CQAAAAAAANgdPPAIAAAAAAAAAAAAAHapk08+ufEgojvvvDM85znPCV/+8pdH5bnrrrvCeeed1xnzQz/0Q+EZz3hGvf2Vr3wlvOpVr+occ/vtt4cXvOAF4Z577qn3PfGJTwxPe9rTRvUHAAAAAAAA7B+ysizLdTcBAAAAAAAAAAAAAAeqa665Juzbt29m/9/93d+Fn/iJn6i3H/jAB4ZPfvKTrTkOP/zwcPTRR7ceu+2228KTnvSk8IUvfKHed6973Sv8m3/zb8JLXvKS8OAHP7h13B133BE+9alPhQ9+8IPhne98Z/jWt74V+v5Pj7/4xS+GJzzhCeGOO+6o9/3ET/xE+M3f/M3wwAc+sBF74YUXhpe85CXhoosuqvcdfPDB4ROf+ER4/OMf31kHAAAAAAAA2D954BEAAAAAAAAAAAAArNGJJ54Yvv71ry+U44UvfGE477zzksf/8R//MTzzmc8MX/nKV2aOfd/3fV942MMeFo488siwb9++8O1vfztcffXV4YorrggbGxuN2CH/p8d//ud/Hp73vOc1xuZ5Hh73uMeFE044Iezbty9cdtll4bLLLmuMy7Is/OEf/mH4V//qX/XWAAAAAAAAAPZPHngEAAAAAAAAAAAAAGu0HQ88CiGE2267LbzkJS8Jf/InfzJXjSOPPDLcfPPNg2Lf9773hRe/+MXhxhtvHBR/+OGHh7e//e3hrLPOmqs3AAAAAAAAYP+Qr7sBAAAAAAAAAAAAAGD1Dj/88PCOd7wjfP7znw8veMELwlFHHdU75gEPeED4qZ/6qfCe97wnXHfddYNr/eiP/mi49NJLw0tf+tJwv/vdLxl31FFHhZe//OXhiiuu8LAjAAAAAAAAOABkZVmW624CAAAAAAAAAAAAANheRVGEL3zhC+HLX/5yuOmmm8Itt9wSDj300HCf+9wnnHjiieHUU08ND37wgxeus2/fvvDpT386fP3rXw/f/OY3Q57n4eijjw4Pf/jDw2Mf+9iQ5/5/uAIAAAAAAMCBwgOPAAAAAAAAAAAAAAAAAAAAAACAldu77gYAAAAAgAPL1772tfC5z30uXHvtteG2224Lxx9/fDjhhBPCmWeeGQ466KC19nbxxReHyy+/PPzTP/1TCCGEBz7wgeGUU04Jj3rUo9baFwAAAAAAAAAAAAAAAOwGHngEAAAAAGyL9773veFNb3pTuOCCC1qP3/e+9w3nnHNO+PVf//Vw9NFHb1tf99xzT3jjG98Y/vAP/zBceeWVrTEnn3xyeNGLXhRe8YpXzP1Qpm9961vhwgsvDJ/97GfDZz/72XDhhReG6667rhHzta99LZx44olz5X/qU58aPv7xj881NoQQ/viP/zj87M/+7NzjAQAAAAAAAAAAAAAAoI8HHgEAAAAAK3XbbbeFn//5nw/vfOc7O+Nuuumm8Hu/93vhfe97X3jb294WnvWsZ628t8svvzw8//nPDxdffHFn3BVXXBFe/epXh/e85z3hne98Zzj55JMH5b/22mvDy172svDZz342fP3rX19GywAAAAAAAAAAAAAAALDf8sAjAAAAAGBlNjY2wjnnnBP+8i//srH/mGOOCY961KPCEUccEa688spwySWXhLIsQwgh/PM//3N47nOfG/7mb/4mPPnJT15Zb9ddd114xjOeMfMgopNPPjmcdtppoSzLcOmll4Yrr7yyPnbRRReFZz7zmeHv/u7vwrHHHttb4/rrrw/vfe97l947AAAAAAAAAAAAAAAA7I888AgAAAAAWJlXv/rVjYcdHXTQQeFNb3pTePGLXxwOPvjgev+Xv/zl8KIXvShccMEFIYQQ7rrrrnDWWWeFL37xi+H4449fel9FUYSzzjqr8bCj448/Ppx33nnhmc98ZiP2wx/+cDj33HPDddddF0II4Wtf+1r4kR/5kfDJT34yZFk2V/08z8Mpp5wSvvKVr8w/iR5f+9rXRsUfffTRK+oEAAAAAAAAAAAAAAAANmVl9f82HQAAAABgia666qrwsIc9LNxzzz31vg984APhuc99bmv8HXfcEZ7+9KfXDz0KIYR//a//dfj93//9pff29re/PfzMz/xMvX3f+943XHTRReHEE09sjf/a174WHvOYx4Sbb7653vdnf/Zn4fnPf35nnc997nPhUY96VDjppJPCYx/72PC4xz0uPPaxjw2PecxjwuGHHz7zwKSvfe1ryR76PPWpTw0f//jH623/0y8AAAAAAAAAAAAAAAA7jQceAQAAAAAr8cIXvjD8t//23+rtn/3Znw1//Md/3Dnmq1/9ajj99NPD3XffHUIIYe/eveEf/uEfwvd93/ctra+NjY3w0Ic+NHzta1+r95133nnhhS98Yee48847L5x77rn19kknnRS++tWvhjzPk2PuvPPOcMcdd4Sjjjqq9bgHHgEAAAAAAAAAAAAAAHAgSf9LHAAAAACAOd1xxx3hve99b2Pfr/7qr/aOO+WUU8JZZ51Vb+/bty/86Z/+6VJ7++QnP9l42NEDH/jA8IIXvKB33E//9E+HBz7wgfX2lVdeGT796U93jjn00EOTDzsCAAAAAAAAAAAAAACAA83edTew29xyyy2N/y/qD37wg8Mhhxyyxo4AAACAA9Vdd90V/vEf/7He/l/+l/8lHHnkketraI3uvPPOcOWVV667jW130kknhUMPPXQttT/ykY+E7373u/X2GWecER72sIcNGnvuueeGd7/73fX2+973vvDv//2/X1pv73//+xvbP/MzPxP27NnTO27Pnj3hBS94QXj961/f6O3JT37y0noD2Kl8/wEAAADsJL4D2eT7DwBYjO8/AAAAgJ3E9x+bfP8BBwYPPFqyj3/8443/D/QAAAAAO8UHPvCB8NznPnfdbazFlVdeGR7xiEesu41t96UvfSmcdtppa6n94Q9/uLH91Kc+dfDYpzzlKWHv3r1h3759IYQQLrnkkvDP//zP4bjjjlt7b0996lMbDzz60Ic+FN70pjctpS+Ancz3HwAAAMBOdqB+B+L7DwBYjO8/AAAAgJ3M9x8HFt9/cKDJ190AAAAAALD7fOlLX2psn3HGGYPHHnbYYeH0009v7Lv00kuX0tddd90Vrrjiisa+Jz7xiYPHn3nmmY3tyy+/PNx9991L6Q0AAAAAAAAAAAAAAAB2Ow88AgAAAACW7rLLLmtsn3zyyaPGn3TSSY3tL3/5ywv3FEII//AP/xA2Njbq7WOPPTbc5z73GTz+Pve5Tzj66KPr7Y2NjfDVr351Kb0t28te9rLw+Mc/Phx77LHh4IMPDve9733DQx/60PC//W//W/h//9//d8f2DQAAAAAAAAAAAAAAwO61d90N7DYPfvCDG9uvOuGp4f6HDP8HU+uWZ6uvkYVyx+TJs3E5shHxQ5cyGxDY12dfX11rlTqWqpnqt6uH1LG4dnqeqR6TJfvXZMHjXTHpNR0wPutek3hMXCtLPMYuy4r2A6k+uvYPeO2l+0jVau9vrusqdV30nfO863XS7C8VG9eue5w5j7PzrXLWORJj6tpZFJ8XjZqN9cmb+7ZyTMbsibarsXkzPpvUCNH4zWNR33ui2LyaRzU2NNXjq+2smXdaNen4RTX7ImsZ3LE/hBDynmdB9h2vawwLW0qtVSjS94wZqZdOnCPeLsv27WJqf9EeUxbR8Spssr8sssZ2qLbLye+NrbWtc5V5I7Ys8ihXtX9PNC5rjK/iy+kak5jqd6hyl80cda16XN48Xk23mL3A6th6R1SzjovCWnKFEELZ8lzUVGwoEzkS10bc05iYmXkOzFkOuKTLnhdub42O48m+45dFiM9Xc7uY3k6uUTwm0dOAG9W8cx6zf7a/jjlP5+pZq7Za8TU5OHdirbrWJ5m752PUkL82hryGunrorj/fG9i847qkrt2dZie3WZ2V6+76TnjD18+v98f/u8WB7H0f+DfhpJOPW3cbS3flFf8cfvSs31p3G+Gmm24KN910U2Pf937v947KEcdffvnlC/cVQghXXHFFZ50hvvd7vzfccMMN9fbll18eHvGIRyzc27L95//8nxvbN998c7j55pvDFVdcEf77f//v4d/+238bnvvc54bf/M3fnHnAFEAs/hxxSPD/VQMAAABYnyKEcNfUtu9ANn3gAx8Y/f+AYH9wxRVXhLPOOmvdbQCwC/n+AwAAANhJfP/RzvcfsDt54NGSHXLIIY3t+x9yn/DgQ49cTzNz2I4HHo19yFDKTn/g0dAvOobkXPSBR3nXA48SY9MPPBoX3zVm6AOPlllz6PEh18bYBwX1PbyobV/vA4/i7cSDePJlPvBoyNok+1jiA4+SDx2ar+/OBx5lQx941H5+4vjuBx41HzYUj4nj6tzxA4+mx6eOTfbn9cOJeh54tGej2dvUm8bMg5iqTxjxA4/quGgB8vh3xwOPqrq9DzxK3Im73uyW9cCjZbyh7i8PPEo9kSJ+OM7M9syTd2b3xzFF4oFH9fYk1Ub7A4/qB/Y0HnjUfAhRHbuxJzrevn/mIUZR3KDYvgce9cQ1YlM167isNa4vX3fsyP2pBycNytn+uuh7sMsiNZdxvEg+8GjcQ3um5zl03dMP3ll8Tcbmbn/g0YiHPE3HjXk41NjccdxSH3jUs6adR4fl6OuhM/ecDy6ap1Z/zqWnXIlVPOxpWVJ/M8f/u8WB7KSTjwunnfagdbexcvHDfYY45phjwrHHHrtQ3VtuuaWxfe973zscdthho3LEPXz7299eqKdK3Ns8c11Vb9utKIrw/ve/P3z0ox8Nb33rW8Pznve8dbcE7GDx54g8+D/4BwAAAHYO34FsOvnkk8Npp5227jYAYL/h+w8AAABgJ/P9xybff8Du5IFH+7HteDhRfw/L/1eoy3iQUWWVDzSqa4yMX+WDduociTVc5OFKizzEpz6W6GtozZ36gKO+42PWaGjuRR50NO88kvs7HhS0zAcc9dVaVt+p3trGjH9Y1IAHHSXG9MUNkXo40ozoQUdV/NbxZk+bG+0Pauq1HQ86Sr1hdj1QqPfBRyPfhFfx8KJFcvY90GhI7ipHvBbVEyp6c0QPOKrOW3y9TccMVF2bWw+hqB6MNNmeuvarsmXdz55GH1loPpxoZv9G6vramkcVWz+QKcpRP+Gk6iXxcJ6th5HNxlX3iviBQNWY6gEpM9vVWkU12/LF98BqTHwviWvN7E/k6eq7q68QZt9v4oevtN3zZ+fc3u88x+Nj1ftx/OCjeP1T826TXqPm/mptZtakZz5D+unL3ddbW47B/Yfm6zw1bkju3rjoM2wR2q/xMbnj+ErbnTO+K6bev2evu/a4rocTpT6v9z3UZ8zfXUMfjrTMv3dX+fCkZf79yvYrio1QFBvrbmPp4jnN87T/17zmNeG1r33tQn3cdtttje173eteo3PEY2699daFeqrs5N6W5fTTTw/Pfvazww/+4A+Gk08+ORx55JHhrrvuCtdff3244IILwrve9a7wxS9+sY7/zne+E84555zwwQ9+MDznOc9ZY+cAAAAAACxm3+Rnt9mNcwIAAAAAAIbx/QfsRh54BAAAAAAsVfxQoUMPPXR0jvihQnHOee3k3hb1kz/5k+G//Jf/0vn/veJ//V//1/Dv/t2/C3/yJ38SXvrSl9YPa9rY2AjnnHNO+MpXvhIe+MAHblfLAAAAAAAAAAAAAAAAHGDydTcAAAAAAOxuWZZty5h57OTexnrxi1/c+bCjaT/1Uz8VPvrRj4Z73/ve9b7bbrstvO51r1tVewAAAAAAAAAAAAAAABD2rruB3S7PNn92ojwrt61WFpZXaxl9ZyNzzPNksKE1hsynL1fes75d41P1U2PGxoeQPv9xrqXW7FuznuNjrpGhfafi2vYPnXOWp2oXg2qP7WtI7c36qZxFYn8ivqPGsvtO9dY2Zmjtalycu2teITEm2VNeNGrH+zv7TeTY6iXaP6TvPIrNo1rVe2Ie/84mcXHerP2/27bjwTPxibt5an9bjmTcHO8U84xZlrG1i5brMc5RxcRrVpTN+DquGl8046bPY564Z0zeV8r6goquzaJKNYkrsub2dOxGfJ1MruUiypzYX70+yur1Ul3UU60PfUfZeq1V8ygmuaucWXtcmJpj1lzfMhpTlont6PU9ky+EUJbNcx6vbzyPKnfv/qnaw3M1r424t+p9qCjTr+FU/321hxyP76dVTPX+XJTDXoNxjen31yJxLoeMnR6/yHwqqfXu662tv9S8kjWi+0Fbzb7co+MmNYuQXqtU7kpco9K2/vHVkvqEMCRXWy9tfc3kHngnK1vWZEz9Ll2v5/6acw/dFQ70+RPCBz7wgXDyySePGnPMMccsXPfwww9vbN9xxx2jc8Rj4pzz2sm9bbfHPe5x4T/8h/8QXvGKV9T73va2t4Xf+q3fCocddtgaOwMAAAAAAAAAAAAAAGC38sAjAAAAANilTj755HDaaadte92d/FChndzbOvzCL/xCeO1rXxu+853vhBBCuPvuu8PHPvax8C//5b9cc2cAAAAAAAAAAAAAAADsRh54tMPlWbnuFhqysLp+ljnXbM5c+QprDZ3fkHx5z3lI5RjSw9ixqfi2a2VsjrHx8+Qak3toTFwruUYt+wePzVPzLAaNH9tXV82t2l21isT+cbXmmk8yV6KnjnnGNUbXbNlf5+gZU8XVOfJi0P6+Yw15lGvmeNE83vVayKPaWXP/1u8stKr2tx2v9mV5+/56O3FXT+3v6mfI2HniVp1jXkX0+ujqpYqNY+r9kzUtymbczLiiGdeo335eqveasr7AykaqrfEhOj6Vr34NVRFVH3smYyfX/SRJWY2N9lfDWl8VqdhU7kh1vyrrtZuNq15rcY54f/X6L8v27a588X2zLPPu2qncif3TueoaUd9bteOcRaOnStt7U5GYa1wz7jdde7ZG35yr9+sisYZDzlM1t6LvXCb2x+PnmU98PF7vvt7ajvXNK9l3dD+Y7rEv99xxLa/4oqX+9JjKIuufuiPHt76+z6xt56PvM3Pcb2zM34tb9+5hlvn3Yt88OICUG6Es9627i+UrN9bdQQghhCOOOKKx/d3vfjfcfvvt4bDDDhuc4/rrr29sH3nkkctobaa3b33rW6NzrKq3dTjkkEPC0572tPDnf/7n9b4vfOELHngEAAAAALBf2ggh7MLvP8LO+P4DAAAAAABYB99/wG60xn9JDwAAAADsRve73/3CUUcd1dj3jW98Y1SOr3/9643thz70oQv31ZYnrjPEqnpblxNPPLGxPc9DoAAAAAAAAAAAAAAAAGCIvetuYLfLQhnyrFx3GyGEzV622zLnni0h17xP+BpTe+icR+XsOXd9ubp6So1NjUnFd11fY3PF8cmaA9aw73z05VikxtC+28bHMf3nuFhofFdMlo+7RjZzFYn942qsou9kb4n4thpDa1a1ZvdPbfeMWca9L65b95MX7TXi/fHatOzfyjlfv1l1g86z5u9GX1kUHNpj8/h44u7fVqNvzNDjqxo7UBnVyIr26z5pSI9Vzjg2tT/EPeSJ+Km4YrKvjMZW565IXG9Rqur6LIussR1C2Hr3qktU1+Dk9VJEcan99fHJ62f6U0cVGx/r2R/q6cfXeLOHEEIoy2huk3Uty7yxv16Dyeu4HhfdB2bytfRR3a/iGnF8MnfL/a06VteI+t6qncpZRPtnr+X4/atI1KxzDKzdFjMzn2h/9f5dzHGetnKUjXkMrV3pW4+usUNz9/XWdqxvbHy8zjN5VZZha//gsXPGNeY8qV+EYddN6vPUdM3UZ4GZ9W6Nmr379uVty13XGPi5pO06mqk/59+nZejP3Wen/I2+XdbxvwVA5dRTTw2f/vSn6+0rrrginHrqqYPHX3XVVTP5luH7v//7w549e8LGxub/N4Trr78+3HrrreF7vud7Bo3/zne+E2644YZ6e8+ePfv9A4/uda97NbbvuOOONXUCAAAAAAAAAAAAAADAbrf6f3EPAAAAABxwHvGIRzS2L7jggsFjb7/99vCFL3yhM9+8DjnkkHDSSSfN3dv0Q5xCCOGhD31oOOSQQ5bS27pMP8AphBCOPvroNXUCAAAAAAAAAAAAAADAbrd33Q3QlIVy3S20yrPV9ZUtKfciT++ap4exazK0Rj7gGujLleptSA9jx6au2bY8qRyja3bMo++89K3BmGth0b6HrNHMdh7nKEaNH9RnPu/1VbTun6fWPOc+nau9r7G124711YyPd+duHzNTKy8auXr3Tx2brVmNbRkztb/OHc+nUaOKjY5lobF/63c2iWtu179DmN3O8vSxEELI4+M98am4eWPGxLUoFxi76txZ0XINxTmrmL799XZ0cbSOn+wrJvuqa7morrfJdTR5LyrrCy66ljeapaaP1yOK6pqsclW19jRqZ5MkVXy8vxq2zE9O9etpskZlOXs+q9dzWWadY+r5VfONxqXytY3dim3vKx3fXrOzn+j+MzRn23tA3Gf8vlYsqfb0sa3a7etevZ8XI87TbI6y0f+Yczttej1m1iIxn77cqd7aco2dV+r8TX9Gre4Ng8eOjGubR/yZvgjj1rLt89bY81Hnat1b365a9X0mTtWqa474TB3Pq88q/2Yuw7he2P+VZRHKcqM/cD9Tll2v8O31wz/8w+EP/uAP6u3zzz9/8NhPfOITYd++ffX2ox71qHDccccttbevfvWrjd6e+cxnDhobz+PZz3720vpal8985jON7Qc84AFr6gQAAAAAgMVshBD29Ubtf3bfdzoAAAAAAMBQvv+A3Wh1/7IeAAAAADhgPetZzwr3ute96u0LLrggfOUrXxk09rzzzmts/8iP/MgyW5vJ9/a3vz1sbPR/WbCxsRHe8Y53rLS37fbFL34xfPGLX2zse+pTn7qeZgAAAAAAAAAAAAAAANj1PPAIAAAAAFi6e9/73uHss89u7Hv961/fO+6rX/1qeP/7319v7927N/zkT/7kUnt7ylOeEh7ykIfU29dcc83Mg4zavOMd7wj/9E//VG+fdNJJ4UlPetJSe9tOGxsb4d/8m3/T2HfyySeHhz/84WvqCAAAAAAAAAAAAAAAgN3OA49WLAvlqJ9VyLNy4Z+xsqwc/DN4Hj0/i/SVrDnHmgytkYey9WdIzr4+U+OHzLGvZuqa7eqhr++h/Q6ZR2pMX84h1+O86xzv71rr1JgsL0OWT+coQp4V6fiO+ST7jGr0zTvLisbPMmrNc+7Tudr7Glu77Vhfzfj4TO68rH+SfVbXRZUrL0LIi95rNY6fHjNzLDEmOd9s8lP131U/C5s/MzfsLIQ8C1keQja1PaPaXwVm+eyxejvf/EluJ+LjuEb9npj4eFeuEEKZ570/c8v3LP7TY1DfqbUYen7a1rDOF10P9ZjmT/K6mlyPW9d6mP0AUV3X1XUe7U+9juv90euq8Rqc1I1jB++P1PeNfPb12Dum2k7cp1LvJ233hNkc7e8L6fj056Xe9++ee3n3PT39vhVC+jPF2NpD3s9mPyNE7+8da5bO0f45o6/fNot+9unrrSvXvPNq/SwafW4dOnbsZ9jOMT2f+Zd5PoZ+vu37+6rrnSlVa8xn69S8Vv23apexf7/vTz+wTq997WvDQQcdVG+fd9554YMf/GAy/s477wznnntuuPvuu+t9P/dzPxdOOumkzjpZljV+zj///M74PXv2hNe97nWNfa94xSvC1VdfnRxz9dVXzzwc6D/8h/8Q8kU+zy/R7/zO74Q777xzcPzdd98dfv7nfz589KMfbex/zWtes+zWAAAAAAAAAAAAAAAAoLYz/jUOAAAAALDrfN/3fV942cte1th39tlnh9/93d9tPNQohBAuu+yy8PSnPz18+tOfrvfd7373W9kDeH7qp34qPOEJT6i3b7rppnDmmWeGv/qrv5qJ/chHPhLOOOOMcPPNN9f7zjzzzHDOOecMqnXDDTeEq6++uvUnds0117TGXXPNNZ01fvmXfzk85CEPCb/yK78SPvOZz4R9+/a1xu3bty/8+Z//eXjCE54Q/viP/7hx7Id+6IfCT/3UTw2aEwAAAAAAAAAAAAAAAMxj77obONDlWbnuFjpl29Dfsp66tUiv85yHsfXyMCx+SN6+flM5hswzNTZL9J/K2TWPoWNWMY+hx8dcE8k1i/bPdZ3lcY6is8bQXtpyx9LnqUjsT+dL1RrTb1eerr7m7aHtWJwjrjl7PMo9Od7Wa1YfKzv7namVF+212sSxqX6j3DP95y3jF72Z51nzd9aSsDpWb+fN36m4OH7o/rExIYRyYFx3rT2L51h2zWIjeSiec1ZMrpuha1HHV+ctbzlW7ZtsF9W5T7zui+oajfe3xFbX9WSzLDb/q7rOy7rmnkbNbJK8iq/Ur4+pWmU8ZiNxjdY9NePrXHnU49S9pD4P9bGyc0xZ5s15xvOYjC/LrHNfd45mrb74zho9tStD5hH3V4+N+ozfB4uRtTv7jvbH29X7fVHOf0+p+i+iGnW/A9e4K2c8dmju6bVN5UqvTfu84pptNarPs2UYNrZv7do+Kw1d7/hvgyJ0r2lXn6keunK15WvTdwV2f/oa9zfTkH7arOJv6ngN2b2Kcl8oyvYHwuzPduKc/tN/+k/h0ksvDR/60IdCCCHcc8894Zd+6ZfCb/zGb4RHP/rR4Xu+53vCVVddFS6++OJQlluv64MPPji8//3vD8cff/xK+srzPLz//e8PT3ziE8M3vvGNEEII3/zmN8OznvWs8NCHPjScdtppoSzLcOmll4YrrriiMfbEE08M73vf+0KWDbtnvOpVrwpve9vbBsU+5SlPad1/wgkntD4gadp1110X3vCGN4Q3vOEN4ZBDDgmnnXZaOP7448MRRxwR7rnnnnD99deHiy66KNx2220zYx/72MeOmhMAAAAAADvRvsnPbrMb5wQAAAAAAAzj+w/YjTzwCAAAAABYmT179oR3v/vd4UUvelF417veVe+//vrrw4c//OHWMccee2x429velnz4z7Icf/zx4a//+q/D85///HDJJZfU+y+//PJw+eWXt4559KMfHd71rneF4447bqW9Lequu+4KF198cW9clmXhl37pl8LrX//6cOihh25DZwAAAAAAAAAAAAAAABzIPPBoxfKsDHlW9geuQLaGuvkKci5zHmPPxTy18zBszJjcfX335eoanxqb9cwjlbMt35jYrv3zzGPo8THXxthaY+afnnvRGde33TiWj1/fzZxFYv/4WmPPfSpPZ19Lqt2WK645ezzK1dX/5Fiyfl40jte1Bu5v66/qJx4zU7M6nprP1E1/KzY0j9W/s0lcc3vmd6xtf543f3fFtsYNeLfqiSmH5Ejm3jP/2ME1BvZXtL9+ZvN19FxsNDbjtcmqGtX+eDvupXEeozEDVddZWV+Q0Wu03Nqeiamu5SLKVb1uimq72e/M/iK9ZtXrs4rN6nlOcpTt13L9uq56KtpeH1X/WeeY6j5WlpsH4vtEGY+f6iu+X9X7kzmater5tNwb2+oOqR3njNem7R47Mza6t8f9xu+TRaJ2I0c0n9mazf3JuJZ5DR1b9T3Tb09PjXkkclbG5p7eP7S/eDsel6rZGjt5vVev/9R1NXbthoyJa9TjovtUEWZz9+WIe6hz9d1TOqRq1LV6xo+5e8/7d19fj/NY19/wq7Lb5sP+6/DDDw/vfOc7w9lnnx3e+MY3hr/7u79rjbvvfe8bzjnnnPC6170uHHPMMdvS2ymnnBI+85nPhDe+8Y3hLW95S7jqqqta40466aTwohe9KLzyla8MBx100Lb0NsZv/uZvho997GPhM5/5TLjxxht744855pjw4z/+4+EXf/EXw8Me9rBt6BAAAAAAAAAAAAAAAAA88AgAAAAA2CZnn312OPvss8PXvva1cPHFF4drr7023H777eH+979/OOGEE8KTnvSkcPDBB4/OW5aLPdzroIMOCq9+9avDq1/96nDRRReFr371q+Haa68NIYTwgAc8IJxyyinhMY95zNz5zzvvvHDeeect1GOfV73qVeFVr3pVCCGEa665JvzDP/xDuOaaa8KNN94Y7rjjjrBnz55w1FFHhaOPPjr84A/+YDjppJNW2g8AAAAAAAAAAAAAAAC08cCjNcmyxf4R1nbJt6HGstYiXyDPPD3kYdyYoTWGzKMvV1+OrvFZYl6pnKlcXT2kxozN1TmPBddoaJ6umKF9t43P8vnGJtc2ka+7z2JQD2PqjT33XX0n+1tS7bZccc3Z41GuxPjOeVXH8qKRs2tMWw+N+ChXakzcb4j2d/aQVbXi39lkbHN75nedJ2/fH0IIed78Xe/P2uNS44fun1IOiGnm3DMuvjPXCt+Nx+YuWl538VyLjcZmtXZZNbaqGW+31cjjC6sOmvyqronE/WDynlZWF2hRVge2YibXdVlkze0qYKO6VqsaeWN/Kr7xepkMLavXYpVj5P6qx7j3zTGbsWVZrcnkWDWv+vUcmvON7mvV+HhdpnOUZdRHan+8tolaXWMWrV3XKmbvKfE9cTZ3d7/x+2dRttSYWYNx88gnPRTReenKmc5VtvaZim+LqWsvmHs6X3VsaI54Ox7XNZ+Z2PgekaiZOtdt76t9Y+Ia8bi615bP5EWiz1QPda4Bn93art+uGqlasXnewdrv6Gmr/Nu6b37s/8qwEcpy37rbWLoybPQH7QAPechDwkMe8pB1t9HqMY95zEIPN9oJHvSgB4UHPehB624DAAAAAIBttxFC2H3ff4T95PsPAAAAAABgFXz/AbvRdjzPBgAAAAAAAAAAAAAAAAAAAAAAOMB54BEAAAAAAAAAAAAAAAAAAAAAALBye9fdwG6XZWXIsnIttdfxNKtVzDVfYs6x/eVhfO2hNcbMqy9nX66u8VnPHFO5Uzm7ekmNGZurcz4rXKuhsUP7bhuf5c19eVbMnastX1d/W7mG1eyq0TtmZK5UT91jllO7rX4cO5Nz5niRrFGNrY/l6blOHx8zru4v7xmTt/e5FV/9Dq1xrbWHvhlVgXkW9ZS3/3df7CL7p5QDYjZz7RkWN2cfO0Zbr0V8HUVrUWyEELbWMqviq1zxdlvu+lzHMZPjRXX9JF4HRfPabUuxdWwSW0xq1q+1ydFJfPW6KquaxZ72/SGErEpeNCr0inPN5Cm3XgP163GyZmVZrUlzPlv3g2atsj7eHD/9Oi/jHNX+srl/uq/pHNX4rflFvQ4a01073h/na8QMzB33u3W82ff0+2vRswaptYp7qY5XnweKqZp9OVM1qj5nemx5fxza39Dcbflm5zosR9+46fn09Vl9Hi7DsL5TPQ4ZU+lb/7a1iv82KUL3+enKNZM78fmobY5dtWJDas/0MnrElp5PUaOt62/5Zdst8wAAAAAAAAAAAAAAAABgd9iP/oU9AAAAAAAAAAAAAAAAAAAAAACwv9q77gYOVPvLk6ayrFx5jXxJNRbpNQ/jx46tN3SeQ/L25erLkXXMd97cXePiMWNzpOK3Y63GxA7tf2Y7nx2XZ8Wo2jM1o5zd56dI7O9Zu5a++8Ym9ydypXrrHrNY7TE1Z3ImcnWN65tHfTwv5trf2n80puq7Ph7t35pflCebPbb1e/rg1Hb8O0u8I+Yd75QzuUfm6MhddtVt5NgzLG5k/YWsIm+Rfj0k68ZjqrUqNkIIW2ucVXHV+Hi7LWd97uOYyfFisr+6/ovq2t7cLOuLtpwZWl3XVczWdjNu67VVzaPabMZP30rKup89jRxZNY+iGVftL4voWq9yV6/B6RplM7a6/5RltSbVmMn8Jq/zalw93/p4NL4lZqtWlCu6N8Y16v1RreaYvHNMqna8P+6hEZOYT1/ureOza1SJ33eL5Dq3r11qe/rzQdFSd0iOuMciMb/psZXUOqdyb/Xa3UvbsSH9DR03dA3iz8j1/WDAPOM++9YutUap8a05Ep/pi9Cfqy1fm77P0kPPzxhD+kpZ1d/ZA94N2U+V5UYoy33rbmPpynJj3S0AAAAAAABrsxFC2H3ff2zOCwAAAAAAODD5/gN2o/3luTsAAAAAAAAAAAAAAAAAAAAAAMB+bO+6G9jtsrC+p0plWbmmyiHkK6i9jPnkYb4cY2qPnfuQ3H05+3JkA+adqpHKPTZ+mbm2Zc2WcM6Xcc3GOXq38+Z21zpkWTGoZir30HFjciZ76qg99jpJ5UrVbhszk3voPLquibxoxNQ1F9zf3l/Z3J9H/eZxzkTfect/59lkbHN75ncWvTvWx+P9+WxM27EF9pep+NYce4bHdvWyiFXknKdWkXjNxGOquGjt4quqPrvTeVO5hoqumer9sAxT+6vre5K6uu7LvNlXWWTN49WAYjKv6rU3eTFU8Y36Ua0yGjN0fyjyZr6pmGpI3e/kflRWE6p7qI5P5lNG84vGT+eI74llIlc979T+qFZzTNR3z5j4/prqoauPeD7Dc89el3Hf1ftxkVzn6Dwk1qxN35r05Yw/KxQdNfvOcSVVo4h66cpV7U+NHTouVberRp0zumd0zbNvHfvOaaqHthxt9Ru5ojtsEdrjhnxW7bsG+z5zd80nZexn6CGvk0Xt70+KXv0KAQAAAAAAAAAAAAAAAMBw+/u/2wMAAAAAAAAAAAAAAAAAAAAAAPYDe9fdwIEuy8p1t9Ap34b+lrUGeZg/zyI9DF2joTXGrHlfziyxJkNqpHKnxnb1sqxcXTX65jR0/YfEzVsr3p/ls3F5Vgwb25Orq8dsYI2uPrvi58uV6CkR31ljwdqjaubxeWnP1Xm+8qIR01V/lKk8dc48WueqdnU8Nb88ypOly2bVoxTzjqBGjUlcHj2DMd4elCsxJrG/HFMj37OcXuaxzFzLFPdVtL+O67j4eLWmxUYIYet8NK6caszMGlS5EvuLyf74mq/Dpl+D0bGNeH/1OigbJaprvaxe50W1XfW8NZP4WFYlj8cU0XXWGz97bdT3m6q/Ipvs3xxTltXaVPPJGuPKcrI9OV6Nn85RqXLFsfE9r86Z2t9y34v7TtWM47d6bc6n7VjcR318YO7U+Om+6/WeqN6fi8Q6p/pu2199ZigS5yGlr/+2zxBFop9UjtQ84vl3xcb7Z9Zu1FrN1h1So46b3AfKkF6HvnWM+x46rq2ftvptueqcib8Jiq439ESNWKpmXXvE3zapefZZ5d/UffNjFyj2hbLYt+4ulm83zgkAAAAAABho3+Rnt9mNcwIAAAAAAIbx/QfsRjv0X9ADAAAAAAAAAAAAAAAAAAAAAAC7iQceAQAAAAAAAAAAAAAAAAAAAAAAK7d33Q3sdlkWQpaVa+0hX0P9Vcw5D4vnnLevedZwaK0xuftyZj1r1FWrL3dqbGpcV75l5VpkPmPj+up15Yr3Z3lzO8+KJeaKjrfk7q2RL2eeQ3Km+ovjO3OPnMfQmp01Zvpr5qxyJc9XvhWfnNskpjoej+3b35Ur7j9E+2fWv9rOqnzR7xBCyLPQqtpf/c7ynvjoWYxtcTMxiec3JvaXqfjWHHuGx3b1Mo9l5toOcb9F0X58Zv9kjYuNEELz/NRnvxozsyZVrsT+orrequ3qGt+KLOsqZWNodd1Xx7e2JzaqazvqYaMZPz2mulWk7jb1mCqubH+dVK/lcvpeMqlfFlkjpmpra38xyV2tTVWzOa6q3ZhH0ewnzhXfO+Je4vmk9k/n6qvZHz+72qk+kscT84rHt84j0W/1fl1E69yXe7rXal/1OaKIzsPMtZCaX0f/qX5jY8/x9OeVohx3ncysXc+4rrFDa9RxoXlfSNVr66evdmrc9NhYX66unCF0/31VtMyxTd9n6a7raqafOf9WS12Xy7Duv+WXJVvdEgEAAAAAAAAAAAAAAADAaPvZv6QHAAAAAAAAAAAAAAAAAAAAAAD2R3vX3QBNeVauu4VW2Tb0lYfl1Zi333nWf2ytMTX6cmc9azakVqpGamxvTx3Hx+Yc21tf/SHHh9QYmjPen+XN7Twr+sekcs/kisfN5h6bsze+6zwkc7X3NbZ2Z18L1u68RmZimzmrXMlzn3eclyimyjF2f9Vj6zrEY+L55GUUF4+Pf2dTY6N98e8s8YzFPG/+rvdn6djUds/+MhXfmmPP8NhlG9PndtUq0tdub+54bFyzOl6tebFRH6rOWRbHzuSq9sfzmewvquts9nVRvZeWVZUqJp5y9dqqeiuq19pkezIgK7JJ71sJsklfZVHVKBr7q1rlwP11nkZ/7TFb94YQ7d+ML8tobaJxZTn9Om+uXzJXFD/TSzW+bN/fONZTcys+74xv9JXoIz4e76+PR/NK5Z3OkVqj6v276Omh9Xws2OfYeU/3W+nre0jueA2G5hg6bsjYvrhKXavlM3gZhs156Bo2cifOSV+urpxdeUPo/9usCOmxQ2qP7afLMv+GTq0hu1i5EUK5b91dLF+50R8DAAAAAADsUhshhF34/Ufw/QcAAAAAABy4fP8Bu9E2/ot6AAAAAAAAAAAAAAAAAAAAAADgQLV33Q2M8ZWvfCV8/vOfD9dcc0244447wqGHHhqOPfbYcPLJJ4dHPvKR4bDDDlt3izPyrAx5Vq67jRBCCNka+sjD8msuYx7znpMxtcfWGJI761nPvppDaqRypMam9nf1ssxc8+acp8bQnPHxLB9/vc3kGLg2WVYMytc4luhv7Jp2zTPZ1xJqp3MsoebA3FXO5LnPo/ipuDhm7D2ujq96aKmZ7CsaW8vjuFEtdcsnyfLEMxfzqFgqrnXsGp/juM7a8xjbb1d80f5aS46N46vj1f58z1TsRntsX+04Z72/42IuqtdDc3dWbu4v6xdC9LqpS2SN/dO3ibJ6PU7qV8eq/VlVdM79IYRQllX9ZkxZTGpmzflt7S8m4/NoXtG4KVWt6h4R59qKy1vjKlXuuvcBx9K5onlE8Y2+BvYRzz3VSypvW+5Un3HO+Ly1rVEsn+QuEmsw77y76lefQ4pEjlTutnxxrqFzHzMuPja2/1R8CFuf08sw7jqKP8u15u7JMSZXV94hNepaPX+XFHN8eBj7GWjI62KsnfK3+7Lt1nkBwG6zP37/AQAAAAAA0MX3HwAAAAAApOz4Bx7dcsst4bd/+7fDW9/61vCNb3wjGbdnz57wgz/4g+Hss88Or371q7exQwAAAAAAgHF8/wEAAAAAAOw2vv8AAAAAAGCIHf3Ao/e85z3hpS99abjxxht7Yzc2NsJFF10Urrnmml31P3hnWbnuFlrlYXV9LWPO+YI55ulhaM0xubOB69xXu6vmvGNT+7vyLSvXkDXsi1nJ+RoYm2fF4HHxsSxPrUmR2D8bn84x57WQyNfZVzRm7LXRVXfRmqElb5yzL1d9PG8/143xeU+/k+MzYxPj4vjNfYkxeZwzmntWxcW/s8m46VxZ++8qqNqe6Tdv399l5JhynhpjFZO1XUatZebaDlWfReKaHBrftj/fE0II9bth1rs2k+NFGcVV+6vrcbbX6nVQ1hd+2RgaH9/anhwvq+0q91SPG4kxk9Cy6qfYk5jXJH7yuq7is+kaVa6y+VqraxZZI0c1dGt/MRlfrVE1/9nXbt1HGc0rio1zxvfOmZ6mj5XNY33zimtu5Zm9VtJjo/7imn3Hp+aXyr01r+baVJ9Lit41naqRWKPqc0YRzb0vZzyfMTFx/33j2uaRyjW7du05U+O66m+t2bj+U/EhzH5+37qndOeMc09L9VXXGJGrLV+b1OekruukUXvE34tFGJYztoy/G4fOh92rLDdCWe5bdxtLV5Yb624BgAF8/wEAAADAamxMfnab3TgngN3H9x8AAAAArIbvP2A32rEPPHrd614XXvva187s/97v/d5wyimnhGOOOSbceeed4Zvf/Gb44he/GG6//fbtbxIAAAAAAGAE338AAAAAAAC7je8/AAAAAAAYY0c+8OiNb3zjzP/Y/RM/8RPh3/7bfxtOP/30mfiiKMIFF1wQ/r//7/8LH/nIR7apSwAAAAAAgOF8/wEAAAAAAOw2vv8AAAAAAGCsHffAo89//vPh1a9+db190EEHhT/90z8NZ599dnJMnufhSU96UnjSk54U9u3btx1tDpZlZciyct1ttMrD9vW1ijXIl5BzbF/z1BxaIxtxPvr66KvZNb5vbOr4PDnH5hqylovMfWiNobFZ3jyeZ0Xv+ORazeQaN494/KCaA3vZii9a93ePWbx2qu7YmmFA7jhncp3z9nM95DzEOfr2V33P5J7ersZU+2ZyVr+jedTjq9/Z5Hhzu/Hf1e86KJKn9mf9ccmxif3rUEzO/TJ6Wmau7cgd5yrS94RGfF/clHIyJuvtv8oZH5/sL6b3b0xCJ9f35P24DNU1XTZTVqrXXtVbUb2uJttTA7Iim/Q/eS1O+qpfaRvVa6tZaya+iF4nU1Jjq7lWx6scW/eQEO1vTrRsW+MoR1lG/Vdjo5xl2cwV99Q4FueO7rd9NbfytNceNra9ZqrHttypnFvzavZXvb8XA2ql1qjarj53FAPXve39MTXn1PEx/Q851hWXGtfWQyp2ds3G9d/2eWxmbHxPiXJWuuYf10n1F0vl7PocGeeO9X3+7TuPrf0M/DuoCONz99mpf6sv24EyTwDYH+y27z8AAAAAAAB8/wEAAAAAwDx20BMCQti3b1/4V//qXzX+R+s3v/nNnf9jd2zv3h33DCcAAAAAAOAA5vsPAAAAAABgt/H9BwAAAAAA89pR/+vwe97znnDxxRfX209/+tPDueeeu8aOdrY8lOtuIWTZ6nrIl5h73j7n6WForWzE+evro6/mkHmkcsybu2tc6tjYXEPWeug5HHON9MVmefN4nhWd49vyDV2jrC93Pv48JPcncsU9DBvTvwZd47vqxmOS52vEfPpy1sfz9vMxM4+puJmYybG+/VX/yeNt/UVjt/qJ45r7F5JPkuVRsnh7pyk2Nn/ne+YcPznHy5hnEV2Ty1y7VeYeqqrZtmb1vsl5mJyXchKT9fZfHU/tDyGUWWhTvU+X0dCsnOyvXyjR62qSOpsaV1av16LanvxHsTmv6nVb7c+qfqP41P7NfZM1ifrYqtU8XhZZo3a8RFvHt2qU1WLUNZo5ymgtZ2s1z1eVr+1eH/c3k3tgza342Xt7GZ3cuI/ZHN29NHNnnTnjXFV/qZ5mzldLHynV55CiJ3eb1Jz7jlefV4qB563t2NAcfT1M5xh6PcWft+LxlbZ5JPuOPvtv3UOaPXTljmvE/cXG5Ezl7qvRV7PN0Gs3tsjfv0WYryYHgPKeEIp71t3F8pW7cE4Au4DvPwAAAADYHhshhH29UfufjXU3AEAL338AAAAAsD18/wG70Y562sCb3/zmxvb/+X/+n2vqBAAAAAAAYDl8/wEAAAAAAOw2vv8AAAAAAGBee9fdQOWKK64IH//4x+vtE088MTztaU9bY0fLkYUy5KFcdxuDZdn29ZovqdYyep6nl7F1s4HXwZhe+noYkiuVI7W/L2dXT2Nzju1tSM4xOYbGZXlqXsWgGl31UrmzKPfQca2xqXXuyNHWQ9fYsedy6Ly7YxP9z/Q2/zzq43n7+ZjpbRI3nSfO0XvN5VHueFyip/axUa2siot/Z5Nxze369/R/Zz3PUsyj49M52o4vQVZsrkU5T+5i8nTUfM98xYuO+8C8c41zLnPNlpm7Gtu1BmNzVOdhcl6qc1pfRfPUqq7Z+DVTVK+TKvtku6iGlZO9WbQ9sTH9+qhyT/qtN5tjqv1l9bquXoQ9+6ePhaJ5zqp7Q3y87rfImnFl1np8M2byWiqrNavWpJmj7qkjV2u+6WOJ/mZy99Ss90e1++p3991esy0mPta37pXqc0zRs4ZtOdI5i0nO6BrpyB3XqMzMa+A82vL15erL0TfvthzJ87PEeaTG1GND8x4yc7zl80Db3KZrVZI1Oz5jpHKnasRSNcf2MaanMbbzb/IicU7XoZr30L9FAYDV2a3ffwAAAAAAAAcu338AAAAAALCI5T9VYE4f+9jHGttPf/rTQ5btnH8oCAAAAAAAMJbvPwAAAAAAgN3G9x8AAAAAACxi77obqPz93/99Y/uMM84IIYRQlmX46Ec/Gv7kT/4kfOYznwn/9E//FPbt2xeOPvro8NCHPjT80A/9UHj+858fTjzxxDV0vTNlWbm22vkKay9jXvP2N0/tLAwbM6anvj76cg2ZRyomlXs7c3bVWsbcx8SFEEKWp+ZVDMrdVSvOPXSt4nGdNVI5E/PKUvNKxHfWGFg7VbOr7kzugfMZM486Ni9a42ZyTeKSx9tqRjV61zKOn66RWpM8zMa29lKNy5q/u9SxO+bZiiErNtemnKenYqO5ne9ZvKEicX2P7S+VZ55cy8zdNbYrX9u4+Fi1/jPnJdVTlbPt+ORYMTlWva4n13D1fl6G6pouG8Oq109ZvZ4mWcti63VVvYbKutaeRq1s0lc1oro9ldHxkNrfcawsJvPIysbxar51/3FcmTWON2Mmr6Vq0vE9ZECutnyNY2V7f5U4d+/+lvtcqn49r8TYob30HWsTr231/l8MyN23Xak+pxQ98wxhdq6p2n37x8wjtT+VY+i81zWP+DPcTP/R3wz1vaaj3zo2Mce+mkNy99Xoq9lmSB9DehpiaN+rkA/8O5CdoSw3QlnuW3cbS1eWG/1BAGwr338AAAAAsH02Qgi77/uPzXkBsJP4/gMAAACA7eP7D9iNdswDjy688MLG9qmnnhquvvrq8HM/93Phb//2b2fiv/GNb4RvfOMb4aMf/Wj4tV/7tfDzP//z4Td/8zfDve997+1qGQAAAAAAoJPvPwAAAAAAgN3G9x8AAAAAACxixzzw6Jvf/GZj+7vf/W543OMeF2644Ybesffcc0/4r//1v4YLLrgg/I//8T/C8ccfv5Serr/++vCtb31r1JgrrrhiKbUBAAAAAID9n+8/AAAAAACA3cb3HwAAAAAALGLHPPDolltuaWyfe+659f/Yfdhhh4WXvOQl4dnPfnZ40IMeFG6//fbw+c9/Prz1rW8Nn/zkJ+sxl1xySXje854XPv7xj4eDDjpo4Z7+63/9r+F1r3vdwnm6ZFm50vzLkm9Dn8tci3n7naeHLIwbM6a3vn6G5urKM2+N1Lgha7isnEPm39fPXOc8T/VTDKrRt93MGccmauTDcybXOTGvoTUH1YjnM7DmPLXCnLnb8s3Uz6McyR6KxvHWeaRiov3VfOLjM3mq49M9xWOrvuvYKi7+nYVO08ezfNiYtrGLKiZrkefdcRNZ0Vy7cuC4Zs2N7uP5nvE569ztr7mh81tZrqG5FzXdW1+NyTpXV3bWey1U+dqOT44V1bUcvV4mVcp46EzK5mt2OqZ6mZTVa7Kotiev30mSeuTG8NdJ/XpO5Sw2c1X3lOp4Nd9q/ExcudXDbEz0WiqjtYtybcVlrfka85nkrnLG96+4z5ncif2NGon6yXn1jpu9x8f9pOY+czyaf1y7a2wsdbz63FLMXNSz9drOUVfu1PpXn2uKtvPRM4+tvtM5Uj301e+bx+za9c8jrh2Prcz00vK3RRm61z9VK1UzVXtIjb5aXfo+uw/pZ6hl/W05zzwBANocqN9/AAAAAAAAu5fvPwAAAAAAWMQS/rX74u66665w1113NfZdc801IYQQHv7wh4fLLrssvOENbwhPf/rTw/d///eHRz/60eHcc88Nn/jEJ8Ib3vCGxrgLLrggvP71r9+23gEAAAAAANr4/gMAAAAAANhtfP8BAAAAAMCi9q67gRBC2NjYaN1/xBFHhA9/+MPhwQ9+cHLsK1/5yvBP//RP4bd+67fqfb/1W78VXv7yl4fDDz986b2OlYUyZFm57jYa8m3sZxVzX0b/8/aVhfHjhvY7pqe+nH25htRK1UiN3c6cQ9Z0GWsQQghZPmRexUI1htTLohpjc7fFD63VH5/uJXluh86zY/1nci84n0FrlHefhzp+Epdcm6k8VUxybD7s+FbC2f1bY6vfcT/x72wyrrk987tLnndvr0JRzFUrK9qvkRBCKOftu2j/bNOQ7xmZM93nZr4RvaZybcd5WkTVX6L/6nzNnNOZeU0fj45V5y51ndfh0eu/LCd7q9fP1vH6v6qy9T2g6rfqf/Ifxea1Ub3Oq/1ZVTyODyGEojmP1NiyyFqP1+MScSGEUJazc2vGFpO4SS9VXNFcy7p2Il9nzirHZEzZkzve33osMZ+tsd09pMa19ZOa+8x5iXqsPvsUZf/9t28NZnMXk9zp139qvftyp0x/lovn1LdmcY6iJ67tWGo9x69deh59Y1PzaM0R3W+q+0yqVh3Xcz66PlP3XWt9n0WHXgtD+2kz5PWwqJ32t/y85vk79oBRbIRQ7Ft3F8s35LMoANtmN3//AQAAAMBOtG/ys9vsxjkB7L98/wEAAADA9vL9B+xGO+KBR/e+971DnuehiP5x+ite8YrO/7G78hu/8RvhrW99a/j2t78dQgjhpptuCh/60IfCj/3Yjy3U1y/8wi+MznHFFVeEs846a6G6AAAAAADA/s/3HwAAAAAAwG7j+w8AAAAAABa1Ix54FEIIhx12WLj11lsb+37mZ35m8Ngf/dEfDX/8x39c7zv//PMX/h+8jz322HDsscculGMV8qxcdwu1bIW9LGOe8/aXhflrD+17TG/Lytl1vK9Gamxq/5Cel5lzkbk34vIx56Vo3T90XvOcj5kceXfOthqpOWap+STjx5+P2X6H1ew8fwPnM3QerXF5lGsyZiZ2EpfM2XI8eSwvBx2v93ddu3VsVDNLDxkkyxdMMFL1xXjeUzf6Ar03vkMW54qUC+QORfv/h6MZ+Z6B+Tp6Hdpnz3wXWculqvqor4nJGk3WtDov9SXeN69G7smoYlIjfv1PXj9lnX3yeiqi41NLFUWGrKxyVLkn/dabZTN+sr+sXu/Vi3mqtfhYWWxWre4ZvcfLrDHfusa0aBmrMfWc65zF5Hi1htH9KVV7SirnVu28EVfvj3LHvfYda6u9Na69h3hc+9jmXGe2e2rGtdrG9NVI9VJp+1xTdMyxvd9xNTfrlpNa49YsNb7zuoqODa09Zh6VVM5Kah6p8Y1c0d8qZeINPfX5qa3/WOqzaFdfQ2qP7aPLIn8vDp0HAMCy+P4DAAAAAADYbXz/AQAAAADAInbIv1oP4cgjj2xsH3fcceHEE08cPP6JT3xiY/uyyy5bQlcAAAAAAADz8/0HAAAAAACw2/j+AwAAAACARexddwOVU045JfzjP/5jvX388cePGv+ABzygsX3jjTcupa9F5VkZ8qxcdxuDZdvY67LWZRk9Z2F8jrH9D+1zTN6+nENq9tVL5Ujt78o3b7+pnEPmN3Tds3z4uudZMapWvH9mu6N2lqqV9+RcQY1U7q796RzzzathJnaxnK295s2c1diZ2Elcct2j443xibHJ43HOajs63jafrdgqd/w7m8Q1t2d+t/bZcWzZisl5yQc+t7FovzZqQ/O0yPpytyjH1is2uo/newbkWNIadOVZYB1XrrO3ak5xzGR/Ue2fnIfqdTL57FDGw1vTTV6X1VZRvU6ro1WtybmsXt+TJNWrubrNldHx6brxsbKY9Du5R/QeL6vXezz/rXtIaszs8eb1UpaTXNW9KMrTjG3PWfcyyV3nTPS4Fb9Vo55jdGxmf3QfHdrDkD7q+aVqJ/ZPfyYqetaoL1dqf7NeManV/hoaOs8hNau5FT1jhvQ9HTekj6G123KnaqRy9uWOx09L5or+linD8LWpx/SsZ1dfXb2N7aPN0N7G2M6/0edZm3ml5rU//W8S267cF0Kxb91dLF+5C+cEsJ/brd9/AAAAALATbYQQduN3BT3/9zMAbDvffwAAAACwfXz/AbvRjvmX6aeddlpj+5BDDhk1Po6/8847F+4JAAAAAABgEb7/AAAAAAAAdhvffwAAAAAAsIgd88CjH/iBH2hs33LLLaPGx/H3u9/9FuwIAAAAAABgMb7/AAAAAAAAdhvffwAAAAAAsIi9626g8uxnPztkWRbKsgwhhHDVVVeFO++8Mxx66KGDxn/pS19qbD/oQQ9aeo/7mywrt71mvsKay5hPFubLMc+8hvY7JvfQnH1xXTX7xqaOb2fOIevQWzMfen6KuWvN08PYOcf7Z7Y75plFc0vFzjO/dK45a65gHsm1yWfPeTV2ZswkNpk7Oj7ousuHxdbHq9rxuOnxy368YdaRMM+7t/sUxfBxRXSuxtZK5Rli3lohhGxkvbKvVrHRnyTf05Ojp6d5zkefOOc852Em52SekzWp1m5mzVvnU8XExyb7yyxRs/qP6D5Qbm2XIWvGVK/PSerqJVVWr+OqZHUPmRSpMsbHp2NCamwR9Z84Xt2vymq+0/fEYrKekz5TY+L711ZcMYnLm+swbc6c9f6yvcdp8T27rpHYX49L5Ix7GNpHm74a9dp15YjP4Zz7p1Wfh4pE/XTf3TXbjlWfhYqOftpyd40bOvdUjiFrlIqJP9ulcle6z0N3rjpndD/augelpT7XdfXT1VubvnOaMs/fgUP73g6r/BsZANh/+P4DAAAAAADYbXz/AQAAAADAIpb9CIS5PeABDwhnnHFGvX3PPfeEj370o4PHf/jDH25sP+UpT1labwAAAAAAAPPw/QcAAAAAALDb+P4DAAAAAIBF7F13A9POPffc8OlPf7reftOb3hT+xb/4F73jPvGJT4S///u/r7fzPA/Pec5zVtLjqmVZue4WOuUr7G+Zc8/CYrnmmefY/sfU6Mvdd3xIrXlrpHIPWY9l5uztPx+23nlWDIrrqjl2f3tsex+pecS5u+Yb5x6as3d/S56h80iuzUxc+vwMzTnTZx6tR8u41JjkukfHu2rWOfIoNq4R7U/2WP9uOZaF5rH6dzaJa27P/I6l9i9DMZlnPuLZjMXA1++YnIvWWkL9bGCtsitvsdE9ON/Tfbyrh3nXc5E1nFfV6zy1qxdIfM+YvL7K+gU2eb0V6Zit7dCMrXNv1qpueWV1P5jsr8ZN3xLjmJAYW+8vq9d3dDzqqY6b7q/Im/MoJvOK7onV2Nm4YnK85dqp7ldRzkYfLTnr/VHutvem2TGJGj3zSeXr6iNeqzpnTw/x/hC2Pi8VyXUel7Or1lbN5vVfROcwfV7SuVPHZuY3sO943JBaQ3OkrokhNVLzGjt+TK46Z/S30db9ql/q89SQ/mJ9f5P0zWOMRf+2nGd+7KfKjRDKfevuYvnKns9+AKyF7z8AAAAA2B4bIYRd+P1H8P0HwE7k+w8AAAAAtofvP2A3WsK/+F+ec889N5x66qn19t/+7d+GN73pTZ1jrr/++nDuuec29v34j/94OOmkk1bSIwAAAAAAwBi+/wAAAAAAAHYb338AAAAAADCvHfXAoz179oTf/u3fDnm+1dYrX/nK8LKXvSzcfPPNM/F/8zd/E570pCeFK6+8st531FFHhf/4H//jtvQ7RJaFkGXl4J/tlGfl6J95rWLuWSiTP0MtMs+x/Q+tMWZt+o4Pmc+8NVK5u/L1zWtszkFrlJchy4ecnyLkWdEb19XP0Pi4p67rop5jNCa5Jon5ZllR//THdq93PL4tT1wrVTO5lnm5+dOTb0zOmT7zYvOnY1xqTPKcJo6nxmdZWc81maNai2p/FZdNfqK1al+kyU8e/2Qh5FnI8hCyIZ8IBgdOKYrNn5n95ebP0PFtP/Pqyjn2ZzvqD5QVRe9PuqeN7p9F5rMO+Z7Nn4kyz0OZJ67dPI9+sslPYn/1OqjjotdRvS9s/Uxeg1uv8/h1334fqPfX94PmfWuIeGx9347vU/E9MYobFNtzP549Xsz8bNWI3wfae0nXHv6+0VcjNZ++fG19rOL9fOhny7H7h3zOSn1+GvsZo+/YPHFDPl+NydH5OW2OGsv8m2Ds3zTL+DtqFX/Prvpv0jGG/v2+//xsy7IBAD124/cfAAAAAADAgc33HwAAAAAAzGvvuhuIPeMZzwi//du/HX7pl36p3vef//N/Dr/3e78XnvjEJ4YHPvCB4Y477gif+9znwte//vXG2IMPPjj82Z/9WXjIQx6y3W0DAAAAAAAk+f4DAAAAAADYbXz/AQAAAADAPHbcA49CCOEXf/EXw549e8KrXvWq8N3vfjeEEMI999wTPvGJTyTHHHfcceF973tfOPPMM7erTQAAAAAAgMF8/wEAAAAAAOw2vv8AAAAAAGCsHfnAoxBCeOlLXxqe+cxnhte+9rXhz//8z8Ott97aGnf/+98/vOQlLwkvf/nLwxFHHLHNXe4seVaurXa2wtpZWDz3omszz/yG1hyTuy92GTXnrbGdOYesWZb31Sx6cwytlYqdZ95ZT19ja7TlS63N0Nzp8fPXCgNzdp3Xmf5TsXmUc8i4yZhk7NjjUzV6Y6r91XZ0fGt89btjHn3yrPl7OxTxtTCidjHsdTwjz+cbt8wexvQxT41E7qwnV5nqqdgYUHNPYuzI/oesyyLrHteIc9XHqv1xP5P91ZJE12z1uaVsm8ZMysnruNoqqtd1dbQ5oL4lTl7f06+e6lhZ3TMmY8piM3t1b4mPh2j61fFQTGpO30tSsfXx5piZ2mU2m3MqbjO2mMTmjblu1WjPWY+fqT17rVS5u/rorNFTuz1Xc16pNdrqcVjNNnHueeczJqb6XFVEF35fL0P6qT4vFT1xXT0uI0dXnq6xfec2/jzYlntorlS/Q3LXNRJ/e5Whf2wIwz47D7mOu8zzt92QuXNgyoqNkBX71t3G0mVDPrsBsFa+/wAAAABgdfZNfnab3TgngN3F9x8AAAAArI7vP2A32rEPPAohhJNOOim8/e1vD3fccUf41Kc+Fa655ppw3XXXhYMPPjgcc8wx4ZGPfGT4gR/4gXW3CQAAAAAAMJjvPwAAAAAAgN3G9x8AAAAAAAy1ox94VLnXve4VfuiHfmjdbaxEnpXrbmGQbIV9ZmH5uedd10XmObTmmBpDY/tqD8nTF5OqkRo3pOYqcmZ53zyK3hyDa428XlK9ZS09VbnTY5r747g4Z1ueses8qv+BfYeevrvyDe4zj9aib1yePh+p2MHH85bzmoip91fbqRz176h2NtVQHv/OJrHxTCN51hMwQjFZ17ynaNHxulpWP8Ww+8AoffNaZh9DavXlTuTIesaVXbWLjZ6ae7qP13lWcH4mqv5b51nNbWj96gXUcs/Y3N+o3BxalpO9WfN49fotmiXKakexp1EzmypSVahuo2UUUxbR6ydxvLoXVeNDsVWjur/MxJZZI2c1JhVf91xmjbhmbDGJia65eo2G5ZyZd0fu1JhkjcT+7lyJec302Ly3D5lH9bmqGLAGXTWn5zE2pvqcVQxc26F1N3M35zdPj8vI0Zenb+zQ3NNS/XZdg0Ny961Do1bib7YyDM9R5+r5DN03j3ks+nf3mLUCABhrN3//AQAAAAAAHJh8/wEAAAAAQJ85/nU+AAAAAAAAAAAAAAAAAAAAAADAOHvX3cBul2VlyLNy3W00ZGvoJwvLr7mMdV1kLcbWH1prTE99PQzJ1ReTqpEaN6TmKnJmed88iu7xI9a9LzY+HvfWdd7GrkGcO4vm2bYuy64xT60wZ862fMlzn0dr0XNe4vjOMZPYwccn++PjbTnWIs+Wl6uYzC1PPFOxiNY5Fdc6dklrtcz5VuJ5jTFmDcbWGnoeBo7PBowrkzU3emrtGdbTEH21GnUn/Sav3WrOif3FZH98z5m6x5Qhq/9remgVUx3f2g5R3GR0dS8pqu2pe8mkv2psHFMdr3KWZdboux5fTHqZ3Juma1RzrfuMY6OccXwlOa41d3Ndy7Ja72otm6/nOGfbe0Rf7lS/qRrx/rb6cY56TN9axvFtNSbzqNemxzw1xuaoPncVUU9d6zFz7hI1qs9RRU9c1zzG5kjPc6tGMXJs/P6fimvrN2VMzum80/pqzNRM/J23dd8bb+hno775LdNO+5t+rB3xeXOnKvZt/uw2u3FOAAAAAADAQBshhN34XcGI/1sQAAAAAABgl/H9B+xGI/+FPQAAAAAAAAAAAAAAAAAAAAAAwHh7190Ai8uyct0thCysrod8CfObd40WqT205pjehvbTl7PreF+N1Ngh80jlnjdnlnfNo+jtZ0iNUf2MvF6yjh5Tc0uvVTNXPL6rt/hYunZ7v23xyXozfXX33ZVvJjZP9Nc3v2hcZ61JbDJnfHyyv+ta3eojylFtp3LVv6MestDY3/jvPJvEpnrIEgfmUEzWNe95tmLR81rtGz+PYgXvWYusXd8azNQasSZDc6dyDhkfjc16xpTJWjv8SbR524srhBAm8y2q/RtR/NbnpLJ6gVav18nQ6vVbVq/nybj6Sq2WtL5fbQZO30LL6t4xOVaNrWLi41XOsqx6isYXk/vF1P2ujO+XHbFt8dUa1fNtq1FmjZh6fx1bTOIm84jvr1HOen7TfUf16/1x7t74jhrRsdk5N2v11ahOW3y8TfV5q0jUHtL/0Ji+49XnsSKxpm19ja0xJG7RHEPGx+sej63Mu5bTNSpxrVTOvtpdNYbWmqnd8fdhfS9c0JjP3kPmDgAAAAAAAAAAAAAAAABsjxU8RQAAAAAAAAAAAAAAAAAAAAAAAKDJA48AAAAAAAAAAAAAAAAAAAAAAICV27vuBg5UWVauu4VRsrC6fvMVrMW867tIL0NrztNbX199OYfUnLfGIrnnzZnl6eN5VnSPXcF5Ss4j6nPsOozJndo/JncyZ7Sm6biW/XPmHDXvPMo1dGxiXFeN5JrFx/NErrY8cY5qO5Wr/h31kIXG/kHyrD+mSzE9j0SuInpN5iOftRiPX8TY2mMU6ddar7HnYZ416Zt7X86u8SPHZj3x5SrPU5eqbuqaHbruHeez+jxVxlOsUtf7J6/7xlYIWTkZPzsgVLfVsrqXTI6VPcerVGU5qRaPL7bmU927UrF195MxqfhQbMZX96+2GnWuydg4tnofKePFrO6JUQ9Dcm71UETxzX5759nI1Tw2dB5dOZO5o1zV546id77Da1Xi2L4c1eezYubiT9fqqxHPb2gvzb7G5RiSO5VzaI6uazZVq5Kqmco9ZI1StYbWbO2j52/LMozP2Vtzwb85x6wV+5lyI4Ri37q7WL5yY90dAAAAAAAAa7MRQtiF338E338AAAAAAMCBy/cfsBut6V+UAwAAAAAAAAAAAAAAAAAAAAAAB5K9625gt8uyMmRZue42WmVh+/rKV7AGy1jXRfoaW39o/Jie+nIOqdlXL5WjL3dX3nlzZnn6eJ4V3WMHruuY85qcR9RnvBZZR6+pOabXrHveQ3KlaxYD41r2z5lzaG+bNaJcfWPHxk+NScbGx1P9dsVVx6p9UcxWrZH3q+lHGubZuLHzKKo+e2oVPddsvsJnMfbVHmOZfRZjz+0c53PRdR+ydqkcI2tnc5ynsqf/eXLWfcVj6/WPa07iisn+vKNmPTS6t5TlZG/WOF5vFdV9oTo6XWNzZ3WbLat7y2R/VSl1vEpVllmj/2xqnmWxeay6l8Wx1dyr+9bY+GmpsbO5m+tcltX6T3IWs6+XVM649lZ80cxd7U/Ms9nPsHmkcs70PtVrKqZPsnZH/zM54nPas3+RfvpyVZ+zikQv0zlSOftypOY5bWjOvtyLxMafOVO147zTxpy7tppDa3fp+zt16x65fXbq3/RD7e/9AwAAAAAAAAAAAAAAALC7rPCpAgAAAAAAAAAAAAAAAAAAAAAAAJv2rrsBlicL5dpq59nqamdLzD1vn/P0MHTMmJ6G5uyLG1IzlWOR3PPmzPL243lWdI4blHuJ65/qc558ybVK1Ij3x+Pb8s2OaV/PZM04Z0vc0Jwz/ca58vS5Hjt2THwydhJTH4/n0xc3XaPaN/RazONxzf1dskUfc1gW/YmKeD2z9rjk+P7X9WD5Cp/rOLbPZfYSr3Fn3YHrP3Q+XfPoy5EaO6R2z/ply7huhvYfx830NjleTO2vXvOTc1e9hMr6BVw2hlav75njeRy3VaKsdk5uBtXtt6zuQ5P91dWTOh6i6ZXT96v4RlPlKLNonnlzHsXm8epeOBPfyDlsbHzv3oorJnGTXtvew1L9VPOMatf749w98W2SNVO5o/eGeFxbzjhX9dmsGDjPIf32z6d9f/XZrShnX9ND1zGdu32ey8jRtw6L9DXkHM8TO127MmZtYn21+mqP7aPLPH/3bt1PoSkrN0JW7lt3G0uXlRvrbgEAAAAAAFibfZOf3WY3zgkAAAAAABjG9x+wG63wSQAAAAAAAAAAAAAAAAAAAAAAAACb9q67AZqyUK67hVZ5tvq+siXWWLTfeXoZO2ZMj0Nz98UNqZnKsUjueXNmefvxPCs6xw3KvaQ1DaGrz+b+bEDfQ+tXueLacXzb+Nkx7X315a61zD/OmVqjmX7juDzKM2A+Q8emanXWiGPiNeqLy9vPW7OPMspV/U70PTO++p11xy2iLKpm+mOLvn5X2Gcx/jVXy5f8TMh5ellGD33rX9caeB6GzCPVd9/YrvkuMrbPvNdJtWZD13h6zET1ua8Mk/3V67t+iTWPb22HRtzmsTA5Vu3c3FHdjsvq/jPZX+VIHi8mNafuiVVMKPJGPyFawjK+/8bPWI3jy6l1SdRo62d67GxcMTnecm3UfQ/LWe/vyT0dH/fbmGPLmCq+b//YmGnV55KinKNW3zwSx1P7pz/LFYl17FvDvp4W6y+xVgNq9uWsxLnH1JgndkwPXbUqQ2sO6WOefsaa9+/s+v4MAAAAAAAAAAAAAAAAACzNkv81PwAAAAAAAAAAAAAAAAAAAAAAwCwPPAIAAAAAAAAAAAAAAAAAAAAAAFZu77ob2O2yUIYslOtuo1OebW9/2ZLrLaP/eXoaO2aePvtqDO1hSO1Urr4aqdyLnOcsbx+bZ0X/2CWt2ZC4dJ/jekjlGRMzk3NAjSyxnnFsci1m4mbzDc0101/ezNU2buyYofGtaz2JrXPG86q2U3HV/rbcUexWrp7rIqt66w5rlWf9MSGEUFS9JOLLlmsoG9lQ0X/9zxja/yKKnvtNvg3PjNzOHvrOw5g1n7fvvnGLjl2Wqoe45kxvU8eLybE8cd+dfFYsEymq+0J1vDobZWtsdayqNdmx0TyHdc5qexJeVverybiy2BpX3duqmGpeda5JbB1XTsYOjB8ypo5L1JrNPbvmZbWQVc4iWpu4/2p/T+5y6gT2rUmyxiRnOXMxNHtrHxvX6M41kzvquat+XLvveNe46vNd0dNnX+2tfOUk32zc2P5SuYb00hfT1ef0+Gl9694Xl+phWqqfvppjaw/tZ9rQ3pZpp/8tP9RumcdKFBubP7vNbpwTAAAAAAAw0EYIYd+6m1gB338AAAAAAMCBy/cfsBttw7/WBwAAAAAAAAAAAAAAAAAAAAAADnR7190Aq5Nn5dpqZyuovYz5zNvXIvMZ2veYGkNjh9RO5eqrkco9pLdkzbx9f54Vc+dc1vFGbKLPdO7+/uM+0uel6OwhHtcWl+onjp3pIVlzNl9frpm+8kRPA+YTjx1bq17zjtx1znhe1XYqLk+cr+ke8476IdSPJqyPZ+1hXY8wzBZ9vGFRzSdVfEqZuN4XbqKlnz5D+p27h57Xdb4Nz5Ts62GZfQxZ86HrvcjaDZlzX46hBteazLtrjeqYSV/xPW/mNRblqsLraU3uG1MhZRRbveTKyY6s2Iwuq9pFtEbVvWhyuIrLpm4u5SRHda+Lc1X3qZm4ctJpT3znmHp+w2q15d6qUUxiq/MRr3ei/2p8Ivf0+2GVu3dN+nJWvVXnpWyZT8dcp/uqeqo+wxUtuRo1O3O2z6PveN+4tj5m1zk614mc059V47mO6WM6VypPV66+Wn3nY0yutr6GxLf1UxnSV1fteftoM+bvwbF9AwAAAAAAAAAAAAAAAADbZxv+NT4AAAAAAAAAAAAAAAAAAAAAAHCg27vuBhguz8p1t1DLVtjLMue5aJ/zjB/b/5gafbFDa3flWVaNMbmzvH1/nhVz5RsTM2r9E31W4rXJov7jWn35hsQM7T/upavGTM5EDzPza4nrnXO+wBqNHZuI74qrc8ZrVG1PYmfi8sR6t+SbrV/FJI7Xce27Q54lDixBUfU2R42y+/UcshU8g7FIrd0K16iuHc03X9MzJuM+YsvsK7XeMzV71r+v5xD6+x6SY1Xaeus9D5trkoXNNSxDtUbRvadMH5/ZMylZvbTKyb2kunWX1f1rcjMpi0mGRFxbbHXvK+P7ayqurGpM4ot80mPLtRMtWd/YVK223FuxxSQ2OmfVmChn3Etce1qcOxU7szaJ8W3SY9v3x6rPLUVHXNccx9QaovrcV0Rz7uthmb305ehas76xQ3NXOs9L4ppcVvy8ffVJfW5dxvUTG/o3yyLzYZcoN0JW7Ft3F8tXbqy7AwAAAAAAYG02Qgi78PuP4PsPAAAAAAA4cPn+A3ajNf3rewAAAAAAAAAAAAAAAAAAAAAA4ECyd90NHCjyrFx3C3PJtqHvZa3NMnqdJ8fY/sfUGBrb18OQPH0xi9RIHcvy+c7ZMuYzdG2H9LjoNTxsPkVnP3GOIX3HMTN9JGsVvbV6+8mjHCPj28b0jR1aoxEXr1G1HcdG+2fi47g2dY7E8Sw1Lp0yqahqZcP2p8Y3+ugZ06eMznG2wmcyDp3nUmtOzS/fQc+bLGZfWyGE1fbYdv00ag84L6m+Z3KtYB5Da7f2MRlbTLZb7m0hhJCFzTUq6xd+2Rhe3VvKxo2hnIxtjKjHbNXarF3dysvqfjbZXxaTDJMa07f8OLZW5Sqrsd1xcb5mzKS/ao6Tfqr7bVyjL76uVW6t1WxsMYmJ+q3uy0Xzmox7ifMNMTOfRG+pcUPGzvSZmGf1OaYo0/0P7Wump579bcfySZ9FfD4G1myTmuPYvofoGzs09/Tny65zMyZnHD9t6NjU596+Hsf2M6anRSzz7/RF1gAAAAAAAAAAAAAAAAAADiQ76F/cAwAAAAAAAAAAAAAAAAAAAAAAu5UHHgEAAAAAAAAAAAAAAAAAAAAAACu3d90N7HZ5VoY8K9fdRkO2hn5WsQbLmMciOcbOaWitMT319TAkV1/MvDUWW9ti7px9Mb3H88XXP4v6j2t21ahi+/pIrns0Lu6lPSbKNSDHoDwtMSEfuTY98QvVSMVN5UvlnomNc1X7W3LO5E2c6+Q1sB2PKiyqvrPxYypjxrYpW667bMmTn2eeB4qi/XXfkK/oYoyvpdbaA8/ZkHnM5M7nHztWdU3nifts2FyL+uVQLXlRDd9aqzJk9X9tjp3eCiErJ7lCM1l1iy+r+9tkf1lMMkzfE6PYUOTNPqrjZTW2Pa7KXd1L6/h5xvTE171PvRdUY2dzN89DWVbnp5pfe844X1vOKldqPnFvWzUmPeXReenoo0/cU6X6XFN05Emtb18vXT0O7b+v9pB8qTmO7XvQWvXMa8x5G1JvOmdl6DWx6NgQZj8X9/U6T0+xsT2u2k77u3/aTu5t7YoihGJj3V0s33Z8jgEAAAAAAHaojRDCvnU3sQK78DsdAAAAAABgIN9/wG60HY9NAAAAAAAAAAAAAAAAAAAAAAAADnB7190Ay5dl5dpq5yuovcz5zJtrnnmNrTUmvq+fvlxDas1bY0juLG+PybNi/pyLrkmipzaptckS/c/b05Cc6fPQHNc2v5mxUUyqdpyrrYeZennUTzRmbPxCNVJxk+Ot18JkTBw7k6vaH8fN9NBWY3bX5qDE/u1QVP3O0UQRn58lTKScrHe2pOc1LqOnwbV24TMmi4H3vFXMPb6+krXnuXbH3cvTtafmHees+ho9jyh+Km11fylDM7beKqp7XHW0Gry5o7rll9X9brK/LKbWsLpPRrGhyBs9VKnLcjI2EVflnr5fjx3TF1/nnZpHPDbOvRVXTOLyxvzDTFzUS2u/zVypmqnxrTEtddtrt8elTH/OKRJjevtP9Tail+pzYVF230P6arUdq+YYz29s34PWqmfO8eeVrrWJP4Omas6Te5ljQ+j+W6Kv76GGfJYe2zcAAAAAAAAAAAAAAAAAsHq78F/fAwAAAAAAAAAAAAAAAAAAAAAAO83edTdAtywr191Cq3yFfS1zzvPmWmR+Q2vO01tfX305h9RctEbn2Lx9bJ4V8+Ub0EvvmiR6ajP2uohrp2oNmsdkbCo2mbtl/0yOKCZLnI8416D55c1cvWPGxneMGZw7T8RPjYlj6/3xmDiuq++6RpRi6DW5jEcWFlW/2bC4unZP/CK1tsN29JB7pmStGHiPX8WaxdduZ/05r4tRNSZzjNckm+zPE/ffsFmjDFWPLTWLKlUitrq31HHV0arm5o7qLaCs7nNTN5uyyBq54thQ5I0eQjSdMr4PT3LXecPW/bYsq1qJ3JWeGm3jqnpxrTj3VlwxiavOU9xDM19bzmSuifj9rp5/pG0e80r1Mq367FP09LNoL5v9ROc+FZeoOXT8Irpq9K7V0PmNmEdfzUVyp8ZWFlnn1GfqofMYY+zfLqu8ftihin2bP7vNbpwTAAAAAAAw0L7Jz26zG+cEAAAAAAAM4/sP2I38a3wAAAAAAAAAAAAAAAAAAAAAAGDl9q67gd2vDFlWrruJueXb0Puy1mcZeeaZ79i6Y+KH9rMd11hfja7jWd5+LM+KuXIOmW9vv4meYkPOQdYzj9n48Ws1NEeql9b4qFY8NtVLnKs1Lo9yTcbMxCbikrnz2fmNHVPH5/09xbHVsZkxcVyylxHnPosCFnhEYVlUNRIBRdV/XHSHSU5goFXML/fsyKUpxt1LQwjLXf9i2P13qaprMq6duFazsBlX1jeIlp7r1/sktsia2zNxVaZq/Td3VG8J5fQ9cXKsylnfR1ti2+Kre2pZVuOLRlzbPCpx7lDk7fNM1KjiB41JxhWTuKjf6v5dbI2Pc8a5UjVSkrUbMd01k/MckLv6PFSUiWszNb9kza1rNz42W7uY1G72N3TtuvpIzSsVP0TvWg3MPW6Numt25R5aoy/HPGsVS33uHjqvZVjk76plrAEAAAAAAAAAAAAAAAAAHAj8K30AAAAAAAAAAAAAAAAAAAAAAGDlPPAIAAAAAAAAAAAAAAAAAAAAAABYub3rboD1ybNy22plK6i1jJyLrMHY+kPjx/Q0NOeQuFTdvrGp41m+/LUdMo/efnv6Grf+xaAeUjW7eq1y9/VbHY976RwXHRs6tndeeTFX7DzxY8bMrHO1ZkPiq5g8cT6q2HhNZ3LH24GhsgUWK8+W10e+xpO2ztpDFO33wh1dezvWdJ7equu97B5bh4Xpazy6z1Qp8ubxLI4u4pzNgdNvEWV1L5wcK4tJtuq+Ookty2p/e3x1n43jNvuZ1J3k7B0zNn7MmInZuGISN/911Jejfi+qL4n0/SzV99j4IfOqPicViVrxWs3TYyo2n/RX9Kx7V6107vZ5pdeqfz69a7WENUnVrKRqd9WoDL2eljW+S+rz+Zj5bYdV/M27PDu5t/XKyiJkxca621i6rOezBAAAAAAAsJtthBD2rbuJFdh93+kAAAAAAABD+f4DdqMd/i/oAQAAAAAAAAAAAAAAAAAAAACA3WDvuhtgdfKsXFvtbAW1l5Fz3jVZpPbQsWN6G5pzSNw61iTPiqXn7Bub5d3Hx63/fP2njvf1Nj127DyX8bqJc8z0mxfJWqnYsblT8Y0xfbGTuN746R6qucV9tcW29h9vz7Tfkns2ZlfIR04sm+OZjGNrtOZY0bMgV5V3Jxg6t6L93rkWO6mXELau3SK+ZySu6TiuQ3WPKeuby+Re2NgKIRRVfLW/WqOt81u9/ZXVvXFyrCwm2ar7bBVXVvub8VvziOKmYkORN/uf1Kju1XHuwfEjxlSqsbNxxeR43pj/Zu72+nGOSv1e0LYmCclcQ2u2rc3UvDaPtb++q89NxYA++6T7aN8/Mz4xv2Xq67HtWKVvrYbOc2zskNrLrJUaP23eXClDPr8v4xoFAAAAAAAAAAAAAAAAAJZrF//rewAAAAAAAAAAAAAAAAAAAAAAYKfYu+4GmF+eletuIWQr7GGZueddq3l6GDtmTG9Dc/fFDamZytGXO8vTx/Os6K27zF76+tnsacz6t/ef7K+n9pgaqdyDe2rpJR4b9xvnmJlPXrTX6ohdNHfrmvbkruZej03Ftxyvx/Ss52z/8fZs24MNHZtnCxRZoaF9ZXMs0rxzzpf4vMdl5trtUmtVzPfesKtVr4cyWpvoms/C1r2mDFn9Xw1FlbJsjZsZVcdX+6d72NxZvX2U1b06ulFVueq4clKlur8WeaOn0HIJlPG9elKjLDZzVffuvtxx/Jgxde2o1mzuYnJ8ah3quSX6rXO3jG2xtVZb6xKPSdVYRF9/1eeoIp7XwLUceqxZs5jU7FmztnPeUys1nyH6+u/LPeb8xZ91+tesGT9mfmNrjcm1jJx9+j7rz3Ou2WWKjc2f3WY3zgkAAAAAABhoI4Swb91NrIDvPwAAAAAA4MDl+w/YjfwrfQAAAAAAAAAAAAAAAAAAAAAAYOX2rrsBNuVZue4WOmXb0N8yayy6nvP0MnbMmB6H5u6LG1Jz3vOQ5fOvearmItdEXz9D1z/LiqXXHjKvKsfgc5+q2bI/nlM8Nq45kzsvhsVNxfbmHho3JHc8n74a0fFGrXj9q/Mysyap9W/fvcjrZa2PKsyz5cVmc0xkTP0QQsgXWKxFxg6uMXI+61Ss8HNA11oX4+/BO9q886mulfg8TF1DWdg8VoZqXxQ7KV3df+K4mVF1/FaKsto5uRFVbydldX8t8qhGaMTVeRLxIYRQFpudVPfospx0Vt2j45tg1UMUN9NLMbVWidxt/XT1FOeefn8ty0mfVa6i+Xpv66txPHr/qXsdoa/fVFx7rmISM+7e2DfPQTkG9NdXK5Ujtb/6nFgMjB8Tk8o9psaiY/p6GFKrMs+1uR05hxr6N8E8awUAAAAAAAAAAAAAAAAAtFvnYxMAAAAAAAAAAAAAAAAAAAAAAIADhAceAQAAAAAAAAAAAAAAAAAAAAAAK7d33Q3sdnkWQp6V625jtGwbel5WjWWs7yK9jB07pt+hufvihtTsy7HIGuVZMSrnIr1k+eJrsVmjveehfQyJ6+u1LUffmPSazs4nzhWPnamVF8PipmLj3KmcY3toHRPPp9pO1YiO1/Fta5in+k+cj8TjBDvPX5Y+tKis7/GGeU/xvuNDY4Y0MzRPY8zI5zeOje/MtcITt5PMM89iCe/z8bkq+u/N226VPVWvlzKqUZ2PljWuh9Q3lSimqOLK1riZUVOlt3JXOzd3VG8xZXW/LfKoRpVrM3t1H47jG2Pi2HLSWaJGiJYo1cuY3EN7io9vxhSTmChXXDOKr6756Vx92urPE5/qra3PeF7V56uiY2xbrbZ6Q/rYrFlMam7/M3yH9tilb83aPtP11RvbV9vn4qHnMK5ZWWRNUjmXmXtey/ibc+zaLiLu90D5uDKPrChCVmysu42ly3biZyYAAAAAAGCbbEx+dpvdOCcAAAAAAGAY33/AbrT9/zoUAAAAAAAAAAAAAAAAAAAAAAA44OxddwOsT5aV+3WtfAk5F+lr7Nih/Y7J2xc7pGZfjt7jefvxPCt6a4+1yPkavv79faf6SK1F77g51qoak6zZ0Us8Ju5rJmdejIpry90Xm8zdE7cZ0z12Zkx8vNruyD3bf2rd23fPZTseSZhnqz0eQghZz0SG5KhjRy7K2PjG2BF9Dcq3nz5jsljgXt63hsUc9/TUOi7SZ8oqci6qWtNq7drWOF7XOibaP5ledX8rQzOudVQ9pjpWrdHmjurtrKzu3UUe1ajybGav7s9l2/vHJGcZx5aTzlI1VpC7HpeqMRk3/V5RxdSqY9H+OGefxvvR5Boty6jPuK94fn01pt4Ph46pVJ+3inL4PIf2Nzhujlqp/an5DDG03zE1lr1Wi/bTVbsyTw9Dc6+ixiot4+9XAAAAAAAAAAAAAAAAANgN9tN/fQ8AAAAAAAAAAAAAAAAAAAAAAOxP9q67AVYvy8pdUTNfQs55+1pkPkP7HlNjHed0pod8/h5S/Y/dP6Sf4etfDIhJ9JeoPc95qnKNHZteu/S84jEz88jbxw6Jq3L3xY6Na8Z0j50ZU8VVx6vtKq7lPM7mTpyXnscHdr5esu6xvTXy9gRZV0+JMb3H+8b1Fh5Se8SzGMfEDqm9ipr7u3nmW/TfTzdzDzgfxcB7Ydzn0B52iqHzHGKyrlnYzFnGSxGf0snx6j5V1jelyT12KrScGVPtr4ps7qjeesr4Xj45Hs92+q2qLCcVq3t13HDRHheKvDmPYrIOk3t8Hb9A7nlU78NlmUf7W/pqHd9cra74eO69vQ2IT/WZmtcyDV2jfNJLscJeqs+Txcw6bJ2fVJ9D5zHG2OtnntrxZ+h47kO1fYZb5lpsVw12sXIjhGJj3V0sX7kL5wQAAAAAAAy0EULYt+4mVsD3HwAAAAAAcODy/QfsRh54BAAAAAAAAAAAAAAAAAAAAADAtrj55pvDpZdeGi6//PJw0003hTvvvDMceeSR4ZhjjgmPecxjwkknnbT0ml/72tfC5z73uXDttdeG2267LRx//PHhhBNOCGeeeWY46KCDllbnnnvuCZ/61KfCN77xjfDNb34zHH744eEBD3hAeNSjHhVOPPHEpdUJYfvmtGweeLQLZFm57hZW2kO+hNzz9rfIvIb2PabG0Ni+2kPyzDv3PCuWnjOZL1/GtZHud/P4+BrxmJntEX1XY6sxff2GRNx0zd5+8mhs1ENf3Kpybh7vr5EcUx2vtqu4vGM9Uucqb9+9FENz51nr7qxrfGJMcn/f8c5iQ3P35Og7PqbWMmpsZ66dqOi5B4UwfA0G5Uqc06LnPhr3MKRWKsc8Y4fqm0dKtS7T49v2ha2XaRmi4/FpKqr4shkfpu678Z56TLW/WqvNHdVbUVlORlb34cnxsqj2T9VIjAlF3ugvRKelTMRVNabfG4bm7ouLc9fxrTHFpM/qukrcw6sFyKM16lDnLvNof7Ovtj7beh0jVbv6LFqMqJXqb+64qesqrpdci4G55+mr73hqzRapGcdV5pnfPP319TNPH2NrpKyyNgAAAAAAAAAAAAAAAMD+7Kqrrgqf/exnw4UXXhg++9nPhosvvjjceuut9fETTjghXH311aPz3nPPPeFv//Zvw1/8xV+E888/P1x66aWd8Q94wAPCz/3cz4Vf+IVfCPe///1H15v23ve+N7zpTW8KF1xwQevx+973vuGcc84Jv/7rvx6OPvrouet861vfCq95zWvCu971rnDTTTe1xpx55pnhFa94RXje8543d50Qtm9Oq7LL/1U+AAAAAAAAAAAAAAAAAAAAAABtzj///PCsZz0r3O9+9wsnnXRSeP7znx/e8IY3hI9//OONhx3N6zOf+Uw47rjjwg//8A+H//Jf/kvvw45CCOHaa68Nv/EbvxFOPfXU8I53vGOuurfddlv4iZ/4ifBjP/ZjyQcDhRDCTTfdFH7v934vPOIRjwgf+chH5qr1oQ99KDziEY8Iv/d7v5d82FEIIXz6058OZ599dnjBC14Qbr/99tF1tnNOq7R33Q0AAAAAAAAAAAAAAAAAAAAAALD9Pve5z4W/+qu/Wln+b33rW+Hmm2+e2X/wwQeH008/Pdz//vcPRxxxRLjxxhvDhRdeGG688cY65pZbbgk//dM/Ha6//vrwile8YnDNjY2NcM4554S//Mu/bOw/5phjwqMe9ahwxBFHhCuvvDJccskloSzLEEII//zP/xye+9znhr/5m78JT37ykwfXOv/888NZZ50V7r777npflmXh0Y9+dPi+7/u+cMstt4RLLrkk3HDDDfXxP/mTPwnf+c53wgc+8IGQ5/mOm9OqeeDRfiTLynW3UFtlL/kSc8/b5yLzG9r/mBpDY/tqD8nTF5Pl7cfzrOjNPbbmKs9D1tPvoLVKrMVQfT0MqT3v+Wo9ljf7qXLPHbeEnCGK66oxsxZVbHW82u7L3VpjdtcQyfXPRiQZWDvrissTBcfu7yzSl7Nn7MAPYZ01Fsm5zLHrzD2vYuD9aJ7eU7mH5EqOTVwDReI1F9caOt/psWPG9En1uUzVGhXVfXdzswzN/VvxVW9hEl824ydbIWzdwuoM9Zhqf7VWmzuqt7uynIys7suT42UxVaN6f4uWu6zeJ4q82d9kbPUe0BfXiI36iceERN99PUyPaas/bev9LnTGtY7tyZ0cF89/QL7UmK3jxeR487VWfR4rEuMW6TdWfRYtytXdY7djPvPUGLpG88ZPW8YaxH1U5ulnWbXX0QM7Q1YUISs21t3G0mXL/NwCAAAAAADsZ/ZNfnab3TgnAAAAAABgGN9/rNMhhxwSHvSgB4Urr7xyaTkPP/zw8OM//uPhJ3/yJ8OZZ54Z7nWvezWOl2UZPvCBD4SXv/zl4Rvf+Ea9/5WvfGU4/fTTwzOe8YxBdV796lc3Hgx00EEHhTe96U3hxS9+cTj44IPr/V/+8pfDi170onDBBReEEEK46667wllnnRW++MUvhuOPP763zjXXXBN+9Ed/tPGwoyc96UnhLW95Szj11FPrfXfddVd485vfHF71qleFe+65J4QQwl/8xV+Ef//v/334j//xP+6oOW2HHfgv7gEAAAAAAAAAAAAAAAAAAAAA2A4HHXRQ+MEf/MHwohe9KLz5zW8OF110Ubj11lvDH/7hHy4l/7HHHhve8IY3hOuuuy780R/9UXj6058+87CjEELIsiz8yI/8SLj44osbDwwKIYRf/uVfDmVZ9ta66qqrwm//9m839r3nPe8Jv/iLv9h4MFAIITz84Q8PH/3oR8MZZ5xR77vxxhvD6173ukHzes1rXhNuvvnmevvMM88Mf/M3fzPT+yGHHBJ++Zd/Obz73e9u7H/Tm94Uvv71r++oOW0HDzxasywrB//spD6XKc/Kxs8yzNvnIvMb2v8q13CVsrwMWT5fjdSc5z5PHb30rUWWFSHLitG9DqnfNzY5brI/yxd4jeVlCHmZnF/n6zcvNn/65pGIa53X0NhJ3Mw9ZjKfenxHjZkx9c/keDU2m/xUaxX9xDU3a4S53imT12g2+WkT10rVzrPNn7rW5k8yLm8pGO+fjp3eXyVPFYnHNXLmzZ+ZsX3HW3L31Rhbc1Vjx+beCVY5r0XWY3StxLUyJG+fZZy/otz8WZbW10X3dv1yrseG9nvP5H7VvE9Wx5v30fr+ObnPbu2fvBfU71HR+0t0n26+P0TvA/W9Pnp/icb1xbXGRv3E+uLa7vnpzz6T9+m295wBhnwe753nApaVq/Wcr7CXsZ/dFpnnstZonr+PxtZe5O+7Vf4Nt86/f3f63+UAAAAAAAAAAAAAAAAAq/DCF74wfOc73wmXXHJJeMtb3hJe/OIXh0c/+tHhoIMOWkr+JzzhCeGqq64Kr3zlK8Nhhx02aMz97ne/8Gd/9mchn/r3vV/5ylfChRde2Dv2da97Xbjnnnvq7Z/92Z8Nz33uc5Px97rXvcJ5553XeHDQH/3RH4Wrrrqqs87ll18e3va2t9XbBx98cDjvvPPCoYcemhxz1llnhRe+8IX19l133TXoQUTbNaftskP/1T0AAAAAAAAAAAAAAAAAAAAAAKt01FFHdT6kZ1HHHHPM4AcdTXvkIx8ZnvzkJzf2fexjH+scc8cdd4T3vve9jX2/+qu/2lvrlFNOCWeddVa9vW/fvvCnf/qnnWP+9E//NGxsbNTbP/qjPxoe+tCH9taK+3n3u98d7rzzzmT8ds5pu3jg0YplWdn5s1NtR595VoZ8CbmXsa6LzHNZ82izrPUfkmfeWnlWhDwr5m1tVC9ZXoYsn289sqwIWUefg9ZoZP1Fzl99PU9q9vU/M76j19SxuN86Li82fxJxtSquJXam5iRuJldebv6kepj6qcdGY+rj1dhs8jOJm+2lOhZmf0aa6xqNa6Vq59nmT11r86cvrrEvPtYW25U8lSeEEPK8+ZPaP3M8a//pG5+39dcRO09819iJMs8P2J+513OecUPHzMR1XLN9OReNW7ehr/d6fUL7vSjb+tm6J1fHo/tq9TO5787ewyfx8ee26Xv7VL5GzomZMdX+6P6biuuMrecevY/0xMV5W99jB74/xO/7W9tDPsuN/KzQ83mrq9/k2EQPYz4vL/r5d5mfTVO9DJlP3zzGzHOVf2/M00+bVfS4U/927vsbfyf2vVP62JGKjd37AwAAAAAAHKA2Qgj7duGP7z8AAAAAAODA5fuPA92jHvWoxva1117bGf+Rj3wkfPe73623zzjjjPCwhz1sUK1zzz23sf2+972vM/79739/5/iUU089NTzhCU+ot2+//fbwV3/1V8n47ZzTdtnB/0obAAAAAAAAAAAAAAAAAAAAAIAD0d69exvbd999d2f8hz/84cb2U5/61MG1nvKUpzTqXXLJJeGf//mfW2Ovu+668PnPf77R55Oe9KTBteK+PvShDyVjt2tO22lvfwi7XZaV21ovX1K9ZfS9SI6x8xhaa0xPfT0MydUXk+XzrVFX3lVcc/NeV+u4jrKsmL/W5HzUNXvOTxXXeh7zopkrqpHKNROXz84nWTdRM55HKndjXCp3tT9ao9lequMz7Y/W+zrJou22mqk+8ubgLI7L4+Qd+1OxM0kT8fmAxUrFpGovI/e8cS3KBcY2e9iznDzrVqSfDjt2rbJi8nruG1e03CNTY+LYvriu67CI7xFRrra+puNSxxuxWXutNahe9mXVdr02k97q/VODimpsOYlsjokyTMVX+6OkxeaI6Xt7Wd33izyqFVrHlPH7xCR32RUX545jy/i+2xMX9dwYm8hZq96jJimScR3i/gb3vUDuZRpaY57+h9aaN/f0589izr7G1K7q9dVaZK0WXef4M/m869Im/vy4yLWwnbb7720AAAAAAAAAAAAAAACA3eSKK65obB9//PGd8V/60pca22ecccbgWocddlg4/fTTwyWXXFLvu/TSS8Nxxx3XW+cHfuAHwmGHHTa41plnntnYvvTSS5Ox2zWn7bSkf9UPAAAAAAAAAAAAAAAAAAAAAACL+853vhP++q//urHv8Y9/fOeYyy67rLF98sknj6p50kknNba//OUvt8bF+1dVJ4Ttm9N28sAjAAAAAAAAAAAAAAAAAAAAAAB2jDe/+c3hu9/9br19xBFHhKc97WnJ+JtuuincdNNNjX3f+73fO6pmHH/55Ze3xl1xxRUL1TnhhBMa2zfeeGO4+eabZ+K2c07bae+6G2D7ZVm5bbXyFdRaRv/z5phnPkNrjelpFes6Vp4VS8+ZWoMsb98/ZB2yBftM1d7Mneg32p/K0ZV7rGqeg3Lm7WsyM3YSl5xPlGc6bmiuMIlLxc/kbptfHs09it3qN67R3Fzm+dhKGm23PWYw9ejBvDk4i+Oi4zPbXcdmkqVypuI6npfY1Uff2CHHh8ZMKUfGb9bYM37M4Nw78HmTReJeucg6FBuNzb7zkFU9DFmfvth4PnFc23zja7eI7xk9OarjqbVsqxXXWEQqZ7w/2q5uB2URxYdJ/PR0qiWY7Kvum2VojokyTMVX++tik+Nba1/d78vqfaDIo1qhMaaOLyc5qveESe4yiuvMHeccGlfO3vfi2Hh/dZ2U5fz3g67625UnNbb6XBLPr/rsVgyoNbSvZa3DqmpsR3+rqL2svqc/rw8572O0fRZfxzqzf8vKImTRZ5bdICuX//c6AAAAAACwv9gIIexbdxMrsPu+0wEAAAAAAIY6ML7/iB+eM8QxxxwTjj322GU1tONcffXV4Td+4zca+172speFgw8+ODnmlltuaWzf+973DocddtiouvGafvvb3x5Ua+y5OPzww8Ohhx4a7rzzzkato446qrPOKue0nTzwCAAAAAAAAAAAAAAAAAAAAABgDc4666zRY17zmteE1772tUvvZSe4++67wznnnBNuvfXWet+JJ54Y/o//4//oHHfbbbc1tu91r3uNrh2Pme5hFbWmH3jUVms757SdPPDoAJBl5bbXzFdQcxnzmDfHPPNZx7oPrT2kt+z/Z+/uoy2pygP/P3uf20033cqbgEiWgN0QEXQNL0kANZJoGkzMCAkGE02AFSYvM5NxgjMrzBrnB0RXjL818hsnLybRTJpoDKABTGYCaDIhozQyIBhGFOnmzQFEQFBp3rrv2fv3x9m77ql9alftejvn3HO/n7XuOl1Vez/Ps3fVqarTp2+1blZ/WexpzolSpmJ7xRw1HH95zuKaxmvpao58nLJxRHPpSJ0+VrC9NJdrO5FLR/pEYktZbL8taLtSb5hD4vV2RQXLumJZRETnO6miNgXtKpfLgk30jbVL7J/Sp6vtImIT2uRjDuq1z/WtmasLXeQ05efCXnOlzrcZPYE2ZX8qnyPWtu32XJvg+DbhuUTn24fri2KGfI4wdsi/h23i/kwR5J5IkY1/rLZsW37Zn0+t5PtMRMja+/VhQBExo17+/G/9dcHoIFekvXVZ/TXCxbZmZX9Wxk6twYTn0LH9Y/LHwUR9NWXXrLEUsVj+fsPainEF/ctqjI65Q6k5UudSj913GRvsj0iuNvvJf14w0f1SHrtO7qpcbWJ32TdUt+4mwnvOLuoGAAAAAAAAAAAAAAAAAAAAAAAAAEzXhRdeKP/7f//vbHkwGMgVV1whmzZtKu0XPhxow4YNtXOHDwcKY3ad6+mnny7NNc0xTdMMfnsfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAV/+k//Sf5xCc+kVv3wQ9+UH70R3+0diyl1FT6TDPXNMfUp6VZF4DuKWVnllv3kLuL8TSN0cd4QnVqm0Y91TWYzmPG5kDp4vVt5qFqvmM5U2KE61NiVdWh3HyvLLuYke1ROr7fsr5BG58rFrt0u4s1MVc60qeifVH9WYxwTnR+eSVW0K8P4XU+fKxg0WMGdb6Tmuij6i1PBCjrG2kbXV9yI1M7VtozF21KOz1IilU3d+8xpqHPOk3wvqyby/dP2X9mKCLx40FlsSI1lG2vGkfWNzj+jS1vXxSzaNt4bNPB+SkWK3G9P4VYE2wfrXVt/TbJLfvzq81Ohi5mvvdYe79+fF78XI16+euC9dcBo4Ncef42xVqX1V9fxk7ANjW2Cc+Zxe0kzFmQdyKWr9cV7I9tNTG3brutfn9F6+5QLEc2lzZcn15/69oiNaw2dcbh74lN4pjbzFGX81u37jbCe8vVfnygB2aY3WcslEUcEwAAAAAAAAAASLTsfhbNIo4JAAAAAAAAAACkWRvff1x33XWydevWWhEOPvjgLguaC//lv/wX+cAHPpBbd9FFF8m///f/Pqn/5s2bc8vPP/987RrCPmHMaeea5pimiQceAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAMbN26VY477rhZlzFTH/vYx+Siiy7Krfv1X/91+fCHP5wcgwceNc8zbTzwaAEoZWddgugeapiHcTUxy7qrcqfUpnSz+udlfyllmvVLGHdsjHXH3nSOk2K7WspyVLWJbtcmt32CXpn7iTa6ZkzfTuf3Z66/7xPGzl7zIfucd1HBsq5aDjuIqKo2VcthgIIcosMkkfVFfcv614kdsBXbRQ/Kt9fI1Xm/rmOsBmbsPVl3zCY4P6f0931ix4EZikj8OFJZ/wb7J9Y3Wx+8T4wtbp8Sy/MxTeR85d/nttm1ro7CVNmYXX1+mx+W8X2taxW2d8vaBu1XUtgwqOvjrxPWXxeMzucKYvvbAWv9+rFrlIttE2NLLJbJ78/xa1nWNpC1yeY3cr6dgmzcQa0p42hr/LODqcgRq7NePuNypZ0LquamrBY/tti4uhhP3Zxd1tBl/XXr7kIf8w8AAAAAAAAAAAAAAAAAAAAAAAAAaOcTn/iE/Nqv/ZpYu/L7hxdccIH8wR/8Qa04++23X275ueeek2effVY2bdqUHOPxxx/PLe+///5JuZ544onkHCIiu3fvnngQUVGuaY5pmtbIb+cDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAObFlVdeKRdccIEYY7J173rXu+TjH/+4KKVqxTrooIPkgAMOyK375je/WSvGQw89lFs++uijC9uF68N+dfMceOCBE7WLTHdM08QDjwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAU/NXf/VX8ou/+IsyHA6zde94xzvkiiuuEK2bPRLn2GOPzS3v2rWrVv/777+/NF5feV7zmtdE205rTNO0NOsC0JxSdtYliO6hhi7H1TRWk3HVzVWnfR/zXJdWprpRRGys0fW6eH2beWh6LDTpF9avInM3lfewNtW5qtpEtmfjLOvv2kzs01ifIOZErvH2YezsNR9ype9keTlNdkcYM7xnmlieLELpyLaqZaXLt5fdwIXbCuoqjVEntmOrbij1oHx7Vd4u2jftU8KmjGsOKTMs3tBkfvzTVFP7jj19tfH+cPXHjjtVliNWb+X64H1kxs9XFbFMcJ0oiyWy8v63JddmHyPsW3P9+KkmS5fV59qGZSjf17p+qnB5pf1KzmxoWdD83PlLqg2vE65dFttfI3x7W3T+jdTjY5sgt6szjBWNMxZror4ORetyg7dWl9YZ618au2assJbS8ZTNZ2LdqVJzzcL4PVKbMfali/n3/D2+meI4w3vQeZxjTIcyNn9fsCBUeJ0HAAAAAAAAAABryFBElmddRA8i/5YFAAAAAAAAAACsAXz/saj++q//Wn7+539elpdX9u9ZZ50ln/rUp2QwaP472scff7zs2LEjW77lllvkp3/6p5P6Pvvss3LXXXdNxIvlGXfXXXfJc889J/vuu29Srptvvjkpj982jTFNU7e/1Q8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQIG//du/lXe84x2yd+/ebN1P/dRPyVVXXSVLS0utYp955pm55Ztuuim57xe+8IXcA5hOOOEEOfTQQwvbHnbYYfK6170uW15eXpYvfvGLybnCut761rdG205rTNPUbi9jqpSysy4ho+eoliLzNFfTUDXelPlQutmczctcK2Ui6yvmpuG4U2I3iePrCceTtQm2r7RPryUbs47MWcvtuTrD+XV9JuqNxMz6B+PP1+FfI3WqeJn5Du41ZSrDmOHjAyeWlaupIJZWacth54l2Jc8wDLel9q27XkRsWR0iIrriaZpV/VPbtGk/xlbV24UW9bVmit/HXYxbGfdk2dTx+VpS2tdpm+s3qqnsOJ04ZcRyVa4vOPkYf77SaTE8H8sEJ6jx84KNnJNjfeuuH0uXpdKRE2dYSlamuzaEvczYXCl//fBtXDDjVvjri1ttrevrry8umfUxg/YiIlbnY2bXi8SY/hoWxhmXtbHlF6GV3MbFdLGG+X7ZfcH4sRLWG8S0JvUCOFv+c4WpmKtUqXM/6xx91Fl3Lsfvx5rW0eU4uj4W6gjvTfs8fgAAAAAAAAAAAAAAAAAAAAAAAABgLfv85z8vP/uzPyt79uzJ1m3btk3+6q/+StavX986/hlnnCEbN26U559/XkREbrnlFrnnnnvk1a9+dWXf7du355bPPvvs0vZnn3223HXXXdnyn/3Zn8m2bdsq89xzzz1y6623ZsubNm0q7TfNMU3LDH/DHgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACw6P7xH/9R3v72t8sLL7yQrfvxH/9xue6662SfffbpJMe+++4r55xzTm7dhz70ocp+9957r1x77bXZ8tLSkvzCL/xCaZ93vetdMhgMsuVrrrlGdu7cWZkrrOfnfu7nZMOGDdH20xzTtCzNugAUU8rOuoRCuse65mHMTcZXt+467fuc777Nw/4so3R5fU3qD2MqZWrlTImZ3F4bV0N1f98mlit1u4xtn2gbqyeod6J/EDsXN1ZP1rdwczOxWOFjA/2yzndQE+1U8Z+LlsPOE+3D7QXPMqzqUyeWiNhY/1zfQWR94rMWk3LUe26jjdVUR82cc6/L8Zj8+zh1vpUZptfic8TaVm0vjT2qIzy+k08lZbmzbS6ascVtwxjBnE70zxXq+lgzua2sb931Ramyc0zQ1vj21m3NtyvsZdza7Nrj27hgxq3w1wW32lrXz19v3AnZmsk96K9FVudjZtePMKbv58cRxgxyJrWJ5AhrlGyum1/U/P2ItflxhjGzeSmoKbatSaymuoip3VwYG7zPexiHv483HcxBH/M5ixypupy7psJ71nmYF/TEDrN7gIViF3BMAAAAAAAAAAAg0VBElmddRA/4/gMAAAAAAAAAgLWL7z8WxS233CJve9vb5Pnnn8/W/eiP/qj8zd/8jWzcuLHTXJdeeqlceeWVsnfvXhER2b59u5x99tnyz//5Py9s/8ILL8gFF1wge/bsydb98i//smzZsqU0z9FHHy3nnXee/Lf/9t9ERGTPnj1y/vnny9///d9HH2D02c9+VrZv354tr1+/Xi655JK5GdO0LNhv7QMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUj388MPy4IMPTvw89thjuXbLy8uF7R588EF58sknC2Pfeeed8ta3vlV2796drfvBH/xB+YM/+AN5/PHHo/FS6inyqle9St7znvfk1p1zzjny+7//+7kHAImIfP3rX5c3v/nNsmPHjmzdQQcdlPQQIhGRyy67TA444IBseceOHfKWt7xF7rnnnly7F198UX7v935P3vGOd+TWv/e975UjjjhirsY0DUuzLgB5StlZl1BI91hXV2Oe17nr0zTGrJXpPGasbqWL18eOP1VSW9O5SenXdt5ncazm5lZX7FO3PVpnuD2y30pj+T5BLVmdQezYsTFqE/Yt3p6pOqTD/iU5J5Z1vrOKrJ9YHl+ndHlbHW6vaF/UJjWWY6P9B8Xry3Kmbq/bTkRsWT0tY/caY7Uz7k1Vdy5cv5T9psywWY4gV63+Lmd4/KvUWGbsZDPRNjgRGZtvF8uRrVf5frkCXR9b8/odizl+Tgm2TaTK2gYxjG9v3dZ8u/GzVtbT+HOivw747S6YcSv8dcKtttb189cfd4LOVeRi+2uT1UFMP77I9ux6E+Yc75vQpmtZvVPI1ZS/d7O2+r2YHS+mfDyrYdzT5u+dzRTnpI/9MItxxIzfy3KsAQAAAAAAAAAAAAAAAAAAAAAAAFhEb3jDG+Shhx6qbPfII4/IUUcdVbjtvPPOk+3bt0+s/+xnPyvf+973cuu+8Y1vyGtf+9radb7pTW+Sm266qbLd7/7u78rdd98t119/vYiI7N27V37jN35D3v/+98uJJ54oL3nJS+T++++XO+64Q6xd+R2y9evXy7XXXiuHHXZYUj0/8AM/INdcc42cccYZ2YOHbr75ZnnNa14jJ510krzqVa+S733ve3LHHXfIE088kev7tre9Td7//vcnjnx6Y5oGHngEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFgIg8FArr76arnwwgvlqquuytY//vjjcsMNNxT2OeSQQ+SKK66QN77xjbVynX766XLttdfK+eefnz3UyFort99+u9x+++2FfX7+539ePvaxj8lgMEjOM80x9U3PugAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALqyefNmufLKK+XTn/60nHLKKdF2Bx54oPz6r/+6fPWrX5UzzzyzUa6f/MmflK9+9avya7/2a3LAAQdE251yyinymc98Rj71qU/Jpk2baueZ5pj6tDTrAjCilJ11CWuansL8p+7jLmtJyal0s3yr9ZhtOl6R+JirYsa2dzmHTcbl82d9tSmP7bYnzUOsbSRX1te3j40nlyPsm18f5bcXD7e8T3RZuVqK12ev4fpxvvNEW12xXNE+tq5gvY22S3gqZGKO2tvH2JQ6asbstG9hvPQnas41M4xvqztnxqT3c22r9r3y9TXZf3XqEZHw7KQajCdrmy2H54QgVnI/ETH+nBjEsEEfEzvPlmyPbPOpfIqVuoIYxre3butku4k1xq3Jrll+uwtm/LnTbXerrc3Pzfg1Koztr1XWX7vc/NvI9iynjx1uL2jTpyx/ZMwr43DvJ6sLt4fxCmMm5qpqvyjK5srz9/Ymsr3OHE1jPudxn1XN4bTN4xyhBWPK73NWK1Pnww4AAAAAAAAAAFgsy+5n0SzimAAAAAAAAAAAQBq+/5iWBx98sLfYl156qVx66aW9xa9yzjnnyDnnnCMPPPCA3HHHHfLoo4/Ks88+Ky9/+cvliCOOkNe//vWyfv361nkOOeQQ+ehHPyof+chH5Oabb5aHHnpIHnvsMdm0aZMcfvjhcsIJJ8hRRx3VwYimN6a+8MAjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMDCOuqoozp74FCZ9evXy4/92I/1nkdkemPqGg88mjGl7KxLKKXnvL5Zmvd9NyvTmBelTOf5u6y7rL7C9nold9g3q8u18dtVthzUrU1ufRZbp9eU3FfH203WVRwr6xuMc2J79lrUVya2dSaMObGsXC3F67PXcP1Eh/E+wbaJ5TBmRfuSbTbWVg+S+idvS9huYzmb5GrbPte3Rl0t2aLjYkqUrThHtJkHMwxiVYzTjNWS2DZ2/Cifu80xENYV5AqvHipr1yBn1te9z40/9yXEivXNCnMxbNAu6x+etyNxSraFKVZyBDGMb2/d1vFa3LUl7Gny9a70dcGMS+6vTW611cH2XN/i2Nm11J30bd3tBfx10QZ9JZsrX4s/pt2KYTx2No6E/KuB/9xhbPl4srlMbJfStkpqTqytuVpLYwUAAAAAAAAAAAAAAAAAAAAAAAAAoEuz+816AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwZizNuoC1Sik76xJmrqs5YC77oZXpPGZsXyldvF53uG9jOSr7ldRQN2adY7VpvXXqyHJok1tf1U90cf9CkTZZ3yDmxPbsNVgvIqIkt20lZ7CceiiXPQLQb9Mqt1pF1mfL4XrfIVwvIqJ18fJE7Ei7qngiYqNtB2kxU7ZX9LVhrpr9O+lTVUMCqxbrmZFdjkfZ4E2XOt9m6Non1GJMeVu3PXa8KZ+rKEZV7ApFZ1LVOGYwl8afE10cM7Y9XOfPHSaoyO/rif0UaT9+DoptM+E5PEiRxQj6G99+Zb2VfNuJnj6mVbm+1m8w/tzp+rvVdvw65Nqs9JV8Wxue620ud7g9azd2HfX5lLuAWFPcp8rKtdfXvjIOa4uPJ98nVudEez8PBTXGYkXXl8TK968eR5/8fa4JcqfWv1r4e2mTeCzMu3kdT933HOaMNZP3LotgEccEAAAAAAAAAAASDUVkedZF9GBY3QQAAAAAAAAAACwovv8AFtFi/bY+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACYS0uzLgBAfUrZWZewMGJz2WSOlTL5ZT27/dQmd9ZXj8aTzYVbH4udm7MgxkTs2LzrfC7/WL6VfmONw0f21X2EX6x90XqtcotKB+uD7dmy0sXrdUGScF2sbVHfgvW2MMcgqW/j9bn8kVw1YtRqJxIfXwUb7qcuNKxlVTHFT46tO5/KuvNEypz5nHWOi1z/Ua6y41NFtxTHSno/hDlq9A2SBssp/YM+xp9PXV8bbPfnHlNwno5tC8+Bxl8vghRZu6D/WAn+fG8l33aip+9jVdDPbTD5uRm/Rlld3Ca7zpmKdll7d510+8Ga5KNnqrK5cfX5+xVrp/fsWz//1tafo7D+PnJ0oW3+Wdcfmrd6AAAAAAAAAAAAAAAAAAAAAAAAAADA4pjeb7kCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA1iwceAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA3i3NugBglrSyvedQU8hRN6fSzWuaxXhWcpuSbcV1tRlrNFfLmF3MoY+R1aJNceyC9bE+4fpKYeySecliT9QX5vavYT/JrZ/4cxeK4mnl6iheP/Hq+Q7heq3zr0Uxw7aJy3Zi+2AyRyxWw/W2LEdVzNTthX0S8o6xEzuwgZo5V02ukBmmt21aZ5AjZf8oa9Jy+thNjiszyhEe16pNzIjwbKmMH5/O1VK9PHbeMLZeH9/ez78Nzv1hu9RtBdsnUmR1F/Q3vizrWuTbTvT0Ma0K+mXJXLuVufLXLeuvY66NNcF5OBtPvp0PbW2kfUmOrG+YexiPlSrLWVJXaX8/dwXzEIvdNmcRf39uOow5D7ocVx/z3rXVUCMgIqN7hzr3P6vFIo4JAAAAAAAAAAAkGorI8qyL6AHffwAAAAAAAAAAsHbx/QewiLp+XAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMCEpVkXsFZZq0RERCk740owL/QqPxaaHMtKF/eZxlzE6m00DmWa5XLjr+o/3iY2Z33I6vZ1+tzaFG7P0UG94RxMxPSvEvST3PpOH9MXxtJqoonSkW1+2b8qHdkertfRXNG2kWU7sX0wGTM1dtX6LGeNHHW3Z+1KckTYcP5TNcg1lViz1OU4TOTJslU5CvpV7WNlTXlsHzP1OBzPHcs5kcPX0D5HwRkiQXgtcXWEdWXLLovx59ugbhu0G2+bpVDF6yPbfQofeiV2QX/jy7KuRb7tRE8f099jZ/2yZGOxw3O6u2a5Nj6mvzz7+/aQvw7aoL+IiA1zVFi5prr+NR7MnI01yJnVF86J8Z9DjNve/TNwYzV1miMYX1dtAQAAAAAAAAAAAAAAAAAAAAAAAAAA0K/uf7sVAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgsDTrAgAsPqXsrEvohNJp40htV9Y3m7OmsbTJxcnVFNsWrPe5k8fj+uf6hPs+jJm9hv0kt744n8ovm0idVY/2C+IoXbAtzOWXfeOJ7Tr/OhGvoKiJtvllO7F9MBkjNXbFehuLXRWvTpuqHL4W1eLZjIk5Ou+baGKfrjLKmPIGqXNohvX7BX1ix4mypjqmjxXuDz++yPrwfRKcBVb6F8WoEJ7NJmKXyepOOZGKiAT70Z9L/Zza8XGofJuq9eF2R7kRZqFz24MYweER8teNrJePacPz8vg4R2NTbpUN22bNbL6dv066/tbU2jO1KOWPs3ytud3pj0Xb7blk/LofjtHfI0TnrEk+1c84+tDH+LH6cTysctbmr9mLwi7GZ3AAAAAAAAAAANDEsvtZNIs4JgAAAAAAAAAAkIbvP4BFNP+/VQoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFY9HngEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB6tzTrAtY6a1VuWSk7o0qKGVef7qEuP/ZZjrnP8aF/ZceO0sXbYn2i6yNxRn1MSXXpOVrRJhe7rN6uc4nP5deX5dYt6/OP59OqpI3bZhJzuPYqfPTfeA7/5zCv75RtD4LoYPvE+oLnDQbr7ETMwWSfstwpbbNcibHrbheJ1x3WMLEjEiTG7qxfgYn9tIbUHbsykXNmyv4ww3p9XPvYcaXsWC1hLJ8rHJ+vP7I+fB8Vnq1iMSr4s5oK+4/PaTRmOO9B3zBWeC4dn0MbtMlS2PL1IX/+dSOzZZdT5ctwbbOZdbFNeJ716/Pl27DdWFt/Obf+eubmqLDPeGlBLt9/PEa4bWJ9aYZm/HU6/JwRb++OYdv8fJaas25tub7+GKjYL3VoN3YTjL2PXLPkP+uYBvMOAAAAAAAAAAAAAAAAAAAAAAAAAADQtbX7W/oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGBqlmZdAPKsVSIiopSdcSWrB3O2+JQysy4hidL5YzB2TIbtushVq6+rK4uhTW696LTtfn1xkkgsL4sh+e1KcutXXv2GFoIYKhZ7fNn/2TeeaKvzr2G/ifWR9iJiJ9oOJtqUxqxql8uVGLvudpF43T63qvmsxYp4nfUJTOyPPnVQ70yZYVKz1DlVpuDcUjVHYQ2x9q5d0XGorEnLVcXVX/Q+mziT+bH6uYkuj2KFZ/5aZ8YsVniirWhnxrL6ebPBPipqW7Y+2K7GRmazUQV9jC/BFrbLlow//9qg30oo61ea/Bz465z11z03R74Sfzvit4f9G8litT8PZHNjguudH5e/Z460axOzbn8svqpjownTQ8wm+hgbZsiYlWvfIlnEMQEAAAAAAAAAgERDEVmedRE9SPv3KQAAAAAAAAAAYBHx/QewiHjgEQAAAAAAAAAAAAAAAAAAAAAsqDvuuEN27twpjzzyiIiIHH744XLMMcfICSecMOPKAAAAAAAAAAAAsBbxwKM5Za0SERGl7IwrGTGuHj0n9awm87Yvq2hlZpi7vzmKzX90vY7XojqaoyZxUo8j3y4bh+5vv2Y5xmuLzV9WT9hXcutXXv2GGir6qDB22H582TfO2uqgrZ7sk1sfaT/GTrQZlPcpiFG6XkRsGLNFrNH2SLyi3KoiVoOYrfqMmZj7LrSsaVWqO2ZT/sTZlP2iTHBOi9UQ5ipq59rEjlVlTXFfHzvlOHL1hu/FBme4nKIzbRbTz1G0Prfd2LR2uVVB22yOVD6mF54jS7YrNyqbjSRo61L560fYLlsyQc6xa5Tf1TYbm1sR9kmU1TJsvkez66HxtbU9OhrU4O5LrO3h3NiAvy/0n0NS+XsgW7PfIqs7h5jE8QQAAAAAAAAAAAAg5v7775fbbrtNbr/9drntttvkjjvukGeeeSbbfsQRR8iDDz44k9r27t0rH/7wh+XjH/+43HfffYVttm7dKhdeeKFcdNFFsm7duilXCAAAAAAAAAAAgLWKBx4BAAAAAAAAAAAAAAAAAAAAQIKbbrpJPvjBD8rtt98uTz311KzLKbRz50555zvfKXfccUdpu127dsnFF18sn/70p+XKK6+UrVu3TqlCAAAAAAAAAAAArGU88GjOWasm1illZ1DJiHH16A5r8GOc5bjmQR9zuxYpPX/zV3Vsl9WslKlsk9uuTXphrm1Wn4sRxgq3Z+v9csn4VmL512C9P8Xp8DV/7lNaJtjEoU709bHD16IOWRudf41tz9YXt7dhOxERPSjuG1uuWG/DeC1irWwviRnmL9pZLWO26iOReZ9S7jZK9+UcUGbYrGPVuBLixvapMsGJIZZrPEdFm/CYVv7kE/bzMVOON1dnuI8n7/rq82fk9FjhydTV7+dyfDzZOhfd+HO5a2Mj20Nl29025UZis5EEbY1PXdxuopcpuKfO+vo5GI3DXXrF+uudW5/NbbBdjOs3dj0M+8pEn/L3QVZbwtvM3ytYGxyrrp6izxO5HAVz07RtVc7yvsXjwGw02Yd9MTOsZZ7mAf1QxjS/p5ljE/dEAAAAAAAAAABgDRmKyPKsi+jBfH2n85WvfEU+97nPzbqMqMcee0x+4id+Qh566KHc+q1bt8pxxx0n1lq5++675b777su2ffnLX5Zt27bJl770JTnkkEOmXTIAAAAAAAAAACX4/gNYRPw2KQAAAAAAAAAAAAAAAAAAAAC0sM8++8iWLVtmWoMxRs4666zcw44OO+wwufHGG2Xnzp1y3XXXyWc/+1nZtWuXXH/99fLyl788a/fAAw/I2WefLdbO3388CQAAAAAAAAAAgMXCA48AAAAAAAAAAAAAAAAAAAAAING6devkn/2zfyYXXnih/PEf/7F8+ctflmeeeUY+/vGPz7Suv/iLv5Bbb701Wz7wwANlx44dsm3btom2Z555puzYsUMOOOCAbN2OHTvkqquumkqtAAAAAAAAAAAAWLuWZl0A6rNW5ZaVmv7/pGJcDbrD3H5cTcczPi91YzQZT9t61zKl281Zn3NeVptSJqlPm/qyvolzVJlLm4l2bec/nqsgR9t93eKxfBN9tSp+9Q21kglaB6+qfHuw3k6sH8RzVK2PtLNFMcviVm0TKa5zPGedHVMRq3X7MRPz3USL/GWi+2kVqzsmZYZpDVPiRmLFjgFlgvN3UY4wZtjGbQ+Pf2VNafvRtopj09Xn5zSbK9/P1x9bzq0bxQjPvmqiXawmP1clNWcxXFTjsvm5scH2rF9QVdi/QBYyG4HNl1nBX49yGXxfW3Den5KsrmF5Df7ewx/buVuRbLrTxuHvBcLPErGco7blx25qzFi/Jn0x31b7/lzt9QMAAAAAAAAAAACYjvPOO09+7dd+TTZs2DDrUnKGw6FccskluXWXX365HHnkkdE+Rx11lFx++eVywQUXZOve9773yc/93M+J7uLfYwEAAAAAAAAAAAAF+CYKAAAAAAAAAAAAAAAAAAAAABIccMABc/ewIxGRL37xi/LAAw9ky4cffri8+93vruz3i7/4i3L44Ydny/fdd5/s2LGjlxoBAAAAAAAAAAAAEZGlWReA9qxVIiKilJ16buNy6xnkBlLE3hezeL8o3WFObbrL5dpmfVzsbI7C9X452F6aUwd1qfz6lVd3PmvwOL6JPloVL/tX3yFbX5BUV7TJtufX24l2g+J+Zesi/zuWDWNVtK/cJjJZX5izzg6piNW6/ZiJeZ5CzirR/YPkuVFmWN0oFivSNzxWlCk4h4Yxw1iR7eH7Q1kTr9HHrDh2w7OpKmxVj48ZjeXnJKvNLZuxaqJ1B239nNhgnv251Nji9ePbYm2zPvnU/vpisxG6fqZgxP5alPX1PXy9oxXKLVp//XPrs7mMbB+P7beJKT5m/bW2y7uSbC6Kxj6j2L3W5OfQdh8bs2WmuE85ftYwa1augYskvAYDAAAAAAAAAIA1ZNn9LJpFHFP3rr322tzyL/3SL8lgUP1vdgaDgbz73e+WD33oQ9m6a665Rt7whjd0XiMAAAAAAAAAAPXx/QewiHjgEQAAAAAAAICpeuCBB+QrX/mKPProo7J792457LDD5IgjjpDTTjtN1q1bN9Pa7rjjDtm5c6c88sgjIjL634+POeYYOeGEE2ZaVxuLOCYAAAAAAAAAAJB3ww035JZPP/305L6nn3567oFH119/vVx++eVdlQYAAAAAAAAAAADk8MCjBWKtyi0rZaeW27jcuoOcfhxt6u8iBmZPKTPrEkprUDp/fIXHW+z4azOuMOdk7IRjXpv0tuPtfe6kHK6NlqCv5NavvLr3rF9OEG3rYmWv4XrfMWun86+FsSJt3LKdWD8obBddLtlmw1gpMSpzVP+vXSIits4OSYyZ3K7AxDynapEzJrpfpmGWucuYYSdhyuZWVeUI+0baFx1LygTn5qpYke1F7xtlTXlMX4+vITwPhPEmMpQw+dz1YxVds2L1Bm1NMBfZPLispuB6Em7z1wdXuc0qDvq60P56E7YbH6c1xaNe6evHMarfX7atux6Kqf8ezGIPIzPeInaWwxVqrQ7Wu9zB/XG27GuLzEuzWvI5sPi63NdmCscNxyYwW5/5zGfk8ssvl1tuuaVw+4EHHijnnnuu/PZv/7a87GUvm1pde/fulQ9/+MPy8Y9/XO67777CNlu3bpULL7xQLrroosYPZXriiSfk9ttvl9tuu01uu+02uf322+Wxxx7LtXnggQfkyCOPbBR/3LTGBAAAAAAAAAAAZu/FF1+UXbt25dadcsopyf1PO+203PLOnTtlz549sn79+k7qAwAAAAAAAAAAAMbxwCMAAAAAAAAAvdq9e7f8i3/xL+TKK68sbffUU0/JRz/6UbnmmmvkiiuukDPOOKP32nbu3CnvfOc75Y477ihtt2vXLrn44ovl05/+tFx55ZWydevWpPiPPvqovOc975HbbrtNHnrooS5KrtT3mAAAAAAAAAAAwHz5xje+IcPhyn/odcghh8hLX/rS5P4vfelL5WUve5k8+eSTIiIyHA7l3nvvleOPP77zWgEAAAAAAAAAAAAeeLTArFW5ZaVs7zmNy6k7yOXrn0bdXpP6U+vscjxVsdrkMlaLiIhWpmF10jh3q3w6LV+srtT+XaqTs3I+dfH+8v3KcmXbXIwsV1V9OojdZA61qm5TN4ZfDtcrHWzXwWtBLRNtdG6zDZZFD4r7x5Zj60TEhrEq2kfX59pEYvqcKiFGQpwu+kzMbRNN6oyI7o8+TDNXl1LrNsPqNhHhflBVsWI1FfQLjzllgvNqGCuMUbI9fG8pa/J9fNvwuPc1+PVu2c9DbvxBm8pzguvrx510Ns5i+9aRnGE7464Pfh5sZPu4cJtbVjJatlnFbnvVbcvYNSroudLXpl2T/HXP9/e3THb8WmyKjz1/jY1dMbPYww6ujx1QbnDWlh9P2bgS53BWVkudVVLrNw3GOcu5aVJvXat936MHxqxctxbJHI5pOBzKueeeK3/7t3+bW3/wwQfLCSecIPvtt5/cd999cuedd4q1o/P1t7/9bXn7298uf/d3fydveMMbeqvtsccek5/4iZ+YeBDR1q1b5bjjjhNrrdx9991y3333Zdu+/OUvy7Zt2+RLX/qSHHLIIZU5Hn/8cfnMZz7Tee0x0xgTAAAAAAAAAGBeDUVkedZF9KD5vzdZK3bt2pVbfuUrX1k7xitf+crsgUcio/9ggQceAQAAAAAAAABmj+8/gEXEA48AAAAAAAAA9Obiiy/OPexo3bp1cvnll8uv/MqvyPr167P1X/va1+TCCy+UW265RUREXnzxRTnrrLPk//yf/yOHHXZY53UZY+Sss87KPRjosMMOk+3bt8u2bdtybW+44Qa54IIL5LHHHhMRkQceeEDOPvts+eIXvyhKNXugmtZajjnmGLnnnnuaDyIw6zEBAAAAAAAAADAN4cN9Uhx88MEL/dD/7373u7nlJmMN+3zve99rUxIAAAAAAAAAAAAQxQOPAAAAAAAAAPTi/vvvl4985CO5dZ/+9Kfl7W9/+0Tb17zmNfL3f//38uY3vzl76NF3vvMdueyyy+SP/uiPOq/tL/7iL+TWW2/Nlg888EDZsWOHHHnkkRNtzzzzTNmxY4ecdNJJ8vTTT4uIyI4dO+Sqq66Sd77znUn5tmzZIieffLL80A/9kJx88sly0kknyebNmzt9uNC0xwQAAAAAAAAAwCycddZZtftccsklcumll3Zey7zYvXt3bnnjxo21Y4R9nnnmmVY1AQAAAAAAAAAAADF61gVgeqxVuZ8+GavEdJSjTb3TGGvX/Nx1NX/ontJWlLadtVfKZj+iRz9KmdFPVd/Ydm1GPyX5fK4oFyPLodxPVT+R0dVFj9WnZPSjg59sHKOfWrQa/YTL/scH9T/ZNh38xNYX/DhWa7Fai+hB8JMQIxpzkPtZGVdx++j6bHtY22CiiVU69xOf6/I4nfWRsbltomHO8nqC/dGHHuqea0XHZsNxh++b5P2V8v5wx2L0mKyqP+G9Vxmr4n1fOPaKc83KunzOiXEmncMqzp9ZrOB8HZ70x8/dE/NYvH7iuuGvK+56k11/gutNIX9dc9e5lb7uWppdm931M4GPEe1Tcp0WkUa5UtfXjdO2bddmmRuTuvi81ednn2l+BgYQd9lll8nevXuz5fPPP7/wYUfexo0bZfv27bJ+/fps3Z/+6Z/K/fff32ldw+FQLrnkkty6yy+/vPDBQN5RRx0ll19+eW7d+973PjEmfl0XEXn1q18tTz31lOzatUuuvPJKee973ytvetObZPPmzY3rLzLNMQEAAAAAAAAAgPkSPvBow4YNtWOEDzwKYwIAAAAAAAAAAABd4YFHAAAAAAAAADr3/PPPy2c+85ncut/6rd+q7HfMMcfk/mfm5eVl+dSnPtVpbV/84hflgQceyJYPP/xwefe7313Z7xd/8Rfl8MMPz5bvu+8+2bFjR2mfDRs2yAEHHNC82ETTHBMAAAAAAAAAAJhvStX/T2Ga9AEAAAAAAAAAAACaWJp1AZgdaye/mFTKdprDuBy6g7i+3iY11u3bpO429aF7qftB6eJ2SpnGuaZxDMRyxMYjumQ8blsWMxYjlit7LdjmTzM6fHXvl/Cxe7rBP5jwffyrDxrG0jp4VcXrC/rYcJseFMdOXR5jw1hVfUpiFdZWlHNi4pvH6qSPFMzxjEX3S5emkWM1KZoPM2wUKtx/KiWO7xNpGx6jygTn1bD+8TiRbeF7UVlT3D7kc4/X5Nb5sWdjrvneCq8AaWdlV4/x14MgZ1avyrfz47djcxm2CddnMW2wPmhvfArrto73H63Lepqa1x5/3XQXtSzzsCSOvw6b4Nj09UX6ZtfTbP+uzK2/VSm6py+M5a7z4T1rSn9/X2Tt9M/V/n7cJI4TzaQeR13qc5/OYjxt9VEzn00TGLtynVok4XV0hm688UZ57rnnsuVTTz1VXv3qVyf1veCCC+Tqq6/Olq+55hp53/ve11lt1157bW75l37pl2QwqL5HHwwG8u53v1s+9KEP5Wp7wxve0FltTS3imAAAAAAAAAAAdRkRafbvLeZb/jud6667TrZu3VorwsEHH9xlQXNn8+bNueXnn3++doywTxgTAAAAAAAAAIDZWBvffwBrDQ88AgAAAAAAANC5G264Ibd8+umnJ/d94xvfKEtLS7K8vCwiInfeead8+9vflkMPPXTmtZ1++um5hwNdf/31cvnll3dSVxuLOCYAAAAAAAAAAIps3bpVjjvuuFmXMVd44BEAAAAAAAAAAABWEx541DNrlVirotuVslOsplpYa1f1mbG4umVMX2OT2ur29XW3rblNDX2wZmV/KD1fx2CZWb9f2s6VUv09ZTE2N2U1V40n2+5ju+VejhkdP09WtvPrlM4va51/DduH28N2ImIn+g4iMSf7Fq23vn9C2+TtZTEDVlXkaBCzVR8pmOMmGuYuUrqPujKNHIsinCvT7Am8fr+qlP6JOf2xq0zk3D4eJ4zhtwXr/XtUWVPe3r9vxnOH55ugnOzs6fuEMcL3oss1/h6NxljpFCzH3t+unfHXmbF2fuz+XG0i1xy3XbmR2qw6W1xKCX9dyzKZfFk2C+ZWDIuvWePXYqv9GPPHU5ZrWHzP3cUV1t9vWNvs/Dp+nR+/Z+xLNvbsHrld/fPKlHwmFJn8HNZnri60qbfr+rqcuy7NQ12+hnmoBWvXV7/61dzyqaeemtx306ZN8trXvlbuvPPObN3dd9/dyQOPXnzxRdm1a1du3SmnnJLc/7TTTsst79y5U/bs2SPr169vXVtTizgmAAAAAAAAAACQbr/99sstP/HEE7VjPP7447nl/fffv01JAAAAAAAAAAAAQNRi/RYpAAAAAAAAgLnw9a9/Pbe8devWWv23bNmSW/7a177WuiYRkW984xsyHK48fPKQQw6Rl770pcn9X/rSl8rLXvaybHk4HMq9997bSW1NLeKYAAAAAAAAAABAuqOPPjq3/NBDD9WOEfYJYwIAAAAAAAAAAABdWZp1AWudtap0u1J2SpUU8/V1WYcJxqwbxu6jti50WZefq9gczesczKPUOVLKdBZT6eLtsfVJdFp9pbXF8vv1iTlW2ruc43H921yHr+6Yrfu4PV1wrvTrfDC/rHX+NWwfbg/a2fFlPQhihDErlrOYg8L1ZX2i62O1lbCpE14j5qJSZvQLwqX7rC2Xg/luIJwzMyxuF1G0X1VVDN8n0s4G71VlCs6hsboj68P3rLKmuJai84TP77e5ZT/26J1n2K+AP8uX372OxwouBhM53LIZv364bTaI4duEyzFBivFrlM1G4NaZyhHl+Gus9ddLE38v+7x2mM+RxaiVuaIunysYT7g+y22Ll1vVUBE7ViPSdLGP2sZrU0P4ObCtrudjteVHR+xQxCzgs8BtvXukvjz11FPy1FNP5da98pWvrBUjbL9z587WdYmI7Nq1qzRPile+8pXy5JNPZss7d+6U448/vnVtTS3imAAAAAAAAAAATSy7n0WziGPq1g/+4A/KYDDI/oOExx9/XJ555hl5yUtektT/+9//fu57gsFgwAOPAAAAAAAAAABzgu8/gEW0gL/VBQAAAAAAAGCWvvvd7+aW9913X9m0aVOtGIccckhu+Xvf+17bskRksrYwT4q+amtqEccEAAAAAAAAAADS7bPPPrJly5bcultuuSW5/44dO3LLRx99tOyzzz6d1AYAAAAAAAAAAACElmZdAAAAAAAAAIB+7Nq1q3afgw8+uNEDc8bt3r07t7xx48baMcI+zzzzTKuavHmuralFHBMAAAAAAAAAAKjnzDPPlHvvvTdbvummm2Tbtm1JfW+66abc8lvf+tYuSwMAAAAAAAAAAAByeODRnLNWVbZRyk69ji5zGhdbN4zpa6tTU90+bWvsooZ5zdGFpvOqdPNxpfatk6PuPNdpH61Dm/LlslyubbQOv97lbjPfUbriHBduH19WOr9O6+A17Btu17nNNls/KKhD11vOYhbEKmlfuU2kuL6i3KoiziIxw9Fr4tzMjdVa9zzxc+fnsgH/PlVVMRJz+XOJMsXn49JYkfX+/aysybcblx1P7r3v8wfLE+OtOueMj8P19VcDFeaI9c3Ox5HaZCyHcdH9OcwGMfz24ByvXFVW/Ho7EbqKv85lVzsTlOJXmEF5PxGRYfG9c+WVVBfnmBdKuePIumPSj91Uf2ZB/0zCZ8dp67KmlM/GXZtFTqAPZ511Vu0+l1xyiVx66aWt8oYP4NmwYUPtGOEDeMKYTc1zbU0t4pgAAAAAAAAAAEA9Z599tvzX//pfs+VPfOIT8v73v18Gg/J/hzAcDuWTn/zkRCwAAAAAAAAAAACgL2voaQgAAAAAAAAAZkGp+g8Ra9KniXmuralFHBMAAAAAAAAAACj3xje+UY466qhs+eGHH554kFGRT37yk/LII49ky1u2bJHXv/71vdQIAAAAAAAAAAAAiIgszboAtGdt8S8jKWWnlrOLXMbF1A1jjdeUWo/vk9rejOWoqrNu7EXTZH+sFnXHo5Qpb6BX4vm2yq1rPXd6MrfSkZjalG/P2rntOog3flrQ4at7P6Q+Zk+r4uXxAH6d1sFrsD5rn29nJ7YX/C9WsRixZccWxSppX7ltgSkzOu4m9kcTZjh6jc1/DcrHcqL7tAtBri7qR4/8/gn3W2D8mPbHeXKsyHrrzoHKFsQL+/j8Pnew7I9pVdV+XLZt1NdfLVRZn1w/f24vy+XWGX9tcdtsEMMkXh996LEp89ctm124XCyT+Av+/hptcr1z/PW7skp/nTb5931W47C6pizXxP2529e2+fk1qyN1bhZc7HPXatNkHHX7mA7nahrzvij7FvUpY+LX6VVsXsa0efPm3PLzzz9fO0bYJ4zZ1DzX1tQijgkAAAAAAAAA0MRQRJZnXUQPyv+dxqIK/7OCf/iHf5DTTz892n4wGMhll10mv/RLv5Stu+iii+RNb3qTHHnkkYV9HnzwQfnN3/zN3LoPfOADotfov2cDAAAAAAAAAMwjvv8AFhEPPAIAAAAAAAAW1HXXXSdbt26t1efggw9unXeeH8Azz7U1tYhjAgAAAAAAAABgnj388MOyvDz5yxWPPfZYbnl5eVkefPDBwhibN2+Wl73sZZ3W9a53vUv+4A/+QG699VYREXnqqafktNNOk+3bt8u2bdtybW+88UY5//zz5emnn87WnXbaaXLuued2WhMAAAAAAAAAAAAQ4oFHC8xaVbheKdt7rjY5TBBLN4jl60mto277LqXk9nMSm4tZ1j9vms6BUqaznEoX1xBb3yinLs85kassd4d1JdPF56eJ9X5Z6cnt/n+Q0sG28H+WCtrZie2D4vZl64JlG8ao6p+6LdeuIsesGfcU0YZ1KjN6D07snxnUUkSZ/FNSK/d5GybyRNZ5PwZWOb9Pw30d5fdHQnt/XPvjPDlWZL1150RlC+KFffx7yucOz1/uVSW2L7ISo6JPtt2fy4Ncub5unXHR/XUgHHNw3VCuGit+vc2Fq8VfH4eRa1bWztda8B4Nt5W1LZDdY2TTsjKQ7HxpKurzsdx4rMnfN4b3kbHPEIugz7E1jZ3Sr6u6m8Sp2yf8LNfEPO4nYLXbunWrHHfccVPPu99+++WWn3vuOXn22Wdl06ZNyTEef/zx3PL+++/fRWkTtT3xxBO1Y/RVW1OLOCYAAAAAAAAAAObZG97wBnnooYcq2z3yyCNy1FFHFW4777zzZPv27Z3WpbWWa6+9Vk455RT55je/KSIi3/rWt+SMM86Qo48+Wo477jix1srdd98tu3btyvU98sgj5ZprrhGl+G4VAAAAAAAAAAAA/ergqQIAAAAAAAAAsOKggw6SAw44ILfO/6P6VOEvCRx99NGt6yqKk/LLCKG+amtqEccEAAAAAAAAAACaOeyww+Tzn/+8nHDCCbn1O3fulOuuu04++9nPTjzs6MQTT5TPf/7zcuihh06zVAAAAAAAAAAAAKxRS7MuANNnbfx/XlHK9pKjTVzjYukGMXwdqfnrtE+tq24N02aNq0/n6zN29Dw0rczUa+pS03kP52MRRefGr3dzkM1F1Zzogj9rf3xVFKNV8bLv6Jf1WCBdsi23ffRqJ7YPituXrStqU6Zu+1kzw9FrODdTjKFM/pwzsd+a1BJqMz5HRWLbDmJHxcbj9Zl73lTNxZzyx3N4nGf8PgzHF1lvx06uygYxwz7+vRTm9ufIoJTsrDzePowRvD+jMUJZf99CF2zz69yycdGz60Kwvsp4qcaHGvW1WaU2V7e/R/LXRZ/aZjXl33Pj9w5Z30RZLcP2/0NiFqtmDbVyuPtD6+4Xp5Fzmrochyn57DXPOco+M/ZVQ92cs4rZl2kcKyhg7eS1cRHY+fk8e+yxx8qOHTuy5V27dsmxxx6b3P/++++fiNeFH/zBH5TBYCDD4ehe6fHHH5dnnnlGXvKSlyT1//73vy9PPvlktjwYDGb+cKBFHBMAAAAAAAAAoImhiCzPuogerM5/KzJLxxxzjNx6663y4Q9/WD72sY9NfO/ibdmyRS688EJ573vfK+vWrZtylQAAAAAAAAAApOD7D2AR8cAjAAAAAAAAAJ07/vjjcw88uuWWW+Snf/qnk/o+++yzctddd03E68I+++wjW7ZskXvvvTdX27Zt25L6j49JROToo4+WffbZp5PamlrEMQEAAAAAAAAAMM8efPDB3nPYlv/Rxbp16+Tiiy+Wiy++WL785S/LvffeK48++qiIiLziFa+QY445Rk466aQuSgUAAAAAAAAAAABq4YFHAAAAAAAAADp35plnyp/8yZ9kyzfddFNy3y984QuyvLzyPzCccMIJcuihh3Za2/jDgW666abkhwOF43jrW9/aWV1tLOKYAAAAAAAAAABAN0466SQebgQAAAAAAAAAAIC5oWddwKIzVsRYlfwza9aq3E8fcZtqM0d1c9dpn1pXVcyUnE3noMv92fWxMW1KW1G62f96pJQVpWx8OYitlBGlTGWcTmuoWBYREW1GP8Fy8tzo0U/WXsnoJ4VWo5+qZaVHP35Z64KfYFsWI9/Oup+V7YPRT9i+qH+sjWP1QOx4rIr2pTnmiRmOfrqI0TKWMmbip7Wwtg7qXKl3WPnTm7Jx9TTe3k2x7sb7Jzyn9Kkkl1VarCo4p1Sd8yLrs/Nbyjkxa5PPlZ1/Yz9ZfyXV5/TgelHYd+XHX0ZW1snKT1e0FdE2cq0dbfMm7hFa3I/ExO4vwvuRyfuV4nuHsnucurW0ufdZq7r8TFYnTt2cXXyO7eOzyiw/s9T5/D+tvwswvP0wQ2eccYZs3LgxW77lllvknnvuSeq7ffv23PLZZ5/dZWkT8T7xiU/IcFh9PzgcDuWTn/xkr7U1tYhjAgAAAAAAAAAAAAAAAAAAAAAAwOKZ0yctAAAAAAAAAFjN9t13XznnnHNy6z70oQ9V9rv33nvl2muvzZaXlpbkF37hFzqt7Y1vfKMcddRR2fLDDz888dCfIp/85CflkUceyZa3bNkir3/96zutralFHBMAAAAAAAAAAAAAAAAAAAAAAAAWDw88mjPGquSfabBW5X7mIWabeaibs077ae6XmC7301qhlBGlzMqytqK07TZHLKa2o58U2ox+WtaQq6NO/rq0Gv2kbvfLSo9+/LLWwY+Kxw7aWvezsn0w+gnbx5Zj60TE6oHY8VhzSFkz8VObGeZ/2ghjtYipjCn96aXOLuYgq39Y+jNVVeOd9c8UtJ73PmoNz1c1tlulxaqCW/yqc2BkvT/fWT2oPm9m5+BIfRPn9KLzbnD+n4gRXC+a0O5HuZ+I7Lrpf5T7qUEpK6pGn7rtRSR6Pe/jnqYrhfcla0wX9+2z/PzRJndXnzHDOH1+Dpq3z+nokDGL+zNHLr30Ulm3bl22vH37dvnrv/7raPsXXnhBLrjgAtmzZ0+27pd/+Zdly5YtpXmUUrmfm266qbT9YDCQyy67LLfuoosukgcffDDa58EHH5Tf/M3fzK37wAc+ILrovmUGFnFMAAAAAAAAAIC6lhf4BwAAAAAAAAAArE2z/o6C7z+APvCbKwAAAAAAAAB68apXvUre85735Nadc8458vu///u5hxqJiHz961+XN7/5zbJjx45s3UEHHSSXXHJJL7W9613vkh/5kR/Jlp966ik57bTT5HOf+9xE2xtvvFFOPfVUefrpp7N1p512mpx77rlJuZ588kl58MEHC39CDz/8cGG7hx9+eK7GBAAAAAAAAAAAAAAAAAAAAAAAADSxNOsC0JyxqnS7VrbznDaSU7XIFcasG8vPQ53x+pxt6m6qKvf4fMTaNBkzminaB1XHjVKmZo6V9krbXA6/XEmb8uU+6eB1jKp6rJ5W+YZ+WSc8j8+3CV5t2FcPivulLo+xYaxYTX0wQ5ejooYSyuaPC1u5gyI1FGlaVyxmm3Ga8uN/4hipo2wORFrV7amqHIHK4xKZunO7mvn3d/i+z47R7Jzi3g8V75t8jKBPbNnX4l5VZHtOWEfYNtyeXT+K61cuu5Wx+02Tv7b6a+1KG7fdRO57ffsw5XgNpvh96a/vNrJch79/sDbtnFa3PVa/2Ge3Nm2rPoN2kaOv/inajA9Aud/93d+Vu+++W66//noREdm7d6/8xm/8hrz//e+XE088UV7ykpfI/fffL3fccYdYu3JlXL9+vVx77bVy2GGH9VKX1lquvfZaOeWUU+Sb3/ymiIh861vfkjPOOEOOPvpoOe6448RaK3fffbfs2rUr1/fII4+Ua665RpRKO3f8u3/37+SKK65IavvGN76xcP0RRxxR+ICkcdMcEwAAAAAAAAAAAAAAAAAAAAAAANAEDzwCAAAAAAAA0JvBYCBXX321XHjhhXLVVVdl6x9//HG54YYbCvsccsghcsUVV0Qf/tOVww47TD7/+c/LO9/5Trnzzjuz9Tt37pSdO3cW9jnxxBPlqquukkMPPbTX2ppaxDEBAAAAAAAAAAAAAAAAAAAAAABgcfDAowVmbPn/xq6V7SyXDXKpFrF9rLoxxsebOrbUXHVq8nV0Ob91xeotG4c1bptuVnfT/bYIUues07nRJrHdZM5oHX6965ONK3uV/PryU0xQhyp+reyng366YJuWQnpQHCu2PE1mbP/VrcMM88vhOGtQtvg4sqrB3IR1herWWRWvSUxHmer3j216fKTUHWqxD0VEVJOcEbZlLbPS5RxUapEr5dibGX/M+xrDZVk5PrL5TugzWj/q568+KmxfyLUxwTUr6xPZniJM23SX+muxmXzf+Gulv78qa1sWW/li/ZSO1Z7FDnO763l4f15VY1G/cN1kH+O2t7+mptY9T8pqrfos1iRmauzUOawz132Op4++VZqOBwvImMnr1iKY0zFt3rxZrrzySjnnnHPkwx/+sHzpS18qbHfggQfKueeeK5dddpkcfPDBU6ntmGOOkVtvvVU+/OEPy8c+9jG5//77C9tt2bJFLrzwQnnve98r69atm0ptTS3imAAAAAAAAAAAKYYisjzrInowxX8bAgAAAAAAAAAA5gzffwCLiAceAQAAAAAAAJiKc845R8455xx54IEH5I477pBHH31Unn32WXn5y18uRxxxhLz+9a+X9evX145rbbuHDK9bt04uvvhiufjii+XLX/6y3HvvvfLoo4+KiMgrXvEKOeaYY+Skk05qHH/79u2yffv2VjXW1feYAAAAAAAAAAAAAAAAAAAAAAAAgCZ44BEAAAAAAACAqTrqqKPkqKOOmnUZhU466aSFexDQIo4JAAAAAAAAAAAAAAAAAAAAAAAAqxMPPOqdEmtVfKtq97/Pt2FK6tIt6wrH3GScbWL4saWOw+eqyjFeU1XbqhpScla1qTvOJozVLofpLUdIVeRSenK84RxNLOtwu6nYXn9Oq2JmYus7EM8Zn9Non2lQo+NLtHtvaR28RtaHfxYRm7UZ5HME7aJS25Uxpn4sE+ybunWYYfH6cB5qULb8PWhVg7mK1RmqU3dqzAaxVbhfKtg2x0+P46hL1a1lkXU4F3WPp6nwx1HVOMePbTcO6/pmx4tvEzsHZutH/fxVp3BeJt5Lvk2QI2vvrhPGXwMic21KrnXxW+I0/jo6rBHI12k6fD/7OkxwD+3WW9N2oNWmmasL/j63Stnnubq6jDXPNbTJ0WV9ZZ95p222+35+5gEAAAAAAAAAAAAAAAAAAAAAAAAAgA6e7gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFBuadYFrHXWqtp9lLI9VJJngrp0y5xF46w7Dh+jTr+646iTI7WtryGWOyVOk7EvknkZd1iH0ml1xepXytSvQafVkDRnun7+4jj+deX9psJ14atvkK3XwWtkfQFbsq243ik868+Y5rlMZL/UjWWG6W31oFZoZdOPHavmp+5e5yS23xLUPobrjKNK3TlcFF3OYaDNsZDpsb5K/nhMGId1x4/y9cb6Rtb7Yz93txY9B/pWwfslcb7HT0XWZzTF11Rr6t+fi+Svvdmf/HV6WBHTX5NN/fekv5+wVgfr3XiC++GwfTju3DiCe9C6n12a9luLUuYo/GzTJEZdVTm7yN1l3an1doHjekFYmbgeLIQFHBIAAAAAAAAAAEg1FJHlWRfRgxn+WwoAAAAAAAAAADBjfP8BLKIpPPUBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACsdUuzLgD1WauS2illO8tpIjl1ixzhOFLrbdpPZGUcVXX7HCmx67TtS2xcsdrG57Bp3fMw7nmldGROIuvH26+q+ZzFI/N0SVI9SG/bkDKjJ2XaMFeMMfnlNjWFsUKtYic+ATR13GOUrag7YFWNcaTW7dWpv27sJjkcVbVvI2wXx3jTcaLxfitUdz/Mar/5Yy517GH78JgtGkfsuI7lzNq77cYt66C9Kbi+Bl0n1suoj8oticgw7V68iL/m2xYxJmKZxM8GNdtPK1Y0hxrtGGt5Rm9XUj9HisQ//7WJ2WVfkfQam2hbGwAAAAAAAAAAAAAAAAAAAAAAAAAAiOO3RwEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQO+WZl0A+mOtKlyvlO0shwly6Baxfb1162vSz9ddVW+d2FVtq3Km5Go6R3VY43Lo/nLMk3Cc4dwWzfVkH1McO7KfYu0Lc2hTGqsyV9F+bLtvi08txbTKvyodrNfBa2R9AVuyrRFjKnN6ygxdDYNmOco0HVdK7LY53bib5UibK2Wbj8OqinFMof6p5HBUm30e0fn7ahXpYz4ntDk+avZv816qy58L/bkxO6eE59Vwjl2/8avSxH6YOCb99kjMGD128TLBddCnSJ3e7JpbckF0128xNd/X7hpth9X38bF7/ew6b8ov2P5+xFqdzz3Wz+eL5orEirdLi7eahZ+PvDZjjsVMjV0nd1WuJjHb9BmXWlsdi3wsIpExlefLVWka9zUAAAAAAAAAAGBOLbufRbOIYwIAAAAAAAAAAGn4/gNYRGv3t9oBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMDU8MAjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQu6VZF7DojFVirIpu18pOsZoRW1KPpxrWFRtrnXGG9aXWUjSuqr6+3qr6fOyUWqraVuUcH0fT/dAHY0fPR9PKzLiSdlSP9Stdb3/VbT/v1Pgj9HTFeUbr4FXll2Pt54wyw8L1Vg+aBzUNj9E2czSNnJG5qs6RPpfKNhuHVQnjaFq/lzKOtjnq5iugmh4L6Hb/tYiZ/D7oo95U/txRcLxZty27ilQdkxPnIdfe+OuLX1655vq3/MRUVd8i55u763jKlPt7uvDKn8UYFt//2rCdiRfp73Gs1cF619ffoybEiufIx4q2a5FjllLrTfk8lZyzRayqvnVil31mbRqzSftxqTX1XUdXuhzPasoNAAAAAAAAAAAAAAAAAAAAAAAAAEBoPp8gAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFsrSrAtY64xVjftqZTusJM9G6lINcxaNM7X+sJY6Nfi+VX18fVU1jddSFbMqd2rOOrFjMctqSZ2j1UrpZuMqmo8wllImra/rl9y+YX2jXMU5uhCdS11yHvPblK5um+tX/Dw+W7ReD9JiesaU5si216grpMywXk1jbN3xeGV115U4zk5yVuVqMZcrOcrnVNnm47Aqda76H0fn+ao0PVbn1TTmrOOcjY7dWYyzyvixFKtv4lzhxm4Sr6Hj1x/fx68LYvjrnTXN79M746/rJn6uidbrr9smf683eW9t3Hqdi1cYMzV3Dakx/P1tm89Pq8k0xlknR+zzYVftRbodc5P8da2VY3HhGZOdJxdKl58NAAAAAAAAAADAKjMUkeVZF9GDOfz3DgAAAAAAAAAAYEr4/gNYRIm/pQ8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANDc3Dzw6NJLLxWlVOOf888/f9ZDmDpjVelPH6xVuZ82mtbbJHdqnzq1dDEH85BjGsJxWKPEmn7GpZQVpezU+omIKG1F6YK+2o5+UttXbJs7WvJXEa0mf6J9tftJbNcHY0Y/TfrEfjqgzLC3n87GWednGrkqcwyb/1RQ1rT+SdZmHA3HN/V6ZvnTVo85Wx874/VVtuvufDahxjndai02u1Z0fB0Irz3h9SxRn/czKWL3LkoZUap6H9a550i9T4rlbnOfhUlVnxGqtvf5mafJ55emn1/Dz6ZdfU4dr2nan7cBAMDi4vsPAAAAAAAAAACwiPgOBAAAAAAAAADQ1Nw88AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACyupVkXgP4Yq0q3a2Vb57CRHKpBbF9val1h7pScqX3CuSuryceMxaraXnfcKbFjMatqybU1rq2uV1edHLOUOq6idkqZ4raRMddtLyIiurhPdH2fKh6Np8q2+43avad0pHFsfQozdDEGwXqTFtuUzGnduspida3BnCk/Vx2x4ZwXaTsnKePsM0dXc1YyV8p2f9zY0jfmmI6PiU6kHFeheRxHgj72fe25qPP+6foc5997deJOvF9938h6469DYznM6Prr3yY2DDEMlnvYTf4ewEaWo/3G7kv8vVq4LVwvvo/J36NN3hcbt35lLmMxYzEWTRfji8WIra/6DDctqWNvMkdNx9jH8TYv840Zsja7LiwUu4BjAgAAAAAAAAAAiYYisjzrInqwOv9tCAAAAAAAAAAA6ALffwCLaG4fePSXf/mXcsoppyS337x5c4/VAAAAAAAAAAAAtMf3HwAAAAAAAAAAYBHxHQgAAAAAAAAAINXcPvDo5S9/uRx55JGzLgMAAAAAAAAAAKAzfP8BAAAAAAAAAAAWEd+BAAAAAAAAAABSze0DjxaFFSVWVO1+SmwP1eQZG69Lq3b5bRBb1YgX1pVay3jO1Hy+T1V7X1NZLVWxqraX5Uitc62pMx9Kme5j6kjbyPpY+2icsXriudLGlWPce0XX75rrp0vObWXbSmM3LUpEzNDFGATrgzmqkyPs21ab8YW6rq1IRb3Kz3kHbLjfvC7HGRtPFzmq9m0XcxWbowLK9n98WNXh8Tyuw+OqD9OY2wldzknd4z2hfZfnguw4D2P691gn71d3jTK2eHke+eu9mXzf+XuF7H7R3zO4ubLW9cnWB/fpbr011dfuMFdVjOj6sfut2OeGcP28K/ts1ZWqOananlJj6rw32T9N56iLY2Ea+yemyd8HLGINAAAAAAAAAAAAAAAAAAAAAAAAAAB4Pf2mOgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwIqlWReAYlZU7T5KbGf5jS3Or1WzHLYgnkqMFdaSUkOYryqXb59aU5tYXebqMmbTGH2MpwtKp9UTq7uov1KmXoya7UVERBf36YUZPfPOyiinGrTch7rkvKV1/jVak8m3C5aVWZkfG4tlhq7PoDxHHVV1p2qSuw+p4+mj3khu5fdbB2yX+z4UPe46nKuqY7sPsTkroeycHM+rSZ/7MMvRcr/U6F/7fdvnOTC7BgXXj5Q+xua6StjV3xOY+vfntfn7ADN6T/p7hsJ7aVeXNaq0rb8fsVYH68vbF/WZJ+H4ERf7bNdE0bHYde66ObrKW1eTz+yYQ9aKLOI9jZ2vz+cAAAAAAAAAAGCalt3PolnEMQEAAAAAAAAAgDR8/wEsovn9DVYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALAwlmZdALpjRZVuV2Jb5zC2OIdW9WPbIJZKjOFrqJPT56rKUdVufPxNxpyiLEfqOMJYXdZqrHYxTWcxUyndfhyxGKlzWlpHLHbN9XXrmRp/aIaPytMF5wWl49vK1tcpx4yOQasjz+4zw0juQf1kZvrHe2Ox+Rg3jfFE90uHuSM5VGzf12Bjx0kX9VftoynM0WTO9nOGDvX5Hm0RO/m9lZqjy/dTGCs79t36ktL9JcvO0ane3yNYE1wv9ViRRpe2jcdw9xip7ctyuPuVyft6d422PF93GsL5n1WMcbHPjX3U0CRXlarP1gAAAAAAAAAAAAAAAAAAAAAAAAAAoLm5/Q3UP/7jP5a3vOUtcvjhh8uGDRvkJS95iRx55JHypje9Sf7jf/yP8oUvfGHWJQIAAAAAAAAAANTC9x8AAAAAAAAAAGAR8R0IAAAAAAAAACDV0qwLiLnyyitzyy+++KLs3r1bHnroIflf/+t/ye/8zu/IySefLB/84AflLW95Sy81PP744/LEE0/U6rNr167csrVKrFUT7ZSyrWprwspkHSIiStrXYoIx6gbjC+epao6a5PQ5qmKntPP5Y3mrYqTW0ofxuZ5F/j70NQ6lTO2cvo/SNlhvC9eLLskRtp2mityqzSPzjIuti89Lk+3dHOnJpMoUz58taDuKNUzLOWt60KxfZD46FZvbPuooy9U2R0ls1fI4sWX7bxpz03WuFCn1LJJpzu0Ua2h07FfVUafOvs7RuYtWRQ7XVNnRtcqaxGvVONPt+8HfQ+Tul/39g8uV3TO41dm9ZrbeuPX52gpjZ9vq9fG5YnNWmivSN1ZDndjzJFZf+NkmpU+Vsph146a2rcrZNH/THJU1RD4jz0Kfx+68vy9mytiVzyaLZBHHBAALZlG+/wAAAAAAAMA8GorI8qyL6MEq+TdfALDGzfo7EL7/AAAAAAAAWFR8/wEsorl94FGK22+/XbZt2yb/4T/8B/nABz4gSnX7S3x/+Id/KJdddlmnMQEAAAAAAAAAAMrw/QcAAAAAAAAAAFhEfX4HwvcfAAAAAAAAALB6zN0Djw4//HD5yZ/8SfnhH/5hOfbYY+XAAw8UrbV85zvfkTvuuEP++3//73LjjTdm7a218ju/8ztijJEPfvCDM6wcAAAAAAAAAACgGN9/AAAAAAAAAACARcR3IAAAAAAAAACAuubmgUc//MM/LDfeeKP8xE/8RPQp/aeddpr863/9r+X222+XX/iFX5CdO3dm2373d39XTjnlFHn7298+rZJbsbb5/0SglO2wEhEr8VqUNMtlgvHpBjX7OUod73jOqnypsVPa+byxnHXHkRI7FrNNrhhrXEzd7XE3LUqZyPri8ZSNM7rNra+bS/Rk+zptU6zsv0bdXV9XU1UM3eJ/WDFufFoXL4ftcnmLC1NFbWuykdhTYYazy+3pQfH6DuZ2MldkrrvINYPYqsP9Z6e5H7wmx36f9SyKGc5RJ8dk3fpj7Wd9rPjrlbFp6x1/PfTX1mkIc47fJ2SfJ/w9gsm/b33b8HOHv1/JrnFh7LEw4VibzsEs5m4taPOZsusc4ee/LmK2yRHNXfLZtyvT2C8AAGD1WWvffwAAAAAAAAAAgLWB70AAAAAAAAAAAE3NzQOPfvInfzK57cknnyxf+tKX5NRTT5V77703W3/xxRfL2972NhkMIg8GqOlf/st/Ke94xztq9dm1a5ecddZZneQHAAAAAAAAAACrG99/AAAAAAAAAACARTRv34Hw/QcAAAAAAAAArB5z88Cjug488ED5y7/8Szn55JPFWisiIvfcc4/8wz/8g7zlLW/pJMchhxwihxxySCexumRt8f9+EFLKts8lxbmU1IttCmrWifWF400Zl89XlcPHroqZ0i41Z9Ma2mha2yJRunjssXlXysRj+T6xmG59GDurQedjF9VQVW9se5TRK38eDOv1DWlVvlxahxu71mnbTbAfivqFbWJiOUuo1NirlK2aE9PyWCmiI19IdzHXVcfVaovtqA73g43Nf2jBj/3Vost9X6nNPk/tG2tXMM6J828fx6S/fpnINdVf3vxbtKKE1Hv0rvh7giyvu79QrmBrgntMNw6/PuPvKcL1uVzG5So+X03UUkObvilx+4idYhY5U6XUllp/0ee8tjGbxJ7IFfkc24V53rdowNj4dWA1W8QxAcAas5a//wAAAAAAAEBbQxFZnnURPZjiv6EAAPSm7+9A+P4DAAAAAABgUfH9B7CI6j/9YY6ceOKJsm3btty6G264YUbVAAAAAAAAAAAAtMf3HwAAAAAAAAAAYBHxHQgAAAAAAAAAQERkadYFtHXmmWfKjTfemC3fddddM6xmvlirCtcrZdvHlnxsJfVjmqA+nViXH1fKOHyOqth1YjbVJkfqOLowjblQOi12rIay2lJjp/YbXx+vx6TF0CboN9l+IkbQpwvZPo68b1fq7TCpcePQQVDjc6nE9gnzEfap03eRxOZhjOpxTmx0P3TwtFE9iMTuYDx9HT9l+6Or/ZC0z6f3tFcb20+r1DTnrpFpnOPq5qhq38WczvDc7q+Xtk0J/jpvBpH1OpdrnDX5e7bs3t/1Ve5C7tuFdfu5szZ/7sjdn+h8rskYks8d8PdIYY4UE+PyJbn14WcJrJjG3KTmiB0bXcQuzCfdjL1J3QAAAH3g+w8AAAAAAAAAALCI+A4EAAAAAAAAANDl4yxm4sgjj8wtP/HEE7MpBAAAAAAAAAAAoCN8/wEAAAAAAAAAABYR34EAAAAAAAAAAJZmXUBbGzduzC0///zzM6qkmLVKjFXJ7bWyPVYzYkvqUQ3zW8nHVFI/jp+n1Dnw40ipOTV2VczxuYu1qcoVy1FnPG36rBVKmcj68rmK9cvRVTFG21WkXdX2UY6EOooY9xy7wbBZ/yLpp7BJxo1RB0GMG5/W+Xaeb29K5kFHntlX1meRxMbv9TkPVblFRLXMb8tymIbHtx5Ut2lad5/7I2G+W+dom7uAarqfFtE8nZf6qCU1ZsIxMXHuqFruQnbN8dcsnyvW3rUbBve/Y/cY4RW+7P5bZOyewATt/f2AmXwv+j7W5O8Hw1zR2Fndow3ZedeUfVZwbW3xuSGbA7fZRmLl5qrGZ6XCWME8lLYN5ki78Rg3ntQ5XWRdjbVJnDqfm0UmP4O2MYt9XHe8baylY7g2ayc/iywCu4BjAoA1at6//wAAAAAAAMA8WnY/i2YRxwQAaxffgQAAAAAAAKAevv8AFlHz32KfE08++WRu+WUve9mMKgEAAAAAAAAAAOgG338AAAAAAAAAAIBFxHcgAAAAAAAAAIBV/8CjW2+9Nbf8ile8YkaVAAAAAAAAAAAAdIPvPwAAAAAAAAAAwCLiOxAAAAAAAAAAwNKsC2jjhRdekGuuuSa37vTTT59NMR0xViW31cp2nt9G8quauazk4yhJ7x/OQdU4fc0pNfrYXcSsapOaqw915qRO23nTpmbfV+niGH59WQ6lTFpbbcpzuu0p9a4a1o1JBc/VM2691vllL1sfm8ux80PYd7XSDZ89OI3xx2rrMnckh2qRw0brHjaOKXpQvr3NnFQdA1OY76nkRrpZznsXuRPfa6Xv83Bb5fIUrpP++m3S79eb8tf97N685F5Bhc+wdU2t72NG27P7Dx/KdbPheMbuU1RV21i9sVpd7Ficsjb+vsva1fHM3tQ5maYuaqnzebWvmOFnzSb63C99zBEAAFibFvH7DwAAAAAAAAAAAL4DAQAAAAAAAACISPjbsavLhz70IXnkkUey5cFgID/1Uz81w4oAAAAAAAAAAADa4fsPAAAAAAAAAACwiPgOBAAAAAAAAAAgIrI06wJERD7xiU/Itm3b5NBDD03u87GPfUwuu+yy3Lrzzz9fjjjiiK7Lm1vGqtLtWtnOctkgl6oZ28pkrUrSYvhxVo1nvMaq+lJjohupx4tSpni9br6fUvvGcue4WL5tLHa2Xrt2bvwT7fVkzqq5ajMXrWPGHpFnCvprVdzGrzfB2LUuX1+WK1VY07wIxzxN4fyGuqitjxwVMVWLum0sthk2C6gH1W3aznPVHHeZK0WdelaDWb5Hq0y7tobvg1rvybBt3TH2MCf+OmlN5Drir+cm/n731/e6V7Gy+4Ls3jfLP3rvZdd1t9oG2ydyZO2Nixt/D4dt/fs9Nje1Yvs5qviMU6WrOGimzrxXfZ6diF3wmTK5bw/HQ936MZ+sGf0smkUcEwCsZnz/AQAAAAAAgOkaisjyrIvoQcN/uwMA6A3fgQAAAAAAAGB6+P4DWERz8Rvpf/qnfypHHXWUnHfeefI//sf/kGeffTba9vbbb5ef+ZmfkV/5lV8Ra1d+Afjwww+XD3zgA9MoFwAAAAAAAAAAoBLffwAAAAAAAAAAgEXEdyAAAAAAAAAAgDaWZl2A9/zzz8uf//mfy5//+Z+L1lqOPvpoOfLII2W//faTwWAg3/nOd+Sf/umf5Nvf/vZE3wMPPFBuuOEGefnLXz6DyueXsapwvVa2cH0dNoitGsS0MoqhJK2vH09K/b6+qrqqYqbESc2V2m98bsNtdeZgrVG6fE78XFbtJx+nzv7MYvsatCle7wXbi/JXcjGS1yfF7PC4Mvm5WFkfPGdPu+PdBO20Ll5fRlc8w8+ssfeNLr4G5NSZ38IcCc9NbJqjLHYfMR1VM7atPO5aPF1VD9Latd2PImn7MlUX9SyaeZiTNsdihbrvm6T5iLUJ18fO7dOY8+w6XnC+9dc/M6i1XgXPo7Umfi6fuIfMYo9iZPcUJr89y+HXT9zXr8ydP8epSNvKmiraj/eJtY1t9/fDsc88dXJ0oU4986hqbrocV91Ytug9VtWno3pX6/4EAADzh+8/AAAAAAAAAADAIuI7EAAAAAAAAABAU3PzwKNxxhj5xje+Id/4xjcq2775zW+W7du3yw/8wA9MoTIAAAAAAAAAAIBm+P4DAAAAAAAAAAAsIr4DAQAAAAAAAADUMRcPPHrPe94jhx9+uNx8883y0EMPVbbftGmTbNu2Tf7Vv/pX8uY3v3kKFTZnrYi1qrKdUnYK1YyYknp0wzr8GJuMw4rrK2l9ff0ptbapq6s4depda+rOScr8K2XSYmmb1l6v5PRtV/ra3HIlbZr1K4ixUlP9WI3fDz61DtcXxNMqv80vWxdE6eK+Wb+E/aiDQlL6LIJw3DFF+6Vxzsh1o82cV42jSewuYybOs2pQp03eh8PasTN6UK/9LN8/qfNRx2o/H7TZ9w01OZYrpcYsaxfbFp7jqtrZ+Tomsuu2Kyv7vOCv80bn26XwsYIYWU53/ffblb+gB9d3axI+u2T1u87ufVzVN7vvGnvfx/r4HOF2H8NandS+uA7XNrtPH8U0NvHcH/Svo03fedbHePznxGnWUfZZuW99HhOWj6JxxnZ73zwvFnFMALCKLfL3HwAAAAAAAJhHQxFZnnURPZj+v6UAAJTjOxAAAAAAAABMD99/AItoLh54dPbZZ8vZZ58tIiLf/e535e6775b/+3//r3z729+W5557Towxsv/++8sBBxwgxx57rLzuda+TwaDmL/IDAAAAAAAAAABMEd9/AAAAAAAAAACARcR3IAAAAAAAAACANubigUfj9t9/f3n9618/6zIAAAAAAAAAAAA6w/cfAAAAAAAAAABgEfEdCAAAAAAAAACgrrl74NFaZa1KbquU7a0OE9Sha+YKx1GnViujvkrS+vhaU2r0dcXqqROraY4+zTJ3qIsaqmKk5FC6Xh2+fRa7Zv9RH5OLkdVQsb6JXvd11djNaHv2ntX59flYKr/NL9tg7D5IUYyieCIipvn8zSWtq9uIdDvu5JxN3g8V17XUcaTW2HXMuvNco05VM7atMweeGdbvM05P8X9RWq3v5bZz3IG6x1JrbfPV6R9rG56PwnZ+uey85beFryH/1kvd1ePXz6G/RvprZn59yF/Xs3vpBvcIyhfsulp//+HWW1N+Xl65Bxpb6ebTn4eUj5342SWMWVXDeJ/Y54pwvb9/Dz/HjN8r1fmsJTK230z+HrtuHOSF+yjG32Om6mK/pNbWBscPAAAAAAAAAAAAAAAAAAAAAAAAAAAjDX6DHgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAoJ6lWReA+qxVheuVsp3nMi6Xbhjb11qnNiuuj6T1aVvjWtf2uKnTX+l6uarap8Tz9TUdp1JmIl8W0+fXptX6XD7Xxpri93msvY8ZVbS9qk8ia3wtYytN4nxrlQ8SUsFz+VLjTptO21+lTDf7I6MTnmnYNGdS7JrHQDROjRpT6lpNMR3VYD/ZmjkmmGG7/ijVZJ+2No2cdXOktI+dS8K+sVix60sL4bV6ZXmsjbvGh9Vn6/29gHv+bBar4n5l/N4nvFcIY4qptz713qOwHj//OsgR+dxSHKO4T917o9Jcfv6DzygpdXaVM8Z/ljE91oJJfcx3n8cTemAlO/8slDn92AQAAAAAAAAAAKZh6H4WzSKOCQAAAAAAAAAApOH7D2ARtfxteAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgGpLsy4A3bFWFa5XyraObYLYumbM8dpS67Ey6qOkff1hHbEa/Dhj42syjmnospYwltL5Za1MZ7lWcraPmRrDj8e3X1l249Q15lKbXN9srhquz9fp/mAqnksX9A3HEe6/RnyIiTJ9rtH7wo5tX6k/3ya6HPLbbQfHm5rCs/1i4+iDLj7XTzAN5k4nzlWd2FUx68xd1dhT60odZ0rMOrFSY7aJ7agm+19EbIuci67pnHZimrn7yFUnZtU5IYw1sRzp38V5Oru2xbaPbTCD3DrlOvsq/G2LDbaXxgxM3CvEYjZcP8rvcxjXVufrj3z+iNdsc/FGMYrH7u9p4p9xTGF/n8Oayfv2qnr9fa5xMcNYYZzx+2YbfI4IPz9N1p9WU922a0mb+ajaP9OqAwAAAAAAAAAAAAAAAAAAAAAAAACAtYjfagcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAL3jgUcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKB3S7MuAP2zVuWWlbKtYxoXUzeI5etJrcOKay/l7c3YOJvUhW51cZw1iaW07SS/UiYXbzzm+LrivkE7bZLWFxoMRWTyfRzmylTU1inj/zCZM3vf+sfqmcS6tKrXfrxPjDXl22dN1Xz2YJ258armKIudOFe6Rs1VMWvFqhh7l+NMrauPOasbu02OgKqbswHbQZ3eNOrt1DTr7TNX29h1zmOxXOH6MGbZud+3bXI+TeXvR1wZPpO/B8iWfZnuet+I7+vOgcqM3mM+tPLPuPW1+PuPqvXjQdwq68sMxpd18ftF52N7sfsZkbH7I1Pctmq7v/83JTmyWK5t+LmkrL62+sjRZcxpzEEVf//Yp5TjI9Us5wodMjJxrloIizgmAAAAAAAAAACQaCgiy7MuogctvlcHAAAAAAAAAACrHN9/AIuou984BwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAiFiadQGLzlglxqpsWSs7w2pGrKtHdVCLH1uTcXVZB2avzn6MHS9VMZocK76P0u5VmWDZxazaPrZOtCmMna2vaKcGw1zOXL3BOmvc+0TqjT1WQ2HeinnNavD9jI8jxcsiImbU1oqScUrnt6/UpIrXh9uLxPpMU1l9VaypblNE1XhmYeocpY7DJNSsE+uripUaR6R6nHX2U5d1pcRrErNJjq5ytaDq1DmPpln/NHL1kaPNebmqnnB7mCs8p/rtJTVlXUxwnYuVovP3DjJMOLfofDDlL5ouhr/WhsL7giLZddqlsC5XeJnOcgbt/HqbMA5/n2Rtu3NIblymPKa/h7E2MkeRmsZzZHNUEcvTLqZxMVWwn4rihOv8/a6pyDWZu1m/lBip4593TepvM59d5O9a3fF0OX4AAAAAAAAAAAAAAAAAAAAAAAAAANqa3W+7AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACANWNp1gWsNcaq0u1a2SlVImJdLaqDnH5cfdZvxdUr7XN0OfZ5NI1x1cmhdHFbpUyt9uH2WP+++bFndWqTWx8uZ/UOhm57cf9cDE8H5wyT+Jw63WJujM/p6wu2+xJ9Ch0s59b5GKOY1rVRYUxTNe4ejukwRxtd1pdal62xjycmPKJqHHXmzFTUp1NrKomTGiOLFRlfk2Ohq/HVidkmdtNcfdcxLV2MtwvTqKPPHF2d6+rUGGsb1hKeE8tq7fqakl27x84lOn+RDG9ZbHCdjp2mk+63/DnM3yMs61yO8DKt/Bq3Iqve5RqvLWy7Esy4tsH4gu3ZeSIcf8lnouz+yBS3jW3394XaFTH+ucv3sSaI5cccfDaw2Wcb42J1d76rylmn7zT5z3lVn2cX1bTnfK3O80xYkQ7+imH+LOKYAAAAAAAAAABAoqHkvr9eGMNZFwAAAAAAAAAAAGaG7z+ARbSKflMdAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACsVkuzLgB5xsafLKeV7SWnHcupesqBbjQ9BsJ+KftZ6XwbrUxSri6PoTqxfL2+T9M6JuKMz4MO5sAth20nlgfu6Yp+eWmYr7EkhzLBc+ki47LBuWNi/AU5wn1cxRpV3M+XrEvWGT9W5WLlQ6jw8XumRm264RM56+RoqkltqXXViR1OeGhiB0Sk1JZal4nUpGs8i7GLGCLl42p8fJXMed366sTuKkdXdSyyaYy/zxx9nAO7qLcqRod1V50aq/jroa9I2ZXa/DUzu65n18PRH5RUjCO8H/C5zOQ5KbtXCK/9rm1Wn1/29wGuKGvzyzKWeqLtsJunYOfuU7K5Ma4eHW8r8XuecP34/W/4GSvsE4sxUXewH8Zri8XydZR9zivr16SvF8tdNt7UuVgrpjEPVfsXAAAAAAAAAAAAAAAAAAAAAAAAAIBFM4XfhgcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGsdDzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC9W5p1AUhnrBIREa1sbzmsy6Ea5mhSY9uc6EbT+a/TL3ZcVMXw25WuX6Pvo5QJll2sYHuTOrJlHeQYDPM5loZuvSlsP1oIxujbuveJZ41bNqPn1ilpsP9S952fmvAReb67CtqNtw37miCnHnW2FdOvyh7PF8bsklbVbco0qS01Z1XsOrW32gGBWF3J4yqpRSfWURajThyRbuc5i9lhfU1z9JFztas7Z/Oaq+tzYh+11olZNZ7w/BW27+Makb1dXGx/TR67rkZPm3pYvD68JkfuedRgbMHndW39vcHKWck19nPg7kNkebTe+vsQ1y9bHrvg22H+HLdy/+RrMK6vux+ZuO77ex63wt/W2Pi5M7snc32ye56wXTDusL+1kzsh7OPHE9YTrtcupnExi3KHfSZjWBejPHeR1L51YnYtJXc4jokY7j1lZfr1z0JsHjBd1qroeWY1m8V5AAAAAAAAAAAAzIvlWRfQk0UdFwAAAAAAAAAAqLao3xMs6riANPx2OwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA6N3SrAtAfcaq3LJWtvMc1uVQPcSeBj9HfcxN3+rM+Sz2T2pOpePtlDK1+xT1T22fFNPF8uPLYuuCWt26bC4iy+JjLg1HrwOTW5ZBwThiYzL5970s69L2NmhfOlc159HHrjX/fhrDx+yZIIYOxpnlTE/lqS4e6RfWV1dkPK1ypsZMqT01VmwH1JnkWD115shE6tA1d3YsTqNYHYxrImbFAV+3xi5yNtFHnV4f9bbVZ01tz0WFMXuot4uYqWNtcmFoyr2d/XUveo0duy5mbfxr5Jo5cS3NYhW3y+X2U+DXuXsDH3HiLOTvHfx9yPIgn9PFs2P3PsrF9uuULyy4rtthmKxc4f2kjxXe80T62OAzkd8erhdZ+UzgPyNE92UQI1zW7h7U2O7Ob1U56/SNCccf9i+KEYsdizVNdeZoHs1y7gAAAAAAAAAAAAAAAAAAAAAAAAAAmAc9/iY6AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAyNKsCwD6oJUt3a4qtnfdr+9YsZhV81AYQ4cxTKPcbTSJ5ftkr9ovm2DZxQ62T9BmopYwxsTy0nD0OjCly5It+9ex8SqfP6jHuDa+qd9PRuWaWbesVH59oWBfh/s+me82ntJP68Q4Iuuz7ZEadMJ4wrLSDt0kqunjAWPjKZI6xqqYdeaq7XyXTXLqpHWxz03sfdxgx3UVK2XfNziuR7EbHNxN5qKtJnXOo2mMo865ojJWj/V2FbvJeKtO6mHMquUU/m0TpPbXS5td+CZjZ+9ubYPXfIyJDjp4nYg71m/Zpy8eW9HledRP52sSd38ig9HL+Hiz+w0X090fiRnk63L3U9ada1R4vffHjttuTfz8l81NRZ+VezrJbV+5d1sZiLWjRv7e2NjiWNaqwuXKWgvyx2LFapiIPXYP2rRvVf19mGVuoBEjE+f5hbCIYwIAAAAAAAAAAImG1U1WpUUdFwAAAAAAAAAAqLao3xMs6riANDP47XcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALDWLM26ALRnrBIREa3sjCtZ21Rk/rvYL13t26IaY3U3iRUTq78qht+udP/HdixXblmb/LpgOYyRLS+5pysOXPv17nXg4oydidWSyhem3bIZtbVG8n1NMDdD1345iDMrrt6Jx+uZYLnq8XvhOOvQ7efChvVWUE0eJ1g1xtRxlMVpG6POXMYmLXVyOhlHpAbdYAfFYrWK2dE+T8pV8yDOaljlz8ZsOu5Ocvdw7epjPL3E7GDsqSfePubZH/ZhCf4t6VKu3A+41WbsPasj9zAD9+qu9xOnRP++D1+98fEujwq0yy6XL9jn3JMrd6V8V6fKba3q9AABAABJREFUnv48Ksq6OVdjF+VwnR36uv19iK/b1+dq8ueOYb5+pXyNYwMP5tna4nNfmDNs57fn9kOQ11pd2sffu03EDtZrF8+MxauKNRnDuhjluVOk9g1zpsSIrS+LVaWqr3JHrZXuroNt6gUAAAAAAAAAAAAAAAAAAAAAAAAAAN1a5b/FDgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVgMeeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHq3NOsCAIwoZXtp25XUnErH2yllavdJ6d+EjxXLXTTecF22rF1dOlheGo5eBy7XknsduHbr1Wh5/diz5wYDF0MV1zV0MU1Q37LLucfXMHq1xsUpmzrfJrYfung0ns8fixWrr5PcHbxfIvsjxtY4VFXqGKvGkVJj2xhl/VPnKDY5yRNRUkdqDSZhB+maB19VzLrxROofuzWP07QaujvvrnpdnEuisXuY515idjwHdU6WoQ5q8aedrAz/Hgpjh2/f2GlsUFCTzr9m1/x17mPY0iBor/O1hJaHK38ejP6s9LKIiFhXmBJXR3AMWDOKrdz9iV0e+A2j9e5+YHwU/l7Hhsv+XscE9QdW7pV8qvTzVHaP5sZhbX5HhPdl1qro9nCbdtuMW6/C+6Yghu8fLuuxe1Nju3mWcJijOG++/tR6vVj/LsVyr7YcWAOMWvk8tEgWcUwAAAAAAAAAACDRUPLf/C4K/g0LAAAAAAAAAABrF99/AIuom9/KBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKLE06wIAtKdU908k1Gr6TwRMHYfSdqJ9rO9EW12RQ5vy5bF1YexseWDyr0vudZ3LvV6Nlte7Z86tGzsVr183el1y67RrY0z+1fPLL+wZxZS9IiJil10ty37cK12sUcGAgjmpeBSeqprDMr781MftdXEYdvFoP1NzzDqc4zhbMUaVPFclNabWE4uR0r9qjqpiVE2ESPVktK0hF6uiHl3zwKqK1yZ2lqODa0GdOVpNupib2jl7uI72ErOHuUl5P9eVWmeX4wnfilXXsIL1asm9p/x7y1/zN6wXERG7YcNoeWldeS1u36vlvSvr9uzJxc7uAVyhKryI2qGL5Wtyc2Xy90h2bP8pNyi/LlseupxZDNfBz4Gr1/rz2UT7sdqye52gXFt8PvIxwvuZlXs9Kdw+auPqsqNG2vUxtjiWryFcLhPWVxUrVkOK1L616o+0ja3vs37l7lGtzO7aVGfuAAAAAAAAAAAAAAAAAAAAAAAAAABAmi4eAwEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFBqadYFoDvGKhER0crOuJLVaxpzp2rkaFpP2C/MWVSD0s1yxcZTVnvVHNSZo4m+bhxKmUb9Ysv52G6bdjn8+sGweP2Sa+/OuGq9e9bcPutHrxv2yXLY9e7P60fbrB6U1/3C86PXLIBxtY5erbhaxqYjqz97UZKi8hgZ31wVsmr3dPk4vnqHQrG69Zgax7AunyxbUb9Kqa2qnooaksbTNEZVv3GxyUiahJIamtRjEg8s3eBgTo3dJkc0N/cQUXX3y6xi97kPq05ITXVRc4sY/hSSDS88D/jYsbdawXlDLan8toG7nm9w1/cNG0av+24evW7cHOQc5uMN947+sOfFlZVL60bb5NlcnUqW3XjcgPx9SLbsYtugRn8PYVbGY3V+nZ9lfy/ht4upuF/x7W2Nc77vq/P1W5vfEbHYufsWV6Zv4+8TJ2K5PtaU11mUU7uYxsUMY4V9wmV/72yC7WV96tRXZPx+3TTYNymKxjFL4TwDYlXle35V4hgHAAAAAAAAAGANW5bF/L9Qe/x3GwAAAAAAAAAAYM7x/QewiBbxXQ0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAObM0qwLAIBZUsqmL+vRUxKVtrltfln8+oFrt+Seqqjcy5L7wzp36l2/TkRE7IaNWQr/Z7vPvqPXpXUu9mC07F71nhfydS7vHf3B+HpfHE8tdnlsHP7PvryBW84Pfbb6eCBlm0f81a2nTi5TMfFalW62CbWpqnpa1lAao6pvF7lTJkEkYSIS6vFS6hIRMQ0OZl3zYG2So+sa5k0fczLL3KnHZR2p75sm+qh3Cjn8KWJiaiLv94lTyni7JbdxMHDL/tq/fvTqr/cbN4uIyPClLxMREbN+tF4t7yl81S88O5Z/9+gPZjha9nPiB7Dk7kuG+fuU7NXdr4hxtZpgu4jYoXK5Ruusi63chS7c7q+XVhLPkWPCGFkNNjb/riaT3+7vz4r6xbZpt97Y8lix5bI6+xDWEau/ql+TtnXnMEWbvuM1FdUFAAAAAAAAAAAAAAAAAAAAAAAAAADmzyr/bXYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALAaLM26AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADA2rJ37165+eab5Zvf/KZ861vfks2bN8srXvEKOeGEE+TII4/sNNcDDzwgX/nKV+TRRx+V3bt3y2GHHSZHHHGEnHbaabJu3brO8izimLrGA49QylolIiJK2RlXMltNxq8XdM4W5VhQusNxaBO8utjabfdn2iW3Ymm0wq5fP3rdsDELZTe+REREhvu+1LXZ6F73zaW0u78zCrX3Bbd9HxERUcvLowbLg1x7ZYYrfZdG72sxrk5XdvaaNZTmqvqqFrGbCsfXhK5u0jhXLLZJ3BE6Pqm2oh5VNa6UGmL529af2r8shlc1EeOqJqVOXSLVteVid3Cw6tSDtccaVqtpjL3u8ZOizvGdoo8a5yFXgspzouff1+GriMjAXYfdNV/Wjz4Q+uv1yusGERExG0bXf7PpkNF6PeqnX/zuqKYXvu9yrFzffZnZNT67Bxi9qqXRMWGX/X2Je/X3chP3L67+sWNJmdG62B7y94Xh9uw+y72frDsnKR96fI59G5s28UoVt89qse1vMnz91tSPpV19xtUXxgrrDJf9ZwgzNo6mY6vTryjvvFDij7P5qw0LwKjRz6JZxDEBAAAAAAAAAIBEQ+nmH0rNm/n6twUAAACAiIj/dUH/2xr+Tvyt7vXTrxm9vvpro9eHp1MWAADo0Vb3+pW3raz7jf8+ev0zt+x/28H/Bsje/ssCgDWA7z+m7f7775fbbrtNbr/9drntttvkjjvukGeeeSbbfsQRR8iDDz7YOs8TTzwhl1xyiVx11VXy1FNPFbY57bTT5KKLLpKf/dmfbZXrM5/5jFx++eVyyy23FG4/8MAD5dxzz5Xf/u3flpe97GWN8yzimPrS8rfhAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACr0U033SRnnHGGHHTQQbJlyxZ55zvfKf/5P/9n+cd//Mfcw466cv3118vxxx8vH/3oR6MPBhIR2bFjh5xzzjny7ne/W5599tnaeXbv3i0///M/L+94xzuiDwYSEXnqqafkox/9qBx//PFy44031s4jsphj6tPSrAtA94xVIiKi1eye6DYPNXTJuvGoBRkPEujRUx6VLtjnbpu4beFx4ftkffXo+JGBeybvkntdv4+IiNh99s36Dvd9qYiImM0HjV43jl7tus2jmHt3j173POfWb3CxXnTrX3Q53P8RYFwNSys1quVR/VZcXeHTH/0DLlV+dacPiZyHt1I4vhRNH/6Z8njBqthVMUzCpOriQdvEcamyGqryR3In9+8qRkockY4mZUxqbUVS6p3IN0dPqtWpczRHNados09jUo+7Ovqocxqxm+qjpth70K/3r+PnA3/cu2u+XXIfu9yrHYz+Tx+zfuNoeb27F9gwekLuYP0BbvvTo+3rnpzMb0bXerX3hdHy+j2j5T3u2f97l10trjx3L2CX3Ry5WwV/H1M6c/6+yKjitv7eyAzCLa1l91luHNbUPyf6e7Kwr1LunsiWn6eyOQo+E/jleeE/e5kO6+pyrPM6bwAAAACA+Rb+r4dFn+K7/xsJAABQZhgsm2D9KvvWDwAAAAAAoDX/9yEHu9dPXzd6/Z9njV4fnW45AACgR/e716/895V1v/f10es/Hptvw79nAACsNl/5ylfkc5/73FRy3XTTTXLWWWfJnj17snVKKTnxxBPlVa96lXz3u9+VO++8U558cuV3K//iL/5Cvv/978t1110nOvF3l4fDoZx77rnyt3/7t7n1Bx98sJxwwgmy3377yX333Sd33nmnWDv6/bdvf/vb8va3v13+7u/+Tt7whjes6TH1LfE30AEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAa8E+++wjW7Zs6Szeww8/LD/zMz+TezDQ61//ern77rvl9ttvl6uvvlo+97nPycMPPywf+chHZN26dVm7v/mbv5H3ve99ybkuvvji3IOB1q1bJ7/3e78nDz/8sNx4441y9dVXy5e//GX56le/KqeeemrW7sUXX5SzzjpLvvWtb63ZMU3D0qwLABaVsUpERLSyM64ESXT1/zGoKval0sF25df7HG7F0ujUa9fvIyIiZsOmrIvZd38RERluOnTUZeNhLveojxm+MOqrB+5VuxyjV7s0urippWUX0I3Ljo3P1aFkVK/1hUpQfzglkWarVhfjUNVNRKTef2EZexRhaoyyRxmaxEHr4oHZhBpUtP52uTuLUSdOSqyUSYmJTlagTr2pqsbVJTMH/4drH3NYpc2xEdPHOGYxN7PI2UTsfeLX6+BCvzT27H3/Z/9EXX/ddtdpWVqfX++u86JH29ct7TdaP9gsIiJ73Hqz/MJKGXueH7VZt2FUxtLz+dwDX4M/Fq0r182/tmNrx9aP7x/fZhjMhY9p8v/fgIq1b8Dfd1lbHivLafLtxu/bwhix2P6+3VTkbFNvl5rm7KLWPuYQmAmrRj+LZhHHBAAAgIV1pXt9m/3q6A8/fbyIiPx/7n9C/Ee3/TH3+txYX//PAvzffgwrcs3B31QCADB36vxPef5bAd/Hfdsh+7rXl7tX//8A/ru3uj/87ZdEROSz6hQREXlX3SIBAPVYszj/vmvcIo4JAAAAq5b/+5G9wfIDb3J/OGL0crFb5DsKAAAWh7+u/6uxdTe/ZPR61wWj1wP+bPQa3itwTwAALfD9x9SsW7dOjjvuODn55JPlh37oh+Tkk0+W1772tXLzzTfLj/3Yj3WS45JLLpGnn346Wz7ttNPk7/7u72TDhg25dvvss4/8m3/zb+SVr3ylnH322dn6yy+/XH71V39VjjjiiNI8999/v3zkIx/Jrfv0pz8tb3/72yfavuY1r5G///u/lze/+c1yyy23iIjId77zHbnsssvkj/7oj9bkmKahzr9bAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsiPPOO0++//3vy5133ikf+9jH5Fd+5VfkxBNPlHXr1nWWY+fOnXLFFVdky+vXr5ft27dPPBho3FlnnSXnnXdetvziiy/KZZddVpnrsssuk71792bL559/fuGDgbyNGzfK9u3bZf369dm6P/3TP5X777+/NM8ijmlalmZdAPpjrBIREa3aP9rNuliqg1izzFGlyzlbVPOwn+aF0pE58Ov9I+X0aM5kaXTKtUvuou5e7dLKBcLs89JR7A0Hi4jIYN3+IiIyXN49Wm+W3av7f4q1+z8UtS5/VWPPtwvrNrFx+O3Fm+uwRrUPMiXR/Vqk6dugbDpS5zv2yMI6+ysao2JgOj4Am5hfNc2dUkedGGVxmsRKiemlTlaV6GSWaDqutaCr/VKmj/nva5+uxWMl9v716/2rf+9l68evtZHrcYSyo+u7NaMPmtaOrvNLS6P/E3lo9hcRkeX1m7M+Zv3GUWh/XxHJ5cu0flj+Oufv5fxyduzHa/XXSDtUhctZOxfbll7wKnJU3DsoNarX2v6e4ZtaSxHt6jOuvrqxxj+PmOD+29rVc1+FFYb9BgAAAAAiIrKPe/2O+x5/8wtuhTp+FuUAAIAOfdW9/p17vfR69wd1Sq7d7rF/B3mQ+zd4L/ZZGAAAAAAAQMfCf22c/XriTX8sIiI3ql8VEZGdU6sIAABM29fG/vw/f2D0+uP2ShER+Td/9k4REfmw2z6F31QCAKATBxxwQO85PvWpT8lwOMyWf+ZnfkaOPvroyn6/9Vu/lXuo0NVXXy1/+Id/GH2o0PPPPy+f+cxnJmJUOeaYY+Sss86Sq6++WkRElpeX5VOf+pS8733vi/ZZxDFNS3+/HQoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWNOuvfba3PIFF1yQ1O/YY4+VH/mRH8mWn332Wfnc5z4XbX/jjTfKc889ly2feuqp8upXvzopV1jTNddcU9p+Ecc0LTzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQuccee0z+6Z/+KVteWlqS17/+9cn9Tz/99Nzy9ddfH217ww03lPYt88Y3vlGWlpay5TvvvFO+/e1vF7ZdxDFN01J1E6x2xioREdHKto5lXSyVGKvL3FkN4mqQeMyqvHXH0aU6ubuavzCnX06towkzliOsv2oOZrl/GtGuzli9WgXLo2fN2cG60evS+pVtSxtGTZY2j7bZ5dHr8jOj7csvjF7NMP86kTN4nt14DX7fmFG9yjW1pjhUE9ao6kZzqovala44dusc2rFy6uyv2OMNq2JE+9UYQHj8O6nHmyp7NGNqHZEaasfpO2bdHF6Xb15U62LfTjN2n/UG5vlQLD2XVKl6L4bX3DHWbwtfPXcdV8svjhb37hYRkeX1u13zfUREZKBH9wfL6zav9F3ax8UcuFyjV5Xlang9G7+GuZ2q3LWx7tGUXQ+NcTW62oYq3sZO/5m8Sk0/d3hfXnSfjtnrYn8Y9ulcs2Z1f3aJmedrMgAAAPAh9/rSF2ZaBgAAmKGX7l358+Xu9d/OohAAWFRG6v27otViEccEAACAVc/9S075vT91f7jkV0VE5D1ucW/YAQAALIzx6/xvuNe7f+udIiJy2V+Mlj/6rtHrc1OrCgAWGN9/LISvfvWrueXXve51smnTpuT+p512Wm757rvvTs516qmnJufZtGmTvPa1r5U777wzl+vQQw+tzLMIY5qm6f82KQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABg4X3ta1/LLW/durVW/y1btpTGG/f1r399KrkWcUzTtDTrAjA9xqrcsla2cSzrYqnEGOO5q/LWjd0HX29Yqx0bR1hf27rb9A/7xuovjWFcDO1jaBej/NGA87C/Vg3tjh/tnjWnB/lXEbGDDaNXsywiIsbsHi3v+a6IiAxe/L6IiKjlPe7VPQvYBPspXK5Vp4+R1twfO7Ws1idOJj4msM6c+PdcPFhqoJJtVfMdG1ed/RSNkTgAXTwAW6MG1baGhHpaxZxFjrY1rDbTmLNZ5O4hdp331iIoGm/lOaOP94e/PpvhqAb3KssvjpZf/K6IiAyXRvcDe1w3rUfLSk1+fLPZ/UTFRSq2WbuahunPwvX3fTaynNpfJH9/ndTXXTdj19qsloK4VX0rc5fExkj4mXPeTLO+eZ8LAAAAAJi2E9zrv51lEQAAYC6Mf23zb92rv1e4UwAAAAAAAFaPf+3/8JrRy6/+8uj10VkUAwAAZub/utd/8f+OXj/2P0av/l7h/512QQAAzKldu3blll/5ylfW6n/EEUfklr/zne/I008/LQcccEBu/VNPPSVPPfVUq1xh+507dxa2W8QxTVP6b7UCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJDou9/9bm75kEMOqdV/8+bNsmHDhty6733ve5V59t13X9m0aVOtXGFtRXmKci3CmKZpadYFLDorSqxVjfsrZTusJs8U1KVr5vNjq1Onz1uVqyq2Fbdd4nGqcjWpv65YDXVyhzGmUXcoNac1rp2ebGetdjHMxLY6OeaOG7O0eK9n7PIo5ItPiIiI2rtbREQGz39ntLznORER0XueHy0P945el/e6WornthYTvGa1BYsmYbwdlFOdo6KOgmOxea6G/UoeL5g0j2OK3lujQCmdI+tTx1X2mMS2MUyN/aSLB2Jr7h9VOp6Gx02ktk5ztMnZVw2r2TTmoKccdY/5Wlb7sZHwvvDzV3ouEFmZi1hMf+3Vk4GU22Z9G/eaXb/d9VwvrXcxvj9qpkcf04bG3Rese8kozvCFghzDfB3RcZRvBupo8hm36ediG72BaRe3r5hFn7O7Vre+qjlc06wWMQv4LHC7yq/jAAAAAAAAAACgOSMiw1kX0QO+8wYAAMAc2ce9XnbE6PV/njp6/Yxbz+0rAABri7/2X+Ne3/ZTo9f/Z//R6//33dHr3umVBACLZ418/7Fr167aIQ4++ODaD9mZld27d+eWN27cWDvGxo0b5YUXVn7P8plnnuktz7iiPF3mmqcxTRMPPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAGTjrrLNq97nkkkvk0ksv7byWPoQP7dmwYUPtGBs3bpSnn346GrPLPGUxu841T2OaJh54NOesVZVtlLKd5TNBPp0YO6wzpSafqyqHjx2LacVtl3icqlyxHGX9Yn2axCrqXxSjraq5nFks42LpxOPNtRddsNE9yVANWpfVnK/P2MirK9IM868iop//Ti6UWn5xtP6FZ9zr6MKh9rin9C3vjcQ0wevY3Ab12Kr/PiDYLdn8h5r8NwSxWF3qM0fiMVtrboqO6zHR+XdK30epb9dYii7GkRqjbB5M4kB0+VxVHvsFVMX+Sa4tRUX9veTEpCnMb5NjsdI8HBdtxlX1XqvMXTL+4L0Vzn/2PvcxUt+LufzhdXj0qtx12+pREjUYLfvr+0r/5VG7vc+OXtdtGrUfb+PuEfy1X4X3AH3QfjyRG62q7ZgLKZ8xm7RtKvwMmqpObak5moy3af1d1wEAAAAA8+7OWRcAAADmGvcKAAAAAABgNflx93rPQ6PXX3PLL86iGAAAMDf8vcBF7vXg745e3+CW/2G65QAAMPeUqv87VPPcZ5q5pjmmPrX9VWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACZs3rw5t/z888/XjhH2CWNOM880c01zTNO0NOsCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGAtuu6662Tr1q21+hx88ME9VdO9RXw40CKOaZp44NECsFYVrlfKto5tXGxdM5avKaWG1BxVMa2szIOS4jZNx1PWr85Yi7TpH/YNl1PGOxHDuGXd3T5vOu9ZbFeTaO1ymIR6XFsZlsZUg+bvk5UYiR2Myb2q4V4REdF7Vi4Oyrh63ata3pN/3fPC6HXvi2559CruVS0vj5aXfRzjix2rIxizX/ZNItOb7YeJcRWvHm2L9GkoWkMLdY/1nLr1pOSqPrxdrOLVdeYoOvaqMlNSVI0jUn9y/5RY4bGeFKt8cLZOXRGqauxek/rrqhjv3JvGHFXo4piI6nN8fdbdhbb1lb3PwnkN3gd+n2bv1dh+0P667hqa8aLz69Ty6Jpv/b2MWxb9QhAyf/237h7BLn1/Ir1y27JYwX2Grzs7Rv0wwuuEST0pYZpMwWe82Oe+eVBUr0i7mm3SDUe5WF0TuRrUmRq7y5zogVGdf2aZC4s4JgAAAAAAAAAAkGbofhbNIo4JAAAAq47/F5cnutd/5V4fnUEtAABgfn3bvf5793qae/2HGdQCAAtjjXz/sXXrVjnuuONmU8sU7LfffrnlJ554olb/3bt3Tzy0Z//996/M89xzz8mzzz4rmzZtSs71+OOPV+YpyrUIY5omfrsVAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANC5o48+Orf80EMP1eoftj/wwAPlgAMOmGh30EEHTaz/5je/2SpXWHts/SKMaZqWZl0A+mOtKlyvlK0dywSxdGIMX0NKztQcKTGtuDZS3MbnCnPUqbdKLFYsd0qMOn27Yqx2OU1hTW1UxeoilzUuxqAihnHPfxvUf7xj9l4z+desbOP+YEZzqJb3jvrteVFERPRz318Jpgeu7TD3qoajPrK8nIshLoZy6/12n2sl99j43Z+tqRqYezHBuSTWL2yXYCL2DHRZg9JVx1mNXJWx0kPFHm9YNfboeOq8JWMpUutPeTRjp7Eanm90+r6tfO8lUl08trLpeFeprua+tq7neZrjmNWcxaQe92V1hzHC/ePez/54yd5rsf2o/TW34DyQ3Ru4Nv767WQ9/D3CcN0ot2/n7gusnhy48n1efG60wt0TyLK7hxj6e4nYuXz21+C1LPysFvvsVidGG+FnsVnmtNGbh/Qauh5ParwucgEAAAAAAAAAAAAAAAAA5tMr3OsX3ettsyoEAACsCv/Hva5zrwe71ydmUAsAAPPg2GOPzS3v2rWrVv/7778/t/ya17ymNNeOHTtyucL8dXLF+i7imKapi1+VBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAg5/jjj88t33XXXfLcc88l97/55ptL45Vtu+WWW5LzPPvss3LXXXcl5VrEMU3T0qwLWHRWlBhRjftrsR1WM2JtvB6l0vKZIIau6FeUsyqXzxGL7WOWxbFu7lVkHmM5YrHHx53aJ6XOOu1S+obL4TjH90esT9Pc2XozlkOHc6JdH1MrZhtZzNT3lBnVaMdr0C6G2yZ+m1/OXkfjyubADVMNh6M/7NkzWn7heRdvmFCPC7K8POqzvLdwvSwP868+5/LKXFv/R+Prl/yr9e2C9224u8LtBSZiNNVVnBS6+XHXZLzh+yOTGiul3uK32liM4tV1xhMdR+p0xlJV1V4k9jjHJrGqYmaxOzhf6XrHj20zHlTrYp9W5lilsWep7riK3ruxGL5tsO+z+8nI9hUF1/Pl/KLy1+3sXmH0qvx1fMld3/XAvbp7pqJ0vu+eF93rHpdzOV+nS2mXlcuZf82uN9l9TMm5yPC83HkWfkYr+9xXpU3fqv5hnV3ErBs7dXxNa62To43Uv2uwLf5OYtFZq6ayr6ZtEccEAAAAAAAAAAASGVnMfzOwiGMCAADAqrPBvX7JvXKbCgAAyvh7hf/jXg+aVSEAsAj4/mMhHHbYYfK6170ue/DO8vKyfPGLX5Rt27Yl9b/ppptyy29961ujbc8880z5kz/5k2jfMl/4whdkeXnll0NPOOEEOfTQQwvbLuKYponfWAUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9OLss8/OLf/Zn/1ZUr977rlHbr311mx506ZNpQ8VOuOMM2Tjxo3Z8i233CL33HNPUq7t27fnlsOaQ4s4pmnhgUdzzohK/umCtSr3k1ynVbmfOrlSY7eJY0WJLZmjWI6y2E36pPQvEsas07cpa5RYM55Ti7HVp4wmcxbLOQ1hvUnHk6tzoq1Rox/rfvzTIpet+zGjnz17RfbsFfXC86Of3c+s/Dz3bPGPb7vnRVF7XhTZs8f97M3/LA9HP0P343JaI9mPGOt+JP9jRz/Zfgi3+/H5n3A+Cn4mhDFSf6apaY0N6y2bv9K5bFJvNEbFTwfjqA5Q46dK1XiaPMk1NWarHLb/n0UzD3PV9NhIOT76jO3Vee/N809MnbmKrh8dE+PX0vz1tOAnuw7HfpZFlpez63p2fX/heRF/zX/h+fh9wXPPit79fdG7v5+1ze4F9i6L7F0Wu8eI3WNEhjL6mbguaBGjy+99XJtkdds3UHVdKRtPrK+1WmzL+9zKvjO4z52ooUX9MV1+Lol9ZquqO+VzaOrn3Cafa+vmSDHNvwMAAAAAAAAAAAAAAAAAAKR51P286H4AAABS+HuH77gfAADWsne9610yGAyy5WuuuUZ27txZ2e9DH/pQbvnnfu7nZMOGDdH2++67r5xzzjmlMYrce+//z969x9tR1Qf//86ck4QQEAgECGC5BQREHy71BwTpg4KCrX2MbSyCbYGCqH1UKlqBapsEL8VWKbZaK2iNVXhxewCtL4EWCn2piTyPEMv9ErlYQO5EJeR2zqzfH2dm9p416zoz+5yTnc/79dovmJm1vt/vWrNm9t5sMnlIrrvuunJ7dHRUTjnlFGefYRzTZOGBRwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAgdhvv/3k1FNPLbc3btwop512mqxfv97a5zvf+Y4sX7683J45c6YsWbLEm2vp0qUyY8aMcnv58uXy3e9+19p+/fr1cvrpp8vGjRvLfWeccYbsu+++zjzDOKbJwgOPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGAL9cQTT8hjjz1Wez399NOVdmNjY8Z2jz32mDz//PPOHMuWLZMddtih3F6xYoUcf/zx8sADD1TabdiwQf7hH/5B3vWud1X2f/SjH5U999zTO5Z99tlHzj777Mq+xYsXy5e+9KXKA4BERO6//3457rjjZMWKFeW+HXfcMeghRMM6pskwOtUFoDuZJM7jqajomEpVYyZJWIxM65c6+oXmKGLaYvXHscVQ+RwlkXNRxA4df5sYIeMIzaFvm+aw6di6mJPYWCrL5ybNn9WWZb2D2uPbrOe4iDESV5uISJLlSUbGLXUllRxJsX88Px8bJ+pN0rG8nar2E+mNLbU8j64Yc/lPVRQx8c+x6v5it4z1zUexr+yq3TuK4/r+optlv619I6rDWF2IXedN5iINXP8Wiae/iPjrssXIzLvNMcy7ffUXgsYRejp8KWPGpQt9ZGSbHE1zumTt79mbtS7Px3TIO8WnM/S6biLoXtCvyVwU5QefH6VtaZ8r+6+v4r29+LY1VuzX3sfL9/08lu39v9D/2afIN5YH3zTxz+LzRpFTjU3ELs6XGi8+RxWfW9LqccN7sL7P1CZGf/9BrqOplKnp/2xh/TubS+g5d7Wz5VPeN+y4eKH1xMZqGtuas+G40TGVlPfBoaKm6kMPAAAAAAAAAACYcpmIjHtbbX74+QMAAADTwCtTXQAAANisbZjqAgBgc8bvH5PmjW98ozz++OPedk8++aTsvffexmOnnnqqLF++3Np3jz32kGuvvVZOOOGE8iE9P/rRj+Sggw6Sww8/XPbZZx/55S9/KXfeeac899xzlb5vf/vb5VOf+lTweC688EK599575YYbbhARkU2bNsmHPvQh+dSnPiWHHXaYbLvttvLII4/InXfeKUr1/ozozJkz5brrrpP58+cH5RnGMU0GHngEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABioY489Vq677jo57bTTygcAKaXkJz/5ifzkJz8x9jn55JPl0ksvlZGRkeA8IyMjctVVV8mZZ54pV155Zbn/2WeflRtvvNHYZ+edd5ZvfvObcswxx0SMaDjHNGhD+NfYTy9KJUGvyZBJYnzFaFp3ppLyFZrDF6tNDCWJKMPYQ2sM6WOrQd8fk7NN31B6DpUlorJ2580WQ6lUlLLfhtpcJ0VOWw59v16jqeZezLyeLM1fSf5KKy81VrwSUWOJyCYR2SSi1o9PvNZtErVuk8iG/tfG/LXB/Nq0KX+NVV8bx0U2josaU5WXFK9Mei818SrH2H8sk9549HGX7S0v44lImr2mm8kYh2tuffMs9fPkeg2qhokYga9BjqMM1uAVKnScDeZgoDm39FcTk5G3yVoNXK8x11Tsa5AmpVbf/HrPrRLJ+t53MylfxTEZyyZe4+PV11j+2rip+lq/wfPa2HutW5+/Noqs21h+vpCNSmSjErUpmXjln0dkPH8Vn1OKz1i19/e+NgGfj0QCPnsXn7sc7yNFm/DPnu7PkV2IWeuT+b1ukELHMYjvIaG1dPF9MCZWf7xG34U7+B4ea6r+mwMAAAAAAAAAAAAAAAAAAAAAAAAAhPjt3/5tueeee+T973+/7LDDDtZ2Rx55pFxzzTVy+eWXy5w5c6LzbLPNNnLFFVfI1VdfLUceeaS13dy5c+UDH/iA3HPPPXLiiSdG5xEZzjEN0uhUFwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAmBqPPfbYpObbeeed5Stf+Yp88YtflB/96Efy+OOPy9NPPy1z5syR3XffXQ499FDZe++9O8m1ePFiWbx4sTz66KNy5513ylNPPSVr166VXXfdVfbcc085+uijZebMma3zDOOYBoUHHk0TSiWN+yaJapU7k3ruVMJi6nWH1JJpfVJLnyK2LWZIHF8MlY890cZbxNZj9o9Xj+nr45sbU//Qvjq9n77dP3eppY29zjTvlzn7NZmrsm+Wx0zbrW1jzBHl3p9NjE+lE+NL8m3TPlXUn+ZjLWLl/1Tjed+NeYAsy/+pKjnVWF9No8XYQ8dVxC7ql+o/VdHOcI8p2yb2Nn3H68mb37esuTYTwWuzyRz5rnff3AXU5pt/7/hCzp83hj/ERBz7odh1FHTemt52mizp0DloIvAeMjQGOZc+3b1V1UNP5r1yKuewS5a1HzOX5b1CP7fjRYP8n8WclTn7PsNpx2r3n/wzRfkZYlzcyvd5Q47ic0QeQ43l7+tjefL884gaG8ljFJ9b0sq26ftIuS8zT6xvXtus4dDvR7Ycrv62Y/r3i9j+TeixTDXobXx9BllfbDvXnCrLm2eb+kP7DvJcm75ft9XlOUWVypLN/ruJyTCOCQAAAAAAAAAABMpkeP4fgH7DOCYAAAAAAAAAABCG3z+G3syZM+VNb3rTpOTae++9O3vgkMswjqlrW9ofhwcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFNgdKoLQHtKJc7jSaKiY2ZijpmKO5ZeS0juLO+TWtqGxsz62umxihi2viofb6KNz1WbLaZvPKE1udrq26E5Y9RyZvl22nxcthhKFc9ey6x9TXEk7XtmW5Y/wjDfZT3XRX3FEw9HqvuLJyEmqZbLtC8tYhVJ87kanwha9CwrKXPkSUarNU/EymPEPo6uGI+q1105LiKSH6u3sdxLPPeYWpxByDp8Pl/a/nGXTcfsu34mggfGtl0nMbVZ6gkdn3M8oXX45iTmdHmWSZu16j133d1+xfIWHIenurp1eb5C0g3qPjndznOX4wy5XxpraJ9aeS7C2v0g0/4p0rsf2d7Xy+3AceZxqu/neYTxvN6x/P19LM3350mK99Di/T//nFKsy95noLSyv5+trbddB4pYvuvIlbPtNdhmPHruLucmuAYtZ9aihjZ92+oity9Gk/Nj++4cairWBAAAAAAAAAAAAAAAAAAAAAAAAAAAU6nDJ0gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACY8cAjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwcKNTXQAGT6nEuD9JVHSsTKqxUnHHKHKH5MrytqmnbUhMWyxfX5WPL9HG5aotdIyh7bK+8+WbC1vfop+e01RDkz4T/dK8X+Zs547hPucqy+dCezRbyHoq132W9xmxtZsInsh49UA2sV+lWS+vtq/cLupJi5zFOcyTFrUU7UbHK+2S0V4OVfyrPubUM0dl3VqD/HitXaXOIpj5XmHsa5JtJs/QG2SdqX4CqoLnUuznvBcsPJbYrpnQekLXn6sE23giYtjq6MUKDzURL7xpzFj7ec+jMVmjVIPVbPhV03FcAZqe+yixa7dRjkkYR5emtF7PZzrb55P+6734WFEMQz/HofeGop/ScvcdU2N5IeP555Kxic8fKt8uP9Noc2o7XvnuYHnPrLWdxM8AxWe3QcZqc91nHdY3mWzfGZvKHPGU5U3FVoMrlqtf1zFqMRu+OXY912gpSza/98gQwzgmAAAAAAAAAAAQJhPR/1e4oTAZ/28FAAAAAAAAAACYnvj9AxhKm+efxgQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJuV0akuYNhlKpFMJZ3GTBPVSRxlqCuJjJ3JRIxU3P1icunzZRtvEdNVcxFLjxHSt63Q3K5a9BgxfQdFZXnOVOU1pnmN1UcI9p9zvT49Rq9P8Qy2zNivjTJ2lteZbyb52i1qkmoJkvQ9Fq5Xt2h98v3ZxIGy6kxVjks+3mJmynb9a36kOo/FHCkJu4+UNZU1GPpp11itjy5r8Ww8X+zpLo1cg03mKjU/ftN7XnL6dWQO5onlu9ZCavHU0cl4YteTb27aPPk08FSHjruJoHPflUlM1cYg59tqMp6gOwnjmpK5m2bKayp4Lsyfb0TEe49IlOXzbe19XGvQd7xsO55//hgbmfintl30qX0eLz636Mf73stcx1x6/YrPX9U4/eMs2ui5TN8fTDlq+x39bMdCv6/p/X01xsTSazDFju3TZX1t24nYP882PS8hubuIIdL73ttEm/PgE7p2u/5vEgAAAAAAAAAAAAAAAAAAAAAAAAAAtNHiKRYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABhRqe6AMTLVOI8niaqcWylxU4CY2VS7ZeKv1+Ry5ejGK9tXCFxbDFsfVU+nkQbR//ch8by1e/rH9JW39Zz2to36VNuZ/l2GjYPrmPec5znqj2iLev79zQ/mGXGtvq57MWYiK20/slI3j7rBVJpfqzYNzJeaaOK+rORvAZVDCDfX62hOAuqfw61eVVZw+tZu5bLOTTJPM++c/XtmH4PaiL0vhWk7djTgFp881/Gyoy7nedWo1+vvSCBMVxzG1qHZ046GU+hyfkLOWci1ftPWw0fPxkzV9B0ef6C8g3mXE2bNTBd6mgp+t3D8llo4pilT1r9HKXHqtVU7C/+2Xe/7h3LP4eM5/8cG6lsl8dtOVRibWc75t3fId86t+V09VPKfOPV+7QZT5exuqJ/j4ypyfcdNLZdGyF1++oIHbv+PTdEl+d6MuZzS6dUMi2uz64N45gAAAAAAAAAAECg8fw1bIZxTAAAAAAAAAAAIAy/fwBDqeEfsQcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAg3OtUFoHuZSoz700RFx1JarCQwRibVfqnY+4XmKMZlG0d/nNgYRV+9n8rHkRjqj42lt7fmNIzDN3Zb7pCcTfqYZCrN+2W1cejjKY9l+f5Un4PiWWxZUO4QKstzpnnsLE+eb+rnWK9xok49lt5mJN+fz8F4PndF+9H8MYtj2rPmVC9J0VZpc1LEjJY5nmuXme8VZVmWe8l0N5l1e9emZ46N9HNfxgp8RqFjrajAevRrshcgYjy2uQmdE1sN/eVEzq91XP2anLN+ITlqOdulbGU6PPpyKscfou2aCBC7lqNMQv3TUsj9Sr9PNZwr4/mz3sstOWz783GUOfrfC4rPDEWbsYnPIWo8rW4rQ19TKi1ef5/aMcv+3nbxectzXMLXf9HH1t71/m87Zvve5OtvipepZjdUvQZTbH1fSJ+mQmMFt5PuzktITt859cXQv8eGaDv/oesQAAAAAAAAAAAAAAAAAAAAAAAAAIBhMR3+mDsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABhyPPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM3OhUFzDslBJRKvG2SxI18FoySx1pRG59LKF1Z9Lrl4q7T5HDFrsYh6vupjFs/VRef2KoPTZWbK2uXDF9TTn7+zatV2X5dqryGtO8xszbt9yfx8jyR7D51qTxmirSpXmQbGJHMhI4N3kNytW/KHBkvLKt0qxaQzZSjZ1qNYzm/cfKAdfbqur8JZk2Zj2mNg6XkHvSRKwt9Jl4aX3t2gTPZS7oWg04hyLiWAMR580yVhVYQ2KrQUQkdG5scxI6D/1c9Uj4uEycY+3XIodXaA0xwpf75mGQ82/RZl15TcZ4Iu9jQ2WQYx9vFrtcT/q9vPic0l9z8Tmk7FPtW7bV2nnfu/py1/roOTtU5Cj/6clhG4ern1Lm90i9T+j7e2aI54sV+9mhCT2H/j0wpobQvrbvmiZdxIjt5xtz//fVtrGsOabwfqsG/58bNl8qHc7vPZb7HQAAAAAAAAAA2AIoGb7/H0REPP/bKQAAAAAAAAAAGGb8/gEMJf4EFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGLjRqS4AE5RKGvdNknaPbssMudPAmHrdIbVkMtEn9Txyrohti1nU7aq1aQxbPyW98SZa/aGxYnLa6tBj6O1Cc7rE5lBZvp3Wc1jn03N+lCqeyZZV/mHKYVO7toqnN6Z57CzfkW8mxfGRev/inJf7inryzrWq0mrdkub9xvLgo+PVnNKbx6JtMdZabBX5GMos/vl2ZS2buZj1IiKN5qomNZ+fJvd663015vzY5iB0rLbxRNRgPQ9N3v+6mJN+AWuky+shek0WhuSanA4m/f42qHwtPj82Ssca7I7t/qvNce8zR/5Zo/94/u/WNvl2Np5Wtn36czjz9x1XZS1FrsR9PELRx7b+9By12g15Td9/9D4h2zH0vnoNIbma9OmqPhvrXErzWmqxLDlCavTNSRZYZ5O5DZ3DGG3OMQAAAAAAAAAAAAAAAAAAAAAAAAAA00UHT3UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwG53qAtCeUonzeJKo6JiZFjMNjKHX4sqdiZZDzG2LmLZYRa2uGpvGcPVTef2JVndoLF+7/ra++m05QnLG9vG1L2Sq9zy1NMlqYzP1UdnE8Swt+oWv3TJ2ViTNg2QTO5KRuOtA5fUnMl6rL8mfFafSrJpTo7dLMq3f2Ehea19txbHxYjxa3UXbbCR4LI1l7nvLpEjj7186NYBxJL66sgbPE0zNC8l3jy847w+hc2AbV8x4bOMIrME7tyIigXPSC+o7Xx2skYi1Oog1GSpofifZVM6H02TUFbuWY0JPxbw2ufdNZ5Mwh7V7vDaH5XnM/2l8Tyg+X2hten2rx/Xc+mc6V2y9by9n3Lnvj1fG0uu29VWOuXDsF6l/xwm9TvSYmWG8tjmaSrXxerZdQtsqsbezxdDrbNo+pG8Zw1FnTJxKzA7P+XRYP1sKlU3jzyItKMt3VAAAAAAAAAAAsAUYz1/DZhjHBAAAAAAAAAAAwvD7BzCUhuxPJQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgOlodKoLwOAplRj3J4kKjpFpMdLAvkXukFyZTLRNxdxWH4ceU6/RVKevniJGTD+V152IuR7fXLna6Xn17dB6bf36+8bmss2JyvL9aW9/ptK8b2ass2A7p2n5bLas8o/+R7b51ljtOihj5EGyrBKzOJ/FeEREkhEtRzbRWKVZZVvSap1JHlTlNZbbZW29eUm0+SszFjHHw6+ptmz3jiYa15t1V0Ow1F+ralhX4oqdBT6DMM2Mu2POl/V8hI5rkONoMLfOeRURabOWQ9fuINZqwFqM1XTtTntTMa4O75G10IMcT+g1OkjTfB12+f5npZ2H2jnPt+ufX3r9lN6m+Fxi6au3K/c7ytTrKmL0cmjrScttbReg6OO7HmzHXTlt47JtFzLlOW8BsfTvLCG5Y/uYvheF8uWKzeG6nmwx2uT0Xb/Fd86m/Y0xG873pNxrAAAAAAAAAAAAAAAAAAAAAAAAAACYhqbBn3gGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADDjgceAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAgRud6gKGnRKRrKNYXT+dSqmkti9JVFDfTOubevrF5MpEiy3mdkVMV81FnXp9vr6+fqa+Kq870erVY9ly23Ka+ujboeM05fbV1zhX1jdXadj8l/ObmfuF0GNIml85Wb5Dv5CKXCP6/jxO3/5a7LJttXOSJ1FpprUfyeNM7CjGn/QVVYxYb1PkKOakNjNpV3eaPpnnrhOR03QPmCre+1zWQa2WtasiYlvXv++8lDXYz0/o+bDOVcwctR1HJZZ5TDHz2i/oHtPV2g18f63oYi2iahLvRU3XpVOT6yY49vRab1P6vjGAebauB21/bdx5LUX/ynGtzlobrW9tO6RurZ5eDm2OTPWZ4hnaFf/uq6tsZ8nhyq1/d7H18W2H1Gfb1muIbR9bT0wuWz4T61xK+NyFno/Q2kLmRf+O2SRGaD1d5Ijl+1Te4FPIlkMl0+q7SmeGcUwAAAAAAAAAACBMJiLjU13EAAzgf48DAAAAAAAAAACbCX7/AIbSAP/UNAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwITRqS4A4UIf0NbmKVZKJcb9SaKc/TKtX+ppb8ply5HJRLtUzMeLOK4ai/r0unw12Pq58qq83kTcsWz9++fSVm9iieEbp97P1Tc4lxbHNC6VTezL0jTPUV3NvnPYy1Vkyyr/6C/Et1ZtlMoDZBNBkxFV3d93zHuR5c2SvKFKM+d+NV4/H2Wb4oDWtjbObKSymaTN5iFKNk2fmZe675a2+1wT1vWWNcihnTMVGaN2zmPOj2XOYuaq9VyErNnYNedbC03Okyb4Wutw3TXS8N44EFM9Fx5drAuvQdw/J6HuLu+fjU3Ce0+na8ASqzaX2riKGsp2hnH72tSO+2pw6MXS6rDk0Nv7aqnWFdbHdrzor38/6e9Tz6nHqG5nyjy3U61ep3scUefcE7tsJ+ExrTEs+23tff1Eet8hfXxz4quhScwYPKQdAAAAAAAAAAAAAAAAAAAAAAAAADCMpunTKgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwDAZneoC0L3Mc7zJU66USirbSaLcNWjtU0/7/hy22JloMaXaTq/RFKuoy1aPrQZXP1sfldebiDuWa9y2vHoffds3TlcsX33W8VramdpmamIVpvlqtcUqFnOS+sdhG1d5QaT5ys/yHdqFoJ+ncjxiOOdZfa3111mrP0+m0qxaUzZS6SciZTaltLnR2pY9UvMVr8bNNVZiboaC1kI2gGf52ebZcN9xcc69ZV3Va7HcvwL6W+cvdM4s8yASPhfWOQgdf6Uez3posxYcY+0XMu+xmtzzvCLX6nQ3iHkPNoh7zADHE3ufamQQcxJoIGuhg5jeedfmrBhHrV/eznjcEqNWgyVGzNwppZ1jS729HO410d+vSZ+gGg11+WLp+zMVOMfOGO4+vvZN+oRe9yG5bJSEzqE9ni2Xq4+rn0j9O2NM35gamsS05mrVG62pdErfxwbGc08FAAAAAAAAAABDLJPh/BFqGMcEAAAAAAAAAADC8PsHMJT4E1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGDgRqe6gGGnVCJKJdH9kkQNoJoJrge9hT4BSx+Tr97MMAeppU9o7Ewm2qViz13E0mPo9ei1hPQL7aPyOhMxxyrimNZJEUtva8upb9tymObU19cmKkeWH0vN4yhYz3mRq1ypfau5+NfUHcOmrG1ET9qrTaVpHrt6FSmVJ80m9tfGV8TOiyuPphPt1XgvR1G3rW3tAs5GjDldrC3T6fMYSNv5U1n8/dSbK2TussjnA1rmMub9wLqGQ+fAMK7Q+bPOScw8dDAHIp5ruel6GMQ5t+aKv64Gsc63eF2dT2eO7s9bk8+QXpMxF7lJXcsd5mo87wFzq89JLVceo2hnOx7axtSuXkPAmvDU3cvhyR0wt9ZxBR43fe8o++p1W2OYx+Hqp+/T6/Dl9rUP6WMT0i90PovvNqH9Y9r6xmPtZ6kppG9o7iYxazmiMzQXutYBAAAAAAAAAAAAAAAAAAAAAAAAAJgKk/cnngEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwBaLBx4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAICBG53qAmCmVNK4b5Koxn0zy37fk7H0ekNqyLQ+qaVPEdsWM5P6XKVSbeuNkR/Xa3D1i+2j8joTrTZbHFOs0Jy+fv3nK3S9xOZw9s0mtrM0zftkxval/HCShq9tWwzJcyYybunnPt7fxro/y5PpzbQLLDFcWSqdaKTGq3NWtC1nINWC2S7eCKZ6Bk4fR67NPdDGttaL9dgqtr42swZzmXquA1tu2zUcMy6t/tA5cV6TsXPQ4Vrw3tc6OOf6nNlzTZNnS1rmd1JMlznw6WJdWHR+TxvAnHZxL/SahBydzHXk/IbMnbWuPJceo2yvHTfGscTw5e59bmlef6+u1LLf3c/U15bbPj7L5zLHuPS6QuoM7ad/x/H18bV31edr58vlUqvL8J3LVZspV0zboH6WmkL6huYOjWOMHd2j2/wIo7Jkct4DJ9kwjgkAAAAAAAAAAAQaz1/DZhjHBAAAAAAAAAAAwvD7BzCUNpM/gQ4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADZno1NdALqnVOI8niQqOmambfuelKXXEJIzy/uklrYxMTPJY0m1jS+GrYainylnpsUs+tpyqby2RMJym/KH1hnSL7ZPcHvT2B3zGHK80MvRnyWr/CNJ3TFUlp+ftFqj7biISKJfCLq0GGfeKat2KM990W68Pt5Eu7pUmhnb1tqVNfiKdGjRNVTtvGQDeO6eZQ5898YQ1rWbhcV2rsvQudDGFzMu67UVWL+kluskgnUOmqyFAZzr4PfIBmMP5rl/NTKIa20qDHLeLbq4d1h1eF6aXI9BJmHOBzLHLeY2di6D6tfq0XOUMSztTMetMSzHe+2Kzynu/sa+tT6eerXYrrm19a21K2JpuTNHjl4f95gzFbZuTDXq3wn8ueLnv6tcrpy2tiF9Q2prWlelnzjWUWD9vnE2uS+1/Sg90PcbAAAAAAAAAAAAAAAAAAAAAAAAAACmkSH50+8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGA6G53qAoZdphLJVBLcPk3UAKuZoBz1JIH5M23b9+QsU05bLn2+bHNSxHTVnEmS1xcXw1ZDyDiKvnrdei4l1VhJXmN/bl+M4FyWfqax+fqEtnfVU+7PJvZnaZrHzoz9SvnhJO3+OilrtKwVYz0arXxRqeXKyLSGfc3UeHWuEu3qUmlmbFfW0OQ5dkXMiHtVrHKdZN3nqK2HrOWz/FL9DtcTO0e2NR8Vo+n4DOMIrd96X42p33Kdhs5B0HUee64d57bQ1XUQ+n5qNIDrZEszyPtZqe29xmAQ98hBrKeBzm9H89pmLqPH56jZVkf9M05aae87bmqj51LK9jlEb2cfr73+1NiuVpMldn9/3xz5zqXtu5ZzXNqxzDKekFi+2E2vF1M/faxNc4XELttKWI6Y77xNzplI77tdbD9XziaxevU0MynvUZqY87OlUSqZknMyaMM4JgAAAAAAAAAAEEhJ8x+zprPB/6+1AAAAAAAAAABguuL3D2Aodf8ntQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQ88AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzc6FQXgKpMJa1jpIlq3FdZ8ieemJleQ4Ncthz6nOjjM9Wsx8pEiyHmGL4aTHNr62vrY2uv8hqTvtpCY4S2M+XW+/r6+NrrcUPq6vWZWDlpvqK8664/R7nqsso/aosx35+MmGOqLKkcL7ZFRJJU1fZV9utrWr8wihySB8/7SWZoqA2njKkNSKXVBmrcvZZN9Jid0OtqeW9zjUc/H9GxUy121mA+UvPJjhm3bYy+8dXqL8SMo+H5cq6z0PNiqb/JebXORaHJudVZzrWu7ZrvWsw9oa3pNnajLtaCRdt7klGHMQdyfqbxfDYab+B4Qmqz5s9z6DHK9r7jfceU0uoNnDO9Ntd49Byuulyxa7Ua+tjmrHfcHMNdv/lY1nJcTb676X18OZr2Cennql9JNzlC8llji3t+Q65v3zkKvUeEvfs3ix2ji/9eAAAAAAAAAAAAAAAAAAAAAAAAAADAVBvcn04GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIjU51AehephLn8TRR0TGVFjPxxMhc+VvmKMbnGkcRyxpD8hhSPe6roX9u9fy2nLZ6be2V9HIkeX2hMULb9Y/T1tfWJzZXpa2Ws+yT5X1S97oq684XmK+9O9ZENYmMu48nvdVc1Flrq+0v6tLXk64IrVLDVZHlB/VD2sWVWK4olbquQnfMpirnI2v5PD2tft9chrDdD2zn1RlLX3ux4zWcn9Ax2q6foL62a8ZXv2U9xZwX6/tGg/kXyzianEud977Sdm27xFy3kbq4hqbEIOfbo4v1VNNRzIGczwHMdZdz2HrMEeMLrTuoJi2vHrsWI29ftNOP9/cvPqvo6yp0rqyfa1R9ruxt3bl747DPg61e2xwUbN95XOP35bK107dNuWP7DCJHaC7X98X+7yJtcgTlsvURex9XP18+X99qDfHa3qd8tWNAsmRKP28MzCA+xwAAAAAAAAAAgM3DeP4aNsM4JgAAAAAAAAAAEIbfP4ChxAOPAAAAAAAAAAAAAAAAAAAAAKCFRx99VH7605/KU089JS+//LLMnz9f9txzT1m4cKHMmDFjyup68cUX5Sc/+Yk8+uijsmbNGlFKyXbbbSd77LGHvOENb5Bdd911ymoDAAAAAAAAAADAlokHHg2YkkSUJNH9ElEDqGZCpuz1pElYXmWJkQT0z/ScgTn02Po4TLV7Y2jnJtXmvehvGleRX89r6xPbXkTKtVOsh9AYMbl8ffXjtnpt7Vxtbcdj27n6+q4lleXtRmw12FZoXZJklZheRejx+lypdOKgGq/Grl0w+gVVxLFeWf3583od94QQZc2h4zbFSLX5z8Ln3SqtTk6bcdbWcOBYa+MqxIyv4TiM95Smdbeot8wdOf/O673pWrOdjz5t1nE/67l36WLdb+G6On9OA8jR9j5c0dE6GsRcTuU4m4zHW29ADXreWkwtRtG+aFfv39dea2vL6a3R8VnHW79lv60GV22++bbVqc+ZKZ5+LLPFcsSY6Oefj9g+vva2PCa+XK7vgfp3VlvOkDnw5rL1sXxv9o3flSs4hjdCfExrri7vhR7FeW3y3yQAAAAAAAAAAAAAhLvmmmvkoosukpUrVxqPz507V0466SS54IILZKeddpqUmpRScuWVV8qXv/xl+eEPf+hse+ihh8r73/9++ZM/+RMZHeV/LQcAAAAAAAAAAMDg8SfrAQAAAAAAAAAAAAAAAAAAACDCyy+/LCeffLK8613vsj7sSETkxRdflK985Sty8MEHy0033TTwup5++mk57rjj5OSTT/Y+7EhEZNWqVfK+971PjjzySFm9evXA6wMAAAAAAAAAAAD4azimKSVJ476JqMZ9M2XOmyZhMZXWPwnolxU5AmPbYppq1+v2xsjnPZXwfnreIqdtLnztTTmK9VCc29AYRTvbPPS39fW11RfazlVPWVc2cTxL07xdZm5X1N93OMsXUJqvpCTvW8QsFpjt3OvnS8ypjZK0GHvkM+QyQ5KiTu2Qyuek1seWMqD+pO0z79J8ji33DWdufU1mLe55qeU+k0WOL7VPWugY24yr8TgsdYfUbL0eAus21tzRvHexrmparDOxnR+LNmu6S9Z1NQDTZcxOA6yxyZq1ir2OHLo8L52NcYrHFzwOT50hueufL6oxixh6u97+4v2/nqvoo9cR/XlEy2k8ptdnqbfeLzUe7+9vH7s5ZmYZty1eta95/n0x9M/ephyxfZrkaNrX9h3P9J3TN++x7YL6WL77+q7VJrnqucM0uf+56ovV5r8PwE2pZPP4DBOp088lA/Too4/KT3/6U3nqqafk5Zdflvnz58uee+4pCxculBkzZkxpbXfeeac8/PDD8uSTT4qIyO677y7777+/HHrooZ3mWbNmjaxYsUKefPJJef7552WnnXaS3XffXRYuXCjbb799p7kAAAAAAAAAAFuITETGp7qIAYj4//kmy/j4uJx00kny/e9/v7J/3rx5cuihh8p2220nP/vZz2TVqlWi1MT/w/PMM8/IO97xDrn55pvljW9840Dqeu655+RNb3qTPPDAA5X9M2bMkEMPPVT23HNPSdNUnnjiCbnjjjtk/fr1ZZs77rhD3vSmN8kPf/hD2XPPPQdSHwAAAAAAAAAA0fj9AxhKPPAIAAAAAAAAwKS45ppr5KKLLrL+Lcdz586Vk046SS644ALZaaedJq2uTZs2yRe+8AX52te+Jj/72c+MbRYsWCBnnnmmnHPOOa0eyrRq1Sq54IIL5Pvf/75s3LixdnzWrFnytre9TZYsWSKHHHJIVOxjjz1W/vM//7Nxbd/4xjfktNNOa9wfAAAAAAAAAIAtxXnnnVd52NGMGTPkoosukrPOOktmzpxZ7r/vvvvkzDPPLH8b2bBhgyxatEjuvvtumT9/fud1/dmf/VntYUfvf//7ZdmyZbLzzjtX9q9Zs0Y+97nPyd/8zd9Ilv9ljE888YS8733vkxtvvLHz2gAAAAAAAAAAAIBCOtUFAAAAAAAAABhuL7/8spx88snyrne9y/qwIxGRF198Ub7yla/IwQcfLDfddNOk1Pbwww/LkUceKeeff771YUciIqtXr5bzzjtPjjrqKFm9enWjXBdeeKEcccQRcv311xsfdiQy8Qcdrr/+ejniiCPkb/7mbxrlAQAAAAAAAAAAg/PII4/IF7/4xcq+q6++Wj74wQ9WHnYkInLQQQfJLbfcIkcddVS574UXXpBly5Z1Xtdjjz0ml19+eWXf+eefL1/5yldqDzsSEdl+++3lr//6r2tjuemmm+T222/vvD4AAAAAAAAAAACgMDrVBaB7ShLn8URUdMxMmWOmiTuWMvRLLH0yPXZgTFs8kV7dep2+GFk+h6m4+xn7enL62vfn0NsW51Y/h7YYRX9bTW3E5LCNPfRcxpxznzJWvuCSkcahejEz9zVnlU6s8iTprf4ylnYBFE1Uqh3I9CuniB2Q39LVJkm1ec8aPDMvnUhqupaCajCtr4bz3+V4yloCx9VmHK3q7rBekbi5r9VdaFF/rZ4O11VN0+s8hG1uOtD4/jSVprjmpuvIqcn9xaCL89np+DbXcQXW7asrKKeWS49ZxKjvz/vl+025VHnMnSOWa1z6MWvdTWJb6m4aMyZX6Pqpjd/QT//e5Otj+54Vkt+XyxrH8H3R1jc0h20crpoyy/fW2Byh+SZyhom5p8ScQ2Muz/d3YFiMj4/LSSedVPlbjkVE5s2bJ4ceeqhst9128rOf/UxWrVolSk18Pn7mmWfkHe94h9x8883yxje+cWC1Pf300/KWt7xFHn/88cr+BQsWyGtf+1pRSsm9995beRDSHXfcIW9961vlxz/+sfEPB9h89rOflU984hOVfbNnz5Y3vOENMn/+fHnqqafk//2//yfr168XEZGNGzfKueeeK0mSyJ//+Z+3GCUAAAAAAAAAAOjSsmXLZNOmTeX2aaedJu94xzus7WfPni3Lly+X173udeVfiPD1r39dPv7xj8s+++zTWV3/+q//WtneZZddZMmSJd5+//t//2+59NJL5a677qrEOuKIIzqrDQAAAAAAAAAAAOjHA48AAAAAAAAADMx5551XedjRjBkz5KKLLpKzzjqr8rcc33fffXLmmWfKypUrRURkw4YNsmjRIrn77rtl/vz5ndeVZZksWrSo8rCj+fPny/Lly+Wtb31rpe2NN94op59+ujz99NMiIvLoo4/KO9/5TvnhD38oSeJ/eNn3vvc9+eQnP1nZd9ZZZ8lnPvMZ2Wmnncp9zz33nPzFX/yFfO1rXyv3nXvuufK6171OTjzxxOgxPvroo1Ht+2sBAAAAAAAAAAB169atk2uuuaay79xzz/X223///WXRokVy1VVXiYjI2NiYXH755bXfD9p45JFHKttvfetbZdasWd5+SZLI7/7u71YeePTwww93VhcAAAAAAAAAAACg44FHA6YkkUz5/9BTiDRRncRR4q8nkbBc+thCalRan8TSJ9O208B4ppi+OosYtX7aXKWGebH2zfcH5zK0t81VcQ7186TH0HO5ciSBfXztTbWHzk3ZN5s4nqVp3k5fDd1RaiJHIuPa/m6uW6diWKlhdWcTB5NUm2dtKpTWN8kbFHNoosf0Kc+1I6YvVymzXckWaXXATc6L7R4TM57aOAqh42kxjtq15anbWquIv97UfK21qbeMETjfrerXWcaj6+J6t407SINrC1WTc8+OXH8BmtxXK/27HHcH42s7nkqs2LE1qN9Xr7eGgJx6Dj1mcbz4PNKLnTjb9/fx5ehCcN3acWt/R422Y8XnR9t5c8XMbHV66rJ9jzPl0tvGjNnXzleXb87KdhJeU9McQbEt30djc4Tk6uV0i7lumn6/D/ke3hXfuUePUsnkfI6YZNNtTI888oh88YtfrOy7+uqrjX/L8UEHHSS33HKLHHfcceVDj1544QVZtmyZ/NM//VPntV122WVy++23l9tz586VFStWyF577VVre+KJJ8qKFSvk8MMPl5deeklERFasWCFXXnmlvPvd73bmGR8fl4997GOiVO97w0c+8hG56KKLam3nzZsnl156qWyzzTZy8cUXi4iIUko++tGPylve8hYZGRmJGqNpLAAAAAAAAACAIZaJ/weyzdE0GtNNN90kr7zySrl91FFHyQEHHBDU9/TTTy8feCQicu2113b6wKO1a9dWtvfYY4/gvq9+9asr28XvIQAAAAAAAAAATDl+/wCGUvd/chsAAAAAAAAARGTZsmWyadOmcvu0004zPuyoMHv2bFm+fLnMnDmz3Pf1r3+99jcStzU+Pi5Lliyp7LvoooucDwjae++9aw8p+uQnPylZ5v6V4V/+5V/kwQcfLLdf85rXyF//9V87+1x44YXymte8pty+77775LLLLnP2AQAAAAAAAAAAg3fjjTdWto899tjgvsccc4yMjvb+rtpVq1bJM88801Vpsuuuu1a2169fH9xXbzt37txOagIAAAAAAAAAAABMeODRZiRTSdSrDSWJ8dW0RmculVRe1tjaKySmr87YWjJJyldoTl8uW/uQPrbzovfX+5ly1GJb+jQdr6tP6HG9XeWVTbyK/Eql+SswZt7fGLvjV20cmeFV1F/br8XKqq/euB0vUz7DyznfoWMMzKXnLGVp/CtkvQTUblwfsfXHjMO2NiPrnZT5dmgzz7H1dzLvLcbaZNyDvKdMF9N6DgawFpqsWdva7WTcHY1vKsZjHFtk/VE1t60hJKce1/J+X38/d7evvJ9P4jVmPdeezxC27UrfWtuJccZ+lnZdH761Wgj93NtG6GfxJnXV2mnfGVzjafuZ3xm74+9R7lzu746h7yNNvmc3/S7tMpn/LQDo2rp16+Saa66p7Dv33HO9/fbff39ZtGhRuT02NiaXX355p7X98Ic/lEcffbTc3n333eUP//APvf3+6I/+SHbfffdy+2c/+5msWLHC2edf/uVfKtsf+chHZNasWc4+s2bNkrPPPtsZBwAAAAAAAAAATL577rmnsn3UUUcF950zZ4687nWvq+y79957O6lLZOKBSv3uvPPO4L533HFHZfsNb3hDJzUBAAAAAAAAAAAAJjzwCAAAAAAAAEDnbrrpJnnllVfK7aOOOkoOOOCAoL6nn356Zfvaa6/ttLbrrruusv3Hf/zHMjIy4u03MjJSezCSq7YXXnhBfvCDH5TbM2fOlFNOOSWoxve85z0yY8aMcvs///M/5cUXXwzqCwAAAAAAAAAABuP++++vbC9YsCCq/7777lvZvu+++1rXVDjuuOPkNa95Tbn9gx/8QO666y5vvyeffFL+z//5P+X2jBkz5OSTT+6sLgAAAAAAAAAAAEA3OtUFYHAylTiPp4mKjqnEHDMRdyxTLbb8SmubWNplRRxHXl+soi5fLaYasnwuUm3stj62XM4cgX2K8+I7D6Zceo5abEd9oePwzkm+bT3XKs3bZUG1GOvL8rWQunMFxWgr5FFz5QLXGmdZdVs7nGiHe+1641XKXUCSB4kdb1LJEdm3WG8N5rg/r4iIZJHP8kurkxZTu76OQuuv1SwSXneDem3r3VevsU6RuDlOzYsydJ5DrtU2a9Urdj2FsMxJU7HX22ZlEPNv0dk9vojX5XnpYB66GF/rMbUYR2z9UbV66vLlduXS+9beg/PjegxV7k+N+4198u2m57q4N4b0L+qqj8/c1zY+V+zafksfW87M8XnH3sc9Hn3b9B0nto+vfUgfazsJ69cmh7e94fuj7/qMzVHN59Y0tzOm5TtyqCY50aEsndTPG5NmGo3pxhtvrGwfe+yxwX2POeYYGR0dlbGxMRERWbVqlTzzzDOyyy67THltxx57rHzuc58rt2+44Qa56KKLjG3//d//XcbHx8vtww8/XLbddtugPK961avksMMOk9tvv11ERMbGxuTf//3f5aSTTgquFQAAAAAAAACwhclEZNzbavPT7f9u09iLL75Y+8sJfuM3fiMqht7+4Ycfbl1XIU1T+ed//md585vfLBs2bJAsy2Tx4sXyb//2b7LXXnsZ+zzzzDOyaNGiyl9i8clPflJ22223zuoCAAAAAAAAAKAVfv8AhhIPPAIAAAAAAADQuXvuuaeyfdRRRwX3nTNnjrzuda+TVatWlfvuvffeTh54tGHDBlm9enVl35FHHhncf+HChZXthx9+WDZu3CgzZ86stW0zB0Wu4oFHIhNzAAAAAAAAAADAlk7/7/wh5s2bJzvvvHOrvGvWrKlsb7311jJnzpyoGHoNv/zlL1vVpFu4cKF873vfk1NOOUWee+45efjhh+X1r3+9nHHGGXLiiSfKnnvuKUmSyBNPPCG33HKLXHLJJfLCCy+U/d/3vvfJX/7lX3ZaEwAAAAAAAAAAAKDjgUcAAAAAAAAAOnf//fdXthcsWBDVf99996088Oi+++6TN7/5za3revDBB2V8vPfXO+y8887yqle9Krj/q171Ktlpp53k+eefFxGR8fFxeeihh+Tggw+utb3vvvsq203mwBXP5+yzz5aVK1fKY489JmvWrJFtttlGdtxxRznggAPkmGOOkUWLFsn+++8fFRMAAAAAAAAAgKm2aNGi6D5LliyRpUuXtsr78ssvV7Znz54dHUPv8+tf/7pVTSbHH3+83H///XLxxRfLZZddJo8++qhcfPHFcvHFF1v7HHDAAXLBBRfIu971rs7rAQAAAAAAAAAAAHQ88GjAMjXx6lKadBMnU/5AaRJWvJJqrET8/fT8tlxKa5do7TJDn9SSs4hVi+GpxdZvIr/WNx+7rW5bLmeO/JivruI8FPNv69eGHjNkTmPms/94cXKTtOOLqL8Gba2qLPwCU8q20jwybdX2hUm0ea0t8FTL6YjVLzFdKBZKz+GR5MFj5k4/p/r1Ys9VXwsxeU25JYsYb1qdyKZ1h9RsXfe+etP6yR5knWXfpvUWDHWLhNdeqcVzz4tdM8Ycbe5LMWsOQbo4p94cDdaiVcs10MV4OxnPFIwjuu6IGn312HK7+tU+K2ht9ZhFLL1fb3+vva2vNbdFzPu4LWatloi5srW1fVexxnasDdvYQteT3s5Um28O9D6x7V1qsSWsb8g4fPXU6nbkjo3t7efs5e4bW0MlZuD8tskRH7vdcWBQXnzxRXnxxRcr+37jN34jKobe/uGHH25dl0j9b32OravoUzzwSGSiNtMDj9rmajsHf//3f1/Zfumll+Sll16S1atXy/e+9z05//zz5R3veIf87d/+be3hSgAAAAAAAAAAoEp/4NFWW20VHUN/4JEesytjY2MiIjJr1ixv24ULF8rSpUvl+OOPH0gtAAAAAAAAAAAAgI4HHgEAAAAAAABDSn/gToh58+bJzjvv3CrvmjVrKttbb721zJkzJyqGXsMvf/nLVjUV9NqajDW0tra5BjUHhSzL5LrrrpNbbrlF/vmf/1l+//d/v9P4AAAAAAAAAAAMsyRp8pe5Df4v9rr00kvlIx/5iKxduzao/YoVK+Stb32rHHzwwfJP//RPcvTRRw+4QgAAAAAAAAAAAGzpeODRZihTce3TFr+NZqraOU3Ckiup9kvE3y80l8rbJY5aMm07tcQo69NiFbXoNej9jH3zsadi7uvL1Z8jtC5dMf/FvIfksLVJLNu+OTKdH9+5Czm3XVNZPhepvkoaxPBI0mIuJ3IlSVbvbymjPA9526BYlXb+Gst5940n1c95+NzZ6rS2r+UKv6FZ11lgblN+yTxjTfW7T54zZv6LPg3nyFujSOM6Xddm43oLIXUXLPWXtUSsE13o/SdmHTVlnavNzGTMVag2a8MqZu1adDFHrcc2heNoVXtg3SG1+eqwxXC+D2p99Bx6zCJWfX9irbFoq9cRfD4afAbq5bTX5arBOA7LPMbEEBHJHOfD3sd9nmLi6ft8sWPWvze2hM2V3s9Vh6mtMWZg7iaxa/2cRwNjhJ5jy7i6iB0Wq7NQsFBqen1W6YrS1s6iRYuiYyxZskSWLl3aqg79byPW/7biEHqfX//6161qKkxmbW1zNZ2D173udfK2t71NDjnkEFmwYIFsv/32smHDBnn22Wdl5cqVcuWVV8rdd99dtv/Vr34lJ510knz3u9+V3/7t346qEQAAAAAAAAAwjYznr2Gjjen666+XBQsWRIWYN29e6zK22Wabyva6deuiY+h99JhtfeYzn5FPfvKTlX2/+Zu/KX/6p38qxxxzjOy2226Spqk8/fTT8uMf/1guueQSufXWW0VE5J577pH/+T//p3z961+XU089tdO6AAAAAAAAAABobAv5/QPY0vDAIwAAAAAAAACd0h/0s9VWW0XH0B/2o8dsajJra5srdg5OOeUU+fKXvyyvfe1rrW3e/OY3yyc+8Qm57LLL5AMf+ED5EKXx8XE56aST5IEHHpDdd989qk4AAAAAAAAAACbTggULnP8tfFCm+wOP/uM//kP+8i//srJv6dKl8ld/9VeSJNW/CGSvvfaSvfbaS9797nfLJZdcIu9///tFKSXj4+NyxhlnyIIFC+Too4/urDYAAAAAAAAAAACgHw882gJkyn08TdzHq7HMjdPEnURJvV8i7j5FLltspdWSOGrI8n+mtvryWHoMXw3OvvmYU22cMblC29raFfNezHVIjtDYOlc73zyGzPNEu4kzmJZn1NQo/0d+stPyrE8c8K27gsrCLwx9LfpqK6WGFZnldabV+W8Uqy+eizdX0a44PxFzI6m+/m1XYZGjWm/MeUhqucL6uta2L7+es1x4Pmn9vATPv94vYI6i6zTUJxKx1qVdvSKGmvuFznPBMh6TmDG6+O6ZIWLW/7Dr6rwEiV1fFl2cv07G3XA8U15/ZN2+emNqscXyvYeZ3h9teYscesze/mq/YttUWxFDP9b480kLTc6DPge27xvWufTMcZtY+ratti6ZcvjqMH3XCukXW4cxZmDuJrFr/Sz7Q9Z66Nhtc9lF7Hq/Rt2AzZr+P9EPqk8Tk1lbbL/Y9meddVZw2/e85z2y//77y7HHHiuvvPKKiEw8UGnZsmVyySWXROUFAAAAAAAAAGBLsN1221W2X3nlFVm7dq3MmTMnOMazzz5b2d5+++27KE1ERD7xiU+IUr0fI0899VRZsmSJt99ZZ50l//3f/y2f/vSnRWTiL0k4++yz5Sc/+UlntQEAAAAAAAAAAAD9eOARAAAAAAAAMKSuv/56WbBgQVSfefPmtc47nf+G48msbZtttpGXXnqpca5B/i3PIiJveMMb5NOf/rScc8455b5vfvOb8nd/93dRfzgDAAAAAAAAAIAtwY477ig77LBD5b/9//znP5cDDzwwOMbjjz9e2d5vv/06qe3JJ5+UH//4x5V9IQ87Kpx33nnyhS98ofxt4o477pC77rpLXv/613dSHwAAAAAAAAAAANCPBx4NmBIRJYP52+gTUf5GATJHmDSw9ExVG6aJvzZ9XmzjCY2tVL3YRGubacdTS4xaP0NsvQ5r33ycqQS278tV5PC19bXTxeTQ6e30GvR2rpi+nGWM/MQlafs1r7I854gy7rfWEBLb0jbR5rhkGJdS+arMtNWqL1ad1ryIWcYz1jXRyTb2eqzwuSjPqSd2KaDeeo7qoH3jKPul9rXqzxm2bmy5SlnAOFNtfJ46XddudJ0h9YnUaqzkbFGvSPj5FAm4N4SOx8UxVpOYdYUOdXGuLWLWpDdW2/XRYpxdjKNx/Q3qjq03pDZfTO97UYPPDHpOPUdxXI9RbJtqLmLUY7u3C9bPJx3wxTTNsf55O/azWeY4b/Y+YWOv1Wbop+/z9Yltb2pj44vdJEctpuU7ruvcx9QxkcMtZO365qzJd/XQ89BrH50iWGj9Ayxh86eS4fzcpo1pwYIF8trXvnbSy+CBR7390/mBRyIif/qnfypLly6VX/3qVyIisnHjRrn11lvl7W9/e+e5AAAAAAAAAAADlon/x7bN0TQa04EHHigrVqwot1evXh31wKNHHnmkFq8LP/3pTyvb++yzj+y9997B/efMmSNHHnmk3HrrreW+22+/nQceAQAAAAAAAACmHr9/AENpcH8qHAAAAAAAAMAWabvttqtsv/LKK7J27dqoGM8++2xle/vtt29blojUa3vuueeiY4TW1jbXoOag36xZs+RNb3pTZd9dd93VeR4AAAAAAAAAAIbBwQcfXNleuXJlcN+1a9fW/hu8Hq+pNWvWVLZ33XXX6Bh6n+eff75NSQAAAAAAAAAAAIAVDzwCAAAAAAAA0Kkdd9xRdthhh8q+n//851ExHn/88cr2fvvt17ouUxw9T4jQ2trmGtQc6Pbaa6/KdpOHQAEAAAAAAAAAsCU48cQTK9u33XZbcN8f/OAHMjY2Vm4feuihsssuu3RSl/6XJsT+RRQiIi+//HJle5tttmlTEgAAAAAAAAAAAGA1OtUFoDklSVT7RFR0jszSJfWkzpS9QZqYg+rjsdWrx7bFExFRedvE0iYrYlj6lbUY+hd16PltObN8fKmEtTflsMb2tCvmNmYNhObWhbTztQnN1YZ+jrvso7K8/rQ6d4VEm9NyIfZL07xt5oxZi1UwxSzylzHcz7zTc/vi9fPNVe3cenKIMUfYM/uKcZT9fLn6+6bm69WeS2sfkMs0fyIiknnGl2rjClif1muuwTkWEX+NIrU6y5yB11PIfSDmnIo4xuMSMtZYlrnZYgxiTj1i10pQzAbvJzUdzEXTsU1l/U1qjn0vdsfy1G2JEVKDnl/PVT+eGGO74hTHan0s9dXmJOC0+T6r+WvxJ9E/29vmxleDrX+TWG3a1cbj2fa1N7Up20pc7EY59JiW76C2OXN9L7T2sfZw9/Plq8QI/C4dGq/aJ7pLTex3fWA6O/DAA2XFihXl9urVq+XAAw8M7v/II4/U4nXhNa95jYyMjMj4+LiIiDz77LPy61//Wrbddtug/r/61a8qf6PxyMiI9UFEBx54oFx33XXl9urVq6NqHdQc6GbPnl3ZXrdu3UDyAAAAAAAAAACwuTvhhBNk9uzZ5X9LX7lypTzwwANywAEHePsuX768sv3Od76zs7p22223yvaDDz4or7zyimy99dbBMe68887K9q677tpJbQAAAAAAAAAAAIBu8v+kOQAAAAAAAIChd/DBB1e2V65cGdx37dq1ctdddznjNTVr1izZd999G9fW/xAnEZH99ttPZs2aZWzbZg5ERH70ox8543Wl/wFOIiI77bTTQPIAAAAAAAAAALC523rrrWXx4sWVfZ/73Oe8/R566KHKX5IwOjoqp5xySmd1vf71r5cddtih3F6/fr1861vfCu7/ve99T5588snKvje+8Y2d1QcAAAAAAAAAAAD0G53qAjB5lCTG/Ymo6FiZ1iU1h7b0rTZOE3N+vV5bnUU8WxwREZW3SSxtsqKWBv1t+W19snxcqYS1N+XwjccWs5jT/rn0xY4dn4vvXIWcSxulravipGb5SU3zs5sk4+5+IbmysD7WdnlNiTbnlX1F31RblVlW3dYOm2L273fWVbRNixjuZ+IlSRYUrz9mWYNn3mvrKnDORUSklitsHCahc9XLFTmuBjlKmWVcqWM8DeoTaXaOS7Y6dZa6m1yj3ntjzHqy5bCNN0bo3KCmi3Nojd1gzRl1cH7bjLP1OFrUH1v3QN+LPe8BIuJ9j7HV56pBz6u31WPWtvP2rjihMaw1Wo73399858YWQ69b//wfwje+XmzzOXbVbqtH79OkblusTmNLs9gxNdTaWr5L2s9LeOxejrj2IbnKGJb6m8TqtQ1uahRaEwZLZclAP1dMlek0phNPPFEuueSScvu2224L7vuDH/xAxsbGyu1DDz1Udtlll05re+ihhyq1vfWtbw3qq4/jbW97m7XtW97yFhkZGZHx8Yn/HnHHHXfIr3/9a9l22229eX79619X/ibl0dFRectb3hJUY6zbb7+9sq3/LdAAAAAAAAAAgM1EJiLj3labH/v/EjUlli5dKldccYVs2rRJRESWL18u73znO+V//a//ZWy/fv16Of3002Xjxo3lvjPOOKP2FzTokqT6u8+tt94qxx57rLHtyMiILF68WC699NJy33nnnSdHH3209y9U+PnPfy7vf//7K/uOPvpomT9/vrMfAAAAAAAAAACTgt8/gKHEn7QHAAAAAAAA0LkTTjhBZs+eXW6vXLlSHnjggaC+y5cvr2y/853v7LK0Wrxvfetb5UOJXMbHx+Xb3/52cG077bRT5W8/3rhxo1x++eVBNV522WXlH5QQEfmt3/otmTt3blDfGHfffbfcfffdlX22PywBAAAAAAAAAABE9tlnHzn77LMr+xYvXixf+tKXKg81EhG5//775bjjjpMVK1aU+3bccUdZsmRJ53X91V/9VeW3mTVr1sjChQvlS1/6krzyyiu19hs3bpRvfvObcvjhh8uTTz5ZOfbXf/3XndcHAAAAAAAAAAAAFEanuoBhp1QimUr8DVtIE9WqvxJ7fYmExc4szdKAoRfz4xtHUaetJtM86zGV1ibRjusPwdOfCNbfv9bXMo6iTz1X3l7sNep9dHrs0LlswzoeR25bn9DjZbtsol2WTpyZtO+M+fraYiWpMu539m14TRc1ljkcj5zT2xZ1KpXmxzNzvVrMMo6jZn3ufHOg1+Jsa6vTErOsIWKOa+fedw5rucLHoRvEuJqej1IW8CzD1DIeT32u6yy6Tl1I3QVL/WUtkddo7P1DJOxe0ZZ3zqapyZibUE3v104xa9Wi7Ry1GleL+pvW3aTe0Fwh93AR8b83iL1OXy2mGmx99BzFtt5ej1kcN9Voj9FwnTR4KnRZg1Z3yPeg2HXV5DxZ+wTOke289Wv7nU+PaYqnf29rWr+r1lpby3dFW+6Y2L0cce1DcpUxHN91Q2NMtAtq1qiGNkLrH8j7IRBo6623lsWLF8u3vvWtct/nPvc5+cY3vuHs99BDD8l1111Xbo+Ojsopp5zSaW3HHHOM7L333vLoo4+KiMgTTzwh3/72t+XUU0919vv2t79d+R/+9913Xzn66KOdff74j/9Y/vM//7Pc/ru/+zs57bTTZNasWdY+GzZskIsvvriyz1dbE+Pj4/KRj3yksm/BggVy0EEHdZ4LAAAAAAAAAIBhcuGFF8q9994rN9xwg4iIbNq0ST70oQ/Jpz71KTnssMNk2223lUceeUTuvPNOUar3o+PMmTPluuuuk/nz53de0x577CGXXXaZvOtd7yr/oodf//rX8qEPfUg+/vGPy+GHHy677babpGkqTz/9tPzkJz+Rl19+uRbnM5/5jBxzzDGd1wcAAAAAAAAAAAAU2v+JbQAAAAAAAAAwWLp0qcyYMaPcXr58uXz3u9+1tl+/fr2cfvrplb/9+IwzzpB9993XmSdJksrrtttuc7YfGRmRZcuWVfadc8458thjj1n7PPbYY7WHA33605+WNHX/J9ZTTz1VXvOa15TbDz74oPzFX/yFs8/5558vDz74YLl90EEHyXve8x5nn3/4h3+Q9evXO9v027hxo7z3ve+VW265pbJ/EH+jNAAAAAAAAAAAw2ZkZESuuuoqOemkkyr7n332Wbnxxhvl6quvljvuuKPysKOdd95ZvvOd7wz0YULvfOc75Tvf+Y7ssssulf3r1q2TH/7wh3LVVVfJFVdcIbfddlvtYUdz5syRf/iHf/D+jgEAAAAAAAAAAAC0xQOPhkCmkqBXE0oS4yu8tvordByhNbnze2KpRJTjeJa/YvraclrbSyKZZRx6n9DYejv9uGn+fH1sbO2K/a4YbXO0qavczpLKy1S//uq1SYNevtym+movrU5rjlq7kHGEtbPmMIwjdI58McNyxNUvWeJ+tRhHm7nyjSc2hzNflrpfkbV1VadvboLr94yjybiajruN0Dmbbq/W4+7gfESdl9D15Ls+OpijVuNqWH+bupvWG5Mr+P4bcW8Pfq8NrcXZxzM/thx6TMNc2mNU2zb9vBLz2UFXfK5ss65s270cqWSG92DXddL0s7SN6buLr35fe1Pu0O9gTWJb21q+o/jOua29sY+Yv2v5cjjHEfhd0XeOQ7/DunLHfnc21TeI7/ioCr1Hbo6v6WSfffaRs88+u7Jv8eLF8qUvfanyUCMRkfvvv1+OO+44WbFiRblvxx13HNgDeN7znvfIEUccUW6/+OKLsnDhQvm3f/u3WtubbrpJjjrqKHnppZfKfQsXLqz9IQaTkZER+fznPy9J0rt2L7roInnf+94nL7zwQqXt888/L2eddZb83d/9XbkvSRL5whe+ICMjI848H/7wh2XvvfeWP//zP5fbb79dxsbGjO3GxsbkO9/5jhxxxBHyjW98o3Ls+OOP9z5YCQAAAAAAAAAwjWVD/JqGttlmG7niiivk6quvliOPPNLabu7cufKBD3xA7rnnHjnxxBMHXtfv/M7vyH333Sef/exnvX+phIjILrvsIh/72Mfk3nvvlQ9+8IMDrw8AAAAAAAAAgChT/RvFFvb7BzBZRqe6AAAAAAAAAADD68ILL5R7771XbrjhBhER2bRpk3zoQx+ST33qU3LYYYfJtttuK4888ojceeed0v+3HM+cOVOuu+46mT9//kDqStNUrrvuOjnyyCPl5z//uYiI/OIXv5ATTjhB9ttvP3nta18rSim59957ZfXq1ZW+e+21l1x77bWVhxi5vP3tb5dPf/rT8olPfKLcd8kll8i3vvUtOeKII2TXXXeVX/ziF/J//+//lXXr1lX6XnjhhcF/+OHpp5+Wz3/+8/L5z39eZs2aJa997Wtl/vz5st1228mmTZvk2WeflTvuuKP2NzaLiPzmb/5m1JgAAAAAAAAAAMCExYsXy+LFi+XRRx+VO++8U5566ilZu3at7LrrrrLnnnvK0UcfLTNnzoyO2/+7Say5c+fK+eefL+eff7488cQTcscdd8gvfvELWbNmjSilZLvttpN58+bJoYceKgsWLGicBwAAAAAAAAAAAGiCBx4BAAAAAAAAGJiRkRG56qqr5Mwzz5Qrr7yy3P/ss8/KjTfeaOyz8847yze/+U055phjBlrb/Pnz5d///d/l3e9+t6xatarc//DDD8vDDz9s7HPYYYfJlVdeKbvssktUrr/4i7+QJElkyZIlsmnTJhERWbdundx2223G9jNmzJBPfepT8vGPfzwqT2HDhg1y5513etslSSIf+tCH5HOf+5xstdVWjXIBAAAAAAAAAACRvffeW/bee++pLqNmjz32kD322GOqywAAAAAAAAAAAABKPPBoC5Ip89/OnibxfwOMEnOsRPyxMq1JavlL4/V6bXUWtbhy+2Kp/HhiyZEV/Uz5LX2LnKG5snwcadAcmmP7mHKHzJ8pZ+y4Y9qUx/Nt23npp7RzXPRRWb5/xHwe/HFNZ73Kd23VY2TWzSQ1z29tOx9Xr301R5JklXaVY1oOGz23t50hly13wTe/xTjK9o1yhNVfCsghm8M4AvLpOUqZZ92nmfVQmzpFwuamjGWrv+Abh4tjjCah95RBCrlXhpoO44nS5lx7xKxJa4y289lifE3rb1JzbK6Q99iawBwh9dvq9dVl71ffb6ujiKHn0mPr/fu3/W2rseM/r/SUnyts4/HNWZP15Bh7ZX/E+YitJ7SGkL76/PuOdxk7NE6Ttra6XbFt77CxOcp+lu+mMTF67YKaBed159rM3muBDmyzzTZyxRVXyOLFi+ULX/iC/PjHPza2mzt3rpx00kmybNkymTdv3qTUtv/++8vtt98uX/jCF+TSSy+VRx55xNhu3333lTPPPFM++tGPyowZMxrlOv/88+XEE0+UZcuWyQ033CAbN26stZk5c6a87W1vk6VLl8ohhxwSHPtv//Zv5dZbb5Xbb79dXnjhBW/7efPmyR/8wR/IBz/4QTnggANihgEAAAAAAAAAAAAAAAAAAAAAAAA0xgOPAAAAAAAAAEyKxYsXy+LFi+XRRx+VO++8U5566ilZu3at7LrrrrLnnnvK0UcfLTNnzoyOq1S7B4DOmDFDzjvvPDnvvPPkjjvukIceekieeuopERHZbbfdZP/995fDDz+8VY7CoYceKtdff7289NJLsmLFCnnyySflhRdekB133FF23313Wbhwoeywww7RcT/2sY/Jxz72MREReeKJJ+TBBx+UJ554Ql544QVZt26djIyMyA477CA77bSTHHLIIbLvvvt2Mh4AAAAAAAAAAAAAAAAAAAAAAAAgBg88GjClJl6DlCTt+mfKHiBN4opXUo2ViL9/popcvnaJsyY9tyu/LZbS5iLRjmd9/57q+fO+tT6eXPUcvRrSvP7Q2Ho735yZ+GK24YsVmktlE+2ytHcW0iSzNZ9oW4yrdubCua6VkPa9OZ2oIclrrqw7fRi+crX2SVrNUe7vm59i/mx6Mczt9PNja2dsG5i7F9s+AYnlnNty6LF7OfzntbYmPeOQzWQcseejlAVcR6llXIHXUcg9x1d/LaZtPCYhYwxhmYdBCJ3baaeruQ4Qu2acsbqY74ZjbzOONnXH5nXd+6wCc4SOI6RmW522vrbcrpr0WHrO+vHEGNNUU69tNWbo55aQ7wKh57KIVdRZqz/gs56tTy+H5Xw5xtF2Luo12M9DV0zfbXx12GoIbSdS/S4SEzMstllsjrKfYY5iY2SBHw1CcjWtoY3Q/9Yw6P8msVlTSfB7z2ZlM/lcuPfee8vee+891WUYHX744Z093Mhlhx12kN/5nd8ZSOw99thD9thjj4HEBgAAAAAAAABMY5mIjE91EQMwef8LDgAAAAAAAAAAmG74/QMYSpP3J8wBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAWa3SqC0B7SoW1S5L42Jkyd0qTsKRK6v0TMffNtN2ppV69JlctRX57zsQZQ+XHE8Px4oF5+lPDbH1sudw58j55/a62rph6btU3h0Ub31y1GYev7qbjqtY1cSbS/MzYYtnqt611Yx1Z3MWUpLbzkOa19h6/qI+xyFXEqB3X69ae5NjrZ3++XX/+/py2cdRyGmOGtdXPky93pa1jTBOxw8blytHLZanLtmZtuTazcbTJJZnnmYqp+7GjIeusrCP0PSny2hXxjDGEbx7QSJNzGRw7Yu05tTj3bcbXtP4mOX33r5pGOcL6+Op31ervaz7uqk2PqeevH/dsG2os2uix9c80bdZTV7cw3/hC+tSOW8YVcw00Obe+dvq+2vnwHTd8f2p6XftyV9pKWFvbZ2Zre0d90TkMcxPat97OfTwkV9sanPlbfvwAAAAAAAAAAAAAAAAAAAAAAAAAAGA640/hAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAgRud6gIweZQy70+S+FiZqnZKE0twUx1S7ZuIuW+m7U4tdRa1uGooctpzuWOo/HhiOJ4V9QX2seVy5fDVEzIHsUJj+sbpitE2h8p6OZLUPJ+9ZNV2+ho2xfSp5ShqsdSp5y7GrVTa1zerxC5i6TH03GU7Sz9jnWk9fzVmVtm2xdLn3lSfXmd0u4DzUl8D7mf6hY7PncPdp3Y9u3IY5nEiR9j5KdsHnPN6jshxBOTy5ZQs8JmLqXmclTo89etC7rNl7Ih7QnB+25wMuUHMpTdn5NoIErp2HZrORZvxxOb03UONonNEvOcGxg6p2xbLV4/tuKs2vR69rR6ztp23N+XWY+ufbXy5bPrvkaHv/aG52qzhzHJuo9aRpa3tc2HTdoNWO9fadvB4JHw81s/Otjl1xIo9D/r3x9B+1TbeJkG5YvM6c22ZHwemHaUG9HlhirG+AAAAAAAAAADYgo3nr2EzjGMCAAAAAAAAAABh+P0DGErt/8Q2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACABw88AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzc61QUMu0wSySQZSOxUVCdxlCNMElh6pqoN0yS8NpXPT+IZT6aK2GE1mOpQ2rnQcxYxbPWrvhyJ1iYrclr61Np7clVj5229c1SNqec25bTVpwttFyI0lm2O2tSisrBFrQzrKTiHrW+mbRoeOZfWVpDeqbqZpNVzHaKYN9tc9GK6n4mXJJkzTpM6kyR8PPr599VR1FC2t4yvGJeJb85q7S3jMK5d3zwOsP6ybxfj6ChnyXSh2KT2sfdrcn13ce8r8wfeh9DT5p4cLGatGXRxXlu990Tm993jjRqOMWZc4e/T7vpD4tjq8tVri22qyd42MebS29eOG3Lon4F9MXxc7X2fIer1mseZGcaRau9fpjau3DrTdwNbnbb9UWu3NkZ3rNpx8ef2xQyuzfH91FenL7frndjax5bD8z3adY57bdzHfTlicllzdPcRwsr33xwG9d8kAAAAAAAAAAAAAAAAAAAAAAAAAABoot2f7AYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgwOtUFxHr3u98tV155ZWXfnnvuKY899tjUFDSFMkmC2qWiGudQlq6JJ3Wmqg3SxF+D0saTWOrOtN2po5aiDlv+Iqeey9dPRETlbRKtTVbUFdpey6X65q4eO28r1bZ6uzb0ObHNhW2/q6bQem3t9P39c1VMfJZOzPxIMm7sG6tpP5GA+rN8O+21q6+HNO+TOWOUOVPD3Gi12MZki6nH7sWxPzOvqLdsGxzTXVvTtnE1+J8F2HR8vRz2dWW9Piw5pIP6y74djqOXy3O928blyW2UBT7HMTWP36XNvcCmy3v3dDSIOQsWuhYC+NZoUIyWc9GkhpB7gVGjXHF9YsbjG4cvVkhttjb+2NXaXO31HLXtzHPcMA/FZwZfX9/+EMbPXg62dpnjfLqOiYTMUfj893KGjUdvFzOXettaLPHHng51+mLa3lldNVhzeL7nuuZD/65o48sRkssau8O399Dv/GhOZUkn7/XTzTCOCQC2FPz+AQAAAAAAgNaU2H/A25wN9/9mAwBDjd8/AAAAAAAA0Bq/fwBDqbs/DT4Jvvvd79b+YzcAAAAAAAAAAMDmjN8/AAAAAAAAAADAsOH3DwAAAAAAAACAzehUFxBqzZo18oEPfGCqy9gsZZIY96ctHvmmtK6JOUWvBlVvkCbu/CqvO/HUmakinj+/Lactl6+fiIjK2yRam+IhgfpTxWztY2L72ul124672sTWFDOu2Ni+8bhi6Wx1N6nTJtHqtO0vt7NeuyS1ncM075O5Y2fa/tRcS38f37hssfUc1Vjm5+kV9ZftAmO65jz2nNbaW2ow1dHL0e34qrHD1m7JVr8zR1j9ZXvHHJV9rXMVdz6a5PbVUJO1eN5j2t0jYJvcf7Y4bc6VQ8y68sbq4DzG1mO7hoNE52rwPjmA8YTGDKnX1saWw1afqyZrDm2/HqN2XMvd/xnO3zesBhfbZ5pQRb9MG0eX16DO9L1Dr8enTTtX/q7oefWctvr174Ux59UeM669iH2OlOV7q29Os4C3f1vs2FzG2C2fpG77vg4AALYs/P4BAAAAAAAAAACGDb9/AAAAAAAAAABcBvOnyAfgox/9qDz11FMiIrLttttOcTUAAAAAAAAAAADt8fsHAAAAAAAAAAAYNvz+AQAAAAAAAABwGZ3qAkLcfPPN8s///M8iIjI6OioXXHCBfOQjH5niqsIolYhSSSexkkR1EqeQib2uVOJyKa15EjDkTJuX1DI+pdWZWGrL+nanlvxFTl8uPYevn4iU51k/T1lRk7VneI5ezLxt5HmKYZuLJnxjsx3X5zRmjmwxfNdjF9erHkPPbdsWEVFZvi81j1mp6kpKksydM6uPp4htG6u+hn3tTDn0XL1Y5iuhGEfZzhJTjxdSX5v2MXVMxG43PnfsuHGKI4e0rN/YN3Kuejnd11zMe59rXmNqcsoG8KzI1D+/m4VBzE2k0DUQFbPh+0IXtdiuSafIvE3GFzu2mHGExu7i/dyWy1avvb09l35Mj1E7ruXWPy+baqjHcG/HiL1He89LXr+rne++Hzq+mHHb2tbmv0XMWixxx9bbx+avxPLkcuW1zo2lf0zsso/le6mtfe+487AzdmyuSsyGHyNc37+71uV9YNgplTZ7v53mhnFMADDMNuffPwAAAAAAADANjeevYTOMYwKAIcbvHwAAAAAAAOgUv38AQ2na/wmotWvXynvf+95y+5xzzpFDDjlk6goCAAAAAAAAAABoid8/AAAAAAAAAADAsOH3DwAAAAAAAABAiGn/wKPzzz9fHnvsMRER2WeffWTp0qVTWg8AAAAAAAAAAEBb/P4BAAAAAAAAAACGDb9/AAAAAAAAAABCjE51AS4rVqyQL3/5y+X2V7/6VZk9e/YUVjS1lEqC2iWJap0rE3OuVMJiK61ZElB6lo8v9dSv8toSRy1Zfii15PXlsuXI+s6Br06f4nzq58uUw9bWF1MfpylO6Lzb2tn2969XW922cen7reMyFpr/I03zurJaPSEy1fx5cEXOgm88prlSWbXeTCund06rBxLLePvnWI+dpPZzZ4vhy2HLZc9pnu9En0tLPHPMsHH42pv6hI6rF9u+nkLHaI8dN04REbHNY4P6e/ky437XOROxj6uXO/zaDX3/89XkzOGptxH9At/CtTk/wTki3xOMMTqoM+TaqmiRM3bMTcYXOp6Y2L66vccDctnqtvW15XTVosfS2+o1ZPpxQy31GO7tXuzuP+PYNMkVulb1dvqcudjaNs0dm78rtXWiz4nlO11ILGs7y377enNcF5b6fLVkAW/FttihOco4Ld72Y+bfX8fkry8AADB4/P4BAAAAAAAAAACGDb9/AAAAAAAAAABCTds/Yb9hwwb5kz/5E8myiT9Seeqpp8rxxx8/xVUBAAAAAAAAAAA0x+8fAAAAAAAAAABg2PD7BwAAAAAAAAAgxuhUF2CzdOlSefDBB0VEZN68efKFL3xhiivafCiVOI8niWocO5Nq7FTCYqm+Zom7PMm0+lNLvSqvJXHUkGmH0kQ/njTOYetbzL8+z1lRg57D0t5Yj9a2OB+h5yFEaD22diH9rXOkzaneLmauejG7f66byqoLKUn1cVRzpsnE2Q8Zj379lm3znEUu23Wi8txJklWOm+4Leuza8dR8bq1xHPee2joJzmk/f7UxthxHL259ffnGHltDNbZ5jE3H14sbfj5KlhylDsZR6+fJ6Zq7eg2e+suamt8zffU2ETPG6WgQcxJdQ+C5j4rZclyu+5dXg9yxc9BkfLFjCs0RU7uvbUhO2zhsfW05XbWExtJr0d/f9TimnPWY1W3fZ6GQOet9/mi2roscRW0x59z2WdOaK2COQmPFtjOpnVPxnS9//bWYDcdhytU2ZlBsie8zcTygHkvs0BxlnIi3Zv07clODeC9DAJVMi88ynWM9AcC0x+8fAAAAAAAAGJhMRManuogBcP8vSACAaYDfPwAAAAAAADAw/P4BDKXunwTSgTvvvFM+//nPl9sXX3yx7LjjjlNYEQAAAAAAAAAAQDv8/gEAAAAAAAAAAIYNv38AAAAAAAAAAGKNTnUBurGxMfmTP/kTGRsbExGRE088UU455ZQprmq4KJVYjyWJioqVSTVWKv7+SmuS2MuZyJHXm1pqU1IPkFjqyPLdaaLvD8thimvrW8yzPqfFg/ZCnjbmq8tGz900joh77LFi58p2vNjuf2hhbT61Jxomqbt+lXkWYoO+Rc5MTVSXJhNF2cZT6asdK7fzXPp49LlVqjojSVJ/xKOeV59/37hscYyxLPedpjknYpqvIn2sMTEn4vrH42prau9aX/Y6BjO+idhhdde4rpPIcfRyuh8/GnNt+q7zXk3Nr/fY98kQbe4/w6jN+fHG7nCufWvbqWEdTeYmdsxNxhWaI6Z+X1tfzpBx2GLYclv3O2rR++h1ZYExTbnrsavbmZZrEJ91umCbg9TxOSmkf5u2bdoN8h4WSv+O1sW1p79bW9s5cpm+t/n6TByPixcTu4zV4O1dn+dQ02GNAACAqcPvHwAAAAAAAAAAYNjw+wcAAAAAAAAAoIlp98CjCy+8UP7rv/5LRETmzJkjX/nKV6aslmeffVaee+65qD6rV68eUDUAAAAAAAAAAGBzxe8fAAAAAAAAAABg2PD7BwAAAAAAAACgiWn1wKP77rtPPv3pT5fbn/rUp2Svvfaasnr+8R//UZYtW9YqhlKJZCrpqCKzNFGdxVKWWpPAHJlU+6fi76e0Jolluop5DBmvyutILPmzfHea6PvdOVTf+PTYtr7FnPrmMLSdqW0x7yHz3d+/EiNwfm3tXP1jxtYfq1DEbDNHKou7Dm3XgoteV5EzSYs5TivHU8msMWr1e8bTy2Gbu2ruas6sksNWU8E2l0UNUbFa5tTzTsQ0j7UYpy+mHi+mzibtY+qYiF0dnz4uX1x37BbvAbZ8lly9nPa1OZHbPD5jrMDr3Db+oBwdvqeH3hOnuy7npHENkfd4ZyzPmvRqUUubuYx/n4sfZ2iO0HHEjNeX2zceV39bHdb9lliu8ej16e/XesyYmvR9+ueNJvX6hN6/9BwxOWO/Q7XJNYg1WzvH4q6v1t6Qy9dG/y4WWpuzbWg725p11OSrI7MsM1fM0NhlrMC34tC5Nefo/n2y7fWBHqWSoZyfYRwTAAyDYfz9AwAAAAAAANNQJuE/9G1OhnFMADAE+P0DAAAAAAAAk4LfP4Ch1PJPdHcnyzI544wzZMOGDSIicvjhh8uHP/zhKa4KAAAAAAAAAACgOX7/AAAAAAAAAAAAw4bfPwAAAAAAAAAAbUybBx598YtflB//+MciIjI6Oipf+9rXZGRkZIqrAgAAAAAAAAAAaI7fPwAAAAAAAAAAwLDh9w8AAAAAAAAAQBujU12AiMgjjzwin/zkJ8vtc845Rw455JCpKyj3p3/6p/Kud70rqs/q1atl0aJFgynIIlOJ83iaqNY5lJYjCYyZyUS/VMJrUKrIYYmp1eIan8rzJ5b8mSpimHM0mbvQvln+T/2pY/1zXcxzm3qaKuoIPddd5LDljBm/LeZksF0nKstrSvVxTZz9NMnKfb458eUo96fVtdNPn0elqqsw6avHlFPPXbbL6u30MQfHirjn2MZea6fMz/irjdcwDlvs0PG42lvbWuqo1xA2riaxezn815H1HDnmU0RELDl7ucOfzegacyWmryZTbE+dTUzm/Wlz0uT8BMeOWE9WHdQXe+6bzEnsWJvlCOsTM97QOmzjC+lvq8dXpy22qZ+tPv19WY+px3LVpB/LtJy+2G1Er+FarYn1mE3Me2tsPaa6Yto1rWPQXPPubRvYzhpPms9HZnnLDYnp+26sAt/Oswb1d7kGfOMAAACbF37/AAAAAAAAAAAAw4bfPwAAAAAAAAAAbU35A4+UUvLe975XXnnlFRER2WeffWTp0qVTW1Ru5513lp133nmqywAAAAAAAAAAAJsZfv8AAAAAAAAAAADDht8/AAAAAAAAAABdmPIHHl166aXyH//xH+X2V7/6VZk9e/YUVjR8MpUY96eJahxTaTETT6xM6jWk4u6jVBHbXUsxPtd4VJ4/seTM8t1pou+3x/bFrLXPY+lzleX/TANi6PXoMYt5Lua2dtwwHltdtfot47X1718jtTF7xmFbX2U/Q33lPFpiTAbbHJTjyrRxpcW4eiNKk8zc1zIn1v2WXBP53PcEpcyrMUmyynbIfUCvw1RPSCzX+ay1DczZi10drz5OV2x7TEsNpjmKaBtXg/2uYhtj7NxV88WNo2TJWQrI3ash5E7qPsfW2L46I4TM5+aoyzmKzh147oO0HEeb95/YOWwy7vgc4e1jxx5Si2+Mvhiumnz12mLb+plqtb3n6rH1mL7tao5qXl/skJiDYpsPEf/7SGi9pnbW89AiZigl7vOh1xZSfy2G4btWUG0dXN+uc2pj65NZ3h71OWxSgwp8620yl03XR5O5Q/eUmpr74aCFrnkAwODx+wcAAAAAAAAmXSYi41NdxADE/+8+AIAB4fcPAAAAAAAATDp+/wCG0pQ/8GjJkiXlv//2b/+2LFiwQB577DFnn6effrqyPTY2Vuuz2267ycyZM7sqEwAAAAAAAAAAIBi/fwAAAAAAAAAAgGHD7x8AAAAAAAAAgC5M+QOP1q1bV/7797//fdl7772jYzz55JO1fqtWrZJDDjmkbXmtqfzVVNJVIQaZskdPk7iqlRYrCeifaaNLLTOltN2Jpez+8djqV3nOxJIrU0V/c2xTXFtMW59irkLmKKZtV1xjDWnnnCvPeGzHY+ZBX9exa9kWJySW7TqwjivL96e9/ZlK81xZs5ieXJVYqfncFeprN9VqybTj9nuKrx69Flss0xoIbWuaA3Pu1NhuIqY25sCYvdiG8+C5HnztY2uYiG0eoz6+Njl6ucLPpZEld0VAHf1c51hnm5M2bPOJCTHnJ1oHc++613n7NszfZE5ic8WMK3YOYmrxjdUXy1ZbSM222PaY9lr199TQ2L7tzJFTz+GLpXN9N7AJ/byoa7PeQt83TblD87Zp12QeuxYyFzb6u55tLqzn1vEN1tYnsywjVyxXvEoMz0cE/TtpiKbvA5O5NvRhT963SQAAoBv23z8AAAAAAAAAAMCWh98/AAAAAAAAAABdGOCfKAcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJgwOtUFwE016JN0kDdT1ShpEleJyvsnEf2yvPLUM2qVH04cAy3qt9Wt8lxJ5Az3z4se2xbTV0vZrogbUUcRU59vfS5rxx3j0DU5l6Fsc6O09Zdo4wyZK9NY2/LFso1Dr79Q7s96+5O0qDvNY2aVPsExLfsrsTKtTWpeu7reurOfgcRSt60evRZbTXocU6zQnLbces5qTPOYa+MNHI+pzl7Mbsflq2Midtj42uTo5XJfT1H3HEcdFZ6aTFzrPIRt7oZZ2zlrJHQNBPCtTW//FrXEzl2TXLHji2kfWk/IOMNjmduF1G3LYY9prtv1+UDPoccOrT8z5A6t35aji89IsTFMtZSf7wI/Z7W9Rk1Cx9Fmzurn1H2eTLlqbSTsXPviTMQKY5sDZfkW6pqzzPK2bIsVErOM4XnL1+fOHqf5Oe/ye0ih+29kUFnS6r17uhrGMQEAAAAAAAAAgECZhP8AuDkZxjEBAAAAAAAAAIAw/P4BDKUp+BPpVWvWrBGlVNTr1ltvrcTYc889a20OOeSQqRkQAAAAAAAAAADY4vH7BwAAAAAAAAAAGDb8/gEAAAAAAAAA6MKUP/AIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMv9GpLgDdU5b9SYuYmar2ThNbFq0WVc+aePpmeaWpdSRF7CKeI1ae31avynMlWq4s30zbTJpHMTe++YhtO+h6bHPm6q8f07f19VVILe1D6jetva7Y6rBdJ7b6TftVlu9Li7mpPpcuTbJKX72m0P3GejLznBW1FELuB0qrO8nr7h235PLUpNcSFathTl/+idju8bpi22Oaz6Wvna+9rQ53LebnI9rG6crhy9XL6b+Go++JnpoqPPWFss0dHGLOU6Qu3ht8azusjrh10SRn7Fhj2ofWEzrOkHi++rzHHTlsfW312z63xOTwbfdyVWsw5bDXX91vq9sXZxBCctXqt7RLPZ8nTbFCxxozJ3p+1erbVzdqNcVc56FzZBmn63xklrdY35z51rBI7zuiNUbgeWlyPYTU58zZqjcAAAAAAAAAAAAAAAAAAAAAAAAAAJsP/hQ+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYuNGpLgCTR1n2Jw1iZaraK01s0Q115H0TT58sryy1Vl7EkzyeI1ae01anynMlWq4s30wNsW0x7bEs7bX5yPqOxT6RrB4rbA5NfHMW2t8Vw7cWfDWYcihtbfrWWRN6DlsuvX5bbf37y31ZPjepHjPNY2aV/ba5NNVqyusaR1FLebxWUz1OfZ2bV3NiGUdoLa66QtdCzJrxzUUvZnW8+jhdMW2xQ8+Xq73veggdXy+H/S7lGrMpV2jOav6wd7BG9wHHWnOKqH/oNZ3DFkLXRFCshvW7rouuczYZb2ifJuMPHXtIbF+d3uOOHLa+vvpN73e2XPYciXM7s9TQJoe17g6vFx9XLr0+37hsn7dicsbmbhLb1taX0zSeWhuJj2Fieqe2jS04pvNcB4WIzq0C4upzVo8Rfz2EzkktV6NeGBQlaav37ulK8XxzAAAAAAAAAAC2XOP5a9gM45gAAAAAAAAAAEAYfv8AhhJ/AgoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzc6FQXMOwySSRTSacx00R1Gs8VLbRyfYwhNSqtT2Lpk2lVpJaKVd/uxFJ4UaetPpXnSrQcWb6ZGuLaYtpjuWuotM3/WTyZrJizYq5iYpn6x8Qw9Y2twRZDXwuF2jgtx/vbFIp6bLHbsK3V2Dkytdf3qUy7TtJiTqqzkSZZpb+rVl8b73FLTf1s9736HFTHkSSZdjzsPhFSl2+d+doZ22aWmLXc9ucL6mO2xTbN80TsiDkKnANbDb5aqrnMY7aN15czNn+1lvD7gGv+ggTUj3CDuIeLhK0zbwzHdd11/ibzENsnpqbQsYfGDKnV18aWKyy2eTy297KYXLb8+n79fd2WyzWeesy4ORnU9dbP9b1Izz+IekJjhs6lsW/wN6jJUxtPRF/rOupwnLZYvnlXAW/Z+nfJeoywcbT5Tt/tt3czX32+eQAAAAAAAAAAAAAAAAAAAAAAAAAAYDJtlg88OvbYY0WF/OlGAAAAAAAAAACAzQS/fwAAAAAAAAAAgGHD7x8AAAAAAAAAAN1m+cCjLV2mkqB2adL+RwE9Qljmeo0htai8T+Jpm+VVpLXq+mNJHstdX+wcZX3N00Q/1ixmIXT8gxZah8rPQ6KdB1N/fW6UZw0nWjtbLf1x9Dah10kh5rzZ6tfr1vfb1ohpHLYYKsvnJNXHm1a20ySz1qrPlS1X8PHMPtf1Ot33BqWNI8nHYaslpi69FltM19oPbRuaeyKme8yxMWPmyNcndHwhdfVypcb9tnHH5g+pwRk78t4x1ffszUns3DbK4VkbUbEsa3WQNcTOUZM5Da0vZvzhMcPr9bW15QzJYRub7bNDk1y2Y/p+/f07Jpcthz6O0FpCxH6+CqHXUZ8j9+fIkPuwLabteNM4Lr6cITVlEhcjRvAcWL4R2nJnjtMTG6vs54ipz5E9Rli7JnM6iE8Gg7j2MEFlSaefH6aLYRwTAAAAAAAAAAAIlInI+FQXMQDh/2sRAAAAAAAAAAAYNvz+AQylZn+SGwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAIPPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM3OtUFYHAylRj3p4lqHFPvac7grsWXX+VtE0+7LM+e1qrqjyV5LHddek0qj504YoeyxbLlNtaZ/7N4Qpk+R3qs2nFtrlTf+fDOc2CdIe1CY+n1F9v6PITwjU+vLYStftvatZ0vU7zQGCqrxkhSfX2leexMdErLb8sVetzWLqxO+1xM5Kqe7cQwHltdtbozS91p2PiMMQPnwpbbnN+8wvWxh46nGts9R23bN61rIpf/ynad/5AaYmsKyhVx74gVev/q0iDH00boOQ2OF7DevDFa1BQ7z03OS2x9MXMSGttXd8y4fDltsVzjsr33x+ZyjcN2LNPqsuWMyaWPxz4n7vHFfCYKFXKu9TZ6HbbPDIO8b8XMhZLwczVZanMa0Td07LZ2meMtTJ+r0JzKEjML/lY6mPXf1bv1IK49AAAAAAAAAAAAAAAAAAAAAAAAAACmo/Z/2hsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBjdKoLwOTLVFLZThPVOFbRM3G2Muf35VVanYmlfZZnT8UeT6kiRlxNKo+dGGJn+a400ffHx6rW2gtoG/MgxJ4XX22ucehrUFfUYMtlqtXWVl9HoVzj811DtrXrG09/LF+M2v5M25+qPHb9uXZpkgXVazse067WxlJnwT+39uf0JZHjCq2pUczAdjH59bHr47XFc8cMO7e+9q4+rrpctZnzu5/TaJuT2JqcOSLqbavp/Wtz0WT+g2N71kpUrIZ1Njl/sX2a1BY6NzGxQ+uOGZ8vvy1WyPhsn0NsOe257DXqx0zvx6acMbms44jc7/tc5urblCueXo/tPbXJdxpf7NBxhsyZTX1t+GvIxN0mJEZILaZYZdvAb3yZ4zTYYvjmU1li6vNi7ju4c9r0E0Gb9YMBUMlwfuYaxjEBAAAAAAAAAIAwWf4aNsM4JgAAAAAAAAAAEIbfP4Ch1N2fBgcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALAYneoCht0gHxbX1dOqMpXYcyQqKIbeyh7RnteXS+XtE0u7rC9rWquoiJHXF1Jgf788dmKIm+W70kTfn+T73eMKbWesS5sTPVbteD4O2/y4YlvbaXNjGo/SzrUvpm1ObOPtb6vn0vly67lCYvjWsl63a25Dx+7dn+X7U1OO6t0jTap3qdDzFdLO16aoszyexs1tNVd1XIlnXKE1meqyxWwzV7b89tzu8bpi+mOHzVXbPq7ayv6WGs01uN8ZXXMUnMNTb4iYMU03XYy/C75zHRWr5Zh87ztd9RGJqzV2juJih7X1tesip2+crs+5tvz2XHH7J/JX6wvNaYvpHE8HMVz9YsTEsNUTOt+mz4Vt6bFixqO37bKupvR3v6jxWL7ZhY7L1j+or+XtMguIGTrG8HE0N8g1EPrJhoe9AwAAAAAAAAAAAAAAAAAAAAAAAACmk+7+tDgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDF6FQXgOYyz/EunmaVqaQaM1FB/fRWibGVOZcvh8rbJY52WZ4xrVVSxMjr0goLraENldeWWGqrtNXGWpzzLp9UFjKfIvFz07929D5KuVdEOd4ip7bfVHPoWvXl1msIiaG3tc2VXrepltrYPXNni1Xuzww5Ur3e6opKk+rdxTZn+rhd47G1qR3X6q3Xaj9/9bmqjitJzHdNX02mumz1hcQytXO29cxJL6b9zmAde2DsXo6w8bn6xPQVsc+9iL/eei3xd0/b3LXhGtOWqMl5CY7d4VyHvn80bV/p26Du0HkMjR1Tf2jbkNy+WL5x2t4n2uSO3T9RR7VOW349hi2m6/2vbYxBnGsb1zhsOXzvI128N4WOS69fBX3LCYylbWeG2LU2DccRUk9su6zB1yf7mrS0D5jvpufSGi+oVbPYQbE6iwQbpdrf26Yj23UEAAAAAAAAAAC2AJmIjE91EQPAj2cAAAAAAAAAAGy5+P0DGEqD+xPmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOR54BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABm50qgvA4GSW/W2ecpWppBorUUH9+lsl1lbVHL7YKm+XONplebZUzG1UvjvRirLVoPqqT7SYWb6ZBsaq1RrYzkSfizaxms5/MTf6vPTH9ClyWseTt0u0dv30PqE5da7Ytrb6cdv14lq7trHb6rXFcubIqjGTVK/bfJdIk+pdRZ8j13hsbbzHPbX2889VdVxJYr5LRp17T32mWLZ4IfNpymnLXY0dOPbI2LbxTeQIu4fE9qvEsNRbi+mYG28Oy/VgzWWZ2y1B7Fx1kjNwDQTFCnzf6KqfSHz9TeY4NEfMOELb+nKHxPGN2fd+76rBlj92v+1905Q/NLZtXK45s9fXzThdQj93dVmH7X2zzVrWx9Hm+u4yVuMatO2oufF+a8tzWN5iXf3ta9LS3lNLyLhC12iTTwxt1r+I/Xs7AAAAAAAAAAAAAAAAAAAAAAAAAADDavL/ZDoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANjijE51AZh8mbbd5qlXmUomYiQquE/RMukotsrbJY52WZ4tlfA6fTWoPGaixczyzdQzQFt/Y1ttjMU5DD139f694nxzUuvrOS8x49Lpsa25DX0TrY++35dTZzznnti+477xmWL5Yvhy6/uNdWda39Q8Z5mqznyaVO8mQblazqFeq7veatv6nFXHkyT63TG8Llt9ttqazFVXuSdih489NnYvR1z9vn4xMWoxDeumEi9gPMG5FM+SbMt3vhrFDFhXg4wRO6Ym6yg0R+w4Ytr7aoiKZZkD2/u2rwZXbl9dtuP6+6Jr/LYY+n7b+GLqbxIj5Lgrtk9IbPs8u/uGfK5qUo+zv+GbTNOYtfMXEFufk3qMMKa5NY3N1DazTLOtvy2fiIhq+Dbc5ZqNKaHpdVD2b9UbXVMqGchnj6nWxWcfAAAAAAAAAACwmRrPX8NmGMcEAAAAAAAAAADC8PsHMJT4U/kAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGDgRqe6gKGnRJRKBhI6SVQncTLHsdAnYmXaGNOA2vQWtlkKjd0/z7a5yfIsqZZdqaKfuwZTbpXHTGojCouh9+8fb8g8dqWYv2LuXGN29Sv3W8+onT4HRe6Q2vTrLNH6WnPa1oqhX+3cWXLqx/X9tvGZYtly+K4LW+6gujPznCWpPo7qHSJN6neT0DlqetxUr15nr17fnNnveIk2tpC6YmqLitkwtyu/bez6uF2xfTl6uSzrK+J+1/S6tsZzjMeawzNO9DSZ38a5OvjM1TRGm3G67j9tc4SOJ2bcoXX4YoaOW8T8vhxSi6sG2zHbfv19L6QGPZY9dlgtrvGExgiJ5YvZJNagctfe36MqMcfQ6wutpetYXelyPLFc/ZXnrTSzfJ/odO0GtWo3D67v220FX3t8bAEAAAAAAAAAAAAAAAAAAAAAAAAATCNN/jwoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAFB54BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABm50qgtAc0olzuNJolrnyLTt0CdkZVptaUAtRQv3qHqxXTGLubHNQZZnSaV6XOWbia+IAFkeK030/f76bfRxFecntR3XcvnmJSq3JXbBlMO2Zou2Kj8viYTV37/O9PkMqSekpn6+dW2bX9t+11poGkuvzTQ+W0zb8bJdprVL9Rrqd4g0qd5FfLlijxvbZJZzWqs3/D6ltLElkeNy1abXFR0zsJ0pvy13L7Z73CE5wnOFX4veGibh/dE2ziZ8czMVuhxfF3zndDLjtZkb/ZoaRK7QsQW3i6jBFzNm/Po9OrQeWw2u2nx16+9vthqa5NDH2aR+61xF7rfFCa2jbfvYcehsn1Gb5LRREna+gmJpfbOA2L714n+XdtTj/RZW1NCuf1AOS6yQ+fad09B33Ni1UenbuGdd1+9/qFMqGcp5HsYxAQAAAAAAAACAQEq6/dFquph+/3sNAAAAAAAAAACYLPz+AQyluD91DQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA0MDoVBeAwVEqMe5PkuaPetMffBf6xKysr5bUk784aq6+HtMVr5iD2DGrvHmiFeEah8orTrRH6WWqaO/JaegfMsa2sjxvmueNnTNbjbb1Z6Ln1OdCz2GqMdPyhdZjG6epvd42dOyJVre+37muImP5agvp6ztetsu0dmm9Xaaqd4k0qd5Fuqilq3r1NTRRr219VMeVRI4rpq7YmCFr15bbX4P9rq/PQdtcvZz+e0n8PT78/tTmPTOUbW6GXcx5mIrYTc+L6zrpKleT8YX2ianFFzN0Lkz331osS13+GuzHbcf09642NejHbGO1xXDFDo3lmyPX/LeZ30HltH3m0z8fhqwrX+4mMQYRaypqCG2rLN/a3OfYktMSq80aLmN4W4THqvWJ7lE1yPdDAAAAAAAAAAAAAAAAAAAAAAAAAGhq3bp18tOf/lTuv/9+eemll2T9+vXyqle9SnbeeWc57LDDZMGCBZLoD+VoYNOmTfKjH/1Ifv7zn8svfvEL2WabbWS33XaTQw89VPbaa6/2A+nz6KOPyk9/+lN56qmn5OWXX5b58+fLnnvuKQsXLpQZM2Z0lmcyx7S54oFHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALCFW7lypVx88cVy/fXXy8aNG63tdt99dznjjDPk7LPPlrlz50bnee6552TJkiVy5ZVXyosvvmhss3DhQjnnnHPk93//96Pj97vmmmvkoosukpUrVxqPz507V0466SS54IILZKeddmqcZzLHtLnjgUdbIKWqT0hLEtU4Vpb/M43pk+dPPXmLo77nuYXEK8asjzXLo6di7qvy3aaHytnyqjxmYokZU7eNPh79PNSOt8il97XOpWr+5D1bbH0ubbX008dsy1WIuR5Cx27Lofcz7e8ylimeK2bBFtt2vGyX9dolqblNptK8hsx4PLaWruoVMdfsOx+9nNW7YKKNL2qdBdQVHTPyvh9aQzWHew5Cc8Xk7OV233/avM/5Yg8i5+am6RxNl5y2NeivIebTT7ucMeMNbRtaQ1zusDkJ+czgq89WV+z+iXrcddtq0WO6c7SL4Zqz2DH75j/knPvaxHwubBur9l5taON939Ni6DGV+M9T08/CWYPYtXobZa6Py5Srt9/f19VfpPfdqtbHEquLdRb6bhxz/prOd2Eq3jdhp1Ta6j19uhrGMQEAAAAAAAAAgEDj+WvYDOOYAAAAAAAAAABAGH7/mDRjY2PyZ3/2Z/KP//iPomx/IKzPk08+KRdccIF89atfleXLl8uJJ54YnOuGG26Q0047TZ599llnuxUrVsiKFSvkPe95j3z1q1+VOXPmBOcQEXn55Zflve99r1xxxRXOdi+++KJ85StfkWuvvVa++c1vygknnBCVR2TyxjQseOARAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGyBlFJy8sknyzXXXFM7dsABB8iBBx4os2fPlueee05+8pOfyEsvvVQef+aZZ+Qd73iHfOc73wl66NFtt90mixYtko0bN5b7kiSRww47TPbZZx9Zs2aNrFq1Sp5//vny+GWXXSa/+tWv5Prrr5c0DfvLwsfHx+Wkk06S73//+5X98+bNk0MPPVS22247+dnPfiarVq0qH/BUjOXmm2+WN77xjUF5JnNMw4QHHg1YphLJVDKQ2GnifyJaCOWoLwnMkWnbIZeSPi+28RR7fbPYH88aK2+jjyvLo6fSzZyaZKqozd1O9Y00yespxjaIc17MRewc2OaySd/aWtBqK+ZEn49Kn6Ktdqx2rj3rztc/Jocttt7edD5CY+n9QupvOwdBc5RpbVJ9XNW7RJrod5FqLtc666JekXrNIqa6A+9b2vgSbXym+25oXXpNtpgxc9ZVDdUc5ncCfS6s/Q3nIyZ/tRb3jbfJfaxtTtQNcs5c68nb17KWB5Ezdg5i2ofWExUzcG58n4VDarPV5avXdVx/L/LV06QGfeyxMVxz11WskHPuaxP6fSckl3e9WL4VuD43T+Y9Wc8VugYmtQbvN6v+vqHtbOvL0SeijpBclbwdxKi0j2qd1zAJ5zp0HIP6bxIAAAAAAAAAAAAAAAAAAAAAAAAAhsPXvva12sOOfuu3fku+/OUvy8EHH1zZPzY2Jt/61rfkIx/5iPzyl78UEZGNGzfKqaeeKg899JBst9121jxPPPGE/N7v/V7lwUBHH320XHrppXLggQeW+zZs2CBf/epX5WMf+5hs2rRJRET+9V//VT75yU/KZz/72aAxnXfeeZWHHc2YMUMuuugiOeuss2TmzJnl/vvuu0/OPPNMWblyZZl70aJFcvfdd8v8+fO9eSZzTMNky3vEEwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACg9sCd3/qt35Kbb7659rAjEZHR0VE5/fTT5eabb5ZZs2aV+5999ln5p3/6J2eeJUuWyEsvvVRuL1y4UG6++ebKg4FERGbNmiUf/vCH5aqrrqrsv+iii+Txxx/3jueRRx6RL37xi5V9V199tXzwgx+sPOxIROSggw6SW265RY466qhy3wsvvCDLli3z5pnMMQ0bHngEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFuYu+++Wx577LHKvr//+7+XGTNmOPv95m/+prz3ve+t7PvXf/1Xa/uHH35YvvnNb5bbM2fOlOXLl8tWW21l7bNo0SI59dRTy+0NGzYEPYho2bJlsmnTpnL7tNNOk3e84x3W9rNnz5bly5dXHob09a9/XR555BFnnskc07DhgUebsUwlzlcXlEoqr+Da8lcMX91Ke7WJZe0niWRS76fUxCsml5JElCFWV7VO1FU9L/q8x543Z2zbOLU1EvKy9dVz1fpK9dUvtD6d77px1W+bK19s13lpW2eT+mPnoEkOlSWVV72GtPJqkqNpvS62ent1h913lUorryZ16zX5aouZs7Y1hNTTy5EaXzFc+UNqqNfU/D4Gv8mY0y7WRNO12S5ns/tazP0r9l7hrjdsbrzvKwG1NX3vtX+2SP3vNZZ6QmtwfR5v+hnCNb6uYrlydPn5yba+rHOmfe7zfbbWjzf5fmSrQR+XKUcs/XtHSOxaG+nus3/oOEK/47QRu5YrfcX9XTF4LWivGF1+VpiM7/xbOuWZ4831xedVAAAAAAAAAAC2YPqPXcP0AgAAAAAAAAAAW6ap/o1iC/j9Q3+oz6tf/Wr5H//jfwT11R8i9PDDD1vbXn755TI+Pl5u/97v/Z7st99+3hznnntuZfuqq66S9evXW9uvW7dOrrnmGmcMk/33318WLVpUbo+Njcnll1/u7DNZYxpGPPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALYwa9eurWzvsccewX1f/epXV7Zfeukla9vrrruusn366acH5TjwwAPliCOOKLfXrl0r//Zv/2Ztf9NNN8krr7xSbh911FFywAEHBOXSa7r22mud7SdrTMOIBx4NsUwlxlcbSiWVl7cGwyu0bm8t+atJLF/9mSSSialf7xWaq95u4hVCSSKqr46259DUX58Lfey14w1qCF2Lvlz68WJ+THPky2FbAyHXi61vaGxbe+U45osV2q+LOegih8qSyqteQ1p5NckRXEvIeDz19uoOu98qldZejccXWFtMzKbtTfWE1DWRqz4npnlpU0NoLUE5DNdv7Gs662J8XY6zq3NqW2cxa61J7ti56eLaaxs7Zo687xue2kLmJvR4vTbH+4ilrtj35jafGfQYvs8KphhdxHLV6ooZGtv2WbBSX9/nOmX4HG7LVTtPWhzX58QuvyfZxqzXPZlscxzWt/qdxdbXvibM35lEXN+13OvHRvW9bIK/q0n8g9Hbvu9N1poEAAAAAAAAAAAAAAAAAAAAAAAAAJNdd921sr1+/frgvnrbuXPnGts9/fTT8l//9V/l9ujoqBx99NHBeY499tjK9g033GBte+ONNzr7uhxzzDEyOjpabq9atUqeeeYZY9vJHNMw4oFHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALCFecMb3iCzZs0qt++//35Zt25dUN877rijFsvknnvuqWy//vWvlzlz5gTXuHDhwsr2vffea22r5zrqqKOC88yZM0de97rXBeWazDENo1F/EwybTCXWY2miomKpPFYS0S8rcvna5bF9NRVH7aOyx2pSf69vnteVWERUXlki1RxZvpkm7hrDaqmOI3SO23Cto6YxirHXxqPNjem8KW0FFPNty1H2047ra6G/f2xf2/pynWtbH1uu2PHFxNbrHWQOlWnHU72G+mpOk6yy7cuht/Mdb1NvwTd31bxpniuzHA8cX2ao31KfaazO2I7r3ncfDZ2zek77ncw2V96YhjmqxQ6srw3XfG4JQs5D49iOdRMco2F9Tc5raJ8mNQXHDpiz0Pd+X50hNfna2I6b3i9qfS312WLac9lr1PvExoitpetYTWP6+pX9nZ/cw3L42od8TgytS49tGl/Te3qT2Pq7X0gME1O7LPDtz742HH0C59eXo+znPBoWo2wX1CrP2/Bcd/H9Cd1T2WA/k0wV1exjMgAAAAAAAAAAGAaZiIxPdREDwO8fAAAAAAAAAABsufj9Y+C23XZb+eM//mO59NJLRURk/fr18vWvf10++MEPOvuNj4/Ll770pcq+U0891dj2vvvuq2wvWLAgqsZ9993XGa/f/fff3zrXqlWrKrne/OY319pN5piG0SCfhwIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAmKYuvPBC2Wuvvcrtj3/843LzzTdb22/atEnOOuusyoOB3vzmN8vv//7vG9uvXr26sv0bv/EbUfXtueeele0XXnhBXnrppVq7F198UV588cVWufT2Dz/8sLHdZI1pWI1OdQHDTuWvQUgGEDNT1ahpEla90volAf30B87Znr5V1OSrpf+obW5ssYr69bqzPFIacRatOfJYiSdW/zkoYuh9Q+ekzK2Nz5hDb6ONPfQc6+1C6HVZa7IcN9WjtFWgz10hdC206WtdX47z6JtvX0xbja6+obljcthieI9n2vHUdD6qd400qd5V2tbQdb0i9bkTMa2j6riSxPx4zqj6A+trErtpH72m0NqqOd3PbbTNXVBsS30+MfUPi6Zz1Sqn59xHxWpYf5P3u9g+MbWFxg6dO9P9yhrTU2fMuG1tfTH094Syn6O22Fy2OXHVFhqr6bib1OWfy/bjqfUL+ObSpq5+ps+JheK9KaSekJpMfOc2G8i3ODd9vFHXd/Bcmfe7xtt8Pfn5YsR+QuhiLXQp9tPGlvfpBAAAAAAAAAAAAAAAAAAAAAAAAECMuXPnyq233iq/93u/J6tWrZJ169bJCSecIIsXL5bFixfLAQccILNnz5bnn39eVq5cKV/96lflwQcfLPv/f//f/yfXXHONJIn5z1WtWbOmsr3zzjtH1bfNNtvIVlttJevXry/3/fKXv5QddtjBmWfrrbeWOXPmROXSa/vlL39pbDdZYxpWPPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKbA6tWro/vMmzcv+iE7LnvttZfcfvvtsnz5crnkkkvkjjvukKuuukquuuoqa58dd9xRzjnnHPnzP/9zmTFjhrXdyy+/XNmePXt2dH2zZ8+uPBzo17/+9cDy9DPl6TKXb0zDigceAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAUWLRoUXSfJUuWyNKlSzutY3x8XMbHx2XWrFmSJIkopaxtX/3qV8sFF1wg7373u50POxKpPxxoq622iq5t9uzZ8tJLL1ljdpnHFbPrXL4xDSseeLQZs98WJiQd5MhUNUqa+LJOUFq/JKBfVuTooJbiSOwcFHXr9WZ5pNQw68X9OdGSFfX65izLD6cdnDBb/W3Yxq6fY1s/E1usom597nzHTfXoc6DyehItt+08mcZXWxeBfRPLOPQ4/Wwxa+PyjNu1DkOvU9+6Clnrvhje41l+PLXnyFT17pEmWWXbu0YCznlwrEw77qzbfW9T2rgSbVyhNTWtLza2rU9oX722Sl9PnfUabO8oRS3muWzDVX/XXPMxmXV0xXe+GsXsYB5873dd9gutNyZ26Lya3ousMTuqM2Qctjb6Pb/Wz1GjLaY9V1z7mFhd1eKrp2nMpn2V51O4q9aYtRja3/aZ0qbt+Quh9w2Jrb9reWNEfBvKtCmx9Q09P67vAo3vqyF5vd9RAnM1qLHt2q3k7ywSfJQkra7l6Srm+gcAAAAAAAAAAENmPH8Nm2EcEwAAAAAAAAAACMPvH5PmRz/6kfzhH/6hPPbYY0Ht//u//1tOP/10+fjHPy6f/exn5cwzzwzOlegP6NjM+0x2rmHQ/Z8sBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABMe7fccoscf/zxlYcd7b777nLhhRfKqlWrZM2aNbJx40Z5+umn5cYbb5RTTz1VRkdHRUTkueeek/e+971y1llniVLmvw5+m222qWyvW7cuuka9jx5zMvNMdq5hNDrVBWBwzLcBkTbP98pUtXea2LJotfT1Szx9siJ2YC2uGooj+ph9fYt69VqzvkipNsPFfdf3ADWVx0isZ8hep97XNw59Lk3jquUIGPtETGXcH8IWS8/tq01fj6a2hUSbw3K/mGOZ5tQ6N56+ej9bbaaYtli2vqE1toltOz4ZOVRWz5GklvWv0ryGzHg85DyEtOlvZz2u1W2rWSTk/lS9OyaB44upz1enKbYvh62vr32lr6XOMpZjXs21hD/30TbPU8k3H9NFzDw3ztHBXLjWddf9YuuNyRE636Z7tjFeRK2+OkPG4WuTWcZnq7NNTtsc6e1dOUJjNK2lSaymtTn7eD4PNokZ0tck5HNVbEy9RlN/X5smn5m7pteYRbxt2tdLm4oCczTsV2kTWEPM2gi9j1pzteoNAAAAAAAAAAAAAAAAAAAAAAAAYJhcf/31smDBgqg+8+bN6yT3c889JyeffLKsX7++3Pe7v/u78u1vf1te9apXVdrusssucsIJJ8gJJ5wg73//++Xtb3+7vPDCCyIicumll8q+++4r5557bi0HDzxql2sY8cAjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJgCCxYskNe+9rVTkvuiiy6S5557rtw+4IAD5KqrrpKtttrK2e/II4+UK6+8Uo4//vhy37Jly+T000+XnXfeudJ2u+22q2z35wvx8ssv1x4OtP3229fa6XleeeUVWbt2rcyZMyc417PPPuvNY8o1qDENKx54NGCZSiRTibddmqhJqGaCK5O/0ip9bCHjUHmfxNM2K2IG1uDKXRzRx+fr66o1y6Olzhm151B5/yTvn+WH09iT0BG9Tt95yiyrRQWs90IRW59LPUa5FhLz8Uos23xbxqOfhzKnY23o+fWY0TX0xbPFKjSJaToeE9sWY7rkUJnWJtXnsHoXSZNMTELuTV3UK1KvWcRUd9j9VWnjSyzji6nPVqdeYyc5LPcMXz9jLMO8ioTV7Y2tfO9GZq7zsTlpOv7O8lvObXSciPeoTvtG1h+aK+a8hHweFYmr1Vdn2+Mi9Xt4LYalXlds2zHbHNnau3KExoqtpU1dvpiNxtPw86CrljbXmq2//jmx4PvuEnrdxKh9zg1YE/o7iTdG9Le6+L7KMnW27wgTfSKvPU8NIefH9y7cZL01XReT942/bhBreVgolbS+70xHwzgmAAAAAAAAAAAQSIn/h7LN0VT+4AYAAAAAAAAAAKYWv38M3NVXX13ZPvfcc70POyocd9xxcswxx8gPfvADERFZt26dXHHFFfLhD3+40m6//farbD/++ONRNert586dKzvssEOt3Y477ig77LCDvPTSS+W+n//853LggQc2zqXXbts/qDENq6n90+sAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgEm1du1a+dnPflbZd9xxx0XFOP744yvbt99+e62N/sCh1atXR+V45JFHKtsHHXSQtW3XuWwPS5rMMQ2j0akuABMylTTumybdPbpNjxRbVTGOkJqUNubE0kd/2J7tKV39c2jLX+zVxxVTt4/KQyRaktAcWd/hNDH3VfkIknxEteP5djGnxRwWc9c/97Z5742n+dos2Maux860VKlUx1OOw1BzbcxabH1uCollTvXaTbFsufW+oe1d9YXGtPUznUdb7MLmmkNpCylJ9Rqqd5E0qd5lQnI0rdd1vRV16/UWfHPXy1m/SyaJ+bGlMfX111i2t9TqyhGcy3Hv8fWtxdJvLg4hY4rKbTgfmBBzXqJjd/DeFRujyXiic0Ssp9DPljF1++pte1ykfo+uxfDUa8vhym2bq9hYrjnX+8TGCJm7QcS09rV8S+hibkL6hOh/n7S938XmqH12NfSvtYn+RtU9vc7M8FZnO6d6X2V5m7SNs9G1Z+3h7letxy30/ttmHQ7iAettrwsAAAAAAAAAAAAAAAAAAAAAAAAA6LdmzZravl133TUqht7++eefr7U5+OCDK9t33XWXvPLKK7L11lsH5fjRj37kjKcfW7FiRbm9cuVK+d3f/d2gPGvXrpW77rorKNdkjmkY8afwAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGALsv3229f2rV27NirGyy+/XNneZpttam3mz58vr3/968vtsbEx+eEPfxic47bbbqtsv+1tb7O2PfHEE519XX7wgx/I2NhYuX3ooYfKLrvsYmw7mWMaRjzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC2IHPmzJFXvepVlX2rVq2KinHHHXdUtnfddVdju3e+852V7W984xtB8R944AG5/fbby+05c+bIW9/6Vmv7E044QWbPnl1ur1y5Uh544IGgXMuXL69s6zXrJmtMw4gHHg2BTCXOVxtKezWtKSiXSkQFtM3yV0h+ay4xj8fWz1VbJolkYuoz8fJRkogy9G9Cr1+v2zR3tTYN10/IGvTFrtdbndviuCuH7VyF5izOh+u8+GKFzqne3lS3bzxN1mxom9Acrji+Nt5rtYNxqCypvOo1pJVXk3EE1xJyzj319uoOv1aVSiuv0Pp89Fpd9bbN5erbJlYttmFMbca5JZqMOexyDTSN0WQ80TkCr12R8HtCaN0x71Ft742+e3B/3aE1uGoLfV/27XfNuS9WfQ7i38+b1BVSg7Ov9vloEJ99uvhs4BtP/LVoHp8zv+VzrC1G7bjUP7t7Y2jnp833wdC+9u9C9rm1ry/3d05bv8zwsoldPzGafnd25e/yuz3MfJ8pNucXAAAAAAAAAADYQo0P8QsAAAAAAAAAAGyZpvo3ii3g949jjz22sn3JJZcE93366aflu9/9bmXfMcccY2z7nve8R0ZGRsrta6+9Vh5++GFvjs997nOV7T/4gz+Qrbbaytp+6623lsWLFztjmDz00ENy3XXXldujo6NyyimnOPtM1piGEQ88AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAtzEknnVTZvvLKK+Xb3/62t9+GDRvkj/7oj+Tll18u922zzTZywgknGNvvt99+cuqpp5bbGzdulNNOO03Wr19vzfGd73xHli9fXm7PnDlTlixZ4q1t6dKlMmPGjHJ7+fLltQcz9Vu/fr2cfvrpsnHjxnLfGWecIfvuu68zz2SOadjwwKMtQKYS46sJpb2a1ODNoRJRAe2y/BWSN5atX1Gbqb5MEskkoO6ImjI18bL1VZKICsgZwjouy/qxraf+ObK9bLFttehza6rVV4etnW/8xRyb5tl3Tfnm1MRah2U8/z97dx5uSVUe+v+t3acbmgaVOYBhbFAGTdCYMOaqRASvEUgacVauZjBqHDKAxp+ImETvVQxXE8M1GkiEgHCBmKig4vCItBgFBxAVBPEiIEMDMjRNd1f9/jhVdXa9td41VNU+5/Tu7+d5zhN31Vrv+65Va1etzQ6b2HH5+obOx9St44TaWLHnYxxFnkmR+8Y5av2l5oqtZbxtqN5w3fH3+KIYNf5CtSWNR9UbqrtPrtRYfWK2chjjTPlbjBbDuFKeI0PEDPYdcE3bbUfOP0vKvi627pha48cTemb577ExdYfuv+2c6c/rPs/crrFSni9d6krel4ztg4rAXiymBpfUPWXofEwuq74+95whnzGh2N0/4zQ/W/j2ue0aZv9Sa23mN+Zf/J8trX4xnwdj6hrPkTK3XT8bWzn7flYHAAAAAAAAAAAAAAAAAAAAAAAAgC5e8pKXyK/92q/Vr4uikFe96lXy5je/We68805nny9/+ctyyCGHyBe/+MXG8VNOOUW23XZbM9fpp5/eOH/11VfL7/zO78gPf/jDRrt169bJhz/8YTnxxBMbx//sz/5M9thjj+CY9t57b3nzm9/cOLZq1Sr5yEc+0vhRIxGRG2+8UY466ii5+uqr62Pbb7999I8QzdeYps3MQhcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJhfo9FILr74Yjn88MPl7rvvFpHZHz363//7f8tHPvIRefrTny577723LF++XNasWSPXXXed3HXXXa04L3jBC+SUU07x5nryk58sl1xyiTz/+c+vf3jo61//uhxwwAHyzGc+U/bee2958MEH5dprr5V77rmn0feFL3yhnHHGGdHjet/73ic33HCDfO5znxMRkfXr18ub3vQmOeOMM+QZz3iGbLPNNnLLLbfItddeK0VR1P2WLVsml156qeyyyy5ReeZzTNOEHzyasEJE8h79R0MV4pAXWTtfVjha2nTrdsRwXitnodplRrtqfn1zVeXUuapXVt1Wv/H6dF15GW1URq/ua5lKomMXZb+sNavhunTf1nlV6/ia1PNmjUvT1yeFdW3Ddeu5zYIxdM7Udo1zgWuUGjNmfVV031DMSp/Y1vlpyVHk6vzIXvN5MSrrcN/RQ++bmPtZ7D0vtu6Ue3xRNO8EmTHOufbt2MF7RsJ8h3LF5uwSs2/spDrycB3TqM/zY5Ixu16PLrn1ey7E9X42Y0eOI1R3yrhCbfPAeGNqtnKEc9vnU2NasXw16HOpMWKuQ2rM0HoqPDv62JhdcgevZeCTxkj8+wCR9j6w7hu456fMsa4ztAZa572VuOlrpnPkCY+0dn1Gu6hPfoFr3rFvzBzF3sNi7699dgUp9/C+rLmZ/K5m01UUo+BzalOUutcAAAAAAAAAAABTpO//0+pixZdeAAAAAAAAAABsvvj+Y16sXLlSvvrVr8orX/lK+da3vlUfz/NcvvOd78h3vvMds2+WZfIHf/AH8nd/93eydOnSYK5nP/vZcumll8prXvOa+geAiqKQb33rW43c41760pfKxz72MVmyZEn0mJYsWSKf+tSn5HWve51ceOGF9fG7775bLr/8cmefnXbaSc4991w58sgjo/OIzN+Ypgn/BhQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAbKae+tSnyurVq+Xcc8+VQw89VLLM/x+DX758ubz85S+Xq6++Ws4++2xZvnx5dK4XvOAFcv3118sf//Efy7bbbmu2O+SQQ+Tiiy+W888/X1asWBEdv7L11lvLBRdcIBdddJEccsghZrvttttOXv/618v1118vxxxzTHIekfkb07SYWegC4JfyQ3ND/HpVXrhvOKMs7ufhdCv/7auZM5SjKNtlRrvxubLmwspVvbLqja3Rpyi7Bu7pUoxVkZWV5WXfUcyEJqrmTc9ZYayFUJwUVU59bfV8t86XczQaW3FWjDqXFSvQblzVp1ArJVMr31xnxhp2ve9i++o6+8S2YmwuOYrccc1Huo7mO2WUNVd+KIfVrku9dTtVt655nLXe2/U1x5ll4Xd4bL11e8d8i/jrD+Vsxep1z067B/bJtalLnavFkMtaf5OqQb+nQqz9WCtuwjhi645pF2qj75VmHE/9oRzWeWvuuowrNZYvx1CxYtZG8txE7NhjY3ap2+wT9Ukirn21Z7SeVbHvucoQ96LWeovI0ZpviVuzrdyOuYrta83zkM8Cq5aYzxvh+1PitU5q3S1HVMzBIwIAAAAAAAAAAAAAAAAAAAAAAABA28zMjLzqVa+SV73qVfLggw/Kt771Lbn11lvlgQcekHXr1sk222wj2267rRx00EHytKc9TWZmuv9szU477SQf/ehH5ayzzpKvf/3rctttt8ldd90lK1askN12200OPvhg2WuvvQYZ16pVq2TVqlVy6623yrXXXit33HGHPPLII/Irv/Irsscee8jhhx8uy5Yt651nPse0qeMHjwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIiLyxCc+UY466qiJ51m2bJk85znPmXgeEZG99tprXn5waD7HtKkaLXQBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABg+s0sdAEYTh443+fXrfIia8bKiqh+VavM2yotR1G2yzw1VHNhjbnKpXOk1BuqJy+jjETlKF9mmb8Wl7xsMjL6FmXOrMzZOu+ZO71+zLkL1FgU9uy156iZS9cXqj8fu1LVPJvXI3IufNfD6qPnXceqa1S5K87rYa3RQN8+sRcih/U+jnqfz0OOIi/bjNxt8qL5ThllzXdIzFyl1huKVdXcaGvWH3vfbY4zy0J3Avte4JsDEXf9dV9jHKk1xNYyZK4+hqhzkvUNZRI1+tZTsG/HevT7xUe/B4OxE8YTW3+oXco86HtiK1ZE/Va+UB3WXPr6WedSY/lyxMbqOj5f39D6KiJ2vLGx+7YTae7rYvu4uPe56nkn8Xuy2Fp0Dt029f0+hFwNR19z/7pSsRKvjze2cdzq0+fzR0w9jVhRrbrFNvv36o2+irzffmGxKlhYAAAAAAAAAABsvjaWf9NmGscEAAAAAAAAAADi8P0HMJX6/AYOAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAlJmFLgDzJzeOd/nVq7zImjGywtvedTZzHHPlsGIX5fnMk7saszVGK0f1Stc4Pu5WH6OevIwycs6Cv5ai7JsF+obo2MXYOKz5s9ZLXVsRuoLhPlVunWuk5tKqf7x2Pc9mrohYrnauceg+hVox+rqZ681zPULvtdD7wHc+9n08yRzWdYo9P185ily1GRnvm2JU1uB+B7neN13qSWknMle/VXclfk3YT47MGPtc3/i6W33z9vyJhMcVW4szdkJ9k9LlfruYTLJ+a00kxehYn+99YNHvsWCOhPHFjiPULmU+8sAchOqPyWW1Cc2lL3ZqTKu9L4eOlRojZq0kjyOw+04Zj9UndQ5F5vZwMW1dOULPfZH4/Xnq/UDX7ooRWgv6qemqoRVD/DnyHo+uIrJvl7Vrhbb6DPF5JPa+mzJlqffyVv9evQEAAAAAAAAAAAAAAAAAAAAAAAAA2PR1+a0bAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAJDMLXcC0K4pMiiIbNGaWFYPGyx3HUn8JK1djHEXUWLUIzU4V24pZza9vXqoxWuOycvhqNPtE1DPbrowdsTyKsoKsrCgv+44ydy26va9mvT6tuq11rK99jLrOQG49l61xjvWv25RjH0m3WLqG8fFZ19qsP+E6xMYMrTuzFsd18o2VHP4cRa5ijHQNzbvNKHPdaZv5Yt97oXa+e0+obq3TvV2NPfOMfba9+x6S8qzT42rFCozTG7vjM3zoZ/ViMfSeJjpv4BpHx+lRv17bIV2ej7HjTBlHqG1KLH1va8UaoP5QPda89hlnbMwueyGrT2qsmOtk9g3stoeYm+h2nlq67jVjnlXWs1TXo/ePZs7gJ5jFRa8B15wWxtD1WJPXrqcuq49/59Dv/VDHCEaIi+Pt27lnvNh7+EI9vzcFhQz/z2wWg9C9HwAAAAAAAAAATLFcRDYudBETMB9fwAEAAAAAAAAAgMWJ7z+AqZT6uzYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADJZha6AKQriiyqXZYVnXNYPwYX+wtZeVnjKKIG3cIaXSima170HFTjssZh5aheuWoz+5THqxrysvdIdLuq1mY8V8wQXUtR5sxUTl+O0PrKA+cLxyz58jfqLY/n5evqOrXm0jHn8fPtj6XHP76GrLqt2PXxwHXwXWcrZmwtFdf9INRmPnOE3te++1mozXzkKPLy/MiqoXnXGWXtu2zMfEbVEnEvrNvmKqdRfyW0Jtz1NMeeOcbu7mffa1Kfb3qcrXiBcXcR+6xG+PokxRpg3vWajRV6PjZyJI45ZVyxbcPP+/h5CI0nlCumZmt+rb6+mEPF8l3zoWJ1GUfd19hV99nr6b6xdefmDr97TE3v7cbF7mF8dYaExtGek7T+Iu1rqtvk6nEWau8TOxfm9erQJ7RD6HOvqGMEI8TFcfZJ7mFjHwEAAAAAAAAAAAAAAAAAAAAAAAAAmGbd/o1uAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACABPzgEQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAmLiZhS4Ak1MUmfN4lhWdY+bqdegXs3JHDaNA/uqsu/p2TF+8ag70mKtxWPVXOXTs8Ve6PqtPK3bZcyTh66BjFmXfrOyblyFG1mSVdD9XjlSFeYXi286NQ42zfJ2Xr6vrpK/neO26b93GmG8rVuuaj+VoraOIPuP99Dzo8Y+Ljalrielv3QOs98t85EiJbeUYco665ihydX5kzeXc3WeU6TtrZK7A+S5tY+uvdLrHF+47b2bMgzvGsM83PW5n7MBcoC1mXjvH7vjsmuvf/Tc/uzw3U+cidnwp8xBqmyfMSWg8oVwxdYfm2YphHffFGyqWb1z6XJcYvn4i4b1Zl7mx+sbWn6uafOPrOidWu/FnQuoeIPQ8SRlHe078sVzzoK+tbpN3fEQVjn76ms21jVwDnnxWn9AOoNfaDbaIj9Xqk9yjqe+zDMMoimwqr8U0jgkAAAAAAAAAAETKpf+XWYvRNI4JAAAAAAAAAADE4fsPYCp1/7e9AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIs0sdAHTruuPxU3yl6iKIjPPZVmRFEuPLabuXOUfGTn1UavqKp4VR2RuzHp8Vf1W3b7Y1RF7NkO5y9hlpKI8nYUCikhR9s3KvnnZd5T569b9UhTGSPX1jFHV1R6Hu259nVxzqvvqNnq+K612vmtuXcvAmrb6+a5H15i6v28clVZdgfPzkSPmPhGagz5zNFSOIlfjHLmudfMONMrcT43YWupcEeMKtY2pX4u9x7dras5DZsyDP4b7fpT6bHPGzt2xQ2LmbDHrOu7B8nd4xrjjdN9ZpT7nusxZ7DhT5iO2rb4HteJEjKdv/TFzbPUN5fbFTo2pY/lyp46119wYe7Suc+PrF9snl/i5ip3X2KeC3i+O0/u9uk9gnxKjy554UvSaaM9xQqzY97dx3DcvoWva730RJ/a69fmx9KGeZSl0vfzYOwAAAAAAAAAAAAAAAAAAAAAAAABgMZnk7+oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACIiMjMQhcAt7xDnyF+vaooMufxLCui+uu6Y2rKy5yjQI7qrLvCuTi+WNX49Hiquq16fTXquqy2Vu52jXP/O8vC+Zt1StkuUIs5i2G5sUa6xKjqqurJytmsz5ftqznT12l8vdZtdGw177lUsf3Xx3vN1Ry01lPiGhi/Hpm4r3EoZqiWcakx5jOH9f7ocz1S659EjrpdXo5zZL+X86J5JxplzTtrdC7HdYltG6q/bucZR8W6ZwTv+YX9BMmytKek9Wybixf3jOtCzxnC16Nf7O67oa7Pty7XOHYOUuYqtq2+x7TiJIwnlDN0PmbOrRjWcSumr5ahYvlypMYy20fs4VJjhvp561HHc1Vf63zCdbDu9KHPK65+eu/YZS/g0mc8oblxXWvdJldl6j4p97XQtUuN6WsXeor3yR26ctH1R7UyapjAc65PPfDLi2yQz7iLzTSOCQAAAAAAAAAARNpY/k2baRwTAAAAAAAAAACIw/cfwFQa4jdyAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvGYWugAMJw+c7/PrVkWRNV5nWRHVr6opJneucoyMHNXRzHm2GcuMUZ7X40ipN8SqQefOy5GMJDynOmZR9s1U37x8Ocqa/SrWvPhyto9Hh2jRdVnjqc+X/ebmTBrHRRzzqmNHzrsVp1m/v08ldg00zqmV3b62/msZ817tGyMlR+p7MBQ75XpYMRZDjiJv58hG7rnKi1FZg/sun3J/jm0b3c4xjrqPMZ5Kn/tSUbjv0pkxR+F4vieKztHj5jflUuaxe45hfqvTerZ5c3vWu7N9lxyRfVJi54E5SxlXKG/ofMy8d81hxfbFGyqWL4eOlZqz7ufZ+Xad95h1FNs3F/84feNrx4prFzo/fr/We8cue4FYofGE5sZ1rXUbvQ/3rY9mbhU3sp+rhrncce1m81l19bt3xDyZY69pl51E3+dft90LAAAAAAAAAAAAAAAAAAAAAAAAAACbrmH+rXEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPfvAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABM3MxCF4D5kxvHu/zqVVFkjddZViTnDuXNyxwjI7Y+mjna5KpOHasah66/qlfXOB6vFcuoIzSOuZxlu7GRFeX/zFRQHbMo+2ZqVvKiqtUeR6zcKL9wzrxbVV87Vtx49PUaX1cjq42eq1aM9ry72o2zrqn1vrDWoW7vyqfnt32NA+8TzziGihEzV5XYOYs9P55jEvXPZ44iL9uM3G3yonlHGmXuu3pMrtS2qfd8kbnx1H2McVWs+1Lo3tnIWfifLJkxZylc79tUMfO3UIYYX/8ahvs9zi7Pu7qOPK1vl7mL7ZMSW98rzJiB8cXkjK3Lug5D5OgS2zqXGksf96235JwR+6vUmH36mX1UnaE58V0XfYfuOj7XPsu1dxRp7x91+5CU8cyn9ryr8571FbuuY59kvnno/P6OyRt7f4pqpfJ3fLYs5JqAR5Etiv3P4KZxTAAAAAAAAAAAIE4uIhsXuogJ4As3AAAAAAAAAAA2X3z/AUyl4f6NcgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMPMQhcw7Yoik6LIJpojy4pe/X0//Bb7i1h6jDE16bxWrlzFHhmxx49aM17F0jGq+nXdVY2u2sxYRg26vZ1zrudIqrZlzMBSKsq+maiY5ctRwlLMjUtYmLMbpvu261RzVLafq9+eO32tdJvQ/FfzPhL32tD5xmNWoteVsXZ8ferz5jWOq2Vc1/FY/YfMYY5/gOth9V8sc1TkKsbIPRd5MSprcN/FY3JZbUP38JTYdZ/IcWl6zsdZzwOzhiL8VMuM+RzSpPcFi1XM/HflWychem1G9+uQM7ZPbLs8YU5D40wZT6ht6HrE5LLaWLFDMX3nY2Om1tSlj7XPipmzrnPTqX7xz42O6Z3/QM7U97ervX6nzO3/3Of73Ketp0hwjiQ8br0/D/UpEh6TsfNuhdTtfU/TrmsyZjix6yX2ad9lLUxyJ9F1bW6uew8AAAAAAAAAAAAAAAAAAAAAAAAAwOI0uX/zHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAoDSz0AWgv6LIvOezrOgcO1evY38hS9cUU0OVK5QjL2OPPDGrM9bMWDGqunW9vtrMWEYNur2V06Uom2RZKHcZU9Q4OiyFwpjFPLDufOqxl7HzcmCjwLisuROZmz99rfT8huY/V+Mdjc1haF0nryvPWg7l0telfa3VODrkCMWIeZ+H1ndozmJiW21C94qY917X+ofMUeRqnCM9zuadaZTpO3c7Vx0rUHeoXZ8+elx1v1E4V8W6D/meDyFFEfekyzzzvLmJnbM++jxzxlnrLqpvYg0p7WPb6ve7N2bkWEO5Y2oLXZ9J5rD6+mIOFcs37tg+1j5rkvPe53rpfZJu2xqfVUNE3ti5sozvjcL7P7eUu5sVI3Vcruuj9/Cpc1HHCVw/H+vJquv1PR1D+cz3prdX3DMi9qmdeq9PiT2J3OivKLKpnPtpHBMAAAAAAAAAAIiUy3BfYi0m0zgmAAAAAAAAAAAQh+8/gKk0+X9DHQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAbPZmFroATF5RZN7zWVZEx7J+JC70y1muGqy8OocVOy9jjjz1V2esGbBiVPXqGsdr03WZsQI1+HLmZa+R6PrKmFkodzNrJuFrrfvUtQTWUeEJnamuut4qZ14GGbXGVcYp63eNV89fda1G1nldg3nN54pvXwejT+q6csyt1beiY1RzaF3j8RzWe8aqT8cI9XfWF6hfz0Hs3MW0iY1t1RbTZj5y1O3ycpwj6zq175qjzH33js4Z2a5vH5G58dX9jHH6WPcr3/MiVeGY51SZcV3m0xDjGEroOZNCr6Okvh3rSOkX29b1fjZjRo45lDtlHOG9QffzXWNbx33xhorlam+2NfZboTnrMo5gLRHXPFf1dn2fuO56OpauM3VvqveXIvYeMvh8d2aIExrXkFpzph53KdevPf9x7SxR68t8n3Tr12gTON9lLfd9end9/wAAAAAAAAAAAAAAAAAAAAAAAAAAMC0Wz7/lDgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAphY/eAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACZuZqELmHaFiOTFsDFH2bDxisIOmGVxxefqdcwvaem8Vq4qthUzd9Q/UrF0ZN2jitHqVx531WbVZcZSuUM5x/PmZa+R6LZVu2bMSruG+MXjmtfxnCl0H12vrrN6z4xa7cr+5TyM11jF0NdMX6fWeVWD/5pb18G9lq3rEbP2g9fSqFNf46y1+uNjW/WF+sfE8M3zeI5Qbb7Y1vmU8Vv1da2/Sw6rXZGr8yP7zZkXzTvVKNN37bicVjtfW1+fqH65u5+If8wu1n3Nxbp2QyqKzeN3J1PmPZVvfUTHSKwvtX1KH/1eNeMljDs2d0y70LUMxeiTo2tsX81Wn9hYvprMcRh7sS71d+0bmsvcs1/sOjf6qeOK04qh6oi9l7g/G8z+X72HTN0D+STfSwLjc32GDPXRe27ftdTa8x/Xbi5XU5f3R9fcvjrMHLHrKTJenxxdpP4zhsnvajZdeTHZvcpCGfqfQwEAAAAAAAAAgE1ILiIbF7qICejz5R0AAAAAAAAAANi08f0HMJU2j3/THgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALKiZhS4A6fIirf0o656rKJqdsywuuf4xuZhf1qpyWTlSYuZlrJERqzqqp8bq56utqkvXY8ZSuUO1NnOVbUXXV8ZUA8rV9XPl0G20wigrb81eWFW3rnduDsrjZbtqres1XJS5s7F50POor5m+Tq3zRv9xc7HUvLauh3u9hNaXq09KX1f/wnGdMnHXVUnNEbOGrTmJjW3V5osde95Xf3B+B6zfiqHbmedzFWdkX4+8GJX1+H/6M+UZ0PV50bWfSHvMdQzP2GOF7o2VmHv3tIqdoyFY1zq6f49aU/umtK/ei8GYCeOPzR9qF3N9QzEmmaNP7NiYVgyrvzensW9KjRVzfbv29e3tYutszWFEnFYM8V+H2M9C4/tHe6+ZtgdKYV6HDuML9bH26604EWvcCmWNJ+Yah2J0zW3V4IwdeS27/Ch6n3XSyL35bikAAAAAAAAAAAAAAAAAAAAAAAAAAJuZuH/LGgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAoIeZhS4Ak5cX7uOjLD1WUbg7ZZmRpKrBlT8yhxW7iun71a68jDUyYlRH9aisfuO16bqsekI1+NpV+dq5yrai66tq8+fwKYwy89Ys6X72+ap+XbdVb1G2y8p2c2u4OUfFWE1zbVUbNYf6OrXOe65X+vUw2kfkqMdl9K1Ya9T3ntTzqwXfN4FxxdRX0TFC9Y/n6BrbOp9Sv1VfbP2++0FsjorZLh97f4ysepp3rFHmulvH19alzlC/TjFyO4aIPR9dxNxXQ0LPhyENUe/QQtcrKdYA40uNkdJev+eCsQNzk5I71DZlbYRi9cnVNbYV0xdvqFje8aj905D1x+T35jT2dr6csXXqp0nrvCOOnivdxvpMo/u1949zn3vmnsNi9PHvs2KYcxQY35D0tY2ZfzOWuX6aur0/uuW2anDmGCBGaswQay1jYRSSDbJ/WGz0PQcAAAAAAAAAAGxGcun2RdhiN41jAgAAAAAAAAAAcfj+A5hKaf/WNQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAczC13AtCvKvyFlA8XJPYWNEpMURbNDloVHXf3gXOhXt6rYVkzXD9fpmLmqb6RiVa/0sKt+ur2vLmtcOlYoZ6OtmauMqVZZYUx/lvnPu2Jr+lrH0H3y8mVVd1VPPZ56jspxq/G5rotu67t2vhqrOe527dU6k8D1c8xla20G3ltWnTHvyULVa81z19pSYui+k4wde348R+rYY6/buC7zO97ON44iV7FGxv20aN65Rpn7p0Fd94HQfd+6d8Q8L6wYKX0bcXJ3LY3YxhxNgms9TJOY+U6O2XPO+vSP7avfT1GxI+cqpob4Ov3thsg1yRxWbF/MoWL5xqWfsUPW37WvtbeLyRmbSz81WucdcfRc6Tb6M4tuH4o3G2M2SPUZp70v9+85+wjVO5cr3E/Xo/f0+hrHzP9cPn+uuRyqX6f3h19sbmfs2HtfVKt+zwvf5+2hxKaYv10NAAAAAAAAAAAAAAAAAAAAAAAAAABh6f8mNgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQCJ+8AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEzczEIXgHRFZLusR47cSDKKDFoUzYZZZled6xwDxrRi5WWskYpRvdLDtNqP16XrsWrQsayczrZmrmbvkbFKCs/i0THm+riP58ZxHz2OvAxR1VvVV9UyN0dl+7JBtQ7Ha9BtszJmPYdlu2rurOuj59g1Tj2OinVd5sYXXsO+teaqT/fTNep+vvyFWgOZdM9hvT/N915gbqxxDxnbV3vX6xJ7fsgcFV+uIldtR9b1ar5DRpm+Y9v5Y+pw9Qu1j8nZJVYrdu6PXecw5m5axc7LYPk6PGuGipPaR79fonJEzmdKLaG2oef3fOSKyTF0bF+82FhmO8+uO3Uf1WffZfZN3OPF5G3NTei8eu2aM91Gfx7RfUJzMv48De0l9T6yovdCMXzrYTzn3Otw/9b8qT76Gse+j12js+Y1dI1D/a18oT6u3K24MWs52CI+Viv2BLYCm9fuYmEVRTbYfmMxmcYxAQAAAAAAAACASLmIbFzoIiYg9ks/AAAAAAAAAAAwffj+A5hK6f9mNgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQKKZhS4Ak1MEzmcdYuZG0FEgWFG0G2SZO5j+ITrrV7mqmFac8VhWjLyMMVIxqle66nxsHK0+Rj1WDTq3ldPZVs1nO2eXq9ukc+SOa9hor17HjKM+XrYeqSjtOcrK40V5PNw2E/91sq6Pb32Z68ZcA+7xxeSoWLkqrZxGjbH5Rew5jMkRW18ldi5j7iVdY7tydI1t9R+yfqt/Sq66bV7Oycj/5MiL5jtllIV/OjS2Xqt9St/YWKlxvDny/vdbLXQduphEnX35rvVCxEztq98PUTkir0NsLTHtQs/vlFihNsG9Qo8cVuxQTN95HTM5t2e/FRsrNWdM3zqGUV+XnGafQLvWeB01VW2szxm6T+yadu3brb2k/Zmg/33KqlePN2acheoTu+dvX4dwm7kccez3SXqfUO6U+3Wo/i7PDWutxhr+KQ8AAAAAAAAAAAAAAAAAAAAAAAAAwKYl/d/UBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASDSz0AVMu7zIJC+yQWOOsmKQOL4oqRXnKtgoIkCh5iUzxpWr1/pXunQcV6xQjOoa6bmtXrmGY/Ypj1s1hHKP99J5U3PGcM3feK5W+9i4jmNVxHoc1fGy7rxsMSqK8rhqr+d07OUo2FbNs5oz6/qMz0/rmiavgWrc7vbjrFyV6JyO2FZfK3ehVmIm3XOY7/PAXFq1zVdsq40VO7a2mDaxOawaU3IVuYo18r/j86L9u4mjTN9x3TXUORLuW9b9KvXeZ8XpE3NI+jpsqmLmeaFid+nvWu/eHAnXMbaelLpD+89QrJhcC5nD6pu6r+nSRz8XQ3FcsZJzxsxVh7p8OX199Z1et2uNV/znfW3NmAm36XoPVu+3szJmUR535+jzucsao/7cZI23i/B1iGc9zUM5YnKZcxOoKe59ECf2fqqvV4r52EnE/rOGof+ZxDQpJvDPbBaDSe7DAAAAAAAAAADAIrex/Js20zgmAAAAAAAAAAAQh+8/gKmU9m9uAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAdDCz0AUgXV5kSe1HWZGcw+oRmzlXAUYRHYtyXFmg3ryK2SOWFaOaWz1nrijVkMw+Rg0puav/pacvlLMPvb6sq5GyDqs6rfFYc1WUL7OsmbNaT9lYddWaGwXa6rnTuavrU9fuqLNS91HHdexWezUDI8csB9dw5Bpw9bfqjY1RqPozR/1d60uprb1ehontuweZ7+uecxrTJpQjVGNqPSIiRa7ajcLPk7xo3t1GmX5XdaslJUafWKGYlj65NgVDPF8WIlefWHotB3Pl8bli60qpP/RcHjJn31y+811jW8d98VL76OefL44ZIzVnzPVIqMuXKyanvqPrtu19pP/87LH4trM5dU3uduP7q6pP1XZub5qVuZpB9X5yCPrzkbWeXDlDYw5fB6smx/Uw2oZyhHL5+vh3CuH3Qah/TIw6VofH+VA7gCHXGwAAAAAAAAAAAAAAAAAAAAAAAAAAi1nav8kNAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQAT94BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJm5moQvA5OVF5j0/yoroWFZLfwaR3NFxZHQqVL2ZUV+u43WIVcXQffWcueaoOlK1tPpUNXTJXcdQuVLqDLHWh44UWkfWeMb76vG0jpev86yKVR2ffZ2pEoqxGcnKttVa0+urajvXzp3buk6usZl9VOxg+7FxjMTdp2LlqvtH5oypNzaGntsh6/PVFpqbrrF13CFju3KY8zpgjthcwXa5ajcK33PyovnOGWX67u2vJaau1Fipcfrk6qNPnZOoZ2hD1jhELL1Wgznz+Jyx9cW2Cz2TU2KGzsfkmmSOrrG7xLT3RM3jk8wZnCvPrrvrPPv6WXds3ac1DvGfH/9sEGpbGLdC31xY50dlsOrc3N5U7xPL9j1uLa7PP+O52u3D49ZjCl0Hu7Z2u67Xum6XkCs1Z2y/lBh1rA6P2q5P55R7N+ZPUWSbxL4p1TSOCQAAAAAAAAAARMol7ku1Tc00jgkAAAAAAAAAAMTh+w9gKqX9m90AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAdzCx0AdOukMn9sNpQv1aVF5mdIyuiYuhWdsTxvDqXEVvVlxk1jc+zNTdVLB2j6mv1q+bINR/VEV2+7tMldyuGOm/l7MK60jpmaD27zldjs8ZjzW9ejnAk1Ryq42Pti/JYpkbSztlsF3udxsemr5V5bY1x+XM053sk7r6V1JwxfevcgRi6f+F454euR2xs33tQx2jVtQhiV5z3kNC8DpBDx7Luo8ntcsc1H/n75EXzHTTK4p+SrnUsEq4zNk6fmJMQU+diNMm6+8bW6y8pt2O9O9t1qDHUJ+X5HooVW19Mzq65+sTuOle+frqP63nmi+Ebj+6TWp/eFwxRV3AOE3KF5q61j/TcWluxVFtrLlLu6fUeslCvzX1iMHQ0a12Fxi3SHnv0ugrk6nOtrRyh9r68fdZmdIzIa9rn0vf5TBadI7H9wu9qAAAAAAAAAAAAAAAAAAAAAAAAAACYM9Rv5gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJhmFroAdJdHtuvzq1Z5kbljZoW3nz7rjqJz6RxG7LKmzFODnhs9B1aMUL/x+dBzUL3SZVd9qvah3K7rpWPonJWYeXb1c+VqHdcxjHZ1LWO16rFZ46mPqxi5VMfd7cdjFWXbvCjK4+6cVbusjGldJ9eYrHWi+9TtrevnyVGPUV1VPQfmelKxXe9Zq97YGDHvRT3PfWO71mdoXnWMIWJbYw6dt9bCQuWohHKF2jX65KrPyN8nL9xPqVEW+5Sz70cx9abGtPTJtSlInY/FkstaX978eVr+LvXG9rGeyV1yh9qGcsXEmGQO67gV01eL2cfYSaXmdvVJrVM//0Pt++UKC41Hz50+r/f54+1bsVRbPRex12P8OaifnfXeslCvjX1iH9a6Sh13Wk5/Lt81j167Rn/f+8LK23Vtxr0fgk1mY8U1M3IM89yK3/lgSEWRzes+Z75M45gAAAAAAAAAAECkXEQ2LnQRE8AXagAAAAAAAAAAbL74/gOYSn1+CwcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACDKzEIXgMkL/bBbl1+9yousGSMrvO19ZzPjeK46jVTDQtWQeWqo5kCPNRTD6jdbX1bW1exTvdLj0u2r3FZOV14rp86dQl9LVx0i7bkKcbXPy7qrcenxZPp4+bqao7xsMSrK9mMprLmp1lG1flrXoYyZlbPnm+PQNbPWV11/xPvGyjGXq5obte5Ca3nsvLl+QrmtNe+41jpGod4RmbjrC8V21RZ8Xxjj6hM71Hc+rkdKjthcVqzUdo0+ueozirtT5UX7zjvK0n6q1LpvxdSdKvUeGSO1zknUMAmTqNO1XmLpNRps36H+2D7WM7lLvFCb+cgVyjGJ2F1q0c+o1Fi+cSTXmViLL0c4V1p7Vy49d/q83s9bcz2bV9fnn+fQuJ3PwfL/tp/bzZx6n6jpfYyvbaguPW6f0BzoUK3r0SO2laNLLitnbN+Ye4dee60YwQg6Xv9nFz+4DgAAAAAAAAAAAAAAAAAAAAAAAADArO7/djgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAkfvAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABM3MxCFzDtikKkKLKJ5siyolf/PKJN6JexcmOMo4jadAtrtvKiimnEcdSg56YaqzWeKkZKv2rseqzVK12Vbq/rHs9t5e0631a/8Vyars8Xw6LHmpev9bisObOuSzH2Msua9VXrJCuj6vXTug5l1rn27uvqq8e6Xmb7iBxz49O5mudHoufGnXM8b923Na+B3IH+ofwi7fnuUltqXcGaBpizScaudMkRm6trzpi2dZ9cxR7FP8Pywn33HmUxT7KxGiLuY32frUOY9P5hUiZZt7UGYun1F9UncTwp7WOfqTExY/OGcg6Rq0+OrrGtfr5aCmPXGbv38dUa20c/z2Nid60rdKdMGY+eO30+V7dQX/tCtdVzEroeMXfrqke996pelyf0Psq3R5vN2f99rsddt3fETp2D1vUwanNdc7PeyHa+dRZ8f3ftl/DIjm3a5TNP3bdzT1vfZ6u13jC7fvpc78Uq5X0BAAAAAAAAAACmzMbyb9pM45gAAAAAAAAAAEAcvv8AplK/f2scAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgwsxCF4D+iiKLapdlReccuXE89ItZuaO2UaCO6qw1qlx1H3mGX82NHrsejx5HTD/dpxqrHp81HrP92JxV+UP16pgpdGxrPenYodU03toaa51bnS+MflVteRl8NFZFUf7PTJVflJVkZdtq/VTrppWj1d5ew3quQtfLXFcR7xMrVx2jrHsk/n4x+a3c1j3Eur4xdev5HjJ2zHvMWZPjPRhbV2xs3/3YN+aYGDHXPDZXbE5f/lCfIm/XKyKSjeKfWXnhviuOMuvpFRb7bK30ecZuClLnow/reqaw1lVU38SxprSPfU7HxoxpF8oZirHQObrG9vUrjF2mjmXF6JRTx46sISZ2sG9q+4S50231Pt1qXzhumXpOQtcj5a6r9+P18696Xe0ty8Kq/aQ1F77nZnjNGv0C43fF1qH63GPM9R7ZLnWd+frE9J2tJRBA4tfJEJ+jUs3nsxUAAAAAAAAAAAAAAAAAAAAAAAAAgMWk/79RDgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEDCz0AVMu7z8izXJX6AqiizYJsuKpJh6bDH156qOkZFTH7Wqzx3dR6qxHrseZzUOXX/VzzUv1tir8elxVa/0OKz2vvzWmrLmP2YN6jnS1yltZTTbV5HqsVbHy3HV9RlzYc1RPjabozJjUTTPVX2K8nVWtqvWTbVWdA7dPqae0PUaqXYV5/ryrAt/LvX+ctTvW9e+3MH3keMeEztHhapbz3toPnyxu87l+Lm6roFiu+7HoflMvR4xdcTm6pKzTx8RkSJ3zNEo8RlVuO+KoyzlyRwn5hmbKvWZPIkaJsG6Ll241klyjMR5i23vuif2jRnTLjZvKNYQuawYfWKH+pr9HDtJK1Zqbld7s62xo02txddnLldaP28uVbduq/fjofaNc6pt6p405b1WPd90jypndUsZFc0smeqQ9v52H09ZC13nQK+B0NwOkcPK1bfPbA3e07Mxwk3KWJH38sh4Pgv5fM7V/4VLtsnsodJM45gAAAAAAAAAAECUQqbzC6LU/wdGAAAAAAAAAAAwPfj+A5hKk/x9HQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABHhB48AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMA8mFnoAtCUDxCjz69YFUXmPZ9lhfe8Vb+vplzlHBk59FFfpXlRxXKfr8apx6PrH6n2jfxG36qPNa6ql45YtXeNX+e3rkPs+vFdZ123zqTPW8bHocdcj7U6rudSzUVMv1yqc/5YRdkuK9vptWK1HzfX133NQutLvx/Gr4c1FxUrl9nfmJeovpG5ffeF0BxZufV1smpKqWuI8SxEbJ3DvEcm5KiEclVCOWNiptbg7Ju77z/ZKD6GiEhehJ+Uo2yIJ3I/oWfyYhUzv11ZayDYr8dcxvaNfT6mxE6pO5R/iFwLmcPqa7b37BSTY0W29+6vjHp0nz5zbN21hpw73SdXt1/dtz1H5XFHjtBchPakvru23p9XzzXr2ar3T1XdWcRbsgg8kvqshdh9uZ6LlHXWNYeVq08fvb5a/fynVazIe3lCTG0Sz+2F340Ak3XrrbfKd77zHbnjjjvk4Ycfll122UX22GMPOeyww2Tp0qULWtu1114rN910k/z85z8XEZHddttN9ttvPzn44IMHzfPAAw/I1VdfLT//+c/l3nvvlR122EF22203Oeyww+RJT3rSoLnma0wAAAAAAAAAAAAAAAAAAAAAAABACD94BAAAAAAAAGBeXHzxxXLmmWfK6tWrnee32247Oemkk+Q973mP7LDDDvNW1/r16+WDH/yg/NM//ZP85Cc/cbZZuXKlvO51r5O3ve1tvX6U6brrrpP3vOc98tnPflYef/zx1vkttthCjj32WDnttNPk13/91zvnmc8xAQAAAAAAAAAAAAAAAAAAAAAAALH4waMplEe2G3WIXRSZ83iWFd5+rpqs/LnKMTJi66OuynLVaKQa6fHocVR1u2qt+sb2qcZVjceqX49/vI9Vdx0jsp2rLk3XZ7XT17Ya93h7PWYdqaozL9vpGFV/PUfjc1yNPS9bjYqyT+aOVZTtsrKqaq2MjPaNelt93Ws2dY2M99HjqvsG3h92Tse6Ev96sXLHrMeh6i5U3VlrZcbXNeR4rHqHjB2qu9Ilh24bvId73g9WzFBuq31sv0aM3Igxio+h5UXcU3KUxT51N22x89GHdR07xYp4/vXtaz0Xh4gdapeSOxRriFxdc/SJbe9fjOOeXGYsdTy2XaOPqid5PDF7OeP4UHPn6qP32Lqvbl94bseheS6M43V7O3SrjbXH1HvL1r5Sqr1dRDKzhu7rLDatnovYNezKETvf5jozM0X0DQw4Zj5i75NdnuJ9njFdc2Jy8iLr9Exf7BbrmB5++GH5gz/4A7ngggu87dasWSMf/ehH5ZJLLpFzzz1Xnv/850+8tptuukle8pKXyLXXXuttd/PNN8upp54qF110kVxwwQWycuXK5Fzve9/75F3vepesX7/ebLNu3Tq57LLL5LOf/aycccYZ8pd/+ZfJeeZzTAAAAAAAAACARWRj+TdtpnFMAAAAAAAAAAAgDt9/AFOJHzwCAAAAAAAAMDEbN26Uk046ST772c82ju+4445y8MEHyxOf+ET5yU9+Itddd50U5S/M/eIXv5DjjjtOvvjFL8oRRxwxsdruuusued7znie33XZb4/jKlSvlwAMPlKIo5IYbbpCf/OQn9blvf/vbcvTRR8s3vvEN2WmnnaJz/c3f/I381V/9VePY8uXL5VnPepbssssucscdd8h//dd/yWOPPSYiIo8//riccsopkmWZ/MVf/MWiHBMAAAAAAAAAAJhz6623yne+8x2544475OGHH5ZddtlF9thjDznssMNk6dKlC12ebNiwQa699lq54YYb5J577pHHH39ctt56a9ltt91kv/32kwMPPFBmZvh/LQcAAAAAAAAAAMDk8a3UhBXF7N+QsmyYOHlEm1FkrKJoFpVl4UHr/FauXMUeGbHHj1pTlBdVDPf5ahy6/qpWV42pfarx6HFUr1ylRc9B4V8cOo4rf6htaN04z6sxV7msubDmzjdH1nWo3n/V+0bnLMpoWRldr5HxeWhfs2bfun6dw3h/uOaqNWZjXFYu3U/nbPQt6x+1rr4/d8x67Fu31a8Yu/qheY+tqc94QvMcqskXO/b8kDkqoVwVX87U2KF+KX3rGLn7PpaNhnsw50XskzLeKIt5QvtNoq6hWNelU6zAc2/Ifr5naN8csW1jagjF6rNXmI8cXfsWxu7PF8+MpY7Htmv0UfVYbVNj++4OyTnMHbPdN1e3Tx1Dt9efg6p5cdXa6mvVYNQa8z7Ky+dIdYfUz9D6dXl+bs+mnn/G3snVNrZOa/wxbfWcRK/hiNixOax2MX3mchv9vL3S7s+xT9iuz5eUHEOI/WcNQ/8zCaCLU089tfFjR0uXLpUzzzxT/vAP/1CWLVtWH//BD34gr3vd62T16tUiIrJu3To5/vjj5fvf/77ssssug9eV57kcf/zxjR8G2mWXXeScc86Ro48+utH28ssvl5NPPlnuuusuEZn9FxZOOOEEueqqqySL+Id1//mf/ynvfOc7G8f+8A//UP76r/9adthhh/rYPffcI+94xzvkn/7pn+pjp5xyijztaU+TY445ZlGNCQAAAAAAAAAAzLr44ovlzDPPrL/j0Lbbbjs56aST5D3veU/je4H5ctNNN8n/+l//Sy688EL55S9/abZbvny5HHHEEfL6179eTjjhhHmsEAAAAAAAAAAAAJubxftvwwMAAAAAAADYpN1yyy1y1llnNY5ddNFF8sY3vrHxY0ciIgcccIBceeWVcuihh9bH7rvvPjn99NMnUtt5550n11xzTf16u+22k6uvvrr1w0AiIsccc4xcffXVsu2229bHrr76arnwwguDeTZu3Ch//ud/LsXYL5C99a1vlbPPPrv1LzXsuOOO8rGPfUze8pa31MeKopA/+7M/k40bNy6aMQEAAAAAAAAAAJGHH35YXvrSl8qJJ55o/tiRiMiaNWvkox/9qBx00EFyxRVXzFt9GzZskHe9611ywAEHyMc+9jHvjx2JiKxdu1a+8IUv8F0BAAAAAAAAAAAAJo4fPNoEFUXaXx+58ReuMTP/YnOZ7Yqs8efMr/7aMZp/Vv2+GlP7WOPw1W7RcxD756Jz6bbWdfFdYz0PdX+jjuq47mv1q2p2ja2Kkcvs39zx5vuhHTOTYqy9a21Y86n7Wjl0jS6h9WWuM8811v3bObPG35C5u/aNuW+E5r3L/Ov+qX1D44yZq9TYOodP7P04+r4deZ/pEjumb5cYIiJFngX/FlJejHr/LaRJzG3fa9+l3yTXdtf3WCjOfN1DUs+njCO5r/Es8D5rVSxrDlOfBa5naezzz6xB/bkkP2Mjn5+N/aDaF7X3TXo8at9l7DGcfaW5z5uL4d+Lxoxjbp+Y9pmmVbPaO/n2UcFYxvhdYufEau/LkfoZoFWbuOfU//4OfCZz1Dlea8x9K/YzZqfnhPHXVern/L6f9TGrup9N499icvrpp8v69evr1695zWvkuOOOM9svX75czjnnnMaPIX384x+XW265ZdC6Nm7cKKeddlrj2Jlnnil77rmn2WevvfaSM888s3Hsne98p+S5/w7wL//yL/KjH/2ofv2UpzxF/vZv/9bb533ve5885SlPqV//4Ac/kPPOO8/bZz7HBAAAAAAAAABYpAqxv8zalP8W4fdjGzdulJNOOkkuuOCCxvEdd9xRjj76aDnxxBPlGc94hmTZ3Hc3v/jFL+S4446Tq666auL1rV27Vo477jg544wzZMOGDfXxLMvkoIMOkhe84AXyspe9TF70ohfJQQcdJDMzMxOvCQAAAAAAAACATvj+A5hK/OARAAAAAAAAgMGtXbtWLr744saxU045Jdhvv/32k+OPP75+vWHDBjn//PMHre2qq66SW2+9tX692267ySte8Ypgv1e+8pWy22671a9/8pOfyNVXX+3t8y//8i+N129961tliy228PbZYost5M1vfrM3jjafYwIAAAAAAAAAYHN36qmnymc/+9n69dKlS+XDH/6w3H777XLFFVfIpz71Kfn2t78t119/vRx66KF1u3Xr1snxxx8vd95558RqK4pCXvKSlzTq23LLLeVd73qX/L//9//k+9//vnzmM5+R8847T/793/9dvv/978uDDz4ol112mbzkJS8Jfo8BAAAAAAAAAAAA9MUPHgEAAAAAAAAY3BVXXCGPPvpo/frQQw+Vpz71qVF9Tz755MbrSy65ZNDaLr300sbrV73qVbJkyZJgvyVLlrR+RMhX23333Sdf+9rX6tfLli2Tl73sZVE1vvzlL5elS5fWr7/61a/KmjVrzPbzNSYAAAAAAAAAADZ3t9xyi5x11lmNYxdddJG88Y1vlGXLljWOH3DAAXLllVc2fvTovvvuk9NPP31i9f3DP/yDfPrTn65f77LLLnLttdfK6aef3viPIIzbaqut5LjjjpN/+7d/k49//OMTqw0AAAAAAAAAAAAQ4QePNgtFEf8XKzf+4urJGn+xOcx2Rdb6a+VUf+0Ys38ptVr1WX2s9r7aQ3Wn0vHGY+r8seMyc3naVrmsGkL9XKxzuWSSy9zx0FovJJNirH21NlzrQ+es+rZjGOvSM5eh9R+aI99cha6lnrOuuVPG3LWfiD3vOrYV03sdOvYN3t8ixhqK3ec6xNaZ2s6VPzZ2So5QjD6x6ph5FvW3uYidjz5zYl3HLteyS/9Jrt3ovc+A793Ue0Wof5/7UWo/b1/jnm/uvzz3xK511+3K56XruRkb234W+5//neY98Tk5e665BwrtcfQ+q70Pm63ZeT3Kv9g9qa4hZp+o95r1Xqs6btTQ5Z6Sep18+/2uc6JjW3tvX2wrR9f2s3mNz15Gnbpen9jPkKnXtOtn39lcw34WBzY1l19+eeP1s5/97Oi+Rx55pMzMzNSvr7vuOvnFL34xVGm9atNtP/e5z5ltv/CFL8jGjRvr18985jNlm222icrzhCc8QZ7xjGfUrzds2CBf+MIXzPbzNSYAAAAAAAAAADZ3p59+uh7V8hsAAQAASURBVKxfv75+/ZrXvEaOO+44s/3y5cvlnHPOafwY0sc//nG55ZZbBq/tZz/7mZx66qn16y233FK++MUvyv777x8dY/w7GgAAAAAAAAAAAGAS+MEjAAAAAAAAAIO7/vrrG6/H/8vFIStWrJCnPe1pjWM33HDDIHWtW7dObr755saxQw45JLr/YYcd1nh90003yeOPP+5s22cOXLmsOZjPMQEAAAAAAAAAsDlbu3atXHzxxY1jp5xySrDffvvtJ8cff3z9esOGDXL++ecPXZ789V//tTz88MP167/6q7+SAw44YPA8AAAAAAAAAAAAQB/84NGEFZJJPk9/g9RbuP9i5cafP2fW+IuN7a2jyCT3xCrKv3a/5p+vVqs+q09s+6p2V/1Fzz9XDp1L16Xrt66x7/pUMfS5Krd1PWL7ucbWrtv9PrHnOpNCtbfWRWwMs516D3RZX2a/wHthPEY7p/8eE/PejR1z136uvq5r54upY6eMI9S3zxzF1BVzPiVHbK6YnDp3zFp05UjNFxNrSEWeDfY3CYuxtiGvS9dYvudgbK6U9rF19MkdvZ/qec/xtQmNp8/9yrq3d3n+dX321O0inouh2EPtJ8djt3KUc2btR1p7wKL9p2PZ42l+ZtBzVO/pHHNs7svVXLSuQ2u86XthvdfUe8yU/Ujs3saq29e+75xYsRvHJG7tWXt+a/y+dRVbp3UdUj53Rt8jE+M2c/T7LO2uZ5h/VuC6f2JWzPt5U/1bLG688cbG65UrVyb132effRqvf/CDH/SuSUTkRz/6kWzcuLF+vdNOO8kTnvCE6P5PeMITZIcddqhfb9y4UX784x872+qaJzUH8zkmAAAAAAAAAMAitnGK/xaJK664Qh599NH69aGHHipPfepTo/qefPLJjdeXXHLJoLU99NBDjR9RWrFihbz5zW8eNAcAAAAAAAAAAPNuob+j2Ay+/wAWAj94BAAAAAAAAGBQa9askTVr1jSO7b777kkxdPubbrqpd10iIjfffLM3T4zY2vrmmq88KbkAAAAAAAAAANicXX755Y3Xz372s6P7HnnkkTIzM1O/vu666+QXv/jFUKXJhRdeKA8//HD9+vd///dlm222GSw+AAAAAAAAAAAAMJSZcBNsKnLJotqNpEiOXRhdsriUkjvrsHI1g2aZO7mO6YqXq1gjFUtH1sPJi6qfXaeur6pL15Pafja/v/5YOo4rf0XPv+vaudpVqvGN96vGVvXJyzatOVL1VuMN9Rvvq6vS8169T0bloq7WsDXXxVjErMySq8swUjHa6yxT/cPXNbRe6r6x/RzXq1Vn4L1Xz51xDxnvb+W31nCobt/at/rqedcxK755CM2j1de6b/nODxXbqj0mR0osVztf29Qa+tSVGsela+w+ijzyobrIxczvfMf0PQeHypnSPlRPbKyYdn1zTTJHzHUpjP1takzfOHQfM3ZCLal1hPZjzj5WDqNOq73e1/hitedKxZK4uXSlrGIH96a6BmcGN2u/GGrf5dkVfF8ktB9qTnQ7a5/vzBHZbi6XJ7Yn72zfuPeJT+x9NCXmXOwOnRo5p+N5j02L/iGcGDvuuKPstNNOvfI+8MADjddbbbWVrFixIimGruHBBx/sVVNF19ZlrLG19c01X3lScgEAAAAAAAAAsDm7/vrrG68PPfTQ6L4rVqyQpz3taXLdddfVx2644QbZeeedB6nty1/+cuP18573vEHiAgAAAAAAAAAAAEPjB48AAAAAAACAKXX88ccn9znttNPk3e9+d6+84//1YBGR5cuXJ8fQfR566KFeNVXms7a+ueYrT0ouAAAAAAAAAAA2ZzfeeGPj9cqVK5P677PPPo0fPPrBD34gz33ucwep7Zvf/GbjdfVjTGvXrpVLL71ULrjgArnhhhvkjjvukC222EJ22GEHOfjgg+V5z3uevPSlL5VtttlmkDoAAAAAAAAAAACAEH7waMLyYvZvCKNsmDi5xAcaib/4wjidRaTIW7msHM1gWeZOquO5YuYq1kjFql7p8l3XsLoeVn3W+GLbj/eZq2OYReDKpevSbfR5S9Vu/DpVsVrjqTvN9tHXI7eOq37jbaqWuq+rrtnjUh5XtTlyF+XKyETXWdXQ7NuuzerfHsdcff71b81tzPsm+H4w5kzfQ1z3CbNv15yO9Rfbt1D1WvOv4/lihvqG+o1fn66xQ/375LBiWXG61KdrqIRq8eWKzdkn9pA5NiWxz4CFztHnOZmaP6V9bF2hmEPm7JOrb2xff33Pjo1txUwZhxnbqKm1d/KNy4wd2S7i2uu5s/pYn018c9+eK3Ve/HNZ9Xelrs6F5qJVQ6BGn+pZUz8HqxjVvV3vI8vzVYYu9xrrqWFeJ1eMxDmx2sXE9rX1tvc8HlPnIFRDqJYuseZiRod05Jj8M7POFVnnUP9MYhoVRTbY5+zFZD72bjH0D/BsueWWyTH0D/DomF3NZ219c81XnpRcAAAAAAAAAIBFLBeRjQtdxATEftk3YWvWrJE1a9Y0ju2+++5JMXT7m266qXddIiIPPPCA3HzzzfXrZcuWyd577y1f/epX5eSTT5Zbb7210f6xxx6TBx98UH7yk5/IxRdfLO94xzvkXe96l/zpn/7pIPUAAAAAAAAAADAYvv8AppL1GzMAAAAAAAAAMIgs5texB+jTxXzWltpvvvL0yQUAAAAAAAAAwObigQceaLzeaqutZMWKFUkxdtppp8brBx98sG9ZIiJy1113NV7vuuuucskll8hzn/vc1o8dudx3333y5je/WV75ylfKhg0bBqkJAAAAAAAAAAAAsMwsdAEAAAAAAAAAJuOyyy6TlStXJvXZcccde+fdeuutG6/Xrl2bHEP30TG7ms/att56a7n//vs750rJ4+s3ZC4AAAAAAAAAABbazTffnNxnxx13bP3YUKqHH3648Xr58uXJMXSfhx56qFdNFf1jTA8//LC84hWvkDyf/c9D77HHHvKGN7xBjjjiCNl+++1lzZo1ctVVV8nf//3fy09/+tO63yc/+UnZeeed5QMf+MAgdQEAAAAAAAAAAAAu/ODRJiQvuvUb9fiPs+fi7jwSfzGF57T1H4vPjfajVuxmgCyzk+mYOlZexhqpGDqiq+Tqeuj5rerTdVW1WONxjSNUfyxrbsfzW21b5wPrsJqP8X7V2Oo5sOZIXY9MH1fxfH2t41VdeRm8WsvVmtXrMx8bRx1DrYisjGGtiVYNZf/M8T7K1Xy31mZgfdX91GvX9bByWzmt/tV9wnVfCPbtmDOmrzVOa/6teDH19BlHqF5fXTH9U3JUQrkqvpxd++haYuoK5YzNncLKkWLIekKGqHcoQ9biWy9D1xDbJ6Wm2Jgx7UJ5QzEmmSPYz9hvxtRlxW7vX9JzWPvgPrGtPVmXubPmzepj7eF8869j6T2+nqPYuUmas/K1Lt+MbUae2x/pZ6vea+bV/bm1h5sV884N3eFT6g/Nq5VLtwvt87u0tddVWJdr6KslJUY7ZmIHse8RyXHmbzuAzdjKlSvlwAMPnPe8i/kHePjBo365AAAAAAAAAABYaMcff3xyn9NOO03e/e5398qrf/Boyy23TI6hf/BIx+xK/+DRvffeW//vE088Uc4999xW7kMOOUTe+MY3yqte9Sq56KKL6uMf/OAH5bjjjpMjjzxykNoAAAAAAAAAAAAArevvpwAAAAAAAACA0xOf+MTG60cffVQeeeSRpBh333134/WTnvSkvmWJSLu2e+65JzlGbG19c81XnpRcAAAAAAAAAABgVmb9F0AH7hMjz93/aZhnPetZcv7557d+7Kiy5ZZbyvnnny/PetazGsff+973Dl4jAAAAAAAAAAAAUJlZ6AIweXkR33YU+T1qLu6GIwknK4wm1ne4+itY/StdRdHumGXuJFUsHSNXMUaqf/XKVWI1v3rudF1VTdZ4Ysbh/jo6niuHFVu31etIz3BmtBMRGZVn9XiqHHl5vJqL6npU10FXrfvF9NXHK9VartauXp/j69KKUVTjK2NYa0L3LxwrKlMza+Y01lfdr/y/rl+1C/YNvR8K9/V03Rf0PcHs2zHneN/UOdLXzaolpR6rllC/mHpj58jqH9smJpcVzxcztYY+dcXmdkmpp6+YejZFkxiX632ZoktNsX1SaouNGWo3ZM4+ufrW6XoOhmJbMVPbe/sYdbX3RvGxrT1cSkwR/5xZfdt7uC5zpdqqGKFxFMbx2Vj+GPqurGOk7I99+yNvv9YeLp15fYz2rvVpzWtqjj6fCeZyufv75iZ1DkI1xPZvxkpoLPb9ICnG/G0t0EFRTOeeLHWtT8r2228v2267rdx///31sZ/97Gey//77R8e47bbbGq/33XffQWrTcXSeGLG17bvvvnL99dd3zpWSx9dvyFwAAAAAAAAAgEUsl/7/j4aL0SIZ09Zbb914vXbt2uQYuo+O2ZUV5wMf+IDMzPj/38VnZmbkzDPPlCOPPLI+9vnPf17uvvtu2WmnnQapDwAAAAAAAACAzvj+A5hK/OARAAAAAAAAgMHtv//+cvXVV9evb7755qQfPLrlllta8YbwlKc8RZYsWSIbN24UEZG7775bHnroIdlmm22i+v/yl7+Ue++9t369ZMkS88eB9t9/f7n00kvr1zfffHNSrbFzMJ9jAgAAAAAAAABgoV122WWycuXKpD477rhj77yb2g8e7bHHHvLbv/3bUf2POOII2XvvvRvfTXz1q1+VE088cZD6AAAAAAAAAAAAgHH84NGEFcXs33zKsu5980Cto0DsXNwNRhKeBD1P1jj0D9WNnLGanbOsGTwUI1f9R2V/1yiqlnru9FxVNVm1dBlHiO6vc/r66PGEMuvzrsx17HIco8BxHbu6LvX1GBtfqO/cdcoa7ao5rdauXqvj67Jak7qOuVzlNS5jVHOo14LV3xVD96m0cgfWV6Ovzmn0DdUbsz7tee2fszXWjnNkzXlKPV1rcfUNxQ7VNGSO2FxW3pjYfe5zep4rMXXG1mNJvR9v6mLnpQ/reqboU2dq35R6Q7Fjc8fk7JtriBxWjMLYJ8bENmMmtvf2UfUNUYu159Jtu8yZ1cfaz1ux/HOl2gbmSMfSe7i5OPExrON6bmPeR3P7vqpT8/mm94l6f5nyPAy9l2LXhiuWzm7OWSB2zGeCdi2qnSeGrzZf/uh7YkSb2M/i1ufYqL4T3BIM9c8S5vufSQDjDjrooMYPHq1evVp+93d/N6rvI488It/73vda8YawxRZbyD777CM//vGPG7UdffTRUf3HxyQisu+++8oWW2zhbKtrXr16dVKtX//6173xKvM5JgAAAAAAAAAAFtrKlSvlwAMPnPe8T3ziExuvH330UXnkkUdkxYoV0THuvvvuxusnPelJQ5TmjHPIIYckxfit3/qtxg8e3XjjjX3LAgAAAAAAAAAAAJxcv7ECAAAAAAAAAL0cc8wxjddf+cpXovt+7Wtfkw0bNtSvDz74YNl5552HKq1Xbbrtsccea7Z93vOeJ0uWLKlff/vb35aHHnooKs9DDz0k1157bf16ZmZGnve855nt52tMAAAAAAAAAABsrrbffnvZdtttG8d+9rOfJcW47bbbGq/33Xff3nWJiOyxxx6t/5jBLrvskhRj1113bby+7777etcFAAAAAAAAAAAAuPCDR1OoKOL+usgL91+wn2TmX+w47Njtv3asrPEXitE6X2SSO/qJiBTlX7uPe26sWrqMI/RnjdMXU9etx1fNhfWn+xWOuahzGeOs58CYd9/1sPpa16l9Hez1qdeiPfZMirG+1vvFv66yxl/sHITWwOwYO75PPPXq/u2c7vd7n5xd+1rt9Zx3mXdLn3H0iR0bI2bdjOeKyZkau2v7mDpT6o2Vej/uO6aFrntIQ16frnV2GWNsvSmxh3zPDZXL6hdzvzVjGPfUlNix44kZR6uPekYNUUvoWRuMGXgOufdHxt6ztS8x9i+ezwuhOYrdd1nz4ouhj+sYoX2vq63ei4aurY4T2g87r09kTa4cek50GyuXzmGdd415Lod7H2t/BrLnIfazjiXUfzZG7GfIuM+kdXvjc3DMZ+FQjUN/ZkeamPfzpvq3WDz/+c+X5cuX169Xr14tP/zhD6P6nnPOOY3XJ5xwwpClteL967/+q2zcuDHYb+PGjfLJT34yurYddthBjjjiiPr1448/Lueff35Ujeedd56sX7++fv3bv/3bst1225nt52tMAAAAAAAAAIBFLBeRjVP4Z31JuAD233//xuubb745qf8tt9zijdfVkiVL5ClPeUrjmP4BpBDd/rHHHutdFwAAAAAAAAAAvfH9BzCV+MEjAAAAAAAAAIPbaqutZNWqVY1j73//+4P9fvzjH8ull15av56ZmZGXvexlg9Z25JFHyl577VW/vv3221s/+uPyyU9+Un7+85/Xr/fZZx85/PDDvX1e9apXNV5/6EMfknXr1nn7rFu3Tv7u7/6ucezVr361t898jgkAAAAAAAAAgM3VQQcd1Hi9evXq6L6PPPKIfO973/PG6+PpT3964/UDDzyQ1F+333777XtWBAAAAAAAAAAAALjxg0cAAAAAAAAAJuLd7363LF26tH59zjnnyKc//Wmz/WOPPSYnn3yyPP744/Wx1772tbLPPvt482RZ1vj7yle+4m2/ZMkSOf300xvH3va2t8lPf/pTs89Pf/pTeetb39o49t73vldGI/8/Yn31q1/d+C8q/+hHP5J3vOMd3j5vf/vb5Uc/+lH9+oADDpCXv/zl3j7zOSYAAAAAAAAAADZXxxxzTON16DuJcV/72tdkw4YN9euDDz5Ydt5556FKkxe84AWN1zfccENS/+uvv77x+slPfnLvmgAAAAAAAAAAAAAX/s2VCcslm9hfX0UR/xccZ+H+ixE7rpSacvXXjpU1/qz+reNF1vqrY6q/uT7+ObFqcI3Dqiu1n2vsur72OJrjNXM72lWxrDmo6jOvRxnTqin3XUvjOrWOm2uhvTatNdjOlUnhWNN6HnzrKhwr0C9hfcX2DeX09g2832OvY1LOwNz45ig076kx+4wjNnbMdfGNOaVdzNrtGttqn9I3pt7U+ofgG9Ok/+bDJOa473i69IutOyV26nsrNs4k7hkx4wk9s6x7qPfZ1LGubs9B/XzvFzvlmRr7vI9Zh+Ye04jVrs3eW4fmyNpnteoXey8au1ezYujzrmsQu5e2cqfGiWlrzaVvL63bhnJZc6TPu+4V9rpq1mLV5KqtFSt0nwr2j/+MGPuZs8/n2qE+W8dYiH9+APSx9957y5vf/ObGsVWrVslHPvKRxo8aiYjceOONctRRR8nVV19dH9t+++3ltNNOm0htL3/5y+W3fuu36tdr1qyRww47TD7/+c+32l5xxRVy6KGHyv33318fO+yww+Skk04K5lmyZIl84AMfkCybez+eeeaZ8kd/9Edy3333Ndree++98od/+IfyoQ99qD6WZZl88IMflCVLliyaMQEAAAAAAAAAsLl6/vOfL8uXL69fr169Wn74wx9G9T3nnHMar0844YQhS5MXvvCFssUWW9Sv/+u//kvWrFkT1ff++++Xb37zm41jRx555KD1AQAAAAAAAAAAABV+8AgAAAAAAADAxLzvfe+TY489tn69fv16edOb3iS/+qu/Kscee6y8+MUvlt/4jd+QAw88sPFjR8uWLZNLL71Udtlll4nUNRqN5NJLL5Xdd9+9PnbnnXfK85//fNlvv/3khBNOkOOPP1723XdfOeaYY+Suu+6q2+25555yySWXNH7EyOeFL3yhvPe9720c+z//5//Ir/7qr8pznvMceelLXyrPfvazZffdd5ePfexjjXbve9/7Wv+16MUwJgAAAAAAAAAANkdbbbWVrFq1qnHs/e9/f7Dfj3/8Y7n00kvr1zMzM/Kyl71s0Nq22WabRm3r1q2Tj3zkI1F9P/KRj8hjjz1Wv95jjz3koIMOGrQ+AAAAAAAAAAAAoDKz0AWgu1y6/ctHIymS+xRGl9C//5R7Uo2Mvta4dN26Jl8teStWU1FkZYxm0FA/EZG87DtSfatXuqxqTvT4qxrG6XqsukJcsXU9ddvW+WbfUO5qjsb76bmp50Dc817Vm5fHdcwqnmtUsX3FOG6tBZG5tVmtxWoN6rXXilmNU4x5cAzEXlfNxu2YRr+I9VVdW+v9YfYzckb1VXOq+1m5xlljTanT10/Edw27xfS9P2LqiTk/RI7UdjqvL7eOXYnJMURfi2uNjQuNZ9qF5qcP37NqPmLEji02R0otQ+YOtQnl8u8ZAn0De1Mrti9u1/Hofr59c2xdZjtPfa06rHolrp1vb23Nvz1Hql3CHLXmxsip56bepzlqqmKE9p7ta+s/P656TtR7HrVvFPVcr5/zqr+V2yd1/bsudercpLabzRGuw1WLlculy3tptl84dupnZN97qk8dUbk7fp7HsArJBtmDLDahZ/JCWLJkiXzqU5+S173udXLhhRfWx++++265/PLLnX122mknOffccyf+Xw7eZZdd5Atf+IK85CUvkeuuu64+ftNNN8lNN93k7POMZzxDLrzwQtl5552Tcr3jHe+QLMvktNNOk/Xr14uIyNq1a+UrX/mKs/3SpUvljDPOkL/8y79MyjOfYwIAAAAAAAAALDIby79ps8jG9O53v1suuOCC+p/3n3POOXLCCSfIi170Imf7xx57TE4++WR5/PHH62Ovfe1rZZ999vHm0f+Rgi9/+cvy7Gc/29vnjDPOkIsuuqjO9Td/8zfyvOc9Tw499FCzz+rVq1v/4Ya3v/3t/EcSAAAAAAAAAACLA99/AFPJ9fstAAAAAAAAADCYrbfeWi644AK56KKL5JBDDjHbbbfddvL6179err/+ejnmmGPmpbb99ttPrrnmGvnbv/1b2Xvvvc12++yzj/zt3/6tfOMb35CVK1d2yvX2t79drrnmGjnuuONk2bJlzjbLli2T4447Tr75zW/KKaec0inPfI4JAAAAAAAAAIDNzd577y1vfvObG8dWrVolH/nIRxo/aiQicuONN8pRRx0lV199dX1s++23l9NOO20ite21116N/5jCunXr5Oijj5aPfvSj9Q80VTZs2CBnn322HH300Y26f/M3f1NOPvnkidQHAAAAAAAAAAAAiIjMLHQBm4OiSO8zyf8oSi7h4COJK9oaW0z9udF3ZPTVdesaXbVYdeR1jKaiaHbIsmbQfOx/67656jsq++qysrp987hr3Loeq67YfhXX3OtDejy5Om/NlWtuq1jVnOg5GJVH8vJ81bfKoY+7atY5rL52Le7+7jGWbaVqq9sZMct+mZrt8euh14G1rip2TH+/2brLvp51LmK/T1r9HHPWqtfM2ZxT3b7iWvt6nlPrtPq5+nadb6uWvn1jzo/ncK2D8RiVUK5Qu5TcQ+Sw+naJEeJa5z6x455vqeMYUugZNR+xuow/NldKTaE6QrFicvXNETNXhbG37BPb6mv1Mdt79r2xOezYcfGcMY26rPFZ++aUWNa+PWWO2uPw57TiuGLE7j1T5t2qo7VvrWIY9+z6WaaOu54vsfWE5tLXNnYOQu386yquFitXqDZfn7m+/vMxn2vrtomP4y6f4Vs5E+ob2hD1A5O0atUqWbVqldx6661y7bXXyh133CGPPPKI/Mqv/Irssccecvjhh5s/BORT9Fz8S5culVNPPVVOPfVU+fa3vy0//vGP5Y477hARkV133VX2228/eeYzn9krR+Xggw+Wyy67TO6//365+uqr5ec//7ncd999sv3228tuu+0mhx12mGy77ba988znmAAAAAAAAAAA2Ny8733vkxtuuEE+97nPiYjI+vXr5U1vepOcccYZ8oxnPEO22WYbueWWW+Taa69tfI+xbNkyufTSS2WXXXaZWG3vec975Ec/+pFcdNFFIiLy8MMPy5/8yZ/IO97xDjnkkENku+22kzVr1sg3vvENeeCBBxp9d9ttN/m///f/dvq+BgAAAAAAAAAAAIjFDx4BAAAAAAAAmFd77bWX7LXXXgtdhtMzn/nMefkhoG233Vb++3//7xPPIzJ/YwIAAAAAAAAAYHOxZMkS+dSnPiWve93r5MILL6yP33333XL55Zc7++y0005y7rnnypFHHjnR2rIsk3/913+V7bbbTs4+++z6+AMPPGDWJiLym7/5m3LppZfKrrvuOtH6AAAAAAAAAAAAAH7waJHq+R+lb8iy9D65+DuNxF+gr/5QPbnqOzLa6xpdNek6dO5ctR+1+jc7ZNlcwFDfvOw7yppFVK/0sPS4Z/u2j7nqCnHF1vXMtZ2NrccXylmdr+ZovH81N3pOdMQqRl6eb81p3dA9t64cum+oFqv/eH1zYyzbqlms1l213nTMQo08G+sfWv/2urJjjvdz9lXj0vTcxfbz1mv0Db2vx9dhq28gV2o/X53VfOt5jq2lz5xVWjUFzo/HrrjGHFunzhlqG5vblyM2V0yMPvFS6HFvTlKfVZOM1eU6xOZMqS1Ux5A5++by9dfPnNjYMdchta/Z3lNjbA47dlw8Z0yjLt3O2rv55t6eI6O9xI3XFdvaP87Fdses2vnuutb8hubdt9/VRtVz3Ngv6r2mtU8c4v0fmktnfVVfY95bORLmKrae2FxpfUP9UuY7rl2fz90p9cQY8p8BIF4h2eDXcjEIPasBAAAAAAAAAMAUy8X+Um5TtkjHtPXWW8sFF1wgq1atkg9+8IPyjW98w9luu+22k5NOOklOP/102XHHHeelti222EL+8R//UU488UR5//vfL1/60pdk48aNzrYHHXSQ/Pmf/7m84hWvkCVLlsxLfQAAAAAAAAAAROP7D2Aq8YNHAAAAAAAAAAAAAAAAAAAAANDBqlWrZNWqVXLrrbfKtddeK3fccYc88sgj8iu/8iuyxx57yOGHHy7Lli1LjlsM8F9OOeqoo+Soo46Se+65R77xjW/InXfeKffee69ss802svPOO8thhx0mT37yk3vnAQAAAAAAAAAAAFIsyh88Wrt2rfzwhz+U2267Te644w556KGHZP369fKEJzxBtt9+eznooIPkwAMPlJmZRVk+AAAAAAAAAABAC99/AAAAAAAAANNrr732kr322muhy3Dacccd5Xd/93cXugwAU4rvPwAAAAAAAAAAqRbNPzH+53/+Z/nSl74k11xzjfzkJz+RPM+97bfeemt58YtfLG9605vk13/91+enyE1Uyn/gJcvi2uXibzgSO6lVj5U7N9qPVHtXTboOnVvn1KtupF4XxVyHLGsGs/rmRTPJqOynh+UavjX2uVhx7Sq+ZlWdehyFqt96Z1bj1e1FRPJyzK35rHOXMYzrMSpjtuZ8LFc1r5k6V+Ws+tYxjVp0/3H1tVP1VGvPWm9ZfZ2yRpy53GPrSvQYq9yijrvXlY6p43nrUDGtNe57X5h9O+d0z+1439hcffpZfQr1zm1fv/C4dcyYenw1TSJHKE5q2/HcFauGmFyxOVPjDRF72sTM1ULmcN27h84b2y6lllDMmJyhfKEYMfXqe15sjj61WX3NXEaNKTns2D1iGnXpdtaezjf39hypdkPMTeB8NUet/WP52jW80B40uDc156xtbp83+39H5ZF6r1rFbj3vdf8e9xrjuC9m6v7cfi/G1eKrx/osEPu+afbxnJTwZ866XcIjOvU/fhpbw5A5AQDAdOL7DwAAAAAAAAAAMG34/gMAAAAAAAAA0If+3YoF8//9f/+ffPKTn5Sbbrop+A+7RUQefvhh+cQnPiG/8Ru/IW9961tlw4YN81AlAAAAAAAAAABAPL7/AAAAAAAAAAAA04bvPwAAAAAAAAAAfcwsdAGWrbbaSvbZZx/Zfffd5QlPeILkeS5r1qyR73//+3LXXXfV7TZu3Ch/93d/Jz/96U/l4osvliVLlixg1W15+Tef+vyKVVH4z2dZXJxc7IYjcSexcls5c9V+5Gin69C5dU6dS1+78bktimbjLGsGq/rq65GrfqOyn2/qrdnUc2DRzXQNIu2x6vGF1rFvrlptytgjPWdF1Xf2fDWn9VyW/fLy+KjRtxlTj7Aaj46pa6kqcs25ztGOWZ431lm1vnScRttq7OKem4pe71ZMK954n7mYuu7m+Op+Ko7rWpt9PWP353TPbUyuus7Y8Y31i+1Tnzevn+eah+4lHecspU1ormJr7do2pQYfnTM2d5/Yk8i1EGLHt9hyup5nQ+dOrTOmptiYoXZD5LJiFJ69XO/YEXWn9rX2nr5cOocdO7IGx3FrHnVba0/nuw72HKl2kXPjW0+hvaS1T7Rijh+35je4N1VFxdx9ffu8Ruy6Q/w+MZTTzJWw5w7tz+21GV9TbD2x7xd3XyN35MzGfAYKfa7tmnuIXCmG+mcJ8/3PJDYlRZEtyD5o0qZxTAAwbabl+w8AAAAAAAAsQrmIbFzoIiaAL70AYNHj+w8AAAAAAABMDN9/AFNp0fzg0YoVK+RFL3qRHHvssXLYYYfJQQcdJKOR+6d7vvGNb8g73/lOufLKK+tjl112mZx55pnyF3/xF/NVMgAAAAAAAAAAgBfffwAAAAAAAAAAgGnD9x8AAAAAAAAAgD4WzQ8eXX/99bJ06dKotocccoh8/vOfl1e/+tXyyU9+sj7+13/91/Knf/qnssUWW0yqzE1C6g+5ub9WcCsK//ksC8fIxd1oJO7gOqeVI3d0H6m2OrfOGco1Prd63oqi2TjLilYfV79c9Rtl7YEEpj2aziXSrk+Po33en6Oas8ZclTFbc1Ier8ZcZa6u5Uia/XSN+djx1vWoYzVzFIU7Zquda2yRMat1Zq2veo4c12Muv1pPouut2os63qytzu1437VjGn2NtV33q2pvZQjPd8XK2c411689v4E6A+PT7WP6WLmq+Y6d45h6us5ZSt0pdep4vrwpuXUNLqG6Qrlja+jCyrU5msRc+NaFT5daYvuk1BSKGTofk6tvDNfzIjZHMHbHft6+Rr2t/Ywnhx07sp3juDWPuq1r/+rq76/ffVzPTWz9vjtjq/5WLZk3Zpfr0L6W6nygxnF6n6f3nNXestq71nvNiH1iLKu+2PUW09ZeV3G1uHKE6rLbmynG+lprM9w3Nkco1xCxw7kBAMDmjO8/AAAAAAAAAADAtOH7DwAAAAAAAABAHym/dTNRsf+wuzIajeTv//7vZcWKFfWxBx98UL785S8PXRoAAAAAAAAAAEAnfP8BAAAAAAAAAACmDd9/AAAAAAAAAAD6mFnoAvp4whOeIEcccYRcccUV9bGbb755AStqy4vZv0kaZf365ym5AucLY6xZRI25NBuNxB0sJYeeez1XoZw613gOPW96booiK/s0g4T65UV7IKOs2yJyxXLVIDJXr9XGmvd2nNn/65qrkTUn5fFqnLrqqra8PO9ah3W9KpaVw4qp2zXqKP9vZrTV19xaX645suqcy13GFj2uMrZe2+p6usdjxfT3jVnb1vuhEloDoVyz+ULv37j1FpXLsy58fWPnOLUeX016rn0xYnNYNfryWjG7tu9aV0oNLil1bW5i5q8L69mVokttsX1i60upIdQ2JmffGEXr6RsXNyq2tR/x9DP7JNbZLUe32NYcutrqParV11+/kUvFiq/f1qq/VUvmjenrb+1BW8cLd50p94zW3k3FXhIIFdoP+Pq0jhvtXdcrtm17XcXXlFKPv71xotE3dL/y9x8iR5eYdq750/WfLUz6n0lsyvIiG2TvsdhM45gAYHO0KXz/AQAAAAAAgEUol/n9Emu+TOOYAGAzxPcfAAAAAAAA6ITvP4CpFPr9mkVvu+22a7x+6KGHFqgSAAAAAAAAAACAYfD9BwAAAAAAAAAAmDZ8/wEAAAAAAAAAEJmCHzy67bbbGq933XXXBaoEAAAAAAAAAABgGHz/AQAAAAAAAAAApg3ffwAAAAAAAAAARERmFrqAPn784x/LNddcU7/Oskz+23/7bwtY0cLIi279RlmHXFasQL/CU2Nm1JGL+8RI3MFcOXRsPVd6DnROnWs8Ryt2q86qT7NhljVjWv0abYoOF8sVx3FM16fb6Hm1rkulmjPfXFU583Iu9JirrtX1al+n8vhY7a15Lc+NyuOZcdyqRbdz1ReK2b7WZTs1R673gJW/UPOflbGCa3tsrqyYmfHeMmsxxikSXtfmHHXINZezOb99c/n6VGLrDM1xTD3W2H1rNTZGzPzG5tIxK6HYqe1ddWkxdYbouiwp9S5mseMdwhDPtq71pvRLrTMmdmz+UO6YOFYM/TwZNHagr29cVl9r/2G275SjW2zfXOq2+nlt9bXn1kzVmqP4+uNyz+bQ9WRRMXV/V23WuWrOQnVan1OcAs/cel8Y2CemsOqLXY/etq11pc/H5+hS12x79/HQZweR8OdZ37qPzREby84xnK6f3wEAwOaH7z8AAAAAAAAAAMC04fsPAAAAAAAAAEAl9Ds1i9add94pJ554omzcuLE+tmrVKtlzzz0XrigAAAAAAAAAAIAe+P4DAAAAAAAAAABMG77/AAAAAAAAAACMm1noAmJt2LBB7r//frnxxhvlP//zP+Xss8+WX/7yl/X5vffeWz7ykY8sYIVuRfk3hGygOJU8obBRIHke6u85Vxh1ZEbOXM3EyDPDOraOqedAj9OXKxhb1VLNQVE0G2ZZM5BrLrv+Mpl1XXQNrrZ6fHourLmr2rnmKi/b6LnIqzkoX4/0nBRVv9nzes5csaocuYqZGcfrXNV4VLtxVZ+qpxWzqql9jZtz5HoPVOtJ59f1FtWciDFnjveRNfZCXeN2TKOfY46sda3XsjlHvXK112CXXL4+sXXqftb16hMz1C8lRuj+pHON8+WNyR2qJaVvxVWnSLjWLlz1dtVnjhYj6zp00XXMXfrF1p0SO9Q2lDMmVzBGYEfpy9G1vj7j0vuPrrn8ObrF9s2lbqv3TVbfVg7P7SB2bsw5CeSezeFW77tac+KO6fusUJ1r1x1XZ8p+t7q/1n3Us7PKucT8HNLk26OHPh+Z6yyirfU5LuaaxubwtZ1t7z5urctmXf7zvnUfmyM2Vjt2dymfr4cSm3IBSttkFMWms6dKkbr2AQALb1P9/gMAAAAAAACLUC4iG4OtNj19vswDACwIvv8AAAAAAADAYPj+A5hKi/YHj97ylrfIWWedFdX2Oc95jvzrv/6r7LTTToPWcPfdd8s999yT1Ofmm28etAYAAAAAAAAAADA9+P4DAAAAAAAAAABMG77/AAAAAAAAAACkWLQ/eBTjRS96kbzhDW+Qo48+eiLx/+Ef/kFOP/30icTuohgoTtahTx5IPgoE9f243Mg4Xhg5M5Ur94xopGZNx2zFUuf1uFy5qhzB2HX7pqJoNsyy9sCH+nE+ncsVW4+jGnNoDbTOj02AnqM8q44btZR1jsq5qCJVOUZVTWNzVcWqxlida813VV4dM2u0q+bful6NPmVbM2Z1vnytr201t3qdzvap6vHnnhtXmUPFqufM8TbJ1XqIj+nvN1u/NeayT2z7Xrnc8xvK5ctRie1r5SrG7iV6fmNjhmoZlxpDt7POx9RrxQzljunbJYaIe44qofrngzXOxco3n10MMf4uMWLHERs7pYZQ7phYwRjGfikUO2ZerBhW36jxJNbbJVd7DxQX25pLVw16fxTb19oH+/a9un5zTgK5XfvOUGwrpjXHvr1tNWepdcasq9jnWms/GdjXxDDXV0L79npyi7mmZg6zrZEr8Oky9BnCFzs2R6i/O2Zi+wlsDxZ+xwEAABa7ze37DwAAAAAAAAAAMP34/gMAAAAAAAAAUNmkf/Doc5/7nGzcuFG23HJL+e3f/u2FLgcAAAAAAAAAAKA3vv8AAAAAAAAAAADThu8/AAAAAAAAAACVRfuDR+9617vkLW95S/167dq1ct9998l3vvMdufTSS+VLX/qSrF+/Xj7zmc/IZz7zGXnDG94gZ511lixZsmThinYoikzyIotuP8qKCVYzKyVDbOV5IOjIEyi3+hjHC5Ur88Zunhyp0Ydi6XG5xmHlsGLr8epxFp71kkWuD18MVw2zfXSb2RjWtU2au/Kknv+qjlFZrx5f9d6p3hd6VOPjzMs2ej7rsapYVSYrZlWLa66qHLo+HbN13hrnWBXWGm3PbzN23V6NKCvjua6jXs+pMUP9Zuu3xlzWYLSvcxlrwllnYH7bc+tfd64csX1jaxPpN7+h2EPFCF0XV65K6LmWEnuSMSqxz+z5eF4vBil7mK5Cz6xJxOgyrlCOlBpC+YPP8Yhc+t4yZI6ufa1+ei+V1Dcyl7XfTIltzamrBv3cje2r91d1O0f/+LqNmBFz1JrHQOzqvI5VGMcb56yxG3Xa68lW7TvqnNW9PPCc1HvWPqz6fO+r9nrS5+PmwpvDrMtqb60/M0XnmLH93THTxNQfshh2CO17y+Sf55uqQtLXyaZgMaxDAIDftHz/AQAAAAAAgEVoY/k3baZxTAAwZfj+AwAAAAAAABPD9x/AVFq0P3i03XbbyXbbbdc6fsQRR8gb3/hGueqqq+QVr3iF3HbbbSIi8vd///eydu1a+fjHPz5YDX/yJ38iJ554YlKfm2++WY4//vjBagAAAAAAAAAAANOD7z8AAAAAAAAAAMC04fsPAAAAAAAAAECKRfuDRyFHHHGEfPnLX5ZnPetZct9994mIyCc+8Ql50YteJMcdd9wgOXbaaSfZaaedBokFAAAAAAAAAAAQwvcfAAAAAAAAAABg2vD9BwAAAAAAAABg3GihC+hjr732kne9612NY//zf/7PBapmGHmRTeyviyLwFz8u+8/sY/y1aizsv3bMrPEXihUzDitHbGxrnO6xZlF/7ZrsuO16us1N6Px4bDvGbP3m+Ms/39qp+ppzod4PczHVcaP/eI5QTPO853pZa9RcP4H3eSGZFI7rOdvXWMOBe4YV01tHh7Xp6+er085lre1wjuRcie0bbQLza/aLiB0bIxQrJpfOGfssiq0hJUafWJb5eAYvZL1D1p36zOoSK6TLuCb5fugaK6Z+333flyNmjrr2Tb0vj/fpe5+1ni8x46nbRd6XXfvD2L7Wfqy9H4ifE2u/3qpbmnPkypG619GxrONFkTnmzL03s/Z4ob3EuJg2s+OZ/UtZT6GcvnkujHVU1yHuz2DWHFm5rHbNttZnFveaDH028n1GC302C9XkG0/oWqV+Nm3UE/hLtVg++wMAgMVnGr//AAAAAAAAAAAAmze+/wAAAAAAAAAAVDbpHzwSEXnJS17SeP2Nb3xDHnjggYUpBgAAAAAAAAAAYAB8/wEAAAAAAAAAAKYN338AAAAAAAAAAEREZha6gL522mkn2XbbbeX+++8XEZE8z+XWW2+Vgw8+eIErW3zyIotuO8qKqHahVjEZcyPIyOic63ae2IWKnamYuapwpEYU6i/Srr+qe5DY7UNlLD+rn5V7tk+zgGpcuq1u166taPWr6tFzMyob5VnVt8qZlcerIsr2al2Oz/2ojJllep6bsaocuYpZWMfH3jc6dj2uQMzMOK9rdOco+xjrp7WmrRxj1y0T9zzq95y+Z1gxdTxvHSpmaE51P90+Jlf8nLbXdqtvaq4O17xuY8xv8Lp45io2RmysmDnrmtuXIzZXbKzUOF2kPIM3Vb5rNd8xu8x3bK7YdjE1DBrLeD6HcoRi+/p37evbS5h9jON2jg51qePWnOp2rr1sbN/YfVbKdbDuaK26I3KEYlsxdSx93D1n3WLW/RNu5fXeM3LPqZ+Hob22i73ujPbOtnHrvdv7QrdNq9eKExMzpm8zTrxQva0a0pobOaf/mT9NiiKbyB5moU3jmABgc8X3HwAAAAAAAEhWSLcvNRe7yf+/9wAA5gnffwAAAAAAACAZ338AUyn0uymbhKVLlzZer1u3boEqAQAAAAAAAAAAGAbffwAAAAAAAAAAgGnD9x8AAAAAAAAAgJmFLqCvxx57TO69997GsZ133nmBqmkritm/hZRl6X3yIq7TKPMPLmboVqbc6DxSHXw/xqd/0UtfCz03uVHNqByJ61q2Yqg2Vb069kjNTkzsuTrT+NagrquqX/ex5saKNz6+KlZVt76G9fnyuL5u9XjLdVmtO1dFRdkmL9uMAsdzFVM8xwv1vsjKc/W4jL5VhMw476o/0/U45nW2fVVLM443RxkrEz3mZrvWe82IacUb7zMXU9dvjbdsr2sfi9fqY9WXOKdRfVNzRVzziu7rm9+YWnyxY+obj2X19+UN9QmtkSFzxcZx6Rp7WsTM0ULGjt239MkZ6pNSQ99YRcQzuWuOmLnp2tfaS/j6peay9kgpOaz51e30c9N3XVo5IvdZum7f2tB3KautnqOYHKHYVcxWLOO4K3ZqzLq/cXsen1PrWV/FWmLUVl3jak/U53loxa7be9v658DKabfz5DLXf1qsmM8Ooc/JsZ95rNq8udO7jOWbzDNxkv/cYKH/mQQAAOhusX//AQAAAAAAAAAAkIrvPwAAAAAAAAAAIu3fktjkXHnllZLnc/8q5FZbbSW77bbbAlYEAAAAAAAAAADQD99/AAAAAAAAAACAacP3HwAAAAAAAAAAEZGZhS6gjzzP5YwzzmgcO+aYY2TZsmULVNHiVBTpfbIsrl1e+BuOsnByq4UVOTc6jBwdct1G5zZi6fHnjmpGZeU6RquvOl/VqWOOHDPR5dr5uMZRnzNy6T6hmjLH+PTY6lxl49b5ql+5vrJM9y/7jR2vYlbzW5Rt8rKNvvZ1DiNmpo7rfOM5qvpCMethe8ZhxZ6r2z1n1XVprz/H2q3raZ7LjOuk31tW3TqeO6bR1xxvWUMrsqdPco6Y9+JAuTzXPNS3ml89p0PEjo1RqPVkxUnJadVQiXmO6FxabO4usSeRaz7Ejmex5gztP4bIHdsnpZZQzFAs1302NkcwdsQ4rBhmzkC9vpzpufrnsOZXt9N7Jt91aeVQfa050nX7rp++61ht9RzF5AjFrmK2Ypk53HFTYta1Rc6l69yo7Fztm/S+UdTz0NpnxrD22JXY6ycSvoZ2Oyte+meDLrFCfedixAnNaSNnfNMy9nDPqqE/y2Ey8iIb9LovFtM4JgDYHPH9BwAAAAAAADrZWP5Nm2kcEwBshvj+AwAAAAAAAJ3w/QcwlVy/ITHvPvzhD8udd96Z1Gf9+vXy2te+Vq655prG8Te84Q1DlgYAAAAAAAAAANAJ338AAAAAAAAAAIBpw/cfAAAAAAAAAIC+FsUPHn384x+XffbZR17xilfIf/zHf8hDDz1ktl27dq3827/9mxx88MFyzjnnNM698pWvlOc+97kTrhYAAAAAAAAAACCM7z8AAAAAAAAAAMC04fsPAAAAAAAAAEBfMwtdQGXt2rVy3nnnyXnnnSdZlsnKlStlzz33lCc96UmybNkyeeihh+S2226TH/zgB7J+/fpW/xe+8IXysY99bAEq98vLv/k0xK9YFUVcuyzzn8+LQAMRGWXuZFYJVsTc0WGkGlvXQs+ZHr9rnLmqZFRWHOqr66xq1PFcsbvyxdb1VPXrPrFromo3Pu4q1qg8WZ2rc2fu83MxZw/k5VpxrfEqVBXTuvajMlZWxqqP1/3LWsrz42H0OV1fbMx62Or8uLqtOjeXo9v6846jjJmp9WbOqarN9V62Y8bNZd2+ytHK4OkTyFFp55o7r997XXNZ7V19Qn2tOQ3VEhPbVV9Mjb5YrrYx7a1afPXE5k6tYYhcm5Mh5yBmHzFUDbF9YmuKiRcdy3iOD5HDihFTm9nXs+/w9fPltXPF5fCNx5pf3ae1Z/Lur/z7KGuO0ur255zLlZbDdWdszYVRk3m8CMeOjWntVXUuF72Xqa9L5C3DFzuWFSJ2jkUc19Bsp3IkfCawYrT6mWvZ32+2b+B87OePuGYqdv9nVeznoy6G/mcI8/3PJAAAQNu0fv8BAAAAAAAAAAA2X3z/AQAAAAAAAADoY9H84NG4oijkpptukptuuinYdvny5fLOd75T/uIv/kKWLl06D9UBAAAAAAAAAACk4/sPAAAAAAAAAAAwbfj+AwAAAAAAAACQalH84NHHPvYx+fSnPy1XXnmlXHvttbJu3bpgn6c+9any8pe/XF7zmtfIk5/85HmoctOR9+g7SmxfFP7zWRaOkRfuRqPMHdxK6YqSG41HqrGeMz0PrnHqseWqglFZqe7b6qfO69pcsbuy5kPEvpb6eOz6Go31q8ZcjWNUBrXmsDqfl+er61GUayWv1sbY2tHrpRrrqIyZqfNVrOp4Na7WtR/731W2as3qnKGYul/V271243LU7au5E92+zOFaV1aOas7EPaeV1vvIcz3smKnjHMvfTG/3UfeY2Fyz+ax59edKymH0CfW15lTH7RI7tcbxWJVQzNT2rnoqvrpSauhaz+bEN2d9WfuBGKl1dRlHbH0xsUOxisBzf5AcxvmYcZp9jbpD9fpy2rni2ntjG/XqPr79UyhXex8VN0c6lq+EVr1Guy45QrHrvZl13Ch8PG58zKp9t+vTaJs1n+/1ns3Ya1bPmZQ7R6gsa226rl/s+8Daz1vrrstnA3sN27Hm+gbOR17LlCd012dLzHiiaxguFCagkCz43N0UTeOYAGBTxvcfAAAAAAAAmFe5iGxc6CImgC/eAGBR4fsPAAAAAAAAzCu+/wCm0qL4waNnPetZ8qxnPUvOOOMMWb9+vdx4441yyy23yM9//nN5+OGHZf369bL11lvLE57wBNlzzz3l4IMPlm233XahywYAAAAAAAAAADDx/QcAAAAAAAAAAJg2fP8BAAAAAAAAAOhrUfzg0bilS5fK05/+dHn605++0KUMoihm/+ZDlvWPEfsjcKPIdjFjt+rOC/eJUeYO6jpqTUmuGo9UQz0PrvHqselx5Cr7qKww2M8zZ7rOEF+siq5H1z13PDH32P8elTn0WKvcVVs9vvp8eVxfh0ZN5Xqp1oceRVGez8vzI3U8K4/XMVU8kbk1VsXOHW1i6H7jvXXdVg5dd92+jDAS3X4sR2vNGTlUNZmKWa0v17oMxWzHShunyNi6UccLde9ozVGnXNa8GtehS47AerL6hq7TeOyKVVcdw6ghZc3Hxuza3lWXlvre9NVjSalzUxA77j6s65Wia50p/WLrHDKmfj93yWXlCPXt2k/E3jtYfWPm1uwb2d6Xw5pn3cfaR+n+rlyx+6tQ3b47TKteo11qDtd4dOx6X2UdV0Grl7nRzx+zqsF/ffp87ojV545vrcnY6+Zvq9tZa9wI4IgRihWa75jPDqHPKrHz3eW5MsRn9Pn4AfWh/lnCfP0zCQAAEDZt338AAAAAAAAAAADw/QcAAAAAAAAAoIvY360BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADobGahC8BwiqJ/jCyLa5dHxov5RS2rbquWvHCfGGXtQNaU6Ai5ajhSDVzj1WPT49D15yrrqKzOd91aMXpeY18uXV/VVo89tobxOaxijKqY5cD0HNS51Pl2nKxs1i6mWh/VeqjqreopyvN5eb66joWKWdcytt6qmFXW6ozOacXSa0b3c8X2tXXlqtuLew5n+5Q5WutLrVGdq4yZ6eui5jiqbjNW2jhF2mu0/d405iiQy9nHmNdBc4Sug2cuROy5jakrNofrPmzFio1ptR8X27cSmsshuOqMlTqekD61TIL1vO6i69hS+qXWG4odE69o3e0nkKNjDF8/vWeI7dst10A5PDXrPtZeR8fQ/VL24rr+Viyjn2t8sXMUyhETu95HBXLpHL61asesavBfn7R5r2KWqj1neSIvU1V7zWq/KBHPk9h7SPT18sSwxhyaq1B/V4yYPrP9wkKfI2IvZcr9uutn5NjPu5OsAfOjkGzQPctiEdpjAAAAAAAAAACAKZbLsF94LRbTOCYAAAAAAAAAABCH7z+AqRTzezQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC98INHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABg4mYWuoBpV4hIPuEcQ/5qVVHEtcuyuHYxY7fqt2qxcueFXdQoawbToXXP3Mg9Gmtoja0aj65f152rrKNWVelzENvfV4fVx5qT6rBvDkfqZJUjLweix173rc6XHfS4i7FrnpfXWK+nKlQVU9dSXce565aVudoDrtZYtZ702FvnVSydS8d1xa7oHK326vhczvZiqeY7uEbVeCpFGTOzrps45jk5ltHe8T7X18qaZ+vaWrm8fcS9dofMEeob6leMXXs9vzF1xeRIiaVj1rVFxB6ir4j9nAjVPCmu9bwp8j1/u+gzL136xtYfGzsmXuG4N6fkisphtAn19eV2PVP65PTnGiiHp2bdx97r6HZxeycR1z4rEMuK4xhf7ByFcrTmwRNTn2vnauawYvuufb1PbM27u91c7PD7wrXfjqH3FjHvwdDnIHMtO9taOaz3Q7/+vj5zfY3jEVMcexXi78+RAcdjp3cZJG9fXetemJ0OAAAAAAAAAAAAAAAAAAAAAAAAAABuQ/5WDgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgNPMQheA/vKe/bv86lVRxLXLsnAbq36rLiu3L1deNE+OsmYQazg6ZO5oOFKN9Hiqcei6db15K5vIyKgsdv41Vw4rZmhd6RL06/FM9byVB/W1reoalUVYc1Odz404s22rgso+6lpXtYzKmFl5vuo3d72yMtdc/+pcrmKr4bXPF/5czfrcdescwfYqZ6NPNZ+i+5Q59PybNZU5HOu0nufEWJUqZui9O1u3e6zWPJvtjdr8Oay5HC5HqG+on4j/WoXqGs9R8eWKuWZdYw/Zd5yu2SU0jmkVMzdd6es3HzFSxhMbOymm5zkck9OXq09fX3//3qFbTl8/a/9h1mfl8NSt+7j2d64Yup9vP6bnTdffihWocS6urW8OV+x6PxTM5c6hY7uu49y56rX/+ljt9Plx1d6mtadUDUb1ZqhZr97DdWG/x6z2rrbW+yAuRspngXZu43igX8zTM/Y+2uUzUNdr1vXzlk/fz+2YjLwIr+NN0TSOCQAAAAAAAAAARNpY/k2baRwTAAAAAAAAAACIw/cfwFTq8ls3AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASWYWuoBplxezf0MaZcPGy7vUENmu8Iw9C4zDqsvKnZIrL9zJR1kziA7p6qWvr74+ehxV/Va947XmzozjsdxBQv3G6Tp0vXp8OqOey2oOx9tlddvmgVH5Wl+fqqaqFj2n9fmx46Oyjkxdw6q+qi5dy0ia/eqc0qbP6dhqeO3zRUIu1beicwTbG/Mym7/sI7pPmcN437RrKnM41mM9z4H3YGzM8X6xY7Xm2WzfKYc1l8Pl0H1T+4n4r5Wuy5UjJVdqTB27EpPD6tslhsV6bmih8S202HEMybou8xGry3hjc4RiFxHP4lCuYI6IWq0Ywdye+rvWbfXz7UXNPlYOo25Xe2uPrmPovtYezjVnuv5WrIg6Z2M76oycm1AOHXs8ru/cbCx/Duvajh8351PvQev9Ydz1cLUJff7Q9c3t2yOesbH3DrO/bmfHM9duwtr0tZ/tk5a7juk/XcaIm6uYayvS7fNsbOxJ5e+dM7H+of+ZBAAAAAAAAAAAAAAAAAAAAAAAAIDNyw9/+EP57ne/K7fffrusXbtWttxyS9lpp51k5cqV8mu/9muyYsWKzrHXr18vX//61+VnP/uZ3HnnnbL11lvLrrvuKgcffLDsueeeww1CRG699Vb5zne+I3fccYc8/PDDsssuu8gee+whhx12mCxdunSwPPM5pk0VP3gEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABARkQceeEDOOuss+cQnPiE/+9nPzHZLliyRX//1X5dVq1bJqaeeGh3/nnvukdNOO00uvPBCWbNmjbPNYYcdJm9729vk93//95PrH3fxxRfLmWeeKatXr3ae32677eSkk06S97znPbLDDjt0zjOfY9rU8YNHm6C8GCbOKOtRQyh2RIwiMI7MqM/K7ctp5dI58qJ5YJQ1O7rC6DL19dHzrOvXdbtqteci7SL65lzXpcehu+q50sfH5676X625Kv/vqGyQlwMdqWx1Lcb5ZqysjFWUbZusWoqyX1b20/Gc5+r6mmPWOVrnjVzjdOz6uJFDVPvW2nWMo+4j7nmt1ov1Pmm/P8ZyWNewHkdaTB3P20fNb92+yq3iWO275bDmcrgcsf3G6RiFWjmu+fXlSMmVGnOIHDExusSJYd0bNwfWPM9nnC7zH5svNrZ+f3XJGcoVU7MVI5jbU7/Vt2u9vv1kai5r3l3trb20jhF7zV1zpuvXsaL3VYG4fXJYsV3XpT0efw2h2K49aTWPrT1o0Tyvj8/1t1XP/jpWtacs1N7O2LfP9Y+/x4Q+L1n7ctd6MtdsQgxf+9k+xvHAozL0JI15H4U+F9ax4pp1ij1ErujYw28/0EMhcc/uTQ3LDAAAAAAAAACAzVghk/3Ca6HwBQgAAAAAAAAAAJsvvv9YEBdddJG8/vWvl/vuuy/YduPGjfLtb39bbr/99ugfPPrc5z4nr3nNa+Tuu+/2trv66qvl6quvlpe//OVy9tlny4oVK6LiVx5++GH5gz/4A7ngggu87dasWSMf/ehH5ZJLLpFzzz1Xnv/85yflEZm/MU0LfvAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADZzp59+urz73e9uHd99991lv/32kx133FEee+wxufPOO+X73/++PPLII0nxv/KVr8jxxx8vjz/+eH0syzJ5xjOeIXvvvbc88MADct1118m9995bnz/vvPPkl7/8pVx22WUyGo2i8mzcuFFOOukk+exnP9s4vuOOO8rBBx8sT3ziE+UnP/mJXHfddVIUs79A9Ytf/EKOO+44+eIXvyhHHHHEohvTNNn8RgwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAqH3wgx9s/djRS1/6Uvne974nt912m3zhC1+Q888/Xy655BJZvXq1/PKXv5SrrrpK3vrWt8r2228fjH/77bfL7/3e7zV+GOjwww+XG264Qb71rW/Jpz71Kfn85z8vt99+u5x11lmydOnSut1//Md/yDvf+c7osZx66qmNHztaunSpfPjDH5bbb79drrjiCvnUpz4l3/72t+X666+XQw89tG63bt06Of744+XOO++MyjOfY5omMwtdwLQryr+usqEKccg7FDaKLCiPiRU4Xxj1ZUYNvpxWLp1Dx84Le8CjbLazNY1VT2ueq7nUdbtqteZiCDq/rlentuakqrGaw/F2eq6qHHo9VTHyMsioPDAXUxpJRmMTo69dUebPy9xSvg7VUvXLsvak63PV3FXXLDdyZNZ5Ty4duz4eyGG1841jLmfZR111fW11jsp4rkJVlKmY1hqw6q7i6TjePuY4y9wqjvd6JOew5nK4HKF+KTF88xubIzaXjlmJia1zVEK5YuMMEXPa+eZsIWL6ntND5Qrl0Pe7PvmDuTznu/bNA/V3yRkcZ4d8Zi6jft3et+/VMUJ7nbqd6ueqXceKvbPoOUq5DqG9mxXbdV10XmufGFoLvmtezWMrdtE8r4/P9Q+znv1WLa09aEQOS2gfb70HXWvWimXFsNt76gnUG1rDMffl8Jyk6fJZqc81bcVaRFsGq5RFVCIAAAAAAAAAAAAAAAAAAAAAAACAReq73/2unHrqqfXrpUuXyvnnny+rVq0y+4xGIzn88MPl8MMPlw0bNgRznHbaaXL//ffXrw877DD54he/KFtuuWWj3RZbbCF/+qd/KrvvvruccMIJ9fEzzzxT/uiP/kj22GMPb55bbrlFzjrrrMaxiy66SI477rhW2wMOOECuvPJKOeqoo2T16tUiInLffffJ6aefLv/4j/+4aMY0bUL/zicAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYApt2LBB/sf/+B+NHy06++yzvT92pM3MzHjP33TTTXLuuefWr5ctWybnnHNO64eBxh1//PHy6le/un69bt06Of3004O1nH766bJ+/fr69Wte8xrnjx1Vli9fLuecc44sW7asPvbxj39cbrnlFm+e+RzTtOEHjxa5YgJ/feRF3F9UrMCfpSjCf7G5YmO75yJr/LViiX/erTnrMiexfDF1HbpuPU5rjlzz1+pb/tXjN8aXSya5ZN7roPPasco/43q1xl9kUhSZc66qczr2XCz3eM3zZbzCVZc1HiNHKNc4O+fsvLfbB95n3vdDJoUjpnXPCMVxxzL6JM6t73qksufSc80TxxHqlxLDmludIyVX7FymxO6bq0vMSeRYTELjneTcpui6TlJyxeYIvV9S5i/0vg/dL7x1Jt7rQ/1i6m3nCu2/OtwT1fxb1823N7VitOtrPvf03Llqj12jrXrFv+fw9Q3tAazY1vHCM49zezl3Djt2tV/L5v5ae7BmG33cGk/MZ5O5+sJ7y/GcKXvR8OeSrPFn1e+LZcWw2/v3/77PcaE9pvXe6/NZzZLyWc2KHfu5KvZzb+xn38Y4JviHdIWxljf1v2ncrwIAAAAAAAAAgEgbp/gPAAAAAAAAAABsnhb6O4rN5PuPiy66SK699tr69VFHHSUnn3zyoDnOP/982bhxbuC/93u/J/vuu2+w3ymnnNJ4/alPfUoee+wxs/3atWvl4osv9sZw2W+//eT444+vX2/YsEHOP/98b5/5GtM04gePAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGAzdPbZZzdev+Md7xg8x6WXXtp4HfuDSvvvv7/81m/9Vv36kUcekc9//vNm+yuuuEIeffTR+vWhhx4qT33qU6Ny6ZouueQSb/v5GtM04gePNkNFj79YeRH+C8YI/HnHWLj/YnPExGuPOWv8tWKIfy5j5ig0J13mTOfS9enxhObSpepjzU095jKGlWMuTtU+q/+seooik8KZc7aW9njVfJT9iyJrjVHH1udbc9fK1Z4Ps15xz29ovel2rjU6PsbmeDLvHFvXyV9XJoXrellr3ojjj2XkTpxbl9TYMUJrNDWXb85iY1hza+UK5YvJ6Ysdm0Pnstb2EKwcMX/zYTHUNESuodZASq5g7MD7IyZncM8wwfe5fU8Pz9lQ99deudT8h/YWLrEx9HNOz13K+grtAYJ7DN9zvGds33Frn2jlsGOXezhj/fnazB1XfwmfLey99Wwua49p1Rj3OSNz/uma2nvPdjwrhlWD9RkgZs6szyzh+5Z/Plx1pXz2smL64sbs77p+VnXRc2f9AQAAAAAAAAAAAAAAAAAAAAAAAMBicvPNN8tXv/rV+vWee+4pz3nOcwbNcdddd8l3v/vd+vXMzIwcfvjh0f2f/exnN15/7nOfM9tefvnl3r4+Rx55pMzMzNSvr7vuOvnFL37hbDufY5pG/OARAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGxmvvzlLzdeH3XUUZJl2aA5rr/++sbrpz/96bJixYro/ocddljj9Q033BCd69BDD43Os2LFCnna054WlWs+xzSNZsJN0EchInmx0FXMGg1wP4kdSkyq0LyE6s19fY3jhcpp3WN1bFc8HUvHzAt38FE229Ea/ngva466XkvfnOtTun49Xmv+Xcer+ati5OUoq7nQw6lijKr25aSOVJWN8VRtyiT62hblePIyZ1VTNU5dSxXbNdd1fSp2VsbQ53WOquxMnW+0UfNvxa5rUjkqOldUHzWeur24r0N1XV3vJ7uuModxTfW8W3H8sdLG52LOxUBzN4lcup+rb2wMa259+axcOmcl5jroHJVQrlDuLjUMwapjUzXUeKzn5iRqSMlVRO1q4moI5Q3F8PUP9jXGMYmc1h7BbO/LYdRt9bH2Oq44Voz2nkfvieJrCu2v9Fzp2P656RnbzBnOZeVox65e2zmsNnPHVWxVTMwdPPXuUucY4B/IhD7zhNabr+1cn7TcMXMWvl8F+kfkiI3VJWbdp+cjfj52CPPxzwsWyT+SWJSKIn4NbkqmcUwAAAAAAAAAACBSLiIbF7qICejyhSEAAAAAAAAAAJgOfP8xcd/85jcbr6sfCCqKQq688ko577zz5JprrpGf//znsmHDBtlhhx1k3333ld/5nd+Rl7zkJbLnnnsGc/zgBz9ovF65cmVSjfvss4833rgbb7yxd67rrruukeu5z31uq918jmkaWb8LAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACYUt/61rcar/fff3/56U9/Kr/zO78jz3ve8+Scc86RG2+8UX75y1/Ko48+Kj/72c/kyiuvlLe//e2y3377yRve8AZ59NFHvTluvvnmxuvdd989qcY99tij8fq+++6T+++/v9VuzZo1smbNml65dPubbrrJ2W6+xjStZha6AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADYHOkfz4mx4447yk477dQ795133tl4/eijj8qznvUsuffee4N9169fL//wD/8gq1evls985jOyyy67ONs98MADjdepdW+99day5ZZbymOPPVYfe/DBB2Xbbbf15tlqq61kxYoVSbl0bQ8++KCz3XyNaVrxg0ebkbzo3neUpbWPSRUKGarXV1Nu9VGvCyNHpmJb8WJitmIVzQOjrNnBVZIeap9rqelQuj49Ht9cWKo+eq6qXNUcVOPS17aqIS8nc1Qe0HM7m6vZJs+M3EZN1XCr0ONzPSqPZlW9KkZR+M/H0HNS1xUZ2+w/9r/b6ykuZ92+mmPR7cdyGOu+XVeZQ8Wy1sL4+oyP1W9OXW1TY9ftjbmbRK6YvrExCrVq9By7clWsnDp3HTvQvk+u2Bq61jONfHPTl75+KVLr6pJLr/s+NYTyh2JZ/WNqyI1xdM3p62vtEcz2vhxG3VYfa2/kimPPp2oX3K3a8UL7Kz1Xeo7MGiPyB2ObOcP5qlxWjrnYjkLHcljnY/patflUbesY1f4wsMe09iMx9bVqsMZlvkc9uRJriJmr8H0q0D8iRyhGSqxG+x6P6iGf8kN+RgMAAAAAAAAAAAAAAAAAAAAAAACweTn++OOT+5x22mny7ne/u3du/cM9J598cv1jRytWrJA//uM/lmOPPVae/OQnyyOPPCLf/e535ROf+IRcddVVdZ/rrrtOfv/3f1+++tWvytKlS1s5Hn744cbr5cuXJ9e5fPnyxo8DPfTQQxPLM86VZ8hcoTFNq5Tf4QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAbOLWrVsn69ataxy7/fbbRUTkgAMOkBtvvFE+8IEPyFFHHSVPecpT5BnPeIacfPLJ8rWvfU0+8IEPNPqtXr1a3v/+9zvz6B8H2nLLLZNr1T8opGPOZ575zjWNZha6gGlXFLN/CynL+sfII8cwSsgVChkK5avJqiO32qvX1jVzzaWOGYqlY+RF88Aoaye3hho73TGXT9cRu25j2lVjruZqVDSP17HqWqqOZXsVLy9PjMaS17F1zLJJXsUqx5mV81xfv/J4Nf+qBBXTHWNknK9rTMih28bmDvX35Y3NWbevroNjhRXGNTZzlLEy0XMmZftWig6x7DlptBv73+33c9y1DbYfm33X/PXJVdH9fH1DOVvtjDl2iZ331Bp8uSqxOWPqCelS70KKHdcQ9HXpomu9KbmLyKdqqJaknEbbUIyY+ciN8XTN6e2b2t6Ty7oOVh9rP6jjeHOqGLFz19o7OetTfRJjDhrbGbk9h75cVg4zdjmXVQ7XvrFq074OcfXFrN3U54Hex8R+Fhrva7HXl6+Pcdxc/4EaEt4PsbWkxkmJVbfv8Kjt+3TukrOrSf5zg4X+ZxKLWS6Z+b7clE3jmAAAAAAAAAAAQKRc0r+M2xRM45gAAAAAAAAAAEAcvv+YqI0bNzqPP/GJT5TLL79cfvVXf9Xs+2d/9mfy85//XD70oQ/Vxz70oQ/JW97yFtl66629ebMOP4SymPvMd65pwA8eAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMACuOyyy2TlypVJfXbcccfeebfaaisZjUaS581fYHrb297m/bGjyhlnnCGf+MQn5MEHHxQRkTVr1sjnPvc5OfHEExvt9A8grV27NrlW3cf1o0rzlWe+c00jfvBoM1AU3fum/hhYnpBrFIgdCuXrbtVh5bR+/G6ka3LE1XMUimVdjypOXtgjG2XNzj0ubU3ns+rT40pZV1Xbeozl8VFRvZ49ocdXX8es2d61Los6ljQajdQszeXOymY6Z7OWonGu7JtVOZsx6tiqNn3cl0MPTbetWLkrVq5GjJ456/binuvZPmUO/T6xcpSxMn3d1Nz3i6Xm3xiXiH1NzblInLvZHPb8dcml+/Xta9UtMjfHIu151vS9xspt1eCrIzZnbO4uXPV2Zc73gDkmyfcci9FnnF1yF94dxVi7QOyY3H1j+PrngXFYffvlHCaX7xpYfay9no7lG5/ey1hzqMfT2js561N9AjEtOrZrPKHY1nk9h75xmDFax6uc7hxzNc3V2L4OVW5/fX3vNb66qmdyn89ROqYWu+dunDPXfaAG670XMb7Qj4SnzFHsD46nfKYU6feZKDVXjCHWDQAAAAAAAAAAAAAAAAAAAAAAAIDNy8qVK+XAAw9ckNwrVqyQhx56qHHsVa96VXTf3/u935N//ud/ro995Stf4QePBs41jfRvOAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAptyTnvSkxuudd95Z9txzz+j+hxxySOP1jTfe2GrzxCc+sfH6nnvuiY4vIvLwww+3fhxI1+3K8+ijj8ojjzySlOvuu+8O5nHlmtSYphU/eAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAm5n99tuv8XqXXXZJ6r/rrrs2Xt93332tNvvuu2/j9W233ZaUQ7ffbrvtZNttt22123777VvHf/azn/XKpWu3jk9qTNNqZqELmHZ5+TefhvwVq6JIa59l8W3zQOxRIJavu9XVymnl0tfONbfWHOm5CMVyxWnFKPyTMsrcxYT6WflF2nXrdjHruxpr1VePqzqel1euGoeuuso1qtqPBRoZK6KOWSZp587KWEWj1mrOXHNaraNq3VQxMtVWH6/r9+TQ2TJPW19uK5czhspVx0jOaV8P69qbOcpYmYoz/h7W79vUWJo1LpH2fIb6pM7dbI6yT6DOVj/PWo2ts2LF8NVdt1ErKDTfMXVbddQ5Evq6cruk1DMpepyLSczzJFWf8abWo9ept21k7LhnrL9NKIbVP48Yj9nXOB4zbuvZn5zLU7/Vx9rLpV3b2HbNmLqmIe8Wodiu+Wjv0bKk8ym5gjHUXi71/Oy5Kn9afTF7cf3MqXOUbUZqn6LrDO0zY8TutevjngUWWnv2+zvQ0VNPbIyUz76hz4Ot3GnNO+Uwcy/Q9mCof5Yw3/9MYlNSFNmi3nt1NY1jAgAAAAAAAAAAkXIR2bjQRUwAX3oBAAAAAAAAALD54vuPiTvwwAPlyiuvrF9vscUWSf11+8cee6zVZv/992+8vvnmm5Ny3HLLLY3XBxxwgNl2//33l6uvvrqRS+dPyWX1nc8xTaMhfxsHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALAJePrTn954/cADDyT11+233377VpuDDjqo8fp73/uePProo9E5vv71r3vj+c6tXr06Os8jjzwi3/ve96JyzeeYphE/eDSF8gH+uiqK+L/gOAr/n7cO4y81V6ud569VQ2C8MXFS5y4vMuefT6g+q13KWrFihWJU1y3mmueSSS7ZXGyjj5W7KDIpiqx1fHwe9TrSOawY1fG5Wu0c1hzotu1xNXNYubwxxP0+Sc05m3f2erT7GOvNrCmTwhFnto/xPo1Y996c5bhi53O8zzA5rLmz59uXa8gYof6Ntp5r58oZe91c9aTUlVpPl7qmwSTnoc91S62nWoex63G8vr41xIyx63vOuk9E9e1wT5/LO9A90Htvd/cxn+tGLB3HvydrzmfruZ34XBlvG9wT6DrF/9x37RlD+4x2ztm/mD1GFcvKYT/Xyxz1vkvPcXtP1tpXGfXF7otTPnfoPq3xlPXrv1BeVw3W3t/3OST0mca6L8XW4F5XcZ9DQp+JunyeG+qzXOjzg2+cqZ9dnXUN8AcAAAAAAAAAAAAAAAAAAAAAAAAAk3bsscdKls39u2m33HKLPPbYY9H9r7/++sbrJz/5ya02u+yyS+OHlTZs2CBXXXVVdI6vfOUrjdfHHnus2faYY47x9vX52te+Jhs2bKhfH3zwwbLzzjs7287nmKYRP3gEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJuZXXfdVQ499ND69fr16+XKK6+M7n/55Zc3Xh955JHOdieccELj9T//8z9Hxf/hD38o11xzTf16xYoVcvTRR5vtn//858vy5cvr16tXr5Yf/vCHUbnOOeecxmtdszZfY5pG/ODRhOXF5P4mWnfiXxdF4f8L1thhborAX58cobmJHWfM/IbmLuXPyqtz6XZd5srsW/5VufIik7zI2u2qmKq9ax7nYlV9MsnFETNQk4teL6E1MVdTJsXYuFw5rLG3cxrtVA6dazxfFUPHst8PaTln87rn3Vz/5vgzKRxxZvsY898alzuGlXO2Tv98RrfvlMOaO3u+Q7mGihHq32hbzrvvGrpyx4zDqiulvli6LutvsZvPcejr0eW6dK0rdr256gzVEhvPJ3Q/Tb0vxOTvcg+fzTncPS90H/Y9+1NjzdVo7yv1fLae0z2eycE9QODZq8+7rkFoX9HOGfe8HL/mVo72/nB2Lqscwf2ZMSYRey5Ce9cun0/03lLXOeTeur0Pc++XfZ9RQvfs1H3+uC6fUWI+E/i4xurr0udzcdfPnM46Iv8mabH984NNWRG5N9rU/obeCwMAAAAAAAAAgE3Ixin+AwAAAAAAAAAAm6eF/o5iM/n+4+STT268PvPMM6P6fe1rX5NvfvOb9evRaCQveMELnG1f/vKXy5IlS+rXl1xyidx0003BHO9///sbr1/84hfLlltuabbfaqutZNWqVd4YLj/+8Y/l0ksvrV/PzMzIy172Mm+f+RrTNOIHjwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgM3TyySfL/vvvX7/+0pe+FPzRo7vvvrv1Q0kvfvGLZZ999nG233fffeXVr351/frxxx+X17zmNfLYY4+ZOf793/9dzjnnnPr1smXL5LTTTvPWJSLy7ne/W5YuXVq/Puecc+TTn/602f6xxx6Tk08+WR5//PH62Gtf+1pzLJX5HNO04QePNmF5Mfxf51oS/mIVhf/PW0/HcRbqr0uOVjvxjz9mXH3n0idUV6hdzNrRbapY1pir43mRSV5k5vUYn4u6j2SSS2bWUJ23xlcUmRRF1jqey1w9dZ1GXVaM0PnxNjqXldNsp3KMs69l3PhCOV15retirQF7/Fn91+4Tdx+z+7tzztZpjct6/xjtO+Ww5s6+xuO5rHwpMUL9fde+1ceY/1ANvjpS6oupsY/xehfj3yT1nec+9Y7fG2LWV0qdqe8DX4wu92wR+z4QEyM1Z8xeIzlXh/uudU+PjeXfVzXnM/7+5X8Gu+at9cwPPGutmCnaOZtzWeVMyWXuHcq5tPfBzX2XK07V15qL2D2pVXNjn1jFVLlcfXw5unzmsT4zWJ8/Yu6J1ueI1M8fzr1Y4njs96z9Z0n+DBf43Bj67Oga7xCfZ2PHNZ+f2wEAAAAAAAAAAAAAwP/P3p2HS1KUif5/M8+hpReEbqBlURsaXEC2VlRoRFFQNkcabeTCeAUFd4QZ9SeOOjag45V7Z3hEUYZBryAzCsgAckdBRcEFEGVAEBCxaRalRRBooBdo+mT8/jgZWZVRGRkRuVTVqfP98NTTVmbE+74RkVWZ2YUJAAAAAAAAAKDI2NiYnHnmmRLHncfQfPSjH5WTTjpJHn/88Z72V199teyzzz5yzz33ZNvmzp0rn//850vznHrqqTJ37tzs/fXXXy8HHHCA3HXXXbl2zzzzjHz5y1+WI444Irf9ox/9qCxYsMA5noULF8pJJ52U27Z06VI566yzcg81EhH53e9+J/vvv79cf/312bbNN9/c+yFE/RrTqBkfdAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgMF44xvfKGeeeaZ8+MMfzrZ96UtfkrPPPlv22msv2XbbbWXdunXym9/8Ru6///5c3xkzZsi3v/1t2X777UtzPP/5z5dLL71UDjzwwOzBQ9ddd53svPPO8opXvEIWLlwoTzzxhNx8883yyCOP5Pq++c1vls9+9rPe4/nCF74gd9xxh1x55ZUiIvLss8/Khz/8YfnsZz8rL3/5y2WTTTaRFStWyM033yxKdf5L9TNmzJDLLrtMtt56a688/RzTKOGBRwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwjZ1wwgkyNjYmH/vYx2Tt2rUiMvmgoJ///OfWPs973vPk0ksvlcWLF3vl2G+//eSyyy6TY489NnsAkFJKbrrpJrnpppsK+xx11FFy7rnnytjYmPdYxsbG5OKLL5bjjz9eLrroomz7ww8/LFdddVVhn/nz58v5558v++67r3cekf6NaZTEgy5gOlAer2GRqLBXpRyOly+l7C9nDZ7jqbJmrpiucYeMyzWXPq+eMRu5bO1C5szWR8fWOe25IklUlMUrO/46sSJJJCqZu/L9SkWiVFSYQ9djjtmsy4zhc4ybbcxcZk5nu7JxWOqxjc83Z1lePe+97S3rVJZDIlEFsXqOM+sc2vpHwfMZ+v1VLYdt7uxrbOazccVw9Q+tR6Qz/7Z1KKsjpJ6yGoteKNfk3NVZz9DjJ6RW37p8Pze1Yjg+92VjauN7LDiX43u2iP36wi+W67zfPZ++43Gd/4rmrucaoIGY3XF9ri96rkuMnLb+3Tls16mu87nzOqzrWtk2F65r0rJrZp/7gaJ6erZbXmUxXPcZruuqos+Ha4y+1/eufmUxfO8Dfe+ZimI6Y9dY66buQX3qb/LeGe0run8dlRcAAAAAAAAAAJimmviXCof1BQAAAAAAAAAApqdB/0YxDX//+MAHPiC33XabvOMd75BNNtnE2m6rrbaSU045RX7/+997P+xIO+SQQ+T222+X97///TJ37lxru7322ksuueQS+da3viWzZ88OyiEiMmfOHLnwwgvlO9/5juy1117WdvPmzZMPfOADcvvtt8tBBx0UnEekf2MaFeODLgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMHg77LCDXHDBBbJu3Tq57rrr5E9/+pM89NBDMmPGDNlyyy1l9913l912261Wjvnz58vZZ58tZ555plx33XVy//33y0MPPSSzZ8+WbbfdVhYtWiTbb799I+NZunSpLF26VO699165+eabZeXKlbJmzRrZaqutZMGCBbLPPvvIjBkzaufp55imOh54NCTUAHJGDcRIPAuPA5K5HkQXe8RQjroiSz228RTVb0thNvWNWTRuc6y2cdnG48MW07YO5nh8DgGzjY5hzoGuJUm3x+l7c3w6Xq4W3cdWg44tnrmz7Z2GSaRy+5J0XxzlR2iOT8eIjHa27bk6Hbn0u8jVrmscZj4zV2cc9XJ25+3NmfYRs05do18tk/WkORxHo3089v6u+SxauyrtQvra586dq2wefWK4+ofWk2vvuY5F9Wg+dTnrMGJqVdZwKrKNvwnmelWhAq9eQsbjW58rpk8cZwzLOH3GY8tv6+u67irLac1lqd/Wvux60jeW/Xqmt791LsyYjv1tsK1HUc1mW9u4bOOw9S+qoXMdlc+h186236w1a1/UJs1vrmVvnYUpgpjns841Tf3YmiuU/bPqjm0/Tqr1y7XxnAPfqaoyp6Fr3OSDzJs8BmymxxUFAAAAAAAAAAAAAAAAAAAAAAAAgFEwc+ZMOeCAA1rNMWPGDHn961/fag5t++2378sDh/o5pqnK59kxAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAtYwPugAMjqrQJ6qYKwlIFjuSJD4xHPuVpZ7IkrusfrNeW1MztBmzaNy2sZrjs42nCltOs15Xyu72tjXN2qT7beumx5ekDePInl3XH+s+6aLGRsWd3Jb9Ok5ZjixWvq4o2562M8Zv9leq0yCKyuswc2nGVFrbdedz5cq2B+YsbGvNWTz/es3Nz2RpjjRWlMayzr91PPn+Pn162unajO228YfE7s1lmzt7Lt+crhjd66C5YmlldYl01iFrH3C2KqqrrLYQ5jhsXOMbFN/6m2RbjxDm8eBsXyGnq07fmD7jdcVKLOP1qcGW39bXdT1VltOay1K/rX3Z9ZVvLNs1kG0uvWL69jPeF82ZK3bo8Ve0bj1tVFiusrW2zq8q39+5dsu3L44VFcbqnd/y/Vrn+qqrrb7WtFxf+F6/V/mGd6+xpZ9HbNc9gCuGzz2a9+ehwuSE3sP4zIkzRgun6eE888NXIlEj1yzDJuQ8CAAAAAAAAAAARkwiIhODLqIFTfxgCAAAAAAAAAAApiZ+/wBGkuu5MAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALWND7qAUZeoydcgxVFzsUKHUiW1a758xmN7mJ3rCV/KkjsqyWnWa6vPDG02Kxu3GbPq+MrYYpp12cosq982R5G5P90Qp+8T/b4nXpTGUV3b8rGzerNYaZ90g7mmiRTv756XOM0bpXn1PrM+Yzhdtbj7m2069eXbFs1BUW5bu5Bc2XbPnKVtrTnT9mK2T2MXfK7KxpZvJ2k7v/6qaySR41svdA5t7UvrKekzmcs2d13jsPV1zKErd0isKjFF8uuhudbFVpvmqrEOpQoO1hFnzm9VRWvt3dezhpBam4zpipVYxu7sV7Lf1td2vvcZry2fbe1s7cuuGXxj2a7ZQuayJ6alJrOdOYdVYofGLFq3nja2ObHEsuUomls9r9Yc6f7suqu4WTYPZcdu71yU77f197kudp4HPWKYsVzsx267fUX87kt9x+x7j2uruTR2eJdO34ZO8YO4hW/z7w0G/XcSAAAAAAAAAAAAAAAAAAAAAAAAAAB0q/N8FAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC888AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALRufNAFoH2Jqh8jjqr1C0ntm8JnPLZ6E1c/y3ZVkjMyctnqM2uyhSwq3Tema3whzJy2eqscX7qPrt+29nrek7RBnL4357y7vqwe3cfMne6I0+B6znQt5v7CXCpK9+UHn6Tb43S7WZPOYfbvXjddrzWH0c7MmdWY/hkZ7QrbeubKtjtyFuX1z5m2F7N9GrdgPXrnPY0t5vqk4+n5zBbXWCR8PGlOI47qWg/XceTK0clVPHdefR1z4OofEsuMqfnEzvpa1thXogoOpJTPcTAdlc1ZVcr7zF/QN7CekPpdsX1jlcVJHGOvU4Otr+0aoVYuyzhsfWzXDEVxbDFs12S2OS0an+8amu3MOTRjF8U1yw2O6dg/GbM4pyuXa/vkvqg4h8rvt66LKq8pH6t4u61Os6aQ+xXzuqKV7zjH6cT+mXTHdl3zu67PQ850vtf6PnX3xA7vMtmvgVN1m2f7JupD/yTS7H30sBjFMQEAAAAAAAAAAE/8AAIAAAAAAAAAAEYNv38AI8n2fBcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDGjA+6gFGn1OSrDVHUTtwiiecY4ho1uVKEhLbV66rP9hC8sieD2dbXXB/fmsrmwSy/6jiL2GLZ6rG29zhWzLnRXbKY6X5z3vX6xEq/7wSKo3zinljKkjuLpeMU7xcRSYy6lIrS7Sq3PUm322rSOXT/qKtdVofY2xS1s+U0pqG8rWcuV5yivOE50/Zitu/8b9d3oEpjRFK+Dp3tvTXaYzQzh6V9AnP4cPUtW9PQ3K5YdWJnfYxvRXOdqtB1m3zHMdXZxt8Ec72C+gbWVWUcrhyumD41Jo45qFODra/t2qZWLss4bH3s1xj+62S7vrDNqTm+8vEYMc2+FWLXjunYPxmzPKcrVu923b7sOCtvo2Pq2mw15a6vSvYVxbSxnd+7Y/Zey5XHtF03hnA9ZNsnpiuGa25Cyva97/OdiyoPGfetoUxTZ+0mamlD6LHY1t9JAAAAAAAAAAAAAAAAAAAAAAAAAABQRdlzXAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABoxPugCUJ1SzceMonr9k4Ca4sBcrtA+4Vz12WpKyvpYttvWx5zjsprMesymtjGHrIONLYQZu8pxqPvoeTXHmeVIt8fp+7LjM1FRGkt1d7XnToPFxkg7uYv35+r23G4MJ8tR9hkwY6l0fFGkStuZ82DWkK/D0tYzV7bdEqdIeE77OijjuLCPPY1tWWuf7yJ7jLDxVNHknPnW51pT3b+bK5bmOk7M2CFzqIxPvrledZjjMPkc/8PANY4mmetRKUZgvb7jC4nriukTK3HMhSuGrYayfmXXLlVyidjX1NbHdj1SdmyYsWzXGbY5NeekfDzluavOYYieel37C+bDNQ5brM52nbt3v85nWwfzms625rqmsutGc58rpnl91ZOz63/7XrO5airje7y4YpbF8b3GdzULuVfwnYMqn5eq9yxNnnGbuG9yaeO+HQ1S4dcbUwLHHQAAAAAAAAAA01ciIhODLqIF/fgRHwAAAAAAAAAADCd+/wBGkuv/4wkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFDb+KALwHBRKqx9FFXPlXjmij1z+IRzhbLVVFaD7cF5tqeJmXNcNodmPWYdtjHXWBZrTLMW17FSNC/mnOgYuq0en63+rH3aIO6qwZxHvSurW/fpqTNKY6nCOElXNWabrG4VpdtVbruk2+N0u1FK17g6W3SMTv583crIZWuXGLlz+Yw6bG19c2Xbu8bhGnN4zrS9xyfdGjuNETliFI3DFaNsvnPtdNyCfbaxu7jmbDJfcUxXTt9x+cSqErM7rk/snr6WbxPXMVBF93Ez3djmOThOhTkMnfeQHK7YrliJx7w4Y1j2l/VzPdDX1teaq2Qctj626ylbrKI4tusM27ya4ypbPzO073HkylFUstnG94HLTXwedK6eurPttjjddRj70vk3r+FMneuwfHuzhrJ9Pdeejlyd65rJP4uu33Uu81quDVWulXP7PU5Vvmczr1iBp0bfufO95yvSxNm6Tn6b0LkCAAAAAAAAAAAAAAAAAAAAAAAAAAB5tmeyAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANIYHHgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgNaND7qAUZekr2HQxtOtlArvE0Vh7RNHjjggni2UK0RZDbb8tnU316FsDs25stVh1lBhWazMnLZ6fY5zs405FzqXHk+WO7K0746l9LYojZEvtCdW+t6cY5XFyddS1Ka3b5RuN3OX19SdwxpD1xPYzpZbpHOcRI62vrly+4xYZi5bO3fOToQ4jWpbD2vsNEaU9i9ah9AYNq65mxyHZ5+StSzrN5kv7Wupt6yvT+6QWFVimrE1Vw5rHONIdK0jOsy5qxVLVY+VBPYNyeWK7YqVeMyRM4Zlf1k/13nY1teaq2Qctj626xRbrKI49uuM8DnJ1+CXfzJXeQ6zn09sZ8zCSrrj9W6zfXPZYunt5hzruS273tVtrOuj8jW55rYojq1u329o27VGd+zOdVRxDN/7lCr3QFXWuCdvAzFEwur3vZf1zV2katc6OXtqGOClQD/+vmBY/k5iGCUqCr6umApGcUwAAAAAAAAAAMDTRPoaNaM4JgAAAAAAAAAA4IffP4CR1MYzcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHLGB10A+idpIEYTT8hSqnx/FIXFSxzxRERiR0xbCJ9SbPltOW3rUDS3trky5yi0Bh8+8ypS77jSfeM0l23ts1oiv/aTfSZ3xpHq7tpDz3GSBouNoyE3D0abTt+0LrMGKd5uDCeXQ6+ZSuuPIqMeI6atnUnPx2SOfNveeqLidp41FeXVscxctnaunEWU5biwxe6tVdJ2Rft8Y4SPw3qc2Obbs5bC+qT4OPepMzR3lWPTN7aZQ/M5TgrjlHzbR9YzxGgqm4vasVX12OZx0mQu39iumInH3DljWPbb+vmcg619bblKxmHrY7tmsMUy45RdG9rm1XdcIZ9gcz7NHK7YRfPjjOnMaezvydDJa4tlO07sc+tuo2Pq+mzznF0rWfYX7bPFDL7mLri+KrtmEXHfp/hwfS5d19hBx6wrVkCw0Gt633uFblWnt0quntx9OJ03cb8NAAAAAAAAAAAAAAAAAAAAAAAAAMB00MTzawAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEqND7qAUaeUSKLaiR1H7cQtkwS2r/JELeU5X1HA+G1r4JpDn1JsIUJzFs2tbf5sc2TOSZPHnpnTdiz45LTNgc6RGO1sc9xdU5I2itNt5lzopll9ur0RM0l3xGnwouMsa2McIVndKkr7qtx2SbfH6XajlFx9euzKiNWTS4rbmftzfY06svKMeqztHDXZ8pblctZWkNO2DspyDPTWkMY017Hrrfs7whYjbO5E7Gtmne8KOTq5iufON4Ytd5VYdWLbcmm+OUtjWr+B0hxeZ4rh4xpXIzlU/RxJYAzfnCFxXTETj7l0xrDst/XzuR6z9rXlsoyjbK5s5/wqsXraWmL4jqvsk2m2NeeziWPXZMZ05TTntmg8oceN7fpR5yq7Bjev0Wxrr2uyxTLjFNZh2W7Tc+2Ubi9bxdB7mip8r8N9mnnH8mwXMv7Q+4k6Z8Wq9y6+4w7Rj2OkjuB1mZqXK32hpN5xO6xGcUwAAAAAAAAAAMCTkuH/wasKfgABAAAAAAAAAGD64vcPYCRVeR4NAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAkPFBF4Dqkhae2BZHzcYLeVBe6NO3lMf4I8d4XHPoMx+2ELautpxFuWzzZ5sr25y45qGMGdNWU8jx2NM2rc86LrNfSXtdX6z0+8nGcZRP2hMrfW/OVff4s9iWNr19o3S7mTtfU/feKGuTz2WLZTLbZTUXtDXrsLG1K6vJzGuNkf5pHqK+tU3mStuKWV8aO3LVkI7D4zGYoTFCxlGVa31E7MeNbe7MGNb+XTlcY+yupyxmUWzfHHVzVqGs3/Z5PsdX2zW0zZzvUEVr3nTOkByu2Ilj3n1qs9VTZy5tfa25LOMomyvbOT80VtF1i21efcdl+6QV1eB7nerKUSe2OS5zbuvk0tt7r+Wiwlzm/qK+ps41mmV9jDhFtVrrsB1nluuuMk1c81eJ283VNCiWZ9uQe7HQe8sqZ7W696++4/bRzwe6t3HfDgAAAAAAAAAAAAAAAAAAAAAAAADAVBb6jBkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIBgPPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC0bnzQBWC4JKpavzhqILcrR4WYyjKeyLPesvlwjdnW1dYtJJdtrmxzZJuHJtjqLktpnYP0z1gZ7y0duudB97GtbaKiNJYqrUHPVZIGigtGko3ZaNPpm9ZkqTfkWNa59ByodBxROg5bTFu7wrqMucmG52hny1XGGsMzp85VlC8R+5r51ZCOo6u/Of+uGL7Kx5HmtPSxzXNZTc6+jrnzWeNE5SfJNTchx42Zo4l5D81dl7J+60wt5hzWYR4zbdbhk8s3ZuJYS584tnpcfcuulWx9rbks4yibK/s5PzC3EadsTkPH5dPONo9mLrOvOXxzf1HcnpiW3D39HLmKYulcthy2ec6uX0rWQcfUx4Dt27MTK/++J17BdmXZZ4uht+trv55rpq62rqOl6j1QYV2e7Vw5Q67ffY8r39y5OgJjV8nRk7OB9Qidk6DY/bt8QB8kKqp1TTKsRnFMAAAAAAAAAADA04SM5n8KdWLQBQAAAAAAAAAAgIHh9w9gJI3ixxoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAyZ8UEXMOpU+hqEqI+5koBBxhULS1xxA2IpR72RR422MbvGZ0td1s3MZcthm6MmnmxmxraN3+dQ0G30MHQsc1x6nXRuvd/WPtcn3Ren78011TUkRjHmXCVdKxOnwc1Yuk1sjD6rW0VpP2N/uj3u2m7OTadtGivdoYyYWS6jn9murK1Zj72W3rq7c/nUZY1hyVmkaGyTOfProSzHgK2GImXHXK6mNHdkHgsluezjSHN6tvfh6ms7lqvk9p3fKuNJVH4hfNawLHe3KvM6yormqCpz3arwrSckl3dMxzeTTxxbXa6+tvN7WT9rLss4bO3LrvFCY7muv/JtA2N7tvPJZfZ1xS5an56YzpyuKv1zdbZbYqj8fvPY7u6XGH16Y0VGLEu7kji2ofseL+Z5vuic7Xvo+R41IWcK37UN+Xy47otCc+fqCGxfKUfNU63v+INiDsHpv80ShmB4AAAAAAAAAAAAAAAAAAAAAAAAAABkmngOCgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQKnxQReA9qgGY0UNxko8C4sDkyY+MT1jKUuNkUdNtvG5xlM2LWbX0Bw+c1NXleNN99FlJ8YG13rl5sHRR69pkjaMo+KK9VzF6e6iNe/ESttGxfvNvkpNbkjS3LrWRHUa6rrMubHRMaO0X1aTo19ZW12PqxazXZW6bDF6jw3/XDa2delp1zXSyDiy9TGn19xef2TpHz4O29xZ25fkcPXt5ExjWD7ZvnPuW09oTFsOzZXLpw5TlbqmAtt4m2SuT4jQ+nxzhcRNHN/EPrFsdbn62s7fZf2suSzjsLUvu14LjWW7riqaW9vY7OPya1c0l2auOseqr96cxn6jfVlNPbEs7fQ8u67BzeubwjZGXba1dcUq6qbrsx8veUHXibpPjetxF997HNv4euK1kDurIax5tRwNnDabvIcJrb+O0bxiGF1K+nO/3G8chwAAAAAAAAAATGOJiEwMuogWjOKPOgAAAAAAAAAAwA+/fwAjyff5LwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJWND7oATA0qsH3UQM7EkTSukMT2kDvfJ3+pkpoiRz228fiMw5bW7Fonhy8zh6021/qJdOrSTXWZibEhTt8nRr+iYWVtdB8do6e+KI2lcjnNuepe8yRd5NgYdadey35dk63Wkrpsc9NbZ5SWoApj6/3dbZz1OWqxtSvjm8vav2sczrZSvB56TfVntiy3SmNEjm9BWwzf/nWYa2/WVFSXq28WwzKHZv+yGGY9rnXrjukTt06uEGZdpip19oOr7jYkFXPWqdU3Z0iOxHE144pVVpOzb4V+tnzKMg5b+7Lzd2gs23WTObfVxuXXrmgufY8DVw4zdlFc21q6rqdcucrr0NvL10XvL7u+1XWq7H35PFvX3IhTtM9ag2N751pn8s+ya3Kf61OR3uss337dyuY1Fzs8tHc9Vc5MoWP1HWdpzrr9WzwFD+fZHQAAAAAAAAAAAAAAAAAAAAAAAACAqc33OS8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACV8cAjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQuvFBFzDqEhFJ1KCryIuj9nOEDLlqOT7z6jvWxBXHI4Zy1BNZaikbh6t+s6uteZ25Cj1+Q9rrtjq37up7THSnMmP15Er/jNN25nroWIlRRNHaJ+nOOF10M5Zrv1JRul3lauvOl6Rt4rSNOTeu8XZq6R2Hmb+sbRHbOumaJ+tSlXK5xl3a1porbef4ZjLjlbeVtG21GEVzpdVdn7J8PmOrylZ33VpU11y5Yttyaf0Y/3Rizm9VVeYuNHdIjsRxFnLFqjMvtuuRQR5fqmQ+bGO1XRO55tYrtm//npr8x2HmMPf7xPbN30Suzr7i7fqc5bpW1bl8ruV0LNsx64pVtN2sz3V9buMap4j9ujzL7Xmg+eTqie3bLiB2aBl17k+rjDnLW7VfC6fvYbhF7+ffE1Sd++lAqWgkr+NGcUwAAAAAAAAAAMBTIqP5A9EojgkAAAAAAAAAAPjh9w9gJFV5bgIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAECQ8UEXgP5LVPW+cdRcHZqrnDopXWP1HU/Zw/F8nxqmLLVEJTXY6rfVbRuuzzBDjwuzeVl/c+zmmHVfPS5lbNcDiNP3ej2K5sHWx8ypslhRGqt4AN1r745VXFe2X9eUbY/SeP6Tbwyvd+4sMXPjcORPjHaJKp4js5ZcDKOPby6bslw9bT3nVVnWszh/GtM48s35D+0/GcMyv4FzVuV48u2bfU6c39j+ddjG7RNbCx1rYvQPyT3dmXNXh7mObdYRkitxfMO4YvnUZotR9SG8ZTmVZTy2PiHXAbYYtmsd29wWzYc1tmctIXNp9u29tgk73opym2P0nWczVuFcWftGpbn0fvP6pUjnmizfp6edEcvM3XNtV9C3U5/x3thvnnPN9j7X5rZxNMn3WKxybxbapVKOinNU56Hide5TTf08wzdZNwAAAAAAAAAAAAAAAAAAAAAAAAAA04Hvs1oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAqGx90ASNPiSgV3i2Kmi+lCUngWOIGxuGbskoq23hC6k4c+11PFSs7PmzHQWjdPnPoO2Tf9Sgbl7lPj1OPy3f+u+dB97F11TmTtIG5LomK0jjKWUsnVtrHmJWsLtt+XXNBneY+s66sBp3CyKnrVWm/KOpdCDOHrW3VWso0las7n7WtkSuR4vXQ69k5Djsj6c2fxnR8Euz1+/Wvo3TtLXX59BWxz2GVWL41NZHDlbtIlXpGSdncVKVqxPStxzdHEnDV4IrpU5sthusawtqvJKeyjM3Wx3ZNYYtT2NYSwzbPIceC7ZNojsc2l0W5zL6+n3YzR8g4zHk2c9rWx8xRNM7susQ63+X7zRpVblv5sduJXcw2t0XHjBnDdmy6rhddnyuR5p7665Mraxt4WqlyFgrOUeNUFzL2nr41T7FtnqHr1jZo1jWd4uNqUyL1judhNYpjAgAAAAAAAAAAniak2r9MOewmBl0AAAAAAAAAAAAYGH7/AEZSU/9fTwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKvxQReAYkq1nyPqw1PskoBxxDXrKUsVGtqnbt96E1t/j76248C2dra6fWqtesiFrLE1dxpDj0vHjI33eiFjo31pXbqPuV/8YuXGZ40VpbFUaSxznJ3tnQ1RpPL1ZXWkOaLyCTfnTseOHP1C2tpqKZgqe9uaueq0zdbL46ivOu9VhM6VeYw0kcuVs5Pbfw5D11rzWXMzh+ZzvLuY9WghdU0FtnE2wVwXX1Vq8s2VBFwRuGK66izrb7s2cPUty6ksY7P1sZ2/bXFc+XPtLDFCxmX7pPnWYOYKOa7MtuZ69cT2yJ9tD8xVxnadqNfWtl9v17l8ruXMPjZZ7oCazJi+15Zmu5BzcMg8hwq9Nq5yVgnOUePUVXWuGrlHqB+iRxN1VdWPe3wAAAAAAAAAAAAAAAAAAAAAAAAAAKaCKs9JAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACMIDjwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQOvGB10ABkep6n2jqLk6tMSznrhCblfoKsOx1etbX2LZ7vMUMtva2dalbG6rzGeZotp8x6r7+h5f3bl0Dj0eM0S235JDx0rSnnGkCuP4xMra6frSBrFxJGZxcnVEaRdV2CZR+fqyFJbcej503LLYvjV0cuRrKWNrWyeXfezGHBk5bEKOP5VmjRzfLtZxd1VtxvCdq6x9+qdtHYv6uHK5cnZyd3KYx3dorNDamswVIlHFB0iVOvvBVm8bVM1cVWr1zZkEnOldMV111pkHW19bTlUyLlsf2zWBLVbZeM1zvm2eQ8cV0ta8xgiZf3MqzByu2D65fa9vQ3Pl9zmO2XS/69pf7+6eB1ufznWT7mPJbdse0NZ2vvftX6Tu9W9ILi20S5UcWa4KfcuOsdJ+DZz+mjyDNlGPrzr30xgeSkW1r2GG0SiOCQAAAAAAAAAAeFJS/QfAYcbvcwAAAAAAAAAATF/8/gGMJJ9nqwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANQyPugCRl0i9R4WN6xPpFKBT4uLouZyJx6548B8rpAh4Wz1+dZUdry4jgfbupTNv898tsUcqzk+XZueu6zWqLh9N93UjJHtT7cnjljd02OLle1PC4vT4Oa82/Z3z4OtjsTYn6g0VqRydfocZirtG6V9zdih7cxayuopalvElquI79h7x5PW4vH4S++6LceIb/86yubMHHtPX0d9rv6T+f3m0ydWUW1l9blyab45q0hUyJmimDm+JmK2yZzfqqqM0zd3EnAGd8V01elTk+0c39RcVqEsc1Q2Xt/rQNu4SmN7tjXnMiSXb446zOsr35zmOHrH6c7pWh8dM2tf3jzXJ7t+srSzxfKp2xnDeG/OoGt/Wc4mhYauU0voPVmd+9Im5qxuiH7ct4TO6bBwre0oPsAeAAAAAAAAAAAAAAAAAAAAAAAAADB1DevzdAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwAgZH3QBKJf0IUc/nnqllH/bKKqfL7HkiyvGLivfN2QTNdmOB9cals1/E/PtyuFLjy9OY7lq654P3SeLYfTN5j/dbpszlcWJ0ji9A+uJZalXOWpSJeNUKkr35fNnsbJa8nUapWW1Fh1nZg4zdmg7XUtZPWbbrF3geLv72pg5rO30WouupbPPXJve+U7rNr4lyubdZI9RXL9trnzU6evLnM8ma/FdU1dOrc15qCJRHgfMgJhzV0edcfrWkXifnd0xXfX61GQ7f1fNrUrGZ+tjuw7x7S9iP9f7zrd9PP5tfa+Nzf4hn3Yzh7lOrv1lec26bOPpzVHUJkpj2mJEub6uudO1dedy90nrs203djR5b+Na06L9TX2TVTl7+H4GC/NV7FtlvuvUKVJtbpquoUgT9yp19ePeHuGUtHPMDdoIDgkAAAAAAAAAAPiaGHQBLRnVcQEAAAAAAAAAALdR/Z1gVMcFeOrHs24AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMA0Nz7oAjB4SY2+bTwxS6ny/VFUPXbiiB1XiG0L6RuqrCbfemxr6LM+tvmuM88ixTXZxmobp64tMdrpOD7zo1PammY50gbmnCWq0zOOlGesKI1VPOBE7PuzsWYxJ9tGUfnBq+vUNZrj7p5717yZNdRtV1SPtZ3neEXcY24ih15T1+dBpVkj6zfCJLNmnxi2PrZxhKxLSH1lOQtjlRznVWOadWq2el2U6l3YkDpGWdHc1GWuWxt1JJ5nX5+4rnpdMepcX9lyq5Lx2frYzr1lsXLtSj4Stvk258Y+Hn+2+fTNVRjT7OuK7cztndoZq7Pd7Gcfn+saOouRtlPZe/ecmddktrFat3u0q/rta8Yqu87pxzd8yHHQzXf9CnNW7VcjZxNzWSd/tzpzV0Wd73cAAAAAAAAAAAAAAAAAAAAAAAAAANDO82oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAByeOARAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABo3figCxh1iVKSKBXcL46iFqppXuLZrskna/lMZ9XpSyyx4wrxbGWGhLLVo7nqsq2Pz3pUOGwr0+PU49F1hxw3WR9lvDfmKJvTqDiHGafsWHLF0pSjpu65jixzoFSU7leF+22MEnN16zrM2Fk7Rw22dpM5ojRHvq1Zj7VdxfGG5DAlaY/Y+untausb05hrs79PjKq6P//mvNnWMuvrGJ+rf74Ov3kNidmTw3M9fChV/KGvUtdUYBtvE5KKsavUlHieXdscr+ZzbWSrwzZnyjK+sjm2XUNUidXT1hLDd37LPk1mHbb5NHPZ5y48R09sZ253Xmt9nuMtYltjvT7m9YfrutJs373N2scRq6c2r7kqz+lS1L/KtXxoDl91rrFDjo9cvzr1Vu9aO7epzfuTqnM7bGx/91Dl7ySmCyX1j/NhNIpjAgAAAAAAAAAAnkblxy/TqI4LAAAAAAAAAAC4jervBKM6LsBTk8+hAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKDQ+6AJQLFGq9RxxFLWeQwt5uFwTT+FyTV/o0BOP5Yg9Y9pCVVkNW12uWsrWY5BPQdPj0fXrOmNlvI/y7bu3uWKazcwc5rHRfSwlae84UoWxzD5JGiy2rHoWr2u/MurI6sv2R+l+Vbg/Ufkay5hzY8bu1Fleg9muSj3e7RrMYRtHEXNdrO3SNY2sn3Q3WwzbOEPG0ZPL0dc1t7q/T/6i471KTaU5VH6BfD4HvpQRu0p9g2TW3wZz/kNVqTHxPHuGxHaNo85c2vrWnbs6bLlDLkV9x2ULGTJ+3/k3cw1qjs28tmswc7s5/2XHenbd4ft5MGrzWWtdn7526J3ffC1mP1sNRTFC+Zyjzdiu68YmVL2dq/NQ7jr11x16E3PX5i3wMDzsvB/3+AAAAAAAAAAAAAAAAAAAAAAAAAAATAWDfLYJAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACYJsYHXQAGJ1Gqct84ihqsJC9x5W4gh23odYaVWGLGnjHLViO0rDq11J1/V/9uZpm6PF2/79wV5Y/TGLY1teXQx0aSbi8br64/m29Hnyy2rrHG8abUZOcoUvmYWU1RmkPlau0qM2POhRnbt4Yytnp6azHatZDD1DuXnR5x6SezN6e9XRqvoBjfGC5lc2UeH23wXSs9v6651fG6+RwHuVwNzW2Rovq6hdZah6uWtiU181epP/E8M4bEdo3DFavs/Fd1jZRlnGW12q4BbLF62pUcur7z7ntMlI7DeG+bQzOGWX6dHM79RrKyT73veMz5N+e8O6dtrczrDdsxYauxO67rus4W2+znW4MP17i1suv6uvXUuI3q1FCnb936B5hbpJn561ZnLivla3oAGKhERbWvZYbRKI4JAAAAAAAAAAB46vcPaP0yquMCAAAAAAAAAABuo/o7waiOC/DU5rMPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAARERkfNAFYGpKlApqH0dRc7l98lWM7RpWlWEklphxQCzXbPuGstUi4l9PlQcFluXtppvpUhJjQ6zyNXTXrNvaxmHuN5tlMcvqy/JHaazigWWxdPv0wImNleyMr1ONbqNz6V0+9RW1S1RvreY826i0b5T2tdVgtitra9ZTtZayHD19jRxFc2KTrbVlXew503ota+7zebPH8K/fV9H8tp3TnFsfrjqtuVR+wpsch41Svt/MU4s5l3VUmaPE84wXEts1JlesOg/QteVWlnGW1Wo711aJlWtXMufm3NjH4893Ps1cdXL0jMO130hWlNs6F45cne3lOcva2ugQurbOdY2bzm8rw3YN7TdX5bkDbzsK+4Vew1fNWaSJh2z7Xkub6gyjas4s95DNoTNHkwUDAAAAAAAAAAAAAAAAAAAAAAAAAIBSVZ8LAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4I0HHgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgNaND7qAUafS1zCIBpg7Ue5ZiKPmKkxcuSrGLRtGaPmJJVZcYRpsZYWEarIeF9fRoPfXSa2PgTgNZq6PzqHHbY5Tr3XStd123Lhi9bS31CQikqSjjtOoZttsXFmsKN1fPquJ6iSL07bmPNvnIp/DrKE0r2fb3lqiXK0hOUL65mrwnMvCGirm9Imh0lmJjE+OtX3JOFzr4ZoDn3HWmUdfdXM0sV7TRfd3R12qRqzE84zgm8NnXK5YrmuNsv5Nzqs1v2XObLk9LtW62nquh2W7WUPZXJq5fOfOJ0edY9Inpy1v0Xbf+S9rl127ZPX4xbT1L4uht5v1uD4XrrhFMXv6OmIXneNCju8QvuP1ilWjxiaGVzV/k3Pb5Hz2xG7rIKhgEJUMz+iHj5J2j71BYc0BAAAAAAAAAJjGEhnNHwtGcUwAAAAAAAAAAMAPv38AI6nqc18AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC8jQ+6APRPEw94ixqIYZOo8grjqLnsiS1HjZi28kPLTkqmIQ6MZQsVEqasHhF7Ta5+PnSILFaaK07fd6+jrkO3ddVl7u/JUdRX58ryR2ms4sH2tE8PhthYme65ysYh+bb6+Iqy/fk6lYrS/apwfxlj6M45zOq21NBdh61tosrnTiztynK49I7TrwYR93rYc6br4rHm/RRyfFRlHpP2Woo/F03msOZWvZPvczyMsqI5qUvViJl4nrF8c7QxPlOd8SrLeG11N3KutcTwnfvJOpqff3MebX3N8s12tmu+Imbb3hrKc4fE6mw3+5XnLGtr07meinI5dY2OS3Ajhq0W433AXGVtPGO7tHGOC62hNFaD18gDyd1AjCbnU8R9H9mW6X3FAAAAAAAAAAAAAAAAAAAAAAAAAABAdW0+6wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEBERMYHXQCmFuXZLmohd6LKs8dR/ayJLXaNmLayq5SbWGLFgbHKZjK0LFtNVZixQsdVRq+DOe86pS1V9/rZ1ixRkzviSKXvJX1fraaQPvqYjbP9UbpfFe4vqjeLnf4ZZe3SvlFx7CxeQQ5bW5NZi2s9cvU6xmobp7MG1cnuqr9nPTxzhtST5UpnJTI+wdb2AeMwudYvZJzex0I6vtj7TBOew0ei8kdfnbWcCszxNklVjJ20cBURMk5X3bZrhTp1KMuYbe3Lzr2hsXralcy/OTf28fjlLppL3+PGN0dhX3Mcrv1Gsjq5fY+fnpxla67ysatem4X0d1yWd9qZOQr62WKZcxV6HVw013HJvqY0dW1cJ0wTNfiusbWG+iX0xqxblIfRPuujiFLVr1mGWR8+LgAAAAAAAAAAYFhNSDv/Aueg8fsHAAAAAAAAAADTF79/ACOpznNcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvIwPuoBRp5RI0ucnq8VD8HS6kCE3VW6i3FnjqFq2pCxmpYiTx0aRKiXajrEqx4JtFts8rGz16+16HFm79H3c1U+vkbWtI3ZPLZZ+uVxpW9eame2TtENcMNs9YxZ72zJKRWltKlfDZCydK40dlcc2azJjV6nD9bkxlsG71iJmXzN2UKyq65H2izzWvJ9c6+Faaz23Iu618T1uqs5xSI4Q3WPsVuVYHAa28TRB1YydVPhU1s0ZEqvsWsDVv815d7Hl9rhsStv51+77qXDNZa6tkb9qjqJxuNqY1ydmbrO2onHZ5s+cf9vxX9bOtYZ6t65Tt9d1+hwDnRjF+80x+973lOUOjRlyHg059nxyNqGJUFXr8f0eKM1dP0QnVhMFOQzz2bvNvzfow9QCAAAAAAAAAAAAAAAAAAAAAAAAAOCt6rNaAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvPHAIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA0LrxQReA5iWqfow4qh/Dl6vcJktJVHG2OKqeJbFsr/o0MUuJIiISWqbtWKiyvk2sUwOHZm16fs251LWZ4+he39jSN2urJnfEkUrfp/0ck9O95jq22TdJK4vTSs1x6Dp9jjuzrVm3dS6MmlTaL9Lj7WrrqsNVg7Wf6lSV1WupI/QzWFSDGbtOrFC2GCpdmcj4RJXlrDqOEE2MORfPOOZDKJU/etsYd6KKP9hNjb8uW31tMOc7VFLhTO+b02ce6tZfp7+yjN1Wd9k1ni1WTztLjJB18D2+zHa2a6aiOWwqR9319aqhYJuZ1zZ2c95ta1y2Pjp21XsAs39ZmLLr1Fw7M4dHP3OOQsfje93lE6MNTYSuW5/v+pXWUD+E9Z6sCYM8C7d5/KA9iTRzXA+bURwTAAAAAAAAAADwpGQ4/kU9AAAAAAAAAACApvD7BzCSqj6TBQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwNv4oAvAcEoCn3AXR+3UIeJ+2F4TqRNVnCWOqkdPLNvrPGXMLLNqeWXrW3Ut23wooq5X15bV31VrnG7T825ra86/Gdu1PdfGyO27Hnodk7RD3DV7yojVM3bJ9+lpr2vK4kXpfvcKJWnbOG1bMM2W8fTmqFpH1Rp8NBnbtQ42Ku0XFXxifI65tpjrZQo5jlxCYyVdKxRX/KZpsn4XfZyViSvW4RO7n1TNepIKnz7fnE3Ole18XqcG1ci3Wnkc3zmwrUPRXNtimke0b+4mc4Ssk9nWrMO8Tqo6vuJclnZmTks72/ZcmyxmlOuja/GJYasr2+7Zrqc2j3au+ddsqxB6H9O0uumbqD9kja111O3fRBGGQSztoI8nAAAAAAAAAAAAAAAAAAAAAAAAAABGUZ1nrwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHgZH3QBo06l/4SKJGqhmvYkAUOMGx6aT+qqKRNljx5H1aImtngVYtnKq1iaiNjXsul1GzQ9d+Zc6eEXDVevXWzpm7VTkzviSKXv80HNtU66ssWe3xe6j25vjierNf1TqU6OSNdltLEx50SPx+eYsNVRt4bJOvLzXDV2T81dc2WLHRorDuznE0OlsxEZx0xZTtc4qs6Zb/6QWgpjG8d9qKrr2LTuY2yqUA3WnFQ4K/vm951bn3i283VIjFC2+kOus2xKLmmMdv7j8i3LnMs6OaqucdF69rQxkrlyh4zLnH/b58C3XXf+qseH2b8sjO34cc+Zfx09Od1d+66Nmvr5+e7JXT91J1bVIgr0Y+2bmPdh4Pt3DVX+TmK6UKr6Z2iYjeKYAAAAAAAAAACAn4n0NWpGcUwAAAAAAAAAAMAPv38Ao6nOMw0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC8jA+6ABRTolrPEUnUeo4iiefQ4gbLs6WskyJRxVHjqFrUpChWpUgiltKkYmki4l63JtdL0+PQdesaYuP9ZKN0X7pNz+eYUZfebs6tGdvcXrSvJ6ZRr4s5vrJ9tvpcisarVJTGVoVtknR/HOUXXb+Lsnb5mnTc7tg2VWtoghnbHFdQrLRXnEYpW9Nuqitb1Ifv+578xvy33a/pmOa8V82pNTmeUWPOVR1JhU9Zk/mbiueKkZTsV5Y5sPWxnXttcVz5c+0818M3XlHbomsb3xy+n0ozh7k+vjWEcOUsy2vOu22NzXa2a7tcmyxmlOuja/GJUVZTd6xQZu6iODqv79rXOX/bYrXJ9x6ojO8aWmuoX4L1HqiKNue9ifluSj/u7QEAAAAAAAAAAAAAAAAAAAAAAAAAmEqqPk8FAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAGw88AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAArRsfdAEYHCUquE8kUQuVFEss5cUNlmCbgTopElUcNY7CoyaW7VWfVGYpTUREKpSXY1svkfprpuvWNepcYx5xdVm6j28tul9Rc70usVGXbw06aNE6JunOOO1lG7seh6t9aV0qStuqNFa+riTdH0fl3xVFc+uK7cusobuSyNamodxFzNi+fOdysq2kbf1iqHQmIuMbTbf3zZvrq/tZ9vvMQ8iYqzKP/6pU11yFru2o6Z6LpiSBZ9UqNSQN1m079/azhiaVnfvz7fzrr/opMXOEzJnZ1lynoPp76jD2O3LbFB07tvnvyWlpZ27vzlF27VVGx9D9y8J41+/YX1qPpQ4zhu0abti+tauui8n3s1tYQwP5bfc0odpYn6bmuKoq99EYXkqi4GuVqUCN4Jja8rvf/U7uuOMOefDBB2X9+vWyzTbbyMKFC+XVr361xPFgnxOfJInceOONsmLFClm5cqXMmDFDtt12W3nZy14mO+20U6O5HnroIfn1r38tDz74oKxatUqe97znyfOf/3zZZ599ZNasWY3mAgAAAAAAAAC0KxGRiUEX0YImfgcFAAAAAAAAAABTE79/AKOJBx4BAAAAAAAAmBaUUnLuuefKV77yFbntttsK22yzzTbyzne+Uz796U/L7Nmz+1rf6tWr5XOf+5xccMEFsnLlysI2u+22m3zoQx+S97znPRLVeIL1T37yE/lf/+t/yTXXXCMTE70//8yZM0eWLFkip512mmy//fZBsbfbbju5//77K9d2zTXXyH777Ve5PwAAAAAAAAAAAAAAAAAAAAAAAIbXYP9T5ZhyVOA/bUiU+1WXsrzqSJTKvWrFsrzqUKr41YSm1susqXttsngSNhdmzLKaXPXq3DpmoiJJlPv/eFg034lEkkjU08ZWi7O9+M+N2c4cR8jnQalIVFdfM7Zrf5t8xuW7hqaQz4+SSJRU/z+oDjv/z0H+WAjKYRz/ddSpYyprY9xNros1R4PHl+v7p84c2T7ntvpt55uy7wvfufBdl5DvP7Ntne9y87u46vew7XyTO+cY8+zK7TqH5cbhOK+Htiub08610GS9Va/hSq+BSvKXsV2PVKmjqev8JjR5H9LENXgT9wR171XauX9q516vyKDvcQH0z1/+8hd505veJO973/usDzsSEVm5cqV84QtfkN13311uuummvtX3q1/9SnbffXc5/fTTrQ87EhG57bbb5H3ve58ceOCB8vDDDwfn2bBhg/zd3/2dHHDAAXL11VcXPuxIZPLhS//+7/8uu+++u1xwwQXBeQAAAAAAAAAAAAAAAAAAAAAAAIAi44MuAAAAAAAAAADatGbNGjnkkEPk5ptvzm1//vOfL7vttptsvPHG8vvf/17uuOOObN8999wjb3rTm+SGG26Ql7zkJa3Wd+edd8qBBx4oq1atym3fZZdd5CUveYmsXbtWbrvtNnnwwQezfT/60Y/k0EMPlZ/+9Kcya9Ys71wf+tCH5N/+7d9y2zbddFN5xSteIVtssYU88MAD8utf/zp7ENJTTz0lxxxzjDznOc+Rt7/97dUHCQAAAAAAAAAAAAAAAAAAAAAAAAgPPGpdoiZf/RBH/ckTQol78JE0X7htzuvOUdloQkMnqjhaHFUvMjFjVY7UYSlTapTZmO51HjPq0XXrOdFrn/VJ37vmqHv45pCz2Gkj3zkJrSFXj5FLx8rGlwaN08rN9pNt8nmVitI25Z/XJG0Xp+2MYeTWI/SzZtZg1mjmLs5v1OeI2SRz3q3tCsZhbytp23q1leV1rb1rznT/shihfI/HIr7rEFKH1tT4hoU5viYlFc/rbdTU5ji1pA85qrCdv3vb+ddvhvQdu5nD7Bfy6TKvdXpiO/ZP5jfa9OwPi2nuz+8zx+7XzrZ+IfcVui7zmkjHsIUqO3Zc+X3r626nCrYNWpu1+H42y5Qdc6X9mkieanKKWp3vRittV5vzMEyfr2HTz7+z6adRHFMTjj322NzDjjbZZBM555xz5Mgjj5Q47tz13HjjjXLMMcfI73//exERefzxx+XQQw+V3/72tzJz5sxWaluzZo0ceuihuYcdvfSlL5Xzzz9fXvWqV2XbJiYm5KKLLpL3v//98tRTT4mIyE033STHH3+8fOtb3/LK9ZWvfCX3sKMoiuTTn/60fPzjH5c5c+Zk2++//3458cQT5YorrhAREaWUHHPMMfLSl75Udtttt6DxbbvttvKLX/wiqM9WW20V1B4AAAAAAAAAMCmR6r8pDrNRHBMAAAAAAAAAAPDD7x/AaGrjuQ8AAAAAAAAAMBR+8YtfyCWXXJK9nzFjhvzkJz+Ro446KvewIxGRV7/61XLdddfJDjvskG2755575Mwzz2ytvjPOOEPuu+++7P2OO+4o1113Xe5hRyIiY2NjcvTRR8uPf/xj2WijjbLt3/72t+WGG25w5nniiSfkM5/5TG7bF7/4RTnttNNyDzsSEVmwYIFcdtll8ra3vS3b9vTTT8vHP/7xkKGJiMj4+Lhst912Qa+NN944OA8AAAAAAAAAAAAAAAAAAAAAAACmBh54BAAAAAAAAGBkfepTn8q9/+QnPyl77rmntf3mm28uX/va13LbTj/9dHnyyScbr23VqlXyz//8z7ltX/va12TevHnWPq985Svlk5/8ZG6bOcYiZ5xxhjz22GPZ+9e//vVy4oknWtvHcSz/+q//Kptvvnm27Qc/+IH87Gc/c+YCAAAAAAAAAAAAAAAAAAAAAAAAbHjg0QhJVP3XICjHP01qc9zKeFWvUfW8KscyXk1SqvjVD0W59LxXXdOQ+n1z6JiJiiRRkdexYdaRSCSJRH6Fdddo9CtaI/O4UCoSpSLr/mx7Op6sn/SOy5wj39jDoOhz3DNmYzz9ZNaiKYlEVThW+sVWdyu5Kn5uyug1H+Ta19GP+qvOe5WamjyeXN9HVedMfyaLPpe2+m3nF1ccn7nwXZ86c+s6rwTFMupoMnZTemuyX0/0nheL27nO1bm2oq996n0eyq5rnJ8P8bvubfK873sdP+h7orrXyuZ1fJVr+ibuKcx7nDr3Om3Nd7/uJV2m4r04MF3cf//9uQf0zJw5s/QhP9p+++0nr3rVq7L3q1atkiuuuKLx+r773e/mHqS01157yete9zpnv5NOOkk23njj7P0111wjf/zjH0v7XHDBBbn3J598sjPPFltsIccdd1xu2ze/+U1nPwAAAAAAAAAAAAAAAAAAAAAAAMCGBx4BAAAAAAAAGEmXXXZZ7v2SJUtk7ty5Xn3f9a535d5feumljdWlmfWZOW3mzp0rhx12WGmsbr/5zW/k3nvvzd5vs8028qY3vckrl1nTFVdcIRMTE159AQAAAAAAAAAAAAAAAAAAAAAAABMPPEJOosJe/aAc/zShjfEpy6tafarwFRyn5NUUpeyvftLj0rlta2obf1F72xr25LLEtOUoWgdzzhKJJJHIvj/gmO2NbeaORKnIuj+EWZcrtmt/oqLslfWR/Lr07PeM2QTXOpVREomSZuoIZc6RyecYcMVouqZB0/UNa539rM887n1Vqc3389qXcTf43dEk3+8d3zkquoapOnbX93et2OI4nxRcC7jG5YrZ2d4757bPhev8bsvtc543r0PM964YTV6v2WLpGrqPq95rhbA6Bnqf0uB1bxPX5nXvFUTq37u0ce/Yj/tBU+h9cb+OOTSv7HM81V/ouOqqq3Lv99tvP+++Ztsf/vCHkiTN/S1KkiTyox/9qDRnGbPtlVdeaW1rzsPrXvc6iSK/666XvvSlstVWW2XvH3nkEbnpppu86wQAAAAAAAAA9M/ECL8AAAAAAAAAAMD0NOjfKPj9A2gHDzwCAAAAAAAAMJJuv/323Pu9997bu+9LX/pSmTdvXvZ+zZo1ct999zVVmqxYsULWrl2bvZ83b568+MUv9u6/ePHi3Ps77rjD2rbOPBS1L8sFAAAAAAAAAAAAAAAAAAAAAAAAlBkfdAGY2hLl1y72+w/GV6LEXkQk9RKXja/qmMyQdSpMVD5aHFWPlhjv23gamrLMZ9Wyu+Pp+l3rorvotTXb65g+Ndli2NtHafvJjkXdsnFY6knSXnE6kp79Rk1m+9L6enJHaWxVuN8cT3eGFj/yQ8WcI1967kQ682dvq9sVxzD7q3T2o4I1t/Xpp9AaVNdcBc9zwPFfV3ed3UJrbiJnvyQVP+mDrluk95xXpGqdqsFvQFusJKA233UKidmbI8+cu1ZjO3P3xjQ3mfWF5ixju5Y018V2jWT2b+IbxbwW6slZoY6QWLb2tjkYJk3WGHIcWWNULKjJqfa9H/RRdl/XlCbrBTA1PPnkk/Lggw/mtu2www5BMRYuXCiPPfZY9v7OO++UhQsXNlLfnXfemXu/4447BvU3x/LHP/5RnnrqKdlkk01az2XGK/Pkk0/K+9//fvnlL38pf/rTn+TJJ5+U5z73ubL55pvLbrvtJvvuu6+87W1vk2233TaoJgAAAAAAAAAAhs29994rv/nNb2TlypWyevVq2XrrrWXBggWyePFi2WijjQZdHgAAAAAAAAAAADA0eOARAAAAAAAAgJGzfPny3PsttthCZs2aFRTjhS98odx0003Z+z/84Q+N1CbSW98LX/jCoP6zZ8+WefPm5R7ItHz5clm0aFHjucz2IfPw+OOPyznnnJPb9uijj8qjjz4qd999t1xyySXy//1//58cffTRcvrpp8v8+fODagMAAAAAAAAAYNAuueQSOeOMM+SGG24o3D9v3jw58sgj5bTTTpMtttiiz9X1Wrt2rey6666yYsWK3PZjjjlGzjvvvMEUBQAAAAAAAAAAgGmFBx61TIlIMuAa4gHnFxFJlLtNHDWfV0lx4kjqJzPHVLV+s8I6lSUqHy2OqkezHbdtHE/K4/gIla1POgVjdeNIZ42N0D30eJK0QZy+ty1HUQ5bTB0jSbPHaTU9+1U+XtJVra2PjVJR2k6lsXQcnSutJeoMxJwjsx5XTNf+ory9OY39jYyjt40P37keNHOO2oxRdS6rMD8v/aTnY1QkDZw/K+X1nMd+zLdvLSF9fa6TXHzPpb5zVBSuztjLYhfFNa9DBvFZMnP21iTGfnuNtvXpjWFpV7BNz1t23WHNXq7J6zBbLH2MF+1u4zqwqjZrqXNPaF7jh2piWE18T2m2e7QmNFln0wb59wJDPC0Dl0g0sOubNo3imKpatWpV7n2VB+mYfZ544ok6JeU0VV/3A4+K6kuSRJ566qlaudqcBxGR9evXy3nnnSc/+MEP5MILL5TXvva1jcYHAAAAAAAAgOliGP6d1TYM629eq1evlve85z1y4YUXlrZ77LHH5Oyzz5ZLL71Uzj//fDnwwAP7VGGxT33qUz0POwIAAAAAAAAAYFjx+wcwmnjgEQAAAAAAADCili9fHtxnyy23rPTwnWGzevXq3PuZM2cGxzD7mA8OqqNf9Zl5quSqMg9xHMuee+4pBx54oOy+++6y/fbby3Of+1xZu3atrFy5Uq6//nq54IIL5L777sv6/PnPf5ZDDjlEfvazn8nLX/7yoBoBAAAAAAAAAOiniYkJOfLII+X73/9+bvuWW24pixYtkk033VTuueceueWWW0Sl/6Gbv/zlL3LYYYfJ1VdfLa95zWsGUbb88pe/lC996UsDyQ0AAAAAAAAAAABoPPAIAAAAAAAAGFFLliwJ7rNs2TI55ZRTGq+l38wH/Wy88cbBMcwH/RQ9PKiqftVXtC00V+g8nHTSSXL44YfLdtttV7h/t912k4MOOkiWLVsmX/rSl+Tkk0+WZ599VkRE1qxZI295y1vk7rvvllmzZgXVCQAAAAAAAABAv3ziE5/IPexoo402kjPOOEPe+973yowZM7Ltd955pxx//PFyww03iIjIM888I0uWLJHf/va3svXWW/e15vXr18txxx0nSTL538HeZJNNGv2PPQAAAAAAAAAAAAC+4kEXgPYlNV59rVOVv5qkLP/U0VS9quBVvSaVezVhkMdIGaUmX7b5N/fruc3WTOqPJzRG2fr6xkokkkSiTsx0nNn+gvlw9vHMDTtzjkMoiURV7Nu2QR8bSkWiVPW5qbMu010Tc1dl/RIVSVJjzXviifsYrnqc9eOzGzIfvmtWZ47N+TTnrun1C8udP/8VXVeZ9fl+x/WeN3vn2nY9YlsXM7e1f1pz2bxm1zw6ruXax1VDWR3OOgtidbdXqncebeNoglJ+rybUva8yr9+rXMM3cS9R997Gdr9V956rqLa27hmt+Su+gOnqhBNOkCiKWn/5PpwpisKvTar0qaqf9YX2C23/93//99aHHXUbGxuTv//7v5dLLrlE4rjzV9UPPvigfPGLXwzKCQAAAAAAAABAv6xYsULOPPPM3LbvfOc7csIJJ+QediQisvPOO8uPf/xj2XvvvbNtjz76qJx66ql9qbXbaaedJnfeeaeIiCxYsEDe97739b0GAAAAAAAAAAAAQIQHHgEAAAAAAAAYQXPmzMm9X7duXXAMs48Zs45+1Ve0LTRXm/MgIvKWt7xFTjjhhNy2s88+u9EcAAAAAAAAAAA05dRTT5Vnn302e3/sscfKYYcdZm0/c+ZMOe+883IPQ/r6178uK1asaLXObrfeequcfvrp2fuzzz5bZs+e3bf8AAAAAAAAAAAAQLfxQReA4ZZ4tuvHk7MSZcld7T9kX0hJPkkk1YM3Wa8ZqmpVieotKo7qTaDtGBn009T0SPU6hM67nqru6bHFcuXQcxQr/T5K2/euhxkr61tSVxGzXffx2Imd1pHOVk+fntxRul8V7k9Upyg9Np02ytrka3DFtO0vymvL2Q9mnSHM+u3tJG3n1191zUBkfIv45uyHKrXUmW+R3mMfdkkDnySl2v809iNHHYmlPtu1gurrN5iZO89We1uxzesKc22d+40kRVNs5g3N2dlennsyVnHbgkuywhhl31K2GL6q9A/tosdTp9S642yT771SaYwGBlg1gu07qFoNzS9Uk/U5c/UvFYbQMH/PNOXyyy+XHXfcMajPlltu2VI1/cUDj+zb1q1bFzSWth94JCLyiU98Qr785S+LSj+Yf/rTn+T222+XXXbZpfFcAAAAAAAAADCqJtLXqBmmMa1bt04uueSS3LaTTz7Z2e/FL36xLFmyRC6++GIREdmwYYN861vfkk9/+tOt1Nltw4YN8u53v1s2bNggIiJHHXWUHHzwwXLjjTe2nhsAAAAAAAAAgLr4/QMYTTzwCAAAAAAAABhRO+64o7zsZS8bSO7DDjtMnv/857ee5zWveU3h9k033TT3/pFHHgmO/fDDD+feb7bZZsExbPpVXxzHMmfOHFm9enUuV8iDrdqcB23rrbeW3XbbTW699dZs22233cYDjwAAAAAAAAAAQ+UHP/iBrF27Nnu/9957y0tf+lKvvu9617uyBx6JiFx66aV9eeDR//k//0duvvlmERGZN2+efPGLX2w9JwAAAAAAAAAAAFCGBx61TCmV/VfJQ0RR1EI17UkC2sZN5y6Z3rjmNCqxB4+kWnCz3io1mlXVGWaSHp9xw8dc2THR9DHQTX/cdP6xKP8+1pPXwHBtocxcrqntXk/dVB8nsVm/7mPETtKecckxm9XXEzvftze2mTtK96vC/ZM50phpG3OuzBrMmE3qqcVRv26f65O+j4w2cQv1DgPXehSteWiMYRDyuZlukia+JKvmVs3n9rlOUY68bdRVV8glpjm+OuMx57PJ2G0pqsk5jp79tti922xtze22Y9PsXjanOoZ5DVR2jdw017Goa6lwWzRUQu55rDEamISqEZo4Jsruj6rqx7HaxNoNgyp/t1CnH9CEN77xjfLGN75xYPlf9KIX5d4/8sgjsnbtWpk1a5Z3jPvvv780Zh1mLDOXy9q1a+XRRx/Nbdtxxx2tuW655ZZcrp133tk7V5vz0G277bbLPfCoykOgAAAAAAAAAABo01VXXZV7v99++3n33XfffWV8fFw2bNggIiK33HKL/OUvf5HnPe95TZaY8/vf/15OPfXU7P2//Mu/yPz581vLBwAAAAAAAAAAAPho87kjAAAAAAAAADAQz33uc2WbbbbJbbvnnnuCYtx777259zvttFPtumyxQmsz2z//+c+XTTbZxCvX8uXLg3KtWLGiNF5TZs6cmXu/bt26VvIAAAAAAAAAAFDV7bffnnu/9957e/edPXu27Lrrrrltd9xxRyN1FUmSRI477jh55plnRETkDW94gxx77LGt5QMAAAAAAAAAAAB88cCjIaWUau01aInj1WguVfxqgjL+aarGarXkX9XqULlXm9pcc5NtTvR86/3ZGtSoyXcNlZp8JSqSREVBMc36dKzO/kgSiaz7S3M5+vbmjkR11d/kmrpyFbWZCkLWQ0kkSsqPj+mu6LioQh/7CfPd6Dw0tT6DzuGsocHPqi2Wz/kia1txDetcQ4TGNsdTdP6wnWOs+43zZa1rop5c5edas51PWzNH6DVEyKWaea3jW0tZPa5Y5nXWVNXENWsT19hVr/Wbuc9o5p6nqJ6m7s1c95f9vl4c5Xv5UeRz/EzVFzp22WWX3PsbbrjBu+9dd90ljz76aPZ+1qxZsv322zdW28KFC2XWrFnZ+0cffVTuvvtu7/7XXXdd7r051rJ9IfMgInL99dd756rjr3/9a+79Flts0UoeAAAAAAAAABhViYhMjOBrmH7/+N3vfpd7v+OOOwb132GHHXLv77zzzto12Zx11lnZ7wkzZ86Uc845p7VcAAAAAAAAAAC0hd8/gNHEA48AAAAAAAAAjKSDDjoo9/7aa6/17mu2PfDAAyWOm/vr1LGxMTnggANKc5Yx2x588MHWtuY8/OxnP/N+mNpdd90lDz30UPZ+iy22kD333NO7Tl8TExPy3//937lt22yzTeN5AAAAAAAAAACo6rHHHpPHHnsst+2FL3xhUAyz/R/+8IfadRW577775JOf/GT2ftmyZcEPZwIAAAAAAAAAAADaMj7oAgAAAAAAAACgDYcffrh85CMfyd5ffvnlsmrVKtlss82cfc8777yeWE07/PDD5Yorrsjef+Mb35D3vve9zn6PP/54rp+IyJIlS6ztFy1aJNttt53cd999IiLy4IMPyg9/+EM58MADnbnMeXjLW94iY2Njzn6hrrzySnn88cez9+Pj4/Ka17ym8TwAAAAAAAAAgKlv+fLlwX223HJLmT9/fq28q1atyr2fNWuWzJ49OyiGWcMTTzxRqyab97znPbJmzRoREdl9993lox/9aCt5AAAAAAAAAAAAgCqa+0+SY8pQSgW/+imxvBrNoYpfdSjjnyZqq16L1KhA16Ek6dPat7nWWQ49r4E5lOq8zFhZGymfb93fJ7ctlpnTjNVTo0SSSBQ0jjYkKpJEddUh+fH1zKWKRHW1bzZXfn8bzPrNdWhCk+s2iDkZZC0+2lizqWAYxt3GMdDEeaVqXbpfUd/a53nj+7y8bbXv1aLae8891b+zvetw5HSdk+uMw3VuLVtH23m4p12F+s1xhBwPZbWU9pHi6xMXPb4qOQehyWvSqtfSquDln7P+vUTdexrbfVYT1y79uE80Dfu9MwA/2223ney7777Z+3Xr1smZZ57p7PfTn/5Ubrzxxuz9ZpttJm95y1sar2/JkiXy3Oc+N3v/y1/+Un760586+33pS1+SdevWZe9f//rXO/8L0v/zf/7P3PvTTz/dmefRRx+Vr33ta7lt73znO539Qq1Zs0Y+8YlP5Lbtt99+ubkBAAAAAAAAAEBbsmSJ7LLLLkGvr371q7Xzrl69Ovd+5syZwTHMPk899VStmop8/etfl6uvvlpEROI4lnPPPVfGx/lv5AIAAAAAAAAAAGB48MAjAAAAAAAAACPr85//fM/7m266ydr+sccek+OOOy637eSTT5ZNN920NM99990nURTlXvfdd19pn80220w+9rGP5bYdf/zx8vjjj1v7/PrXv+4Z0z/90z+V5hER+ehHPyrz5s3L3l9zzTXy5S9/2do+SRJ5//vfL48++mi27cADD5TXve511j5//etf5YILLpCJiQlnPdpTTz0lRxxxhNxxxx257cuWLfOOAQAAAAAAAABAP5gPPNp4442DY5gPPDJj1rVy5crcbw8nnniivPKVr2w0BwAAAAAAAAAAAFAXDzyCF6WU16tNieXVaA6Vf9WhjH8GUY8yXtVqULlXPzS5tra5U2ryZdvfRA2+MRIVZa+sPileM7NeM4ceV2d/JIlEImVtemLm+/TGNHNGolQ+h9nGd3wmWy4zXxVmnKL1ctVt7q+jbqyy/koiURIeu6m5blsbdRZ9dkZRG+Psx3HTZI6pcpw3pcnvLVds13dm0fduU+thy1V2/vI/b+fPm+Z5spvrHJq1k/K5Ksthxtb11bn+82Wrqx+5m9DkfUXVa+Y61+v17xV6/wnV5P2T7X6vjfu+Ybi/xXAwj+FReiHvNa95jSxdujR7v379etl///3lwgsvlCTJf8vceOONsnjxYrnnnnuybTvssIOceOKJrdX3kY98RLbbbrvs/fLly2Xx4sXy61//OtcuSRL59re/Lfvvv7+sX78+237UUUfJ3nvv7cyz6aabymmnnZbbdtJJJ8myZct6/s8UDzzwgBx++OFyySWXZNue85znyP/+3/+7NMfq1avlne98p7z4xS+WU045RW677baeOdaefvpp+eY3vyl77LGHXHnllbl97373u+U1r3mNc0wAAAAAAAAAgDzX725T+TWMoij833Go0ifEBz/4QVm1apWIiCxYsEA+97nPtZoPAAAAAAAAAIC2Dfo3iun2+wfQL+ODLgAAAAAAAAAA2nTeeefJPffcI7fccouIiDz55JNy1FFHycc//nHZfffdZcaMGXL33XfL7bffnus3d+5c+d73viezZs1qrbbZs2fL9773PVm8eLE88cQTIiJy1113yate9SrZdddd5cUvfrE8/fTTcuutt8qf/vSnXN8999xTvva1r3nn+tCHPiS33nqrnHvuuSIy+RC40047Tb70pS/JnnvuKZtvvrn88Y9/lF/96leyYcOGrF8URXL++efLbrvt5pVnxYoVcuqpp8qpp54qs2fPll122UXmz58vz33uc2XdunXy5z//WW6++WZ55plnevoeeuihcs4553iPCQAAAAAAAAAw/Vx++eWy4447BvXZcssta+edM2dO7v26deuCY5h9zJh1XHjhhfLd7343e3/22WfL7NmzG4sPAAAAAAAAAAAANIUHHrWsjSerxQ3Ha5JSytmm6f86Tdn81p2rxBhOXKN0JflgkYQH0/VUrUNXUGcFknSN45b/K0MinbWtso76UNRl6rHb5tC1P4QrRjYuo8bJvlHaV+XqMkOZOcy5MsefdEWI06g9bXpiRuXte3J2ckRp/b5raObWsXSc0r5GDnMOh1H3V2UfPkqt6f7+bercNBXWbxQktc4Exbq/A0IkFfsNmmphDk0hc1N1Tc1P2iDWo+jYMa/vzDbmNVrIN0ZPrJ79Zi3Fc+LbriiHWX9nez5Gb45w1lwtxLLtHxZN3ZclHvc8NnWmqOr8mvch/cxdGKu5UFY+96XDps154WnvwKTZs2fL97//fXnHO94hP/7xj7Ptf/zjH+WPf/xjYZ8ddthBvv3tb8tLXvKS1uvbeeed5Qc/+IEcffTRsmLFimz7b3/7W/ntb39b2OeAAw6Q//iP/wh+GNNXv/pVmTlzpnz5y1/OvjNXrVolV199dWH7OXPmyFlnnSVHHnlkUB5tzZo1cuONNzrbbbTRRrJs2TL5xCc+IWNjY5VyAQAAAAAAAACmhx133FFe9rKX9T3vMD/w6K9//auceOKJ2fujjjpKDj744EZiAwAAAAAAAAAAAE0b5mfnAAAAAAAAAEAjttpqK/nRj34k//qv/yq77rqrtd3WW28tJ598stx6663yyle+sm/1vfrVr5Zbb71VTj75ZNl6662t7XbddVc555xz5Ic//KHMnz8/OM/4+LiceeaZcvXVV8v+++8vcVz8V8SzZ8+Wd7zjHXLbbbfJMccc4xV7yy23lNNOO032339/2WSTTbz6vOAFL5BPfOITsnz5cvnUpz7Fw44AAAAAAAAAAENr0003zb1fu3atrFmzJijGww8/nHu/2Wab1S1LREROPPFEeeSRR0REZN68efLFL36xkbgAAAAAAAAAAABAG8YHXQDCJQ3FGdTTrvR/Od4miqLGctnmqurYk4LS44rlKpkMFkl4ALOO0Bq6u1ed7SRdx7jB9bLmSv9s8pjVMUP+L3T60NVD1uug51/Pq21GzP4hbLHNGsy5KsqZpFHiNKprXM72Rs4yiUpjRap0XD39CnKoNFYUlX+nmDnMGoadSiuPZGrUa+O7XsOWwzz+p7qk8rf+1NLUtVKl3Mo+x0XXESKdz3mTVEkdIYrm0oxtjtkcprm/yfVpMpd5iWp+XvT6udoVtTFjZO28a7G38R2j4xLcenyG0CFsc9VPbXwPJDUGVGcqqq6NqpG1ieNBpN3vY9d9Zb8M8pyD+pTU+3wOq1EcU5OiKJL3ve998r73vU/uvPNOuf3222XlypWyfv162WabbWThwoWy1157WR8CVGa77bar/f00Z84c+cIXviCf//zn5Ze//KWsWLFCVq5cKTNmzJBtttlGdtllF9l5551r5dDe8IY3yBve8Ab585//LL/61a/kwQcflCeeeELmz58vL3jBC2SfffaR2bNnB8WcPXu2/OM//qP84z/+oyil5N5775U//OEP8uCDD8rjjz8u69atkxkzZsjcuXNl/vz5sueee8q2227byHgAAAAAAAAAAJO/X00MuogWDMvvcptvvrnMnTtXHn/88WzbAw88IDvttJN3jPvvvz/3/kUvelHtun7/+9/Lt7/97ez93/3d38natWvlvvvuK+23atWq3PvVq1fn+sRxLC984Qtr1wcAAAAAAAAAQB38/gGMpqF84NHExIQsX75c7rzzTlm5cqU88cQT8pznPEfmzp0rO+ywg+y5557B/2cbAAAAAAAAANB23nnnxh4e1LQ4jmXx4sWyePHi1nNtvfXWcthhhzUeN4oiWbhwoSxcuLDx2MBUxu8fAAAAAAAAwNS20047yfXXX5+9X758edADj1asWNETr65169bl3n/mM5+Rz3zmM8Fx/vM//1P+8z//M3u/6aab9jwUCQCK8PsHAAAAAAAAACDU0Dzw6IEHHpBLL71Urr76avn5z38uTz75pLXt2NiYvPGNb5QTTjhBDj300D5WCQAAAAAAAAAA4I/fPwAAAAAAAIDRscsuu+QeeHTDDTfI3/zN33j1XbNmjdx222098QBgKuL3DwAAAAAAAABAHUPxwKOjjz5avv3tb3u3n5iYkKuuukquuuoqefOb3yxf+9rX5HnPe16LFY6mpEKfuPEqeimlCrdHUdRYDnPsdcaVGOXGgWUqyQeIJHycuobQ3JP5dd5qknS94gbXx5or/dNnvcxybHOkD7csdpRv3z0xVY8TV93dh3yU5Y/SevLHh229zPGZOQtzpFHiNKpuExlzEFval1Fp/VFav+/amTnNOKV9PXOYbLVWiWUTMneDYDve0DHsa+iSVP6Wd9OfoX5oKpdPnKSP4/JluUTyMozjKWJeo5lrZV53VY1T1Kaz3S+n2a5ofXQOM4bZ1Hd9yo4BnaPJbylbLLMO33UJEXq9XuXexldS4cNXdUrqzKV5X9GvvCLtzL/tvrBNbR5HAABg+uL3DwAAAAAAAGC0HHTQQfJv//Zv2ftrr73Wu+/Pf/5z2bBhQ/Z+0aJF/P0fgCmJ3z8AAAAAAAAAAHX14/k1TnfffXfh9m233Vb2228/OfLII+Vtb3ubLFq0SOI4X/J//dd/yWtf+1p56KGH+lEqAAAAAAAAAACAF37/AAAAAAAAAEbLgQceKDNnzsze33DDDXLXXXd59T3vvPNy7w8//PBGatpjjz1EKRX8WrZsWS7OMccck9u/atWqRuoDMHr4/QMAAAAAAAAAUNf4oAswLVq0SN797nfLwQcfLDvssEPP/gcffFBOO+203H8d5e6775YjjjhCfvazn0kURf0sd9pJHPvbfIKWUqpwexNrbo6rzjgSo8w4sDwlnQCRhHXWuUNzTubVOaeHOnPVVA593HUfb/ow14d1ovKd42iygW29zJxeOdIocRq1t4bicfTG6aozaxOlbfIfDD0u13hMRTmsbY0cdbjq7dlvGXeVXPZ2krbzj63SiiOpPydtq7N+debfxfy8DLtkiL/Vze83dPRjbto4gs2Y5jjM6y0VME7z+sqVqyxHbx2eOS3tirbbrplddZuxXNfeZcz668T0jaXb2ebKK1fg9jYlFQYS2sM2t365qnWukzOLUT9Exnaf14ZBHEcYbkpFI3lNEnKOBQD0F79/AAAAAAAAoG0T6WvUDNOYZs2aJUuXLpULLrgg23b66afLN77xjdJ+d999t1x22WXZ+/HxcTn66KNbqxMA+oXfPwAAAAAAANA2fv8ARlObz6fxFkWRHHroofLrX/9abr75ZjnhhBMK/7JbZPKp/+ecc4585StfyW3/xS9+IRdddFE/ygUAAAAAAAAAAHDi9w8AAAAAAABg9Jxyyimy0UYbZe/PO+88ueKKK6ztn376aXnXu94l69evz7Ydd9xx1r8r1KIoyr2uvfba2rUDQBP4/QMAAAAAAAAAUNdQPPDoO9/5jvzXf/2X7Lnnnt59PvjBD8rb3va23Lbu/1rKsFAiopRq9TVMEo9X09qYmyZrT1T+FUIZ/4TmrEKlr1CJUpIM2fHoo8raKjX5ymIY822bQ52rp39BHWabTq5IEtX5L5lYcxk1+eRIJJJEIuv+kHa2efWd75BjWKlIlHL/112qHttTjXmMNMFnjtv6jh9W5udg2Ax7fVOZkkjUkMytXucm1tr87ij6TJvfBU193xSfo/y+233qtucoPw+GtivKoc9n5jmo53xuuTaw7S9qM0hNnGOHaTxVrmtD52AQ9wZ17hGauC/qx/1sv+9FQ7T99wKlrwGPHQCA6W6Uf/8AAAAAAAAApquFCxfKSSedlNu2dOlSOeuss3IPNRIR+d3vfif777+/XH/99dm2zTffXJYtW9aXWgGgDfz+AQAAAAAAAACoaygeeLTddttV6vehD30o9/6aa65poBoAAAAAAAAAAID6+P0DAAAAAAAAGE1f+MIX5OCDD87eP/vss/LhD39YXvCCF8jBBx8sb3/722XPPfeUl73sZbmHHc2YMUMuu+wy2XrrrQdRNgA0gt8/AAAAAAAAAAB1jQ+6gDoWLVqUe79u3TpZtWqVbLbZZoMpaECUUrX6R1HUUCV+Esf+Jp/CZc5N3bF21161ziQtKa5QipLJzpH4da6Ta5jpdWj2WDFip3OWdB9CUfN5XXRd5qGbqMkNcTTZwPwWiLJ2krbr6pv+qcdh5kjS3rER1Yxla1c8jijNYcYsHod5yNr6F/E9Pszc6I+QtZwKQj4HbUs8zw1N0Ws5TLlc1xhtSkpqTCyHh2phzfq5Lr7MdQmp0Zw7309aUA5jHXRO8xLbbGdeOxTF6N1eHKOTQ4L2d+eyzU3orUJZc3PMtnFOVUngZFUZfpU5U4GZmliXut+nde9RiwzyO75bG2PD4CgV/j05FYzimABgOuL3DwAAAAAAAFSRyPD8ttakYRzT2NiYXHzxxXL88cfLRRddlG1/+OGH5aqrrirsM3/+fDn//PNl33337VeZADBU+P0DAAAAAAAAVfD7BzCa+vncjsaNj/c+r2n9+vUDqAQAAAAAAAAAAKAZ/P4BAAAAAAAADL85c+bIhRdeKN/5zndkr732srabN2+efOADH5Dbb79dDjrooD5WCADDhd8/AAAAAAAAAABa798YTyHLly/PvR8fH5cttthiQNUAAAAAAAAAAADUx+8fAAAAAAAAwNSxdOlSWbp0qdx7771y8803y8qVK2XNmjWy1VZbyYIFC2SfffaRGTNmBMdVSrVQbccpp5wip5xySqs5AKAbv38AAAAAAAAAALQp/cCjSy65JPd+zz33lDiOB1TN1FXlB9EoilqoZFJi2d7EytrGWmU8Zp2h9SVGKXFACUomO0fi10nnCsshaY6pSS+1bWnb/dcA8mzzX1SjPq7M48k8dHWfREVp7HwDc/26jzddh5nLNmfmdtd4cm2NHJptu6ns2PWPUTxHJpW2ixztmuY6VlHdoNYU5fRnchCUR+5B1mdT599d68d4zPLq5PRZo6IctuvHou16PhPjKse8NuvEyLezrUdRf5XtK49h1tnyv6+Yz12Sq2xflXYhbOd321rXkQROeJXh+s6RqnGVWmcdqs5rG/9ybRtrbNP2vxwMAABQB79/AAAAAAAAAFPP9ttvL9tvv/2gywCAocXvHwAAAAAAAAAAbcr+7fDq1avl61//em7b4YcfPqBqAAAAAAAAAAAA6uP3DwAAAAAAAAAAMGr4/QMAAAAAAAAA0G180AVU9Q//8A/y0EMPZe8322wzOf744xvN8fDDD8sjjzwS1Gf58uW590pEkq73U/YJUwalVOn+KIoaz5lYtjcxp+Z4qtRv1hdaV5KWEAekVjLZKRK/TtVySJrDT5LOZdzCMaA1+TlKjAGOGftVSds4fa+H6ju/+lgpG4freFI9ufNJ42iyQdH6mXXa6knSXnEapTen5DbEXbNltu1sj9Lt+c+crj+Oyr9bythi1+GzVmHxeufKl0r7RhX6on111rap3P2gVP9yoR3mEWqeP8zzT9Gam328cztylfa1bPepd3K7PXZi7LO1Nbfr3GZ/EXu9thiu3EVzVZS3bHsbmkzle67V7UKOH5vEcU9jCmkdug6qwmxWXes6c+e6D+xXHb6arHcY+c7haM9CPYn051jst1EcEwBMN1Pl9w8AAAAAAAAMn0REJgZdRAv4/QMApj5+/wAAAAAAAEBV/P4BjKYp+cCjyy67TM4666zctn/6p3+SefPmNZrnq1/9qpx66qmNxgQAAAAAAAAAACjC7x8AAAAAAAAAAGDU8PsHAAAAAAAAAMA05R54dOutt8o73/nO3LY3velN8oEPfGBAFYXp51PW4j7mMimlnG2iKGokV9mcVp0Ds/4qteq6QmtIulLHnmmVTHaKxK9DlRyD1M9jWS99tn415kdPsytE9+FmO9TM41zPiflR0/0TNfk/4kjlaumuRx8HeozZmJURK+0Rp1GUsb9TY2dDLMrYl6/btd02d0p1tkSR+3umiO+6FLHNqy2Wrrdqrf1mjm8YJF1rPkx12Zifl37kwujoPt5L202Rtfe97lUF404sHyHbHJkxynLb5s88p+p2vduLaywqWddri2HPXaxoXpr6timL01Pf8H8dNypkuKFzoyqsYGiOOvegPvd1beV2qVtbW3iyOgAAqGOq//4BAAAAAAAAAABg4vcPAAAAAAAAAECRQT4TJ9gDDzwghx56qKxevTrbtmDBAvn3f//3xh6eAwAAAAAAAAAA0E/8/gEAAAAAAAAAAEYNv38AAAAAAAAAAGzGB12Ar4cfflje+MY3yoMPPpht22qrreRHP/qRbLnllq3k/OAHPyhHHHFEUJ/ly5fLkiVLWqknVFKhTz+fgKWUKtze5I8X5hxUHV9Rrb511qkhSdPGnlOiZLJDJFP7B6Ap9SQ2EdGHh+uQMNdTHxvd4zUPNVvMor5FtSQqSnN2Auv/pUPbjrOeWGmPWPJFlh2ntrlRaV1RZMbqrdeVwzYXtu09/S05q2gyFqY22+elydjoL8W8O+nvQGe7gJi2T5Athrldn4eKPjeJJbjtM2bLWRRGz4V5XrfV58phq7WMb+yQXFXuL9qiz+9Vakos9yGmkGkPXSMVED00dp11st2j9SO3Td2aqhimYx3tUqr3+3IUjOKYAGDU8fsHAAAAAAAAmpLIaP7eNYpjAoBRx+8fAAAAAAAAaAq/fwCjaUo88Oixxx6TAw44QO6+++5s2xZbbCFXX321vOhFL2ot7/z582X+/PmtxQcAAAAAAAAAANMXv38AAAAAAAAAAIBRw+8fAAAAAAAAAACXeNAFuDzxxBPypje9SX77299m2+bOnSs/+tGP5GUve9kAKwMAAAAAAAAAAKiG3z8AAAAAAAAAAMCo4fcPAAAAAAAAAICP8UEXUOapp56Sgw46SP77v/872/bc5z5XrrrqKtljjz0GV9gISwLbt/HELKVU4fYoimrHNsdXp35dZ2hduoaQ3Ek6JbFnKiVpbeLu4Btbr0r9VbAb9iew6TlIjMkw6w5dr9KcxR8H0Yed7ZjW/bJ2qlNMHE3uNNfUNi4zlllb0UcgSYPEoozt+diu7XBr8ngbVSo9/qPI8oFqke2zUCcWUESp/h8fvkd1SG2JJWhiiWGLbTt/FuXQbc3PWGd7cX8zha3GohhmfbZrcNt8FOe3tw2NjfYoz09OlfUJvZez3Xe1kctHnXp8tVE3AABAVfz+AQAAAAAAAAAARg2/fwAAAAAAAAAAfA3tsyXWrFkjhxxyiPzyl7/Mts2ZM0euvPJKedWrXjXAygAAAAAAAAAAAKrh9w8AAAAAAAAAADBq+P0DAAAAAAAAABBifNAFFFm3bp28+c1vll/84hfZtlmzZsn3vvc9Wbx48QArC5coJYlSjcSKo6iROE1KHPubfKKWssxjVGNezPqr1GvW5VuPzh2SM0lTxcN3KNQytE9ea4g+QvSymetY9DlyzYn5cYiMWLHRrvuwTFSU5lel9ekNZqwkDRanPbPt3fUbx2hRHZPbo3R78efbrM2nr227OW4b3b+sLthV+W6bDpKCoziW8uOrqM+gdH8uqkoqxPDN67oeGTZqiNbWpmjuq6xhaI5sn2W7ba3t20tyWJLYtifW7b05is6NRbHN/bYcWf/y3ZW0EVOf7xu6Deo737Jd65WP6dc4JGbod5/tfqqNXG3V4TIVzgdN/f3AsOecKpRMjeMmFCsOAMNtlH7/AAAAAAAAwPBJRGRi0EW0YBR/0wGAUcLvHwAAAAAAAGgTv38Ao2nonkfw9NNPy1ve8ha59tprs20bb7yxXHHFFfLa1752cIUBAAAAAAAAAABUxO8fAAAAAAAAAABg1PD7BwAAAAAAAACgivFBF9Bt/fr18ta3vlWuvvrqbNtznvMcufzyy2X//fcfYGXDIVGqVv84ihqqxF/ZU+WaetqWKpiXqOJYzXqr1Kjr8a1B5wzJlaRDjh0plKS1iLsW35guIcdZm09c68fhrudMT2+smsvteiKjOXfKyN1zLHd9TLI2avJ/xNHkTmM4PbWYOZO0ZSy9n8HO3BS3cR33Zm3TEXMw+hKP72YMP/1Z7d3e50J68ufrMssx9zf5JGBbrCZzKMu82y6Xy9ZDfxbNvrpes6/Kthtz3NXOHKsttqs+n8OoH8eabS6Ggb6W8Dm+6t5PdeL4t1WOVawyp6GfpaL7paZzNJHTZVieWN7UcQQAAKYffv8AAAAAAAAAAACjht8/AAAAAAAAAABVtfm8kSAbNmyQt7/97XLllVdm2zbaaCO55JJL5MADDxxgZQAAAAAAAAAAANXw+wcAAAAAAAAAABg1/P4BAAAAAAAAAKhjfNAFiIhMTEzI3/7t38p3v/vdbNv4+LhcdNFF8uY3v3mAlY2WRKngPnEUtVDJpMSVu0ZsZRlrFDges8aQmswaXLmL5sOVL0lTxI5hKZlsGEl769kPLR6OPbLlq5HTXB8zpO/6FcY23utjxTz09Zx1t4+NNklaURypXJ1mfTqG7m+uR3du21opS99OLWmO4t1ZTd11+fbtqSX9c2p/KqpR6agjCTsvKJX2i8LPJwDCVbh0G4hE1fsmTUrG6Yptu55MSr7d9bza2uiYZl2d82O+XydeWS4jh2XMTSy567jRuX1y1T0Gq1y7DfK4d6UuO1Z7Y5U3Donlum/qyV1hEkNzNJHTpm4tQbmmyhctWpOosM/jVDGKYwKAqYzfPwAAAAAAANBPE+lr1IzimABgKuP3DwAAAAAAAPQTv38Ao2koHnj07ne/Wy6++OLcts9//vOyaNEiue+++4JibbXVVrLxxhs3WB0AAAAAAAAAAEA4fv8AAAAAAAAAAACjht8/AAAAAAAAADTl2Wefleuuu04eeOAB+fOf/yxz5syRbbbZRhYtWiTbbbddo7nuvfde+c1vfiMrV66U1atXy9Zbby0LFiyQxYsXy0YbbdRYnn6OaSobigceffOb3+zZ9vGPf1w+/vGPB8e65pprZL/99mugKgAAAAAAAAAAgOr4/QMAAAAAAAAAAIwafv8AAAAAAAAApqf/8T/+h1x00UW5bQsWLAh+ELqIyCOPPCLLli2Tiy66SB577LHCNosXL5aPfOQj8ra3va1KuZlLLrlEzjjjDLnhhhsK98+bN0+OPPJIOe2002SLLbaonKefYxoF8aALwHBLlPJ6tZLb8qpDKZV71ampau4q+Zzt1ORrqogl7MsniiZf/WDOuZ7bsvnN2qR9lZp8eedU1V9m3Wb9RbXY2iQqkkR1JlqlL9v4s34SSSLFC9SZG3ubyViRKFWyP31V0cR3RxMxAGBU+H4n277by77zbbFd38O2c68+Dyllb9M5nxXn6JwPjfNkST/bPvOcqoyX2S7kWsBmqp3D9HVfP6//mqTSf2xC181n7ULvcercY5m5qt5X2epo6lgd5H0sAAAAAAAAAAAAAAAAAAAAAAAAAIyaK664oudhR1VdeeWVsssuu8jZZ59tfTCQiMj1118vS5culXe84x2yZs2a4DyrV6+Wo446So444gjrw45ERB577DE5++yzZZdddpEf/OAHwXlE+jemUTI+6AIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMNl1apV8oEPfKCRWNdee60sWbJE1q9fn22Lokhe/vKXy8KFC2XVqlVyyy23yF//+tds/3/8x3/Ik08+KZdffrnEceyVZ2JiQo488kj5/ve/n9u+5ZZbyqJFi2TTTTeVe+65R2655RZRSomIyF/+8hc57LDD5Oqrr5bXvOY1QzemUTMUDzzSiz+KVPpPmyKJWo3vI3GsYRw1V2Nixq4Ryzz2ooA6q9ahc4bkaoo+FsuOmSSdktjSRM9Ym9W3MTW28Wiucddhxm5yDhPjo6dz6ONTH5fdh3rkaJOklcVRPnhiFG4e80nXiOLsOC+uO8tlidXJWVxLPlaU5hrdcwmA9unvGwwH23ooxzrpc5F5fixqY7t81X1tITrny+LtRfvMeszYZfX6ct1SNZHDJrv+aCGHvpZo85bRFdpnXK57Pt+5MY+d0pyBkxISu2qOJnI6Yw7p3x+0fd9fxzDXNmhK3N8BU9EojgkAprJR/v0DAAAAAAAAw0dJO7/TDRp/ywYAw4XfPwAAAAAAANBP/P4xeB/96Edl5cqVIiKyySabyFNPPVUpzp/+9Cd561vfmnsw0D777CPnnnuu7LTTTtm2Z555Rs455xz52Mc+Js8++6yIiPy///f/5NOf/rR8/vOf98r1iU98Ivewo4022kjOOOMMee973yszZszItt95551y/PHHyw033JDlXrJkifz2t7+VrbfeeqjGNGqm52OeAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACFrr76avm///f/iojI+Pi4nHbaaZVjLVu2TB5//PHs/eLFi+Xqq6/OPRhIROQ5z3mOnHjiiXLxxRfntp9xxhly//33O/OsWLFCzjzzzNy273znO3LCCSfkHnYkIrLzzjvLj3/8Y9l7772zbY8++qiceuqpQzWmUcQDj0aAqvhPPyVKWV+1Y5e8Qimlcq8qdYTmajJ2oiZf00Uc+b/6Sa+XUpMvG9XCSx8D2Ut6X2ZdtuMrUZEkKspiVx1nvk8kibgXpOpnGH4iURJNqed+AhgUfS5IVPXvbvt5xn3d4orpPNem+/X5p+gclJ0jLfVk58MsVr4mc3uS6zv5cp2vzVi217AbpuuxfvC5t/K9Pg+59vG9j6hyb1TnfqhqTmuslu4fy1S9t+73/TUAAAAAAAAAAAAAAAAAAAAAAAAAtG3NmjXynve8J3v/kY98RPbYY49Ksf7whz/I+eefn72fMWOGnHfeebLxxhtb+yxZskSOOeaY7P0zzzzj9SCiU089VZ599tns/bHHHiuHHXaYtf3MmTPlvPPOyz0M6etf/7qsWLGiNE8/xzSKeOARAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEBERP7hH/5B7rvvPhERWbhwoZxyyimVY33rW9+SiYmJ7P1b3/pWedGLXuTsd/LJJ+feX3zxxfL0009b269bt04uueSS0hhFXvziF8uSJUuy9xs2bJBvfetbpX36NaZRxQOPpjEV8E+bEqUKX43ENl6hlFKiAmsJzRmSwzdmoiZf1px9WttQUTT5somj/GvQzDpc8+7aX9bH9dJU+irLqY8jpfIvc3snRiSJirLYPuNIJJJEok5sz7ErFYlSfovrPM4DYk3Gmxwn7KJISRQN1/cGgKlPSe/5q6eN4ztdn3e88hnnPVs92fnPOC/6xNHnKNd52TwX+9Ze9DK5xhcZr34wr+VCrutc14llqt5fVLl2C84h7ut7fb/gum+oeh/iE9uVq+r9lki794PaMNz3Yjh139+M0oujGQAAAAAAAACA6WtihF8AAAAAAAAAAGB6GvRvFNP194/rr79evvKVr2TvzznnHJk5c2bleJdddlnu/bve9S6vfjvttJO8+tWvzt6vWbNGfvjDH1rb/+AHP5C1a9dm7/fee2956Utf6pXLrOnSSy8tbd+vMY0qHngEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANPcM888I+9+97slSRIRETnmmGPkgAMOqBzvoYcekltvvTV7Pz4+Lvvss493//322y/3/sorr7S2veqqq0r7ltl3331lfHw8e3/LLbfIX/7yl8K2/RzTqOKBRwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwzZ1yyiny+9//XkREttxyS/mXf/mXWvFuv/323PvddttNZs+e7d1/8eLFufd33HGHd669997bO8/s2bNl11139crVzzGNKh541LLE8ZoqlOOfNiRKFb5qxZRq86+U6nmF5vTN0WTMOhI1+WpSLNW+dOJo8lVF5Hg1Qddn1qnXSanJV7a9ZG71PlsbHcsWU7+U5F/m/kTZjyNb3dZajfYhH1Pd3vtz0vWqKlGRJKqp1QcwKuJISRy1c02DSUpFomp+/7rOF9l5MDu/RJI4zvjmuVOfJ3rOtZI/35m1FJ2He87Pxst1r9DEvYN5TdHkNVAb11P9VPWawuceyHUt7VrTkPuO0PuN0PuZ7hz1jsXm7+9E3Peqbd2v1hX6+a/7AgAAAAAAAAAAAAAAAAAAAAAAAACXm2++Wf75n/85e//FL35RNt9881ox77zzztz7HXfcMaj/DjvsUBqv2+9+97u+5OrnmEYVDzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgGlqw4YN8u53v1s2bNggIiIHHXSQHH300bXjLl++PPf+hS98YVD/BQsW5N4/+uij8vjjj/e0e+yxx+Sxxx6rlcts/4c//KGwXb/GNMrGB13AdJe0EHMQT7FSokr3RxI1litR+VxxVD22Of8hc6eMOiJHHTqXK0d33LoxkzRUXBJGr12TaxTKNsyyuntiNFNKq/TS6vEmJR8bVf6RcrbrOba7Jkj/zyx/uiE265P89iRtGEequ1tQnVldaZLY8d0xnel5drdruZBAPEkRmNqU8v9SSSxtQ2L4xuzNkbb3PBt1ny/1uUifB13ftrp9z7msJI55jrf1tbU3xca5ObfPjFUeqhW+K+5zZos9rpPaZsvtuucp65vtd/Q37zGqxKgSs26OrF+FXC4+895Pg/iMoT1K+d97TSWjOCYAAAAAAAAAAOAnEZGJQRfRAn6nAwAAAAAAAABg+pouv3+YD9DxseWWW8r8+fObKUhEvvCFL8itt94qIiKzZ8+Ws88+u5G4q1atyr0PrXnOnDmy8cYby9NPP51te+KJJ2Tu3LmleWbNmiWzZ88OymXW9sQTTxS269eYRhkPPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAViyZElwn2XLlskpp5zSSP4777xTPve5z2XvP/vZz8p2223XSOzVq1fn3s+cOTM4xsyZM3MPB3rqqaday9OtKE+TuVxjGmU88GgEhf6XbOJWqshToqz7IolqxU5Ub+w4qhZTz12VOVFpHZEjd0iONmKOgnpHTLG4jaApc30KDlln31BxmkMfOklXTj1WPeRsX1Tc10Z3M/t3x0jSIHHJd4CPRKVxInec6fZ5AIBRFHr+0+eikHNsb4woF0PXYL7vyWm8L6ojKWjTzVZ2z7m6iz6f9/O/YhkXXFeEKLq0sIWqm6vfXHW61kl5HLy+a+0Tq2rsnn51PnSGsnvGtvBfgQUAAAAAAAAAAAAAAAAAAAAAAAAw3SVJIscdd5w888wzIiLyile8Qk488cTG4psPB9p4442DY8ycOVMef/xxa8wm85TFbDqXa0yjjOdBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMA0c+aZZ8ovf/lLEREZHx+Xr33tazI2NtZaviiKRqpPv3ONivFBF4DBSwLatvGELCWqcHsk1T+ciSqOGXt+4M05CRm3SnO7vlxCcvjGtOZKpyOu0N3WV8/wsH6FVhlrke5DSa/ZmCV2bLSrwuybFB/KbmmNcUF/ncOco8RYVN03Md+nDeKoanFuSkVp7skcPJ2vP9pcU2BU6c9NoiJje+d/V/4uH1Eh86FUxWuf9FylsnNXb36Vvbe3LYzd01/X2p2/vI8tpsk8V3e/dV3fmTGt5/0KfK+zfHLoUL7l6Mthy+W+l9B1sN2v+IzPdTwpj4H4Xlv6xAqJV9i3zsSnbPPZpDpjxPSSyGgeL6M4JgAAAAAAAAAA4IffPwAAAAAAAAAAwKiZLr9/XH755bLjjjsGxdhyyy1r17FixQr59Kc/nb3/yEc+InvssUftuN3mzJmTe79u3brgGGYfM2Y/8/Q716jigUcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAA77rijvOxlL+trTqWUvOc975G1a9eKiMjChQvllFNOaTwPDzyql2tUxYMuAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQH+eee6785Cc/yd6fc845MnPmzMbzbLrpprn3jzzySFD/1atX9zwcaLPNNnPmWbt2raxZsyYo18MPP+zMU5SrrTGNsvFBFzDqlChJRAX3iyVqoZr6Esf+Jp+gpSzzFtWYm0RNxoyjsBhF43aNVaW5Is9cOkdZXFdMV4wkndK4oLue7zrz25Si+kSktDJbnzboI9OcT9fnw7W/O6aZy7bfNu6sXcH+WOXrMWOYfbP25vu0QRyFf8eZlJmjdkQAoyBKv1+UKv+S198ZPt+zwyBKv92V45yrT/eq/tesN9857ze9tv2cC2WcL/X50bwOMNt178ti2bY7xmMe093nbP0/zZjDxHqN0QdJPw+WovyO/cpRn8/3mStGSCxr34rzaLuXa8JU+a6v8vcAbWlzPQAAAAAAAAAAAAAAAAAAAAAAAABMTcuWLcv+9yGHHCI77rij3HfffaV9Hnroodz7DRs29PTZZpttZMaMGdn7F73oRbn9999/f1CdZvt58+bJ3Llze9ptvvnmMnfuXHn88cezbQ888IDstNNOlXOZtdu2tzWmUcYDjwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgmli3bl32v7///e/L9ttvHxzjwQcf7Ol3yy23yB577JG9Nx84tHz58qAcK1asyL3feeedrW132mknuf7663O5Qh54ZOay9e3nmEYVDzwaUomo2jFiiRqoJExi2R43mEMZcxNVGGei8jHiqEIM3dfRThm5Ikeu7jm0xdYxbbF8a2uDntsqc9qmQcyFS1LwMVeWfcrylTCRbjenO46M/d070zdxui8x+lRduVzNZo60wDgdSHac6+0NfOfVFUeDr6FMNARzBNShP2OJGq7zgy/9GVQDuL4aVXW+1ZJ0HfS5R6n8dmWc47rPUZ1zbXFbW1/zHO2TQ4x9Zh+ntL2+jsldJ3qet2NHA9u42qBrKboG0mW60rdRX1E9Ir33Ha72Iva1Ne8JfPtViVUlZtan4gTb5qqKKnU3rYn7cUwNiSr/TE9VozgmAAAAAAAAAADgZyJ9jZpRHBMAAAAAAAAAAPDD7x9T3y677JJ7f9ttt8natWtl1qxZXv2vu+660njmvu4HHt1www3yN3/zN1551qxZI7fddptXrn6OaVQN4zNAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABT2NZbby277bZb9n7Dhg3yi1/8wrv/tddem3t/8MEHW9sedNBBpX3L/PznP5cNGzZk7xctWiTPe97zCtv2c0yjigcejbBEVOVX87W4X1Wpgn+C61Mq9wrqK2H1K6VEeeZwxQ6JlYurOq+emJY5tLXvhyh9meKo8+rZZ7yaoNTkS8+FksmX79x0z3t3/+5Xti99TaSvDZaXPkZ0u6xfMvkqypXVkPZ1jaunndT7zE5FVY+jKFISRZ2DIxYlcQvfsd3iSEkcNZvDHEc/Y+nx1BlTk/VPV8whpiPbedE8H5ada/X5WJ+nzT4TjpftXFx03tYv2zjMVwjzusr1Gla2OeptF349bLs+cl2v+1xX6Ri+sXyv1cx7IJ/7oKJ7ryr3X0X1tnmNOej7YQAAAAAAAAAAAAAAAAAAAAAAAAAYNqtWrer5/6+5Xtdcc00uxoIFC3ra7LHHHj25Dj/88Nz7b3zjG1413nXXXXLjjTdm72fPni1vetObrO0PPPBAmTlzZvb+hhtukLvuussr13nnnZd7b9Zs6teYRtUw//9RAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABT1N/+7d/K2NhY9v7SSy+VP/zhD85+p59+eu7929/+dtl4442t7WfNmiVLly4tjVHk7rvvlssuuyx7Pz4+LkcffXRpn36NaVTxwKOWJUoVvoZdIsrr1WzO4lcVyvKPdy0V1iu0bv10ujbVmUNfKn1NJ0pNvpL0pecgUeWvrL/RfiLpeqnJ14b0pd/rPrb3Opbup9ferLWwXktb19p22keSqChrP+zHQxwpiaN2qqwSO4omX+G5Jl9BuURJ1NIKxcJFha9YVNALzYkiJZHjM9rmd4RLndzm2MxYUfpq0iDnqgnZeUyM86AUXz+Z+8uuA2xtJ4xX4bVAwXWBeX2wQfWet7M6xe9cbNat4zWhn+cEc72Crt+NuQttV+eexdWv+2nWdWNl7Src41S9nzJra+o+Lx/b75616fvWqmx/N9CvF+zUCL4AAAAAAAAAAMD0paT8d7qp+uI3EAAAAAAAAAAApi9+/xgNL3rRi+SYY47J3q9fv16OPfZYefrpp619vvvd78p5552XvZ8xY4YsW7bMmeuUU06RjTbaKHt/3nnnyRVXXGFt//TTT8u73vUuWb9+fbbtuOOOkx122KE0Tz/HNIp4NgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAoBWnnnqqzJ07N3t//fXXywEHHCB33XVXrt0zzzwjX/7yl+WII47Ibf/oRz8qCxYscOZZuHChnHTSSbltS5culbPOOiv3UCMRkd/97ney//77y/XXX59t23zzzb0fQtSvMY2i8UEXAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYTc9//vPl0ksvlQMPPDB78NB1110nO++8s7ziFa+QhQsXyhNPPCE333yzPPLII7m+b37zm+Wzn/2sd64vfOELcscdd8iVV14pIiLPPvusfPjDH5bPfvaz8vKXv1w22WQTWbFihdx8882ilMr6zZgxQy677DLZeuuth25Mo4YHHg1I0nXAtyWOotZzJFI+jljq15BYY4dTRr2RZ31F6+WaX123q0795ReVxHPFcsUo65+kQ4uNrnqufOfIh++c1NFmbJM+LLJxOaYqMQ4js7+IyITKty1q091Oj1cvva4hy6W3d+U267W29dxfxOwzZm/auLrHQBT5fz+38S0bB+Sfyjmni9hxngyJkbRyxE0N+nOpVPkc6M+/7dqhTfpzlBTU2PN9WzdXA8dEWb1F7XzaDrPuqXeNIzsHG+9d+1XBudZ2Xjfb2e4N9LW03j3WdZLbkG7TN3TWa2bLcG01NWmQn8luvkM07xVsfMajLBPr6mvrF5pfpNo9p+8c9OSq1MsWa3DXJf24TwcAAAAAAAAAAAAAAAAAAAAAAACA6Wq//faTyy67TI499tjsAUBKKbnpppvkpptuKuxz1FFHybnnnitjY/5PLRgbG5OLL75Yjj/+eLnooouy7Q8//LBcddVVhX3mz58v559/vuy7774BI+rfmEZNP58NAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACYhg455BC5/fbb5f3vf7/MnTvX2m6vvfaSSy65RL71rW/J7Nmzg/PMmTNHLrzwQvnOd74je+21l7XdvHnz5AMf+IDcfvvtctBBBwXnEenfmEbJ+KALQHsSpWrHiKOoXg3iX0MsYbmS0lh+lFFfFFCDOb+2uTLrtNWm0nhRyZzrWHVi1JWkw44dKbrnp+5x1CRbKT4fF9sxF6d9JywxbP30XHZ3m0gbb0gLcpXVOSYmB6bHocepjHaTbfP0Wppr61prHXOqPjOwylEZR/W/V0NFAd+jo6SJuY4GsF5a3MK6mTGTSkdxPd1zqlS1/Hptk4r9B01/JlUD8+8bK/tOb/GQ1ueGsusrX/o4cR0jem8bw9LjSDyC63k1x56dp439yrJ/Q1cuextVuL/TL72WS2dnIm0/picr6cypeU1jvz4sbt/m8VRFP8uxzburnQ9lmVjbZ8vWPiRGtj9wUW3jrlODX4z2V7uJe2FMD4mq9lkfdqM4JgAAAAAAAAAA4GcifY2aURwTAAAApp6N0j/19WkT/04dAAAYffr/76H/f5jPDqoQAJjC+P1juOy3335B/784m/nz58vZZ58tZ555plx33XVy//33y0MPPSSzZ8+WbbfdVhYtWiTbb799AxWLLF26VJYuXSr33nuv3HzzzbJy5UpZs2aNbLXVVrJgwQLZZ599ZMaMGbXz9HNMo4AHHgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+mbGjBny+te/vi+5tt9++748cKifY5rKeOBRy1T6T5siiVqLnQQ+WS2OqteSOOYpDhin7SnpsWW7ZlsrnznWc+WaA12brZaip9lFRkxzfGYsHcPWryh3kqaNjfL1nLjmQFfdxtHYZmxNT1XIIa/nrGc9ovx+ZbTXVEH/iXSj/tPMYa6dzhWl7cYi3S7K5Rjr6pittX5v1q3fGxNvPUY8FsinTdOiqN3v3pDcsfHdUuOr0iouGW9U8TzkmkPXd2pIrFFhrnU/ciX9/GCNiO7jUani+dOfqcSyv02DzO1DVzXVP9W260VzXLpddk7W53fjHN3dT7eZyPqo3Ht9fWWe5zvXgflrufzWSWPGud48t/Sc1yVclT7d6vyXi2zXTbbY5n1L0fFpxvTNbcuZy2e5iLQeZ46LTp+5871Xq3I/Wve/OuW6twuK1cDTvutq+56+bVO9fgAAAAAAAAAAAAAAAAAYBpukfz6V/ln337UDAADTw1j656z0zycGVQgAAMCQqfv/YQUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHDigUcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKB144MuAPUpUY3FiiSq1T9R/rXEUViuxDHO2KP2xNq3XNEc2+bKnAPbOM1aympQaczIEcuMYevXndvsk6Tlx0YqPQd63LZ2Psx69ZTpMpuM7ct26BYdM4nR1uz6rNFpQuk/laV/Z8MGo62yFKbXNE53j0X5WsY6DdOiO33H4vwmHUMs858YOXS/LIfodp0FG4ua+16qy3UsxCW12vqah2ZZDF++Map8LurmnGqiAYwrbvBcXDV3UvMcHkrPs1LV8urjLwno75tTf3Zt5/02lY3L/F6tnavruNPrb86RWY+uylVClfX1nfem56Gbz6Wo9Zxvnp8d53s9Tn2e7+5vO/dn7yX/XhnbzSIm0vP5WNdyjEf5azPz/DBm9DHr7z2XmclHn3mNbzsmbcd00XWa6/i3Xdv5fF/53mtVuT8M/b503Zt5xQi4d6yqyXtljCal/M4dU80ojgkAAAAAAAAAAPhJRGRi0EW0YBD/DggAAABgmpf+qa+5n0r/5HoVAAAU0f8/m1npn5unfz4xgFoAYKrj9w9gNIU+EwQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACDY+KALGHVKRBJRreaIJWoslqpZaxRQS6L8csWRX8yyeXbNke3pd2VPBDPnyjZ2c5y28RTVYOZXaazIESOkn61PkpYdO6bfbKdHG+XaqLRNc8dqVZ6HXaHE6KuM7Tp2km2f3DCRbl+fJOn7fKDutc/6pNHN41ofy2Nq8k89pzrGWDbzRrFdcx+ZcxDlt49F+brGiiP2hT4u47S47iPIdWz2xOoZeF5Ust/Vtw3RQGY8zd3geENjDWKu64gHuE6m7lqSBq8NUEx/RlUDc+0bS3+V1zmXmfRnLlHucdja+sbQ3wfKI1c/mddg5vlev+853xt/TqjuPsrYpwr/3OA472txOmdjXcfIuHFdpa/zxtI/n5Nd3EW5us3LMX0+Lbr2s10nutiurc25Fem9vui53jLXo6d9vkHRDPZew6nS/bZcquTDZxtzWZ+yfrk2jhi+93BVnvhd9V7W917PR9171Dravpfvl9EYBQAAAAAAAAAAAAAAAAAM1iPpn5unfz6d/vnMAGoBAADDb6P0T33tsGpAdQAAAAyr0P/vKgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQLDxQReA+hJRjceMJarUTwXUEnnmSFR5zDhyx7HNkWucSU97O9vYzXGa4ymrX+c38yojRmTEcPUz25f1SZSuM42RjtN3/fKxVBorKsyph6XL682d7jdq626T5TJiB9dask+n1fknlH6vcvsn0vfPpg2fSSajPqMmJvcXZDGPVXO+9TE7lo5sPP0zUekExHrEURahV1T4LlsHz6VV5oKU8PiY5mqIooDvkrSt71pX+3bzjG3UHYv5Wa0e2zzGO9tt3z32ObT18a6lVu/2hBw3dZlrO2x0fUmrR/wkPe/K98tjyLjq15+XZADjG0Ru/fkuOw8OQ0ybqOz0F8hWr96uUyjjOkCkc22grwWya4Ls/WSUDWK+T9LYxQMY6/oGjo3jYiya3DczGpvcr68RIr0/7ZddU+THExvXX7ltRh2284B1zlpYD/Oa2kxRlNOcV1tdZi7z2tvWrm6fyZrck+W656ryWQu9p/Sp0yXk3jFUG/fIGE2J9Of81G+jOCYAAAAAAAAAAOCH3z8AAACA9q1P/9w4/fOZQRUCAACGmr5WmEj/fHpQhQDACOD3D2A0DeszCwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwAgZH3QBoy4RJYmogeSOJarct27NPrmVZ47IEStR7jhxVBzDNU5zHGVPybM9Pcwcpzkes/6iWs28Zi6Vxoii4npt7UP6mPS49HiSNGQc6f0d1Y9Ef2b+bHv6p+/T3cy5TroGooxtehr1Gk6k7yey95N/Ppv++YyafBbv2mjy+f3roqfTnJ2ssVGpfh+ryT/HZExERGao8bSGyfcSpX8maaw47ZfOfve0ROlIsuPbskDZnHY6tsbyEa0Yy++7Jba0KztWzDJtMUKYMaIBnTMGrYm57Id4mq5Pm/TaJ8r/i0B/zpWjj/48t/mUW/2ZVcY3RPcxbY5Nn6uSBg8nfWwm+nvfmCNznnVFrhK6v1PN+XbF8F2nYWEbh97eOe8bfxb019dJ5jXBhJpsvT7989n0Of0TaZRno2dFRGRDNJHGzh+93dcJep/eNjuZmb7XB9jkH+PpNYGub0wHSE++sTGO7msp1/WVi+0YL9pstvW4xC+MVZTTvB631WWOS1mKKBq/rW1Zn8la3AN13Td5r0fAOcynriK+93heNfTxnDuoe/Z+GfXxAQAAAAAAAAAAAAAAAEA/nJr++ZH0z83TP/vx7ywDAICpQ18bzEj/vD/981Ppn//U33IAAACGlu8zQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACrjgUcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKB144MuYNSp9B9TJFHruZOCvFXFgfWG5HbFLpq/ImVzmqjyGHFU3LdsHGbdibVdnjkes+6iWs36dK6e2EbfKO1n1tbdT/eJHDkSpWsxcqbj0eMoaqer0pv0GP9/9v49XJaiPvT/P9Vr7c1lowICinpEbl4QTUgIKtEERcXbUTQYosnjJSFHjSY+JycxN89P0MToSb4+j+biiYkRczGiPKAmCigiJiIxEFDkFtxsQQURjlyEzW3v1fX7Y6p6uj9d1VU90zOz1uz3i2c5u7urPp9PVfd017DcQ6Hq87n8MHxJOqaO16g7Umfs/MSUHZeMr8/HtNXr6E9rrsGO6nXUcoesiYjIA+ZBERG5x9wlIiIPyr1V7EJWRERkxd2eV2TT6NWMXjfZzS7Xbq7OTc0izKh/4efYx22cj+bk+K5+PMbtWJn9bUqKAe9T7dju1XTnMB3HU30XKVabmWJOu+ZCpN+3JKZiDWmeuTYa/x4r57Du2Oj8dWTt4ubKv3/1fbrVzj8PB7z0/T2lzBh/qm21dkjGce1sfZ/bcLFLt11U267hDN/2fefVr2vq49ip1go7XZudrnC/Jthhdrrt0esD5v5RO7d/TXY0XsvarFr3591li4iIFGY08yt29Fr4j3rVQmu0309h9bxwF9RKYGzVGkHNiV5nSaRdLF5XHz3/+jqKretbcQLZYvXpHHpNHbuWdbucPqnPJV2ffXLXs7mfxVK1hOR+NgvmG/iNO3S8oU0zV0NbT7WsN9am710b0ZBrBAAAAAAAAAAAsLGUIu430sul7///DwAAAJiF29zrv7rXZ7rXPv8/ewAAsOvwa4ePu9d/X1QhALAE+P0HsJz4dyoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGDmVhddwK7Kip17TiNm4r7lhPUWGTlzY6di5cxpbA5KG+5bmHhOXXesPv3NevpbxnTdoRp1fb6uZGzXz6hx1PsVqm1Vh8rh25XW16ByuXH4+staON/W7/Jd/bj0eMY1+VrCuesV61mL1ZlSqkvBdhzz9fv91as7vmZHf9phR98ZucPsHL3KgyIi8qDcKyIi29d+OM7n+qwWu41ezeh1s9lzdNxsCdZd2NFAjXv1c7hiVlq1V/Pv/mCsbxsM3TLp3NZN2neanFN0bSmMfr/o+4E+3t2/O1e/2rpjzf/Zsyz0OUWbfx9YO+S7bRjVM2yaGMbf8/uPb5q+Q/SfKKd77TNnqTqrmANcK/49WUbu7kOc89jz3d8N/Dql2q7dJ/y6as2/it8u3bZbI8hobfCAuV9ERO4z20VkvEbYaR9ovpYPVDlWjPsoN3rUy2azxyim3TzabUez4NdmK1Utnp87tx6T9voyvoaTLLFmof56Oa7PnV4P22q/ztkO3lrDRerS6+Hcdl1tY58zUp9dcq7d3M9RsRpCJv2cOunnxaFjpCziczgAAAAAAAAAAAAAAAAAYDl8yL3+3p+MXt/z26PX311INQAAYL3yfx/g/+deX/Ibo9df/8AiqgEAAFi/9PejAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADG510QUsu1KslGIn7l+IGawWO0Udmsmsq8/YU2MdIlZsDmLjKW08Z2GafWL16VrK1vHNEUWgAAEAAElEQVR0jbo+X1e7hnBsq8Zhav1K1baqw/XxbXW70oUs1NT5+us167a+GlMdb45H5/Ll+7JDuXVMndvT9erjOl79uK+jVG38mP3+Ndvc3un+tEN2jl7NA6P9dvS6Y+2eKseOte0iIrJS7CYiIptWtoxiFjvcAFxD4zdHO1ZkZfRqC5e7aNRSP7+FmscVP+9+Xn3DnrefwrQnU19XJhHTmGa9Pmaom25rAvlz66zHCdaVFXk6Zsp79DT9c+duFrFi5wPDKqr71Dyu5sn4a6G0+TX668326DMr/j1oe8yxvh/3jVW/p9rqHt4813qO9Dz7EDnvxNh8p2LM8jz5OVxbwK1Er7vq57Gs9o127rSjPdWawIye6w+aB0VE5AFz32hb7hURkfvLH4223RrBrw9Ku6PKsXn1oaPXYi8X88FG7E12tDZYU2sCv7Yr1HVWrb8C4/D0Oqqv0LpLL7fb89psEFqjjfY3d4Ry6dh6jTxtu1He7osx9nkkFKvdpjt2KneqhklyDt2vy5CfZ6cxi7HN2kaseV6s5D0DN5plHBMAAAAAAAAAAMiz5n6WzTKOCQAAABvPbe716t8evb7FHisiImear4qIyKULqAkAAKw/R7vXt9mHiYjI181dIjJeSwAA+uP3H8By6vp+BwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgEGsLroAdCvFzix2IWbivrZnXSYjV+5Yc+qOxYr1jY2nq+7SNvsUJtxW16JrKFs1puvzdaVq8LF1TFvrZ1yfVNtYu9L63IHCF8CPLFZOmbjMcq5CP+8+ln+11X736qL581ea0u0v3f411298FZT2/tHrznvd9s7RAXe3Lsxo5lfMaMcm2U1ERNZcrDVZaeSurpHI9TnK7/6gmsTmYhbneshv3/OxCtN9Nk3keKpfV4xCv1fVXPWJHZvnPjGm6dPoP1Xv2YmdwyHpc4rl568ra8NvQv9+KiPHZ2keuXWO+vu/tWZJ1FOtFQasL6a6387hLavXEtV6oLbPrwmsWgusuVY7zei5vcM8KCIiD8p9IiLyQHnP6HXtR6P9O0eva26/MeOPb2W5h4vdXFeUKle1JlC1le7B76fOSHvNUKhdfdd9sXWXDezX14le59pqv4qlTnoop45tVezYNZrbbpQ3PNjY54xYrJzPRLFcubknzTtN+5C+nyn7mOXnaAAAAADAZI5wr9e613n8OyMAALD++d8n+bXClYsqBAAAAAAAoIdXu9evy94iInLh7422D/zj0evdc68IAACsBw9zrxe+ze85VEREXieXLaIcAACAdW+9focBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABYInzhEQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAmLnVRRew7KwppTTloDELO8z3VJViJ69BTK/2tkcuk4idU3esvljfWPuuunWdpY3ENqqdiqlzh64WfcZ9XbEa2jnDcURErOtjXB+dv0i088dLN6zCNGus19lu44+H69c5/BT74ZW1qSzUKdSxU/TZ87HrpzX1TtbH/Xhi152RFRERKYpN451rvu+DbvtuERHZYUa361Wz22h75QEREdlpdo5eXcdNPqd7tcbXUK9L3LHEgBJMR399PorYHExZQ45Uiq47qu5bmMnvm/Ecw8fcCGYxl0jz78Wy53O8D+POrZ3wJlO/NsrMGLk5q2fWRJXl8e9pG5hjPzY9riLwXEvFStHnWs+RrsVnyHlnpuY7Ns5xbSOla1dfC43XCs2HVdXWba9FavPjXhM/ztH+tdrATJ/BTqmsXt2cuM8FpTuyJjsarzvK+0REZOfa6HWtvGdUqh09703tqWVM8wnmY1afPdz49HqkNM1zXqjrrLGmdW2LjjVYjsgy2dWltlXjWFe9Ttc1hd7nVsXWbfTx3Bq76uqqZ7Q/PZld+bpyTpJrkraT1DLLGqbKOfBn9/XCLum4hlBK/3vaRsAZBwAAwHq026ILAAAA69qmdBMAQKZSlvN3Bcs4JgAAAGxc29zrDeZzIiLyOPf/83vzH4/+j5b/nzu+Y851AQCAxfC/53ib3/He0d8J3WYeIiIiW+deEQAsH37/ASynYb45BwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAoMPqogtAf6UZ5rvaCjv5912VYvvlEpPd1iZim4xYsfpidfRtL9KuM1ZXaZvtCtNsp3OHcpbVsbwaYjljcURErOtjTPf86nY6Zml9znad4/qabXy1vouvP1a3H1691FDeemxPj67flRzuk3pH6nNauJGsmNF39xZmfCsuzGaX4wH3ulNERNbK0fZOO3pds6Pv+l8zO1wNoyr8/cHaSUY2ma5LJnasiL7n3Kvx14zf9vFsq63fl7qjVTFNJHdkf4iOocejx90ntr6GUzFMbC4zcsbmooqRjJAfa0jzyBW7RpeFH1/Z4/m8q/HXmbXhOfLvsTJyfJZCuf19Z6jbfyhH9axPtNUlVP0Sc9qHH29hw9dydR9e0Fs5tjYoq/de2Xjdae8fbfvne3mfiIhYu7PR37h1gojISrHbaJ+b4ULdtfUa06/HbGghJeO11ErH6Yl0TbbXQvOj16+t9Zb1+21wf1dsvS7SbVLH4zXGL7DUNRDtl/EmTn1eyv2s1vczXU7uWeXtjDfQ52MAAAAAwGxd7l7/xL3+jnvlUx0AALue+m803utef3sRhQAAAAAAAEzI/37jpe71iutH/8fKt79ntP2R3x293qbaAwCA5VH/fccj3Otb/6/7w1UPERGRV8yzIAAAgA1o8m+8AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAyLS66AKWXSkipdhBYhViBonjlWby7wgvbL/vysqZg9zx2Ugsk9E/Vkcsd1fduo+uK1ZPaZvtCtNsV8+pc+gzps+Cr0Hn9jl9rtCZ97Gsa2tU21auRLuyNszChOvzbYoBL+1UzGHejU1+zGt6v5sbY0evq3ZFRERWzKbRtuwmIiKbij2rPjuLLSIiUtqdo3rtg+5I6bbd6xy+47/vaclpr8+LnztjhjszhfHXWaJdZH+oXzFgfaMc+fFiufvEaPUdaDyTxBl6LodWzOQusWvy14e1k9/k/fVSZsbIzVl//8/qblp/j1p1Z4mNy98jS3UZ+lg6TjCvf+b656HrW7q+eo50LT5DzjshNd+5569+Lylaa4S8GH7u1iKFB+/t7tV38deCn8Pq+T6D24J/jlu1ehjvL10thXvdLCIiqytbqrarxq0jzO6jNjJaZ/h1+pCfHfQ6z19fJrbeisxZ6P2m18a6q34/xPbr2DZQRKpN7H7QrjF+UURjRPro2Dk5UjEnbZebd9ocyVhTfE6dOveSrgP4r4N1sPF71oa2jGMCAADA0ni7e/3R3qPXve5cUCEAAGBh/DpAROThdy6qCgBYXqW0/39sy4DfeQEAAGA9usG9fvCw0eubfjB6fdHvjl7/wR1nPQsAwPJZqf35ZP+Hl41e/vzA0eu2OdYDAMuO338Ay6nft9YAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABMYHXRBSBfKXaQOIWYqWOUJu/74gqb/51aqfGl6rYd/U2ir86dM0epPrqeWA2lHbUrTPt4KkdZ7W/yuXXOrlyadW2NaxvLNSQ/Wl+drlfXYGvTkzGkwcRS+bp8vYV1r67Hivve3k128+jV7CYiIrsVe1Ux1lZ3NGLuLO8b5TQr7rVwNYTPhH7PdV37sSOxufS7C/UarCMRM3W+qlzGt7etuKF9wVqMv5bD94gisj90TMco9PtcjasrdjtXdtNEnOmfFbN4nw9RV+wcTkufx12JHns5wBpB8+fN2vndqPvk9Nd7bGWTiuWv7XKC8cX6+vtBmbg06++rVH5/rv051uOK19LOEZszHUOXX/Xz92/bfL531x/uq7+VuboPW9UvUOda5jnz52PNhre76o2JPcfHOUcf06zsKSIiK8UeIiKyeeWhVZvNbv2wKqP1hF9f+PWGXwP4dUlqPTwJm3n71Oe4DHTUe/T1H1vr69g2EDvVJqe+UA1d127ss00sdixHTsxJ23XlmjZ2sG/mZ8jecXfh5zgAAAAAbEQPuNe97hy9fsZtP8d/Zn7R6N9h/J9zRpv/5o67/yCybK/F8r9Rif2Xu/ivXwEAMJ3YbzP8f7F4k3vd4l4f4V6f5V7fdoL7w7mjp/YFZtTDrwMAAAAAAAA2Ov+7iD90r296wuj1bW77TPe6ptoDAICNy//+5CG1fb/l//DY0cv/cZs8+wEAALrN8rtLAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAARIQvPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHOwuugClp2VUqyU0eNmAd85VYrNbluImS6XiY+9ymHz5iBWd06NNtLXRPpOksv3ibXRNejcpW3nLIxqE8nhZ1nPpM/ZlcvniMZwbU2kXer4KJ/P1V3XetQ4BeoUjcczUrrxFNafp5EV96dVuyIiIptlk4iIrMkeo37198mKi+367Ch3czlGbVbNaHvFbGq0G7+OatBzWz8fhZp2P0a932+bxGkqjA32H+W1ndvGNOfKx5qE72vUdrumsD5XY3scef1M4N4SmrfR/th9KzKujLkziTa5T6RUnCHNIpc+fxjzc1PO4P7sz6W1/WP767ucoO9QUvV31ejftzYyr7G+/v7gn6NdcXQMf18KLC+y+Az17rE6Y3Oj78ex89dYM1TPBbeuqNYQsdzuuG1eu9W1bPz22Jpt9q3qdK9+zH7u1tRxvV1UNdTrMo19RVWXfx31XhH/PB+9rrjn/GqxxcUZtdu0speIiOxW7FXl2Gz2HO1z64lV99HOrzdaOY2/NmLXYXD3wuk1dJl5TYc+hVj1hkh/UonU0Jk3XGBorR+KnYrTt01XjiFiV+0zPvfNuoZZ6vpMvxFs9PpnqZT8e8FGsoxjAgAAwPJ6qf9D7N9ZuNcVtV23EtgHAABmZ01t+38fda17/ZJ7fed57g/u/98AAJgPfv8BAAAAzN9d7vUFd45eT3fbj3Cv2+ZaDQAAmIf/Vvvzj9zrK3aMXm+fdzEAsAvg9x/Acpr/t+0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIBdzuqiC9jV2QG/d83M4PurSrFZ7QoJ/1dXs3KY8BwUNm88XTWm6rKqr0m0r+eKxdb1xNr53F05SztqU6j/qq3PoWP7mdQzl5MrFWOWSjdlxeSXUbZYCn0V+VrK2oFqTtz5WLPNeS38PLvjK66Hv9Fau+Jijv7rhaFrtzCjPisrozYPFve6XKOv9101u4mIyCZxr3b0uupiVzmNfzWNmhr1Gv/aHJ8+D9Vx0xj++LU1inYb3Td2fflY4/b+Wm9uN+vtvk/pGFV/489feH8oRi4dw+j7Qse1HhuPjpFq3+ibaJP7fu8zDzl1TZujryLzmYb2XJVTPOc1f46t7R9TX1dlIkafXPp9EFuhpWL6GkO1+fexjcxnV9/cODqGv+9a/6x1ff051eNp9a/F9rNfnQfXxs+Vvt/qcbT6TXAtVLF9LtfX/5eEq+e3Krp+fmPntogc99VVzzYb3q7f2wu3b8U/110h/nm96lYHK+6/aLzJ7D7KvbLF7V91MUfHNxd7jl5ljyrHZhnt283u7l43u1xF47V6XkZec9aHi1gXJtupbWunv8eXKob+rND1yU2v63SsWMxUnEnapHL0ydXqE/ns1ivGHJ/HQ37eBgAAAAAsRqleAQAAAAAAAAAA1quvutf/414Pd6/b3Gvs/6sKAADWP/33Kg6q/fmP3OvX5lQLAADAspjn310FAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC7qNVFF4Dh2Mzv+DYz+J6rUmxwfyFm8pimOZ7C9q9b15Wqx6r2pqN9bmzfLnbc5+zMZV0M02wTi+1nTs9YKFcsdqtO1864djpH7HiojhQ/s5NfPf3pXL6GonagGrNV26bZZ0WfD+u2zUqjYeH218/firstbzK7iYjIZrOHiIismR2uzlGM3WS0fze7+6i93eT6F40aVtz5WK2VtGKar/6QUdv+vOnt6tXo90D9z/qYem8Zf+2GY02iqtP4azFyX+qxX8doj6M7htHz0HFRx+rSMVLtx7Wl5zT3vZkTq4o54bnskyNZQ2TOMLnQnJZT3qX1Obe2fzx/vZWJvpPkSv0XVHzMWKz6e0HX59/XNjKHelz63lHaZpxQLB3D36+sfwa5vv486vGE5tb/SV8Neq70Pd7HGD9b3Z/c/rI2V4Xa55+Rfsy671qV0/dvjsuPc602P+PndnPMa9JUxRTffmTV6O3m+Eb1j17XXD2r/rnsom1yz3v/HLdmzeUcHbcrpWvv1gUyWhdssrtVOTbJZhdjszvm2664utyawL36NZp/9fPgX8fP0dpcSVNiuVhdX33kdikzG4bes1YVptuU6rj+TBC7D4Q+A+lYsZhdMfq2icWeJFerj5nsvyM1Sa6U3M+7QB+ltdH37Ua2jGMCAAAAAAAAAAB5Smn//nsZ8NtCAAAAbAR+3fqhhVYBAABmQf/7qc8spAoA2HXx+w9gOQ3/zTcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADK6qILWHZWrJRiq+1CzAKrGbEZ3/VmBvourPrYtb5zUZpm3YXtX6OvJze3de1NRvtU7NTxPrn6xp6En+0hvxWtdJdDsYC3Qd+Uvn39CvZ1+7lZda122lGrFaN7ue3CzWLpepqV0Ys1jVcRkU12k4iI7DCj151md5ezef2v2tHteze7efTqbuebzSjXJpfT1zSuTWTV/dmPZ9WXqV5Xiua20a8SPl7/sz6mr6fC+Otet7eN9no73Nc22lbt1PGqxth+075vFaLbhMcxjq1q6LgAdd9YjFT7cW3dx0XS7+ucGK2YPftMkiOau+NZs2hDjtOzdgE30QA97+WUz6DQXOWOVV9/ZaJfPVcqh3+/xFZPPlZXHF+frku/z62aw1i/6nlU6+5jpWL4+5f1z2TXz58/PZ763FYxIrFjc6Xb6TNdvyeV/n7v2lbbLmupOptW+2ZMX0t9Vgqrjunnmnv1qWyk/UpVs98eZ/H7Vt3z2C8BNrlepfXzvltjPJvUtl+Tr7rn/KpdGbd1awYfc8W39WsBE14LrFbj1eezeR4bdWS+vfX1NQ92imSl1e/BvFhdn3G0WMxUjJwcqXr71Kk/a2X3G/AZnPM5dd6GHN8i5V7bAAAAAAAAAAAAAAAAAAAAAAAAAADMw5DfZQIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABDEFx4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAICZW110AbuaUuzEfQsxA1bSzUoZ3G8G/I4sPRd9x1eaZo2Fza+tb27r2puMGn3sWMzUcVurTecrretr8ubKz1DOzPSNvR6ESi3UvthodLuYMvCW9X39/K66LDvdHK5UhfnObrsYnYnCt3P7V2tnaKcdRd1kV1x+6yJZl9P3HfXZJKN2m43brl5HsTe5YjfVJmvFpVsxkW3TqLqqrno1/lrRx8eTNT5mG22MUduqfeu42va5m33D99VQn65+JhDH1x97W7Rjq3tLZr9Q35w+o9rSz5XUPSAnRk4tQ+YI5p3iGTqpIeoe2jQ1WTu7e3zs/JRTrF/0WHPr99domdE+N4d+H+mVUk4c/d7R9fn7gFVzFhtP/R7jn1f6XuJj6Rj+vuYeM9X58+fLj6c+jlaMyLjEHS9VDN2/z3nSqpwqV+G2q/WUG6i14xpLt8+fU6uf161kvmNztw+5EuhW+vn1basHyYoL6dYAbv266p/7kfWtb79SuxKrfe7Zv6q2/XrEv65Wz3e/38VR81Gnn3/6uRZaJ60X9XPuhT/hxOn2XZ/lSqvfe7F7YvekdR2Pxeybo2pn+s7IdJ9nvdhnzVkaom4sJyuylFfHMo4JAAAAAAAAAADkWXM/y2YZxwQAAAAAAAAAAPLw+w9gOQ337TUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAARq4suAPlKsZ3HCzEzr8FKGdxvBvjuLD2+vuMpzbi2wvarx+dO5bS1Gk2ibSpmbs5gX+v6msnOeZ9xLMIsKyoiwfUV46+mUPvSNo/5tquucv0u8SF8jhV33tbceVy14/Ox4lpb1zr8jqvHGv1pk4vpY29yxVX7awNccQVtKprb+tWPz+hXVYM+Pjpm1THb6FOY5vtdH4+1r5+OVIzW8WS/9j1Wv8XaMcP3ZX3d6H55McL7TUcskX7fZJiKlapliNjRnIln3jSmrW0jyx27tcPdiWPnspzgbq/rT9UZu3bLjn65OWLPjVicUCxfn65H3xesmys9nno/fd/xzyofKxpDnYfCquOB4fux+/y+ie9Z5XDHS7ftx6/H3ahJ9Sn0tn/WVs/icC6/vvHjKWs3dL/Pz8l4Cpp1+qr8ufUhqg9QbiKMbjgqrLmvauuey/7VTfCKf+7b8DXr13z1daNeC+h1hnGvq9Uw/VpBx2yOr5k3WE70eHXduf1+ONU1E4ihr5+U2NpoEWLna6JYHTNgE7OT+pxYtTP9Zy83dkzs8+OQpq0RAAAAAAAAAAAAAAAAAAAAAAAAAADM1/TfUgMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCwuugCMJxSbHB/IWbmua2UjW0zwHdp+fFMUn9pRvUUtl8dfXJa19bMcH775phmzjaCrrNZqCHrGdDHU1dG53EXq3RvuRW3bX0O/3YwxjV358Vt+36rVZxxcf6mbK1thIrVteJibnID9Nu+Jv+6Whv/SiHBNv61iOxfMbZx3ET214/5eqtX46/pZq6i6mcT7dv3uVQf0ftVjELdO03g7aPzGtVHX1+5/braarruql9nr7wYuTX0jZdDz/8QhqhrV9dnDq2NvAESYue+7PEMi9WZqil0rZeRPjpHLLZ+L+r7d1csXY+uxd87rJqbej/dx9+X/DNH3398LB/D9/f3QPcYapwnf270fVfX4HtU9bn9Zev+26491qfQ29J8pup+PlOpnjOjcYxUzxbb3L9S7Qif69bzXg+oFtNn8fWvuIldq9YQKyIissmtWXOf/yIixp2sIrLt1wTVc9vHavWTRrtGjkDeumW521o1ktD7d9JY45j99ufI7es/G80itqY/Fw5pmrkCYqwdP0uWiV3CMQEAAAAAAAAAgDxWpvud53rFrz8AAAAAAAAAANh18fsPYDlN/600AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACauLLgCzV6rvdivEzDyndd+RZwb4Ti1f/yR1l2ZUR2EX991e09Q/XA3ialisoucUFCbcIbJ7dCyRU89BV6yYwjZj+/kt3f4Vl8T4dq4q327FNNvbQA1laGctp6m2TSOmf1317dT++rGqTunus+IGUqj9hej943uNn2dT9bWq7ubxql11XLdvtquL9RG93+h7YXPbj7sI5DCR78hsXV+Bvl39O/tE9qfex7F+fXIPEbvKMeD3i06Sf1b6zt28xe4hs5A6L7ZnLV3XTJn5LI3V1FWLPqexOdSxYzFD71X9DcKxWLFa9L3E1uYj1kffp/yzx8fyMVr9A3NdWNXGhOvWc1fFts3nob73N+tWfXxbve2fsdWz2TZy+PbSGJ+vb7RvzW37D0Y7q+e5bQ7U11gNrBm6qKWo1goqpp/vNRdzp5vTNVenFT+HkqTPrZHmOddrhfGzWYLtcnLoupozWVsb+eusukZ03HHgUh9M8OOY5hu5++bUn6fmwQ6Q038W6tWnZ147g+9GX8R8AwAAAAAAAAAAAAAAAAAAAAAAAACA+Vn0958AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIBdAF94BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZm510QVg/kqxIiJSiJl5LiuliIiYAb5ba5q6SzOqo7Dr7zu+5nk+1qtiBkPXMfWZN5GcObWUantF5fDHq1DuD9Yfd39YCRRhfaNIHb6Lr9M3W3V/WC2axwvVb5S32WZFtV0pbCN2u5117Zrb9Tk2xrq6wrH88RXT7Fu02ttG+7pUH11LIXq/ihfIYVSf1nUV6BPql2pfr7PVJ9qju1/f/JPGrGJHxpxrkpx95Y5/GQw11tJOf4NOnVvbI0fqOisTz9KuWnQdsTnUcxKLGRqXfj/r54mO5WPoWnwNoXuNle4++j5WWpVT9a+P1983fRd/PsrIM3dcpzRrUnNT1mrVfXUf37bQ2y6Lf8b6589OH7ees8rn942219y2/4C00z+v/XPRxVirxi+NHCHGNfJrBV/fmn+22mbda9X2BPdh9WDTawC9dhj365Ojue3rbs7kuF2phqHXSl05dN/1pM/5KSP3rdh+m/E8jfWtjpuuGZ4spmY7z+Jk+tawKJPM76ytx8+5G4V1/yybZRwTAAAAAAAAAADIs+Z+ls0yjgkAAAAAAAAAAOTh9x/AcuJvxQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgJlbXXQBWJxSrIiIFGIWXMn602durGtr1vE8LuKbzbpyxuYqNoOxWEWtQ25foxoWkY5dZ3PFHbRqf+mPR3JXxzuClzqoj2WaMf04/P6VyKvuX4/h25iqj+3MpY9X21W7cfGrxr8vmvl9G923aLW3jfahayDWxxv31fsj7VtnNH595PQNtdO1BftEj6T7duWcJFYrdmScs8iVo89YMZlJ5ri0/Z6Hfa4Nm4gdu0bLjGd0rA6dMzYnetyheK1YkVr880LH8P1DNfj8+n5kJdynrGL5bQn2D90H9XwWVsVWz5F2TlV/bV5Kdd9v9XHbVTu97Wrz4/HPo52Nev1Dydfh89va/473V+fDrw9d8zU30Oq5UhvWii+3Gofbdq873euaeh33a46jD33O9LO4atc/dMXPSfv6GdFl+zmyHeOJ9fV0LkyvnOK5PqT1Ukddacp0o3UkVm9h+Y5rAAAAAAAAAAAAAAAAAAAAAAAAAMD6w99+AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM7e66AIALAdjzGCxikisnBRFpI3uq9vFQsfi1ZV29Lqi2lq1vaLah+gYsTr9ft9eb1f7AzlM1cY2+hbqeCG2sb1qrDru29vG8VCdvk2VszpuVXvbaK/bNcYR6ePp+kW1H8dR/QLnPNUn2i5Qd6tPZH+qb2hOYnLqGNWSH3PSHNHcU/afxLQ1LyOrb1wT6Hsuyx45U+csVn/OtV1GngSxnDpX17j9GLNjtWoL11LvF8uvnznWjbPdvlmD7xe63+n7pJ8765r6+S71/beK3TF37lipngO+T9VWt2v1M41x1J9Rfj6rB7XO5R8cpX9GmeZx92qK0fE12zw+iuFS+LRue6efVz9Xfg3hXtdss5+ew0no62mIJVsRuXzG141rp/YHYxl/ruZ3T15EzmVhx++gqZVTrDuGVprhxrWeLOu4hlTa7nvURrWMYwIAAAAAAAAAAHlKEVlbdBEzwG++AAAAAAAAAADYdfH7D2A5xb5nAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYDCriy4AWI8KMdltTY+20+YaKta0NY9y9WgbSRfbn8rh+3V1T/X1dAx9PGecvk/utyiuTDD9vg5jmjl9LH/cb68UdtQ+FEv1qWKKdX2b26bV3ja2C9PONa7PZvUperarG/exan+4j1HtWuc8lEPa+4KxA31HtaVF+0b25/YP15Pftm/sVq4p+qZMUxfChphTa/vd5PpcI2Uidqr+rtpS74tS3VFjuUI5YmP040nFit1Dyo7xVn1VmzLSpf1Mbo/D9/X3RCvNHHqOCqtyq+fJOO64X1Wv2+fH6J8nflzVc8HHUFWvVcdNo/ZGDtXHj6e657tJMdaP17fX58XWSx7VGxmjT12o17WykbKqt+uqjJ1LLbXe6qO6BlxMf4qruVD1a9WzuXY8tn4az8XoDzYyG8YFtep68/tDxzaqsufzG2ml4XvQAQAAAAAAAAAAAAAAAAAAAAAAAACYtz7fWQIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADARvvAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADM3OqiC8DiFGIWXUIv09Rb2MV9t9c0dZtI38IMf+50TD1jJjNn10zHxpOKnJO6iLTxffVxv6n3d9afqGOl+3CW6DjU9qrb4WtaqcZpG9srgXiF2Mb2StHcX82Zz+1i+m2fQ8+hb9evr52oXWg8+vyMY1i1P9zO0+1Dbaq2kf2x6yjWvitHnxij3Hlx+sSM5pqw35A1TGPI+pdFaad/vkx6Lm1G7txzFhtHTm2xOmLvrVLdobty6Nix8fj6Y7F8nNC9pozUUfVR+6tciXuli6r6dvfRc1O49mXHs7fUdao5K9XzItrebfsK688Iq8fstn1s/zyvchjfb/S603erYpvG8ZCdrrWfo6JsnkNTNGOuldKgNkd1ulhlz7dcbK2RE6dQOY2am1g7vd3Fl5dqWp2fdMiMnO4cTvAMnSf/uaJc53VuJP5zYmmGuJLWj0V+/t1orAxzH1lvuEsAAAAAAAAAALDrKmU5f/+xjGMCAAAAAAAAAAB5+P0HsJz4W3AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGDmVhddAOavEDO3XIbv1Fqoecy+McNdT0UiVuEOd7XyY46F8rsLdVzPVVcpum9fnfVn1qXHuWKsiIismvD+UM2F2EhM29gu3LYOUZ0P1T4nxkrR/M7JWLtqu4pnW+Oo6jfNY0bCY4+1ix1v5lJtM9tpfXJEYwTmYtqYVeye7YfI2ccQ9SFsyLktbb+b5STXjI3k6DMOXWeqDp0z571YurtabuxY/b7WUJyqb6yGSEzdvsoRGFf7WdLcUVp9P1Ux/f1atStrYXw9vo9/jlU93P5SPWdKPXe6Xe28jfv6feFYop5JVrUv3YCMH0/gchwP1dVTDdT6Yhrt/XFT1DOJrFS1tXOsGJ1rQh1v2VINcXxu3f7q3Lr9qVT1c26bO/V1pOncreO+5p7HutoV1fnza4u8WrFxFLZ5NyzNxvp+dF0/AAAAAAAAAAAAAAAAAAAAAAAAAADrGX8rDgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzNzqogvA/BRi5pbLDPhdWtPUXdh+dfTJZRJtU7GGPB+5sUI1F6a5T8+YSRyP1tRRUtexeg6Talc7HqvLt4mFao83nqNq011WZ99Qzs78keOFsY1aVt0fVvx+4/c3t8MxbWO/jl3Noduv4+j+4Rg22Fa3a/Wv2rfH0c6l6lNz5bXbNbcbbSNjTrVLxY61b+ZKt8mN1Yrds88kOYbKPYQh69/VWDvFM3iAeS8T+XPPbdc4UnXqGlI5Q7li7+dS8mL7mF21lol69P3Lt0/d5+rjj91npdai2dcG2+lxF3Yct/T37kgd1Ry47VI9V3S7an+gSp/VP1v8XLXG6V53utcVabbreq6XPnjpn1X6QW99w0Ztobrrx0Nil3mZ+VYsuto1h9FaI+gc1Zyo47H2nXVVfUZ/sJH3k1+zWpsf3K+Dyx59+vLr9DLzub7e+c95NnrXybee5ybnc2Rppp+DafJjGNbaXveNjWIZxwQAAAAAAAAAAPKUIrK26CJmYHa/nQMAAAAAAAAAAOsdv/8AlhN/iw4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMzc6qILwOwUYuaWy8zgu7Omqb+w/erJzWXmMKddOQqTl38R32TWlTM2Jr83Na6ix7T7ULE+fr+uN9YvFCZVT2wu9DBD7VptjA3Wo8ex4tr5/qtue8X4uOM4Oq/PoWP7Pu25au7X/YOxE310u3GtzXHp+RjFsI262+Oywf16PCGxsbfadcRI55i87yS1DJFjmly5Jq1pUfR1Ng92Hs+kGZwHa/Pr7nuNlZHYfcah68utwefOyeVzpN7/pXTHrNeq60zVU6hx+m8Cbt+HA3X52K37q25pXPvudmXtWi5c2zLyzPS5q/H6cajniJ+bVvtQH7/bV+12jGOMtv2ztZor0X+opdBz5Aftg7Ue+K4W18/XsuJefc7g+dDb6pSvqFSxqy70bdD+1FV59TB8XX6/6tfHOEfzukmFqq6NjjbGxbSJwvya1U5xT8+N4T9/lKrdEDX4z0Kl6f6O7/pnIF3HPMXmYr3r+5kTAAAAAAAAAAAAAAAAAAAAAAAAAACM8Df0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAzPGFRwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYOZWF10ApleIWVhuM+B3Zk07jsL2ryU3p+lRWypm7HhXjsKEj01afyxeo49qo2dXHx/HjsfsOhbK0apJxck547pP67gJHw81123acxLOkWpXGBvNG8tZ1S2jviuF2ja+3Wh7NZAjPic+hm3s97l9ve1aazncn3Wfdg1Wtffb4dhGbG1fPL9uG8qlha6naNvI/njs8P6cvrm5J403TY5Z5e+dK2N+l9F6Gbft+fwe4tqwNvYM6h+7VLFy69M15OT2uVI5fOzYvaOUdJzYMzJWQxGZ0zKQQ8euYrp6/TUxnhOj2uv7c+2Ya7viY1lfRzO3z+mfVVVEt79Uz6E6n073EfV8q6p3O6waZ2weGvlV7GqwpW0Gr2qzjZKqceqaQ+PRKQJt66m10LrAx6hiq6SxWNU6RV0CpaoxlCPFqOsrOp7an/VcFJH9sRhlte2uL38N1M6fvq5nSdexCP5zoE3OYj792WaR48Ouo5T0vWAjWsYxAQAAAAAAAACAPPz+AwAAAAAAAAAALBt+/wEsp+G+rQYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACBiddEFIF8hZmG5zQy/G2vacRW2f225OU2P2lIx53H+pjlLk/aN9euaO3+kMOE2fnfRY8py+/h6I6lrtcX76pyx4+3a7NS5CrEqZnP/ivHtbSNOPUURaTPO1dzv6zZqO9a+MY5IX6NitMel58o2au9qEz3eyimdx0MxutqOYob3p/rl5Jwk1qSxh8iVjJmYq3mYdD6WSWmnfzYNfS5txvOy7zVpO8aZex3ouUrVEMoZy5Ub28fsuteUbv7i96lYv1iuQNvE/bRUqdvPueaO0tpoWz+ewrUp3fEVV4Ov089hoZ5JpXoN1etTVlX4mD6W3o5co/X3gs7vY1rfxg/UT5YxwX76/VW/VPyYV1T9vm+1X18KkbeDL6XevCpTxarmUA1DD2safp3or4/c2Mb1s62Bx4/pXLPgr5tSr1ci++traRt5z8f6Vsfd56TSpL/rOxUrxn9OtDP4PnH9XutbGwAAAAAAAAAAAAAAAAAAAAAAAAAAWJ9m9y02AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzuqiC8BIIWbRJTSYGX4X1rRjLezkteXmNj1qTMVMHe/KVZjwsUnHEYqnZ9OoNqnj49jxOrqOhXJovruPk3MF5Pbxx2MlhvrpKWjPkY9tO2sK9W/HtuH9pnl8xY/D5fQxV/x2rb9vs6Lq0+MoVDsTaa/HKVIfq5qDqr7YuNx+adet4/k2oWNd9UaPB8YRjxVvG2qfkyM3xjQx+8aO9k+MfwiTjAv9zXOeS5v77Jq8Jhu5m09yzVurn6F5Mfw4c3L6HKnYOTGrWJH5KyUcI1ZD2VFPoeamTNxvS1VSe10w3lFafW/29Yz+sOLG55uVked8qcZVP16qV/+c83NRPcNUO/HjjsxV0RiHr982c/k2tmrQ6FC6B2OhD0ubf6/4asZz1ExR6mepOh9V/c1Smn1UHTZyKeq1ThXbNhvUc8Tadl2Dodqko1/rXCb4NbSV5vWjz3VZe7/5dba/hnUM6ejbt56+/Oen0qRnYJL6RNqfH232bOeLfQbqWytQZ0XExm5qG9jyjQgAAAAAAAAAAORacz/LZhnHBAAAAAAAAAAA8vD7D2A5ze5bbQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJzVRRew7IwYKcQsuowgM4fvuxpy7IXtX2/f/KZH+1Ts1PFYrsLE+8Vi6pnpM45cRtUVOxtduf2R2Bj97iKz/HoY3yfVVefQ7f3+0PhM5JgeTmFsVuzQNBRiIzHDx8exbGN7xW+b5vF6m+q1dqxedzUOFTs+PivauK/KUZ0HH6t5vFDHx7XVxqGPqe32edI5uvs3Y4WPxfqE5iI3V99Yk8Rs9Y2MbxK59Q5pmrFjctb2e9YMeW2UkdyTXMs28uTIva70POSM09efyuFjx2LW5yEWq4oRmZsychpDz8FY3YWagzJxPy7dYT/3zfGpWLb5PKv2+77uuLgY1f5I7fV8hTpWRvbr+S9V7EL1H8Vw+9yuVXdspztudP1ugMZt+0h+PCtu2wamyufwz/5UX33OC31p1I6X6pg/D34Oqr4m3H5I4/WJzzX6g40+o8cDsTbcpjrXVWx/3oYbiD/XsTrHtbjcE9zHcvv6z1Ol0VdxPKbXt67QZ03bevcMI+cz3yTzukxm8fkQAAAAAAAAAAAAAAAAAAAAAAAAAIBJzf4bbwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwC6PLzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzt7roAjB7Zg7fa1WIGT6mnbzuvvWYzPY5cVNtYrkKE+8Xi5k7Qzp2qJ/JaBOO3W9/nxw+hI/V54rQfTqmtzNHvV/sWGFsc7+KqftX/cRGaytUm1bOats2tn0tvr8+3tjnXo3q22qnjuv2RvXrrLs1V816q/5VLhvcH8rbPj/xvrG6Q7V3to3sT/XrE6NPrEb7wDj6yq2tj77jmIdZjHO9K+30z+1ZnUubUduk5yw07r7vFaueyTnzoMeUqt/XmYqd81wsEzXpe14p8dw6X6zOQo23jNyvy8Dw2usH49rqe3qz3nFu187l9MdDc+Xrr86H2y7V80+30/ur9rVxj2P4+qVZhzu+5uur6nbtXbMVd3784bK2aFjxY/Y5q7qkOa7quM8lzf0upNU1ilQPfR8zdkWOx6VraNZW5a6dtnTb8DXQqsHH9HPV0dave20ipl+3W1Hn3uesDaSs1jSZ9fprQ691IvtD9eTGbLVzn7NK0zVLk8Xuoj+X2s6zNKwhP69OMwddZvGZGmlWuu8XG9Wut7oFAAAAAAAAAABeKSJriy5iBpbxdzoAAAAAAAAAACAPv/8AltPsvwkHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADs8lYXXQAmZxb4fVWFmOFi2enGMUktJrNPn9iptrGchYn3i8WMzZjOoWOH+plEm9TxWO7msXA94xz+eDREsn3umfJ9jNqO5ujMb5v7IzF9jHFMq+LU26qYkTqMy+33+1pM5Lip1apjVX1Vn9bxaPvmeEJ9q7pd29YcRdvp+YiPw6i2XX3rdWu6XVfMnD5d/frEqNpF6s6RU0dn7in755i2RuRZxDyXNu9OPcR1ZiO5Jhm3rjv3PWhrT6bcMfm6Y3W2aumIG4vlY8T6Fmp32TEOfR/WsasaqljNOKFnbKnyj58XRrUbNVzxudzxUq+FXDup1e7bVHXpuXLbpapTz3+s/SivjmFcnc0+vkfpQq+IylmNq3lcRMQPrXQP4OrcGV9vo9yqXh/T969ymWbc+jEd058XX5ft/9aaWlWDVXMbYKo58nOgYrlXv9+vWcspBqZjGH0NVLn9OQ7vDx2LxUrFbLWrfQ4rTd73gevPJ6kcXWKfZ+06/27yIT8LAwAAAAAAAAAAAAAAAAAAAAAAAACwK1rcN+YAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIBdxuqiC0CTWSffQVWImV1sO90Yp6nNZPbtkyPVNpazMJH9HfFiM6dzxGJPQ+cuIili+7tiaT6Ej9XnitF9/FSk6mq1D9VVxbLN/Sp3PKYN1uT3d+U3Lmf7PNhGDStFs12on+6j642Pz9fvY+rxNPuN+rq2em5aOWx4v9oOn5fcvu36gu0C44i11br65vQXGc9FrlTOzlxT9B2yjkkNWT/6s7bfs2bIa6RM5J7k2oiNJ7duXVPOe9mqO3Gsbl9bTi2+jlis2DM11q8IhCkl0rbVTuVWc1QGamzFcE3az3EVy44arriYfm7LwLqqcG1L9cwt9Ty77TLwLO1q3+jj9q2pqmPPWlvlbNbqx18fj5/f8XhcPT6072PDuX0s6+e4qn1cl01ccuO6fC3h3NF2fdq68flznXo31M9X61r0+42fg+Y51u39WttKvF1RXXOJtYCKlSMWOxVLf67oqk1/RiuNnoXu2nJy5Ep9JratMwRMrrS2uq8sk2UcEwAAAAAAAAAAyFNK+3eey2AZxwQAAAAAAAAAAPLw+w9gOa2Pb9cBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABLbXXRBSw7I4WYdfa9UoWY2eeww415mnpNZt/cHDntUjkLEz7eFTs2m/njU/0CNeS0EREp1G5fQ3t/vU84lt+t+8bo9pNcGb6Pj5G6UuulF8Z2xqraqb6F2OZ2ZH+zbzOX3+9r0GPX4/H9db9m3bbRVh+XSLv2eHxNttYnHGOcwwaPx8Zd9VPHg7FbMRLHAzG78nX1ibWvjkv38dy6Jsk9i5w5hqhr1jZCjUOxdrg1wKzmLafGaa/VMpCj73h0nX1q8vlT9wTr7sg5tfl6YnVUOdXxWL9YexGRQu0qI3Xqe7j+tuEicB5KF0PXVaqc7bVDc0dpbaTduN7CtSnVM7TUc+K247UF5lD1qZ697k9+PLEcRbXdrN3XPArq2qhjpfHj8+38uBqb4/rdHh+6ft6quYnEWg93z6KqqfuaERmvb621Wfv9GtZfT37da6W5zhKpnQd/jn0bFaPK2YrV7BcSa6Nj9e0fbKs+z5Um7/vCuz7T5OTNMcnna7uLft+5n6v19u8kAAAAAAAAAAAAAAAAAAAAAAAAAAC7Nv7WGwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAmDm+8AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMzc6qILwPAKMfPLZYf7zqxp6zYT9M/NmdMulb8w4eNdsWOzG8ulc+j+JlBDqk3fGrpmwceKTEXVtzDduYOxVR+fw+9PnUET6R9sq3JWNbRi2WAtrf2NOmxr36iv7czt++ladL/GPpWr0DFa7fR4fGzbqknH8m2ixyPjbo1LbQfbSDh2V4xQu662XX1E2uPtksqRyjVt3FnkXq85sNh5tjbvmTtEjalck7w/ShWzT526nlR+nyt2L7GBp1qsHp9b56xyqP2x9l19CtW0lGY7HVPHqc+Pfw6UkefauJbmtn4u+idfaUP39madhWuzpuZ1/Mx1MVydsdrq14geqx5XUdXXnaNQ105Zf9b6sbkHc6n2l8aPrzkgn1OPz89HYMpaChVrPKe+hnDOSVTrDtvc4c9tTuhq/qfcP43CxK/JZm5/PuPtYm30+txGYoQ+f3TlE2l/7itN/9mJfe5J5R6C4bueEWHdP8tmGccEAAAAAAAAAADylCKytugiZmDI398CAAAAAAAAAICNhd9/AMuJv/UHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABmbnXRBaBbIWax+e3w34k17ZjMFP1zc/epMVVPYcLHYzm6ZjyWS+fQMUzieE6bIjLM+P70HPq+qZY+VG77nJx+fKkyC2PHdcTaqFiF2M7YplXDOEd73m0jdzUHtT6xWKE49Taxa63KWbVrjmdck23U1JwrVZ+qK1ZDa1yRfo020t1Gx+jbLpZXpD3OVm0dMXNzTBNz0hyLipdrmjnAcErb7248i+vFRmqYJlcsZu51F5qXVD06ZyyXj52694iIWPfk0Ll9rmiOjpj6nl3Vo++nKnSppqQVJ5RLzUkZee6Na1H9q+7t81FaGzymn036XFZz5vaXkX4iImt+nlX9flzVtqvB19/K0ZrbcU1+Xgs/HtOsy+8v3f5CDbs9Z659bV58n3EuaeToy5+Xev9YXUWkzmTswDWuY/h1r7U2a79f1/prx6/Jbe29WJ3ratufj/6x6v3rWrEiOarxRGKHpGK12qvPiaWZ/PvEU5+9cmsCAAAAAAAAAAAAAAAAAAAAAAAAAAAbw/DfZgMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKCsLrqAZVeISCFm0WUEFXZ233c15JjNFLFy68htl1NLYbrbxHJ1nY1YXp1rFme0UKljtXSN2teVmJoqV+446rXpHP5Y7tXT7m+TefuPKx6z3WfU1lTbPlczxrgG29jW/fvk0nLmJBk7UbduL7HjgTlMxZi2XeNY5Bym5qYrZt9Y08SeZQxtkutl3mYx7vXO2uGez/M4x2Wi3knPYdc85MaMxciZFz2uVE6fqyt2K6a6X1l3l4/liuWox43dy3Vb366KqWop1dSFnvul2i7U+MpkLW6/adbW6ONilG5uCmsb9fmYvq9V/cTt17WK1J/fqq1/Xutt12Ncd7NfNa7auMcx/LY7Zpp1jcflx9ks0ufsugqruahyhWNU8y3NdqE5mlR1rv043fg66/fTGKmnGl9if6Fy1tfFVmy4jz+3qsKuWPV4zXoisSL7vZzYOpYWi131i3y2LM30Z3+az5apuncVfNN1XCnD3qPWi2UcEwAAAAAAAAAAyLPmfpbNMo4JAAAAAAAAAADk4fcfwHLi770BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAICZW110ARhOYef//VWFmMFimQljTVJDbp8+NRUm3DaVq+usxfLrXLEYJqNdTptQLYXx+8O1dY0r1rddW7h9McGl4vv4uozajtbQEaOVQ2wztonsr2qwrXiF26fH6tuOc4VjtPvbRrs+bX3dmnH7q/FVcWo5WrFUDD0etd0eb7uW3jFMZDyR/SLNMU0bK6f/NDEnbd+lT52TGrJepC1yvq2d4Hk9Zb1lJOck86Dr7xND902NS9cdy1WPG4vpY8XuZ9Y9BXQOHzsUt4rpjsXalpF6/T29DOTWsXQur1BzVEZqKV235tphtLGWeQp1LXq/uP1lrUb/3Itdg9r4eWlU3c3c9edpqfL3VcUyvtZmTpHxObJT3jqKVo7xMX+dFD5HpK1vl6plnGucxKrrvzo/xo9PryGa+6v2VQ5/nsb9/FrZ52r18edW1xKIVY8Xrj8SS62u9fFQ7FgOLTd2q1/GZ9TS6LvFcIb8vAoAAAAAAAAAAAAAAAAAAAAAAAAAAIYx/2/IAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAuxy+8AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMzc6qILQFNh18d3UBViZhbbTBl7ktpy+/SprTDdbVM5Y2e6qwadMxoj0U4fD7UpImXE98fr9odSMX0Niakd9+vI0fcqGfe32f2rPmIb2zHj8dnGts9Zz1uNo3YsJ4bR26qdzteHceMs1FwF2wbyNvarvu1xdh/PidG3nZF0jlafxPGcuU7FmLZ93aTnfsgahrRe6tiVWDv5M3zI85VbxyTXfBmJnVt/qLZUX90nVreurSuuj6ljtWKoe591T5JQ7GjMRDufs3WvD5RfqunTzxEdq8pV9W/u76Kfa6Ube2Fto5bYePqo5sz19XUWftvXVD0/3X7b7B/KPR67NOr3CxMfezwu48aVX3+15rE6l28gqt7R69oMbtO6lmqcbkBdKQtV5zimOvdqv7XNdY2+5rvoPtW5VZUWahyN+lwfq/tEYunjXqxdPYenc6Vi5+SIxsr8DFyaPjMP5LFiJ7pu17vUexgAAAAAAAAAACwvK/1+p7lR8NsPAAAAAAAAAAB2Xfz+A1hO6+PbdQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwFJbXXQBy87YQgq7Pr5XqhAz8xxmwBzT1JvbN7fewqTbpXLGroKuGmJ5o7FUe91OHw+1KVQTX197f5iP1zVl/pCO2Ypl+rXviqHryn1X1nPmjK2Z02a1rx+uxmya38k4zm0b24WZ/Lsbx/Pq6/Sxm3WnchjXvqtdFVu10Tn1fi8ndqytPl7tl8lzaMk56nGe+rTNyT1krvUSe5ammc/1pLSzeebP47zajNonrSMndu41EJvjnNp0HbE+ul1XbboeHdPHisXw/fW90daeUqmYVYzIM6yVy7Wrj1M/D0o1za1YupaqX/N5WTbCZq7/XC2xGko9p7VxtPOH21bt/LavsOpvGvU3zl8shn9+W+uDNWL7/aXbX/iQxtdaG6s6t31Va4nOHKLqC7ctAn2DOVXcUZ/RhtXPfn9cxdD7/RraWtW/tsgr3TG/dk7lqs5taz3iz3l7oPHYzfOjY/ZtV8/l6ZwxXZ+JuvJlxR7gc3VplvG73Mdic2TWyb+TAAAAAAAAAAAAAAAAAAAAAAAAAABAhC88AgAAAAAAALALuuaaa+Sqq66Sm266SR588EF51KMeJYcccog87WlPk6JY7JeFlWUpX/va12Tbtm1y8803y+bNm+XRj360PPnJT5YnPelJC61tUss4JgAAAAAAAAAAAAAAAAAAAAAAAPTHFx5tYIWYuec0A+actv5J+ufWX5j82Kk6Yn89LlZLV+5oLNVHt0sdH+XNq0/v9fV2/TVAHbt13MfOnPZQe5+j71XhYxTGNvqnah7VYVWMyP4ql21s+5zdOcJ9tXHdNti+nku37Ws8V3biOOP6bHC/zhU7nttGZFxvrF+q/xB9c2Ln5hoyxzxipUx6PSJsPc1nafvdmYe47mwk5ySxY7FSc9w17lgdOlduu656fB06lo6h+1f9pB3XSjOmjxWNkWoXqLtq6/KXiZyl6tdlnN+4Ov3zzNdjXCwJ1jCEqgZXb+mfi3671d7VZuv7mjHWBqtuTK8Z/BwULnXp1zrNKW3UOSumlbt5PoN9RF03qqlfM1t/Tbj9rfMR2F+o/D6X1esP1dd/lihb65Tx9abHpNfp7RzN4zp2rF1X29hnA527S+7nt1gNQyjsYr+8BOtPaW3nfWOjWsYxDclaK3/9138tf/EXfyFXXHFFsM2jHvUoec1rXiNvf/vbZcuWLXOt75577pE//MM/lL//+7+Xm2++OdjmqU99qrz5zW+WX/3VX239O59cN910k1xyySVyySWXyKWXXiqXXnqp3H777Y02dqBraV5jAgAAAAAAAACMfnc8i98fL9oyjgkAAAAAAAAAAOTh9x/AcuILjwAAAAAAAAAsvR/84AfyS7/0S3L++ed3trv55pvlPe95j3zyk5+Uj3/843L00UfPpb7/+I//kFe96lWybdu2znZXXHGFvOENb5AzzzxT/uEf/kEOOOCArPjf/OY35Q/+4A/kkksukVtuuWWIkpNmPSYAAAAAAAAAAAAAAAAAAAAAAABsPHzhEQAAAAAAAICltn37dnnRi14kl112WWP/Yx7zGHnqU58qu+++u/zXf/2XXHXVVdWx66+/Xp7//OfLxRdfLE94whNmWt/VV18tJ5xwgtx5552N/UceeaQ84QlPkHvvvVeuuOIKuemmm6pjX/jCF+TFL36xfPnLX5Y999wzmeP666+Xf/7nfx669Kh5jAkAAAAAAAAAAAAAAAAAAAAAAAAbD194NGeFmEWX0GJmWNMQ450mRu7YCpPZLiNekTgeq6mrBh3TRNqm2oVqK1QoXZ8+Pt4fPtA1lf5QLKbOmdu+K4Yfs4lsJ+PU/jyOYfsXFBAanzG2kddvt+py+43eVv2z6qj6+BjNmOOceXHq+avYrfpssK/OGRt/TlsTOU/T5NC6+uYcz8kxScxZ9A3pW/usDT2+XZm1w60NhrxOysy6JrkWYmPOjaX754xbjyeWS8fuqilVh8+pY8T6hebc31+tdMfSMVLtQm2rNi5nKeHjs+Cfh9aVV9Wg1hRlYBzi9pXquezb6jNYHfftVf+ccZpWXxfbD8ANqJTm/tLtL3xRtVTllG9fv3YY54znSLVttUvUVp+xWJ9qbWObuf0a2tpmB72/vt6q6vbzbP1azZ9zta7S/dQavKy11+vuUtcVyTHOFY89Tdt67phYTV36fv5L1QgA3ute97rGlx095CEPkb/6q7+Sk08+WYpifFf/2te+Jq997Wvlv/7rv0RE5I477pAXv/jF8s1vflP22GOPmdS2fft2efGLX9z4YqAnPvGJ8tGPflSOOeaYat/a2pqcccYZ8sY3vlHuvvtuERG59NJL5ZRTTpGPfexjE+dfXV2VQw89tBrzEBY9JgAAAAAAAAAAAAAAAAAAAAAAAKxffb4TAwAAAAAAAAA2lK985Sty5plnVtubN2+WCy64QF71qlc1vuxIRORpT3uaXHTRRXLooYdW+66//np5//vfP7P63ve+98kNN9xQbR922GFy0UUXNb4YSERkZWVFXv3qV8sXv/hF2bRpU7X/n/7pn+Tiiy/OylUUhTzpSU+S17zmNfKBD3xAvvrVr8rdd98t55577iBj8eY5JgAAAAAAAAAAAAAAAAAAAAAAAGwsq4suYNkVYqQQM5dcZk55RGQmY5o05iTjLkxen5yaUt8aFqsvVkNXPJPZR7fTx4tAGF2nbmOq/eHYurR6/9gs6r6hunLbj+vrjtGKaezE/XUdhdjm/qqdbWz7nL3qjNSb7BdoX+3LrMNIuJ2OM8k36FV9VS26ttCcxerX9cbmO9a/6/xEcybmss85zz0vk7YPmeSa7GuIOjE/8zhf1k7y/J6srjIjV+6YY3Xn9Nd9Y+PR9cZih2rRbVM5fa5Yv1CNVR91v7XSjBWLoXPWa2y1VbkL6/c3YxSqfeFiloFn1FrsHIp/Juk63fgmuPyqunSdam5i57w9jnG70tVTzZlqKxO8x1r1V3PRzOXnvzoffu3j58htr83wVlLV5rer3G4eAidMn1ur1wzSjOnX1IW1wf22lqN1Dao6/Fo7lXO8v3auW2ub8Bj1el7nCsWO5ehq29Vey/msFqsz17w+b29kzFGcdf8sm2Uc07T+4A/+oLH9+7//+3L00UdH2z/84Q+Xv/mbv5FnP/vZ1b73vve98mu/9mvy0Ic+dNDa7rzzTvnTP/3Txr6/+Zu/kX333Tfa56d+6qfk93//9+W0006r9v3BH/yBXHDBBZ25jj/+eLnrrrtkr732mq7ohHmOCQAAAAAAAADQVEr794/LYBnHBAAAAAAAAAAA8vD7D2A5TfL9FAAAAAAAAACw7t14443yr//6r9X2HnvsIb/xG7+R7HfcccfJMcccU23feeed8pnPfGbw+j796U/Lj370o2r76U9/uvzsz/5sst9b3/pW2X333avtL33pS/Ld7363s89DHvKQmX/Zkch8xwQAAAAAAAAAAAAAAAAAAAAAAICNhy88WifMAP8MoRCT9TOLuCmTjLswJviTW2f7ePsnVWespljsEGOMmIw+up0+XpjRT6jeWBvjfnLrDeXQx7rGWs+Z2z6Yy/0YM/rR2337j2JYKcT2rKRNj88YW/2M8za3C2OlMO3cfn9u+/BYR+Oqxtmjb3SMvp6esXy/qjbVvz5X1T6xjZ+cvqEcoVpjfWL7c2KG+nfFSvWJ0TV0/eTqqjv1M6lpcvIzn59FnttcQ74fpqkpt09ubTmx+ubKiR3to+7Dqf59crXqnuKZ7OscjyG8dmnX2VwP+FqrZ7HUn/GZ15Nqn1wjmXre+JpLJLQWGtU9Xn/o8TTXTEN8UGyvedo5qmOptqpdSn3tH+vj18F6LqPrXLXfr72719/NtX/ss0LXnEc/myQ+4/T63NTzs9mQnxcX8bkXwK7l7LPPbmyfeOKJss8++2T1ff3rX9/YPuusswary9P16Zwx++yzj7zsZS/rjLUoyzgmAAAAAAAAAAAAAAAAAAAAAAAADIcvPAIAAAAAAACwlM4999zG9nHHHZfdV7f9/Oc/L2VZDlDVSFmW8oUvfKEzZxfd9pxzzhmgquks45gAAAAAAAAAAAAAAAAAAAAAAAAwrNVFF7DsjPtnEYo55B0yx6TzVJj+/XLrzvlGsFTdqfq6chjVN9Y21a5QJYRqbrfRx00wdmx49d06tu7rj6fOSlf7WI6YwthGDN8/65yrOgqxzf1VO9vY9jmnMa7X5+yOqds360r1de2q8drG/j50zmo7Ur/eHxqnriMWS/fNbZd7rCtmbv/cNn1y5uibc73EHsoQc7jelXb2a4GhzrWdoNa+uXNy5F4XsbntqimWP9ZHt9e1hWpIxdLHYzm6Yqf66PuzlWZu3z8016Xtbuu/XmCS6yXFP+9Kaxt1d81JOqYbo+tbqrWBjhlt77cDOVrPetd2rXe1bdW6qJqbZq7Sz5HbX/o1g98eoIYYvYbzubpyt/qoS1Dvr86TW+RZq85fPXaijV9L++trXJPrJ81+dVUMtVouW2shdTySK8TqWJG2OmdMzuet3FjaPD5f6/lYlKHGuqh/J7ERWJn8WlzPlm9E07nyyisb2894xjOy+z7xiU+UfffdV26//XYREdm+fbvccMMNcsghhwxS27Zt2+Tee++ttvfdd195/OMfn93/2GOPbWxfddVVg9Q1jWUcEwAAAAAAAABsJKUM8/vi9WaWv38GAAAAAAAAAADrG7//AJZTznd7AAAAAAAAAMCG8qMf/Uhuuummxr5DDz20Vwz95UZXX3311HXFYh122GG9+uuxfPe735W777576rqmsYxjAgAAAAAAAAAAAAAAAAAAAAAAwLD4wiMAAAAAAAAAS2fr1q2N7f3220/23HPPXjEe+9jHNra/9a1vTV2Xp+vTuVK2bNki++67b2fMeVvGMQEAAAAAAAAAAAAAAAAAAAAAAGBYq4suAE2FmEWXICLD12EGiFeY/jH6jiP1DWA540jVGcthOvrl9tHtChUyVH+7jT4erkvv9nG6Ru/rS51KHyu3fVeOSWI0+9vWvmlVc2Wsy1HP29xXuO1U6r7t66qxGtvdMBln3L9vLBNpr/cbabfTuWKxctvF9nfF6tO3T7uuXEPEnmeslGmvP4QtYl5LO9nNcprrzWbm7JMjFTM2t13jj+WP5dLtdbuu86vr8LF0jFiOUGwfM7dP1d7du61096/H0Llyz/EkdE7/nC5ts+5WPzeuUq056rFiZ0g/77Pb1+ah9POYuH58n6pONy6/ACilub+sLXjslLeQ8Vyq7XobnyPRNtYumru+4ceqBjSuz8+/DdY9DuPa2fYarky0qcahFpS+Jr9Ot6G1joox3q9iiR5fOFeI/pwQqiOUU9M1dOn7Wa1P7GkN8fkVWE8m+bKY/fffXw444IAZVDNfd955Z2N7kjHpPnfdddc0JTUMVd/tt99ebQ9Z3ySWcUwAAAAAAAAAAAAAAAAAAAAAAAAYFl94BAAAAAAAACypE088sXefd7zjHXLqqacOXsu83XPPPY3tPfbYo3cM3efuu++eqqa69V7fJJZxTAAAAAAAAAAAAAAAAAAAAAAAABgWX3g0Y4UYKcQsLPe8mAFzFWayWH3GW2S2yxlXqt5ULtPRX/eNtdXtCtVMj0MfH7XRMVSfqoZwrnb/eH2xvqG6ctvnntPC2GCM3P7NPi6WimFcjqJq38w5jSqWypHbftTHxpqP2rrjqfMxjtcRq6qzO2cVK1BvvaZQW50rdjzWLrV/kli5x3NyDBF7yL4xfeufh1mMc9lYO/xaYchrocysr++5zhl3KmYsRtf4Y+OJ5dI5dLuuceg6fO5UDH88FNvH1OOI9dHt/b3cSnxcPkYsl+5TuK6lehaXvgbXv2w8B10bt8vXk8pZHXftbY/Lrjofvp5Iu/H6JK/9qO3o1Y9H913LL7Ndj59XP9ZILj8n+nxkPv57qdYdrVrctk8dyO3Xt6X112KTXzNbvRby4/LtjL8GAtdVpI0+Hqsp9Pmjqkftb8VSfUs1Qr2+Lzsu4tjnIJs4qTmfyXRduWb5+XbSmhal71ws6t9JbASl2A13/nMs45gmpb98Z/fdd+8dQ3/5jo45jfVe3ySWcUwAAAAAAAAAsJGsyXS/J16vlnFMAAAAAAAAAAAgD7//AJZTn+/4AAAAAAAAAIAsb3nLW8QYM/OfU089Naueri+dHrLPpNZ7fZNYxjEBAAAAAAAAAAAAAAAAAAAAAABgOquLLgBhhSz2L/aYgfMXA/xFpUnmpO83eqXGnTOOVM7YX9rq6pfbp1DN9Hj08VEbHUP1idQUitW1X0TEh+5qU8+Z2z6YoxXD9uxvG9uzMK7Rtvb5eo3ajsZS7XPkzs04tm1uZ85pM2e/Pj6nrqUrZmw8ul2slpxrJdY3d3y512OfmJO2D+lT37SGqBeTm+X8WzvAs79nfWVmzpxxp+qPxejqFxtPrG6dQ8cO1RDL73PrXLEcXbH1OMpIH92+aufu7XaStV1kHNOI1ePXH6XNqzf0PPd16tmsjrs+uv24BtfT7S/rOdy+1Lc4+7qKKobvb32D0X5R+0WknPIzQbWOrObS7a+FHeftbuvb2cTbV7evxy5V2/E6q5nTr6GtXoeomutrdGttZxt9XFQ7vwYvAwNM1aNjjY8bdVyvmeLnN1RHvZYYXWPIpJ91df1DWvTnb2DWPvWpT8lhhx3Wq8/+++8/o2rma6+99mps33fffb1j6D465jTWe32TWMYxAQAAAAAAAACQ69vf/rZ8/etfl5tvvlnuueceOfDAA+Wggw6SY489VjZt2jT3eu677z655ppr5Nprr5XbbrtN7rnnHtlrr71k3333lSOPPFKe8pSnyOoq/1dyAAAAAAAAAAAAzB+/pQIAAAAAAACW1GGHHSZPfvKTF13GQqz3L99Z7/VNYhnHBAAAAAAAAABAyplnninve9/75OKLLw4e33fffeXkk0+Wd77znbLffvvNtJbLLrtMPvWpT8kFF1wg//Ef/yE7duyItt2yZYucfPLJ8ta3vlWe+tSnzrQuAAAAAAAAAAAAoI4vPJoxIyKFmAXlnn3ewgyXY9J5Kibokzs3OeNL5TeRGLF+sfZdfQrVRY+vfTwUQ/Vp1dUdS++v948NSfdNzmVH+9xLsW/OunEfO1VOY2yPrCqW65uquzC+Rt8+ndNktBlKMcUceHoeYzF1u9j8d9UU65M6l33G2fe6mOY6GmL+tWnqWaYaNiprh18zDHE++taVe22XGXFT9cdq6+oX66PrjtWnY4fipdqkcvn+XbFjMVOxdDv/3LG1lUksR1ddk4rVHduvVWsj69p3rC/Ha4Hu2Pp4zhU9ftYb1zcvVx/jsfodEs7lDvh2pU89wO3Zr6NKvx2ppStVtX4yfq5sPcQ4h/Wh9bXbjONrGYV058zaYBu9xo+1C33+GNepYqjR6vVhqba7Pm+VOlZkoetrienzGVTXnzLLz9V6/Otd37lYzL+R2Bis+2fZrKcxvexlL5PHPOYxM8/zzGc+M7j/YQ97WGP7tttu6x371ltvbWzvvffevWPErPf6JrGMYwIAAAAAAACAjcRK+3eFy2D9/Paj6Z577pFf/dVflY9//OOd7W6//Xb54Ac/KGeddZZ89KMflRNOOGHwWu6//3558pOfLNu2bcvus337dvnbv/1b+ehHPyq/9Vu/Je9617tk06ZNg9cGAAAAAAAAAMA0+P0HsJz4wiMAAAAAAAAAg3ve854nz3ve8xaW//DDD29s33bbbXLvvffKnnvumR3jxhtv7Iw5DR1L50q599575Yc//GFj32GHHTZ1XdNYxjEBAAAAAAAAABCytrYmJ598snzuc59r7N9///3lqKOOkoc97GFy/fXXy+WXX179R3F+8IMfyMte9jI5//zzo/9Bh0nt3Lkz+GVHxhh5whOeII997GNlv/32k3vuuUeuvPLKRtu1tTV573vfK9/61rfkjDPOkNVV/u/lAAAAAAAAAAAAmK1i0QUAAAAAAAAAwNAe+tCHyqMe9ajGvuuvv75XjG9/+9uN7Sc96UlT1xWL1bc23f4xj3mMPOQhD5m6rmks45gAAAAAAAAAAAj53d/93caXHW3atEn+7M/+TL73ve/JeeedJ5/4xCfkP//zP+XKK6+UZzzjGVW7Bx54QE488UT5/ve/P7PaVlZW5IUvfKF8/OMfl1tvvVWuueYaOe+88+Qf//Ef5dOf/rRcf/31cumll8rP/MzPNPqdddZZcuqpp86sLgAAAAAAAAAAAMDjC4/WGTPgP9MojMn6yY4nJvmTjhH+6dJ3blLjy8lvjGn8xGLE+gXrUn0K0/ypYqjxtY9La/R6rDqXMaOfnFhd46vXE+sby5lqH8o/jmGlMDbZr2ovVgqx1Xa/nFaMsePrIzN345pyffyYUzF0+xypsY2vrbz6u4znc/TTriW8v11TvJbYMR07lqsr9qR1p+bO96//9Gmbk7vrJyWWs+unr0lyDF0DxmZxPoY4L7PKNcv3yRB9cmvKiZWbK9avaxyTxtLtjNjqJzfmNKJzFalBH8+pqVoTJNrqNUS0nY/Vo08sR2ut5J7V1brLtI9Na7zOaa7tTH1flTPcNplDta+vlfXaZ5wjsuaPrLH1/tBnAr2mz137d33GiH1GSX3G6fO5Kfez2VCfD0P1z/LzbUrO59X19AOgnyOPPLKxffHFF2f3vfbaa+WHP/xhtb3nnnvKwQcfPFhthxxyiOy5557V9g9/+EO57rrrsvtfdNFFjW091kVYxjEBAAAAAAAAAKBt27ZN3v/+9zf2ffKTn5S3vOUtsnnz5sb+I444Qr74xS82vvTohz/8oZx22mmD17XbbrvJm9/8Zrnhhhvkc5/7nJx88smy3377Bdv+5E/+pFxwwQXyqle9qrH/T/7kT+TGG28cvDYAAAAAAAAAAACgji88AgAAAAAAALCUXvCCFzS2L7zwwuy+uu0JJ5wgRTHcv05dWVmR5z73uZ05u+i2L3zhCweoajrLOCYAAAAAAAAAALTTTjtNduzYUW2/7nWvk5e97GXR9nvssYecfvrpjS9D+vCHPyzbtm0brKbdd99dtm7dKn/+538uj3nMY7L6rKysyIc//GH5b//tv1X7HnzwQfnEJz4xWF0AAAAAAAAAAABACF94NGOm5z9DKIyZ+ic7l5isn7xY3T8xk8xn7rhzajDGNH5yY8T6dY29MKOf2Njj7aQxG6Ext+sb/bTG24rlfiL9jWnXE+sby5lqn3PJ+ronzmlECrFSiM3O2arBWDHG1nKPtifpG6/XSuHapmJXbWX0E83tjvv204jFyI1tMsamj8fa65z12LEYqVixcaT6x/J3jVPn7HN+Yrn6XJN9Yw6RI1lDYfmZ8GeW5nltDBl70vfYJHn7vu+nidX3PjbJvXGI58XQutZzqXpj65h2Dlv9xIzXI93Pi/EzWjpWtD5vc12RW28feg1UzafLpcel28X6Nfu67WSOcLvO+lWf9vHRuji2zvX8mjs0x7G1vj6eahdqH6tXr+dTn4X6fNaa9PPdLD5zxsY3j38WbZnGsl6VYpf2B2Mvf/nLG9uf+tSn5M4778zqe/rpp3fGGoKO+ZGPfCSr3x133CGf+cxnGvtOPPHEocqayjKOCQAAAAAAAAA2irUl/lkv7rvvPjnzzDMb+37nd34n2e/xj398499779y5Uz72sY8NVtfq6mr2Fx3V7bHHHvL617++se9LX/rSUGUBAAAAAAAAADC1Rf+OYlf4/QewCHzhEQAAAAAAAICl9LjHPU6e9axnVdv33XefvP/970/2+/KXvyxf+9rXqu29995bXvrSlw5e34knnigPfehDq+1///d/ly9/+cvJfh/4wAfkvvvuq7af/exny2Mf+9jB65vEMo4JAAAAAAAAAADvvPPOk3vvvbfafsYzniFPfOITs/rqLxY666yzBq1tUkcddVRj++abb15QJQAAAAAAAAAAANhV8IVHC1YYM/jPVPWI6fWTjpf/E2MS/3Tmz5yj3JqMMa2fVKxY31T7Uf3NHz0n7fHqdtJoFRp/u87Rj47ZjuV+Iv3DdY9+Yn1jOXWcEB9rnN9KYWyr7nh/K4XYVv15Oa0YY8fnUeXOidVX7vj0uLpuEbl16vF2tUnF6Nsvp65ULD93OTlTMXQs3S81zq42sVyxnDk5+s5vV4xJY7ZyFHawH0xuyPMw7fnoc93N4ppOmeQ92TdXql2f/Ln3oViOSWLGYqSO5+Toa5J7vBErRtJzWz2LO56v1TMzEqs6h5L3wazXc0D8OqSZQz/Hq7WPWzPU13CptcN6Ml6HNdeRofLHY27Ou18rx9a9VX/3j16r19e34/nt/gwwrin+GSP1GSX2WafP56e+n9Gm/ZwYqnuen3tzpT6LzvofAMN597vf3dq+9NJLo+1vv/12+ZVf+ZXGvt/5nd+Rhz3sYZ15brjhhtY9/YYbbujss/fee8tv/dZvNfadcsopcscdd0T7XHLJJa0x/dEf/VFnnnlaxjEBAAAAAAAAAOCde+65je3jjjsuu++znvUsWV1drbYvv/xy+cEPfjBUaROr1yQi8uCDDy6oEgAAAAAAAAAAAOwq+MIjAAAAAAAAAEvrmc98ppx00knV9oMPPijHH3+8fPzjH5eyLBttv/a1r8mxxx4r119/fbXv0EMPld/4jd+YWX2/+Zu/KY973OOq7a1bt8qxxx4rl1xySaNdWZbyT//0T3L88cc3/qLBq171KnnGM56RleuWW26RG264ofXzve99r9U21O6GG26QW265ZV2NCQAAAAAAAACAebryyisb233+ffaWLVvkKU95SmPfVVddNUhd09i6dWtj+8ADD1xQJQAAAAAAAAAAANhVrKabLMa2bdvkkksukUsvvVQuueQSueyyy+Tuu++ujh900EHJ/0L6elAYI4Ux88kl88kzyjUcM2Hdk8xrbt0mI3YqVipGrH/R0U3PVayt3q3nKpRbl+tjt2NFcnbWnde3a+wi47qr9pn567F1jJRCbHaOXMbYRi2FqeVQ+3TbGN0+q62E26aOa1WNGbmH1Defbh+bq645TOVMzX+f85MrJ+Ys+kZjFvO9DroUpkw32kWVdvbfedn3WrDl5DfZ1LVsbX7sWKxUDP3eLTNy6lyxHL5dVw2p/KkYseP1uH1j6hg5czIt/dwM5ayO6f2Zdfrno+1Yw/r1Qxlp4/fGrtzYOOrtq2O+rWvjt2XA+a7WXtaNy+2o3ubG1+nameb4fbuyilMPrvu6bVFtE+2smsz6urLVx+pxqZzuQGn9uY70r51fKzbYRl9v/jOCteE1nm5Xl+rj6bV/qSan6/OXVVdl6mkRe9JO8tm0zFx7Vjnm9Dl7GTBXcdaUUi7hmtEu4ZiGcPrpp8v1118vl19+uYiI/OhHP5JXvepV8ra3vU1+7Md+TDZv3izXXXdd6y8p7LPPPvLZz35W9txzz5nVtmXLFvnsZz8rxx57rNx1110iInLttdfKMcccI095ylPk8Y9/vNx///3yjW98o/XFREcffbT8zd/8TXauX/iFX5Avf/nLWW0PPvjg4P6f/dmflQsvvLCz7zzHBGxEy/L7DwAAAAAAAKw/pYisLbqIGVhPv/245pprGtuHHXZYr/6HHnpo9fsKEZGrr75anvOc5wxS26TOPPPMxvYxxxyzoEoAbGT8/gMAAAAAAACzwu8/gOW0rr7w6MILL5Q//uM/lksvvVRuv/32RZcDAAAAAAAAYAls2bJFPve5z8kv/dIvyRe/+MVq/3e/+1357ne/G+xz6KGHyj/90z/JE57whJnXd8QRR8h5550nr371q2Xbtm3V/m9+85vyzW9+M9jnuc99rvzjP/7jTL+MaRrLOCZgGvz+AwAAAAAAANj4br/99ta/33vsYx/bK4Zu/61vfWvquqZxySWXyEUXXdTY9/KXv3xB1QDYaPj9BwAAAAAAAABgUuvqC4++/vWvy+c///lFlwEAAAAAAABgyTzykY+UL3zhC/KhD31I/uIv/iL6pTsHHnigvOY1r5H//b//t2zZsmVu9T3taU+Tb3zjG/KHf/iH8nd/93fy/e9/P9juKU95irzlLW+RX/3VXxVjzNzqm8QyjgmYFL//AAAAAAAAAIazdevW3n32339/OeCAA6bKe+eddza299xzz96/S9A13HXXXVPVNI0dO3bIG97whsa+Zz3rWXLMMccsqCIAGw2//wAAAAAAAAAATGpdfeFRzG677SaPecxj5Prrr190KRMrZP39RZ1ihrHNAOMtpvjLTbljy/0LVDnxUrFiMYqObrF51H10q9jchWrQTX3sdsxwrNz+OX2rmlSsPtdqYWwwRrwW26gl1b5ej3G5/HYst243DZ0jXmNzXN0x83LOQiy2kclzGhUzt37dLydGV5+c412xp4k5RJ9G/2J214BWmHJuuXZls5jn0k53l+tzndmy3xqh6z1gbV4sHSPVL/TeLhN9Ujn61ODz65y5OUKxUzF9n64YqeP+/m8z15LVM9YNq4z0q4+7z5jr/POyjFxO9WeutfpYcy2g57C6Xtz+Id6hut5Wjmru/LZrVy/eDSo25kXS6y0/Z+NrIaOPGld1fqp2fvz+uoz39+t261q15l/F1p8drLXBdo36E3083Tf22aDUF6rEP3/YyNosdefvcy0P8Zm5nGINCWDXYYyRN7zhDfKGN7xBrr76arnyyivl5ptvlgcffFAe9ahHySGHHCJPf/rTpSj6r28f97jHVffnSe21117ynve8R9797nfLv//7v8u2bdvk5ptvls2bN8ujHvUoOfLII+WII46YOP6FF144VX2TmPWYgI1uGX7/AQAAAAAAAMzbiSee2LvPO97xDjn11FOnynvPPfc0tvfYY4/eMXSfu+++e6qapvHbv/3bcvnll1fbmzZtkg984AMLqwfA8uD3HwAAAAAAAACAlHX3hUebNm2SJz/5yXL00UfLT/3UT8nRRx8tT3nKU+Siiy6SZz/72YsuDwAAAAAAAMASOOKII9btF+0URSHHHnusHHvssYsuZTDLOCagL37/AQAAAAAAAGxs+guPdt99994x9Bce6Zjz8rd/+7fy/ve/v7Hv1FNPlR//8R9fSD0ANi5+/wEAAAAAAAAAmMS6+sKj1772tfLGN75xol8ArldGjBRiBo3Z/78vPz0z4BgKM12sScZvMnP2iZ2KGYtVRLp1zXG8j27X3KNrCJWsY7djqu1ILN8uVKo/lpoTo7Y1o+LUayiMjUQP9ykk1d62c3T26M/XbEL7EuPp2z41Pzltcmta7/Q4usYVm5PUXAxxPvrO9zTnxxSzO7eFKWcWe1LLci3nsHbYNUiXvue6tJPfVVPXrC3zxx27HlJzN0k//d4vM3PEYtZriLXxOWO5Yjm6cqdiriddtforMHXlxmIYt1bw65ZwDtc3ss7ze2NXtD8PVa21HNX15PaVantI1Rj9tnXjcgul6i1ZzUWzRj9+367+Fo331TnD7bRqrVc/Humjc1ifW1RuF7R0DWy1fxzLx/Zretu6Ppqxq35V3a6ftcF29bae/jzS1TfUv+tzmR9rlSvxedBGruJp1s+TrCCG/uy9LIb8PL9sShEpE58NN6L1twIHAIgs5+8/AAAAAAAAsP6Uspy/K1ivY8r9/6ZO22do5557rrzxjW9s7HvJS14iv/d7v7egigBsVPz+AwAAAAAAAPPA7z+A5bSuvvBon332WXQJAAAAAAAAAAAAg+L3HwAAAAAAAMBwPvWpT8lhhx3Wq8/+++8/dd699tqrsX3ffff1jqH76JizdtFFF8nP/dzPyY4dO6p9z3zmM+WMM85YF1/GBGBj4fcfAAAAAAAAAIBJrasvPFpGhftZJCOz/wVkMeAvOSedrz6/aM3NkRMzFauIhIidl1j7UZ9YjuYRXZMeRiiH3qXb+Jg5sfT+WD2+TXRcun2kXSO2yp/qo2spxKZzGNuIXbhtPQ7dTm/nSOVot8+oP9EmdXwW9zQ/N+tB0VFLrM5U/V0x+8bq267Rpxhmngszv+8MXU/XxkY09PxZO+Dzvsd1VNp+d57YtW7L/Ppjc5eag1C/WB99bygj7XTMULxUG58rlUP3q8fNHbtvF4s5hEnH06cu/zy0oudSXO6Ovq6NtXp/cy2g66+uCbd/krttFbvKZRr1tnL4dtV2LZYbQDmHzxMp1ZrNz6k6D13npbqDqDa+adXXbfvzVs1l1c7Ppb82ajlUbL/Wt66VXjOXkRyhzx/WhteQ+vrQfa26ALvupDpW7PNdqS9qn7vHNWIz1qsis/0cvazffh6bs0X/OwkAAAAAAAAAAABgaIcddpg8+clPnnvejf6FR//5n/8pL37xi+Xee++t9h1zzDHy2c9+Vvbcc8+51QEAAAAAAAAAAADw994AAAAAAAAAAAAAAAAAAAAAoMPDHvawxva9994r27dv7xXj1ltvbWzvvffe05aV5YorrpDnP//5ctddd1X7jjrqKDnvvPPkoQ996FxqAAAAAAAAAAAAALzVRReAJiNmofkLM5v8Q3yzlpmgtty8ubFz4hWRUKlzG+oX6xE7T7o+3Swnh27jY6Zimcj+UD2xNn5/ap7HcWwrf7KP2MZ2un1/XXMwlPrYQ3Iu6VSd6Rzdx4eQqqEPXW+f+mNtUzFy6s+to+98m2L6uStMOXUMbR7XzTzMYxzWLnZNEDLJuIcYR+paLG3e3brrfWHLvDr1HOSML7ePvmeUkXY+XlfuWBufY5rYubGmbT8J/5wvJTzunPyxOlP1d+WI1eX5vbEr1J+X+pXuc8xzXv0Cw78jC+tyu/3VW8z4GqVZo6h2IuLfevG+ze0YvRat3zWs6qtjWr1f9fPzXo3bz4MOLO31lT4v1mXTNei7WL1+/VnF2vb1EOob+4xjQ3UnYlXtMj83hebGm/Yzr42+U/Lxzc/wrJRiW1f6xreMYwIAAAAAAAAAAHlKEVlbdBEzsF5++/Hwhz9c9tlnH7njjjuqfd/5znfkSU96UnaMG2+8sbF9+OGHD1ZfzNVXXy3Pfe5z5fbbb6/2HXnkkfL5z39+bl+4BAAAAAAAAADApPj9B7Cc+Ht+AAAAAAAAAAAAAAAAAAAAAJCgv9xo69atvfpv27atM97Q/uu//kuOP/54ue2226p9T3ziE+X888+X/fbbb6a5AQAAAAAAAAAAgBi+8AgAAAAAAAAAAAAAAAAAAAAAEo488sjG9sUXX5zdd/v27XLFFVd0xhvS1q1b5TnPeY7ccsst1b7DDz9cLrjgAnnEIx4xs7wAAAAAAAAAAABAyuqiC1jPbr311sZ/0SSH/i+1GPfPPBRmPnlEhv2mLDNh3ZPUkJsrJ3aRCJU677H+Xb30OdZ1xoanc4Wa6TY+to4ZixXr31VXrK/up2spjA13qMWK1R9vb5PtjcvrY3fVEcyl+hm1PUmOcftwu3GurjnLyzFp/0XpfX4GHMe0c5rbptG+mLz+wpQT923VsYDrYRE552ER47J2+LVE7jimyR27hkubv1qIvYds2V2XHl/OOHyfVFt/Lykj7eq5Y7FiuVKx+8Sap7419Gkfa+ufoTaySus67tcV1ur9zee9Pg/Vc8Ttz7lLV7F8bN+3yuW2bSSH6jfaF65/Fqp1ns/ltsf1um3pbufV15e6T6na+L5W73fbfvzVHFftanNlm+u7VizryzXuuG3VWW8Xunv5vPozjVUnSPfV10/XZ6JUrFhN2iSfTcvMC21en6+XCXMGAABChvj9BwAAAAAAALAresELXiAf+tCHqu0LL7wwu++//du/yc6dO6vto446amZfPPTtb39bnvOc58jNN99c7TvkkEPkggsukAMPPHAmOQFg0fj9BwAAAAAAAABsHHzhUYe//Mu/lNNOO23RZQAAAAAAAAAAAAyG338AAAAAAAAAkznhhBNkjz32kPvuu09ERC6++GK59tpr5YlPfGKy7+mnn97YfvnLXz6LEuU73/mOPOc5z5Hvfve71b6DDjpILrjgAnnMYx4zk5wAsB7w+w8AAAAAAABgemtra7J161a5+uqr5eabb5a77rpLdtttN9lnn33k0EMPlaOPPlq2bNkyaM4dO3bIRRddJN/5znfk+9//vuy1117yqEc9So466ih53OMeN2iub3/72/L1r39dbr75ZrnnnnvkwAMPlIMOOkiOPfZY2bRp02B55jmmjYovPJqxwhgpjFl0GQ3FDGObAcfat85JcufmKDJCGwk3SvXtOqyvnVi9euixnKHdvq2OnYppIvt9nNDp8G1z+6bOT717bBzj2LYZO3FexrXYRBXt8fg+fWLk5+iOlfM2iF8f3bGT52OQcU4fwyvtaKCpOesSG1Nsf/r8ZFxPmfWaov+4ClP27tPIOcA5Xg85EDfN/Fs73RogJ3ffHLFrvrT5qwz9XrNldw2hccTq1m1j7fy9pewYv4+VyqWPx2Kn4vWtL6f/NDH61NvK73P37edSlYFLNzYnfj1SSvd5WG93wmr9ZG1jh58zv790+6u3jZqjapy18fu2fed/XFM4V2gOdZ+q/kjfar/b9sMPXTN+3V66Rn6EOpanrw3rWobWaaXKW+13r7HPQdY216StuIF9qc9UPqbX5zNb6hzP4nNzadfbuwnrjRUr5bq7607PLuGYAAAAAAAAAABAnlL6//51I1hPY9pzzz3lpJNOkr//+7+v9r33ve+Vj3zkI539rrvuOjn77LOr7dXVVXn1q189eH0333yzHH/88XLDDTdU+x796EfLBRdcIAcddNDg+QAAAAAAAAAAmDV+/zF73/nOd+Sss86S888/X/7t3/5NfvSjH0XbrqysyPOe9zx5y1veIi9+8YunynvbbbfJO97xDjnjjDPk9ttvD7Y59thj5Td/8zfl537u56bKdeaZZ8r73vc+ufjii4PH9913Xzn55JPlne98p+y3334T55nnmDa6WX73DQAAAAAAAAAAAAAAAAAAAAAsjVNPPbXxX/g9/fTT5TOf+Uy0/f333y+vf/3r5cEHH6z2/cqv/IoceuihnXmMMY2fCy+8sLP9rbfeKscff7xs3bq12nfggQfKl770JTnkkEMSowIAAAAAAAAAALuiV7/61XLQQQfJ//yf/1M++9nPdn7ZkYjI2tqanHvuufKSl7xE/vt//+/ygx/8YKK855xzjhx55JHywQ9+MPrFQCIiX/3qV+Wkk06SX/qlX5Lt27f3znPPPffIq171KnnlK18Z/bIjEZHbb79dPvjBD8qRRx4p5513Xu88IvMb07JYXXQB69mv/dqvyStf+cpefbZu3SonnnjiYDUs+hupjDGDx5x0TJPU0jdXkUhhJF1DOkasX7yjHkesqc6tm4VqS8WOxdT7fZxQbb5tdOwdfev7C2M74wT7iO2MnToeqrNPHTmMizdknyIjZqxNTt9k7Gpep4+1nsTGk5qznHlItTFFv7kszOTf6zmL87Yer4X1WNNQrB3++Z2SO5/T1BbL0Tdm6P1R2rxVg34v2jKd29edqjPVzt9ryo44qRix4zmxp80xC7G6/XOo7HhaTzrfsf1GxteGVXn9OsOqSzi3BnHHy9p7wF+xvo5Y20Jvu9pKm5HD7wtWF1etw3wo42tVOWV8jnzbMtm3uV3VKuH2oT66vjLSzur9zZRSvx2MYzTPZalOeiumr0ldM7Z2Pel1dzUXqh5dX+zzk7X+WkjLjaljh0z72XaSlU3XZ6xdCfMAAABC1sPvPwAAAAAAAICN6pBDDpG3vvWt8qd/+qfVvpNOOkne9773yf/4H/9DNm/eXO2/5ppr5JRTTpGvfvWr1b6HP/zh8o53vGPQmu6880553vOeJ9dee221b8uWLfLhD39YNm3aJDfccEOveI973OMGrQ8A5oHffwAAAAAAAAD9XXfddcH9j370o+Xwww+XRzziEbJz507Ztm2bfOMb35CyHP9Nr3/5l3+Rn/mZn5Evf/nL8shHPjI754UXXignnnhi4z8WYYyRn/iJn5BDDjlE7rzzTrn88svl//2//1cd/8d//Ef50Y9+JJ/61KekKPL+ttra2pqcfPLJ8rnPfa6xf//995ejjjpKHvawh8n1118vl19+efV3437wgx/Iy172Mjn//PPlmc985rob0zLhC486HHDAAXLAAQcsugwAAAAAAAAAAIDB8PsPAAAAAAAAYDrvec975KqrrpJzzjlHRER27Nghv/7rvy7vete75Cd+4ifkIQ95iGzbtk0uu+yyxn88ZvPmzXL22WfLgQceOGg9X//61+WKK65o7Nu+fbu86EUvmihe13/wBgDWK37/AQAAAAAAAEznqKOOkl/+5V+WF77whXLooYe2jt90003yzne+Uz70oQ9V+6677jp55StfKf/6r/8qJuM/3P69731PXvGKVzS+GOinf/qn5a//+q/lSU96UrXvgQcekL/6q7+S3/qt35IdO3aIiMg///M/y9vf/nZ597vfnTWe3/3d32182dGmTZuC/wGLq6++Wk455RS5+OKLq9wnnniifPOb38z6nc48x7RM+MKjGTMiMu/v0cq5CUxryDFNWu8kNRSZqYzk1xSLmYpQRMbdNS7dJTe3bhfKkYptIvt9rK7+sXpiY9UxCxP+xXk9R6yOWJ9CIjGrOLazxlBM3Udv+3EYtR3Kr9tGc7txxMZtIuPMaWMi8x7b3yV2DteDWdaWM1epNqbIq68wZbrRhDXMK8ZGyLlRTDs31s5uLZGqbZLcsZh9Yun3UGnzVhr+PWrLdC5dZ6w+3y523N+3yo7xpWLkqtc8y+siJWfMIsOMOxbDPy+tejr7dUAZuAxjdfvndxl50vu96+Uu598N/jIv/P+p0S1A/LvH74+Na5rcoua5mnd3WP//LOvrxFafZvniH7XVOFQ7G9lfX3/px/U4lpsj21zb6Zi61tDnEet6xfrE7lp6hdD1+Uv/H1ZTd8I+sXNzxqy376eefOXVNI9xLe7uvf6Vxko5xTp6vSpZJwMAAAAAAAAAsMtacz/LZj2OaWVlRT7xiU/IKaecImeccUa1/9Zbb5Vzzz032OeAAw6Qj370o/KsZz1rXmUCAAAAAAAAALDh8fuP2TLGyItf/GI59dRT5eijj+5s++hHP1r+6q/+Sn7sx35M3vzmN1f7v/KVr8gZZ5whv/ALv5DM9453vEPuuOOOavvYY4+V888/X3bfffdGu912201+4zd+Qx772MfKy1/+8mr/+973PnnDG94gBx10UGeebdu2yfvf//7Gvk9+8pPyspe9rNX2iCOOkC9+8Yty/PHHV1969MMf/lBOO+00+b//9/+umzEtm/X2dwYBAAAAAAAAAAAAAAAAAAAAYF3ba6+95OMf/7h88pOflKc//enRdvvuu6+86U1vkiuvvFJe8IIXzLFCAAAAAAAAAACAbp/85CflX/7lX5JfdlT3a7/2a/JzP/dzjX1///d/n+z3rW99Sz760Y9W25s3b5bTTz+99cVAdSeeeKK89rWvrbYfeOABOe2005K5TjvtNNmxY0e1/brXvS74ZUfeHnvsIaeffrps3ry52vfhD39Ytm3b1plnnmNaNnzhEQAAAAAAAAAAAAAAAAAAAABM4KSTTpKLL75Ytm3bJmeeeaZ84AMfkD/+4z+Wj3zkI3LBBRfI97//ffnLv/xL2X///XvFtdY2fo477rho2+OOO67VfpofAAAAAAAAAACwa3jc4x43Ub83v/nNje0vfelLyT4f+9jHZG1trdp+xSteIYcffniy3+/8zu80tj/xiU/I/fffH21/3333yZlnntkZI+Txj3+8nHjiidX2zp075WMf+1hnn3mNaRmtLrqAXYUxZtEltMzy266GGG/f+ooJUhrJ65QTO9WkiMxJ1zhj06jr0c1i9YZy6Ry+byqmj5Xbv35M12HU/ti4/e5YnGZ9thk7FjMjVhXT2EYd81TlNt3/R4IicjznGo727dm+S6r+9aBPjbE5SMXIyWGKvDoKU2a165t/yH6Ljo3+Jjkf1g5zd+zK3TeHjtWnv35vlbb7SaHfs7ZM5/L1xepKHa/fg8qeMWL7fcxYvPUiNTddYmP0Z1jfVaPtO+bKuHWIFT3vo1f9/wv046lqiJ3z2p/LSJ/qunDba9I0zmVcPwn2q+dIzbNfZ/i5K9wASzfg6u3h29l6X9dWmm3LyFxV44jmbm539nFtq/Wg6qvb2cj+egxfr76e9GeBUg1Mx2629fU3D1rXK7bOK1UtreOBfanPcfr/1NrnM1tsxTKLz8rz+D/f8u3RAAAAAAAAAAAAALSDDz5YDj744EWXAQAAAAAAAAAAMHNHHXVUY/u+++6TO++8U/bee+9on7PPPrux/frXvz4r15Oe9CR52tOeJl/72tdERGT79u3y+c9/Xl760pcG25933nly7733VtvPeMYz5IlPfGJWrte//vXyiU98oto+66yz5O1vf3u0/bzGtIz4O3oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgKTV1dXWvgcffDDa/pZbbpFvfOMbjf4//dM/nZ3vuOOOa2yfc8450bbnnntuZ98uz3rWsxpju/zyy+UHP/hBsO08x7SM2lcQBmVExBgzcf9FfyPVNLXXDTGOYoJSjPTrlMqRE61IzFlsLrq66bpiTXU7nSuUIxU7FlPH6po7fyxWT2xOCmMbNXXlGMeywfqqmInjxuX0NfkawvWF+8RiGL2t2qXyNXJLdzvTcTyVw0SO992/KNaOZnrIuoaKlRPHFN1tClPOJO+Q/WYda1qpOV4mthzmOZ4jdY79e3MWOXJjh/rn9tXvvdJ2r27q11nqPPi6YrWkjo/qs66u+Z3z9SQ1/vq5n/Za9M9Yq1ZN9XVKqS61WH3+eV5GVnd67TDJ+a3WG65vn6eIv8pLHcOvu2x3/Vn1+VymGcu/hca5mh30HFfxdPtAH722rParvmWknaj9dXp+9SNnHNPNpW2uE716N51nPI7wvFvXO7Z29v1zPqPp8eR+PrS2fYKm/UzY59od6nPssmA24kr3z7JZxjEBAAAAAAAAAIA8pYisLbqIGeC3HwAAAAAAAAAA7Lr4/cf6tHXr1sb26uqq7LffftH2V155ZWP7qU99qmzZsiU737HHHtvYvuqqq7JzPeMZz8jOs2XLFnnKU54il19+eSPXIx7xiGSeWY5pGS36+3QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABvAmWee2dg++uijpSjiX2Fz9dVXN7YPO+ywXvkOPfTQznh111xzzVxyzXNMy2h10QUsu0Lm961Sxpg5ZRp2TMWEZRvp3zE3V06zIjHfqTmKde+qUR+KtdW5da5Qv1RsHzMVy0T2h+qJ1VkYG6ypq5ZCbLitSRyvYoWPN+pTMWdpnKu7rtTbvvt6So95VlLjylFa0yuWde31ue4bp0vsOsq6voruNoXJ+57OnFyz6DtkjOxciTnD5HNky+FvdKlrw79Hh4jdJ9akff17srTpVZE/D6l59bXEakgdH9VlXV3NNrG+OTEXSdenxzdE/dPOTWzORcbPWis6trjY4Vr8VRWKWeX1bVSftcy5GOcyLpeLW3tPdOUfxXB9/EDcDv/E8vtLt7+6JdXClpm3qXGuZoxx3c3txjox0kev7XRf36+MtavVZ2Nt1LmuzlvVvjnHpY2vg3WOcZ/mtv6cZNWar2t9qGOl7nCx1ckkn02tfkMofFvz5Jg7AAAAAAAAAAAAAAAAAAAAAAAAAEO555575MMf/nBj38tf/vLOPlu3bm1sP/axj+2V86CDDmps//CHP5Q77rhD9tlnn8b+22+/XW6//fapcun23/rWt4Lt5jWmZbXuvvDoe9/7nuzcubO1/5Zbbmls79y5U2644YZgjL322kv222+/WZQHAAAAAAAAAADQG7//AAAAAAAAAAAAy4bffwAAAAAAAAC7nt/7vd9r/DvAvffeW0455ZTOPnfeeWdj+4ADDuiVc6+99pLdd99d7r///mrfXXfd1fpyIJ1nzz33lC1btvTKpWu76667gu3mNaZlte6+8OiZz3ym3Hjjjcl2N910kxx88MHBY6997Wvl9NNPH7iyyRhjxBiz6DIailnGHmCoRiYLMknu3C5FxjmMzWuqa1fd+lCsrc4dyxnqH8uRiqljmcj+eiyTiF0Y2+4cqKlrTsexEser7WZOv9/X0nX6fN9YrCGNc8XmqDt31/HovPduP7vxrzep+e7DFKlzV+bFmaCmaa/ZWVzzqfnAfPQ5D7YcZp3TdT1Z2y+HjtWnf9+++j1a2vhKy89ras58DbHcqeOzsqi80/DPpjLwRPf38lKNx59BfffV7Y2LbYOxXQyr98dyxusc5Wr2D8WItnXtSr+W8NvB3uF6p4khUhufW4g1bjFqrqo5kmbbWC4917G5H9URzqnXsdV+v27UsQI5fAy/KxarqkXF9OMLfe4obXhdqnO1+/na0u9Z66KlPlO1rulk5ECMyP5Zfm62dtdeX6y3fyexnlgppcy+m20cdgnHBADLYtl+/wEAAAAAAID1p5T83+VuJMs4JgBYFvz+AwAAAAAAALO2q/z+Y+vWrb1j7L///r2/ZGdaZ599tvz5n/95Y98f/dEfyb777tvZ75577mls77HHHr1z77HHHo0vB7r77rtnlqculGfIXKkxLat194VHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALArOPHEE3v3ecc73iGnnnrq4LXEfOMb35DXvOY1jX3Pf/7z5U1velOyr/5yoN133713/j322EPuuOOOaMwh83TFHDpXakzLqlh0AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA9ec73/mOvPjFL258Ic9BBx0k//AP/yDGmN7xlq3PvHMtg9VFF6DdcMMNiy5hoRb9DVTFjN4LRqYPPEltuV2KzJtAzvmJhUrV33VY943VoXPrfqEcqdixmDpWV5zYnPj9hbHBOn3M2HgLGfeLzrtrY5KxwrU02vixd7QJxTJ6272Gcuq2MdE5dePtut6MhGPHxtV3f+pYTrvWNZERz1oTbFu6/ak51XFy8y7aJDX27TOLeTDF+pnb3GtjvSvtYha0qXNpy+nr0teg7TnWafr7vrl9ClOKiEhp46sGP2epuembu1mHdXU0++bGjPWfF/+sstK8t08yF1PX0nPORALzrsYzji0udjinv4pmcR6q2FUu43KN28Te3VV9ri5/KRd+IG5gpY/T4zar10Kl2+NvNeNc0sjh1z6+/vpaSNdRvTtVH70GLZvDifavK9VY9ZpMx6zaqX5l/ZhqXNrmGlPzoWLrQV3jKFb3NWYz1pi5uYb6zFumm1R25X/pAwAA1pdd/fcfAAAAAAAAAABg+fD7DwAAAAAAAGD53XrrrfK85z1PbrrppmrfIx/5SPnCF74g+++/f1aMvfbaq7F933339a5D99Ex55ln3rmW0br7wiMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2BV86lOfksMOO6xXn9wvG5rG7bffLs997nPluuuuq/btt99+cv7558vhhx+eHYcvPJou1zLiC49mzIhIMaPYhZlR4A5Ghks6af2TdCtMv1455ywVMja+VCWhfrF6YjXoGLpZTg4d2/dJxfJxQrXpY4WxwTqjc+f7iY3nyGgz2m8bNbWOJ2qp9zVqHENKxdZz2Od47Nis7lnrjbWjk7uI82eKdM7ClBPFnrbtEP0aMTLGOpTU+2HZTTP+0s5uURG7Bmw5eU59bdqe9Yeu7VSMvjnr7+HShu+sfm4mnYt6TX3nIBZr2jjz4q/32LXbNZ7Ysdw50LmNW3PYwCrPryNKq/eH6/frlzKyYqzvrd7zLkap1jY+dqxdbJz1tY+vuxUj2LO2VrO2scO39/vL2gKtukVE5qod27XT27p97c/VeXDbvrxCNfa5Y+vAaP/aMX3OfQirjlf9mlM1rrnrlh5Z4JausNgVHKshHEulTHyKsRIueJrPrLFroYo9eehdzsa4sy+GFSs2elfbuGLvSQAAAAAAAAAAsPxKEVlbdBEzsHy/0QEAAAAAAAAAALl2ld9/HHbYYfLkJz95IbXE3HXXXfL85z9fvvnNb1b79tlnH/nCF77Qu9aHPexhje3bbrutV/977rmn9eVAe++9dzLPvffeK9u3b5ctW7Zk57r11luTeUK5ZjWmZcXfEQQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAyN133y0veMEL5D//8z+rfQ996EPl3HPPlR//8R/vHe/www9vbN944429+uv2++67r+yzzz6tdg9/+MNb+7/zne9MlUvXHts/qzEtq9VFF7DsCjP6WSQjsy9giDFOGqIw/XvmftNXTujU2GOHY/26aovVo2PpZl016nw+R9+YPk6oxq5j9dipmIXYcIBa31ibcazIcbe/63T6vrFz5PfrWNW26p+Tcxzbx4jU1jE3Qylic9d5XmZfV4y1o8nS5zy2f5JYs1CY7v8eWW4Nk9Q6xPhMMbs5WuT1tOxy57a0w60pYteKLfvniL3PJ4mR27dPe/++Lm34CeLnIjb2nFyxNv7cDnnuJjVJDf75Zme4no3NkT9b/q7c75yHY8bG45/vVr0t9Nph0Pegj+1ilhnjq+4Vqk8VI5nTxakNtHQ7q1uCa1PaZs7o+alihvvXdlXrRV+nL6NQDX1f38+H0v0b9ehbmprGUh3XMXW70JqvVa/vU8UKn7vS5q85Y3W1Yza3p/nMaWPr9wXctvS41qu+c7PofycBAAAAAAAAAAAAAAAAAAAAAAAAYOPYvn27vOhFL5J///d/r/bttddecs4558gxxxwzUcwnPelJje2tW7f26r9t27bG9hFHHNGZ66tf/Wojl87fJ1es7zzHtIxyv/cFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALCE7rvvPnnJS14iX/nKV6p9e+65p3z2s5+VY489duK4Rx55ZGP7iiuukHvvvTe7/0UXXdQZr+vYxRdfnJ1n+/btcsUVV2TlmueYltHqogvY1Rgxiy6hUsywlCFCF2byKLnf5JWbImeuUk1iMbpqjdWnY8Vy63ahXDpHKrY/rmO14nTmsI3YsTpj4/f7u86LidSpc/haWseNjxM+Xj825DfHVXPTkbferr0/3N5IehzT7t/oSjuavNjcTtp23iY5P5OeU1MMN/71OJddjCnnlsva9fP9lH3eH5MKXVe27BdTX9O2R02+b26fPu0Ld92UkXPqxx4bb9/aFmWa+vw1FruOUnMQ61/UnoNlYtWWO886V/1Za1UO/3wu1eWdqrer1vE6ygV1MUq1Pmk9s3y7WNza+6dwWXzdsTtAtSaqanD9rfUNGnG6VO8ONWfVXLkDVsWq1n5+f23qfAy99vRz4GPp3Lq/Dx06nz6/jlWqPjqmp2OH2uq1catu3a+KGb6OSj2Jkv5M01VnSuz8z+Izsu1Y+3aZ5WdkrE+llFLOcV03L2X0Lg8AAAAAAAAAAJbdmvtZNss4JgAAAAAAAAAAkIfff8zP/fffLy996UvlwgsvrPbtvvvu8pnPfEZ+5md+ZqrYBx54oDz1qU+tvkxo586d8pWvfEWe//znZ/Wv1yQi8sIXvjDa9gUveIF86EMfivbt8m//9m+yc+fOavuoo46SRzziEcG28xzTMlo/f4MeAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADA3Dz74oLziFa+Q888/v9q32267yac+9Sk5/vjjB8nx8pe/vLH9kY98JKvftddeK1/72teq7S1btnR+qdAJJ5wge+yxR7V98cUXy7XXXpuV6/TTT29s65q1eY1pGfGFRwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwi9m5c6f8/M//vJxzzjnVvk2bNsmZZ54pJ5xwwmB5fvEXf1FWVlaq7bPOOku+9a1vJfu9973vbWz//M//vOy+++7R9nvuuaecdNJJnTFCrrvuOjn77LOr7dXVVXn1q1/d2WdeY1pGfOHRjBn1T1+Fmd1Pd93T/bTHYXr/tGL0+KnGYbp/cuc7Z15yY/SpNRZT1xPNnZFL99WxYzF1LD2+cA4rhbHx60T1He+3Uoit9oeu4VabVp1WjLHj+XC1xMYbMq5vFKt1XI2v2k7k7qLHpRmxYiReSzBm5Jies5xYjVpqY40di/aNjKOrX595FBEprZHStifRWiM2sH+9Sc5h4vgQfUxhGz/T8Oev73mchjHlYD/ztNHq1ud2iHM87bVXvz/lXvN9+0zyHozGGuA9lswxYL25Ws/eAe/xk8idg9iaIkdsHPHnXuR579cSA9439bomtg4QGa+P9Bqnq8+obr8uG61nqjimfSw29iqWyh1f8zX3F7X627Ha9YRi6vFH18kdsWIxdU2hdXL0s0AgX2hdHq+h/2eyrs9Dsfpj45jF59hxnfyj/wEAAAAAAAAAAAAAAAAAAAAAAAAAbW1tTX7xF39RPv3pT1f7VldX5YwzzpCXvOQlg+Y6/PDD5bWvfW21/eCDD8rrXvc6uf/++6N9Pv3pT8vpp59ebW/evFne8Y53JHOdeuqpsmnTpmr79NNPl8985jPR9vfff7+8/vWvlwcffLDa9yu/8ity6KGHduaZ55iWDV94BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC7kF/+5V+WT3ziE4197373u+Woo46SG264oddP15f8eKeddprss88+1fZXv/pVee5znyvXXntto90DDzwgf/ZnfyavfOUrG/v/1//6X3LQQQcl8xxyyCHy1re+tbHvpJNOkj//8z9vfKmRiMg111wjxx9/vHz1q1+t9j384Q/P/hKieY1p2awuuoBlZ4xIYWacY7bhGwozXLZpv21rklJyz0VOs1Ss1Phi9XfF1YdibXVunSvULxU7FjOVaxTLdubwMVp1io3GrB/vamNc7tj5iE13ql/9mB9fXz53vb9JxIrlil0LRuLxYrlSNVQ5O2LPSs5cWzuaDD8OvR1rlxNrWqaIxylMOUiOrDp6jqer7pRJ3x99mDnO3UaXO1fWDv+dmPpaKO3kawp9TdqyX6z6e8Bm1qHvKdO08+/3csJ57sqRW+d6Msn5iMXQ/fvMh79Gy8hzJLefyPj5a9VKwz+vS3VrDMUQGT9ry47VaWs94WKUai3jY8dyjXOOlYk5iOb025Ga6+sW36awri63qKve5qk5k2b7UM5qTCrWeO7Ufr9GVTFb7Wo5bKyN23bDa68tI6dWj7cutu7TObVoDaFYVa6892RpmwUPcQeKTcGsP18vowE/xi+d0v2zbJZxTAAAAAAAAAAAII+V+O9qN7L5/z/VAAAAAAAAAADAesHvP2br7/7u71r73va2t8nb3va23rG+9KUvyXHHHdfZ5jGPeYycddZZcsIJJ1RfPHTRRRfJEUccIT/5kz8phxxyiNx1111y2WWXyW233dbo+5KXvETe9a53Zdfznve8R6666io555xzRERkx44d8uu//uvyrne9S37iJ35CHvKQh8i2bdvksssuE1v7O3KbN2+Ws88+Ww488MCsPPMc0zLhC48AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADN13HHHydlnny2ve93rqi8AstbKpZdeKpdeemmwz6te9Sr567/+a1lZWcnOs7KyIp/4xCfklFNOkTPOOKPaf+utt8q5554b7HPAAQfIRz/6UXnWs57VY0TzG9MyKRZdwLIzc/hJKYwZ7CeaY4Kf1lyZfj/j8eX/5J6XnNipsafGFcvRmBP1k1tDKlcoR6xNbDyx/eM4tvqJzquqd5zTSiE2PlfquAnUa4wVY2yrzlRNvl9ILGZ1XPx5cjk6YqX4McYYsWICx33u2P7gMcl7GMTGE9qfqmOWUvNeWiOljd/P/PGuNvOUGk+f6yy3rSls42cSszjXxpTBHwwvNtdDznv9vtR1j8qqd4prtu+9Ovt9lPEcKEwpRcdcTvMebOea/f13lmLPvUmknrHxfnrtk3/txOZfj2u8hmiub3ROM8V7Rq9X9Nop2KfK6dZLEv4cEluHjdeP6bnX67xqjRvJrecstB7W9cU+G7T26xpU//pPat0eXZ+rn/G48j/LdPUJjSfrM1bmzzj38J8x9Tjn+bPeLeOYAAAAAAAAAAAAAAAAAAAAAAAAAOw6XvSiF8mVV14pb3zjG2WfffaJtnv6058uZ555pnzsYx+TLVu29M6z1157ycc//nH55Cc/KU9/+tOj7fbdd19505veJFdeeaW84AUv6J1HZH5jWhariy4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADA/1tqF5T7ggAPkgx/8oLz//e+Xiy66SG688Ua55ZZbZMuWLfLoRz9ajjrqKDn44IMHyXXSSSfJSSedJN/+9rflsssuk5tvvlm2b98uj3zkI+Wggw6Sn/7pn5bNmzdPnWeeY9ro+MKjOSmMWXQJLcUMYw8x3GKKGLldc3PkzFVqzLFcXd1ifWL16Bp0/1A43UbHbsWMHC9M80HWlStWfyE2nNM0j4f7+nrCbXx9ui4fW/dLxeuKGatNt6+2O3LE5ndcQ6Rfx1x15WvGjuTsiL2RWTuazK75yWmzHuXWa4rJxxW7XvoyphwkztDW0zn31+Gixc6VtdOtMELXUtlzzP5atmV+P3+Oc+e3b/suhZvLcsq5W5S+58ef41C/vvMai9WVI5VT982tqX7tVn3dM9OKrs+3k8521TrA/QuDsmPVodcX4moo3ba/umJzUh2vjaPwMXRbVX9qvsexfdza+9w0c+h/NzKeA7+jmVvfMVrta3l97Oqd1joPfjzNmLq2Qu2v99GxrDquY3qt2gJtWzFULi2WO6SKlWirz88kd6329TT751rZ81+6rY8nLeajFCtriy5iBtbnmhoAAAAAAAAAAMzemvtZNss4JgAAAAAAAAAAkIfffyy/zZs3y7Of/ey55Dr44IPn8oVD8xzTRrUx/1Y1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADYUPjCIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMHOriy5g2RXGSGFMfvsZ1tJHj5KzFAPEmyREbt7cec+Zl1TO2OGufrH6dD2xGHp3qJ3O0YodOV4YG8zVlaMd24b3m+bxVu7GvmYbf0zXp2PHauw6Fo/px2GTseI5wrE9EzmeqimcS8WOtI3tz8mV27crRtf+PjmsNcH2sf05bUq3v6u+WUmNu8/cm2Ky+ocYtzHl1DHyc83/PM3CNOPw1/Is6XNq7fSrG32tlZnj0Ne2LdP9YveIVPuudjltOnO4cej667Xq2NPmDJm4/kgt/rzmns9phObKP3PLnqtMf0X7Kz00vtjY9H6/HinV2zpnbvR6I3ce2zXEc1U53LEyci7H6z83kFT72mZhbWNnqfbr81PdUdTc6XE0c7i2eq3p513t9zH94eo8NZtJ8PEZieXpmFU7FcbW+kXvopFT7nOmroh6aanPMGXzNGWzgTka+jNvzkqiz2fyZbSrjx8AAAAAAAAAAAAAAAAAAAAAAAAAsL6sl+/XAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAS2x10QUsu0KG/1YpYwYOmKGYQc5JQ05SS99zkDPHqTpSIWL9u2rVdcVi6N26XShHK3Yytw3mDNXkY7Vz2Ejs5nFdQ9cc+WO59RnXrj3e8P5wTNvRanw8t/0of3fb+LkPtzcZOcex89r2iTlk36FilHY0iV3jzWlTZ137WG22dMeL6ce/CLnz0MWYcoBKYrE35rzOU2qO/DU8bM7wObd28hWSvxbLnvX6955/L2b1cbmGmJtUrMLNVTnF3KxnQ87lLHLovvo6ax13/XLuav75bCUcO9ZuXNvotbDj93AZWW221hsuR5m8/prtRzlUG7+/Wie52OrWEstd+O1Qfr+GMc02fsyl2++noHqXuHJ9DaG5Hc9fc1yx/YWKKZH99TWsrkvH8nRMG2snca05SOTUqvq7m41yZcbUsb0hPrta2318Oe+Yw2KO4kr3z7JZxjEBAAAAAAAAAIA8peT9HnujWcYxAQAAAAAAAACAPPz+A1hO/L03AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwc6uLLmDpGRFj5puymGO+IVNNWvck39qVe05yako1icWI1d1VWyyW3t0np86n27SOG5uVsx6nncNGYjeP6/7t2sbt/LHCqL6R+nzfWMzQXMXGntO3u71NtBQxkTZ6vKn9o3zhOmL1jful64z27ahnvbF2fIZjc+PbxI73bTekPrlMkdd2mvNnzPDfLzrP+dzV5F7Tw+RqXhvW9n+i62uzzKyvfu3bMrOPyxWbg9TxSds2+rm6QzXnxvRzljtXk+g7V/XzqOuK1TvNeFv53fOtnHJlW3//6LH5/P55bkWPU1y7dP3VMV+3Wielzq1er/j2oXd/1SYy3+NYplG/jqXjFLU4vn7bGrs77retH6/pbm/9dvu6qtaUthk7tl/HjK0r632qemz4uO6rz7mNtGvUEVvrJx6PsRqCbdWYU7rq7kvPybw/Ty8l5hAAAAAAAAAAAAAAAAAAAAAAAAAAsI5M8l0xAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvawuuoBlV4hIYRZdxcg8yhhirJN+C5eZIHduvTnNUrFi4+qqOxYz1kW31zlDuVJtCmM7c8dyhnPZSI7mcV1Lu0Yb3B+qU9cX62vUOMe1hffXj6X6GrXdpRpzNGakn4Tbx2oLx85rq2Pm9gvGitQ9jT5jrrPWJPvntNkITJFX/1Tn1pQT923GWZ9znTuHQ7Ll4hcUOe+PyWO3rxlr+60K/DVb9qjFn8uh5tfP0TTzUbi5KHuOf5J6hqk3PO9DxJ62hknounXs1nHXr+yI0bdu/3y0oud0/GebuA211h8uh36n6Rrq9/5Sj9Gvo1Ss8frK7bfh2KLaN2L5NZAbWBkZe3Xc7fC3Y1+LXyuVtflpn0MJ9tX7x8mbmz52vZnO21rvqpi6Xh07JLYOjOX0/LWSc0eJ1RVt73PnNQ/SQ17E5+eued9IYnPHN13Hle6fZbOMY/r/s3fnUZsU9aHHf93zgsAgy7CDwsAgLoABMkZAiBgUVO4RuMEQvV4VRc2iyZUkR1xyAWOMnESumpiEG2IweBUDATRGwBWMEQmbyKLCMAwoqzLsjCxv1f3j7ern6V9XdVU/+9vv93POHHy6q36/X1XX093PtDQAAAAAAAAAACCNEZH5aRcxBjz9AAAAAAAAAABg6eL5B9BN/HtvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABg7HjhEQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGLu5aRew1GTTLsAjH2NRo3ijVjZAfW3HlNo8JW5ozLFxNMUO7dJ9UnP72tXaZLaxBpdbx/KNMxcbyFHdr2PUY1vv9v56Q3OVqfGEtrvYevz9sX37Yn19OUPzUs0ZyBWqIVKbr47Ydn182qito8jcxNo19U+NrRm7cAB8c2eLfab4HJrfphiDGmQOZkGWmXijxv6TH1+Wz/acOsPUac3470D0sXPfn+FimiJWuzsK9100LWpw8xubKzfO2Pj65yPUNhYrL8Zv1PhTa50VqXM2zpz952e3LmptiuudkcB+ta7ajKvWt8hl1Z2Luzcy6uvetKbLutX9U2j9l/sD9fd/CuUt72XUdTI0jnL+A+2rdRWfrXXJKrEytd8UG9wpsnfN7sWu1eG2S7Vvub3o60ooawuMrzIONe267rKdzhnoX4mhtrsQod8Rodw+eqwhoWPdhp63cZ4ZUq+c4/xNDAAAAAAAAAAAAAAAAAAAAAAAAAAAqkbxPhoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIBGc9MuoOuy4k9b+SCdRmjUb8LKRjCeQeakbZdYjjbzEhpzKEdT6lCfUD06t27nqy3PbGM9uoZYzFxseF9Wb9Pfrh7berf31xyqN1PjcjFC25vmobYvUJfu6/rpnD46R297KIe/fUquWM5YzNB8TNqo81rbm+xYbNdWtwttH6eUXFmeVk/qmqjmN637LPQb/xyljrvLYnNgzehvPELHtv87lh6rur6sTbsi67VsEnK7uYrNiRtfynhibdvEGjZXqpS50tx8h/qOYs7cdc8OdIc9Xv2r0q1YPY7QHOnt7rpvbHW8/WvaFNts4Otdj+nPre9nfMevzFvs0+PL3fbgcfPHaezj7gvdAIsNRo3X3U+aYr873fWfNXrzWaRX9ei5LOdC38PqufYsw1p9mX9/7f5Y9fPVH8vl9Mbn3+/rH/u9pucmle/uYNS/dUPzIDLYb/EuYh7CrBixMj/tMkbOer99AAAAAAAAAABgKTAiHXz64X/2CAAAAAAAAAAAlgaefwDdNOr32gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANTMTbuArssykTwbbcxpvKUqG/EYRAafl2FKSc2ZMsexOYnlCu1u6heqS9eS2m4hn22sS9ejY7uYuVjvdl+sUNt6bOvd7mr2TZXLkalxuRih7bF58NVVzx2uq5qzaNfQMJPmHMnbfbEDbetz42+XojaftflVuQI1zRpjFw5aaD0tdm2PQ5a1f3foOOcsy7t1PCYpNnfWjO5GQK8Ba9vH1mvP2rS7I7fGTUJONyexsbvxpIyjTdt+eTFeo8bZf9wGPUaDzH9M6jwPOh9NMXRuX45YG3f9MxLYn5Kj+KdbqaGxuuuhFX/sUDufsu6iSWhey3N9sV+fyfuvBa4Oq/eF+rp+5T1c0U6dYirXGxfL9VGxy/tEaysb3H63uTf+Yi77Uri27j7R1aPHZdT8uhi6ll7OeI7YvbWpDquW20fnqu2PXA57425u1x+r7e9B3xwNK3TXMerf2100jt/zAAAAAAAAAAAAAAAAAAAAAAAAAAAMahrvzgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEvM3LQL6LpcJvdWqSybUCIRyUeYa9hQg9SSekxS5jSWPxaiqX+oTl1XcrvM1ttE6tGx67mtd79vXMG2tRzWu93V75syly9TY3QxQtv1nNTno7c/NUat3sB+X476Pv/2TPx9Qrl825vyNvWtzVlinEGk1jhsnxhrFw5EbKymaKdrSO0/Llk+2rxZZgboM7oaRj0exDXNuTXDXcn12nDfl3YxTNE37Qrf/x01kXx67KHxunGk1B9qG4uRF+M0ieMct9ixc/Mcm+MUo4w1Krqm/vmIrYNa3+K6bkXPoRTtGmIUfaxqo++nXC4dqtzvWX+6Tqu2S7Fd99Ux86JGN47+GkKxcve5HE/R3g00q+53m939pum7aa2dwgLzWj+m/v6+00AoR9nHejfX7vVMdXgVvTH6ubkI3T+GavDpHeuExr4cI/iq6jU9G2e+xYm5C7NixEj7e9tZZzs4JgAAAAAAAAAAkMaIdPJJQRfHBAAAAAAAAAAA0vD8A+gm/r03AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwdrzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjN3ctAvouixb+DNN+QTyjzLFoPUO8vau1GOTUlOsSShGU92h+kJ9dPs8s/52vpi6byy22EBOT+xQ21oO693uxqFD9+fK1FjzyHY9N/XYrub6HAbnPxC7188/D9UYxRwEjqXbr7en1ujNWZsjf8wYXy06dqx+33xPk7ULByJUl9vvzFr9qULrSMsy0zr2sHOS5dOZ08V6LB29NsdJHyNrhsvdP/dtx6HXqLXxs6Fe/yaS0403NE5Xf0rtobZtYoTqGiRGNEck5jhyBmspridW/ON1x9V4zuOxNuX+IocJ5EjhVqBbmbE6y36B7f3XUSt63RR9bbXuUMzaZ1drwlzpM6TuG1rTeVGT6QsQi5UXsfQVqLxfyar7i+FX7mdM0aa3zxUmlXpC93JuLnX//ppq97l6YGq/zln2a1heoatwbVyBfim/aUJ1hdSO3xBff6OCTft3dD+rBzplbedmluYSAAAAAAAAAAAAAAAAAAAAAAAAAIBB3hEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQyty0C+i6PFv4M4vGWdYoxjzM27iylvlj9bYJF4oVG09TzaG+uk+eWX87Hc+TS+eoxRbbvL+hfT2W226923sxF/aH6tf9U2LrOarHttHYsfrK2MV+PXf1/v79vnpD2331hraHYsb6puaclFh942RslpTTFu10rdb0Vk2WD1f3MMchfS2Y1rEHrWvY+UjKMeW1OwmpY3RrdKS5A8ewf90nx9LfnZb1urVrbfpdhftemEguN87QuHzHIFS/a6v3h7bnxbhMi3G11TQPobpC+1PntG3bUdE5a/UX12kjgf2emusxpIjh31/2K3JZ0deZrMhRb+vui/SclXUXm+u5/OOvtAnW6e9bjtPdAxXb9bjzvrsnN6bkWOV4quN0N5y+K1Zu3Vxkqq8akKrJTXdZm2roO931xqrrVAI5fSu/V4dnp/jH3C/l0upipP6Oa6o3lS5rkr+bTcKc9Gv7+xKLhxUrNvotWnxsw29MAAAAAAAAAADQbfPFn67p4pgAAAAAAAAAAEAann8A3TS+f0saAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACgMDftApa6bNoFKPmICxrlG7WyAWprO55Y85R4sTHHxtHUX/fNM+tvp2M25NT5ajnENu/P0tr158pU3boGN67QOHT/lNh6ruqxbVJtzfX5Y5Q5s2o7n9CxyiQUM15vKGe9b7iuJimxdf1Nc+ATmtNR9LE2/AXRMVzbWGxTtGs7zlHK8uFzZ5kZoM9geUdRby3mFOd/sYjNUdP3o3UudYytaR879J2M96uuZWvjdyju+2siOdy4Usbj6g/VHdof6zctg9bVpp9uq4+Lu75YyZLaD1N3am2V+hJzxcbVa9f730Z9fcsYRR+r9rta3OrXa7usv2+7++aE+maqb3n9c+1cv2K776rixuTGkxyr6OfGWd7HFDdc/fNT3oMVjY26UXWnJ1efrqlXa3WD8fx60JczfWro1as0LBFdRxmrrKu5X+j3R/8aSf3dpudoEGVdg4dIFrrKjfp376xbauMFAAAAAAAAAAAAAAAAAAAAAAAAAMy2Ub6PBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwIsXHgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgLGbm3YBS0E2RN98mM4jMI43YmVDjmmQOUnt0iZ2bG5i4wz1b+qXZ9bfp9YuPafOl4tt3p/Y3p+r2ta10eMKjSfUf7jYNhpbt03eLi62d7dk4u83WC4VO9DO31cfQzUXeg5bxE4VyzlOoxiPtdWDHIrp2vn2G7twFPPMDF1PTGx+swFqaDuPWT66YzyONYkFTXOr133r2GoNWNM+nq4vtab+NW5t8xVdf19MIEeb8bi6Q/WG9uvt/ecLdw5xdQwynz794w+OPTKeWVWfz4XPRp2ry/3F9dIMcXfvVps7crEayn6B7SK9+wkrer0Ufa2/bh3Tqu0LGzNvveU43FxF6iz7uXh9OXpjdzHbxnJxin5uJH03YEZt0nPS2y5VqiYX0TdXvXVTDaFj6q9mr27Vrj9GYMkZXW+tPv9+X46YUJ0xvhyj/o0bmgeR4X6LY2kwMi9G5qddxsh1cUwAAAAAAAAAACCNEenkk4Lx/z+qAAAAAAAAAADArOL5B9BN43ifDQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQMXctAvoujxb+DOW2OMJ2ygb4ViGnZdBuqfmbDO3sTkJxUqZyzyz/r61dmm5m3LmUs2l2+ocrn2tnSd2psbh2ujxZeX2tP56e0psUftDsXU7f516zmwRw3/cypxqf/9463UH1kBgbYRyDtI31i80/sq22lgHyz1Krk5rhz+hmSLGJMaVetyyfHZqqfQZYV2DruHFlnMYo1jfMaE5GTR30xqxJi2mrimlliyrvgPX2ua7AP19N4EcvvHoccTqHeX5KhRLb2/KGRt7m1i+filtgzGK640Vf+7+2kPHzHFtTWAc7jpvArnSYhS1uPpDx0eNq3oMsiJHta27r9DjLOsuNuu5ce37M5T5in263nIcbpyp/fpqM7Vj5WK2i6VPE7ntG0mm5irztBERU+xwm8uzgYptqt2qdZZt3LEXf8wyZz3WQm3+7SK9OQj9BvDV18/tTvltpOcsVWi8wwi9sXxcv7e7hDkCAAAAAAAAAAAAAAAAAAAAAAAAAMySabwzBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALDFz0y6g63KZ/FulsmxyufIR5ho01CA1pB6TNnMZixmKlWc23CeUKxQrNafUc+q2OofuU2tfi2eD+/SYs3K7P0ZTbJ0jFFvn1jFiNVbr1HPhP4ZujkLHuGntZmq+gzFqOUO1+OaseRxNa3NUYjn1fu84ko9HWjtrwwcm1CfEFLHccWrbv8kgsWLHNMvMyHNm+WBjHuVcTTPHtLQdW9O6H1XuYXLodWRNWixdS0oN+ntgbfMVXn+vTNM5pBhHqH5Xr65Tbw+1G6WUHG7sTWMelI6t60nN7RtHLHZqLHcdNVKN01RnPYYUMapq/Ypc1nN36O4njNXbq/VZtT+43vr+t+tSrvOiratX36v16k3rJyKSu32ReqI11Lb3YuRu8OoG1rV1u8tjqtrVLmWepRKa/14u//rSsX31O706/Vzf0D2mqzHlG9s79gmNfTlGcFrQa5a3NQ+OuQuzYsTWzsKLXxfHBAAAAAAAAAAA0hipP4Pugi6OCQAAAAAAAAAApOH5B9BN/HtvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABg7OamXUDXZdnCn2nKJ5B/lCkGrXeQt3elHpuU2LFYeWab+zf2DWxPrCEXm9TOlyvWNy+3W+/2amwVK5DTxaqPL5wjFFvvj9XZFKe2rxbL5RCvTPxroGltxHIGa4mstzZ92+4XqY81dRzj0DbXKGqzNhtZrEnIsuZ3gA4yjiyf/LxPImZXxebKrelx5Bgktl5f1qTF8NUQyx/6fljrvzvwnX+NyhGr39U5inlPFcrZP2fD1uPmRs/HKGO6648d4O5Uz0Fqve76b/pyxmLF5julBr3P3U8Z658Dd3+S22q9+r6rP5f7X24VlOu7aOO+HU0xmvpV2ri+Zayira3WomOJ7ufJERp7OWe6ltocFSmtqrW/Tzn/1Xp7+6vf+95a0LklKHSq03UFJXwtQvXH9NZIy45NNUz5d3SMbThW4zLonMz6XAIAAAAAAAAAAAAAAAAAAAAAAAAAlpZB3hEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQCi88AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYzc37QK6LhORPJt2FX7jLGsUYx7mbVxZy/ypuVLi5pltjhHs1xCzZT252KR2vryxvrqWTI3XV6ubE12Gzu1ihXLo7b65zgL7YnXW2jflqMXyH/PeuFX7rCF2IFbKPPti6n4LfZvX6DSk1D3unI6x6SeQ2Pc9hTUL+bJ8uFhN/UN1ZplpjtlifG3rH+UxnsR6GdYgx9etjVmQMse2xXcnNXZqzND8psxhKH8st/7+WBu+m9DfQX2ecfXrel1trhb9uZrDFLHT7mpcTbVaGnKE9oVixfji6W06dlN9ldjFtc5KtV9T7FHX0IY7am5VhXL03ye4sQXHUbR19x29+ouctuin5qh/BZV9ypxSySnFfl13OZ5if6ifiIjR862250V2Y924qrFMqF9/jrK+6thLmcoRaGfUjbH31KOWha677Bs4L/ju210ZoW937FQX61/RclmHxteGnsZp/H42vmMZ0fY35zQtolInzooRY+enXcbIWWm+zwcAAAAAAAAAAN1lRKR7Tz+Epx8AAAAAAAAAACxhPP8AummYd8oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkmZt2AUtVNu0CAvIRFzbKN2plA9TWNn+bHHlmm2NF+we2N8UM9vHXotv7cuq+4Rw6tvVu982LDunqyFTbUeTQ+2I5XPt6jZ4ctVj+uQutjdAxzzzHLxgjUlNou2+N1NrE5i6yP2UcoXpjfP2i9QSPQ3MNsf2LRdM4sqz5nZ+pxynL28/VoGtg2L6tcw0wtsVUgzWjvxOJHR9r2+ccNmbKHIbmom3upu+VtdWzt/5+miKWq3eUx8eNIzRXrhZTG0+4XyxmbP8o6ByhcYyyr25Xi9N3HTQSaBON4fo35xbpXXetuotx9xtGLeHQOF3duuZKPa7e4rNrUa7lYr+rW9/Duf61fv19Q3NRxsoq49L3fibQT0QkV2N2XzFr3ThdUNVOqnKr50pqaqcd1UbXX/ZrOOeYwC+LXv2hfkWuhK9iLFYo9jC/H0NzMQ6xq8GofwcDAAAAAAAAAAAAAAAAAAAAAAAAAICwUb6PBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwGtu2gV0XVb8aSsfpNMYjOONWNmQYxukprY588zGYybHCmxP6BuqOxeb1k5t1/2aYuj6MjUner+es6yyr10st79dDj0n/hxNMVLiLMTyz389tn++m9ZXSn7f9lBM3zGP9W273zfOlO9QSuxRSq1pEqztrby2Y67Nfz66ccVqGSTXoMd0HGthlHO12LWdC2uGvzGJHdP+78WwMdvECs1FbMxtcmeZUW2qVzh3fjJFX1eTq8Hl0rH7a0gdcyiWrmGYHKk52/QJ1afV5rK4RtmEO7da38S58rVz118j/jaxGG6FmEA/3xjd/ZYp27rP1Tkocxefi921mvvrKuspc0oRszonUuyvrvj6eK1nX9k3Mhe5OpbG1a9y9H/LwjFVnbY6B705XPhnec9nrWi9Y10Mp5xXReUsY6tm3jkK5AyJner7j1Pq76XguBLU18UAQWI5AmOekZ/XU7PUx9/EihFbW52LXxfHBAAAAAAAAAAA0hipP5vrgi6OCQAAAAAAAAAApOH5B9BN43ifDQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQMXctAvoujxb+DPSmKMNlyQb8RhEhh/HIDXlmW2XIylmZH8sR0P/XPz1hvqEatFxfP11nZmaK71fz6UO6aslFDM1Vz1HfX5CseL16jmqx06Zx4XYqoYsVIP1bvflD62jlLpnQW08iXUO2q+NNnNmrf+gp8Zw/cd5nJrmKMv87/qM1ZPl6fW2Hdso56JNnbNEHxdrZ+99lIPMrTXtLtSj+C6OMlZszKHx+XLrHKFj7r6/xp0rihrazmVKX1enrk3XMA61cfbNWezYaqFx1Nr1XZutVPPG5iA5h2cc7t7BBHKm5vJtDx2r0HY3B278Zbvisy3K77/f0XW7s1NZrxuvyi3FfhMZb6u+LnctpqtJxemnYjq52158dvd2uVUx1E2f72qq+xjVpze/SmBZGc8wylguZ+Bc11tPgf6hWhroOUrVPy2jvrp5j8P4Tl2LGvMCAAAAAAAAAAAAAAAAAAAAAAAAAJgls/dv1AMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgM7hhUcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGDs5qZdQNdlMrm3SmXZhBLJeMY0aP15ZtvnSo7doo5YzkisXMLjCPUN1adj6f5NtWZqPnVbPd+6BF9NLqaOlZqrnqM+V6Ec8Xr1XOmafLlCdaq+geOj2zXnb94f7tc8roX6ImOP7K+PN3xcYmqxW3yvY31D54g2Odr2GST2OGWZCWwPzE0++nGOYk5S6xqF0Jx1Pbe1o7vCpx4va9rfAKSuJ2vjsQeNFRtf/7h0jlqs4pinzr+LlzK+WJ82sXRb/dmdb02LutrmDOVoU4u7flnxj8PRMVrl0G2LnCaSU3Mrwp0V+tdSvQ6pjKt3/csq+/X4y/7FZ9u3XEN1l3W5uXE1uX4ud7HfBOauP0ZyX9evFtPFq9bijVlsL8ejjoP7+rq50POgcy3EqoSQvH8iRcQEbuj1qcTV1vQ7xFj/9tochvon/xrqn4N2enPcsmNCLQ5vb043wb8qWHSsGLEyP+0yRs7K9O4nAQAAAAAAAADAdBmRDj79EJ5+AAAAAAAAAACwhPH8A+gm/h1BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwdnPTLqDrsmzhzzRM8m1WoxxjntnBahgoV2K7NnVEYubiH19Tv1CdbWP5xpGp+dZt9PHQoXVtOp4vZihnPJdNzpEaK1xTPYee17Ie1VbPSbhdeK1H5yzQV9ftnavY2CP724yjbe7UfsOI5Wzab23zFzx1PKOQ5bHjFn6vZ6hOHTO137Bt29QwUMyGuUDVMHNl7WB3HsMcc2tG/50Mfc9jsXQ/37hcvS5WKJf7PhsdM9Jvoa8p+uaVOmK59fZQDW3omCk5UsbYpr4240jNPY650jFqOYp2/d/Q4LEsrtNWqvW5uyFjm9uZvrsmW7R19xdun8td1uXqdv3UuKTYbzw167Gn9i1z146Hq6lMEYzphGNXmklubXWD56a7nN9I395c6na1kH19ijaBpWYa+op45qFBby1Gm1ZzlGtmePqKNK3f1V3A3AEAAAAAAAAAAAAAAAAAAAAAAAAAZskk34kDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACWqLlpF9B1uczeW6WybPw58swOHWOYMvOWnVOPUcrc5dI89liMptpDsUMx9bgyz3EJjV0fQ51C16lj++K6NnpfPJdNzhGKVdtei5U+t2U9qo+ek9D3IFZLY52JfWP9homdIiX/sP1022HqTe3fNsco53QUanOWx85Xw+339onkbBUrMyOLlZxzhPVPkzWjvxkY9nhY2/6OadDj0TT+tuva2izar2xT1Ovyuz69GKb4XJ0L3a+pbhcrJrV9/3nMJMYehfrc+Ot19ZnEdpW2xXXcqjsPHUPniLVrqsfdbxgJ7I+Mp39l6G9crW9gfO5+xRTl6naVcRTbbLEpVL+rq5dbipjV2qTYb/pyhMae0ldEJFfbe/F6bUy1Sy1XWX8gdhlH3+tZFVikdvNYznOkb29O6yF7ffzb3VoI/Y7oHeu42rxH1L8XSd38ufVxGjzUyEz+TmM4wd92E61icTHWiLGL7UjHdXFMAAAAAAAAAAAgzXzxp2u6OCYAAAAAAAAAAJCG5x9AN/HvvQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgLGbm3YBS0WWTbuCsDyzY4s9ymHnAwRr+0avNscpl+Z5i8UKjScW1xc7NM4scGx97fU60OXpenVsHdOXW7cJ5axtj+Tq71OvOxZL72/uv5BDxYj0ce1jtSzU49d2HONQH3fCeAJrULcNrdVYvyYp9bWNOWjuYcTqq4/TRGNk+WBz0WauQjmi/Tz1j8qgNXXRJObCmnYX7nEce2v9Z9Vhxq/HFfpeWJvV2rhtLr+Lpfc77vttbPuboLyYT1PMgc4ZEqqlqU1Kn6b+/eex2FhT56qpplrb4vpmxX9sdZ26Rl+uWD3uum0ksD9hPG51m1jf8j5EKtt7nyU4D2WsYpu1Lre/fv2Nc7ncES6Pdd84YvWH+rozRpnbbffeX2VqrCqmE4jdO26i9vd1VXNTUjeXZQ36/tHW6y77lPPs3x89pSV8NfXcxNSOyxB663zoUK01TLuI8IZoAAAAAAAAAAAAAAAAAAAAAAAAAADGgX9/DwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjB0vPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGM3N+0Cui7LFv6kyjM7vmJaaFFya/mQwQd5S1ebY7CQI/04xGLHxtuUKxQ7NAdZYP3o9k3rTKfU9escOravhlD+eq5q31AuX/3tY+n9zf0XcqgYkT66fbiWeJtw38g4feOItNH76+Nubt+kFju0ZoeIOWy/cZyHU8c9zpwiIlmetq5i22PxmusyrfuMMv/Ics/I9XrUrB3dHcAkjo81zfWOcr1Zu3C2Do1L19K/Rty8um3l5yJWva+p5Azx5YjROXVNo+DOoyYw7lH0TY3Z1K6Wq7jeWWnOpfvpdk1ta7GKnEYC+wP9fWOK9i1yufsXU7Zzn6XSrn8uylhqbnLbrn4XuXKtLfaZtn1dPzX+XG2v7FP1uzGHYpft9L2TZz3p01ExNfV7/cxfg74XtX3dQr8XevPu3d1XQ4LEU0BvnQxOj2acv4H191Rr+ztxsVoq4xyMESuju1+YHV0cEwAAAAAAAAAASGGkm08KujgmAAAAAAAAAACQhucfQDcN8u4YAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAVuamXUDXZZmVPLOjiTWSKIPJx5B82LdtZQPUlEu7Y5GSI3VuQrmTcgS2Z4G1pds3rUGdXo9H59CxY/t9+es51f7ic2wcvqkLxerVp/fHctTnTs9RrI/bnzJXtTbBYxwZZ2QeUtr4xp4qJX9Kv7b9x5F7EG3qzfK0trGYWdb+PZ61Yx7LkVjroPW0zZEcc4THtutmYa6sTb/Aj2O9aNYs1BNa09bm3lpcv4W+tmibeT+H2oU0tdP78qJuY5vvvFJqdOdJ0+IYtck5SNv6eP019q9tHUP3cdc/K4PlSmlb21/kNBLY35DLHVm3QlP76u3u/sb0LWU9F/VxFLFsu/r7vy3l9bfYZwL3S7pvrZ8af+6ZK1Orqz5mX72hOa70UfmMSl9MUf03QeDHQNNVNBir7Fs9PrE4C7GauXqG+V3o5nmcv2v1jIzy/m4xm4XrOwAAAAAAAAAAAAAAAAAAAAAAAAAAzrDvnAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIiam3YBXZcVf8YpH3cCX84xxMwGHEcuduy5UuY4VkcsZ9OcZpk/dqhPHmjvKyE0Np1T54rt99WgU7k2bWP5xxGKpfoGxlvPUa9fz1WsT+g4xObSGzt0TPV4E/qlxm7bLzRen9S+TbUNmj91vMP2GVecLI+dazzfvbz52AXXVyBXlpnGGtrEahVjRMdh1nN2jbXtLrqzMueu7ujaNdXvg7V5rZ81RaxibHpOXFvXLqSpnY6tP+fF99ao+mI5G+uJ5rRFzsj4+455qG8oZ0iof1OMWr3Fdd1Kcw1tcsXmxN23GGmeM39fKfrGcktlXLqm/vsd11bPRX0cC+1y21y/b67c0S+v48U+PY5yfIHjJmq78a0rtc+qMffGq2KXgVSOvv8dOh69elSoIrS+VzVSPw5a7CrsjkO4f/W4NenVOZj+Wkf1u9U0DG8KP40XBeYlzFoj1s5Pu4yRs7b9/ToAAAAAAAAAAOgGIyLde/oRf04KAAAAAAAAAAC6i+cfQDeN4701AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFXPTLqDr8mzhz1RyTzBXNsIx5mInVkPbY5NSW6yO2HHJsnCOUN880EeX0jRenVfniu3XNfhS1doMGNM33nosnSsW2z+HvjmL9Q2NMzanTX3L/cE6I/0S5qzcnjieptihtk3rO6m2xP5t2rapf1Q5s9x37Nu9h7Op7lCO2PHw9VloF68t1DfYvuXcTivmOLSdq0myZvQ3LYvluDjWLsxBrO6ynT6exhT7e2d918bNr47tYoW477tR7frjxGKE6NomwdWdUnNs7Pp46Zih/m36uOuileYa+s/LsXprOfT+IqcRf23NfYv90dxSGZdvrtx9kGur56I+jqKfra5xt7ya5sptKfcV2/U4ctVP3x8aNQ8L+QNzVLvPqs63qe6uX3s960rH1Dl77WpdF9qpuav0Kec9sN8dp8B+FzrlN43L1fY3li5/FL9J9V3ItH5fL2bMGQAAAAAAAAAAAAAAAAAAAAAAAABglkzynTgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGCJ4oVHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABg7OamXUDX5TI7b5XKssnlysUOHWOYevOWfVPrbVNT7LhnmT9nynrJA311eaF58OUO5dVtdTtdiy9lrU0gZixWLM5CLN0mVq+qpeEYx/vG6/Nt9x3P+hylxR7GoOPxxkrsO+icNcVIrWmYdoPOf5aPPnaWmWi/2PHQdbmYtTgJ9YdyjLp9q9gt6l7qmCsR8S//krULFwq9Zsvtbg6N6duXV/ZZ03xD0YttKv3L/Q1xXF9dp/ucF99vY/1X/lD/agxbxPDnqOestm+i+zqhGLHcun9KjFBOd520Eu+fWm9sLtvkqMV2443mrqQK3AtVNxpbnYt6LVLkKmIWc2fK7eFxWLVdiu16HOX4yuPjj1dpq++vArF7c+RySCVHrcZ+6tjpnPr4lO3KGuohe/X6z9FGqvXW9rv62/yWCeQKcTWM4jenTj0rv6dDIpeNsUqdm1mfw2kyYiSb6lEcD9PBMQEAAAAAAAAAgDTzxZ+u6eKYAAAAAAAAAABAGp5/AN3Ev/cGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADGbm7aBSwVWTbtCsJysWOLPcpx50PESh1jm3pT3xaWZc25m+Lkgb6hMkNzpGtoyhlrq2vSKX01jzqmb071MdbHsp5D1RRp39y3ub7QfKfNVSR2LHdCjrbjadsupW9IU8zg9yMxdmoNbWJmefvzaZ6Zxpxt5jVWQ+3Y5/oYmsb9oTiN9bSsP5Z73PQcYOmwduFsHV17aolYu3ABcWvdfe7n1pXLUd9f5Cx2W+O/oLvzlvHmqOaPfZ4GXX/znPn3heagzXhTYwTrLq6bVvz9+2M0HbMU7j7ASHVcTTn0ONyqc0u3tj8wPpHeGHUbd99krP9+qleLq1WNp286QnW4yOX1utiux1GOr6xf9esXiqGOj6ndj6n9vtAqr1HzH4pd5mhYI716/ftz649ZavEjx40ttYs+toMwte/SwKEGFpvCJrxFGgAAAAAAAAAAAAAAAAAAAAAAAACAdvh38wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwNjNTbuApSDL4m1yseMvpIWUmgeVDxl7mLlKHVebN4FlWVo9oZh5Qv9Q2aG5DNXkq0G3Ta1Tp67t99SgY6fGrNdYj62PbT12YE4G7OdtG5lLtz9triKxY7kTcuixpazF1NipdYXaBddwi1zD1tAUs9yftzsf5ZkJxxpxrsbjkuv5NpH96XOWmnMQus5pGMU4MKNM8/qyduGs7taANQsXEPc9sLb6uf8i4No65ToyLnbzDYpb+66GWhxPjhh3PjKJ40oRmov4+PrGodqGYrhzuYm0b6ohNUaovbuOWs+dmo5R69t2f5HL9OVqH0OKGGn9fWPUbdx9lLHV9vr+ytVty69H75i7pRuqw7Usr9/Fdj0Ofd/VP45QjF4NbedKaoytftb3G0bFLrerHGXNfTWG7s97ff37ffMdYiQ8Nm97d8yH+G3Xpr5hmeAvqgXj/P2L2WatFWunf485ataO/3sFAAAAAAAAAABmk5Xes8Qu4ekHAAAAAAAAAABLF88/gG5q814XAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAgcxNu4Cuy8VKPqJ3q2XZSMIMJB9j7kHnZ5j5SH3TV5al1xaLmUdiNQ0nNv+6zlAtTePRfXS9uoTa/oTYw8bUa8W3Buo5VIzAXMb6+dqE62ze37S9PkZd/3D7RXxzkjiuFsc81jfULhSz6fvTZn6HaVfpkzf3ybP094SmzkEvti3amaT2/W103WWM2vb2cxqbE51zGKm5JmmQdYQZE7h4WrNw4ait3Vx1KHZbW7/QlGvWmKJN2p2IW1e+mKl9BomRGtudj0wkdpt+oXpD20OxQvMwTIxQe989gxX//Nf6tt3fl8sMmqPs7x9n/zVXj9GNS+dw91em6BpsX3y2fVPmxlR81YLjcF3K+tRxMQnj0DGMnhN9/xLI4fSvjfocVAXvZQLfH53Llzd0Jukd22CIIk5ffYm/yYxUj3kbRqUYx+9bGxjGqH6TL1ZLffwAAAAAAAAAAAAAAAAAAAAAAAAAgNmS+t4XAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAgfHCIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMHZz0y6g67Js4c8k5BPKU8kpdmSxhp2nQd7elWVp9beJnUdixobZdBxD9Ybqa9M+VLcuR7fTOVJit4+p96fkUDFUn/B4db96u9R5rY0jcb2l9G27X4+rqZ62sZu2p+YIaZqzNnXEYqX0FxHJ8lBsM3zswDyHcoZqS5nbLFBvqAadY5DYtXaJ42rONbpr0NiMYJyYjODSVSd3axYuKG6tW1tt4FuX1qbd5JR982quWklFO9MX132nQn10DleTO3+ZYhypcZrq0TlS+zXVGdseiuVrr49Rby7SY0TrL667VvxzEpuzlDl190dm0BxFHLf0vTl0DDUuvd/dbxnrn4eyfd/doC2XfbEvc9ubj0sttxtPwjj09l4x1f1G7c8j+xfqUG3KuVCpAjXo8fj48orUj22wf/HPlN9fVqVq+/uv/1iP+veq8ZQyqd/eiw3zEmZlXqzMT7uMkevimAAAAAAAAAAAQJr54k/XdHFMAAAAAAAAAAAgDc8/gG4a5B0xAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAArcxNu4Cuy7OFP1OtQezEcmUjHOswb+PKsnZjTs2Vt4gbm4qUdREaR6je1PZN49Bl6bbD5EiNrdesXlf+2IG6In3D/Tw5Eseu28XmsOk7Gu0b2e8bX+j4x2KH+OYlNUfb3E3f7eDxSYzVGDsPxTataknK1XIcKXFc/Zmqt7c9so5yvd8/bl/blPoGlphrmprmCgtS10wKawa/EbG5/+paO4R5NVd5jIv+bWoox16EsLa5r8tlbfzOxX3HXEz9ObnGvu9q21ipNfSf34zaF+oTixWKkxJLn29NYv06p0jvOmzFH0P3Td1faVPkMIPmcONU8+GL4Rj1ta3f12aVdnoeKse82GaLTeV49D1cmTvry1AfX9nOd38SiOFaBmO5UQXmzteml9P/PdFzWLZvuD7qumr7I9dWX93BXMU/2/6+s3ptjOC3qAnM4bR/Zy8mzBUAAAAAAAAAAAAAAAAAAAAAAAAAYJYM804ZAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAJHPTLqDrMrGSi512GV5ZNv4co3ijVpYNPn9t8+ctcqVOXx5p2DS+WP2hvrpf07h0eaG2Olcsh2/YtTa1mHp/LEe9Vj3fwfGovqnjXqjTrzaeluMN5fOJxfbNTUis7mCONt+X1HXV4jjE9g0Sq9Y2D8U2A8VuM45Q7qzI7dqH2jXWUfQJHVMdMwuNtyF3dJ4HqLuew1/XLBjkuHTFMNftoXMvq+a2Nn61tmahjV5P1hZne3csy3bF57zav1aLbx7cBcSYao6Ach0ZV1M1V/95zhT7XJ9QXbq+lDlK5eoxkZhNuUMxQn1C25tqGTaWPrauve94lLGL67KVagzdV+cI7fe2KXKYQXO42vvrjoy1jKXG15uL6jwYW21XiaXa5tY/nrLOMnc1Tq/YXjw3Jh2jV1f4u6VjiYgY3/2hbqNylqHKOdQ1FO1rkaW8qwvd2xh1DGv7A7X46PWQqpejZUdvDdXPs/Z7Wq/VWRKaqza/DZYca8Ta2b2nHFgXxwQAAAAAAAAAAJJYqT737QqeeAEAAAAAAAAAsHTx/APoplG8jwYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKDR3LQLWKqybNoV+I3rDVhZNrr3yw1TY55YR5vDkyc2js1B07hCfUN9QuNsKlX30TlTc/lyxGPr/bEc9fHp45DSJ6W2ap1VwePSery+8aTVpbenjtNbV2qOxHE3tR0096B1tKmlSZ753//ZNnbjuHL/3GSB3Dpm+c++OMl9c32sq/3q+xvmLm+e11hNKXQ90zTK69wsmqW5biVlmYUucGahs7VFAzcHpnrBKeemaC/5QntrBr/Zc+vJWn8M9/0paxsiluPOb6aI6cbVPw4dKxTbnbdMpF1TbTpGrE9qLeOI5WsfbFtcp6345yY2d/3XuGCbIocZNIfUua9SNJYan97v7teM7Y0j1NbdD+bWPx5XZy+3iyeVeAsbs5bj8Mcyao76TzG1ext17HXusjQ1N2X7vmahb693rJUY4WNaaxuoL6T2/Ujq1ZzbmeRvZZs23Ap9Lw8AAAAAAAAAAAAAAAAAAAAAAAAAANoZ1/ttAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASrzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjN3ctAvouixb+DOoWXkjVZbZkcccdmz5EDW1PSR5QofUOQqNO6V/qG9oLkJl+9qH8uucuq/OkRI7FxVDBannCNTmGWCsb3CuajWG1domx9Tjrver1Z+YKyTpeCTWH1wjLdZT6vdkkP6p44huz33HxYwmdkP9Om9sPL46Y1yfMkYwR3W8ul+vyIbxBOZMxxzEOK5JybmHqHsm5c3HabHL3MncNJzV1RRYmxV9i2NtTLG9iOG+D6bafrg6Xa7mmG7t+/a7c4aJ1KNjNMUcdYxQu/7vtN4XGlcsVijOKGOF4vT30edy18fdn1jxz6HO4asl1sbdd5iWOfr1YhX1q7EGY6nx6f3993DGNrd194m59Y9Hf7tdv/7RlMdBHTMTuf/Tx7h2bfaseT1HZd/A90PX0Ksl/H0yRZdQCxcxdt/YP77U32ah8bVR+04NHCkudpUb5rf6YrJUxjkIK1ZsdKUsPjbwGxoAAAAAAAAAAHTffPGna7o4JgAAAAAAAAAAkIbnH0A3zcr7dAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQIfNTbuArstl/G+VyjI75gx14xhTPuQ4soFytsyRUGPq3IRipfQPzVVoOLp90zh0/tRcKTly0W0iMVR7fbx8tek+bepbqDFM90mP2dzOO46WuepzlXA8Qsc2dVwt1tWwMRrXbMs+we15+jmodeymucljx84k1VL+s8U4dA06V2+7iunJEeobzDnAOX+QsbWWN8/3LJvG/cAgRnEcrWl3AbcNxzXTZ/6iqbULOcp6jSm2+68Uup3kfe10ercrElPH1uPu/97pGKE+MXkR0xTx+o9XLJZbg27u3PnMWF13tV3KvraxmnLo86yLqb9DNrI9FKfNONz120rm7Vdr35cz1qbcX+QwkRwp4yiXrtpfjk+qTLHbjc93HN39nbH++77euIrtNm08eq349oXGE5xbceOpxqtQeUNzpec01r5aVz3tQp1SqTOksf4APWfJ/TzbRn298n3PHd7wvIB5AAAAAAAAAAAAAAAAAAAAAAAAAADMEv69NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMHZz0y6g67JMJMvsVGuY5Fut8hGONRuqjgFztqg/Nq+xWCnHJTafoWHqfk21hOqoxUiszZcrF11PLJdV++O5633S6guNv3HOEue3Pu54jbE2en9s3N7jkTg3wXG1WF/DxmhzHAaNleVNOcxwsfU4G3PpOfHnbooR6u/6lDEisXT7XpFuu9FdwrEGmPdqznquYU37utxW8lxN2hiOTarMnbxN4l2OKtXavguLG0cRq5xvU23b276ww+ZFe9VulNxa1bFD20V65xIT6dMUI1hPMQfWNMfQ21NrStkXi+Xocfr2pcZMnf/+87iJzLPO6a7nVvz9fDXG2tT2FzmM+OdGx2mOVeW+YuGaqu377+1M2VYqbd2c9La7WooYthq0WJbN4yjzq4LU8TGR+0W9VkSkvCOLxi7+GZz/WuRwXVoe+QXVm9t0wXFFmITxtBW66iy26/o06N9f6LF2Xqydn3YZI9fFMQEAAAAAAAAAgDRGRLr4pGB6/y8VAAAAAAAAAAAwbTz/ALppku/CAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAS9TctAvoukxm961SeWbHniMbYax8iGBZy7G2OWax2LFYKcchNPRQ31BNTbXoWKk5da5c6rkzFayeK1DvAP1i9fXq1DWGaggfn5Sxe9sl1Bhro8cePS4txpE6F01rf9gYozge0e15KEf6+zBTc4ZyLeTTc2LU5+Yc5T8bcsS4nC5GLWe5XdXWl7PtPEvePM9tz9utck9DZLzTNor5noplC+/ktbb5JiHTZ33P4bDuGJmFtuX6KTbHcoxCLKfbb016Lal93BpwOd250Nj4XZHuG+LOd0aPq2/91cas1mavvur2UExfTaF9sfpCtfly6FipOd313Yq/X/+4Q7FjufX9ignkaorl9GK6WJJWU18N7n7PlG3dZ6m01e3c/aUt2rlx+ZZ6WV+Zv+gbGGfqePrpdaNj93b4vyem4Tys57nWV9UZ7J9w6jAqROrZRmce5remnssy5sARx2MWr+yhORr/FQwAAAAAAAAAAAAAAAAAAAAAAAAAgHSz9u8MAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACADuKFRwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYOzmpl3AUpdndtoleGVjjJ0PGTwbYs7avuGrTa5Y7NixTpmWUIxQnaGammrRdei24Vy6XTxvpvrotRFr36bO0FwEx9M0R7XYacclZS5jbepzFhm3J0dozKlz0aZ/2zU7iuMR3Z6HcpjkHKk5w7l8x8Woz/5YsfOSi9Of2/Up/1nGqretFtrczldLLVbun9e2xydJINc4DHMtmoSh5nFcYsfHDP4eTndutMZ/NbUqd9Z3VXB93DEt2wbqKefWmKL9QrvyK+y6mb6cRRvRU6DaWhvI6Wqz4bsF9z0NxQjFin3uPzeaIrabAz3foVhOL2Z1u+kbV6hPKIfjYprE9sPE0rX5zgd6rCZxTsp2bk2L/7g0xY7lro23yGWkXmPsmNVyuv3FP2M19Y+1/hshK9o2t3N1W+sfT38d+tuhj3Ht+qz2m4b1WT+GKlQgh1FzV27v+9+ha46ed00fhya9YxRtuhBbhRzmJ56ubpK/kfUaaIO3R3eDFSvWTu4eclJsw29nAAAAAAAAAADQbUbq//eELujimAAAAAAAAAAAQBqefwDdxL+jBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAxm5u2gV0XZ5ZyTM7kljZSKIMJx9DEdmQ8zPMW7tSc7fJETvesSlMWS+hukN1NsXU9YTa6py5WLU/mKIWM1N99bqKt6/XmDonwXaJ467G1nOQFrs2lwnjic1BLGbTOFLqaYrh2z6KGE1xBomV5aEc4fdfpp4jascrmMt3rI36nBbLtQvtH0SZu4jpatM5GnPnzeMpt4fqztPfRzrs9cMbc4TzOTIt5mRcxjHXNcvmg7usTbsBydRZ35qFfq7+Mk7fnLo+tbZFmzKmaVfLKNTqDuz3tXHnGzPGet33xc2drkvXFNref27U9Q4aKxSnKZber8/ZqbX59sXqCrVz138r1fXZ2CcWMzTeIpfpuzuMzU0wZ9HefdNCNTWNtdcmK9pJYztXt3WXsr57JyOJc+PGWfxT7w+Nq5++99SxHZ1Dt2/6/dGUX6R+HGJxmmIFYw9xajEq1TjOUqlXrVH9Vp91S2WcAAAAAAAAAAAAAAAAAAAAAAAAAIDFYZh3xQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACSZm3YBXZcVfyYpn3RCEckyO7JYo3gLV9t6BsmZR3KkHoamOLFxhOoOxWyqSfcJ5c5Ft4vnzlSf0Bqt1VDrF68xNCe6bXCOGuZcjz3WJ1avd66GnIPU45haT1OM1P5tYgxaS2OOPJTDDJ0j9VxTPy713LXYebtcLqbr19/e/e/evkDbwFyF4lTkJmkcun0oR2MdkTqDAjknbZTXzIFrGHQOp8SdC61pvrpadYwzfVUodlvbF8et3aJtLMckuOMTqqVpv/t+W5s39nHr0M1F7LNI77xpIrF7tdRjNG1fyGGLHMPFCsXp7+MMG7NpPHpfagzdrvwO9N3NRfsE9jvBdp77HSOJMVW/XI3TNNy7hcbay5EV7aSxnfHc8eZW1Vk0Cc93sd/113V7jrUbe+3+SLdTOWq1NlwjTHmMAvsDNWg2EscX02l7DfN/z1uFCDINpUz/TD5bmI8wK0ZsbaUvfl0cEwAAAAAAAAAASDNf/OmaLo4JAAAAAAAAAACk4fkH0E2jeLcMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAo7lpF9B1mYjk2bSrqMoyO/Yco3yT1ijqbVtP3iJn7PCmxmoaZ6z+UI5Qbb72ofy5WNUuLWYm9Xj6uxDrExyXZ7ueo+B4WsRciBs+LrpPauzauH3HIzIXg8RM7RuLkdq/TYxBa2nMkYdymKFzpOasH5d67lrsvDmX++za+WIOy8Xs5ajmLOWmr09gHHnafIeOVyhfinFe75LqnZSW8zIuqfNt7eA3Rpk72ZvAlVFNhVVzk7mrRd/msp6irW5jA9vLC48xRbtiw3z6+Mo507Gsf3yuvW8OQ/vcecgMOO/9x9XFdudRo+p03wtrqrlCtTWNJ1R321i+68qoY7p4vu+AVft6c5g2Pt2u//7ASlrs1LXReDyKvEb8Y03NXS51z5jLfWqsbpy9ea/WZ6y/Xf/cuvtXW37lijb63rT6sXZ8XAW+HLW+ZW51byB+vXHX6byhvsHY6nPK+Vofs1SD5EqtRZu139mzjKkCAAAAAAAAAAAAAAAAAAAAAAAAAMySUb6XBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwIsXHgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgLGbm3YBS02W2WmX0GgSb8Aadg6GqTFvmTsbQ+zY+JvGF8sRqlf3a6ohF/++TAWvxQz0W2jbrm9qvb650m1DcxaOGR5HsE9ijpRxtZ2L1LnybU+tu+2429bRppakHHkohxlZjljO+nHx526M2eJ721+Da9ffvrfPeNtKbL+Wm2AO3SY4nkj7UL8moWM/kEA94zTVe4RRzl1E8Hph0q+6Vh8fs3BFcGvAFrHcnFpbxHZrt/8KUoQq2wyp973qr69IkucD5ayNI6mPKfpUr5axOWqTy51XTSRHryZ/7P61r/e586lp6OOrPxQvJWZTfW1q9MXUn5v6xnK475KVtNihccTaVdqq769pmbsXpz+GVNro+ztTpHTj1LHdfaax+lrcH8PV4cYjlfEkj0PV5rt/MZ4xLuSQSg5H59JxfHnLvg11NNXibas+p16Tat+HpF5pNbStZRRGdS3A4mGtERtcfYuXtd0bEwAAAAAAAAAASGNEZH7aRYwBTz8AAAAAAAAAAFi6eP4BdNMk3m8DAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACWuLlpF9B1WWYly+zA/WfljVTDjCFm2DHmQ9SWjSFH6lyljDuUN1R3sH1DTbmE+qTWYlW7FnXV+lY/67r1nDWOK3EuQuMfJEeobaydnoeUPqm5fdtT5yZ1PLH2vn1tY6R8r7I8VK///ZaD5KrNUZ42rpQcofpDObPAuMahrC03lZq8Nas2tRiqXbk/dDwC89IUK9U4r22SUvcYJc3brClqtiZ+ddbL3+r1pK8YxW5r67HLuXJt3Bp2MdR2Mcui9Q2r/H7nCzWE5qT/OMfmzZ2fTDEHmZpv931wc6Q/h7YtxDZF7Oq86xyxOM05qmva2LSYTblCMfW5ITVmf7xYffpz7fi0yOHuI6ykxQ7V5LuGRespcpvE3L44btW4b3E4hlTGqWO7e1DXrv/+St+fGnU3nVu1FvR9cPVj39z3uAihdVWLUfwztN6a7il8+SsxArWE4vjqC/ZRnwe5lvq+j21qGETqncJY7w2mqKvjAgAAAAAAAAAAAAAAAAAAAAAAAAAsTrPyPh0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANBhc9MuoOtymdxbpbLMTihT3SjHmI9gHNkEcqbOd2xuUnKHxhPqG6otl3CuLJBE58gCMfJMf663031T69dz6BvfqOai6bi2zqHnLmEuY32iMRNraWrbJkao/ShiNG0XEcny5u9OnpnR5dLzHMnd62fUZ89c5WnHMJTTbXf9yn/2tXd16LZSfvbvL2PkprE2X5tavXnzXATnNPcfx2g9lRijvz6mroGRaJiDSWtzr2Ft26txkaP/pG/8V1Fdhf66WzdnRf/yeBm1vyFHuL5qrPIiZUxR27JW8UT6v5NFfaZ57lx73xzrfe77be1gd2v9x7wX05/fnXeNyuXmTI8rZRw6dy+XLXKlxfSt3VHH7N+vr3cmMHf6s66haY5qbYtvhpXm2LGaGnMExuzuq0wkty9OL0ZRV/HPcAxdY3U8vc+9Nnpu6vW4WqTVePrpdeNahOZdfyP1uH2a8vfHjp19Q7U1CdUd7efZNqrfrW2uM0v9Tc9LffzNrFjvSl3spvf3QwAAAAAAAAAAYLqM+J/TLXZdHBMAAAAAAAAAAEjD8w+gm/j33gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwNjNTbuArssyK1lmp11GxSTecpWPYczZEH3b1tPmmKXOZ6yGlPGFYoTqzSXUfoAcKlauYjSNr963+lnXH5rTWjtPzvZzEWifsAZS6vG1i81HSuxazBbjCLVNjdG2f6iOQWJleVOO5vdYDlJ36rmgPkdGfVb7PeNIPaYudtNcjFuZO++N09Vbq6toE52D3H/8Go9BZA6GmqNAPaM0lfuDCa4bfa4rmfQrug2tC3WlKDMVsd3clv1NtX3/3Ls2ZUyjYrZVzHHlNODSm2JjrnLZ5jkp13JDe9fGRubXna9MEUP3K+fOl0PtC7V152Nj1bwHavR9F3TMcK7qeEIxU8ajYzpGjTdWY0qdsbmMtW+MXaxeK7Hj5M/hG2NsTsp26pujl6PvHs99VXoxqtujc1PkdPempmzXX69U2rq5qc+Bq8XV2zye/jkNzVHZNtKu6TeFngstdJxCtaSchXWk1N9TOtcofnuGrsiz9jt7ljFXAAAAAAAAAAAAAAAAAAAAAAAAAIBZMol33wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgCWOFx4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAICxm5t2AUvVrL9pKs/s2GJnI4ozTI1Zy75tjlesrtj4U8YVqj8X//asIWkoX6Zi5YEYur/u19hWfdbzHBynZ3vqnMSOfdP8t6nH174+p+njCMZMrKkpbu04JI4nJfaoYmV5Uw7Tuq5BavDVUZ9noz6r/Z5xxI6p66Nj6/blPyPtFwqvtun1qcaQ3CRt947N08YXQ49D1+jTtB58sWPaXhOac4/v+lnP1W6cM8Od7E386qqXsbXFxciNvYjh1kQ5+2ahnTu21q3HIrk1CXcjqo8UKW2Ze1k8xpDcd9Ra/1z1r91ybtQ+t13Hcucv4/a7OdRzZ+tzVY9dXfduuz4/G1s9XmV7z/EI5Q9t1+djE+jXFDO0X89VrJaUOo31z7P+HGqfFFvc9yJ2nPzjG2QctdhFDUbC66qMpXLnRRu3PTo3RS7/PWt1o7HN97lGtXfNQ+Optg3V5xe6V+o/HqGzpZ6bWC1Naus72qPIoT4P8xvNtwZFJvvbeZFeWTEEa+dldH9LMTsWxgUAAAAAAAAAAJYiIyJdfFLAszwAAAAAAAAAAJYunn8A3TTr790BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAdMDftArouk9G/VSrP7IgjpsvGGHvYcWVD9E89Rm1qjM1VSqzYmHLx788CyZtyZoFYeaY/B3Kq/r52ofHo+dftgjk928NzEhpfu/ZNfUN9UuYmNccoahrVXDTN0ahiZXlTDv97KwepN7Zf1zGO87DOHxp705wEYxd9hjlPJstNJVetXrW/tz1t/P0xtOTxDTCHqTWMwiDHeDGySe+erV4h9NfequOQFe3LGTT6ImYq7RbauHraKY9T2X/4u6RyDRflWV2/amdtfb+ry/VtauuNHejvixGKHdruztvGquPat+b1mGM5QrW587TxzVGkb7j+aj8X23fusYF9bruuT+fUn33jSY5drG69RkM5UsbYVJc3dlGDkfC6Cs1JrmKbyNz4xqvbuPtbU5Sh+9RrcbVKZTyVujxjW2jr16u3yvVuOh7R2MU/Y9fF/jWeem9TO9ZJvVTewPZx3F/5zgFNuvpG6HH+jgcAAAAAAAAAAAAAAAAAAAAAAAAAoK2u/vt8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABghsxNu4CuyzMreWankjubQs5RjjUbQay2b/QapP7UeY7FThlvLv42WaCIUM4sEGehT1qsUAxfTj220HGptdM5G+ZIz02obXBOUua/ZV89R63GE2nbdq7a5Ir1aTu3bWJlefNxyDMzdI7Y9qZ6wnNl1OfmcTXlrrf1j9nFKP+Z28b2Tbl0DMlNu+2xfZ79ve3N7b11h+Yvsn6aYgZzxWIOYhr3BeMYR4hJuzJmy4qabLi9FX3MqlcQt9yti+HWWdHOhtr5qL4utXXrxiwL95W+tWJM0a9Xa/m1dJuKNpKrXE319eWwnjl234tojKIYaxdyu/OaifRrytF2uzuXG1u/IwiNUX/v03PV174ea6z+UM6muUudEx1D7w+1TxlHLba6L7Hiz5Eyxqa6msbj7tdM3x102zkpv0YtxuvGqtu4+15TNHV9dPtyXOrO3/ZfBsXfNrSOgnMmda5l6D5EH5/QvbY+o6bcC6V8p1JqquRNihCXUsm0fovPGuahiZH6t6MLujgmAAAAAAAAAACQgqcfAAAAAAAAAACga3j+AXRT2/fBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAtDY37QK6Liv+zLI8s2PPkY0wxzBv6Wo71jbHLjV2bC5yicfJAoWFasgaYuZDxgq282zXxy40Fzqmbtc0R6kxB23f1Dc0z7HxtGlbm4uW7Zv6to3RNEehPsHteShH/N2UrXO1rGGhjlAsoz6rucvjcxmuU8UuYqWeT/tzl33KGC3f+ZmbSpwydt6LE6xL9RU9J3re8+Y59cUI9Q3mSDHsdWuQnC0NNK5JKOqyJvEqqg+b7fXTY7SusckrudyStjZ0UVPt+teKWZZUZlmLi5HUazi9nAtJra1eSfu/H3rsmToOrq1r584DLqY7zxm3v+E46liDbnfneGPrd3e1Y2/a5Sr7edaEHmtq3/A46quhnMfEOanNf+141ePExhGKXbYrVrGVao5+OkZofKH2ofH038sZCbSJxij61+4T64y+1NSWRXWDsfrcE6qpnst11ferepy99s1zXK+uvy7x9tWMmrPGtupz6n1HyveiLb1mtVn5nT1LV+PQnMzKXAGz6Ec/+pHcdNNNctddd8lTTz0lO++8s+yxxx7y0pe+VPJ8uu+JN8bIlVdeKWvXrpW7775bNt54Y9lll11k7733lhe+8IVTrQ0AAAAAAAAAAAAAAAAAAAAAAAAYBi88AgAAAAAAALAkWGvlH/7hH+TTn/60/PCHP/S22XnnneXNb36zfOhDH5Lly5dPtL7HHntMPvKRj8g555wjd999t7fNi1/8Yvn93/99ecc73iFZ6K3YEXfddZdcddVVctVVV8nVV18tV199taxfv77SxtrBX+u2cuVKueOOOwbu/+1vf1sOO+ywgfsDAAAAAAAAAAAAAAAAAAAAAABgdvHCIwAAAAAAAACdd99998mb3vQm+cY3vtHY7u6775aPfexjct5558m5554rq1evnkh9//Vf/yVveMMbZO3atY3tfvjDH8q73vUuOf/88+Vzn/ucbL/99knxb7jhBvngBz8oV111ldx7772jKBkAAAAAAAAAAAAAAAAAAAAAAABojRcezZg8G/y/nD5O2RjryoftP0Rt2RhzxeYsl3isLFJgqJ4sEDtviNc+VnW7Hm/Tca21DeWuxWzOOUjMWPuUvoPOUVNu3TY2Z7H2g9TVdq6acgZz5KEcpnWOaK6WNSzUEYpl1Gc1p3n74xOqI7i9iOH265pS6BiSG//2NjEDsUTPiWqnayr5asgD6yNUb5vrxABjbsw9jHHEnKByTkzzxaw2St/htVklpnWNTHG1cd+DYrN166+4GtmGGnp1VvuKWdZYt+gc823vKjy1qLVqrT9mOQ+ecbkYVs9ZYA7cucPahXG4855R/X0xdK7U7WW8Yr/vnG9s9U4iNI7YnIVqWchb7WuGHIcvdjmPgT46R6ifbt/UJ1R/aLzuPsZ67ozDx6xdztB4RHr3d0YGmxO3Utwq8ubQMdSY9X53z2yK4Yfa9+vVV91u3eVOne30eHvtwzlquQL7dc9QLH38FupMo7+1w/xeDJ3rxvnb2Df2QY0uEqbBWiv+m4/FzdrxfX8Ws8cff1xe+9rXyrXXXlvZ/pznPEde/OIXyyabbCI/+clP5Kabbir33XbbbXLEEUfIFVdcIc9//vPHWt/NN98sRx55pDz00EOV7fvss488//nPlyeeeEJ++MMfyl133VXu+/rXvy5HHXWUXH755bLZZptFc9x2223yb//2b6MuHQAAAAAAAAAwQ4yIzE+7iDHo3hMdAAAAAAAAAACQiucfQDfxwiMAAAAAAAAAnfbWt7618rKjZz/72XLmmWfK8ccfL3nee+XblVdeKW95y1vkJz/5iYiIPPjgg3LUUUfJDTfcIJtuuulYanv88cflqKOOqrzs6AUveIF89rOflV/7tV8rt83Pz8sXv/hF+Z3f+R159NFHRUTk6quvlhNPPFE+//nPD5x/bm5OVq1aVY551HbZZRf57ne/26rPjjvuOJZaAAAAAAAAAAAAAAAAAAAAAAAAMH288GjM8sxKntmJ5MomlMcnjzdpH3OI8WQTyJk637mktcsSig7VlwVy5IGYTeMMxwrkVtt9ayE0VzpmsJ2ktWsVM3L8Uo5v6ly1qUG3rc1vZH+sfUpdbeesaa6COfJQDv+7KAfJMXgNTbmM+jz8eVfXoXPoXNHx5p525TbjzRmUm2pu109tb9ynx9cQw9fetfPGKDdExhMZb/J8tIjZLtboQs2mwFyZhYuUnn9va7cMbLWPdTtMdRLdurJunRWTXMaej190y7pMQ12enOXxNL21a4t/gb78eus2xX5rmuty32Fr64umHLONxUhr586Fpq9d7Vi5Y6i+gy52aHtKLe56YNRYQzXEYvrOnbqNb8y+vu3G0RxTz5X7rPv5coT6OE19vTk8q9xKc92pOUPjqdSjNrlDmzon5ddK1doYQ9y5xL9f30sbq89XvQbhuajGcCH0/a2R8NyExqH1xtXM9W7z20ev4UEuXaE3jo/yd2zs3OZM6jf6rFhq4wV8vvvd78r5559fft54443lW9/6lqxevbrW9qUvfan853/+p7z0pS+V2267TUREbrvtNvnkJz8pJ5988ljqO+OMM2TdunXl5z333FP+8z//U1asWFFpt2zZMnnjG98oz3ve8+RlL3uZPP300yIi8oUvfEHe8573yEEHHRTNlee5PP/5z5eXvOQlsnr1alm9erXsv//+cu+998ruu+8+0nE5c3NzsnLlyrHEBgAAAAAAAAAAAAAAAAAAAAAAwOLT+X/FHgAAAAAAAMDS9cEPfrDy+QMf+ID3ZUfONttsI2eddVZl2+mnny6PPPLIyGt76KGH5K/+6q8q284666zay476veQlL5EPfOADlW16jD6HH364PPzww3LzzTfLZz/72fIlSZtssslgxQMAAAAAAAAAAAAAAAAAAAAAAAADmJt2AZ2XiWSZnXYVFZN8y1U+wrFnI4jRtp42xy6XtLZZZCApNWaBXPkQsXXMUNvQnOh15WuXGjM0l7V2TeMJ1dlyXJU2wXlvOVdqu69dbKx6/yjmZpRzFsyRh3KY0eWIHMtwDU25jPqcNr7YcWrMUcSK5dL92ijrzU0lV2iOhhHKVVL7a/0qGwN9QzlDBhnnCC6i45jfcLIRxRmiZBssInDc+luY4pObM7Xcs9xFcjuKDaZ54P3rz7o1WfS184G+qe1GqKzTjVONq38t1fYVfa3NKm1du9r+4lxibXWR958bjfXnj+VO3e7bp68PRtXXtoaUNvp6UBt3wjgc18bFjMXSn3U/31zFxqH7xtpV6lPfU/d91mNNqVfEfwoN1efuB/XXOY/2K/b3jyM2r7X7YH+NbrspmvvuDd0chdaR/i1gy6+5au85d4bmvVe3/7xaW3feVqou9Tn1d5RvHZUxkiJ4YrZoO2u/vWfG+C9Zi9bCPUT3Jsi2+uZ03x133CHf+c53ys+bbrqp/MEf/EG032GHHSa/9mu/Jv/1X/8lIgsvJvryl78sb3rTm0Za35e+9KXKi5QOPPBAefnLXx7t94d/+Idy+umnyy9/+UsREfn2t78tP/3pT+W5z31usM+zn/3s4QsGAAAAAAAAAMy0eenmfwl1ftoFAAAAAAAAAACAqeH5B9BNXfxeAwAAAAAAAIBceOGFlc/HHHOMbL311kl9TzjhhMrnCy64YGR1Obo+nTNk6623lqOPProxFgAAAAAAAAAAAAAAAAAAAAAAADCLeOERAAAAAAAAgE665JJLKp8PO+yw5L667de+9jUxxoygqgXGGPn617/emLOJbnvxxRePoCoAAAAAAAAAAAAAAAAAAAAAAABgvOamXQAWzOqbp/LMji12NqI4w9SYteybS3r7LDLAlLqzSL58wBxNcXWf0ByF1qyvfXrMxHae7W3aNrUv9ycc69TYbWqI9dX7R5lrkBhN20VEsjyUw/8viEaPS1OuUH3BGhK+g6rO1Byx4xSqqWlfdG6KfmW7vjhuHLU2Ea5dWVNu/Ns9+1z+UN9ev+p+PR7x1RqaI729YZ4X9jfvboyd3HGwbkmmcfMwwL/bHZo7qyfHxe5fu7U+rm11j8the0EWthcfrVt/xXZrwgfGrVHXR8yypHZl7PnBD3o5V65u64/lvtPW1hdBWVfRt/bZzZUJ7G+I7c6bxgbmXx8XFTu2vX+fo9u464dR9cVqaIoZzxkYd6Rffxu3T197TOg4Bfr11xDrE6o/VLfvulgbs7jvWvMxjc1Zf42h+spYRU4j1f16hbpvf69ffV/tvknHcDWoceoafffgxl221D2kjlHLpWJZd1lsuBfVc6GF1kCT+rFuFoo4yG8znbsWs3XE9kb36hIAs+jGG2+sfD7ooIOS+77gBS+QFStWyPr160VE5PHHH5d169bJHnvsMZLa1q5dK0888UT5ecWKFbLXXnsl9z/44IMrn2+66aaR1AUAAAAAAAAAAAAAAAAAAAAAAACM06y+ZwcAAAAAAAAABvbII4/IXXfdVdm2atWqVjH0y41uvvnmoesKxdpzzz1b9ddj+elPfyqPPvro0HWN2iOPPCK/8zu/I/vtt59su+22svHGG8u2224rz3/+8+X1r3+9fOpTn6odJwAAAAAAAAAAAAAAAAAAAAAAAHTX3LQL6Lpcxv9WqTyzY84Qlo0h5ijGkw0YI5f2/bLIJKSOJ0vInQdyxXI0xQ711XMYWsehufbFrcdMzJ1Y4zB90ua/XczU9r7+um+sT6x9yr7Wc9aUIw/lMKPLEVv3wRpCufy1NeXSOWLHyVdTKG8sp+sXGmeS3FRylbHy8FyE2pX15v5YtfHo/XocvvHr2IH9vc+BAbgUKXM36IVuHBf/xfqaysBy0vNvvZNdtDFZpU/Z08W2LQ9UX273FbRuPZtl/jpdu0jo3nehb6MxRY68krNs4/Zb/0HWMa2pj7ecGzdXRR9rA5+j7cM1ufOoUfOuY+r6rW7v+Z6H2ujt7npiVH21deWbK5U3PWe1X238ffvbxjSh46Q+99cQ66NrCR63QG06XyWn+ia4729Tvb6am+qrxSpymkgu3yknV2NzbYIx1Djd+JrWgL5PN0XT0FyF58YzANe3PBX4z0R6bsJx4se6Fluvm8bWgbyB7aP8PavrTLVYL7Gpuj6+4RgZz99sTFvz/fxSsmbNmsrnbbfdVjbbbLNWMXbddVe5+uqry8+33nrrSGoTqde36667tuq/fPlyWbFihaxfv74Sc//99x9JfaPy4IMPyplnnlnZ9sADD8gDDzwgt9xyi5x//vnyJ3/yJ/LGN75RTj/9dNl+++2nVCkAAAAAAAAALH5GuvmkoItjAgAAAAAAAAAAaXj+AXQTLzwCAAAAAAAAOkq/VCfFdttt14mXzjz00EOVz4OMSfd5+OGHhympYlT19b/waJT1TdJTTz0lZ599tlx66aVy7rnnyq//+q9PuyQAAAAAAAAAAAAAAAAAAAAAAACMCS88GrNcrOSZnXYZFdkEc41y7NkIYuXSLkbWYrJSx5pFasgTcsZyhXI09QvNb57arkVsfRxSYzatgVD+UJ/4cYgfz7Zzodv7+sfGXJvLxDka6dyFtudNOfzvmGydo2mdNeRfqCEUM/z+y9Sxxo5TvX09p2sTihUbX9mvjNPLoWOnxirb5cZb20B0TF2LGkdll95W+xxpXwvYvNsXc+TtG2NN8oqdqsUacEtQz4nb7oZXhHTHy5q+cZd9i0bFvrKtipnlrnU1ufs62GLdiVkWLLtcN66PbuDWbhHbzo/+ONVqsFlgf+97bm1e2VeZx4Rcrr37nrucvvOVy6XPq8ZWj0/ZXsXuxanXqPPr7bqvvr4YW11wvvOAnpu2OZ3Q+Jv6hnK5WCbQztcv1ic0vuBx85zjY3WWOcR9J5uPta9/qL7QuNx9pJHA/pQcbuypNagzgRun797ClDH0dmkVS8/xQp3Vz7Z2GfSfo400r22f2HoP8dVd5k2KEJZSwaz99p4VbX8Ho3uOOeaY1n1OOeUUOfXUU0dey6Q99thjlc+bbrpp6xi6z6OPPjpUTf1mvb5h5Xkuq1evliOPPFJ+5Vd+RXbffXfZYost5IknnpC7775bvve978k555wj69atK/vcc8898trXvla+853vyAEHHDC94gEAAAAAAAAAAAAAAAAAAAAAADA2vPAIAAAAAAAAQOfoFwptsskmrWPoFwrpmMOY9fqG8Yd/+Idy7LHHysqVK737X/ziF8urX/1qOeWUU+RTn/qUvO9975Onn35aREQef/xxed3rXie33HKLbLbZZhOsGgAAAAAAAAAAAAAAAAAAAAAAAJPAC49mRDbtAgLyzI4tdjai2LkMHidrOfFt5iNLrCtPqCGWN5Qr1K9p7vNQjkAfnaM5dlrb1Ji+8cXWVdu5ahM7tW69PWUcsT6puUL52sYQEcny+JzlmRlNroa5D9UR/d60rM2Xq+15zJczVH94Lkyl3zDnUte3rCH3z0nZXrWr5M79sco2erset2rnnRe9TZ2wgmsydJ4NnfDatqm0H/6KnrXNOUG21R1L4tpUy67/OFpT5CvnpNhnhpvnSg73P+YDMd2aLYqwqp1b41a1q4yr2GTn3edinZvqfjELG6z1L4Iyl63X6s4Nuq/uE4rh5sTN+SC53HnXRGKnjae6fmr1Bvq6644JzGFKPbGcof39153aHASOg46l57CpX2qfUA26bl1zSh+nzF18o/S5QsfxzVWoTXB/kctIc//mGMV+VWc5TqnS4+znxhyaT3d5MPryp2LpOD698fj329rlMn4+NoFjFuL73ooM9/vRtwYrNQ0cOW58v3ox86yV2s1IF+gTwRS9+93vlk9/+tNjz3PKKafIqaeeGm2Xtf2LoAH7DGrW62vjve99b1K7ZcuWyXvf+15ZtWqVHHvssWKKe+K77rpLPvGJT8gHPvCBcZYJAAAAAAAAAJ1jRGQ+2mrx6eATHQAAAAAAAAAAkIjnH0A38cIjAAAAAAAAoKMuuugi2XPPPVv12W677cZUzWRtvvnmlc8bNmxoHUP30TGHMev1TdLrXvc6efe73y2f+tSnym1/93d/xwuPAAAAAAAAAAAAAAAAAAAAAAAAOogXHgEAAAAAAAAdteeee8ree+897TKmYtZfKDTr9U3aySefLH/9138t1loREfnZz34mN954o+yzzz5TrgwAAAAAAAAAAAAAAAAAAAAAAACjxAuPxiwr/kxCntkJZerJxpgzl+FjZy0nf5A5zBLrzCO1pOSO5QrFaDpOeShXoE9qjqbjV2ubGrNhHKF6Q3MWm++Utd12vlPGo9vE+gyTq22McnsemlMzuhwp34dgHaGY4fpiOXWu1LpDNfq2x+Y7Vr+odv05XOxyWx6ZC9Wu1fHQfWI5Vbva3PR/ViesWtvQeTZ0ogttr7RJu5BkKbFSJeachtRrnoiI1QfEuGMd6OCWSF83d4ytyVRfW2laVmVcP7fdBV3Y4L5GvlG4NWvdWjXLvGWmtvP3XehjbdqCKde4q9tm/v3SN0dqn9te1m39n2P9vG0D43HnYROJ3YtTPyL1XIF6A9v1Ncl45lyfQ2J1peZeyF/ta2LzrrbrOfT1S+2j6w/PWf04xGLpvmV79S1z5wNf7thYo/uLXGaYHHrcxT9DtZTt+o9HUYcba6ht6BTvTpG+86w+n4budXrj8+co43m6x357GfGv2TZ835VKDSP4banPO6lm98o7Gl0fH2bb0UcfLc95znPGnueQQw7xbt9yyy0rn3/+85+3jn3//fdXPm+11VatY4TMen2TttNOO8mLX/xiuf7668ttP/zhD3nhEQAAAAAAAAAAAAAAAAAAAAAAQMfwwiMAAAAAAAAAI/eqV71KXvWqV00t//Oe97zK55///OfyxBNPyGabbZYc44477miMOQwdS+eKeeKJJ+SBBx6obNtzzz2HrmuaVq5cWXnh0SAvgQIAAAAAAAAAAAAAAAAAAAAAAMBs44VHY5ZlVvLMTrsMr2wCdeUy+hxZNnjftsciG6D+PLG+lFpi+UMxYsc2b8qp+rbN4TvmwbZqe2q7pvbjmrOmvrEYsTn1jqNln7Zz19QnuD0P5TCjy9Ey90L+2HHx1zdIrtS6dYxQDd6+xedQHW572a+h3ljOMkduvLXonLV2g+R2fVSM2njLnA0xyg2qge4TOuklnLCzphNmy1gTiTELjDum1c1WHygTPsa9NlLEWmhrjYrh1k+ZozAfmcu8953MigJ07HKdGRVbqbVzsc2y5hoaYy3EsNY/Oe47aG19nO584/rqudN9Q7F8cx5sm/nr1edn43Kq73DtuDbmat5exlT7fdcqo+qN1ZWa25/fFjmb530QOpbOFWoXm7OUWLpvsH3xDXLngf7+ob6xcdX2FznMADlq7YrPbtXExluJqc4Wbsyh74OjLwGmr3no/lafV0P3RPXj521Wja0vy4m/i0ztotyfd7Dfhm2+J7P623vaJvHbf7GyEr5PX8y6Oq5BbLHFFrLzzjvL3XffXW677bbbZN99902Ocfvtt1c+v/CFLxxZfTrWbbfd1qq/bv+c5zxHnv3sZw9d1zRtuummlc8bNmyYUiUAAAAAAAAAsDjNS/3/StMF89MuAAAAAAAAAAAATA3PP4BuWhQvPLr99tvlBz/4gdx9993y2GOPyU477SS77babHHzwXaiX0QAAQ21JREFUwbLRRhtNuzwAAAAAAAAAM2ifffapvPDoiiuuSH7h0Y9//GN54IEHys+bbbaZ7L777iOrbY899pDNNttMnnjiCREReeCBB+SWW26RvfbaK6n/f/7nf1Y+77PPPiOrbVp+8YtfVD5vu+22U6oEmByefwAAAAAAAAAAgK7h+QcAAAAAAAAAIGamX3h0/vnnyxlnnCFXXHGFd/+KFSvk+OOPlw9/+MOd+ZdfssxOuwSvXMZXVzai1+nlQ8xdNsD48sS6Y3Wl5I7GiOzPB+gbyqnbh9ZGU006dmoNTTFD85g6jtR+KTFGMZ7UnCOZu9A48tgcmfYx225vqCF8bMN1DZortW4dQ9cyWA4T7etr1x+v7JtH5ka1i63RSk26j47lPqv9tXGV/Tw5ysTqsz7JhU56gRN31nSSTD3ZD9p+XDFGzbjjMnht+jxtawey2N+/TN2xKba59VD2DS3pzK0vF9k1LDbMe8bh1qSpVNO3v1jLRQzri9FfQv/aVenFLGyweV7NaUNrtDo3rl3lO+rGaty+IofNKzF6+603lq6hfxy6b7ndVnP2tle/XO68bRJylPuCufzbY/2q9ZiiHv9JQM9Zm9ih+dRzEDsOofa+2LG+sdpC2/tjObGYofbuPND//U8dc/L+Iofx5CjHo8YXjlXsV+19cxUbs+PGHptT3+nWVLsE73/1+TV0z6RzVmJHTvc2cIswyO9GE3m/+jh/I4fOuwC65dWvfrV87WtfKz9fdtll8s53vjOp72WXXVb5fOSRR0qeN/2AaGfZsmXyyle+Ur785S9Xcqa+8EjX95rXvGZktU3D/Py8XHPNNZVtO++885SqAcZvKT7/AAAAAAAAALpull/wce2118qtt94qd911l4iI7LLLLrLXXnvJ/vvvP9W6AHQLzz8AAAAAAACA7pnl5x9Y3GbyhUePPfaYvOMd75Bzzz23sd369evl7/7u7+SCCy6Qz372s3LkkUdOqEIAAAAAAAAAs+7YY4+Vk046qfx80UUXyUMPPSRbbbVVtO/ZZ59dizVqxx57bOWFR//0T/+U9EKmBx98sNJPROSYY44ZdXkTdfHFF8uDDz5Yfp6bm5NDDjlkihUB48HzDwAAAAAAAKB7ZvUFH08//bR8/OMfl7POOktuu+02b5s999xTTjzxRDnppJP4lxIADIznHwAAAAAAAED3zOrzD3TH6P6T5CMyPz8vxx9/fO0vu7fbbjs54ogj5PWvf70ccMABkmVZue++++6To48+Wr773e9OutyoLLOt/oxCLnbkf9LH2/5PdDyZTfrjrUds0p96zvif1DpjNaXEqc9z87rJ1Z9Qv/6+oZzB9oG14aspFrttu3K7Zx5T5y6kaT1F6wkdj8Tx+Pal1DOqXOX23EqW+8ZhKn90HN86ia3V9jU0HVsjWV9dyTUEcjXWp8epYuha6vsHq6eSO1/4Exp3U98yR25EchOvRbcrcjdyfVyMrPgT6uu2Fycu7zxkxR99kqv98Z+4s1y8f5JO9ikXhlH9CRU6jT9txh9qq7eX66uaoten71hqxRrorU3Xtvg+lNuLP27d6TD938FirdbXpFrDgRixdoNwsWLntaa+vbZp56NBYsf6hOpvvHfrO47tcqVdb/zXaf/1rW1Nba6Dset1yv1A6jWq7f1kym+UtvdZtfF67uHazlF0f8NvmvaxRnhvHfgtkPI7J/V3SfpvnsF+Yy2MPe1Pikn9VvWPo34Mu/oHIabDf+CsXLlSDj300PLzhg0b5JOf/GS03+WXXy5XXnll+XmrrbaS173udSOv75hjjpEtttii/Pz9739fLr/88mi/T33qU7Jhw4by8yte8QrZddddR17fpDz++ONy8sknV7YddthhlbkBuqBrzz8AAAAAAAAwe6xM/ynFOP7M6hOvxx57TN7whjfI61//+uD/2V+k94KPffbZRy699NKJ1HbrrbfKgQceKO9///uDLzsSEVmzZo2cfPLJctBBB8maNWsmUhuAbuH5BwAAAAAAAMaN5x+TNcvPP9AtM/fCo5NPPlm++tWvlp832mgj+eu//mv52c9+Jpdeeqn8y7/8i1xzzTVy4403ykEHHVS2e/LJJ+WYY46Re+65ZxplAwAAAAAAAJhBH/3oR2ufr7766mD79evXy9vf/vbKtve9732y5ZZbNuZZt26dZFlW+bNu3brGPltttZX88R//cWXbiSeeKA8++GCwz1VXXVUb05//+Z835pmUX/ziF3LOOefI/Px8cp9HH31UXv/618tNN91U2X7KKaeMujxg6nj+AQAAAAAAAHTHLL/g495775VXvepVcu2111a277nnnnL00UfL6173Olm1alVl3zXXXCNHHHGE3H///WOtDUD38PwDAAAAAAAA6I5Zfv6B7pmpFx6tXbu29l9YP++88+Td7363bLzxxpXtL3rRi+Sb3/xm5S+9H3jgATnttNMmUisAAAAAAACA2XfIIYfIcccdV35+6qmn5PDDD5dzzz1XjDGVtldeeaUcfPDBlf/S8apVq+QP/uAPxlbfSSedJCtXriw/r1mzRg4++GC56qqrKu2MMfKFL3xBDj/8cHnqqafK7W94wxsqf0fa5N5775V169bV/vzsZz+rtfW1W7dundx7773B+I899pi8+c1vlr322ktOPfVU+eEPf1ibY+eXv/yl/PM//7Pst99+cvHFF1f2ve1tb5NDDjkkaUzAYsHzDwAAAAAAAKBbZvUFH8YYOeaYY+SOO+4ot+20005y6aWXyq233ioXXXSRfOlLX5I1a9bIxRdfLDvuuGPZ7vbbb5djjz1WrJ3V/6Y0gFnD8w8AAAAAAACgW2b1+Qe6KbMz9FTqLW95i/zzP/9z+fmtb32r/NM//VNjn1tuuUX23Xff8l/ymZubk5/85Ceyxx57jLXWkJtuukn22Wef8vPpex4tz9lk66nUMoi+F6mNTZ6NZsllMnicvOU4B6k5Vl+bmFmkbejNZaF+TbmDfQLjaZMjtW2oXdOchsYUnbvI/lj/pjbJ4/Jsj/XVn1NzNdaRh8bh/xcTB8rROnfK/LerL5RrFHOma6nvD38vQrGCMYrtup03Xm78+/T2ufnmdsuMd7uvrrKP+6zrznV78Y5X+s/X+mRX+1w9uWeR/dHtbds0Jh9A25yjZFpc96z/OxiMFfjsDePaun3un9alztT+TG0v/mnV9meW9cq3rm1eaWPnc/92F6uIEexv+45fKEb52eXy7+/V2pBDj8d9DsQI7w+vu1jslBi+GvqZQN9QrljOWC0pbUxDvSLNtaXWpcddm2tPnFif2PbgXA8xZ6kxfe2sNI8ndbze2DpWy1yxnAsx09s25pD4/If61tslNWud359r9Nes2fmbimZ6Pc0S9xvuZ798UN635kvl9htvvFH23nvvaZU1VfrvbETmJBvFfduMsdaIyDPl56V8zPs9/vjjcuihh8p1111X2f7c5z5XfuVXfkU23nhjueWWW+TGG2+s7N96663liiuukOc///nRHOvWrZPdd9+9su3222+vvMwo5Oabb5aDDz5YHn744cr2fffdV/baay/55S9/Kddff33txUSrV6+Wyy+/XDbbbLNoDhGRww47TC6//PKktiEvf/nL5bLLLvPu883B8uXLZZ999pHtt99etthiC9mwYYPcc889cu2118qTTz5Zi3HUUUfJRRddJHNzc0PVCcyaLj7/2FRm7L+qAQAAAAAAlhQjIhv6Pi/Vvw/Xf2fzbBFZFm6+aM2LyKN9n6d9vNeuXSsveMEL5Omnny63XXTRRXL00Ud722/YsEEOP/xwueKKK8pt73rXu+Tv//7vR17bOeecI29+85vLzytWrJBrrrkm+Lzi9ttvl1/91V+VBx98sNz2hS98QX77t3975LUB6B6efwAAAAAAAIwWzz8W8PxjOmb5+Qe6aWb+LnbDhg1y/vnnV7a9733vi/bba6+95Jhjjik/P/PMM/L5z39+1OUBAAAAAAAAWKSWL18uX/3qV+Xwww+vbP/pT38qX/nKV+SCCy6ovexo1apVcumllya97GhYL3rRi+TSSy+t/Z94b7jhBvnXf/1X+fd///fay45e+cpXyr//+78nv+xoWh5//HG58sor5d/+7d/k//2//ycXXHCBXHHFFbWXHW200UbykY98RL70pS/xsiN0Ds8/AAAAAAAAgG457bTTKv9n/7e+9a3B/7O/iMimm24qZ599tmy88cbltn/8x3+UtWvXjrSu+fl5OeWUUyrbzjjjjMb/OMPuu+8uZ5xxRmXbhz70ITEm8h9BA7Dk8fwDAAAAAAAA6JZZff6B7pqZFx5deuml8sQTT5SfDzroIHnBC16Q1PeEE06ofL7gggtGWtusybLx/UmVZ3bgP9HxiU3601xf85+24xqk3ljsWpzMBv+UMQJ/QjFCNXjHEeojVnLPeGq1NYwvtZ5Qu5Q5TR5Py+Pg03aeg+PybI/1Dc17rL2v3iy3lT+9mKbyJ6VunSu5j8odGlcvjqn9CY01NVfKsY7FqtVS2x8+LqExBmvJF/6Uc+COYcqadXXlRiQ39e2aatcod7UVfVzMrPjj6i5rcH9EJPccn6z44zvZlduqJ/csl8qf4Mm/6aIQu3DoJKE/sXgpf2pznI//zyB1p85B5LPuVq1DrYViffTWk9sXWGe1uez/LkXOW8Waru3X371Au0HEvps6d9N1Ytjzla+uWh2BGG3PcyIN5/9ArljdTdfB1Hqj18XQOTShrl6Odtf/lD6x7W3uSUcVU7er1K/u91LnKNbOG0vd36be1zZ9Pwa9Lw/NQ9Pvi9TfLG1/C8XyN/0GG+XvwbKWKfzOHYRbT7P4B4ms7d4fBO24447y9a9/Xf7+7/9e9t1332C7nXbaSd73vvfJ9ddfLy95yUsmVt9LX/pSuf766+V973uf7LTTTsF2++67r5x55pnyta99TbbffvuJ1Zdiu+22kw9/+MNy+OGHy7Of/eykPs997nPl5JNPljVr1sgHP/hBWbasi//tDSx1PP8AAAAAAADApMx3+M+smOUXfHz3u9+V22+/vfy8yy67yJve9KZov//5P/+n7LLLLuXn2267Tb73ve+NtDYA3cPzDwAAAAAAAEzKtJ9R8PwjjBecYxgz85/KvuSSSyqfDzvssOS+hx56qMzNzckzzzwjIiLXXXed3HfffbLDDjuMskQAAAAAAAAAi1iWZfKud71L3vWud8nNN98sN954o9x9993y1FNPyc477yx77LGHHHjggZLn+m2ccStXrhQ75EunNt98c/nYxz4mH/3oR+X73/++rF27Vu6++27ZeOONZeedd5Z99tlHXvSiFw0c/7LLLhuqvpjly5fLn/7pn8qf/umfirVWbr/9drn11lvlrrvukgcffFA2bNggG2+8sWy99day/fbby+rVqyv/AgXQVTz/AAAAAAAAALpj2Bd8/Mu//Ev5+YILLpAPfehDI6vtwgsvrHx+85vfnPQfGli2bJm86U1vktNPP71S2yGHHDKy2gB0D88/AAAAAAAAgO6Y5ecf6K6ZeeHRjTfeWPl80EEHJfddvny57LvvvnLdddeV22666aaZ+AvvLFv4MwvybLh/4WoQmYwuZz6CeWw7B4PUn5ojS2gX+1frYjFCtTT1ywNjDvVpkyO1bWjem+a2bX2xfqn7m3KE+urtvv61OYn0ibWv7MtDc2X87QdYR9G+wRpCufy1DZIj2q/F3Om66vvjx8X1CcUq+0TGU6shD89ZTdHW5arNXa62q/b+Qqp19/q6f4b6SXW/r526IGS6jb5gxD5764icgdtelAb4l6XHGtsE1kebWC6GngtTHGM9h6E16doXcfqvAVZ30eUZl2qhj3ULSPcr13YRt7+BWVapLyuSWKPXma3EDq1+3U5ExKrY5T43nmIurZt/419f7jxhbf04ue+jtZn3c0w5hybeP7RPx4i2bxiPvh6YlrmcNvWn7nfXS6Pq9l139HzqunQuN24T2N8fJ9Ynlqs3nua5buobyqVraopTq1+q3+fQHOl6m9oF2xS5jDTPXWyOqzGr3Nc9tjaTjoM685TnvECMUKyUS5jxh0r+faRrq+Yf7jeib25iZuU3+bQs9fEDTV70ohcN9fKgccrzXA4++GA5+OCDp13KwLIskz322EP22GOPaZcCTF1Xn38AAAAAAAAAS9Esv+BjmNoOO+ywyguPLr74YjnjjDNGUheAbuL5BwAAAAAAANAds/z8A901xn/7vp0f/ehHlc977rlnq/6rVq2qfL755puHrgkAAAAAAAAAAGAYPP8AAAAAAAAAumMUL/jod9NNN42krieffFLWrFlT2XbggQcm99f/EYZbb71VnnrqqZHUBqCbeP4BAAAAAAAAdMesPv9At81NuwARkfXr18v69esr23bddddWMXT7W2+9dei6JinP7LRL8Mpk/HXl2bD9h69xkHGm5s0S26W8fSwWK1RTU788Mnbdd6AciX1Cx2GcOdvuHyZHylzW5iTSJ9a+3J7Xt+eZ8baNxhpgbn35F2oIxWqurbGOQK5R1q3r0+1ixyUUN7avP7dr52KX/XJTyxnaF8zliVGtwcXr259X60pVtnfXglz9s1JXVvTxb0/+3EseLzB2kcoHeHfkIH1GbdAaTN/aD8ZQ31/jjnHR3hb7E24A3HXBugViEteX+34UH8te856crq1RbUPby/3FmneLNdRuECq3lFNXfAfcjr5j0NtXzJmtjtV912oxXH+bB9rVR+Rip+Yqt6tYvTj1c76rx3HXCxPIVfYL5NQ1NtWTul9fR42tfydicxGaSz1e3zhifWK5wuPq7Q/FCvUN1aTj9ccJjll9/0O5U+Yq1kbfDxtpPi56PI31SZWJtNc1h/KJhO+dy3NmQ6xY7NgpOnY6HuT3la47ZJy/nfU8YCmwE/hbj2no5qgAYLHi+QcAAAAAAAAmyUrt/7nRCbP09GMUL/i47rrrys8333yz/MZv/MbQdf3kJz+R+fn58vP2228vW2yxRXL/LbbYQrbddlv5xS9+ISIi8/Pzcsstt8g+++wzdG0AuofnHwAAAAAAAJgknn+M36w+/0C3zcC/cS/y0EMPVT5vttlmsnz58lYxtt9++8rnhx9+eNiyAAAAAAAAAAAABsbzDwAAAAAAAKA7ZvkFH2vWrGnMk4KXjwBIxfMPAAAAAAAAoDtm+fkHum1u2gWIiDz22GOVz5tuumnrGLrPo48+OlRNIiL333+//PznP2/VRz8wBAAAAAAAAAAASxPPPwAAAAAAAIDumOUXfOjadJ4UvHwEQCqefwAAAAAAAADdMcvPP9BtM/nCo0022aR1DP0X3jrmIP72b/9WTjvttKFi3PfUw5Jlduha2sgmmq0qH0PyTIafv7Yx8gGOWepxbjNFWaRxqM5YLU3zEdoXzpWeO7QvPWc4dmjtReeixbEeNJYen67V2y/TfaqfdR+dI8ub6jSB7WNYT4E6wrna1Rbbt7A/tCNQQ970/ajWF2rrctZq8+R0Y9axyrqLPnpuyvbl/mJHbmq5y/9d9MmXFbFcTBfDHS9X0zJVW5mrGq+/reh9+nOuairrlqpye+8A1ua7HLRVbQOfa4thPrC9T97wZeqPEW3n6eNM88Id0+ZyaPzf39p2a/2fjfqnp411oVwb19Rtd5ttVt0+n6t+vUm3pjge88uKvqbSxj6TF9uLdjZXtRT9TNHePlNs78vhYpV1uZiuQaZi5NXtesp0u8pOlauswR+j1l19GUPt+nPVNgfWja4ptn1hX/N3KxgzsBwrbSJfvqa6UvYvtNHzmVaLjm18uWrHWPdJyxXq36ZN2+3+2prHnFq3L6dek8mxI8elKWYodz1HXOzYhHLXc7W/4AzSp19oHY7LhNMtGvc99Ujl85NPPjmlSmZRV1dNV8cFAItTl59/JPz0AgAAAAAAGBv9dxM8A1nQ1b+z0eMa5OUU22233UAvAeo3qy/4EJnt2gB0D88/AAAAAAAAxoPnH35d/Tsbnn9gqZuJFx5pWewtLyPqMwkfv+OyaZcAAAAAAAAgIiI//elP5YADDph2GTOiq3/lDQCYZV16/sEjdAAAAAAAMEt4BrJgqfydzTHHHNO6zymnnCKnnnrqUHln9QUfvjizVBuA7uP5BwAAAAAAwHjw/GPBUvk7G55/YKnJp12AiMjmm29e+bxhw4bWMXQfHRMAAAAAAAAAAGCSeP4BAAAAAAAAdNcsv+BjlmsDsPjx/AMAAAAAAADoLp4xYFLmpl2AyOz+hffv/d7vyetf//pWfb71rW/JH/zBHwydGwAAAAAAAAAALG48/wAAAAAAAAC6Y1b/vs8XZ5ZqA9A9s3rO4fkHAAAAAAAA0N6s/n0fum8mXni05ZZbVj4/8cQT8vjjj8vy5cuTY9x///2Vz1tttdXQdW2//fay/fbbt+qzyy67iIhU/tL7oosukj333HPoeoBRWbNmjRxzzDHlZ9YoZhHrFLOONYrFgHWKWccaxazrwhp98skn5ac//Wn5+eUvf/kUq5muVatWyY033jjtMiZu1apV0y4BAJY0nn8Ak9WFe3h0H+sUs441isWAdYpZxxrFYtCFdcozkAVL7fnHAw88IA8++KA897nPlWc961mt+m633XZD55/l/8P/LNcGoHt4/gFMVhfu39F9rFPMOtYoFgPWKWYdaxSLQRfWKc8/FvD8I13Xn3+g22bihUfbbLONbL311vLggw+W2+6880554QtfmBzjjjvuqHx+3vOeN7L62thqq63kN37jNyrb9txzT9l7772nUg+QgjWKxYB1ilnHGsViwDrFrGONYtYt1jV6wAEHTLuEmbDJJpssyuMHAFjceP4BTBdrFIsB6xSzjjWKxYB1ilnHGsVisFjXKc9AeP4xabP6gg+Rem0///nPW8cYV20AuofnH8B0sUaxGLBOMetYo1gMWKeYdaxRLAaLdZ3y/IPnH5M2y88/0G35tAtw9F9ur1mzplX/tWvXNsYDAAAAAAAAAACYNJ5/AAAAAAAAAN3gXvDR784772wVY1wv+NBxdJ4Us/LyEQCLA88/AAAAAAAAgG6Y5ecf6LaZeeHRPvvsU/l8xRVXJPd9/PHH5Yc//GFjPAAAAAAAAAAAgEnj+QcAAAAAAADQHbP6go/nP//5smzZsvLz/fffL48++mhy/0ceeUR+8YtflJ+XLVvGv4wAoBHPPwAAAAAAAIDumNXnH+i2mXnh0atf/erK58suuyy573/8x3/IM888U37ef//9ZYcddhhVaQAAAAAAAAAAAAPh+QcAAAAAAADQHbP6go9nPetZsmrVqoFr+973vlf5/LznPU+e9axnjaQ2AN3E8w8AAAAAAACgO2b1+Qe6bWZeeHTkkUfKpptuWn6+4oor5Mc//nFS37PPPrvy+dhjjx1laQAAAAAAAAAAAAPh+QcAAAAAAADQHbP8go9hatNtX/Oa14ygIgBdxvMPAAAAAAAAoDtm+fkHumtmXni02WabyXHHHVfZdvrpp0f73XLLLXLhhReWn+fm5uSNb3zjyOsDAAAAAAAAAABoi+cfAAAAAAAAQHfM8gs+dLxzzjlH5ufno/3m5+flc5/73FhrA9A9PP8AAAAAAAAAumOWn3+gu2bmhUciIqeeeqpstNFG5eezzz5bvvzlLwfb//KXv5QTTjhBnnrqqXLb29/+dlm1atVY6wQAAAAAAAAAAEjF8w8AAAAAAACgG2b5BR+HHnqo7L777uXnn/3sZ7UXGfl87nOfk7vuuqv8vGrVKnnZy1420toAdBPPPwAAAAAAAIBumOXnH+iumXrh0R577CF/+Id/WNl23HHHyd/8zd9U/lJbRORHP/qRHH744fK9732v3LbNNtvIKaecMpFaAQAAAAAAAAAAUvD8AwAAAAAAAOiOSb3gI8uyyp/LLrussf2yZcvktNNOq2w76aSTZN26dcE+69atk/e+972VbR/5yEckz2fq/2IOYEbx/AMAAAAAAADoDl5wjkmbuadRH/vYx+Q1r3lN+fnpp5+W97znPfLc5z5XXvOa18hv/dZvyerVq2Xvvfeu/GX3xhtvLBdeeKHstNNO0ygbAAAAAAAAAAAgiOcfAAAAAAAAQDfM8gs+/sf/+B/y0pe+tPy8fv16Ofjgg+VrX/tare2ll14qBx10kDz44IPltoMPPliOP/74sdQGoJt4/gEAAAAAAAB0wyw//0A3zU27AG3ZsmXyL//yL3LiiSfKF7/4xXL7/fffL5dccom3z/bbby+f/exn5dBDD51UmQAAAAAAAAAAAMl4/gEAAAAAAAB0x8c+9jG56aab5OKLLxaR3gs+/uzP/kwOOOAAefazny1r166Va6+9Vqy1Zb9xv+Ajz3O58MIL5cADD5Q777xTRETuueceOfLII+V5z3ue7L333mKtlZtuuknWrFlT6bty5Uq54IILJMuysdQGoJt4/gEAAAAAAAB0x6w+/0A35dMuwGfzzTeXc889V8477zw58MADg+1WrFghv/u7vys33nijvPrVr55ghQAAAAAAAAAAAO3w/AMAAAAAAADoBveCj+OPP76y3b3g47zzzpNrrrmm8n/233777eVLX/rS2F/wsdNOO8nXv/512X///Svbb731VrnooovkS1/6Uu1lRwcccIB8/etflx122GGstQHoJp5/AAAAAAAAAN0wy88/0D1z0y6gyXHHHSfHHXec3H777XLttdfK3XffLY8//rjsuOOOsttuu8nLXvYy2XjjjaddJgAAAAAAAAAAQDKefwAAAAAAAACLn3vBx3HHHScf//jH5fvf/7633YoVK+T444+X0047TbbbbruJ1LbXXnvJlVdeKR//+MflH/7hH2Tt2rXedqtWrZITTzxR/uiP/kg22mijidQGoLt4/gEAAAAAAAAsfrP8/APdMtMvPHJ233132X333addRrLttttOTjnllMpnYJawRrEYsE4x61ijWAxYp5h1rFHMOtYoAAAYN55/AKPFGsViwDrFrGONYjFgnWLWsUaxGLBOgdEa1ws++v/ryIPYaKON5OSTT5aTTz5ZrrnmGrnlllvk7rvvFhGRnXfeWfbaay/51V/91aFyAIAPzz+A0WKNYjFgnWLWsUaxGLBOMetYo1gMWKfAaPGCc4xbZod9GgYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRT7sAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADQfbzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjB0vPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGPHC48AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMDY8cIjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwdrzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjB0vPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGPHC48AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMDY8cIjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwdrzwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjB0vPAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGPHC48AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMDYzU27gK65/fbb5Qc/+IHcfffd8thjj8lOO+0ku+22mxx88MGy0UYbTbs8AAAwoA0bNsiPf/xjueOOO+Tuu++WRx99VJ5++mnZYostZJtttpF99tlH9t57b5mb4/YKAICu+fGPfyzXX3+9/OxnP5MNGzbIJptsIttvv73sueee8iu/8iuyfPnyaZcIAAAwdjz/AACgm3j+AQDA0sXzDwAAgAU8AwEAoHt4/gEAwNLF8w8AWDz4RTYi559/vpxxxhlyxRVXePevWLFCjj/+ePnwhz8s22677YSrA4DZNj8/L2vWrJGbb75Z7r77bnn44YflWc96lmy99dayatUqWb16NT8iMBX/9E//JN/61rfkyiuvlNtuu02MMY3tN998c/mt3/otec973iP77bffZIoEAABj8dBDD8knP/lJ+cxnPiN33nlnsN2yZctkv/32k+OOO05OPvnkCVYIAAAwGTz/AIDh8AwEs4jnHwAALF08/wAAAOjhGQgADI7nH5hFPP8AAGDp4vkHACxOmbXWTruIxeyxxx6Td7zjHXLuuecmtd9hhx3ks5/9rBx55JFjrgzoWbt2rVx11VVy9dVXy1VXXSXXXnutPProo+X+3XbbTdatWze9ArEk3XnnnXLBBRfIN77xDfmP//gPeeSRR4Jtly1bJq961avk3e9+txx11FETrBJL3XOe8xy56667WvdbtmyZvOc975G//Mu/5I3/mAm//du/LV/84hcr27j+Y1JOPfVUOe200wbu/5a3vEXOPvvs0RUEJDjvvPPkd3/3d+WBBx5I7rPDDjvIvffeO8aqAAAAJovnH1gseAaCWcQzEMw6nn+gK3j+gWni+QcWI55/AAAALOAZCBYDnn9gFvH8A7OO5x/oEp6BYFp4/oHFiOcfALB48QtsCPPz83L88cfLV7/61cr27bbbTvbff3/Zcsst5bbbbpPrrrtO3Hul7rvvPjn66KPlG9/4hhxyyCHTKBtLxGWXXSZ/8Rd/IVdffbWsX79+2uUAFW984xvlC1/4QnL7+fl5ueSSS+SSSy6R//bf/pucddZZssMOO4yxQsBvs802k1WrVsmuu+4qW2yxhRhjZP369XLDDTdUfuDOz8/LJz7xCVm3bp2cf/75smzZsilWjaXuy1/+cu0vugEAYaeddpqceuqpte277rqr7LXXXrLddtvJL3/5S7nnnnvkhhtukMcff3zyRQIAAIwZzz8w63gGglnGMxAsRjz/wGLE8w8AaIfnHwAAAAt4BoJZxvMPzDKef2Ax4vkHFiuegQBAOp5/AMDixguPhnDyySdX/qJ7o402kjPOOEPe+c53ysYbb1xuv/nmm+XEE0+UK664QkREnnzySTnmmGPkhhtukJ122mnidWNp+MEPfiBf+9rXpl0G4HXLLbd4t++yyy7yvOc9T3bYYQd55plnZO3atXL99deLMaZs85WvfEV+/dd/XS6//HLZcccdJ1Uylqjly5fL6173OnnNa14jBx98sOyzzz6S57m37fe//3350Ic+JN/85jfLbRdddJGcccYZ8id/8ieTKhmoeOihh+R3f/d3p10GACwaH//4x2t/2f2GN7xB3v/+98u+++5ba2+MkSuuuEL+9V//VS699NIJVQkAADB+PP/ArOMZCGYZz0CwGPD8A4sdzz8AoB2efwAAAPTwDASzjOcfmGU8/8BiwPMPdAHPQAAgHc8/AGDxy6x77TxaWbt2rbzgBS+Qp59+utx20UUXydFHH+1tv2HDBjn88MPLv/AWEXnXu94lf//3fz/2WrE0feITn5D3vve9te3Petaz5DnPeY7cdttt5bbddttN1q1bN8HqsNStXr1arrnmGhER2X///eVtb3ubvOY1r5FVq1bV2t51113y4Q9/WP7v//2/le2HHHKIfOc735EsyyZSM5amp59+WjbaaKPk9sYYectb3iKf+9znym1bbrml3HffffKsZz1rHCUCjd7+9rfLZz7zGRERefazny2PPvpouY/rPybl1FNPldNOO638/IUvfEEOPPDA5P6bb765bLvttuMoDai4/vrrZfXq1fLMM8+IyML/oe3zn/+8HHfccUn9n3nmGZmb473SAABg8eP5BxYDnoFglvEMBIsBzz+w2PH8A7OA5x9YLHj+AQAA0MMzEMw6nn9glvH8A4sBzz/QBTwDwbTx/AOLBc8/AKAb/K+oRdRpp51W+Yvut771rcG/6BYR2XTTTeXss8+uvPX/H//xH2Xt2rVjrRNL20YbbST77befnHjiiXLmmWfKNddcI48++qicddZZ0y4NS1yWZXLUUUfJVVddJddee628+93v9v5Ft8jCG//PPPNM+fSnP13Z/t3vfle++MUvTqJcLGFt/rJbRCTPc/n0pz8ty5cvL7c9/PDD8u1vf3vUpQFR3/jGN8q/6J6bm5MPf/jDU64IWLDjjjvKypUrk//wl92YhGeeeUbe9ra3lX/ZLSJy5plnJv9lt4jwl90AAKAzeP6BxYJnIJhVPAPBYsDzDyxmPP/ArOL5B2YRzz8AAACqeAaCxYDnH5hVPP/AYsDzDyx2PAPBLOL5B2YRzz8AoDt44dEANmzYIOeff35l2/ve975ov7322kuOOeaY8vMzzzwjn//850ddHiAiIm95y1vkkUcekeuuu07+4R/+Qd75znfKAQcc0Povb4BxOO+88+QrX/mKrF69OrnP7/3e78lv/uZvVradc845oy4NGNoWW2whhxxySGXbmjVrplQNlqrHH39c3vGOd5SfTzrpJNlvv/2mVxAAzLjzzjtPrr322vLz4YcfLieccMIUKwIAAJgOnn9gseAZCGYZz0DQVTz/wCzg+QcAtMPzDwAAgB6egWAx4PkHZhnPP9BVPP/ArOAZCACk4/kHAHQHLzwawKWXXipPPPFE+fmggw6SF7zgBUl99QXzggsuGGltgLP11lvLJptsMu0yAK+VK1cO1O/3f//3K595azpm1YoVKyqfH3300SlVgqXq/e9/v6xbt05ERPbYYw859dRTp1oPAMy6M888s/L5Ax/4wJQqAQAAmC6ef2Cx4BkIZhnPQNBlPP/AtPH8AwDa4fkHAABAD89AsBjw/AOzjOcf6DKef2AW8AwEANLx/AMAuoMXHg3gkksuqXw+7LDDkvseeuihMjc3V36+7rrr5L777htVaQDQafvvv3/l84YNG+Shhx6aTjFAgzvuuKPyeeedd55SJViKvve978mnP/3p8vOZZ54pm2666RQrAoDZtmbNGrn88svLzytXrpRXvOIVU6wIAABgenj+AQDTwzMQLAY8/8A08fwDANrh+QcAAEAVz0AAYDp4/oHFgOcfmDaegQBAOp5/AEC38MKjAdx4442VzwcddFBy3+XLl8u+++5b2XbTTTeNpC4A6Lr+h4XOU089NYVKgLBbbrlFrrzyyvJzlmXy8pe/fIoVYSl58skn5W1ve5sYY0RE5C1veYu88pWvnHJVADDb9H8x6vDDD5csy6ZUDQAAwHTx/AMApodnIJh1PP/ANPH8AwDa4/kHAABAFc9AAGA6eP6BWcfzD0wbz0AAoB2efwBAt/DCowH86Ec/qnzec889W/VftWpV5fPNN988dE0AsBSsWbOm8nlubk623XbbKVUD1N1zzz3y+te/Xubn58ttxx13nKxcuXJ6RWFJOfXUU+UnP/mJiIhst9128vGPf3zKFQHA7Puv//qvymf3f2iz1so3vvENOeGEE+RFL3qRbLnllrJ8+XLZbbfd5JWvfKV87GMfk3Xr1k2hYgAAgPHh+QcATA/PQDDLeP6BaeP5BwC0x/MPAACAKp6BAMB08PwDs4znH5gFPAMBgHZ4/gEA3VJ/TTIarV+/XtavX1/Ztuuuu7aKodvfeuutQ9cFAEvB+eefX/m8evVqyXPe3YfpeeaZZ+TBBx+UH/3oR/KVr3xFzjzzTHnkkUfK/XvssYf8zd/8zRQrxFJy7bXXyl/91V+Vnz/xiU/INttsM8WKAL8zzzxTPvKRj8iPfvQjeeCBB2SjjTaSbbbZRnbbbTc55JBD5NWvfrUceuih0y4TS8jVV19d+fzCF75Q1q1bJ29/+9vlW9/6Vq39nXfeKXfeead885vflP/9v/+3vOMd75C//Mu/lM0222xSJQMAAIwFzz8AYLp4BoJZwvMPzBKef2Cx4PkHZg3PPwAAAHp4BgIA08PzD8wSnn9g1vAMBIsBzz8wa3j+AQDdwguPWnrooYcqnzfbbDNZvnx5qxjbb7995fPDDz88bFkA0HmPPfaY/OM//mNl27HHHjularBU/a//9b/kk5/8ZFLbV7ziFXLOOefUrvvAODzzzDPytre9TZ555hkREXn1q18tb3zjG6dcFeB37rnnVj4/+eST8thjj8kdd9wh3/nOd+SjH/2orF69Wv7iL/5CXvnKV06pSiwl99xzT+XzE088IS95yUvkF7/4RbTv008/LX/7t38rV1xxhfz7v/+77LTTTuMqEwAAYOx4/gEA08MzEEwbzz8wq3j+gcWE5x+YNTz/AAAA6OEZCABMB88/MG08/8As4xkIFguef2DW8PwDALqFVyK39Nhjj1U+b7rppq1j6D6PPvroUDUBwFLw/ve/X+69997y81ZbbSUnnnjiFCsC/F73utfJpZdeKt/61rdkl112mXY5WCI+9rGPyfXXXy8iIsuXL5e/+7u/m3JFwHCuvvpqOeKII+SDH/ygWGunXQ46Tv+f2k444YTyL7uXL18uf/RHfyTf+MY35Mc//rFcc8018pnPfEYOOeSQSp/rrrtOfvM3f1OefvrpSZUNAAAwcjz/AIDp4RkIFgOef2AaeP6BruH5ByaJ5x8AAAA9PAMBgOng+QcWA55/YFp4BoIu4fkHJonnHwDQLXPTLmCx0X/Zvckmm7SOof+yW8cEAFRdeOGF8jd/8zeVbX/+538uK1asmFJFQNjFF18s8/Pzsskmm8iv//qvT7scLAE333yzfOQjHyk//9mf/ZmsXLlyegUBAbvssou89rWvlV/7tV+TF77whbJixQrJ81weeOABufbaa+UrX/mKXHrppWV7a6189KMfFWOM/MVf/MUUK0eXPfnkk/Lkk09Wtv3sZz8TEZEXvehFcskll8hzn/vcyv4DDjhATjjhBPn4xz8uf/zHf1xuv+KKK+T000+XD33oQ+MvHAAAYAx4/gEA08EzECwWPP/ApPH8A4sFzz8wi3j+AQAAUMUzEACYPJ5/YLHg+QemgWcgWAx4/oFZxPMPAOiezPK6xFa+973vycte9rLy83Oe8xz56U9/2irGWWedJe94xzvKz0cccUTlxg4Yt8suu0xe8YpXlJ932203Wbdu3fQKAhpcf/31csghh1QeDB5xxBFyySWXSJZlU6wMS9H69evlkUceKT9v2LBBHnjgAfnBD34gF154oXzrW9+qtP/93/99+eQnPynLli2bdKlYIowx8rKXvUy+//3vi4jIr/7qr8qVV17pXXNc/zEtX/3qV2Vubk5e9apXRa/dV199tbzxjW+UW2+9tbL9oosukqOPPnqcZWKJeuKJJ2T58uW17VtuuaXccMMNtb/s1k466ST5P//n/5SfV6xYIXfccYdsvvnmI68VAABg3Hj+ga7g70CwmPAMBLOC5x+YNTz/wGLA8w/MMp5/AAAAVPEMBF3A34FgMeH5B2YFzz8wi3gGglnH8w/MMp5/AED35NMuYLHRF60NGza0jqH7cCEEAL8777xTjjrqqMpfdO+2227yuc99jr/oxlSsWLFCVq5cWf554QtfKIcccoi8+93vlm9+85vyH//xH7LbbruV7T/96U/LO9/5zilWjK775Cc/Wf5F99zcnJx11lk8YMHMee1rXytHHHFE0rV79erV8v3vf1/22muvyvaTTz5Z5ufnx1UilrDNNttM8rz+VyMnnXRS9C+7RRb+iypbbrll+Xn9+vVy8cUXj7RGAACASeH5BwBMFs9AMEt4/oFZw/MPLAY8/8As4/kHAABAFc9AAGByeP6BWcLzD8winoFg1vH8A7OM5x8A0D288Kgl/rIbACbj/vvvl1e96lVy1113ldt23HFH+frXvy7bbbfdFCsDwg455BD59re/Ldtss0257TOf+Yx86UtfmmJV6Kq1a9fKhz70ofLzSSedJPvtt9/0CgJGZMWKFfKFL3yh8hfkP/7xj+Xb3/72FKtCl/ne8P/mN785ue9//+//vbLtsssuG0VZAAAAE8fzDwCYHJ6BYLHh+Qcmiecf6Cqef2DSeP4BAADQwzMQAJgMnn9gseH5ByaNZyDoIp5/YNJ4/gEA3cILj1rqf3OfyP9v7+5Dq6z/Bo5/dG7mQ+p8zgXOp0gUU8EwxVgZusyCYtqDhUoFhYRB9V+kEQQJBVEQIYHVHxWIplEKzVTUWmL2nCmlWVpqmYYPm063+4+b+/g76u9uc+c6Z+fy9YKg77frOnyKsDxv+Czi5MmTceLEiVZ9xqFDh7LOvXr1autYAKny999/xy233BK7du3K3PXt2zdqa2tjxIgRBZwM/t2QIUPimWeeybpbsmRJgaYhrZqbm+Phhx+OkydPRkTE0KFDY/HixYUdCnJo/PjxMW3atKy7tWvXFmga0u7835MPGDAgKisrW/z+xIkTs847duzIwVQAAPmnfwDkhwZCsdI/yAf9g7TTP8gn/QMA4BwNBCB5+gfFSv8gXzQQ0kz/IJ/0D4B0sfColfr06RPl5eVZd7/++murPmPv3r1ZZ1/cAJzzzz//xLRp0+Lbb7/N3JWXl8fHH38co0aNKuBk0HL33HNP1rmuri6OHj1amGFIpaVLl8Ynn3ySOb/++uvRpUuXAk4EuVddXZ11/uabbwo0CWl3zTXXZJ2vuuqqVr0/aNCgrPPhw4fbPBMAQCHoHwDJ00AodvoHSdM/uBzoH+SL/gEAcI4GApAs/YNip3+QDxoIaad/kC/6B0C6dCr0AMVo5MiR8emnn2bOP/30U4wcObLF7+/evfuCzwMg4tixY1FdXR1ffPFF5q5Hjx6xdu3aGDt2bOEGg1bq379/lJeXx5EjRyIioqmpKfbs2RPjxo0r8GSkxaJFizJ/PmPGjBg+fHj88ssv/+87Bw4cyDqfOXPmgncGDRoUZWVluRoT2uT8Det//vlnYQYh9UaNGhXr1q3LnDt37tyq989/vqGhISdzAQAUgv4BkBwNhDTQP0ia/sHlQP8gX/QPAIBsGghAMvQP0kD/IB80ENJO/yBf9A+AdLHw6BKMHj0668vuzz77LG6//fYWvXvixIkLNlOOHj06p/MBFKMTJ07EjBkzoq6uLnPXvXv3WLNmTVx//fUFnAwuTWlpadb51KlTBZqENKqvr8/8+UcffRRDhgxp9Wfs37//gve+/PJLcZF24/yfWPGf/95DLo0ZMybr3NqfynP+83369GnjRAAAhaN/ACRDAyFN9A+SpH9wOdA/yBf9AwAgmwYCkHv6B2mif5A0DYS00z/IF/0DIF06FnqAYlRdXZ113rBhQ4vf3bRpU5w5cyZzHjduXAwYMCBXowEUpfr6+pg5c2Zs3rw5c9e1a9f48MMPY9KkSQWcDC5NQ0ND/PXXX1l3/nsP0Drn/zrat2/fAk1C2t16663RoUOHzHn37t2t2tL/3XffZZ2vvvrqnM0GAJBv+gdA7mkgpIn+AdB2+gf5on8AAGTTQAByS/8gTfQPgLbTP8gX/QMgXSw8ugTTp0/P2jb52WefxY8//tiid5ctW5Z1vvPOO3M5GkDRaWhoiDvuuCMrHF5xxRWxevXquPHGGws3GLTBunXroqmpKXPu2rVrVFRUFHAigOLz+eefZ50HDRpUoElIu0GDBsUNN9yQOTc2Nsa6deta/P7atWuzzlOmTMnZbAAA+aZ/AOSWBkLa6B8Abad/kC/6BwBANg0EIHf0D9JG/wBoO/2DfNE/ANLFwqNL0LVr16ipqcm6e+GFF/71vV27dsXKlSsz506dOsV9992X8/kAisXp06fjrrvuitra2sxd586d4/3334+pU6cWcDK4dE1NTfHcc89l3VVXV0dZWVmBJiKNjh49Gs3Nza36Y/369VmfMXjw4AueGTt2bGH+huA8DQ0NsWLFiqy7qqqqwgzDZWH+/PlZ55deeqlF723atCm2bt2aOXfs2DFmzJiR09kAAPJJ/wDIHQ2EtNE/yAf9g7TTP8g3/QMA4BwNBCA39A/SRv8gXzQQ0kz/IN/0D4D0sPDoEi1evDhKS0sz52XLlsXq1av/6/MNDQ0xf/78OH36dObuwQcfjGHDhiU6J0B7debMmZg9e3asWbMmc1daWhrLly+P6dOnF3Ay+F+vvPJK/PHHH616p7GxMR588MELtlIvWLAgl6MBpN4LL7wQ+/fvz5xLSkritttuK+BEpN38+fNj5MiRmfMnn3zyr196Hzp06IIvymfPnu33+QBA0dM/ANpOA6E90z8ACkf/IN/0DwCAbBoIQNvoH7Rn+gdA4egf5Jv+AZAeFh5doqFDh8bChQuz7mpqauLVV1/N+kI7ImLHjh0xderU+PTTTzN3ffr0iUWLFuVlVoD25uzZszFnzpxYtWpV5q5Tp07x3nvvxcyZMws4GZzzxhtvxLBhw+L++++PDz74II4dO/Zfn62vr4933nknxo0bF8uWLcv6aw888EDcfPPNCU8L0D69/fbbcfDgwVa9s3Tp0nj22Wez7ubNmxeDBw/O5WiQpaSkJF5++eXo2PHc1yRPPPFELFy4MI4cOXLB87W1tTF58uT4+eefM3fl5eXx/PPP52VeAIAk6R8AbaOB0N7pHwBtp39QLPQPAIBsGgjApdM/aO/0D4C20z8oFvoHQHp0aG5ubi70EMXq7Nmzcfvtt2dtpo6I6N+/f4wfPz6uvPLK2L17d2zfvj3+8x9zWVlZ1NbWxpQpU/I9MpeZffv2xZkzZy64r6uri3vvvTdzrqioiM2bN1/0M7p37x59+/ZNbEYuT3Pnzo233nor627JkiUxa9asVn/WwIED44orrsjVaJAxduzY+PrrrzPnDh06xPDhw6OysjJ69eoVZWVlcezYsdi7d2/88MMP0djYeMFnzJw5M5YvXx6dO3fO5+hwURs2bIibbropcx48eHD88ssvhRuIy0JVVVVs3bo1Zs2aFbNnz46qqqro1q3bRZ/dtm1bPP/887Fy5cqs+4qKiti2bVsMHDgwHyNzmXv11Vfjsccey7orLS2NiRMnRkVFRdTX18dXX30Ve/fuzXqmrKwsVq9e7adUAQCpoX9QDDQQ2isNhPZO/yBt9A8KQf+g2OgfAADnaCC0d/oH7ZX+QXunf5BGGgj5pn9QbPQPgOJn4VEbHT9+PB566KF47733WvR8//79480334zq6uqEJ4OIysrKC/5HrLXmzp17wbZqaKsOHTrk7LPWr18fVVVVOfs8+D/nf+HdGl26dImnn346nnrqqSgtLc3xZHBpfNlNIVRVVcXGjRsz544dO8aIESOisrIyevbsGSUlJXH48OH4+uuvL/qTAHr37h0bN26M0aNH53NsLnOvvfZaPPnkk3Hy5MkWPT9gwIBYsWJFTJo0KeHJAADyS/+gvdNAaK80ENo7/YO00T8oBP2DYqR/AACco4HQnukftFf6B+2d/kEaaSDkm/5BMdI/AIpbp0IPUOy6d+8e7777btTU1MSLL74YdXV1F32ud+/ecffdd8ezzz4b/fr1y/OUAEBrLV26NFavXh3r1q2L7du3x6lTp/71nWuvvTbmzJkT8+bNi6uvvjoPUwIUl6ampti5c2fs3LnzX5+dOnVqLFu2zK+n5N2jjz4a06ZNi8WLF8eqVavi2LFjF31u4MCB8cgjj8Tjjz8ePXv2zPOUAADJ0z8AIJ30D4Dc0z8oBvoHAMA5GggApI/+AZB7+gfFQP8AKG4WHuVITU1N1NTUxJ49e2L79u3x+++/x4kTJ2LgwIExePDgmDx5cpSVlRV6TACghSZMmBATJkyI5557LhobG2PHjh2xe/fu2L9/fxw/fjwaGxuje/fu0aNHj6isrIxx48ZFeXl5occGaFcWLlwYFRUVsWXLlhb91Klu3brFtGnTYsGCBTF16tQ8TAgXN2zYsHj77bejvr4+tmzZEvv27YsDBw5EWVlZ9OvXL6677roYM2ZMoccEAMgL/QMA0kX/AGg7/YNipX8AAGTTQAAgPfQPgLbTPyhW+gdA8erQ3NzcXOghAAAASLejR4/G999/H7/99lscPHgwTp48GU1NTdGrV68oLy+PkSNHxpgxY6KkpKTQowIAAAAAALSI/gEAAAAAAKSN/gEA5IOFRwAAAAAAAAAAAAAAAAAAAAAAQOI6FnoAAAAAAAAAAAAAAAAAAAAAAAAg/Sw8AgAAAAAAAAAAAAAAAAAAAAAAEmfhEQAAAAAAAAAAAAAAAAAAAAAAkDgLjwAAAAAAAAAAAAAAAAAAAAAAgMRZeAQAAAAAAAAAAAAAAAAAAAAAACTOwiMAAAAAAAAAAAAAAAAAAAAAACBxFh4BAAAAAAAAAAAAAAAAAAAAAACJs/AIAAAAAAAAAAAAAAAAAAAAAABInIVHAAAAAAAAAAAAAAAAAAAAAABA4iw8AgAAAAAAAAAAAAAAAAAAAAAAEmfhEQAAAAAAAAAAAAAAAAAAAAAAkDgLjwAAAAAAAAAAAAAAAAAAAAAAgMRZeAQAAAAAAAAAAAAAAAAAAAAAACTOwiMAAAAAAAAAAAAAAAAAAAAAACBxFh4BAAAAAAAAAAAAAAAAAAAAAACJs/AIAAAAAAAAAAAAAAAAAAAAAABInIVHAAAAAAAAAAAAAAAAAAAAAABA4iw8AgAAAAAAAAAAAAAAAAAAAAAAEmfhEQAAAAAAAAAAAAAAAAAAAAAAkDgLjwAAAAAAAAAAAAAAAAAAAAAAgMRZeAQAAAAAAAAAAAAAAAAAAAAAACTOwiMAAAAAAAAAAAAAAAAAAAAAACBxFh4BAAAAAAAAAAAAAAAAAAAAAACJs/AIAAAAAAAAAAAAAAAAAAAAAABInIVHAAAAAAAAAAAAAAAAAAAAAABA4iw8AgAAAAAAAAAAAAAAAAAAAAAAEmfhEQAAAAAAAAAAAAAAAAAAAAAAkDgLjwAAAAAAAAAAAAAAAAAAAAAAgMRZeAQAAAAAAAAAAAAAAAAAAAAAACTOwiMAAAAAAAAAAAAAAAAAAAAAACBxFh4BAAAAAAAAAAAAAAAAAAAAAACJs/AIAAAAAAAAAAAAAAAAAAAAAABInIVHAAAAAAAAAAAAAAAAAAAAAABA4iw8AgAAAAAAAAAAAAAAAAAAAAAAEmfhEQAAAAAAAAAAAAAAAAAAAAAAkDgLjwAAAAAAAAAAAAAAAAAAAAAAgMT9D58knFliPigHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 5850x1500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(19.5,5), dpi=300)\n",
    "pos1 = axs[0].pcolormesh(XY[0], XY[1], U2.reshape(N, N), cmap='inferno')\n",
    "fig.colorbar(pos1, ax=axs[0])\n",
    "pos2 = axs[1].pcolormesh(XY[0], XY[1], error_1.detach().cpu().reshape(N, N), cmap='hot')\n",
    "fig.colorbar(pos2, ax=axs[1])\n",
    "pos3 = axs[2].pcolormesh(XY[0], XY[1], error_2.detach().cpu().reshape(N, N), cmap='hot')\n",
    "fig.colorbar(pos3, ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 129\n",
    "data = dataset_grid(N, [0, 2*np.pi], [0, 2*np.pi])\n",
    "loaders = get_loaders_Sobol(data, N**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31613371019408326\n",
      "0.30931477378505534\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(loaders['train']):\n",
    "    areas, tri = get_areas(x)\n",
    "    areas = areas.to(args['dev'])\n",
    "x = x.to(args['dev'])\n",
    "x_rot = torch.fliplr(x)\n",
    "x_rot[:,1] = -x_rot[:,1]\n",
    "\n",
    "_, q2, gH = PDE_loss_dual(x, net_dual, A_interp_inv, H1)\n",
    "bound_1 = compute_estimate(areas, tri, q2, gH, L).detach()[0].item()\n",
    "print(1/bound_1)\n",
    "\n",
    "W1 = net_dual(x)\n",
    "W2 = net_dual(x_rot)\n",
    "\n",
    "bound_2 = compute_bound_dual(W1, W2, tri, x, A_inv, L)[0,0]\n",
    "print(bound_2)\n",
    "\n",
    "np.save(f'bounds/square/A_l_MNPINN_{total_params}_{N}.npy', (1/bound_1, bound_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
