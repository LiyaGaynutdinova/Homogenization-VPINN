{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.tri import Triangulation\n",
    "from dataset import *\n",
    "from save_load import *\n",
    "from NN_library.PINN import *\n",
    "from NN_library.train_VPINN import *\n",
    "from PDE_losses import compute_int\n",
    "from utility import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_Sobol(14, [0, 2*np.pi], [0, 2*np.pi])\n",
    "loaders = get_loaders_Sobol(data, 2**14+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22a5a67eb90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = 2*np.pi\n",
    "N = 512 # number of nodes in each direction including the border\n",
    "x = np.linspace(0, L, N, endpoint=True)\n",
    "y = np.linspace(0, L, N, endpoint=True)\n",
    "\n",
    "XY = np.meshgrid(x, y)\n",
    "grid_data = torch.tensor(np.vstack((XY[0].flatten(), XY[1].flatten())).T, dtype=torch.float, device=dev)\n",
    "\n",
    "def a_function(x,y):\n",
    "    a = np.where((x<np.pi)&(y<np.pi), 0.1, 1)\n",
    "    return a\n",
    "\n",
    "def A_interp(x):  \n",
    "    eps = torch.pi / 1e2\n",
    "    x_dir = 0.5 * (torch.clamp(torch.sin(x[:,0]), -eps, eps) + eps) / eps\n",
    "    y_dir = 0.5 * (torch.clamp(torch.sin(x[:,1]), -eps, eps) + eps) / eps\n",
    "    a = 0.1 + 0.9*(1 - x_dir * y_dir).view(-1,1,1)\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = a * I\n",
    "    return A\n",
    "\n",
    "def A(x):  \n",
    "    a = torch.where((x[:,0]<torch.pi)&(x[:,1]<torch.pi)&(x[:,0]>0)&(x[:,1]>0), 0.1, 1).view(-1,1,1)\n",
    "    I = torch.eye(2, device=dev).repeat(x.shape[0], 1, 1)\n",
    "    A = a * I\n",
    "    return A\n",
    "\n",
    "def H1(x):\n",
    "    H = torch.zeros_like(x)\n",
    "    H[:,0] = 1.\n",
    "    return H\n",
    "\n",
    "Z = a_function(XY[0].flatten(),XY[1].flatten())\n",
    "plt.pcolormesh(XY[0], XY[1], Z.reshape(N, N))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:,0], data[:,1], s = 0.5, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 65\n"
     ]
    }
   ],
   "source": [
    "net_H1 = PINN(n_periodic=2, n_hidden=4, n_layers=1, period_len=L)\n",
    "total_params = sum(p.numel() for p in net_H1.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "args = {'lr' : 0.00001, 'epochs' : 20000, 'dev' : dev, 'name' : f'NN_library/VPINN/VPINN_H1_{total_params}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 75\n",
    "torch.manual_seed(0)\n",
    "test_functions = [PINN(n_periodic=2, n_hidden=4, n_layers=1, period_len=L).to(dev) for i in range(N_test)]\n",
    "G = torch.load(\"G.pt\")[:N_test,:N_test]\n",
    "G_inv = torch.linalg.inv(G).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_H1 = load_network(net_H1, args['name']+'_19999', args)\n",
    "net_H1 = net_H1.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_H1 = load_network(net_H1, f'NN_library/PINN/PINN_H1_{total_params}_int'+'_19999', args)\n",
    "net_H1 = net_H1.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 mean train loss:  8.40599241e-05, mean val. loss:  3.73483956e-01\n",
      "Epoch: 1 mean train loss:  8.43822199e-05, mean val. loss:  3.73666823e-01\n",
      "Epoch: 2 mean train loss:  8.44071619e-05, mean val. loss:  3.73850048e-01\n",
      "Epoch: 3 mean train loss:  8.42331210e-05, mean val. loss:  3.74035686e-01\n",
      "Epoch: 4 mean train loss:  8.31559009e-05, mean val. loss:  3.74221712e-01\n",
      "Epoch: 5 mean train loss:  8.37484840e-05, mean val. loss:  3.74410540e-01\n",
      "Epoch: 6 mean train loss:  8.41094879e-05, mean val. loss:  3.74601305e-01\n",
      "Epoch: 7 mean train loss:  8.36768304e-05, mean val. loss:  3.74793768e-01\n",
      "Epoch: 8 mean train loss:  8.40870780e-05, mean val. loss:  3.74987662e-01\n",
      "Epoch: 9 mean train loss:  8.41482251e-05, mean val. loss:  3.75182748e-01\n",
      "Epoch: 10 mean train loss:  8.38179840e-05, mean val. loss:  3.75378907e-01\n",
      "Epoch: 11 mean train loss:  8.37641128e-05, mean val. loss:  3.75578344e-01\n",
      "Epoch: 12 mean train loss:  8.36243271e-05, mean val. loss:  3.75778913e-01\n",
      "Epoch: 13 mean train loss:  8.27406766e-05, mean val. loss:  3.75981808e-01\n",
      "Epoch: 14 mean train loss:  8.29349738e-05, mean val. loss:  3.76186907e-01\n",
      "Epoch: 15 mean train loss:  8.31988582e-05, mean val. loss:  3.76393050e-01\n",
      "Epoch: 16 mean train loss:  8.34764214e-05, mean val. loss:  3.76598716e-01\n",
      "Epoch: 17 mean train loss:  8.25428870e-05, mean val. loss:  3.76808405e-01\n",
      "Epoch: 18 mean train loss:  8.28353513e-05, mean val. loss:  3.77017647e-01\n",
      "Epoch: 19 mean train loss:  8.24687595e-05, mean val. loss:  3.77230138e-01\n",
      "Epoch: 20 mean train loss:  8.41723231e-05, mean val. loss:  3.77444237e-01\n",
      "Epoch: 21 mean train loss:  8.20447458e-05, mean val. loss:  3.77660424e-01\n",
      "Epoch: 22 mean train loss:  8.27141921e-05, mean val. loss:  3.77871871e-01\n",
      "Epoch: 23 mean train loss:  8.20140122e-05, mean val. loss:  3.78095806e-01\n",
      "Epoch: 24 mean train loss:  8.12890939e-05, mean val. loss:  3.78312081e-01\n",
      "Epoch: 25 mean train loss:  8.21477734e-05, mean val. loss:  3.78539830e-01\n",
      "Epoch: 26 mean train loss:  8.17078981e-05, mean val. loss:  3.78761739e-01\n",
      "Epoch: 27 mean train loss:  8.28558987e-05, mean val. loss:  3.78993541e-01\n",
      "Epoch: 28 mean train loss:  8.29530763e-05, mean val. loss:  3.79217744e-01\n",
      "Epoch: 29 mean train loss:  8.20528367e-05, mean val. loss:  3.79451394e-01\n",
      "Epoch: 30 mean train loss:  8.07603355e-05, mean val. loss:  3.79679590e-01\n",
      "Epoch: 31 mean train loss:  8.21426220e-05, mean val. loss:  3.79919529e-01\n",
      "Epoch: 32 mean train loss:  8.09974736e-05, mean val. loss:  3.80150974e-01\n",
      "Epoch: 33 mean train loss:  8.15250387e-05, mean val. loss:  3.80394459e-01\n",
      "Epoch: 34 mean train loss:  8.07533797e-05, mean val. loss:  3.80630642e-01\n",
      "Epoch: 35 mean train loss:  8.10378115e-05, mean val. loss:  3.80877733e-01\n",
      "Epoch: 36 mean train loss:  8.12580402e-05, mean val. loss:  3.81122112e-01\n",
      "Epoch: 37 mean train loss:  8.15019594e-05, mean val. loss:  3.81366581e-01\n",
      "Epoch: 38 mean train loss:  8.02747963e-05, mean val. loss:  3.81614804e-01\n",
      "Epoch: 39 mean train loss:  8.02653085e-05, mean val. loss:  3.81862581e-01\n",
      "Epoch: 40 mean train loss:  8.08453478e-05, mean val. loss:  3.82112801e-01\n",
      "Epoch: 41 mean train loss:  8.08657205e-05, mean val. loss:  3.82366866e-01\n",
      "Epoch: 42 mean train loss:  8.07789329e-05, mean val. loss:  3.82620066e-01\n",
      "Epoch: 43 mean train loss:  8.12610961e-05, mean val. loss:  3.82874846e-01\n",
      "Epoch: 44 mean train loss:  8.22727161e-05, mean val. loss:  3.83128494e-01\n",
      "Epoch: 45 mean train loss:  8.05674645e-05, mean val. loss:  3.83384198e-01\n",
      "Epoch: 46 mean train loss:  8.08056793e-05, mean val. loss:  3.83637518e-01\n",
      "Epoch: 47 mean train loss:  8.08757613e-05, mean val. loss:  3.83897126e-01\n",
      "Epoch: 48 mean train loss:  8.01061979e-05, mean val. loss:  3.84158313e-01\n",
      "Epoch: 49 mean train loss:  7.94248190e-05, mean val. loss:  3.84423733e-01\n",
      "Epoch: 50 mean train loss:  7.97938264e-05, mean val. loss:  3.84689659e-01\n",
      "Epoch: 51 mean train loss:  7.92625942e-05, mean val. loss:  3.84960830e-01\n",
      "Epoch: 52 mean train loss:  7.89925107e-05, mean val. loss:  3.85231555e-01\n",
      "Epoch: 53 mean train loss:  7.96914101e-05, mean val. loss:  3.85503292e-01\n",
      "Epoch: 54 mean train loss:  7.85317388e-05, mean val. loss:  3.85776579e-01\n",
      "Epoch: 55 mean train loss:  8.04431329e-05, mean val. loss:  3.86051774e-01\n",
      "Epoch: 56 mean train loss:  8.06277385e-05, mean val. loss:  3.86328518e-01\n",
      "Epoch: 57 mean train loss:  7.82381103e-05, mean val. loss:  3.86610389e-01\n",
      "Epoch: 58 mean train loss:  7.99431000e-05, mean val. loss:  3.86892229e-01\n",
      "Epoch: 59 mean train loss:  7.93103536e-05, mean val. loss:  3.87177438e-01\n",
      "Epoch: 60 mean train loss:  7.94633524e-05, mean val. loss:  3.87465030e-01\n",
      "Epoch: 61 mean train loss:  7.96113163e-05, mean val. loss:  3.87753367e-01\n",
      "Epoch: 62 mean train loss:  7.82951538e-05, mean val. loss:  3.88042212e-01\n",
      "Epoch: 63 mean train loss:  7.93472864e-05, mean val. loss:  3.88331205e-01\n",
      "Epoch: 64 mean train loss:  7.98262190e-05, mean val. loss:  3.88620913e-01\n",
      "Epoch: 65 mean train loss:  7.98556139e-05, mean val. loss:  3.88912320e-01\n",
      "Epoch: 66 mean train loss:  7.79188413e-05, mean val. loss:  3.89206678e-01\n",
      "Epoch: 67 mean train loss:  7.78056274e-05, mean val. loss:  3.89503539e-01\n",
      "Epoch: 68 mean train loss:  7.86762394e-05, mean val. loss:  3.89800251e-01\n",
      "Epoch: 69 mean train loss:  7.83899159e-05, mean val. loss:  3.90094489e-01\n",
      "Epoch: 70 mean train loss:  7.86126766e-05, mean val. loss:  3.90400201e-01\n",
      "Epoch: 71 mean train loss:  7.74091750e-05, mean val. loss:  3.90697896e-01\n",
      "Epoch: 72 mean train loss:  7.80502742e-05, mean val. loss:  3.91007274e-01\n",
      "Epoch: 73 mean train loss:  7.85171869e-05, mean val. loss:  3.91308129e-01\n",
      "Epoch: 74 mean train loss:  7.78215472e-05, mean val. loss:  3.91619951e-01\n",
      "Epoch: 75 mean train loss:  7.78221583e-05, mean val. loss:  3.91925246e-01\n",
      "Epoch: 76 mean train loss:  7.79321708e-05, mean val. loss:  3.92233253e-01\n",
      "Epoch: 77 mean train loss:  7.71384803e-05, mean val. loss:  3.92545521e-01\n",
      "Epoch: 78 mean train loss:  7.81812414e-05, mean val. loss:  3.92853260e-01\n",
      "Epoch: 79 mean train loss:  7.79423572e-05, mean val. loss:  3.93165767e-01\n",
      "Epoch: 80 mean train loss:  7.66774174e-05, mean val. loss:  3.93482476e-01\n",
      "Epoch: 81 mean train loss:  7.77579262e-05, mean val. loss:  3.93801570e-01\n",
      "Epoch: 82 mean train loss:  7.69120525e-05, mean val. loss:  3.94123256e-01\n",
      "Epoch: 83 mean train loss:  7.75575754e-05, mean val. loss:  3.94439787e-01\n",
      "Epoch: 84 mean train loss:  7.72034691e-05, mean val. loss:  3.94762099e-01\n",
      "Epoch: 85 mean train loss:  7.58084643e-05, mean val. loss:  3.95087093e-01\n",
      "Epoch: 86 mean train loss:  7.70739862e-05, mean val. loss:  3.95406127e-01\n",
      "Epoch: 87 mean train loss:  7.62304699e-05, mean val. loss:  3.95733982e-01\n",
      "Epoch: 88 mean train loss:  7.58211245e-05, mean val. loss:  3.96064967e-01\n",
      "Epoch: 89 mean train loss:  7.68982864e-05, mean val. loss:  3.96392494e-01\n",
      "Epoch: 90 mean train loss:  7.69102189e-05, mean val. loss:  3.96729141e-01\n",
      "Epoch: 91 mean train loss:  7.68667087e-05, mean val. loss:  3.97066563e-01\n",
      "Epoch: 92 mean train loss:  7.58884416e-05, mean val. loss:  3.97398412e-01\n",
      "Epoch: 93 mean train loss:  7.65412115e-05, mean val. loss:  3.97738189e-01\n",
      "Epoch: 94 mean train loss:  7.69227918e-05, mean val. loss:  3.98077935e-01\n",
      "Epoch: 95 mean train loss:  7.65091681e-05, mean val. loss:  3.98421139e-01\n",
      "Epoch: 96 mean train loss:  7.65018049e-05, mean val. loss:  3.98764729e-01\n",
      "Epoch: 97 mean train loss:  7.62748823e-05, mean val. loss:  3.99105608e-01\n",
      "Epoch: 98 mean train loss:  7.66141166e-05, mean val. loss:  3.99450600e-01\n",
      "Epoch: 99 mean train loss:  7.55026413e-05, mean val. loss:  3.99800152e-01\n",
      "Epoch: 100 mean train loss:  7.65695295e-05, mean val. loss:  4.00142372e-01\n",
      "Epoch: 101 mean train loss:  7.68626342e-05, mean val. loss:  4.00492042e-01\n",
      "Epoch: 102 mean train loss:  7.63249409e-05, mean val. loss:  4.00844932e-01\n",
      "Epoch: 103 mean train loss:  7.53354689e-05, mean val. loss:  4.01192933e-01\n",
      "Epoch: 104 mean train loss:  7.51130865e-05, mean val. loss:  4.01549608e-01\n",
      "Epoch: 105 mean train loss:  7.61956326e-05, mean val. loss:  4.01908129e-01\n",
      "Epoch: 106 mean train loss:  7.53323839e-05, mean val. loss:  4.02263194e-01\n",
      "Epoch: 107 mean train loss:  7.61825941e-05, mean val. loss:  4.02624309e-01\n",
      "Epoch: 108 mean train loss:  7.56744703e-05, mean val. loss:  4.02978420e-01\n",
      "Epoch: 109 mean train loss:  7.74144137e-05, mean val. loss:  4.03338015e-01\n",
      "Epoch: 110 mean train loss:  7.51329062e-05, mean val. loss:  4.03693765e-01\n",
      "Epoch: 111 mean train loss:  7.56602676e-05, mean val. loss:  4.04051691e-01\n",
      "Epoch: 112 mean train loss:  7.50539184e-05, mean val. loss:  4.04410660e-01\n",
      "Epoch: 113 mean train loss:  7.42677366e-05, mean val. loss:  4.04777795e-01\n",
      "Epoch: 114 mean train loss:  7.53201894e-05, mean val. loss:  4.05146211e-01\n",
      "Epoch: 115 mean train loss:  7.47299055e-05, mean val. loss:  4.05514002e-01\n",
      "Epoch: 116 mean train loss:  7.56219379e-05, mean val. loss:  4.05880988e-01\n",
      "Epoch: 117 mean train loss:  7.47450395e-05, mean val. loss:  4.06255871e-01\n",
      "Epoch: 118 mean train loss:  7.46282458e-05, mean val. loss:  4.06632304e-01\n",
      "Epoch: 119 mean train loss:  7.35798967e-05, mean val. loss:  4.07006443e-01\n",
      "Epoch: 120 mean train loss:  7.51822954e-05, mean val. loss:  4.07381564e-01\n",
      "Epoch: 121 mean train loss:  7.47586892e-05, mean val. loss:  4.07763630e-01\n",
      "Epoch: 122 mean train loss:  7.45579309e-05, mean val. loss:  4.08147246e-01\n",
      "Epoch: 123 mean train loss:  7.28963059e-05, mean val. loss:  4.08529192e-01\n",
      "Epoch: 124 mean train loss:  7.38870876e-05, mean val. loss:  4.08913255e-01\n",
      "Epoch: 125 mean train loss:  7.45180296e-05, mean val. loss:  4.09305662e-01\n",
      "Epoch: 126 mean train loss:  7.39565294e-05, mean val. loss:  4.09701407e-01\n",
      "Epoch: 127 mean train loss:  7.39466632e-05, mean val. loss:  4.10090715e-01\n",
      "Epoch: 128 mean train loss:  7.39131647e-05, mean val. loss:  4.10488904e-01\n",
      "Epoch: 129 mean train loss:  7.36693328e-05, mean val. loss:  4.10891205e-01\n",
      "Epoch: 130 mean train loss:  7.40185496e-05, mean val. loss:  4.11289215e-01\n",
      "Epoch: 131 mean train loss:  7.45313009e-05, mean val. loss:  4.11694765e-01\n",
      "Epoch: 132 mean train loss:  7.50045874e-05, mean val. loss:  4.12101686e-01\n",
      "Epoch: 133 mean train loss:  7.35843496e-05, mean val. loss:  4.12506819e-01\n",
      "Epoch: 134 mean train loss:  7.28806190e-05, mean val. loss:  4.12914008e-01\n",
      "Epoch: 135 mean train loss:  7.38587114e-05, mean val. loss:  4.13316131e-01\n",
      "Epoch: 136 mean train loss:  7.40340329e-05, mean val. loss:  4.13725793e-01\n",
      "Epoch: 137 mean train loss:  7.41643889e-05, mean val. loss:  4.14126575e-01\n",
      "Epoch: 138 mean train loss:  7.28321320e-05, mean val. loss:  4.14529413e-01\n",
      "Epoch: 139 mean train loss:  7.33053312e-05, mean val. loss:  4.14939135e-01\n",
      "Epoch: 140 mean train loss:  7.36294314e-05, mean val. loss:  4.15344566e-01\n",
      "Epoch: 141 mean train loss:  7.35686044e-05, mean val. loss:  4.15750802e-01\n",
      "Epoch: 142 mean train loss:  7.22826808e-05, mean val. loss:  4.16156054e-01\n",
      "Epoch: 143 mean train loss:  7.26694998e-05, mean val. loss:  4.16563123e-01\n",
      "Epoch: 144 mean train loss:  7.26099825e-05, mean val. loss:  4.16972667e-01\n",
      "Epoch: 145 mean train loss:  7.28710729e-05, mean val. loss:  4.17383283e-01\n",
      "Epoch: 146 mean train loss:  7.20528769e-05, mean val. loss:  4.17796403e-01\n",
      "Epoch: 147 mean train loss:  7.22641416e-05, mean val. loss:  4.18211430e-01\n",
      "Epoch: 148 mean train loss:  7.30950560e-05, mean val. loss:  4.18626547e-01\n",
      "Epoch: 149 mean train loss:  7.37920345e-05, mean val. loss:  4.19043720e-01\n",
      "Epoch: 150 mean train loss:  7.21802935e-05, mean val. loss:  4.19470459e-01\n",
      "Epoch: 151 mean train loss:  7.35078356e-05, mean val. loss:  4.19892848e-01\n",
      "Epoch: 152 mean train loss:  7.33763445e-05, mean val. loss:  4.20315355e-01\n",
      "Epoch: 153 mean train loss:  7.23768317e-05, mean val. loss:  4.20747042e-01\n",
      "Epoch: 154 mean train loss:  7.15161150e-05, mean val. loss:  4.21172678e-01\n",
      "Epoch: 155 mean train loss:  7.21971737e-05, mean val. loss:  4.21599060e-01\n",
      "Epoch: 156 mean train loss:  7.17848015e-05, mean val. loss:  4.22028333e-01\n",
      "Epoch: 157 mean train loss:  7.21158285e-05, mean val. loss:  4.22458202e-01\n",
      "Epoch: 158 mean train loss:  7.20222306e-05, mean val. loss:  4.22891051e-01\n",
      "Epoch: 159 mean train loss:  7.22807890e-05, mean val. loss:  4.23322171e-01\n",
      "Epoch: 160 mean train loss:  7.25990976e-05, mean val. loss:  4.23753232e-01\n",
      "Epoch: 161 mean train loss:  7.17812509e-05, mean val. loss:  4.24186170e-01\n",
      "Epoch: 162 mean train loss:  7.11492612e-05, mean val. loss:  4.24623758e-01\n",
      "Epoch: 163 mean train loss:  7.25618156e-05, mean val. loss:  4.25059646e-01\n",
      "Epoch: 164 mean train loss:  7.14751659e-05, mean val. loss:  4.25495803e-01\n",
      "Epoch: 165 mean train loss:  7.23531411e-05, mean val. loss:  4.25933182e-01\n",
      "Epoch: 166 mean train loss:  7.09365413e-05, mean val. loss:  4.26374942e-01\n",
      "Epoch: 167 mean train loss:  7.14148337e-05, mean val. loss:  4.26816732e-01\n",
      "Epoch: 168 mean train loss:  7.16643699e-05, mean val. loss:  4.27257985e-01\n",
      "Epoch: 169 mean train loss:  7.10077584e-05, mean val. loss:  4.27700400e-01\n",
      "Epoch: 170 mean train loss:  7.10539171e-05, mean val. loss:  4.28144366e-01\n",
      "Epoch: 171 mean train loss:  7.07335130e-05, mean val. loss:  4.28591847e-01\n",
      "Epoch: 172 mean train loss:  7.13613117e-05, mean val. loss:  4.29038763e-01\n",
      "Epoch: 173 mean train loss:  7.06246356e-05, mean val. loss:  4.29481864e-01\n",
      "Epoch: 174 mean train loss:  7.13000773e-05, mean val. loss:  4.29927379e-01\n",
      "Epoch: 175 mean train loss:  7.13397458e-05, mean val. loss:  4.30370450e-01\n",
      "Epoch: 176 mean train loss:  7.17955409e-05, mean val. loss:  4.30816323e-01\n",
      "Epoch: 177 mean train loss:  7.06767605e-05, mean val. loss:  4.31264818e-01\n",
      "Epoch: 178 mean train loss:  7.07546715e-05, mean val. loss:  4.31709498e-01\n",
      "Epoch: 179 mean train loss:  7.04894774e-05, mean val. loss:  4.32155669e-01\n",
      "Epoch: 180 mean train loss:  6.97133073e-05, mean val. loss:  4.32610422e-01\n",
      "Epoch: 181 mean train loss:  7.08005973e-05, mean val. loss:  4.33057189e-01\n",
      "Epoch: 182 mean train loss:  7.08921289e-05, mean val. loss:  4.33503181e-01\n",
      "Epoch: 183 mean train loss:  7.08019361e-05, mean val. loss:  4.33954656e-01\n",
      "Epoch: 184 mean train loss:  7.09423621e-05, mean val. loss:  4.34402227e-01\n",
      "Epoch: 185 mean train loss:  7.00667442e-05, mean val. loss:  4.34852034e-01\n",
      "Epoch: 186 mean train loss:  7.02520483e-05, mean val. loss:  4.35308039e-01\n",
      "Epoch: 187 mean train loss:  6.98663061e-05, mean val. loss:  4.35758919e-01\n",
      "Epoch: 188 mean train loss:  7.03592377e-05, mean val. loss:  4.36213166e-01\n",
      "Epoch: 189 mean train loss:  6.93475886e-05, mean val. loss:  4.36667770e-01\n",
      "Epoch: 190 mean train loss:  7.03787664e-05, mean val. loss:  4.37122583e-01\n",
      "Epoch: 191 mean train loss:  7.03586848e-05, mean val. loss:  4.37585473e-01\n",
      "Epoch: 192 mean train loss:  6.91751193e-05, mean val. loss:  4.38040465e-01\n",
      "Epoch: 193 mean train loss:  7.01817335e-05, mean val. loss:  4.38497931e-01\n",
      "Epoch: 194 mean train loss:  7.05042330e-05, mean val. loss:  4.38962370e-01\n",
      "Epoch: 195 mean train loss:  7.03080441e-05, mean val. loss:  4.39425737e-01\n",
      "Epoch: 196 mean train loss:  6.93508191e-05, mean val. loss:  4.39886689e-01\n",
      "Epoch: 197 mean train loss:  6.98832737e-05, mean val. loss:  4.40350026e-01\n",
      "Epoch: 198 mean train loss:  6.89483422e-05, mean val. loss:  4.40815926e-01\n",
      "Epoch: 199 mean train loss:  7.04072299e-05, mean val. loss:  4.41289485e-01\n",
      "Epoch: 200 mean train loss:  6.89639710e-05, mean val. loss:  4.41757202e-01\n",
      "Epoch: 201 mean train loss:  6.83658873e-05, mean val. loss:  4.42231148e-01\n",
      "Epoch: 202 mean train loss:  6.84975239e-05, mean val. loss:  4.42710191e-01\n",
      "Epoch: 203 mean train loss:  6.97823416e-05, mean val. loss:  4.43181455e-01\n",
      "Epoch: 204 mean train loss:  6.89839944e-05, mean val. loss:  4.43654865e-01\n",
      "Epoch: 205 mean train loss:  7.02922407e-05, mean val. loss:  4.44128305e-01\n",
      "Epoch: 206 mean train loss:  6.89508161e-05, mean val. loss:  4.44606811e-01\n",
      "Epoch: 207 mean train loss:  6.89180451e-05, mean val. loss:  4.45090532e-01\n",
      "Epoch: 208 mean train loss:  6.86279382e-05, mean val. loss:  4.45571691e-01\n",
      "Epoch: 209 mean train loss:  6.92683097e-05, mean val. loss:  4.46054935e-01\n",
      "Epoch: 210 mean train loss:  6.87258434e-05, mean val. loss:  4.46543723e-01\n",
      "Epoch: 211 mean train loss:  6.94046321e-05, mean val. loss:  4.47025985e-01\n",
      "Epoch: 212 mean train loss:  6.87447609e-05, mean val. loss:  4.47508812e-01\n",
      "Epoch: 213 mean train loss:  6.87959837e-05, mean val. loss:  4.48001176e-01\n",
      "Epoch: 214 mean train loss:  6.81523816e-05, mean val. loss:  4.48487043e-01\n",
      "Epoch: 215 mean train loss:  6.78973156e-05, mean val. loss:  4.48972672e-01\n",
      "Epoch: 216 mean train loss:  6.94612099e-05, mean val. loss:  4.49455917e-01\n",
      "Epoch: 217 mean train loss:  6.86380954e-05, mean val. loss:  4.49942529e-01\n",
      "Epoch: 218 mean train loss:  6.76498166e-05, mean val. loss:  4.50429112e-01\n",
      "Epoch: 219 mean train loss:  6.88838481e-05, mean val. loss:  4.50918227e-01\n",
      "Epoch: 220 mean train loss:  6.95139752e-05, mean val. loss:  4.51406270e-01\n",
      "Epoch: 221 mean train loss:  6.89412118e-05, mean val. loss:  4.51892376e-01\n",
      "Epoch: 222 mean train loss:  6.71898306e-05, mean val. loss:  4.52380389e-01\n",
      "Epoch: 223 mean train loss:  6.88360306e-05, mean val. loss:  4.52867061e-01\n",
      "Epoch: 224 mean train loss:  6.71727757e-05, mean val. loss:  4.53357607e-01\n",
      "Epoch: 225 mean train loss:  6.81742968e-05, mean val. loss:  4.53848571e-01\n",
      "Epoch: 226 mean train loss:  6.86340500e-05, mean val. loss:  4.54336584e-01\n",
      "Epoch: 227 mean train loss:  6.84437109e-05, mean val. loss:  4.54829693e-01\n",
      "Epoch: 228 mean train loss:  6.88537839e-05, mean val. loss:  4.55319345e-01\n",
      "Epoch: 229 mean train loss:  6.76993222e-05, mean val. loss:  4.55811411e-01\n",
      "Epoch: 230 mean train loss:  6.77988864e-05, mean val. loss:  4.56302494e-01\n",
      "Epoch: 231 mean train loss:  6.65130210e-05, mean val. loss:  4.56797928e-01\n",
      "Epoch: 232 mean train loss:  6.74143957e-05, mean val. loss:  4.57291067e-01\n",
      "Epoch: 233 mean train loss:  6.76042109e-05, mean val. loss:  4.57798451e-01\n",
      "Epoch: 234 mean train loss:  6.77612552e-05, mean val. loss:  4.58291948e-01\n",
      "Epoch: 235 mean train loss:  6.83645776e-05, mean val. loss:  4.58792180e-01\n",
      "Epoch: 236 mean train loss:  6.76078489e-05, mean val. loss:  4.59295094e-01\n",
      "Epoch: 237 mean train loss:  6.71856978e-05, mean val. loss:  4.59796697e-01\n",
      "Epoch: 238 mean train loss:  6.79368386e-05, mean val. loss:  4.60300952e-01\n",
      "Epoch: 239 mean train loss:  6.77146018e-05, mean val. loss:  4.60797489e-01\n",
      "Epoch: 240 mean train loss:  6.70951558e-05, mean val. loss:  4.61304009e-01\n",
      "Epoch: 241 mean train loss:  6.60532678e-05, mean val. loss:  4.61813688e-01\n",
      "Epoch: 242 mean train loss:  6.83446415e-05, mean val. loss:  4.62313205e-01\n",
      "Epoch: 243 mean train loss:  6.77540957e-05, mean val. loss:  4.62816417e-01\n",
      "Epoch: 244 mean train loss:  6.68329012e-05, mean val. loss:  4.63322580e-01\n",
      "Epoch: 245 mean train loss:  6.63968676e-05, mean val. loss:  4.63825077e-01\n",
      "Epoch: 246 mean train loss:  6.60890073e-05, mean val. loss:  4.64336216e-01\n",
      "Epoch: 247 mean train loss:  6.68781286e-05, mean val. loss:  4.64844644e-01\n",
      "Epoch: 248 mean train loss:  6.71276648e-05, mean val. loss:  4.65349674e-01\n",
      "Epoch: 249 mean train loss:  6.60398218e-05, mean val. loss:  4.65864301e-01\n",
      "Epoch: 250 mean train loss:  6.68614812e-05, mean val. loss:  4.66379434e-01\n",
      "Epoch: 251 mean train loss:  6.61688973e-05, mean val. loss:  4.66898024e-01\n",
      "Epoch: 252 mean train loss:  6.70026930e-05, mean val. loss:  4.67413247e-01\n",
      "Epoch: 253 mean train loss:  6.79804070e-05, mean val. loss:  4.67918158e-01\n",
      "Epoch: 254 mean train loss:  6.63870305e-05, mean val. loss:  4.68430996e-01\n",
      "Epoch: 255 mean train loss:  6.58764911e-05, mean val. loss:  4.68934864e-01\n",
      "Epoch: 256 mean train loss:  6.71854650e-05, mean val. loss:  4.69441921e-01\n",
      "Epoch: 257 mean train loss:  6.65060361e-05, mean val. loss:  4.69945163e-01\n",
      "Epoch: 258 mean train loss:  6.50091970e-05, mean val. loss:  4.70454544e-01\n",
      "Epoch: 259 mean train loss:  6.56507618e-05, mean val. loss:  4.70971912e-01\n",
      "Epoch: 260 mean train loss:  6.67863933e-05, mean val. loss:  4.71480787e-01\n",
      "Epoch: 261 mean train loss:  6.59867073e-05, mean val. loss:  4.71999675e-01\n",
      "Epoch: 262 mean train loss:  6.57263445e-05, mean val. loss:  4.72511262e-01\n",
      "Epoch: 263 mean train loss:  6.64631953e-05, mean val. loss:  4.73022103e-01\n",
      "Epoch: 264 mean train loss:  6.62310922e-05, mean val. loss:  4.73539978e-01\n",
      "Epoch: 265 mean train loss:  6.60722726e-05, mean val. loss:  4.74052578e-01\n",
      "Epoch: 266 mean train loss:  6.66139531e-05, mean val. loss:  4.74565417e-01\n",
      "Epoch: 267 mean train loss:  6.70707377e-05, mean val. loss:  4.75080609e-01\n",
      "Epoch: 268 mean train loss:  6.59306825e-05, mean val. loss:  4.75594580e-01\n",
      "Epoch: 269 mean train loss:  6.59626385e-05, mean val. loss:  4.76105720e-01\n",
      "Epoch: 270 mean train loss:  6.68787688e-05, mean val. loss:  4.76614445e-01\n",
      "Epoch: 271 mean train loss:  6.52872259e-05, mean val. loss:  4.77128685e-01\n",
      "Epoch: 272 mean train loss:  6.59763755e-05, mean val. loss:  4.77646321e-01\n",
      "Epoch: 273 mean train loss:  6.52346353e-05, mean val. loss:  4.78172272e-01\n",
      "Epoch: 274 mean train loss:  6.31091534e-05, mean val. loss:  4.78695393e-01\n",
      "Epoch: 275 mean train loss:  6.55737822e-05, mean val. loss:  4.79217350e-01\n",
      "Epoch: 276 mean train loss:  6.60564692e-05, mean val. loss:  4.79742914e-01\n",
      "Epoch: 277 mean train loss:  6.55337062e-05, mean val. loss:  4.80259210e-01\n",
      "Epoch: 278 mean train loss:  6.51665323e-05, mean val. loss:  4.80778277e-01\n",
      "Epoch: 279 mean train loss:  6.52561430e-05, mean val. loss:  4.81296241e-01\n",
      "Epoch: 280 mean train loss:  6.52029703e-05, mean val. loss:  4.81815279e-01\n",
      "Epoch: 281 mean train loss:  6.59806246e-05, mean val. loss:  4.82330412e-01\n",
      "Epoch: 282 mean train loss:  6.55239273e-05, mean val. loss:  4.82846618e-01\n",
      "Epoch: 283 mean train loss:  6.54064643e-05, mean val. loss:  4.83361274e-01\n",
      "Epoch: 284 mean train loss:  6.46625122e-05, mean val. loss:  4.83884782e-01\n",
      "Epoch: 285 mean train loss:  6.42683590e-05, mean val. loss:  4.84409899e-01\n",
      "Epoch: 286 mean train loss:  6.51559094e-05, mean val. loss:  4.84930158e-01\n",
      "Epoch: 287 mean train loss:  6.46655972e-05, mean val. loss:  4.85453218e-01\n",
      "Epoch: 288 mean train loss:  6.46972039e-05, mean val. loss:  4.85977322e-01\n",
      "Epoch: 289 mean train loss:  6.50026777e-05, mean val. loss:  4.86499399e-01\n",
      "Epoch: 290 mean train loss:  6.49544818e-05, mean val. loss:  4.87022012e-01\n",
      "Epoch: 291 mean train loss:  6.46236294e-05, mean val. loss:  4.87543523e-01\n",
      "Epoch: 292 mean train loss:  6.50040165e-05, mean val. loss:  4.88063455e-01\n",
      "Epoch: 293 mean train loss:  6.43346284e-05, mean val. loss:  4.88577813e-01\n",
      "Epoch: 294 mean train loss:  6.53972384e-05, mean val. loss:  4.89100069e-01\n",
      "Epoch: 295 mean train loss:  6.38926285e-05, mean val. loss:  4.89627421e-01\n",
      "Epoch: 296 mean train loss:  6.43562234e-05, mean val. loss:  4.90142643e-01\n",
      "Epoch: 297 mean train loss:  6.44173706e-05, mean val. loss:  4.90666777e-01\n",
      "Epoch: 298 mean train loss:  6.49337017e-05, mean val. loss:  4.91191626e-01\n",
      "Epoch: 299 mean train loss:  6.44371612e-05, mean val. loss:  4.91714746e-01\n",
      "Epoch: 300 mean train loss:  6.37999037e-05, mean val. loss:  4.92229551e-01\n",
      "Epoch: 301 mean train loss:  6.33417221e-05, mean val. loss:  4.92753565e-01\n",
      "Epoch: 302 mean train loss:  6.45506079e-05, mean val. loss:  4.93268698e-01\n",
      "Epoch: 303 mean train loss:  6.35512115e-05, mean val. loss:  4.93794501e-01\n",
      "Epoch: 304 mean train loss:  6.50058500e-05, mean val. loss:  4.94311094e-01\n",
      "Epoch: 305 mean train loss:  6.40148646e-05, mean val. loss:  4.94833499e-01\n",
      "Epoch: 306 mean train loss:  6.46430126e-05, mean val. loss:  4.95350868e-01\n",
      "Epoch: 307 mean train loss:  6.37241174e-05, mean val. loss:  4.95875835e-01\n",
      "Epoch: 308 mean train loss:  6.43671665e-05, mean val. loss:  4.96394306e-01\n",
      "Epoch: 309 mean train loss:  6.38294732e-05, mean val. loss:  4.96918797e-01\n",
      "Epoch: 310 mean train loss:  6.30273134e-05, mean val. loss:  4.97437835e-01\n",
      "Epoch: 311 mean train loss:  6.42328232e-05, mean val. loss:  4.97960150e-01\n",
      "Epoch: 312 mean train loss:  6.36894838e-05, mean val. loss:  4.98477817e-01\n",
      "Epoch: 313 mean train loss:  6.20209030e-05, mean val. loss:  4.99005586e-01\n",
      "Epoch: 314 mean train loss:  6.31438161e-05, mean val. loss:  4.99529928e-01\n",
      "Epoch: 315 mean train loss:  6.42616942e-05, mean val. loss:  5.00061989e-01\n",
      "Epoch: 316 mean train loss:  6.36020559e-05, mean val. loss:  5.00585079e-01\n",
      "Epoch: 317 mean train loss:  6.42543600e-05, mean val. loss:  5.01115978e-01\n",
      "Epoch: 318 mean train loss:  6.34314492e-05, mean val. loss:  5.01637876e-01\n",
      "Epoch: 319 mean train loss:  6.39146601e-05, mean val. loss:  5.02167344e-01\n",
      "Epoch: 320 mean train loss:  6.35470205e-05, mean val. loss:  5.02687395e-01\n",
      "Epoch: 321 mean train loss:  6.39244681e-05, mean val. loss:  5.03216207e-01\n",
      "Epoch: 322 mean train loss:  6.37764169e-05, mean val. loss:  5.03734052e-01\n",
      "Epoch: 323 mean train loss:  6.39023492e-05, mean val. loss:  5.04251361e-01\n",
      "Epoch: 324 mean train loss:  6.39776408e-05, mean val. loss:  5.04765570e-01\n",
      "Epoch: 325 mean train loss:  6.35704491e-05, mean val. loss:  5.05282104e-01\n",
      "Epoch: 326 mean train loss:  6.33054879e-05, mean val. loss:  5.05794048e-01\n",
      "Epoch: 327 mean train loss:  6.31434086e-05, mean val. loss:  5.06304622e-01\n",
      "Epoch: 328 mean train loss:  6.33450109e-05, mean val. loss:  5.06816626e-01\n",
      "Epoch: 329 mean train loss:  6.21101935e-05, mean val. loss:  5.07330120e-01\n",
      "Epoch: 330 mean train loss:  6.32074371e-05, mean val. loss:  5.07842779e-01\n",
      "Epoch: 331 mean train loss:  6.22137450e-05, mean val. loss:  5.08358121e-01\n",
      "Epoch: 332 mean train loss:  6.30460563e-05, mean val. loss:  5.08862078e-01\n",
      "Epoch: 333 mean train loss:  6.20245992e-05, mean val. loss:  5.09381056e-01\n",
      "Epoch: 334 mean train loss:  6.23512897e-05, mean val. loss:  5.09886503e-01\n",
      "Epoch: 335 mean train loss:  6.26651745e-05, mean val. loss:  5.10407805e-01\n",
      "Epoch: 336 mean train loss:  6.28939015e-05, mean val. loss:  5.10919034e-01\n",
      "Epoch: 337 mean train loss:  6.25174434e-05, mean val. loss:  5.11446834e-01\n",
      "Epoch: 338 mean train loss:  6.20358915e-05, mean val. loss:  5.11963248e-01\n",
      "Epoch: 339 mean train loss:  6.30998402e-05, mean val. loss:  5.12492955e-01\n",
      "Epoch: 340 mean train loss:  6.28088892e-05, mean val. loss:  5.13005435e-01\n",
      "Epoch: 341 mean train loss:  6.21364452e-05, mean val. loss:  5.13537288e-01\n",
      "Epoch: 342 mean train loss:  6.29680580e-05, mean val. loss:  5.14052153e-01\n",
      "Epoch: 343 mean train loss:  6.37357298e-05, mean val. loss:  5.14570415e-01\n",
      "Epoch: 344 mean train loss:  6.29872084e-05, mean val. loss:  5.15086591e-01\n",
      "Epoch: 345 mean train loss:  6.24871755e-05, mean val. loss:  5.15603960e-01\n",
      "Epoch: 346 mean train loss:  6.23946544e-05, mean val. loss:  5.16120851e-01\n",
      "Epoch: 347 mean train loss:  6.25963730e-05, mean val. loss:  5.16625345e-01\n",
      "Epoch: 348 mean train loss:  6.12223521e-05, mean val. loss:  5.17141879e-01\n",
      "Epoch: 349 mean train loss:  6.12903677e-05, mean val. loss:  5.17660260e-01\n",
      "Epoch: 350 mean train loss:  6.21320214e-05, mean val. loss:  5.18170893e-01\n",
      "Epoch: 351 mean train loss:  6.21889485e-05, mean val. loss:  5.18686831e-01\n",
      "Epoch: 352 mean train loss:  6.15274475e-05, mean val. loss:  5.19201040e-01\n",
      "Epoch: 353 mean train loss:  6.25680841e-05, mean val. loss:  5.19706011e-01\n",
      "Epoch: 354 mean train loss:  6.17375772e-05, mean val. loss:  5.20211041e-01\n",
      "Epoch: 355 mean train loss:  6.19853963e-05, mean val. loss:  5.20726442e-01\n",
      "Epoch: 356 mean train loss:  6.20981446e-05, mean val. loss:  5.21241009e-01\n",
      "Epoch: 357 mean train loss:  6.28913112e-05, mean val. loss:  5.21746278e-01\n",
      "Epoch: 358 mean train loss:  6.06403628e-05, mean val. loss:  5.22252738e-01\n",
      "Epoch: 359 mean train loss:  6.13449665e-05, mean val. loss:  5.22758782e-01\n",
      "Epoch: 360 mean train loss:  6.23651722e-05, mean val. loss:  5.23262978e-01\n",
      "Epoch: 361 mean train loss:  6.22761145e-05, mean val. loss:  5.23769200e-01\n",
      "Epoch: 362 mean train loss:  6.22598163e-05, mean val. loss:  5.24264216e-01\n",
      "Epoch: 363 mean train loss:  6.13262237e-05, mean val. loss:  5.24754822e-01\n",
      "Epoch: 364 mean train loss:  6.18577178e-05, mean val. loss:  5.25255620e-01\n",
      "Epoch: 365 mean train loss:  6.03473454e-05, mean val. loss:  5.25754988e-01\n",
      "Epoch: 366 mean train loss:  6.19380153e-05, mean val. loss:  5.26256859e-01\n",
      "Epoch: 367 mean train loss:  6.22289081e-05, mean val. loss:  5.26765466e-01\n",
      "Epoch: 368 mean train loss:  6.20823121e-05, mean val. loss:  5.27264714e-01\n",
      "Epoch: 369 mean train loss:  6.14893215e-05, mean val. loss:  5.27767599e-01\n",
      "Epoch: 370 mean train loss:  6.14366436e-05, mean val. loss:  5.28262675e-01\n",
      "Epoch: 371 mean train loss:  6.15844910e-05, mean val. loss:  5.28746665e-01\n",
      "Epoch: 372 mean train loss:  6.18972990e-05, mean val. loss:  5.29238999e-01\n",
      "Epoch: 373 mean train loss:  6.12556760e-05, mean val. loss:  5.29735208e-01\n",
      "Epoch: 374 mean train loss:  6.09259296e-05, mean val. loss:  5.30226409e-01\n",
      "Epoch: 375 mean train loss:  6.14595192e-05, mean val. loss:  5.30728936e-01\n",
      "Epoch: 376 mean train loss:  6.12559961e-05, mean val. loss:  5.31231403e-01\n",
      "Epoch: 377 mean train loss:  6.11715950e-05, mean val. loss:  5.31731546e-01\n",
      "Epoch: 378 mean train loss:  6.04293018e-05, mean val. loss:  5.32235146e-01\n",
      "Epoch: 379 mean train loss:  6.06862886e-05, mean val. loss:  5.32741845e-01\n",
      "Epoch: 380 mean train loss:  6.12572476e-05, mean val. loss:  5.33247471e-01\n",
      "Epoch: 381 mean train loss:  6.11831783e-05, mean val. loss:  5.33753574e-01\n",
      "Epoch: 382 mean train loss:  5.96859318e-05, mean val. loss:  5.34263909e-01\n",
      "Epoch: 383 mean train loss:  6.10951683e-05, mean val. loss:  5.34769416e-01\n",
      "Epoch: 384 mean train loss:  6.07138209e-05, mean val. loss:  5.35276234e-01\n",
      "Epoch: 385 mean train loss:  6.06925460e-05, mean val. loss:  5.35789192e-01\n",
      "Epoch: 386 mean train loss:  6.00203348e-05, mean val. loss:  5.36303818e-01\n",
      "Epoch: 387 mean train loss:  6.09894050e-05, mean val. loss:  5.36816537e-01\n",
      "Epoch: 388 mean train loss:  6.07240945e-05, mean val. loss:  5.37331164e-01\n",
      "Epoch: 389 mean train loss:  6.01742649e-05, mean val. loss:  5.37844837e-01\n",
      "Epoch: 390 mean train loss:  6.04595698e-05, mean val. loss:  5.38358033e-01\n",
      "Epoch: 391 mean train loss:  6.02158834e-05, mean val. loss:  5.38871408e-01\n",
      "Epoch: 392 mean train loss:  6.08067494e-05, mean val. loss:  5.39383590e-01\n",
      "Epoch: 393 mean train loss:  6.09902199e-05, mean val. loss:  5.39888382e-01\n",
      "Epoch: 394 mean train loss:  6.11480209e-05, mean val. loss:  5.40387273e-01\n",
      "Epoch: 395 mean train loss:  5.98274055e-05, mean val. loss:  5.40885329e-01\n",
      "Epoch: 396 mean train loss:  6.06412068e-05, mean val. loss:  5.41379869e-01\n",
      "Epoch: 397 mean train loss:  6.04985980e-05, mean val. loss:  5.41866124e-01\n",
      "Epoch: 398 mean train loss:  6.05829991e-05, mean val. loss:  5.42358935e-01\n",
      "Epoch: 399 mean train loss:  6.06013928e-05, mean val. loss:  5.42844117e-01\n",
      "Epoch: 400 mean train loss:  5.96541504e-05, mean val. loss:  5.43341279e-01\n",
      "Epoch: 401 mean train loss:  6.06322428e-05, mean val. loss:  5.43842912e-01\n",
      "Epoch: 402 mean train loss:  6.06912945e-05, mean val. loss:  5.44346154e-01\n",
      "Epoch: 403 mean train loss:  6.00359926e-05, mean val. loss:  5.44840097e-01\n",
      "Epoch: 404 mean train loss:  5.95456513e-05, mean val. loss:  5.45340776e-01\n",
      "Epoch: 405 mean train loss:  6.04478118e-05, mean val. loss:  5.45834184e-01\n",
      "Epoch: 406 mean train loss:  6.00100611e-05, mean val. loss:  5.46330035e-01\n",
      "Epoch: 407 mean train loss:  6.08928967e-05, mean val. loss:  5.46826243e-01\n",
      "Epoch: 408 mean train loss:  6.02387008e-05, mean val. loss:  5.47313929e-01\n",
      "Epoch: 409 mean train loss:  6.10380084e-05, mean val. loss:  5.47796845e-01\n",
      "Epoch: 410 mean train loss:  5.95259771e-05, mean val. loss:  5.48288763e-01\n",
      "Epoch: 411 mean train loss:  6.02986256e-05, mean val. loss:  5.48774600e-01\n",
      "Epoch: 412 mean train loss:  6.03976368e-05, mean val. loss:  5.49262404e-01\n",
      "Epoch: 413 mean train loss:  5.99411724e-05, mean val. loss:  5.49748719e-01\n",
      "Epoch: 414 mean train loss:  5.99898049e-05, mean val. loss:  5.50232768e-01\n",
      "Epoch: 415 mean train loss:  5.95125020e-05, mean val. loss:  5.50722003e-01\n",
      "Epoch: 416 mean train loss:  5.89354604e-05, mean val. loss:  5.51198542e-01\n",
      "Epoch: 417 mean train loss:  5.99042105e-05, mean val. loss:  5.51691115e-01\n",
      "Epoch: 418 mean train loss:  5.96592727e-05, mean val. loss:  5.52167296e-01\n",
      "Epoch: 419 mean train loss:  6.06330868e-05, mean val. loss:  5.52659929e-01\n",
      "Epoch: 420 mean train loss:  5.87960822e-05, mean val. loss:  5.53141057e-01\n",
      "Epoch: 421 mean train loss:  5.86348469e-05, mean val. loss:  5.53640485e-01\n",
      "Epoch: 422 mean train loss:  5.87159884e-05, mean val. loss:  5.54142177e-01\n",
      "Epoch: 423 mean train loss:  5.95593301e-05, mean val. loss:  5.54642260e-01\n",
      "Epoch: 424 mean train loss:  5.96495811e-05, mean val. loss:  5.55140913e-01\n",
      "Epoch: 425 mean train loss:  5.85213711e-05, mean val. loss:  5.55637658e-01\n",
      "Epoch: 426 mean train loss:  5.98107872e-05, mean val. loss:  5.56127191e-01\n",
      "Epoch: 427 mean train loss:  5.97263279e-05, mean val. loss:  5.56617260e-01\n",
      "Epoch: 428 mean train loss:  6.02365180e-05, mean val. loss:  5.57094157e-01\n",
      "Epoch: 429 mean train loss:  5.90188720e-05, mean val. loss:  5.57576478e-01\n",
      "Epoch: 430 mean train loss:  5.90197451e-05, mean val. loss:  5.58060884e-01\n",
      "Epoch: 431 mean train loss:  5.89102565e-05, mean val. loss:  5.58540642e-01\n",
      "Epoch: 432 mean train loss:  5.83856017e-05, mean val. loss:  5.59031308e-01\n",
      "Epoch: 433 mean train loss:  5.97767066e-05, mean val. loss:  5.59524536e-01\n",
      "Epoch: 434 mean train loss:  5.92907891e-05, mean val. loss:  5.60015202e-01\n",
      "Epoch: 435 mean train loss:  5.86386886e-05, mean val. loss:  5.60507834e-01\n",
      "Epoch: 436 mean train loss:  5.97052567e-05, mean val. loss:  5.60992002e-01\n",
      "Epoch: 437 mean train loss:  5.78279141e-05, mean val. loss:  5.61484873e-01\n",
      "Epoch: 438 mean train loss:  5.89383126e-05, mean val. loss:  5.61974466e-01\n",
      "Epoch: 439 mean train loss:  5.93311852e-05, mean val. loss:  5.62453568e-01\n",
      "Epoch: 440 mean train loss:  5.82327775e-05, mean val. loss:  5.62931716e-01\n",
      "Epoch: 441 mean train loss:  5.91650314e-05, mean val. loss:  5.63416839e-01\n",
      "Epoch: 442 mean train loss:  5.89716656e-05, mean val. loss:  5.63901961e-01\n",
      "Epoch: 443 mean train loss:  5.85202069e-05, mean val. loss:  5.64381838e-01\n",
      "Epoch: 444 mean train loss:  5.82154607e-05, mean val. loss:  5.64869165e-01\n",
      "Epoch: 445 mean train loss:  5.91433200e-05, mean val. loss:  5.65354645e-01\n",
      "Epoch: 446 mean train loss:  5.85432572e-05, mean val. loss:  5.65833747e-01\n",
      "Epoch: 447 mean train loss:  5.87939867e-05, mean val. loss:  5.66315532e-01\n",
      "Epoch: 448 mean train loss:  5.92661963e-05, mean val. loss:  5.66804886e-01\n",
      "Epoch: 449 mean train loss:  5.87337126e-05, mean val. loss:  5.67292511e-01\n",
      "Epoch: 450 mean train loss:  5.96414611e-05, mean val. loss:  5.67762434e-01\n",
      "Epoch: 451 mean train loss:  5.73151337e-05, mean val. loss:  5.68236828e-01\n",
      "Epoch: 452 mean train loss:  5.90508571e-05, mean val. loss:  5.68717062e-01\n",
      "Epoch: 453 mean train loss:  5.77737228e-05, mean val. loss:  5.69190443e-01\n",
      "Epoch: 454 mean train loss:  5.75302402e-05, mean val. loss:  5.69673538e-01\n",
      "Epoch: 455 mean train loss:  5.74704900e-05, mean val. loss:  5.70153236e-01\n",
      "Epoch: 456 mean train loss:  5.77280880e-05, mean val. loss:  5.70639312e-01\n",
      "Epoch: 457 mean train loss:  5.73807629e-05, mean val. loss:  5.71122706e-01\n",
      "Epoch: 458 mean train loss:  5.72424324e-05, mean val. loss:  5.71614385e-01\n",
      "Epoch: 459 mean train loss:  5.82848443e-05, mean val. loss:  5.72096467e-01\n",
      "Epoch: 460 mean train loss:  5.90621494e-05, mean val. loss:  5.72574437e-01\n",
      "Epoch: 461 mean train loss:  5.93019067e-05, mean val. loss:  5.73051870e-01\n",
      "Epoch: 462 mean train loss:  5.90236159e-05, mean val. loss:  5.73534310e-01\n",
      "Epoch: 463 mean train loss:  5.77069004e-05, mean val. loss:  5.74009836e-01\n",
      "Epoch: 464 mean train loss:  5.80956694e-05, mean val. loss:  5.74481606e-01\n",
      "Epoch: 465 mean train loss:  5.90180862e-05, mean val. loss:  5.74953020e-01\n",
      "Epoch: 466 mean train loss:  5.73769503e-05, mean val. loss:  5.75424254e-01\n",
      "Epoch: 467 mean train loss:  5.94907615e-05, mean val. loss:  5.75892031e-01\n",
      "Epoch: 468 mean train loss:  5.68453106e-05, mean val. loss:  5.76364219e-01\n",
      "Epoch: 469 mean train loss:  5.70215052e-05, mean val. loss:  5.76836646e-01\n",
      "Epoch: 470 mean train loss:  5.81676140e-05, mean val. loss:  5.77309430e-01\n",
      "Epoch: 471 mean train loss:  5.86718088e-05, mean val. loss:  5.77783287e-01\n",
      "Epoch: 472 mean train loss:  5.74655132e-05, mean val. loss:  5.78256071e-01\n",
      "Epoch: 473 mean train loss:  5.72380959e-05, mean val. loss:  5.78729451e-01\n",
      "Epoch: 474 mean train loss:  5.77135652e-05, mean val. loss:  5.79198897e-01\n",
      "Epoch: 475 mean train loss:  5.70426928e-05, mean val. loss:  5.79669833e-01\n",
      "Epoch: 476 mean train loss:  5.78796607e-05, mean val. loss:  5.80138624e-01\n",
      "Epoch: 477 mean train loss:  5.69881522e-05, mean val. loss:  5.80607235e-01\n",
      "Epoch: 478 mean train loss:  5.76353632e-05, mean val. loss:  5.81081748e-01\n",
      "Epoch: 479 mean train loss:  5.64773218e-05, mean val. loss:  5.81552923e-01\n",
      "Epoch: 480 mean train loss:  5.63628564e-05, mean val. loss:  5.82019985e-01\n",
      "Epoch: 481 mean train loss:  5.78631589e-05, mean val. loss:  5.82492948e-01\n",
      "Epoch: 482 mean train loss:  5.78437175e-05, mean val. loss:  5.82968295e-01\n",
      "Epoch: 483 mean train loss:  5.74363803e-05, mean val. loss:  5.83448052e-01\n",
      "Epoch: 484 mean train loss:  5.76752645e-05, mean val. loss:  5.83922923e-01\n",
      "Epoch: 485 mean train loss:  5.71535784e-05, mean val. loss:  5.84397852e-01\n",
      "Epoch: 486 mean train loss:  5.74617297e-05, mean val. loss:  5.84872246e-01\n",
      "Epoch: 487 mean train loss:  5.84860099e-05, mean val. loss:  5.85334957e-01\n",
      "Epoch: 488 mean train loss:  5.64557267e-05, mean val. loss:  5.85805416e-01\n",
      "Epoch: 489 mean train loss:  5.80496271e-05, mean val. loss:  5.86276114e-01\n",
      "Epoch: 490 mean train loss:  5.75031736e-05, mean val. loss:  5.86738408e-01\n",
      "Epoch: 491 mean train loss:  5.81775093e-05, mean val. loss:  5.87205887e-01\n",
      "Epoch: 492 mean train loss:  5.66670205e-05, mean val. loss:  5.87669253e-01\n",
      "Epoch: 493 mean train loss:  5.63072390e-05, mean val. loss:  5.88127673e-01\n",
      "Epoch: 494 mean train loss:  5.73967118e-05, mean val. loss:  5.88583350e-01\n",
      "Epoch: 495 mean train loss:  5.71107666e-05, mean val. loss:  5.89051723e-01\n",
      "Epoch: 496 mean train loss:  5.72809367e-05, mean val. loss:  5.89505374e-01\n",
      "Epoch: 497 mean train loss:  5.73761354e-05, mean val. loss:  5.89958429e-01\n",
      "Epoch: 498 mean train loss:  5.66263334e-05, mean val. loss:  5.90422571e-01\n",
      "Epoch: 499 mean train loss:  5.63790381e-05, mean val. loss:  5.90878665e-01\n",
      "Epoch: 500 mean train loss:  5.66496165e-05, mean val. loss:  5.91331363e-01\n",
      "Epoch: 501 mean train loss:  5.75533195e-05, mean val. loss:  5.91787159e-01\n",
      "Epoch: 502 mean train loss:  5.70188276e-05, mean val. loss:  5.92227697e-01\n",
      "Epoch: 503 mean train loss:  5.64211223e-05, mean val. loss:  5.92680812e-01\n",
      "Epoch: 504 mean train loss:  5.70031116e-05, mean val. loss:  5.93126893e-01\n",
      "Epoch: 505 mean train loss:  5.65669907e-05, mean val. loss:  5.93583167e-01\n",
      "Epoch: 506 mean train loss:  5.58628235e-05, mean val. loss:  5.94029725e-01\n",
      "Epoch: 507 mean train loss:  5.62141649e-05, mean val. loss:  5.94493747e-01\n",
      "Epoch: 508 mean train loss:  5.70792763e-05, mean val. loss:  5.94950676e-01\n",
      "Epoch: 509 mean train loss:  5.73237776e-05, mean val. loss:  5.95408380e-01\n",
      "Epoch: 510 mean train loss:  5.52259735e-05, mean val. loss:  5.95870495e-01\n",
      "Epoch: 511 mean train loss:  5.64977527e-05, mean val. loss:  5.96329510e-01\n",
      "Epoch: 512 mean train loss:  5.69471449e-05, mean val. loss:  5.96785009e-01\n",
      "Epoch: 513 mean train loss:  5.74887672e-05, mean val. loss:  5.97237289e-01\n",
      "Epoch: 514 mean train loss:  5.67298557e-05, mean val. loss:  5.97689569e-01\n",
      "Epoch: 515 mean train loss:  5.53290593e-05, mean val. loss:  5.98146617e-01\n",
      "Epoch: 516 mean train loss:  5.60193439e-05, mean val. loss:  5.98604739e-01\n",
      "Epoch: 517 mean train loss:  5.59717009e-05, mean val. loss:  5.99065542e-01\n",
      "Epoch: 518 mean train loss:  5.66274393e-05, mean val. loss:  5.99525273e-01\n",
      "Epoch: 519 mean train loss:  5.75187150e-05, mean val. loss:  5.99980414e-01\n",
      "Epoch: 520 mean train loss:  5.77626051e-05, mean val. loss:  6.00423157e-01\n",
      "Epoch: 521 mean train loss:  5.78122563e-05, mean val. loss:  6.00868106e-01\n",
      "Epoch: 522 mean train loss:  5.70456905e-05, mean val. loss:  6.01312995e-01\n",
      "Epoch: 523 mean train loss:  5.70797711e-05, mean val. loss:  6.01755559e-01\n",
      "Epoch: 524 mean train loss:  5.64771472e-05, mean val. loss:  6.02182925e-01\n",
      "Epoch: 525 mean train loss:  5.73470315e-05, mean val. loss:  6.02617085e-01\n",
      "Epoch: 526 mean train loss:  5.59576438e-05, mean val. loss:  6.03039801e-01\n",
      "Epoch: 527 mean train loss:  5.67329116e-05, mean val. loss:  6.03455842e-01\n",
      "Epoch: 528 mean train loss:  5.63103822e-05, mean val. loss:  6.03880286e-01\n",
      "Epoch: 529 mean train loss:  5.56584564e-05, mean val. loss:  6.04298532e-01\n",
      "Epoch: 530 mean train loss:  5.48152311e-05, mean val. loss:  6.04728162e-01\n",
      "Epoch: 531 mean train loss:  5.58717293e-05, mean val. loss:  6.05157971e-01\n",
      "Epoch: 532 mean train loss:  5.63362264e-05, mean val. loss:  6.05585396e-01\n",
      "Epoch: 533 mean train loss:  5.59648615e-05, mean val. loss:  6.06024027e-01\n",
      "Epoch: 534 mean train loss:  5.54802536e-05, mean val. loss:  6.06464207e-01\n",
      "Epoch: 535 mean train loss:  5.56982704e-05, mean val. loss:  6.06896400e-01\n",
      "Epoch: 536 mean train loss:  5.65047958e-05, mean val. loss:  6.07335389e-01\n",
      "Epoch: 537 mean train loss:  5.64531656e-05, mean val. loss:  6.07773066e-01\n",
      "Epoch: 538 mean train loss:  5.58640168e-05, mean val. loss:  6.08211696e-01\n",
      "Epoch: 539 mean train loss:  5.56485029e-05, mean val. loss:  6.08646631e-01\n",
      "Epoch: 540 mean train loss:  5.58146858e-05, mean val. loss:  6.09079123e-01\n",
      "Epoch: 541 mean train loss:  5.58560132e-05, mean val. loss:  6.09503090e-01\n",
      "Epoch: 542 mean train loss:  5.50672412e-05, mean val. loss:  6.09929681e-01\n",
      "Epoch: 543 mean train loss:  5.52730053e-05, mean val. loss:  6.10356510e-01\n",
      "Epoch: 544 mean train loss:  5.55086299e-05, mean val. loss:  6.10790133e-01\n",
      "Epoch: 545 mean train loss:  5.50900004e-05, mean val. loss:  6.11219764e-01\n",
      "Epoch: 546 mean train loss:  5.59022301e-05, mean val. loss:  6.11653149e-01\n",
      "Epoch: 547 mean train loss:  5.44133363e-05, mean val. loss:  6.12085938e-01\n",
      "Epoch: 548 mean train loss:  5.51147969e-05, mean val. loss:  6.12521648e-01\n",
      "Epoch: 549 mean train loss:  5.48736716e-05, mean val. loss:  6.12957418e-01\n",
      "Epoch: 550 mean train loss:  5.49395918e-05, mean val. loss:  6.13394380e-01\n",
      "Epoch: 551 mean train loss:  5.61030465e-05, mean val. loss:  6.13827169e-01\n",
      "Epoch: 552 mean train loss:  5.58870379e-05, mean val. loss:  6.14262938e-01\n",
      "Epoch: 553 mean train loss:  5.61288616e-05, mean val. loss:  6.14694834e-01\n",
      "Epoch: 554 mean train loss:  5.50069963e-05, mean val. loss:  6.15128756e-01\n",
      "Epoch: 555 mean train loss:  5.68931573e-05, mean val. loss:  6.15559578e-01\n",
      "Epoch: 556 mean train loss:  5.59370383e-05, mean val. loss:  6.15985513e-01\n",
      "Epoch: 557 mean train loss:  5.51754201e-05, mean val. loss:  6.16414666e-01\n",
      "Epoch: 558 mean train loss:  5.45693038e-05, mean val. loss:  6.16847813e-01\n",
      "Epoch: 559 mean train loss:  5.48747485e-05, mean val. loss:  6.17281973e-01\n",
      "Epoch: 560 mean train loss:  5.45467774e-05, mean val. loss:  6.17719948e-01\n",
      "Epoch: 561 mean train loss:  5.55683509e-05, mean val. loss:  6.18156374e-01\n",
      "Epoch: 562 mean train loss:  5.57806925e-05, mean val. loss:  6.18589759e-01\n",
      "Epoch: 563 mean train loss:  5.39219473e-05, mean val. loss:  6.19021177e-01\n",
      "Epoch: 564 mean train loss:  5.38740132e-05, mean val. loss:  6.19459927e-01\n",
      "Epoch: 565 mean train loss:  5.46698866e-05, mean val. loss:  6.19892776e-01\n",
      "Epoch: 566 mean train loss:  5.47913078e-05, mean val. loss:  6.20322406e-01\n",
      "Epoch: 567 mean train loss:  5.55991428e-05, mean val. loss:  6.20755970e-01\n",
      "Epoch: 568 mean train loss:  5.42449998e-05, mean val. loss:  6.21189177e-01\n",
      "Epoch: 569 mean train loss:  5.52444835e-05, mean val. loss:  6.21616721e-01\n",
      "Epoch: 570 mean train loss:  5.43014612e-05, mean val. loss:  6.22047722e-01\n",
      "Epoch: 571 mean train loss:  5.45083312e-05, mean val. loss:  6.22477174e-01\n",
      "Epoch: 572 mean train loss:  5.41111222e-05, mean val. loss:  6.22901201e-01\n",
      "Epoch: 573 mean train loss:  5.41618501e-05, mean val. loss:  6.23333514e-01\n",
      "Epoch: 574 mean train loss:  5.41352201e-05, mean val. loss:  6.23767912e-01\n",
      "Epoch: 575 mean train loss:  5.45767834e-05, mean val. loss:  6.24192357e-01\n",
      "Epoch: 576 mean train loss:  5.39786706e-05, mean val. loss:  6.24627411e-01\n",
      "Epoch: 577 mean train loss:  5.47295786e-05, mean val. loss:  6.25061154e-01\n",
      "Epoch: 578 mean train loss:  5.38247987e-05, mean val. loss:  6.25500083e-01\n",
      "Epoch: 579 mean train loss:  5.47953532e-05, mean val. loss:  6.25928164e-01\n",
      "Epoch: 580 mean train loss:  5.49000688e-05, mean val. loss:  6.26363754e-01\n",
      "Epoch: 581 mean train loss:  5.43058268e-05, mean val. loss:  6.26796484e-01\n",
      "Epoch: 582 mean train loss:  5.43948845e-05, mean val. loss:  6.27217352e-01\n",
      "Epoch: 583 mean train loss:  5.45956427e-05, mean val. loss:  6.27646983e-01\n",
      "Epoch: 584 mean train loss:  5.58247266e-05, mean val. loss:  6.28065407e-01\n",
      "Epoch: 585 mean train loss:  5.53270802e-05, mean val. loss:  6.28485322e-01\n",
      "Epoch: 586 mean train loss:  5.42092021e-05, mean val. loss:  6.28897965e-01\n",
      "Epoch: 587 mean train loss:  5.37792803e-05, mean val. loss:  6.29319489e-01\n",
      "Epoch: 588 mean train loss:  5.43172937e-05, mean val. loss:  6.29733145e-01\n",
      "Epoch: 589 mean train loss:  5.41158952e-05, mean val. loss:  6.30149305e-01\n",
      "Epoch: 590 mean train loss:  5.37835585e-05, mean val. loss:  6.30571663e-01\n",
      "Epoch: 591 mean train loss:  5.47128730e-05, mean val. loss:  6.30975485e-01\n",
      "Epoch: 592 mean train loss:  5.48082462e-05, mean val. loss:  6.31393671e-01\n",
      "Epoch: 593 mean train loss:  5.47206146e-05, mean val. loss:  6.31798208e-01\n",
      "Epoch: 594 mean train loss:  5.40940673e-05, mean val. loss:  6.32216752e-01\n",
      "Epoch: 595 mean train loss:  5.48984390e-05, mean val. loss:  6.32629991e-01\n",
      "Epoch: 596 mean train loss:  5.30782854e-05, mean val. loss:  6.33041739e-01\n",
      "Epoch: 597 mean train loss:  5.39807370e-05, mean val. loss:  6.33459687e-01\n",
      "Epoch: 598 mean train loss:  5.32424601e-05, mean val. loss:  6.33876264e-01\n",
      "Epoch: 599 mean train loss:  5.40532637e-05, mean val. loss:  6.34290516e-01\n",
      "Epoch: 600 mean train loss:  5.39979956e-05, mean val. loss:  6.34700835e-01\n",
      "Epoch: 601 mean train loss:  5.39969187e-05, mean val. loss:  6.35107756e-01\n",
      "Epoch: 602 mean train loss:  5.44806826e-05, mean val. loss:  6.35522544e-01\n",
      "Epoch: 603 mean train loss:  5.47934033e-05, mean val. loss:  6.35933638e-01\n",
      "Epoch: 604 mean train loss:  5.36720036e-05, mean val. loss:  6.36342824e-01\n",
      "Epoch: 605 mean train loss:  5.36494772e-05, mean val. loss:  6.36751056e-01\n",
      "Epoch: 606 mean train loss:  5.35995932e-05, mean val. loss:  6.37158275e-01\n",
      "Epoch: 607 mean train loss:  5.42094058e-05, mean val. loss:  6.37570202e-01\n",
      "Epoch: 608 mean train loss:  5.52923302e-05, mean val. loss:  6.37977600e-01\n",
      "Epoch: 609 mean train loss:  5.41847257e-05, mean val. loss:  6.38379037e-01\n",
      "Epoch: 610 mean train loss:  5.29137324e-05, mean val. loss:  6.38800621e-01\n",
      "Epoch: 611 mean train loss:  5.41252375e-05, mean val. loss:  6.39209211e-01\n",
      "Epoch: 612 mean train loss:  5.45549556e-05, mean val. loss:  6.39631450e-01\n",
      "Epoch: 613 mean train loss:  5.39119937e-05, mean val. loss:  6.40033782e-01\n",
      "Epoch: 614 mean train loss:  5.34057617e-05, mean val. loss:  6.40453398e-01\n",
      "Epoch: 615 mean train loss:  5.39672910e-05, mean val. loss:  6.40852153e-01\n",
      "Epoch: 616 mean train loss:  5.48160169e-05, mean val. loss:  6.41254485e-01\n",
      "Epoch: 617 mean train loss:  5.45352814e-05, mean val. loss:  6.41650617e-01\n",
      "Epoch: 618 mean train loss:  5.37057058e-05, mean val. loss:  6.42048061e-01\n",
      "Epoch: 619 mean train loss:  5.36081498e-05, mean val. loss:  6.42450392e-01\n",
      "Epoch: 620 mean train loss:  5.36307634e-05, mean val. loss:  6.42851114e-01\n",
      "Epoch: 621 mean train loss:  5.41399640e-05, mean val. loss:  6.43241465e-01\n",
      "Epoch: 622 mean train loss:  5.36010193e-05, mean val. loss:  6.43629909e-01\n",
      "Epoch: 623 mean train loss:  5.34254359e-05, mean val. loss:  6.44021451e-01\n",
      "Epoch: 624 mean train loss:  5.23865747e-05, mean val. loss:  6.44416511e-01\n",
      "Epoch: 625 mean train loss:  5.34174615e-05, mean val. loss:  6.44805849e-01\n",
      "Epoch: 626 mean train loss:  5.30806137e-05, mean val. loss:  6.45202577e-01\n",
      "Epoch: 627 mean train loss:  5.39148459e-05, mean val. loss:  6.45600677e-01\n",
      "Epoch: 628 mean train loss:  5.30175457e-05, mean val. loss:  6.45995498e-01\n",
      "Epoch: 629 mean train loss:  5.40828332e-05, mean val. loss:  6.46385133e-01\n",
      "Epoch: 630 mean train loss:  5.33435377e-05, mean val. loss:  6.46782935e-01\n",
      "Epoch: 631 mean train loss:  5.33820712e-05, mean val. loss:  6.47174776e-01\n",
      "Epoch: 632 mean train loss:  5.35403378e-05, mean val. loss:  6.47563815e-01\n",
      "Epoch: 633 mean train loss:  5.29547688e-05, mean val. loss:  6.47945464e-01\n",
      "Epoch: 634 mean train loss:  5.40452311e-05, mean val. loss:  6.48323417e-01\n",
      "Epoch: 635 mean train loss:  5.36243024e-05, mean val. loss:  6.48702621e-01\n",
      "Epoch: 636 mean train loss:  5.22785995e-05, mean val. loss:  6.49084449e-01\n",
      "Epoch: 637 mean train loss:  5.49083925e-05, mean val. loss:  6.49458170e-01\n",
      "Epoch: 638 mean train loss:  5.31543046e-05, mean val. loss:  6.49837554e-01\n",
      "Epoch: 639 mean train loss:  5.42841735e-05, mean val. loss:  6.50212646e-01\n",
      "Epoch: 640 mean train loss:  5.38308523e-05, mean val. loss:  6.50583982e-01\n",
      "Epoch: 641 mean train loss:  5.39761386e-05, mean val. loss:  6.50951028e-01\n",
      "Epoch: 642 mean train loss:  5.32835256e-05, mean val. loss:  6.51316285e-01\n",
      "Epoch: 643 mean train loss:  5.16962900e-05, mean val. loss:  6.51680708e-01\n",
      "Epoch: 644 mean train loss:  5.36309381e-05, mean val. loss:  6.52050853e-01\n",
      "Epoch: 645 mean train loss:  5.34648425e-05, mean val. loss:  6.52413964e-01\n",
      "Epoch: 646 mean train loss:  5.40371111e-05, mean val. loss:  6.52785420e-01\n",
      "Epoch: 647 mean train loss:  5.47780073e-05, mean val. loss:  6.53148711e-01\n",
      "Epoch: 648 mean train loss:  5.39009343e-05, mean val. loss:  6.53498709e-01\n",
      "Epoch: 649 mean train loss:  5.33530547e-05, mean val. loss:  6.53848767e-01\n",
      "Epoch: 650 mean train loss:  5.38026507e-05, mean val. loss:  6.54203832e-01\n",
      "Epoch: 651 mean train loss:  5.37359447e-05, mean val. loss:  6.54553771e-01\n",
      "Epoch: 652 mean train loss:  5.29626850e-05, mean val. loss:  6.54909372e-01\n",
      "Epoch: 653 mean train loss:  5.31918777e-05, mean val. loss:  6.55262530e-01\n",
      "Epoch: 654 mean train loss:  5.25549985e-05, mean val. loss:  6.55623555e-01\n",
      "Epoch: 655 mean train loss:  5.27993543e-05, mean val. loss:  6.55980468e-01\n",
      "Epoch: 656 mean train loss:  5.27596858e-05, mean val. loss:  6.56352699e-01\n",
      "Epoch: 657 mean train loss:  5.42533235e-05, mean val. loss:  6.56712592e-01\n",
      "Epoch: 658 mean train loss:  5.29116951e-05, mean val. loss:  6.57081246e-01\n",
      "Epoch: 659 mean train loss:  5.17769950e-05, mean val. loss:  6.57454610e-01\n",
      "Epoch: 660 mean train loss:  5.24421921e-05, mean val. loss:  6.57822669e-01\n",
      "Epoch: 661 mean train loss:  5.44814684e-05, mean val. loss:  6.58191621e-01\n",
      "Epoch: 662 mean train loss:  5.29182435e-05, mean val. loss:  6.58552110e-01\n",
      "Epoch: 663 mean train loss:  5.30690886e-05, mean val. loss:  6.58914268e-01\n",
      "Epoch: 664 mean train loss:  5.40843175e-05, mean val. loss:  6.59269512e-01\n",
      "Epoch: 665 mean train loss:  5.34267165e-05, mean val. loss:  6.59624279e-01\n",
      "Epoch: 666 mean train loss:  5.34419669e-05, mean val. loss:  6.59983993e-01\n",
      "Epoch: 667 mean train loss:  5.18925372e-05, mean val. loss:  6.60343945e-01\n",
      "Epoch: 668 mean train loss:  5.29131503e-05, mean val. loss:  6.60698652e-01\n",
      "Epoch: 669 mean train loss:  5.27358206e-05, mean val. loss:  6.61049247e-01\n",
      "Epoch: 670 mean train loss:  5.24369243e-05, mean val. loss:  6.61399066e-01\n",
      "Epoch: 671 mean train loss:  5.22032788e-05, mean val. loss:  6.61749661e-01\n",
      "Epoch: 672 mean train loss:  5.28332894e-05, mean val. loss:  6.62098289e-01\n",
      "Epoch: 673 mean train loss:  5.20194008e-05, mean val. loss:  6.62451625e-01\n",
      "Epoch: 674 mean train loss:  5.25861396e-05, mean val. loss:  6.62809849e-01\n",
      "Epoch: 675 mean train loss:  5.30889956e-05, mean val. loss:  6.63169563e-01\n",
      "Epoch: 676 mean train loss:  5.20826143e-05, mean val. loss:  6.63521349e-01\n",
      "Epoch: 677 mean train loss:  5.09703823e-05, mean val. loss:  6.63878858e-01\n",
      "Epoch: 678 mean train loss:  5.37325104e-05, mean val. loss:  6.64235353e-01\n",
      "Epoch: 679 mean train loss:  5.18532179e-05, mean val. loss:  6.64580643e-01\n",
      "Epoch: 680 mean train loss:  5.24665811e-05, mean val. loss:  6.64935589e-01\n",
      "Epoch: 681 mean train loss:  5.32092818e-05, mean val. loss:  6.65282845e-01\n",
      "Epoch: 682 mean train loss:  5.36621956e-05, mean val. loss:  6.65625334e-01\n",
      "Epoch: 683 mean train loss:  5.30198449e-05, mean val. loss:  6.65964961e-01\n",
      "Epoch: 684 mean train loss:  5.26072108e-05, mean val. loss:  6.66300118e-01\n",
      "Epoch: 685 mean train loss:  5.28419041e-05, mean val. loss:  6.66637897e-01\n",
      "Epoch: 686 mean train loss:  5.28598321e-05, mean val. loss:  6.66978419e-01\n",
      "Epoch: 687 mean train loss:  5.27559896e-05, mean val. loss:  6.67304337e-01\n",
      "Epoch: 688 mean train loss:  5.19326713e-05, mean val. loss:  6.67639852e-01\n",
      "Epoch: 689 mean train loss:  5.31566911e-05, mean val. loss:  6.67963505e-01\n",
      "Epoch: 690 mean train loss:  5.24861971e-05, mean val. loss:  6.68296576e-01\n",
      "Epoch: 691 mean train loss:  5.30449324e-05, mean val. loss:  6.68616712e-01\n",
      "Epoch: 692 mean train loss:  5.19874447e-05, mean val. loss:  6.68941379e-01\n",
      "Epoch: 693 mean train loss:  5.19254536e-05, mean val. loss:  6.69264972e-01\n",
      "Epoch: 694 mean train loss:  5.23306080e-05, mean val. loss:  6.69588387e-01\n",
      "Epoch: 695 mean train loss:  5.18035376e-05, mean val. loss:  6.69914782e-01\n",
      "Epoch: 696 mean train loss:  5.17096778e-05, mean val. loss:  6.70243382e-01\n",
      "Epoch: 697 mean train loss:  5.23667259e-05, mean val. loss:  6.70580089e-01\n",
      "Epoch: 698 mean train loss:  5.26842778e-05, mean val. loss:  6.70907080e-01\n",
      "Epoch: 699 mean train loss:  5.27719385e-05, mean val. loss:  6.71235323e-01\n",
      "Epoch: 700 mean train loss:  5.21779293e-05, mean val. loss:  6.71572566e-01\n",
      "Epoch: 701 mean train loss:  5.18063607e-05, mean val. loss:  6.71904743e-01\n",
      "Epoch: 702 mean train loss:  5.26500226e-05, mean val. loss:  6.72230482e-01\n",
      "Epoch: 703 mean train loss:  5.25032810e-05, mean val. loss:  6.72549367e-01\n",
      "Epoch: 704 mean train loss:  5.22576156e-05, mean val. loss:  6.72867656e-01\n",
      "Epoch: 705 mean train loss:  5.18083107e-05, mean val. loss:  6.73187375e-01\n",
      "Epoch: 706 mean train loss:  5.24412899e-05, mean val. loss:  6.73506439e-01\n",
      "Epoch: 707 mean train loss:  5.16231812e-05, mean val. loss:  6.73825204e-01\n",
      "Epoch: 708 mean train loss:  5.14693384e-05, mean val. loss:  6.74148738e-01\n",
      "Epoch: 709 mean train loss:  5.26422227e-05, mean val. loss:  6.74469948e-01\n",
      "Epoch: 710 mean train loss:  5.23040071e-05, mean val. loss:  6.74796402e-01\n",
      "Epoch: 711 mean train loss:  5.04701748e-05, mean val. loss:  6.75131679e-01\n",
      "Epoch: 712 mean train loss:  5.16944565e-05, mean val. loss:  6.75466239e-01\n",
      "Epoch: 713 mean train loss:  5.22183836e-05, mean val. loss:  6.75796926e-01\n",
      "Epoch: 714 mean train loss:  5.37325395e-05, mean val. loss:  6.76107407e-01\n",
      "Epoch: 715 mean train loss:  5.20863687e-05, mean val. loss:  6.76418602e-01\n",
      "Epoch: 716 mean train loss:  5.27265947e-05, mean val. loss:  6.76716805e-01\n",
      "Epoch: 717 mean train loss:  5.08519006e-05, mean val. loss:  6.77032888e-01\n",
      "Epoch: 718 mean train loss:  5.17371809e-05, mean val. loss:  6.77362800e-01\n",
      "Epoch: 719 mean train loss:  5.21937909e-05, mean val. loss:  6.77695930e-01\n",
      "Epoch: 720 mean train loss:  5.29903919e-05, mean val. loss:  6.78025901e-01\n",
      "Epoch: 721 mean train loss:  5.24139614e-05, mean val. loss:  6.78353369e-01\n",
      "Epoch: 722 mean train loss:  5.31187688e-05, mean val. loss:  6.78677857e-01\n",
      "Epoch: 723 mean train loss:  5.19368041e-05, mean val. loss:  6.79008067e-01\n",
      "Epoch: 724 mean train loss:  5.11003309e-05, mean val. loss:  6.79344296e-01\n",
      "Epoch: 725 mean train loss:  5.22924820e-05, mean val. loss:  6.79671645e-01\n",
      "Epoch: 726 mean train loss:  5.15026622e-05, mean val. loss:  6.79991841e-01\n",
      "Epoch: 727 mean train loss:  5.18939050e-05, mean val. loss:  6.80313468e-01\n",
      "Epoch: 728 mean train loss:  5.23680064e-05, mean val. loss:  6.80632889e-01\n",
      "Epoch: 729 mean train loss:  5.11555118e-05, mean val. loss:  6.80962503e-01\n",
      "Epoch: 730 mean train loss:  5.11117105e-05, mean val. loss:  6.81294739e-01\n",
      "Epoch: 731 mean train loss:  5.19135501e-05, mean val. loss:  6.81622028e-01\n",
      "Epoch: 732 mean train loss:  5.08115336e-05, mean val. loss:  6.81951404e-01\n",
      "Epoch: 733 mean train loss:  5.22592163e-05, mean val. loss:  6.82267368e-01\n",
      "Epoch: 734 mean train loss:  5.26701042e-05, mean val. loss:  6.82589889e-01\n",
      "Epoch: 735 mean train loss:  5.13186096e-05, mean val. loss:  6.82919085e-01\n",
      "Epoch: 736 mean train loss:  5.16856671e-05, mean val. loss:  6.83240592e-01\n",
      "Epoch: 737 mean train loss:  5.09291713e-05, mean val. loss:  6.83566034e-01\n",
      "Epoch: 738 mean train loss:  5.23969356e-05, mean val. loss:  6.83893502e-01\n",
      "Epoch: 739 mean train loss:  5.11750986e-05, mean val. loss:  6.84215248e-01\n",
      "Epoch: 740 mean train loss:  5.16135187e-05, mean val. loss:  6.84531033e-01\n",
      "Epoch: 741 mean train loss:  5.20672183e-05, mean val. loss:  6.84842587e-01\n",
      "Epoch: 742 mean train loss:  5.22679184e-05, mean val. loss:  6.85160041e-01\n",
      "Epoch: 743 mean train loss:  5.08292869e-05, mean val. loss:  6.85479105e-01\n",
      "Epoch: 744 mean train loss:  5.16004802e-05, mean val. loss:  6.85791612e-01\n",
      "Epoch: 745 mean train loss:  5.07096411e-05, mean val. loss:  6.86106145e-01\n",
      "Epoch: 746 mean train loss:  5.10786485e-05, mean val. loss:  6.86417699e-01\n",
      "Epoch: 747 mean train loss:  5.05691860e-05, mean val. loss:  6.86729968e-01\n",
      "Epoch: 748 mean train loss:  5.09655220e-05, mean val. loss:  6.87048912e-01\n",
      "Epoch: 749 mean train loss:  5.15178544e-05, mean val. loss:  6.87348604e-01\n",
      "Epoch: 750 mean train loss:  5.16037690e-05, mean val. loss:  6.87663078e-01\n",
      "Epoch: 751 mean train loss:  5.06723300e-05, mean val. loss:  6.87968016e-01\n",
      "Epoch: 752 mean train loss:  5.26181248e-05, mean val. loss:  6.88278437e-01\n",
      "Epoch: 753 mean train loss:  5.08165394e-05, mean val. loss:  6.88576639e-01\n",
      "Epoch: 754 mean train loss:  5.17190201e-05, mean val. loss:  6.88883424e-01\n",
      "Epoch: 755 mean train loss:  5.12971310e-05, mean val. loss:  6.89190328e-01\n",
      "Epoch: 756 mean train loss:  5.14402636e-05, mean val. loss:  6.89500153e-01\n",
      "Epoch: 757 mean train loss:  5.15833963e-05, mean val. loss:  6.89798832e-01\n",
      "Epoch: 758 mean train loss:  5.09055972e-05, mean val. loss:  6.90108478e-01\n",
      "Epoch: 759 mean train loss:  5.12248080e-05, mean val. loss:  6.90409660e-01\n",
      "Epoch: 760 mean train loss:  5.23214694e-05, mean val. loss:  6.90708935e-01\n",
      "Epoch: 761 mean train loss:  5.09778038e-05, mean val. loss:  6.90999985e-01\n",
      "Epoch: 762 mean train loss:  5.09032398e-05, mean val. loss:  6.91300631e-01\n",
      "Epoch: 763 mean train loss:  5.12784463e-05, mean val. loss:  6.91594064e-01\n",
      "Epoch: 764 mean train loss:  5.01240429e-05, mean val. loss:  6.91903532e-01\n",
      "Epoch: 765 mean train loss:  5.16252476e-05, mean val. loss:  6.92222118e-01\n",
      "Epoch: 766 mean train loss:  5.10073442e-05, mean val. loss:  6.92534745e-01\n",
      "Epoch: 767 mean train loss:  4.97902802e-05, mean val. loss:  6.92856729e-01\n",
      "Epoch: 768 mean train loss:  5.12495462e-05, mean val. loss:  6.93183899e-01\n",
      "Epoch: 769 mean train loss:  4.96604480e-05, mean val. loss:  6.93524420e-01\n",
      "Epoch: 770 mean train loss:  5.24447823e-05, mean val. loss:  6.93843246e-01\n",
      "Epoch: 771 mean train loss:  5.11037360e-05, mean val. loss:  6.94172502e-01\n",
      "Epoch: 772 mean train loss:  4.95075947e-05, mean val. loss:  6.94493473e-01\n",
      "Epoch: 773 mean train loss:  5.00694150e-05, mean val. loss:  6.94829583e-01\n",
      "Epoch: 774 mean train loss:  5.09148231e-05, mean val. loss:  6.95146739e-01\n",
      "Epoch: 775 mean train loss:  5.10079553e-05, mean val. loss:  6.95469856e-01\n",
      "Epoch: 776 mean train loss:  5.15459687e-05, mean val. loss:  6.95792615e-01\n",
      "Epoch: 777 mean train loss:  4.95820423e-05, mean val. loss:  6.96113110e-01\n",
      "Epoch: 778 mean train loss:  5.07065270e-05, mean val. loss:  6.96451604e-01\n",
      "Epoch: 779 mean train loss:  5.07668010e-05, mean val. loss:  6.96779847e-01\n",
      "Epoch: 780 mean train loss:  5.06377255e-05, mean val. loss:  6.97120488e-01\n",
      "Epoch: 781 mean train loss:  5.11679973e-05, mean val. loss:  6.97439730e-01\n",
      "Epoch: 782 mean train loss:  5.16082800e-05, mean val. loss:  6.97760165e-01\n",
      "Epoch: 783 mean train loss:  5.06252400e-05, mean val. loss:  6.98083222e-01\n",
      "Epoch: 784 mean train loss:  5.04853379e-05, mean val. loss:  6.98409677e-01\n",
      "Epoch: 785 mean train loss:  5.05946227e-05, mean val. loss:  6.98730290e-01\n",
      "Epoch: 786 mean train loss:  5.01111790e-05, mean val. loss:  6.99046910e-01\n",
      "Epoch: 787 mean train loss:  5.16432337e-05, mean val. loss:  6.99365973e-01\n",
      "Epoch: 788 mean train loss:  5.00208698e-05, mean val. loss:  6.99692070e-01\n",
      "Epoch: 789 mean train loss:  5.06379583e-05, mean val. loss:  7.00015187e-01\n",
      "Epoch: 790 mean train loss:  5.04072523e-05, mean val. loss:  7.00343430e-01\n",
      "Epoch: 791 mean train loss:  5.03294286e-05, mean val. loss:  7.00660944e-01\n",
      "Epoch: 792 mean train loss:  5.04031195e-05, mean val. loss:  7.00974822e-01\n",
      "Epoch: 793 mean train loss:  5.03336487e-05, mean val. loss:  7.01288700e-01\n",
      "Epoch: 794 mean train loss:  5.12239349e-05, mean val. loss:  7.01609075e-01\n",
      "Epoch: 795 mean train loss:  5.04163909e-05, mean val. loss:  7.01921105e-01\n",
      "Epoch: 796 mean train loss:  5.08884259e-05, mean val. loss:  7.02222764e-01\n",
      "Epoch: 797 mean train loss:  5.02830953e-05, mean val. loss:  7.02538013e-01\n",
      "Epoch: 798 mean train loss:  5.00511669e-05, mean val. loss:  7.02856302e-01\n",
      "Epoch: 799 mean train loss:  4.98025620e-05, mean val. loss:  7.03175664e-01\n",
      "Epoch: 800 mean train loss:  5.05311182e-05, mean val. loss:  7.03489602e-01\n",
      "Epoch: 801 mean train loss:  4.94676933e-05, mean val. loss:  7.03815579e-01\n",
      "Epoch: 802 mean train loss:  5.00030292e-05, mean val. loss:  7.04142272e-01\n",
      "Epoch: 803 mean train loss:  4.94588166e-05, mean val. loss:  7.04467714e-01\n",
      "Epoch: 804 mean train loss:  5.05177595e-05, mean val. loss:  7.04784989e-01\n",
      "Epoch: 805 mean train loss:  4.96486318e-05, mean val. loss:  7.05113411e-01\n",
      "Epoch: 806 mean train loss:  5.03510237e-05, mean val. loss:  7.05442607e-01\n",
      "Epoch: 807 mean train loss:  4.99078596e-05, mean val. loss:  7.05763638e-01\n",
      "Epoch: 808 mean train loss:  5.04970376e-05, mean val. loss:  7.06081212e-01\n",
      "Epoch: 809 mean train loss:  5.09653182e-05, mean val. loss:  7.06400335e-01\n",
      "Epoch: 810 mean train loss:  5.00548631e-05, mean val. loss:  7.06716835e-01\n",
      "Epoch: 811 mean train loss:  5.06584183e-05, mean val. loss:  7.07016170e-01\n",
      "Epoch: 812 mean train loss:  5.03024494e-05, mean val. loss:  7.07312405e-01\n",
      "Epoch: 813 mean train loss:  5.03299816e-05, mean val. loss:  7.07601547e-01\n",
      "Epoch: 814 mean train loss:  5.07090881e-05, mean val. loss:  7.07894325e-01\n",
      "Epoch: 815 mean train loss:  5.10663376e-05, mean val. loss:  7.08186626e-01\n",
      "Epoch: 816 mean train loss:  5.08000376e-05, mean val. loss:  7.08468258e-01\n",
      "Epoch: 817 mean train loss:  5.03621122e-05, mean val. loss:  7.08744407e-01\n",
      "Epoch: 818 mean train loss:  5.01400791e-05, mean val. loss:  7.09024370e-01\n",
      "Epoch: 819 mean train loss:  5.02489274e-05, mean val. loss:  7.09307075e-01\n",
      "Epoch: 820 mean train loss:  5.02623443e-05, mean val. loss:  7.09586561e-01\n",
      "Epoch: 821 mean train loss:  5.10402606e-05, mean val. loss:  7.09859788e-01\n",
      "Epoch: 822 mean train loss:  5.00133901e-05, mean val. loss:  7.10117638e-01\n",
      "Epoch: 823 mean train loss:  4.94472042e-05, mean val. loss:  7.10382342e-01\n",
      "Epoch: 824 mean train loss:  4.98463924e-05, mean val. loss:  7.10657477e-01\n",
      "Epoch: 825 mean train loss:  4.95716813e-05, mean val. loss:  7.10934818e-01\n",
      "Epoch: 826 mean train loss:  4.83743497e-05, mean val. loss:  7.11219251e-01\n",
      "Epoch: 827 mean train loss:  5.07071964e-05, mean val. loss:  7.11496592e-01\n",
      "Epoch: 828 mean train loss:  4.97143483e-05, mean val. loss:  7.11779654e-01\n",
      "Epoch: 829 mean train loss:  5.07605728e-05, mean val. loss:  7.12062478e-01\n",
      "Epoch: 830 mean train loss:  5.01039322e-05, mean val. loss:  7.12343991e-01\n",
      "Epoch: 831 mean train loss:  5.01249160e-05, mean val. loss:  7.12616742e-01\n",
      "Epoch: 832 mean train loss:  5.08006779e-05, mean val. loss:  7.12894261e-01\n",
      "Epoch: 833 mean train loss:  5.13367704e-05, mean val. loss:  7.13162184e-01\n",
      "Epoch: 834 mean train loss:  5.00620226e-05, mean val. loss:  7.13444889e-01\n",
      "Epoch: 835 mean train loss:  4.96554712e-05, mean val. loss:  7.13723779e-01\n",
      "Epoch: 836 mean train loss:  4.91017126e-05, mean val. loss:  7.14004755e-01\n",
      "Epoch: 837 mean train loss:  4.99874586e-05, mean val. loss:  7.14279234e-01\n",
      "Epoch: 838 mean train loss:  5.01230825e-05, mean val. loss:  7.14546025e-01\n",
      "Epoch: 839 mean train loss:  4.92678373e-05, mean val. loss:  7.14830339e-01\n",
      "Epoch: 840 mean train loss:  5.05178468e-05, mean val. loss:  7.15108633e-01\n",
      "Epoch: 841 mean train loss:  4.98052686e-05, mean val. loss:  7.15377450e-01\n",
      "Epoch: 842 mean train loss:  4.92109102e-05, mean val. loss:  7.15655863e-01\n",
      "Epoch: 843 mean train loss:  4.98137670e-05, mean val. loss:  7.15938032e-01\n",
      "Epoch: 844 mean train loss:  4.97257861e-05, mean val. loss:  7.16220856e-01\n",
      "Epoch: 845 mean train loss:  4.91157698e-05, mean val. loss:  7.16496825e-01\n",
      "Epoch: 846 mean train loss:  5.05282078e-05, mean val. loss:  7.16772020e-01\n",
      "Epoch: 847 mean train loss:  4.99013404e-05, mean val. loss:  7.17034936e-01\n",
      "Epoch: 848 mean train loss:  4.96561988e-05, mean val. loss:  7.17293561e-01\n",
      "Epoch: 849 mean train loss:  4.94925189e-05, mean val. loss:  7.17551351e-01\n",
      "Epoch: 850 mean train loss:  4.92549152e-05, mean val. loss:  7.17821598e-01\n",
      "Epoch: 851 mean train loss:  4.94143751e-05, mean val. loss:  7.18080103e-01\n",
      "Epoch: 852 mean train loss:  4.94876876e-05, mean val. loss:  7.18338370e-01\n",
      "Epoch: 853 mean train loss:  4.87641664e-05, mean val. loss:  7.18599319e-01\n",
      "Epoch: 854 mean train loss:  4.80285962e-05, mean val. loss:  7.18864381e-01\n",
      "Epoch: 855 mean train loss:  5.00414753e-05, mean val. loss:  7.19132721e-01\n",
      "Epoch: 856 mean train loss:  4.99510788e-05, mean val. loss:  7.19400287e-01\n",
      "Epoch: 857 mean train loss:  4.88746155e-05, mean val. loss:  7.19678640e-01\n",
      "Epoch: 858 mean train loss:  4.91710089e-05, mean val. loss:  7.19958782e-01\n",
      "Epoch: 859 mean train loss:  5.02511684e-05, mean val. loss:  7.20233619e-01\n",
      "Epoch: 860 mean train loss:  4.98699956e-05, mean val. loss:  7.20515013e-01\n",
      "Epoch: 861 mean train loss:  5.00813476e-05, mean val. loss:  7.20776796e-01\n",
      "Epoch: 862 mean train loss:  4.88450169e-05, mean val. loss:  7.21052349e-01\n",
      "Epoch: 863 mean train loss:  4.99247690e-05, mean val. loss:  7.21312642e-01\n",
      "Epoch: 864 mean train loss:  4.99195885e-05, mean val. loss:  7.21585214e-01\n",
      "Epoch: 865 mean train loss:  4.97179281e-05, mean val. loss:  7.21865773e-01\n",
      "Epoch: 866 mean train loss:  4.82502510e-05, mean val. loss:  7.22146630e-01\n",
      "Epoch: 867 mean train loss:  5.04687778e-05, mean val. loss:  7.22428799e-01\n",
      "Epoch: 868 mean train loss:  4.85665514e-05, mean val. loss:  7.22724020e-01\n",
      "Epoch: 869 mean train loss:  4.89879167e-05, mean val. loss:  7.23013401e-01\n",
      "Epoch: 870 mean train loss:  4.99627786e-05, mean val. loss:  7.23294020e-01\n",
      "Epoch: 871 mean train loss:  4.93230764e-05, mean val. loss:  7.23577142e-01\n",
      "Epoch: 872 mean train loss:  4.96698194e-05, mean val. loss:  7.23851562e-01\n",
      "Epoch: 873 mean train loss:  4.88826190e-05, mean val. loss:  7.24121392e-01\n",
      "Epoch: 874 mean train loss:  5.02340263e-05, mean val. loss:  7.24389732e-01\n",
      "Epoch: 875 mean train loss:  4.95045970e-05, mean val. loss:  7.24653363e-01\n",
      "Epoch: 876 mean train loss:  4.91341343e-05, mean val. loss:  7.24911988e-01\n",
      "Epoch: 877 mean train loss:  4.88209771e-05, mean val. loss:  7.25188375e-01\n",
      "Epoch: 878 mean train loss:  5.00380993e-05, mean val. loss:  7.25448191e-01\n",
      "Epoch: 879 mean train loss:  4.98026493e-05, mean val. loss:  7.25714087e-01\n",
      "Epoch: 880 mean train loss:  4.94241831e-05, mean val. loss:  7.25980639e-01\n",
      "Epoch: 881 mean train loss:  4.94867563e-05, mean val. loss:  7.26238728e-01\n",
      "Epoch: 882 mean train loss:  5.02649636e-05, mean val. loss:  7.26501644e-01\n",
      "Epoch: 883 mean train loss:  4.88862279e-05, mean val. loss:  7.26767659e-01\n",
      "Epoch: 884 mean train loss:  4.81961761e-05, mean val. loss:  7.27049053e-01\n",
      "Epoch: 885 mean train loss:  4.97997098e-05, mean val. loss:  7.27327526e-01\n",
      "Epoch: 886 mean train loss:  4.84822085e-05, mean val. loss:  7.27607250e-01\n",
      "Epoch: 887 mean train loss:  4.91838146e-05, mean val. loss:  7.27885664e-01\n",
      "Epoch: 888 mean train loss:  5.00352471e-05, mean val. loss:  7.28145778e-01\n",
      "Epoch: 889 mean train loss:  4.84506018e-05, mean val. loss:  7.28421986e-01\n",
      "Epoch: 890 mean train loss:  5.00345486e-05, mean val. loss:  7.28688657e-01\n",
      "Epoch: 891 mean train loss:  4.92421677e-05, mean val. loss:  7.28952348e-01\n",
      "Epoch: 892 mean train loss:  5.01620234e-05, mean val. loss:  7.29207277e-01\n",
      "Epoch: 893 mean train loss:  5.02284674e-05, mean val. loss:  7.29462087e-01\n",
      "Epoch: 894 mean train loss:  4.96224675e-05, mean val. loss:  7.29711473e-01\n",
      "Epoch: 895 mean train loss:  4.94198757e-05, mean val. loss:  7.29968488e-01\n",
      "Epoch: 896 mean train loss:  4.91555547e-05, mean val. loss:  7.30227232e-01\n",
      "Epoch: 897 mean train loss:  5.08198864e-05, mean val. loss:  7.30469048e-01\n",
      "Epoch: 898 mean train loss:  4.96991270e-05, mean val. loss:  7.30714619e-01\n",
      "Epoch: 899 mean train loss:  4.92715044e-05, mean val. loss:  7.30942965e-01\n",
      "Epoch: 900 mean train loss:  4.94938286e-05, mean val. loss:  7.31171250e-01\n",
      "Epoch: 901 mean train loss:  4.87417856e-05, mean val. loss:  7.31406569e-01\n",
      "Epoch: 902 mean train loss:  4.96019493e-05, mean val. loss:  7.31633425e-01\n",
      "Epoch: 903 mean train loss:  4.97673755e-05, mean val. loss:  7.31861949e-01\n",
      "Epoch: 904 mean train loss:  4.88739461e-05, mean val. loss:  7.32094646e-01\n",
      "Epoch: 905 mean train loss:  4.92646359e-05, mean val. loss:  7.32336521e-01\n",
      "Epoch: 906 mean train loss:  4.95375134e-05, mean val. loss:  7.32569039e-01\n",
      "Epoch: 907 mean train loss:  4.92947293e-05, mean val. loss:  7.32799828e-01\n",
      "Epoch: 908 mean train loss:  4.84076445e-05, mean val. loss:  7.33017802e-01\n",
      "Epoch: 909 mean train loss:  4.90733655e-05, mean val. loss:  7.33234823e-01\n",
      "Epoch: 910 mean train loss:  4.94678970e-05, mean val. loss:  7.33449638e-01\n",
      "Epoch: 911 mean train loss:  4.88498481e-05, mean val. loss:  7.33675539e-01\n",
      "Epoch: 912 mean train loss:  4.84804041e-05, mean val. loss:  7.33894229e-01\n",
      "Epoch: 913 mean train loss:  4.78853181e-05, mean val. loss:  7.34119654e-01\n",
      "Epoch: 914 mean train loss:  4.84297343e-05, mean val. loss:  7.34349608e-01\n",
      "Epoch: 915 mean train loss:  4.89116646e-05, mean val. loss:  7.34576881e-01\n",
      "Epoch: 916 mean train loss:  4.95703425e-05, mean val. loss:  7.34810293e-01\n",
      "Epoch: 917 mean train loss:  4.93736006e-05, mean val. loss:  7.35043168e-01\n",
      "Epoch: 918 mean train loss:  4.89646627e-05, mean val. loss:  7.35276520e-01\n",
      "Epoch: 919 mean train loss:  4.93827101e-05, mean val. loss:  7.35513270e-01\n",
      "Epoch: 920 mean train loss:  4.84947232e-05, mean val. loss:  7.35749722e-01\n",
      "Epoch: 921 mean train loss:  4.91938554e-05, mean val. loss:  7.35975325e-01\n",
      "Epoch: 922 mean train loss:  4.94352716e-05, mean val. loss:  7.36203849e-01\n",
      "Epoch: 923 mean train loss:  4.82681498e-05, mean val. loss:  7.36431777e-01\n",
      "Epoch: 924 mean train loss:  4.85946948e-05, mean val. loss:  7.36669779e-01\n",
      "Epoch: 925 mean train loss:  4.81692259e-05, mean val. loss:  7.36906111e-01\n",
      "Epoch: 926 mean train loss:  4.94725537e-05, mean val. loss:  7.37135231e-01\n",
      "Epoch: 927 mean train loss:  4.87221987e-05, mean val. loss:  7.37364769e-01\n",
      "Epoch: 928 mean train loss:  4.85872151e-05, mean val. loss:  7.37584531e-01\n",
      "Epoch: 929 mean train loss:  4.78760048e-05, mean val. loss:  7.37818062e-01\n",
      "Epoch: 930 mean train loss:  4.88611986e-05, mean val. loss:  7.38059282e-01\n",
      "Epoch: 931 mean train loss:  4.84025804e-05, mean val. loss:  7.38301694e-01\n",
      "Epoch: 932 mean train loss:  4.73205291e-05, mean val. loss:  7.38551080e-01\n",
      "Epoch: 933 mean train loss:  4.87818033e-05, mean val. loss:  7.38796830e-01\n",
      "Epoch: 934 mean train loss:  4.89315134e-05, mean val. loss:  7.39040792e-01\n",
      "Epoch: 935 mean train loss:  4.91491519e-05, mean val. loss:  7.39272892e-01\n",
      "Epoch: 936 mean train loss:  4.80417802e-05, mean val. loss:  7.39517629e-01\n",
      "Epoch: 937 mean train loss:  4.82971955e-05, mean val. loss:  7.39764452e-01\n",
      "Epoch: 938 mean train loss:  4.73012624e-05, mean val. loss:  7.40017891e-01\n",
      "Epoch: 939 mean train loss:  4.81334573e-05, mean val. loss:  7.40271211e-01\n",
      "Epoch: 940 mean train loss:  4.84617485e-05, mean val. loss:  7.40519226e-01\n",
      "Epoch: 941 mean train loss:  4.93646075e-05, mean val. loss:  7.40748942e-01\n",
      "Epoch: 942 mean train loss:  4.91030805e-05, mean val. loss:  7.40986645e-01\n",
      "Epoch: 943 mean train loss:  4.90293023e-05, mean val. loss:  7.41214097e-01\n",
      "Epoch: 944 mean train loss:  4.94614651e-05, mean val. loss:  7.41434753e-01\n",
      "Epoch: 945 mean train loss:  4.88154765e-05, mean val. loss:  7.41660297e-01\n",
      "Epoch: 946 mean train loss:  4.91726678e-05, mean val. loss:  7.41879642e-01\n",
      "Epoch: 947 mean train loss:  4.88106452e-05, mean val. loss:  7.42095947e-01\n",
      "Epoch: 948 mean train loss:  4.86333738e-05, mean val. loss:  7.42317080e-01\n",
      "Epoch: 949 mean train loss:  4.84256307e-05, mean val. loss:  7.42535055e-01\n",
      "Epoch: 950 mean train loss:  4.78025759e-05, mean val. loss:  7.42769122e-01\n",
      "Epoch: 951 mean train loss:  4.90428647e-05, mean val. loss:  7.43006289e-01\n",
      "Epoch: 952 mean train loss:  4.80052549e-05, mean val. loss:  7.43247807e-01\n",
      "Epoch: 953 mean train loss:  4.88973456e-05, mean val. loss:  7.43482232e-01\n",
      "Epoch: 954 mean train loss:  4.91563696e-05, mean val. loss:  7.43720531e-01\n",
      "Epoch: 955 mean train loss:  4.75088600e-05, mean val. loss:  7.43963599e-01\n",
      "Epoch: 956 mean train loss:  4.86052013e-05, mean val. loss:  7.44204342e-01\n",
      "Epoch: 957 mean train loss:  4.86586941e-05, mean val. loss:  7.44441152e-01\n",
      "Epoch: 958 mean train loss:  4.92843392e-05, mean val. loss:  7.44677246e-01\n",
      "Epoch: 959 mean train loss:  4.95633285e-05, mean val. loss:  7.44907856e-01\n",
      "Epoch: 960 mean train loss:  4.91888495e-05, mean val. loss:  7.45137036e-01\n",
      "Epoch: 961 mean train loss:  4.84695775e-05, mean val. loss:  7.45366693e-01\n",
      "Epoch: 962 mean train loss:  4.83537733e-05, mean val. loss:  7.45593905e-01\n",
      "Epoch: 963 mean train loss:  4.83954791e-05, mean val. loss:  7.45820045e-01\n",
      "Epoch: 964 mean train loss:  4.75848210e-05, mean val. loss:  7.46043086e-01\n",
      "Epoch: 965 mean train loss:  4.81165189e-05, mean val. loss:  7.46271670e-01\n",
      "Epoch: 966 mean train loss:  4.86758654e-05, mean val. loss:  7.46505380e-01\n",
      "Epoch: 967 mean train loss:  4.85577330e-05, mean val. loss:  7.46725440e-01\n",
      "Epoch: 968 mean train loss:  4.83222539e-05, mean val. loss:  7.46951580e-01\n",
      "Epoch: 969 mean train loss:  4.84818593e-05, mean val. loss:  7.47179329e-01\n",
      "Epoch: 970 mean train loss:  4.65827470e-05, mean val. loss:  7.47404873e-01\n",
      "Epoch: 971 mean train loss:  4.76838031e-05, mean val. loss:  7.47629642e-01\n",
      "Epoch: 972 mean train loss:  4.81886673e-05, mean val. loss:  7.47854829e-01\n",
      "Epoch: 973 mean train loss:  4.90147213e-05, mean val. loss:  7.48070538e-01\n",
      "Epoch: 974 mean train loss:  4.81879979e-05, mean val. loss:  7.48283744e-01\n",
      "Epoch: 975 mean train loss:  4.74966655e-05, mean val. loss:  7.48507917e-01\n",
      "Epoch: 976 mean train loss:  4.75623237e-05, mean val. loss:  7.48736024e-01\n",
      "Epoch: 977 mean train loss:  4.81982715e-05, mean val. loss:  7.48961806e-01\n",
      "Epoch: 978 mean train loss:  4.78740549e-05, mean val. loss:  7.49188185e-01\n",
      "Epoch: 979 mean train loss:  4.83463227e-05, mean val. loss:  7.49411941e-01\n",
      "Epoch: 980 mean train loss:  4.76004207e-05, mean val. loss:  7.49638081e-01\n",
      "Epoch: 981 mean train loss:  4.82200121e-05, mean val. loss:  7.49863565e-01\n",
      "Epoch: 982 mean train loss:  4.86114586e-05, mean val. loss:  7.50086725e-01\n",
      "Epoch: 983 mean train loss:  4.85061901e-05, mean val. loss:  7.50306249e-01\n",
      "Epoch: 984 mean train loss:  4.73104301e-05, mean val. loss:  7.50546157e-01\n",
      "Epoch: 985 mean train loss:  4.73606051e-05, mean val. loss:  7.50774503e-01\n",
      "Epoch: 986 mean train loss:  4.85896016e-05, mean val. loss:  7.51010239e-01\n",
      "Epoch: 987 mean train loss:  4.85341589e-05, mean val. loss:  7.51229644e-01\n",
      "Epoch: 988 mean train loss:  4.79672453e-05, mean val. loss:  7.51459479e-01\n",
      "Epoch: 989 mean train loss:  4.74670669e-05, mean val. loss:  7.51678228e-01\n",
      "Epoch: 990 mean train loss:  4.86143981e-05, mean val. loss:  7.51914024e-01\n",
      "Epoch: 991 mean train loss:  4.87038051e-05, mean val. loss:  7.52134740e-01\n",
      "Epoch: 992 mean train loss:  4.75244015e-05, mean val. loss:  7.52362013e-01\n",
      "Epoch: 993 mean train loss:  4.66782076e-05, mean val. loss:  7.52578437e-01\n",
      "Epoch: 994 mean train loss:  4.84930933e-05, mean val. loss:  7.52797008e-01\n",
      "Epoch: 995 mean train loss:  4.90293605e-05, mean val. loss:  7.53003538e-01\n",
      "Epoch: 996 mean train loss:  4.70064697e-05, mean val. loss:  7.53208518e-01\n",
      "Epoch: 997 mean train loss:  4.62259050e-05, mean val. loss:  7.53440738e-01\n",
      "Epoch: 998 mean train loss:  4.83892509e-05, mean val. loss:  7.53655493e-01\n",
      "Epoch: 999 mean train loss:  4.85185883e-05, mean val. loss:  7.53870010e-01\n",
      "Epoch: 1000 mean train loss:  4.64686309e-05, mean val. loss:  7.54097939e-01\n",
      "Epoch: 1001 mean train loss:  4.74029803e-05, mean val. loss:  7.54329622e-01\n",
      "Epoch: 1002 mean train loss:  4.76127025e-05, mean val. loss:  7.54559934e-01\n",
      "Epoch: 1003 mean train loss:  4.67788777e-05, mean val. loss:  7.54796743e-01\n",
      "Epoch: 1004 mean train loss:  4.81554598e-05, mean val. loss:  7.55028903e-01\n",
      "Epoch: 1005 mean train loss:  4.83620388e-05, mean val. loss:  7.55259037e-01\n",
      "Epoch: 1006 mean train loss:  4.77038848e-05, mean val. loss:  7.55475521e-01\n",
      "Epoch: 1007 mean train loss:  4.84117772e-05, mean val. loss:  7.55697072e-01\n",
      "Epoch: 1008 mean train loss:  4.74575791e-05, mean val. loss:  7.55921364e-01\n",
      "Epoch: 1009 mean train loss:  4.82542964e-05, mean val. loss:  7.56139934e-01\n",
      "Epoch: 1010 mean train loss:  4.74920380e-05, mean val. loss:  7.56361365e-01\n",
      "Epoch: 1011 mean train loss:  4.87345969e-05, mean val. loss:  7.56569862e-01\n",
      "Epoch: 1012 mean train loss:  4.84832563e-05, mean val. loss:  7.56780326e-01\n",
      "Epoch: 1013 mean train loss:  4.77568246e-05, mean val. loss:  7.56992519e-01\n",
      "Epoch: 1014 mean train loss:  4.86633216e-05, mean val. loss:  7.57207751e-01\n",
      "Epoch: 1015 mean train loss:  4.90084349e-05, mean val. loss:  7.57409394e-01\n",
      "Epoch: 1016 mean train loss:  4.74881963e-05, mean val. loss:  7.57619560e-01\n",
      "Epoch: 1017 mean train loss:  4.68957005e-05, mean val. loss:  7.57824659e-01\n",
      "Epoch: 1018 mean train loss:  4.68019862e-05, mean val. loss:  7.58032322e-01\n",
      "Epoch: 1019 mean train loss:  4.67265490e-05, mean val. loss:  7.58250475e-01\n",
      "Epoch: 1020 mean train loss:  4.69221268e-05, mean val. loss:  7.58478999e-01\n",
      "Epoch: 1021 mean train loss:  4.70926170e-05, mean val. loss:  7.58712530e-01\n",
      "Epoch: 1022 mean train loss:  4.82252217e-05, mean val. loss:  7.58950591e-01\n",
      "Epoch: 1023 mean train loss:  4.75471024e-05, mean val. loss:  7.59178638e-01\n",
      "Epoch: 1024 mean train loss:  4.84309858e-05, mean val. loss:  7.59402215e-01\n",
      "Epoch: 1025 mean train loss:  4.68503567e-05, mean val. loss:  7.59632885e-01\n",
      "Epoch: 1026 mean train loss:  4.77245194e-05, mean val. loss:  7.59861708e-01\n",
      "Epoch: 1027 mean train loss:  4.78092697e-05, mean val. loss:  7.60079801e-01\n",
      "Epoch: 1028 mean train loss:  4.68748331e-05, mean val. loss:  7.60308385e-01\n",
      "Epoch: 1029 mean train loss:  4.73695691e-05, mean val. loss:  7.60545254e-01\n",
      "Epoch: 1030 mean train loss:  4.68287326e-05, mean val. loss:  7.60778725e-01\n",
      "Epoch: 1031 mean train loss:  4.88943770e-05, mean val. loss:  7.60993600e-01\n",
      "Epoch: 1032 mean train loss:  4.92089312e-05, mean val. loss:  7.61209786e-01\n",
      "Epoch: 1033 mean train loss:  4.86423378e-05, mean val. loss:  7.61412501e-01\n",
      "Epoch: 1034 mean train loss:  4.80780727e-05, mean val. loss:  7.61601865e-01\n",
      "Epoch: 1035 mean train loss:  4.66525089e-05, mean val. loss:  7.61794150e-01\n",
      "Epoch: 1036 mean train loss:  4.76861896e-05, mean val. loss:  7.61994839e-01\n",
      "Epoch: 1037 mean train loss:  4.62751486e-05, mean val. loss:  7.62198985e-01\n",
      "Epoch: 1038 mean train loss:  4.67163045e-05, mean val. loss:  7.62405574e-01\n",
      "Epoch: 1039 mean train loss:  4.79592709e-05, mean val. loss:  7.62611687e-01\n",
      "Epoch: 1040 mean train loss:  4.76260902e-05, mean val. loss:  7.62817979e-01\n",
      "Epoch: 1041 mean train loss:  4.78833099e-05, mean val. loss:  7.63031900e-01\n",
      "Epoch: 1042 mean train loss:  4.82647738e-05, mean val. loss:  7.63233066e-01\n",
      "Epoch: 1043 mean train loss:  4.68305079e-05, mean val. loss:  7.63436139e-01\n",
      "Epoch: 1044 mean train loss:  4.74884873e-05, mean val. loss:  7.63642669e-01\n",
      "Epoch: 1045 mean train loss:  4.84115153e-05, mean val. loss:  7.63840914e-01\n",
      "Epoch: 1046 mean train loss:  4.66077472e-05, mean val. loss:  7.64043808e-01\n",
      "Epoch: 1047 mean train loss:  4.77436115e-05, mean val. loss:  7.64255941e-01\n",
      "Epoch: 1048 mean train loss:  4.63669421e-05, mean val. loss:  7.64472663e-01\n",
      "Epoch: 1049 mean train loss:  4.72700340e-05, mean val. loss:  7.64694750e-01\n",
      "Epoch: 1050 mean train loss:  4.79098817e-05, mean val. loss:  7.64906228e-01\n",
      "Epoch: 1051 mean train loss:  4.69700899e-05, mean val. loss:  7.65117526e-01\n",
      "Epoch: 1052 mean train loss:  4.84703341e-05, mean val. loss:  7.65323579e-01\n",
      "Epoch: 1053 mean train loss:  4.73277178e-05, mean val. loss:  7.65534878e-01\n",
      "Epoch: 1054 mean train loss:  4.80986491e-05, mean val. loss:  7.65737712e-01\n",
      "Epoch: 1055 mean train loss:  4.65805642e-05, mean val. loss:  7.65944004e-01\n",
      "Epoch: 1056 mean train loss:  4.76519926e-05, mean val. loss:  7.66147256e-01\n",
      "Epoch: 1057 mean train loss:  4.74865374e-05, mean val. loss:  7.66360879e-01\n",
      "Epoch: 1058 mean train loss:  4.77635767e-05, mean val. loss:  7.66572952e-01\n",
      "Epoch: 1059 mean train loss:  4.74647386e-05, mean val. loss:  7.66778767e-01\n",
      "Epoch: 1060 mean train loss:  4.71114763e-05, mean val. loss:  7.66985595e-01\n",
      "Epoch: 1061 mean train loss:  4.71198000e-05, mean val. loss:  7.67195821e-01\n",
      "Epoch: 1062 mean train loss:  4.64062323e-05, mean val. loss:  7.67417371e-01\n",
      "Epoch: 1063 mean train loss:  4.59545699e-05, mean val. loss:  7.67643094e-01\n",
      "Epoch: 1064 mean train loss:  4.65388293e-05, mean val. loss:  7.67865360e-01\n",
      "Epoch: 1065 mean train loss:  4.69239603e-05, mean val. loss:  7.68093348e-01\n",
      "Epoch: 1066 mean train loss:  4.59604198e-05, mean val. loss:  7.68312275e-01\n",
      "Epoch: 1067 mean train loss:  4.77202120e-05, mean val. loss:  7.68528700e-01\n",
      "Epoch: 1068 mean train loss:  4.69169463e-05, mean val. loss:  7.68756449e-01\n",
      "Epoch: 1069 mean train loss:  4.71014937e-05, mean val. loss:  7.68974602e-01\n",
      "Epoch: 1070 mean train loss:  4.72667452e-05, mean val. loss:  7.69198835e-01\n",
      "Epoch: 1071 mean train loss:  4.82558389e-05, mean val. loss:  7.69404829e-01\n",
      "Epoch: 1072 mean train loss:  4.74627013e-05, mean val. loss:  7.69610345e-01\n",
      "Epoch: 1073 mean train loss:  4.61287564e-05, mean val. loss:  7.69821167e-01\n",
      "Epoch: 1074 mean train loss:  4.67229984e-05, mean val. loss:  7.70036221e-01\n",
      "Epoch: 1075 mean train loss:  4.65780031e-05, mean val. loss:  7.70255268e-01\n",
      "Epoch: 1076 mean train loss:  4.70250088e-05, mean val. loss:  7.70466089e-01\n",
      "Epoch: 1077 mean train loss:  4.62231692e-05, mean val. loss:  7.70695210e-01\n",
      "Epoch: 1078 mean train loss:  4.61866730e-05, mean val. loss:  7.70934999e-01\n",
      "Epoch: 1079 mean train loss:  4.64957557e-05, mean val. loss:  7.71168113e-01\n",
      "Epoch: 1080 mean train loss:  4.79159644e-05, mean val. loss:  7.71389008e-01\n",
      "Epoch: 1081 mean train loss:  4.71541134e-05, mean val. loss:  7.71589756e-01\n",
      "Epoch: 1082 mean train loss:  4.65721823e-05, mean val. loss:  7.71799147e-01\n",
      "Epoch: 1083 mean train loss:  4.69980296e-05, mean val. loss:  7.72007346e-01\n",
      "Epoch: 1084 mean train loss:  4.66982019e-05, mean val. loss:  7.72208869e-01\n",
      "Epoch: 1085 mean train loss:  4.70919185e-05, mean val. loss:  7.72408247e-01\n",
      "Epoch: 1086 mean train loss:  4.58892027e-05, mean val. loss:  7.72600830e-01\n",
      "Epoch: 1087 mean train loss:  4.66563797e-05, mean val. loss:  7.72797942e-01\n",
      "Epoch: 1088 mean train loss:  4.56719426e-05, mean val. loss:  7.72997677e-01\n",
      "Epoch: 1089 mean train loss:  4.73047839e-05, mean val. loss:  7.73192704e-01\n",
      "Epoch: 1090 mean train loss:  4.65250341e-05, mean val. loss:  7.73401499e-01\n",
      "Epoch: 1091 mean train loss:  4.69716615e-05, mean val. loss:  7.73615241e-01\n",
      "Epoch: 1092 mean train loss:  4.74407570e-05, mean val. loss:  7.73819983e-01\n",
      "Epoch: 1093 mean train loss:  4.68432554e-05, mean val. loss:  7.74026930e-01\n",
      "Epoch: 1094 mean train loss:  4.58530849e-05, mean val. loss:  7.74237335e-01\n",
      "Epoch: 1095 mean train loss:  4.66772472e-05, mean val. loss:  7.74433196e-01\n",
      "Epoch: 1096 mean train loss:  4.68898215e-05, mean val. loss:  7.74632215e-01\n",
      "Epoch: 1097 mean train loss:  4.67307400e-05, mean val. loss:  7.74819911e-01\n",
      "Epoch: 1098 mean train loss:  4.73963446e-05, mean val. loss:  7.75012195e-01\n",
      "Epoch: 1099 mean train loss:  4.76862187e-05, mean val. loss:  7.75197446e-01\n",
      "Epoch: 1100 mean train loss:  4.72307438e-05, mean val. loss:  7.75397599e-01\n",
      "Epoch: 1101 mean train loss:  4.71991370e-05, mean val. loss:  7.75591254e-01\n",
      "Epoch: 1102 mean train loss:  4.68131038e-05, mean val. loss:  7.75793374e-01\n",
      "Epoch: 1103 mean train loss:  4.64541372e-05, mean val. loss:  7.75998890e-01\n",
      "Epoch: 1104 mean train loss:  4.68409853e-05, mean val. loss:  7.76206017e-01\n",
      "Epoch: 1105 mean train loss:  4.72327229e-05, mean val. loss:  7.76403546e-01\n",
      "Epoch: 1106 mean train loss:  4.74299595e-05, mean val. loss:  7.76592016e-01\n",
      "Epoch: 1107 mean train loss:  4.71569947e-05, mean val. loss:  7.76782811e-01\n",
      "Epoch: 1108 mean train loss:  4.72230604e-05, mean val. loss:  7.76970685e-01\n",
      "Epoch: 1109 mean train loss:  4.63802135e-05, mean val. loss:  7.77171969e-01\n",
      "Epoch: 1110 mean train loss:  4.64018085e-05, mean val. loss:  7.77378559e-01\n",
      "Epoch: 1111 mean train loss:  4.76334244e-05, mean val. loss:  7.77580738e-01\n",
      "Epoch: 1112 mean train loss:  4.51772648e-05, mean val. loss:  7.77778924e-01\n",
      "Epoch: 1113 mean train loss:  4.70429659e-05, mean val. loss:  7.77959824e-01\n",
      "Epoch: 1114 mean train loss:  4.78080474e-05, mean val. loss:  7.78150499e-01\n",
      "Epoch: 1115 mean train loss:  4.74972767e-05, mean val. loss:  7.78331459e-01\n",
      "Epoch: 1116 mean train loss:  4.69092629e-05, mean val. loss:  7.78515518e-01\n",
      "Epoch: 1117 mean train loss:  4.60632727e-05, mean val. loss:  7.78715253e-01\n",
      "Epoch: 1118 mean train loss:  4.63108008e-05, mean val. loss:  7.78906047e-01\n",
      "Epoch: 1119 mean train loss:  4.73928812e-05, mean val. loss:  7.79094100e-01\n",
      "Epoch: 1120 mean train loss:  4.68397047e-05, mean val. loss:  7.79272616e-01\n",
      "Epoch: 1121 mean train loss:  4.53515095e-05, mean val. loss:  7.79471636e-01\n",
      "Epoch: 1122 mean train loss:  4.50443476e-05, mean val. loss:  7.79666364e-01\n",
      "Epoch: 1123 mean train loss:  4.73359250e-05, mean val. loss:  7.79850662e-01\n",
      "Epoch: 1124 mean train loss:  4.66636848e-05, mean val. loss:  7.80043364e-01\n",
      "Epoch: 1125 mean train loss:  4.67432546e-05, mean val. loss:  7.80230403e-01\n",
      "Epoch: 1126 mean train loss:  4.70047235e-05, mean val. loss:  7.80408561e-01\n",
      "Epoch: 1127 mean train loss:  4.60385345e-05, mean val. loss:  7.80590296e-01\n",
      "Epoch: 1128 mean train loss:  4.64955228e-05, mean val. loss:  7.80772030e-01\n",
      "Epoch: 1129 mean train loss:  4.65099583e-05, mean val. loss:  7.80955851e-01\n",
      "Epoch: 1130 mean train loss:  4.68932558e-05, mean val. loss:  7.81149983e-01\n",
      "Epoch: 1131 mean train loss:  4.72059182e-05, mean val. loss:  7.81333745e-01\n",
      "Epoch: 1132 mean train loss:  4.75073757e-05, mean val. loss:  7.81514645e-01\n",
      "Epoch: 1133 mean train loss:  4.66492784e-05, mean val. loss:  7.81700075e-01\n",
      "Epoch: 1134 mean train loss:  4.59726434e-05, mean val. loss:  7.81881630e-01\n",
      "Epoch: 1135 mean train loss:  4.63840552e-05, mean val. loss:  7.82071173e-01\n",
      "Epoch: 1136 mean train loss:  4.73312102e-05, mean val. loss:  7.82238007e-01\n",
      "Epoch: 1137 mean train loss:  4.67613572e-05, mean val. loss:  7.82409310e-01\n",
      "Epoch: 1138 mean train loss:  4.52140230e-05, mean val. loss:  7.82588542e-01\n",
      "Epoch: 1139 mean train loss:  4.64030018e-05, mean val. loss:  7.82757998e-01\n",
      "Epoch: 1140 mean train loss:  4.63093747e-05, mean val. loss:  7.82928467e-01\n",
      "Epoch: 1141 mean train loss:  4.70519881e-05, mean val. loss:  7.83094168e-01\n",
      "Epoch: 1142 mean train loss:  4.51139058e-05, mean val. loss:  7.83266544e-01\n",
      "Epoch: 1143 mean train loss:  4.65126941e-05, mean val. loss:  7.83435106e-01\n",
      "Epoch: 1144 mean train loss:  4.55082627e-05, mean val. loss:  7.83625484e-01\n",
      "Epoch: 1145 mean train loss:  4.65620833e-05, mean val. loss:  7.83799589e-01\n",
      "Epoch: 1146 mean train loss:  4.60698211e-05, mean val. loss:  7.83975303e-01\n",
      "Epoch: 1147 mean train loss:  4.63832403e-05, mean val. loss:  7.84153402e-01\n",
      "Epoch: 1148 mean train loss:  4.58587892e-05, mean val. loss:  7.84336865e-01\n",
      "Epoch: 1149 mean train loss:  4.65056510e-05, mean val. loss:  7.84524858e-01\n",
      "Epoch: 1150 mean train loss:  4.63431934e-05, mean val. loss:  7.84714997e-01\n",
      "Epoch: 1151 mean train loss:  4.59336443e-05, mean val. loss:  7.84893274e-01\n",
      "Epoch: 1152 mean train loss:  4.50758380e-05, mean val. loss:  7.85083532e-01\n",
      "Epoch: 1153 mean train loss:  4.67572536e-05, mean val. loss:  7.85265923e-01\n",
      "Epoch: 1154 mean train loss:  4.58634458e-05, mean val. loss:  7.85452187e-01\n",
      "Epoch: 1155 mean train loss:  4.67992213e-05, mean val. loss:  7.85639882e-01\n",
      "Epoch: 1156 mean train loss:  4.63519827e-05, mean val. loss:  7.85817087e-01\n",
      "Epoch: 1157 mean train loss:  4.54498222e-05, mean val. loss:  7.86003351e-01\n",
      "Epoch: 1158 mean train loss:  4.61994205e-05, mean val. loss:  7.86181152e-01\n",
      "Epoch: 1159 mean train loss:  4.59891744e-05, mean val. loss:  7.86349356e-01\n",
      "Epoch: 1160 mean train loss:  4.55696136e-05, mean val. loss:  7.86531866e-01\n",
      "Epoch: 1161 mean train loss:  4.52368986e-05, mean val. loss:  7.86720514e-01\n",
      "Epoch: 1162 mean train loss:  4.59073926e-05, mean val. loss:  7.86904037e-01\n",
      "Epoch: 1163 mean train loss:  4.69120569e-05, mean val. loss:  7.87080109e-01\n",
      "Epoch: 1164 mean train loss:  4.57912101e-05, mean val. loss:  7.87248969e-01\n",
      "Epoch: 1165 mean train loss:  4.50938242e-05, mean val. loss:  7.87438750e-01\n",
      "Epoch: 1166 mean train loss:  4.64488403e-05, mean val. loss:  7.87623525e-01\n",
      "Epoch: 1167 mean train loss:  4.65682242e-05, mean val. loss:  7.87809551e-01\n",
      "Epoch: 1168 mean train loss:  4.52960958e-05, mean val. loss:  7.87996233e-01\n",
      "Epoch: 1169 mean train loss:  4.78672446e-05, mean val. loss:  7.88183749e-01\n",
      "Epoch: 1170 mean train loss:  4.67726495e-05, mean val. loss:  7.88374066e-01\n",
      "Epoch: 1171 mean train loss:  4.51944652e-05, mean val. loss:  7.88567960e-01\n",
      "Epoch: 1172 mean train loss:  4.61065210e-05, mean val. loss:  7.88756073e-01\n",
      "Epoch: 1173 mean train loss:  4.57396964e-05, mean val. loss:  7.88951218e-01\n",
      "Epoch: 1174 mean train loss:  4.64823679e-05, mean val. loss:  7.89148569e-01\n",
      "Epoch: 1175 mean train loss:  4.59373114e-05, mean val. loss:  7.89358020e-01\n",
      "Epoch: 1176 mean train loss:  4.58845170e-05, mean val. loss:  7.89562821e-01\n",
      "Epoch: 1177 mean train loss:  4.60933079e-05, mean val. loss:  7.89760709e-01\n",
      "Epoch: 1178 mean train loss:  4.51594824e-05, mean val. loss:  7.89973974e-01\n",
      "Epoch: 1179 mean train loss:  4.59495932e-05, mean val. loss:  7.90180743e-01\n",
      "Epoch: 1180 mean train loss:  4.66176425e-05, mean val. loss:  7.90375233e-01\n",
      "Epoch: 1181 mean train loss:  4.60557640e-05, mean val. loss:  7.90573776e-01\n",
      "Epoch: 1182 mean train loss:  4.65982012e-05, mean val. loss:  7.90773094e-01\n",
      "Epoch: 1183 mean train loss:  4.52006352e-05, mean val. loss:  7.90967405e-01\n",
      "Epoch: 1184 mean train loss:  4.65167104e-05, mean val. loss:  7.91156828e-01\n",
      "Epoch: 1185 mean train loss:  4.48274077e-05, mean val. loss:  7.91347742e-01\n",
      "Epoch: 1186 mean train loss:  4.53815446e-05, mean val. loss:  7.91536331e-01\n",
      "Epoch: 1187 mean train loss:  4.42320597e-05, mean val. loss:  7.91740894e-01\n",
      "Epoch: 1188 mean train loss:  4.59711300e-05, mean val. loss:  7.91952670e-01\n",
      "Epoch: 1189 mean train loss:  4.67436621e-05, mean val. loss:  7.92151690e-01\n",
      "Epoch: 1190 mean train loss:  4.61181044e-05, mean val. loss:  7.92346537e-01\n",
      "Epoch: 1191 mean train loss:  4.62976750e-05, mean val. loss:  7.92540967e-01\n",
      "Epoch: 1192 mean train loss:  4.55955160e-05, mean val. loss:  7.92733669e-01\n",
      "Epoch: 1193 mean train loss:  4.69195074e-05, mean val. loss:  7.92917669e-01\n",
      "Epoch: 1194 mean train loss:  4.60615265e-05, mean val. loss:  7.93108106e-01\n",
      "Epoch: 1195 mean train loss:  4.54825349e-05, mean val. loss:  7.93295443e-01\n",
      "Epoch: 1196 mean train loss:  4.52407985e-05, mean val. loss:  7.93491006e-01\n",
      "Epoch: 1197 mean train loss:  4.53010434e-05, mean val. loss:  7.93695927e-01\n",
      "Epoch: 1198 mean train loss:  4.59696748e-05, mean val. loss:  7.93896675e-01\n",
      "Epoch: 1199 mean train loss:  4.72261454e-05, mean val. loss:  7.94091761e-01\n",
      "Epoch: 1200 mean train loss:  4.49874788e-05, mean val. loss:  7.94299543e-01\n",
      "Epoch: 1201 mean train loss:  4.55841073e-05, mean val. loss:  7.94499338e-01\n",
      "Epoch: 1202 mean train loss:  4.67924983e-05, mean val. loss:  7.94698477e-01\n",
      "Epoch: 1203 mean train loss:  4.66911588e-05, mean val. loss:  7.94894278e-01\n",
      "Epoch: 1204 mean train loss:  4.60123410e-05, mean val. loss:  7.95087397e-01\n",
      "Epoch: 1205 mean train loss:  4.61337040e-05, mean val. loss:  7.95282245e-01\n",
      "Epoch: 1206 mean train loss:  4.63039614e-05, mean val. loss:  7.95471251e-01\n",
      "Epoch: 1207 mean train loss:  4.46250197e-05, mean val. loss:  7.95664012e-01\n",
      "Epoch: 1208 mean train loss:  4.58338764e-05, mean val. loss:  7.95858741e-01\n",
      "Epoch: 1209 mean train loss:  4.55474947e-05, mean val. loss:  7.96058595e-01\n",
      "Epoch: 1210 mean train loss:  4.60800948e-05, mean val. loss:  7.96255648e-01\n",
      "Epoch: 1211 mean train loss:  4.57681017e-05, mean val. loss:  7.96444893e-01\n",
      "Epoch: 1212 mean train loss:  4.55448753e-05, mean val. loss:  7.96627939e-01\n",
      "Epoch: 1213 mean train loss:  4.66340571e-05, mean val. loss:  7.96801686e-01\n",
      "Epoch: 1214 mean train loss:  4.66206111e-05, mean val. loss:  7.96978354e-01\n",
      "Epoch: 1215 mean train loss:  4.56091948e-05, mean val. loss:  7.97145128e-01\n",
      "Epoch: 1216 mean train loss:  4.52375971e-05, mean val. loss:  7.97307551e-01\n",
      "Epoch: 1217 mean train loss:  4.50026710e-05, mean val. loss:  7.97477424e-01\n",
      "Epoch: 1218 mean train loss:  4.53569228e-05, mean val. loss:  7.97647715e-01\n",
      "Epoch: 1219 mean train loss:  4.48579085e-05, mean val. loss:  7.97825515e-01\n",
      "Epoch: 1220 mean train loss:  4.46917838e-05, mean val. loss:  7.98005641e-01\n",
      "Epoch: 1221 mean train loss:  4.54879482e-05, mean val. loss:  7.98195839e-01\n",
      "Epoch: 1222 mean train loss:  4.66663041e-05, mean val. loss:  7.98382938e-01\n",
      "Epoch: 1223 mean train loss:  4.51453961e-05, mean val. loss:  7.98574030e-01\n",
      "Epoch: 1224 mean train loss:  4.51551168e-05, mean val. loss:  7.98769236e-01\n",
      "Epoch: 1225 mean train loss:  4.66014608e-05, mean val. loss:  7.98948586e-01\n",
      "Epoch: 1226 mean train loss:  4.56558773e-05, mean val. loss:  7.99134612e-01\n",
      "Epoch: 1227 mean train loss:  4.55307309e-05, mean val. loss:  7.99324095e-01\n",
      "Epoch: 1228 mean train loss:  4.51331143e-05, mean val. loss:  7.99516976e-01\n",
      "Epoch: 1229 mean train loss:  4.47134953e-05, mean val. loss:  7.99720347e-01\n",
      "Epoch: 1230 mean train loss:  4.52129170e-05, mean val. loss:  7.99924850e-01\n",
      "Epoch: 1231 mean train loss:  4.39619180e-05, mean val. loss:  8.00139487e-01\n",
      "Epoch: 1232 mean train loss:  4.59483708e-05, mean val. loss:  8.00340772e-01\n",
      "Epoch: 1233 mean train loss:  4.46892809e-05, mean val. loss:  8.00545096e-01\n",
      "Epoch: 1234 mean train loss:  4.61377786e-05, mean val. loss:  8.00744236e-01\n",
      "Epoch: 1235 mean train loss:  4.51665837e-05, mean val. loss:  8.00944626e-01\n",
      "Epoch: 1236 mean train loss:  4.53009270e-05, mean val. loss:  8.01147521e-01\n",
      "Epoch: 1237 mean train loss:  4.59337607e-05, mean val. loss:  8.01346302e-01\n",
      "Epoch: 1238 mean train loss:  4.44038888e-05, mean val. loss:  8.01542461e-01\n",
      "Epoch: 1239 mean train loss:  4.52298555e-05, mean val. loss:  8.01748157e-01\n",
      "Epoch: 1240 mean train loss:  4.58433060e-05, mean val. loss:  8.01948965e-01\n",
      "Epoch: 1241 mean train loss:  4.55712434e-05, mean val. loss:  8.02151620e-01\n",
      "Epoch: 1242 mean train loss:  4.51596570e-05, mean val. loss:  8.02344322e-01\n",
      "Epoch: 1243 mean train loss:  4.47864877e-05, mean val. loss:  8.02543342e-01\n",
      "Epoch: 1244 mean train loss:  4.58999421e-05, mean val. loss:  8.02737415e-01\n",
      "Epoch: 1245 mean train loss:  4.50796797e-05, mean val. loss:  8.02935243e-01\n",
      "Epoch: 1246 mean train loss:  4.61898162e-05, mean val. loss:  8.03130031e-01\n",
      "Epoch: 1247 mean train loss:  4.52893437e-05, mean val. loss:  8.03327739e-01\n",
      "Epoch: 1248 mean train loss:  4.57466813e-05, mean val. loss:  8.03525150e-01\n",
      "Epoch: 1249 mean train loss:  4.48715291e-05, mean val. loss:  8.03726614e-01\n",
      "Epoch: 1250 mean train loss:  4.39997530e-05, mean val. loss:  8.03929389e-01\n",
      "Epoch: 1251 mean train loss:  4.62052412e-05, mean val. loss:  8.04127216e-01\n",
      "Epoch: 1252 mean train loss:  4.61477903e-05, mean val. loss:  8.04316282e-01\n",
      "Epoch: 1253 mean train loss:  4.53530229e-05, mean val. loss:  8.04508090e-01\n",
      "Epoch: 1254 mean train loss:  4.45370679e-05, mean val. loss:  8.04714262e-01\n",
      "Epoch: 1255 mean train loss:  4.51076776e-05, mean val. loss:  8.04912627e-01\n",
      "Epoch: 1256 mean train loss:  4.58493014e-05, mean val. loss:  8.05109084e-01\n",
      "Epoch: 1257 mean train loss:  4.58422583e-05, mean val. loss:  8.05306256e-01\n",
      "Epoch: 1258 mean train loss:  4.52706590e-05, mean val. loss:  8.05488646e-01\n",
      "Epoch: 1259 mean train loss:  4.51915548e-05, mean val. loss:  8.05686533e-01\n",
      "Epoch: 1260 mean train loss:  4.58583236e-05, mean val. loss:  8.05862248e-01\n",
      "Epoch: 1261 mean train loss:  4.56003472e-05, mean val. loss:  8.06038678e-01\n",
      "Epoch: 1262 mean train loss:  4.59426665e-05, mean val. loss:  8.06206882e-01\n",
      "Epoch: 1263 mean train loss:  4.47827042e-05, mean val. loss:  8.06374550e-01\n",
      "Epoch: 1264 mean train loss:  4.53214161e-05, mean val. loss:  8.06560218e-01\n",
      "Epoch: 1265 mean train loss:  4.61481977e-05, mean val. loss:  8.06723356e-01\n",
      "Epoch: 1266 mean train loss:  4.48923674e-05, mean val. loss:  8.06888103e-01\n",
      "Epoch: 1267 mean train loss:  4.52202512e-05, mean val. loss:  8.07060122e-01\n",
      "Epoch: 1268 mean train loss:  4.57263086e-05, mean val. loss:  8.07226777e-01\n",
      "Epoch: 1269 mean train loss:  4.57199640e-05, mean val. loss:  8.07406187e-01\n",
      "Epoch: 1270 mean train loss:  4.51783417e-05, mean val. loss:  8.07581186e-01\n",
      "Epoch: 1271 mean train loss:  4.49157669e-05, mean val. loss:  8.07745516e-01\n",
      "Epoch: 1272 mean train loss:  4.38679708e-05, mean val. loss:  8.07922959e-01\n",
      "Epoch: 1273 mean train loss:  4.43262979e-05, mean val. loss:  8.08106780e-01\n",
      "Epoch: 1274 mean train loss:  4.45969636e-05, mean val. loss:  8.08285296e-01\n",
      "Epoch: 1275 mean train loss:  4.57856804e-05, mean val. loss:  8.08462799e-01\n",
      "Epoch: 1276 mean train loss:  4.45306650e-05, mean val. loss:  8.08649480e-01\n",
      "Epoch: 1277 mean train loss:  4.46983031e-05, mean val. loss:  8.08835864e-01\n",
      "Epoch: 1278 mean train loss:  4.46015620e-05, mean val. loss:  8.09026122e-01\n",
      "Epoch: 1279 mean train loss:  4.51790402e-05, mean val. loss:  8.09221864e-01\n",
      "Epoch: 1280 mean train loss:  4.59995936e-05, mean val. loss:  8.09418321e-01\n",
      "Epoch: 1281 mean train loss:  4.58030845e-05, mean val. loss:  8.09607923e-01\n",
      "Epoch: 1282 mean train loss:  4.45414335e-05, mean val. loss:  8.09791446e-01\n",
      "Epoch: 1283 mean train loss:  4.58549475e-05, mean val. loss:  8.09965551e-01\n",
      "Epoch: 1284 mean train loss:  4.57737478e-05, mean val. loss:  8.10129225e-01\n",
      "Epoch: 1285 mean train loss:  4.39996365e-05, mean val. loss:  8.10302019e-01\n",
      "Epoch: 1286 mean train loss:  4.49765357e-05, mean val. loss:  8.10471892e-01\n",
      "Epoch: 1287 mean train loss:  4.53556422e-05, mean val. loss:  8.10641348e-01\n",
      "Epoch: 1288 mean train loss:  4.50567459e-05, mean val. loss:  8.10818076e-01\n",
      "Epoch: 1289 mean train loss:  4.45449841e-05, mean val. loss:  8.10995221e-01\n",
      "Epoch: 1290 mean train loss:  4.52415552e-05, mean val. loss:  8.11180055e-01\n",
      "Epoch: 1291 mean train loss:  4.45715268e-05, mean val. loss:  8.11367095e-01\n",
      "Epoch: 1292 mean train loss:  4.45498736e-05, mean val. loss:  8.11555803e-01\n",
      "Epoch: 1293 mean train loss:  4.48361970e-05, mean val. loss:  8.11727345e-01\n",
      "Epoch: 1294 mean train loss:  4.51265369e-05, mean val. loss:  8.11910808e-01\n",
      "Epoch: 1295 mean train loss:  4.56317212e-05, mean val. loss:  8.12088251e-01\n",
      "Epoch: 1296 mean train loss:  4.36112750e-05, mean val. loss:  8.12274814e-01\n",
      "Epoch: 1297 mean train loss:  4.54537803e-05, mean val. loss:  8.12454760e-01\n",
      "Epoch: 1298 mean train loss:  4.49250801e-05, mean val. loss:  8.12631369e-01\n",
      "Epoch: 1299 mean train loss:  4.48078499e-05, mean val. loss:  8.12814832e-01\n",
      "Epoch: 1300 mean train loss:  4.51028463e-05, mean val. loss:  8.13001215e-01\n",
      "Epoch: 1301 mean train loss:  4.40165750e-05, mean val. loss:  8.13197494e-01\n",
      "Epoch: 1302 mean train loss:  4.54850961e-05, mean val. loss:  8.13383281e-01\n",
      "Epoch: 1303 mean train loss:  4.43478930e-05, mean val. loss:  8.13572407e-01\n",
      "Epoch: 1304 mean train loss:  4.54745023e-05, mean val. loss:  8.13758910e-01\n",
      "Epoch: 1305 mean train loss:  4.55126865e-05, mean val. loss:  8.13935518e-01\n",
      "Epoch: 1306 mean train loss:  4.47156490e-05, mean val. loss:  8.14105332e-01\n",
      "Epoch: 1307 mean train loss:  4.45930636e-05, mean val. loss:  8.14277947e-01\n",
      "Epoch: 1308 mean train loss:  4.56071575e-05, mean val. loss:  8.14450920e-01\n",
      "Epoch: 1309 mean train loss:  4.47495840e-05, mean val. loss:  8.14638495e-01\n",
      "Epoch: 1310 mean train loss:  4.48857318e-05, mean val. loss:  8.14819396e-01\n",
      "Epoch: 1311 mean train loss:  4.54847468e-05, mean val. loss:  8.14993203e-01\n",
      "Epoch: 1312 mean train loss:  4.46984195e-05, mean val. loss:  8.15165222e-01\n",
      "Epoch: 1313 mean train loss:  4.48599458e-05, mean val. loss:  8.15331757e-01\n",
      "Epoch: 1314 mean train loss:  4.48603532e-05, mean val. loss:  8.15513015e-01\n",
      "Epoch: 1315 mean train loss:  4.38906136e-05, mean val. loss:  8.15702558e-01\n",
      "Epoch: 1316 mean train loss:  4.49438812e-05, mean val. loss:  8.15895319e-01\n",
      "Epoch: 1317 mean train loss:  4.39431169e-05, mean val. loss:  8.16087186e-01\n",
      "Epoch: 1318 mean train loss:  4.48183855e-05, mean val. loss:  8.16272557e-01\n",
      "Epoch: 1319 mean train loss:  4.51015076e-05, mean val. loss:  8.16466331e-01\n",
      "Epoch: 1320 mean train loss:  4.46898630e-05, mean val. loss:  8.16668868e-01\n",
      "Epoch: 1321 mean train loss:  4.51100059e-05, mean val. loss:  8.16860437e-01\n",
      "Epoch: 1322 mean train loss:  4.42804303e-05, mean val. loss:  8.17058623e-01\n",
      "Epoch: 1323 mean train loss:  4.52875975e-05, mean val. loss:  8.17251027e-01\n",
      "Epoch: 1324 mean train loss:  4.44174511e-05, mean val. loss:  8.17437112e-01\n",
      "Epoch: 1325 mean train loss:  4.50498192e-05, mean val. loss:  8.17616701e-01\n",
      "Epoch: 1326 mean train loss:  4.51875385e-05, mean val. loss:  8.17792833e-01\n",
      "Epoch: 1327 mean train loss:  4.50681546e-05, mean val. loss:  8.17957461e-01\n",
      "Epoch: 1328 mean train loss:  4.43368917e-05, mean val. loss:  8.18117917e-01\n",
      "Epoch: 1329 mean train loss:  4.52643144e-05, mean val. loss:  8.18281591e-01\n",
      "Epoch: 1330 mean train loss:  4.43240860e-05, mean val. loss:  8.18459868e-01\n",
      "Epoch: 1331 mean train loss:  4.52207169e-05, mean val. loss:  8.18634450e-01\n",
      "Epoch: 1332 mean train loss:  4.53189714e-05, mean val. loss:  8.18807125e-01\n",
      "Epoch: 1333 mean train loss:  4.46356134e-05, mean val. loss:  8.18971574e-01\n",
      "Epoch: 1334 mean train loss:  4.37068520e-05, mean val. loss:  8.19156110e-01\n",
      "Epoch: 1335 mean train loss:  4.57551796e-05, mean val. loss:  8.19335341e-01\n",
      "Epoch: 1336 mean train loss:  4.49531944e-05, mean val. loss:  8.19513321e-01\n",
      "Epoch: 1337 mean train loss:  4.40259464e-05, mean val. loss:  8.19698811e-01\n",
      "Epoch: 1338 mean train loss:  4.46785707e-05, mean val. loss:  8.19894314e-01\n",
      "Epoch: 1339 mean train loss:  4.39167488e-05, mean val. loss:  8.20100307e-01\n",
      "Epoch: 1340 mean train loss:  4.47518541e-05, mean val. loss:  8.20303798e-01\n",
      "Epoch: 1341 mean train loss:  4.36176197e-05, mean val. loss:  8.20496261e-01\n",
      "Epoch: 1342 mean train loss:  4.47333441e-05, mean val. loss:  8.20697725e-01\n",
      "Epoch: 1343 mean train loss:  4.50636144e-05, mean val. loss:  8.20892632e-01\n",
      "Epoch: 1344 mean train loss:  4.44551697e-05, mean val. loss:  8.21078539e-01\n",
      "Epoch: 1345 mean train loss:  4.43009194e-05, mean val. loss:  8.21276903e-01\n",
      "Epoch: 1346 mean train loss:  4.54345718e-05, mean val. loss:  8.21461618e-01\n",
      "Epoch: 1347 mean train loss:  4.42563323e-05, mean val. loss:  8.21659267e-01\n",
      "Epoch: 1348 mean train loss:  4.48624487e-05, mean val. loss:  8.21840346e-01\n",
      "Epoch: 1349 mean train loss:  4.48250212e-05, mean val. loss:  8.22019994e-01\n",
      "Epoch: 1350 mean train loss:  4.37255367e-05, mean val. loss:  8.22205663e-01\n",
      "Epoch: 1351 mean train loss:  4.42746677e-05, mean val. loss:  8.22396457e-01\n",
      "Epoch: 1352 mean train loss:  4.54373076e-05, mean val. loss:  8.22568774e-01\n",
      "Epoch: 1353 mean train loss:  4.52675740e-05, mean val. loss:  8.22729528e-01\n",
      "Epoch: 1354 mean train loss:  4.54479014e-05, mean val. loss:  8.22886288e-01\n",
      "Epoch: 1355 mean train loss:  4.29708161e-05, mean val. loss:  8.23057234e-01\n",
      "Epoch: 1356 mean train loss:  4.45457990e-05, mean val. loss:  8.23220134e-01\n",
      "Epoch: 1357 mean train loss:  4.42517339e-05, mean val. loss:  8.23379338e-01\n",
      "Epoch: 1358 mean train loss:  4.39292053e-05, mean val. loss:  8.23535860e-01\n",
      "Epoch: 1359 mean train loss:  4.51593078e-05, mean val. loss:  8.23687494e-01\n",
      "Epoch: 1360 mean train loss:  4.43836325e-05, mean val. loss:  8.23845387e-01\n",
      "Epoch: 1361 mean train loss:  4.42099990e-05, mean val. loss:  8.23999405e-01\n",
      "Epoch: 1362 mean train loss:  4.37813578e-05, mean val. loss:  8.24170709e-01\n",
      "Epoch: 1363 mean train loss:  4.52424865e-05, mean val. loss:  8.24335158e-01\n",
      "Epoch: 1364 mean train loss:  4.52930690e-05, mean val. loss:  8.24505270e-01\n",
      "Epoch: 1365 mean train loss:  4.45696642e-05, mean val. loss:  8.24681342e-01\n",
      "Epoch: 1366 mean train loss:  4.46048798e-05, mean val. loss:  8.24843764e-01\n",
      "Epoch: 1367 mean train loss:  4.42030141e-05, mean val. loss:  8.25019658e-01\n",
      "Epoch: 1368 mean train loss:  4.39973664e-05, mean val. loss:  8.25192630e-01\n",
      "Epoch: 1369 mean train loss:  4.32466040e-05, mean val. loss:  8.25375080e-01\n",
      "Epoch: 1370 mean train loss:  4.38391580e-05, mean val. loss:  8.25579345e-01\n",
      "Epoch: 1371 mean train loss:  4.28977073e-05, mean val. loss:  8.25801790e-01\n",
      "Epoch: 1372 mean train loss:  4.48088977e-05, mean val. loss:  8.26013207e-01\n",
      "Epoch: 1373 mean train loss:  4.40065050e-05, mean val. loss:  8.26218963e-01\n",
      "Epoch: 1374 mean train loss:  4.53607645e-05, mean val. loss:  8.26417148e-01\n",
      "Epoch: 1375 mean train loss:  4.49746149e-05, mean val. loss:  8.26610923e-01\n",
      "Epoch: 1376 mean train loss:  4.42645978e-05, mean val. loss:  8.26797009e-01\n",
      "Epoch: 1377 mean train loss:  4.46935883e-05, mean val. loss:  8.26998770e-01\n",
      "Epoch: 1378 mean train loss:  4.49957442e-05, mean val. loss:  8.27192545e-01\n",
      "Epoch: 1379 mean train loss:  4.43739700e-05, mean val. loss:  8.27387512e-01\n",
      "Epoch: 1380 mean train loss:  4.40721633e-05, mean val. loss:  8.27583134e-01\n",
      "Epoch: 1381 mean train loss:  4.44621546e-05, mean val. loss:  8.27774048e-01\n",
      "Epoch: 1382 mean train loss:  4.38423594e-05, mean val. loss:  8.27963531e-01\n",
      "Epoch: 1383 mean train loss:  4.44429461e-05, mean val. loss:  8.28137636e-01\n",
      "Epoch: 1384 mean train loss:  4.42237360e-05, mean val. loss:  8.28315556e-01\n",
      "Epoch: 1385 mean train loss:  4.43058088e-05, mean val. loss:  8.28503072e-01\n",
      "Epoch: 1386 mean train loss:  4.37050476e-05, mean val. loss:  8.28686476e-01\n",
      "Epoch: 1387 mean train loss:  4.34873509e-05, mean val. loss:  8.28877687e-01\n",
      "Epoch: 1388 mean train loss:  4.30506188e-05, mean val. loss:  8.29068601e-01\n",
      "Epoch: 1389 mean train loss:  4.46529593e-05, mean val. loss:  8.29263687e-01\n",
      "Epoch: 1390 mean train loss:  4.35797265e-05, mean val. loss:  8.29443812e-01\n",
      "Epoch: 1391 mean train loss:  4.43701283e-05, mean val. loss:  8.29629183e-01\n",
      "Epoch: 1392 mean train loss:  4.37493436e-05, mean val. loss:  8.29808891e-01\n",
      "Epoch: 1393 mean train loss:  4.35476541e-05, mean val. loss:  8.29991877e-01\n",
      "Epoch: 1394 mean train loss:  4.45939950e-05, mean val. loss:  8.30170929e-01\n",
      "Epoch: 1395 mean train loss:  4.29676147e-05, mean val. loss:  8.30353916e-01\n",
      "Epoch: 1396 mean train loss:  4.38220450e-05, mean val. loss:  8.30547214e-01\n",
      "Epoch: 1397 mean train loss:  4.36890987e-05, mean val. loss:  8.30742955e-01\n",
      "Epoch: 1398 mean train loss:  4.39891592e-05, mean val. loss:  8.30940127e-01\n",
      "Epoch: 1399 mean train loss:  4.34655813e-05, mean val. loss:  8.31135690e-01\n",
      "Epoch: 1400 mean train loss:  4.54116380e-05, mean val. loss:  8.31324935e-01\n",
      "Epoch: 1401 mean train loss:  4.45910846e-05, mean val. loss:  8.31505239e-01\n",
      "Epoch: 1402 mean train loss:  4.29587089e-05, mean val. loss:  8.31701219e-01\n",
      "Epoch: 1403 mean train loss:  4.41194861e-05, mean val. loss:  8.31878960e-01\n",
      "Epoch: 1404 mean train loss:  4.36751288e-05, mean val. loss:  8.32060575e-01\n",
      "Epoch: 1405 mean train loss:  4.34629619e-05, mean val. loss:  8.32236350e-01\n",
      "Epoch: 1406 mean train loss:  4.43212339e-05, mean val. loss:  8.32410455e-01\n",
      "Epoch: 1407 mean train loss:  4.43810131e-05, mean val. loss:  8.32579017e-01\n",
      "Epoch: 1408 mean train loss:  4.34807735e-05, mean val. loss:  8.32748473e-01\n",
      "Epoch: 1409 mean train loss:  4.42489400e-05, mean val. loss:  8.32913101e-01\n",
      "Epoch: 1410 mean train loss:  4.39072610e-05, mean val. loss:  8.33081961e-01\n",
      "Epoch: 1411 mean train loss:  4.38926509e-05, mean val. loss:  8.33260894e-01\n",
      "Epoch: 1412 mean train loss:  4.50420775e-05, mean val. loss:  8.33434105e-01\n",
      "Epoch: 1413 mean train loss:  4.38088900e-05, mean val. loss:  8.33611965e-01\n",
      "Epoch: 1414 mean train loss:  4.51571541e-05, mean val. loss:  8.33777845e-01\n",
      "Epoch: 1415 mean train loss:  4.38563875e-05, mean val. loss:  8.33942533e-01\n",
      "Epoch: 1416 mean train loss:  4.38620918e-05, mean val. loss:  8.34107757e-01\n",
      "Epoch: 1417 mean train loss:  4.40577278e-05, mean val. loss:  8.34274828e-01\n",
      "Epoch: 1418 mean train loss:  4.31641238e-05, mean val. loss:  8.34464371e-01\n",
      "Epoch: 1419 mean train loss:  4.33592359e-05, mean val. loss:  8.34648073e-01\n",
      "Epoch: 1420 mean train loss:  4.31830413e-05, mean val. loss:  8.34825635e-01\n",
      "Epoch: 1421 mean train loss:  4.43196041e-05, mean val. loss:  8.35003614e-01\n",
      "Epoch: 1422 mean train loss:  4.28973581e-05, mean val. loss:  8.35194826e-01\n",
      "Epoch: 1423 mean train loss:  4.35024267e-05, mean val. loss:  8.35390985e-01\n",
      "Epoch: 1424 mean train loss:  4.28578933e-05, mean val. loss:  8.35600376e-01\n",
      "Epoch: 1425 mean train loss:  4.46114573e-05, mean val. loss:  8.35794449e-01\n",
      "Epoch: 1426 mean train loss:  4.38423012e-05, mean val. loss:  8.36000800e-01\n",
      "Epoch: 1427 mean train loss:  4.37552226e-05, mean val. loss:  8.36208761e-01\n",
      "Epoch: 1428 mean train loss:  4.43492318e-05, mean val. loss:  8.36403430e-01\n",
      "Epoch: 1429 mean train loss:  4.47081402e-05, mean val. loss:  8.36596727e-01\n",
      "Epoch: 1430 mean train loss:  4.51542437e-05, mean val. loss:  8.36784303e-01\n",
      "Epoch: 1431 mean train loss:  4.29121428e-05, mean val. loss:  8.36973131e-01\n",
      "Epoch: 1432 mean train loss:  4.38779243e-05, mean val. loss:  8.37175310e-01\n",
      "Epoch: 1433 mean train loss:  4.46671038e-05, mean val. loss:  8.37368786e-01\n",
      "Epoch: 1434 mean train loss:  4.47231578e-05, mean val. loss:  8.37553740e-01\n",
      "Epoch: 1435 mean train loss:  4.39432915e-05, mean val. loss:  8.37740660e-01\n",
      "Epoch: 1436 mean train loss:  4.45511541e-05, mean val. loss:  8.37923586e-01\n",
      "Epoch: 1437 mean train loss:  4.39909054e-05, mean val. loss:  8.38113785e-01\n",
      "Epoch: 1438 mean train loss:  4.46217600e-05, mean val. loss:  8.38282883e-01\n",
      "Epoch: 1439 mean train loss:  4.48887004e-05, mean val. loss:  8.38451624e-01\n",
      "Epoch: 1440 mean train loss:  4.43154131e-05, mean val. loss:  8.38610411e-01\n",
      "Epoch: 1441 mean train loss:  4.32872912e-05, mean val. loss:  8.38769197e-01\n",
      "Epoch: 1442 mean train loss:  4.30396758e-05, mean val. loss:  8.38943958e-01\n",
      "Epoch: 1443 mean train loss:  4.45584883e-05, mean val. loss:  8.39120030e-01\n",
      "Epoch: 1444 mean train loss:  4.42070886e-05, mean val. loss:  8.39288473e-01\n",
      "Epoch: 1445 mean train loss:  4.35046968e-05, mean val. loss:  8.39464188e-01\n",
      "Epoch: 1446 mean train loss:  4.33045207e-05, mean val. loss:  8.39655340e-01\n",
      "Epoch: 1447 mean train loss:  4.41204174e-05, mean val. loss:  8.39844048e-01\n",
      "Epoch: 1448 mean train loss:  4.39821742e-05, mean val. loss:  8.40034664e-01\n",
      "Epoch: 1449 mean train loss:  4.32310044e-05, mean val. loss:  8.40217352e-01\n",
      "Epoch: 1450 mean train loss:  4.34328103e-05, mean val. loss:  8.40391755e-01\n",
      "Epoch: 1451 mean train loss:  4.31857770e-05, mean val. loss:  8.40561867e-01\n",
      "Epoch: 1452 mean train loss:  4.35377005e-05, mean val. loss:  8.40738118e-01\n",
      "Epoch: 1453 mean train loss:  4.33007954e-05, mean val. loss:  8.40926409e-01\n",
      "Epoch: 1454 mean train loss:  4.32818779e-05, mean val. loss:  8.41117740e-01\n",
      "Epoch: 1455 mean train loss:  4.40676813e-05, mean val. loss:  8.41301084e-01\n",
      "Epoch: 1456 mean train loss:  4.40150034e-05, mean val. loss:  8.41478288e-01\n",
      "Epoch: 1457 mean train loss:  4.40356089e-05, mean val. loss:  8.41648936e-01\n",
      "Epoch: 1458 mean train loss:  4.38928837e-05, mean val. loss:  8.41821373e-01\n",
      "Epoch: 1459 mean train loss:  4.34694230e-05, mean val. loss:  8.41982365e-01\n",
      "Epoch: 1460 mean train loss:  4.36702394e-05, mean val. loss:  8.42148006e-01\n",
      "Epoch: 1461 mean train loss:  4.33467794e-05, mean val. loss:  8.42299342e-01\n",
      "Epoch: 1462 mean train loss:  4.31341468e-05, mean val. loss:  8.42462957e-01\n",
      "Epoch: 1463 mean train loss:  4.35594702e-05, mean val. loss:  8.42623055e-01\n",
      "Epoch: 1464 mean train loss:  4.35276888e-05, mean val. loss:  8.42772365e-01\n",
      "Epoch: 1465 mean train loss:  4.37435228e-05, mean val. loss:  8.42922807e-01\n",
      "Epoch: 1466 mean train loss:  4.32045199e-05, mean val. loss:  8.43071282e-01\n",
      "Epoch: 1467 mean train loss:  4.37101698e-05, mean val. loss:  8.43204737e-01\n",
      "Epoch: 1468 mean train loss:  4.39332798e-05, mean val. loss:  8.43337893e-01\n",
      "Epoch: 1469 mean train loss:  4.28975909e-05, mean val. loss:  8.43484819e-01\n",
      "Epoch: 1470 mean train loss:  4.29151114e-05, mean val. loss:  8.43650758e-01\n",
      "Epoch: 1471 mean train loss:  4.43711178e-05, mean val. loss:  8.43809307e-01\n",
      "Epoch: 1472 mean train loss:  4.35476541e-05, mean val. loss:  8.43967557e-01\n",
      "Epoch: 1473 mean train loss:  4.35105758e-05, mean val. loss:  8.44116211e-01\n",
      "Epoch: 1474 mean train loss:  4.40825243e-05, mean val. loss:  8.44276011e-01\n",
      "Epoch: 1475 mean train loss:  4.41994052e-05, mean val. loss:  8.44426513e-01\n",
      "Epoch: 1476 mean train loss:  4.30385116e-05, mean val. loss:  8.44579101e-01\n",
      "Epoch: 1477 mean train loss:  4.35847323e-05, mean val. loss:  8.44735682e-01\n",
      "Epoch: 1478 mean train loss:  4.39424184e-05, mean val. loss:  8.44901443e-01\n",
      "Epoch: 1479 mean train loss:  4.40955628e-05, mean val. loss:  8.45064759e-01\n",
      "Epoch: 1480 mean train loss:  4.35089460e-05, mean val. loss:  8.45221877e-01\n",
      "Epoch: 1481 mean train loss:  4.39394498e-05, mean val. loss:  8.45378101e-01\n",
      "Epoch: 1482 mean train loss:  4.28428175e-05, mean val. loss:  8.45525444e-01\n",
      "Epoch: 1483 mean train loss:  4.43023746e-05, mean val. loss:  8.45673919e-01\n",
      "Epoch: 1484 mean train loss:  4.36013797e-05, mean val. loss:  8.45833480e-01\n",
      "Epoch: 1485 mean train loss:  4.28718049e-05, mean val. loss:  8.45997334e-01\n",
      "Epoch: 1486 mean train loss:  4.29418869e-05, mean val. loss:  8.46169651e-01\n",
      "Epoch: 1487 mean train loss:  4.26212791e-05, mean val. loss:  8.46327543e-01\n",
      "Epoch: 1488 mean train loss:  4.36396222e-05, mean val. loss:  8.46504748e-01\n",
      "Epoch: 1489 mean train loss:  4.38401476e-05, mean val. loss:  8.46677065e-01\n",
      "Epoch: 1490 mean train loss:  4.40037111e-05, mean val. loss:  8.46853495e-01\n",
      "Epoch: 1491 mean train loss:  4.32556262e-05, mean val. loss:  8.47028017e-01\n",
      "Epoch: 1492 mean train loss:  4.21468285e-05, mean val. loss:  8.47205162e-01\n",
      "Epoch: 1493 mean train loss:  4.43254830e-05, mean val. loss:  8.47387135e-01\n",
      "Epoch: 1494 mean train loss:  4.27840278e-05, mean val. loss:  8.47582519e-01\n",
      "Epoch: 1495 mean train loss:  4.28512576e-05, mean val. loss:  8.47769141e-01\n",
      "Epoch: 1496 mean train loss:  4.21751174e-05, mean val. loss:  8.47973168e-01\n",
      "Epoch: 1497 mean train loss:  4.27616760e-05, mean val. loss:  8.48181307e-01\n",
      "Epoch: 1498 mean train loss:  4.17436240e-05, mean val. loss:  8.48403335e-01\n",
      "Epoch: 1499 mean train loss:  4.24167374e-05, mean val. loss:  8.48630607e-01\n",
      "Epoch: 1500 mean train loss:  4.41701850e-05, mean val. loss:  8.48849237e-01\n",
      "Epoch: 1501 mean train loss:  4.41359589e-05, mean val. loss:  8.49068105e-01\n",
      "Epoch: 1502 mean train loss:  4.34394460e-05, mean val. loss:  8.49287927e-01\n",
      "Epoch: 1503 mean train loss:  4.38549905e-05, mean val. loss:  8.49505901e-01\n",
      "Epoch: 1504 mean train loss:  4.33981186e-05, mean val. loss:  8.49722445e-01\n",
      "Epoch: 1505 mean train loss:  4.25729668e-05, mean val. loss:  8.49919558e-01\n",
      "Epoch: 1506 mean train loss:  4.38262359e-05, mean val. loss:  8.50126684e-01\n",
      "Epoch: 1507 mean train loss:  4.34892718e-05, mean val. loss:  8.50327790e-01\n",
      "Epoch: 1508 mean train loss:  4.39920113e-05, mean val. loss:  8.50508392e-01\n",
      "Epoch: 1509 mean train loss:  4.24365862e-05, mean val. loss:  8.50699008e-01\n",
      "Epoch: 1510 mean train loss:  4.37418930e-05, mean val. loss:  8.50889564e-01\n",
      "Epoch: 1511 mean train loss:  4.41165175e-05, mean val. loss:  8.51062715e-01\n",
      "Epoch: 1512 mean train loss:  4.20166180e-05, mean val. loss:  8.51239681e-01\n",
      "Epoch: 1513 mean train loss:  4.38210554e-05, mean val. loss:  8.51420105e-01\n",
      "Epoch: 1514 mean train loss:  4.36062692e-05, mean val. loss:  8.51596177e-01\n",
      "Epoch: 1515 mean train loss:  4.21479926e-05, mean val. loss:  8.51792395e-01\n",
      "Epoch: 1516 mean train loss:  4.30874061e-05, mean val. loss:  8.51987898e-01\n",
      "Epoch: 1517 mean train loss:  4.30463115e-05, mean val. loss:  8.52178335e-01\n",
      "Epoch: 1518 mean train loss:  4.35818802e-05, mean val. loss:  8.52374375e-01\n",
      "Epoch: 1519 mean train loss:  4.31230292e-05, mean val. loss:  8.52579713e-01\n",
      "Epoch: 1520 mean train loss:  4.38013813e-05, mean val. loss:  8.52779329e-01\n",
      "Epoch: 1521 mean train loss:  4.34349640e-05, mean val. loss:  8.52975488e-01\n",
      "Epoch: 1522 mean train loss:  4.43471363e-05, mean val. loss:  8.53158116e-01\n",
      "Epoch: 1523 mean train loss:  4.37526032e-05, mean val. loss:  8.53328705e-01\n",
      "Epoch: 1524 mean train loss:  4.31652297e-05, mean val. loss:  8.53509247e-01\n",
      "Epoch: 1525 mean train loss:  4.29465435e-05, mean val. loss:  8.53684723e-01\n",
      "Epoch: 1526 mean train loss:  4.38150018e-05, mean val. loss:  8.53854835e-01\n",
      "Epoch: 1527 mean train loss:  4.25463659e-05, mean val. loss:  8.54018688e-01\n",
      "Epoch: 1528 mean train loss:  4.37445706e-05, mean val. loss:  8.54183853e-01\n",
      "Epoch: 1529 mean train loss:  4.27623163e-05, mean val. loss:  8.54356170e-01\n",
      "Epoch: 1530 mean train loss:  4.32261731e-05, mean val. loss:  8.54524553e-01\n",
      "Epoch: 1531 mean train loss:  4.29712236e-05, mean val. loss:  8.54690909e-01\n",
      "Epoch: 1532 mean train loss:  4.33265814e-05, mean val. loss:  8.54864895e-01\n",
      "Epoch: 1533 mean train loss:  4.22333251e-05, mean val. loss:  8.55041265e-01\n",
      "Epoch: 1534 mean train loss:  4.30253567e-05, mean val. loss:  8.55228305e-01\n",
      "Epoch: 1535 mean train loss:  4.31472436e-05, mean val. loss:  8.55421007e-01\n",
      "Epoch: 1536 mean train loss:  4.29268111e-05, mean val. loss:  8.55615795e-01\n",
      "Epoch: 1537 mean train loss:  4.30052169e-05, mean val. loss:  8.55807066e-01\n",
      "Epoch: 1538 mean train loss:  4.31452063e-05, mean val. loss:  8.56005013e-01\n",
      "Epoch: 1539 mean train loss:  4.29530046e-05, mean val. loss:  8.56203020e-01\n",
      "Epoch: 1540 mean train loss:  4.25087055e-05, mean val. loss:  8.56388211e-01\n",
      "Epoch: 1541 mean train loss:  4.33909590e-05, mean val. loss:  8.56570721e-01\n",
      "Epoch: 1542 mean train loss:  4.33677924e-05, mean val. loss:  8.56744945e-01\n",
      "Epoch: 1543 mean train loss:  4.26507322e-05, mean val. loss:  8.56923342e-01\n",
      "Epoch: 1544 mean train loss:  4.36722185e-05, mean val. loss:  8.57100070e-01\n",
      "Epoch: 1545 mean train loss:  4.30279761e-05, mean val. loss:  8.57275784e-01\n",
      "Epoch: 1546 mean train loss:  4.36705304e-05, mean val. loss:  8.57456982e-01\n",
      "Epoch: 1547 mean train loss:  4.22288431e-05, mean val. loss:  8.57632339e-01\n",
      "Epoch: 1548 mean train loss:  4.21785517e-05, mean val. loss:  8.57825935e-01\n",
      "Epoch: 1549 mean train loss:  4.38228017e-05, mean val. loss:  8.58004332e-01\n",
      "Epoch: 1550 mean train loss:  4.26527695e-05, mean val. loss:  8.58177304e-01\n",
      "Epoch: 1551 mean train loss:  4.36708215e-05, mean val. loss:  8.58345151e-01\n",
      "Epoch: 1552 mean train loss:  4.32524248e-05, mean val. loss:  8.58497679e-01\n",
      "Epoch: 1553 mean train loss:  4.37185518e-05, mean val. loss:  8.58656049e-01\n",
      "Epoch: 1554 mean train loss:  4.36753617e-05, mean val. loss:  8.58795524e-01\n",
      "Epoch: 1555 mean train loss:  4.29800129e-05, mean val. loss:  8.58944237e-01\n",
      "Epoch: 1556 mean train loss:  4.41349694e-05, mean val. loss:  8.59079421e-01\n",
      "Epoch: 1557 mean train loss:  4.29998618e-05, mean val. loss:  8.59217942e-01\n",
      "Epoch: 1558 mean train loss:  4.22525918e-05, mean val. loss:  8.59356701e-01\n",
      "Epoch: 1559 mean train loss:  4.16081166e-05, mean val. loss:  8.59514058e-01\n",
      "Epoch: 1560 mean train loss:  4.26916522e-05, mean val. loss:  8.59666765e-01\n",
      "Epoch: 1561 mean train loss:  4.09227214e-05, mean val. loss:  8.59840930e-01\n",
      "Epoch: 1562 mean train loss:  4.20196448e-05, mean val. loss:  8.60016346e-01\n",
      "Epoch: 1563 mean train loss:  4.21374571e-05, mean val. loss:  8.60199630e-01\n",
      "Epoch: 1564 mean train loss:  4.17910051e-05, mean val. loss:  8.60394001e-01\n",
      "Epoch: 1565 mean train loss:  4.36281553e-05, mean val. loss:  8.60572338e-01\n",
      "Epoch: 1566 mean train loss:  4.31948574e-05, mean val. loss:  8.60747099e-01\n",
      "Epoch: 1567 mean train loss:  4.29821666e-05, mean val. loss:  8.60938668e-01\n",
      "Epoch: 1568 mean train loss:  4.28972417e-05, mean val. loss:  8.61140192e-01\n",
      "Epoch: 1569 mean train loss:  4.32316447e-05, mean val. loss:  8.61338258e-01\n",
      "Epoch: 1570 mean train loss:  4.26659826e-05, mean val. loss:  8.61536026e-01\n",
      "Epoch: 1571 mean train loss:  4.22301237e-05, mean val. loss:  8.61753821e-01\n",
      "Epoch: 1572 mean train loss:  4.35923575e-05, mean val. loss:  8.61958325e-01\n",
      "Epoch: 1573 mean train loss:  4.25886828e-05, mean val. loss:  8.62170994e-01\n",
      "Epoch: 1574 mean train loss:  4.14362294e-05, mean val. loss:  8.62391055e-01\n",
      "Epoch: 1575 mean train loss:  4.23305319e-05, mean val. loss:  8.62609386e-01\n",
      "Epoch: 1576 mean train loss:  4.30538203e-05, mean val. loss:  8.62817407e-01\n",
      "Epoch: 1577 mean train loss:  4.23148158e-05, mean val. loss:  8.63030195e-01\n",
      "Epoch: 1578 mean train loss:  4.32417146e-05, mean val. loss:  8.63224387e-01\n",
      "Epoch: 1579 mean train loss:  4.25162725e-05, mean val. loss:  8.63426983e-01\n",
      "Epoch: 1580 mean train loss:  4.30467189e-05, mean val. loss:  8.63625109e-01\n",
      "Epoch: 1581 mean train loss:  4.24653990e-05, mean val. loss:  8.63847733e-01\n",
      "Epoch: 1582 mean train loss:  4.21479344e-05, mean val. loss:  8.64065886e-01\n",
      "Epoch: 1583 mean train loss:  4.17095143e-05, mean val. loss:  8.64306688e-01\n",
      "Epoch: 1584 mean train loss:  4.27665072e-05, mean val. loss:  8.64538491e-01\n",
      "Epoch: 1585 mean train loss:  4.30414802e-05, mean val. loss:  8.64759028e-01\n",
      "Epoch: 1586 mean train loss:  4.33755922e-05, mean val. loss:  8.64961386e-01\n",
      "Epoch: 1587 mean train loss:  4.33329842e-05, mean val. loss:  8.65149319e-01\n",
      "Epoch: 1588 mean train loss:  4.23162710e-05, mean val. loss:  8.65346789e-01\n",
      "Epoch: 1589 mean train loss:  4.28172061e-05, mean val. loss:  8.65550160e-01\n",
      "Epoch: 1590 mean train loss:  4.31890367e-05, mean val. loss:  8.65745604e-01\n",
      "Epoch: 1591 mean train loss:  4.23131278e-05, mean val. loss:  8.65953743e-01\n",
      "Epoch: 1592 mean train loss:  4.28702915e-05, mean val. loss:  8.66158605e-01\n",
      "Epoch: 1593 mean train loss:  4.33105743e-05, mean val. loss:  8.66347671e-01\n",
      "Epoch: 1594 mean train loss:  4.29881038e-05, mean val. loss:  8.66527379e-01\n",
      "Epoch: 1595 mean train loss:  4.27031191e-05, mean val. loss:  8.66714895e-01\n",
      "Epoch: 1596 mean train loss:  4.32981178e-05, mean val. loss:  8.66902411e-01\n",
      "Epoch: 1597 mean train loss:  4.28752974e-05, mean val. loss:  8.67074132e-01\n",
      "Epoch: 1598 mean train loss:  4.28567291e-05, mean val. loss:  8.67253721e-01\n",
      "Epoch: 1599 mean train loss:  4.24726750e-05, mean val. loss:  8.67442966e-01\n",
      "Epoch: 1600 mean train loss:  4.14834940e-05, mean val. loss:  8.67642224e-01\n",
      "Epoch: 1601 mean train loss:  4.27226769e-05, mean val. loss:  8.67835164e-01\n",
      "Epoch: 1602 mean train loss:  4.23081219e-05, mean val. loss:  8.68026137e-01\n",
      "Epoch: 1603 mean train loss:  4.27135383e-05, mean val. loss:  8.68209064e-01\n",
      "Epoch: 1604 mean train loss:  4.19953722e-05, mean val. loss:  8.68395925e-01\n",
      "Epoch: 1605 mean train loss:  4.28195344e-05, mean val. loss:  8.68583977e-01\n",
      "Epoch: 1606 mean train loss:  4.18284326e-05, mean val. loss:  8.68768334e-01\n",
      "Epoch: 1607 mean train loss:  4.32266388e-05, mean val. loss:  8.68951619e-01\n",
      "Epoch: 1608 mean train loss:  4.23167949e-05, mean val. loss:  8.69135678e-01\n",
      "Epoch: 1609 mean train loss:  4.24572499e-05, mean val. loss:  8.69314194e-01\n",
      "Epoch: 1610 mean train loss:  4.23224992e-05, mean val. loss:  8.69505167e-01\n",
      "Epoch: 1611 mean train loss:  4.16681287e-05, mean val. loss:  8.69704783e-01\n",
      "Epoch: 1612 mean train loss:  4.24392638e-05, mean val. loss:  8.69913399e-01\n",
      "Epoch: 1613 mean train loss:  4.30007349e-05, mean val. loss:  8.70100319e-01\n",
      "Epoch: 1614 mean train loss:  4.27002669e-05, mean val. loss:  8.70293260e-01\n",
      "Epoch: 1615 mean train loss:  4.19330900e-05, mean val. loss:  8.70475411e-01\n",
      "Epoch: 1616 mean train loss:  4.27458435e-05, mean val. loss:  8.70662630e-01\n",
      "Epoch: 1617 mean train loss:  4.17528208e-05, mean val. loss:  8.70855391e-01\n",
      "Epoch: 1618 mean train loss:  4.19581193e-05, mean val. loss:  8.71065557e-01\n",
      "Epoch: 1619 mean train loss:  4.27837367e-05, mean val. loss:  8.71276319e-01\n",
      "Epoch: 1620 mean train loss:  4.15623654e-05, mean val. loss:  8.71491730e-01\n",
      "Epoch: 1621 mean train loss:  4.25152830e-05, mean val. loss:  8.71710896e-01\n",
      "Epoch: 1622 mean train loss:  4.27950290e-05, mean val. loss:  8.71919513e-01\n",
      "Epoch: 1623 mean train loss:  4.20842553e-05, mean val. loss:  8.72122645e-01\n",
      "Epoch: 1624 mean train loss:  4.24567261e-05, mean val. loss:  8.72337580e-01\n",
      "Epoch: 1625 mean train loss:  4.30375803e-05, mean val. loss:  8.72530997e-01\n",
      "Epoch: 1626 mean train loss:  4.19190619e-05, mean val. loss:  8.72737348e-01\n",
      "Epoch: 1627 mean train loss:  4.27075429e-05, mean val. loss:  8.72938633e-01\n",
      "Epoch: 1628 mean train loss:  4.24814643e-05, mean val. loss:  8.73127699e-01\n",
      "Epoch: 1629 mean train loss:  4.22854209e-05, mean val. loss:  8.73328865e-01\n",
      "Epoch: 1630 mean train loss:  4.20628930e-05, mean val. loss:  8.73518527e-01\n",
      "Epoch: 1631 mean train loss:  4.29615611e-05, mean val. loss:  8.73692751e-01\n",
      "Epoch: 1632 mean train loss:  4.19610878e-05, mean val. loss:  8.73876035e-01\n",
      "Epoch: 1633 mean train loss:  4.20458964e-05, mean val. loss:  8.74057293e-01\n",
      "Epoch: 1634 mean train loss:  4.14190581e-05, mean val. loss:  8.74247551e-01\n",
      "Epoch: 1635 mean train loss:  4.33253590e-05, mean val. loss:  8.74439597e-01\n",
      "Epoch: 1636 mean train loss:  4.34211106e-05, mean val. loss:  8.74633491e-01\n",
      "Epoch: 1637 mean train loss:  4.23017191e-05, mean val. loss:  8.74838889e-01\n",
      "Epoch: 1638 mean train loss:  4.09408240e-05, mean val. loss:  8.75039041e-01\n",
      "Epoch: 1639 mean train loss:  4.18603304e-05, mean val. loss:  8.75241637e-01\n",
      "Epoch: 1640 mean train loss:  4.31378721e-05, mean val. loss:  8.75422060e-01\n",
      "Epoch: 1641 mean train loss:  4.26950864e-05, mean val. loss:  8.75608861e-01\n",
      "Epoch: 1642 mean train loss:  4.20935103e-05, mean val. loss:  8.75793099e-01\n",
      "Epoch: 1643 mean train loss:  4.23875754e-05, mean val. loss:  8.75957489e-01\n",
      "Epoch: 1644 mean train loss:  4.31794324e-05, mean val. loss:  8.76110256e-01\n",
      "Epoch: 1645 mean train loss:  4.19416465e-05, mean val. loss:  8.76281381e-01\n",
      "Epoch: 1646 mean train loss:  4.26521874e-05, mean val. loss:  8.76445532e-01\n",
      "Epoch: 1647 mean train loss:  4.22043959e-05, mean val. loss:  8.76619697e-01\n",
      "Epoch: 1648 mean train loss:  4.22026496e-05, mean val. loss:  8.76794875e-01\n",
      "Epoch: 1649 mean train loss:  4.13522357e-05, mean val. loss:  8.76972258e-01\n",
      "Epoch: 1650 mean train loss:  4.25883918e-05, mean val. loss:  8.77147734e-01\n",
      "Epoch: 1651 mean train loss:  4.25362960e-05, mean val. loss:  8.77316117e-01\n",
      "Epoch: 1652 mean train loss:  4.27277409e-05, mean val. loss:  8.77466798e-01\n",
      "Epoch: 1653 mean train loss:  4.23929887e-05, mean val. loss:  8.77624333e-01\n",
      "Epoch: 1654 mean train loss:  4.20587021e-05, mean val. loss:  8.77781987e-01\n",
      "Epoch: 1655 mean train loss:  4.25984617e-05, mean val. loss:  8.77945065e-01\n",
      "Epoch: 1656 mean train loss:  4.31393273e-05, mean val. loss:  8.78106058e-01\n",
      "Epoch: 1657 mean train loss:  4.11205692e-05, mean val. loss:  8.78289819e-01\n",
      "Epoch: 1658 mean train loss:  4.11737710e-05, mean val. loss:  8.78486693e-01\n",
      "Epoch: 1659 mean train loss:  4.11450746e-05, mean val. loss:  8.78689945e-01\n",
      "Epoch: 1660 mean train loss:  4.30583023e-05, mean val. loss:  8.78890634e-01\n",
      "Epoch: 1661 mean train loss:  4.28395579e-05, mean val. loss:  8.79052937e-01\n",
      "Epoch: 1662 mean train loss:  4.23883903e-05, mean val. loss:  8.79227757e-01\n",
      "Epoch: 1663 mean train loss:  4.28411877e-05, mean val. loss:  8.79393935e-01\n",
      "Epoch: 1664 mean train loss:  4.17270930e-05, mean val. loss:  8.79570007e-01\n",
      "Epoch: 1665 mean train loss:  4.23666788e-05, mean val. loss:  8.79735053e-01\n",
      "Epoch: 1666 mean train loss:  4.16683615e-05, mean val. loss:  8.79910588e-01\n",
      "Epoch: 1667 mean train loss:  4.18103882e-05, mean val. loss:  8.80089521e-01\n",
      "Epoch: 1668 mean train loss:  4.22350131e-05, mean val. loss:  8.80277216e-01\n",
      "Epoch: 1669 mean train loss:  4.24906611e-05, mean val. loss:  8.80460501e-01\n",
      "Epoch: 1670 mean train loss:  4.25176113e-05, mean val. loss:  8.80649507e-01\n",
      "Epoch: 1671 mean train loss:  4.22238954e-05, mean val. loss:  8.80853176e-01\n",
      "Epoch: 1672 mean train loss:  4.10715584e-05, mean val. loss:  8.81052852e-01\n",
      "Epoch: 1673 mean train loss:  4.25133039e-05, mean val. loss:  8.81256938e-01\n",
      "Epoch: 1674 mean train loss:  4.18180716e-05, mean val. loss:  8.81464720e-01\n",
      "Epoch: 1675 mean train loss:  4.22591111e-05, mean val. loss:  8.81670892e-01\n",
      "Epoch: 1676 mean train loss:  4.24995087e-05, mean val. loss:  8.81868005e-01\n",
      "Epoch: 1677 mean train loss:  4.18438576e-05, mean val. loss:  8.82072091e-01\n",
      "Epoch: 1678 mean train loss:  4.25133039e-05, mean val. loss:  8.82271171e-01\n",
      "Epoch: 1679 mean train loss:  4.13662638e-05, mean val. loss:  8.82471561e-01\n",
      "Epoch: 1680 mean train loss:  4.28227941e-05, mean val. loss:  8.82662535e-01\n",
      "Epoch: 1681 mean train loss:  4.16316325e-05, mean val. loss:  8.82851541e-01\n",
      "Epoch: 1682 mean train loss:  4.24557365e-05, mean val. loss:  8.83035779e-01\n",
      "Epoch: 1683 mean train loss:  4.20775614e-05, mean val. loss:  8.83218765e-01\n",
      "Epoch: 1684 mean train loss:  4.23042802e-05, mean val. loss:  8.83402288e-01\n",
      "Epoch: 1685 mean train loss:  4.24463651e-05, mean val. loss:  8.83581996e-01\n",
      "Epoch: 1686 mean train loss:  4.12440859e-05, mean val. loss:  8.83762896e-01\n",
      "Epoch: 1687 mean train loss:  4.16529365e-05, mean val. loss:  8.83948267e-01\n",
      "Epoch: 1688 mean train loss:  4.17053816e-05, mean val. loss:  8.84150863e-01\n",
      "Epoch: 1689 mean train loss:  4.20476426e-05, mean val. loss:  8.84351075e-01\n",
      "Epoch: 1690 mean train loss:  4.13798261e-05, mean val. loss:  8.84556174e-01\n",
      "Epoch: 1691 mean train loss:  4.10201028e-05, mean val. loss:  8.84761274e-01\n",
      "Epoch: 1692 mean train loss:  4.21153381e-05, mean val. loss:  8.84967685e-01\n",
      "Epoch: 1693 mean train loss:  4.16529365e-05, mean val. loss:  8.85172784e-01\n",
      "Epoch: 1694 mean train loss:  4.22652811e-05, mean val. loss:  8.85375321e-01\n",
      "Epoch: 1695 mean train loss:  4.27808263e-05, mean val. loss:  8.85559142e-01\n",
      "Epoch: 1696 mean train loss:  4.16521216e-05, mean val. loss:  8.85752380e-01\n",
      "Epoch: 1697 mean train loss:  4.20210999e-05, mean val. loss:  8.85922313e-01\n",
      "Epoch: 1698 mean train loss:  4.17859410e-05, mean val. loss:  8.86100233e-01\n",
      "Epoch: 1699 mean train loss:  4.09083441e-05, mean val. loss:  8.86294663e-01\n",
      "Epoch: 1700 mean train loss:  4.18892014e-05, mean val. loss:  8.86469364e-01\n",
      "Epoch: 1701 mean train loss:  4.25131293e-05, mean val. loss:  8.86645496e-01\n",
      "Epoch: 1702 mean train loss:  4.24885075e-05, mean val. loss:  8.86823118e-01\n",
      "Epoch: 1703 mean train loss:  4.16518305e-05, mean val. loss:  8.87002230e-01\n",
      "Epoch: 1704 mean train loss:  4.15603863e-05, mean val. loss:  8.87192190e-01\n",
      "Epoch: 1705 mean train loss:  4.23887977e-05, mean val. loss:  8.87375534e-01\n",
      "Epoch: 1706 mean train loss:  4.12057270e-05, mean val. loss:  8.87560010e-01\n",
      "Epoch: 1707 mean train loss:  4.20260476e-05, mean val. loss:  8.87754858e-01\n",
      "Epoch: 1708 mean train loss:  4.17269184e-05, mean val. loss:  8.87944281e-01\n",
      "Epoch: 1709 mean train loss:  4.29079519e-05, mean val. loss:  8.88114691e-01\n",
      "Epoch: 1710 mean train loss:  4.10092762e-05, mean val. loss:  8.88289034e-01\n",
      "Epoch: 1711 mean train loss:  4.10994398e-05, mean val. loss:  8.88480127e-01\n",
      "Epoch: 1712 mean train loss:  4.12779045e-05, mean val. loss:  8.88687193e-01\n",
      "Epoch: 1713 mean train loss:  4.22990415e-05, mean val. loss:  8.88879776e-01\n",
      "Epoch: 1714 mean train loss:  4.12298832e-05, mean val. loss:  8.89066935e-01\n",
      "Epoch: 1715 mean train loss:  4.28994535e-05, mean val. loss:  8.89225245e-01\n",
      "Epoch: 1716 mean train loss:  4.14941460e-05, mean val. loss:  8.89392257e-01\n",
      "Epoch: 1717 mean train loss:  4.16645780e-05, mean val. loss:  8.89568448e-01\n",
      "Epoch: 1718 mean train loss:  4.24454338e-05, mean val. loss:  8.89751315e-01\n",
      "Epoch: 1719 mean train loss:  4.18426935e-05, mean val. loss:  8.89940381e-01\n",
      "Epoch: 1720 mean train loss:  4.16885596e-05, mean val. loss:  8.90129328e-01\n",
      "Epoch: 1721 mean train loss:  4.17680130e-05, mean val. loss:  8.90310824e-01\n",
      "Epoch: 1722 mean train loss:  4.18936252e-05, mean val. loss:  8.90499711e-01\n",
      "Epoch: 1723 mean train loss:  4.17043921e-05, mean val. loss:  8.90680969e-01\n",
      "Epoch: 1724 mean train loss:  4.20555589e-05, mean val. loss:  8.90852809e-01\n",
      "Epoch: 1725 mean train loss:  4.19976423e-05, mean val. loss:  8.91018748e-01\n",
      "Epoch: 1726 mean train loss:  4.23307065e-05, mean val. loss:  8.91189635e-01\n",
      "Epoch: 1727 mean train loss:  4.18539857e-05, mean val. loss:  8.91360521e-01\n",
      "Epoch: 1728 mean train loss:  4.22445592e-05, mean val. loss:  8.91529918e-01\n",
      "Epoch: 1729 mean train loss:  4.22216253e-05, mean val. loss:  8.91695678e-01\n",
      "Epoch: 1730 mean train loss:  4.22079465e-05, mean val. loss:  8.91859114e-01\n",
      "Epoch: 1731 mean train loss:  4.16265102e-05, mean val. loss:  8.92039120e-01\n",
      "Epoch: 1732 mean train loss:  4.18351265e-05, mean val. loss:  8.92214000e-01\n",
      "Epoch: 1733 mean train loss:  4.13435628e-05, mean val. loss:  8.92404079e-01\n",
      "Epoch: 1734 mean train loss:  4.18082927e-05, mean val. loss:  8.92587245e-01\n",
      "Epoch: 1735 mean train loss:  4.19475837e-05, mean val. loss:  8.92775536e-01\n",
      "Epoch: 1736 mean train loss:  4.22357698e-05, mean val. loss:  8.92953753e-01\n",
      "Epoch: 1737 mean train loss:  4.08439664e-05, mean val. loss:  8.93152177e-01\n",
      "Epoch: 1738 mean train loss:  4.17446136e-05, mean val. loss:  8.93353641e-01\n",
      "Epoch: 1739 mean train loss:  4.25448525e-05, mean val. loss:  8.93532813e-01\n",
      "Epoch: 1740 mean train loss:  4.17804113e-05, mean val. loss:  8.93715262e-01\n",
      "Epoch: 1741 mean train loss:  4.05124738e-05, mean val. loss:  8.93908978e-01\n",
      "Epoch: 1742 mean train loss:  4.11315705e-05, mean val. loss:  8.94093335e-01\n",
      "Epoch: 1743 mean train loss:  4.03854065e-05, mean val. loss:  8.94290864e-01\n",
      "Epoch: 1744 mean train loss:  4.19605058e-05, mean val. loss:  8.94477487e-01\n",
      "Epoch: 1745 mean train loss:  4.14252863e-05, mean val. loss:  8.94677758e-01\n",
      "Epoch: 1746 mean train loss:  4.23854799e-05, mean val. loss:  8.94869149e-01\n",
      "Epoch: 1747 mean train loss:  4.16488620e-05, mean val. loss:  8.95058393e-01\n",
      "Epoch: 1748 mean train loss:  4.06107865e-05, mean val. loss:  8.95259023e-01\n",
      "Epoch: 1749 mean train loss:  4.18156269e-05, mean val. loss:  8.95469785e-01\n",
      "Epoch: 1750 mean train loss:  4.15710383e-05, mean val. loss:  8.95675421e-01\n",
      "Epoch: 1751 mean train loss:  4.18175478e-05, mean val. loss:  8.95868778e-01\n",
      "Epoch: 1752 mean train loss:  4.19735443e-05, mean val. loss:  8.96072090e-01\n",
      "Epoch: 1753 mean train loss:  4.19868156e-05, mean val. loss:  8.96275342e-01\n",
      "Epoch: 1754 mean train loss:  4.17182455e-05, mean val. loss:  8.96485507e-01\n",
      "Epoch: 1755 mean train loss:  4.14446113e-05, mean val. loss:  8.96689773e-01\n",
      "Epoch: 1756 mean train loss:  4.19433345e-05, mean val. loss:  8.96887422e-01\n",
      "Epoch: 1757 mean train loss:  4.00957069e-05, mean val. loss:  8.97102833e-01\n",
      "Epoch: 1758 mean train loss:  4.14403621e-05, mean val. loss:  8.97319198e-01\n",
      "Epoch: 1759 mean train loss:  4.15615505e-05, mean val. loss:  8.97528589e-01\n",
      "Epoch: 1760 mean train loss:  4.03591548e-05, mean val. loss:  8.97744715e-01\n",
      "Epoch: 1761 mean train loss:  4.13866946e-05, mean val. loss:  8.97958875e-01\n",
      "Epoch: 1762 mean train loss:  4.16760449e-05, mean val. loss:  8.98175657e-01\n",
      "Epoch: 1763 mean train loss:  4.20414726e-05, mean val. loss:  8.98375809e-01\n",
      "Epoch: 1764 mean train loss:  4.12077061e-05, mean val. loss:  8.98583353e-01\n",
      "Epoch: 1765 mean train loss:  4.21846053e-05, mean val. loss:  8.98788512e-01\n",
      "Epoch: 1766 mean train loss:  4.11396031e-05, mean val. loss:  8.98994803e-01\n",
      "Epoch: 1767 mean train loss:  4.10637585e-05, mean val. loss:  8.99203956e-01\n",
      "Epoch: 1768 mean train loss:  4.13109665e-05, mean val. loss:  8.99404645e-01\n",
      "Epoch: 1769 mean train loss:  4.14830283e-05, mean val. loss:  8.99612844e-01\n",
      "Epoch: 1770 mean train loss:  4.04802850e-05, mean val. loss:  8.99823368e-01\n",
      "Epoch: 1771 mean train loss:  4.13950766e-05, mean val. loss:  9.00039852e-01\n",
      "Epoch: 1772 mean train loss:  4.24489845e-05, mean val. loss:  9.00254190e-01\n",
      "Epoch: 1773 mean train loss:  4.15075920e-05, mean val. loss:  9.00461614e-01\n",
      "Epoch: 1774 mean train loss:  4.08670749e-05, mean val. loss:  9.00678575e-01\n",
      "Epoch: 1775 mean train loss:  4.09476925e-05, mean val. loss:  9.00894284e-01\n",
      "Epoch: 1776 mean train loss:  4.17502597e-05, mean val. loss:  9.01093543e-01\n",
      "Epoch: 1777 mean train loss:  4.20780852e-05, mean val. loss:  9.01300132e-01\n",
      "Epoch: 1778 mean train loss:  4.15979885e-05, mean val. loss:  9.01505947e-01\n",
      "Epoch: 1779 mean train loss:  4.15439135e-05, mean val. loss:  9.01701510e-01\n",
      "Epoch: 1780 mean train loss:  4.11975197e-05, mean val. loss:  9.01897430e-01\n",
      "Epoch: 1781 mean train loss:  4.09973436e-05, mean val. loss:  9.02095675e-01\n",
      "Epoch: 1782 mean train loss:  4.17894335e-05, mean val. loss:  9.02283549e-01\n",
      "Epoch: 1783 mean train loss:  4.11348301e-05, mean val. loss:  9.02485251e-01\n",
      "Epoch: 1784 mean train loss:  4.05895989e-05, mean val. loss:  9.02684391e-01\n",
      "Epoch: 1785 mean train loss:  4.11038054e-05, mean val. loss:  9.02888358e-01\n",
      "Epoch: 1786 mean train loss:  4.15467657e-05, mean val. loss:  9.03091013e-01\n",
      "Epoch: 1787 mean train loss:  4.16499097e-05, mean val. loss:  9.03298378e-01\n",
      "Epoch: 1788 mean train loss:  4.16034018e-05, mean val. loss:  9.03489411e-01\n",
      "Epoch: 1789 mean train loss:  4.24386235e-05, mean val. loss:  9.03668106e-01\n",
      "Epoch: 1790 mean train loss:  4.15269751e-05, mean val. loss:  9.03851628e-01\n",
      "Epoch: 1791 mean train loss:  4.22277371e-05, mean val. loss:  9.04017031e-01\n",
      "Epoch: 1792 mean train loss:  4.13384405e-05, mean val. loss:  9.04177666e-01\n",
      "Epoch: 1793 mean train loss:  4.14553215e-05, mean val. loss:  9.04344618e-01\n",
      "Epoch: 1794 mean train loss:  4.07393673e-05, mean val. loss:  9.04512525e-01\n",
      "Epoch: 1795 mean train loss:  4.08513006e-05, mean val. loss:  9.04683709e-01\n",
      "Epoch: 1796 mean train loss:  4.14650422e-05, mean val. loss:  9.04854834e-01\n",
      "Epoch: 1797 mean train loss:  4.19976423e-05, mean val. loss:  9.05027092e-01\n",
      "Epoch: 1798 mean train loss:  4.10929788e-05, mean val. loss:  9.05206203e-01\n",
      "Epoch: 1799 mean train loss:  4.16242983e-05, mean val. loss:  9.05366898e-01\n",
      "Epoch: 1800 mean train loss:  4.15449031e-05, mean val. loss:  9.05524731e-01\n",
      "Epoch: 1801 mean train loss:  4.19766875e-05, mean val. loss:  9.05680954e-01\n",
      "Epoch: 1802 mean train loss:  4.13195812e-05, mean val. loss:  9.05825794e-01\n",
      "Epoch: 1803 mean train loss:  4.19919379e-05, mean val. loss:  9.05970514e-01\n",
      "Epoch: 1804 mean train loss:  4.14125388e-05, mean val. loss:  9.06133115e-01\n",
      "Epoch: 1805 mean train loss:  4.14880924e-05, mean val. loss:  9.06303108e-01\n",
      "Epoch: 1806 mean train loss:  4.13076486e-05, mean val. loss:  9.06481266e-01\n",
      "Epoch: 1807 mean train loss:  4.11634683e-05, mean val. loss:  9.06663179e-01\n",
      "Epoch: 1808 mean train loss:  4.07716143e-05, mean val. loss:  9.06853318e-01\n",
      "Epoch: 1809 mean train loss:  4.11802321e-05, mean val. loss:  9.07038927e-01\n",
      "Epoch: 1810 mean train loss:  4.16632975e-05, mean val. loss:  9.07225192e-01\n",
      "Epoch: 1811 mean train loss:  4.13810485e-05, mean val. loss:  9.07410681e-01\n",
      "Epoch: 1812 mean train loss:  4.14494425e-05, mean val. loss:  9.07602370e-01\n",
      "Epoch: 1813 mean train loss:  4.10671346e-05, mean val. loss:  9.07813370e-01\n",
      "Epoch: 1814 mean train loss:  4.13885573e-05, mean val. loss:  9.08020139e-01\n",
      "Epoch: 1815 mean train loss:  4.10923385e-05, mean val. loss:  9.08233285e-01\n",
      "Epoch: 1816 mean train loss:  4.09320346e-05, mean val. loss:  9.08450425e-01\n",
      "Epoch: 1817 mean train loss:  4.05586907e-05, mean val. loss:  9.08656776e-01\n",
      "Epoch: 1818 mean train loss:  4.10508364e-05, mean val. loss:  9.08875465e-01\n",
      "Epoch: 1819 mean train loss:  4.16517141e-05, mean val. loss:  9.09078658e-01\n",
      "Epoch: 1820 mean train loss:  4.12429799e-05, mean val. loss:  9.09295559e-01\n",
      "Epoch: 1821 mean train loss:  4.10676585e-05, mean val. loss:  9.09504652e-01\n",
      "Epoch: 1822 mean train loss:  4.08423948e-05, mean val. loss:  9.09704804e-01\n",
      "Epoch: 1823 mean train loss:  4.19483404e-05, mean val. loss:  9.09902453e-01\n",
      "Epoch: 1824 mean train loss:  4.14342503e-05, mean val. loss:  9.10096705e-01\n",
      "Epoch: 1825 mean train loss:  4.07654443e-05, mean val. loss:  9.10291135e-01\n",
      "Epoch: 1826 mean train loss:  4.07359912e-05, mean val. loss:  9.10503268e-01\n",
      "Epoch: 1827 mean train loss:  4.10677167e-05, mean val. loss:  9.10704017e-01\n",
      "Epoch: 1828 mean train loss:  4.10830253e-05, mean val. loss:  9.10907090e-01\n",
      "Epoch: 1829 mean train loss:  4.13513044e-05, mean val. loss:  9.11103308e-01\n",
      "Epoch: 1830 mean train loss:  4.08611959e-05, mean val. loss:  9.11297262e-01\n",
      "Epoch: 1831 mean train loss:  4.05710889e-05, mean val. loss:  9.11514103e-01\n",
      "Epoch: 1832 mean train loss:  4.07795305e-05, mean val. loss:  9.11730647e-01\n",
      "Epoch: 1833 mean train loss:  4.08815686e-05, mean val. loss:  9.11944449e-01\n",
      "Epoch: 1834 mean train loss:  4.15751128e-05, mean val. loss:  9.12155509e-01\n",
      "Epoch: 1835 mean train loss:  4.16645198e-05, mean val. loss:  9.12354231e-01\n",
      "Epoch: 1836 mean train loss:  4.15102113e-05, mean val. loss:  9.12534118e-01\n",
      "Epoch: 1837 mean train loss:  4.13319212e-05, mean val. loss:  9.12707806e-01\n",
      "Epoch: 1838 mean train loss:  4.14967071e-05, mean val. loss:  9.12879527e-01\n",
      "Epoch: 1839 mean train loss:  4.08928026e-05, mean val. loss:  9.13042367e-01\n",
      "Epoch: 1840 mean train loss:  4.14308161e-05, mean val. loss:  9.13213074e-01\n",
      "Epoch: 1841 mean train loss:  4.14426322e-05, mean val. loss:  9.13382888e-01\n",
      "Epoch: 1842 mean train loss:  4.10380890e-05, mean val. loss:  9.13551390e-01\n",
      "Epoch: 1843 mean train loss:  4.06728941e-05, mean val. loss:  9.13743138e-01\n",
      "Epoch: 1844 mean train loss:  4.22068406e-05, mean val. loss:  9.13923085e-01\n",
      "Epoch: 1845 mean train loss:  4.09563072e-05, mean val. loss:  9.14106548e-01\n",
      "Epoch: 1846 mean train loss:  4.05260944e-05, mean val. loss:  9.14296746e-01\n",
      "Epoch: 1847 mean train loss:  4.10553766e-05, mean val. loss:  9.14492726e-01\n",
      "Epoch: 1848 mean train loss:  4.23514284e-05, mean val. loss:  9.14664268e-01\n",
      "Epoch: 1849 mean train loss:  4.12700465e-05, mean val. loss:  9.14840460e-01\n",
      "Epoch: 1850 mean train loss:  4.11082874e-05, mean val. loss:  9.15016413e-01\n",
      "Epoch: 1851 mean train loss:  4.07806947e-05, mean val. loss:  9.15192485e-01\n",
      "Epoch: 1852 mean train loss:  4.07602056e-05, mean val. loss:  9.15371299e-01\n",
      "Epoch: 1853 mean train loss:  4.05210885e-05, mean val. loss:  9.15554643e-01\n",
      "Epoch: 1854 mean train loss:  4.00420977e-05, mean val. loss:  9.15748954e-01\n",
      "Epoch: 1855 mean train loss:  4.01569996e-05, mean val. loss:  9.15947556e-01\n",
      "Epoch: 1856 mean train loss:  4.16887924e-05, mean val. loss:  9.16137695e-01\n",
      "Epoch: 1857 mean train loss:  4.08125925e-05, mean val. loss:  9.16354775e-01\n",
      "Epoch: 1858 mean train loss:  4.06801701e-05, mean val. loss:  9.16579425e-01\n",
      "Epoch: 1859 mean train loss:  4.16486291e-05, mean val. loss:  9.16801333e-01\n",
      "Epoch: 1860 mean train loss:  4.12918744e-05, mean val. loss:  9.17016685e-01\n",
      "Epoch: 1861 mean train loss:  4.16747062e-05, mean val. loss:  9.17228460e-01\n",
      "Epoch: 1862 mean train loss:  4.15326795e-05, mean val. loss:  9.17438865e-01\n",
      "Epoch: 1863 mean train loss:  4.12999652e-05, mean val. loss:  9.17620242e-01\n",
      "Epoch: 1864 mean train loss:  4.13830276e-05, mean val. loss:  9.17818069e-01\n",
      "Epoch: 1865 mean train loss:  4.14111419e-05, mean val. loss:  9.18008626e-01\n",
      "Epoch: 1866 mean train loss:  4.10511857e-05, mean val. loss:  9.18193996e-01\n",
      "Epoch: 1867 mean train loss:  4.05749306e-05, mean val. loss:  9.18390989e-01\n",
      "Epoch: 1868 mean train loss:  4.03867452e-05, mean val. loss:  9.18609142e-01\n",
      "Epoch: 1869 mean train loss:  4.14113747e-05, mean val. loss:  9.18822765e-01\n",
      "Epoch: 1870 mean train loss:  4.09960048e-05, mean val. loss:  9.19040799e-01\n",
      "Epoch: 1871 mean train loss:  4.08754568e-05, mean val. loss:  9.19260859e-01\n",
      "Epoch: 1872 mean train loss:  4.09523491e-05, mean val. loss:  9.19488549e-01\n",
      "Epoch: 1873 mean train loss:  4.04217280e-05, mean val. loss:  9.19701219e-01\n",
      "Epoch: 1874 mean train loss:  4.12926893e-05, mean val. loss:  9.19910729e-01\n",
      "Epoch: 1875 mean train loss:  4.11747606e-05, mean val. loss:  9.20107007e-01\n",
      "Epoch: 1876 mean train loss:  4.09418717e-05, mean val. loss:  9.20297086e-01\n",
      "Epoch: 1877 mean train loss:  4.14213864e-05, mean val. loss:  9.20484722e-01\n",
      "Epoch: 1878 mean train loss:  4.08375636e-05, mean val. loss:  9.20677960e-01\n",
      "Epoch: 1879 mean train loss:  4.03411686e-05, mean val. loss:  9.20884192e-01\n",
      "Epoch: 1880 mean train loss:  4.11462970e-05, mean val. loss:  9.21074510e-01\n",
      "Epoch: 1881 mean train loss:  4.03618906e-05, mean val. loss:  9.21264410e-01\n",
      "Epoch: 1882 mean train loss:  4.08323249e-05, mean val. loss:  9.21440065e-01\n",
      "Epoch: 1883 mean train loss:  4.05626488e-05, mean val. loss:  9.21621025e-01\n",
      "Epoch: 1884 mean train loss:  4.06086911e-05, mean val. loss:  9.21791792e-01\n",
      "Epoch: 1885 mean train loss:  4.03325539e-05, mean val. loss:  9.21964049e-01\n",
      "Epoch: 1886 mean train loss:  4.16450202e-05, mean val. loss:  9.22115624e-01\n",
      "Epoch: 1887 mean train loss:  4.05802857e-05, mean val. loss:  9.22288597e-01\n",
      "Epoch: 1888 mean train loss:  4.07104963e-05, mean val. loss:  9.22473073e-01\n",
      "Epoch: 1889 mean train loss:  4.10345965e-05, mean val. loss:  9.22671139e-01\n",
      "Epoch: 1890 mean train loss:  4.13222006e-05, mean val. loss:  9.22846735e-01\n",
      "Epoch: 1891 mean train loss:  4.13855887e-05, mean val. loss:  9.23026323e-01\n",
      "Epoch: 1892 mean train loss:  4.06898907e-05, mean val. loss:  9.23203230e-01\n",
      "Epoch: 1893 mean train loss:  4.09866334e-05, mean val. loss:  9.23370302e-01\n",
      "Epoch: 1894 mean train loss:  4.08982742e-05, mean val. loss:  9.23545063e-01\n",
      "Epoch: 1895 mean train loss:  4.04778984e-05, mean val. loss:  9.23722863e-01\n",
      "Epoch: 1896 mean train loss:  4.08513588e-05, mean val. loss:  9.23898518e-01\n",
      "Epoch: 1897 mean train loss:  4.18803538e-05, mean val. loss:  9.24069047e-01\n",
      "Epoch: 1898 mean train loss:  4.06158506e-05, mean val. loss:  9.24249649e-01\n",
      "Epoch: 1899 mean train loss:  4.07697516e-05, mean val. loss:  9.24409509e-01\n",
      "Epoch: 1900 mean train loss:  4.09830245e-05, mean val. loss:  9.24562633e-01\n",
      "Epoch: 1901 mean train loss:  3.98990232e-05, mean val. loss:  9.24719453e-01\n",
      "Epoch: 1902 mean train loss:  4.13255766e-05, mean val. loss:  9.24878895e-01\n",
      "Epoch: 1903 mean train loss:  4.11098590e-05, mean val. loss:  9.25033152e-01\n",
      "Epoch: 1904 mean train loss:  4.12182417e-05, mean val. loss:  9.25184190e-01\n",
      "Epoch: 1905 mean train loss:  4.13981616e-05, mean val. loss:  9.25334632e-01\n",
      "Epoch: 1906 mean train loss:  4.10316279e-05, mean val. loss:  9.25482512e-01\n",
      "Epoch: 1907 mean train loss:  4.06977488e-05, mean val. loss:  9.25648689e-01\n",
      "Epoch: 1908 mean train loss:  4.03505401e-05, mean val. loss:  9.25814629e-01\n",
      "Epoch: 1909 mean train loss:  4.09196364e-05, mean val. loss:  9.25966680e-01\n",
      "Epoch: 1910 mean train loss:  4.04265593e-05, mean val. loss:  9.26124513e-01\n",
      "Epoch: 1911 mean train loss:  3.97951808e-05, mean val. loss:  9.26304936e-01\n",
      "Epoch: 1912 mean train loss:  4.03582817e-05, mean val. loss:  9.26483274e-01\n",
      "Epoch: 1913 mean train loss:  4.11026413e-05, mean val. loss:  9.26648140e-01\n",
      "Epoch: 1914 mean train loss:  4.04159073e-05, mean val. loss:  9.26819324e-01\n",
      "Epoch: 1915 mean train loss:  4.17135889e-05, mean val. loss:  9.26982999e-01\n",
      "Epoch: 1916 mean train loss:  4.06131730e-05, mean val. loss:  9.27151740e-01\n",
      "Epoch: 1917 mean train loss:  4.00902354e-05, mean val. loss:  9.27337527e-01\n",
      "Epoch: 1918 mean train loss:  4.10755165e-05, mean val. loss:  9.27511096e-01\n",
      "Epoch: 1919 mean train loss:  4.06570616e-05, mean val. loss:  9.27695990e-01\n",
      "Epoch: 1920 mean train loss:  4.02011210e-05, mean val. loss:  9.27887738e-01\n",
      "Epoch: 1921 mean train loss:  4.11137007e-05, mean val. loss:  9.28066552e-01\n",
      "Epoch: 1922 mean train loss:  4.08033957e-05, mean val. loss:  9.28236783e-01\n",
      "Epoch: 1923 mean train loss:  4.11806977e-05, mean val. loss:  9.28406894e-01\n",
      "Epoch: 1924 mean train loss:  4.10642242e-05, mean val. loss:  9.28576648e-01\n",
      "Epoch: 1925 mean train loss:  4.04833117e-05, mean val. loss:  9.28743958e-01\n",
      "Epoch: 1926 mean train loss:  4.07804619e-05, mean val. loss:  9.28909183e-01\n",
      "Epoch: 1927 mean train loss:  4.05897736e-05, mean val. loss:  9.29076552e-01\n",
      "Epoch: 1928 mean train loss:  4.09293571e-05, mean val. loss:  9.29235339e-01\n",
      "Epoch: 1929 mean train loss:  4.11446090e-05, mean val. loss:  9.29390669e-01\n",
      "Epoch: 1930 mean train loss:  4.03642189e-05, mean val. loss:  9.29560483e-01\n",
      "Epoch: 1931 mean train loss:  4.05253377e-05, mean val. loss:  9.29726779e-01\n",
      "Epoch: 1932 mean train loss:  4.09584027e-05, mean val. loss:  9.29878950e-01\n",
      "Epoch: 1933 mean train loss:  4.01221914e-05, mean val. loss:  9.30049062e-01\n",
      "Epoch: 1934 mean train loss:  4.11515357e-05, mean val. loss:  9.30210948e-01\n",
      "Epoch: 1935 mean train loss:  4.08655615e-05, mean val. loss:  9.30360079e-01\n",
      "Epoch: 1936 mean train loss:  3.96614196e-05, mean val. loss:  9.30528343e-01\n",
      "Epoch: 1937 mean train loss:  4.09662607e-05, mean val. loss:  9.30696785e-01\n",
      "Epoch: 1938 mean train loss:  4.00026329e-05, mean val. loss:  9.30871487e-01\n",
      "Epoch: 1939 mean train loss:  4.01210273e-05, mean val. loss:  9.31061804e-01\n",
      "Epoch: 1940 mean train loss:  4.00415156e-05, mean val. loss:  9.31262910e-01\n",
      "Epoch: 1941 mean train loss:  4.11589281e-05, mean val. loss:  9.31455255e-01\n",
      "Epoch: 1942 mean train loss:  4.04028106e-05, mean val. loss:  9.31654930e-01\n",
      "Epoch: 1943 mean train loss:  3.99936689e-05, mean val. loss:  9.31841135e-01\n",
      "Epoch: 1944 mean train loss:  4.01759171e-05, mean val. loss:  9.32020307e-01\n",
      "Epoch: 1945 mean train loss:  4.07040934e-05, mean val. loss:  9.32200730e-01\n",
      "Epoch: 1946 mean train loss:  4.09877393e-05, mean val. loss:  9.32377994e-01\n",
      "Epoch: 1947 mean train loss:  4.20957804e-05, mean val. loss:  9.32538569e-01\n",
      "Epoch: 1948 mean train loss:  4.05377941e-05, mean val. loss:  9.32720006e-01\n",
      "Epoch: 1949 mean train loss:  4.15149843e-05, mean val. loss:  9.32883680e-01\n",
      "Epoch: 1950 mean train loss:  4.14143433e-05, mean val. loss:  9.33028042e-01\n",
      "Epoch: 1951 mean train loss:  4.09017084e-05, mean val. loss:  9.33169186e-01\n",
      "Epoch: 1952 mean train loss:  3.95859824e-05, mean val. loss:  9.33326244e-01\n",
      "Epoch: 1953 mean train loss:  4.04248131e-05, mean val. loss:  9.33479011e-01\n",
      "Epoch: 1954 mean train loss:  3.97142721e-05, mean val. loss:  9.33644772e-01\n",
      "Epoch: 1955 mean train loss:  3.94735252e-05, mean val. loss:  9.33821499e-01\n",
      "Epoch: 1956 mean train loss:  4.02861624e-05, mean val. loss:  9.33999717e-01\n",
      "Epoch: 1957 mean train loss:  4.03113663e-05, mean val. loss:  9.34181273e-01\n",
      "Epoch: 1958 mean train loss:  3.94654926e-05, mean val. loss:  9.34372365e-01\n",
      "Epoch: 1959 mean train loss:  3.98144475e-05, mean val. loss:  9.34564769e-01\n",
      "Epoch: 1960 mean train loss:  4.01025172e-05, mean val. loss:  9.34749126e-01\n",
      "Epoch: 1961 mean train loss:  4.14975220e-05, mean val. loss:  9.34920847e-01\n",
      "Epoch: 1962 mean train loss:  3.97071126e-05, mean val. loss:  9.35100019e-01\n",
      "Epoch: 1963 mean train loss:  4.04340681e-05, mean val. loss:  9.35278356e-01\n",
      "Epoch: 1964 mean train loss:  4.05589235e-05, mean val. loss:  9.35467303e-01\n",
      "Epoch: 1965 mean train loss:  4.10616631e-05, mean val. loss:  9.35663283e-01\n",
      "Epoch: 1966 mean train loss:  3.99501296e-05, mean val. loss:  9.35866892e-01\n",
      "Epoch: 1967 mean train loss:  4.05234750e-05, mean val. loss:  9.36077356e-01\n",
      "Epoch: 1968 mean train loss:  4.00620629e-05, mean val. loss:  9.36289430e-01\n",
      "Epoch: 1969 mean train loss:  3.98184056e-05, mean val. loss:  9.36517477e-01\n",
      "Epoch: 1970 mean train loss:  4.07056068e-05, mean val. loss:  9.36743498e-01\n",
      "Epoch: 1971 mean train loss:  4.00333083e-05, mean val. loss:  9.36982214e-01\n",
      "Epoch: 1972 mean train loss:  4.11649817e-05, mean val. loss:  9.37197328e-01\n",
      "Epoch: 1973 mean train loss:  3.93394730e-05, mean val. loss:  9.37424958e-01\n",
      "Epoch: 1974 mean train loss:  4.06695181e-05, mean val. loss:  9.37653720e-01\n",
      "Epoch: 1975 mean train loss:  4.06312756e-05, mean val. loss:  9.37891901e-01\n",
      "Epoch: 1976 mean train loss:  4.05735336e-05, mean val. loss:  9.38138306e-01\n",
      "Epoch: 1977 mean train loss:  4.04657912e-05, mean val. loss:  9.38371658e-01\n",
      "Epoch: 1978 mean train loss:  4.05993778e-05, mean val. loss:  9.38609302e-01\n",
      "Epoch: 1979 mean train loss:  3.94677045e-05, mean val. loss:  9.38837826e-01\n",
      "Epoch: 1980 mean train loss:  3.98315024e-05, mean val. loss:  9.39089239e-01\n",
      "Epoch: 1981 mean train loss:  3.98307457e-05, mean val. loss:  9.39314783e-01\n",
      "Epoch: 1982 mean train loss:  4.03305166e-05, mean val. loss:  9.39563692e-01\n",
      "Epoch: 1983 mean train loss:  4.10113716e-05, mean val. loss:  9.39813137e-01\n",
      "Epoch: 1984 mean train loss:  3.88657209e-05, mean val. loss:  9.40077662e-01\n",
      "Epoch: 1985 mean train loss:  3.99863347e-05, mean val. loss:  9.40331101e-01\n",
      "Epoch: 1986 mean train loss:  4.02196310e-05, mean val. loss:  9.40572202e-01\n",
      "Epoch: 1987 mean train loss:  4.06636391e-05, mean val. loss:  9.40819025e-01\n",
      "Epoch: 1988 mean train loss:  4.03866870e-05, mean val. loss:  9.41060841e-01\n",
      "Epoch: 1989 mean train loss:  3.98501288e-05, mean val. loss:  9.41291153e-01\n",
      "Epoch: 1990 mean train loss:  4.01663128e-05, mean val. loss:  9.41519737e-01\n",
      "Epoch: 1991 mean train loss:  4.00362187e-05, mean val. loss:  9.41749215e-01\n",
      "Epoch: 1992 mean train loss:  4.08939086e-05, mean val. loss:  9.41953659e-01\n",
      "Epoch: 1993 mean train loss:  4.07765619e-05, mean val. loss:  9.42176402e-01\n",
      "Epoch: 1994 mean train loss:  4.04442544e-05, mean val. loss:  9.42373216e-01\n",
      "Epoch: 1995 mean train loss:  4.12145164e-05, mean val. loss:  9.42566752e-01\n",
      "Epoch: 1996 mean train loss:  4.04409948e-05, mean val. loss:  9.42750156e-01\n",
      "Epoch: 1997 mean train loss:  4.02727746e-05, mean val. loss:  9.42945123e-01\n",
      "Epoch: 1998 mean train loss:  4.07954212e-05, mean val. loss:  9.43128407e-01\n",
      "Epoch: 1999 mean train loss:  4.02537989e-05, mean val. loss:  9.43303704e-01\n",
      "Epoch: 2000 mean train loss:  4.02960577e-05, mean val. loss:  9.43469942e-01\n",
      "Epoch: 2001 mean train loss:  3.95868556e-05, mean val. loss:  9.43647563e-01\n",
      "Epoch: 2002 mean train loss:  4.08538617e-05, mean val. loss:  9.43833292e-01\n",
      "Epoch: 2003 mean train loss:  4.03001322e-05, mean val. loss:  9.44017887e-01\n",
      "Epoch: 2004 mean train loss:  4.03018785e-05, mean val. loss:  9.44201231e-01\n",
      "Epoch: 2005 mean train loss:  4.06546169e-05, mean val. loss:  9.44383681e-01\n",
      "Epoch: 2006 mean train loss:  3.98323173e-05, mean val. loss:  9.44554448e-01\n",
      "Epoch: 2007 mean train loss:  4.00491990e-05, mean val. loss:  9.44728553e-01\n",
      "Epoch: 2008 mean train loss:  4.00304561e-05, mean val. loss:  9.44895446e-01\n",
      "Epoch: 2009 mean train loss:  4.01363941e-05, mean val. loss:  9.45074201e-01\n",
      "Epoch: 2010 mean train loss:  4.01908183e-05, mean val. loss:  9.45266187e-01\n",
      "Epoch: 2011 mean train loss:  3.98709672e-05, mean val. loss:  9.45444405e-01\n",
      "Epoch: 2012 mean train loss:  4.00405843e-05, mean val. loss:  9.45625424e-01\n",
      "Epoch: 2013 mean train loss:  3.95132229e-05, mean val. loss:  9.45808589e-01\n",
      "Epoch: 2014 mean train loss:  4.01773723e-05, mean val. loss:  9.45986271e-01\n",
      "Epoch: 2015 mean train loss:  4.03384911e-05, mean val. loss:  9.46175992e-01\n",
      "Epoch: 2016 mean train loss:  4.06461186e-05, mean val. loss:  9.46356595e-01\n",
      "Epoch: 2017 mean train loss:  4.01731231e-05, mean val. loss:  9.46542621e-01\n",
      "Epoch: 2018 mean train loss:  4.07527550e-05, mean val. loss:  9.46725667e-01\n",
      "Epoch: 2019 mean train loss:  3.97518743e-05, mean val. loss:  9.46920872e-01\n",
      "Epoch: 2020 mean train loss:  3.94476228e-05, mean val. loss:  9.47112143e-01\n",
      "Epoch: 2021 mean train loss:  4.03898302e-05, mean val. loss:  9.47304368e-01\n",
      "Epoch: 2022 mean train loss:  3.99095588e-05, mean val. loss:  9.47507739e-01\n",
      "Epoch: 2023 mean train loss:  4.08694032e-05, mean val. loss:  9.47690845e-01\n",
      "Epoch: 2024 mean train loss:  4.05284809e-05, mean val. loss:  9.47871804e-01\n",
      "Epoch: 2025 mean train loss:  4.04288876e-05, mean val. loss:  9.48047519e-01\n",
      "Epoch: 2026 mean train loss:  4.08595079e-05, mean val. loss:  9.48219359e-01\n",
      "Epoch: 2027 mean train loss:  3.94732924e-05, mean val. loss:  9.48404610e-01\n",
      "Epoch: 2028 mean train loss:  3.92532093e-05, mean val. loss:  9.48606074e-01\n",
      "Epoch: 2029 mean train loss:  3.98529810e-05, mean val. loss:  9.48789656e-01\n",
      "Epoch: 2030 mean train loss:  4.02225996e-05, mean val. loss:  9.48982894e-01\n",
      "Epoch: 2031 mean train loss:  3.93061782e-05, mean val. loss:  9.49185848e-01\n",
      "Epoch: 2032 mean train loss:  3.97961121e-05, mean val. loss:  9.49401617e-01\n",
      "Epoch: 2033 mean train loss:  3.97679978e-05, mean val. loss:  9.49619651e-01\n",
      "Epoch: 2034 mean train loss:  3.99191049e-05, mean val. loss:  9.49827075e-01\n",
      "Epoch: 2035 mean train loss:  4.06283652e-05, mean val. loss:  9.50024486e-01\n",
      "Epoch: 2036 mean train loss:  4.02225996e-05, mean val. loss:  9.50217843e-01\n",
      "Epoch: 2037 mean train loss:  4.07841289e-05, mean val. loss:  9.50401485e-01\n",
      "Epoch: 2038 mean train loss:  3.98101402e-05, mean val. loss:  9.50583518e-01\n",
      "Epoch: 2039 mean train loss:  4.00167773e-05, mean val. loss:  9.50773716e-01\n",
      "Epoch: 2040 mean train loss:  4.15076502e-05, mean val. loss:  9.50947165e-01\n",
      "Epoch: 2041 mean train loss:  4.00093850e-05, mean val. loss:  9.51137304e-01\n",
      "Epoch: 2042 mean train loss:  4.02096775e-05, mean val. loss:  9.51321959e-01\n",
      "Epoch: 2043 mean train loss:  3.99455312e-05, mean val. loss:  9.51504827e-01\n",
      "Epoch: 2044 mean train loss:  3.98313277e-05, mean val. loss:  9.51700330e-01\n",
      "Epoch: 2045 mean train loss:  4.04119492e-05, mean val. loss:  9.51871216e-01\n",
      "Epoch: 2046 mean train loss:  4.01909347e-05, mean val. loss:  9.52040851e-01\n",
      "Epoch: 2047 mean train loss:  3.97709664e-05, mean val. loss:  9.52210188e-01\n",
      "Epoch: 2048 mean train loss:  3.97200347e-05, mean val. loss:  9.52365279e-01\n",
      "Epoch: 2049 mean train loss:  4.01675934e-05, mean val. loss:  9.52516973e-01\n",
      "Epoch: 2050 mean train loss:  3.95918614e-05, mean val. loss:  9.52681243e-01\n",
      "Epoch: 2051 mean train loss:  3.97460535e-05, mean val. loss:  9.52844143e-01\n",
      "Epoch: 2052 mean train loss:  3.98407574e-05, mean val. loss:  9.53005075e-01\n",
      "Epoch: 2053 mean train loss:  4.06008912e-05, mean val. loss:  9.53160882e-01\n",
      "Epoch: 2054 mean train loss:  4.02719597e-05, mean val. loss:  9.53307509e-01\n",
      "Epoch: 2055 mean train loss:  3.94226518e-05, mean val. loss:  9.53477859e-01\n",
      "Epoch: 2056 mean train loss:  4.04539751e-05, mean val. loss:  9.53635752e-01\n",
      "Epoch: 2057 mean train loss:  3.93557712e-05, mean val. loss:  9.53798056e-01\n",
      "Epoch: 2058 mean train loss:  4.02167207e-05, mean val. loss:  9.53965545e-01\n",
      "Epoch: 2059 mean train loss:  3.99137498e-05, mean val. loss:  9.54124272e-01\n",
      "Epoch: 2060 mean train loss:  4.01145662e-05, mean val. loss:  9.54302132e-01\n",
      "Epoch: 2061 mean train loss:  4.03628801e-05, mean val. loss:  9.54471111e-01\n",
      "Epoch: 2062 mean train loss:  4.01320285e-05, mean val. loss:  9.54635561e-01\n",
      "Epoch: 2063 mean train loss:  4.03659651e-05, mean val. loss:  9.54797924e-01\n",
      "Epoch: 2064 mean train loss:  4.01491998e-05, mean val. loss:  9.54967797e-01\n",
      "Epoch: 2065 mean train loss:  3.99443088e-05, mean val. loss:  9.55154896e-01\n",
      "Epoch: 2066 mean train loss:  4.02658479e-05, mean val. loss:  9.55340385e-01\n",
      "Epoch: 2067 mean train loss:  4.06261533e-05, mean val. loss:  9.55529213e-01\n",
      "Epoch: 2068 mean train loss:  3.96806281e-05, mean val. loss:  9.55717087e-01\n",
      "Epoch: 2069 mean train loss:  3.99737619e-05, mean val. loss:  9.55918074e-01\n",
      "Epoch: 2070 mean train loss:  4.05553728e-05, mean val. loss:  9.56098855e-01\n",
      "Epoch: 2071 mean train loss:  4.03547310e-05, mean val. loss:  9.56291258e-01\n",
      "Epoch: 2072 mean train loss:  4.01616562e-05, mean val. loss:  9.56471920e-01\n",
      "Epoch: 2073 mean train loss:  4.02984442e-05, mean val. loss:  9.56660688e-01\n",
      "Epoch: 2074 mean train loss:  3.94487288e-05, mean val. loss:  9.56841230e-01\n",
      "Epoch: 2075 mean train loss:  3.92828952e-05, mean val. loss:  9.57030237e-01\n",
      "Epoch: 2076 mean train loss:  3.94317321e-05, mean val. loss:  9.57227647e-01\n",
      "Epoch: 2077 mean train loss:  3.89088527e-05, mean val. loss:  9.57449853e-01\n",
      "Epoch: 2078 mean train loss:  3.95283569e-05, mean val. loss:  9.57670748e-01\n",
      "Epoch: 2079 mean train loss:  3.97576950e-05, mean val. loss:  9.57884848e-01\n",
      "Epoch: 2080 mean train loss:  4.04791790e-05, mean val. loss:  9.58091378e-01\n",
      "Epoch: 2081 mean train loss:  3.96468677e-05, mean val. loss:  9.58301365e-01\n",
      "Epoch: 2082 mean train loss:  4.07670159e-05, mean val. loss:  9.58497941e-01\n",
      "Epoch: 2083 mean train loss:  4.02261503e-05, mean val. loss:  9.58695829e-01\n",
      "Epoch: 2084 mean train loss:  3.95421521e-05, mean val. loss:  9.58888412e-01\n",
      "Epoch: 2085 mean train loss:  3.95242823e-05, mean val. loss:  9.59100783e-01\n",
      "Epoch: 2086 mean train loss:  4.05495521e-05, mean val. loss:  9.59307730e-01\n",
      "Epoch: 2087 mean train loss:  4.02859878e-05, mean val. loss:  9.59514141e-01\n",
      "Epoch: 2088 mean train loss:  3.98622942e-05, mean val. loss:  9.59718943e-01\n",
      "Epoch: 2089 mean train loss:  3.93617665e-05, mean val. loss:  9.59928870e-01\n",
      "Epoch: 2090 mean train loss:  3.89062334e-05, mean val. loss:  9.60149467e-01\n",
      "Epoch: 2091 mean train loss:  3.99226556e-05, mean val. loss:  9.60367203e-01\n",
      "Epoch: 2092 mean train loss:  3.90381319e-05, mean val. loss:  9.60601151e-01\n",
      "Epoch: 2093 mean train loss:  3.94681119e-05, mean val. loss:  9.60819423e-01\n",
      "Epoch: 2094 mean train loss:  3.96993128e-05, mean val. loss:  9.61032391e-01\n",
      "Epoch: 2095 mean train loss:  3.87622276e-05, mean val. loss:  9.61261094e-01\n",
      "Epoch: 2096 mean train loss:  3.87352193e-05, mean val. loss:  9.61500347e-01\n",
      "Epoch: 2097 mean train loss:  3.84603045e-05, mean val. loss:  9.61755991e-01\n",
      "Epoch: 2098 mean train loss:  4.02456499e-05, mean val. loss:  9.62015748e-01\n",
      "Epoch: 2099 mean train loss:  3.96323740e-05, mean val. loss:  9.62252855e-01\n",
      "Epoch: 2100 mean train loss:  3.95980314e-05, mean val. loss:  9.62493360e-01\n",
      "Epoch: 2101 mean train loss:  4.05518804e-05, mean val. loss:  9.62719619e-01\n",
      "Epoch: 2102 mean train loss:  3.95727693e-05, mean val. loss:  9.62951362e-01\n",
      "Epoch: 2103 mean train loss:  4.01699799e-05, mean val. loss:  9.63166833e-01\n",
      "Epoch: 2104 mean train loss:  4.01269644e-05, mean val. loss:  9.63386595e-01\n",
      "Epoch: 2105 mean train loss:  3.94326053e-05, mean val. loss:  9.63602483e-01\n",
      "Epoch: 2106 mean train loss:  3.96111282e-05, mean val. loss:  9.63817120e-01\n",
      "Epoch: 2107 mean train loss:  3.98596167e-05, mean val. loss:  9.64032054e-01\n",
      "Epoch: 2108 mean train loss:  3.97553667e-05, mean val. loss:  9.64255273e-01\n",
      "Epoch: 2109 mean train loss:  4.01391299e-05, mean val. loss:  9.64468718e-01\n",
      "Epoch: 2110 mean train loss:  4.05100291e-05, mean val. loss:  9.64675248e-01\n",
      "Epoch: 2111 mean train loss:  3.99353448e-05, mean val. loss:  9.64869738e-01\n",
      "Epoch: 2112 mean train loss:  3.98032716e-05, mean val. loss:  9.65063214e-01\n",
      "Epoch: 2113 mean train loss:  3.97809199e-05, mean val. loss:  9.65264261e-01\n",
      "Epoch: 2114 mean train loss:  4.03216109e-05, mean val. loss:  9.65439141e-01\n",
      "Epoch: 2115 mean train loss:  3.98493139e-05, mean val. loss:  9.65629399e-01\n",
      "Epoch: 2116 mean train loss:  3.96737014e-05, mean val. loss:  9.65808451e-01\n",
      "Epoch: 2117 mean train loss:  3.95033276e-05, mean val. loss:  9.65989053e-01\n",
      "Epoch: 2118 mean train loss:  3.98090342e-05, mean val. loss:  9.66173172e-01\n",
      "Epoch: 2119 mean train loss:  4.00240533e-05, mean val. loss:  9.66340959e-01\n",
      "Epoch: 2120 mean train loss:  4.00600838e-05, mean val. loss:  9.66497004e-01\n",
      "Epoch: 2121 mean train loss:  3.91466892e-05, mean val. loss:  9.66664195e-01\n",
      "Epoch: 2122 mean train loss:  4.05721366e-05, mean val. loss:  9.66818035e-01\n",
      "Epoch: 2123 mean train loss:  3.93120572e-05, mean val. loss:  9.66974676e-01\n",
      "Epoch: 2124 mean train loss:  3.99844721e-05, mean val. loss:  9.67129707e-01\n",
      "Epoch: 2125 mean train loss:  3.94918025e-05, mean val. loss:  9.67296660e-01\n",
      "Epoch: 2126 mean train loss:  3.93482624e-05, mean val. loss:  9.67474282e-01\n",
      "Epoch: 2127 mean train loss:  3.96743417e-05, mean val. loss:  9.67635691e-01\n",
      "Epoch: 2128 mean train loss:  3.94065282e-05, mean val. loss:  9.67800140e-01\n",
      "Epoch: 2129 mean train loss:  3.97331896e-05, mean val. loss:  9.67957020e-01\n",
      "Epoch: 2130 mean train loss:  4.03625891e-05, mean val. loss:  9.68119919e-01\n",
      "Epoch: 2131 mean train loss:  4.01525758e-05, mean val. loss:  9.68284011e-01\n",
      "Epoch: 2132 mean train loss:  3.98263219e-05, mean val. loss:  9.68453526e-01\n",
      "Epoch: 2133 mean train loss:  3.92309157e-05, mean val. loss:  9.68634903e-01\n",
      "Epoch: 2134 mean train loss:  4.00413992e-05, mean val. loss:  9.68808472e-01\n",
      "Epoch: 2135 mean train loss:  3.91812064e-05, mean val. loss:  9.68977749e-01\n",
      "Epoch: 2136 mean train loss:  3.98876728e-05, mean val. loss:  9.69158292e-01\n",
      "Epoch: 2137 mean train loss:  3.97329568e-05, mean val. loss:  9.69342768e-01\n",
      "Epoch: 2138 mean train loss:  3.92207876e-05, mean val. loss:  9.69518840e-01\n",
      "Epoch: 2139 mean train loss:  3.91090289e-05, mean val. loss:  9.69700277e-01\n",
      "Epoch: 2140 mean train loss:  3.92936636e-05, mean val. loss:  9.69894171e-01\n",
      "Epoch: 2141 mean train loss:  3.98713164e-05, mean val. loss:  9.70086396e-01\n",
      "Epoch: 2142 mean train loss:  3.91406938e-05, mean val. loss:  9.70277309e-01\n",
      "Epoch: 2143 mean train loss:  3.91176436e-05, mean val. loss:  9.70479608e-01\n",
      "Epoch: 2144 mean train loss:  3.93721275e-05, mean val. loss:  9.70687926e-01\n",
      "Epoch: 2145 mean train loss:  3.91125795e-05, mean val. loss:  9.70892727e-01\n",
      "Epoch: 2146 mean train loss:  3.99356359e-05, mean val. loss:  9.71096277e-01\n",
      "Epoch: 2147 mean train loss:  4.00638673e-05, mean val. loss:  9.71286178e-01\n",
      "Epoch: 2148 mean train loss:  3.95423849e-05, mean val. loss:  9.71472561e-01\n",
      "Epoch: 2149 mean train loss:  3.88819608e-05, mean val. loss:  9.71657634e-01\n",
      "Epoch: 2150 mean train loss:  3.89073975e-05, mean val. loss:  9.71847892e-01\n",
      "Epoch: 2151 mean train loss:  3.91698559e-05, mean val. loss:  9.72040653e-01\n",
      "Epoch: 2152 mean train loss:  3.95368552e-05, mean val. loss:  9.72236991e-01\n",
      "Epoch: 2153 mean train loss:  3.99986748e-05, mean val. loss:  9.72421169e-01\n",
      "Epoch: 2154 mean train loss:  4.04255698e-05, mean val. loss:  9.72598612e-01\n",
      "Epoch: 2155 mean train loss:  3.98431439e-05, mean val. loss:  9.72768962e-01\n",
      "Epoch: 2156 mean train loss:  3.84112354e-05, mean val. loss:  9.72960472e-01\n",
      "Epoch: 2157 mean train loss:  3.99428536e-05, mean val. loss:  9.73141372e-01\n",
      "Epoch: 2158 mean train loss:  3.91904614e-05, mean val. loss:  9.73331332e-01\n",
      "Epoch: 2159 mean train loss:  3.95609532e-05, mean val. loss:  9.73527431e-01\n",
      "Epoch: 2160 mean train loss:  3.95067036e-05, mean val. loss:  9.73731577e-01\n",
      "Epoch: 2161 mean train loss:  3.85527965e-05, mean val. loss:  9.73944664e-01\n",
      "Epoch: 2162 mean train loss:  4.00304561e-05, mean val. loss:  9.74145293e-01\n",
      "Epoch: 2163 mean train loss:  3.98065313e-05, mean val. loss:  9.74333227e-01\n",
      "Epoch: 2164 mean train loss:  3.99119454e-05, mean val. loss:  9.74505603e-01\n",
      "Epoch: 2165 mean train loss:  3.98904085e-05, mean val. loss:  9.74678457e-01\n",
      "Epoch: 2166 mean train loss:  3.92602524e-05, mean val. loss:  9.74844038e-01\n",
      "Epoch: 2167 mean train loss:  3.84484883e-05, mean val. loss:  9.75025892e-01\n",
      "Epoch: 2168 mean train loss:  3.90018686e-05, mean val. loss:  9.75215793e-01\n",
      "Epoch: 2169 mean train loss:  3.85352760e-05, mean val. loss:  9.75421190e-01\n",
      "Epoch: 2170 mean train loss:  3.97302210e-05, mean val. loss:  9.75636303e-01\n",
      "Epoch: 2171 mean train loss:  3.97464028e-05, mean val. loss:  9.75847185e-01\n",
      "Epoch: 2172 mean train loss:  3.85821331e-05, mean val. loss:  9.76065338e-01\n",
      "Epoch: 2173 mean train loss:  3.92385991e-05, mean val. loss:  9.76270378e-01\n",
      "Epoch: 2174 mean train loss:  3.89051856e-05, mean val. loss:  9.76484179e-01\n",
      "Epoch: 2175 mean train loss:  3.95383686e-05, mean val. loss:  9.76676524e-01\n",
      "Epoch: 2176 mean train loss:  3.91759095e-05, mean val. loss:  9.76874113e-01\n",
      "Epoch: 2177 mean train loss:  3.89895285e-05, mean val. loss:  9.77080286e-01\n",
      "Epoch: 2178 mean train loss:  3.80195561e-05, mean val. loss:  9.77309704e-01\n",
      "Epoch: 2179 mean train loss:  3.94359231e-05, mean val. loss:  9.77518022e-01\n",
      "Epoch: 2180 mean train loss:  3.85800959e-05, mean val. loss:  9.77746487e-01\n",
      "Epoch: 2181 mean train loss:  3.89665947e-05, mean val. loss:  9.77978587e-01\n",
      "Epoch: 2182 mean train loss:  3.90350469e-05, mean val. loss:  9.78217244e-01\n",
      "Epoch: 2183 mean train loss:  3.84917948e-05, mean val. loss:  9.78455603e-01\n",
      "Epoch: 2184 mean train loss:  3.98410484e-05, mean val. loss:  9.78690803e-01\n",
      "Epoch: 2185 mean train loss:  4.00925055e-05, mean val. loss:  9.78923500e-01\n",
      "Epoch: 2186 mean train loss:  4.01422731e-05, mean val. loss:  9.79131937e-01\n",
      "Epoch: 2187 mean train loss:  3.87275359e-05, mean val. loss:  9.79352295e-01\n",
      "Epoch: 2188 mean train loss:  3.93372611e-05, mean val. loss:  9.79557216e-01\n",
      "Epoch: 2189 mean train loss:  3.97426775e-05, mean val. loss:  9.79748666e-01\n",
      "Epoch: 2190 mean train loss:  3.84455780e-05, mean val. loss:  9.79946077e-01\n",
      "Epoch: 2191 mean train loss:  3.94047820e-05, mean val. loss:  9.80153739e-01\n",
      "Epoch: 2192 mean train loss:  3.92117072e-05, mean val. loss:  9.80358720e-01\n",
      "Epoch: 2193 mean train loss:  3.90880741e-05, mean val. loss:  9.80549872e-01\n",
      "Epoch: 2194 mean train loss:  3.91629292e-05, mean val. loss:  9.80743587e-01\n",
      "Epoch: 2195 mean train loss:  3.91697395e-05, mean val. loss:  9.80947018e-01\n",
      "Epoch: 2196 mean train loss:  3.91551293e-05, mean val. loss:  9.81141329e-01\n",
      "Epoch: 2197 mean train loss:  3.94892995e-05, mean val. loss:  9.81334388e-01\n",
      "Epoch: 2198 mean train loss:  3.89134511e-05, mean val. loss:  9.81531322e-01\n",
      "Epoch: 2199 mean train loss:  3.94360977e-05, mean val. loss:  9.81715858e-01\n",
      "Epoch: 2200 mean train loss:  3.93454102e-05, mean val. loss:  9.81900930e-01\n",
      "Epoch: 2201 mean train loss:  4.01268480e-05, mean val. loss:  9.82078731e-01\n",
      "Epoch: 2202 mean train loss:  3.96106625e-05, mean val. loss:  9.82259750e-01\n",
      "Epoch: 2203 mean train loss:  3.90698551e-05, mean val. loss:  9.82441843e-01\n",
      "Epoch: 2204 mean train loss:  3.88927874e-05, mean val. loss:  9.82623756e-01\n",
      "Epoch: 2205 mean train loss:  3.88101907e-05, mean val. loss:  9.82797801e-01\n",
      "Epoch: 2206 mean train loss:  3.87596665e-05, mean val. loss:  9.82978940e-01\n",
      "Epoch: 2207 mean train loss:  3.90324276e-05, mean val. loss:  9.83147681e-01\n",
      "Epoch: 2208 mean train loss:  3.92169459e-05, mean val. loss:  9.83319759e-01\n",
      "Epoch: 2209 mean train loss:  3.93872615e-05, mean val. loss:  9.83495831e-01\n",
      "Epoch: 2210 mean train loss:  3.98041448e-05, mean val. loss:  9.83651400e-01\n",
      "Epoch: 2211 mean train loss:  3.85043095e-05, mean val. loss:  9.83825862e-01\n",
      "Epoch: 2212 mean train loss:  3.89916822e-05, mean val. loss:  9.84003305e-01\n",
      "Epoch: 2213 mean train loss:  3.91819049e-05, mean val. loss:  9.84188914e-01\n",
      "Epoch: 2214 mean train loss:  3.94525705e-05, mean val. loss:  9.84373152e-01\n",
      "Epoch: 2215 mean train loss:  4.03429149e-05, mean val. loss:  9.84534025e-01\n",
      "Epoch: 2216 mean train loss:  3.93326627e-05, mean val. loss:  9.84691322e-01\n",
      "Epoch: 2217 mean train loss:  3.88809713e-05, mean val. loss:  9.84861672e-01\n",
      "Epoch: 2218 mean train loss:  3.79984849e-05, mean val. loss:  9.85041022e-01\n",
      "Epoch: 2219 mean train loss:  3.94357485e-05, mean val. loss:  9.85207438e-01\n",
      "Epoch: 2220 mean train loss:  3.90891219e-05, mean val. loss:  9.85370278e-01\n",
      "Epoch: 2221 mean train loss:  3.92223592e-05, mean val. loss:  9.85536039e-01\n",
      "Epoch: 2222 mean train loss:  3.85806197e-05, mean val. loss:  9.85707760e-01\n",
      "Epoch: 2223 mean train loss:  3.96348187e-05, mean val. loss:  9.85868692e-01\n",
      "Epoch: 2224 mean train loss:  3.83813749e-05, mean val. loss:  9.86045301e-01\n",
      "Epoch: 2225 mean train loss:  3.97115364e-05, mean val. loss:  9.86212850e-01\n",
      "Epoch: 2226 mean train loss:  3.97788826e-05, mean val. loss:  9.86388266e-01\n",
      "Epoch: 2227 mean train loss:  3.85216554e-05, mean val. loss:  9.86568570e-01\n",
      "Epoch: 2228 mean train loss:  3.97771364e-05, mean val. loss:  9.86725688e-01\n",
      "Epoch: 2229 mean train loss:  3.88473854e-05, mean val. loss:  9.86883819e-01\n",
      "Epoch: 2230 mean train loss:  3.98986158e-05, mean val. loss:  9.87043142e-01\n",
      "Epoch: 2231 mean train loss:  3.92162474e-05, mean val. loss:  9.87194061e-01\n",
      "Epoch: 2232 mean train loss:  3.89742781e-05, mean val. loss:  9.87339437e-01\n",
      "Epoch: 2233 mean train loss:  3.89261986e-05, mean val. loss:  9.87514853e-01\n",
      "Epoch: 2234 mean train loss:  3.85814346e-05, mean val. loss:  9.87710059e-01\n",
      "Epoch: 2235 mean train loss:  3.95897077e-05, mean val. loss:  9.87894416e-01\n",
      "Epoch: 2236 mean train loss:  3.85106541e-05, mean val. loss:  9.88073111e-01\n",
      "Epoch: 2237 mean train loss:  3.89484921e-05, mean val. loss:  9.88248587e-01\n",
      "Epoch: 2238 mean train loss:  3.87381879e-05, mean val. loss:  9.88442540e-01\n",
      "Epoch: 2239 mean train loss:  3.83658335e-05, mean val. loss:  9.88647640e-01\n",
      "Epoch: 2240 mean train loss:  3.77338729e-05, mean val. loss:  9.88871098e-01\n",
      "Epoch: 2241 mean train loss:  3.92589718e-05, mean val. loss:  9.89086449e-01\n",
      "Epoch: 2242 mean train loss:  3.90248024e-05, mean val. loss:  9.89300966e-01\n",
      "Epoch: 2243 mean train loss:  3.96256801e-05, mean val. loss:  9.89508748e-01\n",
      "Epoch: 2244 mean train loss:  3.78368422e-05, mean val. loss:  9.89746690e-01\n",
      "Epoch: 2245 mean train loss:  3.83334700e-05, mean val. loss:  9.89995539e-01\n",
      "Epoch: 2246 mean train loss:  3.94224189e-05, mean val. loss:  9.90231752e-01\n",
      "Epoch: 2247 mean train loss:  3.90852802e-05, mean val. loss:  9.90449786e-01\n",
      "Epoch: 2248 mean train loss:  3.93189257e-05, mean val. loss:  9.90662992e-01\n",
      "Epoch: 2249 mean train loss:  3.83754959e-05, mean val. loss:  9.90900576e-01\n",
      "Epoch: 2250 mean train loss:  3.89993074e-05, mean val. loss:  9.91134167e-01\n",
      "Epoch: 2251 mean train loss:  3.93652008e-05, mean val. loss:  9.91363466e-01\n",
      "Epoch: 2252 mean train loss:  3.93954688e-05, mean val. loss:  9.91574824e-01\n",
      "Epoch: 2253 mean train loss:  3.91563517e-05, mean val. loss:  9.91797626e-01\n",
      "Epoch: 2254 mean train loss:  3.81818973e-05, mean val. loss:  9.92020011e-01\n",
      "Epoch: 2255 mean train loss:  3.96336545e-05, mean val. loss:  9.92247581e-01\n",
      "Epoch: 2256 mean train loss:  3.97177064e-05, mean val. loss:  9.92461681e-01\n",
      "Epoch: 2257 mean train loss:  3.87940090e-05, mean val. loss:  9.92668629e-01\n",
      "Epoch: 2258 mean train loss:  3.91150243e-05, mean val. loss:  9.92874503e-01\n",
      "Epoch: 2259 mean train loss:  3.87344626e-05, mean val. loss:  9.93094981e-01\n",
      "Epoch: 2260 mean train loss:  3.84035520e-05, mean val. loss:  9.93322134e-01\n",
      "Epoch: 2261 mean train loss:  3.85366729e-05, mean val. loss:  9.93538260e-01\n",
      "Epoch: 2262 mean train loss:  3.89268389e-05, mean val. loss:  9.93759811e-01\n",
      "Epoch: 2263 mean train loss:  3.90259665e-05, mean val. loss:  9.93965864e-01\n",
      "Epoch: 2264 mean train loss:  3.81594291e-05, mean val. loss:  9.94185269e-01\n",
      "Epoch: 2265 mean train loss:  3.80264246e-05, mean val. loss:  9.94410872e-01\n",
      "Epoch: 2266 mean train loss:  3.93073424e-05, mean val. loss:  9.94649351e-01\n",
      "Epoch: 2267 mean train loss:  3.95188108e-05, mean val. loss:  9.94867802e-01\n",
      "Epoch: 2268 mean train loss:  3.88536137e-05, mean val. loss:  9.95089293e-01\n",
      "Epoch: 2269 mean train loss:  3.83264851e-05, mean val. loss:  9.95315373e-01\n",
      "Epoch: 2270 mean train loss:  3.77506949e-05, mean val. loss:  9.95543182e-01\n",
      "Epoch: 2271 mean train loss:  3.86350439e-05, mean val. loss:  9.95774031e-01\n",
      "Epoch: 2272 mean train loss:  3.90987261e-05, mean val. loss:  9.96028244e-01\n",
      "Epoch: 2273 mean train loss:  3.90592031e-05, mean val. loss:  9.96274948e-01\n",
      "Epoch: 2274 mean train loss:  3.82288708e-05, mean val. loss:  9.96510804e-01\n",
      "Epoch: 2275 mean train loss:  3.88209592e-05, mean val. loss:  9.96733427e-01\n",
      "Epoch: 2276 mean train loss:  3.93691007e-05, mean val. loss:  9.96938884e-01\n",
      "Epoch: 2277 mean train loss:  3.90608329e-05, mean val. loss:  9.97128963e-01\n",
      "Epoch: 2278 mean train loss:  3.94227100e-05, mean val. loss:  9.97297227e-01\n",
      "Epoch: 2279 mean train loss:  3.96695104e-05, mean val. loss:  9.97451305e-01\n",
      "Epoch: 2280 mean train loss:  3.88047774e-05, mean val. loss:  9.97599006e-01\n",
      "Epoch: 2281 mean train loss:  3.86019237e-05, mean val. loss:  9.97740984e-01\n",
      "Epoch: 2282 mean train loss:  3.84937157e-05, mean val. loss:  9.97880638e-01\n",
      "Epoch: 2283 mean train loss:  3.90234054e-05, mean val. loss:  9.98016894e-01\n",
      "Epoch: 2284 mean train loss:  3.90821369e-05, mean val. loss:  9.98143196e-01\n",
      "Epoch: 2285 mean train loss:  3.87683394e-05, mean val. loss:  9.98273611e-01\n",
      "Epoch: 2286 mean train loss:  3.89637426e-05, mean val. loss:  9.98391390e-01\n",
      "Epoch: 2287 mean train loss:  3.88861517e-05, mean val. loss:  9.98507559e-01\n",
      "Epoch: 2288 mean train loss:  3.89835914e-05, mean val. loss:  9.98633146e-01\n",
      "Epoch: 2289 mean train loss:  3.89510533e-05, mean val. loss:  9.98748243e-01\n",
      "Epoch: 2290 mean train loss:  3.88769549e-05, mean val. loss:  9.98852611e-01\n",
      "Epoch: 2291 mean train loss:  3.86832980e-05, mean val. loss:  9.98965085e-01\n",
      "Epoch: 2292 mean train loss:  3.89117049e-05, mean val. loss:  9.99085605e-01\n",
      "Epoch: 2293 mean train loss:  3.90050700e-05, mean val. loss:  9.99203265e-01\n",
      "Epoch: 2294 mean train loss:  3.74949886e-05, mean val. loss:  9.99337554e-01\n",
      "Epoch: 2295 mean train loss:  3.81827704e-05, mean val. loss:  9.99493420e-01\n",
      "Epoch: 2296 mean train loss:  3.77342803e-05, mean val. loss:  9.99667883e-01\n",
      "Epoch: 2297 mean train loss:  3.84892337e-05, mean val. loss:  9.99832213e-01\n",
      "Epoch: 2298 mean train loss:  3.92416841e-05, mean val. loss:  9.99995768e-01\n",
      "Epoch: 2299 mean train loss:  3.85965104e-05, mean val. loss:  1.00016224e+00\n",
      "Epoch: 2300 mean train loss:  3.90349305e-05, mean val. loss:  1.00033438e+00\n",
      "Epoch: 2301 mean train loss:  3.97951808e-05, mean val. loss:  1.00050104e+00\n",
      "Epoch: 2302 mean train loss:  3.99228302e-05, mean val. loss:  1.00065279e+00\n",
      "Epoch: 2303 mean train loss:  3.79740959e-05, mean val. loss:  1.00081837e+00\n",
      "Epoch: 2304 mean train loss:  3.84707237e-05, mean val. loss:  1.00099516e+00\n",
      "Epoch: 2305 mean train loss:  3.84076266e-05, mean val. loss:  1.00117588e+00\n",
      "Epoch: 2306 mean train loss:  3.88874905e-05, mean val. loss:  1.00135636e+00\n",
      "Epoch: 2307 mean train loss:  3.86954634e-05, mean val. loss:  1.00154853e+00\n",
      "Epoch: 2308 mean train loss:  3.88093758e-05, mean val. loss:  1.00172448e+00\n",
      "Epoch: 2309 mean train loss:  3.83720035e-05, mean val. loss:  1.00190902e+00\n",
      "Epoch: 2310 mean train loss:  3.90080386e-05, mean val. loss:  1.00210071e+00\n",
      "Epoch: 2311 mean train loss:  3.95288807e-05, mean val. loss:  1.00228822e+00\n",
      "Epoch: 2312 mean train loss:  3.84427258e-05, mean val. loss:  1.00247765e+00\n",
      "Epoch: 2313 mean train loss:  3.80189158e-05, mean val. loss:  1.00266743e+00\n",
      "Epoch: 2314 mean train loss:  3.89270717e-05, mean val. loss:  1.00286627e+00\n",
      "Epoch: 2315 mean train loss:  3.82228754e-05, mean val. loss:  1.00306690e+00\n",
      "Epoch: 2316 mean train loss:  3.87752661e-05, mean val. loss:  1.00324714e+00\n",
      "Epoch: 2317 mean train loss:  3.82367289e-05, mean val. loss:  1.00342429e+00\n",
      "Epoch: 2318 mean train loss:  3.85464518e-05, mean val. loss:  1.00361133e+00\n",
      "Epoch: 2319 mean train loss:  3.86638567e-05, mean val. loss:  1.00379503e+00\n",
      "Epoch: 2320 mean train loss:  3.90135683e-05, mean val. loss:  1.00398743e+00\n",
      "Epoch: 2321 mean train loss:  3.84428422e-05, mean val. loss:  1.00417340e+00\n",
      "Epoch: 2322 mean train loss:  3.87189793e-05, mean val. loss:  1.00436008e+00\n",
      "Epoch: 2323 mean train loss:  3.95640382e-05, mean val. loss:  1.00453722e+00\n",
      "Epoch: 2324 mean train loss:  3.78174591e-05, mean val. loss:  1.00473368e+00\n",
      "Epoch: 2325 mean train loss:  3.80373676e-05, mean val. loss:  1.00492477e+00\n",
      "Epoch: 2326 mean train loss:  3.88249755e-05, mean val. loss:  1.00512111e+00\n",
      "Epoch: 2327 mean train loss:  3.89163615e-05, mean val. loss:  1.00529635e+00\n",
      "Epoch: 2328 mean train loss:  3.85065796e-05, mean val. loss:  1.00547636e+00\n",
      "Epoch: 2329 mean train loss:  3.89721827e-05, mean val. loss:  1.00566125e+00\n",
      "Epoch: 2330 mean train loss:  4.00368008e-05, mean val. loss:  1.00584364e+00\n",
      "Epoch: 2331 mean train loss:  3.87204927e-05, mean val. loss:  1.00600588e+00\n",
      "Epoch: 2332 mean train loss:  3.75112286e-05, mean val. loss:  1.00619411e+00\n",
      "Epoch: 2333 mean train loss:  3.85656022e-05, mean val. loss:  1.00636661e+00\n",
      "Epoch: 2334 mean train loss:  3.84633895e-05, mean val. loss:  1.00655389e+00\n",
      "Epoch: 2335 mean train loss:  3.84786399e-05, mean val. loss:  1.00673640e+00\n",
      "Epoch: 2336 mean train loss:  3.86060565e-05, mean val. loss:  1.00691926e+00\n",
      "Epoch: 2337 mean train loss:  3.76131502e-05, mean val. loss:  1.00711477e+00\n",
      "Epoch: 2338 mean train loss:  3.81195568e-05, mean val. loss:  1.00731230e+00\n",
      "Epoch: 2339 mean train loss:  3.87810287e-05, mean val. loss:  1.00750959e+00\n",
      "Epoch: 2340 mean train loss:  3.95068782e-05, mean val. loss:  1.00769162e+00\n",
      "Epoch: 2341 mean train loss:  3.77567485e-05, mean val. loss:  1.00788200e+00\n",
      "Epoch: 2342 mean train loss:  3.87409818e-05, mean val. loss:  1.00806046e+00\n",
      "Epoch: 2343 mean train loss:  3.77279939e-05, mean val. loss:  1.00826859e+00\n",
      "Epoch: 2344 mean train loss:  3.80635029e-05, mean val. loss:  1.00847304e+00\n",
      "Epoch: 2345 mean train loss:  3.88069311e-05, mean val. loss:  1.00867462e+00\n",
      "Epoch: 2346 mean train loss:  3.82294529e-05, mean val. loss:  1.00886571e+00\n",
      "Epoch: 2347 mean train loss:  3.82745638e-05, mean val. loss:  1.00905073e+00\n",
      "Epoch: 2348 mean train loss:  3.92974471e-05, mean val. loss:  1.00922763e+00\n",
      "Epoch: 2349 mean train loss:  3.86572210e-05, mean val. loss:  1.00940359e+00\n",
      "Epoch: 2350 mean train loss:  3.87947075e-05, mean val. loss:  1.00957668e+00\n",
      "Epoch: 2351 mean train loss:  3.76668759e-05, mean val. loss:  1.00975287e+00\n",
      "Epoch: 2352 mean train loss:  3.89257912e-05, mean val. loss:  1.00993669e+00\n",
      "Epoch: 2353 mean train loss:  3.83874867e-05, mean val. loss:  1.01013279e+00\n",
      "Epoch: 2354 mean train loss:  3.83856241e-05, mean val. loss:  1.01032317e+00\n",
      "Epoch: 2355 mean train loss:  3.94678791e-05, mean val. loss:  1.01049519e+00\n",
      "Epoch: 2356 mean train loss:  3.75917298e-05, mean val. loss:  1.01067746e+00\n",
      "Epoch: 2357 mean train loss:  3.84707237e-05, mean val. loss:  1.01085746e+00\n",
      "Epoch: 2358 mean train loss:  3.85050662e-05, mean val. loss:  1.01105058e+00\n",
      "Epoch: 2359 mean train loss:  3.78559926e-05, mean val. loss:  1.01124299e+00\n",
      "Epoch: 2360 mean train loss:  3.94236995e-05, mean val. loss:  1.01143336e+00\n",
      "Epoch: 2361 mean train loss:  3.79438861e-05, mean val. loss:  1.01164484e+00\n",
      "Epoch: 2362 mean train loss:  3.82883009e-05, mean val. loss:  1.01185715e+00\n",
      "Epoch: 2363 mean train loss:  3.93561204e-05, mean val. loss:  1.01206720e+00\n",
      "Epoch: 2364 mean train loss:  3.85314925e-05, mean val. loss:  1.01225793e+00\n",
      "Epoch: 2365 mean train loss:  3.81022110e-05, mean val. loss:  1.01244497e+00\n",
      "Epoch: 2366 mean train loss:  3.81615246e-05, mean val. loss:  1.01264453e+00\n",
      "Epoch: 2367 mean train loss:  3.85055318e-05, mean val. loss:  1.01285183e+00\n",
      "Epoch: 2368 mean train loss:  3.94262606e-05, mean val. loss:  1.01304615e+00\n",
      "Epoch: 2369 mean train loss:  3.88023327e-05, mean val. loss:  1.01323116e+00\n",
      "Epoch: 2370 mean train loss:  3.83183360e-05, mean val. loss:  1.01342010e+00\n",
      "Epoch: 2371 mean train loss:  3.83257866e-05, mean val. loss:  1.01362228e+00\n",
      "Epoch: 2372 mean train loss:  3.88496555e-05, mean val. loss:  1.01382613e+00\n",
      "Epoch: 2373 mean train loss:  3.80612910e-05, mean val. loss:  1.01402807e+00\n",
      "Epoch: 2374 mean train loss:  3.83135630e-05, mean val. loss:  1.01423919e+00\n",
      "Epoch: 2375 mean train loss:  3.88585031e-05, mean val. loss:  1.01444113e+00\n",
      "Epoch: 2376 mean train loss:  3.89148481e-05, mean val. loss:  1.01462209e+00\n",
      "Epoch: 2377 mean train loss:  3.82171129e-05, mean val. loss:  1.01480412e+00\n",
      "Epoch: 2378 mean train loss:  3.74772353e-05, mean val. loss:  1.01500154e+00\n",
      "Epoch: 2379 mean train loss:  3.90505302e-05, mean val. loss:  1.01518834e+00\n",
      "Epoch: 2380 mean train loss:  3.83297447e-05, mean val. loss:  1.01537848e+00\n",
      "Epoch: 2381 mean train loss:  3.82395810e-05, mean val. loss:  1.01557386e+00\n",
      "Epoch: 2382 mean train loss:  3.87092005e-05, mean val. loss:  1.01575530e+00\n",
      "Epoch: 2383 mean train loss:  3.81063437e-05, mean val. loss:  1.01594412e+00\n",
      "Epoch: 2384 mean train loss:  3.72086070e-05, mean val. loss:  1.01615381e+00\n",
      "Epoch: 2385 mean train loss:  3.79258418e-05, mean val. loss:  1.01636255e+00\n",
      "Epoch: 2386 mean train loss:  3.84068117e-05, mean val. loss:  1.01657045e+00\n",
      "Epoch: 2387 mean train loss:  3.88614135e-05, mean val. loss:  1.01677179e+00\n",
      "Epoch: 2388 mean train loss:  3.89258494e-05, mean val. loss:  1.01695538e+00\n",
      "Epoch: 2389 mean train loss:  3.88478511e-05, mean val. loss:  1.01712382e+00\n",
      "Epoch: 2390 mean train loss:  3.90937785e-05, mean val. loss:  1.01727271e+00\n",
      "Epoch: 2391 mean train loss:  3.80892307e-05, mean val. loss:  1.01744151e+00\n",
      "Epoch: 2392 mean train loss:  3.85303865e-05, mean val. loss:  1.01760066e+00\n",
      "Epoch: 2393 mean train loss:  3.84447048e-05, mean val. loss:  1.01776445e+00\n",
      "Epoch: 2394 mean train loss:  3.96687537e-05, mean val. loss:  1.01790631e+00\n",
      "Epoch: 2395 mean train loss:  3.82480212e-05, mean val. loss:  1.01805472e+00\n",
      "Epoch: 2396 mean train loss:  3.85626918e-05, mean val. loss:  1.01819634e+00\n",
      "Epoch: 2397 mean train loss:  3.87580367e-05, mean val. loss:  1.01833415e+00\n",
      "Epoch: 2398 mean train loss:  3.79923731e-05, mean val. loss:  1.01848352e+00\n",
      "Epoch: 2399 mean train loss:  3.88767803e-05, mean val. loss:  1.01862299e+00\n",
      "Epoch: 2400 mean train loss:  3.80821293e-05, mean val. loss:  1.01876318e+00\n",
      "Epoch: 2401 mean train loss:  3.90489586e-05, mean val. loss:  1.01890302e+00\n",
      "Epoch: 2402 mean train loss:  3.79665289e-05, mean val. loss:  1.01905560e+00\n",
      "Epoch: 2403 mean train loss:  3.84149607e-05, mean val. loss:  1.01918995e+00\n",
      "Epoch: 2404 mean train loss:  3.78040131e-05, mean val. loss:  1.01933980e+00\n",
      "Epoch: 2405 mean train loss:  3.81042482e-05, mean val. loss:  1.01949000e+00\n",
      "Epoch: 2406 mean train loss:  3.87136242e-05, mean val. loss:  1.01964068e+00\n",
      "Epoch: 2407 mean train loss:  3.79507546e-05, mean val. loss:  1.01979661e+00\n",
      "Epoch: 2408 mean train loss:  3.76219978e-05, mean val. loss:  1.01996005e+00\n",
      "Epoch: 2409 mean train loss:  3.78201366e-05, mean val. loss:  1.02014458e+00\n",
      "Epoch: 2410 mean train loss:  3.88830667e-05, mean val. loss:  1.02032137e+00\n",
      "Epoch: 2411 mean train loss:  3.84827727e-05, mean val. loss:  1.02049565e+00\n",
      "Epoch: 2412 mean train loss:  3.77540127e-05, mean val. loss:  1.02066946e+00\n",
      "Epoch: 2413 mean train loss:  3.73020303e-05, mean val. loss:  1.02085960e+00\n",
      "Epoch: 2414 mean train loss:  3.78779951e-05, mean val. loss:  1.02104199e+00\n",
      "Epoch: 2415 mean train loss:  3.77548859e-05, mean val. loss:  1.02122843e+00\n",
      "Epoch: 2416 mean train loss:  3.87341715e-05, mean val. loss:  1.02139497e+00\n",
      "Epoch: 2417 mean train loss:  3.77132674e-05, mean val. loss:  1.02158117e+00\n",
      "Epoch: 2418 mean train loss:  3.86740430e-05, mean val. loss:  1.02175665e+00\n",
      "Epoch: 2419 mean train loss:  3.83834122e-05, mean val. loss:  1.02193427e+00\n",
      "Epoch: 2420 mean train loss:  3.79387056e-05, mean val. loss:  1.02212369e+00\n",
      "Epoch: 2421 mean train loss:  3.81718273e-05, mean val. loss:  1.02231681e+00\n",
      "Epoch: 2422 mean train loss:  3.80868441e-05, mean val. loss:  1.02250195e+00\n",
      "Epoch: 2423 mean train loss:  3.76607641e-05, mean val. loss:  1.02269721e+00\n",
      "Epoch: 2424 mean train loss:  3.73522053e-05, mean val. loss:  1.02292573e+00\n",
      "Epoch: 2425 mean train loss:  3.87011096e-05, mean val. loss:  1.02314329e+00\n",
      "Epoch: 2426 mean train loss:  3.77090764e-05, mean val. loss:  1.02338552e+00\n",
      "Epoch: 2427 mean train loss:  3.78299737e-05, mean val. loss:  1.02364123e+00\n",
      "Epoch: 2428 mean train loss:  3.78695549e-05, mean val. loss:  1.02391303e+00\n",
      "Epoch: 2429 mean train loss:  3.74134979e-05, mean val. loss:  1.02419114e+00\n",
      "Epoch: 2430 mean train loss:  3.81977297e-05, mean val. loss:  1.02446473e+00\n",
      "Epoch: 2431 mean train loss:  3.77842225e-05, mean val. loss:  1.02473557e+00\n",
      "Epoch: 2432 mean train loss:  3.86999454e-05, mean val. loss:  1.02500534e+00\n",
      "Epoch: 2433 mean train loss:  3.81484861e-05, mean val. loss:  1.02527177e+00\n",
      "Epoch: 2434 mean train loss:  3.78758996e-05, mean val. loss:  1.02554762e+00\n",
      "Epoch: 2435 mean train loss:  3.93652590e-05, mean val. loss:  1.02579725e+00\n",
      "Epoch: 2436 mean train loss:  3.75183881e-05, mean val. loss:  1.02605236e+00\n",
      "Epoch: 2437 mean train loss:  3.75712407e-05, mean val. loss:  1.02631247e+00\n",
      "Epoch: 2438 mean train loss:  3.74590163e-05, mean val. loss:  1.02658021e+00\n",
      "Epoch: 2439 mean train loss:  3.80626298e-05, mean val. loss:  1.02682173e+00\n",
      "Epoch: 2440 mean train loss:  3.81372520e-05, mean val. loss:  1.02706611e+00\n",
      "Epoch: 2441 mean train loss:  3.83142615e-05, mean val. loss:  1.02732563e+00\n",
      "Epoch: 2442 mean train loss:  3.86351603e-05, mean val. loss:  1.02757275e+00\n",
      "Epoch: 2443 mean train loss:  3.81761929e-05, mean val. loss:  1.02784169e+00\n",
      "Epoch: 2444 mean train loss:  3.86006432e-05, mean val. loss:  1.02810824e+00\n",
      "Epoch: 2445 mean train loss:  3.77091346e-05, mean val. loss:  1.02837431e+00\n",
      "Epoch: 2446 mean train loss:  3.80009878e-05, mean val. loss:  1.02864468e+00\n",
      "Epoch: 2447 mean train loss:  3.83256120e-05, mean val. loss:  1.02889407e+00\n",
      "Epoch: 2448 mean train loss:  3.85018648e-05, mean val. loss:  1.02913737e+00\n",
      "Epoch: 2449 mean train loss:  3.76349781e-05, mean val. loss:  1.02936518e+00\n",
      "Epoch: 2450 mean train loss:  3.77279357e-05, mean val. loss:  1.02959061e+00\n",
      "Epoch: 2451 mean train loss:  3.85557069e-05, mean val. loss:  1.02981198e+00\n",
      "Epoch: 2452 mean train loss:  3.75746749e-05, mean val. loss:  1.03002810e+00\n",
      "Epoch: 2453 mean train loss:  3.85407475e-05, mean val. loss:  1.03025079e+00\n",
      "Epoch: 2454 mean train loss:  3.77400429e-05, mean val. loss:  1.03047109e+00\n",
      "Epoch: 2455 mean train loss:  3.81370191e-05, mean val. loss:  1.03068531e+00\n",
      "Epoch: 2456 mean train loss:  3.81985446e-05, mean val. loss:  1.03088009e+00\n",
      "Epoch: 2457 mean train loss:  3.76874814e-05, mean val. loss:  1.03106654e+00\n",
      "Epoch: 2458 mean train loss:  3.74094816e-05, mean val. loss:  1.03126121e+00\n",
      "Epoch: 2459 mean train loss:  3.84680461e-05, mean val. loss:  1.03143597e+00\n",
      "Epoch: 2460 mean train loss:  3.80018610e-05, mean val. loss:  1.03159654e+00\n",
      "Epoch: 2461 mean train loss:  3.70543858e-05, mean val. loss:  1.03177631e+00\n",
      "Epoch: 2462 mean train loss:  3.82741855e-05, mean val. loss:  1.03194857e+00\n",
      "Epoch: 2463 mean train loss:  3.90899950e-05, mean val. loss:  1.03211677e+00\n",
      "Epoch: 2464 mean train loss:  3.79722624e-05, mean val. loss:  1.03227413e+00\n",
      "Epoch: 2465 mean train loss:  3.81803256e-05, mean val. loss:  1.03240776e+00\n",
      "Epoch: 2466 mean train loss:  3.85435997e-05, mean val. loss:  1.03253293e+00\n",
      "Epoch: 2467 mean train loss:  3.87220934e-05, mean val. loss:  1.03265488e+00\n",
      "Epoch: 2468 mean train loss:  3.74256051e-05, mean val. loss:  1.03278387e+00\n",
      "Epoch: 2469 mean train loss:  3.84375453e-05, mean val. loss:  1.03290987e+00\n",
      "Epoch: 2470 mean train loss:  3.79261910e-05, mean val. loss:  1.03304315e+00\n",
      "Epoch: 2471 mean train loss:  3.73432413e-05, mean val. loss:  1.03319013e+00\n",
      "Epoch: 2472 mean train loss:  3.79428966e-05, mean val. loss:  1.03334773e+00\n",
      "Epoch: 2473 mean train loss:  3.85625171e-05, mean val. loss:  1.03351116e+00\n",
      "Epoch: 2474 mean train loss:  3.78009281e-05, mean val. loss:  1.03368402e+00\n",
      "Epoch: 2475 mean train loss:  3.85275343e-05, mean val. loss:  1.03384006e+00\n",
      "Epoch: 2476 mean train loss:  3.81640857e-05, mean val. loss:  1.03400505e+00\n",
      "Epoch: 2477 mean train loss:  3.79409466e-05, mean val. loss:  1.03416729e+00\n",
      "Epoch: 2478 mean train loss:  3.85735475e-05, mean val. loss:  1.03434455e+00\n",
      "Epoch: 2479 mean train loss:  3.84611194e-05, mean val. loss:  1.03450787e+00\n",
      "Epoch: 2480 mean train loss:  3.76175158e-05, mean val. loss:  1.03467691e+00\n",
      "Epoch: 2481 mean train loss:  3.77455144e-05, mean val. loss:  1.03484488e+00\n",
      "Epoch: 2482 mean train loss:  3.82297440e-05, mean val. loss:  1.03500295e+00\n",
      "Epoch: 2483 mean train loss:  3.72023787e-05, mean val. loss:  1.03517723e+00\n",
      "Epoch: 2484 mean train loss:  3.81430727e-05, mean val. loss:  1.03535402e+00\n",
      "Epoch: 2485 mean train loss:  3.78130935e-05, mean val. loss:  1.03553641e+00\n",
      "Epoch: 2486 mean train loss:  3.76436510e-05, mean val. loss:  1.03573632e+00\n",
      "Epoch: 2487 mean train loss:  3.76442913e-05, mean val. loss:  1.03594208e+00\n",
      "Epoch: 2488 mean train loss:  3.81350110e-05, mean val. loss:  1.03615463e+00\n",
      "Epoch: 2489 mean train loss:  3.86594329e-05, mean val. loss:  1.03634703e+00\n",
      "Epoch: 2490 mean train loss:  3.71834612e-05, mean val. loss:  1.03655863e+00\n",
      "Epoch: 2491 mean train loss:  3.66940512e-05, mean val. loss:  1.03679645e+00\n",
      "Epoch: 2492 mean train loss:  3.84798332e-05, mean val. loss:  1.03702354e+00\n",
      "Epoch: 2493 mean train loss:  3.70337511e-05, mean val. loss:  1.03726161e+00\n",
      "Epoch: 2494 mean train loss:  3.70748457e-05, mean val. loss:  1.03751183e+00\n",
      "Epoch: 2495 mean train loss:  3.72367504e-05, mean val. loss:  1.03777003e+00\n",
      "Epoch: 2496 mean train loss:  3.82516300e-05, mean val. loss:  1.03802216e+00\n",
      "Epoch: 2497 mean train loss:  3.69004847e-05, mean val. loss:  1.03828728e+00\n",
      "Epoch: 2498 mean train loss:  3.79263656e-05, mean val. loss:  1.03856492e+00\n",
      "Epoch: 2499 mean train loss:  3.68665787e-05, mean val. loss:  1.03885055e+00\n",
      "Epoch: 2500 mean train loss:  3.79489502e-05, mean val. loss:  1.03913975e+00\n",
      "Epoch: 2501 mean train loss:  3.84364394e-05, mean val. loss:  1.03940916e+00\n",
      "Epoch: 2502 mean train loss:  3.82549479e-05, mean val. loss:  1.03967416e+00\n",
      "Epoch: 2503 mean train loss:  3.80579149e-05, mean val. loss:  1.03992331e+00\n",
      "Epoch: 2504 mean train loss:  3.78302066e-05, mean val. loss:  1.04016972e+00\n",
      "Epoch: 2505 mean train loss:  3.81409773e-05, mean val. loss:  1.04040885e+00\n",
      "Epoch: 2506 mean train loss:  3.75585514e-05, mean val. loss:  1.04062545e+00\n",
      "Epoch: 2507 mean train loss:  3.80769488e-05, mean val. loss:  1.04084444e+00\n",
      "Epoch: 2508 mean train loss:  3.86208994e-05, mean val. loss:  1.04105854e+00\n",
      "Epoch: 2509 mean train loss:  3.70214402e-05, mean val. loss:  1.04127538e+00\n",
      "Epoch: 2510 mean train loss:  3.85751482e-05, mean val. loss:  1.04147422e+00\n",
      "Epoch: 2511 mean train loss:  3.74348892e-05, mean val. loss:  1.04165697e+00\n",
      "Epoch: 2512 mean train loss:  3.83855368e-05, mean val. loss:  1.04184067e+00\n",
      "Epoch: 2513 mean train loss:  3.67487082e-05, mean val. loss:  1.04203904e+00\n",
      "Epoch: 2514 mean train loss:  3.84256127e-05, mean val. loss:  1.04221714e+00\n",
      "Epoch: 2515 mean train loss:  3.76802636e-05, mean val. loss:  1.04238522e+00\n",
      "Epoch: 2516 mean train loss:  3.82383878e-05, mean val. loss:  1.04255307e+00\n",
      "Epoch: 2517 mean train loss:  3.81798600e-05, mean val. loss:  1.04272127e+00\n",
      "Epoch: 2518 mean train loss:  3.83586157e-05, mean val. loss:  1.04287708e+00\n",
      "Epoch: 2519 mean train loss:  3.72671639e-05, mean val. loss:  1.04304802e+00\n",
      "Epoch: 2520 mean train loss:  3.86104803e-05, mean val. loss:  1.04320014e+00\n",
      "Epoch: 2521 mean train loss:  3.87699110e-05, mean val. loss:  1.04333270e+00\n",
      "Epoch: 2522 mean train loss:  3.78924306e-05, mean val. loss:  1.04346323e+00\n",
      "Epoch: 2523 mean train loss:  3.78579134e-05, mean val. loss:  1.04359412e+00\n",
      "Epoch: 2524 mean train loss:  3.90209025e-05, mean val. loss:  1.04369581e+00\n",
      "Epoch: 2525 mean train loss:  3.73079383e-05, mean val. loss:  1.04381144e+00\n",
      "Epoch: 2526 mean train loss:  3.70271155e-05, mean val. loss:  1.04394054e+00\n",
      "Epoch: 2527 mean train loss:  3.70018533e-05, mean val. loss:  1.04409885e+00\n",
      "Epoch: 2528 mean train loss:  3.73392832e-05, mean val. loss:  1.04427743e+00\n",
      "Epoch: 2529 mean train loss:  3.74906231e-05, mean val. loss:  1.04445970e+00\n",
      "Epoch: 2530 mean train loss:  3.68265319e-05, mean val. loss:  1.04466093e+00\n",
      "Epoch: 2531 mean train loss:  3.75072705e-05, mean val. loss:  1.04486871e+00\n",
      "Epoch: 2532 mean train loss:  3.82705475e-05, mean val. loss:  1.04507780e+00\n",
      "Epoch: 2533 mean train loss:  3.81972641e-05, mean val. loss:  1.04528928e+00\n",
      "Epoch: 2534 mean train loss:  3.79163248e-05, mean val. loss:  1.04550731e+00\n",
      "Epoch: 2535 mean train loss:  3.80678684e-05, mean val. loss:  1.04571652e+00\n",
      "Epoch: 2536 mean train loss:  3.66509194e-05, mean val. loss:  1.04592216e+00\n",
      "Epoch: 2537 mean train loss:  3.75745585e-05, mean val. loss:  1.04612327e+00\n",
      "Epoch: 2538 mean train loss:  3.79587000e-05, mean val. loss:  1.04631507e+00\n",
      "Epoch: 2539 mean train loss:  3.83455772e-05, mean val. loss:  1.04650640e+00\n",
      "Epoch: 2540 mean train loss:  3.77518008e-05, mean val. loss:  1.04670131e+00\n",
      "Epoch: 2541 mean train loss:  3.81938298e-05, mean val. loss:  1.04688919e+00\n",
      "Epoch: 2542 mean train loss:  3.74673982e-05, mean val. loss:  1.04709673e+00\n",
      "Epoch: 2543 mean train loss:  3.77012184e-05, mean val. loss:  1.04729939e+00\n",
      "Epoch: 2544 mean train loss:  3.71599454e-05, mean val. loss:  1.04752958e+00\n",
      "Epoch: 2545 mean train loss:  3.78657132e-05, mean val. loss:  1.04776430e+00\n",
      "Epoch: 2546 mean train loss:  3.82901344e-05, mean val. loss:  1.04797924e+00\n",
      "Epoch: 2547 mean train loss:  3.66942259e-05, mean val. loss:  1.04821992e+00\n",
      "Epoch: 2548 mean train loss:  3.79113480e-05, mean val. loss:  1.04845119e+00\n",
      "Epoch: 2549 mean train loss:  3.82118160e-05, mean val. loss:  1.04867995e+00\n",
      "Epoch: 2550 mean train loss:  3.82335857e-05, mean val. loss:  1.04888642e+00\n",
      "Epoch: 2551 mean train loss:  3.82185099e-05, mean val. loss:  1.04909587e+00\n",
      "Epoch: 2552 mean train loss:  3.84563464e-05, mean val. loss:  1.04929018e+00\n",
      "Epoch: 2553 mean train loss:  3.71382339e-05, mean val. loss:  1.04948783e+00\n",
      "Epoch: 2554 mean train loss:  3.78745608e-05, mean val. loss:  1.04969358e+00\n",
      "Epoch: 2555 mean train loss:  3.76434764e-05, mean val. loss:  1.04988503e+00\n",
      "Epoch: 2556 mean train loss:  3.85353633e-05, mean val. loss:  1.05005753e+00\n",
      "Epoch: 2557 mean train loss:  3.80714191e-05, mean val. loss:  1.05022025e+00\n",
      "Epoch: 2558 mean train loss:  3.77823017e-05, mean val. loss:  1.05036652e+00\n",
      "Epoch: 2559 mean train loss:  3.83339357e-05, mean val. loss:  1.05051374e+00\n",
      "Epoch: 2560 mean train loss:  3.80352139e-05, mean val. loss:  1.05066109e+00\n",
      "Epoch: 2561 mean train loss:  3.75229865e-05, mean val. loss:  1.05081058e+00\n",
      "Epoch: 2562 mean train loss:  3.77024407e-05, mean val. loss:  1.05096328e+00\n",
      "Epoch: 2563 mean train loss:  3.84805608e-05, mean val. loss:  1.05111766e+00\n",
      "Epoch: 2564 mean train loss:  3.73768853e-05, mean val. loss:  1.05125880e+00\n",
      "Epoch: 2565 mean train loss:  3.77777324e-05, mean val. loss:  1.05138564e+00\n",
      "Epoch: 2566 mean train loss:  3.79520643e-05, mean val. loss:  1.05151188e+00\n",
      "Epoch: 2567 mean train loss:  3.70597991e-05, mean val. loss:  1.05163169e+00\n",
      "Epoch: 2568 mean train loss:  3.74755473e-05, mean val. loss:  1.05176187e+00\n",
      "Epoch: 2569 mean train loss:  3.75056698e-05, mean val. loss:  1.05189419e+00\n",
      "Epoch: 2570 mean train loss:  3.71923961e-05, mean val. loss:  1.05203927e+00\n",
      "Epoch: 2571 mean train loss:  3.77155957e-05, mean val. loss:  1.05218124e+00\n",
      "Epoch: 2572 mean train loss:  3.77324759e-05, mean val. loss:  1.05233371e+00\n",
      "Epoch: 2573 mean train loss:  3.75146046e-05, mean val. loss:  1.05249739e+00\n",
      "Epoch: 2574 mean train loss:  3.70467897e-05, mean val. loss:  1.05265939e+00\n",
      "Epoch: 2575 mean train loss:  3.70466732e-05, mean val. loss:  1.05284023e+00\n",
      "Epoch: 2576 mean train loss:  3.78243276e-05, mean val. loss:  1.05300856e+00\n",
      "Epoch: 2577 mean train loss:  3.81408609e-05, mean val. loss:  1.05318189e+00\n",
      "Epoch: 2578 mean train loss:  3.81371938e-05, mean val. loss:  1.05334103e+00\n",
      "Epoch: 2579 mean train loss:  3.84355371e-05, mean val. loss:  1.05348706e+00\n",
      "Epoch: 2580 mean train loss:  3.75328236e-05, mean val. loss:  1.05364537e+00\n",
      "Epoch: 2581 mean train loss:  3.79888224e-05, mean val. loss:  1.05378878e+00\n",
      "Epoch: 2582 mean train loss:  3.70339258e-05, mean val. loss:  1.05395317e+00\n",
      "Epoch: 2583 mean train loss:  3.79465637e-05, mean val. loss:  1.05412054e+00\n",
      "Epoch: 2584 mean train loss:  3.71675123e-05, mean val. loss:  1.05430520e+00\n",
      "Epoch: 2585 mean train loss:  3.78865516e-05, mean val. loss:  1.05448830e+00\n",
      "Epoch: 2586 mean train loss:  3.75080854e-05, mean val. loss:  1.05467916e+00\n",
      "Epoch: 2587 mean train loss:  3.71580245e-05, mean val. loss:  1.05489421e+00\n",
      "Epoch: 2588 mean train loss:  3.69788031e-05, mean val. loss:  1.05511940e+00\n",
      "Epoch: 2589 mean train loss:  3.69017362e-05, mean val. loss:  1.05534279e+00\n",
      "Epoch: 2590 mean train loss:  3.70795024e-05, mean val. loss:  1.05558145e+00\n",
      "Epoch: 2591 mean train loss:  3.74178635e-05, mean val. loss:  1.05582535e+00\n",
      "Epoch: 2592 mean train loss:  3.64383450e-05, mean val. loss:  1.05609000e+00\n",
      "Epoch: 2593 mean train loss:  3.75392847e-05, mean val. loss:  1.05634892e+00\n",
      "Epoch: 2594 mean train loss:  3.81061982e-05, mean val. loss:  1.05660284e+00\n",
      "Epoch: 2595 mean train loss:  3.74117226e-05, mean val. loss:  1.05684793e+00\n",
      "Epoch: 2596 mean train loss:  3.81121645e-05, mean val. loss:  1.05706871e+00\n",
      "Epoch: 2597 mean train loss:  3.66930035e-05, mean val. loss:  1.05730391e+00\n",
      "Epoch: 2598 mean train loss:  3.67416360e-05, mean val. loss:  1.05755532e+00\n",
      "Epoch: 2599 mean train loss:  3.73910298e-05, mean val. loss:  1.05779517e+00\n",
      "Epoch: 2600 mean train loss:  3.69365735e-05, mean val. loss:  1.05804360e+00\n",
      "Epoch: 2601 mean train loss:  3.71740025e-05, mean val. loss:  1.05829501e+00\n",
      "Epoch: 2602 mean train loss:  3.66617751e-05, mean val. loss:  1.05854928e+00\n",
      "Epoch: 2603 mean train loss:  3.78827099e-05, mean val. loss:  1.05878115e+00\n",
      "Epoch: 2604 mean train loss:  3.71371862e-05, mean val. loss:  1.05901158e+00\n",
      "Epoch: 2605 mean train loss:  3.78320692e-05, mean val. loss:  1.05925298e+00\n",
      "Epoch: 2606 mean train loss:  3.77981341e-05, mean val. loss:  1.05951560e+00\n",
      "Epoch: 2607 mean train loss:  3.75250820e-05, mean val. loss:  1.05974793e+00\n",
      "Epoch: 2608 mean train loss:  3.81906575e-05, mean val. loss:  1.05997717e+00\n",
      "Epoch: 2609 mean train loss:  3.67820030e-05, mean val. loss:  1.06022787e+00\n",
      "Epoch: 2610 mean train loss:  3.64278094e-05, mean val. loss:  1.06049335e+00\n",
      "Epoch: 2611 mean train loss:  3.73687071e-05, mean val. loss:  1.06075096e+00\n",
      "Epoch: 2612 mean train loss:  3.68584297e-05, mean val. loss:  1.06100607e+00\n",
      "Epoch: 2613 mean train loss:  3.71039205e-05, mean val. loss:  1.06126392e+00\n",
      "Epoch: 2614 mean train loss:  3.75625095e-05, mean val. loss:  1.06151581e+00\n",
      "Epoch: 2615 mean train loss:  3.69377667e-05, mean val. loss:  1.06176186e+00\n",
      "Epoch: 2616 mean train loss:  3.74341907e-05, mean val. loss:  1.06201708e+00\n",
      "Epoch: 2617 mean train loss:  3.69694317e-05, mean val. loss:  1.06226587e+00\n",
      "Epoch: 2618 mean train loss:  3.86793399e-05, mean val. loss:  1.06249344e+00\n",
      "Epoch: 2619 mean train loss:  3.72274662e-05, mean val. loss:  1.06271446e+00\n",
      "Epoch: 2620 mean train loss:  3.77161778e-05, mean val. loss:  1.06292307e+00\n",
      "Epoch: 2621 mean train loss:  3.79967969e-05, mean val. loss:  1.06313109e+00\n",
      "Epoch: 2622 mean train loss:  3.62714054e-05, mean val. loss:  1.06335533e+00\n",
      "Epoch: 2623 mean train loss:  3.73460934e-05, mean val. loss:  1.06357574e+00\n",
      "Epoch: 2624 mean train loss:  3.71462665e-05, mean val. loss:  1.06378293e+00\n",
      "Epoch: 2625 mean train loss:  3.83376027e-05, mean val. loss:  1.06396902e+00\n",
      "Epoch: 2626 mean train loss:  3.74474912e-05, mean val. loss:  1.06413817e+00\n",
      "Epoch: 2627 mean train loss:  3.61750717e-05, mean val. loss:  1.06433225e+00\n",
      "Epoch: 2628 mean train loss:  3.78554105e-05, mean val. loss:  1.06452715e+00\n",
      "Epoch: 2629 mean train loss:  3.83707811e-05, mean val. loss:  1.06470227e+00\n",
      "Epoch: 2630 mean train loss:  3.74306692e-05, mean val. loss:  1.06488872e+00\n",
      "Epoch: 2631 mean train loss:  3.65002197e-05, mean val. loss:  1.06508565e+00\n",
      "Epoch: 2632 mean train loss:  3.78621044e-05, mean val. loss:  1.06526673e+00\n",
      "Epoch: 2633 mean train loss:  3.78428376e-05, mean val. loss:  1.06544566e+00\n",
      "Epoch: 2634 mean train loss:  3.68283654e-05, mean val. loss:  1.06562936e+00\n",
      "Epoch: 2635 mean train loss:  3.75850359e-05, mean val. loss:  1.06581521e+00\n",
      "Epoch: 2636 mean train loss:  3.73416115e-05, mean val. loss:  1.06599402e+00\n",
      "Epoch: 2637 mean train loss:  3.77875695e-05, mean val. loss:  1.06615686e+00\n",
      "Epoch: 2638 mean train loss:  3.74721130e-05, mean val. loss:  1.06631982e+00\n",
      "Epoch: 2639 mean train loss:  3.80015699e-05, mean val. loss:  1.06647348e+00\n",
      "Epoch: 2640 mean train loss:  3.68595938e-05, mean val. loss:  1.06664670e+00\n",
      "Epoch: 2641 mean train loss:  3.71564820e-05, mean val. loss:  1.06682837e+00\n",
      "Epoch: 2642 mean train loss:  3.69100017e-05, mean val. loss:  1.06702518e+00\n",
      "Epoch: 2643 mean train loss:  3.70990019e-05, mean val. loss:  1.06720805e+00\n",
      "Epoch: 2644 mean train loss:  3.66603490e-05, mean val. loss:  1.06739962e+00\n",
      "Epoch: 2645 mean train loss:  3.78345430e-05, mean val. loss:  1.06759179e+00\n",
      "Epoch: 2646 mean train loss:  3.79040721e-05, mean val. loss:  1.06776392e+00\n",
      "Epoch: 2647 mean train loss:  3.75956297e-05, mean val. loss:  1.06794560e+00\n",
      "Epoch: 2648 mean train loss:  3.74364899e-05, mean val. loss:  1.06812370e+00\n",
      "Epoch: 2649 mean train loss:  3.73601215e-05, mean val. loss:  1.06829321e+00\n",
      "Epoch: 2650 mean train loss:  3.64705338e-05, mean val. loss:  1.06848347e+00\n",
      "Epoch: 2651 mean train loss:  3.74693191e-05, mean val. loss:  1.06867814e+00\n",
      "Epoch: 2652 mean train loss:  3.73042421e-05, mean val. loss:  1.06887960e+00\n",
      "Epoch: 2653 mean train loss:  3.68387555e-05, mean val. loss:  1.06909537e+00\n",
      "Epoch: 2654 mean train loss:  3.83486040e-05, mean val. loss:  1.06929457e+00\n",
      "Epoch: 2655 mean train loss:  3.61245475e-05, mean val. loss:  1.06948602e+00\n",
      "Epoch: 2656 mean train loss:  3.64943407e-05, mean val. loss:  1.06969893e+00\n",
      "Epoch: 2657 mean train loss:  3.71516217e-05, mean val. loss:  1.06989622e+00\n",
      "Epoch: 2658 mean train loss:  3.72869545e-05, mean val. loss:  1.07009757e+00\n",
      "Epoch: 2659 mean train loss:  3.63770523e-05, mean val. loss:  1.07031476e+00\n",
      "Epoch: 2660 mean train loss:  3.68878536e-05, mean val. loss:  1.07053316e+00\n",
      "Epoch: 2661 mean train loss:  3.79737467e-05, mean val. loss:  1.07075095e+00\n",
      "Epoch: 2662 mean train loss:  3.71917849e-05, mean val. loss:  1.07095814e+00\n",
      "Epoch: 2663 mean train loss:  3.83186853e-05, mean val. loss:  1.07115865e+00\n",
      "Epoch: 2664 mean train loss:  3.79153644e-05, mean val. loss:  1.07134771e+00\n",
      "Epoch: 2665 mean train loss:  3.68673936e-05, mean val. loss:  1.07155061e+00\n",
      "Epoch: 2666 mean train loss:  3.65105516e-05, mean val. loss:  1.07175553e+00\n",
      "Epoch: 2667 mean train loss:  3.70171038e-05, mean val. loss:  1.07195795e+00\n",
      "Epoch: 2668 mean train loss:  3.64872976e-05, mean val. loss:  1.07217586e+00\n",
      "Epoch: 2669 mean train loss:  3.66287713e-05, mean val. loss:  1.07238913e+00\n",
      "Epoch: 2670 mean train loss:  3.73070070e-05, mean val. loss:  1.07259703e+00\n",
      "Epoch: 2671 mean train loss:  3.73102084e-05, mean val. loss:  1.07280219e+00\n",
      "Epoch: 2672 mean train loss:  3.67733010e-05, mean val. loss:  1.07300794e+00\n",
      "Epoch: 2673 mean train loss:  3.68492911e-05, mean val. loss:  1.07321548e+00\n",
      "Epoch: 2674 mean train loss:  3.71449860e-05, mean val. loss:  1.07340848e+00\n",
      "Epoch: 2675 mean train loss:  3.65952728e-05, mean val. loss:  1.07359326e+00\n",
      "Epoch: 2676 mean train loss:  3.72753129e-05, mean val. loss:  1.07377923e+00\n",
      "Epoch: 2677 mean train loss:  3.70558701e-05, mean val. loss:  1.07396019e+00\n",
      "Epoch: 2678 mean train loss:  3.70875059e-05, mean val. loss:  1.07412076e+00\n",
      "Epoch: 2679 mean train loss:  3.75287200e-05, mean val. loss:  1.07428777e+00\n",
      "Epoch: 2680 mean train loss:  3.82142607e-05, mean val. loss:  1.07446253e+00\n",
      "Epoch: 2681 mean train loss:  3.71733913e-05, mean val. loss:  1.07463622e+00\n",
      "Epoch: 2682 mean train loss:  3.69629706e-05, mean val. loss:  1.07480466e+00\n",
      "Epoch: 2683 mean train loss:  3.79160629e-05, mean val. loss:  1.07496345e+00\n",
      "Epoch: 2684 mean train loss:  3.67007451e-05, mean val. loss:  1.07512653e+00\n",
      "Epoch: 2685 mean train loss:  3.64795560e-05, mean val. loss:  1.07530010e+00\n",
      "Epoch: 2686 mean train loss:  3.74456868e-05, mean val. loss:  1.07546794e+00\n",
      "Epoch: 2687 mean train loss:  3.68627370e-05, mean val. loss:  1.07564676e+00\n",
      "Epoch: 2688 mean train loss:  3.70114285e-05, mean val. loss:  1.07581663e+00\n",
      "Epoch: 2689 mean train loss:  3.70667258e-05, mean val. loss:  1.07596958e+00\n",
      "Epoch: 2690 mean train loss:  3.64749867e-05, mean val. loss:  1.07612228e+00\n",
      "Epoch: 2691 mean train loss:  3.65369488e-05, mean val. loss:  1.07628608e+00\n",
      "Epoch: 2692 mean train loss:  3.63177096e-05, mean val. loss:  1.07645392e+00\n",
      "Epoch: 2693 mean train loss:  3.72897484e-05, mean val. loss:  1.07661176e+00\n",
      "Epoch: 2694 mean train loss:  3.70423659e-05, mean val. loss:  1.07678306e+00\n",
      "Epoch: 2695 mean train loss:  3.74407973e-05, mean val. loss:  1.07694447e+00\n",
      "Epoch: 2696 mean train loss:  3.69563058e-05, mean val. loss:  1.07710731e+00\n",
      "Epoch: 2697 mean train loss:  3.63613945e-05, mean val. loss:  1.07729137e+00\n",
      "Epoch: 2698 mean train loss:  3.72292707e-05, mean val. loss:  1.07747054e+00\n",
      "Epoch: 2699 mean train loss:  3.70835187e-05, mean val. loss:  1.07764697e+00\n",
      "Epoch: 2700 mean train loss:  3.79949633e-05, mean val. loss:  1.07780147e+00\n",
      "Epoch: 2701 mean train loss:  3.76449316e-05, mean val. loss:  1.07794249e+00\n",
      "Epoch: 2702 mean train loss:  3.75880627e-05, mean val. loss:  1.07808721e+00\n",
      "Epoch: 2703 mean train loss:  3.74275260e-05, mean val. loss:  1.07822597e+00\n",
      "Epoch: 2704 mean train loss:  3.68041219e-05, mean val. loss:  1.07836461e+00\n",
      "Epoch: 2705 mean train loss:  3.71478382e-05, mean val. loss:  1.07848215e+00\n",
      "Epoch: 2706 mean train loss:  3.72480135e-05, mean val. loss:  1.07860088e+00\n",
      "Epoch: 2707 mean train loss:  3.78669356e-05, mean val. loss:  1.07872069e+00\n",
      "Epoch: 2708 mean train loss:  3.76692042e-05, mean val. loss:  1.07882679e+00\n",
      "Epoch: 2709 mean train loss:  3.70251364e-05, mean val. loss:  1.07894409e+00\n",
      "Epoch: 2710 mean train loss:  3.63546133e-05, mean val. loss:  1.07906973e+00\n",
      "Epoch: 2711 mean train loss:  3.71684437e-05, mean val. loss:  1.07920015e+00\n",
      "Epoch: 2712 mean train loss:  3.75679228e-05, mean val. loss:  1.07933235e+00\n",
      "Epoch: 2713 mean train loss:  3.75451637e-05, mean val. loss:  1.07946932e+00\n",
      "Epoch: 2714 mean train loss:  3.68374167e-05, mean val. loss:  1.07961869e+00\n",
      "Epoch: 2715 mean train loss:  3.69436457e-05, mean val. loss:  1.07976460e+00\n",
      "Epoch: 2716 mean train loss:  3.65697488e-05, mean val. loss:  1.07992125e+00\n",
      "Epoch: 2717 mean train loss:  3.71335482e-05, mean val. loss:  1.08008957e+00\n",
      "Epoch: 2718 mean train loss:  3.74318915e-05, mean val. loss:  1.08023381e+00\n",
      "Epoch: 2719 mean train loss:  3.71584902e-05, mean val. loss:  1.08038831e+00\n",
      "Epoch: 2720 mean train loss:  3.73301154e-05, mean val. loss:  1.08054125e+00\n",
      "Epoch: 2721 mean train loss:  3.63443105e-05, mean val. loss:  1.08070815e+00\n",
      "Epoch: 2722 mean train loss:  3.60701233e-05, mean val. loss:  1.08089352e+00\n",
      "Epoch: 2723 mean train loss:  3.64900334e-05, mean val. loss:  1.08110571e+00\n",
      "Epoch: 2724 mean train loss:  3.66479508e-05, mean val. loss:  1.08131635e+00\n",
      "Epoch: 2725 mean train loss:  3.65180313e-05, mean val. loss:  1.08152962e+00\n",
      "Epoch: 2726 mean train loss:  3.57308018e-05, mean val. loss:  1.08175635e+00\n",
      "Epoch: 2727 mean train loss:  3.69852060e-05, mean val. loss:  1.08198249e+00\n",
      "Epoch: 2728 mean train loss:  3.67005414e-05, mean val. loss:  1.08221185e+00\n",
      "Epoch: 2729 mean train loss:  3.74175142e-05, mean val. loss:  1.08243632e+00\n",
      "Epoch: 2730 mean train loss:  3.61112179e-05, mean val. loss:  1.08266902e+00\n",
      "Epoch: 2731 mean train loss:  3.66276654e-05, mean val. loss:  1.08290255e+00\n",
      "Epoch: 2732 mean train loss:  3.72695213e-05, mean val. loss:  1.08312750e+00\n",
      "Epoch: 2733 mean train loss:  3.65818269e-05, mean val. loss:  1.08334994e+00\n",
      "Epoch: 2734 mean train loss:  3.66579043e-05, mean val. loss:  1.08356845e+00\n",
      "Epoch: 2735 mean train loss:  3.71869537e-05, mean val. loss:  1.08378708e+00\n",
      "Epoch: 2736 mean train loss:  3.61226848e-05, mean val. loss:  1.08403158e+00\n",
      "Epoch: 2737 mean train loss:  3.68437031e-05, mean val. loss:  1.08427548e+00\n",
      "Epoch: 2738 mean train loss:  3.61896819e-05, mean val. loss:  1.08452070e+00\n",
      "Epoch: 2739 mean train loss:  3.65743181e-05, mean val. loss:  1.08475351e+00\n",
      "Epoch: 2740 mean train loss:  3.63013824e-05, mean val. loss:  1.08500528e+00\n",
      "Epoch: 2741 mean train loss:  3.69682384e-05, mean val. loss:  1.08526170e+00\n",
      "Epoch: 2742 mean train loss:  3.56226810e-05, mean val. loss:  1.08551991e+00\n",
      "Epoch: 2743 mean train loss:  3.69121553e-05, mean val. loss:  1.08577466e+00\n",
      "Epoch: 2744 mean train loss:  3.64813313e-05, mean val. loss:  1.08602369e+00\n",
      "Epoch: 2745 mean train loss:  3.65580781e-05, mean val. loss:  1.08627355e+00\n",
      "Epoch: 2746 mean train loss:  3.60524573e-05, mean val. loss:  1.08653569e+00\n",
      "Epoch: 2747 mean train loss:  3.64845037e-05, mean val. loss:  1.08680344e+00\n",
      "Epoch: 2748 mean train loss:  3.75463569e-05, mean val. loss:  1.08705366e+00\n",
      "Epoch: 2749 mean train loss:  3.62975406e-05, mean val. loss:  1.08728957e+00\n",
      "Epoch: 2750 mean train loss:  3.77673132e-05, mean val. loss:  1.08751702e+00\n",
      "Epoch: 2751 mean train loss:  3.68818874e-05, mean val. loss:  1.08774388e+00\n",
      "Epoch: 2752 mean train loss:  3.76918761e-05, mean val. loss:  1.08795893e+00\n",
      "Epoch: 2753 mean train loss:  3.64476291e-05, mean val. loss:  1.08818543e+00\n",
      "Epoch: 2754 mean train loss:  3.68217588e-05, mean val. loss:  1.08841574e+00\n",
      "Epoch: 2755 mean train loss:  3.78364348e-05, mean val. loss:  1.08860433e+00\n",
      "Epoch: 2756 mean train loss:  3.69575282e-05, mean val. loss:  1.08879209e+00\n",
      "Epoch: 2757 mean train loss:  3.66535096e-05, mean val. loss:  1.08898497e+00\n",
      "Epoch: 2758 mean train loss:  3.72970826e-05, mean val. loss:  1.08916605e+00\n",
      "Epoch: 2759 mean train loss:  3.72346258e-05, mean val. loss:  1.08935142e+00\n",
      "Epoch: 2760 mean train loss:  3.71256901e-05, mean val. loss:  1.08953714e+00\n",
      "Epoch: 2761 mean train loss:  3.73001676e-05, mean val. loss:  1.08970964e+00\n",
      "Epoch: 2762 mean train loss:  3.64580774e-05, mean val. loss:  1.08987689e+00\n",
      "Epoch: 2763 mean train loss:  3.69111076e-05, mean val. loss:  1.09004152e+00\n",
      "Epoch: 2764 mean train loss:  3.68572655e-05, mean val. loss:  1.09019852e+00\n",
      "Epoch: 2765 mean train loss:  3.61724815e-05, mean val. loss:  1.09036636e+00\n",
      "Epoch: 2766 mean train loss:  3.70404159e-05, mean val. loss:  1.09051704e+00\n",
      "Epoch: 2767 mean train loss:  3.70691414e-05, mean val. loss:  1.09067225e+00\n",
      "Epoch: 2768 mean train loss:  3.57435783e-05, mean val. loss:  1.09083426e+00\n",
      "Epoch: 2769 mean train loss:  3.63700092e-05, mean val. loss:  1.09099841e+00\n",
      "Epoch: 2770 mean train loss:  3.63440777e-05, mean val. loss:  1.09117579e+00\n",
      "Epoch: 2771 mean train loss:  3.60755948e-05, mean val. loss:  1.09136009e+00\n",
      "Epoch: 2772 mean train loss:  3.72971408e-05, mean val. loss:  1.09153426e+00\n",
      "Epoch: 2773 mean train loss:  3.72190552e-05, mean val. loss:  1.09170640e+00\n",
      "Epoch: 2774 mean train loss:  3.68222245e-05, mean val. loss:  1.09187031e+00\n",
      "Epoch: 2775 mean train loss:  3.66802851e-05, mean val. loss:  1.09204268e+00\n",
      "Epoch: 2776 mean train loss:  3.71857313e-05, mean val. loss:  1.09222376e+00\n",
      "Epoch: 2777 mean train loss:  3.71338683e-05, mean val. loss:  1.09238279e+00\n",
      "Epoch: 2778 mean train loss:  3.67531902e-05, mean val. loss:  1.09256053e+00\n",
      "Epoch: 2779 mean train loss:  3.73245275e-05, mean val. loss:  1.09273326e+00\n",
      "Epoch: 2780 mean train loss:  3.75628588e-05, mean val. loss:  1.09290743e+00\n",
      "Epoch: 2781 mean train loss:  3.61971906e-05, mean val. loss:  1.09307885e+00\n",
      "Epoch: 2782 mean train loss:  3.68774054e-05, mean val. loss:  1.09324718e+00\n",
      "Epoch: 2783 mean train loss:  3.64715524e-05, mean val. loss:  1.09342563e+00\n",
      "Epoch: 2784 mean train loss:  3.68073815e-05, mean val. loss:  1.09358835e+00\n",
      "Epoch: 2785 mean train loss:  3.63764120e-05, mean val. loss:  1.09377193e+00\n",
      "Epoch: 2786 mean train loss:  3.64375592e-05, mean val. loss:  1.09396076e+00\n",
      "Epoch: 2787 mean train loss:  3.63303698e-05, mean val. loss:  1.09415674e+00\n",
      "Epoch: 2788 mean train loss:  3.60570557e-05, mean val. loss:  1.09436798e+00\n",
      "Epoch: 2789 mean train loss:  3.68880865e-05, mean val. loss:  1.09457147e+00\n",
      "Epoch: 2790 mean train loss:  3.65529559e-05, mean val. loss:  1.09476888e+00\n",
      "Epoch: 2791 mean train loss:  3.57827521e-05, mean val. loss:  1.09499407e+00\n",
      "Epoch: 2792 mean train loss:  3.71452770e-05, mean val. loss:  1.09521139e+00\n",
      "Epoch: 2793 mean train loss:  3.67349712e-05, mean val. loss:  1.09542453e+00\n",
      "Epoch: 2794 mean train loss:  3.58106336e-05, mean val. loss:  1.09564042e+00\n",
      "Epoch: 2795 mean train loss:  3.57786776e-05, mean val. loss:  1.09587014e+00\n",
      "Epoch: 2796 mean train loss:  3.70730995e-05, mean val. loss:  1.09608221e+00\n",
      "Epoch: 2797 mean train loss:  3.76201933e-05, mean val. loss:  1.09627748e+00\n",
      "Epoch: 2798 mean train loss:  3.67822067e-05, mean val. loss:  1.09648633e+00\n",
      "Epoch: 2799 mean train loss:  3.68776382e-05, mean val. loss:  1.09668779e+00\n",
      "Epoch: 2800 mean train loss:  3.67772300e-05, mean val. loss:  1.09688640e+00\n",
      "Epoch: 2801 mean train loss:  3.65929445e-05, mean val. loss:  1.09707463e+00\n",
      "Epoch: 2802 mean train loss:  3.70740308e-05, mean val. loss:  1.09725833e+00\n",
      "Epoch: 2803 mean train loss:  3.64369480e-05, mean val. loss:  1.09744060e+00\n",
      "Epoch: 2804 mean train loss:  3.71740025e-05, mean val. loss:  1.09761512e+00\n",
      "Epoch: 2805 mean train loss:  3.68859619e-05, mean val. loss:  1.09778118e+00\n",
      "Epoch: 2806 mean train loss:  3.68764740e-05, mean val. loss:  1.09794796e+00\n",
      "Epoch: 2807 mean train loss:  3.56963137e-05, mean val. loss:  1.09812796e+00\n",
      "Epoch: 2808 mean train loss:  3.69424815e-05, mean val. loss:  1.09830797e+00\n",
      "Epoch: 2809 mean train loss:  3.68566252e-05, mean val. loss:  1.09849167e+00\n",
      "Epoch: 2810 mean train loss:  3.65952728e-05, mean val. loss:  1.09867871e+00\n",
      "Epoch: 2811 mean train loss:  3.74685042e-05, mean val. loss:  1.09886253e+00\n",
      "Epoch: 2812 mean train loss:  3.69267073e-05, mean val. loss:  1.09903359e+00\n",
      "Epoch: 2813 mean train loss:  3.61594721e-05, mean val. loss:  1.09920478e+00\n",
      "Epoch: 2814 mean train loss:  3.73118673e-05, mean val. loss:  1.09935188e+00\n",
      "Epoch: 2815 mean train loss:  3.67473986e-05, mean val. loss:  1.09950221e+00\n",
      "Epoch: 2816 mean train loss:  3.60040576e-05, mean val. loss:  1.09966731e+00\n",
      "Epoch: 2817 mean train loss:  3.64597654e-05, mean val. loss:  1.09983647e+00\n",
      "Epoch: 2818 mean train loss:  3.68601177e-05, mean val. loss:  1.10000253e+00\n",
      "Epoch: 2819 mean train loss:  3.78404220e-05, mean val. loss:  1.10015392e+00\n",
      "Epoch: 2820 mean train loss:  3.63182626e-05, mean val. loss:  1.10030580e+00\n",
      "Epoch: 2821 mean train loss:  3.66900640e-05, mean val. loss:  1.10045385e+00\n",
      "Epoch: 2822 mean train loss:  3.70014168e-05, mean val. loss:  1.10059035e+00\n",
      "Epoch: 2823 mean train loss:  3.66879685e-05, mean val. loss:  1.10073018e+00\n",
      "Epoch: 2824 mean train loss:  3.64145380e-05, mean val. loss:  1.10087800e+00\n",
      "Epoch: 2825 mean train loss:  3.60885751e-05, mean val. loss:  1.10104716e+00\n",
      "Epoch: 2826 mean train loss:  3.62635474e-05, mean val. loss:  1.10121882e+00\n",
      "Epoch: 2827 mean train loss:  3.64615989e-05, mean val. loss:  1.10139537e+00\n",
      "Epoch: 2828 mean train loss:  3.60939885e-05, mean val. loss:  1.10159183e+00\n",
      "Epoch: 2829 mean train loss:  3.68076144e-05, mean val. loss:  1.10178220e+00\n",
      "Epoch: 2830 mean train loss:  3.70619528e-05, mean val. loss:  1.10195160e+00\n",
      "Epoch: 2831 mean train loss:  3.70285125e-05, mean val. loss:  1.10211563e+00\n",
      "Epoch: 2832 mean train loss:  3.63245781e-05, mean val. loss:  1.10226953e+00\n",
      "Epoch: 2833 mean train loss:  3.67247558e-05, mean val. loss:  1.10244656e+00\n",
      "Epoch: 2834 mean train loss:  3.76923417e-05, mean val. loss:  1.10261035e+00\n",
      "Epoch: 2835 mean train loss:  3.61660495e-05, mean val. loss:  1.10278976e+00\n",
      "Epoch: 2836 mean train loss:  3.64265579e-05, mean val. loss:  1.10296738e+00\n",
      "Epoch: 2837 mean train loss:  3.63559229e-05, mean val. loss:  1.10315025e+00\n",
      "Epoch: 2838 mean train loss:  3.59140395e-05, mean val. loss:  1.10332620e+00\n",
      "Epoch: 2839 mean train loss:  3.67737084e-05, mean val. loss:  1.10350740e+00\n",
      "Epoch: 2840 mean train loss:  3.63726576e-05, mean val. loss:  1.10368967e+00\n",
      "Epoch: 2841 mean train loss:  3.63051076e-05, mean val. loss:  1.10387778e+00\n",
      "Epoch: 2842 mean train loss:  3.58832767e-05, mean val. loss:  1.10408294e+00\n",
      "Epoch: 2843 mean train loss:  3.56307719e-05, mean val. loss:  1.10429549e+00\n",
      "Epoch: 2844 mean train loss:  3.72951035e-05, mean val. loss:  1.10450733e+00\n",
      "Epoch: 2845 mean train loss:  3.70522903e-05, mean val. loss:  1.10473192e+00\n",
      "Epoch: 2846 mean train loss:  3.56225064e-05, mean val. loss:  1.10496902e+00\n",
      "Epoch: 2847 mean train loss:  3.71096539e-05, mean val. loss:  1.10519493e+00\n",
      "Epoch: 2848 mean train loss:  3.67837201e-05, mean val. loss:  1.10541987e+00\n",
      "Epoch: 2849 mean train loss:  3.56679666e-05, mean val. loss:  1.10564232e+00\n",
      "Epoch: 2850 mean train loss:  3.66166350e-05, mean val. loss:  1.10586035e+00\n",
      "Epoch: 2851 mean train loss:  3.65454762e-05, mean val. loss:  1.10608125e+00\n",
      "Epoch: 2852 mean train loss:  3.71222559e-05, mean val. loss:  1.10629153e+00\n",
      "Epoch: 2853 mean train loss:  3.54733202e-05, mean val. loss:  1.10652161e+00\n",
      "Epoch: 2854 mean train loss:  3.57693061e-05, mean val. loss:  1.10674870e+00\n",
      "Epoch: 2855 mean train loss:  3.69876216e-05, mean val. loss:  1.10695422e+00\n",
      "Epoch: 2856 mean train loss:  3.68342153e-05, mean val. loss:  1.10713947e+00\n",
      "Epoch: 2857 mean train loss:  3.60768463e-05, mean val. loss:  1.10732615e+00\n",
      "Epoch: 2858 mean train loss:  3.61012644e-05, mean val. loss:  1.10751641e+00\n",
      "Epoch: 2859 mean train loss:  3.66033055e-05, mean val. loss:  1.10771191e+00\n",
      "Epoch: 2860 mean train loss:  3.70980124e-05, mean val. loss:  1.10788202e+00\n",
      "Epoch: 2861 mean train loss:  3.64081934e-05, mean val. loss:  1.10805035e+00\n",
      "Epoch: 2862 mean train loss:  3.65484448e-05, mean val. loss:  1.10819995e+00\n",
      "Epoch: 2863 mean train loss:  3.57503304e-05, mean val. loss:  1.10836756e+00\n",
      "Epoch: 2864 mean train loss:  3.60814156e-05, mean val. loss:  1.10854173e+00\n",
      "Epoch: 2865 mean train loss:  3.70045891e-05, mean val. loss:  1.10871816e+00\n",
      "Epoch: 2866 mean train loss:  3.64594453e-05, mean val. loss:  1.10890126e+00\n",
      "Epoch: 2867 mean train loss:  3.59974802e-05, mean val. loss:  1.10907865e+00\n",
      "Epoch: 2868 mean train loss:  3.63927102e-05, mean val. loss:  1.10924637e+00\n",
      "Epoch: 2869 mean train loss:  3.63634317e-05, mean val. loss:  1.10941529e+00\n",
      "Epoch: 2870 mean train loss:  3.68320325e-05, mean val. loss:  1.10958743e+00\n",
      "Epoch: 2871 mean train loss:  3.61798448e-05, mean val. loss:  1.10976744e+00\n",
      "Epoch: 2872 mean train loss:  3.64799052e-05, mean val. loss:  1.10994494e+00\n",
      "Epoch: 2873 mean train loss:  3.62506544e-05, mean val. loss:  1.11011398e+00\n",
      "Epoch: 2874 mean train loss:  3.68702458e-05, mean val. loss:  1.11028349e+00\n",
      "Epoch: 2875 mean train loss:  3.60243721e-05, mean val. loss:  1.11045671e+00\n",
      "Epoch: 2876 mean train loss:  3.62614519e-05, mean val. loss:  1.11064720e+00\n",
      "Epoch: 2877 mean train loss:  3.57504177e-05, mean val. loss:  1.11085093e+00\n",
      "Epoch: 2878 mean train loss:  3.73147195e-05, mean val. loss:  1.11104798e+00\n",
      "Epoch: 2879 mean train loss:  3.60096456e-05, mean val. loss:  1.11123526e+00\n",
      "Epoch: 2880 mean train loss:  3.69356130e-05, mean val. loss:  1.11140156e+00\n",
      "Epoch: 2881 mean train loss:  3.56893870e-05, mean val. loss:  1.11158109e+00\n",
      "Epoch: 2882 mean train loss:  3.61695420e-05, mean val. loss:  1.11177671e+00\n",
      "Epoch: 2883 mean train loss:  3.52962525e-05, mean val. loss:  1.11197448e+00\n",
      "Epoch: 2884 mean train loss:  3.68622423e-05, mean val. loss:  1.11216879e+00\n",
      "Epoch: 2885 mean train loss:  3.58367979e-05, mean val. loss:  1.11237037e+00\n",
      "Epoch: 2886 mean train loss:  3.62409046e-05, mean val. loss:  1.11257970e+00\n",
      "Epoch: 2887 mean train loss:  3.61526327e-05, mean val. loss:  1.11276531e+00\n",
      "Epoch: 2888 mean train loss:  3.57010576e-05, mean val. loss:  1.11296332e+00\n",
      "Epoch: 2889 mean train loss:  3.64294974e-05, mean val. loss:  1.11316431e+00\n",
      "Epoch: 2890 mean train loss:  3.67211760e-05, mean val. loss:  1.11336339e+00\n",
      "Epoch: 2891 mean train loss:  3.60499544e-05, mean val. loss:  1.11356974e+00\n",
      "Epoch: 2892 mean train loss:  3.59863625e-05, mean val. loss:  1.11378241e+00\n",
      "Epoch: 2893 mean train loss:  3.58453835e-05, mean val. loss:  1.11400449e+00\n",
      "Epoch: 2894 mean train loss:  3.57106328e-05, mean val. loss:  1.11423981e+00\n",
      "Epoch: 2895 mean train loss:  3.61665734e-05, mean val. loss:  1.11448455e+00\n",
      "Epoch: 2896 mean train loss:  3.68155015e-05, mean val. loss:  1.11471546e+00\n",
      "Epoch: 2897 mean train loss:  3.63295840e-05, mean val. loss:  1.11495709e+00\n",
      "Epoch: 2898 mean train loss:  3.65609885e-05, mean val. loss:  1.11520934e+00\n",
      "Epoch: 2899 mean train loss:  3.60406702e-05, mean val. loss:  1.11545277e+00\n",
      "Epoch: 2900 mean train loss:  3.57243407e-05, mean val. loss:  1.11570358e+00\n",
      "Epoch: 2901 mean train loss:  3.71301139e-05, mean val. loss:  1.11593711e+00\n",
      "Epoch: 2902 mean train loss:  3.63179133e-05, mean val. loss:  1.11616635e+00\n",
      "Epoch: 2903 mean train loss:  3.63943400e-05, mean val. loss:  1.11637890e+00\n",
      "Epoch: 2904 mean train loss:  3.68060428e-05, mean val. loss:  1.11658645e+00\n",
      "Epoch: 2905 mean train loss:  3.59338592e-05, mean val. loss:  1.11679220e+00\n",
      "Epoch: 2906 mean train loss:  3.66397435e-05, mean val. loss:  1.11699009e+00\n",
      "Epoch: 2907 mean train loss:  3.69568588e-05, mean val. loss:  1.11718059e+00\n",
      "Epoch: 2908 mean train loss:  3.62932915e-05, mean val. loss:  1.11736560e+00\n",
      "Epoch: 2909 mean train loss:  3.65657033e-05, mean val. loss:  1.11755693e+00\n",
      "Epoch: 2910 mean train loss:  3.57071694e-05, mean val. loss:  1.11775064e+00\n",
      "Epoch: 2911 mean train loss:  3.69760965e-05, mean val. loss:  1.11792219e+00\n",
      "Epoch: 2912 mean train loss:  3.59198311e-05, mean val. loss:  1.11809075e+00\n",
      "Epoch: 2913 mean train loss:  3.63426516e-05, mean val. loss:  1.11825287e+00\n",
      "Epoch: 2914 mean train loss:  3.57801910e-05, mean val. loss:  1.11841428e+00\n",
      "Epoch: 2915 mean train loss:  3.56855453e-05, mean val. loss:  1.11856878e+00\n",
      "Epoch: 2916 mean train loss:  3.55764641e-05, mean val. loss:  1.11874783e+00\n",
      "Epoch: 2917 mean train loss:  3.62017308e-05, mean val. loss:  1.11893129e+00\n",
      "Epoch: 2918 mean train loss:  3.58102261e-05, mean val. loss:  1.11910009e+00\n",
      "Epoch: 2919 mean train loss:  3.60138365e-05, mean val. loss:  1.11927104e+00\n",
      "Epoch: 2920 mean train loss:  3.60911654e-05, mean val. loss:  1.11944067e+00\n",
      "Epoch: 2921 mean train loss:  3.56716337e-05, mean val. loss:  1.11960578e+00\n",
      "Epoch: 2922 mean train loss:  3.65974847e-05, mean val. loss:  1.11976421e+00\n",
      "Epoch: 2923 mean train loss:  3.55143857e-05, mean val. loss:  1.11992979e+00\n",
      "Epoch: 2924 mean train loss:  3.59921833e-05, mean val. loss:  1.12008476e+00\n",
      "Epoch: 2925 mean train loss:  3.55964294e-05, mean val. loss:  1.12024367e+00\n",
      "Epoch: 2926 mean train loss:  3.60585400e-05, mean val. loss:  1.12039649e+00\n",
      "Epoch: 2927 mean train loss:  3.59921833e-05, mean val. loss:  1.12055933e+00\n",
      "Epoch: 2928 mean train loss:  3.58020770e-05, mean val. loss:  1.12072718e+00\n",
      "Epoch: 2929 mean train loss:  3.68188776e-05, mean val. loss:  1.12088478e+00\n",
      "Epoch: 2930 mean train loss:  3.63845611e-05, mean val. loss:  1.12103629e+00\n",
      "Epoch: 2931 mean train loss:  3.59836267e-05, mean val. loss:  1.12119901e+00\n",
      "Epoch: 2932 mean train loss:  3.51215713e-05, mean val. loss:  1.12139153e+00\n",
      "Epoch: 2933 mean train loss:  3.50421178e-05, mean val. loss:  1.12160456e+00\n",
      "Epoch: 2934 mean train loss:  3.65468732e-05, mean val. loss:  1.12180531e+00\n",
      "Epoch: 2935 mean train loss:  3.65757442e-05, mean val. loss:  1.12201095e+00\n",
      "Epoch: 2936 mean train loss:  3.65684391e-05, mean val. loss:  1.12221634e+00\n",
      "Epoch: 2937 mean train loss:  3.61460261e-05, mean val. loss:  1.12241757e+00\n",
      "Epoch: 2938 mean train loss:  3.65887536e-05, mean val. loss:  1.12260985e+00\n",
      "Epoch: 2939 mean train loss:  3.74399824e-05, mean val. loss:  1.12279880e+00\n",
      "Epoch: 2940 mean train loss:  3.60707054e-05, mean val. loss:  1.12298596e+00\n",
      "Epoch: 2941 mean train loss:  3.54451477e-05, mean val. loss:  1.12320209e+00\n",
      "Epoch: 2942 mean train loss:  3.61703569e-05, mean val. loss:  1.12340522e+00\n",
      "Epoch: 2943 mean train loss:  3.61875282e-05, mean val. loss:  1.12359869e+00\n",
      "Epoch: 2944 mean train loss:  3.69135814e-05, mean val. loss:  1.12379146e+00\n",
      "Epoch: 2945 mean train loss:  3.62417777e-05, mean val. loss:  1.12398684e+00\n",
      "Epoch: 2946 mean train loss:  3.63172148e-05, mean val. loss:  1.12418127e+00\n",
      "Epoch: 2947 mean train loss:  3.59692494e-05, mean val. loss:  1.12438393e+00\n",
      "Epoch: 2948 mean train loss:  3.64205334e-05, mean val. loss:  1.12457871e+00\n",
      "Epoch: 2949 mean train loss:  3.61144776e-05, mean val. loss:  1.12476444e+00\n",
      "Epoch: 2950 mean train loss:  3.58808902e-05, mean val. loss:  1.12494493e+00\n",
      "Epoch: 2951 mean train loss:  3.61400016e-05, mean val. loss:  1.12512100e+00\n",
      "Epoch: 2952 mean train loss:  3.60426493e-05, mean val. loss:  1.12530482e+00\n",
      "Epoch: 2953 mean train loss:  3.64076113e-05, mean val. loss:  1.12547457e+00\n",
      "Epoch: 2954 mean train loss:  3.60410486e-05, mean val. loss:  1.12564254e+00\n",
      "Epoch: 2955 mean train loss:  3.67463799e-05, mean val. loss:  1.12580776e+00\n",
      "Epoch: 2956 mean train loss:  3.59375554e-05, mean val. loss:  1.12597668e+00\n",
      "Epoch: 2957 mean train loss:  3.58785328e-05, mean val. loss:  1.12614226e+00\n",
      "Epoch: 2958 mean train loss:  3.61031271e-05, mean val. loss:  1.12631559e+00\n",
      "Epoch: 2959 mean train loss:  3.60984704e-05, mean val. loss:  1.12647700e+00\n",
      "Epoch: 2960 mean train loss:  3.60222766e-05, mean val. loss:  1.12665451e+00\n",
      "Epoch: 2961 mean train loss:  3.61686689e-05, mean val. loss:  1.12683129e+00\n",
      "Epoch: 2962 mean train loss:  3.62210558e-05, mean val. loss:  1.12700677e+00\n",
      "Epoch: 2963 mean train loss:  3.72711220e-05, mean val. loss:  1.12717605e+00\n",
      "Epoch: 2964 mean train loss:  3.52470961e-05, mean val. loss:  1.12734449e+00\n",
      "Epoch: 2965 mean train loss:  3.58704710e-05, mean val. loss:  1.12753189e+00\n",
      "Epoch: 2966 mean train loss:  3.63008003e-05, mean val. loss:  1.12772775e+00\n",
      "Epoch: 2967 mean train loss:  3.53208161e-05, mean val. loss:  1.12794650e+00\n",
      "Epoch: 2968 mean train loss:  3.66129971e-05, mean val. loss:  1.12814260e+00\n",
      "Epoch: 2969 mean train loss:  3.65588057e-05, mean val. loss:  1.12834835e+00\n",
      "Epoch: 2970 mean train loss:  3.66772292e-05, mean val. loss:  1.12854397e+00\n",
      "Epoch: 2971 mean train loss:  3.64531879e-05, mean val. loss:  1.12873220e+00\n",
      "Epoch: 2972 mean train loss:  3.70080816e-05, mean val. loss:  1.12892771e+00\n",
      "Epoch: 2973 mean train loss:  3.49872280e-05, mean val. loss:  1.12914598e+00\n",
      "Epoch: 2974 mean train loss:  3.66356689e-05, mean val. loss:  1.12935650e+00\n",
      "Epoch: 2975 mean train loss:  3.54604563e-05, mean val. loss:  1.12957144e+00\n",
      "Epoch: 2976 mean train loss:  3.61893908e-05, mean val. loss:  1.12979150e+00\n",
      "Epoch: 2977 mean train loss:  3.60798440e-05, mean val. loss:  1.13000810e+00\n",
      "Epoch: 2978 mean train loss:  3.63661093e-05, mean val. loss:  1.13021767e+00\n",
      "Epoch: 2979 mean train loss:  3.54085350e-05, mean val. loss:  1.13043725e+00\n",
      "Epoch: 2980 mean train loss:  3.63045838e-05, mean val. loss:  1.13064873e+00\n",
      "Epoch: 2981 mean train loss:  3.60350823e-05, mean val. loss:  1.13085938e+00\n",
      "Epoch: 2982 mean train loss:  3.56617966e-05, mean val. loss:  1.13108528e+00\n",
      "Epoch: 2983 mean train loss:  3.61080456e-05, mean val. loss:  1.13130319e+00\n",
      "Epoch: 2984 mean train loss:  3.65921587e-05, mean val. loss:  1.13153172e+00\n",
      "Epoch: 2985 mean train loss:  3.68255132e-05, mean val. loss:  1.13174212e+00\n",
      "Epoch: 2986 mean train loss:  3.62455030e-05, mean val. loss:  1.13195837e+00\n",
      "Epoch: 2987 mean train loss:  3.54574295e-05, mean val. loss:  1.13218451e+00\n",
      "Epoch: 2988 mean train loss:  3.60936683e-05, mean val. loss:  1.13240039e+00\n",
      "Epoch: 2989 mean train loss:  3.58470716e-05, mean val. loss:  1.13261628e+00\n",
      "Epoch: 2990 mean train loss:  3.54825752e-05, mean val. loss:  1.13285553e+00\n",
      "Epoch: 2991 mean train loss:  3.66127351e-05, mean val. loss:  1.13308334e+00\n",
      "Epoch: 2992 mean train loss:  3.64115112e-05, mean val. loss:  1.13331509e+00\n",
      "Epoch: 2993 mean train loss:  3.60647391e-05, mean val. loss:  1.13354683e+00\n",
      "Epoch: 2994 mean train loss:  3.61951534e-05, mean val. loss:  1.13375688e+00\n",
      "Epoch: 2995 mean train loss:  3.65294982e-05, mean val. loss:  1.13394094e+00\n",
      "Epoch: 2996 mean train loss:  3.70667549e-05, mean val. loss:  1.13411379e+00\n",
      "Epoch: 2997 mean train loss:  3.60874692e-05, mean val. loss:  1.13428283e+00\n",
      "Epoch: 2998 mean train loss:  3.63566214e-05, mean val. loss:  1.13444293e+00\n",
      "Epoch: 2999 mean train loss:  3.61329294e-05, mean val. loss:  1.13460112e+00\n",
      "Epoch: 3000 mean train loss:  3.52473580e-05, mean val. loss:  1.13474643e+00\n",
      "Epoch: 3001 mean train loss:  3.59060359e-05, mean val. loss:  1.13490272e+00\n",
      "Epoch: 3002 mean train loss:  3.53789073e-05, mean val. loss:  1.13506544e+00\n",
      "Epoch: 3003 mean train loss:  3.62796709e-05, mean val. loss:  1.13523567e+00\n",
      "Epoch: 3004 mean train loss:  3.62687861e-05, mean val. loss:  1.13540494e+00\n",
      "Epoch: 3005 mean train loss:  3.60135164e-05, mean val. loss:  1.13557005e+00\n",
      "Epoch: 3006 mean train loss:  3.61049024e-05, mean val. loss:  1.13573909e+00\n",
      "Epoch: 3007 mean train loss:  3.52200877e-05, mean val. loss:  1.13592827e+00\n",
      "Epoch: 3008 mean train loss:  3.49798356e-05, mean val. loss:  1.13612425e+00\n",
      "Epoch: 3009 mean train loss:  3.55490483e-05, mean val. loss:  1.13632345e+00\n",
      "Epoch: 3010 mean train loss:  3.60425911e-05, mean val. loss:  1.13653219e+00\n",
      "Epoch: 3011 mean train loss:  3.57181416e-05, mean val. loss:  1.13673210e+00\n",
      "Epoch: 3012 mean train loss:  3.61143029e-05, mean val. loss:  1.13693881e+00\n",
      "Epoch: 3013 mean train loss:  3.55685479e-05, mean val. loss:  1.13715315e+00\n",
      "Epoch: 3014 mean train loss:  3.58379912e-05, mean val. loss:  1.13736653e+00\n",
      "Epoch: 3015 mean train loss:  3.57695972e-05, mean val. loss:  1.13757586e+00\n",
      "Epoch: 3016 mean train loss:  3.60044069e-05, mean val. loss:  1.13779378e+00\n",
      "Epoch: 3017 mean train loss:  3.49857728e-05, mean val. loss:  1.13801610e+00\n",
      "Epoch: 3018 mean train loss:  3.54694494e-05, mean val. loss:  1.13824880e+00\n",
      "Epoch: 3019 mean train loss:  3.55763477e-05, mean val. loss:  1.13848162e+00\n",
      "Epoch: 3020 mean train loss:  3.57197132e-05, mean val. loss:  1.13873124e+00\n",
      "Epoch: 3021 mean train loss:  3.56166274e-05, mean val. loss:  1.13898766e+00\n",
      "Epoch: 3022 mean train loss:  3.50731425e-05, mean val. loss:  1.13923395e+00\n",
      "Epoch: 3023 mean train loss:  3.57808894e-05, mean val. loss:  1.13947690e+00\n",
      "Epoch: 3024 mean train loss:  3.55974771e-05, mean val. loss:  1.13971329e+00\n",
      "Epoch: 3025 mean train loss:  3.60503327e-05, mean val. loss:  1.13993454e+00\n",
      "Epoch: 3026 mean train loss:  3.59877595e-05, mean val. loss:  1.14015663e+00\n",
      "Epoch: 3027 mean train loss:  3.56819946e-05, mean val. loss:  1.14037144e+00\n",
      "Epoch: 3028 mean train loss:  3.63526633e-05, mean val. loss:  1.14056575e+00\n",
      "Epoch: 3029 mean train loss:  3.54801305e-05, mean val. loss:  1.14077115e+00\n",
      "Epoch: 3030 mean train loss:  3.56975943e-05, mean val. loss:  1.14097703e+00\n",
      "Epoch: 3031 mean train loss:  3.52528878e-05, mean val. loss:  1.14120257e+00\n",
      "Epoch: 3032 mean train loss:  3.57713725e-05, mean val. loss:  1.14141679e+00\n",
      "Epoch: 3033 mean train loss:  3.69963818e-05, mean val. loss:  1.14161205e+00\n",
      "Epoch: 3034 mean train loss:  3.68606998e-05, mean val. loss:  1.14178860e+00\n",
      "Epoch: 3035 mean train loss:  3.58700054e-05, mean val. loss:  1.14197230e+00\n",
      "Epoch: 3036 mean train loss:  3.58199177e-05, mean val. loss:  1.14214849e+00\n",
      "Epoch: 3037 mean train loss:  3.59404949e-05, mean val. loss:  1.14233124e+00\n",
      "Epoch: 3038 mean train loss:  3.61083949e-05, mean val. loss:  1.14250362e+00\n",
      "Epoch: 3039 mean train loss:  3.56928504e-05, mean val. loss:  1.14265907e+00\n",
      "Epoch: 3040 mean train loss:  3.57874087e-05, mean val. loss:  1.14280760e+00\n",
      "Epoch: 3041 mean train loss:  3.57472454e-05, mean val. loss:  1.14295602e+00\n",
      "Epoch: 3042 mean train loss:  3.66970489e-05, mean val. loss:  1.14308453e+00\n",
      "Epoch: 3043 mean train loss:  3.56119126e-05, mean val. loss:  1.14322293e+00\n",
      "Epoch: 3044 mean train loss:  3.69379122e-05, mean val. loss:  1.14333034e+00\n",
      "Epoch: 3045 mean train loss:  3.58290854e-05, mean val. loss:  1.14343977e+00\n",
      "Epoch: 3046 mean train loss:  3.55532102e-05, mean val. loss:  1.14355195e+00\n",
      "Epoch: 3047 mean train loss:  3.58024263e-05, mean val. loss:  1.14367497e+00\n",
      "Epoch: 3048 mean train loss:  3.55473603e-05, mean val. loss:  1.14380312e+00\n",
      "Epoch: 3049 mean train loss:  3.55116790e-05, mean val. loss:  1.14392579e+00\n",
      "Epoch: 3050 mean train loss:  3.58993129e-05, mean val. loss:  1.14404809e+00\n",
      "Epoch: 3051 mean train loss:  3.59644764e-05, mean val. loss:  1.14416301e+00\n",
      "Epoch: 3052 mean train loss:  3.63935251e-05, mean val. loss:  1.14428687e+00\n",
      "Epoch: 3053 mean train loss:  3.58333928e-05, mean val. loss:  1.14440548e+00\n",
      "Epoch: 3054 mean train loss:  3.58957914e-05, mean val. loss:  1.14453399e+00\n",
      "Epoch: 3055 mean train loss:  3.53419746e-05, mean val. loss:  1.14468050e+00\n",
      "Epoch: 3056 mean train loss:  3.57527460e-05, mean val. loss:  1.14481413e+00\n",
      "Epoch: 3057 mean train loss:  3.57798999e-05, mean val. loss:  1.14494467e+00\n",
      "Epoch: 3058 mean train loss:  3.60638660e-05, mean val. loss:  1.14507544e+00\n",
      "Epoch: 3059 mean train loss:  3.54170334e-05, mean val. loss:  1.14521956e+00\n",
      "Epoch: 3060 mean train loss:  3.59379337e-05, mean val. loss:  1.14536762e+00\n",
      "Epoch: 3061 mean train loss:  3.60012345e-05, mean val. loss:  1.14551687e+00\n",
      "Epoch: 3062 mean train loss:  3.57628742e-05, mean val. loss:  1.14566052e+00\n",
      "Epoch: 3063 mean train loss:  3.63486470e-05, mean val. loss:  1.14580309e+00\n",
      "Epoch: 3064 mean train loss:  3.46815505e-05, mean val. loss:  1.14596832e+00\n",
      "Epoch: 3065 mean train loss:  3.55329830e-05, mean val. loss:  1.14614952e+00\n",
      "Epoch: 3066 mean train loss:  3.66857275e-05, mean val. loss:  1.14631951e+00\n",
      "Epoch: 3067 mean train loss:  3.50018381e-05, mean val. loss:  1.14650559e+00\n",
      "Epoch: 3068 mean train loss:  3.57294630e-05, mean val. loss:  1.14668036e+00\n",
      "Epoch: 3069 mean train loss:  3.43508145e-05, mean val. loss:  1.14688599e+00\n",
      "Epoch: 3070 mean train loss:  3.53251526e-05, mean val. loss:  1.14710999e+00\n",
      "Epoch: 3071 mean train loss:  3.52384232e-05, mean val. loss:  1.14734137e+00\n",
      "Epoch: 3072 mean train loss:  3.59982369e-05, mean val. loss:  1.14756262e+00\n",
      "Epoch: 3073 mean train loss:  3.53869400e-05, mean val. loss:  1.14778936e+00\n",
      "Epoch: 3074 mean train loss:  3.61468992e-05, mean val. loss:  1.14801586e+00\n",
      "Epoch: 3075 mean train loss:  3.57436656e-05, mean val. loss:  1.14823687e+00\n",
      "Epoch: 3076 mean train loss:  3.57509125e-05, mean val. loss:  1.14845037e+00\n",
      "Epoch: 3077 mean train loss:  3.58408142e-05, mean val. loss:  1.14867377e+00\n",
      "Epoch: 3078 mean train loss:  3.62271967e-05, mean val. loss:  1.14889526e+00\n",
      "Epoch: 3079 mean train loss:  3.58588586e-05, mean val. loss:  1.14910781e+00\n",
      "Epoch: 3080 mean train loss:  3.52825446e-05, mean val. loss:  1.14932895e+00\n",
      "Epoch: 3081 mean train loss:  3.51788767e-05, mean val. loss:  1.14955211e+00\n",
      "Epoch: 3082 mean train loss:  3.56673263e-05, mean val. loss:  1.14977264e+00\n",
      "Epoch: 3083 mean train loss:  3.56084493e-05, mean val. loss:  1.14998794e+00\n",
      "Epoch: 3084 mean train loss:  3.50664486e-05, mean val. loss:  1.15021622e+00\n",
      "Epoch: 3085 mean train loss:  3.59156693e-05, mean val. loss:  1.15043592e+00\n",
      "Epoch: 3086 mean train loss:  3.53841460e-05, mean val. loss:  1.15065682e+00\n",
      "Epoch: 3087 mean train loss:  3.59324040e-05, mean val. loss:  1.15085709e+00\n",
      "Epoch: 3088 mean train loss:  3.56093806e-05, mean val. loss:  1.15106928e+00\n",
      "Epoch: 3089 mean train loss:  3.55519296e-05, mean val. loss:  1.15129113e+00\n",
      "Epoch: 3090 mean train loss:  3.59750411e-05, mean val. loss:  1.15150130e+00\n",
      "Epoch: 3091 mean train loss:  3.51298659e-05, mean val. loss:  1.15170360e+00\n",
      "Epoch: 3092 mean train loss:  3.53709911e-05, mean val. loss:  1.15189815e+00\n",
      "Epoch: 3093 mean train loss:  3.58108664e-05, mean val. loss:  1.15208662e+00\n",
      "Epoch: 3094 mean train loss:  3.68190231e-05, mean val. loss:  1.15226519e+00\n",
      "Epoch: 3095 mean train loss:  3.52814386e-05, mean val. loss:  1.15244699e+00\n",
      "Epoch: 3096 mean train loss:  3.54620861e-05, mean val. loss:  1.15263414e+00\n",
      "Epoch: 3097 mean train loss:  3.53297510e-05, mean val. loss:  1.15281153e+00\n",
      "Epoch: 3098 mean train loss:  3.51537892e-05, mean val. loss:  1.15299368e+00\n",
      "Epoch: 3099 mean train loss:  3.64704174e-05, mean val. loss:  1.15316486e+00\n",
      "Epoch: 3100 mean train loss:  3.61144484e-05, mean val. loss:  1.15332794e+00\n",
      "Epoch: 3101 mean train loss:  3.43965658e-05, mean val. loss:  1.15351796e+00\n",
      "Epoch: 3102 mean train loss:  3.52140632e-05, mean val. loss:  1.15372074e+00\n",
      "Epoch: 3103 mean train loss:  3.57611279e-05, mean val. loss:  1.15391779e+00\n",
      "Epoch: 3104 mean train loss:  3.63272266e-05, mean val. loss:  1.15410721e+00\n",
      "Epoch: 3105 mean train loss:  3.61206185e-05, mean val. loss:  1.15429246e+00\n",
      "Epoch: 3106 mean train loss:  3.52636853e-05, mean val. loss:  1.15447748e+00\n",
      "Epoch: 3107 mean train loss:  3.53696814e-05, mean val. loss:  1.15465522e+00\n",
      "Epoch: 3108 mean train loss:  3.51223862e-05, mean val. loss:  1.15484202e+00\n",
      "Epoch: 3109 mean train loss:  3.56517266e-05, mean val. loss:  1.15501726e+00\n",
      "Epoch: 3110 mean train loss:  3.59423866e-05, mean val. loss:  1.15520537e+00\n",
      "Epoch: 3111 mean train loss:  3.55441007e-05, mean val. loss:  1.15541208e+00\n",
      "Epoch: 3112 mean train loss:  3.56695382e-05, mean val. loss:  1.15562391e+00\n",
      "Epoch: 3113 mean train loss:  3.60586564e-05, mean val. loss:  1.15581846e+00\n",
      "Epoch: 3114 mean train loss:  3.63611907e-05, mean val. loss:  1.15601325e+00\n",
      "Epoch: 3115 mean train loss:  3.55923548e-05, mean val. loss:  1.15620327e+00\n",
      "Epoch: 3116 mean train loss:  3.57963145e-05, mean val. loss:  1.15640259e+00\n",
      "Epoch: 3117 mean train loss:  3.58821126e-05, mean val. loss:  1.15659451e+00\n",
      "Epoch: 3118 mean train loss:  3.50558257e-05, mean val. loss:  1.15678322e+00\n",
      "Epoch: 3119 mean train loss:  3.55087104e-05, mean val. loss:  1.15698242e+00\n",
      "Epoch: 3120 mean train loss:  3.50222399e-05, mean val. loss:  1.15716851e+00\n",
      "Epoch: 3121 mean train loss:  3.51570488e-05, mean val. loss:  1.15736783e+00\n",
      "Epoch: 3122 mean train loss:  3.52728821e-05, mean val. loss:  1.15755808e+00\n",
      "Epoch: 3123 mean train loss:  3.50740447e-05, mean val. loss:  1.15775919e+00\n",
      "Epoch: 3124 mean train loss:  3.50856571e-05, mean val. loss:  1.15796888e+00\n",
      "Epoch: 3125 mean train loss:  3.62876453e-05, mean val. loss:  1.15816104e+00\n",
      "Epoch: 3126 mean train loss:  3.57348181e-05, mean val. loss:  1.15835774e+00\n",
      "Epoch: 3127 mean train loss:  3.65000451e-05, mean val. loss:  1.15854180e+00\n",
      "Epoch: 3128 mean train loss:  3.50333867e-05, mean val. loss:  1.15872133e+00\n",
      "Epoch: 3129 mean train loss:  3.46288143e-05, mean val. loss:  1.15890300e+00\n",
      "Epoch: 3130 mean train loss:  3.52526840e-05, mean val. loss:  1.15908992e+00\n",
      "Epoch: 3131 mean train loss:  3.56698292e-05, mean val. loss:  1.15926468e+00\n",
      "Epoch: 3132 mean train loss:  3.48365575e-05, mean val. loss:  1.15945017e+00\n",
      "Epoch: 3133 mean train loss:  3.54530639e-05, mean val. loss:  1.15963149e+00\n",
      "Epoch: 3134 mean train loss:  3.58429097e-05, mean val. loss:  1.15979564e+00\n",
      "Epoch: 3135 mean train loss:  3.65249871e-05, mean val. loss:  1.15996349e+00\n",
      "Epoch: 3136 mean train loss:  3.51499475e-05, mean val. loss:  1.16015005e+00\n",
      "Epoch: 3137 mean train loss:  3.57683748e-05, mean val. loss:  1.16035092e+00\n",
      "Epoch: 3138 mean train loss:  3.51326016e-05, mean val. loss:  1.16056848e+00\n",
      "Epoch: 3139 mean train loss:  3.54248332e-05, mean val. loss:  1.16078305e+00\n",
      "Epoch: 3140 mean train loss:  3.68789188e-05, mean val. loss:  1.16097140e+00\n",
      "Epoch: 3141 mean train loss:  3.47496243e-05, mean val. loss:  1.16118002e+00\n",
      "Epoch: 3142 mean train loss:  3.62337742e-05, mean val. loss:  1.16139925e+00\n",
      "Epoch: 3143 mean train loss:  3.54771037e-05, mean val. loss:  1.16162193e+00\n",
      "Epoch: 3144 mean train loss:  3.60598497e-05, mean val. loss:  1.16184568e+00\n",
      "Epoch: 3145 mean train loss:  3.53667419e-05, mean val. loss:  1.16208851e+00\n",
      "Epoch: 3146 mean train loss:  3.63565923e-05, mean val. loss:  1.16232264e+00\n",
      "Epoch: 3147 mean train loss:  3.52378411e-05, mean val. loss:  1.16256022e+00\n",
      "Epoch: 3148 mean train loss:  3.55190423e-05, mean val. loss:  1.16281164e+00\n",
      "Epoch: 3149 mean train loss:  3.61581624e-05, mean val. loss:  1.16305673e+00\n",
      "Epoch: 3150 mean train loss:  3.57638055e-05, mean val. loss:  1.16329682e+00\n",
      "Epoch: 3151 mean train loss:  3.53332725e-05, mean val. loss:  1.16353953e+00\n",
      "Epoch: 3152 mean train loss:  3.46148736e-05, mean val. loss:  1.16380000e+00\n",
      "Epoch: 3153 mean train loss:  3.46429588e-05, mean val. loss:  1.16405714e+00\n",
      "Epoch: 3154 mean train loss:  3.54339136e-05, mean val. loss:  1.16432321e+00\n",
      "Epoch: 3155 mean train loss:  3.53180803e-05, mean val. loss:  1.16457570e+00\n",
      "Epoch: 3156 mean train loss:  3.47476162e-05, mean val. loss:  1.16483438e+00\n",
      "Epoch: 3157 mean train loss:  3.51487251e-05, mean val. loss:  1.16507554e+00\n",
      "Epoch: 3158 mean train loss:  3.51544586e-05, mean val. loss:  1.16530073e+00\n",
      "Epoch: 3159 mean train loss:  3.55810917e-05, mean val. loss:  1.16550517e+00\n",
      "Epoch: 3160 mean train loss:  3.45770677e-05, mean val. loss:  1.16571414e+00\n",
      "Epoch: 3161 mean train loss:  3.51985218e-05, mean val. loss:  1.16592312e+00\n",
      "Epoch: 3162 mean train loss:  3.48457252e-05, mean val. loss:  1.16615224e+00\n",
      "Epoch: 3163 mean train loss:  3.57105746e-05, mean val. loss:  1.16637003e+00\n",
      "Epoch: 3164 mean train loss:  3.45798908e-05, mean val. loss:  1.16659892e+00\n",
      "Epoch: 3165 mean train loss:  3.48967733e-05, mean val. loss:  1.16682315e+00\n",
      "Epoch: 3166 mean train loss:  3.54086224e-05, mean val. loss:  1.16701984e+00\n",
      "Epoch: 3167 mean train loss:  3.54421209e-05, mean val. loss:  1.16721654e+00\n",
      "Epoch: 3168 mean train loss:  3.48923495e-05, mean val. loss:  1.16742408e+00\n",
      "Epoch: 3169 mean train loss:  3.64367152e-05, mean val. loss:  1.16759539e+00\n",
      "Epoch: 3170 mean train loss:  3.63467261e-05, mean val. loss:  1.16774023e+00\n",
      "Epoch: 3171 mean train loss:  3.49272741e-05, mean val. loss:  1.16789353e+00\n",
      "Epoch: 3172 mean train loss:  3.48018948e-05, mean val. loss:  1.16805470e+00\n",
      "Epoch: 3173 mean train loss:  3.50378396e-05, mean val. loss:  1.16821253e+00\n",
      "Epoch: 3174 mean train loss:  3.56554810e-05, mean val. loss:  1.16837013e+00\n",
      "Epoch: 3175 mean train loss:  3.47130990e-05, mean val. loss:  1.16851830e+00\n",
      "Epoch: 3176 mean train loss:  3.57812096e-05, mean val. loss:  1.16865981e+00\n",
      "Epoch: 3177 mean train loss:  3.58420948e-05, mean val. loss:  1.16879773e+00\n",
      "Epoch: 3178 mean train loss:  3.46031156e-05, mean val. loss:  1.16894627e+00\n",
      "Epoch: 3179 mean train loss:  3.45870503e-05, mean val. loss:  1.16909492e+00\n",
      "Epoch: 3180 mean train loss:  3.49156617e-05, mean val. loss:  1.16924727e+00\n",
      "Epoch: 3181 mean train loss:  3.58429388e-05, mean val. loss:  1.16940236e+00\n",
      "Epoch: 3182 mean train loss:  3.52271600e-05, mean val. loss:  1.16957068e+00\n",
      "Epoch: 3183 mean train loss:  3.57234967e-05, mean val. loss:  1.16972768e+00\n",
      "Epoch: 3184 mean train loss:  3.51685449e-05, mean val. loss:  1.16987479e+00\n",
      "Epoch: 3185 mean train loss:  3.49325419e-05, mean val. loss:  1.17003524e+00\n",
      "Epoch: 3186 mean train loss:  3.46025627e-05, mean val. loss:  1.17021048e+00\n",
      "Epoch: 3187 mean train loss:  3.47161258e-05, mean val. loss:  1.17040408e+00\n",
      "Epoch: 3188 mean train loss:  3.59277765e-05, mean val. loss:  1.17058647e+00\n",
      "Epoch: 3189 mean train loss:  3.50689224e-05, mean val. loss:  1.17077374e+00\n",
      "Epoch: 3190 mean train loss:  3.58408142e-05, mean val. loss:  1.17095554e+00\n",
      "Epoch: 3191 mean train loss:  3.61503917e-05, mean val. loss:  1.17113829e+00\n",
      "Epoch: 3192 mean train loss:  3.55237571e-05, mean val. loss:  1.17132521e+00\n",
      "Epoch: 3193 mean train loss:  3.51494818e-05, mean val. loss:  1.17150664e+00\n",
      "Epoch: 3194 mean train loss:  3.49130423e-05, mean val. loss:  1.17169321e+00\n",
      "Epoch: 3195 mean train loss:  3.49575421e-05, mean val. loss:  1.17188120e+00\n",
      "Epoch: 3196 mean train loss:  3.56517558e-05, mean val. loss:  1.17206252e+00\n",
      "Epoch: 3197 mean train loss:  3.55997472e-05, mean val. loss:  1.17224538e+00\n",
      "Epoch: 3198 mean train loss:  3.52929637e-05, mean val. loss:  1.17244470e+00\n",
      "Epoch: 3199 mean train loss:  3.47014284e-05, mean val. loss:  1.17264247e+00\n",
      "Epoch: 3200 mean train loss:  3.49114998e-05, mean val. loss:  1.17285442e+00\n",
      "Epoch: 3201 mean train loss:  3.53806536e-05, mean val. loss:  1.17305970e+00\n",
      "Epoch: 3202 mean train loss:  3.56332457e-05, mean val. loss:  1.17324042e+00\n",
      "Epoch: 3203 mean train loss:  3.46488669e-05, mean val. loss:  1.17343044e+00\n",
      "Epoch: 3204 mean train loss:  3.50972696e-05, mean val. loss:  1.17363250e+00\n",
      "Epoch: 3205 mean train loss:  3.48702597e-05, mean val. loss:  1.17384374e+00\n",
      "Epoch: 3206 mean train loss:  3.51895287e-05, mean val. loss:  1.17406523e+00\n",
      "Epoch: 3207 mean train loss:  3.49192705e-05, mean val. loss:  1.17428410e+00\n",
      "Epoch: 3208 mean train loss:  3.54679069e-05, mean val. loss:  1.17449117e+00\n",
      "Epoch: 3209 mean train loss:  3.49799811e-05, mean val. loss:  1.17469049e+00\n",
      "Epoch: 3210 mean train loss:  3.52909847e-05, mean val. loss:  1.17489946e+00\n",
      "Epoch: 3211 mean train loss:  3.59557453e-05, mean val. loss:  1.17509484e+00\n",
      "Epoch: 3212 mean train loss:  3.55103111e-05, mean val. loss:  1.17529559e+00\n",
      "Epoch: 3213 mean train loss:  3.55221564e-05, mean val. loss:  1.17549801e+00\n",
      "Epoch: 3214 mean train loss:  3.53150535e-05, mean val. loss:  1.17569280e+00\n",
      "Epoch: 3215 mean train loss:  3.43505526e-05, mean val. loss:  1.17590225e+00\n",
      "Epoch: 3216 mean train loss:  3.51424096e-05, mean val. loss:  1.17610145e+00\n",
      "Epoch: 3217 mean train loss:  3.52052739e-05, mean val. loss:  1.17630792e+00\n",
      "Epoch: 3218 mean train loss:  3.53356590e-05, mean val. loss:  1.17651248e+00\n",
      "Epoch: 3219 mean train loss:  3.49420588e-05, mean val. loss:  1.17670929e+00\n",
      "Epoch: 3220 mean train loss:  3.49563779e-05, mean val. loss:  1.17693150e+00\n",
      "Epoch: 3221 mean train loss:  3.61697166e-05, mean val. loss:  1.17714465e+00\n",
      "Epoch: 3222 mean train loss:  3.48937465e-05, mean val. loss:  1.17736244e+00\n",
      "Epoch: 3223 mean train loss:  3.45568988e-05, mean val. loss:  1.17759430e+00\n",
      "Epoch: 3224 mean train loss:  3.61083075e-05, mean val. loss:  1.17779028e+00\n",
      "Epoch: 3225 mean train loss:  3.51553026e-05, mean val. loss:  1.17798471e+00\n",
      "Epoch: 3226 mean train loss:  3.54927324e-05, mean val. loss:  1.17817128e+00\n",
      "Epoch: 3227 mean train loss:  3.59741098e-05, mean val. loss:  1.17835677e+00\n",
      "Epoch: 3228 mean train loss:  3.49436013e-05, mean val. loss:  1.17855811e+00\n",
      "Epoch: 3229 mean train loss:  3.49053589e-05, mean val. loss:  1.17875528e+00\n",
      "Epoch: 3230 mean train loss:  3.46019515e-05, mean val. loss:  1.17894554e+00\n",
      "Epoch: 3231 mean train loss:  3.49610928e-05, mean val. loss:  1.17914641e+00\n",
      "Epoch: 3232 mean train loss:  3.53729411e-05, mean val. loss:  1.17934120e+00\n",
      "Epoch: 3233 mean train loss:  3.53286159e-05, mean val. loss:  1.17953598e+00\n",
      "Epoch: 3234 mean train loss:  3.49961047e-05, mean val. loss:  1.17972672e+00\n",
      "Epoch: 3235 mean train loss:  3.52813513e-05, mean val. loss:  1.17991996e+00\n",
      "Epoch: 3236 mean train loss:  3.53118230e-05, mean val. loss:  1.18009698e+00\n",
      "Epoch: 3237 mean train loss:  3.47955502e-05, mean val. loss:  1.18027878e+00\n",
      "Epoch: 3238 mean train loss:  3.56209639e-05, mean val. loss:  1.18046772e+00\n",
      "Epoch: 3239 mean train loss:  3.48602189e-05, mean val. loss:  1.18064094e+00\n",
      "Epoch: 3240 mean train loss:  3.44050059e-05, mean val. loss:  1.18082523e+00\n",
      "Epoch: 3241 mean train loss:  3.48681642e-05, mean val. loss:  1.18100929e+00\n",
      "Epoch: 3242 mean train loss:  3.58386314e-05, mean val. loss:  1.18117106e+00\n",
      "Epoch: 3243 mean train loss:  3.50518385e-05, mean val. loss:  1.18134761e+00\n",
      "Epoch: 3244 mean train loss:  3.59502155e-05, mean val. loss:  1.18151987e+00\n",
      "Epoch: 3245 mean train loss:  3.51316412e-05, mean val. loss:  1.18169856e+00\n",
      "Epoch: 3246 mean train loss:  3.46169982e-05, mean val. loss:  1.18188691e+00\n",
      "Epoch: 3247 mean train loss:  3.54633958e-05, mean val. loss:  1.18206406e+00\n",
      "Epoch: 3248 mean train loss:  3.57787067e-05, mean val. loss:  1.18223047e+00\n",
      "Epoch: 3249 mean train loss:  3.52617062e-05, mean val. loss:  1.18239403e+00\n",
      "Epoch: 3250 mean train loss:  3.50502378e-05, mean val. loss:  1.18256140e+00\n",
      "Epoch: 3251 mean train loss:  3.43922002e-05, mean val. loss:  1.18274295e+00\n",
      "Epoch: 3252 mean train loss:  3.58215475e-05, mean val. loss:  1.18292689e+00\n",
      "Epoch: 3253 mean train loss:  3.50251212e-05, mean val. loss:  1.18310809e+00\n",
      "Epoch: 3254 mean train loss:  3.51610070e-05, mean val. loss:  1.18328810e+00\n",
      "Epoch: 3255 mean train loss:  3.49628099e-05, mean val. loss:  1.18348169e+00\n",
      "Epoch: 3256 mean train loss:  3.53540818e-05, mean val. loss:  1.18367362e+00\n",
      "Epoch: 3257 mean train loss:  3.49816110e-05, mean val. loss:  1.18385911e+00\n",
      "Epoch: 3258 mean train loss:  3.54315271e-05, mean val. loss:  1.18403471e+00\n",
      "Epoch: 3259 mean train loss:  3.50022165e-05, mean val. loss:  1.18421960e+00\n",
      "Epoch: 3260 mean train loss:  3.54392105e-05, mean val. loss:  1.18440378e+00\n",
      "Epoch: 3261 mean train loss:  3.47631867e-05, mean val. loss:  1.18458235e+00\n",
      "Epoch: 3262 mean train loss:  3.51532653e-05, mean val. loss:  1.18476629e+00\n",
      "Epoch: 3263 mean train loss:  3.53213109e-05, mean val. loss:  1.18494534e+00\n",
      "Epoch: 3264 mean train loss:  3.50532937e-05, mean val. loss:  1.18511450e+00\n",
      "Epoch: 3265 mean train loss:  3.51124036e-05, mean val. loss:  1.18528855e+00\n",
      "Epoch: 3266 mean train loss:  3.49592883e-05, mean val. loss:  1.18547702e+00\n",
      "Epoch: 3267 mean train loss:  3.55671800e-05, mean val. loss:  1.18566382e+00\n",
      "Epoch: 3268 mean train loss:  3.48080066e-05, mean val. loss:  1.18585575e+00\n",
      "Epoch: 3269 mean train loss:  3.52902280e-05, mean val. loss:  1.18604720e+00\n",
      "Epoch: 3270 mean train loss:  3.47940950e-05, mean val. loss:  1.18624258e+00\n",
      "Epoch: 3271 mean train loss:  3.50983755e-05, mean val. loss:  1.18644428e+00\n",
      "Epoch: 3272 mean train loss:  3.43667925e-05, mean val. loss:  1.18665421e+00\n",
      "Epoch: 3273 mean train loss:  3.57213721e-05, mean val. loss:  1.18686509e+00\n",
      "Epoch: 3274 mean train loss:  3.42348067e-05, mean val. loss:  1.18708241e+00\n",
      "Epoch: 3275 mean train loss:  3.49155744e-05, mean val. loss:  1.18729663e+00\n",
      "Epoch: 3276 mean train loss:  3.57762619e-05, mean val. loss:  1.18748534e+00\n",
      "Epoch: 3277 mean train loss:  3.49448237e-05, mean val. loss:  1.18766820e+00\n",
      "Epoch: 3278 mean train loss:  3.53594369e-05, mean val. loss:  1.18785703e+00\n",
      "Epoch: 3279 mean train loss:  3.52175848e-05, mean val. loss:  1.18804026e+00\n",
      "Epoch: 3280 mean train loss:  3.52812931e-05, mean val. loss:  1.18823314e+00\n",
      "Epoch: 3281 mean train loss:  3.56202072e-05, mean val. loss:  1.18840957e+00\n",
      "Epoch: 3282 mean train loss:  3.53667419e-05, mean val. loss:  1.18856895e+00\n",
      "Epoch: 3283 mean train loss:  3.59597616e-05, mean val. loss:  1.18872702e+00\n",
      "Epoch: 3284 mean train loss:  3.45231092e-05, mean val. loss:  1.18890977e+00\n",
      "Epoch: 3285 mean train loss:  3.50728806e-05, mean val. loss:  1.18908072e+00\n",
      "Epoch: 3286 mean train loss:  3.48742469e-05, mean val. loss:  1.18923867e+00\n",
      "Epoch: 3287 mean train loss:  3.50375194e-05, mean val. loss:  1.18940985e+00\n",
      "Epoch: 3288 mean train loss:  3.49096663e-05, mean val. loss:  1.18958509e+00\n",
      "Epoch: 3289 mean train loss:  3.55113007e-05, mean val. loss:  1.18975520e+00\n",
      "Epoch: 3290 mean train loss:  3.34315118e-05, mean val. loss:  1.18996346e+00\n",
      "Epoch: 3291 mean train loss:  3.50344635e-05, mean val. loss:  1.19015455e+00\n",
      "Epoch: 3292 mean train loss:  3.51506751e-05, mean val. loss:  1.19034648e+00\n",
      "Epoch: 3293 mean train loss:  3.42531421e-05, mean val. loss:  1.19054067e+00\n",
      "Epoch: 3294 mean train loss:  3.45759327e-05, mean val. loss:  1.19073498e+00\n",
      "Epoch: 3295 mean train loss:  3.45403678e-05, mean val. loss:  1.19093537e+00\n",
      "Epoch: 3296 mean train loss:  3.47386813e-05, mean val. loss:  1.19113457e+00\n",
      "Epoch: 3297 mean train loss:  3.45179869e-05, mean val. loss:  1.19133723e+00\n",
      "Epoch: 3298 mean train loss:  3.45657172e-05, mean val. loss:  1.19154823e+00\n",
      "Epoch: 3299 mean train loss:  3.41696723e-05, mean val. loss:  1.19178569e+00\n",
      "Epoch: 3300 mean train loss:  3.42655403e-05, mean val. loss:  1.19202876e+00\n",
      "Epoch: 3301 mean train loss:  3.50602495e-05, mean val. loss:  1.19226086e+00\n",
      "Epoch: 3302 mean train loss:  3.53142386e-05, mean val. loss:  1.19248688e+00\n",
      "Epoch: 3303 mean train loss:  3.49900220e-05, mean val. loss:  1.19269812e+00\n",
      "Epoch: 3304 mean train loss:  3.47124296e-05, mean val. loss:  1.19292212e+00\n",
      "Epoch: 3305 mean train loss:  3.50656919e-05, mean val. loss:  1.19314754e+00\n",
      "Epoch: 3306 mean train loss:  3.47406894e-05, mean val. loss:  1.19339323e+00\n",
      "Epoch: 3307 mean train loss:  3.49639449e-05, mean val. loss:  1.19364464e+00\n",
      "Epoch: 3308 mean train loss:  3.46036686e-05, mean val. loss:  1.19391346e+00\n",
      "Epoch: 3309 mean train loss:  3.48781759e-05, mean val. loss:  1.19418275e+00\n",
      "Epoch: 3310 mean train loss:  3.50870832e-05, mean val. loss:  1.19444120e+00\n",
      "Epoch: 3311 mean train loss:  3.50080663e-05, mean val. loss:  1.19469333e+00\n",
      "Epoch: 3312 mean train loss:  3.46681045e-05, mean val. loss:  1.19495475e+00\n",
      "Epoch: 3313 mean train loss:  3.44884756e-05, mean val. loss:  1.19522238e+00\n",
      "Epoch: 3314 mean train loss:  3.46018933e-05, mean val. loss:  1.19550562e+00\n",
      "Epoch: 3315 mean train loss:  3.52012867e-05, mean val. loss:  1.19577742e+00\n",
      "Epoch: 3316 mean train loss:  3.51355702e-05, mean val. loss:  1.19604075e+00\n",
      "Epoch: 3317 mean train loss:  3.52407515e-05, mean val. loss:  1.19630909e+00\n",
      "Epoch: 3318 mean train loss:  3.52703501e-05, mean val. loss:  1.19657147e+00\n",
      "Epoch: 3319 mean train loss:  3.35055520e-05, mean val. loss:  1.19685590e+00\n",
      "Epoch: 3320 mean train loss:  3.56208475e-05, mean val. loss:  1.19712484e+00\n",
      "Epoch: 3321 mean train loss:  3.58063262e-05, mean val. loss:  1.19736338e+00\n",
      "Epoch: 3322 mean train loss:  3.46062589e-05, mean val. loss:  1.19759798e+00\n",
      "Epoch: 3323 mean train loss:  3.43960710e-05, mean val. loss:  1.19782817e+00\n",
      "Epoch: 3324 mean train loss:  3.48340836e-05, mean val. loss:  1.19804883e+00\n",
      "Epoch: 3325 mean train loss:  3.52360366e-05, mean val. loss:  1.19825625e+00\n",
      "Epoch: 3326 mean train loss:  3.43016873e-05, mean val. loss:  1.19845116e+00\n",
      "Epoch: 3327 mean train loss:  3.47160094e-05, mean val. loss:  1.19867110e+00\n",
      "Epoch: 3328 mean train loss:  3.46422312e-05, mean val. loss:  1.19887638e+00\n",
      "Epoch: 3329 mean train loss:  3.46084125e-05, mean val. loss:  1.19910324e+00\n",
      "Epoch: 3330 mean train loss:  3.51879862e-05, mean val. loss:  1.19930947e+00\n",
      "Epoch: 3331 mean train loss:  3.52190982e-05, mean val. loss:  1.19950557e+00\n",
      "Epoch: 3332 mean train loss:  3.45562003e-05, mean val. loss:  1.19968557e+00\n",
      "Epoch: 3333 mean train loss:  3.49870825e-05, mean val. loss:  1.19986296e+00\n",
      "Epoch: 3334 mean train loss:  3.48350441e-05, mean val. loss:  1.20004439e+00\n",
      "Epoch: 3335 mean train loss:  3.44896980e-05, mean val. loss:  1.20022511e+00\n",
      "Epoch: 3336 mean train loss:  3.46357119e-05, mean val. loss:  1.20039725e+00\n",
      "Epoch: 3337 mean train loss:  3.59136029e-05, mean val. loss:  1.20054245e+00\n",
      "Epoch: 3338 mean train loss:  3.49564361e-05, mean val. loss:  1.20068598e+00\n",
      "Epoch: 3339 mean train loss:  3.46483721e-05, mean val. loss:  1.20082808e+00\n",
      "Epoch: 3340 mean train loss:  3.46291927e-05, mean val. loss:  1.20098078e+00\n",
      "Epoch: 3341 mean train loss:  3.38562822e-05, mean val. loss:  1.20114708e+00\n",
      "Epoch: 3342 mean train loss:  3.47970927e-05, mean val. loss:  1.20131481e+00\n",
      "Epoch: 3343 mean train loss:  3.50383343e-05, mean val. loss:  1.20145750e+00\n",
      "Epoch: 3344 mean train loss:  3.51791095e-05, mean val. loss:  1.20160055e+00\n",
      "Epoch: 3345 mean train loss:  3.44105647e-05, mean val. loss:  1.20174360e+00\n",
      "Epoch: 3346 mean train loss:  3.45476146e-05, mean val. loss:  1.20188069e+00\n",
      "Epoch: 3347 mean train loss:  3.54156364e-05, mean val. loss:  1.20201480e+00\n",
      "Epoch: 3348 mean train loss:  3.40417319e-05, mean val. loss:  1.20216787e+00\n",
      "Epoch: 3349 mean train loss:  3.41139676e-05, mean val. loss:  1.20233488e+00\n",
      "Epoch: 3350 mean train loss:  3.45784647e-05, mean val. loss:  1.20251489e+00\n",
      "Epoch: 3351 mean train loss:  3.39563121e-05, mean val. loss:  1.20270538e+00\n",
      "Epoch: 3352 mean train loss:  3.52724455e-05, mean val. loss:  1.20288920e+00\n",
      "Epoch: 3353 mean train loss:  3.47217428e-05, mean val. loss:  1.20306432e+00\n",
      "Epoch: 3354 mean train loss:  3.45840235e-05, mean val. loss:  1.20322692e+00\n",
      "Epoch: 3355 mean train loss:  3.48975882e-05, mean val. loss:  1.20338690e+00\n",
      "Epoch: 3356 mean train loss:  3.44553846e-05, mean val. loss:  1.20355570e+00\n",
      "Epoch: 3357 mean train loss:  3.48094909e-05, mean val. loss:  1.20372212e+00\n",
      "Epoch: 3358 mean train loss:  3.45196750e-05, mean val. loss:  1.20390046e+00\n",
      "Epoch: 3359 mean train loss:  3.53308569e-05, mean val. loss:  1.20406806e+00\n",
      "Epoch: 3360 mean train loss:  3.48711619e-05, mean val. loss:  1.20423305e+00\n",
      "Epoch: 3361 mean train loss:  3.41348932e-05, mean val. loss:  1.20440805e+00\n",
      "Epoch: 3362 mean train loss:  3.47596942e-05, mean val. loss:  1.20456970e+00\n",
      "Epoch: 3363 mean train loss:  3.49371112e-05, mean val. loss:  1.20474744e+00\n",
      "Epoch: 3364 mean train loss:  3.42614949e-05, mean val. loss:  1.20493102e+00\n",
      "Epoch: 3365 mean train loss:  3.51957860e-05, mean val. loss:  1.20510471e+00\n",
      "Epoch: 3366 mean train loss:  3.48612375e-05, mean val. loss:  1.20529032e+00\n",
      "Epoch: 3367 mean train loss:  3.53175565e-05, mean val. loss:  1.20547128e+00\n",
      "Epoch: 3368 mean train loss:  3.44541331e-05, mean val. loss:  1.20565236e+00\n",
      "Epoch: 3369 mean train loss:  3.48325702e-05, mean val. loss:  1.20583928e+00\n",
      "Epoch: 3370 mean train loss:  3.50146147e-05, mean val. loss:  1.20602000e+00\n",
      "Epoch: 3371 mean train loss:  3.44733999e-05, mean val. loss:  1.20618951e+00\n",
      "Epoch: 3372 mean train loss:  3.53514333e-05, mean val. loss:  1.20635331e+00\n",
      "Epoch: 3373 mean train loss:  3.38691170e-05, mean val. loss:  1.20651507e+00\n",
      "Epoch: 3374 mean train loss:  3.46725865e-05, mean val. loss:  1.20667398e+00\n",
      "Epoch: 3375 mean train loss:  3.50242190e-05, mean val. loss:  1.20683682e+00\n",
      "Epoch: 3376 mean train loss:  3.45149310e-05, mean val. loss:  1.20698559e+00\n",
      "Epoch: 3377 mean train loss:  3.47268360e-05, mean val. loss:  1.20713949e+00\n",
      "Epoch: 3378 mean train loss:  3.48770409e-05, mean val. loss:  1.20728612e+00\n",
      "Epoch: 3379 mean train loss:  3.53878131e-05, mean val. loss:  1.20743692e+00\n",
      "Epoch: 3380 mean train loss:  3.42862331e-05, mean val. loss:  1.20759177e+00\n",
      "Epoch: 3381 mean train loss:  3.47419991e-05, mean val. loss:  1.20774603e+00\n",
      "Epoch: 3382 mean train loss:  3.46768938e-05, mean val. loss:  1.20789993e+00\n",
      "Epoch: 3383 mean train loss:  3.47197638e-05, mean val. loss:  1.20803714e+00\n",
      "Epoch: 3384 mean train loss:  3.47515743e-05, mean val. loss:  1.20817149e+00\n",
      "Epoch: 3385 mean train loss:  3.53049254e-05, mean val. loss:  1.20831072e+00\n",
      "Epoch: 3386 mean train loss:  3.47228779e-05, mean val. loss:  1.20844746e+00\n",
      "Epoch: 3387 mean train loss:  3.51665658e-05, mean val. loss:  1.20858204e+00\n",
      "Epoch: 3388 mean train loss:  3.50399932e-05, mean val. loss:  1.20872056e+00\n",
      "Epoch: 3389 mean train loss:  3.46908055e-05, mean val. loss:  1.20885885e+00\n",
      "Epoch: 3390 mean train loss:  3.52980569e-05, mean val. loss:  1.20899653e+00\n",
      "Epoch: 3391 mean train loss:  3.46837041e-05, mean val. loss:  1.20912790e+00\n",
      "Epoch: 3392 mean train loss:  3.37052043e-05, mean val. loss:  1.20926511e+00\n",
      "Epoch: 3393 mean train loss:  3.37148376e-05, mean val. loss:  1.20942724e+00\n",
      "Epoch: 3394 mean train loss:  3.40926927e-05, mean val. loss:  1.20960343e+00\n",
      "Epoch: 3395 mean train loss:  3.41096602e-05, mean val. loss:  1.20980048e+00\n",
      "Epoch: 3396 mean train loss:  3.38282262e-05, mean val. loss:  1.21000230e+00\n",
      "Epoch: 3397 mean train loss:  3.42091080e-05, mean val. loss:  1.21020615e+00\n",
      "Epoch: 3398 mean train loss:  3.52647039e-05, mean val. loss:  1.21040213e+00\n",
      "Epoch: 3399 mean train loss:  3.48413305e-05, mean val. loss:  1.21060038e+00\n",
      "Epoch: 3400 mean train loss:  3.46463930e-05, mean val. loss:  1.21080637e+00\n",
      "Epoch: 3401 mean train loss:  3.45539011e-05, mean val. loss:  1.21101356e+00\n",
      "Epoch: 3402 mean train loss:  3.41891719e-05, mean val. loss:  1.21122658e+00\n",
      "Epoch: 3403 mean train loss:  3.42474377e-05, mean val. loss:  1.21146894e+00\n",
      "Epoch: 3404 mean train loss:  3.49911861e-05, mean val. loss:  1.21170056e+00\n",
      "Epoch: 3405 mean train loss:  3.39479593e-05, mean val. loss:  1.21193361e+00\n",
      "Epoch: 3406 mean train loss:  3.39672260e-05, mean val. loss:  1.21218491e+00\n",
      "Epoch: 3407 mean train loss:  3.51380149e-05, mean val. loss:  1.21242213e+00\n",
      "Epoch: 3408 mean train loss:  3.40242987e-05, mean val. loss:  1.21268141e+00\n",
      "Epoch: 3409 mean train loss:  3.46051238e-05, mean val. loss:  1.21293366e+00\n",
      "Epoch: 3410 mean train loss:  3.43297434e-05, mean val. loss:  1.21317661e+00\n",
      "Epoch: 3411 mean train loss:  3.42361163e-05, mean val. loss:  1.21342993e+00\n",
      "Epoch: 3412 mean train loss:  3.46371962e-05, mean val. loss:  1.21367157e+00\n",
      "Epoch: 3413 mean train loss:  3.37820093e-05, mean val. loss:  1.21391106e+00\n",
      "Epoch: 3414 mean train loss:  3.49530019e-05, mean val. loss:  1.21413898e+00\n",
      "Epoch: 3415 mean train loss:  3.39734252e-05, mean val. loss:  1.21437156e+00\n",
      "Epoch: 3416 mean train loss:  3.48364410e-05, mean val. loss:  1.21460521e+00\n",
      "Epoch: 3417 mean train loss:  3.51904891e-05, mean val. loss:  1.21484566e+00\n",
      "Epoch: 3418 mean train loss:  3.49479378e-05, mean val. loss:  1.21508348e+00\n",
      "Epoch: 3419 mean train loss:  3.47479654e-05, mean val. loss:  1.21531880e+00\n",
      "Epoch: 3420 mean train loss:  3.50732880e-05, mean val. loss:  1.21556163e+00\n",
      "Epoch: 3421 mean train loss:  3.44552391e-05, mean val. loss:  1.21580398e+00\n",
      "Epoch: 3422 mean train loss:  3.48687754e-05, mean val. loss:  1.21605062e+00\n",
      "Epoch: 3423 mean train loss:  3.40500264e-05, mean val. loss:  1.21629393e+00\n",
      "Epoch: 3424 mean train loss:  3.34767101e-05, mean val. loss:  1.21655989e+00\n",
      "Epoch: 3425 mean train loss:  3.50563787e-05, mean val. loss:  1.21682441e+00\n",
      "Epoch: 3426 mean train loss:  3.51538183e-05, mean val. loss:  1.21707463e+00\n",
      "Epoch: 3427 mean train loss:  3.43243883e-05, mean val. loss:  1.21731973e+00\n",
      "Epoch: 3428 mean train loss:  3.48542817e-05, mean val. loss:  1.21755052e+00\n",
      "Epoch: 3429 mean train loss:  3.48366448e-05, mean val. loss:  1.21777630e+00\n",
      "Epoch: 3430 mean train loss:  3.39360267e-05, mean val. loss:  1.21800590e+00\n",
      "Epoch: 3431 mean train loss:  3.44973232e-05, mean val. loss:  1.21823180e+00\n",
      "Epoch: 3432 mean train loss:  3.44626606e-05, mean val. loss:  1.21844602e+00\n",
      "Epoch: 3433 mean train loss:  3.53944488e-05, mean val. loss:  1.21864069e+00\n",
      "Epoch: 3434 mean train loss:  3.48697940e-05, mean val. loss:  1.21883798e+00\n",
      "Epoch: 3435 mean train loss:  3.52696807e-05, mean val. loss:  1.21902108e+00\n",
      "Epoch: 3436 mean train loss:  3.49682523e-05, mean val. loss:  1.21917820e+00\n",
      "Epoch: 3437 mean train loss:  3.46360612e-05, mean val. loss:  1.21934426e+00\n",
      "Epoch: 3438 mean train loss:  3.39684775e-05, mean val. loss:  1.21949577e+00\n",
      "Epoch: 3439 mean train loss:  3.46449378e-05, mean val. loss:  1.21965683e+00\n",
      "Epoch: 3440 mean train loss:  3.40184488e-05, mean val. loss:  1.21981931e+00\n",
      "Epoch: 3441 mean train loss:  3.52602510e-05, mean val. loss:  1.21996009e+00\n",
      "Epoch: 3442 mean train loss:  3.50113842e-05, mean val. loss:  1.22010350e+00\n",
      "Epoch: 3443 mean train loss:  3.41861742e-05, mean val. loss:  1.22024679e+00\n",
      "Epoch: 3444 mean train loss:  3.46733432e-05, mean val. loss:  1.22039342e+00\n",
      "Epoch: 3445 mean train loss:  3.44492728e-05, mean val. loss:  1.22053170e+00\n",
      "Epoch: 3446 mean train loss:  3.46139423e-05, mean val. loss:  1.22067332e+00\n",
      "Epoch: 3447 mean train loss:  3.45325388e-05, mean val. loss:  1.22081769e+00\n",
      "Epoch: 3448 mean train loss:  3.52368806e-05, mean val. loss:  1.22095895e+00\n",
      "Epoch: 3449 mean train loss:  3.54894728e-05, mean val. loss:  1.22108555e+00\n",
      "Epoch: 3450 mean train loss:  3.43349238e-05, mean val. loss:  1.22121596e+00\n",
      "Epoch: 3451 mean train loss:  3.47084133e-05, mean val. loss:  1.22134638e+00\n",
      "Epoch: 3452 mean train loss:  3.48425529e-05, mean val. loss:  1.22147238e+00\n",
      "Epoch: 3453 mean train loss:  3.42718267e-05, mean val. loss:  1.22159123e+00\n",
      "Epoch: 3454 mean train loss:  3.46046581e-05, mean val. loss:  1.22172105e+00\n",
      "Epoch: 3455 mean train loss:  3.41887353e-05, mean val. loss:  1.22186387e+00\n",
      "Epoch: 3456 mean train loss:  3.50077753e-05, mean val. loss:  1.22201455e+00\n",
      "Epoch: 3457 mean train loss:  3.39819817e-05, mean val. loss:  1.22216523e+00\n",
      "Epoch: 3458 mean train loss:  3.47927271e-05, mean val. loss:  1.22231495e+00\n",
      "Epoch: 3459 mean train loss:  3.42216226e-05, mean val. loss:  1.22248030e+00\n",
      "Epoch: 3460 mean train loss:  3.45321023e-05, mean val. loss:  1.22265506e+00\n",
      "Epoch: 3461 mean train loss:  3.41715640e-05, mean val. loss:  1.22282887e+00\n",
      "Epoch: 3462 mean train loss:  3.43122811e-05, mean val. loss:  1.22301447e+00\n",
      "Epoch: 3463 mean train loss:  3.41798295e-05, mean val. loss:  1.22321272e+00\n",
      "Epoch: 3464 mean train loss:  3.36176308e-05, mean val. loss:  1.22343075e+00\n",
      "Epoch: 3465 mean train loss:  3.43538995e-05, mean val. loss:  1.22365952e+00\n",
      "Epoch: 3466 mean train loss:  3.44500295e-05, mean val. loss:  1.22389054e+00\n",
      "Epoch: 3467 mean train loss:  3.40242404e-05, mean val. loss:  1.22413826e+00\n",
      "Epoch: 3468 mean train loss:  3.41663836e-05, mean val. loss:  1.22440577e+00\n",
      "Epoch: 3469 mean train loss:  3.34252545e-05, mean val. loss:  1.22468328e+00\n",
      "Epoch: 3470 mean train loss:  3.47835594e-05, mean val. loss:  1.22495580e+00\n",
      "Epoch: 3471 mean train loss:  3.34209180e-05, mean val. loss:  1.22523201e+00\n",
      "Epoch: 3472 mean train loss:  3.42029671e-05, mean val. loss:  1.22551382e+00\n",
      "Epoch: 3473 mean train loss:  3.49235197e-05, mean val. loss:  1.22579765e+00\n",
      "Epoch: 3474 mean train loss:  3.56693927e-05, mean val. loss:  1.22606719e+00\n",
      "Epoch: 3475 mean train loss:  3.47231398e-05, mean val. loss:  1.22632563e+00\n",
      "Epoch: 3476 mean train loss:  3.35419027e-05, mean val. loss:  1.22659123e+00\n",
      "Epoch: 3477 mean train loss:  3.41651612e-05, mean val. loss:  1.22685361e+00\n",
      "Epoch: 3478 mean train loss:  3.37101810e-05, mean val. loss:  1.22712827e+00\n",
      "Epoch: 3479 mean train loss:  3.48200556e-05, mean val. loss:  1.22739112e+00\n",
      "Epoch: 3480 mean train loss:  3.39521212e-05, mean val. loss:  1.22766185e+00\n",
      "Epoch: 3481 mean train loss:  3.37579404e-05, mean val. loss:  1.22795212e+00\n",
      "Epoch: 3482 mean train loss:  3.45415319e-05, mean val. loss:  1.22822940e+00\n",
      "Epoch: 3483 mean train loss:  3.36181256e-05, mean val. loss:  1.22850192e+00\n",
      "Epoch: 3484 mean train loss:  3.44523578e-05, mean val. loss:  1.22878468e+00\n",
      "Epoch: 3485 mean train loss:  3.45919980e-05, mean val. loss:  1.22905147e+00\n",
      "Epoch: 3486 mean train loss:  3.43969150e-05, mean val. loss:  1.22931409e+00\n",
      "Epoch: 3487 mean train loss:  3.42022686e-05, mean val. loss:  1.22957695e+00\n",
      "Epoch: 3488 mean train loss:  3.47916503e-05, mean val. loss:  1.22980857e+00\n",
      "Epoch: 3489 mean train loss:  3.45017761e-05, mean val. loss:  1.23002076e+00\n",
      "Epoch: 3490 mean train loss:  3.44173168e-05, mean val. loss:  1.23022747e+00\n",
      "Epoch: 3491 mean train loss:  3.38185637e-05, mean val. loss:  1.23043859e+00\n",
      "Epoch: 3492 mean train loss:  3.42371059e-05, mean val. loss:  1.23064184e+00\n",
      "Epoch: 3493 mean train loss:  3.45984008e-05, mean val. loss:  1.23084295e+00\n",
      "Epoch: 3494 mean train loss:  3.43310530e-05, mean val. loss:  1.23103917e+00\n",
      "Epoch: 3495 mean train loss:  3.45149892e-05, mean val. loss:  1.23123074e+00\n",
      "Epoch: 3496 mean train loss:  3.41362902e-05, mean val. loss:  1.23141217e+00\n",
      "Epoch: 3497 mean train loss:  3.34909128e-05, mean val. loss:  1.23159409e+00\n",
      "Epoch: 3498 mean train loss:  3.46930756e-05, mean val. loss:  1.23177218e+00\n",
      "Epoch: 3499 mean train loss:  3.54224758e-05, mean val. loss:  1.23192811e+00\n",
      "Epoch: 3500 mean train loss:  3.37926613e-05, mean val. loss:  1.23208261e+00\n",
      "Epoch: 3501 mean train loss:  3.45237204e-05, mean val. loss:  1.23224163e+00\n",
      "Epoch: 3502 mean train loss:  3.38242389e-05, mean val. loss:  1.23240888e+00\n",
      "Epoch: 3503 mean train loss:  3.34164361e-05, mean val. loss:  1.23258162e+00\n",
      "Epoch: 3504 mean train loss:  3.39793041e-05, mean val. loss:  1.23276091e+00\n",
      "Epoch: 3505 mean train loss:  3.34736833e-05, mean val. loss:  1.23295951e+00\n",
      "Epoch: 3506 mean train loss:  3.42551502e-05, mean val. loss:  1.23315477e+00\n",
      "Epoch: 3507 mean train loss:  3.42583226e-05, mean val. loss:  1.23333454e+00\n",
      "Epoch: 3508 mean train loss:  3.42500862e-05, mean val. loss:  1.23352551e+00\n",
      "Epoch: 3509 mean train loss:  3.39517428e-05, mean val. loss:  1.23371542e+00\n",
      "Epoch: 3510 mean train loss:  3.38701648e-05, mean val. loss:  1.23390496e+00\n",
      "Epoch: 3511 mean train loss:  3.38004320e-05, mean val. loss:  1.23411620e+00\n",
      "Epoch: 3512 mean train loss:  3.48206959e-05, mean val. loss:  1.23432243e+00\n",
      "Epoch: 3513 mean train loss:  3.33843636e-05, mean val. loss:  1.23454034e+00\n",
      "Epoch: 3514 mean train loss:  3.41425184e-05, mean val. loss:  1.23477304e+00\n",
      "Epoch: 3515 mean train loss:  3.36256053e-05, mean val. loss:  1.23500586e+00\n",
      "Epoch: 3516 mean train loss:  3.39128892e-05, mean val. loss:  1.23523164e+00\n",
      "Epoch: 3517 mean train loss:  3.42846906e-05, mean val. loss:  1.23544967e+00\n",
      "Epoch: 3518 mean train loss:  3.34654469e-05, mean val. loss:  1.23566520e+00\n",
      "Epoch: 3519 mean train loss:  3.39278195e-05, mean val. loss:  1.23588884e+00\n",
      "Epoch: 3520 mean train loss:  3.46626039e-05, mean val. loss:  1.23610365e+00\n",
      "Epoch: 3521 mean train loss:  3.47559107e-05, mean val. loss:  1.23630261e+00\n",
      "Epoch: 3522 mean train loss:  3.40171973e-05, mean val. loss:  1.23650682e+00\n",
      "Epoch: 3523 mean train loss:  3.44530272e-05, mean val. loss:  1.23670423e+00\n",
      "Epoch: 3524 mean train loss:  3.42381536e-05, mean val. loss:  1.23690665e+00\n",
      "Epoch: 3525 mean train loss:  3.43110005e-05, mean val. loss:  1.23711121e+00\n",
      "Epoch: 3526 mean train loss:  3.48887988e-05, mean val. loss:  1.23729789e+00\n",
      "Epoch: 3527 mean train loss:  3.40651022e-05, mean val. loss:  1.23748064e+00\n",
      "Epoch: 3528 mean train loss:  3.39792168e-05, mean val. loss:  1.23765314e+00\n",
      "Epoch: 3529 mean train loss:  3.42030253e-05, mean val. loss:  1.23784292e+00\n",
      "Epoch: 3530 mean train loss:  3.37478123e-05, mean val. loss:  1.23802638e+00\n",
      "Epoch: 3531 mean train loss:  3.41307314e-05, mean val. loss:  1.23819935e+00\n",
      "Epoch: 3532 mean train loss:  3.44454602e-05, mean val. loss:  1.23838210e+00\n",
      "Epoch: 3533 mean train loss:  3.38984246e-05, mean val. loss:  1.23857152e+00\n",
      "Epoch: 3534 mean train loss:  3.42797430e-05, mean val. loss:  1.23877585e+00\n",
      "Epoch: 3535 mean train loss:  3.48490139e-05, mean val. loss:  1.23896313e+00\n",
      "Epoch: 3536 mean train loss:  3.40679544e-05, mean val. loss:  1.23915446e+00\n",
      "Epoch: 3537 mean train loss:  3.47617024e-05, mean val. loss:  1.23933101e+00\n",
      "Epoch: 3538 mean train loss:  3.46579473e-05, mean val. loss:  1.23950517e+00\n",
      "Epoch: 3539 mean train loss:  3.45840526e-05, mean val. loss:  1.23968077e+00\n",
      "Epoch: 3540 mean train loss:  3.45818116e-05, mean val. loss:  1.23985291e+00\n",
      "Epoch: 3541 mean train loss:  3.32308700e-05, mean val. loss:  1.24003553e+00\n",
      "Epoch: 3542 mean train loss:  3.41712148e-05, mean val. loss:  1.24022353e+00\n",
      "Epoch: 3543 mean train loss:  3.40561091e-05, mean val. loss:  1.24040461e+00\n",
      "Epoch: 3544 mean train loss:  3.39226099e-05, mean val. loss:  1.24057639e+00\n",
      "Epoch: 3545 mean train loss:  3.42976127e-05, mean val. loss:  1.24074697e+00\n",
      "Epoch: 3546 mean train loss:  3.47330351e-05, mean val. loss:  1.24090362e+00\n",
      "Epoch: 3547 mean train loss:  3.44295404e-05, mean val. loss:  1.24105763e+00\n",
      "Epoch: 3548 mean train loss:  3.41066334e-05, mean val. loss:  1.24120748e+00\n",
      "Epoch: 3549 mean train loss:  3.34432116e-05, mean val. loss:  1.24137235e+00\n",
      "Epoch: 3550 mean train loss:  3.49548063e-05, mean val. loss:  1.24152517e+00\n",
      "Epoch: 3551 mean train loss:  3.36588128e-05, mean val. loss:  1.24169397e+00\n",
      "Epoch: 3552 mean train loss:  3.40219704e-05, mean val. loss:  1.24188614e+00\n",
      "Epoch: 3553 mean train loss:  3.40120459e-05, mean val. loss:  1.24207461e+00\n",
      "Epoch: 3554 mean train loss:  3.40442930e-05, mean val. loss:  1.24227166e+00\n",
      "Epoch: 3555 mean train loss:  3.35015939e-05, mean val. loss:  1.24247193e+00\n",
      "Epoch: 3556 mean train loss:  3.41157720e-05, mean val. loss:  1.24267054e+00\n",
      "Epoch: 3557 mean train loss:  3.36942321e-05, mean val. loss:  1.24288666e+00\n",
      "Epoch: 3558 mean train loss:  3.38985701e-05, mean val. loss:  1.24310946e+00\n",
      "Epoch: 3559 mean train loss:  3.41736304e-05, mean val. loss:  1.24334288e+00\n",
      "Epoch: 3560 mean train loss:  3.42666171e-05, mean val. loss:  1.24358499e+00\n",
      "Epoch: 3561 mean train loss:  3.37635865e-05, mean val. loss:  1.24383903e+00\n",
      "Epoch: 3562 mean train loss:  3.43885622e-05, mean val. loss:  1.24408734e+00\n",
      "Epoch: 3563 mean train loss:  3.46327433e-05, mean val. loss:  1.24433160e+00\n",
      "Epoch: 3564 mean train loss:  3.37369856e-05, mean val. loss:  1.24458110e+00\n",
      "Epoch: 3565 mean train loss:  3.36183002e-05, mean val. loss:  1.24484921e+00\n",
      "Epoch: 3566 mean train loss:  3.44668806e-05, mean val. loss:  1.24510646e+00\n",
      "Epoch: 3567 mean train loss:  3.41153354e-05, mean val. loss:  1.24536729e+00\n",
      "Epoch: 3568 mean train loss:  3.38490354e-05, mean val. loss:  1.24562871e+00\n",
      "Epoch: 3569 mean train loss:  3.46888264e-05, mean val. loss:  1.24588013e+00\n",
      "Epoch: 3570 mean train loss:  3.37523525e-05, mean val. loss:  1.24612463e+00\n",
      "Epoch: 3571 mean train loss:  3.45889712e-05, mean val. loss:  1.24637616e+00\n",
      "Epoch: 3572 mean train loss:  3.39955441e-05, mean val. loss:  1.24663198e+00\n",
      "Epoch: 3573 mean train loss:  3.46624583e-05, mean val. loss:  1.24686837e+00\n",
      "Epoch: 3574 mean train loss:  3.46313464e-05, mean val. loss:  1.24708951e+00\n",
      "Epoch: 3575 mean train loss:  3.44625150e-05, mean val. loss:  1.24729061e+00\n",
      "Epoch: 3576 mean train loss:  3.37988604e-05, mean val. loss:  1.24750221e+00\n",
      "Epoch: 3577 mean train loss:  3.43958091e-05, mean val. loss:  1.24771082e+00\n",
      "Epoch: 3578 mean train loss:  3.39234539e-05, mean val. loss:  1.24793684e+00\n",
      "Epoch: 3579 mean train loss:  3.38307873e-05, mean val. loss:  1.24815488e+00\n",
      "Epoch: 3580 mean train loss:  3.41073610e-05, mean val. loss:  1.24836516e+00\n",
      "Epoch: 3581 mean train loss:  3.42594285e-05, mean val. loss:  1.24857342e+00\n",
      "Epoch: 3582 mean train loss:  3.34985962e-05, mean val. loss:  1.24879229e+00\n",
      "Epoch: 3583 mean train loss:  3.41739506e-05, mean val. loss:  1.24900866e+00\n",
      "Epoch: 3584 mean train loss:  3.36924568e-05, mean val. loss:  1.24922717e+00\n",
      "Epoch: 3585 mean train loss:  3.42670537e-05, mean val. loss:  1.24943566e+00\n",
      "Epoch: 3586 mean train loss:  3.40959523e-05, mean val. loss:  1.24965453e+00\n",
      "Epoch: 3587 mean train loss:  3.40894912e-05, mean val. loss:  1.24986255e+00\n",
      "Epoch: 3588 mean train loss:  3.43802676e-05, mean val. loss:  1.25006473e+00\n",
      "Epoch: 3589 mean train loss:  3.40876577e-05, mean val. loss:  1.25026870e+00\n",
      "Epoch: 3590 mean train loss:  3.30462644e-05, mean val. loss:  1.25047612e+00\n",
      "Epoch: 3591 mean train loss:  3.42979329e-05, mean val. loss:  1.25066519e+00\n",
      "Epoch: 3592 mean train loss:  3.36943194e-05, mean val. loss:  1.25085318e+00\n",
      "Epoch: 3593 mean train loss:  3.43297434e-05, mean val. loss:  1.25103521e+00\n",
      "Epoch: 3594 mean train loss:  3.42103303e-05, mean val. loss:  1.25121963e+00\n",
      "Epoch: 3595 mean train loss:  3.46503803e-05, mean val. loss:  1.25138068e+00\n",
      "Epoch: 3596 mean train loss:  3.35339282e-05, mean val. loss:  1.25156236e+00\n",
      "Epoch: 3597 mean train loss:  3.43160063e-05, mean val. loss:  1.25173724e+00\n",
      "Epoch: 3598 mean train loss:  3.41497362e-05, mean val. loss:  1.25191164e+00\n",
      "Epoch: 3599 mean train loss:  3.45865556e-05, mean val. loss:  1.25207901e+00\n",
      "Epoch: 3600 mean train loss:  3.40100960e-05, mean val. loss:  1.25225151e+00\n",
      "Epoch: 3601 mean train loss:  3.39767430e-05, mean val. loss:  1.25242364e+00\n",
      "Epoch: 3602 mean train loss:  3.48515459e-05, mean val. loss:  1.25258470e+00\n",
      "Epoch: 3603 mean train loss:  3.37377132e-05, mean val. loss:  1.25273919e+00\n",
      "Epoch: 3604 mean train loss:  3.42524436e-05, mean val. loss:  1.25291288e+00\n",
      "Epoch: 3605 mean train loss:  3.40320112e-05, mean val. loss:  1.25308716e+00\n",
      "Epoch: 3606 mean train loss:  3.41183331e-05, mean val. loss:  1.25325012e+00\n",
      "Epoch: 3607 mean train loss:  3.41362902e-05, mean val. loss:  1.25341487e+00\n",
      "Epoch: 3608 mean train loss:  3.34971119e-05, mean val. loss:  1.25358224e+00\n",
      "Epoch: 3609 mean train loss:  3.42764542e-05, mean val. loss:  1.25373757e+00\n",
      "Epoch: 3610 mean train loss:  3.40863480e-05, mean val. loss:  1.25389981e+00\n",
      "Epoch: 3611 mean train loss:  3.34807555e-05, mean val. loss:  1.25408626e+00\n",
      "Epoch: 3612 mean train loss:  3.49196198e-05, mean val. loss:  1.25426185e+00\n",
      "Epoch: 3613 mean train loss:  3.37706588e-05, mean val. loss:  1.25445223e+00\n",
      "Epoch: 3614 mean train loss:  3.40359984e-05, mean val. loss:  1.25463748e+00\n",
      "Epoch: 3615 mean train loss:  3.40047700e-05, mean val. loss:  1.25483310e+00\n",
      "Epoch: 3616 mean train loss:  3.34253127e-05, mean val. loss:  1.25501883e+00\n",
      "Epoch: 3617 mean train loss:  3.31835472e-05, mean val. loss:  1.25522733e+00\n",
      "Epoch: 3618 mean train loss:  3.37412930e-05, mean val. loss:  1.25544012e+00\n",
      "Epoch: 3619 mean train loss:  3.32665513e-05, mean val. loss:  1.25565350e+00\n",
      "Epoch: 3620 mean train loss:  3.37497040e-05, mean val. loss:  1.25586700e+00\n",
      "Epoch: 3621 mean train loss:  3.35224613e-05, mean val. loss:  1.25608802e+00\n",
      "Epoch: 3622 mean train loss:  3.37918755e-05, mean val. loss:  1.25631559e+00\n",
      "Epoch: 3623 mean train loss:  3.41614941e-05, mean val. loss:  1.25654471e+00\n",
      "Epoch: 3624 mean train loss:  3.33407952e-05, mean val. loss:  1.25678372e+00\n",
      "Epoch: 3625 mean train loss:  3.34888464e-05, mean val. loss:  1.25703490e+00\n",
      "Epoch: 3626 mean train loss:  3.41464765e-05, mean val. loss:  1.25726962e+00\n",
      "Epoch: 3627 mean train loss:  3.37741221e-05, mean val. loss:  1.25750768e+00\n",
      "Epoch: 3628 mean train loss:  3.41719133e-05, mean val. loss:  1.25774455e+00\n",
      "Epoch: 3629 mean train loss:  3.44233704e-05, mean val. loss:  1.25795937e+00\n",
      "Epoch: 3630 mean train loss:  3.37387901e-05, mean val. loss:  1.25819540e+00\n",
      "Epoch: 3631 mean train loss:  3.45220033e-05, mean val. loss:  1.25841117e+00\n",
      "Epoch: 3632 mean train loss:  3.41398118e-05, mean val. loss:  1.25862384e+00\n",
      "Epoch: 3633 mean train loss:  3.26059817e-05, mean val. loss:  1.25885606e+00\n",
      "Epoch: 3634 mean train loss:  3.39784310e-05, mean val. loss:  1.25909007e+00\n",
      "Epoch: 3635 mean train loss:  3.37632373e-05, mean val. loss:  1.25930691e+00\n",
      "Epoch: 3636 mean train loss:  3.32295313e-05, mean val. loss:  1.25952375e+00\n",
      "Epoch: 3637 mean train loss:  3.38521204e-05, mean val. loss:  1.25975299e+00\n",
      "Epoch: 3638 mean train loss:  3.34121578e-05, mean val. loss:  1.25998640e+00\n",
      "Epoch: 3639 mean train loss:  3.43851279e-05, mean val. loss:  1.26019859e+00\n",
      "Epoch: 3640 mean train loss:  3.37915844e-05, mean val. loss:  1.26040947e+00\n",
      "Epoch: 3641 mean train loss:  3.32537165e-05, mean val. loss:  1.26062739e+00\n",
      "Epoch: 3642 mean train loss:  3.41312552e-05, mean val. loss:  1.26086175e+00\n",
      "Epoch: 3643 mean train loss:  3.43090214e-05, mean val. loss:  1.26109195e+00\n",
      "Epoch: 3644 mean train loss:  3.30333132e-05, mean val. loss:  1.26133740e+00\n",
      "Epoch: 3645 mean train loss:  3.43272695e-05, mean val. loss:  1.26158202e+00\n",
      "Epoch: 3646 mean train loss:  3.44626606e-05, mean val. loss:  1.26180935e+00\n",
      "Epoch: 3647 mean train loss:  3.33671924e-05, mean val. loss:  1.26204693e+00\n",
      "Epoch: 3648 mean train loss:  3.34110518e-05, mean val. loss:  1.26226807e+00\n",
      "Epoch: 3649 mean train loss:  3.36280791e-05, mean val. loss:  1.26248372e+00\n",
      "Epoch: 3650 mean train loss:  3.50038172e-05, mean val. loss:  1.26267993e+00\n",
      "Epoch: 3651 mean train loss:  3.37608508e-05, mean val. loss:  1.26287532e+00\n",
      "Epoch: 3652 mean train loss:  3.43393767e-05, mean val. loss:  1.26306868e+00\n",
      "Epoch: 3653 mean train loss:  3.36559315e-05, mean val. loss:  1.26325095e+00\n",
      "Epoch: 3654 mean train loss:  3.43810825e-05, mean val. loss:  1.26344252e+00\n",
      "Epoch: 3655 mean train loss:  3.37062520e-05, mean val. loss:  1.26363206e+00\n",
      "Epoch: 3656 mean train loss:  3.41876876e-05, mean val. loss:  1.26381898e+00\n",
      "Epoch: 3657 mean train loss:  3.35877994e-05, mean val. loss:  1.26402140e+00\n",
      "Epoch: 3658 mean train loss:  3.44714208e-05, mean val. loss:  1.26422369e+00\n",
      "Epoch: 3659 mean train loss:  3.35845980e-05, mean val. loss:  1.26442301e+00\n",
      "Epoch: 3660 mean train loss:  3.38940590e-05, mean val. loss:  1.26462865e+00\n",
      "Epoch: 3661 mean train loss:  3.45604203e-05, mean val. loss:  1.26482284e+00\n",
      "Epoch: 3662 mean train loss:  3.33674252e-05, mean val. loss:  1.26502144e+00\n",
      "Epoch: 3663 mean train loss:  3.38465325e-05, mean val. loss:  1.26521194e+00\n",
      "Epoch: 3664 mean train loss:  3.30438197e-05, mean val. loss:  1.26540959e+00\n",
      "Epoch: 3665 mean train loss:  3.37689416e-05, mean val. loss:  1.26561630e+00\n",
      "Epoch: 3666 mean train loss:  3.47803580e-05, mean val. loss:  1.26580191e+00\n",
      "Epoch: 3667 mean train loss:  3.38032260e-05, mean val. loss:  1.26600409e+00\n",
      "Epoch: 3668 mean train loss:  3.37848614e-05, mean val. loss:  1.26620471e+00\n",
      "Epoch: 3669 mean train loss:  3.47377500e-05, mean val. loss:  1.26640773e+00\n",
      "Epoch: 3670 mean train loss:  3.32833151e-05, mean val. loss:  1.26660252e+00\n",
      "Epoch: 3671 mean train loss:  3.31332267e-05, mean val. loss:  1.26680791e+00\n",
      "Epoch: 3672 mean train loss:  3.38053505e-05, mean val. loss:  1.26701319e+00\n",
      "Epoch: 3673 mean train loss:  3.41636478e-05, mean val. loss:  1.26721728e+00\n",
      "Epoch: 3674 mean train loss:  3.40551196e-05, mean val. loss:  1.26742089e+00\n",
      "Epoch: 3675 mean train loss:  3.37361416e-05, mean val. loss:  1.26760471e+00\n",
      "Epoch: 3676 mean train loss:  3.41793057e-05, mean val. loss:  1.26777673e+00\n",
      "Epoch: 3677 mean train loss:  3.34990036e-05, mean val. loss:  1.26797700e+00\n",
      "Epoch: 3678 mean train loss:  3.39055841e-05, mean val. loss:  1.26816428e+00\n",
      "Epoch: 3679 mean train loss:  3.41327686e-05, mean val. loss:  1.26835859e+00\n",
      "Epoch: 3680 mean train loss:  3.41088744e-05, mean val. loss:  1.26853967e+00\n",
      "Epoch: 3681 mean train loss:  3.39624530e-05, mean val. loss:  1.26871812e+00\n",
      "Epoch: 3682 mean train loss:  3.44236323e-05, mean val. loss:  1.26889908e+00\n",
      "Epoch: 3683 mean train loss:  3.25396250e-05, mean val. loss:  1.26909435e+00\n",
      "Epoch: 3684 mean train loss:  3.40363476e-05, mean val. loss:  1.26929545e+00\n",
      "Epoch: 3685 mean train loss:  3.38075624e-05, mean val. loss:  1.26950717e+00\n",
      "Epoch: 3686 mean train loss:  3.33070057e-05, mean val. loss:  1.26972079e+00\n",
      "Epoch: 3687 mean train loss:  3.39583494e-05, mean val. loss:  1.26993942e+00\n",
      "Epoch: 3688 mean train loss:  3.37414094e-05, mean val. loss:  1.27015948e+00\n",
      "Epoch: 3689 mean train loss:  3.42743588e-05, mean val. loss:  1.27037024e+00\n",
      "Epoch: 3690 mean train loss:  3.41128616e-05, mean val. loss:  1.27057254e+00\n",
      "Epoch: 3691 mean train loss:  3.42968269e-05, mean val. loss:  1.27075410e+00\n",
      "Epoch: 3692 mean train loss:  3.32800846e-05, mean val. loss:  1.27094471e+00\n",
      "Epoch: 3693 mean train loss:  3.37771489e-05, mean val. loss:  1.27114284e+00\n",
      "Epoch: 3694 mean train loss:  3.37359961e-05, mean val. loss:  1.27134526e+00\n",
      "Epoch: 3695 mean train loss:  3.34306969e-05, mean val. loss:  1.27155638e+00\n",
      "Epoch: 3696 mean train loss:  3.41194391e-05, mean val. loss:  1.27176011e+00\n",
      "Epoch: 3697 mean train loss:  3.39569233e-05, mean val. loss:  1.27196801e+00\n",
      "Epoch: 3698 mean train loss:  3.41730483e-05, mean val. loss:  1.27217376e+00\n",
      "Epoch: 3699 mean train loss:  3.31905321e-05, mean val. loss:  1.27238107e+00\n",
      "Epoch: 3700 mean train loss:  3.32828204e-05, mean val. loss:  1.27259731e+00\n",
      "Epoch: 3701 mean train loss:  3.38877726e-05, mean val. loss:  1.27281082e+00\n",
      "Epoch: 3702 mean train loss:  3.41354753e-05, mean val. loss:  1.27303445e+00\n",
      "Epoch: 3703 mean train loss:  3.45616136e-05, mean val. loss:  1.27324736e+00\n",
      "Epoch: 3704 mean train loss:  3.33508069e-05, mean val. loss:  1.27346873e+00\n",
      "Epoch: 3705 mean train loss:  3.30166949e-05, mean val. loss:  1.27369618e+00\n",
      "Epoch: 3706 mean train loss:  3.37621022e-05, mean val. loss:  1.27392161e+00\n",
      "Epoch: 3707 mean train loss:  3.36058729e-05, mean val. loss:  1.27415025e+00\n",
      "Epoch: 3708 mean train loss:  3.34071810e-05, mean val. loss:  1.27437901e+00\n",
      "Epoch: 3709 mean train loss:  3.39506369e-05, mean val. loss:  1.27460718e+00\n",
      "Epoch: 3710 mean train loss:  3.37603560e-05, mean val. loss:  1.27484286e+00\n",
      "Epoch: 3711 mean train loss:  3.39063990e-05, mean val. loss:  1.27508080e+00\n",
      "Epoch: 3712 mean train loss:  3.40180122e-05, mean val. loss:  1.27532554e+00\n",
      "Epoch: 3713 mean train loss:  3.37217061e-05, mean val. loss:  1.27555954e+00\n",
      "Epoch: 3714 mean train loss:  3.38896643e-05, mean val. loss:  1.27580273e+00\n",
      "Epoch: 3715 mean train loss:  3.29592731e-05, mean val. loss:  1.27604377e+00\n",
      "Epoch: 3716 mean train loss:  3.38028185e-05, mean val. loss:  1.27628481e+00\n",
      "Epoch: 3717 mean train loss:  3.31768824e-05, mean val. loss:  1.27652657e+00\n",
      "Epoch: 3718 mean train loss:  3.41496198e-05, mean val. loss:  1.27675259e+00\n",
      "Epoch: 3719 mean train loss:  3.34056385e-05, mean val. loss:  1.27698350e+00\n",
      "Epoch: 3720 mean train loss:  3.31808988e-05, mean val. loss:  1.27723634e+00\n",
      "Epoch: 3721 mean train loss:  3.44259315e-05, mean val. loss:  1.27748787e+00\n",
      "Epoch: 3722 mean train loss:  3.38337268e-05, mean val. loss:  1.27773464e+00\n",
      "Epoch: 3723 mean train loss:  3.34396609e-05, mean val. loss:  1.27799416e+00\n",
      "Epoch: 3724 mean train loss:  3.40082915e-05, mean val. loss:  1.27823794e+00\n",
      "Epoch: 3725 mean train loss:  3.38021491e-05, mean val. loss:  1.27847123e+00\n",
      "Epoch: 3726 mean train loss:  3.30800831e-05, mean val. loss:  1.27870035e+00\n",
      "Epoch: 3727 mean train loss:  3.41324485e-05, mean val. loss:  1.27891254e+00\n",
      "Epoch: 3728 mean train loss:  3.36079684e-05, mean val. loss:  1.27913010e+00\n",
      "Epoch: 3729 mean train loss:  3.43028805e-05, mean val. loss:  1.27933371e+00\n",
      "Epoch: 3730 mean train loss:  3.42347776e-05, mean val. loss:  1.27953053e+00\n",
      "Epoch: 3731 mean train loss:  3.35474615e-05, mean val. loss:  1.27971864e+00\n",
      "Epoch: 3732 mean train loss:  3.42692656e-05, mean val. loss:  1.27990270e+00\n",
      "Epoch: 3733 mean train loss:  3.35467630e-05, mean val. loss:  1.28008878e+00\n",
      "Epoch: 3734 mean train loss:  3.42764833e-05, mean val. loss:  1.28025424e+00\n",
      "Epoch: 3735 mean train loss:  3.35005170e-05, mean val. loss:  1.28041804e+00\n",
      "Epoch: 3736 mean train loss:  3.35351215e-05, mean val. loss:  1.28058100e+00\n",
      "Epoch: 3737 mean train loss:  3.43443826e-05, mean val. loss:  1.28074312e+00\n",
      "Epoch: 3738 mean train loss:  3.40451661e-05, mean val. loss:  1.28089845e+00\n",
      "Epoch: 3739 mean train loss:  3.36390513e-05, mean val. loss:  1.28105688e+00\n",
      "Epoch: 3740 mean train loss:  3.30088369e-05, mean val. loss:  1.28122532e+00\n",
      "Epoch: 3741 mean train loss:  3.35820951e-05, mean val. loss:  1.28139257e+00\n",
      "Epoch: 3742 mean train loss:  3.41762789e-05, mean val. loss:  1.28154624e+00\n",
      "Epoch: 3743 mean train loss:  3.31942574e-05, mean val. loss:  1.28171623e+00\n",
      "Epoch: 3744 mean train loss:  3.34384385e-05, mean val. loss:  1.28189385e+00\n",
      "Epoch: 3745 mean train loss:  3.41594277e-05, mean val. loss:  1.28205478e+00\n",
      "Epoch: 3746 mean train loss:  3.32635827e-05, mean val. loss:  1.28221714e+00\n",
      "Epoch: 3747 mean train loss:  3.28786264e-05, mean val. loss:  1.28238297e+00\n",
      "Epoch: 3748 mean train loss:  3.37138772e-05, mean val. loss:  1.28254032e+00\n",
      "Epoch: 3749 mean train loss:  3.32492054e-05, mean val. loss:  1.28270483e+00\n",
      "Epoch: 3750 mean train loss:  3.42145795e-05, mean val. loss:  1.28287971e+00\n",
      "Epoch: 3751 mean train loss:  3.30993207e-05, mean val. loss:  1.28307652e+00\n",
      "Epoch: 3752 mean train loss:  3.38405662e-05, mean val. loss:  1.28328192e+00\n",
      "Epoch: 3753 mean train loss:  3.37939127e-05, mean val. loss:  1.28347576e+00\n",
      "Epoch: 3754 mean train loss:  3.31256888e-05, mean val. loss:  1.28368473e+00\n",
      "Epoch: 3755 mean train loss:  3.32412019e-05, mean val. loss:  1.28390253e+00\n",
      "Epoch: 3756 mean train loss:  3.34941433e-05, mean val. loss:  1.28412640e+00\n",
      "Epoch: 3757 mean train loss:  3.34795623e-05, mean val. loss:  1.28434837e+00\n",
      "Epoch: 3758 mean train loss:  3.40989791e-05, mean val. loss:  1.28457177e+00\n",
      "Epoch: 3759 mean train loss:  3.38017126e-05, mean val. loss:  1.28479338e+00\n",
      "Epoch: 3760 mean train loss:  3.37153033e-05, mean val. loss:  1.28501344e+00\n",
      "Epoch: 3761 mean train loss:  3.38781101e-05, mean val. loss:  1.28523517e+00\n",
      "Epoch: 3762 mean train loss:  3.34660290e-05, mean val. loss:  1.28546536e+00\n",
      "Epoch: 3763 mean train loss:  3.29996692e-05, mean val. loss:  1.28569078e+00\n",
      "Epoch: 3764 mean train loss:  3.32387281e-05, mean val. loss:  1.28591824e+00\n",
      "Epoch: 3765 mean train loss:  3.42249987e-05, mean val. loss:  1.28614879e+00\n",
      "Epoch: 3766 mean train loss:  3.38637328e-05, mean val. loss:  1.28637695e+00\n",
      "Epoch: 3767 mean train loss:  3.38843092e-05, mean val. loss:  1.28659797e+00\n",
      "Epoch: 3768 mean train loss:  3.43989232e-05, mean val. loss:  1.28680110e+00\n",
      "Epoch: 3769 mean train loss:  3.38950485e-05, mean val. loss:  1.28699446e+00\n",
      "Epoch: 3770 mean train loss:  3.37389356e-05, mean val. loss:  1.28719270e+00\n",
      "Epoch: 3771 mean train loss:  3.34086071e-05, mean val. loss:  1.28738511e+00\n",
      "Epoch: 3772 mean train loss:  3.35393997e-05, mean val. loss:  1.28756464e+00\n",
      "Epoch: 3773 mean train loss:  3.40013939e-05, mean val. loss:  1.28775382e+00\n",
      "Epoch: 3774 mean train loss:  3.35614604e-05, mean val. loss:  1.28793395e+00\n",
      "Epoch: 3775 mean train loss:  3.33703065e-05, mean val. loss:  1.28811610e+00\n",
      "Epoch: 3776 mean train loss:  3.36379744e-05, mean val. loss:  1.28828192e+00\n",
      "Epoch: 3777 mean train loss:  3.32465279e-05, mean val. loss:  1.28845811e+00\n",
      "Epoch: 3778 mean train loss:  3.38694081e-05, mean val. loss:  1.28862500e+00\n",
      "Epoch: 3779 mean train loss:  3.35777295e-05, mean val. loss:  1.28879309e+00\n",
      "Epoch: 3780 mean train loss:  3.36417870e-05, mean val. loss:  1.28896499e+00\n",
      "Epoch: 3781 mean train loss:  3.34738870e-05, mean val. loss:  1.28914511e+00\n",
      "Epoch: 3782 mean train loss:  3.35719960e-05, mean val. loss:  1.28931999e+00\n",
      "Epoch: 3783 mean train loss:  3.30202456e-05, mean val. loss:  1.28949535e+00\n",
      "Epoch: 3784 mean train loss:  3.32132622e-05, mean val. loss:  1.28967440e+00\n",
      "Epoch: 3785 mean train loss:  3.35916411e-05, mean val. loss:  1.28986144e+00\n",
      "Epoch: 3786 mean train loss:  3.35097429e-05, mean val. loss:  1.29005158e+00\n",
      "Epoch: 3787 mean train loss:  3.30808689e-05, mean val. loss:  1.29025817e+00\n",
      "Epoch: 3788 mean train loss:  3.41284904e-05, mean val. loss:  1.29043996e+00\n",
      "Epoch: 3789 mean train loss:  3.32492054e-05, mean val. loss:  1.29063272e+00\n",
      "Epoch: 3790 mean train loss:  3.29276372e-05, mean val. loss:  1.29082489e+00\n",
      "Epoch: 3791 mean train loss:  3.27930320e-05, mean val. loss:  1.29103243e+00\n",
      "Epoch: 3792 mean train loss:  3.30065086e-05, mean val. loss:  1.29125607e+00\n",
      "Epoch: 3793 mean train loss:  3.32045020e-05, mean val. loss:  1.29147661e+00\n",
      "Epoch: 3794 mean train loss:  3.38368700e-05, mean val. loss:  1.29169202e+00\n",
      "Epoch: 3795 mean train loss:  3.31351766e-05, mean val. loss:  1.29192686e+00\n",
      "Epoch: 3796 mean train loss:  3.31728370e-05, mean val. loss:  1.29217744e+00\n",
      "Epoch: 3797 mean train loss:  3.36992380e-05, mean val. loss:  1.29244268e+00\n",
      "Epoch: 3798 mean train loss:  3.37207457e-05, mean val. loss:  1.29270053e+00\n",
      "Epoch: 3799 mean train loss:  3.36598023e-05, mean val. loss:  1.29294896e+00\n",
      "Epoch: 3800 mean train loss:  3.23784479e-05, mean val. loss:  1.29320872e+00\n",
      "Epoch: 3801 mean train loss:  3.31049087e-05, mean val. loss:  1.29348922e+00\n",
      "Epoch: 3802 mean train loss:  3.32020572e-05, mean val. loss:  1.29378271e+00\n",
      "Epoch: 3803 mean train loss:  3.32029886e-05, mean val. loss:  1.29407907e+00\n",
      "Epoch: 3804 mean train loss:  3.37745587e-05, mean val. loss:  1.29436338e+00\n",
      "Epoch: 3805 mean train loss:  3.31073534e-05, mean val. loss:  1.29464626e+00\n",
      "Epoch: 3806 mean train loss:  3.33730131e-05, mean val. loss:  1.29493868e+00\n",
      "Epoch: 3807 mean train loss:  3.42400454e-05, mean val. loss:  1.29522991e+00\n",
      "Epoch: 3808 mean train loss:  3.31614574e-05, mean val. loss:  1.29552078e+00\n",
      "Epoch: 3809 mean train loss:  3.36443482e-05, mean val. loss:  1.29581249e+00\n",
      "Epoch: 3810 mean train loss:  3.29645409e-05, mean val. loss:  1.29611707e+00\n",
      "Epoch: 3811 mean train loss:  3.29737377e-05, mean val. loss:  1.29642808e+00\n",
      "Epoch: 3812 mean train loss:  3.32967320e-05, mean val. loss:  1.29674435e+00\n",
      "Epoch: 3813 mean train loss:  3.31022020e-05, mean val. loss:  1.29706061e+00\n",
      "Epoch: 3814 mean train loss:  3.34269134e-05, mean val. loss:  1.29737055e+00\n",
      "Epoch: 3815 mean train loss:  3.33102071e-05, mean val. loss:  1.29769361e+00\n",
      "Epoch: 3816 mean train loss:  3.31608753e-05, mean val. loss:  1.29800653e+00\n",
      "Epoch: 3817 mean train loss:  3.36482190e-05, mean val. loss:  1.29830670e+00\n",
      "Epoch: 3818 mean train loss:  3.32781055e-05, mean val. loss:  1.29861188e+00\n",
      "Epoch: 3819 mean train loss:  3.36552912e-05, mean val. loss:  1.29889905e+00\n",
      "Epoch: 3820 mean train loss:  3.34739452e-05, mean val. loss:  1.29917192e+00\n",
      "Epoch: 3821 mean train loss:  3.26108420e-05, mean val. loss:  1.29945922e+00\n",
      "Epoch: 3822 mean train loss:  3.31646297e-05, mean val. loss:  1.29974461e+00\n",
      "Epoch: 3823 mean train loss:  3.40293627e-05, mean val. loss:  1.30002296e+00\n",
      "Epoch: 3824 mean train loss:  3.35487130e-05, mean val. loss:  1.30029511e+00\n",
      "Epoch: 3825 mean train loss:  3.32432683e-05, mean val. loss:  1.30055106e+00\n",
      "Epoch: 3826 mean train loss:  3.30965267e-05, mean val. loss:  1.30080557e+00\n",
      "Epoch: 3827 mean train loss:  3.29283939e-05, mean val. loss:  1.30104744e+00\n",
      "Epoch: 3828 mean train loss:  3.31805204e-05, mean val. loss:  1.30127680e+00\n",
      "Epoch: 3829 mean train loss:  3.35736549e-05, mean val. loss:  1.30148852e+00\n",
      "Epoch: 3830 mean train loss:  3.30116891e-05, mean val. loss:  1.30170584e+00\n",
      "Epoch: 3831 mean train loss:  3.26188165e-05, mean val. loss:  1.30192959e+00\n",
      "Epoch: 3832 mean train loss:  3.35432997e-05, mean val. loss:  1.30213952e+00\n",
      "Epoch: 3833 mean train loss:  3.40352708e-05, mean val. loss:  1.30234647e+00\n",
      "Epoch: 3834 mean train loss:  3.31146293e-05, mean val. loss:  1.30255675e+00\n",
      "Epoch: 3835 mean train loss:  3.33080243e-05, mean val. loss:  1.30277050e+00\n",
      "Epoch: 3836 mean train loss:  3.31418705e-05, mean val. loss:  1.30296814e+00\n",
      "Epoch: 3837 mean train loss:  3.33652424e-05, mean val. loss:  1.30317271e+00\n",
      "Epoch: 3838 mean train loss:  3.34778160e-05, mean val. loss:  1.30336332e+00\n",
      "Epoch: 3839 mean train loss:  3.37957463e-05, mean val. loss:  1.30355179e+00\n",
      "Epoch: 3840 mean train loss:  3.40747647e-05, mean val. loss:  1.30372286e+00\n",
      "Epoch: 3841 mean train loss:  3.29178583e-05, mean val. loss:  1.30390072e+00\n",
      "Epoch: 3842 mean train loss:  3.34617798e-05, mean val. loss:  1.30407524e+00\n",
      "Epoch: 3843 mean train loss:  3.30532785e-05, mean val. loss:  1.30424058e+00\n",
      "Epoch: 3844 mean train loss:  3.27773159e-05, mean val. loss:  1.30440211e+00\n",
      "Epoch: 3845 mean train loss:  3.36404191e-05, mean val. loss:  1.30455875e+00\n",
      "Epoch: 3846 mean train loss:  3.31756892e-05, mean val. loss:  1.30473101e+00\n",
      "Epoch: 3847 mean train loss:  3.28159076e-05, mean val. loss:  1.30491281e+00\n",
      "Epoch: 3848 mean train loss:  3.33189673e-05, mean val. loss:  1.30508101e+00\n",
      "Epoch: 3849 mean train loss:  3.32861091e-05, mean val. loss:  1.30526376e+00\n",
      "Epoch: 3850 mean train loss:  3.35344230e-05, mean val. loss:  1.30544341e+00\n",
      "Epoch: 3851 mean train loss:  3.25787987e-05, mean val. loss:  1.30563962e+00\n",
      "Epoch: 3852 mean train loss:  3.29299073e-05, mean val. loss:  1.30583465e+00\n",
      "Epoch: 3853 mean train loss:  3.28234455e-05, mean val. loss:  1.30603325e+00\n",
      "Epoch: 3854 mean train loss:  3.34824144e-05, mean val. loss:  1.30624461e+00\n",
      "Epoch: 3855 mean train loss:  3.32880009e-05, mean val. loss:  1.30647635e+00\n",
      "Epoch: 3856 mean train loss:  3.29826144e-05, mean val. loss:  1.30670679e+00\n",
      "Epoch: 3857 mean train loss:  3.33794160e-05, mean val. loss:  1.30693555e+00\n",
      "Epoch: 3858 mean train loss:  3.33657081e-05, mean val. loss:  1.30715764e+00\n",
      "Epoch: 3859 mean train loss:  3.32176278e-05, mean val. loss:  1.30737913e+00\n",
      "Epoch: 3860 mean train loss:  3.28019960e-05, mean val. loss:  1.30761874e+00\n",
      "Epoch: 3861 mean train loss:  3.35256627e-05, mean val. loss:  1.30785334e+00\n",
      "Epoch: 3862 mean train loss:  3.34762444e-05, mean val. loss:  1.30808640e+00\n",
      "Epoch: 3863 mean train loss:  3.34882061e-05, mean val. loss:  1.30832052e+00\n",
      "Epoch: 3864 mean train loss:  3.30731273e-05, mean val. loss:  1.30855048e+00\n",
      "Epoch: 3865 mean train loss:  3.34359647e-05, mean val. loss:  1.30880082e+00\n",
      "Epoch: 3866 mean train loss:  3.34749056e-05, mean val. loss:  1.30904055e+00\n",
      "Epoch: 3867 mean train loss:  3.23003915e-05, mean val. loss:  1.30930495e+00\n",
      "Epoch: 3868 mean train loss:  3.38027603e-05, mean val. loss:  1.30956566e+00\n",
      "Epoch: 3869 mean train loss:  3.29901231e-05, mean val. loss:  1.30982566e+00\n",
      "Epoch: 3870 mean train loss:  3.36537778e-05, mean val. loss:  1.31007862e+00\n",
      "Epoch: 3871 mean train loss:  3.37941456e-05, mean val. loss:  1.31032741e+00\n",
      "Epoch: 3872 mean train loss:  3.29116883e-05, mean val. loss:  1.31057346e+00\n",
      "Epoch: 3873 mean train loss:  3.28651513e-05, mean val. loss:  1.31081295e+00\n",
      "Epoch: 3874 mean train loss:  3.37695237e-05, mean val. loss:  1.31105018e+00\n",
      "Epoch: 3875 mean train loss:  3.30926268e-05, mean val. loss:  1.31128800e+00\n",
      "Epoch: 3876 mean train loss:  3.29218456e-05, mean val. loss:  1.31153703e+00\n",
      "Epoch: 3877 mean train loss:  3.36079393e-05, mean val. loss:  1.31179392e+00\n",
      "Epoch: 3878 mean train loss:  3.34664655e-05, mean val. loss:  1.31204522e+00\n",
      "Epoch: 3879 mean train loss:  3.32041527e-05, mean val. loss:  1.31228483e+00\n",
      "Epoch: 3880 mean train loss:  3.27683520e-05, mean val. loss:  1.31252289e+00\n",
      "Epoch: 3881 mean train loss:  3.37111414e-05, mean val. loss:  1.31275690e+00\n",
      "Epoch: 3882 mean train loss:  3.27461748e-05, mean val. loss:  1.31299961e+00\n",
      "Epoch: 3883 mean train loss:  3.26620939e-05, mean val. loss:  1.31325531e+00\n",
      "Epoch: 3884 mean train loss:  3.34208016e-05, mean val. loss:  1.31351888e+00\n",
      "Epoch: 3885 mean train loss:  3.36130615e-05, mean val. loss:  1.31377757e+00\n",
      "Epoch: 3886 mean train loss:  3.36102385e-05, mean val. loss:  1.31403387e+00\n",
      "Epoch: 3887 mean train loss:  3.36322119e-05, mean val. loss:  1.31429088e+00\n",
      "Epoch: 3888 mean train loss:  3.28677415e-05, mean val. loss:  1.31454802e+00\n",
      "Epoch: 3889 mean train loss:  3.29905888e-05, mean val. loss:  1.31479621e+00\n",
      "Epoch: 3890 mean train loss:  3.34458018e-05, mean val. loss:  1.31505024e+00\n",
      "Epoch: 3891 mean train loss:  3.33564822e-05, mean val. loss:  1.31529546e+00\n",
      "Epoch: 3892 mean train loss:  3.29429749e-05, mean val. loss:  1.31554759e+00\n",
      "Epoch: 3893 mean train loss:  3.30952462e-05, mean val. loss:  1.31580126e+00\n",
      "Epoch: 3894 mean train loss:  3.24311550e-05, mean val. loss:  1.31606948e+00\n",
      "Epoch: 3895 mean train loss:  3.29523173e-05, mean val. loss:  1.31633842e+00\n",
      "Epoch: 3896 mean train loss:  3.27927701e-05, mean val. loss:  1.31660914e+00\n",
      "Epoch: 3897 mean train loss:  3.33726348e-05, mean val. loss:  1.31687236e+00\n",
      "Epoch: 3898 mean train loss:  3.32892814e-05, mean val. loss:  1.31714034e+00\n",
      "Epoch: 3899 mean train loss:  3.29601753e-05, mean val. loss:  1.31740427e+00\n",
      "Epoch: 3900 mean train loss:  3.37923411e-05, mean val. loss:  1.31765163e+00\n",
      "Epoch: 3901 mean train loss:  3.32806376e-05, mean val. loss:  1.31789315e+00\n",
      "Epoch: 3902 mean train loss:  3.27878806e-05, mean val. loss:  1.31812918e+00\n",
      "Epoch: 3903 mean train loss:  3.33004573e-05, mean val. loss:  1.31836867e+00\n",
      "Epoch: 3904 mean train loss:  3.28267924e-05, mean val. loss:  1.31861913e+00\n",
      "Epoch: 3905 mean train loss:  3.23618588e-05, mean val. loss:  1.31886327e+00\n",
      "Epoch: 3906 mean train loss:  3.29778995e-05, mean val. loss:  1.31911504e+00\n",
      "Epoch: 3907 mean train loss:  3.37904785e-05, mean val. loss:  1.31935346e+00\n",
      "Epoch: 3908 mean train loss:  3.39775579e-05, mean val. loss:  1.31958473e+00\n",
      "Epoch: 3909 mean train loss:  3.34888173e-05, mean val. loss:  1.31980479e+00\n",
      "Epoch: 3910 mean train loss:  3.28734750e-05, mean val. loss:  1.32002389e+00\n",
      "Epoch: 3911 mean train loss:  3.29076720e-05, mean val. loss:  1.32023585e+00\n",
      "Epoch: 3912 mean train loss:  3.30040639e-05, mean val. loss:  1.32045257e+00\n",
      "Epoch: 3913 mean train loss:  3.32880009e-05, mean val. loss:  1.32064748e+00\n",
      "Epoch: 3914 mean train loss:  3.27421876e-05, mean val. loss:  1.32085311e+00\n",
      "Epoch: 3915 mean train loss:  3.29929171e-05, mean val. loss:  1.32106209e+00\n",
      "Epoch: 3916 mean train loss:  3.29051109e-05, mean val. loss:  1.32126510e+00\n",
      "Epoch: 3917 mean train loss:  3.31102055e-05, mean val. loss:  1.32145071e+00\n",
      "Epoch: 3918 mean train loss:  3.34607030e-05, mean val. loss:  1.32163155e+00\n",
      "Epoch: 3919 mean train loss:  3.20020190e-05, mean val. loss:  1.32182348e+00\n",
      "Epoch: 3920 mean train loss:  3.29985050e-05, mean val. loss:  1.32200956e+00\n",
      "Epoch: 3921 mean train loss:  3.28701863e-05, mean val. loss:  1.32218921e+00\n",
      "Epoch: 3922 mean train loss:  3.28040333e-05, mean val. loss:  1.32235599e+00\n",
      "Epoch: 3923 mean train loss:  3.27430025e-05, mean val. loss:  1.32253468e+00\n",
      "Epoch: 3924 mean train loss:  3.32599157e-05, mean val. loss:  1.32270682e+00\n",
      "Epoch: 3925 mean train loss:  3.33994976e-05, mean val. loss:  1.32288790e+00\n",
      "Epoch: 3926 mean train loss:  3.26297013e-05, mean val. loss:  1.32307458e+00\n",
      "Epoch: 3927 mean train loss:  3.30585754e-05, mean val. loss:  1.32327175e+00\n",
      "Epoch: 3928 mean train loss:  3.20126128e-05, mean val. loss:  1.32348621e+00\n",
      "Epoch: 3929 mean train loss:  3.28517635e-05, mean val. loss:  1.32371247e+00\n",
      "Epoch: 3930 mean train loss:  3.35684163e-05, mean val. loss:  1.32393932e+00\n",
      "Epoch: 3931 mean train loss:  3.29512404e-05, mean val. loss:  1.32415771e+00\n",
      "Epoch: 3932 mean train loss:  3.36148078e-05, mean val. loss:  1.32438695e+00\n",
      "Epoch: 3933 mean train loss:  3.25749570e-05, mean val. loss:  1.32462347e+00\n",
      "Epoch: 3934 mean train loss:  3.36557860e-05, mean val. loss:  1.32486773e+00\n",
      "Epoch: 3935 mean train loss:  3.42825078e-05, mean val. loss:  1.32510114e+00\n",
      "Epoch: 3936 mean train loss:  3.34044744e-05, mean val. loss:  1.32534909e+00\n",
      "Epoch: 3937 mean train loss:  3.36838129e-05, mean val. loss:  1.32559323e+00\n",
      "Epoch: 3938 mean train loss:  3.34593642e-05, mean val. loss:  1.32584620e+00\n",
      "Epoch: 3939 mean train loss:  3.34623037e-05, mean val. loss:  1.32610333e+00\n",
      "Epoch: 3940 mean train loss:  3.30462935e-05, mean val. loss:  1.32634783e+00\n",
      "Epoch: 3941 mean train loss:  3.22682026e-05, mean val. loss:  1.32660198e+00\n",
      "Epoch: 3942 mean train loss:  3.37261008e-05, mean val. loss:  1.32685554e+00\n",
      "Epoch: 3943 mean train loss:  3.27847083e-05, mean val. loss:  1.32711172e+00\n",
      "Epoch: 3944 mean train loss:  3.25332221e-05, mean val. loss:  1.32737672e+00\n",
      "Epoch: 3945 mean train loss:  3.28690221e-05, mean val. loss:  1.32764149e+00\n",
      "Epoch: 3946 mean train loss:  3.26773443e-05, mean val. loss:  1.32790434e+00\n",
      "Epoch: 3947 mean train loss:  3.35181248e-05, mean val. loss:  1.32816374e+00\n",
      "Epoch: 3948 mean train loss:  3.29392788e-05, mean val. loss:  1.32842922e+00\n",
      "Epoch: 3949 mean train loss:  3.25946603e-05, mean val. loss:  1.32868946e+00\n",
      "Epoch: 3950 mean train loss:  3.34426004e-05, mean val. loss:  1.32894886e+00\n",
      "Epoch: 3951 mean train loss:  3.29676841e-05, mean val. loss:  1.32920396e+00\n",
      "Epoch: 3952 mean train loss:  3.27297603e-05, mean val. loss:  1.32946718e+00\n",
      "Epoch: 3953 mean train loss:  3.29006580e-05, mean val. loss:  1.32973611e+00\n",
      "Epoch: 3954 mean train loss:  3.32879135e-05, mean val. loss:  1.32999539e+00\n",
      "Epoch: 3955 mean train loss:  3.30859621e-05, mean val. loss:  1.33026445e+00\n",
      "Epoch: 3956 mean train loss:  3.28198657e-05, mean val. loss:  1.33053243e+00\n",
      "Epoch: 3957 mean train loss:  3.28368042e-05, mean val. loss:  1.33081055e+00\n",
      "Epoch: 3958 mean train loss:  3.26265581e-05, mean val. loss:  1.33110356e+00\n",
      "Epoch: 3959 mean train loss:  3.30973417e-05, mean val. loss:  1.33137977e+00\n",
      "Epoch: 3960 mean train loss:  3.31388437e-05, mean val. loss:  1.33166337e+00\n",
      "Epoch: 3961 mean train loss:  3.29051400e-05, mean val. loss:  1.33194411e+00\n",
      "Epoch: 3962 mean train loss:  3.29362229e-05, mean val. loss:  1.33222675e+00\n",
      "Epoch: 3963 mean train loss:  3.30061594e-05, mean val. loss:  1.33249879e+00\n",
      "Epoch: 3964 mean train loss:  3.31432093e-05, mean val. loss:  1.33276248e+00\n",
      "Epoch: 3965 mean train loss:  3.38362879e-05, mean val. loss:  1.33301330e+00\n",
      "Epoch: 3966 mean train loss:  3.24662251e-05, mean val. loss:  1.33327579e+00\n",
      "Epoch: 3967 mean train loss:  3.27508606e-05, mean val. loss:  1.33354628e+00\n",
      "Epoch: 3968 mean train loss:  3.27862508e-05, mean val. loss:  1.33381486e+00\n",
      "Epoch: 3969 mean train loss:  3.26946029e-05, mean val. loss:  1.33408666e+00\n",
      "Epoch: 3970 mean train loss:  3.33908829e-05, mean val. loss:  1.33434784e+00\n",
      "Epoch: 3971 mean train loss:  3.28361930e-05, mean val. loss:  1.33461154e+00\n",
      "Epoch: 3972 mean train loss:  3.30060429e-05, mean val. loss:  1.33487093e+00\n",
      "Epoch: 3973 mean train loss:  3.36159137e-05, mean val. loss:  1.33512104e+00\n",
      "Epoch: 3974 mean train loss:  3.33921053e-05, mean val. loss:  1.33537352e+00\n",
      "Epoch: 3975 mean train loss:  3.29267641e-05, mean val. loss:  1.33560944e+00\n",
      "Epoch: 3976 mean train loss:  3.23040294e-05, mean val. loss:  1.33584595e+00\n",
      "Epoch: 3977 mean train loss:  3.27331363e-05, mean val. loss:  1.33607829e+00\n",
      "Epoch: 3978 mean train loss:  3.24518478e-05, mean val. loss:  1.33631766e+00\n",
      "Epoch: 3979 mean train loss:  3.30898620e-05, mean val. loss:  1.33654583e+00\n",
      "Epoch: 3980 mean train loss:  3.29304021e-05, mean val. loss:  1.33676946e+00\n",
      "Epoch: 3981 mean train loss:  3.33396601e-05, mean val. loss:  1.33698475e+00\n",
      "Epoch: 3982 mean train loss:  3.31831106e-05, mean val. loss:  1.33719015e+00\n",
      "Epoch: 3983 mean train loss:  3.33478383e-05, mean val. loss:  1.33738422e+00\n",
      "Epoch: 3984 mean train loss:  3.33220523e-05, mean val. loss:  1.33757389e+00\n",
      "Epoch: 3985 mean train loss:  3.29401228e-05, mean val. loss:  1.33776319e+00\n",
      "Epoch: 3986 mean train loss:  3.30915500e-05, mean val. loss:  1.33796203e+00\n",
      "Epoch: 3987 mean train loss:  3.26790614e-05, mean val. loss:  1.33816409e+00\n",
      "Epoch: 3988 mean train loss:  3.23850545e-05, mean val. loss:  1.33836496e+00\n",
      "Epoch: 3989 mean train loss:  3.30384064e-05, mean val. loss:  1.33856583e+00\n",
      "Epoch: 3990 mean train loss:  3.27733578e-05, mean val. loss:  1.33877945e+00\n",
      "Epoch: 3991 mean train loss:  3.32576456e-05, mean val. loss:  1.33898473e+00\n",
      "Epoch: 3992 mean train loss:  3.30485927e-05, mean val. loss:  1.33918226e+00\n",
      "Epoch: 3993 mean train loss:  3.21214611e-05, mean val. loss:  1.33938777e+00\n",
      "Epoch: 3994 mean train loss:  3.24664288e-05, mean val. loss:  1.33960629e+00\n",
      "Epoch: 3995 mean train loss:  3.25881701e-05, mean val. loss:  1.33983004e+00\n",
      "Epoch: 3996 mean train loss:  3.27470188e-05, mean val. loss:  1.34005797e+00\n",
      "Epoch: 3997 mean train loss:  3.33035714e-05, mean val. loss:  1.34026754e+00\n",
      "Epoch: 3998 mean train loss:  3.32399213e-05, mean val. loss:  1.34048438e+00\n",
      "Epoch: 3999 mean train loss:  3.25762376e-05, mean val. loss:  1.34071088e+00\n",
      "Epoch: 4000 mean train loss:  3.24036519e-05, mean val. loss:  1.34095180e+00\n",
      "Epoch: 4001 mean train loss:  3.35415243e-05, mean val. loss:  1.34116983e+00\n",
      "Epoch: 4002 mean train loss:  3.26576992e-05, mean val. loss:  1.34138846e+00\n",
      "Epoch: 4003 mean train loss:  3.27953021e-05, mean val. loss:  1.34160101e+00\n",
      "Epoch: 4004 mean train loss:  3.32325872e-05, mean val. loss:  1.34180844e+00\n",
      "Epoch: 4005 mean train loss:  3.28126480e-05, mean val. loss:  1.34201634e+00\n",
      "Epoch: 4006 mean train loss:  3.21060943e-05, mean val. loss:  1.34225130e+00\n",
      "Epoch: 4007 mean train loss:  3.28778115e-05, mean val. loss:  1.34248805e+00\n",
      "Epoch: 4008 mean train loss:  3.30367475e-05, mean val. loss:  1.34272861e+00\n",
      "Epoch: 4009 mean train loss:  3.35169898e-05, mean val. loss:  1.34297407e+00\n",
      "Epoch: 4010 mean train loss:  3.31522606e-05, mean val. loss:  1.34321308e+00\n",
      "Epoch: 4011 mean train loss:  3.30462353e-05, mean val. loss:  1.34345508e+00\n",
      "Epoch: 4012 mean train loss:  3.23111890e-05, mean val. loss:  1.34369588e+00\n",
      "Epoch: 4013 mean train loss:  3.25854053e-05, mean val. loss:  1.34394932e+00\n",
      "Epoch: 4014 mean train loss:  3.28008318e-05, mean val. loss:  1.34420216e+00\n",
      "Epoch: 4015 mean train loss:  3.32631171e-05, mean val. loss:  1.34446037e+00\n",
      "Epoch: 4016 mean train loss:  3.25269648e-05, mean val. loss:  1.34471321e+00\n",
      "Epoch: 4017 mean train loss:  3.34284559e-05, mean val. loss:  1.34494615e+00\n",
      "Epoch: 4018 mean train loss:  3.28936731e-05, mean val. loss:  1.34518087e+00\n",
      "Epoch: 4019 mean train loss:  3.34910583e-05, mean val. loss:  1.34541190e+00\n",
      "Epoch: 4020 mean train loss:  3.30209150e-05, mean val. loss:  1.34564161e+00\n",
      "Epoch: 4021 mean train loss:  3.27116577e-05, mean val. loss:  1.34588420e+00\n",
      "Epoch: 4022 mean train loss:  3.34447832e-05, mean val. loss:  1.34612012e+00\n",
      "Epoch: 4023 mean train loss:  3.27372691e-05, mean val. loss:  1.34636211e+00\n",
      "Epoch: 4024 mean train loss:  3.29194008e-05, mean val. loss:  1.34660423e+00\n",
      "Epoch: 4025 mean train loss:  3.28058959e-05, mean val. loss:  1.34683609e+00\n",
      "Epoch: 4026 mean train loss:  3.28070018e-05, mean val. loss:  1.34707308e+00\n",
      "Epoch: 4027 mean train loss:  3.31284828e-05, mean val. loss:  1.34730566e+00\n",
      "Epoch: 4028 mean train loss:  3.29190807e-05, mean val. loss:  1.34753084e+00\n",
      "Epoch: 4029 mean train loss:  3.30839248e-05, mean val. loss:  1.34774387e+00\n",
      "Epoch: 4030 mean train loss:  3.28081078e-05, mean val. loss:  1.34795737e+00\n",
      "Epoch: 4031 mean train loss:  3.22604319e-05, mean val. loss:  1.34817183e+00\n",
      "Epoch: 4032 mean train loss:  3.22879641e-05, mean val. loss:  1.34839702e+00\n",
      "Epoch: 4033 mean train loss:  3.30486801e-05, mean val. loss:  1.34862781e+00\n",
      "Epoch: 4034 mean train loss:  3.29319155e-05, mean val. loss:  1.34885955e+00\n",
      "Epoch: 4035 mean train loss:  3.25860747e-05, mean val. loss:  1.34909964e+00\n",
      "Epoch: 4036 mean train loss:  3.32929776e-05, mean val. loss:  1.34934139e+00\n",
      "Epoch: 4037 mean train loss:  3.30586918e-05, mean val. loss:  1.34958386e+00\n",
      "Epoch: 4038 mean train loss:  3.22110427e-05, mean val. loss:  1.34982955e+00\n",
      "Epoch: 4039 mean train loss:  3.24256544e-05, mean val. loss:  1.35006976e+00\n",
      "Epoch: 4040 mean train loss:  3.29683826e-05, mean val. loss:  1.35031700e+00\n",
      "Epoch: 4041 mean train loss:  3.34230135e-05, mean val. loss:  1.35056043e+00\n",
      "Epoch: 4042 mean train loss:  3.19994288e-05, mean val. loss:  1.35080922e+00\n",
      "Epoch: 4043 mean train loss:  3.23432905e-05, mean val. loss:  1.35106814e+00\n",
      "Epoch: 4044 mean train loss:  3.25742294e-05, mean val. loss:  1.35133314e+00\n",
      "Epoch: 4045 mean train loss:  3.32008931e-05, mean val. loss:  1.35160708e+00\n",
      "Epoch: 4046 mean train loss:  3.24119173e-05, mean val. loss:  1.35187805e+00\n",
      "Epoch: 4047 mean train loss:  3.19643877e-05, mean val. loss:  1.35215294e+00\n",
      "Epoch: 4048 mean train loss:  3.24320572e-05, mean val. loss:  1.35243332e+00\n",
      "Epoch: 4049 mean train loss:  3.33415810e-05, mean val. loss:  1.35271668e+00\n",
      "Epoch: 4050 mean train loss:  3.29462346e-05, mean val. loss:  1.35300088e+00\n",
      "Epoch: 4051 mean train loss:  3.28188180e-05, mean val. loss:  1.35328865e+00\n",
      "Epoch: 4052 mean train loss:  3.28309834e-05, mean val. loss:  1.35356784e+00\n",
      "Epoch: 4053 mean train loss:  3.27310700e-05, mean val. loss:  1.35384881e+00\n",
      "Epoch: 4054 mean train loss:  3.29446339e-05, mean val. loss:  1.35412788e+00\n",
      "Epoch: 4055 mean train loss:  3.29161994e-05, mean val. loss:  1.35441756e+00\n",
      "Epoch: 4056 mean train loss:  3.29439936e-05, mean val. loss:  1.35471380e+00\n",
      "Epoch: 4057 mean train loss:  3.23298445e-05, mean val. loss:  1.35502112e+00\n",
      "Epoch: 4058 mean train loss:  3.31170158e-05, mean val. loss:  1.35532415e+00\n",
      "Epoch: 4059 mean train loss:  3.31555202e-05, mean val. loss:  1.35561275e+00\n",
      "Epoch: 4060 mean train loss:  3.27267917e-05, mean val. loss:  1.35590911e+00\n",
      "Epoch: 4061 mean train loss:  3.24127614e-05, mean val. loss:  1.35620940e+00\n",
      "Epoch: 4062 mean train loss:  3.29648610e-05, mean val. loss:  1.35649920e+00\n",
      "Epoch: 4063 mean train loss:  3.27807793e-05, mean val. loss:  1.35679448e+00\n",
      "Epoch: 4064 mean train loss:  3.26280133e-05, mean val. loss:  1.35708106e+00\n",
      "Epoch: 4065 mean train loss:  3.30190524e-05, mean val. loss:  1.35735869e+00\n",
      "Epoch: 4066 mean train loss:  3.30361654e-05, mean val. loss:  1.35761964e+00\n",
      "Epoch: 4067 mean train loss:  3.22642154e-05, mean val. loss:  1.35788405e+00\n",
      "Epoch: 4068 mean train loss:  3.35603254e-05, mean val. loss:  1.35813653e+00\n",
      "Epoch: 4069 mean train loss:  3.26371228e-05, mean val. loss:  1.35838532e+00\n",
      "Epoch: 4070 mean train loss:  3.32461786e-05, mean val. loss:  1.35861385e+00\n",
      "Epoch: 4071 mean train loss:  3.27183225e-05, mean val. loss:  1.35883880e+00\n",
      "Epoch: 4072 mean train loss:  3.31557239e-05, mean val. loss:  1.35906410e+00\n",
      "Epoch: 4073 mean train loss:  3.30520561e-05, mean val. loss:  1.35928464e+00\n",
      "Epoch: 4074 mean train loss:  3.26492009e-05, mean val. loss:  1.35950756e+00\n",
      "Epoch: 4075 mean train loss:  3.27070011e-05, mean val. loss:  1.35972738e+00\n",
      "Epoch: 4076 mean train loss:  3.22871492e-05, mean val. loss:  1.35993993e+00\n",
      "Epoch: 4077 mean train loss:  3.22888081e-05, mean val. loss:  1.36014473e+00\n",
      "Epoch: 4078 mean train loss:  3.30952753e-05, mean val. loss:  1.36035013e+00\n",
      "Epoch: 4079 mean train loss:  3.20209656e-05, mean val. loss:  1.36056554e+00\n",
      "Epoch: 4080 mean train loss:  3.26894515e-05, mean val. loss:  1.36076844e+00\n",
      "Epoch: 4081 mean train loss:  3.29659379e-05, mean val. loss:  1.36097956e+00\n",
      "Epoch: 4082 mean train loss:  3.29421309e-05, mean val. loss:  1.36119676e+00\n",
      "Epoch: 4083 mean train loss:  3.22212290e-05, mean val. loss:  1.36140764e+00\n",
      "Epoch: 4084 mean train loss:  3.22592387e-05, mean val. loss:  1.36162746e+00\n",
      "Epoch: 4085 mean train loss:  3.19469546e-05, mean val. loss:  1.36184072e+00\n",
      "Epoch: 4086 mean train loss:  3.22291162e-05, mean val. loss:  1.36205745e+00\n",
      "Epoch: 4087 mean train loss:  3.32458003e-05, mean val. loss:  1.36226201e+00\n",
      "Epoch: 4088 mean train loss:  3.21058615e-05, mean val. loss:  1.36247623e+00\n",
      "Epoch: 4089 mean train loss:  3.20675317e-05, mean val. loss:  1.36271560e+00\n",
      "Epoch: 4090 mean train loss:  3.27789166e-05, mean val. loss:  1.36294842e+00\n",
      "Epoch: 4091 mean train loss:  3.25636065e-05, mean val. loss:  1.36317503e+00\n",
      "Epoch: 4092 mean train loss:  3.23756249e-05, mean val. loss:  1.36341393e+00\n",
      "Epoch: 4093 mean train loss:  3.24815628e-05, mean val. loss:  1.36364996e+00\n",
      "Epoch: 4094 mean train loss:  3.25801957e-05, mean val. loss:  1.36389434e+00\n",
      "Epoch: 4095 mean train loss:  3.24131397e-05, mean val. loss:  1.36414933e+00\n",
      "Epoch: 4096 mean train loss:  3.26616573e-05, mean val. loss:  1.36439979e+00\n",
      "Epoch: 4097 mean train loss:  3.27102316e-05, mean val. loss:  1.36465871e+00\n",
      "Epoch: 4098 mean train loss:  3.26300797e-05, mean val. loss:  1.36492002e+00\n",
      "Epoch: 4099 mean train loss:  3.23161366e-05, mean val. loss:  1.36519134e+00\n",
      "Epoch: 4100 mean train loss:  3.28178867e-05, mean val. loss:  1.36546135e+00\n",
      "Epoch: 4101 mean train loss:  3.31425399e-05, mean val. loss:  1.36573052e+00\n",
      "Epoch: 4102 mean train loss:  3.22648848e-05, mean val. loss:  1.36599815e+00\n",
      "Epoch: 4103 mean train loss:  3.24230350e-05, mean val. loss:  1.36627817e+00\n",
      "Epoch: 4104 mean train loss:  3.31851479e-05, mean val. loss:  1.36655962e+00\n",
      "Epoch: 4105 mean train loss:  3.25652945e-05, mean val. loss:  1.36685073e+00\n",
      "Epoch: 4106 mean train loss:  3.27868038e-05, mean val. loss:  1.36714065e+00\n",
      "Epoch: 4107 mean train loss:  3.27044399e-05, mean val. loss:  1.36742485e+00\n",
      "Epoch: 4108 mean train loss:  3.29472532e-05, mean val. loss:  1.36769998e+00\n",
      "Epoch: 4109 mean train loss:  3.24962602e-05, mean val. loss:  1.36796451e+00\n",
      "Epoch: 4110 mean train loss:  3.27136077e-05, mean val. loss:  1.36821687e+00\n",
      "Epoch: 4111 mean train loss:  3.28514725e-05, mean val. loss:  1.36848223e+00\n",
      "Epoch: 4112 mean train loss:  3.31702759e-05, mean val. loss:  1.36873066e+00\n",
      "Epoch: 4113 mean train loss:  3.17856611e-05, mean val. loss:  1.36898601e+00\n",
      "Epoch: 4114 mean train loss:  3.26021109e-05, mean val. loss:  1.36922765e+00\n",
      "Epoch: 4115 mean train loss:  3.30597395e-05, mean val. loss:  1.36948049e+00\n",
      "Epoch: 4116 mean train loss:  3.27641028e-05, mean val. loss:  1.36973250e+00\n",
      "Epoch: 4117 mean train loss:  3.26438458e-05, mean val. loss:  1.36997640e+00\n",
      "Epoch: 4118 mean train loss:  3.25973087e-05, mean val. loss:  1.37022400e+00\n",
      "Epoch: 4119 mean train loss:  3.26884328e-05, mean val. loss:  1.37047768e+00\n",
      "Epoch: 4120 mean train loss:  3.27534217e-05, mean val. loss:  1.37073517e+00\n",
      "Epoch: 4121 mean train loss:  3.25145083e-05, mean val. loss:  1.37098992e+00\n",
      "Epoch: 4122 mean train loss:  3.28952447e-05, mean val. loss:  1.37125385e+00\n",
      "Epoch: 4123 mean train loss:  3.23507120e-05, mean val. loss:  1.37152207e+00\n",
      "Epoch: 4124 mean train loss:  3.34244105e-05, mean val. loss:  1.37177134e+00\n",
      "Epoch: 4125 mean train loss:  3.27512971e-05, mean val. loss:  1.37202322e+00\n",
      "Epoch: 4126 mean train loss:  3.19513783e-05, mean val. loss:  1.37228227e+00\n",
      "Epoch: 4127 mean train loss:  3.22972774e-05, mean val. loss:  1.37253189e+00\n",
      "Epoch: 4128 mean train loss:  3.27804009e-05, mean val. loss:  1.37279356e+00\n",
      "Epoch: 4129 mean train loss:  3.15465732e-05, mean val. loss:  1.37305939e+00\n",
      "Epoch: 4130 mean train loss:  3.25170113e-05, mean val. loss:  1.37333918e+00\n",
      "Epoch: 4131 mean train loss:  3.30659968e-05, mean val. loss:  1.37361443e+00\n",
      "Epoch: 4132 mean train loss:  3.29730101e-05, mean val. loss:  1.37388563e+00\n",
      "Epoch: 4133 mean train loss:  3.21991392e-05, mean val. loss:  1.37416112e+00\n",
      "Epoch: 4134 mean train loss:  3.26495792e-05, mean val. loss:  1.37444222e+00\n",
      "Epoch: 4135 mean train loss:  3.26377049e-05, mean val. loss:  1.37472999e+00\n",
      "Epoch: 4136 mean train loss:  3.23678541e-05, mean val. loss:  1.37501717e+00\n",
      "Epoch: 4137 mean train loss:  3.23288841e-05, mean val. loss:  1.37530518e+00\n",
      "Epoch: 4138 mean train loss:  3.26945737e-05, mean val. loss:  1.37559032e+00\n",
      "Epoch: 4139 mean train loss:  3.25946021e-05, mean val. loss:  1.37587857e+00\n",
      "Epoch: 4140 mean train loss:  3.26109584e-05, mean val. loss:  1.37616611e+00\n",
      "Epoch: 4141 mean train loss:  3.22225969e-05, mean val. loss:  1.37644517e+00\n",
      "Epoch: 4142 mean train loss:  3.21070838e-05, mean val. loss:  1.37672246e+00\n",
      "Epoch: 4143 mean train loss:  3.29127070e-05, mean val. loss:  1.37698591e+00\n",
      "Epoch: 4144 mean train loss:  3.25607543e-05, mean val. loss:  1.37724543e+00\n",
      "Epoch: 4145 mean train loss:  3.23634304e-05, mean val. loss:  1.37750638e+00\n",
      "Epoch: 4146 mean train loss:  3.28818569e-05, mean val. loss:  1.37776947e+00\n",
      "Epoch: 4147 mean train loss:  3.21642437e-05, mean val. loss:  1.37803435e+00\n",
      "Epoch: 4148 mean train loss:  3.24240245e-05, mean val. loss:  1.37829602e+00\n",
      "Epoch: 4149 mean train loss:  3.26566224e-05, mean val. loss:  1.37855387e+00\n",
      "Epoch: 4150 mean train loss:  3.22592387e-05, mean val. loss:  1.37881184e+00\n",
      "Epoch: 4151 mean train loss:  3.27611051e-05, mean val. loss:  1.37906826e+00\n",
      "Epoch: 4152 mean train loss:  3.17772210e-05, mean val. loss:  1.37932885e+00\n",
      "Epoch: 4153 mean train loss:  3.25509172e-05, mean val. loss:  1.37958646e+00\n",
      "Epoch: 4154 mean train loss:  3.24724533e-05, mean val. loss:  1.37984514e+00\n",
      "Epoch: 4155 mean train loss:  3.22844717e-05, mean val. loss:  1.38009989e+00\n",
      "Epoch: 4156 mean train loss:  3.26179143e-05, mean val. loss:  1.38034987e+00\n",
      "Epoch: 4157 mean train loss:  3.29292088e-05, mean val. loss:  1.38058913e+00\n",
      "Epoch: 4158 mean train loss:  3.25616857e-05, mean val. loss:  1.38083196e+00\n",
      "Epoch: 4159 mean train loss:  3.23842396e-05, mean val. loss:  1.38107157e+00\n",
      "Epoch: 4160 mean train loss:  3.22387787e-05, mean val. loss:  1.38130236e+00\n",
      "Epoch: 4161 mean train loss:  3.23879358e-05, mean val. loss:  1.38153315e+00\n",
      "Epoch: 4162 mean train loss:  3.21104890e-05, mean val. loss:  1.38176751e+00\n",
      "Epoch: 4163 mean train loss:  3.22093256e-05, mean val. loss:  1.38199496e+00\n",
      "Epoch: 4164 mean train loss:  3.23459681e-05, mean val. loss:  1.38223624e+00\n",
      "Epoch: 4165 mean train loss:  3.22413980e-05, mean val. loss:  1.38247502e+00\n",
      "Epoch: 4166 mean train loss:  3.20486142e-05, mean val. loss:  1.38272774e+00\n",
      "Epoch: 4167 mean train loss:  3.19824321e-05, mean val. loss:  1.38298118e+00\n",
      "Epoch: 4168 mean train loss:  3.15327488e-05, mean val. loss:  1.38325572e+00\n",
      "Epoch: 4169 mean train loss:  3.26866575e-05, mean val. loss:  1.38352823e+00\n",
      "Epoch: 4170 mean train loss:  3.22955311e-05, mean val. loss:  1.38381779e+00\n",
      "Epoch: 4171 mean train loss:  3.19229439e-05, mean val. loss:  1.38410783e+00\n",
      "Epoch: 4172 mean train loss:  3.18918028e-05, mean val. loss:  1.38441205e+00\n",
      "Epoch: 4173 mean train loss:  3.23347049e-05, mean val. loss:  1.38471162e+00\n",
      "Epoch: 4174 mean train loss:  3.25642759e-05, mean val. loss:  1.38501716e+00\n",
      "Epoch: 4175 mean train loss:  3.18739912e-05, mean val. loss:  1.38532877e+00\n",
      "Epoch: 4176 mean train loss:  3.25501023e-05, mean val. loss:  1.38562906e+00\n",
      "Epoch: 4177 mean train loss:  3.19162791e-05, mean val. loss:  1.38593531e+00\n",
      "Epoch: 4178 mean train loss:  3.23514105e-05, mean val. loss:  1.38623631e+00\n",
      "Epoch: 4179 mean train loss:  3.25199217e-05, mean val. loss:  1.38654125e+00\n",
      "Epoch: 4180 mean train loss:  3.27973976e-05, mean val. loss:  1.38683474e+00\n",
      "Epoch: 4181 mean train loss:  3.18523380e-05, mean val. loss:  1.38713968e+00\n",
      "Epoch: 4182 mean train loss:  3.25270521e-05, mean val. loss:  1.38744771e+00\n",
      "Epoch: 4183 mean train loss:  3.20932013e-05, mean val. loss:  1.38774788e+00\n",
      "Epoch: 4184 mean train loss:  3.30769981e-05, mean val. loss:  1.38802421e+00\n",
      "Epoch: 4185 mean train loss:  3.24763532e-05, mean val. loss:  1.38828456e+00\n",
      "Epoch: 4186 mean train loss:  3.21163679e-05, mean val. loss:  1.38854301e+00\n",
      "Epoch: 4187 mean train loss:  3.22322012e-05, mean val. loss:  1.38879514e+00\n",
      "Epoch: 4188 mean train loss:  3.23434651e-05, mean val. loss:  1.38903999e+00\n",
      "Epoch: 4189 mean train loss:  3.23362474e-05, mean val. loss:  1.38927162e+00\n",
      "Epoch: 4190 mean train loss:  3.17470985e-05, mean val. loss:  1.38950181e+00\n",
      "Epoch: 4191 mean train loss:  3.21109546e-05, mean val. loss:  1.38972938e+00\n",
      "Epoch: 4192 mean train loss:  3.21695697e-05, mean val. loss:  1.38995588e+00\n",
      "Epoch: 4193 mean train loss:  3.18493403e-05, mean val. loss:  1.39018726e+00\n",
      "Epoch: 4194 mean train loss:  3.26184090e-05, mean val. loss:  1.39041162e+00\n",
      "Epoch: 4195 mean train loss:  3.26998997e-05, mean val. loss:  1.39062762e+00\n",
      "Epoch: 4196 mean train loss:  3.33402713e-05, mean val. loss:  1.39081860e+00\n",
      "Epoch: 4197 mean train loss:  3.24472785e-05, mean val. loss:  1.39101946e+00\n",
      "Epoch: 4198 mean train loss:  3.19205865e-05, mean val. loss:  1.39122951e+00\n",
      "Epoch: 4199 mean train loss:  3.30083130e-05, mean val. loss:  1.39143491e+00\n",
      "Epoch: 4200 mean train loss:  3.24282737e-05, mean val. loss:  1.39164984e+00\n",
      "Epoch: 4201 mean train loss:  3.26744630e-05, mean val. loss:  1.39185107e+00\n",
      "Epoch: 4202 mean train loss:  3.19408718e-05, mean val. loss:  1.39205146e+00\n",
      "Epoch: 4203 mean train loss:  3.17047816e-05, mean val. loss:  1.39226520e+00\n",
      "Epoch: 4204 mean train loss:  3.25310975e-05, mean val. loss:  1.39248681e+00\n",
      "Epoch: 4205 mean train loss:  3.17369413e-05, mean val. loss:  1.39271975e+00\n",
      "Epoch: 4206 mean train loss:  3.22767009e-05, mean val. loss:  1.39295113e+00\n",
      "Epoch: 4207 mean train loss:  3.22962587e-05, mean val. loss:  1.39318776e+00\n",
      "Epoch: 4208 mean train loss:  3.27723392e-05, mean val. loss:  1.39342391e+00\n",
      "Epoch: 4209 mean train loss:  3.28053138e-05, mean val. loss:  1.39365792e+00\n",
      "Epoch: 4210 mean train loss:  3.22112464e-05, mean val. loss:  1.39389646e+00\n",
      "Epoch: 4211 mean train loss:  3.26289155e-05, mean val. loss:  1.39413118e+00\n",
      "Epoch: 4212 mean train loss:  3.17656377e-05, mean val. loss:  1.39437234e+00\n",
      "Epoch: 4213 mean train loss:  3.26691370e-05, mean val. loss:  1.39461339e+00\n",
      "Epoch: 4214 mean train loss:  3.23910499e-05, mean val. loss:  1.39485168e+00\n",
      "Epoch: 4215 mean train loss:  3.18431994e-05, mean val. loss:  1.39510024e+00\n",
      "Epoch: 4216 mean train loss:  3.22403794e-05, mean val. loss:  1.39536190e+00\n",
      "Epoch: 4217 mean train loss:  3.29726317e-05, mean val. loss:  1.39561903e+00\n",
      "Epoch: 4218 mean train loss:  3.22630221e-05, mean val. loss:  1.39588618e+00\n",
      "Epoch: 4219 mean train loss:  3.20090039e-05, mean val. loss:  1.39617431e+00\n",
      "Epoch: 4220 mean train loss:  3.19402316e-05, mean val. loss:  1.39645600e+00\n",
      "Epoch: 4221 mean train loss:  3.22833657e-05, mean val. loss:  1.39673483e+00\n",
      "Epoch: 4222 mean train loss:  3.24416906e-05, mean val. loss:  1.39701021e+00\n",
      "Epoch: 4223 mean train loss:  3.30404728e-05, mean val. loss:  1.39728904e+00\n",
      "Epoch: 4224 mean train loss:  3.22893320e-05, mean val. loss:  1.39758706e+00\n",
      "Epoch: 4225 mean train loss:  3.18468083e-05, mean val. loss:  1.39789236e+00\n",
      "Epoch: 4226 mean train loss:  3.24552529e-05, mean val. loss:  1.39820206e+00\n",
      "Epoch: 4227 mean train loss:  3.28077003e-05, mean val. loss:  1.39851475e+00\n",
      "Epoch: 4228 mean train loss:  3.15368525e-05, mean val. loss:  1.39883733e+00\n",
      "Epoch: 4229 mean train loss:  3.21358093e-05, mean val. loss:  1.39916456e+00\n",
      "Epoch: 4230 mean train loss:  3.25735309e-05, mean val. loss:  1.39949286e+00\n",
      "Epoch: 4231 mean train loss:  3.22857522e-05, mean val. loss:  1.39982390e+00\n",
      "Epoch: 4232 mean train loss:  3.21127882e-05, mean val. loss:  1.40014851e+00\n",
      "Epoch: 4233 mean train loss:  3.24300490e-05, mean val. loss:  1.40048063e+00\n",
      "Epoch: 4234 mean train loss:  3.22462874e-05, mean val. loss:  1.40081203e+00\n",
      "Epoch: 4235 mean train loss:  3.26654117e-05, mean val. loss:  1.40113556e+00\n",
      "Epoch: 4236 mean train loss:  3.22998967e-05, mean val. loss:  1.40146089e+00\n",
      "Epoch: 4237 mean train loss:  3.16344376e-05, mean val. loss:  1.40178752e+00\n",
      "Epoch: 4238 mean train loss:  3.23545246e-05, mean val. loss:  1.40211415e+00\n",
      "Epoch: 4239 mean train loss:  3.21845000e-05, mean val. loss:  1.40243924e+00\n",
      "Epoch: 4240 mean train loss:  3.23934073e-05, mean val. loss:  1.40276539e+00\n",
      "Epoch: 4241 mean train loss:  3.24148568e-05, mean val. loss:  1.40307379e+00\n",
      "Epoch: 4242 mean train loss:  3.24767898e-05, mean val. loss:  1.40338266e+00\n",
      "Epoch: 4243 mean train loss:  3.18518723e-05, mean val. loss:  1.40368438e+00\n",
      "Epoch: 4244 mean train loss:  3.21635453e-05, mean val. loss:  1.40399289e+00\n",
      "Epoch: 4245 mean train loss:  3.21434054e-05, mean val. loss:  1.40429020e+00\n",
      "Epoch: 4246 mean train loss:  3.21789412e-05, mean val. loss:  1.40458739e+00\n",
      "Epoch: 4247 mean train loss:  3.19890096e-05, mean val. loss:  1.40487170e+00\n",
      "Epoch: 4248 mean train loss:  3.23008862e-05, mean val. loss:  1.40514612e+00\n",
      "Epoch: 4249 mean train loss:  3.28416354e-05, mean val. loss:  1.40540206e+00\n",
      "Epoch: 4250 mean train loss:  3.23115382e-05, mean val. loss:  1.40564156e+00\n",
      "Epoch: 4251 mean train loss:  3.22603446e-05, mean val. loss:  1.40588605e+00\n",
      "Epoch: 4252 mean train loss:  3.20053659e-05, mean val. loss:  1.40612614e+00\n",
      "Epoch: 4253 mean train loss:  3.21794359e-05, mean val. loss:  1.40637064e+00\n",
      "Epoch: 4254 mean train loss:  3.25605797e-05, mean val. loss:  1.40661359e+00\n",
      "Epoch: 4255 mean train loss:  3.22260312e-05, mean val. loss:  1.40685165e+00\n",
      "Epoch: 4256 mean train loss:  3.17525701e-05, mean val. loss:  1.40708256e+00\n",
      "Epoch: 4257 mean train loss:  3.15150828e-05, mean val. loss:  1.40732467e+00\n",
      "Epoch: 4258 mean train loss:  3.17714585e-05, mean val. loss:  1.40756738e+00\n",
      "Epoch: 4259 mean train loss:  3.22583655e-05, mean val. loss:  1.40780640e+00\n",
      "Epoch: 4260 mean train loss:  3.18516395e-05, mean val. loss:  1.40805626e+00\n",
      "Epoch: 4261 mean train loss:  3.15970101e-05, mean val. loss:  1.40830588e+00\n",
      "Epoch: 4262 mean train loss:  3.27341841e-05, mean val. loss:  1.40854681e+00\n",
      "Epoch: 4263 mean train loss:  3.19756509e-05, mean val. loss:  1.40879714e+00\n",
      "Epoch: 4264 mean train loss:  3.23400018e-05, mean val. loss:  1.40903950e+00\n",
      "Epoch: 4265 mean train loss:  3.16219521e-05, mean val. loss:  1.40928543e+00\n",
      "Epoch: 4266 mean train loss:  3.19944520e-05, mean val. loss:  1.40952730e+00\n",
      "Epoch: 4267 mean train loss:  3.22355481e-05, mean val. loss:  1.40978098e+00\n",
      "Epoch: 4268 mean train loss:  3.27931775e-05, mean val. loss:  1.41002500e+00\n",
      "Epoch: 4269 mean train loss:  3.26193986e-05, mean val. loss:  1.41027081e+00\n",
      "Epoch: 4270 mean train loss:  3.23620334e-05, mean val. loss:  1.41051805e+00\n",
      "Epoch: 4271 mean train loss:  3.16677615e-05, mean val. loss:  1.41077864e+00\n",
      "Epoch: 4272 mean train loss:  3.26397421e-05, mean val. loss:  1.41103566e+00\n",
      "Epoch: 4273 mean train loss:  3.19720421e-05, mean val. loss:  1.41129160e+00\n",
      "Epoch: 4274 mean train loss:  3.16009682e-05, mean val. loss:  1.41155756e+00\n",
      "Epoch: 4275 mean train loss:  3.20016698e-05, mean val. loss:  1.41181684e+00\n",
      "Epoch: 4276 mean train loss:  3.23326385e-05, mean val. loss:  1.41207957e+00\n",
      "Epoch: 4277 mean train loss:  3.24684952e-05, mean val. loss:  1.41233575e+00\n",
      "Epoch: 4278 mean train loss:  3.23562999e-05, mean val. loss:  1.41259301e+00\n",
      "Epoch: 4279 mean train loss:  3.23825807e-05, mean val. loss:  1.41285193e+00\n",
      "Epoch: 4280 mean train loss:  3.20250692e-05, mean val. loss:  1.41310942e+00\n",
      "Epoch: 4281 mean train loss:  3.21518455e-05, mean val. loss:  1.41336358e+00\n",
      "Epoch: 4282 mean train loss:  3.14436038e-05, mean val. loss:  1.41362369e+00\n",
      "Epoch: 4283 mean train loss:  3.24507128e-05, mean val. loss:  1.41388369e+00\n",
      "Epoch: 4284 mean train loss:  3.22507403e-05, mean val. loss:  1.41413724e+00\n",
      "Epoch: 4285 mean train loss:  3.27074667e-05, mean val. loss:  1.41439021e+00\n",
      "Epoch: 4286 mean train loss:  3.20757099e-05, mean val. loss:  1.41464305e+00\n",
      "Epoch: 4287 mean train loss:  3.19836545e-05, mean val. loss:  1.41489422e+00\n",
      "Epoch: 4288 mean train loss:  3.22272244e-05, mean val. loss:  1.41513371e+00\n",
      "Epoch: 4289 mean train loss:  3.11949698e-05, mean val. loss:  1.41539490e+00\n",
      "Epoch: 4290 mean train loss:  3.25356086e-05, mean val. loss:  1.41565192e+00\n",
      "Epoch: 4291 mean train loss:  3.25162255e-05, mean val. loss:  1.41590428e+00\n",
      "Epoch: 4292 mean train loss:  3.13378405e-05, mean val. loss:  1.41616595e+00\n",
      "Epoch: 4293 mean train loss:  3.22078704e-05, mean val. loss:  1.41641903e+00\n",
      "Epoch: 4294 mean train loss:  3.17533850e-05, mean val. loss:  1.41668451e+00\n",
      "Epoch: 4295 mean train loss:  3.15372017e-05, mean val. loss:  1.41694546e+00\n",
      "Epoch: 4296 mean train loss:  3.16769874e-05, mean val. loss:  1.41721892e+00\n",
      "Epoch: 4297 mean train loss:  3.17239901e-05, mean val. loss:  1.41749537e+00\n",
      "Epoch: 4298 mean train loss:  3.17020458e-05, mean val. loss:  1.41779315e+00\n",
      "Epoch: 4299 mean train loss:  3.15446814e-05, mean val. loss:  1.41810143e+00\n",
      "Epoch: 4300 mean train loss:  3.18750390e-05, mean val. loss:  1.41840422e+00\n",
      "Epoch: 4301 mean train loss:  3.24186403e-05, mean val. loss:  1.41869700e+00\n",
      "Epoch: 4302 mean train loss:  3.14309727e-05, mean val. loss:  1.41900253e+00\n",
      "Epoch: 4303 mean train loss:  3.23492568e-05, mean val. loss:  1.41931105e+00\n",
      "Epoch: 4304 mean train loss:  3.15492507e-05, mean val. loss:  1.41962731e+00\n",
      "Epoch: 4305 mean train loss:  3.20659892e-05, mean val. loss:  1.41994905e+00\n",
      "Epoch: 4306 mean train loss:  3.23489076e-05, mean val. loss:  1.42027688e+00\n",
      "Epoch: 4307 mean train loss:  3.24044668e-05, mean val. loss:  1.42060983e+00\n",
      "Epoch: 4308 mean train loss:  3.23367422e-05, mean val. loss:  1.42094433e+00\n",
      "Epoch: 4309 mean train loss:  3.16664809e-05, mean val. loss:  1.42128098e+00\n",
      "Epoch: 4310 mean train loss:  3.17625527e-05, mean val. loss:  1.42162454e+00\n",
      "Epoch: 4311 mean train loss:  3.18314997e-05, mean val. loss:  1.42196572e+00\n",
      "Epoch: 4312 mean train loss:  3.19103419e-05, mean val. loss:  1.42231131e+00\n",
      "Epoch: 4313 mean train loss:  3.17459344e-05, mean val. loss:  1.42264903e+00\n",
      "Epoch: 4314 mean train loss:  3.21331318e-05, mean val. loss:  1.42297184e+00\n",
      "Epoch: 4315 mean train loss:  3.21527186e-05, mean val. loss:  1.42327523e+00\n",
      "Epoch: 4316 mean train loss:  3.21399420e-05, mean val. loss:  1.42357552e+00\n",
      "Epoch: 4317 mean train loss:  3.27471062e-05, mean val. loss:  1.42386842e+00\n",
      "Epoch: 4318 mean train loss:  3.18701204e-05, mean val. loss:  1.42416394e+00\n",
      "Epoch: 4319 mean train loss:  3.21251282e-05, mean val. loss:  1.42444372e+00\n",
      "Epoch: 4320 mean train loss:  3.28335154e-05, mean val. loss:  1.42470193e+00\n",
      "Epoch: 4321 mean train loss:  3.20660765e-05, mean val. loss:  1.42493939e+00\n",
      "Epoch: 4322 mean train loss:  3.13105411e-05, mean val. loss:  1.42518806e+00\n",
      "Epoch: 4323 mean train loss:  3.16855439e-05, mean val. loss:  1.42542660e+00\n",
      "Epoch: 4324 mean train loss:  3.15450307e-05, mean val. loss:  1.42567670e+00\n",
      "Epoch: 4325 mean train loss:  3.28488240e-05, mean val. loss:  1.42590547e+00\n",
      "Epoch: 4326 mean train loss:  3.24012362e-05, mean val. loss:  1.42613268e+00\n",
      "Epoch: 4327 mean train loss:  3.21551634e-05, mean val. loss:  1.42635024e+00\n",
      "Epoch: 4328 mean train loss:  3.25528090e-05, mean val. loss:  1.42656898e+00\n",
      "Epoch: 4329 mean train loss:  3.09784373e-05, mean val. loss:  1.42679989e+00\n",
      "Epoch: 4330 mean train loss:  3.28586902e-05, mean val. loss:  1.42702949e+00\n",
      "Epoch: 4331 mean train loss:  3.26769077e-05, mean val. loss:  1.42725301e+00\n",
      "Epoch: 4332 mean train loss:  3.26652080e-05, mean val. loss:  1.42747521e+00\n",
      "Epoch: 4333 mean train loss:  3.21981788e-05, mean val. loss:  1.42769265e+00\n",
      "Epoch: 4334 mean train loss:  3.17327795e-05, mean val. loss:  1.42791057e+00\n",
      "Epoch: 4335 mean train loss:  3.26673617e-05, mean val. loss:  1.42811489e+00\n",
      "Epoch: 4336 mean train loss:  3.16903170e-05, mean val. loss:  1.42833924e+00\n",
      "Epoch: 4337 mean train loss:  3.15949728e-05, mean val. loss:  1.42856967e+00\n",
      "Epoch: 4338 mean train loss:  3.17912782e-05, mean val. loss:  1.42881286e+00\n",
      "Epoch: 4339 mean train loss:  3.24960565e-05, mean val. loss:  1.42905033e+00\n",
      "Epoch: 4340 mean train loss:  3.20975669e-05, mean val. loss:  1.42928612e+00\n",
      "Epoch: 4341 mean train loss:  3.22146807e-05, mean val. loss:  1.42953277e+00\n",
      "Epoch: 4342 mean train loss:  3.18328966e-05, mean val. loss:  1.42977905e+00\n",
      "Epoch: 4343 mean train loss:  3.17828672e-05, mean val. loss:  1.43003500e+00\n",
      "Epoch: 4344 mean train loss:  3.22069682e-05, mean val. loss:  1.43029046e+00\n",
      "Epoch: 4345 mean train loss:  3.22060368e-05, mean val. loss:  1.43055165e+00\n",
      "Epoch: 4346 mean train loss:  3.21145053e-05, mean val. loss:  1.43082142e+00\n",
      "Epoch: 4347 mean train loss:  3.14718636e-05, mean val. loss:  1.43111122e+00\n",
      "Epoch: 4348 mean train loss:  3.23031563e-05, mean val. loss:  1.43139970e+00\n",
      "Epoch: 4349 mean train loss:  3.17138620e-05, mean val. loss:  1.43169880e+00\n",
      "Epoch: 4350 mean train loss:  3.24474822e-05, mean val. loss:  1.43199825e+00\n",
      "Epoch: 4351 mean train loss:  3.22061242e-05, mean val. loss:  1.43229425e+00\n",
      "Epoch: 4352 mean train loss:  3.18690727e-05, mean val. loss:  1.43258655e+00\n",
      "Epoch: 4353 mean train loss:  3.14776553e-05, mean val. loss:  1.43288732e+00\n",
      "Epoch: 4354 mean train loss:  3.15549260e-05, mean val. loss:  1.43318534e+00\n",
      "Epoch: 4355 mean train loss:  3.12822522e-05, mean val. loss:  1.43349242e+00\n",
      "Epoch: 4356 mean train loss:  3.11574258e-05, mean val. loss:  1.43379867e+00\n",
      "Epoch: 4357 mean train loss:  3.17338563e-05, mean val. loss:  1.43410873e+00\n",
      "Epoch: 4358 mean train loss:  3.17727681e-05, mean val. loss:  1.43440580e+00\n",
      "Epoch: 4359 mean train loss:  3.23780987e-05, mean val. loss:  1.43469822e+00\n",
      "Epoch: 4360 mean train loss:  3.22364795e-05, mean val. loss:  1.43499124e+00\n",
      "Epoch: 4361 mean train loss:  3.18513776e-05, mean val. loss:  1.43528020e+00\n",
      "Epoch: 4362 mean train loss:  3.13871424e-05, mean val. loss:  1.43557990e+00\n",
      "Epoch: 4363 mean train loss:  3.11887998e-05, mean val. loss:  1.43589211e+00\n",
      "Epoch: 4364 mean train loss:  3.25877918e-05, mean val. loss:  1.43620205e+00\n",
      "Epoch: 4365 mean train loss:  3.18006787e-05, mean val. loss:  1.43651688e+00\n",
      "Epoch: 4366 mean train loss:  3.25854344e-05, mean val. loss:  1.43682158e+00\n",
      "Epoch: 4367 mean train loss:  3.10693285e-05, mean val. loss:  1.43713248e+00\n",
      "Epoch: 4368 mean train loss:  3.18077218e-05, mean val. loss:  1.43743956e+00\n",
      "Epoch: 4369 mean train loss:  3.15829238e-05, mean val. loss:  1.43775249e+00\n",
      "Epoch: 4370 mean train loss:  3.22795240e-05, mean val. loss:  1.43805683e+00\n",
      "Epoch: 4371 mean train loss:  3.17001541e-05, mean val. loss:  1.43836832e+00\n",
      "Epoch: 4372 mean train loss:  3.13358614e-05, mean val. loss:  1.43867862e+00\n",
      "Epoch: 4373 mean train loss:  3.21650004e-05, mean val. loss:  1.43898964e+00\n",
      "Epoch: 4374 mean train loss:  3.21025145e-05, mean val. loss:  1.43929994e+00\n",
      "Epoch: 4375 mean train loss:  3.16840888e-05, mean val. loss:  1.43961704e+00\n",
      "Epoch: 4376 mean train loss:  3.15891812e-05, mean val. loss:  1.43992925e+00\n",
      "Epoch: 4377 mean train loss:  3.16637161e-05, mean val. loss:  1.44023550e+00\n",
      "Epoch: 4378 mean train loss:  3.19106039e-05, mean val. loss:  1.44053936e+00\n",
      "Epoch: 4379 mean train loss:  3.18527455e-05, mean val. loss:  1.44084537e+00\n",
      "Epoch: 4380 mean train loss:  3.19572864e-05, mean val. loss:  1.44115305e+00\n",
      "Epoch: 4381 mean train loss:  3.20989056e-05, mean val. loss:  1.44144952e+00\n",
      "Epoch: 4382 mean train loss:  3.12621705e-05, mean val. loss:  1.44174266e+00\n",
      "Epoch: 4383 mean train loss:  3.22571432e-05, mean val. loss:  1.44201851e+00\n",
      "Epoch: 4384 mean train loss:  3.16987571e-05, mean val. loss:  1.44229603e+00\n",
      "Epoch: 4385 mean train loss:  3.12793418e-05, mean val. loss:  1.44257009e+00\n",
      "Epoch: 4386 mean train loss:  3.09753232e-05, mean val. loss:  1.44284987e+00\n",
      "Epoch: 4387 mean train loss:  3.12025950e-05, mean val. loss:  1.44313383e+00\n",
      "Epoch: 4388 mean train loss:  3.23520217e-05, mean val. loss:  1.44341159e+00\n",
      "Epoch: 4389 mean train loss:  3.20301624e-05, mean val. loss:  1.44367981e+00\n",
      "Epoch: 4390 mean train loss:  3.22795822e-05, mean val. loss:  1.44395232e+00\n",
      "Epoch: 4391 mean train loss:  3.14554782e-05, mean val. loss:  1.44422221e+00\n",
      "Epoch: 4392 mean train loss:  3.14771605e-05, mean val. loss:  1.44449615e+00\n",
      "Epoch: 4393 mean train loss:  3.17246595e-05, mean val. loss:  1.44478238e+00\n",
      "Epoch: 4394 mean train loss:  3.17589147e-05, mean val. loss:  1.44507182e+00\n",
      "Epoch: 4395 mean train loss:  3.15507932e-05, mean val. loss:  1.44537020e+00\n",
      "Epoch: 4396 mean train loss:  3.18487873e-05, mean val. loss:  1.44567096e+00\n",
      "Epoch: 4397 mean train loss:  3.13141500e-05, mean val. loss:  1.44597363e+00\n",
      "Epoch: 4398 mean train loss:  3.10029718e-05, mean val. loss:  1.44628179e+00\n",
      "Epoch: 4399 mean train loss:  3.19509418e-05, mean val. loss:  1.44659042e+00\n",
      "Epoch: 4400 mean train loss:  3.21734697e-05, mean val. loss:  1.44689286e+00\n",
      "Epoch: 4401 mean train loss:  3.17552476e-05, mean val. loss:  1.44719827e+00\n",
      "Epoch: 4402 mean train loss:  3.14983190e-05, mean val. loss:  1.44751251e+00\n",
      "Epoch: 4403 mean train loss:  3.20061808e-05, mean val. loss:  1.44782674e+00\n",
      "Epoch: 4404 mean train loss:  3.10338801e-05, mean val. loss:  1.44815183e+00\n",
      "Epoch: 4405 mean train loss:  3.19845567e-05, mean val. loss:  1.44848108e+00\n",
      "Epoch: 4406 mean train loss:  3.24156426e-05, mean val. loss:  1.44879520e+00\n",
      "Epoch: 4407 mean train loss:  3.21497500e-05, mean val. loss:  1.44910586e+00\n",
      "Epoch: 4408 mean train loss:  3.15304787e-05, mean val. loss:  1.44941878e+00\n",
      "Epoch: 4409 mean train loss:  3.16075457e-05, mean val. loss:  1.44973171e+00\n",
      "Epoch: 4410 mean train loss:  3.16100195e-05, mean val. loss:  1.45005333e+00\n",
      "Epoch: 4411 mean train loss:  3.12026532e-05, mean val. loss:  1.45037270e+00\n",
      "Epoch: 4412 mean train loss:  3.11572512e-05, mean val. loss:  1.45070314e+00\n",
      "Epoch: 4413 mean train loss:  3.13480850e-05, mean val. loss:  1.45103371e+00\n",
      "Epoch: 4414 mean train loss:  3.20498948e-05, mean val. loss:  1.45136344e+00\n",
      "Epoch: 4415 mean train loss:  3.17330123e-05, mean val. loss:  1.45170045e+00\n",
      "Epoch: 4416 mean train loss:  3.22677370e-05, mean val. loss:  1.45203638e+00\n",
      "Epoch: 4417 mean train loss:  3.17161903e-05, mean val. loss:  1.45236003e+00\n",
      "Epoch: 4418 mean train loss:  3.16785590e-05, mean val. loss:  1.45268166e+00\n",
      "Epoch: 4419 mean train loss:  3.16264632e-05, mean val. loss:  1.45300210e+00\n",
      "Epoch: 4420 mean train loss:  3.12605989e-05, mean val. loss:  1.45333195e+00\n",
      "Epoch: 4421 mean train loss:  3.11420590e-05, mean val. loss:  1.45367396e+00\n",
      "Epoch: 4422 mean train loss:  3.15381330e-05, mean val. loss:  1.45400631e+00\n",
      "Epoch: 4423 mean train loss:  3.20414256e-05, mean val. loss:  1.45433891e+00\n",
      "Epoch: 4424 mean train loss:  3.17244849e-05, mean val. loss:  1.45466280e+00\n",
      "Epoch: 4425 mean train loss:  3.19178798e-05, mean val. loss:  1.45498550e+00\n",
      "Epoch: 4426 mean train loss:  3.12527118e-05, mean val. loss:  1.45530832e+00\n",
      "Epoch: 4427 mean train loss:  3.19939281e-05, mean val. loss:  1.45563257e+00\n",
      "Epoch: 4428 mean train loss:  3.20701802e-05, mean val. loss:  1.45593846e+00\n",
      "Epoch: 4429 mean train loss:  3.17079248e-05, mean val. loss:  1.45622826e+00\n",
      "Epoch: 4430 mean train loss:  3.18797829e-05, mean val. loss:  1.45650661e+00\n",
      "Epoch: 4431 mean train loss:  3.20333347e-05, mean val. loss:  1.45677316e+00\n",
      "Epoch: 4432 mean train loss:  3.14159552e-05, mean val. loss:  1.45704222e+00\n",
      "Epoch: 4433 mean train loss:  3.18715756e-05, mean val. loss:  1.45731735e+00\n",
      "Epoch: 4434 mean train loss:  3.11199983e-05, mean val. loss:  1.45759010e+00\n",
      "Epoch: 4435 mean train loss:  3.10229079e-05, mean val. loss:  1.45786047e+00\n",
      "Epoch: 4436 mean train loss:  3.14839126e-05, mean val. loss:  1.45813489e+00\n",
      "Epoch: 4437 mean train loss:  3.10385658e-05, mean val. loss:  1.45839834e+00\n",
      "Epoch: 4438 mean train loss:  3.15207872e-05, mean val. loss:  1.45866680e+00\n",
      "Epoch: 4439 mean train loss:  3.17653466e-05, mean val. loss:  1.45893264e+00\n",
      "Epoch: 4440 mean train loss:  3.16106016e-05, mean val. loss:  1.45920086e+00\n",
      "Epoch: 4441 mean train loss:  3.17756785e-05, mean val. loss:  1.45946026e+00\n",
      "Epoch: 4442 mean train loss:  3.18356324e-05, mean val. loss:  1.45971298e+00\n",
      "Epoch: 4443 mean train loss:  3.18884850e-05, mean val. loss:  1.45996618e+00\n",
      "Epoch: 4444 mean train loss:  3.12873162e-05, mean val. loss:  1.46022630e+00\n",
      "Epoch: 4445 mean train loss:  3.11655458e-05, mean val. loss:  1.46050477e+00\n",
      "Epoch: 4446 mean train loss:  3.15563229e-05, mean val. loss:  1.46078765e+00\n",
      "Epoch: 4447 mean train loss:  3.19008832e-05, mean val. loss:  1.46106708e+00\n",
      "Epoch: 4448 mean train loss:  3.13282653e-05, mean val. loss:  1.46135926e+00\n",
      "Epoch: 4449 mean train loss:  3.14585632e-05, mean val. loss:  1.46164465e+00\n",
      "Epoch: 4450 mean train loss:  3.11290787e-05, mean val. loss:  1.46195138e+00\n",
      "Epoch: 4451 mean train loss:  3.15788784e-05, mean val. loss:  1.46225452e+00\n",
      "Epoch: 4452 mean train loss:  3.14455247e-05, mean val. loss:  1.46256900e+00\n",
      "Epoch: 4453 mean train loss:  3.13908095e-05, mean val. loss:  1.46289480e+00\n",
      "Epoch: 4454 mean train loss:  3.15048383e-05, mean val. loss:  1.46323347e+00\n",
      "Epoch: 4455 mean train loss:  3.17996310e-05, mean val. loss:  1.46358156e+00\n",
      "Epoch: 4456 mean train loss:  3.15135112e-05, mean val. loss:  1.46394587e+00\n",
      "Epoch: 4457 mean train loss:  3.14631616e-05, mean val. loss:  1.46431363e+00\n",
      "Epoch: 4458 mean train loss:  3.14002682e-05, mean val. loss:  1.46468830e+00\n",
      "Epoch: 4459 mean train loss:  3.09243042e-05, mean val. loss:  1.46508241e+00\n",
      "Epoch: 4460 mean train loss:  3.17486410e-05, mean val. loss:  1.46546376e+00\n",
      "Epoch: 4461 mean train loss:  3.17227095e-05, mean val. loss:  1.46584702e+00\n",
      "Epoch: 4462 mean train loss:  3.12890334e-05, mean val. loss:  1.46624172e+00\n",
      "Epoch: 4463 mean train loss:  3.16940423e-05, mean val. loss:  1.46663570e+00\n",
      "Epoch: 4464 mean train loss:  3.10751493e-05, mean val. loss:  1.46702850e+00\n",
      "Epoch: 4465 mean train loss:  3.22874694e-05, mean val. loss:  1.46741486e+00\n",
      "Epoch: 4466 mean train loss:  3.16537917e-05, mean val. loss:  1.46780276e+00\n",
      "Epoch: 4467 mean train loss:  3.18052480e-05, mean val. loss:  1.46818101e+00\n",
      "Epoch: 4468 mean train loss:  3.17630474e-05, mean val. loss:  1.46854520e+00\n",
      "Epoch: 4469 mean train loss:  3.13734054e-05, mean val. loss:  1.46889484e+00\n",
      "Epoch: 4470 mean train loss:  3.17801023e-05, mean val. loss:  1.46923018e+00\n",
      "Epoch: 4471 mean train loss:  3.17943632e-05, mean val. loss:  1.46955526e+00\n",
      "Epoch: 4472 mean train loss:  3.14292847e-05, mean val. loss:  1.46987319e+00\n",
      "Epoch: 4473 mean train loss:  3.18886596e-05, mean val. loss:  1.47018313e+00\n",
      "Epoch: 4474 mean train loss:  3.14053323e-05, mean val. loss:  1.47049105e+00\n",
      "Epoch: 4475 mean train loss:  3.17812664e-05, mean val. loss:  1.47078633e+00\n",
      "Epoch: 4476 mean train loss:  3.14093777e-05, mean val. loss:  1.47107184e+00\n",
      "Epoch: 4477 mean train loss:  3.12158954e-05, mean val. loss:  1.47134840e+00\n",
      "Epoch: 4478 mean train loss:  3.09416209e-05, mean val. loss:  1.47161996e+00\n",
      "Epoch: 4479 mean train loss:  3.12293705e-05, mean val. loss:  1.47188902e+00\n",
      "Epoch: 4480 mean train loss:  3.10266623e-05, mean val. loss:  1.47215009e+00\n",
      "Epoch: 4481 mean train loss:  3.13751516e-05, mean val. loss:  1.47241700e+00\n",
      "Epoch: 4482 mean train loss:  3.12446500e-05, mean val. loss:  1.47267759e+00\n",
      "Epoch: 4483 mean train loss:  3.08424642e-05, mean val. loss:  1.47295141e+00\n",
      "Epoch: 4484 mean train loss:  3.13656637e-05, mean val. loss:  1.47321975e+00\n",
      "Epoch: 4485 mean train loss:  3.15682846e-05, mean val. loss:  1.47349107e+00\n",
      "Epoch: 4486 mean train loss:  3.12355987e-05, mean val. loss:  1.47376812e+00\n",
      "Epoch: 4487 mean train loss:  3.15085053e-05, mean val. loss:  1.47403443e+00\n",
      "Epoch: 4488 mean train loss:  3.12225602e-05, mean val. loss:  1.47430921e+00\n",
      "Epoch: 4489 mean train loss:  3.07336741e-05, mean val. loss:  1.47458589e+00\n",
      "Epoch: 4490 mean train loss:  3.12553602e-05, mean val. loss:  1.47485173e+00\n",
      "Epoch: 4491 mean train loss:  3.06356815e-05, mean val. loss:  1.47513437e+00\n",
      "Epoch: 4492 mean train loss:  3.11721233e-05, mean val. loss:  1.47542727e+00\n",
      "Epoch: 4493 mean train loss:  3.14346398e-05, mean val. loss:  1.47572589e+00\n",
      "Epoch: 4494 mean train loss:  3.10895557e-05, mean val. loss:  1.47601962e+00\n",
      "Epoch: 4495 mean train loss:  3.18118255e-05, mean val. loss:  1.47632802e+00\n",
      "Epoch: 4496 mean train loss:  3.12659249e-05, mean val. loss:  1.47664177e+00\n",
      "Epoch: 4497 mean train loss:  3.16177611e-05, mean val. loss:  1.47697341e+00\n",
      "Epoch: 4498 mean train loss:  3.10606265e-05, mean val. loss:  1.47730827e+00\n",
      "Epoch: 4499 mean train loss:  3.17851081e-05, mean val. loss:  1.47763979e+00\n",
      "Epoch: 4500 mean train loss:  3.07582377e-05, mean val. loss:  1.47795653e+00\n",
      "Epoch: 4501 mean train loss:  3.19958490e-05, mean val. loss:  1.47828245e+00\n",
      "Epoch: 4502 mean train loss:  3.14099598e-05, mean val. loss:  1.47860420e+00\n",
      "Epoch: 4503 mean train loss:  3.11689801e-05, mean val. loss:  1.47892404e+00\n",
      "Epoch: 4504 mean train loss:  3.14758217e-05, mean val. loss:  1.47924888e+00\n",
      "Epoch: 4505 mean train loss:  3.12185439e-05, mean val. loss:  1.47957122e+00\n",
      "Epoch: 4506 mean train loss:  3.13761411e-05, mean val. loss:  1.47989774e+00\n",
      "Epoch: 4507 mean train loss:  3.12594348e-05, mean val. loss:  1.48022628e+00\n",
      "Epoch: 4508 mean train loss:  3.13399360e-05, mean val. loss:  1.48055017e+00\n",
      "Epoch: 4509 mean train loss:  3.12376942e-05, mean val. loss:  1.48086894e+00\n",
      "Epoch: 4510 mean train loss:  3.18569073e-05, mean val. loss:  1.48118186e+00\n",
      "Epoch: 4511 mean train loss:  3.10787291e-05, mean val. loss:  1.48149824e+00\n",
      "Epoch: 4512 mean train loss:  3.14991630e-05, mean val. loss:  1.48182034e+00\n",
      "Epoch: 4513 mean train loss:  3.15815269e-05, mean val. loss:  1.48214781e+00\n",
      "Epoch: 4514 mean train loss:  3.12483171e-05, mean val. loss:  1.48246884e+00\n",
      "Epoch: 4515 mean train loss:  3.12635093e-05, mean val. loss:  1.48278964e+00\n",
      "Epoch: 4516 mean train loss:  3.08802992e-05, mean val. loss:  1.48311424e+00\n",
      "Epoch: 4517 mean train loss:  3.15031793e-05, mean val. loss:  1.48345268e+00\n",
      "Epoch: 4518 mean train loss:  3.11437470e-05, mean val. loss:  1.48380160e+00\n",
      "Epoch: 4519 mean train loss:  3.17814411e-05, mean val. loss:  1.48414516e+00\n",
      "Epoch: 4520 mean train loss:  3.08747403e-05, mean val. loss:  1.48448265e+00\n",
      "Epoch: 4521 mean train loss:  3.12629272e-05, mean val. loss:  1.48482144e+00\n",
      "Epoch: 4522 mean train loss:  3.08760209e-05, mean val. loss:  1.48515737e+00\n",
      "Epoch: 4523 mean train loss:  3.15965444e-05, mean val. loss:  1.48550642e+00\n",
      "Epoch: 4524 mean train loss:  3.11529730e-05, mean val. loss:  1.48585522e+00\n",
      "Epoch: 4525 mean train loss:  3.18541424e-05, mean val. loss:  1.48620391e+00\n",
      "Epoch: 4526 mean train loss:  3.16613587e-05, mean val. loss:  1.48654902e+00\n",
      "Epoch: 4527 mean train loss:  3.14209610e-05, mean val. loss:  1.48689723e+00\n",
      "Epoch: 4528 mean train loss:  3.13753844e-05, mean val. loss:  1.48724389e+00\n",
      "Epoch: 4529 mean train loss:  3.19554238e-05, mean val. loss:  1.48757493e+00\n",
      "Epoch: 4530 mean train loss:  3.13742494e-05, mean val. loss:  1.48789442e+00\n",
      "Epoch: 4531 mean train loss:  3.09679599e-05, mean val. loss:  1.48820746e+00\n",
      "Epoch: 4532 mean train loss:  3.16352816e-05, mean val. loss:  1.48850656e+00\n",
      "Epoch: 4533 mean train loss:  3.06332659e-05, mean val. loss:  1.48879945e+00\n",
      "Epoch: 4534 mean train loss:  3.09785537e-05, mean val. loss:  1.48908639e+00\n",
      "Epoch: 4535 mean train loss:  3.17150552e-05, mean val. loss:  1.48935986e+00\n",
      "Epoch: 4536 mean train loss:  3.16418591e-05, mean val. loss:  1.48963141e+00\n",
      "Epoch: 4537 mean train loss:  3.12226184e-05, mean val. loss:  1.48989320e+00\n",
      "Epoch: 4538 mean train loss:  3.20193358e-05, mean val. loss:  1.49015391e+00\n",
      "Epoch: 4539 mean train loss:  3.11390904e-05, mean val. loss:  1.49041784e+00\n",
      "Epoch: 4540 mean train loss:  3.11509357e-05, mean val. loss:  1.49067056e+00\n",
      "Epoch: 4541 mean train loss:  3.17739905e-05, mean val. loss:  1.49092126e+00\n",
      "Epoch: 4542 mean train loss:  3.12541961e-05, mean val. loss:  1.49116480e+00\n",
      "Epoch: 4543 mean train loss:  3.14192148e-05, mean val. loss:  1.49140191e+00\n",
      "Epoch: 4544 mean train loss:  3.16643273e-05, mean val. loss:  1.49163568e+00\n",
      "Epoch: 4545 mean train loss:  3.11058539e-05, mean val. loss:  1.49187136e+00\n",
      "Epoch: 4546 mean train loss:  3.12352495e-05, mean val. loss:  1.49210966e+00\n",
      "Epoch: 4547 mean train loss:  3.11409822e-05, mean val. loss:  1.49234974e+00\n",
      "Epoch: 4548 mean train loss:  3.12428165e-05, mean val. loss:  1.49259937e+00\n",
      "Epoch: 4549 mean train loss:  3.15515499e-05, mean val. loss:  1.49286211e+00\n",
      "Epoch: 4550 mean train loss:  3.18814709e-05, mean val. loss:  1.49312854e+00\n",
      "Epoch: 4551 mean train loss:  3.13727651e-05, mean val. loss:  1.49340224e+00\n",
      "Epoch: 4552 mean train loss:  3.15590005e-05, mean val. loss:  1.49369097e+00\n",
      "Epoch: 4553 mean train loss:  3.17294034e-05, mean val. loss:  1.49397135e+00\n",
      "Epoch: 4554 mean train loss:  3.14184290e-05, mean val. loss:  1.49426138e+00\n",
      "Epoch: 4555 mean train loss:  3.05776775e-05, mean val. loss:  1.49455142e+00\n",
      "Epoch: 4556 mean train loss:  3.11304757e-05, mean val. loss:  1.49484432e+00\n",
      "Epoch: 4557 mean train loss:  3.11876647e-05, mean val. loss:  1.49514878e+00\n",
      "Epoch: 4558 mean train loss:  3.13818746e-05, mean val. loss:  1.49545574e+00\n",
      "Epoch: 4559 mean train loss:  3.14503268e-05, mean val. loss:  1.49576879e+00\n",
      "Epoch: 4560 mean train loss:  3.10713076e-05, mean val. loss:  1.49608505e+00\n",
      "Epoch: 4561 mean train loss:  3.07609444e-05, mean val. loss:  1.49641073e+00\n",
      "Epoch: 4562 mean train loss:  3.11218610e-05, mean val. loss:  1.49673307e+00\n",
      "Epoch: 4563 mean train loss:  3.13228520e-05, mean val. loss:  1.49705315e+00\n",
      "Epoch: 4564 mean train loss:  3.20472172e-05, mean val. loss:  1.49736667e+00\n",
      "Epoch: 4565 mean train loss:  3.12931079e-05, mean val. loss:  1.49767601e+00\n",
      "Epoch: 4566 mean train loss:  3.15808866e-05, mean val. loss:  1.49800086e+00\n",
      "Epoch: 4567 mean train loss:  3.14973004e-05, mean val. loss:  1.49832022e+00\n",
      "Epoch: 4568 mean train loss:  3.14536737e-05, mean val. loss:  1.49863350e+00\n",
      "Epoch: 4569 mean train loss:  3.20336840e-05, mean val. loss:  1.49892557e+00\n",
      "Epoch: 4570 mean train loss:  3.09353636e-05, mean val. loss:  1.49921143e+00\n",
      "Epoch: 4571 mean train loss:  3.13786732e-05, mean val. loss:  1.49950230e+00\n",
      "Epoch: 4572 mean train loss:  3.09059396e-05, mean val. loss:  1.49978018e+00\n",
      "Epoch: 4573 mean train loss:  3.11108597e-05, mean val. loss:  1.50005138e+00\n",
      "Epoch: 4574 mean train loss:  3.07699083e-05, mean val. loss:  1.50031781e+00\n",
      "Epoch: 4575 mean train loss:  3.20207037e-05, mean val. loss:  1.50057495e+00\n",
      "Epoch: 4576 mean train loss:  3.11688345e-05, mean val. loss:  1.50082898e+00\n",
      "Epoch: 4577 mean train loss:  3.14251520e-05, mean val. loss:  1.50108016e+00\n",
      "Epoch: 4578 mean train loss:  3.11937765e-05, mean val. loss:  1.50132310e+00\n",
      "Epoch: 4579 mean train loss:  3.03966226e-05, mean val. loss:  1.50156724e+00\n",
      "Epoch: 4580 mean train loss:  3.08387971e-05, mean val. loss:  1.50180399e+00\n",
      "Epoch: 4581 mean train loss:  3.16957885e-05, mean val. loss:  1.50203526e+00\n",
      "Epoch: 4582 mean train loss:  3.08917079e-05, mean val. loss:  1.50226212e+00\n",
      "Epoch: 4583 mean train loss:  3.12924094e-05, mean val. loss:  1.50249445e+00\n",
      "Epoch: 4584 mean train loss:  3.14401404e-05, mean val. loss:  1.50273347e+00\n",
      "Epoch: 4585 mean train loss:  3.11470067e-05, mean val. loss:  1.50297916e+00\n",
      "Epoch: 4586 mean train loss:  3.13727360e-05, mean val. loss:  1.50322402e+00\n",
      "Epoch: 4587 mean train loss:  3.14404315e-05, mean val. loss:  1.50347662e+00\n",
      "Epoch: 4588 mean train loss:  3.11428448e-05, mean val. loss:  1.50373733e+00\n",
      "Epoch: 4589 mean train loss:  3.07146402e-05, mean val. loss:  1.50401151e+00\n",
      "Epoch: 4590 mean train loss:  3.15370853e-05, mean val. loss:  1.50429189e+00\n",
      "Epoch: 4591 mean train loss:  3.12863558e-05, mean val. loss:  1.50457132e+00\n",
      "Epoch: 4592 mean train loss:  3.06940055e-05, mean val. loss:  1.50485969e+00\n",
      "Epoch: 4593 mean train loss:  3.14774516e-05, mean val. loss:  1.50514889e+00\n",
      "Epoch: 4594 mean train loss:  3.11670301e-05, mean val. loss:  1.50544071e+00\n",
      "Epoch: 4595 mean train loss:  3.15890356e-05, mean val. loss:  1.50574529e+00\n",
      "Epoch: 4596 mean train loss:  3.10413307e-05, mean val. loss:  1.50605893e+00\n",
      "Epoch: 4597 mean train loss:  3.06806760e-05, mean val. loss:  1.50639725e+00\n",
      "Epoch: 4598 mean train loss:  3.07605660e-05, mean val. loss:  1.50673485e+00\n",
      "Epoch: 4599 mean train loss:  3.17613303e-05, mean val. loss:  1.50706792e+00\n",
      "Epoch: 4600 mean train loss:  3.10282339e-05, mean val. loss:  1.50740910e+00\n",
      "Epoch: 4601 mean train loss:  3.11931653e-05, mean val. loss:  1.50776279e+00\n",
      "Epoch: 4602 mean train loss:  3.18260863e-05, mean val. loss:  1.50811934e+00\n",
      "Epoch: 4603 mean train loss:  3.09221214e-05, mean val. loss:  1.50848484e+00\n",
      "Epoch: 4604 mean train loss:  3.07563460e-05, mean val. loss:  1.50885594e+00\n",
      "Epoch: 4605 mean train loss:  2.98297673e-05, mean val. loss:  1.50924337e+00\n",
      "Epoch: 4606 mean train loss:  3.14828940e-05, mean val. loss:  1.50961781e+00\n",
      "Epoch: 4607 mean train loss:  3.12452321e-05, mean val. loss:  1.50999498e+00\n",
      "Epoch: 4608 mean train loss:  3.13783530e-05, mean val. loss:  1.51035869e+00\n",
      "Epoch: 4609 mean train loss:  3.12846969e-05, mean val. loss:  1.51072681e+00\n",
      "Epoch: 4610 mean train loss:  3.11265467e-05, mean val. loss:  1.51108694e+00\n",
      "Epoch: 4611 mean train loss:  3.07880691e-05, mean val. loss:  1.51144588e+00\n",
      "Epoch: 4612 mean train loss:  3.13744531e-05, mean val. loss:  1.51180339e+00\n",
      "Epoch: 4613 mean train loss:  3.13487544e-05, mean val. loss:  1.51215112e+00\n",
      "Epoch: 4614 mean train loss:  3.11754411e-05, mean val. loss:  1.51249325e+00\n",
      "Epoch: 4615 mean train loss:  3.10052710e-05, mean val. loss:  1.51282012e+00\n",
      "Epoch: 4616 mean train loss:  3.05199646e-05, mean val. loss:  1.51314366e+00\n",
      "Epoch: 4617 mean train loss:  3.17163649e-05, mean val. loss:  1.51345325e+00\n",
      "Epoch: 4618 mean train loss:  3.10075120e-05, mean val. loss:  1.51375628e+00\n",
      "Epoch: 4619 mean train loss:  3.13613855e-05, mean val. loss:  1.51405466e+00\n",
      "Epoch: 4620 mean train loss:  3.09440366e-05, mean val. loss:  1.51434517e+00\n",
      "Epoch: 4621 mean train loss:  3.08537565e-05, mean val. loss:  1.51464474e+00\n",
      "Epoch: 4622 mean train loss:  3.12859484e-05, mean val. loss:  1.51493895e+00\n",
      "Epoch: 4623 mean train loss:  3.16734368e-05, mean val. loss:  1.51523566e+00\n",
      "Epoch: 4624 mean train loss:  3.09385068e-05, mean val. loss:  1.51553166e+00\n",
      "Epoch: 4625 mean train loss:  3.14719218e-05, mean val. loss:  1.51582098e+00\n",
      "Epoch: 4626 mean train loss:  3.10582109e-05, mean val. loss:  1.51610732e+00\n",
      "Epoch: 4627 mean train loss:  3.12289631e-05, mean val. loss:  1.51638103e+00\n",
      "Epoch: 4628 mean train loss:  3.10651667e-05, mean val. loss:  1.51665878e+00\n",
      "Epoch: 4629 mean train loss:  3.15665384e-05, mean val. loss:  1.51693261e+00\n",
      "Epoch: 4630 mean train loss:  3.17085942e-05, mean val. loss:  1.51720059e+00\n",
      "Epoch: 4631 mean train loss:  3.03157722e-05, mean val. loss:  1.51747525e+00\n",
      "Epoch: 4632 mean train loss:  3.12066695e-05, mean val. loss:  1.51776886e+00\n",
      "Epoch: 4633 mean train loss:  3.09042516e-05, mean val. loss:  1.51806974e+00\n",
      "Epoch: 4634 mean train loss:  3.13282071e-05, mean val. loss:  1.51837611e+00\n",
      "Epoch: 4635 mean train loss:  3.11266922e-05, mean val. loss:  1.51868486e+00\n",
      "Epoch: 4636 mean train loss:  3.10860923e-05, mean val. loss:  1.51900113e+00\n",
      "Epoch: 4637 mean train loss:  3.07001173e-05, mean val. loss:  1.51933348e+00\n",
      "Epoch: 4638 mean train loss:  3.09147581e-05, mean val. loss:  1.51967669e+00\n",
      "Epoch: 4639 mean train loss:  3.07385344e-05, mean val. loss:  1.52002585e+00\n",
      "Epoch: 4640 mean train loss:  3.09622556e-05, mean val. loss:  1.52038264e+00\n",
      "Epoch: 4641 mean train loss:  3.14073986e-05, mean val. loss:  1.52074051e+00\n",
      "Epoch: 4642 mean train loss:  3.08079179e-05, mean val. loss:  1.52110982e+00\n",
      "Epoch: 4643 mean train loss:  3.08641756e-05, mean val. loss:  1.52148318e+00\n",
      "Epoch: 4644 mean train loss:  3.11273907e-05, mean val. loss:  1.52185357e+00\n",
      "Epoch: 4645 mean train loss:  3.14282370e-05, mean val. loss:  1.52222943e+00\n",
      "Epoch: 4646 mean train loss:  3.05631838e-05, mean val. loss:  1.52259827e+00\n",
      "Epoch: 4647 mean train loss:  3.11107724e-05, mean val. loss:  1.52296841e+00\n",
      "Epoch: 4648 mean train loss:  3.08103627e-05, mean val. loss:  1.52333486e+00\n",
      "Epoch: 4649 mean train loss:  3.11811455e-05, mean val. loss:  1.52370656e+00\n",
      "Epoch: 4650 mean train loss:  3.10675823e-05, mean val. loss:  1.52408350e+00\n",
      "Epoch: 4651 mean train loss:  3.08801536e-05, mean val. loss:  1.52445841e+00\n",
      "Epoch: 4652 mean train loss:  3.03835841e-05, mean val. loss:  1.52484572e+00\n",
      "Epoch: 4653 mean train loss:  3.13041091e-05, mean val. loss:  1.52523971e+00\n",
      "Epoch: 4654 mean train loss:  3.06666479e-05, mean val. loss:  1.52563226e+00\n",
      "Epoch: 4655 mean train loss:  3.11433396e-05, mean val. loss:  1.52602065e+00\n",
      "Epoch: 4656 mean train loss:  3.08743329e-05, mean val. loss:  1.52640688e+00\n",
      "Epoch: 4657 mean train loss:  3.15090001e-05, mean val. loss:  1.52678430e+00\n",
      "Epoch: 4658 mean train loss:  3.14300705e-05, mean val. loss:  1.52715445e+00\n",
      "Epoch: 4659 mean train loss:  3.13935452e-05, mean val. loss:  1.52750540e+00\n",
      "Epoch: 4660 mean train loss:  3.06913862e-05, mean val. loss:  1.52784431e+00\n",
      "Epoch: 4661 mean train loss:  3.05881549e-05, mean val. loss:  1.52817345e+00\n",
      "Epoch: 4662 mean train loss:  3.10381292e-05, mean val. loss:  1.52849770e+00\n",
      "Epoch: 4663 mean train loss:  3.09006718e-05, mean val. loss:  1.52880800e+00\n",
      "Epoch: 4664 mean train loss:  3.09308234e-05, mean val. loss:  1.52910829e+00\n",
      "Epoch: 4665 mean train loss:  3.05165595e-05, mean val. loss:  1.52940691e+00\n",
      "Epoch: 4666 mean train loss:  3.08649905e-05, mean val. loss:  1.52969658e+00\n",
      "Epoch: 4667 mean train loss:  3.07366427e-05, mean val. loss:  1.52999425e+00\n",
      "Epoch: 4668 mean train loss:  3.13007331e-05, mean val. loss:  1.53028893e+00\n",
      "Epoch: 4669 mean train loss:  3.11058247e-05, mean val. loss:  1.53057063e+00\n",
      "Epoch: 4670 mean train loss:  3.15257348e-05, mean val. loss:  1.53085160e+00\n",
      "Epoch: 4671 mean train loss:  3.06049769e-05, mean val. loss:  1.53113091e+00\n",
      "Epoch: 4672 mean train loss:  3.06904258e-05, mean val. loss:  1.53141940e+00\n",
      "Epoch: 4673 mean train loss:  3.14067292e-05, mean val. loss:  1.53170311e+00\n",
      "Epoch: 4674 mean train loss:  3.13997734e-05, mean val. loss:  1.53197992e+00\n",
      "Epoch: 4675 mean train loss:  3.10996547e-05, mean val. loss:  1.53224874e+00\n",
      "Epoch: 4676 mean train loss:  3.06791626e-05, mean val. loss:  1.53251803e+00\n",
      "Epoch: 4677 mean train loss:  3.05302092e-05, mean val. loss:  1.53279519e+00\n",
      "Epoch: 4678 mean train loss:  3.03150737e-05, mean val. loss:  1.53308642e+00\n",
      "Epoch: 4679 mean train loss:  3.07633891e-05, mean val. loss:  1.53337669e+00\n",
      "Epoch: 4680 mean train loss:  3.13952914e-05, mean val. loss:  1.53366435e+00\n",
      "Epoch: 4681 mean train loss:  3.08434828e-05, mean val. loss:  1.53394759e+00\n",
      "Epoch: 4682 mean train loss:  3.15156940e-05, mean val. loss:  1.53424561e+00\n",
      "Epoch: 4683 mean train loss:  3.08051240e-05, mean val. loss:  1.53454387e+00\n",
      "Epoch: 4684 mean train loss:  3.06064903e-05, mean val. loss:  1.53485954e+00\n",
      "Epoch: 4685 mean train loss:  3.03261622e-05, mean val. loss:  1.53517222e+00\n",
      "Epoch: 4686 mean train loss:  3.08109738e-05, mean val. loss:  1.53549778e+00\n",
      "Epoch: 4687 mean train loss:  3.05689173e-05, mean val. loss:  1.53583384e+00\n",
      "Epoch: 4688 mean train loss:  3.05503781e-05, mean val. loss:  1.53617513e+00\n",
      "Epoch: 4689 mean train loss:  3.07608861e-05, mean val. loss:  1.53652239e+00\n",
      "Epoch: 4690 mean train loss:  3.06495349e-05, mean val. loss:  1.53688610e+00\n",
      "Epoch: 4691 mean train loss:  3.08103336e-05, mean val. loss:  1.53725684e+00\n",
      "Epoch: 4692 mean train loss:  3.10829782e-05, mean val. loss:  1.53763425e+00\n",
      "Epoch: 4693 mean train loss:  3.08985182e-05, mean val. loss:  1.53802526e+00\n",
      "Epoch: 4694 mean train loss:  3.05465655e-05, mean val. loss:  1.53843284e+00\n",
      "Epoch: 4695 mean train loss:  3.04789282e-05, mean val. loss:  1.53882670e+00\n",
      "Epoch: 4696 mean train loss:  3.12841439e-05, mean val. loss:  1.53921425e+00\n",
      "Epoch: 4697 mean train loss:  3.02630942e-05, mean val. loss:  1.53960538e+00\n",
      "Epoch: 4698 mean train loss:  3.12309130e-05, mean val. loss:  1.53999901e+00\n",
      "Epoch: 4699 mean train loss:  3.08727904e-05, mean val. loss:  1.54039347e+00\n",
      "Epoch: 4700 mean train loss:  3.09437164e-05, mean val. loss:  1.54079473e+00\n",
      "Epoch: 4701 mean train loss:  3.04627756e-05, mean val. loss:  1.54120255e+00\n",
      "Epoch: 4702 mean train loss:  3.07955779e-05, mean val. loss:  1.54160488e+00\n",
      "Epoch: 4703 mean train loss:  3.05456924e-05, mean val. loss:  1.54199839e+00\n",
      "Epoch: 4704 mean train loss:  3.09747411e-05, mean val. loss:  1.54240406e+00\n",
      "Epoch: 4705 mean train loss:  3.08955496e-05, mean val. loss:  1.54281032e+00\n",
      "Epoch: 4706 mean train loss:  3.08299495e-05, mean val. loss:  1.54321432e+00\n",
      "Epoch: 4707 mean train loss:  3.09894676e-05, mean val. loss:  1.54361725e+00\n",
      "Epoch: 4708 mean train loss:  3.02635017e-05, mean val. loss:  1.54401481e+00\n",
      "Epoch: 4709 mean train loss:  3.07694427e-05, mean val. loss:  1.54440558e+00\n",
      "Epoch: 4710 mean train loss:  3.06846923e-05, mean val. loss:  1.54479504e+00\n",
      "Epoch: 4711 mean train loss:  3.09547468e-05, mean val. loss:  1.54517233e+00\n",
      "Epoch: 4712 mean train loss:  3.05859721e-05, mean val. loss:  1.54554057e+00\n",
      "Epoch: 4713 mean train loss:  3.06135335e-05, mean val. loss:  1.54590571e+00\n",
      "Epoch: 4714 mean train loss:  3.05596332e-05, mean val. loss:  1.54626048e+00\n",
      "Epoch: 4715 mean train loss:  3.10585601e-05, mean val. loss:  1.54660213e+00\n",
      "Epoch: 4716 mean train loss:  3.05631838e-05, mean val. loss:  1.54693532e+00\n",
      "Epoch: 4717 mean train loss:  3.09212774e-05, mean val. loss:  1.54725993e+00\n",
      "Epoch: 4718 mean train loss:  3.01498803e-05, mean val. loss:  1.54758561e+00\n",
      "Epoch: 4719 mean train loss:  3.11014592e-05, mean val. loss:  1.54790366e+00\n",
      "Epoch: 4720 mean train loss:  3.06805596e-05, mean val. loss:  1.54821444e+00\n",
      "Epoch: 4721 mean train loss:  3.05903668e-05, mean val. loss:  1.54852200e+00\n",
      "Epoch: 4722 mean train loss:  3.05606518e-05, mean val. loss:  1.54882109e+00\n",
      "Epoch: 4723 mean train loss:  3.07055307e-05, mean val. loss:  1.54911888e+00\n",
      "Epoch: 4724 mean train loss:  3.08204908e-05, mean val. loss:  1.54941750e+00\n",
      "Epoch: 4725 mean train loss:  3.06499423e-05, mean val. loss:  1.54972219e+00\n",
      "Epoch: 4726 mean train loss:  3.09920288e-05, mean val. loss:  1.55003214e+00\n",
      "Epoch: 4727 mean train loss:  3.04277346e-05, mean val. loss:  1.55033898e+00\n",
      "Epoch: 4728 mean train loss:  3.17216618e-05, mean val. loss:  1.55063152e+00\n",
      "Epoch: 4729 mean train loss:  3.08956369e-05, mean val. loss:  1.55092716e+00\n",
      "Epoch: 4730 mean train loss:  3.04899586e-05, mean val. loss:  1.55124342e+00\n",
      "Epoch: 4731 mean train loss:  3.05770664e-05, mean val. loss:  1.55156124e+00\n",
      "Epoch: 4732 mean train loss:  3.04231944e-05, mean val. loss:  1.55188489e+00\n",
      "Epoch: 4733 mean train loss:  3.07049486e-05, mean val. loss:  1.55221152e+00\n",
      "Epoch: 4734 mean train loss:  3.09746538e-05, mean val. loss:  1.55254364e+00\n",
      "Epoch: 4735 mean train loss:  3.08497401e-05, mean val. loss:  1.55288327e+00\n",
      "Epoch: 4736 mean train loss:  3.04307614e-05, mean val. loss:  1.55324090e+00\n",
      "Epoch: 4737 mean train loss:  3.01322725e-05, mean val. loss:  1.55359769e+00\n",
      "Epoch: 4738 mean train loss:  3.03020352e-05, mean val. loss:  1.55395997e+00\n",
      "Epoch: 4739 mean train loss:  3.10727046e-05, mean val. loss:  1.55432141e+00\n",
      "Epoch: 4740 mean train loss:  3.13047785e-05, mean val. loss:  1.55467033e+00\n",
      "Epoch: 4741 mean train loss:  3.11945041e-05, mean val. loss:  1.55500710e+00\n",
      "Epoch: 4742 mean train loss:  3.06211296e-05, mean val. loss:  1.55534148e+00\n",
      "Epoch: 4743 mean train loss:  3.07643204e-05, mean val. loss:  1.55566669e+00\n",
      "Epoch: 4744 mean train loss:  3.00937390e-05, mean val. loss:  1.55598950e+00\n",
      "Epoch: 4745 mean train loss:  3.12167685e-05, mean val. loss:  1.55631495e+00\n",
      "Epoch: 4746 mean train loss:  3.07555019e-05, mean val. loss:  1.55663311e+00\n",
      "Epoch: 4747 mean train loss:  3.09860625e-05, mean val. loss:  1.55694687e+00\n",
      "Epoch: 4748 mean train loss:  3.06847214e-05, mean val. loss:  1.55726230e+00\n",
      "Epoch: 4749 mean train loss:  3.05628928e-05, mean val. loss:  1.55757523e+00\n",
      "Epoch: 4750 mean train loss:  3.03182460e-05, mean val. loss:  1.55788469e+00\n",
      "Epoch: 4751 mean train loss:  3.10050382e-05, mean val. loss:  1.55818510e+00\n",
      "Epoch: 4752 mean train loss:  3.05237190e-05, mean val. loss:  1.55848897e+00\n",
      "Epoch: 4753 mean train loss:  3.04032001e-05, mean val. loss:  1.55879474e+00\n",
      "Epoch: 4754 mean train loss:  3.07257287e-05, mean val. loss:  1.55909741e+00\n",
      "Epoch: 4755 mean train loss:  3.08468880e-05, mean val. loss:  1.55940962e+00\n",
      "Epoch: 4756 mean train loss:  3.02373664e-05, mean val. loss:  1.55972600e+00\n",
      "Epoch: 4757 mean train loss:  3.11575714e-05, mean val. loss:  1.56004357e+00\n",
      "Epoch: 4758 mean train loss:  3.07510491e-05, mean val. loss:  1.56037033e+00\n",
      "Epoch: 4759 mean train loss:  3.00605607e-05, mean val. loss:  1.56070828e+00\n",
      "Epoch: 4760 mean train loss:  3.07284936e-05, mean val. loss:  1.56104743e+00\n",
      "Epoch: 4761 mean train loss:  3.07303562e-05, mean val. loss:  1.56138194e+00\n",
      "Epoch: 4762 mean train loss:  3.07255250e-05, mean val. loss:  1.56171179e+00\n",
      "Epoch: 4763 mean train loss:  3.03646375e-05, mean val. loss:  1.56203985e+00\n",
      "Epoch: 4764 mean train loss:  3.04849818e-05, mean val. loss:  1.56236625e+00\n",
      "Epoch: 4765 mean train loss:  3.03198467e-05, mean val. loss:  1.56269133e+00\n",
      "Epoch: 4766 mean train loss:  3.05947033e-05, mean val. loss:  1.56301498e+00\n",
      "Epoch: 4767 mean train loss:  3.05731664e-05, mean val. loss:  1.56334245e+00\n",
      "Epoch: 4768 mean train loss:  3.07716837e-05, mean val. loss:  1.56366527e+00\n",
      "Epoch: 4769 mean train loss:  3.08007002e-05, mean val. loss:  1.56398284e+00\n",
      "Epoch: 4770 mean train loss:  3.07583541e-05, mean val. loss:  1.56429899e+00\n",
      "Epoch: 4771 mean train loss:  3.02905100e-05, mean val. loss:  1.56461382e+00\n",
      "Epoch: 4772 mean train loss:  3.08224116e-05, mean val. loss:  1.56492710e+00\n",
      "Epoch: 4773 mean train loss:  3.03703127e-05, mean val. loss:  1.56524587e+00\n",
      "Epoch: 4774 mean train loss:  3.11923504e-05, mean val. loss:  1.56556118e+00\n",
      "Epoch: 4775 mean train loss:  3.14944191e-05, mean val. loss:  1.56587219e+00\n",
      "Epoch: 4776 mean train loss:  3.07853916e-05, mean val. loss:  1.56618750e+00\n",
      "Epoch: 4777 mean train loss:  3.05842259e-05, mean val. loss:  1.56650186e+00\n",
      "Epoch: 4778 mean train loss:  3.09970055e-05, mean val. loss:  1.56681478e+00\n",
      "Epoch: 4779 mean train loss:  3.07930459e-05, mean val. loss:  1.56712902e+00\n",
      "Epoch: 4780 mean train loss:  3.07484588e-05, mean val. loss:  1.56745219e+00\n",
      "Epoch: 4781 mean train loss:  3.08559102e-05, mean val. loss:  1.56776655e+00\n",
      "Epoch: 4782 mean train loss:  3.05739231e-05, mean val. loss:  1.56807888e+00\n",
      "Epoch: 4783 mean train loss:  3.15140351e-05, mean val. loss:  1.56838739e+00\n",
      "Epoch: 4784 mean train loss:  3.02424596e-05, mean val. loss:  1.56870580e+00\n",
      "Epoch: 4785 mean train loss:  2.97778170e-05, mean val. loss:  1.56903327e+00\n",
      "Epoch: 4786 mean train loss:  3.07548326e-05, mean val. loss:  1.56936193e+00\n",
      "Epoch: 4787 mean train loss:  3.04935384e-05, mean val. loss:  1.56969416e+00\n",
      "Epoch: 4788 mean train loss:  3.06429865e-05, mean val. loss:  1.57003641e+00\n",
      "Epoch: 4789 mean train loss:  3.08327726e-05, mean val. loss:  1.57036734e+00\n",
      "Epoch: 4790 mean train loss:  3.12844058e-05, mean val. loss:  1.57070470e+00\n",
      "Epoch: 4791 mean train loss:  3.12727934e-05, mean val. loss:  1.57104027e+00\n",
      "Epoch: 4792 mean train loss:  3.14114732e-05, mean val. loss:  1.57138526e+00\n",
      "Epoch: 4793 mean train loss:  3.08456074e-05, mean val. loss:  1.57173455e+00\n",
      "Epoch: 4794 mean train loss:  3.05342255e-05, mean val. loss:  1.57208383e+00\n",
      "Epoch: 4795 mean train loss:  3.06565780e-05, mean val. loss:  1.57242000e+00\n",
      "Epoch: 4796 mean train loss:  3.09159805e-05, mean val. loss:  1.57276094e+00\n",
      "Epoch: 4797 mean train loss:  3.03197885e-05, mean val. loss:  1.57309735e+00\n",
      "Epoch: 4798 mean train loss:  3.05961294e-05, mean val. loss:  1.57341683e+00\n",
      "Epoch: 4799 mean train loss:  3.06650472e-05, mean val. loss:  1.57373202e+00\n",
      "Epoch: 4800 mean train loss:  3.02862609e-05, mean val. loss:  1.57405388e+00\n",
      "Epoch: 4801 mean train loss:  3.04375135e-05, mean val. loss:  1.57436669e+00\n",
      "Epoch: 4802 mean train loss:  3.11136246e-05, mean val. loss:  1.57466221e+00\n",
      "Epoch: 4803 mean train loss:  3.03088746e-05, mean val. loss:  1.57494414e+00\n",
      "Epoch: 4804 mean train loss:  2.98589002e-05, mean val. loss:  1.57522631e+00\n",
      "Epoch: 4805 mean train loss:  3.07189766e-05, mean val. loss:  1.57551420e+00\n",
      "Epoch: 4806 mean train loss:  3.07866721e-05, mean val. loss:  1.57578802e+00\n",
      "Epoch: 4807 mean train loss:  3.00164975e-05, mean val. loss:  1.57606792e+00\n",
      "Epoch: 4808 mean train loss:  3.05323338e-05, mean val. loss:  1.57634497e+00\n",
      "Epoch: 4809 mean train loss:  3.04341374e-05, mean val. loss:  1.57661915e+00\n",
      "Epoch: 4810 mean train loss:  3.07629816e-05, mean val. loss:  1.57689619e+00\n",
      "Epoch: 4811 mean train loss:  3.02258995e-05, mean val. loss:  1.57718146e+00\n",
      "Epoch: 4812 mean train loss:  3.12484044e-05, mean val. loss:  1.57747066e+00\n",
      "Epoch: 4813 mean train loss:  3.06339352e-05, mean val. loss:  1.57776403e+00\n",
      "Epoch: 4814 mean train loss:  3.04080313e-05, mean val. loss:  1.57807148e+00\n",
      "Epoch: 4815 mean train loss:  2.98072991e-05, mean val. loss:  1.57839322e+00\n",
      "Epoch: 4816 mean train loss:  3.01015971e-05, mean val. loss:  1.57873166e+00\n",
      "Epoch: 4817 mean train loss:  3.01187974e-05, mean val. loss:  1.57909143e+00\n",
      "Epoch: 4818 mean train loss:  3.06356524e-05, mean val. loss:  1.57946408e+00\n",
      "Epoch: 4819 mean train loss:  3.12135671e-05, mean val. loss:  1.57984412e+00\n",
      "Epoch: 4820 mean train loss:  3.06167931e-05, mean val. loss:  1.58023572e+00\n",
      "Epoch: 4821 mean train loss:  3.11607437e-05, mean val. loss:  1.58063030e+00\n",
      "Epoch: 4822 mean train loss:  3.01588734e-05, mean val. loss:  1.58102930e+00\n",
      "Epoch: 4823 mean train loss:  3.00293614e-05, mean val. loss:  1.58144021e+00\n",
      "Epoch: 4824 mean train loss:  3.09589086e-05, mean val. loss:  1.58184481e+00\n",
      "Epoch: 4825 mean train loss:  3.04413552e-05, mean val. loss:  1.58226001e+00\n",
      "Epoch: 4826 mean train loss:  3.11313197e-05, mean val. loss:  1.58267021e+00\n",
      "Epoch: 4827 mean train loss:  3.05392314e-05, mean val. loss:  1.58308220e+00\n",
      "Epoch: 4828 mean train loss:  3.08661838e-05, mean val. loss:  1.58349133e+00\n",
      "Epoch: 4829 mean train loss:  3.04602145e-05, mean val. loss:  1.58389056e+00\n",
      "Epoch: 4830 mean train loss:  3.07743612e-05, mean val. loss:  1.58428991e+00\n",
      "Epoch: 4831 mean train loss:  3.09368188e-05, mean val. loss:  1.58467281e+00\n",
      "Epoch: 4832 mean train loss:  3.00235988e-05, mean val. loss:  1.58504844e+00\n",
      "Epoch: 4833 mean train loss:  3.02970002e-05, mean val. loss:  1.58541715e+00\n",
      "Epoch: 4834 mean train loss:  3.01207765e-05, mean val. loss:  1.58577096e+00\n",
      "Epoch: 4835 mean train loss:  3.00768006e-05, mean val. loss:  1.58613420e+00\n",
      "Epoch: 4836 mean train loss:  3.06754373e-05, mean val. loss:  1.58649004e+00\n",
      "Epoch: 4837 mean train loss:  3.11175827e-05, mean val. loss:  1.58683395e+00\n",
      "Epoch: 4838 mean train loss:  3.04187415e-05, mean val. loss:  1.58717310e+00\n",
      "Epoch: 4839 mean train loss:  3.02374829e-05, mean val. loss:  1.58750570e+00\n",
      "Epoch: 4840 mean train loss:  2.97340157e-05, mean val. loss:  1.58783221e+00\n",
      "Epoch: 4841 mean train loss:  3.07075679e-05, mean val. loss:  1.58815467e+00\n",
      "Epoch: 4842 mean train loss:  3.03780253e-05, mean val. loss:  1.58847630e+00\n",
      "Epoch: 4843 mean train loss:  3.00745305e-05, mean val. loss:  1.58880532e+00\n",
      "Epoch: 4844 mean train loss:  3.01013642e-05, mean val. loss:  1.58912897e+00\n",
      "Epoch: 4845 mean train loss:  3.07095761e-05, mean val. loss:  1.58945763e+00\n",
      "Epoch: 4846 mean train loss:  3.10413307e-05, mean val. loss:  1.58978403e+00\n",
      "Epoch: 4847 mean train loss:  3.04672576e-05, mean val. loss:  1.59010684e+00\n",
      "Epoch: 4848 mean train loss:  3.06128350e-05, mean val. loss:  1.59043622e+00\n",
      "Epoch: 4849 mean train loss:  3.09608586e-05, mean val. loss:  1.59076631e+00\n",
      "Epoch: 4850 mean train loss:  3.09837924e-05, mean val. loss:  1.59109867e+00\n",
      "Epoch: 4851 mean train loss:  3.04288696e-05, mean val. loss:  1.59143364e+00\n",
      "Epoch: 4852 mean train loss:  3.00965912e-05, mean val. loss:  1.59176922e+00\n",
      "Epoch: 4853 mean train loss:  3.00948741e-05, mean val. loss:  1.59210825e+00\n",
      "Epoch: 4854 mean train loss:  3.06015427e-05, mean val. loss:  1.59244716e+00\n",
      "Epoch: 4855 mean train loss:  3.06357106e-05, mean val. loss:  1.59278762e+00\n",
      "Epoch: 4856 mean train loss:  3.03605048e-05, mean val. loss:  1.59311652e+00\n",
      "Epoch: 4857 mean train loss:  3.04224959e-05, mean val. loss:  1.59343815e+00\n",
      "Epoch: 4858 mean train loss:  3.05571593e-05, mean val. loss:  1.59376001e+00\n",
      "Epoch: 4859 mean train loss:  3.06961592e-05, mean val. loss:  1.59406722e+00\n",
      "Epoch: 4860 mean train loss:  3.04786081e-05, mean val. loss:  1.59437966e+00\n",
      "Epoch: 4861 mean train loss:  3.05929570e-05, mean val. loss:  1.59469271e+00\n",
      "Epoch: 4862 mean train loss:  3.06641450e-05, mean val. loss:  1.59500468e+00\n",
      "Epoch: 4863 mean train loss:  2.99577950e-05, mean val. loss:  1.59531391e+00\n",
      "Epoch: 4864 mean train loss:  3.06528527e-05, mean val. loss:  1.59562123e+00\n",
      "Epoch: 4865 mean train loss:  3.05671419e-05, mean val. loss:  1.59591961e+00\n",
      "Epoch: 4866 mean train loss:  2.99925159e-05, mean val. loss:  1.59621882e+00\n",
      "Epoch: 4867 mean train loss:  3.07099835e-05, mean val. loss:  1.59651649e+00\n",
      "Epoch: 4868 mean train loss:  2.99091334e-05, mean val. loss:  1.59681785e+00\n",
      "Epoch: 4869 mean train loss:  3.04348650e-05, mean val. loss:  1.59712231e+00\n",
      "Epoch: 4870 mean train loss:  3.02679255e-05, mean val. loss:  1.59742761e+00\n",
      "Epoch: 4871 mean train loss:  2.99377716e-05, mean val. loss:  1.59774590e+00\n",
      "Epoch: 4872 mean train loss:  2.99795938e-05, mean val. loss:  1.59806550e+00\n",
      "Epoch: 4873 mean train loss:  3.04210698e-05, mean val. loss:  1.59838951e+00\n",
      "Epoch: 4874 mean train loss:  3.08745948e-05, mean val. loss:  1.59872246e+00\n",
      "Epoch: 4875 mean train loss:  3.08311137e-05, mean val. loss:  1.59905565e+00\n",
      "Epoch: 4876 mean train loss:  3.03751149e-05, mean val. loss:  1.59939337e+00\n",
      "Epoch: 4877 mean train loss:  3.03890265e-05, mean val. loss:  1.59974110e+00\n",
      "Epoch: 4878 mean train loss:  2.97586084e-05, mean val. loss:  1.60010588e+00\n",
      "Epoch: 4879 mean train loss:  3.04021232e-05, mean val. loss:  1.60047126e+00\n",
      "Epoch: 4880 mean train loss:  3.08048620e-05, mean val. loss:  1.60084081e+00\n",
      "Epoch: 4881 mean train loss:  3.04889400e-05, mean val. loss:  1.60122061e+00\n",
      "Epoch: 4882 mean train loss:  3.08846065e-05, mean val. loss:  1.60159731e+00\n",
      "Epoch: 4883 mean train loss:  3.07466544e-05, mean val. loss:  1.60196924e+00\n",
      "Epoch: 4884 mean train loss:  2.98030791e-05, mean val. loss:  1.60233152e+00\n",
      "Epoch: 4885 mean train loss:  3.02679837e-05, mean val. loss:  1.60269737e+00\n",
      "Epoch: 4886 mean train loss:  3.02731059e-05, mean val. loss:  1.60305405e+00\n",
      "Epoch: 4887 mean train loss:  2.99421372e-05, mean val. loss:  1.60341179e+00\n",
      "Epoch: 4888 mean train loss:  3.02111439e-05, mean val. loss:  1.60375774e+00\n",
      "Epoch: 4889 mean train loss:  3.02299159e-05, mean val. loss:  1.60409617e+00\n",
      "Epoch: 4890 mean train loss:  3.07146111e-05, mean val. loss:  1.60443270e+00\n",
      "Epoch: 4891 mean train loss:  2.98990926e-05, mean val. loss:  1.60477352e+00\n",
      "Epoch: 4892 mean train loss:  2.97224906e-05, mean val. loss:  1.60511088e+00\n",
      "Epoch: 4893 mean train loss:  3.00648971e-05, mean val. loss:  1.60544705e+00\n",
      "Epoch: 4894 mean train loss:  2.99477542e-05, mean val. loss:  1.60579491e+00\n",
      "Epoch: 4895 mean train loss:  3.05353024e-05, mean val. loss:  1.60613430e+00\n",
      "Epoch: 4896 mean train loss:  3.09061143e-05, mean val. loss:  1.60648000e+00\n",
      "Epoch: 4897 mean train loss:  3.06314323e-05, mean val. loss:  1.60683632e+00\n",
      "Epoch: 4898 mean train loss:  3.02061671e-05, mean val. loss:  1.60720146e+00\n",
      "Epoch: 4899 mean train loss:  2.97295628e-05, mean val. loss:  1.60757351e+00\n",
      "Epoch: 4900 mean train loss:  2.99437961e-05, mean val. loss:  1.60794616e+00\n",
      "Epoch: 4901 mean train loss:  2.97348888e-05, mean val. loss:  1.60832262e+00\n",
      "Epoch: 4902 mean train loss:  3.05056165e-05, mean val. loss:  1.60870028e+00\n",
      "Epoch: 4903 mean train loss:  3.03925772e-05, mean val. loss:  1.60907650e+00\n",
      "Epoch: 4904 mean train loss:  3.00482498e-05, mean val. loss:  1.60945857e+00\n",
      "Epoch: 4905 mean train loss:  3.06706061e-05, mean val. loss:  1.60983956e+00\n",
      "Epoch: 4906 mean train loss:  2.99857056e-05, mean val. loss:  1.61022317e+00\n",
      "Epoch: 4907 mean train loss:  3.09769239e-05, mean val. loss:  1.61060429e+00\n",
      "Epoch: 4908 mean train loss:  3.05788708e-05, mean val. loss:  1.61096609e+00\n",
      "Epoch: 4909 mean train loss:  3.05189460e-05, mean val. loss:  1.61132240e+00\n",
      "Epoch: 4910 mean train loss:  3.04493587e-05, mean val. loss:  1.61167395e+00\n",
      "Epoch: 4911 mean train loss:  3.02268600e-05, mean val. loss:  1.61202216e+00\n",
      "Epoch: 4912 mean train loss:  3.01907712e-05, mean val. loss:  1.61236656e+00\n",
      "Epoch: 4913 mean train loss:  3.08791350e-05, mean val. loss:  1.61270618e+00\n",
      "Epoch: 4914 mean train loss:  3.03009292e-05, mean val. loss:  1.61303771e+00\n",
      "Epoch: 4915 mean train loss:  2.94623896e-05, mean val. loss:  1.61337459e+00\n",
      "Epoch: 4916 mean train loss:  2.97193474e-05, mean val. loss:  1.61369586e+00\n",
      "Epoch: 4917 mean train loss:  2.97626830e-05, mean val. loss:  1.61402893e+00\n",
      "Epoch: 4918 mean train loss:  3.00206011e-05, mean val. loss:  1.61435509e+00\n",
      "Epoch: 4919 mean train loss:  3.02998815e-05, mean val. loss:  1.61467707e+00\n",
      "Epoch: 4920 mean train loss:  3.07934824e-05, mean val. loss:  1.61499250e+00\n",
      "Epoch: 4921 mean train loss:  3.01085529e-05, mean val. loss:  1.61531508e+00\n",
      "Epoch: 4922 mean train loss:  3.04493587e-05, mean val. loss:  1.61563587e+00\n",
      "Epoch: 4923 mean train loss:  3.02040717e-05, mean val. loss:  1.61596012e+00\n",
      "Epoch: 4924 mean train loss:  2.95717036e-05, mean val. loss:  1.61629462e+00\n",
      "Epoch: 4925 mean train loss:  3.05297726e-05, mean val. loss:  1.61662984e+00\n",
      "Epoch: 4926 mean train loss:  3.04833811e-05, mean val. loss:  1.61696684e+00\n",
      "Epoch: 4927 mean train loss:  3.06309958e-05, mean val. loss:  1.61731374e+00\n",
      "Epoch: 4928 mean train loss:  2.97521474e-05, mean val. loss:  1.61766410e+00\n",
      "Epoch: 4929 mean train loss:  3.05151625e-05, mean val. loss:  1.61802495e+00\n",
      "Epoch: 4930 mean train loss:  3.02854169e-05, mean val. loss:  1.61839080e+00\n",
      "Epoch: 4931 mean train loss:  3.06586153e-05, mean val. loss:  1.61875331e+00\n",
      "Epoch: 4932 mean train loss:  3.03529378e-05, mean val. loss:  1.61912739e+00\n",
      "Epoch: 4933 mean train loss:  3.00924294e-05, mean val. loss:  1.61949384e+00\n",
      "Epoch: 4934 mean train loss:  3.00046522e-05, mean val. loss:  1.61986244e+00\n",
      "Epoch: 4935 mean train loss:  3.06650763e-05, mean val. loss:  1.62021959e+00\n",
      "Epoch: 4936 mean train loss:  2.97583756e-05, mean val. loss:  1.62058401e+00\n",
      "Epoch: 4937 mean train loss:  3.07927548e-05, mean val. loss:  1.62095523e+00\n",
      "Epoch: 4938 mean train loss:  3.02082626e-05, mean val. loss:  1.62132847e+00\n",
      "Epoch: 4939 mean train loss:  2.93562480e-05, mean val. loss:  1.62170041e+00\n",
      "Epoch: 4940 mean train loss:  2.99472013e-05, mean val. loss:  1.62207353e+00\n",
      "Epoch: 4941 mean train loss:  3.02790140e-05, mean val. loss:  1.62244618e+00\n",
      "Epoch: 4942 mean train loss:  2.97445222e-05, mean val. loss:  1.62282288e+00\n",
      "Epoch: 4943 mean train loss:  3.00749380e-05, mean val. loss:  1.62319231e+00\n",
      "Epoch: 4944 mean train loss:  3.07275914e-05, mean val. loss:  1.62356412e+00\n",
      "Epoch: 4945 mean train loss:  3.02301778e-05, mean val. loss:  1.62394094e+00\n",
      "Epoch: 4946 mean train loss:  3.06637667e-05, mean val. loss:  1.62432384e+00\n",
      "Epoch: 4947 mean train loss:  2.97776132e-05, mean val. loss:  1.62469947e+00\n",
      "Epoch: 4948 mean train loss:  3.08481394e-05, mean val. loss:  1.62507689e+00\n",
      "Epoch: 4949 mean train loss:  3.04511050e-05, mean val. loss:  1.62546062e+00\n",
      "Epoch: 4950 mean train loss:  3.01374239e-05, mean val. loss:  1.62583447e+00\n",
      "Epoch: 4951 mean train loss:  3.03171983e-05, mean val. loss:  1.62620759e+00\n",
      "Epoch: 4952 mean train loss:  2.94477504e-05, mean val. loss:  1.62657893e+00\n",
      "Epoch: 4953 mean train loss:  3.08378367e-05, mean val. loss:  1.62694120e+00\n",
      "Epoch: 4954 mean train loss:  3.06619040e-05, mean val. loss:  1.62728369e+00\n",
      "Epoch: 4955 mean train loss:  3.05289868e-05, mean val. loss:  1.62761736e+00\n",
      "Epoch: 4956 mean train loss:  3.06706934e-05, mean val. loss:  1.62794054e+00\n",
      "Epoch: 4957 mean train loss:  3.05652502e-05, mean val. loss:  1.62825251e+00\n",
      "Epoch: 4958 mean train loss:  2.96526123e-05, mean val. loss:  1.62854791e+00\n",
      "Epoch: 4959 mean train loss:  3.04161222e-05, mean val. loss:  1.62883162e+00\n",
      "Epoch: 4960 mean train loss:  2.98642262e-05, mean val. loss:  1.62910962e+00\n",
      "Epoch: 4961 mean train loss:  2.99304666e-05, mean val. loss:  1.62937343e+00\n",
      "Epoch: 4962 mean train loss:  3.03020643e-05, mean val. loss:  1.62963617e+00\n",
      "Epoch: 4963 mean train loss:  3.02133849e-05, mean val. loss:  1.62989318e+00\n",
      "Epoch: 4964 mean train loss:  3.09470051e-05, mean val. loss:  1.63015234e+00\n",
      "Epoch: 4965 mean train loss:  3.04648420e-05, mean val. loss:  1.63040519e+00\n",
      "Epoch: 4966 mean train loss:  3.01380642e-05, mean val. loss:  1.63066459e+00\n",
      "Epoch: 4967 mean train loss:  3.05966241e-05, mean val. loss:  1.63092327e+00\n",
      "Epoch: 4968 mean train loss:  2.99949897e-05, mean val. loss:  1.63118458e+00\n",
      "Epoch: 4969 mean train loss:  3.00099491e-05, mean val. loss:  1.63145494e+00\n",
      "Epoch: 4970 mean train loss:  2.98472878e-05, mean val. loss:  1.63173711e+00\n",
      "Epoch: 4971 mean train loss:  3.05415015e-05, mean val. loss:  1.63202000e+00\n",
      "Epoch: 4972 mean train loss:  2.94033962e-05, mean val. loss:  1.63231921e+00\n",
      "Epoch: 4973 mean train loss:  3.01043037e-05, mean val. loss:  1.63264608e+00\n",
      "Epoch: 4974 mean train loss:  2.96021462e-05, mean val. loss:  1.63298941e+00\n",
      "Epoch: 4975 mean train loss:  3.01586988e-05, mean val. loss:  1.63335752e+00\n",
      "Epoch: 4976 mean train loss:  2.97476654e-05, mean val. loss:  1.63373995e+00\n",
      "Epoch: 4977 mean train loss:  3.05236899e-05, mean val. loss:  1.63413894e+00\n",
      "Epoch: 4978 mean train loss:  2.97597726e-05, mean val. loss:  1.63454759e+00\n",
      "Epoch: 4979 mean train loss:  2.98887026e-05, mean val. loss:  1.63497710e+00\n",
      "Epoch: 4980 mean train loss:  2.96990038e-05, mean val. loss:  1.63542283e+00\n",
      "Epoch: 4981 mean train loss:  3.04962741e-05, mean val. loss:  1.63588524e+00\n",
      "Epoch: 4982 mean train loss:  3.03270062e-05, mean val. loss:  1.63636541e+00\n",
      "Epoch: 4983 mean train loss:  3.10628966e-05, mean val. loss:  1.63683581e+00\n",
      "Epoch: 4984 mean train loss:  2.98459490e-05, mean val. loss:  1.63730907e+00\n",
      "Epoch: 4985 mean train loss:  3.07527662e-05, mean val. loss:  1.63777363e+00\n",
      "Epoch: 4986 mean train loss:  2.99914391e-05, mean val. loss:  1.63823414e+00\n",
      "Epoch: 4987 mean train loss:  3.00220563e-05, mean val. loss:  1.63869083e+00\n",
      "Epoch: 4988 mean train loss:  3.01257824e-05, mean val. loss:  1.63913441e+00\n",
      "Epoch: 4989 mean train loss:  3.02660919e-05, mean val. loss:  1.63955903e+00\n",
      "Epoch: 4990 mean train loss:  3.00973188e-05, mean val. loss:  1.63997018e+00\n",
      "Epoch: 4991 mean train loss:  3.01429536e-05, mean val. loss:  1.64036965e+00\n",
      "Epoch: 4992 mean train loss:  3.00030224e-05, mean val. loss:  1.64074397e+00\n",
      "Epoch: 4993 mean train loss:  2.98562227e-05, mean val. loss:  1.64110982e+00\n",
      "Epoch: 4994 mean train loss:  3.06272123e-05, mean val. loss:  1.64145482e+00\n",
      "Epoch: 4995 mean train loss:  3.02317494e-05, mean val. loss:  1.64179599e+00\n",
      "Epoch: 4996 mean train loss:  2.97360239e-05, mean val. loss:  1.64211595e+00\n",
      "Epoch: 4997 mean train loss:  3.00790125e-05, mean val. loss:  1.64243746e+00\n",
      "Epoch: 4998 mean train loss:  3.01812252e-05, mean val. loss:  1.64275718e+00\n",
      "Epoch: 4999 mean train loss:  3.02822737e-05, mean val. loss:  1.64307392e+00\n",
      "Epoch: 5000 mean train loss:  2.96771759e-05, mean val. loss:  1.64339781e+00\n",
      "Epoch: 5001 mean train loss:  3.00769170e-05, mean val. loss:  1.64372206e+00\n",
      "Epoch: 5002 mean train loss:  3.01150721e-05, mean val. loss:  1.64405429e+00\n",
      "Epoch: 5003 mean train loss:  2.98398954e-05, mean val. loss:  1.64438272e+00\n",
      "Epoch: 5004 mean train loss:  3.02162953e-05, mean val. loss:  1.64471960e+00\n",
      "Epoch: 5005 mean train loss:  3.02797707e-05, mean val. loss:  1.64507675e+00\n",
      "Epoch: 5006 mean train loss:  3.03856505e-05, mean val. loss:  1.64543593e+00\n",
      "Epoch: 5007 mean train loss:  2.99465610e-05, mean val. loss:  1.64580095e+00\n",
      "Epoch: 5008 mean train loss:  3.02436238e-05, mean val. loss:  1.64616990e+00\n",
      "Epoch: 5009 mean train loss:  3.03780544e-05, mean val. loss:  1.64654768e+00\n",
      "Epoch: 5010 mean train loss:  3.04284331e-05, mean val. loss:  1.64692533e+00\n",
      "Epoch: 5011 mean train loss:  2.95994396e-05, mean val. loss:  1.64730585e+00\n",
      "Epoch: 5012 mean train loss:  3.01849213e-05, mean val. loss:  1.64769530e+00\n",
      "Epoch: 5013 mean train loss:  2.94848287e-05, mean val. loss:  1.64809287e+00\n",
      "Epoch: 5014 mean train loss:  2.94663769e-05, mean val. loss:  1.64849007e+00\n",
      "Epoch: 5015 mean train loss:  2.92150653e-05, mean val. loss:  1.64888966e+00\n",
      "Epoch: 5016 mean train loss:  3.00760148e-05, mean val. loss:  1.64929032e+00\n",
      "Epoch: 5017 mean train loss:  3.02934786e-05, mean val. loss:  1.64969361e+00\n",
      "Epoch: 5018 mean train loss:  2.98040104e-05, mean val. loss:  1.65009534e+00\n",
      "Epoch: 5019 mean train loss:  2.96603830e-05, mean val. loss:  1.65049362e+00\n",
      "Epoch: 5020 mean train loss:  2.97414372e-05, mean val. loss:  1.65089118e+00\n",
      "Epoch: 5021 mean train loss:  3.00841057e-05, mean val. loss:  1.65129256e+00\n",
      "Epoch: 5022 mean train loss:  3.01837863e-05, mean val. loss:  1.65169609e+00\n",
      "Epoch: 5023 mean train loss:  3.00570100e-05, mean val. loss:  1.65209258e+00\n",
      "Epoch: 5024 mean train loss:  3.05764843e-05, mean val. loss:  1.65249455e+00\n",
      "Epoch: 5025 mean train loss:  2.99566018e-05, mean val. loss:  1.65289533e+00\n",
      "Epoch: 5026 mean train loss:  2.96499638e-05, mean val. loss:  1.65329790e+00\n",
      "Epoch: 5027 mean train loss:  2.97478691e-05, mean val. loss:  1.65369940e+00\n",
      "Epoch: 5028 mean train loss:  3.00806423e-05, mean val. loss:  1.65408552e+00\n",
      "Epoch: 5029 mean train loss:  3.01243854e-05, mean val. loss:  1.65447366e+00\n",
      "Epoch: 5030 mean train loss:  3.01137334e-05, mean val. loss:  1.65483856e+00\n",
      "Epoch: 5031 mean train loss:  2.99745006e-05, mean val. loss:  1.65519702e+00\n",
      "Epoch: 5032 mean train loss:  3.00881220e-05, mean val. loss:  1.65553796e+00\n",
      "Epoch: 5033 mean train loss:  2.95709469e-05, mean val. loss:  1.65586495e+00\n",
      "Epoch: 5034 mean train loss:  3.00411193e-05, mean val. loss:  1.65618205e+00\n",
      "Epoch: 5035 mean train loss:  2.99469684e-05, mean val. loss:  1.65648580e+00\n",
      "Epoch: 5036 mean train loss:  2.99979292e-05, mean val. loss:  1.65677774e+00\n",
      "Epoch: 5037 mean train loss:  2.94815400e-05, mean val. loss:  1.65706921e+00\n",
      "Epoch: 5038 mean train loss:  3.03627457e-05, mean val. loss:  1.65735412e+00\n",
      "Epoch: 5039 mean train loss:  2.91625620e-05, mean val. loss:  1.65763879e+00\n",
      "Epoch: 5040 mean train loss:  3.00605025e-05, mean val. loss:  1.65793109e+00\n",
      "Epoch: 5041 mean train loss:  2.96325597e-05, mean val. loss:  1.65822339e+00\n",
      "Epoch: 5042 mean train loss:  2.91861070e-05, mean val. loss:  1.65852213e+00\n",
      "Epoch: 5043 mean train loss:  2.99467065e-05, mean val. loss:  1.65882671e+00\n",
      "Epoch: 5044 mean train loss:  3.04691494e-05, mean val. loss:  1.65913355e+00\n",
      "Epoch: 5045 mean train loss:  2.98762752e-05, mean val. loss:  1.65944934e+00\n",
      "Epoch: 5046 mean train loss:  3.00507527e-05, mean val. loss:  1.65977156e+00\n",
      "Epoch: 5047 mean train loss:  2.96754297e-05, mean val. loss:  1.66010189e+00\n",
      "Epoch: 5048 mean train loss:  3.05414433e-05, mean val. loss:  1.66043997e+00\n",
      "Epoch: 5049 mean train loss:  3.01946129e-05, mean val. loss:  1.66078031e+00\n",
      "Epoch: 5050 mean train loss:  3.00543616e-05, mean val. loss:  1.66112518e+00\n",
      "Epoch: 5051 mean train loss:  3.00030806e-05, mean val. loss:  1.66148138e+00\n",
      "Epoch: 5052 mean train loss:  2.95451027e-05, mean val. loss:  1.66185045e+00\n",
      "Epoch: 5053 mean train loss:  2.96249054e-05, mean val. loss:  1.66222024e+00\n",
      "Epoch: 5054 mean train loss:  3.00177198e-05, mean val. loss:  1.66259325e+00\n",
      "Epoch: 5055 mean train loss:  2.98469677e-05, mean val. loss:  1.66296947e+00\n",
      "Epoch: 5056 mean train loss:  3.02409462e-05, mean val. loss:  1.66334736e+00\n",
      "Epoch: 5057 mean train loss:  3.02401604e-05, mean val. loss:  1.66372800e+00\n",
      "Epoch: 5058 mean train loss:  2.95061618e-05, mean val. loss:  1.66411602e+00\n",
      "Epoch: 5059 mean train loss:  2.90782773e-05, mean val. loss:  1.66451359e+00\n",
      "Epoch: 5060 mean train loss:  3.02813714e-05, mean val. loss:  1.66491115e+00\n",
      "Epoch: 5061 mean train loss:  2.94641650e-05, mean val. loss:  1.66531157e+00\n",
      "Epoch: 5062 mean train loss:  2.93145713e-05, mean val. loss:  1.66571283e+00\n",
      "Epoch: 5063 mean train loss:  2.94871570e-05, mean val. loss:  1.66611314e+00\n",
      "Epoch: 5064 mean train loss:  2.96912331e-05, mean val. loss:  1.66650879e+00\n",
      "Epoch: 5065 mean train loss:  3.02034896e-05, mean val. loss:  1.66691756e+00\n",
      "Epoch: 5066 mean train loss:  3.00868123e-05, mean val. loss:  1.66731966e+00\n",
      "Epoch: 5067 mean train loss:  2.99581152e-05, mean val. loss:  1.66773415e+00\n",
      "Epoch: 5068 mean train loss:  2.95580539e-05, mean val. loss:  1.66814172e+00\n",
      "Epoch: 5069 mean train loss:  2.94884667e-05, mean val. loss:  1.66855133e+00\n",
      "Epoch: 5070 mean train loss:  2.95422215e-05, mean val. loss:  1.66897702e+00\n",
      "Epoch: 5071 mean train loss:  2.94955389e-05, mean val. loss:  1.66941059e+00\n",
      "Epoch: 5072 mean train loss:  2.98446976e-05, mean val. loss:  1.66984034e+00\n",
      "Epoch: 5073 mean train loss:  2.98095401e-05, mean val. loss:  1.67026830e+00\n",
      "Epoch: 5074 mean train loss:  2.97903607e-05, mean val. loss:  1.67070317e+00\n",
      "Epoch: 5075 mean train loss:  2.95853533e-05, mean val. loss:  1.67114091e+00\n",
      "Epoch: 5076 mean train loss:  2.98793311e-05, mean val. loss:  1.67157876e+00\n",
      "Epoch: 5077 mean train loss:  2.93662597e-05, mean val. loss:  1.67202210e+00\n",
      "Epoch: 5078 mean train loss:  2.91620672e-05, mean val. loss:  1.67246771e+00\n",
      "Epoch: 5079 mean train loss:  2.94282800e-05, mean val. loss:  1.67290878e+00\n",
      "Epoch: 5080 mean train loss:  2.97320366e-05, mean val. loss:  1.67334938e+00\n",
      "Epoch: 5081 mean train loss:  2.97854422e-05, mean val. loss:  1.67379475e+00\n",
      "Epoch: 5082 mean train loss:  2.97945517e-05, mean val. loss:  1.67423916e+00\n",
      "Epoch: 5083 mean train loss:  2.98159139e-05, mean val. loss:  1.67467093e+00\n",
      "Epoch: 5084 mean train loss:  3.01641994e-05, mean val. loss:  1.67509377e+00\n",
      "Epoch: 5085 mean train loss:  2.93955381e-05, mean val. loss:  1.67552257e+00\n",
      "Epoch: 5086 mean train loss:  2.95943755e-05, mean val. loss:  1.67594528e+00\n",
      "Epoch: 5087 mean train loss:  2.96764774e-05, mean val. loss:  1.67635989e+00\n",
      "Epoch: 5088 mean train loss:  3.00120737e-05, mean val. loss:  1.67675781e+00\n",
      "Epoch: 5089 mean train loss:  2.98757805e-05, mean val. loss:  1.67715073e+00\n",
      "Epoch: 5090 mean train loss:  2.94303754e-05, mean val. loss:  1.67752492e+00\n",
      "Epoch: 5091 mean train loss:  2.98494124e-05, mean val. loss:  1.67789948e+00\n",
      "Epoch: 5092 mean train loss:  2.94009806e-05, mean val. loss:  1.67826939e+00\n",
      "Epoch: 5093 mean train loss:  3.01823893e-05, mean val. loss:  1.67863655e+00\n",
      "Epoch: 5094 mean train loss:  2.94147758e-05, mean val. loss:  1.67899835e+00\n",
      "Epoch: 5095 mean train loss:  2.95998179e-05, mean val. loss:  1.67935002e+00\n",
      "Epoch: 5096 mean train loss:  2.99015956e-05, mean val. loss:  1.67969286e+00\n",
      "Epoch: 5097 mean train loss:  2.97435327e-05, mean val. loss:  1.68003666e+00\n",
      "Epoch: 5098 mean train loss:  2.96616927e-05, mean val. loss:  1.68037939e+00\n",
      "Epoch: 5099 mean train loss:  2.99638778e-05, mean val. loss:  1.68071413e+00\n",
      "Epoch: 5100 mean train loss:  2.99065432e-05, mean val. loss:  1.68104923e+00\n",
      "Epoch: 5101 mean train loss:  2.94851488e-05, mean val. loss:  1.68136942e+00\n",
      "Epoch: 5102 mean train loss:  2.94052588e-05, mean val. loss:  1.68168736e+00\n",
      "Epoch: 5103 mean train loss:  3.00013344e-05, mean val. loss:  1.68201423e+00\n",
      "Epoch: 5104 mean train loss:  3.02772969e-05, mean val. loss:  1.68232834e+00\n",
      "Epoch: 5105 mean train loss:  2.95413774e-05, mean val. loss:  1.68265009e+00\n",
      "Epoch: 5106 mean train loss:  2.91289471e-05, mean val. loss:  1.68296528e+00\n",
      "Epoch: 5107 mean train loss:  2.98497325e-05, mean val. loss:  1.68326688e+00\n",
      "Epoch: 5108 mean train loss:  2.96917278e-05, mean val. loss:  1.68357635e+00\n",
      "Epoch: 5109 mean train loss:  3.02254921e-05, mean val. loss:  1.68388486e+00\n",
      "Epoch: 5110 mean train loss:  2.93174526e-05, mean val. loss:  1.68419206e+00\n",
      "Epoch: 5111 mean train loss:  3.02538683e-05, mean val. loss:  1.68448913e+00\n",
      "Epoch: 5112 mean train loss:  2.96768558e-05, mean val. loss:  1.68477869e+00\n",
      "Epoch: 5113 mean train loss:  2.94930360e-05, mean val. loss:  1.68505979e+00\n",
      "Epoch: 5114 mean train loss:  2.91674514e-05, mean val. loss:  1.68534338e+00\n",
      "Epoch: 5115 mean train loss:  2.97664374e-05, mean val. loss:  1.68561590e+00\n",
      "Epoch: 5116 mean train loss:  2.95985083e-05, mean val. loss:  1.68588865e+00\n",
      "Epoch: 5117 mean train loss:  3.00058164e-05, mean val. loss:  1.68615329e+00\n",
      "Epoch: 5118 mean train loss:  3.00993561e-05, mean val. loss:  1.68641484e+00\n",
      "Epoch: 5119 mean train loss:  2.95685313e-05, mean val. loss:  1.68667233e+00\n",
      "Epoch: 5120 mean train loss:  2.95496429e-05, mean val. loss:  1.68693185e+00\n",
      "Epoch: 5121 mean train loss:  2.93876801e-05, mean val. loss:  1.68719578e+00\n",
      "Epoch: 5122 mean train loss:  2.89799063e-05, mean val. loss:  1.68744957e+00\n",
      "Epoch: 5123 mean train loss:  2.95492064e-05, mean val. loss:  1.68771279e+00\n",
      "Epoch: 5124 mean train loss:  2.95272912e-05, mean val. loss:  1.68798387e+00\n",
      "Epoch: 5125 mean train loss:  2.90992321e-05, mean val. loss:  1.68826199e+00\n",
      "Epoch: 5126 mean train loss:  2.95942300e-05, mean val. loss:  1.68854463e+00\n",
      "Epoch: 5127 mean train loss:  2.97092774e-05, mean val. loss:  1.68883801e+00\n",
      "Epoch: 5128 mean train loss:  2.96326180e-05, mean val. loss:  1.68913710e+00\n",
      "Epoch: 5129 mean train loss:  2.97199294e-05, mean val. loss:  1.68944180e+00\n",
      "Epoch: 5130 mean train loss:  2.96954240e-05, mean val. loss:  1.68975794e+00\n",
      "Epoch: 5131 mean train loss:  2.96672224e-05, mean val. loss:  1.69008386e+00\n",
      "Epoch: 5132 mean train loss:  2.94386118e-05, mean val. loss:  1.69042158e+00\n",
      "Epoch: 5133 mean train loss:  2.94533675e-05, mean val. loss:  1.69077802e+00\n",
      "Epoch: 5134 mean train loss:  2.94602069e-05, mean val. loss:  1.69114482e+00\n",
      "Epoch: 5135 mean train loss:  2.93258636e-05, mean val. loss:  1.69152892e+00\n",
      "Epoch: 5136 mean train loss:  2.97455408e-05, mean val. loss:  1.69192791e+00\n",
      "Epoch: 5137 mean train loss:  2.92857294e-05, mean val. loss:  1.69233763e+00\n",
      "Epoch: 5138 mean train loss:  2.91835167e-05, mean val. loss:  1.69276309e+00\n",
      "Epoch: 5139 mean train loss:  3.00471147e-05, mean val. loss:  1.69319522e+00\n",
      "Epoch: 5140 mean train loss:  2.93337216e-05, mean val. loss:  1.69363427e+00\n",
      "Epoch: 5141 mean train loss:  2.99348612e-05, mean val. loss:  1.69407964e+00\n",
      "Epoch: 5142 mean train loss:  2.92859040e-05, mean val. loss:  1.69453108e+00\n",
      "Epoch: 5143 mean train loss:  2.97696679e-05, mean val. loss:  1.69498289e+00\n",
      "Epoch: 5144 mean train loss:  2.94877100e-05, mean val. loss:  1.69544256e+00\n",
      "Epoch: 5145 mean train loss:  2.95003701e-05, mean val. loss:  1.69590902e+00\n",
      "Epoch: 5146 mean train loss:  3.02307599e-05, mean val. loss:  1.69637406e+00\n",
      "Epoch: 5147 mean train loss:  2.99315434e-05, mean val. loss:  1.69683564e+00\n",
      "Epoch: 5148 mean train loss:  2.97402439e-05, mean val. loss:  1.69728768e+00\n",
      "Epoch: 5149 mean train loss:  2.94332567e-05, mean val. loss:  1.69773149e+00\n",
      "Epoch: 5150 mean train loss:  2.93184130e-05, mean val. loss:  1.69816470e+00\n",
      "Epoch: 5151 mean train loss:  2.95038626e-05, mean val. loss:  1.69858134e+00\n",
      "Epoch: 5152 mean train loss:  2.95873906e-05, mean val. loss:  1.69898069e+00\n",
      "Epoch: 5153 mean train loss:  2.97421357e-05, mean val. loss:  1.69936645e+00\n",
      "Epoch: 5154 mean train loss:  2.91006581e-05, mean val. loss:  1.69972908e+00\n",
      "Epoch: 5155 mean train loss:  3.00638494e-05, mean val. loss:  1.70006716e+00\n",
      "Epoch: 5156 mean train loss:  2.94934725e-05, mean val. loss:  1.70039070e+00\n",
      "Epoch: 5157 mean train loss:  2.96685903e-05, mean val. loss:  1.70068896e+00\n",
      "Epoch: 5158 mean train loss:  2.90540338e-05, mean val. loss:  1.70098531e+00\n",
      "Epoch: 5159 mean train loss:  2.90898315e-05, mean val. loss:  1.70127523e+00\n",
      "Epoch: 5160 mean train loss:  2.93665216e-05, mean val. loss:  1.70155108e+00\n",
      "Epoch: 5161 mean train loss:  2.98005180e-05, mean val. loss:  1.70181942e+00\n",
      "Epoch: 5162 mean train loss:  2.98926607e-05, mean val. loss:  1.70208478e+00\n",
      "Epoch: 5163 mean train loss:  2.96534272e-05, mean val. loss:  1.70235062e+00\n",
      "Epoch: 5164 mean train loss:  2.97950173e-05, mean val. loss:  1.70262218e+00\n",
      "Epoch: 5165 mean train loss:  2.98578234e-05, mean val. loss:  1.70289147e+00\n",
      "Epoch: 5166 mean train loss:  2.88703013e-05, mean val. loss:  1.70316792e+00\n",
      "Epoch: 5167 mean train loss:  2.87948060e-05, mean val. loss:  1.70345211e+00\n",
      "Epoch: 5168 mean train loss:  2.94163765e-05, mean val. loss:  1.70373464e+00\n",
      "Epoch: 5169 mean train loss:  2.95299687e-05, mean val. loss:  1.70403898e+00\n",
      "Epoch: 5170 mean train loss:  2.96040962e-05, mean val. loss:  1.70434690e+00\n",
      "Epoch: 5171 mean train loss:  2.93269695e-05, mean val. loss:  1.70465708e+00\n",
      "Epoch: 5172 mean train loss:  2.96302605e-05, mean val. loss:  1.70498669e+00\n",
      "Epoch: 5173 mean train loss:  2.94170168e-05, mean val. loss:  1.70533502e+00\n",
      "Epoch: 5174 mean train loss:  2.97291263e-05, mean val. loss:  1.70570207e+00\n",
      "Epoch: 5175 mean train loss:  2.99284584e-05, mean val. loss:  1.70607841e+00\n",
      "Epoch: 5176 mean train loss:  2.93449266e-05, mean val. loss:  1.70645595e+00\n",
      "Epoch: 5177 mean train loss:  2.87475996e-05, mean val. loss:  1.70685112e+00\n",
      "Epoch: 5178 mean train loss:  2.88062729e-05, mean val. loss:  1.70724368e+00\n",
      "Epoch: 5179 mean train loss:  3.00743850e-05, mean val. loss:  1.70763016e+00\n",
      "Epoch: 5180 mean train loss:  2.99286330e-05, mean val. loss:  1.70802605e+00\n",
      "Epoch: 5181 mean train loss:  3.00670217e-05, mean val. loss:  1.70841360e+00\n",
      "Epoch: 5182 mean train loss:  2.93830817e-05, mean val. loss:  1.70880103e+00\n",
      "Epoch: 5183 mean train loss:  2.97270890e-05, mean val. loss:  1.70918703e+00\n",
      "Epoch: 5184 mean train loss:  2.95420177e-05, mean val. loss:  1.70956957e+00\n",
      "Epoch: 5185 mean train loss:  2.98278173e-05, mean val. loss:  1.70994294e+00\n",
      "Epoch: 5186 mean train loss:  2.91595352e-05, mean val. loss:  1.71031570e+00\n",
      "Epoch: 5187 mean train loss:  2.98519153e-05, mean val. loss:  1.71067035e+00\n",
      "Epoch: 5188 mean train loss:  2.99165258e-05, mean val. loss:  1.71101725e+00\n",
      "Epoch: 5189 mean train loss:  2.91737379e-05, mean val. loss:  1.71135986e+00\n",
      "Epoch: 5190 mean train loss:  2.99873354e-05, mean val. loss:  1.71170533e+00\n",
      "Epoch: 5191 mean train loss:  2.87930598e-05, mean val. loss:  1.71204436e+00\n",
      "Epoch: 5192 mean train loss:  2.93479534e-05, mean val. loss:  1.71238244e+00\n",
      "Epoch: 5193 mean train loss:  2.92198674e-05, mean val. loss:  1.71271062e+00\n",
      "Epoch: 5194 mean train loss:  2.91432661e-05, mean val. loss:  1.71304679e+00\n",
      "Epoch: 5195 mean train loss:  2.94064812e-05, mean val. loss:  1.71337843e+00\n",
      "Epoch: 5196 mean train loss:  2.96733342e-05, mean val. loss:  1.71370661e+00\n",
      "Epoch: 5197 mean train loss:  2.96466169e-05, mean val. loss:  1.71403086e+00\n",
      "Epoch: 5198 mean train loss:  2.97794468e-05, mean val. loss:  1.71435368e+00\n",
      "Epoch: 5199 mean train loss:  2.90961179e-05, mean val. loss:  1.71468031e+00\n",
      "Epoch: 5200 mean train loss:  2.95483915e-05, mean val. loss:  1.71502185e+00\n",
      "Epoch: 5201 mean train loss:  2.88718729e-05, mean val. loss:  1.71536744e+00\n",
      "Epoch: 5202 mean train loss:  2.92752520e-05, mean val. loss:  1.71571374e+00\n",
      "Epoch: 5203 mean train loss:  2.95810169e-05, mean val. loss:  1.71607316e+00\n",
      "Epoch: 5204 mean train loss:  2.95992941e-05, mean val. loss:  1.71643400e+00\n",
      "Epoch: 5205 mean train loss:  2.92175682e-05, mean val. loss:  1.71681225e+00\n",
      "Epoch: 5206 mean train loss:  2.95638165e-05, mean val. loss:  1.71719468e+00\n",
      "Epoch: 5207 mean train loss:  2.95314530e-05, mean val. loss:  1.71758389e+00\n",
      "Epoch: 5208 mean train loss:  2.93858175e-05, mean val. loss:  1.71797311e+00\n",
      "Epoch: 5209 mean train loss:  2.94673082e-05, mean val. loss:  1.71836936e+00\n",
      "Epoch: 5210 mean train loss:  2.97257793e-05, mean val. loss:  1.71876419e+00\n",
      "Epoch: 5211 mean train loss:  2.93816556e-05, mean val. loss:  1.71916306e+00\n",
      "Epoch: 5212 mean train loss:  2.92000186e-05, mean val. loss:  1.71955931e+00\n",
      "Epoch: 5213 mean train loss:  2.91303732e-05, mean val. loss:  1.71995032e+00\n",
      "Epoch: 5214 mean train loss:  2.91316246e-05, mean val. loss:  1.72033215e+00\n",
      "Epoch: 5215 mean train loss:  2.92938785e-05, mean val. loss:  1.72071409e+00\n",
      "Epoch: 5216 mean train loss:  2.87428556e-05, mean val. loss:  1.72109663e+00\n",
      "Epoch: 5217 mean train loss:  2.88402662e-05, mean val. loss:  1.72148013e+00\n",
      "Epoch: 5218 mean train loss:  2.89964082e-05, mean val. loss:  1.72186327e+00\n",
      "Epoch: 5219 mean train loss:  3.00371030e-05, mean val. loss:  1.72224545e+00\n",
      "Epoch: 5220 mean train loss:  2.90248718e-05, mean val. loss:  1.72262609e+00\n",
      "Epoch: 5221 mean train loss:  2.97187653e-05, mean val. loss:  1.72301126e+00\n",
      "Epoch: 5222 mean train loss:  2.93343910e-05, mean val. loss:  1.72339702e+00\n",
      "Epoch: 5223 mean train loss:  2.93582561e-05, mean val. loss:  1.72378755e+00\n",
      "Epoch: 5224 mean train loss:  2.92128534e-05, mean val. loss:  1.72417879e+00\n",
      "Epoch: 5225 mean train loss:  2.93591293e-05, mean val. loss:  1.72456288e+00\n",
      "Epoch: 5226 mean train loss:  2.98533123e-05, mean val. loss:  1.72495127e+00\n",
      "Epoch: 5227 mean train loss:  2.98422528e-05, mean val. loss:  1.72533917e+00\n",
      "Epoch: 5228 mean train loss:  2.94884667e-05, mean val. loss:  1.72571659e+00\n",
      "Epoch: 5229 mean train loss:  2.94445490e-05, mean val. loss:  1.72609258e+00\n",
      "Epoch: 5230 mean train loss:  2.87736184e-05, mean val. loss:  1.72646642e+00\n",
      "Epoch: 5231 mean train loss:  2.95622740e-05, mean val. loss:  1.72684503e+00\n",
      "Epoch: 5232 mean train loss:  2.96079961e-05, mean val. loss:  1.72721338e+00\n",
      "Epoch: 5233 mean train loss:  2.96854705e-05, mean val. loss:  1.72756982e+00\n",
      "Epoch: 5234 mean train loss:  2.91395700e-05, mean val. loss:  1.72792256e+00\n",
      "Epoch: 5235 mean train loss:  3.00259562e-05, mean val. loss:  1.72826290e+00\n",
      "Epoch: 5236 mean train loss:  2.92245240e-05, mean val. loss:  1.72860467e+00\n",
      "Epoch: 5237 mean train loss:  2.91545293e-05, mean val. loss:  1.72895014e+00\n",
      "Epoch: 5238 mean train loss:  2.91214674e-05, mean val. loss:  1.72929764e+00\n",
      "Epoch: 5239 mean train loss:  2.93310150e-05, mean val. loss:  1.72965121e+00\n",
      "Epoch: 5240 mean train loss:  2.97898368e-05, mean val. loss:  1.72999465e+00\n",
      "Epoch: 5241 mean train loss:  2.94966740e-05, mean val. loss:  1.73033214e+00\n",
      "Epoch: 5242 mean train loss:  2.99323583e-05, mean val. loss:  1.73066294e+00\n",
      "Epoch: 5243 mean train loss:  2.93337216e-05, mean val. loss:  1.73098135e+00\n",
      "Epoch: 5244 mean train loss:  2.93874764e-05, mean val. loss:  1.73130310e+00\n",
      "Epoch: 5245 mean train loss:  2.94421916e-05, mean val. loss:  1.73161411e+00\n",
      "Epoch: 5246 mean train loss:  2.88981246e-05, mean val. loss:  1.73193264e+00\n",
      "Epoch: 5247 mean train loss:  2.87989387e-05, mean val. loss:  1.73223841e+00\n",
      "Epoch: 5248 mean train loss:  2.95503123e-05, mean val. loss:  1.73254633e+00\n",
      "Epoch: 5249 mean train loss:  2.95076170e-05, mean val. loss:  1.73286116e+00\n",
      "Epoch: 5250 mean train loss:  2.92322948e-05, mean val. loss:  1.73317325e+00\n",
      "Epoch: 5251 mean train loss:  2.93929479e-05, mean val. loss:  1.73348129e+00\n",
      "Epoch: 5252 mean train loss:  3.00485117e-05, mean val. loss:  1.73379278e+00\n",
      "Epoch: 5253 mean train loss:  2.91929464e-05, mean val. loss:  1.73410559e+00\n",
      "Epoch: 5254 mean train loss:  2.98286614e-05, mean val. loss:  1.73442137e+00\n",
      "Epoch: 5255 mean train loss:  2.95370701e-05, mean val. loss:  1.73475587e+00\n",
      "Epoch: 5256 mean train loss:  2.91105243e-05, mean val. loss:  1.73508501e+00\n",
      "Epoch: 5257 mean train loss:  2.92333716e-05, mean val. loss:  1.73541081e+00\n",
      "Epoch: 5258 mean train loss:  2.95272039e-05, mean val. loss:  1.73573995e+00\n",
      "Epoch: 5259 mean train loss:  2.88937299e-05, mean val. loss:  1.73607588e+00\n",
      "Epoch: 5260 mean train loss:  2.87699222e-05, mean val. loss:  1.73642159e+00\n",
      "Epoch: 5261 mean train loss:  2.97015067e-05, mean val. loss:  1.73676598e+00\n",
      "Epoch: 5262 mean train loss:  2.96616927e-05, mean val. loss:  1.73711252e+00\n",
      "Epoch: 5263 mean train loss:  2.96580547e-05, mean val. loss:  1.73745811e+00\n",
      "Epoch: 5264 mean train loss:  2.93164339e-05, mean val. loss:  1.73781323e+00\n",
      "Epoch: 5265 mean train loss:  2.96185608e-05, mean val. loss:  1.73817277e+00\n",
      "Epoch: 5266 mean train loss:  2.90511816e-05, mean val. loss:  1.73853564e+00\n",
      "Epoch: 5267 mean train loss:  2.92534532e-05, mean val. loss:  1.73889112e+00\n",
      "Epoch: 5268 mean train loss:  2.92702171e-05, mean val. loss:  1.73924899e+00\n",
      "Epoch: 5269 mean train loss:  2.90676835e-05, mean val. loss:  1.73960888e+00\n",
      "Epoch: 5270 mean train loss:  2.88726296e-05, mean val. loss:  1.73996842e+00\n",
      "Epoch: 5271 mean train loss:  2.93906487e-05, mean val. loss:  1.74033260e+00\n",
      "Epoch: 5272 mean train loss:  2.92203913e-05, mean val. loss:  1.74070168e+00\n",
      "Epoch: 5273 mean train loss:  2.87937582e-05, mean val. loss:  1.74107647e+00\n",
      "Epoch: 5274 mean train loss:  2.94672791e-05, mean val. loss:  1.74144876e+00\n",
      "Epoch: 5275 mean train loss:  2.97211518e-05, mean val. loss:  1.74182141e+00\n",
      "Epoch: 5276 mean train loss:  2.86596187e-05, mean val. loss:  1.74218631e+00\n",
      "Epoch: 5277 mean train loss:  2.92244367e-05, mean val. loss:  1.74255848e+00\n",
      "Epoch: 5278 mean train loss:  2.94147758e-05, mean val. loss:  1.74293494e+00\n",
      "Epoch: 5279 mean train loss:  2.91604665e-05, mean val. loss:  1.74330342e+00\n",
      "Epoch: 5280 mean train loss:  2.96899234e-05, mean val. loss:  1.74367392e+00\n",
      "Epoch: 5281 mean train loss:  2.92394543e-05, mean val. loss:  1.74404562e+00\n",
      "Epoch: 5282 mean train loss:  2.90255412e-05, mean val. loss:  1.74441302e+00\n",
      "Epoch: 5283 mean train loss:  2.94095080e-05, mean val. loss:  1.74478066e+00\n",
      "Epoch: 5284 mean train loss:  2.91058386e-05, mean val. loss:  1.74514818e+00\n",
      "Epoch: 5285 mean train loss:  2.90310709e-05, mean val. loss:  1.74551439e+00\n",
      "Epoch: 5286 mean train loss:  2.92811019e-05, mean val. loss:  1.74588287e+00\n",
      "Epoch: 5287 mean train loss:  2.84973939e-05, mean val. loss:  1.74625278e+00\n",
      "Epoch: 5288 mean train loss:  2.87937000e-05, mean val. loss:  1.74662030e+00\n",
      "Epoch: 5289 mean train loss:  2.87785660e-05, mean val. loss:  1.74699223e+00\n",
      "Epoch: 5290 mean train loss:  2.89127929e-05, mean val. loss:  1.74737215e+00\n",
      "Epoch: 5291 mean train loss:  2.89849413e-05, mean val. loss:  1.74776995e+00\n",
      "Epoch: 5292 mean train loss:  2.84367998e-05, mean val. loss:  1.74816859e+00\n",
      "Epoch: 5293 mean train loss:  2.90441967e-05, mean val. loss:  1.74857414e+00\n",
      "Epoch: 5294 mean train loss:  2.87622970e-05, mean val. loss:  1.74898243e+00\n",
      "Epoch: 5295 mean train loss:  2.88985029e-05, mean val. loss:  1.74940372e+00\n",
      "Epoch: 5296 mean train loss:  2.94858764e-05, mean val. loss:  1.74983013e+00\n",
      "Epoch: 5297 mean train loss:  2.93471967e-05, mean val. loss:  1.75025940e+00\n",
      "Epoch: 5298 mean train loss:  2.90200405e-05, mean val. loss:  1.75068462e+00\n",
      "Epoch: 5299 mean train loss:  2.91389006e-05, mean val. loss:  1.75111997e+00\n",
      "Epoch: 5300 mean train loss:  2.93734192e-05, mean val. loss:  1.75156450e+00\n",
      "Epoch: 5301 mean train loss:  2.90889584e-05, mean val. loss:  1.75200510e+00\n",
      "Epoch: 5302 mean train loss:  2.91878823e-05, mean val. loss:  1.75245273e+00\n",
      "Epoch: 5303 mean train loss:  2.92770856e-05, mean val. loss:  1.75290465e+00\n",
      "Epoch: 5304 mean train loss:  2.85416318e-05, mean val. loss:  1.75335348e+00\n",
      "Epoch: 5305 mean train loss:  2.86549912e-05, mean val. loss:  1.75380158e+00\n",
      "Epoch: 5306 mean train loss:  2.91208853e-05, mean val. loss:  1.75424850e+00\n",
      "Epoch: 5307 mean train loss:  2.89834861e-05, mean val. loss:  1.75468683e+00\n",
      "Epoch: 5308 mean train loss:  2.96101207e-05, mean val. loss:  1.75511765e+00\n",
      "Epoch: 5309 mean train loss:  2.92965851e-05, mean val. loss:  1.75554323e+00\n",
      "Epoch: 5310 mean train loss:  2.90312746e-05, mean val. loss:  1.75596535e+00\n",
      "Epoch: 5311 mean train loss:  2.90722819e-05, mean val. loss:  1.75638819e+00\n",
      "Epoch: 5312 mean train loss:  2.88751908e-05, mean val. loss:  1.75682342e+00\n",
      "Epoch: 5313 mean train loss:  2.94582569e-05, mean val. loss:  1.75724006e+00\n",
      "Epoch: 5314 mean train loss:  2.87913135e-05, mean val. loss:  1.75765383e+00\n",
      "Epoch: 5315 mean train loss:  2.92748155e-05, mean val. loss:  1.75806355e+00\n",
      "Epoch: 5316 mean train loss:  2.90666358e-05, mean val. loss:  1.75846589e+00\n",
      "Epoch: 5317 mean train loss:  2.86169525e-05, mean val. loss:  1.75885594e+00\n",
      "Epoch: 5318 mean train loss:  2.88755982e-05, mean val. loss:  1.75924540e+00\n",
      "Epoch: 5319 mean train loss:  2.89765012e-05, mean val. loss:  1.75962973e+00\n",
      "Epoch: 5320 mean train loss:  2.95202772e-05, mean val. loss:  1.76001525e+00\n",
      "Epoch: 5321 mean train loss:  2.90508906e-05, mean val. loss:  1.76038218e+00\n",
      "Epoch: 5322 mean train loss:  2.89188174e-05, mean val. loss:  1.76075161e+00\n",
      "Epoch: 5323 mean train loss:  2.88790907e-05, mean val. loss:  1.76111579e+00\n",
      "Epoch: 5324 mean train loss:  2.93640769e-05, mean val. loss:  1.76147020e+00\n",
      "Epoch: 5325 mean train loss:  2.86826689e-05, mean val. loss:  1.76181364e+00\n",
      "Epoch: 5326 mean train loss:  2.95564823e-05, mean val. loss:  1.76215041e+00\n",
      "Epoch: 5327 mean train loss:  2.87868024e-05, mean val. loss:  1.76249146e+00\n",
      "Epoch: 5328 mean train loss:  2.91353790e-05, mean val. loss:  1.76283574e+00\n",
      "Epoch: 5329 mean train loss:  2.93282792e-05, mean val. loss:  1.76317084e+00\n",
      "Epoch: 5330 mean train loss:  2.90674507e-05, mean val. loss:  1.76349723e+00\n",
      "Epoch: 5331 mean train loss:  2.94092752e-05, mean val. loss:  1.76381946e+00\n",
      "Epoch: 5332 mean train loss:  2.84064154e-05, mean val. loss:  1.76412511e+00\n",
      "Epoch: 5333 mean train loss:  2.88440788e-05, mean val. loss:  1.76442659e+00\n",
      "Epoch: 5334 mean train loss:  2.93015619e-05, mean val. loss:  1.76473045e+00\n",
      "Epoch: 5335 mean train loss:  2.91470787e-05, mean val. loss:  1.76501882e+00\n",
      "Epoch: 5336 mean train loss:  2.85039132e-05, mean val. loss:  1.76529908e+00\n",
      "Epoch: 5337 mean train loss:  2.95551727e-05, mean val. loss:  1.76556981e+00\n",
      "Epoch: 5338 mean train loss:  2.86186987e-05, mean val. loss:  1.76583278e+00\n",
      "Epoch: 5339 mean train loss:  2.83299305e-05, mean val. loss:  1.76609480e+00\n",
      "Epoch: 5340 mean train loss:  2.93064222e-05, mean val. loss:  1.76635134e+00\n",
      "Epoch: 5341 mean train loss:  2.87331932e-05, mean val. loss:  1.76660657e+00\n",
      "Epoch: 5342 mean train loss:  2.83971895e-05, mean val. loss:  1.76686168e+00\n",
      "Epoch: 5343 mean train loss:  2.85548740e-05, mean val. loss:  1.76712787e+00\n",
      "Epoch: 5344 mean train loss:  2.95983627e-05, mean val. loss:  1.76740134e+00\n",
      "Epoch: 5345 mean train loss:  2.86489376e-05, mean val. loss:  1.76769304e+00\n",
      "Epoch: 5346 mean train loss:  2.89800228e-05, mean val. loss:  1.76799405e+00\n",
      "Epoch: 5347 mean train loss:  2.87960866e-05, mean val. loss:  1.76830876e+00\n",
      "Epoch: 5348 mean train loss:  2.85917195e-05, mean val. loss:  1.76864076e+00\n",
      "Epoch: 5349 mean train loss:  2.87753064e-05, mean val. loss:  1.76898038e+00\n",
      "Epoch: 5350 mean train loss:  2.93835474e-05, mean val. loss:  1.76932836e+00\n",
      "Epoch: 5351 mean train loss:  2.85755377e-05, mean val. loss:  1.76969469e+00\n",
      "Epoch: 5352 mean train loss:  2.85146816e-05, mean val. loss:  1.77007115e+00\n",
      "Epoch: 5353 mean train loss:  2.95392529e-05, mean val. loss:  1.77044845e+00\n",
      "Epoch: 5354 mean train loss:  2.89485033e-05, mean val. loss:  1.77084506e+00\n",
      "Epoch: 5355 mean train loss:  2.89496675e-05, mean val. loss:  1.77125049e+00\n",
      "Epoch: 5356 mean train loss:  2.88219308e-05, mean val. loss:  1.77166641e+00\n",
      "Epoch: 5357 mean train loss:  2.91119795e-05, mean val. loss:  1.77208126e+00\n",
      "Epoch: 5358 mean train loss:  2.91043543e-05, mean val. loss:  1.77250242e+00\n",
      "Epoch: 5359 mean train loss:  2.89388990e-05, mean val. loss:  1.77293015e+00\n",
      "Epoch: 5360 mean train loss:  2.87806324e-05, mean val. loss:  1.77335560e+00\n",
      "Epoch: 5361 mean train loss:  2.86391005e-05, mean val. loss:  1.77377331e+00\n",
      "Epoch: 5362 mean train loss:  2.89560412e-05, mean val. loss:  1.77417338e+00\n",
      "Epoch: 5363 mean train loss:  2.87142175e-05, mean val. loss:  1.77456415e+00\n",
      "Epoch: 5364 mean train loss:  2.92071200e-05, mean val. loss:  1.77494490e+00\n",
      "Epoch: 5365 mean train loss:  2.84077832e-05, mean val. loss:  1.77531683e+00\n",
      "Epoch: 5366 mean train loss:  2.89416639e-05, mean val. loss:  1.77567732e+00\n",
      "Epoch: 5367 mean train loss:  2.87919829e-05, mean val. loss:  1.77602959e+00\n",
      "Epoch: 5368 mean train loss:  2.93694902e-05, mean val. loss:  1.77637219e+00\n",
      "Epoch: 5369 mean train loss:  2.91766773e-05, mean val. loss:  1.77670360e+00\n",
      "Epoch: 5370 mean train loss:  2.91186152e-05, mean val. loss:  1.77702224e+00\n",
      "Epoch: 5371 mean train loss:  2.93869234e-05, mean val. loss:  1.77733839e+00\n",
      "Epoch: 5372 mean train loss:  2.89422751e-05, mean val. loss:  1.77763057e+00\n",
      "Epoch: 5373 mean train loss:  2.92350887e-05, mean val. loss:  1.77790570e+00\n",
      "Epoch: 5374 mean train loss:  2.90272874e-05, mean val. loss:  1.77818298e+00\n",
      "Epoch: 5375 mean train loss:  2.88022566e-05, mean val. loss:  1.77844477e+00\n",
      "Epoch: 5376 mean train loss:  2.94351776e-05, mean val. loss:  1.77870178e+00\n",
      "Epoch: 5377 mean train loss:  2.88176525e-05, mean val. loss:  1.77894735e+00\n",
      "Epoch: 5378 mean train loss:  2.89870368e-05, mean val. loss:  1.77919400e+00\n",
      "Epoch: 5379 mean train loss:  2.85368878e-05, mean val. loss:  1.77944458e+00\n",
      "Epoch: 5380 mean train loss:  2.91255710e-05, mean val. loss:  1.77969170e+00\n",
      "Epoch: 5381 mean train loss:  2.95022037e-05, mean val. loss:  1.77993762e+00\n",
      "Epoch: 5382 mean train loss:  2.84667476e-05, mean val. loss:  1.78018951e+00\n",
      "Epoch: 5383 mean train loss:  2.87139846e-05, mean val. loss:  1.78044939e+00\n",
      "Epoch: 5384 mean train loss:  2.93063640e-05, mean val. loss:  1.78071201e+00\n",
      "Epoch: 5385 mean train loss:  2.86729191e-05, mean val. loss:  1.78097510e+00\n",
      "Epoch: 5386 mean train loss:  2.82537076e-05, mean val. loss:  1.78124654e+00\n",
      "Epoch: 5387 mean train loss:  2.88287993e-05, mean val. loss:  1.78152370e+00\n",
      "Epoch: 5388 mean train loss:  2.93300836e-05, mean val. loss:  1.78179979e+00\n",
      "Epoch: 5389 mean train loss:  2.91692559e-05, mean val. loss:  1.78208327e+00\n",
      "Epoch: 5390 mean train loss:  2.88556330e-05, mean val. loss:  1.78237224e+00\n",
      "Epoch: 5391 mean train loss:  2.88620649e-05, mean val. loss:  1.78268898e+00\n",
      "Epoch: 5392 mean train loss:  2.88888114e-05, mean val. loss:  1.78301775e+00\n",
      "Epoch: 5393 mean train loss:  2.85540591e-05, mean val. loss:  1.78336692e+00\n",
      "Epoch: 5394 mean train loss:  2.83254485e-05, mean val. loss:  1.78372705e+00\n",
      "Epoch: 5395 mean train loss:  2.94785714e-05, mean val. loss:  1.78410578e+00\n",
      "Epoch: 5396 mean train loss:  2.84781854e-05, mean val. loss:  1.78449821e+00\n",
      "Epoch: 5397 mean train loss:  2.88576994e-05, mean val. loss:  1.78490222e+00\n",
      "Epoch: 5398 mean train loss:  2.84188427e-05, mean val. loss:  1.78531158e+00\n",
      "Epoch: 5399 mean train loss:  2.93591293e-05, mean val. loss:  1.78574204e+00\n",
      "Epoch: 5400 mean train loss:  2.92562181e-05, mean val. loss:  1.78616905e+00\n",
      "Epoch: 5401 mean train loss:  2.90397438e-05, mean val. loss:  1.78660679e+00\n",
      "Epoch: 5402 mean train loss:  2.84510315e-05, mean val. loss:  1.78705931e+00\n",
      "Epoch: 5403 mean train loss:  2.90249009e-05, mean val. loss:  1.78749764e+00\n",
      "Epoch: 5404 mean train loss:  2.88455049e-05, mean val. loss:  1.78792989e+00\n",
      "Epoch: 5405 mean train loss:  2.89411691e-05, mean val. loss:  1.78837073e+00\n",
      "Epoch: 5406 mean train loss:  2.86822324e-05, mean val. loss:  1.78880239e+00\n",
      "Epoch: 5407 mean train loss:  2.88613373e-05, mean val. loss:  1.78922296e+00\n",
      "Epoch: 5408 mean train loss:  2.89703603e-05, mean val. loss:  1.78963614e+00\n",
      "Epoch: 5409 mean train loss:  2.85121787e-05, mean val. loss:  1.79002762e+00\n",
      "Epoch: 5410 mean train loss:  2.86090362e-05, mean val. loss:  1.79042089e+00\n",
      "Epoch: 5411 mean train loss:  2.85030110e-05, mean val. loss:  1.79080892e+00\n",
      "Epoch: 5412 mean train loss:  2.88304291e-05, mean val. loss:  1.79118252e+00\n",
      "Epoch: 5413 mean train loss:  2.85271090e-05, mean val. loss:  1.79155731e+00\n",
      "Epoch: 5414 mean train loss:  2.91690812e-05, mean val. loss:  1.79192185e+00\n",
      "Epoch: 5415 mean train loss:  2.86818540e-05, mean val. loss:  1.79227960e+00\n",
      "Epoch: 5416 mean train loss:  2.85774004e-05, mean val. loss:  1.79263115e+00\n",
      "Epoch: 5417 mean train loss:  2.88173906e-05, mean val. loss:  1.79297113e+00\n",
      "Epoch: 5418 mean train loss:  2.91493780e-05, mean val. loss:  1.79329586e+00\n",
      "Epoch: 5419 mean train loss:  2.88087467e-05, mean val. loss:  1.79361904e+00\n",
      "Epoch: 5420 mean train loss:  2.89710006e-05, mean val. loss:  1.79393947e+00\n",
      "Epoch: 5421 mean train loss:  2.90400058e-05, mean val. loss:  1.79425836e+00\n",
      "Epoch: 5422 mean train loss:  2.97322113e-05, mean val. loss:  1.79456651e+00\n",
      "Epoch: 5423 mean train loss:  2.88175943e-05, mean val. loss:  1.79486275e+00\n",
      "Epoch: 5424 mean train loss:  2.87405564e-05, mean val. loss:  1.79514825e+00\n",
      "Epoch: 5425 mean train loss:  2.87670991e-05, mean val. loss:  1.79542029e+00\n",
      "Epoch: 5426 mean train loss:  2.90646276e-05, mean val. loss:  1.79568958e+00\n",
      "Epoch: 5427 mean train loss:  2.91970500e-05, mean val. loss:  1.79595375e+00\n",
      "Epoch: 5428 mean train loss:  2.87142757e-05, mean val. loss:  1.79620624e+00\n",
      "Epoch: 5429 mean train loss:  2.86635768e-05, mean val. loss:  1.79644871e+00\n",
      "Epoch: 5430 mean train loss:  2.83143891e-05, mean val. loss:  1.79668653e+00\n",
      "Epoch: 5431 mean train loss:  2.89225718e-05, mean val. loss:  1.79690599e+00\n",
      "Epoch: 5432 mean train loss:  2.85538845e-05, mean val. loss:  1.79713392e+00\n",
      "Epoch: 5433 mean train loss:  2.85491697e-05, mean val. loss:  1.79736412e+00\n",
      "Epoch: 5434 mean train loss:  2.92487966e-05, mean val. loss:  1.79759812e+00\n",
      "Epoch: 5435 mean train loss:  2.83303671e-05, mean val. loss:  1.79784107e+00\n",
      "Epoch: 5436 mean train loss:  2.87153525e-05, mean val. loss:  1.79809713e+00\n",
      "Epoch: 5437 mean train loss:  2.85955030e-05, mean val. loss:  1.79836571e+00\n",
      "Epoch: 5438 mean train loss:  2.86968716e-05, mean val. loss:  1.79864275e+00\n",
      "Epoch: 5439 mean train loss:  2.90893076e-05, mean val. loss:  1.79894018e+00\n",
      "Epoch: 5440 mean train loss:  2.86162540e-05, mean val. loss:  1.79926181e+00\n",
      "Epoch: 5441 mean train loss:  2.89238233e-05, mean val. loss:  1.79959142e+00\n",
      "Epoch: 5442 mean train loss:  2.94723723e-05, mean val. loss:  1.79993379e+00\n",
      "Epoch: 5443 mean train loss:  2.80245440e-05, mean val. loss:  1.80030167e+00\n",
      "Epoch: 5444 mean train loss:  2.87298171e-05, mean val. loss:  1.80067527e+00\n",
      "Epoch: 5445 mean train loss:  2.86536233e-05, mean val. loss:  1.80105376e+00\n",
      "Epoch: 5446 mean train loss:  2.87092989e-05, mean val. loss:  1.80144489e+00\n",
      "Epoch: 5447 mean train loss:  2.84493726e-05, mean val. loss:  1.80183971e+00\n",
      "Epoch: 5448 mean train loss:  2.87821167e-05, mean val. loss:  1.80224609e+00\n",
      "Epoch: 5449 mean train loss:  2.89559539e-05, mean val. loss:  1.80264425e+00\n",
      "Epoch: 5450 mean train loss:  2.87078146e-05, mean val. loss:  1.80304885e+00\n",
      "Epoch: 5451 mean train loss:  2.83065601e-05, mean val. loss:  1.80345869e+00\n",
      "Epoch: 5452 mean train loss:  2.87045841e-05, mean val. loss:  1.80387104e+00\n",
      "Epoch: 5453 mean train loss:  2.88961746e-05, mean val. loss:  1.80428195e+00\n",
      "Epoch: 5454 mean train loss:  2.83374975e-05, mean val. loss:  1.80470228e+00\n",
      "Epoch: 5455 mean train loss:  2.88382580e-05, mean val. loss:  1.80511522e+00\n",
      "Epoch: 5456 mean train loss:  2.89599993e-05, mean val. loss:  1.80554068e+00\n",
      "Epoch: 5457 mean train loss:  2.84899434e-05, mean val. loss:  1.80596709e+00\n",
      "Epoch: 5458 mean train loss:  2.89272284e-05, mean val. loss:  1.80639601e+00\n",
      "Epoch: 5459 mean train loss:  2.82777473e-05, mean val. loss:  1.80682230e+00\n",
      "Epoch: 5460 mean train loss:  2.84367998e-05, mean val. loss:  1.80724728e+00\n",
      "Epoch: 5461 mean train loss:  2.84599082e-05, mean val. loss:  1.80766749e+00\n",
      "Epoch: 5462 mean train loss:  2.90629396e-05, mean val. loss:  1.80808580e+00\n",
      "Epoch: 5463 mean train loss:  2.91503966e-05, mean val. loss:  1.80849469e+00\n",
      "Epoch: 5464 mean train loss:  2.85455317e-05, mean val. loss:  1.80889726e+00\n",
      "Epoch: 5465 mean train loss:  2.87434086e-05, mean val. loss:  1.80929017e+00\n",
      "Epoch: 5466 mean train loss:  2.85138376e-05, mean val. loss:  1.80968440e+00\n",
      "Epoch: 5467 mean train loss:  2.86842114e-05, mean val. loss:  1.81006157e+00\n",
      "Epoch: 5468 mean train loss:  2.93715857e-05, mean val. loss:  1.81042933e+00\n",
      "Epoch: 5469 mean train loss:  2.84848502e-05, mean val. loss:  1.81079578e+00\n",
      "Epoch: 5470 mean train loss:  2.91479228e-05, mean val. loss:  1.81115067e+00\n",
      "Epoch: 5471 mean train loss:  2.85780698e-05, mean val. loss:  1.81149697e+00\n",
      "Epoch: 5472 mean train loss:  2.81868270e-05, mean val. loss:  1.81183970e+00\n",
      "Epoch: 5473 mean train loss:  2.91944889e-05, mean val. loss:  1.81217051e+00\n",
      "Epoch: 5474 mean train loss:  2.89178279e-05, mean val. loss:  1.81249595e+00\n",
      "Epoch: 5475 mean train loss:  2.84808921e-05, mean val. loss:  1.81280744e+00\n",
      "Epoch: 5476 mean train loss:  2.91249598e-05, mean val. loss:  1.81309795e+00\n",
      "Epoch: 5477 mean train loss:  2.88303127e-05, mean val. loss:  1.81337059e+00\n",
      "Epoch: 5478 mean train loss:  2.80110689e-05, mean val. loss:  1.81362545e+00\n",
      "Epoch: 5479 mean train loss:  2.86134309e-05, mean val. loss:  1.81388819e+00\n",
      "Epoch: 5480 mean train loss:  2.80588865e-05, mean val. loss:  1.81414151e+00\n",
      "Epoch: 5481 mean train loss:  2.88687879e-05, mean val. loss:  1.81439006e+00\n",
      "Epoch: 5482 mean train loss:  2.81637185e-05, mean val. loss:  1.81464624e+00\n",
      "Epoch: 5483 mean train loss:  2.87900330e-05, mean val. loss:  1.81489789e+00\n",
      "Epoch: 5484 mean train loss:  2.84092966e-05, mean val. loss:  1.81515014e+00\n",
      "Epoch: 5485 mean train loss:  2.82305700e-05, mean val. loss:  1.81541252e+00\n",
      "Epoch: 5486 mean train loss:  2.88417505e-05, mean val. loss:  1.81568384e+00\n",
      "Epoch: 5487 mean train loss:  2.81550747e-05, mean val. loss:  1.81597185e+00\n",
      "Epoch: 5488 mean train loss:  2.91897450e-05, mean val. loss:  1.81626141e+00\n",
      "Epoch: 5489 mean train loss:  2.85211718e-05, mean val. loss:  1.81655705e+00\n",
      "Epoch: 5490 mean train loss:  2.84666312e-05, mean val. loss:  1.81687534e+00\n",
      "Epoch: 5491 mean train loss:  2.90586031e-05, mean val. loss:  1.81721067e+00\n",
      "Epoch: 5492 mean train loss:  2.80727691e-05, mean val. loss:  1.81754971e+00\n",
      "Epoch: 5493 mean train loss:  2.87475996e-05, mean val. loss:  1.81790304e+00\n",
      "Epoch: 5494 mean train loss:  2.85090646e-05, mean val. loss:  1.81825614e+00\n",
      "Epoch: 5495 mean train loss:  2.76310020e-05, mean val. loss:  1.81860781e+00\n",
      "Epoch: 5496 mean train loss:  2.80601089e-05, mean val. loss:  1.81895876e+00\n",
      "Epoch: 5497 mean train loss:  2.88097071e-05, mean val. loss:  1.81929743e+00\n",
      "Epoch: 5498 mean train loss:  2.85058923e-05, mean val. loss:  1.81964874e+00\n",
      "Epoch: 5499 mean train loss:  2.82551337e-05, mean val. loss:  1.82001102e+00\n",
      "Epoch: 5500 mean train loss:  2.80562090e-05, mean val. loss:  1.82038271e+00\n",
      "Epoch: 5501 mean train loss:  2.86938739e-05, mean val. loss:  1.82075500e+00\n",
      "Epoch: 5502 mean train loss:  2.85320857e-05, mean val. loss:  1.82113111e+00\n",
      "Epoch: 5503 mean train loss:  2.89789168e-05, mean val. loss:  1.82151175e+00\n",
      "Epoch: 5504 mean train loss:  2.84501584e-05, mean val. loss:  1.82188964e+00\n",
      "Epoch: 5505 mean train loss:  2.86197755e-05, mean val. loss:  1.82226992e+00\n",
      "Epoch: 5506 mean train loss:  2.84521666e-05, mean val. loss:  1.82265663e+00\n",
      "Epoch: 5507 mean train loss:  2.81988468e-05, mean val. loss:  1.82304680e+00\n",
      "Epoch: 5508 mean train loss:  2.87226867e-05, mean val. loss:  1.82342303e+00\n",
      "Epoch: 5509 mean train loss:  2.86396826e-05, mean val. loss:  1.82379353e+00\n",
      "Epoch: 5510 mean train loss:  2.83838890e-05, mean val. loss:  1.82416737e+00\n",
      "Epoch: 5511 mean train loss:  2.90163443e-05, mean val. loss:  1.82452905e+00\n",
      "Epoch: 5512 mean train loss:  2.79336236e-05, mean val. loss:  1.82488549e+00\n",
      "Epoch: 5513 mean train loss:  2.87727744e-05, mean val. loss:  1.82524431e+00\n",
      "Epoch: 5514 mean train loss:  2.82952678e-05, mean val. loss:  1.82559752e+00\n",
      "Epoch: 5515 mean train loss:  2.83110421e-05, mean val. loss:  1.82595325e+00\n",
      "Epoch: 5516 mean train loss:  2.81088869e-05, mean val. loss:  1.82629693e+00\n",
      "Epoch: 5517 mean train loss:  2.85056885e-05, mean val. loss:  1.82664073e+00\n",
      "Epoch: 5518 mean train loss:  2.84352864e-05, mean val. loss:  1.82698524e+00\n",
      "Epoch: 5519 mean train loss:  2.86853465e-05, mean val. loss:  1.82733750e+00\n",
      "Epoch: 5520 mean train loss:  2.84903508e-05, mean val. loss:  1.82768357e+00\n",
      "Epoch: 5521 mean train loss:  2.87565053e-05, mean val. loss:  1.82802546e+00\n",
      "Epoch: 5522 mean train loss:  2.90732714e-05, mean val. loss:  1.82837379e+00\n",
      "Epoch: 5523 mean train loss:  2.85746646e-05, mean val. loss:  1.82872486e+00\n",
      "Epoch: 5524 mean train loss:  2.83463160e-05, mean val. loss:  1.82909167e+00\n",
      "Epoch: 5525 mean train loss:  2.88517622e-05, mean val. loss:  1.82945371e+00\n",
      "Epoch: 5526 mean train loss:  2.83806585e-05, mean val. loss:  1.82980955e+00\n",
      "Epoch: 5527 mean train loss:  2.82142428e-05, mean val. loss:  1.83016646e+00\n",
      "Epoch: 5528 mean train loss:  2.78747757e-05, mean val. loss:  1.83053041e+00\n",
      "Epoch: 5529 mean train loss:  2.80217500e-05, mean val. loss:  1.83088553e+00\n",
      "Epoch: 5530 mean train loss:  2.87413422e-05, mean val. loss:  1.83123291e+00\n",
      "Epoch: 5531 mean train loss:  2.89352029e-05, mean val. loss:  1.83158302e+00\n",
      "Epoch: 5532 mean train loss:  2.84719863e-05, mean val. loss:  1.83192742e+00\n",
      "Epoch: 5533 mean train loss:  2.84763228e-05, mean val. loss:  1.83226669e+00\n",
      "Epoch: 5534 mean train loss:  2.84977141e-05, mean val. loss:  1.83260596e+00\n",
      "Epoch: 5535 mean train loss:  2.85534188e-05, mean val. loss:  1.83293259e+00\n",
      "Epoch: 5536 mean train loss:  2.84605776e-05, mean val. loss:  1.83325362e+00\n",
      "Epoch: 5537 mean train loss:  2.82592082e-05, mean val. loss:  1.83356571e+00\n",
      "Epoch: 5538 mean train loss:  2.87365110e-05, mean val. loss:  1.83387625e+00\n",
      "Epoch: 5539 mean train loss:  2.84889829e-05, mean val. loss:  1.83418202e+00\n",
      "Epoch: 5540 mean train loss:  2.84660491e-05, mean val. loss:  1.83448541e+00\n",
      "Epoch: 5541 mean train loss:  2.89474847e-05, mean val. loss:  1.83479142e+00\n",
      "Epoch: 5542 mean train loss:  2.81209941e-05, mean val. loss:  1.83509159e+00\n",
      "Epoch: 5543 mean train loss:  2.85906426e-05, mean val. loss:  1.83539116e+00\n",
      "Epoch: 5544 mean train loss:  2.91149481e-05, mean val. loss:  1.83569229e+00\n",
      "Epoch: 5545 mean train loss:  2.83900008e-05, mean val. loss:  1.83598709e+00\n",
      "Epoch: 5546 mean train loss:  2.81926477e-05, mean val. loss:  1.83627367e+00\n",
      "Epoch: 5547 mean train loss:  2.86204158e-05, mean val. loss:  1.83655751e+00\n",
      "Epoch: 5548 mean train loss:  2.89014424e-05, mean val. loss:  1.83684635e+00\n",
      "Epoch: 5549 mean train loss:  2.85888382e-05, mean val. loss:  1.83714175e+00\n",
      "Epoch: 5550 mean train loss:  2.86009454e-05, mean val. loss:  1.83743370e+00\n",
      "Epoch: 5551 mean train loss:  2.83614499e-05, mean val. loss:  1.83771515e+00\n",
      "Epoch: 5552 mean train loss:  2.87893345e-05, mean val. loss:  1.83799076e+00\n",
      "Epoch: 5553 mean train loss:  2.82220135e-05, mean val. loss:  1.83827949e+00\n",
      "Epoch: 5554 mean train loss:  2.86058057e-05, mean val. loss:  1.83856726e+00\n",
      "Epoch: 5555 mean train loss:  2.83593545e-05, mean val. loss:  1.83884680e+00\n",
      "Epoch: 5556 mean train loss:  2.87804869e-05, mean val. loss:  1.83912325e+00\n",
      "Epoch: 5557 mean train loss:  2.80961394e-05, mean val. loss:  1.83941317e+00\n",
      "Epoch: 5558 mean train loss:  2.83014670e-05, mean val. loss:  1.83970106e+00\n",
      "Epoch: 5559 mean train loss:  2.88089213e-05, mean val. loss:  1.83999038e+00\n",
      "Epoch: 5560 mean train loss:  2.78395601e-05, mean val. loss:  1.84028494e+00\n",
      "Epoch: 5561 mean train loss:  2.86978320e-05, mean val. loss:  1.84057641e+00\n",
      "Epoch: 5562 mean train loss:  2.78340303e-05, mean val. loss:  1.84087026e+00\n",
      "Epoch: 5563 mean train loss:  2.88839801e-05, mean val. loss:  1.84117675e+00\n",
      "Epoch: 5564 mean train loss:  2.85907008e-05, mean val. loss:  1.84149539e+00\n",
      "Epoch: 5565 mean train loss:  2.90056632e-05, mean val. loss:  1.84182298e+00\n",
      "Epoch: 5566 mean train loss:  2.84216949e-05, mean val. loss:  1.84216404e+00\n",
      "Epoch: 5567 mean train loss:  2.82945985e-05, mean val. loss:  1.84250784e+00\n",
      "Epoch: 5568 mean train loss:  2.86625000e-05, mean val. loss:  1.84285676e+00\n",
      "Epoch: 5569 mean train loss:  2.83536792e-05, mean val. loss:  1.84320796e+00\n",
      "Epoch: 5570 mean train loss:  2.86919822e-05, mean val. loss:  1.84356117e+00\n",
      "Epoch: 5571 mean train loss:  2.86831055e-05, mean val. loss:  1.84390962e+00\n",
      "Epoch: 5572 mean train loss:  2.81585963e-05, mean val. loss:  1.84426427e+00\n",
      "Epoch: 5573 mean train loss:  2.82315013e-05, mean val. loss:  1.84461510e+00\n",
      "Epoch: 5574 mean train loss:  2.83034460e-05, mean val. loss:  1.84496498e+00\n",
      "Epoch: 5575 mean train loss:  2.82536203e-05, mean val. loss:  1.84532106e+00\n",
      "Epoch: 5576 mean train loss:  2.80833920e-05, mean val. loss:  1.84567034e+00\n",
      "Epoch: 5577 mean train loss:  2.82269611e-05, mean val. loss:  1.84602129e+00\n",
      "Epoch: 5578 mean train loss:  2.79927335e-05, mean val. loss:  1.84636950e+00\n",
      "Epoch: 5579 mean train loss:  2.82795518e-05, mean val. loss:  1.84670961e+00\n",
      "Epoch: 5580 mean train loss:  2.82800756e-05, mean val. loss:  1.84704804e+00\n",
      "Epoch: 5581 mean train loss:  2.80376407e-05, mean val. loss:  1.84738600e+00\n",
      "Epoch: 5582 mean train loss:  2.82954425e-05, mean val. loss:  1.84771764e+00\n",
      "Epoch: 5583 mean train loss:  2.82183464e-05, mean val. loss:  1.84805191e+00\n",
      "Epoch: 5584 mean train loss:  2.81014654e-05, mean val. loss:  1.84839535e+00\n",
      "Epoch: 5585 mean train loss:  2.81298126e-05, mean val. loss:  1.84873116e+00\n",
      "Epoch: 5586 mean train loss:  2.84247508e-05, mean val. loss:  1.84906173e+00\n",
      "Epoch: 5587 mean train loss:  2.80625827e-05, mean val. loss:  1.84939742e+00\n",
      "Epoch: 5588 mean train loss:  2.83945119e-05, mean val. loss:  1.84972751e+00\n",
      "Epoch: 5589 mean train loss:  2.82372930e-05, mean val. loss:  1.85006690e+00\n",
      "Epoch: 5590 mean train loss:  2.87383446e-05, mean val. loss:  1.85040736e+00\n",
      "Epoch: 5591 mean train loss:  2.89045274e-05, mean val. loss:  1.85074019e+00\n",
      "Epoch: 5592 mean train loss:  2.85598217e-05, mean val. loss:  1.85106242e+00\n",
      "Epoch: 5593 mean train loss:  2.81084504e-05, mean val. loss:  1.85139120e+00\n",
      "Epoch: 5594 mean train loss:  2.83806585e-05, mean val. loss:  1.85172653e+00\n",
      "Epoch: 5595 mean train loss:  2.81061220e-05, mean val. loss:  1.85206139e+00\n",
      "Epoch: 5596 mean train loss:  2.82671244e-05, mean val. loss:  1.85238647e+00\n",
      "Epoch: 5597 mean train loss:  2.82386900e-05, mean val. loss:  1.85270739e+00\n",
      "Epoch: 5598 mean train loss:  2.83118861e-05, mean val. loss:  1.85302937e+00\n",
      "Epoch: 5599 mean train loss:  2.87387229e-05, mean val. loss:  1.85333645e+00\n",
      "Epoch: 5600 mean train loss:  2.81666289e-05, mean val. loss:  1.85365522e+00\n",
      "Epoch: 5601 mean train loss:  2.86591239e-05, mean val. loss:  1.85396874e+00\n",
      "Epoch: 5602 mean train loss:  2.78387743e-05, mean val. loss:  1.85428679e+00\n",
      "Epoch: 5603 mean train loss:  2.84517591e-05, mean val. loss:  1.85458791e+00\n",
      "Epoch: 5604 mean train loss:  2.86810682e-05, mean val. loss:  1.85488570e+00\n",
      "Epoch: 5605 mean train loss:  2.79732922e-05, mean val. loss:  1.85518181e+00\n",
      "Epoch: 5606 mean train loss:  2.82931433e-05, mean val. loss:  1.85548210e+00\n",
      "Epoch: 5607 mean train loss:  2.81247776e-05, mean val. loss:  1.85578156e+00\n",
      "Epoch: 5608 mean train loss:  2.79844680e-05, mean val. loss:  1.85608268e+00\n",
      "Epoch: 5609 mean train loss:  2.83791742e-05, mean val. loss:  1.85637867e+00\n",
      "Epoch: 5610 mean train loss:  2.80400855e-05, mean val. loss:  1.85667348e+00\n",
      "Epoch: 5611 mean train loss:  2.82202673e-05, mean val. loss:  1.85696232e+00\n",
      "Epoch: 5612 mean train loss:  2.80152599e-05, mean val. loss:  1.85726166e+00\n",
      "Epoch: 5613 mean train loss:  2.81898247e-05, mean val. loss:  1.85755420e+00\n",
      "Epoch: 5614 mean train loss:  2.85319111e-05, mean val. loss:  1.85784841e+00\n",
      "Epoch: 5615 mean train loss:  2.83147383e-05, mean val. loss:  1.85813797e+00\n",
      "Epoch: 5616 mean train loss:  2.84596754e-05, mean val. loss:  1.85842657e+00\n",
      "Epoch: 5617 mean train loss:  2.80144741e-05, mean val. loss:  1.85872793e+00\n",
      "Epoch: 5618 mean train loss:  2.81067914e-05, mean val. loss:  1.85902715e+00\n",
      "Epoch: 5619 mean train loss:  2.78381340e-05, mean val. loss:  1.85933256e+00\n",
      "Epoch: 5620 mean train loss:  2.82404071e-05, mean val. loss:  1.85965741e+00\n",
      "Epoch: 5621 mean train loss:  2.79518717e-05, mean val. loss:  1.85998487e+00\n",
      "Epoch: 5622 mean train loss:  2.82182009e-05, mean val. loss:  1.86031640e+00\n",
      "Epoch: 5623 mean train loss:  2.78727966e-05, mean val. loss:  1.86064827e+00\n",
      "Epoch: 5624 mean train loss:  2.82876717e-05, mean val. loss:  1.86099207e+00\n",
      "Epoch: 5625 mean train loss:  2.82939000e-05, mean val. loss:  1.86134911e+00\n",
      "Epoch: 5626 mean train loss:  2.86363647e-05, mean val. loss:  1.86171353e+00\n",
      "Epoch: 5627 mean train loss:  2.80896493e-05, mean val. loss:  1.86207950e+00\n",
      "Epoch: 5628 mean train loss:  2.77722429e-05, mean val. loss:  1.86245620e+00\n",
      "Epoch: 5629 mean train loss:  2.81071407e-05, mean val. loss:  1.86283123e+00\n",
      "Epoch: 5630 mean train loss:  2.81126122e-05, mean val. loss:  1.86319709e+00\n",
      "Epoch: 5631 mean train loss:  2.80188979e-05, mean val. loss:  1.86356151e+00\n",
      "Epoch: 5632 mean train loss:  2.85981223e-05, mean val. loss:  1.86391926e+00\n",
      "Epoch: 5633 mean train loss:  2.82282708e-05, mean val. loss:  1.86428189e+00\n",
      "Epoch: 5634 mean train loss:  2.84419220e-05, mean val. loss:  1.86463630e+00\n",
      "Epoch: 5635 mean train loss:  2.81616230e-05, mean val. loss:  1.86496556e+00\n",
      "Epoch: 5636 mean train loss:  2.82558030e-05, mean val. loss:  1.86528981e+00\n",
      "Epoch: 5637 mean train loss:  2.80823442e-05, mean val. loss:  1.86560094e+00\n",
      "Epoch: 5638 mean train loss:  2.81424727e-05, mean val. loss:  1.86589944e+00\n",
      "Epoch: 5639 mean train loss:  2.80553650e-05, mean val. loss:  1.86618507e+00\n",
      "Epoch: 5640 mean train loss:  2.85639835e-05, mean val. loss:  1.86646116e+00\n",
      "Epoch: 5641 mean train loss:  2.81836838e-05, mean val. loss:  1.86673343e+00\n",
      "Epoch: 5642 mean train loss:  2.82259425e-05, mean val. loss:  1.86698747e+00\n",
      "Epoch: 5643 mean train loss:  2.77220679e-05, mean val. loss:  1.86725652e+00\n",
      "Epoch: 5644 mean train loss:  2.83127883e-05, mean val. loss:  1.86751688e+00\n",
      "Epoch: 5645 mean train loss:  2.79830419e-05, mean val. loss:  1.86777103e+00\n",
      "Epoch: 5646 mean train loss:  2.86481227e-05, mean val. loss:  1.86802590e+00\n",
      "Epoch: 5647 mean train loss:  2.82932888e-05, mean val. loss:  1.86828446e+00\n",
      "Epoch: 5648 mean train loss:  2.84544076e-05, mean val. loss:  1.86854398e+00\n",
      "Epoch: 5649 mean train loss:  2.77754152e-05, mean val. loss:  1.86880386e+00\n",
      "Epoch: 5650 mean train loss:  2.70996534e-05, mean val. loss:  1.86907113e+00\n",
      "Epoch: 5651 mean train loss:  2.81351968e-05, mean val. loss:  1.86933947e+00\n",
      "Epoch: 5652 mean train loss:  2.88597657e-05, mean val. loss:  1.86961210e+00\n",
      "Epoch: 5653 mean train loss:  2.76675855e-05, mean val. loss:  1.86987936e+00\n",
      "Epoch: 5654 mean train loss:  2.80769600e-05, mean val. loss:  1.87015545e+00\n",
      "Epoch: 5655 mean train loss:  2.80151435e-05, mean val. loss:  1.87043595e+00\n",
      "Epoch: 5656 mean train loss:  2.81340326e-05, mean val. loss:  1.87071753e+00\n",
      "Epoch: 5657 mean train loss:  2.82850233e-05, mean val. loss:  1.87101305e+00\n",
      "Epoch: 5658 mean train loss:  2.85083661e-05, mean val. loss:  1.87130749e+00\n",
      "Epoch: 5659 mean train loss:  2.79005035e-05, mean val. loss:  1.87160599e+00\n",
      "Epoch: 5660 mean train loss:  2.81394750e-05, mean val. loss:  1.87190306e+00\n",
      "Epoch: 5661 mean train loss:  2.77564104e-05, mean val. loss:  1.87220526e+00\n",
      "Epoch: 5662 mean train loss:  2.83979462e-05, mean val. loss:  1.87250865e+00\n",
      "Epoch: 5663 mean train loss:  2.78014923e-05, mean val. loss:  1.87282097e+00\n",
      "Epoch: 5664 mean train loss:  2.80445383e-05, mean val. loss:  1.87313235e+00\n",
      "Epoch: 5665 mean train loss:  2.84079579e-05, mean val. loss:  1.87344480e+00\n",
      "Epoch: 5666 mean train loss:  2.75864732e-05, mean val. loss:  1.87375546e+00\n",
      "Epoch: 5667 mean train loss:  2.81161047e-05, mean val. loss:  1.87406671e+00\n",
      "Epoch: 5668 mean train loss:  2.80952081e-05, mean val. loss:  1.87438416e+00\n",
      "Epoch: 5669 mean train loss:  2.79660744e-05, mean val. loss:  1.87470019e+00\n",
      "Epoch: 5670 mean train loss:  2.81232933e-05, mean val. loss:  1.87501657e+00\n",
      "Epoch: 5671 mean train loss:  2.84233538e-05, mean val. loss:  1.87533283e+00\n",
      "Epoch: 5672 mean train loss:  2.80752720e-05, mean val. loss:  1.87565374e+00\n",
      "Epoch: 5673 mean train loss:  2.82876426e-05, mean val. loss:  1.87596321e+00\n",
      "Epoch: 5674 mean train loss:  2.78333318e-05, mean val. loss:  1.87627292e+00\n",
      "Epoch: 5675 mean train loss:  2.78284424e-05, mean val. loss:  1.87657857e+00\n",
      "Epoch: 5676 mean train loss:  2.81163666e-05, mean val. loss:  1.87688708e+00\n",
      "Epoch: 5677 mean train loss:  2.78799562e-05, mean val. loss:  1.87719202e+00\n",
      "Epoch: 5678 mean train loss:  2.81384855e-05, mean val. loss:  1.87749302e+00\n",
      "Epoch: 5679 mean train loss:  2.86563300e-05, mean val. loss:  1.87778902e+00\n",
      "Epoch: 5680 mean train loss:  2.72928737e-05, mean val. loss:  1.87808204e+00\n",
      "Epoch: 5681 mean train loss:  2.83150584e-05, mean val. loss:  1.87837672e+00\n",
      "Epoch: 5682 mean train loss:  2.81465764e-05, mean val. loss:  1.87865222e+00\n",
      "Epoch: 5683 mean train loss:  2.79990199e-05, mean val. loss:  1.87892246e+00\n",
      "Epoch: 5684 mean train loss:  2.78599618e-05, mean val. loss:  1.87919044e+00\n",
      "Epoch: 5685 mean train loss:  2.81430548e-05, mean val. loss:  1.87945116e+00\n",
      "Epoch: 5686 mean train loss:  2.81582470e-05, mean val. loss:  1.87970018e+00\n",
      "Epoch: 5687 mean train loss:  2.83931149e-05, mean val. loss:  1.87994778e+00\n",
      "Epoch: 5688 mean train loss:  2.83046975e-05, mean val. loss:  1.88019538e+00\n",
      "Epoch: 5689 mean train loss:  2.83953559e-05, mean val. loss:  1.88044047e+00\n",
      "Epoch: 5690 mean train loss:  2.79995729e-05, mean val. loss:  1.88068151e+00\n",
      "Epoch: 5691 mean train loss:  2.77596992e-05, mean val. loss:  1.88092268e+00\n",
      "Epoch: 5692 mean train loss:  2.78777443e-05, mean val. loss:  1.88117373e+00\n",
      "Epoch: 5693 mean train loss:  2.77607760e-05, mean val. loss:  1.88142848e+00\n",
      "Epoch: 5694 mean train loss:  2.76309729e-05, mean val. loss:  1.88169062e+00\n",
      "Epoch: 5695 mean train loss:  2.79635715e-05, mean val. loss:  1.88195479e+00\n",
      "Epoch: 5696 mean train loss:  2.79388914e-05, mean val. loss:  1.88223028e+00\n",
      "Epoch: 5697 mean train loss:  2.75860366e-05, mean val. loss:  1.88252366e+00\n",
      "Epoch: 5698 mean train loss:  2.83006812e-05, mean val. loss:  1.88282585e+00\n",
      "Epoch: 5699 mean train loss:  2.81124085e-05, mean val. loss:  1.88313270e+00\n",
      "Epoch: 5700 mean train loss:  2.74238118e-05, mean val. loss:  1.88345289e+00\n",
      "Epoch: 5701 mean train loss:  2.81964312e-05, mean val. loss:  1.88376963e+00\n",
      "Epoch: 5702 mean train loss:  2.83260888e-05, mean val. loss:  1.88408744e+00\n",
      "Epoch: 5703 mean train loss:  2.84951238e-05, mean val. loss:  1.88440847e+00\n",
      "Epoch: 5704 mean train loss:  2.77567597e-05, mean val. loss:  1.88474095e+00\n",
      "Epoch: 5705 mean train loss:  2.82902911e-05, mean val. loss:  1.88507736e+00\n",
      "Epoch: 5706 mean train loss:  2.81168905e-05, mean val. loss:  1.88541126e+00\n",
      "Epoch: 5707 mean train loss:  2.85597343e-05, mean val. loss:  1.88572776e+00\n",
      "Epoch: 5708 mean train loss:  2.81136890e-05, mean val. loss:  1.88604116e+00\n",
      "Epoch: 5709 mean train loss:  2.84443377e-05, mean val. loss:  1.88634741e+00\n",
      "Epoch: 5710 mean train loss:  2.76929932e-05, mean val. loss:  1.88664603e+00\n",
      "Epoch: 5711 mean train loss:  2.72988400e-05, mean val. loss:  1.88693118e+00\n",
      "Epoch: 5712 mean train loss:  2.86537106e-05, mean val. loss:  1.88719630e+00\n",
      "Epoch: 5713 mean train loss:  2.79309752e-05, mean val. loss:  1.88744199e+00\n",
      "Epoch: 5714 mean train loss:  2.82111578e-05, mean val. loss:  1.88767803e+00\n",
      "Epoch: 5715 mean train loss:  2.75081838e-05, mean val. loss:  1.88789868e+00\n",
      "Epoch: 5716 mean train loss:  2.76601932e-05, mean val. loss:  1.88810217e+00\n",
      "Epoch: 5717 mean train loss:  2.77496001e-05, mean val. loss:  1.88829911e+00\n",
      "Epoch: 5718 mean train loss:  2.82916881e-05, mean val. loss:  1.88848603e+00\n",
      "Epoch: 5719 mean train loss:  2.78505613e-05, mean val. loss:  1.88867819e+00\n",
      "Epoch: 5720 mean train loss:  2.82061228e-05, mean val. loss:  1.88886404e+00\n",
      "Epoch: 5721 mean train loss:  2.78071093e-05, mean val. loss:  1.88906670e+00\n",
      "Epoch: 5722 mean train loss:  2.80560052e-05, mean val. loss:  1.88927305e+00\n",
      "Epoch: 5723 mean train loss:  2.74341437e-05, mean val. loss:  1.88948798e+00\n",
      "Epoch: 5724 mean train loss:  2.82306864e-05, mean val. loss:  1.88971245e+00\n",
      "Epoch: 5725 mean train loss:  2.82145047e-05, mean val. loss:  1.88993561e+00\n",
      "Epoch: 5726 mean train loss:  2.72809993e-05, mean val. loss:  1.89017439e+00\n",
      "Epoch: 5727 mean train loss:  2.79119995e-05, mean val. loss:  1.89041650e+00\n",
      "Epoch: 5728 mean train loss:  2.81318498e-05, mean val. loss:  1.89067400e+00\n",
      "Epoch: 5729 mean train loss:  2.85237620e-05, mean val. loss:  1.89093006e+00\n",
      "Epoch: 5730 mean train loss:  2.77029467e-05, mean val. loss:  1.89119363e+00\n",
      "Epoch: 5731 mean train loss:  2.78800144e-05, mean val. loss:  1.89149010e+00\n",
      "Epoch: 5732 mean train loss:  2.81817920e-05, mean val. loss:  1.89179969e+00\n",
      "Epoch: 5733 mean train loss:  2.81140965e-05, mean val. loss:  1.89211917e+00\n",
      "Epoch: 5734 mean train loss:  2.78463995e-05, mean val. loss:  1.89246082e+00\n",
      "Epoch: 5735 mean train loss:  2.81838584e-05, mean val. loss:  1.89281893e+00\n",
      "Epoch: 5736 mean train loss:  2.75483180e-05, mean val. loss:  1.89319813e+00\n",
      "Epoch: 5737 mean train loss:  2.79661326e-05, mean val. loss:  1.89358175e+00\n",
      "Epoch: 5738 mean train loss:  2.85266142e-05, mean val. loss:  1.89396822e+00\n",
      "Epoch: 5739 mean train loss:  2.75650236e-05, mean val. loss:  1.89435530e+00\n",
      "Epoch: 5740 mean train loss:  2.69809389e-05, mean val. loss:  1.89475298e+00\n",
      "Epoch: 5741 mean train loss:  2.79906380e-05, mean val. loss:  1.89515758e+00\n",
      "Epoch: 5742 mean train loss:  2.83389818e-05, mean val. loss:  1.89556122e+00\n",
      "Epoch: 5743 mean train loss:  2.84549897e-05, mean val. loss:  1.89595342e+00\n",
      "Epoch: 5744 mean train loss:  2.79161031e-05, mean val. loss:  1.89633954e+00\n",
      "Epoch: 5745 mean train loss:  2.73229962e-05, mean val. loss:  1.89672911e+00\n",
      "Epoch: 5746 mean train loss:  2.77997751e-05, mean val. loss:  1.89709830e+00\n",
      "Epoch: 5747 mean train loss:  2.77436047e-05, mean val. loss:  1.89745796e+00\n",
      "Epoch: 5748 mean train loss:  2.81180546e-05, mean val. loss:  1.89779902e+00\n",
      "Epoch: 5749 mean train loss:  2.77740473e-05, mean val. loss:  1.89812481e+00\n",
      "Epoch: 5750 mean train loss:  2.75710481e-05, mean val. loss:  1.89843214e+00\n",
      "Epoch: 5751 mean train loss:  2.78758525e-05, mean val. loss:  1.89873421e+00\n",
      "Epoch: 5752 mean train loss:  2.84934358e-05, mean val. loss:  1.89901125e+00\n",
      "Epoch: 5753 mean train loss:  2.81012617e-05, mean val. loss:  1.89927590e+00\n",
      "Epoch: 5754 mean train loss:  2.74549820e-05, mean val. loss:  1.89953089e+00\n",
      "Epoch: 5755 mean train loss:  2.75618222e-05, mean val. loss:  1.89977717e+00\n",
      "Epoch: 5756 mean train loss:  2.81198591e-05, mean val. loss:  1.90001714e+00\n",
      "Epoch: 5757 mean train loss:  2.77593208e-05, mean val. loss:  1.90026069e+00\n",
      "Epoch: 5758 mean train loss:  2.82818801e-05, mean val. loss:  1.90050554e+00\n",
      "Epoch: 5759 mean train loss:  2.77521904e-05, mean val. loss:  1.90073550e+00\n",
      "Epoch: 5760 mean train loss:  2.77522486e-05, mean val. loss:  1.90095234e+00\n",
      "Epoch: 5761 mean train loss:  2.74942431e-05, mean val. loss:  1.90117109e+00\n",
      "Epoch: 5762 mean train loss:  2.78117077e-05, mean val. loss:  1.90139151e+00\n",
      "Epoch: 5763 mean train loss:  2.74566410e-05, mean val. loss:  1.90160215e+00\n",
      "Epoch: 5764 mean train loss:  2.80953827e-05, mean val. loss:  1.90181136e+00\n",
      "Epoch: 5765 mean train loss:  2.82109831e-05, mean val. loss:  1.90202773e+00\n",
      "Epoch: 5766 mean train loss:  2.82631954e-05, mean val. loss:  1.90224993e+00\n",
      "Epoch: 5767 mean train loss:  2.82875262e-05, mean val. loss:  1.90247381e+00\n",
      "Epoch: 5768 mean train loss:  2.73947662e-05, mean val. loss:  1.90270996e+00\n",
      "Epoch: 5769 mean train loss:  2.76230276e-05, mean val. loss:  1.90294695e+00\n",
      "Epoch: 5770 mean train loss:  2.83043191e-05, mean val. loss:  1.90319371e+00\n",
      "Epoch: 5771 mean train loss:  2.83466943e-05, mean val. loss:  1.90344381e+00\n",
      "Epoch: 5772 mean train loss:  2.71984318e-05, mean val. loss:  1.90370309e+00\n",
      "Epoch: 5773 mean train loss:  2.79333617e-05, mean val. loss:  1.90396059e+00\n",
      "Epoch: 5774 mean train loss:  2.73134210e-05, mean val. loss:  1.90422654e+00\n",
      "Epoch: 5775 mean train loss:  2.82638066e-05, mean val. loss:  1.90448785e+00\n",
      "Epoch: 5776 mean train loss:  2.78910156e-05, mean val. loss:  1.90475953e+00\n",
      "Epoch: 5777 mean train loss:  2.73395854e-05, mean val. loss:  1.90504003e+00\n",
      "Epoch: 5778 mean train loss:  2.82092369e-05, mean val. loss:  1.90531969e+00\n",
      "Epoch: 5779 mean train loss:  2.81111861e-05, mean val. loss:  1.90560150e+00\n",
      "Epoch: 5780 mean train loss:  2.74971535e-05, mean val. loss:  1.90588570e+00\n",
      "Epoch: 5781 mean train loss:  2.70240416e-05, mean val. loss:  1.90617812e+00\n",
      "Epoch: 5782 mean train loss:  2.81575194e-05, mean val. loss:  1.90647888e+00\n",
      "Epoch: 5783 mean train loss:  2.82111578e-05, mean val. loss:  1.90676928e+00\n",
      "Epoch: 5784 mean train loss:  2.74921476e-05, mean val. loss:  1.90704513e+00\n",
      "Epoch: 5785 mean train loss:  2.76082719e-05, mean val. loss:  1.90732741e+00\n",
      "Epoch: 5786 mean train loss:  2.79916567e-05, mean val. loss:  1.90760553e+00\n",
      "Epoch: 5787 mean train loss:  2.76580977e-05, mean val. loss:  1.90788150e+00\n",
      "Epoch: 5788 mean train loss:  2.75296334e-05, mean val. loss:  1.90816998e+00\n",
      "Epoch: 5789 mean train loss:  2.79318192e-05, mean val. loss:  1.90845001e+00\n",
      "Epoch: 5790 mean train loss:  2.75651691e-05, mean val. loss:  1.90874410e+00\n",
      "Epoch: 5791 mean train loss:  2.81796383e-05, mean val. loss:  1.90903950e+00\n",
      "Epoch: 5792 mean train loss:  2.78449152e-05, mean val. loss:  1.90933383e+00\n",
      "Epoch: 5793 mean train loss:  2.74865888e-05, mean val. loss:  1.90962052e+00\n",
      "Epoch: 5794 mean train loss:  2.84003618e-05, mean val. loss:  1.90991199e+00\n",
      "Epoch: 5795 mean train loss:  2.83231202e-05, mean val. loss:  1.91018772e+00\n",
      "Epoch: 5796 mean train loss:  2.76813225e-05, mean val. loss:  1.91046131e+00\n",
      "Epoch: 5797 mean train loss:  2.67888245e-05, mean val. loss:  1.91073048e+00\n",
      "Epoch: 5798 mean train loss:  2.77756189e-05, mean val. loss:  1.91098011e+00\n",
      "Epoch: 5799 mean train loss:  2.79391534e-05, mean val. loss:  1.91120911e+00\n",
      "Epoch: 5800 mean train loss:  2.76318169e-05, mean val. loss:  1.91144264e+00\n",
      "Epoch: 5801 mean train loss:  2.78022198e-05, mean val. loss:  1.91166985e+00\n",
      "Epoch: 5802 mean train loss:  2.78390362e-05, mean val. loss:  1.91189361e+00\n",
      "Epoch: 5803 mean train loss:  2.75056518e-05, mean val. loss:  1.91210175e+00\n",
      "Epoch: 5804 mean train loss:  2.69378361e-05, mean val. loss:  1.91231084e+00\n",
      "Epoch: 5805 mean train loss:  2.78563530e-05, mean val. loss:  1.91252303e+00\n",
      "Epoch: 5806 mean train loss:  2.75955244e-05, mean val. loss:  1.91273665e+00\n",
      "Epoch: 5807 mean train loss:  2.74224731e-05, mean val. loss:  1.91296077e+00\n",
      "Epoch: 5808 mean train loss:  2.75888306e-05, mean val. loss:  1.91319215e+00\n",
      "Epoch: 5809 mean train loss:  2.78501830e-05, mean val. loss:  1.91343403e+00\n",
      "Epoch: 5810 mean train loss:  2.74433696e-05, mean val. loss:  1.91369700e+00\n",
      "Epoch: 5811 mean train loss:  2.77624931e-05, mean val. loss:  1.91396058e+00\n",
      "Epoch: 5812 mean train loss:  2.73258192e-05, mean val. loss:  1.91424453e+00\n",
      "Epoch: 5813 mean train loss:  2.78051884e-05, mean val. loss:  1.91453755e+00\n",
      "Epoch: 5814 mean train loss:  2.76661885e-05, mean val. loss:  1.91484153e+00\n",
      "Epoch: 5815 mean train loss:  2.81255634e-05, mean val. loss:  1.91514802e+00\n",
      "Epoch: 5816 mean train loss:  2.74896447e-05, mean val. loss:  1.91547358e+00\n",
      "Epoch: 5817 mean train loss:  2.74658669e-05, mean val. loss:  1.91580415e+00\n",
      "Epoch: 5818 mean train loss:  2.75060884e-05, mean val. loss:  1.91613925e+00\n",
      "Epoch: 5819 mean train loss:  2.75713974e-05, mean val. loss:  1.91647160e+00\n",
      "Epoch: 5820 mean train loss:  2.76160426e-05, mean val. loss:  1.91681743e+00\n",
      "Epoch: 5821 mean train loss:  2.76732026e-05, mean val. loss:  1.91715169e+00\n",
      "Epoch: 5822 mean train loss:  2.81554239e-05, mean val. loss:  1.91747570e+00\n",
      "Epoch: 5823 mean train loss:  2.73709884e-05, mean val. loss:  1.91779256e+00\n",
      "Epoch: 5824 mean train loss:  2.76305072e-05, mean val. loss:  1.91808867e+00\n",
      "Epoch: 5825 mean train loss:  2.77892977e-05, mean val. loss:  1.91837394e+00\n",
      "Epoch: 5826 mean train loss:  2.77647923e-05, mean val. loss:  1.91864681e+00\n",
      "Epoch: 5827 mean train loss:  2.76588253e-05, mean val. loss:  1.91891861e+00\n",
      "Epoch: 5828 mean train loss:  2.71390018e-05, mean val. loss:  1.91917515e+00\n",
      "Epoch: 5829 mean train loss:  2.73652840e-05, mean val. loss:  1.91944170e+00\n",
      "Epoch: 5830 mean train loss:  2.77286454e-05, mean val. loss:  1.91969228e+00\n",
      "Epoch: 5831 mean train loss:  2.76818464e-05, mean val. loss:  1.91995752e+00\n",
      "Epoch: 5832 mean train loss:  2.76051578e-05, mean val. loss:  1.92022872e+00\n",
      "Epoch: 5833 mean train loss:  2.72297766e-05, mean val. loss:  1.92051029e+00\n",
      "Epoch: 5834 mean train loss:  2.70428136e-05, mean val. loss:  1.92079532e+00\n",
      "Epoch: 5835 mean train loss:  2.82125839e-05, mean val. loss:  1.92107737e+00\n",
      "Epoch: 5836 mean train loss:  2.79274827e-05, mean val. loss:  1.92136872e+00\n",
      "Epoch: 5837 mean train loss:  2.74504710e-05, mean val. loss:  1.92166042e+00\n",
      "Epoch: 5838 mean train loss:  2.76231440e-05, mean val. loss:  1.92197371e+00\n",
      "Epoch: 5839 mean train loss:  2.76257633e-05, mean val. loss:  1.92229283e+00\n",
      "Epoch: 5840 mean train loss:  2.81550456e-05, mean val. loss:  1.92261934e+00\n",
      "Epoch: 5841 mean train loss:  2.76891806e-05, mean val. loss:  1.92294264e+00\n",
      "Epoch: 5842 mean train loss:  2.76936335e-05, mean val. loss:  1.92326808e+00\n",
      "Epoch: 5843 mean train loss:  2.71398749e-05, mean val. loss:  1.92358398e+00\n",
      "Epoch: 5844 mean train loss:  2.73745391e-05, mean val. loss:  1.92389572e+00\n",
      "Epoch: 5845 mean train loss:  2.73486366e-05, mean val. loss:  1.92420709e+00\n",
      "Epoch: 5846 mean train loss:  2.75252969e-05, mean val. loss:  1.92451060e+00\n",
      "Epoch: 5847 mean train loss:  2.73879268e-05, mean val. loss:  1.92480826e+00\n",
      "Epoch: 5848 mean train loss:  2.72686011e-05, mean val. loss:  1.92508960e+00\n",
      "Epoch: 5849 mean train loss:  2.73656333e-05, mean val. loss:  1.92536867e+00\n",
      "Epoch: 5850 mean train loss:  2.71101308e-05, mean val. loss:  1.92563844e+00\n",
      "Epoch: 5851 mean train loss:  2.77225045e-05, mean val. loss:  1.92589962e+00\n",
      "Epoch: 5852 mean train loss:  2.75971543e-05, mean val. loss:  1.92615771e+00\n",
      "Epoch: 5853 mean train loss:  2.70161545e-05, mean val. loss:  1.92641461e+00\n",
      "Epoch: 5854 mean train loss:  2.70640594e-05, mean val. loss:  1.92666280e+00\n",
      "Epoch: 5855 mean train loss:  2.73477344e-05, mean val. loss:  1.92691481e+00\n",
      "Epoch: 5856 mean train loss:  2.75681377e-05, mean val. loss:  1.92716563e+00\n",
      "Epoch: 5857 mean train loss:  2.76961073e-05, mean val. loss:  1.92740607e+00\n",
      "Epoch: 5858 mean train loss:  2.76235805e-05, mean val. loss:  1.92763686e+00\n",
      "Epoch: 5859 mean train loss:  2.76122591e-05, mean val. loss:  1.92786527e+00\n",
      "Epoch: 5860 mean train loss:  2.78939260e-05, mean val. loss:  1.92809081e+00\n",
      "Epoch: 5861 mean train loss:  2.76877545e-05, mean val. loss:  1.92830217e+00\n",
      "Epoch: 5862 mean train loss:  2.76563806e-05, mean val. loss:  1.92850173e+00\n",
      "Epoch: 5863 mean train loss:  2.74046906e-05, mean val. loss:  1.92869329e+00\n",
      "Epoch: 5864 mean train loss:  2.71398749e-05, mean val. loss:  1.92888939e+00\n",
      "Epoch: 5865 mean train loss:  2.69522716e-05, mean val. loss:  1.92909038e+00\n",
      "Epoch: 5866 mean train loss:  2.75236089e-05, mean val. loss:  1.92928481e+00\n",
      "Epoch: 5867 mean train loss:  2.71668832e-05, mean val. loss:  1.92948091e+00\n",
      "Epoch: 5868 mean train loss:  2.80011445e-05, mean val. loss:  1.92965877e+00\n",
      "Epoch: 5869 mean train loss:  2.77487852e-05, mean val. loss:  1.92982709e+00\n",
      "Epoch: 5870 mean train loss:  2.85029819e-05, mean val. loss:  1.92998302e+00\n",
      "Epoch: 5871 mean train loss:  2.76612991e-05, mean val. loss:  1.93013513e+00\n",
      "Epoch: 5872 mean train loss:  2.76869978e-05, mean val. loss:  1.93029320e+00\n",
      "Epoch: 5873 mean train loss:  2.72770121e-05, mean val. loss:  1.93043900e+00\n",
      "Epoch: 5874 mean train loss:  2.71704339e-05, mean val. loss:  1.93058407e+00\n",
      "Epoch: 5875 mean train loss:  2.73523619e-05, mean val. loss:  1.93071938e+00\n",
      "Epoch: 5876 mean train loss:  2.76313804e-05, mean val. loss:  1.93085217e+00\n",
      "Epoch: 5877 mean train loss:  2.76911305e-05, mean val. loss:  1.93098176e+00\n",
      "Epoch: 5878 mean train loss:  2.75624334e-05, mean val. loss:  1.93111622e+00\n",
      "Epoch: 5879 mean train loss:  2.73697078e-05, mean val. loss:  1.93124962e+00\n",
      "Epoch: 5880 mean train loss:  2.76649953e-05, mean val. loss:  1.93138921e+00\n",
      "Epoch: 5881 mean train loss:  2.73750338e-05, mean val. loss:  1.93153691e+00\n",
      "Epoch: 5882 mean train loss:  2.73249752e-05, mean val. loss:  1.93169868e+00\n",
      "Epoch: 5883 mean train loss:  2.76658393e-05, mean val. loss:  1.93186951e+00\n",
      "Epoch: 5884 mean train loss:  2.77895306e-05, mean val. loss:  1.93204117e+00\n",
      "Epoch: 5885 mean train loss:  2.75128114e-05, mean val. loss:  1.93222487e+00\n",
      "Epoch: 5886 mean train loss:  2.75901693e-05, mean val. loss:  1.93241322e+00\n",
      "Epoch: 5887 mean train loss:  2.75527127e-05, mean val. loss:  1.93261802e+00\n",
      "Epoch: 5888 mean train loss:  2.72465695e-05, mean val. loss:  1.93283129e+00\n",
      "Epoch: 5889 mean train loss:  2.76911305e-05, mean val. loss:  1.93305063e+00\n",
      "Epoch: 5890 mean train loss:  2.70720338e-05, mean val. loss:  1.93326652e+00\n",
      "Epoch: 5891 mean train loss:  2.67872238e-05, mean val. loss:  1.93348742e+00\n",
      "Epoch: 5892 mean train loss:  2.77187792e-05, mean val. loss:  1.93371415e+00\n",
      "Epoch: 5893 mean train loss:  2.81762914e-05, mean val. loss:  1.93394852e+00\n",
      "Epoch: 5894 mean train loss:  2.70656019e-05, mean val. loss:  1.93419898e+00\n",
      "Epoch: 5895 mean train loss:  2.75639468e-05, mean val. loss:  1.93445849e+00\n",
      "Epoch: 5896 mean train loss:  2.76014907e-05, mean val. loss:  1.93471658e+00\n",
      "Epoch: 5897 mean train loss:  2.74086779e-05, mean val. loss:  1.93496978e+00\n",
      "Epoch: 5898 mean train loss:  2.76484934e-05, mean val. loss:  1.93521988e+00\n",
      "Epoch: 5899 mean train loss:  2.73022160e-05, mean val. loss:  1.93547487e+00\n",
      "Epoch: 5900 mean train loss:  2.71284080e-05, mean val. loss:  1.93573236e+00\n",
      "Epoch: 5901 mean train loss:  2.74245976e-05, mean val. loss:  1.93598866e+00\n",
      "Epoch: 5902 mean train loss:  2.74041668e-05, mean val. loss:  1.93624592e+00\n",
      "Epoch: 5903 mean train loss:  2.72994803e-05, mean val. loss:  1.93651330e+00\n",
      "Epoch: 5904 mean train loss:  2.75814091e-05, mean val. loss:  1.93678355e+00\n",
      "Epoch: 5905 mean train loss:  2.75403727e-05, mean val. loss:  1.93706536e+00\n",
      "Epoch: 5906 mean train loss:  2.71025929e-05, mean val. loss:  1.93736827e+00\n",
      "Epoch: 5907 mean train loss:  2.75471248e-05, mean val. loss:  1.93765485e+00\n",
      "Epoch: 5908 mean train loss:  2.77233776e-05, mean val. loss:  1.93793750e+00\n",
      "Epoch: 5909 mean train loss:  2.71697063e-05, mean val. loss:  1.93823409e+00\n",
      "Epoch: 5910 mean train loss:  2.73936603e-05, mean val. loss:  1.93854761e+00\n",
      "Epoch: 5911 mean train loss:  2.73874903e-05, mean val. loss:  1.93885696e+00\n",
      "Epoch: 5912 mean train loss:  2.77367944e-05, mean val. loss:  1.93915701e+00\n",
      "Epoch: 5913 mean train loss:  2.71983736e-05, mean val. loss:  1.93944967e+00\n",
      "Epoch: 5914 mean train loss:  2.74471822e-05, mean val. loss:  1.93972719e+00\n",
      "Epoch: 5915 mean train loss:  2.73048936e-05, mean val. loss:  1.93999243e+00\n",
      "Epoch: 5916 mean train loss:  2.77592917e-05, mean val. loss:  1.94025779e+00\n",
      "Epoch: 5917 mean train loss:  2.68873991e-05, mean val. loss:  1.94052315e+00\n",
      "Epoch: 5918 mean train loss:  2.71866738e-05, mean val. loss:  1.94078410e+00\n",
      "Epoch: 5919 mean train loss:  2.74883350e-05, mean val. loss:  1.94103551e+00\n",
      "Epoch: 5920 mean train loss:  2.72154866e-05, mean val. loss:  1.94128478e+00\n",
      "Epoch: 5921 mean train loss:  2.75270431e-05, mean val. loss:  1.94151366e+00\n",
      "Epoch: 5922 mean train loss:  2.70145829e-05, mean val. loss:  1.94174707e+00\n",
      "Epoch: 5923 mean train loss:  2.73350452e-05, mean val. loss:  1.94196248e+00\n",
      "Epoch: 5924 mean train loss:  2.69854499e-05, mean val. loss:  1.94219279e+00\n",
      "Epoch: 5925 mean train loss:  2.70218006e-05, mean val. loss:  1.94242072e+00\n",
      "Epoch: 5926 mean train loss:  2.69424636e-05, mean val. loss:  1.94265771e+00\n",
      "Epoch: 5927 mean train loss:  2.75922357e-05, mean val. loss:  1.94288921e+00\n",
      "Epoch: 5928 mean train loss:  2.71136523e-05, mean val. loss:  1.94312799e+00\n",
      "Epoch: 5929 mean train loss:  2.75179045e-05, mean val. loss:  1.94337225e+00\n",
      "Epoch: 5930 mean train loss:  2.71011668e-05, mean val. loss:  1.94363213e+00\n",
      "Epoch: 5931 mean train loss:  2.70101009e-05, mean val. loss:  1.94391656e+00\n",
      "Epoch: 5932 mean train loss:  2.73430778e-05, mean val. loss:  1.94421244e+00\n",
      "Epoch: 5933 mean train loss:  2.74144113e-05, mean val. loss:  1.94449997e+00\n",
      "Epoch: 5934 mean train loss:  2.75308266e-05, mean val. loss:  1.94478238e+00\n",
      "Epoch: 5935 mean train loss:  2.72243633e-05, mean val. loss:  1.94508028e+00\n",
      "Epoch: 5936 mean train loss:  2.79358646e-05, mean val. loss:  1.94536996e+00\n",
      "Epoch: 5937 mean train loss:  2.71358294e-05, mean val. loss:  1.94566107e+00\n",
      "Epoch: 5938 mean train loss:  2.73381011e-05, mean val. loss:  1.94595838e+00\n",
      "Epoch: 5939 mean train loss:  2.80326931e-05, mean val. loss:  1.94625497e+00\n",
      "Epoch: 5940 mean train loss:  2.72068137e-05, mean val. loss:  1.94655263e+00\n",
      "Epoch: 5941 mean train loss:  2.77327490e-05, mean val. loss:  1.94683528e+00\n",
      "Epoch: 5942 mean train loss:  2.72042817e-05, mean val. loss:  1.94710243e+00\n",
      "Epoch: 5943 mean train loss:  2.71839381e-05, mean val. loss:  1.94735730e+00\n",
      "Epoch: 5944 mean train loss:  2.70293094e-05, mean val. loss:  1.94761193e+00\n",
      "Epoch: 5945 mean train loss:  2.75030325e-05, mean val. loss:  1.94786072e+00\n",
      "Epoch: 5946 mean train loss:  2.75259372e-05, mean val. loss:  1.94808340e+00\n",
      "Epoch: 5947 mean train loss:  2.72968027e-05, mean val. loss:  1.94830751e+00\n",
      "Epoch: 5948 mean train loss:  2.73494516e-05, mean val. loss:  1.94852054e+00\n",
      "Epoch: 5949 mean train loss:  2.70026503e-05, mean val. loss:  1.94871533e+00\n",
      "Epoch: 5950 mean train loss:  2.72415346e-05, mean val. loss:  1.94890845e+00\n",
      "Epoch: 5951 mean train loss:  2.76587380e-05, mean val. loss:  1.94908488e+00\n",
      "Epoch: 5952 mean train loss:  2.74390623e-05, mean val. loss:  1.94925141e+00\n",
      "Epoch: 5953 mean train loss:  2.71880999e-05, mean val. loss:  1.94940209e+00\n",
      "Epoch: 5954 mean train loss:  2.72525649e-05, mean val. loss:  1.94954062e+00\n",
      "Epoch: 5955 mean train loss:  2.73241021e-05, mean val. loss:  1.94967258e+00\n",
      "Epoch: 5956 mean train loss:  2.72700563e-05, mean val. loss:  1.94979894e+00\n",
      "Epoch: 5957 mean train loss:  2.69152806e-05, mean val. loss:  1.94992042e+00\n",
      "Epoch: 5958 mean train loss:  2.70011660e-05, mean val. loss:  1.95002937e+00\n",
      "Epoch: 5959 mean train loss:  2.70178134e-05, mean val. loss:  1.95014560e+00\n",
      "Epoch: 5960 mean train loss:  2.75167404e-05, mean val. loss:  1.95027506e+00\n",
      "Epoch: 5961 mean train loss:  2.78906373e-05, mean val. loss:  1.95039618e+00\n",
      "Epoch: 5962 mean train loss:  2.75256461e-05, mean val. loss:  1.95052516e+00\n",
      "Epoch: 5963 mean train loss:  2.65122799e-05, mean val. loss:  1.95066881e+00\n",
      "Epoch: 5964 mean train loss:  2.70729652e-05, mean val. loss:  1.95081890e+00\n",
      "Epoch: 5965 mean train loss:  2.72252364e-05, mean val. loss:  1.95097375e+00\n",
      "Epoch: 5966 mean train loss:  2.75678467e-05, mean val. loss:  1.95114684e+00\n",
      "Epoch: 5967 mean train loss:  2.70564633e-05, mean val. loss:  1.95133603e+00\n",
      "Epoch: 5968 mean train loss:  2.71229947e-05, mean val. loss:  1.95152307e+00\n",
      "Epoch: 5969 mean train loss:  2.73257901e-05, mean val. loss:  1.95172775e+00\n",
      "Epoch: 5970 mean train loss:  2.76177016e-05, mean val. loss:  1.95194972e+00\n",
      "Epoch: 5971 mean train loss:  2.72957550e-05, mean val. loss:  1.95217586e+00\n",
      "Epoch: 5972 mean train loss:  2.73563201e-05, mean val. loss:  1.95241606e+00\n",
      "Epoch: 5973 mean train loss:  2.77124345e-05, mean val. loss:  1.95266795e+00\n",
      "Epoch: 5974 mean train loss:  2.73330079e-05, mean val. loss:  1.95293713e+00\n",
      "Epoch: 5975 mean train loss:  2.71677563e-05, mean val. loss:  1.95320392e+00\n",
      "Epoch: 5976 mean train loss:  2.73486949e-05, mean val. loss:  1.95347452e+00\n",
      "Epoch: 5977 mean train loss:  2.71198223e-05, mean val. loss:  1.95375335e+00\n",
      "Epoch: 5978 mean train loss:  2.75707571e-05, mean val. loss:  1.95402384e+00\n",
      "Epoch: 5979 mean train loss:  2.68532021e-05, mean val. loss:  1.95430052e+00\n",
      "Epoch: 5980 mean train loss:  2.70935416e-05, mean val. loss:  1.95458400e+00\n",
      "Epoch: 5981 mean train loss:  2.72146717e-05, mean val. loss:  1.95487106e+00\n",
      "Epoch: 5982 mean train loss:  2.75841448e-05, mean val. loss:  1.95514619e+00\n",
      "Epoch: 5983 mean train loss:  2.71921162e-05, mean val. loss:  1.95542181e+00\n",
      "Epoch: 5984 mean train loss:  2.72664474e-05, mean val. loss:  1.95568311e+00\n",
      "Epoch: 5985 mean train loss:  2.72221514e-05, mean val. loss:  1.95595717e+00\n",
      "Epoch: 5986 mean train loss:  2.67847208e-05, mean val. loss:  1.95620823e+00\n",
      "Epoch: 5987 mean train loss:  2.74424383e-05, mean val. loss:  1.95645416e+00\n",
      "Epoch: 5988 mean train loss:  2.70971213e-05, mean val. loss:  1.95670319e+00\n",
      "Epoch: 5989 mean train loss:  2.74895865e-05, mean val. loss:  1.95693457e+00\n",
      "Epoch: 5990 mean train loss:  2.72181351e-05, mean val. loss:  1.95714879e+00\n",
      "Epoch: 5991 mean train loss:  2.74950871e-05, mean val. loss:  1.95734978e+00\n",
      "Epoch: 5992 mean train loss:  2.73593178e-05, mean val. loss:  1.95753324e+00\n",
      "Epoch: 5993 mean train loss:  2.76536739e-05, mean val. loss:  1.95768583e+00\n",
      "Epoch: 5994 mean train loss:  2.71953759e-05, mean val. loss:  1.95783508e+00\n",
      "Epoch: 5995 mean train loss:  2.74762569e-05, mean val. loss:  1.95796752e+00\n",
      "Epoch: 5996 mean train loss:  2.71216850e-05, mean val. loss:  1.95808315e+00\n",
      "Epoch: 5997 mean train loss:  2.73626647e-05, mean val. loss:  1.95819378e+00\n",
      "Epoch: 5998 mean train loss:  2.70319579e-05, mean val. loss:  1.95828879e+00\n",
      "Epoch: 5999 mean train loss:  2.73251499e-05, mean val. loss:  1.95837474e+00\n",
      "Epoch: 6000 mean train loss:  2.66613788e-05, mean val. loss:  1.95845771e+00\n",
      "Epoch: 6001 mean train loss:  2.71764584e-05, mean val. loss:  1.95853353e+00\n",
      "Epoch: 6002 mean train loss:  2.70336459e-05, mean val. loss:  1.95862186e+00\n",
      "Epoch: 6003 mean train loss:  2.73213373e-05, mean val. loss:  1.95871127e+00\n",
      "Epoch: 6004 mean train loss:  2.67254363e-05, mean val. loss:  1.95881391e+00\n",
      "Epoch: 6005 mean train loss:  2.77945655e-05, mean val. loss:  1.95891702e+00\n",
      "Epoch: 6006 mean train loss:  2.71649915e-05, mean val. loss:  1.95903134e+00\n",
      "Epoch: 6007 mean train loss:  2.69047159e-05, mean val. loss:  1.95916259e+00\n",
      "Epoch: 6008 mean train loss:  2.69787852e-05, mean val. loss:  1.95931458e+00\n",
      "Epoch: 6009 mean train loss:  2.70113524e-05, mean val. loss:  1.95949495e+00\n",
      "Epoch: 6010 mean train loss:  2.67761643e-05, mean val. loss:  1.95969880e+00\n",
      "Epoch: 6011 mean train loss:  2.73050973e-05, mean val. loss:  1.95991778e+00\n",
      "Epoch: 6012 mean train loss:  2.67230498e-05, mean val. loss:  1.96017838e+00\n",
      "Epoch: 6013 mean train loss:  2.71862082e-05, mean val. loss:  1.96045160e+00\n",
      "Epoch: 6014 mean train loss:  2.68723816e-05, mean val. loss:  1.96076643e+00\n",
      "Epoch: 6015 mean train loss:  2.68700824e-05, mean val. loss:  1.96110821e+00\n",
      "Epoch: 6016 mean train loss:  2.68583826e-05, mean val. loss:  1.96147859e+00\n",
      "Epoch: 6017 mean train loss:  2.67496798e-05, mean val. loss:  1.96186101e+00\n",
      "Epoch: 6018 mean train loss:  2.68894364e-05, mean val. loss:  1.96225822e+00\n",
      "Epoch: 6019 mean train loss:  2.72826292e-05, mean val. loss:  1.96265459e+00\n",
      "Epoch: 6020 mean train loss:  2.66948191e-05, mean val. loss:  1.96304357e+00\n",
      "Epoch: 6021 mean train loss:  2.69081793e-05, mean val. loss:  1.96343398e+00\n",
      "Epoch: 6022 mean train loss:  2.69106240e-05, mean val. loss:  1.96381199e+00\n",
      "Epoch: 6023 mean train loss:  2.68683652e-05, mean val. loss:  1.96417594e+00\n",
      "Epoch: 6024 mean train loss:  2.76308856e-05, mean val. loss:  1.96452379e+00\n",
      "Epoch: 6025 mean train loss:  2.68442964e-05, mean val. loss:  1.96484256e+00\n",
      "Epoch: 6026 mean train loss:  2.72708712e-05, mean val. loss:  1.96515429e+00\n",
      "Epoch: 6027 mean train loss:  2.69968878e-05, mean val. loss:  1.96544838e+00\n",
      "Epoch: 6028 mean train loss:  2.66772404e-05, mean val. loss:  1.96572280e+00\n",
      "Epoch: 6029 mean train loss:  2.74218037e-05, mean val. loss:  1.96597648e+00\n",
      "Epoch: 6030 mean train loss:  2.72736070e-05, mean val. loss:  1.96620357e+00\n",
      "Epoch: 6031 mean train loss:  2.65878625e-05, mean val. loss:  1.96642303e+00\n",
      "Epoch: 6032 mean train loss:  2.64114933e-05, mean val. loss:  1.96662545e+00\n",
      "Epoch: 6033 mean train loss:  2.71969184e-05, mean val. loss:  1.96681750e+00\n",
      "Epoch: 6034 mean train loss:  2.69034936e-05, mean val. loss:  1.96699488e+00\n",
      "Epoch: 6035 mean train loss:  2.71560857e-05, mean val. loss:  1.96717739e+00\n",
      "Epoch: 6036 mean train loss:  2.75062630e-05, mean val. loss:  1.96735084e+00\n",
      "Epoch: 6037 mean train loss:  2.66564602e-05, mean val. loss:  1.96752810e+00\n",
      "Epoch: 6038 mean train loss:  2.66522693e-05, mean val. loss:  1.96770704e+00\n",
      "Epoch: 6039 mean train loss:  2.69019220e-05, mean val. loss:  1.96787822e+00\n",
      "Epoch: 6040 mean train loss:  2.74976192e-05, mean val. loss:  1.96803820e+00\n",
      "Epoch: 6041 mean train loss:  2.69237789e-05, mean val. loss:  1.96819913e+00\n",
      "Epoch: 6042 mean train loss:  2.68399017e-05, mean val. loss:  1.96835065e+00\n",
      "Epoch: 6043 mean train loss:  2.66813731e-05, mean val. loss:  1.96851850e+00\n",
      "Epoch: 6044 mean train loss:  2.71630706e-05, mean val. loss:  1.96867561e+00\n",
      "Epoch: 6045 mean train loss:  2.73039041e-05, mean val. loss:  1.96881938e+00\n",
      "Epoch: 6046 mean train loss:  2.67977011e-05, mean val. loss:  1.96897566e+00\n",
      "Epoch: 6047 mean train loss:  2.72097532e-05, mean val. loss:  1.96913958e+00\n",
      "Epoch: 6048 mean train loss:  2.72995676e-05, mean val. loss:  1.96930265e+00\n",
      "Epoch: 6049 mean train loss:  2.68985459e-05, mean val. loss:  1.96946931e+00\n",
      "Epoch: 6050 mean train loss:  2.66328570e-05, mean val. loss:  1.96963525e+00\n",
      "Epoch: 6051 mean train loss:  2.68254080e-05, mean val. loss:  1.96980762e+00\n",
      "Epoch: 6052 mean train loss:  2.66665011e-05, mean val. loss:  1.96998405e+00\n",
      "Epoch: 6053 mean train loss:  2.68608273e-05, mean val. loss:  1.97015965e+00\n",
      "Epoch: 6054 mean train loss:  2.70332093e-05, mean val. loss:  1.97033179e+00\n",
      "Epoch: 6055 mean train loss:  2.70890596e-05, mean val. loss:  1.97049987e+00\n",
      "Epoch: 6056 mean train loss:  2.65697017e-05, mean val. loss:  1.97068012e+00\n",
      "Epoch: 6057 mean train loss:  2.68116419e-05, mean val. loss:  1.97085524e+00\n",
      "Epoch: 6058 mean train loss:  2.71946192e-05, mean val. loss:  1.97102129e+00\n",
      "Epoch: 6059 mean train loss:  2.67004070e-05, mean val. loss:  1.97117817e+00\n",
      "Epoch: 6060 mean train loss:  2.70186865e-05, mean val. loss:  1.97133696e+00\n",
      "Epoch: 6061 mean train loss:  2.70882156e-05, mean val. loss:  1.97149181e+00\n",
      "Epoch: 6062 mean train loss:  2.67034338e-05, mean val. loss:  1.97164690e+00\n",
      "Epoch: 6063 mean train loss:  2.65924609e-05, mean val. loss:  1.97180474e+00\n",
      "Epoch: 6064 mean train loss:  2.72962207e-05, mean val. loss:  1.97196376e+00\n",
      "Epoch: 6065 mean train loss:  2.70998862e-05, mean val. loss:  1.97213423e+00\n",
      "Epoch: 6066 mean train loss:  2.72169418e-05, mean val. loss:  1.97230864e+00\n",
      "Epoch: 6067 mean train loss:  2.64357659e-05, mean val. loss:  1.97249043e+00\n",
      "Epoch: 6068 mean train loss:  2.70255841e-05, mean val. loss:  1.97267568e+00\n",
      "Epoch: 6069 mean train loss:  2.66540737e-05, mean val. loss:  1.97287476e+00\n",
      "Epoch: 6070 mean train loss:  2.74356862e-05, mean val. loss:  1.97307944e+00\n",
      "Epoch: 6071 mean train loss:  2.68686854e-05, mean val. loss:  1.97328877e+00\n",
      "Epoch: 6072 mean train loss:  2.67655996e-05, mean val. loss:  1.97349024e+00\n",
      "Epoch: 6073 mean train loss:  2.66369316e-05, mean val. loss:  1.97369170e+00\n",
      "Epoch: 6074 mean train loss:  2.72445614e-05, mean val. loss:  1.97388387e+00\n",
      "Epoch: 6075 mean train loss:  2.73773039e-05, mean val. loss:  1.97408390e+00\n",
      "Epoch: 6076 mean train loss:  2.62764515e-05, mean val. loss:  1.97428536e+00\n",
      "Epoch: 6077 mean train loss:  2.66470015e-05, mean val. loss:  1.97448802e+00\n",
      "Epoch: 6078 mean train loss:  2.72215402e-05, mean val. loss:  1.97467816e+00\n",
      "Epoch: 6079 mean train loss:  2.64173141e-05, mean val. loss:  1.97487414e+00\n",
      "Epoch: 6080 mean train loss:  2.71938043e-05, mean val. loss:  1.97505379e+00\n",
      "Epoch: 6081 mean train loss:  2.68167641e-05, mean val. loss:  1.97522068e+00\n",
      "Epoch: 6082 mean train loss:  2.66304123e-05, mean val. loss:  1.97537720e+00\n",
      "Epoch: 6083 mean train loss:  2.65987765e-05, mean val. loss:  1.97553670e+00\n",
      "Epoch: 6084 mean train loss:  2.77785002e-05, mean val. loss:  1.97568369e+00\n",
      "Epoch: 6085 mean train loss:  2.68633594e-05, mean val. loss:  1.97583091e+00\n",
      "Epoch: 6086 mean train loss:  2.72542820e-05, mean val. loss:  1.97598624e+00\n",
      "Epoch: 6087 mean train loss:  2.73092301e-05, mean val. loss:  1.97613859e+00\n",
      "Epoch: 6088 mean train loss:  2.65372801e-05, mean val. loss:  1.97627938e+00\n",
      "Epoch: 6089 mean train loss:  2.62475514e-05, mean val. loss:  1.97642446e+00\n",
      "Epoch: 6090 mean train loss:  2.73237238e-05, mean val. loss:  1.97656333e+00\n",
      "Epoch: 6091 mean train loss:  2.66461575e-05, mean val. loss:  1.97671878e+00\n",
      "Epoch: 6092 mean train loss:  2.70582677e-05, mean val. loss:  1.97687542e+00\n",
      "Epoch: 6093 mean train loss:  2.70905148e-05, mean val. loss:  1.97703338e+00\n",
      "Epoch: 6094 mean train loss:  2.72020116e-05, mean val. loss:  1.97718835e+00\n",
      "Epoch: 6095 mean train loss:  2.70146993e-05, mean val. loss:  1.97732639e+00\n",
      "Epoch: 6096 mean train loss:  2.71678146e-05, mean val. loss:  1.97746181e+00\n",
      "Epoch: 6097 mean train loss:  2.66835559e-05, mean val. loss:  1.97760260e+00\n",
      "Epoch: 6098 mean train loss:  2.74380436e-05, mean val. loss:  1.97773314e+00\n",
      "Epoch: 6099 mean train loss:  2.65807030e-05, mean val. loss:  1.97786546e+00\n",
      "Epoch: 6100 mean train loss:  2.72754696e-05, mean val. loss:  1.97801018e+00\n",
      "Epoch: 6101 mean train loss:  2.69956654e-05, mean val. loss:  1.97815156e+00\n",
      "Epoch: 6102 mean train loss:  2.66061106e-05, mean val. loss:  1.97829449e+00\n",
      "Epoch: 6103 mean train loss:  2.76944775e-05, mean val. loss:  1.97843254e+00\n",
      "Epoch: 6104 mean train loss:  2.60486850e-05, mean val. loss:  1.97856092e+00\n",
      "Epoch: 6105 mean train loss:  2.65241833e-05, mean val. loss:  1.97868800e+00\n",
      "Epoch: 6106 mean train loss:  2.68939475e-05, mean val. loss:  1.97880614e+00\n",
      "Epoch: 6107 mean train loss:  2.70430173e-05, mean val. loss:  1.97892487e+00\n",
      "Epoch: 6108 mean train loss:  2.66830903e-05, mean val. loss:  1.97904432e+00\n",
      "Epoch: 6109 mean train loss:  2.71056779e-05, mean val. loss:  1.97917140e+00\n",
      "Epoch: 6110 mean train loss:  2.64421978e-05, mean val. loss:  1.97930598e+00\n",
      "Epoch: 6111 mean train loss:  2.65858835e-05, mean val. loss:  1.97944367e+00\n",
      "Epoch: 6112 mean train loss:  2.72846082e-05, mean val. loss:  1.97957695e+00\n",
      "Epoch: 6113 mean train loss:  2.67434225e-05, mean val. loss:  1.97971904e+00\n",
      "Epoch: 6114 mean train loss:  2.66080897e-05, mean val. loss:  1.97987187e+00\n",
      "Epoch: 6115 mean train loss:  2.72575417e-05, mean val. loss:  1.98002887e+00\n",
      "Epoch: 6116 mean train loss:  2.69636803e-05, mean val. loss:  1.98019123e+00\n",
      "Epoch: 6117 mean train loss:  2.69511947e-05, mean val. loss:  1.98036909e+00\n",
      "Epoch: 6118 mean train loss:  2.72661564e-05, mean val. loss:  1.98054111e+00\n",
      "Epoch: 6119 mean train loss:  2.72946490e-05, mean val. loss:  1.98069692e+00\n",
      "Epoch: 6120 mean train loss:  2.64417904e-05, mean val. loss:  1.98086441e+00\n",
      "Epoch: 6121 mean train loss:  2.70351593e-05, mean val. loss:  1.98102546e+00\n",
      "Epoch: 6122 mean train loss:  2.65785784e-05, mean val. loss:  1.98118341e+00\n",
      "Epoch: 6123 mean train loss:  2.63673137e-05, mean val. loss:  1.98133683e+00\n",
      "Epoch: 6124 mean train loss:  2.64908012e-05, mean val. loss:  1.98148966e+00\n",
      "Epoch: 6125 mean train loss:  2.68728181e-05, mean val. loss:  1.98164487e+00\n",
      "Epoch: 6126 mean train loss:  2.68639415e-05, mean val. loss:  1.98180211e+00\n",
      "Epoch: 6127 mean train loss:  2.67022988e-05, mean val. loss:  1.98197067e+00\n",
      "Epoch: 6128 mean train loss:  2.63563124e-05, mean val. loss:  1.98213100e+00\n",
      "Epoch: 6129 mean train loss:  2.72862962e-05, mean val. loss:  1.98228848e+00\n",
      "Epoch: 6130 mean train loss:  2.65632407e-05, mean val. loss:  1.98244917e+00\n",
      "Epoch: 6131 mean train loss:  2.75464263e-05, mean val. loss:  1.98259342e+00\n",
      "Epoch: 6132 mean train loss:  2.67039868e-05, mean val. loss:  1.98274839e+00\n",
      "Epoch: 6133 mean train loss:  2.70673772e-05, mean val. loss:  1.98291636e+00\n",
      "Epoch: 6134 mean train loss:  2.62567482e-05, mean val. loss:  1.98308718e+00\n",
      "Epoch: 6135 mean train loss:  2.66229326e-05, mean val. loss:  1.98326004e+00\n",
      "Epoch: 6136 mean train loss:  2.58154760e-05, mean val. loss:  1.98344052e+00\n",
      "Epoch: 6137 mean train loss:  2.72103061e-05, mean val. loss:  1.98362017e+00\n",
      "Epoch: 6138 mean train loss:  2.71287281e-05, mean val. loss:  1.98378861e+00\n",
      "Epoch: 6139 mean train loss:  2.63843976e-05, mean val. loss:  1.98396909e+00\n",
      "Epoch: 6140 mean train loss:  2.66892312e-05, mean val. loss:  1.98414588e+00\n",
      "Epoch: 6141 mean train loss:  2.69721495e-05, mean val. loss:  1.98431718e+00\n",
      "Epoch: 6142 mean train loss:  2.64298869e-05, mean val. loss:  1.98449397e+00\n",
      "Epoch: 6143 mean train loss:  2.69250304e-05, mean val. loss:  1.98465979e+00\n",
      "Epoch: 6144 mean train loss:  2.63980182e-05, mean val. loss:  1.98483002e+00\n",
      "Epoch: 6145 mean train loss:  2.66871648e-05, mean val. loss:  1.98499668e+00\n",
      "Epoch: 6146 mean train loss:  2.66898714e-05, mean val. loss:  1.98514807e+00\n",
      "Epoch: 6147 mean train loss:  2.73976766e-05, mean val. loss:  1.98528969e+00\n",
      "Epoch: 6148 mean train loss:  2.69501179e-05, mean val. loss:  1.98541999e+00\n",
      "Epoch: 6149 mean train loss:  2.68025324e-05, mean val. loss:  1.98552835e+00\n",
      "Epoch: 6150 mean train loss:  2.69471202e-05, mean val. loss:  1.98562396e+00\n",
      "Epoch: 6151 mean train loss:  2.65382114e-05, mean val. loss:  1.98571038e+00\n",
      "Epoch: 6152 mean train loss:  2.62762478e-05, mean val. loss:  1.98580980e+00\n",
      "Epoch: 6153 mean train loss:  2.69620505e-05, mean val. loss:  1.98589981e+00\n",
      "Epoch: 6154 mean train loss:  2.59957742e-05, mean val. loss:  1.98600125e+00\n",
      "Epoch: 6155 mean train loss:  2.61394598e-05, mean val. loss:  1.98610830e+00\n",
      "Epoch: 6156 mean train loss:  2.66929273e-05, mean val. loss:  1.98621631e+00\n",
      "Epoch: 6157 mean train loss:  2.67479627e-05, mean val. loss:  1.98632240e+00\n",
      "Epoch: 6158 mean train loss:  2.67611176e-05, mean val. loss:  1.98644650e+00\n",
      "Epoch: 6159 mean train loss:  2.67047144e-05, mean val. loss:  1.98657858e+00\n",
      "Epoch: 6160 mean train loss:  2.67227297e-05, mean val. loss:  1.98672867e+00\n",
      "Epoch: 6161 mean train loss:  2.71262252e-05, mean val. loss:  1.98688638e+00\n",
      "Epoch: 6162 mean train loss:  2.63805850e-05, mean val. loss:  1.98707449e+00\n",
      "Epoch: 6163 mean train loss:  2.68637086e-05, mean val. loss:  1.98726344e+00\n",
      "Epoch: 6164 mean train loss:  2.64638511e-05, mean val. loss:  1.98747504e+00\n",
      "Epoch: 6165 mean train loss:  2.72789912e-05, mean val. loss:  1.98769331e+00\n",
      "Epoch: 6166 mean train loss:  2.68569565e-05, mean val. loss:  1.98790610e+00\n",
      "Epoch: 6167 mean train loss:  2.66792777e-05, mean val. loss:  1.98811722e+00\n",
      "Epoch: 6168 mean train loss:  2.69397860e-05, mean val. loss:  1.98831797e+00\n",
      "Epoch: 6169 mean train loss:  2.69147858e-05, mean val. loss:  1.98851418e+00\n",
      "Epoch: 6170 mean train loss:  2.62919348e-05, mean val. loss:  1.98871887e+00\n",
      "Epoch: 6171 mean train loss:  2.63776165e-05, mean val. loss:  1.98891973e+00\n",
      "Epoch: 6172 mean train loss:  2.65589624e-05, mean val. loss:  1.98910058e+00\n",
      "Epoch: 6173 mean train loss:  2.58815417e-05, mean val. loss:  1.98927236e+00\n",
      "Epoch: 6174 mean train loss:  2.62618123e-05, mean val. loss:  1.98943222e+00\n",
      "Epoch: 6175 mean train loss:  2.67974974e-05, mean val. loss:  1.98959076e+00\n",
      "Epoch: 6176 mean train loss:  2.67189171e-05, mean val. loss:  1.98974681e+00\n",
      "Epoch: 6177 mean train loss:  2.67451105e-05, mean val. loss:  1.98990691e+00\n",
      "Epoch: 6178 mean train loss:  2.66364659e-05, mean val. loss:  1.99006045e+00\n",
      "Epoch: 6179 mean train loss:  2.69172015e-05, mean val. loss:  1.99021542e+00\n",
      "Epoch: 6180 mean train loss:  2.70642922e-05, mean val. loss:  1.99037051e+00\n",
      "Epoch: 6181 mean train loss:  2.67302676e-05, mean val. loss:  1.99052739e+00\n",
      "Epoch: 6182 mean train loss:  2.67314899e-05, mean val. loss:  1.99069154e+00\n",
      "Epoch: 6183 mean train loss:  2.65462149e-05, mean val. loss:  1.99086809e+00\n",
      "Epoch: 6184 mean train loss:  2.62741814e-05, mean val. loss:  1.99105513e+00\n",
      "Epoch: 6185 mean train loss:  2.69789016e-05, mean val. loss:  1.99124980e+00\n",
      "Epoch: 6186 mean train loss:  2.67449941e-05, mean val. loss:  1.99144983e+00\n",
      "Epoch: 6187 mean train loss:  2.66805873e-05, mean val. loss:  1.99166000e+00\n",
      "Epoch: 6188 mean train loss:  2.64349510e-05, mean val. loss:  1.99187684e+00\n",
      "Epoch: 6189 mean train loss:  2.72195612e-05, mean val. loss:  1.99208701e+00\n",
      "Epoch: 6190 mean train loss:  2.69172597e-05, mean val. loss:  1.99230707e+00\n",
      "Epoch: 6191 mean train loss:  2.63340771e-05, mean val. loss:  1.99252427e+00\n",
      "Epoch: 6192 mean train loss:  2.70028831e-05, mean val. loss:  1.99271882e+00\n",
      "Epoch: 6193 mean train loss:  2.70256423e-05, mean val. loss:  1.99290824e+00\n",
      "Epoch: 6194 mean train loss:  2.60589295e-05, mean val. loss:  1.99309576e+00\n",
      "Epoch: 6195 mean train loss:  2.64789851e-05, mean val. loss:  1.99327540e+00\n",
      "Epoch: 6196 mean train loss:  2.66597199e-05, mean val. loss:  1.99344826e+00\n",
      "Epoch: 6197 mean train loss:  2.67277646e-05, mean val. loss:  1.99359083e+00\n",
      "Epoch: 6198 mean train loss:  2.67491559e-05, mean val. loss:  1.99372077e+00\n",
      "Epoch: 6199 mean train loss:  2.66350980e-05, mean val. loss:  1.99383986e+00\n",
      "Epoch: 6200 mean train loss:  2.65064300e-05, mean val. loss:  1.99394083e+00\n",
      "Epoch: 6201 mean train loss:  2.61078821e-05, mean val. loss:  1.99403954e+00\n",
      "Epoch: 6202 mean train loss:  2.63516849e-05, mean val. loss:  1.99413085e+00\n",
      "Epoch: 6203 mean train loss:  2.65147246e-05, mean val. loss:  1.99421990e+00\n",
      "Epoch: 6204 mean train loss:  2.66562565e-05, mean val. loss:  1.99431109e+00\n",
      "Epoch: 6205 mean train loss:  2.69086158e-05, mean val. loss:  1.99439597e+00\n",
      "Epoch: 6206 mean train loss:  2.62396934e-05, mean val. loss:  1.99448824e+00\n",
      "Epoch: 6207 mean train loss:  2.66017742e-05, mean val. loss:  1.99456704e+00\n",
      "Epoch: 6208 mean train loss:  2.62174290e-05, mean val. loss:  1.99465930e+00\n",
      "Epoch: 6209 mean train loss:  2.68692384e-05, mean val. loss:  1.99475718e+00\n",
      "Epoch: 6210 mean train loss:  2.63929542e-05, mean val. loss:  1.99485338e+00\n",
      "Epoch: 6211 mean train loss:  2.64090195e-05, mean val. loss:  1.99494803e+00\n",
      "Epoch: 6212 mean train loss:  2.65108538e-05, mean val. loss:  1.99503589e+00\n",
      "Epoch: 6213 mean train loss:  2.67113792e-05, mean val. loss:  1.99512064e+00\n",
      "Epoch: 6214 mean train loss:  2.68123113e-05, mean val. loss:  1.99519956e+00\n",
      "Epoch: 6215 mean train loss:  2.62698741e-05, mean val. loss:  1.99528790e+00\n",
      "Epoch: 6216 mean train loss:  2.70650198e-05, mean val. loss:  1.99537158e+00\n",
      "Epoch: 6217 mean train loss:  2.68742733e-05, mean val. loss:  1.99546409e+00\n",
      "Epoch: 6218 mean train loss:  2.63970578e-05, mean val. loss:  1.99556947e+00\n",
      "Epoch: 6219 mean train loss:  2.69750308e-05, mean val. loss:  1.99566972e+00\n",
      "Epoch: 6220 mean train loss:  2.64474656e-05, mean val. loss:  1.99579489e+00\n",
      "Epoch: 6221 mean train loss:  2.61667010e-05, mean val. loss:  1.99592996e+00\n",
      "Epoch: 6222 mean train loss:  2.65191193e-05, mean val. loss:  1.99606812e+00\n",
      "Epoch: 6223 mean train loss:  2.64044793e-05, mean val. loss:  1.99621010e+00\n",
      "Epoch: 6224 mean train loss:  2.64218834e-05, mean val. loss:  1.99635518e+00\n",
      "Epoch: 6225 mean train loss:  2.70423770e-05, mean val. loss:  1.99649954e+00\n",
      "Epoch: 6226 mean train loss:  2.62567773e-05, mean val. loss:  1.99666417e+00\n",
      "Epoch: 6227 mean train loss:  2.64676055e-05, mean val. loss:  1.99683857e+00\n",
      "Epoch: 6228 mean train loss:  2.64668779e-05, mean val. loss:  1.99702632e+00\n",
      "Epoch: 6229 mean train loss:  2.59619846e-05, mean val. loss:  1.99721372e+00\n",
      "Epoch: 6230 mean train loss:  2.69464799e-05, mean val. loss:  1.99738729e+00\n",
      "Epoch: 6231 mean train loss:  2.64667906e-05, mean val. loss:  1.99756813e+00\n",
      "Epoch: 6232 mean train loss:  2.62602989e-05, mean val. loss:  1.99775434e+00\n",
      "Epoch: 6233 mean train loss:  2.58510117e-05, mean val. loss:  1.99796033e+00\n",
      "Epoch: 6234 mean train loss:  2.59104709e-05, mean val. loss:  1.99818563e+00\n",
      "Epoch: 6235 mean train loss:  2.68600998e-05, mean val. loss:  1.99839795e+00\n",
      "Epoch: 6236 mean train loss:  2.68697331e-05, mean val. loss:  1.99859810e+00\n",
      "Epoch: 6237 mean train loss:  2.64807022e-05, mean val. loss:  1.99880421e+00\n",
      "Epoch: 6238 mean train loss:  2.66252027e-05, mean val. loss:  1.99901903e+00\n",
      "Epoch: 6239 mean train loss:  2.71689205e-05, mean val. loss:  1.99922657e+00\n",
      "Epoch: 6240 mean train loss:  2.63350084e-05, mean val. loss:  1.99942780e+00\n",
      "Epoch: 6241 mean train loss:  2.58641085e-05, mean val. loss:  1.99964595e+00\n",
      "Epoch: 6242 mean train loss:  2.62444082e-05, mean val. loss:  1.99986064e+00\n",
      "Epoch: 6243 mean train loss:  2.63470574e-05, mean val. loss:  2.00007987e+00\n",
      "Epoch: 6244 mean train loss:  2.68612057e-05, mean val. loss:  2.00029445e+00\n",
      "Epoch: 6245 mean train loss:  2.62950780e-05, mean val. loss:  2.00049496e+00\n",
      "Epoch: 6246 mean train loss:  2.60963279e-05, mean val. loss:  2.00069284e+00\n",
      "Epoch: 6247 mean train loss:  2.69281154e-05, mean val. loss:  2.00087357e+00\n",
      "Epoch: 6248 mean train loss:  2.64822738e-05, mean val. loss:  2.00103760e+00\n",
      "Epoch: 6249 mean train loss:  2.68644944e-05, mean val. loss:  2.00119138e+00\n",
      "Epoch: 6250 mean train loss:  2.66216230e-05, mean val. loss:  2.00133705e+00\n",
      "Epoch: 6251 mean train loss:  2.66595453e-05, mean val. loss:  2.00146675e+00\n",
      "Epoch: 6252 mean train loss:  2.66596908e-05, mean val. loss:  2.00158215e+00\n",
      "Epoch: 6253 mean train loss:  2.65490380e-05, mean val. loss:  2.00167847e+00\n",
      "Epoch: 6254 mean train loss:  2.61933019e-05, mean val. loss:  2.00176001e+00\n",
      "Epoch: 6255 mean train loss:  2.63632392e-05, mean val. loss:  2.00184703e+00\n",
      "Epoch: 6256 mean train loss:  2.65190611e-05, mean val. loss:  2.00191689e+00\n",
      "Epoch: 6257 mean train loss:  2.62848625e-05, mean val. loss:  2.00197577e+00\n",
      "Epoch: 6258 mean train loss:  2.61904788e-05, mean val. loss:  2.00201964e+00\n",
      "Epoch: 6259 mean train loss:  2.63625989e-05, mean val. loss:  2.00205779e+00\n",
      "Epoch: 6260 mean train loss:  2.61559908e-05, mean val. loss:  2.00209212e+00\n",
      "Epoch: 6261 mean train loss:  2.67734868e-05, mean val. loss:  2.00212479e+00\n",
      "Epoch: 6262 mean train loss:  2.69741577e-05, mean val. loss:  2.00214410e+00\n",
      "Epoch: 6263 mean train loss:  2.69535813e-05, mean val. loss:  2.00215769e+00\n",
      "Epoch: 6264 mean train loss:  2.64649861e-05, mean val. loss:  2.00217509e+00\n",
      "Epoch: 6265 mean train loss:  2.61271780e-05, mean val. loss:  2.00219369e+00\n",
      "Epoch: 6266 mean train loss:  2.68595468e-05, mean val. loss:  2.00220180e+00\n",
      "Epoch: 6267 mean train loss:  2.61091045e-05, mean val. loss:  2.00220513e+00\n",
      "Epoch: 6268 mean train loss:  2.61139066e-05, mean val. loss:  2.00220895e+00\n",
      "Epoch: 6269 mean train loss:  2.66575371e-05, mean val. loss:  2.00220537e+00\n",
      "Epoch: 6270 mean train loss:  2.63578258e-05, mean val. loss:  2.00219607e+00\n",
      "Epoch: 6271 mean train loss:  2.66666757e-05, mean val. loss:  2.00219512e+00\n",
      "Epoch: 6272 mean train loss:  2.63015099e-05, mean val. loss:  2.00220299e+00\n",
      "Epoch: 6273 mean train loss:  2.63791298e-05, mean val. loss:  2.00221753e+00\n",
      "Epoch: 6274 mean train loss:  2.63671391e-05, mean val. loss:  2.00224853e+00\n",
      "Epoch: 6275 mean train loss:  2.65461567e-05, mean val. loss:  2.00227380e+00\n",
      "Epoch: 6276 mean train loss:  2.67786672e-05, mean val. loss:  2.00231791e+00\n",
      "Epoch: 6277 mean train loss:  2.67103023e-05, mean val. loss:  2.00237250e+00\n",
      "Epoch: 6278 mean train loss:  2.65747367e-05, mean val. loss:  2.00242329e+00\n",
      "Epoch: 6279 mean train loss:  2.60893721e-05, mean val. loss:  2.00249910e+00\n",
      "Epoch: 6280 mean train loss:  2.65206909e-05, mean val. loss:  2.00257444e+00\n",
      "Epoch: 6281 mean train loss:  2.65547133e-05, mean val. loss:  2.00264812e+00\n",
      "Epoch: 6282 mean train loss:  2.61806417e-05, mean val. loss:  2.00272727e+00\n",
      "Epoch: 6283 mean train loss:  2.65397539e-05, mean val. loss:  2.00282025e+00\n",
      "Epoch: 6284 mean train loss:  2.64576229e-05, mean val. loss:  2.00292683e+00\n",
      "Epoch: 6285 mean train loss:  2.63952243e-05, mean val. loss:  2.00304270e+00\n",
      "Epoch: 6286 mean train loss:  2.65017152e-05, mean val. loss:  2.00318289e+00\n",
      "Epoch: 6287 mean train loss:  2.62585236e-05, mean val. loss:  2.00332952e+00\n",
      "Epoch: 6288 mean train loss:  2.69556476e-05, mean val. loss:  2.00348043e+00\n",
      "Epoch: 6289 mean train loss:  2.60864908e-05, mean val. loss:  2.00364661e+00\n",
      "Epoch: 6290 mean train loss:  2.60245870e-05, mean val. loss:  2.00382781e+00\n",
      "Epoch: 6291 mean train loss:  2.64322443e-05, mean val. loss:  2.00401807e+00\n",
      "Epoch: 6292 mean train loss:  2.66820716e-05, mean val. loss:  2.00421953e+00\n",
      "Epoch: 6293 mean train loss:  2.64221453e-05, mean val. loss:  2.00442791e+00\n",
      "Epoch: 6294 mean train loss:  2.64383852e-05, mean val. loss:  2.00462198e+00\n",
      "Epoch: 6295 mean train loss:  2.65092531e-05, mean val. loss:  2.00482345e+00\n",
      "Epoch: 6296 mean train loss:  2.66538991e-05, mean val. loss:  2.00503993e+00\n",
      "Epoch: 6297 mean train loss:  2.65807030e-05, mean val. loss:  2.00526595e+00\n",
      "Epoch: 6298 mean train loss:  2.61740934e-05, mean val. loss:  2.00548840e+00\n",
      "Epoch: 6299 mean train loss:  2.65284034e-05, mean val. loss:  2.00571942e+00\n",
      "Epoch: 6300 mean train loss:  2.67721189e-05, mean val. loss:  2.00593996e+00\n",
      "Epoch: 6301 mean train loss:  2.61652749e-05, mean val. loss:  2.00616717e+00\n",
      "Epoch: 6302 mean train loss:  2.56135827e-05, mean val. loss:  2.00637627e+00\n",
      "Epoch: 6303 mean train loss:  2.60504021e-05, mean val. loss:  2.00658131e+00\n",
      "Epoch: 6304 mean train loss:  2.59445515e-05, mean val. loss:  2.00677180e+00\n",
      "Epoch: 6305 mean train loss:  2.68328586e-05, mean val. loss:  2.00694466e+00\n",
      "Epoch: 6306 mean train loss:  2.56266503e-05, mean val. loss:  2.00711632e+00\n",
      "Epoch: 6307 mean train loss:  2.65576818e-05, mean val. loss:  2.00727248e+00\n",
      "Epoch: 6308 mean train loss:  2.63222319e-05, mean val. loss:  2.00741625e+00\n",
      "Epoch: 6309 mean train loss:  2.67118157e-05, mean val. loss:  2.00756669e+00\n",
      "Epoch: 6310 mean train loss:  2.62005487e-05, mean val. loss:  2.00770950e+00\n",
      "Epoch: 6311 mean train loss:  2.65785784e-05, mean val. loss:  2.00784254e+00\n",
      "Epoch: 6312 mean train loss:  2.59385561e-05, mean val. loss:  2.00795269e+00\n",
      "Epoch: 6313 mean train loss:  2.61970272e-05, mean val. loss:  2.00806403e+00\n",
      "Epoch: 6314 mean train loss:  2.60766828e-05, mean val. loss:  2.00815701e+00\n",
      "Epoch: 6315 mean train loss:  2.63302645e-05, mean val. loss:  2.00822902e+00\n",
      "Epoch: 6316 mean train loss:  2.64433329e-05, mean val. loss:  2.00829077e+00\n",
      "Epoch: 6317 mean train loss:  2.62913527e-05, mean val. loss:  2.00834060e+00\n",
      "Epoch: 6318 mean train loss:  2.55982741e-05, mean val. loss:  2.00839019e+00\n",
      "Epoch: 6319 mean train loss:  2.65325652e-05, mean val. loss:  2.00843120e+00\n",
      "Epoch: 6320 mean train loss:  2.67982832e-05, mean val. loss:  2.00845218e+00\n",
      "Epoch: 6321 mean train loss:  2.63317197e-05, mean val. loss:  2.00846267e+00\n",
      "Epoch: 6322 mean train loss:  2.59098597e-05, mean val. loss:  2.00848126e+00\n",
      "Epoch: 6323 mean train loss:  2.64509872e-05, mean val. loss:  2.00849032e+00\n",
      "Epoch: 6324 mean train loss:  2.60840461e-05, mean val. loss:  2.00849986e+00\n",
      "Epoch: 6325 mean train loss:  2.63153051e-05, mean val. loss:  2.00849414e+00\n",
      "Epoch: 6326 mean train loss:  2.66277348e-05, mean val. loss:  2.00847840e+00\n",
      "Epoch: 6327 mean train loss:  2.67240102e-05, mean val. loss:  2.00846195e+00\n",
      "Epoch: 6328 mean train loss:  2.60396046e-05, mean val. loss:  2.00844622e+00\n",
      "Epoch: 6329 mean train loss:  2.61027017e-05, mean val. loss:  2.00843334e+00\n",
      "Epoch: 6330 mean train loss:  2.63406255e-05, mean val. loss:  2.00842714e+00\n",
      "Epoch: 6331 mean train loss:  2.64781411e-05, mean val. loss:  2.00842094e+00\n",
      "Epoch: 6332 mean train loss:  2.65796261e-05, mean val. loss:  2.00840616e+00\n",
      "Epoch: 6333 mean train loss:  2.63888505e-05, mean val. loss:  2.00841331e+00\n",
      "Epoch: 6334 mean train loss:  2.61733076e-05, mean val. loss:  2.00845265e+00\n",
      "Epoch: 6335 mean train loss:  2.59555818e-05, mean val. loss:  2.00851250e+00\n",
      "Epoch: 6336 mean train loss:  2.66402494e-05, mean val. loss:  2.00858760e+00\n",
      "Epoch: 6337 mean train loss:  2.64970295e-05, mean val. loss:  2.00866532e+00\n",
      "Epoch: 6338 mean train loss:  2.60629167e-05, mean val. loss:  2.00875020e+00\n",
      "Epoch: 6339 mean train loss:  2.62414105e-05, mean val. loss:  2.00884461e+00\n",
      "Epoch: 6340 mean train loss:  2.60816014e-05, mean val. loss:  2.00894046e+00\n",
      "Epoch: 6341 mean train loss:  2.60046218e-05, mean val. loss:  2.00903344e+00\n",
      "Epoch: 6342 mean train loss:  2.65538692e-05, mean val. loss:  2.00914121e+00\n",
      "Epoch: 6343 mean train loss:  2.61222885e-05, mean val. loss:  2.00925493e+00\n",
      "Epoch: 6344 mean train loss:  2.58622167e-05, mean val. loss:  2.00939322e+00\n",
      "Epoch: 6345 mean train loss:  2.62912363e-05, mean val. loss:  2.00953746e+00\n",
      "Epoch: 6346 mean train loss:  2.63015099e-05, mean val. loss:  2.00968122e+00\n",
      "Epoch: 6347 mean train loss:  2.57073552e-05, mean val. loss:  2.00983691e+00\n",
      "Epoch: 6348 mean train loss:  2.59109656e-05, mean val. loss:  2.01000977e+00\n",
      "Epoch: 6349 mean train loss:  2.61943787e-05, mean val. loss:  2.01019764e+00\n",
      "Epoch: 6350 mean train loss:  2.60374800e-05, mean val. loss:  2.01039457e+00\n",
      "Epoch: 6351 mean train loss:  2.62974645e-05, mean val. loss:  2.01062608e+00\n",
      "Epoch: 6352 mean train loss:  2.63807306e-05, mean val. loss:  2.01086593e+00\n",
      "Epoch: 6353 mean train loss:  2.61510431e-05, mean val. loss:  2.01110172e+00\n",
      "Epoch: 6354 mean train loss:  2.64534028e-05, mean val. loss:  2.01134920e+00\n",
      "Epoch: 6355 mean train loss:  2.64857372e-05, mean val. loss:  2.01159358e+00\n",
      "Epoch: 6356 mean train loss:  2.70596938e-05, mean val. loss:  2.01183748e+00\n",
      "Epoch: 6357 mean train loss:  2.62470567e-05, mean val. loss:  2.01208186e+00\n",
      "Epoch: 6358 mean train loss:  2.59035442e-05, mean val. loss:  2.01233220e+00\n",
      "Epoch: 6359 mean train loss:  2.58806103e-05, mean val. loss:  2.01257372e+00\n",
      "Epoch: 6360 mean train loss:  2.60839006e-05, mean val. loss:  2.01280856e+00\n",
      "Epoch: 6361 mean train loss:  2.58933578e-05, mean val. loss:  2.01304531e+00\n",
      "Epoch: 6362 mean train loss:  2.60397792e-05, mean val. loss:  2.01326942e+00\n",
      "Epoch: 6363 mean train loss:  2.55974592e-05, mean val. loss:  2.01349306e+00\n",
      "Epoch: 6364 mean train loss:  2.58935615e-05, mean val. loss:  2.01371980e+00\n",
      "Epoch: 6365 mean train loss:  2.58581713e-05, mean val. loss:  2.01394176e+00\n",
      "Epoch: 6366 mean train loss:  2.57688516e-05, mean val. loss:  2.01416397e+00\n",
      "Epoch: 6367 mean train loss:  2.57582578e-05, mean val. loss:  2.01437354e+00\n",
      "Epoch: 6368 mean train loss:  2.63623660e-05, mean val. loss:  2.01456308e+00\n",
      "Epoch: 6369 mean train loss:  2.63204565e-05, mean val. loss:  2.01474214e+00\n",
      "Epoch: 6370 mean train loss:  2.62826215e-05, mean val. loss:  2.01491499e+00\n",
      "Epoch: 6371 mean train loss:  2.63816037e-05, mean val. loss:  2.01508236e+00\n",
      "Epoch: 6372 mean train loss:  2.62193789e-05, mean val. loss:  2.01522541e+00\n",
      "Epoch: 6373 mean train loss:  2.59941735e-05, mean val. loss:  2.01535845e+00\n",
      "Epoch: 6374 mean train loss:  2.60580855e-05, mean val. loss:  2.01549172e+00\n",
      "Epoch: 6375 mean train loss:  2.62559624e-05, mean val. loss:  2.01561427e+00\n",
      "Epoch: 6376 mean train loss:  2.61720270e-05, mean val. loss:  2.01572752e+00\n",
      "Epoch: 6377 mean train loss:  2.64336995e-05, mean val. loss:  2.01582289e+00\n",
      "Epoch: 6378 mean train loss:  2.66216230e-05, mean val. loss:  2.01589704e+00\n",
      "Epoch: 6379 mean train loss:  2.55845080e-05, mean val. loss:  2.01595640e+00\n",
      "Epoch: 6380 mean train loss:  2.56468484e-05, mean val. loss:  2.01599097e+00\n",
      "Epoch: 6381 mean train loss:  2.59218505e-05, mean val. loss:  2.01601005e+00\n",
      "Epoch: 6382 mean train loss:  2.56282510e-05, mean val. loss:  2.01603389e+00\n",
      "Epoch: 6383 mean train loss:  2.62443791e-05, mean val. loss:  2.01603389e+00\n",
      "Epoch: 6384 mean train loss:  2.61871319e-05, mean val. loss:  2.01602936e+00\n",
      "Epoch: 6385 mean train loss:  2.60523520e-05, mean val. loss:  2.01601410e+00\n",
      "Epoch: 6386 mean train loss:  2.66602437e-05, mean val. loss:  2.01597762e+00\n",
      "Epoch: 6387 mean train loss:  2.61055247e-05, mean val. loss:  2.01593375e+00\n",
      "Epoch: 6388 mean train loss:  2.63040129e-05, mean val. loss:  2.01587296e+00\n",
      "Epoch: 6389 mean train loss:  2.68435106e-05, mean val. loss:  2.01580930e+00\n",
      "Epoch: 6390 mean train loss:  2.63854745e-05, mean val. loss:  2.01574039e+00\n",
      "Epoch: 6391 mean train loss:  2.56232161e-05, mean val. loss:  2.01568103e+00\n",
      "Epoch: 6392 mean train loss:  2.54719635e-05, mean val. loss:  2.01562405e+00\n",
      "Epoch: 6393 mean train loss:  2.58447835e-05, mean val. loss:  2.01555419e+00\n",
      "Epoch: 6394 mean train loss:  2.58864311e-05, mean val. loss:  2.01549268e+00\n",
      "Epoch: 6395 mean train loss:  2.62589892e-05, mean val. loss:  2.01541853e+00\n",
      "Epoch: 6396 mean train loss:  2.62259273e-05, mean val. loss:  2.01535845e+00\n",
      "Epoch: 6397 mean train loss:  2.59084336e-05, mean val. loss:  2.01530457e+00\n",
      "Epoch: 6398 mean train loss:  2.59542430e-05, mean val. loss:  2.01525807e+00\n",
      "Epoch: 6399 mean train loss:  2.63020629e-05, mean val. loss:  2.01522422e+00\n",
      "Epoch: 6400 mean train loss:  2.59118387e-05, mean val. loss:  2.01521134e+00\n",
      "Epoch: 6401 mean train loss:  2.58893124e-05, mean val. loss:  2.01520443e+00\n",
      "Epoch: 6402 mean train loss:  2.60972301e-05, mean val. loss:  2.01521945e+00\n",
      "Epoch: 6403 mean train loss:  2.58291839e-05, mean val. loss:  2.01526499e+00\n",
      "Epoch: 6404 mean train loss:  2.59655935e-05, mean val. loss:  2.01532865e+00\n",
      "Epoch: 6405 mean train loss:  2.59514200e-05, mean val. loss:  2.01541948e+00\n",
      "Epoch: 6406 mean train loss:  2.62472895e-05, mean val. loss:  2.01552558e+00\n",
      "Epoch: 6407 mean train loss:  2.62027606e-05, mean val. loss:  2.01563883e+00\n",
      "Epoch: 6408 mean train loss:  2.58679502e-05, mean val. loss:  2.01576471e+00\n",
      "Epoch: 6409 mean train loss:  2.62796821e-05, mean val. loss:  2.01590538e+00\n",
      "Epoch: 6410 mean train loss:  2.57750798e-05, mean val. loss:  2.01606822e+00\n",
      "Epoch: 6411 mean train loss:  2.57529027e-05, mean val. loss:  2.01625443e+00\n",
      "Epoch: 6412 mean train loss:  2.59456574e-05, mean val. loss:  2.01644874e+00\n",
      "Epoch: 6413 mean train loss:  2.68312579e-05, mean val. loss:  2.01663065e+00\n",
      "Epoch: 6414 mean train loss:  2.65054405e-05, mean val. loss:  2.01681709e+00\n",
      "Epoch: 6415 mean train loss:  2.60276429e-05, mean val. loss:  2.01700687e+00\n",
      "Epoch: 6416 mean train loss:  2.57653883e-05, mean val. loss:  2.01719379e+00\n",
      "Epoch: 6417 mean train loss:  2.62780522e-05, mean val. loss:  2.01737857e+00\n",
      "Epoch: 6418 mean train loss:  2.57669599e-05, mean val. loss:  2.01755118e+00\n",
      "Epoch: 6419 mean train loss:  2.54270271e-05, mean val. loss:  2.01772738e+00\n",
      "Epoch: 6420 mean train loss:  2.62975227e-05, mean val. loss:  2.01789737e+00\n",
      "Epoch: 6421 mean train loss:  2.63395195e-05, mean val. loss:  2.01804376e+00\n",
      "Epoch: 6422 mean train loss:  2.54706538e-05, mean val. loss:  2.01817846e+00\n",
      "Epoch: 6423 mean train loss:  2.60730158e-05, mean val. loss:  2.01830721e+00\n",
      "Epoch: 6424 mean train loss:  2.59288936e-05, mean val. loss:  2.01844096e+00\n",
      "Epoch: 6425 mean train loss:  2.52354657e-05, mean val. loss:  2.01856470e+00\n",
      "Epoch: 6426 mean train loss:  2.53745529e-05, mean val. loss:  2.01869893e+00\n",
      "Epoch: 6427 mean train loss:  2.62479298e-05, mean val. loss:  2.01882672e+00\n",
      "Epoch: 6428 mean train loss:  2.64935079e-05, mean val. loss:  2.01893711e+00\n",
      "Epoch: 6429 mean train loss:  2.59384688e-05, mean val. loss:  2.01904345e+00\n",
      "Epoch: 6430 mean train loss:  2.64240080e-05, mean val. loss:  2.01912689e+00\n",
      "Epoch: 6431 mean train loss:  2.55085761e-05, mean val. loss:  2.01921272e+00\n",
      "Epoch: 6432 mean train loss:  2.65941489e-05, mean val. loss:  2.01928186e+00\n",
      "Epoch: 6433 mean train loss:  2.61251116e-05, mean val. loss:  2.01934910e+00\n",
      "Epoch: 6434 mean train loss:  2.58449581e-05, mean val. loss:  2.01940179e+00\n",
      "Epoch: 6435 mean train loss:  2.61170208e-05, mean val. loss:  2.01943803e+00\n",
      "Epoch: 6436 mean train loss:  2.55799096e-05, mean val. loss:  2.01945543e+00\n",
      "Epoch: 6437 mean train loss:  2.60779634e-05, mean val. loss:  2.01944852e+00\n",
      "Epoch: 6438 mean train loss:  2.57656211e-05, mean val. loss:  2.01944375e+00\n",
      "Epoch: 6439 mean train loss:  2.61062814e-05, mean val. loss:  2.01942897e+00\n",
      "Epoch: 6440 mean train loss:  2.56257772e-05, mean val. loss:  2.01941037e+00\n",
      "Epoch: 6441 mean train loss:  2.60248780e-05, mean val. loss:  2.01937962e+00\n",
      "Epoch: 6442 mean train loss:  2.58999062e-05, mean val. loss:  2.01934433e+00\n",
      "Epoch: 6443 mean train loss:  2.59174849e-05, mean val. loss:  2.01930809e+00\n",
      "Epoch: 6444 mean train loss:  2.62293906e-05, mean val. loss:  2.01927090e+00\n",
      "Epoch: 6445 mean train loss:  2.61666719e-05, mean val. loss:  2.01923609e+00\n",
      "Epoch: 6446 mean train loss:  2.57441425e-05, mean val. loss:  2.01920485e+00\n",
      "Epoch: 6447 mean train loss:  2.60556408e-05, mean val. loss:  2.01918125e+00\n",
      "Epoch: 6448 mean train loss:  2.58979562e-05, mean val. loss:  2.01916146e+00\n",
      "Epoch: 6449 mean train loss:  2.58063083e-05, mean val. loss:  2.01915956e+00\n",
      "Epoch: 6450 mean train loss:  2.65314011e-05, mean val. loss:  2.01916409e+00\n",
      "Epoch: 6451 mean train loss:  2.55444029e-05, mean val. loss:  2.01917076e+00\n",
      "Epoch: 6452 mean train loss:  2.60480738e-05, mean val. loss:  2.01917529e+00\n",
      "Epoch: 6453 mean train loss:  2.56721687e-05, mean val. loss:  2.01919675e+00\n",
      "Epoch: 6454 mean train loss:  2.60622182e-05, mean val. loss:  2.01924062e+00\n",
      "Epoch: 6455 mean train loss:  2.61582027e-05, mean val. loss:  2.01928687e+00\n",
      "Epoch: 6456 mean train loss:  2.60234519e-05, mean val. loss:  2.01934123e+00\n",
      "Epoch: 6457 mean train loss:  2.55059858e-05, mean val. loss:  2.01940560e+00\n",
      "Epoch: 6458 mean train loss:  2.58314540e-05, mean val. loss:  2.01946855e+00\n",
      "Epoch: 6459 mean train loss:  2.63375114e-05, mean val. loss:  2.01953125e+00\n",
      "Epoch: 6460 mean train loss:  2.56109925e-05, mean val. loss:  2.01962137e+00\n",
      "Epoch: 6461 mean train loss:  2.58238288e-05, mean val. loss:  2.01972866e+00\n",
      "Epoch: 6462 mean train loss:  2.61132955e-05, mean val. loss:  2.01984739e+00\n",
      "Epoch: 6463 mean train loss:  2.61896930e-05, mean val. loss:  2.01996183e+00\n",
      "Epoch: 6464 mean train loss:  2.54613406e-05, mean val. loss:  2.02008128e+00\n",
      "Epoch: 6465 mean train loss:  2.59282824e-05, mean val. loss:  2.02018714e+00\n",
      "Epoch: 6466 mean train loss:  2.56189378e-05, mean val. loss:  2.02029848e+00\n",
      "Epoch: 6467 mean train loss:  2.62198155e-05, mean val. loss:  2.02040553e+00\n",
      "Epoch: 6468 mean train loss:  2.56963831e-05, mean val. loss:  2.02050090e+00\n",
      "Epoch: 6469 mean train loss:  2.58667569e-05, mean val. loss:  2.02059484e+00\n",
      "Epoch: 6470 mean train loss:  2.60733650e-05, mean val. loss:  2.02070045e+00\n",
      "Epoch: 6471 mean train loss:  2.62248213e-05, mean val. loss:  2.02079010e+00\n",
      "Epoch: 6472 mean train loss:  2.57382926e-05, mean val. loss:  2.02087212e+00\n",
      "Epoch: 6473 mean train loss:  2.56873318e-05, mean val. loss:  2.02096772e+00\n",
      "Epoch: 6474 mean train loss:  2.54836632e-05, mean val. loss:  2.02106071e+00\n",
      "Epoch: 6475 mean train loss:  2.61027890e-05, mean val. loss:  2.02113748e+00\n",
      "Epoch: 6476 mean train loss:  2.57196662e-05, mean val. loss:  2.02120543e+00\n",
      "Epoch: 6477 mean train loss:  2.60608795e-05, mean val. loss:  2.02127218e+00\n",
      "Epoch: 6478 mean train loss:  2.54947518e-05, mean val. loss:  2.02135158e+00\n",
      "Epoch: 6479 mean train loss:  2.67271535e-05, mean val. loss:  2.02140307e+00\n",
      "Epoch: 6480 mean train loss:  2.57593638e-05, mean val. loss:  2.02145338e+00\n",
      "Epoch: 6481 mean train loss:  2.62946996e-05, mean val. loss:  2.02149105e+00\n",
      "Epoch: 6482 mean train loss:  2.61212117e-05, mean val. loss:  2.02152014e+00\n",
      "Epoch: 6483 mean train loss:  2.60444940e-05, mean val. loss:  2.02154207e+00\n",
      "Epoch: 6484 mean train loss:  2.60067172e-05, mean val. loss:  2.02155423e+00\n",
      "Epoch: 6485 mean train loss:  2.61430396e-05, mean val. loss:  2.02153230e+00\n",
      "Epoch: 6486 mean train loss:  2.55588966e-05, mean val. loss:  2.02150583e+00\n",
      "Epoch: 6487 mean train loss:  2.55842169e-05, mean val. loss:  2.02146244e+00\n",
      "Epoch: 6488 mean train loss:  2.54866900e-05, mean val. loss:  2.02140999e+00\n",
      "Epoch: 6489 mean train loss:  2.62107060e-05, mean val. loss:  2.02136350e+00\n",
      "Epoch: 6490 mean train loss:  2.53530161e-05, mean val. loss:  2.02130604e+00\n",
      "Epoch: 6491 mean train loss:  2.62276444e-05, mean val. loss:  2.02123833e+00\n",
      "Epoch: 6492 mean train loss:  2.55665509e-05, mean val. loss:  2.02118874e+00\n",
      "Epoch: 6493 mean train loss:  2.64179544e-05, mean val. loss:  2.02114224e+00\n",
      "Epoch: 6494 mean train loss:  2.58446671e-05, mean val. loss:  2.02110243e+00\n",
      "Epoch: 6495 mean train loss:  2.58999644e-05, mean val. loss:  2.02107096e+00\n",
      "Epoch: 6496 mean train loss:  2.56329658e-05, mean val. loss:  2.02105379e+00\n",
      "Epoch: 6497 mean train loss:  2.60538945e-05, mean val. loss:  2.02104616e+00\n",
      "Epoch: 6498 mean train loss:  2.62705726e-05, mean val. loss:  2.02104115e+00\n",
      "Epoch: 6499 mean train loss:  2.56995554e-05, mean val. loss:  2.02106094e+00\n",
      "Epoch: 6500 mean train loss:  2.65424896e-05, mean val. loss:  2.02109122e+00\n",
      "Epoch: 6501 mean train loss:  2.59976368e-05, mean val. loss:  2.02113247e+00\n",
      "Epoch: 6502 mean train loss:  2.59558146e-05, mean val. loss:  2.02118611e+00\n",
      "Epoch: 6503 mean train loss:  2.59473454e-05, mean val. loss:  2.02125740e+00\n",
      "Epoch: 6504 mean train loss:  2.55423656e-05, mean val. loss:  2.02133727e+00\n",
      "Epoch: 6505 mean train loss:  2.51907040e-05, mean val. loss:  2.02144337e+00\n",
      "Epoch: 6506 mean train loss:  2.64833798e-05, mean val. loss:  2.02152872e+00\n",
      "Epoch: 6507 mean train loss:  2.57749052e-05, mean val. loss:  2.02162147e+00\n",
      "Epoch: 6508 mean train loss:  2.57694628e-05, mean val. loss:  2.02171183e+00\n",
      "Epoch: 6509 mean train loss:  2.54755141e-05, mean val. loss:  2.02182245e+00\n",
      "Epoch: 6510 mean train loss:  2.59061053e-05, mean val. loss:  2.02193666e+00\n",
      "Epoch: 6511 mean train loss:  2.54537154e-05, mean val. loss:  2.02205801e+00\n",
      "Epoch: 6512 mean train loss:  2.56895146e-05, mean val. loss:  2.02218676e+00\n",
      "Epoch: 6513 mean train loss:  2.54085171e-05, mean val. loss:  2.02235389e+00\n",
      "Epoch: 6514 mean train loss:  2.54053157e-05, mean val. loss:  2.02252960e+00\n",
      "Epoch: 6515 mean train loss:  2.53714679e-05, mean val. loss:  2.02271461e+00\n",
      "Epoch: 6516 mean train loss:  2.61346868e-05, mean val. loss:  2.02289152e+00\n",
      "Epoch: 6517 mean train loss:  2.56062485e-05, mean val. loss:  2.02308965e+00\n",
      "Epoch: 6518 mean train loss:  2.61855603e-05, mean val. loss:  2.02327466e+00\n",
      "Epoch: 6519 mean train loss:  2.58743821e-05, mean val. loss:  2.02346992e+00\n",
      "Epoch: 6520 mean train loss:  2.58668733e-05, mean val. loss:  2.02365661e+00\n",
      "Epoch: 6521 mean train loss:  2.58654472e-05, mean val. loss:  2.02385092e+00\n",
      "Epoch: 6522 mean train loss:  2.58765067e-05, mean val. loss:  2.02404141e+00\n",
      "Epoch: 6523 mean train loss:  2.60424276e-05, mean val. loss:  2.02423120e+00\n",
      "Epoch: 6524 mean train loss:  2.57794745e-05, mean val. loss:  2.02439809e+00\n",
      "Epoch: 6525 mean train loss:  2.56714411e-05, mean val. loss:  2.02456975e+00\n",
      "Epoch: 6526 mean train loss:  2.57345964e-05, mean val. loss:  2.02471924e+00\n",
      "Epoch: 6527 mean train loss:  2.53973121e-05, mean val. loss:  2.02484655e+00\n",
      "Epoch: 6528 mean train loss:  2.56953062e-05, mean val. loss:  2.02497673e+00\n",
      "Epoch: 6529 mean train loss:  2.64958362e-05, mean val. loss:  2.02507830e+00\n",
      "Epoch: 6530 mean train loss:  2.63277325e-05, mean val. loss:  2.02515650e+00\n",
      "Epoch: 6531 mean train loss:  2.53634353e-05, mean val. loss:  2.02520633e+00\n",
      "Epoch: 6532 mean train loss:  2.60142260e-05, mean val. loss:  2.02524304e+00\n",
      "Epoch: 6533 mean train loss:  2.57577049e-05, mean val. loss:  2.02525067e+00\n",
      "Epoch: 6534 mean train loss:  2.51460297e-05, mean val. loss:  2.02526450e+00\n",
      "Epoch: 6535 mean train loss:  2.60749657e-05, mean val. loss:  2.02525115e+00\n",
      "Epoch: 6536 mean train loss:  2.57233623e-05, mean val. loss:  2.02522755e+00\n",
      "Epoch: 6537 mean train loss:  2.64071568e-05, mean val. loss:  2.02519083e+00\n",
      "Epoch: 6538 mean train loss:  2.55664927e-05, mean val. loss:  2.02515221e+00\n",
      "Epoch: 6539 mean train loss:  2.55477207e-05, mean val. loss:  2.02510738e+00\n",
      "Epoch: 6540 mean train loss:  2.59920489e-05, mean val. loss:  2.02506351e+00\n",
      "Epoch: 6541 mean train loss:  2.58070650e-05, mean val. loss:  2.02502298e+00\n",
      "Epoch: 6542 mean train loss:  2.54350016e-05, mean val. loss:  2.02499175e+00\n",
      "Epoch: 6543 mean train loss:  2.57132051e-05, mean val. loss:  2.02497172e+00\n",
      "Epoch: 6544 mean train loss:  2.53932085e-05, mean val. loss:  2.02496767e+00\n",
      "Epoch: 6545 mean train loss:  2.55399500e-05, mean val. loss:  2.02496266e+00\n",
      "Epoch: 6546 mean train loss:  2.56068015e-05, mean val. loss:  2.02498221e+00\n",
      "Epoch: 6547 mean train loss:  2.60304660e-05, mean val. loss:  2.02498817e+00\n",
      "Epoch: 6548 mean train loss:  2.54508050e-05, mean val. loss:  2.02500868e+00\n",
      "Epoch: 6549 mean train loss:  2.54920451e-05, mean val. loss:  2.02505994e+00\n",
      "Epoch: 6550 mean train loss:  2.52905884e-05, mean val. loss:  2.02512646e+00\n",
      "Epoch: 6551 mean train loss:  2.50262674e-05, mean val. loss:  2.02520800e+00\n",
      "Epoch: 6552 mean train loss:  2.58792134e-05, mean val. loss:  2.02531624e+00\n",
      "Epoch: 6553 mean train loss:  2.58414075e-05, mean val. loss:  2.02543306e+00\n",
      "Epoch: 6554 mean train loss:  2.56214407e-05, mean val. loss:  2.02555275e+00\n",
      "Epoch: 6555 mean train loss:  2.57712090e-05, mean val. loss:  2.02568054e+00\n",
      "Epoch: 6556 mean train loss:  2.56716739e-05, mean val. loss:  2.02582955e+00\n",
      "Epoch: 6557 mean train loss:  2.61044770e-05, mean val. loss:  2.02598619e+00\n",
      "Epoch: 6558 mean train loss:  2.58041546e-05, mean val. loss:  2.02613688e+00\n",
      "Epoch: 6559 mean train loss:  2.56973144e-05, mean val. loss:  2.02627110e+00\n",
      "Epoch: 6560 mean train loss:  2.63134716e-05, mean val. loss:  2.02640200e+00\n",
      "Epoch: 6561 mean train loss:  2.58205109e-05, mean val. loss:  2.02651501e+00\n",
      "Epoch: 6562 mean train loss:  2.56775238e-05, mean val. loss:  2.02662897e+00\n",
      "Epoch: 6563 mean train loss:  2.60261295e-05, mean val. loss:  2.02670860e+00\n",
      "Epoch: 6564 mean train loss:  2.54872721e-05, mean val. loss:  2.02678275e+00\n",
      "Epoch: 6565 mean train loss:  2.55837804e-05, mean val. loss:  2.02683592e+00\n",
      "Epoch: 6566 mean train loss:  2.56191415e-05, mean val. loss:  2.02686405e+00\n",
      "Epoch: 6567 mean train loss:  2.56877393e-05, mean val. loss:  2.02686906e+00\n",
      "Epoch: 6568 mean train loss:  2.50297016e-05, mean val. loss:  2.02685475e+00\n",
      "Epoch: 6569 mean train loss:  2.53223698e-05, mean val. loss:  2.02684736e+00\n",
      "Epoch: 6570 mean train loss:  2.57922220e-05, mean val. loss:  2.02683043e+00\n",
      "Epoch: 6571 mean train loss:  2.57577631e-05, mean val. loss:  2.02678752e+00\n",
      "Epoch: 6572 mean train loss:  2.59862863e-05, mean val. loss:  2.02672529e+00\n",
      "Epoch: 6573 mean train loss:  2.63537804e-05, mean val. loss:  2.02664185e+00\n",
      "Epoch: 6574 mean train loss:  2.54068000e-05, mean val. loss:  2.02656245e+00\n",
      "Epoch: 6575 mean train loss:  2.56542698e-05, mean val. loss:  2.02648735e+00\n",
      "Epoch: 6576 mean train loss:  2.64013652e-05, mean val. loss:  2.02640224e+00\n",
      "Epoch: 6577 mean train loss:  2.56178319e-05, mean val. loss:  2.02633238e+00\n",
      "Epoch: 6578 mean train loss:  2.56270578e-05, mean val. loss:  2.02625036e+00\n",
      "Epoch: 6579 mean train loss:  2.54626502e-05, mean val. loss:  2.02618146e+00\n",
      "Epoch: 6580 mean train loss:  2.52996979e-05, mean val. loss:  2.02612472e+00\n",
      "Epoch: 6581 mean train loss:  2.58451910e-05, mean val. loss:  2.02607417e+00\n",
      "Epoch: 6582 mean train loss:  2.56602943e-05, mean val. loss:  2.02603674e+00\n",
      "Epoch: 6583 mean train loss:  2.55004852e-05, mean val. loss:  2.02600980e+00\n",
      "Epoch: 6584 mean train loss:  2.61266250e-05, mean val. loss:  2.02597713e+00\n",
      "Epoch: 6585 mean train loss:  2.58314249e-05, mean val. loss:  2.02596068e+00\n",
      "Epoch: 6586 mean train loss:  2.49514706e-05, mean val. loss:  2.02596211e+00\n",
      "Epoch: 6587 mean train loss:  2.58435030e-05, mean val. loss:  2.02597380e+00\n",
      "Epoch: 6588 mean train loss:  2.53368344e-05, mean val. loss:  2.02600145e+00\n",
      "Epoch: 6589 mean train loss:  2.56157073e-05, mean val. loss:  2.02604437e+00\n",
      "Epoch: 6590 mean train loss:  2.62838148e-05, mean val. loss:  2.02606416e+00\n",
      "Epoch: 6591 mean train loss:  2.54448387e-05, mean val. loss:  2.02610350e+00\n",
      "Epoch: 6592 mean train loss:  2.57503416e-05, mean val. loss:  2.02613878e+00\n",
      "Epoch: 6593 mean train loss:  2.59425433e-05, mean val. loss:  2.02617717e+00\n",
      "Epoch: 6594 mean train loss:  2.58332875e-05, mean val. loss:  2.02621603e+00\n",
      "Epoch: 6595 mean train loss:  2.52399768e-05, mean val. loss:  2.02625418e+00\n",
      "Epoch: 6596 mean train loss:  2.60864035e-05, mean val. loss:  2.02628946e+00\n",
      "Epoch: 6597 mean train loss:  2.59720837e-05, mean val. loss:  2.02631474e+00\n",
      "Epoch: 6598 mean train loss:  2.54609622e-05, mean val. loss:  2.02634215e+00\n",
      "Epoch: 6599 mean train loss:  2.58957734e-05, mean val. loss:  2.02635026e+00\n",
      "Epoch: 6600 mean train loss:  2.52874452e-05, mean val. loss:  2.02635980e+00\n",
      "Epoch: 6601 mean train loss:  2.58203654e-05, mean val. loss:  2.02636361e+00\n",
      "Epoch: 6602 mean train loss:  2.51824385e-05, mean val. loss:  2.02635908e+00\n",
      "Epoch: 6603 mean train loss:  2.55006889e-05, mean val. loss:  2.02636051e+00\n",
      "Epoch: 6604 mean train loss:  2.57443171e-05, mean val. loss:  2.02636170e+00\n",
      "Epoch: 6605 mean train loss:  2.55263876e-05, mean val. loss:  2.02636385e+00\n",
      "Epoch: 6606 mean train loss:  2.53775506e-05, mean val. loss:  2.02636504e+00\n",
      "Epoch: 6607 mean train loss:  2.52920145e-05, mean val. loss:  2.02637386e+00\n",
      "Epoch: 6608 mean train loss:  2.60712986e-05, mean val. loss:  2.02637744e+00\n",
      "Epoch: 6609 mean train loss:  2.56541534e-05, mean val. loss:  2.02638435e+00\n",
      "Epoch: 6610 mean train loss:  2.59835506e-05, mean val. loss:  2.02639508e+00\n",
      "Epoch: 6611 mean train loss:  2.52879690e-05, mean val. loss:  2.02640820e+00\n",
      "Epoch: 6612 mean train loss:  2.60733941e-05, mean val. loss:  2.02643418e+00\n",
      "Epoch: 6613 mean train loss:  2.58197833e-05, mean val. loss:  2.02648211e+00\n",
      "Epoch: 6614 mean train loss:  2.54839542e-05, mean val. loss:  2.02653313e+00\n",
      "Epoch: 6615 mean train loss:  2.56505155e-05, mean val. loss:  2.02659369e+00\n",
      "Epoch: 6616 mean train loss:  2.53401522e-05, mean val. loss:  2.02667689e+00\n",
      "Epoch: 6617 mean train loss:  2.58274085e-05, mean val. loss:  2.02675438e+00\n",
      "Epoch: 6618 mean train loss:  2.61712412e-05, mean val. loss:  2.02681875e+00\n",
      "Epoch: 6619 mean train loss:  2.57302308e-05, mean val. loss:  2.02687860e+00\n",
      "Epoch: 6620 mean train loss:  2.58264481e-05, mean val. loss:  2.02693629e+00\n",
      "Epoch: 6621 mean train loss:  2.56945787e-05, mean val. loss:  2.02698755e+00\n",
      "Epoch: 6622 mean train loss:  2.54234765e-05, mean val. loss:  2.02705336e+00\n",
      "Epoch: 6623 mean train loss:  2.59886729e-05, mean val. loss:  2.02711654e+00\n",
      "Epoch: 6624 mean train loss:  2.56625644e-05, mean val. loss:  2.02717733e+00\n",
      "Epoch: 6625 mean train loss:  2.56402709e-05, mean val. loss:  2.02724075e+00\n",
      "Epoch: 6626 mean train loss:  2.58888467e-05, mean val. loss:  2.02729201e+00\n",
      "Epoch: 6627 mean train loss:  2.53057806e-05, mean val. loss:  2.02734113e+00\n",
      "Epoch: 6628 mean train loss:  2.58612272e-05, mean val. loss:  2.02738094e+00\n",
      "Epoch: 6629 mean train loss:  2.53178296e-05, mean val. loss:  2.02740240e+00\n",
      "Epoch: 6630 mean train loss:  2.50428275e-05, mean val. loss:  2.02743840e+00\n",
      "Epoch: 6631 mean train loss:  2.56054918e-05, mean val. loss:  2.02744246e+00\n",
      "Epoch: 6632 mean train loss:  2.51045276e-05, mean val. loss:  2.02744627e+00\n",
      "Epoch: 6633 mean train loss:  2.54082843e-05, mean val. loss:  2.02745247e+00\n",
      "Epoch: 6634 mean train loss:  2.50833982e-05, mean val. loss:  2.02746153e+00\n",
      "Epoch: 6635 mean train loss:  2.55522027e-05, mean val. loss:  2.02745223e+00\n",
      "Epoch: 6636 mean train loss:  2.57942011e-05, mean val. loss:  2.02743697e+00\n",
      "Epoch: 6637 mean train loss:  2.51739111e-05, mean val. loss:  2.02744389e+00\n",
      "Epoch: 6638 mean train loss:  2.52056052e-05, mean val. loss:  2.02747488e+00\n",
      "Epoch: 6639 mean train loss:  2.61664973e-05, mean val. loss:  2.02752233e+00\n",
      "Epoch: 6640 mean train loss:  2.54634069e-05, mean val. loss:  2.02756977e+00\n",
      "Epoch: 6641 mean train loss:  2.59342778e-05, mean val. loss:  2.02762699e+00\n",
      "Epoch: 6642 mean train loss:  2.56408239e-05, mean val. loss:  2.02768517e+00\n",
      "Epoch: 6643 mean train loss:  2.57744978e-05, mean val. loss:  2.02773976e+00\n",
      "Epoch: 6644 mean train loss:  2.59751396e-05, mean val. loss:  2.02778649e+00\n",
      "Epoch: 6645 mean train loss:  2.61013629e-05, mean val. loss:  2.02783179e+00\n",
      "Epoch: 6646 mean train loss:  2.55166087e-05, mean val. loss:  2.02789879e+00\n",
      "Epoch: 6647 mean train loss:  2.51493184e-05, mean val. loss:  2.02797389e+00\n",
      "Epoch: 6648 mean train loss:  2.53946637e-05, mean val. loss:  2.02806830e+00\n",
      "Epoch: 6649 mean train loss:  2.51660240e-05, mean val. loss:  2.02817893e+00\n",
      "Epoch: 6650 mean train loss:  2.51888996e-05, mean val. loss:  2.02827597e+00\n",
      "Epoch: 6651 mean train loss:  2.56871572e-05, mean val. loss:  2.02836251e+00\n",
      "Epoch: 6652 mean train loss:  2.49758596e-05, mean val. loss:  2.02847004e+00\n",
      "Epoch: 6653 mean train loss:  2.55109335e-05, mean val. loss:  2.02859020e+00\n",
      "Epoch: 6654 mean train loss:  2.55641935e-05, mean val. loss:  2.02873135e+00\n",
      "Epoch: 6655 mean train loss:  2.54319166e-05, mean val. loss:  2.02887511e+00\n",
      "Epoch: 6656 mean train loss:  2.51618039e-05, mean val. loss:  2.02902126e+00\n",
      "Epoch: 6657 mean train loss:  2.52258033e-05, mean val. loss:  2.02917194e+00\n",
      "Epoch: 6658 mean train loss:  2.57384963e-05, mean val. loss:  2.02930927e+00\n",
      "Epoch: 6659 mean train loss:  2.57469073e-05, mean val. loss:  2.02942204e+00\n",
      "Epoch: 6660 mean train loss:  2.58787768e-05, mean val. loss:  2.02952957e+00\n",
      "Epoch: 6661 mean train loss:  2.51018791e-05, mean val. loss:  2.02963758e+00\n",
      "Epoch: 6662 mean train loss:  2.53693725e-05, mean val. loss:  2.02973986e+00\n",
      "Epoch: 6663 mean train loss:  2.50972807e-05, mean val. loss:  2.02983904e+00\n",
      "Epoch: 6664 mean train loss:  2.57461797e-05, mean val. loss:  2.02994943e+00\n",
      "Epoch: 6665 mean train loss:  2.54275801e-05, mean val. loss:  2.03003931e+00\n",
      "Epoch: 6666 mean train loss:  2.61473469e-05, mean val. loss:  2.03009963e+00\n",
      "Epoch: 6667 mean train loss:  2.55657942e-05, mean val. loss:  2.03014565e+00\n",
      "Epoch: 6668 mean train loss:  2.58086657e-05, mean val. loss:  2.03017521e+00\n",
      "Epoch: 6669 mean train loss:  2.56912317e-05, mean val. loss:  2.03017616e+00\n",
      "Epoch: 6670 mean train loss:  2.55399791e-05, mean val. loss:  2.03015304e+00\n",
      "Epoch: 6671 mean train loss:  2.55135819e-05, mean val. loss:  2.03010178e+00\n",
      "Epoch: 6672 mean train loss:  2.51837773e-05, mean val. loss:  2.03004503e+00\n",
      "Epoch: 6673 mean train loss:  2.55808118e-05, mean val. loss:  2.02994990e+00\n",
      "Epoch: 6674 mean train loss:  2.53615435e-05, mean val. loss:  2.02984881e+00\n",
      "Epoch: 6675 mean train loss:  2.54207698e-05, mean val. loss:  2.02972102e+00\n",
      "Epoch: 6676 mean train loss:  2.50124431e-05, mean val. loss:  2.02958417e+00\n",
      "Epoch: 6677 mean train loss:  2.54086626e-05, mean val. loss:  2.02946305e+00\n",
      "Epoch: 6678 mean train loss:  2.57362262e-05, mean val. loss:  2.02933002e+00\n",
      "Epoch: 6679 mean train loss:  2.54171027e-05, mean val. loss:  2.02918530e+00\n",
      "Epoch: 6680 mean train loss:  2.52320606e-05, mean val. loss:  2.02905703e+00\n",
      "Epoch: 6681 mean train loss:  2.55859050e-05, mean val. loss:  2.02894521e+00\n",
      "Epoch: 6682 mean train loss:  2.50678568e-05, mean val. loss:  2.02884221e+00\n",
      "Epoch: 6683 mean train loss:  2.55332270e-05, mean val. loss:  2.02874923e+00\n",
      "Epoch: 6684 mean train loss:  2.56999338e-05, mean val. loss:  2.02865028e+00\n",
      "Epoch: 6685 mean train loss:  2.55427149e-05, mean val. loss:  2.02855968e+00\n",
      "Epoch: 6686 mean train loss:  2.50090670e-05, mean val. loss:  2.02850151e+00\n",
      "Epoch: 6687 mean train loss:  2.53884064e-05, mean val. loss:  2.02846408e+00\n",
      "Epoch: 6688 mean train loss:  2.59072403e-05, mean val. loss:  2.02842665e+00\n",
      "Epoch: 6689 mean train loss:  2.52204482e-05, mean val. loss:  2.02841568e+00\n",
      "Epoch: 6690 mean train loss:  2.53834587e-05, mean val. loss:  2.02843285e+00\n",
      "Epoch: 6691 mean train loss:  2.51416932e-05, mean val. loss:  2.02846766e+00\n",
      "Epoch: 6692 mean train loss:  2.48315919e-05, mean val. loss:  2.02850032e+00\n",
      "Epoch: 6693 mean train loss:  2.53601756e-05, mean val. loss:  2.02855515e+00\n",
      "Epoch: 6694 mean train loss:  2.55328778e-05, mean val. loss:  2.02860475e+00\n",
      "Epoch: 6695 mean train loss:  2.55335181e-05, mean val. loss:  2.02866578e+00\n",
      "Epoch: 6696 mean train loss:  2.58127984e-05, mean val. loss:  2.02870202e+00\n",
      "Epoch: 6697 mean train loss:  2.55339837e-05, mean val. loss:  2.02873039e+00\n",
      "Epoch: 6698 mean train loss:  2.50964658e-05, mean val. loss:  2.02876091e+00\n",
      "Epoch: 6699 mean train loss:  2.53660837e-05, mean val. loss:  2.02878714e+00\n",
      "Epoch: 6700 mean train loss:  2.57371867e-05, mean val. loss:  2.02881503e+00\n",
      "Epoch: 6701 mean train loss:  2.52571481e-05, mean val. loss:  2.02885628e+00\n",
      "Epoch: 6702 mean train loss:  2.54881161e-05, mean val. loss:  2.02887559e+00\n",
      "Epoch: 6703 mean train loss:  2.56373896e-05, mean val. loss:  2.02886224e+00\n",
      "Epoch: 6704 mean train loss:  2.53226026e-05, mean val. loss:  2.02886248e+00\n",
      "Epoch: 6705 mean train loss:  2.56185012e-05, mean val. loss:  2.02884698e+00\n",
      "Epoch: 6706 mean train loss:  2.48520228e-05, mean val. loss:  2.02883577e+00\n",
      "Epoch: 6707 mean train loss:  2.48783326e-05, mean val. loss:  2.02884483e+00\n",
      "Epoch: 6708 mean train loss:  2.49131117e-05, mean val. loss:  2.02884960e+00\n",
      "Epoch: 6709 mean train loss:  2.58331420e-05, mean val. loss:  2.02881718e+00\n",
      "Epoch: 6710 mean train loss:  2.52301106e-05, mean val. loss:  2.02878571e+00\n",
      "Epoch: 6711 mean train loss:  2.55985069e-05, mean val. loss:  2.02875304e+00\n",
      "Epoch: 6712 mean train loss:  2.55956838e-05, mean val. loss:  2.02871132e+00\n",
      "Epoch: 6713 mean train loss:  2.53731559e-05, mean val. loss:  2.02866960e+00\n",
      "Epoch: 6714 mean train loss:  2.56447820e-05, mean val. loss:  2.02863979e+00\n",
      "Epoch: 6715 mean train loss:  2.54682091e-05, mean val. loss:  2.02860451e+00\n",
      "Epoch: 6716 mean train loss:  2.48084834e-05, mean val. loss:  2.02859616e+00\n",
      "Epoch: 6717 mean train loss:  2.53484177e-05, mean val. loss:  2.02858233e+00\n",
      "Epoch: 6718 mean train loss:  2.53423059e-05, mean val. loss:  2.02857018e+00\n",
      "Epoch: 6719 mean train loss:  2.53085745e-05, mean val. loss:  2.02856350e+00\n",
      "Epoch: 6720 mean train loss:  2.58670771e-05, mean val. loss:  2.02856064e+00\n",
      "Epoch: 6721 mean train loss:  2.56970234e-05, mean val. loss:  2.02856779e+00\n",
      "Epoch: 6722 mean train loss:  2.57035717e-05, mean val. loss:  2.02858019e+00\n",
      "Epoch: 6723 mean train loss:  2.53803737e-05, mean val. loss:  2.02859139e+00\n",
      "Epoch: 6724 mean train loss:  2.48286524e-05, mean val. loss:  2.02861309e+00\n",
      "Epoch: 6725 mean train loss:  2.57733627e-05, mean val. loss:  2.02863026e+00\n",
      "Epoch: 6726 mean train loss:  2.50337762e-05, mean val. loss:  2.02864480e+00\n",
      "Epoch: 6727 mean train loss:  2.52415193e-05, mean val. loss:  2.02866006e+00\n",
      "Epoch: 6728 mean train loss:  2.54567130e-05, mean val. loss:  2.02864623e+00\n",
      "Epoch: 6729 mean train loss:  2.53806356e-05, mean val. loss:  2.02862167e+00\n",
      "Epoch: 6730 mean train loss:  2.54364568e-05, mean val. loss:  2.02859712e+00\n",
      "Epoch: 6731 mean train loss:  2.51584570e-05, mean val. loss:  2.02858448e+00\n",
      "Epoch: 6732 mean train loss:  2.53334874e-05, mean val. loss:  2.02855539e+00\n",
      "Epoch: 6733 mean train loss:  2.51465826e-05, mean val. loss:  2.02852750e+00\n",
      "Epoch: 6734 mean train loss:  2.53147155e-05, mean val. loss:  2.02848339e+00\n",
      "Epoch: 6735 mean train loss:  2.53832259e-05, mean val. loss:  2.02845573e+00\n",
      "Epoch: 6736 mean train loss:  2.55741761e-05, mean val. loss:  2.02842236e+00\n",
      "Epoch: 6737 mean train loss:  2.53444596e-05, mean val. loss:  2.02838469e+00\n",
      "Epoch: 6738 mean train loss:  2.53584585e-05, mean val. loss:  2.02833986e+00\n",
      "Epoch: 6739 mean train loss:  2.53239705e-05, mean val. loss:  2.02831197e+00\n",
      "Epoch: 6740 mean train loss:  2.52507161e-05, mean val. loss:  2.02827144e+00\n",
      "Epoch: 6741 mean train loss:  2.52974569e-05, mean val. loss:  2.02821398e+00\n",
      "Epoch: 6742 mean train loss:  2.54160259e-05, mean val. loss:  2.02816272e+00\n",
      "Epoch: 6743 mean train loss:  2.57006614e-05, mean val. loss:  2.02813220e+00\n",
      "Epoch: 6744 mean train loss:  2.54611368e-05, mean val. loss:  2.02812338e+00\n",
      "Epoch: 6745 mean train loss:  2.52300233e-05, mean val. loss:  2.02812290e+00\n",
      "Epoch: 6746 mean train loss:  2.51962338e-05, mean val. loss:  2.02812457e+00\n",
      "Epoch: 6747 mean train loss:  2.51363381e-05, mean val. loss:  2.02811646e+00\n",
      "Epoch: 6748 mean train loss:  2.49357545e-05, mean val. loss:  2.02813196e+00\n",
      "Epoch: 6749 mean train loss:  2.55537452e-05, mean val. loss:  2.02813148e+00\n",
      "Epoch: 6750 mean train loss:  2.55495834e-05, mean val. loss:  2.02813458e+00\n",
      "Epoch: 6751 mean train loss:  2.54703918e-05, mean val. loss:  2.02814579e+00\n",
      "Epoch: 6752 mean train loss:  2.50443991e-05, mean val. loss:  2.02818799e+00\n",
      "Epoch: 6753 mean train loss:  2.51300226e-05, mean val. loss:  2.02823758e+00\n",
      "Epoch: 6754 mean train loss:  2.52441969e-05, mean val. loss:  2.02830744e+00\n",
      "Epoch: 6755 mean train loss:  2.51995807e-05, mean val. loss:  2.02840447e+00\n",
      "Epoch: 6756 mean train loss:  2.49562145e-05, mean val. loss:  2.02849627e+00\n",
      "Epoch: 6757 mean train loss:  2.51837191e-05, mean val. loss:  2.02859974e+00\n",
      "Epoch: 6758 mean train loss:  2.48485594e-05, mean val. loss:  2.02871847e+00\n",
      "Epoch: 6759 mean train loss:  2.48162542e-05, mean val. loss:  2.02884674e+00\n",
      "Epoch: 6760 mean train loss:  2.51280726e-05, mean val. loss:  2.02898598e+00\n",
      "Epoch: 6761 mean train loss:  2.51377933e-05, mean val. loss:  2.02913857e+00\n",
      "Epoch: 6762 mean train loss:  2.49522855e-05, mean val. loss:  2.02930927e+00\n",
      "Epoch: 6763 mean train loss:  2.54331098e-05, mean val. loss:  2.02948856e+00\n",
      "Epoch: 6764 mean train loss:  2.53078761e-05, mean val. loss:  2.02967620e+00\n",
      "Epoch: 6765 mean train loss:  2.55420164e-05, mean val. loss:  2.02985287e+00\n",
      "Epoch: 6766 mean train loss:  2.51047022e-05, mean val. loss:  2.03002691e+00\n",
      "Epoch: 6767 mean train loss:  2.50319426e-05, mean val. loss:  2.03019834e+00\n",
      "Epoch: 6768 mean train loss:  2.50096200e-05, mean val. loss:  2.03038239e+00\n",
      "Epoch: 6769 mean train loss:  2.53499602e-05, mean val. loss:  2.03056717e+00\n",
      "Epoch: 6770 mean train loss:  2.57632346e-05, mean val. loss:  2.03073263e+00\n",
      "Epoch: 6771 mean train loss:  2.54553743e-05, mean val. loss:  2.03088641e+00\n",
      "Epoch: 6772 mean train loss:  2.55508348e-05, mean val. loss:  2.03101206e+00\n",
      "Epoch: 6773 mean train loss:  2.52969330e-05, mean val. loss:  2.03110647e+00\n",
      "Epoch: 6774 mean train loss:  2.55446939e-05, mean val. loss:  2.03116441e+00\n",
      "Epoch: 6775 mean train loss:  2.54457991e-05, mean val. loss:  2.03120327e+00\n",
      "Epoch: 6776 mean train loss:  2.45773699e-05, mean val. loss:  2.03123403e+00\n",
      "Epoch: 6777 mean train loss:  2.49581644e-05, mean val. loss:  2.03124857e+00\n",
      "Epoch: 6778 mean train loss:  2.48356373e-05, mean val. loss:  2.03124905e+00\n",
      "Epoch: 6779 mean train loss:  2.46135169e-05, mean val. loss:  2.03123784e+00\n",
      "Epoch: 6780 mean train loss:  2.54279585e-05, mean val. loss:  2.03121424e+00\n",
      "Epoch: 6781 mean train loss:  2.54022889e-05, mean val. loss:  2.03117061e+00\n",
      "Epoch: 6782 mean train loss:  2.52261816e-05, mean val. loss:  2.03112102e+00\n",
      "Epoch: 6783 mean train loss:  2.50914891e-05, mean val. loss:  2.03105283e+00\n",
      "Epoch: 6784 mean train loss:  2.53694016e-05, mean val. loss:  2.03099513e+00\n",
      "Epoch: 6785 mean train loss:  2.55798222e-05, mean val. loss:  2.03091955e+00\n",
      "Epoch: 6786 mean train loss:  2.54479237e-05, mean val. loss:  2.03082299e+00\n",
      "Epoch: 6787 mean train loss:  2.57491192e-05, mean val. loss:  2.03072500e+00\n",
      "Epoch: 6788 mean train loss:  2.43869727e-05, mean val. loss:  2.03063083e+00\n",
      "Epoch: 6789 mean train loss:  2.56681233e-05, mean val. loss:  2.03052521e+00\n",
      "Epoch: 6790 mean train loss:  2.54234765e-05, mean val. loss:  2.03042388e+00\n",
      "Epoch: 6791 mean train loss:  2.47300777e-05, mean val. loss:  2.03032660e+00\n",
      "Epoch: 6792 mean train loss:  2.51930323e-05, mean val. loss:  2.03020263e+00\n",
      "Epoch: 6793 mean train loss:  2.51520250e-05, mean val. loss:  2.03010392e+00\n",
      "Epoch: 6794 mean train loss:  2.55787745e-05, mean val. loss:  2.03000522e+00\n",
      "Epoch: 6795 mean train loss:  2.53596809e-05, mean val. loss:  2.02991605e+00\n",
      "Epoch: 6796 mean train loss:  2.47642165e-05, mean val. loss:  2.02981544e+00\n",
      "Epoch: 6797 mean train loss:  2.54376500e-05, mean val. loss:  2.02969670e+00\n",
      "Epoch: 6798 mean train loss:  2.51409947e-05, mean val. loss:  2.02959204e+00\n",
      "Epoch: 6799 mean train loss:  2.50002777e-05, mean val. loss:  2.02950191e+00\n",
      "Epoch: 6800 mean train loss:  2.49461737e-05, mean val. loss:  2.02940655e+00\n",
      "Epoch: 6801 mean train loss:  2.48592987e-05, mean val. loss:  2.02932167e+00\n",
      "Epoch: 6802 mean train loss:  2.51315651e-05, mean val. loss:  2.02922845e+00\n",
      "Epoch: 6803 mean train loss:  2.47772259e-05, mean val. loss:  2.02916241e+00\n",
      "Epoch: 6804 mean train loss:  2.47027783e-05, mean val. loss:  2.02913141e+00\n",
      "Epoch: 6805 mean train loss:  2.49920122e-05, mean val. loss:  2.02912331e+00\n",
      "Epoch: 6806 mean train loss:  2.50641024e-05, mean val. loss:  2.02911210e+00\n",
      "Epoch: 6807 mean train loss:  2.54230690e-05, mean val. loss:  2.02911878e+00\n",
      "Epoch: 6808 mean train loss:  2.53152684e-05, mean val. loss:  2.02914476e+00\n",
      "Epoch: 6809 mean train loss:  2.53894832e-05, mean val. loss:  2.02917552e+00\n",
      "Epoch: 6810 mean train loss:  2.49807490e-05, mean val. loss:  2.02923822e+00\n",
      "Epoch: 6811 mean train loss:  2.49137811e-05, mean val. loss:  2.02931333e+00\n",
      "Epoch: 6812 mean train loss:  2.54367478e-05, mean val. loss:  2.02941275e+00\n",
      "Epoch: 6813 mean train loss:  2.52127065e-05, mean val. loss:  2.02951264e+00\n",
      "Epoch: 6814 mean train loss:  2.51548772e-05, mean val. loss:  2.02959824e+00\n",
      "Epoch: 6815 mean train loss:  2.48662545e-05, mean val. loss:  2.02968907e+00\n",
      "Epoch: 6816 mean train loss:  2.47696298e-05, mean val. loss:  2.02978754e+00\n",
      "Epoch: 6817 mean train loss:  2.47240532e-05, mean val. loss:  2.02990627e+00\n",
      "Epoch: 6818 mean train loss:  2.48085416e-05, mean val. loss:  2.03001976e+00\n",
      "Epoch: 6819 mean train loss:  2.49117147e-05, mean val. loss:  2.03012180e+00\n",
      "Epoch: 6820 mean train loss:  2.49621226e-05, mean val. loss:  2.03023791e+00\n",
      "Epoch: 6821 mean train loss:  2.51726597e-05, mean val. loss:  2.03033066e+00\n",
      "Epoch: 6822 mean train loss:  2.56152707e-05, mean val. loss:  2.03040218e+00\n",
      "Epoch: 6823 mean train loss:  2.54201295e-05, mean val. loss:  2.03045821e+00\n",
      "Epoch: 6824 mean train loss:  2.55011255e-05, mean val. loss:  2.03048038e+00\n",
      "Epoch: 6825 mean train loss:  2.43634568e-05, mean val. loss:  2.03050089e+00\n",
      "Epoch: 6826 mean train loss:  2.48182914e-05, mean val. loss:  2.03052282e+00\n",
      "Epoch: 6827 mean train loss:  2.47273711e-05, mean val. loss:  2.03054667e+00\n",
      "Epoch: 6828 mean train loss:  2.46508571e-05, mean val. loss:  2.03056002e+00\n",
      "Epoch: 6829 mean train loss:  2.54090701e-05, mean val. loss:  2.03055358e+00\n",
      "Epoch: 6830 mean train loss:  2.43079849e-05, mean val. loss:  2.03057194e+00\n",
      "Epoch: 6831 mean train loss:  2.47210264e-05, mean val. loss:  2.03058362e+00\n",
      "Epoch: 6832 mean train loss:  2.53518228e-05, mean val. loss:  2.03059840e+00\n",
      "Epoch: 6833 mean train loss:  2.53479520e-05, mean val. loss:  2.03060603e+00\n",
      "Epoch: 6834 mean train loss:  2.48629949e-05, mean val. loss:  2.03059101e+00\n",
      "Epoch: 6835 mean train loss:  2.48537690e-05, mean val. loss:  2.03058076e+00\n",
      "Epoch: 6836 mean train loss:  2.51755409e-05, mean val. loss:  2.03056598e+00\n",
      "Epoch: 6837 mean train loss:  2.52665195e-05, mean val. loss:  2.03055191e+00\n",
      "Epoch: 6838 mean train loss:  2.47559219e-05, mean val. loss:  2.03055000e+00\n",
      "Epoch: 6839 mean train loss:  2.53376493e-05, mean val. loss:  2.03052092e+00\n",
      "Epoch: 6840 mean train loss:  2.49138393e-05, mean val. loss:  2.03049231e+00\n",
      "Epoch: 6841 mean train loss:  2.50187295e-05, mean val. loss:  2.03047371e+00\n",
      "Epoch: 6842 mean train loss:  2.53402686e-05, mean val. loss:  2.03044128e+00\n",
      "Epoch: 6843 mean train loss:  2.49462901e-05, mean val. loss:  2.03040719e+00\n",
      "Epoch: 6844 mean train loss:  2.46939308e-05, mean val. loss:  2.03037000e+00\n",
      "Epoch: 6845 mean train loss:  2.45083356e-05, mean val. loss:  2.03035593e+00\n",
      "Epoch: 6846 mean train loss:  2.48385768e-05, mean val. loss:  2.03033376e+00\n",
      "Epoch: 6847 mean train loss:  2.45878182e-05, mean val. loss:  2.03032613e+00\n",
      "Epoch: 6848 mean train loss:  2.48414581e-05, mean val. loss:  2.03032851e+00\n",
      "Epoch: 6849 mean train loss:  2.49594450e-05, mean val. loss:  2.03032112e+00\n",
      "Epoch: 6850 mean train loss:  2.51634046e-05, mean val. loss:  2.03029585e+00\n",
      "Epoch: 6851 mean train loss:  2.51565943e-05, mean val. loss:  2.03029203e+00\n",
      "Epoch: 6852 mean train loss:  2.52496393e-05, mean val. loss:  2.03027940e+00\n",
      "Epoch: 6853 mean train loss:  2.48684664e-05, mean val. loss:  2.03025532e+00\n",
      "Epoch: 6854 mean train loss:  2.49197474e-05, mean val. loss:  2.03022647e+00\n",
      "Epoch: 6855 mean train loss:  2.50504236e-05, mean val. loss:  2.03020024e+00\n",
      "Epoch: 6856 mean train loss:  2.47332209e-05, mean val. loss:  2.03017831e+00\n",
      "Epoch: 6857 mean train loss:  2.49375589e-05, mean val. loss:  2.03015733e+00\n",
      "Epoch: 6858 mean train loss:  2.47645949e-05, mean val. loss:  2.03017330e+00\n",
      "Epoch: 6859 mean train loss:  2.48137803e-05, mean val. loss:  2.03017187e+00\n",
      "Epoch: 6860 mean train loss:  2.48968136e-05, mean val. loss:  2.03017664e+00\n",
      "Epoch: 6861 mean train loss:  2.49773439e-05, mean val. loss:  2.03019166e+00\n",
      "Epoch: 6862 mean train loss:  2.47927383e-05, mean val. loss:  2.03021550e+00\n",
      "Epoch: 6863 mean train loss:  2.55094201e-05, mean val. loss:  2.03024149e+00\n",
      "Epoch: 6864 mean train loss:  2.49992299e-05, mean val. loss:  2.03026652e+00\n",
      "Epoch: 6865 mean train loss:  2.54579063e-05, mean val. loss:  2.03027582e+00\n",
      "Epoch: 6866 mean train loss:  2.53746402e-05, mean val. loss:  2.03027582e+00\n",
      "Epoch: 6867 mean train loss:  2.48238503e-05, mean val. loss:  2.03028345e+00\n",
      "Epoch: 6868 mean train loss:  2.51347374e-05, mean val. loss:  2.03029704e+00\n",
      "Epoch: 6869 mean train loss:  2.58750515e-05, mean val. loss:  2.03028274e+00\n",
      "Epoch: 6870 mean train loss:  2.49360164e-05, mean val. loss:  2.03024650e+00\n",
      "Epoch: 6871 mean train loss:  2.54517363e-05, mean val. loss:  2.03019643e+00\n",
      "Epoch: 6872 mean train loss:  2.46692507e-05, mean val. loss:  2.03015327e+00\n",
      "Epoch: 6873 mean train loss:  2.52539758e-05, mean val. loss:  2.03008580e+00\n",
      "Epoch: 6874 mean train loss:  2.46432028e-05, mean val. loss:  2.03001714e+00\n",
      "Epoch: 6875 mean train loss:  2.52200116e-05, mean val. loss:  2.02994442e+00\n",
      "Epoch: 6876 mean train loss:  2.45264382e-05, mean val. loss:  2.02989626e+00\n",
      "Epoch: 6877 mean train loss:  2.45521078e-05, mean val. loss:  2.02983999e+00\n",
      "Epoch: 6878 mean train loss:  2.52730679e-05, mean val. loss:  2.02979207e+00\n",
      "Epoch: 6879 mean train loss:  2.50499870e-05, mean val. loss:  2.02973723e+00\n",
      "Epoch: 6880 mean train loss:  2.46785348e-05, mean val. loss:  2.02969098e+00\n",
      "Epoch: 6881 mean train loss:  2.53616890e-05, mean val. loss:  2.02964330e+00\n",
      "Epoch: 6882 mean train loss:  2.52140453e-05, mean val. loss:  2.02960539e+00\n",
      "Epoch: 6883 mean train loss:  2.51623860e-05, mean val. loss:  2.02956128e+00\n",
      "Epoch: 6884 mean train loss:  2.54581682e-05, mean val. loss:  2.02949858e+00\n",
      "Epoch: 6885 mean train loss:  2.50595040e-05, mean val. loss:  2.02942610e+00\n",
      "Epoch: 6886 mean train loss:  2.53862527e-05, mean val. loss:  2.02936530e+00\n",
      "Epoch: 6887 mean train loss:  2.46070558e-05, mean val. loss:  2.02931428e+00\n",
      "Epoch: 6888 mean train loss:  2.44328985e-05, mean val. loss:  2.02930498e+00\n",
      "Epoch: 6889 mean train loss:  2.51788297e-05, mean val. loss:  2.02927327e+00\n",
      "Epoch: 6890 mean train loss:  2.50537414e-05, mean val. loss:  2.02923775e+00\n",
      "Epoch: 6891 mean train loss:  2.48433789e-05, mean val. loss:  2.02920246e+00\n",
      "Epoch: 6892 mean train loss:  2.45350238e-05, mean val. loss:  2.02917075e+00\n",
      "Epoch: 6893 mean train loss:  2.53905018e-05, mean val. loss:  2.02912307e+00\n",
      "Epoch: 6894 mean train loss:  2.45943083e-05, mean val. loss:  2.02907705e+00\n",
      "Epoch: 6895 mean train loss:  2.48906435e-05, mean val. loss:  2.02903962e+00\n",
      "Epoch: 6896 mean train loss:  2.50340381e-05, mean val. loss:  2.02899861e+00\n",
      "Epoch: 6897 mean train loss:  2.45978008e-05, mean val. loss:  2.02897859e+00\n",
      "Epoch: 6898 mean train loss:  2.50172743e-05, mean val. loss:  2.02894425e+00\n",
      "Epoch: 6899 mean train loss:  2.47307180e-05, mean val. loss:  2.02891493e+00\n",
      "Epoch: 6900 mean train loss:  2.53785402e-05, mean val. loss:  2.02886057e+00\n",
      "Epoch: 6901 mean train loss:  2.50510348e-05, mean val. loss:  2.02878523e+00\n",
      "Epoch: 6902 mean train loss:  2.51743186e-05, mean val. loss:  2.02871585e+00\n",
      "Epoch: 6903 mean train loss:  2.51238816e-05, mean val. loss:  2.02865529e+00\n",
      "Epoch: 6904 mean train loss:  2.46115960e-05, mean val. loss:  2.02858329e+00\n",
      "Epoch: 6905 mean train loss:  2.46098498e-05, mean val. loss:  2.02852726e+00\n",
      "Epoch: 6906 mean train loss:  2.48329598e-05, mean val. loss:  2.02848506e+00\n",
      "Epoch: 6907 mean train loss:  2.47712887e-05, mean val. loss:  2.02843070e+00\n",
      "Epoch: 6908 mean train loss:  2.48625001e-05, mean val. loss:  2.02837205e+00\n",
      "Epoch: 6909 mean train loss:  2.47504504e-05, mean val. loss:  2.02832174e+00\n",
      "Epoch: 6910 mean train loss:  2.53552571e-05, mean val. loss:  2.02825594e+00\n",
      "Epoch: 6911 mean train loss:  2.50062440e-05, mean val. loss:  2.02820802e+00\n",
      "Epoch: 6912 mean train loss:  2.49519071e-05, mean val. loss:  2.02817464e+00\n",
      "Epoch: 6913 mean train loss:  2.44378462e-05, mean val. loss:  2.02813721e+00\n",
      "Epoch: 6914 mean train loss:  2.52441387e-05, mean val. loss:  2.02809143e+00\n",
      "Epoch: 6915 mean train loss:  2.48482684e-05, mean val. loss:  2.02804732e+00\n",
      "Epoch: 6916 mean train loss:  2.50130543e-05, mean val. loss:  2.02800226e+00\n",
      "Epoch: 6917 mean train loss:  2.48176220e-05, mean val. loss:  2.02793264e+00\n",
      "Epoch: 6918 mean train loss:  2.53362232e-05, mean val. loss:  2.02786255e+00\n",
      "Epoch: 6919 mean train loss:  2.54937913e-05, mean val. loss:  2.02777839e+00\n",
      "Epoch: 6920 mean train loss:  2.50471639e-05, mean val. loss:  2.02768445e+00\n",
      "Epoch: 6921 mean train loss:  2.48987926e-05, mean val. loss:  2.02758169e+00\n",
      "Epoch: 6922 mean train loss:  2.51020247e-05, mean val. loss:  2.02748823e+00\n",
      "Epoch: 6923 mean train loss:  2.53961771e-05, mean val. loss:  2.02738547e+00\n",
      "Epoch: 6924 mean train loss:  2.49666919e-05, mean val. loss:  2.02728367e+00\n",
      "Epoch: 6925 mean train loss:  2.48328142e-05, mean val. loss:  2.02717638e+00\n",
      "Epoch: 6926 mean train loss:  2.52212049e-05, mean val. loss:  2.02707791e+00\n",
      "Epoch: 6927 mean train loss:  2.52079335e-05, mean val. loss:  2.02697968e+00\n",
      "Epoch: 6928 mean train loss:  2.48673896e-05, mean val. loss:  2.02686787e+00\n",
      "Epoch: 6929 mean train loss:  2.48517317e-05, mean val. loss:  2.02675152e+00\n",
      "Epoch: 6930 mean train loss:  2.50084559e-05, mean val. loss:  2.02663636e+00\n",
      "Epoch: 6931 mean train loss:  2.47521675e-05, mean val. loss:  2.02653909e+00\n",
      "Epoch: 6932 mean train loss:  2.46794370e-05, mean val. loss:  2.02643585e+00\n",
      "Epoch: 6933 mean train loss:  2.48493161e-05, mean val. loss:  2.02633429e+00\n",
      "Epoch: 6934 mean train loss:  2.45999836e-05, mean val. loss:  2.02624583e+00\n",
      "Epoch: 6935 mean train loss:  2.49758014e-05, mean val. loss:  2.02615690e+00\n",
      "Epoch: 6936 mean train loss:  2.48670694e-05, mean val. loss:  2.02607489e+00\n",
      "Epoch: 6937 mean train loss:  2.49836594e-05, mean val. loss:  2.02599406e+00\n",
      "Epoch: 6938 mean train loss:  2.48248689e-05, mean val. loss:  2.02591681e+00\n",
      "Epoch: 6939 mean train loss:  2.51770252e-05, mean val. loss:  2.02586389e+00\n",
      "Epoch: 6940 mean train loss:  2.52459722e-05, mean val. loss:  2.02581835e+00\n",
      "Epoch: 6941 mean train loss:  2.45179690e-05, mean val. loss:  2.02579188e+00\n",
      "Epoch: 6942 mean train loss:  2.48563010e-05, mean val. loss:  2.02575517e+00\n",
      "Epoch: 6943 mean train loss:  2.49357254e-05, mean val. loss:  2.02572155e+00\n",
      "Epoch: 6944 mean train loss:  2.47087155e-05, mean val. loss:  2.02571201e+00\n",
      "Epoch: 6945 mean train loss:  2.47376447e-05, mean val. loss:  2.02571201e+00\n",
      "Epoch: 6946 mean train loss:  2.49790319e-05, mean val. loss:  2.02572036e+00\n",
      "Epoch: 6947 mean train loss:  2.45094707e-05, mean val. loss:  2.02573776e+00\n",
      "Epoch: 6948 mean train loss:  2.47959979e-05, mean val. loss:  2.02575803e+00\n",
      "Epoch: 6949 mean train loss:  2.44975090e-05, mean val. loss:  2.02579045e+00\n",
      "Epoch: 6950 mean train loss:  2.49183213e-05, mean val. loss:  2.02580929e+00\n",
      "Epoch: 6951 mean train loss:  2.47135176e-05, mean val. loss:  2.02581143e+00\n",
      "Epoch: 6952 mean train loss:  2.43400282e-05, mean val. loss:  2.02584052e+00\n",
      "Epoch: 6953 mean train loss:  2.49221630e-05, mean val. loss:  2.02588797e+00\n",
      "Epoch: 6954 mean train loss:  2.47866847e-05, mean val. loss:  2.02595019e+00\n",
      "Epoch: 6955 mean train loss:  2.46083946e-05, mean val. loss:  2.02602291e+00\n",
      "Epoch: 6956 mean train loss:  2.53859616e-05, mean val. loss:  2.02607679e+00\n",
      "Epoch: 6957 mean train loss:  2.44862167e-05, mean val. loss:  2.02615094e+00\n",
      "Epoch: 6958 mean train loss:  2.51583697e-05, mean val. loss:  2.02620840e+00\n",
      "Epoch: 6959 mean train loss:  2.47119460e-05, mean val. loss:  2.02626109e+00\n",
      "Epoch: 6960 mean train loss:  2.42909882e-05, mean val. loss:  2.02634883e+00\n",
      "Epoch: 6961 mean train loss:  2.48454744e-05, mean val. loss:  2.02642894e+00\n",
      "Epoch: 6962 mean train loss:  2.49711738e-05, mean val. loss:  2.02648544e+00\n",
      "Epoch: 6963 mean train loss:  2.50848534e-05, mean val. loss:  2.02653694e+00\n",
      "Epoch: 6964 mean train loss:  2.45414558e-05, mean val. loss:  2.02657390e+00\n",
      "Epoch: 6965 mean train loss:  2.48889264e-05, mean val. loss:  2.02660537e+00\n",
      "Epoch: 6966 mean train loss:  2.44335097e-05, mean val. loss:  2.02664280e+00\n",
      "Epoch: 6967 mean train loss:  2.46504787e-05, mean val. loss:  2.02667642e+00\n",
      "Epoch: 6968 mean train loss:  2.48080760e-05, mean val. loss:  2.02671790e+00\n",
      "Epoch: 6969 mean train loss:  2.46720156e-05, mean val. loss:  2.02677703e+00\n",
      "Epoch: 6970 mean train loss:  2.52219324e-05, mean val. loss:  2.02683377e+00\n",
      "Epoch: 6971 mean train loss:  2.48737051e-05, mean val. loss:  2.02689457e+00\n",
      "Epoch: 6972 mean train loss:  2.49663426e-05, mean val. loss:  2.02694893e+00\n",
      "Epoch: 6973 mean train loss:  2.48184369e-05, mean val. loss:  2.02701616e+00\n",
      "Epoch: 6974 mean train loss:  2.43746617e-05, mean val. loss:  2.02710509e+00\n",
      "Epoch: 6975 mean train loss:  2.50375306e-05, mean val. loss:  2.02718472e+00\n",
      "Epoch: 6976 mean train loss:  2.50213488e-05, mean val. loss:  2.02725625e+00\n",
      "Epoch: 6977 mean train loss:  2.49206496e-05, mean val. loss:  2.02733207e+00\n",
      "Epoch: 6978 mean train loss:  2.47890130e-05, mean val. loss:  2.02741909e+00\n",
      "Epoch: 6979 mean train loss:  2.46640702e-05, mean val. loss:  2.02748299e+00\n",
      "Epoch: 6980 mean train loss:  2.54508632e-05, mean val. loss:  2.02751994e+00\n",
      "Epoch: 6981 mean train loss:  2.48606084e-05, mean val. loss:  2.02754402e+00\n",
      "Epoch: 6982 mean train loss:  2.50566227e-05, mean val. loss:  2.02753782e+00\n",
      "Epoch: 6983 mean train loss:  2.47597927e-05, mean val. loss:  2.02750683e+00\n",
      "Epoch: 6984 mean train loss:  2.54184997e-05, mean val. loss:  2.02746439e+00\n",
      "Epoch: 6985 mean train loss:  2.44817638e-05, mean val. loss:  2.02741361e+00\n",
      "Epoch: 6986 mean train loss:  2.50685262e-05, mean val. loss:  2.02733564e+00\n",
      "Epoch: 6987 mean train loss:  2.48188153e-05, mean val. loss:  2.02722287e+00\n",
      "Epoch: 6988 mean train loss:  2.45040574e-05, mean val. loss:  2.02710342e+00\n",
      "Epoch: 6989 mean train loss:  2.44213152e-05, mean val. loss:  2.02698016e+00\n",
      "Epoch: 6990 mean train loss:  2.44293187e-05, mean val. loss:  2.02685022e+00\n",
      "Epoch: 6991 mean train loss:  2.49060104e-05, mean val. loss:  2.02669525e+00\n",
      "Epoch: 6992 mean train loss:  2.51412857e-05, mean val. loss:  2.02651548e+00\n",
      "Epoch: 6993 mean train loss:  2.50224548e-05, mean val. loss:  2.02631879e+00\n",
      "Epoch: 6994 mean train loss:  2.44299590e-05, mean val. loss:  2.02612948e+00\n",
      "Epoch: 6995 mean train loss:  2.50356679e-05, mean val. loss:  2.02593398e+00\n",
      "Epoch: 6996 mean train loss:  2.48597062e-05, mean val. loss:  2.02574611e+00\n",
      "Epoch: 6997 mean train loss:  2.45655829e-05, mean val. loss:  2.02557039e+00\n",
      "Epoch: 6998 mean train loss:  2.45503907e-05, mean val. loss:  2.02541351e+00\n",
      "Epoch: 6999 mean train loss:  2.47019634e-05, mean val. loss:  2.02527356e+00\n",
      "Epoch: 7000 mean train loss:  2.50127923e-05, mean val. loss:  2.02513981e+00\n",
      "Epoch: 7001 mean train loss:  2.47807184e-05, mean val. loss:  2.02503753e+00\n",
      "Epoch: 7002 mean train loss:  2.51551392e-05, mean val. loss:  2.02491546e+00\n",
      "Epoch: 7003 mean train loss:  2.43586837e-05, mean val. loss:  2.02481318e+00\n",
      "Epoch: 7004 mean train loss:  2.46876734e-05, mean val. loss:  2.02472615e+00\n",
      "Epoch: 7005 mean train loss:  2.45783303e-05, mean val. loss:  2.02464747e+00\n",
      "Epoch: 7006 mean train loss:  2.46410782e-05, mean val. loss:  2.02458549e+00\n",
      "Epoch: 7007 mean train loss:  2.46122945e-05, mean val. loss:  2.02455568e+00\n",
      "Epoch: 7008 mean train loss:  2.47049611e-05, mean val. loss:  2.02452254e+00\n",
      "Epoch: 7009 mean train loss:  2.46725685e-05, mean val. loss:  2.02450299e+00\n",
      "Epoch: 7010 mean train loss:  2.48938741e-05, mean val. loss:  2.02450109e+00\n",
      "Epoch: 7011 mean train loss:  2.48943397e-05, mean val. loss:  2.02449536e+00\n",
      "Epoch: 7012 mean train loss:  2.46208801e-05, mean val. loss:  2.02451015e+00\n",
      "Epoch: 7013 mean train loss:  2.48297583e-05, mean val. loss:  2.02454066e+00\n",
      "Epoch: 7014 mean train loss:  2.46831332e-05, mean val. loss:  2.02456641e+00\n",
      "Epoch: 7015 mean train loss:  2.51480087e-05, mean val. loss:  2.02458787e+00\n",
      "Epoch: 7016 mean train loss:  2.50109879e-05, mean val. loss:  2.02459979e+00\n",
      "Epoch: 7017 mean train loss:  2.45429110e-05, mean val. loss:  2.02462459e+00\n",
      "Epoch: 7018 mean train loss:  2.47927965e-05, mean val. loss:  2.02464080e+00\n",
      "Epoch: 7019 mean train loss:  2.48126453e-05, mean val. loss:  2.02464795e+00\n",
      "Epoch: 7020 mean train loss:  2.49032455e-05, mean val. loss:  2.02464795e+00\n",
      "Epoch: 7021 mean train loss:  2.44473340e-05, mean val. loss:  2.02465296e+00\n",
      "Epoch: 7022 mean train loss:  2.49239965e-05, mean val. loss:  2.02466631e+00\n",
      "Epoch: 7023 mean train loss:  2.49724835e-05, mean val. loss:  2.02469277e+00\n",
      "Epoch: 7024 mean train loss:  2.44598486e-05, mean val. loss:  2.02472758e+00\n",
      "Epoch: 7025 mean train loss:  2.43994291e-05, mean val. loss:  2.02476764e+00\n",
      "Epoch: 7026 mean train loss:  2.48793513e-05, mean val. loss:  2.02479053e+00\n",
      "Epoch: 7027 mean train loss:  2.46274867e-05, mean val. loss:  2.02480602e+00\n",
      "Epoch: 7028 mean train loss:  2.46767886e-05, mean val. loss:  2.02482080e+00\n",
      "Epoch: 7029 mean train loss:  2.45915144e-05, mean val. loss:  2.02483559e+00\n",
      "Epoch: 7030 mean train loss:  2.47858115e-05, mean val. loss:  2.02485943e+00\n",
      "Epoch: 7031 mean train loss:  2.46950658e-05, mean val. loss:  2.02489471e+00\n",
      "Epoch: 7032 mean train loss:  2.46806303e-05, mean val. loss:  2.02493143e+00\n",
      "Epoch: 7033 mean train loss:  2.46504787e-05, mean val. loss:  2.02497292e+00\n",
      "Epoch: 7034 mean train loss:  2.44302209e-05, mean val. loss:  2.02498674e+00\n",
      "Epoch: 7035 mean train loss:  2.49175064e-05, mean val. loss:  2.02499843e+00\n",
      "Epoch: 7036 mean train loss:  2.47450953e-05, mean val. loss:  2.02501440e+00\n",
      "Epoch: 7037 mean train loss:  2.47786811e-05, mean val. loss:  2.02503371e+00\n",
      "Epoch: 7038 mean train loss:  2.47585704e-05, mean val. loss:  2.02500963e+00\n",
      "Epoch: 7039 mean train loss:  2.43827817e-05, mean val. loss:  2.02497625e+00\n",
      "Epoch: 7040 mean train loss:  2.47423886e-05, mean val. loss:  2.02492905e+00\n",
      "Epoch: 7041 mean train loss:  2.44502444e-05, mean val. loss:  2.02487898e+00\n",
      "Epoch: 7042 mean train loss:  2.48903234e-05, mean val. loss:  2.02479744e+00\n",
      "Epoch: 7043 mean train loss:  2.50810699e-05, mean val. loss:  2.02471519e+00\n",
      "Epoch: 7044 mean train loss:  2.51786259e-05, mean val. loss:  2.02461195e+00\n",
      "Epoch: 7045 mean train loss:  2.50725425e-05, mean val. loss:  2.02449441e+00\n",
      "Epoch: 7046 mean train loss:  2.46781856e-05, mean val. loss:  2.02437305e+00\n",
      "Epoch: 7047 mean train loss:  2.49203003e-05, mean val. loss:  2.02425671e+00\n",
      "Epoch: 7048 mean train loss:  2.47462885e-05, mean val. loss:  2.02413034e+00\n",
      "Epoch: 7049 mean train loss:  2.47505959e-05, mean val. loss:  2.02398586e+00\n",
      "Epoch: 7050 mean train loss:  2.44370895e-05, mean val. loss:  2.02383375e+00\n",
      "Epoch: 7051 mean train loss:  2.46968120e-05, mean val. loss:  2.02369022e+00\n",
      "Epoch: 7052 mean train loss:  2.47145072e-05, mean val. loss:  2.02354288e+00\n",
      "Epoch: 7053 mean train loss:  2.48313008e-05, mean val. loss:  2.02340388e+00\n",
      "Epoch: 7054 mean train loss:  2.49417790e-05, mean val. loss:  2.02326870e+00\n",
      "Epoch: 7055 mean train loss:  2.46761483e-05, mean val. loss:  2.02314401e+00\n",
      "Epoch: 7056 mean train loss:  2.42511451e-05, mean val. loss:  2.02303720e+00\n",
      "Epoch: 7057 mean train loss:  2.46897689e-05, mean val. loss:  2.02293205e+00\n",
      "Epoch: 7058 mean train loss:  2.48713186e-05, mean val. loss:  2.02285314e+00\n",
      "Epoch: 7059 mean train loss:  2.45258270e-05, mean val. loss:  2.02278876e+00\n",
      "Epoch: 7060 mean train loss:  2.44861585e-05, mean val. loss:  2.02274442e+00\n",
      "Epoch: 7061 mean train loss:  2.46943964e-05, mean val. loss:  2.02271557e+00\n",
      "Epoch: 7062 mean train loss:  2.48097349e-05, mean val. loss:  2.02270031e+00\n",
      "Epoch: 7063 mean train loss:  2.48255965e-05, mean val. loss:  2.02270961e+00\n",
      "Epoch: 7064 mean train loss:  2.45735282e-05, mean val. loss:  2.02272081e+00\n",
      "Epoch: 7065 mean train loss:  2.47558055e-05, mean val. loss:  2.02273679e+00\n",
      "Epoch: 7066 mean train loss:  2.43096438e-05, mean val. loss:  2.02279043e+00\n",
      "Epoch: 7067 mean train loss:  2.47948046e-05, mean val. loss:  2.02284694e+00\n",
      "Epoch: 7068 mean train loss:  2.47419230e-05, mean val. loss:  2.02290559e+00\n",
      "Epoch: 7069 mean train loss:  2.46922718e-05, mean val. loss:  2.02297378e+00\n",
      "Epoch: 7070 mean train loss:  2.47794087e-05, mean val. loss:  2.02304220e+00\n",
      "Epoch: 7071 mean train loss:  2.45920091e-05, mean val. loss:  2.02309775e+00\n",
      "Epoch: 7072 mean train loss:  2.50662561e-05, mean val. loss:  2.02312636e+00\n",
      "Epoch: 7073 mean train loss:  2.44797557e-05, mean val. loss:  2.02315521e+00\n",
      "Epoch: 7074 mean train loss:  2.48525757e-05, mean val. loss:  2.02316594e+00\n",
      "Epoch: 7075 mean train loss:  2.44462863e-05, mean val. loss:  2.02317834e+00\n",
      "Epoch: 7076 mean train loss:  2.46401469e-05, mean val. loss:  2.02318645e+00\n",
      "Epoch: 7077 mean train loss:  2.45968404e-05, mean val. loss:  2.02319002e+00\n",
      "Epoch: 7078 mean train loss:  2.47144781e-05, mean val. loss:  2.02318406e+00\n",
      "Epoch: 7079 mean train loss:  2.50244339e-05, mean val. loss:  2.02315807e+00\n",
      "Epoch: 7080 mean train loss:  2.50417506e-05, mean val. loss:  2.02312589e+00\n",
      "Epoch: 7081 mean train loss:  2.46640993e-05, mean val. loss:  2.02305841e+00\n",
      "Epoch: 7082 mean train loss:  2.44871771e-05, mean val. loss:  2.02298403e+00\n",
      "Epoch: 7083 mean train loss:  2.42335955e-05, mean val. loss:  2.02291608e+00\n",
      "Epoch: 7084 mean train loss:  2.46773125e-05, mean val. loss:  2.02281570e+00\n",
      "Epoch: 7085 mean train loss:  2.48021679e-05, mean val. loss:  2.02270508e+00\n",
      "Epoch: 7086 mean train loss:  2.40759691e-05, mean val. loss:  2.02262139e+00\n",
      "Epoch: 7087 mean train loss:  2.45544361e-05, mean val. loss:  2.02253270e+00\n",
      "Epoch: 7088 mean train loss:  2.43934919e-05, mean val. loss:  2.02246952e+00\n",
      "Epoch: 7089 mean train loss:  2.44247494e-05, mean val. loss:  2.02238822e+00\n",
      "Epoch: 7090 mean train loss:  2.43896211e-05, mean val. loss:  2.02234602e+00\n",
      "Epoch: 7091 mean train loss:  2.48938159e-05, mean val. loss:  2.02232504e+00\n",
      "Epoch: 7092 mean train loss:  2.49094737e-05, mean val. loss:  2.02233744e+00\n",
      "Epoch: 7093 mean train loss:  2.47101416e-05, mean val. loss:  2.02235794e+00\n",
      "Epoch: 7094 mean train loss:  2.46394484e-05, mean val. loss:  2.02238536e+00\n",
      "Epoch: 7095 mean train loss:  2.44131370e-05, mean val. loss:  2.02243567e+00\n",
      "Epoch: 7096 mean train loss:  2.45036499e-05, mean val. loss:  2.02250504e+00\n",
      "Epoch: 7097 mean train loss:  2.44187249e-05, mean val. loss:  2.02258515e+00\n",
      "Epoch: 7098 mean train loss:  2.40441877e-05, mean val. loss:  2.02269506e+00\n",
      "Epoch: 7099 mean train loss:  2.40775989e-05, mean val. loss:  2.02281332e+00\n",
      "Epoch: 7100 mean train loss:  2.43629038e-05, mean val. loss:  2.02292752e+00\n",
      "Epoch: 7101 mean train loss:  2.45689880e-05, mean val. loss:  2.02304292e+00\n",
      "Epoch: 7102 mean train loss:  2.45386036e-05, mean val. loss:  2.02314997e+00\n",
      "Epoch: 7103 mean train loss:  2.44326075e-05, mean val. loss:  2.02328849e+00\n",
      "Epoch: 7104 mean train loss:  2.47859280e-05, mean val. loss:  2.02339721e+00\n",
      "Epoch: 7105 mean train loss:  2.43377872e-05, mean val. loss:  2.02351546e+00\n",
      "Epoch: 7106 mean train loss:  2.47865100e-05, mean val. loss:  2.02362084e+00\n",
      "Epoch: 7107 mean train loss:  2.43762915e-05, mean val. loss:  2.02373505e+00\n",
      "Epoch: 7108 mean train loss:  2.43838294e-05, mean val. loss:  2.02382207e+00\n",
      "Epoch: 7109 mean train loss:  2.50962330e-05, mean val. loss:  2.02389264e+00\n",
      "Epoch: 7110 mean train loss:  2.44061812e-05, mean val. loss:  2.02396226e+00\n",
      "Epoch: 7111 mean train loss:  2.45398842e-05, mean val. loss:  2.02401423e+00\n",
      "Epoch: 7112 mean train loss:  2.47412827e-05, mean val. loss:  2.02402568e+00\n",
      "Epoch: 7113 mean train loss:  2.44507974e-05, mean val. loss:  2.02400827e+00\n",
      "Epoch: 7114 mean train loss:  2.44241091e-05, mean val. loss:  2.02397656e+00\n",
      "Epoch: 7115 mean train loss:  2.48452707e-05, mean val. loss:  2.02389407e+00\n",
      "Epoch: 7116 mean train loss:  2.48367724e-05, mean val. loss:  2.02380037e+00\n",
      "Epoch: 7117 mean train loss:  2.46584532e-05, mean val. loss:  2.02368259e+00\n",
      "Epoch: 7118 mean train loss:  2.45510018e-05, mean val. loss:  2.02354813e+00\n",
      "Epoch: 7119 mean train loss:  2.43207614e-05, mean val. loss:  2.02342510e+00\n",
      "Epoch: 7120 mean train loss:  2.49798759e-05, mean val. loss:  2.02327847e+00\n",
      "Epoch: 7121 mean train loss:  2.47893622e-05, mean val. loss:  2.02310801e+00\n",
      "Epoch: 7122 mean train loss:  2.44911062e-05, mean val. loss:  2.02293396e+00\n",
      "Epoch: 7123 mean train loss:  2.46723648e-05, mean val. loss:  2.02277327e+00\n",
      "Epoch: 7124 mean train loss:  2.44617113e-05, mean val. loss:  2.02260375e+00\n",
      "Epoch: 7125 mean train loss:  2.42796668e-05, mean val. loss:  2.02244067e+00\n",
      "Epoch: 7126 mean train loss:  2.47135758e-05, mean val. loss:  2.02227283e+00\n",
      "Epoch: 7127 mean train loss:  2.45695119e-05, mean val. loss:  2.02207375e+00\n",
      "Epoch: 7128 mean train loss:  2.44771072e-05, mean val. loss:  2.02189684e+00\n",
      "Epoch: 7129 mean train loss:  2.45249539e-05, mean val. loss:  2.02173495e+00\n",
      "Epoch: 7130 mean train loss:  2.43747490e-05, mean val. loss:  2.02157784e+00\n",
      "Epoch: 7131 mean train loss:  2.49083387e-05, mean val. loss:  2.02141142e+00\n",
      "Epoch: 7132 mean train loss:  2.47491989e-05, mean val. loss:  2.02126408e+00\n",
      "Epoch: 7133 mean train loss:  2.44565017e-05, mean val. loss:  2.02112246e+00\n",
      "Epoch: 7134 mean train loss:  2.48337165e-05, mean val. loss:  2.02098989e+00\n",
      "Epoch: 7135 mean train loss:  2.42220121e-05, mean val. loss:  2.02088165e+00\n",
      "Epoch: 7136 mean train loss:  2.45461124e-05, mean val. loss:  2.02077293e+00\n",
      "Epoch: 7137 mean train loss:  2.40048394e-05, mean val. loss:  2.02069926e+00\n",
      "Epoch: 7138 mean train loss:  2.47498974e-05, mean val. loss:  2.02063990e+00\n",
      "Epoch: 7139 mean train loss:  2.44787370e-05, mean val. loss:  2.02058649e+00\n",
      "Epoch: 7140 mean train loss:  2.44492549e-05, mean val. loss:  2.02055025e+00\n",
      "Epoch: 7141 mean train loss:  2.45378178e-05, mean val. loss:  2.02051687e+00\n",
      "Epoch: 7142 mean train loss:  2.45457340e-05, mean val. loss:  2.02048397e+00\n",
      "Epoch: 7143 mean train loss:  2.49647128e-05, mean val. loss:  2.02044225e+00\n",
      "Epoch: 7144 mean train loss:  2.42991373e-05, mean val. loss:  2.02041578e+00\n",
      "Epoch: 7145 mean train loss:  2.45108968e-05, mean val. loss:  2.02038002e+00\n",
      "Epoch: 7146 mean train loss:  2.42964015e-05, mean val. loss:  2.02036142e+00\n",
      "Epoch: 7147 mean train loss:  2.46530690e-05, mean val. loss:  2.02032495e+00\n",
      "Epoch: 7148 mean train loss:  2.44714320e-05, mean val. loss:  2.02027464e+00\n",
      "Epoch: 7149 mean train loss:  2.47148564e-05, mean val. loss:  2.02023983e+00\n",
      "Epoch: 7150 mean train loss:  2.44503026e-05, mean val. loss:  2.02021384e+00\n",
      "Epoch: 7151 mean train loss:  2.44165130e-05, mean val. loss:  2.02018356e+00\n",
      "Epoch: 7152 mean train loss:  2.46254203e-05, mean val. loss:  2.02015567e+00\n",
      "Epoch: 7153 mean train loss:  2.44986149e-05, mean val. loss:  2.02012658e+00\n",
      "Epoch: 7154 mean train loss:  2.43333925e-05, mean val. loss:  2.02009225e+00\n",
      "Epoch: 7155 mean train loss:  2.43849936e-05, mean val. loss:  2.02006221e+00\n",
      "Epoch: 7156 mean train loss:  2.40499794e-05, mean val. loss:  2.02006245e+00\n",
      "Epoch: 7157 mean train loss:  2.43254763e-05, mean val. loss:  2.02005076e+00\n",
      "Epoch: 7158 mean train loss:  2.44890689e-05, mean val. loss:  2.02003741e+00\n",
      "Epoch: 7159 mean train loss:  2.47262069e-05, mean val. loss:  2.02002716e+00\n",
      "Epoch: 7160 mean train loss:  2.42003589e-05, mean val. loss:  2.02001953e+00\n",
      "Epoch: 7161 mean train loss:  2.43358954e-05, mean val. loss:  2.02002192e+00\n",
      "Epoch: 7162 mean train loss:  2.45238771e-05, mean val. loss:  2.02002716e+00\n",
      "Epoch: 7163 mean train loss:  2.47696589e-05, mean val. loss:  2.02002048e+00\n",
      "Epoch: 7164 mean train loss:  2.42690148e-05, mean val. loss:  2.02003694e+00\n",
      "Epoch: 7165 mean train loss:  2.44835101e-05, mean val. loss:  2.02006316e+00\n",
      "Epoch: 7166 mean train loss:  2.46312120e-05, mean val. loss:  2.02007294e+00\n",
      "Epoch: 7167 mean train loss:  2.42529495e-05, mean val. loss:  2.02008605e+00\n",
      "Epoch: 7168 mean train loss:  2.44461698e-05, mean val. loss:  2.02009988e+00\n",
      "Epoch: 7169 mean train loss:  2.42346432e-05, mean val. loss:  2.02013946e+00\n",
      "Epoch: 7170 mean train loss:  2.46108393e-05, mean val. loss:  2.02018595e+00\n",
      "Epoch: 7171 mean train loss:  2.47590069e-05, mean val. loss:  2.02021003e+00\n",
      "Epoch: 7172 mean train loss:  2.48072611e-05, mean val. loss:  2.02023935e+00\n",
      "Epoch: 7173 mean train loss:  2.43482937e-05, mean val. loss:  2.02026105e+00\n",
      "Epoch: 7174 mean train loss:  2.47363350e-05, mean val. loss:  2.02026200e+00\n",
      "Epoch: 7175 mean train loss:  2.43313843e-05, mean val. loss:  2.02024508e+00\n",
      "Epoch: 7176 mean train loss:  2.48130236e-05, mean val. loss:  2.02021837e+00\n",
      "Epoch: 7177 mean train loss:  2.40234076e-05, mean val. loss:  2.02019119e+00\n",
      "Epoch: 7178 mean train loss:  2.46095296e-05, mean val. loss:  2.02014351e+00\n",
      "Epoch: 7179 mean train loss:  2.42411334e-05, mean val. loss:  2.02008390e+00\n",
      "Epoch: 7180 mean train loss:  2.45736446e-05, mean val. loss:  2.02001452e+00\n",
      "Epoch: 7181 mean train loss:  2.42684328e-05, mean val. loss:  2.01991343e+00\n",
      "Epoch: 7182 mean train loss:  2.47630524e-05, mean val. loss:  2.01978159e+00\n",
      "Epoch: 7183 mean train loss:  2.41973612e-05, mean val. loss:  2.01962423e+00\n",
      "Epoch: 7184 mean train loss:  2.46397103e-05, mean val. loss:  2.01944947e+00\n",
      "Epoch: 7185 mean train loss:  2.42783281e-05, mean val. loss:  2.01927304e+00\n",
      "Epoch: 7186 mean train loss:  2.43921240e-05, mean val. loss:  2.01910281e+00\n",
      "Epoch: 7187 mean train loss:  2.44541152e-05, mean val. loss:  2.01893950e+00\n",
      "Epoch: 7188 mean train loss:  2.43459363e-05, mean val. loss:  2.01876068e+00\n",
      "Epoch: 7189 mean train loss:  2.44723924e-05, mean val. loss:  2.01856303e+00\n",
      "Epoch: 7190 mean train loss:  2.45762349e-05, mean val. loss:  2.01836705e+00\n",
      "Epoch: 7191 mean train loss:  2.43636896e-05, mean val. loss:  2.01817608e+00\n",
      "Epoch: 7192 mean train loss:  2.42864480e-05, mean val. loss:  2.01802182e+00\n",
      "Epoch: 7193 mean train loss:  2.43725081e-05, mean val. loss:  2.01787162e+00\n",
      "Epoch: 7194 mean train loss:  2.44510302e-05, mean val. loss:  2.01772165e+00\n",
      "Epoch: 7195 mean train loss:  2.44105759e-05, mean val. loss:  2.01757669e+00\n",
      "Epoch: 7196 mean train loss:  2.41996022e-05, mean val. loss:  2.01745462e+00\n",
      "Epoch: 7197 mean train loss:  2.45466363e-05, mean val. loss:  2.01734447e+00\n",
      "Epoch: 7198 mean train loss:  2.44942785e-05, mean val. loss:  2.01726222e+00\n",
      "Epoch: 7199 mean train loss:  2.45312112e-05, mean val. loss:  2.01718569e+00\n",
      "Epoch: 7200 mean train loss:  2.41423259e-05, mean val. loss:  2.01712728e+00\n",
      "Epoch: 7201 mean train loss:  2.42867682e-05, mean val. loss:  2.01711631e+00\n",
      "Epoch: 7202 mean train loss:  2.41447997e-05, mean val. loss:  2.01712012e+00\n",
      "Epoch: 7203 mean train loss:  2.44584226e-05, mean val. loss:  2.01716185e+00\n",
      "Epoch: 7204 mean train loss:  2.41863891e-05, mean val. loss:  2.01722956e+00\n",
      "Epoch: 7205 mean train loss:  2.42853712e-05, mean val. loss:  2.01733303e+00\n",
      "Epoch: 7206 mean train loss:  2.44468974e-05, mean val. loss:  2.01745796e+00\n",
      "Epoch: 7207 mean train loss:  2.43260292e-05, mean val. loss:  2.01760316e+00\n",
      "Epoch: 7208 mean train loss:  2.42558599e-05, mean val. loss:  2.01774406e+00\n",
      "Epoch: 7209 mean train loss:  2.41801899e-05, mean val. loss:  2.01790214e+00\n",
      "Epoch: 7210 mean train loss:  2.44809198e-05, mean val. loss:  2.01806140e+00\n",
      "Epoch: 7211 mean train loss:  2.46587733e-05, mean val. loss:  2.01820683e+00\n",
      "Epoch: 7212 mean train loss:  2.44829862e-05, mean val. loss:  2.01834559e+00\n",
      "Epoch: 7213 mean train loss:  2.40595546e-05, mean val. loss:  2.01848435e+00\n",
      "Epoch: 7214 mean train loss:  2.44971889e-05, mean val. loss:  2.01857615e+00\n",
      "Epoch: 7215 mean train loss:  2.47001008e-05, mean val. loss:  2.01866269e+00\n",
      "Epoch: 7216 mean train loss:  2.41791131e-05, mean val. loss:  2.01874924e+00\n",
      "Epoch: 7217 mean train loss:  2.46280688e-05, mean val. loss:  2.01881790e+00\n",
      "Epoch: 7218 mean train loss:  2.44491966e-05, mean val. loss:  2.01885414e+00\n",
      "Epoch: 7219 mean train loss:  2.44119437e-05, mean val. loss:  2.01885366e+00\n",
      "Epoch: 7220 mean train loss:  2.42413080e-05, mean val. loss:  2.01884246e+00\n",
      "Epoch: 7221 mean train loss:  2.45682022e-05, mean val. loss:  2.01880598e+00\n",
      "Epoch: 7222 mean train loss:  2.44474504e-05, mean val. loss:  2.01874423e+00\n",
      "Epoch: 7223 mean train loss:  2.45131378e-05, mean val. loss:  2.01868033e+00\n",
      "Epoch: 7224 mean train loss:  2.44634866e-05, mean val. loss:  2.01860452e+00\n",
      "Epoch: 7225 mean train loss:  2.41616508e-05, mean val. loss:  2.01852584e+00\n",
      "Epoch: 7226 mean train loss:  2.40623776e-05, mean val. loss:  2.01843858e+00\n",
      "Epoch: 7227 mean train loss:  2.43097893e-05, mean val. loss:  2.01833606e+00\n",
      "Epoch: 7228 mean train loss:  2.46862473e-05, mean val. loss:  2.01823854e+00\n",
      "Epoch: 7229 mean train loss:  2.46586278e-05, mean val. loss:  2.01813030e+00\n",
      "Epoch: 7230 mean train loss:  2.41253583e-05, mean val. loss:  2.01799750e+00\n",
      "Epoch: 7231 mean train loss:  2.42207607e-05, mean val. loss:  2.01787448e+00\n",
      "Epoch: 7232 mean train loss:  2.46362470e-05, mean val. loss:  2.01777720e+00\n",
      "Epoch: 7233 mean train loss:  2.41328089e-05, mean val. loss:  2.01767230e+00\n",
      "Epoch: 7234 mean train loss:  2.40972731e-05, mean val. loss:  2.01758242e+00\n",
      "Epoch: 7235 mean train loss:  2.41471862e-05, mean val. loss:  2.01749301e+00\n",
      "Epoch: 7236 mean train loss:  2.42962851e-05, mean val. loss:  2.01741934e+00\n",
      "Epoch: 7237 mean train loss:  2.41007074e-05, mean val. loss:  2.01735830e+00\n",
      "Epoch: 7238 mean train loss:  2.45878473e-05, mean val. loss:  2.01731133e+00\n",
      "Epoch: 7239 mean train loss:  2.43164250e-05, mean val. loss:  2.01724625e+00\n",
      "Epoch: 7240 mean train loss:  2.43313843e-05, mean val. loss:  2.01721096e+00\n",
      "Epoch: 7241 mean train loss:  2.44357798e-05, mean val. loss:  2.01718712e+00\n",
      "Epoch: 7242 mean train loss:  2.42156093e-05, mean val. loss:  2.01718497e+00\n",
      "Epoch: 7243 mean train loss:  2.44539988e-05, mean val. loss:  2.01718688e+00\n",
      "Epoch: 7244 mean train loss:  2.45688425e-05, mean val. loss:  2.01716018e+00\n",
      "Epoch: 7245 mean train loss:  2.44216062e-05, mean val. loss:  2.01714587e+00\n",
      "Epoch: 7246 mean train loss:  2.40490190e-05, mean val. loss:  2.01712179e+00\n",
      "Epoch: 7247 mean train loss:  2.40837398e-05, mean val. loss:  2.01709366e+00\n",
      "Epoch: 7248 mean train loss:  2.41091475e-05, mean val. loss:  2.01705360e+00\n",
      "Epoch: 7249 mean train loss:  2.43970426e-05, mean val. loss:  2.01702547e+00\n",
      "Epoch: 7250 mean train loss:  2.44314142e-05, mean val. loss:  2.01698637e+00\n",
      "Epoch: 7251 mean train loss:  2.43416289e-05, mean val. loss:  2.01691937e+00\n",
      "Epoch: 7252 mean train loss:  2.41598755e-05, mean val. loss:  2.01687169e+00\n",
      "Epoch: 7253 mean train loss:  2.44165130e-05, mean val. loss:  2.01681471e+00\n",
      "Epoch: 7254 mean train loss:  2.41095840e-05, mean val. loss:  2.01674223e+00\n",
      "Epoch: 7255 mean train loss:  2.44096154e-05, mean val. loss:  2.01665449e+00\n",
      "Epoch: 7256 mean train loss:  2.40608933e-05, mean val. loss:  2.01655126e+00\n",
      "Epoch: 7257 mean train loss:  2.46085110e-05, mean val. loss:  2.01644373e+00\n",
      "Epoch: 7258 mean train loss:  2.44588882e-05, mean val. loss:  2.01634097e+00\n",
      "Epoch: 7259 mean train loss:  2.41543457e-05, mean val. loss:  2.01625919e+00\n",
      "Epoch: 7260 mean train loss:  2.46522541e-05, mean val. loss:  2.01615405e+00\n",
      "Epoch: 7261 mean train loss:  2.44758558e-05, mean val. loss:  2.01603866e+00\n",
      "Epoch: 7262 mean train loss:  2.44161056e-05, mean val. loss:  2.01590276e+00\n",
      "Epoch: 7263 mean train loss:  2.43917748e-05, mean val. loss:  2.01576972e+00\n",
      "Epoch: 7264 mean train loss:  2.43865070e-05, mean val. loss:  2.01562428e+00\n",
      "Epoch: 7265 mean train loss:  2.42676178e-05, mean val. loss:  2.01548481e+00\n",
      "Epoch: 7266 mean train loss:  2.39250367e-05, mean val. loss:  2.01533604e+00\n",
      "Epoch: 7267 mean train loss:  2.43141258e-05, mean val. loss:  2.01517820e+00\n",
      "Epoch: 7268 mean train loss:  2.41418020e-05, mean val. loss:  2.01502347e+00\n",
      "Epoch: 7269 mean train loss:  2.39422661e-05, mean val. loss:  2.01487017e+00\n",
      "Epoch: 7270 mean train loss:  2.44414841e-05, mean val. loss:  2.01473165e+00\n",
      "Epoch: 7271 mean train loss:  2.44165713e-05, mean val. loss:  2.01460505e+00\n",
      "Epoch: 7272 mean train loss:  2.44638650e-05, mean val. loss:  2.01446962e+00\n",
      "Epoch: 7273 mean train loss:  2.43826944e-05, mean val. loss:  2.01435089e+00\n",
      "Epoch: 7274 mean train loss:  2.42392125e-05, mean val. loss:  2.01425934e+00\n",
      "Epoch: 7275 mean train loss:  2.43210525e-05, mean val. loss:  2.01417065e+00\n",
      "Epoch: 7276 mean train loss:  2.43276590e-05, mean val. loss:  2.01410508e+00\n",
      "Epoch: 7277 mean train loss:  2.41508824e-05, mean val. loss:  2.01405001e+00\n",
      "Epoch: 7278 mean train loss:  2.41088565e-05, mean val. loss:  2.01402116e+00\n",
      "Epoch: 7279 mean train loss:  2.42864480e-05, mean val. loss:  2.01398587e+00\n",
      "Epoch: 7280 mean train loss:  2.39862420e-05, mean val. loss:  2.01396871e+00\n",
      "Epoch: 7281 mean train loss:  2.45343253e-05, mean val. loss:  2.01395488e+00\n",
      "Epoch: 7282 mean train loss:  2.41017551e-05, mean val. loss:  2.01395750e+00\n",
      "Epoch: 7283 mean train loss:  2.42707611e-05, mean val. loss:  2.01397109e+00\n",
      "Epoch: 7284 mean train loss:  2.41740781e-05, mean val. loss:  2.01397800e+00\n",
      "Epoch: 7285 mean train loss:  2.41285015e-05, mean val. loss:  2.01398778e+00\n",
      "Epoch: 7286 mean train loss:  2.42356618e-05, mean val. loss:  2.01400113e+00\n",
      "Epoch: 7287 mean train loss:  2.39515794e-05, mean val. loss:  2.01403189e+00\n",
      "Epoch: 7288 mean train loss:  2.41103990e-05, mean val. loss:  2.01407671e+00\n",
      "Epoch: 7289 mean train loss:  2.42066453e-05, mean val. loss:  2.01411128e+00\n",
      "Epoch: 7290 mean train loss:  2.46237614e-05, mean val. loss:  2.01414108e+00\n",
      "Epoch: 7291 mean train loss:  2.43523391e-05, mean val. loss:  2.01418591e+00\n",
      "Epoch: 7292 mean train loss:  2.43579270e-05, mean val. loss:  2.01421738e+00\n",
      "Epoch: 7293 mean train loss:  2.42360693e-05, mean val. loss:  2.01424694e+00\n",
      "Epoch: 7294 mean train loss:  2.40252120e-05, mean val. loss:  2.01428437e+00\n",
      "Epoch: 7295 mean train loss:  2.38984940e-05, mean val. loss:  2.01435280e+00\n",
      "Epoch: 7296 mean train loss:  2.39686924e-05, mean val. loss:  2.01443195e+00\n",
      "Epoch: 7297 mean train loss:  2.39468063e-05, mean val. loss:  2.01452851e+00\n",
      "Epoch: 7298 mean train loss:  2.42118549e-05, mean val. loss:  2.01458788e+00\n",
      "Epoch: 7299 mean train loss:  2.41848757e-05, mean val. loss:  2.01465821e+00\n",
      "Epoch: 7300 mean train loss:  2.44834519e-05, mean val. loss:  2.01472759e+00\n",
      "Epoch: 7301 mean train loss:  2.41419184e-05, mean val. loss:  2.01479244e+00\n",
      "Epoch: 7302 mean train loss:  2.39847577e-05, mean val. loss:  2.01486087e+00\n",
      "Epoch: 7303 mean train loss:  2.42695678e-05, mean val. loss:  2.01489949e+00\n",
      "Epoch: 7304 mean train loss:  2.40659574e-05, mean val. loss:  2.01494455e+00\n",
      "Epoch: 7305 mean train loss:  2.43486720e-05, mean val. loss:  2.01499033e+00\n",
      "Epoch: 7306 mean train loss:  2.43443938e-05, mean val. loss:  2.01500177e+00\n",
      "Epoch: 7307 mean train loss:  2.43995746e-05, mean val. loss:  2.01499772e+00\n",
      "Epoch: 7308 mean train loss:  2.42797541e-05, mean val. loss:  2.01498294e+00\n",
      "Epoch: 7309 mean train loss:  2.40136578e-05, mean val. loss:  2.01494646e+00\n",
      "Epoch: 7310 mean train loss:  2.39057117e-05, mean val. loss:  2.01491070e+00\n",
      "Epoch: 7311 mean train loss:  2.42960523e-05, mean val. loss:  2.01483107e+00\n",
      "Epoch: 7312 mean train loss:  2.41638918e-05, mean val. loss:  2.01476479e+00\n",
      "Epoch: 7313 mean train loss:  2.39379588e-05, mean val. loss:  2.01469302e+00\n",
      "Epoch: 7314 mean train loss:  2.44378462e-05, mean val. loss:  2.01459432e+00\n",
      "Epoch: 7315 mean train loss:  2.43332470e-05, mean val. loss:  2.01447868e+00\n",
      "Epoch: 7316 mean train loss:  2.38503271e-05, mean val. loss:  2.01436424e+00\n",
      "Epoch: 7317 mean train loss:  2.40661029e-05, mean val. loss:  2.01422191e+00\n",
      "Epoch: 7318 mean train loss:  2.42457900e-05, mean val. loss:  2.01404929e+00\n",
      "Epoch: 7319 mean train loss:  2.41388043e-05, mean val. loss:  2.01388645e+00\n",
      "Epoch: 7320 mean train loss:  2.41066446e-05, mean val. loss:  2.01372147e+00\n",
      "Epoch: 7321 mean train loss:  2.40519294e-05, mean val. loss:  2.01356888e+00\n",
      "Epoch: 7322 mean train loss:  2.40957306e-05, mean val. loss:  2.01343226e+00\n",
      "Epoch: 7323 mean train loss:  2.46006530e-05, mean val. loss:  2.01331449e+00\n",
      "Epoch: 7324 mean train loss:  2.43686372e-05, mean val. loss:  2.01318836e+00\n",
      "Epoch: 7325 mean train loss:  2.42972455e-05, mean val. loss:  2.01305747e+00\n",
      "Epoch: 7326 mean train loss:  2.38606590e-05, mean val. loss:  2.01294303e+00\n",
      "Epoch: 7327 mean train loss:  2.41496018e-05, mean val. loss:  2.01286888e+00\n",
      "Epoch: 7328 mean train loss:  2.43922404e-05, mean val. loss:  2.01281333e+00\n",
      "Epoch: 7329 mean train loss:  2.41590315e-05, mean val. loss:  2.01273751e+00\n",
      "Epoch: 7330 mean train loss:  2.41711095e-05, mean val. loss:  2.01269650e+00\n",
      "Epoch: 7331 mean train loss:  2.41012604e-05, mean val. loss:  2.01263738e+00\n",
      "Epoch: 7332 mean train loss:  2.44390685e-05, mean val. loss:  2.01256037e+00\n",
      "Epoch: 7333 mean train loss:  2.40194786e-05, mean val. loss:  2.01250029e+00\n",
      "Epoch: 7334 mean train loss:  2.38391804e-05, mean val. loss:  2.01245046e+00\n",
      "Epoch: 7335 mean train loss:  2.40684894e-05, mean val. loss:  2.01238227e+00\n",
      "Epoch: 7336 mean train loss:  2.43403774e-05, mean val. loss:  2.01231241e+00\n",
      "Epoch: 7337 mean train loss:  2.42081878e-05, mean val. loss:  2.01226830e+00\n",
      "Epoch: 7338 mean train loss:  2.37774802e-05, mean val. loss:  2.01222992e+00\n",
      "Epoch: 7339 mean train loss:  2.39279470e-05, mean val. loss:  2.01219106e+00\n",
      "Epoch: 7340 mean train loss:  2.41514354e-05, mean val. loss:  2.01215267e+00\n",
      "Epoch: 7341 mean train loss:  2.41264352e-05, mean val. loss:  2.01210570e+00\n",
      "Epoch: 7342 mean train loss:  2.41603120e-05, mean val. loss:  2.01204348e+00\n",
      "Epoch: 7343 mean train loss:  2.39131332e-05, mean val. loss:  2.01203322e+00\n",
      "Epoch: 7344 mean train loss:  2.41693051e-05, mean val. loss:  2.01203251e+00\n",
      "Epoch: 7345 mean train loss:  2.39013461e-05, mean val. loss:  2.01205206e+00\n",
      "Epoch: 7346 mean train loss:  2.41489033e-05, mean val. loss:  2.01207781e+00\n",
      "Epoch: 7347 mean train loss:  2.37514323e-05, mean val. loss:  2.01211858e+00\n",
      "Epoch: 7348 mean train loss:  2.44589464e-05, mean val. loss:  2.01216245e+00\n",
      "Epoch: 7349 mean train loss:  2.42063543e-05, mean val. loss:  2.01225090e+00\n",
      "Epoch: 7350 mean train loss:  2.43745744e-05, mean val. loss:  2.01233244e+00\n",
      "Epoch: 7351 mean train loss:  2.39921501e-05, mean val. loss:  2.01242161e+00\n",
      "Epoch: 7352 mean train loss:  2.42035894e-05, mean val. loss:  2.01250291e+00\n",
      "Epoch: 7353 mean train loss:  2.42609531e-05, mean val. loss:  2.01258874e+00\n",
      "Epoch: 7354 mean train loss:  2.44202383e-05, mean val. loss:  2.01265860e+00\n",
      "Epoch: 7355 mean train loss:  2.36998894e-05, mean val. loss:  2.01275659e+00\n",
      "Epoch: 7356 mean train loss:  2.42704409e-05, mean val. loss:  2.01287651e+00\n",
      "Epoch: 7357 mean train loss:  2.43698596e-05, mean val. loss:  2.01297140e+00\n",
      "Epoch: 7358 mean train loss:  2.41588277e-05, mean val. loss:  2.01305962e+00\n",
      "Epoch: 7359 mean train loss:  2.42680835e-05, mean val. loss:  2.01312613e+00\n",
      "Epoch: 7360 mean train loss:  2.44236435e-05, mean val. loss:  2.01318383e+00\n",
      "Epoch: 7361 mean train loss:  2.41298694e-05, mean val. loss:  2.01324010e+00\n",
      "Epoch: 7362 mean train loss:  2.34460167e-05, mean val. loss:  2.01330090e+00\n",
      "Epoch: 7363 mean train loss:  2.40475347e-05, mean val. loss:  2.01332760e+00\n",
      "Epoch: 7364 mean train loss:  2.37364729e-05, mean val. loss:  2.01334381e+00\n",
      "Epoch: 7365 mean train loss:  2.42721871e-05, mean val. loss:  2.01335335e+00\n",
      "Epoch: 7366 mean train loss:  2.42754468e-05, mean val. loss:  2.01332211e+00\n",
      "Epoch: 7367 mean train loss:  2.38133362e-05, mean val. loss:  2.01326752e+00\n",
      "Epoch: 7368 mean train loss:  2.40458467e-05, mean val. loss:  2.01318812e+00\n",
      "Epoch: 7369 mean train loss:  2.37869681e-05, mean val. loss:  2.01310110e+00\n",
      "Epoch: 7370 mean train loss:  2.39879009e-05, mean val. loss:  2.01299667e+00\n",
      "Epoch: 7371 mean train loss:  2.41606904e-05, mean val. loss:  2.01287603e+00\n",
      "Epoch: 7372 mean train loss:  2.41158414e-05, mean val. loss:  2.01274180e+00\n",
      "Epoch: 7373 mean train loss:  2.46893323e-05, mean val. loss:  2.01257563e+00\n",
      "Epoch: 7374 mean train loss:  2.39604269e-05, mean val. loss:  2.01240087e+00\n",
      "Epoch: 7375 mean train loss:  2.44006515e-05, mean val. loss:  2.01222634e+00\n",
      "Epoch: 7376 mean train loss:  2.39430519e-05, mean val. loss:  2.01204157e+00\n",
      "Epoch: 7377 mean train loss:  2.36882479e-05, mean val. loss:  2.01186323e+00\n",
      "Epoch: 7378 mean train loss:  2.38880457e-05, mean val. loss:  2.01168919e+00\n",
      "Epoch: 7379 mean train loss:  2.37022177e-05, mean val. loss:  2.01150799e+00\n",
      "Epoch: 7380 mean train loss:  2.42224196e-05, mean val. loss:  2.01132631e+00\n",
      "Epoch: 7381 mean train loss:  2.35571933e-05, mean val. loss:  2.01118994e+00\n",
      "Epoch: 7382 mean train loss:  2.41314119e-05, mean val. loss:  2.01107597e+00\n",
      "Epoch: 7383 mean train loss:  2.37834465e-05, mean val. loss:  2.01095772e+00\n",
      "Epoch: 7384 mean train loss:  2.38869106e-05, mean val. loss:  2.01088285e+00\n",
      "Epoch: 7385 mean train loss:  2.42700917e-05, mean val. loss:  2.01081419e+00\n",
      "Epoch: 7386 mean train loss:  2.41246598e-05, mean val. loss:  2.01074719e+00\n",
      "Epoch: 7387 mean train loss:  2.39132205e-05, mean val. loss:  2.01072216e+00\n",
      "Epoch: 7388 mean train loss:  2.39689543e-05, mean val. loss:  2.01072335e+00\n",
      "Epoch: 7389 mean train loss:  2.40581285e-05, mean val. loss:  2.01071596e+00\n",
      "Epoch: 7390 mean train loss:  2.39052752e-05, mean val. loss:  2.01072621e+00\n",
      "Epoch: 7391 mean train loss:  2.40583904e-05, mean val. loss:  2.01072598e+00\n",
      "Epoch: 7392 mean train loss:  2.37229106e-05, mean val. loss:  2.01070189e+00\n",
      "Epoch: 7393 mean train loss:  2.41997186e-05, mean val. loss:  2.01067567e+00\n",
      "Epoch: 7394 mean train loss:  2.40096415e-05, mean val. loss:  2.01066542e+00\n",
      "Epoch: 7395 mean train loss:  2.42048409e-05, mean val. loss:  2.01065040e+00\n",
      "Epoch: 7396 mean train loss:  2.43395043e-05, mean val. loss:  2.01063395e+00\n",
      "Epoch: 7397 mean train loss:  2.39503279e-05, mean val. loss:  2.01061535e+00\n",
      "Epoch: 7398 mean train loss:  2.44031253e-05, mean val. loss:  2.01058316e+00\n",
      "Epoch: 7399 mean train loss:  2.38647335e-05, mean val. loss:  2.01055002e+00\n",
      "Epoch: 7400 mean train loss:  2.42154929e-05, mean val. loss:  2.01047134e+00\n",
      "Epoch: 7401 mean train loss:  2.38770735e-05, mean val. loss:  2.01036429e+00\n",
      "Epoch: 7402 mean train loss:  2.40640366e-05, mean val. loss:  2.01025295e+00\n",
      "Epoch: 7403 mean train loss:  2.39794899e-05, mean val. loss:  2.01010132e+00\n",
      "Epoch: 7404 mean train loss:  2.42024544e-05, mean val. loss:  2.00991774e+00\n",
      "Epoch: 7405 mean train loss:  2.38571083e-05, mean val. loss:  2.00975227e+00\n",
      "Epoch: 7406 mean train loss:  2.41523958e-05, mean val. loss:  2.00958562e+00\n",
      "Epoch: 7407 mean train loss:  2.43513496e-05, mean val. loss:  2.00942135e+00\n",
      "Epoch: 7408 mean train loss:  2.42805399e-05, mean val. loss:  2.00927901e+00\n",
      "Epoch: 7409 mean train loss:  2.43186369e-05, mean val. loss:  2.00914454e+00\n",
      "Epoch: 7410 mean train loss:  2.42026581e-05, mean val. loss:  2.00902224e+00\n",
      "Epoch: 7411 mean train loss:  2.41905800e-05, mean val. loss:  2.00890899e+00\n",
      "Epoch: 7412 mean train loss:  2.43654940e-05, mean val. loss:  2.00881076e+00\n",
      "Epoch: 7413 mean train loss:  2.41914240e-05, mean val. loss:  2.00871754e+00\n",
      "Epoch: 7414 mean train loss:  2.37795466e-05, mean val. loss:  2.00866389e+00\n",
      "Epoch: 7415 mean train loss:  2.42526585e-05, mean val. loss:  2.00864363e+00\n",
      "Epoch: 7416 mean train loss:  2.39585352e-05, mean val. loss:  2.00861406e+00\n",
      "Epoch: 7417 mean train loss:  2.38408975e-05, mean val. loss:  2.00860381e+00\n",
      "Epoch: 7418 mean train loss:  2.43193936e-05, mean val. loss:  2.00855136e+00\n",
      "Epoch: 7419 mean train loss:  2.42860697e-05, mean val. loss:  2.00853252e+00\n",
      "Epoch: 7420 mean train loss:  2.38356879e-05, mean val. loss:  2.00852466e+00\n",
      "Epoch: 7421 mean train loss:  2.43678223e-05, mean val. loss:  2.00852966e+00\n",
      "Epoch: 7422 mean train loss:  2.38327775e-05, mean val. loss:  2.00854111e+00\n",
      "Epoch: 7423 mean train loss:  2.42017850e-05, mean val. loss:  2.00856233e+00\n",
      "Epoch: 7424 mean train loss:  2.40228255e-05, mean val. loss:  2.00858331e+00\n",
      "Epoch: 7425 mean train loss:  2.37986969e-05, mean val. loss:  2.00858808e+00\n",
      "Epoch: 7426 mean train loss:  2.38329521e-05, mean val. loss:  2.00861168e+00\n",
      "Epoch: 7427 mean train loss:  2.40649097e-05, mean val. loss:  2.00863218e+00\n",
      "Epoch: 7428 mean train loss:  2.42369715e-05, mean val. loss:  2.00862908e+00\n",
      "Epoch: 7429 mean train loss:  2.40447698e-05, mean val. loss:  2.00866723e+00\n",
      "Epoch: 7430 mean train loss:  2.40757945e-05, mean val. loss:  2.00868750e+00\n",
      "Epoch: 7431 mean train loss:  2.43584509e-05, mean val. loss:  2.00868964e+00\n",
      "Epoch: 7432 mean train loss:  2.41457310e-05, mean val. loss:  2.00867605e+00\n",
      "Epoch: 7433 mean train loss:  2.37711065e-05, mean val. loss:  2.00863838e+00\n",
      "Epoch: 7434 mean train loss:  2.40870577e-05, mean val. loss:  2.00860190e+00\n",
      "Epoch: 7435 mean train loss:  2.38866487e-05, mean val. loss:  2.00856614e+00\n",
      "Epoch: 7436 mean train loss:  2.36385968e-05, mean val. loss:  2.00851560e+00\n",
      "Epoch: 7437 mean train loss:  2.40127556e-05, mean val. loss:  2.00845695e+00\n",
      "Epoch: 7438 mean train loss:  2.43325194e-05, mean val. loss:  2.00841022e+00\n",
      "Epoch: 7439 mean train loss:  2.40104855e-05, mean val. loss:  2.00835180e+00\n",
      "Epoch: 7440 mean train loss:  2.42964888e-05, mean val. loss:  2.00826740e+00\n",
      "Epoch: 7441 mean train loss:  2.39419169e-05, mean val. loss:  2.00817704e+00\n",
      "Epoch: 7442 mean train loss:  2.38454086e-05, mean val. loss:  2.00809240e+00\n",
      "Epoch: 7443 mean train loss:  2.38104258e-05, mean val. loss:  2.00800991e+00\n",
      "Epoch: 7444 mean train loss:  2.39594083e-05, mean val. loss:  2.00791883e+00\n",
      "Epoch: 7445 mean train loss:  2.38838838e-05, mean val. loss:  2.00782228e+00\n",
      "Epoch: 7446 mean train loss:  2.36386841e-05, mean val. loss:  2.00771904e+00\n",
      "Epoch: 7447 mean train loss:  2.37584172e-05, mean val. loss:  2.00763392e+00\n",
      "Epoch: 7448 mean train loss:  2.41110101e-05, mean val. loss:  2.00754952e+00\n",
      "Epoch: 7449 mean train loss:  2.38657522e-05, mean val. loss:  2.00746012e+00\n",
      "Epoch: 7450 mean train loss:  2.39655783e-05, mean val. loss:  2.00737858e+00\n",
      "Epoch: 7451 mean train loss:  2.43629038e-05, mean val. loss:  2.00727916e+00\n",
      "Epoch: 7452 mean train loss:  2.36939813e-05, mean val. loss:  2.00721192e+00\n",
      "Epoch: 7453 mean train loss:  2.38314969e-05, mean val. loss:  2.00714087e+00\n",
      "Epoch: 7454 mean train loss:  2.42333335e-05, mean val. loss:  2.00710440e+00\n",
      "Epoch: 7455 mean train loss:  2.39090587e-05, mean val. loss:  2.00707889e+00\n",
      "Epoch: 7456 mean train loss:  2.39475921e-05, mean val. loss:  2.00707507e+00\n",
      "Epoch: 7457 mean train loss:  2.39261135e-05, mean val. loss:  2.00708294e+00\n",
      "Epoch: 7458 mean train loss:  2.36330088e-05, mean val. loss:  2.00709867e+00\n",
      "Epoch: 7459 mean train loss:  2.38447683e-05, mean val. loss:  2.00709343e+00\n",
      "Epoch: 7460 mean train loss:  2.36140331e-05, mean val. loss:  2.00709414e+00\n",
      "Epoch: 7461 mean train loss:  2.39814690e-05, mean val. loss:  2.00711393e+00\n",
      "Epoch: 7462 mean train loss:  2.42538517e-05, mean val. loss:  2.00712228e+00\n",
      "Epoch: 7463 mean train loss:  2.38909270e-05, mean val. loss:  2.00713038e+00\n",
      "Epoch: 7464 mean train loss:  2.40023655e-05, mean val. loss:  2.00715184e+00\n",
      "Epoch: 7465 mean train loss:  2.37739296e-05, mean val. loss:  2.00715542e+00\n",
      "Epoch: 7466 mean train loss:  2.39599904e-05, mean val. loss:  2.00712657e+00\n",
      "Epoch: 7467 mean train loss:  2.39362125e-05, mean val. loss:  2.00709534e+00\n",
      "Epoch: 7468 mean train loss:  2.41814414e-05, mean val. loss:  2.00706172e+00\n",
      "Epoch: 7469 mean train loss:  2.36634223e-05, mean val. loss:  2.00703311e+00\n",
      "Epoch: 7470 mean train loss:  2.39117944e-05, mean val. loss:  2.00698781e+00\n",
      "Epoch: 7471 mean train loss:  2.41459347e-05, mean val. loss:  2.00692439e+00\n",
      "Epoch: 7472 mean train loss:  2.41836242e-05, mean val. loss:  2.00686049e+00\n",
      "Epoch: 7473 mean train loss:  2.39912188e-05, mean val. loss:  2.00678635e+00\n",
      "Epoch: 7474 mean train loss:  2.43958493e-05, mean val. loss:  2.00673270e+00\n",
      "Epoch: 7475 mean train loss:  2.39906658e-05, mean val. loss:  2.00667167e+00\n",
      "Epoch: 7476 mean train loss:  2.37524509e-05, mean val. loss:  2.00663209e+00\n",
      "Epoch: 7477 mean train loss:  2.38936045e-05, mean val. loss:  2.00659728e+00\n",
      "Epoch: 7478 mean train loss:  2.37899076e-05, mean val. loss:  2.00657344e+00\n",
      "Epoch: 7479 mean train loss:  2.38293142e-05, mean val. loss:  2.00656700e+00\n",
      "Epoch: 7480 mean train loss:  2.39153160e-05, mean val. loss:  2.00654101e+00\n",
      "Epoch: 7481 mean train loss:  2.37917702e-05, mean val. loss:  2.00652623e+00\n",
      "Epoch: 7482 mean train loss:  2.39774236e-05, mean val. loss:  2.00651765e+00\n",
      "Epoch: 7483 mean train loss:  2.40313530e-05, mean val. loss:  2.00649595e+00\n",
      "Epoch: 7484 mean train loss:  2.42973038e-05, mean val. loss:  2.00647569e+00\n",
      "Epoch: 7485 mean train loss:  2.36200867e-05, mean val. loss:  2.00646377e+00\n",
      "Epoch: 7486 mean train loss:  2.40525114e-05, mean val. loss:  2.00645018e+00\n",
      "Epoch: 7487 mean train loss:  2.39661313e-05, mean val. loss:  2.00640559e+00\n",
      "Epoch: 7488 mean train loss:  2.41826929e-05, mean val. loss:  2.00635624e+00\n",
      "Epoch: 7489 mean train loss:  2.37789645e-05, mean val. loss:  2.00629616e+00\n",
      "Epoch: 7490 mean train loss:  2.35375483e-05, mean val. loss:  2.00623441e+00\n",
      "Epoch: 7491 mean train loss:  2.39685178e-05, mean val. loss:  2.00618649e+00\n",
      "Epoch: 7492 mean train loss:  2.38764624e-05, mean val. loss:  2.00613141e+00\n",
      "Epoch: 7493 mean train loss:  2.38233479e-05, mean val. loss:  2.00606632e+00\n",
      "Epoch: 7494 mean train loss:  2.39566143e-05, mean val. loss:  2.00598884e+00\n",
      "Epoch: 7495 mean train loss:  2.38176435e-05, mean val. loss:  2.00592518e+00\n",
      "Epoch: 7496 mean train loss:  2.38463981e-05, mean val. loss:  2.00588059e+00\n",
      "Epoch: 7497 mean train loss:  2.38560024e-05, mean val. loss:  2.00582767e+00\n",
      "Epoch: 7498 mean train loss:  2.40155496e-05, mean val. loss:  2.00576663e+00\n",
      "Epoch: 7499 mean train loss:  2.39264336e-05, mean val. loss:  2.00568533e+00\n",
      "Epoch: 7500 mean train loss:  2.35252373e-05, mean val. loss:  2.00562906e+00\n",
      "Epoch: 7501 mean train loss:  2.36990163e-05, mean val. loss:  2.00559258e+00\n",
      "Epoch: 7502 mean train loss:  2.39017536e-05, mean val. loss:  2.00555515e+00\n",
      "Epoch: 7503 mean train loss:  2.39358633e-05, mean val. loss:  2.00551915e+00\n",
      "Epoch: 7504 mean train loss:  2.41653761e-05, mean val. loss:  2.00546288e+00\n",
      "Epoch: 7505 mean train loss:  2.40010268e-05, mean val. loss:  2.00540709e+00\n",
      "Epoch: 7506 mean train loss:  2.34988984e-05, mean val. loss:  2.00533938e+00\n",
      "Epoch: 7507 mean train loss:  2.38840585e-05, mean val. loss:  2.00527787e+00\n",
      "Epoch: 7508 mean train loss:  2.37358036e-05, mean val. loss:  2.00519562e+00\n",
      "Epoch: 7509 mean train loss:  2.37124623e-05, mean val. loss:  2.00513124e+00\n",
      "Epoch: 7510 mean train loss:  2.35606858e-05, mean val. loss:  2.00508404e+00\n",
      "Epoch: 7511 mean train loss:  2.34497420e-05, mean val. loss:  2.00505519e+00\n",
      "Epoch: 7512 mean train loss:  2.40608060e-05, mean val. loss:  2.00503469e+00\n",
      "Epoch: 7513 mean train loss:  2.40521622e-05, mean val. loss:  2.00501323e+00\n",
      "Epoch: 7514 mean train loss:  2.37588829e-05, mean val. loss:  2.00498915e+00\n",
      "Epoch: 7515 mean train loss:  2.41014059e-05, mean val. loss:  2.00496268e+00\n",
      "Epoch: 7516 mean train loss:  2.37511122e-05, mean val. loss:  2.00494719e+00\n",
      "Epoch: 7517 mean train loss:  2.39601359e-05, mean val. loss:  2.00493455e+00\n",
      "Epoch: 7518 mean train loss:  2.36726191e-05, mean val. loss:  2.00491261e+00\n",
      "Epoch: 7519 mean train loss:  2.38929351e-05, mean val. loss:  2.00490928e+00\n",
      "Epoch: 7520 mean train loss:  2.33338797e-05, mean val. loss:  2.00494337e+00\n",
      "Epoch: 7521 mean train loss:  2.37532950e-05, mean val. loss:  2.00498343e+00\n",
      "Epoch: 7522 mean train loss:  2.37981731e-05, mean val. loss:  2.00503516e+00\n",
      "Epoch: 7523 mean train loss:  2.37317290e-05, mean val. loss:  2.00510788e+00\n",
      "Epoch: 7524 mean train loss:  2.37324566e-05, mean val. loss:  2.00520444e+00\n",
      "Epoch: 7525 mean train loss:  2.36350170e-05, mean val. loss:  2.00532818e+00\n",
      "Epoch: 7526 mean train loss:  2.35360058e-05, mean val. loss:  2.00546861e+00\n",
      "Epoch: 7527 mean train loss:  2.38352804e-05, mean val. loss:  2.00558972e+00\n",
      "Epoch: 7528 mean train loss:  2.37237837e-05, mean val. loss:  2.00570226e+00\n",
      "Epoch: 7529 mean train loss:  2.34238687e-05, mean val. loss:  2.00579095e+00\n",
      "Epoch: 7530 mean train loss:  2.36284395e-05, mean val. loss:  2.00590801e+00\n",
      "Epoch: 7531 mean train loss:  2.38086213e-05, mean val. loss:  2.00600934e+00\n",
      "Epoch: 7532 mean train loss:  2.42121750e-05, mean val. loss:  2.00607872e+00\n",
      "Epoch: 7533 mean train loss:  2.37012573e-05, mean val. loss:  2.00613475e+00\n",
      "Epoch: 7534 mean train loss:  2.40849913e-05, mean val. loss:  2.00618815e+00\n",
      "Epoch: 7535 mean train loss:  2.36357155e-05, mean val. loss:  2.00622511e+00\n",
      "Epoch: 7536 mean train loss:  2.35381885e-05, mean val. loss:  2.00627136e+00\n",
      "Epoch: 7537 mean train loss:  2.39910732e-05, mean val. loss:  2.00631452e+00\n",
      "Epoch: 7538 mean train loss:  2.38385983e-05, mean val. loss:  2.00633335e+00\n",
      "Epoch: 7539 mean train loss:  2.40002410e-05, mean val. loss:  2.00630879e+00\n",
      "Epoch: 7540 mean train loss:  2.36291671e-05, mean val. loss:  2.00625801e+00\n",
      "Epoch: 7541 mean train loss:  2.39193905e-05, mean val. loss:  2.00620055e+00\n",
      "Epoch: 7542 mean train loss:  2.39176734e-05, mean val. loss:  2.00611210e+00\n",
      "Epoch: 7543 mean train loss:  2.38885114e-05, mean val. loss:  2.00601339e+00\n",
      "Epoch: 7544 mean train loss:  2.35891785e-05, mean val. loss:  2.00588918e+00\n",
      "Epoch: 7545 mean train loss:  2.36551277e-05, mean val. loss:  2.00573707e+00\n",
      "Epoch: 7546 mean train loss:  2.38727662e-05, mean val. loss:  2.00556517e+00\n",
      "Epoch: 7547 mean train loss:  2.35820189e-05, mean val. loss:  2.00539470e+00\n",
      "Epoch: 7548 mean train loss:  2.40002992e-05, mean val. loss:  2.00520682e+00\n",
      "Epoch: 7549 mean train loss:  2.35280022e-05, mean val. loss:  2.00504208e+00\n",
      "Epoch: 7550 mean train loss:  2.41578091e-05, mean val. loss:  2.00489044e+00\n",
      "Epoch: 7551 mean train loss:  2.38019566e-05, mean val. loss:  2.00473237e+00\n",
      "Epoch: 7552 mean train loss:  2.34490435e-05, mean val. loss:  2.00459361e+00\n",
      "Epoch: 7553 mean train loss:  2.39785004e-05, mean val. loss:  2.00447321e+00\n",
      "Epoch: 7554 mean train loss:  2.38287612e-05, mean val. loss:  2.00436091e+00\n",
      "Epoch: 7555 mean train loss:  2.33504106e-05, mean val. loss:  2.00426221e+00\n",
      "Epoch: 7556 mean train loss:  2.36756168e-05, mean val. loss:  2.00418162e+00\n",
      "Epoch: 7557 mean train loss:  2.39184592e-05, mean val. loss:  2.00410986e+00\n",
      "Epoch: 7558 mean train loss:  2.35901098e-05, mean val. loss:  2.00404644e+00\n",
      "Epoch: 7559 mean train loss:  2.35905463e-05, mean val. loss:  2.00399065e+00\n",
      "Epoch: 7560 mean train loss:  2.40084482e-05, mean val. loss:  2.00392747e+00\n",
      "Epoch: 7561 mean train loss:  2.38360662e-05, mean val. loss:  2.00385880e+00\n",
      "Epoch: 7562 mean train loss:  2.36030319e-05, mean val. loss:  2.00381827e+00\n",
      "Epoch: 7563 mean train loss:  2.35335028e-05, mean val. loss:  2.00379276e+00\n",
      "Epoch: 7564 mean train loss:  2.40721274e-05, mean val. loss:  2.00375652e+00\n",
      "Epoch: 7565 mean train loss:  2.40087975e-05, mean val. loss:  2.00371146e+00\n",
      "Epoch: 7566 mean train loss:  2.36332999e-05, mean val. loss:  2.00366187e+00\n",
      "Epoch: 7567 mean train loss:  2.38771318e-05, mean val. loss:  2.00358319e+00\n",
      "Epoch: 7568 mean train loss:  2.34475592e-05, mean val. loss:  2.00348496e+00\n",
      "Epoch: 7569 mean train loss:  2.39035289e-05, mean val. loss:  2.00340319e+00\n",
      "Epoch: 7570 mean train loss:  2.35316111e-05, mean val. loss:  2.00333762e+00\n",
      "Epoch: 7571 mean train loss:  2.36731139e-05, mean val. loss:  2.00327420e+00\n",
      "Epoch: 7572 mean train loss:  2.37325439e-05, mean val. loss:  2.00319409e+00\n",
      "Epoch: 7573 mean train loss:  2.39880173e-05, mean val. loss:  2.00311041e+00\n",
      "Epoch: 7574 mean train loss:  2.34169129e-05, mean val. loss:  2.00300884e+00\n",
      "Epoch: 7575 mean train loss:  2.36393826e-05, mean val. loss:  2.00292277e+00\n",
      "Epoch: 7576 mean train loss:  2.38422072e-05, mean val. loss:  2.00285101e+00\n",
      "Epoch: 7577 mean train loss:  2.36738415e-05, mean val. loss:  2.00277424e+00\n",
      "Epoch: 7578 mean train loss:  2.36689812e-05, mean val. loss:  2.00269151e+00\n",
      "Epoch: 7579 mean train loss:  2.37321656e-05, mean val. loss:  2.00259972e+00\n",
      "Epoch: 7580 mean train loss:  2.35071348e-05, mean val. loss:  2.00253272e+00\n",
      "Epoch: 7581 mean train loss:  2.36362685e-05, mean val. loss:  2.00244474e+00\n",
      "Epoch: 7582 mean train loss:  2.37103377e-05, mean val. loss:  2.00233722e+00\n",
      "Epoch: 7583 mean train loss:  2.36624910e-05, mean val. loss:  2.00221157e+00\n",
      "Epoch: 7584 mean train loss:  2.36211636e-05, mean val. loss:  2.00209594e+00\n",
      "Epoch: 7585 mean train loss:  2.37830391e-05, mean val. loss:  2.00200272e+00\n",
      "Epoch: 7586 mean train loss:  2.40991358e-05, mean val. loss:  2.00192404e+00\n",
      "Epoch: 7587 mean train loss:  2.42438982e-05, mean val. loss:  2.00182652e+00\n",
      "Epoch: 7588 mean train loss:  2.34289910e-05, mean val. loss:  2.00172496e+00\n",
      "Epoch: 7589 mean train loss:  2.33880128e-05, mean val. loss:  2.00163054e+00\n",
      "Epoch: 7590 mean train loss:  2.36646156e-05, mean val. loss:  2.00153136e+00\n",
      "Epoch: 7591 mean train loss:  2.36134510e-05, mean val. loss:  2.00143051e+00\n",
      "Epoch: 7592 mean train loss:  2.35296902e-05, mean val. loss:  2.00130463e+00\n",
      "Epoch: 7593 mean train loss:  2.38521316e-05, mean val. loss:  2.00116920e+00\n",
      "Epoch: 7594 mean train loss:  2.40999216e-05, mean val. loss:  2.00103831e+00\n",
      "Epoch: 7595 mean train loss:  2.37587374e-05, mean val. loss:  2.00094652e+00\n",
      "Epoch: 7596 mean train loss:  2.38007051e-05, mean val. loss:  2.00084901e+00\n",
      "Epoch: 7597 mean train loss:  2.40750378e-05, mean val. loss:  2.00075150e+00\n",
      "Epoch: 7598 mean train loss:  2.37044005e-05, mean val. loss:  2.00067711e+00\n",
      "Epoch: 7599 mean train loss:  2.36241904e-05, mean val. loss:  2.00063992e+00\n",
      "Epoch: 7600 mean train loss:  2.34702602e-05, mean val. loss:  2.00059986e+00\n",
      "Epoch: 7601 mean train loss:  2.37597269e-05, mean val. loss:  2.00057650e+00\n",
      "Epoch: 7602 mean train loss:  2.37108034e-05, mean val. loss:  2.00060034e+00\n",
      "Epoch: 7603 mean train loss:  2.35202024e-05, mean val. loss:  2.00063753e+00\n",
      "Epoch: 7604 mean train loss:  2.38713401e-05, mean val. loss:  2.00069022e+00\n",
      "Epoch: 7605 mean train loss:  2.38776847e-05, mean val. loss:  2.00074363e+00\n",
      "Epoch: 7606 mean train loss:  2.37360073e-05, mean val. loss:  2.00080776e+00\n",
      "Epoch: 7607 mean train loss:  2.34431354e-05, mean val. loss:  2.00086474e+00\n",
      "Epoch: 7608 mean train loss:  2.37804779e-05, mean val. loss:  2.00093818e+00\n",
      "Epoch: 7609 mean train loss:  2.38537905e-05, mean val. loss:  2.00098729e+00\n",
      "Epoch: 7610 mean train loss:  2.33540195e-05, mean val. loss:  2.00105977e+00\n",
      "Epoch: 7611 mean train loss:  2.38235807e-05, mean val. loss:  2.00113630e+00\n",
      "Epoch: 7612 mean train loss:  2.37958448e-05, mean val. loss:  2.00120616e+00\n",
      "Epoch: 7613 mean train loss:  2.37345230e-05, mean val. loss:  2.00124836e+00\n",
      "Epoch: 7614 mean train loss:  2.38185457e-05, mean val. loss:  2.00127983e+00\n",
      "Epoch: 7615 mean train loss:  2.36735796e-05, mean val. loss:  2.00131440e+00\n",
      "Epoch: 7616 mean train loss:  2.35665357e-05, mean val. loss:  2.00133848e+00\n",
      "Epoch: 7617 mean train loss:  2.32421153e-05, mean val. loss:  2.00131822e+00\n",
      "Epoch: 7618 mean train loss:  2.35497137e-05, mean val. loss:  2.00126481e+00\n",
      "Epoch: 7619 mean train loss:  2.39510846e-05, mean val. loss:  2.00117970e+00\n",
      "Epoch: 7620 mean train loss:  2.35762272e-05, mean val. loss:  2.00108624e+00\n",
      "Epoch: 7621 mean train loss:  2.38106295e-05, mean val. loss:  2.00099826e+00\n",
      "Epoch: 7622 mean train loss:  2.36530323e-05, mean val. loss:  2.00089216e+00\n",
      "Epoch: 7623 mean train loss:  2.38845241e-05, mean val. loss:  2.00078821e+00\n",
      "Epoch: 7624 mean train loss:  2.36872002e-05, mean val. loss:  2.00064707e+00\n",
      "Epoch: 7625 mean train loss:  2.36527121e-05, mean val. loss:  2.00049067e+00\n",
      "Epoch: 7626 mean train loss:  2.37813802e-05, mean val. loss:  2.00031090e+00\n",
      "Epoch: 7627 mean train loss:  2.37648492e-05, mean val. loss:  2.00012636e+00\n",
      "Epoch: 7628 mean train loss:  2.32912134e-05, mean val. loss:  1.99996436e+00\n",
      "Epoch: 7629 mean train loss:  2.31979357e-05, mean val. loss:  1.99980533e+00\n",
      "Epoch: 7630 mean train loss:  2.36787600e-05, mean val. loss:  1.99965918e+00\n",
      "Epoch: 7631 mean train loss:  2.37678178e-05, mean val. loss:  1.99951875e+00\n",
      "Epoch: 7632 mean train loss:  2.37407221e-05, mean val. loss:  1.99940932e+00\n",
      "Epoch: 7633 mean train loss:  2.35530315e-05, mean val. loss:  1.99934936e+00\n",
      "Epoch: 7634 mean train loss:  2.38836801e-05, mean val. loss:  1.99926376e+00\n",
      "Epoch: 7635 mean train loss:  2.38461944e-05, mean val. loss:  1.99917579e+00\n",
      "Epoch: 7636 mean train loss:  2.39413930e-05, mean val. loss:  1.99908471e+00\n",
      "Epoch: 7637 mean train loss:  2.34858599e-05, mean val. loss:  1.99899614e+00\n",
      "Epoch: 7638 mean train loss:  2.37375207e-05, mean val. loss:  1.99891603e+00\n",
      "Epoch: 7639 mean train loss:  2.37518689e-05, mean val. loss:  1.99885869e+00\n",
      "Epoch: 7640 mean train loss:  2.38164503e-05, mean val. loss:  1.99882770e+00\n",
      "Epoch: 7641 mean train loss:  2.36682827e-05, mean val. loss:  1.99881184e+00\n",
      "Epoch: 7642 mean train loss:  2.34285835e-05, mean val. loss:  1.99879992e+00\n",
      "Epoch: 7643 mean train loss:  2.38580978e-05, mean val. loss:  1.99878275e+00\n",
      "Epoch: 7644 mean train loss:  2.39164801e-05, mean val. loss:  1.99875319e+00\n",
      "Epoch: 7645 mean train loss:  2.34941836e-05, mean val. loss:  1.99873197e+00\n",
      "Epoch: 7646 mean train loss:  2.36442429e-05, mean val. loss:  1.99868667e+00\n",
      "Epoch: 7647 mean train loss:  2.35181360e-05, mean val. loss:  1.99864161e+00\n",
      "Epoch: 7648 mean train loss:  2.33043975e-05, mean val. loss:  1.99860132e+00\n",
      "Epoch: 7649 mean train loss:  2.38835055e-05, mean val. loss:  1.99856400e+00\n",
      "Epoch: 7650 mean train loss:  2.39586516e-05, mean val. loss:  1.99854386e+00\n",
      "Epoch: 7651 mean train loss:  2.36245105e-05, mean val. loss:  1.99850678e+00\n",
      "Epoch: 7652 mean train loss:  2.32723833e-05, mean val. loss:  1.99846840e+00\n",
      "Epoch: 7653 mean train loss:  2.32379825e-05, mean val. loss:  1.99843943e+00\n",
      "Epoch: 7654 mean train loss:  2.37257918e-05, mean val. loss:  1.99840450e+00\n",
      "Epoch: 7655 mean train loss:  2.40115041e-05, mean val. loss:  1.99834669e+00\n",
      "Epoch: 7656 mean train loss:  2.36865308e-05, mean val. loss:  1.99830902e+00\n",
      "Epoch: 7657 mean train loss:  2.35946500e-05, mean val. loss:  1.99828386e+00\n",
      "Epoch: 7658 mean train loss:  2.38663051e-05, mean val. loss:  1.99826491e+00\n",
      "Epoch: 7659 mean train loss:  2.35077168e-05, mean val. loss:  1.99825358e+00\n",
      "Epoch: 7660 mean train loss:  2.37426138e-05, mean val. loss:  1.99826670e+00\n",
      "Epoch: 7661 mean train loss:  2.36931082e-05, mean val. loss:  1.99829304e+00\n",
      "Epoch: 7662 mean train loss:  2.36633059e-05, mean val. loss:  1.99832809e+00\n",
      "Epoch: 7663 mean train loss:  2.40506197e-05, mean val. loss:  1.99834526e+00\n",
      "Epoch: 7664 mean train loss:  2.39160145e-05, mean val. loss:  1.99833918e+00\n",
      "Epoch: 7665 mean train loss:  2.33895553e-05, mean val. loss:  1.99835539e+00\n",
      "Epoch: 7666 mean train loss:  2.37864442e-05, mean val. loss:  1.99835527e+00\n",
      "Epoch: 7667 mean train loss:  2.34186300e-05, mean val. loss:  1.99836242e+00\n",
      "Epoch: 7668 mean train loss:  2.36406340e-05, mean val. loss:  1.99834740e+00\n",
      "Epoch: 7669 mean train loss:  2.37191853e-05, mean val. loss:  1.99835110e+00\n",
      "Epoch: 7670 mean train loss:  2.38263747e-05, mean val. loss:  1.99835837e+00\n",
      "Epoch: 7671 mean train loss:  2.34736654e-05, mean val. loss:  1.99837577e+00\n",
      "Epoch: 7672 mean train loss:  2.32986058e-05, mean val. loss:  1.99840033e+00\n",
      "Epoch: 7673 mean train loss:  2.36167689e-05, mean val. loss:  1.99841011e+00\n",
      "Epoch: 7674 mean train loss:  2.36715714e-05, mean val. loss:  1.99840367e+00\n",
      "Epoch: 7675 mean train loss:  2.31496524e-05, mean val. loss:  1.99839699e+00\n",
      "Epoch: 7676 mean train loss:  2.32779130e-05, mean val. loss:  1.99838388e+00\n",
      "Epoch: 7677 mean train loss:  2.34976178e-05, mean val. loss:  1.99841070e+00\n",
      "Epoch: 7678 mean train loss:  2.33713654e-05, mean val. loss:  1.99843872e+00\n",
      "Epoch: 7679 mean train loss:  2.33299506e-05, mean val. loss:  1.99849653e+00\n",
      "Epoch: 7680 mean train loss:  2.37311469e-05, mean val. loss:  1.99854553e+00\n",
      "Epoch: 7681 mean train loss:  2.35765183e-05, mean val. loss:  1.99862158e+00\n",
      "Epoch: 7682 mean train loss:  2.37685163e-05, mean val. loss:  1.99873888e+00\n",
      "Epoch: 7683 mean train loss:  2.37378408e-05, mean val. loss:  1.99882305e+00\n",
      "Epoch: 7684 mean train loss:  2.36140622e-05, mean val. loss:  1.99890018e+00\n",
      "Epoch: 7685 mean train loss:  2.34975014e-05, mean val. loss:  1.99898469e+00\n",
      "Epoch: 7686 mean train loss:  2.34412728e-05, mean val. loss:  1.99905646e+00\n",
      "Epoch: 7687 mean train loss:  2.32857710e-05, mean val. loss:  1.99913049e+00\n",
      "Epoch: 7688 mean train loss:  2.32837338e-05, mean val. loss:  1.99919987e+00\n",
      "Epoch: 7689 mean train loss:  2.37291679e-05, mean val. loss:  1.99927127e+00\n",
      "Epoch: 7690 mean train loss:  2.38509674e-05, mean val. loss:  1.99930549e+00\n",
      "Epoch: 7691 mean train loss:  2.32726161e-05, mean val. loss:  1.99934924e+00\n",
      "Epoch: 7692 mean train loss:  2.35221523e-05, mean val. loss:  1.99939060e+00\n",
      "Epoch: 7693 mean train loss:  2.35140324e-05, mean val. loss:  1.99942374e+00\n",
      "Epoch: 7694 mean train loss:  2.36932538e-05, mean val. loss:  1.99942112e+00\n",
      "Epoch: 7695 mean train loss:  2.36803025e-05, mean val. loss:  1.99940836e+00\n",
      "Epoch: 7696 mean train loss:  2.29822181e-05, mean val. loss:  1.99938047e+00\n",
      "Epoch: 7697 mean train loss:  2.34881300e-05, mean val. loss:  1.99936795e+00\n",
      "Epoch: 7698 mean train loss:  2.33243918e-05, mean val. loss:  1.99935174e+00\n",
      "Epoch: 7699 mean train loss:  2.38769280e-05, mean val. loss:  1.99929500e+00\n",
      "Epoch: 7700 mean train loss:  2.36760243e-05, mean val. loss:  1.99923575e+00\n",
      "Epoch: 7701 mean train loss:  2.33298051e-05, mean val. loss:  1.99918914e+00\n",
      "Epoch: 7702 mean train loss:  2.32692691e-05, mean val. loss:  1.99912179e+00\n",
      "Epoch: 7703 mean train loss:  2.34108884e-05, mean val. loss:  1.99903202e+00\n",
      "Epoch: 7704 mean train loss:  2.34109466e-05, mean val. loss:  1.99894643e+00\n",
      "Epoch: 7705 mean train loss:  2.30820733e-05, mean val. loss:  1.99884665e+00\n",
      "Epoch: 7706 mean train loss:  2.37475906e-05, mean val. loss:  1.99879861e+00\n",
      "Epoch: 7707 mean train loss:  2.38492503e-05, mean val. loss:  1.99876153e+00\n",
      "Epoch: 7708 mean train loss:  2.30814330e-05, mean val. loss:  1.99873900e+00\n",
      "Epoch: 7709 mean train loss:  2.33909814e-05, mean val. loss:  1.99871945e+00\n",
      "Epoch: 7710 mean train loss:  2.35508487e-05, mean val. loss:  1.99872792e+00\n",
      "Epoch: 7711 mean train loss:  2.33601895e-05, mean val. loss:  1.99872291e+00\n",
      "Epoch: 7712 mean train loss:  2.34636536e-05, mean val. loss:  1.99873781e+00\n",
      "Epoch: 7713 mean train loss:  2.34626932e-05, mean val. loss:  1.99875224e+00\n",
      "Epoch: 7714 mean train loss:  2.32790480e-05, mean val. loss:  1.99879813e+00\n",
      "Epoch: 7715 mean train loss:  2.33161263e-05, mean val. loss:  1.99883616e+00\n",
      "Epoch: 7716 mean train loss:  2.34509935e-05, mean val. loss:  1.99888134e+00\n",
      "Epoch: 7717 mean train loss:  2.33786413e-05, mean val. loss:  1.99893272e+00\n",
      "Epoch: 7718 mean train loss:  2.36311171e-05, mean val. loss:  1.99898899e+00\n",
      "Epoch: 7719 mean train loss:  2.34692707e-05, mean val. loss:  1.99904811e+00\n",
      "Epoch: 7720 mean train loss:  2.33004685e-05, mean val. loss:  1.99911714e+00\n",
      "Epoch: 7721 mean train loss:  2.31719750e-05, mean val. loss:  1.99919379e+00\n",
      "Epoch: 7722 mean train loss:  2.32745369e-05, mean val. loss:  1.99926877e+00\n",
      "Epoch: 7723 mean train loss:  2.32025923e-05, mean val. loss:  1.99935198e+00\n",
      "Epoch: 7724 mean train loss:  2.32523307e-05, mean val. loss:  1.99943566e+00\n",
      "Epoch: 7725 mean train loss:  2.36042251e-05, mean val. loss:  1.99948442e+00\n",
      "Epoch: 7726 mean train loss:  2.33401079e-05, mean val. loss:  1.99951601e+00\n",
      "Epoch: 7727 mean train loss:  2.36218038e-05, mean val. loss:  1.99954391e+00\n",
      "Epoch: 7728 mean train loss:  2.38181674e-05, mean val. loss:  1.99953234e+00\n",
      "Epoch: 7729 mean train loss:  2.34892359e-05, mean val. loss:  1.99951077e+00\n",
      "Epoch: 7730 mean train loss:  2.30353908e-05, mean val. loss:  1.99946690e+00\n",
      "Epoch: 7731 mean train loss:  2.36314663e-05, mean val. loss:  1.99938822e+00\n",
      "Epoch: 7732 mean train loss:  2.37488130e-05, mean val. loss:  1.99927759e+00\n",
      "Epoch: 7733 mean train loss:  2.34853069e-05, mean val. loss:  1.99915910e+00\n",
      "Epoch: 7734 mean train loss:  2.35684274e-05, mean val. loss:  1.99903357e+00\n",
      "Epoch: 7735 mean train loss:  2.35101033e-05, mean val. loss:  1.99887967e+00\n",
      "Epoch: 7736 mean train loss:  2.34510226e-05, mean val. loss:  1.99873090e+00\n",
      "Epoch: 7737 mean train loss:  2.36440101e-05, mean val. loss:  1.99853051e+00\n",
      "Epoch: 7738 mean train loss:  2.29026773e-05, mean val. loss:  1.99832320e+00\n",
      "Epoch: 7739 mean train loss:  2.36807682e-05, mean val. loss:  1.99810779e+00\n",
      "Epoch: 7740 mean train loss:  2.31713057e-05, mean val. loss:  1.99788558e+00\n",
      "Epoch: 7741 mean train loss:  2.35710177e-05, mean val. loss:  1.99765551e+00\n",
      "Epoch: 7742 mean train loss:  2.34537001e-05, mean val. loss:  1.99743807e+00\n",
      "Epoch: 7743 mean train loss:  2.35154002e-05, mean val. loss:  1.99725342e+00\n",
      "Epoch: 7744 mean train loss:  2.34401668e-05, mean val. loss:  1.99707270e+00\n",
      "Epoch: 7745 mean train loss:  2.35360640e-05, mean val. loss:  1.99688709e+00\n",
      "Epoch: 7746 mean train loss:  2.35946500e-05, mean val. loss:  1.99671781e+00\n",
      "Epoch: 7747 mean train loss:  2.35080370e-05, mean val. loss:  1.99657154e+00\n",
      "Epoch: 7748 mean train loss:  2.34620820e-05, mean val. loss:  1.99641752e+00\n",
      "Epoch: 7749 mean train loss:  2.36408960e-05, mean val. loss:  1.99629951e+00\n",
      "Epoch: 7750 mean train loss:  2.33880710e-05, mean val. loss:  1.99620950e+00\n",
      "Epoch: 7751 mean train loss:  2.29764846e-05, mean val. loss:  1.99613118e+00\n",
      "Epoch: 7752 mean train loss:  2.34368490e-05, mean val. loss:  1.99605513e+00\n",
      "Epoch: 7753 mean train loss:  2.37247441e-05, mean val. loss:  1.99597788e+00\n",
      "Epoch: 7754 mean train loss:  2.37486965e-05, mean val. loss:  1.99592066e+00\n",
      "Epoch: 7755 mean train loss:  2.33668252e-05, mean val. loss:  1.99587190e+00\n",
      "Epoch: 7756 mean train loss:  2.37268396e-05, mean val. loss:  1.99582326e+00\n",
      "Epoch: 7757 mean train loss:  2.35631305e-05, mean val. loss:  1.99577093e+00\n",
      "Epoch: 7758 mean train loss:  2.37181375e-05, mean val. loss:  1.99572623e+00\n",
      "Epoch: 7759 mean train loss:  2.36066116e-05, mean val. loss:  1.99566340e+00\n",
      "Epoch: 7760 mean train loss:  2.34406325e-05, mean val. loss:  1.99558079e+00\n",
      "Epoch: 7761 mean train loss:  2.37450295e-05, mean val. loss:  1.99549687e+00\n",
      "Epoch: 7762 mean train loss:  2.34956678e-05, mean val. loss:  1.99541068e+00\n",
      "Epoch: 7763 mean train loss:  2.36920605e-05, mean val. loss:  1.99529731e+00\n",
      "Epoch: 7764 mean train loss:  2.27896089e-05, mean val. loss:  1.99519444e+00\n",
      "Epoch: 7765 mean train loss:  2.35073676e-05, mean val. loss:  1.99505377e+00\n",
      "Epoch: 7766 mean train loss:  2.35117332e-05, mean val. loss:  1.99491072e+00\n",
      "Epoch: 7767 mean train loss:  2.30104488e-05, mean val. loss:  1.99477029e+00\n",
      "Epoch: 7768 mean train loss:  2.34063482e-05, mean val. loss:  1.99464023e+00\n",
      "Epoch: 7769 mean train loss:  2.32988095e-05, mean val. loss:  1.99452221e+00\n",
      "Epoch: 7770 mean train loss:  2.33722385e-05, mean val. loss:  1.99443412e+00\n",
      "Epoch: 7771 mean train loss:  2.32582970e-05, mean val. loss:  1.99435687e+00\n",
      "Epoch: 7772 mean train loss:  2.35371990e-05, mean val. loss:  1.99426687e+00\n",
      "Epoch: 7773 mean train loss:  2.32213060e-05, mean val. loss:  1.99423695e+00\n",
      "Epoch: 7774 mean train loss:  2.33706087e-05, mean val. loss:  1.99425280e+00\n",
      "Epoch: 7775 mean train loss:  2.32369930e-05, mean val. loss:  1.99427545e+00\n",
      "Epoch: 7776 mean train loss:  2.38344946e-05, mean val. loss:  1.99430859e+00\n",
      "Epoch: 7777 mean train loss:  2.34038744e-05, mean val. loss:  1.99436176e+00\n",
      "Epoch: 7778 mean train loss:  2.32885359e-05, mean val. loss:  1.99443817e+00\n",
      "Epoch: 7779 mean train loss:  2.36736960e-05, mean val. loss:  1.99453342e+00\n",
      "Epoch: 7780 mean train loss:  2.36935157e-05, mean val. loss:  1.99462545e+00\n",
      "Epoch: 7781 mean train loss:  2.32642342e-05, mean val. loss:  1.99471354e+00\n",
      "Epoch: 7782 mean train loss:  2.35370244e-05, mean val. loss:  1.99478972e+00\n",
      "Epoch: 7783 mean train loss:  2.35023617e-05, mean val. loss:  1.99487710e+00\n",
      "Epoch: 7784 mean train loss:  2.33610626e-05, mean val. loss:  1.99496365e+00\n",
      "Epoch: 7785 mean train loss:  2.35197658e-05, mean val. loss:  1.99507248e+00\n",
      "Epoch: 7786 mean train loss:  2.34707259e-05, mean val. loss:  1.99518883e+00\n",
      "Epoch: 7787 mean train loss:  2.32718303e-05, mean val. loss:  1.99531519e+00\n",
      "Epoch: 7788 mean train loss:  2.34657782e-05, mean val. loss:  1.99543440e+00\n",
      "Epoch: 7789 mean train loss:  2.32100429e-05, mean val. loss:  1.99552238e+00\n",
      "Epoch: 7790 mean train loss:  2.31982267e-05, mean val. loss:  1.99558711e+00\n",
      "Epoch: 7791 mean train loss:  2.31728482e-05, mean val. loss:  1.99566245e+00\n",
      "Epoch: 7792 mean train loss:  2.32703751e-05, mean val. loss:  1.99573696e+00\n",
      "Epoch: 7793 mean train loss:  2.35730258e-05, mean val. loss:  1.99578428e+00\n",
      "Epoch: 7794 mean train loss:  2.30079750e-05, mean val. loss:  1.99583292e+00\n",
      "Epoch: 7795 mean train loss:  2.32206658e-05, mean val. loss:  1.99588501e+00\n",
      "Epoch: 7796 mean train loss:  2.31511658e-05, mean val. loss:  1.99593234e+00\n",
      "Epoch: 7797 mean train loss:  2.33863248e-05, mean val. loss:  1.99600041e+00\n",
      "Epoch: 7798 mean train loss:  2.33757310e-05, mean val. loss:  1.99604666e+00\n",
      "Epoch: 7799 mean train loss:  2.35335610e-05, mean val. loss:  1.99608994e+00\n",
      "Epoch: 7800 mean train loss:  2.31300364e-05, mean val. loss:  1.99612439e+00\n",
      "Epoch: 7801 mean train loss:  2.33338214e-05, mean val. loss:  1.99618196e+00\n",
      "Epoch: 7802 mean train loss:  2.33094324e-05, mean val. loss:  1.99620247e+00\n",
      "Epoch: 7803 mean train loss:  2.35851621e-05, mean val. loss:  1.99622238e+00\n",
      "Epoch: 7804 mean train loss:  2.36032647e-05, mean val. loss:  1.99622452e+00\n",
      "Epoch: 7805 mean train loss:  2.34969484e-05, mean val. loss:  1.99622202e+00\n",
      "Epoch: 7806 mean train loss:  2.33949686e-05, mean val. loss:  1.99619472e+00\n",
      "Epoch: 7807 mean train loss:  2.32197053e-05, mean val. loss:  1.99616432e+00\n",
      "Epoch: 7808 mean train loss:  2.32293678e-05, mean val. loss:  1.99614775e+00\n",
      "Epoch: 7809 mean train loss:  2.33039318e-05, mean val. loss:  1.99612308e+00\n",
      "Epoch: 7810 mean train loss:  2.31727900e-05, mean val. loss:  1.99610329e+00\n",
      "Epoch: 7811 mean train loss:  2.34949111e-05, mean val. loss:  1.99606478e+00\n",
      "Epoch: 7812 mean train loss:  2.33961619e-05, mean val. loss:  1.99599504e+00\n",
      "Epoch: 7813 mean train loss:  2.34968320e-05, mean val. loss:  1.99592364e+00\n",
      "Epoch: 7814 mean train loss:  2.34057661e-05, mean val. loss:  1.99585307e+00\n",
      "Epoch: 7815 mean train loss:  2.32403399e-05, mean val. loss:  1.99576914e+00\n",
      "Epoch: 7816 mean train loss:  2.34570471e-05, mean val. loss:  1.99565804e+00\n",
      "Epoch: 7817 mean train loss:  2.32447055e-05, mean val. loss:  1.99555337e+00\n",
      "Epoch: 7818 mean train loss:  2.29442376e-05, mean val. loss:  1.99546027e+00\n",
      "Epoch: 7819 mean train loss:  2.35455227e-05, mean val. loss:  1.99534166e+00\n",
      "Epoch: 7820 mean train loss:  2.32938619e-05, mean val. loss:  1.99520981e+00\n",
      "Epoch: 7821 mean train loss:  2.36917112e-05, mean val. loss:  1.99504936e+00\n",
      "Epoch: 7822 mean train loss:  2.33046303e-05, mean val. loss:  1.99489629e+00\n",
      "Epoch: 7823 mean train loss:  2.31576269e-05, mean val. loss:  1.99475288e+00\n",
      "Epoch: 7824 mean train loss:  2.35579209e-05, mean val. loss:  1.99461091e+00\n",
      "Epoch: 7825 mean train loss:  2.31996528e-05, mean val. loss:  1.99448013e+00\n",
      "Epoch: 7826 mean train loss:  2.31665035e-05, mean val. loss:  1.99436665e+00\n",
      "Epoch: 7827 mean train loss:  2.32608581e-05, mean val. loss:  1.99426305e+00\n",
      "Epoch: 7828 mean train loss:  2.35642074e-05, mean val. loss:  1.99415803e+00\n",
      "Epoch: 7829 mean train loss:  2.34645267e-05, mean val. loss:  1.99402738e+00\n",
      "Epoch: 7830 mean train loss:  2.36431370e-05, mean val. loss:  1.99388301e+00\n",
      "Epoch: 7831 mean train loss:  2.32993916e-05, mean val. loss:  1.99374521e+00\n",
      "Epoch: 7832 mean train loss:  2.32331513e-05, mean val. loss:  1.99359691e+00\n",
      "Epoch: 7833 mean train loss:  2.30253791e-05, mean val. loss:  1.99348974e+00\n",
      "Epoch: 7834 mean train loss:  2.31315789e-05, mean val. loss:  1.99343431e+00\n",
      "Epoch: 7835 mean train loss:  2.34636536e-05, mean val. loss:  1.99337924e+00\n",
      "Epoch: 7836 mean train loss:  2.33094033e-05, mean val. loss:  1.99334586e+00\n",
      "Epoch: 7837 mean train loss:  2.30311998e-05, mean val. loss:  1.99334514e+00\n",
      "Epoch: 7838 mean train loss:  2.34916806e-05, mean val. loss:  1.99333239e+00\n",
      "Epoch: 7839 mean train loss:  2.31230806e-05, mean val. loss:  1.99335945e+00\n",
      "Epoch: 7840 mean train loss:  2.32429593e-05, mean val. loss:  1.99341452e+00\n",
      "Epoch: 7841 mean train loss:  2.32099555e-05, mean val. loss:  1.99351025e+00\n",
      "Epoch: 7842 mean train loss:  2.36601918e-05, mean val. loss:  1.99358404e+00\n",
      "Epoch: 7843 mean train loss:  2.35584448e-05, mean val. loss:  1.99366534e+00\n",
      "Epoch: 7844 mean train loss:  2.34170584e-05, mean val. loss:  1.99373305e+00\n",
      "Epoch: 7845 mean train loss:  2.33813480e-05, mean val. loss:  1.99381638e+00\n",
      "Epoch: 7846 mean train loss:  2.35447951e-05, mean val. loss:  1.99391544e+00\n",
      "Epoch: 7847 mean train loss:  2.36803317e-05, mean val. loss:  1.99401283e+00\n",
      "Epoch: 7848 mean train loss:  2.31681042e-05, mean val. loss:  1.99411464e+00\n",
      "Epoch: 7849 mean train loss:  2.34181352e-05, mean val. loss:  1.99422681e+00\n",
      "Epoch: 7850 mean train loss:  2.33939791e-05, mean val. loss:  1.99430227e+00\n",
      "Epoch: 7851 mean train loss:  2.33630417e-05, mean val. loss:  1.99435556e+00\n",
      "Epoch: 7852 mean train loss:  2.30571895e-05, mean val. loss:  1.99438334e+00\n",
      "Epoch: 7853 mean train loss:  2.33450555e-05, mean val. loss:  1.99440432e+00\n",
      "Epoch: 7854 mean train loss:  2.32112070e-05, mean val. loss:  1.99441469e+00\n",
      "Epoch: 7855 mean train loss:  2.31739541e-05, mean val. loss:  1.99441695e+00\n",
      "Epoch: 7856 mean train loss:  2.31571903e-05, mean val. loss:  1.99443483e+00\n",
      "Epoch: 7857 mean train loss:  2.29883008e-05, mean val. loss:  1.99444914e+00\n",
      "Epoch: 7858 mean train loss:  2.34054460e-05, mean val. loss:  1.99443316e+00\n",
      "Epoch: 7859 mean train loss:  2.32605380e-05, mean val. loss:  1.99441695e+00\n",
      "Epoch: 7860 mean train loss:  2.28413555e-05, mean val. loss:  1.99439168e+00\n",
      "Epoch: 7861 mean train loss:  2.33307364e-05, mean val. loss:  1.99437499e+00\n",
      "Epoch: 7862 mean train loss:  2.29815196e-05, mean val. loss:  1.99438977e+00\n",
      "Epoch: 7863 mean train loss:  2.33891187e-05, mean val. loss:  1.99436677e+00\n",
      "Epoch: 7864 mean train loss:  2.30895530e-05, mean val. loss:  1.99433827e+00\n",
      "Epoch: 7865 mean train loss:  2.32544553e-05, mean val. loss:  1.99431956e+00\n",
      "Epoch: 7866 mean train loss:  2.34742474e-05, mean val. loss:  1.99431503e+00\n",
      "Epoch: 7867 mean train loss:  2.33711326e-05, mean val. loss:  1.99428558e+00\n",
      "Epoch: 7868 mean train loss:  2.33259925e-05, mean val. loss:  1.99426723e+00\n",
      "Epoch: 7869 mean train loss:  2.34243344e-05, mean val. loss:  1.99426150e+00\n",
      "Epoch: 7870 mean train loss:  2.30417645e-05, mean val. loss:  1.99422944e+00\n",
      "Epoch: 7871 mean train loss:  2.35185726e-05, mean val. loss:  1.99417758e+00\n",
      "Epoch: 7872 mean train loss:  2.34139152e-05, mean val. loss:  1.99411070e+00\n",
      "Epoch: 7873 mean train loss:  2.34663603e-05, mean val. loss:  1.99403024e+00\n",
      "Epoch: 7874 mean train loss:  2.30995356e-05, mean val. loss:  1.99394190e+00\n",
      "Epoch: 7875 mean train loss:  2.31388549e-05, mean val. loss:  1.99383867e+00\n",
      "Epoch: 7876 mean train loss:  2.31389422e-05, mean val. loss:  1.99374807e+00\n",
      "Epoch: 7877 mean train loss:  2.34454055e-05, mean val. loss:  1.99365127e+00\n",
      "Epoch: 7878 mean train loss:  2.35197658e-05, mean val. loss:  1.99352300e+00\n",
      "Epoch: 7879 mean train loss:  2.34635663e-05, mean val. loss:  1.99338484e+00\n",
      "Epoch: 7880 mean train loss:  2.29690922e-05, mean val. loss:  1.99323893e+00\n",
      "Epoch: 7881 mean train loss:  2.34249455e-05, mean val. loss:  1.99308562e+00\n",
      "Epoch: 7882 mean train loss:  2.35933112e-05, mean val. loss:  1.99289739e+00\n",
      "Epoch: 7883 mean train loss:  2.36483465e-05, mean val. loss:  1.99268138e+00\n",
      "Epoch: 7884 mean train loss:  2.34944746e-05, mean val. loss:  1.99243593e+00\n",
      "Epoch: 7885 mean train loss:  2.33209284e-05, mean val. loss:  1.99219120e+00\n",
      "Epoch: 7886 mean train loss:  2.32378370e-05, mean val. loss:  1.99191630e+00\n",
      "Epoch: 7887 mean train loss:  2.30877486e-05, mean val. loss:  1.99164951e+00\n",
      "Epoch: 7888 mean train loss:  2.33719475e-05, mean val. loss:  1.99138200e+00\n",
      "Epoch: 7889 mean train loss:  2.32501188e-05, mean val. loss:  1.99112880e+00\n",
      "Epoch: 7890 mean train loss:  2.31156591e-05, mean val. loss:  1.99088061e+00\n",
      "Epoch: 7891 mean train loss:  2.32550665e-05, mean val. loss:  1.99064732e+00\n",
      "Epoch: 7892 mean train loss:  2.29367579e-05, mean val. loss:  1.99044573e+00\n",
      "Epoch: 7893 mean train loss:  2.30903097e-05, mean val. loss:  1.99026358e+00\n",
      "Epoch: 7894 mean train loss:  2.29166762e-05, mean val. loss:  1.99010551e+00\n",
      "Epoch: 7895 mean train loss:  2.30906880e-05, mean val. loss:  1.98999059e+00\n",
      "Epoch: 7896 mean train loss:  2.29960424e-05, mean val. loss:  1.98992956e+00\n",
      "Epoch: 7897 mean train loss:  2.30238948e-05, mean val. loss:  1.98989272e+00\n",
      "Epoch: 7898 mean train loss:  2.32275634e-05, mean val. loss:  1.98988938e+00\n",
      "Epoch: 7899 mean train loss:  2.32841412e-05, mean val. loss:  1.98992050e+00\n",
      "Epoch: 7900 mean train loss:  2.31722661e-05, mean val. loss:  1.98998618e+00\n",
      "Epoch: 7901 mean train loss:  2.32848397e-05, mean val. loss:  1.99009776e+00\n",
      "Epoch: 7902 mean train loss:  2.32406310e-05, mean val. loss:  1.99025834e+00\n",
      "Epoch: 7903 mean train loss:  2.32369639e-05, mean val. loss:  1.99045348e+00\n",
      "Epoch: 7904 mean train loss:  2.32480234e-05, mean val. loss:  1.99065435e+00\n",
      "Epoch: 7905 mean train loss:  2.32486345e-05, mean val. loss:  1.99088836e+00\n",
      "Epoch: 7906 mean train loss:  2.33593746e-05, mean val. loss:  1.99111652e+00\n",
      "Epoch: 7907 mean train loss:  2.33253231e-05, mean val. loss:  1.99137056e+00\n",
      "Epoch: 7908 mean train loss:  2.34895851e-05, mean val. loss:  1.99162841e+00\n",
      "Epoch: 7909 mean train loss:  2.32866150e-05, mean val. loss:  1.99189758e+00\n",
      "Epoch: 7910 mean train loss:  2.30508740e-05, mean val. loss:  1.99216771e+00\n",
      "Epoch: 7911 mean train loss:  2.32304155e-05, mean val. loss:  1.99241984e+00\n",
      "Epoch: 7912 mean train loss:  2.31410377e-05, mean val. loss:  1.99267745e+00\n",
      "Epoch: 7913 mean train loss:  2.36649939e-05, mean val. loss:  1.99292004e+00\n",
      "Epoch: 7914 mean train loss:  2.34468898e-05, mean val. loss:  1.99313438e+00\n",
      "Epoch: 7915 mean train loss:  2.29690922e-05, mean val. loss:  1.99335110e+00\n",
      "Epoch: 7916 mean train loss:  2.30534934e-05, mean val. loss:  1.99355841e+00\n",
      "Epoch: 7917 mean train loss:  2.29607103e-05, mean val. loss:  1.99376523e+00\n",
      "Epoch: 7918 mean train loss:  2.32734892e-05, mean val. loss:  1.99394059e+00\n",
      "Epoch: 7919 mean train loss:  2.34172330e-05, mean val. loss:  1.99411190e+00\n",
      "Epoch: 7920 mean train loss:  2.30342848e-05, mean val. loss:  1.99426401e+00\n",
      "Epoch: 7921 mean train loss:  2.31383019e-05, mean val. loss:  1.99438012e+00\n",
      "Epoch: 7922 mean train loss:  2.30941805e-05, mean val. loss:  1.99444914e+00\n",
      "Epoch: 7923 mean train loss:  2.32797174e-05, mean val. loss:  1.99447417e+00\n",
      "Epoch: 7924 mean train loss:  2.29428406e-05, mean val. loss:  1.99447787e+00\n",
      "Epoch: 7925 mean train loss:  2.29817815e-05, mean val. loss:  1.99445367e+00\n",
      "Epoch: 7926 mean train loss:  2.30113510e-05, mean val. loss:  1.99440300e+00\n",
      "Epoch: 7927 mean train loss:  2.35624611e-05, mean val. loss:  1.99433303e+00\n",
      "Epoch: 7928 mean train loss:  2.32529128e-05, mean val. loss:  1.99426079e+00\n",
      "Epoch: 7929 mean train loss:  2.30907171e-05, mean val. loss:  1.99418342e+00\n",
      "Epoch: 7930 mean train loss:  2.34027975e-05, mean val. loss:  1.99410927e+00\n",
      "Epoch: 7931 mean train loss:  2.34941544e-05, mean val. loss:  1.99402153e+00\n",
      "Epoch: 7932 mean train loss:  2.31245940e-05, mean val. loss:  1.99395442e+00\n",
      "Epoch: 7933 mean train loss:  2.35407206e-05, mean val. loss:  1.99388266e+00\n",
      "Epoch: 7934 mean train loss:  2.34662730e-05, mean val. loss:  1.99379921e+00\n",
      "Epoch: 7935 mean train loss:  2.33037863e-05, mean val. loss:  1.99371159e+00\n",
      "Epoch: 7936 mean train loss:  2.27618148e-05, mean val. loss:  1.99364460e+00\n",
      "Epoch: 7937 mean train loss:  2.31373124e-05, mean val. loss:  1.99356842e+00\n",
      "Epoch: 7938 mean train loss:  2.33347237e-05, mean val. loss:  1.99344778e+00\n",
      "Epoch: 7939 mean train loss:  2.32878665e-05, mean val. loss:  1.99331689e+00\n",
      "Epoch: 7940 mean train loss:  2.35346379e-05, mean val. loss:  1.99321568e+00\n",
      "Epoch: 7941 mean train loss:  2.31922313e-05, mean val. loss:  1.99309969e+00\n",
      "Epoch: 7942 mean train loss:  2.31137383e-05, mean val. loss:  1.99298382e+00\n",
      "Epoch: 7943 mean train loss:  2.29263678e-05, mean val. loss:  1.99286747e+00\n",
      "Epoch: 7944 mean train loss:  2.26934790e-05, mean val. loss:  1.99277389e+00\n",
      "Epoch: 7945 mean train loss:  2.28450517e-05, mean val. loss:  1.99268210e+00\n",
      "Epoch: 7946 mean train loss:  2.30773003e-05, mean val. loss:  1.99256575e+00\n",
      "Epoch: 7947 mean train loss:  2.34403415e-05, mean val. loss:  1.99246466e+00\n",
      "Epoch: 7948 mean train loss:  2.34773615e-05, mean val. loss:  1.99235439e+00\n",
      "Epoch: 7949 mean train loss:  2.30313162e-05, mean val. loss:  1.99226439e+00\n",
      "Epoch: 7950 mean train loss:  2.29878933e-05, mean val. loss:  1.99218166e+00\n",
      "Epoch: 7951 mean train loss:  2.31447630e-05, mean val. loss:  1.99209797e+00\n",
      "Epoch: 7952 mean train loss:  2.29356228e-05, mean val. loss:  1.99201298e+00\n",
      "Epoch: 7953 mean train loss:  2.33622559e-05, mean val. loss:  1.99194884e+00\n",
      "Epoch: 7954 mean train loss:  2.32392049e-05, mean val. loss:  1.99186969e+00\n",
      "Epoch: 7955 mean train loss:  2.31569866e-05, mean val. loss:  1.99180150e+00\n",
      "Epoch: 7956 mean train loss:  2.33282626e-05, mean val. loss:  1.99173415e+00\n",
      "Epoch: 7957 mean train loss:  2.28209246e-05, mean val. loss:  1.99168789e+00\n",
      "Epoch: 7958 mean train loss:  2.32050370e-05, mean val. loss:  1.99166512e+00\n",
      "Epoch: 7959 mean train loss:  2.27486889e-05, mean val. loss:  1.99169254e+00\n",
      "Epoch: 7960 mean train loss:  2.32700550e-05, mean val. loss:  1.99173152e+00\n",
      "Epoch: 7961 mean train loss:  2.37974164e-05, mean val. loss:  1.99178076e+00\n",
      "Epoch: 7962 mean train loss:  2.32622260e-05, mean val. loss:  1.99186635e+00\n",
      "Epoch: 7963 mean train loss:  2.30589358e-05, mean val. loss:  1.99197340e+00\n",
      "Epoch: 7964 mean train loss:  2.30712176e-05, mean val. loss:  1.99207020e+00\n",
      "Epoch: 7965 mean train loss:  2.32557941e-05, mean val. loss:  1.99214244e+00\n",
      "Epoch: 7966 mean train loss:  2.32147286e-05, mean val. loss:  1.99219787e+00\n",
      "Epoch: 7967 mean train loss:  2.28466815e-05, mean val. loss:  1.99226356e+00\n",
      "Epoch: 7968 mean train loss:  2.30711885e-05, mean val. loss:  1.99232364e+00\n",
      "Epoch: 7969 mean train loss:  2.33627507e-05, mean val. loss:  1.99236286e+00\n",
      "Epoch: 7970 mean train loss:  2.34295730e-05, mean val. loss:  1.99236429e+00\n",
      "Epoch: 7971 mean train loss:  2.32973834e-05, mean val. loss:  1.99232388e+00\n",
      "Epoch: 7972 mean train loss:  2.31546874e-05, mean val. loss:  1.99225891e+00\n",
      "Epoch: 7973 mean train loss:  2.30460719e-05, mean val. loss:  1.99218869e+00\n",
      "Epoch: 7974 mean train loss:  2.31028534e-05, mean val. loss:  1.99211836e+00\n",
      "Epoch: 7975 mean train loss:  2.33963947e-05, mean val. loss:  1.99202645e+00\n",
      "Epoch: 7976 mean train loss:  2.31937738e-05, mean val. loss:  1.99192238e+00\n",
      "Epoch: 7977 mean train loss:  2.32539605e-05, mean val. loss:  1.99177992e+00\n",
      "Epoch: 7978 mean train loss:  2.28403951e-05, mean val. loss:  1.99160779e+00\n",
      "Epoch: 7979 mean train loss:  2.29633879e-05, mean val. loss:  1.99140167e+00\n",
      "Epoch: 7980 mean train loss:  2.28242425e-05, mean val. loss:  1.99120092e+00\n",
      "Epoch: 7981 mean train loss:  2.30466248e-05, mean val. loss:  1.99103081e+00\n",
      "Epoch: 7982 mean train loss:  2.32468301e-05, mean val. loss:  1.99085367e+00\n",
      "Epoch: 7983 mean train loss:  2.31427257e-05, mean val. loss:  1.99070072e+00\n",
      "Epoch: 7984 mean train loss:  2.32488383e-05, mean val. loss:  1.99053288e+00\n",
      "Epoch: 7985 mean train loss:  2.32802122e-05, mean val. loss:  1.99040234e+00\n",
      "Epoch: 7986 mean train loss:  2.29747675e-05, mean val. loss:  1.99029243e+00\n",
      "Epoch: 7987 mean train loss:  2.33178434e-05, mean val. loss:  1.99019837e+00\n",
      "Epoch: 7988 mean train loss:  2.29128345e-05, mean val. loss:  1.99011588e+00\n",
      "Epoch: 7989 mean train loss:  2.33416504e-05, mean val. loss:  1.99004722e+00\n",
      "Epoch: 7990 mean train loss:  2.31398153e-05, mean val. loss:  1.98999023e+00\n",
      "Epoch: 7991 mean train loss:  2.32873135e-05, mean val. loss:  1.98997045e+00\n",
      "Epoch: 7992 mean train loss:  2.29705183e-05, mean val. loss:  1.98996317e+00\n",
      "Epoch: 7993 mean train loss:  2.33960745e-05, mean val. loss:  1.98997414e+00\n",
      "Epoch: 7994 mean train loss:  2.30703154e-05, mean val. loss:  1.99000585e+00\n",
      "Epoch: 7995 mean train loss:  2.30562873e-05, mean val. loss:  1.99004996e+00\n",
      "Epoch: 7996 mean train loss:  2.28276767e-05, mean val. loss:  1.99010861e+00\n",
      "Epoch: 7997 mean train loss:  2.29599827e-05, mean val. loss:  1.99017727e+00\n",
      "Epoch: 7998 mean train loss:  2.25435360e-05, mean val. loss:  1.99029064e+00\n",
      "Epoch: 7999 mean train loss:  2.28661520e-05, mean val. loss:  1.99041021e+00\n",
      "Epoch: 8000 mean train loss:  2.32024759e-05, mean val. loss:  1.99052989e+00\n",
      "Epoch: 8001 mean train loss:  2.29774741e-05, mean val. loss:  1.99066365e+00\n",
      "Epoch: 8002 mean train loss:  2.30826263e-05, mean val. loss:  1.99079156e+00\n",
      "Epoch: 8003 mean train loss:  2.32458115e-05, mean val. loss:  1.99092317e+00\n",
      "Epoch: 8004 mean train loss:  2.31936574e-05, mean val. loss:  1.99105299e+00\n",
      "Epoch: 8005 mean train loss:  2.28563731e-05, mean val. loss:  1.99118829e+00\n",
      "Epoch: 8006 mean train loss:  2.31007580e-05, mean val. loss:  1.99128354e+00\n",
      "Epoch: 8007 mean train loss:  2.33218307e-05, mean val. loss:  1.99137640e+00\n",
      "Epoch: 8008 mean train loss:  2.30610603e-05, mean val. loss:  1.99146903e+00\n",
      "Epoch: 8009 mean train loss:  2.30234000e-05, mean val. loss:  1.99155605e+00\n",
      "Epoch: 8010 mean train loss:  2.28628633e-05, mean val. loss:  1.99164677e+00\n",
      "Epoch: 8011 mean train loss:  2.29884172e-05, mean val. loss:  1.99171090e+00\n",
      "Epoch: 8012 mean train loss:  2.30141159e-05, mean val. loss:  1.99174738e+00\n",
      "Epoch: 8013 mean train loss:  2.33003229e-05, mean val. loss:  1.99177670e+00\n",
      "Epoch: 8014 mean train loss:  2.29620782e-05, mean val. loss:  1.99175978e+00\n",
      "Epoch: 8015 mean train loss:  2.28623976e-05, mean val. loss:  1.99175346e+00\n",
      "Epoch: 8016 mean train loss:  2.32235470e-05, mean val. loss:  1.99172688e+00\n",
      "Epoch: 8017 mean train loss:  2.32549501e-05, mean val. loss:  1.99170661e+00\n",
      "Epoch: 8018 mean train loss:  2.30079168e-05, mean val. loss:  1.99167454e+00\n",
      "Epoch: 8019 mean train loss:  2.29180441e-05, mean val. loss:  1.99162352e+00\n",
      "Epoch: 8020 mean train loss:  2.31342856e-05, mean val. loss:  1.99154711e+00\n",
      "Epoch: 8021 mean train loss:  2.30765145e-05, mean val. loss:  1.99149215e+00\n",
      "Epoch: 8022 mean train loss:  2.26679840e-05, mean val. loss:  1.99141192e+00\n",
      "Epoch: 8023 mean train loss:  2.30811711e-05, mean val. loss:  1.99135172e+00\n",
      "Epoch: 8024 mean train loss:  2.29783473e-05, mean val. loss:  1.99128664e+00\n",
      "Epoch: 8025 mean train loss:  2.29411235e-05, mean val. loss:  1.99124026e+00\n",
      "Epoch: 8026 mean train loss:  2.29317229e-05, mean val. loss:  1.99119699e+00\n",
      "Epoch: 8027 mean train loss:  2.30658625e-05, mean val. loss:  1.99115896e+00\n",
      "Epoch: 8028 mean train loss:  2.32790189e-05, mean val. loss:  1.99113095e+00\n",
      "Epoch: 8029 mean train loss:  2.30952865e-05, mean val. loss:  1.99111915e+00\n",
      "Epoch: 8030 mean train loss:  2.30564910e-05, mean val. loss:  1.99111891e+00\n",
      "Epoch: 8031 mean train loss:  2.30064616e-05, mean val. loss:  1.99112296e+00\n",
      "Epoch: 8032 mean train loss:  2.29474972e-05, mean val. loss:  1.99117255e+00\n",
      "Epoch: 8033 mean train loss:  2.31888553e-05, mean val. loss:  1.99122334e+00\n",
      "Epoch: 8034 mean train loss:  2.27907149e-05, mean val. loss:  1.99127889e+00\n",
      "Epoch: 8035 mean train loss:  2.29176367e-05, mean val. loss:  1.99136317e+00\n",
      "Epoch: 8036 mean train loss:  2.30470905e-05, mean val. loss:  1.99144721e+00\n",
      "Epoch: 8037 mean train loss:  2.27559940e-05, mean val. loss:  1.99153280e+00\n",
      "Epoch: 8038 mean train loss:  2.30723526e-05, mean val. loss:  1.99161339e+00\n",
      "Epoch: 8039 mean train loss:  2.29290745e-05, mean val. loss:  1.99166787e+00\n",
      "Epoch: 8040 mean train loss:  2.31604499e-05, mean val. loss:  1.99169660e+00\n",
      "Epoch: 8041 mean train loss:  2.29126017e-05, mean val. loss:  1.99173331e+00\n",
      "Epoch: 8042 mean train loss:  2.26887059e-05, mean val. loss:  1.99180734e+00\n",
      "Epoch: 8043 mean train loss:  2.30648438e-05, mean val. loss:  1.99184835e+00\n",
      "Epoch: 8044 mean train loss:  2.30022706e-05, mean val. loss:  1.99190485e+00\n",
      "Epoch: 8045 mean train loss:  2.30025034e-05, mean val. loss:  1.99196160e+00\n",
      "Epoch: 8046 mean train loss:  2.30429578e-05, mean val. loss:  1.99199605e+00\n",
      "Epoch: 8047 mean train loss:  2.32825405e-05, mean val. loss:  1.99201500e+00\n",
      "Epoch: 8048 mean train loss:  2.31004378e-05, mean val. loss:  1.99201095e+00\n",
      "Epoch: 8049 mean train loss:  2.31754675e-05, mean val. loss:  1.99198222e+00\n",
      "Epoch: 8050 mean train loss:  2.29220896e-05, mean val. loss:  1.99195218e+00\n",
      "Epoch: 8051 mean train loss:  2.29025318e-05, mean val. loss:  1.99189532e+00\n",
      "Epoch: 8052 mean train loss:  2.30310543e-05, mean val. loss:  1.99180615e+00\n",
      "Epoch: 8053 mean train loss:  2.28705758e-05, mean val. loss:  1.99170792e+00\n",
      "Epoch: 8054 mean train loss:  2.30214500e-05, mean val. loss:  1.99163592e+00\n",
      "Epoch: 8055 mean train loss:  2.27306737e-05, mean val. loss:  1.99151528e+00\n",
      "Epoch: 8056 mean train loss:  2.29313446e-05, mean val. loss:  1.99137008e+00\n",
      "Epoch: 8057 mean train loss:  2.29876605e-05, mean val. loss:  1.99123657e+00\n",
      "Epoch: 8058 mean train loss:  2.31484009e-05, mean val. loss:  1.99113047e+00\n",
      "Epoch: 8059 mean train loss:  2.32077437e-05, mean val. loss:  1.99101651e+00\n",
      "Epoch: 8060 mean train loss:  2.32457824e-05, mean val. loss:  1.99088025e+00\n",
      "Epoch: 8061 mean train loss:  2.28641438e-05, mean val. loss:  1.99075723e+00\n",
      "Epoch: 8062 mean train loss:  2.35697953e-05, mean val. loss:  1.99063754e+00\n",
      "Epoch: 8063 mean train loss:  2.27141718e-05, mean val. loss:  1.99053538e+00\n",
      "Epoch: 8064 mean train loss:  2.30600417e-05, mean val. loss:  1.99041402e+00\n",
      "Epoch: 8065 mean train loss:  2.31138256e-05, mean val. loss:  1.99031365e+00\n",
      "Epoch: 8066 mean train loss:  2.28958670e-05, mean val. loss:  1.99021447e+00\n",
      "Epoch: 8067 mean train loss:  2.30517762e-05, mean val. loss:  1.99011159e+00\n",
      "Epoch: 8068 mean train loss:  2.29324214e-05, mean val. loss:  1.99005616e+00\n",
      "Epoch: 8069 mean train loss:  2.29990110e-05, mean val. loss:  1.98999298e+00\n",
      "Epoch: 8070 mean train loss:  2.31779413e-05, mean val. loss:  1.98992956e+00\n",
      "Epoch: 8071 mean train loss:  2.30613223e-05, mean val. loss:  1.98988914e+00\n",
      "Epoch: 8072 mean train loss:  2.30023579e-05, mean val. loss:  1.98984826e+00\n",
      "Epoch: 8073 mean train loss:  2.27620476e-05, mean val. loss:  1.98983502e+00\n",
      "Epoch: 8074 mean train loss:  2.30943260e-05, mean val. loss:  1.98982227e+00\n",
      "Epoch: 8075 mean train loss:  2.28022982e-05, mean val. loss:  1.98983204e+00\n",
      "Epoch: 8076 mean train loss:  2.32338789e-05, mean val. loss:  1.98984873e+00\n",
      "Epoch: 8077 mean train loss:  2.31318409e-05, mean val. loss:  1.98988640e+00\n",
      "Epoch: 8078 mean train loss:  2.30146688e-05, mean val. loss:  1.98991859e+00\n",
      "Epoch: 8079 mean train loss:  2.29769212e-05, mean val. loss:  1.98995030e+00\n",
      "Epoch: 8080 mean train loss:  2.26112898e-05, mean val. loss:  1.98999393e+00\n",
      "Epoch: 8081 mean train loss:  2.30582082e-05, mean val. loss:  1.99004328e+00\n",
      "Epoch: 8082 mean train loss:  2.27657147e-05, mean val. loss:  1.99006367e+00\n",
      "Epoch: 8083 mean train loss:  2.30639125e-05, mean val. loss:  1.99006987e+00\n",
      "Epoch: 8084 mean train loss:  2.32055318e-05, mean val. loss:  1.99006832e+00\n",
      "Epoch: 8085 mean train loss:  2.27726996e-05, mean val. loss:  1.99008596e+00\n",
      "Epoch: 8086 mean train loss:  2.28731951e-05, mean val. loss:  1.99007022e+00\n",
      "Epoch: 8087 mean train loss:  2.30334408e-05, mean val. loss:  1.99004793e+00\n",
      "Epoch: 8088 mean train loss:  2.26959237e-05, mean val. loss:  1.99002433e+00\n",
      "Epoch: 8089 mean train loss:  2.32302118e-05, mean val. loss:  1.98997974e+00\n",
      "Epoch: 8090 mean train loss:  2.28963909e-05, mean val. loss:  1.98991787e+00\n",
      "Epoch: 8091 mean train loss:  2.30016012e-05, mean val. loss:  1.98987317e+00\n",
      "Epoch: 8092 mean train loss:  2.27563432e-05, mean val. loss:  1.98983920e+00\n",
      "Epoch: 8093 mean train loss:  2.32194434e-05, mean val. loss:  1.98983955e+00\n",
      "Epoch: 8094 mean train loss:  2.28959252e-05, mean val. loss:  1.98982275e+00\n",
      "Epoch: 8095 mean train loss:  2.31385347e-05, mean val. loss:  1.98979032e+00\n",
      "Epoch: 8096 mean train loss:  2.25784315e-05, mean val. loss:  1.98975825e+00\n",
      "Epoch: 8097 mean train loss:  2.29813159e-05, mean val. loss:  1.98969328e+00\n",
      "Epoch: 8098 mean train loss:  2.30264268e-05, mean val. loss:  1.98961616e+00\n",
      "Epoch: 8099 mean train loss:  2.29616708e-05, mean val. loss:  1.98954797e+00\n",
      "Epoch: 8100 mean train loss:  2.28224962e-05, mean val. loss:  1.98949456e+00\n",
      "Epoch: 8101 mean train loss:  2.28723220e-05, mean val. loss:  1.98942208e+00\n",
      "Epoch: 8102 mean train loss:  2.28946737e-05, mean val. loss:  1.98935413e+00\n",
      "Epoch: 8103 mean train loss:  2.30415608e-05, mean val. loss:  1.98928976e+00\n",
      "Epoch: 8104 mean train loss:  2.27705168e-05, mean val. loss:  1.98922241e+00\n",
      "Epoch: 8105 mean train loss:  2.28375720e-05, mean val. loss:  1.98916781e+00\n",
      "Epoch: 8106 mean train loss:  2.27850105e-05, mean val. loss:  1.98914301e+00\n",
      "Epoch: 8107 mean train loss:  2.26467382e-05, mean val. loss:  1.98915040e+00\n",
      "Epoch: 8108 mean train loss:  2.29912985e-05, mean val. loss:  1.98915732e+00\n",
      "Epoch: 8109 mean train loss:  2.27995624e-05, mean val. loss:  1.98915243e+00\n",
      "Epoch: 8110 mean train loss:  2.30694131e-05, mean val. loss:  1.98915243e+00\n",
      "Epoch: 8111 mean train loss:  2.32174061e-05, mean val. loss:  1.98917711e+00\n",
      "Epoch: 8112 mean train loss:  2.30440346e-05, mean val. loss:  1.98919916e+00\n",
      "Epoch: 8113 mean train loss:  2.31940649e-05, mean val. loss:  1.98921978e+00\n",
      "Epoch: 8114 mean train loss:  2.28541903e-05, mean val. loss:  1.98923326e+00\n",
      "Epoch: 8115 mean train loss:  2.31917365e-05, mean val. loss:  1.98925519e+00\n",
      "Epoch: 8116 mean train loss:  2.26796255e-05, mean val. loss:  1.98925281e+00\n",
      "Epoch: 8117 mean train loss:  2.27970886e-05, mean val. loss:  1.98928154e+00\n",
      "Epoch: 8118 mean train loss:  2.27100973e-05, mean val. loss:  1.98930871e+00\n",
      "Epoch: 8119 mean train loss:  2.29225843e-05, mean val. loss:  1.98934042e+00\n",
      "Epoch: 8120 mean train loss:  2.27107957e-05, mean val. loss:  1.98934925e+00\n",
      "Epoch: 8121 mean train loss:  2.26988341e-05, mean val. loss:  1.98938000e+00\n",
      "Epoch: 8122 mean train loss:  2.31491285e-05, mean val. loss:  1.98942864e+00\n",
      "Epoch: 8123 mean train loss:  2.28985155e-05, mean val. loss:  1.98949182e+00\n",
      "Epoch: 8124 mean train loss:  2.29762518e-05, mean val. loss:  1.98955274e+00\n",
      "Epoch: 8125 mean train loss:  2.29948200e-05, mean val. loss:  1.98959506e+00\n",
      "Epoch: 8126 mean train loss:  2.26846605e-05, mean val. loss:  1.98964643e+00\n",
      "Epoch: 8127 mean train loss:  2.26450793e-05, mean val. loss:  1.98971558e+00\n",
      "Epoch: 8128 mean train loss:  2.27543351e-05, mean val. loss:  1.98980832e+00\n",
      "Epoch: 8129 mean train loss:  2.29305879e-05, mean val. loss:  1.98988342e+00\n",
      "Epoch: 8130 mean train loss:  2.29413854e-05, mean val. loss:  1.98998296e+00\n",
      "Epoch: 8131 mean train loss:  2.27784039e-05, mean val. loss:  1.99011123e+00\n",
      "Epoch: 8132 mean train loss:  2.27670243e-05, mean val. loss:  1.99021709e+00\n",
      "Epoch: 8133 mean train loss:  2.30204896e-05, mean val. loss:  1.99030495e+00\n",
      "Epoch: 8134 mean train loss:  2.29679863e-05, mean val. loss:  1.99037552e+00\n",
      "Epoch: 8135 mean train loss:  2.30710721e-05, mean val. loss:  1.99041748e+00\n",
      "Epoch: 8136 mean train loss:  2.29598663e-05, mean val. loss:  1.99045396e+00\n",
      "Epoch: 8137 mean train loss:  2.28066056e-05, mean val. loss:  1.99047947e+00\n",
      "Epoch: 8138 mean train loss:  2.30994774e-05, mean val. loss:  1.99049854e+00\n",
      "Epoch: 8139 mean train loss:  2.26518896e-05, mean val. loss:  1.99055207e+00\n",
      "Epoch: 8140 mean train loss:  2.31028243e-05, mean val. loss:  1.99057209e+00\n",
      "Epoch: 8141 mean train loss:  2.27935670e-05, mean val. loss:  1.99059820e+00\n",
      "Epoch: 8142 mean train loss:  2.22212693e-05, mean val. loss:  1.99063325e+00\n",
      "Epoch: 8143 mean train loss:  2.27995624e-05, mean val. loss:  1.99066210e+00\n",
      "Epoch: 8144 mean train loss:  2.26689153e-05, mean val. loss:  1.99066281e+00\n",
      "Epoch: 8145 mean train loss:  2.26266566e-05, mean val. loss:  1.99064732e+00\n",
      "Epoch: 8146 mean train loss:  2.27505807e-05, mean val. loss:  1.99064767e+00\n",
      "Epoch: 8147 mean train loss:  2.30591977e-05, mean val. loss:  1.99063849e+00\n",
      "Epoch: 8148 mean train loss:  2.28199933e-05, mean val. loss:  1.99062717e+00\n",
      "Epoch: 8149 mean train loss:  2.30417063e-05, mean val. loss:  1.99060595e+00\n",
      "Epoch: 8150 mean train loss:  2.26037519e-05, mean val. loss:  1.99059999e+00\n",
      "Epoch: 8151 mean train loss:  2.29379511e-05, mean val. loss:  1.99060118e+00\n",
      "Epoch: 8152 mean train loss:  2.27208075e-05, mean val. loss:  1.99061120e+00\n",
      "Epoch: 8153 mean train loss:  2.29692087e-05, mean val. loss:  1.99063337e+00\n",
      "Epoch: 8154 mean train loss:  2.31481099e-05, mean val. loss:  1.99065185e+00\n",
      "Epoch: 8155 mean train loss:  2.32138846e-05, mean val. loss:  1.99066257e+00\n",
      "Epoch: 8156 mean train loss:  2.29009311e-05, mean val. loss:  1.99064767e+00\n",
      "Epoch: 8157 mean train loss:  2.33014289e-05, mean val. loss:  1.99060190e+00\n",
      "Epoch: 8158 mean train loss:  2.27516866e-05, mean val. loss:  1.99056411e+00\n",
      "Epoch: 8159 mean train loss:  2.28493300e-05, mean val. loss:  1.99054801e+00\n",
      "Epoch: 8160 mean train loss:  2.26992997e-05, mean val. loss:  1.99051619e+00\n",
      "Epoch: 8161 mean train loss:  2.32980819e-05, mean val. loss:  1.99047089e+00\n",
      "Epoch: 8162 mean train loss:  2.27104465e-05, mean val. loss:  1.99043953e+00\n",
      "Epoch: 8163 mean train loss:  2.28061690e-05, mean val. loss:  1.99040270e+00\n",
      "Epoch: 8164 mean train loss:  2.26780248e-05, mean val. loss:  1.99037516e+00\n",
      "Epoch: 8165 mean train loss:  2.28651916e-05, mean val. loss:  1.99032664e+00\n",
      "Epoch: 8166 mean train loss:  2.28118734e-05, mean val. loss:  1.99028540e+00\n",
      "Epoch: 8167 mean train loss:  2.27496785e-05, mean val. loss:  1.99026668e+00\n",
      "Epoch: 8168 mean train loss:  2.29476427e-05, mean val. loss:  1.99025965e+00\n",
      "Epoch: 8169 mean train loss:  2.27841374e-05, mean val. loss:  1.99026930e+00\n",
      "Epoch: 8170 mean train loss:  2.25621043e-05, mean val. loss:  1.99031174e+00\n",
      "Epoch: 8171 mean train loss:  2.30312289e-05, mean val. loss:  1.99033928e+00\n",
      "Epoch: 8172 mean train loss:  2.30371952e-05, mean val. loss:  1.99038875e+00\n",
      "Epoch: 8173 mean train loss:  2.28919380e-05, mean val. loss:  1.99044013e+00\n",
      "Epoch: 8174 mean train loss:  2.28159479e-05, mean val. loss:  1.99049413e+00\n",
      "Epoch: 8175 mean train loss:  2.22586968e-05, mean val. loss:  1.99057603e+00\n",
      "Epoch: 8176 mean train loss:  2.28595745e-05, mean val. loss:  1.99063826e+00\n",
      "Epoch: 8177 mean train loss:  2.30233709e-05, mean val. loss:  1.99072146e+00\n",
      "Epoch: 8178 mean train loss:  2.31415615e-05, mean val. loss:  1.99080336e+00\n",
      "Epoch: 8179 mean train loss:  2.30310834e-05, mean val. loss:  1.99091208e+00\n",
      "Epoch: 8180 mean train loss:  2.25821277e-05, mean val. loss:  1.99103034e+00\n",
      "Epoch: 8181 mean train loss:  2.31197046e-05, mean val. loss:  1.99114144e+00\n",
      "Epoch: 8182 mean train loss:  2.28710705e-05, mean val. loss:  1.99127555e+00\n",
      "Epoch: 8183 mean train loss:  2.23882380e-05, mean val. loss:  1.99137604e+00\n",
      "Epoch: 8184 mean train loss:  2.29772413e-05, mean val. loss:  1.99145544e+00\n",
      "Epoch: 8185 mean train loss:  2.22297385e-05, mean val. loss:  1.99153137e+00\n",
      "Epoch: 8186 mean train loss:  2.29831203e-05, mean val. loss:  1.99159718e+00\n",
      "Epoch: 8187 mean train loss:  2.29006982e-05, mean val. loss:  1.99164379e+00\n",
      "Epoch: 8188 mean train loss:  2.27943528e-05, mean val. loss:  1.99169075e+00\n",
      "Epoch: 8189 mean train loss:  2.29604775e-05, mean val. loss:  1.99170470e+00\n",
      "Epoch: 8190 mean train loss:  2.26499687e-05, mean val. loss:  1.99168050e+00\n",
      "Epoch: 8191 mean train loss:  2.29351281e-05, mean val. loss:  1.99162447e+00\n",
      "Epoch: 8192 mean train loss:  2.29246507e-05, mean val. loss:  1.99153686e+00\n",
      "Epoch: 8193 mean train loss:  2.27566052e-05, mean val. loss:  1.99142194e+00\n",
      "Epoch: 8194 mean train loss:  2.28401041e-05, mean val. loss:  1.99128842e+00\n",
      "Epoch: 8195 mean train loss:  2.27789569e-05, mean val. loss:  1.99113595e+00\n",
      "Epoch: 8196 mean train loss:  2.23432435e-05, mean val. loss:  1.99099016e+00\n",
      "Epoch: 8197 mean train loss:  2.29048601e-05, mean val. loss:  1.99083638e+00\n",
      "Epoch: 8198 mean train loss:  2.29182479e-05, mean val. loss:  1.99068570e+00\n",
      "Epoch: 8199 mean train loss:  2.26018601e-05, mean val. loss:  1.99055088e+00\n",
      "Epoch: 8200 mean train loss:  2.28844292e-05, mean val. loss:  1.99039507e+00\n",
      "Epoch: 8201 mean train loss:  2.26381817e-05, mean val. loss:  1.99026358e+00\n",
      "Epoch: 8202 mean train loss:  2.27239798e-05, mean val. loss:  1.99013233e+00\n",
      "Epoch: 8203 mean train loss:  2.34032050e-05, mean val. loss:  1.98999953e+00\n",
      "Epoch: 8204 mean train loss:  2.28934223e-05, mean val. loss:  1.98987031e+00\n",
      "Epoch: 8205 mean train loss:  2.28681602e-05, mean val. loss:  1.98975599e+00\n",
      "Epoch: 8206 mean train loss:  2.30378064e-05, mean val. loss:  1.98964858e+00\n",
      "Epoch: 8207 mean train loss:  2.27917044e-05, mean val. loss:  1.98956525e+00\n",
      "Epoch: 8208 mean train loss:  2.28924910e-05, mean val. loss:  1.98946869e+00\n",
      "Epoch: 8209 mean train loss:  2.30969163e-05, mean val. loss:  1.98940361e+00\n",
      "Epoch: 8210 mean train loss:  2.30103324e-05, mean val. loss:  1.98935080e+00\n",
      "Epoch: 8211 mean train loss:  2.29323341e-05, mean val. loss:  1.98931956e+00\n",
      "Epoch: 8212 mean train loss:  2.29441212e-05, mean val. loss:  1.98929036e+00\n",
      "Epoch: 8213 mean train loss:  2.29357393e-05, mean val. loss:  1.98926342e+00\n",
      "Epoch: 8214 mean train loss:  2.26600387e-05, mean val. loss:  1.98924458e+00\n",
      "Epoch: 8215 mean train loss:  2.28634453e-05, mean val. loss:  1.98925066e+00\n",
      "Epoch: 8216 mean train loss:  2.29599245e-05, mean val. loss:  1.98925900e+00\n",
      "Epoch: 8217 mean train loss:  2.28928111e-05, mean val. loss:  1.98928273e+00\n",
      "Epoch: 8218 mean train loss:  2.27355631e-05, mean val. loss:  1.98932278e+00\n",
      "Epoch: 8219 mean train loss:  2.26276461e-05, mean val. loss:  1.98938394e+00\n",
      "Epoch: 8220 mean train loss:  2.24629184e-05, mean val. loss:  1.98943663e+00\n",
      "Epoch: 8221 mean train loss:  2.24927498e-05, mean val. loss:  1.98951232e+00\n",
      "Epoch: 8222 mean train loss:  2.25330004e-05, mean val. loss:  1.98960483e+00\n",
      "Epoch: 8223 mean train loss:  2.25737167e-05, mean val. loss:  1.98970211e+00\n",
      "Epoch: 8224 mean train loss:  2.30389414e-05, mean val. loss:  1.98983872e+00\n",
      "Epoch: 8225 mean train loss:  2.29362340e-05, mean val. loss:  1.98996258e+00\n",
      "Epoch: 8226 mean train loss:  2.27163837e-05, mean val. loss:  1.99007785e+00\n",
      "Epoch: 8227 mean train loss:  2.28077697e-05, mean val. loss:  1.99020123e+00\n",
      "Epoch: 8228 mean train loss:  2.26075354e-05, mean val. loss:  1.99035144e+00\n",
      "Epoch: 8229 mean train loss:  2.25688564e-05, mean val. loss:  1.99049616e+00\n",
      "Epoch: 8230 mean train loss:  2.29923753e-05, mean val. loss:  1.99062061e+00\n",
      "Epoch: 8231 mean train loss:  2.29438883e-05, mean val. loss:  1.99073863e+00\n",
      "Epoch: 8232 mean train loss:  2.24634423e-05, mean val. loss:  1.99087942e+00\n",
      "Epoch: 8233 mean train loss:  2.27032579e-05, mean val. loss:  1.99099600e+00\n",
      "Epoch: 8234 mean train loss:  2.27158598e-05, mean val. loss:  1.99110007e+00\n",
      "Epoch: 8235 mean train loss:  2.28832650e-05, mean val. loss:  1.99117517e+00\n",
      "Epoch: 8236 mean train loss:  2.24350952e-05, mean val. loss:  1.99123585e+00\n",
      "Epoch: 8237 mean train loss:  2.24885298e-05, mean val. loss:  1.99129200e+00\n",
      "Epoch: 8238 mean train loss:  2.29155412e-05, mean val. loss:  1.99133956e+00\n",
      "Epoch: 8239 mean train loss:  2.25912954e-05, mean val. loss:  1.99140143e+00\n",
      "Epoch: 8240 mean train loss:  2.27400742e-05, mean val. loss:  1.99148571e+00\n",
      "Epoch: 8241 mean train loss:  2.28062854e-05, mean val. loss:  1.99154782e+00\n",
      "Epoch: 8242 mean train loss:  2.28130375e-05, mean val. loss:  1.99157786e+00\n",
      "Epoch: 8243 mean train loss:  2.26324482e-05, mean val. loss:  1.99156809e+00\n",
      "Epoch: 8244 mean train loss:  2.26847769e-05, mean val. loss:  1.99155438e+00\n",
      "Epoch: 8245 mean train loss:  2.30335863e-05, mean val. loss:  1.99155903e+00\n",
      "Epoch: 8246 mean train loss:  2.29561410e-05, mean val. loss:  1.99157286e+00\n",
      "Epoch: 8247 mean train loss:  2.30443839e-05, mean val. loss:  1.99152350e+00\n",
      "Epoch: 8248 mean train loss:  2.26512202e-05, mean val. loss:  1.99144602e+00\n",
      "Epoch: 8249 mean train loss:  2.26631819e-05, mean val. loss:  1.99136734e+00\n",
      "Epoch: 8250 mean train loss:  2.22968229e-05, mean val. loss:  1.99124324e+00\n",
      "Epoch: 8251 mean train loss:  2.29112338e-05, mean val. loss:  1.99109435e+00\n",
      "Epoch: 8252 mean train loss:  2.25194090e-05, mean val. loss:  1.99095917e+00\n",
      "Epoch: 8253 mean train loss:  2.28033750e-05, mean val. loss:  1.99080682e+00\n",
      "Epoch: 8254 mean train loss:  2.22215604e-05, mean val. loss:  1.99066663e+00\n",
      "Epoch: 8255 mean train loss:  2.28798017e-05, mean val. loss:  1.99050581e+00\n",
      "Epoch: 8256 mean train loss:  2.28201679e-05, mean val. loss:  1.99034452e+00\n",
      "Epoch: 8257 mean train loss:  2.27905402e-05, mean val. loss:  1.99017978e+00\n",
      "Epoch: 8258 mean train loss:  2.28569552e-05, mean val. loss:  1.99004221e+00\n",
      "Epoch: 8259 mean train loss:  2.25972908e-05, mean val. loss:  1.98991358e+00\n",
      "Epoch: 8260 mean train loss:  2.26198463e-05, mean val. loss:  1.98981774e+00\n",
      "Epoch: 8261 mean train loss:  2.28795689e-05, mean val. loss:  1.98968136e+00\n",
      "Epoch: 8262 mean train loss:  2.28935678e-05, mean val. loss:  1.98957157e+00\n",
      "Epoch: 8263 mean train loss:  2.29251164e-05, mean val. loss:  1.98946619e+00\n",
      "Epoch: 8264 mean train loss:  2.30645819e-05, mean val. loss:  1.98939276e+00\n",
      "Epoch: 8265 mean train loss:  2.27054988e-05, mean val. loss:  1.98935056e+00\n",
      "Epoch: 8266 mean train loss:  2.27020937e-05, mean val. loss:  1.98934078e+00\n",
      "Epoch: 8267 mean train loss:  2.26192060e-05, mean val. loss:  1.98937345e+00\n",
      "Epoch: 8268 mean train loss:  2.27587880e-05, mean val. loss:  1.98946095e+00\n",
      "Epoch: 8269 mean train loss:  2.27034034e-05, mean val. loss:  1.98956990e+00\n",
      "Epoch: 8270 mean train loss:  2.26460979e-05, mean val. loss:  1.98972428e+00\n",
      "Epoch: 8271 mean train loss:  2.29956058e-05, mean val. loss:  1.98989284e+00\n",
      "Epoch: 8272 mean train loss:  2.21722876e-05, mean val. loss:  1.99008906e+00\n",
      "Epoch: 8273 mean train loss:  2.26646080e-05, mean val. loss:  1.99027705e+00\n",
      "Epoch: 8274 mean train loss:  2.26392585e-05, mean val. loss:  1.99047589e+00\n",
      "Epoch: 8275 mean train loss:  2.29504367e-05, mean val. loss:  1.99066687e+00\n",
      "Epoch: 8276 mean train loss:  2.25954864e-05, mean val. loss:  1.99085844e+00\n",
      "Epoch: 8277 mean train loss:  2.25650438e-05, mean val. loss:  1.99104512e+00\n",
      "Epoch: 8278 mean train loss:  2.28268036e-05, mean val. loss:  1.99124837e+00\n",
      "Epoch: 8279 mean train loss:  2.28112622e-05, mean val. loss:  1.99142230e+00\n",
      "Epoch: 8280 mean train loss:  2.24834075e-05, mean val. loss:  1.99162662e+00\n",
      "Epoch: 8281 mean train loss:  2.24454270e-05, mean val. loss:  1.99186146e+00\n",
      "Epoch: 8282 mean train loss:  2.28821300e-05, mean val. loss:  1.99205160e+00\n",
      "Epoch: 8283 mean train loss:  2.26859411e-05, mean val. loss:  1.99220920e+00\n",
      "Epoch: 8284 mean train loss:  2.27240380e-05, mean val. loss:  1.99234045e+00\n",
      "Epoch: 8285 mean train loss:  2.28239514e-05, mean val. loss:  1.99245393e+00\n",
      "Epoch: 8286 mean train loss:  2.27416167e-05, mean val. loss:  1.99255002e+00\n",
      "Epoch: 8287 mean train loss:  2.28247663e-05, mean val. loss:  1.99262059e+00\n",
      "Epoch: 8288 mean train loss:  2.27731070e-05, mean val. loss:  1.99267733e+00\n",
      "Epoch: 8289 mean train loss:  2.26924603e-05, mean val. loss:  1.99271441e+00\n",
      "Epoch: 8290 mean train loss:  2.22567760e-05, mean val. loss:  1.99273849e+00\n",
      "Epoch: 8291 mean train loss:  2.28131248e-05, mean val. loss:  1.99275076e+00\n",
      "Epoch: 8292 mean train loss:  2.28435383e-05, mean val. loss:  1.99269342e+00\n",
      "Epoch: 8293 mean train loss:  2.28470890e-05, mean val. loss:  1.99260902e+00\n",
      "Epoch: 8294 mean train loss:  2.26042175e-05, mean val. loss:  1.99249721e+00\n",
      "Epoch: 8295 mean train loss:  2.27008422e-05, mean val. loss:  1.99238288e+00\n",
      "Epoch: 8296 mean train loss:  2.25185358e-05, mean val. loss:  1.99226415e+00\n",
      "Epoch: 8297 mean train loss:  2.27772689e-05, mean val. loss:  1.99213374e+00\n",
      "Epoch: 8298 mean train loss:  2.25589029e-05, mean val. loss:  1.99198341e+00\n",
      "Epoch: 8299 mean train loss:  2.27286364e-05, mean val. loss:  1.99181521e+00\n",
      "Epoch: 8300 mean train loss:  2.28640565e-05, mean val. loss:  1.99165690e+00\n",
      "Epoch: 8301 mean train loss:  2.27820710e-05, mean val. loss:  1.99151301e+00\n",
      "Epoch: 8302 mean train loss:  2.24972027e-05, mean val. loss:  1.99137938e+00\n",
      "Epoch: 8303 mean train loss:  2.24150717e-05, mean val. loss:  1.99122584e+00\n",
      "Epoch: 8304 mean train loss:  2.27786659e-05, mean val. loss:  1.99110079e+00\n",
      "Epoch: 8305 mean train loss:  2.26090779e-05, mean val. loss:  1.99098766e+00\n",
      "Epoch: 8306 mean train loss:  2.24987452e-05, mean val. loss:  1.99087894e+00\n",
      "Epoch: 8307 mean train loss:  2.25062540e-05, mean val. loss:  1.99083817e+00\n",
      "Epoch: 8308 mean train loss:  2.27945275e-05, mean val. loss:  1.99082541e+00\n",
      "Epoch: 8309 mean train loss:  2.26143165e-05, mean val. loss:  1.99078882e+00\n",
      "Epoch: 8310 mean train loss:  2.26711272e-05, mean val. loss:  1.99079597e+00\n",
      "Epoch: 8311 mean train loss:  2.28387653e-05, mean val. loss:  1.99079967e+00\n",
      "Epoch: 8312 mean train loss:  2.27814890e-05, mean val. loss:  1.99081933e+00\n",
      "Epoch: 8313 mean train loss:  2.25317490e-05, mean val. loss:  1.99086714e+00\n",
      "Epoch: 8314 mean train loss:  2.26233387e-05, mean val. loss:  1.99092829e+00\n",
      "Epoch: 8315 mean train loss:  2.24600080e-05, mean val. loss:  1.99100566e+00\n",
      "Epoch: 8316 mean train loss:  2.29049183e-05, mean val. loss:  1.99105275e+00\n",
      "Epoch: 8317 mean train loss:  2.23169045e-05, mean val. loss:  1.99109101e+00\n",
      "Epoch: 8318 mean train loss:  2.25778494e-05, mean val. loss:  1.99112821e+00\n",
      "Epoch: 8319 mean train loss:  2.28428980e-05, mean val. loss:  1.99114168e+00\n",
      "Epoch: 8320 mean train loss:  2.24591349e-05, mean val. loss:  1.99118197e+00\n",
      "Epoch: 8321 mean train loss:  2.27445853e-05, mean val. loss:  1.99121320e+00\n",
      "Epoch: 8322 mean train loss:  2.27391720e-05, mean val. loss:  1.99125159e+00\n",
      "Epoch: 8323 mean train loss:  2.26577104e-05, mean val. loss:  1.99127269e+00\n",
      "Epoch: 8324 mean train loss:  2.30662699e-05, mean val. loss:  1.99124134e+00\n",
      "Epoch: 8325 mean train loss:  2.26489792e-05, mean val. loss:  1.99121559e+00\n",
      "Epoch: 8326 mean train loss:  2.23341631e-05, mean val. loss:  1.99119353e+00\n",
      "Epoch: 8327 mean train loss:  2.24858231e-05, mean val. loss:  1.99116647e+00\n",
      "Epoch: 8328 mean train loss:  2.26580887e-05, mean val. loss:  1.99113190e+00\n",
      "Epoch: 8329 mean train loss:  2.28692661e-05, mean val. loss:  1.99108088e+00\n",
      "Epoch: 8330 mean train loss:  2.26531702e-05, mean val. loss:  1.99099267e+00\n",
      "Epoch: 8331 mean train loss:  2.28717690e-05, mean val. loss:  1.99092472e+00\n",
      "Epoch: 8332 mean train loss:  2.27342825e-05, mean val. loss:  1.99084306e+00\n",
      "Epoch: 8333 mean train loss:  2.26015109e-05, mean val. loss:  1.99078989e+00\n",
      "Epoch: 8334 mean train loss:  2.28952849e-05, mean val. loss:  1.99074638e+00\n",
      "Epoch: 8335 mean train loss:  2.25249096e-05, mean val. loss:  1.99069726e+00\n",
      "Epoch: 8336 mean train loss:  2.27927230e-05, mean val. loss:  1.99067545e+00\n",
      "Epoch: 8337 mean train loss:  2.22608505e-05, mean val. loss:  1.99067080e+00\n",
      "Epoch: 8338 mean train loss:  2.24478135e-05, mean val. loss:  1.99068260e+00\n",
      "Epoch: 8339 mean train loss:  2.25183321e-05, mean val. loss:  1.99070454e+00\n",
      "Epoch: 8340 mean train loss:  2.24326504e-05, mean val. loss:  1.99074101e+00\n",
      "Epoch: 8341 mean train loss:  2.25520344e-05, mean val. loss:  1.99080884e+00\n",
      "Epoch: 8342 mean train loss:  2.28049757e-05, mean val. loss:  1.99086487e+00\n",
      "Epoch: 8343 mean train loss:  2.23721145e-05, mean val. loss:  1.99094343e+00\n",
      "Epoch: 8344 mean train loss:  2.27134442e-05, mean val. loss:  1.99102771e+00\n",
      "Epoch: 8345 mean train loss:  2.22654198e-05, mean val. loss:  1.99114227e+00\n",
      "Epoch: 8346 mean train loss:  2.27643468e-05, mean val. loss:  1.99122763e+00\n",
      "Epoch: 8347 mean train loss:  2.28834688e-05, mean val. loss:  1.99129379e+00\n",
      "Epoch: 8348 mean train loss:  2.26824777e-05, mean val. loss:  1.99138570e+00\n",
      "Epoch: 8349 mean train loss:  2.23518000e-05, mean val. loss:  1.99144459e+00\n",
      "Epoch: 8350 mean train loss:  2.23801471e-05, mean val. loss:  1.99151456e+00\n",
      "Epoch: 8351 mean train loss:  2.23985408e-05, mean val. loss:  1.99160337e+00\n",
      "Epoch: 8352 mean train loss:  2.28169956e-05, mean val. loss:  1.99170947e+00\n",
      "Epoch: 8353 mean train loss:  2.27705459e-05, mean val. loss:  1.99182689e+00\n",
      "Epoch: 8354 mean train loss:  2.27413548e-05, mean val. loss:  1.99195194e+00\n",
      "Epoch: 8355 mean train loss:  2.24259566e-05, mean val. loss:  1.99208331e+00\n",
      "Epoch: 8356 mean train loss:  2.29298894e-05, mean val. loss:  1.99216497e+00\n",
      "Epoch: 8357 mean train loss:  2.26052944e-05, mean val. loss:  1.99224961e+00\n",
      "Epoch: 8358 mean train loss:  2.25100084e-05, mean val. loss:  1.99230289e+00\n",
      "Epoch: 8359 mean train loss:  2.22379749e-05, mean val. loss:  1.99234247e+00\n",
      "Epoch: 8360 mean train loss:  2.27342825e-05, mean val. loss:  1.99237382e+00\n",
      "Epoch: 8361 mean train loss:  2.23341049e-05, mean val. loss:  1.99236059e+00\n",
      "Epoch: 8362 mean train loss:  2.26328848e-05, mean val. loss:  1.99234688e+00\n",
      "Epoch: 8363 mean train loss:  2.29911820e-05, mean val. loss:  1.99232221e+00\n",
      "Epoch: 8364 mean train loss:  2.23935058e-05, mean val. loss:  1.99230850e+00\n",
      "Epoch: 8365 mean train loss:  2.23187963e-05, mean val. loss:  1.99231076e+00\n",
      "Epoch: 8366 mean train loss:  2.26832635e-05, mean val. loss:  1.99232185e+00\n",
      "Epoch: 8367 mean train loss:  2.25533440e-05, mean val. loss:  1.99234045e+00\n",
      "Epoch: 8368 mean train loss:  2.28089921e-05, mean val. loss:  1.99233723e+00\n",
      "Epoch: 8369 mean train loss:  2.27791897e-05, mean val. loss:  1.99234772e+00\n",
      "Epoch: 8370 mean train loss:  2.24455143e-05, mean val. loss:  1.99237442e+00\n",
      "Epoch: 8371 mean train loss:  2.27781420e-05, mean val. loss:  1.99236596e+00\n",
      "Epoch: 8372 mean train loss:  2.26965349e-05, mean val. loss:  1.99235547e+00\n",
      "Epoch: 8373 mean train loss:  2.24611722e-05, mean val. loss:  1.99231243e+00\n",
      "Epoch: 8374 mean train loss:  2.25467957e-05, mean val. loss:  1.99223971e+00\n",
      "Epoch: 8375 mean train loss:  2.26647535e-05, mean val. loss:  1.99217498e+00\n",
      "Epoch: 8376 mean train loss:  2.26618722e-05, mean val. loss:  1.99210167e+00\n",
      "Epoch: 8377 mean train loss:  2.25912081e-05, mean val. loss:  1.99201143e+00\n",
      "Epoch: 8378 mean train loss:  2.22968811e-05, mean val. loss:  1.99190390e+00\n",
      "Epoch: 8379 mean train loss:  2.25838157e-05, mean val. loss:  1.99179471e+00\n",
      "Epoch: 8380 mean train loss:  2.29508732e-05, mean val. loss:  1.99168003e+00\n",
      "Epoch: 8381 mean train loss:  2.28440331e-05, mean val. loss:  1.99154305e+00\n",
      "Epoch: 8382 mean train loss:  2.25011609e-05, mean val. loss:  1.99142540e+00\n",
      "Epoch: 8383 mean train loss:  2.26621341e-05, mean val. loss:  1.99132121e+00\n",
      "Epoch: 8384 mean train loss:  2.32029706e-05, mean val. loss:  1.99118567e+00\n",
      "Epoch: 8385 mean train loss:  2.26396660e-05, mean val. loss:  1.99109340e+00\n",
      "Epoch: 8386 mean train loss:  2.25238618e-05, mean val. loss:  1.99103510e+00\n",
      "Epoch: 8387 mean train loss:  2.27290730e-05, mean val. loss:  1.99097466e+00\n",
      "Epoch: 8388 mean train loss:  2.28013378e-05, mean val. loss:  1.99093223e+00\n",
      "Epoch: 8389 mean train loss:  2.26052071e-05, mean val. loss:  1.99090600e+00\n",
      "Epoch: 8390 mean train loss:  2.27108540e-05, mean val. loss:  1.99089885e+00\n",
      "Epoch: 8391 mean train loss:  2.26443808e-05, mean val. loss:  1.99089444e+00\n",
      "Epoch: 8392 mean train loss:  2.26240081e-05, mean val. loss:  1.99087203e+00\n",
      "Epoch: 8393 mean train loss:  2.25243275e-05, mean val. loss:  1.99087274e+00\n",
      "Epoch: 8394 mean train loss:  2.24114920e-05, mean val. loss:  1.99090171e+00\n",
      "Epoch: 8395 mean train loss:  2.26806151e-05, mean val. loss:  1.99093938e+00\n",
      "Epoch: 8396 mean train loss:  2.25229887e-05, mean val. loss:  1.99097586e+00\n",
      "Epoch: 8397 mean train loss:  2.27215642e-05, mean val. loss:  1.99103224e+00\n",
      "Epoch: 8398 mean train loss:  2.22811068e-05, mean val. loss:  1.99112725e+00\n",
      "Epoch: 8399 mean train loss:  2.25747062e-05, mean val. loss:  1.99124229e+00\n",
      "Epoch: 8400 mean train loss:  2.27165874e-05, mean val. loss:  1.99134207e+00\n",
      "Epoch: 8401 mean train loss:  2.22035742e-05, mean val. loss:  1.99145627e+00\n",
      "Epoch: 8402 mean train loss:  2.25328840e-05, mean val. loss:  1.99156857e+00\n",
      "Epoch: 8403 mean train loss:  2.21942901e-05, mean val. loss:  1.99168491e+00\n",
      "Epoch: 8404 mean train loss:  2.27179553e-05, mean val. loss:  1.99180698e+00\n",
      "Epoch: 8405 mean train loss:  2.26063712e-05, mean val. loss:  1.99189973e+00\n",
      "Epoch: 8406 mean train loss:  2.28181307e-05, mean val. loss:  1.99197698e+00\n",
      "Epoch: 8407 mean train loss:  2.24120158e-05, mean val. loss:  1.99204671e+00\n",
      "Epoch: 8408 mean train loss:  2.24758405e-05, mean val. loss:  1.99209440e+00\n",
      "Epoch: 8409 mean train loss:  2.24181858e-05, mean val. loss:  1.99211621e+00\n",
      "Epoch: 8410 mean train loss:  2.25387630e-05, mean val. loss:  1.99214244e+00\n",
      "Epoch: 8411 mean train loss:  2.29579164e-05, mean val. loss:  1.99219942e+00\n",
      "Epoch: 8412 mean train loss:  2.26912671e-05, mean val. loss:  1.99223685e+00\n",
      "Epoch: 8413 mean train loss:  2.22746166e-05, mean val. loss:  1.99224234e+00\n",
      "Epoch: 8414 mean train loss:  2.23528477e-05, mean val. loss:  1.99224377e+00\n",
      "Epoch: 8415 mean train loss:  2.23837269e-05, mean val. loss:  1.99226677e+00\n",
      "Epoch: 8416 mean train loss:  2.23727548e-05, mean val. loss:  1.99229729e+00\n",
      "Epoch: 8417 mean train loss:  2.23302050e-05, mean val. loss:  1.99232769e+00\n",
      "Epoch: 8418 mean train loss:  2.24932155e-05, mean val. loss:  1.99236965e+00\n",
      "Epoch: 8419 mean train loss:  2.28572462e-05, mean val. loss:  1.99236834e+00\n",
      "Epoch: 8420 mean train loss:  2.21839291e-05, mean val. loss:  1.99239624e+00\n",
      "Epoch: 8421 mean train loss:  2.25086696e-05, mean val. loss:  1.99245167e+00\n",
      "Epoch: 8422 mean train loss:  2.26662087e-05, mean val. loss:  1.99248898e+00\n",
      "Epoch: 8423 mean train loss:  2.27218261e-05, mean val. loss:  1.99254191e+00\n",
      "Epoch: 8424 mean train loss:  2.23340176e-05, mean val. loss:  1.99259102e+00\n",
      "Epoch: 8425 mean train loss:  2.24919931e-05, mean val. loss:  1.99266791e+00\n",
      "Epoch: 8426 mean train loss:  2.27861165e-05, mean val. loss:  1.99271238e+00\n",
      "Epoch: 8427 mean train loss:  2.26381817e-05, mean val. loss:  1.99277604e+00\n",
      "Epoch: 8428 mean train loss:  2.27298588e-05, mean val. loss:  1.99285400e+00\n",
      "Epoch: 8429 mean train loss:  2.24540418e-05, mean val. loss:  1.99290550e+00\n",
      "Epoch: 8430 mean train loss:  2.26450793e-05, mean val. loss:  1.99295008e+00\n",
      "Epoch: 8431 mean train loss:  2.24607356e-05, mean val. loss:  1.99302292e+00\n",
      "Epoch: 8432 mean train loss:  2.26910342e-05, mean val. loss:  1.99307168e+00\n",
      "Epoch: 8433 mean train loss:  2.27346318e-05, mean val. loss:  1.99309206e+00\n",
      "Epoch: 8434 mean train loss:  2.27991259e-05, mean val. loss:  1.99305499e+00\n",
      "Epoch: 8435 mean train loss:  2.26437696e-05, mean val. loss:  1.99299455e+00\n",
      "Epoch: 8436 mean train loss:  2.27470009e-05, mean val. loss:  1.99292827e+00\n",
      "Epoch: 8437 mean train loss:  2.23773532e-05, mean val. loss:  1.99284470e+00\n",
      "Epoch: 8438 mean train loss:  2.27318669e-05, mean val. loss:  1.99273717e+00\n",
      "Epoch: 8439 mean train loss:  2.21292430e-05, mean val. loss:  1.99262047e+00\n",
      "Epoch: 8440 mean train loss:  2.26078846e-05, mean val. loss:  1.99253523e+00\n",
      "Epoch: 8441 mean train loss:  2.24784890e-05, mean val. loss:  1.99246836e+00\n",
      "Epoch: 8442 mean train loss:  2.23326788e-05, mean val. loss:  1.99243534e+00\n",
      "Epoch: 8443 mean train loss:  2.23823008e-05, mean val. loss:  1.99240494e+00\n",
      "Epoch: 8444 mean train loss:  2.23701354e-05, mean val. loss:  1.99240816e+00\n",
      "Epoch: 8445 mean train loss:  2.28147837e-05, mean val. loss:  1.99241841e+00\n",
      "Epoch: 8446 mean train loss:  2.25322729e-05, mean val. loss:  1.99241734e+00\n",
      "Epoch: 8447 mean train loss:  2.27845157e-05, mean val. loss:  1.99244595e+00\n",
      "Epoch: 8448 mean train loss:  2.26509001e-05, mean val. loss:  1.99247193e+00\n",
      "Epoch: 8449 mean train loss:  2.27528508e-05, mean val. loss:  1.99251330e+00\n",
      "Epoch: 8450 mean train loss:  2.23124516e-05, mean val. loss:  1.99259329e+00\n",
      "Epoch: 8451 mean train loss:  2.24949326e-05, mean val. loss:  1.99270797e+00\n",
      "Epoch: 8452 mean train loss:  2.21689115e-05, mean val. loss:  1.99284875e+00\n",
      "Epoch: 8453 mean train loss:  2.24991236e-05, mean val. loss:  1.99296260e+00\n",
      "Epoch: 8454 mean train loss:  2.23613461e-05, mean val. loss:  1.99309576e+00\n",
      "Epoch: 8455 mean train loss:  2.24284013e-05, mean val. loss:  1.99321687e+00\n",
      "Epoch: 8456 mean train loss:  2.21626542e-05, mean val. loss:  1.99332380e+00\n",
      "Epoch: 8457 mean train loss:  2.22030794e-05, mean val. loss:  1.99344587e+00\n",
      "Epoch: 8458 mean train loss:  2.23908282e-05, mean val. loss:  1.99358463e+00\n",
      "Epoch: 8459 mean train loss:  2.20851507e-05, mean val. loss:  1.99371076e+00\n",
      "Epoch: 8460 mean train loss:  2.24297983e-05, mean val. loss:  1.99387300e+00\n",
      "Epoch: 8461 mean train loss:  2.22483650e-05, mean val. loss:  1.99402678e+00\n",
      "Epoch: 8462 mean train loss:  2.25102121e-05, mean val. loss:  1.99416530e+00\n",
      "Epoch: 8463 mean train loss:  2.25249678e-05, mean val. loss:  1.99429953e+00\n",
      "Epoch: 8464 mean train loss:  2.28159188e-05, mean val. loss:  1.99442041e+00\n",
      "Epoch: 8465 mean train loss:  2.28549761e-05, mean val. loss:  1.99449623e+00\n",
      "Epoch: 8466 mean train loss:  2.27451674e-05, mean val. loss:  1.99456239e+00\n",
      "Epoch: 8467 mean train loss:  2.25681288e-05, mean val. loss:  1.99459875e+00\n",
      "Epoch: 8468 mean train loss:  2.25469121e-05, mean val. loss:  1.99462283e+00\n",
      "Epoch: 8469 mean train loss:  2.23145180e-05, mean val. loss:  1.99460077e+00\n",
      "Epoch: 8470 mean train loss:  2.28136487e-05, mean val. loss:  1.99455655e+00\n",
      "Epoch: 8471 mean train loss:  2.23607640e-05, mean val. loss:  1.99451685e+00\n",
      "Epoch: 8472 mean train loss:  2.24346295e-05, mean val. loss:  1.99447501e+00\n",
      "Epoch: 8473 mean train loss:  2.24975229e-05, mean val. loss:  1.99439514e+00\n",
      "Epoch: 8474 mean train loss:  2.28198769e-05, mean val. loss:  1.99431288e+00\n",
      "Epoch: 8475 mean train loss:  2.30472651e-05, mean val. loss:  1.99421299e+00\n",
      "Epoch: 8476 mean train loss:  2.26700213e-05, mean val. loss:  1.99411952e+00\n",
      "Epoch: 8477 mean train loss:  2.27524724e-05, mean val. loss:  1.99402452e+00\n",
      "Epoch: 8478 mean train loss:  2.23411771e-05, mean val. loss:  1.99390948e+00\n",
      "Epoch: 8479 mean train loss:  2.23198440e-05, mean val. loss:  1.99378908e+00\n",
      "Epoch: 8480 mean train loss:  2.27556739e-05, mean val. loss:  1.99368942e+00\n",
      "Epoch: 8481 mean train loss:  2.20343936e-05, mean val. loss:  1.99363196e+00\n",
      "Epoch: 8482 mean train loss:  2.25371623e-05, mean val. loss:  1.99354196e+00\n",
      "Epoch: 8483 mean train loss:  2.26259872e-05, mean val. loss:  1.99342239e+00\n",
      "Epoch: 8484 mean train loss:  2.21479568e-05, mean val. loss:  1.99329352e+00\n",
      "Epoch: 8485 mean train loss:  2.27960409e-05, mean val. loss:  1.99314964e+00\n",
      "Epoch: 8486 mean train loss:  2.22491217e-05, mean val. loss:  1.99302065e+00\n",
      "Epoch: 8487 mean train loss:  2.23810493e-05, mean val. loss:  1.99292195e+00\n",
      "Epoch: 8488 mean train loss:  2.24626856e-05, mean val. loss:  1.99282587e+00\n",
      "Epoch: 8489 mean train loss:  2.22568051e-05, mean val. loss:  1.99276340e+00\n",
      "Epoch: 8490 mean train loss:  2.22099188e-05, mean val. loss:  1.99272084e+00\n",
      "Epoch: 8491 mean train loss:  2.23570096e-05, mean val. loss:  1.99270749e+00\n",
      "Epoch: 8492 mean train loss:  2.27758137e-05, mean val. loss:  1.99270725e+00\n",
      "Epoch: 8493 mean train loss:  2.21644295e-05, mean val. loss:  1.99276316e+00\n",
      "Epoch: 8494 mean train loss:  2.26374250e-05, mean val. loss:  1.99283922e+00\n",
      "Epoch: 8495 mean train loss:  2.24421383e-05, mean val. loss:  1.99291468e+00\n",
      "Epoch: 8496 mean train loss:  2.21460941e-05, mean val. loss:  1.99304676e+00\n",
      "Epoch: 8497 mean train loss:  2.26710981e-05, mean val. loss:  1.99316955e+00\n",
      "Epoch: 8498 mean train loss:  2.27110286e-05, mean val. loss:  1.99327004e+00\n",
      "Epoch: 8499 mean train loss:  2.21927476e-05, mean val. loss:  1.99335790e+00\n",
      "Epoch: 8500 mean train loss:  2.24732212e-05, mean val. loss:  1.99347782e+00\n",
      "Epoch: 8501 mean train loss:  2.30255537e-05, mean val. loss:  1.99358761e+00\n",
      "Epoch: 8502 mean train loss:  2.24122487e-05, mean val. loss:  1.99370158e+00\n",
      "Epoch: 8503 mean train loss:  2.23627139e-05, mean val. loss:  1.99382281e+00\n",
      "Epoch: 8504 mean train loss:  2.26247939e-05, mean val. loss:  1.99394262e+00\n",
      "Epoch: 8505 mean train loss:  2.25232216e-05, mean val. loss:  1.99407446e+00\n",
      "Epoch: 8506 mean train loss:  2.22238596e-05, mean val. loss:  1.99422419e+00\n",
      "Epoch: 8507 mean train loss:  2.25084368e-05, mean val. loss:  1.99439669e+00\n",
      "Epoch: 8508 mean train loss:  2.26931297e-05, mean val. loss:  1.99456143e+00\n",
      "Epoch: 8509 mean train loss:  2.29613215e-05, mean val. loss:  1.99470234e+00\n",
      "Epoch: 8510 mean train loss:  2.24972318e-05, mean val. loss:  1.99483669e+00\n",
      "Epoch: 8511 mean train loss:  2.24539253e-05, mean val. loss:  1.99494529e+00\n",
      "Epoch: 8512 mean train loss:  2.25093972e-05, mean val. loss:  1.99502480e+00\n",
      "Epoch: 8513 mean train loss:  2.23073876e-05, mean val. loss:  1.99510062e+00\n",
      "Epoch: 8514 mean train loss:  2.21735390e-05, mean val. loss:  1.99520016e+00\n",
      "Epoch: 8515 mean train loss:  2.21081718e-05, mean val. loss:  1.99529767e+00\n",
      "Epoch: 8516 mean train loss:  2.24217074e-05, mean val. loss:  1.99537969e+00\n",
      "Epoch: 8517 mean train loss:  2.25106778e-05, mean val. loss:  1.99545968e+00\n",
      "Epoch: 8518 mean train loss:  2.25586700e-05, mean val. loss:  1.99548757e+00\n",
      "Epoch: 8519 mean train loss:  2.23434472e-05, mean val. loss:  1.99550247e+00\n",
      "Epoch: 8520 mean train loss:  2.21277878e-05, mean val. loss:  1.99548352e+00\n",
      "Epoch: 8521 mean train loss:  2.26845150e-05, mean val. loss:  1.99544728e+00\n",
      "Epoch: 8522 mean train loss:  2.23083480e-05, mean val. loss:  1.99539840e+00\n",
      "Epoch: 8523 mean train loss:  2.23425450e-05, mean val. loss:  1.99535239e+00\n",
      "Epoch: 8524 mean train loss:  2.20415241e-05, mean val. loss:  1.99532354e+00\n",
      "Epoch: 8525 mean train loss:  2.24556716e-05, mean val. loss:  1.99530804e+00\n",
      "Epoch: 8526 mean train loss:  2.24550604e-05, mean val. loss:  1.99528408e+00\n",
      "Epoch: 8527 mean train loss:  2.24549440e-05, mean val. loss:  1.99526477e+00\n",
      "Epoch: 8528 mean train loss:  2.25040421e-05, mean val. loss:  1.99524391e+00\n",
      "Epoch: 8529 mean train loss:  2.25013064e-05, mean val. loss:  1.99523532e+00\n",
      "Epoch: 8530 mean train loss:  2.24104151e-05, mean val. loss:  1.99524784e+00\n",
      "Epoch: 8531 mean train loss:  2.21604714e-05, mean val. loss:  1.99526358e+00\n",
      "Epoch: 8532 mean train loss:  2.25024123e-05, mean val. loss:  1.99532020e+00\n",
      "Epoch: 8533 mean train loss:  2.21931259e-05, mean val. loss:  1.99536920e+00\n",
      "Epoch: 8534 mean train loss:  2.24814576e-05, mean val. loss:  1.99544108e+00\n",
      "Epoch: 8535 mean train loss:  2.24049727e-05, mean val. loss:  1.99553561e+00\n",
      "Epoch: 8536 mean train loss:  2.18850037e-05, mean val. loss:  1.99567878e+00\n",
      "Epoch: 8537 mean train loss:  2.24060786e-05, mean val. loss:  1.99578691e+00\n",
      "Epoch: 8538 mean train loss:  2.23238312e-05, mean val. loss:  1.99589491e+00\n",
      "Epoch: 8539 mean train loss:  2.23619281e-05, mean val. loss:  1.99600697e+00\n",
      "Epoch: 8540 mean train loss:  2.20703078e-05, mean val. loss:  1.99616337e+00\n",
      "Epoch: 8541 mean train loss:  2.19852955e-05, mean val. loss:  1.99631333e+00\n",
      "Epoch: 8542 mean train loss:  2.24018877e-05, mean val. loss:  1.99646413e+00\n",
      "Epoch: 8543 mean train loss:  2.24999385e-05, mean val. loss:  1.99660468e+00\n",
      "Epoch: 8544 mean train loss:  2.21539522e-05, mean val. loss:  1.99674976e+00\n",
      "Epoch: 8545 mean train loss:  2.23989482e-05, mean val. loss:  1.99689424e+00\n",
      "Epoch: 8546 mean train loss:  2.23703682e-05, mean val. loss:  1.99701333e+00\n",
      "Epoch: 8547 mean train loss:  2.21293885e-05, mean val. loss:  1.99712861e+00\n",
      "Epoch: 8548 mean train loss:  2.24918767e-05, mean val. loss:  1.99723208e+00\n",
      "Epoch: 8549 mean train loss:  2.25332333e-05, mean val. loss:  1.99732912e+00\n",
      "Epoch: 8550 mean train loss:  2.22267990e-05, mean val. loss:  1.99737620e+00\n",
      "Epoch: 8551 mean train loss:  2.23138486e-05, mean val. loss:  1.99740350e+00\n",
      "Epoch: 8552 mean train loss:  2.23709503e-05, mean val. loss:  1.99741518e+00\n",
      "Epoch: 8553 mean train loss:  2.23216775e-05, mean val. loss:  1.99740887e+00\n",
      "Epoch: 8554 mean train loss:  2.22921954e-05, mean val. loss:  1.99739611e+00\n",
      "Epoch: 8555 mean train loss:  2.24055257e-05, mean val. loss:  1.99737310e+00\n",
      "Epoch: 8556 mean train loss:  2.23481911e-05, mean val. loss:  1.99734318e+00\n",
      "Epoch: 8557 mean train loss:  2.20991496e-05, mean val. loss:  1.99730551e+00\n",
      "Epoch: 8558 mean train loss:  2.26677803e-05, mean val. loss:  1.99726844e+00\n",
      "Epoch: 8559 mean train loss:  2.25570693e-05, mean val. loss:  1.99717557e+00\n",
      "Epoch: 8560 mean train loss:  2.20892252e-05, mean val. loss:  1.99708450e+00\n",
      "Epoch: 8561 mean train loss:  2.23552634e-05, mean val. loss:  1.99699283e+00\n",
      "Epoch: 8562 mean train loss:  2.21234222e-05, mean val. loss:  1.99691129e+00\n",
      "Epoch: 8563 mean train loss:  2.22874805e-05, mean val. loss:  1.99683166e+00\n",
      "Epoch: 8564 mean train loss:  2.23021489e-05, mean val. loss:  1.99676442e+00\n",
      "Epoch: 8565 mean train loss:  2.21162918e-05, mean val. loss:  1.99670863e+00\n",
      "Epoch: 8566 mean train loss:  2.23463285e-05, mean val. loss:  1.99664366e+00\n",
      "Epoch: 8567 mean train loss:  2.23077659e-05, mean val. loss:  1.99655974e+00\n",
      "Epoch: 8568 mean train loss:  2.25108524e-05, mean val. loss:  1.99648499e+00\n",
      "Epoch: 8569 mean train loss:  2.20682414e-05, mean val. loss:  1.99640989e+00\n",
      "Epoch: 8570 mean train loss:  2.22085800e-05, mean val. loss:  1.99634862e+00\n",
      "Epoch: 8571 mean train loss:  2.20637012e-05, mean val. loss:  1.99630678e+00\n",
      "Epoch: 8572 mean train loss:  2.21188529e-05, mean val. loss:  1.99627590e+00\n",
      "Epoch: 8573 mean train loss:  2.24044197e-05, mean val. loss:  1.99621010e+00\n",
      "Epoch: 8574 mean train loss:  2.25847180e-05, mean val. loss:  1.99613726e+00\n",
      "Epoch: 8575 mean train loss:  2.22829985e-05, mean val. loss:  1.99607134e+00\n",
      "Epoch: 8576 mean train loss:  2.23735406e-05, mean val. loss:  1.99603295e+00\n",
      "Epoch: 8577 mean train loss:  2.25926633e-05, mean val. loss:  1.99602139e+00\n",
      "Epoch: 8578 mean train loss:  2.23109091e-05, mean val. loss:  1.99601102e+00\n",
      "Epoch: 8579 mean train loss:  2.24606483e-05, mean val. loss:  1.99601877e+00\n",
      "Epoch: 8580 mean train loss:  2.25433614e-05, mean val. loss:  1.99601185e+00\n",
      "Epoch: 8581 mean train loss:  2.19291833e-05, mean val. loss:  1.99604166e+00\n",
      "Epoch: 8582 mean train loss:  2.22448434e-05, mean val. loss:  1.99609196e+00\n",
      "Epoch: 8583 mean train loss:  2.19958893e-05, mean val. loss:  1.99615192e+00\n",
      "Epoch: 8584 mean train loss:  2.24895193e-05, mean val. loss:  1.99622738e+00\n",
      "Epoch: 8585 mean train loss:  2.20694928e-05, mean val. loss:  1.99634409e+00\n",
      "Epoch: 8586 mean train loss:  2.21880910e-05, mean val. loss:  1.99646115e+00\n",
      "Epoch: 8587 mean train loss:  2.23515672e-05, mean val. loss:  1.99656737e+00\n",
      "Epoch: 8588 mean train loss:  2.24563410e-05, mean val. loss:  1.99665272e+00\n",
      "Epoch: 8589 mean train loss:  2.19507201e-05, mean val. loss:  1.99676406e+00\n",
      "Epoch: 8590 mean train loss:  2.25638505e-05, mean val. loss:  1.99688745e+00\n",
      "Epoch: 8591 mean train loss:  2.21793016e-05, mean val. loss:  1.99701798e+00\n",
      "Epoch: 8592 mean train loss:  2.17840134e-05, mean val. loss:  1.99714530e+00\n",
      "Epoch: 8593 mean train loss:  2.23572715e-05, mean val. loss:  1.99725366e+00\n",
      "Epoch: 8594 mean train loss:  2.22958624e-05, mean val. loss:  1.99738181e+00\n",
      "Epoch: 8595 mean train loss:  2.23437964e-05, mean val. loss:  1.99749529e+00\n",
      "Epoch: 8596 mean train loss:  2.22823874e-05, mean val. loss:  1.99761164e+00\n",
      "Epoch: 8597 mean train loss:  2.22133531e-05, mean val. loss:  1.99774790e+00\n",
      "Epoch: 8598 mean train loss:  2.23310490e-05, mean val. loss:  1.99788249e+00\n",
      "Epoch: 8599 mean train loss:  2.19512149e-05, mean val. loss:  1.99796212e+00\n",
      "Epoch: 8600 mean train loss:  2.22493545e-05, mean val. loss:  1.99805808e+00\n",
      "Epoch: 8601 mean train loss:  2.22731032e-05, mean val. loss:  1.99815309e+00\n",
      "Epoch: 8602 mean train loss:  2.24840769e-05, mean val. loss:  1.99822950e+00\n",
      "Epoch: 8603 mean train loss:  2.22423114e-05, mean val. loss:  1.99827516e+00\n",
      "Epoch: 8604 mean train loss:  2.20763613e-05, mean val. loss:  1.99833739e+00\n",
      "Epoch: 8605 mean train loss:  2.25176918e-05, mean val. loss:  1.99836326e+00\n",
      "Epoch: 8606 mean train loss:  2.23367824e-05, mean val. loss:  1.99838185e+00\n",
      "Epoch: 8607 mean train loss:  2.21771188e-05, mean val. loss:  1.99841547e+00\n",
      "Epoch: 8608 mean train loss:  2.21820374e-05, mean val. loss:  1.99844742e+00\n",
      "Epoch: 8609 mean train loss:  2.23913812e-05, mean val. loss:  1.99844742e+00\n",
      "Epoch: 8610 mean train loss:  2.26334087e-05, mean val. loss:  1.99841440e+00\n",
      "Epoch: 8611 mean train loss:  2.22697854e-05, mean val. loss:  1.99838233e+00\n",
      "Epoch: 8612 mean train loss:  2.25363183e-05, mean val. loss:  1.99832642e+00\n",
      "Epoch: 8613 mean train loss:  2.23912939e-05, mean val. loss:  1.99825096e+00\n",
      "Epoch: 8614 mean train loss:  2.22156814e-05, mean val. loss:  1.99817514e+00\n",
      "Epoch: 8615 mean train loss:  2.25164986e-05, mean val. loss:  1.99810493e+00\n",
      "Epoch: 8616 mean train loss:  2.24385876e-05, mean val. loss:  1.99804807e+00\n",
      "Epoch: 8617 mean train loss:  2.20835791e-05, mean val. loss:  1.99799955e+00\n",
      "Epoch: 8618 mean train loss:  2.24809337e-05, mean val. loss:  1.99796152e+00\n",
      "Epoch: 8619 mean train loss:  2.23975221e-05, mean val. loss:  1.99791038e+00\n",
      "Epoch: 8620 mean train loss:  2.22230155e-05, mean val. loss:  1.99787617e+00\n",
      "Epoch: 8621 mean train loss:  2.24286632e-05, mean val. loss:  1.99787498e+00\n",
      "Epoch: 8622 mean train loss:  2.19682115e-05, mean val. loss:  1.99790227e+00\n",
      "Epoch: 8623 mean train loss:  2.24475807e-05, mean val. loss:  1.99793887e+00\n",
      "Epoch: 8624 mean train loss:  2.21105001e-05, mean val. loss:  1.99798584e+00\n",
      "Epoch: 8625 mean train loss:  2.23466486e-05, mean val. loss:  1.99800563e+00\n",
      "Epoch: 8626 mean train loss:  2.23205425e-05, mean val. loss:  1.99805546e+00\n",
      "Epoch: 8627 mean train loss:  2.19609938e-05, mean val. loss:  1.99811018e+00\n",
      "Epoch: 8628 mean train loss:  2.21584924e-05, mean val. loss:  1.99817896e+00\n",
      "Epoch: 8629 mean train loss:  2.24589894e-05, mean val. loss:  1.99822998e+00\n",
      "Epoch: 8630 mean train loss:  2.23399547e-05, mean val. loss:  1.99827588e+00\n",
      "Epoch: 8631 mean train loss:  2.22669914e-05, mean val. loss:  1.99831247e+00\n",
      "Epoch: 8632 mean train loss:  2.22552044e-05, mean val. loss:  1.99836576e+00\n",
      "Epoch: 8633 mean train loss:  2.19794456e-05, mean val. loss:  1.99843466e+00\n",
      "Epoch: 8634 mean train loss:  2.20725487e-05, mean val. loss:  1.99851799e+00\n",
      "Epoch: 8635 mean train loss:  2.22655653e-05, mean val. loss:  1.99859846e+00\n",
      "Epoch: 8636 mean train loss:  2.22648960e-05, mean val. loss:  1.99864948e+00\n",
      "Epoch: 8637 mean train loss:  2.23440584e-05, mean val. loss:  1.99866724e+00\n",
      "Epoch: 8638 mean train loss:  2.23747047e-05, mean val. loss:  1.99868095e+00\n",
      "Epoch: 8639 mean train loss:  2.21704249e-05, mean val. loss:  1.99869800e+00\n",
      "Epoch: 8640 mean train loss:  2.23662064e-05, mean val. loss:  1.99870718e+00\n",
      "Epoch: 8641 mean train loss:  2.16995832e-05, mean val. loss:  1.99872720e+00\n",
      "Epoch: 8642 mean train loss:  2.21692026e-05, mean val. loss:  1.99874759e+00\n",
      "Epoch: 8643 mean train loss:  2.25598633e-05, mean val. loss:  1.99875033e+00\n",
      "Epoch: 8644 mean train loss:  2.22502567e-05, mean val. loss:  1.99870002e+00\n",
      "Epoch: 8645 mean train loss:  2.19356734e-05, mean val. loss:  1.99864078e+00\n",
      "Epoch: 8646 mean train loss:  2.23261304e-05, mean val. loss:  1.99860978e+00\n",
      "Epoch: 8647 mean train loss:  2.23333191e-05, mean val. loss:  1.99852943e+00\n",
      "Epoch: 8648 mean train loss:  2.21476657e-05, mean val. loss:  1.99845111e+00\n",
      "Epoch: 8649 mean train loss:  2.23165553e-05, mean val. loss:  1.99833608e+00\n",
      "Epoch: 8650 mean train loss:  2.23327661e-05, mean val. loss:  1.99825454e+00\n",
      "Epoch: 8651 mean train loss:  2.22906237e-05, mean val. loss:  1.99817991e+00\n",
      "Epoch: 8652 mean train loss:  2.23142561e-05, mean val. loss:  1.99809682e+00\n",
      "Epoch: 8653 mean train loss:  2.23809038e-05, mean val. loss:  1.99802077e+00\n",
      "Epoch: 8654 mean train loss:  2.24294490e-05, mean val. loss:  1.99796176e+00\n",
      "Epoch: 8655 mean train loss:  2.24286632e-05, mean val. loss:  1.99790871e+00\n",
      "Epoch: 8656 mean train loss:  2.19220528e-05, mean val. loss:  1.99785173e+00\n",
      "Epoch: 8657 mean train loss:  2.21058726e-05, mean val. loss:  1.99782836e+00\n",
      "Epoch: 8658 mean train loss:  2.21485680e-05, mean val. loss:  1.99778926e+00\n",
      "Epoch: 8659 mean train loss:  2.25185649e-05, mean val. loss:  1.99777400e+00\n",
      "Epoch: 8660 mean train loss:  2.22215895e-05, mean val. loss:  1.99777699e+00\n",
      "Epoch: 8661 mean train loss:  2.21710361e-05, mean val. loss:  1.99781501e+00\n",
      "Epoch: 8662 mean train loss:  2.22107628e-05, mean val. loss:  1.99785113e+00\n",
      "Epoch: 8663 mean train loss:  2.18052301e-05, mean val. loss:  1.99794304e+00\n",
      "Epoch: 8664 mean train loss:  2.26764241e-05, mean val. loss:  1.99800789e+00\n",
      "Epoch: 8665 mean train loss:  2.21137598e-05, mean val. loss:  1.99805903e+00\n",
      "Epoch: 8666 mean train loss:  2.18159403e-05, mean val. loss:  1.99813890e+00\n",
      "Epoch: 8667 mean train loss:  2.23191746e-05, mean val. loss:  1.99823558e+00\n",
      "Epoch: 8668 mean train loss:  2.23303505e-05, mean val. loss:  1.99832559e+00\n",
      "Epoch: 8669 mean train loss:  2.23544776e-05, mean val. loss:  1.99843514e+00\n",
      "Epoch: 8670 mean train loss:  2.21896626e-05, mean val. loss:  1.99855506e+00\n",
      "Epoch: 8671 mean train loss:  2.21900700e-05, mean val. loss:  1.99865210e+00\n",
      "Epoch: 8672 mean train loss:  2.21730734e-05, mean val. loss:  1.99875188e+00\n",
      "Epoch: 8673 mean train loss:  2.23901588e-05, mean val. loss:  1.99883819e+00\n",
      "Epoch: 8674 mean train loss:  2.24602118e-05, mean val. loss:  1.99888933e+00\n",
      "Epoch: 8675 mean train loss:  2.19232170e-05, mean val. loss:  1.99895751e+00\n",
      "Epoch: 8676 mean train loss:  2.21104710e-05, mean val. loss:  1.99903226e+00\n",
      "Epoch: 8677 mean train loss:  2.22986273e-05, mean val. loss:  1.99909747e+00\n",
      "Epoch: 8678 mean train loss:  2.23933021e-05, mean val. loss:  1.99914479e+00\n",
      "Epoch: 8679 mean train loss:  2.20114307e-05, mean val. loss:  1.99921477e+00\n",
      "Epoch: 8680 mean train loss:  2.22870440e-05, mean val. loss:  1.99927855e+00\n",
      "Epoch: 8681 mean train loss:  2.21875380e-05, mean val. loss:  1.99931920e+00\n",
      "Epoch: 8682 mean train loss:  2.20648071e-05, mean val. loss:  1.99935365e+00\n",
      "Epoch: 8683 mean train loss:  2.23060779e-05, mean val. loss:  1.99938774e+00\n",
      "Epoch: 8684 mean train loss:  2.23935931e-05, mean val. loss:  1.99945569e+00\n",
      "Epoch: 8685 mean train loss:  2.22559902e-05, mean val. loss:  1.99950445e+00\n",
      "Epoch: 8686 mean train loss:  2.21759838e-05, mean val. loss:  1.99957716e+00\n",
      "Epoch: 8687 mean train loss:  2.22292729e-05, mean val. loss:  1.99964547e+00\n",
      "Epoch: 8688 mean train loss:  2.23423413e-05, mean val. loss:  1.99969983e+00\n",
      "Epoch: 8689 mean train loss:  2.23493844e-05, mean val. loss:  1.99976647e+00\n",
      "Epoch: 8690 mean train loss:  2.20657093e-05, mean val. loss:  1.99984443e+00\n",
      "Epoch: 8691 mean train loss:  2.17545312e-05, mean val. loss:  1.99993098e+00\n",
      "Epoch: 8692 mean train loss:  2.21707451e-05, mean val. loss:  2.00000286e+00\n",
      "Epoch: 8693 mean train loss:  2.20833172e-05, mean val. loss:  2.00013971e+00\n",
      "Epoch: 8694 mean train loss:  2.21543305e-05, mean val. loss:  2.00026417e+00\n",
      "Epoch: 8695 mean train loss:  2.21521768e-05, mean val. loss:  2.00043130e+00\n",
      "Epoch: 8696 mean train loss:  2.23207753e-05, mean val. loss:  2.00058889e+00\n",
      "Epoch: 8697 mean train loss:  2.25053227e-05, mean val. loss:  2.00074410e+00\n",
      "Epoch: 8698 mean train loss:  2.25114054e-05, mean val. loss:  2.00087714e+00\n",
      "Epoch: 8699 mean train loss:  2.21460359e-05, mean val. loss:  2.00103641e+00\n",
      "Epoch: 8700 mean train loss:  2.26773554e-05, mean val. loss:  2.00116205e+00\n",
      "Epoch: 8701 mean train loss:  2.19485082e-05, mean val. loss:  2.00129509e+00\n",
      "Epoch: 8702 mean train loss:  2.22490926e-05, mean val. loss:  2.00139308e+00\n",
      "Epoch: 8703 mean train loss:  2.21231894e-05, mean val. loss:  2.00146937e+00\n",
      "Epoch: 8704 mean train loss:  2.19397480e-05, mean val. loss:  2.00154448e+00\n",
      "Epoch: 8705 mean train loss:  2.21911469e-05, mean val. loss:  2.00156784e+00\n",
      "Epoch: 8706 mean train loss:  2.21384107e-05, mean val. loss:  2.00152802e+00\n",
      "Epoch: 8707 mean train loss:  2.21053779e-05, mean val. loss:  2.00146008e+00\n",
      "Epoch: 8708 mean train loss:  2.21358496e-05, mean val. loss:  2.00137424e+00\n",
      "Epoch: 8709 mean train loss:  2.22682429e-05, mean val. loss:  2.00125170e+00\n",
      "Epoch: 8710 mean train loss:  2.23251700e-05, mean val. loss:  2.00112796e+00\n",
      "Epoch: 8711 mean train loss:  2.16479239e-05, mean val. loss:  2.00101399e+00\n",
      "Epoch: 8712 mean train loss:  2.21829396e-05, mean val. loss:  2.00092864e+00\n",
      "Epoch: 8713 mean train loss:  2.17906781e-05, mean val. loss:  2.00083089e+00\n",
      "Epoch: 8714 mean train loss:  2.19500798e-05, mean val. loss:  2.00075150e+00\n",
      "Epoch: 8715 mean train loss:  2.21480732e-05, mean val. loss:  2.00067973e+00\n",
      "Epoch: 8716 mean train loss:  2.21233349e-05, mean val. loss:  2.00061488e+00\n",
      "Epoch: 8717 mean train loss:  2.25242111e-05, mean val. loss:  2.00054979e+00\n",
      "Epoch: 8718 mean train loss:  2.22996168e-05, mean val. loss:  2.00052524e+00\n",
      "Epoch: 8719 mean train loss:  2.21769151e-05, mean val. loss:  2.00054193e+00\n",
      "Epoch: 8720 mean train loss:  2.17723427e-05, mean val. loss:  2.00057387e+00\n",
      "Epoch: 8721 mean train loss:  2.20555812e-05, mean val. loss:  2.00065708e+00\n",
      "Epoch: 8722 mean train loss:  2.20359943e-05, mean val. loss:  2.00080633e+00\n",
      "Epoch: 8723 mean train loss:  2.21102382e-05, mean val. loss:  2.00100422e+00\n",
      "Epoch: 8724 mean train loss:  2.19868380e-05, mean val. loss:  2.00120330e+00\n",
      "Epoch: 8725 mean train loss:  2.24639371e-05, mean val. loss:  2.00137568e+00\n",
      "Epoch: 8726 mean train loss:  2.20129441e-05, mean val. loss:  2.00154972e+00\n",
      "Epoch: 8727 mean train loss:  2.27374258e-05, mean val. loss:  2.00176334e+00\n",
      "Epoch: 8728 mean train loss:  2.18072091e-05, mean val. loss:  2.00197554e+00\n",
      "Epoch: 8729 mean train loss:  2.24086398e-05, mean val. loss:  2.00215030e+00\n",
      "Epoch: 8730 mean train loss:  2.20953953e-05, mean val. loss:  2.00231242e+00\n",
      "Epoch: 8731 mean train loss:  2.23295938e-05, mean val. loss:  2.00241065e+00\n",
      "Epoch: 8732 mean train loss:  2.22586386e-05, mean val. loss:  2.00245976e+00\n",
      "Epoch: 8733 mean train loss:  2.15832551e-05, mean val. loss:  2.00248051e+00\n",
      "Epoch: 8734 mean train loss:  2.21335213e-05, mean val. loss:  2.00245142e+00\n",
      "Epoch: 8735 mean train loss:  2.21407390e-05, mean val. loss:  2.00241041e+00\n",
      "Epoch: 8736 mean train loss:  2.20540387e-05, mean val. loss:  2.00236011e+00\n",
      "Epoch: 8737 mean train loss:  2.16742337e-05, mean val. loss:  2.00230598e+00\n",
      "Epoch: 8738 mean train loss:  2.18620698e-05, mean val. loss:  2.00223875e+00\n",
      "Epoch: 8739 mean train loss:  2.25048570e-05, mean val. loss:  2.00214624e+00\n",
      "Epoch: 8740 mean train loss:  2.22719391e-05, mean val. loss:  2.00202012e+00\n",
      "Epoch: 8741 mean train loss:  2.18513014e-05, mean val. loss:  2.00188708e+00\n",
      "Epoch: 8742 mean train loss:  2.22883828e-05, mean val. loss:  2.00176787e+00\n",
      "Epoch: 8743 mean train loss:  2.17926572e-05, mean val. loss:  2.00166607e+00\n",
      "Epoch: 8744 mean train loss:  2.25937983e-05, mean val. loss:  2.00158429e+00\n",
      "Epoch: 8745 mean train loss:  2.21688824e-05, mean val. loss:  2.00149322e+00\n",
      "Epoch: 8746 mean train loss:  2.20137590e-05, mean val. loss:  2.00143385e+00\n",
      "Epoch: 8747 mean train loss:  2.20788643e-05, mean val. loss:  2.00139666e+00\n",
      "Epoch: 8748 mean train loss:  2.19753128e-05, mean val. loss:  2.00138187e+00\n",
      "Epoch: 8749 mean train loss:  2.20060756e-05, mean val. loss:  2.00138450e+00\n",
      "Epoch: 8750 mean train loss:  2.20411457e-05, mean val. loss:  2.00146413e+00\n",
      "Epoch: 8751 mean train loss:  2.19452486e-05, mean val. loss:  2.00156331e+00\n",
      "Epoch: 8752 mean train loss:  2.20898073e-05, mean val. loss:  2.00163150e+00\n",
      "Epoch: 8753 mean train loss:  2.23345123e-05, mean val. loss:  2.00173879e+00\n",
      "Epoch: 8754 mean train loss:  2.19262729e-05, mean val. loss:  2.00188422e+00\n",
      "Epoch: 8755 mean train loss:  2.16665794e-05, mean val. loss:  2.00204420e+00\n",
      "Epoch: 8756 mean train loss:  2.22353556e-05, mean val. loss:  2.00221205e+00\n",
      "Epoch: 8757 mean train loss:  2.21551454e-05, mean val. loss:  2.00238466e+00\n",
      "Epoch: 8758 mean train loss:  2.21495866e-05, mean val. loss:  2.00255466e+00\n",
      "Epoch: 8759 mean train loss:  2.23367533e-05, mean val. loss:  2.00271916e+00\n",
      "Epoch: 8760 mean train loss:  2.18990608e-05, mean val. loss:  2.00287771e+00\n",
      "Epoch: 8761 mean train loss:  2.22833478e-05, mean val. loss:  2.00301361e+00\n",
      "Epoch: 8762 mean train loss:  2.18456553e-05, mean val. loss:  2.00315881e+00\n",
      "Epoch: 8763 mean train loss:  2.21247028e-05, mean val. loss:  2.00327921e+00\n",
      "Epoch: 8764 mean train loss:  2.20535148e-05, mean val. loss:  2.00340629e+00\n",
      "Epoch: 8765 mean train loss:  2.23558454e-05, mean val. loss:  2.00349188e+00\n",
      "Epoch: 8766 mean train loss:  2.17352645e-05, mean val. loss:  2.00355721e+00\n",
      "Epoch: 8767 mean train loss:  2.21086957e-05, mean val. loss:  2.00359678e+00\n",
      "Epoch: 8768 mean train loss:  2.18780478e-05, mean val. loss:  2.00361252e+00\n",
      "Epoch: 8769 mean train loss:  2.20686488e-05, mean val. loss:  2.00361276e+00\n",
      "Epoch: 8770 mean train loss:  2.20315706e-05, mean val. loss:  2.00360703e+00\n",
      "Epoch: 8771 mean train loss:  2.21180089e-05, mean val. loss:  2.00360131e+00\n",
      "Epoch: 8772 mean train loss:  2.21986847e-05, mean val. loss:  2.00357580e+00\n",
      "Epoch: 8773 mean train loss:  2.18170171e-05, mean val. loss:  2.00356197e+00\n",
      "Epoch: 8774 mean train loss:  2.21998198e-05, mean val. loss:  2.00356269e+00\n",
      "Epoch: 8775 mean train loss:  2.19417270e-05, mean val. loss:  2.00355577e+00\n",
      "Epoch: 8776 mean train loss:  2.23258103e-05, mean val. loss:  2.00356030e+00\n",
      "Epoch: 8777 mean train loss:  2.24558171e-05, mean val. loss:  2.00354719e+00\n",
      "Epoch: 8778 mean train loss:  2.23270617e-05, mean val. loss:  2.00356579e+00\n",
      "Epoch: 8779 mean train loss:  2.19365174e-05, mean val. loss:  2.00359440e+00\n",
      "Epoch: 8780 mean train loss:  2.20145448e-05, mean val. loss:  2.00365973e+00\n",
      "Epoch: 8781 mean train loss:  2.18618079e-05, mean val. loss:  2.00370741e+00\n",
      "Epoch: 8782 mean train loss:  2.24718533e-05, mean val. loss:  2.00372887e+00\n",
      "Epoch: 8783 mean train loss:  2.24189716e-05, mean val. loss:  2.00375056e+00\n",
      "Epoch: 8784 mean train loss:  2.17265333e-05, mean val. loss:  2.00378609e+00\n",
      "Epoch: 8785 mean train loss:  2.16627959e-05, mean val. loss:  2.00384021e+00\n",
      "Epoch: 8786 mean train loss:  2.20975489e-05, mean val. loss:  2.00388336e+00\n",
      "Epoch: 8787 mean train loss:  2.20896327e-05, mean val. loss:  2.00397515e+00\n",
      "Epoch: 8788 mean train loss:  2.23086972e-05, mean val. loss:  2.00407743e+00\n",
      "Epoch: 8789 mean train loss:  2.19565700e-05, mean val. loss:  2.00419116e+00\n",
      "Epoch: 8790 mean train loss:  2.18606147e-05, mean val. loss:  2.00429964e+00\n",
      "Epoch: 8791 mean train loss:  2.24094256e-05, mean val. loss:  2.00438952e+00\n",
      "Epoch: 8792 mean train loss:  2.23723764e-05, mean val. loss:  2.00450253e+00\n",
      "Epoch: 8793 mean train loss:  2.21003138e-05, mean val. loss:  2.00464177e+00\n",
      "Epoch: 8794 mean train loss:  2.21100636e-05, mean val. loss:  2.00480914e+00\n",
      "Epoch: 8795 mean train loss:  2.22013623e-05, mean val. loss:  2.00497079e+00\n",
      "Epoch: 8796 mean train loss:  2.21208029e-05, mean val. loss:  2.00514817e+00\n",
      "Epoch: 8797 mean train loss:  2.25583790e-05, mean val. loss:  2.00529456e+00\n",
      "Epoch: 8798 mean train loss:  2.19546782e-05, mean val. loss:  2.00539827e+00\n",
      "Epoch: 8799 mean train loss:  2.18395144e-05, mean val. loss:  2.00551081e+00\n",
      "Epoch: 8800 mean train loss:  2.19107606e-05, mean val. loss:  2.00561762e+00\n",
      "Epoch: 8801 mean train loss:  2.21570372e-05, mean val. loss:  2.00573134e+00\n",
      "Epoch: 8802 mean train loss:  2.20974907e-05, mean val. loss:  2.00581360e+00\n",
      "Epoch: 8803 mean train loss:  2.20935617e-05, mean val. loss:  2.00587130e+00\n",
      "Epoch: 8804 mean train loss:  2.24561372e-05, mean val. loss:  2.00591302e+00\n",
      "Epoch: 8805 mean train loss:  2.20799120e-05, mean val. loss:  2.00595522e+00\n",
      "Epoch: 8806 mean train loss:  2.19789799e-05, mean val. loss:  2.00594306e+00\n",
      "Epoch: 8807 mean train loss:  2.21027003e-05, mean val. loss:  2.00594759e+00\n",
      "Epoch: 8808 mean train loss:  2.22357630e-05, mean val. loss:  2.00593805e+00\n",
      "Epoch: 8809 mean train loss:  2.18502828e-05, mean val. loss:  2.00591326e+00\n",
      "Epoch: 8810 mean train loss:  2.20065413e-05, mean val. loss:  2.00588369e+00\n",
      "Epoch: 8811 mean train loss:  2.20072106e-05, mean val. loss:  2.00584817e+00\n",
      "Epoch: 8812 mean train loss:  2.18183850e-05, mean val. loss:  2.00581169e+00\n",
      "Epoch: 8813 mean train loss:  2.18670466e-05, mean val. loss:  2.00580120e+00\n",
      "Epoch: 8814 mean train loss:  2.18125060e-05, mean val. loss:  2.00577784e+00\n",
      "Epoch: 8815 mean train loss:  2.19474896e-05, mean val. loss:  2.00576353e+00\n",
      "Epoch: 8816 mean train loss:  2.23257521e-05, mean val. loss:  2.00575590e+00\n",
      "Epoch: 8817 mean train loss:  2.20976071e-05, mean val. loss:  2.00574851e+00\n",
      "Epoch: 8818 mean train loss:  2.21325899e-05, mean val. loss:  2.00575852e+00\n",
      "Epoch: 8819 mean train loss:  2.19973444e-05, mean val. loss:  2.00578594e+00\n",
      "Epoch: 8820 mean train loss:  2.20372167e-05, mean val. loss:  2.00582933e+00\n",
      "Epoch: 8821 mean train loss:  2.22228700e-05, mean val. loss:  2.00586176e+00\n",
      "Epoch: 8822 mean train loss:  2.21633818e-05, mean val. loss:  2.00592399e+00\n",
      "Epoch: 8823 mean train loss:  2.20314250e-05, mean val. loss:  2.00598240e+00\n",
      "Epoch: 8824 mean train loss:  2.20359070e-05, mean val. loss:  2.00602412e+00\n",
      "Epoch: 8825 mean train loss:  2.21772934e-05, mean val. loss:  2.00606751e+00\n",
      "Epoch: 8826 mean train loss:  2.17681227e-05, mean val. loss:  2.00610352e+00\n",
      "Epoch: 8827 mean train loss:  2.20380025e-05, mean val. loss:  2.00608110e+00\n",
      "Epoch: 8828 mean train loss:  2.16918706e-05, mean val. loss:  2.00605655e+00\n",
      "Epoch: 8829 mean train loss:  2.20132642e-05, mean val. loss:  2.00599670e+00\n",
      "Epoch: 8830 mean train loss:  2.21603550e-05, mean val. loss:  2.00593948e+00\n",
      "Epoch: 8831 mean train loss:  2.19399808e-05, mean val. loss:  2.00590754e+00\n",
      "Epoch: 8832 mean train loss:  2.18481291e-05, mean val. loss:  2.00588202e+00\n",
      "Epoch: 8833 mean train loss:  2.18506611e-05, mean val. loss:  2.00585079e+00\n",
      "Epoch: 8834 mean train loss:  2.17658526e-05, mean val. loss:  2.00582647e+00\n",
      "Epoch: 8835 mean train loss:  2.19534268e-05, mean val. loss:  2.00579262e+00\n",
      "Epoch: 8836 mean train loss:  2.17855850e-05, mean val. loss:  2.00574589e+00\n",
      "Epoch: 8837 mean train loss:  2.23309034e-05, mean val. loss:  2.00569057e+00\n",
      "Epoch: 8838 mean train loss:  2.22727540e-05, mean val. loss:  2.00564885e+00\n",
      "Epoch: 8839 mean train loss:  2.18601199e-05, mean val. loss:  2.00560474e+00\n",
      "Epoch: 8840 mean train loss:  2.19058129e-05, mean val. loss:  2.00557423e+00\n",
      "Epoch: 8841 mean train loss:  2.19849171e-05, mean val. loss:  2.00556350e+00\n",
      "Epoch: 8842 mean train loss:  2.20092770e-05, mean val. loss:  2.00560641e+00\n",
      "Epoch: 8843 mean train loss:  2.20871298e-05, mean val. loss:  2.00565791e+00\n",
      "Epoch: 8844 mean train loss:  2.18625646e-05, mean val. loss:  2.00572348e+00\n",
      "Epoch: 8845 mean train loss:  2.17025517e-05, mean val. loss:  2.00584364e+00\n",
      "Epoch: 8846 mean train loss:  2.21067166e-05, mean val. loss:  2.00596642e+00\n",
      "Epoch: 8847 mean train loss:  2.18628556e-05, mean val. loss:  2.00610352e+00\n",
      "Epoch: 8848 mean train loss:  2.18938803e-05, mean val. loss:  2.00624156e+00\n",
      "Epoch: 8849 mean train loss:  2.19762151e-05, mean val. loss:  2.00637698e+00\n",
      "Epoch: 8850 mean train loss:  2.21786904e-05, mean val. loss:  2.00654483e+00\n",
      "Epoch: 8851 mean train loss:  2.22164381e-05, mean val. loss:  2.00672936e+00\n",
      "Epoch: 8852 mean train loss:  2.23024399e-05, mean val. loss:  2.00686502e+00\n",
      "Epoch: 8853 mean train loss:  2.18986534e-05, mean val. loss:  2.00698781e+00\n",
      "Epoch: 8854 mean train loss:  2.17984198e-05, mean val. loss:  2.00711346e+00\n",
      "Epoch: 8855 mean train loss:  2.16300541e-05, mean val. loss:  2.00725651e+00\n",
      "Epoch: 8856 mean train loss:  2.24152463e-05, mean val. loss:  2.00734615e+00\n",
      "Epoch: 8857 mean train loss:  2.18646310e-05, mean val. loss:  2.00741386e+00\n",
      "Epoch: 8858 mean train loss:  2.18769128e-05, mean val. loss:  2.00743961e+00\n",
      "Epoch: 8859 mean train loss:  2.20216170e-05, mean val. loss:  2.00743294e+00\n",
      "Epoch: 8860 mean train loss:  2.23759853e-05, mean val. loss:  2.00738168e+00\n",
      "Epoch: 8861 mean train loss:  2.21900409e-05, mean val. loss:  2.00727725e+00\n",
      "Epoch: 8862 mean train loss:  2.17989436e-05, mean val. loss:  2.00718546e+00\n",
      "Epoch: 8863 mean train loss:  2.20524089e-05, mean val. loss:  2.00708127e+00\n",
      "Epoch: 8864 mean train loss:  2.24621617e-05, mean val. loss:  2.00697923e+00\n",
      "Epoch: 8865 mean train loss:  2.20206857e-05, mean val. loss:  2.00689960e+00\n",
      "Epoch: 8866 mean train loss:  2.20887596e-05, mean val. loss:  2.00679851e+00\n",
      "Epoch: 8867 mean train loss:  2.18643981e-05, mean val. loss:  2.00672245e+00\n",
      "Epoch: 8868 mean train loss:  2.21336668e-05, mean val. loss:  2.00666404e+00\n",
      "Epoch: 8869 mean train loss:  2.23928946e-05, mean val. loss:  2.00657845e+00\n",
      "Epoch: 8870 mean train loss:  2.18095083e-05, mean val. loss:  2.00649500e+00\n",
      "Epoch: 8871 mean train loss:  2.23153038e-05, mean val. loss:  2.00641322e+00\n",
      "Epoch: 8872 mean train loss:  2.19015928e-05, mean val. loss:  2.00634098e+00\n",
      "Epoch: 8873 mean train loss:  2.21419032e-05, mean val. loss:  2.00633478e+00\n",
      "Epoch: 8874 mean train loss:  2.19732174e-05, mean val. loss:  2.00629497e+00\n",
      "Epoch: 8875 mean train loss:  2.18593050e-05, mean val. loss:  2.00625396e+00\n",
      "Epoch: 8876 mean train loss:  2.20304064e-05, mean val. loss:  2.00623584e+00\n",
      "Epoch: 8877 mean train loss:  2.19196954e-05, mean val. loss:  2.00624228e+00\n",
      "Epoch: 8878 mean train loss:  2.20462971e-05, mean val. loss:  2.00625157e+00\n",
      "Epoch: 8879 mean train loss:  2.22742965e-05, mean val. loss:  2.00624752e+00\n",
      "Epoch: 8880 mean train loss:  2.21257505e-05, mean val. loss:  2.00626421e+00\n",
      "Epoch: 8881 mean train loss:  2.20390211e-05, mean val. loss:  2.00626898e+00\n",
      "Epoch: 8882 mean train loss:  2.19969079e-05, mean val. loss:  2.00630784e+00\n",
      "Epoch: 8883 mean train loss:  2.22231902e-05, mean val. loss:  2.00634789e+00\n",
      "Epoch: 8884 mean train loss:  2.20793881e-05, mean val. loss:  2.00641084e+00\n",
      "Epoch: 8885 mean train loss:  2.17604975e-05, mean val. loss:  2.00647402e+00\n",
      "Epoch: 8886 mean train loss:  2.20942311e-05, mean val. loss:  2.00653887e+00\n",
      "Epoch: 8887 mean train loss:  2.20931834e-05, mean val. loss:  2.00659347e+00\n",
      "Epoch: 8888 mean train loss:  2.18203058e-05, mean val. loss:  2.00665975e+00\n",
      "Epoch: 8889 mean train loss:  2.25422264e-05, mean val. loss:  2.00671577e+00\n",
      "Epoch: 8890 mean train loss:  2.20862566e-05, mean val. loss:  2.00676751e+00\n",
      "Epoch: 8891 mean train loss:  2.19917856e-05, mean val. loss:  2.00682759e+00\n",
      "Epoch: 8892 mean train loss:  2.19272915e-05, mean val. loss:  2.00684524e+00\n",
      "Epoch: 8893 mean train loss:  2.19466456e-05, mean val. loss:  2.00685692e+00\n",
      "Epoch: 8894 mean train loss:  2.19013600e-05, mean val. loss:  2.00690699e+00\n",
      "Epoch: 8895 mean train loss:  2.22143426e-05, mean val. loss:  2.00694728e+00\n",
      "Epoch: 8896 mean train loss:  2.18477799e-05, mean val. loss:  2.00699854e+00\n",
      "Epoch: 8897 mean train loss:  2.15763866e-05, mean val. loss:  2.00707650e+00\n",
      "Epoch: 8898 mean train loss:  2.21368391e-05, mean val. loss:  2.00713730e+00\n",
      "Epoch: 8899 mean train loss:  2.21938535e-05, mean val. loss:  2.00721598e+00\n",
      "Epoch: 8900 mean train loss:  2.20491202e-05, mean val. loss:  2.00730991e+00\n",
      "Epoch: 8901 mean train loss:  2.15970795e-05, mean val. loss:  2.00743413e+00\n",
      "Epoch: 8902 mean train loss:  2.22473755e-05, mean val. loss:  2.00757670e+00\n",
      "Epoch: 8903 mean train loss:  2.23114621e-05, mean val. loss:  2.00772500e+00\n",
      "Epoch: 8904 mean train loss:  2.20112852e-05, mean val. loss:  2.00790834e+00\n",
      "Epoch: 8905 mean train loss:  2.19734793e-05, mean val. loss:  2.00810981e+00\n",
      "Epoch: 8906 mean train loss:  2.19799695e-05, mean val. loss:  2.00830388e+00\n",
      "Epoch: 8907 mean train loss:  2.18518253e-05, mean val. loss:  2.00847769e+00\n",
      "Epoch: 8908 mean train loss:  2.16742628e-05, mean val. loss:  2.00865889e+00\n",
      "Epoch: 8909 mean train loss:  2.19407375e-05, mean val. loss:  2.00881720e+00\n",
      "Epoch: 8910 mean train loss:  2.18930072e-05, mean val. loss:  2.00894904e+00\n",
      "Epoch: 8911 mean train loss:  2.17508641e-05, mean val. loss:  2.00907779e+00\n",
      "Epoch: 8912 mean train loss:  2.18785135e-05, mean val. loss:  2.00921249e+00\n",
      "Epoch: 8913 mean train loss:  2.24152463e-05, mean val. loss:  2.00936174e+00\n",
      "Epoch: 8914 mean train loss:  2.20398069e-05, mean val. loss:  2.00951600e+00\n",
      "Epoch: 8915 mean train loss:  2.17807828e-05, mean val. loss:  2.00965500e+00\n",
      "Epoch: 8916 mean train loss:  2.19472859e-05, mean val. loss:  2.00979972e+00\n",
      "Epoch: 8917 mean train loss:  2.21349474e-05, mean val. loss:  2.00995135e+00\n",
      "Epoch: 8918 mean train loss:  2.19800277e-05, mean val. loss:  2.01010084e+00\n",
      "Epoch: 8919 mean train loss:  2.21774972e-05, mean val. loss:  2.01023126e+00\n",
      "Epoch: 8920 mean train loss:  2.19864887e-05, mean val. loss:  2.01035190e+00\n",
      "Epoch: 8921 mean train loss:  2.18625355e-05, mean val. loss:  2.01045752e+00\n",
      "Epoch: 8922 mean train loss:  2.21246155e-05, mean val. loss:  2.01055646e+00\n",
      "Epoch: 8923 mean train loss:  2.18618370e-05, mean val. loss:  2.01064253e+00\n",
      "Epoch: 8924 mean train loss:  2.20960064e-05, mean val. loss:  2.01074362e+00\n",
      "Epoch: 8925 mean train loss:  2.19178619e-05, mean val. loss:  2.01083684e+00\n",
      "Epoch: 8926 mean train loss:  2.18476052e-05, mean val. loss:  2.01092100e+00\n",
      "Epoch: 8927 mean train loss:  2.20265938e-05, mean val. loss:  2.01101065e+00\n",
      "Epoch: 8928 mean train loss:  2.18711793e-05, mean val. loss:  2.01107740e+00\n",
      "Epoch: 8929 mean train loss:  2.23147799e-05, mean val. loss:  2.01114082e+00\n",
      "Epoch: 8930 mean train loss:  2.19392532e-05, mean val. loss:  2.01121283e+00\n",
      "Epoch: 8931 mean train loss:  2.15951586e-05, mean val. loss:  2.01133633e+00\n",
      "Epoch: 8932 mean train loss:  2.17588386e-05, mean val. loss:  2.01145649e+00\n",
      "Epoch: 8933 mean train loss:  2.20325601e-05, mean val. loss:  2.01161289e+00\n",
      "Epoch: 8934 mean train loss:  2.14409956e-05, mean val. loss:  2.01175928e+00\n",
      "Epoch: 8935 mean train loss:  2.18048808e-05, mean val. loss:  2.01192808e+00\n",
      "Epoch: 8936 mean train loss:  2.19799404e-05, mean val. loss:  2.01208973e+00\n",
      "Epoch: 8937 mean train loss:  2.17635825e-05, mean val. loss:  2.01227665e+00\n",
      "Epoch: 8938 mean train loss:  2.17657653e-05, mean val. loss:  2.01246691e+00\n",
      "Epoch: 8939 mean train loss:  2.15151813e-05, mean val. loss:  2.01265383e+00\n",
      "Epoch: 8940 mean train loss:  2.17887573e-05, mean val. loss:  2.01281595e+00\n",
      "Epoch: 8941 mean train loss:  2.23078532e-05, mean val. loss:  2.01298642e+00\n",
      "Epoch: 8942 mean train loss:  2.19862850e-05, mean val. loss:  2.01315403e+00\n",
      "Epoch: 8943 mean train loss:  2.18304631e-05, mean val. loss:  2.01330090e+00\n",
      "Epoch: 8944 mean train loss:  2.19168724e-05, mean val. loss:  2.01343536e+00\n",
      "Epoch: 8945 mean train loss:  2.20529037e-05, mean val. loss:  2.01355338e+00\n",
      "Epoch: 8946 mean train loss:  2.19943176e-05, mean val. loss:  2.01363969e+00\n",
      "Epoch: 8947 mean train loss:  2.20153306e-05, mean val. loss:  2.01370907e+00\n",
      "Epoch: 8948 mean train loss:  2.19367212e-05, mean val. loss:  2.01372051e+00\n",
      "Epoch: 8949 mean train loss:  2.14879401e-05, mean val. loss:  2.01368761e+00\n",
      "Epoch: 8950 mean train loss:  2.16781627e-05, mean val. loss:  2.01367640e+00\n",
      "Epoch: 8951 mean train loss:  2.17015913e-05, mean val. loss:  2.01364183e+00\n",
      "Epoch: 8952 mean train loss:  2.20986258e-05, mean val. loss:  2.01355648e+00\n",
      "Epoch: 8953 mean train loss:  2.19188223e-05, mean val. loss:  2.01343560e+00\n",
      "Epoch: 8954 mean train loss:  2.17445195e-05, mean val. loss:  2.01331210e+00\n",
      "Epoch: 8955 mean train loss:  2.20679503e-05, mean val. loss:  2.01316333e+00\n",
      "Epoch: 8956 mean train loss:  2.19723443e-05, mean val. loss:  2.01302147e+00\n",
      "Epoch: 8957 mean train loss:  2.17327324e-05, mean val. loss:  2.01288438e+00\n",
      "Epoch: 8958 mean train loss:  2.19416979e-05, mean val. loss:  2.01275063e+00\n",
      "Epoch: 8959 mean train loss:  2.18436471e-05, mean val. loss:  2.01261711e+00\n",
      "Epoch: 8960 mean train loss:  2.19068024e-05, mean val. loss:  2.01250768e+00\n",
      "Epoch: 8961 mean train loss:  2.21372757e-05, mean val. loss:  2.01239324e+00\n",
      "Epoch: 8962 mean train loss:  2.19496433e-05, mean val. loss:  2.01228333e+00\n",
      "Epoch: 8963 mean train loss:  2.19722569e-05, mean val. loss:  2.01222682e+00\n",
      "Epoch: 8964 mean train loss:  2.19369831e-05, mean val. loss:  2.01220465e+00\n",
      "Epoch: 8965 mean train loss:  2.18303467e-05, mean val. loss:  2.01219964e+00\n",
      "Epoch: 8966 mean train loss:  2.18085479e-05, mean val. loss:  2.01219964e+00\n",
      "Epoch: 8967 mean train loss:  2.19682406e-05, mean val. loss:  2.01223516e+00\n",
      "Epoch: 8968 mean train loss:  2.18229834e-05, mean val. loss:  2.01231813e+00\n",
      "Epoch: 8969 mean train loss:  2.16105254e-05, mean val. loss:  2.01242423e+00\n",
      "Epoch: 8970 mean train loss:  2.17570632e-05, mean val. loss:  2.01252460e+00\n",
      "Epoch: 8971 mean train loss:  2.17336928e-05, mean val. loss:  2.01262426e+00\n",
      "Epoch: 8972 mean train loss:  2.20495276e-05, mean val. loss:  2.01272845e+00\n",
      "Epoch: 8973 mean train loss:  2.16257176e-05, mean val. loss:  2.01287150e+00\n",
      "Epoch: 8974 mean train loss:  2.20937072e-05, mean val. loss:  2.01303124e+00\n",
      "Epoch: 8975 mean train loss:  2.15190521e-05, mean val. loss:  2.01318741e+00\n",
      "Epoch: 8976 mean train loss:  2.17013003e-05, mean val. loss:  2.01336265e+00\n",
      "Epoch: 8977 mean train loss:  2.19509820e-05, mean val. loss:  2.01351309e+00\n",
      "Epoch: 8978 mean train loss:  2.18778150e-05, mean val. loss:  2.01365495e+00\n",
      "Epoch: 8979 mean train loss:  2.21868686e-05, mean val. loss:  2.01380253e+00\n",
      "Epoch: 8980 mean train loss:  2.19451613e-05, mean val. loss:  2.01391220e+00\n",
      "Epoch: 8981 mean train loss:  2.17254565e-05, mean val. loss:  2.01397300e+00\n",
      "Epoch: 8982 mean train loss:  2.17039487e-05, mean val. loss:  2.01401925e+00\n",
      "Epoch: 8983 mean train loss:  2.20745569e-05, mean val. loss:  2.01404667e+00\n",
      "Epoch: 8984 mean train loss:  2.17830529e-05, mean val. loss:  2.01405644e+00\n",
      "Epoch: 8985 mean train loss:  2.22915842e-05, mean val. loss:  2.01400971e+00\n",
      "Epoch: 8986 mean train loss:  2.17758934e-05, mean val. loss:  2.01390529e+00\n",
      "Epoch: 8987 mean train loss:  2.16192857e-05, mean val. loss:  2.01379609e+00\n",
      "Epoch: 8988 mean train loss:  2.15907639e-05, mean val. loss:  2.01366735e+00\n",
      "Epoch: 8989 mean train loss:  2.24773248e-05, mean val. loss:  2.01351237e+00\n",
      "Epoch: 8990 mean train loss:  2.18864006e-05, mean val. loss:  2.01336241e+00\n",
      "Epoch: 8991 mean train loss:  2.20989750e-05, mean val. loss:  2.01322961e+00\n",
      "Epoch: 8992 mean train loss:  2.19777285e-05, mean val. loss:  2.01308870e+00\n",
      "Epoch: 8993 mean train loss:  2.17369816e-05, mean val. loss:  2.01296544e+00\n",
      "Epoch: 8994 mean train loss:  2.19007488e-05, mean val. loss:  2.01285052e+00\n",
      "Epoch: 8995 mean train loss:  2.18436762e-05, mean val. loss:  2.01275086e+00\n",
      "Epoch: 8996 mean train loss:  2.19886424e-05, mean val. loss:  2.01268387e+00\n",
      "Epoch: 8997 mean train loss:  2.18271744e-05, mean val. loss:  2.01265502e+00\n",
      "Epoch: 8998 mean train loss:  2.18909117e-05, mean val. loss:  2.01264572e+00\n",
      "Epoch: 8999 mean train loss:  2.20134971e-05, mean val. loss:  2.01266503e+00\n",
      "Epoch: 9000 mean train loss:  2.18285131e-05, mean val. loss:  2.01271176e+00\n",
      "Epoch: 9001 mean train loss:  2.19961221e-05, mean val. loss:  2.01276684e+00\n",
      "Epoch: 9002 mean train loss:  2.18262721e-05, mean val. loss:  2.01285648e+00\n",
      "Epoch: 9003 mean train loss:  2.16314802e-05, mean val. loss:  2.01299262e+00\n",
      "Epoch: 9004 mean train loss:  2.20045331e-05, mean val. loss:  2.01315212e+00\n",
      "Epoch: 9005 mean train loss:  2.23358220e-05, mean val. loss:  2.01330996e+00\n",
      "Epoch: 9006 mean train loss:  2.18697824e-05, mean val. loss:  2.01347470e+00\n",
      "Epoch: 9007 mean train loss:  2.17342458e-05, mean val. loss:  2.01365900e+00\n",
      "Epoch: 9008 mean train loss:  2.19827343e-05, mean val. loss:  2.01385140e+00\n",
      "Epoch: 9009 mean train loss:  2.19234789e-05, mean val. loss:  2.01402426e+00\n",
      "Epoch: 9010 mean train loss:  2.17442575e-05, mean val. loss:  2.01419806e+00\n",
      "Epoch: 9011 mean train loss:  2.18783389e-05, mean val. loss:  2.01438618e+00\n",
      "Epoch: 9012 mean train loss:  2.17817724e-05, mean val. loss:  2.01453590e+00\n",
      "Epoch: 9013 mean train loss:  2.19980720e-05, mean val. loss:  2.01467943e+00\n",
      "Epoch: 9014 mean train loss:  2.17665220e-05, mean val. loss:  2.01481819e+00\n",
      "Epoch: 9015 mean train loss:  2.18452187e-05, mean val. loss:  2.01493907e+00\n",
      "Epoch: 9016 mean train loss:  2.17895140e-05, mean val. loss:  2.01504588e+00\n",
      "Epoch: 9017 mean train loss:  2.14389584e-05, mean val. loss:  2.01516509e+00\n",
      "Epoch: 9018 mean train loss:  2.17449851e-05, mean val. loss:  2.01525831e+00\n",
      "Epoch: 9019 mean train loss:  2.17181514e-05, mean val. loss:  2.01537991e+00\n",
      "Epoch: 9020 mean train loss:  2.18820642e-05, mean val. loss:  2.01550484e+00\n",
      "Epoch: 9021 mean train loss:  2.17766792e-05, mean val. loss:  2.01561308e+00\n",
      "Epoch: 9022 mean train loss:  2.17960915e-05, mean val. loss:  2.01573253e+00\n",
      "Epoch: 9023 mean train loss:  2.16461485e-05, mean val. loss:  2.01586390e+00\n",
      "Epoch: 9024 mean train loss:  2.16535991e-05, mean val. loss:  2.01600575e+00\n",
      "Epoch: 9025 mean train loss:  2.17987981e-05, mean val. loss:  2.01619220e+00\n",
      "Epoch: 9026 mean train loss:  2.19158246e-05, mean val. loss:  2.01637006e+00\n",
      "Epoch: 9027 mean train loss:  2.18279311e-05, mean val. loss:  2.01651931e+00\n",
      "Epoch: 9028 mean train loss:  2.19184440e-05, mean val. loss:  2.01666427e+00\n",
      "Epoch: 9029 mean train loss:  2.18613714e-05, mean val. loss:  2.01677084e+00\n",
      "Epoch: 9030 mean train loss:  2.18608184e-05, mean val. loss:  2.01687741e+00\n",
      "Epoch: 9031 mean train loss:  2.18163768e-05, mean val. loss:  2.01698637e+00\n",
      "Epoch: 9032 mean train loss:  2.19209760e-05, mean val. loss:  2.01709366e+00\n",
      "Epoch: 9033 mean train loss:  2.17896886e-05, mean val. loss:  2.01719093e+00\n",
      "Epoch: 9034 mean train loss:  2.17712950e-05, mean val. loss:  2.01728606e+00\n",
      "Epoch: 9035 mean train loss:  2.18501373e-05, mean val. loss:  2.01738858e+00\n",
      "Epoch: 9036 mean train loss:  2.17002234e-05, mean val. loss:  2.01748729e+00\n",
      "Epoch: 9037 mean train loss:  2.15149194e-05, mean val. loss:  2.01755404e+00\n",
      "Epoch: 9038 mean train loss:  2.17705092e-05, mean val. loss:  2.01762128e+00\n",
      "Epoch: 9039 mean train loss:  2.15321197e-05, mean val. loss:  2.01768255e+00\n",
      "Epoch: 9040 mean train loss:  2.17878551e-05, mean val. loss:  2.01770973e+00\n",
      "Epoch: 9041 mean train loss:  2.15639302e-05, mean val. loss:  2.01777291e+00\n",
      "Epoch: 9042 mean train loss:  2.18494097e-05, mean val. loss:  2.01777601e+00\n",
      "Epoch: 9043 mean train loss:  2.18524947e-05, mean val. loss:  2.01776552e+00\n",
      "Epoch: 9044 mean train loss:  2.18207715e-05, mean val. loss:  2.01773310e+00\n",
      "Epoch: 9045 mean train loss:  2.18266796e-05, mean val. loss:  2.01769876e+00\n",
      "Epoch: 9046 mean train loss:  2.16884655e-05, mean val. loss:  2.01768017e+00\n",
      "Epoch: 9047 mean train loss:  2.15949549e-05, mean val. loss:  2.01768780e+00\n",
      "Epoch: 9048 mean train loss:  2.16636981e-05, mean val. loss:  2.01770091e+00\n",
      "Epoch: 9049 mean train loss:  2.19201320e-05, mean val. loss:  2.01771903e+00\n",
      "Epoch: 9050 mean train loss:  2.15214095e-05, mean val. loss:  2.01776528e+00\n",
      "Epoch: 9051 mean train loss:  2.16730405e-05, mean val. loss:  2.01782346e+00\n",
      "Epoch: 9052 mean train loss:  2.16283370e-05, mean val. loss:  2.01786876e+00\n",
      "Epoch: 9053 mean train loss:  2.18140776e-05, mean val. loss:  2.01790118e+00\n",
      "Epoch: 9054 mean train loss:  2.14892498e-05, mean val. loss:  2.01793551e+00\n",
      "Epoch: 9055 mean train loss:  2.15175387e-05, mean val. loss:  2.01799035e+00\n",
      "Epoch: 9056 mean train loss:  2.15887267e-05, mean val. loss:  2.01808095e+00\n",
      "Epoch: 9057 mean train loss:  2.17802881e-05, mean val. loss:  2.01816154e+00\n",
      "Epoch: 9058 mean train loss:  2.19466747e-05, mean val. loss:  2.01819158e+00\n",
      "Epoch: 9059 mean train loss:  2.13818275e-05, mean val. loss:  2.01825762e+00\n",
      "Epoch: 9060 mean train loss:  2.17841007e-05, mean val. loss:  2.01832128e+00\n",
      "Epoch: 9061 mean train loss:  2.18692294e-05, mean val. loss:  2.01836991e+00\n",
      "Epoch: 9062 mean train loss:  2.15448090e-05, mean val. loss:  2.01840496e+00\n",
      "Epoch: 9063 mean train loss:  2.21663795e-05, mean val. loss:  2.01840925e+00\n",
      "Epoch: 9064 mean train loss:  2.16108165e-05, mean val. loss:  2.01841569e+00\n",
      "Epoch: 9065 mean train loss:  2.18205096e-05, mean val. loss:  2.01839972e+00\n",
      "Epoch: 9066 mean train loss:  2.20338115e-05, mean val. loss:  2.01836848e+00\n",
      "Epoch: 9067 mean train loss:  2.19319772e-05, mean val. loss:  2.01832533e+00\n",
      "Epoch: 9068 mean train loss:  2.15135515e-05, mean val. loss:  2.01830053e+00\n",
      "Epoch: 9069 mean train loss:  2.19068897e-05, mean val. loss:  2.01829243e+00\n",
      "Epoch: 9070 mean train loss:  2.19920767e-05, mean val. loss:  2.01828218e+00\n",
      "Epoch: 9071 mean train loss:  2.19933863e-05, mean val. loss:  2.01828837e+00\n",
      "Epoch: 9072 mean train loss:  2.16033659e-05, mean val. loss:  2.01831293e+00\n",
      "Epoch: 9073 mean train loss:  2.14597676e-05, mean val. loss:  2.01833105e+00\n",
      "Epoch: 9074 mean train loss:  2.18955392e-05, mean val. loss:  2.01836467e+00\n",
      "Epoch: 9075 mean train loss:  2.16073822e-05, mean val. loss:  2.01838017e+00\n",
      "Epoch: 9076 mean train loss:  2.16959161e-05, mean val. loss:  2.01835895e+00\n",
      "Epoch: 9077 mean train loss:  2.18293862e-05, mean val. loss:  2.01833630e+00\n",
      "Epoch: 9078 mean train loss:  2.18793575e-05, mean val. loss:  2.01831222e+00\n",
      "Epoch: 9079 mean train loss:  2.14010943e-05, mean val. loss:  2.01832128e+00\n",
      "Epoch: 9080 mean train loss:  2.16816552e-05, mean val. loss:  2.01831007e+00\n",
      "Epoch: 9081 mean train loss:  2.13628518e-05, mean val. loss:  2.01833987e+00\n",
      "Epoch: 9082 mean train loss:  2.14486790e-05, mean val. loss:  2.01840711e+00\n",
      "Epoch: 9083 mean train loss:  2.14533356e-05, mean val. loss:  2.01852131e+00\n",
      "Epoch: 9084 mean train loss:  2.17554625e-05, mean val. loss:  2.01858592e+00\n",
      "Epoch: 9085 mean train loss:  2.20209477e-05, mean val. loss:  2.01869369e+00\n",
      "Epoch: 9086 mean train loss:  2.14465545e-05, mean val. loss:  2.01876879e+00\n",
      "Epoch: 9087 mean train loss:  2.17118650e-05, mean val. loss:  2.01886153e+00\n",
      "Epoch: 9088 mean train loss:  2.16618646e-05, mean val. loss:  2.01897240e+00\n",
      "Epoch: 9089 mean train loss:  2.16131157e-05, mean val. loss:  2.01907158e+00\n",
      "Epoch: 9090 mean train loss:  2.17448687e-05, mean val. loss:  2.01916051e+00\n",
      "Epoch: 9091 mean train loss:  2.15035398e-05, mean val. loss:  2.01926351e+00\n",
      "Epoch: 9092 mean train loss:  2.14176544e-05, mean val. loss:  2.01937532e+00\n",
      "Epoch: 9093 mean train loss:  2.18290370e-05, mean val. loss:  2.01949644e+00\n",
      "Epoch: 9094 mean train loss:  2.15368636e-05, mean val. loss:  2.01960111e+00\n",
      "Epoch: 9095 mean train loss:  2.17377383e-05, mean val. loss:  2.01969862e+00\n",
      "Epoch: 9096 mean train loss:  2.17400666e-05, mean val. loss:  2.01978874e+00\n",
      "Epoch: 9097 mean train loss:  2.16124754e-05, mean val. loss:  2.01989126e+00\n",
      "Epoch: 9098 mean train loss:  2.18053756e-05, mean val. loss:  2.01998878e+00\n",
      "Epoch: 9099 mean train loss:  2.15781329e-05, mean val. loss:  2.02010679e+00\n",
      "Epoch: 9100 mean train loss:  2.15414620e-05, mean val. loss:  2.02020431e+00\n",
      "Epoch: 9101 mean train loss:  2.17267079e-05, mean val. loss:  2.02028227e+00\n",
      "Epoch: 9102 mean train loss:  2.14840984e-05, mean val. loss:  2.02035284e+00\n",
      "Epoch: 9103 mean train loss:  2.17464403e-05, mean val. loss:  2.02041984e+00\n",
      "Epoch: 9104 mean train loss:  2.15126493e-05, mean val. loss:  2.02046442e+00\n",
      "Epoch: 9105 mean train loss:  2.17237975e-05, mean val. loss:  2.02052522e+00\n",
      "Epoch: 9106 mean train loss:  2.16517074e-05, mean val. loss:  2.02062249e+00\n",
      "Epoch: 9107 mean train loss:  2.14944594e-05, mean val. loss:  2.02070451e+00\n",
      "Epoch: 9108 mean train loss:  2.16402987e-05, mean val. loss:  2.02080655e+00\n",
      "Epoch: 9109 mean train loss:  2.17142515e-05, mean val. loss:  2.02091217e+00\n",
      "Epoch: 9110 mean train loss:  2.15531909e-05, mean val. loss:  2.02103972e+00\n",
      "Epoch: 9111 mean train loss:  2.16408516e-05, mean val. loss:  2.02114820e+00\n",
      "Epoch: 9112 mean train loss:  2.15837208e-05, mean val. loss:  2.02125835e+00\n",
      "Epoch: 9113 mean train loss:  2.14139582e-05, mean val. loss:  2.02137518e+00\n",
      "Epoch: 9114 mean train loss:  2.16140470e-05, mean val. loss:  2.02147007e+00\n",
      "Epoch: 9115 mean train loss:  2.16804910e-05, mean val. loss:  2.02151084e+00\n",
      "Epoch: 9116 mean train loss:  2.18777568e-05, mean val. loss:  2.02152276e+00\n",
      "Epoch: 9117 mean train loss:  2.16685294e-05, mean val. loss:  2.02152133e+00\n",
      "Epoch: 9118 mean train loss:  2.19112553e-05, mean val. loss:  2.02147937e+00\n",
      "Epoch: 9119 mean train loss:  2.16454791e-05, mean val. loss:  2.02144551e+00\n",
      "Epoch: 9120 mean train loss:  2.17826746e-05, mean val. loss:  2.02145863e+00\n",
      "Epoch: 9121 mean train loss:  2.15541222e-05, mean val. loss:  2.02149796e+00\n",
      "Epoch: 9122 mean train loss:  2.14303436e-05, mean val. loss:  2.02153254e+00\n",
      "Epoch: 9123 mean train loss:  2.14322936e-05, mean val. loss:  2.02154326e+00\n",
      "Epoch: 9124 mean train loss:  2.16748158e-05, mean val. loss:  2.02154636e+00\n",
      "Epoch: 9125 mean train loss:  2.16750195e-05, mean val. loss:  2.02149343e+00\n",
      "Epoch: 9126 mean train loss:  2.19688809e-05, mean val. loss:  2.02146339e+00\n",
      "Epoch: 9127 mean train loss:  2.16106419e-05, mean val. loss:  2.02145791e+00\n",
      "Epoch: 9128 mean train loss:  2.17679190e-05, mean val. loss:  2.02144361e+00\n",
      "Epoch: 9129 mean train loss:  2.16960907e-05, mean val. loss:  2.02144098e+00\n",
      "Epoch: 9130 mean train loss:  2.15263863e-05, mean val. loss:  2.02146029e+00\n",
      "Epoch: 9131 mean train loss:  2.12415180e-05, mean val. loss:  2.02149153e+00\n",
      "Epoch: 9132 mean train loss:  2.15088949e-05, mean val. loss:  2.02151752e+00\n",
      "Epoch: 9133 mean train loss:  2.12077575e-05, mean val. loss:  2.02159882e+00\n",
      "Epoch: 9134 mean train loss:  2.17499037e-05, mean val. loss:  2.02165508e+00\n",
      "Epoch: 9135 mean train loss:  2.15449254e-05, mean val. loss:  2.02175164e+00\n",
      "Epoch: 9136 mean train loss:  2.15863693e-05, mean val. loss:  2.02186108e+00\n",
      "Epoch: 9137 mean train loss:  2.16906774e-05, mean val. loss:  2.02198696e+00\n",
      "Epoch: 9138 mean train loss:  2.14930333e-05, mean val. loss:  2.02209926e+00\n",
      "Epoch: 9139 mean train loss:  2.14593310e-05, mean val. loss:  2.02225161e+00\n",
      "Epoch: 9140 mean train loss:  2.11109000e-05, mean val. loss:  2.02242756e+00\n",
      "Epoch: 9141 mean train loss:  2.19492067e-05, mean val. loss:  2.02256870e+00\n",
      "Epoch: 9142 mean train loss:  2.17962952e-05, mean val. loss:  2.02269363e+00\n",
      "Epoch: 9143 mean train loss:  2.17111374e-05, mean val. loss:  2.02280569e+00\n",
      "Epoch: 9144 mean train loss:  2.15863110e-05, mean val. loss:  2.02289629e+00\n",
      "Epoch: 9145 mean train loss:  2.15191685e-05, mean val. loss:  2.02298808e+00\n",
      "Epoch: 9146 mean train loss:  2.20177753e-05, mean val. loss:  2.02304840e+00\n",
      "Epoch: 9147 mean train loss:  2.21235678e-05, mean val. loss:  2.02307773e+00\n",
      "Epoch: 9148 mean train loss:  2.16875051e-05, mean val. loss:  2.02308393e+00\n",
      "Epoch: 9149 mean train loss:  2.17177148e-05, mean val. loss:  2.02310324e+00\n",
      "Epoch: 9150 mean train loss:  2.13469611e-05, mean val. loss:  2.02310634e+00\n",
      "Epoch: 9151 mean train loss:  2.19287758e-05, mean val. loss:  2.02307677e+00\n",
      "Epoch: 9152 mean train loss:  2.15707696e-05, mean val. loss:  2.02306795e+00\n",
      "Epoch: 9153 mean train loss:  2.16422195e-05, mean val. loss:  2.02305508e+00\n",
      "Epoch: 9154 mean train loss:  2.14529573e-05, mean val. loss:  2.02303243e+00\n",
      "Epoch: 9155 mean train loss:  2.16745830e-05, mean val. loss:  2.02300000e+00\n",
      "Epoch: 9156 mean train loss:  2.14531610e-05, mean val. loss:  2.02295566e+00\n",
      "Epoch: 9157 mean train loss:  2.17224006e-05, mean val. loss:  2.02292371e+00\n",
      "Epoch: 9158 mean train loss:  2.16173066e-05, mean val. loss:  2.02289605e+00\n",
      "Epoch: 9159 mean train loss:  2.13616877e-05, mean val. loss:  2.02287960e+00\n",
      "Epoch: 9160 mean train loss:  2.12716695e-05, mean val. loss:  2.02286339e+00\n",
      "Epoch: 9161 mean train loss:  2.16499320e-05, mean val. loss:  2.02285624e+00\n",
      "Epoch: 9162 mean train loss:  2.16357294e-05, mean val. loss:  2.02284884e+00\n",
      "Epoch: 9163 mean train loss:  2.15904438e-05, mean val. loss:  2.02288294e+00\n",
      "Epoch: 9164 mean train loss:  2.16156768e-05, mean val. loss:  2.02290654e+00\n",
      "Epoch: 9165 mean train loss:  2.16874469e-05, mean val. loss:  2.02293205e+00\n",
      "Epoch: 9166 mean train loss:  2.18612549e-05, mean val. loss:  2.02297616e+00\n",
      "Epoch: 9167 mean train loss:  2.18719069e-05, mean val. loss:  2.02304602e+00\n",
      "Epoch: 9168 mean train loss:  2.15522887e-05, mean val. loss:  2.02312565e+00\n",
      "Epoch: 9169 mean train loss:  2.18623900e-05, mean val. loss:  2.02317786e+00\n",
      "Epoch: 9170 mean train loss:  2.15077889e-05, mean val. loss:  2.02322364e+00\n",
      "Epoch: 9171 mean train loss:  2.16477492e-05, mean val. loss:  2.02328587e+00\n",
      "Epoch: 9172 mean train loss:  2.16717599e-05, mean val. loss:  2.02335620e+00\n",
      "Epoch: 9173 mean train loss:  2.15913460e-05, mean val. loss:  2.02341914e+00\n",
      "Epoch: 9174 mean train loss:  2.20032816e-05, mean val. loss:  2.02350068e+00\n",
      "Epoch: 9175 mean train loss:  2.18504574e-05, mean val. loss:  2.02358460e+00\n",
      "Epoch: 9176 mean train loss:  2.20361981e-05, mean val. loss:  2.02362657e+00\n",
      "Epoch: 9177 mean train loss:  2.18575587e-05, mean val. loss:  2.02363777e+00\n",
      "Epoch: 9178 mean train loss:  2.15582841e-05, mean val. loss:  2.02363729e+00\n",
      "Epoch: 9179 mean train loss:  2.17644556e-05, mean val. loss:  2.02362490e+00\n",
      "Epoch: 9180 mean train loss:  2.18956557e-05, mean val. loss:  2.02360463e+00\n",
      "Epoch: 9181 mean train loss:  2.16193730e-05, mean val. loss:  2.02360225e+00\n",
      "Epoch: 9182 mean train loss:  2.17068300e-05, mean val. loss:  2.02358174e+00\n",
      "Epoch: 9183 mean train loss:  2.18429195e-05, mean val. loss:  2.02351451e+00\n",
      "Epoch: 9184 mean train loss:  2.17445486e-05, mean val. loss:  2.02346468e+00\n",
      "Epoch: 9185 mean train loss:  2.16009794e-05, mean val. loss:  2.02338648e+00\n",
      "Epoch: 9186 mean train loss:  2.12726882e-05, mean val. loss:  2.02334404e+00\n",
      "Epoch: 9187 mean train loss:  2.16990302e-05, mean val. loss:  2.02330780e+00\n",
      "Epoch: 9188 mean train loss:  2.15106411e-05, mean val. loss:  2.02325988e+00\n",
      "Epoch: 9189 mean train loss:  2.15392793e-05, mean val. loss:  2.02323198e+00\n",
      "Epoch: 9190 mean train loss:  2.18214991e-05, mean val. loss:  2.02317476e+00\n",
      "Epoch: 9191 mean train loss:  2.17159977e-05, mean val. loss:  2.02314973e+00\n",
      "Epoch: 9192 mean train loss:  2.18457426e-05, mean val. loss:  2.02316785e+00\n",
      "Epoch: 9193 mean train loss:  2.18071218e-05, mean val. loss:  2.02318311e+00\n",
      "Epoch: 9194 mean train loss:  2.15866603e-05, mean val. loss:  2.02319312e+00\n",
      "Epoch: 9195 mean train loss:  2.12887535e-05, mean val. loss:  2.02326870e+00\n",
      "Epoch: 9196 mean train loss:  2.14655593e-05, mean val. loss:  2.02337050e+00\n",
      "Epoch: 9197 mean train loss:  2.15360487e-05, mean val. loss:  2.02350116e+00\n",
      "Epoch: 9198 mean train loss:  2.15012114e-05, mean val. loss:  2.02364540e+00\n",
      "Epoch: 9199 mean train loss:  2.17711204e-05, mean val. loss:  2.02379894e+00\n",
      "Epoch: 9200 mean train loss:  2.12412851e-05, mean val. loss:  2.02397609e+00\n",
      "Epoch: 9201 mean train loss:  2.17502238e-05, mean val. loss:  2.02413249e+00\n",
      "Epoch: 9202 mean train loss:  2.18120986e-05, mean val. loss:  2.02428818e+00\n",
      "Epoch: 9203 mean train loss:  2.19466747e-05, mean val. loss:  2.02449656e+00\n",
      "Epoch: 9204 mean train loss:  2.20303482e-05, mean val. loss:  2.02470136e+00\n",
      "Epoch: 9205 mean train loss:  2.15236796e-05, mean val. loss:  2.02495432e+00\n",
      "Epoch: 9206 mean train loss:  2.17569468e-05, mean val. loss:  2.02521014e+00\n",
      "Epoch: 9207 mean train loss:  2.11635779e-05, mean val. loss:  2.02546906e+00\n",
      "Epoch: 9208 mean train loss:  2.15728942e-05, mean val. loss:  2.02573204e+00\n",
      "Epoch: 9209 mean train loss:  2.13606982e-05, mean val. loss:  2.02596307e+00\n",
      "Epoch: 9210 mean train loss:  2.17272609e-05, mean val. loss:  2.02617812e+00\n",
      "Epoch: 9211 mean train loss:  2.14584288e-05, mean val. loss:  2.02638745e+00\n",
      "Epoch: 9212 mean train loss:  2.19762442e-05, mean val. loss:  2.02656651e+00\n",
      "Epoch: 9213 mean train loss:  2.15264445e-05, mean val. loss:  2.02672911e+00\n",
      "Epoch: 9214 mean train loss:  2.17902416e-05, mean val. loss:  2.02692604e+00\n",
      "Epoch: 9215 mean train loss:  2.14762695e-05, mean val. loss:  2.02707672e+00\n",
      "Epoch: 9216 mean train loss:  2.18657660e-05, mean val. loss:  2.02719665e+00\n",
      "Epoch: 9217 mean train loss:  2.14989122e-05, mean val. loss:  2.02731133e+00\n",
      "Epoch: 9218 mean train loss:  2.14594766e-05, mean val. loss:  2.02740645e+00\n",
      "Epoch: 9219 mean train loss:  2.15215259e-05, mean val. loss:  2.02751708e+00\n",
      "Epoch: 9220 mean train loss:  2.16203916e-05, mean val. loss:  2.02760959e+00\n",
      "Epoch: 9221 mean train loss:  2.16803164e-05, mean val. loss:  2.02768588e+00\n",
      "Epoch: 9222 mean train loss:  2.14911124e-05, mean val. loss:  2.02776599e+00\n",
      "Epoch: 9223 mean train loss:  2.13046151e-05, mean val. loss:  2.02785563e+00\n",
      "Epoch: 9224 mean train loss:  2.15363107e-05, mean val. loss:  2.02793312e+00\n",
      "Epoch: 9225 mean train loss:  2.23703100e-05, mean val. loss:  2.02802300e+00\n",
      "Epoch: 9226 mean train loss:  2.15440814e-05, mean val. loss:  2.02812672e+00\n",
      "Epoch: 9227 mean train loss:  2.15279288e-05, mean val. loss:  2.02822042e+00\n",
      "Epoch: 9228 mean train loss:  2.14877946e-05, mean val. loss:  2.02832341e+00\n",
      "Epoch: 9229 mean train loss:  2.15472246e-05, mean val. loss:  2.02843404e+00\n",
      "Epoch: 9230 mean train loss:  2.15399486e-05, mean val. loss:  2.02852345e+00\n",
      "Epoch: 9231 mean train loss:  2.12808372e-05, mean val. loss:  2.02860093e+00\n",
      "Epoch: 9232 mean train loss:  2.15847394e-05, mean val. loss:  2.02867484e+00\n",
      "Epoch: 9233 mean train loss:  2.17756024e-05, mean val. loss:  2.02872634e+00\n",
      "Epoch: 9234 mean train loss:  2.13461753e-05, mean val. loss:  2.02874970e+00\n",
      "Epoch: 9235 mean train loss:  2.11305742e-05, mean val. loss:  2.02876830e+00\n",
      "Epoch: 9236 mean train loss:  2.13877647e-05, mean val. loss:  2.02875924e+00\n",
      "Epoch: 9237 mean train loss:  2.13424792e-05, mean val. loss:  2.02877975e+00\n",
      "Epoch: 9238 mean train loss:  2.11269071e-05, mean val. loss:  2.02881956e+00\n",
      "Epoch: 9239 mean train loss:  2.18118366e-05, mean val. loss:  2.02890515e+00\n",
      "Epoch: 9240 mean train loss:  2.16897170e-05, mean val. loss:  2.02898145e+00\n",
      "Epoch: 9241 mean train loss:  2.16874178e-05, mean val. loss:  2.02908492e+00\n",
      "Epoch: 9242 mean train loss:  2.14111060e-05, mean val. loss:  2.02918482e+00\n",
      "Epoch: 9243 mean train loss:  2.13966414e-05, mean val. loss:  2.02928996e+00\n",
      "Epoch: 9244 mean train loss:  2.13828753e-05, mean val. loss:  2.02938151e+00\n",
      "Epoch: 9245 mean train loss:  2.15184700e-05, mean val. loss:  2.02949595e+00\n",
      "Epoch: 9246 mean train loss:  2.15523469e-05, mean val. loss:  2.02958941e+00\n",
      "Epoch: 9247 mean train loss:  2.15661712e-05, mean val. loss:  2.02966428e+00\n",
      "Epoch: 9248 mean train loss:  2.16676563e-05, mean val. loss:  2.02972817e+00\n",
      "Epoch: 9249 mean train loss:  2.17659981e-05, mean val. loss:  2.02979040e+00\n",
      "Epoch: 9250 mean train loss:  2.16837216e-05, mean val. loss:  2.02984405e+00\n",
      "Epoch: 9251 mean train loss:  2.17932975e-05, mean val. loss:  2.02989054e+00\n",
      "Epoch: 9252 mean train loss:  2.15057808e-05, mean val. loss:  2.02987576e+00\n",
      "Epoch: 9253 mean train loss:  2.14002794e-05, mean val. loss:  2.02984118e+00\n",
      "Epoch: 9254 mean train loss:  2.18232453e-05, mean val. loss:  2.02979279e+00\n",
      "Epoch: 9255 mean train loss:  2.15958280e-05, mean val. loss:  2.02973390e+00\n",
      "Epoch: 9256 mean train loss:  2.16342742e-05, mean val. loss:  2.02966547e+00\n",
      "Epoch: 9257 mean train loss:  2.15765031e-05, mean val. loss:  2.02958250e+00\n",
      "Epoch: 9258 mean train loss:  2.20071524e-05, mean val. loss:  2.02949309e+00\n",
      "Epoch: 9259 mean train loss:  2.16418703e-05, mean val. loss:  2.02941036e+00\n",
      "Epoch: 9260 mean train loss:  2.13837484e-05, mean val. loss:  2.02931619e+00\n",
      "Epoch: 9261 mean train loss:  2.15545297e-05, mean val. loss:  2.02924347e+00\n",
      "Epoch: 9262 mean train loss:  2.13268795e-05, mean val. loss:  2.02924323e+00\n",
      "Epoch: 9263 mean train loss:  2.11374136e-05, mean val. loss:  2.02924609e+00\n",
      "Epoch: 9264 mean train loss:  2.17404158e-05, mean val. loss:  2.02926016e+00\n",
      "Epoch: 9265 mean train loss:  2.12612213e-05, mean val. loss:  2.02931046e+00\n",
      "Epoch: 9266 mean train loss:  2.15405598e-05, mean val. loss:  2.02941751e+00\n",
      "Epoch: 9267 mean train loss:  2.13776948e-05, mean val. loss:  2.02955031e+00\n",
      "Epoch: 9268 mean train loss:  2.14556640e-05, mean val. loss:  2.02972317e+00\n",
      "Epoch: 9269 mean train loss:  2.16825574e-05, mean val. loss:  2.02990294e+00\n",
      "Epoch: 9270 mean train loss:  2.17572961e-05, mean val. loss:  2.03007793e+00\n",
      "Epoch: 9271 mean train loss:  2.16308690e-05, mean val. loss:  2.03024006e+00\n",
      "Epoch: 9272 mean train loss:  2.15042382e-05, mean val. loss:  2.03045034e+00\n",
      "Epoch: 9273 mean train loss:  2.14641623e-05, mean val. loss:  2.03065205e+00\n",
      "Epoch: 9274 mean train loss:  2.10533035e-05, mean val. loss:  2.03087759e+00\n",
      "Epoch: 9275 mean train loss:  2.16728076e-05, mean val. loss:  2.03109694e+00\n",
      "Epoch: 9276 mean train loss:  2.14873871e-05, mean val. loss:  2.03130198e+00\n",
      "Epoch: 9277 mean train loss:  2.15265027e-05, mean val. loss:  2.03150415e+00\n",
      "Epoch: 9278 mean train loss:  2.13440508e-05, mean val. loss:  2.03167534e+00\n",
      "Epoch: 9279 mean train loss:  2.11898296e-05, mean val. loss:  2.03185892e+00\n",
      "Epoch: 9280 mean train loss:  2.19226931e-05, mean val. loss:  2.03200245e+00\n",
      "Epoch: 9281 mean train loss:  2.14725151e-05, mean val. loss:  2.03210807e+00\n",
      "Epoch: 9282 mean train loss:  2.14113097e-05, mean val. loss:  2.03220987e+00\n",
      "Epoch: 9283 mean train loss:  2.18083442e-05, mean val. loss:  2.03229833e+00\n",
      "Epoch: 9284 mean train loss:  2.13336316e-05, mean val. loss:  2.03241110e+00\n",
      "Epoch: 9285 mean train loss:  2.10844737e-05, mean val. loss:  2.03250790e+00\n",
      "Epoch: 9286 mean train loss:  2.17198976e-05, mean val. loss:  2.03256392e+00\n",
      "Epoch: 9287 mean train loss:  2.14739703e-05, mean val. loss:  2.03262854e+00\n",
      "Epoch: 9288 mean train loss:  2.16075859e-05, mean val. loss:  2.03264189e+00\n",
      "Epoch: 9289 mean train loss:  2.13514722e-05, mean val. loss:  2.03263664e+00\n",
      "Epoch: 9290 mean train loss:  2.16663175e-05, mean val. loss:  2.03262472e+00\n",
      "Epoch: 9291 mean train loss:  2.14668398e-05, mean val. loss:  2.03256869e+00\n",
      "Epoch: 9292 mean train loss:  2.15055479e-05, mean val. loss:  2.03251910e+00\n",
      "Epoch: 9293 mean train loss:  2.14208849e-05, mean val. loss:  2.03245711e+00\n",
      "Epoch: 9294 mean train loss:  2.15942855e-05, mean val. loss:  2.03239202e+00\n",
      "Epoch: 9295 mean train loss:  2.15460605e-05, mean val. loss:  2.03230619e+00\n",
      "Epoch: 9296 mean train loss:  2.16659100e-05, mean val. loss:  2.03222132e+00\n",
      "Epoch: 9297 mean train loss:  2.14956526e-05, mean val. loss:  2.03217220e+00\n",
      "Epoch: 9298 mean train loss:  2.14254833e-05, mean val. loss:  2.03212214e+00\n",
      "Epoch: 9299 mean train loss:  2.18675414e-05, mean val. loss:  2.03210235e+00\n",
      "Epoch: 9300 mean train loss:  2.12863670e-05, mean val. loss:  2.03211498e+00\n",
      "Epoch: 9301 mean train loss:  2.17834022e-05, mean val. loss:  2.03211403e+00\n",
      "Epoch: 9302 mean train loss:  2.13205931e-05, mean val. loss:  2.03209352e+00\n",
      "Epoch: 9303 mean train loss:  2.16102053e-05, mean val. loss:  2.03210258e+00\n",
      "Epoch: 9304 mean train loss:  2.13349413e-05, mean val. loss:  2.03205824e+00\n",
      "Epoch: 9305 mean train loss:  2.11643928e-05, mean val. loss:  2.03209448e+00\n",
      "Epoch: 9306 mean train loss:  2.17503984e-05, mean val. loss:  2.03209567e+00\n",
      "Epoch: 9307 mean train loss:  2.16778426e-05, mean val. loss:  2.03208709e+00\n",
      "Epoch: 9308 mean train loss:  2.16529879e-05, mean val. loss:  2.03204751e+00\n",
      "Epoch: 9309 mean train loss:  2.15533655e-05, mean val. loss:  2.03202605e+00\n",
      "Epoch: 9310 mean train loss:  2.12478917e-05, mean val. loss:  2.03200150e+00\n",
      "Epoch: 9311 mean train loss:  2.17767083e-05, mean val. loss:  2.03194427e+00\n",
      "Epoch: 9312 mean train loss:  2.13060994e-05, mean val. loss:  2.03191185e+00\n",
      "Epoch: 9313 mean train loss:  2.16511544e-05, mean val. loss:  2.03186798e+00\n",
      "Epoch: 9314 mean train loss:  2.20756920e-05, mean val. loss:  2.03184891e+00\n",
      "Epoch: 9315 mean train loss:  2.14133179e-05, mean val. loss:  2.03182435e+00\n",
      "Epoch: 9316 mean train loss:  2.14730389e-05, mean val. loss:  2.03180289e+00\n",
      "Epoch: 9317 mean train loss:  2.12444575e-05, mean val. loss:  2.03179002e+00\n",
      "Epoch: 9318 mean train loss:  2.14101165e-05, mean val. loss:  2.03176475e+00\n",
      "Epoch: 9319 mean train loss:  2.17730121e-05, mean val. loss:  2.03176355e+00\n",
      "Epoch: 9320 mean train loss:  2.12976593e-05, mean val. loss:  2.03180218e+00\n",
      "Epoch: 9321 mean train loss:  2.15675391e-05, mean val. loss:  2.03182769e+00\n",
      "Epoch: 9322 mean train loss:  2.15780456e-05, mean val. loss:  2.03189802e+00\n",
      "Epoch: 9323 mean train loss:  2.13687599e-05, mean val. loss:  2.03198576e+00\n",
      "Epoch: 9324 mean train loss:  2.14623287e-05, mean val. loss:  2.03210568e+00\n",
      "Epoch: 9325 mean train loss:  2.15036853e-05, mean val. loss:  2.03224492e+00\n",
      "Epoch: 9326 mean train loss:  2.17135239e-05, mean val. loss:  2.03239799e+00\n",
      "Epoch: 9327 mean train loss:  2.14807515e-05, mean val. loss:  2.03255391e+00\n",
      "Epoch: 9328 mean train loss:  2.14731845e-05, mean val. loss:  2.03270125e+00\n",
      "Epoch: 9329 mean train loss:  2.16622429e-05, mean val. loss:  2.03286171e+00\n",
      "Epoch: 9330 mean train loss:  2.12236482e-05, mean val. loss:  2.03304267e+00\n",
      "Epoch: 9331 mean train loss:  2.12233281e-05, mean val. loss:  2.03325510e+00\n",
      "Epoch: 9332 mean train loss:  2.17418419e-05, mean val. loss:  2.03348494e+00\n",
      "Epoch: 9333 mean train loss:  2.13417807e-05, mean val. loss:  2.03369808e+00\n",
      "Epoch: 9334 mean train loss:  2.12421874e-05, mean val. loss:  2.03388238e+00\n",
      "Epoch: 9335 mean train loss:  2.18147470e-05, mean val. loss:  2.03404665e+00\n",
      "Epoch: 9336 mean train loss:  2.17181514e-05, mean val. loss:  2.03418064e+00\n",
      "Epoch: 9337 mean train loss:  2.14303145e-05, mean val. loss:  2.03431368e+00\n",
      "Epoch: 9338 mean train loss:  2.13263265e-05, mean val. loss:  2.03445554e+00\n",
      "Epoch: 9339 mean train loss:  2.12938176e-05, mean val. loss:  2.03460717e+00\n",
      "Epoch: 9340 mean train loss:  2.15002801e-05, mean val. loss:  2.03477073e+00\n",
      "Epoch: 9341 mean train loss:  2.15191976e-05, mean val. loss:  2.03491068e+00\n",
      "Epoch: 9342 mean train loss:  2.10923899e-05, mean val. loss:  2.03503537e+00\n",
      "Epoch: 9343 mean train loss:  2.16282497e-05, mean val. loss:  2.03515720e+00\n",
      "Epoch: 9344 mean train loss:  2.13332532e-05, mean val. loss:  2.03527737e+00\n",
      "Epoch: 9345 mean train loss:  2.13694002e-05, mean val. loss:  2.03538561e+00\n",
      "Epoch: 9346 mean train loss:  2.15804612e-05, mean val. loss:  2.03549743e+00\n",
      "Epoch: 9347 mean train loss:  2.12036539e-05, mean val. loss:  2.03560352e+00\n",
      "Epoch: 9348 mean train loss:  2.13817693e-05, mean val. loss:  2.03567266e+00\n",
      "Epoch: 9349 mean train loss:  2.14310712e-05, mean val. loss:  2.03572440e+00\n",
      "Epoch: 9350 mean train loss:  2.16122426e-05, mean val. loss:  2.03577352e+00\n",
      "Epoch: 9351 mean train loss:  2.15371256e-05, mean val. loss:  2.03578377e+00\n",
      "Epoch: 9352 mean train loss:  2.14424799e-05, mean val. loss:  2.03578496e+00\n",
      "Epoch: 9353 mean train loss:  2.14002503e-05, mean val. loss:  2.03582740e+00\n",
      "Epoch: 9354 mean train loss:  2.15248147e-05, mean val. loss:  2.03589487e+00\n",
      "Epoch: 9355 mean train loss:  2.15975451e-05, mean val. loss:  2.03596449e+00\n",
      "Epoch: 9356 mean train loss:  2.15266191e-05, mean val. loss:  2.03602552e+00\n",
      "Epoch: 9357 mean train loss:  2.13021412e-05, mean val. loss:  2.03607321e+00\n",
      "Epoch: 9358 mean train loss:  2.13984167e-05, mean val. loss:  2.03612781e+00\n",
      "Epoch: 9359 mean train loss:  2.15837790e-05, mean val. loss:  2.03619003e+00\n",
      "Epoch: 9360 mean train loss:  2.13675958e-05, mean val. loss:  2.03628278e+00\n",
      "Epoch: 9361 mean train loss:  2.11878214e-05, mean val. loss:  2.03637052e+00\n",
      "Epoch: 9362 mean train loss:  2.17143679e-05, mean val. loss:  2.03648138e+00\n",
      "Epoch: 9363 mean train loss:  2.13610474e-05, mean val. loss:  2.03660035e+00\n",
      "Epoch: 9364 mean train loss:  2.14955362e-05, mean val. loss:  2.03672695e+00\n",
      "Epoch: 9365 mean train loss:  2.13858730e-05, mean val. loss:  2.03687692e+00\n",
      "Epoch: 9366 mean train loss:  2.12182349e-05, mean val. loss:  2.03704453e+00\n",
      "Epoch: 9367 mean train loss:  2.10089493e-05, mean val. loss:  2.03718066e+00\n",
      "Epoch: 9368 mean train loss:  2.15118635e-05, mean val. loss:  2.03731298e+00\n",
      "Epoch: 9369 mean train loss:  2.14308093e-05, mean val. loss:  2.03742290e+00\n",
      "Epoch: 9370 mean train loss:  2.17058696e-05, mean val. loss:  2.03751421e+00\n",
      "Epoch: 9371 mean train loss:  2.11189908e-05, mean val. loss:  2.03760910e+00\n",
      "Epoch: 9372 mean train loss:  2.11567676e-05, mean val. loss:  2.03771877e+00\n",
      "Epoch: 9373 mean train loss:  2.15110485e-05, mean val. loss:  2.03779817e+00\n",
      "Epoch: 9374 mean train loss:  2.10902072e-05, mean val. loss:  2.03787971e+00\n",
      "Epoch: 9375 mean train loss:  2.14326719e-05, mean val. loss:  2.03797936e+00\n",
      "Epoch: 9376 mean train loss:  2.15015898e-05, mean val. loss:  2.03809118e+00\n",
      "Epoch: 9377 mean train loss:  2.14665197e-05, mean val. loss:  2.03820658e+00\n",
      "Epoch: 9378 mean train loss:  2.14361062e-05, mean val. loss:  2.03832603e+00\n",
      "Epoch: 9379 mean train loss:  2.16876506e-05, mean val. loss:  2.03844619e+00\n",
      "Epoch: 9380 mean train loss:  2.14456231e-05, mean val. loss:  2.03854156e+00\n",
      "Epoch: 9381 mean train loss:  2.13779858e-05, mean val. loss:  2.03859591e+00\n",
      "Epoch: 9382 mean train loss:  2.11628212e-05, mean val. loss:  2.03862238e+00\n",
      "Epoch: 9383 mean train loss:  2.16985936e-05, mean val. loss:  2.03863978e+00\n",
      "Epoch: 9384 mean train loss:  2.12648301e-05, mean val. loss:  2.03866935e+00\n",
      "Epoch: 9385 mean train loss:  2.12854357e-05, mean val. loss:  2.03866601e+00\n",
      "Epoch: 9386 mean train loss:  2.15434411e-05, mean val. loss:  2.03866267e+00\n",
      "Epoch: 9387 mean train loss:  2.13082531e-05, mean val. loss:  2.03869224e+00\n",
      "Epoch: 9388 mean train loss:  2.11282459e-05, mean val. loss:  2.03872085e+00\n",
      "Epoch: 9389 mean train loss:  2.11729784e-05, mean val. loss:  2.03877425e+00\n",
      "Epoch: 9390 mean train loss:  2.14883476e-05, mean val. loss:  2.03877401e+00\n",
      "Epoch: 9391 mean train loss:  2.13088642e-05, mean val. loss:  2.03877568e+00\n",
      "Epoch: 9392 mean train loss:  2.16242333e-05, mean val. loss:  2.03875875e+00\n",
      "Epoch: 9393 mean train loss:  2.14184402e-05, mean val. loss:  2.03872871e+00\n",
      "Epoch: 9394 mean train loss:  2.15050532e-05, mean val. loss:  2.03870678e+00\n",
      "Epoch: 9395 mean train loss:  2.12384330e-05, mean val. loss:  2.03867722e+00\n",
      "Epoch: 9396 mean train loss:  2.09182617e-05, mean val. loss:  2.03867960e+00\n",
      "Epoch: 9397 mean train loss:  2.10655271e-05, mean val. loss:  2.03870058e+00\n",
      "Epoch: 9398 mean train loss:  2.12347368e-05, mean val. loss:  2.03873181e+00\n",
      "Epoch: 9399 mean train loss:  2.16356711e-05, mean val. loss:  2.03877044e+00\n",
      "Epoch: 9400 mean train loss:  2.13061285e-05, mean val. loss:  2.03881240e+00\n",
      "Epoch: 9401 mean train loss:  2.10849685e-05, mean val. loss:  2.03889537e+00\n",
      "Epoch: 9402 mean train loss:  2.12444866e-05, mean val. loss:  2.03899360e+00\n",
      "Epoch: 9403 mean train loss:  2.14065367e-05, mean val. loss:  2.03907943e+00\n",
      "Epoch: 9404 mean train loss:  2.14159372e-05, mean val. loss:  2.03913498e+00\n",
      "Epoch: 9405 mean train loss:  2.15691107e-05, mean val. loss:  2.03919435e+00\n",
      "Epoch: 9406 mean train loss:  2.14793836e-05, mean val. loss:  2.03924656e+00\n",
      "Epoch: 9407 mean train loss:  2.12633749e-05, mean val. loss:  2.03930163e+00\n",
      "Epoch: 9408 mean train loss:  2.13273452e-05, mean val. loss:  2.03935361e+00\n",
      "Epoch: 9409 mean train loss:  2.14795000e-05, mean val. loss:  2.03941655e+00\n",
      "Epoch: 9410 mean train loss:  2.13504827e-05, mean val. loss:  2.03946114e+00\n",
      "Epoch: 9411 mean train loss:  2.14350875e-05, mean val. loss:  2.03951311e+00\n",
      "Epoch: 9412 mean train loss:  2.12627638e-05, mean val. loss:  2.03957200e+00\n",
      "Epoch: 9413 mean train loss:  2.13164021e-05, mean val. loss:  2.03963780e+00\n",
      "Epoch: 9414 mean train loss:  2.15554319e-05, mean val. loss:  2.03973389e+00\n",
      "Epoch: 9415 mean train loss:  2.15621549e-05, mean val. loss:  2.03981471e+00\n",
      "Epoch: 9416 mean train loss:  2.12105224e-05, mean val. loss:  2.03989863e+00\n",
      "Epoch: 9417 mean train loss:  2.15447217e-05, mean val. loss:  2.03993726e+00\n",
      "Epoch: 9418 mean train loss:  2.10666331e-05, mean val. loss:  2.03999090e+00\n",
      "Epoch: 9419 mean train loss:  2.13764433e-05, mean val. loss:  2.04004097e+00\n",
      "Epoch: 9420 mean train loss:  2.15496111e-05, mean val. loss:  2.04010630e+00\n",
      "Epoch: 9421 mean train loss:  2.11714942e-05, mean val. loss:  2.04019094e+00\n",
      "Epoch: 9422 mean train loss:  2.13852618e-05, mean val. loss:  2.04027033e+00\n",
      "Epoch: 9423 mean train loss:  2.13644817e-05, mean val. loss:  2.04035187e+00\n",
      "Epoch: 9424 mean train loss:  2.16070039e-05, mean val. loss:  2.04041052e+00\n",
      "Epoch: 9425 mean train loss:  2.14830507e-05, mean val. loss:  2.04048514e+00\n",
      "Epoch: 9426 mean train loss:  2.12341838e-05, mean val. loss:  2.04057240e+00\n",
      "Epoch: 9427 mean train loss:  2.11220176e-05, mean val. loss:  2.04067397e+00\n",
      "Epoch: 9428 mean train loss:  2.13928579e-05, mean val. loss:  2.04077601e+00\n",
      "Epoch: 9429 mean train loss:  2.09316204e-05, mean val. loss:  2.04087853e+00\n",
      "Epoch: 9430 mean train loss:  2.14501633e-05, mean val. loss:  2.04093909e+00\n",
      "Epoch: 9431 mean train loss:  2.12547602e-05, mean val. loss:  2.04099417e+00\n",
      "Epoch: 9432 mean train loss:  2.14762986e-05, mean val. loss:  2.04105592e+00\n",
      "Epoch: 9433 mean train loss:  2.11303413e-05, mean val. loss:  2.04114628e+00\n",
      "Epoch: 9434 mean train loss:  2.17228662e-05, mean val. loss:  2.04123354e+00\n",
      "Epoch: 9435 mean train loss:  2.11646257e-05, mean val. loss:  2.04132414e+00\n",
      "Epoch: 9436 mean train loss:  2.13524327e-05, mean val. loss:  2.04144549e+00\n",
      "Epoch: 9437 mean train loss:  2.12907034e-05, mean val. loss:  2.04154778e+00\n",
      "Epoch: 9438 mean train loss:  2.12494633e-05, mean val. loss:  2.04165626e+00\n",
      "Epoch: 9439 mean train loss:  2.12959130e-05, mean val. loss:  2.04178071e+00\n",
      "Epoch: 9440 mean train loss:  2.08400306e-05, mean val. loss:  2.04193068e+00\n",
      "Epoch: 9441 mean train loss:  2.14478059e-05, mean val. loss:  2.04210520e+00\n",
      "Epoch: 9442 mean train loss:  2.14533648e-05, mean val. loss:  2.04225302e+00\n",
      "Epoch: 9443 mean train loss:  2.15366308e-05, mean val. loss:  2.04239774e+00\n",
      "Epoch: 9444 mean train loss:  2.13115709e-05, mean val. loss:  2.04255462e+00\n",
      "Epoch: 9445 mean train loss:  2.12953018e-05, mean val. loss:  2.04270029e+00\n",
      "Epoch: 9446 mean train loss:  2.14983593e-05, mean val. loss:  2.04285645e+00\n",
      "Epoch: 9447 mean train loss:  2.09324062e-05, mean val. loss:  2.04303026e+00\n",
      "Epoch: 9448 mean train loss:  2.12993764e-05, mean val. loss:  2.04321432e+00\n",
      "Epoch: 9449 mean train loss:  2.11502193e-05, mean val. loss:  2.04336429e+00\n",
      "Epoch: 9450 mean train loss:  2.13489111e-05, mean val. loss:  2.04349566e+00\n",
      "Epoch: 9451 mean train loss:  2.14987376e-05, mean val. loss:  2.04358554e+00\n",
      "Epoch: 9452 mean train loss:  2.09513528e-05, mean val. loss:  2.04368758e+00\n",
      "Epoch: 9453 mean train loss:  2.10474827e-05, mean val. loss:  2.04379988e+00\n",
      "Epoch: 9454 mean train loss:  2.12354353e-05, mean val. loss:  2.04389811e+00\n",
      "Epoch: 9455 mean train loss:  2.10288854e-05, mean val. loss:  2.04399776e+00\n",
      "Epoch: 9456 mean train loss:  2.13509193e-05, mean val. loss:  2.04405880e+00\n",
      "Epoch: 9457 mean train loss:  2.13359308e-05, mean val. loss:  2.04413629e+00\n",
      "Epoch: 9458 mean train loss:  2.13571766e-05, mean val. loss:  2.04418564e+00\n",
      "Epoch: 9459 mean train loss:  2.12732994e-05, mean val. loss:  2.04420757e+00\n",
      "Epoch: 9460 mean train loss:  2.16957706e-05, mean val. loss:  2.04422164e+00\n",
      "Epoch: 9461 mean train loss:  2.12710293e-05, mean val. loss:  2.04421782e+00\n",
      "Epoch: 9462 mean train loss:  2.13563035e-05, mean val. loss:  2.04419780e+00\n",
      "Epoch: 9463 mean train loss:  2.11635488e-05, mean val. loss:  2.04417872e+00\n",
      "Epoch: 9464 mean train loss:  2.09220161e-05, mean val. loss:  2.04414701e+00\n",
      "Epoch: 9465 mean train loss:  2.11001607e-05, mean val. loss:  2.04411435e+00\n",
      "Epoch: 9466 mean train loss:  2.11918086e-05, mean val. loss:  2.04406977e+00\n",
      "Epoch: 9467 mean train loss:  2.10722792e-05, mean val. loss:  2.04403162e+00\n",
      "Epoch: 9468 mean train loss:  2.11617735e-05, mean val. loss:  2.04401302e+00\n",
      "Epoch: 9469 mean train loss:  2.10434082e-05, mean val. loss:  2.04398060e+00\n",
      "Epoch: 9470 mean train loss:  2.14837783e-05, mean val. loss:  2.04395723e+00\n",
      "Epoch: 9471 mean train loss:  2.11343868e-05, mean val. loss:  2.04398441e+00\n",
      "Epoch: 9472 mean train loss:  2.12717277e-05, mean val. loss:  2.04399991e+00\n",
      "Epoch: 9473 mean train loss:  2.12365412e-05, mean val. loss:  2.04402781e+00\n",
      "Epoch: 9474 mean train loss:  2.12963496e-05, mean val. loss:  2.04406333e+00\n",
      "Epoch: 9475 mean train loss:  2.11138395e-05, mean val. loss:  2.04413581e+00\n",
      "Epoch: 9476 mean train loss:  2.11437582e-05, mean val. loss:  2.04420185e+00\n",
      "Epoch: 9477 mean train loss:  2.12010927e-05, mean val. loss:  2.04429269e+00\n",
      "Epoch: 9478 mean train loss:  2.10878206e-05, mean val. loss:  2.04438877e+00\n",
      "Epoch: 9479 mean train loss:  2.11317092e-05, mean val. loss:  2.04451132e+00\n",
      "Epoch: 9480 mean train loss:  2.15970795e-05, mean val. loss:  2.04461002e+00\n",
      "Epoch: 9481 mean train loss:  2.13255698e-05, mean val. loss:  2.04472733e+00\n",
      "Epoch: 9482 mean train loss:  2.12655868e-05, mean val. loss:  2.04481506e+00\n",
      "Epoch: 9483 mean train loss:  2.13423627e-05, mean val. loss:  2.04491901e+00\n",
      "Epoch: 9484 mean train loss:  2.09363352e-05, mean val. loss:  2.04504037e+00\n",
      "Epoch: 9485 mean train loss:  2.15566542e-05, mean val. loss:  2.04515505e+00\n",
      "Epoch: 9486 mean train loss:  2.14230677e-05, mean val. loss:  2.04527330e+00\n",
      "Epoch: 9487 mean train loss:  2.10848521e-05, mean val. loss:  2.04545832e+00\n",
      "Epoch: 9488 mean train loss:  2.11843580e-05, mean val. loss:  2.04562593e+00\n",
      "Epoch: 9489 mean train loss:  2.14272295e-05, mean val. loss:  2.04575324e+00\n",
      "Epoch: 9490 mean train loss:  2.14515603e-05, mean val. loss:  2.04586911e+00\n",
      "Epoch: 9491 mean train loss:  2.12203304e-05, mean val. loss:  2.04596710e+00\n",
      "Epoch: 9492 mean train loss:  2.10687285e-05, mean val. loss:  2.04605961e+00\n",
      "Epoch: 9493 mean train loss:  2.13784224e-05, mean val. loss:  2.04613972e+00\n",
      "Epoch: 9494 mean train loss:  2.10375292e-05, mean val. loss:  2.04624510e+00\n",
      "Epoch: 9495 mean train loss:  2.13761814e-05, mean val. loss:  2.04632163e+00\n",
      "Epoch: 9496 mean train loss:  2.14608735e-05, mean val. loss:  2.04636335e+00\n",
      "Epoch: 9497 mean train loss:  2.10947474e-05, mean val. loss:  2.04642987e+00\n",
      "Epoch: 9498 mean train loss:  2.14683532e-05, mean val. loss:  2.04646325e+00\n",
      "Epoch: 9499 mean train loss:  2.09932041e-05, mean val. loss:  2.04648590e+00\n",
      "Epoch: 9500 mean train loss:  2.11118022e-05, mean val. loss:  2.04648638e+00\n",
      "Epoch: 9501 mean train loss:  2.14726606e-05, mean val. loss:  2.04652119e+00\n",
      "Epoch: 9502 mean train loss:  2.11753068e-05, mean val. loss:  2.04656816e+00\n",
      "Epoch: 9503 mean train loss:  2.08113634e-05, mean val. loss:  2.04662538e+00\n",
      "Epoch: 9504 mean train loss:  2.10534490e-05, mean val. loss:  2.04670191e+00\n",
      "Epoch: 9505 mean train loss:  2.06787663e-05, mean val. loss:  2.04679084e+00\n",
      "Epoch: 9506 mean train loss:  2.14206520e-05, mean val. loss:  2.04685879e+00\n",
      "Epoch: 9507 mean train loss:  2.14781030e-05, mean val. loss:  2.04692030e+00\n",
      "Epoch: 9508 mean train loss:  2.17410270e-05, mean val. loss:  2.04696202e+00\n",
      "Epoch: 9509 mean train loss:  2.12608720e-05, mean val. loss:  2.04700303e+00\n",
      "Epoch: 9510 mean train loss:  2.12234736e-05, mean val. loss:  2.04703355e+00\n",
      "Epoch: 9511 mean train loss:  2.15228647e-05, mean val. loss:  2.04706645e+00\n",
      "Epoch: 9512 mean train loss:  2.15200125e-05, mean val. loss:  2.04706573e+00\n",
      "Epoch: 9513 mean train loss:  2.14043830e-05, mean val. loss:  2.04708695e+00\n",
      "Epoch: 9514 mean train loss:  2.08815909e-05, mean val. loss:  2.04709363e+00\n",
      "Epoch: 9515 mean train loss:  2.11303122e-05, mean val. loss:  2.04709077e+00\n",
      "Epoch: 9516 mean train loss:  2.13898602e-05, mean val. loss:  2.04709363e+00\n",
      "Epoch: 9517 mean train loss:  2.12502491e-05, mean val. loss:  2.04708886e+00\n",
      "Epoch: 9518 mean train loss:  2.11696315e-05, mean val. loss:  2.04706430e+00\n",
      "Epoch: 9519 mean train loss:  2.14741449e-05, mean val. loss:  2.04706049e+00\n",
      "Epoch: 9520 mean train loss:  2.11325823e-05, mean val. loss:  2.04707694e+00\n",
      "Epoch: 9521 mean train loss:  2.13856983e-05, mean val. loss:  2.04704309e+00\n",
      "Epoch: 9522 mean train loss:  2.11035658e-05, mean val. loss:  2.04699111e+00\n",
      "Epoch: 9523 mean train loss:  2.11305160e-05, mean val. loss:  2.04696536e+00\n",
      "Epoch: 9524 mean train loss:  2.08157580e-05, mean val. loss:  2.04694033e+00\n",
      "Epoch: 9525 mean train loss:  2.11950392e-05, mean val. loss:  2.04692411e+00\n",
      "Epoch: 9526 mean train loss:  2.14049651e-05, mean val. loss:  2.04693079e+00\n",
      "Epoch: 9527 mean train loss:  2.13018502e-05, mean val. loss:  2.04693723e+00\n",
      "Epoch: 9528 mean train loss:  2.10378785e-05, mean val. loss:  2.04696536e+00\n",
      "Epoch: 9529 mean train loss:  2.15152686e-05, mean val. loss:  2.04701543e+00\n",
      "Epoch: 9530 mean train loss:  2.11186998e-05, mean val. loss:  2.04708052e+00\n",
      "Epoch: 9531 mean train loss:  2.13160238e-05, mean val. loss:  2.04715300e+00\n",
      "Epoch: 9532 mean train loss:  2.15396867e-05, mean val. loss:  2.04722428e+00\n",
      "Epoch: 9533 mean train loss:  2.11121514e-05, mean val. loss:  2.04735684e+00\n",
      "Epoch: 9534 mean train loss:  2.09484133e-05, mean val. loss:  2.04749393e+00\n",
      "Epoch: 9535 mean train loss:  2.15059263e-05, mean val. loss:  2.04764676e+00\n",
      "Epoch: 9536 mean train loss:  2.12449522e-05, mean val. loss:  2.04777551e+00\n",
      "Epoch: 9537 mean train loss:  2.10963190e-05, mean val. loss:  2.04790378e+00\n",
      "Epoch: 9538 mean train loss:  2.16408807e-05, mean val. loss:  2.04798150e+00\n",
      "Epoch: 9539 mean train loss:  2.12265877e-05, mean val. loss:  2.04810905e+00\n",
      "Epoch: 9540 mean train loss:  2.13694584e-05, mean val. loss:  2.04823232e+00\n",
      "Epoch: 9541 mean train loss:  2.07721605e-05, mean val. loss:  2.04836297e+00\n",
      "Epoch: 9542 mean train loss:  2.14104657e-05, mean val. loss:  2.04847789e+00\n",
      "Epoch: 9543 mean train loss:  2.14109896e-05, mean val. loss:  2.04858470e+00\n",
      "Epoch: 9544 mean train loss:  2.12575833e-05, mean val. loss:  2.04867530e+00\n",
      "Epoch: 9545 mean train loss:  2.10866565e-05, mean val. loss:  2.04878521e+00\n",
      "Epoch: 9546 mean train loss:  2.15158798e-05, mean val. loss:  2.04887605e+00\n",
      "Epoch: 9547 mean train loss:  2.08897691e-05, mean val. loss:  2.04895830e+00\n",
      "Epoch: 9548 mean train loss:  2.08858401e-05, mean val. loss:  2.04907489e+00\n",
      "Epoch: 9549 mean train loss:  2.12374434e-05, mean val. loss:  2.04920292e+00\n",
      "Epoch: 9550 mean train loss:  2.14959728e-05, mean val. loss:  2.04931617e+00\n",
      "Epoch: 9551 mean train loss:  2.11822626e-05, mean val. loss:  2.04948187e+00\n",
      "Epoch: 9552 mean train loss:  2.11304869e-05, mean val. loss:  2.04967690e+00\n",
      "Epoch: 9553 mean train loss:  2.13276071e-05, mean val. loss:  2.04983234e+00\n",
      "Epoch: 9554 mean train loss:  2.13312742e-05, mean val. loss:  2.05001283e+00\n",
      "Epoch: 9555 mean train loss:  2.12365994e-05, mean val. loss:  2.05015779e+00\n",
      "Epoch: 9556 mean train loss:  2.07702687e-05, mean val. loss:  2.05030870e+00\n",
      "Epoch: 9557 mean train loss:  2.11930019e-05, mean val. loss:  2.05041933e+00\n",
      "Epoch: 9558 mean train loss:  2.09424470e-05, mean val. loss:  2.05053854e+00\n",
      "Epoch: 9559 mean train loss:  2.10619473e-05, mean val. loss:  2.05063581e+00\n",
      "Epoch: 9560 mean train loss:  2.12698942e-05, mean val. loss:  2.05073738e+00\n",
      "Epoch: 9561 mean train loss:  2.13000167e-05, mean val. loss:  2.05087113e+00\n",
      "Epoch: 9562 mean train loss:  2.10959115e-05, mean val. loss:  2.05102563e+00\n",
      "Epoch: 9563 mean train loss:  2.09753052e-05, mean val. loss:  2.05117059e+00\n",
      "Epoch: 9564 mean train loss:  2.11741426e-05, mean val. loss:  2.05132604e+00\n",
      "Epoch: 9565 mean train loss:  2.13726598e-05, mean val. loss:  2.05146956e+00\n",
      "Epoch: 9566 mean train loss:  2.11138977e-05, mean val. loss:  2.05159521e+00\n",
      "Epoch: 9567 mean train loss:  2.10262369e-05, mean val. loss:  2.05170488e+00\n",
      "Epoch: 9568 mean train loss:  2.13430612e-05, mean val. loss:  2.05180597e+00\n",
      "Epoch: 9569 mean train loss:  2.10570870e-05, mean val. loss:  2.05189681e+00\n",
      "Epoch: 9570 mean train loss:  2.11891020e-05, mean val. loss:  2.05197692e+00\n",
      "Epoch: 9571 mean train loss:  2.15186737e-05, mean val. loss:  2.05205774e+00\n",
      "Epoch: 9572 mean train loss:  2.10887520e-05, mean val. loss:  2.05211926e+00\n",
      "Epoch: 9573 mean train loss:  2.10674480e-05, mean val. loss:  2.05219316e+00\n",
      "Epoch: 9574 mean train loss:  2.11406441e-05, mean val. loss:  2.05227447e+00\n",
      "Epoch: 9575 mean train loss:  2.09840946e-05, mean val. loss:  2.05234838e+00\n",
      "Epoch: 9576 mean train loss:  2.08762649e-05, mean val. loss:  2.05239248e+00\n",
      "Epoch: 9577 mean train loss:  2.10796134e-05, mean val. loss:  2.05248594e+00\n",
      "Epoch: 9578 mean train loss:  2.14090978e-05, mean val. loss:  2.05260253e+00\n",
      "Epoch: 9579 mean train loss:  2.11948354e-05, mean val. loss:  2.05270720e+00\n",
      "Epoch: 9580 mean train loss:  2.12847663e-05, mean val. loss:  2.05278134e+00\n",
      "Epoch: 9581 mean train loss:  2.11687002e-05, mean val. loss:  2.05284905e+00\n",
      "Epoch: 9582 mean train loss:  2.11691076e-05, mean val. loss:  2.05293941e+00\n",
      "Epoch: 9583 mean train loss:  2.08774582e-05, mean val. loss:  2.05304003e+00\n",
      "Epoch: 9584 mean train loss:  2.13966996e-05, mean val. loss:  2.05309319e+00\n",
      "Epoch: 9585 mean train loss:  2.09573482e-05, mean val. loss:  2.05315375e+00\n",
      "Epoch: 9586 mean train loss:  2.10491708e-05, mean val. loss:  2.05320787e+00\n",
      "Epoch: 9587 mean train loss:  2.11740262e-05, mean val. loss:  2.05326414e+00\n",
      "Epoch: 9588 mean train loss:  2.08854908e-05, mean val. loss:  2.05330443e+00\n",
      "Epoch: 9589 mean train loss:  2.08526326e-05, mean val. loss:  2.05336547e+00\n",
      "Epoch: 9590 mean train loss:  2.13234744e-05, mean val. loss:  2.05343127e+00\n",
      "Epoch: 9591 mean train loss:  2.11130537e-05, mean val. loss:  2.05346990e+00\n",
      "Epoch: 9592 mean train loss:  2.13395397e-05, mean val. loss:  2.05348730e+00\n",
      "Epoch: 9593 mean train loss:  2.12844170e-05, mean val. loss:  2.05350065e+00\n",
      "Epoch: 9594 mean train loss:  2.07284174e-05, mean val. loss:  2.05350018e+00\n",
      "Epoch: 9595 mean train loss:  2.08546116e-05, mean val. loss:  2.05354357e+00\n",
      "Epoch: 9596 mean train loss:  2.08992860e-05, mean val. loss:  2.05357957e+00\n",
      "Epoch: 9597 mean train loss:  2.13093008e-05, mean val. loss:  2.05359864e+00\n",
      "Epoch: 9598 mean train loss:  2.10945436e-05, mean val. loss:  2.05366802e+00\n",
      "Epoch: 9599 mean train loss:  2.13394233e-05, mean val. loss:  2.05373120e+00\n",
      "Epoch: 9600 mean train loss:  2.12083396e-05, mean val. loss:  2.05378175e+00\n",
      "Epoch: 9601 mean train loss:  2.08311249e-05, mean val. loss:  2.05385208e+00\n",
      "Epoch: 9602 mean train loss:  2.11259176e-05, mean val. loss:  2.05393600e+00\n",
      "Epoch: 9603 mean train loss:  2.12592422e-05, mean val. loss:  2.05400276e+00\n",
      "Epoch: 9604 mean train loss:  2.12948944e-05, mean val. loss:  2.05408382e+00\n",
      "Epoch: 9605 mean train loss:  2.12042942e-05, mean val. loss:  2.05418181e+00\n",
      "Epoch: 9606 mean train loss:  2.15606124e-05, mean val. loss:  2.05426788e+00\n",
      "Epoch: 9607 mean train loss:  2.13096791e-05, mean val. loss:  2.05435848e+00\n",
      "Epoch: 9608 mean train loss:  2.12414016e-05, mean val. loss:  2.05445528e+00\n",
      "Epoch: 9609 mean train loss:  2.12042069e-05, mean val. loss:  2.05455399e+00\n",
      "Epoch: 9610 mean train loss:  2.12995510e-05, mean val. loss:  2.05463529e+00\n",
      "Epoch: 9611 mean train loss:  2.10370054e-05, mean val. loss:  2.05472112e+00\n",
      "Epoch: 9612 mean train loss:  2.11290026e-05, mean val. loss:  2.05480814e+00\n",
      "Epoch: 9613 mean train loss:  2.11122679e-05, mean val. loss:  2.05488253e+00\n",
      "Epoch: 9614 mean train loss:  2.13927415e-05, mean val. loss:  2.05491233e+00\n",
      "Epoch: 9615 mean train loss:  2.11498409e-05, mean val. loss:  2.05497527e+00\n",
      "Epoch: 9616 mean train loss:  2.11610459e-05, mean val. loss:  2.05502152e+00\n",
      "Epoch: 9617 mean train loss:  2.11447477e-05, mean val. loss:  2.05508232e+00\n",
      "Epoch: 9618 mean train loss:  2.09592690e-05, mean val. loss:  2.05515814e+00\n",
      "Epoch: 9619 mean train loss:  2.11520237e-05, mean val. loss:  2.05522418e+00\n",
      "Epoch: 9620 mean train loss:  2.13322928e-05, mean val. loss:  2.05525184e+00\n",
      "Epoch: 9621 mean train loss:  2.11834849e-05, mean val. loss:  2.05528808e+00\n",
      "Epoch: 9622 mean train loss:  2.13415478e-05, mean val. loss:  2.05532908e+00\n",
      "Epoch: 9623 mean train loss:  2.11957376e-05, mean val. loss:  2.05538177e+00\n",
      "Epoch: 9624 mean train loss:  2.09505088e-05, mean val. loss:  2.05540705e+00\n",
      "Epoch: 9625 mean train loss:  2.11318838e-05, mean val. loss:  2.05539966e+00\n",
      "Epoch: 9626 mean train loss:  2.10507715e-05, mean val. loss:  2.05541992e+00\n",
      "Epoch: 9627 mean train loss:  2.12498708e-05, mean val. loss:  2.05543637e+00\n",
      "Epoch: 9628 mean train loss:  2.07488774e-05, mean val. loss:  2.05547428e+00\n",
      "Epoch: 9629 mean train loss:  2.11977749e-05, mean val. loss:  2.05553460e+00\n",
      "Epoch: 9630 mean train loss:  2.11561564e-05, mean val. loss:  2.05559254e+00\n",
      "Epoch: 9631 mean train loss:  2.14432948e-05, mean val. loss:  2.05562735e+00\n",
      "Epoch: 9632 mean train loss:  2.08138954e-05, mean val. loss:  2.05564809e+00\n",
      "Epoch: 9633 mean train loss:  2.14444881e-05, mean val. loss:  2.05564070e+00\n",
      "Epoch: 9634 mean train loss:  2.10919534e-05, mean val. loss:  2.05566454e+00\n",
      "Epoch: 9635 mean train loss:  2.10949511e-05, mean val. loss:  2.05568576e+00\n",
      "Epoch: 9636 mean train loss:  2.11576116e-05, mean val. loss:  2.05573583e+00\n",
      "Epoch: 9637 mean train loss:  2.07108387e-05, mean val. loss:  2.05576873e+00\n",
      "Epoch: 9638 mean train loss:  2.12713785e-05, mean val. loss:  2.05577946e+00\n",
      "Epoch: 9639 mean train loss:  2.08648271e-05, mean val. loss:  2.05578589e+00\n",
      "Epoch: 9640 mean train loss:  2.09897989e-05, mean val. loss:  2.05578804e+00\n",
      "Epoch: 9641 mean train loss:  2.11506558e-05, mean val. loss:  2.05578542e+00\n",
      "Epoch: 9642 mean train loss:  2.12180603e-05, mean val. loss:  2.05578780e+00\n",
      "Epoch: 9643 mean train loss:  2.13206513e-05, mean val. loss:  2.05579066e+00\n",
      "Epoch: 9644 mean train loss:  2.06958794e-05, mean val. loss:  2.05582547e+00\n",
      "Epoch: 9645 mean train loss:  2.10798753e-05, mean val. loss:  2.05587077e+00\n",
      "Epoch: 9646 mean train loss:  2.12826999e-05, mean val. loss:  2.05594587e+00\n",
      "Epoch: 9647 mean train loss:  2.09648861e-05, mean val. loss:  2.05606270e+00\n",
      "Epoch: 9648 mean train loss:  2.08540878e-05, mean val. loss:  2.05621219e+00\n",
      "Epoch: 9649 mean train loss:  2.10374128e-05, mean val. loss:  2.05638862e+00\n",
      "Epoch: 9650 mean train loss:  2.09793798e-05, mean val. loss:  2.05659699e+00\n",
      "Epoch: 9651 mean train loss:  2.09820864e-05, mean val. loss:  2.05681729e+00\n",
      "Epoch: 9652 mean train loss:  2.09108111e-05, mean val. loss:  2.05705452e+00\n",
      "Epoch: 9653 mean train loss:  2.09076097e-05, mean val. loss:  2.05732131e+00\n",
      "Epoch: 9654 mean train loss:  2.10060680e-05, mean val. loss:  2.05759382e+00\n",
      "Epoch: 9655 mean train loss:  2.14590691e-05, mean val. loss:  2.05784321e+00\n",
      "Epoch: 9656 mean train loss:  2.12875020e-05, mean val. loss:  2.05808187e+00\n",
      "Epoch: 9657 mean train loss:  2.09022255e-05, mean val. loss:  2.05836535e+00\n",
      "Epoch: 9658 mean train loss:  2.11703300e-05, mean val. loss:  2.05860806e+00\n",
      "Epoch: 9659 mean train loss:  2.12305167e-05, mean val. loss:  2.05884671e+00\n",
      "Epoch: 9660 mean train loss:  2.10360740e-05, mean val. loss:  2.05907869e+00\n",
      "Epoch: 9661 mean train loss:  2.11145380e-05, mean val. loss:  2.05931187e+00\n",
      "Epoch: 9662 mean train loss:  2.06347613e-05, mean val. loss:  2.05955648e+00\n",
      "Epoch: 9663 mean train loss:  2.11040315e-05, mean val. loss:  2.05977750e+00\n",
      "Epoch: 9664 mean train loss:  2.10860744e-05, mean val. loss:  2.05997920e+00\n",
      "Epoch: 9665 mean train loss:  2.09474820e-05, mean val. loss:  2.06014132e+00\n",
      "Epoch: 9666 mean train loss:  2.12472223e-05, mean val. loss:  2.06024051e+00\n",
      "Epoch: 9667 mean train loss:  2.09994032e-05, mean val. loss:  2.06031656e+00\n",
      "Epoch: 9668 mean train loss:  2.08939018e-05, mean val. loss:  2.06037259e+00\n",
      "Epoch: 9669 mean train loss:  2.07453268e-05, mean val. loss:  2.06042528e+00\n",
      "Epoch: 9670 mean train loss:  2.12337181e-05, mean val. loss:  2.06043935e+00\n",
      "Epoch: 9671 mean train loss:  2.13457970e-05, mean val. loss:  2.06042886e+00\n",
      "Epoch: 9672 mean train loss:  2.07862176e-05, mean val. loss:  2.06041574e+00\n",
      "Epoch: 9673 mean train loss:  2.08347337e-05, mean val. loss:  2.06042171e+00\n",
      "Epoch: 9674 mean train loss:  2.13840976e-05, mean val. loss:  2.06041050e+00\n",
      "Epoch: 9675 mean train loss:  2.07416306e-05, mean val. loss:  2.06041646e+00\n",
      "Epoch: 9676 mean train loss:  2.13143940e-05, mean val. loss:  2.06041908e+00\n",
      "Epoch: 9677 mean train loss:  2.11299048e-05, mean val. loss:  2.06042910e+00\n",
      "Epoch: 9678 mean train loss:  2.10524013e-05, mean val. loss:  2.06045914e+00\n",
      "Epoch: 9679 mean train loss:  2.08289712e-05, mean val. loss:  2.06050372e+00\n",
      "Epoch: 9680 mean train loss:  2.14877946e-05, mean val. loss:  2.06053543e+00\n",
      "Epoch: 9681 mean train loss:  2.12404120e-05, mean val. loss:  2.06057382e+00\n",
      "Epoch: 9682 mean train loss:  2.10512371e-05, mean val. loss:  2.06064129e+00\n",
      "Epoch: 9683 mean train loss:  2.10998987e-05, mean val. loss:  2.06069541e+00\n",
      "Epoch: 9684 mean train loss:  2.09697173e-05, mean val. loss:  2.06076980e+00\n",
      "Epoch: 9685 mean train loss:  2.12032464e-05, mean val. loss:  2.06084347e+00\n",
      "Epoch: 9686 mean train loss:  2.12723098e-05, mean val. loss:  2.06092215e+00\n",
      "Epoch: 9687 mean train loss:  2.10547587e-05, mean val. loss:  2.06102085e+00\n",
      "Epoch: 9688 mean train loss:  2.10298458e-05, mean val. loss:  2.06110549e+00\n",
      "Epoch: 9689 mean train loss:  2.10075523e-05, mean val. loss:  2.06117558e+00\n",
      "Epoch: 9690 mean train loss:  2.12028972e-05, mean val. loss:  2.06123829e+00\n",
      "Epoch: 9691 mean train loss:  2.12552841e-05, mean val. loss:  2.06126308e+00\n",
      "Epoch: 9692 mean train loss:  2.07001285e-05, mean val. loss:  2.06131029e+00\n",
      "Epoch: 9693 mean train loss:  2.09925347e-05, mean val. loss:  2.06137800e+00\n",
      "Epoch: 9694 mean train loss:  2.11364531e-05, mean val. loss:  2.06143999e+00\n",
      "Epoch: 9695 mean train loss:  2.09782447e-05, mean val. loss:  2.06153083e+00\n",
      "Epoch: 9696 mean train loss:  2.09442514e-05, mean val. loss:  2.06162524e+00\n",
      "Epoch: 9697 mean train loss:  2.13037420e-05, mean val. loss:  2.06169963e+00\n",
      "Epoch: 9698 mean train loss:  2.10979488e-05, mean val. loss:  2.06178284e+00\n",
      "Epoch: 9699 mean train loss:  2.07735866e-05, mean val. loss:  2.06188560e+00\n",
      "Epoch: 9700 mean train loss:  2.10650906e-05, mean val. loss:  2.06196666e+00\n",
      "Epoch: 9701 mean train loss:  2.06492550e-05, mean val. loss:  2.06207466e+00\n",
      "Epoch: 9702 mean train loss:  2.06920668e-05, mean val. loss:  2.06221128e+00\n",
      "Epoch: 9703 mean train loss:  2.08269339e-05, mean val. loss:  2.06233525e+00\n",
      "Epoch: 9704 mean train loss:  2.12196610e-05, mean val. loss:  2.06247115e+00\n",
      "Epoch: 9705 mean train loss:  2.10345897e-05, mean val. loss:  2.06263137e+00\n",
      "Epoch: 9706 mean train loss:  2.09482096e-05, mean val. loss:  2.06280351e+00\n",
      "Epoch: 9707 mean train loss:  2.08047568e-05, mean val. loss:  2.06300449e+00\n",
      "Epoch: 9708 mean train loss:  2.11315928e-05, mean val. loss:  2.06319642e+00\n",
      "Epoch: 9709 mean train loss:  2.10134313e-05, mean val. loss:  2.06336451e+00\n",
      "Epoch: 9710 mean train loss:  2.14042375e-05, mean val. loss:  2.06350541e+00\n",
      "Epoch: 9711 mean train loss:  2.13200401e-05, mean val. loss:  2.06362581e+00\n",
      "Epoch: 9712 mean train loss:  2.10612779e-05, mean val. loss:  2.06370234e+00\n",
      "Epoch: 9713 mean train loss:  2.08757119e-05, mean val. loss:  2.06374955e+00\n",
      "Epoch: 9714 mean train loss:  2.05886608e-05, mean val. loss:  2.06376052e+00\n",
      "Epoch: 9715 mean train loss:  2.10584840e-05, mean val. loss:  2.06378102e+00\n",
      "Epoch: 9716 mean train loss:  2.13264429e-05, mean val. loss:  2.06378627e+00\n",
      "Epoch: 9717 mean train loss:  2.12515006e-05, mean val. loss:  2.06378174e+00\n",
      "Epoch: 9718 mean train loss:  2.12131417e-05, mean val. loss:  2.06376767e+00\n",
      "Epoch: 9719 mean train loss:  2.10864819e-05, mean val. loss:  2.06373072e+00\n",
      "Epoch: 9720 mean train loss:  2.07779813e-05, mean val. loss:  2.06367588e+00\n",
      "Epoch: 9721 mean train loss:  2.11302831e-05, mean val. loss:  2.06356907e+00\n",
      "Epoch: 9722 mean train loss:  2.08517595e-05, mean val. loss:  2.06345677e+00\n",
      "Epoch: 9723 mean train loss:  2.15023174e-05, mean val. loss:  2.06334043e+00\n",
      "Epoch: 9724 mean train loss:  2.13262683e-05, mean val. loss:  2.06320691e+00\n",
      "Epoch: 9725 mean train loss:  2.07096164e-05, mean val. loss:  2.06311035e+00\n",
      "Epoch: 9726 mean train loss:  2.08102283e-05, mean val. loss:  2.06303072e+00\n",
      "Epoch: 9727 mean train loss:  2.09312129e-05, mean val. loss:  2.06297541e+00\n",
      "Epoch: 9728 mean train loss:  2.11481238e-05, mean val. loss:  2.06291509e+00\n",
      "Epoch: 9729 mean train loss:  2.07393896e-05, mean val. loss:  2.06287289e+00\n",
      "Epoch: 9730 mean train loss:  2.10430298e-05, mean val. loss:  2.06282783e+00\n",
      "Epoch: 9731 mean train loss:  2.08842393e-05, mean val. loss:  2.06282949e+00\n",
      "Epoch: 9732 mean train loss:  2.11508886e-05, mean val. loss:  2.06283855e+00\n",
      "Epoch: 9733 mean train loss:  2.08724523e-05, mean val. loss:  2.06283545e+00\n",
      "Epoch: 9734 mean train loss:  2.10270227e-05, mean val. loss:  2.06285000e+00\n",
      "Epoch: 9735 mean train loss:  2.08670390e-05, mean val. loss:  2.06291342e+00\n",
      "Epoch: 9736 mean train loss:  2.10566795e-05, mean val. loss:  2.06297159e+00\n",
      "Epoch: 9737 mean train loss:  2.08311249e-05, mean val. loss:  2.06306028e+00\n",
      "Epoch: 9738 mean train loss:  2.10543803e-05, mean val. loss:  2.06317735e+00\n",
      "Epoch: 9739 mean train loss:  2.11041479e-05, mean val. loss:  2.06331062e+00\n",
      "Epoch: 9740 mean train loss:  2.08599376e-05, mean val. loss:  2.06347442e+00\n",
      "Epoch: 9741 mean train loss:  2.10258295e-05, mean val. loss:  2.06367636e+00\n",
      "Epoch: 9742 mean train loss:  2.10776343e-05, mean val. loss:  2.06389451e+00\n",
      "Epoch: 9743 mean train loss:  2.08342099e-05, mean val. loss:  2.06408405e+00\n",
      "Epoch: 9744 mean train loss:  2.09329883e-05, mean val. loss:  2.06428409e+00\n",
      "Epoch: 9745 mean train loss:  2.08327547e-05, mean val. loss:  2.06451631e+00\n",
      "Epoch: 9746 mean train loss:  2.09507416e-05, mean val. loss:  2.06473422e+00\n",
      "Epoch: 9747 mean train loss:  2.08083075e-05, mean val. loss:  2.06498170e+00\n",
      "Epoch: 9748 mean train loss:  2.11250153e-05, mean val. loss:  2.06520271e+00\n",
      "Epoch: 9749 mean train loss:  2.11468723e-05, mean val. loss:  2.06543756e+00\n",
      "Epoch: 9750 mean train loss:  2.09230056e-05, mean val. loss:  2.06566143e+00\n",
      "Epoch: 9751 mean train loss:  2.11341248e-05, mean val. loss:  2.06584382e+00\n",
      "Epoch: 9752 mean train loss:  2.11716397e-05, mean val. loss:  2.06599617e+00\n",
      "Epoch: 9753 mean train loss:  2.09399732e-05, mean val. loss:  2.06612301e+00\n",
      "Epoch: 9754 mean train loss:  2.05841206e-05, mean val. loss:  2.06625795e+00\n",
      "Epoch: 9755 mean train loss:  2.08280107e-05, mean val. loss:  2.06639266e+00\n",
      "Epoch: 9756 mean train loss:  2.08888086e-05, mean val. loss:  2.06651545e+00\n",
      "Epoch: 9757 mean train loss:  2.11905572e-05, mean val. loss:  2.06662202e+00\n",
      "Epoch: 9758 mean train loss:  2.11414590e-05, mean val. loss:  2.06671214e+00\n",
      "Epoch: 9759 mean train loss:  2.08105776e-05, mean val. loss:  2.06682611e+00\n",
      "Epoch: 9760 mean train loss:  2.10368016e-05, mean val. loss:  2.06692410e+00\n",
      "Epoch: 9761 mean train loss:  2.10197468e-05, mean val. loss:  2.06702924e+00\n",
      "Epoch: 9762 mean train loss:  2.10487051e-05, mean val. loss:  2.06710315e+00\n",
      "Epoch: 9763 mean train loss:  2.11055740e-05, mean val. loss:  2.06714058e+00\n",
      "Epoch: 9764 mean train loss:  2.11423030e-05, mean val. loss:  2.06715631e+00\n",
      "Epoch: 9765 mean train loss:  2.10556900e-05, mean val. loss:  2.06716061e+00\n",
      "Epoch: 9766 mean train loss:  2.10144499e-05, mean val. loss:  2.06717467e+00\n",
      "Epoch: 9767 mean train loss:  2.11743754e-05, mean val. loss:  2.06713820e+00\n",
      "Epoch: 9768 mean train loss:  2.10919825e-05, mean val. loss:  2.06711507e+00\n",
      "Epoch: 9769 mean train loss:  2.06849072e-05, mean val. loss:  2.06712532e+00\n",
      "Epoch: 9770 mean train loss:  2.08199199e-05, mean val. loss:  2.06711650e+00\n",
      "Epoch: 9771 mean train loss:  2.10563012e-05, mean val. loss:  2.06713915e+00\n",
      "Epoch: 9772 mean train loss:  2.11867737e-05, mean val. loss:  2.06716228e+00\n",
      "Epoch: 9773 mean train loss:  2.12332816e-05, mean val. loss:  2.06717706e+00\n",
      "Epoch: 9774 mean train loss:  2.07389821e-05, mean val. loss:  2.06722689e+00\n",
      "Epoch: 9775 mean train loss:  2.07359844e-05, mean val. loss:  2.06728339e+00\n",
      "Epoch: 9776 mean train loss:  2.05550168e-05, mean val. loss:  2.06738687e+00\n",
      "Epoch: 9777 mean train loss:  2.07566191e-05, mean val. loss:  2.06751084e+00\n",
      "Epoch: 9778 mean train loss:  2.07719568e-05, mean val. loss:  2.06768608e+00\n",
      "Epoch: 9779 mean train loss:  2.10356084e-05, mean val. loss:  2.06789017e+00\n",
      "Epoch: 9780 mean train loss:  2.13782478e-05, mean val. loss:  2.06813335e+00\n",
      "Epoch: 9781 mean train loss:  2.09440186e-05, mean val. loss:  2.06840920e+00\n",
      "Epoch: 9782 mean train loss:  2.11080187e-05, mean val. loss:  2.06871128e+00\n",
      "Epoch: 9783 mean train loss:  2.05959950e-05, mean val. loss:  2.06902242e+00\n",
      "Epoch: 9784 mean train loss:  2.12080777e-05, mean val. loss:  2.06933498e+00\n",
      "Epoch: 9785 mean train loss:  2.07820558e-05, mean val. loss:  2.06967402e+00\n",
      "Epoch: 9786 mean train loss:  2.07316480e-05, mean val. loss:  2.07000041e+00\n",
      "Epoch: 9787 mean train loss:  2.12066225e-05, mean val. loss:  2.07028627e+00\n",
      "Epoch: 9788 mean train loss:  2.05421820e-05, mean val. loss:  2.07056427e+00\n",
      "Epoch: 9789 mean train loss:  2.08204729e-05, mean val. loss:  2.07082915e+00\n",
      "Epoch: 9790 mean train loss:  2.11445149e-05, mean val. loss:  2.07101393e+00\n",
      "Epoch: 9791 mean train loss:  2.11283332e-05, mean val. loss:  2.07111907e+00\n",
      "Epoch: 9792 mean train loss:  2.11077277e-05, mean val. loss:  2.07117343e+00\n",
      "Epoch: 9793 mean train loss:  2.12630839e-05, mean val. loss:  2.07119298e+00\n",
      "Epoch: 9794 mean train loss:  2.07160483e-05, mean val. loss:  2.07121015e+00\n",
      "Epoch: 9795 mean train loss:  2.06995464e-05, mean val. loss:  2.07121015e+00\n",
      "Epoch: 9796 mean train loss:  2.10528087e-05, mean val. loss:  2.07118678e+00\n",
      "Epoch: 9797 mean train loss:  2.08369747e-05, mean val. loss:  2.07113171e+00\n",
      "Epoch: 9798 mean train loss:  2.10067956e-05, mean val. loss:  2.07105422e+00\n",
      "Epoch: 9799 mean train loss:  2.11798470e-05, mean val. loss:  2.07097244e+00\n",
      "Epoch: 9800 mean train loss:  2.11932929e-05, mean val. loss:  2.07087135e+00\n",
      "Epoch: 9801 mean train loss:  2.06339173e-05, mean val. loss:  2.07076216e+00\n",
      "Epoch: 9802 mean train loss:  2.10851140e-05, mean val. loss:  2.07066727e+00\n",
      "Epoch: 9803 mean train loss:  2.09829886e-05, mean val. loss:  2.07059932e+00\n",
      "Epoch: 9804 mean train loss:  2.11762090e-05, mean val. loss:  2.07054806e+00\n",
      "Epoch: 9805 mean train loss:  2.12565938e-05, mean val. loss:  2.07045913e+00\n",
      "Epoch: 9806 mean train loss:  2.07589765e-05, mean val. loss:  2.07041574e+00\n",
      "Epoch: 9807 mean train loss:  2.07675039e-05, mean val. loss:  2.07037091e+00\n",
      "Epoch: 9808 mean train loss:  2.07769626e-05, mean val. loss:  2.07033777e+00\n",
      "Epoch: 9809 mean train loss:  2.06210243e-05, mean val. loss:  2.07037163e+00\n",
      "Epoch: 9810 mean train loss:  2.10225990e-05, mean val. loss:  2.07040668e+00\n",
      "Epoch: 9811 mean train loss:  2.10842118e-05, mean val. loss:  2.07046437e+00\n",
      "Epoch: 9812 mean train loss:  2.09986174e-05, mean val. loss:  2.07047868e+00\n",
      "Epoch: 9813 mean train loss:  2.08976562e-05, mean val. loss:  2.07050800e+00\n",
      "Epoch: 9814 mean train loss:  2.11959123e-05, mean val. loss:  2.07055116e+00\n",
      "Epoch: 9815 mean train loss:  2.08113051e-05, mean val. loss:  2.07064533e+00\n",
      "Epoch: 9816 mean train loss:  2.10596772e-05, mean val. loss:  2.07073641e+00\n",
      "Epoch: 9817 mean train loss:  2.09128484e-05, mean val. loss:  2.07083559e+00\n",
      "Epoch: 9818 mean train loss:  2.08263518e-05, mean val. loss:  2.07092047e+00\n",
      "Epoch: 9819 mean train loss:  2.06794066e-05, mean val. loss:  2.07104206e+00\n",
      "Epoch: 9820 mean train loss:  2.14642205e-05, mean val. loss:  2.07113576e+00\n",
      "Epoch: 9821 mean train loss:  2.07558041e-05, mean val. loss:  2.07126689e+00\n",
      "Epoch: 9822 mean train loss:  2.12807790e-05, mean val. loss:  2.07138801e+00\n",
      "Epoch: 9823 mean train loss:  2.12498999e-05, mean val. loss:  2.07151318e+00\n",
      "Epoch: 9824 mean train loss:  2.11015285e-05, mean val. loss:  2.07164121e+00\n",
      "Epoch: 9825 mean train loss:  2.11930892e-05, mean val. loss:  2.07179093e+00\n",
      "Epoch: 9826 mean train loss:  2.07164849e-05, mean val. loss:  2.07192588e+00\n",
      "Epoch: 9827 mean train loss:  2.09959107e-05, mean val. loss:  2.07205343e+00\n",
      "Epoch: 9828 mean train loss:  2.08739657e-05, mean val. loss:  2.07218122e+00\n",
      "Epoch: 9829 mean train loss:  2.08810670e-05, mean val. loss:  2.07230473e+00\n",
      "Epoch: 9830 mean train loss:  2.12560699e-05, mean val. loss:  2.07240748e+00\n",
      "Epoch: 9831 mean train loss:  2.08466954e-05, mean val. loss:  2.07248998e+00\n",
      "Epoch: 9832 mean train loss:  2.08021665e-05, mean val. loss:  2.07255030e+00\n",
      "Epoch: 9833 mean train loss:  2.10547296e-05, mean val. loss:  2.07258224e+00\n",
      "Epoch: 9834 mean train loss:  2.09018472e-05, mean val. loss:  2.07262182e+00\n",
      "Epoch: 9835 mean train loss:  2.08807760e-05, mean val. loss:  2.07266641e+00\n",
      "Epoch: 9836 mean train loss:  2.08549318e-05, mean val. loss:  2.07272387e+00\n",
      "Epoch: 9837 mean train loss:  2.08246929e-05, mean val. loss:  2.07282519e+00\n",
      "Epoch: 9838 mean train loss:  2.07830453e-05, mean val. loss:  2.07293558e+00\n",
      "Epoch: 9839 mean train loss:  2.10624130e-05, mean val. loss:  2.07302976e+00\n",
      "Epoch: 9840 mean train loss:  2.08050478e-05, mean val. loss:  2.07313323e+00\n",
      "Epoch: 9841 mean train loss:  2.08017882e-05, mean val. loss:  2.07324886e+00\n",
      "Epoch: 9842 mean train loss:  2.08684360e-05, mean val. loss:  2.07337952e+00\n",
      "Epoch: 9843 mean train loss:  2.10299040e-05, mean val. loss:  2.07352400e+00\n",
      "Epoch: 9844 mean train loss:  2.09202699e-05, mean val. loss:  2.07367015e+00\n",
      "Epoch: 9845 mean train loss:  2.08514975e-05, mean val. loss:  2.07381463e+00\n",
      "Epoch: 9846 mean train loss:  2.09962309e-05, mean val. loss:  2.07393169e+00\n",
      "Epoch: 9847 mean train loss:  2.08486163e-05, mean val. loss:  2.07407713e+00\n",
      "Epoch: 9848 mean train loss:  2.10459984e-05, mean val. loss:  2.07420850e+00\n",
      "Epoch: 9849 mean train loss:  2.12579325e-05, mean val. loss:  2.07432675e+00\n",
      "Epoch: 9850 mean train loss:  2.10205035e-05, mean val. loss:  2.07444930e+00\n",
      "Epoch: 9851 mean train loss:  2.06105469e-05, mean val. loss:  2.07456374e+00\n",
      "Epoch: 9852 mean train loss:  2.09482969e-05, mean val. loss:  2.07465553e+00\n",
      "Epoch: 9853 mean train loss:  2.07881094e-05, mean val. loss:  2.07474327e+00\n",
      "Epoch: 9854 mean train loss:  2.10287981e-05, mean val. loss:  2.07484889e+00\n",
      "Epoch: 9855 mean train loss:  2.06984987e-05, mean val. loss:  2.07496119e+00\n",
      "Epoch: 9856 mean train loss:  2.07835692e-05, mean val. loss:  2.07507682e+00\n",
      "Epoch: 9857 mean train loss:  2.11966108e-05, mean val. loss:  2.07514691e+00\n",
      "Epoch: 9858 mean train loss:  2.06929690e-05, mean val. loss:  2.07524800e+00\n",
      "Epoch: 9859 mean train loss:  2.08186102e-05, mean val. loss:  2.07536221e+00\n",
      "Epoch: 9860 mean train loss:  2.08024867e-05, mean val. loss:  2.07547331e+00\n",
      "Epoch: 9861 mean train loss:  2.08131387e-05, mean val. loss:  2.07560468e+00\n",
      "Epoch: 9862 mean train loss:  2.06573459e-05, mean val. loss:  2.07578206e+00\n",
      "Epoch: 9863 mean train loss:  2.10275757e-05, mean val. loss:  2.07597613e+00\n",
      "Epoch: 9864 mean train loss:  2.06155819e-05, mean val. loss:  2.07618022e+00\n",
      "Epoch: 9865 mean train loss:  2.09586287e-05, mean val. loss:  2.07638168e+00\n",
      "Epoch: 9866 mean train loss:  2.09521095e-05, mean val. loss:  2.07660508e+00\n",
      "Epoch: 9867 mean train loss:  2.07091216e-05, mean val. loss:  2.07683063e+00\n",
      "Epoch: 9868 mean train loss:  2.09874124e-05, mean val. loss:  2.07705784e+00\n",
      "Epoch: 9869 mean train loss:  2.13351159e-05, mean val. loss:  2.07725406e+00\n",
      "Epoch: 9870 mean train loss:  2.08297570e-05, mean val. loss:  2.07744098e+00\n",
      "Epoch: 9871 mean train loss:  2.08392448e-05, mean val. loss:  2.07759476e+00\n",
      "Epoch: 9872 mean train loss:  2.10448634e-05, mean val. loss:  2.07773066e+00\n",
      "Epoch: 9873 mean train loss:  2.08764104e-05, mean val. loss:  2.07787156e+00\n",
      "Epoch: 9874 mean train loss:  2.10828730e-05, mean val. loss:  2.07796407e+00\n",
      "Epoch: 9875 mean train loss:  2.08727142e-05, mean val. loss:  2.07800937e+00\n",
      "Epoch: 9876 mean train loss:  2.08556012e-05, mean val. loss:  2.07803488e+00\n",
      "Epoch: 9877 mean train loss:  2.07499252e-05, mean val. loss:  2.07804489e+00\n",
      "Epoch: 9878 mean train loss:  2.08443671e-05, mean val. loss:  2.07805824e+00\n",
      "Epoch: 9879 mean train loss:  2.09172431e-05, mean val. loss:  2.07805705e+00\n",
      "Epoch: 9880 mean train loss:  2.10705039e-05, mean val. loss:  2.07806802e+00\n",
      "Epoch: 9881 mean train loss:  2.10309518e-05, mean val. loss:  2.07805753e+00\n",
      "Epoch: 9882 mean train loss:  2.07762350e-05, mean val. loss:  2.07802486e+00\n",
      "Epoch: 9883 mean train loss:  2.04844691e-05, mean val. loss:  2.07800341e+00\n",
      "Epoch: 9884 mean train loss:  2.10626749e-05, mean val. loss:  2.07793713e+00\n",
      "Epoch: 9885 mean train loss:  2.09375576e-05, mean val. loss:  2.07783747e+00\n",
      "Epoch: 9886 mean train loss:  2.09495774e-05, mean val. loss:  2.07772541e+00\n",
      "Epoch: 9887 mean train loss:  2.09831342e-05, mean val. loss:  2.07763624e+00\n",
      "Epoch: 9888 mean train loss:  2.06081604e-05, mean val. loss:  2.07756186e+00\n",
      "Epoch: 9889 mean train loss:  2.07250414e-05, mean val. loss:  2.07749844e+00\n",
      "Epoch: 9890 mean train loss:  2.10478029e-05, mean val. loss:  2.07744598e+00\n",
      "Epoch: 9891 mean train loss:  2.08647398e-05, mean val. loss:  2.07742047e+00\n",
      "Epoch: 9892 mean train loss:  2.07004487e-05, mean val. loss:  2.07740426e+00\n",
      "Epoch: 9893 mean train loss:  2.08034762e-05, mean val. loss:  2.07738066e+00\n",
      "Epoch: 9894 mean train loss:  2.07043195e-05, mean val. loss:  2.07736444e+00\n",
      "Epoch: 9895 mean train loss:  2.09856953e-05, mean val. loss:  2.07736659e+00\n",
      "Epoch: 9896 mean train loss:  2.07649719e-05, mean val. loss:  2.07740593e+00\n",
      "Epoch: 9897 mean train loss:  2.06055993e-05, mean val. loss:  2.07745314e+00\n",
      "Epoch: 9898 mean train loss:  2.06307741e-05, mean val. loss:  2.07753110e+00\n",
      "Epoch: 9899 mean train loss:  2.08426791e-05, mean val. loss:  2.07763386e+00\n",
      "Epoch: 9900 mean train loss:  2.10209982e-05, mean val. loss:  2.07773733e+00\n",
      "Epoch: 9901 mean train loss:  2.08405545e-05, mean val. loss:  2.07787061e+00\n",
      "Epoch: 9902 mean train loss:  2.10497237e-05, mean val. loss:  2.07801962e+00\n",
      "Epoch: 9903 mean train loss:  2.08553683e-05, mean val. loss:  2.07817936e+00\n",
      "Epoch: 9904 mean train loss:  2.11675942e-05, mean val. loss:  2.07836461e+00\n",
      "Epoch: 9905 mean train loss:  2.08032725e-05, mean val. loss:  2.07858586e+00\n",
      "Epoch: 9906 mean train loss:  2.07393605e-05, mean val. loss:  2.07879734e+00\n",
      "Epoch: 9907 mean train loss:  2.08629062e-05, mean val. loss:  2.07900333e+00\n",
      "Epoch: 9908 mean train loss:  2.05668912e-05, mean val. loss:  2.07922149e+00\n",
      "Epoch: 9909 mean train loss:  2.07667763e-05, mean val. loss:  2.07943892e+00\n",
      "Epoch: 9910 mean train loss:  2.06403201e-05, mean val. loss:  2.07966590e+00\n",
      "Epoch: 9911 mean train loss:  2.10727449e-05, mean val. loss:  2.07988548e+00\n",
      "Epoch: 9912 mean train loss:  2.08369747e-05, mean val. loss:  2.08012104e+00\n",
      "Epoch: 9913 mean train loss:  2.06945115e-05, mean val. loss:  2.08037806e+00\n",
      "Epoch: 9914 mean train loss:  2.07612466e-05, mean val. loss:  2.08063412e+00\n",
      "Epoch: 9915 mean train loss:  2.06159020e-05, mean val. loss:  2.08088326e+00\n",
      "Epoch: 9916 mean train loss:  2.09240534e-05, mean val. loss:  2.08109093e+00\n",
      "Epoch: 9917 mean train loss:  2.06428231e-05, mean val. loss:  2.08128881e+00\n",
      "Epoch: 9918 mean train loss:  2.06877594e-05, mean val. loss:  2.08146787e+00\n",
      "Epoch: 9919 mean train loss:  2.06417171e-05, mean val. loss:  2.08164716e+00\n",
      "Epoch: 9920 mean train loss:  2.06449185e-05, mean val. loss:  2.08182168e+00\n",
      "Epoch: 9921 mean train loss:  2.10108119e-05, mean val. loss:  2.08198380e+00\n",
      "Epoch: 9922 mean train loss:  2.08334532e-05, mean val. loss:  2.08209658e+00\n",
      "Epoch: 9923 mean train loss:  2.07925623e-05, mean val. loss:  2.08216214e+00\n",
      "Epoch: 9924 mean train loss:  2.07120029e-05, mean val. loss:  2.08220673e+00\n",
      "Epoch: 9925 mean train loss:  2.06280674e-05, mean val. loss:  2.08224702e+00\n",
      "Epoch: 9926 mean train loss:  2.07664270e-05, mean val. loss:  2.08227992e+00\n",
      "Epoch: 9927 mean train loss:  2.07914272e-05, mean val. loss:  2.08224654e+00\n",
      "Epoch: 9928 mean train loss:  2.04253593e-05, mean val. loss:  2.08223844e+00\n",
      "Epoch: 9929 mean train loss:  2.11928564e-05, mean val. loss:  2.08221912e+00\n",
      "Epoch: 9930 mean train loss:  2.10215512e-05, mean val. loss:  2.08220983e+00\n",
      "Epoch: 9931 mean train loss:  2.06680852e-05, mean val. loss:  2.08220625e+00\n",
      "Epoch: 9932 mean train loss:  2.10058352e-05, mean val. loss:  2.08221579e+00\n",
      "Epoch: 9933 mean train loss:  2.06252153e-05, mean val. loss:  2.08223367e+00\n",
      "Epoch: 9934 mean train loss:  2.08141282e-05, mean val. loss:  2.08228898e+00\n",
      "Epoch: 9935 mean train loss:  2.07457051e-05, mean val. loss:  2.08236194e+00\n",
      "Epoch: 9936 mean train loss:  2.07303674e-05, mean val. loss:  2.08246136e+00\n",
      "Epoch: 9937 mean train loss:  2.08247220e-05, mean val. loss:  2.08255124e+00\n",
      "Epoch: 9938 mean train loss:  2.09418649e-05, mean val. loss:  2.08263659e+00\n",
      "Epoch: 9939 mean train loss:  2.09211721e-05, mean val. loss:  2.08270216e+00\n",
      "Epoch: 9940 mean train loss:  2.07423000e-05, mean val. loss:  2.08280563e+00\n",
      "Epoch: 9941 mean train loss:  2.09590944e-05, mean val. loss:  2.08293414e+00\n",
      "Epoch: 9942 mean train loss:  2.07971898e-05, mean val. loss:  2.08302426e+00\n",
      "Epoch: 9943 mean train loss:  2.06601107e-05, mean val. loss:  2.08313894e+00\n",
      "Epoch: 9944 mean train loss:  2.06688419e-05, mean val. loss:  2.08326650e+00\n",
      "Epoch: 9945 mean train loss:  2.07264093e-05, mean val. loss:  2.08338785e+00\n",
      "Epoch: 9946 mean train loss:  2.07304838e-05, mean val. loss:  2.08348727e+00\n",
      "Epoch: 9947 mean train loss:  2.10464932e-05, mean val. loss:  2.08357978e+00\n",
      "Epoch: 9948 mean train loss:  2.08728597e-05, mean val. loss:  2.08365130e+00\n",
      "Epoch: 9949 mean train loss:  2.08084239e-05, mean val. loss:  2.08372855e+00\n",
      "Epoch: 9950 mean train loss:  2.05439574e-05, mean val. loss:  2.08379412e+00\n",
      "Epoch: 9951 mean train loss:  2.07945704e-05, mean val. loss:  2.08384562e+00\n",
      "Epoch: 9952 mean train loss:  2.07516714e-05, mean val. loss:  2.08386397e+00\n",
      "Epoch: 9953 mean train loss:  2.09625869e-05, mean val. loss:  2.08387136e+00\n",
      "Epoch: 9954 mean train loss:  2.10642465e-05, mean val. loss:  2.08385706e+00\n",
      "Epoch: 9955 mean train loss:  2.03309755e-05, mean val. loss:  2.08382320e+00\n",
      "Epoch: 9956 mean train loss:  2.05201213e-05, mean val. loss:  2.08378553e+00\n",
      "Epoch: 9957 mean train loss:  2.07291159e-05, mean val. loss:  2.08374691e+00\n",
      "Epoch: 9958 mean train loss:  2.10171274e-05, mean val. loss:  2.08369517e+00\n",
      "Epoch: 9959 mean train loss:  2.07700359e-05, mean val. loss:  2.08369708e+00\n",
      "Epoch: 9960 mean train loss:  2.05448305e-05, mean val. loss:  2.08372879e+00\n",
      "Epoch: 9961 mean train loss:  2.08876445e-05, mean val. loss:  2.08375359e+00\n",
      "Epoch: 9962 mean train loss:  2.07060075e-05, mean val. loss:  2.08383107e+00\n",
      "Epoch: 9963 mean train loss:  2.09850841e-05, mean val. loss:  2.08393145e+00\n",
      "Epoch: 9964 mean train loss:  2.05457327e-05, mean val. loss:  2.08408308e+00\n",
      "Epoch: 9965 mean train loss:  2.08843267e-05, mean val. loss:  2.08424830e+00\n",
      "Epoch: 9966 mean train loss:  2.05560355e-05, mean val. loss:  2.08444715e+00\n",
      "Epoch: 9967 mean train loss:  2.05219840e-05, mean val. loss:  2.08468533e+00\n",
      "Epoch: 9968 mean train loss:  2.08132260e-05, mean val. loss:  2.08496332e+00\n",
      "Epoch: 9969 mean train loss:  2.05165707e-05, mean val. loss:  2.08523870e+00\n",
      "Epoch: 9970 mean train loss:  2.10136932e-05, mean val. loss:  2.08549380e+00\n",
      "Epoch: 9971 mean train loss:  2.09531572e-05, mean val. loss:  2.08570695e+00\n",
      "Epoch: 9972 mean train loss:  2.06894183e-05, mean val. loss:  2.08592463e+00\n",
      "Epoch: 9973 mean train loss:  2.07800476e-05, mean val. loss:  2.08611512e+00\n",
      "Epoch: 9974 mean train loss:  2.11240258e-05, mean val. loss:  2.08626199e+00\n",
      "Epoch: 9975 mean train loss:  2.08044366e-05, mean val. loss:  2.08640313e+00\n",
      "Epoch: 9976 mean train loss:  2.08523707e-05, mean val. loss:  2.08653092e+00\n",
      "Epoch: 9977 mean train loss:  2.07401463e-05, mean val. loss:  2.08663750e+00\n",
      "Epoch: 9978 mean train loss:  2.09404388e-05, mean val. loss:  2.08668709e+00\n",
      "Epoch: 9979 mean train loss:  2.06796685e-05, mean val. loss:  2.08672047e+00\n",
      "Epoch: 9980 mean train loss:  2.06896220e-05, mean val. loss:  2.08668900e+00\n",
      "Epoch: 9981 mean train loss:  2.07450066e-05, mean val. loss:  2.08663225e+00\n",
      "Epoch: 9982 mean train loss:  2.11172155e-05, mean val. loss:  2.08653545e+00\n",
      "Epoch: 9983 mean train loss:  2.07867997e-05, mean val. loss:  2.08644271e+00\n",
      "Epoch: 9984 mean train loss:  2.09262944e-05, mean val. loss:  2.08631873e+00\n",
      "Epoch: 9985 mean train loss:  2.07876437e-05, mean val. loss:  2.08618140e+00\n",
      "Epoch: 9986 mean train loss:  2.10751314e-05, mean val. loss:  2.08603501e+00\n",
      "Epoch: 9987 mean train loss:  2.07766716e-05, mean val. loss:  2.08589816e+00\n",
      "Epoch: 9988 mean train loss:  2.10371509e-05, mean val. loss:  2.08574939e+00\n",
      "Epoch: 9989 mean train loss:  2.07226258e-05, mean val. loss:  2.08563232e+00\n",
      "Epoch: 9990 mean train loss:  2.06978293e-05, mean val. loss:  2.08552504e+00\n",
      "Epoch: 9991 mean train loss:  2.09153513e-05, mean val. loss:  2.08544421e+00\n",
      "Epoch: 9992 mean train loss:  2.07498670e-05, mean val. loss:  2.08537769e+00\n",
      "Epoch: 9993 mean train loss:  2.09871505e-05, mean val. loss:  2.08533239e+00\n",
      "Epoch: 9994 mean train loss:  2.08521087e-05, mean val. loss:  2.08529568e+00\n",
      "Epoch: 9995 mean train loss:  2.06042896e-05, mean val. loss:  2.08527207e+00\n",
      "Epoch: 9996 mean train loss:  2.11292645e-05, mean val. loss:  2.08528686e+00\n",
      "Epoch: 9997 mean train loss:  2.09021964e-05, mean val. loss:  2.08533716e+00\n",
      "Epoch: 9998 mean train loss:  2.10126163e-05, mean val. loss:  2.08535814e+00\n",
      "Epoch: 9999 mean train loss:  2.09641003e-05, mean val. loss:  2.08540487e+00\n",
      "Epoch: 10000 mean train loss:  2.08333950e-05, mean val. loss:  2.08546782e+00\n",
      "Epoch: 10001 mean train loss:  2.06813274e-05, mean val. loss:  2.08556843e+00\n",
      "Epoch: 10002 mean train loss:  2.05727702e-05, mean val. loss:  2.08571243e+00\n",
      "Epoch: 10003 mean train loss:  2.11329025e-05, mean val. loss:  2.08587790e+00\n",
      "Epoch: 10004 mean train loss:  2.04296084e-05, mean val. loss:  2.08605409e+00\n",
      "Epoch: 10005 mean train loss:  2.05042888e-05, mean val. loss:  2.08624434e+00\n",
      "Epoch: 10006 mean train loss:  2.09360442e-05, mean val. loss:  2.08645749e+00\n",
      "Epoch: 10007 mean train loss:  2.07707344e-05, mean val. loss:  2.08666110e+00\n",
      "Epoch: 10008 mean train loss:  2.11379956e-05, mean val. loss:  2.08683920e+00\n",
      "Epoch: 10009 mean train loss:  2.04778335e-05, mean val. loss:  2.08702374e+00\n",
      "Epoch: 10010 mean train loss:  2.05640681e-05, mean val. loss:  2.08723950e+00\n",
      "Epoch: 10011 mean train loss:  2.03863019e-05, mean val. loss:  2.08747363e+00\n",
      "Epoch: 10012 mean train loss:  2.04760290e-05, mean val. loss:  2.08771253e+00\n",
      "Epoch: 10013 mean train loss:  2.10835424e-05, mean val. loss:  2.08796000e+00\n",
      "Epoch: 10014 mean train loss:  2.06967816e-05, mean val. loss:  2.08822155e+00\n",
      "Epoch: 10015 mean train loss:  2.03547825e-05, mean val. loss:  2.08849025e+00\n",
      "Epoch: 10016 mean train loss:  2.10565049e-05, mean val. loss:  2.08874750e+00\n",
      "Epoch: 10017 mean train loss:  2.06543191e-05, mean val. loss:  2.08895802e+00\n",
      "Epoch: 10018 mean train loss:  2.08721031e-05, mean val. loss:  2.08919430e+00\n",
      "Epoch: 10019 mean train loss:  2.06777768e-05, mean val. loss:  2.08944511e+00\n",
      "Epoch: 10020 mean train loss:  2.10547296e-05, mean val. loss:  2.08965707e+00\n",
      "Epoch: 10021 mean train loss:  2.08815618e-05, mean val. loss:  2.08986139e+00\n",
      "Epoch: 10022 mean train loss:  2.10329308e-05, mean val. loss:  2.09002614e+00\n",
      "Epoch: 10023 mean train loss:  2.06679979e-05, mean val. loss:  2.09020543e+00\n",
      "Epoch: 10024 mean train loss:  2.06377590e-05, mean val. loss:  2.09036183e+00\n",
      "Epoch: 10025 mean train loss:  2.09214340e-05, mean val. loss:  2.09047484e+00\n",
      "Epoch: 10026 mean train loss:  2.07476842e-05, mean val. loss:  2.09056163e+00\n",
      "Epoch: 10027 mean train loss:  2.06747791e-05, mean val. loss:  2.09061575e+00\n",
      "Epoch: 10028 mean train loss:  2.08704441e-05, mean val. loss:  2.09066558e+00\n",
      "Epoch: 10029 mean train loss:  2.01991643e-05, mean val. loss:  2.09071851e+00\n",
      "Epoch: 10030 mean train loss:  2.08992278e-05, mean val. loss:  2.09077692e+00\n",
      "Epoch: 10031 mean train loss:  2.10320286e-05, mean val. loss:  2.09080124e+00\n",
      "Epoch: 10032 mean train loss:  2.02555966e-05, mean val. loss:  2.09086394e+00\n",
      "Epoch: 10033 mean train loss:  2.06272816e-05, mean val. loss:  2.09092212e+00\n",
      "Epoch: 10034 mean train loss:  2.07069388e-05, mean val. loss:  2.09098315e+00\n",
      "Epoch: 10035 mean train loss:  2.08332203e-05, mean val. loss:  2.09102035e+00\n",
      "Epoch: 10036 mean train loss:  2.07302219e-05, mean val. loss:  2.09106445e+00\n",
      "Epoch: 10037 mean train loss:  2.05578399e-05, mean val. loss:  2.09111881e+00\n",
      "Epoch: 10038 mean train loss:  2.07016419e-05, mean val. loss:  2.09117985e+00\n",
      "Epoch: 10039 mean train loss:  2.10684084e-05, mean val. loss:  2.09123993e+00\n",
      "Epoch: 10040 mean train loss:  2.07958219e-05, mean val. loss:  2.09130120e+00\n",
      "Epoch: 10041 mean train loss:  2.06530967e-05, mean val. loss:  2.09136510e+00\n",
      "Epoch: 10042 mean train loss:  2.06551340e-05, mean val. loss:  2.09142375e+00\n",
      "Epoch: 10043 mean train loss:  2.04558310e-05, mean val. loss:  2.09150195e+00\n",
      "Epoch: 10044 mean train loss:  2.04149110e-05, mean val. loss:  2.09158897e+00\n",
      "Epoch: 10045 mean train loss:  2.07518751e-05, mean val. loss:  2.09172201e+00\n",
      "Epoch: 10046 mean train loss:  2.08213169e-05, mean val. loss:  2.09185791e+00\n",
      "Epoch: 10047 mean train loss:  2.01901130e-05, mean val. loss:  2.09199381e+00\n",
      "Epoch: 10048 mean train loss:  2.05009128e-05, mean val. loss:  2.09212661e+00\n",
      "Epoch: 10049 mean train loss:  2.07484700e-05, mean val. loss:  2.09230280e+00\n",
      "Epoch: 10050 mean train loss:  2.07018456e-05, mean val. loss:  2.09248710e+00\n",
      "Epoch: 10051 mean train loss:  2.05795805e-05, mean val. loss:  2.09266734e+00\n",
      "Epoch: 10052 mean train loss:  2.04526295e-05, mean val. loss:  2.09285474e+00\n",
      "Epoch: 10053 mean train loss:  2.06429686e-05, mean val. loss:  2.09301496e+00\n",
      "Epoch: 10054 mean train loss:  2.10377621e-05, mean val. loss:  2.09315801e+00\n",
      "Epoch: 10055 mean train loss:  2.07934063e-05, mean val. loss:  2.09329414e+00\n",
      "Epoch: 10056 mean train loss:  2.03749514e-05, mean val. loss:  2.09342647e+00\n",
      "Epoch: 10057 mean train loss:  2.07005360e-05, mean val. loss:  2.09353733e+00\n",
      "Epoch: 10058 mean train loss:  2.04965472e-05, mean val. loss:  2.09364104e+00\n",
      "Epoch: 10059 mean train loss:  2.06262339e-05, mean val. loss:  2.09376311e+00\n",
      "Epoch: 10060 mean train loss:  2.06225377e-05, mean val. loss:  2.09391117e+00\n",
      "Epoch: 10061 mean train loss:  2.05858669e-05, mean val. loss:  2.09404397e+00\n",
      "Epoch: 10062 mean train loss:  2.01756775e-05, mean val. loss:  2.09418893e+00\n",
      "Epoch: 10063 mean train loss:  2.06823170e-05, mean val. loss:  2.09432507e+00\n",
      "Epoch: 10064 mean train loss:  2.07503908e-05, mean val. loss:  2.09447551e+00\n",
      "Epoch: 10065 mean train loss:  2.07537378e-05, mean val. loss:  2.09465289e+00\n",
      "Epoch: 10066 mean train loss:  2.10239086e-05, mean val. loss:  2.09482551e+00\n",
      "Epoch: 10067 mean train loss:  2.07861303e-05, mean val. loss:  2.09497571e+00\n",
      "Epoch: 10068 mean train loss:  2.08263809e-05, mean val. loss:  2.09511995e+00\n",
      "Epoch: 10069 mean train loss:  2.09766731e-05, mean val. loss:  2.09523726e+00\n",
      "Epoch: 10070 mean train loss:  2.05113902e-05, mean val. loss:  2.09533286e+00\n",
      "Epoch: 10071 mean train loss:  2.05349061e-05, mean val. loss:  2.09542608e+00\n",
      "Epoch: 10072 mean train loss:  2.06760014e-05, mean val. loss:  2.09548211e+00\n",
      "Epoch: 10073 mean train loss:  2.07615376e-05, mean val. loss:  2.09551072e+00\n",
      "Epoch: 10074 mean train loss:  2.08277779e-05, mean val. loss:  2.09553766e+00\n",
      "Epoch: 10075 mean train loss:  2.06461991e-05, mean val. loss:  2.09557629e+00\n",
      "Epoch: 10076 mean train loss:  2.06070254e-05, mean val. loss:  2.09562612e+00\n",
      "Epoch: 10077 mean train loss:  2.08739948e-05, mean val. loss:  2.09566355e+00\n",
      "Epoch: 10078 mean train loss:  2.06560071e-05, mean val. loss:  2.09570456e+00\n",
      "Epoch: 10079 mean train loss:  2.06518453e-05, mean val. loss:  2.09575963e+00\n",
      "Epoch: 10080 mean train loss:  2.05816177e-05, mean val. loss:  2.09580946e+00\n",
      "Epoch: 10081 mean train loss:  2.04258249e-05, mean val. loss:  2.09580588e+00\n",
      "Epoch: 10082 mean train loss:  2.08777783e-05, mean val. loss:  2.09580517e+00\n",
      "Epoch: 10083 mean train loss:  2.07586854e-05, mean val. loss:  2.09579778e+00\n",
      "Epoch: 10084 mean train loss:  2.07799894e-05, mean val. loss:  2.09578419e+00\n",
      "Epoch: 10085 mean train loss:  2.08638085e-05, mean val. loss:  2.09576249e+00\n",
      "Epoch: 10086 mean train loss:  2.07039411e-05, mean val. loss:  2.09577417e+00\n",
      "Epoch: 10087 mean train loss:  2.09336577e-05, mean val. loss:  2.09576225e+00\n",
      "Epoch: 10088 mean train loss:  2.09794962e-05, mean val. loss:  2.09573293e+00\n",
      "Epoch: 10089 mean train loss:  2.08843267e-05, mean val. loss:  2.09569526e+00\n",
      "Epoch: 10090 mean train loss:  2.06587138e-05, mean val. loss:  2.09566617e+00\n",
      "Epoch: 10091 mean train loss:  2.05668039e-05, mean val. loss:  2.09561920e+00\n",
      "Epoch: 10092 mean train loss:  2.05452961e-05, mean val. loss:  2.09562778e+00\n",
      "Epoch: 10093 mean train loss:  2.07827252e-05, mean val. loss:  2.09564829e+00\n",
      "Epoch: 10094 mean train loss:  2.06868281e-05, mean val. loss:  2.09567738e+00\n",
      "Epoch: 10095 mean train loss:  2.08814163e-05, mean val. loss:  2.09570575e+00\n",
      "Epoch: 10096 mean train loss:  2.04640965e-05, mean val. loss:  2.09579301e+00\n",
      "Epoch: 10097 mean train loss:  2.06175901e-05, mean val. loss:  2.09590459e+00\n",
      "Epoch: 10098 mean train loss:  2.06092081e-05, mean val. loss:  2.09603429e+00\n",
      "Epoch: 10099 mean train loss:  2.06730911e-05, mean val. loss:  2.09615254e+00\n",
      "Epoch: 10100 mean train loss:  2.02916272e-05, mean val. loss:  2.09631395e+00\n",
      "Epoch: 10101 mean train loss:  2.09502177e-05, mean val. loss:  2.09650207e+00\n",
      "Epoch: 10102 mean train loss:  2.06630211e-05, mean val. loss:  2.09669471e+00\n",
      "Epoch: 10103 mean train loss:  2.06972472e-05, mean val. loss:  2.09692073e+00\n",
      "Epoch: 10104 mean train loss:  2.08509155e-05, mean val. loss:  2.09713340e+00\n",
      "Epoch: 10105 mean train loss:  2.06536497e-05, mean val. loss:  2.09736156e+00\n",
      "Epoch: 10106 mean train loss:  2.06680270e-05, mean val. loss:  2.09754324e+00\n",
      "Epoch: 10107 mean train loss:  2.05357210e-05, mean val. loss:  2.09771156e+00\n",
      "Epoch: 10108 mean train loss:  2.09448335e-05, mean val. loss:  2.09786034e+00\n",
      "Epoch: 10109 mean train loss:  2.09614227e-05, mean val. loss:  2.09796405e+00\n",
      "Epoch: 10110 mean train loss:  2.06301920e-05, mean val. loss:  2.09806347e+00\n",
      "Epoch: 10111 mean train loss:  2.08863057e-05, mean val. loss:  2.09815693e+00\n",
      "Epoch: 10112 mean train loss:  2.05392425e-05, mean val. loss:  2.09823775e+00\n",
      "Epoch: 10113 mean train loss:  2.06228870e-05, mean val. loss:  2.09832644e+00\n",
      "Epoch: 10114 mean train loss:  2.04244279e-05, mean val. loss:  2.09839201e+00\n",
      "Epoch: 10115 mean train loss:  2.07619159e-05, mean val. loss:  2.09845376e+00\n",
      "Epoch: 10116 mean train loss:  2.05464021e-05, mean val. loss:  2.09853816e+00\n",
      "Epoch: 10117 mean train loss:  2.05881952e-05, mean val. loss:  2.09862208e+00\n",
      "Epoch: 10118 mean train loss:  2.05599645e-05, mean val. loss:  2.09871554e+00\n",
      "Epoch: 10119 mean train loss:  2.02957308e-05, mean val. loss:  2.09877658e+00\n",
      "Epoch: 10120 mean train loss:  2.03250675e-05, mean val. loss:  2.09889197e+00\n",
      "Epoch: 10121 mean train loss:  2.06723344e-05, mean val. loss:  2.09901619e+00\n",
      "Epoch: 10122 mean train loss:  2.06706172e-05, mean val. loss:  2.09914470e+00\n",
      "Epoch: 10123 mean train loss:  2.03881063e-05, mean val. loss:  2.09931922e+00\n",
      "Epoch: 10124 mean train loss:  2.04069365e-05, mean val. loss:  2.09951782e+00\n",
      "Epoch: 10125 mean train loss:  2.07322300e-05, mean val. loss:  2.09973907e+00\n",
      "Epoch: 10126 mean train loss:  2.07087141e-05, mean val. loss:  2.09995675e+00\n",
      "Epoch: 10127 mean train loss:  2.06763798e-05, mean val. loss:  2.10016108e+00\n",
      "Epoch: 10128 mean train loss:  2.09090649e-05, mean val. loss:  2.10034275e+00\n",
      "Epoch: 10129 mean train loss:  2.04734097e-05, mean val. loss:  2.10051560e+00\n",
      "Epoch: 10130 mean train loss:  2.06516706e-05, mean val. loss:  2.10069156e+00\n",
      "Epoch: 10131 mean train loss:  2.04430253e-05, mean val. loss:  2.10081935e+00\n",
      "Epoch: 10132 mean train loss:  2.08848214e-05, mean val. loss:  2.10095572e+00\n",
      "Epoch: 10133 mean train loss:  2.07520206e-05, mean val. loss:  2.10107684e+00\n",
      "Epoch: 10134 mean train loss:  2.04037060e-05, mean val. loss:  2.10121965e+00\n",
      "Epoch: 10135 mean train loss:  2.07228004e-05, mean val. loss:  2.10132861e+00\n",
      "Epoch: 10136 mean train loss:  2.04067328e-05, mean val. loss:  2.10141921e+00\n",
      "Epoch: 10137 mean train loss:  2.06090044e-05, mean val. loss:  2.10148621e+00\n",
      "Epoch: 10138 mean train loss:  2.05844408e-05, mean val. loss:  2.10152555e+00\n",
      "Epoch: 10139 mean train loss:  2.05549004e-05, mean val. loss:  2.10153294e+00\n",
      "Epoch: 10140 mean train loss:  2.06496916e-05, mean val. loss:  2.10152054e+00\n",
      "Epoch: 10141 mean train loss:  2.06566183e-05, mean val. loss:  2.10151720e+00\n",
      "Epoch: 10142 mean train loss:  2.05690158e-05, mean val. loss:  2.10154724e+00\n",
      "Epoch: 10143 mean train loss:  2.05298711e-05, mean val. loss:  2.10157204e+00\n",
      "Epoch: 10144 mean train loss:  2.06482946e-05, mean val. loss:  2.10162663e+00\n",
      "Epoch: 10145 mean train loss:  2.04351672e-05, mean val. loss:  2.10168958e+00\n",
      "Epoch: 10146 mean train loss:  2.04175594e-05, mean val. loss:  2.10180068e+00\n",
      "Epoch: 10147 mean train loss:  2.08588317e-05, mean val. loss:  2.10184932e+00\n",
      "Epoch: 10148 mean train loss:  2.06464902e-05, mean val. loss:  2.10193539e+00\n",
      "Epoch: 10149 mean train loss:  2.06827535e-05, mean val. loss:  2.10203099e+00\n",
      "Epoch: 10150 mean train loss:  2.04880489e-05, mean val. loss:  2.10213089e+00\n",
      "Epoch: 10151 mean train loss:  2.08744896e-05, mean val. loss:  2.10219002e+00\n",
      "Epoch: 10152 mean train loss:  2.04920652e-05, mean val. loss:  2.10225606e+00\n",
      "Epoch: 10153 mean train loss:  2.05305696e-05, mean val. loss:  2.10231161e+00\n",
      "Epoch: 10154 mean train loss:  2.06924742e-05, mean val. loss:  2.10237765e+00\n",
      "Epoch: 10155 mean train loss:  2.05020769e-05, mean val. loss:  2.10247302e+00\n",
      "Epoch: 10156 mean train loss:  2.06658151e-05, mean val. loss:  2.10257244e+00\n",
      "Epoch: 10157 mean train loss:  2.05744000e-05, mean val. loss:  2.10268378e+00\n",
      "Epoch: 10158 mean train loss:  2.07206176e-05, mean val. loss:  2.10278511e+00\n",
      "Epoch: 10159 mean train loss:  2.07071716e-05, mean val. loss:  2.10290360e+00\n",
      "Epoch: 10160 mean train loss:  2.04506214e-05, mean val. loss:  2.10303998e+00\n",
      "Epoch: 10161 mean train loss:  2.05359247e-05, mean val. loss:  2.10316730e+00\n",
      "Epoch: 10162 mean train loss:  2.03961681e-05, mean val. loss:  2.10328937e+00\n",
      "Epoch: 10163 mean train loss:  2.06259429e-05, mean val. loss:  2.10341859e+00\n",
      "Epoch: 10164 mean train loss:  2.03973032e-05, mean val. loss:  2.10353494e+00\n",
      "Epoch: 10165 mean train loss:  2.03857489e-05, mean val. loss:  2.10365534e+00\n",
      "Epoch: 10166 mean train loss:  2.06170953e-05, mean val. loss:  2.10376906e+00\n",
      "Epoch: 10167 mean train loss:  2.05508259e-05, mean val. loss:  2.10385513e+00\n",
      "Epoch: 10168 mean train loss:  2.04158132e-05, mean val. loss:  2.10398793e+00\n",
      "Epoch: 10169 mean train loss:  2.08386336e-05, mean val. loss:  2.10413790e+00\n",
      "Epoch: 10170 mean train loss:  2.08173296e-05, mean val. loss:  2.10427523e+00\n",
      "Epoch: 10171 mean train loss:  2.02956726e-05, mean val. loss:  2.10445881e+00\n",
      "Epoch: 10172 mean train loss:  2.04025127e-05, mean val. loss:  2.10466623e+00\n",
      "Epoch: 10173 mean train loss:  2.02424708e-05, mean val. loss:  2.10491085e+00\n",
      "Epoch: 10174 mean train loss:  2.07104022e-05, mean val. loss:  2.10513759e+00\n",
      "Epoch: 10175 mean train loss:  2.02855736e-05, mean val. loss:  2.10539937e+00\n",
      "Epoch: 10176 mean train loss:  2.05059187e-05, mean val. loss:  2.10567355e+00\n",
      "Epoch: 10177 mean train loss:  2.05841206e-05, mean val. loss:  2.10598350e+00\n",
      "Epoch: 10178 mean train loss:  2.06160184e-05, mean val. loss:  2.10631537e+00\n",
      "Epoch: 10179 mean train loss:  2.11072911e-05, mean val. loss:  2.10660791e+00\n",
      "Epoch: 10180 mean train loss:  2.05075485e-05, mean val. loss:  2.10689759e+00\n",
      "Epoch: 10181 mean train loss:  2.06500117e-05, mean val. loss:  2.10716629e+00\n",
      "Epoch: 10182 mean train loss:  2.00355717e-05, mean val. loss:  2.10741425e+00\n",
      "Epoch: 10183 mean train loss:  2.05346441e-05, mean val. loss:  2.10762167e+00\n",
      "Epoch: 10184 mean train loss:  2.06671539e-05, mean val. loss:  2.10775542e+00\n",
      "Epoch: 10185 mean train loss:  2.05155520e-05, mean val. loss:  2.10785317e+00\n",
      "Epoch: 10186 mean train loss:  2.03113595e-05, mean val. loss:  2.10788918e+00\n",
      "Epoch: 10187 mean train loss:  2.06920668e-05, mean val. loss:  2.10784888e+00\n",
      "Epoch: 10188 mean train loss:  2.07831326e-05, mean val. loss:  2.10777688e+00\n",
      "Epoch: 10189 mean train loss:  2.01287912e-05, mean val. loss:  2.10767984e+00\n",
      "Epoch: 10190 mean train loss:  2.06946279e-05, mean val. loss:  2.10755301e+00\n",
      "Epoch: 10191 mean train loss:  2.05851684e-05, mean val. loss:  2.10740924e+00\n",
      "Epoch: 10192 mean train loss:  2.05212855e-05, mean val. loss:  2.10725689e+00\n",
      "Epoch: 10193 mean train loss:  2.06059194e-05, mean val. loss:  2.10707498e+00\n",
      "Epoch: 10194 mean train loss:  2.08798738e-05, mean val. loss:  2.10688972e+00\n",
      "Epoch: 10195 mean train loss:  2.07261182e-05, mean val. loss:  2.10666871e+00\n",
      "Epoch: 10196 mean train loss:  2.04884564e-05, mean val. loss:  2.10644507e+00\n",
      "Epoch: 10197 mean train loss:  2.08515848e-05, mean val. loss:  2.10622311e+00\n",
      "Epoch: 10198 mean train loss:  2.07624398e-05, mean val. loss:  2.10600781e+00\n",
      "Epoch: 10199 mean train loss:  2.03553645e-05, mean val. loss:  2.10583663e+00\n",
      "Epoch: 10200 mean train loss:  2.05286487e-05, mean val. loss:  2.10568047e+00\n",
      "Epoch: 10201 mean train loss:  2.06888944e-05, mean val. loss:  2.10554790e+00\n",
      "Epoch: 10202 mean train loss:  2.08703859e-05, mean val. loss:  2.10544515e+00\n",
      "Epoch: 10203 mean train loss:  2.02848751e-05, mean val. loss:  2.10538363e+00\n",
      "Epoch: 10204 mean train loss:  2.05769029e-05, mean val. loss:  2.10535192e+00\n",
      "Epoch: 10205 mean train loss:  2.06355471e-05, mean val. loss:  2.10536170e+00\n",
      "Epoch: 10206 mean train loss:  2.04698881e-05, mean val. loss:  2.10540175e+00\n",
      "Epoch: 10207 mean train loss:  2.04413955e-05, mean val. loss:  2.10549927e+00\n",
      "Epoch: 10208 mean train loss:  2.01732328e-05, mean val. loss:  2.10564256e+00\n",
      "Epoch: 10209 mean train loss:  2.04191019e-05, mean val. loss:  2.10580730e+00\n",
      "Epoch: 10210 mean train loss:  2.06789118e-05, mean val. loss:  2.10600924e+00\n",
      "Epoch: 10211 mean train loss:  2.07474513e-05, mean val. loss:  2.10623503e+00\n",
      "Epoch: 10212 mean train loss:  2.08080164e-05, mean val. loss:  2.10645604e+00\n",
      "Epoch: 10213 mean train loss:  2.04746029e-05, mean val. loss:  2.10669208e+00\n",
      "Epoch: 10214 mean train loss:  2.03910167e-05, mean val. loss:  2.10691285e+00\n",
      "Epoch: 10215 mean train loss:  2.01492512e-05, mean val. loss:  2.10718083e+00\n",
      "Epoch: 10216 mean train loss:  2.05973338e-05, mean val. loss:  2.10747957e+00\n",
      "Epoch: 10217 mean train loss:  2.06333934e-05, mean val. loss:  2.10776925e+00\n",
      "Epoch: 10218 mean train loss:  2.06092081e-05, mean val. loss:  2.10807014e+00\n",
      "Epoch: 10219 mean train loss:  2.08664278e-05, mean val. loss:  2.10832024e+00\n",
      "Epoch: 10220 mean train loss:  2.02329829e-05, mean val. loss:  2.10856819e+00\n",
      "Epoch: 10221 mean train loss:  2.04374664e-05, mean val. loss:  2.10883617e+00\n",
      "Epoch: 10222 mean train loss:  2.05993128e-05, mean val. loss:  2.10906672e+00\n",
      "Epoch: 10223 mean train loss:  2.06718687e-05, mean val. loss:  2.10925937e+00\n",
      "Epoch: 10224 mean train loss:  2.05865363e-05, mean val. loss:  2.10938811e+00\n",
      "Epoch: 10225 mean train loss:  2.05039396e-05, mean val. loss:  2.10952210e+00\n",
      "Epoch: 10226 mean train loss:  2.05590622e-05, mean val. loss:  2.10962391e+00\n",
      "Epoch: 10227 mean train loss:  2.05809192e-05, mean val. loss:  2.10969210e+00\n",
      "Epoch: 10228 mean train loss:  2.04515527e-05, mean val. loss:  2.10975289e+00\n",
      "Epoch: 10229 mean train loss:  2.03142990e-05, mean val. loss:  2.10984182e+00\n",
      "Epoch: 10230 mean train loss:  2.05369433e-05, mean val. loss:  2.10991764e+00\n",
      "Epoch: 10231 mean train loss:  2.01340881e-05, mean val. loss:  2.10998702e+00\n",
      "Epoch: 10232 mean train loss:  2.07656412e-05, mean val. loss:  2.11003661e+00\n",
      "Epoch: 10233 mean train loss:  2.06244586e-05, mean val. loss:  2.11010122e+00\n",
      "Epoch: 10234 mean train loss:  2.02501833e-05, mean val. loss:  2.11021209e+00\n",
      "Epoch: 10235 mean train loss:  2.05841789e-05, mean val. loss:  2.11030149e+00\n",
      "Epoch: 10236 mean train loss:  2.03619129e-05, mean val. loss:  2.11040807e+00\n",
      "Epoch: 10237 mean train loss:  2.03171512e-05, mean val. loss:  2.11051273e+00\n",
      "Epoch: 10238 mean train loss:  2.07719859e-05, mean val. loss:  2.11062932e+00\n",
      "Epoch: 10239 mean train loss:  2.05938122e-05, mean val. loss:  2.11076450e+00\n",
      "Epoch: 10240 mean train loss:  2.03291129e-05, mean val. loss:  2.11093783e+00\n",
      "Epoch: 10241 mean train loss:  2.05925317e-05, mean val. loss:  2.11108708e+00\n",
      "Epoch: 10242 mean train loss:  2.04022508e-05, mean val. loss:  2.11123109e+00\n",
      "Epoch: 10243 mean train loss:  2.06013501e-05, mean val. loss:  2.11135030e+00\n",
      "Epoch: 10244 mean train loss:  2.05220422e-05, mean val. loss:  2.11150169e+00\n",
      "Epoch: 10245 mean train loss:  2.03140953e-05, mean val. loss:  2.11168647e+00\n",
      "Epoch: 10246 mean train loss:  2.03757081e-05, mean val. loss:  2.11192226e+00\n",
      "Epoch: 10247 mean train loss:  2.06890400e-05, mean val. loss:  2.11213422e+00\n",
      "Epoch: 10248 mean train loss:  2.04698590e-05, mean val. loss:  2.11234736e+00\n",
      "Epoch: 10249 mean train loss:  2.04470125e-05, mean val. loss:  2.11257529e+00\n",
      "Epoch: 10250 mean train loss:  2.05047254e-05, mean val. loss:  2.11278486e+00\n",
      "Epoch: 10251 mean train loss:  2.07828416e-05, mean val. loss:  2.11296034e+00\n",
      "Epoch: 10252 mean train loss:  2.05655233e-05, mean val. loss:  2.11316514e+00\n",
      "Epoch: 10253 mean train loss:  2.05932884e-05, mean val. loss:  2.11335874e+00\n",
      "Epoch: 10254 mean train loss:  2.04932003e-05, mean val. loss:  2.11352825e+00\n",
      "Epoch: 10255 mean train loss:  2.01560324e-05, mean val. loss:  2.11371326e+00\n",
      "Epoch: 10256 mean train loss:  2.06025143e-05, mean val. loss:  2.11389136e+00\n",
      "Epoch: 10257 mean train loss:  2.03922391e-05, mean val. loss:  2.11402678e+00\n",
      "Epoch: 10258 mean train loss:  2.05354881e-05, mean val. loss:  2.11409235e+00\n",
      "Epoch: 10259 mean train loss:  2.05083925e-05, mean val. loss:  2.11413074e+00\n",
      "Epoch: 10260 mean train loss:  2.03099044e-05, mean val. loss:  2.11415172e+00\n",
      "Epoch: 10261 mean train loss:  2.05615943e-05, mean val. loss:  2.11418486e+00\n",
      "Epoch: 10262 mean train loss:  2.03520758e-05, mean val. loss:  2.11421371e+00\n",
      "Epoch: 10263 mean train loss:  2.05947435e-05, mean val. loss:  2.11425638e+00\n",
      "Epoch: 10264 mean train loss:  2.07720732e-05, mean val. loss:  2.11429191e+00\n",
      "Epoch: 10265 mean train loss:  2.05490796e-05, mean val. loss:  2.11429644e+00\n",
      "Epoch: 10266 mean train loss:  2.02877854e-05, mean val. loss:  2.11436439e+00\n",
      "Epoch: 10267 mean train loss:  2.05227407e-05, mean val. loss:  2.11443210e+00\n",
      "Epoch: 10268 mean train loss:  2.04144162e-05, mean val. loss:  2.11450124e+00\n",
      "Epoch: 10269 mean train loss:  2.04730313e-05, mean val. loss:  2.11454034e+00\n",
      "Epoch: 10270 mean train loss:  2.04464886e-05, mean val. loss:  2.11461878e+00\n",
      "Epoch: 10271 mean train loss:  2.06962868e-05, mean val. loss:  2.11468768e+00\n",
      "Epoch: 10272 mean train loss:  2.05378747e-05, mean val. loss:  2.11470056e+00\n",
      "Epoch: 10273 mean train loss:  2.03962845e-05, mean val. loss:  2.11472893e+00\n",
      "Epoch: 10274 mean train loss:  2.03226227e-05, mean val. loss:  2.11478734e+00\n",
      "Epoch: 10275 mean train loss:  2.02647061e-05, mean val. loss:  2.11488366e+00\n",
      "Epoch: 10276 mean train loss:  2.07120611e-05, mean val. loss:  2.11498737e+00\n",
      "Epoch: 10277 mean train loss:  2.07445992e-05, mean val. loss:  2.11511040e+00\n",
      "Epoch: 10278 mean train loss:  2.05514079e-05, mean val. loss:  2.11524606e+00\n",
      "Epoch: 10279 mean train loss:  2.02138617e-05, mean val. loss:  2.11543012e+00\n",
      "Epoch: 10280 mean train loss:  2.01493967e-05, mean val. loss:  2.11563778e+00\n",
      "Epoch: 10281 mean train loss:  2.04569369e-05, mean val. loss:  2.11586499e+00\n",
      "Epoch: 10282 mean train loss:  2.02491938e-05, mean val. loss:  2.11610150e+00\n",
      "Epoch: 10283 mean train loss:  2.03824893e-05, mean val. loss:  2.11631656e+00\n",
      "Epoch: 10284 mean train loss:  2.05257093e-05, mean val. loss:  2.11654425e+00\n",
      "Epoch: 10285 mean train loss:  2.03300442e-05, mean val. loss:  2.11676455e+00\n",
      "Epoch: 10286 mean train loss:  2.03446252e-05, mean val. loss:  2.11701536e+00\n",
      "Epoch: 10287 mean train loss:  2.05314136e-05, mean val. loss:  2.11728096e+00\n",
      "Epoch: 10288 mean train loss:  2.05684337e-05, mean val. loss:  2.11752295e+00\n",
      "Epoch: 10289 mean train loss:  2.02907249e-05, mean val. loss:  2.11776900e+00\n",
      "Epoch: 10290 mean train loss:  2.07608391e-05, mean val. loss:  2.11801457e+00\n",
      "Epoch: 10291 mean train loss:  2.02707888e-05, mean val. loss:  2.11825657e+00\n",
      "Epoch: 10292 mean train loss:  2.03520176e-05, mean val. loss:  2.11848831e+00\n",
      "Epoch: 10293 mean train loss:  2.06915429e-05, mean val. loss:  2.11866736e+00\n",
      "Epoch: 10294 mean train loss:  2.01989023e-05, mean val. loss:  2.11884713e+00\n",
      "Epoch: 10295 mean train loss:  2.05699762e-05, mean val. loss:  2.11901689e+00\n",
      "Epoch: 10296 mean train loss:  2.05557153e-05, mean val. loss:  2.11913514e+00\n",
      "Epoch: 10297 mean train loss:  2.00748618e-05, mean val. loss:  2.11923194e+00\n",
      "Epoch: 10298 mean train loss:  2.05910183e-05, mean val. loss:  2.11935973e+00\n",
      "Epoch: 10299 mean train loss:  2.07786215e-05, mean val. loss:  2.11941123e+00\n",
      "Epoch: 10300 mean train loss:  2.06623517e-05, mean val. loss:  2.11940432e+00\n",
      "Epoch: 10301 mean train loss:  2.03143281e-05, mean val. loss:  2.11941028e+00\n",
      "Epoch: 10302 mean train loss:  2.05472461e-05, mean val. loss:  2.11939049e+00\n",
      "Epoch: 10303 mean train loss:  2.05312390e-05, mean val. loss:  2.11935735e+00\n",
      "Epoch: 10304 mean train loss:  2.04133976e-05, mean val. loss:  2.11931992e+00\n",
      "Epoch: 10305 mean train loss:  2.10255384e-05, mean val. loss:  2.11926436e+00\n",
      "Epoch: 10306 mean train loss:  2.04435491e-05, mean val. loss:  2.11917043e+00\n",
      "Epoch: 10307 mean train loss:  2.04747776e-05, mean val. loss:  2.11907172e+00\n",
      "Epoch: 10308 mean train loss:  2.01844086e-05, mean val. loss:  2.11894345e+00\n",
      "Epoch: 10309 mean train loss:  2.06680270e-05, mean val. loss:  2.11876440e+00\n",
      "Epoch: 10310 mean train loss:  2.03248637e-05, mean val. loss:  2.11855793e+00\n",
      "Epoch: 10311 mean train loss:  2.04659300e-05, mean val. loss:  2.11836815e+00\n",
      "Epoch: 10312 mean train loss:  2.03294330e-05, mean val. loss:  2.11816406e+00\n",
      "Epoch: 10313 mean train loss:  2.05152028e-05, mean val. loss:  2.11798763e+00\n",
      "Epoch: 10314 mean train loss:  2.04074895e-05, mean val. loss:  2.11786771e+00\n",
      "Epoch: 10315 mean train loss:  2.03187810e-05, mean val. loss:  2.11777878e+00\n",
      "Epoch: 10316 mean train loss:  2.04878743e-05, mean val. loss:  2.11767960e+00\n",
      "Epoch: 10317 mean train loss:  2.02962256e-05, mean val. loss:  2.11763072e+00\n",
      "Epoch: 10318 mean train loss:  2.03723903e-05, mean val. loss:  2.11764932e+00\n",
      "Epoch: 10319 mean train loss:  2.04526004e-05, mean val. loss:  2.11769891e+00\n",
      "Epoch: 10320 mean train loss:  2.03482050e-05, mean val. loss:  2.11781359e+00\n",
      "Epoch: 10321 mean train loss:  2.04331009e-05, mean val. loss:  2.11796427e+00\n",
      "Epoch: 10322 mean train loss:  2.04236421e-05, mean val. loss:  2.11814737e+00\n",
      "Epoch: 10323 mean train loss:  2.05510587e-05, mean val. loss:  2.11835718e+00\n",
      "Epoch: 10324 mean train loss:  2.02432275e-05, mean val. loss:  2.11862206e+00\n",
      "Epoch: 10325 mean train loss:  2.07559788e-05, mean val. loss:  2.11893177e+00\n",
      "Epoch: 10326 mean train loss:  2.04877288e-05, mean val. loss:  2.11925149e+00\n",
      "Epoch: 10327 mean train loss:  2.03453237e-05, mean val. loss:  2.11959887e+00\n",
      "Epoch: 10328 mean train loss:  2.03587406e-05, mean val. loss:  2.11995149e+00\n",
      "Epoch: 10329 mean train loss:  2.04924727e-05, mean val. loss:  2.12030125e+00\n",
      "Epoch: 10330 mean train loss:  2.03951786e-05, mean val. loss:  2.12065101e+00\n",
      "Epoch: 10331 mean train loss:  2.04769021e-05, mean val. loss:  2.12100577e+00\n",
      "Epoch: 10332 mean train loss:  2.08053680e-05, mean val. loss:  2.12131047e+00\n",
      "Epoch: 10333 mean train loss:  2.05757096e-05, mean val. loss:  2.12160063e+00\n",
      "Epoch: 10334 mean train loss:  2.03341478e-05, mean val. loss:  2.12184811e+00\n",
      "Epoch: 10335 mean train loss:  2.00949435e-05, mean val. loss:  2.12207818e+00\n",
      "Epoch: 10336 mean train loss:  2.07807461e-05, mean val. loss:  2.12225533e+00\n",
      "Epoch: 10337 mean train loss:  2.04245152e-05, mean val. loss:  2.12236953e+00\n",
      "Epoch: 10338 mean train loss:  2.03993695e-05, mean val. loss:  2.12248445e+00\n",
      "Epoch: 10339 mean train loss:  2.06400582e-05, mean val. loss:  2.12256074e+00\n",
      "Epoch: 10340 mean train loss:  2.05209362e-05, mean val. loss:  2.12258577e+00\n",
      "Epoch: 10341 mean train loss:  2.03257077e-05, mean val. loss:  2.12253952e+00\n",
      "Epoch: 10342 mean train loss:  2.02309166e-05, mean val. loss:  2.12249875e+00\n",
      "Epoch: 10343 mean train loss:  2.01387738e-05, mean val. loss:  2.12244797e+00\n",
      "Epoch: 10344 mean train loss:  2.08674755e-05, mean val. loss:  2.12233019e+00\n",
      "Epoch: 10345 mean train loss:  2.06720433e-05, mean val. loss:  2.12217879e+00\n",
      "Epoch: 10346 mean train loss:  2.04732351e-05, mean val. loss:  2.12202907e+00\n",
      "Epoch: 10347 mean train loss:  2.00706418e-05, mean val. loss:  2.12188077e+00\n",
      "Epoch: 10348 mean train loss:  2.00739887e-05, mean val. loss:  2.12175560e+00\n",
      "Epoch: 10349 mean train loss:  2.06468976e-05, mean val. loss:  2.12164927e+00\n",
      "Epoch: 10350 mean train loss:  2.03846139e-05, mean val. loss:  2.12159276e+00\n",
      "Epoch: 10351 mean train loss:  2.00700597e-05, mean val. loss:  2.12158394e+00\n",
      "Epoch: 10352 mean train loss:  2.03548989e-05, mean val. loss:  2.12160182e+00\n",
      "Epoch: 10353 mean train loss:  2.05534452e-05, mean val. loss:  2.12163234e+00\n",
      "Epoch: 10354 mean train loss:  2.03803938e-05, mean val. loss:  2.12169838e+00\n",
      "Epoch: 10355 mean train loss:  2.05191318e-05, mean val. loss:  2.12178516e+00\n",
      "Epoch: 10356 mean train loss:  2.03667441e-05, mean val. loss:  2.12192488e+00\n",
      "Epoch: 10357 mean train loss:  2.02673837e-05, mean val. loss:  2.12209606e+00\n",
      "Epoch: 10358 mean train loss:  2.03690433e-05, mean val. loss:  2.12229371e+00\n",
      "Epoch: 10359 mean train loss:  2.03794625e-05, mean val. loss:  2.12254119e+00\n",
      "Epoch: 10360 mean train loss:  2.05290562e-05, mean val. loss:  2.12280178e+00\n",
      "Epoch: 10361 mean train loss:  2.04013195e-05, mean val. loss:  2.12306023e+00\n",
      "Epoch: 10362 mean train loss:  2.03579257e-05, mean val. loss:  2.12329030e+00\n",
      "Epoch: 10363 mean train loss:  2.05733813e-05, mean val. loss:  2.12348104e+00\n",
      "Epoch: 10364 mean train loss:  2.05446850e-05, mean val. loss:  2.12366986e+00\n",
      "Epoch: 10365 mean train loss:  2.03141535e-05, mean val. loss:  2.12386608e+00\n",
      "Epoch: 10366 mean train loss:  2.03282689e-05, mean val. loss:  2.12407780e+00\n",
      "Epoch: 10367 mean train loss:  1.99833594e-05, mean val. loss:  2.12431693e+00\n",
      "Epoch: 10368 mean train loss:  2.01661896e-05, mean val. loss:  2.12453771e+00\n",
      "Epoch: 10369 mean train loss:  2.01918010e-05, mean val. loss:  2.12475777e+00\n",
      "Epoch: 10370 mean train loss:  2.02761148e-05, mean val. loss:  2.12496662e+00\n",
      "Epoch: 10371 mean train loss:  2.00179056e-05, mean val. loss:  2.12518382e+00\n",
      "Epoch: 10372 mean train loss:  2.02120573e-05, mean val. loss:  2.12536740e+00\n",
      "Epoch: 10373 mean train loss:  2.00365030e-05, mean val. loss:  2.12556839e+00\n",
      "Epoch: 10374 mean train loss:  2.05698889e-05, mean val. loss:  2.12573147e+00\n",
      "Epoch: 10375 mean train loss:  2.00551876e-05, mean val. loss:  2.12592530e+00\n",
      "Epoch: 10376 mean train loss:  2.03702657e-05, mean val. loss:  2.12613249e+00\n",
      "Epoch: 10377 mean train loss:  2.02215160e-05, mean val. loss:  2.12631774e+00\n",
      "Epoch: 10378 mean train loss:  2.00770155e-05, mean val. loss:  2.12649965e+00\n",
      "Epoch: 10379 mean train loss:  2.02624069e-05, mean val. loss:  2.12669277e+00\n",
      "Epoch: 10380 mean train loss:  2.05001270e-05, mean val. loss:  2.12688875e+00\n",
      "Epoch: 10381 mean train loss:  2.01136281e-05, mean val. loss:  2.12708306e+00\n",
      "Epoch: 10382 mean train loss:  2.04022508e-05, mean val. loss:  2.12727809e+00\n",
      "Epoch: 10383 mean train loss:  2.05872930e-05, mean val. loss:  2.12747598e+00\n",
      "Epoch: 10384 mean train loss:  2.05076358e-05, mean val. loss:  2.12766552e+00\n",
      "Epoch: 10385 mean train loss:  2.01255025e-05, mean val. loss:  2.12788177e+00\n",
      "Epoch: 10386 mean train loss:  2.03623786e-05, mean val. loss:  2.12806726e+00\n",
      "Epoch: 10387 mean train loss:  2.04086246e-05, mean val. loss:  2.12820029e+00\n",
      "Epoch: 10388 mean train loss:  2.04105745e-05, mean val. loss:  2.12828374e+00\n",
      "Epoch: 10389 mean train loss:  2.04420649e-05, mean val. loss:  2.12833095e+00\n",
      "Epoch: 10390 mean train loss:  2.03064119e-05, mean val. loss:  2.12839890e+00\n",
      "Epoch: 10391 mean train loss:  2.02444789e-05, mean val. loss:  2.12842631e+00\n",
      "Epoch: 10392 mean train loss:  2.03506788e-05, mean val. loss:  2.12844062e+00\n",
      "Epoch: 10393 mean train loss:  2.00918876e-05, mean val. loss:  2.12844062e+00\n",
      "Epoch: 10394 mean train loss:  2.00973882e-05, mean val. loss:  2.12847877e+00\n",
      "Epoch: 10395 mean train loss:  2.05032993e-05, mean val. loss:  2.12851262e+00\n",
      "Epoch: 10396 mean train loss:  2.05504184e-05, mean val. loss:  2.12855387e+00\n",
      "Epoch: 10397 mean train loss:  2.04370008e-05, mean val. loss:  2.12860346e+00\n",
      "Epoch: 10398 mean train loss:  2.04292010e-05, mean val. loss:  2.12864709e+00\n",
      "Epoch: 10399 mean train loss:  2.02928786e-05, mean val. loss:  2.12869883e+00\n",
      "Epoch: 10400 mean train loss:  2.05064716e-05, mean val. loss:  2.12879539e+00\n",
      "Epoch: 10401 mean train loss:  2.01949733e-05, mean val. loss:  2.12891507e+00\n",
      "Epoch: 10402 mean train loss:  2.07145058e-05, mean val. loss:  2.12899232e+00\n",
      "Epoch: 10403 mean train loss:  2.06762925e-05, mean val. loss:  2.12906671e+00\n",
      "Epoch: 10404 mean train loss:  2.00989889e-05, mean val. loss:  2.12913203e+00\n",
      "Epoch: 10405 mean train loss:  2.04509124e-05, mean val. loss:  2.12918591e+00\n",
      "Epoch: 10406 mean train loss:  2.01237854e-05, mean val. loss:  2.12924671e+00\n",
      "Epoch: 10407 mean train loss:  2.03403470e-05, mean val. loss:  2.12929964e+00\n",
      "Epoch: 10408 mean train loss:  2.05585675e-05, mean val. loss:  2.12932181e+00\n",
      "Epoch: 10409 mean train loss:  2.04838871e-05, mean val. loss:  2.12931561e+00\n",
      "Epoch: 10410 mean train loss:  2.03081290e-05, mean val. loss:  2.12930608e+00\n",
      "Epoch: 10411 mean train loss:  2.03649688e-05, mean val. loss:  2.12929463e+00\n",
      "Epoch: 10412 mean train loss:  2.03042582e-05, mean val. loss:  2.12929487e+00\n",
      "Epoch: 10413 mean train loss:  2.00520735e-05, mean val. loss:  2.12928534e+00\n",
      "Epoch: 10414 mean train loss:  2.02184310e-05, mean val. loss:  2.12926531e+00\n",
      "Epoch: 10415 mean train loss:  2.05320248e-05, mean val. loss:  2.12921882e+00\n",
      "Epoch: 10416 mean train loss:  2.03005620e-05, mean val. loss:  2.12918139e+00\n",
      "Epoch: 10417 mean train loss:  2.00833019e-05, mean val. loss:  2.12919641e+00\n",
      "Epoch: 10418 mean train loss:  2.05599936e-05, mean val. loss:  2.12919688e+00\n",
      "Epoch: 10419 mean train loss:  2.00723880e-05, mean val. loss:  2.12924147e+00\n",
      "Epoch: 10420 mean train loss:  1.99624919e-05, mean val. loss:  2.12932301e+00\n",
      "Epoch: 10421 mean train loss:  2.04464013e-05, mean val. loss:  2.12945104e+00\n",
      "Epoch: 10422 mean train loss:  2.02990777e-05, mean val. loss:  2.12956810e+00\n",
      "Epoch: 10423 mean train loss:  2.03138916e-05, mean val. loss:  2.12968183e+00\n",
      "Epoch: 10424 mean train loss:  2.01961957e-05, mean val. loss:  2.12983298e+00\n",
      "Epoch: 10425 mean train loss:  1.99396745e-05, mean val. loss:  2.12996912e+00\n",
      "Epoch: 10426 mean train loss:  2.02548399e-05, mean val. loss:  2.13010693e+00\n",
      "Epoch: 10427 mean train loss:  2.03656964e-05, mean val. loss:  2.13024497e+00\n",
      "Epoch: 10428 mean train loss:  2.02830124e-05, mean val. loss:  2.13042068e+00\n",
      "Epoch: 10429 mean train loss:  2.02028023e-05, mean val. loss:  2.13059425e+00\n",
      "Epoch: 10430 mean train loss:  2.03802774e-05, mean val. loss:  2.13078094e+00\n",
      "Epoch: 10431 mean train loss:  2.07359262e-05, mean val. loss:  2.13099384e+00\n",
      "Epoch: 10432 mean train loss:  2.04301614e-05, mean val. loss:  2.13123417e+00\n",
      "Epoch: 10433 mean train loss:  2.03431991e-05, mean val. loss:  2.13143039e+00\n",
      "Epoch: 10434 mean train loss:  2.06073455e-05, mean val. loss:  2.13160229e+00\n",
      "Epoch: 10435 mean train loss:  2.02986994e-05, mean val. loss:  2.13177657e+00\n",
      "Epoch: 10436 mean train loss:  2.01362418e-05, mean val. loss:  2.13194513e+00\n",
      "Epoch: 10437 mean train loss:  2.01330404e-05, mean val. loss:  2.13210177e+00\n",
      "Epoch: 10438 mean train loss:  2.03243690e-05, mean val. loss:  2.13225389e+00\n",
      "Epoch: 10439 mean train loss:  2.01439252e-05, mean val. loss:  2.13238049e+00\n",
      "Epoch: 10440 mean train loss:  2.06185505e-05, mean val. loss:  2.13246703e+00\n",
      "Epoch: 10441 mean train loss:  2.02616211e-05, mean val. loss:  2.13252544e+00\n",
      "Epoch: 10442 mean train loss:  1.99281203e-05, mean val. loss:  2.13262200e+00\n",
      "Epoch: 10443 mean train loss:  2.04996904e-05, mean val. loss:  2.13270760e+00\n",
      "Epoch: 10444 mean train loss:  2.02184601e-05, mean val. loss:  2.13282418e+00\n",
      "Epoch: 10445 mean train loss:  2.00278300e-05, mean val. loss:  2.13293052e+00\n",
      "Epoch: 10446 mean train loss:  2.03892705e-05, mean val. loss:  2.13304663e+00\n",
      "Epoch: 10447 mean train loss:  2.01380171e-05, mean val. loss:  2.13315916e+00\n",
      "Epoch: 10448 mean train loss:  2.03555683e-05, mean val. loss:  2.13327885e+00\n",
      "Epoch: 10449 mean train loss:  2.01215735e-05, mean val. loss:  2.13343573e+00\n",
      "Epoch: 10450 mean train loss:  1.99848728e-05, mean val. loss:  2.13361144e+00\n",
      "Epoch: 10451 mean train loss:  2.01658113e-05, mean val. loss:  2.13380933e+00\n",
      "Epoch: 10452 mean train loss:  2.01428193e-05, mean val. loss:  2.13401246e+00\n",
      "Epoch: 10453 mean train loss:  2.05475371e-05, mean val. loss:  2.13422561e+00\n",
      "Epoch: 10454 mean train loss:  2.01419462e-05, mean val. loss:  2.13447070e+00\n",
      "Epoch: 10455 mean train loss:  2.03252712e-05, mean val. loss:  2.13471723e+00\n",
      "Epoch: 10456 mean train loss:  2.01699440e-05, mean val. loss:  2.13499546e+00\n",
      "Epoch: 10457 mean train loss:  2.05129909e-05, mean val. loss:  2.13528490e+00\n",
      "Epoch: 10458 mean train loss:  2.01854564e-05, mean val. loss:  2.13558984e+00\n",
      "Epoch: 10459 mean train loss:  2.02819065e-05, mean val. loss:  2.13585567e+00\n",
      "Epoch: 10460 mean train loss:  2.01102230e-05, mean val. loss:  2.13608813e+00\n",
      "Epoch: 10461 mean train loss:  2.04969256e-05, mean val. loss:  2.13628316e+00\n",
      "Epoch: 10462 mean train loss:  2.03110685e-05, mean val. loss:  2.13649178e+00\n",
      "Epoch: 10463 mean train loss:  2.02915107e-05, mean val. loss:  2.13663197e+00\n",
      "Epoch: 10464 mean train loss:  2.00550712e-05, mean val. loss:  2.13676333e+00\n",
      "Epoch: 10465 mean train loss:  2.03645614e-05, mean val. loss:  2.13687134e+00\n",
      "Epoch: 10466 mean train loss:  2.01564690e-05, mean val. loss:  2.13696432e+00\n",
      "Epoch: 10467 mean train loss:  2.01415096e-05, mean val. loss:  2.13705087e+00\n",
      "Epoch: 10468 mean train loss:  2.07733829e-05, mean val. loss:  2.13711190e+00\n",
      "Epoch: 10469 mean train loss:  2.04437820e-05, mean val. loss:  2.13715029e+00\n",
      "Epoch: 10470 mean train loss:  1.98669732e-05, mean val. loss:  2.13716817e+00\n",
      "Epoch: 10471 mean train loss:  2.04140961e-05, mean val. loss:  2.13718939e+00\n",
      "Epoch: 10472 mean train loss:  2.01407529e-05, mean val. loss:  2.13724804e+00\n",
      "Epoch: 10473 mean train loss:  2.02730880e-05, mean val. loss:  2.13732958e+00\n",
      "Epoch: 10474 mean train loss:  2.03157251e-05, mean val. loss:  2.13739347e+00\n",
      "Epoch: 10475 mean train loss:  2.04487296e-05, mean val. loss:  2.13745570e+00\n",
      "Epoch: 10476 mean train loss:  2.06461700e-05, mean val. loss:  2.13751221e+00\n",
      "Epoch: 10477 mean train loss:  2.03376694e-05, mean val. loss:  2.13758922e+00\n",
      "Epoch: 10478 mean train loss:  2.01604562e-05, mean val. loss:  2.13771462e+00\n",
      "Epoch: 10479 mean train loss:  2.00219802e-05, mean val. loss:  2.13787532e+00\n",
      "Epoch: 10480 mean train loss:  1.99224742e-05, mean val. loss:  2.13804340e+00\n",
      "Epoch: 10481 mean train loss:  2.03461095e-05, mean val. loss:  2.13821888e+00\n",
      "Epoch: 10482 mean train loss:  2.01649964e-05, mean val. loss:  2.13841534e+00\n",
      "Epoch: 10483 mean train loss:  2.02187803e-05, mean val. loss:  2.13859439e+00\n",
      "Epoch: 10484 mean train loss:  2.03577511e-05, mean val. loss:  2.13875008e+00\n",
      "Epoch: 10485 mean train loss:  2.02316442e-05, mean val. loss:  2.13889527e+00\n",
      "Epoch: 10486 mean train loss:  2.02503870e-05, mean val. loss:  2.13898802e+00\n",
      "Epoch: 10487 mean train loss:  2.03094969e-05, mean val. loss:  2.13907170e+00\n",
      "Epoch: 10488 mean train loss:  2.02563242e-05, mean val. loss:  2.13914537e+00\n",
      "Epoch: 10489 mean train loss:  2.05191027e-05, mean val. loss:  2.13918948e+00\n",
      "Epoch: 10490 mean train loss:  2.02962547e-05, mean val. loss:  2.13921571e+00\n",
      "Epoch: 10491 mean train loss:  2.03843229e-05, mean val. loss:  2.13924909e+00\n",
      "Epoch: 10492 mean train loss:  1.97956106e-05, mean val. loss:  2.13931918e+00\n",
      "Epoch: 10493 mean train loss:  2.04959651e-05, mean val. loss:  2.13934040e+00\n",
      "Epoch: 10494 mean train loss:  2.01028015e-05, mean val. loss:  2.13936472e+00\n",
      "Epoch: 10495 mean train loss:  2.02808587e-05, mean val. loss:  2.13932252e+00\n",
      "Epoch: 10496 mean train loss:  2.00179638e-05, mean val. loss:  2.13929057e+00\n",
      "Epoch: 10497 mean train loss:  2.03350501e-05, mean val. loss:  2.13933063e+00\n",
      "Epoch: 10498 mean train loss:  1.98166526e-05, mean val. loss:  2.13940597e+00\n",
      "Epoch: 10499 mean train loss:  2.01664225e-05, mean val. loss:  2.13951683e+00\n",
      "Epoch: 10500 mean train loss:  2.00460199e-05, mean val. loss:  2.13962984e+00\n",
      "Epoch: 10501 mean train loss:  2.06045806e-05, mean val. loss:  2.13972545e+00\n",
      "Epoch: 10502 mean train loss:  2.01415969e-05, mean val. loss:  2.13980293e+00\n",
      "Epoch: 10503 mean train loss:  2.02787633e-05, mean val. loss:  2.13988328e+00\n",
      "Epoch: 10504 mean train loss:  2.08939600e-05, mean val. loss:  2.13992119e+00\n",
      "Epoch: 10505 mean train loss:  2.00460781e-05, mean val. loss:  2.13996053e+00\n",
      "Epoch: 10506 mean train loss:  2.02037336e-05, mean val. loss:  2.14000726e+00\n",
      "Epoch: 10507 mean train loss:  2.01313815e-05, mean val. loss:  2.14006639e+00\n",
      "Epoch: 10508 mean train loss:  2.03779491e-05, mean val. loss:  2.14009857e+00\n",
      "Epoch: 10509 mean train loss:  2.01529183e-05, mean val. loss:  2.14017129e+00\n",
      "Epoch: 10510 mean train loss:  2.05762044e-05, mean val. loss:  2.14022136e+00\n",
      "Epoch: 10511 mean train loss:  2.03882810e-05, mean val. loss:  2.14029002e+00\n",
      "Epoch: 10512 mean train loss:  2.01728544e-05, mean val. loss:  2.14035916e+00\n",
      "Epoch: 10513 mean train loss:  2.00691866e-05, mean val. loss:  2.14039397e+00\n",
      "Epoch: 10514 mean train loss:  2.03979434e-05, mean val. loss:  2.14042497e+00\n",
      "Epoch: 10515 mean train loss:  2.00768409e-05, mean val. loss:  2.14047313e+00\n",
      "Epoch: 10516 mean train loss:  2.01981456e-05, mean val. loss:  2.14054060e+00\n",
      "Epoch: 10517 mean train loss:  2.00701470e-05, mean val. loss:  2.14060807e+00\n",
      "Epoch: 10518 mean train loss:  2.04183452e-05, mean val. loss:  2.14066267e+00\n",
      "Epoch: 10519 mean train loss:  1.98406051e-05, mean val. loss:  2.14073014e+00\n",
      "Epoch: 10520 mean train loss:  2.01292569e-05, mean val. loss:  2.14079714e+00\n",
      "Epoch: 10521 mean train loss:  1.99975912e-05, mean val. loss:  2.14088631e+00\n",
      "Epoch: 10522 mean train loss:  2.02278316e-05, mean val. loss:  2.14098263e+00\n",
      "Epoch: 10523 mean train loss:  2.00992508e-05, mean val. loss:  2.14113164e+00\n",
      "Epoch: 10524 mean train loss:  2.02068477e-05, mean val. loss:  2.14128113e+00\n",
      "Epoch: 10525 mean train loss:  2.02377269e-05, mean val. loss:  2.14145470e+00\n",
      "Epoch: 10526 mean train loss:  2.03264062e-05, mean val. loss:  2.14162683e+00\n",
      "Epoch: 10527 mean train loss:  2.00413633e-05, mean val. loss:  2.14179683e+00\n",
      "Epoch: 10528 mean train loss:  2.05721008e-05, mean val. loss:  2.14196682e+00\n",
      "Epoch: 10529 mean train loss:  2.02693918e-05, mean val. loss:  2.14213037e+00\n",
      "Epoch: 10530 mean train loss:  2.02338269e-05, mean val. loss:  2.14232707e+00\n",
      "Epoch: 10531 mean train loss:  2.02579540e-05, mean val. loss:  2.14250803e+00\n",
      "Epoch: 10532 mean train loss:  2.01452931e-05, mean val. loss:  2.14266253e+00\n",
      "Epoch: 10533 mean train loss:  2.03025702e-05, mean val. loss:  2.14276004e+00\n",
      "Epoch: 10534 mean train loss:  2.05044635e-05, mean val. loss:  2.14286137e+00\n",
      "Epoch: 10535 mean train loss:  2.01926741e-05, mean val. loss:  2.14296103e+00\n",
      "Epoch: 10536 mean train loss:  2.04531243e-05, mean val. loss:  2.14305234e+00\n",
      "Epoch: 10537 mean train loss:  2.00985523e-05, mean val. loss:  2.14315295e+00\n",
      "Epoch: 10538 mean train loss:  2.00640352e-05, mean val. loss:  2.14325881e+00\n",
      "Epoch: 10539 mean train loss:  2.03484669e-05, mean val. loss:  2.14332628e+00\n",
      "Epoch: 10540 mean train loss:  2.00910436e-05, mean val. loss:  2.14342809e+00\n",
      "Epoch: 10541 mean train loss:  2.04943353e-05, mean val. loss:  2.14348245e+00\n",
      "Epoch: 10542 mean train loss:  2.03085074e-05, mean val. loss:  2.14351320e+00\n",
      "Epoch: 10543 mean train loss:  2.05007091e-05, mean val. loss:  2.14354777e+00\n",
      "Epoch: 10544 mean train loss:  1.99410715e-05, mean val. loss:  2.14360309e+00\n",
      "Epoch: 10545 mean train loss:  2.01635412e-05, mean val. loss:  2.14364123e+00\n",
      "Epoch: 10546 mean train loss:  1.99048663e-05, mean val. loss:  2.14369917e+00\n",
      "Epoch: 10547 mean train loss:  2.02801311e-05, mean val. loss:  2.14372993e+00\n",
      "Epoch: 10548 mean train loss:  2.00004142e-05, mean val. loss:  2.14379120e+00\n",
      "Epoch: 10549 mean train loss:  2.02415395e-05, mean val. loss:  2.14387012e+00\n",
      "Epoch: 10550 mean train loss:  2.01704097e-05, mean val. loss:  2.14393616e+00\n",
      "Epoch: 10551 mean train loss:  2.00441282e-05, mean val. loss:  2.14399242e+00\n",
      "Epoch: 10552 mean train loss:  2.02026858e-05, mean val. loss:  2.14405274e+00\n",
      "Epoch: 10553 mean train loss:  2.01402581e-05, mean val. loss:  2.14413738e+00\n",
      "Epoch: 10554 mean train loss:  2.01441289e-05, mean val. loss:  2.14424300e+00\n",
      "Epoch: 10555 mean train loss:  2.00165377e-05, mean val. loss:  2.14437556e+00\n",
      "Epoch: 10556 mean train loss:  2.03195377e-05, mean val. loss:  2.14452696e+00\n",
      "Epoch: 10557 mean train loss:  2.01544026e-05, mean val. loss:  2.14469409e+00\n",
      "Epoch: 10558 mean train loss:  2.04715761e-05, mean val. loss:  2.14485812e+00\n",
      "Epoch: 10559 mean train loss:  2.00855429e-05, mean val. loss:  2.14504886e+00\n",
      "Epoch: 10560 mean train loss:  2.05413671e-05, mean val. loss:  2.14519477e+00\n",
      "Epoch: 10561 mean train loss:  2.02649389e-05, mean val. loss:  2.14531898e+00\n",
      "Epoch: 10562 mean train loss:  2.01638613e-05, mean val. loss:  2.14549637e+00\n",
      "Epoch: 10563 mean train loss:  2.01421499e-05, mean val. loss:  2.14567041e+00\n",
      "Epoch: 10564 mean train loss:  2.00607174e-05, mean val. loss:  2.14585161e+00\n",
      "Epoch: 10565 mean train loss:  1.99149945e-05, mean val. loss:  2.14605188e+00\n",
      "Epoch: 10566 mean train loss:  1.98972994e-05, mean val. loss:  2.14624405e+00\n",
      "Epoch: 10567 mean train loss:  2.01973598e-05, mean val. loss:  2.14642286e+00\n",
      "Epoch: 10568 mean train loss:  2.02233787e-05, mean val. loss:  2.14658833e+00\n",
      "Epoch: 10569 mean train loss:  1.99640926e-05, mean val. loss:  2.14674234e+00\n",
      "Epoch: 10570 mean train loss:  2.05987890e-05, mean val. loss:  2.14689803e+00\n",
      "Epoch: 10571 mean train loss:  2.01271032e-05, mean val. loss:  2.14705968e+00\n",
      "Epoch: 10572 mean train loss:  1.99947972e-05, mean val. loss:  2.14722848e+00\n",
      "Epoch: 10573 mean train loss:  2.01435178e-05, mean val. loss:  2.14739108e+00\n",
      "Epoch: 10574 mean train loss:  2.01703806e-05, mean val. loss:  2.14756083e+00\n",
      "Epoch: 10575 mean train loss:  1.99291098e-05, mean val. loss:  2.14774680e+00\n",
      "Epoch: 10576 mean train loss:  2.00730283e-05, mean val. loss:  2.14795375e+00\n",
      "Epoch: 10577 mean train loss:  2.01744260e-05, mean val. loss:  2.14816952e+00\n",
      "Epoch: 10578 mean train loss:  2.01769581e-05, mean val. loss:  2.14837646e+00\n",
      "Epoch: 10579 mean train loss:  2.01954390e-05, mean val. loss:  2.14855623e+00\n",
      "Epoch: 10580 mean train loss:  2.03932286e-05, mean val. loss:  2.14873338e+00\n",
      "Epoch: 10581 mean train loss:  2.01869989e-05, mean val. loss:  2.14890075e+00\n",
      "Epoch: 10582 mean train loss:  2.00173235e-05, mean val. loss:  2.14906025e+00\n",
      "Epoch: 10583 mean train loss:  2.01441871e-05, mean val. loss:  2.14920878e+00\n",
      "Epoch: 10584 mean train loss:  2.01762887e-05, mean val. loss:  2.14934278e+00\n",
      "Epoch: 10585 mean train loss:  2.02385709e-05, mean val. loss:  2.14946437e+00\n",
      "Epoch: 10586 mean train loss:  2.02322262e-05, mean val. loss:  2.14954853e+00\n",
      "Epoch: 10587 mean train loss:  2.01930525e-05, mean val. loss:  2.14963579e+00\n",
      "Epoch: 10588 mean train loss:  1.99915958e-05, mean val. loss:  2.14969945e+00\n",
      "Epoch: 10589 mean train loss:  2.00266659e-05, mean val. loss:  2.14974713e+00\n",
      "Epoch: 10590 mean train loss:  2.01450894e-05, mean val. loss:  2.14977884e+00\n",
      "Epoch: 10591 mean train loss:  2.04027456e-05, mean val. loss:  2.14981961e+00\n",
      "Epoch: 10592 mean train loss:  2.04142998e-05, mean val. loss:  2.14985204e+00\n",
      "Epoch: 10593 mean train loss:  2.02770752e-05, mean val. loss:  2.14988899e+00\n",
      "Epoch: 10594 mean train loss:  2.00516952e-05, mean val. loss:  2.14993453e+00\n",
      "Epoch: 10595 mean train loss:  2.03178497e-05, mean val. loss:  2.14996839e+00\n",
      "Epoch: 10596 mean train loss:  2.01883668e-05, mean val. loss:  2.15001249e+00\n",
      "Epoch: 10597 mean train loss:  2.04496901e-05, mean val. loss:  2.15008831e+00\n",
      "Epoch: 10598 mean train loss:  2.00666836e-05, mean val. loss:  2.15016580e+00\n",
      "Epoch: 10599 mean train loss:  2.01884832e-05, mean val. loss:  2.15023637e+00\n",
      "Epoch: 10600 mean train loss:  2.02221563e-05, mean val. loss:  2.15031600e+00\n",
      "Epoch: 10601 mean train loss:  2.01582734e-05, mean val. loss:  2.15041590e+00\n",
      "Epoch: 10602 mean train loss:  2.03747477e-05, mean val. loss:  2.15053797e+00\n",
      "Epoch: 10603 mean train loss:  2.03536474e-05, mean val. loss:  2.15065193e+00\n",
      "Epoch: 10604 mean train loss:  2.01372313e-05, mean val. loss:  2.15080547e+00\n",
      "Epoch: 10605 mean train loss:  2.02813535e-05, mean val. loss:  2.15094995e+00\n",
      "Epoch: 10606 mean train loss:  2.01219227e-05, mean val. loss:  2.15111661e+00\n",
      "Epoch: 10607 mean train loss:  2.03564705e-05, mean val. loss:  2.15126371e+00\n",
      "Epoch: 10608 mean train loss:  2.02443625e-05, mean val. loss:  2.15139008e+00\n",
      "Epoch: 10609 mean train loss:  2.00227369e-05, mean val. loss:  2.15152359e+00\n",
      "Epoch: 10610 mean train loss:  2.03212840e-05, mean val. loss:  2.15163851e+00\n",
      "Epoch: 10611 mean train loss:  2.04718963e-05, mean val. loss:  2.15174365e+00\n",
      "Epoch: 10612 mean train loss:  2.01681105e-05, mean val. loss:  2.15183854e+00\n",
      "Epoch: 10613 mean train loss:  1.99286733e-05, mean val. loss:  2.15194917e+00\n",
      "Epoch: 10614 mean train loss:  2.01193325e-05, mean val. loss:  2.15206623e+00\n",
      "Epoch: 10615 mean train loss:  2.02029478e-05, mean val. loss:  2.15217996e+00\n",
      "Epoch: 10616 mean train loss:  1.99082424e-05, mean val. loss:  2.15228128e+00\n",
      "Epoch: 10617 mean train loss:  2.02460214e-05, mean val. loss:  2.15239167e+00\n",
      "Epoch: 10618 mean train loss:  2.00911309e-05, mean val. loss:  2.15254903e+00\n",
      "Epoch: 10619 mean train loss:  2.03885720e-05, mean val. loss:  2.15266418e+00\n",
      "Epoch: 10620 mean train loss:  2.02180818e-05, mean val. loss:  2.15278363e+00\n",
      "Epoch: 10621 mean train loss:  2.01288203e-05, mean val. loss:  2.15288591e+00\n",
      "Epoch: 10622 mean train loss:  2.02706433e-05, mean val. loss:  2.15297723e+00\n",
      "Epoch: 10623 mean train loss:  2.01589137e-05, mean val. loss:  2.15304899e+00\n",
      "Epoch: 10624 mean train loss:  2.02349329e-05, mean val. loss:  2.15311766e+00\n",
      "Epoch: 10625 mean train loss:  2.01778021e-05, mean val. loss:  2.15317559e+00\n",
      "Epoch: 10626 mean train loss:  2.04007665e-05, mean val. loss:  2.15322804e+00\n",
      "Epoch: 10627 mean train loss:  2.04059179e-05, mean val. loss:  2.15326166e+00\n",
      "Epoch: 10628 mean train loss:  2.01874645e-05, mean val. loss:  2.15328884e+00\n",
      "Epoch: 10629 mean train loss:  1.98828056e-05, mean val. loss:  2.15335345e+00\n",
      "Epoch: 10630 mean train loss:  2.00143259e-05, mean val. loss:  2.15343761e+00\n",
      "Epoch: 10631 mean train loss:  1.98273337e-05, mean val. loss:  2.15354991e+00\n",
      "Epoch: 10632 mean train loss:  2.01639486e-05, mean val. loss:  2.15367365e+00\n",
      "Epoch: 10633 mean train loss:  2.01976800e-05, mean val. loss:  2.15378451e+00\n",
      "Epoch: 10634 mean train loss:  2.00239301e-05, mean val. loss:  2.15390086e+00\n",
      "Epoch: 10635 mean train loss:  1.99868227e-05, mean val. loss:  2.15405965e+00\n",
      "Epoch: 10636 mean train loss:  2.01296061e-05, mean val. loss:  2.15423036e+00\n",
      "Epoch: 10637 mean train loss:  2.02707888e-05, mean val. loss:  2.15439606e+00\n",
      "Epoch: 10638 mean train loss:  2.01690709e-05, mean val. loss:  2.15459800e+00\n",
      "Epoch: 10639 mean train loss:  1.98859780e-05, mean val. loss:  2.15480018e+00\n",
      "Epoch: 10640 mean train loss:  2.01390940e-05, mean val. loss:  2.15500522e+00\n",
      "Epoch: 10641 mean train loss:  2.01344374e-05, mean val. loss:  2.15523839e+00\n",
      "Epoch: 10642 mean train loss:  2.03485251e-05, mean val. loss:  2.15546656e+00\n",
      "Epoch: 10643 mean train loss:  2.01184012e-05, mean val. loss:  2.15567684e+00\n",
      "Epoch: 10644 mean train loss:  2.03690142e-05, mean val. loss:  2.15590429e+00\n",
      "Epoch: 10645 mean train loss:  2.02917145e-05, mean val. loss:  2.15611315e+00\n",
      "Epoch: 10646 mean train loss:  2.02709925e-05, mean val. loss:  2.15634537e+00\n",
      "Epoch: 10647 mean train loss:  2.02192168e-05, mean val. loss:  2.15655136e+00\n",
      "Epoch: 10648 mean train loss:  2.02191877e-05, mean val. loss:  2.15669370e+00\n",
      "Epoch: 10649 mean train loss:  2.04388634e-05, mean val. loss:  2.15679145e+00\n",
      "Epoch: 10650 mean train loss:  2.01254734e-05, mean val. loss:  2.15683484e+00\n",
      "Epoch: 10651 mean train loss:  2.00219220e-05, mean val. loss:  2.15686011e+00\n",
      "Epoch: 10652 mean train loss:  2.03759701e-05, mean val. loss:  2.15682960e+00\n",
      "Epoch: 10653 mean train loss:  1.99924398e-05, mean val. loss:  2.15674829e+00\n",
      "Epoch: 10654 mean train loss:  2.02060619e-05, mean val. loss:  2.15666604e+00\n",
      "Epoch: 10655 mean train loss:  2.05299584e-05, mean val. loss:  2.15656877e+00\n",
      "Epoch: 10656 mean train loss:  2.01005605e-05, mean val. loss:  2.15645480e+00\n",
      "Epoch: 10657 mean train loss:  2.01598159e-05, mean val. loss:  2.15635443e+00\n",
      "Epoch: 10658 mean train loss:  2.03009986e-05, mean val. loss:  2.15621376e+00\n",
      "Epoch: 10659 mean train loss:  2.02768715e-05, mean val. loss:  2.15606761e+00\n",
      "Epoch: 10660 mean train loss:  2.01186340e-05, mean val. loss:  2.15592480e+00\n",
      "Epoch: 10661 mean train loss:  1.99681672e-05, mean val. loss:  2.15579963e+00\n",
      "Epoch: 10662 mean train loss:  2.04163953e-05, mean val. loss:  2.15568423e+00\n",
      "Epoch: 10663 mean train loss:  2.00493960e-05, mean val. loss:  2.15558958e+00\n",
      "Epoch: 10664 mean train loss:  2.02284136e-05, mean val. loss:  2.15551662e+00\n",
      "Epoch: 10665 mean train loss:  2.01172661e-05, mean val. loss:  2.15545082e+00\n",
      "Epoch: 10666 mean train loss:  2.01758230e-05, mean val. loss:  2.15542912e+00\n",
      "Epoch: 10667 mean train loss:  2.00468057e-05, mean val. loss:  2.15543723e+00\n",
      "Epoch: 10668 mean train loss:  1.99443894e-05, mean val. loss:  2.15545869e+00\n",
      "Epoch: 10669 mean train loss:  2.02537922e-05, mean val. loss:  2.15553498e+00\n",
      "Epoch: 10670 mean train loss:  2.01430521e-05, mean val. loss:  2.15561843e+00\n",
      "Epoch: 10671 mean train loss:  2.00416835e-05, mean val. loss:  2.15577030e+00\n",
      "Epoch: 10672 mean train loss:  1.99956994e-05, mean val. loss:  2.15597820e+00\n",
      "Epoch: 10673 mean train loss:  2.02126394e-05, mean val. loss:  2.15624070e+00\n",
      "Epoch: 10674 mean train loss:  1.98803900e-05, mean val. loss:  2.15657163e+00\n",
      "Epoch: 10675 mean train loss:  2.01442454e-05, mean val. loss:  2.15691829e+00\n",
      "Epoch: 10676 mean train loss:  2.01853691e-05, mean val. loss:  2.15730190e+00\n",
      "Epoch: 10677 mean train loss:  2.02952942e-05, mean val. loss:  2.15771794e+00\n",
      "Epoch: 10678 mean train loss:  2.01216317e-05, mean val. loss:  2.15811610e+00\n",
      "Epoch: 10679 mean train loss:  2.01138319e-05, mean val. loss:  2.15852737e+00\n",
      "Epoch: 10680 mean train loss:  2.00821378e-05, mean val. loss:  2.15889192e+00\n",
      "Epoch: 10681 mean train loss:  2.01420917e-05, mean val. loss:  2.15922284e+00\n",
      "Epoch: 10682 mean train loss:  2.01951771e-05, mean val. loss:  2.15951109e+00\n",
      "Epoch: 10683 mean train loss:  2.00081558e-05, mean val. loss:  2.15977120e+00\n",
      "Epoch: 10684 mean train loss:  2.01936637e-05, mean val. loss:  2.15997052e+00\n",
      "Epoch: 10685 mean train loss:  2.00632494e-05, mean val. loss:  2.16013479e+00\n",
      "Epoch: 10686 mean train loss:  2.00532959e-05, mean val. loss:  2.16029644e+00\n",
      "Epoch: 10687 mean train loss:  2.00775976e-05, mean val. loss:  2.16047931e+00\n",
      "Epoch: 10688 mean train loss:  1.98181369e-05, mean val. loss:  2.16064882e+00\n",
      "Epoch: 10689 mean train loss:  2.00389186e-05, mean val. loss:  2.16080832e+00\n",
      "Epoch: 10690 mean train loss:  2.00364739e-05, mean val. loss:  2.16095448e+00\n",
      "Epoch: 10691 mean train loss:  1.99595233e-05, mean val. loss:  2.16111183e+00\n",
      "Epoch: 10692 mean train loss:  2.01069051e-05, mean val. loss:  2.16127563e+00\n",
      "Epoch: 10693 mean train loss:  1.97987247e-05, mean val. loss:  2.16145349e+00\n",
      "Epoch: 10694 mean train loss:  2.00920622e-05, mean val. loss:  2.16166282e+00\n",
      "Epoch: 10695 mean train loss:  1.99126080e-05, mean val. loss:  2.16186237e+00\n",
      "Epoch: 10696 mean train loss:  2.01212533e-05, mean val. loss:  2.16206312e+00\n",
      "Epoch: 10697 mean train loss:  2.02702649e-05, mean val. loss:  2.16223454e+00\n",
      "Epoch: 10698 mean train loss:  1.99162168e-05, mean val. loss:  2.16240573e+00\n",
      "Epoch: 10699 mean train loss:  2.01237854e-05, mean val. loss:  2.16258907e+00\n",
      "Epoch: 10700 mean train loss:  2.02386582e-05, mean val. loss:  2.16277432e+00\n",
      "Epoch: 10701 mean train loss:  2.00033246e-05, mean val. loss:  2.16298604e+00\n",
      "Epoch: 10702 mean train loss:  2.02251540e-05, mean val. loss:  2.16320610e+00\n",
      "Epoch: 10703 mean train loss:  1.99987553e-05, mean val. loss:  2.16342187e+00\n",
      "Epoch: 10704 mean train loss:  2.02776573e-05, mean val. loss:  2.16364217e+00\n",
      "Epoch: 10705 mean train loss:  2.03647360e-05, mean val. loss:  2.16382289e+00\n",
      "Epoch: 10706 mean train loss:  1.96734618e-05, mean val. loss:  2.16398215e+00\n",
      "Epoch: 10707 mean train loss:  2.02595838e-05, mean val. loss:  2.16411829e+00\n",
      "Epoch: 10708 mean train loss:  2.03193049e-05, mean val. loss:  2.16422248e+00\n",
      "Epoch: 10709 mean train loss:  1.99420319e-05, mean val. loss:  2.16431522e+00\n",
      "Epoch: 10710 mean train loss:  2.00262584e-05, mean val. loss:  2.16438174e+00\n",
      "Epoch: 10711 mean train loss:  1.98229973e-05, mean val. loss:  2.16443443e+00\n",
      "Epoch: 10712 mean train loss:  2.00263457e-05, mean val. loss:  2.16448450e+00\n",
      "Epoch: 10713 mean train loss:  2.00522481e-05, mean val. loss:  2.16448927e+00\n",
      "Epoch: 10714 mean train loss:  2.01608054e-05, mean val. loss:  2.16449714e+00\n",
      "Epoch: 10715 mean train loss:  1.98346970e-05, mean val. loss:  2.16451383e+00\n",
      "Epoch: 10716 mean train loss:  1.99741335e-05, mean val. loss:  2.16449380e+00\n",
      "Epoch: 10717 mean train loss:  1.95661560e-05, mean val. loss:  2.16449237e+00\n",
      "Epoch: 10718 mean train loss:  1.99166243e-05, mean val. loss:  2.16449094e+00\n",
      "Epoch: 10719 mean train loss:  2.02701776e-05, mean val. loss:  2.16450667e+00\n",
      "Epoch: 10720 mean train loss:  2.00138311e-05, mean val. loss:  2.16453624e+00\n",
      "Epoch: 10721 mean train loss:  2.01507355e-05, mean val. loss:  2.16454458e+00\n",
      "Epoch: 10722 mean train loss:  2.00132199e-05, mean val. loss:  2.16457462e+00\n",
      "Epoch: 10723 mean train loss:  2.02430820e-05, mean val. loss:  2.16460872e+00\n",
      "Epoch: 10724 mean train loss:  1.99694769e-05, mean val. loss:  2.16466308e+00\n",
      "Epoch: 10725 mean train loss:  2.00746872e-05, mean val. loss:  2.16478705e+00\n",
      "Epoch: 10726 mean train loss:  1.99787319e-05, mean val. loss:  2.16490149e+00\n",
      "Epoch: 10727 mean train loss:  2.03418313e-05, mean val. loss:  2.16498947e+00\n",
      "Epoch: 10728 mean train loss:  2.00612994e-05, mean val. loss:  2.16509676e+00\n",
      "Epoch: 10729 mean train loss:  2.00565264e-05, mean val. loss:  2.16519475e+00\n",
      "Epoch: 10730 mean train loss:  1.99672650e-05, mean val. loss:  2.16530061e+00\n",
      "Epoch: 10731 mean train loss:  1.99898495e-05, mean val. loss:  2.16541862e+00\n",
      "Epoch: 10732 mean train loss:  1.99607166e-05, mean val. loss:  2.16554046e+00\n",
      "Epoch: 10733 mean train loss:  1.96095498e-05, mean val. loss:  2.16564846e+00\n",
      "Epoch: 10734 mean train loss:  2.01177318e-05, mean val. loss:  2.16571307e+00\n",
      "Epoch: 10735 mean train loss:  1.97468617e-05, mean val. loss:  2.16578102e+00\n",
      "Epoch: 10736 mean train loss:  2.00285576e-05, mean val. loss:  2.16586280e+00\n",
      "Epoch: 10737 mean train loss:  2.01612420e-05, mean val. loss:  2.16591311e+00\n",
      "Epoch: 10738 mean train loss:  1.97176705e-05, mean val. loss:  2.16598225e+00\n",
      "Epoch: 10739 mean train loss:  1.99642382e-05, mean val. loss:  2.16607690e+00\n",
      "Epoch: 10740 mean train loss:  2.00173235e-05, mean val. loss:  2.16618824e+00\n",
      "Epoch: 10741 mean train loss:  1.99239585e-05, mean val. loss:  2.16630721e+00\n",
      "Epoch: 10742 mean train loss:  1.99554779e-05, mean val. loss:  2.16643906e+00\n",
      "Epoch: 10743 mean train loss:  2.00050126e-05, mean val. loss:  2.16659832e+00\n",
      "Epoch: 10744 mean train loss:  1.99330098e-05, mean val. loss:  2.16677141e+00\n",
      "Epoch: 10745 mean train loss:  1.99565839e-05, mean val. loss:  2.16697073e+00\n",
      "Epoch: 10746 mean train loss:  2.01513467e-05, mean val. loss:  2.16716957e+00\n",
      "Epoch: 10747 mean train loss:  2.01042858e-05, mean val. loss:  2.16737437e+00\n",
      "Epoch: 10748 mean train loss:  2.03765230e-05, mean val. loss:  2.16757417e+00\n",
      "Epoch: 10749 mean train loss:  2.01342045e-05, mean val. loss:  2.16776323e+00\n",
      "Epoch: 10750 mean train loss:  1.97635964e-05, mean val. loss:  2.16797113e+00\n",
      "Epoch: 10751 mean train loss:  1.97484333e-05, mean val. loss:  2.16816068e+00\n",
      "Epoch: 10752 mean train loss:  2.00645009e-05, mean val. loss:  2.16835165e+00\n",
      "Epoch: 10753 mean train loss:  2.00944196e-05, mean val. loss:  2.16852641e+00\n",
      "Epoch: 10754 mean train loss:  2.02001829e-05, mean val. loss:  2.16865301e+00\n",
      "Epoch: 10755 mean train loss:  1.97312620e-05, mean val. loss:  2.16877246e+00\n",
      "Epoch: 10756 mean train loss:  1.99259666e-05, mean val. loss:  2.16882777e+00\n",
      "Epoch: 10757 mean train loss:  2.01347284e-05, mean val. loss:  2.16888857e+00\n",
      "Epoch: 10758 mean train loss:  2.00647628e-05, mean val. loss:  2.16893172e+00\n",
      "Epoch: 10759 mean train loss:  2.00585346e-05, mean val. loss:  2.16894031e+00\n",
      "Epoch: 10760 mean train loss:  2.00861832e-05, mean val. loss:  2.16892695e+00\n",
      "Epoch: 10761 mean train loss:  2.02684314e-05, mean val. loss:  2.16890550e+00\n",
      "Epoch: 10762 mean train loss:  2.00008217e-05, mean val. loss:  2.16886497e+00\n",
      "Epoch: 10763 mean train loss:  2.00237846e-05, mean val. loss:  2.16883969e+00\n",
      "Epoch: 10764 mean train loss:  2.01473595e-05, mean val. loss:  2.16878700e+00\n",
      "Epoch: 10765 mean train loss:  2.01079529e-05, mean val. loss:  2.16878533e+00\n",
      "Epoch: 10766 mean train loss:  1.99521601e-05, mean val. loss:  2.16880178e+00\n",
      "Epoch: 10767 mean train loss:  1.99005008e-05, mean val. loss:  2.16884685e+00\n",
      "Epoch: 10768 mean train loss:  2.01320800e-05, mean val. loss:  2.16891050e+00\n",
      "Epoch: 10769 mean train loss:  1.98199996e-05, mean val. loss:  2.16902566e+00\n",
      "Epoch: 10770 mean train loss:  2.00969516e-05, mean val. loss:  2.16918802e+00\n",
      "Epoch: 10771 mean train loss:  2.01627263e-05, mean val. loss:  2.16939950e+00\n",
      "Epoch: 10772 mean train loss:  2.00915965e-05, mean val. loss:  2.16964841e+00\n",
      "Epoch: 10773 mean train loss:  2.00111535e-05, mean val. loss:  2.16994691e+00\n",
      "Epoch: 10774 mean train loss:  2.00002687e-05, mean val. loss:  2.17029262e+00\n",
      "Epoch: 10775 mean train loss:  1.99813803e-05, mean val. loss:  2.17065835e+00\n",
      "Epoch: 10776 mean train loss:  2.02818192e-05, mean val. loss:  2.17106771e+00\n",
      "Epoch: 10777 mean train loss:  1.98908383e-05, mean val. loss:  2.17145729e+00\n",
      "Epoch: 10778 mean train loss:  2.02561787e-05, mean val. loss:  2.17182541e+00\n",
      "Epoch: 10779 mean train loss:  1.99070782e-05, mean val. loss:  2.17217016e+00\n",
      "Epoch: 10780 mean train loss:  1.98141206e-05, mean val. loss:  2.17247701e+00\n",
      "Epoch: 10781 mean train loss:  2.00095819e-05, mean val. loss:  2.17273068e+00\n",
      "Epoch: 10782 mean train loss:  2.02557712e-05, mean val. loss:  2.17294812e+00\n",
      "Epoch: 10783 mean train loss:  1.99670903e-05, mean val. loss:  2.17310071e+00\n",
      "Epoch: 10784 mean train loss:  2.02394731e-05, mean val. loss:  2.17319274e+00\n",
      "Epoch: 10785 mean train loss:  2.00303621e-05, mean val. loss:  2.17323303e+00\n",
      "Epoch: 10786 mean train loss:  1.99964270e-05, mean val. loss:  2.17323899e+00\n",
      "Epoch: 10787 mean train loss:  1.99943024e-05, mean val. loss:  2.17319775e+00\n",
      "Epoch: 10788 mean train loss:  1.99979695e-05, mean val. loss:  2.17315888e+00\n",
      "Epoch: 10789 mean train loss:  1.97782647e-05, mean val. loss:  2.17308903e+00\n",
      "Epoch: 10790 mean train loss:  1.97743939e-05, mean val. loss:  2.17300153e+00\n",
      "Epoch: 10791 mean train loss:  2.01998628e-05, mean val. loss:  2.17289948e+00\n",
      "Epoch: 10792 mean train loss:  2.01729708e-05, mean val. loss:  2.17279935e+00\n",
      "Epoch: 10793 mean train loss:  2.00938957e-05, mean val. loss:  2.17269683e+00\n",
      "Epoch: 10794 mean train loss:  1.97182235e-05, mean val. loss:  2.17260528e+00\n",
      "Epoch: 10795 mean train loss:  1.98689522e-05, mean val. loss:  2.17252636e+00\n",
      "Epoch: 10796 mean train loss:  2.00908980e-05, mean val. loss:  2.17246866e+00\n",
      "Epoch: 10797 mean train loss:  1.98381313e-05, mean val. loss:  2.17244720e+00\n",
      "Epoch: 10798 mean train loss:  1.96908950e-05, mean val. loss:  2.17246962e+00\n",
      "Epoch: 10799 mean train loss:  2.01624352e-05, mean val. loss:  2.17251515e+00\n",
      "Epoch: 10800 mean train loss:  2.00957002e-05, mean val. loss:  2.17259932e+00\n",
      "Epoch: 10801 mean train loss:  1.98263733e-05, mean val. loss:  2.17271614e+00\n",
      "Epoch: 10802 mean train loss:  1.99796632e-05, mean val. loss:  2.17285419e+00\n",
      "Epoch: 10803 mean train loss:  2.02610099e-05, mean val. loss:  2.17299700e+00\n",
      "Epoch: 10804 mean train loss:  1.99305359e-05, mean val. loss:  2.17314768e+00\n",
      "Epoch: 10805 mean train loss:  2.01831281e-05, mean val. loss:  2.17329049e+00\n",
      "Epoch: 10806 mean train loss:  1.99632486e-05, mean val. loss:  2.17346501e+00\n",
      "Epoch: 10807 mean train loss:  2.01144430e-05, mean val. loss:  2.17364931e+00\n",
      "Epoch: 10808 mean train loss:  1.98050984e-05, mean val. loss:  2.17380118e+00\n",
      "Epoch: 10809 mean train loss:  1.97091431e-05, mean val. loss:  2.17397785e+00\n",
      "Epoch: 10810 mean train loss:  1.98199705e-05, mean val. loss:  2.17415071e+00\n",
      "Epoch: 10811 mean train loss:  2.01387156e-05, mean val. loss:  2.17432594e+00\n",
      "Epoch: 10812 mean train loss:  1.98720663e-05, mean val. loss:  2.17449427e+00\n",
      "Epoch: 10813 mean train loss:  2.00406066e-05, mean val. loss:  2.17461300e+00\n",
      "Epoch: 10814 mean train loss:  2.01004150e-05, mean val. loss:  2.17473459e+00\n",
      "Epoch: 10815 mean train loss:  2.00791110e-05, mean val. loss:  2.17485857e+00\n",
      "Epoch: 10816 mean train loss:  1.99416245e-05, mean val. loss:  2.17498279e+00\n",
      "Epoch: 10817 mean train loss:  2.00435752e-05, mean val. loss:  2.17509913e+00\n",
      "Epoch: 10818 mean train loss:  2.02982919e-05, mean val. loss:  2.17521048e+00\n",
      "Epoch: 10819 mean train loss:  1.99986389e-05, mean val. loss:  2.17531013e+00\n",
      "Epoch: 10820 mean train loss:  1.97054178e-05, mean val. loss:  2.17543006e+00\n",
      "Epoch: 10821 mean train loss:  2.00539071e-05, mean val. loss:  2.17558742e+00\n",
      "Epoch: 10822 mean train loss:  1.93854212e-05, mean val. loss:  2.17576671e+00\n",
      "Epoch: 10823 mean train loss:  1.98330963e-05, mean val. loss:  2.17596316e+00\n",
      "Epoch: 10824 mean train loss:  2.01214280e-05, mean val. loss:  2.17615175e+00\n",
      "Epoch: 10825 mean train loss:  1.99122005e-05, mean val. loss:  2.17632985e+00\n",
      "Epoch: 10826 mean train loss:  1.98112393e-05, mean val. loss:  2.17653155e+00\n",
      "Epoch: 10827 mean train loss:  1.99737842e-05, mean val. loss:  2.17675376e+00\n",
      "Epoch: 10828 mean train loss:  1.99585920e-05, mean val. loss:  2.17700124e+00\n",
      "Epoch: 10829 mean train loss:  1.99764327e-05, mean val. loss:  2.17724466e+00\n",
      "Epoch: 10830 mean train loss:  1.97945628e-05, mean val. loss:  2.17747736e+00\n",
      "Epoch: 10831 mean train loss:  1.99076603e-05, mean val. loss:  2.17771411e+00\n",
      "Epoch: 10832 mean train loss:  1.97347254e-05, mean val. loss:  2.17794561e+00\n",
      "Epoch: 10833 mean train loss:  1.97894406e-05, mean val. loss:  2.17818713e+00\n",
      "Epoch: 10834 mean train loss:  1.99859496e-05, mean val. loss:  2.17844296e+00\n",
      "Epoch: 10835 mean train loss:  1.97861809e-05, mean val. loss:  2.17865109e+00\n",
      "Epoch: 10836 mean train loss:  2.00117938e-05, mean val. loss:  2.17886353e+00\n",
      "Epoch: 10837 mean train loss:  2.04168900e-05, mean val. loss:  2.17903113e+00\n",
      "Epoch: 10838 mean train loss:  1.96137989e-05, mean val. loss:  2.17920351e+00\n",
      "Epoch: 10839 mean train loss:  2.01563817e-05, mean val. loss:  2.17936158e+00\n",
      "Epoch: 10840 mean train loss:  2.00398208e-05, mean val. loss:  2.17949271e+00\n",
      "Epoch: 10841 mean train loss:  1.96894980e-05, mean val. loss:  2.17960763e+00\n",
      "Epoch: 10842 mean train loss:  2.01032963e-05, mean val. loss:  2.17972231e+00\n",
      "Epoch: 10843 mean train loss:  1.97211048e-05, mean val. loss:  2.17982960e+00\n",
      "Epoch: 10844 mean train loss:  1.99905189e-05, mean val. loss:  2.17991972e+00\n",
      "Epoch: 10845 mean train loss:  1.98813214e-05, mean val. loss:  2.17998004e+00\n",
      "Epoch: 10846 mean train loss:  2.01811199e-05, mean val. loss:  2.18000126e+00\n",
      "Epoch: 10847 mean train loss:  1.97571644e-05, mean val. loss:  2.18001080e+00\n",
      "Epoch: 10848 mean train loss:  2.02935771e-05, mean val. loss:  2.18003011e+00\n",
      "Epoch: 10849 mean train loss:  1.97604531e-05, mean val. loss:  2.18005466e+00\n",
      "Epoch: 10850 mean train loss:  1.98672060e-05, mean val. loss:  2.18004560e+00\n",
      "Epoch: 10851 mean train loss:  1.98245107e-05, mean val. loss:  2.18007874e+00\n",
      "Epoch: 10852 mean train loss:  2.00257055e-05, mean val. loss:  2.18012094e+00\n",
      "Epoch: 10853 mean train loss:  1.98013731e-05, mean val. loss:  2.18019772e+00\n",
      "Epoch: 10854 mean train loss:  2.00601935e-05, mean val. loss:  2.18025780e+00\n",
      "Epoch: 10855 mean train loss:  1.99952628e-05, mean val. loss:  2.18030977e+00\n",
      "Epoch: 10856 mean train loss:  1.96998299e-05, mean val. loss:  2.18041420e+00\n",
      "Epoch: 10857 mean train loss:  1.98507041e-05, mean val. loss:  2.18058062e+00\n",
      "Epoch: 10858 mean train loss:  1.99862698e-05, mean val. loss:  2.18074107e+00\n",
      "Epoch: 10859 mean train loss:  1.97303598e-05, mean val. loss:  2.18091774e+00\n",
      "Epoch: 10860 mean train loss:  1.97022455e-05, mean val. loss:  2.18114114e+00\n",
      "Epoch: 10861 mean train loss:  2.00611539e-05, mean val. loss:  2.18133235e+00\n",
      "Epoch: 10862 mean train loss:  2.01002113e-05, mean val. loss:  2.18155456e+00\n",
      "Epoch: 10863 mean train loss:  1.96576584e-05, mean val. loss:  2.18177128e+00\n",
      "Epoch: 10864 mean train loss:  1.98050111e-05, mean val. loss:  2.18198681e+00\n",
      "Epoch: 10865 mean train loss:  1.97788759e-05, mean val. loss:  2.18221831e+00\n",
      "Epoch: 10866 mean train loss:  1.97500340e-05, mean val. loss:  2.18246770e+00\n",
      "Epoch: 10867 mean train loss:  2.03163363e-05, mean val. loss:  2.18266010e+00\n",
      "Epoch: 10868 mean train loss:  1.98792841e-05, mean val. loss:  2.18284130e+00\n",
      "Epoch: 10869 mean train loss:  2.02038500e-05, mean val. loss:  2.18296838e+00\n",
      "Epoch: 10870 mean train loss:  1.96007895e-05, mean val. loss:  2.18309546e+00\n",
      "Epoch: 10871 mean train loss:  1.99408387e-05, mean val. loss:  2.18320322e+00\n",
      "Epoch: 10872 mean train loss:  2.00039067e-05, mean val. loss:  2.18329453e+00\n",
      "Epoch: 10873 mean train loss:  2.00042850e-05, mean val. loss:  2.18337870e+00\n",
      "Epoch: 10874 mean train loss:  2.02594965e-05, mean val. loss:  2.18344092e+00\n",
      "Epoch: 10875 mean train loss:  1.98530033e-05, mean val. loss:  2.18347049e+00\n",
      "Epoch: 10876 mean train loss:  2.02881347e-05, mean val. loss:  2.18345261e+00\n",
      "Epoch: 10877 mean train loss:  1.99377828e-05, mean val. loss:  2.18342471e+00\n",
      "Epoch: 10878 mean train loss:  2.04018725e-05, mean val. loss:  2.18335557e+00\n",
      "Epoch: 10879 mean train loss:  2.02304218e-05, mean val. loss:  2.18326950e+00\n",
      "Epoch: 10880 mean train loss:  2.00115319e-05, mean val. loss:  2.18320441e+00\n",
      "Epoch: 10881 mean train loss:  1.99089991e-05, mean val. loss:  2.18312120e+00\n",
      "Epoch: 10882 mean train loss:  1.98547496e-05, mean val. loss:  2.18303990e+00\n",
      "Epoch: 10883 mean train loss:  1.99938077e-05, mean val. loss:  2.18300128e+00\n",
      "Epoch: 10884 mean train loss:  1.96985202e-05, mean val. loss:  2.18296075e+00\n",
      "Epoch: 10885 mean train loss:  2.00736977e-05, mean val. loss:  2.18293667e+00\n",
      "Epoch: 10886 mean train loss:  2.01707589e-05, mean val. loss:  2.18290257e+00\n",
      "Epoch: 10887 mean train loss:  1.97220070e-05, mean val. loss:  2.18288159e+00\n",
      "Epoch: 10888 mean train loss:  2.01300718e-05, mean val. loss:  2.18285680e+00\n",
      "Epoch: 10889 mean train loss:  1.95971807e-05, mean val. loss:  2.18285632e+00\n",
      "Epoch: 10890 mean train loss:  1.98576308e-05, mean val. loss:  2.18285370e+00\n",
      "Epoch: 10891 mean train loss:  1.97796617e-05, mean val. loss:  2.18288779e+00\n",
      "Epoch: 10892 mean train loss:  1.97256741e-05, mean val. loss:  2.18293786e+00\n",
      "Epoch: 10893 mean train loss:  1.98878115e-05, mean val. loss:  2.18303323e+00\n",
      "Epoch: 10894 mean train loss:  1.98032940e-05, mean val. loss:  2.18316102e+00\n",
      "Epoch: 10895 mean train loss:  2.00921495e-05, mean val. loss:  2.18331027e+00\n",
      "Epoch: 10896 mean train loss:  1.99755013e-05, mean val. loss:  2.18349266e+00\n",
      "Epoch: 10897 mean train loss:  1.99710776e-05, mean val. loss:  2.18371153e+00\n",
      "Epoch: 10898 mean train loss:  1.97629852e-05, mean val. loss:  2.18398714e+00\n",
      "Epoch: 10899 mean train loss:  1.95203465e-05, mean val. loss:  2.18431783e+00\n",
      "Epoch: 10900 mean train loss:  2.01085641e-05, mean val. loss:  2.18466997e+00\n",
      "Epoch: 10901 mean train loss:  1.98234047e-05, mean val. loss:  2.18505049e+00\n",
      "Epoch: 10902 mean train loss:  2.00826908e-05, mean val. loss:  2.18544507e+00\n",
      "Epoch: 10903 mean train loss:  2.00446812e-05, mean val. loss:  2.18581247e+00\n",
      "Epoch: 10904 mean train loss:  1.96375477e-05, mean val. loss:  2.18615651e+00\n",
      "Epoch: 10905 mean train loss:  1.97552727e-05, mean val. loss:  2.18651676e+00\n",
      "Epoch: 10906 mean train loss:  1.96603360e-05, mean val. loss:  2.18685484e+00\n",
      "Epoch: 10907 mean train loss:  1.98608614e-05, mean val. loss:  2.18715382e+00\n",
      "Epoch: 10908 mean train loss:  1.96346955e-05, mean val. loss:  2.18741584e+00\n",
      "Epoch: 10909 mean train loss:  1.99579808e-05, mean val. loss:  2.18764210e+00\n",
      "Epoch: 10910 mean train loss:  1.96663023e-05, mean val. loss:  2.18786216e+00\n",
      "Epoch: 10911 mean train loss:  1.95574830e-05, mean val. loss:  2.18806458e+00\n",
      "Epoch: 10912 mean train loss:  1.96812907e-05, mean val. loss:  2.18828273e+00\n",
      "Epoch: 10913 mean train loss:  2.01263174e-05, mean val. loss:  2.18846750e+00\n",
      "Epoch: 10914 mean train loss:  1.97385671e-05, mean val. loss:  2.18862391e+00\n",
      "Epoch: 10915 mean train loss:  1.99001224e-05, mean val. loss:  2.18875194e+00\n",
      "Epoch: 10916 mean train loss:  1.99009082e-05, mean val. loss:  2.18886805e+00\n",
      "Epoch: 10917 mean train loss:  2.00162758e-05, mean val. loss:  2.18898821e+00\n",
      "Epoch: 10918 mean train loss:  1.97737245e-05, mean val. loss:  2.18910480e+00\n",
      "Epoch: 10919 mean train loss:  1.98217749e-05, mean val. loss:  2.18922830e+00\n",
      "Epoch: 10920 mean train loss:  1.97122572e-05, mean val. loss:  2.18933415e+00\n",
      "Epoch: 10921 mean train loss:  1.95059692e-05, mean val. loss:  2.18946624e+00\n",
      "Epoch: 10922 mean train loss:  1.98679045e-05, mean val. loss:  2.18961358e+00\n",
      "Epoch: 10923 mean train loss:  1.98894704e-05, mean val. loss:  2.18974495e+00\n",
      "Epoch: 10924 mean train loss:  1.99437491e-05, mean val. loss:  2.18986392e+00\n",
      "Epoch: 10925 mean train loss:  2.00697395e-05, mean val. loss:  2.19000721e+00\n",
      "Epoch: 10926 mean train loss:  1.98835041e-05, mean val. loss:  2.19014955e+00\n",
      "Epoch: 10927 mean train loss:  1.97771587e-05, mean val. loss:  2.19029593e+00\n",
      "Epoch: 10928 mean train loss:  1.98118505e-05, mean val. loss:  2.19044065e+00\n",
      "Epoch: 10929 mean train loss:  1.99179340e-05, mean val. loss:  2.19059968e+00\n",
      "Epoch: 10930 mean train loss:  2.03727395e-05, mean val. loss:  2.19074368e+00\n",
      "Epoch: 10931 mean train loss:  2.01551302e-05, mean val. loss:  2.19086194e+00\n",
      "Epoch: 10932 mean train loss:  1.96251785e-05, mean val. loss:  2.19099641e+00\n",
      "Epoch: 10933 mean train loss:  1.97794288e-05, mean val. loss:  2.19112444e+00\n",
      "Epoch: 10934 mean train loss:  1.97674090e-05, mean val. loss:  2.19126225e+00\n",
      "Epoch: 10935 mean train loss:  1.98036723e-05, mean val. loss:  2.19136167e+00\n",
      "Epoch: 10936 mean train loss:  2.01114162e-05, mean val. loss:  2.19143939e+00\n",
      "Epoch: 10937 mean train loss:  1.98027701e-05, mean val. loss:  2.19151688e+00\n",
      "Epoch: 10938 mean train loss:  1.97390327e-05, mean val. loss:  2.19158769e+00\n",
      "Epoch: 10939 mean train loss:  1.97605113e-05, mean val. loss:  2.19163537e+00\n",
      "Epoch: 10940 mean train loss:  1.99299247e-05, mean val. loss:  2.19168234e+00\n",
      "Epoch: 10941 mean train loss:  1.96944748e-05, mean val. loss:  2.19173360e+00\n",
      "Epoch: 10942 mean train loss:  1.98782654e-05, mean val. loss:  2.19178748e+00\n",
      "Epoch: 10943 mean train loss:  1.99053029e-05, mean val. loss:  2.19186473e+00\n",
      "Epoch: 10944 mean train loss:  1.98028283e-05, mean val. loss:  2.19195747e+00\n",
      "Epoch: 10945 mean train loss:  1.98748021e-05, mean val. loss:  2.19206071e+00\n",
      "Epoch: 10946 mean train loss:  1.96876354e-05, mean val. loss:  2.19216990e+00\n",
      "Epoch: 10947 mean train loss:  1.97557965e-05, mean val. loss:  2.19231176e+00\n",
      "Epoch: 10948 mean train loss:  1.97321642e-05, mean val. loss:  2.19248915e+00\n",
      "Epoch: 10949 mean train loss:  1.97530899e-05, mean val. loss:  2.19269562e+00\n",
      "Epoch: 10950 mean train loss:  1.98496855e-05, mean val. loss:  2.19289804e+00\n",
      "Epoch: 10951 mean train loss:  1.97795453e-05, mean val. loss:  2.19308496e+00\n",
      "Epoch: 10952 mean train loss:  2.04258540e-05, mean val. loss:  2.19321775e+00\n",
      "Epoch: 10953 mean train loss:  2.01041694e-05, mean val. loss:  2.19334793e+00\n",
      "Epoch: 10954 mean train loss:  1.98163616e-05, mean val. loss:  2.19345021e+00\n",
      "Epoch: 10955 mean train loss:  2.01096700e-05, mean val. loss:  2.19353127e+00\n",
      "Epoch: 10956 mean train loss:  1.99600472e-05, mean val. loss:  2.19359612e+00\n",
      "Epoch: 10957 mean train loss:  1.96247711e-05, mean val. loss:  2.19363451e+00\n",
      "Epoch: 10958 mean train loss:  1.96413603e-05, mean val. loss:  2.19367743e+00\n",
      "Epoch: 10959 mean train loss:  1.99063797e-05, mean val. loss:  2.19372916e+00\n",
      "Epoch: 10960 mean train loss:  1.97571644e-05, mean val. loss:  2.19378400e+00\n",
      "Epoch: 10961 mean train loss:  1.98500347e-05, mean val. loss:  2.19384098e+00\n",
      "Epoch: 10962 mean train loss:  1.98478810e-05, mean val. loss:  2.19392014e+00\n",
      "Epoch: 10963 mean train loss:  1.97969784e-05, mean val. loss:  2.19398832e+00\n",
      "Epoch: 10964 mean train loss:  2.00369104e-05, mean val. loss:  2.19404840e+00\n",
      "Epoch: 10965 mean train loss:  1.97291083e-05, mean val. loss:  2.19414282e+00\n",
      "Epoch: 10966 mean train loss:  1.95666507e-05, mean val. loss:  2.19425511e+00\n",
      "Epoch: 10967 mean train loss:  1.99490460e-05, mean val. loss:  2.19434381e+00\n",
      "Epoch: 10968 mean train loss:  1.99247443e-05, mean val. loss:  2.19443440e+00\n",
      "Epoch: 10969 mean train loss:  2.01293733e-05, mean val. loss:  2.19456768e+00\n",
      "Epoch: 10970 mean train loss:  1.99411588e-05, mean val. loss:  2.19473457e+00\n",
      "Epoch: 10971 mean train loss:  1.99092319e-05, mean val. loss:  2.19491935e+00\n",
      "Epoch: 10972 mean train loss:  2.00158975e-05, mean val. loss:  2.19510818e+00\n",
      "Epoch: 10973 mean train loss:  1.95475877e-05, mean val. loss:  2.19532847e+00\n",
      "Epoch: 10974 mean train loss:  1.97486370e-05, mean val. loss:  2.19555879e+00\n",
      "Epoch: 10975 mean train loss:  1.99088536e-05, mean val. loss:  2.19578457e+00\n",
      "Epoch: 10976 mean train loss:  2.00166251e-05, mean val. loss:  2.19597459e+00\n",
      "Epoch: 10977 mean train loss:  1.96697074e-05, mean val. loss:  2.19614339e+00\n",
      "Epoch: 10978 mean train loss:  1.94275344e-05, mean val. loss:  2.19633889e+00\n",
      "Epoch: 10979 mean train loss:  1.98238122e-05, mean val. loss:  2.19654655e+00\n",
      "Epoch: 10980 mean train loss:  2.00276845e-05, mean val. loss:  2.19673800e+00\n",
      "Epoch: 10981 mean train loss:  1.97962800e-05, mean val. loss:  2.19688582e+00\n",
      "Epoch: 10982 mean train loss:  1.95450557e-05, mean val. loss:  2.19702983e+00\n",
      "Epoch: 10983 mean train loss:  1.96842302e-05, mean val. loss:  2.19716811e+00\n",
      "Epoch: 10984 mean train loss:  1.97279442e-05, mean val. loss:  2.19729877e+00\n",
      "Epoch: 10985 mean train loss:  2.01825169e-05, mean val. loss:  2.19739175e+00\n",
      "Epoch: 10986 mean train loss:  1.97273330e-05, mean val. loss:  2.19746828e+00\n",
      "Epoch: 10987 mean train loss:  1.96053006e-05, mean val. loss:  2.19755960e+00\n",
      "Epoch: 10988 mean train loss:  1.97283516e-05, mean val. loss:  2.19765329e+00\n",
      "Epoch: 10989 mean train loss:  1.94709864e-05, mean val. loss:  2.19774461e+00\n",
      "Epoch: 10990 mean train loss:  1.95690664e-05, mean val. loss:  2.19783378e+00\n",
      "Epoch: 10991 mean train loss:  1.98360940e-05, mean val. loss:  2.19791412e+00\n",
      "Epoch: 10992 mean train loss:  1.99529750e-05, mean val. loss:  2.19798923e+00\n",
      "Epoch: 10993 mean train loss:  1.98942143e-05, mean val. loss:  2.19804859e+00\n",
      "Epoch: 10994 mean train loss:  1.96336186e-05, mean val. loss:  2.19810677e+00\n",
      "Epoch: 10995 mean train loss:  1.95631874e-05, mean val. loss:  2.19819736e+00\n",
      "Epoch: 10996 mean train loss:  1.97926711e-05, mean val. loss:  2.19830775e+00\n",
      "Epoch: 10997 mean train loss:  1.98966591e-05, mean val. loss:  2.19842052e+00\n",
      "Epoch: 10998 mean train loss:  2.00452632e-05, mean val. loss:  2.19854665e+00\n",
      "Epoch: 10999 mean train loss:  1.96843466e-05, mean val. loss:  2.19866395e+00\n",
      "Epoch: 11000 mean train loss:  1.99772767e-05, mean val. loss:  2.19876266e+00\n",
      "Epoch: 11001 mean train loss:  1.96714536e-05, mean val. loss:  2.19885039e+00\n",
      "Epoch: 11002 mean train loss:  1.98879570e-05, mean val. loss:  2.19892764e+00\n",
      "Epoch: 11003 mean train loss:  1.96481706e-05, mean val. loss:  2.19900036e+00\n",
      "Epoch: 11004 mean train loss:  1.97772169e-05, mean val. loss:  2.19905901e+00\n",
      "Epoch: 11005 mean train loss:  1.97952031e-05, mean val. loss:  2.19912457e+00\n",
      "Epoch: 11006 mean train loss:  1.95975299e-05, mean val. loss:  2.19919753e+00\n",
      "Epoch: 11007 mean train loss:  1.99895876e-05, mean val. loss:  2.19927382e+00\n",
      "Epoch: 11008 mean train loss:  1.97449117e-05, mean val. loss:  2.19934344e+00\n",
      "Epoch: 11009 mean train loss:  1.97665649e-05, mean val. loss:  2.19937944e+00\n",
      "Epoch: 11010 mean train loss:  1.98152848e-05, mean val. loss:  2.19942594e+00\n",
      "Epoch: 11011 mean train loss:  1.99997739e-05, mean val. loss:  2.19949889e+00\n",
      "Epoch: 11012 mean train loss:  1.96243636e-05, mean val. loss:  2.19958949e+00\n",
      "Epoch: 11013 mean train loss:  1.98601047e-05, mean val. loss:  2.19968677e+00\n",
      "Epoch: 11014 mean train loss:  1.98266061e-05, mean val. loss:  2.19979811e+00\n",
      "Epoch: 11015 mean train loss:  1.97939517e-05, mean val. loss:  2.19990706e+00\n",
      "Epoch: 11016 mean train loss:  1.98638591e-05, mean val. loss:  2.20001435e+00\n",
      "Epoch: 11017 mean train loss:  1.95312605e-05, mean val. loss:  2.20016575e+00\n",
      "Epoch: 11018 mean train loss:  1.98213966e-05, mean val. loss:  2.20036197e+00\n",
      "Epoch: 11019 mean train loss:  2.01878720e-05, mean val. loss:  2.20056272e+00\n",
      "Epoch: 11020 mean train loss:  2.00169161e-05, mean val. loss:  2.20077562e+00\n",
      "Epoch: 11021 mean train loss:  1.98945054e-05, mean val. loss:  2.20099854e+00\n",
      "Epoch: 11022 mean train loss:  1.99567585e-05, mean val. loss:  2.20118904e+00\n",
      "Epoch: 11023 mean train loss:  1.95546309e-05, mean val. loss:  2.20138407e+00\n",
      "Epoch: 11024 mean train loss:  2.00422364e-05, mean val. loss:  2.20156431e+00\n",
      "Epoch: 11025 mean train loss:  2.01509392e-05, mean val. loss:  2.20167303e+00\n",
      "Epoch: 11026 mean train loss:  1.99163624e-05, mean val. loss:  2.20180178e+00\n",
      "Epoch: 11027 mean train loss:  2.00358045e-05, mean val. loss:  2.20191407e+00\n",
      "Epoch: 11028 mean train loss:  1.96363253e-05, mean val. loss:  2.20203233e+00\n",
      "Epoch: 11029 mean train loss:  1.95733446e-05, mean val. loss:  2.20212936e+00\n",
      "Epoch: 11030 mean train loss:  1.95743050e-05, mean val. loss:  2.20222211e+00\n",
      "Epoch: 11031 mean train loss:  1.96868205e-05, mean val. loss:  2.20230722e+00\n",
      "Epoch: 11032 mean train loss:  1.99229689e-05, mean val. loss:  2.20237589e+00\n",
      "Epoch: 11033 mean train loss:  1.98120833e-05, mean val. loss:  2.20241928e+00\n",
      "Epoch: 11034 mean train loss:  1.92041916e-05, mean val. loss:  2.20250034e+00\n",
      "Epoch: 11035 mean train loss:  1.99192727e-05, mean val. loss:  2.20259738e+00\n",
      "Epoch: 11036 mean train loss:  1.96898181e-05, mean val. loss:  2.20271683e+00\n",
      "Epoch: 11037 mean train loss:  1.98108028e-05, mean val. loss:  2.20282745e+00\n",
      "Epoch: 11038 mean train loss:  2.00113573e-05, mean val. loss:  2.20294857e+00\n",
      "Epoch: 11039 mean train loss:  1.99321075e-05, mean val. loss:  2.20306993e+00\n",
      "Epoch: 11040 mean train loss:  1.97461050e-05, mean val. loss:  2.20321202e+00\n",
      "Epoch: 11041 mean train loss:  2.00022478e-05, mean val. loss:  2.20335150e+00\n",
      "Epoch: 11042 mean train loss:  1.94037566e-05, mean val. loss:  2.20355368e+00\n",
      "Epoch: 11043 mean train loss:  1.95346947e-05, mean val. loss:  2.20377326e+00\n",
      "Epoch: 11044 mean train loss:  1.99491333e-05, mean val. loss:  2.20399380e+00\n",
      "Epoch: 11045 mean train loss:  1.98650523e-05, mean val. loss:  2.20420122e+00\n",
      "Epoch: 11046 mean train loss:  1.97909831e-05, mean val. loss:  2.20437646e+00\n",
      "Epoch: 11047 mean train loss:  1.98638008e-05, mean val. loss:  2.20453811e+00\n",
      "Epoch: 11048 mean train loss:  1.97932241e-05, mean val. loss:  2.20468903e+00\n",
      "Epoch: 11049 mean train loss:  1.97402842e-05, mean val. loss:  2.20482779e+00\n",
      "Epoch: 11050 mean train loss:  2.00615905e-05, mean val. loss:  2.20493484e+00\n",
      "Epoch: 11051 mean train loss:  1.96104520e-05, mean val. loss:  2.20504165e+00\n",
      "Epoch: 11052 mean train loss:  1.95958419e-05, mean val. loss:  2.20513606e+00\n",
      "Epoch: 11053 mean train loss:  1.94584427e-05, mean val. loss:  2.20521545e+00\n",
      "Epoch: 11054 mean train loss:  1.96943292e-05, mean val. loss:  2.20527148e+00\n",
      "Epoch: 11055 mean train loss:  1.97332702e-05, mean val. loss:  2.20529675e+00\n",
      "Epoch: 11056 mean train loss:  1.97932241e-05, mean val. loss:  2.20530462e+00\n",
      "Epoch: 11057 mean train loss:  1.97003828e-05, mean val. loss:  2.20533824e+00\n",
      "Epoch: 11058 mean train loss:  1.96064357e-05, mean val. loss:  2.20542765e+00\n",
      "Epoch: 11059 mean train loss:  2.00675277e-05, mean val. loss:  2.20550156e+00\n",
      "Epoch: 11060 mean train loss:  1.99231727e-05, mean val. loss:  2.20559025e+00\n",
      "Epoch: 11061 mean train loss:  2.02184892e-05, mean val. loss:  2.20566010e+00\n",
      "Epoch: 11062 mean train loss:  1.97984627e-05, mean val. loss:  2.20576644e+00\n",
      "Epoch: 11063 mean train loss:  1.94454333e-05, mean val. loss:  2.20590830e+00\n",
      "Epoch: 11064 mean train loss:  1.93023589e-05, mean val. loss:  2.20608783e+00\n",
      "Epoch: 11065 mean train loss:  1.99215719e-05, mean val. loss:  2.20628166e+00\n",
      "Epoch: 11066 mean train loss:  1.98076596e-05, mean val. loss:  2.20651412e+00\n",
      "Epoch: 11067 mean train loss:  1.99242786e-05, mean val. loss:  2.20672846e+00\n",
      "Epoch: 11068 mean train loss:  1.97402260e-05, mean val. loss:  2.20695162e+00\n",
      "Epoch: 11069 mean train loss:  1.97328627e-05, mean val. loss:  2.20713544e+00\n",
      "Epoch: 11070 mean train loss:  1.98330963e-05, mean val. loss:  2.20732069e+00\n",
      "Epoch: 11071 mean train loss:  1.98629859e-05, mean val. loss:  2.20745969e+00\n",
      "Epoch: 11072 mean train loss:  1.96681358e-05, mean val. loss:  2.20761108e+00\n",
      "Epoch: 11073 mean train loss:  1.98836497e-05, mean val. loss:  2.20774603e+00\n",
      "Epoch: 11074 mean train loss:  1.94795721e-05, mean val. loss:  2.20789814e+00\n",
      "Epoch: 11075 mean train loss:  1.96856854e-05, mean val. loss:  2.20804572e+00\n",
      "Epoch: 11076 mean train loss:  2.01312941e-05, mean val. loss:  2.20817804e+00\n",
      "Epoch: 11077 mean train loss:  1.94215681e-05, mean val. loss:  2.20834279e+00\n",
      "Epoch: 11078 mean train loss:  1.94345484e-05, mean val. loss:  2.20850396e+00\n",
      "Epoch: 11079 mean train loss:  1.98254420e-05, mean val. loss:  2.20864964e+00\n",
      "Epoch: 11080 mean train loss:  1.96734909e-05, mean val. loss:  2.20879006e+00\n",
      "Epoch: 11081 mean train loss:  1.97509071e-05, mean val. loss:  2.20890832e+00\n",
      "Epoch: 11082 mean train loss:  1.95922039e-05, mean val. loss:  2.20899916e+00\n",
      "Epoch: 11083 mean train loss:  1.94989552e-05, mean val. loss:  2.20908785e+00\n",
      "Epoch: 11084 mean train loss:  1.96223555e-05, mean val. loss:  2.20916057e+00\n",
      "Epoch: 11085 mean train loss:  1.96798937e-05, mean val. loss:  2.20923758e+00\n",
      "Epoch: 11086 mean train loss:  1.97479385e-05, mean val. loss:  2.20929217e+00\n",
      "Epoch: 11087 mean train loss:  1.97692425e-05, mean val. loss:  2.20934701e+00\n",
      "Epoch: 11088 mean train loss:  1.96347828e-05, mean val. loss:  2.20944762e+00\n",
      "Epoch: 11089 mean train loss:  1.97867921e-05, mean val. loss:  2.20955634e+00\n",
      "Epoch: 11090 mean train loss:  1.98273628e-05, mean val. loss:  2.20966458e+00\n",
      "Epoch: 11091 mean train loss:  1.94922904e-05, mean val. loss:  2.20980048e+00\n",
      "Epoch: 11092 mean train loss:  1.96405163e-05, mean val. loss:  2.20995092e+00\n",
      "Epoch: 11093 mean train loss:  1.97559712e-05, mean val. loss:  2.21014667e+00\n",
      "Epoch: 11094 mean train loss:  1.97081536e-05, mean val. loss:  2.21035171e+00\n",
      "Epoch: 11095 mean train loss:  1.96786714e-05, mean val. loss:  2.21057510e+00\n",
      "Epoch: 11096 mean train loss:  1.95281173e-05, mean val. loss:  2.21080995e+00\n",
      "Epoch: 11097 mean train loss:  1.95051834e-05, mean val. loss:  2.21103287e+00\n",
      "Epoch: 11098 mean train loss:  1.98193302e-05, mean val. loss:  2.21124029e+00\n",
      "Epoch: 11099 mean train loss:  1.95898756e-05, mean val. loss:  2.21143770e+00\n",
      "Epoch: 11100 mean train loss:  1.97035552e-05, mean val. loss:  2.21161485e+00\n",
      "Epoch: 11101 mean train loss:  1.95006432e-05, mean val. loss:  2.21177840e+00\n",
      "Epoch: 11102 mean train loss:  1.95390021e-05, mean val. loss:  2.21196318e+00\n",
      "Epoch: 11103 mean train loss:  1.95533503e-05, mean val. loss:  2.21216679e+00\n",
      "Epoch: 11104 mean train loss:  1.92916486e-05, mean val. loss:  2.21233010e+00\n",
      "Epoch: 11105 mean train loss:  1.95666507e-05, mean val. loss:  2.21247196e+00\n",
      "Epoch: 11106 mean train loss:  1.96229084e-05, mean val. loss:  2.21261096e+00\n",
      "Epoch: 11107 mean train loss:  1.95269822e-05, mean val. loss:  2.21276569e+00\n",
      "Epoch: 11108 mean train loss:  1.98261114e-05, mean val. loss:  2.21292686e+00\n",
      "Epoch: 11109 mean train loss:  1.92940643e-05, mean val. loss:  2.21310139e+00\n",
      "Epoch: 11110 mean train loss:  1.97548652e-05, mean val. loss:  2.21327615e+00\n",
      "Epoch: 11111 mean train loss:  1.98537891e-05, mean val. loss:  2.21341252e+00\n",
      "Epoch: 11112 mean train loss:  1.95997418e-05, mean val. loss:  2.21353602e+00\n",
      "Epoch: 11113 mean train loss:  1.96949113e-05, mean val. loss:  2.21365404e+00\n",
      "Epoch: 11114 mean train loss:  1.94821041e-05, mean val. loss:  2.21380854e+00\n",
      "Epoch: 11115 mean train loss:  1.92106527e-05, mean val. loss:  2.21397638e+00\n",
      "Epoch: 11116 mean train loss:  1.96858600e-05, mean val. loss:  2.21416259e+00\n",
      "Epoch: 11117 mean train loss:  1.99787319e-05, mean val. loss:  2.21433425e+00\n",
      "Epoch: 11118 mean train loss:  1.95152243e-05, mean val. loss:  2.21451068e+00\n",
      "Epoch: 11119 mean train loss:  1.98631606e-05, mean val. loss:  2.21470165e+00\n",
      "Epoch: 11120 mean train loss:  1.97594636e-05, mean val. loss:  2.21486855e+00\n",
      "Epoch: 11121 mean train loss:  1.95323664e-05, mean val. loss:  2.21504450e+00\n",
      "Epoch: 11122 mean train loss:  1.96948531e-05, mean val. loss:  2.21517968e+00\n",
      "Epoch: 11123 mean train loss:  1.93313463e-05, mean val. loss:  2.21530151e+00\n",
      "Epoch: 11124 mean train loss:  1.98272464e-05, mean val. loss:  2.21541262e+00\n",
      "Epoch: 11125 mean train loss:  1.98058260e-05, mean val. loss:  2.21547747e+00\n",
      "Epoch: 11126 mean train loss:  1.97165937e-05, mean val. loss:  2.21554995e+00\n",
      "Epoch: 11127 mean train loss:  1.94642926e-05, mean val. loss:  2.21562958e+00\n",
      "Epoch: 11128 mean train loss:  2.00024806e-05, mean val. loss:  2.21566200e+00\n",
      "Epoch: 11129 mean train loss:  1.95537577e-05, mean val. loss:  2.21565056e+00\n",
      "Epoch: 11130 mean train loss:  1.96449109e-05, mean val. loss:  2.21563840e+00\n",
      "Epoch: 11131 mean train loss:  1.98250345e-05, mean val. loss:  2.21564627e+00\n",
      "Epoch: 11132 mean train loss:  1.97630725e-05, mean val. loss:  2.21560740e+00\n",
      "Epoch: 11133 mean train loss:  1.97164482e-05, mean val. loss:  2.21555448e+00\n",
      "Epoch: 11134 mean train loss:  1.96756155e-05, mean val. loss:  2.21549892e+00\n",
      "Epoch: 11135 mean train loss:  1.99442729e-05, mean val. loss:  2.21543360e+00\n",
      "Epoch: 11136 mean train loss:  1.96167093e-05, mean val. loss:  2.21537232e+00\n",
      "Epoch: 11137 mean train loss:  1.94905151e-05, mean val. loss:  2.21530843e+00\n",
      "Epoch: 11138 mean train loss:  1.95488683e-05, mean val. loss:  2.21526933e+00\n",
      "Epoch: 11139 mean train loss:  1.96054170e-05, mean val. loss:  2.21524382e+00\n",
      "Epoch: 11140 mean train loss:  1.94992754e-05, mean val. loss:  2.21524310e+00\n",
      "Epoch: 11141 mean train loss:  1.97419722e-05, mean val. loss:  2.21525764e+00\n",
      "Epoch: 11142 mean train loss:  1.98254129e-05, mean val. loss:  2.21525645e+00\n",
      "Epoch: 11143 mean train loss:  2.00214563e-05, mean val. loss:  2.21527719e+00\n",
      "Epoch: 11144 mean train loss:  1.93834421e-05, mean val. loss:  2.21533728e+00\n",
      "Epoch: 11145 mean train loss:  1.98498019e-05, mean val. loss:  2.21542311e+00\n",
      "Epoch: 11146 mean train loss:  1.96580368e-05, mean val. loss:  2.21553993e+00\n",
      "Epoch: 11147 mean train loss:  1.96846377e-05, mean val. loss:  2.21569657e+00\n",
      "Epoch: 11148 mean train loss:  1.93636224e-05, mean val. loss:  2.21585417e+00\n",
      "Epoch: 11149 mean train loss:  1.94057066e-05, mean val. loss:  2.21603632e+00\n",
      "Epoch: 11150 mean train loss:  1.96789624e-05, mean val. loss:  2.21621823e+00\n",
      "Epoch: 11151 mean train loss:  1.95983448e-05, mean val. loss:  2.21641397e+00\n",
      "Epoch: 11152 mean train loss:  1.96645269e-05, mean val. loss:  2.21661448e+00\n",
      "Epoch: 11153 mean train loss:  1.94956956e-05, mean val. loss:  2.21681857e+00\n",
      "Epoch: 11154 mean train loss:  1.96176698e-05, mean val. loss:  2.21702290e+00\n",
      "Epoch: 11155 mean train loss:  1.95325701e-05, mean val. loss:  2.21722651e+00\n",
      "Epoch: 11156 mean train loss:  1.99942442e-05, mean val. loss:  2.21742177e+00\n",
      "Epoch: 11157 mean train loss:  1.95435714e-05, mean val. loss:  2.21763968e+00\n",
      "Epoch: 11158 mean train loss:  1.94536988e-05, mean val. loss:  2.21783876e+00\n",
      "Epoch: 11159 mean train loss:  1.98032649e-05, mean val. loss:  2.21802545e+00\n",
      "Epoch: 11160 mean train loss:  1.92698790e-05, mean val. loss:  2.21819997e+00\n",
      "Epoch: 11161 mean train loss:  1.98638882e-05, mean val. loss:  2.21833277e+00\n",
      "Epoch: 11162 mean train loss:  1.95769535e-05, mean val. loss:  2.21845889e+00\n",
      "Epoch: 11163 mean train loss:  1.97161862e-05, mean val. loss:  2.21859312e+00\n",
      "Epoch: 11164 mean train loss:  1.95132743e-05, mean val. loss:  2.21876836e+00\n",
      "Epoch: 11165 mean train loss:  1.98949710e-05, mean val. loss:  2.21894765e+00\n",
      "Epoch: 11166 mean train loss:  1.92152511e-05, mean val. loss:  2.21913886e+00\n",
      "Epoch: 11167 mean train loss:  1.97655172e-05, mean val. loss:  2.21933389e+00\n",
      "Epoch: 11168 mean train loss:  1.93918240e-05, mean val. loss:  2.21953416e+00\n",
      "Epoch: 11169 mean train loss:  1.96732581e-05, mean val. loss:  2.21974158e+00\n",
      "Epoch: 11170 mean train loss:  1.96984038e-05, mean val. loss:  2.21993661e+00\n",
      "Epoch: 11171 mean train loss:  1.95773609e-05, mean val. loss:  2.22014117e+00\n",
      "Epoch: 11172 mean train loss:  1.94928143e-05, mean val. loss:  2.22033238e+00\n",
      "Epoch: 11173 mean train loss:  1.96608598e-05, mean val. loss:  2.22054505e+00\n",
      "Epoch: 11174 mean train loss:  1.93500600e-05, mean val. loss:  2.22078753e+00\n",
      "Epoch: 11175 mean train loss:  1.95350440e-05, mean val. loss:  2.22101045e+00\n",
      "Epoch: 11176 mean train loss:  1.96791952e-05, mean val. loss:  2.22120380e+00\n",
      "Epoch: 11177 mean train loss:  1.95383327e-05, mean val. loss:  2.22140408e+00\n",
      "Epoch: 11178 mean train loss:  2.00821960e-05, mean val. loss:  2.22155094e+00\n",
      "Epoch: 11179 mean train loss:  1.93366141e-05, mean val. loss:  2.22168636e+00\n",
      "Epoch: 11180 mean train loss:  1.97071640e-05, mean val. loss:  2.22181582e+00\n",
      "Epoch: 11181 mean train loss:  1.95297471e-05, mean val. loss:  2.22193456e+00\n",
      "Epoch: 11182 mean train loss:  1.99024507e-05, mean val. loss:  2.22200322e+00\n",
      "Epoch: 11183 mean train loss:  1.97846966e-05, mean val. loss:  2.22207737e+00\n",
      "Epoch: 11184 mean train loss:  1.94488093e-05, mean val. loss:  2.22216940e+00\n",
      "Epoch: 11185 mean train loss:  1.96498586e-05, mean val. loss:  2.22227979e+00\n",
      "Epoch: 11186 mean train loss:  1.95300672e-05, mean val. loss:  2.22237730e+00\n",
      "Epoch: 11187 mean train loss:  1.96122855e-05, mean val. loss:  2.22247601e+00\n",
      "Epoch: 11188 mean train loss:  1.96957262e-05, mean val. loss:  2.22255683e+00\n",
      "Epoch: 11189 mean train loss:  1.98769558e-05, mean val. loss:  2.22259784e+00\n",
      "Epoch: 11190 mean train loss:  1.95316388e-05, mean val. loss:  2.22266150e+00\n",
      "Epoch: 11191 mean train loss:  1.93978776e-05, mean val. loss:  2.22272301e+00\n",
      "Epoch: 11192 mean train loss:  1.97968038e-05, mean val. loss:  2.22277594e+00\n",
      "Epoch: 11193 mean train loss:  1.97636837e-05, mean val. loss:  2.22283363e+00\n",
      "Epoch: 11194 mean train loss:  1.97335612e-05, mean val. loss:  2.22288799e+00\n",
      "Epoch: 11195 mean train loss:  1.90771243e-05, mean val. loss:  2.22296715e+00\n",
      "Epoch: 11196 mean train loss:  1.92733714e-05, mean val. loss:  2.22306824e+00\n",
      "Epoch: 11197 mean train loss:  1.92606240e-05, mean val. loss:  2.22322655e+00\n",
      "Epoch: 11198 mean train loss:  1.97660120e-05, mean val. loss:  2.22338676e+00\n",
      "Epoch: 11199 mean train loss:  1.90716819e-05, mean val. loss:  2.22358537e+00\n",
      "Epoch: 11200 mean train loss:  1.95518078e-05, mean val. loss:  2.22380161e+00\n",
      "Epoch: 11201 mean train loss:  1.98484049e-05, mean val. loss:  2.22400808e+00\n",
      "Epoch: 11202 mean train loss:  1.95867324e-05, mean val. loss:  2.22423983e+00\n",
      "Epoch: 11203 mean train loss:  1.97675254e-05, mean val. loss:  2.22450519e+00\n",
      "Epoch: 11204 mean train loss:  1.94809109e-05, mean val. loss:  2.22478557e+00\n",
      "Epoch: 11205 mean train loss:  1.97118206e-05, mean val. loss:  2.22506762e+00\n",
      "Epoch: 11206 mean train loss:  1.96018373e-05, mean val. loss:  2.22540569e+00\n",
      "Epoch: 11207 mean train loss:  1.94791937e-05, mean val. loss:  2.22574997e+00\n",
      "Epoch: 11208 mean train loss:  1.94848981e-05, mean val. loss:  2.22609520e+00\n",
      "Epoch: 11209 mean train loss:  1.96188048e-05, mean val. loss:  2.22640944e+00\n",
      "Epoch: 11210 mean train loss:  1.96755282e-05, mean val. loss:  2.22673249e+00\n",
      "Epoch: 11211 mean train loss:  1.99256756e-05, mean val. loss:  2.22704911e+00\n",
      "Epoch: 11212 mean train loss:  1.96665060e-05, mean val. loss:  2.22731376e+00\n",
      "Epoch: 11213 mean train loss:  1.98166235e-05, mean val. loss:  2.22750568e+00\n",
      "Epoch: 11214 mean train loss:  1.96763431e-05, mean val. loss:  2.22768641e+00\n",
      "Epoch: 11215 mean train loss:  1.95497996e-05, mean val. loss:  2.22781849e+00\n",
      "Epoch: 11216 mean train loss:  1.98900816e-05, mean val. loss:  2.22788620e+00\n",
      "Epoch: 11217 mean train loss:  1.99290807e-05, mean val. loss:  2.22790909e+00\n",
      "Epoch: 11218 mean train loss:  1.96367328e-05, mean val. loss:  2.22789288e+00\n",
      "Epoch: 11219 mean train loss:  1.94419117e-05, mean val. loss:  2.22782063e+00\n",
      "Epoch: 11220 mean train loss:  1.93087035e-05, mean val. loss:  2.22772694e+00\n",
      "Epoch: 11221 mean train loss:  1.96087640e-05, mean val. loss:  2.22762561e+00\n",
      "Epoch: 11222 mean train loss:  1.95243629e-05, mean val. loss:  2.22752929e+00\n",
      "Epoch: 11223 mean train loss:  1.93703745e-05, mean val. loss:  2.22743511e+00\n",
      "Epoch: 11224 mean train loss:  1.95726461e-05, mean val. loss:  2.22735405e+00\n",
      "Epoch: 11225 mean train loss:  1.96723850e-05, mean val. loss:  2.22731376e+00\n",
      "Epoch: 11226 mean train loss:  1.95280882e-05, mean val. loss:  2.22730494e+00\n",
      "Epoch: 11227 mean train loss:  1.93896121e-05, mean val. loss:  2.22732902e+00\n",
      "Epoch: 11228 mean train loss:  1.94563472e-05, mean val. loss:  2.22737408e+00\n",
      "Epoch: 11229 mean train loss:  2.00422073e-05, mean val. loss:  2.22744203e+00\n",
      "Epoch: 11230 mean train loss:  1.92947919e-05, mean val. loss:  2.22759795e+00\n",
      "Epoch: 11231 mean train loss:  1.98082998e-05, mean val. loss:  2.22778249e+00\n",
      "Epoch: 11232 mean train loss:  1.96529436e-05, mean val. loss:  2.22798753e+00\n",
      "Epoch: 11233 mean train loss:  1.97250338e-05, mean val. loss:  2.22819138e+00\n",
      "Epoch: 11234 mean train loss:  1.96835026e-05, mean val. loss:  2.22841167e+00\n",
      "Epoch: 11235 mean train loss:  1.96688052e-05, mean val. loss:  2.22861409e+00\n",
      "Epoch: 11236 mean train loss:  1.96789042e-05, mean val. loss:  2.22880268e+00\n",
      "Epoch: 11237 mean train loss:  1.93683954e-05, mean val. loss:  2.22901201e+00\n",
      "Epoch: 11238 mean train loss:  1.94518943e-05, mean val. loss:  2.22921515e+00\n",
      "Epoch: 11239 mean train loss:  1.94802124e-05, mean val. loss:  2.22943592e+00\n",
      "Epoch: 11240 mean train loss:  1.95901375e-05, mean val. loss:  2.22966361e+00\n",
      "Epoch: 11241 mean train loss:  1.95854809e-05, mean val. loss:  2.22990894e+00\n",
      "Epoch: 11242 mean train loss:  1.96595502e-05, mean val. loss:  2.23016143e+00\n",
      "Epoch: 11243 mean train loss:  1.98346970e-05, mean val. loss:  2.23036265e+00\n",
      "Epoch: 11244 mean train loss:  1.96319015e-05, mean val. loss:  2.23055482e+00\n",
      "Epoch: 11245 mean train loss:  1.96633919e-05, mean val. loss:  2.23074079e+00\n",
      "Epoch: 11246 mean train loss:  1.96820183e-05, mean val. loss:  2.23088288e+00\n",
      "Epoch: 11247 mean train loss:  1.95004395e-05, mean val. loss:  2.23099899e+00\n",
      "Epoch: 11248 mean train loss:  1.92683656e-05, mean val. loss:  2.23114157e+00\n",
      "Epoch: 11249 mean train loss:  1.93079468e-05, mean val. loss:  2.23125625e+00\n",
      "Epoch: 11250 mean train loss:  1.97945919e-05, mean val. loss:  2.23138165e+00\n",
      "Epoch: 11251 mean train loss:  1.97234622e-05, mean val. loss:  2.23148847e+00\n",
      "Epoch: 11252 mean train loss:  1.94316381e-05, mean val. loss:  2.23160601e+00\n",
      "Epoch: 11253 mean train loss:  1.95190078e-05, mean val. loss:  2.23172426e+00\n",
      "Epoch: 11254 mean train loss:  1.95537286e-05, mean val. loss:  2.23183084e+00\n",
      "Epoch: 11255 mean train loss:  1.94073655e-05, mean val. loss:  2.23196006e+00\n",
      "Epoch: 11256 mean train loss:  1.93973829e-05, mean val. loss:  2.23209834e+00\n",
      "Epoch: 11257 mean train loss:  1.92252919e-05, mean val. loss:  2.23230314e+00\n",
      "Epoch: 11258 mean train loss:  1.95574248e-05, mean val. loss:  2.23251915e+00\n",
      "Epoch: 11259 mean train loss:  1.93426386e-05, mean val. loss:  2.23276830e+00\n",
      "Epoch: 11260 mean train loss:  1.97837071e-05, mean val. loss:  2.23300838e+00\n",
      "Epoch: 11261 mean train loss:  1.97504123e-05, mean val. loss:  2.23322797e+00\n",
      "Epoch: 11262 mean train loss:  1.97117915e-05, mean val. loss:  2.23342538e+00\n",
      "Epoch: 11263 mean train loss:  1.90373976e-05, mean val. loss:  2.23367739e+00\n",
      "Epoch: 11264 mean train loss:  1.94817549e-05, mean val. loss:  2.23388100e+00\n",
      "Epoch: 11265 mean train loss:  1.96241308e-05, mean val. loss:  2.23407102e+00\n",
      "Epoch: 11266 mean train loss:  1.96933106e-05, mean val. loss:  2.23425293e+00\n",
      "Epoch: 11267 mean train loss:  1.95063767e-05, mean val. loss:  2.23441601e+00\n",
      "Epoch: 11268 mean train loss:  1.94704044e-05, mean val. loss:  2.23455548e+00\n",
      "Epoch: 11269 mean train loss:  1.97557092e-05, mean val. loss:  2.23463392e+00\n",
      "Epoch: 11270 mean train loss:  1.97574263e-05, mean val. loss:  2.23468709e+00\n",
      "Epoch: 11271 mean train loss:  1.92590815e-05, mean val. loss:  2.23470950e+00\n",
      "Epoch: 11272 mean train loss:  1.99651113e-05, mean val. loss:  2.23473120e+00\n",
      "Epoch: 11273 mean train loss:  1.95322209e-05, mean val. loss:  2.23474216e+00\n",
      "Epoch: 11274 mean train loss:  1.98484631e-05, mean val. loss:  2.23471451e+00\n",
      "Epoch: 11275 mean train loss:  1.96738692e-05, mean val. loss:  2.23464346e+00\n",
      "Epoch: 11276 mean train loss:  1.97819900e-05, mean val. loss:  2.23457146e+00\n",
      "Epoch: 11277 mean train loss:  1.96762558e-05, mean val. loss:  2.23446536e+00\n",
      "Epoch: 11278 mean train loss:  1.94044551e-05, mean val. loss:  2.23434758e+00\n",
      "Epoch: 11279 mean train loss:  1.94141467e-05, mean val. loss:  2.23422265e+00\n",
      "Epoch: 11280 mean train loss:  1.95342000e-05, mean val. loss:  2.23409820e+00\n",
      "Epoch: 11281 mean train loss:  1.96167675e-05, mean val. loss:  2.23398304e+00\n",
      "Epoch: 11282 mean train loss:  1.96311157e-05, mean val. loss:  2.23388219e+00\n",
      "Epoch: 11283 mean train loss:  1.94764580e-05, mean val. loss:  2.23380971e+00\n",
      "Epoch: 11284 mean train loss:  1.91626896e-05, mean val. loss:  2.23378015e+00\n",
      "Epoch: 11285 mean train loss:  1.92566949e-05, mean val. loss:  2.23379517e+00\n",
      "Epoch: 11286 mean train loss:  1.93550368e-05, mean val. loss:  2.23384237e+00\n",
      "Epoch: 11287 mean train loss:  1.94640015e-05, mean val. loss:  2.23393464e+00\n",
      "Epoch: 11288 mean train loss:  1.95208413e-05, mean val. loss:  2.23409414e+00\n",
      "Epoch: 11289 mean train loss:  1.95230532e-05, mean val. loss:  2.23432684e+00\n",
      "Epoch: 11290 mean train loss:  1.96793990e-05, mean val. loss:  2.23462510e+00\n",
      "Epoch: 11291 mean train loss:  1.93598389e-05, mean val. loss:  2.23501992e+00\n",
      "Epoch: 11292 mean train loss:  1.95666798e-05, mean val. loss:  2.23542738e+00\n",
      "Epoch: 11293 mean train loss:  1.96651381e-05, mean val. loss:  2.23584080e+00\n",
      "Epoch: 11294 mean train loss:  1.93032902e-05, mean val. loss:  2.23629403e+00\n",
      "Epoch: 11295 mean train loss:  1.92329753e-05, mean val. loss:  2.23676467e+00\n",
      "Epoch: 11296 mean train loss:  1.94919412e-05, mean val. loss:  2.23725319e+00\n",
      "Epoch: 11297 mean train loss:  1.95826287e-05, mean val. loss:  2.23772669e+00\n",
      "Epoch: 11298 mean train loss:  1.96508190e-05, mean val. loss:  2.23819757e+00\n",
      "Epoch: 11299 mean train loss:  1.94674649e-05, mean val. loss:  2.23866296e+00\n",
      "Epoch: 11300 mean train loss:  1.97762274e-05, mean val. loss:  2.23907375e+00\n",
      "Epoch: 11301 mean train loss:  1.96282926e-05, mean val. loss:  2.23944831e+00\n",
      "Epoch: 11302 mean train loss:  1.95512257e-05, mean val. loss:  2.23978806e+00\n",
      "Epoch: 11303 mean train loss:  1.95342291e-05, mean val. loss:  2.24006057e+00\n",
      "Epoch: 11304 mean train loss:  1.98466005e-05, mean val. loss:  2.24027586e+00\n",
      "Epoch: 11305 mean train loss:  1.93484011e-05, mean val. loss:  2.24046350e+00\n",
      "Epoch: 11306 mean train loss:  1.95004104e-05, mean val. loss:  2.24063540e+00\n",
      "Epoch: 11307 mean train loss:  1.92829757e-05, mean val. loss:  2.24075103e+00\n",
      "Epoch: 11308 mean train loss:  1.93613523e-05, mean val. loss:  2.24079800e+00\n",
      "Epoch: 11309 mean train loss:  1.97290210e-05, mean val. loss:  2.24077582e+00\n",
      "Epoch: 11310 mean train loss:  1.96371111e-05, mean val. loss:  2.24069238e+00\n",
      "Epoch: 11311 mean train loss:  1.94195309e-05, mean val. loss:  2.24060011e+00\n",
      "Epoch: 11312 mean train loss:  1.94726745e-05, mean val. loss:  2.24049044e+00\n",
      "Epoch: 11313 mean train loss:  1.94709573e-05, mean val. loss:  2.24036646e+00\n",
      "Epoch: 11314 mean train loss:  1.92585867e-05, mean val. loss:  2.24019551e+00\n",
      "Epoch: 11315 mean train loss:  1.99884817e-05, mean val. loss:  2.23999858e+00\n",
      "Epoch: 11316 mean train loss:  1.94767199e-05, mean val. loss:  2.23980594e+00\n",
      "Epoch: 11317 mean train loss:  1.96555920e-05, mean val. loss:  2.23964190e+00\n",
      "Epoch: 11318 mean train loss:  1.97304471e-05, mean val. loss:  2.23949170e+00\n",
      "Epoch: 11319 mean train loss:  1.96677574e-05, mean val. loss:  2.23938513e+00\n",
      "Epoch: 11320 mean train loss:  1.96818728e-05, mean val. loss:  2.23930144e+00\n",
      "Epoch: 11321 mean train loss:  1.96296023e-05, mean val. loss:  2.23923159e+00\n",
      "Epoch: 11322 mean train loss:  1.94385357e-05, mean val. loss:  2.23919487e+00\n",
      "Epoch: 11323 mean train loss:  1.93364103e-05, mean val. loss:  2.23919487e+00\n",
      "Epoch: 11324 mean train loss:  1.93843443e-05, mean val. loss:  2.23925567e+00\n",
      "Epoch: 11325 mean train loss:  1.97737245e-05, mean val. loss:  2.23932290e+00\n",
      "Epoch: 11326 mean train loss:  1.93024753e-05, mean val. loss:  2.23945856e+00\n",
      "Epoch: 11327 mean train loss:  1.95715402e-05, mean val. loss:  2.23964524e+00\n",
      "Epoch: 11328 mean train loss:  1.97570771e-05, mean val. loss:  2.23985004e+00\n",
      "Epoch: 11329 mean train loss:  1.98051566e-05, mean val. loss:  2.24010110e+00\n",
      "Epoch: 11330 mean train loss:  1.92596926e-05, mean val. loss:  2.24043632e+00\n",
      "Epoch: 11331 mean train loss:  1.97633635e-05, mean val. loss:  2.24078560e+00\n",
      "Epoch: 11332 mean train loss:  1.95899047e-05, mean val. loss:  2.24117398e+00\n",
      "Epoch: 11333 mean train loss:  1.94206077e-05, mean val. loss:  2.24157166e+00\n",
      "Epoch: 11334 mean train loss:  1.91998552e-05, mean val. loss:  2.24198127e+00\n",
      "Epoch: 11335 mean train loss:  1.97625777e-05, mean val. loss:  2.24239206e+00\n",
      "Epoch: 11336 mean train loss:  1.97316404e-05, mean val. loss:  2.24279571e+00\n",
      "Epoch: 11337 mean train loss:  1.96480250e-05, mean val. loss:  2.24317884e+00\n",
      "Epoch: 11338 mean train loss:  1.95532921e-05, mean val. loss:  2.24352598e+00\n",
      "Epoch: 11339 mean train loss:  1.95546891e-05, mean val. loss:  2.24383163e+00\n",
      "Epoch: 11340 mean train loss:  1.91348954e-05, mean val. loss:  2.24408841e+00\n",
      "Epoch: 11341 mean train loss:  1.94979075e-05, mean val. loss:  2.24433398e+00\n",
      "Epoch: 11342 mean train loss:  1.94051827e-05, mean val. loss:  2.24455070e+00\n",
      "Epoch: 11343 mean train loss:  1.92581210e-05, mean val. loss:  2.24474835e+00\n",
      "Epoch: 11344 mean train loss:  1.94823951e-05, mean val. loss:  2.24489975e+00\n",
      "Epoch: 11345 mean train loss:  1.97487825e-05, mean val. loss:  2.24501348e+00\n",
      "Epoch: 11346 mean train loss:  1.97326008e-05, mean val. loss:  2.24510026e+00\n",
      "Epoch: 11347 mean train loss:  1.96808542e-05, mean val. loss:  2.24515700e+00\n",
      "Epoch: 11348 mean train loss:  1.93080632e-05, mean val. loss:  2.24522495e+00\n",
      "Epoch: 11349 mean train loss:  1.95251487e-05, mean val. loss:  2.24527383e+00\n",
      "Epoch: 11350 mean train loss:  1.91922300e-05, mean val. loss:  2.24529886e+00\n",
      "Epoch: 11351 mean train loss:  1.96302426e-05, mean val. loss:  2.24533224e+00\n",
      "Epoch: 11352 mean train loss:  1.94725289e-05, mean val. loss:  2.24534941e+00\n",
      "Epoch: 11353 mean train loss:  1.93692977e-05, mean val. loss:  2.24537301e+00\n",
      "Epoch: 11354 mean train loss:  1.93772139e-05, mean val. loss:  2.24540091e+00\n",
      "Epoch: 11355 mean train loss:  1.95242756e-05, mean val. loss:  2.24545288e+00\n",
      "Epoch: 11356 mean train loss:  1.96406327e-05, mean val. loss:  2.24548936e+00\n",
      "Epoch: 11357 mean train loss:  1.96281180e-05, mean val. loss:  2.24554753e+00\n",
      "Epoch: 11358 mean train loss:  1.93646702e-05, mean val. loss:  2.24564290e+00\n",
      "Epoch: 11359 mean train loss:  1.93729647e-05, mean val. loss:  2.24576831e+00\n",
      "Epoch: 11360 mean train loss:  1.92934531e-05, mean val. loss:  2.24589968e+00\n",
      "Epoch: 11361 mean train loss:  1.97287009e-05, mean val. loss:  2.24604559e+00\n",
      "Epoch: 11362 mean train loss:  1.90087594e-05, mean val. loss:  2.24622178e+00\n",
      "Epoch: 11363 mean train loss:  1.94643799e-05, mean val. loss:  2.24641299e+00\n",
      "Epoch: 11364 mean train loss:  1.95936882e-05, mean val. loss:  2.24662638e+00\n",
      "Epoch: 11365 mean train loss:  1.93725573e-05, mean val. loss:  2.24686885e+00\n",
      "Epoch: 11366 mean train loss:  1.96013425e-05, mean val. loss:  2.24711943e+00\n",
      "Epoch: 11367 mean train loss:  1.96217443e-05, mean val. loss:  2.24737215e+00\n",
      "Epoch: 11368 mean train loss:  1.96214824e-05, mean val. loss:  2.24761248e+00\n",
      "Epoch: 11369 mean train loss:  1.93077140e-05, mean val. loss:  2.24785423e+00\n",
      "Epoch: 11370 mean train loss:  1.95917091e-05, mean val. loss:  2.24809909e+00\n",
      "Epoch: 11371 mean train loss:  1.98308844e-05, mean val. loss:  2.24834442e+00\n",
      "Epoch: 11372 mean train loss:  1.93465385e-05, mean val. loss:  2.24860716e+00\n",
      "Epoch: 11373 mean train loss:  1.95158937e-05, mean val. loss:  2.24889135e+00\n",
      "Epoch: 11374 mean train loss:  1.96033507e-05, mean val. loss:  2.24914980e+00\n",
      "Epoch: 11375 mean train loss:  1.95797766e-05, mean val. loss:  2.24938893e+00\n",
      "Epoch: 11376 mean train loss:  1.97319605e-05, mean val. loss:  2.24958968e+00\n",
      "Epoch: 11377 mean train loss:  1.90253195e-05, mean val. loss:  2.24977708e+00\n",
      "Epoch: 11378 mean train loss:  1.98299531e-05, mean val. loss:  2.24990845e+00\n",
      "Epoch: 11379 mean train loss:  1.92857697e-05, mean val. loss:  2.25004125e+00\n",
      "Epoch: 11380 mean train loss:  2.00494833e-05, mean val. loss:  2.25011206e+00\n",
      "Epoch: 11381 mean train loss:  1.96689798e-05, mean val. loss:  2.25016069e+00\n",
      "Epoch: 11382 mean train loss:  1.97708141e-05, mean val. loss:  2.25018597e+00\n",
      "Epoch: 11383 mean train loss:  1.94309105e-05, mean val. loss:  2.25018191e+00\n",
      "Epoch: 11384 mean train loss:  1.94825698e-05, mean val. loss:  2.25015330e+00\n",
      "Epoch: 11385 mean train loss:  1.96847832e-05, mean val. loss:  2.25011539e+00\n",
      "Epoch: 11386 mean train loss:  1.93883025e-05, mean val. loss:  2.25006938e+00\n",
      "Epoch: 11387 mean train loss:  1.93965971e-05, mean val. loss:  2.25000286e+00\n",
      "Epoch: 11388 mean train loss:  1.92972075e-05, mean val. loss:  2.24994636e+00\n",
      "Epoch: 11389 mean train loss:  2.00128998e-05, mean val. loss:  2.24986768e+00\n",
      "Epoch: 11390 mean train loss:  1.95493631e-05, mean val. loss:  2.24979758e+00\n",
      "Epoch: 11391 mean train loss:  1.96131878e-05, mean val. loss:  2.24971914e+00\n",
      "Epoch: 11392 mean train loss:  1.93371088e-05, mean val. loss:  2.24964428e+00\n",
      "Epoch: 11393 mean train loss:  1.95814064e-05, mean val. loss:  2.24959970e+00\n",
      "Epoch: 11394 mean train loss:  1.94873137e-05, mean val. loss:  2.24957681e+00\n",
      "Epoch: 11395 mean train loss:  1.97158661e-05, mean val. loss:  2.24955082e+00\n",
      "Epoch: 11396 mean train loss:  1.95989851e-05, mean val. loss:  2.24956179e+00\n",
      "Epoch: 11397 mean train loss:  1.92312291e-05, mean val. loss:  2.24959660e+00\n",
      "Epoch: 11398 mean train loss:  1.93646702e-05, mean val. loss:  2.24967623e+00\n",
      "Epoch: 11399 mean train loss:  1.98087073e-05, mean val. loss:  2.24978423e+00\n",
      "Epoch: 11400 mean train loss:  1.92266307e-05, mean val. loss:  2.24992657e+00\n",
      "Epoch: 11401 mean train loss:  1.95417088e-05, mean val. loss:  2.25008035e+00\n",
      "Epoch: 11402 mean train loss:  1.96328037e-05, mean val. loss:  2.25024700e+00\n",
      "Epoch: 11403 mean train loss:  1.94105378e-05, mean val. loss:  2.25044680e+00\n",
      "Epoch: 11404 mean train loss:  1.94137101e-05, mean val. loss:  2.25068736e+00\n",
      "Epoch: 11405 mean train loss:  1.94802706e-05, mean val. loss:  2.25095487e+00\n",
      "Epoch: 11406 mean train loss:  1.94036402e-05, mean val. loss:  2.25124502e+00\n",
      "Epoch: 11407 mean train loss:  1.93477899e-05, mean val. loss:  2.25155759e+00\n",
      "Epoch: 11408 mean train loss:  1.94988097e-05, mean val. loss:  2.25184011e+00\n",
      "Epoch: 11409 mean train loss:  1.97530899e-05, mean val. loss:  2.25211596e+00\n",
      "Epoch: 11410 mean train loss:  1.95777975e-05, mean val. loss:  2.25238228e+00\n",
      "Epoch: 11411 mean train loss:  1.95325119e-05, mean val. loss:  2.25262856e+00\n",
      "Epoch: 11412 mean train loss:  1.90866413e-05, mean val. loss:  2.25285959e+00\n",
      "Epoch: 11413 mean train loss:  1.93009037e-05, mean val. loss:  2.25307012e+00\n",
      "Epoch: 11414 mean train loss:  1.98453490e-05, mean val. loss:  2.25323772e+00\n",
      "Epoch: 11415 mean train loss:  1.96725887e-05, mean val. loss:  2.25339031e+00\n",
      "Epoch: 11416 mean train loss:  1.94914755e-05, mean val. loss:  2.25352526e+00\n",
      "Epoch: 11417 mean train loss:  1.95210159e-05, mean val. loss:  2.25362563e+00\n",
      "Epoch: 11418 mean train loss:  1.95280882e-05, mean val. loss:  2.25372648e+00\n",
      "Epoch: 11419 mean train loss:  1.94812892e-05, mean val. loss:  2.25381351e+00\n",
      "Epoch: 11420 mean train loss:  1.94677559e-05, mean val. loss:  2.25389719e+00\n",
      "Epoch: 11421 mean train loss:  1.89270650e-05, mean val. loss:  2.25397897e+00\n",
      "Epoch: 11422 mean train loss:  1.93054148e-05, mean val. loss:  2.25403404e+00\n",
      "Epoch: 11423 mean train loss:  1.91113504e-05, mean val. loss:  2.25408387e+00\n",
      "Epoch: 11424 mean train loss:  1.94666791e-05, mean val. loss:  2.25416541e+00\n",
      "Epoch: 11425 mean train loss:  1.93622545e-05, mean val. loss:  2.25427055e+00\n",
      "Epoch: 11426 mean train loss:  1.94088207e-05, mean val. loss:  2.25438499e+00\n",
      "Epoch: 11427 mean train loss:  1.94579770e-05, mean val. loss:  2.25452018e+00\n",
      "Epoch: 11428 mean train loss:  1.94847526e-05, mean val. loss:  2.25469565e+00\n",
      "Epoch: 11429 mean train loss:  1.96648180e-05, mean val. loss:  2.25489879e+00\n",
      "Epoch: 11430 mean train loss:  1.96451147e-05, mean val. loss:  2.25511479e+00\n",
      "Epoch: 11431 mean train loss:  1.95725879e-05, mean val. loss:  2.25539041e+00\n",
      "Epoch: 11432 mean train loss:  1.98262278e-05, mean val. loss:  2.25568438e+00\n",
      "Epoch: 11433 mean train loss:  1.92862644e-05, mean val. loss:  2.25598478e+00\n",
      "Epoch: 11434 mean train loss:  1.88670820e-05, mean val. loss:  2.25630021e+00\n",
      "Epoch: 11435 mean train loss:  1.94973545e-05, mean val. loss:  2.25658441e+00\n",
      "Epoch: 11436 mean train loss:  1.94004097e-05, mean val. loss:  2.25689197e+00\n",
      "Epoch: 11437 mean train loss:  1.95539615e-05, mean val. loss:  2.25718999e+00\n",
      "Epoch: 11438 mean train loss:  1.94647873e-05, mean val. loss:  2.25744390e+00\n",
      "Epoch: 11439 mean train loss:  1.91136787e-05, mean val. loss:  2.25766993e+00\n",
      "Epoch: 11440 mean train loss:  1.91605941e-05, mean val. loss:  2.25786352e+00\n",
      "Epoch: 11441 mean train loss:  1.93819578e-05, mean val. loss:  2.25804234e+00\n",
      "Epoch: 11442 mean train loss:  1.96846959e-05, mean val. loss:  2.25818515e+00\n",
      "Epoch: 11443 mean train loss:  1.91977306e-05, mean val. loss:  2.25833321e+00\n",
      "Epoch: 11444 mean train loss:  1.89496204e-05, mean val. loss:  2.25847030e+00\n",
      "Epoch: 11445 mean train loss:  1.89946732e-05, mean val. loss:  2.25862336e+00\n",
      "Epoch: 11446 mean train loss:  1.95920293e-05, mean val. loss:  2.25877881e+00\n",
      "Epoch: 11447 mean train loss:  1.95451139e-05, mean val. loss:  2.25895333e+00\n",
      "Epoch: 11448 mean train loss:  1.95849570e-05, mean val. loss:  2.25913119e+00\n",
      "Epoch: 11449 mean train loss:  1.92623120e-05, mean val. loss:  2.25933313e+00\n",
      "Epoch: 11450 mean train loss:  1.94217428e-05, mean val. loss:  2.25953674e+00\n",
      "Epoch: 11451 mean train loss:  1.97229965e-05, mean val. loss:  2.25970149e+00\n",
      "Epoch: 11452 mean train loss:  1.92995067e-05, mean val. loss:  2.25988126e+00\n",
      "Epoch: 11453 mean train loss:  1.92853331e-05, mean val. loss:  2.26006770e+00\n",
      "Epoch: 11454 mean train loss:  1.93783198e-05, mean val. loss:  2.26027489e+00\n",
      "Epoch: 11455 mean train loss:  1.95009343e-05, mean val. loss:  2.26047492e+00\n",
      "Epoch: 11456 mean train loss:  1.97737827e-05, mean val. loss:  2.26064181e+00\n",
      "Epoch: 11457 mean train loss:  1.95450848e-05, mean val. loss:  2.26081705e+00\n",
      "Epoch: 11458 mean train loss:  1.94981403e-05, mean val. loss:  2.26095557e+00\n",
      "Epoch: 11459 mean train loss:  1.93013111e-05, mean val. loss:  2.26107144e+00\n",
      "Epoch: 11460 mean train loss:  1.95832108e-05, mean val. loss:  2.26114154e+00\n",
      "Epoch: 11461 mean train loss:  1.92937441e-05, mean val. loss:  2.26118302e+00\n",
      "Epoch: 11462 mean train loss:  1.92775042e-05, mean val. loss:  2.26119113e+00\n",
      "Epoch: 11463 mean train loss:  1.93804153e-05, mean val. loss:  2.26119447e+00\n",
      "Epoch: 11464 mean train loss:  1.94713648e-05, mean val. loss:  2.26120710e+00\n",
      "Epoch: 11465 mean train loss:  1.90872233e-05, mean val. loss:  2.26122832e+00\n",
      "Epoch: 11466 mean train loss:  1.89244456e-05, mean val. loss:  2.26126838e+00\n",
      "Epoch: 11467 mean train loss:  1.93360902e-05, mean val. loss:  2.26132965e+00\n",
      "Epoch: 11468 mean train loss:  1.92062871e-05, mean val. loss:  2.26139212e+00\n",
      "Epoch: 11469 mean train loss:  1.94383319e-05, mean val. loss:  2.26147985e+00\n",
      "Epoch: 11470 mean train loss:  1.97289628e-05, mean val. loss:  2.26157331e+00\n",
      "Epoch: 11471 mean train loss:  1.96328911e-05, mean val. loss:  2.26166463e+00\n",
      "Epoch: 11472 mean train loss:  1.94009626e-05, mean val. loss:  2.26176143e+00\n",
      "Epoch: 11473 mean train loss:  1.95156608e-05, mean val. loss:  2.26189780e+00\n",
      "Epoch: 11474 mean train loss:  1.90625433e-05, mean val. loss:  2.26205754e+00\n",
      "Epoch: 11475 mean train loss:  1.94842287e-05, mean val. loss:  2.26221609e+00\n",
      "Epoch: 11476 mean train loss:  1.93837041e-05, mean val. loss:  2.26239085e+00\n",
      "Epoch: 11477 mean train loss:  1.94270397e-05, mean val. loss:  2.26255727e+00\n",
      "Epoch: 11478 mean train loss:  1.92070147e-05, mean val. loss:  2.26274133e+00\n",
      "Epoch: 11479 mean train loss:  1.91625732e-05, mean val. loss:  2.26295805e+00\n",
      "Epoch: 11480 mean train loss:  1.96815818e-05, mean val. loss:  2.26313591e+00\n",
      "Epoch: 11481 mean train loss:  1.93361193e-05, mean val. loss:  2.26331091e+00\n",
      "Epoch: 11482 mean train loss:  1.96754409e-05, mean val. loss:  2.26343322e+00\n",
      "Epoch: 11483 mean train loss:  1.94155728e-05, mean val. loss:  2.26354408e+00\n",
      "Epoch: 11484 mean train loss:  1.92442385e-05, mean val. loss:  2.26365423e+00\n",
      "Epoch: 11485 mean train loss:  1.94090535e-05, mean val. loss:  2.26376677e+00\n",
      "Epoch: 11486 mean train loss:  1.96959300e-05, mean val. loss:  2.26387835e+00\n",
      "Epoch: 11487 mean train loss:  1.94137683e-05, mean val. loss:  2.26399493e+00\n",
      "Epoch: 11488 mean train loss:  1.93420856e-05, mean val. loss:  2.26411676e+00\n",
      "Epoch: 11489 mean train loss:  1.90177525e-05, mean val. loss:  2.26425028e+00\n",
      "Epoch: 11490 mean train loss:  1.96127221e-05, mean val. loss:  2.26436543e+00\n",
      "Epoch: 11491 mean train loss:  1.94884196e-05, mean val. loss:  2.26447678e+00\n",
      "Epoch: 11492 mean train loss:  1.90799474e-05, mean val. loss:  2.26466203e+00\n",
      "Epoch: 11493 mean train loss:  1.94332970e-05, mean val. loss:  2.26482368e+00\n",
      "Epoch: 11494 mean train loss:  1.94947352e-05, mean val. loss:  2.26496673e+00\n",
      "Epoch: 11495 mean train loss:  1.93974993e-05, mean val. loss:  2.26513934e+00\n",
      "Epoch: 11496 mean train loss:  1.91408326e-05, mean val. loss:  2.26531744e+00\n",
      "Epoch: 11497 mean train loss:  1.89906568e-05, mean val. loss:  2.26550889e+00\n",
      "Epoch: 11498 mean train loss:  1.92802399e-05, mean val. loss:  2.26569819e+00\n",
      "Epoch: 11499 mean train loss:  1.88898121e-05, mean val. loss:  2.26587749e+00\n",
      "Epoch: 11500 mean train loss:  1.92042498e-05, mean val. loss:  2.26605511e+00\n",
      "Epoch: 11501 mean train loss:  1.95653120e-05, mean val. loss:  2.26622057e+00\n",
      "Epoch: 11502 mean train loss:  1.93766609e-05, mean val. loss:  2.26636147e+00\n",
      "Epoch: 11503 mean train loss:  1.91849249e-05, mean val. loss:  2.26652646e+00\n",
      "Epoch: 11504 mean train loss:  1.91851868e-05, mean val. loss:  2.26670408e+00\n",
      "Epoch: 11505 mean train loss:  1.92163279e-05, mean val. loss:  2.26685238e+00\n",
      "Epoch: 11506 mean train loss:  1.93860615e-05, mean val. loss:  2.26700568e+00\n",
      "Epoch: 11507 mean train loss:  1.95063767e-05, mean val. loss:  2.26714349e+00\n",
      "Epoch: 11508 mean train loss:  1.92002044e-05, mean val. loss:  2.26729298e+00\n",
      "Epoch: 11509 mean train loss:  1.92571315e-05, mean val. loss:  2.26743150e+00\n",
      "Epoch: 11510 mean train loss:  1.93274464e-05, mean val. loss:  2.26757932e+00\n",
      "Epoch: 11511 mean train loss:  1.93333835e-05, mean val. loss:  2.26770353e+00\n",
      "Epoch: 11512 mean train loss:  1.90856808e-05, mean val. loss:  2.26782513e+00\n",
      "Epoch: 11513 mean train loss:  1.93080341e-05, mean val. loss:  2.26792955e+00\n",
      "Epoch: 11514 mean train loss:  1.93713349e-05, mean val. loss:  2.26801205e+00\n",
      "Epoch: 11515 mean train loss:  1.93437736e-05, mean val. loss:  2.26807690e+00\n",
      "Epoch: 11516 mean train loss:  1.91553263e-05, mean val. loss:  2.26818609e+00\n",
      "Epoch: 11517 mean train loss:  1.92725274e-05, mean val. loss:  2.26828289e+00\n",
      "Epoch: 11518 mean train loss:  1.91056461e-05, mean val. loss:  2.26845169e+00\n",
      "Epoch: 11519 mean train loss:  1.94231106e-05, mean val. loss:  2.26861143e+00\n",
      "Epoch: 11520 mean train loss:  1.96566980e-05, mean val. loss:  2.26879811e+00\n",
      "Epoch: 11521 mean train loss:  1.93415617e-05, mean val. loss:  2.26900721e+00\n",
      "Epoch: 11522 mean train loss:  1.93684536e-05, mean val. loss:  2.26923847e+00\n",
      "Epoch: 11523 mean train loss:  1.88672275e-05, mean val. loss:  2.26950765e+00\n",
      "Epoch: 11524 mean train loss:  1.90610881e-05, mean val. loss:  2.26977801e+00\n",
      "Epoch: 11525 mean train loss:  1.91724394e-05, mean val. loss:  2.27006340e+00\n",
      "Epoch: 11526 mean train loss:  1.91911531e-05, mean val. loss:  2.27034140e+00\n",
      "Epoch: 11527 mean train loss:  1.90632709e-05, mean val. loss:  2.27063990e+00\n",
      "Epoch: 11528 mean train loss:  1.95991015e-05, mean val. loss:  2.27091002e+00\n",
      "Epoch: 11529 mean train loss:  1.93297165e-05, mean val. loss:  2.27120876e+00\n",
      "Epoch: 11530 mean train loss:  1.95148168e-05, mean val. loss:  2.27148414e+00\n",
      "Epoch: 11531 mean train loss:  1.93023006e-05, mean val. loss:  2.27177835e+00\n",
      "Epoch: 11532 mean train loss:  1.90915889e-05, mean val. loss:  2.27205420e+00\n",
      "Epoch: 11533 mean train loss:  1.93573942e-05, mean val. loss:  2.27230906e+00\n",
      "Epoch: 11534 mean train loss:  1.89565471e-05, mean val. loss:  2.27255058e+00\n",
      "Epoch: 11535 mean train loss:  1.93616434e-05, mean val. loss:  2.27275157e+00\n",
      "Epoch: 11536 mean train loss:  1.94066961e-05, mean val. loss:  2.27292156e+00\n",
      "Epoch: 11537 mean train loss:  1.96196197e-05, mean val. loss:  2.27305198e+00\n",
      "Epoch: 11538 mean train loss:  1.94758177e-05, mean val. loss:  2.27316833e+00\n",
      "Epoch: 11539 mean train loss:  1.88575359e-05, mean val. loss:  2.27327514e+00\n",
      "Epoch: 11540 mean train loss:  1.93063752e-05, mean val. loss:  2.27334380e+00\n",
      "Epoch: 11541 mean train loss:  1.95326866e-05, mean val. loss:  2.27339745e+00\n",
      "Epoch: 11542 mean train loss:  1.95293396e-05, mean val. loss:  2.27344537e+00\n",
      "Epoch: 11543 mean train loss:  1.90731080e-05, mean val. loss:  2.27346444e+00\n",
      "Epoch: 11544 mean train loss:  1.96338515e-05, mean val. loss:  2.27343130e+00\n",
      "Epoch: 11545 mean train loss:  1.92390289e-05, mean val. loss:  2.27339530e+00\n",
      "Epoch: 11546 mean train loss:  1.92146690e-05, mean val. loss:  2.27335095e+00\n",
      "Epoch: 11547 mean train loss:  1.94452587e-05, mean val. loss:  2.27328920e+00\n",
      "Epoch: 11548 mean train loss:  1.91063154e-05, mean val. loss:  2.27323937e+00\n",
      "Epoch: 11549 mean train loss:  1.92356238e-05, mean val. loss:  2.27320004e+00\n",
      "Epoch: 11550 mean train loss:  1.91546278e-05, mean val. loss:  2.27317715e+00\n",
      "Epoch: 11551 mean train loss:  1.94036693e-05, mean val. loss:  2.27316499e+00\n",
      "Epoch: 11552 mean train loss:  1.92066655e-05, mean val. loss:  2.27319503e+00\n",
      "Epoch: 11553 mean train loss:  1.95597822e-05, mean val. loss:  2.27323389e+00\n",
      "Epoch: 11554 mean train loss:  1.93035812e-05, mean val. loss:  2.27329469e+00\n",
      "Epoch: 11555 mean train loss:  1.91565778e-05, mean val. loss:  2.27338195e+00\n",
      "Epoch: 11556 mean train loss:  1.94537570e-05, mean val. loss:  2.27347493e+00\n",
      "Epoch: 11557 mean train loss:  1.96426990e-05, mean val. loss:  2.27358031e+00\n",
      "Epoch: 11558 mean train loss:  1.90697901e-05, mean val. loss:  2.27376938e+00\n",
      "Epoch: 11559 mean train loss:  1.91072468e-05, mean val. loss:  2.27399349e+00\n",
      "Epoch: 11560 mean train loss:  1.92795123e-05, mean val. loss:  2.27426171e+00\n",
      "Epoch: 11561 mean train loss:  1.98327180e-05, mean val. loss:  2.27453923e+00\n",
      "Epoch: 11562 mean train loss:  1.91137369e-05, mean val. loss:  2.27481127e+00\n",
      "Epoch: 11563 mean train loss:  1.93639717e-05, mean val. loss:  2.27509594e+00\n",
      "Epoch: 11564 mean train loss:  1.95719185e-05, mean val. loss:  2.27536964e+00\n",
      "Epoch: 11565 mean train loss:  1.91848085e-05, mean val. loss:  2.27564645e+00\n",
      "Epoch: 11566 mean train loss:  1.90679857e-05, mean val. loss:  2.27590966e+00\n",
      "Epoch: 11567 mean train loss:  1.95556204e-05, mean val. loss:  2.27613735e+00\n",
      "Epoch: 11568 mean train loss:  1.92610896e-05, mean val. loss:  2.27636218e+00\n",
      "Epoch: 11569 mean train loss:  1.91920553e-05, mean val. loss:  2.27655673e+00\n",
      "Epoch: 11570 mean train loss:  1.93504093e-05, mean val. loss:  2.27670455e+00\n",
      "Epoch: 11571 mean train loss:  1.92699954e-05, mean val. loss:  2.27682042e+00\n",
      "Epoch: 11572 mean train loss:  1.95107423e-05, mean val. loss:  2.27689958e+00\n",
      "Epoch: 11573 mean train loss:  1.92976440e-05, mean val. loss:  2.27694178e+00\n",
      "Epoch: 11574 mean train loss:  1.95967732e-05, mean val. loss:  2.27696466e+00\n",
      "Epoch: 11575 mean train loss:  1.92366133e-05, mean val. loss:  2.27697134e+00\n",
      "Epoch: 11576 mean train loss:  1.90359715e-05, mean val. loss:  2.27697206e+00\n",
      "Epoch: 11577 mean train loss:  1.91990985e-05, mean val. loss:  2.27696586e+00\n",
      "Epoch: 11578 mean train loss:  1.94518652e-05, mean val. loss:  2.27696228e+00\n",
      "Epoch: 11579 mean train loss:  1.95178727e-05, mean val. loss:  2.27693748e+00\n",
      "Epoch: 11580 mean train loss:  1.91420841e-05, mean val. loss:  2.27691031e+00\n",
      "Epoch: 11581 mean train loss:  1.92931329e-05, mean val. loss:  2.27687693e+00\n",
      "Epoch: 11582 mean train loss:  1.91681320e-05, mean val. loss:  2.27685952e+00\n",
      "Epoch: 11583 mean train loss:  1.91777945e-05, mean val. loss:  2.27687311e+00\n",
      "Epoch: 11584 mean train loss:  1.95636530e-05, mean val. loss:  2.27690601e+00\n",
      "Epoch: 11585 mean train loss:  1.93190936e-05, mean val. loss:  2.27695727e+00\n",
      "Epoch: 11586 mean train loss:  1.93222077e-05, mean val. loss:  2.27705669e+00\n",
      "Epoch: 11587 mean train loss:  1.93535234e-05, mean val. loss:  2.27718878e+00\n",
      "Epoch: 11588 mean train loss:  1.90215360e-05, mean val. loss:  2.27734423e+00\n",
      "Epoch: 11589 mean train loss:  1.93378946e-05, mean val. loss:  2.27754498e+00\n",
      "Epoch: 11590 mean train loss:  1.91689178e-05, mean val. loss:  2.27776408e+00\n",
      "Epoch: 11591 mean train loss:  1.93242449e-05, mean val. loss:  2.27803206e+00\n",
      "Epoch: 11592 mean train loss:  1.90785795e-05, mean val. loss:  2.27834153e+00\n",
      "Epoch: 11593 mean train loss:  1.88792183e-05, mean val. loss:  2.27871943e+00\n",
      "Epoch: 11594 mean train loss:  1.92378939e-05, mean val. loss:  2.27911043e+00\n",
      "Epoch: 11595 mean train loss:  1.94311433e-05, mean val. loss:  2.27950072e+00\n",
      "Epoch: 11596 mean train loss:  1.98468624e-05, mean val. loss:  2.27987099e+00\n",
      "Epoch: 11597 mean train loss:  1.93302112e-05, mean val. loss:  2.28022814e+00\n",
      "Epoch: 11598 mean train loss:  1.93349260e-05, mean val. loss:  2.28059196e+00\n",
      "Epoch: 11599 mean train loss:  1.93279993e-05, mean val. loss:  2.28094506e+00\n",
      "Epoch: 11600 mean train loss:  1.94488675e-05, mean val. loss:  2.28130269e+00\n",
      "Epoch: 11601 mean train loss:  1.89694983e-05, mean val. loss:  2.28163624e+00\n",
      "Epoch: 11602 mean train loss:  1.92493317e-05, mean val. loss:  2.28192759e+00\n",
      "Epoch: 11603 mean train loss:  1.92103907e-05, mean val. loss:  2.28219962e+00\n",
      "Epoch: 11604 mean train loss:  1.89577986e-05, mean val. loss:  2.28244233e+00\n",
      "Epoch: 11605 mean train loss:  1.90607097e-05, mean val. loss:  2.28263402e+00\n",
      "Epoch: 11606 mean train loss:  1.93026208e-05, mean val. loss:  2.28278399e+00\n",
      "Epoch: 11607 mean train loss:  1.94565800e-05, mean val. loss:  2.28290987e+00\n",
      "Epoch: 11608 mean train loss:  1.90342835e-05, mean val. loss:  2.28301120e+00\n",
      "Epoch: 11609 mean train loss:  1.96702022e-05, mean val. loss:  2.28308749e+00\n",
      "Epoch: 11610 mean train loss:  1.94657478e-05, mean val. loss:  2.28311872e+00\n",
      "Epoch: 11611 mean train loss:  1.92676380e-05, mean val. loss:  2.28312087e+00\n",
      "Epoch: 11612 mean train loss:  1.94059103e-05, mean val. loss:  2.28310347e+00\n",
      "Epoch: 11613 mean train loss:  1.96231122e-05, mean val. loss:  2.28304219e+00\n",
      "Epoch: 11614 mean train loss:  1.88843987e-05, mean val. loss:  2.28303266e+00\n",
      "Epoch: 11615 mean train loss:  1.92595762e-05, mean val. loss:  2.28300691e+00\n",
      "Epoch: 11616 mean train loss:  1.91813160e-05, mean val. loss:  2.28296852e+00\n",
      "Epoch: 11617 mean train loss:  1.90395222e-05, mean val. loss:  2.28293252e+00\n",
      "Epoch: 11618 mean train loss:  1.93992164e-05, mean val. loss:  2.28290296e+00\n",
      "Epoch: 11619 mean train loss:  1.94290769e-05, mean val. loss:  2.28286481e+00\n",
      "Epoch: 11620 mean train loss:  1.92892912e-05, mean val. loss:  2.28286147e+00\n",
      "Epoch: 11621 mean train loss:  1.93864689e-05, mean val. loss:  2.28288555e+00\n",
      "Epoch: 11622 mean train loss:  1.89862621e-05, mean val. loss:  2.28291321e+00\n",
      "Epoch: 11623 mean train loss:  1.93222368e-05, mean val. loss:  2.28294468e+00\n",
      "Epoch: 11624 mean train loss:  1.95365283e-05, mean val. loss:  2.28297758e+00\n",
      "Epoch: 11625 mean train loss:  1.87952246e-05, mean val. loss:  2.28304744e+00\n",
      "Epoch: 11626 mean train loss:  1.91939180e-05, mean val. loss:  2.28316379e+00\n",
      "Epoch: 11627 mean train loss:  1.91204890e-05, mean val. loss:  2.28328371e+00\n",
      "Epoch: 11628 mean train loss:  1.93417654e-05, mean val. loss:  2.28343487e+00\n",
      "Epoch: 11629 mean train loss:  1.93988672e-05, mean val. loss:  2.28358698e+00\n",
      "Epoch: 11630 mean train loss:  1.91477011e-05, mean val. loss:  2.28378630e+00\n",
      "Epoch: 11631 mean train loss:  1.90115243e-05, mean val. loss:  2.28402400e+00\n",
      "Epoch: 11632 mean train loss:  1.89504935e-05, mean val. loss:  2.28429604e+00\n",
      "Epoch: 11633 mean train loss:  1.94413587e-05, mean val. loss:  2.28457689e+00\n",
      "Epoch: 11634 mean train loss:  1.92867883e-05, mean val. loss:  2.28482509e+00\n",
      "Epoch: 11635 mean train loss:  1.88526756e-05, mean val. loss:  2.28511786e+00\n",
      "Epoch: 11636 mean train loss:  1.95573375e-05, mean val. loss:  2.28540754e+00\n",
      "Epoch: 11637 mean train loss:  1.92419393e-05, mean val. loss:  2.28565407e+00\n",
      "Epoch: 11638 mean train loss:  1.97822810e-05, mean val. loss:  2.28585386e+00\n",
      "Epoch: 11639 mean train loss:  1.94823951e-05, mean val. loss:  2.28604436e+00\n",
      "Epoch: 11640 mean train loss:  1.87900150e-05, mean val. loss:  2.28626943e+00\n",
      "Epoch: 11641 mean train loss:  1.92412117e-05, mean val. loss:  2.28651166e+00\n",
      "Epoch: 11642 mean train loss:  1.96725596e-05, mean val. loss:  2.28674889e+00\n",
      "Epoch: 11643 mean train loss:  1.90836727e-05, mean val. loss:  2.28697491e+00\n",
      "Epoch: 11644 mean train loss:  1.90948776e-05, mean val. loss:  2.28719211e+00\n",
      "Epoch: 11645 mean train loss:  1.90782011e-05, mean val. loss:  2.28743124e+00\n",
      "Epoch: 11646 mean train loss:  1.93264859e-05, mean val. loss:  2.28764319e+00\n",
      "Epoch: 11647 mean train loss:  1.94400491e-05, mean val. loss:  2.28780627e+00\n",
      "Epoch: 11648 mean train loss:  1.93989836e-05, mean val. loss:  2.28795719e+00\n",
      "Epoch: 11649 mean train loss:  1.89121056e-05, mean val. loss:  2.28808832e+00\n",
      "Epoch: 11650 mean train loss:  1.92424923e-05, mean val. loss:  2.28826237e+00\n",
      "Epoch: 11651 mean train loss:  1.92428997e-05, mean val. loss:  2.28838754e+00\n",
      "Epoch: 11652 mean train loss:  1.91227882e-05, mean val. loss:  2.28849149e+00\n",
      "Epoch: 11653 mean train loss:  1.94435997e-05, mean val. loss:  2.28859282e+00\n",
      "Epoch: 11654 mean train loss:  1.93525921e-05, mean val. loss:  2.28867364e+00\n",
      "Epoch: 11655 mean train loss:  1.90615247e-05, mean val. loss:  2.28875494e+00\n",
      "Epoch: 11656 mean train loss:  1.93183951e-05, mean val. loss:  2.28884864e+00\n",
      "Epoch: 11657 mean train loss:  1.91760191e-05, mean val. loss:  2.28892589e+00\n",
      "Epoch: 11658 mean train loss:  1.91218860e-05, mean val. loss:  2.28898668e+00\n",
      "Epoch: 11659 mean train loss:  1.92560256e-05, mean val. loss:  2.28903913e+00\n",
      "Epoch: 11660 mean train loss:  1.93687156e-05, mean val. loss:  2.28908992e+00\n",
      "Epoch: 11661 mean train loss:  1.91699364e-05, mean val. loss:  2.28916121e+00\n",
      "Epoch: 11662 mean train loss:  1.91981671e-05, mean val. loss:  2.28925490e+00\n",
      "Epoch: 11663 mean train loss:  1.92844018e-05, mean val. loss:  2.28934383e+00\n",
      "Epoch: 11664 mean train loss:  1.91841100e-05, mean val. loss:  2.28942060e+00\n",
      "Epoch: 11665 mean train loss:  1.92350126e-05, mean val. loss:  2.28951263e+00\n",
      "Epoch: 11666 mean train loss:  1.94195309e-05, mean val. loss:  2.28957891e+00\n",
      "Epoch: 11667 mean train loss:  1.90369319e-05, mean val. loss:  2.28962326e+00\n",
      "Epoch: 11668 mean train loss:  1.94624008e-05, mean val. loss:  2.28965688e+00\n",
      "Epoch: 11669 mean train loss:  1.90774444e-05, mean val. loss:  2.28972268e+00\n",
      "Epoch: 11670 mean train loss:  1.92605075e-05, mean val. loss:  2.28976679e+00\n",
      "Epoch: 11671 mean train loss:  1.91796280e-05, mean val. loss:  2.28985381e+00\n",
      "Epoch: 11672 mean train loss:  1.91098079e-05, mean val. loss:  2.28993845e+00\n",
      "Epoch: 11673 mean train loss:  1.91657164e-05, mean val. loss:  2.29006004e+00\n",
      "Epoch: 11674 mean train loss:  1.89880666e-05, mean val. loss:  2.29021144e+00\n",
      "Epoch: 11675 mean train loss:  1.93544838e-05, mean val. loss:  2.29035544e+00\n",
      "Epoch: 11676 mean train loss:  1.93914457e-05, mean val. loss:  2.29053712e+00\n",
      "Epoch: 11677 mean train loss:  1.90414139e-05, mean val. loss:  2.29075146e+00\n",
      "Epoch: 11678 mean train loss:  1.91175786e-05, mean val. loss:  2.29098463e+00\n",
      "Epoch: 11679 mean train loss:  1.94824242e-05, mean val. loss:  2.29126167e+00\n",
      "Epoch: 11680 mean train loss:  1.89399871e-05, mean val. loss:  2.29156399e+00\n",
      "Epoch: 11681 mean train loss:  1.93174928e-05, mean val. loss:  2.29187799e+00\n",
      "Epoch: 11682 mean train loss:  1.86144607e-05, mean val. loss:  2.29224253e+00\n",
      "Epoch: 11683 mean train loss:  1.91915606e-05, mean val. loss:  2.29260921e+00\n",
      "Epoch: 11684 mean train loss:  1.93748274e-05, mean val. loss:  2.29296470e+00\n",
      "Epoch: 11685 mean train loss:  1.92960142e-05, mean val. loss:  2.29331756e+00\n",
      "Epoch: 11686 mean train loss:  1.91249419e-05, mean val. loss:  2.29368258e+00\n",
      "Epoch: 11687 mean train loss:  1.92781736e-05, mean val. loss:  2.29400849e+00\n",
      "Epoch: 11688 mean train loss:  1.93296873e-05, mean val. loss:  2.29431462e+00\n",
      "Epoch: 11689 mean train loss:  1.92439184e-05, mean val. loss:  2.29461026e+00\n",
      "Epoch: 11690 mean train loss:  1.92245934e-05, mean val. loss:  2.29490066e+00\n",
      "Epoch: 11691 mean train loss:  1.93253218e-05, mean val. loss:  2.29516912e+00\n",
      "Epoch: 11692 mean train loss:  1.91555591e-05, mean val. loss:  2.29545689e+00\n",
      "Epoch: 11693 mean train loss:  1.88940321e-05, mean val. loss:  2.29574990e+00\n",
      "Epoch: 11694 mean train loss:  1.95678440e-05, mean val. loss:  2.29598093e+00\n",
      "Epoch: 11695 mean train loss:  1.94587046e-05, mean val. loss:  2.29617357e+00\n",
      "Epoch: 11696 mean train loss:  1.91865547e-05, mean val. loss:  2.29637456e+00\n",
      "Epoch: 11697 mean train loss:  1.88257836e-05, mean val. loss:  2.29656839e+00\n",
      "Epoch: 11698 mean train loss:  1.88866397e-05, mean val. loss:  2.29675817e+00\n",
      "Epoch: 11699 mean train loss:  1.93373708e-05, mean val. loss:  2.29690075e+00\n",
      "Epoch: 11700 mean train loss:  1.91114086e-05, mean val. loss:  2.29706240e+00\n",
      "Epoch: 11701 mean train loss:  1.94033200e-05, mean val. loss:  2.29721165e+00\n",
      "Epoch: 11702 mean train loss:  1.89350976e-05, mean val. loss:  2.29735970e+00\n",
      "Epoch: 11703 mean train loss:  1.90086721e-05, mean val. loss:  2.29750848e+00\n",
      "Epoch: 11704 mean train loss:  1.89549464e-05, mean val. loss:  2.29767394e+00\n",
      "Epoch: 11705 mean train loss:  1.94309978e-05, mean val. loss:  2.29781651e+00\n",
      "Epoch: 11706 mean train loss:  1.93405722e-05, mean val. loss:  2.29794621e+00\n",
      "Epoch: 11707 mean train loss:  1.88503473e-05, mean val. loss:  2.29807281e+00\n",
      "Epoch: 11708 mean train loss:  1.91970030e-05, mean val. loss:  2.29817533e+00\n",
      "Epoch: 11709 mean train loss:  1.91993022e-05, mean val. loss:  2.29826474e+00\n",
      "Epoch: 11710 mean train loss:  1.93174346e-05, mean val. loss:  2.29834771e+00\n",
      "Epoch: 11711 mean train loss:  1.88894337e-05, mean val. loss:  2.29844379e+00\n",
      "Epoch: 11712 mean train loss:  1.91527361e-05, mean val. loss:  2.29850960e+00\n",
      "Epoch: 11713 mean train loss:  1.91789295e-05, mean val. loss:  2.29858828e+00\n",
      "Epoch: 11714 mean train loss:  1.91743311e-05, mean val. loss:  2.29866147e+00\n",
      "Epoch: 11715 mean train loss:  1.93579763e-05, mean val. loss:  2.29870319e+00\n",
      "Epoch: 11716 mean train loss:  1.91179861e-05, mean val. loss:  2.29872513e+00\n",
      "Epoch: 11717 mean train loss:  1.91160943e-05, mean val. loss:  2.29872704e+00\n",
      "Epoch: 11718 mean train loss:  1.93149026e-05, mean val. loss:  2.29875040e+00\n",
      "Epoch: 11719 mean train loss:  1.89731654e-05, mean val. loss:  2.29875684e+00\n",
      "Epoch: 11720 mean train loss:  1.89504644e-05, mean val. loss:  2.29875445e+00\n",
      "Epoch: 11721 mean train loss:  1.95885659e-05, mean val. loss:  2.29875731e+00\n",
      "Epoch: 11722 mean train loss:  1.89160928e-05, mean val. loss:  2.29877663e+00\n",
      "Epoch: 11723 mean train loss:  1.91338768e-05, mean val. loss:  2.29879642e+00\n",
      "Epoch: 11724 mean train loss:  1.90344872e-05, mean val. loss:  2.29883909e+00\n",
      "Epoch: 11725 mean train loss:  1.96417095e-05, mean val. loss:  2.29888916e+00\n",
      "Epoch: 11726 mean train loss:  1.90575956e-05, mean val. loss:  2.29897261e+00\n",
      "Epoch: 11727 mean train loss:  1.90976425e-05, mean val. loss:  2.29908490e+00\n",
      "Epoch: 11728 mean train loss:  1.93970627e-05, mean val. loss:  2.29920936e+00\n",
      "Epoch: 11729 mean train loss:  1.93149899e-05, mean val. loss:  2.29935360e+00\n",
      "Epoch: 11730 mean train loss:  1.95877801e-05, mean val. loss:  2.29949260e+00\n",
      "Epoch: 11731 mean train loss:  1.88245322e-05, mean val. loss:  2.29965448e+00\n",
      "Epoch: 11732 mean train loss:  1.91873405e-05, mean val. loss:  2.29983735e+00\n",
      "Epoch: 11733 mean train loss:  1.89987768e-05, mean val. loss:  2.30003452e+00\n",
      "Epoch: 11734 mean train loss:  1.94481399e-05, mean val. loss:  2.30022693e+00\n",
      "Epoch: 11735 mean train loss:  1.92201405e-05, mean val. loss:  2.30043006e+00\n",
      "Epoch: 11736 mean train loss:  1.93887972e-05, mean val. loss:  2.30060339e+00\n",
      "Epoch: 11737 mean train loss:  1.90732826e-05, mean val. loss:  2.30075216e+00\n",
      "Epoch: 11738 mean train loss:  1.90408900e-05, mean val. loss:  2.30093193e+00\n",
      "Epoch: 11739 mean train loss:  1.87636469e-05, mean val. loss:  2.30113196e+00\n",
      "Epoch: 11740 mean train loss:  1.94247405e-05, mean val. loss:  2.30128527e+00\n",
      "Epoch: 11741 mean train loss:  1.92802690e-05, mean val. loss:  2.30144858e+00\n",
      "Epoch: 11742 mean train loss:  1.92694715e-05, mean val. loss:  2.30161762e+00\n",
      "Epoch: 11743 mean train loss:  1.89035200e-05, mean val. loss:  2.30181170e+00\n",
      "Epoch: 11744 mean train loss:  1.88637932e-05, mean val. loss:  2.30200434e+00\n",
      "Epoch: 11745 mean train loss:  1.91658910e-05, mean val. loss:  2.30222535e+00\n",
      "Epoch: 11746 mean train loss:  1.92960724e-05, mean val. loss:  2.30244923e+00\n",
      "Epoch: 11747 mean train loss:  1.88168779e-05, mean val. loss:  2.30270362e+00\n",
      "Epoch: 11748 mean train loss:  1.87091355e-05, mean val. loss:  2.30300164e+00\n",
      "Epoch: 11749 mean train loss:  1.92363514e-05, mean val. loss:  2.30330181e+00\n",
      "Epoch: 11750 mean train loss:  1.90955470e-05, mean val. loss:  2.30360556e+00\n",
      "Epoch: 11751 mean train loss:  1.93804153e-05, mean val. loss:  2.30387521e+00\n",
      "Epoch: 11752 mean train loss:  1.92341686e-05, mean val. loss:  2.30415845e+00\n",
      "Epoch: 11753 mean train loss:  1.90863502e-05, mean val. loss:  2.30443096e+00\n",
      "Epoch: 11754 mean train loss:  1.92459556e-05, mean val. loss:  2.30471253e+00\n",
      "Epoch: 11755 mean train loss:  1.90970313e-05, mean val. loss:  2.30497313e+00\n",
      "Epoch: 11756 mean train loss:  1.88953418e-05, mean val. loss:  2.30524278e+00\n",
      "Epoch: 11757 mean train loss:  1.90633582e-05, mean val. loss:  2.30549860e+00\n",
      "Epoch: 11758 mean train loss:  1.90254650e-05, mean val. loss:  2.30574155e+00\n",
      "Epoch: 11759 mean train loss:  1.91716244e-05, mean val. loss:  2.30593944e+00\n",
      "Epoch: 11760 mean train loss:  1.89257553e-05, mean val. loss:  2.30611944e+00\n",
      "Epoch: 11761 mean train loss:  1.89175189e-05, mean val. loss:  2.30629325e+00\n",
      "Epoch: 11762 mean train loss:  1.92358275e-05, mean val. loss:  2.30643034e+00\n",
      "Epoch: 11763 mean train loss:  1.91988074e-05, mean val. loss:  2.30657172e+00\n",
      "Epoch: 11764 mean train loss:  1.95755274e-05, mean val. loss:  2.30667925e+00\n",
      "Epoch: 11765 mean train loss:  1.91971485e-05, mean val. loss:  2.30679584e+00\n",
      "Epoch: 11766 mean train loss:  1.91739528e-05, mean val. loss:  2.30690813e+00\n",
      "Epoch: 11767 mean train loss:  1.91356521e-05, mean val. loss:  2.30700898e+00\n",
      "Epoch: 11768 mean train loss:  1.93919986e-05, mean val. loss:  2.30708838e+00\n",
      "Epoch: 11769 mean train loss:  1.90056162e-05, mean val. loss:  2.30718946e+00\n",
      "Epoch: 11770 mean train loss:  1.90223800e-05, mean val. loss:  2.30729342e+00\n",
      "Epoch: 11771 mean train loss:  1.91756699e-05, mean val. loss:  2.30737376e+00\n",
      "Epoch: 11772 mean train loss:  1.88678096e-05, mean val. loss:  2.30746913e+00\n",
      "Epoch: 11773 mean train loss:  1.90931023e-05, mean val. loss:  2.30754018e+00\n",
      "Epoch: 11774 mean train loss:  1.90174324e-05, mean val. loss:  2.30758786e+00\n",
      "Epoch: 11775 mean train loss:  1.87936530e-05, mean val. loss:  2.30765629e+00\n",
      "Epoch: 11776 mean train loss:  1.91578874e-05, mean val. loss:  2.30774856e+00\n",
      "Epoch: 11777 mean train loss:  1.91744475e-05, mean val. loss:  2.30781651e+00\n",
      "Epoch: 11778 mean train loss:  1.91287254e-05, mean val. loss:  2.30787683e+00\n",
      "Epoch: 11779 mean train loss:  1.91987201e-05, mean val. loss:  2.30793262e+00\n",
      "Epoch: 11780 mean train loss:  1.89038983e-05, mean val. loss:  2.30802917e+00\n",
      "Epoch: 11781 mean train loss:  1.90313731e-05, mean val. loss:  2.30812669e+00\n",
      "Epoch: 11782 mean train loss:  1.90872233e-05, mean val. loss:  2.30823922e+00\n",
      "Epoch: 11783 mean train loss:  1.90432183e-05, mean val. loss:  2.30837560e+00\n",
      "Epoch: 11784 mean train loss:  1.93708111e-05, mean val. loss:  2.30850697e+00\n",
      "Epoch: 11785 mean train loss:  1.92835287e-05, mean val. loss:  2.30867720e+00\n",
      "Epoch: 11786 mean train loss:  1.88469421e-05, mean val. loss:  2.30886364e+00\n",
      "Epoch: 11787 mean train loss:  1.88306731e-05, mean val. loss:  2.30906868e+00\n",
      "Epoch: 11788 mean train loss:  1.94319873e-05, mean val. loss:  2.30926824e+00\n",
      "Epoch: 11789 mean train loss:  1.91333529e-05, mean val. loss:  2.30950618e+00\n",
      "Epoch: 11790 mean train loss:  1.86567195e-05, mean val. loss:  2.30974078e+00\n",
      "Epoch: 11791 mean train loss:  1.90079154e-05, mean val. loss:  2.30997467e+00\n",
      "Epoch: 11792 mean train loss:  1.90201390e-05, mean val. loss:  2.31018257e+00\n",
      "Epoch: 11793 mean train loss:  1.90722349e-05, mean val. loss:  2.31041384e+00\n",
      "Epoch: 11794 mean train loss:  1.89038983e-05, mean val. loss:  2.31063437e+00\n",
      "Epoch: 11795 mean train loss:  1.91294530e-05, mean val. loss:  2.31082964e+00\n",
      "Epoch: 11796 mean train loss:  1.90541032e-05, mean val. loss:  2.31100106e+00\n",
      "Epoch: 11797 mean train loss:  1.91814033e-05, mean val. loss:  2.31116104e+00\n",
      "Epoch: 11798 mean train loss:  1.91451982e-05, mean val. loss:  2.31131387e+00\n",
      "Epoch: 11799 mean train loss:  1.90267456e-05, mean val. loss:  2.31147909e+00\n",
      "Epoch: 11800 mean train loss:  1.91720610e-05, mean val. loss:  2.31165981e+00\n",
      "Epoch: 11801 mean train loss:  1.93787855e-05, mean val. loss:  2.31185269e+00\n",
      "Epoch: 11802 mean train loss:  1.94380118e-05, mean val. loss:  2.31203508e+00\n",
      "Epoch: 11803 mean train loss:  1.90031424e-05, mean val. loss:  2.31224442e+00\n",
      "Epoch: 11804 mean train loss:  1.89017446e-05, mean val. loss:  2.31243968e+00\n",
      "Epoch: 11805 mean train loss:  1.89992134e-05, mean val. loss:  2.31264544e+00\n",
      "Epoch: 11806 mean train loss:  1.89398415e-05, mean val. loss:  2.31285930e+00\n",
      "Epoch: 11807 mean train loss:  1.89426646e-05, mean val. loss:  2.31307960e+00\n",
      "Epoch: 11808 mean train loss:  1.91166764e-05, mean val. loss:  2.31330776e+00\n",
      "Epoch: 11809 mean train loss:  1.89210114e-05, mean val. loss:  2.31351590e+00\n",
      "Epoch: 11810 mean train loss:  1.92051521e-05, mean val. loss:  2.31366301e+00\n",
      "Epoch: 11811 mean train loss:  1.90497667e-05, mean val. loss:  2.31381440e+00\n",
      "Epoch: 11812 mean train loss:  1.92082080e-05, mean val. loss:  2.31394053e+00\n",
      "Epoch: 11813 mean train loss:  1.88569538e-05, mean val. loss:  2.31403852e+00\n",
      "Epoch: 11814 mean train loss:  1.91035215e-05, mean val. loss:  2.31411529e+00\n",
      "Epoch: 11815 mean train loss:  1.89689454e-05, mean val. loss:  2.31417966e+00\n",
      "Epoch: 11816 mean train loss:  1.93367887e-05, mean val. loss:  2.31424785e+00\n",
      "Epoch: 11817 mean train loss:  1.88280537e-05, mean val. loss:  2.31434083e+00\n",
      "Epoch: 11818 mean train loss:  1.91486033e-05, mean val. loss:  2.31441855e+00\n",
      "Epoch: 11819 mean train loss:  1.90468272e-05, mean val. loss:  2.31452489e+00\n",
      "Epoch: 11820 mean train loss:  1.92686566e-05, mean val. loss:  2.31464529e+00\n",
      "Epoch: 11821 mean train loss:  1.91270083e-05, mean val. loss:  2.31477332e+00\n",
      "Epoch: 11822 mean train loss:  1.92274165e-05, mean val. loss:  2.31488657e+00\n",
      "Epoch: 11823 mean train loss:  1.90030260e-05, mean val. loss:  2.31502533e+00\n",
      "Epoch: 11824 mean train loss:  1.93521555e-05, mean val. loss:  2.31516600e+00\n",
      "Epoch: 11825 mean train loss:  1.89540442e-05, mean val. loss:  2.31532526e+00\n",
      "Epoch: 11826 mean train loss:  1.89115235e-05, mean val. loss:  2.31550217e+00\n",
      "Epoch: 11827 mean train loss:  1.91097497e-05, mean val. loss:  2.31572843e+00\n",
      "Epoch: 11828 mean train loss:  1.88659178e-05, mean val. loss:  2.31596518e+00\n",
      "Epoch: 11829 mean train loss:  1.93831220e-05, mean val. loss:  2.31617570e+00\n",
      "Epoch: 11830 mean train loss:  1.92236039e-05, mean val. loss:  2.31638885e+00\n",
      "Epoch: 11831 mean train loss:  1.92114967e-05, mean val. loss:  2.31662488e+00\n",
      "Epoch: 11832 mean train loss:  1.89068378e-05, mean val. loss:  2.31685281e+00\n",
      "Epoch: 11833 mean train loss:  1.91626314e-05, mean val. loss:  2.31707096e+00\n",
      "Epoch: 11834 mean train loss:  1.88889098e-05, mean val. loss:  2.31731510e+00\n",
      "Epoch: 11835 mean train loss:  1.86912075e-05, mean val. loss:  2.31756806e+00\n",
      "Epoch: 11836 mean train loss:  1.90698775e-05, mean val. loss:  2.31782508e+00\n",
      "Epoch: 11837 mean train loss:  1.91261643e-05, mean val. loss:  2.31804800e+00\n",
      "Epoch: 11838 mean train loss:  1.91398140e-05, mean val. loss:  2.31826830e+00\n",
      "Epoch: 11839 mean train loss:  1.92861480e-05, mean val. loss:  2.31846595e+00\n",
      "Epoch: 11840 mean train loss:  1.92117004e-05, mean val. loss:  2.31867123e+00\n",
      "Epoch: 11841 mean train loss:  1.90736318e-05, mean val. loss:  2.31884909e+00\n",
      "Epoch: 11842 mean train loss:  1.90218561e-05, mean val. loss:  2.31899190e+00\n",
      "Epoch: 11843 mean train loss:  1.92823354e-05, mean val. loss:  2.31913185e+00\n",
      "Epoch: 11844 mean train loss:  1.92841981e-05, mean val. loss:  2.31926155e+00\n",
      "Epoch: 11845 mean train loss:  1.87191181e-05, mean val. loss:  2.31937933e+00\n",
      "Epoch: 11846 mean train loss:  1.93443848e-05, mean val. loss:  2.31947756e+00\n",
      "Epoch: 11847 mean train loss:  1.89382117e-05, mean val. loss:  2.31955266e+00\n",
      "Epoch: 11848 mean train loss:  1.91641448e-05, mean val. loss:  2.31957459e+00\n",
      "Epoch: 11849 mean train loss:  1.92392326e-05, mean val. loss:  2.31960082e+00\n",
      "Epoch: 11850 mean train loss:  1.88025879e-05, mean val. loss:  2.31961584e+00\n",
      "Epoch: 11851 mean train loss:  1.90544815e-05, mean val. loss:  2.31962252e+00\n",
      "Epoch: 11852 mean train loss:  1.89425482e-05, mean val. loss:  2.31966114e+00\n",
      "Epoch: 11853 mean train loss:  1.90680730e-05, mean val. loss:  2.31968307e+00\n",
      "Epoch: 11854 mean train loss:  1.90161227e-05, mean val. loss:  2.31970310e+00\n",
      "Epoch: 11855 mean train loss:  1.91074505e-05, mean val. loss:  2.31972933e+00\n",
      "Epoch: 11856 mean train loss:  1.88153062e-05, mean val. loss:  2.31976175e+00\n",
      "Epoch: 11857 mean train loss:  1.91920844e-05, mean val. loss:  2.31981254e+00\n",
      "Epoch: 11858 mean train loss:  1.90862629e-05, mean val. loss:  2.31985688e+00\n",
      "Epoch: 11859 mean train loss:  1.88634440e-05, mean val. loss:  2.31994796e+00\n",
      "Epoch: 11860 mean train loss:  1.88802078e-05, mean val. loss:  2.32006288e+00\n",
      "Epoch: 11861 mean train loss:  1.90931023e-05, mean val. loss:  2.32023287e+00\n",
      "Epoch: 11862 mean train loss:  1.91620784e-05, mean val. loss:  2.32042646e+00\n",
      "Epoch: 11863 mean train loss:  1.93939195e-05, mean val. loss:  2.32066035e+00\n",
      "Epoch: 11864 mean train loss:  1.87598635e-05, mean val. loss:  2.32091379e+00\n",
      "Epoch: 11865 mean train loss:  1.87841069e-05, mean val. loss:  2.32118225e+00\n",
      "Epoch: 11866 mean train loss:  1.94054737e-05, mean val. loss:  2.32144690e+00\n",
      "Epoch: 11867 mean train loss:  1.90570427e-05, mean val. loss:  2.32170987e+00\n",
      "Epoch: 11868 mean train loss:  1.89853599e-05, mean val. loss:  2.32199907e+00\n",
      "Epoch: 11869 mean train loss:  1.88771810e-05, mean val. loss:  2.32232308e+00\n",
      "Epoch: 11870 mean train loss:  1.91573054e-05, mean val. loss:  2.32259655e+00\n",
      "Epoch: 11871 mean train loss:  1.87817495e-05, mean val. loss:  2.32284784e+00\n",
      "Epoch: 11872 mean train loss:  1.91964209e-05, mean val. loss:  2.32307529e+00\n",
      "Epoch: 11873 mean train loss:  1.91888539e-05, mean val. loss:  2.32324839e+00\n",
      "Epoch: 11874 mean train loss:  1.89105631e-05, mean val. loss:  2.32336926e+00\n",
      "Epoch: 11875 mean train loss:  1.89705170e-05, mean val. loss:  2.32345748e+00\n",
      "Epoch: 11876 mean train loss:  1.90880382e-05, mean val. loss:  2.32351112e+00\n",
      "Epoch: 11877 mean train loss:  1.89905404e-05, mean val. loss:  2.32354498e+00\n",
      "Epoch: 11878 mean train loss:  1.91307336e-05, mean val. loss:  2.32354093e+00\n",
      "Epoch: 11879 mean train loss:  1.87905971e-05, mean val. loss:  2.32356143e+00\n",
      "Epoch: 11880 mean train loss:  1.92445877e-05, mean val. loss:  2.32358551e+00\n",
      "Epoch: 11881 mean train loss:  1.91321597e-05, mean val. loss:  2.32359958e+00\n",
      "Epoch: 11882 mean train loss:  1.86304678e-05, mean val. loss:  2.32366610e+00\n",
      "Epoch: 11883 mean train loss:  1.89687998e-05, mean val. loss:  2.32376480e+00\n",
      "Epoch: 11884 mean train loss:  1.94169115e-05, mean val. loss:  2.32386518e+00\n",
      "Epoch: 11885 mean train loss:  1.90967403e-05, mean val. loss:  2.32400036e+00\n",
      "Epoch: 11886 mean train loss:  1.91176659e-05, mean val. loss:  2.32414460e+00\n",
      "Epoch: 11887 mean train loss:  1.88813719e-05, mean val. loss:  2.32430720e+00\n",
      "Epoch: 11888 mean train loss:  1.88867562e-05, mean val. loss:  2.32452154e+00\n",
      "Epoch: 11889 mean train loss:  1.88558188e-05, mean val. loss:  2.32475901e+00\n",
      "Epoch: 11890 mean train loss:  1.91124564e-05, mean val. loss:  2.32502556e+00\n",
      "Epoch: 11891 mean train loss:  1.89795392e-05, mean val. loss:  2.32530475e+00\n",
      "Epoch: 11892 mean train loss:  1.90068677e-05, mean val. loss:  2.32560468e+00\n",
      "Epoch: 11893 mean train loss:  1.92531734e-05, mean val. loss:  2.32590580e+00\n",
      "Epoch: 11894 mean train loss:  1.87208643e-05, mean val. loss:  2.32622361e+00\n",
      "Epoch: 11895 mean train loss:  1.89931015e-05, mean val. loss:  2.32654929e+00\n",
      "Epoch: 11896 mean train loss:  1.90127757e-05, mean val. loss:  2.32687092e+00\n",
      "Epoch: 11897 mean train loss:  1.92372827e-05, mean val. loss:  2.32718873e+00\n",
      "Epoch: 11898 mean train loss:  1.91105064e-05, mean val. loss:  2.32751250e+00\n",
      "Epoch: 11899 mean train loss:  1.89745333e-05, mean val. loss:  2.32783008e+00\n",
      "Epoch: 11900 mean train loss:  1.88677222e-05, mean val. loss:  2.32810092e+00\n",
      "Epoch: 11901 mean train loss:  1.88528211e-05, mean val. loss:  2.32839751e+00\n",
      "Epoch: 11902 mean train loss:  1.91196159e-05, mean val. loss:  2.32866526e+00\n",
      "Epoch: 11903 mean train loss:  1.89052080e-05, mean val. loss:  2.32888842e+00\n",
      "Epoch: 11904 mean train loss:  1.89903949e-05, mean val. loss:  2.32910872e+00\n",
      "Epoch: 11905 mean train loss:  1.90426072e-05, mean val. loss:  2.32929730e+00\n",
      "Epoch: 11906 mean train loss:  1.88019476e-05, mean val. loss:  2.32948184e+00\n",
      "Epoch: 11907 mean train loss:  1.89753191e-05, mean val. loss:  2.32965970e+00\n",
      "Epoch: 11908 mean train loss:  1.92206353e-05, mean val. loss:  2.32980824e+00\n",
      "Epoch: 11909 mean train loss:  1.89237762e-05, mean val. loss:  2.32994199e+00\n",
      "Epoch: 11910 mean train loss:  1.89675193e-05, mean val. loss:  2.33005452e+00\n",
      "Epoch: 11911 mean train loss:  1.88603590e-05, mean val. loss:  2.33015800e+00\n",
      "Epoch: 11912 mean train loss:  1.92474108e-05, mean val. loss:  2.33026862e+00\n",
      "Epoch: 11913 mean train loss:  1.89868151e-05, mean val. loss:  2.33037281e+00\n",
      "Epoch: 11914 mean train loss:  1.87924888e-05, mean val. loss:  2.33048677e+00\n",
      "Epoch: 11915 mean train loss:  1.90718274e-05, mean val. loss:  2.33061957e+00\n",
      "Epoch: 11916 mean train loss:  1.90619321e-05, mean val. loss:  2.33076525e+00\n",
      "Epoch: 11917 mean train loss:  1.88318081e-05, mean val. loss:  2.33094192e+00\n",
      "Epoch: 11918 mean train loss:  1.88907143e-05, mean val. loss:  2.33115864e+00\n",
      "Epoch: 11919 mean train loss:  1.91275612e-05, mean val. loss:  2.33140874e+00\n",
      "Epoch: 11920 mean train loss:  1.85515382e-05, mean val. loss:  2.33168316e+00\n",
      "Epoch: 11921 mean train loss:  1.91884465e-05, mean val. loss:  2.33196521e+00\n",
      "Epoch: 11922 mean train loss:  1.89931015e-05, mean val. loss:  2.33227110e+00\n",
      "Epoch: 11923 mean train loss:  1.90857681e-05, mean val. loss:  2.33254361e+00\n",
      "Epoch: 11924 mean train loss:  1.93399028e-05, mean val. loss:  2.33279347e+00\n",
      "Epoch: 11925 mean train loss:  1.90126011e-05, mean val. loss:  2.33300233e+00\n",
      "Epoch: 11926 mean train loss:  1.93076849e-05, mean val. loss:  2.33315253e+00\n",
      "Epoch: 11927 mean train loss:  1.91961008e-05, mean val. loss:  2.33326268e+00\n",
      "Epoch: 11928 mean train loss:  1.89763377e-05, mean val. loss:  2.33334684e+00\n",
      "Epoch: 11929 mean train loss:  1.88007834e-05, mean val. loss:  2.33340573e+00\n",
      "Epoch: 11930 mean train loss:  1.92218577e-05, mean val. loss:  2.33341980e+00\n",
      "Epoch: 11931 mean train loss:  1.91530271e-05, mean val. loss:  2.33339167e+00\n",
      "Epoch: 11932 mean train loss:  1.88764825e-05, mean val. loss:  2.33336663e+00\n",
      "Epoch: 11933 mean train loss:  1.90456049e-05, mean val. loss:  2.33332109e+00\n",
      "Epoch: 11934 mean train loss:  1.90497376e-05, mean val. loss:  2.33327222e+00\n",
      "Epoch: 11935 mean train loss:  1.87644619e-05, mean val. loss:  2.33325362e+00\n",
      "Epoch: 11936 mean train loss:  1.89231359e-05, mean val. loss:  2.33323836e+00\n",
      "Epoch: 11937 mean train loss:  1.91370200e-05, mean val. loss:  2.33322191e+00\n",
      "Epoch: 11938 mean train loss:  1.92291918e-05, mean val. loss:  2.33323431e+00\n",
      "Epoch: 11939 mean train loss:  1.87396654e-05, mean val. loss:  2.33329129e+00\n",
      "Epoch: 11940 mean train loss:  1.89966813e-05, mean val. loss:  2.33337235e+00\n",
      "Epoch: 11941 mean train loss:  1.90125138e-05, mean val. loss:  2.33346844e+00\n",
      "Epoch: 11942 mean train loss:  1.87952246e-05, mean val. loss:  2.33361363e+00\n",
      "Epoch: 11943 mean train loss:  1.88011327e-05, mean val. loss:  2.33383536e+00\n",
      "Epoch: 11944 mean train loss:  1.91867293e-05, mean val. loss:  2.33407879e+00\n",
      "Epoch: 11945 mean train loss:  1.89683051e-05, mean val. loss:  2.33435225e+00\n",
      "Epoch: 11946 mean train loss:  1.91796862e-05, mean val. loss:  2.33463049e+00\n",
      "Epoch: 11947 mean train loss:  1.90786086e-05, mean val. loss:  2.33492732e+00\n",
      "Epoch: 11948 mean train loss:  1.88364356e-05, mean val. loss:  2.33523226e+00\n",
      "Epoch: 11949 mean train loss:  1.90390856e-05, mean val. loss:  2.33555388e+00\n",
      "Epoch: 11950 mean train loss:  1.90888532e-05, mean val. loss:  2.33585048e+00\n",
      "Epoch: 11951 mean train loss:  1.91898434e-05, mean val. loss:  2.33611083e+00\n",
      "Epoch: 11952 mean train loss:  1.88797130e-05, mean val. loss:  2.33636904e+00\n",
      "Epoch: 11953 mean train loss:  1.89591083e-05, mean val. loss:  2.33664298e+00\n",
      "Epoch: 11954 mean train loss:  1.92962179e-05, mean val. loss:  2.33686709e+00\n",
      "Epoch: 11955 mean train loss:  1.89438288e-05, mean val. loss:  2.33706069e+00\n",
      "Epoch: 11956 mean train loss:  1.92307634e-05, mean val. loss:  2.33721519e+00\n",
      "Epoch: 11957 mean train loss:  1.91244180e-05, mean val. loss:  2.33733535e+00\n",
      "Epoch: 11958 mean train loss:  1.90258725e-05, mean val. loss:  2.33743167e+00\n",
      "Epoch: 11959 mean train loss:  1.88927806e-05, mean val. loss:  2.33749986e+00\n",
      "Epoch: 11960 mean train loss:  1.93149317e-05, mean val. loss:  2.33754587e+00\n",
      "Epoch: 11961 mean train loss:  1.89258426e-05, mean val. loss:  2.33758807e+00\n",
      "Epoch: 11962 mean train loss:  1.92351872e-05, mean val. loss:  2.33764219e+00\n",
      "Epoch: 11963 mean train loss:  1.90366700e-05, mean val. loss:  2.33770800e+00\n",
      "Epoch: 11964 mean train loss:  1.89902494e-05, mean val. loss:  2.33779383e+00\n",
      "Epoch: 11965 mean train loss:  1.89825369e-05, mean val. loss:  2.33786297e+00\n",
      "Epoch: 11966 mean train loss:  1.89192942e-05, mean val. loss:  2.33799219e+00\n",
      "Epoch: 11967 mean train loss:  1.92199077e-05, mean val. loss:  2.33813190e+00\n",
      "Epoch: 11968 mean train loss:  1.88040140e-05, mean val. loss:  2.33831453e+00\n",
      "Epoch: 11969 mean train loss:  1.89915881e-05, mean val. loss:  2.33850455e+00\n",
      "Epoch: 11970 mean train loss:  1.87291298e-05, mean val. loss:  2.33870912e+00\n",
      "Epoch: 11971 mean train loss:  1.92697626e-05, mean val. loss:  2.33893919e+00\n",
      "Epoch: 11972 mean train loss:  1.89790444e-05, mean val. loss:  2.33916879e+00\n",
      "Epoch: 11973 mean train loss:  1.89972634e-05, mean val. loss:  2.33943391e+00\n",
      "Epoch: 11974 mean train loss:  1.90486608e-05, mean val. loss:  2.33970904e+00\n",
      "Epoch: 11975 mean train loss:  1.93351880e-05, mean val. loss:  2.33995962e+00\n",
      "Epoch: 11976 mean train loss:  1.89087004e-05, mean val. loss:  2.34020686e+00\n",
      "Epoch: 11977 mean train loss:  1.87510159e-05, mean val. loss:  2.34045649e+00\n",
      "Epoch: 11978 mean train loss:  1.90530263e-05, mean val. loss:  2.34065914e+00\n",
      "Epoch: 11979 mean train loss:  1.87583792e-05, mean val. loss:  2.34087968e+00\n",
      "Epoch: 11980 mean train loss:  1.87671976e-05, mean val. loss:  2.34109259e+00\n",
      "Epoch: 11981 mean train loss:  1.88038684e-05, mean val. loss:  2.34126711e+00\n",
      "Epoch: 11982 mean train loss:  1.89331186e-05, mean val. loss:  2.34141207e+00\n",
      "Epoch: 11983 mean train loss:  1.88110862e-05, mean val. loss:  2.34155560e+00\n",
      "Epoch: 11984 mean train loss:  1.87989790e-05, mean val. loss:  2.34166861e+00\n",
      "Epoch: 11985 mean train loss:  1.88309932e-05, mean val. loss:  2.34178066e+00\n",
      "Epoch: 11986 mean train loss:  1.86852994e-05, mean val. loss:  2.34188342e+00\n",
      "Epoch: 11987 mean train loss:  1.87937403e-05, mean val. loss:  2.34199667e+00\n",
      "Epoch: 11988 mean train loss:  1.87882106e-05, mean val. loss:  2.34212112e+00\n",
      "Epoch: 11989 mean train loss:  1.89823040e-05, mean val. loss:  2.34224868e+00\n",
      "Epoch: 11990 mean train loss:  1.90157734e-05, mean val. loss:  2.34235764e+00\n",
      "Epoch: 11991 mean train loss:  1.87413825e-05, mean val. loss:  2.34248042e+00\n",
      "Epoch: 11992 mean train loss:  1.89225830e-05, mean val. loss:  2.34261394e+00\n",
      "Epoch: 11993 mean train loss:  1.93114101e-05, mean val. loss:  2.34272385e+00\n",
      "Epoch: 11994 mean train loss:  1.90086430e-05, mean val. loss:  2.34283519e+00\n",
      "Epoch: 11995 mean train loss:  1.88067497e-05, mean val. loss:  2.34296584e+00\n",
      "Epoch: 11996 mean train loss:  1.88421691e-05, mean val. loss:  2.34311533e+00\n",
      "Epoch: 11997 mean train loss:  1.88486010e-05, mean val. loss:  2.34329796e+00\n",
      "Epoch: 11998 mean train loss:  1.88010454e-05, mean val. loss:  2.34348941e+00\n",
      "Epoch: 11999 mean train loss:  1.91514555e-05, mean val. loss:  2.34369254e+00\n",
      "Epoch: 12000 mean train loss:  1.88885024e-05, mean val. loss:  2.34389114e+00\n",
      "Epoch: 12001 mean train loss:  1.90448482e-05, mean val. loss:  2.34411502e+00\n",
      "Epoch: 12002 mean train loss:  1.88559061e-05, mean val. loss:  2.34434414e+00\n",
      "Epoch: 12003 mean train loss:  1.87526457e-05, mean val. loss:  2.34458303e+00\n",
      "Epoch: 12004 mean train loss:  1.89754937e-05, mean val. loss:  2.34483194e+00\n",
      "Epoch: 12005 mean train loss:  1.88008125e-05, mean val. loss:  2.34509945e+00\n",
      "Epoch: 12006 mean train loss:  1.91074214e-05, mean val. loss:  2.34536576e+00\n",
      "Epoch: 12007 mean train loss:  1.90533756e-05, mean val. loss:  2.34560227e+00\n",
      "Epoch: 12008 mean train loss:  1.86510442e-05, mean val. loss:  2.34586692e+00\n",
      "Epoch: 12009 mean train loss:  1.87082624e-05, mean val. loss:  2.34612489e+00\n",
      "Epoch: 12010 mean train loss:  1.88246777e-05, mean val. loss:  2.34635735e+00\n",
      "Epoch: 12011 mean train loss:  1.86782854e-05, mean val. loss:  2.34659362e+00\n",
      "Epoch: 12012 mean train loss:  1.89746497e-05, mean val. loss:  2.34681487e+00\n",
      "Epoch: 12013 mean train loss:  1.89301791e-05, mean val. loss:  2.34701157e+00\n",
      "Epoch: 12014 mean train loss:  1.89305283e-05, mean val. loss:  2.34719229e+00\n",
      "Epoch: 12015 mean train loss:  1.93174346e-05, mean val. loss:  2.34733748e+00\n",
      "Epoch: 12016 mean train loss:  1.86526740e-05, mean val. loss:  2.34749746e+00\n",
      "Epoch: 12017 mean train loss:  1.87787518e-05, mean val. loss:  2.34764647e+00\n",
      "Epoch: 12018 mean train loss:  1.87520345e-05, mean val. loss:  2.34776258e+00\n",
      "Epoch: 12019 mean train loss:  1.89965067e-05, mean val. loss:  2.34785080e+00\n",
      "Epoch: 12020 mean train loss:  1.87388505e-05, mean val. loss:  2.34792924e+00\n",
      "Epoch: 12021 mean train loss:  1.91356812e-05, mean val. loss:  2.34798622e+00\n",
      "Epoch: 12022 mean train loss:  1.92130101e-05, mean val. loss:  2.34802914e+00\n",
      "Epoch: 12023 mean train loss:  1.87953119e-05, mean val. loss:  2.34808874e+00\n",
      "Epoch: 12024 mean train loss:  1.83749071e-05, mean val. loss:  2.34814382e+00\n",
      "Epoch: 12025 mean train loss:  1.87788974e-05, mean val. loss:  2.34820509e+00\n",
      "Epoch: 12026 mean train loss:  1.89906568e-05, mean val. loss:  2.34825802e+00\n",
      "Epoch: 12027 mean train loss:  1.85337267e-05, mean val. loss:  2.34834218e+00\n",
      "Epoch: 12028 mean train loss:  1.91754662e-05, mean val. loss:  2.34843993e+00\n",
      "Epoch: 12029 mean train loss:  1.88863487e-05, mean val. loss:  2.34853292e+00\n",
      "Epoch: 12030 mean train loss:  1.87167316e-05, mean val. loss:  2.34867072e+00\n",
      "Epoch: 12031 mean train loss:  1.86054676e-05, mean val. loss:  2.34885550e+00\n",
      "Epoch: 12032 mean train loss:  1.89977000e-05, mean val. loss:  2.34906077e+00\n",
      "Epoch: 12033 mean train loss:  1.87988044e-05, mean val. loss:  2.34925032e+00\n",
      "Epoch: 12034 mean train loss:  1.88530539e-05, mean val. loss:  2.34941411e+00\n",
      "Epoch: 12035 mean train loss:  1.89412385e-05, mean val. loss:  2.34958243e+00\n",
      "Epoch: 12036 mean train loss:  1.87586120e-05, mean val. loss:  2.34974313e+00\n",
      "Epoch: 12037 mean train loss:  1.85767421e-05, mean val. loss:  2.34988713e+00\n",
      "Epoch: 12038 mean train loss:  1.88511040e-05, mean val. loss:  2.35003281e+00\n",
      "Epoch: 12039 mean train loss:  1.90360588e-05, mean val. loss:  2.35018468e+00\n",
      "Epoch: 12040 mean train loss:  1.90850405e-05, mean val. loss:  2.35029745e+00\n",
      "Epoch: 12041 mean train loss:  1.88514823e-05, mean val. loss:  2.35040903e+00\n",
      "Epoch: 12042 mean train loss:  1.93406304e-05, mean val. loss:  2.35048604e+00\n",
      "Epoch: 12043 mean train loss:  1.87477563e-05, mean val. loss:  2.35054874e+00\n",
      "Epoch: 12044 mean train loss:  1.85936806e-05, mean val. loss:  2.35064721e+00\n",
      "Epoch: 12045 mean train loss:  1.87632686e-05, mean val. loss:  2.35072398e+00\n",
      "Epoch: 12046 mean train loss:  1.91732543e-05, mean val. loss:  2.35077047e+00\n",
      "Epoch: 12047 mean train loss:  1.88674603e-05, mean val. loss:  2.35081053e+00\n",
      "Epoch: 12048 mean train loss:  1.89836428e-05, mean val. loss:  2.35087538e+00\n",
      "Epoch: 12049 mean train loss:  1.85858225e-05, mean val. loss:  2.35094905e+00\n",
      "Epoch: 12050 mean train loss:  1.87686819e-05, mean val. loss:  2.35105419e+00\n",
      "Epoch: 12051 mean train loss:  1.92178995e-05, mean val. loss:  2.35114121e+00\n",
      "Epoch: 12052 mean train loss:  1.89231068e-05, mean val. loss:  2.35128617e+00\n",
      "Epoch: 12053 mean train loss:  1.91057916e-05, mean val. loss:  2.35143137e+00\n",
      "Epoch: 12054 mean train loss:  1.89908023e-05, mean val. loss:  2.35160160e+00\n",
      "Epoch: 12055 mean train loss:  1.88724662e-05, mean val. loss:  2.35179377e+00\n",
      "Epoch: 12056 mean train loss:  1.91802392e-05, mean val. loss:  2.35199738e+00\n",
      "Epoch: 12057 mean train loss:  1.89581770e-05, mean val. loss:  2.35219908e+00\n",
      "Epoch: 12058 mean train loss:  1.90149876e-05, mean val. loss:  2.35240865e+00\n",
      "Epoch: 12059 mean train loss:  1.89295679e-05, mean val. loss:  2.35264540e+00\n",
      "Epoch: 12060 mean train loss:  1.90334104e-05, mean val. loss:  2.35288835e+00\n",
      "Epoch: 12061 mean train loss:  1.91366998e-05, mean val. loss:  2.35312581e+00\n",
      "Epoch: 12062 mean train loss:  1.90327992e-05, mean val. loss:  2.35337567e+00\n",
      "Epoch: 12063 mean train loss:  1.85375975e-05, mean val. loss:  2.35368490e+00\n",
      "Epoch: 12064 mean train loss:  1.85034587e-05, mean val. loss:  2.35397863e+00\n",
      "Epoch: 12065 mean train loss:  1.92200532e-05, mean val. loss:  2.35424399e+00\n",
      "Epoch: 12066 mean train loss:  1.89681596e-05, mean val. loss:  2.35451484e+00\n",
      "Epoch: 12067 mean train loss:  1.88404520e-05, mean val. loss:  2.35477757e+00\n",
      "Epoch: 12068 mean train loss:  1.93952874e-05, mean val. loss:  2.35500503e+00\n",
      "Epoch: 12069 mean train loss:  1.87405676e-05, mean val. loss:  2.35526323e+00\n",
      "Epoch: 12070 mean train loss:  1.89340499e-05, mean val. loss:  2.35551167e+00\n",
      "Epoch: 12071 mean train loss:  1.89132697e-05, mean val. loss:  2.35575557e+00\n",
      "Epoch: 12072 mean train loss:  1.82336662e-05, mean val. loss:  2.35602856e+00\n",
      "Epoch: 12073 mean train loss:  1.90699357e-05, mean val. loss:  2.35628033e+00\n",
      "Epoch: 12074 mean train loss:  1.88769482e-05, mean val. loss:  2.35651445e+00\n",
      "Epoch: 12075 mean train loss:  1.87311671e-05, mean val. loss:  2.35676742e+00\n",
      "Epoch: 12076 mean train loss:  1.92862935e-05, mean val. loss:  2.35697341e+00\n",
      "Epoch: 12077 mean train loss:  1.87297410e-05, mean val. loss:  2.35721302e+00\n",
      "Epoch: 12078 mean train loss:  1.84977253e-05, mean val. loss:  2.35747576e+00\n",
      "Epoch: 12079 mean train loss:  1.90510764e-05, mean val. loss:  2.35771370e+00\n",
      "Epoch: 12080 mean train loss:  1.92050356e-05, mean val. loss:  2.35790396e+00\n",
      "Epoch: 12081 mean train loss:  1.86803809e-05, mean val. loss:  2.35811067e+00\n",
      "Epoch: 12082 mean train loss:  1.92007865e-05, mean val. loss:  2.35827541e+00\n",
      "Epoch: 12083 mean train loss:  1.88359991e-05, mean val. loss:  2.35842776e+00\n",
      "Epoch: 12084 mean train loss:  1.87807600e-05, mean val. loss:  2.35856295e+00\n",
      "Epoch: 12085 mean train loss:  1.88247068e-05, mean val. loss:  2.35869932e+00\n",
      "Epoch: 12086 mean train loss:  1.87982805e-05, mean val. loss:  2.35883737e+00\n",
      "Epoch: 12087 mean train loss:  1.88177510e-05, mean val. loss:  2.35893798e+00\n",
      "Epoch: 12088 mean train loss:  1.88742706e-05, mean val. loss:  2.35902691e+00\n",
      "Epoch: 12089 mean train loss:  1.88561680e-05, mean val. loss:  2.35910416e+00\n",
      "Epoch: 12090 mean train loss:  1.89449929e-05, mean val. loss:  2.35918546e+00\n",
      "Epoch: 12091 mean train loss:  1.87979895e-05, mean val. loss:  2.35928941e+00\n",
      "Epoch: 12092 mean train loss:  1.87512778e-05, mean val. loss:  2.35939741e+00\n",
      "Epoch: 12093 mean train loss:  1.89343118e-05, mean val. loss:  2.35949969e+00\n",
      "Epoch: 12094 mean train loss:  1.86348334e-05, mean val. loss:  2.35962892e+00\n",
      "Epoch: 12095 mean train loss:  1.87611440e-05, mean val. loss:  2.35978031e+00\n",
      "Epoch: 12096 mean train loss:  1.87044207e-05, mean val. loss:  2.35997033e+00\n",
      "Epoch: 12097 mean train loss:  1.91980798e-05, mean val. loss:  2.36015177e+00\n",
      "Epoch: 12098 mean train loss:  1.86349498e-05, mean val. loss:  2.36034250e+00\n",
      "Epoch: 12099 mean train loss:  1.89651910e-05, mean val. loss:  2.36055040e+00\n",
      "Epoch: 12100 mean train loss:  1.87480764e-05, mean val. loss:  2.36076570e+00\n",
      "Epoch: 12101 mean train loss:  1.88283448e-05, mean val. loss:  2.36097789e+00\n",
      "Epoch: 12102 mean train loss:  1.89580023e-05, mean val. loss:  2.36120915e+00\n",
      "Epoch: 12103 mean train loss:  1.89936836e-05, mean val. loss:  2.36141729e+00\n",
      "Epoch: 12104 mean train loss:  1.86385005e-05, mean val. loss:  2.36161923e+00\n",
      "Epoch: 12105 mean train loss:  1.87089609e-05, mean val. loss:  2.36182475e+00\n",
      "Epoch: 12106 mean train loss:  1.90416176e-05, mean val. loss:  2.36201167e+00\n",
      "Epoch: 12107 mean train loss:  1.89305283e-05, mean val. loss:  2.36217785e+00\n",
      "Epoch: 12108 mean train loss:  1.88890554e-05, mean val. loss:  2.36233854e+00\n",
      "Epoch: 12109 mean train loss:  1.85325916e-05, mean val. loss:  2.36248636e+00\n",
      "Epoch: 12110 mean train loss:  1.87755213e-05, mean val. loss:  2.36262941e+00\n",
      "Epoch: 12111 mean train loss:  1.86339312e-05, mean val. loss:  2.36274815e+00\n",
      "Epoch: 12112 mean train loss:  1.88903941e-05, mean val. loss:  2.36283469e+00\n",
      "Epoch: 12113 mean train loss:  1.89713901e-05, mean val. loss:  2.36290455e+00\n",
      "Epoch: 12114 mean train loss:  1.86398393e-05, mean val. loss:  2.36296272e+00\n",
      "Epoch: 12115 mean train loss:  1.90010469e-05, mean val. loss:  2.36302590e+00\n",
      "Epoch: 12116 mean train loss:  1.88678387e-05, mean val. loss:  2.36307621e+00\n",
      "Epoch: 12117 mean train loss:  1.91238069e-05, mean val. loss:  2.36315608e+00\n",
      "Epoch: 12118 mean train loss:  1.90224091e-05, mean val. loss:  2.36321497e+00\n",
      "Epoch: 12119 mean train loss:  1.89161801e-05, mean val. loss:  2.36328053e+00\n",
      "Epoch: 12120 mean train loss:  1.84640521e-05, mean val. loss:  2.36339998e+00\n",
      "Epoch: 12121 mean train loss:  1.90312858e-05, mean val. loss:  2.36355448e+00\n",
      "Epoch: 12122 mean train loss:  1.89086131e-05, mean val. loss:  2.36371469e+00\n",
      "Epoch: 12123 mean train loss:  1.83722586e-05, mean val. loss:  2.36390090e+00\n",
      "Epoch: 12124 mean train loss:  1.88813428e-05, mean val. loss:  2.36408281e+00\n",
      "Epoch: 12125 mean train loss:  1.85773533e-05, mean val. loss:  2.36425352e+00\n",
      "Epoch: 12126 mean train loss:  1.85460376e-05, mean val. loss:  2.36445785e+00\n",
      "Epoch: 12127 mean train loss:  1.88318081e-05, mean val. loss:  2.36463714e+00\n",
      "Epoch: 12128 mean train loss:  1.87132391e-05, mean val. loss:  2.36483049e+00\n",
      "Epoch: 12129 mean train loss:  1.89193233e-05, mean val. loss:  2.36501384e+00\n",
      "Epoch: 12130 mean train loss:  1.90016290e-05, mean val. loss:  2.36519766e+00\n",
      "Epoch: 12131 mean train loss:  1.85072713e-05, mean val. loss:  2.36542344e+00\n",
      "Epoch: 12132 mean train loss:  1.90215942e-05, mean val. loss:  2.36561322e+00\n",
      "Epoch: 12133 mean train loss:  1.87469414e-05, mean val. loss:  2.36582637e+00\n",
      "Epoch: 12134 mean train loss:  1.90926658e-05, mean val. loss:  2.36604834e+00\n",
      "Epoch: 12135 mean train loss:  1.89241255e-05, mean val. loss:  2.36626220e+00\n",
      "Epoch: 12136 mean train loss:  1.88275299e-05, mean val. loss:  2.36648870e+00\n",
      "Epoch: 12137 mean train loss:  1.91085273e-05, mean val. loss:  2.36670327e+00\n",
      "Epoch: 12138 mean train loss:  1.87357946e-05, mean val. loss:  2.36692977e+00\n",
      "Epoch: 12139 mean train loss:  1.86880352e-05, mean val. loss:  2.36712050e+00\n",
      "Epoch: 12140 mean train loss:  1.91151630e-05, mean val. loss:  2.36727691e+00\n",
      "Epoch: 12141 mean train loss:  1.89054699e-05, mean val. loss:  2.36742210e+00\n",
      "Epoch: 12142 mean train loss:  1.90708961e-05, mean val. loss:  2.36752415e+00\n",
      "Epoch: 12143 mean train loss:  1.86289544e-05, mean val. loss:  2.36763597e+00\n",
      "Epoch: 12144 mean train loss:  1.88740087e-05, mean val. loss:  2.36773086e+00\n",
      "Epoch: 12145 mean train loss:  1.87267433e-05, mean val. loss:  2.36781144e+00\n",
      "Epoch: 12146 mean train loss:  1.89083803e-05, mean val. loss:  2.36788821e+00\n",
      "Epoch: 12147 mean train loss:  1.83219090e-05, mean val. loss:  2.36796641e+00\n",
      "Epoch: 12148 mean train loss:  1.84915261e-05, mean val. loss:  2.36807060e+00\n",
      "Epoch: 12149 mean train loss:  1.88715057e-05, mean val. loss:  2.36819601e+00\n",
      "Epoch: 12150 mean train loss:  1.88664126e-05, mean val. loss:  2.36833239e+00\n",
      "Epoch: 12151 mean train loss:  1.88168196e-05, mean val. loss:  2.36848664e+00\n",
      "Epoch: 12152 mean train loss:  1.89690036e-05, mean val. loss:  2.36868429e+00\n",
      "Epoch: 12153 mean train loss:  1.86782854e-05, mean val. loss:  2.36891675e+00\n",
      "Epoch: 12154 mean train loss:  1.90759893e-05, mean val. loss:  2.36917567e+00\n",
      "Epoch: 12155 mean train loss:  1.86351244e-05, mean val. loss:  2.36944580e+00\n",
      "Epoch: 12156 mean train loss:  1.86707475e-05, mean val. loss:  2.36974263e+00\n",
      "Epoch: 12157 mean train loss:  1.90902792e-05, mean val. loss:  2.37003112e+00\n",
      "Epoch: 12158 mean train loss:  1.89189450e-05, mean val. loss:  2.37030101e+00\n",
      "Epoch: 12159 mean train loss:  1.87901314e-05, mean val. loss:  2.37056828e+00\n",
      "Epoch: 12160 mean train loss:  1.91894651e-05, mean val. loss:  2.37079334e+00\n",
      "Epoch: 12161 mean train loss:  1.89157727e-05, mean val. loss:  2.37101245e+00\n",
      "Epoch: 12162 mean train loss:  1.86554098e-05, mean val. loss:  2.37122416e+00\n",
      "Epoch: 12163 mean train loss:  1.88901904e-05, mean val. loss:  2.37142611e+00\n",
      "Epoch: 12164 mean train loss:  1.89581478e-05, mean val. loss:  2.37155437e+00\n",
      "Epoch: 12165 mean train loss:  1.86752004e-05, mean val. loss:  2.37169242e+00\n",
      "Epoch: 12166 mean train loss:  1.86303223e-05, mean val. loss:  2.37181544e+00\n",
      "Epoch: 12167 mean train loss:  1.86105608e-05, mean val. loss:  2.37192464e+00\n",
      "Epoch: 12168 mean train loss:  1.87909463e-05, mean val. loss:  2.37203884e+00\n",
      "Epoch: 12169 mean train loss:  1.91661238e-05, mean val. loss:  2.37209368e+00\n",
      "Epoch: 12170 mean train loss:  1.84553501e-05, mean val. loss:  2.37218189e+00\n",
      "Epoch: 12171 mean train loss:  1.87893747e-05, mean val. loss:  2.37225580e+00\n",
      "Epoch: 12172 mean train loss:  1.91375148e-05, mean val. loss:  2.37234783e+00\n",
      "Epoch: 12173 mean train loss:  1.89025886e-05, mean val. loss:  2.37246799e+00\n",
      "Epoch: 12174 mean train loss:  1.86247926e-05, mean val. loss:  2.37260270e+00\n",
      "Epoch: 12175 mean train loss:  1.87681871e-05, mean val. loss:  2.37272668e+00\n",
      "Epoch: 12176 mean train loss:  1.89797720e-05, mean val. loss:  2.37285042e+00\n",
      "Epoch: 12177 mean train loss:  1.86006073e-05, mean val. loss:  2.37300301e+00\n",
      "Epoch: 12178 mean train loss:  1.89403945e-05, mean val. loss:  2.37315774e+00\n",
      "Epoch: 12179 mean train loss:  1.87988626e-05, mean val. loss:  2.37332082e+00\n",
      "Epoch: 12180 mean train loss:  1.84988894e-05, mean val. loss:  2.37351346e+00\n",
      "Epoch: 12181 mean train loss:  1.90839055e-05, mean val. loss:  2.37370777e+00\n",
      "Epoch: 12182 mean train loss:  1.89132406e-05, mean val. loss:  2.37391305e+00\n",
      "Epoch: 12183 mean train loss:  1.90011633e-05, mean val. loss:  2.37413263e+00\n",
      "Epoch: 12184 mean train loss:  1.86431280e-05, mean val. loss:  2.37434101e+00\n",
      "Epoch: 12185 mean train loss:  1.83623051e-05, mean val. loss:  2.37456489e+00\n",
      "Epoch: 12186 mean train loss:  1.89486891e-05, mean val. loss:  2.37479305e+00\n",
      "Epoch: 12187 mean train loss:  1.86628313e-05, mean val. loss:  2.37501335e+00\n",
      "Epoch: 12188 mean train loss:  1.86992693e-05, mean val. loss:  2.37523770e+00\n",
      "Epoch: 12189 mean train loss:  1.87209807e-05, mean val. loss:  2.37545681e+00\n",
      "Epoch: 12190 mean train loss:  1.88050908e-05, mean val. loss:  2.37569094e+00\n",
      "Epoch: 12191 mean train loss:  1.87462429e-05, mean val. loss:  2.37591100e+00\n",
      "Epoch: 12192 mean train loss:  1.85563113e-05, mean val. loss:  2.37610650e+00\n",
      "Epoch: 12193 mean train loss:  1.85329991e-05, mean val. loss:  2.37630653e+00\n",
      "Epoch: 12194 mean train loss:  1.87771511e-05, mean val. loss:  2.37649822e+00\n",
      "Epoch: 12195 mean train loss:  1.86706311e-05, mean val. loss:  2.37670779e+00\n",
      "Epoch: 12196 mean train loss:  1.89377461e-05, mean val. loss:  2.37691283e+00\n",
      "Epoch: 12197 mean train loss:  1.86352117e-05, mean val. loss:  2.37713194e+00\n",
      "Epoch: 12198 mean train loss:  1.90405117e-05, mean val. loss:  2.37732840e+00\n",
      "Epoch: 12199 mean train loss:  1.83987431e-05, mean val. loss:  2.37755537e+00\n",
      "Epoch: 12200 mean train loss:  1.87153928e-05, mean val. loss:  2.37780309e+00\n",
      "Epoch: 12201 mean train loss:  1.87198748e-05, mean val. loss:  2.37804842e+00\n",
      "Epoch: 12202 mean train loss:  1.87945261e-05, mean val. loss:  2.37825632e+00\n",
      "Epoch: 12203 mean train loss:  1.88540143e-05, mean val. loss:  2.37846303e+00\n",
      "Epoch: 12204 mean train loss:  1.87746482e-05, mean val. loss:  2.37867618e+00\n",
      "Epoch: 12205 mean train loss:  1.89342245e-05, mean val. loss:  2.37891412e+00\n",
      "Epoch: 12206 mean train loss:  1.90176361e-05, mean val. loss:  2.37912607e+00\n",
      "Epoch: 12207 mean train loss:  1.88080885e-05, mean val. loss:  2.37932801e+00\n",
      "Epoch: 12208 mean train loss:  1.87332917e-05, mean val. loss:  2.37950873e+00\n",
      "Epoch: 12209 mean train loss:  1.91540457e-05, mean val. loss:  2.37965083e+00\n",
      "Epoch: 12210 mean train loss:  1.87475525e-05, mean val. loss:  2.37975454e+00\n",
      "Epoch: 12211 mean train loss:  1.86013931e-05, mean val. loss:  2.37982249e+00\n",
      "Epoch: 12212 mean train loss:  1.86745892e-05, mean val. loss:  2.37991214e+00\n",
      "Epoch: 12213 mean train loss:  1.86393445e-05, mean val. loss:  2.37997365e+00\n",
      "Epoch: 12214 mean train loss:  1.85060198e-05, mean val. loss:  2.38006568e+00\n",
      "Epoch: 12215 mean train loss:  1.88297126e-05, mean val. loss:  2.38012028e+00\n",
      "Epoch: 12216 mean train loss:  1.87166734e-05, mean val. loss:  2.38016558e+00\n",
      "Epoch: 12217 mean train loss:  1.84791279e-05, mean val. loss:  2.38023996e+00\n",
      "Epoch: 12218 mean train loss:  1.85859099e-05, mean val. loss:  2.38033557e+00\n",
      "Epoch: 12219 mean train loss:  1.86666439e-05, mean val. loss:  2.38042331e+00\n",
      "Epoch: 12220 mean train loss:  1.87277910e-05, mean val. loss:  2.38054633e+00\n",
      "Epoch: 12221 mean train loss:  1.85534591e-05, mean val. loss:  2.38071442e+00\n",
      "Epoch: 12222 mean train loss:  1.88469130e-05, mean val. loss:  2.38091230e+00\n",
      "Epoch: 12223 mean train loss:  1.84906530e-05, mean val. loss:  2.38115358e+00\n",
      "Epoch: 12224 mean train loss:  1.90775027e-05, mean val. loss:  2.38141704e+00\n",
      "Epoch: 12225 mean train loss:  1.89074781e-05, mean val. loss:  2.38169551e+00\n",
      "Epoch: 12226 mean train loss:  1.85343088e-05, mean val. loss:  2.38200521e+00\n",
      "Epoch: 12227 mean train loss:  1.89227576e-05, mean val. loss:  2.38231468e+00\n",
      "Epoch: 12228 mean train loss:  1.86042162e-05, mean val. loss:  2.38265920e+00\n",
      "Epoch: 12229 mean train loss:  1.87098922e-05, mean val. loss:  2.38301277e+00\n",
      "Epoch: 12230 mean train loss:  1.87736587e-05, mean val. loss:  2.38335156e+00\n",
      "Epoch: 12231 mean train loss:  1.88257254e-05, mean val. loss:  2.38365245e+00\n",
      "Epoch: 12232 mean train loss:  1.91374857e-05, mean val. loss:  2.38392258e+00\n",
      "Epoch: 12233 mean train loss:  1.87101832e-05, mean val. loss:  2.38415933e+00\n",
      "Epoch: 12234 mean train loss:  1.87986298e-05, mean val. loss:  2.38437438e+00\n",
      "Epoch: 12235 mean train loss:  1.89206330e-05, mean val. loss:  2.38454700e+00\n",
      "Epoch: 12236 mean train loss:  1.85484823e-05, mean val. loss:  2.38467026e+00\n",
      "Epoch: 12237 mean train loss:  1.88812846e-05, mean val. loss:  2.38477612e+00\n",
      "Epoch: 12238 mean train loss:  1.87480473e-05, mean val. loss:  2.38484502e+00\n",
      "Epoch: 12239 mean train loss:  1.88106205e-05, mean val. loss:  2.38488388e+00\n",
      "Epoch: 12240 mean train loss:  1.90802384e-05, mean val. loss:  2.38488293e+00\n",
      "Epoch: 12241 mean train loss:  1.87932455e-05, mean val. loss:  2.38486958e+00\n",
      "Epoch: 12242 mean train loss:  1.85904210e-05, mean val. loss:  2.38486338e+00\n",
      "Epoch: 12243 mean train loss:  1.85613753e-05, mean val. loss:  2.38489938e+00\n",
      "Epoch: 12244 mean train loss:  1.84663222e-05, mean val. loss:  2.38498068e+00\n",
      "Epoch: 12245 mean train loss:  1.86418765e-05, mean val. loss:  2.38507938e+00\n",
      "Epoch: 12246 mean train loss:  1.90315477e-05, mean val. loss:  2.38522744e+00\n",
      "Epoch: 12247 mean train loss:  1.87984260e-05, mean val. loss:  2.38539171e+00\n",
      "Epoch: 12248 mean train loss:  1.90028804e-05, mean val. loss:  2.38557339e+00\n",
      "Epoch: 12249 mean train loss:  1.86735706e-05, mean val. loss:  2.38581967e+00\n",
      "Epoch: 12250 mean train loss:  1.88207487e-05, mean val. loss:  2.38607955e+00\n",
      "Epoch: 12251 mean train loss:  1.82430667e-05, mean val. loss:  2.38638377e+00\n",
      "Epoch: 12252 mean train loss:  1.86962425e-05, mean val. loss:  2.38672829e+00\n",
      "Epoch: 12253 mean train loss:  1.88145204e-05, mean val. loss:  2.38706803e+00\n",
      "Epoch: 12254 mean train loss:  1.88319827e-05, mean val. loss:  2.38745904e+00\n",
      "Epoch: 12255 mean train loss:  1.83574157e-05, mean val. loss:  2.38788342e+00\n",
      "Epoch: 12256 mean train loss:  1.85054087e-05, mean val. loss:  2.38830996e+00\n",
      "Epoch: 12257 mean train loss:  1.86688849e-05, mean val. loss:  2.38873768e+00\n",
      "Epoch: 12258 mean train loss:  1.84629753e-05, mean val. loss:  2.38916397e+00\n",
      "Epoch: 12259 mean train loss:  1.86162069e-05, mean val. loss:  2.38954639e+00\n",
      "Epoch: 12260 mean train loss:  1.87078258e-05, mean val. loss:  2.38993001e+00\n",
      "Epoch: 12261 mean train loss:  1.83994125e-05, mean val. loss:  2.39031005e+00\n",
      "Epoch: 12262 mean train loss:  1.87637343e-05, mean val. loss:  2.39061999e+00\n",
      "Epoch: 12263 mean train loss:  1.84443779e-05, mean val. loss:  2.39089847e+00\n",
      "Epoch: 12264 mean train loss:  1.85141107e-05, mean val. loss:  2.39114618e+00\n",
      "Epoch: 12265 mean train loss:  1.85438257e-05, mean val. loss:  2.39136863e+00\n",
      "Epoch: 12266 mean train loss:  1.88638223e-05, mean val. loss:  2.39156222e+00\n",
      "Epoch: 12267 mean train loss:  1.89108541e-05, mean val. loss:  2.39170313e+00\n",
      "Epoch: 12268 mean train loss:  1.86434190e-05, mean val. loss:  2.39178228e+00\n",
      "Epoch: 12269 mean train loss:  1.89668790e-05, mean val. loss:  2.39183879e+00\n",
      "Epoch: 12270 mean train loss:  1.88025879e-05, mean val. loss:  2.39186859e+00\n",
      "Epoch: 12271 mean train loss:  1.83130032e-05, mean val. loss:  2.39188480e+00\n",
      "Epoch: 12272 mean train loss:  1.84733653e-05, mean val. loss:  2.39189529e+00\n",
      "Epoch: 12273 mean train loss:  1.88742124e-05, mean val. loss:  2.39192414e+00\n",
      "Epoch: 12274 mean train loss:  1.86213583e-05, mean val. loss:  2.39195228e+00\n",
      "Epoch: 12275 mean train loss:  1.85606768e-05, mean val. loss:  2.39198947e+00\n",
      "Epoch: 12276 mean train loss:  1.88053527e-05, mean val. loss:  2.39200807e+00\n",
      "Epoch: 12277 mean train loss:  1.86486868e-05, mean val. loss:  2.39205647e+00\n",
      "Epoch: 12278 mean train loss:  1.89238635e-05, mean val. loss:  2.39209414e+00\n",
      "Epoch: 12279 mean train loss:  1.86739198e-05, mean val. loss:  2.39214396e+00\n",
      "Epoch: 12280 mean train loss:  1.87956321e-05, mean val. loss:  2.39223456e+00\n",
      "Epoch: 12281 mean train loss:  1.85480458e-05, mean val. loss:  2.39237833e+00\n",
      "Epoch: 12282 mean train loss:  1.84999371e-05, mean val. loss:  2.39254999e+00\n",
      "Epoch: 12283 mean train loss:  1.87709811e-05, mean val. loss:  2.39275217e+00\n",
      "Epoch: 12284 mean train loss:  1.85703393e-05, mean val. loss:  2.39298844e+00\n",
      "Epoch: 12285 mean train loss:  1.86484249e-05, mean val. loss:  2.39324212e+00\n",
      "Epoch: 12286 mean train loss:  1.84862874e-05, mean val. loss:  2.39352727e+00\n",
      "Epoch: 12287 mean train loss:  1.88913837e-05, mean val. loss:  2.39379740e+00\n",
      "Epoch: 12288 mean train loss:  1.86638790e-05, mean val. loss:  2.39405537e+00\n",
      "Epoch: 12289 mean train loss:  1.86768884e-05, mean val. loss:  2.39431357e+00\n",
      "Epoch: 12290 mean train loss:  1.87943224e-05, mean val. loss:  2.39454722e+00\n",
      "Epoch: 12291 mean train loss:  1.87847181e-05, mean val. loss:  2.39479041e+00\n",
      "Epoch: 12292 mean train loss:  1.85891986e-05, mean val. loss:  2.39501548e+00\n",
      "Epoch: 12293 mean train loss:  1.90266001e-05, mean val. loss:  2.39521956e+00\n",
      "Epoch: 12294 mean train loss:  1.85041281e-05, mean val. loss:  2.39541578e+00\n",
      "Epoch: 12295 mean train loss:  1.85416429e-05, mean val. loss:  2.39558506e+00\n",
      "Epoch: 12296 mean train loss:  1.88366685e-05, mean val. loss:  2.39574528e+00\n",
      "Epoch: 12297 mean train loss:  1.86498510e-05, mean val. loss:  2.39587998e+00\n",
      "Epoch: 12298 mean train loss:  1.84620731e-05, mean val. loss:  2.39600158e+00\n",
      "Epoch: 12299 mean train loss:  1.89126295e-05, mean val. loss:  2.39606857e+00\n",
      "Epoch: 12300 mean train loss:  1.84970559e-05, mean val. loss:  2.39615250e+00\n",
      "Epoch: 12301 mean train loss:  1.87808182e-05, mean val. loss:  2.39624834e+00\n",
      "Epoch: 12302 mean train loss:  1.85513636e-05, mean val. loss:  2.39636087e+00\n",
      "Epoch: 12303 mean train loss:  1.89951097e-05, mean val. loss:  2.39646077e+00\n",
      "Epoch: 12304 mean train loss:  1.89938000e-05, mean val. loss:  2.39656901e+00\n",
      "Epoch: 12305 mean train loss:  1.86192628e-05, mean val. loss:  2.39667654e+00\n",
      "Epoch: 12306 mean train loss:  1.87534606e-05, mean val. loss:  2.39678907e+00\n",
      "Epoch: 12307 mean train loss:  1.88982231e-05, mean val. loss:  2.39687848e+00\n",
      "Epoch: 12308 mean train loss:  1.87219703e-05, mean val. loss:  2.39697504e+00\n",
      "Epoch: 12309 mean train loss:  1.89860584e-05, mean val. loss:  2.39706922e+00\n",
      "Epoch: 12310 mean train loss:  1.88479898e-05, mean val. loss:  2.39721060e+00\n",
      "Epoch: 12311 mean train loss:  1.89382699e-05, mean val. loss:  2.39734221e+00\n",
      "Epoch: 12312 mean train loss:  1.87113765e-05, mean val. loss:  2.39747429e+00\n",
      "Epoch: 12313 mean train loss:  1.86051766e-05, mean val. loss:  2.39758301e+00\n",
      "Epoch: 12314 mean train loss:  1.90437131e-05, mean val. loss:  2.39768744e+00\n",
      "Epoch: 12315 mean train loss:  1.87827391e-05, mean val. loss:  2.39781046e+00\n",
      "Epoch: 12316 mean train loss:  1.83703960e-05, mean val. loss:  2.39799023e+00\n",
      "Epoch: 12317 mean train loss:  1.84840173e-05, mean val. loss:  2.39819622e+00\n",
      "Epoch: 12318 mean train loss:  1.86489488e-05, mean val. loss:  2.39841318e+00\n",
      "Epoch: 12319 mean train loss:  1.82804069e-05, mean val. loss:  2.39866853e+00\n",
      "Epoch: 12320 mean train loss:  1.87262194e-05, mean val. loss:  2.39893770e+00\n",
      "Epoch: 12321 mean train loss:  1.82752847e-05, mean val. loss:  2.39923382e+00\n",
      "Epoch: 12322 mean train loss:  1.86033431e-05, mean val. loss:  2.39953303e+00\n",
      "Epoch: 12323 mean train loss:  1.87027326e-05, mean val. loss:  2.39984059e+00\n",
      "Epoch: 12324 mean train loss:  1.86485995e-05, mean val. loss:  2.40014696e+00\n",
      "Epoch: 12325 mean train loss:  1.86995021e-05, mean val. loss:  2.40049243e+00\n",
      "Epoch: 12326 mean train loss:  1.86628313e-05, mean val. loss:  2.40084410e+00\n",
      "Epoch: 12327 mean train loss:  1.85341923e-05, mean val. loss:  2.40121794e+00\n",
      "Epoch: 12328 mean train loss:  1.88195263e-05, mean val. loss:  2.40155721e+00\n",
      "Epoch: 12329 mean train loss:  1.83152733e-05, mean val. loss:  2.40189219e+00\n",
      "Epoch: 12330 mean train loss:  1.86465331e-05, mean val. loss:  2.40221691e+00\n",
      "Epoch: 12331 mean train loss:  1.90351566e-05, mean val. loss:  2.40250659e+00\n",
      "Epoch: 12332 mean train loss:  1.86465622e-05, mean val. loss:  2.40281701e+00\n",
      "Epoch: 12333 mean train loss:  1.85733079e-05, mean val. loss:  2.40310097e+00\n",
      "Epoch: 12334 mean train loss:  1.87961268e-05, mean val. loss:  2.40337491e+00\n",
      "Epoch: 12335 mean train loss:  1.86063116e-05, mean val. loss:  2.40361428e+00\n",
      "Epoch: 12336 mean train loss:  1.90067512e-05, mean val. loss:  2.40379286e+00\n",
      "Epoch: 12337 mean train loss:  1.86940306e-05, mean val. loss:  2.40394330e+00\n",
      "Epoch: 12338 mean train loss:  1.85866666e-05, mean val. loss:  2.40406322e+00\n",
      "Epoch: 12339 mean train loss:  1.83117227e-05, mean val. loss:  2.40421796e+00\n",
      "Epoch: 12340 mean train loss:  1.87652186e-05, mean val. loss:  2.40437031e+00\n",
      "Epoch: 12341 mean train loss:  1.85665849e-05, mean val. loss:  2.40450144e+00\n",
      "Epoch: 12342 mean train loss:  1.89204293e-05, mean val. loss:  2.40458608e+00\n",
      "Epoch: 12343 mean train loss:  1.86194666e-05, mean val. loss:  2.40465474e+00\n",
      "Epoch: 12344 mean train loss:  1.84702512e-05, mean val. loss:  2.40474153e+00\n",
      "Epoch: 12345 mean train loss:  1.85273821e-05, mean val. loss:  2.40484428e+00\n",
      "Epoch: 12346 mean train loss:  1.86350662e-05, mean val. loss:  2.40492940e+00\n",
      "Epoch: 12347 mean train loss:  1.86795078e-05, mean val. loss:  2.40505004e+00\n",
      "Epoch: 12348 mean train loss:  1.87635305e-05, mean val. loss:  2.40517902e+00\n",
      "Epoch: 12349 mean train loss:  1.84729288e-05, mean val. loss:  2.40535259e+00\n",
      "Epoch: 12350 mean train loss:  1.88265694e-05, mean val. loss:  2.40554190e+00\n",
      "Epoch: 12351 mean train loss:  1.86578254e-05, mean val. loss:  2.40572977e+00\n",
      "Epoch: 12352 mean train loss:  1.83983939e-05, mean val. loss:  2.40594482e+00\n",
      "Epoch: 12353 mean train loss:  1.85124227e-05, mean val. loss:  2.40616918e+00\n",
      "Epoch: 12354 mean train loss:  1.87678088e-05, mean val. loss:  2.40638471e+00\n",
      "Epoch: 12355 mean train loss:  1.87680998e-05, mean val. loss:  2.40660214e+00\n",
      "Epoch: 12356 mean train loss:  1.85790122e-05, mean val. loss:  2.40681791e+00\n",
      "Epoch: 12357 mean train loss:  1.85995596e-05, mean val. loss:  2.40703440e+00\n",
      "Epoch: 12358 mean train loss:  1.86087564e-05, mean val. loss:  2.40722227e+00\n",
      "Epoch: 12359 mean train loss:  1.86898978e-05, mean val. loss:  2.40740967e+00\n",
      "Epoch: 12360 mean train loss:  1.85459503e-05, mean val. loss:  2.40759611e+00\n",
      "Epoch: 12361 mean train loss:  1.85963581e-05, mean val. loss:  2.40780401e+00\n",
      "Epoch: 12362 mean train loss:  1.83135271e-05, mean val. loss:  2.40803027e+00\n",
      "Epoch: 12363 mean train loss:  1.84561650e-05, mean val. loss:  2.40825224e+00\n",
      "Epoch: 12364 mean train loss:  1.88338745e-05, mean val. loss:  2.40844870e+00\n",
      "Epoch: 12365 mean train loss:  1.82721706e-05, mean val. loss:  2.40866995e+00\n",
      "Epoch: 12366 mean train loss:  1.86222314e-05, mean val. loss:  2.40887332e+00\n",
      "Epoch: 12367 mean train loss:  1.82485965e-05, mean val. loss:  2.40909433e+00\n",
      "Epoch: 12368 mean train loss:  1.85274112e-05, mean val. loss:  2.40933228e+00\n",
      "Epoch: 12369 mean train loss:  1.84261589e-05, mean val. loss:  2.40957785e+00\n",
      "Epoch: 12370 mean train loss:  1.87887636e-05, mean val. loss:  2.40981078e+00\n",
      "Epoch: 12371 mean train loss:  1.87915284e-05, mean val. loss:  2.41004086e+00\n",
      "Epoch: 12372 mean train loss:  1.86959514e-05, mean val. loss:  2.41027474e+00\n",
      "Epoch: 12373 mean train loss:  1.88027625e-05, mean val. loss:  2.41048956e+00\n",
      "Epoch: 12374 mean train loss:  1.86665857e-05, mean val. loss:  2.41065598e+00\n",
      "Epoch: 12375 mean train loss:  1.88704289e-05, mean val. loss:  2.41078949e+00\n",
      "Epoch: 12376 mean train loss:  1.90967112e-05, mean val. loss:  2.41086769e+00\n",
      "Epoch: 12377 mean train loss:  1.85442623e-05, mean val. loss:  2.41091704e+00\n",
      "Epoch: 12378 mean train loss:  1.86657708e-05, mean val. loss:  2.41094446e+00\n",
      "Epoch: 12379 mean train loss:  1.87416153e-05, mean val. loss:  2.41090608e+00\n",
      "Epoch: 12380 mean train loss:  1.88331760e-05, mean val. loss:  2.41088772e+00\n",
      "Epoch: 12381 mean train loss:  1.87159167e-05, mean val. loss:  2.41083670e+00\n",
      "Epoch: 12382 mean train loss:  1.82159420e-05, mean val. loss:  2.41078591e+00\n",
      "Epoch: 12383 mean train loss:  1.87221158e-05, mean val. loss:  2.41071129e+00\n",
      "Epoch: 12384 mean train loss:  1.84197561e-05, mean val. loss:  2.41065240e+00\n",
      "Epoch: 12385 mean train loss:  1.85969402e-05, mean val. loss:  2.41063094e+00\n",
      "Epoch: 12386 mean train loss:  1.88140257e-05, mean val. loss:  2.41062975e+00\n",
      "Epoch: 12387 mean train loss:  1.87742407e-05, mean val. loss:  2.41063452e+00\n",
      "Epoch: 12388 mean train loss:  1.85764220e-05, mean val. loss:  2.41066575e+00\n",
      "Epoch: 12389 mean train loss:  1.84817764e-05, mean val. loss:  2.41077638e+00\n",
      "Epoch: 12390 mean train loss:  1.86374818e-05, mean val. loss:  2.41095448e+00\n",
      "Epoch: 12391 mean train loss:  1.85058743e-05, mean val. loss:  2.41120076e+00\n",
      "Epoch: 12392 mean train loss:  1.87121914e-05, mean val. loss:  2.41150022e+00\n",
      "Epoch: 12393 mean train loss:  1.87247351e-05, mean val. loss:  2.41183352e+00\n",
      "Epoch: 12394 mean train loss:  1.88391423e-05, mean val. loss:  2.41217852e+00\n",
      "Epoch: 12395 mean train loss:  1.85724639e-05, mean val. loss:  2.41253614e+00\n",
      "Epoch: 12396 mean train loss:  1.83868979e-05, mean val. loss:  2.41292667e+00\n",
      "Epoch: 12397 mean train loss:  1.86942052e-05, mean val. loss:  2.41332173e+00\n",
      "Epoch: 12398 mean train loss:  1.88542472e-05, mean val. loss:  2.41370749e+00\n",
      "Epoch: 12399 mean train loss:  1.84158853e-05, mean val. loss:  2.41407847e+00\n",
      "Epoch: 12400 mean train loss:  1.84736273e-05, mean val. loss:  2.41442394e+00\n",
      "Epoch: 12401 mean train loss:  1.87137048e-05, mean val. loss:  2.41474676e+00\n",
      "Epoch: 12402 mean train loss:  1.83575321e-05, mean val. loss:  2.41506958e+00\n",
      "Epoch: 12403 mean train loss:  1.83864613e-05, mean val. loss:  2.41537070e+00\n",
      "Epoch: 12404 mean train loss:  1.88193226e-05, mean val. loss:  2.41563821e+00\n",
      "Epoch: 12405 mean train loss:  1.84449309e-05, mean val. loss:  2.41589260e+00\n",
      "Epoch: 12406 mean train loss:  1.85490935e-05, mean val. loss:  2.41611981e+00\n",
      "Epoch: 12407 mean train loss:  1.88395497e-05, mean val. loss:  2.41632366e+00\n",
      "Epoch: 12408 mean train loss:  1.85269164e-05, mean val. loss:  2.41650581e+00\n",
      "Epoch: 12409 mean train loss:  1.87477272e-05, mean val. loss:  2.41664910e+00\n",
      "Epoch: 12410 mean train loss:  1.86114921e-05, mean val. loss:  2.41679597e+00\n",
      "Epoch: 12411 mean train loss:  1.87898695e-05, mean val. loss:  2.41694665e+00\n",
      "Epoch: 12412 mean train loss:  1.87538681e-05, mean val. loss:  2.41708994e+00\n",
      "Epoch: 12413 mean train loss:  1.87727856e-05, mean val. loss:  2.41720343e+00\n",
      "Epoch: 12414 mean train loss:  1.84854434e-05, mean val. loss:  2.41732359e+00\n",
      "Epoch: 12415 mean train loss:  1.86546531e-05, mean val. loss:  2.41741490e+00\n",
      "Epoch: 12416 mean train loss:  1.86176621e-05, mean val. loss:  2.41749191e+00\n",
      "Epoch: 12417 mean train loss:  1.88030244e-05, mean val. loss:  2.41755581e+00\n",
      "Epoch: 12418 mean train loss:  1.85580866e-05, mean val. loss:  2.41763711e+00\n",
      "Epoch: 12419 mean train loss:  1.84231321e-05, mean val. loss:  2.41772103e+00\n",
      "Epoch: 12420 mean train loss:  1.86433608e-05, mean val. loss:  2.41781425e+00\n",
      "Epoch: 12421 mean train loss:  1.87767437e-05, mean val. loss:  2.41790891e+00\n",
      "Epoch: 12422 mean train loss:  1.83490629e-05, mean val. loss:  2.41802216e+00\n",
      "Epoch: 12423 mean train loss:  1.81225478e-05, mean val. loss:  2.41817784e+00\n",
      "Epoch: 12424 mean train loss:  1.84286619e-05, mean val. loss:  2.41833782e+00\n",
      "Epoch: 12425 mean train loss:  1.84247911e-05, mean val. loss:  2.41856408e+00\n",
      "Epoch: 12426 mean train loss:  1.85202225e-05, mean val. loss:  2.41881061e+00\n",
      "Epoch: 12427 mean train loss:  1.87093392e-05, mean val. loss:  2.41904807e+00\n",
      "Epoch: 12428 mean train loss:  1.83363154e-05, mean val. loss:  2.41931200e+00\n",
      "Epoch: 12429 mean train loss:  1.86450488e-05, mean val. loss:  2.41958880e+00\n",
      "Epoch: 12430 mean train loss:  1.84791279e-05, mean val. loss:  2.41987348e+00\n",
      "Epoch: 12431 mean train loss:  1.85648969e-05, mean val. loss:  2.42017317e+00\n",
      "Epoch: 12432 mean train loss:  1.85706303e-05, mean val. loss:  2.42046881e+00\n",
      "Epoch: 12433 mean train loss:  1.86948164e-05, mean val. loss:  2.42073178e+00\n",
      "Epoch: 12434 mean train loss:  1.84038945e-05, mean val. loss:  2.42099118e+00\n",
      "Epoch: 12435 mean train loss:  1.84924284e-05, mean val. loss:  2.42123127e+00\n",
      "Epoch: 12436 mean train loss:  1.88507256e-05, mean val. loss:  2.42143559e+00\n",
      "Epoch: 12437 mean train loss:  1.85444369e-05, mean val. loss:  2.42162299e+00\n",
      "Epoch: 12438 mean train loss:  1.84742385e-05, mean val. loss:  2.42178082e+00\n",
      "Epoch: 12439 mean train loss:  1.84770906e-05, mean val. loss:  2.42194629e+00\n",
      "Epoch: 12440 mean train loss:  1.85530516e-05, mean val. loss:  2.42210031e+00\n",
      "Epoch: 12441 mean train loss:  1.85271492e-05, mean val. loss:  2.42223573e+00\n",
      "Epoch: 12442 mean train loss:  1.85059616e-05, mean val. loss:  2.42231822e+00\n",
      "Epoch: 12443 mean train loss:  1.84530218e-05, mean val. loss:  2.42239237e+00\n",
      "Epoch: 12444 mean train loss:  1.85411191e-05, mean val. loss:  2.42245388e+00\n",
      "Epoch: 12445 mean train loss:  1.85801764e-05, mean val. loss:  2.42252517e+00\n",
      "Epoch: 12446 mean train loss:  1.87284022e-05, mean val. loss:  2.42258286e+00\n",
      "Epoch: 12447 mean train loss:  1.85570389e-05, mean val. loss:  2.42261100e+00\n",
      "Epoch: 12448 mean train loss:  1.88072736e-05, mean val. loss:  2.42261577e+00\n",
      "Epoch: 12449 mean train loss:  1.86237157e-05, mean val. loss:  2.42260861e+00\n",
      "Epoch: 12450 mean train loss:  1.81495270e-05, mean val. loss:  2.42261028e+00\n",
      "Epoch: 12451 mean train loss:  1.88163831e-05, mean val. loss:  2.42258310e+00\n",
      "Epoch: 12452 mean train loss:  1.85687677e-05, mean val. loss:  2.42257786e+00\n",
      "Epoch: 12453 mean train loss:  1.85619574e-05, mean val. loss:  2.42256761e+00\n",
      "Epoch: 12454 mean train loss:  1.84173696e-05, mean val. loss:  2.42259002e+00\n",
      "Epoch: 12455 mean train loss:  1.84098899e-05, mean val. loss:  2.42265105e+00\n",
      "Epoch: 12456 mean train loss:  1.84498786e-05, mean val. loss:  2.42273521e+00\n",
      "Epoch: 12457 mean train loss:  1.83514494e-05, mean val. loss:  2.42288399e+00\n",
      "Epoch: 12458 mean train loss:  1.89310813e-05, mean val. loss:  2.42304683e+00\n",
      "Epoch: 12459 mean train loss:  1.84686214e-05, mean val. loss:  2.42324734e+00\n",
      "Epoch: 12460 mean train loss:  1.85770041e-05, mean val. loss:  2.42344928e+00\n",
      "Epoch: 12461 mean train loss:  1.86654215e-05, mean val. loss:  2.42366600e+00\n",
      "Epoch: 12462 mean train loss:  1.85548270e-05, mean val. loss:  2.42389607e+00\n",
      "Epoch: 12463 mean train loss:  1.85196695e-05, mean val. loss:  2.42416763e+00\n",
      "Epoch: 12464 mean train loss:  1.84313394e-05, mean val. loss:  2.42447329e+00\n",
      "Epoch: 12465 mean train loss:  1.82670483e-05, mean val. loss:  2.42481160e+00\n",
      "Epoch: 12466 mean train loss:  1.87098922e-05, mean val. loss:  2.42516565e+00\n",
      "Epoch: 12467 mean train loss:  1.87262776e-05, mean val. loss:  2.42551160e+00\n",
      "Epoch: 12468 mean train loss:  1.83403026e-05, mean val. loss:  2.42585111e+00\n",
      "Epoch: 12469 mean train loss:  1.84116070e-05, mean val. loss:  2.42621469e+00\n",
      "Epoch: 12470 mean train loss:  1.83492957e-05, mean val. loss:  2.42653632e+00\n",
      "Epoch: 12471 mean train loss:  1.84005767e-05, mean val. loss:  2.42685151e+00\n",
      "Epoch: 12472 mean train loss:  1.86229590e-05, mean val. loss:  2.42713737e+00\n",
      "Epoch: 12473 mean train loss:  1.86242687e-05, mean val. loss:  2.42738104e+00\n",
      "Epoch: 12474 mean train loss:  1.88024715e-05, mean val. loss:  2.42757607e+00\n",
      "Epoch: 12475 mean train loss:  1.82447839e-05, mean val. loss:  2.42778945e+00\n",
      "Epoch: 12476 mean train loss:  1.83772354e-05, mean val. loss:  2.42799187e+00\n",
      "Epoch: 12477 mean train loss:  1.88776467e-05, mean val. loss:  2.42814326e+00\n",
      "Epoch: 12478 mean train loss:  1.84129749e-05, mean val. loss:  2.42827964e+00\n",
      "Epoch: 12479 mean train loss:  1.86107645e-05, mean val. loss:  2.42839766e+00\n",
      "Epoch: 12480 mean train loss:  1.86009274e-05, mean val. loss:  2.42851090e+00\n",
      "Epoch: 12481 mean train loss:  1.86368125e-05, mean val. loss:  2.42860460e+00\n",
      "Epoch: 12482 mean train loss:  1.86744437e-05, mean val. loss:  2.42871046e+00\n",
      "Epoch: 12483 mean train loss:  1.87591359e-05, mean val. loss:  2.42883873e+00\n",
      "Epoch: 12484 mean train loss:  1.84744422e-05, mean val. loss:  2.42897654e+00\n",
      "Epoch: 12485 mean train loss:  1.84209784e-05, mean val. loss:  2.42916417e+00\n",
      "Epoch: 12486 mean train loss:  1.83224620e-05, mean val. loss:  2.42935848e+00\n",
      "Epoch: 12487 mean train loss:  1.84014789e-05, mean val. loss:  2.42956614e+00\n",
      "Epoch: 12488 mean train loss:  1.85940298e-05, mean val. loss:  2.42981887e+00\n",
      "Epoch: 12489 mean train loss:  1.85064564e-05, mean val. loss:  2.43006897e+00\n",
      "Epoch: 12490 mean train loss:  1.84439123e-05, mean val. loss:  2.43031287e+00\n",
      "Epoch: 12491 mean train loss:  1.86582038e-05, mean val. loss:  2.43053317e+00\n",
      "Epoch: 12492 mean train loss:  1.86453690e-05, mean val. loss:  2.43076539e+00\n",
      "Epoch: 12493 mean train loss:  1.84430100e-05, mean val. loss:  2.43102741e+00\n",
      "Epoch: 12494 mean train loss:  1.86195830e-05, mean val. loss:  2.43127036e+00\n",
      "Epoch: 12495 mean train loss:  1.83151278e-05, mean val. loss:  2.43150067e+00\n",
      "Epoch: 12496 mean train loss:  1.89572456e-05, mean val. loss:  2.43168402e+00\n",
      "Epoch: 12497 mean train loss:  1.87232508e-05, mean val. loss:  2.43184257e+00\n",
      "Epoch: 12498 mean train loss:  1.86325924e-05, mean val. loss:  2.43200350e+00\n",
      "Epoch: 12499 mean train loss:  1.85554090e-05, mean val. loss:  2.43215799e+00\n",
      "Epoch: 12500 mean train loss:  1.86982215e-05, mean val. loss:  2.43228817e+00\n",
      "Epoch: 12501 mean train loss:  1.82168151e-05, mean val. loss:  2.43241405e+00\n",
      "Epoch: 12502 mean train loss:  1.81831711e-05, mean val. loss:  2.43254828e+00\n",
      "Epoch: 12503 mean train loss:  1.85842218e-05, mean val. loss:  2.43268561e+00\n",
      "Epoch: 12504 mean train loss:  1.85744138e-05, mean val. loss:  2.43281293e+00\n",
      "Epoch: 12505 mean train loss:  1.86543912e-05, mean val. loss:  2.43295717e+00\n",
      "Epoch: 12506 mean train loss:  1.84180099e-05, mean val. loss:  2.43312120e+00\n",
      "Epoch: 12507 mean train loss:  1.85092213e-05, mean val. loss:  2.43327141e+00\n",
      "Epoch: 12508 mean train loss:  1.85619865e-05, mean val. loss:  2.43341875e+00\n",
      "Epoch: 12509 mean train loss:  1.85166136e-05, mean val. loss:  2.43357754e+00\n",
      "Epoch: 12510 mean train loss:  1.83140219e-05, mean val. loss:  2.43375707e+00\n",
      "Epoch: 12511 mean train loss:  1.83816301e-05, mean val. loss:  2.43394041e+00\n",
      "Epoch: 12512 mean train loss:  1.87685655e-05, mean val. loss:  2.43410397e+00\n",
      "Epoch: 12513 mean train loss:  1.85829413e-05, mean val. loss:  2.43423867e+00\n",
      "Epoch: 12514 mean train loss:  1.85144891e-05, mean val. loss:  2.43438101e+00\n",
      "Epoch: 12515 mean train loss:  1.85802637e-05, mean val. loss:  2.43452096e+00\n",
      "Epoch: 12516 mean train loss:  1.89265993e-05, mean val. loss:  2.43464947e+00\n",
      "Epoch: 12517 mean train loss:  1.85125973e-05, mean val. loss:  2.43479156e+00\n",
      "Epoch: 12518 mean train loss:  1.85827957e-05, mean val. loss:  2.43496037e+00\n",
      "Epoch: 12519 mean train loss:  1.83276716e-05, mean val. loss:  2.43515754e+00\n",
      "Epoch: 12520 mean train loss:  1.83240627e-05, mean val. loss:  2.43536973e+00\n",
      "Epoch: 12521 mean train loss:  1.86765683e-05, mean val. loss:  2.43559718e+00\n",
      "Epoch: 12522 mean train loss:  1.83784286e-05, mean val. loss:  2.43581295e+00\n",
      "Epoch: 12523 mean train loss:  1.83887896e-05, mean val. loss:  2.43602943e+00\n",
      "Epoch: 12524 mean train loss:  1.87034020e-05, mean val. loss:  2.43623400e+00\n",
      "Epoch: 12525 mean train loss:  1.87950500e-05, mean val. loss:  2.43642759e+00\n",
      "Epoch: 12526 mean train loss:  1.83265656e-05, mean val. loss:  2.43662500e+00\n",
      "Epoch: 12527 mean train loss:  1.87320693e-05, mean val. loss:  2.43682384e+00\n",
      "Epoch: 12528 mean train loss:  1.84495293e-05, mean val. loss:  2.43700910e+00\n",
      "Epoch: 12529 mean train loss:  1.83167576e-05, mean val. loss:  2.43718982e+00\n",
      "Epoch: 12530 mean train loss:  1.89296843e-05, mean val. loss:  2.43733859e+00\n",
      "Epoch: 12531 mean train loss:  1.86355901e-05, mean val. loss:  2.43748426e+00\n",
      "Epoch: 12532 mean train loss:  1.85189128e-05, mean val. loss:  2.43762207e+00\n",
      "Epoch: 12533 mean train loss:  1.84542150e-05, mean val. loss:  2.43773270e+00\n",
      "Epoch: 12534 mean train loss:  1.82726653e-05, mean val. loss:  2.43783188e+00\n",
      "Epoch: 12535 mean train loss:  1.82990043e-05, mean val. loss:  2.43794489e+00\n",
      "Epoch: 12536 mean train loss:  1.86281395e-05, mean val. loss:  2.43804860e+00\n",
      "Epoch: 12537 mean train loss:  1.86475518e-05, mean val. loss:  2.43811417e+00\n",
      "Epoch: 12538 mean train loss:  1.83875381e-05, mean val. loss:  2.43819237e+00\n",
      "Epoch: 12539 mean train loss:  1.81267969e-05, mean val. loss:  2.43829846e+00\n",
      "Epoch: 12540 mean train loss:  1.85152458e-05, mean val. loss:  2.43839574e+00\n",
      "Epoch: 12541 mean train loss:  1.86777324e-05, mean val. loss:  2.43847966e+00\n",
      "Epoch: 12542 mean train loss:  1.86131510e-05, mean val. loss:  2.43855357e+00\n",
      "Epoch: 12543 mean train loss:  1.83296797e-05, mean val. loss:  2.43862295e+00\n",
      "Epoch: 12544 mean train loss:  1.85099489e-05, mean val. loss:  2.43870592e+00\n",
      "Epoch: 12545 mean train loss:  1.87078840e-05, mean val. loss:  2.43879580e+00\n",
      "Epoch: 12546 mean train loss:  1.82544172e-05, mean val. loss:  2.43889976e+00\n",
      "Epoch: 12547 mean train loss:  1.83813681e-05, mean val. loss:  2.43905258e+00\n",
      "Epoch: 12548 mean train loss:  1.86626567e-05, mean val. loss:  2.43920755e+00\n",
      "Epoch: 12549 mean train loss:  1.83625089e-05, mean val. loss:  2.43941879e+00\n",
      "Epoch: 12550 mean train loss:  1.84018572e-05, mean val. loss:  2.43964100e+00\n",
      "Epoch: 12551 mean train loss:  1.86529942e-05, mean val. loss:  2.43983865e+00\n",
      "Epoch: 12552 mean train loss:  1.84080272e-05, mean val. loss:  2.44008923e+00\n",
      "Epoch: 12553 mean train loss:  1.83805823e-05, mean val. loss:  2.44037390e+00\n",
      "Epoch: 12554 mean train loss:  1.83875673e-05, mean val. loss:  2.44067931e+00\n",
      "Epoch: 12555 mean train loss:  1.86322432e-05, mean val. loss:  2.44101596e+00\n",
      "Epoch: 12556 mean train loss:  1.81945215e-05, mean val. loss:  2.44136882e+00\n",
      "Epoch: 12557 mean train loss:  1.85716199e-05, mean val. loss:  2.44169688e+00\n",
      "Epoch: 12558 mean train loss:  1.85686513e-05, mean val. loss:  2.44201183e+00\n",
      "Epoch: 12559 mean train loss:  1.83834636e-05, mean val. loss:  2.44231224e+00\n",
      "Epoch: 12560 mean train loss:  1.87521800e-05, mean val. loss:  2.44257760e+00\n",
      "Epoch: 12561 mean train loss:  1.83886150e-05, mean val. loss:  2.44285035e+00\n",
      "Epoch: 12562 mean train loss:  1.84755190e-05, mean val. loss:  2.44307899e+00\n",
      "Epoch: 12563 mean train loss:  1.84752571e-05, mean val. loss:  2.44328856e+00\n",
      "Epoch: 12564 mean train loss:  1.83403026e-05, mean val. loss:  2.44349933e+00\n",
      "Epoch: 12565 mean train loss:  1.84561359e-05, mean val. loss:  2.44367814e+00\n",
      "Epoch: 12566 mean train loss:  1.87306723e-05, mean val. loss:  2.44381690e+00\n",
      "Epoch: 12567 mean train loss:  1.84166420e-05, mean val. loss:  2.44396257e+00\n",
      "Epoch: 12568 mean train loss:  1.82837830e-05, mean val. loss:  2.44411850e+00\n",
      "Epoch: 12569 mean train loss:  1.85069512e-05, mean val. loss:  2.44426513e+00\n",
      "Epoch: 12570 mean train loss:  1.87920814e-05, mean val. loss:  2.44438148e+00\n",
      "Epoch: 12571 mean train loss:  1.83282827e-05, mean val. loss:  2.44453573e+00\n",
      "Epoch: 12572 mean train loss:  1.86169345e-05, mean val. loss:  2.44467545e+00\n",
      "Epoch: 12573 mean train loss:  1.85097742e-05, mean val. loss:  2.44483352e+00\n",
      "Epoch: 12574 mean train loss:  1.83577649e-05, mean val. loss:  2.44501662e+00\n",
      "Epoch: 12575 mean train loss:  1.82735093e-05, mean val. loss:  2.44521236e+00\n",
      "Epoch: 12576 mean train loss:  1.85135577e-05, mean val. loss:  2.44541717e+00\n",
      "Epoch: 12577 mean train loss:  1.82038348e-05, mean val. loss:  2.44564843e+00\n",
      "Epoch: 12578 mean train loss:  1.82003423e-05, mean val. loss:  2.44588518e+00\n",
      "Epoch: 12579 mean train loss:  1.82120712e-05, mean val. loss:  2.44615984e+00\n",
      "Epoch: 12580 mean train loss:  1.84404198e-05, mean val. loss:  2.44642282e+00\n",
      "Epoch: 12581 mean train loss:  1.81982468e-05, mean val. loss:  2.44672036e+00\n",
      "Epoch: 12582 mean train loss:  1.83948432e-05, mean val. loss:  2.44704056e+00\n",
      "Epoch: 12583 mean train loss:  1.82568328e-05, mean val. loss:  2.44738054e+00\n",
      "Epoch: 12584 mean train loss:  1.85039244e-05, mean val. loss:  2.44774461e+00\n",
      "Epoch: 12585 mean train loss:  1.85633544e-05, mean val. loss:  2.44809723e+00\n",
      "Epoch: 12586 mean train loss:  1.84959499e-05, mean val. loss:  2.44843173e+00\n",
      "Epoch: 12587 mean train loss:  1.82878866e-05, mean val. loss:  2.44874549e+00\n",
      "Epoch: 12588 mean train loss:  1.83125376e-05, mean val. loss:  2.44906402e+00\n",
      "Epoch: 12589 mean train loss:  1.86686812e-05, mean val. loss:  2.44936609e+00\n",
      "Epoch: 12590 mean train loss:  1.84451055e-05, mean val. loss:  2.44966197e+00\n",
      "Epoch: 12591 mean train loss:  1.84495584e-05, mean val. loss:  2.44994712e+00\n",
      "Epoch: 12592 mean train loss:  1.84004602e-05, mean val. loss:  2.45023537e+00\n",
      "Epoch: 12593 mean train loss:  1.86414400e-05, mean val. loss:  2.45050073e+00\n",
      "Epoch: 12594 mean train loss:  1.82330259e-05, mean val. loss:  2.45075321e+00\n",
      "Epoch: 12595 mean train loss:  1.82565418e-05, mean val. loss:  2.45099568e+00\n",
      "Epoch: 12596 mean train loss:  1.82194344e-05, mean val. loss:  2.45122361e+00\n",
      "Epoch: 12597 mean train loss:  1.84946111e-05, mean val. loss:  2.45145392e+00\n",
      "Epoch: 12598 mean train loss:  1.86750549e-05, mean val. loss:  2.45165944e+00\n",
      "Epoch: 12599 mean train loss:  1.89823331e-05, mean val. loss:  2.45180345e+00\n",
      "Epoch: 12600 mean train loss:  1.83765078e-05, mean val. loss:  2.45196152e+00\n",
      "Epoch: 12601 mean train loss:  1.88616104e-05, mean val. loss:  2.45209908e+00\n",
      "Epoch: 12602 mean train loss:  1.84679229e-05, mean val. loss:  2.45222616e+00\n",
      "Epoch: 12603 mean train loss:  1.81609648e-05, mean val. loss:  2.45233583e+00\n",
      "Epoch: 12604 mean train loss:  1.89353887e-05, mean val. loss:  2.45238423e+00\n",
      "Epoch: 12605 mean train loss:  1.77630573e-05, mean val. loss:  2.45239353e+00\n",
      "Epoch: 12606 mean train loss:  1.86479010e-05, mean val. loss:  2.45235848e+00\n",
      "Epoch: 12607 mean train loss:  1.82007207e-05, mean val. loss:  2.45231843e+00\n",
      "Epoch: 12608 mean train loss:  1.85316021e-05, mean val. loss:  2.45221138e+00\n",
      "Epoch: 12609 mean train loss:  1.81326759e-05, mean val. loss:  2.45205736e+00\n",
      "Epoch: 12610 mean train loss:  1.83711818e-05, mean val. loss:  2.45191097e+00\n",
      "Epoch: 12611 mean train loss:  1.85490353e-05, mean val. loss:  2.45175719e+00\n",
      "Epoch: 12612 mean train loss:  1.84990640e-05, mean val. loss:  2.45160556e+00\n",
      "Epoch: 12613 mean train loss:  1.84571545e-05, mean val. loss:  2.45146847e+00\n",
      "Epoch: 12614 mean train loss:  1.85690005e-05, mean val. loss:  2.45135641e+00\n",
      "Epoch: 12615 mean train loss:  1.86429534e-05, mean val. loss:  2.45125484e+00\n",
      "Epoch: 12616 mean train loss:  1.81861687e-05, mean val. loss:  2.45122695e+00\n",
      "Epoch: 12617 mean train loss:  1.85688841e-05, mean val. loss:  2.45122123e+00\n",
      "Epoch: 12618 mean train loss:  1.82917283e-05, mean val. loss:  2.45128965e+00\n",
      "Epoch: 12619 mean train loss:  1.82974036e-05, mean val. loss:  2.45142722e+00\n",
      "Epoch: 12620 mean train loss:  1.83872471e-05, mean val. loss:  2.45162320e+00\n",
      "Epoch: 12621 mean train loss:  1.88158010e-05, mean val. loss:  2.45188355e+00\n",
      "Epoch: 12622 mean train loss:  1.85247627e-05, mean val. loss:  2.45220089e+00\n",
      "Epoch: 12623 mean train loss:  1.87263067e-05, mean val. loss:  2.45253420e+00\n",
      "Epoch: 12624 mean train loss:  1.86783727e-05, mean val. loss:  2.45289803e+00\n",
      "Epoch: 12625 mean train loss:  1.88480481e-05, mean val. loss:  2.45329213e+00\n",
      "Epoch: 12626 mean train loss:  1.85361132e-05, mean val. loss:  2.45367169e+00\n",
      "Epoch: 12627 mean train loss:  1.84561359e-05, mean val. loss:  2.45402145e+00\n",
      "Epoch: 12628 mean train loss:  1.82613730e-05, mean val. loss:  2.45434451e+00\n",
      "Epoch: 12629 mean train loss:  1.79862254e-05, mean val. loss:  2.45467806e+00\n",
      "Epoch: 12630 mean train loss:  1.84733071e-05, mean val. loss:  2.45498133e+00\n",
      "Epoch: 12631 mean train loss:  1.87459518e-05, mean val. loss:  2.45525479e+00\n",
      "Epoch: 12632 mean train loss:  1.85561948e-05, mean val. loss:  2.45550632e+00\n",
      "Epoch: 12633 mean train loss:  1.80060742e-05, mean val. loss:  2.45576048e+00\n",
      "Epoch: 12634 mean train loss:  1.84814271e-05, mean val. loss:  2.45600176e+00\n",
      "Epoch: 12635 mean train loss:  1.82762451e-05, mean val. loss:  2.45621777e+00\n",
      "Epoch: 12636 mean train loss:  1.82829972e-05, mean val. loss:  2.45642805e+00\n",
      "Epoch: 12637 mean train loss:  1.85187673e-05, mean val. loss:  2.45663834e+00\n",
      "Epoch: 12638 mean train loss:  1.82209769e-05, mean val. loss:  2.45686412e+00\n",
      "Epoch: 12639 mean train loss:  1.79607887e-05, mean val. loss:  2.45710778e+00\n",
      "Epoch: 12640 mean train loss:  1.86102698e-05, mean val. loss:  2.45735049e+00\n",
      "Epoch: 12641 mean train loss:  1.86094840e-05, mean val. loss:  2.45760107e+00\n",
      "Epoch: 12642 mean train loss:  1.84701348e-05, mean val. loss:  2.45784926e+00\n",
      "Epoch: 12643 mean train loss:  1.83722877e-05, mean val. loss:  2.45811057e+00\n",
      "Epoch: 12644 mean train loss:  1.82334625e-05, mean val. loss:  2.45838070e+00\n",
      "Epoch: 12645 mean train loss:  1.86878606e-05, mean val. loss:  2.45863652e+00\n",
      "Epoch: 12646 mean train loss:  1.85337267e-05, mean val. loss:  2.45890975e+00\n",
      "Epoch: 12647 mean train loss:  1.88297709e-05, mean val. loss:  2.45915389e+00\n",
      "Epoch: 12648 mean train loss:  1.84737146e-05, mean val. loss:  2.45939231e+00\n",
      "Epoch: 12649 mean train loss:  1.83072698e-05, mean val. loss:  2.45964479e+00\n",
      "Epoch: 12650 mean train loss:  1.83207158e-05, mean val. loss:  2.45988822e+00\n",
      "Epoch: 12651 mean train loss:  1.84170494e-05, mean val. loss:  2.46012235e+00\n",
      "Epoch: 12652 mean train loss:  1.83288066e-05, mean val. loss:  2.46035051e+00\n",
      "Epoch: 12653 mean train loss:  1.83056400e-05, mean val. loss:  2.46057558e+00\n",
      "Epoch: 12654 mean train loss:  1.86194666e-05, mean val. loss:  2.46078181e+00\n",
      "Epoch: 12655 mean train loss:  1.82145450e-05, mean val. loss:  2.46097398e+00\n",
      "Epoch: 12656 mean train loss:  1.82780495e-05, mean val. loss:  2.46117783e+00\n",
      "Epoch: 12657 mean train loss:  1.82283111e-05, mean val. loss:  2.46135545e+00\n",
      "Epoch: 12658 mean train loss:  1.82449003e-05, mean val. loss:  2.46153021e+00\n",
      "Epoch: 12659 mean train loss:  1.88109989e-05, mean val. loss:  2.46166205e+00\n",
      "Epoch: 12660 mean train loss:  1.84774690e-05, mean val. loss:  2.46175504e+00\n",
      "Epoch: 12661 mean train loss:  1.83843367e-05, mean val. loss:  2.46184182e+00\n",
      "Epoch: 12662 mean train loss:  1.86595134e-05, mean val. loss:  2.46191740e+00\n",
      "Epoch: 12663 mean train loss:  1.85953395e-05, mean val. loss:  2.46197701e+00\n",
      "Epoch: 12664 mean train loss:  1.84819801e-05, mean val. loss:  2.46201324e+00\n",
      "Epoch: 12665 mean train loss:  1.86492107e-05, mean val. loss:  2.46205616e+00\n",
      "Epoch: 12666 mean train loss:  1.85518875e-05, mean val. loss:  2.46207476e+00\n",
      "Epoch: 12667 mean train loss:  1.84119854e-05, mean val. loss:  2.46208882e+00\n",
      "Epoch: 12668 mean train loss:  1.82439107e-05, mean val. loss:  2.46211195e+00\n",
      "Epoch: 12669 mean train loss:  1.82412041e-05, mean val. loss:  2.46213794e+00\n",
      "Epoch: 12670 mean train loss:  1.84541568e-05, mean val. loss:  2.46221280e+00\n",
      "Epoch: 12671 mean train loss:  1.83339871e-05, mean val. loss:  2.46233487e+00\n",
      "Epoch: 12672 mean train loss:  1.82127405e-05, mean val. loss:  2.46249700e+00\n",
      "Epoch: 12673 mean train loss:  1.84003147e-05, mean val. loss:  2.46266651e+00\n",
      "Epoch: 12674 mean train loss:  1.85231329e-05, mean val. loss:  2.46284556e+00\n",
      "Epoch: 12675 mean train loss:  1.84819219e-05, mean val. loss:  2.46303487e+00\n",
      "Epoch: 12676 mean train loss:  1.84694945e-05, mean val. loss:  2.46325517e+00\n",
      "Epoch: 12677 mean train loss:  1.80777570e-05, mean val. loss:  2.46352339e+00\n",
      "Epoch: 12678 mean train loss:  1.84758974e-05, mean val. loss:  2.46379876e+00\n",
      "Epoch: 12679 mean train loss:  1.82535150e-05, mean val. loss:  2.46409583e+00\n",
      "Epoch: 12680 mean train loss:  1.82958902e-05, mean val. loss:  2.46442080e+00\n",
      "Epoch: 12681 mean train loss:  1.84799428e-05, mean val. loss:  2.46474504e+00\n",
      "Epoch: 12682 mean train loss:  1.82069489e-05, mean val. loss:  2.46510839e+00\n",
      "Epoch: 12683 mean train loss:  1.83792436e-05, mean val. loss:  2.46546006e+00\n",
      "Epoch: 12684 mean train loss:  1.82087242e-05, mean val. loss:  2.46579194e+00\n",
      "Epoch: 12685 mean train loss:  1.82449294e-05, mean val. loss:  2.46609282e+00\n",
      "Epoch: 12686 mean train loss:  1.81903888e-05, mean val. loss:  2.46637273e+00\n",
      "Epoch: 12687 mean train loss:  1.84243836e-05, mean val. loss:  2.46661186e+00\n",
      "Epoch: 12688 mean train loss:  1.84667297e-05, mean val. loss:  2.46679330e+00\n",
      "Epoch: 12689 mean train loss:  1.82935328e-05, mean val. loss:  2.46697664e+00\n",
      "Epoch: 12690 mean train loss:  1.85926328e-05, mean val. loss:  2.46715355e+00\n",
      "Epoch: 12691 mean train loss:  1.83542725e-05, mean val. loss:  2.46730828e+00\n",
      "Epoch: 12692 mean train loss:  1.83532829e-05, mean val. loss:  2.46746159e+00\n",
      "Epoch: 12693 mean train loss:  1.80620409e-05, mean val. loss:  2.46762586e+00\n",
      "Epoch: 12694 mean train loss:  1.81982759e-05, mean val. loss:  2.46782947e+00\n",
      "Epoch: 12695 mean train loss:  1.85770332e-05, mean val. loss:  2.46801329e+00\n",
      "Epoch: 12696 mean train loss:  1.83965021e-05, mean val. loss:  2.46819615e+00\n",
      "Epoch: 12697 mean train loss:  1.85067183e-05, mean val. loss:  2.46837044e+00\n",
      "Epoch: 12698 mean train loss:  1.82973745e-05, mean val. loss:  2.46853185e+00\n",
      "Epoch: 12699 mean train loss:  1.85581157e-05, mean val. loss:  2.46869540e+00\n",
      "Epoch: 12700 mean train loss:  1.86010730e-05, mean val. loss:  2.46883059e+00\n",
      "Epoch: 12701 mean train loss:  1.81188516e-05, mean val. loss:  2.46899843e+00\n",
      "Epoch: 12702 mean train loss:  1.85206300e-05, mean val. loss:  2.46915889e+00\n",
      "Epoch: 12703 mean train loss:  1.83836673e-05, mean val. loss:  2.46930218e+00\n",
      "Epoch: 12704 mean train loss:  1.87617843e-05, mean val. loss:  2.46942878e+00\n",
      "Epoch: 12705 mean train loss:  1.82138174e-05, mean val. loss:  2.46956491e+00\n",
      "Epoch: 12706 mean train loss:  1.83384982e-05, mean val. loss:  2.46969748e+00\n",
      "Epoch: 12707 mean train loss:  1.83135562e-05, mean val. loss:  2.46977901e+00\n",
      "Epoch: 12708 mean train loss:  1.83426018e-05, mean val. loss:  2.46984673e+00\n",
      "Epoch: 12709 mean train loss:  1.80878269e-05, mean val. loss:  2.46995234e+00\n",
      "Epoch: 12710 mean train loss:  1.82578515e-05, mean val. loss:  2.47006917e+00\n",
      "Epoch: 12711 mean train loss:  1.82970834e-05, mean val. loss:  2.47021866e+00\n",
      "Epoch: 12712 mean train loss:  1.81526120e-05, mean val. loss:  2.47040153e+00\n",
      "Epoch: 12713 mean train loss:  1.83997035e-05, mean val. loss:  2.47061777e+00\n",
      "Epoch: 12714 mean train loss:  1.82458316e-05, mean val. loss:  2.47087908e+00\n",
      "Epoch: 12715 mean train loss:  1.84807868e-05, mean val. loss:  2.47115636e+00\n",
      "Epoch: 12716 mean train loss:  1.81850628e-05, mean val. loss:  2.47148013e+00\n",
      "Epoch: 12717 mean train loss:  1.84057571e-05, mean val. loss:  2.47180033e+00\n",
      "Epoch: 12718 mean train loss:  1.85982208e-05, mean val. loss:  2.47212434e+00\n",
      "Epoch: 12719 mean train loss:  1.86553807e-05, mean val. loss:  2.47241545e+00\n",
      "Epoch: 12720 mean train loss:  1.81506621e-05, mean val. loss:  2.47272468e+00\n",
      "Epoch: 12721 mean train loss:  1.85301469e-05, mean val. loss:  2.47301793e+00\n",
      "Epoch: 12722 mean train loss:  1.85227836e-05, mean val. loss:  2.47327542e+00\n",
      "Epoch: 12723 mean train loss:  1.83670199e-05, mean val. loss:  2.47352934e+00\n",
      "Epoch: 12724 mean train loss:  1.85982208e-05, mean val. loss:  2.47374606e+00\n",
      "Epoch: 12725 mean train loss:  1.86193793e-05, mean val. loss:  2.47392082e+00\n",
      "Epoch: 12726 mean train loss:  1.85496465e-05, mean val. loss:  2.47403979e+00\n",
      "Epoch: 12727 mean train loss:  1.82004296e-05, mean val. loss:  2.47412205e+00\n",
      "Epoch: 12728 mean train loss:  1.84955425e-05, mean val. loss:  2.47416806e+00\n",
      "Epoch: 12729 mean train loss:  1.82075601e-05, mean val. loss:  2.47418022e+00\n",
      "Epoch: 12730 mean train loss:  1.85727840e-05, mean val. loss:  2.47416902e+00\n",
      "Epoch: 12731 mean train loss:  1.86172547e-05, mean val. loss:  2.47412276e+00\n",
      "Epoch: 12732 mean train loss:  1.81102660e-05, mean val. loss:  2.47409344e+00\n",
      "Epoch: 12733 mean train loss:  1.86498510e-05, mean val. loss:  2.47407508e+00\n",
      "Epoch: 12734 mean train loss:  1.81925134e-05, mean val. loss:  2.47408414e+00\n",
      "Epoch: 12735 mean train loss:  1.83607044e-05, mean val. loss:  2.47412705e+00\n",
      "Epoch: 12736 mean train loss:  1.80326751e-05, mean val. loss:  2.47419381e+00\n",
      "Epoch: 12737 mean train loss:  1.85371900e-05, mean val. loss:  2.47426748e+00\n",
      "Epoch: 12738 mean train loss:  1.83825032e-05, mean val. loss:  2.47438264e+00\n",
      "Epoch: 12739 mean train loss:  1.83865486e-05, mean val. loss:  2.47448158e+00\n",
      "Epoch: 12740 mean train loss:  1.84081146e-05, mean val. loss:  2.47463322e+00\n",
      "Epoch: 12741 mean train loss:  1.85003155e-05, mean val. loss:  2.47482300e+00\n",
      "Epoch: 12742 mean train loss:  1.84637902e-05, mean val. loss:  2.47507095e+00\n",
      "Epoch: 12743 mean train loss:  1.86873949e-05, mean val. loss:  2.47531796e+00\n",
      "Epoch: 12744 mean train loss:  1.82735384e-05, mean val. loss:  2.47557569e+00\n",
      "Epoch: 12745 mean train loss:  1.81285432e-05, mean val. loss:  2.47587109e+00\n",
      "Epoch: 12746 mean train loss:  1.83220254e-05, mean val. loss:  2.47614074e+00\n",
      "Epoch: 12747 mean train loss:  1.81093346e-05, mean val. loss:  2.47643423e+00\n",
      "Epoch: 12748 mean train loss:  1.86455436e-05, mean val. loss:  2.47668624e+00\n",
      "Epoch: 12749 mean train loss:  1.81290670e-05, mean val. loss:  2.47692657e+00\n",
      "Epoch: 12750 mean train loss:  1.83147204e-05, mean val. loss:  2.47718120e+00\n",
      "Epoch: 12751 mean train loss:  1.86564284e-05, mean val. loss:  2.47739553e+00\n",
      "Epoch: 12752 mean train loss:  1.83444354e-05, mean val. loss:  2.47758436e+00\n",
      "Epoch: 12753 mean train loss:  1.83992379e-05, mean val. loss:  2.47774696e+00\n",
      "Epoch: 12754 mean train loss:  1.83098018e-05, mean val. loss:  2.47792172e+00\n",
      "Epoch: 12755 mean train loss:  1.86387042e-05, mean val. loss:  2.47809672e+00\n",
      "Epoch: 12756 mean train loss:  1.80990319e-05, mean val. loss:  2.47825885e+00\n",
      "Epoch: 12757 mean train loss:  1.82767399e-05, mean val. loss:  2.47842026e+00\n",
      "Epoch: 12758 mean train loss:  1.83746452e-05, mean val. loss:  2.47857094e+00\n",
      "Epoch: 12759 mean train loss:  1.81696087e-05, mean val. loss:  2.47877336e+00\n",
      "Epoch: 12760 mean train loss:  1.83911470e-05, mean val. loss:  2.47894526e+00\n",
      "Epoch: 12761 mean train loss:  1.86660909e-05, mean val. loss:  2.47913170e+00\n",
      "Epoch: 12762 mean train loss:  1.84143137e-05, mean val. loss:  2.47933364e+00\n",
      "Epoch: 12763 mean train loss:  1.84473465e-05, mean val. loss:  2.47953749e+00\n",
      "Epoch: 12764 mean train loss:  1.82549120e-05, mean val. loss:  2.47977614e+00\n",
      "Epoch: 12765 mean train loss:  1.82989752e-05, mean val. loss:  2.48004150e+00\n",
      "Epoch: 12766 mean train loss:  1.85172830e-05, mean val. loss:  2.48031735e+00\n",
      "Epoch: 12767 mean train loss:  1.82803487e-05, mean val. loss:  2.48061562e+00\n",
      "Epoch: 12768 mean train loss:  1.82812510e-05, mean val. loss:  2.48093724e+00\n",
      "Epoch: 12769 mean train loss:  1.84901874e-05, mean val. loss:  2.48125100e+00\n",
      "Epoch: 12770 mean train loss:  1.84390228e-05, mean val. loss:  2.48154712e+00\n",
      "Epoch: 12771 mean train loss:  1.84639357e-05, mean val. loss:  2.48182678e+00\n",
      "Epoch: 12772 mean train loss:  1.83463271e-05, mean val. loss:  2.48210311e+00\n",
      "Epoch: 12773 mean train loss:  1.85784593e-05, mean val. loss:  2.48232651e+00\n",
      "Epoch: 12774 mean train loss:  1.81512733e-05, mean val. loss:  2.48254371e+00\n",
      "Epoch: 12775 mean train loss:  1.83081720e-05, mean val. loss:  2.48276520e+00\n",
      "Epoch: 12776 mean train loss:  1.82104995e-05, mean val. loss:  2.48296070e+00\n",
      "Epoch: 12777 mean train loss:  1.84607634e-05, mean val. loss:  2.48312330e+00\n",
      "Epoch: 12778 mean train loss:  1.79238850e-05, mean val. loss:  2.48328376e+00\n",
      "Epoch: 12779 mean train loss:  1.82045042e-05, mean val. loss:  2.48343277e+00\n",
      "Epoch: 12780 mean train loss:  1.84059900e-05, mean val. loss:  2.48358130e+00\n",
      "Epoch: 12781 mean train loss:  1.84479286e-05, mean val. loss:  2.48373985e+00\n",
      "Epoch: 12782 mean train loss:  1.85646641e-05, mean val. loss:  2.48391080e+00\n",
      "Epoch: 12783 mean train loss:  1.81475771e-05, mean val. loss:  2.48410320e+00\n",
      "Epoch: 12784 mean train loss:  1.85352110e-05, mean val. loss:  2.48428154e+00\n",
      "Epoch: 12785 mean train loss:  1.81278156e-05, mean val. loss:  2.48442483e+00\n",
      "Epoch: 12786 mean train loss:  1.83381489e-05, mean val. loss:  2.48456883e+00\n",
      "Epoch: 12787 mean train loss:  1.83201337e-05, mean val. loss:  2.48472691e+00\n",
      "Epoch: 12788 mean train loss:  1.83802622e-05, mean val. loss:  2.48485827e+00\n",
      "Epoch: 12789 mean train loss:  1.82718213e-05, mean val. loss:  2.48500085e+00\n",
      "Epoch: 12790 mean train loss:  1.85643439e-05, mean val. loss:  2.48514009e+00\n",
      "Epoch: 12791 mean train loss:  1.81119249e-05, mean val. loss:  2.48528552e+00\n",
      "Epoch: 12792 mean train loss:  1.83092197e-05, mean val. loss:  2.48544621e+00\n",
      "Epoch: 12793 mean train loss:  1.81450741e-05, mean val. loss:  2.48557281e+00\n",
      "Epoch: 12794 mean train loss:  1.83015072e-05, mean val. loss:  2.48567080e+00\n",
      "Epoch: 12795 mean train loss:  1.83238590e-05, mean val. loss:  2.48576951e+00\n",
      "Epoch: 12796 mean train loss:  1.83471420e-05, mean val. loss:  2.48586178e+00\n",
      "Epoch: 12797 mean train loss:  1.82035728e-05, mean val. loss:  2.48596621e+00\n",
      "Epoch: 12798 mean train loss:  1.85136741e-05, mean val. loss:  2.48607969e+00\n",
      "Epoch: 12799 mean train loss:  1.84064265e-05, mean val. loss:  2.48619652e+00\n",
      "Epoch: 12800 mean train loss:  1.82184449e-05, mean val. loss:  2.48630476e+00\n",
      "Epoch: 12801 mean train loss:  1.81879732e-05, mean val. loss:  2.48643494e+00\n",
      "Epoch: 12802 mean train loss:  1.83965603e-05, mean val. loss:  2.48656678e+00\n",
      "Epoch: 12803 mean train loss:  1.80271163e-05, mean val. loss:  2.48671007e+00\n",
      "Epoch: 12804 mean train loss:  1.79801427e-05, mean val. loss:  2.48688149e+00\n",
      "Epoch: 12805 mean train loss:  1.81635842e-05, mean val. loss:  2.48709655e+00\n",
      "Epoch: 12806 mean train loss:  1.83033990e-05, mean val. loss:  2.48732519e+00\n",
      "Epoch: 12807 mean train loss:  1.85692625e-05, mean val. loss:  2.48757935e+00\n",
      "Epoch: 12808 mean train loss:  1.82527001e-05, mean val. loss:  2.48783422e+00\n",
      "Epoch: 12809 mean train loss:  1.84037199e-05, mean val. loss:  2.48808384e+00\n",
      "Epoch: 12810 mean train loss:  1.85532845e-05, mean val. loss:  2.48834038e+00\n",
      "Epoch: 12811 mean train loss:  1.82643707e-05, mean val. loss:  2.48860645e+00\n",
      "Epoch: 12812 mean train loss:  1.84009259e-05, mean val. loss:  2.48886561e+00\n",
      "Epoch: 12813 mean train loss:  1.81834912e-05, mean val. loss:  2.48915958e+00\n",
      "Epoch: 12814 mean train loss:  1.83130032e-05, mean val. loss:  2.48946309e+00\n",
      "Epoch: 12815 mean train loss:  1.84968230e-05, mean val. loss:  2.48976111e+00\n",
      "Epoch: 12816 mean train loss:  1.84232194e-05, mean val. loss:  2.49006295e+00\n",
      "Epoch: 12817 mean train loss:  1.78798800e-05, mean val. loss:  2.49041319e+00\n",
      "Epoch: 12818 mean train loss:  1.84997916e-05, mean val. loss:  2.49072027e+00\n",
      "Epoch: 12819 mean train loss:  1.82656222e-05, mean val. loss:  2.49102211e+00\n",
      "Epoch: 12820 mean train loss:  1.82777585e-05, mean val. loss:  2.49132419e+00\n",
      "Epoch: 12821 mean train loss:  1.84092205e-05, mean val. loss:  2.49158859e+00\n",
      "Epoch: 12822 mean train loss:  1.85788085e-05, mean val. loss:  2.49181509e+00\n",
      "Epoch: 12823 mean train loss:  1.82216754e-05, mean val. loss:  2.49201298e+00\n",
      "Epoch: 12824 mean train loss:  1.84012752e-05, mean val. loss:  2.49220848e+00\n",
      "Epoch: 12825 mean train loss:  1.81545911e-05, mean val. loss:  2.49238825e+00\n",
      "Epoch: 12826 mean train loss:  1.81922805e-05, mean val. loss:  2.49256015e+00\n",
      "Epoch: 12827 mean train loss:  1.86120742e-05, mean val. loss:  2.49268079e+00\n",
      "Epoch: 12828 mean train loss:  1.83477532e-05, mean val. loss:  2.49277616e+00\n",
      "Epoch: 12829 mean train loss:  1.83532538e-05, mean val. loss:  2.49283552e+00\n",
      "Epoch: 12830 mean train loss:  1.82033400e-05, mean val. loss:  2.49289775e+00\n",
      "Epoch: 12831 mean train loss:  1.82407093e-05, mean val. loss:  2.49295664e+00\n",
      "Epoch: 12832 mean train loss:  1.83181837e-05, mean val. loss:  2.49302959e+00\n",
      "Epoch: 12833 mean train loss:  1.80190837e-05, mean val. loss:  2.49312735e+00\n",
      "Epoch: 12834 mean train loss:  1.83731318e-05, mean val. loss:  2.49324679e+00\n",
      "Epoch: 12835 mean train loss:  1.85324752e-05, mean val. loss:  2.49334812e+00\n",
      "Epoch: 12836 mean train loss:  1.82912045e-05, mean val. loss:  2.49344563e+00\n",
      "Epoch: 12837 mean train loss:  1.81716168e-05, mean val. loss:  2.49358487e+00\n",
      "Epoch: 12838 mean train loss:  1.81126234e-05, mean val. loss:  2.49373293e+00\n",
      "Epoch: 12839 mean train loss:  1.83855882e-05, mean val. loss:  2.49389100e+00\n",
      "Epoch: 12840 mean train loss:  1.83414086e-05, mean val. loss:  2.49403906e+00\n",
      "Epoch: 12841 mean train loss:  1.81401847e-05, mean val. loss:  2.49419284e+00\n",
      "Epoch: 12842 mean train loss:  1.82524673e-05, mean val. loss:  2.49437690e+00\n",
      "Epoch: 12843 mean train loss:  1.82952499e-05, mean val. loss:  2.49454880e+00\n",
      "Epoch: 12844 mean train loss:  1.83933298e-05, mean val. loss:  2.49475217e+00\n",
      "Epoch: 12845 mean train loss:  1.80554925e-05, mean val. loss:  2.49500847e+00\n",
      "Epoch: 12846 mean train loss:  1.83615193e-05, mean val. loss:  2.49526119e+00\n",
      "Epoch: 12847 mean train loss:  1.80818606e-05, mean val. loss:  2.49550676e+00\n",
      "Epoch: 12848 mean train loss:  1.83877419e-05, mean val. loss:  2.49574709e+00\n",
      "Epoch: 12849 mean train loss:  1.82792428e-05, mean val. loss:  2.49597645e+00\n",
      "Epoch: 12850 mean train loss:  1.82648946e-05, mean val. loss:  2.49618149e+00\n",
      "Epoch: 12851 mean train loss:  1.82205404e-05, mean val. loss:  2.49636912e+00\n",
      "Epoch: 12852 mean train loss:  1.80253410e-05, mean val. loss:  2.49654508e+00\n",
      "Epoch: 12853 mean train loss:  1.86185353e-05, mean val. loss:  2.49672890e+00\n",
      "Epoch: 12854 mean train loss:  1.83263037e-05, mean val. loss:  2.49691939e+00\n",
      "Epoch: 12855 mean train loss:  1.81570358e-05, mean val. loss:  2.49712610e+00\n",
      "Epoch: 12856 mean train loss:  1.82814838e-05, mean val. loss:  2.49732399e+00\n",
      "Epoch: 12857 mean train loss:  1.83803495e-05, mean val. loss:  2.49750566e+00\n",
      "Epoch: 12858 mean train loss:  1.84043602e-05, mean val. loss:  2.49768400e+00\n",
      "Epoch: 12859 mean train loss:  1.79529015e-05, mean val. loss:  2.49790311e+00\n",
      "Epoch: 12860 mean train loss:  1.80321222e-05, mean val. loss:  2.49814081e+00\n",
      "Epoch: 12861 mean train loss:  1.80408242e-05, mean val. loss:  2.49839759e+00\n",
      "Epoch: 12862 mean train loss:  1.86706311e-05, mean val. loss:  2.49861693e+00\n",
      "Epoch: 12863 mean train loss:  1.81754003e-05, mean val. loss:  2.49884987e+00\n",
      "Epoch: 12864 mean train loss:  1.82766817e-05, mean val. loss:  2.49907565e+00\n",
      "Epoch: 12865 mean train loss:  1.82171934e-05, mean val. loss:  2.49929094e+00\n",
      "Epoch: 12866 mean train loss:  1.80688221e-05, mean val. loss:  2.49954414e+00\n",
      "Epoch: 12867 mean train loss:  1.83686498e-05, mean val. loss:  2.49979973e+00\n",
      "Epoch: 12868 mean train loss:  1.84681558e-05, mean val. loss:  2.50005245e+00\n",
      "Epoch: 12869 mean train loss:  1.85743847e-05, mean val. loss:  2.50023794e+00\n",
      "Epoch: 12870 mean train loss:  1.85596291e-05, mean val. loss:  2.50041604e+00\n",
      "Epoch: 12871 mean train loss:  1.82974909e-05, mean val. loss:  2.50058937e+00\n",
      "Epoch: 12872 mean train loss:  1.81946671e-05, mean val. loss:  2.50076842e+00\n",
      "Epoch: 12873 mean train loss:  1.85455428e-05, mean val. loss:  2.50093412e+00\n",
      "Epoch: 12874 mean train loss:  1.81435316e-05, mean val. loss:  2.50109911e+00\n",
      "Epoch: 12875 mean train loss:  1.82356162e-05, mean val. loss:  2.50125241e+00\n",
      "Epoch: 12876 mean train loss:  1.83100929e-05, mean val. loss:  2.50138998e+00\n",
      "Epoch: 12877 mean train loss:  1.82851509e-05, mean val. loss:  2.50152016e+00\n",
      "Epoch: 12878 mean train loss:  1.82674848e-05, mean val. loss:  2.50165963e+00\n",
      "Epoch: 12879 mean train loss:  1.84738892e-05, mean val. loss:  2.50175667e+00\n",
      "Epoch: 12880 mean train loss:  1.82004878e-05, mean val. loss:  2.50185370e+00\n",
      "Epoch: 12881 mean train loss:  1.86390826e-05, mean val. loss:  2.50193763e+00\n",
      "Epoch: 12882 mean train loss:  1.80317438e-05, mean val. loss:  2.50203085e+00\n",
      "Epoch: 12883 mean train loss:  1.84752862e-05, mean val. loss:  2.50214005e+00\n",
      "Epoch: 12884 mean train loss:  1.80817151e-05, mean val. loss:  2.50226903e+00\n",
      "Epoch: 12885 mean train loss:  1.85584067e-05, mean val. loss:  2.50236154e+00\n",
      "Epoch: 12886 mean train loss:  1.81897194e-05, mean val. loss:  2.50247574e+00\n",
      "Epoch: 12887 mean train loss:  1.82146905e-05, mean val. loss:  2.50261855e+00\n",
      "Epoch: 12888 mean train loss:  1.80916104e-05, mean val. loss:  2.50276399e+00\n",
      "Epoch: 12889 mean train loss:  1.80463248e-05, mean val. loss:  2.50295424e+00\n",
      "Epoch: 12890 mean train loss:  1.83481316e-05, mean val. loss:  2.50318456e+00\n",
      "Epoch: 12891 mean train loss:  1.83014199e-05, mean val. loss:  2.50342298e+00\n",
      "Epoch: 12892 mean train loss:  1.83062220e-05, mean val. loss:  2.50366044e+00\n",
      "Epoch: 12893 mean train loss:  1.82065414e-05, mean val. loss:  2.50391388e+00\n",
      "Epoch: 12894 mean train loss:  1.83200173e-05, mean val. loss:  2.50416923e+00\n",
      "Epoch: 12895 mean train loss:  1.81660289e-05, mean val. loss:  2.50444293e+00\n",
      "Epoch: 12896 mean train loss:  1.85819226e-05, mean val. loss:  2.50469303e+00\n",
      "Epoch: 12897 mean train loss:  1.83330558e-05, mean val. loss:  2.50495768e+00\n",
      "Epoch: 12898 mean train loss:  1.85160316e-05, mean val. loss:  2.50522637e+00\n",
      "Epoch: 12899 mean train loss:  1.85284007e-05, mean val. loss:  2.50544357e+00\n",
      "Epoch: 12900 mean train loss:  1.81160867e-05, mean val. loss:  2.50566196e+00\n",
      "Epoch: 12901 mean train loss:  1.82715012e-05, mean val. loss:  2.50585365e+00\n",
      "Epoch: 12902 mean train loss:  1.80769130e-05, mean val. loss:  2.50605321e+00\n",
      "Epoch: 12903 mean train loss:  1.84870441e-05, mean val. loss:  2.50623393e+00\n",
      "Epoch: 12904 mean train loss:  1.80828793e-05, mean val. loss:  2.50639868e+00\n",
      "Epoch: 12905 mean train loss:  1.81386713e-05, mean val. loss:  2.50657034e+00\n",
      "Epoch: 12906 mean train loss:  1.85033423e-05, mean val. loss:  2.50671458e+00\n",
      "Epoch: 12907 mean train loss:  1.82289805e-05, mean val. loss:  2.50684643e+00\n",
      "Epoch: 12908 mean train loss:  1.81169016e-05, mean val. loss:  2.50696111e+00\n",
      "Epoch: 12909 mean train loss:  1.80639327e-05, mean val. loss:  2.50708604e+00\n",
      "Epoch: 12910 mean train loss:  1.83350348e-05, mean val. loss:  2.50720048e+00\n",
      "Epoch: 12911 mean train loss:  1.84809905e-05, mean val. loss:  2.50734854e+00\n",
      "Epoch: 12912 mean train loss:  1.84825039e-05, mean val. loss:  2.50751233e+00\n",
      "Epoch: 12913 mean train loss:  1.82166987e-05, mean val. loss:  2.50769901e+00\n",
      "Epoch: 12914 mean train loss:  1.82155345e-05, mean val. loss:  2.50792193e+00\n",
      "Epoch: 12915 mean train loss:  1.82664953e-05, mean val. loss:  2.50812244e+00\n",
      "Epoch: 12916 mean train loss:  1.84792734e-05, mean val. loss:  2.50833035e+00\n",
      "Epoch: 12917 mean train loss:  1.81012438e-05, mean val. loss:  2.50856209e+00\n",
      "Epoch: 12918 mean train loss:  1.81238283e-05, mean val. loss:  2.50880885e+00\n",
      "Epoch: 12919 mean train loss:  1.80116622e-05, mean val. loss:  2.50909734e+00\n",
      "Epoch: 12920 mean train loss:  1.81205978e-05, mean val. loss:  2.50940990e+00\n",
      "Epoch: 12921 mean train loss:  1.82185322e-05, mean val. loss:  2.50971889e+00\n",
      "Epoch: 12922 mean train loss:  1.82704243e-05, mean val. loss:  2.51001573e+00\n",
      "Epoch: 12923 mean train loss:  1.77458569e-05, mean val. loss:  2.51036072e+00\n",
      "Epoch: 12924 mean train loss:  1.81558426e-05, mean val. loss:  2.51068616e+00\n",
      "Epoch: 12925 mean train loss:  1.84459495e-05, mean val. loss:  2.51100397e+00\n",
      "Epoch: 12926 mean train loss:  1.84481614e-05, mean val. loss:  2.51128983e+00\n",
      "Epoch: 12927 mean train loss:  1.82798249e-05, mean val. loss:  2.51155376e+00\n",
      "Epoch: 12928 mean train loss:  1.80975185e-05, mean val. loss:  2.51181769e+00\n",
      "Epoch: 12929 mean train loss:  1.80606730e-05, mean val. loss:  2.51209068e+00\n",
      "Epoch: 12930 mean train loss:  1.81011565e-05, mean val. loss:  2.51236963e+00\n",
      "Epoch: 12931 mean train loss:  1.85254321e-05, mean val. loss:  2.51263785e+00\n",
      "Epoch: 12932 mean train loss:  1.81846553e-05, mean val. loss:  2.51288271e+00\n",
      "Epoch: 12933 mean train loss:  1.81911164e-05, mean val. loss:  2.51313591e+00\n",
      "Epoch: 12934 mean train loss:  1.81033101e-05, mean val. loss:  2.51337075e+00\n",
      "Epoch: 12935 mean train loss:  1.84450182e-05, mean val. loss:  2.51357436e+00\n",
      "Epoch: 12936 mean train loss:  1.79150375e-05, mean val. loss:  2.51381063e+00\n",
      "Epoch: 12937 mean train loss:  1.84186210e-05, mean val. loss:  2.51405239e+00\n",
      "Epoch: 12938 mean train loss:  1.82139047e-05, mean val. loss:  2.51428318e+00\n",
      "Epoch: 12939 mean train loss:  1.83446682e-05, mean val. loss:  2.51449537e+00\n",
      "Epoch: 12940 mean train loss:  1.85034005e-05, mean val. loss:  2.51467013e+00\n",
      "Epoch: 12941 mean train loss:  1.81209180e-05, mean val. loss:  2.51482248e+00\n",
      "Epoch: 12942 mean train loss:  1.84098317e-05, mean val. loss:  2.51500916e+00\n",
      "Epoch: 12943 mean train loss:  1.79990893e-05, mean val. loss:  2.51518393e+00\n",
      "Epoch: 12944 mean train loss:  1.80633797e-05, mean val. loss:  2.51535177e+00\n",
      "Epoch: 12945 mean train loss:  1.79561903e-05, mean val. loss:  2.51551795e+00\n",
      "Epoch: 12946 mean train loss:  1.82403310e-05, mean val. loss:  2.51566505e+00\n",
      "Epoch: 12947 mean train loss:  1.81530777e-05, mean val. loss:  2.51581478e+00\n",
      "Epoch: 12948 mean train loss:  1.84600940e-05, mean val. loss:  2.51594043e+00\n",
      "Epoch: 12949 mean train loss:  1.79663475e-05, mean val. loss:  2.51607156e+00\n",
      "Epoch: 12950 mean train loss:  1.83676020e-05, mean val. loss:  2.51617861e+00\n",
      "Epoch: 12951 mean train loss:  1.80517964e-05, mean val. loss:  2.51630068e+00\n",
      "Epoch: 12952 mean train loss:  1.81001087e-05, mean val. loss:  2.51642656e+00\n",
      "Epoch: 12953 mean train loss:  1.81198120e-05, mean val. loss:  2.51655149e+00\n",
      "Epoch: 12954 mean train loss:  1.80896604e-05, mean val. loss:  2.51668453e+00\n",
      "Epoch: 12955 mean train loss:  1.84137025e-05, mean val. loss:  2.51681066e+00\n",
      "Epoch: 12956 mean train loss:  1.83226075e-05, mean val. loss:  2.51690316e+00\n",
      "Epoch: 12957 mean train loss:  1.82894291e-05, mean val. loss:  2.51701617e+00\n",
      "Epoch: 12958 mean train loss:  1.80827337e-05, mean val. loss:  2.51714158e+00\n",
      "Epoch: 12959 mean train loss:  1.82013318e-05, mean val. loss:  2.51728559e+00\n",
      "Epoch: 12960 mean train loss:  1.83039228e-05, mean val. loss:  2.51745200e+00\n",
      "Epoch: 12961 mean train loss:  1.83212396e-05, mean val. loss:  2.51759553e+00\n",
      "Epoch: 12962 mean train loss:  1.80697534e-05, mean val. loss:  2.51773715e+00\n",
      "Epoch: 12963 mean train loss:  1.82627118e-05, mean val. loss:  2.51786780e+00\n",
      "Epoch: 12964 mean train loss:  1.77831389e-05, mean val. loss:  2.51801991e+00\n",
      "Epoch: 12965 mean train loss:  1.79728377e-05, mean val. loss:  2.51818109e+00\n",
      "Epoch: 12966 mean train loss:  1.82456570e-05, mean val. loss:  2.51835203e+00\n",
      "Epoch: 12967 mean train loss:  1.84316013e-05, mean val. loss:  2.51856971e+00\n",
      "Epoch: 12968 mean train loss:  1.83101802e-05, mean val. loss:  2.51877403e+00\n",
      "Epoch: 12969 mean train loss:  1.79428607e-05, mean val. loss:  2.51901507e+00\n",
      "Epoch: 12970 mean train loss:  1.81284267e-05, mean val. loss:  2.51926708e+00\n",
      "Epoch: 12971 mean train loss:  1.79793278e-05, mean val. loss:  2.51955509e+00\n",
      "Epoch: 12972 mean train loss:  1.79711205e-05, mean val. loss:  2.51989746e+00\n",
      "Epoch: 12973 mean train loss:  1.80900970e-05, mean val. loss:  2.52021742e+00\n",
      "Epoch: 12974 mean train loss:  1.79180061e-05, mean val. loss:  2.52053332e+00\n",
      "Epoch: 12975 mean train loss:  1.81328796e-05, mean val. loss:  2.52084231e+00\n",
      "Epoch: 12976 mean train loss:  1.82626827e-05, mean val. loss:  2.52114677e+00\n",
      "Epoch: 12977 mean train loss:  1.82641670e-05, mean val. loss:  2.52143502e+00\n",
      "Epoch: 12978 mean train loss:  1.80809293e-05, mean val. loss:  2.52172112e+00\n",
      "Epoch: 12979 mean train loss:  1.81399519e-05, mean val. loss:  2.52197433e+00\n",
      "Epoch: 12980 mean train loss:  1.83776428e-05, mean val. loss:  2.52218986e+00\n",
      "Epoch: 12981 mean train loss:  1.82602089e-05, mean val. loss:  2.52238321e+00\n",
      "Epoch: 12982 mean train loss:  1.83454249e-05, mean val. loss:  2.52258730e+00\n",
      "Epoch: 12983 mean train loss:  1.80567149e-05, mean val. loss:  2.52280641e+00\n",
      "Epoch: 12984 mean train loss:  1.80534844e-05, mean val. loss:  2.52302647e+00\n",
      "Epoch: 12985 mean train loss:  1.82006625e-05, mean val. loss:  2.52325368e+00\n",
      "Epoch: 12986 mean train loss:  1.79188210e-05, mean val. loss:  2.52347636e+00\n",
      "Epoch: 12987 mean train loss:  1.78887276e-05, mean val. loss:  2.52374625e+00\n",
      "Epoch: 12988 mean train loss:  1.81576470e-05, mean val. loss:  2.52400064e+00\n",
      "Epoch: 12989 mean train loss:  1.82239746e-05, mean val. loss:  2.52428102e+00\n",
      "Epoch: 12990 mean train loss:  1.80664938e-05, mean val. loss:  2.52454638e+00\n",
      "Epoch: 12991 mean train loss:  1.80784264e-05, mean val. loss:  2.52480364e+00\n",
      "Epoch: 12992 mean train loss:  1.82844233e-05, mean val. loss:  2.52504945e+00\n",
      "Epoch: 12993 mean train loss:  1.84919045e-05, mean val. loss:  2.52527094e+00\n",
      "Epoch: 12994 mean train loss:  1.79480412e-05, mean val. loss:  2.52549601e+00\n",
      "Epoch: 12995 mean train loss:  1.81592477e-05, mean val. loss:  2.52567458e+00\n",
      "Epoch: 12996 mean train loss:  1.81960058e-05, mean val. loss:  2.52582836e+00\n",
      "Epoch: 12997 mean train loss:  1.81257201e-05, mean val. loss:  2.52595019e+00\n",
      "Epoch: 12998 mean train loss:  1.82930671e-05, mean val. loss:  2.52606964e+00\n",
      "Epoch: 12999 mean train loss:  1.76881440e-05, mean val. loss:  2.52622151e+00\n",
      "Epoch: 13000 mean train loss:  1.83037773e-05, mean val. loss:  2.52634573e+00\n",
      "Epoch: 13001 mean train loss:  1.83847733e-05, mean val. loss:  2.52645326e+00\n",
      "Epoch: 13002 mean train loss:  1.84802921e-05, mean val. loss:  2.52656507e+00\n",
      "Epoch: 13003 mean train loss:  1.82795920e-05, mean val. loss:  2.52665806e+00\n",
      "Epoch: 13004 mean train loss:  1.81930955e-05, mean val. loss:  2.52674985e+00\n",
      "Epoch: 13005 mean train loss:  1.78889313e-05, mean val. loss:  2.52686810e+00\n",
      "Epoch: 13006 mean train loss:  1.82571239e-05, mean val. loss:  2.52697444e+00\n",
      "Epoch: 13007 mean train loss:  1.82725780e-05, mean val. loss:  2.52708364e+00\n",
      "Epoch: 13008 mean train loss:  1.82697549e-05, mean val. loss:  2.52717137e+00\n",
      "Epoch: 13009 mean train loss:  1.82156218e-05, mean val. loss:  2.52725196e+00\n",
      "Epoch: 13010 mean train loss:  1.83050288e-05, mean val. loss:  2.52731705e+00\n",
      "Epoch: 13011 mean train loss:  1.80606439e-05, mean val. loss:  2.52738714e+00\n",
      "Epoch: 13012 mean train loss:  1.81849464e-05, mean val. loss:  2.52746153e+00\n",
      "Epoch: 13013 mean train loss:  1.79941708e-05, mean val. loss:  2.52755237e+00\n",
      "Epoch: 13014 mean train loss:  1.78981281e-05, mean val. loss:  2.52766490e+00\n",
      "Epoch: 13015 mean train loss:  1.78496994e-05, mean val. loss:  2.52782464e+00\n",
      "Epoch: 13016 mean train loss:  1.79751660e-05, mean val. loss:  2.52797008e+00\n",
      "Epoch: 13017 mean train loss:  1.83980446e-05, mean val. loss:  2.52810955e+00\n",
      "Epoch: 13018 mean train loss:  1.78253686e-05, mean val. loss:  2.52828836e+00\n",
      "Epoch: 13019 mean train loss:  1.78852933e-05, mean val. loss:  2.52849650e+00\n",
      "Epoch: 13020 mean train loss:  1.82985677e-05, mean val. loss:  2.52870655e+00\n",
      "Epoch: 13021 mean train loss:  1.80575880e-05, mean val. loss:  2.52892733e+00\n",
      "Epoch: 13022 mean train loss:  1.82061049e-05, mean val. loss:  2.52917981e+00\n",
      "Epoch: 13023 mean train loss:  1.79794151e-05, mean val. loss:  2.52946281e+00\n",
      "Epoch: 13024 mean train loss:  1.85348908e-05, mean val. loss:  2.52975559e+00\n",
      "Epoch: 13025 mean train loss:  1.80873612e-05, mean val. loss:  2.53002667e+00\n",
      "Epoch: 13026 mean train loss:  1.84118690e-05, mean val. loss:  2.53028464e+00\n",
      "Epoch: 13027 mean train loss:  1.79973431e-05, mean val. loss:  2.53054404e+00\n",
      "Epoch: 13028 mean train loss:  1.82942604e-05, mean val. loss:  2.53079367e+00\n",
      "Epoch: 13029 mean train loss:  1.81633222e-05, mean val. loss:  2.53105187e+00\n",
      "Epoch: 13030 mean train loss:  1.79133785e-05, mean val. loss:  2.53132915e+00\n",
      "Epoch: 13031 mean train loss:  1.82034564e-05, mean val. loss:  2.53157544e+00\n",
      "Epoch: 13032 mean train loss:  1.80274074e-05, mean val. loss:  2.53182411e+00\n",
      "Epoch: 13033 mean train loss:  1.83585507e-05, mean val. loss:  2.53203511e+00\n",
      "Epoch: 13034 mean train loss:  1.81860232e-05, mean val. loss:  2.53225303e+00\n",
      "Epoch: 13035 mean train loss:  1.80117495e-05, mean val. loss:  2.53245187e+00\n",
      "Epoch: 13036 mean train loss:  1.79995841e-05, mean val. loss:  2.53265190e+00\n",
      "Epoch: 13037 mean train loss:  1.83623924e-05, mean val. loss:  2.53280926e+00\n",
      "Epoch: 13038 mean train loss:  1.85430690e-05, mean val. loss:  2.53293753e+00\n",
      "Epoch: 13039 mean train loss:  1.81143987e-05, mean val. loss:  2.53304338e+00\n",
      "Epoch: 13040 mean train loss:  1.83359371e-05, mean val. loss:  2.53314090e+00\n",
      "Epoch: 13041 mean train loss:  1.81687356e-05, mean val. loss:  2.53323102e+00\n",
      "Epoch: 13042 mean train loss:  1.80603529e-05, mean val. loss:  2.53334236e+00\n",
      "Epoch: 13043 mean train loss:  1.80806965e-05, mean val. loss:  2.53345823e+00\n",
      "Epoch: 13044 mean train loss:  1.79074123e-05, mean val. loss:  2.53358865e+00\n",
      "Epoch: 13045 mean train loss:  1.80178904e-05, mean val. loss:  2.53373718e+00\n",
      "Epoch: 13046 mean train loss:  1.80842180e-05, mean val. loss:  2.53391027e+00\n",
      "Epoch: 13047 mean train loss:  1.78613991e-05, mean val. loss:  2.53411841e+00\n",
      "Epoch: 13048 mean train loss:  1.81353826e-05, mean val. loss:  2.53436399e+00\n",
      "Epoch: 13049 mean train loss:  1.81522628e-05, mean val. loss:  2.53462863e+00\n",
      "Epoch: 13050 mean train loss:  1.74791203e-05, mean val. loss:  2.53496647e+00\n",
      "Epoch: 13051 mean train loss:  1.76896574e-05, mean val. loss:  2.53533506e+00\n",
      "Epoch: 13052 mean train loss:  1.82552903e-05, mean val. loss:  2.53570247e+00\n",
      "Epoch: 13053 mean train loss:  1.84493838e-05, mean val. loss:  2.53607345e+00\n",
      "Epoch: 13054 mean train loss:  1.79983908e-05, mean val. loss:  2.53648376e+00\n",
      "Epoch: 13055 mean train loss:  1.79688795e-05, mean val. loss:  2.53690577e+00\n",
      "Epoch: 13056 mean train loss:  1.78278424e-05, mean val. loss:  2.53735089e+00\n",
      "Epoch: 13057 mean train loss:  1.81544165e-05, mean val. loss:  2.53780198e+00\n",
      "Epoch: 13058 mean train loss:  1.80538045e-05, mean val. loss:  2.53820753e+00\n",
      "Epoch: 13059 mean train loss:  1.82175136e-05, mean val. loss:  2.53856897e+00\n",
      "Epoch: 13060 mean train loss:  1.77015900e-05, mean val. loss:  2.53892970e+00\n",
      "Epoch: 13061 mean train loss:  1.81674259e-05, mean val. loss:  2.53927040e+00\n",
      "Epoch: 13062 mean train loss:  1.81299110e-05, mean val. loss:  2.53956795e+00\n",
      "Epoch: 13063 mean train loss:  1.81906798e-05, mean val. loss:  2.53982759e+00\n",
      "Epoch: 13064 mean train loss:  1.80534844e-05, mean val. loss:  2.54006338e+00\n",
      "Epoch: 13065 mean train loss:  1.81273208e-05, mean val. loss:  2.54027057e+00\n",
      "Epoch: 13066 mean train loss:  1.79169874e-05, mean val. loss:  2.54046869e+00\n",
      "Epoch: 13067 mean train loss:  1.78697228e-05, mean val. loss:  2.54063010e+00\n",
      "Epoch: 13068 mean train loss:  1.83587545e-05, mean val. loss:  2.54073071e+00\n",
      "Epoch: 13069 mean train loss:  1.81892829e-05, mean val. loss:  2.54080606e+00\n",
      "Epoch: 13070 mean train loss:  1.81286305e-05, mean val. loss:  2.54084659e+00\n",
      "Epoch: 13071 mean train loss:  1.81751966e-05, mean val. loss:  2.54086924e+00\n",
      "Epoch: 13072 mean train loss:  1.78359041e-05, mean val. loss:  2.54089189e+00\n",
      "Epoch: 13073 mean train loss:  1.78026385e-05, mean val. loss:  2.54093146e+00\n",
      "Epoch: 13074 mean train loss:  1.78445480e-05, mean val. loss:  2.54101038e+00\n",
      "Epoch: 13075 mean train loss:  1.77239417e-05, mean val. loss:  2.54110670e+00\n",
      "Epoch: 13076 mean train loss:  1.79884955e-05, mean val. loss:  2.54122519e+00\n",
      "Epoch: 13077 mean train loss:  1.84648670e-05, mean val. loss:  2.54135180e+00\n",
      "Epoch: 13078 mean train loss:  1.78890768e-05, mean val. loss:  2.54150558e+00\n",
      "Epoch: 13079 mean train loss:  1.76830799e-05, mean val. loss:  2.54170465e+00\n",
      "Epoch: 13080 mean train loss:  1.81782525e-05, mean val. loss:  2.54193783e+00\n",
      "Epoch: 13081 mean train loss:  1.80648640e-05, mean val. loss:  2.54221249e+00\n",
      "Epoch: 13082 mean train loss:  1.79521448e-05, mean val. loss:  2.54251647e+00\n",
      "Epoch: 13083 mean train loss:  1.80945790e-05, mean val. loss:  2.54286623e+00\n",
      "Epoch: 13084 mean train loss:  1.81444339e-05, mean val. loss:  2.54325032e+00\n",
      "Epoch: 13085 mean train loss:  1.82505173e-05, mean val. loss:  2.54360700e+00\n",
      "Epoch: 13086 mean train loss:  1.81857904e-05, mean val. loss:  2.54392123e+00\n",
      "Epoch: 13087 mean train loss:  1.80467614e-05, mean val. loss:  2.54421759e+00\n",
      "Epoch: 13088 mean train loss:  1.83105585e-05, mean val. loss:  2.54450297e+00\n",
      "Epoch: 13089 mean train loss:  1.80600327e-05, mean val. loss:  2.54476452e+00\n",
      "Epoch: 13090 mean train loss:  1.82092190e-05, mean val. loss:  2.54499650e+00\n",
      "Epoch: 13091 mean train loss:  1.83138764e-05, mean val. loss:  2.54518056e+00\n",
      "Epoch: 13092 mean train loss:  1.82041258e-05, mean val. loss:  2.54530454e+00\n",
      "Epoch: 13093 mean train loss:  1.82070071e-05, mean val. loss:  2.54540324e+00\n",
      "Epoch: 13094 mean train loss:  1.79087801e-05, mean val. loss:  2.54547167e+00\n",
      "Epoch: 13095 mean train loss:  1.83236261e-05, mean val. loss:  2.54551387e+00\n",
      "Epoch: 13096 mean train loss:  1.84471137e-05, mean val. loss:  2.54550600e+00\n",
      "Epoch: 13097 mean train loss:  1.80625066e-05, mean val. loss:  2.54550791e+00\n",
      "Epoch: 13098 mean train loss:  1.82480435e-05, mean val. loss:  2.54548883e+00\n",
      "Epoch: 13099 mean train loss:  1.82854710e-05, mean val. loss:  2.54546905e+00\n",
      "Epoch: 13100 mean train loss:  1.79054332e-05, mean val. loss:  2.54546356e+00\n",
      "Epoch: 13101 mean train loss:  1.81304931e-05, mean val. loss:  2.54547048e+00\n",
      "Epoch: 13102 mean train loss:  1.80809293e-05, mean val. loss:  2.54549861e+00\n",
      "Epoch: 13103 mean train loss:  1.79518247e-05, mean val. loss:  2.54555893e+00\n",
      "Epoch: 13104 mean train loss:  1.80756615e-05, mean val. loss:  2.54564500e+00\n",
      "Epoch: 13105 mean train loss:  1.79643685e-05, mean val. loss:  2.54578495e+00\n",
      "Epoch: 13106 mean train loss:  1.79109920e-05, mean val. loss:  2.54596686e+00\n",
      "Epoch: 13107 mean train loss:  1.82080548e-05, mean val. loss:  2.54612899e+00\n",
      "Epoch: 13108 mean train loss:  1.78980990e-05, mean val. loss:  2.54636049e+00\n",
      "Epoch: 13109 mean train loss:  1.78768532e-05, mean val. loss:  2.54662395e+00\n",
      "Epoch: 13110 mean train loss:  1.79496419e-05, mean val. loss:  2.54691529e+00\n",
      "Epoch: 13111 mean train loss:  1.78480987e-05, mean val. loss:  2.54727006e+00\n",
      "Epoch: 13112 mean train loss:  1.79108174e-05, mean val. loss:  2.54765248e+00\n",
      "Epoch: 13113 mean train loss:  1.83418451e-05, mean val. loss:  2.54802346e+00\n",
      "Epoch: 13114 mean train loss:  1.80022616e-05, mean val. loss:  2.54838967e+00\n",
      "Epoch: 13115 mean train loss:  1.83870725e-05, mean val. loss:  2.54874802e+00\n",
      "Epoch: 13116 mean train loss:  1.79029012e-05, mean val. loss:  2.54911757e+00\n",
      "Epoch: 13117 mean train loss:  1.79020572e-05, mean val. loss:  2.54947138e+00\n",
      "Epoch: 13118 mean train loss:  1.81276991e-05, mean val. loss:  2.54979730e+00\n",
      "Epoch: 13119 mean train loss:  1.81583746e-05, mean val. loss:  2.55013609e+00\n",
      "Epoch: 13120 mean train loss:  1.83151278e-05, mean val. loss:  2.55044413e+00\n",
      "Epoch: 13121 mean train loss:  1.81051437e-05, mean val. loss:  2.55071831e+00\n",
      "Epoch: 13122 mean train loss:  1.83178636e-05, mean val. loss:  2.55094004e+00\n",
      "Epoch: 13123 mean train loss:  1.78084883e-05, mean val. loss:  2.55115676e+00\n",
      "Epoch: 13124 mean train loss:  1.83901866e-05, mean val. loss:  2.55136299e+00\n",
      "Epoch: 13125 mean train loss:  1.78839837e-05, mean val. loss:  2.55154443e+00\n",
      "Epoch: 13126 mean train loss:  1.81611977e-05, mean val. loss:  2.55171776e+00\n",
      "Epoch: 13127 mean train loss:  1.82398653e-05, mean val. loss:  2.55185986e+00\n",
      "Epoch: 13128 mean train loss:  1.77964102e-05, mean val. loss:  2.55201960e+00\n",
      "Epoch: 13129 mean train loss:  1.77567126e-05, mean val. loss:  2.55219841e+00\n",
      "Epoch: 13130 mean train loss:  1.81371288e-05, mean val. loss:  2.55238652e+00\n",
      "Epoch: 13131 mean train loss:  1.79195777e-05, mean val. loss:  2.55260491e+00\n",
      "Epoch: 13132 mean train loss:  1.80933566e-05, mean val. loss:  2.55282640e+00\n",
      "Epoch: 13133 mean train loss:  1.81906216e-05, mean val. loss:  2.55303097e+00\n",
      "Epoch: 13134 mean train loss:  1.79646013e-05, mean val. loss:  2.55324268e+00\n",
      "Epoch: 13135 mean train loss:  1.83655939e-05, mean val. loss:  2.55342436e+00\n",
      "Epoch: 13136 mean train loss:  1.79594499e-05, mean val. loss:  2.55361748e+00\n",
      "Epoch: 13137 mean train loss:  1.78242335e-05, mean val. loss:  2.55384135e+00\n",
      "Epoch: 13138 mean train loss:  1.79460621e-05, mean val. loss:  2.55406070e+00\n",
      "Epoch: 13139 mean train loss:  1.77306938e-05, mean val. loss:  2.55430794e+00\n",
      "Epoch: 13140 mean train loss:  1.80947536e-05, mean val. loss:  2.55456090e+00\n",
      "Epoch: 13141 mean train loss:  1.78453920e-05, mean val. loss:  2.55483198e+00\n",
      "Epoch: 13142 mean train loss:  1.80690840e-05, mean val. loss:  2.55511642e+00\n",
      "Epoch: 13143 mean train loss:  1.81763025e-05, mean val. loss:  2.55540967e+00\n",
      "Epoch: 13144 mean train loss:  1.80727220e-05, mean val. loss:  2.55571246e+00\n",
      "Epoch: 13145 mean train loss:  1.78933842e-05, mean val. loss:  2.55600095e+00\n",
      "Epoch: 13146 mean train loss:  1.80671923e-05, mean val. loss:  2.55626035e+00\n",
      "Epoch: 13147 mean train loss:  1.78660266e-05, mean val. loss:  2.55652094e+00\n",
      "Epoch: 13148 mean train loss:  1.78715563e-05, mean val. loss:  2.55676079e+00\n",
      "Epoch: 13149 mean train loss:  1.78170740e-05, mean val. loss:  2.55698633e+00\n",
      "Epoch: 13150 mean train loss:  1.81633222e-05, mean val. loss:  2.55716491e+00\n",
      "Epoch: 13151 mean train loss:  1.79083145e-05, mean val. loss:  2.55733871e+00\n",
      "Epoch: 13152 mean train loss:  1.78957707e-05, mean val. loss:  2.55753016e+00\n",
      "Epoch: 13153 mean train loss:  1.79921917e-05, mean val. loss:  2.55770516e+00\n",
      "Epoch: 13154 mean train loss:  1.79184135e-05, mean val. loss:  2.55787921e+00\n",
      "Epoch: 13155 mean train loss:  1.81182695e-05, mean val. loss:  2.55802274e+00\n",
      "Epoch: 13156 mean train loss:  1.79647759e-05, mean val. loss:  2.55815220e+00\n",
      "Epoch: 13157 mean train loss:  1.79625931e-05, mean val. loss:  2.55825734e+00\n",
      "Epoch: 13158 mean train loss:  1.80414063e-05, mean val. loss:  2.55838013e+00\n",
      "Epoch: 13159 mean train loss:  1.82712683e-05, mean val. loss:  2.55847931e+00\n",
      "Epoch: 13160 mean train loss:  1.79518829e-05, mean val. loss:  2.55857611e+00\n",
      "Epoch: 13161 mean train loss:  1.80363131e-05, mean val. loss:  2.55869651e+00\n",
      "Epoch: 13162 mean train loss:  1.79863709e-05, mean val. loss:  2.55880976e+00\n",
      "Epoch: 13163 mean train loss:  1.82917865e-05, mean val. loss:  2.55891728e+00\n",
      "Epoch: 13164 mean train loss:  1.79013587e-05, mean val. loss:  2.55904245e+00\n",
      "Epoch: 13165 mean train loss:  1.81241776e-05, mean val. loss:  2.55920386e+00\n",
      "Epoch: 13166 mean train loss:  1.80592178e-05, mean val. loss:  2.55936885e+00\n",
      "Epoch: 13167 mean train loss:  1.75384630e-05, mean val. loss:  2.55959368e+00\n",
      "Epoch: 13168 mean train loss:  1.79547060e-05, mean val. loss:  2.55985546e+00\n",
      "Epoch: 13169 mean train loss:  1.77073525e-05, mean val. loss:  2.56013513e+00\n",
      "Epoch: 13170 mean train loss:  1.80662028e-05, mean val. loss:  2.56042671e+00\n",
      "Epoch: 13171 mean train loss:  1.79681228e-05, mean val. loss:  2.56074309e+00\n",
      "Epoch: 13172 mean train loss:  1.82048534e-05, mean val. loss:  2.56102824e+00\n",
      "Epoch: 13173 mean train loss:  1.78366899e-05, mean val. loss:  2.56131935e+00\n",
      "Epoch: 13174 mean train loss:  1.82439690e-05, mean val. loss:  2.56158614e+00\n",
      "Epoch: 13175 mean train loss:  1.76873873e-05, mean val. loss:  2.56186152e+00\n",
      "Epoch: 13176 mean train loss:  1.81776704e-05, mean val. loss:  2.56213570e+00\n",
      "Epoch: 13177 mean train loss:  1.80833158e-05, mean val. loss:  2.56237745e+00\n",
      "Epoch: 13178 mean train loss:  1.82993826e-05, mean val. loss:  2.56258988e+00\n",
      "Epoch: 13179 mean train loss:  1.80453062e-05, mean val. loss:  2.56280708e+00\n",
      "Epoch: 13180 mean train loss:  1.81932992e-05, mean val. loss:  2.56300545e+00\n",
      "Epoch: 13181 mean train loss:  1.80053758e-05, mean val. loss:  2.56318283e+00\n",
      "Epoch: 13182 mean train loss:  1.79878843e-05, mean val. loss:  2.56336737e+00\n",
      "Epoch: 13183 mean train loss:  1.80144561e-05, mean val. loss:  2.56354165e+00\n",
      "Epoch: 13184 mean train loss:  1.78811606e-05, mean val. loss:  2.56370473e+00\n",
      "Epoch: 13185 mean train loss:  1.79749331e-05, mean val. loss:  2.56385279e+00\n",
      "Epoch: 13186 mean train loss:  1.78816263e-05, mean val. loss:  2.56401348e+00\n",
      "Epoch: 13187 mean train loss:  1.81334617e-05, mean val. loss:  2.56419015e+00\n",
      "Epoch: 13188 mean train loss:  1.79540075e-05, mean val. loss:  2.56438160e+00\n",
      "Epoch: 13189 mean train loss:  1.77957409e-05, mean val. loss:  2.56459308e+00\n",
      "Epoch: 13190 mean train loss:  1.81641954e-05, mean val. loss:  2.56480241e+00\n",
      "Epoch: 13191 mean train loss:  1.78706541e-05, mean val. loss:  2.56502080e+00\n",
      "Epoch: 13192 mean train loss:  1.76092726e-05, mean val. loss:  2.56527805e+00\n",
      "Epoch: 13193 mean train loss:  1.81482174e-05, mean val. loss:  2.56552076e+00\n",
      "Epoch: 13194 mean train loss:  1.79263589e-05, mean val. loss:  2.56576157e+00\n",
      "Epoch: 13195 mean train loss:  1.82028743e-05, mean val. loss:  2.56600332e+00\n",
      "Epoch: 13196 mean train loss:  1.75649475e-05, mean val. loss:  2.56631923e+00\n",
      "Epoch: 13197 mean train loss:  1.81364594e-05, mean val. loss:  2.56663203e+00\n",
      "Epoch: 13198 mean train loss:  1.81645155e-05, mean val. loss:  2.56694651e+00\n",
      "Epoch: 13199 mean train loss:  1.78684713e-05, mean val. loss:  2.56726027e+00\n",
      "Epoch: 13200 mean train loss:  1.77564216e-05, mean val. loss:  2.56758070e+00\n",
      "Epoch: 13201 mean train loss:  1.81795040e-05, mean val. loss:  2.56786585e+00\n",
      "Epoch: 13202 mean train loss:  1.80400384e-05, mean val. loss:  2.56812024e+00\n",
      "Epoch: 13203 mean train loss:  1.79976341e-05, mean val. loss:  2.56832719e+00\n",
      "Epoch: 13204 mean train loss:  1.79958297e-05, mean val. loss:  2.56854320e+00\n",
      "Epoch: 13205 mean train loss:  1.78746704e-05, mean val. loss:  2.56872487e+00\n",
      "Epoch: 13206 mean train loss:  1.80969946e-05, mean val. loss:  2.56889701e+00\n",
      "Epoch: 13207 mean train loss:  1.79713534e-05, mean val. loss:  2.56904840e+00\n",
      "Epoch: 13208 mean train loss:  1.82363729e-05, mean val. loss:  2.56912279e+00\n",
      "Epoch: 13209 mean train loss:  1.80166971e-05, mean val. loss:  2.56917500e+00\n",
      "Epoch: 13210 mean train loss:  1.81903015e-05, mean val. loss:  2.56920242e+00\n",
      "Epoch: 13211 mean train loss:  1.77521433e-05, mean val. loss:  2.56923556e+00\n",
      "Epoch: 13212 mean train loss:  1.78533664e-05, mean val. loss:  2.56924534e+00\n",
      "Epoch: 13213 mean train loss:  1.81382929e-05, mean val. loss:  2.56925917e+00\n",
      "Epoch: 13214 mean train loss:  1.76162575e-05, mean val. loss:  2.56929994e+00\n",
      "Epoch: 13215 mean train loss:  1.81728974e-05, mean val. loss:  2.56931710e+00\n",
      "Epoch: 13216 mean train loss:  1.80080242e-05, mean val. loss:  2.56933308e+00\n",
      "Epoch: 13217 mean train loss:  1.77480397e-05, mean val. loss:  2.56937218e+00\n",
      "Epoch: 13218 mean train loss:  1.79428025e-05, mean val. loss:  2.56943226e+00\n",
      "Epoch: 13219 mean train loss:  1.77155307e-05, mean val. loss:  2.56951880e+00\n",
      "Epoch: 13220 mean train loss:  1.77333422e-05, mean val. loss:  2.56966448e+00\n",
      "Epoch: 13221 mean train loss:  1.77586044e-05, mean val. loss:  2.56982660e+00\n",
      "Epoch: 13222 mean train loss:  1.77270849e-05, mean val. loss:  2.57004237e+00\n",
      "Epoch: 13223 mean train loss:  1.80996722e-05, mean val. loss:  2.57028222e+00\n",
      "Epoch: 13224 mean train loss:  1.79558410e-05, mean val. loss:  2.57056260e+00\n",
      "Epoch: 13225 mean train loss:  1.77382317e-05, mean val. loss:  2.57087970e+00\n",
      "Epoch: 13226 mean train loss:  1.81318319e-05, mean val. loss:  2.57121897e+00\n",
      "Epoch: 13227 mean train loss:  1.78584887e-05, mean val. loss:  2.57160711e+00\n",
      "Epoch: 13228 mean train loss:  1.81679789e-05, mean val. loss:  2.57198286e+00\n",
      "Epoch: 13229 mean train loss:  1.79896888e-05, mean val. loss:  2.57237172e+00\n",
      "Epoch: 13230 mean train loss:  1.78061018e-05, mean val. loss:  2.57276249e+00\n",
      "Epoch: 13231 mean train loss:  1.80454226e-05, mean val. loss:  2.57313561e+00\n",
      "Epoch: 13232 mean train loss:  1.78815972e-05, mean val. loss:  2.57351279e+00\n",
      "Epoch: 13233 mean train loss:  1.82178337e-05, mean val. loss:  2.57381988e+00\n",
      "Epoch: 13234 mean train loss:  1.81926007e-05, mean val. loss:  2.57411432e+00\n",
      "Epoch: 13235 mean train loss:  1.80749630e-05, mean val. loss:  2.57435489e+00\n",
      "Epoch: 13236 mean train loss:  1.77365728e-05, mean val. loss:  2.57456541e+00\n",
      "Epoch: 13237 mean train loss:  1.80117495e-05, mean val. loss:  2.57473612e+00\n",
      "Epoch: 13238 mean train loss:  1.79347699e-05, mean val. loss:  2.57487702e+00\n",
      "Epoch: 13239 mean train loss:  1.78114569e-05, mean val. loss:  2.57502770e+00\n",
      "Epoch: 13240 mean train loss:  1.77792390e-05, mean val. loss:  2.57516623e+00\n",
      "Epoch: 13241 mean train loss:  1.76856993e-05, mean val. loss:  2.57529712e+00\n",
      "Epoch: 13242 mean train loss:  1.81663781e-05, mean val. loss:  2.57539725e+00\n",
      "Epoch: 13243 mean train loss:  1.82077347e-05, mean val. loss:  2.57550859e+00\n",
      "Epoch: 13244 mean train loss:  1.84585806e-05, mean val. loss:  2.57561588e+00\n",
      "Epoch: 13245 mean train loss:  1.77512702e-05, mean val. loss:  2.57574248e+00\n",
      "Epoch: 13246 mean train loss:  1.78980699e-05, mean val. loss:  2.57587719e+00\n",
      "Epoch: 13247 mean train loss:  1.80896313e-05, mean val. loss:  2.57597995e+00\n",
      "Epoch: 13248 mean train loss:  1.78026967e-05, mean val. loss:  2.57610154e+00\n",
      "Epoch: 13249 mean train loss:  1.78024638e-05, mean val. loss:  2.57623768e+00\n",
      "Epoch: 13250 mean train loss:  1.82780786e-05, mean val. loss:  2.57638407e+00\n",
      "Epoch: 13251 mean train loss:  1.77921611e-05, mean val. loss:  2.57651806e+00\n",
      "Epoch: 13252 mean train loss:  1.76730100e-05, mean val. loss:  2.57667923e+00\n",
      "Epoch: 13253 mean train loss:  1.82536896e-05, mean val. loss:  2.57683325e+00\n",
      "Epoch: 13254 mean train loss:  1.79921626e-05, mean val. loss:  2.57698321e+00\n",
      "Epoch: 13255 mean train loss:  1.78967894e-05, mean val. loss:  2.57712817e+00\n",
      "Epoch: 13256 mean train loss:  1.78334885e-05, mean val. loss:  2.57725716e+00\n",
      "Epoch: 13257 mean train loss:  1.79685012e-05, mean val. loss:  2.57740045e+00\n",
      "Epoch: 13258 mean train loss:  1.79354101e-05, mean val. loss:  2.57751441e+00\n",
      "Epoch: 13259 mean train loss:  1.79296476e-05, mean val. loss:  2.57763243e+00\n",
      "Epoch: 13260 mean train loss:  1.79140479e-05, mean val. loss:  2.57773995e+00\n",
      "Epoch: 13261 mean train loss:  1.76440517e-05, mean val. loss:  2.57786512e+00\n",
      "Epoch: 13262 mean train loss:  1.79413764e-05, mean val. loss:  2.57803154e+00\n",
      "Epoch: 13263 mean train loss:  1.80551433e-05, mean val. loss:  2.57820082e+00\n",
      "Epoch: 13264 mean train loss:  1.81185605e-05, mean val. loss:  2.57837868e+00\n",
      "Epoch: 13265 mean train loss:  1.76991452e-05, mean val. loss:  2.57858372e+00\n",
      "Epoch: 13266 mean train loss:  1.77420152e-05, mean val. loss:  2.57879806e+00\n",
      "Epoch: 13267 mean train loss:  1.79604394e-05, mean val. loss:  2.57902026e+00\n",
      "Epoch: 13268 mean train loss:  1.80382049e-05, mean val. loss:  2.57925081e+00\n",
      "Epoch: 13269 mean train loss:  1.78325572e-05, mean val. loss:  2.57949710e+00\n",
      "Epoch: 13270 mean train loss:  1.81068899e-05, mean val. loss:  2.57976580e+00\n",
      "Epoch: 13271 mean train loss:  1.77618640e-05, mean val. loss:  2.58005857e+00\n",
      "Epoch: 13272 mean train loss:  1.75560417e-05, mean val. loss:  2.58038878e+00\n",
      "Epoch: 13273 mean train loss:  1.80930656e-05, mean val. loss:  2.58073854e+00\n",
      "Epoch: 13274 mean train loss:  1.73746375e-05, mean val. loss:  2.58115602e+00\n",
      "Epoch: 13275 mean train loss:  1.80350034e-05, mean val. loss:  2.58159256e+00\n",
      "Epoch: 13276 mean train loss:  1.79095659e-05, mean val. loss:  2.58204222e+00\n",
      "Epoch: 13277 mean train loss:  1.80141651e-05, mean val. loss:  2.58246374e+00\n",
      "Epoch: 13278 mean train loss:  1.80111383e-05, mean val. loss:  2.58287168e+00\n",
      "Epoch: 13279 mean train loss:  1.77693437e-05, mean val. loss:  2.58328319e+00\n",
      "Epoch: 13280 mean train loss:  1.77672482e-05, mean val. loss:  2.58368015e+00\n",
      "Epoch: 13281 mean train loss:  1.80999632e-05, mean val. loss:  2.58405542e+00\n",
      "Epoch: 13282 mean train loss:  1.75939349e-05, mean val. loss:  2.58441448e+00\n",
      "Epoch: 13283 mean train loss:  1.78567134e-05, mean val. loss:  2.58473611e+00\n",
      "Epoch: 13284 mean train loss:  1.81130308e-05, mean val. loss:  2.58503008e+00\n",
      "Epoch: 13285 mean train loss:  1.80114876e-05, mean val. loss:  2.58531165e+00\n",
      "Epoch: 13286 mean train loss:  1.79310155e-05, mean val. loss:  2.58557606e+00\n",
      "Epoch: 13287 mean train loss:  1.80867792e-05, mean val. loss:  2.58580923e+00\n",
      "Epoch: 13288 mean train loss:  1.77598267e-05, mean val. loss:  2.58604956e+00\n",
      "Epoch: 13289 mean train loss:  1.80945499e-05, mean val. loss:  2.58625698e+00\n",
      "Epoch: 13290 mean train loss:  1.80259813e-05, mean val. loss:  2.58645582e+00\n",
      "Epoch: 13291 mean train loss:  1.81464420e-05, mean val. loss:  2.58664703e+00\n",
      "Epoch: 13292 mean train loss:  1.78294722e-05, mean val. loss:  2.58683181e+00\n",
      "Epoch: 13293 mean train loss:  1.80694624e-05, mean val. loss:  2.58699417e+00\n",
      "Epoch: 13294 mean train loss:  1.76072645e-05, mean val. loss:  2.58716369e+00\n",
      "Epoch: 13295 mean train loss:  1.80596253e-05, mean val. loss:  2.58732772e+00\n",
      "Epoch: 13296 mean train loss:  1.74593879e-05, mean val. loss:  2.58752036e+00\n",
      "Epoch: 13297 mean train loss:  1.75183231e-05, mean val. loss:  2.58773351e+00\n",
      "Epoch: 13298 mean train loss:  1.76709436e-05, mean val. loss:  2.58794594e+00\n",
      "Epoch: 13299 mean train loss:  1.75814494e-05, mean val. loss:  2.58821869e+00\n",
      "Epoch: 13300 mean train loss:  1.80952775e-05, mean val. loss:  2.58847237e+00\n",
      "Epoch: 13301 mean train loss:  1.78410264e-05, mean val. loss:  2.58873248e+00\n",
      "Epoch: 13302 mean train loss:  1.77928596e-05, mean val. loss:  2.58901286e+00\n",
      "Epoch: 13303 mean train loss:  1.81056384e-05, mean val. loss:  2.58930492e+00\n",
      "Epoch: 13304 mean train loss:  1.81523210e-05, mean val. loss:  2.58959150e+00\n",
      "Epoch: 13305 mean train loss:  1.78995542e-05, mean val. loss:  2.58990884e+00\n",
      "Epoch: 13306 mean train loss:  1.79862836e-05, mean val. loss:  2.59020281e+00\n",
      "Epoch: 13307 mean train loss:  1.77171896e-05, mean val. loss:  2.59051895e+00\n",
      "Epoch: 13308 mean train loss:  1.74040615e-05, mean val. loss:  2.59085536e+00\n",
      "Epoch: 13309 mean train loss:  1.79760391e-05, mean val. loss:  2.59115148e+00\n",
      "Epoch: 13310 mean train loss:  1.76189642e-05, mean val. loss:  2.59142184e+00\n",
      "Epoch: 13311 mean train loss:  1.74621528e-05, mean val. loss:  2.59167957e+00\n",
      "Epoch: 13312 mean train loss:  1.78991759e-05, mean val. loss:  2.59191227e+00\n",
      "Epoch: 13313 mean train loss:  1.79651834e-05, mean val. loss:  2.59210634e+00\n",
      "Epoch: 13314 mean train loss:  1.79577910e-05, mean val. loss:  2.59227324e+00\n",
      "Epoch: 13315 mean train loss:  1.80280767e-05, mean val. loss:  2.59242225e+00\n",
      "Epoch: 13316 mean train loss:  1.79045310e-05, mean val. loss:  2.59255266e+00\n",
      "Epoch: 13317 mean train loss:  1.79077033e-05, mean val. loss:  2.59267139e+00\n",
      "Epoch: 13318 mean train loss:  1.76965550e-05, mean val. loss:  2.59280181e+00\n",
      "Epoch: 13319 mean train loss:  1.79252238e-05, mean val. loss:  2.59289527e+00\n",
      "Epoch: 13320 mean train loss:  1.77850306e-05, mean val. loss:  2.59298420e+00\n",
      "Epoch: 13321 mean train loss:  1.79839262e-05, mean val. loss:  2.59307241e+00\n",
      "Epoch: 13322 mean train loss:  1.78914634e-05, mean val. loss:  2.59312940e+00\n",
      "Epoch: 13323 mean train loss:  1.75534515e-05, mean val. loss:  2.59320474e+00\n",
      "Epoch: 13324 mean train loss:  1.78620976e-05, mean val. loss:  2.59327793e+00\n",
      "Epoch: 13325 mean train loss:  1.79662602e-05, mean val. loss:  2.59333110e+00\n",
      "Epoch: 13326 mean train loss:  1.80498755e-05, mean val. loss:  2.59338355e+00\n",
      "Epoch: 13327 mean train loss:  1.78568880e-05, mean val. loss:  2.59343171e+00\n",
      "Epoch: 13328 mean train loss:  1.77543552e-05, mean val. loss:  2.59347486e+00\n",
      "Epoch: 13329 mean train loss:  1.82140502e-05, mean val. loss:  2.59353161e+00\n",
      "Epoch: 13330 mean train loss:  1.76825561e-05, mean val. loss:  2.59361458e+00\n",
      "Epoch: 13331 mean train loss:  1.77892507e-05, mean val. loss:  2.59374189e+00\n",
      "Epoch: 13332 mean train loss:  1.77186739e-05, mean val. loss:  2.59388757e+00\n",
      "Epoch: 13333 mean train loss:  1.77639013e-05, mean val. loss:  2.59404993e+00\n",
      "Epoch: 13334 mean train loss:  1.80413481e-05, mean val. loss:  2.59423137e+00\n",
      "Epoch: 13335 mean train loss:  1.80947536e-05, mean val. loss:  2.59439564e+00\n",
      "Epoch: 13336 mean train loss:  1.76184112e-05, mean val. loss:  2.59459710e+00\n",
      "Epoch: 13337 mean train loss:  1.77111360e-05, mean val. loss:  2.59483624e+00\n",
      "Epoch: 13338 mean train loss:  1.78259215e-05, mean val. loss:  2.59509516e+00\n",
      "Epoch: 13339 mean train loss:  1.76472822e-05, mean val. loss:  2.59538531e+00\n",
      "Epoch: 13340 mean train loss:  1.78716436e-05, mean val. loss:  2.59568191e+00\n",
      "Epoch: 13341 mean train loss:  1.80609641e-05, mean val. loss:  2.59595799e+00\n",
      "Epoch: 13342 mean train loss:  1.79351773e-05, mean val. loss:  2.59625793e+00\n",
      "Epoch: 13343 mean train loss:  1.78657356e-05, mean val. loss:  2.59656191e+00\n",
      "Epoch: 13344 mean train loss:  1.79006020e-05, mean val. loss:  2.59686136e+00\n",
      "Epoch: 13345 mean train loss:  1.78003684e-05, mean val. loss:  2.59715509e+00\n",
      "Epoch: 13346 mean train loss:  1.77058973e-05, mean val. loss:  2.59745765e+00\n",
      "Epoch: 13347 mean train loss:  1.78399205e-05, mean val. loss:  2.59775329e+00\n",
      "Epoch: 13348 mean train loss:  1.78537448e-05, mean val. loss:  2.59799695e+00\n",
      "Epoch: 13349 mean train loss:  1.76878821e-05, mean val. loss:  2.59824586e+00\n",
      "Epoch: 13350 mean train loss:  1.79737981e-05, mean val. loss:  2.59845328e+00\n",
      "Epoch: 13351 mean train loss:  1.76524918e-05, mean val. loss:  2.59862995e+00\n",
      "Epoch: 13352 mean train loss:  1.78299670e-05, mean val. loss:  2.59879565e+00\n",
      "Epoch: 13353 mean train loss:  1.78825285e-05, mean val. loss:  2.59896374e+00\n",
      "Epoch: 13354 mean train loss:  1.78049377e-05, mean val. loss:  2.59910607e+00\n",
      "Epoch: 13355 mean train loss:  1.78874761e-05, mean val. loss:  2.59925461e+00\n",
      "Epoch: 13356 mean train loss:  1.78677728e-05, mean val. loss:  2.59939384e+00\n",
      "Epoch: 13357 mean train loss:  1.80639036e-05, mean val. loss:  2.59950590e+00\n",
      "Epoch: 13358 mean train loss:  1.79892231e-05, mean val. loss:  2.59962416e+00\n",
      "Epoch: 13359 mean train loss:  1.75375026e-05, mean val. loss:  2.59974909e+00\n",
      "Epoch: 13360 mean train loss:  1.77118636e-05, mean val. loss:  2.59989142e+00\n",
      "Epoch: 13361 mean train loss:  1.77660550e-05, mean val. loss:  2.60006976e+00\n",
      "Epoch: 13362 mean train loss:  1.75585446e-05, mean val. loss:  2.60027885e+00\n",
      "Epoch: 13363 mean train loss:  1.74366869e-05, mean val. loss:  2.60051274e+00\n",
      "Epoch: 13364 mean train loss:  1.75848254e-05, mean val. loss:  2.60078835e+00\n",
      "Epoch: 13365 mean train loss:  1.80020870e-05, mean val. loss:  2.60107279e+00\n",
      "Epoch: 13366 mean train loss:  1.77124166e-05, mean val. loss:  2.60140657e+00\n",
      "Epoch: 13367 mean train loss:  1.76213216e-05, mean val. loss:  2.60177851e+00\n",
      "Epoch: 13368 mean train loss:  1.78204209e-05, mean val. loss:  2.60216355e+00\n",
      "Epoch: 13369 mean train loss:  1.78301998e-05, mean val. loss:  2.60256743e+00\n",
      "Epoch: 13370 mean train loss:  1.78467308e-05, mean val. loss:  2.60296488e+00\n",
      "Epoch: 13371 mean train loss:  1.79133203e-05, mean val. loss:  2.60335255e+00\n",
      "Epoch: 13372 mean train loss:  1.80793577e-05, mean val. loss:  2.60374141e+00\n",
      "Epoch: 13373 mean train loss:  1.80829957e-05, mean val. loss:  2.60408092e+00\n",
      "Epoch: 13374 mean train loss:  1.78298797e-05, mean val. loss:  2.60437322e+00\n",
      "Epoch: 13375 mean train loss:  1.75599125e-05, mean val. loss:  2.60465693e+00\n",
      "Epoch: 13376 mean train loss:  1.77595648e-05, mean val. loss:  2.60490155e+00\n",
      "Epoch: 13377 mean train loss:  1.79296185e-05, mean val. loss:  2.60513043e+00\n",
      "Epoch: 13378 mean train loss:  1.78155606e-05, mean val. loss:  2.60531402e+00\n",
      "Epoch: 13379 mean train loss:  1.78503105e-05, mean val. loss:  2.60548973e+00\n",
      "Epoch: 13380 mean train loss:  1.81928626e-05, mean val. loss:  2.60562086e+00\n",
      "Epoch: 13381 mean train loss:  1.80702773e-05, mean val. loss:  2.60571170e+00\n",
      "Epoch: 13382 mean train loss:  1.78207993e-05, mean val. loss:  2.60579062e+00\n",
      "Epoch: 13383 mean train loss:  1.77012116e-05, mean val. loss:  2.60589862e+00\n",
      "Epoch: 13384 mean train loss:  1.78082555e-05, mean val. loss:  2.60601449e+00\n",
      "Epoch: 13385 mean train loss:  1.78680348e-05, mean val. loss:  2.60612893e+00\n",
      "Epoch: 13386 mean train loss:  1.76710892e-05, mean val. loss:  2.60621881e+00\n",
      "Epoch: 13387 mean train loss:  1.78626506e-05, mean val. loss:  2.60630012e+00\n",
      "Epoch: 13388 mean train loss:  1.80642237e-05, mean val. loss:  2.60640001e+00\n",
      "Epoch: 13389 mean train loss:  1.79095659e-05, mean val. loss:  2.60650325e+00\n",
      "Epoch: 13390 mean train loss:  1.77224865e-05, mean val. loss:  2.60662770e+00\n",
      "Epoch: 13391 mean train loss:  1.81299692e-05, mean val. loss:  2.60674334e+00\n",
      "Epoch: 13392 mean train loss:  1.79204508e-05, mean val. loss:  2.60688043e+00\n",
      "Epoch: 13393 mean train loss:  1.80631469e-05, mean val. loss:  2.60700846e+00\n",
      "Epoch: 13394 mean train loss:  1.80075294e-05, mean val. loss:  2.60714197e+00\n",
      "Epoch: 13395 mean train loss:  1.78702467e-05, mean val. loss:  2.60726357e+00\n",
      "Epoch: 13396 mean train loss:  1.78300834e-05, mean val. loss:  2.60736752e+00\n",
      "Epoch: 13397 mean train loss:  1.77375914e-05, mean val. loss:  2.60750294e+00\n",
      "Epoch: 13398 mean train loss:  1.76648318e-05, mean val. loss:  2.60767388e+00\n",
      "Epoch: 13399 mean train loss:  1.78690243e-05, mean val. loss:  2.60782003e+00\n",
      "Epoch: 13400 mean train loss:  1.78010087e-05, mean val. loss:  2.60797524e+00\n",
      "Epoch: 13401 mean train loss:  1.79540657e-05, mean val. loss:  2.60814500e+00\n",
      "Epoch: 13402 mean train loss:  1.77641632e-05, mean val. loss:  2.60832167e+00\n",
      "Epoch: 13403 mean train loss:  1.79773197e-05, mean val. loss:  2.60850120e+00\n",
      "Epoch: 13404 mean train loss:  1.74898305e-05, mean val. loss:  2.60868430e+00\n",
      "Epoch: 13405 mean train loss:  1.80371862e-05, mean val. loss:  2.60887265e+00\n",
      "Epoch: 13406 mean train loss:  1.76206522e-05, mean val. loss:  2.60907626e+00\n",
      "Epoch: 13407 mean train loss:  1.77331385e-05, mean val. loss:  2.60929918e+00\n",
      "Epoch: 13408 mean train loss:  1.77007751e-05, mean val. loss:  2.60954499e+00\n",
      "Epoch: 13409 mean train loss:  1.80132629e-05, mean val. loss:  2.60978484e+00\n",
      "Epoch: 13410 mean train loss:  1.77550246e-05, mean val. loss:  2.60999131e+00\n",
      "Epoch: 13411 mean train loss:  1.78238843e-05, mean val. loss:  2.61019206e+00\n",
      "Epoch: 13412 mean train loss:  1.80240022e-05, mean val. loss:  2.61035681e+00\n",
      "Epoch: 13413 mean train loss:  1.79857307e-05, mean val. loss:  2.61053157e+00\n",
      "Epoch: 13414 mean train loss:  1.77524635e-05, mean val. loss:  2.61072969e+00\n",
      "Epoch: 13415 mean train loss:  1.80791540e-05, mean val. loss:  2.61092925e+00\n",
      "Epoch: 13416 mean train loss:  1.78150076e-05, mean val. loss:  2.61112118e+00\n",
      "Epoch: 13417 mean train loss:  1.74310699e-05, mean val. loss:  2.61132288e+00\n",
      "Epoch: 13418 mean train loss:  1.79283961e-05, mean val. loss:  2.61149693e+00\n",
      "Epoch: 13419 mean train loss:  1.76045287e-05, mean val. loss:  2.61169672e+00\n",
      "Epoch: 13420 mean train loss:  1.79576164e-05, mean val. loss:  2.61189771e+00\n",
      "Epoch: 13421 mean train loss:  1.78244663e-05, mean val. loss:  2.61211252e+00\n",
      "Epoch: 13422 mean train loss:  1.78234186e-05, mean val. loss:  2.61234188e+00\n",
      "Epoch: 13423 mean train loss:  1.75703317e-05, mean val. loss:  2.61256552e+00\n",
      "Epoch: 13424 mean train loss:  1.76649482e-05, mean val. loss:  2.61279964e+00\n",
      "Epoch: 13425 mean train loss:  1.80933857e-05, mean val. loss:  2.61305356e+00\n",
      "Epoch: 13426 mean train loss:  1.78139599e-05, mean val. loss:  2.61333919e+00\n",
      "Epoch: 13427 mean train loss:  1.76624453e-05, mean val. loss:  2.61362720e+00\n",
      "Epoch: 13428 mean train loss:  1.78970222e-05, mean val. loss:  2.61392713e+00\n",
      "Epoch: 13429 mean train loss:  1.75367168e-05, mean val. loss:  2.61427760e+00\n",
      "Epoch: 13430 mean train loss:  1.77217589e-05, mean val. loss:  2.61463404e+00\n",
      "Epoch: 13431 mean train loss:  1.74161687e-05, mean val. loss:  2.61501932e+00\n",
      "Epoch: 13432 mean train loss:  1.79479539e-05, mean val. loss:  2.61538887e+00\n",
      "Epoch: 13433 mean train loss:  1.80365168e-05, mean val. loss:  2.61574697e+00\n",
      "Epoch: 13434 mean train loss:  1.74372108e-05, mean val. loss:  2.61612654e+00\n",
      "Epoch: 13435 mean train loss:  1.77713810e-05, mean val. loss:  2.61651945e+00\n",
      "Epoch: 13436 mean train loss:  1.76307512e-05, mean val. loss:  2.61691952e+00\n",
      "Epoch: 13437 mean train loss:  1.78229238e-05, mean val. loss:  2.61730075e+00\n",
      "Epoch: 13438 mean train loss:  1.77665497e-05, mean val. loss:  2.61767817e+00\n",
      "Epoch: 13439 mean train loss:  1.79060735e-05, mean val. loss:  2.61802101e+00\n",
      "Epoch: 13440 mean train loss:  1.79956842e-05, mean val. loss:  2.61832714e+00\n",
      "Epoch: 13441 mean train loss:  1.79755734e-05, mean val. loss:  2.61862063e+00\n",
      "Epoch: 13442 mean train loss:  1.75900641e-05, mean val. loss:  2.61888242e+00\n",
      "Epoch: 13443 mean train loss:  1.76249596e-05, mean val. loss:  2.61914086e+00\n",
      "Epoch: 13444 mean train loss:  1.78043265e-05, mean val. loss:  2.61939001e+00\n",
      "Epoch: 13445 mean train loss:  1.78980408e-05, mean val. loss:  2.61960745e+00\n",
      "Epoch: 13446 mean train loss:  1.79060153e-05, mean val. loss:  2.61979365e+00\n",
      "Epoch: 13447 mean train loss:  1.75689929e-05, mean val. loss:  2.61999583e+00\n",
      "Epoch: 13448 mean train loss:  1.75111345e-05, mean val. loss:  2.62018037e+00\n",
      "Epoch: 13449 mean train loss:  1.76950998e-05, mean val. loss:  2.62035799e+00\n",
      "Epoch: 13450 mean train loss:  1.80161733e-05, mean val. loss:  2.62048936e+00\n",
      "Epoch: 13451 mean train loss:  1.78096234e-05, mean val. loss:  2.62064004e+00\n",
      "Epoch: 13452 mean train loss:  1.79542694e-05, mean val. loss:  2.62078762e+00\n",
      "Epoch: 13453 mean train loss:  1.79718481e-05, mean val. loss:  2.62092018e+00\n",
      "Epoch: 13454 mean train loss:  1.82381773e-05, mean val. loss:  2.62104177e+00\n",
      "Epoch: 13455 mean train loss:  1.81082578e-05, mean val. loss:  2.62112761e+00\n",
      "Epoch: 13456 mean train loss:  1.75928581e-05, mean val. loss:  2.62124562e+00\n",
      "Epoch: 13457 mean train loss:  1.78737682e-05, mean val. loss:  2.62134290e+00\n",
      "Epoch: 13458 mean train loss:  1.76479225e-05, mean val. loss:  2.62145662e+00\n",
      "Epoch: 13459 mean train loss:  1.77469046e-05, mean val. loss:  2.62156343e+00\n",
      "Epoch: 13460 mean train loss:  1.80280767e-05, mean val. loss:  2.62166309e+00\n",
      "Epoch: 13461 mean train loss:  1.75347668e-05, mean val. loss:  2.62175202e+00\n",
      "Epoch: 13462 mean train loss:  1.78889022e-05, mean val. loss:  2.62185383e+00\n",
      "Epoch: 13463 mean train loss:  1.78459159e-05, mean val. loss:  2.62194920e+00\n",
      "Epoch: 13464 mean train loss:  1.77315960e-05, mean val. loss:  2.62208271e+00\n",
      "Epoch: 13465 mean train loss:  1.77767361e-05, mean val. loss:  2.62218404e+00\n",
      "Epoch: 13466 mean train loss:  1.80070929e-05, mean val. loss:  2.62226081e+00\n",
      "Epoch: 13467 mean train loss:  1.80787174e-05, mean val. loss:  2.62235117e+00\n",
      "Epoch: 13468 mean train loss:  1.77421316e-05, mean val. loss:  2.62245607e+00\n",
      "Epoch: 13469 mean train loss:  1.79012713e-05, mean val. loss:  2.62257886e+00\n",
      "Epoch: 13470 mean train loss:  1.76620670e-05, mean val. loss:  2.62271500e+00\n",
      "Epoch: 13471 mean train loss:  1.78003684e-05, mean val. loss:  2.62283945e+00\n",
      "Epoch: 13472 mean train loss:  1.79451599e-05, mean val. loss:  2.62301397e+00\n",
      "Epoch: 13473 mean train loss:  1.76507165e-05, mean val. loss:  2.62319493e+00\n",
      "Epoch: 13474 mean train loss:  1.77087495e-05, mean val. loss:  2.62338138e+00\n",
      "Epoch: 13475 mean train loss:  1.80253119e-05, mean val. loss:  2.62357712e+00\n",
      "Epoch: 13476 mean train loss:  1.76314788e-05, mean val. loss:  2.62381673e+00\n",
      "Epoch: 13477 mean train loss:  1.76639878e-05, mean val. loss:  2.62410092e+00\n",
      "Epoch: 13478 mean train loss:  1.77470210e-05, mean val. loss:  2.62442017e+00\n",
      "Epoch: 13479 mean train loss:  1.78507762e-05, mean val. loss:  2.62475324e+00\n",
      "Epoch: 13480 mean train loss:  1.76595058e-05, mean val. loss:  2.62506104e+00\n",
      "Epoch: 13481 mean train loss:  1.75758905e-05, mean val. loss:  2.62537670e+00\n",
      "Epoch: 13482 mean train loss:  1.75760943e-05, mean val. loss:  2.62570715e+00\n",
      "Epoch: 13483 mean train loss:  1.77870388e-05, mean val. loss:  2.62605906e+00\n",
      "Epoch: 13484 mean train loss:  1.77756010e-05, mean val. loss:  2.62640858e+00\n",
      "Epoch: 13485 mean train loss:  1.76653266e-05, mean val. loss:  2.62678480e+00\n",
      "Epoch: 13486 mean train loss:  1.76693138e-05, mean val. loss:  2.62713599e+00\n",
      "Epoch: 13487 mean train loss:  1.77130569e-05, mean val. loss:  2.62748408e+00\n",
      "Epoch: 13488 mean train loss:  1.77272887e-05, mean val. loss:  2.62786961e+00\n",
      "Epoch: 13489 mean train loss:  1.80651259e-05, mean val. loss:  2.62820077e+00\n",
      "Epoch: 13490 mean train loss:  1.78309565e-05, mean val. loss:  2.62850165e+00\n",
      "Epoch: 13491 mean train loss:  1.77306065e-05, mean val. loss:  2.62880731e+00\n",
      "Epoch: 13492 mean train loss:  1.76774338e-05, mean val. loss:  2.62908602e+00\n",
      "Epoch: 13493 mean train loss:  1.76846224e-05, mean val. loss:  2.62934113e+00\n",
      "Epoch: 13494 mean train loss:  1.79601775e-05, mean val. loss:  2.62955642e+00\n",
      "Epoch: 13495 mean train loss:  1.75543828e-05, mean val. loss:  2.62976527e+00\n",
      "Epoch: 13496 mean train loss:  1.80309580e-05, mean val. loss:  2.62990308e+00\n",
      "Epoch: 13497 mean train loss:  1.77228940e-05, mean val. loss:  2.63001585e+00\n",
      "Epoch: 13498 mean train loss:  1.78629998e-05, mean val. loss:  2.63012719e+00\n",
      "Epoch: 13499 mean train loss:  1.77669863e-05, mean val. loss:  2.63024974e+00\n",
      "Epoch: 13500 mean train loss:  1.79686758e-05, mean val. loss:  2.63035655e+00\n",
      "Epoch: 13501 mean train loss:  1.81180658e-05, mean val. loss:  2.63045907e+00\n",
      "Epoch: 13502 mean train loss:  1.77563634e-05, mean val. loss:  2.63057017e+00\n",
      "Epoch: 13503 mean train loss:  1.77754264e-05, mean val. loss:  2.63069248e+00\n",
      "Epoch: 13504 mean train loss:  1.82162621e-05, mean val. loss:  2.63082051e+00\n",
      "Epoch: 13505 mean train loss:  1.78710325e-05, mean val. loss:  2.63094997e+00\n",
      "Epoch: 13506 mean train loss:  1.75798486e-05, mean val. loss:  2.63112473e+00\n",
      "Epoch: 13507 mean train loss:  1.77756301e-05, mean val. loss:  2.63131881e+00\n",
      "Epoch: 13508 mean train loss:  1.76152680e-05, mean val. loss:  2.63153148e+00\n",
      "Epoch: 13509 mean train loss:  1.76736794e-05, mean val. loss:  2.63176537e+00\n",
      "Epoch: 13510 mean train loss:  1.77854381e-05, mean val. loss:  2.63201642e+00\n",
      "Epoch: 13511 mean train loss:  1.75550813e-05, mean val. loss:  2.63231087e+00\n",
      "Epoch: 13512 mean train loss:  1.79749914e-05, mean val. loss:  2.63261151e+00\n",
      "Epoch: 13513 mean train loss:  1.77892798e-05, mean val. loss:  2.63294101e+00\n",
      "Epoch: 13514 mean train loss:  1.78975170e-05, mean val. loss:  2.63326240e+00\n",
      "Epoch: 13515 mean train loss:  1.76470494e-05, mean val. loss:  2.63362598e+00\n",
      "Epoch: 13516 mean train loss:  1.81695796e-05, mean val. loss:  2.63397908e+00\n",
      "Epoch: 13517 mean train loss:  1.78261835e-05, mean val. loss:  2.63434339e+00\n",
      "Epoch: 13518 mean train loss:  1.77606125e-05, mean val. loss:  2.63470840e+00\n",
      "Epoch: 13519 mean train loss:  1.75116293e-05, mean val. loss:  2.63507438e+00\n",
      "Epoch: 13520 mean train loss:  1.79604976e-05, mean val. loss:  2.63539767e+00\n",
      "Epoch: 13521 mean train loss:  1.76397443e-05, mean val. loss:  2.63571310e+00\n",
      "Epoch: 13522 mean train loss:  1.76673639e-05, mean val. loss:  2.63601422e+00\n",
      "Epoch: 13523 mean train loss:  1.77881448e-05, mean val. loss:  2.63632131e+00\n",
      "Epoch: 13524 mean train loss:  1.80938805e-05, mean val. loss:  2.63654780e+00\n",
      "Epoch: 13525 mean train loss:  1.76021422e-05, mean val. loss:  2.63676071e+00\n",
      "Epoch: 13526 mean train loss:  1.79285707e-05, mean val. loss:  2.63693595e+00\n",
      "Epoch: 13527 mean train loss:  1.77220209e-05, mean val. loss:  2.63709545e+00\n",
      "Epoch: 13528 mean train loss:  1.77434704e-05, mean val. loss:  2.63725543e+00\n",
      "Epoch: 13529 mean train loss:  1.73542066e-05, mean val. loss:  2.63740587e+00\n",
      "Epoch: 13530 mean train loss:  1.77488837e-05, mean val. loss:  2.63749671e+00\n",
      "Epoch: 13531 mean train loss:  1.77455659e-05, mean val. loss:  2.63754773e+00\n",
      "Epoch: 13532 mean train loss:  1.78594782e-05, mean val. loss:  2.63756227e+00\n",
      "Epoch: 13533 mean train loss:  1.79711205e-05, mean val. loss:  2.63754296e+00\n",
      "Epoch: 13534 mean train loss:  1.78822665e-05, mean val. loss:  2.63752604e+00\n",
      "Epoch: 13535 mean train loss:  1.80212082e-05, mean val. loss:  2.63748384e+00\n",
      "Epoch: 13536 mean train loss:  1.76595640e-05, mean val. loss:  2.63745952e+00\n",
      "Epoch: 13537 mean train loss:  1.79370400e-05, mean val. loss:  2.63744283e+00\n",
      "Epoch: 13538 mean train loss:  1.75898604e-05, mean val. loss:  2.63746047e+00\n",
      "Epoch: 13539 mean train loss:  1.78059272e-05, mean val. loss:  2.63751650e+00\n",
      "Epoch: 13540 mean train loss:  1.80206844e-05, mean val. loss:  2.63760352e+00\n",
      "Epoch: 13541 mean train loss:  1.81143405e-05, mean val. loss:  2.63770509e+00\n",
      "Epoch: 13542 mean train loss:  1.79867493e-05, mean val. loss:  2.63784099e+00\n",
      "Epoch: 13543 mean train loss:  1.76981266e-05, mean val. loss:  2.63802099e+00\n",
      "Epoch: 13544 mean train loss:  1.78489718e-05, mean val. loss:  2.63820267e+00\n",
      "Epoch: 13545 mean train loss:  1.75048190e-05, mean val. loss:  2.63839102e+00\n",
      "Epoch: 13546 mean train loss:  1.77568872e-05, mean val. loss:  2.63865328e+00\n",
      "Epoch: 13547 mean train loss:  1.79848284e-05, mean val. loss:  2.63889861e+00\n",
      "Epoch: 13548 mean train loss:  1.77046459e-05, mean val. loss:  2.63914847e+00\n",
      "Epoch: 13549 mean train loss:  1.74371526e-05, mean val. loss:  2.63943195e+00\n",
      "Epoch: 13550 mean train loss:  1.77192269e-05, mean val. loss:  2.63973761e+00\n",
      "Epoch: 13551 mean train loss:  1.79176568e-05, mean val. loss:  2.64003587e+00\n",
      "Epoch: 13552 mean train loss:  1.78071787e-05, mean val. loss:  2.64032364e+00\n",
      "Epoch: 13553 mean train loss:  1.74814486e-05, mean val. loss:  2.64060402e+00\n",
      "Epoch: 13554 mean train loss:  1.77058100e-05, mean val. loss:  2.64089966e+00\n",
      "Epoch: 13555 mean train loss:  1.75430032e-05, mean val. loss:  2.64120674e+00\n",
      "Epoch: 13556 mean train loss:  1.76925678e-05, mean val. loss:  2.64147592e+00\n",
      "Epoch: 13557 mean train loss:  1.74327579e-05, mean val. loss:  2.64174843e+00\n",
      "Epoch: 13558 mean train loss:  1.79565104e-05, mean val. loss:  2.64198017e+00\n",
      "Epoch: 13559 mean train loss:  1.77326438e-05, mean val. loss:  2.64218974e+00\n",
      "Epoch: 13560 mean train loss:  1.77913171e-05, mean val. loss:  2.64237714e+00\n",
      "Epoch: 13561 mean train loss:  1.77316542e-05, mean val. loss:  2.64255166e+00\n",
      "Epoch: 13562 mean train loss:  1.76759495e-05, mean val. loss:  2.64273143e+00\n",
      "Epoch: 13563 mean train loss:  1.73741719e-05, mean val. loss:  2.64290595e+00\n",
      "Epoch: 13564 mean train loss:  1.79561321e-05, mean val. loss:  2.64307594e+00\n",
      "Epoch: 13565 mean train loss:  1.76454778e-05, mean val. loss:  2.64324164e+00\n",
      "Epoch: 13566 mean train loss:  1.77299662e-05, mean val. loss:  2.64344668e+00\n",
      "Epoch: 13567 mean train loss:  1.78171904e-05, mean val. loss:  2.64367318e+00\n",
      "Epoch: 13568 mean train loss:  1.78779883e-05, mean val. loss:  2.64388561e+00\n",
      "Epoch: 13569 mean train loss:  1.76008325e-05, mean val. loss:  2.64412737e+00\n",
      "Epoch: 13570 mean train loss:  1.77915790e-05, mean val. loss:  2.64434695e+00\n",
      "Epoch: 13571 mean train loss:  1.78749324e-05, mean val. loss:  2.64454627e+00\n",
      "Epoch: 13572 mean train loss:  1.77592738e-05, mean val. loss:  2.64476848e+00\n",
      "Epoch: 13573 mean train loss:  1.78872142e-05, mean val. loss:  2.64499211e+00\n",
      "Epoch: 13574 mean train loss:  1.75166933e-05, mean val. loss:  2.64525700e+00\n",
      "Epoch: 13575 mean train loss:  1.78067130e-05, mean val. loss:  2.64551210e+00\n",
      "Epoch: 13576 mean train loss:  1.74729212e-05, mean val. loss:  2.64575338e+00\n",
      "Epoch: 13577 mean train loss:  1.78362825e-05, mean val. loss:  2.64599371e+00\n",
      "Epoch: 13578 mean train loss:  1.77617185e-05, mean val. loss:  2.64620829e+00\n",
      "Epoch: 13579 mean train loss:  1.79838389e-05, mean val. loss:  2.64640427e+00\n",
      "Epoch: 13580 mean train loss:  1.74836605e-05, mean val. loss:  2.64659357e+00\n",
      "Epoch: 13581 mean train loss:  1.76185567e-05, mean val. loss:  2.64676309e+00\n",
      "Epoch: 13582 mean train loss:  1.76064204e-05, mean val. loss:  2.64693928e+00\n",
      "Epoch: 13583 mean train loss:  1.75959722e-05, mean val. loss:  2.64712763e+00\n",
      "Epoch: 13584 mean train loss:  1.76212634e-05, mean val. loss:  2.64728737e+00\n",
      "Epoch: 13585 mean train loss:  1.78343907e-05, mean val. loss:  2.64745593e+00\n",
      "Epoch: 13586 mean train loss:  1.77227193e-05, mean val. loss:  2.64763570e+00\n",
      "Epoch: 13587 mean train loss:  1.74079905e-05, mean val. loss:  2.64784408e+00\n",
      "Epoch: 13588 mean train loss:  1.78810733e-05, mean val. loss:  2.64809036e+00\n",
      "Epoch: 13589 mean train loss:  1.74189918e-05, mean val. loss:  2.64834690e+00\n",
      "Epoch: 13590 mean train loss:  1.75455352e-05, mean val. loss:  2.64865088e+00\n",
      "Epoch: 13591 mean train loss:  1.79072376e-05, mean val. loss:  2.64891839e+00\n",
      "Epoch: 13592 mean train loss:  1.75213790e-05, mean val. loss:  2.64920235e+00\n",
      "Epoch: 13593 mean train loss:  1.78247574e-05, mean val. loss:  2.64949536e+00\n",
      "Epoch: 13594 mean train loss:  1.77159091e-05, mean val. loss:  2.64983034e+00\n",
      "Epoch: 13595 mean train loss:  1.75977766e-05, mean val. loss:  2.65018964e+00\n",
      "Epoch: 13596 mean train loss:  1.76154135e-05, mean val. loss:  2.65056252e+00\n",
      "Epoch: 13597 mean train loss:  1.78370974e-05, mean val. loss:  2.65096211e+00\n",
      "Epoch: 13598 mean train loss:  1.75301975e-05, mean val. loss:  2.65137529e+00\n",
      "Epoch: 13599 mean train loss:  1.75165187e-05, mean val. loss:  2.65180278e+00\n",
      "Epoch: 13600 mean train loss:  1.75552268e-05, mean val. loss:  2.65223050e+00\n",
      "Epoch: 13601 mean train loss:  1.78093032e-05, mean val. loss:  2.65262365e+00\n",
      "Epoch: 13602 mean train loss:  1.76835165e-05, mean val. loss:  2.65302134e+00\n",
      "Epoch: 13603 mean train loss:  1.75772002e-05, mean val. loss:  2.65339613e+00\n",
      "Epoch: 13604 mean train loss:  1.77275506e-05, mean val. loss:  2.65371442e+00\n",
      "Epoch: 13605 mean train loss:  1.78316259e-05, mean val. loss:  2.65401244e+00\n",
      "Epoch: 13606 mean train loss:  1.74980669e-05, mean val. loss:  2.65427160e+00\n",
      "Epoch: 13607 mean train loss:  1.75382593e-05, mean val. loss:  2.65449095e+00\n",
      "Epoch: 13608 mean train loss:  1.76292961e-05, mean val. loss:  2.65466785e+00\n",
      "Epoch: 13609 mean train loss:  1.75850582e-05, mean val. loss:  2.65483046e+00\n",
      "Epoch: 13610 mean train loss:  1.78341579e-05, mean val. loss:  2.65495706e+00\n",
      "Epoch: 13611 mean train loss:  1.78904156e-05, mean val. loss:  2.65503645e+00\n",
      "Epoch: 13612 mean train loss:  1.74895686e-05, mean val. loss:  2.65510654e+00\n",
      "Epoch: 13613 mean train loss:  1.77650945e-05, mean val. loss:  2.65516305e+00\n",
      "Epoch: 13614 mean train loss:  1.75758905e-05, mean val. loss:  2.65524030e+00\n",
      "Epoch: 13615 mean train loss:  1.75563328e-05, mean val. loss:  2.65536475e+00\n",
      "Epoch: 13616 mean train loss:  1.76981266e-05, mean val. loss:  2.65547585e+00\n",
      "Epoch: 13617 mean train loss:  1.76125614e-05, mean val. loss:  2.65557885e+00\n",
      "Epoch: 13618 mean train loss:  1.75776950e-05, mean val. loss:  2.65571237e+00\n",
      "Epoch: 13619 mean train loss:  1.77444599e-05, mean val. loss:  2.65581560e+00\n",
      "Epoch: 13620 mean train loss:  1.79768540e-05, mean val. loss:  2.65592599e+00\n",
      "Epoch: 13621 mean train loss:  1.75086025e-05, mean val. loss:  2.65602183e+00\n",
      "Epoch: 13622 mean train loss:  1.77967304e-05, mean val. loss:  2.65610766e+00\n",
      "Epoch: 13623 mean train loss:  1.75362802e-05, mean val. loss:  2.65621328e+00\n",
      "Epoch: 13624 mean train loss:  1.76089525e-05, mean val. loss:  2.65633607e+00\n",
      "Epoch: 13625 mean train loss:  1.75830501e-05, mean val. loss:  2.65647054e+00\n",
      "Epoch: 13626 mean train loss:  1.80377101e-05, mean val. loss:  2.65660501e+00\n",
      "Epoch: 13627 mean train loss:  1.74755405e-05, mean val. loss:  2.65674734e+00\n",
      "Epoch: 13628 mean train loss:  1.77418115e-05, mean val. loss:  2.65686965e+00\n",
      "Epoch: 13629 mean train loss:  1.74980087e-05, mean val. loss:  2.65699410e+00\n",
      "Epoch: 13630 mean train loss:  1.78125920e-05, mean val. loss:  2.65713644e+00\n",
      "Epoch: 13631 mean train loss:  1.75542373e-05, mean val. loss:  2.65729618e+00\n",
      "Epoch: 13632 mean train loss:  1.75730383e-05, mean val. loss:  2.65743899e+00\n",
      "Epoch: 13633 mean train loss:  1.72652071e-05, mean val. loss:  2.65761709e+00\n",
      "Epoch: 13634 mean train loss:  1.79997296e-05, mean val. loss:  2.65782094e+00\n",
      "Epoch: 13635 mean train loss:  1.78552291e-05, mean val. loss:  2.65801501e+00\n",
      "Epoch: 13636 mean train loss:  1.76581671e-05, mean val. loss:  2.65821886e+00\n",
      "Epoch: 13637 mean train loss:  1.75718451e-05, mean val. loss:  2.65841317e+00\n",
      "Epoch: 13638 mean train loss:  1.75312161e-05, mean val. loss:  2.65859199e+00\n",
      "Epoch: 13639 mean train loss:  1.75408786e-05, mean val. loss:  2.65876794e+00\n",
      "Epoch: 13640 mean train loss:  1.75840105e-05, mean val. loss:  2.65895963e+00\n",
      "Epoch: 13641 mean train loss:  1.75076129e-05, mean val. loss:  2.65917444e+00\n",
      "Epoch: 13642 mean train loss:  1.75078167e-05, mean val. loss:  2.65941882e+00\n",
      "Epoch: 13643 mean train loss:  1.78544724e-05, mean val. loss:  2.65966725e+00\n",
      "Epoch: 13644 mean train loss:  1.77281618e-05, mean val. loss:  2.65993261e+00\n",
      "Epoch: 13645 mean train loss:  1.72480650e-05, mean val. loss:  2.66019511e+00\n",
      "Epoch: 13646 mean train loss:  1.76392496e-05, mean val. loss:  2.66047788e+00\n",
      "Epoch: 13647 mean train loss:  1.78543269e-05, mean val. loss:  2.66074991e+00\n",
      "Epoch: 13648 mean train loss:  1.79651834e-05, mean val. loss:  2.66100383e+00\n",
      "Epoch: 13649 mean train loss:  1.74669258e-05, mean val. loss:  2.66125202e+00\n",
      "Epoch: 13650 mean train loss:  1.82350050e-05, mean val. loss:  2.66147184e+00\n",
      "Epoch: 13651 mean train loss:  1.76616595e-05, mean val. loss:  2.66168094e+00\n",
      "Epoch: 13652 mean train loss:  1.74228626e-05, mean val. loss:  2.66189218e+00\n",
      "Epoch: 13653 mean train loss:  1.76880858e-05, mean val. loss:  2.66207457e+00\n",
      "Epoch: 13654 mean train loss:  1.79359340e-05, mean val. loss:  2.66220808e+00\n",
      "Epoch: 13655 mean train loss:  1.75902387e-05, mean val. loss:  2.66232753e+00\n",
      "Epoch: 13656 mean train loss:  1.75901223e-05, mean val. loss:  2.66245008e+00\n",
      "Epoch: 13657 mean train loss:  1.77932379e-05, mean val. loss:  2.66254497e+00\n",
      "Epoch: 13658 mean train loss:  1.75498717e-05, mean val. loss:  2.66265130e+00\n",
      "Epoch: 13659 mean train loss:  1.76256872e-05, mean val. loss:  2.66275525e+00\n",
      "Epoch: 13660 mean train loss:  1.79535127e-05, mean val. loss:  2.66285014e+00\n",
      "Epoch: 13661 mean train loss:  1.73860171e-05, mean val. loss:  2.66298079e+00\n",
      "Epoch: 13662 mean train loss:  1.77529873e-05, mean val. loss:  2.66311288e+00\n",
      "Epoch: 13663 mean train loss:  1.78421615e-05, mean val. loss:  2.66329002e+00\n",
      "Epoch: 13664 mean train loss:  1.74955348e-05, mean val. loss:  2.66350460e+00\n",
      "Epoch: 13665 mean train loss:  1.76171015e-05, mean val. loss:  2.66377401e+00\n",
      "Epoch: 13666 mean train loss:  1.78262999e-05, mean val. loss:  2.66409969e+00\n",
      "Epoch: 13667 mean train loss:  1.76889007e-05, mean val. loss:  2.66446376e+00\n",
      "Epoch: 13668 mean train loss:  1.75238238e-05, mean val. loss:  2.66484809e+00\n",
      "Epoch: 13669 mean train loss:  1.74893648e-05, mean val. loss:  2.66525960e+00\n",
      "Epoch: 13670 mean train loss:  1.75336609e-05, mean val. loss:  2.66570187e+00\n",
      "Epoch: 13671 mean train loss:  1.76672183e-05, mean val. loss:  2.66617727e+00\n",
      "Epoch: 13672 mean train loss:  1.77495240e-05, mean val. loss:  2.66666293e+00\n",
      "Epoch: 13673 mean train loss:  1.78217306e-05, mean val. loss:  2.66712213e+00\n",
      "Epoch: 13674 mean train loss:  1.77648035e-05, mean val. loss:  2.66755486e+00\n",
      "Epoch: 13675 mean train loss:  1.72905275e-05, mean val. loss:  2.66800594e+00\n",
      "Epoch: 13676 mean train loss:  1.79644849e-05, mean val. loss:  2.66836691e+00\n",
      "Epoch: 13677 mean train loss:  1.79485942e-05, mean val. loss:  2.66866827e+00\n",
      "Epoch: 13678 mean train loss:  1.75472524e-05, mean val. loss:  2.66893864e+00\n",
      "Epoch: 13679 mean train loss:  1.76797912e-05, mean val. loss:  2.66917229e+00\n",
      "Epoch: 13680 mean train loss:  1.76773465e-05, mean val. loss:  2.66937399e+00\n",
      "Epoch: 13681 mean train loss:  1.77006004e-05, mean val. loss:  2.66955233e+00\n",
      "Epoch: 13682 mean train loss:  1.75562163e-05, mean val. loss:  2.66968584e+00\n",
      "Epoch: 13683 mean train loss:  1.74368615e-05, mean val. loss:  2.66981792e+00\n",
      "Epoch: 13684 mean train loss:  1.72884320e-05, mean val. loss:  2.66997361e+00\n",
      "Epoch: 13685 mean train loss:  1.77363690e-05, mean val. loss:  2.67012000e+00\n",
      "Epoch: 13686 mean train loss:  1.75097957e-05, mean val. loss:  2.67028117e+00\n",
      "Epoch: 13687 mean train loss:  1.79926865e-05, mean val. loss:  2.67043138e+00\n",
      "Epoch: 13688 mean train loss:  1.77104375e-05, mean val. loss:  2.67058611e+00\n",
      "Epoch: 13689 mean train loss:  1.77966722e-05, mean val. loss:  2.67075586e+00\n",
      "Epoch: 13690 mean train loss:  1.75651221e-05, mean val. loss:  2.67093086e+00\n",
      "Epoch: 13691 mean train loss:  1.78338960e-05, mean val. loss:  2.67109513e+00\n",
      "Epoch: 13692 mean train loss:  1.73489971e-05, mean val. loss:  2.67126656e+00\n",
      "Epoch: 13693 mean train loss:  1.74793531e-05, mean val. loss:  2.67146134e+00\n",
      "Epoch: 13694 mean train loss:  1.79945491e-05, mean val. loss:  2.67161751e+00\n",
      "Epoch: 13695 mean train loss:  1.76523754e-05, mean val. loss:  2.67176604e+00\n",
      "Epoch: 13696 mean train loss:  1.74760935e-05, mean val. loss:  2.67192769e+00\n",
      "Epoch: 13697 mean train loss:  1.76703034e-05, mean val. loss:  2.67208505e+00\n",
      "Epoch: 13698 mean train loss:  1.79198978e-05, mean val. loss:  2.67220592e+00\n",
      "Epoch: 13699 mean train loss:  1.81156502e-05, mean val. loss:  2.67230320e+00\n",
      "Epoch: 13700 mean train loss:  1.75532186e-05, mean val. loss:  2.67237258e+00\n",
      "Epoch: 13701 mean train loss:  1.78311020e-05, mean val. loss:  2.67241573e+00\n",
      "Epoch: 13702 mean train loss:  1.77742331e-05, mean val. loss:  2.67241549e+00\n",
      "Epoch: 13703 mean train loss:  1.76861649e-05, mean val. loss:  2.67241907e+00\n",
      "Epoch: 13704 mean train loss:  1.75226014e-05, mean val. loss:  2.67240095e+00\n",
      "Epoch: 13705 mean train loss:  1.77036854e-05, mean val. loss:  2.67236853e+00\n",
      "Epoch: 13706 mean train loss:  1.79699564e-05, mean val. loss:  2.67233062e+00\n",
      "Epoch: 13707 mean train loss:  1.72603759e-05, mean val. loss:  2.67232156e+00\n",
      "Epoch: 13708 mean train loss:  1.75251043e-05, mean val. loss:  2.67232323e+00\n",
      "Epoch: 13709 mean train loss:  1.74370361e-05, mean val. loss:  2.67237449e+00\n",
      "Epoch: 13710 mean train loss:  1.78519695e-05, mean val. loss:  2.67244792e+00\n",
      "Epoch: 13711 mean train loss:  1.73648004e-05, mean val. loss:  2.67259717e+00\n",
      "Epoch: 13712 mean train loss:  1.77444308e-05, mean val. loss:  2.67278051e+00\n",
      "Epoch: 13713 mean train loss:  1.73993758e-05, mean val. loss:  2.67304420e+00\n",
      "Epoch: 13714 mean train loss:  1.76167523e-05, mean val. loss:  2.67335486e+00\n",
      "Epoch: 13715 mean train loss:  1.75834575e-05, mean val. loss:  2.67370152e+00\n",
      "Epoch: 13716 mean train loss:  1.78244663e-05, mean val. loss:  2.67408037e+00\n",
      "Epoch: 13717 mean train loss:  1.78340997e-05, mean val. loss:  2.67447972e+00\n",
      "Epoch: 13718 mean train loss:  1.79419585e-05, mean val. loss:  2.67486501e+00\n",
      "Epoch: 13719 mean train loss:  1.78313931e-05, mean val. loss:  2.67520356e+00\n",
      "Epoch: 13720 mean train loss:  1.77113398e-05, mean val. loss:  2.67556405e+00\n",
      "Epoch: 13721 mean train loss:  1.77678303e-05, mean val. loss:  2.67588854e+00\n",
      "Epoch: 13722 mean train loss:  1.80549396e-05, mean val. loss:  2.67616010e+00\n",
      "Epoch: 13723 mean train loss:  1.78233313e-05, mean val. loss:  2.67643619e+00\n",
      "Epoch: 13724 mean train loss:  1.77042675e-05, mean val. loss:  2.67668343e+00\n",
      "Epoch: 13725 mean train loss:  1.75758032e-05, mean val. loss:  2.67691159e+00\n",
      "Epoch: 13726 mean train loss:  1.75804016e-05, mean val. loss:  2.67709780e+00\n",
      "Epoch: 13727 mean train loss:  1.77012698e-05, mean val. loss:  2.67726469e+00\n",
      "Epoch: 13728 mean train loss:  1.77701295e-05, mean val. loss:  2.67740464e+00\n",
      "Epoch: 13729 mean train loss:  1.75531022e-05, mean val. loss:  2.67753434e+00\n",
      "Epoch: 13730 mean train loss:  1.75925670e-05, mean val. loss:  2.67767334e+00\n",
      "Epoch: 13731 mean train loss:  1.75560999e-05, mean val. loss:  2.67784262e+00\n",
      "Epoch: 13732 mean train loss:  1.75578170e-05, mean val. loss:  2.67804503e+00\n",
      "Epoch: 13733 mean train loss:  1.76604080e-05, mean val. loss:  2.67825174e+00\n",
      "Epoch: 13734 mean train loss:  1.75837777e-05, mean val. loss:  2.67850304e+00\n",
      "Epoch: 13735 mean train loss:  1.75857567e-05, mean val. loss:  2.67880750e+00\n",
      "Epoch: 13736 mean train loss:  1.75827590e-05, mean val. loss:  2.67913890e+00\n",
      "Epoch: 13737 mean train loss:  1.73545559e-05, mean val. loss:  2.67950988e+00\n",
      "Epoch: 13738 mean train loss:  1.75730383e-05, mean val. loss:  2.67988896e+00\n",
      "Epoch: 13739 mean train loss:  1.75724854e-05, mean val. loss:  2.68029523e+00\n",
      "Epoch: 13740 mean train loss:  1.77577022e-05, mean val. loss:  2.68071270e+00\n",
      "Epoch: 13741 mean train loss:  1.78971677e-05, mean val. loss:  2.68111563e+00\n",
      "Epoch: 13742 mean train loss:  1.73345325e-05, mean val. loss:  2.68152094e+00\n",
      "Epoch: 13743 mean train loss:  1.73020526e-05, mean val. loss:  2.68194652e+00\n",
      "Epoch: 13744 mean train loss:  1.77224283e-05, mean val. loss:  2.68231463e+00\n",
      "Epoch: 13745 mean train loss:  1.74122979e-05, mean val. loss:  2.68267441e+00\n",
      "Epoch: 13746 mean train loss:  1.75313326e-05, mean val. loss:  2.68305326e+00\n",
      "Epoch: 13747 mean train loss:  1.75397145e-05, mean val. loss:  2.68342996e+00\n",
      "Epoch: 13748 mean train loss:  1.75700407e-05, mean val. loss:  2.68375802e+00\n",
      "Epoch: 13749 mean train loss:  1.74802844e-05, mean val. loss:  2.68407297e+00\n",
      "Epoch: 13750 mean train loss:  1.74053421e-05, mean val. loss:  2.68438148e+00\n",
      "Epoch: 13751 mean train loss:  1.73720473e-05, mean val. loss:  2.68469691e+00\n",
      "Epoch: 13752 mean train loss:  1.72563014e-05, mean val. loss:  2.68500519e+00\n",
      "Epoch: 13753 mean train loss:  1.74826710e-05, mean val. loss:  2.68529081e+00\n",
      "Epoch: 13754 mean train loss:  1.75554596e-05, mean val. loss:  2.68558812e+00\n",
      "Epoch: 13755 mean train loss:  1.73143344e-05, mean val. loss:  2.68590045e+00\n",
      "Epoch: 13756 mean train loss:  1.74960587e-05, mean val. loss:  2.68618393e+00\n",
      "Epoch: 13757 mean train loss:  1.73054868e-05, mean val. loss:  2.68643904e+00\n",
      "Epoch: 13758 mean train loss:  1.76934118e-05, mean val. loss:  2.68665814e+00\n",
      "Epoch: 13759 mean train loss:  1.77097390e-05, mean val. loss:  2.68683314e+00\n",
      "Epoch: 13760 mean train loss:  1.75186724e-05, mean val. loss:  2.68701005e+00\n",
      "Epoch: 13761 mean train loss:  1.77032780e-05, mean val. loss:  2.68716884e+00\n",
      "Epoch: 13762 mean train loss:  1.76992908e-05, mean val. loss:  2.68732262e+00\n",
      "Epoch: 13763 mean train loss:  1.73505396e-05, mean val. loss:  2.68748546e+00\n",
      "Epoch: 13764 mean train loss:  1.77727197e-05, mean val. loss:  2.68765211e+00\n",
      "Epoch: 13765 mean train loss:  1.75298774e-05, mean val. loss:  2.68781471e+00\n",
      "Epoch: 13766 mean train loss:  1.75427995e-05, mean val. loss:  2.68801093e+00\n",
      "Epoch: 13767 mean train loss:  1.80066854e-05, mean val. loss:  2.68820429e+00\n",
      "Epoch: 13768 mean train loss:  1.76342728e-05, mean val. loss:  2.68841290e+00\n",
      "Epoch: 13769 mean train loss:  1.78045884e-05, mean val. loss:  2.68858790e+00\n",
      "Epoch: 13770 mean train loss:  1.75448949e-05, mean val. loss:  2.68881178e+00\n",
      "Epoch: 13771 mean train loss:  1.79289491e-05, mean val. loss:  2.68901420e+00\n",
      "Epoch: 13772 mean train loss:  1.75035384e-05, mean val. loss:  2.68926287e+00\n",
      "Epoch: 13773 mean train loss:  1.77130278e-05, mean val. loss:  2.68949914e+00\n",
      "Epoch: 13774 mean train loss:  1.73207372e-05, mean val. loss:  2.68972945e+00\n",
      "Epoch: 13775 mean train loss:  1.78694027e-05, mean val. loss:  2.68990469e+00\n",
      "Epoch: 13776 mean train loss:  1.76343601e-05, mean val. loss:  2.69004989e+00\n",
      "Epoch: 13777 mean train loss:  1.75804016e-05, mean val. loss:  2.69019628e+00\n",
      "Epoch: 13778 mean train loss:  1.73579028e-05, mean val. loss:  2.69033194e+00\n",
      "Epoch: 13779 mean train loss:  1.77070324e-05, mean val. loss:  2.69043016e+00\n",
      "Epoch: 13780 mean train loss:  1.74940214e-05, mean val. loss:  2.69050050e+00\n",
      "Epoch: 13781 mean train loss:  1.76511530e-05, mean val. loss:  2.69055533e+00\n",
      "Epoch: 13782 mean train loss:  1.77869224e-05, mean val. loss:  2.69057417e+00\n",
      "Epoch: 13783 mean train loss:  1.78024347e-05, mean val. loss:  2.69058228e+00\n",
      "Epoch: 13784 mean train loss:  1.76352914e-05, mean val. loss:  2.69061041e+00\n",
      "Epoch: 13785 mean train loss:  1.76320900e-05, mean val. loss:  2.69064379e+00\n",
      "Epoch: 13786 mean train loss:  1.78458868e-05, mean val. loss:  2.69068742e+00\n",
      "Epoch: 13787 mean train loss:  1.77947513e-05, mean val. loss:  2.69078517e+00\n",
      "Epoch: 13788 mean train loss:  1.73657609e-05, mean val. loss:  2.69093299e+00\n",
      "Epoch: 13789 mean train loss:  1.75469031e-05, mean val. loss:  2.69115448e+00\n",
      "Epoch: 13790 mean train loss:  1.73276931e-05, mean val. loss:  2.69145536e+00\n",
      "Epoch: 13791 mean train loss:  1.77059555e-05, mean val. loss:  2.69175053e+00\n",
      "Epoch: 13792 mean train loss:  1.76542962e-05, mean val. loss:  2.69211388e+00\n",
      "Epoch: 13793 mean train loss:  1.75310415e-05, mean val. loss:  2.69253373e+00\n",
      "Epoch: 13794 mean train loss:  1.76428875e-05, mean val. loss:  2.69295263e+00\n",
      "Epoch: 13795 mean train loss:  1.74186134e-05, mean val. loss:  2.69336271e+00\n",
      "Epoch: 13796 mean train loss:  1.76316826e-05, mean val. loss:  2.69380283e+00\n",
      "Epoch: 13797 mean train loss:  1.78386399e-05, mean val. loss:  2.69424152e+00\n",
      "Epoch: 13798 mean train loss:  1.74568268e-05, mean val. loss:  2.69468427e+00\n",
      "Epoch: 13799 mean train loss:  1.74794404e-05, mean val. loss:  2.69511843e+00\n",
      "Epoch: 13800 mean train loss:  1.74847955e-05, mean val. loss:  2.69551563e+00\n",
      "Epoch: 13801 mean train loss:  1.74215529e-05, mean val. loss:  2.69590473e+00\n",
      "Epoch: 13802 mean train loss:  1.75402674e-05, mean val. loss:  2.69627190e+00\n",
      "Epoch: 13803 mean train loss:  1.74159941e-05, mean val. loss:  2.69661069e+00\n",
      "Epoch: 13804 mean train loss:  1.75190798e-05, mean val. loss:  2.69693995e+00\n",
      "Epoch: 13805 mean train loss:  1.72219588e-05, mean val. loss:  2.69725537e+00\n",
      "Epoch: 13806 mean train loss:  1.80259813e-05, mean val. loss:  2.69750762e+00\n",
      "Epoch: 13807 mean train loss:  1.73513254e-05, mean val. loss:  2.69774604e+00\n",
      "Epoch: 13808 mean train loss:  1.79607305e-05, mean val. loss:  2.69792461e+00\n",
      "Epoch: 13809 mean train loss:  1.75471068e-05, mean val. loss:  2.69808078e+00\n",
      "Epoch: 13810 mean train loss:  1.77767070e-05, mean val. loss:  2.69824219e+00\n",
      "Epoch: 13811 mean train loss:  1.74375018e-05, mean val. loss:  2.69843674e+00\n",
      "Epoch: 13812 mean train loss:  1.74794695e-05, mean val. loss:  2.69861817e+00\n",
      "Epoch: 13813 mean train loss:  1.77859329e-05, mean val. loss:  2.69875717e+00\n",
      "Epoch: 13814 mean train loss:  1.76836620e-05, mean val. loss:  2.69887114e+00\n",
      "Epoch: 13815 mean train loss:  1.73828157e-05, mean val. loss:  2.69900227e+00\n",
      "Epoch: 13816 mean train loss:  1.75629684e-05, mean val. loss:  2.69912696e+00\n",
      "Epoch: 13817 mean train loss:  1.75489695e-05, mean val. loss:  2.69928312e+00\n",
      "Epoch: 13818 mean train loss:  1.73559820e-05, mean val. loss:  2.69943476e+00\n",
      "Epoch: 13819 mean train loss:  1.74921588e-05, mean val. loss:  2.69958878e+00\n",
      "Epoch: 13820 mean train loss:  1.77352631e-05, mean val. loss:  2.69972968e+00\n",
      "Epoch: 13821 mean train loss:  1.73309236e-05, mean val. loss:  2.69990849e+00\n",
      "Epoch: 13822 mean train loss:  1.78411137e-05, mean val. loss:  2.70010710e+00\n",
      "Epoch: 13823 mean train loss:  1.74483575e-05, mean val. loss:  2.70031023e+00\n",
      "Epoch: 13824 mean train loss:  1.74678280e-05, mean val. loss:  2.70054507e+00\n",
      "Epoch: 13825 mean train loss:  1.72123837e-05, mean val. loss:  2.70081758e+00\n",
      "Epoch: 13826 mean train loss:  1.77923066e-05, mean val. loss:  2.70109296e+00\n",
      "Epoch: 13827 mean train loss:  1.74982706e-05, mean val. loss:  2.70137787e+00\n",
      "Epoch: 13828 mean train loss:  1.73044973e-05, mean val. loss:  2.70169258e+00\n",
      "Epoch: 13829 mean train loss:  1.74426823e-05, mean val. loss:  2.70201945e+00\n",
      "Epoch: 13830 mean train loss:  1.78062182e-05, mean val. loss:  2.70232248e+00\n",
      "Epoch: 13831 mean train loss:  1.75763562e-05, mean val. loss:  2.70260358e+00\n",
      "Epoch: 13832 mean train loss:  1.76104659e-05, mean val. loss:  2.70287657e+00\n",
      "Epoch: 13833 mean train loss:  1.73993758e-05, mean val. loss:  2.70316076e+00\n",
      "Epoch: 13834 mean train loss:  1.74744637e-05, mean val. loss:  2.70342517e+00\n",
      "Epoch: 13835 mean train loss:  1.73920125e-05, mean val. loss:  2.70365381e+00\n",
      "Epoch: 13836 mean train loss:  1.73903245e-05, mean val. loss:  2.70387888e+00\n",
      "Epoch: 13837 mean train loss:  1.77227485e-05, mean val. loss:  2.70409155e+00\n",
      "Epoch: 13838 mean train loss:  1.71614229e-05, mean val. loss:  2.70431232e+00\n",
      "Epoch: 13839 mean train loss:  1.77877373e-05, mean val. loss:  2.70450759e+00\n",
      "Epoch: 13840 mean train loss:  1.74930610e-05, mean val. loss:  2.70473552e+00\n",
      "Epoch: 13841 mean train loss:  1.76602043e-05, mean val. loss:  2.70494318e+00\n",
      "Epoch: 13842 mean train loss:  1.76328176e-05, mean val. loss:  2.70515299e+00\n",
      "Epoch: 13843 mean train loss:  1.76639296e-05, mean val. loss:  2.70535398e+00\n",
      "Epoch: 13844 mean train loss:  1.75392488e-05, mean val. loss:  2.70552921e+00\n",
      "Epoch: 13845 mean train loss:  1.76358153e-05, mean val. loss:  2.70567632e+00\n",
      "Epoch: 13846 mean train loss:  1.77135225e-05, mean val. loss:  2.70578837e+00\n",
      "Epoch: 13847 mean train loss:  1.74263259e-05, mean val. loss:  2.70587754e+00\n",
      "Epoch: 13848 mean train loss:  1.75660825e-05, mean val. loss:  2.70597506e+00\n",
      "Epoch: 13849 mean train loss:  1.72507134e-05, mean val. loss:  2.70609188e+00\n",
      "Epoch: 13850 mean train loss:  1.75593887e-05, mean val. loss:  2.70618534e+00\n",
      "Epoch: 13851 mean train loss:  1.74616580e-05, mean val. loss:  2.70628166e+00\n",
      "Epoch: 13852 mean train loss:  1.74436136e-05, mean val. loss:  2.70638967e+00\n",
      "Epoch: 13853 mean train loss:  1.76179165e-05, mean val. loss:  2.70651269e+00\n",
      "Epoch: 13854 mean train loss:  1.76220492e-05, mean val. loss:  2.70665956e+00\n",
      "Epoch: 13855 mean train loss:  1.76290923e-05, mean val. loss:  2.70683980e+00\n",
      "Epoch: 13856 mean train loss:  1.76335161e-05, mean val. loss:  2.70702124e+00\n",
      "Epoch: 13857 mean train loss:  1.74578745e-05, mean val. loss:  2.70724773e+00\n",
      "Epoch: 13858 mean train loss:  1.76139874e-05, mean val. loss:  2.70746398e+00\n",
      "Epoch: 13859 mean train loss:  1.75322348e-05, mean val. loss:  2.70771456e+00\n",
      "Epoch: 13860 mean train loss:  1.74726010e-05, mean val. loss:  2.70797873e+00\n",
      "Epoch: 13861 mean train loss:  1.73714070e-05, mean val. loss:  2.70824504e+00\n",
      "Epoch: 13862 mean train loss:  1.75035966e-05, mean val. loss:  2.70850873e+00\n",
      "Epoch: 13863 mean train loss:  1.73001899e-05, mean val. loss:  2.70878530e+00\n",
      "Epoch: 13864 mean train loss:  1.74724846e-05, mean val. loss:  2.70903659e+00\n",
      "Epoch: 13865 mean train loss:  1.75008317e-05, mean val. loss:  2.70929384e+00\n",
      "Epoch: 13866 mean train loss:  1.76578178e-05, mean val. loss:  2.70954704e+00\n",
      "Epoch: 13867 mean train loss:  1.76554313e-05, mean val. loss:  2.70976114e+00\n",
      "Epoch: 13868 mean train loss:  1.73574081e-05, mean val. loss:  2.70997691e+00\n",
      "Epoch: 13869 mean train loss:  1.78408227e-05, mean val. loss:  2.71014500e+00\n",
      "Epoch: 13870 mean train loss:  1.75629975e-05, mean val. loss:  2.71029067e+00\n",
      "Epoch: 13871 mean train loss:  1.75237656e-05, mean val. loss:  2.71044254e+00\n",
      "Epoch: 13872 mean train loss:  1.76447502e-05, mean val. loss:  2.71058440e+00\n",
      "Epoch: 13873 mean train loss:  1.78367191e-05, mean val. loss:  2.71068025e+00\n",
      "Epoch: 13874 mean train loss:  1.74280722e-05, mean val. loss:  2.71075749e+00\n",
      "Epoch: 13875 mean train loss:  1.75386376e-05, mean val. loss:  2.71084809e+00\n",
      "Epoch: 13876 mean train loss:  1.73461158e-05, mean val. loss:  2.71092701e+00\n",
      "Epoch: 13877 mean train loss:  1.76033063e-05, mean val. loss:  2.71100664e+00\n",
      "Epoch: 13878 mean train loss:  1.73067674e-05, mean val. loss:  2.71112561e+00\n",
      "Epoch: 13879 mean train loss:  1.72529253e-05, mean val. loss:  2.71126938e+00\n",
      "Epoch: 13880 mean train loss:  1.75064197e-05, mean val. loss:  2.71145082e+00\n",
      "Epoch: 13881 mean train loss:  1.75008608e-05, mean val. loss:  2.71166468e+00\n",
      "Epoch: 13882 mean train loss:  1.73364533e-05, mean val. loss:  2.71189475e+00\n",
      "Epoch: 13883 mean train loss:  1.79488270e-05, mean val. loss:  2.71210527e+00\n",
      "Epoch: 13884 mean train loss:  1.74293818e-05, mean val. loss:  2.71233726e+00\n",
      "Epoch: 13885 mean train loss:  1.73794688e-05, mean val. loss:  2.71259308e+00\n",
      "Epoch: 13886 mean train loss:  1.71904103e-05, mean val. loss:  2.71290231e+00\n",
      "Epoch: 13887 mean train loss:  1.73387234e-05, mean val. loss:  2.71323991e+00\n",
      "Epoch: 13888 mean train loss:  1.73453300e-05, mean val. loss:  2.71359706e+00\n",
      "Epoch: 13889 mean train loss:  1.73979206e-05, mean val. loss:  2.71394777e+00\n",
      "Epoch: 13890 mean train loss:  1.77064794e-05, mean val. loss:  2.71428108e+00\n",
      "Epoch: 13891 mean train loss:  1.72679720e-05, mean val. loss:  2.71464348e+00\n",
      "Epoch: 13892 mean train loss:  1.73829030e-05, mean val. loss:  2.71497560e+00\n",
      "Epoch: 13893 mean train loss:  1.75843306e-05, mean val. loss:  2.71527505e+00\n",
      "Epoch: 13894 mean train loss:  1.76836329e-05, mean val. loss:  2.71554780e+00\n",
      "Epoch: 13895 mean train loss:  1.71620341e-05, mean val. loss:  2.71581888e+00\n",
      "Epoch: 13896 mean train loss:  1.76283938e-05, mean val. loss:  2.71608829e+00\n",
      "Epoch: 13897 mean train loss:  1.76717003e-05, mean val. loss:  2.71635318e+00\n",
      "Epoch: 13898 mean train loss:  1.79726630e-05, mean val. loss:  2.71657276e+00\n",
      "Epoch: 13899 mean train loss:  1.75609312e-05, mean val. loss:  2.71677995e+00\n",
      "Epoch: 13900 mean train loss:  1.72993168e-05, mean val. loss:  2.71700096e+00\n",
      "Epoch: 13901 mean train loss:  1.74435845e-05, mean val. loss:  2.71716619e+00\n",
      "Epoch: 13902 mean train loss:  1.74349407e-05, mean val. loss:  2.71730280e+00\n",
      "Epoch: 13903 mean train loss:  1.71844440e-05, mean val. loss:  2.71747446e+00\n",
      "Epoch: 13904 mean train loss:  1.77195761e-05, mean val. loss:  2.71762013e+00\n",
      "Epoch: 13905 mean train loss:  1.73973385e-05, mean val. loss:  2.71780467e+00\n",
      "Epoch: 13906 mean train loss:  1.73924200e-05, mean val. loss:  2.71794820e+00\n",
      "Epoch: 13907 mean train loss:  1.75899477e-05, mean val. loss:  2.71808362e+00\n",
      "Epoch: 13908 mean train loss:  1.74467568e-05, mean val. loss:  2.71820021e+00\n",
      "Epoch: 13909 mean train loss:  1.73921580e-05, mean val. loss:  2.71828437e+00\n",
      "Epoch: 13910 mean train loss:  1.76719332e-05, mean val. loss:  2.71835685e+00\n",
      "Epoch: 13911 mean train loss:  1.75273744e-05, mean val. loss:  2.71846080e+00\n",
      "Epoch: 13912 mean train loss:  1.76022586e-05, mean val. loss:  2.71857572e+00\n",
      "Epoch: 13913 mean train loss:  1.77866896e-05, mean val. loss:  2.71869063e+00\n",
      "Epoch: 13914 mean train loss:  1.75343594e-05, mean val. loss:  2.71880627e+00\n",
      "Epoch: 13915 mean train loss:  1.74923334e-05, mean val. loss:  2.71892095e+00\n",
      "Epoch: 13916 mean train loss:  1.75095047e-05, mean val. loss:  2.71907163e+00\n",
      "Epoch: 13917 mean train loss:  1.74670713e-05, mean val. loss:  2.71920896e+00\n",
      "Epoch: 13918 mean train loss:  1.72760629e-05, mean val. loss:  2.71934390e+00\n",
      "Epoch: 13919 mean train loss:  1.72792061e-05, mean val. loss:  2.71952510e+00\n",
      "Epoch: 13920 mean train loss:  1.74671295e-05, mean val. loss:  2.71970820e+00\n",
      "Epoch: 13921 mean train loss:  1.71950378e-05, mean val. loss:  2.71992278e+00\n",
      "Epoch: 13922 mean train loss:  1.72602013e-05, mean val. loss:  2.72016668e+00\n",
      "Epoch: 13923 mean train loss:  1.73966691e-05, mean val. loss:  2.72043157e+00\n",
      "Epoch: 13924 mean train loss:  1.76613685e-05, mean val. loss:  2.72070384e+00\n",
      "Epoch: 13925 mean train loss:  1.73178269e-05, mean val. loss:  2.72102594e+00\n",
      "Epoch: 13926 mean train loss:  1.78626797e-05, mean val. loss:  2.72133350e+00\n",
      "Epoch: 13927 mean train loss:  1.75419846e-05, mean val. loss:  2.72167206e+00\n",
      "Epoch: 13928 mean train loss:  1.74819434e-05, mean val. loss:  2.72202134e+00\n",
      "Epoch: 13929 mean train loss:  1.72645086e-05, mean val. loss:  2.72240424e+00\n",
      "Epoch: 13930 mean train loss:  1.72648579e-05, mean val. loss:  2.72279811e+00\n",
      "Epoch: 13931 mean train loss:  1.72860746e-05, mean val. loss:  2.72317910e+00\n",
      "Epoch: 13932 mean train loss:  1.76223693e-05, mean val. loss:  2.72354746e+00\n",
      "Epoch: 13933 mean train loss:  1.76557805e-05, mean val. loss:  2.72389674e+00\n",
      "Epoch: 13934 mean train loss:  1.75808673e-05, mean val. loss:  2.72421646e+00\n",
      "Epoch: 13935 mean train loss:  1.74273737e-05, mean val. loss:  2.72453666e+00\n",
      "Epoch: 13936 mean train loss:  1.75339228e-05, mean val. loss:  2.72485137e+00\n",
      "Epoch: 13937 mean train loss:  1.75943715e-05, mean val. loss:  2.72513652e+00\n",
      "Epoch: 13938 mean train loss:  1.76939648e-05, mean val. loss:  2.72538328e+00\n",
      "Epoch: 13939 mean train loss:  1.76460599e-05, mean val. loss:  2.72559667e+00\n",
      "Epoch: 13940 mean train loss:  1.77178881e-05, mean val. loss:  2.72575855e+00\n",
      "Epoch: 13941 mean train loss:  1.72830769e-05, mean val. loss:  2.72593021e+00\n",
      "Epoch: 13942 mean train loss:  1.76184403e-05, mean val. loss:  2.72607875e+00\n",
      "Epoch: 13943 mean train loss:  1.78072078e-05, mean val. loss:  2.72615457e+00\n",
      "Epoch: 13944 mean train loss:  1.74196612e-05, mean val. loss:  2.72623706e+00\n",
      "Epoch: 13945 mean train loss:  1.74638699e-05, mean val. loss:  2.72631097e+00\n",
      "Epoch: 13946 mean train loss:  1.73198932e-05, mean val. loss:  2.72640228e+00\n",
      "Epoch: 13947 mean train loss:  1.74609013e-05, mean val. loss:  2.72651410e+00\n",
      "Epoch: 13948 mean train loss:  1.74772285e-05, mean val. loss:  2.72665644e+00\n",
      "Epoch: 13949 mean train loss:  1.75555469e-05, mean val. loss:  2.72678256e+00\n",
      "Epoch: 13950 mean train loss:  1.76299654e-05, mean val. loss:  2.72692871e+00\n",
      "Epoch: 13951 mean train loss:  1.77954789e-05, mean val. loss:  2.72708392e+00\n",
      "Epoch: 13952 mean train loss:  1.70750427e-05, mean val. loss:  2.72732997e+00\n",
      "Epoch: 13953 mean train loss:  1.77281327e-05, mean val. loss:  2.72760344e+00\n",
      "Epoch: 13954 mean train loss:  1.75461173e-05, mean val. loss:  2.72784042e+00\n",
      "Epoch: 13955 mean train loss:  1.74737652e-05, mean val. loss:  2.72808003e+00\n",
      "Epoch: 13956 mean train loss:  1.72651198e-05, mean val. loss:  2.72834206e+00\n",
      "Epoch: 13957 mean train loss:  1.75116875e-05, mean val. loss:  2.72860360e+00\n",
      "Epoch: 13958 mean train loss:  1.74681190e-05, mean val. loss:  2.72886515e+00\n",
      "Epoch: 13959 mean train loss:  1.75838068e-05, mean val. loss:  2.72915483e+00\n",
      "Epoch: 13960 mean train loss:  1.73976587e-05, mean val. loss:  2.72944307e+00\n",
      "Epoch: 13961 mean train loss:  1.75557507e-05, mean val. loss:  2.72971272e+00\n",
      "Epoch: 13962 mean train loss:  1.75403839e-05, mean val. loss:  2.72996163e+00\n",
      "Epoch: 13963 mean train loss:  1.74827292e-05, mean val. loss:  2.73019528e+00\n",
      "Epoch: 13964 mean train loss:  1.74469315e-05, mean val. loss:  2.73040986e+00\n",
      "Epoch: 13965 mean train loss:  1.78024638e-05, mean val. loss:  2.73060513e+00\n",
      "Epoch: 13966 mean train loss:  1.73835433e-05, mean val. loss:  2.73078871e+00\n",
      "Epoch: 13967 mean train loss:  1.74598536e-05, mean val. loss:  2.73100352e+00\n",
      "Epoch: 13968 mean train loss:  1.73117151e-05, mean val. loss:  2.73120522e+00\n",
      "Epoch: 13969 mean train loss:  1.75115420e-05, mean val. loss:  2.73141432e+00\n",
      "Epoch: 13970 mean train loss:  1.75819732e-05, mean val. loss:  2.73158455e+00\n",
      "Epoch: 13971 mean train loss:  1.74779852e-05, mean val. loss:  2.73174214e+00\n",
      "Epoch: 13972 mean train loss:  1.73286244e-05, mean val. loss:  2.73193359e+00\n",
      "Epoch: 13973 mean train loss:  1.74079323e-05, mean val. loss:  2.73215413e+00\n",
      "Epoch: 13974 mean train loss:  1.77680049e-05, mean val. loss:  2.73235679e+00\n",
      "Epoch: 13975 mean train loss:  1.73684966e-05, mean val. loss:  2.73255324e+00\n",
      "Epoch: 13976 mean train loss:  1.72966975e-05, mean val. loss:  2.73276377e+00\n",
      "Epoch: 13977 mean train loss:  1.71468128e-05, mean val. loss:  2.73298335e+00\n",
      "Epoch: 13978 mean train loss:  1.73532753e-05, mean val. loss:  2.73319864e+00\n",
      "Epoch: 13979 mean train loss:  1.75059249e-05, mean val. loss:  2.73342776e+00\n",
      "Epoch: 13980 mean train loss:  1.76602334e-05, mean val. loss:  2.73364949e+00\n",
      "Epoch: 13981 mean train loss:  1.75428868e-05, mean val. loss:  2.73387170e+00\n",
      "Epoch: 13982 mean train loss:  1.74722809e-05, mean val. loss:  2.73406601e+00\n",
      "Epoch: 13983 mean train loss:  1.75810419e-05, mean val. loss:  2.73423338e+00\n",
      "Epoch: 13984 mean train loss:  1.70890125e-05, mean val. loss:  2.73443294e+00\n",
      "Epoch: 13985 mean train loss:  1.69974519e-05, mean val. loss:  2.73466730e+00\n",
      "Epoch: 13986 mean train loss:  1.74954184e-05, mean val. loss:  2.73487496e+00\n",
      "Epoch: 13987 mean train loss:  1.74664892e-05, mean val. loss:  2.73509407e+00\n",
      "Epoch: 13988 mean train loss:  1.74188754e-05, mean val. loss:  2.73529983e+00\n",
      "Epoch: 13989 mean train loss:  1.75124733e-05, mean val. loss:  2.73551035e+00\n",
      "Epoch: 13990 mean train loss:  1.74304587e-05, mean val. loss:  2.73574233e+00\n",
      "Epoch: 13991 mean train loss:  1.71535357e-05, mean val. loss:  2.73602152e+00\n",
      "Epoch: 13992 mean train loss:  1.70897983e-05, mean val. loss:  2.73632050e+00\n",
      "Epoch: 13993 mean train loss:  1.70304556e-05, mean val. loss:  2.73667216e+00\n",
      "Epoch: 13994 mean train loss:  1.74760644e-05, mean val. loss:  2.73708868e+00\n",
      "Epoch: 13995 mean train loss:  1.77321781e-05, mean val. loss:  2.73750329e+00\n",
      "Epoch: 13996 mean train loss:  1.74603774e-05, mean val. loss:  2.73790741e+00\n",
      "Epoch: 13997 mean train loss:  1.71831343e-05, mean val. loss:  2.73834801e+00\n",
      "Epoch: 13998 mean train loss:  1.77660258e-05, mean val. loss:  2.73879552e+00\n",
      "Epoch: 13999 mean train loss:  1.72911678e-05, mean val. loss:  2.73925376e+00\n",
      "Epoch: 14000 mean train loss:  1.77552574e-05, mean val. loss:  2.73969841e+00\n",
      "Epoch: 14001 mean train loss:  1.73808075e-05, mean val. loss:  2.74008489e+00\n",
      "Epoch: 14002 mean train loss:  1.74919551e-05, mean val. loss:  2.74043226e+00\n",
      "Epoch: 14003 mean train loss:  1.76037429e-05, mean val. loss:  2.74076247e+00\n",
      "Epoch: 14004 mean train loss:  1.74945162e-05, mean val. loss:  2.74108052e+00\n",
      "Epoch: 14005 mean train loss:  1.74376182e-05, mean val. loss:  2.74130154e+00\n",
      "Epoch: 14006 mean train loss:  1.72610162e-05, mean val. loss:  2.74150968e+00\n",
      "Epoch: 14007 mean train loss:  1.71131105e-05, mean val. loss:  2.74169707e+00\n",
      "Epoch: 14008 mean train loss:  1.74946908e-05, mean val. loss:  2.74184132e+00\n",
      "Epoch: 14009 mean train loss:  1.73578737e-05, mean val. loss:  2.74194264e+00\n",
      "Epoch: 14010 mean train loss:  1.73845910e-05, mean val. loss:  2.74199200e+00\n",
      "Epoch: 14011 mean train loss:  1.78128248e-05, mean val. loss:  2.74201679e+00\n",
      "Epoch: 14012 mean train loss:  1.73510634e-05, mean val. loss:  2.74207449e+00\n",
      "Epoch: 14013 mean train loss:  1.75110472e-05, mean val. loss:  2.74212074e+00\n",
      "Epoch: 14014 mean train loss:  1.71490246e-05, mean val. loss:  2.74218941e+00\n",
      "Epoch: 14015 mean train loss:  1.72457367e-05, mean val. loss:  2.74227524e+00\n",
      "Epoch: 14016 mean train loss:  1.74498418e-05, mean val. loss:  2.74237967e+00\n",
      "Epoch: 14017 mean train loss:  1.71932043e-05, mean val. loss:  2.74252081e+00\n",
      "Epoch: 14018 mean train loss:  1.74365705e-05, mean val. loss:  2.74270582e+00\n",
      "Epoch: 14019 mean train loss:  1.72745495e-05, mean val. loss:  2.74289298e+00\n",
      "Epoch: 14020 mean train loss:  1.77805778e-05, mean val. loss:  2.74307346e+00\n",
      "Epoch: 14021 mean train loss:  1.76871545e-05, mean val. loss:  2.74328113e+00\n",
      "Epoch: 14022 mean train loss:  1.70929998e-05, mean val. loss:  2.74352074e+00\n",
      "Epoch: 14023 mean train loss:  1.74703309e-05, mean val. loss:  2.74378538e+00\n",
      "Epoch: 14024 mean train loss:  1.72646833e-05, mean val. loss:  2.74406528e+00\n",
      "Epoch: 14025 mean train loss:  1.76365429e-05, mean val. loss:  2.74434304e+00\n",
      "Epoch: 14026 mean train loss:  1.76063331e-05, mean val. loss:  2.74462867e+00\n",
      "Epoch: 14027 mean train loss:  1.75002788e-05, mean val. loss:  2.74490762e+00\n",
      "Epoch: 14028 mean train loss:  1.73328444e-05, mean val. loss:  2.74520779e+00\n",
      "Epoch: 14029 mean train loss:  1.73249282e-05, mean val. loss:  2.74550319e+00\n",
      "Epoch: 14030 mean train loss:  1.73805456e-05, mean val. loss:  2.74579191e+00\n",
      "Epoch: 14031 mean train loss:  1.71300198e-05, mean val. loss:  2.74607968e+00\n",
      "Epoch: 14032 mean train loss:  1.73092994e-05, mean val. loss:  2.74637508e+00\n",
      "Epoch: 14033 mean train loss:  1.72984437e-05, mean val. loss:  2.74666810e+00\n",
      "Epoch: 14034 mean train loss:  1.71862484e-05, mean val. loss:  2.74698496e+00\n",
      "Epoch: 14035 mean train loss:  1.72324362e-05, mean val. loss:  2.74726892e+00\n",
      "Epoch: 14036 mean train loss:  1.74387824e-05, mean val. loss:  2.74751902e+00\n",
      "Epoch: 14037 mean train loss:  1.74967863e-05, mean val. loss:  2.74771976e+00\n",
      "Epoch: 14038 mean train loss:  1.73742301e-05, mean val. loss:  2.74788737e+00\n",
      "Epoch: 14039 mean train loss:  1.72969885e-05, mean val. loss:  2.74807072e+00\n",
      "Epoch: 14040 mean train loss:  1.71459978e-05, mean val. loss:  2.74823904e+00\n",
      "Epoch: 14041 mean train loss:  1.71675347e-05, mean val. loss:  2.74838853e+00\n",
      "Epoch: 14042 mean train loss:  1.73292938e-05, mean val. loss:  2.74853969e+00\n",
      "Epoch: 14043 mean train loss:  1.74781308e-05, mean val. loss:  2.74866748e+00\n",
      "Epoch: 14044 mean train loss:  1.75315363e-05, mean val. loss:  2.74878478e+00\n",
      "Epoch: 14045 mean train loss:  1.72947184e-05, mean val. loss:  2.74886894e+00\n",
      "Epoch: 14046 mean train loss:  1.75482419e-05, mean val. loss:  2.74895263e+00\n",
      "Epoch: 14047 mean train loss:  1.73737353e-05, mean val. loss:  2.74906754e+00\n",
      "Epoch: 14048 mean train loss:  1.73354638e-05, mean val. loss:  2.74918342e+00\n",
      "Epoch: 14049 mean train loss:  1.74659071e-05, mean val. loss:  2.74929738e+00\n",
      "Epoch: 14050 mean train loss:  1.72688742e-05, mean val. loss:  2.74944520e+00\n",
      "Epoch: 14051 mean train loss:  1.74652087e-05, mean val. loss:  2.74962044e+00\n",
      "Epoch: 14052 mean train loss:  1.74180605e-05, mean val. loss:  2.74981880e+00\n",
      "Epoch: 14053 mean train loss:  1.72679720e-05, mean val. loss:  2.75006723e+00\n",
      "Epoch: 14054 mean train loss:  1.71105494e-05, mean val. loss:  2.75035238e+00\n",
      "Epoch: 14055 mean train loss:  1.74328452e-05, mean val. loss:  2.75065088e+00\n",
      "Epoch: 14056 mean train loss:  1.72461150e-05, mean val. loss:  2.75096035e+00\n",
      "Epoch: 14057 mean train loss:  1.75967871e-05, mean val. loss:  2.75122976e+00\n",
      "Epoch: 14058 mean train loss:  1.76146568e-05, mean val. loss:  2.75147223e+00\n",
      "Epoch: 14059 mean train loss:  1.71923894e-05, mean val. loss:  2.75170159e+00\n",
      "Epoch: 14060 mean train loss:  1.76033354e-05, mean val. loss:  2.75190973e+00\n",
      "Epoch: 14061 mean train loss:  1.70084240e-05, mean val. loss:  2.75214386e+00\n",
      "Epoch: 14062 mean train loss:  1.73451263e-05, mean val. loss:  2.75236702e+00\n",
      "Epoch: 14063 mean train loss:  1.72892469e-05, mean val. loss:  2.75258207e+00\n",
      "Epoch: 14064 mean train loss:  1.73008011e-05, mean val. loss:  2.75280952e+00\n",
      "Epoch: 14065 mean train loss:  1.71601714e-05, mean val. loss:  2.75305629e+00\n",
      "Epoch: 14066 mean train loss:  1.74872112e-05, mean val. loss:  2.75327587e+00\n",
      "Epoch: 14067 mean train loss:  1.72076980e-05, mean val. loss:  2.75348997e+00\n",
      "Epoch: 14068 mean train loss:  1.71862775e-05, mean val. loss:  2.75375152e+00\n",
      "Epoch: 14069 mean train loss:  1.74776942e-05, mean val. loss:  2.75403619e+00\n",
      "Epoch: 14070 mean train loss:  1.75379682e-05, mean val. loss:  2.75434041e+00\n",
      "Epoch: 14071 mean train loss:  1.72729779e-05, mean val. loss:  2.75468540e+00\n",
      "Epoch: 14072 mean train loss:  1.71147403e-05, mean val. loss:  2.75505233e+00\n",
      "Epoch: 14073 mean train loss:  1.72914006e-05, mean val. loss:  2.75544357e+00\n",
      "Epoch: 14074 mean train loss:  1.75044406e-05, mean val. loss:  2.75584841e+00\n",
      "Epoch: 14075 mean train loss:  1.72017899e-05, mean val. loss:  2.75627351e+00\n",
      "Epoch: 14076 mean train loss:  1.71877327e-05, mean val. loss:  2.75669718e+00\n",
      "Epoch: 14077 mean train loss:  1.74562447e-05, mean val. loss:  2.75709796e+00\n",
      "Epoch: 14078 mean train loss:  1.69528357e-05, mean val. loss:  2.75757575e+00\n",
      "Epoch: 14079 mean train loss:  1.71958236e-05, mean val. loss:  2.75806522e+00\n",
      "Epoch: 14080 mean train loss:  1.72277505e-05, mean val. loss:  2.75852895e+00\n",
      "Epoch: 14081 mean train loss:  1.74535089e-05, mean val. loss:  2.75896645e+00\n",
      "Epoch: 14082 mean train loss:  1.72200962e-05, mean val. loss:  2.75937009e+00\n",
      "Epoch: 14083 mean train loss:  1.78328482e-05, mean val. loss:  2.75967956e+00\n",
      "Epoch: 14084 mean train loss:  1.75912282e-05, mean val. loss:  2.75994992e+00\n",
      "Epoch: 14085 mean train loss:  1.70881103e-05, mean val. loss:  2.76020384e+00\n",
      "Epoch: 14086 mean train loss:  1.70526910e-05, mean val. loss:  2.76043391e+00\n",
      "Epoch: 14087 mean train loss:  1.73073786e-05, mean val. loss:  2.76065707e+00\n",
      "Epoch: 14088 mean train loss:  1.75125897e-05, mean val. loss:  2.76084661e+00\n",
      "Epoch: 14089 mean train loss:  1.73685839e-05, mean val. loss:  2.76100802e+00\n",
      "Epoch: 14090 mean train loss:  1.76221365e-05, mean val. loss:  2.76114225e+00\n",
      "Epoch: 14091 mean train loss:  1.70019630e-05, mean val. loss:  2.76129031e+00\n",
      "Epoch: 14092 mean train loss:  1.74790039e-05, mean val. loss:  2.76144767e+00\n",
      "Epoch: 14093 mean train loss:  1.78719347e-05, mean val. loss:  2.76158810e+00\n",
      "Epoch: 14094 mean train loss:  1.73700682e-05, mean val. loss:  2.76171923e+00\n",
      "Epoch: 14095 mean train loss:  1.71564170e-05, mean val. loss:  2.76186347e+00\n",
      "Epoch: 14096 mean train loss:  1.72779546e-05, mean val. loss:  2.76201081e+00\n",
      "Epoch: 14097 mean train loss:  1.73897715e-05, mean val. loss:  2.76216698e+00\n",
      "Epoch: 14098 mean train loss:  1.71148858e-05, mean val. loss:  2.76235127e+00\n",
      "Epoch: 14099 mean train loss:  1.73694280e-05, mean val. loss:  2.76252341e+00\n",
      "Epoch: 14100 mean train loss:  1.73063600e-05, mean val. loss:  2.76275039e+00\n",
      "Epoch: 14101 mean train loss:  1.72280124e-05, mean val. loss:  2.76297569e+00\n",
      "Epoch: 14102 mean train loss:  1.71801948e-05, mean val. loss:  2.76321268e+00\n",
      "Epoch: 14103 mean train loss:  1.74988236e-05, mean val. loss:  2.76342392e+00\n",
      "Epoch: 14104 mean train loss:  1.73486187e-05, mean val. loss:  2.76365995e+00\n",
      "Epoch: 14105 mean train loss:  1.74313900e-05, mean val. loss:  2.76388240e+00\n",
      "Epoch: 14106 mean train loss:  1.68810366e-05, mean val. loss:  2.76413512e+00\n",
      "Epoch: 14107 mean train loss:  1.72356085e-05, mean val. loss:  2.76440191e+00\n",
      "Epoch: 14108 mean train loss:  1.76115718e-05, mean val. loss:  2.76462865e+00\n",
      "Epoch: 14109 mean train loss:  1.72422442e-05, mean val. loss:  2.76485324e+00\n",
      "Epoch: 14110 mean train loss:  1.73193112e-05, mean val. loss:  2.76512885e+00\n",
      "Epoch: 14111 mean train loss:  1.72731816e-05, mean val. loss:  2.76536012e+00\n",
      "Epoch: 14112 mean train loss:  1.73858716e-05, mean val. loss:  2.76561689e+00\n",
      "Epoch: 14113 mean train loss:  1.72269356e-05, mean val. loss:  2.76587987e+00\n",
      "Epoch: 14114 mean train loss:  1.73574663e-05, mean val. loss:  2.76613426e+00\n",
      "Epoch: 14115 mean train loss:  1.75344758e-05, mean val. loss:  2.76636529e+00\n",
      "Epoch: 14116 mean train loss:  1.74696906e-05, mean val. loss:  2.76659513e+00\n",
      "Epoch: 14117 mean train loss:  1.72882865e-05, mean val. loss:  2.76682043e+00\n",
      "Epoch: 14118 mean train loss:  1.74279267e-05, mean val. loss:  2.76704025e+00\n",
      "Epoch: 14119 mean train loss:  1.72882283e-05, mean val. loss:  2.76725912e+00\n",
      "Epoch: 14120 mean train loss:  1.71680877e-05, mean val. loss:  2.76747298e+00\n",
      "Epoch: 14121 mean train loss:  1.72717846e-05, mean val. loss:  2.76773477e+00\n",
      "Epoch: 14122 mean train loss:  1.75448658e-05, mean val. loss:  2.76800895e+00\n",
      "Epoch: 14123 mean train loss:  1.74132292e-05, mean val. loss:  2.76825166e+00\n",
      "Epoch: 14124 mean train loss:  1.74049055e-05, mean val. loss:  2.76848674e+00\n",
      "Epoch: 14125 mean train loss:  1.74730085e-05, mean val. loss:  2.76868868e+00\n",
      "Epoch: 14126 mean train loss:  1.73574954e-05, mean val. loss:  2.76893663e+00\n",
      "Epoch: 14127 mean train loss:  1.74287707e-05, mean val. loss:  2.76917887e+00\n",
      "Epoch: 14128 mean train loss:  1.74662564e-05, mean val. loss:  2.76939225e+00\n",
      "Epoch: 14129 mean train loss:  1.73850858e-05, mean val. loss:  2.76960039e+00\n",
      "Epoch: 14130 mean train loss:  1.69970444e-05, mean val. loss:  2.76983261e+00\n",
      "Epoch: 14131 mean train loss:  1.72481232e-05, mean val. loss:  2.77008963e+00\n",
      "Epoch: 14132 mean train loss:  1.73922745e-05, mean val. loss:  2.77032208e+00\n",
      "Epoch: 14133 mean train loss:  1.74559245e-05, mean val. loss:  2.77055073e+00\n",
      "Epoch: 14134 mean train loss:  1.73868029e-05, mean val. loss:  2.77080917e+00\n",
      "Epoch: 14135 mean train loss:  1.72135187e-05, mean val. loss:  2.77106833e+00\n",
      "Epoch: 14136 mean train loss:  1.72803120e-05, mean val. loss:  2.77131605e+00\n",
      "Epoch: 14137 mean train loss:  1.73879380e-05, mean val. loss:  2.77155209e+00\n",
      "Epoch: 14138 mean train loss:  1.70287094e-05, mean val. loss:  2.77181053e+00\n",
      "Epoch: 14139 mean train loss:  1.76136964e-05, mean val. loss:  2.77205300e+00\n",
      "Epoch: 14140 mean train loss:  1.76127942e-05, mean val. loss:  2.77224898e+00\n",
      "Epoch: 14141 mean train loss:  1.71365391e-05, mean val. loss:  2.77246761e+00\n",
      "Epoch: 14142 mean train loss:  1.73371809e-05, mean val. loss:  2.77267861e+00\n",
      "Epoch: 14143 mean train loss:  1.71865977e-05, mean val. loss:  2.77288103e+00\n",
      "Epoch: 14144 mean train loss:  1.73666340e-05, mean val. loss:  2.77306938e+00\n",
      "Epoch: 14145 mean train loss:  1.73508597e-05, mean val. loss:  2.77326560e+00\n",
      "Epoch: 14146 mean train loss:  1.69481209e-05, mean val. loss:  2.77348852e+00\n",
      "Epoch: 14147 mean train loss:  1.72272848e-05, mean val. loss:  2.77371597e+00\n",
      "Epoch: 14148 mean train loss:  1.75009191e-05, mean val. loss:  2.77398205e+00\n",
      "Epoch: 14149 mean train loss:  1.71984429e-05, mean val. loss:  2.77427363e+00\n",
      "Epoch: 14150 mean train loss:  1.73092994e-05, mean val. loss:  2.77460790e+00\n",
      "Epoch: 14151 mean train loss:  1.71429419e-05, mean val. loss:  2.77496958e+00\n",
      "Epoch: 14152 mean train loss:  1.76114263e-05, mean val. loss:  2.77532506e+00\n",
      "Epoch: 14153 mean train loss:  1.73582230e-05, mean val. loss:  2.77569771e+00\n",
      "Epoch: 14154 mean train loss:  1.72222499e-05, mean val. loss:  2.77607751e+00\n",
      "Epoch: 14155 mean train loss:  1.71497522e-05, mean val. loss:  2.77646756e+00\n",
      "Epoch: 14156 mean train loss:  1.72436703e-05, mean val. loss:  2.77682877e+00\n",
      "Epoch: 14157 mean train loss:  1.72285654e-05, mean val. loss:  2.77721238e+00\n",
      "Epoch: 14158 mean train loss:  1.73245207e-05, mean val. loss:  2.77757955e+00\n",
      "Epoch: 14159 mean train loss:  1.72643631e-05, mean val. loss:  2.77791095e+00\n",
      "Epoch: 14160 mean train loss:  1.75913447e-05, mean val. loss:  2.77822328e+00\n",
      "Epoch: 14161 mean train loss:  1.72562723e-05, mean val. loss:  2.77851677e+00\n",
      "Epoch: 14162 mean train loss:  1.73414010e-05, mean val. loss:  2.77878523e+00\n",
      "Epoch: 14163 mean train loss:  1.74556044e-05, mean val. loss:  2.77898169e+00\n",
      "Epoch: 14164 mean train loss:  1.69910491e-05, mean val. loss:  2.77916265e+00\n",
      "Epoch: 14165 mean train loss:  1.74452434e-05, mean val. loss:  2.77930689e+00\n",
      "Epoch: 14166 mean train loss:  1.76558970e-05, mean val. loss:  2.77940941e+00\n",
      "Epoch: 14167 mean train loss:  1.72493164e-05, mean val. loss:  2.77946115e+00\n",
      "Epoch: 14168 mean train loss:  1.73656153e-05, mean val. loss:  2.77947283e+00\n",
      "Epoch: 14169 mean train loss:  1.74816232e-05, mean val. loss:  2.77942371e+00\n",
      "Epoch: 14170 mean train loss:  1.73152657e-05, mean val. loss:  2.77937865e+00\n",
      "Epoch: 14171 mean train loss:  1.73135195e-05, mean val. loss:  2.77929854e+00\n",
      "Epoch: 14172 mean train loss:  1.72479195e-05, mean val. loss:  2.77922368e+00\n",
      "Epoch: 14173 mean train loss:  1.71754800e-05, mean val. loss:  2.77917051e+00\n",
      "Epoch: 14174 mean train loss:  1.72783912e-05, mean val. loss:  2.77915215e+00\n",
      "Epoch: 14175 mean train loss:  1.72851433e-05, mean val. loss:  2.77915287e+00\n",
      "Epoch: 14176 mean train loss:  1.72823493e-05, mean val. loss:  2.77916646e+00\n",
      "Epoch: 14177 mean train loss:  1.69453851e-05, mean val. loss:  2.77925515e+00\n",
      "Epoch: 14178 mean train loss:  1.72940199e-05, mean val. loss:  2.77938676e+00\n",
      "Epoch: 14179 mean train loss:  1.76016474e-05, mean val. loss:  2.77954078e+00\n",
      "Epoch: 14180 mean train loss:  1.71218126e-05, mean val. loss:  2.77973223e+00\n",
      "Epoch: 14181 mean train loss:  1.73414883e-05, mean val. loss:  2.77992034e+00\n",
      "Epoch: 14182 mean train loss:  1.72459404e-05, mean val. loss:  2.78013992e+00\n",
      "Epoch: 14183 mean train loss:  1.76424510e-05, mean val. loss:  2.78035808e+00\n",
      "Epoch: 14184 mean train loss:  1.70687563e-05, mean val. loss:  2.78063011e+00\n",
      "Epoch: 14185 mean train loss:  1.73631706e-05, mean val. loss:  2.78090334e+00\n",
      "Epoch: 14186 mean train loss:  1.71311258e-05, mean val. loss:  2.78120208e+00\n",
      "Epoch: 14187 mean train loss:  1.73822045e-05, mean val. loss:  2.78147817e+00\n",
      "Epoch: 14188 mean train loss:  1.72148866e-05, mean val. loss:  2.78174829e+00\n",
      "Epoch: 14189 mean train loss:  1.73387525e-05, mean val. loss:  2.78200483e+00\n",
      "Epoch: 14190 mean train loss:  1.73844455e-05, mean val. loss:  2.78227234e+00\n",
      "Epoch: 14191 mean train loss:  1.73714361e-05, mean val. loss:  2.78255343e+00\n",
      "Epoch: 14192 mean train loss:  1.73586886e-05, mean val. loss:  2.78280020e+00\n",
      "Epoch: 14193 mean train loss:  1.73406443e-05, mean val. loss:  2.78306961e+00\n",
      "Epoch: 14194 mean train loss:  1.72969885e-05, mean val. loss:  2.78333950e+00\n",
      "Epoch: 14195 mean train loss:  1.72528089e-05, mean val. loss:  2.78364658e+00\n",
      "Epoch: 14196 mean train loss:  1.71648862e-05, mean val. loss:  2.78395081e+00\n",
      "Epoch: 14197 mean train loss:  1.73726585e-05, mean val. loss:  2.78427839e+00\n",
      "Epoch: 14198 mean train loss:  1.76451867e-05, mean val. loss:  2.78460693e+00\n",
      "Epoch: 14199 mean train loss:  1.74912857e-05, mean val. loss:  2.78493333e+00\n",
      "Epoch: 14200 mean train loss:  1.71680294e-05, mean val. loss:  2.78532553e+00\n",
      "Epoch: 14201 mean train loss:  1.72265864e-05, mean val. loss:  2.78569388e+00\n",
      "Epoch: 14202 mean train loss:  1.75329624e-05, mean val. loss:  2.78601718e+00\n",
      "Epoch: 14203 mean train loss:  1.69918640e-05, mean val. loss:  2.78635573e+00\n",
      "Epoch: 14204 mean train loss:  1.75384339e-05, mean val. loss:  2.78664708e+00\n",
      "Epoch: 14205 mean train loss:  1.74461456e-05, mean val. loss:  2.78693581e+00\n",
      "Epoch: 14206 mean train loss:  1.72261498e-05, mean val. loss:  2.78722692e+00\n",
      "Epoch: 14207 mean train loss:  1.70932035e-05, mean val. loss:  2.78752971e+00\n",
      "Epoch: 14208 mean train loss:  1.71825523e-05, mean val. loss:  2.78782821e+00\n",
      "Epoch: 14209 mean train loss:  1.74718443e-05, mean val. loss:  2.78808761e+00\n",
      "Epoch: 14210 mean train loss:  1.72694854e-05, mean val. loss:  2.78835058e+00\n",
      "Epoch: 14211 mean train loss:  1.73604349e-05, mean val. loss:  2.78856778e+00\n",
      "Epoch: 14212 mean train loss:  1.75120949e-05, mean val. loss:  2.78875732e+00\n",
      "Epoch: 14213 mean train loss:  1.71277497e-05, mean val. loss:  2.78893828e+00\n",
      "Epoch: 14214 mean train loss:  1.71407592e-05, mean val. loss:  2.78911018e+00\n",
      "Epoch: 14215 mean train loss:  1.70932617e-05, mean val. loss:  2.78931713e+00\n",
      "Epoch: 14216 mean train loss:  1.72687869e-05, mean val. loss:  2.78949666e+00\n",
      "Epoch: 14217 mean train loss:  1.70989078e-05, mean val. loss:  2.78964829e+00\n",
      "Epoch: 14218 mean train loss:  1.72668952e-05, mean val. loss:  2.78982425e+00\n",
      "Epoch: 14219 mean train loss:  1.71258871e-05, mean val. loss:  2.78998899e+00\n",
      "Epoch: 14220 mean train loss:  1.72028376e-05, mean val. loss:  2.79018068e+00\n",
      "Epoch: 14221 mean train loss:  1.73439039e-05, mean val. loss:  2.79039907e+00\n",
      "Epoch: 14222 mean train loss:  1.74371526e-05, mean val. loss:  2.79063058e+00\n",
      "Epoch: 14223 mean train loss:  1.71844440e-05, mean val. loss:  2.79086423e+00\n",
      "Epoch: 14224 mean train loss:  1.72869768e-05, mean val. loss:  2.79108882e+00\n",
      "Epoch: 14225 mean train loss:  1.74575252e-05, mean val. loss:  2.79133606e+00\n",
      "Epoch: 14226 mean train loss:  1.72588334e-05, mean val. loss:  2.79159093e+00\n",
      "Epoch: 14227 mean train loss:  1.72808650e-05, mean val. loss:  2.79182911e+00\n",
      "Epoch: 14228 mean train loss:  1.73425651e-05, mean val. loss:  2.79205203e+00\n",
      "Epoch: 14229 mean train loss:  1.75308378e-05, mean val. loss:  2.79224944e+00\n",
      "Epoch: 14230 mean train loss:  1.71387801e-05, mean val. loss:  2.79242039e+00\n",
      "Epoch: 14231 mean train loss:  1.72217260e-05, mean val. loss:  2.79258227e+00\n",
      "Epoch: 14232 mean train loss:  1.74676243e-05, mean val. loss:  2.79265714e+00\n",
      "Epoch: 14233 mean train loss:  1.71592692e-05, mean val. loss:  2.79273891e+00\n",
      "Epoch: 14234 mean train loss:  1.74519373e-05, mean val. loss:  2.79276133e+00\n",
      "Epoch: 14235 mean train loss:  1.71414868e-05, mean val. loss:  2.79279065e+00\n",
      "Epoch: 14236 mean train loss:  1.71715510e-05, mean val. loss:  2.79283047e+00\n",
      "Epoch: 14237 mean train loss:  1.71737338e-05, mean val. loss:  2.79289794e+00\n",
      "Epoch: 14238 mean train loss:  1.72331929e-05, mean val. loss:  2.79294252e+00\n",
      "Epoch: 14239 mean train loss:  1.72689615e-05, mean val. loss:  2.79301810e+00\n",
      "Epoch: 14240 mean train loss:  1.71538850e-05, mean val. loss:  2.79317975e+00\n",
      "Epoch: 14241 mean train loss:  1.72044674e-05, mean val. loss:  2.79334950e+00\n",
      "Epoch: 14242 mean train loss:  1.72084547e-05, mean val. loss:  2.79358530e+00\n",
      "Epoch: 14243 mean train loss:  1.71081338e-05, mean val. loss:  2.79386806e+00\n",
      "Epoch: 14244 mean train loss:  1.71394786e-05, mean val. loss:  2.79417396e+00\n",
      "Epoch: 14245 mean train loss:  1.72592700e-05, mean val. loss:  2.79454327e+00\n",
      "Epoch: 14246 mean train loss:  1.73927983e-05, mean val. loss:  2.79491663e+00\n",
      "Epoch: 14247 mean train loss:  1.74900633e-05, mean val. loss:  2.79527450e+00\n",
      "Epoch: 14248 mean train loss:  1.73890148e-05, mean val. loss:  2.79565620e+00\n",
      "Epoch: 14249 mean train loss:  1.74873276e-05, mean val. loss:  2.79603076e+00\n",
      "Epoch: 14250 mean train loss:  1.73348235e-05, mean val. loss:  2.79640007e+00\n",
      "Epoch: 14251 mean train loss:  1.77164038e-05, mean val. loss:  2.79673409e+00\n",
      "Epoch: 14252 mean train loss:  1.72148284e-05, mean val. loss:  2.79705024e+00\n",
      "Epoch: 14253 mean train loss:  1.72859873e-05, mean val. loss:  2.79737544e+00\n",
      "Epoch: 14254 mean train loss:  1.73848821e-05, mean val. loss:  2.79766464e+00\n",
      "Epoch: 14255 mean train loss:  1.70045823e-05, mean val. loss:  2.79793859e+00\n",
      "Epoch: 14256 mean train loss:  1.71996362e-05, mean val. loss:  2.79815984e+00\n",
      "Epoch: 14257 mean train loss:  1.70021376e-05, mean val. loss:  2.79838634e+00\n",
      "Epoch: 14258 mean train loss:  1.71617721e-05, mean val. loss:  2.79859638e+00\n",
      "Epoch: 14259 mean train loss:  1.75487658e-05, mean val. loss:  2.79880381e+00\n",
      "Epoch: 14260 mean train loss:  1.73943699e-05, mean val. loss:  2.79900169e+00\n",
      "Epoch: 14261 mean train loss:  1.70032727e-05, mean val. loss:  2.79920459e+00\n",
      "Epoch: 14262 mean train loss:  1.71034189e-05, mean val. loss:  2.79941797e+00\n",
      "Epoch: 14263 mean train loss:  1.75259192e-05, mean val. loss:  2.79961920e+00\n",
      "Epoch: 14264 mean train loss:  1.74250454e-05, mean val. loss:  2.79978371e+00\n",
      "Epoch: 14265 mean train loss:  1.71677675e-05, mean val. loss:  2.79995513e+00\n",
      "Epoch: 14266 mean train loss:  1.70863932e-05, mean val. loss:  2.80018115e+00\n",
      "Epoch: 14267 mean train loss:  1.72157888e-05, mean val. loss:  2.80041552e+00\n",
      "Epoch: 14268 mean train loss:  1.73061562e-05, mean val. loss:  2.80065084e+00\n",
      "Epoch: 14269 mean train loss:  1.74736197e-05, mean val. loss:  2.80087781e+00\n",
      "Epoch: 14270 mean train loss:  1.70807180e-05, mean val. loss:  2.80110788e+00\n",
      "Epoch: 14271 mean train loss:  1.74001616e-05, mean val. loss:  2.80133677e+00\n",
      "Epoch: 14272 mean train loss:  1.72907603e-05, mean val. loss:  2.80156446e+00\n",
      "Epoch: 14273 mean train loss:  1.72410801e-05, mean val. loss:  2.80175257e+00\n",
      "Epoch: 14274 mean train loss:  1.69830164e-05, mean val. loss:  2.80194235e+00\n",
      "Epoch: 14275 mean train loss:  1.70605781e-05, mean val. loss:  2.80215311e+00\n",
      "Epoch: 14276 mean train loss:  1.71813590e-05, mean val. loss:  2.80235291e+00\n",
      "Epoch: 14277 mean train loss:  1.73228327e-05, mean val. loss:  2.80255604e+00\n",
      "Epoch: 14278 mean train loss:  1.69517880e-05, mean val. loss:  2.80277586e+00\n",
      "Epoch: 14279 mean train loss:  1.72981818e-05, mean val. loss:  2.80293894e+00\n",
      "Epoch: 14280 mean train loss:  1.74369779e-05, mean val. loss:  2.80308557e+00\n",
      "Epoch: 14281 mean train loss:  1.74033048e-05, mean val. loss:  2.80322766e+00\n",
      "Epoch: 14282 mean train loss:  1.76128233e-05, mean val. loss:  2.80332160e+00\n",
      "Epoch: 14283 mean train loss:  1.70817657e-05, mean val. loss:  2.80343246e+00\n",
      "Epoch: 14284 mean train loss:  1.69075502e-05, mean val. loss:  2.80357790e+00\n",
      "Epoch: 14285 mean train loss:  1.70443091e-05, mean val. loss:  2.80377007e+00\n",
      "Epoch: 14286 mean train loss:  1.72028085e-05, mean val. loss:  2.80397749e+00\n",
      "Epoch: 14287 mean train loss:  1.74597662e-05, mean val. loss:  2.80418205e+00\n",
      "Epoch: 14288 mean train loss:  1.72209984e-05, mean val. loss:  2.80441165e+00\n",
      "Epoch: 14289 mean train loss:  1.70580461e-05, mean val. loss:  2.80468106e+00\n",
      "Epoch: 14290 mean train loss:  1.69817358e-05, mean val. loss:  2.80493426e+00\n",
      "Epoch: 14291 mean train loss:  1.70751882e-05, mean val. loss:  2.80523252e+00\n",
      "Epoch: 14292 mean train loss:  1.72983564e-05, mean val. loss:  2.80552649e+00\n",
      "Epoch: 14293 mean train loss:  1.74865709e-05, mean val. loss:  2.80582714e+00\n",
      "Epoch: 14294 mean train loss:  1.73322333e-05, mean val. loss:  2.80614424e+00\n",
      "Epoch: 14295 mean train loss:  1.72950968e-05, mean val. loss:  2.80644441e+00\n",
      "Epoch: 14296 mean train loss:  1.69714331e-05, mean val. loss:  2.80675030e+00\n",
      "Epoch: 14297 mean train loss:  1.71956490e-05, mean val. loss:  2.80707717e+00\n",
      "Epoch: 14298 mean train loss:  1.72376167e-05, mean val. loss:  2.80742097e+00\n",
      "Epoch: 14299 mean train loss:  1.68779807e-05, mean val. loss:  2.80776358e+00\n",
      "Epoch: 14300 mean train loss:  1.72806613e-05, mean val. loss:  2.80810952e+00\n",
      "Epoch: 14301 mean train loss:  1.74276356e-05, mean val. loss:  2.80843568e+00\n",
      "Epoch: 14302 mean train loss:  1.70292042e-05, mean val. loss:  2.80874634e+00\n",
      "Epoch: 14303 mean train loss:  1.72718719e-05, mean val. loss:  2.80906320e+00\n",
      "Epoch: 14304 mean train loss:  1.70068233e-05, mean val. loss:  2.80935884e+00\n",
      "Epoch: 14305 mean train loss:  1.73242006e-05, mean val. loss:  2.80964088e+00\n",
      "Epoch: 14306 mean train loss:  1.71805441e-05, mean val. loss:  2.80990767e+00\n",
      "Epoch: 14307 mean train loss:  1.73761509e-05, mean val. loss:  2.81014013e+00\n",
      "Epoch: 14308 mean train loss:  1.71204156e-05, mean val. loss:  2.81036854e+00\n",
      "Epoch: 14309 mean train loss:  1.70839485e-05, mean val. loss:  2.81058407e+00\n",
      "Epoch: 14310 mean train loss:  1.72629079e-05, mean val. loss:  2.81079364e+00\n",
      "Epoch: 14311 mean train loss:  1.73609005e-05, mean val. loss:  2.81095076e+00\n",
      "Epoch: 14312 mean train loss:  1.74169254e-05, mean val. loss:  2.81110954e+00\n",
      "Epoch: 14313 mean train loss:  1.70044659e-05, mean val. loss:  2.81128192e+00\n",
      "Epoch: 14314 mean train loss:  1.69065315e-05, mean val. loss:  2.81149316e+00\n",
      "Epoch: 14315 mean train loss:  1.73852313e-05, mean val. loss:  2.81167459e+00\n",
      "Epoch: 14316 mean train loss:  1.71262363e-05, mean val. loss:  2.81184459e+00\n",
      "Epoch: 14317 mean train loss:  1.75289460e-05, mean val. loss:  2.81200004e+00\n",
      "Epoch: 14318 mean train loss:  1.73900044e-05, mean val. loss:  2.81214452e+00\n",
      "Epoch: 14319 mean train loss:  1.75077876e-05, mean val. loss:  2.81226707e+00\n",
      "Epoch: 14320 mean train loss:  1.71274587e-05, mean val. loss:  2.81239200e+00\n",
      "Epoch: 14321 mean train loss:  1.72868022e-05, mean val. loss:  2.81250858e+00\n",
      "Epoch: 14322 mean train loss:  1.73718436e-05, mean val. loss:  2.81261086e+00\n",
      "Epoch: 14323 mean train loss:  1.72670698e-05, mean val. loss:  2.81272411e+00\n",
      "Epoch: 14324 mean train loss:  1.72663131e-05, mean val. loss:  2.81284595e+00\n",
      "Epoch: 14325 mean train loss:  1.69854902e-05, mean val. loss:  2.81296086e+00\n",
      "Epoch: 14326 mean train loss:  1.69897103e-05, mean val. loss:  2.81310773e+00\n",
      "Epoch: 14327 mean train loss:  1.75763853e-05, mean val. loss:  2.81322289e+00\n",
      "Epoch: 14328 mean train loss:  1.71275751e-05, mean val. loss:  2.81336856e+00\n",
      "Epoch: 14329 mean train loss:  1.72818545e-05, mean val. loss:  2.81351829e+00\n",
      "Epoch: 14330 mean train loss:  1.73700682e-05, mean val. loss:  2.81365561e+00\n",
      "Epoch: 14331 mean train loss:  1.72079599e-05, mean val. loss:  2.81381249e+00\n",
      "Epoch: 14332 mean train loss:  1.69955892e-05, mean val. loss:  2.81398153e+00\n",
      "Epoch: 14333 mean train loss:  1.70370622e-05, mean val. loss:  2.81419420e+00\n",
      "Epoch: 14334 mean train loss:  1.71640422e-05, mean val. loss:  2.81442785e+00\n",
      "Epoch: 14335 mean train loss:  1.76389876e-05, mean val. loss:  2.81463885e+00\n",
      "Epoch: 14336 mean train loss:  1.71955617e-05, mean val. loss:  2.81484604e+00\n",
      "Epoch: 14337 mean train loss:  1.71563006e-05, mean val. loss:  2.81507254e+00\n",
      "Epoch: 14338 mean train loss:  1.70209387e-05, mean val. loss:  2.81530738e+00\n",
      "Epoch: 14339 mean train loss:  1.71583670e-05, mean val. loss:  2.81557274e+00\n",
      "Epoch: 14340 mean train loss:  1.72807486e-05, mean val. loss:  2.81584120e+00\n",
      "Epoch: 14341 mean train loss:  1.74297020e-05, mean val. loss:  2.81609964e+00\n",
      "Epoch: 14342 mean train loss:  1.72164291e-05, mean val. loss:  2.81630874e+00\n",
      "Epoch: 14343 mean train loss:  1.70282146e-05, mean val. loss:  2.81656075e+00\n",
      "Epoch: 14344 mean train loss:  1.70833373e-05, mean val. loss:  2.81685090e+00\n",
      "Epoch: 14345 mean train loss:  1.73330773e-05, mean val. loss:  2.81714964e+00\n",
      "Epoch: 14346 mean train loss:  1.72992877e-05, mean val. loss:  2.81742334e+00\n",
      "Epoch: 14347 mean train loss:  1.74008892e-05, mean val. loss:  2.81768346e+00\n",
      "Epoch: 14348 mean train loss:  1.71058928e-05, mean val. loss:  2.81794333e+00\n",
      "Epoch: 14349 mean train loss:  1.71667489e-05, mean val. loss:  2.81822658e+00\n",
      "Epoch: 14350 mean train loss:  1.72849977e-05, mean val. loss:  2.81849980e+00\n",
      "Epoch: 14351 mean train loss:  1.72388391e-05, mean val. loss:  2.81876326e+00\n",
      "Epoch: 14352 mean train loss:  1.70535059e-05, mean val. loss:  2.81904101e+00\n",
      "Epoch: 14353 mean train loss:  1.73091248e-05, mean val. loss:  2.81932902e+00\n",
      "Epoch: 14354 mean train loss:  1.69950363e-05, mean val. loss:  2.81962800e+00\n",
      "Epoch: 14355 mean train loss:  1.72620930e-05, mean val. loss:  2.81993270e+00\n",
      "Epoch: 14356 mean train loss:  1.71832799e-05, mean val. loss:  2.82024240e+00\n",
      "Epoch: 14357 mean train loss:  1.73608132e-05, mean val. loss:  2.82055044e+00\n",
      "Epoch: 14358 mean train loss:  1.76034227e-05, mean val. loss:  2.82084203e+00\n",
      "Epoch: 14359 mean train loss:  1.71449501e-05, mean val. loss:  2.82115746e+00\n",
      "Epoch: 14360 mean train loss:  1.68815022e-05, mean val. loss:  2.82148004e+00\n",
      "Epoch: 14361 mean train loss:  1.73481239e-05, mean val. loss:  2.82178664e+00\n",
      "Epoch: 14362 mean train loss:  1.72980945e-05, mean val. loss:  2.82207417e+00\n",
      "Epoch: 14363 mean train loss:  1.70253334e-05, mean val. loss:  2.82234287e+00\n",
      "Epoch: 14364 mean train loss:  1.74007437e-05, mean val. loss:  2.82260609e+00\n",
      "Epoch: 14365 mean train loss:  1.70317653e-05, mean val. loss:  2.82281995e+00\n",
      "Epoch: 14366 mean train loss:  1.70254498e-05, mean val. loss:  2.82305551e+00\n",
      "Epoch: 14367 mean train loss:  1.70846470e-05, mean val. loss:  2.82324934e+00\n",
      "Epoch: 14368 mean train loss:  1.69154373e-05, mean val. loss:  2.82342958e+00\n",
      "Epoch: 14369 mean train loss:  1.72612490e-05, mean val. loss:  2.82360411e+00\n",
      "Epoch: 14370 mean train loss:  1.70875865e-05, mean val. loss:  2.82378149e+00\n",
      "Epoch: 14371 mean train loss:  1.72650907e-05, mean val. loss:  2.82396030e+00\n",
      "Epoch: 14372 mean train loss:  1.73555745e-05, mean val. loss:  2.82413530e+00\n",
      "Epoch: 14373 mean train loss:  1.72222208e-05, mean val. loss:  2.82432461e+00\n",
      "Epoch: 14374 mean train loss:  1.72104628e-05, mean val. loss:  2.82451129e+00\n",
      "Epoch: 14375 mean train loss:  1.73655571e-05, mean val. loss:  2.82468677e+00\n",
      "Epoch: 14376 mean train loss:  1.75364548e-05, mean val. loss:  2.82484698e+00\n",
      "Epoch: 14377 mean train loss:  1.71781867e-05, mean val. loss:  2.82500124e+00\n",
      "Epoch: 14378 mean train loss:  1.75074965e-05, mean val. loss:  2.82514644e+00\n",
      "Epoch: 14379 mean train loss:  1.71202119e-05, mean val. loss:  2.82523775e+00\n",
      "Epoch: 14380 mean train loss:  1.70527201e-05, mean val. loss:  2.82531953e+00\n",
      "Epoch: 14381 mean train loss:  1.68962870e-05, mean val. loss:  2.82542062e+00\n",
      "Epoch: 14382 mean train loss:  1.72219588e-05, mean val. loss:  2.82547736e+00\n",
      "Epoch: 14383 mean train loss:  1.70291169e-05, mean val. loss:  2.82553434e+00\n",
      "Epoch: 14384 mean train loss:  1.70652056e-05, mean val. loss:  2.82561040e+00\n",
      "Epoch: 14385 mean train loss:  1.70340354e-05, mean val. loss:  2.82567835e+00\n",
      "Epoch: 14386 mean train loss:  1.71453576e-05, mean val. loss:  2.82580829e+00\n",
      "Epoch: 14387 mean train loss:  1.74128800e-05, mean val. loss:  2.82593155e+00\n",
      "Epoch: 14388 mean train loss:  1.75049063e-05, mean val. loss:  2.82600927e+00\n",
      "Epoch: 14389 mean train loss:  1.70556013e-05, mean val. loss:  2.82613492e+00\n",
      "Epoch: 14390 mean train loss:  1.69028935e-05, mean val. loss:  2.82631326e+00\n",
      "Epoch: 14391 mean train loss:  1.71668362e-05, mean val. loss:  2.82652545e+00\n",
      "Epoch: 14392 mean train loss:  1.73183216e-05, mean val. loss:  2.82674718e+00\n",
      "Epoch: 14393 mean train loss:  1.71531283e-05, mean val. loss:  2.82703209e+00\n",
      "Epoch: 14394 mean train loss:  1.70090352e-05, mean val. loss:  2.82733488e+00\n",
      "Epoch: 14395 mean train loss:  1.73382869e-05, mean val. loss:  2.82764745e+00\n",
      "Epoch: 14396 mean train loss:  1.70319690e-05, mean val. loss:  2.82795215e+00\n",
      "Epoch: 14397 mean train loss:  1.68746046e-05, mean val. loss:  2.82829452e+00\n",
      "Epoch: 14398 mean train loss:  1.70701533e-05, mean val. loss:  2.82865167e+00\n",
      "Epoch: 14399 mean train loss:  1.69264968e-05, mean val. loss:  2.82902622e+00\n",
      "Epoch: 14400 mean train loss:  1.70973362e-05, mean val. loss:  2.82940745e+00\n",
      "Epoch: 14401 mean train loss:  1.71944266e-05, mean val. loss:  2.82978296e+00\n",
      "Epoch: 14402 mean train loss:  1.71821448e-05, mean val. loss:  2.83010864e+00\n",
      "Epoch: 14403 mean train loss:  1.70487911e-05, mean val. loss:  2.83042622e+00\n",
      "Epoch: 14404 mean train loss:  1.71685824e-05, mean val. loss:  2.83070922e+00\n",
      "Epoch: 14405 mean train loss:  1.70193962e-05, mean val. loss:  2.83101368e+00\n",
      "Epoch: 14406 mean train loss:  1.69116247e-05, mean val. loss:  2.83132648e+00\n",
      "Epoch: 14407 mean train loss:  1.72840664e-05, mean val. loss:  2.83160639e+00\n",
      "Epoch: 14408 mean train loss:  1.68841507e-05, mean val. loss:  2.83188581e+00\n",
      "Epoch: 14409 mean train loss:  1.69240229e-05, mean val. loss:  2.83216929e+00\n",
      "Epoch: 14410 mean train loss:  1.70643907e-05, mean val. loss:  2.83240271e+00\n",
      "Epoch: 14411 mean train loss:  1.68604602e-05, mean val. loss:  2.83264947e+00\n",
      "Epoch: 14412 mean train loss:  1.71322608e-05, mean val. loss:  2.83289576e+00\n",
      "Epoch: 14413 mean train loss:  1.67914841e-05, mean val. loss:  2.83314013e+00\n",
      "Epoch: 14414 mean train loss:  1.71934371e-05, mean val. loss:  2.83337188e+00\n",
      "Epoch: 14415 mean train loss:  1.69825507e-05, mean val. loss:  2.83358550e+00\n",
      "Epoch: 14416 mean train loss:  1.73593871e-05, mean val. loss:  2.83376265e+00\n",
      "Epoch: 14417 mean train loss:  1.69163977e-05, mean val. loss:  2.83394837e+00\n",
      "Epoch: 14418 mean train loss:  1.68177939e-05, mean val. loss:  2.83415627e+00\n",
      "Epoch: 14419 mean train loss:  1.74373563e-05, mean val. loss:  2.83431911e+00\n",
      "Epoch: 14420 mean train loss:  1.71464926e-05, mean val. loss:  2.83449054e+00\n",
      "Epoch: 14421 mean train loss:  1.73034205e-05, mean val. loss:  2.83465695e+00\n",
      "Epoch: 14422 mean train loss:  1.67379621e-05, mean val. loss:  2.83486795e+00\n",
      "Epoch: 14423 mean train loss:  1.70393032e-05, mean val. loss:  2.83509135e+00\n",
      "Epoch: 14424 mean train loss:  1.69548730e-05, mean val. loss:  2.83533859e+00\n",
      "Epoch: 14425 mean train loss:  1.73348817e-05, mean val. loss:  2.83556175e+00\n",
      "Epoch: 14426 mean train loss:  1.74238230e-05, mean val. loss:  2.83574939e+00\n",
      "Epoch: 14427 mean train loss:  1.74023444e-05, mean val. loss:  2.83591986e+00\n",
      "Epoch: 14428 mean train loss:  1.75536261e-05, mean val. loss:  2.83603454e+00\n",
      "Epoch: 14429 mean train loss:  1.71458232e-05, mean val. loss:  2.83612275e+00\n",
      "Epoch: 14430 mean train loss:  1.70484418e-05, mean val. loss:  2.83620691e+00\n",
      "Epoch: 14431 mean train loss:  1.71333086e-05, mean val. loss:  2.83628535e+00\n",
      "Epoch: 14432 mean train loss:  1.73587177e-05, mean val. loss:  2.83635283e+00\n",
      "Epoch: 14433 mean train loss:  1.72021100e-05, mean val. loss:  2.83642244e+00\n",
      "Epoch: 14434 mean train loss:  1.71527790e-05, mean val. loss:  2.83645296e+00\n",
      "Epoch: 14435 mean train loss:  1.72809232e-05, mean val. loss:  2.83651090e+00\n",
      "Epoch: 14436 mean train loss:  1.71099091e-05, mean val. loss:  2.83658719e+00\n",
      "Epoch: 14437 mean train loss:  1.70861022e-05, mean val. loss:  2.83673406e+00\n",
      "Epoch: 14438 mean train loss:  1.70559797e-05, mean val. loss:  2.83693171e+00\n",
      "Epoch: 14439 mean train loss:  1.73629378e-05, mean val. loss:  2.83715534e+00\n",
      "Epoch: 14440 mean train loss:  1.72898290e-05, mean val. loss:  2.83740973e+00\n",
      "Epoch: 14441 mean train loss:  1.72225991e-05, mean val. loss:  2.83771920e+00\n",
      "Epoch: 14442 mean train loss:  1.73025765e-05, mean val. loss:  2.83806038e+00\n",
      "Epoch: 14443 mean train loss:  1.71320280e-05, mean val. loss:  2.83847332e+00\n",
      "Epoch: 14444 mean train loss:  1.69919804e-05, mean val. loss:  2.83890963e+00\n",
      "Epoch: 14445 mean train loss:  1.71057472e-05, mean val. loss:  2.83937454e+00\n",
      "Epoch: 14446 mean train loss:  1.73186127e-05, mean val. loss:  2.83984613e+00\n",
      "Epoch: 14447 mean train loss:  1.67070539e-05, mean val. loss:  2.84036398e+00\n",
      "Epoch: 14448 mean train loss:  1.71391002e-05, mean val. loss:  2.84088802e+00\n",
      "Epoch: 14449 mean train loss:  1.70146523e-05, mean val. loss:  2.84141016e+00\n",
      "Epoch: 14450 mean train loss:  1.73484150e-05, mean val. loss:  2.84189630e+00\n",
      "Epoch: 14451 mean train loss:  1.71795837e-05, mean val. loss:  2.84229684e+00\n",
      "Epoch: 14452 mean train loss:  1.69981504e-05, mean val. loss:  2.84265924e+00\n",
      "Epoch: 14453 mean train loss:  1.71028660e-05, mean val. loss:  2.84299326e+00\n",
      "Epoch: 14454 mean train loss:  1.72351720e-05, mean val. loss:  2.84330821e+00\n",
      "Epoch: 14455 mean train loss:  1.70273997e-05, mean val. loss:  2.84357166e+00\n",
      "Epoch: 14456 mean train loss:  1.70344720e-05, mean val. loss:  2.84379530e+00\n",
      "Epoch: 14457 mean train loss:  1.72827276e-05, mean val. loss:  2.84397864e+00\n",
      "Epoch: 14458 mean train loss:  1.69945415e-05, mean val. loss:  2.84414721e+00\n",
      "Epoch: 14459 mean train loss:  1.66690443e-05, mean val. loss:  2.84427953e+00\n",
      "Epoch: 14460 mean train loss:  1.69749546e-05, mean val. loss:  2.84440637e+00\n",
      "Epoch: 14461 mean train loss:  1.69076666e-05, mean val. loss:  2.84456277e+00\n",
      "Epoch: 14462 mean train loss:  1.68122060e-05, mean val. loss:  2.84476304e+00\n",
      "Epoch: 14463 mean train loss:  1.70691346e-05, mean val. loss:  2.84498143e+00\n",
      "Epoch: 14464 mean train loss:  1.71364518e-05, mean val. loss:  2.84516668e+00\n",
      "Epoch: 14465 mean train loss:  1.71957363e-05, mean val. loss:  2.84537601e+00\n",
      "Epoch: 14466 mean train loss:  1.68457045e-05, mean val. loss:  2.84566998e+00\n",
      "Epoch: 14467 mean train loss:  1.70924468e-05, mean val. loss:  2.84596324e+00\n",
      "Epoch: 14468 mean train loss:  1.69783016e-05, mean val. loss:  2.84624577e+00\n",
      "Epoch: 14469 mean train loss:  1.70505373e-05, mean val. loss:  2.84648514e+00\n",
      "Epoch: 14470 mean train loss:  1.71286229e-05, mean val. loss:  2.84671736e+00\n",
      "Epoch: 14471 mean train loss:  1.69394771e-05, mean val. loss:  2.84694219e+00\n",
      "Epoch: 14472 mean train loss:  1.71271386e-05, mean val. loss:  2.84714699e+00\n",
      "Epoch: 14473 mean train loss:  1.72451546e-05, mean val. loss:  2.84730911e+00\n",
      "Epoch: 14474 mean train loss:  1.69468985e-05, mean val. loss:  2.84748197e+00\n",
      "Epoch: 14475 mean train loss:  1.69721898e-05, mean val. loss:  2.84762430e+00\n",
      "Epoch: 14476 mean train loss:  1.65874953e-05, mean val. loss:  2.84777212e+00\n",
      "Epoch: 14477 mean train loss:  1.69643899e-05, mean val. loss:  2.84789109e+00\n",
      "Epoch: 14478 mean train loss:  1.74159650e-05, mean val. loss:  2.84796071e+00\n",
      "Epoch: 14479 mean train loss:  1.72375876e-05, mean val. loss:  2.84801960e+00\n",
      "Epoch: 14480 mean train loss:  1.71900319e-05, mean val. loss:  2.84803176e+00\n",
      "Epoch: 14481 mean train loss:  1.71418651e-05, mean val. loss:  2.84805083e+00\n",
      "Epoch: 14482 mean train loss:  1.72435248e-05, mean val. loss:  2.84804869e+00\n",
      "Epoch: 14483 mean train loss:  1.70195999e-05, mean val. loss:  2.84805393e+00\n",
      "Epoch: 14484 mean train loss:  1.71599968e-05, mean val. loss:  2.84805512e+00\n",
      "Epoch: 14485 mean train loss:  1.69986452e-05, mean val. loss:  2.84810948e+00\n",
      "Epoch: 14486 mean train loss:  1.72546424e-05, mean val. loss:  2.84818459e+00\n",
      "Epoch: 14487 mean train loss:  1.70081039e-05, mean val. loss:  2.84830236e+00\n",
      "Epoch: 14488 mean train loss:  1.72561558e-05, mean val. loss:  2.84844065e+00\n",
      "Epoch: 14489 mean train loss:  1.71191932e-05, mean val. loss:  2.84861183e+00\n",
      "Epoch: 14490 mean train loss:  1.72451546e-05, mean val. loss:  2.84880257e+00\n",
      "Epoch: 14491 mean train loss:  1.72018772e-05, mean val. loss:  2.84904623e+00\n",
      "Epoch: 14492 mean train loss:  1.72508298e-05, mean val. loss:  2.84930944e+00\n",
      "Epoch: 14493 mean train loss:  1.67879334e-05, mean val. loss:  2.84958792e+00\n",
      "Epoch: 14494 mean train loss:  1.71493448e-05, mean val. loss:  2.84988523e+00\n",
      "Epoch: 14495 mean train loss:  1.72860746e-05, mean val. loss:  2.85019636e+00\n",
      "Epoch: 14496 mean train loss:  1.69776904e-05, mean val. loss:  2.85053134e+00\n",
      "Epoch: 14497 mean train loss:  1.70124986e-05, mean val. loss:  2.85087919e+00\n",
      "Epoch: 14498 mean train loss:  1.70908461e-05, mean val. loss:  2.85125351e+00\n",
      "Epoch: 14499 mean train loss:  1.70172134e-05, mean val. loss:  2.85165620e+00\n",
      "Epoch: 14500 mean train loss:  1.70134299e-05, mean val. loss:  2.85207844e+00\n",
      "Epoch: 14501 mean train loss:  1.69596169e-05, mean val. loss:  2.85250211e+00\n",
      "Epoch: 14502 mean train loss:  1.68786501e-05, mean val. loss:  2.85296917e+00\n",
      "Epoch: 14503 mean train loss:  1.69022242e-05, mean val. loss:  2.85342097e+00\n",
      "Epoch: 14504 mean train loss:  1.70644198e-05, mean val. loss:  2.85391498e+00\n",
      "Epoch: 14505 mean train loss:  1.71264401e-05, mean val. loss:  2.85438323e+00\n",
      "Epoch: 14506 mean train loss:  1.70770509e-05, mean val. loss:  2.85484242e+00\n",
      "Epoch: 14507 mean train loss:  1.70772546e-05, mean val. loss:  2.85530329e+00\n",
      "Epoch: 14508 mean train loss:  1.71854917e-05, mean val. loss:  2.85574913e+00\n",
      "Epoch: 14509 mean train loss:  1.71874999e-05, mean val. loss:  2.85614634e+00\n",
      "Epoch: 14510 mean train loss:  1.70300191e-05, mean val. loss:  2.85649300e+00\n",
      "Epoch: 14511 mean train loss:  1.71678257e-05, mean val. loss:  2.85679269e+00\n",
      "Epoch: 14512 mean train loss:  1.69974519e-05, mean val. loss:  2.85706687e+00\n",
      "Epoch: 14513 mean train loss:  1.68828992e-05, mean val. loss:  2.85730910e+00\n",
      "Epoch: 14514 mean train loss:  1.69256527e-05, mean val. loss:  2.85749960e+00\n",
      "Epoch: 14515 mean train loss:  1.69148261e-05, mean val. loss:  2.85767508e+00\n",
      "Epoch: 14516 mean train loss:  1.71612191e-05, mean val. loss:  2.85781574e+00\n",
      "Epoch: 14517 mean train loss:  1.68129045e-05, mean val. loss:  2.85795736e+00\n",
      "Epoch: 14518 mean train loss:  1.69804553e-05, mean val. loss:  2.85807896e+00\n",
      "Epoch: 14519 mean train loss:  1.69659615e-05, mean val. loss:  2.85816336e+00\n",
      "Epoch: 14520 mean train loss:  1.70590065e-05, mean val. loss:  2.85825801e+00\n",
      "Epoch: 14521 mean train loss:  1.68557744e-05, mean val. loss:  2.85837054e+00\n",
      "Epoch: 14522 mean train loss:  1.68291735e-05, mean val. loss:  2.85851026e+00\n",
      "Epoch: 14523 mean train loss:  1.68524566e-05, mean val. loss:  2.85866594e+00\n",
      "Epoch: 14524 mean train loss:  1.73998124e-05, mean val. loss:  2.85881186e+00\n",
      "Epoch: 14525 mean train loss:  1.69215782e-05, mean val. loss:  2.85897374e+00\n",
      "Epoch: 14526 mean train loss:  1.73557783e-05, mean val. loss:  2.85915804e+00\n",
      "Epoch: 14527 mean train loss:  1.71930005e-05, mean val. loss:  2.85934496e+00\n",
      "Epoch: 14528 mean train loss:  1.70271960e-05, mean val. loss:  2.85952306e+00\n",
      "Epoch: 14529 mean train loss:  1.68164552e-05, mean val. loss:  2.85972619e+00\n",
      "Epoch: 14530 mean train loss:  1.73825538e-05, mean val. loss:  2.85988498e+00\n",
      "Epoch: 14531 mean train loss:  1.72075524e-05, mean val. loss:  2.86002636e+00\n",
      "Epoch: 14532 mean train loss:  1.70936401e-05, mean val. loss:  2.86016631e+00\n",
      "Epoch: 14533 mean train loss:  1.68990809e-05, mean val. loss:  2.86028123e+00\n",
      "Epoch: 14534 mean train loss:  1.70430576e-05, mean val. loss:  2.86037993e+00\n",
      "Epoch: 14535 mean train loss:  1.68297265e-05, mean val. loss:  2.86048245e+00\n",
      "Epoch: 14536 mean train loss:  1.69501873e-05, mean val. loss:  2.86057448e+00\n",
      "Epoch: 14537 mean train loss:  1.69562991e-05, mean val. loss:  2.86068487e+00\n",
      "Epoch: 14538 mean train loss:  1.70054263e-05, mean val. loss:  2.86080050e+00\n",
      "Epoch: 14539 mean train loss:  1.70943094e-05, mean val. loss:  2.86088920e+00\n",
      "Epoch: 14540 mean train loss:  1.71411666e-05, mean val. loss:  2.86100698e+00\n",
      "Epoch: 14541 mean train loss:  1.69906416e-05, mean val. loss:  2.86113906e+00\n",
      "Epoch: 14542 mean train loss:  1.71511201e-05, mean val. loss:  2.86130548e+00\n",
      "Epoch: 14543 mean train loss:  1.66772807e-05, mean val. loss:  2.86154747e+00\n",
      "Epoch: 14544 mean train loss:  1.69908453e-05, mean val. loss:  2.86180305e+00\n",
      "Epoch: 14545 mean train loss:  1.71604916e-05, mean val. loss:  2.86207676e+00\n",
      "Epoch: 14546 mean train loss:  1.70323474e-05, mean val. loss:  2.86238313e+00\n",
      "Epoch: 14547 mean train loss:  1.71186111e-05, mean val. loss:  2.86266732e+00\n",
      "Epoch: 14548 mean train loss:  1.71562424e-05, mean val. loss:  2.86295366e+00\n",
      "Epoch: 14549 mean train loss:  1.70746644e-05, mean val. loss:  2.86326098e+00\n",
      "Epoch: 14550 mean train loss:  1.68873230e-05, mean val. loss:  2.86355996e+00\n",
      "Epoch: 14551 mean train loss:  1.71225402e-05, mean val. loss:  2.86388302e+00\n",
      "Epoch: 14552 mean train loss:  1.71337742e-05, mean val. loss:  2.86420608e+00\n",
      "Epoch: 14553 mean train loss:  1.70789426e-05, mean val. loss:  2.86453009e+00\n",
      "Epoch: 14554 mean train loss:  1.73270528e-05, mean val. loss:  2.86481857e+00\n",
      "Epoch: 14555 mean train loss:  1.69115083e-05, mean val. loss:  2.86511683e+00\n",
      "Epoch: 14556 mean train loss:  1.71891297e-05, mean val. loss:  2.86539578e+00\n",
      "Epoch: 14557 mean train loss:  1.72595319e-05, mean val. loss:  2.86564994e+00\n",
      "Epoch: 14558 mean train loss:  1.74198358e-05, mean val. loss:  2.86587143e+00\n",
      "Epoch: 14559 mean train loss:  1.70072890e-05, mean val. loss:  2.86608577e+00\n",
      "Epoch: 14560 mean train loss:  1.70467247e-05, mean val. loss:  2.86629200e+00\n",
      "Epoch: 14561 mean train loss:  1.70813291e-05, mean val. loss:  2.86649013e+00\n",
      "Epoch: 14562 mean train loss:  1.71617139e-05, mean val. loss:  2.86667395e+00\n",
      "Epoch: 14563 mean train loss:  1.71412248e-05, mean val. loss:  2.86684632e+00\n",
      "Epoch: 14564 mean train loss:  1.66516111e-05, mean val. loss:  2.86703062e+00\n",
      "Epoch: 14565 mean train loss:  1.71553402e-05, mean val. loss:  2.86718678e+00\n",
      "Epoch: 14566 mean train loss:  1.69556588e-05, mean val. loss:  2.86737084e+00\n",
      "Epoch: 14567 mean train loss:  1.71895954e-05, mean val. loss:  2.86756516e+00\n",
      "Epoch: 14568 mean train loss:  1.69598788e-05, mean val. loss:  2.86782217e+00\n",
      "Epoch: 14569 mean train loss:  1.70006242e-05, mean val. loss:  2.86811996e+00\n",
      "Epoch: 14570 mean train loss:  1.69535342e-05, mean val. loss:  2.86844730e+00\n",
      "Epoch: 14571 mean train loss:  1.70856074e-05, mean val. loss:  2.86876631e+00\n",
      "Epoch: 14572 mean train loss:  1.70023122e-05, mean val. loss:  2.86912370e+00\n",
      "Epoch: 14573 mean train loss:  1.75129971e-05, mean val. loss:  2.86943555e+00\n",
      "Epoch: 14574 mean train loss:  1.72207947e-05, mean val. loss:  2.86971259e+00\n",
      "Epoch: 14575 mean train loss:  1.73437875e-05, mean val. loss:  2.86998296e+00\n",
      "Epoch: 14576 mean train loss:  1.68984698e-05, mean val. loss:  2.87027383e+00\n",
      "Epoch: 14577 mean train loss:  1.68343831e-05, mean val. loss:  2.87054968e+00\n",
      "Epoch: 14578 mean train loss:  1.69138366e-05, mean val. loss:  2.87082791e+00\n",
      "Epoch: 14579 mean train loss:  1.71931752e-05, mean val. loss:  2.87108898e+00\n",
      "Epoch: 14580 mean train loss:  1.67363323e-05, mean val. loss:  2.87137651e+00\n",
      "Epoch: 14581 mean train loss:  1.72864820e-05, mean val. loss:  2.87163377e+00\n",
      "Epoch: 14582 mean train loss:  1.69939594e-05, mean val. loss:  2.87183738e+00\n",
      "Epoch: 14583 mean train loss:  1.71740248e-05, mean val. loss:  2.87204933e+00\n",
      "Epoch: 14584 mean train loss:  1.71202992e-05, mean val. loss:  2.87226295e+00\n",
      "Epoch: 14585 mean train loss:  1.70283311e-05, mean val. loss:  2.87247467e+00\n",
      "Epoch: 14586 mean train loss:  1.71896245e-05, mean val. loss:  2.87269449e+00\n",
      "Epoch: 14587 mean train loss:  1.73504523e-05, mean val. loss:  2.87285781e+00\n",
      "Epoch: 14588 mean train loss:  1.69747218e-05, mean val. loss:  2.87301588e+00\n",
      "Epoch: 14589 mean train loss:  1.68389524e-05, mean val. loss:  2.87319207e+00\n",
      "Epoch: 14590 mean train loss:  1.70429412e-05, mean val. loss:  2.87337565e+00\n",
      "Epoch: 14591 mean train loss:  1.69265550e-05, mean val. loss:  2.87357306e+00\n",
      "Epoch: 14592 mean train loss:  1.69640989e-05, mean val. loss:  2.87380171e+00\n",
      "Epoch: 14593 mean train loss:  1.72240834e-05, mean val. loss:  2.87404871e+00\n",
      "Epoch: 14594 mean train loss:  1.70875282e-05, mean val. loss:  2.87428284e+00\n",
      "Epoch: 14595 mean train loss:  1.70042331e-05, mean val. loss:  2.87453985e+00\n",
      "Epoch: 14596 mean train loss:  1.69662526e-05, mean val. loss:  2.87480831e+00\n",
      "Epoch: 14597 mean train loss:  1.70789135e-05, mean val. loss:  2.87506962e+00\n",
      "Epoch: 14598 mean train loss:  1.71832216e-05, mean val. loss:  2.87533283e+00\n",
      "Epoch: 14599 mean train loss:  1.68869738e-05, mean val. loss:  2.87563968e+00\n",
      "Epoch: 14600 mean train loss:  1.69796986e-05, mean val. loss:  2.87595248e+00\n",
      "Epoch: 14601 mean train loss:  1.72447762e-05, mean val. loss:  2.87624788e+00\n",
      "Epoch: 14602 mean train loss:  1.69544073e-05, mean val. loss:  2.87648559e+00\n",
      "Epoch: 14603 mean train loss:  1.69584528e-05, mean val. loss:  2.87667847e+00\n",
      "Epoch: 14604 mean train loss:  1.72855216e-05, mean val. loss:  2.87688279e+00\n",
      "Epoch: 14605 mean train loss:  1.69513805e-05, mean val. loss:  2.87708783e+00\n",
      "Epoch: 14606 mean train loss:  1.71960564e-05, mean val. loss:  2.87729907e+00\n",
      "Epoch: 14607 mean train loss:  1.70732674e-05, mean val. loss:  2.87747145e+00\n",
      "Epoch: 14608 mean train loss:  1.67012331e-05, mean val. loss:  2.87763357e+00\n",
      "Epoch: 14609 mean train loss:  1.68333354e-05, mean val. loss:  2.87779498e+00\n",
      "Epoch: 14610 mean train loss:  1.68795232e-05, mean val. loss:  2.87797832e+00\n",
      "Epoch: 14611 mean train loss:  1.69007108e-05, mean val. loss:  2.87818217e+00\n",
      "Epoch: 14612 mean train loss:  1.66298705e-05, mean val. loss:  2.87842131e+00\n",
      "Epoch: 14613 mean train loss:  1.71306310e-05, mean val. loss:  2.87867332e+00\n",
      "Epoch: 14614 mean train loss:  1.68225961e-05, mean val. loss:  2.87894344e+00\n",
      "Epoch: 14615 mean train loss:  1.70127605e-05, mean val. loss:  2.87921715e+00\n",
      "Epoch: 14616 mean train loss:  1.69105188e-05, mean val. loss:  2.87951851e+00\n",
      "Epoch: 14617 mean train loss:  1.69000123e-05, mean val. loss:  2.87984514e+00\n",
      "Epoch: 14618 mean train loss:  1.72733562e-05, mean val. loss:  2.88014817e+00\n",
      "Epoch: 14619 mean train loss:  1.70081621e-05, mean val. loss:  2.88044238e+00\n",
      "Epoch: 14620 mean train loss:  1.68809784e-05, mean val. loss:  2.88072801e+00\n",
      "Epoch: 14621 mean train loss:  1.70059502e-05, mean val. loss:  2.88100100e+00\n",
      "Epoch: 14622 mean train loss:  1.68706174e-05, mean val. loss:  2.88126826e+00\n",
      "Epoch: 14623 mean train loss:  1.68892147e-05, mean val. loss:  2.88152480e+00\n",
      "Epoch: 14624 mean train loss:  1.66774553e-05, mean val. loss:  2.88176680e+00\n",
      "Epoch: 14625 mean train loss:  1.69511768e-05, mean val. loss:  2.88197923e+00\n",
      "Epoch: 14626 mean train loss:  1.67164544e-05, mean val. loss:  2.88222027e+00\n",
      "Epoch: 14627 mean train loss:  1.68317347e-05, mean val. loss:  2.88245773e+00\n",
      "Epoch: 14628 mean train loss:  1.68888073e-05, mean val. loss:  2.88272238e+00\n",
      "Epoch: 14629 mean train loss:  1.71521388e-05, mean val. loss:  2.88296294e+00\n",
      "Epoch: 14630 mean train loss:  1.70233543e-05, mean val. loss:  2.88322115e+00\n",
      "Epoch: 14631 mean train loss:  1.68643019e-05, mean val. loss:  2.88347483e+00\n",
      "Epoch: 14632 mean train loss:  1.69822015e-05, mean val. loss:  2.88373280e+00\n",
      "Epoch: 14633 mean train loss:  1.69543782e-05, mean val. loss:  2.88399696e+00\n",
      "Epoch: 14634 mean train loss:  1.68941624e-05, mean val. loss:  2.88428712e+00\n",
      "Epoch: 14635 mean train loss:  1.71396241e-05, mean val. loss:  2.88457632e+00\n",
      "Epoch: 14636 mean train loss:  1.68474216e-05, mean val. loss:  2.88487339e+00\n",
      "Epoch: 14637 mean train loss:  1.68250408e-05, mean val. loss:  2.88514352e+00\n",
      "Epoch: 14638 mean train loss:  1.67075195e-05, mean val. loss:  2.88543940e+00\n",
      "Epoch: 14639 mean train loss:  1.68941333e-05, mean val. loss:  2.88575840e+00\n",
      "Epoch: 14640 mean train loss:  1.69762643e-05, mean val. loss:  2.88604736e+00\n",
      "Epoch: 14641 mean train loss:  1.69310370e-05, mean val. loss:  2.88630843e+00\n",
      "Epoch: 14642 mean train loss:  1.71798165e-05, mean val. loss:  2.88652706e+00\n",
      "Epoch: 14643 mean train loss:  1.68408733e-05, mean val. loss:  2.88678122e+00\n",
      "Epoch: 14644 mean train loss:  1.69911073e-05, mean val. loss:  2.88706279e+00\n",
      "Epoch: 14645 mean train loss:  1.68955594e-05, mean val. loss:  2.88738823e+00\n",
      "Epoch: 14646 mean train loss:  1.68245460e-05, mean val. loss:  2.88773131e+00\n",
      "Epoch: 14647 mean train loss:  1.70611020e-05, mean val. loss:  2.88805652e+00\n",
      "Epoch: 14648 mean train loss:  1.72036525e-05, mean val. loss:  2.88835835e+00\n",
      "Epoch: 14649 mean train loss:  1.67952967e-05, mean val. loss:  2.88869095e+00\n",
      "Epoch: 14650 mean train loss:  1.67654071e-05, mean val. loss:  2.88900685e+00\n",
      "Epoch: 14651 mean train loss:  1.68651168e-05, mean val. loss:  2.88934350e+00\n",
      "Epoch: 14652 mean train loss:  1.71447755e-05, mean val. loss:  2.88964200e+00\n",
      "Epoch: 14653 mean train loss:  1.69309496e-05, mean val. loss:  2.88990617e+00\n",
      "Epoch: 14654 mean train loss:  1.70754793e-05, mean val. loss:  2.89014125e+00\n",
      "Epoch: 14655 mean train loss:  1.67965773e-05, mean val. loss:  2.89037752e+00\n",
      "Epoch: 14656 mean train loss:  1.65592355e-05, mean val. loss:  2.89063168e+00\n",
      "Epoch: 14657 mean train loss:  1.71363354e-05, mean val. loss:  2.89082146e+00\n",
      "Epoch: 14658 mean train loss:  1.69815903e-05, mean val. loss:  2.89098501e+00\n",
      "Epoch: 14659 mean train loss:  1.68263505e-05, mean val. loss:  2.89113998e+00\n",
      "Epoch: 14660 mean train loss:  1.68625847e-05, mean val. loss:  2.89128327e+00\n",
      "Epoch: 14661 mean train loss:  1.68278930e-05, mean val. loss:  2.89145637e+00\n",
      "Epoch: 14662 mean train loss:  1.66987302e-05, mean val. loss:  2.89165020e+00\n",
      "Epoch: 14663 mean train loss:  1.68076367e-05, mean val. loss:  2.89189768e+00\n",
      "Epoch: 14664 mean train loss:  1.69290579e-05, mean val. loss:  2.89212346e+00\n",
      "Epoch: 14665 mean train loss:  1.66496320e-05, mean val. loss:  2.89239812e+00\n",
      "Epoch: 14666 mean train loss:  1.72319706e-05, mean val. loss:  2.89264941e+00\n",
      "Epoch: 14667 mean train loss:  1.68167171e-05, mean val. loss:  2.89295316e+00\n",
      "Epoch: 14668 mean train loss:  1.68426486e-05, mean val. loss:  2.89325571e+00\n",
      "Epoch: 14669 mean train loss:  1.70691637e-05, mean val. loss:  2.89355469e+00\n",
      "Epoch: 14670 mean train loss:  1.70308922e-05, mean val. loss:  2.89382720e+00\n",
      "Epoch: 14671 mean train loss:  1.67494873e-05, mean val. loss:  2.89409471e+00\n",
      "Epoch: 14672 mean train loss:  1.71665743e-05, mean val. loss:  2.89431286e+00\n",
      "Epoch: 14673 mean train loss:  1.72301370e-05, mean val. loss:  2.89444590e+00\n",
      "Epoch: 14674 mean train loss:  1.70137791e-05, mean val. loss:  2.89455175e+00\n",
      "Epoch: 14675 mean train loss:  1.71042921e-05, mean val. loss:  2.89464116e+00\n",
      "Epoch: 14676 mean train loss:  1.69138657e-05, mean val. loss:  2.89469576e+00\n",
      "Epoch: 14677 mean train loss:  1.69920677e-05, mean val. loss:  2.89471841e+00\n",
      "Epoch: 14678 mean train loss:  1.68756524e-05, mean val. loss:  2.89472842e+00\n",
      "Epoch: 14679 mean train loss:  1.70769636e-05, mean val. loss:  2.89474082e+00\n",
      "Epoch: 14680 mean train loss:  1.65561796e-05, mean val. loss:  2.89474940e+00\n",
      "Epoch: 14681 mean train loss:  1.69372070e-05, mean val. loss:  2.89476061e+00\n",
      "Epoch: 14682 mean train loss:  1.68869446e-05, mean val. loss:  2.89480114e+00\n",
      "Epoch: 14683 mean train loss:  1.71924185e-05, mean val. loss:  2.89483786e+00\n",
      "Epoch: 14684 mean train loss:  1.71409338e-05, mean val. loss:  2.89487958e+00\n",
      "Epoch: 14685 mean train loss:  1.68605475e-05, mean val. loss:  2.89497757e+00\n",
      "Epoch: 14686 mean train loss:  1.67794351e-05, mean val. loss:  2.89510608e+00\n",
      "Epoch: 14687 mean train loss:  1.70747517e-05, mean val. loss:  2.89527488e+00\n",
      "Epoch: 14688 mean train loss:  1.70635467e-05, mean val. loss:  2.89544034e+00\n",
      "Epoch: 14689 mean train loss:  1.69122359e-05, mean val. loss:  2.89562225e+00\n",
      "Epoch: 14690 mean train loss:  1.68644765e-05, mean val. loss:  2.89584517e+00\n",
      "Epoch: 14691 mean train loss:  1.71260035e-05, mean val. loss:  2.89608145e+00\n",
      "Epoch: 14692 mean train loss:  1.67563849e-05, mean val. loss:  2.89634323e+00\n",
      "Epoch: 14693 mean train loss:  1.70166313e-05, mean val. loss:  2.89663649e+00\n",
      "Epoch: 14694 mean train loss:  1.74020533e-05, mean val. loss:  2.89689255e+00\n",
      "Epoch: 14695 mean train loss:  1.66052196e-05, mean val. loss:  2.89719701e+00\n",
      "Epoch: 14696 mean train loss:  1.70296989e-05, mean val. loss:  2.89753079e+00\n",
      "Epoch: 14697 mean train loss:  1.72085420e-05, mean val. loss:  2.89786005e+00\n",
      "Epoch: 14698 mean train loss:  1.70331332e-05, mean val. loss:  2.89818382e+00\n",
      "Epoch: 14699 mean train loss:  1.69007690e-05, mean val. loss:  2.89849114e+00\n",
      "Epoch: 14700 mean train loss:  1.71136344e-05, mean val. loss:  2.89879918e+00\n",
      "Epoch: 14701 mean train loss:  1.70888088e-05, mean val. loss:  2.89906788e+00\n",
      "Epoch: 14702 mean train loss:  1.69457344e-05, mean val. loss:  2.89931822e+00\n",
      "Epoch: 14703 mean train loss:  1.68392435e-05, mean val. loss:  2.89960670e+00\n",
      "Epoch: 14704 mean train loss:  1.66249520e-05, mean val. loss:  2.89993072e+00\n",
      "Epoch: 14705 mean train loss:  1.74752786e-05, mean val. loss:  2.90022111e+00\n",
      "Epoch: 14706 mean train loss:  1.71247520e-05, mean val. loss:  2.90047407e+00\n",
      "Epoch: 14707 mean train loss:  1.71560096e-05, mean val. loss:  2.90071273e+00\n",
      "Epoch: 14708 mean train loss:  1.71335414e-05, mean val. loss:  2.90094757e+00\n",
      "Epoch: 14709 mean train loss:  1.68160768e-05, mean val. loss:  2.90117931e+00\n",
      "Epoch: 14710 mean train loss:  1.69384875e-05, mean val. loss:  2.90142632e+00\n",
      "Epoch: 14711 mean train loss:  1.71896536e-05, mean val. loss:  2.90165401e+00\n",
      "Epoch: 14712 mean train loss:  1.64677331e-05, mean val. loss:  2.90188861e+00\n",
      "Epoch: 14713 mean train loss:  1.68273982e-05, mean val. loss:  2.90211797e+00\n",
      "Epoch: 14714 mean train loss:  1.71288557e-05, mean val. loss:  2.90230680e+00\n",
      "Epoch: 14715 mean train loss:  1.71842694e-05, mean val. loss:  2.90249014e+00\n",
      "Epoch: 14716 mean train loss:  1.67037360e-05, mean val. loss:  2.90269971e+00\n",
      "Epoch: 14717 mean train loss:  1.69652631e-05, mean val. loss:  2.90289116e+00\n",
      "Epoch: 14718 mean train loss:  1.67681137e-05, mean val. loss:  2.90313148e+00\n",
      "Epoch: 14719 mean train loss:  1.70203275e-05, mean val. loss:  2.90334940e+00\n",
      "Epoch: 14720 mean train loss:  1.68013212e-05, mean val. loss:  2.90358448e+00\n",
      "Epoch: 14721 mean train loss:  1.69304549e-05, mean val. loss:  2.90383148e+00\n",
      "Epoch: 14722 mean train loss:  1.70210842e-05, mean val. loss:  2.90408492e+00\n",
      "Epoch: 14723 mean train loss:  1.68227125e-05, mean val. loss:  2.90434694e+00\n",
      "Epoch: 14724 mean train loss:  1.65956444e-05, mean val. loss:  2.90467501e+00\n",
      "Epoch: 14725 mean train loss:  1.67944818e-05, mean val. loss:  2.90505981e+00\n",
      "Epoch: 14726 mean train loss:  1.68901170e-05, mean val. loss:  2.90545392e+00\n",
      "Epoch: 14727 mean train loss:  1.69334817e-05, mean val. loss:  2.90590167e+00\n",
      "Epoch: 14728 mean train loss:  1.69823761e-05, mean val. loss:  2.90635610e+00\n",
      "Epoch: 14729 mean train loss:  1.68452680e-05, mean val. loss:  2.90686345e+00\n",
      "Epoch: 14730 mean train loss:  1.68513507e-05, mean val. loss:  2.90736914e+00\n",
      "Epoch: 14731 mean train loss:  1.65963429e-05, mean val. loss:  2.90791512e+00\n",
      "Epoch: 14732 mean train loss:  1.68159022e-05, mean val. loss:  2.90845728e+00\n",
      "Epoch: 14733 mean train loss:  1.70379062e-05, mean val. loss:  2.90899324e+00\n",
      "Epoch: 14734 mean train loss:  1.69349951e-05, mean val. loss:  2.90951085e+00\n",
      "Epoch: 14735 mean train loss:  1.67297549e-05, mean val. loss:  2.91003156e+00\n",
      "Epoch: 14736 mean train loss:  1.70125859e-05, mean val. loss:  2.91048431e+00\n",
      "Epoch: 14737 mean train loss:  1.71438442e-05, mean val. loss:  2.91091657e+00\n",
      "Epoch: 14738 mean train loss:  1.70019630e-05, mean val. loss:  2.91128826e+00\n",
      "Epoch: 14739 mean train loss:  1.65513775e-05, mean val. loss:  2.91162252e+00\n",
      "Epoch: 14740 mean train loss:  1.69834530e-05, mean val. loss:  2.91187644e+00\n",
      "Epoch: 14741 mean train loss:  1.67585385e-05, mean val. loss:  2.91212559e+00\n",
      "Epoch: 14742 mean train loss:  1.69670675e-05, mean val. loss:  2.91231894e+00\n",
      "Epoch: 14743 mean train loss:  1.70910498e-05, mean val. loss:  2.91248178e+00\n",
      "Epoch: 14744 mean train loss:  1.72155560e-05, mean val. loss:  2.91256714e+00\n",
      "Epoch: 14745 mean train loss:  1.70074345e-05, mean val. loss:  2.91262984e+00\n",
      "Epoch: 14746 mean train loss:  1.74479792e-05, mean val. loss:  2.91263127e+00\n",
      "Epoch: 14747 mean train loss:  1.70830463e-05, mean val. loss:  2.91257143e+00\n",
      "Epoch: 14748 mean train loss:  1.70930580e-05, mean val. loss:  2.91248417e+00\n",
      "Epoch: 14749 mean train loss:  1.67755061e-05, mean val. loss:  2.91237950e+00\n",
      "Epoch: 14750 mean train loss:  1.69797568e-05, mean val. loss:  2.91227889e+00\n",
      "Epoch: 14751 mean train loss:  1.70792046e-05, mean val. loss:  2.91214371e+00\n",
      "Epoch: 14752 mean train loss:  1.68315019e-05, mean val. loss:  2.91202140e+00\n",
      "Epoch: 14753 mean train loss:  1.69405830e-05, mean val. loss:  2.91188836e+00\n",
      "Epoch: 14754 mean train loss:  1.69389532e-05, mean val. loss:  2.91181660e+00\n",
      "Epoch: 14755 mean train loss:  1.67741091e-05, mean val. loss:  2.91178370e+00\n",
      "Epoch: 14756 mean train loss:  1.68577826e-05, mean val. loss:  2.91177559e+00\n",
      "Epoch: 14757 mean train loss:  1.72866858e-05, mean val. loss:  2.91182017e+00\n",
      "Epoch: 14758 mean train loss:  1.68037950e-05, mean val. loss:  2.91193175e+00\n",
      "Epoch: 14759 mean train loss:  1.72788568e-05, mean val. loss:  2.91207862e+00\n",
      "Epoch: 14760 mean train loss:  1.69781852e-05, mean val. loss:  2.91226029e+00\n",
      "Epoch: 14761 mean train loss:  1.69874693e-05, mean val. loss:  2.91245532e+00\n",
      "Epoch: 14762 mean train loss:  1.70426501e-05, mean val. loss:  2.91269016e+00\n",
      "Epoch: 14763 mean train loss:  1.66741083e-05, mean val. loss:  2.91298246e+00\n",
      "Epoch: 14764 mean train loss:  1.67966937e-05, mean val. loss:  2.91328263e+00\n",
      "Epoch: 14765 mean train loss:  1.70769345e-05, mean val. loss:  2.91360927e+00\n",
      "Epoch: 14766 mean train loss:  1.69384293e-05, mean val. loss:  2.91393065e+00\n",
      "Epoch: 14767 mean train loss:  1.69432315e-05, mean val. loss:  2.91427207e+00\n",
      "Epoch: 14768 mean train loss:  1.69943378e-05, mean val. loss:  2.91462755e+00\n",
      "Epoch: 14769 mean train loss:  1.67430844e-05, mean val. loss:  2.91501570e+00\n",
      "Epoch: 14770 mean train loss:  1.69091218e-05, mean val. loss:  2.91539454e+00\n",
      "Epoch: 14771 mean train loss:  1.71790016e-05, mean val. loss:  2.91576314e+00\n",
      "Epoch: 14772 mean train loss:  1.65715464e-05, mean val. loss:  2.91617942e+00\n",
      "Epoch: 14773 mean train loss:  1.65687234e-05, mean val. loss:  2.91661668e+00\n",
      "Epoch: 14774 mean train loss:  1.69455307e-05, mean val. loss:  2.91705203e+00\n",
      "Epoch: 14775 mean train loss:  1.69659033e-05, mean val. loss:  2.91747522e+00\n",
      "Epoch: 14776 mean train loss:  1.69409032e-05, mean val. loss:  2.91789198e+00\n",
      "Epoch: 14777 mean train loss:  1.68891856e-05, mean val. loss:  2.91832399e+00\n",
      "Epoch: 14778 mean train loss:  1.69361592e-05, mean val. loss:  2.91875887e+00\n",
      "Epoch: 14779 mean train loss:  1.67265825e-05, mean val. loss:  2.91920304e+00\n",
      "Epoch: 14780 mean train loss:  1.68937258e-05, mean val. loss:  2.91961837e+00\n",
      "Epoch: 14781 mean train loss:  1.68629922e-05, mean val. loss:  2.92001796e+00\n",
      "Epoch: 14782 mean train loss:  1.70837739e-05, mean val. loss:  2.92038870e+00\n",
      "Epoch: 14783 mean train loss:  1.68033584e-05, mean val. loss:  2.92073727e+00\n",
      "Epoch: 14784 mean train loss:  1.71977154e-05, mean val. loss:  2.92101336e+00\n",
      "Epoch: 14785 mean train loss:  1.68447150e-05, mean val. loss:  2.92129445e+00\n",
      "Epoch: 14786 mean train loss:  1.67419494e-05, mean val. loss:  2.92154980e+00\n",
      "Epoch: 14787 mean train loss:  1.68797560e-05, mean val. loss:  2.92176867e+00\n",
      "Epoch: 14788 mean train loss:  1.67785038e-05, mean val. loss:  2.92196369e+00\n",
      "Epoch: 14789 mean train loss:  1.70128478e-05, mean val. loss:  2.92212629e+00\n",
      "Epoch: 14790 mean train loss:  1.68421539e-05, mean val. loss:  2.92226577e+00\n",
      "Epoch: 14791 mean train loss:  1.66620011e-05, mean val. loss:  2.92239523e+00\n",
      "Epoch: 14792 mean train loss:  1.68892439e-05, mean val. loss:  2.92250037e+00\n",
      "Epoch: 14793 mean train loss:  1.67747785e-05, mean val. loss:  2.92263126e+00\n",
      "Epoch: 14794 mean train loss:  1.67856342e-05, mean val. loss:  2.92275000e+00\n",
      "Epoch: 14795 mean train loss:  1.70328713e-05, mean val. loss:  2.92287731e+00\n",
      "Epoch: 14796 mean train loss:  1.71716965e-05, mean val. loss:  2.92298388e+00\n",
      "Epoch: 14797 mean train loss:  1.66124664e-05, mean val. loss:  2.92313719e+00\n",
      "Epoch: 14798 mean train loss:  1.67672115e-05, mean val. loss:  2.92327285e+00\n",
      "Epoch: 14799 mean train loss:  1.68474799e-05, mean val. loss:  2.92343211e+00\n",
      "Epoch: 14800 mean train loss:  1.68916595e-05, mean val. loss:  2.92358375e+00\n",
      "Epoch: 14801 mean train loss:  1.69527775e-05, mean val. loss:  2.92374682e+00\n",
      "Epoch: 14802 mean train loss:  1.68760889e-05, mean val. loss:  2.92387342e+00\n",
      "Epoch: 14803 mean train loss:  1.67395629e-05, mean val. loss:  2.92399383e+00\n",
      "Epoch: 14804 mean train loss:  1.73175649e-05, mean val. loss:  2.92409420e+00\n",
      "Epoch: 14805 mean train loss:  1.68872357e-05, mean val. loss:  2.92415762e+00\n",
      "Epoch: 14806 mean train loss:  1.69274863e-05, mean val. loss:  2.92425895e+00\n",
      "Epoch: 14807 mean train loss:  1.68387487e-05, mean val. loss:  2.92435002e+00\n",
      "Epoch: 14808 mean train loss:  1.69155828e-05, mean val. loss:  2.92442465e+00\n",
      "Epoch: 14809 mean train loss:  1.71423599e-05, mean val. loss:  2.92446375e+00\n",
      "Epoch: 14810 mean train loss:  1.68049883e-05, mean val. loss:  2.92451382e+00\n",
      "Epoch: 14811 mean train loss:  1.68251572e-05, mean val. loss:  2.92455029e+00\n",
      "Epoch: 14812 mean train loss:  1.66390673e-05, mean val. loss:  2.92460394e+00\n",
      "Epoch: 14813 mean train loss:  1.69096165e-05, mean val. loss:  2.92469263e+00\n",
      "Epoch: 14814 mean train loss:  1.68886327e-05, mean val. loss:  2.92479181e+00\n",
      "Epoch: 14815 mean train loss:  1.67780090e-05, mean val. loss:  2.92492890e+00\n",
      "Epoch: 14816 mean train loss:  1.69093255e-05, mean val. loss:  2.92510271e+00\n",
      "Epoch: 14817 mean train loss:  1.69490813e-05, mean val. loss:  2.92531610e+00\n",
      "Epoch: 14818 mean train loss:  1.67166581e-05, mean val. loss:  2.92557979e+00\n",
      "Epoch: 14819 mean train loss:  1.66528043e-05, mean val. loss:  2.92589211e+00\n",
      "Epoch: 14820 mean train loss:  1.68281549e-05, mean val. loss:  2.92624283e+00\n",
      "Epoch: 14821 mean train loss:  1.70321728e-05, mean val. loss:  2.92660284e+00\n",
      "Epoch: 14822 mean train loss:  1.67984981e-05, mean val. loss:  2.92699456e+00\n",
      "Epoch: 14823 mean train loss:  1.66678219e-05, mean val. loss:  2.92741442e+00\n",
      "Epoch: 14824 mean train loss:  1.70911662e-05, mean val. loss:  2.92781615e+00\n",
      "Epoch: 14825 mean train loss:  1.67954422e-05, mean val. loss:  2.92823458e+00\n",
      "Epoch: 14826 mean train loss:  1.70488784e-05, mean val. loss:  2.92860532e+00\n",
      "Epoch: 14827 mean train loss:  1.70337735e-05, mean val. loss:  2.92896724e+00\n",
      "Epoch: 14828 mean train loss:  1.69125851e-05, mean val. loss:  2.92934966e+00\n",
      "Epoch: 14829 mean train loss:  1.69794366e-05, mean val. loss:  2.92967057e+00\n",
      "Epoch: 14830 mean train loss:  1.68410479e-05, mean val. loss:  2.92996025e+00\n",
      "Epoch: 14831 mean train loss:  1.67724502e-05, mean val. loss:  2.93024516e+00\n",
      "Epoch: 14832 mean train loss:  1.68737315e-05, mean val. loss:  2.93050957e+00\n",
      "Epoch: 14833 mean train loss:  1.67004764e-05, mean val. loss:  2.93076658e+00\n",
      "Epoch: 14834 mean train loss:  1.67343533e-05, mean val. loss:  2.93099022e+00\n",
      "Epoch: 14835 mean train loss:  1.67184044e-05, mean val. loss:  2.93123913e+00\n",
      "Epoch: 14836 mean train loss:  1.65223901e-05, mean val. loss:  2.93157029e+00\n",
      "Epoch: 14837 mean train loss:  1.67643884e-05, mean val. loss:  2.93188596e+00\n",
      "Epoch: 14838 mean train loss:  1.67733233e-05, mean val. loss:  2.93224502e+00\n",
      "Epoch: 14839 mean train loss:  1.70333078e-05, mean val. loss:  2.93260431e+00\n",
      "Epoch: 14840 mean train loss:  1.70638668e-05, mean val. loss:  2.93294573e+00\n",
      "Epoch: 14841 mean train loss:  1.69000705e-05, mean val. loss:  2.93330479e+00\n",
      "Epoch: 14842 mean train loss:  1.70074054e-05, mean val. loss:  2.93366909e+00\n",
      "Epoch: 14843 mean train loss:  1.70933199e-05, mean val. loss:  2.93403959e+00\n",
      "Epoch: 14844 mean train loss:  1.69549603e-05, mean val. loss:  2.93437409e+00\n",
      "Epoch: 14845 mean train loss:  1.66742248e-05, mean val. loss:  2.93475485e+00\n",
      "Epoch: 14846 mean train loss:  1.67918333e-05, mean val. loss:  2.93511653e+00\n",
      "Epoch: 14847 mean train loss:  1.69787090e-05, mean val. loss:  2.93544745e+00\n",
      "Epoch: 14848 mean train loss:  1.70251587e-05, mean val. loss:  2.93571877e+00\n",
      "Epoch: 14849 mean train loss:  1.67065009e-05, mean val. loss:  2.93600082e+00\n",
      "Epoch: 14850 mean train loss:  1.68871775e-05, mean val. loss:  2.93626666e+00\n",
      "Epoch: 14851 mean train loss:  1.70925050e-05, mean val. loss:  2.93648577e+00\n",
      "Epoch: 14852 mean train loss:  1.70377316e-05, mean val. loss:  2.93667459e+00\n",
      "Epoch: 14853 mean train loss:  1.66035315e-05, mean val. loss:  2.93687725e+00\n",
      "Epoch: 14854 mean train loss:  1.67897088e-05, mean val. loss:  2.93707967e+00\n",
      "Epoch: 14855 mean train loss:  1.67305989e-05, mean val. loss:  2.93728447e+00\n",
      "Epoch: 14856 mean train loss:  1.67609542e-05, mean val. loss:  2.93750238e+00\n",
      "Epoch: 14857 mean train loss:  1.70050480e-05, mean val. loss:  2.93770075e+00\n",
      "Epoch: 14858 mean train loss:  1.68840925e-05, mean val. loss:  2.93790412e+00\n",
      "Epoch: 14859 mean train loss:  1.67488470e-05, mean val. loss:  2.93816471e+00\n",
      "Epoch: 14860 mean train loss:  1.68739352e-05, mean val. loss:  2.93840861e+00\n",
      "Epoch: 14861 mean train loss:  1.65916863e-05, mean val. loss:  2.93866873e+00\n",
      "Epoch: 14862 mean train loss:  1.66772516e-05, mean val. loss:  2.93893433e+00\n",
      "Epoch: 14863 mean train loss:  1.68128463e-05, mean val. loss:  2.93917155e+00\n",
      "Epoch: 14864 mean train loss:  1.67209073e-05, mean val. loss:  2.93938088e+00\n",
      "Epoch: 14865 mean train loss:  1.69326959e-05, mean val. loss:  2.93959641e+00\n",
      "Epoch: 14866 mean train loss:  1.68809493e-05, mean val. loss:  2.93978381e+00\n",
      "Epoch: 14867 mean train loss:  1.67563267e-05, mean val. loss:  2.93992639e+00\n",
      "Epoch: 14868 mean train loss:  1.66151731e-05, mean val. loss:  2.94008255e+00\n",
      "Epoch: 14869 mean train loss:  1.69225095e-05, mean val. loss:  2.94024038e+00\n",
      "Epoch: 14870 mean train loss:  1.69514096e-05, mean val. loss:  2.94036102e+00\n",
      "Epoch: 14871 mean train loss:  1.68009719e-05, mean val. loss:  2.94048691e+00\n",
      "Epoch: 14872 mean train loss:  1.66910759e-05, mean val. loss:  2.94060779e+00\n",
      "Epoch: 14873 mean train loss:  1.69785344e-05, mean val. loss:  2.94072175e+00\n",
      "Epoch: 14874 mean train loss:  1.66311511e-05, mean val. loss:  2.94084644e+00\n",
      "Epoch: 14875 mean train loss:  1.67164253e-05, mean val. loss:  2.94097972e+00\n",
      "Epoch: 14876 mean train loss:  1.69388368e-05, mean val. loss:  2.94113874e+00\n",
      "Epoch: 14877 mean train loss:  1.66425598e-05, mean val. loss:  2.94131303e+00\n",
      "Epoch: 14878 mean train loss:  1.67245162e-05, mean val. loss:  2.94150162e+00\n",
      "Epoch: 14879 mean train loss:  1.68655824e-05, mean val. loss:  2.94170475e+00\n",
      "Epoch: 14880 mean train loss:  1.69245759e-05, mean val. loss:  2.94194961e+00\n",
      "Epoch: 14881 mean train loss:  1.67698308e-05, mean val. loss:  2.94218993e+00\n",
      "Epoch: 14882 mean train loss:  1.66347891e-05, mean val. loss:  2.94249368e+00\n",
      "Epoch: 14883 mean train loss:  1.67874387e-05, mean val. loss:  2.94282627e+00\n",
      "Epoch: 14884 mean train loss:  1.68073166e-05, mean val. loss:  2.94318342e+00\n",
      "Epoch: 14885 mean train loss:  1.69456180e-05, mean val. loss:  2.94356251e+00\n",
      "Epoch: 14886 mean train loss:  1.65653764e-05, mean val. loss:  2.94396019e+00\n",
      "Epoch: 14887 mean train loss:  1.68515544e-05, mean val. loss:  2.94430113e+00\n",
      "Epoch: 14888 mean train loss:  1.67997496e-05, mean val. loss:  2.94467854e+00\n",
      "Epoch: 14889 mean train loss:  1.72388973e-05, mean val. loss:  2.94503570e+00\n",
      "Epoch: 14890 mean train loss:  1.70661660e-05, mean val. loss:  2.94539571e+00\n",
      "Epoch: 14891 mean train loss:  1.68616243e-05, mean val. loss:  2.94577050e+00\n",
      "Epoch: 14892 mean train loss:  1.67845574e-05, mean val. loss:  2.94614911e+00\n",
      "Epoch: 14893 mean train loss:  1.67328981e-05, mean val. loss:  2.94650936e+00\n",
      "Epoch: 14894 mean train loss:  1.67607213e-05, mean val. loss:  2.94686818e+00\n",
      "Epoch: 14895 mean train loss:  1.65759993e-05, mean val. loss:  2.94726872e+00\n",
      "Epoch: 14896 mean train loss:  1.68780971e-05, mean val. loss:  2.94764495e+00\n",
      "Epoch: 14897 mean train loss:  1.67899998e-05, mean val. loss:  2.94798899e+00\n",
      "Epoch: 14898 mean train loss:  1.64963130e-05, mean val. loss:  2.94831753e+00\n",
      "Epoch: 14899 mean train loss:  1.69588311e-05, mean val. loss:  2.94861555e+00\n",
      "Epoch: 14900 mean train loss:  1.67698599e-05, mean val. loss:  2.94890380e+00\n",
      "Epoch: 14901 mean train loss:  1.67622638e-05, mean val. loss:  2.94916964e+00\n",
      "Epoch: 14902 mean train loss:  1.69158156e-05, mean val. loss:  2.94941497e+00\n",
      "Epoch: 14903 mean train loss:  1.70333369e-05, mean val. loss:  2.94964480e+00\n",
      "Epoch: 14904 mean train loss:  1.66266109e-05, mean val. loss:  2.94986320e+00\n",
      "Epoch: 14905 mean train loss:  1.65982638e-05, mean val. loss:  2.95008969e+00\n",
      "Epoch: 14906 mean train loss:  1.65642123e-05, mean val. loss:  2.95032310e+00\n",
      "Epoch: 14907 mean train loss:  1.67795806e-05, mean val. loss:  2.95054531e+00\n",
      "Epoch: 14908 mean train loss:  1.65661622e-05, mean val. loss:  2.95079494e+00\n",
      "Epoch: 14909 mean train loss:  1.67061226e-05, mean val. loss:  2.95103884e+00\n",
      "Epoch: 14910 mean train loss:  1.67384569e-05, mean val. loss:  2.95131183e+00\n",
      "Epoch: 14911 mean train loss:  1.66727696e-05, mean val. loss:  2.95158005e+00\n",
      "Epoch: 14912 mean train loss:  1.70620042e-05, mean val. loss:  2.95181680e+00\n",
      "Epoch: 14913 mean train loss:  1.68217230e-05, mean val. loss:  2.95203257e+00\n",
      "Epoch: 14914 mean train loss:  1.68794650e-05, mean val. loss:  2.95223665e+00\n",
      "Epoch: 14915 mean train loss:  1.68427650e-05, mean val. loss:  2.95238876e+00\n",
      "Epoch: 14916 mean train loss:  1.71330175e-05, mean val. loss:  2.95247698e+00\n",
      "Epoch: 14917 mean train loss:  1.69658160e-05, mean val. loss:  2.95251870e+00\n",
      "Epoch: 14918 mean train loss:  1.65220699e-05, mean val. loss:  2.95257115e+00\n",
      "Epoch: 14919 mean train loss:  1.68631377e-05, mean val. loss:  2.95257163e+00\n",
      "Epoch: 14920 mean train loss:  1.68039114e-05, mean val. loss:  2.95257115e+00\n",
      "Epoch: 14921 mean train loss:  1.70614512e-05, mean val. loss:  2.95256519e+00\n",
      "Epoch: 14922 mean train loss:  1.69631676e-05, mean val. loss:  2.95250535e+00\n",
      "Epoch: 14923 mean train loss:  1.69941632e-05, mean val. loss:  2.95242071e+00\n",
      "Epoch: 14924 mean train loss:  1.69260020e-05, mean val. loss:  2.95233989e+00\n",
      "Epoch: 14925 mean train loss:  1.67714315e-05, mean val. loss:  2.95226336e+00\n",
      "Epoch: 14926 mean train loss:  1.68267288e-05, mean val. loss:  2.95219421e+00\n",
      "Epoch: 14927 mean train loss:  1.67390390e-05, mean val. loss:  2.95216179e+00\n",
      "Epoch: 14928 mean train loss:  1.66660466e-05, mean val. loss:  2.95217848e+00\n",
      "Epoch: 14929 mean train loss:  1.68771076e-05, mean val. loss:  2.95222378e+00\n",
      "Epoch: 14930 mean train loss:  1.70482963e-05, mean val. loss:  2.95227218e+00\n",
      "Epoch: 14931 mean train loss:  1.65600795e-05, mean val. loss:  2.95240450e+00\n",
      "Epoch: 14932 mean train loss:  1.64661615e-05, mean val. loss:  2.95259309e+00\n",
      "Epoch: 14933 mean train loss:  1.67590333e-05, mean val. loss:  2.95283294e+00\n",
      "Epoch: 14934 mean train loss:  1.71682041e-05, mean val. loss:  2.95308900e+00\n",
      "Epoch: 14935 mean train loss:  1.65576348e-05, mean val. loss:  2.95337629e+00\n",
      "Epoch: 14936 mean train loss:  1.68547849e-05, mean val. loss:  2.95370150e+00\n",
      "Epoch: 14937 mean train loss:  1.68284751e-05, mean val. loss:  2.95407057e+00\n",
      "Epoch: 14938 mean train loss:  1.66224781e-05, mean val. loss:  2.95447373e+00\n",
      "Epoch: 14939 mean train loss:  1.68888946e-05, mean val. loss:  2.95491195e+00\n",
      "Epoch: 14940 mean train loss:  1.65853417e-05, mean val. loss:  2.95537138e+00\n",
      "Epoch: 14941 mean train loss:  1.68693950e-05, mean val. loss:  2.95586824e+00\n",
      "Epoch: 14942 mean train loss:  1.67801627e-05, mean val. loss:  2.95636868e+00\n",
      "Epoch: 14943 mean train loss:  1.68546394e-05, mean val. loss:  2.95685410e+00\n",
      "Epoch: 14944 mean train loss:  1.68923580e-05, mean val. loss:  2.95733666e+00\n",
      "Epoch: 14945 mean train loss:  1.67640101e-05, mean val. loss:  2.95776820e+00\n",
      "Epoch: 14946 mean train loss:  1.69647101e-05, mean val. loss:  2.95812511e+00\n",
      "Epoch: 14947 mean train loss:  1.68696861e-05, mean val. loss:  2.95849013e+00\n",
      "Epoch: 14948 mean train loss:  1.67249236e-05, mean val. loss:  2.95882750e+00\n",
      "Epoch: 14949 mean train loss:  1.67160470e-05, mean val. loss:  2.95912838e+00\n",
      "Epoch: 14950 mean train loss:  1.67575781e-05, mean val. loss:  2.95938969e+00\n",
      "Epoch: 14951 mean train loss:  1.65925885e-05, mean val. loss:  2.95960903e+00\n",
      "Epoch: 14952 mean train loss:  1.68603146e-05, mean val. loss:  2.95981812e+00\n",
      "Epoch: 14953 mean train loss:  1.67742837e-05, mean val. loss:  2.95998836e+00\n",
      "Epoch: 14954 mean train loss:  1.66896207e-05, mean val. loss:  2.96011186e+00\n",
      "Epoch: 14955 mean train loss:  1.71017600e-05, mean val. loss:  2.96022367e+00\n",
      "Epoch: 14956 mean train loss:  1.65777165e-05, mean val. loss:  2.96041441e+00\n",
      "Epoch: 14957 mean train loss:  1.67520484e-05, mean val. loss:  2.96064591e+00\n",
      "Epoch: 14958 mean train loss:  1.67101971e-05, mean val. loss:  2.96089602e+00\n",
      "Epoch: 14959 mean train loss:  1.67659018e-05, mean val. loss:  2.96120811e+00\n",
      "Epoch: 14960 mean train loss:  1.67429389e-05, mean val. loss:  2.96153903e+00\n",
      "Epoch: 14961 mean train loss:  1.66482350e-05, mean val. loss:  2.96190810e+00\n",
      "Epoch: 14962 mean train loss:  1.69103441e-05, mean val. loss:  2.96227002e+00\n",
      "Epoch: 14963 mean train loss:  1.67967810e-05, mean val. loss:  2.96265769e+00\n",
      "Epoch: 14964 mean train loss:  1.66821410e-05, mean val. loss:  2.96305847e+00\n",
      "Epoch: 14965 mean train loss:  1.66026002e-05, mean val. loss:  2.96347523e+00\n",
      "Epoch: 14966 mean train loss:  1.69142440e-05, mean val. loss:  2.96388578e+00\n",
      "Epoch: 14967 mean train loss:  1.66293175e-05, mean val. loss:  2.96432924e+00\n",
      "Epoch: 14968 mean train loss:  1.67513499e-05, mean val. loss:  2.96473002e+00\n",
      "Epoch: 14969 mean train loss:  1.66213722e-05, mean val. loss:  2.96511030e+00\n",
      "Epoch: 14970 mean train loss:  1.66872924e-05, mean val. loss:  2.96544290e+00\n",
      "Epoch: 14971 mean train loss:  1.65757374e-05, mean val. loss:  2.96578455e+00\n",
      "Epoch: 14972 mean train loss:  1.68221013e-05, mean val. loss:  2.96614051e+00\n",
      "Epoch: 14973 mean train loss:  1.68075785e-05, mean val. loss:  2.96648741e+00\n",
      "Epoch: 14974 mean train loss:  1.65369129e-05, mean val. loss:  2.96682644e+00\n",
      "Epoch: 14975 mean train loss:  1.66523387e-05, mean val. loss:  2.96716571e+00\n",
      "Epoch: 14976 mean train loss:  1.69216073e-05, mean val. loss:  2.96747613e+00\n",
      "Epoch: 14977 mean train loss:  1.64766680e-05, mean val. loss:  2.96779847e+00\n",
      "Epoch: 14978 mean train loss:  1.65735255e-05, mean val. loss:  2.96813774e+00\n",
      "Epoch: 14979 mean train loss:  1.68048718e-05, mean val. loss:  2.96846724e+00\n",
      "Epoch: 14980 mean train loss:  1.66181999e-05, mean val. loss:  2.96872473e+00\n",
      "Epoch: 14981 mean train loss:  1.69970444e-05, mean val. loss:  2.96892929e+00\n",
      "Epoch: 14982 mean train loss:  1.69665727e-05, mean val. loss:  2.96912360e+00\n",
      "Epoch: 14983 mean train loss:  1.66540267e-05, mean val. loss:  2.96933174e+00\n",
      "Epoch: 14984 mean train loss:  1.65870006e-05, mean val. loss:  2.96950984e+00\n",
      "Epoch: 14985 mean train loss:  1.68655533e-05, mean val. loss:  2.96966910e+00\n",
      "Epoch: 14986 mean train loss:  1.69622072e-05, mean val. loss:  2.96975994e+00\n",
      "Epoch: 14987 mean train loss:  1.67039980e-05, mean val. loss:  2.96983695e+00\n",
      "Epoch: 14988 mean train loss:  1.65221572e-05, mean val. loss:  2.96991992e+00\n",
      "Epoch: 14989 mean train loss:  1.68898259e-05, mean val. loss:  2.96999216e+00\n",
      "Epoch: 14990 mean train loss:  1.67434046e-05, mean val. loss:  2.97001982e+00\n",
      "Epoch: 14991 mean train loss:  1.65486708e-05, mean val. loss:  2.97001576e+00\n",
      "Epoch: 14992 mean train loss:  1.70331041e-05, mean val. loss:  2.96999049e+00\n",
      "Epoch: 14993 mean train loss:  1.69733248e-05, mean val. loss:  2.96992850e+00\n",
      "Epoch: 14994 mean train loss:  1.68439001e-05, mean val. loss:  2.96990061e+00\n",
      "Epoch: 14995 mean train loss:  1.68335973e-05, mean val. loss:  2.96989584e+00\n",
      "Epoch: 14996 mean train loss:  1.68938714e-05, mean val. loss:  2.96986437e+00\n",
      "Epoch: 14997 mean train loss:  1.66452373e-05, mean val. loss:  2.96984148e+00\n",
      "Epoch: 14998 mean train loss:  1.68334518e-05, mean val. loss:  2.96982718e+00\n",
      "Epoch: 14999 mean train loss:  1.70146814e-05, mean val. loss:  2.96982980e+00\n",
      "Epoch: 15000 mean train loss:  1.69934356e-05, mean val. loss:  2.96985388e+00\n",
      "Epoch: 15001 mean train loss:  1.69253908e-05, mean val. loss:  2.96989608e+00\n",
      "Epoch: 15002 mean train loss:  1.67743710e-05, mean val. loss:  2.96998906e+00\n",
      "Epoch: 15003 mean train loss:  1.66940154e-05, mean val. loss:  2.97011018e+00\n",
      "Epoch: 15004 mean train loss:  1.65115343e-05, mean val. loss:  2.97026944e+00\n",
      "Epoch: 15005 mean train loss:  1.65383099e-05, mean val. loss:  2.97045636e+00\n",
      "Epoch: 15006 mean train loss:  1.65240199e-05, mean val. loss:  2.97072577e+00\n",
      "Epoch: 15007 mean train loss:  1.68271945e-05, mean val. loss:  2.97103524e+00\n",
      "Epoch: 15008 mean train loss:  1.68860715e-05, mean val. loss:  2.97136188e+00\n",
      "Epoch: 15009 mean train loss:  1.67426479e-05, mean val. loss:  2.97172260e+00\n",
      "Epoch: 15010 mean train loss:  1.66919199e-05, mean val. loss:  2.97209311e+00\n",
      "Epoch: 15011 mean train loss:  1.65960519e-05, mean val. loss:  2.97250485e+00\n",
      "Epoch: 15012 mean train loss:  1.68223341e-05, mean val. loss:  2.97292328e+00\n",
      "Epoch: 15013 mean train loss:  1.69038540e-05, mean val. loss:  2.97332263e+00\n",
      "Epoch: 15014 mean train loss:  1.66460522e-05, mean val. loss:  2.97375298e+00\n",
      "Epoch: 15015 mean train loss:  1.66311220e-05, mean val. loss:  2.97418141e+00\n",
      "Epoch: 15016 mean train loss:  1.67210819e-05, mean val. loss:  2.97463608e+00\n",
      "Epoch: 15017 mean train loss:  1.69290579e-05, mean val. loss:  2.97506166e+00\n",
      "Epoch: 15018 mean train loss:  1.66597310e-05, mean val. loss:  2.97549367e+00\n",
      "Epoch: 15019 mean train loss:  1.69593259e-05, mean val. loss:  2.97584319e+00\n",
      "Epoch: 15020 mean train loss:  1.67637190e-05, mean val. loss:  2.97617483e+00\n",
      "Epoch: 15021 mean train loss:  1.67276594e-05, mean val. loss:  2.97650433e+00\n",
      "Epoch: 15022 mean train loss:  1.65828678e-05, mean val. loss:  2.97685790e+00\n",
      "Epoch: 15023 mean train loss:  1.66509126e-05, mean val. loss:  2.97723079e+00\n",
      "Epoch: 15024 mean train loss:  1.65934616e-05, mean val. loss:  2.97760844e+00\n",
      "Epoch: 15025 mean train loss:  1.64394442e-05, mean val. loss:  2.97797632e+00\n",
      "Epoch: 15026 mean train loss:  1.65688980e-05, mean val. loss:  2.97836924e+00\n",
      "Epoch: 15027 mean train loss:  1.68762053e-05, mean val. loss:  2.97873354e+00\n",
      "Epoch: 15028 mean train loss:  1.67677645e-05, mean val. loss:  2.97905970e+00\n",
      "Epoch: 15029 mean train loss:  1.67797552e-05, mean val. loss:  2.97935390e+00\n",
      "Epoch: 15030 mean train loss:  1.67421531e-05, mean val. loss:  2.97964692e+00\n",
      "Epoch: 15031 mean train loss:  1.65753590e-05, mean val. loss:  2.97995377e+00\n",
      "Epoch: 15032 mean train loss:  1.69326668e-05, mean val. loss:  2.98023987e+00\n",
      "Epoch: 15033 mean train loss:  1.68299885e-05, mean val. loss:  2.98048544e+00\n",
      "Epoch: 15034 mean train loss:  1.68263214e-05, mean val. loss:  2.98071551e+00\n",
      "Epoch: 15035 mean train loss:  1.66700338e-05, mean val. loss:  2.98091173e+00\n",
      "Epoch: 15036 mean train loss:  1.66688405e-05, mean val. loss:  2.98109865e+00\n",
      "Epoch: 15037 mean train loss:  1.66526297e-05, mean val. loss:  2.98128724e+00\n",
      "Epoch: 15038 mean train loss:  1.66313257e-05, mean val. loss:  2.98146510e+00\n",
      "Epoch: 15039 mean train loss:  1.67030084e-05, mean val. loss:  2.98159337e+00\n",
      "Epoch: 15040 mean train loss:  1.69879931e-05, mean val. loss:  2.98169899e+00\n",
      "Epoch: 15041 mean train loss:  1.69472187e-05, mean val. loss:  2.98179150e+00\n",
      "Epoch: 15042 mean train loss:  1.68367987e-05, mean val. loss:  2.98188710e+00\n",
      "Epoch: 15043 mean train loss:  1.64101075e-05, mean val. loss:  2.98201966e+00\n",
      "Epoch: 15044 mean train loss:  1.66433165e-05, mean val. loss:  2.98214364e+00\n",
      "Epoch: 15045 mean train loss:  1.66090031e-05, mean val. loss:  2.98227024e+00\n",
      "Epoch: 15046 mean train loss:  1.64965750e-05, mean val. loss:  2.98240066e+00\n",
      "Epoch: 15047 mean train loss:  1.67049293e-05, mean val. loss:  2.98254180e+00\n",
      "Epoch: 15048 mean train loss:  1.68139522e-05, mean val. loss:  2.98271108e+00\n",
      "Epoch: 15049 mean train loss:  1.67215185e-05, mean val. loss:  2.98291206e+00\n",
      "Epoch: 15050 mean train loss:  1.66250102e-05, mean val. loss:  2.98315859e+00\n",
      "Epoch: 15051 mean train loss:  1.66671816e-05, mean val. loss:  2.98341894e+00\n",
      "Epoch: 15052 mean train loss:  1.66477112e-05, mean val. loss:  2.98368955e+00\n",
      "Epoch: 15053 mean train loss:  1.63775403e-05, mean val. loss:  2.98403597e+00\n",
      "Epoch: 15054 mean train loss:  1.66315003e-05, mean val. loss:  2.98445964e+00\n",
      "Epoch: 15055 mean train loss:  1.69441046e-05, mean val. loss:  2.98488474e+00\n",
      "Epoch: 15056 mean train loss:  1.69078121e-05, mean val. loss:  2.98528528e+00\n",
      "Epoch: 15057 mean train loss:  1.68113620e-05, mean val. loss:  2.98567677e+00\n",
      "Epoch: 15058 mean train loss:  1.67487306e-05, mean val. loss:  2.98606324e+00\n",
      "Epoch: 15059 mean train loss:  1.69074629e-05, mean val. loss:  2.98643756e+00\n",
      "Epoch: 15060 mean train loss:  1.65239908e-05, mean val. loss:  2.98685169e+00\n",
      "Epoch: 15061 mean train loss:  1.70069980e-05, mean val. loss:  2.98725247e+00\n",
      "Epoch: 15062 mean train loss:  1.66138925e-05, mean val. loss:  2.98760200e+00\n",
      "Epoch: 15063 mean train loss:  1.65489910e-05, mean val. loss:  2.98794270e+00\n",
      "Epoch: 15064 mean train loss:  1.66981190e-05, mean val. loss:  2.98824978e+00\n",
      "Epoch: 15065 mean train loss:  1.67844119e-05, mean val. loss:  2.98851442e+00\n",
      "Epoch: 15066 mean train loss:  1.64212834e-05, mean val. loss:  2.98880887e+00\n",
      "Epoch: 15067 mean train loss:  1.64014054e-05, mean val. loss:  2.98913002e+00\n",
      "Epoch: 15068 mean train loss:  1.66893296e-05, mean val. loss:  2.98943114e+00\n",
      "Epoch: 15069 mean train loss:  1.65669480e-05, mean val. loss:  2.98970485e+00\n",
      "Epoch: 15070 mean train loss:  1.67287944e-05, mean val. loss:  2.98995829e+00\n",
      "Epoch: 15071 mean train loss:  1.65312667e-05, mean val. loss:  2.99018121e+00\n",
      "Epoch: 15072 mean train loss:  1.70058047e-05, mean val. loss:  2.99034286e+00\n",
      "Epoch: 15073 mean train loss:  1.68462284e-05, mean val. loss:  2.99049497e+00\n",
      "Epoch: 15074 mean train loss:  1.66425016e-05, mean val. loss:  2.99061179e+00\n",
      "Epoch: 15075 mean train loss:  1.66403479e-05, mean val. loss:  2.99070907e+00\n",
      "Epoch: 15076 mean train loss:  1.66404061e-05, mean val. loss:  2.99075222e+00\n",
      "Epoch: 15077 mean train loss:  1.68814440e-05, mean val. loss:  2.99075770e+00\n",
      "Epoch: 15078 mean train loss:  1.69125851e-05, mean val. loss:  2.99072266e+00\n",
      "Epoch: 15079 mean train loss:  1.66821992e-05, mean val. loss:  2.99067450e+00\n",
      "Epoch: 15080 mean train loss:  1.65818492e-05, mean val. loss:  2.99066854e+00\n",
      "Epoch: 15081 mean train loss:  1.69887498e-05, mean val. loss:  2.99065161e+00\n",
      "Epoch: 15082 mean train loss:  1.65135134e-05, mean val. loss:  2.99061871e+00\n",
      "Epoch: 15083 mean train loss:  1.70678541e-05, mean val. loss:  2.99059224e+00\n",
      "Epoch: 15084 mean train loss:  1.69469276e-05, mean val. loss:  2.99059558e+00\n",
      "Epoch: 15085 mean train loss:  1.65585079e-05, mean val. loss:  2.99061465e+00\n",
      "Epoch: 15086 mean train loss:  1.64551602e-05, mean val. loss:  2.99068546e+00\n",
      "Epoch: 15087 mean train loss:  1.67636608e-05, mean val. loss:  2.99081969e+00\n",
      "Epoch: 15088 mean train loss:  1.65973906e-05, mean val. loss:  2.99101543e+00\n",
      "Epoch: 15089 mean train loss:  1.66267273e-05, mean val. loss:  2.99125266e+00\n",
      "Epoch: 15090 mean train loss:  1.66685204e-05, mean val. loss:  2.99151015e+00\n",
      "Epoch: 15091 mean train loss:  1.65162492e-05, mean val. loss:  2.99182796e+00\n",
      "Epoch: 15092 mean train loss:  1.66389800e-05, mean val. loss:  2.99215055e+00\n",
      "Epoch: 15093 mean train loss:  1.69973646e-05, mean val. loss:  2.99246693e+00\n",
      "Epoch: 15094 mean train loss:  1.66086247e-05, mean val. loss:  2.99279618e+00\n",
      "Epoch: 15095 mean train loss:  1.64352532e-05, mean val. loss:  2.99310732e+00\n",
      "Epoch: 15096 mean train loss:  1.67691032e-05, mean val. loss:  2.99340677e+00\n",
      "Epoch: 15097 mean train loss:  1.65970705e-05, mean val. loss:  2.99372435e+00\n",
      "Epoch: 15098 mean train loss:  1.67018443e-05, mean val. loss:  2.99401259e+00\n",
      "Epoch: 15099 mean train loss:  1.69630803e-05, mean val. loss:  2.99431062e+00\n",
      "Epoch: 15100 mean train loss:  1.66591490e-05, mean val. loss:  2.99461794e+00\n",
      "Epoch: 15101 mean train loss:  1.66708196e-05, mean val. loss:  2.99493194e+00\n",
      "Epoch: 15102 mean train loss:  1.68262923e-05, mean val. loss:  2.99518824e+00\n",
      "Epoch: 15103 mean train loss:  1.65596139e-05, mean val. loss:  2.99542284e+00\n",
      "Epoch: 15104 mean train loss:  1.64316152e-05, mean val. loss:  2.99565864e+00\n",
      "Epoch: 15105 mean train loss:  1.65529200e-05, mean val. loss:  2.99585795e+00\n",
      "Epoch: 15106 mean train loss:  1.64418016e-05, mean val. loss:  2.99607158e+00\n",
      "Epoch: 15107 mean train loss:  1.64586527e-05, mean val. loss:  2.99630260e+00\n",
      "Epoch: 15108 mean train loss:  1.65209058e-05, mean val. loss:  2.99651051e+00\n",
      "Epoch: 15109 mean train loss:  1.64967205e-05, mean val. loss:  2.99676490e+00\n",
      "Epoch: 15110 mean train loss:  1.67726539e-05, mean val. loss:  2.99700522e+00\n",
      "Epoch: 15111 mean train loss:  1.64967496e-05, mean val. loss:  2.99724650e+00\n",
      "Epoch: 15112 mean train loss:  1.67487597e-05, mean val. loss:  2.99745893e+00\n",
      "Epoch: 15113 mean train loss:  1.67991966e-05, mean val. loss:  2.99767065e+00\n",
      "Epoch: 15114 mean train loss:  1.63691584e-05, mean val. loss:  2.99791265e+00\n",
      "Epoch: 15115 mean train loss:  1.66056561e-05, mean val. loss:  2.99817801e+00\n",
      "Epoch: 15116 mean train loss:  1.69154373e-05, mean val. loss:  2.99843049e+00\n",
      "Epoch: 15117 mean train loss:  1.67310936e-05, mean val. loss:  2.99871159e+00\n",
      "Epoch: 15118 mean train loss:  1.67952967e-05, mean val. loss:  2.99892879e+00\n",
      "Epoch: 15119 mean train loss:  1.67998078e-05, mean val. loss:  2.99915719e+00\n",
      "Epoch: 15120 mean train loss:  1.67199760e-05, mean val. loss:  2.99938250e+00\n",
      "Epoch: 15121 mean train loss:  1.65925885e-05, mean val. loss:  2.99962831e+00\n",
      "Epoch: 15122 mean train loss:  1.68877887e-05, mean val. loss:  2.99984717e+00\n",
      "Epoch: 15123 mean train loss:  1.69608393e-05, mean val. loss:  3.00005913e+00\n",
      "Epoch: 15124 mean train loss:  1.63674122e-05, mean val. loss:  3.00030541e+00\n",
      "Epoch: 15125 mean train loss:  1.66595273e-05, mean val. loss:  3.00054336e+00\n",
      "Epoch: 15126 mean train loss:  1.67296384e-05, mean val. loss:  3.00078249e+00\n",
      "Epoch: 15127 mean train loss:  1.64946541e-05, mean val. loss:  3.00104523e+00\n",
      "Epoch: 15128 mean train loss:  1.63280056e-05, mean val. loss:  3.00133419e+00\n",
      "Epoch: 15129 mean train loss:  1.67452381e-05, mean val. loss:  3.00160933e+00\n",
      "Epoch: 15130 mean train loss:  1.67088583e-05, mean val. loss:  3.00191021e+00\n",
      "Epoch: 15131 mean train loss:  1.68581319e-05, mean val. loss:  3.00220680e+00\n",
      "Epoch: 15132 mean train loss:  1.66843820e-05, mean val. loss:  3.00251055e+00\n",
      "Epoch: 15133 mean train loss:  1.66392711e-05, mean val. loss:  3.00283384e+00\n",
      "Epoch: 15134 mean train loss:  1.65782694e-05, mean val. loss:  3.00313187e+00\n",
      "Epoch: 15135 mean train loss:  1.66608079e-05, mean val. loss:  3.00342369e+00\n",
      "Epoch: 15136 mean train loss:  1.65996607e-05, mean val. loss:  3.00371218e+00\n",
      "Epoch: 15137 mean train loss:  1.64960511e-05, mean val. loss:  3.00398159e+00\n",
      "Epoch: 15138 mean train loss:  1.65996607e-05, mean val. loss:  3.00428414e+00\n",
      "Epoch: 15139 mean train loss:  1.64878729e-05, mean val. loss:  3.00463343e+00\n",
      "Epoch: 15140 mean train loss:  1.63211371e-05, mean val. loss:  3.00500417e+00\n",
      "Epoch: 15141 mean train loss:  1.65057427e-05, mean val. loss:  3.00538850e+00\n",
      "Epoch: 15142 mean train loss:  1.64410449e-05, mean val. loss:  3.00578451e+00\n",
      "Epoch: 15143 mean train loss:  1.65268430e-05, mean val. loss:  3.00620008e+00\n",
      "Epoch: 15144 mean train loss:  1.63872901e-05, mean val. loss:  3.00666451e+00\n",
      "Epoch: 15145 mean train loss:  1.69015257e-05, mean val. loss:  3.00710297e+00\n",
      "Epoch: 15146 mean train loss:  1.65846723e-05, mean val. loss:  3.00757790e+00\n",
      "Epoch: 15147 mean train loss:  1.65224483e-05, mean val. loss:  3.00805140e+00\n",
      "Epoch: 15148 mean train loss:  1.65894162e-05, mean val. loss:  3.00853872e+00\n",
      "Epoch: 15149 mean train loss:  1.68496917e-05, mean val. loss:  3.00897121e+00\n",
      "Epoch: 15150 mean train loss:  1.63701479e-05, mean val. loss:  3.00939107e+00\n",
      "Epoch: 15151 mean train loss:  1.68026017e-05, mean val. loss:  3.00976467e+00\n",
      "Epoch: 15152 mean train loss:  1.66237587e-05, mean val. loss:  3.01009011e+00\n",
      "Epoch: 15153 mean train loss:  1.66704995e-05, mean val. loss:  3.01036048e+00\n",
      "Epoch: 15154 mean train loss:  1.65962556e-05, mean val. loss:  3.01057553e+00\n",
      "Epoch: 15155 mean train loss:  1.65335368e-05, mean val. loss:  3.01071548e+00\n",
      "Epoch: 15156 mean train loss:  1.68681145e-05, mean val. loss:  3.01075053e+00\n",
      "Epoch: 15157 mean train loss:  1.66522223e-05, mean val. loss:  3.01073980e+00\n",
      "Epoch: 15158 mean train loss:  1.66719547e-05, mean val. loss:  3.01067781e+00\n",
      "Epoch: 15159 mean train loss:  1.65943347e-05, mean val. loss:  3.01055479e+00\n",
      "Epoch: 15160 mean train loss:  1.69081031e-05, mean val. loss:  3.01036477e+00\n",
      "Epoch: 15161 mean train loss:  1.68087718e-05, mean val. loss:  3.01015639e+00\n",
      "Epoch: 15162 mean train loss:  1.65499805e-05, mean val. loss:  3.00993776e+00\n",
      "Epoch: 15163 mean train loss:  1.66354876e-05, mean val. loss:  3.00974345e+00\n",
      "Epoch: 15164 mean train loss:  1.65609235e-05, mean val. loss:  3.00959754e+00\n",
      "Epoch: 15165 mean train loss:  1.62472425e-05, mean val. loss:  3.00952864e+00\n",
      "Epoch: 15166 mean train loss:  1.66586833e-05, mean val. loss:  3.00949550e+00\n",
      "Epoch: 15167 mean train loss:  1.68373226e-05, mean val. loss:  3.00951767e+00\n",
      "Epoch: 15168 mean train loss:  1.70838321e-05, mean val. loss:  3.00955653e+00\n",
      "Epoch: 15169 mean train loss:  1.66659011e-05, mean val. loss:  3.00969243e+00\n",
      "Epoch: 15170 mean train loss:  1.61970675e-05, mean val. loss:  3.00994992e+00\n",
      "Epoch: 15171 mean train loss:  1.66559767e-05, mean val. loss:  3.01027369e+00\n",
      "Epoch: 15172 mean train loss:  1.66106038e-05, mean val. loss:  3.01065564e+00\n",
      "Epoch: 15173 mean train loss:  1.66390091e-05, mean val. loss:  3.01110029e+00\n",
      "Epoch: 15174 mean train loss:  1.65373494e-05, mean val. loss:  3.01157475e+00\n",
      "Epoch: 15175 mean train loss:  1.66541431e-05, mean val. loss:  3.01207662e+00\n",
      "Epoch: 15176 mean train loss:  1.63382792e-05, mean val. loss:  3.01261330e+00\n",
      "Epoch: 15177 mean train loss:  1.64168014e-05, mean val. loss:  3.01315856e+00\n",
      "Epoch: 15178 mean train loss:  1.68964325e-05, mean val. loss:  3.01367569e+00\n",
      "Epoch: 15179 mean train loss:  1.65555102e-05, mean val. loss:  3.01425028e+00\n",
      "Epoch: 15180 mean train loss:  1.66308600e-05, mean val. loss:  3.01479721e+00\n",
      "Epoch: 15181 mean train loss:  1.69530394e-05, mean val. loss:  3.01529956e+00\n",
      "Epoch: 15182 mean train loss:  1.64000085e-05, mean val. loss:  3.01582813e+00\n",
      "Epoch: 15183 mean train loss:  1.66278041e-05, mean val. loss:  3.01632190e+00\n",
      "Epoch: 15184 mean train loss:  1.67897670e-05, mean val. loss:  3.01672077e+00\n",
      "Epoch: 15185 mean train loss:  1.64898520e-05, mean val. loss:  3.01711774e+00\n",
      "Epoch: 15186 mean train loss:  1.61418866e-05, mean val. loss:  3.01751566e+00\n",
      "Epoch: 15187 mean train loss:  1.65062083e-05, mean val. loss:  3.01790142e+00\n",
      "Epoch: 15188 mean train loss:  1.65207603e-05, mean val. loss:  3.01823640e+00\n",
      "Epoch: 15189 mean train loss:  1.65876118e-05, mean val. loss:  3.01854515e+00\n",
      "Epoch: 15190 mean train loss:  1.64162484e-05, mean val. loss:  3.01887155e+00\n",
      "Epoch: 15191 mean train loss:  1.65333331e-05, mean val. loss:  3.01921248e+00\n",
      "Epoch: 15192 mean train loss:  1.67419494e-05, mean val. loss:  3.01956201e+00\n",
      "Epoch: 15193 mean train loss:  1.67014659e-05, mean val. loss:  3.01990104e+00\n",
      "Epoch: 15194 mean train loss:  1.66288519e-05, mean val. loss:  3.02023554e+00\n",
      "Epoch: 15195 mean train loss:  1.64416851e-05, mean val. loss:  3.02055359e+00\n",
      "Epoch: 15196 mean train loss:  1.66273094e-05, mean val. loss:  3.02086139e+00\n",
      "Epoch: 15197 mean train loss:  1.67012622e-05, mean val. loss:  3.02115726e+00\n",
      "Epoch: 15198 mean train loss:  1.68233528e-05, mean val. loss:  3.02143502e+00\n",
      "Epoch: 15199 mean train loss:  1.66333048e-05, mean val. loss:  3.02168965e+00\n",
      "Epoch: 15200 mean train loss:  1.67652906e-05, mean val. loss:  3.02188659e+00\n",
      "Epoch: 15201 mean train loss:  1.66372920e-05, mean val. loss:  3.02203941e+00\n",
      "Epoch: 15202 mean train loss:  1.65106030e-05, mean val. loss:  3.02215457e+00\n",
      "Epoch: 15203 mean train loss:  1.66771351e-05, mean val. loss:  3.02222633e+00\n",
      "Epoch: 15204 mean train loss:  1.65328966e-05, mean val. loss:  3.02229714e+00\n",
      "Epoch: 15205 mean train loss:  1.67133403e-05, mean val. loss:  3.02230358e+00\n",
      "Epoch: 15206 mean train loss:  1.65825477e-05, mean val. loss:  3.02229953e+00\n",
      "Epoch: 15207 mean train loss:  1.65649690e-05, mean val. loss:  3.02225304e+00\n",
      "Epoch: 15208 mean train loss:  1.66378159e-05, mean val. loss:  3.02216649e+00\n",
      "Epoch: 15209 mean train loss:  1.67028047e-05, mean val. loss:  3.02204394e+00\n",
      "Epoch: 15210 mean train loss:  1.66498648e-05, mean val. loss:  3.02188635e+00\n",
      "Epoch: 15211 mean train loss:  1.64803059e-05, mean val. loss:  3.02172852e+00\n",
      "Epoch: 15212 mean train loss:  1.67528924e-05, mean val. loss:  3.02158427e+00\n",
      "Epoch: 15213 mean train loss:  1.65778038e-05, mean val. loss:  3.02148438e+00\n",
      "Epoch: 15214 mean train loss:  1.66718964e-05, mean val. loss:  3.02137518e+00\n",
      "Epoch: 15215 mean train loss:  1.64831290e-05, mean val. loss:  3.02133512e+00\n",
      "Epoch: 15216 mean train loss:  1.65681413e-05, mean val. loss:  3.02134395e+00\n",
      "Epoch: 15217 mean train loss:  1.66019599e-05, mean val. loss:  3.02136898e+00\n",
      "Epoch: 15218 mean train loss:  1.67661055e-05, mean val. loss:  3.02147222e+00\n",
      "Epoch: 15219 mean train loss:  1.64413650e-05, mean val. loss:  3.02161789e+00\n",
      "Epoch: 15220 mean train loss:  1.67536200e-05, mean val. loss:  3.02184129e+00\n",
      "Epoch: 15221 mean train loss:  1.68441911e-05, mean val. loss:  3.02213168e+00\n",
      "Epoch: 15222 mean train loss:  1.67204416e-05, mean val. loss:  3.02247357e+00\n",
      "Epoch: 15223 mean train loss:  1.68186962e-05, mean val. loss:  3.02286029e+00\n",
      "Epoch: 15224 mean train loss:  1.66326063e-05, mean val. loss:  3.02328181e+00\n",
      "Epoch: 15225 mean train loss:  1.64247758e-05, mean val. loss:  3.02375555e+00\n",
      "Epoch: 15226 mean train loss:  1.65114470e-05, mean val. loss:  3.02425385e+00\n",
      "Epoch: 15227 mean train loss:  1.68287952e-05, mean val. loss:  3.02473259e+00\n",
      "Epoch: 15228 mean train loss:  1.62802171e-05, mean val. loss:  3.02526712e+00\n",
      "Epoch: 15229 mean train loss:  1.64144149e-05, mean val. loss:  3.02582002e+00\n",
      "Epoch: 15230 mean train loss:  1.69220730e-05, mean val. loss:  3.02631497e+00\n",
      "Epoch: 15231 mean train loss:  1.67088001e-05, mean val. loss:  3.02678061e+00\n",
      "Epoch: 15232 mean train loss:  1.67123799e-05, mean val. loss:  3.02723408e+00\n",
      "Epoch: 15233 mean train loss:  1.66504178e-05, mean val. loss:  3.02767134e+00\n",
      "Epoch: 15234 mean train loss:  1.64028024e-05, mean val. loss:  3.02807593e+00\n",
      "Epoch: 15235 mean train loss:  1.67486432e-05, mean val. loss:  3.02842593e+00\n",
      "Epoch: 15236 mean train loss:  1.66059472e-05, mean val. loss:  3.02875304e+00\n",
      "Epoch: 15237 mean train loss:  1.67532708e-05, mean val. loss:  3.02905297e+00\n",
      "Epoch: 15238 mean train loss:  1.63840596e-05, mean val. loss:  3.02933359e+00\n",
      "Epoch: 15239 mean train loss:  1.65686361e-05, mean val. loss:  3.02958250e+00\n",
      "Epoch: 15240 mean train loss:  1.65168603e-05, mean val. loss:  3.02984786e+00\n",
      "Epoch: 15241 mean train loss:  1.66056270e-05, mean val. loss:  3.03013921e+00\n",
      "Epoch: 15242 mean train loss:  1.66766695e-05, mean val. loss:  3.03043246e+00\n",
      "Epoch: 15243 mean train loss:  1.62354554e-05, mean val. loss:  3.03078842e+00\n",
      "Epoch: 15244 mean train loss:  1.67188409e-05, mean val. loss:  3.03113675e+00\n",
      "Epoch: 15245 mean train loss:  1.63982040e-05, mean val. loss:  3.03146815e+00\n",
      "Epoch: 15246 mean train loss:  1.65836536e-05, mean val. loss:  3.03174376e+00\n",
      "Epoch: 15247 mean train loss:  1.67799008e-05, mean val. loss:  3.03199244e+00\n",
      "Epoch: 15248 mean train loss:  1.66372338e-05, mean val. loss:  3.03224802e+00\n",
      "Epoch: 15249 mean train loss:  1.68696861e-05, mean val. loss:  3.03246331e+00\n",
      "Epoch: 15250 mean train loss:  1.67697435e-05, mean val. loss:  3.03263807e+00\n",
      "Epoch: 15251 mean train loss:  1.63514051e-05, mean val. loss:  3.03281808e+00\n",
      "Epoch: 15252 mean train loss:  1.64841476e-05, mean val. loss:  3.03299475e+00\n",
      "Epoch: 15253 mean train loss:  1.65135716e-05, mean val. loss:  3.03313804e+00\n",
      "Epoch: 15254 mean train loss:  1.67191611e-05, mean val. loss:  3.03327632e+00\n",
      "Epoch: 15255 mean train loss:  1.69823179e-05, mean val. loss:  3.03337383e+00\n",
      "Epoch: 15256 mean train loss:  1.66188111e-05, mean val. loss:  3.03348732e+00\n",
      "Epoch: 15257 mean train loss:  1.64196827e-05, mean val. loss:  3.03360319e+00\n",
      "Epoch: 15258 mean train loss:  1.69294653e-05, mean val. loss:  3.03366089e+00\n",
      "Epoch: 15259 mean train loss:  1.63789664e-05, mean val. loss:  3.03375292e+00\n",
      "Epoch: 15260 mean train loss:  1.65989040e-05, mean val. loss:  3.03386545e+00\n",
      "Epoch: 15261 mean train loss:  1.64631056e-05, mean val. loss:  3.03402019e+00\n",
      "Epoch: 15262 mean train loss:  1.68101105e-05, mean val. loss:  3.03416228e+00\n",
      "Epoch: 15263 mean train loss:  1.65324891e-05, mean val. loss:  3.03433013e+00\n",
      "Epoch: 15264 mean train loss:  1.65420643e-05, mean val. loss:  3.03453588e+00\n",
      "Epoch: 15265 mean train loss:  1.68540282e-05, mean val. loss:  3.03476834e+00\n",
      "Epoch: 15266 mean train loss:  1.63128716e-05, mean val. loss:  3.03506279e+00\n",
      "Epoch: 15267 mean train loss:  1.62560318e-05, mean val. loss:  3.03540421e+00\n",
      "Epoch: 15268 mean train loss:  1.65680540e-05, mean val. loss:  3.03576827e+00\n",
      "Epoch: 15269 mean train loss:  1.66078680e-05, mean val. loss:  3.03615928e+00\n",
      "Epoch: 15270 mean train loss:  1.67867693e-05, mean val. loss:  3.03654099e+00\n",
      "Epoch: 15271 mean train loss:  1.66205864e-05, mean val. loss:  3.03694773e+00\n",
      "Epoch: 15272 mean train loss:  1.67277758e-05, mean val. loss:  3.03733182e+00\n",
      "Epoch: 15273 mean train loss:  1.64355442e-05, mean val. loss:  3.03774142e+00\n",
      "Epoch: 15274 mean train loss:  1.67643593e-05, mean val. loss:  3.03812361e+00\n",
      "Epoch: 15275 mean train loss:  1.63640361e-05, mean val. loss:  3.03849745e+00\n",
      "Epoch: 15276 mean train loss:  1.64927915e-05, mean val. loss:  3.03887177e+00\n",
      "Epoch: 15277 mean train loss:  1.64645608e-05, mean val. loss:  3.03919077e+00\n",
      "Epoch: 15278 mean train loss:  1.63834775e-05, mean val. loss:  3.03951907e+00\n",
      "Epoch: 15279 mean train loss:  1.66519312e-05, mean val. loss:  3.03980136e+00\n",
      "Epoch: 15280 mean train loss:  1.65303354e-05, mean val. loss:  3.04004145e+00\n",
      "Epoch: 15281 mean train loss:  1.65640377e-05, mean val. loss:  3.04029036e+00\n",
      "Epoch: 15282 mean train loss:  1.64177618e-05, mean val. loss:  3.04055381e+00\n",
      "Epoch: 15283 mean train loss:  1.65400270e-05, mean val. loss:  3.04082274e+00\n",
      "Epoch: 15284 mean train loss:  1.67918042e-05, mean val. loss:  3.04108787e+00\n",
      "Epoch: 15285 mean train loss:  1.66386017e-05, mean val. loss:  3.04132318e+00\n",
      "Epoch: 15286 mean train loss:  1.64318189e-05, mean val. loss:  3.04154682e+00\n",
      "Epoch: 15287 mean train loss:  1.68144179e-05, mean val. loss:  3.04174376e+00\n",
      "Epoch: 15288 mean train loss:  1.66572281e-05, mean val. loss:  3.04190683e+00\n",
      "Epoch: 15289 mean train loss:  1.63870573e-05, mean val. loss:  3.04207563e+00\n",
      "Epoch: 15290 mean train loss:  1.65479723e-05, mean val. loss:  3.04225469e+00\n",
      "Epoch: 15291 mean train loss:  1.65946549e-05, mean val. loss:  3.04239655e+00\n",
      "Epoch: 15292 mean train loss:  1.65203819e-05, mean val. loss:  3.04256296e+00\n",
      "Epoch: 15293 mean train loss:  1.66680838e-05, mean val. loss:  3.04272461e+00\n",
      "Epoch: 15294 mean train loss:  1.67549588e-05, mean val. loss:  3.04284835e+00\n",
      "Epoch: 15295 mean train loss:  1.67809485e-05, mean val. loss:  3.04298639e+00\n",
      "Epoch: 15296 mean train loss:  1.64261728e-05, mean val. loss:  3.04315162e+00\n",
      "Epoch: 15297 mean train loss:  1.62645883e-05, mean val. loss:  3.04336524e+00\n",
      "Epoch: 15298 mean train loss:  1.65793754e-05, mean val. loss:  3.04361606e+00\n",
      "Epoch: 15299 mean train loss:  1.66975369e-05, mean val. loss:  3.04389238e+00\n",
      "Epoch: 15300 mean train loss:  1.66787358e-05, mean val. loss:  3.04415488e+00\n",
      "Epoch: 15301 mean train loss:  1.65563251e-05, mean val. loss:  3.04441786e+00\n",
      "Epoch: 15302 mean train loss:  1.68679690e-05, mean val. loss:  3.04465342e+00\n",
      "Epoch: 15303 mean train loss:  1.63869699e-05, mean val. loss:  3.04492760e+00\n",
      "Epoch: 15304 mean train loss:  1.66307145e-05, mean val. loss:  3.04518533e+00\n",
      "Epoch: 15305 mean train loss:  1.63666555e-05, mean val. loss:  3.04544854e+00\n",
      "Epoch: 15306 mean train loss:  1.65901729e-05, mean val. loss:  3.04567885e+00\n",
      "Epoch: 15307 mean train loss:  1.67215185e-05, mean val. loss:  3.04594421e+00\n",
      "Epoch: 15308 mean train loss:  1.64766097e-05, mean val. loss:  3.04622674e+00\n",
      "Epoch: 15309 mean train loss:  1.63959921e-05, mean val. loss:  3.04656196e+00\n",
      "Epoch: 15310 mean train loss:  1.66795799e-05, mean val. loss:  3.04689622e+00\n",
      "Epoch: 15311 mean train loss:  1.67089165e-05, mean val. loss:  3.04724431e+00\n",
      "Epoch: 15312 mean train loss:  1.65986421e-05, mean val. loss:  3.04755688e+00\n",
      "Epoch: 15313 mean train loss:  1.64308003e-05, mean val. loss:  3.04788756e+00\n",
      "Epoch: 15314 mean train loss:  1.63865916e-05, mean val. loss:  3.04827881e+00\n",
      "Epoch: 15315 mean train loss:  1.67395629e-05, mean val. loss:  3.04865479e+00\n",
      "Epoch: 15316 mean train loss:  1.64360390e-05, mean val. loss:  3.04902601e+00\n",
      "Epoch: 15317 mean train loss:  1.66027457e-05, mean val. loss:  3.04938698e+00\n",
      "Epoch: 15318 mean train loss:  1.62172655e-05, mean val. loss:  3.04976654e+00\n",
      "Epoch: 15319 mean train loss:  1.63598161e-05, mean val. loss:  3.05018616e+00\n",
      "Epoch: 15320 mean train loss:  1.66886020e-05, mean val. loss:  3.05053282e+00\n",
      "Epoch: 15321 mean train loss:  1.63575751e-05, mean val. loss:  3.05087829e+00\n",
      "Epoch: 15322 mean train loss:  1.62051874e-05, mean val. loss:  3.05123472e+00\n",
      "Epoch: 15323 mean train loss:  1.61310309e-05, mean val. loss:  3.05159712e+00\n",
      "Epoch: 15324 mean train loss:  1.66531245e-05, mean val. loss:  3.05190349e+00\n",
      "Epoch: 15325 mean train loss:  1.63062068e-05, mean val. loss:  3.05221939e+00\n",
      "Epoch: 15326 mean train loss:  1.65264937e-05, mean val. loss:  3.05253625e+00\n",
      "Epoch: 15327 mean train loss:  1.67788821e-05, mean val. loss:  3.05276155e+00\n",
      "Epoch: 15328 mean train loss:  1.65438687e-05, mean val. loss:  3.05298162e+00\n",
      "Epoch: 15329 mean train loss:  1.66443933e-05, mean val. loss:  3.05314016e+00\n",
      "Epoch: 15330 mean train loss:  1.60057680e-05, mean val. loss:  3.05333138e+00\n",
      "Epoch: 15331 mean train loss:  1.66548707e-05, mean val. loss:  3.05347848e+00\n",
      "Epoch: 15332 mean train loss:  1.68427650e-05, mean val. loss:  3.05356598e+00\n",
      "Epoch: 15333 mean train loss:  1.64732337e-05, mean val. loss:  3.05361867e+00\n",
      "Epoch: 15334 mean train loss:  1.66312675e-05, mean val. loss:  3.05362129e+00\n",
      "Epoch: 15335 mean train loss:  1.63085351e-05, mean val. loss:  3.05362344e+00\n",
      "Epoch: 15336 mean train loss:  1.64094090e-05, mean val. loss:  3.05365515e+00\n",
      "Epoch: 15337 mean train loss:  1.66263781e-05, mean val. loss:  3.05361962e+00\n",
      "Epoch: 15338 mean train loss:  1.62938086e-05, mean val. loss:  3.05361223e+00\n",
      "Epoch: 15339 mean train loss:  1.67253893e-05, mean val. loss:  3.05364656e+00\n",
      "Epoch: 15340 mean train loss:  1.63320510e-05, mean val. loss:  3.05374169e+00\n",
      "Epoch: 15341 mean train loss:  1.67048420e-05, mean val. loss:  3.05383301e+00\n",
      "Epoch: 15342 mean train loss:  1.64466328e-05, mean val. loss:  3.05394363e+00\n",
      "Epoch: 15343 mean train loss:  1.63857185e-05, mean val. loss:  3.05409813e+00\n",
      "Epoch: 15344 mean train loss:  1.68529805e-05, mean val. loss:  3.05430055e+00\n",
      "Epoch: 15345 mean train loss:  1.65248639e-05, mean val. loss:  3.05457664e+00\n",
      "Epoch: 15346 mean train loss:  1.61884818e-05, mean val. loss:  3.05488825e+00\n",
      "Epoch: 15347 mean train loss:  1.65370293e-05, mean val. loss:  3.05519295e+00\n",
      "Epoch: 15348 mean train loss:  1.66724203e-05, mean val. loss:  3.05553651e+00\n",
      "Epoch: 15349 mean train loss:  1.66025420e-05, mean val. loss:  3.05590987e+00\n",
      "Epoch: 15350 mean train loss:  1.65417150e-05, mean val. loss:  3.05632639e+00\n",
      "Epoch: 15351 mean train loss:  1.66441605e-05, mean val. loss:  3.05675340e+00\n",
      "Epoch: 15352 mean train loss:  1.66748068e-05, mean val. loss:  3.05718637e+00\n",
      "Epoch: 15353 mean train loss:  1.63926452e-05, mean val. loss:  3.05767345e+00\n",
      "Epoch: 15354 mean train loss:  1.67698890e-05, mean val. loss:  3.05813503e+00\n",
      "Epoch: 15355 mean train loss:  1.63678778e-05, mean val. loss:  3.05860877e+00\n",
      "Epoch: 15356 mean train loss:  1.66870013e-05, mean val. loss:  3.05904865e+00\n",
      "Epoch: 15357 mean train loss:  1.66421523e-05, mean val. loss:  3.05944276e+00\n",
      "Epoch: 15358 mean train loss:  1.66949758e-05, mean val. loss:  3.05981636e+00\n",
      "Epoch: 15359 mean train loss:  1.63382501e-05, mean val. loss:  3.06020021e+00\n",
      "Epoch: 15360 mean train loss:  1.63975928e-05, mean val. loss:  3.06059384e+00\n",
      "Epoch: 15361 mean train loss:  1.66304526e-05, mean val. loss:  3.06095743e+00\n",
      "Epoch: 15362 mean train loss:  1.66388636e-05, mean val. loss:  3.06130052e+00\n",
      "Epoch: 15363 mean train loss:  1.66587415e-05, mean val. loss:  3.06160116e+00\n",
      "Epoch: 15364 mean train loss:  1.64709345e-05, mean val. loss:  3.06187201e+00\n",
      "Epoch: 15365 mean train loss:  1.64782978e-05, mean val. loss:  3.06214046e+00\n",
      "Epoch: 15366 mean train loss:  1.66725367e-05, mean val. loss:  3.06238818e+00\n",
      "Epoch: 15367 mean train loss:  1.66352838e-05, mean val. loss:  3.06258249e+00\n",
      "Epoch: 15368 mean train loss:  1.65273959e-05, mean val. loss:  3.06280160e+00\n",
      "Epoch: 15369 mean train loss:  1.64473313e-05, mean val. loss:  3.06301069e+00\n",
      "Epoch: 15370 mean train loss:  1.61962525e-05, mean val. loss:  3.06325769e+00\n",
      "Epoch: 15371 mean train loss:  1.65604288e-05, mean val. loss:  3.06353283e+00\n",
      "Epoch: 15372 mean train loss:  1.65906968e-05, mean val. loss:  3.06379128e+00\n",
      "Epoch: 15373 mean train loss:  1.65934325e-05, mean val. loss:  3.06399131e+00\n",
      "Epoch: 15374 mean train loss:  1.63926743e-05, mean val. loss:  3.06422901e+00\n",
      "Epoch: 15375 mean train loss:  1.67582766e-05, mean val. loss:  3.06444693e+00\n",
      "Epoch: 15376 mean train loss:  1.66349928e-05, mean val. loss:  3.06465459e+00\n",
      "Epoch: 15377 mean train loss:  1.65323727e-05, mean val. loss:  3.06483221e+00\n",
      "Epoch: 15378 mean train loss:  1.64407829e-05, mean val. loss:  3.06502557e+00\n",
      "Epoch: 15379 mean train loss:  1.68032420e-05, mean val. loss:  3.06517196e+00\n",
      "Epoch: 15380 mean train loss:  1.63329532e-05, mean val. loss:  3.06530428e+00\n",
      "Epoch: 15381 mean train loss:  1.64168305e-05, mean val. loss:  3.06546617e+00\n",
      "Epoch: 15382 mean train loss:  1.64661033e-05, mean val. loss:  3.06566381e+00\n",
      "Epoch: 15383 mean train loss:  1.63468067e-05, mean val. loss:  3.06590533e+00\n",
      "Epoch: 15384 mean train loss:  1.68246916e-05, mean val. loss:  3.06615305e+00\n",
      "Epoch: 15385 mean train loss:  1.62451470e-05, mean val. loss:  3.06639981e+00\n",
      "Epoch: 15386 mean train loss:  1.60556519e-05, mean val. loss:  3.06666565e+00\n",
      "Epoch: 15387 mean train loss:  1.66776299e-05, mean val. loss:  3.06693292e+00\n",
      "Epoch: 15388 mean train loss:  1.64900557e-05, mean val. loss:  3.06718922e+00\n",
      "Epoch: 15389 mean train loss:  1.64172670e-05, mean val. loss:  3.06746674e+00\n",
      "Epoch: 15390 mean train loss:  1.64766097e-05, mean val. loss:  3.06775260e+00\n",
      "Epoch: 15391 mean train loss:  1.67429971e-05, mean val. loss:  3.06807375e+00\n",
      "Epoch: 15392 mean train loss:  1.62527431e-05, mean val. loss:  3.06838250e+00\n",
      "Epoch: 15393 mean train loss:  1.63239893e-05, mean val. loss:  3.06869459e+00\n",
      "Epoch: 15394 mean train loss:  1.66910177e-05, mean val. loss:  3.06896544e+00\n",
      "Epoch: 15395 mean train loss:  1.67453836e-05, mean val. loss:  3.06922221e+00\n",
      "Epoch: 15396 mean train loss:  1.67055114e-05, mean val. loss:  3.06944108e+00\n",
      "Epoch: 15397 mean train loss:  1.65789970e-05, mean val. loss:  3.06964779e+00\n",
      "Epoch: 15398 mean train loss:  1.63891818e-05, mean val. loss:  3.06983209e+00\n",
      "Epoch: 15399 mean train loss:  1.67262915e-05, mean val. loss:  3.06997609e+00\n",
      "Epoch: 15400 mean train loss:  1.63857476e-05, mean val. loss:  3.07008958e+00\n",
      "Epoch: 15401 mean train loss:  1.67305698e-05, mean val. loss:  3.07014298e+00\n",
      "Epoch: 15402 mean train loss:  1.64909288e-05, mean val. loss:  3.07017231e+00\n",
      "Epoch: 15403 mean train loss:  1.64908997e-05, mean val. loss:  3.07021165e+00\n",
      "Epoch: 15404 mean train loss:  1.64328085e-05, mean val. loss:  3.07025385e+00\n",
      "Epoch: 15405 mean train loss:  1.67184917e-05, mean val. loss:  3.07029891e+00\n",
      "Epoch: 15406 mean train loss:  1.68172992e-05, mean val. loss:  3.07033968e+00\n",
      "Epoch: 15407 mean train loss:  1.63293153e-05, mean val. loss:  3.07044315e+00\n",
      "Epoch: 15408 mean train loss:  1.60681666e-05, mean val. loss:  3.07060909e+00\n",
      "Epoch: 15409 mean train loss:  1.66129030e-05, mean val. loss:  3.07078791e+00\n",
      "Epoch: 15410 mean train loss:  1.64229423e-05, mean val. loss:  3.07104754e+00\n",
      "Epoch: 15411 mean train loss:  1.64709927e-05, mean val. loss:  3.07140088e+00\n",
      "Epoch: 15412 mean train loss:  1.63373188e-05, mean val. loss:  3.07180548e+00\n",
      "Epoch: 15413 mean train loss:  1.64206140e-05, mean val. loss:  3.07225680e+00\n",
      "Epoch: 15414 mean train loss:  1.66762329e-05, mean val. loss:  3.07273889e+00\n",
      "Epoch: 15415 mean train loss:  1.65003585e-05, mean val. loss:  3.07327151e+00\n",
      "Epoch: 15416 mean train loss:  1.66473619e-05, mean val. loss:  3.07385087e+00\n",
      "Epoch: 15417 mean train loss:  1.65565580e-05, mean val. loss:  3.07445097e+00\n",
      "Epoch: 15418 mean train loss:  1.64361554e-05, mean val. loss:  3.07508636e+00\n",
      "Epoch: 15419 mean train loss:  1.65248930e-05, mean val. loss:  3.07569528e+00\n",
      "Epoch: 15420 mean train loss:  1.61994831e-05, mean val. loss:  3.07627439e+00\n",
      "Epoch: 15421 mean train loss:  1.62996002e-05, mean val. loss:  3.07684827e+00\n",
      "Epoch: 15422 mean train loss:  1.63419463e-05, mean val. loss:  3.07741666e+00\n",
      "Epoch: 15423 mean train loss:  1.64580415e-05, mean val. loss:  3.07794547e+00\n",
      "Epoch: 15424 mean train loss:  1.65180536e-05, mean val. loss:  3.07839608e+00\n",
      "Epoch: 15425 mean train loss:  1.65183155e-05, mean val. loss:  3.07878566e+00\n",
      "Epoch: 15426 mean train loss:  1.63922086e-05, mean val. loss:  3.07914329e+00\n",
      "Epoch: 15427 mean train loss:  1.63590885e-05, mean val. loss:  3.07946229e+00\n",
      "Epoch: 15428 mean train loss:  1.65383681e-05, mean val. loss:  3.07974792e+00\n",
      "Epoch: 15429 mean train loss:  1.61098724e-05, mean val. loss:  3.08001399e+00\n",
      "Epoch: 15430 mean train loss:  1.64990197e-05, mean val. loss:  3.08021450e+00\n",
      "Epoch: 15431 mean train loss:  1.63616787e-05, mean val. loss:  3.08041358e+00\n",
      "Epoch: 15432 mean train loss:  1.63821678e-05, mean val. loss:  3.08057618e+00\n",
      "Epoch: 15433 mean train loss:  1.66975369e-05, mean val. loss:  3.08068252e+00\n",
      "Epoch: 15434 mean train loss:  1.66227401e-05, mean val. loss:  3.08074307e+00\n",
      "Epoch: 15435 mean train loss:  1.63009972e-05, mean val. loss:  3.08079863e+00\n",
      "Epoch: 15436 mean train loss:  1.65599049e-05, mean val. loss:  3.08086324e+00\n",
      "Epoch: 15437 mean train loss:  1.63913064e-05, mean val. loss:  3.08090281e+00\n",
      "Epoch: 15438 mean train loss:  1.62777142e-05, mean val. loss:  3.08095074e+00\n",
      "Epoch: 15439 mean train loss:  1.64110970e-05, mean val. loss:  3.08095908e+00\n",
      "Epoch: 15440 mean train loss:  1.64498924e-05, mean val. loss:  3.08098316e+00\n",
      "Epoch: 15441 mean train loss:  1.64636585e-05, mean val. loss:  3.08102560e+00\n",
      "Epoch: 15442 mean train loss:  1.62020442e-05, mean val. loss:  3.08105707e+00\n",
      "Epoch: 15443 mean train loss:  1.65510573e-05, mean val. loss:  3.08110166e+00\n",
      "Epoch: 15444 mean train loss:  1.63491932e-05, mean val. loss:  3.08115149e+00\n",
      "Epoch: 15445 mean train loss:  1.64833327e-05, mean val. loss:  3.08119869e+00\n",
      "Epoch: 15446 mean train loss:  1.61486678e-05, mean val. loss:  3.08126354e+00\n",
      "Epoch: 15447 mean train loss:  1.63405493e-05, mean val. loss:  3.08136988e+00\n",
      "Epoch: 15448 mean train loss:  1.63769873e-05, mean val. loss:  3.08147621e+00\n",
      "Epoch: 15449 mean train loss:  1.62372307e-05, mean val. loss:  3.08166552e+00\n",
      "Epoch: 15450 mean train loss:  1.64211378e-05, mean val. loss:  3.08185577e+00\n",
      "Epoch: 15451 mean train loss:  1.62536453e-05, mean val. loss:  3.08204508e+00\n",
      "Epoch: 15452 mean train loss:  1.67491089e-05, mean val. loss:  3.08224320e+00\n",
      "Epoch: 15453 mean train loss:  1.62894430e-05, mean val. loss:  3.08247447e+00\n",
      "Epoch: 15454 mean train loss:  1.62220385e-05, mean val. loss:  3.08273435e+00\n",
      "Epoch: 15455 mean train loss:  1.64119410e-05, mean val. loss:  3.08295321e+00\n",
      "Epoch: 15456 mean train loss:  1.63206714e-05, mean val. loss:  3.08321381e+00\n",
      "Epoch: 15457 mean train loss:  1.62021315e-05, mean val. loss:  3.08349776e+00\n",
      "Epoch: 15458 mean train loss:  1.63663353e-05, mean val. loss:  3.08375216e+00\n",
      "Epoch: 15459 mean train loss:  1.65703823e-05, mean val. loss:  3.08399940e+00\n",
      "Epoch: 15460 mean train loss:  1.65422971e-05, mean val. loss:  3.08421826e+00\n",
      "Epoch: 15461 mean train loss:  1.64048688e-05, mean val. loss:  3.08440924e+00\n",
      "Epoch: 15462 mean train loss:  1.65596139e-05, mean val. loss:  3.08460212e+00\n",
      "Epoch: 15463 mean train loss:  1.67789985e-05, mean val. loss:  3.08478546e+00\n",
      "Epoch: 15464 mean train loss:  1.64506491e-05, mean val. loss:  3.08502126e+00\n",
      "Epoch: 15465 mean train loss:  1.66447135e-05, mean val. loss:  3.08523083e+00\n",
      "Epoch: 15466 mean train loss:  1.65371748e-05, mean val. loss:  3.08542228e+00\n",
      "Epoch: 15467 mean train loss:  1.67481485e-05, mean val. loss:  3.08563566e+00\n",
      "Epoch: 15468 mean train loss:  1.67403778e-05, mean val. loss:  3.08583117e+00\n",
      "Epoch: 15469 mean train loss:  1.66042882e-05, mean val. loss:  3.08602047e+00\n",
      "Epoch: 15470 mean train loss:  1.64950907e-05, mean val. loss:  3.08624172e+00\n",
      "Epoch: 15471 mean train loss:  1.63064396e-05, mean val. loss:  3.08651257e+00\n",
      "Epoch: 15472 mean train loss:  1.62646757e-05, mean val. loss:  3.08680844e+00\n",
      "Epoch: 15473 mean train loss:  1.66051905e-05, mean val. loss:  3.08713579e+00\n",
      "Epoch: 15474 mean train loss:  1.63247460e-05, mean val. loss:  3.08745337e+00\n",
      "Epoch: 15475 mean train loss:  1.64842641e-05, mean val. loss:  3.08780098e+00\n",
      "Epoch: 15476 mean train loss:  1.63857767e-05, mean val. loss:  3.08811235e+00\n",
      "Epoch: 15477 mean train loss:  1.62167416e-05, mean val. loss:  3.08846998e+00\n",
      "Epoch: 15478 mean train loss:  1.65312376e-05, mean val. loss:  3.08883858e+00\n",
      "Epoch: 15479 mean train loss:  1.64670346e-05, mean val. loss:  3.08925033e+00\n",
      "Epoch: 15480 mean train loss:  1.60607742e-05, mean val. loss:  3.08968878e+00\n",
      "Epoch: 15481 mean train loss:  1.63952645e-05, mean val. loss:  3.09014559e+00\n",
      "Epoch: 15482 mean train loss:  1.63876684e-05, mean val. loss:  3.09059405e+00\n",
      "Epoch: 15483 mean train loss:  1.66442769e-05, mean val. loss:  3.09099960e+00\n",
      "Epoch: 15484 mean train loss:  1.61228527e-05, mean val. loss:  3.09142804e+00\n",
      "Epoch: 15485 mean train loss:  1.65195961e-05, mean val. loss:  3.09183407e+00\n",
      "Epoch: 15486 mean train loss:  1.65074598e-05, mean val. loss:  3.09223104e+00\n",
      "Epoch: 15487 mean train loss:  1.62689539e-05, mean val. loss:  3.09262872e+00\n",
      "Epoch: 15488 mean train loss:  1.62486685e-05, mean val. loss:  3.09300494e+00\n",
      "Epoch: 15489 mean train loss:  1.65287056e-05, mean val. loss:  3.09338975e+00\n",
      "Epoch: 15490 mean train loss:  1.61596108e-05, mean val. loss:  3.09375906e+00\n",
      "Epoch: 15491 mean train loss:  1.62858050e-05, mean val. loss:  3.09411001e+00\n",
      "Epoch: 15492 mean train loss:  1.63528603e-05, mean val. loss:  3.09445548e+00\n",
      "Epoch: 15493 mean train loss:  1.65454694e-05, mean val. loss:  3.09476042e+00\n",
      "Epoch: 15494 mean train loss:  1.61448261e-05, mean val. loss:  3.09507775e+00\n",
      "Epoch: 15495 mean train loss:  1.61642674e-05, mean val. loss:  3.09538865e+00\n",
      "Epoch: 15496 mean train loss:  1.64123485e-05, mean val. loss:  3.09569287e+00\n",
      "Epoch: 15497 mean train loss:  1.65169477e-05, mean val. loss:  3.09596705e+00\n",
      "Epoch: 15498 mean train loss:  1.63748045e-05, mean val. loss:  3.09624267e+00\n",
      "Epoch: 15499 mean train loss:  1.61984644e-05, mean val. loss:  3.09650326e+00\n",
      "Epoch: 15500 mean train loss:  1.63195946e-05, mean val. loss:  3.09678173e+00\n",
      "Epoch: 15501 mean train loss:  1.66089158e-05, mean val. loss:  3.09703493e+00\n",
      "Epoch: 15502 mean train loss:  1.63069053e-05, mean val. loss:  3.09728932e+00\n",
      "Epoch: 15503 mean train loss:  1.64344674e-05, mean val. loss:  3.09751463e+00\n",
      "Epoch: 15504 mean train loss:  1.63012883e-05, mean val. loss:  3.09773159e+00\n",
      "Epoch: 15505 mean train loss:  1.63838267e-05, mean val. loss:  3.09799051e+00\n",
      "Epoch: 15506 mean train loss:  1.65770762e-05, mean val. loss:  3.09821534e+00\n",
      "Epoch: 15507 mean train loss:  1.67555118e-05, mean val. loss:  3.09841561e+00\n",
      "Epoch: 15508 mean train loss:  1.65211677e-05, mean val. loss:  3.09856844e+00\n",
      "Epoch: 15509 mean train loss:  1.62246870e-05, mean val. loss:  3.09869432e+00\n",
      "Epoch: 15510 mean train loss:  1.61082426e-05, mean val. loss:  3.09882402e+00\n",
      "Epoch: 15511 mean train loss:  1.62478536e-05, mean val. loss:  3.09895301e+00\n",
      "Epoch: 15512 mean train loss:  1.61945936e-05, mean val. loss:  3.09906530e+00\n",
      "Epoch: 15513 mean train loss:  1.67520484e-05, mean val. loss:  3.09914136e+00\n",
      "Epoch: 15514 mean train loss:  1.63474178e-05, mean val. loss:  3.09920025e+00\n",
      "Epoch: 15515 mean train loss:  1.66248647e-05, mean val. loss:  3.09924245e+00\n",
      "Epoch: 15516 mean train loss:  1.65135716e-05, mean val. loss:  3.09929514e+00\n",
      "Epoch: 15517 mean train loss:  1.64948869e-05, mean val. loss:  3.09932685e+00\n",
      "Epoch: 15518 mean train loss:  1.63139775e-05, mean val. loss:  3.09940314e+00\n",
      "Epoch: 15519 mean train loss:  1.63759105e-05, mean val. loss:  3.09950185e+00\n",
      "Epoch: 15520 mean train loss:  1.66478858e-05, mean val. loss:  3.09956050e+00\n",
      "Epoch: 15521 mean train loss:  1.63650548e-05, mean val. loss:  3.09960699e+00\n",
      "Epoch: 15522 mean train loss:  1.66766404e-05, mean val. loss:  3.09963346e+00\n",
      "Epoch: 15523 mean train loss:  1.66113605e-05, mean val. loss:  3.09965801e+00\n",
      "Epoch: 15524 mean train loss:  1.63660443e-05, mean val. loss:  3.09970522e+00\n",
      "Epoch: 15525 mean train loss:  1.63341174e-05, mean val. loss:  3.09984207e+00\n",
      "Epoch: 15526 mean train loss:  1.64960511e-05, mean val. loss:  3.09998608e+00\n",
      "Epoch: 15527 mean train loss:  1.62238721e-05, mean val. loss:  3.10015798e+00\n",
      "Epoch: 15528 mean train loss:  1.65350502e-05, mean val. loss:  3.10032034e+00\n",
      "Epoch: 15529 mean train loss:  1.62270444e-05, mean val. loss:  3.10052371e+00\n",
      "Epoch: 15530 mean train loss:  1.64478261e-05, mean val. loss:  3.10070205e+00\n",
      "Epoch: 15531 mean train loss:  1.63789664e-05, mean val. loss:  3.10092545e+00\n",
      "Epoch: 15532 mean train loss:  1.64848461e-05, mean val. loss:  3.10118365e+00\n",
      "Epoch: 15533 mean train loss:  1.63583318e-05, mean val. loss:  3.10151553e+00\n",
      "Epoch: 15534 mean train loss:  1.64568191e-05, mean val. loss:  3.10184598e+00\n",
      "Epoch: 15535 mean train loss:  1.63424411e-05, mean val. loss:  3.10218883e+00\n",
      "Epoch: 15536 mean train loss:  1.60830386e-05, mean val. loss:  3.10253739e+00\n",
      "Epoch: 15537 mean train loss:  1.62115612e-05, mean val. loss:  3.10289359e+00\n",
      "Epoch: 15538 mean train loss:  1.66308309e-05, mean val. loss:  3.10325170e+00\n",
      "Epoch: 15539 mean train loss:  1.62801298e-05, mean val. loss:  3.10362673e+00\n",
      "Epoch: 15540 mean train loss:  1.63887162e-05, mean val. loss:  3.10396719e+00\n",
      "Epoch: 15541 mean train loss:  1.65604288e-05, mean val. loss:  3.10431814e+00\n",
      "Epoch: 15542 mean train loss:  1.59853080e-05, mean val. loss:  3.10470724e+00\n",
      "Epoch: 15543 mean train loss:  1.60264026e-05, mean val. loss:  3.10514712e+00\n",
      "Epoch: 15544 mean train loss:  1.64657249e-05, mean val. loss:  3.10558939e+00\n",
      "Epoch: 15545 mean train loss:  1.64305675e-05, mean val. loss:  3.10604239e+00\n",
      "Epoch: 15546 mean train loss:  1.59757619e-05, mean val. loss:  3.10653114e+00\n",
      "Epoch: 15547 mean train loss:  1.64888334e-05, mean val. loss:  3.10700464e+00\n",
      "Epoch: 15548 mean train loss:  1.64324883e-05, mean val. loss:  3.10751534e+00\n",
      "Epoch: 15549 mean train loss:  1.67080434e-05, mean val. loss:  3.10798669e+00\n",
      "Epoch: 15550 mean train loss:  1.64303638e-05, mean val. loss:  3.10842180e+00\n",
      "Epoch: 15551 mean train loss:  1.62969227e-05, mean val. loss:  3.10891318e+00\n",
      "Epoch: 15552 mean train loss:  1.64707017e-05, mean val. loss:  3.10936952e+00\n",
      "Epoch: 15553 mean train loss:  1.64102530e-05, mean val. loss:  3.10980082e+00\n",
      "Epoch: 15554 mean train loss:  1.63681107e-05, mean val. loss:  3.11018085e+00\n",
      "Epoch: 15555 mean train loss:  1.61298085e-05, mean val. loss:  3.11056900e+00\n",
      "Epoch: 15556 mean train loss:  1.65619713e-05, mean val. loss:  3.11096454e+00\n",
      "Epoch: 15557 mean train loss:  1.62769866e-05, mean val. loss:  3.11136723e+00\n",
      "Epoch: 15558 mean train loss:  1.64500088e-05, mean val. loss:  3.11174226e+00\n",
      "Epoch: 15559 mean train loss:  1.60900818e-05, mean val. loss:  3.11214447e+00\n",
      "Epoch: 15560 mean train loss:  1.65627571e-05, mean val. loss:  3.11248755e+00\n",
      "Epoch: 15561 mean train loss:  1.63773657e-05, mean val. loss:  3.11280560e+00\n",
      "Epoch: 15562 mean train loss:  1.62366196e-05, mean val. loss:  3.11314631e+00\n",
      "Epoch: 15563 mean train loss:  1.61186035e-05, mean val. loss:  3.11351585e+00\n",
      "Epoch: 15564 mean train loss:  1.61163043e-05, mean val. loss:  3.11389160e+00\n",
      "Epoch: 15565 mean train loss:  1.63582154e-05, mean val. loss:  3.11422062e+00\n",
      "Epoch: 15566 mean train loss:  1.63327495e-05, mean val. loss:  3.11448908e+00\n",
      "Epoch: 15567 mean train loss:  1.63037621e-05, mean val. loss:  3.11474729e+00\n",
      "Epoch: 15568 mean train loss:  1.65844685e-05, mean val. loss:  3.11492085e+00\n",
      "Epoch: 15569 mean train loss:  1.65452948e-05, mean val. loss:  3.11505604e+00\n",
      "Epoch: 15570 mean train loss:  1.62800425e-05, mean val. loss:  3.11517453e+00\n",
      "Epoch: 15571 mean train loss:  1.64120574e-05, mean val. loss:  3.11528611e+00\n",
      "Epoch: 15572 mean train loss:  1.60210475e-05, mean val. loss:  3.11540914e+00\n",
      "Epoch: 15573 mean train loss:  1.63241930e-05, mean val. loss:  3.11553097e+00\n",
      "Epoch: 15574 mean train loss:  1.63492514e-05, mean val. loss:  3.11560822e+00\n",
      "Epoch: 15575 mean train loss:  1.61806529e-05, mean val. loss:  3.11568427e+00\n",
      "Epoch: 15576 mean train loss:  1.65778329e-05, mean val. loss:  3.11576676e+00\n",
      "Epoch: 15577 mean train loss:  1.66203536e-05, mean val. loss:  3.11581945e+00\n",
      "Epoch: 15578 mean train loss:  1.63213990e-05, mean val. loss:  3.11588073e+00\n",
      "Epoch: 15579 mean train loss:  1.65770471e-05, mean val. loss:  3.11594033e+00\n",
      "Epoch: 15580 mean train loss:  1.64296362e-05, mean val. loss:  3.11594725e+00\n",
      "Epoch: 15581 mean train loss:  1.65216625e-05, mean val. loss:  3.11593270e+00\n",
      "Epoch: 15582 mean train loss:  1.65463716e-05, mean val. loss:  3.11591148e+00\n",
      "Epoch: 15583 mean train loss:  1.62871729e-05, mean val. loss:  3.11591887e+00\n",
      "Epoch: 15584 mean train loss:  1.65012316e-05, mean val. loss:  3.11589718e+00\n",
      "Epoch: 15585 mean train loss:  1.65388337e-05, mean val. loss:  3.11589050e+00\n",
      "Epoch: 15586 mean train loss:  1.64945959e-05, mean val. loss:  3.11585593e+00\n",
      "Epoch: 15587 mean train loss:  1.62224751e-05, mean val. loss:  3.11582828e+00\n",
      "Epoch: 15588 mean train loss:  1.61801581e-05, mean val. loss:  3.11580467e+00\n",
      "Epoch: 15589 mean train loss:  1.60570489e-05, mean val. loss:  3.11584640e+00\n",
      "Epoch: 15590 mean train loss:  1.60642085e-05, mean val. loss:  3.11592460e+00\n",
      "Epoch: 15591 mean train loss:  1.63965742e-05, mean val. loss:  3.11604142e+00\n",
      "Epoch: 15592 mean train loss:  1.65226520e-05, mean val. loss:  3.11619902e+00\n",
      "Epoch: 15593 mean train loss:  1.67169492e-05, mean val. loss:  3.11636877e+00\n",
      "Epoch: 15594 mean train loss:  1.63957302e-05, mean val. loss:  3.11656761e+00\n",
      "Epoch: 15595 mean train loss:  1.63381337e-05, mean val. loss:  3.11679649e+00\n",
      "Epoch: 15596 mean train loss:  1.64488447e-05, mean val. loss:  3.11708474e+00\n",
      "Epoch: 15597 mean train loss:  1.63764926e-05, mean val. loss:  3.11740899e+00\n",
      "Epoch: 15598 mean train loss:  1.61134521e-05, mean val. loss:  3.11780190e+00\n",
      "Epoch: 15599 mean train loss:  1.62936631e-05, mean val. loss:  3.11821055e+00\n",
      "Epoch: 15600 mean train loss:  1.65451784e-05, mean val. loss:  3.11864400e+00\n",
      "Epoch: 15601 mean train loss:  1.62821671e-05, mean val. loss:  3.11911726e+00\n",
      "Epoch: 15602 mean train loss:  1.63882214e-05, mean val. loss:  3.11959338e+00\n",
      "Epoch: 15603 mean train loss:  1.62972719e-05, mean val. loss:  3.12006187e+00\n",
      "Epoch: 15604 mean train loss:  1.63554796e-05, mean val. loss:  3.12050319e+00\n",
      "Epoch: 15605 mean train loss:  1.61512871e-05, mean val. loss:  3.12094450e+00\n",
      "Epoch: 15606 mean train loss:  1.66090613e-05, mean val. loss:  3.12131786e+00\n",
      "Epoch: 15607 mean train loss:  1.63152872e-05, mean val. loss:  3.12164307e+00\n",
      "Epoch: 15608 mean train loss:  1.64695957e-05, mean val. loss:  3.12193799e+00\n",
      "Epoch: 15609 mean train loss:  1.63679069e-05, mean val. loss:  3.12223554e+00\n",
      "Epoch: 15610 mean train loss:  1.63414516e-05, mean val. loss:  3.12248945e+00\n",
      "Epoch: 15611 mean train loss:  1.60725322e-05, mean val. loss:  3.12277579e+00\n",
      "Epoch: 15612 mean train loss:  1.63722434e-05, mean val. loss:  3.12306094e+00\n",
      "Epoch: 15613 mean train loss:  1.62226788e-05, mean val. loss:  3.12333369e+00\n",
      "Epoch: 15614 mean train loss:  1.62539363e-05, mean val. loss:  3.12362480e+00\n",
      "Epoch: 15615 mean train loss:  1.64057710e-05, mean val. loss:  3.12395835e+00\n",
      "Epoch: 15616 mean train loss:  1.65899401e-05, mean val. loss:  3.12426949e+00\n",
      "Epoch: 15617 mean train loss:  1.62628712e-05, mean val. loss:  3.12460613e+00\n",
      "Epoch: 15618 mean train loss:  1.61502394e-05, mean val. loss:  3.12502027e+00\n",
      "Epoch: 15619 mean train loss:  1.62659853e-05, mean val. loss:  3.12549090e+00\n",
      "Epoch: 15620 mean train loss:  1.64803350e-05, mean val. loss:  3.12596464e+00\n",
      "Epoch: 15621 mean train loss:  1.65632518e-05, mean val. loss:  3.12641788e+00\n",
      "Epoch: 15622 mean train loss:  1.63828663e-05, mean val. loss:  3.12688971e+00\n",
      "Epoch: 15623 mean train loss:  1.61779171e-05, mean val. loss:  3.12736607e+00\n",
      "Epoch: 15624 mean train loss:  1.65064703e-05, mean val. loss:  3.12780476e+00\n",
      "Epoch: 15625 mean train loss:  1.65368547e-05, mean val. loss:  3.12817526e+00\n",
      "Epoch: 15626 mean train loss:  1.61624630e-05, mean val. loss:  3.12849021e+00\n",
      "Epoch: 15627 mean train loss:  1.62463693e-05, mean val. loss:  3.12877321e+00\n",
      "Epoch: 15628 mean train loss:  1.64806261e-05, mean val. loss:  3.12900472e+00\n",
      "Epoch: 15629 mean train loss:  1.61933131e-05, mean val. loss:  3.12919331e+00\n",
      "Epoch: 15630 mean train loss:  1.63754448e-05, mean val. loss:  3.12932444e+00\n",
      "Epoch: 15631 mean train loss:  1.60260533e-05, mean val. loss:  3.12943339e+00\n",
      "Epoch: 15632 mean train loss:  1.63884251e-05, mean val. loss:  3.12952566e+00\n",
      "Epoch: 15633 mean train loss:  1.63752120e-05, mean val. loss:  3.12958598e+00\n",
      "Epoch: 15634 mean train loss:  1.63233490e-05, mean val. loss:  3.12966681e+00\n",
      "Epoch: 15635 mean train loss:  1.58750045e-05, mean val. loss:  3.12981105e+00\n",
      "Epoch: 15636 mean train loss:  1.63003861e-05, mean val. loss:  3.12996411e+00\n",
      "Epoch: 15637 mean train loss:  1.63574005e-05, mean val. loss:  3.13010478e+00\n",
      "Epoch: 15638 mean train loss:  1.62510842e-05, mean val. loss:  3.13034034e+00\n",
      "Epoch: 15639 mean train loss:  1.64000085e-05, mean val. loss:  3.13055563e+00\n",
      "Epoch: 15640 mean train loss:  1.63695076e-05, mean val. loss:  3.13074207e+00\n",
      "Epoch: 15641 mean train loss:  1.66228856e-05, mean val. loss:  3.13092041e+00\n",
      "Epoch: 15642 mean train loss:  1.62629585e-05, mean val. loss:  3.13115692e+00\n",
      "Epoch: 15643 mean train loss:  1.65786478e-05, mean val. loss:  3.13137412e+00\n",
      "Epoch: 15644 mean train loss:  1.61556236e-05, mean val. loss:  3.13163590e+00\n",
      "Epoch: 15645 mean train loss:  1.63255900e-05, mean val. loss:  3.13186097e+00\n",
      "Epoch: 15646 mean train loss:  1.64366793e-05, mean val. loss:  3.13204670e+00\n",
      "Epoch: 15647 mean train loss:  1.60239870e-05, mean val. loss:  3.13222551e+00\n",
      "Epoch: 15648 mean train loss:  1.65135716e-05, mean val. loss:  3.13237286e+00\n",
      "Epoch: 15649 mean train loss:  1.62094366e-05, mean val. loss:  3.13247800e+00\n",
      "Epoch: 15650 mean train loss:  1.62607175e-05, mean val. loss:  3.13256001e+00\n",
      "Epoch: 15651 mean train loss:  1.63294899e-05, mean val. loss:  3.13258195e+00\n",
      "Epoch: 15652 mean train loss:  1.60935742e-05, mean val. loss:  3.13262224e+00\n",
      "Epoch: 15653 mean train loss:  1.61808275e-05, mean val. loss:  3.13268065e+00\n",
      "Epoch: 15654 mean train loss:  1.64046942e-05, mean val. loss:  3.13277316e+00\n",
      "Epoch: 15655 mean train loss:  1.62336219e-05, mean val. loss:  3.13289142e+00\n",
      "Epoch: 15656 mean train loss:  1.63185177e-05, mean val. loss:  3.13307548e+00\n",
      "Epoch: 15657 mean train loss:  1.62812357e-05, mean val. loss:  3.13329792e+00\n",
      "Epoch: 15658 mean train loss:  1.65108067e-05, mean val. loss:  3.13355446e+00\n",
      "Epoch: 15659 mean train loss:  1.64615340e-05, mean val. loss:  3.13380170e+00\n",
      "Epoch: 15660 mean train loss:  1.59794872e-05, mean val. loss:  3.13415456e+00\n",
      "Epoch: 15661 mean train loss:  1.65412785e-05, mean val. loss:  3.13450599e+00\n",
      "Epoch: 15662 mean train loss:  1.63811492e-05, mean val. loss:  3.13490558e+00\n",
      "Epoch: 15663 mean train loss:  1.64099620e-05, mean val. loss:  3.13533401e+00\n",
      "Epoch: 15664 mean train loss:  1.61358621e-05, mean val. loss:  3.13582563e+00\n",
      "Epoch: 15665 mean train loss:  1.61513744e-05, mean val. loss:  3.13635874e+00\n",
      "Epoch: 15666 mean train loss:  1.66764949e-05, mean val. loss:  3.13684916e+00\n",
      "Epoch: 15667 mean train loss:  1.61527423e-05, mean val. loss:  3.13731027e+00\n",
      "Epoch: 15668 mean train loss:  1.62034412e-05, mean val. loss:  3.13774300e+00\n",
      "Epoch: 15669 mean train loss:  1.61002972e-05, mean val. loss:  3.13816905e+00\n",
      "Epoch: 15670 mean train loss:  1.62126380e-05, mean val. loss:  3.13856816e+00\n",
      "Epoch: 15671 mean train loss:  1.63859513e-05, mean val. loss:  3.13889265e+00\n",
      "Epoch: 15672 mean train loss:  1.61904900e-05, mean val. loss:  3.13919091e+00\n",
      "Epoch: 15673 mean train loss:  1.61722710e-05, mean val. loss:  3.13945341e+00\n",
      "Epoch: 15674 mean train loss:  1.60402269e-05, mean val. loss:  3.13969564e+00\n",
      "Epoch: 15675 mean train loss:  1.63809746e-05, mean val. loss:  3.13986444e+00\n",
      "Epoch: 15676 mean train loss:  1.63333025e-05, mean val. loss:  3.14001465e+00\n",
      "Epoch: 15677 mean train loss:  1.61471253e-05, mean val. loss:  3.14018440e+00\n",
      "Epoch: 15678 mean train loss:  1.63768418e-05, mean val. loss:  3.14035702e+00\n",
      "Epoch: 15679 mean train loss:  1.63231452e-05, mean val. loss:  3.14052677e+00\n",
      "Epoch: 15680 mean train loss:  1.62923243e-05, mean val. loss:  3.14070392e+00\n",
      "Epoch: 15681 mean train loss:  1.63320510e-05, mean val. loss:  3.14087892e+00\n",
      "Epoch: 15682 mean train loss:  1.66250102e-05, mean val. loss:  3.14106631e+00\n",
      "Epoch: 15683 mean train loss:  1.65992533e-05, mean val. loss:  3.14121604e+00\n",
      "Epoch: 15684 mean train loss:  1.60671771e-05, mean val. loss:  3.14142847e+00\n",
      "Epoch: 15685 mean train loss:  1.63672958e-05, mean val. loss:  3.14163208e+00\n",
      "Epoch: 15686 mean train loss:  1.65143574e-05, mean val. loss:  3.14182806e+00\n",
      "Epoch: 15687 mean train loss:  1.60367053e-05, mean val. loss:  3.14204359e+00\n",
      "Epoch: 15688 mean train loss:  1.64498051e-05, mean val. loss:  3.14225626e+00\n",
      "Epoch: 15689 mean train loss:  1.62602519e-05, mean val. loss:  3.14248991e+00\n",
      "Epoch: 15690 mean train loss:  1.61853968e-05, mean val. loss:  3.14276814e+00\n",
      "Epoch: 15691 mean train loss:  1.60367053e-05, mean val. loss:  3.14304781e+00\n",
      "Epoch: 15692 mean train loss:  1.65074016e-05, mean val. loss:  3.14333057e+00\n",
      "Epoch: 15693 mean train loss:  1.64154044e-05, mean val. loss:  3.14359927e+00\n",
      "Epoch: 15694 mean train loss:  1.64320809e-05, mean val. loss:  3.14384842e+00\n",
      "Epoch: 15695 mean train loss:  1.60647323e-05, mean val. loss:  3.14409614e+00\n",
      "Epoch: 15696 mean train loss:  1.60493073e-05, mean val. loss:  3.14436555e+00\n",
      "Epoch: 15697 mean train loss:  1.60522759e-05, mean val. loss:  3.14457917e+00\n",
      "Epoch: 15698 mean train loss:  1.66081009e-05, mean val. loss:  3.14476228e+00\n",
      "Epoch: 15699 mean train loss:  1.63301593e-05, mean val. loss:  3.14490342e+00\n",
      "Epoch: 15700 mean train loss:  1.65338279e-05, mean val. loss:  3.14502263e+00\n",
      "Epoch: 15701 mean train loss:  1.62776560e-05, mean val. loss:  3.14512873e+00\n",
      "Epoch: 15702 mean train loss:  1.62289361e-05, mean val. loss:  3.14524364e+00\n",
      "Epoch: 15703 mean train loss:  1.63189543e-05, mean val. loss:  3.14530873e+00\n",
      "Epoch: 15704 mean train loss:  1.63480872e-05, mean val. loss:  3.14534545e+00\n",
      "Epoch: 15705 mean train loss:  1.66452664e-05, mean val. loss:  3.14538312e+00\n",
      "Epoch: 15706 mean train loss:  1.64760859e-05, mean val. loss:  3.14547110e+00\n",
      "Epoch: 15707 mean train loss:  1.62437209e-05, mean val. loss:  3.14558649e+00\n",
      "Epoch: 15708 mean train loss:  1.63398799e-05, mean val. loss:  3.14572382e+00\n",
      "Epoch: 15709 mean train loss:  1.65426754e-05, mean val. loss:  3.14586830e+00\n",
      "Epoch: 15710 mean train loss:  1.59674673e-05, mean val. loss:  3.14607549e+00\n",
      "Epoch: 15711 mean train loss:  1.64086523e-05, mean val. loss:  3.14631414e+00\n",
      "Epoch: 15712 mean train loss:  1.62606302e-05, mean val. loss:  3.14662552e+00\n",
      "Epoch: 15713 mean train loss:  1.61044300e-05, mean val. loss:  3.14698005e+00\n",
      "Epoch: 15714 mean train loss:  1.63357763e-05, mean val. loss:  3.14733195e+00\n",
      "Epoch: 15715 mean train loss:  1.62508513e-05, mean val. loss:  3.14767694e+00\n",
      "Epoch: 15716 mean train loss:  1.61559146e-05, mean val. loss:  3.14804602e+00\n",
      "Epoch: 15717 mean train loss:  1.64188095e-05, mean val. loss:  3.14844418e+00\n",
      "Epoch: 15718 mean train loss:  1.65219244e-05, mean val. loss:  3.14885235e+00\n",
      "Epoch: 15719 mean train loss:  1.62246579e-05, mean val. loss:  3.14929461e+00\n",
      "Epoch: 15720 mean train loss:  1.62167416e-05, mean val. loss:  3.14975834e+00\n",
      "Epoch: 15721 mean train loss:  1.62320503e-05, mean val. loss:  3.15025091e+00\n",
      "Epoch: 15722 mean train loss:  1.61331263e-05, mean val. loss:  3.15070605e+00\n",
      "Epoch: 15723 mean train loss:  1.64277735e-05, mean val. loss:  3.15111756e+00\n",
      "Epoch: 15724 mean train loss:  1.63890654e-05, mean val. loss:  3.15149426e+00\n",
      "Epoch: 15725 mean train loss:  1.65563251e-05, mean val. loss:  3.15182996e+00\n",
      "Epoch: 15726 mean train loss:  1.65905512e-05, mean val. loss:  3.15213132e+00\n",
      "Epoch: 15727 mean train loss:  1.63712248e-05, mean val. loss:  3.15239596e+00\n",
      "Epoch: 15728 mean train loss:  1.61901698e-05, mean val. loss:  3.15264702e+00\n",
      "Epoch: 15729 mean train loss:  1.65445963e-05, mean val. loss:  3.15284014e+00\n",
      "Epoch: 15730 mean train loss:  1.62422657e-05, mean val. loss:  3.15302539e+00\n",
      "Epoch: 15731 mean train loss:  1.63350196e-05, mean val. loss:  3.15316129e+00\n",
      "Epoch: 15732 mean train loss:  1.60898489e-05, mean val. loss:  3.15328360e+00\n",
      "Epoch: 15733 mean train loss:  1.61818753e-05, mean val. loss:  3.15341902e+00\n",
      "Epoch: 15734 mean train loss:  1.62771321e-05, mean val. loss:  3.15356874e+00\n",
      "Epoch: 15735 mean train loss:  1.62476208e-05, mean val. loss:  3.15365934e+00\n",
      "Epoch: 15736 mean train loss:  1.62522774e-05, mean val. loss:  3.15376091e+00\n",
      "Epoch: 15737 mean train loss:  1.62633951e-05, mean val. loss:  3.15389442e+00\n",
      "Epoch: 15738 mean train loss:  1.61469507e-05, mean val. loss:  3.15407324e+00\n",
      "Epoch: 15739 mean train loss:  1.63325749e-05, mean val. loss:  3.15430427e+00\n",
      "Epoch: 15740 mean train loss:  1.63710210e-05, mean val. loss:  3.15455031e+00\n",
      "Epoch: 15741 mean train loss:  1.61647913e-05, mean val. loss:  3.15482116e+00\n",
      "Epoch: 15742 mean train loss:  1.64214289e-05, mean val. loss:  3.15512967e+00\n",
      "Epoch: 15743 mean train loss:  1.63208460e-05, mean val. loss:  3.15550923e+00\n",
      "Epoch: 15744 mean train loss:  1.64082157e-05, mean val. loss:  3.15589643e+00\n",
      "Epoch: 15745 mean train loss:  1.61583885e-05, mean val. loss:  3.15631032e+00\n",
      "Epoch: 15746 mean train loss:  1.60624331e-05, mean val. loss:  3.15674949e+00\n",
      "Epoch: 15747 mean train loss:  1.63389777e-05, mean val. loss:  3.15721655e+00\n",
      "Epoch: 15748 mean train loss:  1.60218915e-05, mean val. loss:  3.15770483e+00\n",
      "Epoch: 15749 mean train loss:  1.65436359e-05, mean val. loss:  3.15811777e+00\n",
      "Epoch: 15750 mean train loss:  1.62213109e-05, mean val. loss:  3.15850973e+00\n",
      "Epoch: 15751 mean train loss:  1.59991905e-05, mean val. loss:  3.15887785e+00\n",
      "Epoch: 15752 mean train loss:  1.64685189e-05, mean val. loss:  3.15919590e+00\n",
      "Epoch: 15753 mean train loss:  1.58220064e-05, mean val. loss:  3.15952253e+00\n",
      "Epoch: 15754 mean train loss:  1.64642115e-05, mean val. loss:  3.15978503e+00\n",
      "Epoch: 15755 mean train loss:  1.62669457e-05, mean val. loss:  3.16002417e+00\n",
      "Epoch: 15756 mean train loss:  1.60703785e-05, mean val. loss:  3.16024566e+00\n",
      "Epoch: 15757 mean train loss:  1.58996263e-05, mean val. loss:  3.16047263e+00\n",
      "Epoch: 15758 mean train loss:  1.62966608e-05, mean val. loss:  3.16060376e+00\n",
      "Epoch: 15759 mean train loss:  1.62493961e-05, mean val. loss:  3.16072798e+00\n",
      "Epoch: 15760 mean train loss:  1.63303630e-05, mean val. loss:  3.16081762e+00\n",
      "Epoch: 15761 mean train loss:  1.63071963e-05, mean val. loss:  3.16089606e+00\n",
      "Epoch: 15762 mean train loss:  1.61434291e-05, mean val. loss:  3.16097593e+00\n",
      "Epoch: 15763 mean train loss:  1.60150812e-05, mean val. loss:  3.16110778e+00\n",
      "Epoch: 15764 mean train loss:  1.63914519e-05, mean val. loss:  3.16125536e+00\n",
      "Epoch: 15765 mean train loss:  1.63804798e-05, mean val. loss:  3.16141033e+00\n",
      "Epoch: 15766 mean train loss:  1.65315578e-05, mean val. loss:  3.16156578e+00\n",
      "Epoch: 15767 mean train loss:  1.65767269e-05, mean val. loss:  3.16169643e+00\n",
      "Epoch: 15768 mean train loss:  1.60790514e-05, mean val. loss:  3.16185880e+00\n",
      "Epoch: 15769 mean train loss:  1.60997442e-05, mean val. loss:  3.16203380e+00\n",
      "Epoch: 15770 mean train loss:  1.67670078e-05, mean val. loss:  3.16219091e+00\n",
      "Epoch: 15771 mean train loss:  1.62517827e-05, mean val. loss:  3.16234803e+00\n",
      "Epoch: 15772 mean train loss:  1.66151731e-05, mean val. loss:  3.16248846e+00\n",
      "Epoch: 15773 mean train loss:  1.62855431e-05, mean val. loss:  3.16261959e+00\n",
      "Epoch: 15774 mean train loss:  1.63490186e-05, mean val. loss:  3.16276169e+00\n",
      "Epoch: 15775 mean train loss:  1.61604257e-05, mean val. loss:  3.16289783e+00\n",
      "Epoch: 15776 mean train loss:  1.66134560e-05, mean val. loss:  3.16296959e+00\n",
      "Epoch: 15777 mean train loss:  1.60159834e-05, mean val. loss:  3.16309524e+00\n",
      "Epoch: 15778 mean train loss:  1.61875505e-05, mean val. loss:  3.16326404e+00\n",
      "Epoch: 15779 mean train loss:  1.62772194e-05, mean val. loss:  3.16339731e+00\n",
      "Epoch: 15780 mean train loss:  1.63161603e-05, mean val. loss:  3.16353416e+00\n",
      "Epoch: 15781 mean train loss:  1.60605123e-05, mean val. loss:  3.16367984e+00\n",
      "Epoch: 15782 mean train loss:  1.62369979e-05, mean val. loss:  3.16384387e+00\n",
      "Epoch: 15783 mean train loss:  1.64159865e-05, mean val. loss:  3.16400123e+00\n",
      "Epoch: 15784 mean train loss:  1.61687494e-05, mean val. loss:  3.16421175e+00\n",
      "Epoch: 15785 mean train loss:  1.63267250e-05, mean val. loss:  3.16448998e+00\n",
      "Epoch: 15786 mean train loss:  1.63628429e-05, mean val. loss:  3.16479111e+00\n",
      "Epoch: 15787 mean train loss:  1.65105739e-05, mean val. loss:  3.16508317e+00\n",
      "Epoch: 15788 mean train loss:  1.63320219e-05, mean val. loss:  3.16536713e+00\n",
      "Epoch: 15789 mean train loss:  1.61757343e-05, mean val. loss:  3.16569114e+00\n",
      "Epoch: 15790 mean train loss:  1.61489588e-05, mean val. loss:  3.16607022e+00\n",
      "Epoch: 15791 mean train loss:  1.62495417e-05, mean val. loss:  3.16643715e+00\n",
      "Epoch: 15792 mean train loss:  1.62590586e-05, mean val. loss:  3.16684699e+00\n",
      "Epoch: 15793 mean train loss:  1.59069023e-05, mean val. loss:  3.16729879e+00\n",
      "Epoch: 15794 mean train loss:  1.64003577e-05, mean val. loss:  3.16767907e+00\n",
      "Epoch: 15795 mean train loss:  1.61455246e-05, mean val. loss:  3.16804481e+00\n",
      "Epoch: 15796 mean train loss:  1.59458432e-05, mean val. loss:  3.16841578e+00\n",
      "Epoch: 15797 mean train loss:  1.63535296e-05, mean val. loss:  3.16878390e+00\n",
      "Epoch: 15798 mean train loss:  1.66351383e-05, mean val. loss:  3.16911268e+00\n",
      "Epoch: 15799 mean train loss:  1.61758799e-05, mean val. loss:  3.16942072e+00\n",
      "Epoch: 15800 mean train loss:  1.62514043e-05, mean val. loss:  3.16971350e+00\n",
      "Epoch: 15801 mean train loss:  1.64380181e-05, mean val. loss:  3.16993809e+00\n",
      "Epoch: 15802 mean train loss:  1.60596974e-05, mean val. loss:  3.17020869e+00\n",
      "Epoch: 15803 mean train loss:  1.60310592e-05, mean val. loss:  3.17047739e+00\n",
      "Epoch: 15804 mean train loss:  1.62682263e-05, mean val. loss:  3.17071772e+00\n",
      "Epoch: 15805 mean train loss:  1.64132216e-05, mean val. loss:  3.17094898e+00\n",
      "Epoch: 15806 mean train loss:  1.59972697e-05, mean val. loss:  3.17120552e+00\n",
      "Epoch: 15807 mean train loss:  1.63205550e-05, mean val. loss:  3.17143369e+00\n",
      "Epoch: 15808 mean train loss:  1.61453790e-05, mean val. loss:  3.17163014e+00\n",
      "Epoch: 15809 mean train loss:  1.60714553e-05, mean val. loss:  3.17177486e+00\n",
      "Epoch: 15810 mean train loss:  1.60213967e-05, mean val. loss:  3.17193913e+00\n",
      "Epoch: 15811 mean train loss:  1.62768702e-05, mean val. loss:  3.17211175e+00\n",
      "Epoch: 15812 mean train loss:  1.61848147e-05, mean val. loss:  3.17226505e+00\n",
      "Epoch: 15813 mean train loss:  1.62728829e-05, mean val. loss:  3.17240119e+00\n",
      "Epoch: 15814 mean train loss:  1.61999196e-05, mean val. loss:  3.17253208e+00\n",
      "Epoch: 15815 mean train loss:  1.62051874e-05, mean val. loss:  3.17267609e+00\n",
      "Epoch: 15816 mean train loss:  1.61067874e-05, mean val. loss:  3.17283010e+00\n",
      "Epoch: 15817 mean train loss:  1.62368233e-05, mean val. loss:  3.17295980e+00\n",
      "Epoch: 15818 mean train loss:  1.62938377e-05, mean val. loss:  3.17307878e+00\n",
      "Epoch: 15819 mean train loss:  1.60550408e-05, mean val. loss:  3.17320633e+00\n",
      "Epoch: 15820 mean train loss:  1.63370860e-05, mean val. loss:  3.17330933e+00\n",
      "Epoch: 15821 mean train loss:  1.64417434e-05, mean val. loss:  3.17336941e+00\n",
      "Epoch: 15822 mean train loss:  1.63712539e-05, mean val. loss:  3.17343044e+00\n",
      "Epoch: 15823 mean train loss:  1.62419747e-05, mean val. loss:  3.17348552e+00\n",
      "Epoch: 15824 mean train loss:  1.62295473e-05, mean val. loss:  3.17352438e+00\n",
      "Epoch: 15825 mean train loss:  1.58855983e-05, mean val. loss:  3.17362070e+00\n",
      "Epoch: 15826 mean train loss:  1.60642085e-05, mean val. loss:  3.17375326e+00\n",
      "Epoch: 15827 mean train loss:  1.63007062e-05, mean val. loss:  3.17394257e+00\n",
      "Epoch: 15828 mean train loss:  1.61424687e-05, mean val. loss:  3.17413878e+00\n",
      "Epoch: 15829 mean train loss:  1.61298376e-05, mean val. loss:  3.17436600e+00\n",
      "Epoch: 15830 mean train loss:  1.64937228e-05, mean val. loss:  3.17459273e+00\n",
      "Epoch: 15831 mean train loss:  1.64510566e-05, mean val. loss:  3.17489791e+00\n",
      "Epoch: 15832 mean train loss:  1.59895280e-05, mean val. loss:  3.17523479e+00\n",
      "Epoch: 15833 mean train loss:  1.62577489e-05, mean val. loss:  3.17561507e+00\n",
      "Epoch: 15834 mean train loss:  1.62697979e-05, mean val. loss:  3.17605352e+00\n",
      "Epoch: 15835 mean train loss:  1.62022770e-05, mean val. loss:  3.17647505e+00\n",
      "Epoch: 15836 mean train loss:  1.65791716e-05, mean val. loss:  3.17692494e+00\n",
      "Epoch: 15837 mean train loss:  1.62047800e-05, mean val. loss:  3.17740726e+00\n",
      "Epoch: 15838 mean train loss:  1.60093477e-05, mean val. loss:  3.17793107e+00\n",
      "Epoch: 15839 mean train loss:  1.63124932e-05, mean val. loss:  3.17845583e+00\n",
      "Epoch: 15840 mean train loss:  1.63640361e-05, mean val. loss:  3.17894554e+00\n",
      "Epoch: 15841 mean train loss:  1.63107761e-05, mean val. loss:  3.17942929e+00\n",
      "Epoch: 15842 mean train loss:  1.62190408e-05, mean val. loss:  3.17988920e+00\n",
      "Epoch: 15843 mean train loss:  1.61298376e-05, mean val. loss:  3.18033314e+00\n",
      "Epoch: 15844 mean train loss:  1.61284115e-05, mean val. loss:  3.18077779e+00\n",
      "Epoch: 15845 mean train loss:  1.59262854e-05, mean val. loss:  3.18121672e+00\n",
      "Epoch: 15846 mean train loss:  1.59936899e-05, mean val. loss:  3.18163824e+00\n",
      "Epoch: 15847 mean train loss:  1.63274526e-05, mean val. loss:  3.18204522e+00\n",
      "Epoch: 15848 mean train loss:  1.57909526e-05, mean val. loss:  3.18248367e+00\n",
      "Epoch: 15849 mean train loss:  1.66429672e-05, mean val. loss:  3.18289590e+00\n",
      "Epoch: 15850 mean train loss:  1.60641794e-05, mean val. loss:  3.18332434e+00\n",
      "Epoch: 15851 mean train loss:  1.63072837e-05, mean val. loss:  3.18372083e+00\n",
      "Epoch: 15852 mean train loss:  1.61520729e-05, mean val. loss:  3.18409252e+00\n",
      "Epoch: 15853 mean train loss:  1.59579213e-05, mean val. loss:  3.18448877e+00\n",
      "Epoch: 15854 mean train loss:  1.60600466e-05, mean val. loss:  3.18487573e+00\n",
      "Epoch: 15855 mean train loss:  1.61000644e-05, mean val. loss:  3.18528581e+00\n",
      "Epoch: 15856 mean train loss:  1.63770746e-05, mean val. loss:  3.18568063e+00\n",
      "Epoch: 15857 mean train loss:  1.63895602e-05, mean val. loss:  3.18601799e+00\n",
      "Epoch: 15858 mean train loss:  1.59012852e-05, mean val. loss:  3.18638325e+00\n",
      "Epoch: 15859 mean train loss:  1.61815551e-05, mean val. loss:  3.18676329e+00\n",
      "Epoch: 15860 mean train loss:  1.60822528e-05, mean val. loss:  3.18716025e+00\n",
      "Epoch: 15861 mean train loss:  1.59487245e-05, mean val. loss:  3.18758440e+00\n",
      "Epoch: 15862 mean train loss:  1.62166543e-05, mean val. loss:  3.18792439e+00\n",
      "Epoch: 15863 mean train loss:  1.63856894e-05, mean val. loss:  3.18820834e+00\n",
      "Epoch: 15864 mean train loss:  1.62433425e-05, mean val. loss:  3.18849564e+00\n",
      "Epoch: 15865 mean train loss:  1.64895901e-05, mean val. loss:  3.18870211e+00\n",
      "Epoch: 15866 mean train loss:  1.63394725e-05, mean val. loss:  3.18886614e+00\n",
      "Epoch: 15867 mean train loss:  1.62370270e-05, mean val. loss:  3.18899727e+00\n",
      "Epoch: 15868 mean train loss:  1.63013756e-05, mean val. loss:  3.18910933e+00\n",
      "Epoch: 15869 mean train loss:  1.64337980e-05, mean val. loss:  3.18911743e+00\n",
      "Epoch: 15870 mean train loss:  1.62087090e-05, mean val. loss:  3.18913794e+00\n",
      "Epoch: 15871 mean train loss:  1.62494252e-05, mean val. loss:  3.18917155e+00\n",
      "Epoch: 15872 mean train loss:  1.62618526e-05, mean val. loss:  3.18916869e+00\n",
      "Epoch: 15873 mean train loss:  1.62356009e-05, mean val. loss:  3.18917465e+00\n",
      "Epoch: 15874 mean train loss:  1.63170043e-05, mean val. loss:  3.18913269e+00\n",
      "Epoch: 15875 mean train loss:  1.64182275e-05, mean val. loss:  3.18908858e+00\n",
      "Epoch: 15876 mean train loss:  1.62754441e-05, mean val. loss:  3.18901181e+00\n",
      "Epoch: 15877 mean train loss:  1.61260832e-05, mean val. loss:  3.18896365e+00\n",
      "Epoch: 15878 mean train loss:  1.63727673e-05, mean val. loss:  3.18890381e+00\n",
      "Epoch: 15879 mean train loss:  1.60519849e-05, mean val. loss:  3.18889189e+00\n",
      "Epoch: 15880 mean train loss:  1.59456977e-05, mean val. loss:  3.18899751e+00\n",
      "Epoch: 15881 mean train loss:  1.61446515e-05, mean val. loss:  3.18914747e+00\n",
      "Epoch: 15882 mean train loss:  1.61072239e-05, mean val. loss:  3.18935061e+00\n",
      "Epoch: 15883 mean train loss:  1.61769858e-05, mean val. loss:  3.18957353e+00\n",
      "Epoch: 15884 mean train loss:  1.59796909e-05, mean val. loss:  3.18982768e+00\n",
      "Epoch: 15885 mean train loss:  1.62961951e-05, mean val. loss:  3.19007778e+00\n",
      "Epoch: 15886 mean train loss:  1.63235236e-05, mean val. loss:  3.19035339e+00\n",
      "Epoch: 15887 mean train loss:  1.61962525e-05, mean val. loss:  3.19065261e+00\n",
      "Epoch: 15888 mean train loss:  1.61101925e-05, mean val. loss:  3.19099021e+00\n",
      "Epoch: 15889 mean train loss:  1.60560012e-05, mean val. loss:  3.19133425e+00\n",
      "Epoch: 15890 mean train loss:  1.60759664e-05, mean val. loss:  3.19167900e+00\n",
      "Epoch: 15891 mean train loss:  1.65697711e-05, mean val. loss:  3.19199657e+00\n",
      "Epoch: 15892 mean train loss:  1.63957302e-05, mean val. loss:  3.19225454e+00\n",
      "Epoch: 15893 mean train loss:  1.59268675e-05, mean val. loss:  3.19255066e+00\n",
      "Epoch: 15894 mean train loss:  1.60899945e-05, mean val. loss:  3.19282746e+00\n",
      "Epoch: 15895 mean train loss:  1.62400247e-05, mean val. loss:  3.19306087e+00\n",
      "Epoch: 15896 mean train loss:  1.61202624e-05, mean val. loss:  3.19330835e+00\n",
      "Epoch: 15897 mean train loss:  1.63361547e-05, mean val. loss:  3.19352484e+00\n",
      "Epoch: 15898 mean train loss:  1.63790537e-05, mean val. loss:  3.19373870e+00\n",
      "Epoch: 15899 mean train loss:  1.60679629e-05, mean val. loss:  3.19402289e+00\n",
      "Epoch: 15900 mean train loss:  1.61933131e-05, mean val. loss:  3.19432855e+00\n",
      "Epoch: 15901 mean train loss:  1.61528005e-05, mean val. loss:  3.19470024e+00\n",
      "Epoch: 15902 mean train loss:  1.61872013e-05, mean val. loss:  3.19510508e+00\n",
      "Epoch: 15903 mean train loss:  1.60110940e-05, mean val. loss:  3.19558072e+00\n",
      "Epoch: 15904 mean train loss:  1.60091440e-05, mean val. loss:  3.19613290e+00\n",
      "Epoch: 15905 mean train loss:  1.60353375e-05, mean val. loss:  3.19671464e+00\n",
      "Epoch: 15906 mean train loss:  1.61857461e-05, mean val. loss:  3.19732428e+00\n",
      "Epoch: 15907 mean train loss:  1.61021890e-05, mean val. loss:  3.19792628e+00\n",
      "Epoch: 15908 mean train loss:  1.61484932e-05, mean val. loss:  3.19850945e+00\n",
      "Epoch: 15909 mean train loss:  1.59617048e-05, mean val. loss:  3.19910789e+00\n",
      "Epoch: 15910 mean train loss:  1.62025390e-05, mean val. loss:  3.19966340e+00\n",
      "Epoch: 15911 mean train loss:  1.62522483e-05, mean val. loss:  3.20018601e+00\n",
      "Epoch: 15912 mean train loss:  1.63253280e-05, mean val. loss:  3.20063710e+00\n",
      "Epoch: 15913 mean train loss:  1.62879587e-05, mean val. loss:  3.20100451e+00\n",
      "Epoch: 15914 mean train loss:  1.63663353e-05, mean val. loss:  3.20131230e+00\n",
      "Epoch: 15915 mean train loss:  1.59457850e-05, mean val. loss:  3.20160747e+00\n",
      "Epoch: 15916 mean train loss:  1.62347278e-05, mean val. loss:  3.20187330e+00\n",
      "Epoch: 15917 mean train loss:  1.58674375e-05, mean val. loss:  3.20212865e+00\n",
      "Epoch: 15918 mean train loss:  1.62383076e-05, mean val. loss:  3.20234108e+00\n",
      "Epoch: 15919 mean train loss:  1.64772500e-05, mean val. loss:  3.20248175e+00\n",
      "Epoch: 15920 mean train loss:  1.63086806e-05, mean val. loss:  3.20261383e+00\n",
      "Epoch: 15921 mean train loss:  1.59616466e-05, mean val. loss:  3.20275402e+00\n",
      "Epoch: 15922 mean train loss:  1.63303921e-05, mean val. loss:  3.20285344e+00\n",
      "Epoch: 15923 mean train loss:  1.60714844e-05, mean val. loss:  3.20295453e+00\n",
      "Epoch: 15924 mean train loss:  1.62845536e-05, mean val. loss:  3.20305896e+00\n",
      "Epoch: 15925 mean train loss:  1.59067567e-05, mean val. loss:  3.20321083e+00\n",
      "Epoch: 15926 mean train loss:  1.62721262e-05, mean val. loss:  3.20336390e+00\n",
      "Epoch: 15927 mean train loss:  1.58823968e-05, mean val. loss:  3.20355368e+00\n",
      "Epoch: 15928 mean train loss:  1.63680525e-05, mean val. loss:  3.20372057e+00\n",
      "Epoch: 15929 mean train loss:  1.61408097e-05, mean val. loss:  3.20392275e+00\n",
      "Epoch: 15930 mean train loss:  1.59605697e-05, mean val. loss:  3.20414901e+00\n",
      "Epoch: 15931 mean train loss:  1.64155790e-05, mean val. loss:  3.20434999e+00\n",
      "Epoch: 15932 mean train loss:  1.62824581e-05, mean val. loss:  3.20452476e+00\n",
      "Epoch: 15933 mean train loss:  1.61996286e-05, mean val. loss:  3.20468903e+00\n",
      "Epoch: 15934 mean train loss:  1.61152275e-05, mean val. loss:  3.20484495e+00\n",
      "Epoch: 15935 mean train loss:  1.59590563e-05, mean val. loss:  3.20505595e+00\n",
      "Epoch: 15936 mean train loss:  1.64311205e-05, mean val. loss:  3.20526266e+00\n",
      "Epoch: 15937 mean train loss:  1.62247452e-05, mean val. loss:  3.20546031e+00\n",
      "Epoch: 15938 mean train loss:  1.62634824e-05, mean val. loss:  3.20564365e+00\n",
      "Epoch: 15939 mean train loss:  1.61695934e-05, mean val. loss:  3.20579576e+00\n",
      "Epoch: 15940 mean train loss:  1.63211662e-05, mean val. loss:  3.20591044e+00\n",
      "Epoch: 15941 mean train loss:  1.62447104e-05, mean val. loss:  3.20603728e+00\n",
      "Epoch: 15942 mean train loss:  1.60809432e-05, mean val. loss:  3.20616078e+00\n",
      "Epoch: 15943 mean train loss:  1.60641503e-05, mean val. loss:  3.20632410e+00\n",
      "Epoch: 15944 mean train loss:  1.63539662e-05, mean val. loss:  3.20643616e+00\n",
      "Epoch: 15945 mean train loss:  1.63302466e-05, mean val. loss:  3.20657659e+00\n",
      "Epoch: 15946 mean train loss:  1.61501812e-05, mean val. loss:  3.20670915e+00\n",
      "Epoch: 15947 mean train loss:  1.60482887e-05, mean val. loss:  3.20691037e+00\n",
      "Epoch: 15948 mean train loss:  1.62543147e-05, mean val. loss:  3.20711017e+00\n",
      "Epoch: 15949 mean train loss:  1.60995696e-05, mean val. loss:  3.20733404e+00\n",
      "Epoch: 15950 mean train loss:  1.62451179e-05, mean val. loss:  3.20758510e+00\n",
      "Epoch: 15951 mean train loss:  1.60911004e-05, mean val. loss:  3.20781016e+00\n",
      "Epoch: 15952 mean train loss:  1.58995390e-05, mean val. loss:  3.20806003e+00\n",
      "Epoch: 15953 mean train loss:  1.61364442e-05, mean val. loss:  3.20837307e+00\n",
      "Epoch: 15954 mean train loss:  1.61932549e-05, mean val. loss:  3.20868206e+00\n",
      "Epoch: 15955 mean train loss:  1.63821678e-05, mean val. loss:  3.20901799e+00\n",
      "Epoch: 15956 mean train loss:  1.66014070e-05, mean val. loss:  3.20931530e+00\n",
      "Epoch: 15957 mean train loss:  1.61291100e-05, mean val. loss:  3.20966005e+00\n",
      "Epoch: 15958 mean train loss:  1.63098739e-05, mean val. loss:  3.21002126e+00\n",
      "Epoch: 15959 mean train loss:  1.61211938e-05, mean val. loss:  3.21037626e+00\n",
      "Epoch: 15960 mean train loss:  1.63929653e-05, mean val. loss:  3.21068192e+00\n",
      "Epoch: 15961 mean train loss:  1.63169752e-05, mean val. loss:  3.21095824e+00\n",
      "Epoch: 15962 mean train loss:  1.60945056e-05, mean val. loss:  3.21124005e+00\n",
      "Epoch: 15963 mean train loss:  1.61486969e-05, mean val. loss:  3.21150422e+00\n",
      "Epoch: 15964 mean train loss:  1.61855714e-05, mean val. loss:  3.21177697e+00\n",
      "Epoch: 15965 mean train loss:  1.63067016e-05, mean val. loss:  3.21198845e+00\n",
      "Epoch: 15966 mean train loss:  1.63224759e-05, mean val. loss:  3.21216726e+00\n",
      "Epoch: 15967 mean train loss:  1.58875191e-05, mean val. loss:  3.21236610e+00\n",
      "Epoch: 15968 mean train loss:  1.55912130e-05, mean val. loss:  3.21262765e+00\n",
      "Epoch: 15969 mean train loss:  1.60128402e-05, mean val. loss:  3.21291709e+00\n",
      "Epoch: 15970 mean train loss:  1.61392381e-05, mean val. loss:  3.21316981e+00\n",
      "Epoch: 15971 mean train loss:  1.62713986e-05, mean val. loss:  3.21339655e+00\n",
      "Epoch: 15972 mean train loss:  1.63220684e-05, mean val. loss:  3.21361279e+00\n",
      "Epoch: 15973 mean train loss:  1.59442716e-05, mean val. loss:  3.21388912e+00\n",
      "Epoch: 15974 mean train loss:  1.62151409e-05, mean val. loss:  3.21418118e+00\n",
      "Epoch: 15975 mean train loss:  1.61135686e-05, mean val. loss:  3.21445704e+00\n",
      "Epoch: 15976 mean train loss:  1.62584474e-05, mean val. loss:  3.21472383e+00\n",
      "Epoch: 15977 mean train loss:  1.57824252e-05, mean val. loss:  3.21509218e+00\n",
      "Epoch: 15978 mean train loss:  1.63968070e-05, mean val. loss:  3.21544433e+00\n",
      "Epoch: 15979 mean train loss:  1.63376390e-05, mean val. loss:  3.21576858e+00\n",
      "Epoch: 15980 mean train loss:  1.60928175e-05, mean val. loss:  3.21609378e+00\n",
      "Epoch: 15981 mean train loss:  1.61671196e-05, mean val. loss:  3.21639705e+00\n",
      "Epoch: 15982 mean train loss:  1.60574855e-05, mean val. loss:  3.21669030e+00\n",
      "Epoch: 15983 mean train loss:  1.60147902e-05, mean val. loss:  3.21693969e+00\n",
      "Epoch: 15984 mean train loss:  1.60077179e-05, mean val. loss:  3.21719742e+00\n",
      "Epoch: 15985 mean train loss:  1.62204669e-05, mean val. loss:  3.21747112e+00\n",
      "Epoch: 15986 mean train loss:  1.60501804e-05, mean val. loss:  3.21774983e+00\n",
      "Epoch: 15987 mean train loss:  1.60736381e-05, mean val. loss:  3.21799994e+00\n",
      "Epoch: 15988 mean train loss:  1.60249474e-05, mean val. loss:  3.21822500e+00\n",
      "Epoch: 15989 mean train loss:  1.62058277e-05, mean val. loss:  3.21843910e+00\n",
      "Epoch: 15990 mean train loss:  1.59606570e-05, mean val. loss:  3.21865654e+00\n",
      "Epoch: 15991 mean train loss:  1.58211333e-05, mean val. loss:  3.21888232e+00\n",
      "Epoch: 15992 mean train loss:  1.62642391e-05, mean val. loss:  3.21908855e+00\n",
      "Epoch: 15993 mean train loss:  1.61499775e-05, mean val. loss:  3.21929574e+00\n",
      "Epoch: 15994 mean train loss:  1.61083299e-05, mean val. loss:  3.21953917e+00\n",
      "Epoch: 15995 mean train loss:  1.59310584e-05, mean val. loss:  3.21978235e+00\n",
      "Epoch: 15996 mean train loss:  1.61076605e-05, mean val. loss:  3.22004056e+00\n",
      "Epoch: 15997 mean train loss:  1.60933996e-05, mean val. loss:  3.22031474e+00\n",
      "Epoch: 15998 mean train loss:  1.61615026e-05, mean val. loss:  3.22063518e+00\n",
      "Epoch: 15999 mean train loss:  1.58011098e-05, mean val. loss:  3.22102642e+00\n",
      "Epoch: 16000 mean train loss:  1.57654868e-05, mean val. loss:  3.22145844e+00\n",
      "Epoch: 16001 mean train loss:  1.60562922e-05, mean val. loss:  3.22189307e+00\n",
      "Epoch: 16002 mean train loss:  1.59763731e-05, mean val. loss:  3.22232795e+00\n",
      "Epoch: 16003 mean train loss:  1.63026853e-05, mean val. loss:  3.22273111e+00\n",
      "Epoch: 16004 mean train loss:  1.60441850e-05, mean val. loss:  3.22313929e+00\n",
      "Epoch: 16005 mean train loss:  1.60263153e-05, mean val. loss:  3.22354460e+00\n",
      "Epoch: 16006 mean train loss:  1.62718061e-05, mean val. loss:  3.22391200e+00\n",
      "Epoch: 16007 mean train loss:  1.60795462e-05, mean val. loss:  3.22426772e+00\n",
      "Epoch: 16008 mean train loss:  1.58237817e-05, mean val. loss:  3.22457385e+00\n",
      "Epoch: 16009 mean train loss:  1.60591444e-05, mean val. loss:  3.22485924e+00\n",
      "Epoch: 16010 mean train loss:  1.61623757e-05, mean val. loss:  3.22511363e+00\n",
      "Epoch: 16011 mean train loss:  1.63735240e-05, mean val. loss:  3.22531080e+00\n",
      "Epoch: 16012 mean train loss:  1.61456992e-05, mean val. loss:  3.22543049e+00\n",
      "Epoch: 16013 mean train loss:  1.60033815e-05, mean val. loss:  3.22553039e+00\n",
      "Epoch: 16014 mean train loss:  1.60920317e-05, mean val. loss:  3.22561240e+00\n",
      "Epoch: 16015 mean train loss:  1.60879572e-05, mean val. loss:  3.22565961e+00\n",
      "Epoch: 16016 mean train loss:  1.59617339e-05, mean val. loss:  3.22571445e+00\n",
      "Epoch: 16017 mean train loss:  1.62789947e-05, mean val. loss:  3.22577691e+00\n",
      "Epoch: 16018 mean train loss:  1.62337383e-05, mean val. loss:  3.22583270e+00\n",
      "Epoch: 16019 mean train loss:  1.57493632e-05, mean val. loss:  3.22594357e+00\n",
      "Epoch: 16020 mean train loss:  1.62980868e-05, mean val. loss:  3.22607875e+00\n",
      "Epoch: 16021 mean train loss:  1.62334181e-05, mean val. loss:  3.22621155e+00\n",
      "Epoch: 16022 mean train loss:  1.60713680e-05, mean val. loss:  3.22642899e+00\n",
      "Epoch: 16023 mean train loss:  1.60341151e-05, mean val. loss:  3.22667742e+00\n",
      "Epoch: 16024 mean train loss:  1.61361822e-05, mean val. loss:  3.22694993e+00\n",
      "Epoch: 16025 mean train loss:  1.60773343e-05, mean val. loss:  3.22726846e+00\n",
      "Epoch: 16026 mean train loss:  1.59691554e-05, mean val. loss:  3.22759104e+00\n",
      "Epoch: 16027 mean train loss:  1.61782955e-05, mean val. loss:  3.22795463e+00\n",
      "Epoch: 16028 mean train loss:  1.59254705e-05, mean val. loss:  3.22839427e+00\n",
      "Epoch: 16029 mean train loss:  1.62412180e-05, mean val. loss:  3.22883439e+00\n",
      "Epoch: 16030 mean train loss:  1.60370255e-05, mean val. loss:  3.22927904e+00\n",
      "Epoch: 16031 mean train loss:  1.62980868e-05, mean val. loss:  3.22971559e+00\n",
      "Epoch: 16032 mean train loss:  1.62703800e-05, mean val. loss:  3.23009944e+00\n",
      "Epoch: 16033 mean train loss:  1.60593481e-05, mean val. loss:  3.23048663e+00\n",
      "Epoch: 16034 mean train loss:  1.63376098e-05, mean val. loss:  3.23087621e+00\n",
      "Epoch: 16035 mean train loss:  1.59112969e-05, mean val. loss:  3.23126912e+00\n",
      "Epoch: 16036 mean train loss:  1.58826879e-05, mean val. loss:  3.23170853e+00\n",
      "Epoch: 16037 mean train loss:  1.62637152e-05, mean val. loss:  3.23212814e+00\n",
      "Epoch: 16038 mean train loss:  1.61128992e-05, mean val. loss:  3.23251295e+00\n",
      "Epoch: 16039 mean train loss:  1.61732023e-05, mean val. loss:  3.23290229e+00\n",
      "Epoch: 16040 mean train loss:  1.63733785e-05, mean val. loss:  3.23318768e+00\n",
      "Epoch: 16041 mean train loss:  1.58135954e-05, mean val. loss:  3.23350763e+00\n",
      "Epoch: 16042 mean train loss:  1.59043411e-05, mean val. loss:  3.23383260e+00\n",
      "Epoch: 16043 mean train loss:  1.58660987e-05, mean val. loss:  3.23412681e+00\n",
      "Epoch: 16044 mean train loss:  1.63648510e-05, mean val. loss:  3.23434687e+00\n",
      "Epoch: 16045 mean train loss:  1.61196804e-05, mean val. loss:  3.23456192e+00\n",
      "Epoch: 16046 mean train loss:  1.61317294e-05, mean val. loss:  3.23475194e+00\n",
      "Epoch: 16047 mean train loss:  1.60599302e-05, mean val. loss:  3.23495698e+00\n",
      "Epoch: 16048 mean train loss:  1.62095239e-05, mean val. loss:  3.23510528e+00\n",
      "Epoch: 16049 mean train loss:  1.61955249e-05, mean val. loss:  3.23517513e+00\n",
      "Epoch: 16050 mean train loss:  1.59787305e-05, mean val. loss:  3.23522902e+00\n",
      "Epoch: 16051 mean train loss:  1.62780052e-05, mean val. loss:  3.23524213e+00\n",
      "Epoch: 16052 mean train loss:  1.62236684e-05, mean val. loss:  3.23520994e+00\n",
      "Epoch: 16053 mean train loss:  1.62890356e-05, mean val. loss:  3.23518181e+00\n",
      "Epoch: 16054 mean train loss:  1.59288174e-05, mean val. loss:  3.23515415e+00\n",
      "Epoch: 16055 mean train loss:  1.62112119e-05, mean val. loss:  3.23513722e+00\n",
      "Epoch: 16056 mean train loss:  1.58671755e-05, mean val. loss:  3.23519397e+00\n",
      "Epoch: 16057 mean train loss:  1.59839692e-05, mean val. loss:  3.23528647e+00\n",
      "Epoch: 16058 mean train loss:  1.63089717e-05, mean val. loss:  3.23543835e+00\n",
      "Epoch: 16059 mean train loss:  1.60060590e-05, mean val. loss:  3.23567367e+00\n",
      "Epoch: 16060 mean train loss:  1.58710754e-05, mean val. loss:  3.23597240e+00\n",
      "Epoch: 16061 mean train loss:  1.56210153e-05, mean val. loss:  3.23638606e+00\n",
      "Epoch: 16062 mean train loss:  1.62182259e-05, mean val. loss:  3.23684478e+00\n",
      "Epoch: 16063 mean train loss:  1.59512565e-05, mean val. loss:  3.23737192e+00\n",
      "Epoch: 16064 mean train loss:  1.59769552e-05, mean val. loss:  3.23789597e+00\n",
      "Epoch: 16065 mean train loss:  1.63099030e-05, mean val. loss:  3.23843741e+00\n",
      "Epoch: 16066 mean train loss:  1.60171767e-05, mean val. loss:  3.23898101e+00\n",
      "Epoch: 16067 mean train loss:  1.55830639e-05, mean val. loss:  3.23956990e+00\n",
      "Epoch: 16068 mean train loss:  1.58893818e-05, mean val. loss:  3.24018216e+00\n",
      "Epoch: 16069 mean train loss:  1.61171192e-05, mean val. loss:  3.24076152e+00\n",
      "Epoch: 16070 mean train loss:  1.59805350e-05, mean val. loss:  3.24129224e+00\n",
      "Epoch: 16071 mean train loss:  1.61505013e-05, mean val. loss:  3.24174619e+00\n",
      "Epoch: 16072 mean train loss:  1.57809700e-05, mean val. loss:  3.24219394e+00\n",
      "Epoch: 16073 mean train loss:  1.62592623e-05, mean val. loss:  3.24261236e+00\n",
      "Epoch: 16074 mean train loss:  1.60679920e-05, mean val. loss:  3.24298978e+00\n",
      "Epoch: 16075 mean train loss:  1.59712508e-05, mean val. loss:  3.24329257e+00\n",
      "Epoch: 16076 mean train loss:  1.61905191e-05, mean val. loss:  3.24353504e+00\n",
      "Epoch: 16077 mean train loss:  1.58045150e-05, mean val. loss:  3.24375486e+00\n",
      "Epoch: 16078 mean train loss:  1.59867050e-05, mean val. loss:  3.24398041e+00\n",
      "Epoch: 16079 mean train loss:  1.61323696e-05, mean val. loss:  3.24416780e+00\n",
      "Epoch: 16080 mean train loss:  1.60551863e-05, mean val. loss:  3.24433470e+00\n",
      "Epoch: 16081 mean train loss:  1.61361240e-05, mean val. loss:  3.24447083e+00\n",
      "Epoch: 16082 mean train loss:  1.60435739e-05, mean val. loss:  3.24462628e+00\n",
      "Epoch: 16083 mean train loss:  1.58975017e-05, mean val. loss:  3.24480605e+00\n",
      "Epoch: 16084 mean train loss:  1.62615906e-05, mean val. loss:  3.24495173e+00\n",
      "Epoch: 16085 mean train loss:  1.63671211e-05, mean val. loss:  3.24505734e+00\n",
      "Epoch: 16086 mean train loss:  1.62460492e-05, mean val. loss:  3.24510717e+00\n",
      "Epoch: 16087 mean train loss:  1.60412164e-05, mean val. loss:  3.24518108e+00\n",
      "Epoch: 16088 mean train loss:  1.59949414e-05, mean val. loss:  3.24528193e+00\n",
      "Epoch: 16089 mean train loss:  1.57999748e-05, mean val. loss:  3.24542689e+00\n",
      "Epoch: 16090 mean train loss:  1.59450574e-05, mean val. loss:  3.24561071e+00\n",
      "Epoch: 16091 mean train loss:  1.59547781e-05, mean val. loss:  3.24575639e+00\n",
      "Epoch: 16092 mean train loss:  1.59254414e-05, mean val. loss:  3.24586940e+00\n",
      "Epoch: 16093 mean train loss:  1.57149625e-05, mean val. loss:  3.24603844e+00\n",
      "Epoch: 16094 mean train loss:  1.60854834e-05, mean val. loss:  3.24621916e+00\n",
      "Epoch: 16095 mean train loss:  1.61440112e-05, mean val. loss:  3.24637556e+00\n",
      "Epoch: 16096 mean train loss:  1.60128984e-05, mean val. loss:  3.24656200e+00\n",
      "Epoch: 16097 mean train loss:  1.60581549e-05, mean val. loss:  3.24670911e+00\n",
      "Epoch: 16098 mean train loss:  1.61005883e-05, mean val. loss:  3.24683452e+00\n",
      "Epoch: 16099 mean train loss:  1.58461917e-05, mean val. loss:  3.24704385e+00\n",
      "Epoch: 16100 mean train loss:  1.60008785e-05, mean val. loss:  3.24725771e+00\n",
      "Epoch: 16101 mean train loss:  1.60702912e-05, mean val. loss:  3.24750113e+00\n",
      "Epoch: 16102 mean train loss:  1.60579220e-05, mean val. loss:  3.24776196e+00\n",
      "Epoch: 16103 mean train loss:  1.62786164e-05, mean val. loss:  3.24802423e+00\n",
      "Epoch: 16104 mean train loss:  1.60928757e-05, mean val. loss:  3.24829936e+00\n",
      "Epoch: 16105 mean train loss:  1.61581265e-05, mean val. loss:  3.24861169e+00\n",
      "Epoch: 16106 mean train loss:  1.62599317e-05, mean val. loss:  3.24894214e+00\n",
      "Epoch: 16107 mean train loss:  1.57129543e-05, mean val. loss:  3.24935317e+00\n",
      "Epoch: 16108 mean train loss:  1.60759955e-05, mean val. loss:  3.24977231e+00\n",
      "Epoch: 16109 mean train loss:  1.63449149e-05, mean val. loss:  3.25014448e+00\n",
      "Epoch: 16110 mean train loss:  1.59960764e-05, mean val. loss:  3.25050402e+00\n",
      "Epoch: 16111 mean train loss:  1.62019278e-05, mean val. loss:  3.25083184e+00\n",
      "Epoch: 16112 mean train loss:  1.59202609e-05, mean val. loss:  3.25113940e+00\n",
      "Epoch: 16113 mean train loss:  1.61254720e-05, mean val. loss:  3.25142479e+00\n",
      "Epoch: 16114 mean train loss:  1.58090552e-05, mean val. loss:  3.25170970e+00\n",
      "Epoch: 16115 mean train loss:  1.57678151e-05, mean val. loss:  3.25203419e+00\n",
      "Epoch: 16116 mean train loss:  1.56364695e-05, mean val. loss:  3.25236011e+00\n",
      "Epoch: 16117 mean train loss:  1.60900527e-05, mean val. loss:  3.25267935e+00\n",
      "Epoch: 16118 mean train loss:  1.58583862e-05, mean val. loss:  3.25300956e+00\n",
      "Epoch: 16119 mean train loss:  1.61931966e-05, mean val. loss:  3.25331235e+00\n",
      "Epoch: 16120 mean train loss:  1.62061770e-05, mean val. loss:  3.25362349e+00\n",
      "Epoch: 16121 mean train loss:  1.57620525e-05, mean val. loss:  3.25398207e+00\n",
      "Epoch: 16122 mean train loss:  1.58516632e-05, mean val. loss:  3.25435519e+00\n",
      "Epoch: 16123 mean train loss:  1.62515207e-05, mean val. loss:  3.25472021e+00\n",
      "Epoch: 16124 mean train loss:  1.62362412e-05, mean val. loss:  3.25509286e+00\n",
      "Epoch: 16125 mean train loss:  1.60745694e-05, mean val. loss:  3.25554466e+00\n",
      "Epoch: 16126 mean train loss:  1.64601952e-05, mean val. loss:  3.25599790e+00\n",
      "Epoch: 16127 mean train loss:  1.63282093e-05, mean val. loss:  3.25643229e+00\n",
      "Epoch: 16128 mean train loss:  1.60430791e-05, mean val. loss:  3.25685430e+00\n",
      "Epoch: 16129 mean train loss:  1.57473842e-05, mean val. loss:  3.25727606e+00\n",
      "Epoch: 16130 mean train loss:  1.62652868e-05, mean val. loss:  3.25761795e+00\n",
      "Epoch: 16131 mean train loss:  1.60454365e-05, mean val. loss:  3.25795007e+00\n",
      "Epoch: 16132 mean train loss:  1.59835618e-05, mean val. loss:  3.25827956e+00\n",
      "Epoch: 16133 mean train loss:  1.58390321e-05, mean val. loss:  3.25856137e+00\n",
      "Epoch: 16134 mean train loss:  1.62031793e-05, mean val. loss:  3.25877738e+00\n",
      "Epoch: 16135 mean train loss:  1.57012837e-05, mean val. loss:  3.25895905e+00\n",
      "Epoch: 16136 mean train loss:  1.58552721e-05, mean val. loss:  3.25915885e+00\n",
      "Epoch: 16137 mean train loss:  1.62619399e-05, mean val. loss:  3.25928926e+00\n",
      "Epoch: 16138 mean train loss:  1.61141506e-05, mean val. loss:  3.25938559e+00\n",
      "Epoch: 16139 mean train loss:  1.61362404e-05, mean val. loss:  3.25940180e+00\n",
      "Epoch: 16140 mean train loss:  1.60280033e-05, mean val. loss:  3.25938702e+00\n",
      "Epoch: 16141 mean train loss:  1.59726769e-05, mean val. loss:  3.25938129e+00\n",
      "Epoch: 16142 mean train loss:  1.60968048e-05, mean val. loss:  3.25935578e+00\n",
      "Epoch: 16143 mean train loss:  1.59218034e-05, mean val. loss:  3.25928950e+00\n",
      "Epoch: 16144 mean train loss:  1.62442448e-05, mean val. loss:  3.25921178e+00\n",
      "Epoch: 16145 mean train loss:  1.58512848e-05, mean val. loss:  3.25916862e+00\n",
      "Epoch: 16146 mean train loss:  1.60103082e-05, mean val. loss:  3.25910902e+00\n",
      "Epoch: 16147 mean train loss:  1.61117641e-05, mean val. loss:  3.25907874e+00\n",
      "Epoch: 16148 mean train loss:  1.58278272e-05, mean val. loss:  3.25911617e+00\n",
      "Epoch: 16149 mean train loss:  1.58102193e-05, mean val. loss:  3.25921321e+00\n",
      "Epoch: 16150 mean train loss:  1.60337659e-05, mean val. loss:  3.25939059e+00\n",
      "Epoch: 16151 mean train loss:  1.59670890e-05, mean val. loss:  3.25959373e+00\n",
      "Epoch: 16152 mean train loss:  1.60253840e-05, mean val. loss:  3.25986338e+00\n",
      "Epoch: 16153 mean train loss:  1.62291981e-05, mean val. loss:  3.26017666e+00\n",
      "Epoch: 16154 mean train loss:  1.62075739e-05, mean val. loss:  3.26047516e+00\n",
      "Epoch: 16155 mean train loss:  1.57922041e-05, mean val. loss:  3.26082110e+00\n",
      "Epoch: 16156 mean train loss:  1.60126656e-05, mean val. loss:  3.26117301e+00\n",
      "Epoch: 16157 mean train loss:  1.60859781e-05, mean val. loss:  3.26149225e+00\n",
      "Epoch: 16158 mean train loss:  1.57359464e-05, mean val. loss:  3.26184058e+00\n",
      "Epoch: 16159 mean train loss:  1.58376060e-05, mean val. loss:  3.26217961e+00\n",
      "Epoch: 16160 mean train loss:  1.63586810e-05, mean val. loss:  3.26247525e+00\n",
      "Epoch: 16161 mean train loss:  1.58697949e-05, mean val. loss:  3.26276660e+00\n",
      "Epoch: 16162 mean train loss:  1.61108910e-05, mean val. loss:  3.26302099e+00\n",
      "Epoch: 16163 mean train loss:  1.62016950e-05, mean val. loss:  3.26323462e+00\n",
      "Epoch: 16164 mean train loss:  1.61804492e-05, mean val. loss:  3.26339602e+00\n",
      "Epoch: 16165 mean train loss:  1.59328920e-05, mean val. loss:  3.26358056e+00\n",
      "Epoch: 16166 mean train loss:  1.60873460e-05, mean val. loss:  3.26375222e+00\n",
      "Epoch: 16167 mean train loss:  1.62472425e-05, mean val. loss:  3.26389527e+00\n",
      "Epoch: 16168 mean train loss:  1.59139163e-05, mean val. loss:  3.26406026e+00\n",
      "Epoch: 16169 mean train loss:  1.59980264e-05, mean val. loss:  3.26421070e+00\n",
      "Epoch: 16170 mean train loss:  1.57963077e-05, mean val. loss:  3.26439118e+00\n",
      "Epoch: 16171 mean train loss:  1.60293130e-05, mean val. loss:  3.26460052e+00\n",
      "Epoch: 16172 mean train loss:  1.60698837e-05, mean val. loss:  3.26483727e+00\n",
      "Epoch: 16173 mean train loss:  1.58538460e-05, mean val. loss:  3.26510000e+00\n",
      "Epoch: 16174 mean train loss:  1.59491901e-05, mean val. loss:  3.26542807e+00\n",
      "Epoch: 16175 mean train loss:  1.59855117e-05, mean val. loss:  3.26582217e+00\n",
      "Epoch: 16176 mean train loss:  1.56483729e-05, mean val. loss:  3.26628184e+00\n",
      "Epoch: 16177 mean train loss:  1.61821081e-05, mean val. loss:  3.26671481e+00\n",
      "Epoch: 16178 mean train loss:  1.60562340e-05, mean val. loss:  3.26713562e+00\n",
      "Epoch: 16179 mean train loss:  1.59531482e-05, mean val. loss:  3.26756263e+00\n",
      "Epoch: 16180 mean train loss:  1.60179334e-05, mean val. loss:  3.26801705e+00\n",
      "Epoch: 16181 mean train loss:  1.60351337e-05, mean val. loss:  3.26844406e+00\n",
      "Epoch: 16182 mean train loss:  1.58450275e-05, mean val. loss:  3.26885223e+00\n",
      "Epoch: 16183 mean train loss:  1.59732590e-05, mean val. loss:  3.26925159e+00\n",
      "Epoch: 16184 mean train loss:  1.59748597e-05, mean val. loss:  3.26959896e+00\n",
      "Epoch: 16185 mean train loss:  1.60010823e-05, mean val. loss:  3.26994681e+00\n",
      "Epoch: 16186 mean train loss:  1.59305055e-05, mean val. loss:  3.27026391e+00\n",
      "Epoch: 16187 mean train loss:  1.59532938e-05, mean val. loss:  3.27054024e+00\n",
      "Epoch: 16188 mean train loss:  1.61336502e-05, mean val. loss:  3.27076602e+00\n",
      "Epoch: 16189 mean train loss:  1.61979406e-05, mean val. loss:  3.27094197e+00\n",
      "Epoch: 16190 mean train loss:  1.57631584e-05, mean val. loss:  3.27111101e+00\n",
      "Epoch: 16191 mean train loss:  1.57562026e-05, mean val. loss:  3.27129626e+00\n",
      "Epoch: 16192 mean train loss:  1.62354845e-05, mean val. loss:  3.27143431e+00\n",
      "Epoch: 16193 mean train loss:  1.60081545e-05, mean val. loss:  3.27151108e+00\n",
      "Epoch: 16194 mean train loss:  1.58901967e-05, mean val. loss:  3.27161312e+00\n",
      "Epoch: 16195 mean train loss:  1.61409844e-05, mean val. loss:  3.27166986e+00\n",
      "Epoch: 16196 mean train loss:  1.59667106e-05, mean val. loss:  3.27171135e+00\n",
      "Epoch: 16197 mean train loss:  1.58047187e-05, mean val. loss:  3.27178073e+00\n",
      "Epoch: 16198 mean train loss:  1.58949406e-05, mean val. loss:  3.27181983e+00\n",
      "Epoch: 16199 mean train loss:  1.59484625e-05, mean val. loss:  3.27193594e+00\n",
      "Epoch: 16200 mean train loss:  1.62887154e-05, mean val. loss:  3.27206707e+00\n",
      "Epoch: 16201 mean train loss:  1.60636846e-05, mean val. loss:  3.27223516e+00\n",
      "Epoch: 16202 mean train loss:  1.60828640e-05, mean val. loss:  3.27237606e+00\n",
      "Epoch: 16203 mean train loss:  1.61708158e-05, mean val. loss:  3.27246833e+00\n",
      "Epoch: 16204 mean train loss:  1.55743328e-05, mean val. loss:  3.27268696e+00\n",
      "Epoch: 16205 mean train loss:  1.61993667e-05, mean val. loss:  3.27289200e+00\n",
      "Epoch: 16206 mean train loss:  1.60722411e-05, mean val. loss:  3.27311301e+00\n",
      "Epoch: 16207 mean train loss:  1.58307084e-05, mean val. loss:  3.27338171e+00\n",
      "Epoch: 16208 mean train loss:  1.57383329e-05, mean val. loss:  3.27372551e+00\n",
      "Epoch: 16209 mean train loss:  1.61652279e-05, mean val. loss:  3.27409959e+00\n",
      "Epoch: 16210 mean train loss:  1.59288174e-05, mean val. loss:  3.27449751e+00\n",
      "Epoch: 16211 mean train loss:  1.61836506e-05, mean val. loss:  3.27487493e+00\n",
      "Epoch: 16212 mean train loss:  1.60858326e-05, mean val. loss:  3.27525783e+00\n",
      "Epoch: 16213 mean train loss:  1.59778283e-05, mean val. loss:  3.27565336e+00\n",
      "Epoch: 16214 mean train loss:  1.61780044e-05, mean val. loss:  3.27602601e+00\n",
      "Epoch: 16215 mean train loss:  1.60432828e-05, mean val. loss:  3.27638769e+00\n",
      "Epoch: 16216 mean train loss:  1.59488409e-05, mean val. loss:  3.27678275e+00\n",
      "Epoch: 16217 mean train loss:  1.59160409e-05, mean val. loss:  3.27719498e+00\n",
      "Epoch: 16218 mean train loss:  1.58353068e-05, mean val. loss:  3.27761769e+00\n",
      "Epoch: 16219 mean train loss:  1.57226168e-05, mean val. loss:  3.27805901e+00\n",
      "Epoch: 16220 mean train loss:  1.58869079e-05, mean val. loss:  3.27853179e+00\n",
      "Epoch: 16221 mean train loss:  1.58949115e-05, mean val. loss:  3.27905703e+00\n",
      "Epoch: 16222 mean train loss:  1.62888027e-05, mean val. loss:  3.27949333e+00\n",
      "Epoch: 16223 mean train loss:  1.59130432e-05, mean val. loss:  3.27990127e+00\n",
      "Epoch: 16224 mean train loss:  1.61101052e-05, mean val. loss:  3.28030252e+00\n",
      "Epoch: 16225 mean train loss:  1.57121394e-05, mean val. loss:  3.28069401e+00\n",
      "Epoch: 16226 mean train loss:  1.57711620e-05, mean val. loss:  3.28105140e+00\n",
      "Epoch: 16227 mean train loss:  1.59840565e-05, mean val. loss:  3.28140616e+00\n",
      "Epoch: 16228 mean train loss:  1.62451761e-05, mean val. loss:  3.28173542e+00\n",
      "Epoch: 16229 mean train loss:  1.58949988e-05, mean val. loss:  3.28206682e+00\n",
      "Epoch: 16230 mean train loss:  1.60861528e-05, mean val. loss:  3.28237796e+00\n",
      "Epoch: 16231 mean train loss:  1.63442746e-05, mean val. loss:  3.28259778e+00\n",
      "Epoch: 16232 mean train loss:  1.59895280e-05, mean val. loss:  3.28279614e+00\n",
      "Epoch: 16233 mean train loss:  1.59638585e-05, mean val. loss:  3.28296494e+00\n",
      "Epoch: 16234 mean train loss:  1.58490147e-05, mean val. loss:  3.28312302e+00\n",
      "Epoch: 16235 mean train loss:  1.59279443e-05, mean val. loss:  3.28323269e+00\n",
      "Epoch: 16236 mean train loss:  1.60966592e-05, mean val. loss:  3.28331399e+00\n",
      "Epoch: 16237 mean train loss:  1.60018390e-05, mean val. loss:  3.28338480e+00\n",
      "Epoch: 16238 mean train loss:  1.58898474e-05, mean val. loss:  3.28346968e+00\n",
      "Epoch: 16239 mean train loss:  1.60904310e-05, mean val. loss:  3.28353047e+00\n",
      "Epoch: 16240 mean train loss:  1.59463962e-05, mean val. loss:  3.28359342e+00\n",
      "Epoch: 16241 mean train loss:  1.61629287e-05, mean val. loss:  3.28366613e+00\n",
      "Epoch: 16242 mean train loss:  1.59221527e-05, mean val. loss:  3.28380203e+00\n",
      "Epoch: 16243 mean train loss:  1.61450880e-05, mean val. loss:  3.28390670e+00\n",
      "Epoch: 16244 mean train loss:  1.56674650e-05, mean val. loss:  3.28407001e+00\n",
      "Epoch: 16245 mean train loss:  1.59632182e-05, mean val. loss:  3.28426695e+00\n",
      "Epoch: 16246 mean train loss:  1.57985487e-05, mean val. loss:  3.28452444e+00\n",
      "Epoch: 16247 mean train loss:  1.59045449e-05, mean val. loss:  3.28480959e+00\n",
      "Epoch: 16248 mean train loss:  1.60628988e-05, mean val. loss:  3.28514051e+00\n",
      "Epoch: 16249 mean train loss:  1.56422611e-05, mean val. loss:  3.28553247e+00\n",
      "Epoch: 16250 mean train loss:  1.58950861e-05, mean val. loss:  3.28591657e+00\n",
      "Epoch: 16251 mean train loss:  1.60194177e-05, mean val. loss:  3.28632760e+00\n",
      "Epoch: 16252 mean train loss:  1.59906049e-05, mean val. loss:  3.28677344e+00\n",
      "Epoch: 16253 mean train loss:  1.58330076e-05, mean val. loss:  3.28719139e+00\n",
      "Epoch: 16254 mean train loss:  1.58097246e-05, mean val. loss:  3.28763390e+00\n",
      "Epoch: 16255 mean train loss:  1.55562593e-05, mean val. loss:  3.28807807e+00\n",
      "Epoch: 16256 mean train loss:  1.58217445e-05, mean val. loss:  3.28853297e+00\n",
      "Epoch: 16257 mean train loss:  1.58106268e-05, mean val. loss:  3.28896713e+00\n",
      "Epoch: 16258 mean train loss:  1.59775373e-05, mean val. loss:  3.28937244e+00\n",
      "Epoch: 16259 mean train loss:  1.56696769e-05, mean val. loss:  3.28981137e+00\n",
      "Epoch: 16260 mean train loss:  1.58004696e-05, mean val. loss:  3.29026175e+00\n",
      "Epoch: 16261 mean train loss:  1.57772447e-05, mean val. loss:  3.29066849e+00\n",
      "Epoch: 16262 mean train loss:  1.59050687e-05, mean val. loss:  3.29104304e+00\n",
      "Epoch: 16263 mean train loss:  1.57245668e-05, mean val. loss:  3.29141784e+00\n",
      "Epoch: 16264 mean train loss:  1.59635092e-05, mean val. loss:  3.29180574e+00\n",
      "Epoch: 16265 mean train loss:  1.60712225e-05, mean val. loss:  3.29220343e+00\n",
      "Epoch: 16266 mean train loss:  1.60421769e-05, mean val. loss:  3.29258251e+00\n",
      "Epoch: 16267 mean train loss:  1.55615853e-05, mean val. loss:  3.29301691e+00\n",
      "Epoch: 16268 mean train loss:  1.58678158e-05, mean val. loss:  3.29344177e+00\n",
      "Epoch: 16269 mean train loss:  1.58682524e-05, mean val. loss:  3.29382777e+00\n",
      "Epoch: 16270 mean train loss:  1.60945056e-05, mean val. loss:  3.29417920e+00\n",
      "Epoch: 16271 mean train loss:  1.59764022e-05, mean val. loss:  3.29449797e+00\n",
      "Epoch: 16272 mean train loss:  1.60899071e-05, mean val. loss:  3.29478121e+00\n",
      "Epoch: 16273 mean train loss:  1.57251488e-05, mean val. loss:  3.29506063e+00\n",
      "Epoch: 16274 mean train loss:  1.61444477e-05, mean val. loss:  3.29530406e+00\n",
      "Epoch: 16275 mean train loss:  1.59513438e-05, mean val. loss:  3.29545093e+00\n",
      "Epoch: 16276 mean train loss:  1.58769544e-05, mean val. loss:  3.29559994e+00\n",
      "Epoch: 16277 mean train loss:  1.58200855e-05, mean val. loss:  3.29573345e+00\n",
      "Epoch: 16278 mean train loss:  1.57570175e-05, mean val. loss:  3.29586530e+00\n",
      "Epoch: 16279 mean train loss:  1.59210467e-05, mean val. loss:  3.29599190e+00\n",
      "Epoch: 16280 mean train loss:  1.60532945e-05, mean val. loss:  3.29611778e+00\n",
      "Epoch: 16281 mean train loss:  1.54041918e-05, mean val. loss:  3.29628158e+00\n",
      "Epoch: 16282 mean train loss:  1.57396425e-05, mean val. loss:  3.29649758e+00\n",
      "Epoch: 16283 mean train loss:  1.61048083e-05, mean val. loss:  3.29670882e+00\n",
      "Epoch: 16284 mean train loss:  1.57674658e-05, mean val. loss:  3.29695153e+00\n",
      "Epoch: 16285 mean train loss:  1.57470931e-05, mean val. loss:  3.29720688e+00\n",
      "Epoch: 16286 mean train loss:  1.60213094e-05, mean val. loss:  3.29744864e+00\n",
      "Epoch: 16287 mean train loss:  1.60946220e-05, mean val. loss:  3.29766011e+00\n",
      "Epoch: 16288 mean train loss:  1.61057978e-05, mean val. loss:  3.29787874e+00\n",
      "Epoch: 16289 mean train loss:  1.57900504e-05, mean val. loss:  3.29810882e+00\n",
      "Epoch: 16290 mean train loss:  1.60323107e-05, mean val. loss:  3.29832411e+00\n",
      "Epoch: 16291 mean train loss:  1.59940973e-05, mean val. loss:  3.29859352e+00\n",
      "Epoch: 16292 mean train loss:  1.59436022e-05, mean val. loss:  3.29885244e+00\n",
      "Epoch: 16293 mean train loss:  1.58269540e-05, mean val. loss:  3.29909921e+00\n",
      "Epoch: 16294 mean train loss:  1.57151080e-05, mean val. loss:  3.29936767e+00\n",
      "Epoch: 16295 mean train loss:  1.60137133e-05, mean val. loss:  3.29964733e+00\n",
      "Epoch: 16296 mean train loss:  1.56560563e-05, mean val. loss:  3.29998636e+00\n",
      "Epoch: 16297 mean train loss:  1.59378687e-05, mean val. loss:  3.30031204e+00\n",
      "Epoch: 16298 mean train loss:  1.57813774e-05, mean val. loss:  3.30062413e+00\n",
      "Epoch: 16299 mean train loss:  1.60258205e-05, mean val. loss:  3.30088568e+00\n",
      "Epoch: 16300 mean train loss:  1.58683397e-05, mean val. loss:  3.30118012e+00\n",
      "Epoch: 16301 mean train loss:  1.59055635e-05, mean val. loss:  3.30151749e+00\n",
      "Epoch: 16302 mean train loss:  1.58778857e-05, mean val. loss:  3.30184746e+00\n",
      "Epoch: 16303 mean train loss:  1.61066127e-05, mean val. loss:  3.30219388e+00\n",
      "Epoch: 16304 mean train loss:  1.55715970e-05, mean val. loss:  3.30258107e+00\n",
      "Epoch: 16305 mean train loss:  1.58140028e-05, mean val. loss:  3.30294085e+00\n",
      "Epoch: 16306 mean train loss:  1.59986375e-05, mean val. loss:  3.30327249e+00\n",
      "Epoch: 16307 mean train loss:  1.57622271e-05, mean val. loss:  3.30355120e+00\n",
      "Epoch: 16308 mean train loss:  1.60539930e-05, mean val. loss:  3.30378819e+00\n",
      "Epoch: 16309 mean train loss:  1.60317286e-05, mean val. loss:  3.30399108e+00\n",
      "Epoch: 16310 mean train loss:  1.57166796e-05, mean val. loss:  3.30417132e+00\n",
      "Epoch: 16311 mean train loss:  1.60439813e-05, mean val. loss:  3.30430937e+00\n",
      "Epoch: 16312 mean train loss:  1.58854236e-05, mean val. loss:  3.30440521e+00\n",
      "Epoch: 16313 mean train loss:  1.58061157e-05, mean val. loss:  3.30448985e+00\n",
      "Epoch: 16314 mean train loss:  1.56564347e-05, mean val. loss:  3.30458736e+00\n",
      "Epoch: 16315 mean train loss:  1.55540765e-05, mean val. loss:  3.30470490e+00\n",
      "Epoch: 16316 mean train loss:  1.60599011e-05, mean val. loss:  3.30481815e+00\n",
      "Epoch: 16317 mean train loss:  1.58738112e-05, mean val. loss:  3.30495214e+00\n",
      "Epoch: 16318 mean train loss:  1.62056240e-05, mean val. loss:  3.30504942e+00\n",
      "Epoch: 16319 mean train loss:  1.58791372e-05, mean val. loss:  3.30517244e+00\n",
      "Epoch: 16320 mean train loss:  1.59312331e-05, mean val. loss:  3.30533290e+00\n",
      "Epoch: 16321 mean train loss:  1.57590257e-05, mean val. loss:  3.30556989e+00\n",
      "Epoch: 16322 mean train loss:  1.60009949e-05, mean val. loss:  3.30582714e+00\n",
      "Epoch: 16323 mean train loss:  1.58032053e-05, mean val. loss:  3.30608749e+00\n",
      "Epoch: 16324 mean train loss:  1.59768970e-05, mean val. loss:  3.30633092e+00\n",
      "Epoch: 16325 mean train loss:  1.61048956e-05, mean val. loss:  3.30660295e+00\n",
      "Epoch: 16326 mean train loss:  1.60000927e-05, mean val. loss:  3.30689955e+00\n",
      "Epoch: 16327 mean train loss:  1.58832700e-05, mean val. loss:  3.30724430e+00\n",
      "Epoch: 16328 mean train loss:  1.59450283e-05, mean val. loss:  3.30759048e+00\n",
      "Epoch: 16329 mean train loss:  1.57427276e-05, mean val. loss:  3.30797362e+00\n",
      "Epoch: 16330 mean train loss:  1.60002674e-05, mean val. loss:  3.30833483e+00\n",
      "Epoch: 16331 mean train loss:  1.56578899e-05, mean val. loss:  3.30871820e+00\n",
      "Epoch: 16332 mean train loss:  1.58905459e-05, mean val. loss:  3.30910373e+00\n",
      "Epoch: 16333 mean train loss:  1.59581250e-05, mean val. loss:  3.30941701e+00\n",
      "Epoch: 16334 mean train loss:  1.56792521e-05, mean val. loss:  3.30978632e+00\n",
      "Epoch: 16335 mean train loss:  1.56655733e-05, mean val. loss:  3.31015849e+00\n",
      "Epoch: 16336 mean train loss:  1.58072799e-05, mean val. loss:  3.31051898e+00\n",
      "Epoch: 16337 mean train loss:  1.57154573e-05, mean val. loss:  3.31085968e+00\n",
      "Epoch: 16338 mean train loss:  1.58592011e-05, mean val. loss:  3.31114078e+00\n",
      "Epoch: 16339 mean train loss:  1.61610369e-05, mean val. loss:  3.31136250e+00\n",
      "Epoch: 16340 mean train loss:  1.63408113e-05, mean val. loss:  3.31153202e+00\n",
      "Epoch: 16341 mean train loss:  1.59895862e-05, mean val. loss:  3.31165576e+00\n",
      "Epoch: 16342 mean train loss:  1.58162729e-05, mean val. loss:  3.31177354e+00\n",
      "Epoch: 16343 mean train loss:  1.59291085e-05, mean val. loss:  3.31184220e+00\n",
      "Epoch: 16344 mean train loss:  1.59381598e-05, mean val. loss:  3.31188250e+00\n",
      "Epoch: 16345 mean train loss:  1.60128693e-05, mean val. loss:  3.31189966e+00\n",
      "Epoch: 16346 mean train loss:  1.60549534e-05, mean val. loss:  3.31188273e+00\n",
      "Epoch: 16347 mean train loss:  1.59683987e-05, mean val. loss:  3.31188893e+00\n",
      "Epoch: 16348 mean train loss:  1.61405187e-05, mean val. loss:  3.31184006e+00\n",
      "Epoch: 16349 mean train loss:  1.59761112e-05, mean val. loss:  3.31183290e+00\n",
      "Epoch: 16350 mean train loss:  1.55810849e-05, mean val. loss:  3.31187224e+00\n",
      "Epoch: 16351 mean train loss:  1.58500043e-05, mean val. loss:  3.31193733e+00\n",
      "Epoch: 16352 mean train loss:  1.61033240e-05, mean val. loss:  3.31202626e+00\n",
      "Epoch: 16353 mean train loss:  1.60339114e-05, mean val. loss:  3.31213689e+00\n",
      "Epoch: 16354 mean train loss:  1.56310853e-05, mean val. loss:  3.31235600e+00\n",
      "Epoch: 16355 mean train loss:  1.59951160e-05, mean val. loss:  3.31261730e+00\n",
      "Epoch: 16356 mean train loss:  1.55291054e-05, mean val. loss:  3.31296587e+00\n",
      "Epoch: 16357 mean train loss:  1.56279712e-05, mean val. loss:  3.31344199e+00\n",
      "Epoch: 16358 mean train loss:  1.58254115e-05, mean val. loss:  3.31394792e+00\n",
      "Epoch: 16359 mean train loss:  1.57858594e-05, mean val. loss:  3.31451583e+00\n",
      "Epoch: 16360 mean train loss:  1.59986957e-05, mean val. loss:  3.31510735e+00\n",
      "Epoch: 16361 mean train loss:  1.58289040e-05, mean val. loss:  3.31572461e+00\n",
      "Epoch: 16362 mean train loss:  1.56954338e-05, mean val. loss:  3.31640220e+00\n",
      "Epoch: 16363 mean train loss:  1.59232586e-05, mean val. loss:  3.31709933e+00\n",
      "Epoch: 16364 mean train loss:  1.59500923e-05, mean val. loss:  3.31778407e+00\n",
      "Epoch: 16365 mean train loss:  1.60966010e-05, mean val. loss:  3.31840396e+00\n",
      "Epoch: 16366 mean train loss:  1.58246548e-05, mean val. loss:  3.31899619e+00\n",
      "Epoch: 16367 mean train loss:  1.61241624e-05, mean val. loss:  3.31949902e+00\n",
      "Epoch: 16368 mean train loss:  1.60958734e-05, mean val. loss:  3.31987786e+00\n",
      "Epoch: 16369 mean train loss:  1.58986077e-05, mean val. loss:  3.32024860e+00\n",
      "Epoch: 16370 mean train loss:  1.58064940e-05, mean val. loss:  3.32053590e+00\n",
      "Epoch: 16371 mean train loss:  1.57515169e-05, mean val. loss:  3.32084489e+00\n",
      "Epoch: 16372 mean train loss:  1.56917959e-05, mean val. loss:  3.32113051e+00\n",
      "Epoch: 16373 mean train loss:  1.58197363e-05, mean val. loss:  3.32138681e+00\n",
      "Epoch: 16374 mean train loss:  1.61925564e-05, mean val. loss:  3.32160664e+00\n",
      "Epoch: 16375 mean train loss:  1.57366449e-05, mean val. loss:  3.32179666e+00\n",
      "Epoch: 16376 mean train loss:  1.59534975e-05, mean val. loss:  3.32200432e+00\n",
      "Epoch: 16377 mean train loss:  1.58832700e-05, mean val. loss:  3.32222843e+00\n",
      "Epoch: 16378 mean train loss:  1.57406612e-05, mean val. loss:  3.32249165e+00\n",
      "Epoch: 16379 mean train loss:  1.54899317e-05, mean val. loss:  3.32280087e+00\n",
      "Epoch: 16380 mean train loss:  1.56879833e-05, mean val. loss:  3.32313704e+00\n",
      "Epoch: 16381 mean train loss:  1.58108014e-05, mean val. loss:  3.32353425e+00\n",
      "Epoch: 16382 mean train loss:  1.57955510e-05, mean val. loss:  3.32397199e+00\n",
      "Epoch: 16383 mean train loss:  1.57549221e-05, mean val. loss:  3.32444072e+00\n",
      "Epoch: 16384 mean train loss:  1.60753843e-05, mean val. loss:  3.32490301e+00\n",
      "Epoch: 16385 mean train loss:  1.59219489e-05, mean val. loss:  3.32535076e+00\n",
      "Epoch: 16386 mean train loss:  1.57449977e-05, mean val. loss:  3.32580090e+00\n",
      "Epoch: 16387 mean train loss:  1.58852199e-05, mean val. loss:  3.32623577e+00\n",
      "Epoch: 16388 mean train loss:  1.59537594e-05, mean val. loss:  3.32661319e+00\n",
      "Epoch: 16389 mean train loss:  1.57829199e-05, mean val. loss:  3.32693648e+00\n",
      "Epoch: 16390 mean train loss:  1.57316099e-05, mean val. loss:  3.32720590e+00\n",
      "Epoch: 16391 mean train loss:  1.56865863e-05, mean val. loss:  3.32742929e+00\n",
      "Epoch: 16392 mean train loss:  1.59170886e-05, mean val. loss:  3.32760358e+00\n",
      "Epoch: 16393 mean train loss:  1.58193579e-05, mean val. loss:  3.32774234e+00\n",
      "Epoch: 16394 mean train loss:  1.58848707e-05, mean val. loss:  3.32789493e+00\n",
      "Epoch: 16395 mean train loss:  1.59502961e-05, mean val. loss:  3.32797623e+00\n",
      "Epoch: 16396 mean train loss:  1.57812610e-05, mean val. loss:  3.32803369e+00\n",
      "Epoch: 16397 mean train loss:  1.57936884e-05, mean val. loss:  3.32807016e+00\n",
      "Epoch: 16398 mean train loss:  1.56477327e-05, mean val. loss:  3.32810211e+00\n",
      "Epoch: 16399 mean train loss:  1.59290503e-05, mean val. loss:  3.32809711e+00\n",
      "Epoch: 16400 mean train loss:  1.61873177e-05, mean val. loss:  3.32807350e+00\n",
      "Epoch: 16401 mean train loss:  1.56604219e-05, mean val. loss:  3.32813239e+00\n",
      "Epoch: 16402 mean train loss:  1.60250638e-05, mean val. loss:  3.32818484e+00\n",
      "Epoch: 16403 mean train loss:  1.59055926e-05, mean val. loss:  3.32824945e+00\n",
      "Epoch: 16404 mean train loss:  1.57720351e-05, mean val. loss:  3.32834625e+00\n",
      "Epoch: 16405 mean train loss:  1.60246564e-05, mean val. loss:  3.32840419e+00\n",
      "Epoch: 16406 mean train loss:  1.57758186e-05, mean val. loss:  3.32853127e+00\n",
      "Epoch: 16407 mean train loss:  1.58875773e-05, mean val. loss:  3.32866931e+00\n",
      "Epoch: 16408 mean train loss:  1.58361800e-05, mean val. loss:  3.32882071e+00\n",
      "Epoch: 16409 mean train loss:  1.62580109e-05, mean val. loss:  3.32892466e+00\n",
      "Epoch: 16410 mean train loss:  1.59248593e-05, mean val. loss:  3.32905102e+00\n",
      "Epoch: 16411 mean train loss:  1.57788745e-05, mean val. loss:  3.32920742e+00\n",
      "Epoch: 16412 mean train loss:  1.56179594e-05, mean val. loss:  3.32936072e+00\n",
      "Epoch: 16413 mean train loss:  1.60806521e-05, mean val. loss:  3.32952356e+00\n",
      "Epoch: 16414 mean train loss:  1.57820468e-05, mean val. loss:  3.32974839e+00\n",
      "Epoch: 16415 mean train loss:  1.57055911e-05, mean val. loss:  3.32997298e+00\n",
      "Epoch: 16416 mean train loss:  1.56689493e-05, mean val. loss:  3.33028340e+00\n",
      "Epoch: 16417 mean train loss:  1.60505006e-05, mean val. loss:  3.33061695e+00\n",
      "Epoch: 16418 mean train loss:  1.55771268e-05, mean val. loss:  3.33106422e+00\n",
      "Epoch: 16419 mean train loss:  1.58261100e-05, mean val. loss:  3.33156443e+00\n",
      "Epoch: 16420 mean train loss:  1.56851602e-05, mean val. loss:  3.33211231e+00\n",
      "Epoch: 16421 mean train loss:  1.57309987e-05, mean val. loss:  3.33268785e+00\n",
      "Epoch: 16422 mean train loss:  1.59164483e-05, mean val. loss:  3.33323669e+00\n",
      "Epoch: 16423 mean train loss:  1.60281488e-05, mean val. loss:  3.33376837e+00\n",
      "Epoch: 16424 mean train loss:  1.59788760e-05, mean val. loss:  3.33427548e+00\n",
      "Epoch: 16425 mean train loss:  1.56673486e-05, mean val. loss:  3.33479738e+00\n",
      "Epoch: 16426 mean train loss:  1.56188617e-05, mean val. loss:  3.33536410e+00\n",
      "Epoch: 16427 mean train loss:  1.57061149e-05, mean val. loss:  3.33594966e+00\n",
      "Epoch: 16428 mean train loss:  1.60046620e-05, mean val. loss:  3.33648682e+00\n",
      "Epoch: 16429 mean train loss:  1.58805924e-05, mean val. loss:  3.33703804e+00\n",
      "Epoch: 16430 mean train loss:  1.61794305e-05, mean val. loss:  3.33752131e+00\n",
      "Epoch: 16431 mean train loss:  1.58884795e-05, mean val. loss:  3.33794403e+00\n",
      "Epoch: 16432 mean train loss:  1.55928137e-05, mean val. loss:  3.33836603e+00\n",
      "Epoch: 16433 mean train loss:  1.58400508e-05, mean val. loss:  3.33873892e+00\n",
      "Epoch: 16434 mean train loss:  1.59943011e-05, mean val. loss:  3.33909726e+00\n",
      "Epoch: 16435 mean train loss:  1.59842311e-05, mean val. loss:  3.33938479e+00\n",
      "Epoch: 16436 mean train loss:  1.58098410e-05, mean val. loss:  3.33964276e+00\n",
      "Epoch: 16437 mean train loss:  1.61017233e-05, mean val. loss:  3.33980918e+00\n",
      "Epoch: 16438 mean train loss:  1.58289040e-05, mean val. loss:  3.33995175e+00\n",
      "Epoch: 16439 mean train loss:  1.60091731e-05, mean val. loss:  3.34010768e+00\n",
      "Epoch: 16440 mean train loss:  1.59218034e-05, mean val. loss:  3.34025645e+00\n",
      "Epoch: 16441 mean train loss:  1.56650785e-05, mean val. loss:  3.34043813e+00\n",
      "Epoch: 16442 mean train loss:  1.55519811e-05, mean val. loss:  3.34062338e+00\n",
      "Epoch: 16443 mean train loss:  1.58141775e-05, mean val. loss:  3.34083939e+00\n",
      "Epoch: 16444 mean train loss:  1.59589399e-05, mean val. loss:  3.34108901e+00\n",
      "Epoch: 16445 mean train loss:  1.61709031e-05, mean val. loss:  3.34130692e+00\n",
      "Epoch: 16446 mean train loss:  1.60074269e-05, mean val. loss:  3.34147573e+00\n",
      "Epoch: 16447 mean train loss:  1.59586780e-05, mean val. loss:  3.34167266e+00\n",
      "Epoch: 16448 mean train loss:  1.58073672e-05, mean val. loss:  3.34188914e+00\n",
      "Epoch: 16449 mean train loss:  1.59801566e-05, mean val. loss:  3.34205461e+00\n",
      "Epoch: 16450 mean train loss:  1.60066411e-05, mean val. loss:  3.34220123e+00\n",
      "Epoch: 16451 mean train loss:  1.56413589e-05, mean val. loss:  3.34243321e+00\n",
      "Epoch: 16452 mean train loss:  1.60103955e-05, mean val. loss:  3.34266877e+00\n",
      "Epoch: 16453 mean train loss:  1.60812633e-05, mean val. loss:  3.34289765e+00\n",
      "Epoch: 16454 mean train loss:  1.57270697e-05, mean val. loss:  3.34311509e+00\n",
      "Epoch: 16455 mean train loss:  1.57881295e-05, mean val. loss:  3.34331799e+00\n",
      "Epoch: 16456 mean train loss:  1.58825424e-05, mean val. loss:  3.34351921e+00\n",
      "Epoch: 16457 mean train loss:  1.59693300e-05, mean val. loss:  3.34370589e+00\n",
      "Epoch: 16458 mean train loss:  1.56224996e-05, mean val. loss:  3.34394264e+00\n",
      "Epoch: 16459 mean train loss:  1.57355971e-05, mean val. loss:  3.34416151e+00\n",
      "Epoch: 16460 mean train loss:  1.55274174e-05, mean val. loss:  3.34440541e+00\n",
      "Epoch: 16461 mean train loss:  1.57470640e-05, mean val. loss:  3.34465504e+00\n",
      "Epoch: 16462 mean train loss:  1.57387112e-05, mean val. loss:  3.34494543e+00\n",
      "Epoch: 16463 mean train loss:  1.55228190e-05, mean val. loss:  3.34528279e+00\n",
      "Epoch: 16464 mean train loss:  1.56467140e-05, mean val. loss:  3.34562325e+00\n",
      "Epoch: 16465 mean train loss:  1.58890907e-05, mean val. loss:  3.34594870e+00\n",
      "Epoch: 16466 mean train loss:  1.57429313e-05, mean val. loss:  3.34629822e+00\n",
      "Epoch: 16467 mean train loss:  1.56020396e-05, mean val. loss:  3.34661627e+00\n",
      "Epoch: 16468 mean train loss:  1.59031770e-05, mean val. loss:  3.34692049e+00\n",
      "Epoch: 16469 mean train loss:  1.57539034e-05, mean val. loss:  3.34729171e+00\n",
      "Epoch: 16470 mean train loss:  1.60395284e-05, mean val. loss:  3.34766483e+00\n",
      "Epoch: 16471 mean train loss:  1.56513415e-05, mean val. loss:  3.34800220e+00\n",
      "Epoch: 16472 mean train loss:  1.56393216e-05, mean val. loss:  3.34833169e+00\n",
      "Epoch: 16473 mean train loss:  1.59043702e-05, mean val. loss:  3.34862542e+00\n",
      "Epoch: 16474 mean train loss:  1.57517497e-05, mean val. loss:  3.34894919e+00\n",
      "Epoch: 16475 mean train loss:  1.59467163e-05, mean val. loss:  3.34920526e+00\n",
      "Epoch: 16476 mean train loss:  1.57421455e-05, mean val. loss:  3.34945536e+00\n",
      "Epoch: 16477 mean train loss:  1.56518363e-05, mean val. loss:  3.34970403e+00\n",
      "Epoch: 16478 mean train loss:  1.58336770e-05, mean val. loss:  3.34994721e+00\n",
      "Epoch: 16479 mean train loss:  1.56899041e-05, mean val. loss:  3.35016680e+00\n",
      "Epoch: 16480 mean train loss:  1.58059993e-05, mean val. loss:  3.35036492e+00\n",
      "Epoch: 16481 mean train loss:  1.59198826e-05, mean val. loss:  3.35047436e+00\n",
      "Epoch: 16482 mean train loss:  1.59812043e-05, mean val. loss:  3.35055804e+00\n",
      "Epoch: 16483 mean train loss:  1.59031188e-05, mean val. loss:  3.35064220e+00\n",
      "Epoch: 16484 mean train loss:  1.57130416e-05, mean val. loss:  3.35074496e+00\n",
      "Epoch: 16485 mean train loss:  1.60736963e-05, mean val. loss:  3.35081148e+00\n",
      "Epoch: 16486 mean train loss:  1.55467715e-05, mean val. loss:  3.35090876e+00\n",
      "Epoch: 16487 mean train loss:  1.60329218e-05, mean val. loss:  3.35094357e+00\n",
      "Epoch: 16488 mean train loss:  1.60247728e-05, mean val. loss:  3.35095572e+00\n",
      "Epoch: 16489 mean train loss:  1.56974711e-05, mean val. loss:  3.35093355e+00\n",
      "Epoch: 16490 mean train loss:  1.58330076e-05, mean val. loss:  3.35089660e+00\n",
      "Epoch: 16491 mean train loss:  1.59023912e-05, mean val. loss:  3.35084248e+00\n",
      "Epoch: 16492 mean train loss:  1.54825975e-05, mean val. loss:  3.35085416e+00\n",
      "Epoch: 16493 mean train loss:  1.60313211e-05, mean val. loss:  3.35087419e+00\n",
      "Epoch: 16494 mean train loss:  1.56311435e-05, mean val. loss:  3.35091925e+00\n",
      "Epoch: 16495 mean train loss:  1.57638569e-05, mean val. loss:  3.35097384e+00\n",
      "Epoch: 16496 mean train loss:  1.57579198e-05, mean val. loss:  3.35111547e+00\n",
      "Epoch: 16497 mean train loss:  1.60637428e-05, mean val. loss:  3.35123992e+00\n",
      "Epoch: 16498 mean train loss:  1.57969189e-05, mean val. loss:  3.35141349e+00\n",
      "Epoch: 16499 mean train loss:  1.56026799e-05, mean val. loss:  3.35166454e+00\n",
      "Epoch: 16500 mean train loss:  1.59611227e-05, mean val. loss:  3.35194969e+00\n",
      "Epoch: 16501 mean train loss:  1.56251772e-05, mean val. loss:  3.35234809e+00\n",
      "Epoch: 16502 mean train loss:  1.59644405e-05, mean val. loss:  3.35275435e+00\n",
      "Epoch: 16503 mean train loss:  1.59680494e-05, mean val. loss:  3.35320783e+00\n",
      "Epoch: 16504 mean train loss:  1.61035568e-05, mean val. loss:  3.35366535e+00\n",
      "Epoch: 16505 mean train loss:  1.56175520e-05, mean val. loss:  3.35416126e+00\n",
      "Epoch: 16506 mean train loss:  1.58651965e-05, mean val. loss:  3.35465813e+00\n",
      "Epoch: 16507 mean train loss:  1.58937182e-05, mean val. loss:  3.35512519e+00\n",
      "Epoch: 16508 mean train loss:  1.57893228e-05, mean val. loss:  3.35560799e+00\n",
      "Epoch: 16509 mean train loss:  1.58033799e-05, mean val. loss:  3.35603881e+00\n",
      "Epoch: 16510 mean train loss:  1.58663315e-05, mean val. loss:  3.35644245e+00\n",
      "Epoch: 16511 mean train loss:  1.55762827e-05, mean val. loss:  3.35692358e+00\n",
      "Epoch: 16512 mean train loss:  1.59056799e-05, mean val. loss:  3.35733795e+00\n",
      "Epoch: 16513 mean train loss:  1.56335882e-05, mean val. loss:  3.35778642e+00\n",
      "Epoch: 16514 mean train loss:  1.55766611e-05, mean val. loss:  3.35817814e+00\n",
      "Epoch: 16515 mean train loss:  1.59461051e-05, mean val. loss:  3.35854292e+00\n",
      "Epoch: 16516 mean train loss:  1.62637443e-05, mean val. loss:  3.35881042e+00\n",
      "Epoch: 16517 mean train loss:  1.56760216e-05, mean val. loss:  3.35909891e+00\n",
      "Epoch: 16518 mean train loss:  1.56734604e-05, mean val. loss:  3.35940504e+00\n",
      "Epoch: 16519 mean train loss:  1.60006166e-05, mean val. loss:  3.35964370e+00\n",
      "Epoch: 16520 mean train loss:  1.56174065e-05, mean val. loss:  3.35989594e+00\n",
      "Epoch: 16521 mean train loss:  1.57479953e-05, mean val. loss:  3.36014080e+00\n",
      "Epoch: 16522 mean train loss:  1.59982010e-05, mean val. loss:  3.36035323e+00\n",
      "Epoch: 16523 mean train loss:  1.59571355e-05, mean val. loss:  3.36055350e+00\n",
      "Epoch: 16524 mean train loss:  1.57663890e-05, mean val. loss:  3.36069632e+00\n",
      "Epoch: 16525 mean train loss:  1.56649039e-05, mean val. loss:  3.36086249e+00\n",
      "Epoch: 16526 mean train loss:  1.54959271e-05, mean val. loss:  3.36110568e+00\n",
      "Epoch: 16527 mean train loss:  1.58436305e-05, mean val. loss:  3.36134076e+00\n",
      "Epoch: 16528 mean train loss:  1.55289599e-05, mean val. loss:  3.36158562e+00\n",
      "Epoch: 16529 mean train loss:  1.58737530e-05, mean val. loss:  3.36180949e+00\n",
      "Epoch: 16530 mean train loss:  1.56255555e-05, mean val. loss:  3.36203623e+00\n",
      "Epoch: 16531 mean train loss:  1.57987233e-05, mean val. loss:  3.36221528e+00\n",
      "Epoch: 16532 mean train loss:  1.57295726e-05, mean val. loss:  3.36245203e+00\n",
      "Epoch: 16533 mean train loss:  1.62456417e-05, mean val. loss:  3.36261582e+00\n",
      "Epoch: 16534 mean train loss:  1.58357143e-05, mean val. loss:  3.36277318e+00\n",
      "Epoch: 16535 mean train loss:  1.57735776e-05, mean val. loss:  3.36293983e+00\n",
      "Epoch: 16536 mean train loss:  1.56968017e-05, mean val. loss:  3.36308694e+00\n",
      "Epoch: 16537 mean train loss:  1.58496550e-05, mean val. loss:  3.36325765e+00\n",
      "Epoch: 16538 mean train loss:  1.59303600e-05, mean val. loss:  3.36340308e+00\n",
      "Epoch: 16539 mean train loss:  1.57342874e-05, mean val. loss:  3.36355352e+00\n",
      "Epoch: 16540 mean train loss:  1.58222974e-05, mean val. loss:  3.36368704e+00\n",
      "Epoch: 16541 mean train loss:  1.55396701e-05, mean val. loss:  3.36383152e+00\n",
      "Epoch: 16542 mean train loss:  1.56956958e-05, mean val. loss:  3.36397839e+00\n",
      "Epoch: 16543 mean train loss:  1.57654285e-05, mean val. loss:  3.36410451e+00\n",
      "Epoch: 16544 mean train loss:  1.56850729e-05, mean val. loss:  3.36426592e+00\n",
      "Epoch: 16545 mean train loss:  1.58209295e-05, mean val. loss:  3.36445975e+00\n",
      "Epoch: 16546 mean train loss:  1.56826572e-05, mean val. loss:  3.36465645e+00\n",
      "Epoch: 16547 mean train loss:  1.57298928e-05, mean val. loss:  3.36495829e+00\n",
      "Epoch: 16548 mean train loss:  1.56897004e-05, mean val. loss:  3.36533308e+00\n",
      "Epoch: 16549 mean train loss:  1.55324524e-05, mean val. loss:  3.36577535e+00\n",
      "Epoch: 16550 mean train loss:  1.58845796e-05, mean val. loss:  3.36618876e+00\n",
      "Epoch: 16551 mean train loss:  1.59875199e-05, mean val. loss:  3.36662030e+00\n",
      "Epoch: 16552 mean train loss:  1.57426985e-05, mean val. loss:  3.36705995e+00\n",
      "Epoch: 16553 mean train loss:  1.57244212e-05, mean val. loss:  3.36751461e+00\n",
      "Epoch: 16554 mean train loss:  1.59899355e-05, mean val. loss:  3.36795354e+00\n",
      "Epoch: 16555 mean train loss:  1.59366464e-05, mean val. loss:  3.36838412e+00\n",
      "Epoch: 16556 mean train loss:  1.55654561e-05, mean val. loss:  3.36878300e+00\n",
      "Epoch: 16557 mean train loss:  1.56870519e-05, mean val. loss:  3.36913538e+00\n",
      "Epoch: 16558 mean train loss:  1.60828058e-05, mean val. loss:  3.36942482e+00\n",
      "Epoch: 16559 mean train loss:  1.56544847e-05, mean val. loss:  3.36971235e+00\n",
      "Epoch: 16560 mean train loss:  1.56251481e-05, mean val. loss:  3.36993432e+00\n",
      "Epoch: 16561 mean train loss:  1.56651076e-05, mean val. loss:  3.37013865e+00\n",
      "Epoch: 16562 mean train loss:  1.57845789e-05, mean val. loss:  3.37028313e+00\n",
      "Epoch: 16563 mean train loss:  1.55390589e-05, mean val. loss:  3.37041664e+00\n",
      "Epoch: 16564 mean train loss:  1.57075119e-05, mean val. loss:  3.37054086e+00\n",
      "Epoch: 16565 mean train loss:  1.55653397e-05, mean val. loss:  3.37069559e+00\n",
      "Epoch: 16566 mean train loss:  1.58884213e-05, mean val. loss:  3.37083292e+00\n",
      "Epoch: 16567 mean train loss:  1.59198826e-05, mean val. loss:  3.37095881e+00\n",
      "Epoch: 16568 mean train loss:  1.59530027e-05, mean val. loss:  3.37110996e+00\n",
      "Epoch: 16569 mean train loss:  1.59469782e-05, mean val. loss:  3.37124372e+00\n",
      "Epoch: 16570 mean train loss:  1.55901653e-05, mean val. loss:  3.37140036e+00\n",
      "Epoch: 16571 mean train loss:  1.57516624e-05, mean val. loss:  3.37159872e+00\n",
      "Epoch: 16572 mean train loss:  1.59092888e-05, mean val. loss:  3.37180138e+00\n",
      "Epoch: 16573 mean train loss:  1.58178445e-05, mean val. loss:  3.37198496e+00\n",
      "Epoch: 16574 mean train loss:  1.56537280e-05, mean val. loss:  3.37215352e+00\n",
      "Epoch: 16575 mean train loss:  1.57240720e-05, mean val. loss:  3.37233567e+00\n",
      "Epoch: 16576 mean train loss:  1.56298920e-05, mean val. loss:  3.37253523e+00\n",
      "Epoch: 16577 mean train loss:  1.57220929e-05, mean val. loss:  3.37274432e+00\n",
      "Epoch: 16578 mean train loss:  1.58948533e-05, mean val. loss:  3.37297249e+00\n",
      "Epoch: 16579 mean train loss:  1.55003800e-05, mean val. loss:  3.37325525e+00\n",
      "Epoch: 16580 mean train loss:  1.57974719e-05, mean val. loss:  3.37354159e+00\n",
      "Epoch: 16581 mean train loss:  1.56368187e-05, mean val. loss:  3.37389541e+00\n",
      "Epoch: 16582 mean train loss:  1.56753813e-05, mean val. loss:  3.37428164e+00\n",
      "Epoch: 16583 mean train loss:  1.56248861e-05, mean val. loss:  3.37473488e+00\n",
      "Epoch: 16584 mean train loss:  1.58054172e-05, mean val. loss:  3.37518167e+00\n",
      "Epoch: 16585 mean train loss:  1.55325979e-05, mean val. loss:  3.37569904e+00\n",
      "Epoch: 16586 mean train loss:  1.59192132e-05, mean val. loss:  3.37619781e+00\n",
      "Epoch: 16587 mean train loss:  1.60556228e-05, mean val. loss:  3.37666416e+00\n",
      "Epoch: 16588 mean train loss:  1.57448812e-05, mean val. loss:  3.37714696e+00\n",
      "Epoch: 16589 mean train loss:  1.56633905e-05, mean val. loss:  3.37763739e+00\n",
      "Epoch: 16590 mean train loss:  1.54747686e-05, mean val. loss:  3.37815309e+00\n",
      "Epoch: 16591 mean train loss:  1.57249742e-05, mean val. loss:  3.37862873e+00\n",
      "Epoch: 16592 mean train loss:  1.57849863e-05, mean val. loss:  3.37906766e+00\n",
      "Epoch: 16593 mean train loss:  1.59278861e-05, mean val. loss:  3.37949324e+00\n",
      "Epoch: 16594 mean train loss:  1.58250623e-05, mean val. loss:  3.37985015e+00\n",
      "Epoch: 16595 mean train loss:  1.58186594e-05, mean val. loss:  3.38013482e+00\n",
      "Epoch: 16596 mean train loss:  1.60980853e-05, mean val. loss:  3.38035226e+00\n",
      "Epoch: 16597 mean train loss:  1.55557354e-05, mean val. loss:  3.38056612e+00\n",
      "Epoch: 16598 mean train loss:  1.60982599e-05, mean val. loss:  3.38073111e+00\n",
      "Epoch: 16599 mean train loss:  1.59135379e-05, mean val. loss:  3.38092065e+00\n",
      "Epoch: 16600 mean train loss:  1.58868206e-05, mean val. loss:  3.38113689e+00\n",
      "Epoch: 16601 mean train loss:  1.58782059e-05, mean val. loss:  3.38128114e+00\n",
      "Epoch: 16602 mean train loss:  1.57575705e-05, mean val. loss:  3.38142490e+00\n",
      "Epoch: 16603 mean train loss:  1.57331815e-05, mean val. loss:  3.38159347e+00\n",
      "Epoch: 16604 mean train loss:  1.56961905e-05, mean val. loss:  3.38177085e+00\n",
      "Epoch: 16605 mean train loss:  1.56197930e-05, mean val. loss:  3.38197923e+00\n",
      "Epoch: 16606 mean train loss:  1.59836200e-05, mean val. loss:  3.38216734e+00\n",
      "Epoch: 16607 mean train loss:  1.56500610e-05, mean val. loss:  3.38237405e+00\n",
      "Epoch: 16608 mean train loss:  1.56941533e-05, mean val. loss:  3.38260317e+00\n",
      "Epoch: 16609 mean train loss:  1.58383627e-05, mean val. loss:  3.38290596e+00\n",
      "Epoch: 16610 mean train loss:  1.59642659e-05, mean val. loss:  3.38318610e+00\n",
      "Epoch: 16611 mean train loss:  1.57185714e-05, mean val. loss:  3.38343835e+00\n",
      "Epoch: 16612 mean train loss:  1.55245070e-05, mean val. loss:  3.38373446e+00\n",
      "Epoch: 16613 mean train loss:  1.58754410e-05, mean val. loss:  3.38404942e+00\n",
      "Epoch: 16614 mean train loss:  1.57452596e-05, mean val. loss:  3.38439131e+00\n",
      "Epoch: 16615 mean train loss:  1.56878668e-05, mean val. loss:  3.38467956e+00\n",
      "Epoch: 16616 mean train loss:  1.58095791e-05, mean val. loss:  3.38490891e+00\n",
      "Epoch: 16617 mean train loss:  1.57800387e-05, mean val. loss:  3.38511276e+00\n",
      "Epoch: 16618 mean train loss:  1.57745671e-05, mean val. loss:  3.38526201e+00\n",
      "Epoch: 16619 mean train loss:  1.58991315e-05, mean val. loss:  3.38538098e+00\n",
      "Epoch: 16620 mean train loss:  1.56596652e-05, mean val. loss:  3.38551927e+00\n",
      "Epoch: 16621 mean train loss:  1.57910399e-05, mean val. loss:  3.38566256e+00\n",
      "Epoch: 16622 mean train loss:  1.59761985e-05, mean val. loss:  3.38577080e+00\n",
      "Epoch: 16623 mean train loss:  1.57131581e-05, mean val. loss:  3.38587070e+00\n",
      "Epoch: 16624 mean train loss:  1.56107708e-05, mean val. loss:  3.38597441e+00\n",
      "Epoch: 16625 mean train loss:  1.53912697e-05, mean val. loss:  3.38612366e+00\n",
      "Epoch: 16626 mean train loss:  1.55661255e-05, mean val. loss:  3.38626170e+00\n",
      "Epoch: 16627 mean train loss:  1.56506139e-05, mean val. loss:  3.38644361e+00\n",
      "Epoch: 16628 mean train loss:  1.56690367e-05, mean val. loss:  3.38664556e+00\n",
      "Epoch: 16629 mean train loss:  1.58891198e-05, mean val. loss:  3.38688946e+00\n",
      "Epoch: 16630 mean train loss:  1.59305637e-05, mean val. loss:  3.38714480e+00\n",
      "Epoch: 16631 mean train loss:  1.56280294e-05, mean val. loss:  3.38744783e+00\n",
      "Epoch: 16632 mean train loss:  1.62311771e-05, mean val. loss:  3.38770580e+00\n",
      "Epoch: 16633 mean train loss:  1.55958114e-05, mean val. loss:  3.38797450e+00\n",
      "Epoch: 16634 mean train loss:  1.58571638e-05, mean val. loss:  3.38826394e+00\n",
      "Epoch: 16635 mean train loss:  1.53665314e-05, mean val. loss:  3.38860273e+00\n",
      "Epoch: 16636 mean train loss:  1.59644696e-05, mean val. loss:  3.38891578e+00\n",
      "Epoch: 16637 mean train loss:  1.60074269e-05, mean val. loss:  3.38925910e+00\n",
      "Epoch: 16638 mean train loss:  1.58356561e-05, mean val. loss:  3.38958955e+00\n",
      "Epoch: 16639 mean train loss:  1.60376076e-05, mean val. loss:  3.38991666e+00\n",
      "Epoch: 16640 mean train loss:  1.57473260e-05, mean val. loss:  3.39026451e+00\n",
      "Epoch: 16641 mean train loss:  1.56666501e-05, mean val. loss:  3.39063144e+00\n",
      "Epoch: 16642 mean train loss:  1.56805036e-05, mean val. loss:  3.39096522e+00\n",
      "Epoch: 16643 mean train loss:  1.58645853e-05, mean val. loss:  3.39127660e+00\n",
      "Epoch: 16644 mean train loss:  1.57775648e-05, mean val. loss:  3.39161801e+00\n",
      "Epoch: 16645 mean train loss:  1.55430171e-05, mean val. loss:  3.39198637e+00\n",
      "Epoch: 16646 mean train loss:  1.56927272e-05, mean val. loss:  3.39236975e+00\n",
      "Epoch: 16647 mean train loss:  1.56709866e-05, mean val. loss:  3.39280677e+00\n",
      "Epoch: 16648 mean train loss:  1.54873123e-05, mean val. loss:  3.39329505e+00\n",
      "Epoch: 16649 mean train loss:  1.56917376e-05, mean val. loss:  3.39377689e+00\n",
      "Epoch: 16650 mean train loss:  1.56519236e-05, mean val. loss:  3.39426422e+00\n",
      "Epoch: 16651 mean train loss:  1.54297159e-05, mean val. loss:  3.39474773e+00\n",
      "Epoch: 16652 mean train loss:  1.58321345e-05, mean val. loss:  3.39518619e+00\n",
      "Epoch: 16653 mean train loss:  1.57054747e-05, mean val. loss:  3.39564013e+00\n",
      "Epoch: 16654 mean train loss:  1.58652547e-05, mean val. loss:  3.39608407e+00\n",
      "Epoch: 16655 mean train loss:  1.57750328e-05, mean val. loss:  3.39650512e+00\n",
      "Epoch: 16656 mean train loss:  1.58773037e-05, mean val. loss:  3.39691162e+00\n",
      "Epoch: 16657 mean train loss:  1.56242750e-05, mean val. loss:  3.39736342e+00\n",
      "Epoch: 16658 mean train loss:  1.55914167e-05, mean val. loss:  3.39779878e+00\n",
      "Epoch: 16659 mean train loss:  1.59165356e-05, mean val. loss:  3.39816642e+00\n",
      "Epoch: 16660 mean train loss:  1.55163580e-05, mean val. loss:  3.39850259e+00\n",
      "Epoch: 16661 mean train loss:  1.59000629e-05, mean val. loss:  3.39879441e+00\n",
      "Epoch: 16662 mean train loss:  1.56877795e-05, mean val. loss:  3.39907455e+00\n",
      "Epoch: 16663 mean train loss:  1.58298935e-05, mean val. loss:  3.39930034e+00\n",
      "Epoch: 16664 mean train loss:  1.54320733e-05, mean val. loss:  3.39951587e+00\n",
      "Epoch: 16665 mean train loss:  1.56768365e-05, mean val. loss:  3.39970613e+00\n",
      "Epoch: 16666 mean train loss:  1.56561728e-05, mean val. loss:  3.39989424e+00\n",
      "Epoch: 16667 mean train loss:  1.55348971e-05, mean val. loss:  3.40009642e+00\n",
      "Epoch: 16668 mean train loss:  1.53889996e-05, mean val. loss:  3.40031242e+00\n",
      "Epoch: 16669 mean train loss:  1.59239280e-05, mean val. loss:  3.40051341e+00\n",
      "Epoch: 16670 mean train loss:  1.57171162e-05, mean val. loss:  3.40071201e+00\n",
      "Epoch: 16671 mean train loss:  1.57324539e-05, mean val. loss:  3.40090537e+00\n",
      "Epoch: 16672 mean train loss:  1.56759925e-05, mean val. loss:  3.40111041e+00\n",
      "Epoch: 16673 mean train loss:  1.58473849e-05, mean val. loss:  3.40123892e+00\n",
      "Epoch: 16674 mean train loss:  1.57929026e-05, mean val. loss:  3.40135431e+00\n",
      "Epoch: 16675 mean train loss:  1.60701748e-05, mean val. loss:  3.40145707e+00\n",
      "Epoch: 16676 mean train loss:  1.58057373e-05, mean val. loss:  3.40151691e+00\n",
      "Epoch: 16677 mean train loss:  1.54259033e-05, mean val. loss:  3.40163589e+00\n",
      "Epoch: 16678 mean train loss:  1.58324838e-05, mean val. loss:  3.40170336e+00\n",
      "Epoch: 16679 mean train loss:  1.57447357e-05, mean val. loss:  3.40171552e+00\n",
      "Epoch: 16680 mean train loss:  1.59016054e-05, mean val. loss:  3.40167308e+00\n",
      "Epoch: 16681 mean train loss:  1.59807096e-05, mean val. loss:  3.40156651e+00\n",
      "Epoch: 16682 mean train loss:  1.55243324e-05, mean val. loss:  3.40152168e+00\n",
      "Epoch: 16683 mean train loss:  1.58510520e-05, mean val. loss:  3.40149069e+00\n",
      "Epoch: 16684 mean train loss:  1.56449678e-05, mean val. loss:  3.40146923e+00\n",
      "Epoch: 16685 mean train loss:  1.58128969e-05, mean val. loss:  3.40147400e+00\n",
      "Epoch: 16686 mean train loss:  1.55042799e-05, mean val. loss:  3.40151763e+00\n",
      "Epoch: 16687 mean train loss:  1.56812894e-05, mean val. loss:  3.40160871e+00\n",
      "Epoch: 16688 mean train loss:  1.56782917e-05, mean val. loss:  3.40174460e+00\n",
      "Epoch: 16689 mean train loss:  1.57992763e-05, mean val. loss:  3.40195155e+00\n",
      "Epoch: 16690 mean train loss:  1.58631301e-05, mean val. loss:  3.40220952e+00\n",
      "Epoch: 16691 mean train loss:  1.56734895e-05, mean val. loss:  3.40254116e+00\n",
      "Epoch: 16692 mean train loss:  1.55859743e-05, mean val. loss:  3.40293932e+00\n",
      "Epoch: 16693 mean train loss:  1.55571324e-05, mean val. loss:  3.40336776e+00\n",
      "Epoch: 16694 mean train loss:  1.55316666e-05, mean val. loss:  3.40384507e+00\n",
      "Epoch: 16695 mean train loss:  1.56950264e-05, mean val. loss:  3.40435028e+00\n",
      "Epoch: 16696 mean train loss:  1.58121693e-05, mean val. loss:  3.40479589e+00\n",
      "Epoch: 16697 mean train loss:  1.58254406e-05, mean val. loss:  3.40527391e+00\n",
      "Epoch: 16698 mean train loss:  1.54888257e-05, mean val. loss:  3.40575171e+00\n",
      "Epoch: 16699 mean train loss:  1.59561168e-05, mean val. loss:  3.40624452e+00\n",
      "Epoch: 16700 mean train loss:  1.58729672e-05, mean val. loss:  3.40670061e+00\n",
      "Epoch: 16701 mean train loss:  1.57544855e-05, mean val. loss:  3.40715170e+00\n",
      "Epoch: 16702 mean train loss:  1.55301823e-05, mean val. loss:  3.40762830e+00\n",
      "Epoch: 16703 mean train loss:  1.58277689e-05, mean val. loss:  3.40808463e+00\n",
      "Epoch: 16704 mean train loss:  1.59932242e-05, mean val. loss:  3.40852928e+00\n",
      "Epoch: 16705 mean train loss:  1.55969756e-05, mean val. loss:  3.40897346e+00\n",
      "Epoch: 16706 mean train loss:  1.58553594e-05, mean val. loss:  3.40937829e+00\n",
      "Epoch: 16707 mean train loss:  1.57989853e-05, mean val. loss:  3.40976191e+00\n",
      "Epoch: 16708 mean train loss:  1.57284667e-05, mean val. loss:  3.41008615e+00\n",
      "Epoch: 16709 mean train loss:  1.58109760e-05, mean val. loss:  3.41039777e+00\n",
      "Epoch: 16710 mean train loss:  1.53901929e-05, mean val. loss:  3.41075158e+00\n",
      "Epoch: 16711 mean train loss:  1.54087611e-05, mean val. loss:  3.41109562e+00\n",
      "Epoch: 16712 mean train loss:  1.55395537e-05, mean val. loss:  3.41149497e+00\n",
      "Epoch: 16713 mean train loss:  1.61645876e-05, mean val. loss:  3.41184044e+00\n",
      "Epoch: 16714 mean train loss:  1.56428432e-05, mean val. loss:  3.41219211e+00\n",
      "Epoch: 16715 mean train loss:  1.56076276e-05, mean val. loss:  3.41253662e+00\n",
      "Epoch: 16716 mean train loss:  1.54627487e-05, mean val. loss:  3.41290188e+00\n",
      "Epoch: 16717 mean train loss:  1.57172035e-05, mean val. loss:  3.41330194e+00\n",
      "Epoch: 16718 mean train loss:  1.58703770e-05, mean val. loss:  3.41367626e+00\n",
      "Epoch: 16719 mean train loss:  1.55611197e-05, mean val. loss:  3.41402936e+00\n",
      "Epoch: 16720 mean train loss:  1.59303017e-05, mean val. loss:  3.41433668e+00\n",
      "Epoch: 16721 mean train loss:  1.54055015e-05, mean val. loss:  3.41468859e+00\n",
      "Epoch: 16722 mean train loss:  1.55529415e-05, mean val. loss:  3.41503525e+00\n",
      "Epoch: 16723 mean train loss:  1.57241302e-05, mean val. loss:  3.41534567e+00\n",
      "Epoch: 16724 mean train loss:  1.56586757e-05, mean val. loss:  3.41563344e+00\n",
      "Epoch: 16725 mean train loss:  1.58975017e-05, mean val. loss:  3.41593337e+00\n",
      "Epoch: 16726 mean train loss:  1.56383321e-05, mean val. loss:  3.41620255e+00\n",
      "Epoch: 16727 mean train loss:  1.56451424e-05, mean val. loss:  3.41645026e+00\n",
      "Epoch: 16728 mean train loss:  1.57233444e-05, mean val. loss:  3.41666532e+00\n",
      "Epoch: 16729 mean train loss:  1.54141744e-05, mean val. loss:  3.41689944e+00\n",
      "Epoch: 16730 mean train loss:  1.55344023e-05, mean val. loss:  3.41710997e+00\n",
      "Epoch: 16731 mean train loss:  1.57950271e-05, mean val. loss:  3.41726327e+00\n",
      "Epoch: 16732 mean train loss:  1.54087611e-05, mean val. loss:  3.41746616e+00\n",
      "Epoch: 16733 mean train loss:  1.57807372e-05, mean val. loss:  3.41766453e+00\n",
      "Epoch: 16734 mean train loss:  1.56871101e-05, mean val. loss:  3.41784668e+00\n",
      "Epoch: 16735 mean train loss:  1.56648457e-05, mean val. loss:  3.41802716e+00\n",
      "Epoch: 16736 mean train loss:  1.55217422e-05, mean val. loss:  3.41821837e+00\n",
      "Epoch: 16737 mean train loss:  1.54027366e-05, mean val. loss:  3.41846156e+00\n",
      "Epoch: 16738 mean train loss:  1.56813476e-05, mean val. loss:  3.41870832e+00\n",
      "Epoch: 16739 mean train loss:  1.56036986e-05, mean val. loss:  3.41896200e+00\n",
      "Epoch: 16740 mean train loss:  1.56399328e-05, mean val. loss:  3.41917944e+00\n",
      "Epoch: 16741 mean train loss:  1.57605391e-05, mean val. loss:  3.41935754e+00\n",
      "Epoch: 16742 mean train loss:  1.55844609e-05, mean val. loss:  3.41962337e+00\n",
      "Epoch: 16743 mean train loss:  1.55713060e-05, mean val. loss:  3.41990852e+00\n",
      "Epoch: 16744 mean train loss:  1.60034106e-05, mean val. loss:  3.42015457e+00\n",
      "Epoch: 16745 mean train loss:  1.57349568e-05, mean val. loss:  3.42046642e+00\n",
      "Epoch: 16746 mean train loss:  1.54517184e-05, mean val. loss:  3.42077756e+00\n",
      "Epoch: 16747 mean train loss:  1.54163281e-05, mean val. loss:  3.42108703e+00\n",
      "Epoch: 16748 mean train loss:  1.55171147e-05, mean val. loss:  3.42140698e+00\n",
      "Epoch: 16749 mean train loss:  1.56726455e-05, mean val. loss:  3.42170405e+00\n",
      "Epoch: 16750 mean train loss:  1.57338800e-05, mean val. loss:  3.42201042e+00\n",
      "Epoch: 16751 mean train loss:  1.57104805e-05, mean val. loss:  3.42229033e+00\n",
      "Epoch: 16752 mean train loss:  1.56048045e-05, mean val. loss:  3.42252707e+00\n",
      "Epoch: 16753 mean train loss:  1.57699687e-05, mean val. loss:  3.42272782e+00\n",
      "Epoch: 16754 mean train loss:  1.56312017e-05, mean val. loss:  3.42291355e+00\n",
      "Epoch: 16755 mean train loss:  1.58460462e-05, mean val. loss:  3.42306757e+00\n",
      "Epoch: 16756 mean train loss:  1.57334143e-05, mean val. loss:  3.42322516e+00\n",
      "Epoch: 16757 mean train loss:  1.55566959e-05, mean val. loss:  3.42336702e+00\n",
      "Epoch: 16758 mean train loss:  1.58723269e-05, mean val. loss:  3.42343903e+00\n",
      "Epoch: 16759 mean train loss:  1.57088507e-05, mean val. loss:  3.42348838e+00\n",
      "Epoch: 16760 mean train loss:  1.55840535e-05, mean val. loss:  3.42355585e+00\n",
      "Epoch: 16761 mean train loss:  1.54122827e-05, mean val. loss:  3.42369175e+00\n",
      "Epoch: 16762 mean train loss:  1.57669128e-05, mean val. loss:  3.42385483e+00\n",
      "Epoch: 16763 mean train loss:  1.56072783e-05, mean val. loss:  3.42407775e+00\n",
      "Epoch: 16764 mean train loss:  1.55998860e-05, mean val. loss:  3.42433095e+00\n",
      "Epoch: 16765 mean train loss:  1.56525639e-05, mean val. loss:  3.42461014e+00\n",
      "Epoch: 16766 mean train loss:  1.56281458e-05, mean val. loss:  3.42495680e+00\n",
      "Epoch: 16767 mean train loss:  1.58288458e-05, mean val. loss:  3.42528796e+00\n",
      "Epoch: 16768 mean train loss:  1.58449984e-05, mean val. loss:  3.42562699e+00\n",
      "Epoch: 16769 mean train loss:  1.57116156e-05, mean val. loss:  3.42599082e+00\n",
      "Epoch: 16770 mean train loss:  1.56908645e-05, mean val. loss:  3.42636228e+00\n",
      "Epoch: 16771 mean train loss:  1.56952301e-05, mean val. loss:  3.42672682e+00\n",
      "Epoch: 16772 mean train loss:  1.54820154e-05, mean val. loss:  3.42708302e+00\n",
      "Epoch: 16773 mean train loss:  1.53593719e-05, mean val. loss:  3.42749476e+00\n",
      "Epoch: 16774 mean train loss:  1.56973547e-05, mean val. loss:  3.42789245e+00\n",
      "Epoch: 16775 mean train loss:  1.56526512e-05, mean val. loss:  3.42823315e+00\n",
      "Epoch: 16776 mean train loss:  1.55705784e-05, mean val. loss:  3.42858839e+00\n",
      "Epoch: 16777 mean train loss:  1.58037874e-05, mean val. loss:  3.42892337e+00\n",
      "Epoch: 16778 mean train loss:  1.60045456e-05, mean val. loss:  3.42924881e+00\n",
      "Epoch: 16779 mean train loss:  1.54134468e-05, mean val. loss:  3.42961192e+00\n",
      "Epoch: 16780 mean train loss:  1.56098395e-05, mean val. loss:  3.42997336e+00\n",
      "Epoch: 16781 mean train loss:  1.55379530e-05, mean val. loss:  3.43031454e+00\n",
      "Epoch: 16782 mean train loss:  1.56695605e-05, mean val. loss:  3.43065500e+00\n",
      "Epoch: 16783 mean train loss:  1.56824826e-05, mean val. loss:  3.43099427e+00\n",
      "Epoch: 16784 mean train loss:  1.56002934e-05, mean val. loss:  3.43129158e+00\n",
      "Epoch: 16785 mean train loss:  1.55133603e-05, mean val. loss:  3.43160176e+00\n",
      "Epoch: 16786 mean train loss:  1.54072186e-05, mean val. loss:  3.43198943e+00\n",
      "Epoch: 16787 mean train loss:  1.57987233e-05, mean val. loss:  3.43233609e+00\n",
      "Epoch: 16788 mean train loss:  1.59900519e-05, mean val. loss:  3.43262815e+00\n",
      "Epoch: 16789 mean train loss:  1.56115857e-05, mean val. loss:  3.43292928e+00\n",
      "Epoch: 16790 mean train loss:  1.54132431e-05, mean val. loss:  3.43329501e+00\n",
      "Epoch: 16791 mean train loss:  1.56799506e-05, mean val. loss:  3.43366623e+00\n",
      "Epoch: 16792 mean train loss:  1.55839662e-05, mean val. loss:  3.43401742e+00\n",
      "Epoch: 16793 mean train loss:  1.53107394e-05, mean val. loss:  3.43435574e+00\n",
      "Epoch: 16794 mean train loss:  1.58590556e-05, mean val. loss:  3.43469310e+00\n",
      "Epoch: 16795 mean train loss:  1.55917078e-05, mean val. loss:  3.43503928e+00\n",
      "Epoch: 16796 mean train loss:  1.54796289e-05, mean val. loss:  3.43536401e+00\n",
      "Epoch: 16797 mean train loss:  1.56627793e-05, mean val. loss:  3.43563914e+00\n",
      "Epoch: 16798 mean train loss:  1.58657786e-05, mean val. loss:  3.43590784e+00\n",
      "Epoch: 16799 mean train loss:  1.57358299e-05, mean val. loss:  3.43614125e+00\n",
      "Epoch: 16800 mean train loss:  1.57716568e-05, mean val. loss:  3.43631959e+00\n",
      "Epoch: 16801 mean train loss:  1.57234608e-05, mean val. loss:  3.43650222e+00\n",
      "Epoch: 16802 mean train loss:  1.54847512e-05, mean val. loss:  3.43673301e+00\n",
      "Epoch: 16803 mean train loss:  1.58081821e-05, mean val. loss:  3.43689728e+00\n",
      "Epoch: 16804 mean train loss:  1.57090544e-05, mean val. loss:  3.43704367e+00\n",
      "Epoch: 16805 mean train loss:  1.54591980e-05, mean val. loss:  3.43720412e+00\n",
      "Epoch: 16806 mean train loss:  1.55692105e-05, mean val. loss:  3.43732142e+00\n",
      "Epoch: 16807 mean train loss:  1.55677844e-05, mean val. loss:  3.43744183e+00\n",
      "Epoch: 16808 mean train loss:  1.56387396e-05, mean val. loss:  3.43752313e+00\n",
      "Epoch: 16809 mean train loss:  1.55692978e-05, mean val. loss:  3.43761134e+00\n",
      "Epoch: 16810 mean train loss:  1.54964218e-05, mean val. loss:  3.43775749e+00\n",
      "Epoch: 16811 mean train loss:  1.57123723e-05, mean val. loss:  3.43788671e+00\n",
      "Epoch: 16812 mean train loss:  1.57376926e-05, mean val. loss:  3.43804216e+00\n",
      "Epoch: 16813 mean train loss:  1.51647255e-05, mean val. loss:  3.43830085e+00\n",
      "Epoch: 16814 mean train loss:  1.56520982e-05, mean val. loss:  3.43853927e+00\n",
      "Epoch: 16815 mean train loss:  1.56645547e-05, mean val. loss:  3.43883514e+00\n",
      "Epoch: 16816 mean train loss:  1.58414477e-05, mean val. loss:  3.43910885e+00\n",
      "Epoch: 16817 mean train loss:  1.53120782e-05, mean val. loss:  3.43945885e+00\n",
      "Epoch: 16818 mean train loss:  1.58463954e-05, mean val. loss:  3.43975210e+00\n",
      "Epoch: 16819 mean train loss:  1.55427551e-05, mean val. loss:  3.44006443e+00\n",
      "Epoch: 16820 mean train loss:  1.57318427e-05, mean val. loss:  3.44030976e+00\n",
      "Epoch: 16821 mean train loss:  1.54470326e-05, mean val. loss:  3.44054413e+00\n",
      "Epoch: 16822 mean train loss:  1.57990435e-05, mean val. loss:  3.44076228e+00\n",
      "Epoch: 16823 mean train loss:  1.55095768e-05, mean val. loss:  3.44097519e+00\n",
      "Epoch: 16824 mean train loss:  1.55309681e-05, mean val. loss:  3.44119692e+00\n",
      "Epoch: 16825 mean train loss:  1.60111813e-05, mean val. loss:  3.44138861e+00\n",
      "Epoch: 16826 mean train loss:  1.57331815e-05, mean val. loss:  3.44159889e+00\n",
      "Epoch: 16827 mean train loss:  1.53832952e-05, mean val. loss:  3.44182658e+00\n",
      "Epoch: 16828 mean train loss:  1.56515162e-05, mean val. loss:  3.44206834e+00\n",
      "Epoch: 16829 mean train loss:  1.57553586e-05, mean val. loss:  3.44230986e+00\n",
      "Epoch: 16830 mean train loss:  1.56330061e-05, mean val. loss:  3.44254351e+00\n",
      "Epoch: 16831 mean train loss:  1.56357710e-05, mean val. loss:  3.44280553e+00\n",
      "Epoch: 16832 mean train loss:  1.54994777e-05, mean val. loss:  3.44314218e+00\n",
      "Epoch: 16833 mean train loss:  1.58378971e-05, mean val. loss:  3.44342709e+00\n",
      "Epoch: 16834 mean train loss:  1.57628092e-05, mean val. loss:  3.44370127e+00\n",
      "Epoch: 16835 mean train loss:  1.54426962e-05, mean val. loss:  3.44407058e+00\n",
      "Epoch: 16836 mean train loss:  1.55598391e-05, mean val. loss:  3.44447041e+00\n",
      "Epoch: 16837 mean train loss:  1.56424649e-05, mean val. loss:  3.44488621e+00\n",
      "Epoch: 16838 mean train loss:  1.53806177e-05, mean val. loss:  3.44536757e+00\n",
      "Epoch: 16839 mean train loss:  1.57510513e-05, mean val. loss:  3.44578624e+00\n",
      "Epoch: 16840 mean train loss:  1.55676389e-05, mean val. loss:  3.44620109e+00\n",
      "Epoch: 16841 mean train loss:  1.56677852e-05, mean val. loss:  3.44662523e+00\n",
      "Epoch: 16842 mean train loss:  1.56741298e-05, mean val. loss:  3.44697952e+00\n",
      "Epoch: 16843 mean train loss:  1.54360605e-05, mean val. loss:  3.44735241e+00\n",
      "Epoch: 16844 mean train loss:  1.55124580e-05, mean val. loss:  3.44767857e+00\n",
      "Epoch: 16845 mean train loss:  1.53502042e-05, mean val. loss:  3.44805646e+00\n",
      "Epoch: 16846 mean train loss:  1.55127782e-05, mean val. loss:  3.44845700e+00\n",
      "Epoch: 16847 mean train loss:  1.54391746e-05, mean val. loss:  3.44885707e+00\n",
      "Epoch: 16848 mean train loss:  1.56355673e-05, mean val. loss:  3.44918942e+00\n",
      "Epoch: 16849 mean train loss:  1.55175512e-05, mean val. loss:  3.44955420e+00\n",
      "Epoch: 16850 mean train loss:  1.59833580e-05, mean val. loss:  3.44984698e+00\n",
      "Epoch: 16851 mean train loss:  1.54921727e-05, mean val. loss:  3.45017600e+00\n",
      "Epoch: 16852 mean train loss:  1.55160669e-05, mean val. loss:  3.45046353e+00\n",
      "Epoch: 16853 mean train loss:  1.56917085e-05, mean val. loss:  3.45071483e+00\n",
      "Epoch: 16854 mean train loss:  1.54144363e-05, mean val. loss:  3.45097184e+00\n",
      "Epoch: 16855 mean train loss:  1.53645524e-05, mean val. loss:  3.45134163e+00\n",
      "Epoch: 16856 mean train loss:  1.56823080e-05, mean val. loss:  3.45170999e+00\n",
      "Epoch: 16857 mean train loss:  1.56728784e-05, mean val. loss:  3.45209432e+00\n",
      "Epoch: 16858 mean train loss:  1.54383597e-05, mean val. loss:  3.45249033e+00\n",
      "Epoch: 16859 mean train loss:  1.56727037e-05, mean val. loss:  3.45287323e+00\n",
      "Epoch: 16860 mean train loss:  1.55602756e-05, mean val. loss:  3.45322633e+00\n",
      "Epoch: 16861 mean train loss:  1.58440671e-05, mean val. loss:  3.45353293e+00\n",
      "Epoch: 16862 mean train loss:  1.54370791e-05, mean val. loss:  3.45386434e+00\n",
      "Epoch: 16863 mean train loss:  1.57385075e-05, mean val. loss:  3.45414829e+00\n",
      "Epoch: 16864 mean train loss:  1.58025650e-05, mean val. loss:  3.45435905e+00\n",
      "Epoch: 16865 mean train loss:  1.55867892e-05, mean val. loss:  3.45457649e+00\n",
      "Epoch: 16866 mean train loss:  1.53331785e-05, mean val. loss:  3.45485377e+00\n",
      "Epoch: 16867 mean train loss:  1.54489535e-05, mean val. loss:  3.45515299e+00\n",
      "Epoch: 16868 mean train loss:  1.57870818e-05, mean val. loss:  3.45541286e+00\n",
      "Epoch: 16869 mean train loss:  1.54455192e-05, mean val. loss:  3.45565987e+00\n",
      "Epoch: 16870 mean train loss:  1.58721523e-05, mean val. loss:  3.45580268e+00\n",
      "Epoch: 16871 mean train loss:  1.57415343e-05, mean val. loss:  3.45599103e+00\n",
      "Epoch: 16872 mean train loss:  1.56684255e-05, mean val. loss:  3.45618010e+00\n",
      "Epoch: 16873 mean train loss:  1.53746805e-05, mean val. loss:  3.45641637e+00\n",
      "Epoch: 16874 mean train loss:  1.54369045e-05, mean val. loss:  3.45670462e+00\n",
      "Epoch: 16875 mean train loss:  1.57045433e-05, mean val. loss:  3.45699859e+00\n",
      "Epoch: 16876 mean train loss:  1.54795416e-05, mean val. loss:  3.45733190e+00\n",
      "Epoch: 16877 mean train loss:  1.58387702e-05, mean val. loss:  3.45764661e+00\n",
      "Epoch: 16878 mean train loss:  1.52126304e-05, mean val. loss:  3.45794821e+00\n",
      "Epoch: 16879 mean train loss:  1.56833266e-05, mean val. loss:  3.45820284e+00\n",
      "Epoch: 16880 mean train loss:  1.56599563e-05, mean val. loss:  3.45845127e+00\n",
      "Epoch: 16881 mean train loss:  1.54658337e-05, mean val. loss:  3.45874953e+00\n",
      "Epoch: 16882 mean train loss:  1.54970912e-05, mean val. loss:  3.45904016e+00\n",
      "Epoch: 16883 mean train loss:  1.54041627e-05, mean val. loss:  3.45933843e+00\n",
      "Epoch: 16884 mean train loss:  1.57499453e-05, mean val. loss:  3.45963597e+00\n",
      "Epoch: 16885 mean train loss:  1.58371695e-05, mean val. loss:  3.45988488e+00\n",
      "Epoch: 16886 mean train loss:  1.56352180e-05, mean val. loss:  3.46013284e+00\n",
      "Epoch: 16887 mean train loss:  1.56234310e-05, mean val. loss:  3.46037030e+00\n",
      "Epoch: 16888 mean train loss:  1.57760223e-05, mean val. loss:  3.46050787e+00\n",
      "Epoch: 16889 mean train loss:  1.56942115e-05, mean val. loss:  3.46062160e+00\n",
      "Epoch: 16890 mean train loss:  1.57413888e-05, mean val. loss:  3.46072006e+00\n",
      "Epoch: 16891 mean train loss:  1.52327120e-05, mean val. loss:  3.46083474e+00\n",
      "Epoch: 16892 mean train loss:  1.56440656e-05, mean val. loss:  3.46095324e+00\n",
      "Epoch: 16893 mean train loss:  1.54746231e-05, mean val. loss:  3.46112704e+00\n",
      "Epoch: 16894 mean train loss:  1.54805603e-05, mean val. loss:  3.46132326e+00\n",
      "Epoch: 16895 mean train loss:  1.58622279e-05, mean val. loss:  3.46150231e+00\n",
      "Epoch: 16896 mean train loss:  1.56156893e-05, mean val. loss:  3.46172523e+00\n",
      "Epoch: 16897 mean train loss:  1.52539869e-05, mean val. loss:  3.46204662e+00\n",
      "Epoch: 16898 mean train loss:  1.55158632e-05, mean val. loss:  3.46247196e+00\n",
      "Epoch: 16899 mean train loss:  1.56216847e-05, mean val. loss:  3.46290731e+00\n",
      "Epoch: 16900 mean train loss:  1.56840833e-05, mean val. loss:  3.46333241e+00\n",
      "Epoch: 16901 mean train loss:  1.55760790e-05, mean val. loss:  3.46374774e+00\n",
      "Epoch: 16902 mean train loss:  1.55041926e-05, mean val. loss:  3.46415544e+00\n",
      "Epoch: 16903 mean train loss:  1.55814341e-05, mean val. loss:  3.46456671e+00\n",
      "Epoch: 16904 mean train loss:  1.59142946e-05, mean val. loss:  3.46491504e+00\n",
      "Epoch: 16905 mean train loss:  1.55681628e-05, mean val. loss:  3.46528983e+00\n",
      "Epoch: 16906 mean train loss:  1.55441230e-05, mean val. loss:  3.46564794e+00\n",
      "Epoch: 16907 mean train loss:  1.55963935e-05, mean val. loss:  3.46596241e+00\n",
      "Epoch: 16908 mean train loss:  1.56052702e-05, mean val. loss:  3.46626258e+00\n",
      "Epoch: 16909 mean train loss:  1.56497408e-05, mean val. loss:  3.46651578e+00\n",
      "Epoch: 16910 mean train loss:  1.53805595e-05, mean val. loss:  3.46678090e+00\n",
      "Epoch: 16911 mean train loss:  1.56273018e-05, mean val. loss:  3.46701312e+00\n",
      "Epoch: 16912 mean train loss:  1.56909227e-05, mean val. loss:  3.46720648e+00\n",
      "Epoch: 16913 mean train loss:  1.56046008e-05, mean val. loss:  3.46734428e+00\n",
      "Epoch: 16914 mean train loss:  1.56892056e-05, mean val. loss:  3.46746039e+00\n",
      "Epoch: 16915 mean train loss:  1.54671143e-05, mean val. loss:  3.46758986e+00\n",
      "Epoch: 16916 mean train loss:  1.53169094e-05, mean val. loss:  3.46773148e+00\n",
      "Epoch: 16917 mean train loss:  1.54004956e-05, mean val. loss:  3.46789813e+00\n",
      "Epoch: 16918 mean train loss:  1.60401105e-05, mean val. loss:  3.46802235e+00\n",
      "Epoch: 16919 mean train loss:  1.51661225e-05, mean val. loss:  3.46826887e+00\n",
      "Epoch: 16920 mean train loss:  1.57125178e-05, mean val. loss:  3.46848965e+00\n",
      "Epoch: 16921 mean train loss:  1.56416208e-05, mean val. loss:  3.46873260e+00\n",
      "Epoch: 16922 mean train loss:  1.58210751e-05, mean val. loss:  3.46896911e+00\n",
      "Epoch: 16923 mean train loss:  1.55349553e-05, mean val. loss:  3.46921968e+00\n",
      "Epoch: 16924 mean train loss:  1.54250883e-05, mean val. loss:  3.46948910e+00\n",
      "Epoch: 16925 mean train loss:  1.54002337e-05, mean val. loss:  3.46985531e+00\n",
      "Epoch: 16926 mean train loss:  1.54745940e-05, mean val. loss:  3.47018719e+00\n",
      "Epoch: 16927 mean train loss:  1.56661845e-05, mean val. loss:  3.47052240e+00\n",
      "Epoch: 16928 mean train loss:  1.58988987e-05, mean val. loss:  3.47083950e+00\n",
      "Epoch: 16929 mean train loss:  1.55471498e-05, mean val. loss:  3.47116494e+00\n",
      "Epoch: 16930 mean train loss:  1.52630673e-05, mean val. loss:  3.47151875e+00\n",
      "Epoch: 16931 mean train loss:  1.54813461e-05, mean val. loss:  3.47187495e+00\n",
      "Epoch: 16932 mean train loss:  1.56708120e-05, mean val. loss:  3.47224689e+00\n",
      "Epoch: 16933 mean train loss:  1.53222354e-05, mean val. loss:  3.47267079e+00\n",
      "Epoch: 16934 mean train loss:  1.53073051e-05, mean val. loss:  3.47314453e+00\n",
      "Epoch: 16935 mean train loss:  1.59835326e-05, mean val. loss:  3.47356939e+00\n",
      "Epoch: 16936 mean train loss:  1.56108581e-05, mean val. loss:  3.47399569e+00\n",
      "Epoch: 16937 mean train loss:  1.53126311e-05, mean val. loss:  3.47442150e+00\n",
      "Epoch: 16938 mean train loss:  1.58070179e-05, mean val. loss:  3.47475457e+00\n",
      "Epoch: 16939 mean train loss:  1.53105066e-05, mean val. loss:  3.47511387e+00\n",
      "Epoch: 16940 mean train loss:  1.54384179e-05, mean val. loss:  3.47549796e+00\n",
      "Epoch: 16941 mean train loss:  1.55126618e-05, mean val. loss:  3.47586179e+00\n",
      "Epoch: 16942 mean train loss:  1.56742753e-05, mean val. loss:  3.47623134e+00\n",
      "Epoch: 16943 mean train loss:  1.55129819e-05, mean val. loss:  3.47657037e+00\n",
      "Epoch: 16944 mean train loss:  1.54518057e-05, mean val. loss:  3.47689843e+00\n",
      "Epoch: 16945 mean train loss:  1.54440640e-05, mean val. loss:  3.47724795e+00\n",
      "Epoch: 16946 mean train loss:  1.53051515e-05, mean val. loss:  3.47763777e+00\n",
      "Epoch: 16947 mean train loss:  1.55187445e-05, mean val. loss:  3.47797370e+00\n",
      "Epoch: 16948 mean train loss:  1.53570145e-05, mean val. loss:  3.47832108e+00\n",
      "Epoch: 16949 mean train loss:  1.53541332e-05, mean val. loss:  3.47873616e+00\n",
      "Epoch: 16950 mean train loss:  1.54316949e-05, mean val. loss:  3.47914100e+00\n",
      "Epoch: 16951 mean train loss:  1.55253219e-05, mean val. loss:  3.47952461e+00\n",
      "Epoch: 16952 mean train loss:  1.53150177e-05, mean val. loss:  3.47991300e+00\n",
      "Epoch: 16953 mean train loss:  1.53659494e-05, mean val. loss:  3.48032928e+00\n",
      "Epoch: 16954 mean train loss:  1.55346061e-05, mean val. loss:  3.48075938e+00\n",
      "Epoch: 16955 mean train loss:  1.55374000e-05, mean val. loss:  3.48120308e+00\n",
      "Epoch: 16956 mean train loss:  1.59016636e-05, mean val. loss:  3.48162985e+00\n",
      "Epoch: 16957 mean train loss:  1.57453178e-05, mean val. loss:  3.48200989e+00\n",
      "Epoch: 16958 mean train loss:  1.55584130e-05, mean val. loss:  3.48236847e+00\n",
      "Epoch: 16959 mean train loss:  1.56624592e-05, mean val. loss:  3.48265409e+00\n",
      "Epoch: 16960 mean train loss:  1.55404559e-05, mean val. loss:  3.48289919e+00\n",
      "Epoch: 16961 mean train loss:  1.57802715e-05, mean val. loss:  3.48309541e+00\n",
      "Epoch: 16962 mean train loss:  1.53903966e-05, mean val. loss:  3.48328972e+00\n",
      "Epoch: 16963 mean train loss:  1.56271853e-05, mean val. loss:  3.48345423e+00\n",
      "Epoch: 16964 mean train loss:  1.55057351e-05, mean val. loss:  3.48359704e+00\n",
      "Epoch: 16965 mean train loss:  1.57266913e-05, mean val. loss:  3.48371768e+00\n",
      "Epoch: 16966 mean train loss:  1.54743320e-05, mean val. loss:  3.48383260e+00\n",
      "Epoch: 16967 mean train loss:  1.58225594e-05, mean val. loss:  3.48392320e+00\n",
      "Epoch: 16968 mean train loss:  1.56856258e-05, mean val. loss:  3.48398781e+00\n",
      "Epoch: 16969 mean train loss:  1.60010532e-05, mean val. loss:  3.48399734e+00\n",
      "Epoch: 16970 mean train loss:  1.52936555e-05, mean val. loss:  3.48407078e+00\n",
      "Epoch: 16971 mean train loss:  1.57283794e-05, mean val. loss:  3.48414969e+00\n",
      "Epoch: 16972 mean train loss:  1.54040463e-05, mean val. loss:  3.48430085e+00\n",
      "Epoch: 16973 mean train loss:  1.56163296e-05, mean val. loss:  3.48446703e+00\n",
      "Epoch: 16974 mean train loss:  1.58851326e-05, mean val. loss:  3.48455954e+00\n",
      "Epoch: 16975 mean train loss:  1.57025352e-05, mean val. loss:  3.48466039e+00\n",
      "Epoch: 16976 mean train loss:  1.55131565e-05, mean val. loss:  3.48479033e+00\n",
      "Epoch: 16977 mean train loss:  1.55669986e-05, mean val. loss:  3.48494172e+00\n",
      "Epoch: 16978 mean train loss:  1.52872526e-05, mean val. loss:  3.48511839e+00\n",
      "Epoch: 16979 mean train loss:  1.53698784e-05, mean val. loss:  3.48534393e+00\n",
      "Epoch: 16980 mean train loss:  1.57972972e-05, mean val. loss:  3.48549724e+00\n",
      "Epoch: 16981 mean train loss:  1.53371948e-05, mean val. loss:  3.48570919e+00\n",
      "Epoch: 16982 mean train loss:  1.53858855e-05, mean val. loss:  3.48595333e+00\n",
      "Epoch: 16983 mean train loss:  1.53991859e-05, mean val. loss:  3.48624301e+00\n",
      "Epoch: 16984 mean train loss:  1.55365851e-05, mean val. loss:  3.48655224e+00\n",
      "Epoch: 16985 mean train loss:  1.56741880e-05, mean val. loss:  3.48680639e+00\n",
      "Epoch: 16986 mean train loss:  1.56650203e-05, mean val. loss:  3.48707795e+00\n",
      "Epoch: 16987 mean train loss:  1.53054425e-05, mean val. loss:  3.48740673e+00\n",
      "Epoch: 16988 mean train loss:  1.55685702e-05, mean val. loss:  3.48770809e+00\n",
      "Epoch: 16989 mean train loss:  1.54547743e-05, mean val. loss:  3.48808837e+00\n",
      "Epoch: 16990 mean train loss:  1.55061716e-05, mean val. loss:  3.48849082e+00\n",
      "Epoch: 16991 mean train loss:  1.56458700e-05, mean val. loss:  3.48891997e+00\n",
      "Epoch: 16992 mean train loss:  1.55949965e-05, mean val. loss:  3.48933935e+00\n",
      "Epoch: 16993 mean train loss:  1.52555876e-05, mean val. loss:  3.48979115e+00\n",
      "Epoch: 16994 mean train loss:  1.54440349e-05, mean val. loss:  3.49024749e+00\n",
      "Epoch: 16995 mean train loss:  1.52738648e-05, mean val. loss:  3.49073172e+00\n",
      "Epoch: 16996 mean train loss:  1.53969158e-05, mean val. loss:  3.49126148e+00\n",
      "Epoch: 16997 mean train loss:  1.54900481e-05, mean val. loss:  3.49178505e+00\n",
      "Epoch: 16998 mean train loss:  1.52132125e-05, mean val. loss:  3.49236488e+00\n",
      "Epoch: 16999 mean train loss:  1.52703724e-05, mean val. loss:  3.49298930e+00\n",
      "Epoch: 17000 mean train loss:  1.56885071e-05, mean val. loss:  3.49361610e+00\n",
      "Epoch: 17001 mean train loss:  1.56325113e-05, mean val. loss:  3.49420452e+00\n",
      "Epoch: 17002 mean train loss:  1.56899914e-05, mean val. loss:  3.49471450e+00\n",
      "Epoch: 17003 mean train loss:  1.55367015e-05, mean val. loss:  3.49518442e+00\n",
      "Epoch: 17004 mean train loss:  1.53103611e-05, mean val. loss:  3.49565244e+00\n",
      "Epoch: 17005 mean train loss:  1.58047187e-05, mean val. loss:  3.49601793e+00\n",
      "Epoch: 17006 mean train loss:  1.56806200e-05, mean val. loss:  3.49633360e+00\n",
      "Epoch: 17007 mean train loss:  1.53090223e-05, mean val. loss:  3.49660587e+00\n",
      "Epoch: 17008 mean train loss:  1.52387365e-05, mean val. loss:  3.49693155e+00\n",
      "Epoch: 17009 mean train loss:  1.54034642e-05, mean val. loss:  3.49720216e+00\n",
      "Epoch: 17010 mean train loss:  1.55295129e-05, mean val. loss:  3.49743390e+00\n",
      "Epoch: 17011 mean train loss:  1.55236339e-05, mean val. loss:  3.49755120e+00\n",
      "Epoch: 17012 mean train loss:  1.54596346e-05, mean val. loss:  3.49765539e+00\n",
      "Epoch: 17013 mean train loss:  1.54203153e-05, mean val. loss:  3.49777102e+00\n",
      "Epoch: 17014 mean train loss:  1.49942643e-05, mean val. loss:  3.49798703e+00\n",
      "Epoch: 17015 mean train loss:  1.54179870e-05, mean val. loss:  3.49821877e+00\n",
      "Epoch: 17016 mean train loss:  1.54868176e-05, mean val. loss:  3.49842858e+00\n",
      "Epoch: 17017 mean train loss:  1.54078007e-05, mean val. loss:  3.49860883e+00\n",
      "Epoch: 17018 mean train loss:  1.51688291e-05, mean val. loss:  3.49884510e+00\n",
      "Epoch: 17019 mean train loss:  1.55807938e-05, mean val. loss:  3.49911189e+00\n",
      "Epoch: 17020 mean train loss:  1.54773588e-05, mean val. loss:  3.49936676e+00\n",
      "Epoch: 17021 mean train loss:  1.54440640e-05, mean val. loss:  3.49970627e+00\n",
      "Epoch: 17022 mean train loss:  1.51847780e-05, mean val. loss:  3.50010514e+00\n",
      "Epoch: 17023 mean train loss:  1.58291950e-05, mean val. loss:  3.50040460e+00\n",
      "Epoch: 17024 mean train loss:  1.57673494e-05, mean val. loss:  3.50068450e+00\n",
      "Epoch: 17025 mean train loss:  1.52359426e-05, mean val. loss:  3.50100589e+00\n",
      "Epoch: 17026 mean train loss:  1.56950264e-05, mean val. loss:  3.50131464e+00\n",
      "Epoch: 17027 mean train loss:  1.54997106e-05, mean val. loss:  3.50162292e+00\n",
      "Epoch: 17028 mean train loss:  1.54565787e-05, mean val. loss:  3.50193024e+00\n",
      "Epoch: 17029 mean train loss:  1.52450812e-05, mean val. loss:  3.50227094e+00\n",
      "Epoch: 17030 mean train loss:  1.52928114e-05, mean val. loss:  3.50263524e+00\n",
      "Epoch: 17031 mean train loss:  1.53523870e-05, mean val. loss:  3.50300837e+00\n",
      "Epoch: 17032 mean train loss:  1.55355665e-05, mean val. loss:  3.50336790e+00\n",
      "Epoch: 17033 mean train loss:  1.55100715e-05, mean val. loss:  3.50373960e+00\n",
      "Epoch: 17034 mean train loss:  1.55152520e-05, mean val. loss:  3.50403166e+00\n",
      "Epoch: 17035 mean train loss:  1.54533191e-05, mean val. loss:  3.50430322e+00\n",
      "Epoch: 17036 mean train loss:  1.54208392e-05, mean val. loss:  3.50456715e+00\n",
      "Epoch: 17037 mean train loss:  1.54467416e-05, mean val. loss:  3.50477481e+00\n",
      "Epoch: 17038 mean train loss:  1.55183952e-05, mean val. loss:  3.50495934e+00\n",
      "Epoch: 17039 mean train loss:  1.53059955e-05, mean val. loss:  3.50513124e+00\n",
      "Epoch: 17040 mean train loss:  1.57674076e-05, mean val. loss:  3.50531077e+00\n",
      "Epoch: 17041 mean train loss:  1.56900496e-05, mean val. loss:  3.50544596e+00\n",
      "Epoch: 17042 mean train loss:  1.52655412e-05, mean val. loss:  3.50560975e+00\n",
      "Epoch: 17043 mean train loss:  1.53578003e-05, mean val. loss:  3.50572658e+00\n",
      "Epoch: 17044 mean train loss:  1.53332949e-05, mean val. loss:  3.50590253e+00\n",
      "Epoch: 17045 mean train loss:  1.55309099e-05, mean val. loss:  3.50608110e+00\n",
      "Epoch: 17046 mean train loss:  1.53649598e-05, mean val. loss:  3.50623178e+00\n",
      "Epoch: 17047 mean train loss:  1.54677546e-05, mean val. loss:  3.50639844e+00\n",
      "Epoch: 17048 mean train loss:  1.56566384e-05, mean val. loss:  3.50652409e+00\n",
      "Epoch: 17049 mean train loss:  1.54183945e-05, mean val. loss:  3.50665307e+00\n",
      "Epoch: 17050 mean train loss:  1.54241279e-05, mean val. loss:  3.50685215e+00\n",
      "Epoch: 17051 mean train loss:  1.58596667e-05, mean val. loss:  3.50696540e+00\n",
      "Epoch: 17052 mean train loss:  1.56289316e-05, mean val. loss:  3.50708675e+00\n",
      "Epoch: 17053 mean train loss:  1.53974979e-05, mean val. loss:  3.50723886e+00\n",
      "Epoch: 17054 mean train loss:  1.53735455e-05, mean val. loss:  3.50744891e+00\n",
      "Epoch: 17055 mean train loss:  1.54703739e-05, mean val. loss:  3.50762367e+00\n",
      "Epoch: 17056 mean train loss:  1.52678986e-05, mean val. loss:  3.50785136e+00\n",
      "Epoch: 17057 mean train loss:  1.53542205e-05, mean val. loss:  3.50809240e+00\n",
      "Epoch: 17058 mean train loss:  1.52981374e-05, mean val. loss:  3.50834012e+00\n",
      "Epoch: 17059 mean train loss:  1.54656882e-05, mean val. loss:  3.50859809e+00\n",
      "Epoch: 17060 mean train loss:  1.56556198e-05, mean val. loss:  3.50880146e+00\n",
      "Epoch: 17061 mean train loss:  1.54054724e-05, mean val. loss:  3.50898767e+00\n",
      "Epoch: 17062 mean train loss:  1.54405716e-05, mean val. loss:  3.50915527e+00\n",
      "Epoch: 17063 mean train loss:  1.53743895e-05, mean val. loss:  3.50934815e+00\n",
      "Epoch: 17064 mean train loss:  1.51773274e-05, mean val. loss:  3.50956059e+00\n",
      "Epoch: 17065 mean train loss:  1.53251749e-05, mean val. loss:  3.50977492e+00\n",
      "Epoch: 17066 mean train loss:  1.52227585e-05, mean val. loss:  3.51002550e+00\n",
      "Epoch: 17067 mean train loss:  1.55222660e-05, mean val. loss:  3.51027942e+00\n",
      "Epoch: 17068 mean train loss:  1.55167072e-05, mean val. loss:  3.51054406e+00\n",
      "Epoch: 17069 mean train loss:  1.54852169e-05, mean val. loss:  3.51079321e+00\n",
      "Epoch: 17070 mean train loss:  1.50898122e-05, mean val. loss:  3.51115036e+00\n",
      "Epoch: 17071 mean train loss:  1.53050350e-05, mean val. loss:  3.51154399e+00\n",
      "Epoch: 17072 mean train loss:  1.57662143e-05, mean val. loss:  3.51190329e+00\n",
      "Epoch: 17073 mean train loss:  1.55674352e-05, mean val. loss:  3.51226592e+00\n",
      "Epoch: 17074 mean train loss:  1.53037254e-05, mean val. loss:  3.51265812e+00\n",
      "Epoch: 17075 mean train loss:  1.54756417e-05, mean val. loss:  3.51302791e+00\n",
      "Epoch: 17076 mean train loss:  1.55190064e-05, mean val. loss:  3.51340175e+00\n",
      "Epoch: 17077 mean train loss:  1.55485468e-05, mean val. loss:  3.51374817e+00\n",
      "Epoch: 17078 mean train loss:  1.52632128e-05, mean val. loss:  3.51411414e+00\n",
      "Epoch: 17079 mean train loss:  1.53208675e-05, mean val. loss:  3.51451087e+00\n",
      "Epoch: 17080 mean train loss:  1.53790170e-05, mean val. loss:  3.51488590e+00\n",
      "Epoch: 17081 mean train loss:  1.53744768e-05, mean val. loss:  3.51523590e+00\n",
      "Epoch: 17082 mean train loss:  1.55008747e-05, mean val. loss:  3.51552796e+00\n",
      "Epoch: 17083 mean train loss:  1.56881870e-05, mean val. loss:  3.51575994e+00\n",
      "Epoch: 17084 mean train loss:  1.54017471e-05, mean val. loss:  3.51594710e+00\n",
      "Epoch: 17085 mean train loss:  1.53075380e-05, mean val. loss:  3.51611614e+00\n",
      "Epoch: 17086 mean train loss:  1.52640569e-05, mean val. loss:  3.51626921e+00\n",
      "Epoch: 17087 mean train loss:  1.55997404e-05, mean val. loss:  3.51637268e+00\n",
      "Epoch: 17088 mean train loss:  1.56411261e-05, mean val. loss:  3.51650333e+00\n",
      "Epoch: 17089 mean train loss:  1.52910070e-05, mean val. loss:  3.51670742e+00\n",
      "Epoch: 17090 mean train loss:  1.55607413e-05, mean val. loss:  3.51691270e+00\n",
      "Epoch: 17091 mean train loss:  1.53712754e-05, mean val. loss:  3.51724124e+00\n",
      "Epoch: 17092 mean train loss:  1.54681038e-05, mean val. loss:  3.51761413e+00\n",
      "Epoch: 17093 mean train loss:  1.56038732e-05, mean val. loss:  3.51800609e+00\n",
      "Epoch: 17094 mean train loss:  1.55165035e-05, mean val. loss:  3.51840544e+00\n",
      "Epoch: 17095 mean train loss:  1.55779708e-05, mean val. loss:  3.51884675e+00\n",
      "Epoch: 17096 mean train loss:  1.55930175e-05, mean val. loss:  3.51929688e+00\n",
      "Epoch: 17097 mean train loss:  1.54804729e-05, mean val. loss:  3.51977801e+00\n",
      "Epoch: 17098 mean train loss:  1.52218854e-05, mean val. loss:  3.52034926e+00\n",
      "Epoch: 17099 mean train loss:  1.53679284e-05, mean val. loss:  3.52093983e+00\n",
      "Epoch: 17100 mean train loss:  1.53051224e-05, mean val. loss:  3.52156329e+00\n",
      "Epoch: 17101 mean train loss:  1.58470939e-05, mean val. loss:  3.52206206e+00\n",
      "Epoch: 17102 mean train loss:  1.54968584e-05, mean val. loss:  3.52252293e+00\n",
      "Epoch: 17103 mean train loss:  1.56516326e-05, mean val. loss:  3.52291703e+00\n",
      "Epoch: 17104 mean train loss:  1.53614674e-05, mean val. loss:  3.52328682e+00\n",
      "Epoch: 17105 mean train loss:  1.55681919e-05, mean val. loss:  3.52365136e+00\n",
      "Epoch: 17106 mean train loss:  1.56509923e-05, mean val. loss:  3.52394629e+00\n",
      "Epoch: 17107 mean train loss:  1.54817826e-05, mean val. loss:  3.52426577e+00\n",
      "Epoch: 17108 mean train loss:  1.57149625e-05, mean val. loss:  3.52452779e+00\n",
      "Epoch: 17109 mean train loss:  1.53396395e-05, mean val. loss:  3.52475190e+00\n",
      "Epoch: 17110 mean train loss:  1.52902212e-05, mean val. loss:  3.52498937e+00\n",
      "Epoch: 17111 mean train loss:  1.54013687e-05, mean val. loss:  3.52526569e+00\n",
      "Epoch: 17112 mean train loss:  1.51727290e-05, mean val. loss:  3.52558684e+00\n",
      "Epoch: 17113 mean train loss:  1.56641763e-05, mean val. loss:  3.52587652e+00\n",
      "Epoch: 17114 mean train loss:  1.54260197e-05, mean val. loss:  3.52617431e+00\n",
      "Epoch: 17115 mean train loss:  1.53295987e-05, mean val. loss:  3.52648497e+00\n",
      "Epoch: 17116 mean train loss:  1.54718000e-05, mean val. loss:  3.52685070e+00\n",
      "Epoch: 17117 mean train loss:  1.54977024e-05, mean val. loss:  3.52721238e+00\n",
      "Epoch: 17118 mean train loss:  1.55385351e-05, mean val. loss:  3.52761030e+00\n",
      "Epoch: 17119 mean train loss:  1.56120805e-05, mean val. loss:  3.52801371e+00\n",
      "Epoch: 17120 mean train loss:  1.52777066e-05, mean val. loss:  3.52845669e+00\n",
      "Epoch: 17121 mean train loss:  1.54131558e-05, mean val. loss:  3.52883577e+00\n",
      "Epoch: 17122 mean train loss:  1.53324509e-05, mean val. loss:  3.52926207e+00\n",
      "Epoch: 17123 mean train loss:  1.54651352e-05, mean val. loss:  3.52969813e+00\n",
      "Epoch: 17124 mean train loss:  1.52444991e-05, mean val. loss:  3.53018975e+00\n",
      "Epoch: 17125 mean train loss:  1.56643509e-05, mean val. loss:  3.53061247e+00\n",
      "Epoch: 17126 mean train loss:  1.57247414e-05, mean val. loss:  3.53095627e+00\n",
      "Epoch: 17127 mean train loss:  1.56256719e-05, mean val. loss:  3.53121853e+00\n",
      "Epoch: 17128 mean train loss:  1.55846647e-05, mean val. loss:  3.53148389e+00\n",
      "Epoch: 17129 mean train loss:  1.52257271e-05, mean val. loss:  3.53176212e+00\n",
      "Epoch: 17130 mean train loss:  1.53517467e-05, mean val. loss:  3.53199697e+00\n",
      "Epoch: 17131 mean train loss:  1.54245936e-05, mean val. loss:  3.53220057e+00\n",
      "Epoch: 17132 mean train loss:  1.51642889e-05, mean val. loss:  3.53237319e+00\n",
      "Epoch: 17133 mean train loss:  1.54470035e-05, mean val. loss:  3.53251338e+00\n",
      "Epoch: 17134 mean train loss:  1.54084701e-05, mean val. loss:  3.53262210e+00\n",
      "Epoch: 17135 mean train loss:  1.51324493e-05, mean val. loss:  3.53270817e+00\n",
      "Epoch: 17136 mean train loss:  1.55723246e-05, mean val. loss:  3.53278732e+00\n",
      "Epoch: 17137 mean train loss:  1.52549765e-05, mean val. loss:  3.53287959e+00\n",
      "Epoch: 17138 mean train loss:  1.54097506e-05, mean val. loss:  3.53297472e+00\n",
      "Epoch: 17139 mean train loss:  1.54201407e-05, mean val. loss:  3.53306961e+00\n",
      "Epoch: 17140 mean train loss:  1.52011053e-05, mean val. loss:  3.53319359e+00\n",
      "Epoch: 17141 mean train loss:  1.55959569e-05, mean val. loss:  3.53328371e+00\n",
      "Epoch: 17142 mean train loss:  1.52795983e-05, mean val. loss:  3.53345799e+00\n",
      "Epoch: 17143 mean train loss:  1.52519497e-05, mean val. loss:  3.53368902e+00\n",
      "Epoch: 17144 mean train loss:  1.57681934e-05, mean val. loss:  3.53389883e+00\n",
      "Epoch: 17145 mean train loss:  1.53575675e-05, mean val. loss:  3.53417444e+00\n",
      "Epoch: 17146 mean train loss:  1.53726724e-05, mean val. loss:  3.53445435e+00\n",
      "Epoch: 17147 mean train loss:  1.53647270e-05, mean val. loss:  3.53471184e+00\n",
      "Epoch: 17148 mean train loss:  1.54432782e-05, mean val. loss:  3.53494143e+00\n",
      "Epoch: 17149 mean train loss:  1.53610308e-05, mean val. loss:  3.53519130e+00\n",
      "Epoch: 17150 mean train loss:  1.54944428e-05, mean val. loss:  3.53542447e+00\n",
      "Epoch: 17151 mean train loss:  1.55963935e-05, mean val. loss:  3.53559303e+00\n",
      "Epoch: 17152 mean train loss:  1.54795125e-05, mean val. loss:  3.53573799e+00\n",
      "Epoch: 17153 mean train loss:  1.54104200e-05, mean val. loss:  3.53592753e+00\n",
      "Epoch: 17154 mean train loss:  1.50704000e-05, mean val. loss:  3.53616452e+00\n",
      "Epoch: 17155 mean train loss:  1.52786088e-05, mean val. loss:  3.53641438e+00\n",
      "Epoch: 17156 mean train loss:  1.56343449e-05, mean val. loss:  3.53663850e+00\n",
      "Epoch: 17157 mean train loss:  1.50026463e-05, mean val. loss:  3.53695536e+00\n",
      "Epoch: 17158 mean train loss:  1.52348948e-05, mean val. loss:  3.53731775e+00\n",
      "Epoch: 17159 mean train loss:  1.52791035e-05, mean val. loss:  3.53770685e+00\n",
      "Epoch: 17160 mean train loss:  1.56731112e-05, mean val. loss:  3.53810668e+00\n",
      "Epoch: 17161 mean train loss:  1.53825094e-05, mean val. loss:  3.53849316e+00\n",
      "Epoch: 17162 mean train loss:  1.52756402e-05, mean val. loss:  3.53890276e+00\n",
      "Epoch: 17163 mean train loss:  1.53908622e-05, mean val. loss:  3.53932905e+00\n",
      "Epoch: 17164 mean train loss:  1.56303286e-05, mean val. loss:  3.53976607e+00\n",
      "Epoch: 17165 mean train loss:  1.54289010e-05, mean val. loss:  3.54023814e+00\n",
      "Epoch: 17166 mean train loss:  1.55803282e-05, mean val. loss:  3.54069448e+00\n",
      "Epoch: 17167 mean train loss:  1.54816953e-05, mean val. loss:  3.54117465e+00\n",
      "Epoch: 17168 mean train loss:  1.51235727e-05, mean val. loss:  3.54166031e+00\n",
      "Epoch: 17169 mean train loss:  1.51724962e-05, mean val. loss:  3.54218364e+00\n",
      "Epoch: 17170 mean train loss:  1.53651927e-05, mean val. loss:  3.54267001e+00\n",
      "Epoch: 17171 mean train loss:  1.56366732e-05, mean val. loss:  3.54306936e+00\n",
      "Epoch: 17172 mean train loss:  1.52034627e-05, mean val. loss:  3.54348898e+00\n",
      "Epoch: 17173 mean train loss:  1.55408634e-05, mean val. loss:  3.54388142e+00\n",
      "Epoch: 17174 mean train loss:  1.54015725e-05, mean val. loss:  3.54423618e+00\n",
      "Epoch: 17175 mean train loss:  1.54647278e-05, mean val. loss:  3.54458451e+00\n",
      "Epoch: 17176 mean train loss:  1.57264585e-05, mean val. loss:  3.54484034e+00\n",
      "Epoch: 17177 mean train loss:  1.56071910e-05, mean val. loss:  3.54500842e+00\n",
      "Epoch: 17178 mean train loss:  1.51030545e-05, mean val. loss:  3.54520607e+00\n",
      "Epoch: 17179 mean train loss:  1.50066626e-05, mean val. loss:  3.54543686e+00\n",
      "Epoch: 17180 mean train loss:  1.53024448e-05, mean val. loss:  3.54568648e+00\n",
      "Epoch: 17181 mean train loss:  1.55087037e-05, mean val. loss:  3.54593635e+00\n",
      "Epoch: 17182 mean train loss:  1.53615838e-05, mean val. loss:  3.54616785e+00\n",
      "Epoch: 17183 mean train loss:  1.53156288e-05, mean val. loss:  3.54637337e+00\n",
      "Epoch: 17184 mean train loss:  1.52989523e-05, mean val. loss:  3.54656076e+00\n",
      "Epoch: 17185 mean train loss:  1.54495356e-05, mean val. loss:  3.54671621e+00\n",
      "Epoch: 17186 mean train loss:  1.53561996e-05, mean val. loss:  3.54689169e+00\n",
      "Epoch: 17187 mean train loss:  1.56149908e-05, mean val. loss:  3.54707122e+00\n",
      "Epoch: 17188 mean train loss:  1.54155132e-05, mean val. loss:  3.54726458e+00\n",
      "Epoch: 17189 mean train loss:  1.53329165e-05, mean val. loss:  3.54743242e+00\n",
      "Epoch: 17190 mean train loss:  1.52900466e-05, mean val. loss:  3.54761004e+00\n",
      "Epoch: 17191 mean train loss:  1.53854489e-05, mean val. loss:  3.54780865e+00\n",
      "Epoch: 17192 mean train loss:  1.55589078e-05, mean val. loss:  3.54796004e+00\n",
      "Epoch: 17193 mean train loss:  1.54274458e-05, mean val. loss:  3.54805326e+00\n",
      "Epoch: 17194 mean train loss:  1.56151946e-05, mean val. loss:  3.54810476e+00\n",
      "Epoch: 17195 mean train loss:  1.55683956e-05, mean val. loss:  3.54813838e+00\n",
      "Epoch: 17196 mean train loss:  1.54474983e-05, mean val. loss:  3.54815483e+00\n",
      "Epoch: 17197 mean train loss:  1.55554153e-05, mean val. loss:  3.54813027e+00\n",
      "Epoch: 17198 mean train loss:  1.53223518e-05, mean val. loss:  3.54812741e+00\n",
      "Epoch: 17199 mean train loss:  1.54637091e-05, mean val. loss:  3.54816318e+00\n",
      "Epoch: 17200 mean train loss:  1.52831490e-05, mean val. loss:  3.54825830e+00\n",
      "Epoch: 17201 mean train loss:  1.51962158e-05, mean val. loss:  3.54838920e+00\n",
      "Epoch: 17202 mean train loss:  1.54799491e-05, mean val. loss:  3.54851484e+00\n",
      "Epoch: 17203 mean train loss:  1.54409208e-05, mean val. loss:  3.54864812e+00\n",
      "Epoch: 17204 mean train loss:  1.53578294e-05, mean val. loss:  3.54880953e+00\n",
      "Epoch: 17205 mean train loss:  1.55051530e-05, mean val. loss:  3.54898286e+00\n",
      "Epoch: 17206 mean train loss:  1.55327143e-05, mean val. loss:  3.54923439e+00\n",
      "Epoch: 17207 mean train loss:  1.53818692e-05, mean val. loss:  3.54952717e+00\n",
      "Epoch: 17208 mean train loss:  1.53718283e-05, mean val. loss:  3.54988360e+00\n",
      "Epoch: 17209 mean train loss:  1.56251772e-05, mean val. loss:  3.55023146e+00\n",
      "Epoch: 17210 mean train loss:  1.54277950e-05, mean val. loss:  3.55055547e+00\n",
      "Epoch: 17211 mean train loss:  1.55138841e-05, mean val. loss:  3.55087805e+00\n",
      "Epoch: 17212 mean train loss:  1.50863198e-05, mean val. loss:  3.55125022e+00\n",
      "Epoch: 17213 mean train loss:  1.56426977e-05, mean val. loss:  3.55157638e+00\n",
      "Epoch: 17214 mean train loss:  1.54012814e-05, mean val. loss:  3.55193591e+00\n",
      "Epoch: 17215 mean train loss:  1.54719746e-05, mean val. loss:  3.55227804e+00\n",
      "Epoch: 17216 mean train loss:  1.51742133e-05, mean val. loss:  3.55264211e+00\n",
      "Epoch: 17217 mean train loss:  1.53272704e-05, mean val. loss:  3.55297470e+00\n",
      "Epoch: 17218 mean train loss:  1.53530273e-05, mean val. loss:  3.55329657e+00\n",
      "Epoch: 17219 mean train loss:  1.53523288e-05, mean val. loss:  3.55364466e+00\n",
      "Epoch: 17220 mean train loss:  1.54284353e-05, mean val. loss:  3.55402732e+00\n",
      "Epoch: 17221 mean train loss:  1.57171453e-05, mean val. loss:  3.55436635e+00\n",
      "Epoch: 17222 mean train loss:  1.53292785e-05, mean val. loss:  3.55470943e+00\n",
      "Epoch: 17223 mean train loss:  1.54496811e-05, mean val. loss:  3.55502558e+00\n",
      "Epoch: 17224 mean train loss:  1.58441253e-05, mean val. loss:  3.55531073e+00\n",
      "Epoch: 17225 mean train loss:  1.56842871e-05, mean val. loss:  3.55554771e+00\n",
      "Epoch: 17226 mean train loss:  1.53511646e-05, mean val. loss:  3.55584741e+00\n",
      "Epoch: 17227 mean train loss:  1.58005278e-05, mean val. loss:  3.55606937e+00\n",
      "Epoch: 17228 mean train loss:  1.53649016e-05, mean val. loss:  3.55640960e+00\n",
      "Epoch: 17229 mean train loss:  1.53933652e-05, mean val. loss:  3.55672622e+00\n",
      "Epoch: 17230 mean train loss:  1.51274435e-05, mean val. loss:  3.55708241e+00\n",
      "Epoch: 17231 mean train loss:  1.55234011e-05, mean val. loss:  3.55742097e+00\n",
      "Epoch: 17232 mean train loss:  1.53885630e-05, mean val. loss:  3.55774522e+00\n",
      "Epoch: 17233 mean train loss:  1.52477587e-05, mean val. loss:  3.55804729e+00\n",
      "Epoch: 17234 mean train loss:  1.54757290e-05, mean val. loss:  3.55830574e+00\n",
      "Epoch: 17235 mean train loss:  1.55085290e-05, mean val. loss:  3.55854487e+00\n",
      "Epoch: 17236 mean train loss:  1.55757007e-05, mean val. loss:  3.55876446e+00\n",
      "Epoch: 17237 mean train loss:  1.52692373e-05, mean val. loss:  3.55906272e+00\n",
      "Epoch: 17238 mean train loss:  1.52064895e-05, mean val. loss:  3.55939555e+00\n",
      "Epoch: 17239 mean train loss:  1.53481087e-05, mean val. loss:  3.55973506e+00\n",
      "Epoch: 17240 mean train loss:  1.54621666e-05, mean val. loss:  3.56005096e+00\n",
      "Epoch: 17241 mean train loss:  1.56711903e-05, mean val. loss:  3.56032610e+00\n",
      "Epoch: 17242 mean train loss:  1.52492139e-05, mean val. loss:  3.56061482e+00\n",
      "Epoch: 17243 mean train loss:  1.54526788e-05, mean val. loss:  3.56089807e+00\n",
      "Epoch: 17244 mean train loss:  1.53608853e-05, mean val. loss:  3.56118226e+00\n",
      "Epoch: 17245 mean train loss:  1.55050657e-05, mean val. loss:  3.56144500e+00\n",
      "Epoch: 17246 mean train loss:  1.53243309e-05, mean val. loss:  3.56168532e+00\n",
      "Epoch: 17247 mean train loss:  1.53474102e-05, mean val. loss:  3.56191945e+00\n",
      "Epoch: 17248 mean train loss:  1.52446155e-05, mean val. loss:  3.56213355e+00\n",
      "Epoch: 17249 mean train loss:  1.54656882e-05, mean val. loss:  3.56233191e+00\n",
      "Epoch: 17250 mean train loss:  1.54704321e-05, mean val. loss:  3.56252456e+00\n",
      "Epoch: 17251 mean train loss:  1.53151050e-05, mean val. loss:  3.56279492e+00\n",
      "Epoch: 17252 mean train loss:  1.54271256e-05, mean val. loss:  3.56309533e+00\n",
      "Epoch: 17253 mean train loss:  1.53944420e-05, mean val. loss:  3.56336451e+00\n",
      "Epoch: 17254 mean train loss:  1.53299479e-05, mean val. loss:  3.56364512e+00\n",
      "Epoch: 17255 mean train loss:  1.53618748e-05, mean val. loss:  3.56396770e+00\n",
      "Epoch: 17256 mean train loss:  1.53159199e-05, mean val. loss:  3.56428123e+00\n",
      "Epoch: 17257 mean train loss:  1.53150177e-05, mean val. loss:  3.56457162e+00\n",
      "Epoch: 17258 mean train loss:  1.53899018e-05, mean val. loss:  3.56487751e+00\n",
      "Epoch: 17259 mean train loss:  1.51978165e-05, mean val. loss:  3.56520557e+00\n",
      "Epoch: 17260 mean train loss:  1.57026516e-05, mean val. loss:  3.56548214e+00\n",
      "Epoch: 17261 mean train loss:  1.54499430e-05, mean val. loss:  3.56570554e+00\n",
      "Epoch: 17262 mean train loss:  1.51850400e-05, mean val. loss:  3.56592774e+00\n",
      "Epoch: 17263 mean train loss:  1.52754073e-05, mean val. loss:  3.56614900e+00\n",
      "Epoch: 17264 mean train loss:  1.54981390e-05, mean val. loss:  3.56634521e+00\n",
      "Epoch: 17265 mean train loss:  1.51990680e-05, mean val. loss:  3.56652975e+00\n",
      "Epoch: 17266 mean train loss:  1.53442379e-05, mean val. loss:  3.56673598e+00\n",
      "Epoch: 17267 mean train loss:  1.50088454e-05, mean val. loss:  3.56705070e+00\n",
      "Epoch: 17268 mean train loss:  1.49417028e-05, mean val. loss:  3.56741023e+00\n",
      "Epoch: 17269 mean train loss:  1.55910093e-05, mean val. loss:  3.56770420e+00\n",
      "Epoch: 17270 mean train loss:  1.52642897e-05, mean val. loss:  3.56806159e+00\n",
      "Epoch: 17271 mean train loss:  1.53054425e-05, mean val. loss:  3.56844354e+00\n",
      "Epoch: 17272 mean train loss:  1.51667628e-05, mean val. loss:  3.56882954e+00\n",
      "Epoch: 17273 mean train loss:  1.55597809e-05, mean val. loss:  3.56918859e+00\n",
      "Epoch: 17274 mean train loss:  1.53931906e-05, mean val. loss:  3.56958747e+00\n",
      "Epoch: 17275 mean train loss:  1.54726731e-05, mean val. loss:  3.56997919e+00\n",
      "Epoch: 17276 mean train loss:  1.53127476e-05, mean val. loss:  3.57031012e+00\n",
      "Epoch: 17277 mean train loss:  1.51171407e-05, mean val. loss:  3.57067347e+00\n",
      "Epoch: 17278 mean train loss:  1.53593719e-05, mean val. loss:  3.57101488e+00\n",
      "Epoch: 17279 mean train loss:  1.51685672e-05, mean val. loss:  3.57129049e+00\n",
      "Epoch: 17280 mean train loss:  1.51326531e-05, mean val. loss:  3.57160568e+00\n",
      "Epoch: 17281 mean train loss:  1.52956927e-05, mean val. loss:  3.57190323e+00\n",
      "Epoch: 17282 mean train loss:  1.53872825e-05, mean val. loss:  3.57219529e+00\n",
      "Epoch: 17283 mean train loss:  1.52813154e-05, mean val. loss:  3.57247972e+00\n",
      "Epoch: 17284 mean train loss:  1.50422857e-05, mean val. loss:  3.57278109e+00\n",
      "Epoch: 17285 mean train loss:  1.53720321e-05, mean val. loss:  3.57305932e+00\n",
      "Epoch: 17286 mean train loss:  1.52958673e-05, mean val. loss:  3.57333302e+00\n",
      "Epoch: 17287 mean train loss:  1.54801237e-05, mean val. loss:  3.57357836e+00\n",
      "Epoch: 17288 mean train loss:  1.53367873e-05, mean val. loss:  3.57381392e+00\n",
      "Epoch: 17289 mean train loss:  1.51253189e-05, mean val. loss:  3.57405376e+00\n",
      "Epoch: 17290 mean train loss:  1.52503490e-05, mean val. loss:  3.57430077e+00\n",
      "Epoch: 17291 mean train loss:  1.55121379e-05, mean val. loss:  3.57455897e+00\n",
      "Epoch: 17292 mean train loss:  1.53500878e-05, mean val. loss:  3.57485032e+00\n",
      "Epoch: 17293 mean train loss:  1.53299188e-05, mean val. loss:  3.57508326e+00\n",
      "Epoch: 17294 mean train loss:  1.52223511e-05, mean val. loss:  3.57529640e+00\n",
      "Epoch: 17295 mean train loss:  1.53744768e-05, mean val. loss:  3.57548547e+00\n",
      "Epoch: 17296 mean train loss:  1.52228167e-05, mean val. loss:  3.57565451e+00\n",
      "Epoch: 17297 mean train loss:  1.53106521e-05, mean val. loss:  3.57577419e+00\n",
      "Epoch: 17298 mean train loss:  1.57624308e-05, mean val. loss:  3.57580090e+00\n",
      "Epoch: 17299 mean train loss:  1.54039881e-05, mean val. loss:  3.57583880e+00\n",
      "Epoch: 17300 mean train loss:  1.51140848e-05, mean val. loss:  3.57583570e+00\n",
      "Epoch: 17301 mean train loss:  1.57000031e-05, mean val. loss:  3.57575965e+00\n",
      "Epoch: 17302 mean train loss:  1.50756387e-05, mean val. loss:  3.57573104e+00\n",
      "Epoch: 17303 mean train loss:  1.51592540e-05, mean val. loss:  3.57578230e+00\n",
      "Epoch: 17304 mean train loss:  1.55123416e-05, mean val. loss:  3.57583642e+00\n",
      "Epoch: 17305 mean train loss:  1.54644658e-05, mean val. loss:  3.57591987e+00\n",
      "Epoch: 17306 mean train loss:  1.53125147e-05, mean val. loss:  3.57605386e+00\n",
      "Epoch: 17307 mean train loss:  1.51314889e-05, mean val. loss:  3.57627439e+00\n",
      "Epoch: 17308 mean train loss:  1.53937435e-05, mean val. loss:  3.57651472e+00\n",
      "Epoch: 17309 mean train loss:  1.53329747e-05, mean val. loss:  3.57682490e+00\n",
      "Epoch: 17310 mean train loss:  1.53898145e-05, mean val. loss:  3.57717562e+00\n",
      "Epoch: 17311 mean train loss:  1.52189459e-05, mean val. loss:  3.57752347e+00\n",
      "Epoch: 17312 mean train loss:  1.54131267e-05, mean val. loss:  3.57787085e+00\n",
      "Epoch: 17313 mean train loss:  1.52207504e-05, mean val. loss:  3.57825184e+00\n",
      "Epoch: 17314 mean train loss:  1.52023858e-05, mean val. loss:  3.57873392e+00\n",
      "Epoch: 17315 mean train loss:  1.53300643e-05, mean val. loss:  3.57917643e+00\n",
      "Epoch: 17316 mean train loss:  1.57323084e-05, mean val. loss:  3.57953954e+00\n",
      "Epoch: 17317 mean train loss:  1.49914995e-05, mean val. loss:  3.57990623e+00\n",
      "Epoch: 17318 mean train loss:  1.51260465e-05, mean val. loss:  3.58019304e+00\n",
      "Epoch: 17319 mean train loss:  1.49947009e-05, mean val. loss:  3.58046508e+00\n",
      "Epoch: 17320 mean train loss:  1.53757283e-05, mean val. loss:  3.58074188e+00\n",
      "Epoch: 17321 mean train loss:  1.51080603e-05, mean val. loss:  3.58102107e+00\n",
      "Epoch: 17322 mean train loss:  1.50152773e-05, mean val. loss:  3.58132291e+00\n",
      "Epoch: 17323 mean train loss:  1.50665292e-05, mean val. loss:  3.58160567e+00\n",
      "Epoch: 17324 mean train loss:  1.53621077e-05, mean val. loss:  3.58188152e+00\n",
      "Epoch: 17325 mean train loss:  1.53085566e-05, mean val. loss:  3.58218741e+00\n",
      "Epoch: 17326 mean train loss:  1.54298032e-05, mean val. loss:  3.58250904e+00\n",
      "Epoch: 17327 mean train loss:  1.53568108e-05, mean val. loss:  3.58283424e+00\n",
      "Epoch: 17328 mean train loss:  1.52779685e-05, mean val. loss:  3.58317089e+00\n",
      "Epoch: 17329 mean train loss:  1.52551802e-05, mean val. loss:  3.58355808e+00\n",
      "Epoch: 17330 mean train loss:  1.54465379e-05, mean val. loss:  3.58393836e+00\n",
      "Epoch: 17331 mean train loss:  1.53441797e-05, mean val. loss:  3.58433414e+00\n",
      "Epoch: 17332 mean train loss:  1.54176378e-05, mean val. loss:  3.58468390e+00\n",
      "Epoch: 17333 mean train loss:  1.53537258e-05, mean val. loss:  3.58503628e+00\n",
      "Epoch: 17334 mean train loss:  1.54299778e-05, mean val. loss:  3.58536887e+00\n",
      "Epoch: 17335 mean train loss:  1.53276196e-05, mean val. loss:  3.58566689e+00\n",
      "Epoch: 17336 mean train loss:  1.52985449e-05, mean val. loss:  3.58592844e+00\n",
      "Epoch: 17337 mean train loss:  1.52811990e-05, mean val. loss:  3.58621430e+00\n",
      "Epoch: 17338 mean train loss:  1.54015725e-05, mean val. loss:  3.58644438e+00\n",
      "Epoch: 17339 mean train loss:  1.55803864e-05, mean val. loss:  3.58658719e+00\n",
      "Epoch: 17340 mean train loss:  1.54690351e-05, mean val. loss:  3.58671546e+00\n",
      "Epoch: 17341 mean train loss:  1.54055015e-05, mean val. loss:  3.58681250e+00\n",
      "Epoch: 17342 mean train loss:  1.53641158e-05, mean val. loss:  3.58687282e+00\n",
      "Epoch: 17343 mean train loss:  1.48795079e-05, mean val. loss:  3.58698416e+00\n",
      "Epoch: 17344 mean train loss:  1.53783476e-05, mean val. loss:  3.58712673e+00\n",
      "Epoch: 17345 mean train loss:  1.49946136e-05, mean val. loss:  3.58732033e+00\n",
      "Epoch: 17346 mean train loss:  1.54227891e-05, mean val. loss:  3.58751249e+00\n",
      "Epoch: 17347 mean train loss:  1.51184504e-05, mean val. loss:  3.58778119e+00\n",
      "Epoch: 17348 mean train loss:  1.52115244e-05, mean val. loss:  3.58807588e+00\n",
      "Epoch: 17349 mean train loss:  1.55930175e-05, mean val. loss:  3.58832598e+00\n",
      "Epoch: 17350 mean train loss:  1.52678986e-05, mean val. loss:  3.58859420e+00\n",
      "Epoch: 17351 mean train loss:  1.54028821e-05, mean val. loss:  3.58888102e+00\n",
      "Epoch: 17352 mean train loss:  1.53227593e-05, mean val. loss:  3.58920908e+00\n",
      "Epoch: 17353 mean train loss:  1.53667061e-05, mean val. loss:  3.58953071e+00\n",
      "Epoch: 17354 mean train loss:  1.53833826e-05, mean val. loss:  3.58988166e+00\n",
      "Epoch: 17355 mean train loss:  1.52442663e-05, mean val. loss:  3.59025335e+00\n",
      "Epoch: 17356 mean train loss:  1.51627464e-05, mean val. loss:  3.59059691e+00\n",
      "Epoch: 17357 mean train loss:  1.53388537e-05, mean val. loss:  3.59093475e+00\n",
      "Epoch: 17358 mean train loss:  1.52130378e-05, mean val. loss:  3.59124994e+00\n",
      "Epoch: 17359 mean train loss:  1.53256115e-05, mean val. loss:  3.59152484e+00\n",
      "Epoch: 17360 mean train loss:  1.52144930e-05, mean val. loss:  3.59180021e+00\n",
      "Epoch: 17361 mean train loss:  1.50390551e-05, mean val. loss:  3.59209752e+00\n",
      "Epoch: 17362 mean train loss:  1.52584689e-05, mean val. loss:  3.59234834e+00\n",
      "Epoch: 17363 mean train loss:  1.54358277e-05, mean val. loss:  3.59258342e+00\n",
      "Epoch: 17364 mean train loss:  1.51595741e-05, mean val. loss:  3.59286118e+00\n",
      "Epoch: 17365 mean train loss:  1.50459819e-05, mean val. loss:  3.59316063e+00\n",
      "Epoch: 17366 mean train loss:  1.54533773e-05, mean val. loss:  3.59342551e+00\n",
      "Epoch: 17367 mean train loss:  1.52772409e-05, mean val. loss:  3.59367514e+00\n",
      "Epoch: 17368 mean train loss:  1.50625710e-05, mean val. loss:  3.59398079e+00\n",
      "Epoch: 17369 mean train loss:  1.52762223e-05, mean val. loss:  3.59431744e+00\n",
      "Epoch: 17370 mean train loss:  1.52881548e-05, mean val. loss:  3.59470272e+00\n",
      "Epoch: 17371 mean train loss:  1.54847512e-05, mean val. loss:  3.59504008e+00\n",
      "Epoch: 17372 mean train loss:  1.53410365e-05, mean val. loss:  3.59535122e+00\n",
      "Epoch: 17373 mean train loss:  1.52904249e-05, mean val. loss:  3.59563518e+00\n",
      "Epoch: 17374 mean train loss:  1.54605659e-05, mean val. loss:  3.59586740e+00\n",
      "Epoch: 17375 mean train loss:  1.50029955e-05, mean val. loss:  3.59614658e+00\n",
      "Epoch: 17376 mean train loss:  1.53121364e-05, mean val. loss:  3.59640479e+00\n",
      "Epoch: 17377 mean train loss:  1.53347210e-05, mean val. loss:  3.59665346e+00\n",
      "Epoch: 17378 mean train loss:  1.52419379e-05, mean val. loss:  3.59694242e+00\n",
      "Epoch: 17379 mean train loss:  1.55119924e-05, mean val. loss:  3.59719348e+00\n",
      "Epoch: 17380 mean train loss:  1.52116118e-05, mean val. loss:  3.59743094e+00\n",
      "Epoch: 17381 mean train loss:  1.50572450e-05, mean val. loss:  3.59769297e+00\n",
      "Epoch: 17382 mean train loss:  1.52138527e-05, mean val. loss:  3.59796333e+00\n",
      "Epoch: 17383 mean train loss:  1.52483117e-05, mean val. loss:  3.59818459e+00\n",
      "Epoch: 17384 mean train loss:  1.51144923e-05, mean val. loss:  3.59851408e+00\n",
      "Epoch: 17385 mean train loss:  1.52538996e-05, mean val. loss:  3.59886169e+00\n",
      "Epoch: 17386 mean train loss:  1.52849825e-05, mean val. loss:  3.59914708e+00\n",
      "Epoch: 17387 mean train loss:  1.53516012e-05, mean val. loss:  3.59947467e+00\n",
      "Epoch: 17388 mean train loss:  1.49357656e-05, mean val. loss:  3.59987283e+00\n",
      "Epoch: 17389 mean train loss:  1.53397268e-05, mean val. loss:  3.60026360e+00\n",
      "Epoch: 17390 mean train loss:  1.53664150e-05, mean val. loss:  3.60067129e+00\n",
      "Epoch: 17391 mean train loss:  1.51353015e-05, mean val. loss:  3.60107541e+00\n",
      "Epoch: 17392 mean train loss:  1.52377761e-05, mean val. loss:  3.60146141e+00\n",
      "Epoch: 17393 mean train loss:  1.51907734e-05, mean val. loss:  3.60186696e+00\n",
      "Epoch: 17394 mean train loss:  1.51417917e-05, mean val. loss:  3.60225463e+00\n",
      "Epoch: 17395 mean train loss:  1.53659785e-05, mean val. loss:  3.60260701e+00\n",
      "Epoch: 17396 mean train loss:  1.53384462e-05, mean val. loss:  3.60291815e+00\n",
      "Epoch: 17397 mean train loss:  1.53394067e-05, mean val. loss:  3.60319781e+00\n",
      "Epoch: 17398 mean train loss:  1.53832952e-05, mean val. loss:  3.60342336e+00\n",
      "Epoch: 17399 mean train loss:  1.51333516e-05, mean val. loss:  3.60365057e+00\n",
      "Epoch: 17400 mean train loss:  1.53826550e-05, mean val. loss:  3.60384941e+00\n",
      "Epoch: 17401 mean train loss:  1.51126296e-05, mean val. loss:  3.60403347e+00\n",
      "Epoch: 17402 mean train loss:  1.51566928e-05, mean val. loss:  3.60420966e+00\n",
      "Epoch: 17403 mean train loss:  1.56140886e-05, mean val. loss:  3.60431123e+00\n",
      "Epoch: 17404 mean train loss:  1.51913264e-05, mean val. loss:  3.60444212e+00\n",
      "Epoch: 17405 mean train loss:  1.52316352e-05, mean val. loss:  3.60460162e+00\n",
      "Epoch: 17406 mean train loss:  1.53244473e-05, mean val. loss:  3.60483813e+00\n",
      "Epoch: 17407 mean train loss:  1.53790461e-05, mean val. loss:  3.60512161e+00\n",
      "Epoch: 17408 mean train loss:  1.52238645e-05, mean val. loss:  3.60545325e+00\n",
      "Epoch: 17409 mean train loss:  1.53595465e-05, mean val. loss:  3.60577822e+00\n",
      "Epoch: 17410 mean train loss:  1.51698478e-05, mean val. loss:  3.60611582e+00\n",
      "Epoch: 17411 mean train loss:  1.56406022e-05, mean val. loss:  3.60641170e+00\n",
      "Epoch: 17412 mean train loss:  1.52254361e-05, mean val. loss:  3.60673404e+00\n",
      "Epoch: 17413 mean train loss:  1.53134461e-05, mean val. loss:  3.60706592e+00\n",
      "Epoch: 17414 mean train loss:  1.55413873e-05, mean val. loss:  3.60736895e+00\n",
      "Epoch: 17415 mean train loss:  1.52353023e-05, mean val. loss:  3.60773945e+00\n",
      "Epoch: 17416 mean train loss:  1.52872235e-05, mean val. loss:  3.60807991e+00\n",
      "Epoch: 17417 mean train loss:  1.51375716e-05, mean val. loss:  3.60851049e+00\n",
      "Epoch: 17418 mean train loss:  1.55180169e-05, mean val. loss:  3.60890007e+00\n",
      "Epoch: 17419 mean train loss:  1.50977867e-05, mean val. loss:  3.60931110e+00\n",
      "Epoch: 17420 mean train loss:  1.52261055e-05, mean val. loss:  3.60969162e+00\n",
      "Epoch: 17421 mean train loss:  1.53971778e-05, mean val. loss:  3.61002755e+00\n",
      "Epoch: 17422 mean train loss:  1.52354769e-05, mean val. loss:  3.61035800e+00\n",
      "Epoch: 17423 mean train loss:  1.51883869e-05, mean val. loss:  3.61070418e+00\n",
      "Epoch: 17424 mean train loss:  1.54468289e-05, mean val. loss:  3.61096311e+00\n",
      "Epoch: 17425 mean train loss:  1.53330911e-05, mean val. loss:  3.61126184e+00\n",
      "Epoch: 17426 mean train loss:  1.51871354e-05, mean val. loss:  3.61165071e+00\n",
      "Epoch: 17427 mean train loss:  1.53638539e-05, mean val. loss:  3.61200333e+00\n",
      "Epoch: 17428 mean train loss:  1.51288696e-05, mean val. loss:  3.61240625e+00\n",
      "Epoch: 17429 mean train loss:  1.53960427e-05, mean val. loss:  3.61275935e+00\n",
      "Epoch: 17430 mean train loss:  1.53574510e-05, mean val. loss:  3.61306977e+00\n",
      "Epoch: 17431 mean train loss:  1.54963927e-05, mean val. loss:  3.61333466e+00\n",
      "Epoch: 17432 mean train loss:  1.51734275e-05, mean val. loss:  3.61355186e+00\n",
      "Epoch: 17433 mean train loss:  1.50432170e-05, mean val. loss:  3.61380482e+00\n",
      "Epoch: 17434 mean train loss:  1.52364955e-05, mean val. loss:  3.61407280e+00\n",
      "Epoch: 17435 mean train loss:  1.53956353e-05, mean val. loss:  3.61426425e+00\n",
      "Epoch: 17436 mean train loss:  1.55228772e-05, mean val. loss:  3.61442232e+00\n",
      "Epoch: 17437 mean train loss:  1.51576824e-05, mean val. loss:  3.61462736e+00\n",
      "Epoch: 17438 mean train loss:  1.48817198e-05, mean val. loss:  3.61488509e+00\n",
      "Epoch: 17439 mean train loss:  1.50959240e-05, mean val. loss:  3.61513853e+00\n",
      "Epoch: 17440 mean train loss:  1.50797423e-05, mean val. loss:  3.61544943e+00\n",
      "Epoch: 17441 mean train loss:  1.53476140e-05, mean val. loss:  3.61572433e+00\n",
      "Epoch: 17442 mean train loss:  1.52336725e-05, mean val. loss:  3.61597776e+00\n",
      "Epoch: 17443 mean train loss:  1.52286666e-05, mean val. loss:  3.61624169e+00\n",
      "Epoch: 17444 mean train loss:  1.51196145e-05, mean val. loss:  3.61651635e+00\n",
      "Epoch: 17445 mean train loss:  1.52650464e-05, mean val. loss:  3.61677766e+00\n",
      "Epoch: 17446 mean train loss:  1.51528511e-05, mean val. loss:  3.61708069e+00\n",
      "Epoch: 17447 mean train loss:  1.55635062e-05, mean val. loss:  3.61729479e+00\n",
      "Epoch: 17448 mean train loss:  1.54683366e-05, mean val. loss:  3.61748362e+00\n",
      "Epoch: 17449 mean train loss:  1.51614659e-05, mean val. loss:  3.61771417e+00\n",
      "Epoch: 17450 mean train loss:  1.51118729e-05, mean val. loss:  3.61798573e+00\n",
      "Epoch: 17451 mean train loss:  1.49221160e-05, mean val. loss:  3.61829901e+00\n",
      "Epoch: 17452 mean train loss:  1.50306441e-05, mean val. loss:  3.61863160e+00\n",
      "Epoch: 17453 mean train loss:  1.51463319e-05, mean val. loss:  3.61900163e+00\n",
      "Epoch: 17454 mean train loss:  1.49430125e-05, mean val. loss:  3.61936522e+00\n",
      "Epoch: 17455 mean train loss:  1.55089365e-05, mean val. loss:  3.61969066e+00\n",
      "Epoch: 17456 mean train loss:  1.50933629e-05, mean val. loss:  3.62005520e+00\n",
      "Epoch: 17457 mean train loss:  1.54015725e-05, mean val. loss:  3.62037206e+00\n",
      "Epoch: 17458 mean train loss:  1.52245339e-05, mean val. loss:  3.62070346e+00\n",
      "Epoch: 17459 mean train loss:  1.50371634e-05, mean val. loss:  3.62106848e+00\n",
      "Epoch: 17460 mean train loss:  1.52033754e-05, mean val. loss:  3.62143397e+00\n",
      "Epoch: 17461 mean train loss:  1.50935084e-05, mean val. loss:  3.62181067e+00\n",
      "Epoch: 17462 mean train loss:  1.50693813e-05, mean val. loss:  3.62220311e+00\n",
      "Epoch: 17463 mean train loss:  1.53052970e-05, mean val. loss:  3.62257528e+00\n",
      "Epoch: 17464 mean train loss:  1.51491549e-05, mean val. loss:  3.62294745e+00\n",
      "Epoch: 17465 mean train loss:  1.52623397e-05, mean val. loss:  3.62334418e+00\n",
      "Epoch: 17466 mean train loss:  1.52759312e-05, mean val. loss:  3.62371230e+00\n",
      "Epoch: 17467 mean train loss:  1.48787221e-05, mean val. loss:  3.62411690e+00\n",
      "Epoch: 17468 mean train loss:  1.53681904e-05, mean val. loss:  3.62449408e+00\n",
      "Epoch: 17469 mean train loss:  1.54059380e-05, mean val. loss:  3.62484026e+00\n",
      "Epoch: 17470 mean train loss:  1.51278800e-05, mean val. loss:  3.62515736e+00\n",
      "Epoch: 17471 mean train loss:  1.54423178e-05, mean val. loss:  3.62539625e+00\n",
      "Epoch: 17472 mean train loss:  1.51387358e-05, mean val. loss:  3.62561941e+00\n",
      "Epoch: 17473 mean train loss:  1.52465946e-05, mean val. loss:  3.62580466e+00\n",
      "Epoch: 17474 mean train loss:  1.50961569e-05, mean val. loss:  3.62596607e+00\n",
      "Epoch: 17475 mean train loss:  1.52971770e-05, mean val. loss:  3.62607527e+00\n",
      "Epoch: 17476 mean train loss:  1.52683351e-05, mean val. loss:  3.62614894e+00\n",
      "Epoch: 17477 mean train loss:  1.52024149e-05, mean val. loss:  3.62617779e+00\n",
      "Epoch: 17478 mean train loss:  1.51428976e-05, mean val. loss:  3.62616587e+00\n",
      "Epoch: 17479 mean train loss:  1.51646382e-05, mean val. loss:  3.62610483e+00\n",
      "Epoch: 17480 mean train loss:  1.51988643e-05, mean val. loss:  3.62608385e+00\n",
      "Epoch: 17481 mean train loss:  1.55403977e-05, mean val. loss:  3.62601185e+00\n",
      "Epoch: 17482 mean train loss:  1.54281443e-05, mean val. loss:  3.62595272e+00\n",
      "Epoch: 17483 mean train loss:  1.50551787e-05, mean val. loss:  3.62598133e+00\n",
      "Epoch: 17484 mean train loss:  1.50845444e-05, mean val. loss:  3.62604666e+00\n",
      "Epoch: 17485 mean train loss:  1.50623964e-05, mean val. loss:  3.62614226e+00\n",
      "Epoch: 17486 mean train loss:  1.53091387e-05, mean val. loss:  3.62621975e+00\n",
      "Epoch: 17487 mean train loss:  1.54466834e-05, mean val. loss:  3.62627316e+00\n",
      "Epoch: 17488 mean train loss:  1.52371940e-05, mean val. loss:  3.62642741e+00\n",
      "Epoch: 17489 mean train loss:  1.51832355e-05, mean val. loss:  3.62664819e+00\n",
      "Epoch: 17490 mean train loss:  1.54036970e-05, mean val. loss:  3.62689209e+00\n",
      "Epoch: 17491 mean train loss:  1.51981076e-05, mean val. loss:  3.62722445e+00\n",
      "Epoch: 17492 mean train loss:  1.52100110e-05, mean val. loss:  3.62760329e+00\n",
      "Epoch: 17493 mean train loss:  1.50411506e-05, mean val. loss:  3.62806249e+00\n",
      "Epoch: 17494 mean train loss:  1.52961584e-05, mean val. loss:  3.62848520e+00\n",
      "Epoch: 17495 mean train loss:  1.52350985e-05, mean val. loss:  3.62895274e+00\n",
      "Epoch: 17496 mean train loss:  1.50925189e-05, mean val. loss:  3.62946510e+00\n",
      "Epoch: 17497 mean train loss:  1.51751447e-05, mean val. loss:  3.63004708e+00\n",
      "Epoch: 17498 mean train loss:  1.53055298e-05, mean val. loss:  3.63060164e+00\n",
      "Epoch: 17499 mean train loss:  1.49322441e-05, mean val. loss:  3.63118148e+00\n",
      "Epoch: 17500 mean train loss:  1.55300077e-05, mean val. loss:  3.63175988e+00\n",
      "Epoch: 17501 mean train loss:  1.50905398e-05, mean val. loss:  3.63230801e+00\n",
      "Epoch: 17502 mean train loss:  1.51754066e-05, mean val. loss:  3.63282704e+00\n",
      "Epoch: 17503 mean train loss:  1.54940353e-05, mean val. loss:  3.63326621e+00\n",
      "Epoch: 17504 mean train loss:  1.52397843e-05, mean val. loss:  3.63365602e+00\n",
      "Epoch: 17505 mean train loss:  1.50203996e-05, mean val. loss:  3.63405776e+00\n",
      "Epoch: 17506 mean train loss:  1.51540735e-05, mean val. loss:  3.63440347e+00\n",
      "Epoch: 17507 mean train loss:  1.51934801e-05, mean val. loss:  3.63476682e+00\n",
      "Epoch: 17508 mean train loss:  1.52664143e-05, mean val. loss:  3.63514829e+00\n",
      "Epoch: 17509 mean train loss:  1.51379791e-05, mean val. loss:  3.63548946e+00\n",
      "Epoch: 17510 mean train loss:  1.50358828e-05, mean val. loss:  3.63581538e+00\n",
      "Epoch: 17511 mean train loss:  1.51146087e-05, mean val. loss:  3.63615990e+00\n",
      "Epoch: 17512 mean train loss:  1.49953994e-05, mean val. loss:  3.63659620e+00\n",
      "Epoch: 17513 mean train loss:  1.51491840e-05, mean val. loss:  3.63704610e+00\n",
      "Epoch: 17514 mean train loss:  1.56906608e-05, mean val. loss:  3.63740325e+00\n",
      "Epoch: 17515 mean train loss:  1.54843146e-05, mean val. loss:  3.63770843e+00\n",
      "Epoch: 17516 mean train loss:  1.51467975e-05, mean val. loss:  3.63798189e+00\n",
      "Epoch: 17517 mean train loss:  1.55497983e-05, mean val. loss:  3.63822532e+00\n",
      "Epoch: 17518 mean train loss:  1.51762506e-05, mean val. loss:  3.63842845e+00\n",
      "Epoch: 17519 mean train loss:  1.52195862e-05, mean val. loss:  3.63861847e+00\n",
      "Epoch: 17520 mean train loss:  1.52506400e-05, mean val. loss:  3.63881755e+00\n",
      "Epoch: 17521 mean train loss:  1.54304144e-05, mean val. loss:  3.63898945e+00\n",
      "Epoch: 17522 mean train loss:  1.52017747e-05, mean val. loss:  3.63916183e+00\n",
      "Epoch: 17523 mean train loss:  1.49398693e-05, mean val. loss:  3.63938880e+00\n",
      "Epoch: 17524 mean train loss:  1.48165855e-05, mean val. loss:  3.63969946e+00\n",
      "Epoch: 17525 mean train loss:  1.50224660e-05, mean val. loss:  3.64001060e+00\n",
      "Epoch: 17526 mean train loss:  1.49994448e-05, mean val. loss:  3.64032125e+00\n",
      "Epoch: 17527 mean train loss:  1.52146094e-05, mean val. loss:  3.64061403e+00\n",
      "Epoch: 17528 mean train loss:  1.53863220e-05, mean val. loss:  3.64083338e+00\n",
      "Epoch: 17529 mean train loss:  1.50482811e-05, mean val. loss:  3.64109921e+00\n",
      "Epoch: 17530 mean train loss:  1.51924032e-05, mean val. loss:  3.64130783e+00\n",
      "Epoch: 17531 mean train loss:  1.49688567e-05, mean val. loss:  3.64155269e+00\n",
      "Epoch: 17532 mean train loss:  1.51886779e-05, mean val. loss:  3.64175487e+00\n",
      "Epoch: 17533 mean train loss:  1.52068678e-05, mean val. loss:  3.64196730e+00\n",
      "Epoch: 17534 mean train loss:  1.49847474e-05, mean val. loss:  3.64216185e+00\n",
      "Epoch: 17535 mean train loss:  1.50712149e-05, mean val. loss:  3.64235687e+00\n",
      "Epoch: 17536 mean train loss:  1.50844862e-05, mean val. loss:  3.64259315e+00\n",
      "Epoch: 17537 mean train loss:  1.54782610e-05, mean val. loss:  3.64281082e+00\n",
      "Epoch: 17538 mean train loss:  1.50361157e-05, mean val. loss:  3.64305210e+00\n",
      "Epoch: 17539 mean train loss:  1.51631539e-05, mean val. loss:  3.64328504e+00\n",
      "Epoch: 17540 mean train loss:  1.51440618e-05, mean val. loss:  3.64347720e+00\n",
      "Epoch: 17541 mean train loss:  1.52644352e-05, mean val. loss:  3.64368296e+00\n",
      "Epoch: 17542 mean train loss:  1.53257861e-05, mean val. loss:  3.64389515e+00\n",
      "Epoch: 17543 mean train loss:  1.54959853e-05, mean val. loss:  3.64406490e+00\n",
      "Epoch: 17544 mean train loss:  1.52685971e-05, mean val. loss:  3.64417768e+00\n",
      "Epoch: 17545 mean train loss:  1.51247368e-05, mean val. loss:  3.64432311e+00\n",
      "Epoch: 17546 mean train loss:  1.51903369e-05, mean val. loss:  3.64451909e+00\n",
      "Epoch: 17547 mean train loss:  1.47833780e-05, mean val. loss:  3.64475131e+00\n",
      "Epoch: 17548 mean train loss:  1.53323053e-05, mean val. loss:  3.64499950e+00\n",
      "Epoch: 17549 mean train loss:  1.53855653e-05, mean val. loss:  3.64523458e+00\n",
      "Epoch: 17550 mean train loss:  1.49302359e-05, mean val. loss:  3.64551759e+00\n",
      "Epoch: 17551 mean train loss:  1.51853019e-05, mean val. loss:  3.64578962e+00\n",
      "Epoch: 17552 mean train loss:  1.51575659e-05, mean val. loss:  3.64606524e+00\n",
      "Epoch: 17553 mean train loss:  1.54766021e-05, mean val. loss:  3.64632416e+00\n",
      "Epoch: 17554 mean train loss:  1.49372499e-05, mean val. loss:  3.64663363e+00\n",
      "Epoch: 17555 mean train loss:  1.51920249e-05, mean val. loss:  3.64702678e+00\n",
      "Epoch: 17556 mean train loss:  1.52435096e-05, mean val. loss:  3.64739704e+00\n",
      "Epoch: 17557 mean train loss:  1.53942965e-05, mean val. loss:  3.64768147e+00\n",
      "Epoch: 17558 mean train loss:  1.50643173e-05, mean val. loss:  3.64797640e+00\n",
      "Epoch: 17559 mean train loss:  1.49127736e-05, mean val. loss:  3.64830470e+00\n",
      "Epoch: 17560 mean train loss:  1.50049163e-05, mean val. loss:  3.64872670e+00\n",
      "Epoch: 17561 mean train loss:  1.49351836e-05, mean val. loss:  3.64920259e+00\n",
      "Epoch: 17562 mean train loss:  1.52565481e-05, mean val. loss:  3.64966679e+00\n",
      "Epoch: 17563 mean train loss:  1.54724112e-05, mean val. loss:  3.65009093e+00\n",
      "Epoch: 17564 mean train loss:  1.53412111e-05, mean val. loss:  3.65047789e+00\n",
      "Epoch: 17565 mean train loss:  1.52945868e-05, mean val. loss:  3.65084028e+00\n",
      "Epoch: 17566 mean train loss:  1.49276166e-05, mean val. loss:  3.65123010e+00\n",
      "Epoch: 17567 mean train loss:  1.48083782e-05, mean val. loss:  3.65168524e+00\n",
      "Epoch: 17568 mean train loss:  1.51972345e-05, mean val. loss:  3.65211368e+00\n",
      "Epoch: 17569 mean train loss:  1.52882712e-05, mean val. loss:  3.65251780e+00\n",
      "Epoch: 17570 mean train loss:  1.52777648e-05, mean val. loss:  3.65285778e+00\n",
      "Epoch: 17571 mean train loss:  1.53904839e-05, mean val. loss:  3.65315914e+00\n",
      "Epoch: 17572 mean train loss:  1.50548294e-05, mean val. loss:  3.65346551e+00\n",
      "Epoch: 17573 mean train loss:  1.52404536e-05, mean val. loss:  3.65373945e+00\n",
      "Epoch: 17574 mean train loss:  1.52757275e-05, mean val. loss:  3.65397668e+00\n",
      "Epoch: 17575 mean train loss:  1.51495915e-05, mean val. loss:  3.65417123e+00\n",
      "Epoch: 17576 mean train loss:  1.52909197e-05, mean val. loss:  3.65431046e+00\n",
      "Epoch: 17577 mean train loss:  1.50277629e-05, mean val. loss:  3.65447187e+00\n",
      "Epoch: 17578 mean train loss:  1.48986001e-05, mean val. loss:  3.65467191e+00\n",
      "Epoch: 17579 mean train loss:  1.52232533e-05, mean val. loss:  3.65489388e+00\n",
      "Epoch: 17580 mean train loss:  1.51229324e-05, mean val. loss:  3.65510917e+00\n",
      "Epoch: 17581 mean train loss:  1.49714178e-05, mean val. loss:  3.65534401e+00\n",
      "Epoch: 17582 mean train loss:  1.54418813e-05, mean val. loss:  3.65557361e+00\n",
      "Epoch: 17583 mean train loss:  1.51610002e-05, mean val. loss:  3.65583110e+00\n",
      "Epoch: 17584 mean train loss:  1.53692672e-05, mean val. loss:  3.65608096e+00\n",
      "Epoch: 17585 mean train loss:  1.56663300e-05, mean val. loss:  3.65630746e+00\n",
      "Epoch: 17586 mean train loss:  1.51563727e-05, mean val. loss:  3.65662622e+00\n",
      "Epoch: 17587 mean train loss:  1.49621337e-05, mean val. loss:  3.65704656e+00\n",
      "Epoch: 17588 mean train loss:  1.49768894e-05, mean val. loss:  3.65752316e+00\n",
      "Epoch: 17589 mean train loss:  1.50254345e-05, mean val. loss:  3.65800047e+00\n",
      "Epoch: 17590 mean train loss:  1.54141744e-05, mean val. loss:  3.65840912e+00\n",
      "Epoch: 17591 mean train loss:  1.49939151e-05, mean val. loss:  3.65885568e+00\n",
      "Epoch: 17592 mean train loss:  1.53421133e-05, mean val. loss:  3.65928078e+00\n",
      "Epoch: 17593 mean train loss:  1.50226406e-05, mean val. loss:  3.65968418e+00\n",
      "Epoch: 17594 mean train loss:  1.52981374e-05, mean val. loss:  3.65999770e+00\n",
      "Epoch: 17595 mean train loss:  1.51031709e-05, mean val. loss:  3.66026306e+00\n",
      "Epoch: 17596 mean train loss:  1.52932189e-05, mean val. loss:  3.66040015e+00\n",
      "Epoch: 17597 mean train loss:  1.48650433e-05, mean val. loss:  3.66056514e+00\n",
      "Epoch: 17598 mean train loss:  1.52469729e-05, mean val. loss:  3.66073680e+00\n",
      "Epoch: 17599 mean train loss:  1.54534937e-05, mean val. loss:  3.66084886e+00\n",
      "Epoch: 17600 mean train loss:  1.53520086e-05, mean val. loss:  3.66097021e+00\n",
      "Epoch: 17601 mean train loss:  1.49994739e-05, mean val. loss:  3.66113663e+00\n",
      "Epoch: 17602 mean train loss:  1.50966516e-05, mean val. loss:  3.66131330e+00\n",
      "Epoch: 17603 mean train loss:  1.51074782e-05, mean val. loss:  3.66156125e+00\n",
      "Epoch: 17604 mean train loss:  1.51613494e-05, mean val. loss:  3.66184521e+00\n",
      "Epoch: 17605 mean train loss:  1.51782879e-05, mean val. loss:  3.66214657e+00\n",
      "Epoch: 17606 mean train loss:  1.54249428e-05, mean val. loss:  3.66240144e+00\n",
      "Epoch: 17607 mean train loss:  1.49640837e-05, mean val. loss:  3.66268539e+00\n",
      "Epoch: 17608 mean train loss:  1.55912421e-05, mean val. loss:  3.66298199e+00\n",
      "Epoch: 17609 mean train loss:  1.51904242e-05, mean val. loss:  3.66328144e+00\n",
      "Epoch: 17610 mean train loss:  1.51118438e-05, mean val. loss:  3.66352415e+00\n",
      "Epoch: 17611 mean train loss:  1.53151341e-05, mean val. loss:  3.66371751e+00\n",
      "Epoch: 17612 mean train loss:  1.50687993e-05, mean val. loss:  3.66395760e+00\n",
      "Epoch: 17613 mean train loss:  1.49236585e-05, mean val. loss:  3.66426921e+00\n",
      "Epoch: 17614 mean train loss:  1.51967979e-05, mean val. loss:  3.66455483e+00\n",
      "Epoch: 17615 mean train loss:  1.52857101e-05, mean val. loss:  3.66481018e+00\n",
      "Epoch: 17616 mean train loss:  1.55517482e-05, mean val. loss:  3.66503358e+00\n",
      "Epoch: 17617 mean train loss:  1.54393201e-05, mean val. loss:  3.66518998e+00\n",
      "Epoch: 17618 mean train loss:  1.52497960e-05, mean val. loss:  3.66534209e+00\n",
      "Epoch: 17619 mean train loss:  1.51249114e-05, mean val. loss:  3.66546893e+00\n",
      "Epoch: 17620 mean train loss:  1.52329158e-05, mean val. loss:  3.66560054e+00\n",
      "Epoch: 17621 mean train loss:  1.50299747e-05, mean val. loss:  3.66575193e+00\n",
      "Epoch: 17622 mean train loss:  1.55170856e-05, mean val. loss:  3.66584826e+00\n",
      "Epoch: 17623 mean train loss:  1.52201392e-05, mean val. loss:  3.66599870e+00\n",
      "Epoch: 17624 mean train loss:  1.51262502e-05, mean val. loss:  3.66616893e+00\n",
      "Epoch: 17625 mean train loss:  1.52951397e-05, mean val. loss:  3.66636324e+00\n",
      "Epoch: 17626 mean train loss:  1.50528795e-05, mean val. loss:  3.66661930e+00\n",
      "Epoch: 17627 mean train loss:  1.53163273e-05, mean val. loss:  3.66689348e+00\n",
      "Epoch: 17628 mean train loss:  1.50362321e-05, mean val. loss:  3.66718340e+00\n",
      "Epoch: 17629 mean train loss:  1.49529369e-05, mean val. loss:  3.66754317e+00\n",
      "Epoch: 17630 mean train loss:  1.52188586e-05, mean val. loss:  3.66788840e+00\n",
      "Epoch: 17631 mean train loss:  1.48816325e-05, mean val. loss:  3.66832519e+00\n",
      "Epoch: 17632 mean train loss:  1.54299778e-05, mean val. loss:  3.66872001e+00\n",
      "Epoch: 17633 mean train loss:  1.52203138e-05, mean val. loss:  3.66910911e+00\n",
      "Epoch: 17634 mean train loss:  1.51623972e-05, mean val. loss:  3.66952467e+00\n",
      "Epoch: 17635 mean train loss:  1.51918211e-05, mean val. loss:  3.66994786e+00\n",
      "Epoch: 17636 mean train loss:  1.51259301e-05, mean val. loss:  3.67038798e+00\n",
      "Epoch: 17637 mean train loss:  1.50287524e-05, mean val. loss:  3.67087173e+00\n",
      "Epoch: 17638 mean train loss:  1.49717089e-05, mean val. loss:  3.67139816e+00\n",
      "Epoch: 17639 mean train loss:  1.50571577e-05, mean val. loss:  3.67194200e+00\n",
      "Epoch: 17640 mean train loss:  1.51332351e-05, mean val. loss:  3.67250586e+00\n",
      "Epoch: 17641 mean train loss:  1.51626009e-05, mean val. loss:  3.67298889e+00\n",
      "Epoch: 17642 mean train loss:  1.51236309e-05, mean val. loss:  3.67347836e+00\n",
      "Epoch: 17643 mean train loss:  1.50030537e-05, mean val. loss:  3.67394423e+00\n",
      "Epoch: 17644 mean train loss:  1.50343985e-05, mean val. loss:  3.67439246e+00\n",
      "Epoch: 17645 mean train loss:  1.51355634e-05, mean val. loss:  3.67479324e+00\n",
      "Epoch: 17646 mean train loss:  1.55225862e-05, mean val. loss:  3.67515659e+00\n",
      "Epoch: 17647 mean train loss:  1.50792475e-05, mean val. loss:  3.67553091e+00\n",
      "Epoch: 17648 mean train loss:  1.50909473e-05, mean val. loss:  3.67588854e+00\n",
      "Epoch: 17649 mean train loss:  1.52844586e-05, mean val. loss:  3.67614841e+00\n",
      "Epoch: 17650 mean train loss:  1.53174624e-05, mean val. loss:  3.67639399e+00\n",
      "Epoch: 17651 mean train loss:  1.48606196e-05, mean val. loss:  3.67667532e+00\n",
      "Epoch: 17652 mean train loss:  1.50922278e-05, mean val. loss:  3.67695737e+00\n",
      "Epoch: 17653 mean train loss:  1.50656560e-05, mean val. loss:  3.67720342e+00\n",
      "Epoch: 17654 mean train loss:  1.50797714e-05, mean val. loss:  3.67743301e+00\n",
      "Epoch: 17655 mean train loss:  1.47594837e-05, mean val. loss:  3.67770076e+00\n",
      "Epoch: 17656 mean train loss:  1.52651046e-05, mean val. loss:  3.67796183e+00\n",
      "Epoch: 17657 mean train loss:  1.50482520e-05, mean val. loss:  3.67823887e+00\n",
      "Epoch: 17658 mean train loss:  1.50254637e-05, mean val. loss:  3.67860007e+00\n",
      "Epoch: 17659 mean train loss:  1.49373955e-05, mean val. loss:  3.67900777e+00\n",
      "Epoch: 17660 mean train loss:  1.50310807e-05, mean val. loss:  3.67939115e+00\n",
      "Epoch: 17661 mean train loss:  1.52156572e-05, mean val. loss:  3.67977285e+00\n",
      "Epoch: 17662 mean train loss:  1.53326837e-05, mean val. loss:  3.68022919e+00\n",
      "Epoch: 17663 mean train loss:  1.52894645e-05, mean val. loss:  3.68064976e+00\n",
      "Epoch: 17664 mean train loss:  1.51161803e-05, mean val. loss:  3.68104959e+00\n",
      "Epoch: 17665 mean train loss:  1.53006404e-05, mean val. loss:  3.68134212e+00\n",
      "Epoch: 17666 mean train loss:  1.51231943e-05, mean val. loss:  3.68164611e+00\n",
      "Epoch: 17667 mean train loss:  1.49653933e-05, mean val. loss:  3.68198061e+00\n",
      "Epoch: 17668 mean train loss:  1.51338172e-05, mean val. loss:  3.68225026e+00\n",
      "Epoch: 17669 mean train loss:  1.51590793e-05, mean val. loss:  3.68253636e+00\n",
      "Epoch: 17670 mean train loss:  1.51817221e-05, mean val. loss:  3.68278933e+00\n",
      "Epoch: 17671 mean train loss:  1.47385581e-05, mean val. loss:  3.68304086e+00\n",
      "Epoch: 17672 mean train loss:  1.51304994e-05, mean val. loss:  3.68325543e+00\n",
      "Epoch: 17673 mean train loss:  1.51705754e-05, mean val. loss:  3.68341708e+00\n",
      "Epoch: 17674 mean train loss:  1.51886197e-05, mean val. loss:  3.68361640e+00\n",
      "Epoch: 17675 mean train loss:  1.49291300e-05, mean val. loss:  3.68389583e+00\n",
      "Epoch: 17676 mean train loss:  1.52477878e-05, mean val. loss:  3.68417120e+00\n",
      "Epoch: 17677 mean train loss:  1.52880093e-05, mean val. loss:  3.68447351e+00\n",
      "Epoch: 17678 mean train loss:  1.52137363e-05, mean val. loss:  3.68475866e+00\n",
      "Epoch: 17679 mean train loss:  1.53329456e-05, mean val. loss:  3.68504119e+00\n",
      "Epoch: 17680 mean train loss:  1.49099214e-05, mean val. loss:  3.68539810e+00\n",
      "Epoch: 17681 mean train loss:  1.52121065e-05, mean val. loss:  3.68574715e+00\n",
      "Epoch: 17682 mean train loss:  1.50220585e-05, mean val. loss:  3.68612099e+00\n",
      "Epoch: 17683 mean train loss:  1.53117871e-05, mean val. loss:  3.68652105e+00\n",
      "Epoch: 17684 mean train loss:  1.50202250e-05, mean val. loss:  3.68693089e+00\n",
      "Epoch: 17685 mean train loss:  1.52690627e-05, mean val. loss:  3.68731761e+00\n",
      "Epoch: 17686 mean train loss:  1.51659769e-05, mean val. loss:  3.68767762e+00\n",
      "Epoch: 17687 mean train loss:  1.50200212e-05, mean val. loss:  3.68802762e+00\n",
      "Epoch: 17688 mean train loss:  1.48130348e-05, mean val. loss:  3.68840289e+00\n",
      "Epoch: 17689 mean train loss:  1.53850124e-05, mean val. loss:  3.68870473e+00\n",
      "Epoch: 17690 mean train loss:  1.51384447e-05, mean val. loss:  3.68901205e+00\n",
      "Epoch: 17691 mean train loss:  1.51961867e-05, mean val. loss:  3.68929100e+00\n",
      "Epoch: 17692 mean train loss:  1.47927494e-05, mean val. loss:  3.68964529e+00\n",
      "Epoch: 17693 mean train loss:  1.50553533e-05, mean val. loss:  3.69003057e+00\n",
      "Epoch: 17694 mean train loss:  1.51578279e-05, mean val. loss:  3.69037747e+00\n",
      "Epoch: 17695 mean train loss:  1.49365223e-05, mean val. loss:  3.69073725e+00\n",
      "Epoch: 17696 mean train loss:  1.50744163e-05, mean val. loss:  3.69106627e+00\n",
      "Epoch: 17697 mean train loss:  1.53426663e-05, mean val. loss:  3.69138384e+00\n",
      "Epoch: 17698 mean train loss:  1.48237450e-05, mean val. loss:  3.69172430e+00\n",
      "Epoch: 17699 mean train loss:  1.50690903e-05, mean val. loss:  3.69208121e+00\n",
      "Epoch: 17700 mean train loss:  1.52712746e-05, mean val. loss:  3.69237494e+00\n",
      "Epoch: 17701 mean train loss:  1.52349530e-05, mean val. loss:  3.69265914e+00\n",
      "Epoch: 17702 mean train loss:  1.52928696e-05, mean val. loss:  3.69293857e+00\n",
      "Epoch: 17703 mean train loss:  1.50882406e-05, mean val. loss:  3.69314742e+00\n",
      "Epoch: 17704 mean train loss:  1.53615838e-05, mean val. loss:  3.69332051e+00\n",
      "Epoch: 17705 mean train loss:  1.49501720e-05, mean val. loss:  3.69350219e+00\n",
      "Epoch: 17706 mean train loss:  1.52566063e-05, mean val. loss:  3.69362640e+00\n",
      "Epoch: 17707 mean train loss:  1.50747364e-05, mean val. loss:  3.69371700e+00\n",
      "Epoch: 17708 mean train loss:  1.51819550e-05, mean val. loss:  3.69382262e+00\n",
      "Epoch: 17709 mean train loss:  1.52549474e-05, mean val. loss:  3.69384694e+00\n",
      "Epoch: 17710 mean train loss:  1.50113774e-05, mean val. loss:  3.69391155e+00\n",
      "Epoch: 17711 mean train loss:  1.51712447e-05, mean val. loss:  3.69399619e+00\n",
      "Epoch: 17712 mean train loss:  1.47778774e-05, mean val. loss:  3.69418144e+00\n",
      "Epoch: 17713 mean train loss:  1.51851273e-05, mean val. loss:  3.69439578e+00\n",
      "Epoch: 17714 mean train loss:  1.51427230e-05, mean val. loss:  3.69461465e+00\n",
      "Epoch: 17715 mean train loss:  1.47566025e-05, mean val. loss:  3.69496989e+00\n",
      "Epoch: 17716 mean train loss:  1.52402499e-05, mean val. loss:  3.69528174e+00\n",
      "Epoch: 17717 mean train loss:  1.52034045e-05, mean val. loss:  3.69565034e+00\n",
      "Epoch: 17718 mean train loss:  1.50881824e-05, mean val. loss:  3.69606376e+00\n",
      "Epoch: 17719 mean train loss:  1.48880936e-05, mean val. loss:  3.69643307e+00\n",
      "Epoch: 17720 mean train loss:  1.52047432e-05, mean val. loss:  3.69674540e+00\n",
      "Epoch: 17721 mean train loss:  1.50323613e-05, mean val. loss:  3.69706869e+00\n",
      "Epoch: 17722 mean train loss:  1.49885018e-05, mean val. loss:  3.69745255e+00\n",
      "Epoch: 17723 mean train loss:  1.49965053e-05, mean val. loss:  3.69781899e+00\n",
      "Epoch: 17724 mean train loss:  1.52149878e-05, mean val. loss:  3.69817686e+00\n",
      "Epoch: 17725 mean train loss:  1.48705149e-05, mean val. loss:  3.69858813e+00\n",
      "Epoch: 17726 mean train loss:  1.54299778e-05, mean val. loss:  3.69893885e+00\n",
      "Epoch: 17727 mean train loss:  1.50502601e-05, mean val. loss:  3.69923449e+00\n",
      "Epoch: 17728 mean train loss:  1.49964471e-05, mean val. loss:  3.69954205e+00\n",
      "Epoch: 17729 mean train loss:  1.49821863e-05, mean val. loss:  3.69980335e+00\n",
      "Epoch: 17730 mean train loss:  1.48136460e-05, mean val. loss:  3.70012331e+00\n",
      "Epoch: 17731 mean train loss:  1.50603009e-05, mean val. loss:  3.70042205e+00\n",
      "Epoch: 17732 mean train loss:  1.50194392e-05, mean val. loss:  3.70073938e+00\n",
      "Epoch: 17733 mean train loss:  1.50142878e-05, mean val. loss:  3.70107031e+00\n",
      "Epoch: 17734 mean train loss:  1.51149579e-05, mean val. loss:  3.70140290e+00\n",
      "Epoch: 17735 mean train loss:  1.49483094e-05, mean val. loss:  3.70174170e+00\n",
      "Epoch: 17736 mean train loss:  1.50243577e-05, mean val. loss:  3.70213223e+00\n",
      "Epoch: 17737 mean train loss:  1.51244749e-05, mean val. loss:  3.70255566e+00\n",
      "Epoch: 17738 mean train loss:  1.48432155e-05, mean val. loss:  3.70308805e+00\n",
      "Epoch: 17739 mean train loss:  1.49817206e-05, mean val. loss:  3.70364046e+00\n",
      "Epoch: 17740 mean train loss:  1.53105066e-05, mean val. loss:  3.70425010e+00\n",
      "Epoch: 17741 mean train loss:  1.48796826e-05, mean val. loss:  3.70487523e+00\n",
      "Epoch: 17742 mean train loss:  1.52699940e-05, mean val. loss:  3.70545197e+00\n",
      "Epoch: 17743 mean train loss:  1.47864048e-05, mean val. loss:  3.70605636e+00\n",
      "Epoch: 17744 mean train loss:  1.53344299e-05, mean val. loss:  3.70660973e+00\n",
      "Epoch: 17745 mean train loss:  1.50870474e-05, mean val. loss:  3.70713282e+00\n",
      "Epoch: 17746 mean train loss:  1.52193534e-05, mean val. loss:  3.70758748e+00\n",
      "Epoch: 17747 mean train loss:  1.51767745e-05, mean val. loss:  3.70795727e+00\n",
      "Epoch: 17748 mean train loss:  1.52154244e-05, mean val. loss:  3.70823979e+00\n",
      "Epoch: 17749 mean train loss:  1.47785759e-05, mean val. loss:  3.70855665e+00\n",
      "Epoch: 17750 mean train loss:  1.51977874e-05, mean val. loss:  3.70880985e+00\n",
      "Epoch: 17751 mean train loss:  1.50483102e-05, mean val. loss:  3.70898700e+00\n",
      "Epoch: 17752 mean train loss:  1.52954017e-05, mean val. loss:  3.70908713e+00\n",
      "Epoch: 17753 mean train loss:  1.51397835e-05, mean val. loss:  3.70909286e+00\n",
      "Epoch: 17754 mean train loss:  1.53389992e-05, mean val. loss:  3.70898485e+00\n",
      "Epoch: 17755 mean train loss:  1.52460416e-05, mean val. loss:  3.70886183e+00\n",
      "Epoch: 17756 mean train loss:  1.47346582e-05, mean val. loss:  3.70880437e+00\n",
      "Epoch: 17757 mean train loss:  1.51440327e-05, mean val. loss:  3.70873761e+00\n",
      "Epoch: 17758 mean train loss:  1.52261928e-05, mean val. loss:  3.70864010e+00\n",
      "Epoch: 17759 mean train loss:  1.48742110e-05, mean val. loss:  3.70860338e+00\n",
      "Epoch: 17760 mean train loss:  1.49800617e-05, mean val. loss:  3.70865273e+00\n",
      "Epoch: 17761 mean train loss:  1.49396365e-05, mean val. loss:  3.70870709e+00\n",
      "Epoch: 17762 mean train loss:  1.50375417e-05, mean val. loss:  3.70877957e+00\n",
      "Epoch: 17763 mean train loss:  1.53530564e-05, mean val. loss:  3.70894361e+00\n",
      "Epoch: 17764 mean train loss:  1.49126572e-05, mean val. loss:  3.70911121e+00\n",
      "Epoch: 17765 mean train loss:  1.53129804e-05, mean val. loss:  3.70928979e+00\n",
      "Epoch: 17766 mean train loss:  1.47226965e-05, mean val. loss:  3.70955968e+00\n",
      "Epoch: 17767 mean train loss:  1.49833213e-05, mean val. loss:  3.70988488e+00\n",
      "Epoch: 17768 mean train loss:  1.52343418e-05, mean val. loss:  3.71020365e+00\n",
      "Epoch: 17769 mean train loss:  1.52547145e-05, mean val. loss:  3.71051598e+00\n",
      "Epoch: 17770 mean train loss:  1.49461848e-05, mean val. loss:  3.71091723e+00\n",
      "Epoch: 17771 mean train loss:  1.48659165e-05, mean val. loss:  3.71137691e+00\n",
      "Epoch: 17772 mean train loss:  1.53046567e-05, mean val. loss:  3.71182036e+00\n",
      "Epoch: 17773 mean train loss:  1.50201959e-05, mean val. loss:  3.71229506e+00\n",
      "Epoch: 17774 mean train loss:  1.50926644e-05, mean val. loss:  3.71271062e+00\n",
      "Epoch: 17775 mean train loss:  1.51191198e-05, mean val. loss:  3.71309853e+00\n",
      "Epoch: 17776 mean train loss:  1.49108528e-05, mean val. loss:  3.71343756e+00\n",
      "Epoch: 17777 mean train loss:  1.50304113e-05, mean val. loss:  3.71384335e+00\n",
      "Epoch: 17778 mean train loss:  1.49024127e-05, mean val. loss:  3.71421695e+00\n",
      "Epoch: 17779 mean train loss:  1.52701687e-05, mean val. loss:  3.71455431e+00\n",
      "Epoch: 17780 mean train loss:  1.49032858e-05, mean val. loss:  3.71485353e+00\n",
      "Epoch: 17781 mean train loss:  1.50584383e-05, mean val. loss:  3.71516657e+00\n",
      "Epoch: 17782 mean train loss:  1.50159467e-05, mean val. loss:  3.71548390e+00\n",
      "Epoch: 17783 mean train loss:  1.50590495e-05, mean val. loss:  3.71580935e+00\n",
      "Epoch: 17784 mean train loss:  1.49793632e-05, mean val. loss:  3.71616030e+00\n",
      "Epoch: 17785 mean train loss:  1.49577099e-05, mean val. loss:  3.71650910e+00\n",
      "Epoch: 17786 mean train loss:  1.52186549e-05, mean val. loss:  3.71682668e+00\n",
      "Epoch: 17787 mean train loss:  1.50917331e-05, mean val. loss:  3.71715975e+00\n",
      "Epoch: 17788 mean train loss:  1.51075947e-05, mean val. loss:  3.71758199e+00\n",
      "Epoch: 17789 mean train loss:  1.52831490e-05, mean val. loss:  3.71803927e+00\n",
      "Epoch: 17790 mean train loss:  1.49668485e-05, mean val. loss:  3.71852899e+00\n",
      "Epoch: 17791 mean train loss:  1.49703119e-05, mean val. loss:  3.71902108e+00\n",
      "Epoch: 17792 mean train loss:  1.52644352e-05, mean val. loss:  3.71943331e+00\n",
      "Epoch: 17793 mean train loss:  1.49052648e-05, mean val. loss:  3.71989584e+00\n",
      "Epoch: 17794 mean train loss:  1.49328262e-05, mean val. loss:  3.72034192e+00\n",
      "Epoch: 17795 mean train loss:  1.49424595e-05, mean val. loss:  3.72080922e+00\n",
      "Epoch: 17796 mean train loss:  1.52748544e-05, mean val. loss:  3.72125793e+00\n",
      "Epoch: 17797 mean train loss:  1.50611741e-05, mean val. loss:  3.72169042e+00\n",
      "Epoch: 17798 mean train loss:  1.49140251e-05, mean val. loss:  3.72215486e+00\n",
      "Epoch: 17799 mean train loss:  1.53622532e-05, mean val. loss:  3.72258329e+00\n",
      "Epoch: 17800 mean train loss:  1.51852146e-05, mean val. loss:  3.72301483e+00\n",
      "Epoch: 17801 mean train loss:  1.53143774e-05, mean val. loss:  3.72337699e+00\n",
      "Epoch: 17802 mean train loss:  1.51300046e-05, mean val. loss:  3.72372031e+00\n",
      "Epoch: 17803 mean train loss:  1.46858802e-05, mean val. loss:  3.72407079e+00\n",
      "Epoch: 17804 mean train loss:  1.51054992e-05, mean val. loss:  3.72441649e+00\n",
      "Epoch: 17805 mean train loss:  1.52183638e-05, mean val. loss:  3.72472882e+00\n",
      "Epoch: 17806 mean train loss:  1.48550025e-05, mean val. loss:  3.72507024e+00\n",
      "Epoch: 17807 mean train loss:  1.53185683e-05, mean val. loss:  3.72536445e+00\n",
      "Epoch: 17808 mean train loss:  1.49992120e-05, mean val. loss:  3.72567415e+00\n",
      "Epoch: 17809 mean train loss:  1.48908002e-05, mean val. loss:  3.72600865e+00\n",
      "Epoch: 17810 mean train loss:  1.49613188e-05, mean val. loss:  3.72633600e+00\n",
      "Epoch: 17811 mean train loss:  1.51026470e-05, mean val. loss:  3.72668028e+00\n",
      "Epoch: 17812 mean train loss:  1.49469997e-05, mean val. loss:  3.72707152e+00\n",
      "Epoch: 17813 mean train loss:  1.50047126e-05, mean val. loss:  3.72745275e+00\n",
      "Epoch: 17814 mean train loss:  1.47538958e-05, mean val. loss:  3.72787642e+00\n",
      "Epoch: 17815 mean train loss:  1.47641695e-05, mean val. loss:  3.72833085e+00\n",
      "Epoch: 17816 mean train loss:  1.48921681e-05, mean val. loss:  3.72879100e+00\n",
      "Epoch: 17817 mean train loss:  1.53237197e-05, mean val. loss:  3.72917628e+00\n",
      "Epoch: 17818 mean train loss:  1.51052664e-05, mean val. loss:  3.72948933e+00\n",
      "Epoch: 17819 mean train loss:  1.53084984e-05, mean val. loss:  3.72978568e+00\n",
      "Epoch: 17820 mean train loss:  1.48891995e-05, mean val. loss:  3.73013186e+00\n",
      "Epoch: 17821 mean train loss:  1.49012485e-05, mean val. loss:  3.73052359e+00\n",
      "Epoch: 17822 mean train loss:  1.50589040e-05, mean val. loss:  3.73085594e+00\n",
      "Epoch: 17823 mean train loss:  1.51065178e-05, mean val. loss:  3.73110104e+00\n",
      "Epoch: 17824 mean train loss:  1.51671120e-05, mean val. loss:  3.73132944e+00\n",
      "Epoch: 17825 mean train loss:  1.48397812e-05, mean val. loss:  3.73156834e+00\n",
      "Epoch: 17826 mean train loss:  1.48154213e-05, mean val. loss:  3.73182297e+00\n",
      "Epoch: 17827 mean train loss:  1.49711559e-05, mean val. loss:  3.73207641e+00\n",
      "Epoch: 17828 mean train loss:  1.51367276e-05, mean val. loss:  3.73238492e+00\n",
      "Epoch: 17829 mean train loss:  1.49296538e-05, mean val. loss:  3.73268533e+00\n",
      "Epoch: 17830 mean train loss:  1.52744469e-05, mean val. loss:  3.73297048e+00\n",
      "Epoch: 17831 mean train loss:  1.48348627e-05, mean val. loss:  3.73328352e+00\n",
      "Epoch: 17832 mean train loss:  1.51625718e-05, mean val. loss:  3.73355222e+00\n",
      "Epoch: 17833 mean train loss:  1.53357105e-05, mean val. loss:  3.73380804e+00\n",
      "Epoch: 17834 mean train loss:  1.52109133e-05, mean val. loss:  3.73396802e+00\n",
      "Epoch: 17835 mean train loss:  1.52049470e-05, mean val. loss:  3.73405004e+00\n",
      "Epoch: 17836 mean train loss:  1.50818960e-05, mean val. loss:  3.73403358e+00\n",
      "Epoch: 17837 mean train loss:  1.49317784e-05, mean val. loss:  3.73398304e+00\n",
      "Epoch: 17838 mean train loss:  1.55488378e-05, mean val. loss:  3.73388791e+00\n",
      "Epoch: 17839 mean train loss:  1.50285487e-05, mean val. loss:  3.73376966e+00\n",
      "Epoch: 17840 mean train loss:  1.48394902e-05, mean val. loss:  3.73367429e+00\n",
      "Epoch: 17841 mean train loss:  1.51443819e-05, mean val. loss:  3.73354650e+00\n",
      "Epoch: 17842 mean train loss:  1.48760737e-05, mean val. loss:  3.73349309e+00\n",
      "Epoch: 17843 mean train loss:  1.47705432e-05, mean val. loss:  3.73348379e+00\n",
      "Epoch: 17844 mean train loss:  1.47481041e-05, mean val. loss:  3.73353338e+00\n",
      "Epoch: 17845 mean train loss:  1.52157445e-05, mean val. loss:  3.73360276e+00\n",
      "Epoch: 17846 mean train loss:  1.50032574e-05, mean val. loss:  3.73373628e+00\n",
      "Epoch: 17847 mean train loss:  1.51148706e-05, mean val. loss:  3.73393250e+00\n",
      "Epoch: 17848 mean train loss:  1.50416454e-05, mean val. loss:  3.73414731e+00\n",
      "Epoch: 17849 mean train loss:  1.51109998e-05, mean val. loss:  3.73443174e+00\n",
      "Epoch: 17850 mean train loss:  1.50208070e-05, mean val. loss:  3.73477840e+00\n",
      "Epoch: 17851 mean train loss:  1.49863772e-05, mean val. loss:  3.73510885e+00\n",
      "Epoch: 17852 mean train loss:  1.51617569e-05, mean val. loss:  3.73546481e+00\n",
      "Epoch: 17853 mean train loss:  1.51852437e-05, mean val. loss:  3.73579907e+00\n",
      "Epoch: 17854 mean train loss:  1.50061387e-05, mean val. loss:  3.73616409e+00\n",
      "Epoch: 17855 mean train loss:  1.51638233e-05, mean val. loss:  3.73658013e+00\n",
      "Epoch: 17856 mean train loss:  1.49584666e-05, mean val. loss:  3.73701382e+00\n",
      "Epoch: 17857 mean train loss:  1.49032567e-05, mean val. loss:  3.73747158e+00\n",
      "Epoch: 17858 mean train loss:  1.47940882e-05, mean val. loss:  3.73793745e+00\n",
      "Epoch: 17859 mean train loss:  1.50668202e-05, mean val. loss:  3.73839331e+00\n",
      "Epoch: 17860 mean train loss:  1.49647531e-05, mean val. loss:  3.73888969e+00\n",
      "Epoch: 17861 mean train loss:  1.50008418e-05, mean val. loss:  3.73936224e+00\n",
      "Epoch: 17862 mean train loss:  1.51417044e-05, mean val. loss:  3.73980618e+00\n",
      "Epoch: 17863 mean train loss:  1.49443513e-05, mean val. loss:  3.74027610e+00\n",
      "Epoch: 17864 mean train loss:  1.48081162e-05, mean val. loss:  3.74074459e+00\n",
      "Epoch: 17865 mean train loss:  1.49944681e-05, mean val. loss:  3.74123907e+00\n",
      "Epoch: 17866 mean train loss:  1.49182160e-05, mean val. loss:  3.74168253e+00\n",
      "Epoch: 17867 mean train loss:  1.50275300e-05, mean val. loss:  3.74213529e+00\n",
      "Epoch: 17868 mean train loss:  1.46973180e-05, mean val. loss:  3.74264383e+00\n",
      "Epoch: 17869 mean train loss:  1.48830877e-05, mean val. loss:  3.74317408e+00\n",
      "Epoch: 17870 mean train loss:  1.52785797e-05, mean val. loss:  3.74367881e+00\n",
      "Epoch: 17871 mean train loss:  1.49983680e-05, mean val. loss:  3.74413443e+00\n",
      "Epoch: 17872 mean train loss:  1.49029947e-05, mean val. loss:  3.74457669e+00\n",
      "Epoch: 17873 mean train loss:  1.49109692e-05, mean val. loss:  3.74497604e+00\n",
      "Epoch: 17874 mean train loss:  1.50136475e-05, mean val. loss:  3.74533892e+00\n",
      "Epoch: 17875 mean train loss:  1.50192645e-05, mean val. loss:  3.74566627e+00\n",
      "Epoch: 17876 mean train loss:  1.50027336e-05, mean val. loss:  3.74595308e+00\n",
      "Epoch: 17877 mean train loss:  1.48908584e-05, mean val. loss:  3.74624395e+00\n",
      "Epoch: 17878 mean train loss:  1.49921398e-05, mean val. loss:  3.74650598e+00\n",
      "Epoch: 17879 mean train loss:  1.50448177e-05, mean val. loss:  3.74679732e+00\n",
      "Epoch: 17880 mean train loss:  1.50416745e-05, mean val. loss:  3.74709988e+00\n",
      "Epoch: 17881 mean train loss:  1.51971763e-05, mean val. loss:  3.74732137e+00\n",
      "Epoch: 17882 mean train loss:  1.50880660e-05, mean val. loss:  3.74744463e+00\n",
      "Epoch: 17883 mean train loss:  1.51484273e-05, mean val. loss:  3.74750447e+00\n",
      "Epoch: 17884 mean train loss:  1.51707209e-05, mean val. loss:  3.74752259e+00\n",
      "Epoch: 17885 mean train loss:  1.51550048e-05, mean val. loss:  3.74754500e+00\n",
      "Epoch: 17886 mean train loss:  1.46929815e-05, mean val. loss:  3.74757576e+00\n",
      "Epoch: 17887 mean train loss:  1.49894622e-05, mean val. loss:  3.74762416e+00\n",
      "Epoch: 17888 mean train loss:  1.48343388e-05, mean val. loss:  3.74768305e+00\n",
      "Epoch: 17889 mean train loss:  1.51765416e-05, mean val. loss:  3.74771237e+00\n",
      "Epoch: 17890 mean train loss:  1.50298001e-05, mean val. loss:  3.74775505e+00\n",
      "Epoch: 17891 mean train loss:  1.52461871e-05, mean val. loss:  3.74780202e+00\n",
      "Epoch: 17892 mean train loss:  1.47988612e-05, mean val. loss:  3.74786115e+00\n",
      "Epoch: 17893 mean train loss:  1.48006657e-05, mean val. loss:  3.74800158e+00\n",
      "Epoch: 17894 mean train loss:  1.49147527e-05, mean val. loss:  3.74824238e+00\n",
      "Epoch: 17895 mean train loss:  1.50931301e-05, mean val. loss:  3.74849916e+00\n",
      "Epoch: 17896 mean train loss:  1.49997650e-05, mean val. loss:  3.74878263e+00\n",
      "Epoch: 17897 mean train loss:  1.49642583e-05, mean val. loss:  3.74914598e+00\n",
      "Epoch: 17898 mean train loss:  1.50501728e-05, mean val. loss:  3.74952149e+00\n",
      "Epoch: 17899 mean train loss:  1.46429520e-05, mean val. loss:  3.75000691e+00\n",
      "Epoch: 17900 mean train loss:  1.51834392e-05, mean val. loss:  3.75047541e+00\n",
      "Epoch: 17901 mean train loss:  1.49968546e-05, mean val. loss:  3.75098062e+00\n",
      "Epoch: 17902 mean train loss:  1.49511034e-05, mean val. loss:  3.75153565e+00\n",
      "Epoch: 17903 mean train loss:  1.48616382e-05, mean val. loss:  3.75214243e+00\n",
      "Epoch: 17904 mean train loss:  1.50846317e-05, mean val. loss:  3.75275135e+00\n",
      "Epoch: 17905 mean train loss:  1.47512474e-05, mean val. loss:  3.75341105e+00\n",
      "Epoch: 17906 mean train loss:  1.49807020e-05, mean val. loss:  3.75402284e+00\n",
      "Epoch: 17907 mean train loss:  1.50286360e-05, mean val. loss:  3.75457454e+00\n",
      "Epoch: 17908 mean train loss:  1.47881510e-05, mean val. loss:  3.75513506e+00\n",
      "Epoch: 17909 mean train loss:  1.49331172e-05, mean val. loss:  3.75566626e+00\n",
      "Epoch: 17910 mean train loss:  1.50307023e-05, mean val. loss:  3.75615931e+00\n",
      "Epoch: 17911 mean train loss:  1.52003777e-05, mean val. loss:  3.75656319e+00\n",
      "Epoch: 17912 mean train loss:  1.48734543e-05, mean val. loss:  3.75690603e+00\n",
      "Epoch: 17913 mean train loss:  1.51137065e-05, mean val. loss:  3.75715280e+00\n",
      "Epoch: 17914 mean train loss:  1.51922286e-05, mean val. loss:  3.75730324e+00\n",
      "Epoch: 17915 mean train loss:  1.48531690e-05, mean val. loss:  3.75751686e+00\n",
      "Epoch: 17916 mean train loss:  1.47033134e-05, mean val. loss:  3.75775027e+00\n",
      "Epoch: 17917 mean train loss:  1.50757260e-05, mean val. loss:  3.75792694e+00\n",
      "Epoch: 17918 mean train loss:  1.49475527e-05, mean val. loss:  3.75807643e+00\n",
      "Epoch: 17919 mean train loss:  1.49341940e-05, mean val. loss:  3.75819445e+00\n",
      "Epoch: 17920 mean train loss:  1.49188854e-05, mean val. loss:  3.75832081e+00\n",
      "Epoch: 17921 mean train loss:  1.50146079e-05, mean val. loss:  3.75846219e+00\n",
      "Epoch: 17922 mean train loss:  1.51827408e-05, mean val. loss:  3.75858760e+00\n",
      "Epoch: 17923 mean train loss:  1.46788661e-05, mean val. loss:  3.75877333e+00\n",
      "Epoch: 17924 mean train loss:  1.50233100e-05, mean val. loss:  3.75892687e+00\n",
      "Epoch: 17925 mean train loss:  1.47746759e-05, mean val. loss:  3.75911760e+00\n",
      "Epoch: 17926 mean train loss:  1.49420230e-05, mean val. loss:  3.75930595e+00\n",
      "Epoch: 17927 mean train loss:  1.50733686e-05, mean val. loss:  3.75951409e+00\n",
      "Epoch: 17928 mean train loss:  1.49630068e-05, mean val. loss:  3.75975251e+00\n",
      "Epoch: 17929 mean train loss:  1.48730760e-05, mean val. loss:  3.75999093e+00\n",
      "Epoch: 17930 mean train loss:  1.50876585e-05, mean val. loss:  3.76023054e+00\n",
      "Epoch: 17931 mean train loss:  1.50356791e-05, mean val. loss:  3.76047802e+00\n",
      "Epoch: 17932 mean train loss:  1.48472900e-05, mean val. loss:  3.76071143e+00\n",
      "Epoch: 17933 mean train loss:  1.48824183e-05, mean val. loss:  3.76098251e+00\n",
      "Epoch: 17934 mean train loss:  1.51926070e-05, mean val. loss:  3.76117015e+00\n",
      "Epoch: 17935 mean train loss:  1.49084372e-05, mean val. loss:  3.76138926e+00\n",
      "Epoch: 17936 mean train loss:  1.50327105e-05, mean val. loss:  3.76161504e+00\n",
      "Epoch: 17937 mean train loss:  1.50463893e-05, mean val. loss:  3.76179361e+00\n",
      "Epoch: 17938 mean train loss:  1.49060797e-05, mean val. loss:  3.76202679e+00\n",
      "Epoch: 17939 mean train loss:  1.50084670e-05, mean val. loss:  3.76227212e+00\n",
      "Epoch: 17940 mean train loss:  1.52003195e-05, mean val. loss:  3.76254129e+00\n",
      "Epoch: 17941 mean train loss:  1.49761327e-05, mean val. loss:  3.76279783e+00\n",
      "Epoch: 17942 mean train loss:  1.50899577e-05, mean val. loss:  3.76308012e+00\n",
      "Epoch: 17943 mean train loss:  1.46749953e-05, mean val. loss:  3.76340652e+00\n",
      "Epoch: 17944 mean train loss:  1.48671679e-05, mean val. loss:  3.76376987e+00\n",
      "Epoch: 17945 mean train loss:  1.50269479e-05, mean val. loss:  3.76412868e+00\n",
      "Epoch: 17946 mean train loss:  1.50545675e-05, mean val. loss:  3.76449609e+00\n",
      "Epoch: 17947 mean train loss:  1.49857369e-05, mean val. loss:  3.76486564e+00\n",
      "Epoch: 17948 mean train loss:  1.50560809e-05, mean val. loss:  3.76526117e+00\n",
      "Epoch: 17949 mean train loss:  1.48849795e-05, mean val. loss:  3.76563764e+00\n",
      "Epoch: 17950 mean train loss:  1.49620464e-05, mean val. loss:  3.76596165e+00\n",
      "Epoch: 17951 mean train loss:  1.51139393e-05, mean val. loss:  3.76631355e+00\n",
      "Epoch: 17952 mean train loss:  1.50404812e-05, mean val. loss:  3.76665545e+00\n",
      "Epoch: 17953 mean train loss:  1.48899562e-05, mean val. loss:  3.76701236e+00\n",
      "Epoch: 17954 mean train loss:  1.48985418e-05, mean val. loss:  3.76737666e+00\n",
      "Epoch: 17955 mean train loss:  1.51837303e-05, mean val. loss:  3.76772022e+00\n",
      "Epoch: 17956 mean train loss:  1.51006097e-05, mean val. loss:  3.76798844e+00\n",
      "Epoch: 17957 mean train loss:  1.51033746e-05, mean val. loss:  3.76825166e+00\n",
      "Epoch: 17958 mean train loss:  1.48829713e-05, mean val. loss:  3.76851344e+00\n",
      "Epoch: 17959 mean train loss:  1.48207473e-05, mean val. loss:  3.76874280e+00\n",
      "Epoch: 17960 mean train loss:  1.48860563e-05, mean val. loss:  3.76891398e+00\n",
      "Epoch: 17961 mean train loss:  1.50558772e-05, mean val. loss:  3.76905513e+00\n",
      "Epoch: 17962 mean train loss:  1.50223204e-05, mean val. loss:  3.76921177e+00\n",
      "Epoch: 17963 mean train loss:  1.48978725e-05, mean val. loss:  3.76938486e+00\n",
      "Epoch: 17964 mean train loss:  1.48717372e-05, mean val. loss:  3.76956797e+00\n",
      "Epoch: 17965 mean train loss:  1.47965620e-05, mean val. loss:  3.76979423e+00\n",
      "Epoch: 17966 mean train loss:  1.52059947e-05, mean val. loss:  3.77002478e+00\n",
      "Epoch: 17967 mean train loss:  1.50331180e-05, mean val. loss:  3.77031970e+00\n",
      "Epoch: 17968 mean train loss:  1.52375724e-05, mean val. loss:  3.77064300e+00\n",
      "Epoch: 17969 mean train loss:  1.51468848e-05, mean val. loss:  3.77099895e+00\n",
      "Epoch: 17970 mean train loss:  1.46920502e-05, mean val. loss:  3.77133560e+00\n",
      "Epoch: 17971 mean train loss:  1.49628031e-05, mean val. loss:  3.77166605e+00\n",
      "Epoch: 17972 mean train loss:  1.51402201e-05, mean val. loss:  3.77197790e+00\n",
      "Epoch: 17973 mean train loss:  1.44667283e-05, mean val. loss:  3.77239704e+00\n",
      "Epoch: 17974 mean train loss:  1.48823019e-05, mean val. loss:  3.77286267e+00\n",
      "Epoch: 17975 mean train loss:  1.49424886e-05, mean val. loss:  3.77334166e+00\n",
      "Epoch: 17976 mean train loss:  1.51901040e-05, mean val. loss:  3.77382851e+00\n",
      "Epoch: 17977 mean train loss:  1.49321859e-05, mean val. loss:  3.77425647e+00\n",
      "Epoch: 17978 mean train loss:  1.50970009e-05, mean val. loss:  3.77463841e+00\n",
      "Epoch: 17979 mean train loss:  1.51105633e-05, mean val. loss:  3.77501082e+00\n",
      "Epoch: 17980 mean train loss:  1.48286636e-05, mean val. loss:  3.77535748e+00\n",
      "Epoch: 17981 mean train loss:  1.49740081e-05, mean val. loss:  3.77569556e+00\n",
      "Epoch: 17982 mean train loss:  1.49343105e-05, mean val. loss:  3.77598763e+00\n",
      "Epoch: 17983 mean train loss:  1.51339918e-05, mean val. loss:  3.77621126e+00\n",
      "Epoch: 17984 mean train loss:  1.48423715e-05, mean val. loss:  3.77647495e+00\n",
      "Epoch: 17985 mean train loss:  1.53066940e-05, mean val. loss:  3.77669215e+00\n",
      "Epoch: 17986 mean train loss:  1.48229592e-05, mean val. loss:  3.77696872e+00\n",
      "Epoch: 17987 mean train loss:  1.49561674e-05, mean val. loss:  3.77724433e+00\n",
      "Epoch: 17988 mean train loss:  1.48547406e-05, mean val. loss:  3.77751207e+00\n",
      "Epoch: 17989 mean train loss:  1.49945263e-05, mean val. loss:  3.77783847e+00\n",
      "Epoch: 17990 mean train loss:  1.49517437e-05, mean val. loss:  3.77818942e+00\n",
      "Epoch: 17991 mean train loss:  1.48652762e-05, mean val. loss:  3.77857041e+00\n",
      "Epoch: 17992 mean train loss:  1.48769468e-05, mean val. loss:  3.77904129e+00\n",
      "Epoch: 17993 mean train loss:  1.49136758e-05, mean val. loss:  3.77953410e+00\n",
      "Epoch: 17994 mean train loss:  1.50277629e-05, mean val. loss:  3.78001380e+00\n",
      "Epoch: 17995 mean train loss:  1.48679246e-05, mean val. loss:  3.78051138e+00\n",
      "Epoch: 17996 mean train loss:  1.47938845e-05, mean val. loss:  3.78100991e+00\n",
      "Epoch: 17997 mean train loss:  1.49384432e-05, mean val. loss:  3.78147864e+00\n",
      "Epoch: 17998 mean train loss:  1.45609956e-05, mean val. loss:  3.78204703e+00\n",
      "Epoch: 17999 mean train loss:  1.49752595e-05, mean val. loss:  3.78259683e+00\n",
      "Epoch: 18000 mean train loss:  1.48661784e-05, mean val. loss:  3.78313494e+00\n",
      "Epoch: 18001 mean train loss:  1.50953711e-05, mean val. loss:  3.78358531e+00\n",
      "Epoch: 18002 mean train loss:  1.47567189e-05, mean val. loss:  3.78398466e+00\n",
      "Epoch: 18003 mean train loss:  1.47999963e-05, mean val. loss:  3.78434157e+00\n",
      "Epoch: 18004 mean train loss:  1.47140818e-05, mean val. loss:  3.78467512e+00\n",
      "Epoch: 18005 mean train loss:  1.51335553e-05, mean val. loss:  3.78489780e+00\n",
      "Epoch: 18006 mean train loss:  1.48715335e-05, mean val. loss:  3.78510499e+00\n",
      "Epoch: 18007 mean train loss:  1.49220868e-05, mean val. loss:  3.78528976e+00\n",
      "Epoch: 18008 mean train loss:  1.50105625e-05, mean val. loss:  3.78544307e+00\n",
      "Epoch: 18009 mean train loss:  1.49929838e-05, mean val. loss:  3.78563595e+00\n",
      "Epoch: 18010 mean train loss:  1.51824788e-05, mean val. loss:  3.78574729e+00\n",
      "Epoch: 18011 mean train loss:  1.50085543e-05, mean val. loss:  3.78584719e+00\n",
      "Epoch: 18012 mean train loss:  1.49311963e-05, mean val. loss:  3.78594279e+00\n",
      "Epoch: 18013 mean train loss:  1.50218548e-05, mean val. loss:  3.78600264e+00\n",
      "Epoch: 18014 mean train loss:  1.49389962e-05, mean val. loss:  3.78607845e+00\n",
      "Epoch: 18015 mean train loss:  1.50827109e-05, mean val. loss:  3.78619051e+00\n",
      "Epoch: 18016 mean train loss:  1.48440886e-05, mean val. loss:  3.78631735e+00\n",
      "Epoch: 18017 mean train loss:  1.47093087e-05, mean val. loss:  3.78653049e+00\n",
      "Epoch: 18018 mean train loss:  1.49962434e-05, mean val. loss:  3.78672314e+00\n",
      "Epoch: 18019 mean train loss:  1.48024410e-05, mean val. loss:  3.78688145e+00\n",
      "Epoch: 18020 mean train loss:  1.47299434e-05, mean val. loss:  3.78707528e+00\n",
      "Epoch: 18021 mean train loss:  1.51928689e-05, mean val. loss:  3.78722143e+00\n",
      "Epoch: 18022 mean train loss:  1.50870765e-05, mean val. loss:  3.78734922e+00\n",
      "Epoch: 18023 mean train loss:  1.52866996e-05, mean val. loss:  3.78741193e+00\n",
      "Epoch: 18024 mean train loss:  1.48021500e-05, mean val. loss:  3.78749299e+00\n",
      "Epoch: 18025 mean train loss:  1.49723783e-05, mean val. loss:  3.78759122e+00\n",
      "Epoch: 18026 mean train loss:  1.46236271e-05, mean val. loss:  3.78779840e+00\n",
      "Epoch: 18027 mean train loss:  1.48130348e-05, mean val. loss:  3.78800869e+00\n",
      "Epoch: 18028 mean train loss:  1.48547697e-05, mean val. loss:  3.78825307e+00\n",
      "Epoch: 18029 mean train loss:  1.50357082e-05, mean val. loss:  3.78841996e+00\n",
      "Epoch: 18030 mean train loss:  1.51405693e-05, mean val. loss:  3.78856134e+00\n",
      "Epoch: 18031 mean train loss:  1.49887928e-05, mean val. loss:  3.78873086e+00\n",
      "Epoch: 18032 mean train loss:  1.51589920e-05, mean val. loss:  3.78888845e+00\n",
      "Epoch: 18033 mean train loss:  1.52342545e-05, mean val. loss:  3.78903580e+00\n",
      "Epoch: 18034 mean train loss:  1.47263636e-05, mean val. loss:  3.78921986e+00\n",
      "Epoch: 18035 mean train loss:  1.51496497e-05, mean val. loss:  3.78936839e+00\n",
      "Epoch: 18036 mean train loss:  1.50003470e-05, mean val. loss:  3.78951001e+00\n",
      "Epoch: 18037 mean train loss:  1.48605905e-05, mean val. loss:  3.78971243e+00\n",
      "Epoch: 18038 mean train loss:  1.49525004e-05, mean val. loss:  3.78990483e+00\n",
      "Epoch: 18039 mean train loss:  1.47949613e-05, mean val. loss:  3.79012823e+00\n",
      "Epoch: 18040 mean train loss:  1.48806139e-05, mean val. loss:  3.79035568e+00\n",
      "Epoch: 18041 mean train loss:  1.52729917e-05, mean val. loss:  3.79055309e+00\n",
      "Epoch: 18042 mean train loss:  1.48644904e-05, mean val. loss:  3.79077816e+00\n",
      "Epoch: 18043 mean train loss:  1.48265972e-05, mean val. loss:  3.79102230e+00\n",
      "Epoch: 18044 mean train loss:  1.48247636e-05, mean val. loss:  3.79128432e+00\n",
      "Epoch: 18045 mean train loss:  1.47851242e-05, mean val. loss:  3.79155731e+00\n",
      "Epoch: 18046 mean train loss:  1.52257271e-05, mean val. loss:  3.79178190e+00\n",
      "Epoch: 18047 mean train loss:  1.48207764e-05, mean val. loss:  3.79209733e+00\n",
      "Epoch: 18048 mean train loss:  1.48161489e-05, mean val. loss:  3.79242063e+00\n",
      "Epoch: 18049 mean train loss:  1.50783453e-05, mean val. loss:  3.79277015e+00\n",
      "Epoch: 18050 mean train loss:  1.51798304e-05, mean val. loss:  3.79310274e+00\n",
      "Epoch: 18051 mean train loss:  1.51233107e-05, mean val. loss:  3.79338694e+00\n",
      "Epoch: 18052 mean train loss:  1.48457766e-05, mean val. loss:  3.79371715e+00\n",
      "Epoch: 18053 mean train loss:  1.47192623e-05, mean val. loss:  3.79410028e+00\n",
      "Epoch: 18054 mean train loss:  1.49131811e-05, mean val. loss:  3.79446411e+00\n",
      "Epoch: 18055 mean train loss:  1.50496489e-05, mean val. loss:  3.79481769e+00\n",
      "Epoch: 18056 mean train loss:  1.47492683e-05, mean val. loss:  3.79521918e+00\n",
      "Epoch: 18057 mean train loss:  1.49148400e-05, mean val. loss:  3.79562545e+00\n",
      "Epoch: 18058 mean train loss:  1.45924569e-05, mean val. loss:  3.79605103e+00\n",
      "Epoch: 18059 mean train loss:  1.50150154e-05, mean val. loss:  3.79640985e+00\n",
      "Epoch: 18060 mean train loss:  1.48933905e-05, mean val. loss:  3.79676104e+00\n",
      "Epoch: 18061 mean train loss:  1.50192354e-05, mean val. loss:  3.79709005e+00\n",
      "Epoch: 18062 mean train loss:  1.45979575e-05, mean val. loss:  3.79750490e+00\n",
      "Epoch: 18063 mean train loss:  1.49517437e-05, mean val. loss:  3.79791474e+00\n",
      "Epoch: 18064 mean train loss:  1.48405670e-05, mean val. loss:  3.79829860e+00\n",
      "Epoch: 18065 mean train loss:  1.46653620e-05, mean val. loss:  3.79873490e+00\n",
      "Epoch: 18066 mean train loss:  1.48221152e-05, mean val. loss:  3.79916191e+00\n",
      "Epoch: 18067 mean train loss:  1.48173131e-05, mean val. loss:  3.79959202e+00\n",
      "Epoch: 18068 mean train loss:  1.49531697e-05, mean val. loss:  3.80005956e+00\n",
      "Epoch: 18069 mean train loss:  1.52093126e-05, mean val. loss:  3.80043697e+00\n",
      "Epoch: 18070 mean train loss:  1.47761020e-05, mean val. loss:  3.80083513e+00\n",
      "Epoch: 18071 mean train loss:  1.50482520e-05, mean val. loss:  3.80122662e+00\n",
      "Epoch: 18072 mean train loss:  1.49914413e-05, mean val. loss:  3.80162692e+00\n",
      "Epoch: 18073 mean train loss:  1.48683903e-05, mean val. loss:  3.80203938e+00\n",
      "Epoch: 18074 mean train loss:  1.49551779e-05, mean val. loss:  3.80244255e+00\n",
      "Epoch: 18075 mean train loss:  1.49154803e-05, mean val. loss:  3.80287433e+00\n",
      "Epoch: 18076 mean train loss:  1.51009299e-05, mean val. loss:  3.80326033e+00\n",
      "Epoch: 18077 mean train loss:  1.47631217e-05, mean val. loss:  3.80362558e+00\n",
      "Epoch: 18078 mean train loss:  1.50438282e-05, mean val. loss:  3.80397797e+00\n",
      "Epoch: 18079 mean train loss:  1.51415297e-05, mean val. loss:  3.80423856e+00\n",
      "Epoch: 18080 mean train loss:  1.49190600e-05, mean val. loss:  3.80442500e+00\n",
      "Epoch: 18081 mean train loss:  1.49335247e-05, mean val. loss:  3.80459476e+00\n",
      "Epoch: 18082 mean train loss:  1.49061088e-05, mean val. loss:  3.80474257e+00\n",
      "Epoch: 18083 mean train loss:  1.48499385e-05, mean val. loss:  3.80485201e+00\n",
      "Epoch: 18084 mean train loss:  1.51374843e-05, mean val. loss:  3.80493760e+00\n",
      "Epoch: 18085 mean train loss:  1.50630949e-05, mean val. loss:  3.80502486e+00\n",
      "Epoch: 18086 mean train loss:  1.49045081e-05, mean val. loss:  3.80502653e+00\n",
      "Epoch: 18087 mean train loss:  1.49724656e-05, mean val. loss:  3.80505371e+00\n",
      "Epoch: 18088 mean train loss:  1.48890249e-05, mean val. loss:  3.80510545e+00\n",
      "Epoch: 18089 mean train loss:  1.47271203e-05, mean val. loss:  3.80521560e+00\n",
      "Epoch: 18090 mean train loss:  1.50502019e-05, mean val. loss:  3.80526805e+00\n",
      "Epoch: 18091 mean train loss:  1.48939143e-05, mean val. loss:  3.80537033e+00\n",
      "Epoch: 18092 mean train loss:  1.47672836e-05, mean val. loss:  3.80549502e+00\n",
      "Epoch: 18093 mean train loss:  1.47991232e-05, mean val. loss:  3.80564785e+00\n",
      "Epoch: 18094 mean train loss:  1.48400140e-05, mean val. loss:  3.80578065e+00\n",
      "Epoch: 18095 mean train loss:  1.46875682e-05, mean val. loss:  3.80604315e+00\n",
      "Epoch: 18096 mean train loss:  1.48676045e-05, mean val. loss:  3.80633163e+00\n",
      "Epoch: 18097 mean train loss:  1.49745902e-05, mean val. loss:  3.80660272e+00\n",
      "Epoch: 18098 mean train loss:  1.48205727e-05, mean val. loss:  3.80690002e+00\n",
      "Epoch: 18099 mean train loss:  1.49992702e-05, mean val. loss:  3.80727053e+00\n",
      "Epoch: 18100 mean train loss:  1.49836997e-05, mean val. loss:  3.80767179e+00\n",
      "Epoch: 18101 mean train loss:  1.49396947e-05, mean val. loss:  3.80809283e+00\n",
      "Epoch: 18102 mean train loss:  1.50190608e-05, mean val. loss:  3.80845785e+00\n",
      "Epoch: 18103 mean train loss:  1.48727559e-05, mean val. loss:  3.80883861e+00\n",
      "Epoch: 18104 mean train loss:  1.47285173e-05, mean val. loss:  3.80926681e+00\n",
      "Epoch: 18105 mean train loss:  1.48944382e-05, mean val. loss:  3.80966282e+00\n",
      "Epoch: 18106 mean train loss:  1.49611733e-05, mean val. loss:  3.81002569e+00\n",
      "Epoch: 18107 mean train loss:  1.46893144e-05, mean val. loss:  3.81039357e+00\n",
      "Epoch: 18108 mean train loss:  1.51514541e-05, mean val. loss:  3.81073427e+00\n",
      "Epoch: 18109 mean train loss:  1.51081185e-05, mean val. loss:  3.81101894e+00\n",
      "Epoch: 18110 mean train loss:  1.47947867e-05, mean val. loss:  3.81130338e+00\n",
      "Epoch: 18111 mean train loss:  1.49070402e-05, mean val. loss:  3.81156278e+00\n",
      "Epoch: 18112 mean train loss:  1.48427789e-05, mean val. loss:  3.81178689e+00\n",
      "Epoch: 18113 mean train loss:  1.51210115e-05, mean val. loss:  3.81200314e+00\n",
      "Epoch: 18114 mean train loss:  1.49654807e-05, mean val. loss:  3.81218767e+00\n",
      "Epoch: 18115 mean train loss:  1.51447020e-05, mean val. loss:  3.81239939e+00\n",
      "Epoch: 18116 mean train loss:  1.49013067e-05, mean val. loss:  3.81256604e+00\n",
      "Epoch: 18117 mean train loss:  1.47288374e-05, mean val. loss:  3.81284976e+00\n",
      "Epoch: 18118 mean train loss:  1.49534899e-05, mean val. loss:  3.81314230e+00\n",
      "Epoch: 18119 mean train loss:  1.48548861e-05, mean val. loss:  3.81345510e+00\n",
      "Epoch: 18120 mean train loss:  1.49401894e-05, mean val. loss:  3.81379128e+00\n",
      "Epoch: 18121 mean train loss:  1.47707760e-05, mean val. loss:  3.81416893e+00\n",
      "Epoch: 18122 mean train loss:  1.51703425e-05, mean val. loss:  3.81454062e+00\n",
      "Epoch: 18123 mean train loss:  1.50317501e-05, mean val. loss:  3.81486058e+00\n",
      "Epoch: 18124 mean train loss:  1.45886443e-05, mean val. loss:  3.81524873e+00\n",
      "Epoch: 18125 mean train loss:  1.45835802e-05, mean val. loss:  3.81568575e+00\n",
      "Epoch: 18126 mean train loss:  1.49151310e-05, mean val. loss:  3.81608939e+00\n",
      "Epoch: 18127 mean train loss:  1.46842212e-05, mean val. loss:  3.81654668e+00\n",
      "Epoch: 18128 mean train loss:  1.48091931e-05, mean val. loss:  3.81698203e+00\n",
      "Epoch: 18129 mean train loss:  1.51430431e-05, mean val. loss:  3.81733537e+00\n",
      "Epoch: 18130 mean train loss:  1.50928972e-05, mean val. loss:  3.81767559e+00\n",
      "Epoch: 18131 mean train loss:  1.48854160e-05, mean val. loss:  3.81800508e+00\n",
      "Epoch: 18132 mean train loss:  1.47537503e-05, mean val. loss:  3.81830144e+00\n",
      "Epoch: 18133 mean train loss:  1.45238591e-05, mean val. loss:  3.81869030e+00\n",
      "Epoch: 18134 mean train loss:  1.51699642e-05, mean val. loss:  3.81907845e+00\n",
      "Epoch: 18135 mean train loss:  1.50125124e-05, mean val. loss:  3.81941867e+00\n",
      "Epoch: 18136 mean train loss:  1.51507556e-05, mean val. loss:  3.81970191e+00\n",
      "Epoch: 18137 mean train loss:  1.48126855e-05, mean val. loss:  3.81998444e+00\n",
      "Epoch: 18138 mean train loss:  1.51210988e-05, mean val. loss:  3.82017493e+00\n",
      "Epoch: 18139 mean train loss:  1.49008993e-05, mean val. loss:  3.82036352e+00\n",
      "Epoch: 18140 mean train loss:  1.50268024e-05, mean val. loss:  3.82050991e+00\n",
      "Epoch: 18141 mean train loss:  1.46353268e-05, mean val. loss:  3.82074714e+00\n",
      "Epoch: 18142 mean train loss:  1.48222607e-05, mean val. loss:  3.82099366e+00\n",
      "Epoch: 18143 mean train loss:  1.47458923e-05, mean val. loss:  3.82126975e+00\n",
      "Epoch: 18144 mean train loss:  1.50140550e-05, mean val. loss:  3.82153630e+00\n",
      "Epoch: 18145 mean train loss:  1.49107946e-05, mean val. loss:  3.82178807e+00\n",
      "Epoch: 18146 mean train loss:  1.46985694e-05, mean val. loss:  3.82205510e+00\n",
      "Epoch: 18147 mean train loss:  1.47790124e-05, mean val. loss:  3.82236147e+00\n",
      "Epoch: 18148 mean train loss:  1.50063715e-05, mean val. loss:  3.82265973e+00\n",
      "Epoch: 18149 mean train loss:  1.50666456e-05, mean val. loss:  3.82285309e+00\n",
      "Epoch: 18150 mean train loss:  1.50723790e-05, mean val. loss:  3.82302356e+00\n",
      "Epoch: 18151 mean train loss:  1.50209235e-05, mean val. loss:  3.82316852e+00\n",
      "Epoch: 18152 mean train loss:  1.50445849e-05, mean val. loss:  3.82327461e+00\n",
      "Epoch: 18153 mean train loss:  1.49041298e-05, mean val. loss:  3.82339954e+00\n",
      "Epoch: 18154 mean train loss:  1.47314568e-05, mean val. loss:  3.82360554e+00\n",
      "Epoch: 18155 mean train loss:  1.48004619e-05, mean val. loss:  3.82388687e+00\n",
      "Epoch: 18156 mean train loss:  1.49924308e-05, mean val. loss:  3.82418847e+00\n",
      "Epoch: 18157 mean train loss:  1.48339022e-05, mean val. loss:  3.82451415e+00\n",
      "Epoch: 18158 mean train loss:  1.50266278e-05, mean val. loss:  3.82481790e+00\n",
      "Epoch: 18159 mean train loss:  1.49994157e-05, mean val. loss:  3.82511902e+00\n",
      "Epoch: 18160 mean train loss:  1.50252599e-05, mean val. loss:  3.82542610e+00\n",
      "Epoch: 18161 mean train loss:  1.49641419e-05, mean val. loss:  3.82574677e+00\n",
      "Epoch: 18162 mean train loss:  1.49553816e-05, mean val. loss:  3.82603502e+00\n",
      "Epoch: 18163 mean train loss:  1.48212712e-05, mean val. loss:  3.82637358e+00\n",
      "Epoch: 18164 mean train loss:  1.47667306e-05, mean val. loss:  3.82672811e+00\n",
      "Epoch: 18165 mean train loss:  1.48358813e-05, mean val. loss:  3.82712388e+00\n",
      "Epoch: 18166 mean train loss:  1.48947875e-05, mean val. loss:  3.82755947e+00\n",
      "Epoch: 18167 mean train loss:  1.49049447e-05, mean val. loss:  3.82801890e+00\n",
      "Epoch: 18168 mean train loss:  1.48207764e-05, mean val. loss:  3.82848859e+00\n",
      "Epoch: 18169 mean train loss:  1.46279926e-05, mean val. loss:  3.82902527e+00\n",
      "Epoch: 18170 mean train loss:  1.47169631e-05, mean val. loss:  3.82959843e+00\n",
      "Epoch: 18171 mean train loss:  1.49230182e-05, mean val. loss:  3.83017445e+00\n",
      "Epoch: 18172 mean train loss:  1.47837563e-05, mean val. loss:  3.83072162e+00\n",
      "Epoch: 18173 mean train loss:  1.46185048e-05, mean val. loss:  3.83126664e+00\n",
      "Epoch: 18174 mean train loss:  1.49362604e-05, mean val. loss:  3.83174777e+00\n",
      "Epoch: 18175 mean train loss:  1.48660911e-05, mean val. loss:  3.83222795e+00\n",
      "Epoch: 18176 mean train loss:  1.48986583e-05, mean val. loss:  3.83270073e+00\n",
      "Epoch: 18177 mean train loss:  1.49186235e-05, mean val. loss:  3.83314800e+00\n",
      "Epoch: 18178 mean train loss:  1.47136743e-05, mean val. loss:  3.83360291e+00\n",
      "Epoch: 18179 mean train loss:  1.48121326e-05, mean val. loss:  3.83395123e+00\n",
      "Epoch: 18180 mean train loss:  1.48488034e-05, mean val. loss:  3.83430600e+00\n",
      "Epoch: 18181 mean train loss:  1.47918763e-05, mean val. loss:  3.83459568e+00\n",
      "Epoch: 18182 mean train loss:  1.46014208e-05, mean val. loss:  3.83491158e+00\n",
      "Epoch: 18183 mean train loss:  1.48750842e-05, mean val. loss:  3.83519411e+00\n",
      "Epoch: 18184 mean train loss:  1.49623957e-05, mean val. loss:  3.83545375e+00\n",
      "Epoch: 18185 mean train loss:  1.47101237e-05, mean val. loss:  3.83568382e+00\n",
      "Epoch: 18186 mean train loss:  1.49557018e-05, mean val. loss:  3.83589554e+00\n",
      "Epoch: 18187 mean train loss:  1.47316896e-05, mean val. loss:  3.83606148e+00\n",
      "Epoch: 18188 mean train loss:  1.46270613e-05, mean val. loss:  3.83630347e+00\n",
      "Epoch: 18189 mean train loss:  1.46309903e-05, mean val. loss:  3.83659959e+00\n",
      "Epoch: 18190 mean train loss:  1.50356500e-05, mean val. loss:  3.83680606e+00\n",
      "Epoch: 18191 mean train loss:  1.46774983e-05, mean val. loss:  3.83707428e+00\n",
      "Epoch: 18192 mean train loss:  1.49679254e-05, mean val. loss:  3.83731747e+00\n",
      "Epoch: 18193 mean train loss:  1.51788990e-05, mean val. loss:  3.83748937e+00\n",
      "Epoch: 18194 mean train loss:  1.46518869e-05, mean val. loss:  3.83772016e+00\n",
      "Epoch: 18195 mean train loss:  1.48787221e-05, mean val. loss:  3.83801627e+00\n",
      "Epoch: 18196 mean train loss:  1.48616382e-05, mean val. loss:  3.83828926e+00\n",
      "Epoch: 18197 mean train loss:  1.51146669e-05, mean val. loss:  3.83851385e+00\n",
      "Epoch: 18198 mean train loss:  1.47052924e-05, mean val. loss:  3.83874321e+00\n",
      "Epoch: 18199 mean train loss:  1.49549451e-05, mean val. loss:  3.83888650e+00\n",
      "Epoch: 18200 mean train loss:  1.49120460e-05, mean val. loss:  3.83904886e+00\n",
      "Epoch: 18201 mean train loss:  1.49539555e-05, mean val. loss:  3.83919930e+00\n",
      "Epoch: 18202 mean train loss:  1.48757244e-05, mean val. loss:  3.83935142e+00\n",
      "Epoch: 18203 mean train loss:  1.49337575e-05, mean val. loss:  3.83948660e+00\n",
      "Epoch: 18204 mean train loss:  1.47837854e-05, mean val. loss:  3.83961630e+00\n",
      "Epoch: 18205 mean train loss:  1.46226084e-05, mean val. loss:  3.83980179e+00\n",
      "Epoch: 18206 mean train loss:  1.48461259e-05, mean val. loss:  3.83997560e+00\n",
      "Epoch: 18207 mean train loss:  1.46312232e-05, mean val. loss:  3.84017062e+00\n",
      "Epoch: 18208 mean train loss:  1.47099199e-05, mean val. loss:  3.84034514e+00\n",
      "Epoch: 18209 mean train loss:  1.47032551e-05, mean val. loss:  3.84049392e+00\n",
      "Epoch: 18210 mean train loss:  1.46943785e-05, mean val. loss:  3.84063268e+00\n",
      "Epoch: 18211 mean train loss:  1.48010149e-05, mean val. loss:  3.84076285e+00\n",
      "Epoch: 18212 mean train loss:  1.45713857e-05, mean val. loss:  3.84099030e+00\n",
      "Epoch: 18213 mean train loss:  1.47636456e-05, mean val. loss:  3.84134173e+00\n",
      "Epoch: 18214 mean train loss:  1.46250823e-05, mean val. loss:  3.84167552e+00\n",
      "Epoch: 18215 mean train loss:  1.47310202e-05, mean val. loss:  3.84200072e+00\n",
      "Epoch: 18216 mean train loss:  1.49425468e-05, mean val. loss:  3.84235907e+00\n",
      "Epoch: 18217 mean train loss:  1.48983963e-05, mean val. loss:  3.84277940e+00\n",
      "Epoch: 18218 mean train loss:  1.48137333e-05, mean val. loss:  3.84319782e+00\n",
      "Epoch: 18219 mean train loss:  1.47241226e-05, mean val. loss:  3.84363914e+00\n",
      "Epoch: 18220 mean train loss:  1.46378297e-05, mean val. loss:  3.84417367e+00\n",
      "Epoch: 18221 mean train loss:  1.47260725e-05, mean val. loss:  3.84468269e+00\n",
      "Epoch: 18222 mean train loss:  1.47596002e-05, mean val. loss:  3.84513497e+00\n",
      "Epoch: 18223 mean train loss:  1.51497661e-05, mean val. loss:  3.84552193e+00\n",
      "Epoch: 18224 mean train loss:  1.48739782e-05, mean val. loss:  3.84590864e+00\n",
      "Epoch: 18225 mean train loss:  1.47588144e-05, mean val. loss:  3.84627748e+00\n",
      "Epoch: 18226 mean train loss:  1.46994425e-05, mean val. loss:  3.84661698e+00\n",
      "Epoch: 18227 mean train loss:  1.48405088e-05, mean val. loss:  3.84686899e+00\n",
      "Epoch: 18228 mean train loss:  1.46634120e-05, mean val. loss:  3.84709096e+00\n",
      "Epoch: 18229 mean train loss:  1.47067476e-05, mean val. loss:  3.84736419e+00\n",
      "Epoch: 18230 mean train loss:  1.46584352e-05, mean val. loss:  3.84766030e+00\n",
      "Epoch: 18231 mean train loss:  1.46674574e-05, mean val. loss:  3.84798884e+00\n",
      "Epoch: 18232 mean train loss:  1.47459214e-05, mean val. loss:  3.84838486e+00\n",
      "Epoch: 18233 mean train loss:  1.51830027e-05, mean val. loss:  3.84869194e+00\n",
      "Epoch: 18234 mean train loss:  1.47842802e-05, mean val. loss:  3.84903836e+00\n",
      "Epoch: 18235 mean train loss:  1.48498802e-05, mean val. loss:  3.84938693e+00\n",
      "Epoch: 18236 mean train loss:  1.48887921e-05, mean val. loss:  3.84971595e+00\n",
      "Epoch: 18237 mean train loss:  1.50270935e-05, mean val. loss:  3.85004497e+00\n",
      "Epoch: 18238 mean train loss:  1.49905391e-05, mean val. loss:  3.85034227e+00\n",
      "Epoch: 18239 mean train loss:  1.45963859e-05, mean val. loss:  3.85074377e+00\n",
      "Epoch: 18240 mean train loss:  1.47828250e-05, mean val. loss:  3.85115337e+00\n",
      "Epoch: 18241 mean train loss:  1.44526130e-05, mean val. loss:  3.85161257e+00\n",
      "Epoch: 18242 mean train loss:  1.49266561e-05, mean val. loss:  3.85199046e+00\n",
      "Epoch: 18243 mean train loss:  1.48954859e-05, mean val. loss:  3.85230350e+00\n",
      "Epoch: 18244 mean train loss:  1.46702223e-05, mean val. loss:  3.85261035e+00\n",
      "Epoch: 18245 mean train loss:  1.47598621e-05, mean val. loss:  3.85289168e+00\n",
      "Epoch: 18246 mean train loss:  1.47962419e-05, mean val. loss:  3.85316539e+00\n",
      "Epoch: 18247 mean train loss:  1.47577957e-05, mean val. loss:  3.85345292e+00\n",
      "Epoch: 18248 mean train loss:  1.48867548e-05, mean val. loss:  3.85370398e+00\n",
      "Epoch: 18249 mean train loss:  1.48054387e-05, mean val. loss:  3.85395217e+00\n",
      "Epoch: 18250 mean train loss:  1.50121050e-05, mean val. loss:  3.85420990e+00\n",
      "Epoch: 18251 mean train loss:  1.47261308e-05, mean val. loss:  3.85450840e+00\n",
      "Epoch: 18252 mean train loss:  1.47722312e-05, mean val. loss:  3.85474634e+00\n",
      "Epoch: 18253 mean train loss:  1.44762453e-05, mean val. loss:  3.85501766e+00\n",
      "Epoch: 18254 mean train loss:  1.47434475e-05, mean val. loss:  3.85524869e+00\n",
      "Epoch: 18255 mean train loss:  1.49338448e-05, mean val. loss:  3.85550332e+00\n",
      "Epoch: 18256 mean train loss:  1.46883249e-05, mean val. loss:  3.85579729e+00\n",
      "Epoch: 18257 mean train loss:  1.47421961e-05, mean val. loss:  3.85611558e+00\n",
      "Epoch: 18258 mean train loss:  1.48149556e-05, mean val. loss:  3.85639405e+00\n",
      "Epoch: 18259 mean train loss:  1.47245592e-05, mean val. loss:  3.85667920e+00\n",
      "Epoch: 18260 mean train loss:  1.51541899e-05, mean val. loss:  3.85687828e+00\n",
      "Epoch: 18261 mean train loss:  1.45324739e-05, mean val. loss:  3.85710239e+00\n",
      "Epoch: 18262 mean train loss:  1.48467370e-05, mean val. loss:  3.85734510e+00\n",
      "Epoch: 18263 mean train loss:  1.48540421e-05, mean val. loss:  3.85760117e+00\n",
      "Epoch: 18264 mean train loss:  1.49334955e-05, mean val. loss:  3.85784006e+00\n",
      "Epoch: 18265 mean train loss:  1.51435961e-05, mean val. loss:  3.85801888e+00\n",
      "Epoch: 18266 mean train loss:  1.48593681e-05, mean val. loss:  3.85814357e+00\n",
      "Epoch: 18267 mean train loss:  1.46393722e-05, mean val. loss:  3.85830617e+00\n",
      "Epoch: 18268 mean train loss:  1.51717977e-05, mean val. loss:  3.85842919e+00\n",
      "Epoch: 18269 mean train loss:  1.48104446e-05, mean val. loss:  3.85855651e+00\n",
      "Epoch: 18270 mean train loss:  1.47360843e-05, mean val. loss:  3.85868025e+00\n",
      "Epoch: 18271 mean train loss:  1.48442923e-05, mean val. loss:  3.85884881e+00\n",
      "Epoch: 18272 mean train loss:  1.46779930e-05, mean val. loss:  3.85904527e+00\n",
      "Epoch: 18273 mean train loss:  1.46578823e-05, mean val. loss:  3.85927153e+00\n",
      "Epoch: 18274 mean train loss:  1.49224652e-05, mean val. loss:  3.85951543e+00\n",
      "Epoch: 18275 mean train loss:  1.48227555e-05, mean val. loss:  3.85976744e+00\n",
      "Epoch: 18276 mean train loss:  1.48437684e-05, mean val. loss:  3.86007214e+00\n",
      "Epoch: 18277 mean train loss:  1.49176340e-05, mean val. loss:  3.86034894e+00\n",
      "Epoch: 18278 mean train loss:  1.48108811e-05, mean val. loss:  3.86059546e+00\n",
      "Epoch: 18279 mean train loss:  1.48943218e-05, mean val. loss:  3.86081767e+00\n",
      "Epoch: 18280 mean train loss:  1.46878883e-05, mean val. loss:  3.86106873e+00\n",
      "Epoch: 18281 mean train loss:  1.48620456e-05, mean val. loss:  3.86138654e+00\n",
      "Epoch: 18282 mean train loss:  1.48675172e-05, mean val. loss:  3.86167574e+00\n",
      "Epoch: 18283 mean train loss:  1.47249375e-05, mean val. loss:  3.86196375e+00\n",
      "Epoch: 18284 mean train loss:  1.48948457e-05, mean val. loss:  3.86221004e+00\n",
      "Epoch: 18285 mean train loss:  1.45572703e-05, mean val. loss:  3.86256146e+00\n",
      "Epoch: 18286 mean train loss:  1.46165839e-05, mean val. loss:  3.86295414e+00\n",
      "Epoch: 18287 mean train loss:  1.48696126e-05, mean val. loss:  3.86330509e+00\n",
      "Epoch: 18288 mean train loss:  1.47228420e-05, mean val. loss:  3.86362576e+00\n",
      "Epoch: 18289 mean train loss:  1.49410334e-05, mean val. loss:  3.86392641e+00\n",
      "Epoch: 18290 mean train loss:  1.47387327e-05, mean val. loss:  3.86418819e+00\n",
      "Epoch: 18291 mean train loss:  1.47051469e-05, mean val. loss:  3.86447954e+00\n",
      "Epoch: 18292 mean train loss:  1.44488586e-05, mean val. loss:  3.86483073e+00\n",
      "Epoch: 18293 mean train loss:  1.49213593e-05, mean val. loss:  3.86507630e+00\n",
      "Epoch: 18294 mean train loss:  1.45685626e-05, mean val. loss:  3.86537075e+00\n",
      "Epoch: 18295 mean train loss:  1.45358499e-05, mean val. loss:  3.86568785e+00\n",
      "Epoch: 18296 mean train loss:  1.48833496e-05, mean val. loss:  3.86600471e+00\n",
      "Epoch: 18297 mean train loss:  1.47361134e-05, mean val. loss:  3.86632323e+00\n",
      "Epoch: 18298 mean train loss:  1.48143445e-05, mean val. loss:  3.86669421e+00\n",
      "Epoch: 18299 mean train loss:  1.46891398e-05, mean val. loss:  3.86710143e+00\n",
      "Epoch: 18300 mean train loss:  1.47505780e-05, mean val. loss:  3.86749411e+00\n",
      "Epoch: 18301 mean train loss:  1.50658889e-05, mean val. loss:  3.86781693e+00\n",
      "Epoch: 18302 mean train loss:  1.49373373e-05, mean val. loss:  3.86820173e+00\n",
      "Epoch: 18303 mean train loss:  1.49862317e-05, mean val. loss:  3.86858988e+00\n",
      "Epoch: 18304 mean train loss:  1.48524123e-05, mean val. loss:  3.86896706e+00\n",
      "Epoch: 18305 mean train loss:  1.48281106e-05, mean val. loss:  3.86930609e+00\n",
      "Epoch: 18306 mean train loss:  1.46855600e-05, mean val. loss:  3.86967492e+00\n",
      "Epoch: 18307 mean train loss:  1.50146952e-05, mean val. loss:  3.86999416e+00\n",
      "Epoch: 18308 mean train loss:  1.46968232e-05, mean val. loss:  3.87025213e+00\n",
      "Epoch: 18309 mean train loss:  1.48708932e-05, mean val. loss:  3.87051821e+00\n",
      "Epoch: 18310 mean train loss:  1.47026731e-05, mean val. loss:  3.87077832e+00\n",
      "Epoch: 18311 mean train loss:  1.47264800e-05, mean val. loss:  3.87101436e+00\n",
      "Epoch: 18312 mean train loss:  1.47014507e-05, mean val. loss:  3.87121367e+00\n",
      "Epoch: 18313 mean train loss:  1.49354164e-05, mean val. loss:  3.87136841e+00\n",
      "Epoch: 18314 mean train loss:  1.47380342e-05, mean val. loss:  3.87151003e+00\n",
      "Epoch: 18315 mean train loss:  1.45341619e-05, mean val. loss:  3.87173057e+00\n",
      "Epoch: 18316 mean train loss:  1.45547965e-05, mean val. loss:  3.87194490e+00\n",
      "Epoch: 18317 mean train loss:  1.48140243e-05, mean val. loss:  3.87216806e+00\n",
      "Epoch: 18318 mean train loss:  1.47809042e-05, mean val. loss:  3.87245655e+00\n",
      "Epoch: 18319 mean train loss:  1.48825930e-05, mean val. loss:  3.87278152e+00\n",
      "Epoch: 18320 mean train loss:  1.47582614e-05, mean val. loss:  3.87319660e+00\n",
      "Epoch: 18321 mean train loss:  1.46775274e-05, mean val. loss:  3.87355900e+00\n",
      "Epoch: 18322 mean train loss:  1.46998209e-05, mean val. loss:  3.87390614e+00\n",
      "Epoch: 18323 mean train loss:  1.50228443e-05, mean val. loss:  3.87417674e+00\n",
      "Epoch: 18324 mean train loss:  1.46881794e-05, mean val. loss:  3.87459707e+00\n",
      "Epoch: 18325 mean train loss:  1.46290986e-05, mean val. loss:  3.87508535e+00\n",
      "Epoch: 18326 mean train loss:  1.47412356e-05, mean val. loss:  3.87553740e+00\n",
      "Epoch: 18327 mean train loss:  1.48573890e-05, mean val. loss:  3.87596583e+00\n",
      "Epoch: 18328 mean train loss:  1.46383827e-05, mean val. loss:  3.87636542e+00\n",
      "Epoch: 18329 mean train loss:  1.49954285e-05, mean val. loss:  3.87671375e+00\n",
      "Epoch: 18330 mean train loss:  1.48529944e-05, mean val. loss:  3.87706804e+00\n",
      "Epoch: 18331 mean train loss:  1.48270337e-05, mean val. loss:  3.87735605e+00\n",
      "Epoch: 18332 mean train loss:  1.46148377e-05, mean val. loss:  3.87764716e+00\n",
      "Epoch: 18333 mean train loss:  1.47948740e-05, mean val. loss:  3.87799287e+00\n",
      "Epoch: 18334 mean train loss:  1.48806721e-05, mean val. loss:  3.87827039e+00\n",
      "Epoch: 18335 mean train loss:  1.46279635e-05, mean val. loss:  3.87855864e+00\n",
      "Epoch: 18336 mean train loss:  1.47612882e-05, mean val. loss:  3.87874889e+00\n",
      "Epoch: 18337 mean train loss:  1.46556413e-05, mean val. loss:  3.87890935e+00\n",
      "Epoch: 18338 mean train loss:  1.46590755e-05, mean val. loss:  3.87908578e+00\n",
      "Epoch: 18339 mean train loss:  1.49031112e-05, mean val. loss:  3.87928891e+00\n",
      "Epoch: 18340 mean train loss:  1.47201936e-05, mean val. loss:  3.87947297e+00\n",
      "Epoch: 18341 mean train loss:  1.48719992e-05, mean val. loss:  3.87960577e+00\n",
      "Epoch: 18342 mean train loss:  1.50551787e-05, mean val. loss:  3.87972879e+00\n",
      "Epoch: 18343 mean train loss:  1.45131198e-05, mean val. loss:  3.87992811e+00\n",
      "Epoch: 18344 mean train loss:  1.48854451e-05, mean val. loss:  3.88012290e+00\n",
      "Epoch: 18345 mean train loss:  1.46757520e-05, mean val. loss:  3.88035226e+00\n",
      "Epoch: 18346 mean train loss:  1.49341649e-05, mean val. loss:  3.88051963e+00\n",
      "Epoch: 18347 mean train loss:  1.50445558e-05, mean val. loss:  3.88067913e+00\n",
      "Epoch: 18348 mean train loss:  1.44347432e-05, mean val. loss:  3.88090873e+00\n",
      "Epoch: 18349 mean train loss:  1.49308471e-05, mean val. loss:  3.88106251e+00\n",
      "Epoch: 18350 mean train loss:  1.45070371e-05, mean val. loss:  3.88128018e+00\n",
      "Epoch: 18351 mean train loss:  1.47460087e-05, mean val. loss:  3.88146544e+00\n",
      "Epoch: 18352 mean train loss:  1.45861995e-05, mean val. loss:  3.88167906e+00\n",
      "Epoch: 18353 mean train loss:  1.45401864e-05, mean val. loss:  3.88195777e+00\n",
      "Epoch: 18354 mean train loss:  1.46807870e-05, mean val. loss:  3.88226891e+00\n",
      "Epoch: 18355 mean train loss:  1.47523242e-05, mean val. loss:  3.88254070e+00\n",
      "Epoch: 18356 mean train loss:  1.47725805e-05, mean val. loss:  3.88282156e+00\n",
      "Epoch: 18357 mean train loss:  1.45840459e-05, mean val. loss:  3.88312221e+00\n",
      "Epoch: 18358 mean train loss:  1.46595121e-05, mean val. loss:  3.88349891e+00\n",
      "Epoch: 18359 mean train loss:  1.48455147e-05, mean val. loss:  3.88388538e+00\n",
      "Epoch: 18360 mean train loss:  1.46818056e-05, mean val. loss:  3.88434124e+00\n",
      "Epoch: 18361 mean train loss:  1.48706022e-05, mean val. loss:  3.88480377e+00\n",
      "Epoch: 18362 mean train loss:  1.46350649e-05, mean val. loss:  3.88529205e+00\n",
      "Epoch: 18363 mean train loss:  1.48598920e-05, mean val. loss:  3.88580132e+00\n",
      "Epoch: 18364 mean train loss:  1.50341948e-05, mean val. loss:  3.88624406e+00\n",
      "Epoch: 18365 mean train loss:  1.44844817e-05, mean val. loss:  3.88672209e+00\n",
      "Epoch: 18366 mean train loss:  1.45233935e-05, mean val. loss:  3.88721037e+00\n",
      "Epoch: 18367 mean train loss:  1.49116968e-05, mean val. loss:  3.88768053e+00\n",
      "Epoch: 18368 mean train loss:  1.46153907e-05, mean val. loss:  3.88814235e+00\n",
      "Epoch: 18369 mean train loss:  1.47772371e-05, mean val. loss:  3.88856626e+00\n",
      "Epoch: 18370 mean train loss:  1.50079431e-05, mean val. loss:  3.88893890e+00\n",
      "Epoch: 18371 mean train loss:  1.48075633e-05, mean val. loss:  3.88939691e+00\n",
      "Epoch: 18372 mean train loss:  1.43995858e-05, mean val. loss:  3.88989067e+00\n",
      "Epoch: 18373 mean train loss:  1.44515943e-05, mean val. loss:  3.89041948e+00\n",
      "Epoch: 18374 mean train loss:  1.46796810e-05, mean val. loss:  3.89093637e+00\n",
      "Epoch: 18375 mean train loss:  1.50345440e-05, mean val. loss:  3.89131784e+00\n",
      "Epoch: 18376 mean train loss:  1.47804967e-05, mean val. loss:  3.89170194e+00\n",
      "Epoch: 18377 mean train loss:  1.46233069e-05, mean val. loss:  3.89205909e+00\n",
      "Epoch: 18378 mean train loss:  1.46414677e-05, mean val. loss:  3.89239788e+00\n",
      "Epoch: 18379 mean train loss:  1.47150131e-05, mean val. loss:  3.89267278e+00\n",
      "Epoch: 18380 mean train loss:  1.47738319e-05, mean val. loss:  3.89292789e+00\n",
      "Epoch: 18381 mean train loss:  1.48396066e-05, mean val. loss:  3.89318895e+00\n",
      "Epoch: 18382 mean train loss:  1.46138191e-05, mean val. loss:  3.89338541e+00\n",
      "Epoch: 18383 mean train loss:  1.46483653e-05, mean val. loss:  3.89363074e+00\n",
      "Epoch: 18384 mean train loss:  1.47363462e-05, mean val. loss:  3.89385724e+00\n",
      "Epoch: 18385 mean train loss:  1.44243531e-05, mean val. loss:  3.89414144e+00\n",
      "Epoch: 18386 mean train loss:  1.46263046e-05, mean val. loss:  3.89441705e+00\n",
      "Epoch: 18387 mean train loss:  1.47657120e-05, mean val. loss:  3.89468098e+00\n",
      "Epoch: 18388 mean train loss:  1.47972023e-05, mean val. loss:  3.89494467e+00\n",
      "Epoch: 18389 mean train loss:  1.46141392e-05, mean val. loss:  3.89524817e+00\n",
      "Epoch: 18390 mean train loss:  1.47281971e-05, mean val. loss:  3.89558721e+00\n",
      "Epoch: 18391 mean train loss:  1.44521182e-05, mean val. loss:  3.89602733e+00\n",
      "Epoch: 18392 mean train loss:  1.48176041e-05, mean val. loss:  3.89653373e+00\n",
      "Epoch: 18393 mean train loss:  1.47190876e-05, mean val. loss:  3.89705062e+00\n",
      "Epoch: 18394 mean train loss:  1.48605614e-05, mean val. loss:  3.89745855e+00\n",
      "Epoch: 18395 mean train loss:  1.46586099e-05, mean val. loss:  3.89791179e+00\n",
      "Epoch: 18396 mean train loss:  1.50620472e-05, mean val. loss:  3.89826393e+00\n",
      "Epoch: 18397 mean train loss:  1.47366954e-05, mean val. loss:  3.89855552e+00\n",
      "Epoch: 18398 mean train loss:  1.50215928e-05, mean val. loss:  3.89884567e+00\n",
      "Epoch: 18399 mean train loss:  1.48566905e-05, mean val. loss:  3.89910769e+00\n",
      "Epoch: 18400 mean train loss:  1.48588442e-05, mean val. loss:  3.89932346e+00\n",
      "Epoch: 18401 mean train loss:  1.47706014e-05, mean val. loss:  3.89949870e+00\n",
      "Epoch: 18402 mean train loss:  1.48613472e-05, mean val. loss:  3.89965081e+00\n",
      "Epoch: 18403 mean train loss:  1.49723492e-05, mean val. loss:  3.89968276e+00\n",
      "Epoch: 18404 mean train loss:  1.49642001e-05, mean val. loss:  3.89967179e+00\n",
      "Epoch: 18405 mean train loss:  1.46141101e-05, mean val. loss:  3.89974952e+00\n",
      "Epoch: 18406 mean train loss:  1.49563712e-05, mean val. loss:  3.89979362e+00\n",
      "Epoch: 18407 mean train loss:  1.50936539e-05, mean val. loss:  3.89984894e+00\n",
      "Epoch: 18408 mean train loss:  1.43419602e-05, mean val. loss:  3.90000653e+00\n",
      "Epoch: 18409 mean train loss:  1.47828832e-05, mean val. loss:  3.90018535e+00\n",
      "Epoch: 18410 mean train loss:  1.45758677e-05, mean val. loss:  3.90043998e+00\n",
      "Epoch: 18411 mean train loss:  1.45962404e-05, mean val. loss:  3.90079689e+00\n",
      "Epoch: 18412 mean train loss:  1.47308165e-05, mean val. loss:  3.90126777e+00\n",
      "Epoch: 18413 mean train loss:  1.49679254e-05, mean val. loss:  3.90173697e+00\n",
      "Epoch: 18414 mean train loss:  1.49114931e-05, mean val. loss:  3.90223432e+00\n",
      "Epoch: 18415 mean train loss:  1.45189988e-05, mean val. loss:  3.90279102e+00\n",
      "Epoch: 18416 mean train loss:  1.47148385e-05, mean val. loss:  3.90336728e+00\n",
      "Epoch: 18417 mean train loss:  1.45676895e-05, mean val. loss:  3.90397453e+00\n",
      "Epoch: 18418 mean train loss:  1.46816019e-05, mean val. loss:  3.90455556e+00\n",
      "Epoch: 18419 mean train loss:  1.45528466e-05, mean val. loss:  3.90516877e+00\n",
      "Epoch: 18420 mean train loss:  1.48143445e-05, mean val. loss:  3.90574503e+00\n",
      "Epoch: 18421 mean train loss:  1.44661753e-05, mean val. loss:  3.90633297e+00\n",
      "Epoch: 18422 mean train loss:  1.44826190e-05, mean val. loss:  3.90689850e+00\n",
      "Epoch: 18423 mean train loss:  1.47401297e-05, mean val. loss:  3.90741611e+00\n",
      "Epoch: 18424 mean train loss:  1.45544764e-05, mean val. loss:  3.90795279e+00\n",
      "Epoch: 18425 mean train loss:  1.46576785e-05, mean val. loss:  3.90847468e+00\n",
      "Epoch: 18426 mean train loss:  1.49684784e-05, mean val. loss:  3.90893531e+00\n",
      "Epoch: 18427 mean train loss:  1.45974336e-05, mean val. loss:  3.90937614e+00\n",
      "Epoch: 18428 mean train loss:  1.45776430e-05, mean val. loss:  3.90983748e+00\n",
      "Epoch: 18429 mean train loss:  1.46622187e-05, mean val. loss:  3.91026735e+00\n",
      "Epoch: 18430 mean train loss:  1.45692902e-05, mean val. loss:  3.91072035e+00\n",
      "Epoch: 18431 mean train loss:  1.49418192e-05, mean val. loss:  3.91114426e+00\n",
      "Epoch: 18432 mean train loss:  1.48375984e-05, mean val. loss:  3.91152382e+00\n",
      "Epoch: 18433 mean train loss:  1.45428930e-05, mean val. loss:  3.91192007e+00\n",
      "Epoch: 18434 mean train loss:  1.48758118e-05, mean val. loss:  3.91225004e+00\n",
      "Epoch: 18435 mean train loss:  1.46975508e-05, mean val. loss:  3.91255975e+00\n",
      "Epoch: 18436 mean train loss:  1.48621912e-05, mean val. loss:  3.91278434e+00\n",
      "Epoch: 18437 mean train loss:  1.47667015e-05, mean val. loss:  3.91292787e+00\n",
      "Epoch: 18438 mean train loss:  1.46382663e-05, mean val. loss:  3.91308999e+00\n",
      "Epoch: 18439 mean train loss:  1.45443191e-05, mean val. loss:  3.91327262e+00\n",
      "Epoch: 18440 mean train loss:  1.47613464e-05, mean val. loss:  3.91339469e+00\n",
      "Epoch: 18441 mean train loss:  1.44772930e-05, mean val. loss:  3.91352201e+00\n",
      "Epoch: 18442 mean train loss:  1.45858503e-05, mean val. loss:  3.91361666e+00\n",
      "Epoch: 18443 mean train loss:  1.46261591e-05, mean val. loss:  3.91374826e+00\n",
      "Epoch: 18444 mean train loss:  1.48290419e-05, mean val. loss:  3.91383505e+00\n",
      "Epoch: 18445 mean train loss:  1.45674567e-05, mean val. loss:  3.91394687e+00\n",
      "Epoch: 18446 mean train loss:  1.48477848e-05, mean val. loss:  3.91403770e+00\n",
      "Epoch: 18447 mean train loss:  1.49588741e-05, mean val. loss:  3.91404963e+00\n",
      "Epoch: 18448 mean train loss:  1.47606770e-05, mean val. loss:  3.91407704e+00\n",
      "Epoch: 18449 mean train loss:  1.47752580e-05, mean val. loss:  3.91409039e+00\n",
      "Epoch: 18450 mean train loss:  1.47534884e-05, mean val. loss:  3.91417265e+00\n",
      "Epoch: 18451 mean train loss:  1.47666433e-05, mean val. loss:  3.91429138e+00\n",
      "Epoch: 18452 mean train loss:  1.49244152e-05, mean val. loss:  3.91443348e+00\n",
      "Epoch: 18453 mean train loss:  1.47197861e-05, mean val. loss:  3.91461158e+00\n",
      "Epoch: 18454 mean train loss:  1.47824467e-05, mean val. loss:  3.91487646e+00\n",
      "Epoch: 18455 mean train loss:  1.44362566e-05, mean val. loss:  3.91522384e+00\n",
      "Epoch: 18456 mean train loss:  1.47795945e-05, mean val. loss:  3.91566253e+00\n",
      "Epoch: 18457 mean train loss:  1.49543048e-05, mean val. loss:  3.91605377e+00\n",
      "Epoch: 18458 mean train loss:  1.48509280e-05, mean val. loss:  3.91639423e+00\n",
      "Epoch: 18459 mean train loss:  1.45423110e-05, mean val. loss:  3.91674376e+00\n",
      "Epoch: 18460 mean train loss:  1.47614337e-05, mean val. loss:  3.91712594e+00\n",
      "Epoch: 18461 mean train loss:  1.45868980e-05, mean val. loss:  3.91756177e+00\n",
      "Epoch: 18462 mean train loss:  1.45163212e-05, mean val. loss:  3.91794419e+00\n",
      "Epoch: 18463 mean train loss:  1.45574159e-05, mean val. loss:  3.91837168e+00\n",
      "Epoch: 18464 mean train loss:  1.47489191e-05, mean val. loss:  3.91874456e+00\n",
      "Epoch: 18465 mean train loss:  1.49125117e-05, mean val. loss:  3.91911864e+00\n",
      "Epoch: 18466 mean train loss:  1.46771781e-05, mean val. loss:  3.91950631e+00\n",
      "Epoch: 18467 mean train loss:  1.45068043e-05, mean val. loss:  3.91993380e+00\n",
      "Epoch: 18468 mean train loss:  1.45212980e-05, mean val. loss:  3.92034459e+00\n",
      "Epoch: 18469 mean train loss:  1.48068066e-05, mean val. loss:  3.92072701e+00\n",
      "Epoch: 18470 mean train loss:  1.46186794e-05, mean val. loss:  3.92114973e+00\n",
      "Epoch: 18471 mean train loss:  1.44780497e-05, mean val. loss:  3.92156315e+00\n",
      "Epoch: 18472 mean train loss:  1.49565167e-05, mean val. loss:  3.92195797e+00\n",
      "Epoch: 18473 mean train loss:  1.46842794e-05, mean val. loss:  3.92231321e+00\n",
      "Epoch: 18474 mean train loss:  1.46195525e-05, mean val. loss:  3.92268777e+00\n",
      "Epoch: 18475 mean train loss:  1.47368119e-05, mean val. loss:  3.92302656e+00\n",
      "Epoch: 18476 mean train loss:  1.46483653e-05, mean val. loss:  3.92339158e+00\n",
      "Epoch: 18477 mean train loss:  1.46469392e-05, mean val. loss:  3.92376423e+00\n",
      "Epoch: 18478 mean train loss:  1.49402185e-05, mean val. loss:  3.92403817e+00\n",
      "Epoch: 18479 mean train loss:  1.47949904e-05, mean val. loss:  3.92428160e+00\n",
      "Epoch: 18480 mean train loss:  1.48173422e-05, mean val. loss:  3.92450380e+00\n",
      "Epoch: 18481 mean train loss:  1.48245308e-05, mean val. loss:  3.92462397e+00\n",
      "Epoch: 18482 mean train loss:  1.45933300e-05, mean val. loss:  3.92465639e+00\n",
      "Epoch: 18483 mean train loss:  1.47675746e-05, mean val. loss:  3.92467737e+00\n",
      "Epoch: 18484 mean train loss:  1.44899823e-05, mean val. loss:  3.92475152e+00\n",
      "Epoch: 18485 mean train loss:  1.48651015e-05, mean val. loss:  3.92475986e+00\n",
      "Epoch: 18486 mean train loss:  1.48033432e-05, mean val. loss:  3.92474651e+00\n",
      "Epoch: 18487 mean train loss:  1.44047372e-05, mean val. loss:  3.92476630e+00\n",
      "Epoch: 18488 mean train loss:  1.47594546e-05, mean val. loss:  3.92480731e+00\n",
      "Epoch: 18489 mean train loss:  1.44703663e-05, mean val. loss:  3.92493391e+00\n",
      "Epoch: 18490 mean train loss:  1.45562808e-05, mean val. loss:  3.92507124e+00\n",
      "Epoch: 18491 mean train loss:  1.46655948e-05, mean val. loss:  3.92528248e+00\n",
      "Epoch: 18492 mean train loss:  1.47656538e-05, mean val. loss:  3.92549038e+00\n",
      "Epoch: 18493 mean train loss:  1.47476676e-05, mean val. loss:  3.92577100e+00\n",
      "Epoch: 18494 mean train loss:  1.46943203e-05, mean val. loss:  3.92607784e+00\n",
      "Epoch: 18495 mean train loss:  1.47651008e-05, mean val. loss:  3.92639828e+00\n",
      "Epoch: 18496 mean train loss:  1.44935912e-05, mean val. loss:  3.92671895e+00\n",
      "Epoch: 18497 mean train loss:  1.45910017e-05, mean val. loss:  3.92705536e+00\n",
      "Epoch: 18498 mean train loss:  1.49193220e-05, mean val. loss:  3.92741489e+00\n",
      "Epoch: 18499 mean train loss:  1.46150414e-05, mean val. loss:  3.92779589e+00\n",
      "Epoch: 18500 mean train loss:  1.44726655e-05, mean val. loss:  3.92822504e+00\n",
      "Epoch: 18501 mean train loss:  1.47376850e-05, mean val. loss:  3.92866850e+00\n",
      "Epoch: 18502 mean train loss:  1.44858204e-05, mean val. loss:  3.92914867e+00\n",
      "Epoch: 18503 mean train loss:  1.47008977e-05, mean val. loss:  3.92966819e+00\n",
      "Epoch: 18504 mean train loss:  1.47859682e-05, mean val. loss:  3.93022799e+00\n",
      "Epoch: 18505 mean train loss:  1.47140236e-05, mean val. loss:  3.93081212e+00\n",
      "Epoch: 18506 mean train loss:  1.48373656e-05, mean val. loss:  3.93136597e+00\n",
      "Epoch: 18507 mean train loss:  1.46488892e-05, mean val. loss:  3.93184352e+00\n",
      "Epoch: 18508 mean train loss:  1.47759565e-05, mean val. loss:  3.93234611e+00\n",
      "Epoch: 18509 mean train loss:  1.45228987e-05, mean val. loss:  3.93279314e+00\n",
      "Epoch: 18510 mean train loss:  1.45996455e-05, mean val. loss:  3.93324304e+00\n",
      "Epoch: 18511 mean train loss:  1.46970560e-05, mean val. loss:  3.93369412e+00\n",
      "Epoch: 18512 mean train loss:  1.47776736e-05, mean val. loss:  3.93416524e+00\n",
      "Epoch: 18513 mean train loss:  1.48876279e-05, mean val. loss:  3.93456292e+00\n",
      "Epoch: 18514 mean train loss:  1.44443475e-05, mean val. loss:  3.93494940e+00\n",
      "Epoch: 18515 mean train loss:  1.47287792e-05, mean val. loss:  3.93530154e+00\n",
      "Epoch: 18516 mean train loss:  1.43663492e-05, mean val. loss:  3.93563676e+00\n",
      "Epoch: 18517 mean train loss:  1.46679231e-05, mean val. loss:  3.93592477e+00\n",
      "Epoch: 18518 mean train loss:  1.49157713e-05, mean val. loss:  3.93613577e+00\n",
      "Epoch: 18519 mean train loss:  1.48792751e-05, mean val. loss:  3.93627381e+00\n",
      "Epoch: 18520 mean train loss:  1.44543010e-05, mean val. loss:  3.93649483e+00\n",
      "Epoch: 18521 mean train loss:  1.44518563e-05, mean val. loss:  3.93677497e+00\n",
      "Epoch: 18522 mean train loss:  1.47296523e-05, mean val. loss:  3.93697453e+00\n",
      "Epoch: 18523 mean train loss:  1.47266837e-05, mean val. loss:  3.93719769e+00\n",
      "Epoch: 18524 mean train loss:  1.48509280e-05, mean val. loss:  3.93736100e+00\n",
      "Epoch: 18525 mean train loss:  1.46171369e-05, mean val. loss:  3.93748975e+00\n",
      "Epoch: 18526 mean train loss:  1.47632672e-05, mean val. loss:  3.93773341e+00\n",
      "Epoch: 18527 mean train loss:  1.48028776e-05, mean val. loss:  3.93793416e+00\n",
      "Epoch: 18528 mean train loss:  1.47745886e-05, mean val. loss:  3.93807149e+00\n",
      "Epoch: 18529 mean train loss:  1.47629180e-05, mean val. loss:  3.93824244e+00\n",
      "Epoch: 18530 mean train loss:  1.45282247e-05, mean val. loss:  3.93842173e+00\n",
      "Epoch: 18531 mean train loss:  1.47807295e-05, mean val. loss:  3.93851566e+00\n",
      "Epoch: 18532 mean train loss:  1.47989194e-05, mean val. loss:  3.93865943e+00\n",
      "Epoch: 18533 mean train loss:  1.45932718e-05, mean val. loss:  3.93884325e+00\n",
      "Epoch: 18534 mean train loss:  1.45010417e-05, mean val. loss:  3.93906999e+00\n",
      "Epoch: 18535 mean train loss:  1.45913218e-05, mean val. loss:  3.93936896e+00\n",
      "Epoch: 18536 mean train loss:  1.45906815e-05, mean val. loss:  3.93969846e+00\n",
      "Epoch: 18537 mean train loss:  1.45228987e-05, mean val. loss:  3.94007540e+00\n",
      "Epoch: 18538 mean train loss:  1.44272926e-05, mean val. loss:  3.94051003e+00\n",
      "Epoch: 18539 mean train loss:  1.45212398e-05, mean val. loss:  3.94093561e+00\n",
      "Epoch: 18540 mean train loss:  1.46681850e-05, mean val. loss:  3.94134879e+00\n",
      "Epoch: 18541 mean train loss:  1.44399819e-05, mean val. loss:  3.94186449e+00\n",
      "Epoch: 18542 mean train loss:  1.48366089e-05, mean val. loss:  3.94229865e+00\n",
      "Epoch: 18543 mean train loss:  1.46481907e-05, mean val. loss:  3.94278240e+00\n",
      "Epoch: 18544 mean train loss:  1.47349201e-05, mean val. loss:  3.94333529e+00\n",
      "Epoch: 18545 mean train loss:  1.45645172e-05, mean val. loss:  3.94393229e+00\n",
      "Epoch: 18546 mean train loss:  1.44842197e-05, mean val. loss:  3.94448042e+00\n",
      "Epoch: 18547 mean train loss:  1.49084080e-05, mean val. loss:  3.94492936e+00\n",
      "Epoch: 18548 mean train loss:  1.48187100e-05, mean val. loss:  3.94535565e+00\n",
      "Epoch: 18549 mean train loss:  1.46580860e-05, mean val. loss:  3.94574475e+00\n",
      "Epoch: 18550 mean train loss:  1.43704237e-05, mean val. loss:  3.94620419e+00\n",
      "Epoch: 18551 mean train loss:  1.49995030e-05, mean val. loss:  3.94659114e+00\n",
      "Epoch: 18552 mean train loss:  1.47881219e-05, mean val. loss:  3.94694591e+00\n",
      "Epoch: 18553 mean train loss:  1.46043603e-05, mean val. loss:  3.94723368e+00\n",
      "Epoch: 18554 mean train loss:  1.45547383e-05, mean val. loss:  3.94753051e+00\n",
      "Epoch: 18555 mean train loss:  1.45789818e-05, mean val. loss:  3.94782186e+00\n",
      "Epoch: 18556 mean train loss:  1.49532571e-05, mean val. loss:  3.94803262e+00\n",
      "Epoch: 18557 mean train loss:  1.45749073e-05, mean val. loss:  3.94826770e+00\n",
      "Epoch: 18558 mean train loss:  1.46649836e-05, mean val. loss:  3.94851446e+00\n",
      "Epoch: 18559 mean train loss:  1.45912636e-05, mean val. loss:  3.94875193e+00\n",
      "Epoch: 18560 mean train loss:  1.47225219e-05, mean val. loss:  3.94899940e+00\n",
      "Epoch: 18561 mean train loss:  1.45891390e-05, mean val. loss:  3.94920206e+00\n",
      "Epoch: 18562 mean train loss:  1.46239181e-05, mean val. loss:  3.94939399e+00\n",
      "Epoch: 18563 mean train loss:  1.44879741e-05, mean val. loss:  3.94950485e+00\n",
      "Epoch: 18564 mean train loss:  1.45621016e-05, mean val. loss:  3.94957161e+00\n",
      "Epoch: 18565 mean train loss:  1.43240322e-05, mean val. loss:  3.94971848e+00\n",
      "Epoch: 18566 mean train loss:  1.43758953e-05, mean val. loss:  3.94991302e+00\n",
      "Epoch: 18567 mean train loss:  1.47165847e-05, mean val. loss:  3.95012164e+00\n",
      "Epoch: 18568 mean train loss:  1.46889652e-05, mean val. loss:  3.95031691e+00\n",
      "Epoch: 18569 mean train loss:  1.47106475e-05, mean val. loss:  3.95062566e+00\n",
      "Epoch: 18570 mean train loss:  1.44579390e-05, mean val. loss:  3.95099306e+00\n",
      "Epoch: 18571 mean train loss:  1.45115191e-05, mean val. loss:  3.95139527e+00\n",
      "Epoch: 18572 mean train loss:  1.48892286e-05, mean val. loss:  3.95182467e+00\n",
      "Epoch: 18573 mean train loss:  1.46221137e-05, mean val. loss:  3.95223093e+00\n",
      "Epoch: 18574 mean train loss:  1.48368999e-05, mean val. loss:  3.95267439e+00\n",
      "Epoch: 18575 mean train loss:  1.47662358e-05, mean val. loss:  3.95311093e+00\n",
      "Epoch: 18576 mean train loss:  1.47753744e-05, mean val. loss:  3.95349646e+00\n",
      "Epoch: 18577 mean train loss:  1.44605874e-05, mean val. loss:  3.95387554e+00\n",
      "Epoch: 18578 mean train loss:  1.44908554e-05, mean val. loss:  3.95428610e+00\n",
      "Epoch: 18579 mean train loss:  1.46860839e-05, mean val. loss:  3.95465064e+00\n",
      "Epoch: 18580 mean train loss:  1.46079692e-05, mean val. loss:  3.95501661e+00\n",
      "Epoch: 18581 mean train loss:  1.48997060e-05, mean val. loss:  3.95530629e+00\n",
      "Epoch: 18582 mean train loss:  1.44363148e-05, mean val. loss:  3.95559192e+00\n",
      "Epoch: 18583 mean train loss:  1.45915255e-05, mean val. loss:  3.95583797e+00\n",
      "Epoch: 18584 mean train loss:  1.47803803e-05, mean val. loss:  3.95606923e+00\n",
      "Epoch: 18585 mean train loss:  1.43861107e-05, mean val. loss:  3.95631337e+00\n",
      "Epoch: 18586 mean train loss:  1.44475198e-05, mean val. loss:  3.95656681e+00\n",
      "Epoch: 18587 mean train loss:  1.45891099e-05, mean val. loss:  3.95684338e+00\n",
      "Epoch: 18588 mean train loss:  1.45806407e-05, mean val. loss:  3.95710969e+00\n",
      "Epoch: 18589 mean train loss:  1.49021216e-05, mean val. loss:  3.95734453e+00\n",
      "Epoch: 18590 mean train loss:  1.45077356e-05, mean val. loss:  3.95762420e+00\n",
      "Epoch: 18591 mean train loss:  1.45379454e-05, mean val. loss:  3.95792580e+00\n",
      "Epoch: 18592 mean train loss:  1.46630628e-05, mean val. loss:  3.95826030e+00\n",
      "Epoch: 18593 mean train loss:  1.49708067e-05, mean val. loss:  3.95858192e+00\n",
      "Epoch: 18594 mean train loss:  1.45459780e-05, mean val. loss:  3.95891237e+00\n",
      "Epoch: 18595 mean train loss:  1.43644575e-05, mean val. loss:  3.95934129e+00\n",
      "Epoch: 18596 mean train loss:  1.47585815e-05, mean val. loss:  3.95969272e+00\n",
      "Epoch: 18597 mean train loss:  1.46312232e-05, mean val. loss:  3.95996833e+00\n",
      "Epoch: 18598 mean train loss:  1.47084065e-05, mean val. loss:  3.96026349e+00\n",
      "Epoch: 18599 mean train loss:  1.47905084e-05, mean val. loss:  3.96044993e+00\n",
      "Epoch: 18600 mean train loss:  1.45631493e-05, mean val. loss:  3.96065378e+00\n",
      "Epoch: 18601 mean train loss:  1.44796795e-05, mean val. loss:  3.96080923e+00\n",
      "Epoch: 18602 mean train loss:  1.46948441e-05, mean val. loss:  3.96091890e+00\n",
      "Epoch: 18603 mean train loss:  1.46447856e-05, mean val. loss:  3.96105671e+00\n",
      "Epoch: 18604 mean train loss:  1.44281075e-05, mean val. loss:  3.96121025e+00\n",
      "Epoch: 18605 mean train loss:  1.47993851e-05, mean val. loss:  3.96133518e+00\n",
      "Epoch: 18606 mean train loss:  1.46120437e-05, mean val. loss:  3.96155381e+00\n",
      "Epoch: 18607 mean train loss:  1.42796198e-05, mean val. loss:  3.96183443e+00\n",
      "Epoch: 18608 mean train loss:  1.48269173e-05, mean val. loss:  3.96208882e+00\n",
      "Epoch: 18609 mean train loss:  1.45265949e-05, mean val. loss:  3.96231699e+00\n",
      "Epoch: 18610 mean train loss:  1.47615501e-05, mean val. loss:  3.96258378e+00\n",
      "Epoch: 18611 mean train loss:  1.46654493e-05, mean val. loss:  3.96281052e+00\n",
      "Epoch: 18612 mean train loss:  1.44753139e-05, mean val. loss:  3.96305513e+00\n",
      "Epoch: 18613 mean train loss:  1.46357925e-05, mean val. loss:  3.96334529e+00\n",
      "Epoch: 18614 mean train loss:  1.45193480e-05, mean val. loss:  3.96368098e+00\n",
      "Epoch: 18615 mean train loss:  1.46137318e-05, mean val. loss:  3.96401405e+00\n",
      "Epoch: 18616 mean train loss:  1.47584360e-05, mean val. loss:  3.96432066e+00\n",
      "Epoch: 18617 mean train loss:  1.47365790e-05, mean val. loss:  3.96459460e+00\n",
      "Epoch: 18618 mean train loss:  1.45804370e-05, mean val. loss:  3.96489739e+00\n",
      "Epoch: 18619 mean train loss:  1.46618404e-05, mean val. loss:  3.96517229e+00\n",
      "Epoch: 18620 mean train loss:  1.44878577e-05, mean val. loss:  3.96546888e+00\n",
      "Epoch: 18621 mean train loss:  1.45244703e-05, mean val. loss:  3.96575999e+00\n",
      "Epoch: 18622 mean train loss:  1.43915822e-05, mean val. loss:  3.96614504e+00\n",
      "Epoch: 18623 mean train loss:  1.49246189e-05, mean val. loss:  3.96641469e+00\n",
      "Epoch: 18624 mean train loss:  1.43268844e-05, mean val. loss:  3.96680641e+00\n",
      "Epoch: 18625 mean train loss:  1.43910875e-05, mean val. loss:  3.96721768e+00\n",
      "Epoch: 18626 mean train loss:  1.45971135e-05, mean val. loss:  3.96760011e+00\n",
      "Epoch: 18627 mean train loss:  1.45832601e-05, mean val. loss:  3.96800017e+00\n",
      "Epoch: 18628 mean train loss:  1.48016261e-05, mean val. loss:  3.96836567e+00\n",
      "Epoch: 18629 mean train loss:  1.45975791e-05, mean val. loss:  3.96871972e+00\n",
      "Epoch: 18630 mean train loss:  1.46875100e-05, mean val. loss:  3.96905088e+00\n",
      "Epoch: 18631 mean train loss:  1.47596293e-05, mean val. loss:  3.96928883e+00\n",
      "Epoch: 18632 mean train loss:  1.44478981e-05, mean val. loss:  3.96962690e+00\n",
      "Epoch: 18633 mean train loss:  1.45833474e-05, mean val. loss:  3.96994781e+00\n",
      "Epoch: 18634 mean train loss:  1.42992358e-05, mean val. loss:  3.97026539e+00\n",
      "Epoch: 18635 mean train loss:  1.46579987e-05, mean val. loss:  3.97060728e+00\n",
      "Epoch: 18636 mean train loss:  1.44664664e-05, mean val. loss:  3.97091198e+00\n",
      "Epoch: 18637 mean train loss:  1.44916703e-05, mean val. loss:  3.97125196e+00\n",
      "Epoch: 18638 mean train loss:  1.44733640e-05, mean val. loss:  3.97162795e+00\n",
      "Epoch: 18639 mean train loss:  1.47744722e-05, mean val. loss:  3.97194099e+00\n",
      "Epoch: 18640 mean train loss:  1.45156228e-05, mean val. loss:  3.97230387e+00\n",
      "Epoch: 18641 mean train loss:  1.45859667e-05, mean val. loss:  3.97272754e+00\n",
      "Epoch: 18642 mean train loss:  1.44285150e-05, mean val. loss:  3.97320700e+00\n",
      "Epoch: 18643 mean train loss:  1.46137900e-05, mean val. loss:  3.97362065e+00\n",
      "Epoch: 18644 mean train loss:  1.45915546e-05, mean val. loss:  3.97399378e+00\n",
      "Epoch: 18645 mean train loss:  1.48537511e-05, mean val. loss:  3.97427893e+00\n",
      "Epoch: 18646 mean train loss:  1.46184466e-05, mean val. loss:  3.97459126e+00\n",
      "Epoch: 18647 mean train loss:  1.42990029e-05, mean val. loss:  3.97494364e+00\n",
      "Epoch: 18648 mean train loss:  1.44170772e-05, mean val. loss:  3.97530150e+00\n",
      "Epoch: 18649 mean train loss:  1.45575614e-05, mean val. loss:  3.97562528e+00\n",
      "Epoch: 18650 mean train loss:  1.45686208e-05, mean val. loss:  3.97590733e+00\n",
      "Epoch: 18651 mean train loss:  1.45957165e-05, mean val. loss:  3.97617221e+00\n",
      "Epoch: 18652 mean train loss:  1.44299993e-05, mean val. loss:  3.97645044e+00\n",
      "Epoch: 18653 mean train loss:  1.48313993e-05, mean val. loss:  3.97665191e+00\n",
      "Epoch: 18654 mean train loss:  1.44715596e-05, mean val. loss:  3.97684407e+00\n",
      "Epoch: 18655 mean train loss:  1.46140519e-05, mean val. loss:  3.97700548e+00\n",
      "Epoch: 18656 mean train loss:  1.46189413e-05, mean val. loss:  3.97716713e+00\n",
      "Epoch: 18657 mean train loss:  1.44538935e-05, mean val. loss:  3.97732997e+00\n",
      "Epoch: 18658 mean train loss:  1.45597151e-05, mean val. loss:  3.97748208e+00\n",
      "Epoch: 18659 mean train loss:  1.44024671e-05, mean val. loss:  3.97770429e+00\n",
      "Epoch: 18660 mean train loss:  1.45779923e-05, mean val. loss:  3.97794843e+00\n",
      "Epoch: 18661 mean train loss:  1.44644582e-05, mean val. loss:  3.97816777e+00\n",
      "Epoch: 18662 mean train loss:  1.46927487e-05, mean val. loss:  3.97827744e+00\n",
      "Epoch: 18663 mean train loss:  1.44625956e-05, mean val. loss:  3.97835565e+00\n",
      "Epoch: 18664 mean train loss:  1.47742685e-05, mean val. loss:  3.97837162e+00\n",
      "Epoch: 18665 mean train loss:  1.45286613e-05, mean val. loss:  3.97838140e+00\n",
      "Epoch: 18666 mean train loss:  1.45494996e-05, mean val. loss:  3.97839689e+00\n",
      "Epoch: 18667 mean train loss:  1.46561360e-05, mean val. loss:  3.97839427e+00\n",
      "Epoch: 18668 mean train loss:  1.46235398e-05, mean val. loss:  3.97848654e+00\n",
      "Epoch: 18669 mean train loss:  1.44994119e-05, mean val. loss:  3.97860646e+00\n",
      "Epoch: 18670 mean train loss:  1.46181264e-05, mean val. loss:  3.97873783e+00\n",
      "Epoch: 18671 mean train loss:  1.47043611e-05, mean val. loss:  3.97885895e+00\n",
      "Epoch: 18672 mean train loss:  1.46018574e-05, mean val. loss:  3.97900867e+00\n",
      "Epoch: 18673 mean train loss:  1.47224928e-05, mean val. loss:  3.97911644e+00\n",
      "Epoch: 18674 mean train loss:  1.45205122e-05, mean val. loss:  3.97923613e+00\n",
      "Epoch: 18675 mean train loss:  1.41964701e-05, mean val. loss:  3.97948408e+00\n",
      "Epoch: 18676 mean train loss:  1.43341022e-05, mean val. loss:  3.97975779e+00\n",
      "Epoch: 18677 mean train loss:  1.45234808e-05, mean val. loss:  3.98009706e+00\n",
      "Epoch: 18678 mean train loss:  1.46517123e-05, mean val. loss:  3.98051167e+00\n",
      "Epoch: 18679 mean train loss:  1.46675156e-05, mean val. loss:  3.98092079e+00\n",
      "Epoch: 18680 mean train loss:  1.46527018e-05, mean val. loss:  3.98141050e+00\n",
      "Epoch: 18681 mean train loss:  1.48016843e-05, mean val. loss:  3.98194456e+00\n",
      "Epoch: 18682 mean train loss:  1.47564861e-05, mean val. loss:  3.98245239e+00\n",
      "Epoch: 18683 mean train loss:  1.47467363e-05, mean val. loss:  3.98292255e+00\n",
      "Epoch: 18684 mean train loss:  1.44866644e-05, mean val. loss:  3.98346281e+00\n",
      "Epoch: 18685 mean train loss:  1.44998485e-05, mean val. loss:  3.98401093e+00\n",
      "Epoch: 18686 mean train loss:  1.45094818e-05, mean val. loss:  3.98452210e+00\n",
      "Epoch: 18687 mean train loss:  1.46306411e-05, mean val. loss:  3.98500347e+00\n",
      "Epoch: 18688 mean train loss:  1.44492951e-05, mean val. loss:  3.98548770e+00\n",
      "Epoch: 18689 mean train loss:  1.47710671e-05, mean val. loss:  3.98595190e+00\n",
      "Epoch: 18690 mean train loss:  1.44672813e-05, mean val. loss:  3.98649478e+00\n",
      "Epoch: 18691 mean train loss:  1.45806116e-05, mean val. loss:  3.98700118e+00\n",
      "Epoch: 18692 mean train loss:  1.45339291e-05, mean val. loss:  3.98752546e+00\n",
      "Epoch: 18693 mean train loss:  1.45861413e-05, mean val. loss:  3.98803687e+00\n",
      "Epoch: 18694 mean train loss:  1.46615785e-05, mean val. loss:  3.98851347e+00\n",
      "Epoch: 18695 mean train loss:  1.47971441e-05, mean val. loss:  3.98897386e+00\n",
      "Epoch: 18696 mean train loss:  1.44749938e-05, mean val. loss:  3.98936963e+00\n",
      "Epoch: 18697 mean train loss:  1.45132653e-05, mean val. loss:  3.98979950e+00\n",
      "Epoch: 18698 mean train loss:  1.46146049e-05, mean val. loss:  3.99016976e+00\n",
      "Epoch: 18699 mean train loss:  1.46695820e-05, mean val. loss:  3.99047184e+00\n",
      "Epoch: 18700 mean train loss:  1.46794482e-05, mean val. loss:  3.99076414e+00\n",
      "Epoch: 18701 mean train loss:  1.43971120e-05, mean val. loss:  3.99107099e+00\n",
      "Epoch: 18702 mean train loss:  1.44702499e-05, mean val. loss:  3.99137831e+00\n",
      "Epoch: 18703 mean train loss:  1.44250225e-05, mean val. loss:  3.99166441e+00\n",
      "Epoch: 18704 mean train loss:  1.46181555e-05, mean val. loss:  3.99195313e+00\n",
      "Epoch: 18705 mean train loss:  1.45180675e-05, mean val. loss:  3.99222803e+00\n",
      "Epoch: 18706 mean train loss:  1.47373939e-05, mean val. loss:  3.99242735e+00\n",
      "Epoch: 18707 mean train loss:  1.45648373e-05, mean val. loss:  3.99263620e+00\n",
      "Epoch: 18708 mean train loss:  1.46473467e-05, mean val. loss:  3.99287128e+00\n",
      "Epoch: 18709 mean train loss:  1.44864898e-05, mean val. loss:  3.99311209e+00\n",
      "Epoch: 18710 mean train loss:  1.45875092e-05, mean val. loss:  3.99337649e+00\n",
      "Epoch: 18711 mean train loss:  1.47623941e-05, mean val. loss:  3.99361515e+00\n",
      "Epoch: 18712 mean train loss:  1.43792422e-05, mean val. loss:  3.99395680e+00\n",
      "Epoch: 18713 mean train loss:  1.44549995e-05, mean val. loss:  3.99435210e+00\n",
      "Epoch: 18714 mean train loss:  1.46690290e-05, mean val. loss:  3.99471998e+00\n",
      "Epoch: 18715 mean train loss:  1.45797967e-05, mean val. loss:  3.99511790e+00\n",
      "Epoch: 18716 mean train loss:  1.45131489e-05, mean val. loss:  3.99550748e+00\n",
      "Epoch: 18717 mean train loss:  1.45048834e-05, mean val. loss:  3.99587893e+00\n",
      "Epoch: 18718 mean train loss:  1.46328821e-05, mean val. loss:  3.99629235e+00\n",
      "Epoch: 18719 mean train loss:  1.46802922e-05, mean val. loss:  3.99667072e+00\n",
      "Epoch: 18720 mean train loss:  1.46591337e-05, mean val. loss:  3.99696827e+00\n",
      "Epoch: 18721 mean train loss:  1.43335201e-05, mean val. loss:  3.99719453e+00\n",
      "Epoch: 18722 mean train loss:  1.44814840e-05, mean val. loss:  3.99744058e+00\n",
      "Epoch: 18723 mean train loss:  1.44693768e-05, mean val. loss:  3.99764013e+00\n",
      "Epoch: 18724 mean train loss:  1.45638478e-05, mean val. loss:  3.99783659e+00\n",
      "Epoch: 18725 mean train loss:  1.44882069e-05, mean val. loss:  3.99803138e+00\n",
      "Epoch: 18726 mean train loss:  1.48259278e-05, mean val. loss:  3.99819565e+00\n",
      "Epoch: 18727 mean train loss:  1.46838720e-05, mean val. loss:  3.99837184e+00\n",
      "Epoch: 18728 mean train loss:  1.42598583e-05, mean val. loss:  3.99862838e+00\n",
      "Epoch: 18729 mean train loss:  1.46261882e-05, mean val. loss:  3.99891186e+00\n",
      "Epoch: 18730 mean train loss:  1.45892554e-05, mean val. loss:  3.99926114e+00\n",
      "Epoch: 18731 mean train loss:  1.46927487e-05, mean val. loss:  3.99953461e+00\n",
      "Epoch: 18732 mean train loss:  1.44045625e-05, mean val. loss:  3.99984026e+00\n",
      "Epoch: 18733 mean train loss:  1.46216189e-05, mean val. loss:  4.00024605e+00\n",
      "Epoch: 18734 mean train loss:  1.44972000e-05, mean val. loss:  4.00065947e+00\n",
      "Epoch: 18735 mean train loss:  1.43335492e-05, mean val. loss:  4.00115013e+00\n",
      "Epoch: 18736 mean train loss:  1.43212965e-05, mean val. loss:  4.00161219e+00\n",
      "Epoch: 18737 mean train loss:  1.45135273e-05, mean val. loss:  4.00215626e+00\n",
      "Epoch: 18738 mean train loss:  1.44094229e-05, mean val. loss:  4.00265217e+00\n",
      "Epoch: 18739 mean train loss:  1.45684462e-05, mean val. loss:  4.00312901e+00\n",
      "Epoch: 18740 mean train loss:  1.46961538e-05, mean val. loss:  4.00353432e+00\n",
      "Epoch: 18741 mean train loss:  1.45156810e-05, mean val. loss:  4.00393105e+00\n",
      "Epoch: 18742 mean train loss:  1.47926039e-05, mean val. loss:  4.00430107e+00\n",
      "Epoch: 18743 mean train loss:  1.46623061e-05, mean val. loss:  4.00464725e+00\n",
      "Epoch: 18744 mean train loss:  1.41651253e-05, mean val. loss:  4.00494909e+00\n",
      "Epoch: 18745 mean train loss:  1.43734214e-05, mean val. loss:  4.00525904e+00\n",
      "Epoch: 18746 mean train loss:  1.44314254e-05, mean val. loss:  4.00555706e+00\n",
      "Epoch: 18747 mean train loss:  1.45386730e-05, mean val. loss:  4.00587082e+00\n",
      "Epoch: 18748 mean train loss:  1.45572121e-05, mean val. loss:  4.00611115e+00\n",
      "Epoch: 18749 mean train loss:  1.47898973e-05, mean val. loss:  4.00624895e+00\n",
      "Epoch: 18750 mean train loss:  1.44071528e-05, mean val. loss:  4.00642347e+00\n",
      "Epoch: 18751 mean train loss:  1.45133527e-05, mean val. loss:  4.00664854e+00\n",
      "Epoch: 18752 mean train loss:  1.44482474e-05, mean val. loss:  4.00688791e+00\n",
      "Epoch: 18753 mean train loss:  1.47037790e-05, mean val. loss:  4.00706339e+00\n",
      "Epoch: 18754 mean train loss:  1.42767676e-05, mean val. loss:  4.00730228e+00\n",
      "Epoch: 18755 mean train loss:  1.45635859e-05, mean val. loss:  4.00749874e+00\n",
      "Epoch: 18756 mean train loss:  1.42898643e-05, mean val. loss:  4.00779009e+00\n",
      "Epoch: 18757 mean train loss:  1.44912919e-05, mean val. loss:  4.00807762e+00\n",
      "Epoch: 18758 mean train loss:  1.43881771e-05, mean val. loss:  4.00836515e+00\n",
      "Epoch: 18759 mean train loss:  1.45983067e-05, mean val. loss:  4.00856113e+00\n",
      "Epoch: 18760 mean train loss:  1.44201622e-05, mean val. loss:  4.00875521e+00\n",
      "Epoch: 18761 mean train loss:  1.44325313e-05, mean val. loss:  4.00895739e+00\n",
      "Epoch: 18762 mean train loss:  1.44914375e-05, mean val. loss:  4.00914001e+00\n",
      "Epoch: 18763 mean train loss:  1.48574763e-05, mean val. loss:  4.00928736e+00\n",
      "Epoch: 18764 mean train loss:  1.45168160e-05, mean val. loss:  4.00945807e+00\n",
      "Epoch: 18765 mean train loss:  1.44949590e-05, mean val. loss:  4.00961733e+00\n",
      "Epoch: 18766 mean train loss:  1.44249934e-05, mean val. loss:  4.00986099e+00\n",
      "Epoch: 18767 mean train loss:  1.43818907e-05, mean val. loss:  4.01014376e+00\n",
      "Epoch: 18768 mean train loss:  1.45645754e-05, mean val. loss:  4.01043606e+00\n",
      "Epoch: 18769 mean train loss:  1.47238316e-05, mean val. loss:  4.01074839e+00\n",
      "Epoch: 18770 mean train loss:  1.45069498e-05, mean val. loss:  4.01112843e+00\n",
      "Epoch: 18771 mean train loss:  1.43490324e-05, mean val. loss:  4.01153374e+00\n",
      "Epoch: 18772 mean train loss:  1.47674000e-05, mean val. loss:  4.01199102e+00\n",
      "Epoch: 18773 mean train loss:  1.46792445e-05, mean val. loss:  4.01242876e+00\n",
      "Epoch: 18774 mean train loss:  1.44306687e-05, mean val. loss:  4.01286364e+00\n",
      "Epoch: 18775 mean train loss:  1.45609083e-05, mean val. loss:  4.01334715e+00\n",
      "Epoch: 18776 mean train loss:  1.43251091e-05, mean val. loss:  4.01387787e+00\n",
      "Epoch: 18777 mean train loss:  1.45636441e-05, mean val. loss:  4.01441479e+00\n",
      "Epoch: 18778 mean train loss:  1.44499354e-05, mean val. loss:  4.01495981e+00\n",
      "Epoch: 18779 mean train loss:  1.43701036e-05, mean val. loss:  4.01548100e+00\n",
      "Epoch: 18780 mean train loss:  1.46555249e-05, mean val. loss:  4.01590776e+00\n",
      "Epoch: 18781 mean train loss:  1.43565703e-05, mean val. loss:  4.01628733e+00\n",
      "Epoch: 18782 mean train loss:  1.45116064e-05, mean val. loss:  4.01670313e+00\n",
      "Epoch: 18783 mean train loss:  1.44065998e-05, mean val. loss:  4.01709318e+00\n",
      "Epoch: 18784 mean train loss:  1.44415826e-05, mean val. loss:  4.01742554e+00\n",
      "Epoch: 18785 mean train loss:  1.43548823e-05, mean val. loss:  4.01771975e+00\n",
      "Epoch: 18786 mean train loss:  1.47417886e-05, mean val. loss:  4.01793098e+00\n",
      "Epoch: 18787 mean train loss:  1.45435042e-05, mean val. loss:  4.01806927e+00\n",
      "Epoch: 18788 mean train loss:  1.45198428e-05, mean val. loss:  4.01820040e+00\n",
      "Epoch: 18789 mean train loss:  1.45503436e-05, mean val. loss:  4.01829243e+00\n",
      "Epoch: 18790 mean train loss:  1.46508683e-05, mean val. loss:  4.01834679e+00\n",
      "Epoch: 18791 mean train loss:  1.46574457e-05, mean val. loss:  4.01836586e+00\n",
      "Epoch: 18792 mean train loss:  1.44910882e-05, mean val. loss:  4.01844263e+00\n",
      "Epoch: 18793 mean train loss:  1.46311650e-05, mean val. loss:  4.01847029e+00\n",
      "Epoch: 18794 mean train loss:  1.45237718e-05, mean val. loss:  4.01858187e+00\n",
      "Epoch: 18795 mean train loss:  1.44332589e-05, mean val. loss:  4.01868391e+00\n",
      "Epoch: 18796 mean train loss:  1.44200167e-05, mean val. loss:  4.01883745e+00\n",
      "Epoch: 18797 mean train loss:  1.45472877e-05, mean val. loss:  4.01900816e+00\n",
      "Epoch: 18798 mean train loss:  1.44375954e-05, mean val. loss:  4.01926088e+00\n",
      "Epoch: 18799 mean train loss:  1.45235972e-05, mean val. loss:  4.01956367e+00\n",
      "Epoch: 18800 mean train loss:  1.45755184e-05, mean val. loss:  4.01993513e+00\n",
      "Epoch: 18801 mean train loss:  1.43919024e-05, mean val. loss:  4.02033663e+00\n",
      "Epoch: 18802 mean train loss:  1.44308142e-05, mean val. loss:  4.02074051e+00\n",
      "Epoch: 18803 mean train loss:  1.43799116e-05, mean val. loss:  4.02122116e+00\n",
      "Epoch: 18804 mean train loss:  1.42197532e-05, mean val. loss:  4.02169514e+00\n",
      "Epoch: 18805 mean train loss:  1.46993552e-05, mean val. loss:  4.02210951e+00\n",
      "Epoch: 18806 mean train loss:  1.46246166e-05, mean val. loss:  4.02249002e+00\n",
      "Epoch: 18807 mean train loss:  1.45010999e-05, mean val. loss:  4.02286100e+00\n",
      "Epoch: 18808 mean train loss:  1.43648358e-05, mean val. loss:  4.02323437e+00\n",
      "Epoch: 18809 mean train loss:  1.45864324e-05, mean val. loss:  4.02356148e+00\n",
      "Epoch: 18810 mean train loss:  1.43588695e-05, mean val. loss:  4.02394009e+00\n",
      "Epoch: 18811 mean train loss:  1.46480161e-05, mean val. loss:  4.02431822e+00\n",
      "Epoch: 18812 mean train loss:  1.45648082e-05, mean val. loss:  4.02466393e+00\n",
      "Epoch: 18813 mean train loss:  1.44428050e-05, mean val. loss:  4.02502871e+00\n",
      "Epoch: 18814 mean train loss:  1.45804079e-05, mean val. loss:  4.02534103e+00\n",
      "Epoch: 18815 mean train loss:  1.43946381e-05, mean val. loss:  4.02569485e+00\n",
      "Epoch: 18816 mean train loss:  1.44704536e-05, mean val. loss:  4.02601957e+00\n",
      "Epoch: 18817 mean train loss:  1.46332895e-05, mean val. loss:  4.02628660e+00\n",
      "Epoch: 18818 mean train loss:  1.43311918e-05, mean val. loss:  4.02658319e+00\n",
      "Epoch: 18819 mean train loss:  1.44676596e-05, mean val. loss:  4.02684259e+00\n",
      "Epoch: 18820 mean train loss:  1.45139056e-05, mean val. loss:  4.02707672e+00\n",
      "Epoch: 18821 mean train loss:  1.45540107e-05, mean val. loss:  4.02723026e+00\n",
      "Epoch: 18822 mean train loss:  1.43744983e-05, mean val. loss:  4.02744102e+00\n",
      "Epoch: 18823 mean train loss:  1.45928061e-05, mean val. loss:  4.02761698e+00\n",
      "Epoch: 18824 mean train loss:  1.47901301e-05, mean val. loss:  4.02775288e+00\n",
      "Epoch: 18825 mean train loss:  1.47531973e-05, mean val. loss:  4.02784967e+00\n",
      "Epoch: 18826 mean train loss:  1.47093961e-05, mean val. loss:  4.02795696e+00\n",
      "Epoch: 18827 mean train loss:  1.43653597e-05, mean val. loss:  4.02806950e+00\n",
      "Epoch: 18828 mean train loss:  1.43350917e-05, mean val. loss:  4.02823734e+00\n",
      "Epoch: 18829 mean train loss:  1.43587822e-05, mean val. loss:  4.02847385e+00\n",
      "Epoch: 18830 mean train loss:  1.44137011e-05, mean val. loss:  4.02874851e+00\n",
      "Epoch: 18831 mean train loss:  1.46388775e-05, mean val. loss:  4.02899933e+00\n",
      "Epoch: 18832 mean train loss:  1.47005776e-05, mean val. loss:  4.02925777e+00\n",
      "Epoch: 18833 mean train loss:  1.44700170e-05, mean val. loss:  4.02955437e+00\n",
      "Epoch: 18834 mean train loss:  1.45485974e-05, mean val. loss:  4.02990580e+00\n",
      "Epoch: 18835 mean train loss:  1.43158832e-05, mean val. loss:  4.03031397e+00\n",
      "Epoch: 18836 mean train loss:  1.41901546e-05, mean val. loss:  4.03081846e+00\n",
      "Epoch: 18837 mean train loss:  1.46102102e-05, mean val. loss:  4.03131199e+00\n",
      "Epoch: 18838 mean train loss:  1.47323590e-05, mean val. loss:  4.03180981e+00\n",
      "Epoch: 18839 mean train loss:  1.46285456e-05, mean val. loss:  4.03236580e+00\n",
      "Epoch: 18840 mean train loss:  1.41620694e-05, mean val. loss:  4.03298473e+00\n",
      "Epoch: 18841 mean train loss:  1.43607322e-05, mean val. loss:  4.03362036e+00\n",
      "Epoch: 18842 mean train loss:  1.44745864e-05, mean val. loss:  4.03426743e+00\n",
      "Epoch: 18843 mean train loss:  1.46228704e-05, mean val. loss:  4.03482866e+00\n",
      "Epoch: 18844 mean train loss:  1.42967619e-05, mean val. loss:  4.03538609e+00\n",
      "Epoch: 18845 mean train loss:  1.44830847e-05, mean val. loss:  4.03583479e+00\n",
      "Epoch: 18846 mean train loss:  1.47359679e-05, mean val. loss:  4.03624678e+00\n",
      "Epoch: 18847 mean train loss:  1.47104147e-05, mean val. loss:  4.03663254e+00\n",
      "Epoch: 18848 mean train loss:  1.45102385e-05, mean val. loss:  4.03704023e+00\n",
      "Epoch: 18849 mean train loss:  1.44118676e-05, mean val. loss:  4.03734255e+00\n",
      "Epoch: 18850 mean train loss:  1.44015939e-05, mean val. loss:  4.03760767e+00\n",
      "Epoch: 18851 mean train loss:  1.42987119e-05, mean val. loss:  4.03793383e+00\n",
      "Epoch: 18852 mean train loss:  1.43373327e-05, mean val. loss:  4.03829908e+00\n",
      "Epoch: 18853 mean train loss:  1.44506339e-05, mean val. loss:  4.03859472e+00\n",
      "Epoch: 18854 mean train loss:  1.44437363e-05, mean val. loss:  4.03889894e+00\n",
      "Epoch: 18855 mean train loss:  1.44522928e-05, mean val. loss:  4.03924036e+00\n",
      "Epoch: 18856 mean train loss:  1.44875958e-05, mean val. loss:  4.03958082e+00\n",
      "Epoch: 18857 mean train loss:  1.46492966e-05, mean val. loss:  4.03997326e+00\n",
      "Epoch: 18858 mean train loss:  1.44986843e-05, mean val. loss:  4.04037189e+00\n",
      "Epoch: 18859 mean train loss:  1.44611695e-05, mean val. loss:  4.04076624e+00\n",
      "Epoch: 18860 mean train loss:  1.44920778e-05, mean val. loss:  4.04109001e+00\n",
      "Epoch: 18861 mean train loss:  1.46316015e-05, mean val. loss:  4.04139137e+00\n",
      "Epoch: 18862 mean train loss:  1.43304351e-05, mean val. loss:  4.04173279e+00\n",
      "Epoch: 18863 mean train loss:  1.42717618e-05, mean val. loss:  4.04210615e+00\n",
      "Epoch: 18864 mean train loss:  1.45352969e-05, mean val. loss:  4.04244328e+00\n",
      "Epoch: 18865 mean train loss:  1.46448438e-05, mean val. loss:  4.04270744e+00\n",
      "Epoch: 18866 mean train loss:  1.45383819e-05, mean val. loss:  4.04293728e+00\n",
      "Epoch: 18867 mean train loss:  1.47342216e-05, mean val. loss:  4.04315138e+00\n",
      "Epoch: 18868 mean train loss:  1.44835212e-05, mean val. loss:  4.04334736e+00\n",
      "Epoch: 18869 mean train loss:  1.45015074e-05, mean val. loss:  4.04351807e+00\n",
      "Epoch: 18870 mean train loss:  1.46177190e-05, mean val. loss:  4.04370070e+00\n",
      "Epoch: 18871 mean train loss:  1.43298530e-05, mean val. loss:  4.04397106e+00\n",
      "Epoch: 18872 mean train loss:  1.43540674e-05, mean val. loss:  4.04426146e+00\n",
      "Epoch: 18873 mean train loss:  1.45123049e-05, mean val. loss:  4.04455042e+00\n",
      "Epoch: 18874 mean train loss:  1.44673686e-05, mean val. loss:  4.04483366e+00\n",
      "Epoch: 18875 mean train loss:  1.46250241e-05, mean val. loss:  4.04509163e+00\n",
      "Epoch: 18876 mean train loss:  1.43410871e-05, mean val. loss:  4.04544163e+00\n",
      "Epoch: 18877 mean train loss:  1.44438527e-05, mean val. loss:  4.04581738e+00\n",
      "Epoch: 18878 mean train loss:  1.45558151e-05, mean val. loss:  4.04614496e+00\n",
      "Epoch: 18879 mean train loss:  1.45306694e-05, mean val. loss:  4.04653072e+00\n",
      "Epoch: 18880 mean train loss:  1.44584046e-05, mean val. loss:  4.04687500e+00\n",
      "Epoch: 18881 mean train loss:  1.44284568e-05, mean val. loss:  4.04718256e+00\n",
      "Epoch: 18882 mean train loss:  1.42973149e-05, mean val. loss:  4.04749346e+00\n",
      "Epoch: 18883 mean train loss:  1.42973440e-05, mean val. loss:  4.04780674e+00\n",
      "Epoch: 18884 mean train loss:  1.44902733e-05, mean val. loss:  4.04806328e+00\n",
      "Epoch: 18885 mean train loss:  1.44813675e-05, mean val. loss:  4.04823685e+00\n",
      "Epoch: 18886 mean train loss:  1.42986537e-05, mean val. loss:  4.04842854e+00\n",
      "Epoch: 18887 mean train loss:  1.44655351e-05, mean val. loss:  4.04857779e+00\n",
      "Epoch: 18888 mean train loss:  1.44950463e-05, mean val. loss:  4.04860878e+00\n",
      "Epoch: 18889 mean train loss:  1.44959486e-05, mean val. loss:  4.04865456e+00\n",
      "Epoch: 18890 mean train loss:  1.43757788e-05, mean val. loss:  4.04867697e+00\n",
      "Epoch: 18891 mean train loss:  1.45582599e-05, mean val. loss:  4.04871607e+00\n",
      "Epoch: 18892 mean train loss:  1.45040976e-05, mean val. loss:  4.04877520e+00\n",
      "Epoch: 18893 mean train loss:  1.43811922e-05, mean val. loss:  4.04884338e+00\n",
      "Epoch: 18894 mean train loss:  1.43362558e-05, mean val. loss:  4.04895544e+00\n",
      "Epoch: 18895 mean train loss:  1.44242076e-05, mean val. loss:  4.04914665e+00\n",
      "Epoch: 18896 mean train loss:  1.45146041e-05, mean val. loss:  4.04933023e+00\n",
      "Epoch: 18897 mean train loss:  1.46213861e-05, mean val. loss:  4.04948950e+00\n",
      "Epoch: 18898 mean train loss:  1.43969664e-05, mean val. loss:  4.04973745e+00\n",
      "Epoch: 18899 mean train loss:  1.47081446e-05, mean val. loss:  4.04998589e+00\n",
      "Epoch: 18900 mean train loss:  1.44779042e-05, mean val. loss:  4.05031109e+00\n",
      "Epoch: 18901 mean train loss:  1.46277307e-05, mean val. loss:  4.05058432e+00\n",
      "Epoch: 18902 mean train loss:  1.41765922e-05, mean val. loss:  4.05093908e+00\n",
      "Epoch: 18903 mean train loss:  1.45702506e-05, mean val. loss:  4.05133057e+00\n",
      "Epoch: 18904 mean train loss:  1.46040111e-05, mean val. loss:  4.05173969e+00\n",
      "Epoch: 18905 mean train loss:  1.42679492e-05, mean val. loss:  4.05220509e+00\n",
      "Epoch: 18906 mean train loss:  1.46555831e-05, mean val. loss:  4.05267572e+00\n",
      "Epoch: 18907 mean train loss:  1.43394573e-05, mean val. loss:  4.05320787e+00\n",
      "Epoch: 18908 mean train loss:  1.44929800e-05, mean val. loss:  4.05379200e+00\n",
      "Epoch: 18909 mean train loss:  1.46116654e-05, mean val. loss:  4.05436134e+00\n",
      "Epoch: 18910 mean train loss:  1.46472012e-05, mean val. loss:  4.05485344e+00\n",
      "Epoch: 18911 mean train loss:  1.46072707e-05, mean val. loss:  4.05529404e+00\n",
      "Epoch: 18912 mean train loss:  1.43616926e-05, mean val. loss:  4.05569077e+00\n",
      "Epoch: 18913 mean train loss:  1.45009253e-05, mean val. loss:  4.05608559e+00\n",
      "Epoch: 18914 mean train loss:  1.45346276e-05, mean val. loss:  4.05648851e+00\n",
      "Epoch: 18915 mean train loss:  1.47444662e-05, mean val. loss:  4.05679369e+00\n",
      "Epoch: 18916 mean train loss:  1.43962097e-05, mean val. loss:  4.05708599e+00\n",
      "Epoch: 18917 mean train loss:  1.43620418e-05, mean val. loss:  4.05741024e+00\n",
      "Epoch: 18918 mean train loss:  1.45823869e-05, mean val. loss:  4.05772209e+00\n",
      "Epoch: 18919 mean train loss:  1.44338992e-05, mean val. loss:  4.05801439e+00\n",
      "Epoch: 18920 mean train loss:  1.47141400e-05, mean val. loss:  4.05823994e+00\n",
      "Epoch: 18921 mean train loss:  1.45172235e-05, mean val. loss:  4.05844831e+00\n",
      "Epoch: 18922 mean train loss:  1.46551465e-05, mean val. loss:  4.05860901e+00\n",
      "Epoch: 18923 mean train loss:  1.45726954e-05, mean val. loss:  4.05879927e+00\n",
      "Epoch: 18924 mean train loss:  1.41088385e-05, mean val. loss:  4.05900812e+00\n",
      "Epoch: 18925 mean train loss:  1.44072110e-05, mean val. loss:  4.05926037e+00\n",
      "Epoch: 18926 mean train loss:  1.43604120e-05, mean val. loss:  4.05953693e+00\n",
      "Epoch: 18927 mean train loss:  1.43897487e-05, mean val. loss:  4.05981636e+00\n",
      "Epoch: 18928 mean train loss:  1.45459198e-05, mean val. loss:  4.06010723e+00\n",
      "Epoch: 18929 mean train loss:  1.44670485e-05, mean val. loss:  4.06039858e+00\n",
      "Epoch: 18930 mean train loss:  1.45374797e-05, mean val. loss:  4.06072283e+00\n",
      "Epoch: 18931 mean train loss:  1.44736550e-05, mean val. loss:  4.06105661e+00\n",
      "Epoch: 18932 mean train loss:  1.41941709e-05, mean val. loss:  4.06150198e+00\n",
      "Epoch: 18933 mean train loss:  1.45423692e-05, mean val. loss:  4.06192636e+00\n",
      "Epoch: 18934 mean train loss:  1.41232449e-05, mean val. loss:  4.06245613e+00\n",
      "Epoch: 18935 mean train loss:  1.43606449e-05, mean val. loss:  4.06299067e+00\n",
      "Epoch: 18936 mean train loss:  1.44459482e-05, mean val. loss:  4.06355381e+00\n",
      "Epoch: 18937 mean train loss:  1.42348581e-05, mean val. loss:  4.06411934e+00\n",
      "Epoch: 18938 mean train loss:  1.42441713e-05, mean val. loss:  4.06464386e+00\n",
      "Epoch: 18939 mean train loss:  1.45570957e-05, mean val. loss:  4.06515837e+00\n",
      "Epoch: 18940 mean train loss:  1.43089856e-05, mean val. loss:  4.06561756e+00\n",
      "Epoch: 18941 mean train loss:  1.44088408e-05, mean val. loss:  4.06616020e+00\n",
      "Epoch: 18942 mean train loss:  1.44618971e-05, mean val. loss:  4.06664419e+00\n",
      "Epoch: 18943 mean train loss:  1.41939963e-05, mean val. loss:  4.06719255e+00\n",
      "Epoch: 18944 mean train loss:  1.46115781e-05, mean val. loss:  4.06765461e+00\n",
      "Epoch: 18945 mean train loss:  1.45730155e-05, mean val. loss:  4.06810522e+00\n",
      "Epoch: 18946 mean train loss:  1.45354134e-05, mean val. loss:  4.06850481e+00\n",
      "Epoch: 18947 mean train loss:  1.44439982e-05, mean val. loss:  4.06893301e+00\n",
      "Epoch: 18948 mean train loss:  1.45467056e-05, mean val. loss:  4.06933355e+00\n",
      "Epoch: 18949 mean train loss:  1.45395461e-05, mean val. loss:  4.06969833e+00\n",
      "Epoch: 18950 mean train loss:  1.47161772e-05, mean val. loss:  4.06998539e+00\n",
      "Epoch: 18951 mean train loss:  1.42331701e-05, mean val. loss:  4.07029200e+00\n",
      "Epoch: 18952 mean train loss:  1.44109654e-05, mean val. loss:  4.07056999e+00\n",
      "Epoch: 18953 mean train loss:  1.41780765e-05, mean val. loss:  4.07090473e+00\n",
      "Epoch: 18954 mean train loss:  1.45967933e-05, mean val. loss:  4.07122421e+00\n",
      "Epoch: 18955 mean train loss:  1.43601501e-05, mean val. loss:  4.07149839e+00\n",
      "Epoch: 18956 mean train loss:  1.45431550e-05, mean val. loss:  4.07173586e+00\n",
      "Epoch: 18957 mean train loss:  1.43636134e-05, mean val. loss:  4.07189560e+00\n",
      "Epoch: 18958 mean train loss:  1.43194920e-05, mean val. loss:  4.07206106e+00\n",
      "Epoch: 18959 mean train loss:  1.43795332e-05, mean val. loss:  4.07228899e+00\n",
      "Epoch: 18960 mean train loss:  1.46259554e-05, mean val. loss:  4.07250929e+00\n",
      "Epoch: 18961 mean train loss:  1.42124481e-05, mean val. loss:  4.07267475e+00\n",
      "Epoch: 18962 mean train loss:  1.43715297e-05, mean val. loss:  4.07285595e+00\n",
      "Epoch: 18963 mean train loss:  1.44607329e-05, mean val. loss:  4.07290554e+00\n",
      "Epoch: 18964 mean train loss:  1.44675432e-05, mean val. loss:  4.07303476e+00\n",
      "Epoch: 18965 mean train loss:  1.47696119e-05, mean val. loss:  4.07309532e+00\n",
      "Epoch: 18966 mean train loss:  1.46086677e-05, mean val. loss:  4.07313776e+00\n",
      "Epoch: 18967 mean train loss:  1.46306120e-05, mean val. loss:  4.07312155e+00\n",
      "Epoch: 18968 mean train loss:  1.43880025e-05, mean val. loss:  4.07312584e+00\n",
      "Epoch: 18969 mean train loss:  1.43729267e-05, mean val. loss:  4.07322407e+00\n",
      "Epoch: 18970 mean train loss:  1.45319209e-05, mean val. loss:  4.07339096e+00\n",
      "Epoch: 18971 mean train loss:  1.45733648e-05, mean val. loss:  4.07355118e+00\n",
      "Epoch: 18972 mean train loss:  1.43232755e-05, mean val. loss:  4.07378054e+00\n",
      "Epoch: 18973 mean train loss:  1.43594516e-05, mean val. loss:  4.07401180e+00\n",
      "Epoch: 18974 mean train loss:  1.40571792e-05, mean val. loss:  4.07428217e+00\n",
      "Epoch: 18975 mean train loss:  1.43211510e-05, mean val. loss:  4.07460403e+00\n",
      "Epoch: 18976 mean train loss:  1.43155630e-05, mean val. loss:  4.07495070e+00\n",
      "Epoch: 18977 mean train loss:  1.43173093e-05, mean val. loss:  4.07528305e+00\n",
      "Epoch: 18978 mean train loss:  1.44655933e-05, mean val. loss:  4.07563400e+00\n",
      "Epoch: 18979 mean train loss:  1.45207450e-05, mean val. loss:  4.07599640e+00\n",
      "Epoch: 18980 mean train loss:  1.45446393e-05, mean val. loss:  4.07636786e+00\n",
      "Epoch: 18981 mean train loss:  1.45691447e-05, mean val. loss:  4.07675982e+00\n",
      "Epoch: 18982 mean train loss:  1.44794467e-05, mean val. loss:  4.07711363e+00\n",
      "Epoch: 18983 mean train loss:  1.44038640e-05, mean val. loss:  4.07747698e+00\n",
      "Epoch: 18984 mean train loss:  1.43597135e-05, mean val. loss:  4.07781649e+00\n",
      "Epoch: 18985 mean train loss:  1.45692611e-05, mean val. loss:  4.07812309e+00\n",
      "Epoch: 18986 mean train loss:  1.46047678e-05, mean val. loss:  4.07832956e+00\n",
      "Epoch: 18987 mean train loss:  1.43055804e-05, mean val. loss:  4.07852888e+00\n",
      "Epoch: 18988 mean train loss:  1.43832585e-05, mean val. loss:  4.07875395e+00\n",
      "Epoch: 18989 mean train loss:  1.44086080e-05, mean val. loss:  4.07897472e+00\n",
      "Epoch: 18990 mean train loss:  1.42591889e-05, mean val. loss:  4.07919359e+00\n",
      "Epoch: 18991 mean train loss:  1.45319500e-05, mean val. loss:  4.07943821e+00\n",
      "Epoch: 18992 mean train loss:  1.43686193e-05, mean val. loss:  4.07971048e+00\n",
      "Epoch: 18993 mean train loss:  1.45549129e-05, mean val. loss:  4.08000994e+00\n",
      "Epoch: 18994 mean train loss:  1.45084923e-05, mean val. loss:  4.08031082e+00\n",
      "Epoch: 18995 mean train loss:  1.44287478e-05, mean val. loss:  4.08069563e+00\n",
      "Epoch: 18996 mean train loss:  1.46464445e-05, mean val. loss:  4.08110046e+00\n",
      "Epoch: 18997 mean train loss:  1.41616038e-05, mean val. loss:  4.08154678e+00\n",
      "Epoch: 18998 mean train loss:  1.45317463e-05, mean val. loss:  4.08200836e+00\n",
      "Epoch: 18999 mean train loss:  1.44566875e-05, mean val. loss:  4.08244181e+00\n",
      "Epoch: 19000 mean train loss:  1.42912322e-05, mean val. loss:  4.08290911e+00\n",
      "Epoch: 19001 mean train loss:  1.44737423e-05, mean val. loss:  4.08342695e+00\n",
      "Epoch: 19002 mean train loss:  1.43265061e-05, mean val. loss:  4.08390760e+00\n",
      "Epoch: 19003 mean train loss:  1.45341910e-05, mean val. loss:  4.08430719e+00\n",
      "Epoch: 19004 mean train loss:  1.41524943e-05, mean val. loss:  4.08471346e+00\n",
      "Epoch: 19005 mean train loss:  1.42663775e-05, mean val. loss:  4.08512497e+00\n",
      "Epoch: 19006 mean train loss:  1.42247300e-05, mean val. loss:  4.08555603e+00\n",
      "Epoch: 19007 mean train loss:  1.46116363e-05, mean val. loss:  4.08589363e+00\n",
      "Epoch: 19008 mean train loss:  1.44658552e-05, mean val. loss:  4.08622694e+00\n",
      "Epoch: 19009 mean train loss:  1.45201047e-05, mean val. loss:  4.08652782e+00\n",
      "Epoch: 19010 mean train loss:  1.40267948e-05, mean val. loss:  4.08684540e+00\n",
      "Epoch: 19011 mean train loss:  1.42837234e-05, mean val. loss:  4.08714247e+00\n",
      "Epoch: 19012 mean train loss:  1.44091900e-05, mean val. loss:  4.08748817e+00\n",
      "Epoch: 19013 mean train loss:  1.44295627e-05, mean val. loss:  4.08780766e+00\n",
      "Epoch: 19014 mean train loss:  1.44227815e-05, mean val. loss:  4.08814478e+00\n",
      "Epoch: 19015 mean train loss:  1.42648350e-05, mean val. loss:  4.08850622e+00\n",
      "Epoch: 19016 mean train loss:  1.44925434e-05, mean val. loss:  4.08888817e+00\n",
      "Epoch: 19017 mean train loss:  1.44600053e-05, mean val. loss:  4.08921957e+00\n",
      "Epoch: 19018 mean train loss:  1.42971112e-05, mean val. loss:  4.08960056e+00\n",
      "Epoch: 19019 mean train loss:  1.43597135e-05, mean val. loss:  4.08997631e+00\n",
      "Epoch: 19020 mean train loss:  1.44331425e-05, mean val. loss:  4.09034300e+00\n",
      "Epoch: 19021 mean train loss:  1.43671641e-05, mean val. loss:  4.09068012e+00\n",
      "Epoch: 19022 mean train loss:  1.45972590e-05, mean val. loss:  4.09094429e+00\n",
      "Epoch: 19023 mean train loss:  1.44720252e-05, mean val. loss:  4.09114838e+00\n",
      "Epoch: 19024 mean train loss:  1.44578516e-05, mean val. loss:  4.09133053e+00\n",
      "Epoch: 19025 mean train loss:  1.44350342e-05, mean val. loss:  4.09143496e+00\n",
      "Epoch: 19026 mean train loss:  1.42899225e-05, mean val. loss:  4.09159946e+00\n",
      "Epoch: 19027 mean train loss:  1.45310478e-05, mean val. loss:  4.09172010e+00\n",
      "Epoch: 19028 mean train loss:  1.44529331e-05, mean val. loss:  4.09178114e+00\n",
      "Epoch: 19029 mean train loss:  1.43941434e-05, mean val. loss:  4.09184980e+00\n",
      "Epoch: 19030 mean train loss:  1.44026417e-05, mean val. loss:  4.09191132e+00\n",
      "Epoch: 19031 mean train loss:  1.44971127e-05, mean val. loss:  4.09192228e+00\n",
      "Epoch: 19032 mean train loss:  1.43993821e-05, mean val. loss:  4.09194279e+00\n",
      "Epoch: 19033 mean train loss:  1.42549106e-05, mean val. loss:  4.09202290e+00\n",
      "Epoch: 19034 mean train loss:  1.42539211e-05, mean val. loss:  4.09218025e+00\n",
      "Epoch: 19035 mean train loss:  1.40902703e-05, mean val. loss:  4.09246445e+00\n",
      "Epoch: 19036 mean train loss:  1.40554330e-05, mean val. loss:  4.09291315e+00\n",
      "Epoch: 19037 mean train loss:  1.45127415e-05, mean val. loss:  4.09338713e+00\n",
      "Epoch: 19038 mean train loss:  1.41647179e-05, mean val. loss:  4.09393358e+00\n",
      "Epoch: 19039 mean train loss:  1.40784541e-05, mean val. loss:  4.09455824e+00\n",
      "Epoch: 19040 mean train loss:  1.46654784e-05, mean val. loss:  4.09519768e+00\n",
      "Epoch: 19041 mean train loss:  1.44459773e-05, mean val. loss:  4.09579420e+00\n",
      "Epoch: 19042 mean train loss:  1.42597419e-05, mean val. loss:  4.09648657e+00\n",
      "Epoch: 19043 mean train loss:  1.42153876e-05, mean val. loss:  4.09718943e+00\n",
      "Epoch: 19044 mean train loss:  1.43618381e-05, mean val. loss:  4.09787893e+00\n",
      "Epoch: 19045 mean train loss:  1.43625657e-05, mean val. loss:  4.09849501e+00\n",
      "Epoch: 19046 mean train loss:  1.44160003e-05, mean val. loss:  4.09899139e+00\n",
      "Epoch: 19047 mean train loss:  1.45756057e-05, mean val. loss:  4.09934759e+00\n",
      "Epoch: 19048 mean train loss:  1.43845682e-05, mean val. loss:  4.09961557e+00\n",
      "Epoch: 19049 mean train loss:  1.45669328e-05, mean val. loss:  4.09982824e+00\n",
      "Epoch: 19050 mean train loss:  1.43880607e-05, mean val. loss:  4.10006237e+00\n",
      "Epoch: 19051 mean train loss:  1.45301165e-05, mean val. loss:  4.10021257e+00\n",
      "Epoch: 19052 mean train loss:  1.42795325e-05, mean val. loss:  4.10032845e+00\n",
      "Epoch: 19053 mean train loss:  1.40362827e-05, mean val. loss:  4.10051107e+00\n",
      "Epoch: 19054 mean train loss:  1.47095125e-05, mean val. loss:  4.10063362e+00\n",
      "Epoch: 19055 mean train loss:  1.42600620e-05, mean val. loss:  4.10086060e+00\n",
      "Epoch: 19056 mean train loss:  1.46096863e-05, mean val. loss:  4.10100985e+00\n",
      "Epoch: 19057 mean train loss:  1.42811623e-05, mean val. loss:  4.10123014e+00\n",
      "Epoch: 19058 mean train loss:  1.46589591e-05, mean val. loss:  4.10143566e+00\n",
      "Epoch: 19059 mean train loss:  1.44857331e-05, mean val. loss:  4.10161161e+00\n",
      "Epoch: 19060 mean train loss:  1.42771751e-05, mean val. loss:  4.10182667e+00\n",
      "Epoch: 19061 mean train loss:  1.42554054e-05, mean val. loss:  4.10206509e+00\n",
      "Epoch: 19062 mean train loss:  1.43019133e-05, mean val. loss:  4.10224342e+00\n",
      "Epoch: 19063 mean train loss:  1.41719647e-05, mean val. loss:  4.10247564e+00\n",
      "Epoch: 19064 mean train loss:  1.43843063e-05, mean val. loss:  4.10271740e+00\n",
      "Epoch: 19065 mean train loss:  1.42428908e-05, mean val. loss:  4.10301638e+00\n",
      "Epoch: 19066 mean train loss:  1.43285724e-05, mean val. loss:  4.10334253e+00\n",
      "Epoch: 19067 mean train loss:  1.43132347e-05, mean val. loss:  4.10369444e+00\n",
      "Epoch: 19068 mean train loss:  1.45057274e-05, mean val. loss:  4.10398054e+00\n",
      "Epoch: 19069 mean train loss:  1.44613150e-05, mean val. loss:  4.10426998e+00\n",
      "Epoch: 19070 mean train loss:  1.43217330e-05, mean val. loss:  4.10459948e+00\n",
      "Epoch: 19071 mean train loss:  1.44192891e-05, mean val. loss:  4.10493565e+00\n",
      "Epoch: 19072 mean train loss:  1.44955702e-05, mean val. loss:  4.10517740e+00\n",
      "Epoch: 19073 mean train loss:  1.42731587e-05, mean val. loss:  4.10546398e+00\n",
      "Epoch: 19074 mean train loss:  1.43831130e-05, mean val. loss:  4.10575151e+00\n",
      "Epoch: 19075 mean train loss:  1.43285724e-05, mean val. loss:  4.10602856e+00\n",
      "Epoch: 19076 mean train loss:  1.45267695e-05, mean val. loss:  4.10623741e+00\n",
      "Epoch: 19077 mean train loss:  1.44783116e-05, mean val. loss:  4.10645533e+00\n",
      "Epoch: 19078 mean train loss:  1.44410587e-05, mean val. loss:  4.10673904e+00\n",
      "Epoch: 19079 mean train loss:  1.42397475e-05, mean val. loss:  4.10708237e+00\n",
      "Epoch: 19080 mean train loss:  1.43932994e-05, mean val. loss:  4.10743475e+00\n",
      "Epoch: 19081 mean train loss:  1.42954523e-05, mean val. loss:  4.10777760e+00\n",
      "Epoch: 19082 mean train loss:  1.42783392e-05, mean val. loss:  4.10818195e+00\n",
      "Epoch: 19083 mean train loss:  1.41730416e-05, mean val. loss:  4.10858440e+00\n",
      "Epoch: 19084 mean train loss:  1.40723423e-05, mean val. loss:  4.10896921e+00\n",
      "Epoch: 19085 mean train loss:  1.45294471e-05, mean val. loss:  4.10925961e+00\n",
      "Epoch: 19086 mean train loss:  1.46342209e-05, mean val. loss:  4.10948133e+00\n",
      "Epoch: 19087 mean train loss:  1.42608187e-05, mean val. loss:  4.10970783e+00\n",
      "Epoch: 19088 mean train loss:  1.42823847e-05, mean val. loss:  4.10995770e+00\n",
      "Epoch: 19089 mean train loss:  1.42609351e-05, mean val. loss:  4.11027336e+00\n",
      "Epoch: 19090 mean train loss:  1.42704812e-05, mean val. loss:  4.11056995e+00\n",
      "Epoch: 19091 mean train loss:  1.46201637e-05, mean val. loss:  4.11075401e+00\n",
      "Epoch: 19092 mean train loss:  1.42400386e-05, mean val. loss:  4.11099911e+00\n",
      "Epoch: 19093 mean train loss:  1.45577360e-05, mean val. loss:  4.11120892e+00\n",
      "Epoch: 19094 mean train loss:  1.43624493e-05, mean val. loss:  4.11145210e+00\n",
      "Epoch: 19095 mean train loss:  1.44937076e-05, mean val. loss:  4.11165714e+00\n",
      "Epoch: 19096 mean train loss:  1.42593053e-05, mean val. loss:  4.11183214e+00\n",
      "Epoch: 19097 mean train loss:  1.42552017e-05, mean val. loss:  4.11201763e+00\n",
      "Epoch: 19098 mean train loss:  1.44262158e-05, mean val. loss:  4.11216927e+00\n",
      "Epoch: 19099 mean train loss:  1.45580270e-05, mean val. loss:  4.11227655e+00\n",
      "Epoch: 19100 mean train loss:  1.43895159e-05, mean val. loss:  4.11240149e+00\n",
      "Epoch: 19101 mean train loss:  1.43476063e-05, mean val. loss:  4.11251497e+00\n",
      "Epoch: 19102 mean train loss:  1.43196667e-05, mean val. loss:  4.11257887e+00\n",
      "Epoch: 19103 mean train loss:  1.43707730e-05, mean val. loss:  4.11267376e+00\n",
      "Epoch: 19104 mean train loss:  1.42676872e-05, mean val. loss:  4.11281395e+00\n",
      "Epoch: 19105 mean train loss:  1.44179212e-05, mean val. loss:  4.11296844e+00\n",
      "Epoch: 19106 mean train loss:  1.44545047e-05, mean val. loss:  4.11309910e+00\n",
      "Epoch: 19107 mean train loss:  1.43648940e-05, mean val. loss:  4.11327648e+00\n",
      "Epoch: 19108 mean train loss:  1.45109370e-05, mean val. loss:  4.11349440e+00\n",
      "Epoch: 19109 mean train loss:  1.41929777e-05, mean val. loss:  4.11375713e+00\n",
      "Epoch: 19110 mean train loss:  1.43161451e-05, mean val. loss:  4.11399221e+00\n",
      "Epoch: 19111 mean train loss:  1.40620978e-05, mean val. loss:  4.11431503e+00\n",
      "Epoch: 19112 mean train loss:  1.44029036e-05, mean val. loss:  4.11466074e+00\n",
      "Epoch: 19113 mean train loss:  1.44099467e-05, mean val. loss:  4.11505175e+00\n",
      "Epoch: 19114 mean train loss:  1.43307843e-05, mean val. loss:  4.11544323e+00\n",
      "Epoch: 19115 mean train loss:  1.43083744e-05, mean val. loss:  4.11578035e+00\n",
      "Epoch: 19116 mean train loss:  1.44178048e-05, mean val. loss:  4.11610937e+00\n",
      "Epoch: 19117 mean train loss:  1.45553495e-05, mean val. loss:  4.11644840e+00\n",
      "Epoch: 19118 mean train loss:  1.41949567e-05, mean val. loss:  4.11679792e+00\n",
      "Epoch: 19119 mean train loss:  1.43509824e-05, mean val. loss:  4.11716843e+00\n",
      "Epoch: 19120 mean train loss:  1.40393968e-05, mean val. loss:  4.11763239e+00\n",
      "Epoch: 19121 mean train loss:  1.42569479e-05, mean val. loss:  4.11804342e+00\n",
      "Epoch: 19122 mean train loss:  1.44433579e-05, mean val. loss:  4.11852789e+00\n",
      "Epoch: 19123 mean train loss:  1.43875368e-05, mean val. loss:  4.11899519e+00\n",
      "Epoch: 19124 mean train loss:  1.42064819e-05, mean val. loss:  4.11948776e+00\n",
      "Epoch: 19125 mean train loss:  1.43145735e-05, mean val. loss:  4.11995506e+00\n",
      "Epoch: 19126 mean train loss:  1.43354409e-05, mean val. loss:  4.12045431e+00\n",
      "Epoch: 19127 mean train loss:  1.42780191e-05, mean val. loss:  4.12094164e+00\n",
      "Epoch: 19128 mean train loss:  1.44666410e-05, mean val. loss:  4.12132978e+00\n",
      "Epoch: 19129 mean train loss:  1.43484212e-05, mean val. loss:  4.12172270e+00\n",
      "Epoch: 19130 mean train loss:  1.42630597e-05, mean val. loss:  4.12210274e+00\n",
      "Epoch: 19131 mean train loss:  1.44740625e-05, mean val. loss:  4.12239981e+00\n",
      "Epoch: 19132 mean train loss:  1.44552905e-05, mean val. loss:  4.12272644e+00\n",
      "Epoch: 19133 mean train loss:  1.41696655e-05, mean val. loss:  4.12310314e+00\n",
      "Epoch: 19134 mean train loss:  1.43994403e-05, mean val. loss:  4.12342548e+00\n",
      "Epoch: 19135 mean train loss:  1.41696364e-05, mean val. loss:  4.12384844e+00\n",
      "Epoch: 19136 mean train loss:  1.42710342e-05, mean val. loss:  4.12431765e+00\n",
      "Epoch: 19137 mean train loss:  1.41817436e-05, mean val. loss:  4.12474489e+00\n",
      "Epoch: 19138 mean train loss:  1.45237136e-05, mean val. loss:  4.12513113e+00\n",
      "Epoch: 19139 mean train loss:  1.44120713e-05, mean val. loss:  4.12545490e+00\n",
      "Epoch: 19140 mean train loss:  1.43690268e-05, mean val. loss:  4.12576199e+00\n",
      "Epoch: 19141 mean train loss:  1.40922784e-05, mean val. loss:  4.12607002e+00\n",
      "Epoch: 19142 mean train loss:  1.44004298e-05, mean val. loss:  4.12635517e+00\n",
      "Epoch: 19143 mean train loss:  1.43809593e-05, mean val. loss:  4.12658596e+00\n",
      "Epoch: 19144 mean train loss:  1.45566009e-05, mean val. loss:  4.12677288e+00\n",
      "Epoch: 19145 mean train loss:  1.44025253e-05, mean val. loss:  4.12696838e+00\n",
      "Epoch: 19146 mean train loss:  1.41405035e-05, mean val. loss:  4.12722111e+00\n",
      "Epoch: 19147 mean train loss:  1.41756318e-05, mean val. loss:  4.12752485e+00\n",
      "Epoch: 19148 mean train loss:  1.42526696e-05, mean val. loss:  4.12787485e+00\n",
      "Epoch: 19149 mean train loss:  1.43911457e-05, mean val. loss:  4.12815905e+00\n",
      "Epoch: 19150 mean train loss:  1.43098878e-05, mean val. loss:  4.12843990e+00\n",
      "Epoch: 19151 mean train loss:  1.43664365e-05, mean val. loss:  4.12877846e+00\n",
      "Epoch: 19152 mean train loss:  1.42448698e-05, mean val. loss:  4.12914515e+00\n",
      "Epoch: 19153 mean train loss:  1.42184144e-05, mean val. loss:  4.12954378e+00\n",
      "Epoch: 19154 mean train loss:  1.42295612e-05, mean val. loss:  4.12999201e+00\n",
      "Epoch: 19155 mean train loss:  1.42156787e-05, mean val. loss:  4.13046598e+00\n",
      "Epoch: 19156 mean train loss:  1.41454511e-05, mean val. loss:  4.13097525e+00\n",
      "Epoch: 19157 mean train loss:  1.43785728e-05, mean val. loss:  4.13147545e+00\n",
      "Epoch: 19158 mean train loss:  1.44851510e-05, mean val. loss:  4.13193560e+00\n",
      "Epoch: 19159 mean train loss:  1.43693178e-05, mean val. loss:  4.13241196e+00\n",
      "Epoch: 19160 mean train loss:  1.40359334e-05, mean val. loss:  4.13292789e+00\n",
      "Epoch: 19161 mean train loss:  1.44272926e-05, mean val. loss:  4.13333225e+00\n",
      "Epoch: 19162 mean train loss:  1.42692588e-05, mean val. loss:  4.13374376e+00\n",
      "Epoch: 19163 mean train loss:  1.40076736e-05, mean val. loss:  4.13420200e+00\n",
      "Epoch: 19164 mean train loss:  1.41922792e-05, mean val. loss:  4.13469648e+00\n",
      "Epoch: 19165 mean train loss:  1.44527585e-05, mean val. loss:  4.13518524e+00\n",
      "Epoch: 19166 mean train loss:  1.40228658e-05, mean val. loss:  4.13573551e+00\n",
      "Epoch: 19167 mean train loss:  1.46255770e-05, mean val. loss:  4.13622904e+00\n",
      "Epoch: 19168 mean train loss:  1.42101198e-05, mean val. loss:  4.13669062e+00\n",
      "Epoch: 19169 mean train loss:  1.44327059e-05, mean val. loss:  4.13707209e+00\n",
      "Epoch: 19170 mean train loss:  1.41201890e-05, mean val. loss:  4.13748217e+00\n",
      "Epoch: 19171 mean train loss:  1.45364029e-05, mean val. loss:  4.13781023e+00\n",
      "Epoch: 19172 mean train loss:  1.42136123e-05, mean val. loss:  4.13806868e+00\n",
      "Epoch: 19173 mean train loss:  1.43799989e-05, mean val. loss:  4.13830853e+00\n",
      "Epoch: 19174 mean train loss:  1.42288918e-05, mean val. loss:  4.13850546e+00\n",
      "Epoch: 19175 mean train loss:  1.44301739e-05, mean val. loss:  4.13870239e+00\n",
      "Epoch: 19176 mean train loss:  1.43092766e-05, mean val. loss:  4.13890266e+00\n",
      "Epoch: 19177 mean train loss:  1.45364611e-05, mean val. loss:  4.13899803e+00\n",
      "Epoch: 19178 mean train loss:  1.39780750e-05, mean val. loss:  4.13917828e+00\n",
      "Epoch: 19179 mean train loss:  1.42661447e-05, mean val. loss:  4.13934374e+00\n",
      "Epoch: 19180 mean train loss:  1.43268844e-05, mean val. loss:  4.13951778e+00\n",
      "Epoch: 19181 mean train loss:  1.43498182e-05, mean val. loss:  4.13968134e+00\n",
      "Epoch: 19182 mean train loss:  1.43837242e-05, mean val. loss:  4.13979006e+00\n",
      "Epoch: 19183 mean train loss:  1.40288030e-05, mean val. loss:  4.13995504e+00\n",
      "Epoch: 19184 mean train loss:  1.40250777e-05, mean val. loss:  4.14022303e+00\n",
      "Epoch: 19185 mean train loss:  1.40084594e-05, mean val. loss:  4.14059401e+00\n",
      "Epoch: 19186 mean train loss:  1.46139937e-05, mean val. loss:  4.14091778e+00\n",
      "Epoch: 19187 mean train loss:  1.44141086e-05, mean val. loss:  4.14127970e+00\n",
      "Epoch: 19188 mean train loss:  1.42163481e-05, mean val. loss:  4.14160156e+00\n",
      "Epoch: 19189 mean train loss:  1.42906792e-05, mean val. loss:  4.14196301e+00\n",
      "Epoch: 19190 mean train loss:  1.42268545e-05, mean val. loss:  4.14237309e+00\n",
      "Epoch: 19191 mean train loss:  1.42146309e-05, mean val. loss:  4.14277363e+00\n",
      "Epoch: 19192 mean train loss:  1.44269725e-05, mean val. loss:  4.14300680e+00\n",
      "Epoch: 19193 mean train loss:  1.42022036e-05, mean val. loss:  4.14325571e+00\n",
      "Epoch: 19194 mean train loss:  1.42514473e-05, mean val. loss:  4.14350128e+00\n",
      "Epoch: 19195 mean train loss:  1.43329962e-05, mean val. loss:  4.14370775e+00\n",
      "Epoch: 19196 mean train loss:  1.40469347e-05, mean val. loss:  4.14388943e+00\n",
      "Epoch: 19197 mean train loss:  1.42567442e-05, mean val. loss:  4.14401817e+00\n",
      "Epoch: 19198 mean train loss:  1.42613717e-05, mean val. loss:  4.14416552e+00\n",
      "Epoch: 19199 mean train loss:  1.44758669e-05, mean val. loss:  4.14424610e+00\n",
      "Epoch: 19200 mean train loss:  1.42489152e-05, mean val. loss:  4.14436865e+00\n",
      "Epoch: 19201 mean train loss:  1.42254576e-05, mean val. loss:  4.14447451e+00\n",
      "Epoch: 19202 mean train loss:  1.42864301e-05, mean val. loss:  4.14458132e+00\n",
      "Epoch: 19203 mean train loss:  1.42566860e-05, mean val. loss:  4.14479160e+00\n",
      "Epoch: 19204 mean train loss:  1.43327925e-05, mean val. loss:  4.14502048e+00\n",
      "Epoch: 19205 mean train loss:  1.39124459e-05, mean val. loss:  4.14537430e+00\n",
      "Epoch: 19206 mean train loss:  1.46500242e-05, mean val. loss:  4.14567041e+00\n",
      "Epoch: 19207 mean train loss:  1.43714715e-05, mean val. loss:  4.14589071e+00\n",
      "Epoch: 19208 mean train loss:  1.43234211e-05, mean val. loss:  4.14617443e+00\n",
      "Epoch: 19209 mean train loss:  1.42451026e-05, mean val. loss:  4.14645576e+00\n",
      "Epoch: 19210 mean train loss:  1.43133802e-05, mean val. loss:  4.14668131e+00\n",
      "Epoch: 19211 mean train loss:  1.41674827e-05, mean val. loss:  4.14693356e+00\n",
      "Epoch: 19212 mean train loss:  1.43568031e-05, mean val. loss:  4.14717484e+00\n",
      "Epoch: 19213 mean train loss:  1.40711199e-05, mean val. loss:  4.14745522e+00\n",
      "Epoch: 19214 mean train loss:  1.42883218e-05, mean val. loss:  4.14771700e+00\n",
      "Epoch: 19215 mean train loss:  1.43464713e-05, mean val. loss:  4.14790869e+00\n",
      "Epoch: 19216 mean train loss:  1.45267404e-05, mean val. loss:  4.14811516e+00\n",
      "Epoch: 19217 mean train loss:  1.43313373e-05, mean val. loss:  4.14835405e+00\n",
      "Epoch: 19218 mean train loss:  1.42011559e-05, mean val. loss:  4.14866686e+00\n",
      "Epoch: 19219 mean train loss:  1.44415535e-05, mean val. loss:  4.14892817e+00\n",
      "Epoch: 19220 mean train loss:  1.43960351e-05, mean val. loss:  4.14914179e+00\n",
      "Epoch: 19221 mean train loss:  1.44084916e-05, mean val. loss:  4.14938307e+00\n",
      "Epoch: 19222 mean train loss:  1.43205980e-05, mean val. loss:  4.14964247e+00\n",
      "Epoch: 19223 mean train loss:  1.42631470e-05, mean val. loss:  4.14990997e+00\n",
      "Epoch: 19224 mean train loss:  1.42497011e-05, mean val. loss:  4.15020895e+00\n",
      "Epoch: 19225 mean train loss:  1.45409431e-05, mean val. loss:  4.15052652e+00\n",
      "Epoch: 19226 mean train loss:  1.40453922e-05, mean val. loss:  4.15089035e+00\n",
      "Epoch: 19227 mean train loss:  1.44778751e-05, mean val. loss:  4.15126801e+00\n",
      "Epoch: 19228 mean train loss:  1.40570628e-05, mean val. loss:  4.15168619e+00\n",
      "Epoch: 19229 mean train loss:  1.42994395e-05, mean val. loss:  4.15207243e+00\n",
      "Epoch: 19230 mean train loss:  1.41930359e-05, mean val. loss:  4.15241575e+00\n",
      "Epoch: 19231 mean train loss:  1.44045916e-05, mean val. loss:  4.15277147e+00\n",
      "Epoch: 19232 mean train loss:  1.42055505e-05, mean val. loss:  4.15316248e+00\n",
      "Epoch: 19233 mean train loss:  1.41959754e-05, mean val. loss:  4.15359640e+00\n",
      "Epoch: 19234 mean train loss:  1.42859062e-05, mean val. loss:  4.15407801e+00\n",
      "Epoch: 19235 mean train loss:  1.45011873e-05, mean val. loss:  4.15452766e+00\n",
      "Epoch: 19236 mean train loss:  1.42333447e-05, mean val. loss:  4.15501499e+00\n",
      "Epoch: 19237 mean train loss:  1.42530480e-05, mean val. loss:  4.15550518e+00\n",
      "Epoch: 19238 mean train loss:  1.41759228e-05, mean val. loss:  4.15600443e+00\n",
      "Epoch: 19239 mean train loss:  1.42522331e-05, mean val. loss:  4.15651131e+00\n",
      "Epoch: 19240 mean train loss:  1.43676880e-05, mean val. loss:  4.15701962e+00\n",
      "Epoch: 19241 mean train loss:  1.42696372e-05, mean val. loss:  4.15757322e+00\n",
      "Epoch: 19242 mean train loss:  1.43882644e-05, mean val. loss:  4.15809345e+00\n",
      "Epoch: 19243 mean train loss:  1.44254591e-05, mean val. loss:  4.15856695e+00\n",
      "Epoch: 19244 mean train loss:  1.42731878e-05, mean val. loss:  4.15893269e+00\n",
      "Epoch: 19245 mean train loss:  1.44243822e-05, mean val. loss:  4.15922832e+00\n",
      "Epoch: 19246 mean train loss:  1.43322395e-05, mean val. loss:  4.15947676e+00\n",
      "Epoch: 19247 mean train loss:  1.43155921e-05, mean val. loss:  4.15964746e+00\n",
      "Epoch: 19248 mean train loss:  1.42188219e-05, mean val. loss:  4.15979052e+00\n",
      "Epoch: 19249 mean train loss:  1.43567158e-05, mean val. loss:  4.15987730e+00\n",
      "Epoch: 19250 mean train loss:  1.43604120e-05, mean val. loss:  4.16003418e+00\n",
      "Epoch: 19251 mean train loss:  1.42563658e-05, mean val. loss:  4.16017437e+00\n",
      "Epoch: 19252 mean train loss:  1.40771735e-05, mean val. loss:  4.16041470e+00\n",
      "Epoch: 19253 mean train loss:  1.42808713e-05, mean val. loss:  4.16070366e+00\n",
      "Epoch: 19254 mean train loss:  1.39955373e-05, mean val. loss:  4.16109228e+00\n",
      "Epoch: 19255 mean train loss:  1.42202480e-05, mean val. loss:  4.16137362e+00\n",
      "Epoch: 19256 mean train loss:  1.44283986e-05, mean val. loss:  4.16161489e+00\n",
      "Epoch: 19257 mean train loss:  1.42430072e-05, mean val. loss:  4.16185713e+00\n",
      "Epoch: 19258 mean train loss:  1.39741460e-05, mean val. loss:  4.16221380e+00\n",
      "Epoch: 19259 mean train loss:  1.44721707e-05, mean val. loss:  4.16253948e+00\n",
      "Epoch: 19260 mean train loss:  1.44123915e-05, mean val. loss:  4.16278267e+00\n",
      "Epoch: 19261 mean train loss:  1.44219666e-05, mean val. loss:  4.16297388e+00\n",
      "Epoch: 19262 mean train loss:  1.44470832e-05, mean val. loss:  4.16321325e+00\n",
      "Epoch: 19263 mean train loss:  1.42661447e-05, mean val. loss:  4.16347027e+00\n",
      "Epoch: 19264 mean train loss:  1.43981597e-05, mean val. loss:  4.16364193e+00\n",
      "Epoch: 19265 mean train loss:  1.43997604e-05, mean val. loss:  4.16382504e+00\n",
      "Epoch: 19266 mean train loss:  1.44805817e-05, mean val. loss:  4.16394472e+00\n",
      "Epoch: 19267 mean train loss:  1.42457720e-05, mean val. loss:  4.16411924e+00\n",
      "Epoch: 19268 mean train loss:  1.42938516e-05, mean val. loss:  4.16437340e+00\n",
      "Epoch: 19269 mean train loss:  1.41652417e-05, mean val. loss:  4.16468382e+00\n",
      "Epoch: 19270 mean train loss:  1.39746408e-05, mean val. loss:  4.16511869e+00\n",
      "Epoch: 19271 mean train loss:  1.44894584e-05, mean val. loss:  4.16553497e+00\n",
      "Epoch: 19272 mean train loss:  1.44193764e-05, mean val. loss:  4.16592503e+00\n",
      "Epoch: 19273 mean train loss:  1.45109952e-05, mean val. loss:  4.16629601e+00\n",
      "Epoch: 19274 mean train loss:  1.41495548e-05, mean val. loss:  4.16672516e+00\n",
      "Epoch: 19275 mean train loss:  1.42336939e-05, mean val. loss:  4.16715431e+00\n",
      "Epoch: 19276 mean train loss:  1.42334611e-05, mean val. loss:  4.16762733e+00\n",
      "Epoch: 19277 mean train loss:  1.43531070e-05, mean val. loss:  4.16808796e+00\n",
      "Epoch: 19278 mean train loss:  1.43793295e-05, mean val. loss:  4.16860580e+00\n",
      "Epoch: 19279 mean train loss:  1.39770564e-05, mean val. loss:  4.16914606e+00\n",
      "Epoch: 19280 mean train loss:  1.42223435e-05, mean val. loss:  4.16965914e+00\n",
      "Epoch: 19281 mean train loss:  1.40344491e-05, mean val. loss:  4.17015505e+00\n",
      "Epoch: 19282 mean train loss:  1.45225786e-05, mean val. loss:  4.17056942e+00\n",
      "Epoch: 19283 mean train loss:  1.42828794e-05, mean val. loss:  4.17106056e+00\n",
      "Epoch: 19284 mean train loss:  1.41142227e-05, mean val. loss:  4.17156887e+00\n",
      "Epoch: 19285 mean train loss:  1.41821511e-05, mean val. loss:  4.17215204e+00\n",
      "Epoch: 19286 mean train loss:  1.45330559e-05, mean val. loss:  4.17263269e+00\n",
      "Epoch: 19287 mean train loss:  1.41534838e-05, mean val. loss:  4.17314863e+00\n",
      "Epoch: 19288 mean train loss:  1.38853211e-05, mean val. loss:  4.17373753e+00\n",
      "Epoch: 19289 mean train loss:  1.44940859e-05, mean val. loss:  4.17428446e+00\n",
      "Epoch: 19290 mean train loss:  1.41424534e-05, mean val. loss:  4.17479897e+00\n",
      "Epoch: 19291 mean train loss:  1.43931247e-05, mean val. loss:  4.17527676e+00\n",
      "Epoch: 19292 mean train loss:  1.42179779e-05, mean val. loss:  4.17574215e+00\n",
      "Epoch: 19293 mean train loss:  1.41075288e-05, mean val. loss:  4.17622232e+00\n",
      "Epoch: 19294 mean train loss:  1.39396871e-05, mean val. loss:  4.17671347e+00\n",
      "Epoch: 19295 mean train loss:  1.42405333e-05, mean val. loss:  4.17714453e+00\n",
      "Epoch: 19296 mean train loss:  1.43970246e-05, mean val. loss:  4.17760038e+00\n",
      "Epoch: 19297 mean train loss:  1.40950433e-05, mean val. loss:  4.17804861e+00\n",
      "Epoch: 19298 mean train loss:  1.45839294e-05, mean val. loss:  4.17833567e+00\n",
      "Epoch: 19299 mean train loss:  1.42679492e-05, mean val. loss:  4.17857170e+00\n",
      "Epoch: 19300 mean train loss:  1.42779900e-05, mean val. loss:  4.17878389e+00\n",
      "Epoch: 19301 mean train loss:  1.43061334e-05, mean val. loss:  4.17894030e+00\n",
      "Epoch: 19302 mean train loss:  1.42307836e-05, mean val. loss:  4.17913103e+00\n",
      "Epoch: 19303 mean train loss:  1.40927732e-05, mean val. loss:  4.17923069e+00\n",
      "Epoch: 19304 mean train loss:  1.45210361e-05, mean val. loss:  4.17928743e+00\n",
      "Epoch: 19305 mean train loss:  1.43642246e-05, mean val. loss:  4.17934847e+00\n",
      "Epoch: 19306 mean train loss:  1.47702522e-05, mean val. loss:  4.17924643e+00\n",
      "Epoch: 19307 mean train loss:  1.41664059e-05, mean val. loss:  4.17914248e+00\n",
      "Epoch: 19308 mean train loss:  1.40692864e-05, mean val. loss:  4.17908001e+00\n",
      "Epoch: 19309 mean train loss:  1.41320343e-05, mean val. loss:  4.17900229e+00\n",
      "Epoch: 19310 mean train loss:  1.39689073e-05, mean val. loss:  4.17893982e+00\n",
      "Epoch: 19311 mean train loss:  1.45077938e-05, mean val. loss:  4.17874241e+00\n",
      "Epoch: 19312 mean train loss:  1.42797362e-05, mean val. loss:  4.17861509e+00\n",
      "Epoch: 19313 mean train loss:  1.42644276e-05, mean val. loss:  4.17858553e+00\n",
      "Epoch: 19314 mean train loss:  1.40839547e-05, mean val. loss:  4.17865181e+00\n",
      "Epoch: 19315 mean train loss:  1.42474310e-05, mean val. loss:  4.17876530e+00\n",
      "Epoch: 19316 mean train loss:  1.42331992e-05, mean val. loss:  4.17897272e+00\n",
      "Epoch: 19317 mean train loss:  1.43677171e-05, mean val. loss:  4.17921209e+00\n",
      "Epoch: 19318 mean train loss:  1.42292120e-05, mean val. loss:  4.17950869e+00\n",
      "Epoch: 19319 mean train loss:  1.43566867e-05, mean val. loss:  4.17984009e+00\n",
      "Epoch: 19320 mean train loss:  1.41450437e-05, mean val. loss:  4.18034315e+00\n",
      "Epoch: 19321 mean train loss:  1.41779892e-05, mean val. loss:  4.18090105e+00\n",
      "Epoch: 19322 mean train loss:  1.41749042e-05, mean val. loss:  4.18147516e+00\n",
      "Epoch: 19323 mean train loss:  1.40074408e-05, mean val. loss:  4.18216848e+00\n",
      "Epoch: 19324 mean train loss:  1.43031357e-05, mean val. loss:  4.18286324e+00\n",
      "Epoch: 19325 mean train loss:  1.43304933e-05, mean val. loss:  4.18355322e+00\n",
      "Epoch: 19326 mean train loss:  1.42568606e-05, mean val. loss:  4.18425894e+00\n",
      "Epoch: 19327 mean train loss:  1.42804929e-05, mean val. loss:  4.18496084e+00\n",
      "Epoch: 19328 mean train loss:  1.42589852e-05, mean val. loss:  4.18559170e+00\n",
      "Epoch: 19329 mean train loss:  1.42798817e-05, mean val. loss:  4.18615246e+00\n",
      "Epoch: 19330 mean train loss:  1.40845659e-05, mean val. loss:  4.18667459e+00\n",
      "Epoch: 19331 mean train loss:  1.40566553e-05, mean val. loss:  4.18723822e+00\n",
      "Epoch: 19332 mean train loss:  1.40947232e-05, mean val. loss:  4.18783140e+00\n",
      "Epoch: 19333 mean train loss:  1.42680365e-05, mean val. loss:  4.18839741e+00\n",
      "Epoch: 19334 mean train loss:  1.43536308e-05, mean val. loss:  4.18886709e+00\n",
      "Epoch: 19335 mean train loss:  1.41946657e-05, mean val. loss:  4.18935490e+00\n",
      "Epoch: 19336 mean train loss:  1.39861950e-05, mean val. loss:  4.18990326e+00\n",
      "Epoch: 19337 mean train loss:  1.42835197e-05, mean val. loss:  4.19037390e+00\n",
      "Epoch: 19338 mean train loss:  1.42694334e-05, mean val. loss:  4.19081402e+00\n",
      "Epoch: 19339 mean train loss:  1.41520868e-05, mean val. loss:  4.19123745e+00\n",
      "Epoch: 19340 mean train loss:  1.42363424e-05, mean val. loss:  4.19168329e+00\n",
      "Epoch: 19341 mean train loss:  1.41973724e-05, mean val. loss:  4.19201088e+00\n",
      "Epoch: 19342 mean train loss:  1.44521764e-05, mean val. loss:  4.19231462e+00\n",
      "Epoch: 19343 mean train loss:  1.41019991e-05, mean val. loss:  4.19260359e+00\n",
      "Epoch: 19344 mean train loss:  1.40379998e-05, mean val. loss:  4.19290781e+00\n",
      "Epoch: 19345 mean train loss:  1.43125653e-05, mean val. loss:  4.19313955e+00\n",
      "Epoch: 19346 mean train loss:  1.40375341e-05, mean val. loss:  4.19343185e+00\n",
      "Epoch: 19347 mean train loss:  1.42127101e-05, mean val. loss:  4.19364500e+00\n",
      "Epoch: 19348 mean train loss:  1.44052901e-05, mean val. loss:  4.19379425e+00\n",
      "Epoch: 19349 mean train loss:  1.41946948e-05, mean val. loss:  4.19390059e+00\n",
      "Epoch: 19350 mean train loss:  1.42328499e-05, mean val. loss:  4.19399357e+00\n",
      "Epoch: 19351 mean train loss:  1.41575874e-05, mean val. loss:  4.19413948e+00\n",
      "Epoch: 19352 mean train loss:  1.44807564e-05, mean val. loss:  4.19424009e+00\n",
      "Epoch: 19353 mean train loss:  1.45925442e-05, mean val. loss:  4.19427538e+00\n",
      "Epoch: 19354 mean train loss:  1.41160854e-05, mean val. loss:  4.19429541e+00\n",
      "Epoch: 19355 mean train loss:  1.43971120e-05, mean val. loss:  4.19432640e+00\n",
      "Epoch: 19356 mean train loss:  1.41350320e-05, mean val. loss:  4.19439697e+00\n",
      "Epoch: 19357 mean train loss:  1.43518846e-05, mean val. loss:  4.19438696e+00\n",
      "Epoch: 19358 mean train loss:  1.44004880e-05, mean val. loss:  4.19441509e+00\n",
      "Epoch: 19359 mean train loss:  1.44075602e-05, mean val. loss:  4.19440079e+00\n",
      "Epoch: 19360 mean train loss:  1.39108161e-05, mean val. loss:  4.19446468e+00\n",
      "Epoch: 19361 mean train loss:  1.41990022e-05, mean val. loss:  4.19454956e+00\n",
      "Epoch: 19362 mean train loss:  1.43800862e-05, mean val. loss:  4.19464350e+00\n",
      "Epoch: 19363 mean train loss:  1.43860816e-05, mean val. loss:  4.19477701e+00\n",
      "Epoch: 19364 mean train loss:  1.42052886e-05, mean val. loss:  4.19505358e+00\n",
      "Epoch: 19365 mean train loss:  1.42056087e-05, mean val. loss:  4.19532824e+00\n",
      "Epoch: 19366 mean train loss:  1.40214397e-05, mean val. loss:  4.19568014e+00\n",
      "Epoch: 19367 mean train loss:  1.42035133e-05, mean val. loss:  4.19609356e+00\n",
      "Epoch: 19368 mean train loss:  1.42177159e-05, mean val. loss:  4.19652414e+00\n",
      "Epoch: 19369 mean train loss:  1.43043872e-05, mean val. loss:  4.19696617e+00\n",
      "Epoch: 19370 mean train loss:  1.45650411e-05, mean val. loss:  4.19739962e+00\n",
      "Epoch: 19371 mean train loss:  1.44502555e-05, mean val. loss:  4.19782591e+00\n",
      "Epoch: 19372 mean train loss:  1.43867801e-05, mean val. loss:  4.19827175e+00\n",
      "Epoch: 19373 mean train loss:  1.43361394e-05, mean val. loss:  4.19875240e+00\n",
      "Epoch: 19374 mean train loss:  1.41062192e-05, mean val. loss:  4.19917727e+00\n",
      "Epoch: 19375 mean train loss:  1.45912927e-05, mean val. loss:  4.19955492e+00\n",
      "Epoch: 19376 mean train loss:  1.43666985e-05, mean val. loss:  4.19992638e+00\n",
      "Epoch: 19377 mean train loss:  1.42817444e-05, mean val. loss:  4.20032358e+00\n",
      "Epoch: 19378 mean train loss:  1.42758654e-05, mean val. loss:  4.20064831e+00\n",
      "Epoch: 19379 mean train loss:  1.41345081e-05, mean val. loss:  4.20097065e+00\n",
      "Epoch: 19380 mean train loss:  1.41816854e-05, mean val. loss:  4.20137310e+00\n",
      "Epoch: 19381 mean train loss:  1.44538353e-05, mean val. loss:  4.20171547e+00\n",
      "Epoch: 19382 mean train loss:  1.44857331e-05, mean val. loss:  4.20203066e+00\n",
      "Epoch: 19383 mean train loss:  1.42912904e-05, mean val. loss:  4.20231628e+00\n",
      "Epoch: 19384 mean train loss:  1.42072095e-05, mean val. loss:  4.20260429e+00\n",
      "Epoch: 19385 mean train loss:  1.39536278e-05, mean val. loss:  4.20294476e+00\n",
      "Epoch: 19386 mean train loss:  1.42733916e-05, mean val. loss:  4.20320225e+00\n",
      "Epoch: 19387 mean train loss:  1.42300560e-05, mean val. loss:  4.20348978e+00\n",
      "Epoch: 19388 mean train loss:  1.41993223e-05, mean val. loss:  4.20387936e+00\n",
      "Epoch: 19389 mean train loss:  1.39795593e-05, mean val. loss:  4.20430946e+00\n",
      "Epoch: 19390 mean train loss:  1.40979246e-05, mean val. loss:  4.20480919e+00\n",
      "Epoch: 19391 mean train loss:  1.42066274e-05, mean val. loss:  4.20524025e+00\n",
      "Epoch: 19392 mean train loss:  1.40607008e-05, mean val. loss:  4.20572758e+00\n",
      "Epoch: 19393 mean train loss:  1.42969075e-05, mean val. loss:  4.20617819e+00\n",
      "Epoch: 19394 mean train loss:  1.43053185e-05, mean val. loss:  4.20664215e+00\n",
      "Epoch: 19395 mean train loss:  1.42817153e-05, mean val. loss:  4.20707893e+00\n",
      "Epoch: 19396 mean train loss:  1.41547644e-05, mean val. loss:  4.20760727e+00\n",
      "Epoch: 19397 mean train loss:  1.40867487e-05, mean val. loss:  4.20819187e+00\n",
      "Epoch: 19398 mean train loss:  1.39202457e-05, mean val. loss:  4.20884132e+00\n",
      "Epoch: 19399 mean train loss:  1.42313947e-05, mean val. loss:  4.20946693e+00\n",
      "Epoch: 19400 mean train loss:  1.41852652e-05, mean val. loss:  4.21002436e+00\n",
      "Epoch: 19401 mean train loss:  1.41327910e-05, mean val. loss:  4.21055412e+00\n",
      "Epoch: 19402 mean train loss:  1.42731587e-05, mean val. loss:  4.21102476e+00\n",
      "Epoch: 19403 mean train loss:  1.39068288e-05, mean val. loss:  4.21154165e+00\n",
      "Epoch: 19404 mean train loss:  1.39068288e-05, mean val. loss:  4.21206045e+00\n",
      "Epoch: 19405 mean train loss:  1.41365745e-05, mean val. loss:  4.21249628e+00\n",
      "Epoch: 19406 mean train loss:  1.40998163e-05, mean val. loss:  4.21287823e+00\n",
      "Epoch: 19407 mean train loss:  1.41918717e-05, mean val. loss:  4.21327782e+00\n",
      "Epoch: 19408 mean train loss:  1.41366036e-05, mean val. loss:  4.21363640e+00\n",
      "Epoch: 19409 mean train loss:  1.44243531e-05, mean val. loss:  4.21392584e+00\n",
      "Epoch: 19410 mean train loss:  1.40547054e-05, mean val. loss:  4.21413708e+00\n",
      "Epoch: 19411 mean train loss:  1.40909979e-05, mean val. loss:  4.21435738e+00\n",
      "Epoch: 19412 mean train loss:  1.42061908e-05, mean val. loss:  4.21457005e+00\n",
      "Epoch: 19413 mean train loss:  1.39693730e-05, mean val. loss:  4.21478319e+00\n",
      "Epoch: 19414 mean train loss:  1.41038327e-05, mean val. loss:  4.21504974e+00\n",
      "Epoch: 19415 mean train loss:  1.43607613e-05, mean val. loss:  4.21528053e+00\n",
      "Epoch: 19416 mean train loss:  1.40480406e-05, mean val. loss:  4.21549749e+00\n",
      "Epoch: 19417 mean train loss:  1.47599494e-05, mean val. loss:  4.21551180e+00\n",
      "Epoch: 19418 mean train loss:  1.46151287e-05, mean val. loss:  4.21545982e+00\n",
      "Epoch: 19419 mean train loss:  1.44584628e-05, mean val. loss:  4.21540356e+00\n",
      "Epoch: 19420 mean train loss:  1.43779034e-05, mean val. loss:  4.21526909e+00\n",
      "Epoch: 19421 mean train loss:  1.41881173e-05, mean val. loss:  4.21507454e+00\n",
      "Epoch: 19422 mean train loss:  1.40969350e-05, mean val. loss:  4.21494579e+00\n",
      "Epoch: 19423 mean train loss:  1.40749617e-05, mean val. loss:  4.21487904e+00\n",
      "Epoch: 19424 mean train loss:  1.41483033e-05, mean val. loss:  4.21483517e+00\n",
      "Epoch: 19425 mean train loss:  1.41432101e-05, mean val. loss:  4.21481562e+00\n",
      "Epoch: 19426 mean train loss:  1.42197532e-05, mean val. loss:  4.21483755e+00\n",
      "Epoch: 19427 mean train loss:  1.43499637e-05, mean val. loss:  4.21484280e+00\n",
      "Epoch: 19428 mean train loss:  1.42282224e-05, mean val. loss:  4.21491385e+00\n",
      "Epoch: 19429 mean train loss:  1.39585463e-05, mean val. loss:  4.21507788e+00\n",
      "Epoch: 19430 mean train loss:  1.41288619e-05, mean val. loss:  4.21523046e+00\n",
      "Epoch: 19431 mean train loss:  1.44232472e-05, mean val. loss:  4.21536779e+00\n",
      "Epoch: 19432 mean train loss:  1.42125937e-05, mean val. loss:  4.21557808e+00\n",
      "Epoch: 19433 mean train loss:  1.41604687e-05, mean val. loss:  4.21582603e+00\n",
      "Epoch: 19434 mean train loss:  1.42194331e-05, mean val. loss:  4.21613503e+00\n",
      "Epoch: 19435 mean train loss:  1.43524085e-05, mean val. loss:  4.21641207e+00\n",
      "Epoch: 19436 mean train loss:  1.40862248e-05, mean val. loss:  4.21684694e+00\n",
      "Epoch: 19437 mean train loss:  1.42728677e-05, mean val. loss:  4.21727324e+00\n",
      "Epoch: 19438 mean train loss:  1.39942276e-05, mean val. loss:  4.21779680e+00\n",
      "Epoch: 19439 mean train loss:  1.40991178e-05, mean val. loss:  4.21831608e+00\n",
      "Epoch: 19440 mean train loss:  1.43872749e-05, mean val. loss:  4.21880817e+00\n",
      "Epoch: 19441 mean train loss:  1.41148630e-05, mean val. loss:  4.21934319e+00\n",
      "Epoch: 19442 mean train loss:  1.42778154e-05, mean val. loss:  4.21984625e+00\n",
      "Epoch: 19443 mean train loss:  1.38953910e-05, mean val. loss:  4.22044897e+00\n",
      "Epoch: 19444 mean train loss:  1.38489122e-05, mean val. loss:  4.22119093e+00\n",
      "Epoch: 19445 mean train loss:  1.40946067e-05, mean val. loss:  4.22197390e+00\n",
      "Epoch: 19446 mean train loss:  1.41451892e-05, mean val. loss:  4.22282553e+00\n",
      "Epoch: 19447 mean train loss:  1.42168428e-05, mean val. loss:  4.22370863e+00\n",
      "Epoch: 19448 mean train loss:  1.42310164e-05, mean val. loss:  4.22456884e+00\n",
      "Epoch: 19449 mean train loss:  1.42708013e-05, mean val. loss:  4.22546387e+00\n",
      "Epoch: 19450 mean train loss:  1.43801735e-05, mean val. loss:  4.22623777e+00\n",
      "Epoch: 19451 mean train loss:  1.42253120e-05, mean val. loss:  4.22697067e+00\n",
      "Epoch: 19452 mean train loss:  1.42060453e-05, mean val. loss:  4.22772408e+00\n",
      "Epoch: 19453 mean train loss:  1.39351177e-05, mean val. loss:  4.22844362e+00\n",
      "Epoch: 19454 mean train loss:  1.39186741e-05, mean val. loss:  4.22917986e+00\n",
      "Epoch: 19455 mean train loss:  1.44585792e-05, mean val. loss:  4.22972679e+00\n",
      "Epoch: 19456 mean train loss:  1.41465280e-05, mean val. loss:  4.23014832e+00\n",
      "Epoch: 19457 mean train loss:  1.43139041e-05, mean val. loss:  4.23044872e+00\n",
      "Epoch: 19458 mean train loss:  1.39575277e-05, mean val. loss:  4.23072577e+00\n",
      "Epoch: 19459 mean train loss:  1.41948112e-05, mean val. loss:  4.23095608e+00\n",
      "Epoch: 19460 mean train loss:  1.42260105e-05, mean val. loss:  4.23112297e+00\n",
      "Epoch: 19461 mean train loss:  1.43179495e-05, mean val. loss:  4.23117828e+00\n",
      "Epoch: 19462 mean train loss:  1.43730140e-05, mean val. loss:  4.23111534e+00\n",
      "Epoch: 19463 mean train loss:  1.40410848e-05, mean val. loss:  4.23109913e+00\n",
      "Epoch: 19464 mean train loss:  1.43900688e-05, mean val. loss:  4.23097515e+00\n",
      "Epoch: 19465 mean train loss:  1.40978373e-05, mean val. loss:  4.23086166e+00\n",
      "Epoch: 19466 mean train loss:  1.43446669e-05, mean val. loss:  4.23070097e+00\n",
      "Epoch: 19467 mean train loss:  1.41051423e-05, mean val. loss:  4.23051500e+00\n",
      "Epoch: 19468 mean train loss:  1.40949851e-05, mean val. loss:  4.23038864e+00\n",
      "Epoch: 19469 mean train loss:  1.40677730e-05, mean val. loss:  4.23031187e+00\n",
      "Epoch: 19470 mean train loss:  1.41095370e-05, mean val. loss:  4.23030567e+00\n",
      "Epoch: 19471 mean train loss:  1.40789780e-05, mean val. loss:  4.23035860e+00\n",
      "Epoch: 19472 mean train loss:  1.42173958e-05, mean val. loss:  4.23047972e+00\n",
      "Epoch: 19473 mean train loss:  1.44052028e-05, mean val. loss:  4.23062325e+00\n",
      "Epoch: 19474 mean train loss:  1.39910553e-05, mean val. loss:  4.23087406e+00\n",
      "Epoch: 19475 mean train loss:  1.41839555e-05, mean val. loss:  4.23118162e+00\n",
      "Epoch: 19476 mean train loss:  1.40488264e-05, mean val. loss:  4.23157644e+00\n",
      "Epoch: 19477 mean train loss:  1.42960926e-05, mean val. loss:  4.23193645e+00\n",
      "Epoch: 19478 mean train loss:  1.40135235e-05, mean val. loss:  4.23243237e+00\n",
      "Epoch: 19479 mean train loss:  1.38351461e-05, mean val. loss:  4.23300362e+00\n",
      "Epoch: 19480 mean train loss:  1.41140190e-05, mean val. loss:  4.23363209e+00\n",
      "Epoch: 19481 mean train loss:  1.44306105e-05, mean val. loss:  4.23420143e+00\n",
      "Epoch: 19482 mean train loss:  1.42172212e-05, mean val. loss:  4.23470163e+00\n",
      "Epoch: 19483 mean train loss:  1.42681529e-05, mean val. loss:  4.23522568e+00\n",
      "Epoch: 19484 mean train loss:  1.40644843e-05, mean val. loss:  4.23579836e+00\n",
      "Epoch: 19485 mean train loss:  1.42384088e-05, mean val. loss:  4.23635960e+00\n",
      "Epoch: 19486 mean train loss:  1.39946060e-05, mean val. loss:  4.23685169e+00\n",
      "Epoch: 19487 mean train loss:  1.44838705e-05, mean val. loss:  4.23719263e+00\n",
      "Epoch: 19488 mean train loss:  1.38918695e-05, mean val. loss:  4.23757696e+00\n",
      "Epoch: 19489 mean train loss:  1.41164637e-05, mean val. loss:  4.23792648e+00\n",
      "Epoch: 19490 mean train loss:  1.40688207e-05, mean val. loss:  4.23822832e+00\n",
      "Epoch: 19491 mean train loss:  1.40051707e-05, mean val. loss:  4.23857832e+00\n",
      "Epoch: 19492 mean train loss:  1.40456832e-05, mean val. loss:  4.23883200e+00\n",
      "Epoch: 19493 mean train loss:  1.41691417e-05, mean val. loss:  4.23901749e+00\n",
      "Epoch: 19494 mean train loss:  1.44849473e-05, mean val. loss:  4.23915052e+00\n",
      "Epoch: 19495 mean train loss:  1.44002261e-05, mean val. loss:  4.23930311e+00\n",
      "Epoch: 19496 mean train loss:  1.45592203e-05, mean val. loss:  4.23929453e+00\n",
      "Epoch: 19497 mean train loss:  1.41680939e-05, mean val. loss:  4.23930836e+00\n",
      "Epoch: 19498 mean train loss:  1.38092728e-05, mean val. loss:  4.23939371e+00\n",
      "Epoch: 19499 mean train loss:  1.39668409e-05, mean val. loss:  4.23954535e+00\n",
      "Epoch: 19500 mean train loss:  1.42757781e-05, mean val. loss:  4.23969555e+00\n",
      "Epoch: 19501 mean train loss:  1.41594210e-05, mean val. loss:  4.23986101e+00\n",
      "Epoch: 19502 mean train loss:  1.41883502e-05, mean val. loss:  4.23998213e+00\n",
      "Epoch: 19503 mean train loss:  1.41609926e-05, mean val. loss:  4.24009657e+00\n",
      "Epoch: 19504 mean train loss:  1.40725460e-05, mean val. loss:  4.24024343e+00\n",
      "Epoch: 19505 mean train loss:  1.42140489e-05, mean val. loss:  4.24041605e+00\n",
      "Epoch: 19506 mean train loss:  1.43178622e-05, mean val. loss:  4.24060488e+00\n",
      "Epoch: 19507 mean train loss:  1.42602366e-05, mean val. loss:  4.24079418e+00\n",
      "Epoch: 19508 mean train loss:  1.40051707e-05, mean val. loss:  4.24101543e+00\n",
      "Epoch: 19509 mean train loss:  1.45268277e-05, mean val. loss:  4.24113464e+00\n",
      "Epoch: 19510 mean train loss:  1.42126228e-05, mean val. loss:  4.24130440e+00\n",
      "Epoch: 19511 mean train loss:  1.41703640e-05, mean val. loss:  4.24145508e+00\n",
      "Epoch: 19512 mean train loss:  1.41065975e-05, mean val. loss:  4.24160528e+00\n",
      "Epoch: 19513 mean train loss:  1.40970515e-05, mean val. loss:  4.24174547e+00\n",
      "Epoch: 19514 mean train loss:  1.41210330e-05, mean val. loss:  4.24183083e+00\n",
      "Epoch: 19515 mean train loss:  1.42929784e-05, mean val. loss:  4.24189520e+00\n",
      "Epoch: 19516 mean train loss:  1.39599724e-05, mean val. loss:  4.24197769e+00\n",
      "Epoch: 19517 mean train loss:  1.38694013e-05, mean val. loss:  4.24214411e+00\n",
      "Epoch: 19518 mean train loss:  1.42980134e-05, mean val. loss:  4.24235916e+00\n",
      "Epoch: 19519 mean train loss:  1.41604396e-05, mean val. loss:  4.24268246e+00\n",
      "Epoch: 19520 mean train loss:  1.42249337e-05, mean val. loss:  4.24304390e+00\n",
      "Epoch: 19521 mean train loss:  1.39659387e-05, mean val. loss:  4.24342346e+00\n",
      "Epoch: 19522 mean train loss:  1.40646880e-05, mean val. loss:  4.24389076e+00\n",
      "Epoch: 19523 mean train loss:  1.38503965e-05, mean val. loss:  4.24449110e+00\n",
      "Epoch: 19524 mean train loss:  1.41550845e-05, mean val. loss:  4.24509716e+00\n",
      "Epoch: 19525 mean train loss:  1.41094206e-05, mean val. loss:  4.24578238e+00\n",
      "Epoch: 19526 mean train loss:  1.41569762e-05, mean val. loss:  4.24647617e+00\n",
      "Epoch: 19527 mean train loss:  1.40499615e-05, mean val. loss:  4.24717426e+00\n",
      "Epoch: 19528 mean train loss:  1.41444616e-05, mean val. loss:  4.24792480e+00\n",
      "Epoch: 19529 mean train loss:  1.40665506e-05, mean val. loss:  4.24869823e+00\n",
      "Epoch: 19530 mean train loss:  1.41462078e-05, mean val. loss:  4.24941444e+00\n",
      "Epoch: 19531 mean train loss:  1.42930658e-05, mean val. loss:  4.25008869e+00\n",
      "Epoch: 19532 mean train loss:  1.41111668e-05, mean val. loss:  4.25070238e+00\n",
      "Epoch: 19533 mean train loss:  1.41466444e-05, mean val. loss:  4.25129271e+00\n",
      "Epoch: 19534 mean train loss:  1.43983052e-05, mean val. loss:  4.25175285e+00\n",
      "Epoch: 19535 mean train loss:  1.42614299e-05, mean val. loss:  4.25216913e+00\n",
      "Epoch: 19536 mean train loss:  1.40834018e-05, mean val. loss:  4.25254011e+00\n",
      "Epoch: 19537 mean train loss:  1.40636694e-05, mean val. loss:  4.25289488e+00\n",
      "Epoch: 19538 mean train loss:  1.40363700e-05, mean val. loss:  4.25331783e+00\n",
      "Epoch: 19539 mean train loss:  1.39961194e-05, mean val. loss:  4.25374889e+00\n",
      "Epoch: 19540 mean train loss:  1.39336626e-05, mean val. loss:  4.25416565e+00\n",
      "Epoch: 19541 mean train loss:  1.42109930e-05, mean val. loss:  4.25455046e+00\n",
      "Epoch: 19542 mean train loss:  1.42000790e-05, mean val. loss:  4.25492001e+00\n",
      "Epoch: 19543 mean train loss:  1.39372132e-05, mean val. loss:  4.25528669e+00\n",
      "Epoch: 19544 mean train loss:  1.42237986e-05, mean val. loss:  4.25558472e+00\n",
      "Epoch: 19545 mean train loss:  1.41352648e-05, mean val. loss:  4.25584650e+00\n",
      "Epoch: 19546 mean train loss:  1.39008916e-05, mean val. loss:  4.25608206e+00\n",
      "Epoch: 19547 mean train loss:  1.41393393e-05, mean val. loss:  4.25626802e+00\n",
      "Epoch: 19548 mean train loss:  1.41577038e-05, mean val. loss:  4.25638866e+00\n",
      "Epoch: 19549 mean train loss:  1.39751355e-05, mean val. loss:  4.25643301e+00\n",
      "Epoch: 19550 mean train loss:  1.41639612e-05, mean val. loss:  4.25654459e+00\n",
      "Epoch: 19551 mean train loss:  1.38124160e-05, mean val. loss:  4.25677252e+00\n",
      "Epoch: 19552 mean train loss:  1.37894531e-05, mean val. loss:  4.25707674e+00\n",
      "Epoch: 19553 mean train loss:  1.41421915e-05, mean val. loss:  4.25738478e+00\n",
      "Epoch: 19554 mean train loss:  1.40218472e-05, mean val. loss:  4.25761604e+00\n",
      "Epoch: 19555 mean train loss:  1.40757184e-05, mean val. loss:  4.25787973e+00\n",
      "Epoch: 19556 mean train loss:  1.39328185e-05, mean val. loss:  4.25817776e+00\n",
      "Epoch: 19557 mean train loss:  1.41550263e-05, mean val. loss:  4.25845575e+00\n",
      "Epoch: 19558 mean train loss:  1.42856443e-05, mean val. loss:  4.25867510e+00\n",
      "Epoch: 19559 mean train loss:  1.42318022e-05, mean val. loss:  4.25890446e+00\n",
      "Epoch: 19560 mean train loss:  1.42343051e-05, mean val. loss:  4.25903225e+00\n",
      "Epoch: 19561 mean train loss:  1.41244091e-05, mean val. loss:  4.25915146e+00\n",
      "Epoch: 19562 mean train loss:  1.42402132e-05, mean val. loss:  4.25916958e+00\n",
      "Epoch: 19563 mean train loss:  1.42051722e-05, mean val. loss:  4.25913286e+00\n",
      "Epoch: 19564 mean train loss:  1.39106123e-05, mean val. loss:  4.25914192e+00\n",
      "Epoch: 19565 mean train loss:  1.40453922e-05, mean val. loss:  4.25913715e+00\n",
      "Epoch: 19566 mean train loss:  1.40066841e-05, mean val. loss:  4.25923300e+00\n",
      "Epoch: 19567 mean train loss:  1.36652088e-05, mean val. loss:  4.25943327e+00\n",
      "Epoch: 19568 mean train loss:  1.41912315e-05, mean val. loss:  4.25965405e+00\n",
      "Epoch: 19569 mean train loss:  1.39426556e-05, mean val. loss:  4.25992250e+00\n",
      "Epoch: 19570 mean train loss:  1.40191696e-05, mean val. loss:  4.26030111e+00\n",
      "Epoch: 19571 mean train loss:  1.38700707e-05, mean val. loss:  4.26068830e+00\n",
      "Epoch: 19572 mean train loss:  1.41645723e-05, mean val. loss:  4.26112509e+00\n",
      "Epoch: 19573 mean train loss:  1.43265643e-05, mean val. loss:  4.26152468e+00\n",
      "Epoch: 19574 mean train loss:  1.39300246e-05, mean val. loss:  4.26198578e+00\n",
      "Epoch: 19575 mean train loss:  1.42427161e-05, mean val. loss:  4.26241589e+00\n",
      "Epoch: 19576 mean train loss:  1.43002253e-05, mean val. loss:  4.26286364e+00\n",
      "Epoch: 19577 mean train loss:  1.39350595e-05, mean val. loss:  4.26334476e+00\n",
      "Epoch: 19578 mean train loss:  1.40992925e-05, mean val. loss:  4.26386166e+00\n",
      "Epoch: 19579 mean train loss:  1.44378573e-05, mean val. loss:  4.26425886e+00\n",
      "Epoch: 19580 mean train loss:  1.42612262e-05, mean val. loss:  4.26469517e+00\n",
      "Epoch: 19581 mean train loss:  1.39633776e-05, mean val. loss:  4.26519775e+00\n",
      "Epoch: 19582 mean train loss:  1.43141078e-05, mean val. loss:  4.26571560e+00\n",
      "Epoch: 19583 mean train loss:  1.43644866e-05, mean val. loss:  4.26618481e+00\n",
      "Epoch: 19584 mean train loss:  1.38633768e-05, mean val. loss:  4.26669168e+00\n",
      "Epoch: 19585 mean train loss:  1.39620388e-05, mean val. loss:  4.26723719e+00\n",
      "Epoch: 19586 mean train loss:  1.41444325e-05, mean val. loss:  4.26779795e+00\n",
      "Epoch: 19587 mean train loss:  1.41642522e-05, mean val. loss:  4.26830769e+00\n",
      "Epoch: 19588 mean train loss:  1.38113392e-05, mean val. loss:  4.26889801e+00\n",
      "Epoch: 19589 mean train loss:  1.40886696e-05, mean val. loss:  4.26948833e+00\n",
      "Epoch: 19590 mean train loss:  1.41053170e-05, mean val. loss:  4.27007627e+00\n",
      "Epoch: 19591 mean train loss:  1.40334596e-05, mean val. loss:  4.27071667e+00\n",
      "Epoch: 19592 mean train loss:  1.39893673e-05, mean val. loss:  4.27138090e+00\n",
      "Epoch: 19593 mean train loss:  1.39810436e-05, mean val. loss:  4.27205706e+00\n",
      "Epoch: 19594 mean train loss:  1.38941396e-05, mean val. loss:  4.27285051e+00\n",
      "Epoch: 19595 mean train loss:  1.41091004e-05, mean val. loss:  4.27357674e+00\n",
      "Epoch: 19596 mean train loss:  1.40519405e-05, mean val. loss:  4.27427530e+00\n",
      "Epoch: 19597 mean train loss:  1.39363692e-05, mean val. loss:  4.27501535e+00\n",
      "Epoch: 19598 mean train loss:  1.40107586e-05, mean val. loss:  4.27572012e+00\n",
      "Epoch: 19599 mean train loss:  1.41182391e-05, mean val. loss:  4.27633572e+00\n",
      "Epoch: 19600 mean train loss:  1.39540352e-05, mean val. loss:  4.27691936e+00\n",
      "Epoch: 19601 mean train loss:  1.41603523e-05, mean val. loss:  4.27751493e+00\n",
      "Epoch: 19602 mean train loss:  1.38865435e-05, mean val. loss:  4.27811050e+00\n",
      "Epoch: 19603 mean train loss:  1.42053468e-05, mean val. loss:  4.27855492e+00\n",
      "Epoch: 19604 mean train loss:  1.39293552e-05, mean val. loss:  4.27902174e+00\n",
      "Epoch: 19605 mean train loss:  1.41914061e-05, mean val. loss:  4.27939034e+00\n",
      "Epoch: 19606 mean train loss:  1.41799392e-05, mean val. loss:  4.27969742e+00\n",
      "Epoch: 19607 mean train loss:  1.41554920e-05, mean val. loss:  4.27989674e+00\n",
      "Epoch: 19608 mean train loss:  1.42370118e-05, mean val. loss:  4.28000689e+00\n",
      "Epoch: 19609 mean train loss:  1.43172219e-05, mean val. loss:  4.27995872e+00\n",
      "Epoch: 19610 mean train loss:  1.41657947e-05, mean val. loss:  4.27982235e+00\n",
      "Epoch: 19611 mean train loss:  1.39917247e-05, mean val. loss:  4.27966309e+00\n",
      "Epoch: 19612 mean train loss:  1.44836667e-05, mean val. loss:  4.27942753e+00\n",
      "Epoch: 19613 mean train loss:  1.41185010e-05, mean val. loss:  4.27914810e+00\n",
      "Epoch: 19614 mean train loss:  1.41425699e-05, mean val. loss:  4.27891350e+00\n",
      "Epoch: 19615 mean train loss:  1.41048804e-05, mean val. loss:  4.27861214e+00\n",
      "Epoch: 19616 mean train loss:  1.42478675e-05, mean val. loss:  4.27829170e+00\n",
      "Epoch: 19617 mean train loss:  1.41045020e-05, mean val. loss:  4.27798367e+00\n",
      "Epoch: 19618 mean train loss:  1.39133190e-05, mean val. loss:  4.27779818e+00\n",
      "Epoch: 19619 mean train loss:  1.38699543e-05, mean val. loss:  4.27774239e+00\n",
      "Epoch: 19620 mean train loss:  1.41178316e-05, mean val. loss:  4.27784014e+00\n",
      "Epoch: 19621 mean train loss:  1.40945194e-05, mean val. loss:  4.27796793e+00\n",
      "Epoch: 19622 mean train loss:  1.42012141e-05, mean val. loss:  4.27809191e+00\n",
      "Epoch: 19623 mean train loss:  1.40823831e-05, mean val. loss:  4.27829695e+00\n",
      "Epoch: 19624 mean train loss:  1.39317708e-05, mean val. loss:  4.27863646e+00\n",
      "Epoch: 19625 mean train loss:  1.42659119e-05, mean val. loss:  4.27905607e+00\n",
      "Epoch: 19626 mean train loss:  1.40253687e-05, mean val. loss:  4.27958536e+00\n",
      "Epoch: 19627 mean train loss:  1.39395706e-05, mean val. loss:  4.28012753e+00\n",
      "Epoch: 19628 mean train loss:  1.43896614e-05, mean val. loss:  4.28068399e+00\n",
      "Epoch: 19629 mean train loss:  1.39754266e-05, mean val. loss:  4.28130341e+00\n",
      "Epoch: 19630 mean train loss:  1.39390468e-05, mean val. loss:  4.28195286e+00\n",
      "Epoch: 19631 mean train loss:  1.41205965e-05, mean val. loss:  4.28258801e+00\n",
      "Epoch: 19632 mean train loss:  1.41570636e-05, mean val. loss:  4.28317213e+00\n",
      "Epoch: 19633 mean train loss:  1.40778138e-05, mean val. loss:  4.28373909e+00\n",
      "Epoch: 19634 mean train loss:  1.40611373e-05, mean val. loss:  4.28422594e+00\n",
      "Epoch: 19635 mean train loss:  1.40420161e-05, mean val. loss:  4.28472376e+00\n",
      "Epoch: 19636 mean train loss:  1.40291522e-05, mean val. loss:  4.28516150e+00\n",
      "Epoch: 19637 mean train loss:  1.41199853e-05, mean val. loss:  4.28562832e+00\n",
      "Epoch: 19638 mean train loss:  1.39732147e-05, mean val. loss:  4.28601265e+00\n",
      "Epoch: 19639 mean train loss:  1.40178017e-05, mean val. loss:  4.28640604e+00\n",
      "Epoch: 19640 mean train loss:  1.42036879e-05, mean val. loss:  4.28677797e+00\n",
      "Epoch: 19641 mean train loss:  1.41306955e-05, mean val. loss:  4.28705168e+00\n",
      "Epoch: 19642 mean train loss:  1.40709744e-05, mean val. loss:  4.28737545e+00\n",
      "Epoch: 19643 mean train loss:  1.41368655e-05, mean val. loss:  4.28760242e+00\n",
      "Epoch: 19644 mean train loss:  1.40116317e-05, mean val. loss:  4.28786850e+00\n",
      "Epoch: 19645 mean train loss:  1.42373028e-05, mean val. loss:  4.28804398e+00\n",
      "Epoch: 19646 mean train loss:  1.40725169e-05, mean val. loss:  4.28820610e+00\n",
      "Epoch: 19647 mean train loss:  1.40305783e-05, mean val. loss:  4.28840828e+00\n",
      "Epoch: 19648 mean train loss:  1.40317134e-05, mean val. loss:  4.28861284e+00\n",
      "Epoch: 19649 mean train loss:  1.41653582e-05, mean val. loss:  4.28880072e+00\n",
      "Epoch: 19650 mean train loss:  1.39770564e-05, mean val. loss:  4.28897429e+00\n",
      "Epoch: 19651 mean train loss:  1.40212069e-05, mean val. loss:  4.28917122e+00\n",
      "Epoch: 19652 mean train loss:  1.43465295e-05, mean val. loss:  4.28931284e+00\n",
      "Epoch: 19653 mean train loss:  1.38133473e-05, mean val. loss:  4.28953505e+00\n",
      "Epoch: 19654 mean train loss:  1.41258643e-05, mean val. loss:  4.28979206e+00\n",
      "Epoch: 19655 mean train loss:  1.38916948e-05, mean val. loss:  4.29008770e+00\n",
      "Epoch: 19656 mean train loss:  1.40296761e-05, mean val. loss:  4.29042912e+00\n",
      "Epoch: 19657 mean train loss:  1.43653306e-05, mean val. loss:  4.29078817e+00\n",
      "Epoch: 19658 mean train loss:  1.38206815e-05, mean val. loss:  4.29115391e+00\n",
      "Epoch: 19659 mean train loss:  1.40021148e-05, mean val. loss:  4.29152822e+00\n",
      "Epoch: 19660 mean train loss:  1.42792996e-05, mean val. loss:  4.29185390e+00\n",
      "Epoch: 19661 mean train loss:  1.40471675e-05, mean val. loss:  4.29212999e+00\n",
      "Epoch: 19662 mean train loss:  1.41393975e-05, mean val. loss:  4.29238462e+00\n",
      "Epoch: 19663 mean train loss:  1.39433832e-05, mean val. loss:  4.29263830e+00\n",
      "Epoch: 19664 mean train loss:  1.42649515e-05, mean val. loss:  4.29291105e+00\n",
      "Epoch: 19665 mean train loss:  1.41460041e-05, mean val. loss:  4.29314709e+00\n",
      "Epoch: 19666 mean train loss:  1.40843622e-05, mean val. loss:  4.29336071e+00\n",
      "Epoch: 19667 mean train loss:  1.39987969e-05, mean val. loss:  4.29361105e+00\n",
      "Epoch: 19668 mean train loss:  1.39036856e-05, mean val. loss:  4.29391146e+00\n",
      "Epoch: 19669 mean train loss:  1.39356998e-05, mean val. loss:  4.29420710e+00\n",
      "Epoch: 19670 mean train loss:  1.40963821e-05, mean val. loss:  4.29448462e+00\n",
      "Epoch: 19671 mean train loss:  1.41539203e-05, mean val. loss:  4.29474068e+00\n",
      "Epoch: 19672 mean train loss:  1.42237986e-05, mean val. loss:  4.29502010e+00\n",
      "Epoch: 19673 mean train loss:  1.39682088e-05, mean val. loss:  4.29532051e+00\n",
      "Epoch: 19674 mean train loss:  1.37031602e-05, mean val. loss:  4.29575682e+00\n",
      "Epoch: 19675 mean train loss:  1.38655887e-05, mean val. loss:  4.29617071e+00\n",
      "Epoch: 19676 mean train loss:  1.39779586e-05, mean val. loss:  4.29663563e+00\n",
      "Epoch: 19677 mean train loss:  1.39950716e-05, mean val. loss:  4.29716349e+00\n",
      "Epoch: 19678 mean train loss:  1.39641634e-05, mean val. loss:  4.29768801e+00\n",
      "Epoch: 19679 mean train loss:  1.40393968e-05, mean val. loss:  4.29828596e+00\n",
      "Epoch: 19680 mean train loss:  1.42886420e-05, mean val. loss:  4.29886818e+00\n",
      "Epoch: 19681 mean train loss:  1.40324119e-05, mean val. loss:  4.29950428e+00\n",
      "Epoch: 19682 mean train loss:  1.43998186e-05, mean val. loss:  4.29998016e+00\n",
      "Epoch: 19683 mean train loss:  1.42029021e-05, mean val. loss:  4.30048037e+00\n",
      "Epoch: 19684 mean train loss:  1.41046185e-05, mean val. loss:  4.30088472e+00\n",
      "Epoch: 19685 mean train loss:  1.38525793e-05, mean val. loss:  4.30132580e+00\n",
      "Epoch: 19686 mean train loss:  1.42456556e-05, mean val. loss:  4.30170441e+00\n",
      "Epoch: 19687 mean train loss:  1.38339237e-05, mean val. loss:  4.30205059e+00\n",
      "Epoch: 19688 mean train loss:  1.40556076e-05, mean val. loss:  4.30229378e+00\n",
      "Epoch: 19689 mean train loss:  1.40972552e-05, mean val. loss:  4.30250931e+00\n",
      "Epoch: 19690 mean train loss:  1.38552859e-05, mean val. loss:  4.30267429e+00\n",
      "Epoch: 19691 mean train loss:  1.38171890e-05, mean val. loss:  4.30287695e+00\n",
      "Epoch: 19692 mean train loss:  1.38973410e-05, mean val. loss:  4.30306768e+00\n",
      "Epoch: 19693 mean train loss:  1.45088998e-05, mean val. loss:  4.30311012e+00\n",
      "Epoch: 19694 mean train loss:  1.44138176e-05, mean val. loss:  4.30299377e+00\n",
      "Epoch: 19695 mean train loss:  1.40794727e-05, mean val. loss:  4.30284071e+00\n",
      "Epoch: 19696 mean train loss:  1.42409117e-05, mean val. loss:  4.30266333e+00\n",
      "Epoch: 19697 mean train loss:  1.42133795e-05, mean val. loss:  4.30252457e+00\n",
      "Epoch: 19698 mean train loss:  1.38377945e-05, mean val. loss:  4.30254459e+00\n",
      "Epoch: 19699 mean train loss:  1.40809279e-05, mean val. loss:  4.30254078e+00\n",
      "Epoch: 19700 mean train loss:  1.42189383e-05, mean val. loss:  4.30257559e+00\n",
      "Epoch: 19701 mean train loss:  1.38923351e-05, mean val. loss:  4.30273294e+00\n",
      "Epoch: 19702 mean train loss:  1.40198099e-05, mean val. loss:  4.30294752e+00\n",
      "Epoch: 19703 mean train loss:  1.42443459e-05, mean val. loss:  4.30313778e+00\n",
      "Epoch: 19704 mean train loss:  1.39926851e-05, mean val. loss:  4.30338240e+00\n",
      "Epoch: 19705 mean train loss:  1.41083437e-05, mean val. loss:  4.30366611e+00\n",
      "Epoch: 19706 mean train loss:  1.38207106e-05, mean val. loss:  4.30402136e+00\n",
      "Epoch: 19707 mean train loss:  1.43188518e-05, mean val. loss:  4.30442381e+00\n",
      "Epoch: 19708 mean train loss:  1.38625037e-05, mean val. loss:  4.30489254e+00\n",
      "Epoch: 19709 mean train loss:  1.39197800e-05, mean val. loss:  4.30536461e+00\n",
      "Epoch: 19710 mean train loss:  1.39007752e-05, mean val. loss:  4.30589771e+00\n",
      "Epoch: 19711 mean train loss:  1.41937053e-05, mean val. loss:  4.30637312e+00\n",
      "Epoch: 19712 mean train loss:  1.41491182e-05, mean val. loss:  4.30685234e+00\n",
      "Epoch: 19713 mean train loss:  1.41529599e-05, mean val. loss:  4.30728579e+00\n",
      "Epoch: 19714 mean train loss:  1.41495839e-05, mean val. loss:  4.30769253e+00\n",
      "Epoch: 19715 mean train loss:  1.39620388e-05, mean val. loss:  4.30805159e+00\n",
      "Epoch: 19716 mean train loss:  1.40132033e-05, mean val. loss:  4.30844975e+00\n",
      "Epoch: 19717 mean train loss:  1.41474011e-05, mean val. loss:  4.30877447e+00\n",
      "Epoch: 19718 mean train loss:  1.39323529e-05, mean val. loss:  4.30916929e+00\n",
      "Epoch: 19719 mean train loss:  1.39416370e-05, mean val. loss:  4.30950403e+00\n",
      "Epoch: 19720 mean train loss:  1.42646022e-05, mean val. loss:  4.30980778e+00\n",
      "Epoch: 19721 mean train loss:  1.38239702e-05, mean val. loss:  4.31015682e+00\n",
      "Epoch: 19722 mean train loss:  1.39834301e-05, mean val. loss:  4.31049681e+00\n",
      "Epoch: 19723 mean train loss:  1.39484182e-05, mean val. loss:  4.31086969e+00\n",
      "Epoch: 19724 mean train loss:  1.42122153e-05, mean val. loss:  4.31127071e+00\n",
      "Epoch: 19725 mean train loss:  1.39024924e-05, mean val. loss:  4.31166935e+00\n",
      "Epoch: 19726 mean train loss:  1.41280470e-05, mean val. loss:  4.31202793e+00\n",
      "Epoch: 19727 mean train loss:  1.39538315e-05, mean val. loss:  4.31238651e+00\n",
      "Epoch: 19728 mean train loss:  1.41705968e-05, mean val. loss:  4.31270790e+00\n",
      "Epoch: 19729 mean train loss:  1.39848562e-05, mean val. loss:  4.31302261e+00\n",
      "Epoch: 19730 mean train loss:  1.41753990e-05, mean val. loss:  4.31333494e+00\n",
      "Epoch: 19731 mean train loss:  1.40704797e-05, mean val. loss:  4.31369400e+00\n",
      "Epoch: 19732 mean train loss:  1.40627380e-05, mean val. loss:  4.31402636e+00\n",
      "Epoch: 19733 mean train loss:  1.38501346e-05, mean val. loss:  4.31441832e+00\n",
      "Epoch: 19734 mean train loss:  1.40547636e-05, mean val. loss:  4.31481791e+00\n",
      "Epoch: 19735 mean train loss:  1.42233912e-05, mean val. loss:  4.31510353e+00\n",
      "Epoch: 19736 mean train loss:  1.41202763e-05, mean val. loss:  4.31537819e+00\n",
      "Epoch: 19737 mean train loss:  1.41013588e-05, mean val. loss:  4.31562901e+00\n",
      "Epoch: 19738 mean train loss:  1.42023782e-05, mean val. loss:  4.31579161e+00\n",
      "Epoch: 19739 mean train loss:  1.38957694e-05, mean val. loss:  4.31601906e+00\n",
      "Epoch: 19740 mean train loss:  1.40029006e-05, mean val. loss:  4.31617641e+00\n",
      "Epoch: 19741 mean train loss:  1.38702162e-05, mean val. loss:  4.31638145e+00\n",
      "Epoch: 19742 mean train loss:  1.41427154e-05, mean val. loss:  4.31656551e+00\n",
      "Epoch: 19743 mean train loss:  1.39891345e-05, mean val. loss:  4.31679678e+00\n",
      "Epoch: 19744 mean train loss:  1.41284254e-05, mean val. loss:  4.31693268e+00\n",
      "Epoch: 19745 mean train loss:  1.41549681e-05, mean val. loss:  4.31705809e+00\n",
      "Epoch: 19746 mean train loss:  1.38253090e-05, mean val. loss:  4.31731606e+00\n",
      "Epoch: 19747 mean train loss:  1.39136973e-05, mean val. loss:  4.31759930e+00\n",
      "Epoch: 19748 mean train loss:  1.38572941e-05, mean val. loss:  4.31797314e+00\n",
      "Epoch: 19749 mean train loss:  1.40341581e-05, mean val. loss:  4.31830359e+00\n",
      "Epoch: 19750 mean train loss:  1.40406482e-05, mean val. loss:  4.31860924e+00\n",
      "Epoch: 19751 mean train loss:  1.39354088e-05, mean val. loss:  4.31893349e+00\n",
      "Epoch: 19752 mean train loss:  1.39202166e-05, mean val. loss:  4.31921816e+00\n",
      "Epoch: 19753 mean train loss:  1.42225472e-05, mean val. loss:  4.31942987e+00\n",
      "Epoch: 19754 mean train loss:  1.39450131e-05, mean val. loss:  4.31967974e+00\n",
      "Epoch: 19755 mean train loss:  1.42368372e-05, mean val. loss:  4.31990242e+00\n",
      "Epoch: 19756 mean train loss:  1.39502517e-05, mean val. loss:  4.32009745e+00\n",
      "Epoch: 19757 mean train loss:  1.41210912e-05, mean val. loss:  4.32032537e+00\n",
      "Epoch: 19758 mean train loss:  1.40319462e-05, mean val. loss:  4.32055187e+00\n",
      "Epoch: 19759 mean train loss:  1.41255732e-05, mean val. loss:  4.32080317e+00\n",
      "Epoch: 19760 mean train loss:  1.44529913e-05, mean val. loss:  4.32095146e+00\n",
      "Epoch: 19761 mean train loss:  1.41389319e-05, mean val. loss:  4.32107019e+00\n",
      "Epoch: 19762 mean train loss:  1.41639903e-05, mean val. loss:  4.32118177e+00\n",
      "Epoch: 19763 mean train loss:  1.40922493e-05, mean val. loss:  4.32131815e+00\n",
      "Epoch: 19764 mean train loss:  1.41702185e-05, mean val. loss:  4.32150841e+00\n",
      "Epoch: 19765 mean train loss:  1.38450705e-05, mean val. loss:  4.32179451e+00\n",
      "Epoch: 19766 mean train loss:  1.40151824e-05, mean val. loss:  4.32212734e+00\n",
      "Epoch: 19767 mean train loss:  1.41104101e-05, mean val. loss:  4.32251072e+00\n",
      "Epoch: 19768 mean train loss:  1.39362528e-05, mean val. loss:  4.32284641e+00\n",
      "Epoch: 19769 mean train loss:  1.38058094e-05, mean val. loss:  4.32327652e+00\n",
      "Epoch: 19770 mean train loss:  1.38959731e-05, mean val. loss:  4.32367277e+00\n",
      "Epoch: 19771 mean train loss:  1.40693446e-05, mean val. loss:  4.32403040e+00\n",
      "Epoch: 19772 mean train loss:  1.38960604e-05, mean val. loss:  4.32446480e+00\n",
      "Epoch: 19773 mean train loss:  1.39485928e-05, mean val. loss:  4.32492638e+00\n",
      "Epoch: 19774 mean train loss:  1.44802034e-05, mean val. loss:  4.32527685e+00\n",
      "Epoch: 19775 mean train loss:  1.40083721e-05, mean val. loss:  4.32561064e+00\n",
      "Epoch: 19776 mean train loss:  1.39632029e-05, mean val. loss:  4.32597208e+00\n",
      "Epoch: 19777 mean train loss:  1.40012708e-05, mean val. loss:  4.32635021e+00\n",
      "Epoch: 19778 mean train loss:  1.39345648e-05, mean val. loss:  4.32674885e+00\n",
      "Epoch: 19779 mean train loss:  1.39587792e-05, mean val. loss:  4.32718182e+00\n",
      "Epoch: 19780 mean train loss:  1.38955365e-05, mean val. loss:  4.32763147e+00\n",
      "Epoch: 19781 mean train loss:  1.40558113e-05, mean val. loss:  4.32812262e+00\n",
      "Epoch: 19782 mean train loss:  1.41310447e-05, mean val. loss:  4.32857513e+00\n",
      "Epoch: 19783 mean train loss:  1.44286023e-05, mean val. loss:  4.32896757e+00\n",
      "Epoch: 19784 mean train loss:  1.40042393e-05, mean val. loss:  4.32937813e+00\n",
      "Epoch: 19785 mean train loss:  1.37298193e-05, mean val. loss:  4.32977819e+00\n",
      "Epoch: 19786 mean train loss:  1.38841569e-05, mean val. loss:  4.33025408e+00\n",
      "Epoch: 19787 mean train loss:  1.37197203e-05, mean val. loss:  4.33074236e+00\n",
      "Epoch: 19788 mean train loss:  1.37974566e-05, mean val. loss:  4.33128405e+00\n",
      "Epoch: 19789 mean train loss:  1.39515032e-05, mean val. loss:  4.33188629e+00\n",
      "Epoch: 19790 mean train loss:  1.40058983e-05, mean val. loss:  4.33246613e+00\n",
      "Epoch: 19791 mean train loss:  1.38819451e-05, mean val. loss:  4.33307505e+00\n",
      "Epoch: 19792 mean train loss:  1.39866606e-05, mean val. loss:  4.33362246e+00\n",
      "Epoch: 19793 mean train loss:  1.41017372e-05, mean val. loss:  4.33410072e+00\n",
      "Epoch: 19794 mean train loss:  1.40512129e-05, mean val. loss:  4.33458281e+00\n",
      "Epoch: 19795 mean train loss:  1.43054349e-05, mean val. loss:  4.33502197e+00\n",
      "Epoch: 19796 mean train loss:  1.41064811e-05, mean val. loss:  4.33539677e+00\n",
      "Epoch: 19797 mean train loss:  1.37936731e-05, mean val. loss:  4.33576250e+00\n",
      "Epoch: 19798 mean train loss:  1.40596239e-05, mean val. loss:  4.33605194e+00\n",
      "Epoch: 19799 mean train loss:  1.40834891e-05, mean val. loss:  4.33633280e+00\n",
      "Epoch: 19800 mean train loss:  1.40448392e-05, mean val. loss:  4.33656025e+00\n",
      "Epoch: 19801 mean train loss:  1.39144249e-05, mean val. loss:  4.33682299e+00\n",
      "Epoch: 19802 mean train loss:  1.40936754e-05, mean val. loss:  4.33713341e+00\n",
      "Epoch: 19803 mean train loss:  1.38466421e-05, mean val. loss:  4.33751917e+00\n",
      "Epoch: 19804 mean train loss:  1.42381468e-05, mean val. loss:  4.33785868e+00\n",
      "Epoch: 19805 mean train loss:  1.39558106e-05, mean val. loss:  4.33821821e+00\n",
      "Epoch: 19806 mean train loss:  1.38577307e-05, mean val. loss:  4.33864927e+00\n",
      "Epoch: 19807 mean train loss:  1.37381139e-05, mean val. loss:  4.33918762e+00\n",
      "Epoch: 19808 mean train loss:  1.40692282e-05, mean val. loss:  4.33972216e+00\n",
      "Epoch: 19809 mean train loss:  1.38107280e-05, mean val. loss:  4.34028673e+00\n",
      "Epoch: 19810 mean train loss:  1.36696908e-05, mean val. loss:  4.34091473e+00\n",
      "Epoch: 19811 mean train loss:  1.38142786e-05, mean val. loss:  4.34162569e+00\n",
      "Epoch: 19812 mean train loss:  1.41896016e-05, mean val. loss:  4.34224463e+00\n",
      "Epoch: 19813 mean train loss:  1.39171316e-05, mean val. loss:  4.34281540e+00\n",
      "Epoch: 19814 mean train loss:  1.37804600e-05, mean val. loss:  4.34341192e+00\n",
      "Epoch: 19815 mean train loss:  1.36626768e-05, mean val. loss:  4.34400511e+00\n",
      "Epoch: 19816 mean train loss:  1.44127407e-05, mean val. loss:  4.34445238e+00\n",
      "Epoch: 19817 mean train loss:  1.39008916e-05, mean val. loss:  4.34486341e+00\n",
      "Epoch: 19818 mean train loss:  1.41025230e-05, mean val. loss:  4.34519720e+00\n",
      "Epoch: 19819 mean train loss:  1.41278724e-05, mean val. loss:  4.34550333e+00\n",
      "Epoch: 19820 mean train loss:  1.39036565e-05, mean val. loss:  4.34586954e+00\n",
      "Epoch: 19821 mean train loss:  1.40824122e-05, mean val. loss:  4.34606600e+00\n",
      "Epoch: 19822 mean train loss:  1.41328783e-05, mean val. loss:  4.34627724e+00\n",
      "Epoch: 19823 mean train loss:  1.38725445e-05, mean val. loss:  4.34647417e+00\n",
      "Epoch: 19824 mean train loss:  1.40826451e-05, mean val. loss:  4.34663057e+00\n",
      "Epoch: 19825 mean train loss:  1.38762989e-05, mean val. loss:  4.34672880e+00\n",
      "Epoch: 19826 mean train loss:  1.40469056e-05, mean val. loss:  4.34676075e+00\n",
      "Epoch: 19827 mean train loss:  1.39042386e-05, mean val. loss:  4.34675694e+00\n",
      "Epoch: 19828 mean train loss:  1.37980969e-05, mean val. loss:  4.34674597e+00\n",
      "Epoch: 19829 mean train loss:  1.39754266e-05, mean val. loss:  4.34678078e+00\n",
      "Epoch: 19830 mean train loss:  1.39376498e-05, mean val. loss:  4.34686756e+00\n",
      "Epoch: 19831 mean train loss:  1.37689058e-05, mean val. loss:  4.34706020e+00\n",
      "Epoch: 19832 mean train loss:  1.39694312e-05, mean val. loss:  4.34725428e+00\n",
      "Epoch: 19833 mean train loss:  1.40156772e-05, mean val. loss:  4.34737158e+00\n",
      "Epoch: 19834 mean train loss:  1.40788907e-05, mean val. loss:  4.34754801e+00\n",
      "Epoch: 19835 mean train loss:  1.38834002e-05, mean val. loss:  4.34784508e+00\n",
      "Epoch: 19836 mean train loss:  1.39816548e-05, mean val. loss:  4.34812689e+00\n",
      "Epoch: 19837 mean train loss:  1.39444310e-05, mean val. loss:  4.34841490e+00\n",
      "Epoch: 19838 mean train loss:  1.41260098e-05, mean val. loss:  4.34865379e+00\n",
      "Epoch: 19839 mean train loss:  1.40830525e-05, mean val. loss:  4.34883881e+00\n",
      "Epoch: 19840 mean train loss:  1.41733326e-05, mean val. loss:  4.34897900e+00\n",
      "Epoch: 19841 mean train loss:  1.42990029e-05, mean val. loss:  4.34908819e+00\n",
      "Epoch: 19842 mean train loss:  1.38576434e-05, mean val. loss:  4.34918451e+00\n",
      "Epoch: 19843 mean train loss:  1.39252516e-05, mean val. loss:  4.34933853e+00\n",
      "Epoch: 19844 mean train loss:  1.37027819e-05, mean val. loss:  4.34954071e+00\n",
      "Epoch: 19845 mean train loss:  1.39242911e-05, mean val. loss:  4.34972095e+00\n",
      "Epoch: 19846 mean train loss:  1.36916933e-05, mean val. loss:  4.35003185e+00\n",
      "Epoch: 19847 mean train loss:  1.37782481e-05, mean val. loss:  4.35042143e+00\n",
      "Epoch: 19848 mean train loss:  1.40102056e-05, mean val. loss:  4.35081673e+00\n",
      "Epoch: 19849 mean train loss:  1.40919583e-05, mean val. loss:  4.35121298e+00\n",
      "Epoch: 19850 mean train loss:  1.39988551e-05, mean val. loss:  4.35166788e+00\n",
      "Epoch: 19851 mean train loss:  1.40269403e-05, mean val. loss:  4.35222816e+00\n",
      "Epoch: 19852 mean train loss:  1.37409661e-05, mean val. loss:  4.35284948e+00\n",
      "Epoch: 19853 mean train loss:  1.39706244e-05, mean val. loss:  4.35350800e+00\n",
      "Epoch: 19854 mean train loss:  1.39701588e-05, mean val. loss:  4.35415268e+00\n",
      "Epoch: 19855 mean train loss:  1.41135824e-05, mean val. loss:  4.35466957e+00\n",
      "Epoch: 19856 mean train loss:  1.37917814e-05, mean val. loss:  4.35518646e+00\n",
      "Epoch: 19857 mean train loss:  1.36923045e-05, mean val. loss:  4.35571146e+00\n",
      "Epoch: 19858 mean train loss:  1.38833129e-05, mean val. loss:  4.35627031e+00\n",
      "Epoch: 19859 mean train loss:  1.35646551e-05, mean val. loss:  4.35688782e+00\n",
      "Epoch: 19860 mean train loss:  1.40402699e-05, mean val. loss:  4.35733080e+00\n",
      "Epoch: 19861 mean train loss:  1.40351476e-05, mean val. loss:  4.35763931e+00\n",
      "Epoch: 19862 mean train loss:  1.41856435e-05, mean val. loss:  4.35790062e+00\n",
      "Epoch: 19863 mean train loss:  1.34813308e-05, mean val. loss:  4.35821962e+00\n",
      "Epoch: 19864 mean train loss:  1.40848279e-05, mean val. loss:  4.35856581e+00\n",
      "Epoch: 19865 mean train loss:  1.41055498e-05, mean val. loss:  4.35879946e+00\n",
      "Epoch: 19866 mean train loss:  1.40537159e-05, mean val. loss:  4.35907125e+00\n",
      "Epoch: 19867 mean train loss:  1.39379990e-05, mean val. loss:  4.35937405e+00\n",
      "Epoch: 19868 mean train loss:  1.39714684e-05, mean val. loss:  4.35968018e+00\n",
      "Epoch: 19869 mean train loss:  1.41210039e-05, mean val. loss:  4.35995483e+00\n",
      "Epoch: 19870 mean train loss:  1.42235658e-05, mean val. loss:  4.36009884e+00\n",
      "Epoch: 19871 mean train loss:  1.37802272e-05, mean val. loss:  4.36028290e+00\n",
      "Epoch: 19872 mean train loss:  1.41586643e-05, mean val. loss:  4.36039019e+00\n",
      "Epoch: 19873 mean train loss:  1.41737110e-05, mean val. loss:  4.36044264e+00\n",
      "Epoch: 19874 mean train loss:  1.40516495e-05, mean val. loss:  4.36042404e+00\n",
      "Epoch: 19875 mean train loss:  1.39002805e-05, mean val. loss:  4.36038446e+00\n",
      "Epoch: 19876 mean train loss:  1.42024655e-05, mean val. loss:  4.36029005e+00\n",
      "Epoch: 19877 mean train loss:  1.39314216e-05, mean val. loss:  4.36016846e+00\n",
      "Epoch: 19878 mean train loss:  1.39790645e-05, mean val. loss:  4.36004353e+00\n",
      "Epoch: 19879 mean train loss:  1.38659088e-05, mean val. loss:  4.35987997e+00\n",
      "Epoch: 19880 mean train loss:  1.38461473e-05, mean val. loss:  4.35973406e+00\n",
      "Epoch: 19881 mean train loss:  1.38603791e-05, mean val. loss:  4.35960388e+00\n",
      "Epoch: 19882 mean train loss:  1.37597381e-05, mean val. loss:  4.35961294e+00\n",
      "Epoch: 19883 mean train loss:  1.40195480e-05, mean val. loss:  4.35964823e+00\n",
      "Epoch: 19884 mean train loss:  1.37670431e-05, mean val. loss:  4.35975695e+00\n",
      "Epoch: 19885 mean train loss:  1.39244075e-05, mean val. loss:  4.35994959e+00\n",
      "Epoch: 19886 mean train loss:  1.37713796e-05, mean val. loss:  4.36021328e+00\n",
      "Epoch: 19887 mean train loss:  1.41568307e-05, mean val. loss:  4.36046648e+00\n",
      "Epoch: 19888 mean train loss:  1.40311313e-05, mean val. loss:  4.36068821e+00\n",
      "Epoch: 19889 mean train loss:  1.39439653e-05, mean val. loss:  4.36104059e+00\n",
      "Epoch: 19890 mean train loss:  1.39099138e-05, mean val. loss:  4.36147547e+00\n",
      "Epoch: 19891 mean train loss:  1.37453899e-05, mean val. loss:  4.36192131e+00\n",
      "Epoch: 19892 mean train loss:  1.37097086e-05, mean val. loss:  4.36240864e+00\n",
      "Epoch: 19893 mean train loss:  1.38996693e-05, mean val. loss:  4.36281919e+00\n",
      "Epoch: 19894 mean train loss:  1.41918135e-05, mean val. loss:  4.36319685e+00\n",
      "Epoch: 19895 mean train loss:  1.40343327e-05, mean val. loss:  4.36357594e+00\n",
      "Epoch: 19896 mean train loss:  1.39485637e-05, mean val. loss:  4.36392832e+00\n",
      "Epoch: 19897 mean train loss:  1.39942567e-05, mean val. loss:  4.36429262e+00\n",
      "Epoch: 19898 mean train loss:  1.37437892e-05, mean val. loss:  4.36466980e+00\n",
      "Epoch: 19899 mean train loss:  1.38820324e-05, mean val. loss:  4.36504936e+00\n",
      "Epoch: 19900 mean train loss:  1.37636962e-05, mean val. loss:  4.36548233e+00\n",
      "Epoch: 19901 mean train loss:  1.40796474e-05, mean val. loss:  4.36591148e+00\n",
      "Epoch: 19902 mean train loss:  1.40425109e-05, mean val. loss:  4.36625433e+00\n",
      "Epoch: 19903 mean train loss:  1.39832846e-05, mean val. loss:  4.36663866e+00\n",
      "Epoch: 19904 mean train loss:  1.38296455e-05, mean val. loss:  4.36703777e+00\n",
      "Epoch: 19905 mean train loss:  1.39275799e-05, mean val. loss:  4.36743116e+00\n",
      "Epoch: 19906 mean train loss:  1.37401512e-05, mean val. loss:  4.36789179e+00\n",
      "Epoch: 19907 mean train loss:  1.37946918e-05, mean val. loss:  4.36836243e+00\n",
      "Epoch: 19908 mean train loss:  1.36516464e-05, mean val. loss:  4.36894894e+00\n",
      "Epoch: 19909 mean train loss:  1.39029580e-05, mean val. loss:  4.36953878e+00\n",
      "Epoch: 19910 mean train loss:  1.37395109e-05, mean val. loss:  4.37018681e+00\n",
      "Epoch: 19911 mean train loss:  1.38792966e-05, mean val. loss:  4.37089062e+00\n",
      "Epoch: 19912 mean train loss:  1.37129100e-05, mean val. loss:  4.37157536e+00\n",
      "Epoch: 19913 mean train loss:  1.40128250e-05, mean val. loss:  4.37218475e+00\n",
      "Epoch: 19914 mean train loss:  1.40010961e-05, mean val. loss:  4.37268639e+00\n",
      "Epoch: 19915 mean train loss:  1.39327603e-05, mean val. loss:  4.37308645e+00\n",
      "Epoch: 19916 mean train loss:  1.37994648e-05, mean val. loss:  4.37350702e+00\n",
      "Epoch: 19917 mean train loss:  1.37405586e-05, mean val. loss:  4.37390089e+00\n",
      "Epoch: 19918 mean train loss:  1.38911419e-05, mean val. loss:  4.37424803e+00\n",
      "Epoch: 19919 mean train loss:  1.36405288e-05, mean val. loss:  4.37466240e+00\n",
      "Epoch: 19920 mean train loss:  1.39326148e-05, mean val. loss:  4.37502098e+00\n",
      "Epoch: 19921 mean train loss:  1.39361073e-05, mean val. loss:  4.37537861e+00\n",
      "Epoch: 19922 mean train loss:  1.38261821e-05, mean val. loss:  4.37577629e+00\n",
      "Epoch: 19923 mean train loss:  1.38399191e-05, mean val. loss:  4.37617636e+00\n",
      "Epoch: 19924 mean train loss:  1.38502801e-05, mean val. loss:  4.37649012e+00\n",
      "Epoch: 19925 mean train loss:  1.37688476e-05, mean val. loss:  4.37682056e+00\n",
      "Epoch: 19926 mean train loss:  1.37816241e-05, mean val. loss:  4.37715054e+00\n",
      "Epoch: 19927 mean train loss:  1.40746997e-05, mean val. loss:  4.37742090e+00\n",
      "Epoch: 19928 mean train loss:  1.37870084e-05, mean val. loss:  4.37774420e+00\n",
      "Epoch: 19929 mean train loss:  1.39775802e-05, mean val. loss:  4.37794685e+00\n",
      "Epoch: 19930 mean train loss:  1.42398640e-05, mean val. loss:  4.37809563e+00\n",
      "Epoch: 19931 mean train loss:  1.40593911e-05, mean val. loss:  4.37817097e+00\n",
      "Epoch: 19932 mean train loss:  1.40522607e-05, mean val. loss:  4.37814236e+00\n",
      "Epoch: 19933 mean train loss:  1.39254262e-05, mean val. loss:  4.37807941e+00\n",
      "Epoch: 19934 mean train loss:  1.36330200e-05, mean val. loss:  4.37806511e+00\n",
      "Epoch: 19935 mean train loss:  1.37786556e-05, mean val. loss:  4.37801743e+00\n",
      "Epoch: 19936 mean train loss:  1.38178875e-05, mean val. loss:  4.37796021e+00\n",
      "Epoch: 19937 mean train loss:  1.38351461e-05, mean val. loss:  4.37785244e+00\n",
      "Epoch: 19938 mean train loss:  1.37717871e-05, mean val. loss:  4.37779665e+00\n",
      "Epoch: 19939 mean train loss:  1.37793540e-05, mean val. loss:  4.37784386e+00\n",
      "Epoch: 19940 mean train loss:  1.39638432e-05, mean val. loss:  4.37797022e+00\n",
      "Epoch: 19941 mean train loss:  1.40599150e-05, mean val. loss:  4.37812805e+00\n",
      "Epoch: 19942 mean train loss:  1.38121250e-05, mean val. loss:  4.37836838e+00\n",
      "Epoch: 19943 mean train loss:  1.37002789e-05, mean val. loss:  4.37864590e+00\n",
      "Epoch: 19944 mean train loss:  1.37973402e-05, mean val. loss:  4.37901640e+00\n",
      "Epoch: 19945 mean train loss:  1.38552277e-05, mean val. loss:  4.37949514e+00\n",
      "Epoch: 19946 mean train loss:  1.38323812e-05, mean val. loss:  4.38006163e+00\n",
      "Epoch: 19947 mean train loss:  1.40421325e-05, mean val. loss:  4.38061380e+00\n",
      "Epoch: 19948 mean train loss:  1.40389893e-05, mean val. loss:  4.38116407e+00\n",
      "Epoch: 19949 mean train loss:  1.37795869e-05, mean val. loss:  4.38173294e+00\n",
      "Epoch: 19950 mean train loss:  1.40369521e-05, mean val. loss:  4.38240767e+00\n",
      "Epoch: 19951 mean train loss:  1.38983887e-05, mean val. loss:  4.38305807e+00\n",
      "Epoch: 19952 mean train loss:  1.38118339e-05, mean val. loss:  4.38371944e+00\n",
      "Epoch: 19953 mean train loss:  1.40366319e-05, mean val. loss:  4.38428736e+00\n",
      "Epoch: 19954 mean train loss:  1.38966716e-05, mean val. loss:  4.38479900e+00\n",
      "Epoch: 19955 mean train loss:  1.40033371e-05, mean val. loss:  4.38520908e+00\n",
      "Epoch: 19956 mean train loss:  1.36676244e-05, mean val. loss:  4.38559294e+00\n",
      "Epoch: 19957 mean train loss:  1.39304611e-05, mean val. loss:  4.38598442e+00\n",
      "Epoch: 19958 mean train loss:  1.40976335e-05, mean val. loss:  4.38628101e+00\n",
      "Epoch: 19959 mean train loss:  1.40479242e-05, mean val. loss:  4.38659143e+00\n",
      "Epoch: 19960 mean train loss:  1.37896568e-05, mean val. loss:  4.38689995e+00\n",
      "Epoch: 19961 mean train loss:  1.40070042e-05, mean val. loss:  4.38718796e+00\n",
      "Epoch: 19962 mean train loss:  1.38920150e-05, mean val. loss:  4.38742876e+00\n",
      "Epoch: 19963 mean train loss:  1.39334297e-05, mean val. loss:  4.38769388e+00\n",
      "Epoch: 19964 mean train loss:  1.39467884e-05, mean val. loss:  4.38793993e+00\n",
      "Epoch: 19965 mean train loss:  1.37826428e-05, mean val. loss:  4.38820362e+00\n",
      "Epoch: 19966 mean train loss:  1.40729244e-05, mean val. loss:  4.38846874e+00\n",
      "Epoch: 19967 mean train loss:  1.36487943e-05, mean val. loss:  4.38878393e+00\n",
      "Epoch: 19968 mean train loss:  1.36847666e-05, mean val. loss:  4.38911343e+00\n",
      "Epoch: 19969 mean train loss:  1.39299955e-05, mean val. loss:  4.38938999e+00\n",
      "Epoch: 19970 mean train loss:  1.36742601e-05, mean val. loss:  4.38969946e+00\n",
      "Epoch: 19971 mean train loss:  1.37272873e-05, mean val. loss:  4.39005804e+00\n",
      "Epoch: 19972 mean train loss:  1.38729229e-05, mean val. loss:  4.39041138e+00\n",
      "Epoch: 19973 mean train loss:  1.38776959e-05, mean val. loss:  4.39075327e+00\n",
      "Epoch: 19974 mean train loss:  1.40477205e-05, mean val. loss:  4.39106083e+00\n",
      "Epoch: 19975 mean train loss:  1.38843316e-05, mean val. loss:  4.39132738e+00\n",
      "Epoch: 19976 mean train loss:  1.39664626e-05, mean val. loss:  4.39163160e+00\n",
      "Epoch: 19977 mean train loss:  1.39561598e-05, mean val. loss:  4.39195728e+00\n",
      "Epoch: 19978 mean train loss:  1.41164346e-05, mean val. loss:  4.39222860e+00\n",
      "Epoch: 19979 mean train loss:  1.40768243e-05, mean val. loss:  4.39248848e+00\n",
      "Epoch: 19980 mean train loss:  1.39300828e-05, mean val. loss:  4.39279032e+00\n",
      "Epoch: 19981 mean train loss:  1.39651529e-05, mean val. loss:  4.39297676e+00\n",
      "Epoch: 19982 mean train loss:  1.39018812e-05, mean val. loss:  4.39308548e+00\n",
      "Epoch: 19983 mean train loss:  1.37235620e-05, mean val. loss:  4.39324427e+00\n",
      "Epoch: 19984 mean train loss:  1.37724273e-05, mean val. loss:  4.39343262e+00\n",
      "Epoch: 19985 mean train loss:  1.35925075e-05, mean val. loss:  4.39374924e+00\n",
      "Epoch: 19986 mean train loss:  1.37516181e-05, mean val. loss:  4.39408255e+00\n",
      "Epoch: 19987 mean train loss:  1.39506883e-05, mean val. loss:  4.39446306e+00\n",
      "Epoch: 19988 mean train loss:  1.38338946e-05, mean val. loss:  4.39492035e+00\n",
      "Epoch: 19989 mean train loss:  1.40868651e-05, mean val. loss:  4.39540434e+00\n",
      "Epoch: 19990 mean train loss:  1.38154428e-05, mean val. loss:  4.39598036e+00\n",
      "Epoch: 19991 mean train loss:  1.38601754e-05, mean val. loss:  4.39654016e+00\n",
      "Epoch: 19992 mean train loss:  1.41113414e-05, mean val. loss:  4.39702892e+00\n",
      "Epoch: 19993 mean train loss:  1.37628813e-05, mean val. loss:  4.39759636e+00\n",
      "Epoch: 19994 mean train loss:  1.38670148e-05, mean val. loss:  4.39821386e+00\n",
      "Epoch: 19995 mean train loss:  1.39166950e-05, mean val. loss:  4.39889383e+00\n",
      "Epoch: 19996 mean train loss:  1.42274075e-05, mean val. loss:  4.39945650e+00\n",
      "Epoch: 19997 mean train loss:  1.37931784e-05, mean val. loss:  4.39995098e+00\n",
      "Epoch: 19998 mean train loss:  1.38939649e-05, mean val. loss:  4.40029478e+00\n",
      "Epoch: 19999 mean train loss:  1.37145689e-05, mean val. loss:  4.40058899e+00\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = train(net_H1, loaders, args, A, H1, test_functions, G_inv, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train, weak', 'validation, strong'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NN_library/training_data/VPINN_H1_{total_params}_1', np.array(losses_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1 = net_H1(grid_data).detach().cpu()\n",
    "error_1 = PDE_loss(grid_data, net_H1, A, H1).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x22a2b7db890>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 5850x1500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(19.5,5), dpi=300)\n",
    "pos1 = axs[0].pcolormesh(XY[0], XY[1], U1.reshape(N, N), cmap='inferno')\n",
    "fig.colorbar(pos1, ax=axs[0])\n",
    "pos2 = axs[1].pcolormesh(XY[0], XY[1], error_1.reshape(N, N), cmap='inferno')\n",
    "fig.colorbar(pos2, ax=axs[1])\n",
    "pos3 = axs[2].pcolormesh(XY[0], XY[1], np.log10(error_1.reshape(N, N)), cmap='inferno')\n",
    "fig.colorbar(pos3, ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data[:].to(dev).requires_grad_(True)\n",
    "T1 = net_H1(inputs)\n",
    "dq1 = torch.autograd.grad(\n",
    "            outputs=T1, inputs=inputs,\n",
    "            grad_outputs=torch.ones_like(T1)\n",
    "    )[0].detach()\n",
    "triang = Triangulation(data[:,0], data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.687227  ]\n",
      " [-0.00079829]]\n",
      "[[ 0.67873229]\n",
      " [-0.00118693]]\n"
     ]
    }
   ],
   "source": [
    "A_bound = compute_bound(T1, dq1, triang, inputs, A, H1, L)\n",
    "print(A_bound[0])\n",
    "print(A_bound[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0000045e+00 -2.1582788e-05]\n"
     ]
    }
   ],
   "source": [
    "A_avg = compute_avg(dq1, inputs, A, H1, L)\n",
    "print(A_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
